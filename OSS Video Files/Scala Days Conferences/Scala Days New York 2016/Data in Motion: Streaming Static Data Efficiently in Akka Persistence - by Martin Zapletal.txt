Title: Data in Motion: Streaming Static Data Efficiently in Akka Persistence - by Martin Zapletal
Publication date: 2016-06-17
Playlist: Scala Days New York 2016
Description: 
	This talk was recorded at Scala Days New York, 2016. Follow along on Twitter @scaladays and on the website for more information http://scaladays.org/.

Abstract:
Processing streaming data is becoming increasingly important in many areas. Scala and the Lightbend Reactive platform offer multiple solutions for processing streaming data, including Akka, Akka Streams and Apache Spark. This talk introduces the advantages and concepts of streaming data processing. It will mention differences between static data and data in motion and their usage as streaming data sources. The main goal of the presentation is detailed discussion of Akka Persistence Query and implementation of the stream production specification in Cassandra plugin for Akka Persistence (akka-persistence-cassandra) that the author participated in. Focus is on architecture and design considerations, implementation details, performance tuning and distributed system specifics such as correctness, efficiency, consistency, order, causality or failure scenario handling that are inherently part of the solution and apply to wide variety of distributed systems. Finally, other improvements to the Cassandra plugin for Akka Persistence project such as reusing the stream generation for non blocking asynchronous Akka Persistence recovery as well as application of the project and the discussed concepts to build modern reactive enterprise stream processing and asynchronous messaging distributed applications are presented.
Captions: 
	00:00:02,830 --> 00:00:06,669
hello everyone thank you for coming

00:00:04,479 --> 00:00:08,590
sorry for the issues my name is Martin

00:00:06,669 --> 00:00:11,769
I work for cake solutions a software

00:00:08,590 --> 00:00:13,690
services company working with Scala big

00:00:11,769 --> 00:00:15,280
data distributed systems enterprise

00:00:13,690 --> 00:00:16,960
systems and machine learning as well

00:00:15,280 --> 00:00:18,760
today I'll talk about data in motion

00:00:16,960 --> 00:00:20,980
about streaming static data efficiently

00:00:18,760 --> 00:00:22,779
I'll focus on a couple systems a

00:00:20,980 --> 00:00:25,720
specific use case but I will generalize

00:00:22,779 --> 00:00:27,430
the use cases to many many other areas

00:00:25,720 --> 00:00:30,220
of software engineering and distributed

00:00:27,430 --> 00:00:32,379
systems so just to introduce the topic

00:00:30,220 --> 00:00:34,510
traditionally many companies or still

00:00:32,379 --> 00:00:36,339
are using many different systems

00:00:34,510 --> 00:00:37,659
communicating randomly with each other

00:00:36,339 --> 00:00:39,790
and it's difficult to scale these

00:00:37,659 --> 00:00:41,559
systems and maintain consistency of the

00:00:39,790 --> 00:00:44,620
data in the distributed environment

00:00:41,559 --> 00:00:46,750
similarly many companies are using batch

00:00:44,620 --> 00:00:48,850
processing so they take a snapshot of

00:00:46,750 --> 00:00:50,619
data at a point in time and run some

00:00:48,850 --> 00:00:52,750
analytics on transformations on the data

00:00:50,619 --> 00:00:55,089
but what they really want to do is to

00:00:52,750 --> 00:00:56,680
run this pipeline as often as possible

00:00:55,089 --> 00:00:59,110
or immediately but they run it every

00:00:56,680 --> 00:01:02,080
hour or every day which is not really

00:00:59,110 --> 00:01:03,850
what they can do so they can do a bit

00:01:02,080 --> 00:01:07,150
better we would really want our data

00:01:03,850 --> 00:01:09,730
pipelines to be scalable to be

00:01:07,150 --> 00:01:11,890
performant to be elastic consistent

00:01:09,730 --> 00:01:15,310
available fault overrun and at the same

00:01:11,890 --> 00:01:16,990
time achieve the production readiness in

00:01:15,310 --> 00:01:20,640
distributed environment so maintain all

00:01:16,990 --> 00:01:24,310
these things in production environment

00:01:20,640 --> 00:01:28,330
one way how to handle this is using is

00:01:24,310 --> 00:01:29,560
using streams so streams have multiple

00:01:28,330 --> 00:01:31,420
publishers and multiple multiple

00:01:29,560 --> 00:01:33,730
subscribers and we can define some sort

00:01:31,420 --> 00:01:37,210
of computational graph this graph can

00:01:33,730 --> 00:01:38,980
have joins can have broadcast it can

00:01:37,210 --> 00:01:42,970
have projections maps and and other

00:01:38,980 --> 00:01:44,860
other operations and we can also have

00:01:42,970 --> 00:01:47,020
the arrows going the other way so back

00:01:44,860 --> 00:01:50,800
pressure or acknowledgments for reliable

00:01:47,020 --> 00:01:53,260
delivery we usually define this

00:01:50,800 --> 00:01:54,970
computation graph independently on the

00:01:53,260 --> 00:01:56,740
execution of the graph of the or the

00:01:54,970 --> 00:01:58,810
materialization so these are two

00:01:56,740 --> 00:02:03,100
independent step there are many use

00:01:58,810 --> 00:02:04,840
cases for streaming data such as central

00:02:03,100 --> 00:02:07,210
data social network data Internet of

00:02:04,840 --> 00:02:08,920
Things finding patterns in the data

00:02:07,210 --> 00:02:11,920
training machine learning models or

00:02:08,920 --> 00:02:13,780
using models on streaming data so today

00:02:11,920 --> 00:02:15,880
I'll talk about streaming static data

00:02:13,780 --> 00:02:18,970
there are essentially two ways

00:02:15,880 --> 00:02:21,070
or resources nan replayable sources that

00:02:18,970 --> 00:02:23,380
cannot be replayed multiple times so if

00:02:21,070 --> 00:02:25,510
we lose the data we cannot reply the

00:02:23,380 --> 00:02:27,400
data again and replayable sources that

00:02:25,510 --> 00:02:31,180
we can reply multiple times and we'll

00:02:27,400 --> 00:02:33,280
talk about these the most today a good

00:02:31,180 --> 00:02:34,990
example is the general database so we

00:02:33,280 --> 00:02:36,790
have a database with multiple rows each

00:02:34,990 --> 00:02:39,040
row has something like a primary key and

00:02:36,790 --> 00:02:40,990
these keys define some sort of ordering

00:02:39,040 --> 00:02:42,550
between those rows and we can

00:02:40,990 --> 00:02:46,360
essentially just read the database row

00:02:42,550 --> 00:02:48,310
by row and produce a stream of elements

00:02:46,360 --> 00:02:49,960
which works very well we have all the

00:02:48,310 --> 00:02:52,810
attributes that we want such as ordering

00:02:49,960 --> 00:02:56,020
but what if we add a new element into

00:02:52,810 --> 00:02:58,450
the database now it is difficult to know

00:02:56,020 --> 00:02:59,860
which elements were actually added we

00:02:58,450 --> 00:03:01,660
would have to remember all elements that

00:02:59,860 --> 00:03:04,980
we already emitted or we would have to

00:03:01,660 --> 00:03:07,000
remember at least a time stamp for each

00:03:04,980 --> 00:03:08,350
materialization of the stream so every

00:03:07,000 --> 00:03:10,480
time we would replay the database we

00:03:08,350 --> 00:03:12,970
would need to remember a timestamp of

00:03:10,480 --> 00:03:17,830
the last emitted event which can be very

00:03:12,970 --> 00:03:20,380
very of memory or disk consuming also if

00:03:17,830 --> 00:03:21,940
we allow a mitad this new row we would

00:03:20,380 --> 00:03:23,740
not have the ordering properties that we

00:03:21,940 --> 00:03:25,300
had previously so multiple

00:03:23,740 --> 00:03:27,730
materializations of the same stream

00:03:25,300 --> 00:03:29,710
would not be the same similarly with

00:03:27,730 --> 00:03:33,250
updates to database tables so now the

00:03:29,710 --> 00:03:35,650
value of five changed to 55 and again we

00:03:33,250 --> 00:03:38,110
would need to somehow know that this was

00:03:35,650 --> 00:03:40,180
changed similarly to previous slide and

00:03:38,110 --> 00:03:42,400
also if we emitted it we would again we

00:03:40,180 --> 00:03:45,310
wouldn't have the properties that we had

00:03:42,400 --> 00:03:47,920
previously such as ordering and we would

00:03:45,310 --> 00:03:50,410
have two values for the same key which

00:03:47,920 --> 00:03:52,240
we will ideally do not want so a mutable

00:03:50,410 --> 00:03:54,850
table does not represent the stream but

00:03:52,240 --> 00:03:57,280
it represents a snapshot of a streaming

00:03:54,850 --> 00:03:59,740
point in time another option how to get

00:03:57,280 --> 00:04:01,180
data from a database is to push data

00:03:59,740 --> 00:04:02,830
from the database so there are databases

00:04:01,180 --> 00:04:05,650
that actually support this such as

00:04:02,830 --> 00:04:07,810
MongoDB for SQL my skew are others so if

00:04:05,650 --> 00:04:10,120
a new element is added the database will

00:04:07,810 --> 00:04:11,709
immediately push it into the stream it's

00:04:10,120 --> 00:04:15,010
this is usually called change data

00:04:11,709 --> 00:04:16,570
capture or a change lock a very useful

00:04:15,010 --> 00:04:18,400
feature and in combination with the

00:04:16,570 --> 00:04:20,230
previous one so we pulled the data we

00:04:18,400 --> 00:04:22,390
take the consistent snapshot of the data

00:04:20,230 --> 00:04:24,160
and then we emitted the changes we can

00:04:22,390 --> 00:04:26,440
actually reconstruct the same database

00:04:24,160 --> 00:04:28,390
again from the changes and it will

00:04:26,440 --> 00:04:29,350
always be kept up to date with the

00:04:28,390 --> 00:04:32,320
changes

00:04:29,350 --> 00:04:34,210
but there's a simpler option how to emit

00:04:32,320 --> 00:04:36,340
data from a database and it's called

00:04:34,210 --> 00:04:38,350
log data structure the log is an

00:04:36,340 --> 00:04:40,450
extremely simple but very very powerful

00:04:38,350 --> 00:04:43,180
data structure we always only append

00:04:40,450 --> 00:04:44,620
data at the end of the log we can reply

00:04:43,180 --> 00:04:47,350
it from the beginning or from an offset

00:04:44,620 --> 00:04:49,120
that it defines the total order of

00:04:47,350 --> 00:04:50,830
operation so every time we replayed the

00:04:49,120 --> 00:04:54,130
same log it'll always have the same

00:04:50,830 --> 00:04:56,350
ordering and also we just need to

00:04:54,130 --> 00:04:58,330
remember a single number and offset that

00:04:56,350 --> 00:05:00,820
defines where we are in the industry

00:04:58,330 --> 00:05:02,230
it's very very useful for many use cases

00:05:00,820 --> 00:05:04,570
and it's used where usually where

00:05:02,230 --> 00:05:06,580
durability is extremely important such

00:05:04,570 --> 00:05:09,460
as the in databases in right ahead logs

00:05:06,580 --> 00:05:10,990
so we in a database when you're

00:05:09,460 --> 00:05:13,030
performing a transaction the database

00:05:10,990 --> 00:05:15,100
usually writes first into this write a

00:05:13,030 --> 00:05:16,720
headlock and if there's a failure it

00:05:15,100 --> 00:05:18,550
will reconcile the write a headlock with

00:05:16,720 --> 00:05:20,230
the state of the database and make sure

00:05:18,550 --> 00:05:24,100
it's it's correct it's also used in

00:05:20,230 --> 00:05:26,470
database replication and in distributed

00:05:24,100 --> 00:05:29,020
consensus algorithms for log replication

00:05:26,470 --> 00:05:33,100
and other use cases where durability is

00:05:29,020 --> 00:05:35,200
very important so when streaming or when

00:05:33,100 --> 00:05:36,910
creating a stream from a log data

00:05:35,200 --> 00:05:38,650
structure it's much simpler than the

00:05:36,910 --> 00:05:41,500
previous case because new element will

00:05:38,650 --> 00:05:44,380
always be added at the end of the stream

00:05:41,500 --> 00:05:46,120
or order log so we just remember the

00:05:44,380 --> 00:05:48,100
offset of the last one that we emitted

00:05:46,120 --> 00:05:50,020
and we simply omit the new one another

00:05:48,100 --> 00:05:52,000
property that this gives us gives us is

00:05:50,020 --> 00:05:54,010
the ordering so the ordering is

00:05:52,000 --> 00:05:56,470
maintained multiple materializations of

00:05:54,010 --> 00:05:58,870
the same log produce exactly the same

00:05:56,470 --> 00:06:02,350
stream or the previous one is a prefix

00:05:58,870 --> 00:06:03,970
of the next one right so let's talk

00:06:02,350 --> 00:06:06,130
about a couple systems I'm assuming most

00:06:03,970 --> 00:06:09,340
of you are familiar with akka it's a

00:06:06,130 --> 00:06:12,360
actor framework on the JVM it uses the

00:06:09,340 --> 00:06:14,380
actor abstraction for isolation

00:06:12,360 --> 00:06:16,510
supervision and asynchronous message

00:06:14,380 --> 00:06:19,630
passing but it's mostly useful for

00:06:16,510 --> 00:06:21,280
building general distributed systems in

00:06:19,630 --> 00:06:24,190
this case we have multiple machines so

00:06:21,280 --> 00:06:26,530
we have a Nicastro raka sharding we can

00:06:24,190 --> 00:06:29,530
address each actor by its key by by its

00:06:26,530 --> 00:06:31,390
key by this actors name and we can send

00:06:29,530 --> 00:06:33,940
messages to them these actors they have

00:06:31,390 --> 00:06:35,530
a mutable state but this state is only a

00:06:33,940 --> 00:06:37,300
memory so when the actor receives a

00:06:35,530 --> 00:06:39,700
message it can change its mutable state

00:06:37,300 --> 00:06:42,729
but if the actor dies or if the if the

00:06:39,700 --> 00:06:45,460
whole machine died we would lose all

00:06:42,729 --> 00:06:49,419
we'll use that mutable state so our

00:06:45,460 --> 00:06:51,099
copper system stores the events so

00:06:49,419 --> 00:06:53,740
they're the changes to the mutable state

00:06:51,099 --> 00:06:56,259
in a database in a log in it's

00:06:53,740 --> 00:06:58,029
essentially a right a headlock and if

00:06:56,259 --> 00:06:59,919
there's a failure then the actor can

00:06:58,029 --> 00:07:03,039
reply these messages from the log and

00:06:59,919 --> 00:07:05,469
reconstruct it's it's mutable state

00:07:03,039 --> 00:07:07,270
similarly with rebalances so we've been

00:07:05,469 --> 00:07:10,229
the active moves to a different node can

00:07:07,270 --> 00:07:13,150
do the same and reconstruct the state

00:07:10,229 --> 00:07:16,330
there's also a component called a copper

00:07:13,150 --> 00:07:18,969
systems query and this creates a stream

00:07:16,330 --> 00:07:20,919
of events from that from that event

00:07:18,969 --> 00:07:22,509
source journal from that log there are

00:07:20,919 --> 00:07:24,430
three streams supported at the moment

00:07:22,509 --> 00:07:27,550
events by persistence ID which gives us

00:07:24,430 --> 00:07:30,909
all events for a single particular actor

00:07:27,550 --> 00:07:32,919
event all all persistence IDs which

00:07:30,909 --> 00:07:35,860
gives us all names of all actors in the

00:07:32,919 --> 00:07:38,589
system's now in the system an event by

00:07:35,860 --> 00:07:42,550
tag which creates a stream of all events

00:07:38,589 --> 00:07:44,169
that have a particular tag so most

00:07:42,550 --> 00:07:46,449
credit for the implementation of this

00:07:44,169 --> 00:07:48,580
and in our cup resistance using

00:07:46,449 --> 00:07:49,839
Cassandra database goes to Patrick

00:07:48,580 --> 00:07:51,580
Norville and Martin creaser

00:07:49,839 --> 00:07:54,550
but I had the opportunity to participate

00:07:51,580 --> 00:07:56,110
on the implementation so I'll talk about

00:07:54,550 --> 00:07:57,879
some of the implementation details

00:07:56,110 --> 00:08:01,149
design considerations and I will explain

00:07:57,879 --> 00:08:03,039
the decision and decisions and have some

00:08:01,149 --> 00:08:04,810
of the implications especially and

00:08:03,039 --> 00:08:10,389
generalize that to distribute to the

00:08:04,810 --> 00:08:12,249
environment so there are multiple

00:08:10,389 --> 00:08:14,080
implementations of our cup persistence

00:08:12,249 --> 00:08:16,509
we'll talk about the one for Cassandra

00:08:14,080 --> 00:08:18,370
database Cassandra is a distributed

00:08:16,509 --> 00:08:21,819
highly scaleable and available database

00:08:18,370 --> 00:08:26,800
reduces replication and sharding and a

00:08:21,819 --> 00:08:28,180
wide row data model if we use the cap

00:08:26,800 --> 00:08:30,669
theorem classification it would lie

00:08:28,180 --> 00:08:33,399
somewhere in the AP area so it's

00:08:30,669 --> 00:08:35,050
available in partition tolerant the one

00:08:33,399 --> 00:08:37,120
of the main abstractions is called the

00:08:35,050 --> 00:08:39,219
partition partition can have multiple

00:08:37,120 --> 00:08:41,019
rows a single partition always lives on

00:08:39,219 --> 00:08:42,430
a single machine but a partition can

00:08:41,019 --> 00:08:44,440
have multiple rows we will not talk

00:08:42,430 --> 00:08:45,310
about rows here we will just represent

00:08:44,440 --> 00:08:47,800
them as a thanks

00:08:45,310 --> 00:08:49,480
so each event will be a row but in this

00:08:47,800 --> 00:08:51,250
case we're only really interested in the

00:08:49,480 --> 00:08:54,850
sequence number so the increasing

00:08:51,250 --> 00:08:56,540
sequence number is event 0 1 2 etc each

00:08:54,850 --> 00:08:58,430
partition has a partitioning key which

00:08:56,540 --> 00:09:01,160
essentially a pointer to this partition

00:08:58,430 --> 00:09:04,670
in our case is the tuple persistence ID

00:09:01,160 --> 00:09:06,560
partition number and if we have this

00:09:04,670 --> 00:09:10,250
point we can find this partition very

00:09:06,560 --> 00:09:12,709
efficiently the persistence IDs name of

00:09:10,250 --> 00:09:14,509
the actor and partition number is the

00:09:12,709 --> 00:09:16,279
number of partitions so the partition

00:09:14,509 --> 00:09:18,980
lives on a single physical machine so it

00:09:16,279 --> 00:09:20,209
can grow very large but we want to

00:09:18,980 --> 00:09:22,040
manage that size of the partition

00:09:20,209 --> 00:09:23,600
because if it's too large it's not very

00:09:22,040 --> 00:09:25,430
efficient if it's too small

00:09:23,600 --> 00:09:27,350
we need to query more partitions so

00:09:25,430 --> 00:09:29,480
that's not efficient as well so we want

00:09:27,350 --> 00:09:31,100
to manage that size so we can form one

00:09:29,480 --> 00:09:32,720
actor we can have multiple partition

00:09:31,100 --> 00:09:37,339
numbers because we manage the size of

00:09:32,720 --> 00:09:40,639
the partition finding an event in a

00:09:37,339 --> 00:09:42,259
partition is very efficient however find

00:09:40,639 --> 00:09:44,269
going through multiple partitions or

00:09:42,259 --> 00:09:45,829
scanning multiple partitions can be

00:09:44,269 --> 00:09:49,759
incredibly inefficient so we would want

00:09:45,829 --> 00:09:52,639
to avoid that as much as possible so we

00:09:49,759 --> 00:09:57,829
have implemented this generic this is

00:09:52,639 --> 00:10:00,529
readable at all so we've implemented

00:09:57,829 --> 00:10:02,180
this generic actor publisher that

00:10:00,529 --> 00:10:04,130
essentially reads data from the log

00:10:02,180 --> 00:10:07,100
format from a database and produces

00:10:04,130 --> 00:10:08,600
anarchist stream it defines a couple of

00:10:07,100 --> 00:10:11,209
methods that the implementing classes

00:10:08,600 --> 00:10:12,889
have to have to implement such as define

00:10:11,209 --> 00:10:14,600
how what the initial state is how to

00:10:12,889 --> 00:10:16,930
update the state if a new element is

00:10:14,600 --> 00:10:19,490
received and a couple other methods

00:10:16,930 --> 00:10:21,459
which might seem quite simple but it

00:10:19,490 --> 00:10:24,110
actually requires a lot of

00:10:21,459 --> 00:10:27,170
asynchronously handling couple of events

00:10:24,110 --> 00:10:30,230
so for example it's it's innaka streams

00:10:27,170 --> 00:10:31,790
so the downstream components using back

00:10:30,230 --> 00:10:34,370
pressure so they can ask for more data

00:10:31,790 --> 00:10:36,769
so it has to keep a buffer of elements

00:10:34,370 --> 00:10:40,370
and provide new data when the downstream

00:10:36,769 --> 00:10:43,399
components ask it has to handle the

00:10:40,370 --> 00:10:45,260
timeout queries because at some point it

00:10:43,399 --> 00:10:47,269
will reach the end of the lock and it

00:10:45,260 --> 00:10:49,850
needs to know when to query the data

00:10:47,269 --> 00:10:51,709
again so we keep some sort of timeout

00:10:49,850 --> 00:10:53,540
and required database multiple times

00:10:51,709 --> 00:10:56,569
after the timeout it needs to

00:10:53,540 --> 00:10:58,639
asynchronously try to get data from the

00:10:56,569 --> 00:11:00,259
database it needs to prefetch the data

00:10:58,639 --> 00:11:02,209
from the same partition or the next

00:11:00,259 --> 00:11:04,250
partition it needs to maintain as a

00:11:02,209 --> 00:11:06,920
buffer so when all this has to be

00:11:04,250 --> 00:11:10,160
handled fully asynchronously in Edisto

00:11:06,920 --> 00:11:12,470
or in a synchronous environment

00:11:10,160 --> 00:11:16,490
and it has ideally should never blow the

00:11:12,470 --> 00:11:18,079
thread so the first implementation is

00:11:16,490 --> 00:11:20,149
the vent by persistence ID which should

00:11:18,079 --> 00:11:23,000
give us all events for a single

00:11:20,149 --> 00:11:25,009
persistence ID for a single actor the

00:11:23,000 --> 00:11:27,230
database query looks exactly as we would

00:11:25,009 --> 00:11:29,000
expect a passenger query to look like so

00:11:27,230 --> 00:11:31,639
we specify both the parts of the

00:11:29,000 --> 00:11:34,220
partitioning key and a range of sequence

00:11:31,639 --> 00:11:35,600
numbers so querying the database like

00:11:34,220 --> 00:11:37,639
this should be very efficient so we

00:11:35,600 --> 00:11:39,800
essentially just go through all events

00:11:37,639 --> 00:11:42,050
in a one single partition or a couple of

00:11:39,800 --> 00:11:46,160
partitions for that single actor which

00:11:42,050 --> 00:11:48,560
should be very very efficient and the

00:11:46,160 --> 00:11:49,879
code is actually very simple as well the

00:11:48,560 --> 00:11:51,500
only state we need to remember our

00:11:49,879 --> 00:11:52,819
essentially three numbers so we need to

00:11:51,500 --> 00:11:54,980
increase the sequence number in the

00:11:52,819 --> 00:11:58,160
count and the partition number so just

00:11:54,980 --> 00:12:02,300
three numbers very efficient all

00:11:58,160 --> 00:12:04,850
persistence IDs a query or a stream that

00:12:02,300 --> 00:12:06,740
should give us all persistence IDs names

00:12:04,850 --> 00:12:08,420
of all actors in the system it's a bit

00:12:06,740 --> 00:12:10,160
more complicated so the query still

00:12:08,420 --> 00:12:12,800
looks reasonable it's like this thing

00:12:10,160 --> 00:12:15,350
persistent ID partition number from the

00:12:12,800 --> 00:12:17,029
table but if you think about it it has

00:12:15,350 --> 00:12:20,870
to go through all nodes in the cluster

00:12:17,029 --> 00:12:22,730
and the top of this thing also specifies

00:12:20,870 --> 00:12:24,709
the whole topic so it will it will give

00:12:22,730 --> 00:12:26,420
us this on this one so we omit the zero

00:12:24,709 --> 00:12:28,850
which is the name of the actor we will

00:12:26,420 --> 00:12:30,259
also give us the second couple but we

00:12:28,850 --> 00:12:32,269
have already emitted zero so we don't

00:12:30,259 --> 00:12:38,170
want to do it again and then we can omit

00:12:32,269 --> 00:12:40,310
the one which is still fine but the

00:12:38,170 --> 00:12:41,959
partitions in Cassandra they have no

00:12:40,310 --> 00:12:43,910
ordering between each other so if a new

00:12:41,959 --> 00:12:46,220
one is added we don't know where so we

00:12:43,910 --> 00:12:48,350
just need to query the database again

00:12:46,220 --> 00:12:49,759
and we need to keep a set of all

00:12:48,350 --> 00:12:51,589
persistence IDs that were already

00:12:49,759 --> 00:12:54,199
emitted we need to compare these two

00:12:51,589 --> 00:12:56,360
sets and only emit the new ones so

00:12:54,199 --> 00:12:58,279
that's very inefficient because we're

00:12:56,360 --> 00:13:00,709
doing streams and streams are unbounded

00:12:58,279 --> 00:13:03,230
so we can pretend we need to potentially

00:13:00,709 --> 00:13:04,610
maintain and unbounded a set of

00:13:03,230 --> 00:13:09,170
persistence IDs that were already

00:13:04,610 --> 00:13:12,860
emitted and event by tag is by far the

00:13:09,170 --> 00:13:15,350
most complicated one because if we

00:13:12,860 --> 00:13:17,120
wanted to omit all so firstly when

00:13:15,350 --> 00:13:19,399
writing an event we can assign a tag to

00:13:17,120 --> 00:13:21,290
this event so events for a single actor

00:13:19,399 --> 00:13:22,519
they are ordered by sequence number so

00:13:21,290 --> 00:13:23,870
they have some sort of causal

00:13:22,519 --> 00:13:25,880
consistency

00:13:23,870 --> 00:13:27,230
but events between multiple actors they

00:13:25,880 --> 00:13:29,120
have no ordering whatsoever they're

00:13:27,230 --> 00:13:30,589
completely independent but we can assign

00:13:29,120 --> 00:13:32,779
an event when writing an event we can

00:13:30,589 --> 00:13:35,060
assign it a tag and it links events

00:13:32,779 --> 00:13:37,010
together for which is useful for

00:13:35,060 --> 00:13:40,490
creating some sort of aggregations in

00:13:37,010 --> 00:13:41,870
between actors but to be able to query

00:13:40,490 --> 00:13:43,279
those we would have to go through all

00:13:41,870 --> 00:13:45,380
potentially in parallel but we would

00:13:43,279 --> 00:13:46,760
have to go through all partitions in

00:13:45,380 --> 00:13:48,650
this and the whole cluster so we would

00:13:46,760 --> 00:13:51,500
have to scan all of them and only omit

00:13:48,650 --> 00:13:53,750
those that have a tag when we would omit

00:13:51,500 --> 00:13:55,160
those to the stream which is incredibly

00:13:53,750 --> 00:13:57,740
inefficient that is something that we

00:13:55,160 --> 00:14:00,740
never want to do in a database in

00:13:57,740 --> 00:14:02,750
distributed database like a saundra so

00:14:00,740 --> 00:14:04,550
the way this is usually handled is by

00:14:02,750 --> 00:14:06,589
restructuring the data in a different

00:14:04,550 --> 00:14:08,390
way so we create a materialized view a

00:14:06,589 --> 00:14:09,950
different view of the data different the

00:14:08,390 --> 00:14:12,410
data is different the repartition

00:14:09,950 --> 00:14:15,680
differently structured on the disk and

00:14:12,410 --> 00:14:20,200
then we can easily quite efficiently go

00:14:15,680 --> 00:14:20,200
through the events that are in the view

00:14:22,870 --> 00:14:28,160
so that works very well but there are a

00:14:26,270 --> 00:14:29,930
few issues firstly it's heavily

00:14:28,160 --> 00:14:31,580
dependent on Cassandra r3 implementation

00:14:29,930 --> 00:14:33,680
of materialized views so we kind of use

00:14:31,580 --> 00:14:36,560
the different backends such as Kafka or

00:14:33,680 --> 00:14:39,260
something else also we don't have much

00:14:36,560 --> 00:14:40,910
control over the sub partitioning so we

00:14:39,260 --> 00:14:42,770
need to use something like a time bucket

00:14:40,910 --> 00:14:45,709
it can be a day it can be a week or a

00:14:42,770 --> 00:14:49,040
year but we cannot manage it more more

00:14:45,709 --> 00:14:50,750
with more granularity so in some day you

00:14:49,040 --> 00:14:52,520
can have plenty of events with the same

00:14:50,750 --> 00:14:55,010
tag other day you may have very little

00:14:52,520 --> 00:14:56,750
of them so it's not efficient and after

00:14:55,010 --> 00:14:58,310
a few years you will have plenty of

00:14:56,750 --> 00:15:01,370
partitions so going through all of them

00:14:58,310 --> 00:15:02,750
will not be very efficient at all but

00:15:01,370 --> 00:15:05,510
the biggest issue really is that

00:15:02,750 --> 00:15:07,790
Cassandra replicates the data from the

00:15:05,510 --> 00:15:10,550
original table to the view in parallel

00:15:07,790 --> 00:15:13,490
which might not sound like an issue but

00:15:10,550 --> 00:15:15,050
we do have some causal consistency

00:15:13,490 --> 00:15:17,180
between events for a single actor so

00:15:15,050 --> 00:15:20,150
they define ordering and we also have

00:15:17,180 --> 00:15:22,339
consistency between multiple partitions

00:15:20,150 --> 00:15:23,810
for the same actor so we cannot just

00:15:22,339 --> 00:15:25,790
replicate them in parallel because we

00:15:23,810 --> 00:15:27,620
would lose the causal consistency events

00:15:25,790 --> 00:15:30,230
that are in later partition may be

00:15:27,620 --> 00:15:32,360
replicated earlier so it will eventually

00:15:30,230 --> 00:15:35,660
be fine but for a period of time we may

00:15:32,360 --> 00:15:37,490
lose the the the causal consistency the

00:15:35,660 --> 00:15:39,310
partial consistency or the

00:15:37,490 --> 00:15:42,500
chronically increasing sequence number

00:15:39,310 --> 00:15:44,420
similarly when writing to events for the

00:15:42,500 --> 00:15:46,370
single actor one write can go to one

00:15:44,420 --> 00:15:48,440
note another note write can go to a

00:15:46,370 --> 00:15:50,440
different note and the one the later the

00:15:48,440 --> 00:15:53,120
later one make can be replicated first

00:15:50,440 --> 00:15:54,320
under certain synchro circumstances so

00:15:53,120 --> 00:15:56,540
again we only have eventual consistency

00:15:54,320 --> 00:15:58,550
but we would really want stronger

00:15:56,540 --> 00:16:01,790
consistency guarantees which might be an

00:15:58,550 --> 00:16:03,860
issue using this implementation the

00:16:01,790 --> 00:16:05,600
problem is there is no way how to handle

00:16:03,860 --> 00:16:07,580
this on the read side of fact

00:16:05,600 --> 00:16:08,540
efficiently so assume we have omitted

00:16:07,580 --> 00:16:11,390
id0

00:16:08,540 --> 00:16:15,649
event one for ID zero that's fine but

00:16:11,390 --> 00:16:17,209
the next one is a van 104 ID zero so we

00:16:15,649 --> 00:16:18,950
don't know if this is the next one that

00:16:17,209 --> 00:16:20,779
we should emit into the stream because

00:16:18,950 --> 00:16:22,779
there may be others in between them but

00:16:20,779 --> 00:16:25,339
we just don't know we can the only thing

00:16:22,779 --> 00:16:26,810
this can be only this can only work we

00:16:25,339 --> 00:16:29,839
can manage the increasing sequence

00:16:26,810 --> 00:16:32,089
number only if all events for an actor

00:16:29,839 --> 00:16:33,830
are tagged with the same tag then we can

00:16:32,089 --> 00:16:36,800
maintain the sequence number and make

00:16:33,830 --> 00:16:38,779
sure the causality is there but in

00:16:36,800 --> 00:16:40,190
general case we cannot do it so the way

00:16:38,779 --> 00:16:42,500
it is currently handled in acha acha

00:16:40,190 --> 00:16:44,810
persistence query is we just delay the

00:16:42,500 --> 00:16:46,910
reads from the database by a

00:16:44,810 --> 00:16:48,820
configurable amount of seconds it's by

00:16:46,910 --> 00:16:51,350
default it's ten seconds which

00:16:48,820 --> 00:16:53,510
unfortunately you have to wait for for

00:16:51,350 --> 00:16:56,240
the events ten seconds which may not be

00:16:53,510 --> 00:16:58,370
ideal use case especially for real-time

00:16:56,240 --> 00:17:00,290
use cases and at the same time it's

00:16:58,370 --> 00:17:02,990
still non-deterministic so if you're a

00:17:00,290 --> 00:17:04,520
cluster if there's a lot of events in

00:17:02,990 --> 00:17:07,370
the cluster or if you have a network

00:17:04,520 --> 00:17:09,319
partition the replication may actually

00:17:07,370 --> 00:17:11,959
take much longer than that so it's still

00:17:09,319 --> 00:17:14,589
non-deterministic and we may not get the

00:17:11,959 --> 00:17:14,589
correct result

00:17:20,630 --> 00:17:25,250
one of the most important features of

00:17:22,370 --> 00:17:26,870
waka persistence Cassandra is replay so

00:17:25,250 --> 00:17:30,559
we need to replay all events for a

00:17:26,870 --> 00:17:32,720
single actor and reconstruct its its

00:17:30,559 --> 00:17:36,010
state this is the original

00:17:32,720 --> 00:17:39,710
implementation firstly just wrapped a

00:17:36,010 --> 00:17:41,360
blocking role in a future and secondly

00:17:39,710 --> 00:17:43,190
to use this notion of iterators which

00:17:41,360 --> 00:17:46,400
used a lot of ours and knows which is

00:17:43,190 --> 00:17:47,660
not really the ideal functional code on

00:17:46,400 --> 00:17:49,549
the other hand it was pretty low level

00:17:47,660 --> 00:17:53,659
and it was actually really performant on

00:17:49,549 --> 00:17:56,720
a single thread but the biggest issue

00:17:53,659 --> 00:17:58,580
was this this code this row iterator so

00:17:56,720 --> 00:18:00,200
it calls the new it the session dot

00:17:58,580 --> 00:18:01,880
executor which is a blocking call to a

00:18:00,200 --> 00:18:04,039
database which we would ideally want to

00:18:01,880 --> 00:18:07,820
avoid and another issue a bit more

00:18:04,039 --> 00:18:13,429
hidden one is that the data stacks java

00:18:07,820 --> 00:18:14,780
cassandra driver calls does a blocking

00:18:13,429 --> 00:18:16,610
call to a database and then keeps an

00:18:14,780 --> 00:18:18,080
internal buffer and when you call

00:18:16,610 --> 00:18:19,580
iterator load next it will give you a

00:18:18,080 --> 00:18:22,130
next element from the buffer which works

00:18:19,580 --> 00:18:23,659
fine but when you exhaust the buffer it

00:18:22,130 --> 00:18:25,700
will actually perform another blocking

00:18:23,659 --> 00:18:28,909
call to a database which ideally we

00:18:25,700 --> 00:18:30,650
would want to avoid but with acha

00:18:28,909 --> 00:18:32,450
persistent square implemented can

00:18:30,650 --> 00:18:35,090
actually reuse the actor the event by

00:18:32,450 --> 00:18:37,730
persistent ID stream that that we that

00:18:35,090 --> 00:18:40,549
we saw previously and we can actually

00:18:37,730 --> 00:18:42,289
just simply call queries go to event by

00:18:40,549 --> 00:18:43,940
persistent ID which will give a stream

00:18:42,289 --> 00:18:46,730
of all events for a single persistence

00:18:43,940 --> 00:18:48,950
ID is exactly what we need for replay so

00:18:46,730 --> 00:18:50,539
the code is much simplified it's fully

00:18:48,950 --> 00:18:53,750
asynchronous non-blocking calls to

00:18:50,539 --> 00:18:56,179
database and also we specify the correct

00:18:53,750 --> 00:18:57,919
consistency guarantee so read and write

00:18:56,179 --> 00:18:59,510
of at least a quorum we will get the

00:18:57,919 --> 00:19:03,440
strong consistency that we need for this

00:18:59,510 --> 00:19:05,990
use case this new implementation arm is

00:19:03,440 --> 00:19:07,490
slower on a single thread on the other

00:19:05,990 --> 00:19:09,370
hand if there are multiple threads

00:19:07,490 --> 00:19:11,929
available the performance is actually

00:19:09,370 --> 00:19:14,419
you in most cases better than the

00:19:11,929 --> 00:19:16,700
previous implementation if you want to

00:19:14,419 --> 00:19:18,409
tune the implementation definitely have

00:19:16,700 --> 00:19:21,169
a look at things like the thread pool

00:19:18,409 --> 00:19:23,539
assigned to the replay so the dispatch

00:19:21,169 --> 00:19:24,890
are available and also definitely have a

00:19:23,539 --> 00:19:26,360
look at things like the size of the

00:19:24,890 --> 00:19:29,590
buffer size of partitions and other

00:19:26,360 --> 00:19:29,590
other parameters

00:19:30,260 --> 00:19:34,590
so that that's the current

00:19:32,790 --> 00:19:36,150
implementation but we also during the

00:19:34,590 --> 00:19:38,220
design phase we discussed another

00:19:36,150 --> 00:19:40,470
possible implementation I would like to

00:19:38,220 --> 00:19:43,230
talk you through this this other

00:19:40,470 --> 00:19:44,970
alternative implementation because it

00:19:43,230 --> 00:19:47,310
not very nicely explained some of the

00:19:44,970 --> 00:19:49,470
concepts how streaming works and how

00:19:47,310 --> 00:19:53,460
many concepts work in general

00:19:49,470 --> 00:19:55,680
distributed environment so the first the

00:19:53,460 --> 00:19:58,110
first component would be a essentially a

00:19:55,680 --> 00:20:01,470
write a head lock a physical stream

00:19:58,110 --> 00:20:03,000
per node so when an actor writes a

00:20:01,470 --> 00:20:05,460
message on a node it will immediately

00:20:03,000 --> 00:20:07,470
write it to physical node to a physical

00:20:05,460 --> 00:20:10,130
partition on the same machine so each of

00:20:07,470 --> 00:20:12,810
those physical partitions can have a

00:20:10,130 --> 00:20:15,510
events for multiple actors and because

00:20:12,810 --> 00:20:19,910
an actor can move between nodes actors

00:20:15,510 --> 00:20:19,910
events can lay in multiple partitions

00:20:19,940 --> 00:20:24,600
because cassandra gives us atomicity and

00:20:23,010 --> 00:20:27,360
isolation when writing to a single

00:20:24,600 --> 00:20:28,800
partition that's a good start for the

00:20:27,360 --> 00:20:30,660
consistency guarantees that we will

00:20:28,800 --> 00:20:32,610
later need then we will merge all these

00:20:30,660 --> 00:20:35,700
physical streams into one single all

00:20:32,610 --> 00:20:37,440
events stream we define an order of

00:20:35,700 --> 00:20:39,750
these events and the only thing we need

00:20:37,440 --> 00:20:41,730
to maintain this causality so the

00:20:39,750 --> 00:20:45,570
partial ordering the increasing sequence

00:20:41,730 --> 00:20:48,540
number for a single actor and the last

00:20:45,570 --> 00:20:50,160
part of this of this pipeline would be

00:20:48,540 --> 00:20:51,870
the person the materialized views that

00:20:50,160 --> 00:20:54,000
we had previously so the first one is

00:20:51,870 --> 00:20:56,280
exactly the same its event by

00:20:54,000 --> 00:20:59,880
persistence ID but the second one is the

00:20:56,280 --> 00:21:01,860
bank by tag previously we had to use the

00:20:59,880 --> 00:21:03,510
the time bucket but now we have much

00:21:01,860 --> 00:21:05,400
more control we can reply the previous

00:21:03,510 --> 00:21:07,410
clip stream as many times as we want we

00:21:05,400 --> 00:21:09,510
can maintain any status we want so we

00:21:07,410 --> 00:21:12,090
can just build very efficient view and

00:21:09,510 --> 00:21:15,420
we can maintain the sub partitioning as

00:21:12,090 --> 00:21:19,020
we want similarly or persistence IDs

00:21:15,420 --> 00:21:20,580
previously we had to maintain the set of

00:21:19,020 --> 00:21:23,160
all persistence IDs that we have already

00:21:20,580 --> 00:21:25,770
emitted but now we can create a very

00:21:23,160 --> 00:21:30,150
efficient view and query that very very

00:21:25,770 --> 00:21:31,740
efficiently the only thing that we again

00:21:30,150 --> 00:21:35,760
need to maintain is the causal

00:21:31,740 --> 00:21:37,110
consistency and in all these views so

00:21:35,760 --> 00:21:39,300
the pipeline not only makes us

00:21:37,110 --> 00:21:42,210
independent on Cassandra 3 but it also

00:21:39,300 --> 00:21:45,450
gives us all the guarantees that we need

00:21:42,210 --> 00:21:48,240
such as the causality consistence and

00:21:45,450 --> 00:21:50,310
performance when reading from from the

00:21:48,240 --> 00:21:54,120
database but most importantly it makes

00:21:50,310 --> 00:21:56,490
us the reasoning about consistency and

00:21:54,120 --> 00:21:58,890
and and failures in this pipeline very

00:21:56,490 --> 00:22:00,600
very efficient and simple so you might

00:21:58,890 --> 00:22:02,790
you might be thinking why wouldn't we

00:22:00,600 --> 00:22:05,160
when when in reactor writes and append

00:22:02,790 --> 00:22:06,690
why would we immediately write it into

00:22:05,160 --> 00:22:09,900
all these views why do we have to go

00:22:06,690 --> 00:22:11,520
through that long pipeline so the reason

00:22:09,900 --> 00:22:13,980
is we are in distributed environment so

00:22:11,520 --> 00:22:15,510
we cannot just simply write in a single

00:22:13,980 --> 00:22:17,280
transaction we could do that in a

00:22:15,510 --> 00:22:19,080
relational database and that would work

00:22:17,280 --> 00:22:20,910
very well but in distributed environment

00:22:19,080 --> 00:22:23,280
we would have to do a distributed

00:22:20,910 --> 00:22:26,100
transaction which is which is very

00:22:23,280 --> 00:22:27,840
expensive cassandra batches so we

00:22:26,100 --> 00:22:30,330
created one batch of cassandra batches

00:22:27,840 --> 00:22:32,490
only give us Thomas it but not isolation

00:22:30,330 --> 00:22:34,770
so the batch will always will always

00:22:32,490 --> 00:22:36,750
succeed or fail but it only has a

00:22:34,770 --> 00:22:38,670
ventral consistency which is a big issue

00:22:36,750 --> 00:22:40,440
for us because the events by persistence

00:22:38,670 --> 00:22:42,780
ID that is used for replay needs

00:22:40,440 --> 00:22:44,280
stronger form of consistency it needs to

00:22:42,780 --> 00:22:47,160
read your own right consistency

00:22:44,280 --> 00:22:49,410
otherwise an actor actor could write

00:22:47,160 --> 00:22:51,090
events then fail and it would recover

00:22:49,410 --> 00:22:52,800
and it would not read all the event that

00:22:51,090 --> 00:22:54,840
it has written it will only read part of

00:22:52,800 --> 00:22:57,630
them which is a big issue because you

00:22:54,840 --> 00:23:01,190
definitely need consistency for your

00:22:57,630 --> 00:23:01,190
actors when recovering from a failure

00:23:02,990 --> 00:23:08,370
okay so we could write into a single

00:23:06,720 --> 00:23:10,950
partition the event by persistence ID

00:23:08,370 --> 00:23:12,480
because that atomic and isolated so we

00:23:10,950 --> 00:23:13,890
would have the the guarantees that we

00:23:12,480 --> 00:23:15,960
need and then we could write a different

00:23:13,890 --> 00:23:17,760
batch with all the other views because

00:23:15,960 --> 00:23:20,460
they don't have that need or read your

00:23:17,760 --> 00:23:22,290
own writes consistency the problem with

00:23:20,460 --> 00:23:25,290
that is this is called dual writes so

00:23:22,290 --> 00:23:27,120
you do to write but the system can fail

00:23:25,290 --> 00:23:29,520
asynchronously at any time so it can

00:23:27,120 --> 00:23:32,160
happen that it fails and one of the one

00:23:29,520 --> 00:23:33,660
of the right succeeds but the other one

00:23:32,160 --> 00:23:35,160
doesn't and therefore we would have

00:23:33,660 --> 00:23:39,450
inconsistent data which is something

00:23:35,160 --> 00:23:40,830
that we again cannot afford all right so

00:23:39,450 --> 00:23:42,840
let's discuss some of the concepts that

00:23:40,830 --> 00:23:44,610
we use in this pipeline first one is the

00:23:42,840 --> 00:23:46,410
Ben time processing most stream

00:23:44,610 --> 00:23:49,590
processing systems only use processing

00:23:46,410 --> 00:23:51,660
time processing which means they process

00:23:49,590 --> 00:23:53,490
the events as they arrive but very often

00:23:51,660 --> 00:23:55,440
in many many business use cases you want

00:23:53,490 --> 00:23:56,040
to use the event time but when time is

00:23:55,440 --> 00:23:57,840
the time the

00:23:56,040 --> 00:24:00,840
ban was created or some business time

00:23:57,840 --> 00:24:03,150
associated with that with that event and

00:24:00,840 --> 00:24:06,660
those two times are not the same because

00:24:03,150 --> 00:24:09,830
the the pipeline can cause a delay so

00:24:06,660 --> 00:24:12,330
that or the network transitioning so

00:24:09,830 --> 00:24:14,070
these blue events were created at around

00:24:12,330 --> 00:24:15,390
the same time so they belong together we

00:24:14,070 --> 00:24:17,880
would want them to be in a window

00:24:15,390 --> 00:24:20,010
together but the one of them was delayed

00:24:17,880 --> 00:24:22,350
so if we used processing time windowing

00:24:20,010 --> 00:24:24,240
when the blue window closes that this

00:24:22,350 --> 00:24:26,820
one event would not be there would not

00:24:24,240 --> 00:24:29,010
be in the window so we really want to

00:24:26,820 --> 00:24:31,560
use event time windowing so we need to

00:24:29,010 --> 00:24:32,970
wait for a period of time and until all

00:24:31,560 --> 00:24:35,070
the blue events are in the blue window

00:24:32,970 --> 00:24:37,050
and then close the blue window there are

00:24:35,070 --> 00:24:38,880
two difficulties usually associated with

00:24:37,050 --> 00:24:40,920
that the first one we need to keep a

00:24:38,880 --> 00:24:42,930
buffer I need to keep a buffer off of

00:24:40,920 --> 00:24:45,870
the blue event because we did not close

00:24:42,930 --> 00:24:48,600
the window the biggest difficulty is we

00:24:45,870 --> 00:24:50,700
need to maintain correctness the

00:24:48,600 --> 00:24:52,200
completeness which is difficult because

00:24:50,700 --> 00:24:53,940
we don't really know how many blue

00:24:52,200 --> 00:24:55,440
events there are we don't know how long

00:24:53,940 --> 00:24:57,000
we should write there we have only this

00:24:55,440 --> 00:24:58,290
one but there could be others more

00:24:57,000 --> 00:25:01,230
delight so we don't really know how long

00:24:58,290 --> 00:25:04,350
we should wait this is usually handled

00:25:01,230 --> 00:25:06,750
by using watermarks triggers or some

00:25:04,350 --> 00:25:08,430
sort of heuristics another concept

00:25:06,750 --> 00:25:12,270
common in distributed environment is

00:25:08,430 --> 00:25:15,030
ordering often we don't have access to a

00:25:12,270 --> 00:25:17,760
global shared clock that would be atomic

00:25:15,030 --> 00:25:19,530
and accessible by everyone so and also

00:25:17,760 --> 00:25:21,800
often we don't have communication

00:25:19,530 --> 00:25:24,840
patterns available to be able to use

00:25:21,800 --> 00:25:26,700
logical clock or vector clock so many

00:25:24,840 --> 00:25:28,770
stream processing systems when reading

00:25:26,700 --> 00:25:30,930
from a database they actually only use a

00:25:28,770 --> 00:25:33,210
single thread and then connected like

00:25:30,930 --> 00:25:35,820
JDBC and they query the database and

00:25:33,210 --> 00:25:37,710
create a single order of operations just

00:25:35,820 --> 00:25:39,990
which can be quite an efficient the only

00:25:37,710 --> 00:25:42,030
option the other option is to you what

00:25:39,990 --> 00:25:43,740
what's part Cassandra connector for

00:25:42,030 --> 00:25:45,450
example uses because partitions in

00:25:43,740 --> 00:25:46,830
Cassandra are independent so it can

00:25:45,450 --> 00:25:51,420
query them heavily in parallel and

00:25:46,830 --> 00:25:54,000
produce multiple independent streams so

00:25:51,420 --> 00:25:55,740
the way this applies to our use case is

00:25:54,000 --> 00:25:57,750
that we have these multiple physical

00:25:55,740 --> 00:26:01,230
streams please write a head lock but we

00:25:57,750 --> 00:26:04,490
need to merge them into a single stream

00:26:01,230 --> 00:26:07,280
where we define the order of events

00:26:04,490 --> 00:26:09,110
we can merge we can merge this one so

00:26:07,280 --> 00:26:11,120
we've been 0-4 id0 but because we're

00:26:09,110 --> 00:26:13,070
merging in parallel we cannot merge this

00:26:11,120 --> 00:26:15,110
one because it is event two and we're

00:26:13,070 --> 00:26:16,730
expecting event one so what we need to

00:26:15,110 --> 00:26:19,580
do is to maintain some sort of buffer

00:26:16,730 --> 00:26:21,559
over the last merged ID for the

00:26:19,580 --> 00:26:23,660
particular persistence ID and we just

00:26:21,559 --> 00:26:25,760
progress at this buffer and this pointer

00:26:23,660 --> 00:26:27,200
until all the events are merged and

00:26:25,760 --> 00:26:28,910
therefore we maintain the causal

00:26:27,200 --> 00:26:31,430
consistency the stronger form of

00:26:28,910 --> 00:26:33,110
consistency that we needed which is very

00:26:31,430 --> 00:26:35,840
similar to event time processing that we

00:26:33,110 --> 00:26:38,990
talked about before similarly I talked

00:26:35,840 --> 00:26:41,500
about ordering because we have this

00:26:38,990 --> 00:26:44,540
partial order per persistence ID but

00:26:41,500 --> 00:26:46,370
other events for multiple actors they

00:26:44,540 --> 00:26:49,580
don't have any any relationship at all

00:26:46,370 --> 00:26:50,870
they don't have any order but we write

00:26:49,580 --> 00:26:52,760
them into the right a headlock

00:26:50,870 --> 00:26:54,860
immediately as we can and we write them

00:26:52,760 --> 00:26:56,330
at around the same the events that are

00:26:54,860 --> 00:26:58,700
written at around the same wall work

00:26:56,330 --> 00:27:00,170
time will be merged into the final

00:26:58,700 --> 00:27:02,540
stream at around the same wall clock

00:27:00,170 --> 00:27:04,970
time it may not be exactly the right

00:27:02,540 --> 00:27:07,670
global work time but will it will be

00:27:04,970 --> 00:27:09,320
pretty much very close and if there are

00:27:07,670 --> 00:27:10,940
any inconsistencies it doesn't matter

00:27:09,320 --> 00:27:14,260
that much because there's no ordering

00:27:10,940 --> 00:27:16,610
defined so we can just create our own

00:27:14,260 --> 00:27:19,400
another concept that I not need to talk

00:27:16,610 --> 00:27:21,830
about is replay in this case because

00:27:19,400 --> 00:27:23,690
CQRS of similar pipelines only give us a

00:27:21,830 --> 00:27:24,980
ventral consistency we cannot just

00:27:23,690 --> 00:27:26,990
replay the events that are in the

00:27:24,980 --> 00:27:29,570
materialized view because there might be

00:27:26,990 --> 00:27:31,280
still some events for the same actor in

00:27:29,570 --> 00:27:32,390
the processing pipeline and therefore we

00:27:31,280 --> 00:27:34,400
wouldn't have the video all right

00:27:32,390 --> 00:27:37,010
consistency that we absolutely require

00:27:34,400 --> 00:27:39,050
so we need to finish the the read the

00:27:37,010 --> 00:27:40,550
replay by actually replying the element

00:27:39,050 --> 00:27:43,030
that were not yet merged into the

00:27:40,550 --> 00:27:45,800
multi-vise views and as well completed

00:27:43,030 --> 00:27:48,020
the reading by replying those that are

00:27:45,800 --> 00:27:49,480
still in the right ahead locks which may

00:27:48,020 --> 00:27:51,830
not seem efficient but there are many

00:27:49,480 --> 00:27:56,660
frameworks that actually do this such as

00:27:51,830 --> 00:27:58,490
eventuate another concept in in

00:27:56,660 --> 00:28:00,530
distributed environment is exactly once

00:27:58,490 --> 00:28:02,750
delivery so we would want to send

00:28:00,530 --> 00:28:04,610
messages across network to another

00:28:02,750 --> 00:28:07,490
machine so the publisher to the

00:28:04,610 --> 00:28:10,130
subscriber but the network is unreliable

00:28:07,490 --> 00:28:13,040
it can lose messages it can reorder

00:28:10,130 --> 00:28:15,440
messages similarly so most most stream

00:28:13,040 --> 00:28:17,690
processing frameworks send some sort of

00:28:15,440 --> 00:28:18,200
acknowledgments storm for example source

00:28:17,690 --> 00:28:19,790
sends

00:28:18,200 --> 00:28:23,120
acknowledgement for each element others

00:28:19,790 --> 00:28:25,370
send acknowledgement for batches of

00:28:23,120 --> 00:28:27,050
elements for efficiency but the

00:28:25,370 --> 00:28:30,590
acknowledgement may be lost as well and

00:28:27,050 --> 00:28:33,050
similarly even the machine so both the

00:28:30,590 --> 00:28:35,150
publisher and subscriber can die for

00:28:33,050 --> 00:28:38,360
unbounded amount of time so how do we

00:28:35,150 --> 00:28:39,800
achieve exactly one delivery reliable

00:28:38,360 --> 00:28:43,070
delivery of messages between the

00:28:39,800 --> 00:28:45,260
publisher and subscriber usually we need

00:28:43,070 --> 00:28:47,360
some sort of durability on both ends of

00:28:45,260 --> 00:28:49,070
the pipeline so we need durability in

00:28:47,360 --> 00:28:50,930
the publisher so we can replay the

00:28:49,070 --> 00:28:53,510
messages and many times and we need

00:28:50,930 --> 00:28:55,250
reliability in the subscriber so it can

00:28:53,510 --> 00:29:02,510
D duplicate messages it will receive the

00:28:55,250 --> 00:29:04,070
same message multiple times now this is

00:29:02,510 --> 00:29:06,410
usually handled in stream processing

00:29:04,070 --> 00:29:08,090
frameworks is they either duplicate the

00:29:06,410 --> 00:29:09,620
processing pipeline so it's they have

00:29:08,090 --> 00:29:11,180
multiple replicas and if one of them

00:29:09,620 --> 00:29:14,060
dies the other one can continue

00:29:11,180 --> 00:29:15,680
processing or they write the data they

00:29:14,060 --> 00:29:17,780
receive immediately in to write the head

00:29:15,680 --> 00:29:18,920
lock so it becomes a reliable source and

00:29:17,780 --> 00:29:22,070
therefore they can reliably

00:29:18,920 --> 00:29:24,380
retransmitted multiple times this is

00:29:22,070 --> 00:29:26,330
Apache flink an example it uses a

00:29:24,380 --> 00:29:28,790
distributed snapshotting algorithm and

00:29:26,330 --> 00:29:31,040
it assumes replayable sources so it

00:29:28,790 --> 00:29:33,740
snapshot the current progress of the

00:29:31,040 --> 00:29:35,630
reliable source and also snapshots of

00:29:33,740 --> 00:29:37,430
the current state of all state for

00:29:35,630 --> 00:29:39,020
operations and then if there is a

00:29:37,430 --> 00:29:41,000
failure it will just recover from this

00:29:39,020 --> 00:29:43,940
snapshot and replay the sources from the

00:29:41,000 --> 00:29:45,920
last known point in time and this is how

00:29:43,940 --> 00:29:47,180
this thing is implemented in Apache gear

00:29:45,920 --> 00:29:50,870
pump and other stream processing

00:29:47,180 --> 00:29:53,240
framework the API just gives you the

00:29:50,870 --> 00:29:54,800
last known global time of the last

00:29:53,240 --> 00:29:57,140
processed event and if there is a

00:29:54,800 --> 00:29:58,940
failure we can just recover from that

00:29:57,140 --> 00:30:01,760
failure by replaying the source from the

00:29:58,940 --> 00:30:04,580
last known event similarly in this is

00:30:01,760 --> 00:30:06,380
Apache spark graphical connector and

00:30:04,580 --> 00:30:08,210
similarly just when it fails it just

00:30:06,380 --> 00:30:10,070
reads the last known offset for Kafka

00:30:08,210 --> 00:30:13,580
and it replaced the events from that

00:30:10,070 --> 00:30:14,990
from that point in time in our case in

00:30:13,580 --> 00:30:16,910
our alternative architecture this is

00:30:14,990 --> 00:30:19,250
actually even simpler now because we can

00:30:16,910 --> 00:30:21,710
just simply selectively progress an

00:30:19,250 --> 00:30:23,330
offset when when merging the streams or

00:30:21,710 --> 00:30:25,940
when working with the streams if there's

00:30:23,330 --> 00:30:27,770
a failure we can simply have durably

00:30:25,940 --> 00:30:30,320
stored the offset and recover from that

00:30:27,770 --> 00:30:31,730
failure or or we don't even need to do

00:30:30,320 --> 00:30:32,050
that because if you think about it we

00:30:31,730 --> 00:30:34,090
all

00:30:32,050 --> 00:30:36,010
have variability on both sides of the

00:30:34,090 --> 00:30:37,960
pipeline so we have the right to head

00:30:36,010 --> 00:30:39,670
logs and we have on the other side we

00:30:37,960 --> 00:30:41,620
have the materialized views if there's a

00:30:39,670 --> 00:30:44,760
fairly we can potentially just compute

00:30:41,620 --> 00:30:49,840
the offset progress from these of this

00:30:44,760 --> 00:30:51,700
durably stored databases another concept

00:30:49,840 --> 00:30:54,070
commonly known and commonly used in this

00:30:51,700 --> 00:30:55,960
to be the environment are optimizations

00:30:54,070 --> 00:30:57,310
we don't need to only optimize on a

00:30:55,960 --> 00:30:59,440
single machine as we would normally

00:30:57,310 --> 00:31:01,050
working when using monolithic

00:30:59,440 --> 00:31:05,290
applications but we also need to

00:31:01,050 --> 00:31:07,780
optimize the distributive environment so

00:31:05,290 --> 00:31:09,730
such as simplifying the computation

00:31:07,780 --> 00:31:12,640
graph reducing communication of pushing

00:31:09,730 --> 00:31:14,260
operations to database optimizing

00:31:12,640 --> 00:31:16,780
partitioning improving locality of the

00:31:14,260 --> 00:31:19,030
data scaling dynamically based on load

00:31:16,780 --> 00:31:21,040
of placing operations on the nodes where

00:31:19,030 --> 00:31:23,080
they will have the best performance so

00:31:21,040 --> 00:31:25,570
some operations did more i/o or memory

00:31:23,080 --> 00:31:27,520
or CPU GPU so we can place that

00:31:25,570 --> 00:31:30,340
operation on the node that actually can

00:31:27,520 --> 00:31:32,020
do that well in this example we query a

00:31:30,340 --> 00:31:34,360
database then we do some projection

00:31:32,020 --> 00:31:36,700
filtering and then finally we do some

00:31:34,360 --> 00:31:38,890
group by so we group events that are

00:31:36,700 --> 00:31:42,070
have the same color to the same physical

00:31:38,890 --> 00:31:43,960
node potentially we can push some of

00:31:42,070 --> 00:31:46,120
these operations to a database so the

00:31:43,960 --> 00:31:47,470
database can do select where therefore

00:31:46,120 --> 00:31:50,590
we get rid of some of the processing

00:31:47,470 --> 00:31:52,450
stages and finally we can even push the

00:31:50,590 --> 00:31:53,980
group by to the database that's not

00:31:52,450 --> 00:31:55,930
always possible not all databases

00:31:53,980 --> 00:31:58,030
especially distributed databases can do

00:31:55,930 --> 00:31:59,980
guru by but if we know how the data are

00:31:58,030 --> 00:32:02,520
partitioned how they physically on the

00:31:59,980 --> 00:32:05,710
machine we can potentially achieve that

00:32:02,520 --> 00:32:08,920
this is an example of SPARC Cassandra

00:32:05,710 --> 00:32:10,990
connector so SPARC knows or asks

00:32:08,920 --> 00:32:13,060
Cassandra how about partition reduces

00:32:10,990 --> 00:32:15,420
and it can then knows how the data are

00:32:13,060 --> 00:32:19,330
partitioned in the cluster it knows

00:32:15,420 --> 00:32:20,770
which node has which a token range so it

00:32:19,330 --> 00:32:22,780
can actually use that knowledge to

00:32:20,770 --> 00:32:26,320
construct a query and query the data

00:32:22,780 --> 00:32:28,990
directly from the database a spark also

00:32:26,320 --> 00:32:30,400
knows where it's no slave physically and

00:32:28,990 --> 00:32:32,770
it would knows where Cassandra's node

00:32:30,400 --> 00:32:34,150
list so we can potentially if Cassandra

00:32:32,770 --> 00:32:36,970
is partners live on the same physical

00:32:34,150 --> 00:32:38,500
machine that spark node can query that

00:32:36,970 --> 00:32:40,780
the same Cassandra node on the same

00:32:38,500 --> 00:32:43,630
physical machine and therefore it will

00:32:40,780 --> 00:32:45,730
be more efficient and it will just

00:32:43,630 --> 00:32:48,910
optimize some of the

00:32:45,730 --> 00:32:52,750
querying across or moving data across

00:32:48,910 --> 00:32:55,450
network this is another example for

00:32:52,750 --> 00:32:57,520
mapache sparring this is from on the

00:32:55,450 --> 00:32:59,740
catalyst optimizer so it optimizes the

00:32:57,520 --> 00:33:01,120
computational graph this pushes our

00:32:59,740 --> 00:33:02,830
filters through projections so it

00:33:01,120 --> 00:33:05,080
simplifies the graph and becomes more

00:33:02,830 --> 00:33:07,360
efficient it's not part specific Apache

00:33:05,080 --> 00:33:10,380
flying Google's tensor flow and other

00:33:07,360 --> 00:33:12,610
frameworks do exactly the same thing you

00:33:10,380 --> 00:33:14,650
might have noticed I was talking a lot

00:33:12,610 --> 00:33:16,960
about streams but they were actually

00:33:14,650 --> 00:33:18,610
represented as database tables the

00:33:16,960 --> 00:33:22,440
reason is there's a lot of relationship

00:33:18,610 --> 00:33:22,440
between database tables and streams a

00:33:22,950 --> 00:33:27,970
good example is the log so log can be

00:33:26,260 --> 00:33:30,160
turned into a stream very easily a

00:33:27,970 --> 00:33:32,560
mutable table is a snapshot of a stream

00:33:30,160 --> 00:33:33,760
and and changes to a mutable table or a

00:33:32,560 --> 00:33:36,130
change log which can again be

00:33:33,760 --> 00:33:37,480
represented as a stream but we can we

00:33:36,130 --> 00:33:39,000
don't have to demonstrate that on a

00:33:37,480 --> 00:33:42,160
stream processing system we can use

00:33:39,000 --> 00:33:44,140
simply acha so again we have our cut

00:33:42,160 --> 00:33:46,030
cluster or acha sharding so multiple

00:33:44,140 --> 00:33:48,280
actors living on multiple physical

00:33:46,030 --> 00:33:52,500
machines and each actor can be addressed

00:33:48,280 --> 00:33:55,300
by by a key by its aim this actor

00:33:52,500 --> 00:33:56,860
receives messages and didn't have some

00:33:55,300 --> 00:33:59,560
transformations to its internal state

00:33:56,860 --> 00:34:02,380
and produces messages both these can be

00:33:59,560 --> 00:34:06,970
seen as a stream and the actor has a

00:34:02,380 --> 00:34:08,920
mutable state this mutable state if you

00:34:06,970 --> 00:34:10,990
think about it it's essentially just a

00:34:08,920 --> 00:34:12,910
key value store can be addressed by this

00:34:10,990 --> 00:34:14,800
actors key by its name and you can

00:34:12,910 --> 00:34:17,080
access it as a key value store which is

00:34:14,800 --> 00:34:22,240
localized to a single thread so you get

00:34:17,080 --> 00:34:24,130
our consistency for free and it's also

00:34:22,240 --> 00:34:29,290
how many stream rustic systems actually

00:34:24,130 --> 00:34:31,510
handle a state partitioned by key but

00:34:29,290 --> 00:34:33,310
this state is unreliable so we can we

00:34:31,510 --> 00:34:36,310
can however use this right a head lock a

00:34:33,310 --> 00:34:38,710
couple of systems to reliably store the

00:34:36,310 --> 00:34:41,020
changes to the mutable state and we can

00:34:38,710 --> 00:34:43,660
recover from failures and as you can as

00:34:41,020 --> 00:34:45,250
you've seen this database table this

00:34:43,660 --> 00:34:48,010
right a head lock and again be turned

00:34:45,250 --> 00:34:50,070
into a stream quite easily by using our

00:34:48,010 --> 00:34:52,620
cover systems query

00:34:50,070 --> 00:34:55,320
dr. can also store a snapshot so it does

00:34:52,620 --> 00:34:57,870
not have them to replay all the messages

00:34:55,320 --> 00:35:00,090
in the log but it uses the snapshot and

00:34:57,870 --> 00:35:01,830
then replace all messages that happened

00:35:00,090 --> 00:35:03,630
after the snapshot which we have also

00:35:01,830 --> 00:35:07,560
discussed previously in the change data

00:35:03,630 --> 00:35:09,540
capture and actors or akka can also have

00:35:07,560 --> 00:35:13,470
a distributed state a global state which

00:35:09,540 --> 00:35:16,920
can either be a an event bus event event

00:35:13,470 --> 00:35:18,780
bus or CR DTS or distributed snapshot

00:35:16,920 --> 00:35:25,440
which again can be represented as both a

00:35:18,780 --> 00:35:27,300
table or a stream as Martin Flatman and

00:35:25,440 --> 00:35:30,360
and others correctly point out there are

00:35:27,300 --> 00:35:32,700
many use cases for infinite streams of

00:35:30,360 --> 00:35:34,740
data so assume we have this original

00:35:32,700 --> 00:35:37,140
system it can be a database table or any

00:35:34,740 --> 00:35:39,660
system and if it emits a stream of all

00:35:37,140 --> 00:35:41,700
events or all changes to its state

00:35:39,660 --> 00:35:43,860
anyone else can then subscribe to the

00:35:41,700 --> 00:35:45,150
stream and they can be updated all the

00:35:43,860 --> 00:35:47,910
time they don't have to query for the

00:35:45,150 --> 00:35:50,580
data they don't have to send a request

00:35:47,910 --> 00:35:52,950
they will always be kept updated with

00:35:50,580 --> 00:35:54,780
the latest changes to its state this is

00:35:52,950 --> 00:35:57,390
very useful we can reconstruct the

00:35:54,780 --> 00:36:01,880
original table in a different way such

00:35:57,390 --> 00:36:04,650
as a cache or search index or a

00:36:01,880 --> 00:36:06,510
materialized view or we can just consume

00:36:04,650 --> 00:36:10,170
it in any microt service or any system

00:36:06,510 --> 00:36:15,920
and be kept up to date all the time this

00:36:10,170 --> 00:36:18,810
is used for example in so-called

00:36:15,920 --> 00:36:20,160
streaming data pipeline where we have

00:36:18,810 --> 00:36:22,050
something like Kafka in the middle

00:36:20,160 --> 00:36:24,560
something that can handle large-scale

00:36:22,050 --> 00:36:24,560
streams

00:36:31,100 --> 00:36:36,110
the stream then can be queried they can

00:36:33,020 --> 00:36:38,120
be replayed and anyone who's interested

00:36:36,110 --> 00:36:39,830
can publish their data into this into

00:36:38,120 --> 00:36:44,330
the streaming data platform so any

00:36:39,830 --> 00:36:46,310
social media devices any any services or

00:36:44,330 --> 00:36:48,260
micro services can just publish the

00:36:46,310 --> 00:36:50,990
stream of events into this streaming

00:36:48,260 --> 00:36:52,820
data pipeline and then anyone on the

00:36:50,990 --> 00:36:55,460
other side who's interested in the dead

00:36:52,820 --> 00:36:57,380
data can query or be updated can

00:36:55,460 --> 00:36:59,390
subscribe to the screen stream and be

00:36:57,380 --> 00:37:01,730
updated all the time immediately without

00:36:59,390 --> 00:37:03,700
having to query for the data so we can

00:37:01,730 --> 00:37:05,870
have again something like search engines

00:37:03,700 --> 00:37:08,270
applications services anyone who's

00:37:05,870 --> 00:37:09,590
interested and just need some some

00:37:08,270 --> 00:37:11,120
common interface such as you know

00:37:09,590 --> 00:37:14,000
serialization format it can handle

00:37:11,120 --> 00:37:15,560
versioning but then the teams can work

00:37:14,000 --> 00:37:17,990
completely independently on each other

00:37:15,560 --> 00:37:19,490
and it gives us all the real all the

00:37:17,990 --> 00:37:21,920
guarantees that we that we discussed

00:37:19,490 --> 00:37:23,270
previously so it gives us the

00:37:21,920 --> 00:37:25,370
consistency guarantees that we need

00:37:23,270 --> 00:37:26,630
exactly once delivery fault-tolerance

00:37:25,370 --> 00:37:28,820
because these are all distributed

00:37:26,630 --> 00:37:31,280
systems we can use things like load

00:37:28,820 --> 00:37:33,440
balancing so create multiple replicas of

00:37:31,280 --> 00:37:37,100
the same service and all these things

00:37:33,440 --> 00:37:38,330
can be achieved if we use or if we build

00:37:37,100 --> 00:37:40,880
to the guarantees that we discussed

00:37:38,330 --> 00:37:45,530
previously this will work very very

00:37:40,880 --> 00:37:47,960
efficiently but the most common use case

00:37:45,530 --> 00:37:49,970
of off streams are just general

00:37:47,960 --> 00:37:52,040
distributed systems so we have multiple

00:37:49,970 --> 00:37:54,890
micro services each micro service has

00:37:52,040 --> 00:37:56,660
its own database depending on what what

00:37:54,890 --> 00:38:01,150
consistency it needs so it can use an

00:37:56,660 --> 00:38:03,470
SQL database no SQL database or CQRS and

00:38:01,150 --> 00:38:05,390
the services communicate with each other

00:38:03,470 --> 00:38:07,070
by sending messages they don't do a

00:38:05,390 --> 00:38:08,780
request response they just send message

00:38:07,070 --> 00:38:10,580
whenever needed and they will receive

00:38:08,780 --> 00:38:12,800
message and they should be handle by

00:38:10,580 --> 00:38:17,150
able to handle that message at anytime

00:38:12,800 --> 00:38:18,980
in the future and again if we achieve

00:38:17,150 --> 00:38:20,360
the same guarantees that we discussed in

00:38:18,980 --> 00:38:22,010
the stream processing pipeline

00:38:20,360 --> 00:38:25,430
it's just consistency exactly once

00:38:22,010 --> 00:38:27,530
delivery durability our fault tolerance

00:38:25,430 --> 00:38:30,080
and all these then we can have a very

00:38:27,530 --> 00:38:31,580
reliable and very efficient distributed

00:38:30,080 --> 00:38:36,860
system or general micro service

00:38:31,580 --> 00:38:39,350
architecture so you might have noticed

00:38:36,860 --> 00:38:42,740
this was not really a the most common

00:38:39,350 --> 00:38:44,480
example of distributed system or micro

00:38:42,740 --> 00:38:45,050
service architecture because because it

00:38:44,480 --> 00:38:47,420
wasn't

00:38:45,050 --> 00:38:49,670
one a very specific one it was an

00:38:47,420 --> 00:38:51,560
example of Google's tensor flow using a

00:38:49,670 --> 00:38:54,740
building asynchronous data and model

00:38:51,560 --> 00:38:55,970
parallelism for machine learning model

00:38:54,740 --> 00:38:59,000
training if you're not familiar with

00:38:55,970 --> 00:39:00,380
tensor flow it's Google's framework for

00:38:59,000 --> 00:39:05,120
large scale machine learning goal

00:39:00,380 --> 00:39:06,800
large-scale data processing it works in

00:39:05,120 --> 00:39:08,870
distributed environments so it can have

00:39:06,800 --> 00:39:11,930
multiple machines each machine can have

00:39:08,870 --> 00:39:14,350
multiple devices device is a CPU or GPU

00:39:11,930 --> 00:39:16,370
and then Google tensorflow

00:39:14,350 --> 00:39:17,960
describes the computation as a

00:39:16,370 --> 00:39:20,420
computational graph a directed acyclic

00:39:17,960 --> 00:39:23,600
graph and then each node of that graph

00:39:20,420 --> 00:39:24,830
is assigned to one of the devices they

00:39:23,600 --> 00:39:28,450
sometimes have different requirements

00:39:24,830 --> 00:39:31,430
than micro-service architectures such as

00:39:28,450 --> 00:39:34,280
they have reduced sometimes consistency

00:39:31,430 --> 00:39:35,720
requirements because many of the machine

00:39:34,280 --> 00:39:38,200
learning algorithms can handle

00:39:35,720 --> 00:39:40,210
randomness or noise quite efficiently

00:39:38,200 --> 00:39:43,250
sometimes they don't want to process

00:39:40,210 --> 00:39:46,070
each message individually because they

00:39:43,250 --> 00:39:49,280
build a batch because they can eat they

00:39:46,070 --> 00:39:51,110
can use glass or a GPU computation on

00:39:49,280 --> 00:39:53,090
that batch but other than that they have

00:39:51,110 --> 00:39:55,070
exactly the same guarantee so they need

00:39:53,090 --> 00:39:57,110
to be able to send messages to each

00:39:55,070 --> 00:39:58,760
other across network so they need the

00:39:57,110 --> 00:40:00,770
reliable delivery they need exactly

00:39:58,760 --> 00:40:02,360
wants delivery and they need photo

00:40:00,770 --> 00:40:03,830
around because each of these machines

00:40:02,360 --> 00:40:06,980
can die at any time

00:40:03,830 --> 00:40:09,830
they need they definitely need

00:40:06,980 --> 00:40:12,140
optimizations because this process is

00:40:09,830 --> 00:40:14,330
run many many times in many iterations

00:40:12,140 --> 00:40:18,080
to be efficient it needs to be very very

00:40:14,330 --> 00:40:20,000
well optimized so it has exactly the

00:40:18,080 --> 00:40:22,310
same guarantees they're not using a cow

00:40:20,000 --> 00:40:23,570
or any other synchronous messaging

00:40:22,310 --> 00:40:26,210
system they're using their own

00:40:23,570 --> 00:40:30,170
technologies like Google G RPC but the

00:40:26,210 --> 00:40:33,110
concepts are exactly the same so this

00:40:30,170 --> 00:40:35,210
may all look really appealing really

00:40:33,110 --> 00:40:36,830
really interesting but there are still

00:40:35,210 --> 00:40:38,300
some challenges that have to be solved

00:40:36,830 --> 00:40:39,950
when we were building such system and

00:40:38,300 --> 00:40:41,960
especially in distributed environment

00:40:39,950 --> 00:40:44,090
such as exactly ones delivery

00:40:41,960 --> 00:40:46,040
consistency availability photo events

00:40:44,090 --> 00:40:48,260
across service invariants therefore

00:40:46,040 --> 00:40:50,110
distributed transactions automated

00:40:48,260 --> 00:40:52,460
deployment and configuration management

00:40:50,110 --> 00:40:54,530
automated elasticity's of the ability to

00:40:52,460 --> 00:40:56,170
scale up and down and how you handle

00:40:54,530 --> 00:40:58,880
failures without downtime

00:40:56,170 --> 00:41:01,280
upgrades of versions of our service

00:40:58,880 --> 00:41:03,260
with no downtime ideally graceful

00:41:01,280 --> 00:41:04,610
shutdown of services so not losing data

00:41:03,260 --> 00:41:07,640
our messages when shutting down a

00:41:04,610 --> 00:41:09,530
service distributed system verification

00:41:07,640 --> 00:41:11,480
logging and traceability is very

00:41:09,530 --> 00:41:14,780
important and it's not usually grounded

00:41:11,480 --> 00:41:17,210
or split brain scenarios all these are

00:41:14,780 --> 00:41:19,250
often taken as granted this has solve

00:41:17,210 --> 00:41:20,930
problems but you really should focus on

00:41:19,250 --> 00:41:22,370
these and verify that all these are

00:41:20,930 --> 00:41:25,760
correct especially in distributed

00:41:22,370 --> 00:41:28,520
environment so in conclusion we moved

00:41:25,760 --> 00:41:31,250
from request and response mutable state

00:41:28,520 --> 00:41:34,490
and a synchronous messaging request

00:41:31,250 --> 00:41:36,980
response type of messaging to streams

00:41:34,490 --> 00:41:40,070
asynchronous messaging and most

00:41:36,980 --> 00:41:41,720
importantly we have to treat the

00:41:40,070 --> 00:41:43,760
production and distributed systems as

00:41:41,720 --> 00:41:44,920
production ready we need to verify them

00:41:43,760 --> 00:41:46,700
and we definitely need to treat

00:41:44,920 --> 00:41:50,600
distributed systems as first-class

00:41:46,700 --> 00:41:54,110
citizens thank you very much any any

00:41:50,600 --> 00:41:57,290
questions well so the question was

00:41:54,110 --> 00:42:00,440
whether we can write into Cassandra and

00:41:57,290 --> 00:42:03,320
Kafka at the same time when for example

00:42:00,440 --> 00:42:05,360
in our persistence unfortunately no not

00:42:03,320 --> 00:42:06,890
at the moment and you still couldn't

00:42:05,360 --> 00:42:09,050
write into both because you would have

00:42:06,890 --> 00:42:11,210
to do all right scenario so the only way

00:42:09,050 --> 00:42:13,250
is to write it consistently with strong

00:42:11,210 --> 00:42:15,260
consistency into one of one of these and

00:42:13,250 --> 00:42:17,240
then read it from there and I'll

00:42:15,260 --> 00:42:21,550
continue the pipeline similarly to what

00:42:17,240 --> 00:42:21,550

YouTube URL: https://www.youtube.com/watch?v=K4FY0XKediU


