Title: Fast, Simple Concurrency with Scala Native - Richard Whaling
Publication date: 2019-07-11
Playlist: Scala Days Lausanne 2019
Description: 
	This video was recorded at Scala Days Lausanne 2019
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

More information and the abstract can be found here:
https://scaladays.org/schedule/fast-simple-concurrency-with-scala-native
Captions: 
	00:00:00,030 --> 00:00:05,310
cool let's let's kick this off so thanks

00:00:03,540 --> 00:00:07,620
for coming to my talk fast simple

00:00:05,310 --> 00:00:10,110
concurrency with Scala native Scala days

00:00:07,620 --> 00:00:11,639
is extra amazing this year and it's a

00:00:10,110 --> 00:00:14,639
it's an honor to be here with all of

00:00:11,639 --> 00:00:18,060
y'all so this is a talk about bare-metal

00:00:14,639 --> 00:00:19,830
concurrency on Scala native and what

00:00:18,060 --> 00:00:22,439
that really means to me it gets to the

00:00:19,830 --> 00:00:24,449
heart of what the native platform is and

00:00:22,439 --> 00:00:28,830
what makes it different from Scala on

00:00:24,449 --> 00:00:31,019
the JVM or even Scala and JavaScript but

00:00:28,830 --> 00:00:33,270
because this is a platform where you you

00:00:31,019 --> 00:00:35,460
start off with less it's to me it's

00:00:33,270 --> 00:00:36,930
really about Scala as a platform itself

00:00:35,460 --> 00:00:41,340
if you're doing Scala

00:00:36,930 --> 00:00:43,079
all the way down to the OS level you you

00:00:41,340 --> 00:00:44,879
have nothing but Scala in your stack

00:00:43,079 --> 00:00:48,300
which can be intimidating but also be

00:00:44,879 --> 00:00:50,129
really exciting and to me it's

00:00:48,300 --> 00:00:52,050
especially about sustainable libraries

00:00:50,129 --> 00:00:54,360
and communities how do we build enough

00:00:52,050 --> 00:00:57,510
functionality on this new platform to

00:00:54,360 --> 00:00:58,739
sort of bootstrap a workable ecosystem

00:00:57,510 --> 00:01:00,180
where you can start using this in

00:00:58,739 --> 00:01:02,160
production

00:01:00,180 --> 00:01:04,769
concretely though this is a talk about

00:01:02,160 --> 00:01:07,950
native loop which is the new concurrency

00:01:04,769 --> 00:01:10,590
library for Scala native ODOT for it's

00:01:07,950 --> 00:01:14,340
an extensible event loop and i/o system

00:01:10,590 --> 00:01:15,570
it's backed by libuv a sea library that

00:01:14,340 --> 00:01:18,509
provides an event loop that I'll talk a

00:01:15,570 --> 00:01:20,700
lot more about shortly and it works

00:01:18,509 --> 00:01:23,970
great with other C libraries things

00:01:20,700 --> 00:01:26,880
you've heard of like Lib curl and really

00:01:23,970 --> 00:01:28,710
easy to extend and it's here on github

00:01:26,880 --> 00:01:30,990
it lives in the main Scala native

00:01:28,710 --> 00:01:32,610
organization but it's not merged into

00:01:30,990 --> 00:01:34,710
the court yet it's still provided as a

00:01:32,610 --> 00:01:36,299
library and with the techniques I'm

00:01:34,710 --> 00:01:39,390
showing you here anyone could build

00:01:36,299 --> 00:01:40,799
their own concurrency system if they

00:01:39,390 --> 00:01:42,960
wanted to which is one of the things

00:01:40,799 --> 00:01:45,990
that's most exciting about Scala native

00:01:42,960 --> 00:01:47,939
for me anyway so what what's in this

00:01:45,990 --> 00:01:50,040
talk we're gonna start with like a slide

00:01:47,939 --> 00:01:52,020
or two about concurrency and Scala just

00:01:50,040 --> 00:01:54,060
for background we're gonna talk about

00:01:52,020 --> 00:01:56,579
Scala native on background and then

00:01:54,060 --> 00:01:58,950
libuv then we're gonna do an overview of

00:01:56,579 --> 00:02:00,390
the high-level API and then we're

00:01:58,950 --> 00:02:03,899
actually going to do a deep dive into

00:02:00,390 --> 00:02:05,549
the execution context itself and then

00:02:03,899 --> 00:02:09,619
conclude by just talking about where we

00:02:05,549 --> 00:02:12,080
go so about me I'm Richard Whalen

00:02:09,619 --> 00:02:14,330
I've been writing scala for about 10

00:02:12,080 --> 00:02:17,300
years or something but a full time more

00:02:14,330 --> 00:02:19,610
like 4 or 5 I'm a data engineer at m1

00:02:17,300 --> 00:02:21,319
finance in Chicago if you haven't heard

00:02:19,610 --> 00:02:23,150
of us we're a fem tech startup we

00:02:21,319 --> 00:02:25,970
provide banking and brokerage services

00:02:23,150 --> 00:02:28,130
we have about a hundred thousand happy

00:02:25,970 --> 00:02:30,440
users and manage about half a billion

00:02:28,130 --> 00:02:32,239
dollars in assets we're hiring back in

00:02:30,440 --> 00:02:34,250
Scala engineers in the US and if you

00:02:32,239 --> 00:02:35,540
want to hear more about it just come up

00:02:34,250 --> 00:02:37,489
and talk to me afterwards I would love

00:02:35,540 --> 00:02:40,310
to tell you about it I'm also the author

00:02:37,489 --> 00:02:43,400
of this book modern systems programming

00:02:40,310 --> 00:02:47,030
with Scala native which is available in

00:02:43,400 --> 00:02:48,980
digital preview now from pragmatic here

00:02:47,030 --> 00:02:52,730
and not only that I actually have a

00:02:48,980 --> 00:02:54,680
discount code right here for it and also

00:02:52,730 --> 00:02:56,660
if you you buy the beta release you get

00:02:54,680 --> 00:02:58,880
the full book now all ten chapters are

00:02:56,660 --> 00:03:01,220
there and probably around the end of the

00:02:58,880 --> 00:03:04,040
summer when I finish the book and revise

00:03:01,220 --> 00:03:05,599
it for ode up four you'll get you'll get

00:03:04,040 --> 00:03:07,250
all the updates to your eCopy and you'll

00:03:05,599 --> 00:03:10,130
get a huge discount on buying a paper

00:03:07,250 --> 00:03:13,040
copy and I'll tweet tweet this out and

00:03:10,130 --> 00:03:14,380
show the code again at the end but yeah

00:03:13,040 --> 00:03:17,060
let's get started

00:03:14,380 --> 00:03:20,000
so yeah let's talk about concurrency

00:03:17,060 --> 00:03:21,829
really quickly so I'll just throw out

00:03:20,000 --> 00:03:22,430
some definitions programs that do one

00:03:21,829 --> 00:03:24,650
thing at a time

00:03:22,430 --> 00:03:26,329
we'll call synchronous and programs that

00:03:24,650 --> 00:03:28,150
do more than one thing at a time we can

00:03:26,329 --> 00:03:29,959
call asynchronous or concurrent

00:03:28,150 --> 00:03:32,150
synchronous programs are generally

00:03:29,959 --> 00:03:34,489
easier to write but they do bottleneck

00:03:32,150 --> 00:03:36,769
or block on certain workloads

00:03:34,489 --> 00:03:38,750
whereas concurrent programs can perform

00:03:36,769 --> 00:03:42,170
better but also can be fiendishly

00:03:38,750 --> 00:03:45,620
complex and buggy I'd argue Java and C++

00:03:42,170 --> 00:03:48,470
more or less got it wrong and some

00:03:45,620 --> 00:03:50,420
JavaScript probably got it right and in

00:03:48,470 --> 00:03:51,579
some sense solid they certainly continue

00:03:50,420 --> 00:03:56,510
to evolve as do we all

00:03:51,579 --> 00:03:58,190
Scala of course famously has made huge

00:03:56,510 --> 00:04:01,190
advances in concurrency for the whole

00:03:58,190 --> 00:04:03,410
lifetime of Scala really the heart of it

00:04:01,190 --> 00:04:05,930
really is the the Scala duck concurrent

00:04:03,410 --> 00:04:08,120
API and in particular the the future an

00:04:05,930 --> 00:04:10,060
execution context it provides it lets

00:04:08,120 --> 00:04:13,370
you do things more or less like this

00:04:10,060 --> 00:04:15,669
we're a future passes the the

00:04:13,370 --> 00:04:17,900
asynchronous result of a computation

00:04:15,669 --> 00:04:19,430
ideally there's no mutable and state

00:04:17,900 --> 00:04:21,500
state involved everything is passed

00:04:19,430 --> 00:04:23,060
around in these futures and you have

00:04:21,500 --> 00:04:24,590
this implicit execution

00:04:23,060 --> 00:04:26,750
texe the lets you abstract over

00:04:24,590 --> 00:04:28,070
different backends that can be a thread

00:04:26,750 --> 00:04:30,230
pool that could be a dedicated

00:04:28,070 --> 00:04:32,660
background thread that could be on your

00:04:30,230 --> 00:04:35,480
main thread and this this abstraction

00:04:32,660 --> 00:04:37,610
over the exact mechanism of execution is

00:04:35,480 --> 00:04:39,919
something that I still think makes makes

00:04:37,610 --> 00:04:41,780
Scala really special it's things that go

00:04:39,919 --> 00:04:44,840
and JavaScript don't let you do right

00:04:41,780 --> 00:04:47,530
and it's the sort of modularity is what

00:04:44,840 --> 00:04:50,650
makes this whole talk even possible that

00:04:47,530 --> 00:04:53,150
said concurrency is still difficult

00:04:50,650 --> 00:04:55,250
right if you do run blocking code it's

00:04:53,150 --> 00:04:57,979
really easy to starve the the execution

00:04:55,250 --> 00:04:59,630
context of threads accidentally it's

00:04:57,979 --> 00:05:01,550
easily to run into race conditions if

00:04:59,630 --> 00:05:03,650
you do have mutable state I've certainly

00:05:01,550 --> 00:05:07,419
accidentally closed over cinder and an

00:05:03,650 --> 00:05:10,040
actor of probably dozens of times and

00:05:07,419 --> 00:05:12,590
people try to solve this by introducing

00:05:10,040 --> 00:05:14,000
higher-level models actors streaming a

00:05:12,590 --> 00:05:16,729
lot of the newer stuff people are

00:05:14,000 --> 00:05:19,460
talking about here this conference but

00:05:16,729 --> 00:05:22,010
we problems do persist it's still hard

00:05:19,460 --> 00:05:24,260
right and I'm gonna argue that the the

00:05:22,010 --> 00:05:27,560
JVM is threading a memory model is sort

00:05:24,260 --> 00:05:31,160
of fundamentally hostile to closures in

00:05:27,560 --> 00:05:34,220
general right and to some extent that's

00:05:31,160 --> 00:05:35,990
- Scala and I but I don't think it's

00:05:34,220 --> 00:05:38,510
controversial to just point out that a

00:05:35,990 --> 00:05:40,789
huge proportion of JVM libraries will

00:05:38,510 --> 00:05:42,890
happily block your threads and there's

00:05:40,789 --> 00:05:44,810
good reasons not to rely on those I

00:05:42,890 --> 00:05:47,120
thought Rob Norris is talk yesterday

00:05:44,810 --> 00:05:47,840
made this point much more strongly than

00:05:47,120 --> 00:05:49,460
I ever could

00:05:47,840 --> 00:05:52,820
highly recommend that if you didn't get

00:05:49,460 --> 00:05:54,919
to see it so the design questions for

00:05:52,820 --> 00:05:56,750
for us going into like what we want

00:05:54,919 --> 00:05:59,990
concurrency to look like on Scala native

00:05:56,750 --> 00:06:01,639
you know we can't we can't piggyback on

00:05:59,990 --> 00:06:04,190
what worked for the JVM but we also

00:06:01,639 --> 00:06:07,250
aren't constrained by it we can we can

00:06:04,190 --> 00:06:09,560
start over from scratch so the question

00:06:07,250 --> 00:06:11,780
for me is whether Scala native can

00:06:09,560 --> 00:06:14,090
provide a model of i/o and concurrency

00:06:11,780 --> 00:06:16,190
that's true - Scala but I kind of hope

00:06:14,090 --> 00:06:18,710
that it could provide a model of i/o a

00:06:16,190 --> 00:06:21,740
concurrency that's that's more true to

00:06:18,710 --> 00:06:24,050
Scala than the JVM ever was that Scala

00:06:21,740 --> 00:06:27,380
without the Java isms can can be more

00:06:24,050 --> 00:06:29,139
Scala so let's let's talk about Scala

00:06:27,380 --> 00:06:32,810
NATO for a bit before we go any deeper

00:06:29,139 --> 00:06:35,570
so Scotland ativ is is Scala it's not a

00:06:32,810 --> 00:06:36,860
fork it's a compiler plug-in just like

00:06:35,570 --> 00:06:39,500
Scala jeaious

00:06:36,860 --> 00:06:42,350
instead of targeting JVM bytecode it

00:06:39,500 --> 00:06:47,540
targets the LLVM compiler back-end same

00:06:42,350 --> 00:06:51,200
compiler framework the rust clang use so

00:06:47,540 --> 00:06:53,270
it produces basically compact optimized

00:06:51,200 --> 00:06:55,610
native binaries with a small footprint

00:06:53,270 --> 00:06:57,710
and a low memory overhead you don't have

00:06:55,610 --> 00:07:00,650
a JVM but you do have pretty good

00:06:57,710 --> 00:07:02,870
coverage of JDK classes because we've

00:07:00,650 --> 00:07:06,050
reimplemented a good portion of the jdk

00:07:02,870 --> 00:07:09,680
and pure scala right so ordinary Scala

00:07:06,050 --> 00:07:11,510
code for the most part just works but

00:07:09,680 --> 00:07:12,680
you get other cool things with Scala

00:07:11,510 --> 00:07:15,440
native to that I think are really

00:07:12,680 --> 00:07:17,210
special you get full control over memory

00:07:15,440 --> 00:07:19,610
allocation like you'd have in a C

00:07:17,210 --> 00:07:22,010
program you get struct an array of

00:07:19,610 --> 00:07:24,740
primitives for off heap memory like

00:07:22,010 --> 00:07:26,600
you'd have in a C program you get CF fi

00:07:24,740 --> 00:07:29,360
this really a foreign function interface

00:07:26,600 --> 00:07:31,820
the ability to call into C libraries and

00:07:29,360 --> 00:07:34,160
also to pass your own code into them I

00:07:31,820 --> 00:07:35,870
started my career as a C programmer and

00:07:34,160 --> 00:07:38,540
I've been doing CF fi from a lot of

00:07:35,870 --> 00:07:40,520
languages for a long time and Scala

00:07:38,540 --> 00:07:41,990
Natives is the the best I've ever seen I

00:07:40,520 --> 00:07:44,990
really fell in love with it the first

00:07:41,990 --> 00:07:48,740
time I I started using them all of this

00:07:44,990 --> 00:07:51,560
adds up to basically an unembodied cell

00:07:48,740 --> 00:07:53,660
with the capabilities of C with all the

00:07:51,560 --> 00:07:55,880
power and danger that go with that and

00:07:53,660 --> 00:07:58,820
to me that's immensely exciting and

00:07:55,880 --> 00:08:01,160
powerful so that also comes with a

00:07:58,820 --> 00:08:03,410
warning right this is very low-level

00:08:01,160 --> 00:08:06,350
computing it's powerful but it's also

00:08:03,410 --> 00:08:09,290
dangerous you don't need any unsafe

00:08:06,350 --> 00:08:11,300
functionality to use Scala native and

00:08:09,290 --> 00:08:12,950
sort of the best practice as that we've

00:08:11,300 --> 00:08:15,260
settled on as a community over the last

00:08:12,950 --> 00:08:17,750
two years is that you want what we want

00:08:15,260 --> 00:08:20,840
to provide our safe idiomatic Scala

00:08:17,750 --> 00:08:24,890
api's on top of the the low-level code

00:08:20,840 --> 00:08:26,960
so really library authors and sort of

00:08:24,890 --> 00:08:28,220
language maintainer so really the people

00:08:26,960 --> 00:08:30,920
who should have to worry about the

00:08:28,220 --> 00:08:34,820
unsafe stuff not end users who should

00:08:30,920 --> 00:08:37,460
really just see ordinary Scala but what

00:08:34,820 --> 00:08:39,890
even is idiomatic Scala when you really

00:08:37,460 --> 00:08:43,160
open this up right Scala native is is

00:08:39,890 --> 00:08:44,300
single threaded and the the JDK isn't

00:08:43,160 --> 00:08:46,430
complete there are things that are

00:08:44,300 --> 00:08:48,070
missing like reflection like runtime

00:08:46,430 --> 00:08:51,680
class loading

00:08:48,070 --> 00:08:54,500
so you you fill in the gaps by by using

00:08:51,680 --> 00:08:56,600
the CF fi they're awesome C libraries

00:08:54,500 --> 00:08:58,700
for everything under the Sun just about

00:08:56,600 --> 00:09:03,080
everything the JVM can do and a lot of

00:08:58,700 --> 00:09:04,910
cool things the JVM can't do so then the

00:09:03,080 --> 00:09:07,370
question is do we pick these off one at

00:09:04,910 --> 00:09:08,570
a time or do we try to make a bolder

00:09:07,370 --> 00:09:10,460
move to provide the essential

00:09:08,570 --> 00:09:14,210
capabilities that we need to write a

00:09:10,460 --> 00:09:15,770
productive Scala code and from there I

00:09:14,210 --> 00:09:17,960
think originally about two years ago I

00:09:15,770 --> 00:09:20,450
just started experimenting with a C

00:09:17,960 --> 00:09:22,400
library called libuv this is a

00:09:20,450 --> 00:09:23,900
cross-platform C library that provides

00:09:22,400 --> 00:09:26,810
an event loop

00:09:23,900 --> 00:09:28,010
it's famously used by nodejs and it was

00:09:26,810 --> 00:09:30,710
originally spun out of the node.js

00:09:28,010 --> 00:09:33,710
project but it's also used by by julia

00:09:30,710 --> 00:09:36,589
by nee ovum and as a library and Ruby

00:09:33,710 --> 00:09:39,350
Python Perl 6 all kinds of things

00:09:36,589 --> 00:09:42,860
it has great non-blocking i/o

00:09:39,350 --> 00:09:46,460
capabilities and it supports Windows Mac

00:09:42,860 --> 00:09:49,160
and bsd which is really great so it's

00:09:46,460 --> 00:09:51,680
it's a I think just off the looking at

00:09:49,160 --> 00:09:55,100
the label on the can it's a great fit

00:09:51,680 --> 00:09:57,830
for Scala native so what it does is that

00:09:55,100 --> 00:10:00,920
it abstracts both over different kinds

00:09:57,830 --> 00:10:03,589
of i/o and different operating system

00:10:00,920 --> 00:10:06,200
mechanisms that underlie it like Linux

00:10:03,589 --> 00:10:08,240
famously has this Ipoh mechanism for a

00:10:06,200 --> 00:10:12,260
single that's quite fast but quite

00:10:08,240 --> 00:10:15,680
painful BSD has KQ Windows has IO

00:10:12,260 --> 00:10:18,350
completion ports but even then like in

00:10:15,680 --> 00:10:20,540
Linux eople works on TCP sockets and

00:10:18,350 --> 00:10:24,170
pipes but it actually doesn't work on

00:10:20,540 --> 00:10:26,240
file IO and most file systems file IO is

00:10:24,170 --> 00:10:29,690
always blocking in Linux which is kind

00:10:26,240 --> 00:10:32,900
of fiendish so what libuv does is it

00:10:29,690 --> 00:10:34,370
actually has a thread pool separate from

00:10:32,900 --> 00:10:37,220
the main single threaded event loop

00:10:34,370 --> 00:10:40,100
where it will run any blocking system

00:10:37,220 --> 00:10:42,470
calls for things like file i/o for

00:10:40,100 --> 00:10:46,010
things like DNS and in theory it can

00:10:42,470 --> 00:10:47,990
also run user code also actually so you

00:10:46,010 --> 00:10:50,510
even get that thread pool functionality

00:10:47,990 --> 00:10:52,880
for free and it provides all of this

00:10:50,510 --> 00:10:55,190
with a really consistent API where any

00:10:52,880 --> 00:10:58,400
resource you can do something - whether

00:10:55,190 --> 00:11:00,050
it's whether it's an i/o resource or

00:10:58,400 --> 00:11:01,430
just sort of a life cycle hook is

00:11:00,050 --> 00:11:03,920
represented by a handle

00:11:01,430 --> 00:11:07,100
and then you just pass C function into

00:11:03,920 --> 00:11:08,810
the C library calls to use a call back

00:11:07,100 --> 00:11:11,720
when when different events happen

00:11:08,810 --> 00:11:13,430
essentially so the way that works out

00:11:11,720 --> 00:11:14,960
with Scala native is that our Scala

00:11:13,430 --> 00:11:19,550
native code will just run on a single

00:11:14,960 --> 00:11:21,640
thread we'll be able to use a pole etc

00:11:19,550 --> 00:11:23,630
for high-performance i/o on whatever

00:11:21,640 --> 00:11:25,430
operating system we're using that's

00:11:23,630 --> 00:11:28,670
totally abstracted for us we get this

00:11:25,430 --> 00:11:31,180
pretty good callback api and we get sort

00:11:28,670 --> 00:11:33,950
of production grade performance for free

00:11:31,180 --> 00:11:36,050
and also a lot of really good patterns

00:11:33,950 --> 00:11:38,660
for extending this with other c

00:11:36,050 --> 00:11:41,240
libraries with async capabilities i've

00:11:38,660 --> 00:11:43,370
done curl so far but there's even like

00:11:41,240 --> 00:11:45,530
sample code out there for forgetting

00:11:43,370 --> 00:11:49,400
like Postgres clients and Redis clients

00:11:45,530 --> 00:11:51,140
and other things integrated but the the

00:11:49,400 --> 00:11:53,750
big question is one of design and

00:11:51,140 --> 00:11:57,610
whether if we're using this sort of

00:11:53,750 --> 00:12:01,000
low-level nodejs library if we can avoid

00:11:57,610 --> 00:12:03,320
the the callback hell or pyramid of Doom

00:12:01,000 --> 00:12:06,140
anti-pattern that javascript code had

00:12:03,320 --> 00:12:08,420
you know five ten years ago so yeah

00:12:06,140 --> 00:12:10,700
let's just actually talk about where the

00:12:08,420 --> 00:12:12,590
API is though and then we'll cut back to

00:12:10,700 --> 00:12:15,620
actually how we implement something

00:12:12,590 --> 00:12:18,830
that's that's more idiomatic so first of

00:12:15,620 --> 00:12:22,700
all we're going to have a real execution

00:12:18,830 --> 00:12:25,400
context and real futures I I'm not

00:12:22,700 --> 00:12:27,020
comfortable with a node.js style API and

00:12:25,400 --> 00:12:29,930
Scala we're gonna have pretty

00:12:27,020 --> 00:12:31,790
comprehensive i/o capabilities get get

00:12:29,930 --> 00:12:34,910
all the basics handled we're gonna

00:12:31,790 --> 00:12:37,160
provide an HTTP and HTTPS client server

00:12:34,910 --> 00:12:39,890
and honestly those are more important to

00:12:37,160 --> 00:12:43,370
me than file IO right when is less time

00:12:39,890 --> 00:12:45,320
I wrote a wrote output to a file from a

00:12:43,370 --> 00:12:47,330
scholar program I don't even know I

00:12:45,320 --> 00:12:50,120
think they're really like the the price

00:12:47,330 --> 00:12:52,430
of entry nowadays but it's to provide

00:12:50,120 --> 00:12:54,170
all of this with a sense of minimalism

00:12:52,430 --> 00:12:55,910
and sustainability right the

00:12:54,170 --> 00:12:58,580
functionality we're talking about here

00:12:55,910 --> 00:13:00,800
is potentially enormous and just trying

00:12:58,580 --> 00:13:03,830
to get something that works and gets us

00:13:00,800 --> 00:13:05,780
sort of useful production grade programs

00:13:03,830 --> 00:13:08,990
as quickly as possible is the most

00:13:05,780 --> 00:13:11,630
important thing here the medical for me

00:13:08,990 --> 00:13:14,420
is relevant to design like a state of

00:13:11,630 --> 00:13:16,370
the art API is to provide an onion

00:13:14,420 --> 00:13:18,170
you know I'm opinionated base for the

00:13:16,370 --> 00:13:20,480
other idioms that are always rapidly

00:13:18,170 --> 00:13:23,060
evolving things like streams like the

00:13:20,480 --> 00:13:26,690
actor model like the various IO monad

00:13:23,060 --> 00:13:28,550
implementations right which turns out to

00:13:26,690 --> 00:13:29,990
be pretty tricky but I'll show you what

00:13:28,550 --> 00:13:33,079
I've got so far

00:13:29,990 --> 00:13:35,510
so the for the event loop basically we

00:13:33,079 --> 00:13:38,600
have a trait for event loops which just

00:13:35,510 --> 00:13:42,380
extends execution context and it adds to

00:13:38,600 --> 00:13:43,850
capabilities it adds this loop extension

00:13:42,380 --> 00:13:45,769
mechanism which I'll show you more about

00:13:43,850 --> 00:13:47,630
shortly but that's basically how you'll

00:13:45,769 --> 00:13:48,980
register other modules and other C

00:13:47,630 --> 00:13:50,870
libraries with it

00:13:48,980 --> 00:13:52,100
so even third-party libraries can

00:13:50,870 --> 00:13:54,949
integrate with the event loop and

00:13:52,100 --> 00:13:58,639
synchronize with it but it also exposes

00:13:54,949 --> 00:14:00,199
the low-level libuv loop primitive and

00:13:58,639 --> 00:14:02,000
then it has this loop dot run method

00:14:00,199 --> 00:14:04,940
which is what we can use to actually

00:14:02,000 --> 00:14:07,160
yield to the event loop and start

00:14:04,940 --> 00:14:09,949
actually doing i/o unlike traditional

00:14:07,160 --> 00:14:12,680
Scala single threaded execution context

00:14:09,949 --> 00:14:14,199
we aren't eagerly executing futures as

00:14:12,680 --> 00:14:16,699
soon as they're ready instead we

00:14:14,199 --> 00:14:19,699
basically defer them to the point in our

00:14:16,699 --> 00:14:22,459
life cycle where we sort of in all

00:14:19,699 --> 00:14:24,380
future execution so this this is this

00:14:22,459 --> 00:14:26,470
ends up being like medium intrusive but

00:14:24,380 --> 00:14:29,000
we can also talk about ways to

00:14:26,470 --> 00:14:30,949
streamline that and I've got a scholar

00:14:29,000 --> 00:14:32,750
native issue open to just let the

00:14:30,949 --> 00:14:37,370
basically to let the Scala native

00:14:32,750 --> 00:14:40,089
runtime hook into this or other supplied

00:14:37,370 --> 00:14:42,740
libraries that can provide an event loop

00:14:40,089 --> 00:14:45,199
so for for an example of some of these

00:14:42,740 --> 00:14:46,850
loop extensions and capabilities the

00:14:45,199 --> 00:14:50,089
simplest one would just be like a timer

00:14:46,850 --> 00:14:53,060
delay or schedule right so we can have a

00:14:50,089 --> 00:14:56,060
really simple API like this where you

00:14:53,060 --> 00:14:59,000
just give it a duration it returns a

00:14:56,060 --> 00:15:00,829
future of unit after that duration it

00:14:59,000 --> 00:15:02,329
can do repeating schedules so it can be

00:15:00,829 --> 00:15:05,060
a really nice primitive for building

00:15:02,329 --> 00:15:06,380
your own scheduler and of course this

00:15:05,060 --> 00:15:08,089
we're not even

00:15:06,380 --> 00:15:09,740
we're not even touching the the future

00:15:08,089 --> 00:15:11,720
class right we're the only thing we're

00:15:09,740 --> 00:15:13,850
doing is supply in an execution context

00:15:11,720 --> 00:15:15,680
so regular map flatmap

00:15:13,850 --> 00:15:18,760
all of the different callbacks and

00:15:15,680 --> 00:15:21,769
Combinator's just just work for free

00:15:18,760 --> 00:15:25,110
this was my first attempt at streams

00:15:21,769 --> 00:15:26,970
this is what's in the book right now

00:15:25,110 --> 00:15:28,950
but I actually considered this design a

00:15:26,970 --> 00:15:31,560
failure I tried to do something very

00:15:28,950 --> 00:15:34,200
like reactive stream style and what I

00:15:31,560 --> 00:15:37,530
found was it was really tricky to model

00:15:34,200 --> 00:15:40,020
raw sockets or files well files in

00:15:37,530 --> 00:15:42,000
particular have this notion of position

00:15:40,020 --> 00:15:44,130
and seek ability the reactive streams

00:15:42,000 --> 00:15:45,270
doesn't model and demand it doesn't

00:15:44,130 --> 00:15:47,760
really make sense

00:15:45,270 --> 00:15:50,220
whereas sockets are bi-directional and

00:15:47,760 --> 00:15:53,190
sort of demand can can flow in both

00:15:50,220 --> 00:15:55,050
directions and change state sort of

00:15:53,190 --> 00:15:58,530
based on its specific protocol

00:15:55,050 --> 00:16:00,390
implementations so that that turned out

00:15:58,530 --> 00:16:03,510
to be really tricky to do with sort of

00:16:00,390 --> 00:16:05,910
purely reactive streams mechanism so

00:16:03,510 --> 00:16:08,580
when I redesigned this for ODOT for the

00:16:05,910 --> 00:16:10,560
goal was actually to streamline it to

00:16:08,580 --> 00:16:14,220
simplify it and again to provide this

00:16:10,560 --> 00:16:16,860
sort of unup Enya nated base layer for

00:16:14,220 --> 00:16:20,460
things like reactive streams monix Cats

00:16:16,860 --> 00:16:24,420
effect Zeo and sort of punt that to the

00:16:20,460 --> 00:16:27,060
experts so what we've got now it's much

00:16:24,420 --> 00:16:29,610
much simpler it's much closer to just a

00:16:27,060 --> 00:16:33,810
sort of direct exposure of all of libuv

00:16:29,610 --> 00:16:35,730
scape abilities including pausing which

00:16:33,810 --> 00:16:37,650
is really important the the idea is I

00:16:35,730 --> 00:16:39,900
don't for now I'm actually not

00:16:37,650 --> 00:16:42,660
implementing back pressure on my side of

00:16:39,900 --> 00:16:45,480
this instead I'm just exposing a pause

00:16:42,660 --> 00:16:47,970
method that would allow a layer on top

00:16:45,480 --> 00:16:49,440
of this to implement back pressure but

00:16:47,970 --> 00:16:52,680
the cool thing is because this is a

00:16:49,440 --> 00:16:55,110
totally safe idiomatic scala api with no

00:16:52,680 --> 00:16:57,390
C functions or pointer arithmetic it's a

00:16:55,110 --> 00:17:01,200
lot easier for someone else to come in

00:16:57,390 --> 00:17:03,270
and implement any IO idiom they want I

00:17:01,200 --> 00:17:05,459
think it's quite likely that after we've

00:17:03,270 --> 00:17:08,880
gone through the experience of porting

00:17:05,459 --> 00:17:11,970
some like IO monads to this we might

00:17:08,880 --> 00:17:14,520
decide to have a higher level API baked

00:17:11,970 --> 00:17:16,650
in down the road but for now I think

00:17:14,520 --> 00:17:20,280
this is the the right base to move

00:17:16,650 --> 00:17:22,950
forward rapidly it's also not basically

00:17:20,280 --> 00:17:26,520
not typed it strings in strings out is

00:17:22,950 --> 00:17:28,920
sort of what IO speaks and for now I

00:17:26,520 --> 00:17:32,250
think that's actually a safer than

00:17:28,920 --> 00:17:34,860
trying to to come to define complicated

00:17:32,250 --> 00:17:36,540
type signatures likewise the real

00:17:34,860 --> 00:17:38,340
question I had here is whether I should

00:17:36,540 --> 00:17:41,340
even sort of bind

00:17:38,340 --> 00:17:43,380
future into the the low-level API

00:17:41,340 --> 00:17:46,320
signature whether I should entirely work

00:17:43,380 --> 00:17:49,500
with callbacks to allow us to implement

00:17:46,320 --> 00:17:53,490
like an IO monad without having to

00:17:49,500 --> 00:17:54,630
bother allocating futures at all I think

00:17:53,490 --> 00:17:56,490
again that's something that'll come out

00:17:54,630 --> 00:17:58,409
in the wash once we've actually had more

00:17:56,490 --> 00:18:00,960
experience implementing these things but

00:17:58,409 --> 00:18:02,580
it's really interesting and definitely

00:18:00,960 --> 00:18:05,100
the most experimental part of what I'm

00:18:02,580 --> 00:18:07,770
going to show you today in contrast

00:18:05,100 --> 00:18:10,169
things like curl the the famous Lib curl

00:18:07,770 --> 00:18:12,510
C library and it's incredibly

00:18:10,169 --> 00:18:16,140
comprehensive is pretty pretty simple by

00:18:12,510 --> 00:18:18,710
comparison it does what it says it's we

00:18:16,140 --> 00:18:22,110
get this really awesome highly scalable

00:18:18,710 --> 00:18:24,149
HTTP and HTTPS client it does a great

00:18:22,110 --> 00:18:27,390
job integrating with libuv poles and

00:18:24,149 --> 00:18:28,470
timers it has great HTTP support and I

00:18:27,390 --> 00:18:30,809
don't know if anyone's ever tried to

00:18:28,470 --> 00:18:32,909
implement HTTP from scratch but it's

00:18:30,809 --> 00:18:35,340
hard so the fact that we get it for free

00:18:32,909 --> 00:18:38,159
is amazing it also supports like 20

00:18:35,340 --> 00:18:42,299
other protocols like FTP and SCP and

00:18:38,159 --> 00:18:44,700
IMAP my goal though again is not to like

00:18:42,299 --> 00:18:47,970
design an API for this myself but to

00:18:44,700 --> 00:18:49,500
like get support for stdp or request

00:18:47,970 --> 00:18:52,470
Scala and things like that that I think

00:18:49,500 --> 00:18:54,750
do a really really great job of this and

00:18:52,470 --> 00:18:56,760
then I can just punt the the API design

00:18:54,750 --> 00:18:58,260
the the harder problem there though is

00:18:56,760 --> 00:19:00,929
whether there is like a primitive

00:18:58,260 --> 00:19:04,440
low-level API that we could all agree on

00:19:00,929 --> 00:19:08,070
right which I don't think we have yet in

00:19:04,440 --> 00:19:10,950
contrast for the the serve HTTP server

00:19:08,070 --> 00:19:13,529
API we actually have two different we

00:19:10,950 --> 00:19:16,890
actually have to clear layers here right

00:19:13,529 --> 00:19:18,690
we have this imperative server API where

00:19:16,890 --> 00:19:21,240
you just serve on a port and then have a

00:19:18,690 --> 00:19:23,520
handler that every time a request comes

00:19:21,240 --> 00:19:25,620
in the handler it gets called with both

00:19:23,520 --> 00:19:27,600
the request and the connection object

00:19:25,620 --> 00:19:30,390
and the ideas the connection object is

00:19:27,600 --> 00:19:33,299
basically a proxy that provides the

00:19:30,390 --> 00:19:35,159
capability to respond and by providing

00:19:33,299 --> 00:19:37,110
that capability rather than like taking

00:19:35,159 --> 00:19:40,320
a either a future of requests on a

00:19:37,110 --> 00:19:42,090
response or future of requests onto a

00:19:40,320 --> 00:19:44,490
function of requests on a future

00:19:42,090 --> 00:19:47,460
response we can totally abstract over

00:19:44,490 --> 00:19:50,640
that and allow a middleware layer above

00:19:47,460 --> 00:19:52,430
this to decide what types to plug in

00:19:50,640 --> 00:19:54,260
entirely

00:19:52,430 --> 00:19:56,630
and I'll show you then the next slide

00:19:54,260 --> 00:19:58,640
it's like if you want to get started

00:19:56,630 --> 00:20:00,350
writing a server DSL on top of this with

00:19:58,640 --> 00:20:03,020
like routers and stuff like that it's

00:20:00,350 --> 00:20:06,350
like 50 lines of code or something with

00:20:03,020 --> 00:20:08,660
like JSON and stuff like that again in

00:20:06,350 --> 00:20:10,460
the interest of punting on API design

00:20:08,660 --> 00:20:13,250
I'd be really interested to see if we

00:20:10,460 --> 00:20:15,710
can even avoid imposing a request and

00:20:13,250 --> 00:20:18,980
response type the ideal thing would be

00:20:15,710 --> 00:20:21,410
if Scala had a lightweight HTTP server

00:20:18,980 --> 00:20:24,440
middleware standard like ruby has rack

00:20:21,410 --> 00:20:26,030
and Scala has whiskey Scala has servlets

00:20:24,440 --> 00:20:29,420
but frankly I think they're way too

00:20:26,030 --> 00:20:31,430
heavyweight a lightweight servlet style

00:20:29,420 --> 00:20:32,750
API designed for Scala I think would be

00:20:31,430 --> 00:20:33,650
a really healthy thing for the whole

00:20:32,750 --> 00:20:37,010
ecosystem

00:20:33,650 --> 00:20:41,660
I suggest we call them Scala s-- any

00:20:37,010 --> 00:20:44,360
takers on that No okay I tried so yes so

00:20:41,660 --> 00:20:46,850
if you have like 45 minutes you can

00:20:44,360 --> 00:20:48,650
build a simple server DSL on top of it

00:20:46,850 --> 00:20:50,180
that looks more like this and I'm

00:20:48,650 --> 00:20:52,180
actually gonna ship this and the the

00:20:50,180 --> 00:20:55,610
first published version of the library

00:20:52,180 --> 00:20:57,620
so you can just get like synchronous and

00:20:55,610 --> 00:20:59,720
asynchronous request handlers getting

00:20:57,620 --> 00:21:01,550
JSON support baked in is pretty easy

00:20:59,720 --> 00:21:04,130
because we've got we've got two working

00:21:01,550 --> 00:21:05,780
JSON libraries on Scala native we got

00:21:04,130 --> 00:21:09,440
Argan on and spray json right now I

00:21:05,780 --> 00:21:11,240
think this router is rudimentary but

00:21:09,440 --> 00:21:14,450
they're really good ones we could pour

00:21:11,240 --> 00:21:18,320
it over cask is awesome

00:21:14,450 --> 00:21:20,660
plays routing DSL surd is surprisingly

00:21:18,320 --> 00:21:22,040
um compact and isolated and I think you

00:21:20,660 --> 00:21:23,570
could port it over a pretty pretty

00:21:22,040 --> 00:21:26,960
quickly and that would be really fun and

00:21:23,570 --> 00:21:28,400
a huge win and because all of the

00:21:26,960 --> 00:21:30,260
capabilities I've showed you over these

00:21:28,400 --> 00:21:32,810
last five slides are all running on the

00:21:30,260 --> 00:21:35,050
same that loop and coordinated by libuv

00:21:32,810 --> 00:21:38,570
you can mix and match all of these

00:21:35,050 --> 00:21:40,340
seamlessly and once you have write a web

00:21:38,570 --> 00:21:42,830
server that can take asynchronous

00:21:40,340 --> 00:21:46,130
request handlers and a web client that

00:21:42,830 --> 00:21:48,380
returns futures you have the basics for

00:21:46,130 --> 00:21:52,700
for distributed systems in a modern

00:21:48,380 --> 00:21:55,190
environment and it just works which gets

00:21:52,700 --> 00:21:56,660
really exciting but if we're gonna talk

00:21:55,190 --> 00:22:00,290
about distributed systems we should also

00:21:56,660 --> 00:22:01,850
talk about performance right so I like

00:22:00,290 --> 00:22:03,740
the first time I gave a talk about Scala

00:22:01,850 --> 00:22:05,730
native two years ago it's strange loop I

00:22:03,740 --> 00:22:07,680
was all about having like land

00:22:05,730 --> 00:22:09,870
speed record performance graphs and

00:22:07,680 --> 00:22:11,880
stuff like that what I found is that

00:22:09,870 --> 00:22:14,040
they aren't super representative of

00:22:11,880 --> 00:22:16,590
actual application behavior in reality

00:22:14,040 --> 00:22:18,930
modern server-side back-end applications

00:22:16,590 --> 00:22:21,930
are much more likely to bottleneck on

00:22:18,930 --> 00:22:24,690
their backing data store not on pushing

00:22:21,930 --> 00:22:27,330
through HTTP requests right that said I

00:22:24,690 --> 00:22:29,340
am load testing this code base with

00:22:27,330 --> 00:22:31,530
gatlin quite regularly and what I'm

00:22:29,340 --> 00:22:33,390
aiming for is high hundreds low

00:22:31,530 --> 00:22:35,190
thousands of requests per second on

00:22:33,390 --> 00:22:37,080
benchmarks with a good quality of

00:22:35,190 --> 00:22:40,200
service trying to do a little bit better

00:22:37,080 --> 00:22:42,870
than no jeaious but the the real impact

00:22:40,200 --> 00:22:45,530
is actually on service density right

00:22:42,870 --> 00:22:48,180
Scala native is seriously lightweight

00:22:45,530 --> 00:22:50,130
all of these instances will be running

00:22:48,180 --> 00:22:51,840
on less than one CPU core they take a

00:22:50,130 --> 00:22:54,240
hundred and two hundred megabytes of RAM

00:22:51,840 --> 00:22:57,540
the binary footprint is often less than

00:22:54,240 --> 00:22:59,850
ten megabytes versus like realistic JVM

00:22:57,540 --> 00:23:02,250
like akka or play micro services or

00:22:59,850 --> 00:23:05,520
you're talking two to four CPU cores or

00:23:02,250 --> 00:23:08,010
more you're talking at least a gigabyte

00:23:05,520 --> 00:23:10,140
of RAM realistically if not two to four

00:23:08,010 --> 00:23:12,870
is what a lot of my services run on in

00:23:10,140 --> 00:23:16,590
prod and like a ten times larger like

00:23:12,870 --> 00:23:18,330
disk footprint if not worse and like if

00:23:16,590 --> 00:23:20,730
you're in a real world scenario like

00:23:18,330 --> 00:23:22,470
let's say you have like eight clustered

00:23:20,730 --> 00:23:25,590
micro services you have three to five

00:23:22,470 --> 00:23:27,900
instances of each of those maybe at

00:23:25,590 --> 00:23:29,460
worst two of those eight services

00:23:27,900 --> 00:23:31,860
actually run at like a hundred percent

00:23:29,460 --> 00:23:33,480
saturation under peak load you've got a

00:23:31,860 --> 00:23:36,390
system where you have a lot of idle

00:23:33,480 --> 00:23:39,660
resources and you're paying for all of

00:23:36,390 --> 00:23:41,750
this this overhead the the JVM sort of

00:23:39,660 --> 00:23:44,880
taxes out of you and that really adds up

00:23:41,750 --> 00:23:46,830
and I think Scala natives like tiny

00:23:44,880 --> 00:23:49,920
footprint really makes it suited

00:23:46,830 --> 00:23:53,790
economically to the the modern style of

00:23:49,920 --> 00:23:55,260
like small distributed services don't

00:23:53,790 --> 00:23:57,270
even get me started about how great it

00:23:55,260 --> 00:23:59,790
is for server lists where you pay for

00:23:57,270 --> 00:24:02,010
the the basically megabyte of memory

00:23:59,790 --> 00:24:03,780
times seconds it's outstanding and

00:24:02,010 --> 00:24:06,960
serverless I wish I could give a whole

00:24:03,780 --> 00:24:08,580
talk on that but yeah so that's that's

00:24:06,960 --> 00:24:10,200
sort of the high level let's um let's

00:24:08,580 --> 00:24:13,590
let's go deep we've got we're doing

00:24:10,200 --> 00:24:15,720
great on time so let's let's do it so to

00:24:13,590 --> 00:24:17,430
do this we're gonna need to unsafe

00:24:15,720 --> 00:24:19,059
techniques from systems programming

00:24:17,430 --> 00:24:21,309
we're going to need pointers

00:24:19,059 --> 00:24:24,600
and we're gonna need unsafe casts right

00:24:21,309 --> 00:24:27,039
so high level pointer is a

00:24:24,600 --> 00:24:29,169
representation of the location of a

00:24:27,039 --> 00:24:30,820
piece of data basically represents an

00:24:29,169 --> 00:24:33,759
address in memory is an a byte unsigned

00:24:30,820 --> 00:24:36,159
integer equivalent to long so like a

00:24:33,759 --> 00:24:38,740
pointer T is the address of a value of

00:24:36,159 --> 00:24:41,710
some type T you can think of it as being

00:24:38,740 --> 00:24:44,110
like a mutable cell for a value right

00:24:41,710 --> 00:24:45,129
and it can feel a little ungainly but

00:24:44,110 --> 00:24:47,379
there's something about that that's

00:24:45,129 --> 00:24:50,169
almost more elegant than Scala's var

00:24:47,379 --> 00:24:51,820
right if you look at UM other strict

00:24:50,169 --> 00:24:54,669
functional programming languages like

00:24:51,820 --> 00:24:57,279
standard ml and oh camel they tend to

00:24:54,669 --> 00:24:58,840
use like mutable cell containers for

00:24:57,279 --> 00:25:01,840
things like this and it can be really

00:24:58,840 --> 00:25:03,940
elegant actually and then an unsafe

00:25:01,840 --> 00:25:06,549
caste is really just a C programming

00:25:03,940 --> 00:25:08,889
technique where you have a pointer T if

00:25:06,549 --> 00:25:10,779
you just make a compile-time claim this

00:25:08,889 --> 00:25:14,470
isn't a pointer T this is a pointer to X

00:25:10,779 --> 00:25:17,259
I know what I'm doing and it often works

00:25:14,470 --> 00:25:19,480
so you can treat a pointer as any other

00:25:17,259 --> 00:25:21,879
pointer type and when necessary you can

00:25:19,480 --> 00:25:25,830
just cast the pointer to a long and sort

00:25:21,879 --> 00:25:30,909
of Yolo it which can be scary but

00:25:25,830 --> 00:25:33,700
powerful right C famously has no generic

00:25:30,909 --> 00:25:35,769
programming mechanisms unless you count

00:25:33,700 --> 00:25:39,220
macros and I definitely don't count C

00:25:35,769 --> 00:25:42,700
macros but C has void pointers basically

00:25:39,220 --> 00:25:44,860
pointers to whatever or to any and it's

00:25:42,700 --> 00:25:48,490
really common for C libraries that are

00:25:44,860 --> 00:25:51,159
designed for generic programming to just

00:25:48,490 --> 00:25:53,139
provide sort of a wild-card void pointer

00:25:51,159 --> 00:25:55,720
field and their data structures where

00:25:53,139 --> 00:25:57,369
you the user can just sort of throw in a

00:25:55,720 --> 00:26:00,850
pointer to whatever data structure you

00:25:57,369 --> 00:26:02,830
want and in scala native will represent

00:26:00,850 --> 00:26:04,840
these void pointers is just a pointer to

00:26:02,830 --> 00:26:06,940
byte it's just the address of a binary

00:26:04,840 --> 00:26:09,129
blob somewhere we don't know how many

00:26:06,940 --> 00:26:13,480
bytes it is but we know where it is and

00:26:09,129 --> 00:26:16,330
that as we'll see is is enough so in in

00:26:13,480 --> 00:26:17,799
practice it looks like this so if you

00:26:16,330 --> 00:26:19,990
want to get a pointer there's a few ways

00:26:17,799 --> 00:26:21,730
you can get them but the best way is is

00:26:19,990 --> 00:26:23,679
malloc that's not a Scala native

00:26:21,730 --> 00:26:26,529
intrinsic that's me calling the C

00:26:23,679 --> 00:26:27,970
standard Lib malloc literally and the

00:26:26,529 --> 00:26:29,799
way it works is you tell it how many

00:26:27,970 --> 00:26:32,019
bytes you want which I'm give it I'm

00:26:29,799 --> 00:26:32,299
asking it for size of long bytes which

00:26:32,019 --> 00:26:34,759
is

00:26:32,299 --> 00:26:36,830
for the record and what it returns is

00:26:34,759 --> 00:26:38,869
just a return it returns a void pointer

00:26:36,830 --> 00:26:41,509
and see so it just says okay here's a

00:26:38,869 --> 00:26:43,429
pointer back to a byte somewhere it

00:26:41,509 --> 00:26:45,979
doesn't even matter cause an even

00:26:43,429 --> 00:26:48,559
natively track the the type that you've

00:26:45,979 --> 00:26:50,330
asked it for so on line three this is

00:26:48,559 --> 00:26:52,519
the first time we do an unsafe cast

00:26:50,330 --> 00:26:54,799
because just allocating typed memory it

00:26:52,519 --> 00:26:57,729
requires a cast right so we're doing raw

00:26:54,799 --> 00:27:01,190
data dot as instance of pointer long

00:26:57,729 --> 00:27:03,679
which then just asserts right there's no

00:27:01,190 --> 00:27:05,599
computation associated with that but

00:27:03,679 --> 00:27:07,759
it's actually a point or two along the

00:27:05,599 --> 00:27:09,739
other thing if anyone's used Malik is

00:27:07,759 --> 00:27:11,929
malloc returns uninitialized memory

00:27:09,739 --> 00:27:14,599
which is quite scary to us the scala

00:27:11,929 --> 00:27:16,879
programmers it might be zeroed or it

00:27:14,599 --> 00:27:21,440
might be just have garbage data from the

00:27:16,879 --> 00:27:24,289
last the last piece of code to use it so

00:27:21,440 --> 00:27:26,509
we want to we want to initialize it so

00:27:24,289 --> 00:27:28,429
the way you initialize it is just by

00:27:26,509 --> 00:27:30,649
setting the value to zero and that's

00:27:28,429 --> 00:27:33,320
what we're doing on line six the way we

00:27:30,649 --> 00:27:35,200
both set and dereference pointers and

00:27:33,320 --> 00:27:38,329
Scala native is with the prefix

00:27:35,200 --> 00:27:40,700
exclamation mark or bang operator if

00:27:38,329 --> 00:27:43,849
anyone's done this and standard ml it's

00:27:40,700 --> 00:27:46,459
pretty similar syntactically so the idea

00:27:43,849 --> 00:27:48,169
is if you use exclamation mark on the

00:27:46,459 --> 00:27:50,359
left hand side of an assignment it's an

00:27:48,169 --> 00:27:52,999
update operator and just stores the

00:27:50,359 --> 00:27:54,919
value into the pointer and if you use it

00:27:52,999 --> 00:27:56,989
on the right hand side or just an

00:27:54,919 --> 00:27:59,779
expression position right then it's a

00:27:56,989 --> 00:28:03,499
dereference it returns the value stored

00:27:59,779 --> 00:28:05,239
in the pointer right so like on line 10

00:28:03,499 --> 00:28:09,859
where I'm starting to like print these I

00:28:05,239 --> 00:28:12,559
can print both the pointer itself long

00:28:09,859 --> 00:28:14,570
pointer as well as its value which is

00:28:12,559 --> 00:28:16,669
bang long pointer so the ability to

00:28:14,570 --> 00:28:19,089
distinguish data from its address is

00:28:16,669 --> 00:28:21,229
something that's basically impossible

00:28:19,089 --> 00:28:23,089
you know when you're when you have a

00:28:21,229 --> 00:28:25,669
garbage collector to deal with or in

00:28:23,089 --> 00:28:28,099
Java right but it's also immensely

00:28:25,669 --> 00:28:29,599
powerful and then let's say we want to

00:28:28,099 --> 00:28:32,299
update it we can do that again on line

00:28:29,599 --> 00:28:34,909
11 we imprinted again and then of course

00:28:32,299 --> 00:28:36,679
with malloc you're responsible for the

00:28:34,909 --> 00:28:38,509
memory you're using your garbage

00:28:36,679 --> 00:28:41,570
collector won't claim it back for you so

00:28:38,509 --> 00:28:43,399
if you don't free it it leaks we let it

00:28:41,570 --> 00:28:47,119
go with free but

00:28:43,399 --> 00:28:49,839
if we mistakenly try to read it again

00:28:47,119 --> 00:28:52,489
after we free we we could get a segfault

00:28:49,839 --> 00:28:54,320
which will just blow up our program

00:28:52,489 --> 00:28:56,059
without a stack trace or you could

00:28:54,320 --> 00:28:57,739
accidentally corrupt your corrupt your

00:28:56,059 --> 00:29:01,519
data if you're not lucky enough to to

00:28:57,739 --> 00:29:04,249
seg fault pointers are genuinely

00:29:01,519 --> 00:29:05,809
dangerous and others incredibly powerful

00:29:04,249 --> 00:29:06,320
and great for a performance and Smoove

00:29:05,809 --> 00:29:08,149
code

00:29:06,320 --> 00:29:10,129
it's also worth being really careful

00:29:08,149 --> 00:29:12,320
about where and your code base this

00:29:10,129 --> 00:29:13,969
lives and not letting it sort of spread

00:29:12,320 --> 00:29:16,690
out over everything you know what this

00:29:13,969 --> 00:29:19,969
isolated to a few critical sections

00:29:16,690 --> 00:29:21,769
where it can make a huge impact but it's

00:29:19,969 --> 00:29:24,080
also worth being very cautious about

00:29:21,769 --> 00:29:26,559
having this in a large code base with a

00:29:24,080 --> 00:29:29,239
bunch of people working on it

00:29:26,559 --> 00:29:31,279
so the other thing you need to we need

00:29:29,239 --> 00:29:34,849
to actually get libuv running is we need

00:29:31,279 --> 00:29:36,769
to use the CF Fi and declare bindings so

00:29:34,849 --> 00:29:39,259
again the thing that's really amazing

00:29:36,769 --> 00:29:42,739
about Scala native for me is how easy it

00:29:39,259 --> 00:29:45,619
is to just link to either C standard Lib

00:29:42,739 --> 00:29:47,539
functions or third-party C libraries you

00:29:45,619 --> 00:29:49,969
literally just have this a texture knob

00:29:47,539 --> 00:29:52,039
jekt and then on line four we just have

00:29:49,969 --> 00:29:55,549
a def cue sort which is just going to

00:29:52,039 --> 00:29:58,869
have the signature whose types align

00:29:55,549 --> 00:30:01,669
with the C standard Lib Q sort function

00:29:58,869 --> 00:30:04,429
and then say equals extern and that

00:30:01,669 --> 00:30:06,409
that's literally all there is to it

00:30:04,429 --> 00:30:08,479
I'm gonna help it out a little bit by

00:30:06,409 --> 00:30:10,429
declaring a type called comparator

00:30:08,479 --> 00:30:13,460
because if anyone's used Q sort it

00:30:10,429 --> 00:30:16,580
basically takes an opaque byte array and

00:30:13,460 --> 00:30:19,219
then it takes a function pointer a C

00:30:16,580 --> 00:30:21,139
function pointer for the actual function

00:30:19,219 --> 00:30:22,789
to use to compare items in it which is

00:30:21,139 --> 00:30:25,969
how it's sort of abstracts over the

00:30:22,789 --> 00:30:27,769
structure of the array and the really

00:30:25,969 --> 00:30:30,049
cool thing that Scala native can do that

00:30:27,769 --> 00:30:34,129
go can't and that is really nice is we

00:30:30,049 --> 00:30:36,950
can pass Scala functions into this just

00:30:34,129 --> 00:30:39,169
like C functions there's there's also

00:30:36,950 --> 00:30:41,450
some limitations on that though C

00:30:39,169 --> 00:30:42,589
functions are unlike Scala functions and

00:30:41,450 --> 00:30:44,869
that they're static right they don't

00:30:42,589 --> 00:30:47,450
have lexical scope they can access

00:30:44,869 --> 00:30:49,190
static variables like object members but

00:30:47,450 --> 00:30:52,070
they can't access a member of a class

00:30:49,190 --> 00:30:53,539
for example so that constrains your

00:30:52,070 --> 00:30:55,460
design patterns a little bit and you'll

00:30:53,539 --> 00:30:57,350
see me using objects instead of case

00:30:55,460 --> 00:30:59,330
classes in a lot of places that

00:30:57,350 --> 00:31:01,850
might feel a little little funky but

00:30:59,330 --> 00:31:05,870
that's mostly to make them work safely

00:31:01,850 --> 00:31:08,480
with with C functions so for example if

00:31:05,870 --> 00:31:11,440
we want to like declare struck a data

00:31:08,480 --> 00:31:14,090
structure that's like a C style like

00:31:11,440 --> 00:31:17,090
data structure we can declare it like on

00:31:14,090 --> 00:31:19,370
line to this my struct is just basically

00:31:17,090 --> 00:31:21,160
declared like a tuple and then we can

00:31:19,370 --> 00:31:23,900
declare an instance of a comparator

00:31:21,160 --> 00:31:27,320
function which basically works as a

00:31:23,900 --> 00:31:29,930
single abstract method class the of

00:31:27,320 --> 00:31:32,030
course with Scala 212 in a month or two

00:31:29,930 --> 00:31:33,260
for Scala native the syntax for this

00:31:32,030 --> 00:31:36,170
will get cleaner and that'll just be a

00:31:33,260 --> 00:31:39,050
lambda right but this is how it works in

00:31:36,170 --> 00:31:41,060
2.11 and then once we've done that like

00:31:39,050 --> 00:31:44,300
if you see on line 15 then you can just

00:31:41,060 --> 00:31:46,670
call Q sort and pass it your comparator

00:31:44,300 --> 00:31:48,920
and it just works and if anyone wants to

00:31:46,670 --> 00:31:52,850
see my talk last year for how crazy fast

00:31:48,920 --> 00:31:54,530
the C standard Lib quicksort is I get

00:31:52,850 --> 00:31:56,030
really excited about that but I don't

00:31:54,530 --> 00:31:59,210
have time to go into it today

00:31:56,030 --> 00:32:01,250
unfortunately so all of this reminds me

00:31:59,210 --> 00:32:04,730
of a like ancient piece of program or

00:32:01,250 --> 00:32:06,860
wisdom green sponge tenth rule which is

00:32:04,730 --> 00:32:09,740
to say that any sufficiently complicated

00:32:06,860 --> 00:32:13,340
C or Fortran program contains an ad hoc

00:32:09,740 --> 00:32:15,760
informally specified bug-ridden and slow

00:32:13,340 --> 00:32:19,070
implementation of half of Common Lisp

00:32:15,760 --> 00:32:21,290
and the way I unpack that is to say that

00:32:19,070 --> 00:32:23,750
the the techniques I've shown you these

00:32:21,290 --> 00:32:27,050
void pointers function arguments and

00:32:23,750 --> 00:32:28,490
casts their C affordances for generic

00:32:27,050 --> 00:32:30,670
programming but they're also just how

00:32:28,490 --> 00:32:32,990
you implement a dynamic language and C

00:32:30,670 --> 00:32:34,730
and the errors you get when you do this

00:32:32,990 --> 00:32:36,350
wrong or a lot closer to the kind of

00:32:34,730 --> 00:32:38,690
errors you get in a Python or a

00:32:36,350 --> 00:32:42,350
JavaScript function where you just get a

00:32:38,690 --> 00:32:45,110
feel on an object that isn't there or

00:32:42,350 --> 00:32:46,700
call a function that isn't there so you

00:32:45,110 --> 00:32:48,260
don't always have you don't have the

00:32:46,700 --> 00:32:51,620
kind of safety scholar normally

00:32:48,260 --> 00:32:53,810
guarantees you but it this suffices

00:32:51,620 --> 00:32:56,930
right this is this is enough to make it

00:32:53,810 --> 00:33:00,170
work and to provide a safe wrapper on

00:32:56,930 --> 00:33:03,020
top of libuv API surface

00:33:00,170 --> 00:33:06,950
so let's actually get into the the real

00:33:03,020 --> 00:33:09,290
implementation now so to start scala

00:33:06,950 --> 00:33:11,630
nated actually already has an execution

00:33:09,290 --> 00:33:14,330
context that it includes

00:33:11,630 --> 00:33:16,370
it's literally just these 22 lines of

00:33:14,330 --> 00:33:17,660
code before I saw Victor's talk

00:33:16,370 --> 00:33:19,460
yesterday I would have said this is the

00:33:17,660 --> 00:33:22,340
smallest one possible but now I've seen

00:33:19,460 --> 00:33:24,560
one that's half this size but the idea

00:33:22,340 --> 00:33:26,540
is it does something a little unusual in

00:33:24,560 --> 00:33:30,020
that the way it implements the execute

00:33:26,540 --> 00:33:32,030
method is that it doesn't immediately

00:33:30,020 --> 00:33:33,980
execute runnable x' when they're ready

00:33:32,030 --> 00:33:37,130
to run instead it just appends them onto

00:33:33,980 --> 00:33:39,620
a queue and defers execution until this

00:33:37,130 --> 00:33:41,570
private loop method gets invoked and

00:33:39,620 --> 00:33:43,580
this is where the Scala native runtime

00:33:41,570 --> 00:33:45,980
hooks in and it basically just calls

00:33:43,580 --> 00:33:47,800
this loop after the main method or after

00:33:45,980 --> 00:33:50,750
the main function of the class returns

00:33:47,800 --> 00:33:53,570
and then it just basically pulls the

00:33:50,750 --> 00:33:55,430
queue until it's exhausted and of course

00:33:53,570 --> 00:33:56,840
each one of those queue each one of the

00:33:55,430 --> 00:33:58,670
runnable x' on the queue can complete a

00:33:56,840 --> 00:34:01,100
future which can spawn more so it can

00:33:58,670 --> 00:34:03,040
get repopulated so maybe it won't get

00:34:01,100 --> 00:34:06,050
exhausted or maybe it eventually will

00:34:03,040 --> 00:34:07,610
but the implementation of future takes

00:34:06,050 --> 00:34:09,919
care of all that if you implement your

00:34:07,610 --> 00:34:13,340
own execution context you don't worry

00:34:09,919 --> 00:34:15,740
about dispatching or linking or

00:34:13,340 --> 00:34:18,200
callbacks you just worry about running

00:34:15,740 --> 00:34:20,240
the the runnable x' the the future class

00:34:18,200 --> 00:34:24,350
gives you so it's surprisingly

00:34:20,240 --> 00:34:26,450
lightweight to implement one of these so

00:34:24,350 --> 00:34:29,330
we're gonna use a very similar technique

00:34:26,450 --> 00:34:31,250
to make this work on libuv the libuv

00:34:29,330 --> 00:34:34,330
event loop has quite a few different

00:34:31,250 --> 00:34:38,030
life cycle hooks that we can attach to

00:34:34,330 --> 00:34:40,340
from my experience the the best time to

00:34:38,030 --> 00:34:43,429
run these things has been immediately

00:34:40,340 --> 00:34:46,640
prior to poling for i/o the idea is

00:34:43,429 --> 00:34:48,919
before we start we stop running our code

00:34:46,640 --> 00:34:51,530
and start pulling for i/o we want to

00:34:48,919 --> 00:34:53,360
exhaust all work that's available and

00:34:51,530 --> 00:34:55,159
then we'll just do i/o until we have

00:34:53,360 --> 00:34:57,590
more work to do

00:34:55,159 --> 00:35:01,250
effectively now the one catch with that

00:34:57,590 --> 00:35:02,960
is if you were if you recall the the

00:35:01,250 --> 00:35:05,900
some of the api slides i showed you in

00:35:02,960 --> 00:35:08,060
the last section we have tasks that we

00:35:05,900 --> 00:35:11,000
can do like reading from a socket that

00:35:08,060 --> 00:35:14,000
aren't represented by a future right so

00:35:11,000 --> 00:35:17,540
this execution context has to do one

00:35:14,000 --> 00:35:21,950
extra task it has to track non future IO

00:35:17,540 --> 00:35:24,020
tasks and delay termination of the loop

00:35:21,950 --> 00:35:25,970
until all IO is come

00:35:24,020 --> 00:35:28,580
pleat and there's no more i/o work it

00:35:25,970 --> 00:35:30,140
can do as well as no more futures that's

00:35:28,580 --> 00:35:32,480
sort of the one complication you get

00:35:30,140 --> 00:35:35,030
from cert from building i/o into your

00:35:32,480 --> 00:35:37,250
event loop and the way we do that is

00:35:35,030 --> 00:35:39,320
with this this loop extension that we

00:35:37,250 --> 00:35:42,890
briefly saw saw earlier it's just a

00:35:39,320 --> 00:35:44,720
trait and it allows any either code

00:35:42,890 --> 00:35:48,230
within the library or third-party

00:35:44,720 --> 00:35:52,670
library code to register with the event

00:35:48,230 --> 00:35:54,230
loop that some number of i/o tasks are

00:35:52,670 --> 00:35:56,060
going and it can just sort of check in

00:35:54,230 --> 00:35:58,460
on this and see see if there's work

00:35:56,060 --> 00:36:00,200
being done it's a really great way to

00:35:58,460 --> 00:36:03,320
just keep the code really modular rather

00:36:00,200 --> 00:36:06,230
than have a giant class that sort of

00:36:03,320 --> 00:36:08,060
models this very large sea library so

00:36:06,230 --> 00:36:10,520
the way we actually implement this trait

00:36:08,060 --> 00:36:13,220
we start out with something very very

00:36:10,520 --> 00:36:14,660
close to the initial code right we have

00:36:13,220 --> 00:36:17,720
a list buffer of runnable x'

00:36:14,660 --> 00:36:20,750
we will get an instance of the the UV

00:36:17,720 --> 00:36:23,570
default event loop and again we'll defer

00:36:20,750 --> 00:36:27,230
running our runnable x' until later when

00:36:23,570 --> 00:36:30,220
a callback handle fires and that gets

00:36:27,230 --> 00:36:34,730
implemented like this this dispatch step

00:36:30,220 --> 00:36:37,070
callback and all it does is again it

00:36:34,730 --> 00:36:39,410
just walks through the queue it runs the

00:36:37,070 --> 00:36:43,670
tasks that it can and then the one check

00:36:39,410 --> 00:36:46,580
is before actually stopping right it

00:36:43,670 --> 00:36:49,640
checks to see if the task he was empty

00:36:46,580 --> 00:36:51,830
and no extensions have outstanding work

00:36:49,640 --> 00:36:54,050
then and only then it'll actually stop

00:36:51,830 --> 00:36:56,330
the handle which will allow event the

00:36:54,050 --> 00:36:59,360
libuv event loop to terminate and then

00:36:56,330 --> 00:37:00,800
our program can connects it otherwise it

00:36:59,360 --> 00:37:04,970
just goes back through the loop and

00:37:00,800 --> 00:37:08,030
pulls for more i/o and does more work so

00:37:04,970 --> 00:37:09,800
all the actual interesting work gets

00:37:08,030 --> 00:37:11,750
implemented as these loop extensions

00:37:09,800 --> 00:37:13,820
that bolt onto it the whole execution

00:37:11,750 --> 00:37:15,860
context is I think it's less than a

00:37:13,820 --> 00:37:18,830
hundred lines of code all the the fun

00:37:15,860 --> 00:37:21,170
stuff isn't the extensions and then

00:37:18,830 --> 00:37:23,210
basically for whenever we add one in

00:37:21,170 --> 00:37:25,520
we'll just register it with this add

00:37:23,210 --> 00:37:27,650
extension method it's really simple so

00:37:25,520 --> 00:37:29,690
let's implement the sort of delay timer

00:37:27,650 --> 00:37:32,450
loop extension now because yeah we're

00:37:29,690 --> 00:37:36,500
really good on time this is awesome so

00:37:32,450 --> 00:37:37,560
the the trick here is the the the timer

00:37:36,500 --> 00:37:40,290
extends Luke

00:37:37,560 --> 00:37:43,560
Extension active requests is just gonna

00:37:40,290 --> 00:37:45,750
be the size of this mutable hash map

00:37:43,560 --> 00:37:48,510
we're gonna keep around the keys of this

00:37:45,750 --> 00:37:51,140
hash map are going to be Long's

00:37:48,510 --> 00:37:54,270
and the values are going to be promises

00:37:51,140 --> 00:37:57,060
have people seen the promise class

00:37:54,270 --> 00:37:59,280
mostly I think Victor gave a better

00:37:57,060 --> 00:38:01,890
summary of it than I possibly could I

00:37:59,280 --> 00:38:04,830
think what he said is a promise is an

00:38:01,890 --> 00:38:05,820
obligation to provide a value at some

00:38:04,830 --> 00:38:08,820
point in the future

00:38:05,820 --> 00:38:10,920
it lets us spawn off futures that aren't

00:38:08,820 --> 00:38:14,370
attached to a runnable and then let's

00:38:10,920 --> 00:38:17,100
our code supply values or failures to

00:38:14,370 --> 00:38:20,010
them whenever we choose so it's a great

00:38:17,100 --> 00:38:24,090
way to actually implement IO and return

00:38:20,010 --> 00:38:26,130
futures safely and then we'll just have

00:38:24,090 --> 00:38:28,380
two additional methods we'll have the

00:38:26,130 --> 00:38:30,210
delay method which just is what we saw

00:38:28,380 --> 00:38:32,280
in the public API signature takes a

00:38:30,210 --> 00:38:34,950
duration returns a future and then we'll

00:38:32,280 --> 00:38:36,990
have this sum timer callback function so

00:38:34,950 --> 00:38:39,840
the signature of that timer callback

00:38:36,990 --> 00:38:41,490
function you can see on line 18 it's as

00:38:39,840 --> 00:38:44,070
simple as it can be it takes a timer

00:38:41,490 --> 00:38:46,680
handle returns a unit so it does almost

00:38:44,070 --> 00:38:49,590
nothing but there's also not a lot of

00:38:46,680 --> 00:38:51,990
arguments you can throw into it we're

00:38:49,590 --> 00:38:54,420
gonna model the timer handle as a

00:38:51,990 --> 00:38:57,540
pointer of long even though this is a

00:38:54,420 --> 00:39:00,270
large opaque data structure right if you

00:38:57,540 --> 00:39:02,370
look at the C includes it's got lots of

00:39:00,270 --> 00:39:04,620
macros and it's different on various

00:39:02,370 --> 00:39:07,280
platforms so it really be a pain to

00:39:04,620 --> 00:39:10,170
model this data structure feel by field

00:39:07,280 --> 00:39:12,300
to treat it just as a pointer long is

00:39:10,170 --> 00:39:14,100
cool and is sort of what makes this work

00:39:12,300 --> 00:39:16,290
and I'll show you how that works but

00:39:14,100 --> 00:39:18,690
it's it's a fun little trick and then

00:39:16,290 --> 00:39:23,120
the two C functions we need to call our

00:39:18,690 --> 00:39:25,800
we need timer in it and timer start and

00:39:23,120 --> 00:39:28,380
they do what what would they say timer

00:39:25,800 --> 00:39:31,470
in it initializes a timer handle pointer

00:39:28,380 --> 00:39:34,320
and attaches it to a loop and then timer

00:39:31,470 --> 00:39:37,710
start takes a callback and a timeout and

00:39:34,320 --> 00:39:39,960
optionally a repeat count and then start

00:39:37,710 --> 00:39:44,010
starts running it whenever the event

00:39:39,960 --> 00:39:46,230
loop is ready to go so the the one dumb

00:39:44,010 --> 00:39:48,810
trick for opaque structs is that it's

00:39:46,230 --> 00:39:50,700
very often required and C or in Scala

00:39:48,810 --> 00:39:51,150
native to work with a data structure

00:39:50,700 --> 00:39:53,549
with

00:39:51,150 --> 00:39:55,380
knowing its internal layout that makes

00:39:53,549 --> 00:39:57,329
it really hard to allocate or initialize

00:39:55,380 --> 00:39:59,039
it yourself programmatically but if the

00:39:57,329 --> 00:40:01,920
library you're working with gives you

00:39:59,039 --> 00:40:03,779
helper functions it can often allocate

00:40:01,920 --> 00:40:05,609
and initialize that for you so you don't

00:40:03,779 --> 00:40:08,609
even have to know how many bytes wide

00:40:05,609 --> 00:40:10,710
this thing is and then there's this

00:40:08,609 --> 00:40:12,720
amazing C technique called tight puns

00:40:10,710 --> 00:40:16,769
which allow us to cast data between

00:40:12,720 --> 00:40:20,069
unrelated types so you can sort of both

00:40:16,769 --> 00:40:22,259
cast a struct with three fields to sort

00:40:20,069 --> 00:40:24,539
of a prefix of its fields and as long as

00:40:22,259 --> 00:40:27,539
you don't modify or touch the trailing

00:40:24,539 --> 00:40:29,640
fields nothing goes wrong right and then

00:40:27,539 --> 00:40:31,559
you can cast like a struct containing a

00:40:29,640 --> 00:40:33,869
pointer of byte to a struct containing

00:40:31,559 --> 00:40:36,349
along because they're the same size and

00:40:33,869 --> 00:40:39,420
then that's here's the really cool one a

00:40:36,349 --> 00:40:41,640
pointer to a one field struct containing

00:40:39,420 --> 00:40:44,549
a team is equivalent to a pointer

00:40:41,640 --> 00:40:47,970
containing a T there's no like padding

00:40:44,549 --> 00:40:50,999
there it's all totally static layout so

00:40:47,970 --> 00:40:52,829
it's it's a it feels scary and kind of

00:40:50,999 --> 00:40:54,990
ugly but it also works and it can be

00:40:52,829 --> 00:40:58,079
really fast because what we'll do is

00:40:54,990 --> 00:41:00,299
we're going to use that long cell

00:40:58,079 --> 00:41:02,279
basically to store serial numbers and

00:41:00,299 --> 00:41:05,519
that will allow us to basically keep

00:41:02,279 --> 00:41:08,579
enough tracking data about which timer

00:41:05,519 --> 00:41:11,099
instances we've created in this mutable

00:41:08,579 --> 00:41:13,410
hash map but it'll allow us to avoid

00:41:11,099 --> 00:41:15,779
having to do an extra dereference and

00:41:13,410 --> 00:41:17,670
memory load write to find out the

00:41:15,779 --> 00:41:20,279
contents like we would if we had like a

00:41:17,670 --> 00:41:22,829
larger custom data structure attached

00:41:20,279 --> 00:41:25,140
there so once we do that the delay

00:41:22,829 --> 00:41:26,670
method works like this we convert the

00:41:25,140 --> 00:41:28,799
duration to milliseconds because that's

00:41:26,670 --> 00:41:30,839
what it wants we instantiate a promise

00:41:28,799 --> 00:41:33,329
we generate a serial number and store

00:41:30,839 --> 00:41:36,089
the promise in our map and then on line

00:41:33,329 --> 00:41:38,009
9 we use malloc to allocate a timer

00:41:36,089 --> 00:41:40,380
handle and then we use this helper

00:41:38,009 --> 00:41:43,170
function libuv gives us UV handle size

00:41:40,380 --> 00:41:45,809
to actually get a correctly sized chunk

00:41:43,170 --> 00:41:48,690
of data for this for our platform then

00:41:45,809 --> 00:41:51,269
we initialize it and then we on line 11

00:41:48,690 --> 00:41:55,319
we just use the dereference operator to

00:41:51,269 --> 00:41:57,299
assign our timer ID to the timer handle

00:41:55,319 --> 00:41:59,190
which feels really scary and destructive

00:41:57,299 --> 00:42:01,410
but because it's only going to do an

00:41:59,190 --> 00:42:03,500
update on the leading 8 bytes of this

00:42:01,410 --> 00:42:06,750
data structure which

00:42:03,500 --> 00:42:08,640
happily for us was designed for us to do

00:42:06,750 --> 00:42:11,280
this all of the scary private data

00:42:08,640 --> 00:42:13,110
fields are the trailing fields so we can

00:42:11,280 --> 00:42:15,720
just chop them off it's really awesome

00:42:13,110 --> 00:42:17,400
that it works and then we start the

00:42:15,720 --> 00:42:19,860
timer and return the future that we

00:42:17,400 --> 00:42:22,490
spawn off the promise and then likewise

00:42:19,860 --> 00:42:25,700
the actual callback that we pass in to

00:42:22,490 --> 00:42:28,590
when this is done is even simpler right

00:42:25,700 --> 00:42:30,450
we get the timer handle back is the sole

00:42:28,590 --> 00:42:33,180
argument and then we just dereference it

00:42:30,450 --> 00:42:34,920
which gives us back the timer ID we just

00:42:33,180 --> 00:42:38,400
store it in that that one leading 8 byte

00:42:34,920 --> 00:42:42,990
field and then we pull the promise out

00:42:38,400 --> 00:42:45,570
of our map using the timer ID that we

00:42:42,990 --> 00:42:48,150
just dereferenced we we remove the the

00:42:45,570 --> 00:42:51,090
promise from the map and then we we

00:42:48,150 --> 00:42:52,890
succeed the promise with with unit and

00:42:51,090 --> 00:42:56,280
that's literally all it takes and then

00:42:52,890 --> 00:42:59,940
this just works the first time it did it

00:42:56,280 --> 00:43:01,010
felt a little magical but it this is all

00:42:59,940 --> 00:43:03,120
it takes and then you just have a

00:43:01,010 --> 00:43:04,170
there's all the scary stuff going

00:43:03,120 --> 00:43:07,580
underneath but you didn't you have

00:43:04,170 --> 00:43:11,640
idiomatic safe Scala that's just usable

00:43:07,580 --> 00:43:13,890
so so where do we go from here

00:43:11,640 --> 00:43:15,630
like I said I'm trying to improve

00:43:13,890 --> 00:43:17,760
support and Scala native for

00:43:15,630 --> 00:43:20,930
user-supplied event loops to make like

00:43:17,760 --> 00:43:24,120
the loop run invocation less intrusive

00:43:20,930 --> 00:43:26,700
getting good integration with stdp is a

00:43:24,120 --> 00:43:29,160
really high priority for me it's one of

00:43:26,700 --> 00:43:30,930
my favorite libraries and I think it

00:43:29,160 --> 00:43:34,020
does a great job at this

00:43:30,930 --> 00:43:36,630
Pavel's ruski already did a great stdp

00:43:34,020 --> 00:43:39,900
native curl binding actually for the

00:43:36,630 --> 00:43:42,450
blocking SVP api so i'd love to get

00:43:39,900 --> 00:43:44,550
these consolidated I'd like to spin out

00:43:42,450 --> 00:43:47,520
the high level server API I think it's

00:43:44,550 --> 00:43:49,140
great to have something like now here

00:43:47,520 --> 00:43:52,230
for now but in the long run I don't

00:43:49,140 --> 00:43:54,780
think like a server DSL belongs like in

00:43:52,230 --> 00:43:56,610
a core Scala native package and it makes

00:43:54,780 --> 00:43:58,410
sense once we have an ecosystem just

00:43:56,610 --> 00:44:01,080
spend spin that out entirely and let the

00:43:58,410 --> 00:44:02,760
community take it over and then I really

00:44:01,080 --> 00:44:05,540
want to get feedback on the design of

00:44:02,760 --> 00:44:08,150
the streaming io API not just like

00:44:05,540 --> 00:44:11,940
design critiques but really like the

00:44:08,150 --> 00:44:14,310
effort and the time of implementing like

00:44:11,940 --> 00:44:16,730
zo and Kats effect and stuff on top of

00:44:14,310 --> 00:44:18,740
this and see if that gives

00:44:16,730 --> 00:44:22,250
any guidance towards sort of the next

00:44:18,740 --> 00:44:24,710
round of of iteration on this all of

00:44:22,250 --> 00:44:27,700
that said we would love to have a lot

00:44:24,710 --> 00:44:30,770
more contributors to make this work

00:44:27,700 --> 00:44:33,710
there's so many low-hanging fruit out

00:44:30,770 --> 00:44:35,810
there for Scala native and there's a lot

00:44:33,710 --> 00:44:38,660
of like weekend sized projects that can

00:44:35,810 --> 00:44:40,160
have a really really huge impact it's a

00:44:38,660 --> 00:44:41,210
it's a really exciting moment where

00:44:40,160 --> 00:44:44,510
there's a lot of things that are

00:44:41,210 --> 00:44:46,700
possible and like our existing

00:44:44,510 --> 00:44:48,650
contributor community is amazing and

00:44:46,700 --> 00:44:50,630
passionate and they've done phenomenal

00:44:48,650 --> 00:44:54,290
things none of this will be possible

00:44:50,630 --> 00:44:55,700
without them and yeah if anyone's here

00:44:54,290 --> 00:44:57,740
and thanks oh gee I could write a better

00:44:55,700 --> 00:45:00,170
server DSL than that and forty-five

00:44:57,740 --> 00:45:02,240
minutes please please do come talk to me

00:45:00,170 --> 00:45:04,010
or just get it out there like get

00:45:02,240 --> 00:45:06,110
involved I we would love to have you

00:45:04,010 --> 00:45:08,990
where we have a great get er chat drop

00:45:06,110 --> 00:45:16,970
on by and yeah that's what I got thank

00:45:08,990 --> 00:45:27,320
you thank you any questions to our

00:45:16,970 --> 00:45:29,750
native speaker I actually have lots of

00:45:27,320 --> 00:45:31,880
questions but just raise one yeah you

00:45:29,750 --> 00:45:35,060
didn't mention garbage collection yeah

00:45:31,880 --> 00:45:37,670
how does that happen they're so ordinary

00:45:35,060 --> 00:45:39,230
Scala Valley so Scala native has a state

00:45:37,670 --> 00:45:41,570
of the art garbage collector that

00:45:39,230 --> 00:45:44,960
performs a little bit better than than

00:45:41,570 --> 00:45:46,370
hot spot right really nice and Dennis's

00:45:44,960 --> 00:45:48,100
paper on that it would be the reference

00:45:46,370 --> 00:45:51,740
I'm not an expert on garbage collection

00:45:48,100 --> 00:45:54,110
but that scholars have been garbage

00:45:51,740 --> 00:45:56,540
exactly where as pointers are not

00:45:54,110 --> 00:45:57,860
however pointer the new thing in Scala

00:45:56,540 --> 00:46:01,400
native oh not for is that you can

00:45:57,860 --> 00:46:02,930
actually box a pointer right so the the

00:46:01,400 --> 00:46:04,340
contents of the pointer won't be got

00:46:02,930 --> 00:46:07,220
garbage collected but then you can

00:46:04,340 --> 00:46:10,040
actually store a pointer as a value in a

00:46:07,220 --> 00:46:12,290
hashmap like I did so it allows you to

00:46:10,040 --> 00:46:14,660
sort of mix the domains of unsafe values

00:46:12,290 --> 00:46:16,520
and Scala values in a way that's it's

00:46:14,660 --> 00:46:18,290
it's actually new it's like I'm

00:46:16,520 --> 00:46:20,120
basically rewriting a lot of the book

00:46:18,290 --> 00:46:21,860
around this technique but it's

00:46:20,120 --> 00:46:23,390
definitely the the state of the art all

00:46:21,860 --> 00:46:26,780
right I'll raise the second question

00:46:23,390 --> 00:46:30,110
yeah thankfully you use it because it

00:46:26,780 --> 00:46:35,240
sounds like a good use case for tasty

00:46:30,110 --> 00:46:38,480
probably use it where for compiling for

00:46:35,240 --> 00:46:40,940
compiling I know Dennis has like an

00:46:38,480 --> 00:46:43,550
experimental fork of Scala C that runs

00:46:40,940 --> 00:46:45,380
in Scala native so that's been that's

00:46:43,550 --> 00:46:46,850
been that's been demonstrated but I

00:46:45,380 --> 00:46:49,400
don't think it's merged yet

00:46:46,850 --> 00:46:51,770
I think Scala format actually does run

00:46:49,400 --> 00:46:54,080
in Scala native now so having like tools

00:46:51,770 --> 00:46:55,850
like that run in Scala native where you

00:46:54,080 --> 00:46:58,400
get that really quick start up time is

00:46:55,850 --> 00:47:00,410
one area of immediate impact right not

00:46:58,400 --> 00:47:02,090
having to pay that like Java started

00:47:00,410 --> 00:47:04,280
obviously like Jorge's work on bloop

00:47:02,090 --> 00:47:06,020
makes it a lot less painful to deal with

00:47:04,280 --> 00:47:08,240
with SBT and stuff like that for

00:47:06,020 --> 00:47:10,940
developer tooling but even just like the

00:47:08,240 --> 00:47:12,920
lower memory overhead of Scala native

00:47:10,940 --> 00:47:27,230
really makes an impact there so that's

00:47:12,920 --> 00:47:29,180
that's an exciting yeah so with no more

00:47:27,230 --> 00:47:30,140
questions thank you very much once again

00:47:29,180 --> 00:47:36,819
thank you so much

00:47:30,140 --> 00:47:36,819

YouTube URL: https://www.youtube.com/watch?v=ptE42nqw4Ho


