Title: Massively Parallel Distributed Scala Compilation And You - Stu Hood
Publication date: 2019-07-11
Playlist: Scala Days Lausanne 2019
Description: 
	This video was recorded at Scala Days Lausanne 2019
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

More information and the abstract can be found here:
https://scaladays.org/schedule/massively-parallel-distributed-scala-compilation----and-you
Captions: 
	00:00:01,399 --> 00:00:07,170
awesome so it's a big venue and a lot of

00:00:05,190 --> 00:00:09,030
people are still gonna be filing in but

00:00:07,170 --> 00:00:11,010
we might as well get started or maybe

00:00:09,030 --> 00:00:14,000
they won't but that's fine

00:00:11,010 --> 00:00:16,470
welcome I'm gonna be talking about

00:00:14,000 --> 00:00:20,070
massively parallel distributed Scala

00:00:16,470 --> 00:00:22,260
compilation and you and I I want to

00:00:20,070 --> 00:00:24,180
really stick to the and new part I

00:00:22,260 --> 00:00:25,769
believe that this is relevant to

00:00:24,180 --> 00:00:27,960
absolutely everyone who builds scholar

00:00:25,769 --> 00:00:30,539
or builds code in general it's not just

00:00:27,960 --> 00:00:32,130
stuff that Twitter's doing so if you

00:00:30,539 --> 00:00:33,360
don't have that take away and ask me

00:00:32,130 --> 00:00:36,719
questions at the end to help me clarify

00:00:33,360 --> 00:00:39,390
that so a little bit about myself I'm

00:00:36,719 --> 00:00:42,120
the tech lead of Twitter's build team as

00:00:39,390 --> 00:00:43,500
of about a year ago and I hope to tag

00:00:42,120 --> 00:00:45,210
team out with one of the other excellent

00:00:43,500 --> 00:00:48,210
folks on our team so that I can code a

00:00:45,210 --> 00:00:51,180
little bit more but Twitter is still

00:00:48,210 --> 00:00:52,170
very much a scholar shop and we have

00:00:51,180 --> 00:00:54,059
more than 10 million lines of

00:00:52,170 --> 00:00:56,730
handwritten scholar code and many

00:00:54,059 --> 00:00:59,730
millions of lines of generated Scala

00:00:56,730 --> 00:01:02,309
code that hasn't really changed the the

00:00:59,730 --> 00:01:05,100
ratios between the code bases that we

00:01:02,309 --> 00:01:06,659
have has stayed pretty steady Scala has

00:01:05,100 --> 00:01:10,380
continued to grow at a rate faster than

00:01:06,659 --> 00:01:14,460
most other languages we have but what is

00:01:10,380 --> 00:01:17,549
the trajectory of building code and in

00:01:14,460 --> 00:01:19,380
particular maybe building Scala code we

00:01:17,549 --> 00:01:21,180
should these build should contain a lot

00:01:19,380 --> 00:01:24,540
more Scala code that would be great

00:01:21,180 --> 00:01:26,369
maybe some some Scala 3 code and they

00:01:24,540 --> 00:01:27,720
should be a lot faster but that

00:01:26,369 --> 00:01:29,759
represents a little bit of a paradox

00:01:27,720 --> 00:01:34,619
right now because Scala code takes a

00:01:29,759 --> 00:01:36,180
while to compile that's fine Scala the

00:01:34,619 --> 00:01:37,920
rule of thumb is sort of that Scala it

00:01:36,180 --> 00:01:40,229
takes something like 10x the time of

00:01:37,920 --> 00:01:42,030
Java to compile right but there's a long

00:01:40,229 --> 00:01:44,100
tradition of high level languages taking

00:01:42,030 --> 00:01:48,180
longer than their low-level counterparts

00:01:44,100 --> 00:01:49,740
to compile C and C++ and then things

00:01:48,180 --> 00:01:51,570
like Haskell taking a long time rust

00:01:49,740 --> 00:01:54,090
providing a whole bunch of abstractions

00:01:51,570 --> 00:01:56,130
that that cost you a little bit in terms

00:01:54,090 --> 00:02:00,750
of performance but we'd like them also

00:01:56,130 --> 00:02:04,259
to be faster so the status quo builds

00:02:00,750 --> 00:02:06,210
today an open source is that in CI maybe

00:02:04,259 --> 00:02:09,390
it's Travis or drone or something you

00:02:06,210 --> 00:02:11,879
have some amount of caching and CI times

00:02:09,390 --> 00:02:13,500
that probably float up into the 15

00:02:11,879 --> 00:02:17,400
minute range

00:02:13,500 --> 00:02:19,350
there are there are amounts of time that

00:02:17,400 --> 00:02:22,520
people are comfortable waiting and you

00:02:19,350 --> 00:02:25,350
sort of have these natural plateaus on

00:02:22,520 --> 00:02:27,450
laptops you might have cold builds that

00:02:25,350 --> 00:02:29,730
take minutes and you don't really have

00:02:27,450 --> 00:02:32,280
an expectation that if you are checking

00:02:29,730 --> 00:02:33,810
something out that you have to build it

00:02:32,280 --> 00:02:38,070
you expect that you will have to build

00:02:33,810 --> 00:02:39,660
it and it's probably the case that the

00:02:38,070 --> 00:02:42,750
project sizes that are built in

00:02:39,660 --> 00:02:44,910
open-source are lightly constrained by

00:02:42,750 --> 00:02:46,170
this because the amount of

00:02:44,910 --> 00:02:49,950
infrastructure you just get out of the

00:02:46,170 --> 00:02:52,860
box isn't really there and so if you

00:02:49,950 --> 00:02:54,660
wanted to your CI - - if you wanted to

00:02:52,860 --> 00:02:56,400
build significantly more code and still

00:02:54,660 --> 00:02:57,900
have your CI take 15 minutes you'd have

00:02:56,400 --> 00:03:00,960
to build a lot of infrastructure to make

00:02:57,900 --> 00:03:04,500
that happen and if you wanted your cold

00:03:00,960 --> 00:03:06,810
builds on a laptop to continue to stay

00:03:04,500 --> 00:03:08,310
at a comfortable level as the project

00:03:06,810 --> 00:03:11,550
got larger larger you'd have to build a

00:03:08,310 --> 00:03:15,270
lot of infrastructure and that probably

00:03:11,550 --> 00:03:16,680
puts a cap on how large projects get at

00:03:15,270 --> 00:03:19,650
Twitter we've built some of that

00:03:16,680 --> 00:03:21,090
infrastructure so although we have many

00:03:19,650 --> 00:03:23,459
millions of lines of code that the

00:03:21,090 --> 00:03:27,480
submit to master step is still about 15

00:03:23,459 --> 00:03:29,930
minutes and in some cases lower we have

00:03:27,480 --> 00:03:32,040
parallel builds with distributed caching

00:03:29,930 --> 00:03:35,790
in the sense that when you're building

00:03:32,040 --> 00:03:41,640
that mono repo and it contains something

00:03:35,790 --> 00:03:43,380
like 30,000 targets or projects it the

00:03:41,640 --> 00:03:48,140
worst case bill disk is something like

00:03:43,380 --> 00:03:51,959
45 minutes across 700 issue machines and

00:03:48,140 --> 00:03:53,580
then on laptops cold builds mostly hit

00:03:51,959 --> 00:03:55,310
the cache cold in the sense that I have

00:03:53,580 --> 00:03:58,470
not personally built something that

00:03:55,310 --> 00:04:02,730
someone else has built in particular ci

00:03:58,470 --> 00:04:04,860
has built and but completely missing the

00:04:02,730 --> 00:04:06,750
cache might mean a much longer built

00:04:04,860 --> 00:04:08,340
when do you completely miss the cache

00:04:06,750 --> 00:04:11,489
well you've changed something completely

00:04:08,340 --> 00:04:15,300
fundamental locally that has never been

00:04:11,489 --> 00:04:16,799
built in CI and building from sources a

00:04:15,300 --> 00:04:19,709
feature you want to be able to change

00:04:16,799 --> 00:04:22,140
something very fundamental in one step

00:04:19,709 --> 00:04:24,840
and then see the effect on your codebase

00:04:22,140 --> 00:04:26,430
it's it's extremely powerful from an

00:04:24,840 --> 00:04:29,039
integration perspective

00:04:26,430 --> 00:04:32,419
have everything tested like that you can

00:04:29,039 --> 00:04:37,410
make tiny changes and see their impact

00:04:32,419 --> 00:04:39,470
dozens of projects away but it means

00:04:37,410 --> 00:04:41,729
that you have to have very good cash so

00:04:39,470 --> 00:04:44,729
currently it's a manual step to get CI

00:04:41,729 --> 00:04:47,160
involved at Twitter you run arc sandbox

00:04:44,729 --> 00:04:49,410
or you run arc diff and that basically

00:04:47,160 --> 00:04:52,949
tells the code review system that you'd

00:04:49,410 --> 00:04:55,850
like to either try out do a dry run of

00:04:52,949 --> 00:04:58,110
landing something or post it and

00:04:55,850 --> 00:05:02,400
simultaneously do a dry run of landing

00:04:58,110 --> 00:05:04,080
it without actually landing it and this

00:05:02,400 --> 00:05:05,490
manual step to get CI involved is also

00:05:04,080 --> 00:05:09,509
the case and open source going back a

00:05:05,490 --> 00:05:13,080
little bit in that you will eventually

00:05:09,509 --> 00:05:14,820
learn oh sorry and we have recent

00:05:13,080 --> 00:05:17,160
hardware a recent hardware refresh that

00:05:14,820 --> 00:05:20,639
is finishing rolling out of six core i9

00:05:17,160 --> 00:05:23,190
MacBooks which were nice but the

00:05:20,639 --> 00:05:25,470
downsides of the the builds today are

00:05:23,190 --> 00:05:28,590
you probably don't want to carry more

00:05:25,470 --> 00:05:30,539
than six cores on your back and in but

00:05:28,590 --> 00:05:31,889
also in theory you could get a lot more

00:05:30,539 --> 00:05:33,539
performance if you carried more than six

00:05:31,889 --> 00:05:36,840
cores with you and if they were

00:05:33,539 --> 00:05:39,960
commodity if you have distributed

00:05:36,840 --> 00:05:42,120
caching only builds that run and see I

00:05:39,960 --> 00:05:45,510
probably populate it

00:05:42,120 --> 00:05:46,770
so if yeah and then caches are also

00:05:45,510 --> 00:05:48,780
likely uncoordinated if you're using

00:05:46,770 --> 00:05:51,690
something like Travis or drone each each

00:05:48,780 --> 00:05:53,780
shard might have its own cache and it it

00:05:51,690 --> 00:05:55,979
writes to the thing and then at the end

00:05:53,780 --> 00:05:57,740
uploads and downloads that the next time

00:05:55,979 --> 00:06:01,380
it needs to run that shard for example

00:05:57,740 --> 00:06:06,300
and getting more fine-grained than that

00:06:01,380 --> 00:06:07,530
can be hard to do correctly so and as I

00:06:06,300 --> 00:06:09,810
said it's a manual step to get more

00:06:07,530 --> 00:06:12,630
machines involved so users sort of

00:06:09,810 --> 00:06:14,430
learned where that's that boundary is

00:06:12,630 --> 00:06:16,380
where is it reasonable like I don't want

00:06:14,430 --> 00:06:19,770
to wait five minutes for these tests and

00:06:16,380 --> 00:06:22,139
I've learned I've learned what not to do

00:06:19,770 --> 00:06:23,729
because I accidentally used the

00:06:22,139 --> 00:06:25,979
instructions that were a little old on

00:06:23,729 --> 00:06:27,599
the site that said build the whole

00:06:25,979 --> 00:06:29,310
codebase than the entire test every time

00:06:27,599 --> 00:06:31,349
you want to do something and that slowly

00:06:29,310 --> 00:06:34,050
grew up to ten minutes worth of worth of

00:06:31,349 --> 00:06:35,580
time so they slowly learn like okay

00:06:34,050 --> 00:06:38,250
that's the thing I can run locally

00:06:35,580 --> 00:06:42,150
that's the thing I can't run locally

00:06:38,250 --> 00:06:43,470
and that's unfortunate so what we'd like

00:06:42,150 --> 00:06:46,140
to do is improve this workflow

00:06:43,470 --> 00:06:48,500
transparently and I say transparently

00:06:46,140 --> 00:06:50,550
because it's definitely the case that

00:06:48,500 --> 00:06:52,710
remote dev machines are this very

00:06:50,550 --> 00:06:55,680
interesting thing and in particular the

00:06:52,710 --> 00:06:58,500
vs code mode that they announced

00:06:55,680 --> 00:07:01,110
recently is very interesting because it

00:06:58,500 --> 00:07:05,160
means that if you have a pet box

00:07:01,110 --> 00:07:07,140
somewhere or are willing to pay for like

00:07:05,160 --> 00:07:10,620
long long running VMs that you connect

00:07:07,140 --> 00:07:11,700
to you can have a lot more course but

00:07:10,620 --> 00:07:13,440
we'd like to do better because that

00:07:11,700 --> 00:07:15,660
doesn't that means changing your

00:07:13,440 --> 00:07:17,850
workflow to it to some degree and then

00:07:15,660 --> 00:07:21,330
also having a pet machine somewhere

00:07:17,850 --> 00:07:25,530
that's hosting your code so we'd like to

00:07:21,330 --> 00:07:26,850
head toward a world where yeah we're the

00:07:25,530 --> 00:07:28,050
explicit remote steps are less necessary

00:07:26,850 --> 00:07:29,550
eventually you're going to post your

00:07:28,050 --> 00:07:31,440
code for review maybe that's still a

00:07:29,550 --> 00:07:33,630
remote step but in the meantime it'd be

00:07:31,440 --> 00:07:35,550
nice if you did not have to do anything

00:07:33,630 --> 00:07:37,350
to verify anything beyond what your

00:07:35,550 --> 00:07:40,650
usual workflow would be to verify that

00:07:37,350 --> 00:07:42,780
it's correct until projects catch up

00:07:40,650 --> 00:07:47,640
again every time you make things faster

00:07:42,780 --> 00:07:50,640
projects catch up by growing larger so

00:07:47,640 --> 00:07:51,450
this is a this is a another talk in a

00:07:50,640 --> 00:07:55,320
continuing story

00:07:51,450 --> 00:08:01,560
most recently Eugene gave a talk eugene

00:07:55,320 --> 00:08:04,380
bamako whoo moment of silence gave a

00:08:01,560 --> 00:08:07,050
talk and scale by the bay about RSC and

00:08:04,380 --> 00:08:09,270
that that will play a part here Twitter

00:08:07,050 --> 00:08:12,660
also I think has another talk in this

00:08:09,270 --> 00:08:14,190
conference about growth and I'm not

00:08:12,660 --> 00:08:15,720
going to talk about grow the run time

00:08:14,190 --> 00:08:18,900
I'm gonna talk about another component

00:08:15,720 --> 00:08:20,330
of girl and then also I previously

00:08:18,900 --> 00:08:24,500
discussed how Twitter builds source

00:08:20,330 --> 00:08:26,760
Scala from source and it's relevant here

00:08:24,500 --> 00:08:29,190
so one thing I want to say though is

00:08:26,760 --> 00:08:33,630
that this is what I'm presenting is a

00:08:29,190 --> 00:08:36,780
very long running project and we are on

00:08:33,630 --> 00:08:39,450
the cusp of having excellent results

00:08:36,780 --> 00:08:41,340
from it and but these are preliminary

00:08:39,450 --> 00:08:43,880
results to sort of stay tuned for the

00:08:41,340 --> 00:08:43,880
rest of that

00:08:44,499 --> 00:08:50,509
Coolio so the project is code named

00:08:47,509 --> 00:08:52,970
aquila what is clearly a-- aquellia is a

00:08:50,509 --> 00:08:56,239
bird it is the most numerous bird in the

00:08:52,970 --> 00:09:01,100
world which is a fun fact there's a

00:08:56,239 --> 00:09:04,009
red-billed quelea in africa they destroy

00:09:01,100 --> 00:09:05,239
fields so quickly that farmers basically

00:09:04,009 --> 00:09:07,339
have to stand out there and manually

00:09:05,239 --> 00:09:10,009
like whack whack them with sticks to get

00:09:07,339 --> 00:09:12,199
them off the crops the farmers will blow

00:09:10,009 --> 00:09:14,660
up trees at night because the it's this

00:09:12,199 --> 00:09:16,790
herd of you know heard of locusts but

00:09:14,660 --> 00:09:22,899
they in parallel do something very

00:09:16,790 --> 00:09:25,910
quickly so let's let's destroy this code

00:09:22,899 --> 00:09:28,129
so fundamentally we want to change how

00:09:25,910 --> 00:09:29,749
we invoke compilers in order to unlock

00:09:28,129 --> 00:09:32,689
additional parallelism and so Eugene's

00:09:29,749 --> 00:09:36,319
talk gave a little bit about this the

00:09:32,689 --> 00:09:40,369
outlining that RSC does is a is a key

00:09:36,319 --> 00:09:42,649
component of this and then also using

00:09:40,369 --> 00:09:45,410
that foundation executing the compilers

00:09:42,649 --> 00:09:47,779
remotely and transparently right this is

00:09:45,410 --> 00:09:51,739
not a dead box this is I ran my build

00:09:47,779 --> 00:09:54,739
tool and some of the the execution was

00:09:51,739 --> 00:09:57,019
not on my laptop and initially this is

00:09:54,739 --> 00:09:58,309
only the JVM and only compiles but

00:09:57,019 --> 00:10:00,049
there's no reason this can't extend to

00:09:58,309 --> 00:10:02,959
all the other processes in your pipeline

00:10:00,049 --> 00:10:05,449
there's there's prior art in in basil

00:10:02,959 --> 00:10:10,189
and at Google and other places so what

00:10:05,449 --> 00:10:14,749
is outlining so outlining in this case

00:10:10,189 --> 00:10:17,079
in kulia is implemented by RSC and it is

00:10:14,749 --> 00:10:20,179
what I'm calling external outlining

00:10:17,079 --> 00:10:23,769
basically the compiler extracts and

00:10:20,179 --> 00:10:27,019
serializes headers from your code and

00:10:23,769 --> 00:10:29,269
the and serializes part is what makes it

00:10:27,019 --> 00:10:29,869
external right you run a tool on your

00:10:29,269 --> 00:10:33,709
source files

00:10:29,869 --> 00:10:35,629
it emits something that's binary but can

00:10:33,709 --> 00:10:39,639
be persisted and then passed between

00:10:35,629 --> 00:10:44,239
machines and allows you to launch

00:10:39,639 --> 00:10:48,439
dependent compiles before you've

00:10:44,239 --> 00:10:49,429
finished compiling the dependency and

00:10:48,439 --> 00:10:51,529
that's also known as pipeline

00:10:49,429 --> 00:10:56,089
compilation so I'm gonna I'm gonna

00:10:51,529 --> 00:10:57,649
straight up use Eugene's slides on this

00:10:56,089 --> 00:10:59,869
section so

00:10:57,649 --> 00:11:01,490
given this example you have a target a

00:10:59,869 --> 00:11:04,550
that takes two seconds to compile a

00:11:01,490 --> 00:11:06,889
target B that takes one-and-a-half and

00:11:04,550 --> 00:11:07,399
Scott and a target see that takes one

00:11:06,889 --> 00:11:09,410
second

00:11:07,399 --> 00:11:11,990
finally a target that takes one and a

00:11:09,410 --> 00:11:16,670
half and we have dependencies between

00:11:11,990 --> 00:11:20,749
them so B and C both depend on a they

00:11:16,670 --> 00:11:22,759
can't start until a has completed and D

00:11:20,749 --> 00:11:25,009
depends on C and can't start until C is

00:11:22,759 --> 00:11:27,850
completed compiling right so these are

00:11:25,009 --> 00:11:30,649
the compiler running and then completing

00:11:27,850 --> 00:11:32,269
we refer to or this is referred to as

00:11:30,649 --> 00:11:35,179
one phase compilation because there's no

00:11:32,269 --> 00:11:37,819
header extraction happening here but if

00:11:35,179 --> 00:11:40,369
you extract headers and imagine that

00:11:37,819 --> 00:11:41,779
extracting headers takes about 50% of

00:11:40,369 --> 00:11:45,490
the amount of time it would take to

00:11:41,779 --> 00:11:52,100
compile something you see this graph

00:11:45,490 --> 00:11:55,189
this Gantt chart and effectively the

00:11:52,100 --> 00:11:57,379
compile of B and C only depends on the

00:11:55,189 --> 00:11:58,699
outline of a you don't have to finish

00:11:57,379 --> 00:12:01,449
compiling a before you can start

00:11:58,699 --> 00:12:04,490
compiling B and C

00:12:01,449 --> 00:12:09,290
additionally the outlining of C only

00:12:04,490 --> 00:12:11,929
depends on the outlining of a which

00:12:09,290 --> 00:12:17,209
means that the compilation of D can

00:12:11,929 --> 00:12:19,699
start much sooner so that's interesting

00:12:17,209 --> 00:12:22,100
more interesting is that if outlining is

00:12:19,699 --> 00:12:25,970
significantly faster

00:12:22,100 --> 00:12:28,160
you've sped up your entire build from

00:12:25,970 --> 00:12:30,290
starting at 4.5 seconds to 2 seconds

00:12:28,160 --> 00:12:31,939
right this is a hypothetical but it's

00:12:30,290 --> 00:12:33,499
really interesting the other thing

00:12:31,939 --> 00:12:34,879
that's interesting is that we're

00:12:33,499 --> 00:12:37,100
allowing more things to happen in

00:12:34,879 --> 00:12:38,569
parallel at the end here all for these

00:12:37,100 --> 00:12:43,100
compiles are actually happening almost

00:12:38,569 --> 00:12:44,949
entirely in parallel and I believe I'm

00:12:43,100 --> 00:12:48,379
using that word correctly

00:12:44,949 --> 00:12:50,120
okay so outlining part obviously it's

00:12:48,379 --> 00:12:53,959
it's like extracting headers well what

00:12:50,120 --> 00:12:57,139
are headers headers C++ for all because

00:12:53,959 --> 00:13:00,470
C++ has handwritten headers it is the

00:12:57,139 --> 00:13:04,189
case that after a cecum C++ compilation

00:13:00,470 --> 00:13:05,809
precedes it walks through all the

00:13:04,189 --> 00:13:07,670
headers and then is able to compile all

00:13:05,809 --> 00:13:11,120
the files in parallel right so and

00:13:07,670 --> 00:13:12,980
that's where for me

00:13:11,120 --> 00:13:14,720
the connection to Scala compilation

00:13:12,980 --> 00:13:17,990
clicked after talking to somebody about

00:13:14,720 --> 00:13:19,819
how the post post compilation works but

00:13:17,990 --> 00:13:22,249
Google has also done a little bit in

00:13:19,819 --> 00:13:25,249
this area turbine is an open source

00:13:22,249 --> 00:13:27,769
header compiler for Java and so it's

00:13:25,249 --> 00:13:33,559
it's analogous to what RSC does for

00:13:27,769 --> 00:13:36,230
Scala it emits tiny jars that contained

00:13:33,559 --> 00:13:39,050
just enough to fool the compiler that

00:13:36,230 --> 00:13:40,999
depends on you into thinking that you've

00:13:39,050 --> 00:13:44,660
already been compiled and RC does a

00:13:40,999 --> 00:13:46,309
similar thing using just the Scala

00:13:44,660 --> 00:13:49,579
signatures rather than full class wells

00:13:46,309 --> 00:13:51,430
and then Scala also recently and this is

00:13:49,579 --> 00:13:54,350
mostly unlike the last year and a half

00:13:51,430 --> 00:13:55,639
has had a series of steps in the

00:13:54,350 --> 00:13:57,290
direction of outlining and pipelining

00:13:55,639 --> 00:14:00,709
outlining and pipelining are very

00:13:57,290 --> 00:14:05,629
interesting so Kentucky Mule did

00:14:00,709 --> 00:14:08,689
outlining types or outline types so

00:14:05,629 --> 00:14:12,230
similar a similar situation pipelining

00:14:08,689 --> 00:14:13,850
in bloop based on using the it's just

00:14:12,230 --> 00:14:16,550
the first few phases of the compilers

00:14:13,850 --> 00:14:18,170
and then using the symbol table of the

00:14:16,550 --> 00:14:20,569
compiler and then using the symbol table

00:14:18,170 --> 00:14:25,519
from the compile as an input to the the

00:14:20,569 --> 00:14:27,199
other compilers and then yum had an

00:14:25,519 --> 00:14:29,959
implementation as well for dotty and

00:14:27,199 --> 00:14:32,029
then I heard just yesterday that Jason's

00:14:29,959 --> 00:14:34,670
dog has had has done recent work in

00:14:32,029 --> 00:14:36,529
Scala C so that's really exciting and

00:14:34,670 --> 00:14:37,639
I'll at the end I'll sort of talk about

00:14:36,529 --> 00:14:41,300
why I think this is particularly

00:14:37,639 --> 00:14:44,179
exciting but in quia we're using

00:14:41,300 --> 00:14:47,029
reasoning RC the other component of this

00:14:44,179 --> 00:14:48,290
is remote execution right we have this

00:14:47,029 --> 00:14:49,879
new parallelism but how do we take

00:14:48,290 --> 00:14:51,170
advantage of it right we don't we're not

00:14:49,879 --> 00:14:54,589
carrying more than six cores on our

00:14:51,170 --> 00:14:57,439
backs and shouldn't so the remote

00:14:54,589 --> 00:15:01,850
execution strategy is to have clients

00:14:57,439 --> 00:15:04,910
that talk to you an API let me let me go

00:15:01,850 --> 00:15:08,059
forward a little bit in in this model

00:15:04,910 --> 00:15:10,759
we're using a standard actually basil

00:15:08,059 --> 00:15:13,519
has a remote execution API standard

00:15:10,759 --> 00:15:15,790
second version that's implemented by

00:15:13,519 --> 00:15:18,160
four or five different clients including

00:15:15,790 --> 00:15:20,660
one I'll talk about it in a second and

00:15:18,160 --> 00:15:23,179
four five different servers again one of

00:15:20,660 --> 00:15:24,560
which I'll talk about in a second it's

00:15:23,179 --> 00:15:27,230
roughly invoke the

00:15:24,560 --> 00:15:30,560
Yunis UNIX process in the context of

00:15:27,230 --> 00:15:33,320
this content-addressable store digest ie

00:15:30,560 --> 00:15:35,930
unique identifiers for a collection of

00:15:33,320 --> 00:15:38,120
files and give me back the digest of the

00:15:35,930 --> 00:15:39,980
outputs again a unique identifier for a

00:15:38,120 --> 00:15:41,390
collection of files you can sort of

00:15:39,980 --> 00:15:43,400
think about the inputs and outputs as

00:15:41,390 --> 00:15:47,750
zip files except significantly more

00:15:43,400 --> 00:15:49,880
efficiently stored so going back to the

00:15:47,750 --> 00:15:52,430
diagram clients would would use this API

00:15:49,880 --> 00:15:54,440
to communicate with the API which would

00:15:52,430 --> 00:15:57,050
talk back and forth with the content

00:15:54,440 --> 00:15:58,910
addressable storage which is the rough

00:15:57,050 --> 00:16:00,200
equivalent of s3 if you have something

00:15:58,910 --> 00:16:02,660
very tiny maybe you use a different

00:16:00,200 --> 00:16:04,279
database for the very tiny stuff and

00:16:02,660 --> 00:16:07,220
then a pool of workers and the workers

00:16:04,279 --> 00:16:09,589
are all going to invoke UNIX processes

00:16:07,220 --> 00:16:13,520
right basic things and in this case

00:16:09,589 --> 00:16:18,130
we're invoking Ora C and zinc where zinc

00:16:13,520 --> 00:16:23,450
is the incremental compiler library for

00:16:18,130 --> 00:16:27,410
Scala and is what Twitter uses in

00:16:23,450 --> 00:16:31,910
general so the client in this case is

00:16:27,410 --> 00:16:33,560
pants our case we overhauled the engine

00:16:31,910 --> 00:16:35,839
of pants to support snapshotting of

00:16:33,560 --> 00:16:38,089
inputs basically building those those

00:16:35,839 --> 00:16:39,770
content-addressable digests of the

00:16:38,089 --> 00:16:43,490
inputs as just a native kind of

00:16:39,770 --> 00:16:47,240
assumption so that the execution of

00:16:43,490 --> 00:16:49,460
pants relies on their existence we have

00:16:47,240 --> 00:16:52,660
about 10 percent of pants implemented

00:16:49,460 --> 00:16:56,720
and Russ now and it's mostly this layer

00:16:52,660 --> 00:17:00,170
when when you snapshot those inputs you

00:16:56,720 --> 00:17:02,360
put them in a local LM DB database and

00:17:00,170 --> 00:17:04,400
when you need to send them somewhere you

00:17:02,360 --> 00:17:08,630
take them from that database and upload

00:17:04,400 --> 00:17:13,220
them and the API is implemented as a G

00:17:08,630 --> 00:17:15,350
RPC API and all this is open source so

00:17:13,220 --> 00:17:17,620
if somebody wanted to link against this

00:17:15,350 --> 00:17:23,089
code and again using it today they could

00:17:17,620 --> 00:17:25,550
but also basil as as demonstrated by the

00:17:23,089 --> 00:17:27,589
name of the API implements this API as a

00:17:25,550 --> 00:17:31,280
client right so in theory this could

00:17:27,589 --> 00:17:32,750
also be basil and then servers so

00:17:31,280 --> 00:17:35,170
Twitter internally has had a system

00:17:32,750 --> 00:17:37,370
called scoot for a while that implements

00:17:35,170 --> 00:17:40,340
remote execution

00:17:37,370 --> 00:17:42,650
it basically has two modes in the past

00:17:40,340 --> 00:17:45,530
we've used it exclusively as get me a

00:17:42,650 --> 00:17:49,970
get workspace and then run whatever but

00:17:45,530 --> 00:17:51,680
sorry run a build tool probably but

00:17:49,970 --> 00:17:53,300
getting a get workspace is a very

00:17:51,680 --> 00:17:56,270
relatively expensive operation

00:17:53,300 --> 00:17:57,830
especially at Twitter but in general

00:17:56,270 --> 00:18:01,430
that's going to be maybe a gigabyte or

00:17:57,830 --> 00:18:05,170
something of stuff this alternate mode

00:18:01,430 --> 00:18:08,180
where it's basically just hosting the

00:18:05,170 --> 00:18:11,660
remote execution API is dealing with

00:18:08,180 --> 00:18:13,940
tiny processes invoked on exactly some

00:18:11,660 --> 00:18:17,350
small set of files right pretty

00:18:13,940 --> 00:18:19,940
different mode but this could also be

00:18:17,350 --> 00:18:22,970
one of the other implementations of the

00:18:19,940 --> 00:18:26,929
the backend of this API right Google has

00:18:22,970 --> 00:18:29,720
something called RBE in alpha now which

00:18:26,929 --> 00:18:31,250
is very interesting and they won't quite

00:18:29,720 --> 00:18:34,610
commit to weather it will definitely

00:18:31,250 --> 00:18:36,140
become a product but it might and then

00:18:34,610 --> 00:18:38,059
there are open source implementations

00:18:36,140 --> 00:18:39,800
called build farm and build barn from

00:18:38,059 --> 00:18:43,160
uber and live work right so their

00:18:39,800 --> 00:18:45,380
options the API is stateless and

00:18:43,160 --> 00:18:47,660
hermetic so the tools that you run

00:18:45,380 --> 00:18:50,240
inside of it have to declare everything

00:18:47,660 --> 00:18:51,770
they're going to use and have to declare

00:18:50,240 --> 00:18:55,730
what they want to capture on the other

00:18:51,770 --> 00:18:57,380
side and so generally that means you

00:18:55,730 --> 00:18:58,870
bring your own binaries because you

00:18:57,380 --> 00:19:02,720
don't want to assume anything about the

00:18:58,870 --> 00:19:05,300
execution environment all of the workers

00:19:02,720 --> 00:19:07,490
have little local caches of the the cast

00:19:05,300 --> 00:19:10,370
contents and so they won't have to go to

00:19:07,490 --> 00:19:14,720
the network to actually get the compiler

00:19:10,370 --> 00:19:16,790
for example or the JVM you can also sort

00:19:14,720 --> 00:19:19,850
of like bust a hole break the fourth

00:19:16,790 --> 00:19:22,280
wall and use whatever you know about

00:19:19,850 --> 00:19:25,340
your execution environment if it's not

00:19:22,280 --> 00:19:29,750
doctor iced or if it is da Christ for

00:19:25,340 --> 00:19:34,910
that matter to to sort of get a JVM if

00:19:29,750 --> 00:19:36,980
you want to but stateless and hermetic

00:19:34,910 --> 00:19:38,750
makes performance a little bit more

00:19:36,980 --> 00:19:42,850
challenging because the JVM likes to

00:19:38,750 --> 00:19:48,770
warm up so how do we get around that

00:19:42,850 --> 00:19:50,990
girl comes back in so we Danny in

00:19:48,770 --> 00:19:55,220
particular I'll talk talk about the

00:19:50,990 --> 00:19:58,370
our team later on worked on making

00:19:55,220 --> 00:19:59,900
native images for RSA and Zink and with

00:19:58,370 --> 00:20:01,429
a little bit of trial and error had

00:19:59,900 --> 00:20:05,720
great success

00:20:01,429 --> 00:20:08,809
it does have overhead but Grall native

00:20:05,720 --> 00:20:10,790
image compilation is a pretty nascent

00:20:08,809 --> 00:20:12,980
technology and so we expect that to get

00:20:10,790 --> 00:20:16,280
better but also what the amount of

00:20:12,980 --> 00:20:18,700
parallelism this unlocks makes makes

00:20:16,280 --> 00:20:20,840
this a non-issue for us right now

00:20:18,700 --> 00:20:22,850
there's a native image that you can run

00:20:20,840 --> 00:20:24,290
and if sorry a native image agent that

00:20:22,850 --> 00:20:26,690
you can run on your application to

00:20:24,290 --> 00:20:28,460
generate the config that you need to use

00:20:26,690 --> 00:20:32,360
the native image so the important and

00:20:28,460 --> 00:20:34,610
helpful so where are we now this system

00:20:32,360 --> 00:20:37,340
is working end to end as of the the last

00:20:34,610 --> 00:20:40,490
few weeks so we have an example of a

00:20:37,340 --> 00:20:41,660
real internal library and we could point

00:20:40,490 --> 00:20:42,980
it at a whole bunch of other ones but

00:20:41,660 --> 00:20:46,370
the ones I'm showing today are from this

00:20:42,980 --> 00:20:48,650
library I was surprised by how much

00:20:46,370 --> 00:20:49,580
generated go to contains but it contains

00:20:48,650 --> 00:20:51,260
4.5 million

00:20:49,580 --> 00:20:53,809
generated lines of Scala and three

00:20:51,260 --> 00:20:57,790
hundred handwritten lines of Scala and

00:20:53,809 --> 00:21:00,200
then 75k and two million lines of Java

00:20:57,790 --> 00:21:01,790
so obviously we've been profiling the

00:21:00,200 --> 00:21:04,670
heck out of this thing in the last few

00:21:01,790 --> 00:21:08,020
weeks and we've been using distributed

00:21:04,670 --> 00:21:10,490
tracing and we've been optimizing and

00:21:08,020 --> 00:21:13,610
before starting the project we we

00:21:10,490 --> 00:21:16,190
simulated what we expected to see and

00:21:13,610 --> 00:21:17,780
that's part of why we are so we we have

00:21:16,190 --> 00:21:20,840
been so excited about this project there

00:21:17,780 --> 00:21:22,070
this has a huge amount of promise and so

00:21:20,840 --> 00:21:23,870
what we've been doing is pushing down

00:21:22,070 --> 00:21:29,900
toward that hypothetical lower bound

00:21:23,870 --> 00:21:33,140
we've seen so what are we doing we've

00:21:29,900 --> 00:21:35,570
been optimizing right in and out of the

00:21:33,140 --> 00:21:37,550
compiler using jars helps quite a bit

00:21:35,570 --> 00:21:39,980
because you can reduce round trips to

00:21:37,550 --> 00:21:41,660
the network it already helps to use jars

00:21:39,980 --> 00:21:42,290
locally because interacting with

00:21:41,660 --> 00:21:45,230
syscalls

00:21:42,290 --> 00:21:46,580
is very expensive but additionally when

00:21:45,230 --> 00:21:48,950
you're interacting with a network file

00:21:46,580 --> 00:21:51,370
system effectively reducing i/o is

00:21:48,950 --> 00:21:54,080
significant

00:21:51,370 --> 00:21:56,090
we're also avoiding materializing

00:21:54,080 --> 00:22:00,559
outputs so this whole API actually

00:21:56,090 --> 00:22:02,090
allows for never downloading the output

00:22:00,559 --> 00:22:04,429
of a compiled if you don't want to if

00:22:02,090 --> 00:22:04,880
all you needed was hey there were no

00:22:04,429 --> 00:22:07,279
error

00:22:04,880 --> 00:22:09,980
that's a tiny little output at the very

00:22:07,279 --> 00:22:12,380
end of your compile it's just standard

00:22:09,980 --> 00:22:14,870
out it's none of the class files and

00:22:12,380 --> 00:22:16,130
that's all you need to download but even

00:22:14,870 --> 00:22:17,720
if you do download it you don't actually

00:22:16,130 --> 00:22:18,830
have to put it on the file system you

00:22:17,720 --> 00:22:24,110
just have to put it in your local

00:22:18,830 --> 00:22:28,450
database and and file systems are still

00:22:24,110 --> 00:22:32,690
slow Apple tried and did a great job

00:22:28,450 --> 00:22:38,149
okay so we're also optimizing by adding

00:22:32,690 --> 00:22:43,389
batching I'm actually going to go ahead

00:22:38,149 --> 00:22:47,419
and use another Eugene slide so if you

00:22:43,389 --> 00:22:48,980
know not so what you can do is if you if

00:22:47,419 --> 00:22:51,740
you extract the headers for a particular

00:22:48,980 --> 00:22:54,519
target a set of files you can compile

00:22:51,740 --> 00:22:57,620
all the files in that target in parallel

00:22:54,519 --> 00:22:59,960
because they have already been compiled

00:22:57,620 --> 00:23:02,240
right this is weird

00:22:59,960 --> 00:23:04,669
time loop type thing but that means that

00:23:02,240 --> 00:23:06,500
if you have ten files you outline them

00:23:04,669 --> 00:23:08,629
in a fraction of the time it takes to

00:23:06,500 --> 00:23:11,480
compile them you can then compile all of

00:23:08,629 --> 00:23:13,759
them in parallel you can literally

00:23:11,480 --> 00:23:16,429
invoke separate compilers so just

00:23:13,759 --> 00:23:19,129
invoking separate compilers is what this

00:23:16,429 --> 00:23:23,629
tool is all about the overhead of doing

00:23:19,129 --> 00:23:26,090
this is that compilation depends on

00:23:23,629 --> 00:23:28,039
outlining right so if outlining takes

00:23:26,090 --> 00:23:29,299
some amount of time you then have to

00:23:28,039 --> 00:23:31,309
wait for outline to finish before you

00:23:29,299 --> 00:23:34,250
can begin the compilation where

00:23:31,309 --> 00:23:38,179
otherwise compilation only depends on

00:23:34,250 --> 00:23:39,289
the outlining of dependencies and you

00:23:38,179 --> 00:23:41,929
have to invoke the compiler multiple

00:23:39,289 --> 00:23:45,320
times right so it has some overhead you

00:23:41,929 --> 00:23:48,289
the compilers are used to being kept

00:23:45,320 --> 00:23:50,659
warm and compiling large batches of

00:23:48,289 --> 00:23:52,820
things in general and so if you give

00:23:50,659 --> 00:23:55,220
them smaller inputs you're paying worth

00:23:52,820 --> 00:23:56,840
at overhead cost and this helps to take

00:23:55,220 --> 00:23:59,509
advantage of otherwise idle course so

00:23:56,840 --> 00:24:02,149
looking at this again if you break each

00:23:59,509 --> 00:24:06,110
of those targets basically in half you

00:24:02,149 --> 00:24:07,759
pay some overhead cost to compile them

00:24:06,110 --> 00:24:08,960
but you get even more parallelism you

00:24:07,759 --> 00:24:11,299
basically just double the doubled your

00:24:08,960 --> 00:24:13,129
parallelism with with 20% overhead and

00:24:11,299 --> 00:24:14,810
you're able to get down to 1.4 I think

00:24:13,129 --> 00:24:16,430
we were at 2 before

00:24:14,810 --> 00:24:20,630
thank you again to Eugene for those

00:24:16,430 --> 00:24:23,480
slides so what else are we doing

00:24:20,630 --> 00:24:26,090
optimizing you don't have to run

00:24:23,480 --> 00:24:28,730
everything remotely so figuring out what

00:24:26,090 --> 00:24:31,700
you want to run remotely this is an

00:24:28,730 --> 00:24:33,380
important place to optimize and if you

00:24:31,700 --> 00:24:40,040
just cherry-pick individual things that

00:24:33,380 --> 00:24:41,810
you you want to run remotely you know

00:24:40,040 --> 00:24:43,460
sorry if you just cherry-pick individual

00:24:41,810 --> 00:24:45,740
things you can say this one this one

00:24:43,460 --> 00:24:47,720
doesn't take very long I know it doesn't

00:24:45,740 --> 00:24:49,940
take very long I'll run it locally it

00:24:47,720 --> 00:24:53,930
has it has high remote overhead for some

00:24:49,940 --> 00:24:56,390
reason right we haven't done any of that

00:24:53,930 --> 00:24:58,790
for our demo or not demo but for the

00:24:56,390 --> 00:25:02,900
result I'll show but we think it will be

00:24:58,790 --> 00:25:05,660
very important and finally of course

00:25:02,900 --> 00:25:07,190
I've talked about the ratio between the

00:25:05,660 --> 00:25:10,490
compilation time in the outlining time

00:25:07,190 --> 00:25:12,650
that's critical as well so we've been

00:25:10,490 --> 00:25:15,350
doing a lot of profiling we win has been

00:25:12,650 --> 00:25:20,930
doing a lot of profiling and optimizing

00:25:15,350 --> 00:25:26,600
of our se and getting that ratio down is

00:25:20,930 --> 00:25:29,990
super helpful okay so as I said we've

00:25:26,600 --> 00:25:31,840
been doing distributed profiling I'm

00:25:29,990 --> 00:25:34,160
gonna rely on video compression not to

00:25:31,840 --> 00:25:41,470
leak any secrets what you're seeing here

00:25:34,160 --> 00:25:44,030
is a zip and trace 15 seconds worth and

00:25:41,470 --> 00:25:48,260
what you'll notice is that in the

00:25:44,030 --> 00:25:52,460
beginning of this video things are

00:25:48,260 --> 00:25:53,690
pretty parallel and that's this is what

00:25:52,460 --> 00:25:56,120
a hundred quarters worth of parallelism

00:25:53,690 --> 00:25:58,340
looks like right there a whole bunch of

00:25:56,120 --> 00:26:00,080
compiles happening in parallel but what

00:25:58,340 --> 00:26:03,920
you'll also notice is at the end we have

00:26:00,080 --> 00:26:07,220
this one two three go the long tail goes

00:26:03,920 --> 00:26:11,780
off the end apparently I'm going to need

00:26:07,220 --> 00:26:15,440
to I'm going to need to skip some other

00:26:11,780 --> 00:26:17,960
way the long tail swoops off to the end

00:26:15,440 --> 00:26:20,780
here there is a long tail we still have

00:26:17,960 --> 00:26:22,790
to investigate what the long tail is we

00:26:20,780 --> 00:26:25,580
have a pretty good idea that it relates

00:26:22,790 --> 00:26:27,260
to cycles between Java and Scala because

00:26:25,580 --> 00:26:28,280
what we haven't done yet not cycles

00:26:27,260 --> 00:26:30,140
sorry

00:26:28,280 --> 00:26:32,390
paths that alternate between Java and

00:26:30,140 --> 00:26:35,120
Scala we are not currently outlining

00:26:32,390 --> 00:26:39,020
Java code and that means that if a Java

00:26:35,120 --> 00:26:41,570
compile sits between a Scala compile and

00:26:39,020 --> 00:26:45,080
another Scala compile the Scala compile

00:26:41,570 --> 00:26:48,500
has to wait for Java and Java has to

00:26:45,080 --> 00:26:52,540
wait for the full compilation of the

00:26:48,500 --> 00:26:55,220
Scala code we think that's solvable

00:26:52,540 --> 00:26:59,080
we will also confirm that hypothesis

00:26:55,220 --> 00:27:01,610
before before continuing to implement it

00:26:59,080 --> 00:27:02,960
yeah but where is the result like what

00:27:01,610 --> 00:27:06,290
is the result

00:27:02,960 --> 00:27:08,210
so again doubling down on on the the

00:27:06,290 --> 00:27:12,470
caveat emptor this is this is really new

00:27:08,210 --> 00:27:16,400
and this is today's result but by

00:27:12,470 --> 00:27:17,990
tomorrow it will be history already so

00:27:16,400 --> 00:27:19,760
performance has doubled in the last week

00:27:17,990 --> 00:27:21,730
and part of that was that we had our

00:27:19,760 --> 00:27:26,990
entire team in London which was awesome

00:27:21,730 --> 00:27:29,230
very excited so cold compiles cold

00:27:26,990 --> 00:27:32,330
compiles using nail gun which is a

00:27:29,230 --> 00:27:35,690
little server that keeps a zinc instance

00:27:32,330 --> 00:27:39,230
warm on four cores which is what this

00:27:35,690 --> 00:27:41,929
laptop is is a four core box takes 628

00:27:39,230 --> 00:27:43,520
seconds eight cores which is apparently

00:27:41,929 --> 00:27:45,440
not something that you can carry on your

00:27:43,520 --> 00:27:49,400
back we don't have the six core number

00:27:45,440 --> 00:27:52,610
here is 430 and the interesting thing is

00:27:49,400 --> 00:27:55,370
at sixteen course you're a little bit

00:27:52,610 --> 00:27:57,710
slower I could fill out that graph a

00:27:55,370 --> 00:28:01,160
little bit more but you've reached the

00:27:57,710 --> 00:28:04,520
limits of parallelism for this target

00:28:01,160 --> 00:28:07,940
right this set of 700 projects that were

00:28:04,520 --> 00:28:10,730
compiling the graph shape without

00:28:07,940 --> 00:28:14,179
outlining I'm sorry and this is without

00:28:10,730 --> 00:28:15,710
outlining right zink only the graph

00:28:14,179 --> 00:28:18,740
shape without outlining means that you

00:28:15,710 --> 00:28:21,110
can only extract so much parallelism and

00:28:18,740 --> 00:28:22,360
so even if we were to give people bigger

00:28:21,110 --> 00:28:25,870
boxes

00:28:22,360 --> 00:28:31,600
we would not be able to utilize them so

00:28:25,870 --> 00:28:34,179
RSC and zinc using 32 course is just

00:28:31,600 --> 00:28:35,919
about as fast as eight course right so

00:28:34,179 --> 00:28:39,070
that's that's not that's not wonderfully

00:28:35,919 --> 00:28:41,140
exciting but using 128 cores it's faster

00:28:39,070 --> 00:28:44,710
than eight so it's faster than something

00:28:41,140 --> 00:28:47,169
that you can carry on your back this is

00:28:44,710 --> 00:28:50,350
this is the beginning of something very

00:28:47,169 --> 00:28:52,870
very very exciting and it might be weeks

00:28:50,350 --> 00:28:56,380
away from being massively more exciting

00:28:52,870 --> 00:28:58,600
stay tuned the other thing though is

00:28:56,380 --> 00:28:59,830
that benchmarking cold compiles doesn't

00:28:58,600 --> 00:29:01,450
actually show all the benefits of

00:28:59,830 --> 00:29:05,049
promoting we were talking about

00:29:01,450 --> 00:29:07,899
workflows and remote execution allows

00:29:05,049 --> 00:29:10,899
for this transparent thing where because

00:29:07,899 --> 00:29:12,840
the compiles are happening remotely in a

00:29:10,899 --> 00:29:16,320
hermetic environment that basically

00:29:12,840 --> 00:29:20,080
can't possibly quote-unquote be polluted

00:29:16,320 --> 00:29:22,840
you can you can put the outputs of those

00:29:20,080 --> 00:29:28,200
compiles in caches immediately so all of

00:29:22,840 --> 00:29:32,230
all of people's edits can be cached and

00:29:28,200 --> 00:29:34,269
that means like if I if I merge master

00:29:32,230 --> 00:29:36,730
into my branch and I had significant

00:29:34,269 --> 00:29:38,620
changes in my branch I'm gonna see a

00:29:36,730 --> 00:29:40,299
cold compile I'm gonna populate the

00:29:38,620 --> 00:29:42,250
cache and everyone else using my branch

00:29:40,299 --> 00:29:45,370
is populated I didn't have any sort of

00:29:42,250 --> 00:29:47,049
manual step to to basically say I'm

00:29:45,370 --> 00:29:49,240
about to share this branch should I warm

00:29:47,049 --> 00:29:53,130
it up for people which is the thing that

00:29:49,240 --> 00:29:56,470
people sort of have to do at Twitter and

00:29:53,130 --> 00:29:57,820
also I sort of I reference not needing

00:29:56,470 --> 00:30:01,179
to download things if you assume that

00:29:57,820 --> 00:30:03,220
tests are remoted as well which is very

00:30:01,179 --> 00:30:05,500
very useful as well because tests are

00:30:03,220 --> 00:30:06,610
embarrassingly parallel without any sort

00:30:05,500 --> 00:30:10,330
of outlining right if you have a hundred

00:30:06,610 --> 00:30:14,289
tests you can run them all in batches on

00:30:10,330 --> 00:30:15,549
a remote machine this means that you you

00:30:14,289 --> 00:30:17,590
actually get that uniformity I was

00:30:15,549 --> 00:30:22,659
talking about you can run as much of the

00:30:17,590 --> 00:30:26,230
repository as you'd like without without

00:30:22,659 --> 00:30:29,710
really thinking too much about it so

00:30:26,230 --> 00:30:32,200
what are the next steps for us we're

00:30:29,710 --> 00:30:33,639
gonna continue optimizing because we're

00:30:32,200 --> 00:30:38,950
on the cusp of having an excellent

00:30:33,639 --> 00:30:40,870
result we will probably outline Java our

00:30:38,950 --> 00:30:44,559
simulations didn't show it being a

00:30:40,870 --> 00:30:46,450
massive speed-up but it's looking a

00:30:44,559 --> 00:30:50,110
little bit like in reality it's more

00:30:46,450 --> 00:30:53,169
significant than we thought and most

00:30:50,110 --> 00:30:55,179
likely that will be either RSC adding

00:30:53,169 --> 00:30:58,299
support for parsing Java such that it

00:30:55,179 --> 00:31:01,000
can produce and outline for that Java

00:30:58,299 --> 00:31:05,830
code or using turbine which is Google

00:31:01,000 --> 00:31:07,389
again google's java header compiler we

00:31:05,830 --> 00:31:09,700
will also potentially do dynamic

00:31:07,389 --> 00:31:12,429
batching because one of the things we've

00:31:09,700 --> 00:31:15,370
seen is that having a big a big knob in

00:31:12,429 --> 00:31:17,200
terms of how much how what the maximum

00:31:15,370 --> 00:31:19,960
number of files compiled at once

00:31:17,200 --> 00:31:22,270
is is a little bit limited in the sense

00:31:19,960 --> 00:31:24,100
that if you encounter any large target

00:31:22,270 --> 00:31:26,080
near the beginning where we already have

00:31:24,100 --> 00:31:28,419
all of that parallelism we don't want

00:31:26,080 --> 00:31:30,250
more parallelism right so you want to do

00:31:28,419 --> 00:31:31,990
something closer to fork/join where if I

00:31:30,250 --> 00:31:35,830
have extra parallelism break it up

00:31:31,990 --> 00:31:38,940
otherwise don't also speculation is very

00:31:35,830 --> 00:31:41,049
important and speculation refers to

00:31:38,940 --> 00:31:44,139
making two attempts for something and

00:31:41,049 --> 00:31:47,340
then taking the best of them and in this

00:31:44,139 --> 00:31:50,260
context that might look like waiting on

00:31:47,340 --> 00:31:52,659
the order of 50 to 100 milliseconds

00:31:50,260 --> 00:31:55,360
after starting something in your more

00:31:52,659 --> 00:31:58,059
desired location and then starting it in

00:31:55,360 --> 00:31:59,620
your less desired location in hopes that

00:31:58,059 --> 00:32:01,440
the less desired location ends up being

00:31:59,620 --> 00:32:04,840
faster than your more desired location

00:32:01,440 --> 00:32:08,470
and so local remote that might be if I'm

00:32:04,840 --> 00:32:12,399
on a shoddy internet connection I might

00:32:08,470 --> 00:32:15,610
speculate by starting locally and then

00:32:12,399 --> 00:32:17,470
only kicking to the remote if if my

00:32:15,610 --> 00:32:20,860
local compilation is taken longer than

00:32:17,470 --> 00:32:24,220
some amount of time and generally you

00:32:20,860 --> 00:32:26,769
tune speculation via percentiles

00:32:24,220 --> 00:32:28,779
so if I've made it past my my 50th

00:32:26,769 --> 00:32:31,000
percentile or my 90th percentile I want

00:32:28,779 --> 00:32:32,679
to try to remote something or vice-versa

00:32:31,000 --> 00:32:34,480
if you're running in CI maybe you want

00:32:32,679 --> 00:32:35,740
to speculate remotely and only start

00:32:34,480 --> 00:32:37,559
compiling something locally if it takes

00:32:35,740 --> 00:32:40,509
too long

00:32:37,559 --> 00:32:43,179
finally the input fetching so this

00:32:40,509 --> 00:32:47,830
running of UNIX processes in in a

00:32:43,179 --> 00:32:50,409
cluster we have potentially encountered

00:32:47,830 --> 00:32:51,970
things about the API that we believe can

00:32:50,409 --> 00:32:53,860
be improved to allow for more

00:32:51,970 --> 00:32:57,190
prefetching so you spend less time on

00:32:53,860 --> 00:32:58,269
startup it's already pretty low we're

00:32:57,190 --> 00:32:59,980
spending significantly less time

00:32:58,269 --> 00:33:02,769
starting up processes than we are

00:32:59,980 --> 00:33:04,929
running them in in most cases but there

00:33:02,769 --> 00:33:06,610
are other cases where the profit process

00:33:04,929 --> 00:33:09,250
is pretty short-lived relative to what

00:33:06,610 --> 00:33:11,590
it needs and so we could always go

00:33:09,250 --> 00:33:13,240
faster there the other thing is it's

00:33:11,590 --> 00:33:16,269
possible to make the fetching of the

00:33:13,240 --> 00:33:19,779
things the process needs to run a sync

00:33:16,269 --> 00:33:22,179
with the process running if below it

00:33:19,779 --> 00:33:23,919
you're using a fuse file system because

00:33:22,179 --> 00:33:27,600
you can be back filling the things that

00:33:23,919 --> 00:33:30,429
it might request as it starts and

00:33:27,600 --> 00:33:32,409
possibly beat them you know beat it so

00:33:30,429 --> 00:33:37,389
that none of those file file system

00:33:32,409 --> 00:33:39,129
operations actually end up blocking so

00:33:37,389 --> 00:33:42,340
for for the next steps for the project

00:33:39,129 --> 00:33:45,879
post optimising or mixed in with

00:33:42,340 --> 00:33:51,250
optimizing is shipping remoting per zinc

00:33:45,879 --> 00:33:52,990
in seal in CI this does allow us like

00:33:51,250 --> 00:33:56,500
shipping for zinc is an important step

00:33:52,990 --> 00:33:58,419
toward shipping with RSC there are a few

00:33:56,500 --> 00:34:02,409
more steps involved in actually rolling

00:33:58,419 --> 00:34:04,600
RSC out if you've if you've seen past

00:34:02,409 --> 00:34:06,879
talks about RSC RSC requires public

00:34:04,600 --> 00:34:09,010
return types or I'm sorry return types

00:34:06,879 --> 00:34:10,359
on public members which is not a thing

00:34:09,010 --> 00:34:13,629
that the Scott language currently

00:34:10,359 --> 00:34:14,770
requires it's a thing that it's a

00:34:13,629 --> 00:34:18,010
convention that a lot of people who

00:34:14,770 --> 00:34:19,450
write write Scala follow in the sense

00:34:18,010 --> 00:34:23,919
that you should have a return type if

00:34:19,450 --> 00:34:27,190
you're going to have a public API but

00:34:23,919 --> 00:34:29,040
requiring it is is a step there is a

00:34:27,190 --> 00:34:32,829
Scala fix rewrite it's very reasonable

00:34:29,040 --> 00:34:35,859
and it's automatic for for most of the

00:34:32,829 --> 00:34:38,169
cases so we fully expect that that as we

00:34:35,859 --> 00:34:40,480
are as we are able to demonstrate the

00:34:38,169 --> 00:34:43,990
huge benefit of this work it will be

00:34:40,480 --> 00:34:45,639
straightforward to roll it out and then

00:34:43,990 --> 00:34:47,349
also enabling remoting from laptops

00:34:45,639 --> 00:34:48,970
we're going to ship in CI first but on

00:34:47,349 --> 00:34:51,040
laptops you additionally need that

00:34:48,970 --> 00:34:53,740
speculation I was talking about

00:34:51,040 --> 00:34:56,230
because particularly in in shaadi

00:34:53,740 --> 00:34:58,510
Network environments or completely

00:34:56,230 --> 00:35:03,820
offline cases you don't want to rely on

00:34:58,510 --> 00:35:06,580
the the server being there and then

00:35:03,820 --> 00:35:08,349
perhaps more controversial we think that

00:35:06,580 --> 00:35:11,170
it might be possible to remove zinc from

00:35:08,349 --> 00:35:13,030
this equation so a little bit more about

00:35:11,170 --> 00:35:16,960
zinc because I realized I didn't really

00:35:13,030 --> 00:35:20,020
lay that down zinc is extracted from SBT

00:35:16,960 --> 00:35:21,430
SBT uses zinc as it's incremental

00:35:20,020 --> 00:35:25,930
compilation library

00:35:21,430 --> 00:35:29,290
so when SBT recompiles things

00:35:25,930 --> 00:35:32,230
it uses an analysis file that tracks the

00:35:29,290 --> 00:35:34,300
dependencies between files to know which

00:35:32,230 --> 00:35:35,500
files it needs to compile right so you

00:35:34,300 --> 00:35:39,340
have a project and it might contain

00:35:35,500 --> 00:35:41,609
20-ish files if the zinc analysis shows

00:35:39,340 --> 00:35:45,609
that only one of those 20 files is

00:35:41,609 --> 00:35:46,750
affected by some edit it will compile it

00:35:45,609 --> 00:35:53,200
will basically invoke the compiler with

00:35:46,750 --> 00:35:55,420
exactly one file and this is a little

00:35:53,200 --> 00:35:58,660
bit fragile it can lead to under

00:35:55,420 --> 00:35:59,740
compilation can't can and has and over

00:35:58,660 --> 00:36:01,330
the years a lot of bugs have been fixed

00:35:59,740 --> 00:36:04,060
there but new ones have been introduced

00:36:01,330 --> 00:36:09,040
and compilers change and so it's a

00:36:04,060 --> 00:36:12,430
moving target and so we want to maybe

00:36:09,040 --> 00:36:14,200
see whether external outlining is a

00:36:12,430 --> 00:36:16,660
better answer to the question of how do

00:36:14,200 --> 00:36:21,310
i how do i make incremental Scala

00:36:16,660 --> 00:36:22,720
compilers fast so I talked about sub

00:36:21,310 --> 00:36:25,869
target compilation the other thing is

00:36:22,720 --> 00:36:30,250
that the analysis file contains enough

00:36:25,869 --> 00:36:32,890
information to know whether a the public

00:36:30,250 --> 00:36:34,990
API of something has changed since the

00:36:32,890 --> 00:36:36,220
last build if the public API has changed

00:36:34,990 --> 00:36:37,540
since the last builds you have to you

00:36:36,220 --> 00:36:41,859
have to recompile the things that touch

00:36:37,540 --> 00:36:45,339
that public API outlines are the public

00:36:41,859 --> 00:36:47,349
API in a serialize format right so that

00:36:45,339 --> 00:36:50,770
that one sort of trivially trivially

00:36:47,349 --> 00:36:52,750
handled I have a cache key it indicates

00:36:50,770 --> 00:36:56,020
that I depend on this outline and not

00:36:52,750 --> 00:36:58,030
the whole compilation and so I don't

00:36:56,020 --> 00:37:00,940
need to recompile unless the public

00:36:58,030 --> 00:37:02,950
members have changed so that that that

00:37:00,940 --> 00:37:04,270
falls out automatically Subterra

00:37:02,950 --> 00:37:06,690
compilation

00:37:04,270 --> 00:37:09,760
we can't do one out of twenty probably

00:37:06,690 --> 00:37:12,640
but if you apply batching and basically

00:37:09,760 --> 00:37:14,200
partition the the sets of files as I was

00:37:12,640 --> 00:37:18,430
talking about with those batching and

00:37:14,200 --> 00:37:20,620
dynamic batching you can only you can

00:37:18,430 --> 00:37:22,510
you can compile just the partitions that

00:37:20,620 --> 00:37:26,110
have changed so if you maybe have ten

00:37:22,510 --> 00:37:28,330
partitions of of 200 files your

00:37:26,110 --> 00:37:32,860
incremental recompile will take a

00:37:28,330 --> 00:37:36,570
maximum of some amount of time and you

00:37:32,860 --> 00:37:38,800
can also paralyze it so that's exciting

00:37:36,570 --> 00:37:40,630
yeah so we'll see whether that works out

00:37:38,800 --> 00:37:41,830
we're interested in talking with more

00:37:40,630 --> 00:37:46,830
folks in the community about that one

00:37:41,830 --> 00:37:50,640
and it's it's sort of post shipping clea

00:37:46,830 --> 00:37:53,200
so the rest of the trajectory I think I

00:37:50,640 --> 00:37:56,470
talked about the trajectory for builds

00:37:53,200 --> 00:37:58,560
this applies everywhere and the reason I

00:37:56,470 --> 00:38:02,260
think it applies everywhere is that

00:37:58,560 --> 00:38:05,470
these are a lot of open source code

00:38:02,260 --> 00:38:07,900
bases compile times are going to

00:38:05,470 --> 00:38:10,020
continue to go up I think that all build

00:38:07,900 --> 00:38:13,570
tools should support remote execution

00:38:10,020 --> 00:38:15,730
and I think that all compilers should

00:38:13,570 --> 00:38:19,060
support external outlining as defined

00:38:15,730 --> 00:38:21,910
before right you a compiler it basically

00:38:19,060 --> 00:38:26,140
should be on the compiler writer to have

00:38:21,910 --> 00:38:28,540
an API not even API CLI interface that

00:38:26,140 --> 00:38:30,490
says please outline this don't don't

00:38:28,540 --> 00:38:33,850
fully compile it and give me giving me

00:38:30,490 --> 00:38:37,350
back a file that I can use it should it

00:38:33,850 --> 00:38:39,250
should just be the de-facto standard and

00:38:37,350 --> 00:38:40,570
open source users should be able to

00:38:39,250 --> 00:38:43,690
bring their own remote execution

00:38:40,570 --> 00:38:47,440
credentials so if you come along to a

00:38:43,690 --> 00:38:50,200
large project you you have an account

00:38:47,440 --> 00:38:52,180
with somebody or you have a hosted

00:38:50,200 --> 00:38:52,930
hosted remote execution cluster but

00:38:52,180 --> 00:38:54,520
probably you have an account with

00:38:52,930 --> 00:38:59,620
somebody because it's the era of the

00:38:54,520 --> 00:39:01,030
cloud but that open source project may

00:38:59,620 --> 00:39:03,430
not trust you enough to give you access

00:39:01,030 --> 00:39:05,830
to their cluster of machines but you

00:39:03,430 --> 00:39:07,510
could bring your own why not maybe they

00:39:05,830 --> 00:39:09,610
give you read-only access to their cash

00:39:07,510 --> 00:39:13,210
and you populate with writes your own

00:39:09,610 --> 00:39:15,640
cash right and also I think all

00:39:13,210 --> 00:39:18,220
organizations should transparently share

00:39:15,640 --> 00:39:21,280
a remoting cluster rather than this

00:39:18,220 --> 00:39:23,020
Sara Lee having dedicated dev boxes so

00:39:21,280 --> 00:39:25,510
the idea that every mission every you

00:39:23,020 --> 00:39:27,760
know one of your your developers has a

00:39:25,510 --> 00:39:32,680
laptop and then a dedicated dev box or

00:39:27,760 --> 00:39:33,880
even a cluster of VMs that will not get

00:39:32,680 --> 00:39:35,349
you the amount of performance that is

00:39:33,880 --> 00:39:36,820
possible with remoting and there's

00:39:35,349 --> 00:39:38,800
there's some lower bound depending on

00:39:36,820 --> 00:39:40,480
how efficient remoting is and we'll get

00:39:38,800 --> 00:39:42,760
there but maybe if you're an

00:39:40,480 --> 00:39:45,070
organization of ten people

00:39:42,760 --> 00:39:47,320
that's a cluster of ten machines that

00:39:45,070 --> 00:39:49,150
when you are not compiling something

00:39:47,320 --> 00:39:52,150
somebody else could be using to compile

00:39:49,150 --> 00:39:53,800
something right and so you'd basically

00:39:52,150 --> 00:39:56,530
get better performance beyond some

00:39:53,800 --> 00:39:59,770
threshold of doesn't and it will be

00:39:56,530 --> 00:40:01,090
pretty low so we think that everyone can

00:39:59,770 --> 00:40:05,230
sort of help prepare the community for

00:40:01,090 --> 00:40:06,760
that future and there are multiple

00:40:05,230 --> 00:40:08,710
open-source implementations of the

00:40:06,760 --> 00:40:10,480
remote execution clients but there

00:40:08,710 --> 00:40:13,869
should definitely be more right this is

00:40:10,480 --> 00:40:16,000
a very basic at a high level it's a very

00:40:13,869 --> 00:40:18,310
basic API it takes some time to

00:40:16,000 --> 00:40:20,500
implement but there are reference

00:40:18,310 --> 00:40:23,980
implementations in Java and rust and I

00:40:20,500 --> 00:40:25,480
think Python as well and it should be

00:40:23,980 --> 00:40:27,280
integrated with with basically all the

00:40:25,480 --> 00:40:28,720
build tools right we should have

00:40:27,280 --> 00:40:32,500
transparent support for doing this if I

00:40:28,720 --> 00:40:34,930
bring my creds my credentials along SBT

00:40:32,500 --> 00:40:36,070
maven blue mill fury etc they should all

00:40:34,930 --> 00:40:38,050
they should all probably have

00:40:36,070 --> 00:40:44,830
implementations and then the servers

00:40:38,050 --> 00:40:47,349
exist also the Google RBE service is the

00:40:44,830 --> 00:40:52,210
thing so even if you didn't want to host

00:40:47,349 --> 00:40:54,790
your own cluster you could do that and I

00:40:52,210 --> 00:40:55,720
do hope that somebody will take one of

00:40:54,790 --> 00:40:59,710
the other of them open-source

00:40:55,720 --> 00:41:02,589
implementations and host that ticket

00:40:59,710 --> 00:41:06,099
help give Google's and make them sweat a

00:41:02,589 --> 00:41:07,720
little and then also there are multiple

00:41:06,099 --> 00:41:09,720
nascent implementations of outlining for

00:41:07,720 --> 00:41:13,920
Scala I sort of referenced a few of them

00:41:09,720 --> 00:41:16,780
earlier we think that polishing and

00:41:13,920 --> 00:41:18,570
externalizing importantly like making

00:41:16,780 --> 00:41:22,599
this a thing that the compiler emits

00:41:18,570 --> 00:41:24,730
somewhere to disk is an important thing

00:41:22,599 --> 00:41:26,800
if you if you also have a way to use it

00:41:24,730 --> 00:41:29,740
in memory that's fine but making sure

00:41:26,800 --> 00:41:31,119
that you emit it is is critical for this

00:41:29,740 --> 00:41:34,229
model

00:41:31,119 --> 00:41:36,369
for the shared-nothing sort of model

00:41:34,229 --> 00:41:38,289
yeah so we'd like to basically make

00:41:36,369 --> 00:41:40,140
external outlining for parallelism and

00:41:38,289 --> 00:41:41,890
change detection a de facto standard

00:41:40,140 --> 00:41:43,059
there are a whole bunch of language

00:41:41,890 --> 00:41:48,849
communities in which it has already the

00:41:43,059 --> 00:41:50,559
standard so in conclusion I did

00:41:48,849 --> 00:41:52,599
relatively little coding on this I'm

00:41:50,559 --> 00:41:57,669
incredibly proud of everyone who's been

00:41:52,599 --> 00:42:00,309
involved this is bolded but you can't

00:41:57,669 --> 00:42:04,449
tell that Daniel Danny

00:42:00,309 --> 00:42:06,429
Nora and win in particular Andrew I

00:42:04,449 --> 00:42:08,679
wanted to call out as having spent a

00:42:06,429 --> 00:42:10,719
huge amount of time on this the rest of

00:42:08,679 --> 00:42:13,419
the team has been coming up to speed on

00:42:10,719 --> 00:42:17,109
all of their efforts and it's it's it's

00:42:13,419 --> 00:42:19,059
awesome on that note we're hiring we

00:42:17,109 --> 00:42:21,789
have seven positions within engineering

00:42:19,059 --> 00:42:25,269
effectiveness which is Twitter's

00:42:21,789 --> 00:42:27,130
developer productivity team in SF and

00:42:25,269 --> 00:42:32,589
London and the London team is kick-ass

00:42:27,130 --> 00:42:35,469
as well so nearby and we will likely EE

00:42:32,589 --> 00:42:38,890
be making a push to improve IDE support

00:42:35,469 --> 00:42:40,689
later this year Olaf of metals Fame is

00:42:38,890 --> 00:42:42,249
joining us in London which is super

00:42:40,689 --> 00:42:43,449
awesome and we're very excited about

00:42:42,249 --> 00:42:46,390
that

00:42:43,449 --> 00:42:48,579
no promises that we will we will switch

00:42:46,390 --> 00:42:53,380
to metals but it's it's looking pretty

00:42:48,579 --> 00:42:55,869
shiny and we have tons more hiring

00:42:53,380 --> 00:42:59,469
within the platform or --get twitter we

00:42:55,869 --> 00:43:01,869
invest in this tooling to make lives of

00:42:59,469 --> 00:43:04,719
all of our Twitter developers awesome

00:43:01,869 --> 00:43:07,779
and we write open source code to help

00:43:04,719 --> 00:43:12,509
the community in general and we're

00:43:07,779 --> 00:43:12,509
hiring thank you

00:43:17,690 --> 00:43:24,030
so now it's time for question if you

00:43:20,760 --> 00:43:27,350
have some who wants to ask a question

00:43:24,030 --> 00:43:27,350
like raise your arm

00:43:28,760 --> 00:43:45,900
nobody hello wait for the microphone so

00:43:43,980 --> 00:43:48,660
you said outlining first remote

00:43:45,900 --> 00:43:50,820
execution was about the same speed as

00:43:48,660 --> 00:43:53,220
the local execution including the

00:43:50,820 --> 00:43:55,410
overhead from using the growl native

00:43:53,220 --> 00:44:00,210
image mm-hmm does that mean that without

00:43:55,410 --> 00:44:02,010
overhead you'll be twice as fast so I

00:44:00,210 --> 00:44:05,120
didn't put I didn't put the number on

00:44:02,010 --> 00:44:07,020
the slides but without any overhead

00:44:05,120 --> 00:44:09,240
stimulation show that this can be

00:44:07,020 --> 00:44:11,670
something like 16 times as fast okay

00:44:09,240 --> 00:44:13,860
yeah there are a huge number of

00:44:11,670 --> 00:44:15,930
different overheads and we basically

00:44:13,860 --> 00:44:22,730
started optimizing in the last like two

00:44:15,930 --> 00:44:26,220
weeks so it the timing was not

00:44:22,730 --> 00:44:28,620
completely ideal because I think the

00:44:26,220 --> 00:44:31,380
previous Friday the system began working

00:44:28,620 --> 00:44:33,650
end to end so this is just extremely

00:44:31,380 --> 00:44:37,680
nascent the simulation show

00:44:33,650 --> 00:44:41,610
significantly better speed ups the girl

00:44:37,680 --> 00:44:43,170
native image aspect is probably not the

00:44:41,610 --> 00:44:47,330
largest overhead right now we have a

00:44:43,170 --> 00:44:47,330
whole bunch more yeah

00:44:48,170 --> 00:45:07,440
who has another question when it becomes

00:45:02,910 --> 00:45:08,610
more mature maybe so all of the

00:45:07,440 --> 00:45:16,320
components that we they are already

00:45:08,610 --> 00:45:18,990
open-source pants oh well pants is pants

00:45:16,320 --> 00:45:23,400
has been open source for six years RSC

00:45:18,990 --> 00:45:24,420
is open source for two years scoot is

00:45:23,400 --> 00:45:26,730
open source but they're they're not

00:45:24,420 --> 00:45:29,910
really pushing to get a community right

00:45:26,730 --> 00:45:32,280
now there are a few different other

00:45:29,910 --> 00:45:35,190
open-source servers that are pushing for

00:45:32,280 --> 00:45:36,390
communities but but in reality there's

00:45:35,190 --> 00:45:38,070
nothing stopping people from doing this

00:45:36,390 --> 00:45:40,890
today so I was really serious when I

00:45:38,070 --> 00:45:42,240
think when I said that I think that we

00:45:40,890 --> 00:45:44,340
getting a lot more people involved would

00:45:42,240 --> 00:45:48,690
be great in the in the whole general

00:45:44,340 --> 00:45:50,850
concept of remoting so many quickly and

00:45:48,690 --> 00:45:53,520
if at any astir that was just I was

00:45:50,850 --> 00:45:55,140
going to make a massive bit of

00:45:53,520 --> 00:45:56,400
documentation to run through everything

00:45:55,140 --> 00:45:58,770
that we were doing internally was scoot

00:45:56,400 --> 00:46:00,420
and pants an RSC and he didn't quite get

00:45:58,770 --> 00:46:01,920
to that but all of the components are

00:46:00,420 --> 00:46:03,120
open-source they can be done if you know

00:46:01,920 --> 00:46:04,590
what to do and that will make sure that

00:46:03,120 --> 00:46:06,960
that something that ever knows you as

00:46:04,590 --> 00:46:08,550
well right yeah I'm definitely happy

00:46:06,960 --> 00:46:11,450
happy to answer questions I didn't

00:46:08,550 --> 00:46:14,250
mention we have pulled ahead of ye and

00:46:11,450 --> 00:46:16,260
Daniel and Danny and win here and they

00:46:14,250 --> 00:46:18,060
all contributed a huge amount so thank

00:46:16,260 --> 00:46:22,770
you you can ask them questions as well

00:46:18,060 --> 00:46:24,990
so having public types to methods if you

00:46:22,770 --> 00:46:27,840
do it just globally it has slowed things

00:46:24,990 --> 00:46:29,010
down I found in the experimentation you

00:46:27,840 --> 00:46:32,960
saying you'd want to do it on public

00:46:29,010 --> 00:46:35,660
methods for RSC mm-hmm but how do you

00:46:32,960 --> 00:46:38,100
enforce or ensure your developers

00:46:35,660 --> 00:46:39,870
privatized appropriately has in

00:46:38,100 --> 00:46:42,780
Houghteling and you keep a public API

00:46:39,870 --> 00:46:45,660
versus yeah the default being public

00:46:42,780 --> 00:46:49,290
right the convention is definitely will

00:46:45,660 --> 00:46:52,260
will definitely sort of grow out of how

00:46:49,290 --> 00:46:56,430
it feels to do this I think I expect

00:46:52,260 --> 00:46:58,260
that being forced to add a return type

00:46:56,430 --> 00:47:00,510
on something this public will be a good

00:46:58,260 --> 00:47:02,040
thing in terms of forcing you to

00:47:00,510 --> 00:47:03,330
consider whether you needed it to be

00:47:02,040 --> 00:47:05,700
public right

00:47:03,330 --> 00:47:10,590
it will probably lead to more private

00:47:05,700 --> 00:47:12,420
things and so public by default means

00:47:10,590 --> 00:47:14,250
you will need a return type by default

00:47:12,420 --> 00:47:15,810
you'd need the private key word in more

00:47:14,250 --> 00:47:18,710
places you're sort of trading off the

00:47:15,810 --> 00:47:23,100
private key word versus the return type

00:47:18,710 --> 00:47:26,300
we hadn't measured the the slowdown by

00:47:23,100 --> 00:47:28,710
adding return types so that would so I

00:47:26,300 --> 00:47:30,030
doing globally and found that yeah I

00:47:28,710 --> 00:47:32,340
need to make things actually slightly

00:47:30,030 --> 00:47:34,500
slower yeah interesting okay but

00:47:32,340 --> 00:47:36,330
interestingly my talk tomorrow I'm

00:47:34,500 --> 00:47:37,770
talking about doing this doing the

00:47:36,330 --> 00:47:38,610
privatization automatically as well

00:47:37,770 --> 00:47:41,210
awesome

00:47:38,610 --> 00:47:47,589
yeah so I think I want to see

00:47:41,210 --> 00:47:47,589
Thank you very maybe the last question

00:47:50,109 --> 00:47:56,540
then if there is no more question then

00:47:53,059 --> 00:48:00,800
you can go to lunch and just I have a

00:47:56,540 --> 00:48:04,000
little announcement we are we have a

00:48:00,800 --> 00:48:06,349
diversity lunch it is for people who are

00:48:04,000 --> 00:48:09,170
from a minority who is not well

00:48:06,349 --> 00:48:11,329
represented in the sky communities so if

00:48:09,170 --> 00:48:13,609
you want it is a dinner it isn't the

00:48:11,329 --> 00:48:16,000
restaurant next to the micro dress when

00:48:13,609 --> 00:48:16,000

YouTube URL: https://www.youtube.com/watch?v=87K4_v2IvBg


