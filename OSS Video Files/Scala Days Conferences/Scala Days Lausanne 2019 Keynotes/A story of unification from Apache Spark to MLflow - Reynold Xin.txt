Title: A story of unification from Apache Spark to MLflow - Reynold Xin
Publication date: 2019-07-11
Playlist: Scala Days Lausanne 2019 Keynotes
Description: 
	This video was recorded at Scala Days Lausanne 2019
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

More information and the abstract can be found here:
https://scaladays.org/schedule/a-story-of-unification-from-apache-spark-to-mlflow
Captions: 
	00:00:00,060 --> 00:00:04,710
all right thank you it's really honored

00:00:02,580 --> 00:00:06,629
to be here during to the ten year

00:00:04,710 --> 00:00:08,550
anniversary of the Scala days conference

00:00:06,629 --> 00:00:10,769
and what was brainstorming was March in

00:00:08,550 --> 00:00:12,360
order to talk about and I offer hey I

00:00:10,769 --> 00:00:13,920
could talk a lot more about how SPARC

00:00:12,360 --> 00:00:16,049
and Scala has been working together how

00:00:13,920 --> 00:00:17,490
great and essential Scala is to spark

00:00:16,049 --> 00:00:19,949
and Mark and said why don't you talk

00:00:17,490 --> 00:00:21,810
about something else the rest of the

00:00:19,949 --> 00:00:24,449
conference filled up with Scala stories

00:00:21,810 --> 00:00:34,800
and conferences and texts so let's talk

00:00:24,449 --> 00:00:36,469
about something else and all right so

00:00:34,800 --> 00:00:39,270
I'm going to be talking to you about

00:00:36,469 --> 00:00:40,500
actually three different things one is I

00:00:39,270 --> 00:00:42,059
wanted it for those of you who have

00:00:40,500 --> 00:00:43,620
never been to a sparkling conference

00:00:42,059 --> 00:00:45,059
Indian know how smart were started I'm

00:00:43,620 --> 00:00:48,420
gonna tell me the story of how SPARC

00:00:45,059 --> 00:00:50,370
truly it was first created and then a

00:00:48,420 --> 00:00:51,930
lot has changed since part was created

00:00:50,370 --> 00:00:53,219
so we're gonna talk to you about what

00:00:51,930 --> 00:00:55,140
has changed what are the new problems

00:00:53,219 --> 00:00:57,480
who have seen among our customers as

00:00:55,140 --> 00:00:59,609
spark users and two of the open source

00:00:57,480 --> 00:01:02,100
project were created in response to

00:00:59,609 --> 00:01:03,719
those Delta and Emma flow all right I'm

00:01:02,100 --> 00:01:05,549
just a little bit more about myself I

00:01:03,719 --> 00:01:07,770
was doing my PhD at UC Berkeley and lab

00:01:05,549 --> 00:01:09,510
the thing that I'm most proud of is

00:01:07,770 --> 00:01:12,240
actually I've deleted more code than add

00:01:09,510 --> 00:01:18,060
it in spark once you consider all the

00:01:12,240 --> 00:01:20,189
porter class combined but basically the

00:01:18,060 --> 00:01:22,470
night my net contribution reached zero I

00:01:20,189 --> 00:01:26,100
pop a bottle champagne like great have

00:01:22,470 --> 00:01:29,180
made it the so here's the three things

00:01:26,100 --> 00:01:33,180
I'll be talking about let's get started

00:01:29,180 --> 00:01:35,700
sparks are also around 10 years old by

00:01:33,180 --> 00:01:37,950
this time mr. Diaz aversive academic

00:01:35,700 --> 00:01:40,350
prototype around 10 years ago and the

00:01:37,950 --> 00:01:43,409
reason we started was because of this

00:01:40,350 --> 00:01:45,509
guy lustre and ten years ago Lester was

00:01:43,409 --> 00:01:46,950
a PhD student also at the UC Berkeley

00:01:45,509 --> 00:01:48,860
amp lab he was doing machine learning

00:01:46,950 --> 00:01:53,520
advised by Michael Jordan

00:01:48,860 --> 00:01:55,680
the and luster back then found out one

00:01:53,520 --> 00:01:58,049
thing online which the Netflix price how

00:01:55,680 --> 00:01:59,939
many of you remember what this thing was

00:01:58,049 --> 00:02:03,509
about like a third of you put up your

00:01:59,939 --> 00:02:05,040
hands Netflix back then decided hey

00:02:03,509 --> 00:02:07,229
we're gonna create a challenge by

00:02:05,040 --> 00:02:10,080
anonymizing our dataset movie rating

00:02:07,229 --> 00:02:12,270
datasets by our users and we'll put this

00:02:10,080 --> 00:02:13,330
out and create a public contest where

00:02:12,270 --> 00:02:16,270
ever can come up with

00:02:13,330 --> 00:02:19,180
best recommendation algorithm for movies

00:02:16,270 --> 00:02:21,070
will win a million dollars a million

00:02:19,180 --> 00:02:22,180
dollars is a lot of money for Lester

00:02:21,070 --> 00:02:24,490
back then

00:02:22,180 --> 00:02:27,250
Lester was making about $2,000 a month

00:02:24,490 --> 00:02:28,990
out of his stipend unlike the PhD

00:02:27,250 --> 00:02:32,920
students at EPFL he was making a lot

00:02:28,990 --> 00:02:35,740
more the so he jump on the con the

00:02:32,920 --> 00:02:38,020
contest but he soon realized hey this is

00:02:35,740 --> 00:02:40,960
the first dataset he had to work with

00:02:38,020 --> 00:02:43,510
there was larger than the amount of disk

00:02:40,960 --> 00:02:45,250
space on his laptop back then and he

00:02:43,510 --> 00:02:47,950
needed some solutions had to scale out

00:02:45,250 --> 00:02:50,290
to process all of those usual waiting

00:02:47,950 --> 00:02:51,910
data and he looked around it's not a lot

00:02:50,290 --> 00:02:53,650
of good solutions back then they either

00:02:51,910 --> 00:02:54,850
work really well on a single nope it

00:02:53,650 --> 00:02:56,320
doesn't work well and that this should

00:02:54,850 --> 00:02:57,970
be the setting oh they worked well in

00:02:56,320 --> 00:02:59,800
the distributor setting but it's very

00:02:57,970 --> 00:03:02,320
inefficient the prototype it's very slow

00:02:59,800 --> 00:03:04,330
to iterate so Lester talk to this guy

00:03:02,320 --> 00:03:07,540
Matt a but he's a hurry up many of you

00:03:04,330 --> 00:03:09,430
know original creators have sparked it

00:03:07,540 --> 00:03:12,340
home at hey hey I think if you give me a

00:03:09,430 --> 00:03:14,350
couple primitives I could actually

00:03:12,340 --> 00:03:16,750
leverage that to protime my machine

00:03:14,350 --> 00:03:18,670
learning algorithms very quickly and one

00:03:16,750 --> 00:03:20,770
of the key with machine learning is you

00:03:18,670 --> 00:03:22,930
have to be doing a lot of experiments so

00:03:20,770 --> 00:03:24,610
iterative speed matters a lot more than

00:03:22,930 --> 00:03:25,720
the result you get in these particular

00:03:24,610 --> 00:03:28,959
snapshot in time

00:03:25,720 --> 00:03:30,760
so Matteo should roll I think he first

00:03:28,959 --> 00:03:32,320
he might've first tried Ruby and then

00:03:30,760 --> 00:03:34,780
later decided his doesn't really work

00:03:32,320 --> 00:03:37,270
let me use a more proper language and

00:03:34,780 --> 00:03:39,810
then he actually picked up Scala so I

00:03:37,270 --> 00:03:42,100
think Scala was also one of the first

00:03:39,810 --> 00:03:46,810
actually spark was one of the first

00:03:42,100 --> 00:03:49,150
projects Matea use Scala for the so

00:03:46,810 --> 00:03:50,620
about weekend later only six hundred

00:03:49,150 --> 00:03:52,360
lines of code there's the very very

00:03:50,620 --> 00:03:54,100
first version of spark of course it

00:03:52,360 --> 00:03:56,500
looked very different from spark what

00:03:54,100 --> 00:03:58,510
sparked is today but we call it the

00:03:56,500 --> 00:04:00,489
first unify analytics engine in the

00:03:58,510 --> 00:04:03,580
sense that sparked had two

00:04:00,489 --> 00:04:07,180
functionalities one is there's a way to

00:04:03,580 --> 00:04:08,530
process data because a lot of so a lot

00:04:07,180 --> 00:04:10,810
of parts of the machine learning is

00:04:08,530 --> 00:04:12,400
about how to process and prepare data to

00:04:10,810 --> 00:04:14,200
be ready for machine learning models to

00:04:12,400 --> 00:04:16,450
train on the second part is the actual

00:04:14,200 --> 00:04:18,609
you spark to pro time machine learning

00:04:16,450 --> 00:04:21,209
algorithms especially edition billions

00:04:18,609 --> 00:04:22,770
and

00:04:21,209 --> 00:04:25,440
it's kind of smart just took off slowly

00:04:22,770 --> 00:04:27,570
from there okay but what happened to

00:04:25,440 --> 00:04:29,759
luster it's usually what people care

00:04:27,570 --> 00:04:33,180
about so this is actually the board of

00:04:29,759 --> 00:04:34,940
the Netflix price from back then and you

00:04:33,180 --> 00:04:39,530
can see the top two places are tied

00:04:34,940 --> 00:04:42,419
unless there was part of ensemble team

00:04:39,530 --> 00:04:45,690
for the ensemble team submit it 20

00:04:42,419 --> 00:04:48,539
minutes later in the first place so they

00:04:45,690 --> 00:04:49,979
lost a million dollars and here's a

00:04:48,539 --> 00:04:54,270
picture of the other team happily

00:04:49,979 --> 00:04:56,130
accepting the check if spark was

00:04:54,270 --> 00:04:57,449
invented 20 minutes earlier last step

00:04:56,130 --> 00:05:02,220
would it be the million dollar richer

00:04:57,449 --> 00:05:04,199
right now a lot has changed

00:05:02,220 --> 00:05:06,780
we started daily bricks the company in

00:05:04,199 --> 00:05:08,550
2013 about six years ago we started

00:05:06,780 --> 00:05:10,770
working with a lot of real real world

00:05:08,550 --> 00:05:12,509
customers in production and we tried to

00:05:10,770 --> 00:05:14,310
do as to understand here what are people

00:05:12,509 --> 00:05:16,320
doing with data what are the biggest

00:05:14,310 --> 00:05:18,270
challenges people are facing and there's

00:05:16,320 --> 00:05:20,789
a couple assumptions we made when we

00:05:18,270 --> 00:05:22,979
started data brings one and I think a

00:05:20,789 --> 00:05:24,360
very important one was hey sparks and

00:05:22,979 --> 00:05:26,880
you have an analytics engine that's all

00:05:24,360 --> 00:05:28,800
you need we can do machine learning you

00:05:26,880 --> 00:05:30,630
can train the models for you you can do

00:05:28,800 --> 00:05:32,490
data prep for you that's it we're just

00:05:30,630 --> 00:05:33,750
gonna focus on that everything else

00:05:32,490 --> 00:05:35,490
let's forget about it

00:05:33,750 --> 00:05:39,360
but as we work more and more with

00:05:35,490 --> 00:05:40,860
customers we start to realize hey spark

00:05:39,360 --> 00:05:43,050
is pretty powerful but there's actually

00:05:40,860 --> 00:05:44,849
a lot of surrounding pieces are --court

00:05:43,050 --> 00:05:46,409
who I say data engineering and also

00:05:44,849 --> 00:05:48,720
software engineering the nos say is

00:05:46,409 --> 00:05:49,860
solved by spark itself it's a matter of

00:05:48,720 --> 00:05:51,990
facts a completely different paradigm

00:05:49,860 --> 00:05:53,880
the smart dazeem and touches so it's not

00:05:51,990 --> 00:05:57,060
even appropriate to just extend spark to

00:05:53,880 --> 00:05:59,550
cover some of the use cases and one of

00:05:57,060 --> 00:06:01,349
the misconception about machine learning

00:05:59,550 --> 00:06:03,360
in this industry is when you talk to a

00:06:01,349 --> 00:06:05,669
lot of people they haven't done it for a

00:06:03,360 --> 00:06:07,530
while they would think hey machine

00:06:05,669 --> 00:06:09,930
learning is about if I do download 10 so

00:06:07,530 --> 00:06:12,389
for download hi torch and I apply the

00:06:09,930 --> 00:06:14,909
latest neural net architecture and a fee

00:06:12,389 --> 00:06:18,270
some data into it and boom I get great

00:06:14,909 --> 00:06:19,759
results but the reality is there's

00:06:18,270 --> 00:06:22,139
actually a lot more to machine learning

00:06:19,759 --> 00:06:23,970
than just machine learning itself and

00:06:22,139 --> 00:06:26,580
this is I think this is probably the

00:06:23,970 --> 00:06:29,550
best illustration of that I've taken out

00:06:26,580 --> 00:06:30,659
of a Google nips paper in 2015 hidden

00:06:29,550 --> 00:06:33,120
technical debt and machine learning

00:06:30,659 --> 00:06:35,130
systems and what this chart shows is

00:06:33,120 --> 00:06:37,890
Google has taken

00:06:35,130 --> 00:06:39,240
they've done the analysis of all their

00:06:37,890 --> 00:06:41,850
machine learning applications within

00:06:39,240 --> 00:06:44,070
Google and YouTube the box represent a

00:06:41,850 --> 00:06:45,900
specific module for the machine learning

00:06:44,070 --> 00:06:48,240
applications and the size of the box

00:06:45,900 --> 00:06:50,940
indicated basically the amount of code

00:06:48,240 --> 00:06:53,220
that's written for those systems so the

00:06:50,940 --> 00:06:56,760
proxy for complexity of the components

00:06:53,220 --> 00:06:59,070
as you can see or maybe you can't see in

00:06:56,760 --> 00:07:01,310
the middle of it it's a little black box

00:06:59,070 --> 00:07:03,690
this says machine learning code and

00:07:01,310 --> 00:07:05,400
everywhere else is a lot of other boxes

00:07:03,690 --> 00:07:07,500
configuration data collection

00:07:05,400 --> 00:07:10,170
verification serving infrastructure

00:07:07,500 --> 00:07:13,230
resource management all that are not

00:07:10,170 --> 00:07:15,720
specific to machine learning but are

00:07:13,230 --> 00:07:17,190
things that needs to be done as a matter

00:07:15,720 --> 00:07:18,750
of fact when you have to build real war

00:07:17,190 --> 00:07:22,920
machine learning applications you spend

00:07:18,750 --> 00:07:25,200
most of the time doing this and even

00:07:22,920 --> 00:07:27,120
more interestingly is people that really

00:07:25,200 --> 00:07:28,410
understand the black box are the hard

00:07:27,120 --> 00:07:30,060
core data scientist or machine learning

00:07:28,410 --> 00:07:32,910
engineers but people they actually

00:07:30,060 --> 00:07:34,500
understand the rest of the boxes are

00:07:32,910 --> 00:07:37,980
what we call data engineers who software

00:07:34,500 --> 00:07:40,470
engineers and this to persona is used

00:07:37,980 --> 00:07:42,240
very different technologies they often

00:07:40,470 --> 00:07:44,280
sit in different places in many

00:07:42,240 --> 00:07:46,140
enterprises they don't even have the

00:07:44,280 --> 00:07:47,280
same reporting chain one of my favorite

00:07:46,140 --> 00:07:50,160
question might go through a customer's

00:07:47,280 --> 00:07:51,540
ask them hit you know where if I see a

00:07:50,160 --> 00:07:53,310
data scientist ask them hey we do you

00:07:51,540 --> 00:07:55,380
know where data engineers sit in a

00:07:53,310 --> 00:07:57,870
building and often the response is no I

00:07:55,380 --> 00:07:59,640
they occasionally talk to them on slack

00:07:57,870 --> 00:08:00,900
and the tech that you is also very

00:07:59,640 --> 00:08:02,700
different which created a lot of

00:08:00,900 --> 00:08:06,660
challenges in building this overall

00:08:02,700 --> 00:08:08,760
systems so what we've been doing a lot

00:08:06,660 --> 00:08:10,500
of that data breaks after realizing that

00:08:08,760 --> 00:08:12,230
is how do we bridge the gap between the

00:08:10,500 --> 00:08:15,330
different personas how do we actually

00:08:12,230 --> 00:08:19,560
make all this work by enabling people to

00:08:15,330 --> 00:08:21,030
collaborate by unifying them right so

00:08:19,560 --> 00:08:23,340
with that I want to talk to you about

00:08:21,030 --> 00:08:25,170
two separate projects the first one is

00:08:23,340 --> 00:08:28,050
Delta it's about scalable and reliable

00:08:25,170 --> 00:08:30,540
data lakes and the key to Delta is to

00:08:28,050 --> 00:08:32,490
focus on making your data ready for

00:08:30,540 --> 00:08:33,780
analytics which could include a lot of

00:08:32,490 --> 00:08:35,760
them are just doing machine learning and

00:08:33,780 --> 00:08:37,770
the second part is ml flow which focused

00:08:35,760 --> 00:08:41,610
more on a lifecycle management of

00:08:37,770 --> 00:08:43,229
machinery let's get started from I don't

00:08:41,610 --> 00:08:45,870
know how many of you are data engineers

00:08:43,229 --> 00:08:48,300
here scoffs extremely essential to data

00:08:45,870 --> 00:08:49,070
engineers life many the event Starla's

00:08:48,300 --> 00:08:50,209
basis

00:08:49,070 --> 00:08:52,130
to a programming language for data

00:08:50,209 --> 00:08:55,040
engineers a lot of it thanks to spark

00:08:52,130 --> 00:08:58,699
and Kafka and a lot of frameworks but

00:08:55,040 --> 00:09:01,490
one of the big sort of change in the

00:08:58,699 --> 00:09:03,649
industry for data in the past decade is

00:09:01,490 --> 00:09:06,050
the transition or the addition of the

00:09:03,649 --> 00:09:08,149
data lakes in addition to shift your

00:09:06,050 --> 00:09:09,850
school data warehouses and the concept

00:09:08,149 --> 00:09:12,350
of data Lake is pretty simple you

00:09:09,850 --> 00:09:14,630
collect all of your data everything you

00:09:12,350 --> 00:09:18,560
have structure on structure sensor data

00:09:14,630 --> 00:09:20,779
images tables transactions logs and dump

00:09:18,560 --> 00:09:23,300
into this data Lake which typically is a

00:09:20,779 --> 00:09:26,060
distributed file system or some object

00:09:23,300 --> 00:09:28,160
store in the cloud like s3 and then you

00:09:26,060 --> 00:09:30,829
can have all our data there for

00:09:28,160 --> 00:09:33,139
downstream analytics use cases do fancy

00:09:30,829 --> 00:09:35,089
machine learning algorithms to data

00:09:33,139 --> 00:09:37,459
science on it and your world is perfect

00:09:35,089 --> 00:09:40,490
and it's kind of a picture the industry

00:09:37,459 --> 00:09:43,009
has painted right and when we started

00:09:40,490 --> 00:09:45,290
data brings one of the assumption we won

00:09:43,009 --> 00:09:47,420
with Hayes let's focus on computer which

00:09:45,290 --> 00:09:49,759
is SPARC and storage des likes about

00:09:47,420 --> 00:09:53,750
storage I think it's a saw prom we

00:09:49,759 --> 00:09:56,959
actually believe this picture until we

00:09:53,750 --> 00:09:59,810
started building data leaks ourselves

00:09:56,959 --> 00:10:02,120
and we realize a few problems and here's

00:09:59,810 --> 00:10:03,800
essentially mirrors our journey we had

00:10:02,120 --> 00:10:05,420
to go through at data bricks and

00:10:03,800 --> 00:10:07,459
building our internal data leaks and a

00:10:05,420 --> 00:10:10,639
lot of our data pipelines so what does

00:10:07,459 --> 00:10:11,930
it look like what we wanted to do is we

00:10:10,639 --> 00:10:15,290
have a lot of events that we collect

00:10:11,930 --> 00:10:16,730
from our services they actually in the

00:10:15,290 --> 00:10:19,490
order I think tens of terabytes a day

00:10:16,730 --> 00:10:21,769
and then there's a few things we want to

00:10:19,490 --> 00:10:23,569
do one is we want to report our metrics

00:10:21,769 --> 00:10:26,510
in real time so we can see what

00:10:23,569 --> 00:10:28,639
exceptions are or curring across all of

00:10:26,510 --> 00:10:29,899
our manage bar clusters we want to see

00:10:28,639 --> 00:10:32,839
how people are using the spark cluster

00:10:29,899 --> 00:10:35,149
is how you put our using spark api's and

00:10:32,839 --> 00:10:37,040
so it's a streaming analytics and the

00:10:35,149 --> 00:10:39,260
same time we also want to dump all those

00:10:37,040 --> 00:10:41,389
into a data Lake so we can do historic

00:10:39,260 --> 00:10:44,899
analysis for example when we want to

00:10:41,389 --> 00:10:46,970
analyze or to decide hey what API can we

00:10:44,899 --> 00:10:48,560
break in the future for say the next

00:10:46,970 --> 00:10:50,839
version of spark wouldn't analyze how

00:10:48,560 --> 00:10:52,790
often do people rely on the API if the

00:10:50,839 --> 00:10:54,709
Navy had almost nobody uses it's causing

00:10:52,790 --> 00:10:58,519
a lot of pain maybe should we remove it

00:10:54,709 --> 00:11:00,410
right and then we also have machine

00:10:58,519 --> 00:11:01,760
learning algorithms running to predict

00:11:00,410 --> 00:11:03,590
for example what's the behavior of

00:11:01,760 --> 00:11:06,920
different

00:11:03,590 --> 00:11:08,940
customers and users in terms of usage

00:11:06,920 --> 00:11:10,560
now the first thing we have the bureau

00:11:08,940 --> 00:11:13,320
is hey the spill streaming analytics

00:11:10,560 --> 00:11:16,020
alright once we dump all the events into

00:11:13,320 --> 00:11:17,700
Kafka it's not too difficult to actually

00:11:16,020 --> 00:11:20,190
get spark working you writes with a

00:11:17,700 --> 00:11:21,840
streaming job in spark and can get your

00:11:20,190 --> 00:11:23,460
streaming analytics in right you get

00:11:21,840 --> 00:11:25,080
your events coming in in a few seconds

00:11:23,460 --> 00:11:29,070
later and they show up on the dashboard

00:11:25,080 --> 00:11:31,050
somewhere it's great no the problem is

00:11:29,070 --> 00:11:33,990
hey but we do also want to analyze and

00:11:31,050 --> 00:11:36,480
save all those events and our dashboards

00:11:33,990 --> 00:11:38,580
can't just look at the last maybe day it

00:11:36,480 --> 00:11:41,310
also in the history of cat in many cases

00:11:38,580 --> 00:11:43,050
were present for example to a CEO or to

00:11:41,310 --> 00:11:46,170
the board you need to show historic

00:11:43,050 --> 00:11:47,760
trends right so we applied this Trinette

00:11:46,170 --> 00:11:48,950
little trickle and the architecture

00:11:47,760 --> 00:11:50,790
which many of you might have heard of

00:11:48,950 --> 00:11:52,590
from what he does is

00:11:50,790 --> 00:11:54,240
hey you bifurcate your pipelines you

00:11:52,590 --> 00:11:56,490
have one pipeline for real-time you have

00:11:54,240 --> 00:11:59,700
one pipeline for batch which is

00:11:56,490 --> 00:12:01,740
basically the offline historic data now

00:11:59,700 --> 00:12:05,160
every time you bifurcate in software

00:12:01,740 --> 00:12:07,620
engineering your risk architecture

00:12:05,160 --> 00:12:09,270
attack that as well as correctness bugs

00:12:07,620 --> 00:12:12,720
because now you have to write everything

00:12:09,270 --> 00:12:15,330
twice right but it's not too bad because

00:12:12,720 --> 00:12:18,000
spark in many ways to spark a pion

00:12:15,330 --> 00:12:19,860
allows you to express one set of program

00:12:18,000 --> 00:12:22,230
and you can run a boat in a batch

00:12:19,860 --> 00:12:25,040
fashion in a streaming fashion and all

00:12:22,230 --> 00:12:27,090
you do is to really configure it and

00:12:25,040 --> 00:12:28,500
then the other thing we need to do is

00:12:27,090 --> 00:12:30,630
okay we need to do a on the reporting

00:12:28,500 --> 00:12:33,720
but one of the problem without your

00:12:30,630 --> 00:12:36,000
streaming data is creates a lot of SEO

00:12:33,720 --> 00:12:38,160
create as you have events coming in real

00:12:36,000 --> 00:12:39,960
time and you want to really drive down

00:12:38,160 --> 00:12:42,120
your latency we're start writing out the

00:12:39,960 --> 00:12:44,910
data very quickly to your data lakes in

00:12:42,120 --> 00:12:47,730
our case s3 and as part of that you

00:12:44,910 --> 00:12:50,280
create a lot of actually there's a few

00:12:47,730 --> 00:12:51,870
problem when small data small files so

00:12:50,280 --> 00:12:54,630
you create a lot of small files and you

00:12:51,870 --> 00:12:56,820
turn down your profile jobs the time are

00:12:54,630 --> 00:12:58,410
dominated by just metadata fetching of

00:12:56,820 --> 00:13:01,500
those small files or even just file this

00:12:58,410 --> 00:13:03,870
thing um the other problem is as the

00:13:01,500 --> 00:13:06,180
number of pipelines increase so I'm

00:13:03,870 --> 00:13:07,650
joining you here just one simple diagram

00:13:06,180 --> 00:13:09,210
but in reality the different data

00:13:07,650 --> 00:13:11,040
engineers running different pipelines

00:13:09,210 --> 00:13:14,400
sometimes are collecting from different

00:13:11,040 --> 00:13:16,300
data sources when you actually have one

00:13:14,400 --> 00:13:18,190
program writing data in the specifics

00:13:16,300 --> 00:13:19,660
kima and not a program written three

00:13:18,190 --> 00:13:21,520
months later made by a different team

00:13:19,660 --> 00:13:22,960
they assume a slightly different schema

00:13:21,520 --> 00:13:26,620
writing to the same source now you have

00:13:22,960 --> 00:13:28,900
pretty messy data um some people added

00:13:26,620 --> 00:13:31,570
validation hey let's make sure we have a

00:13:28,900 --> 00:13:34,210
validation jobs and of course you have

00:13:31,570 --> 00:13:36,550
to add it to both places right and the

00:13:34,210 --> 00:13:39,490
other bigger problem is hey

00:13:36,550 --> 00:13:44,050
we write software with bugs everybody

00:13:39,490 --> 00:13:46,750
does what happens if there's a actually

00:13:44,050 --> 00:13:48,190
some failure sometimes not even because

00:13:46,750 --> 00:13:51,610
of our box because some machine went

00:13:48,190 --> 00:13:54,190
down the middle of processing the the

00:13:51,610 --> 00:13:56,710
waste of most of Industry worked out to

00:13:54,190 --> 00:13:59,950
separate tackle desist a partition their

00:13:56,710 --> 00:14:03,520
data in disjoint chunks for example by

00:13:59,950 --> 00:14:05,350
date and every time if this fail job and

00:14:03,520 --> 00:14:08,710
you have partially written out specific

00:14:05,350 --> 00:14:10,990
set of data for example for today the

00:14:08,710 --> 00:14:13,600
next time we want to fix it you override

00:14:10,990 --> 00:14:15,580
the entire day of data all right now

00:14:13,600 --> 00:14:18,820
we're just adding more and more logic to

00:14:15,580 --> 00:14:20,740
the application and then you reprocess

00:14:18,820 --> 00:14:22,930
today's data and sometimes you found a

00:14:20,740 --> 00:14:24,360
bug three days maybe later you replace

00:14:22,930 --> 00:14:27,220
all the three days data

00:14:24,360 --> 00:14:28,960
I'm through processing and then the

00:14:27,220 --> 00:14:30,910
other one is hey but occasionally I

00:14:28,960 --> 00:14:33,070
realize hey maybe one of my customer

00:14:30,910 --> 00:14:35,410
changed their name I don't really want

00:14:33,070 --> 00:14:39,430
to like how do I actually update that

00:14:35,410 --> 00:14:41,770
reflect on all my records the standard

00:14:39,430 --> 00:14:43,900
processes that actually do also

00:14:41,770 --> 00:14:46,030
partition replacements so sometimes you

00:14:43,900 --> 00:14:47,590
partition the customers you partition

00:14:46,030 --> 00:14:48,910
the data also by customers and then you

00:14:47,590 --> 00:14:50,650
replace the entire customers data

00:14:48,910 --> 00:14:53,080
sometimes if you can't do that for

00:14:50,650 --> 00:14:56,200
example you have too many customers you

00:14:53,080 --> 00:14:57,760
have to rewrite all of your data it's

00:14:56,200 --> 00:14:59,770
kind of very difficult and then the last

00:14:57,760 --> 00:15:01,450
but not least is hey I have downstream

00:14:59,770 --> 00:15:04,210
jobs are also during it maybe real-time

00:15:01,450 --> 00:15:09,990
querying on the data everything out and

00:15:04,210 --> 00:15:13,270
now every time you have every process

00:15:09,990 --> 00:15:14,920
the reprocessing required deleting some

00:15:13,270 --> 00:15:16,630
of the existing data and if there

00:15:14,920 --> 00:15:18,400
happened to be one of the job that's

00:15:16,630 --> 00:15:21,550
actually reading the data is being

00:15:18,400 --> 00:15:23,890
deleted that job would fail so now you

00:15:21,550 --> 00:15:25,570
have to even come up with this of

00:15:23,890 --> 00:15:27,430
scheduling ways of doing hey man section

00:15:25,570 --> 00:15:29,380
maybe I only run in at 3 a.m. San

00:15:27,430 --> 00:15:30,160
Francisco time until they added a

00:15:29,380 --> 00:15:33,860
European

00:15:30,160 --> 00:15:35,240
every safe everybody's good right so

00:15:33,860 --> 00:15:36,740
it's a lot of complexity everyone we

00:15:35,240 --> 00:15:38,240
realize we have this team code data

00:15:36,740 --> 00:15:40,100
engineering team with five people five

00:15:38,240 --> 00:15:41,840
engineers a rock solid data engineers

00:15:40,100 --> 00:15:43,370
but really what they're solving is not

00:15:41,840 --> 00:15:45,560
data problems they just solving a lot of

00:15:43,370 --> 00:15:48,500
distributive system problems and

00:15:45,560 --> 00:15:50,060
concurrency problems and that's because

00:15:48,500 --> 00:15:51,350
actually the underlying data that

00:15:50,060 --> 00:15:54,740
doesn't provide them the sufficient

00:15:51,350 --> 00:15:57,680
guarantees and they are distracted by I

00:15:54,740 --> 00:15:59,180
think three or four big things and here

00:15:57,680 --> 00:15:59,510
I'm gonna make particularly a little bit

00:15:59,180 --> 00:16:02,300
more

00:15:59,510 --> 00:16:03,980
the first is no autonomous City provided

00:16:02,300 --> 00:16:08,240
by the underlying data links the storage

00:16:03,980 --> 00:16:11,420
system just takes whatever it gets if

00:16:08,240 --> 00:16:13,100
you have a partial right coming from a

00:16:11,420 --> 00:16:14,690
job that actually fell the source system

00:16:13,100 --> 00:16:16,190
doesn't understand any of it because

00:16:14,690 --> 00:16:18,170
just a but this should be the file

00:16:16,190 --> 00:16:21,380
system doesn't have any higher-level

00:16:18,170 --> 00:16:23,090
semantics and when you have about your

00:16:21,380 --> 00:16:25,220
partially written data it's very

00:16:23,090 --> 00:16:26,930
difficult to trust the data system and

00:16:25,220 --> 00:16:29,780
the other one is there's no quality

00:16:26,930 --> 00:16:31,910
enforcement you could have if writing

00:16:29,780 --> 00:16:34,400
garbage you have data coming from with

00:16:31,910 --> 00:16:37,070
slightly different schema and that's

00:16:34,400 --> 00:16:40,550
very difficult to actually make sure the

00:16:37,070 --> 00:16:43,370
downstream jobs are correct and lasses

00:16:40,550 --> 00:16:45,260
there's no isolation which means when

00:16:43,370 --> 00:16:47,570
you have a right job as deleting data is

00:16:45,260 --> 00:16:53,090
to do two reprocessing the read job

00:16:47,570 --> 00:16:55,040
would just fail right and maybe to show

00:16:53,090 --> 00:16:58,310
you how real and how widespread this

00:16:55,040 --> 00:17:00,800
promise I've also went through a lot of

00:16:58,310 --> 00:17:02,990
my email so that's of support tickets

00:17:00,800 --> 00:17:05,690
I've gotten from our customers and have

00:17:02,990 --> 00:17:08,330
taken screenshots of them have naanum

00:17:05,690 --> 00:17:11,300
eyes it there's a lot of problems with

00:17:08,330 --> 00:17:15,260
data leaks despite them being served

00:17:11,300 --> 00:17:17,180
tenure or mature technologies one here's

00:17:15,260 --> 00:17:18,860
some examples extremely slow data frame

00:17:17,180 --> 00:17:20,600
loading commands block on metadata

00:17:18,860 --> 00:17:22,699
operations when profile them

00:17:20,600 --> 00:17:24,440
hey the crew itself finished me buddy

00:17:22,699 --> 00:17:25,130
like a one or two seconds but spends

00:17:24,440 --> 00:17:28,150
like a minute

00:17:25,130 --> 00:17:29,870
listing files figuring out what to read

00:17:28,150 --> 00:17:32,090
they didn't that really is very

00:17:29,870 --> 00:17:33,860
difficult you get fat enough found

00:17:32,090 --> 00:17:35,510
exception this is what I was talking

00:17:33,860 --> 00:17:38,410
about when you have a read job that's

00:17:35,510 --> 00:17:40,790
reading right jobs deleting data

00:17:38,410 --> 00:17:42,590
different fields have conflicting

00:17:40,790 --> 00:17:43,460
schemas because now you might have data

00:17:42,590 --> 00:17:45,640
program

00:17:43,460 --> 00:17:50,179
different times assume different schema

00:17:45,640 --> 00:17:51,919
and see a lot of this coming to many

00:17:50,179 --> 00:17:53,120
small files is one of the classic you've

00:17:51,919 --> 00:17:55,520
spent any time doing with data

00:17:53,120 --> 00:17:56,779
engineering concatenate hey if have to

00:17:55,520 --> 00:17:59,120
make small files I need to concatenate

00:17:56,779 --> 00:18:00,380
them how do I control them and all that

00:17:59,120 --> 00:18:02,630
alright everybody runs in as a matter of

00:18:00,380 --> 00:18:04,279
fact at some point this class of issues

00:18:02,630 --> 00:18:08,870
I think take up half of my engineering

00:18:04,279 --> 00:18:11,690
support ticket so we started working on

00:18:08,870 --> 00:18:13,220
after realizing that I hate half of our

00:18:11,690 --> 00:18:15,289
support occurs I didn't have not very

00:18:13,220 --> 00:18:17,330
little to do with spark or anything is

00:18:15,289 --> 00:18:18,649
really just the underlying store system

00:18:17,330 --> 00:18:21,049
doesn't quite provide you the right

00:18:18,649 --> 00:18:26,419
guarantees we started in your open

00:18:21,049 --> 00:18:29,330
source project or Delta and before I

00:18:26,419 --> 00:18:31,279
tell you how Delta works I was maybe

00:18:29,330 --> 00:18:35,750
explain to you what the picture look

00:18:31,279 --> 00:18:39,940
like when you go from this to Delta this

00:18:35,750 --> 00:18:42,799
is pretty complicated but with Delta

00:18:39,940 --> 00:18:44,179
here's the picture we get with Delta so

00:18:42,799 --> 00:18:46,010
you still have all your events is coming

00:18:44,179 --> 00:18:47,510
in from variety of different sources and

00:18:46,010 --> 00:18:50,000
you actually dump them into a single

00:18:47,510 --> 00:18:52,580
Delta table and typically the first day

00:18:50,000 --> 00:18:55,520
of the table would be a what we call

00:18:52,580 --> 00:18:58,789
bronze table and it just includes raw

00:18:55,520 --> 00:19:01,340
events and then you incrementally refine

00:18:58,789 --> 00:19:03,140
them you create so the pipeline of Delta

00:19:01,340 --> 00:19:06,679
tables and each of them you have some

00:19:03,140 --> 00:19:08,929
ETL job connecting them to go from one

00:19:06,679 --> 00:19:10,250
to another and when we see it you might

00:19:08,929 --> 00:19:12,140
not actually get for example exactly

00:19:10,250 --> 00:19:13,640
three you might have a pipe on a 20 you

00:19:12,140 --> 00:19:15,350
may have a pile at 30 and in my

00:19:13,640 --> 00:19:16,970
bifurcate because you have different

00:19:15,350 --> 00:19:19,100
business logics but the way it works is

00:19:16,970 --> 00:19:21,470
the first one is typically the raw event

00:19:19,100 --> 00:19:24,080
there's like no parsing virtually no

00:19:21,470 --> 00:19:26,779
application logic so it becomes an

00:19:24,080 --> 00:19:28,580
archive and it's a which the infinite

00:19:26,779 --> 00:19:30,380
retention because it's how much you can

00:19:28,580 --> 00:19:33,830
fit in your distributed file system r3

00:19:30,380 --> 00:19:35,929
and then you incrementally refine the

00:19:33,830 --> 00:19:38,390
quality of the data goes higher and

00:19:35,929 --> 00:19:40,850
higher as you go through this Delta

00:19:38,390 --> 00:19:42,380
pipelines and at some point you have

00:19:40,850 --> 00:19:45,380
some data is completely ready for

00:19:42,380 --> 00:19:47,360
analytics that's either doing machine

00:19:45,380 --> 00:19:51,919
learning or either doing streaming or

00:19:47,360 --> 00:19:54,679
doing a dash boarding and what dalvik

00:19:51,919 --> 00:19:57,679
provides is a few guarantees one is it

00:19:54,679 --> 00:19:59,690
has foo asset transactions

00:19:57,679 --> 00:20:03,049
so now you actually get authenticity you

00:19:59,690 --> 00:20:06,049
get isolation and you get basically the

00:20:03,049 --> 00:20:10,100
sequel style transactions and dela

00:20:06,049 --> 00:20:11,870
superstores powered by spark but maybe

00:20:10,100 --> 00:20:15,409
just walk through a little bit so bronze

00:20:11,870 --> 00:20:17,539
table as simple as possible so whatever

00:20:15,409 --> 00:20:21,909
you're reading right the whole point of

00:20:17,539 --> 00:20:21,909
that is to store all of the raw data

00:20:22,029 --> 00:20:30,049
let's click on acting and then the

00:20:28,370 --> 00:20:31,789
intermediate table sometimes maybe you

00:20:30,049 --> 00:20:33,500
parse you do some parsing for example

00:20:31,789 --> 00:20:35,240
JSON parsing so now you structure the

00:20:33,500 --> 00:20:36,679
JSON into different fields and maybe

00:20:35,240 --> 00:20:38,990
you'll clean up some garbage you have

00:20:36,679 --> 00:20:40,520
and then often the business of our gates

00:20:38,990 --> 00:20:41,630
people actually rolled up the data for

00:20:40,520 --> 00:20:43,899
example you might be getting your

00:20:41,630 --> 00:20:46,909
advanced data in say micro secondly

00:20:43,899 --> 00:20:49,039
granularity but for the purpose of the

00:20:46,909 --> 00:20:50,750
end use cases you don't quite care about

00:20:49,039 --> 00:20:53,270
a microsecond ground oddity so we roll

00:20:50,750 --> 00:20:55,490
them up to every second or even every

00:20:53,270 --> 00:20:59,779
day sometimes and now you can't you read

00:20:55,490 --> 00:21:02,299
that we spark and now presto and one of

00:20:59,779 --> 00:21:04,039
the key here is hey it looks ok now you

00:21:02,299 --> 00:21:05,510
have a line and maybe we can buy for

00:21:04,039 --> 00:21:08,029
case you have a tree like what's the big

00:21:05,510 --> 00:21:10,130
deal here one of the thing is hey if

00:21:08,029 --> 00:21:11,899
your business logic changes let's say

00:21:10,130 --> 00:21:13,669
the way you parse date/time changes

00:21:11,899 --> 00:21:16,240
slightly because you realize hey you're

00:21:13,669 --> 00:21:20,210
getting date/time it didn't formats now

00:21:16,240 --> 00:21:21,620
it's actually very easy to reprocess all

00:21:20,210 --> 00:21:24,440
the old data because all of them are

00:21:21,620 --> 00:21:26,270
stored in a single Delta table with

00:21:24,440 --> 00:21:26,870
infinite retention google back as long

00:21:26,270 --> 00:21:28,700
as you want

00:21:26,870 --> 00:21:31,940
and then all you need to do is write

00:21:28,700 --> 00:21:33,679
maybe a batch job to actually actually

00:21:31,940 --> 00:21:35,029
all you need to do is write a spark job

00:21:33,679 --> 00:21:38,090
that can run either in batch or

00:21:35,029 --> 00:21:40,340
streaming and then just delete wiped out

00:21:38,090 --> 00:21:43,730
your downstream tables like the silver

00:21:40,340 --> 00:21:45,470
and go tables and then just restart the

00:21:43,730 --> 00:21:51,260
streaming job from a specific point in

00:21:45,470 --> 00:21:54,140
time so it's pretty powerful it makes a

00:21:51,260 --> 00:21:55,940
lot of things very simple it essentially

00:21:54,140 --> 00:21:58,250
after we rolled out Delta among the

00:21:55,940 --> 00:22:01,370
customers that started using it we

00:21:58,250 --> 00:22:03,440
started seeing very few of the tickets

00:22:01,370 --> 00:22:04,820
filed data port but how does it work

00:22:03,440 --> 00:22:06,740
right we have the technical conference

00:22:04,820 --> 00:22:08,750
people care about exactly how does it

00:22:06,740 --> 00:22:10,250
work on the hood it seemed like a little

00:22:08,750 --> 00:22:12,380
bit magic

00:22:10,250 --> 00:22:14,300
it's the way Delta works is actually

00:22:12,380 --> 00:22:16,790
pretty simple we applies a lot of the

00:22:14,300 --> 00:22:18,200
old school database techniques and we

00:22:16,790 --> 00:22:20,480
base it to this new setting with some

00:22:18,200 --> 00:22:23,620
tweaks and the idea here is we have a

00:22:20,480 --> 00:22:26,330
right ahead transaction log for data and

00:22:23,620 --> 00:22:28,850
if you look at any of the Delta table

00:22:26,330 --> 00:22:30,530
this store under the hood you realize

00:22:28,850 --> 00:22:32,240
hey there's data files and the data

00:22:30,530 --> 00:22:35,000
files are typically partitioned by its

00:22:32,240 --> 00:22:36,470
for example date or country and you have

00:22:35,000 --> 00:22:38,390
the data files that store in park'

00:22:36,470 --> 00:22:41,570
format which the most common columnar

00:22:38,390 --> 00:22:43,790
format for our data and then you have

00:22:41,570 --> 00:22:45,740
the transaction log in addition to

00:22:43,790 --> 00:22:47,180
restoring a bunch of raw files the

00:22:45,740 --> 00:22:49,070
transaction logs are labeled they're

00:22:47,180 --> 00:22:51,440
monotonically increasing it was like

00:22:49,070 --> 00:22:55,040
zero days on one JSON just keep going up

00:22:51,440 --> 00:22:56,810
and a table was basically defined by a

00:22:55,040 --> 00:22:59,600
set of actions in the transaction log

00:22:56,810 --> 00:23:01,280
and the different actions here are I'm

00:22:59,600 --> 00:23:04,060
just gonna miss all of them

00:23:01,280 --> 00:23:06,470
you have adding a file removing a foul

00:23:04,060 --> 00:23:08,410
and there's a bunch of other thing it's

00:23:06,470 --> 00:23:11,960
a little bit difficult to get into here

00:23:08,410 --> 00:23:13,490
but once you have actually for example

00:23:11,960 --> 00:23:16,370
really in all of the transaction logs

00:23:13,490 --> 00:23:18,380
you actively have the latest snapshot of

00:23:16,370 --> 00:23:23,300
the table you know what are the valid

00:23:18,380 --> 00:23:26,600
files in the table and if you want to

00:23:23,300 --> 00:23:29,570
actually change it the table will just

00:23:26,600 --> 00:23:31,670
keep adding the new transaction log

00:23:29,570 --> 00:23:33,410
files to it for example the first

00:23:31,670 --> 00:23:35,690
version of the first transaction logs

00:23:33,410 --> 00:23:39,350
zero JSON might say hey adding first

00:23:35,690 --> 00:23:41,960
file and second file but let's say you

00:23:39,350 --> 00:23:44,870
realize hey Wanda partying too dark a is

00:23:41,960 --> 00:23:46,430
too small and reading them is not really

00:23:44,870 --> 00:23:48,800
efficient that's actually coalesce and

00:23:46,430 --> 00:23:49,910
combine them into a single bigger file

00:23:48,800 --> 00:23:52,220
you could actually create a new

00:23:49,910 --> 00:23:54,020
transaction one dot JSON and what they

00:23:52,220 --> 00:23:55,880
described as hey let's remove one and

00:23:54,020 --> 00:23:58,970
two dot park' and it has three top RK

00:23:55,880 --> 00:24:05,600
which is just by writing one and two

00:23:58,970 --> 00:24:08,390
together all right I'm pretty simple now

00:24:05,600 --> 00:24:10,190
one thing to actually enforce and to

00:24:08,390 --> 00:24:11,810
give you a transactional guarantees we

00:24:10,190 --> 00:24:15,320
need to agree on the ordering of changes

00:24:11,810 --> 00:24:17,150
so when there are multiple riders when

00:24:15,320 --> 00:24:19,160
they're multiple riders for example if

00:24:17,150 --> 00:24:21,650
the usual one right zero dot JSON and

00:24:19,160 --> 00:24:23,510
usually two writes one JSON but then

00:24:21,650 --> 00:24:27,230
they both try to write to the

00:24:23,510 --> 00:24:29,600
JSON one of them has to fail so in this

00:24:27,230 --> 00:24:31,670
case if you sure to races at your wins

00:24:29,600 --> 00:24:33,530
you sure to acclaim the two basic

00:24:31,670 --> 00:24:36,890
version - right

00:24:33,530 --> 00:24:38,720
but in many cases if there's actually no

00:24:36,890 --> 00:24:40,580
conflict usually one could just you

00:24:38,720 --> 00:24:42,050
retry and the software does it

00:24:40,580 --> 00:24:43,250
automatically for the users who usually

00:24:42,050 --> 00:24:46,040
doesn't have to worry about it

00:24:43,250 --> 00:24:47,750
- right three dodges on but how do we

00:24:46,040 --> 00:24:49,430
actually solve conflict because for

00:24:47,750 --> 00:24:51,680
example two transactions might conflict

00:24:49,430 --> 00:24:53,060
they might be for example compacting the

00:24:51,680 --> 00:24:55,630
same set of file in the right amount

00:24:53,060 --> 00:24:59,120
that you actually have data duplication

00:24:55,630 --> 00:25:01,280
it's also pretty simple really what we

00:24:59,120 --> 00:25:03,290
do is hey if somebody else commits

00:25:01,280 --> 00:25:06,230
before you for example in this case

00:25:03,290 --> 00:25:07,850
usually two commits after a usual one so

00:25:06,230 --> 00:25:10,460
when usually two tries to commit you

00:25:07,850 --> 00:25:12,890
realize hey once taken what I need to do

00:25:10,460 --> 00:25:14,840
is out reading one JSON to understand

00:25:12,890 --> 00:25:18,470
what was returned and what was deleted

00:25:14,840 --> 00:25:20,420
and if the right set of the user wants

00:25:18,470 --> 00:25:23,600
transaction didn't conflict was my

00:25:20,420 --> 00:25:25,400
resend user - I could just commit again

00:25:23,600 --> 00:25:27,560
without actually failing the end user

00:25:25,400 --> 00:25:29,270
shop but if there's a real conflict for

00:25:27,560 --> 00:25:30,770
example boats are deleting the same file

00:25:29,270 --> 00:25:32,870
it probably means the natural

00:25:30,770 --> 00:25:35,180
application logic conflict now in to

00:25:32,870 --> 00:25:36,710
fail the job of user to tell the user

00:25:35,180 --> 00:25:40,100
hey somebody else have done something

00:25:36,710 --> 00:25:43,070
that conflicts with what you're doing so

00:25:40,100 --> 00:25:45,080
you can try again now up until this

00:25:43,070 --> 00:25:46,940
point Hayes is like super simple have to

00:25:45,080 --> 00:25:48,770
ever take in a database internals class

00:25:46,940 --> 00:25:52,070
you know exactly what's happening here

00:25:48,770 --> 00:25:53,990
right but one of the big change was Big

00:25:52,070 --> 00:25:55,970
Data is hey if you have streaming job

00:25:53,990 --> 00:25:57,710
that's coming in and every maybe you

00:25:55,970 --> 00:25:58,720
have a second day even 100 millisecond

00:25:57,710 --> 00:26:01,340
you're writing in your transaction

00:25:58,720 --> 00:26:04,760
you're gonna have a lot of JSON files it

00:26:01,340 --> 00:26:07,550
will then be a prom if you just turn the

00:26:04,760 --> 00:26:10,760
metadata or too many files prom too many

00:26:07,550 --> 00:26:13,970
metadata file prom so we thought about

00:26:10,760 --> 00:26:16,400
this a lot and we decided hey we

00:26:13,970 --> 00:26:18,680
actually have a very scalable engine to

00:26:16,400 --> 00:26:20,750
process large amount of data and if the

00:26:18,680 --> 00:26:23,180
metadata really gets too large why don't

00:26:20,750 --> 00:26:24,860
we treat mana data exactly as data so

00:26:23,180 --> 00:26:28,100
there's not metadata is not a special

00:26:24,860 --> 00:26:30,410
class in the system and what we do is

00:26:28,100 --> 00:26:32,570
every once in a while we actually take

00:26:30,410 --> 00:26:34,460
all the JSON files read them in using

00:26:32,570 --> 00:26:36,740
spark itself when checkpoint them in to

00:26:34,460 --> 00:26:37,309
park a format so six you me scalable

00:26:36,740 --> 00:26:39,379
higher through

00:26:37,309 --> 00:26:41,360
what to read and then in the future

00:26:39,379 --> 00:26:43,879
we'll just read that checkpoint directly

00:26:41,360 --> 00:26:45,919
from using spark itself so all the

00:26:43,879 --> 00:26:49,460
metadata becomes just normal data here

00:26:45,919 --> 00:26:51,919
and this is how we can also for example

00:26:49,460 --> 00:26:53,539
process billions of files in a single

00:26:51,919 --> 00:26:56,720
table and you could have actually

00:26:53,539 --> 00:27:02,090
hundred petabyte tables because all the

00:26:56,720 --> 00:27:03,710
metadata are no longer bottlenecks um I

00:27:02,090 --> 00:27:05,809
don't have enough time to go into the

00:27:03,710 --> 00:27:07,879
different use cases of Delta here but

00:27:05,809 --> 00:27:09,830
the projects basically one year old it's

00:27:07,879 --> 00:27:11,419
been in production one year old dated

00:27:09,830 --> 00:27:14,090
bricks recently open source state about

00:27:11,419 --> 00:27:16,039
two months ago every month right now is

00:27:14,090 --> 00:27:19,970
processing at you and Zappa pile of data

00:27:16,039 --> 00:27:21,950
on the dataverse platform and it's the

00:27:19,970 --> 00:27:24,169
numbers actually increasing very quickly

00:27:21,950 --> 00:27:25,669
the thing is production-ready it solves

00:27:24,169 --> 00:27:26,840
a lot of the data engineering problem

00:27:25,669 --> 00:27:28,909
they've decided hey it doesn't just

00:27:26,840 --> 00:27:30,200
benefit did theta Brooks customers we're

00:27:28,909 --> 00:27:32,299
gonna create an open-source version of

00:27:30,200 --> 00:27:35,899
it to actually make sure it works for

00:27:32,299 --> 00:27:38,889
everybody all right so did Delta's about

00:27:35,899 --> 00:27:42,139
basically the data engineering pieces of

00:27:38,889 --> 00:27:43,580
analytics and machine learning the next

00:27:42,139 --> 00:27:44,929
one with the talk to you about ml flow

00:27:43,580 --> 00:27:51,440
which machine learning lifecycle

00:27:44,929 --> 00:27:54,460
management and if I were to take a more

00:27:51,440 --> 00:27:56,570
machine-learning view of data pipelines

00:27:54,460 --> 00:27:59,450
or just machine learning pipelines

00:27:56,570 --> 00:28:01,039
already I have shown you were charter

00:27:59,450 --> 00:28:02,450
breaking down into different components

00:28:01,039 --> 00:28:04,999
but here let's simplify a little bit to

00:28:02,450 --> 00:28:06,889
just basically three steps this is a

00:28:04,999 --> 00:28:08,570
typical process machine learning

00:28:06,889 --> 00:28:10,580
engineers the data scientist go through

00:28:08,570 --> 00:28:12,470
and what they do is into prepared first

00:28:10,580 --> 00:28:14,570
thing to prepare data a lot of is done

00:28:12,470 --> 00:28:17,480
since Park a lot of it's done this days

00:28:14,570 --> 00:28:18,980
also in Delta and second is based on

00:28:17,480 --> 00:28:22,610
those data they will be building models

00:28:18,980 --> 00:28:24,559
and one of the things as I talked to you

00:28:22,610 --> 00:28:26,080
about earlier is hey the thing about

00:28:24,559 --> 00:28:28,519
machine learning is you've gotta be

00:28:26,080 --> 00:28:30,320
experimenting a lot to get to the best

00:28:28,519 --> 00:28:31,999
result it's not about the best result

00:28:30,320 --> 00:28:34,639
particular snapshot in time it's about

00:28:31,999 --> 00:28:36,049
iterate iterations so you have to build

00:28:34,639 --> 00:28:38,119
a lot of things have to be doing a lot

00:28:36,049 --> 00:28:39,259
of experiments and then last but not

00:28:38,119 --> 00:28:40,580
least once you have something you're

00:28:39,259 --> 00:28:42,289
actually kind of happy with you have to

00:28:40,580 --> 00:28:43,940
deploy it in production right you can't

00:28:42,289 --> 00:28:44,419
just build a model and say hey I'm done

00:28:43,940 --> 00:28:46,399
with it

00:28:44,419 --> 00:28:48,289
you're gonna use that model somehow and

00:28:46,399 --> 00:28:49,789
there's actually very disparate

00:28:48,289 --> 00:28:51,060
technologies throughout this entire

00:28:49,789 --> 00:28:52,920
stack

00:28:51,060 --> 00:28:55,260
the technologies are not really designed

00:28:52,920 --> 00:28:56,790
to work with each other and as I said

00:28:55,260 --> 00:28:58,830
earlier you also require different

00:28:56,790 --> 00:29:01,200
personas different engineers as

00:28:58,830 --> 00:29:04,470
scientists and they need to be working

00:29:01,200 --> 00:29:07,490
together but there's really no to before

00:29:04,470 --> 00:29:09,780
to make them easier to work together and

00:29:07,490 --> 00:29:11,610
so where are you there's also needs to

00:29:09,780 --> 00:29:14,610
be a way for Standardization across

00:29:11,610 --> 00:29:16,440
these three steps and this is why we

00:29:14,610 --> 00:29:18,690
actually started the open-source ml flow

00:29:16,440 --> 00:29:21,720
projects with three separate components

00:29:18,690 --> 00:29:27,110
tracking projects models now going to

00:29:21,720 --> 00:29:29,880
each one of them so but before I explain

00:29:27,110 --> 00:29:33,860
ml flow let's paint a little bit before

00:29:29,880 --> 00:29:36,150
and after picture so here's how a very

00:29:33,860 --> 00:29:38,430
civil I thought maybe mall that were

00:29:36,150 --> 00:29:41,670
training a CO could look like a lot of

00:29:38,430 --> 00:29:42,960
data Sciences use Python which by the

00:29:41,670 --> 00:29:46,260
way it's also a problem because most

00:29:42,960 --> 00:29:48,060
data engineers don't use Python but they

00:29:46,260 --> 00:29:49,650
load some data and they just like the

00:29:48,060 --> 00:29:53,310
different functions that return a right

00:29:49,650 --> 00:29:55,080
to print debugging right they say hey

00:29:53,310 --> 00:29:57,260
this is some of my parameters for my

00:29:55,080 --> 00:30:00,090
machining model and for my data and

00:29:57,260 --> 00:30:03,510
here's sort of the accuracy I get based

00:30:00,090 --> 00:30:05,190
on my test dataset and once I'm done

00:30:03,510 --> 00:30:08,400
with it I'll dump the model somewhere

00:30:05,190 --> 00:30:09,750
was python pickle so i can actually

00:30:08,400 --> 00:30:12,630
reuse it later in a different program

00:30:09,750 --> 00:30:16,860
and so this is a result you actually get

00:30:12,630 --> 00:30:18,150
looking at the standard out and now

00:30:16,860 --> 00:30:19,680
there's different questions hey what if

00:30:18,150 --> 00:30:21,420
I change my input data this doesn't

00:30:19,680 --> 00:30:24,630
describe my input data it describes the

00:30:21,420 --> 00:30:26,600
parameters machine learning model is

00:30:24,630 --> 00:30:30,360
produced by a combination of Cole

00:30:26,600 --> 00:30:32,840
parameters and data when your data set

00:30:30,360 --> 00:30:35,220
changes you get different models and

00:30:32,840 --> 00:30:37,710
what if I tombs all the other parameters

00:30:35,220 --> 00:30:40,740
now maybe I should put it in a

00:30:37,710 --> 00:30:43,470
spreadsheet and what if I actually

00:30:40,740 --> 00:30:46,020
update what if the library I depend on

00:30:43,470 --> 00:30:48,390
get upgraded and they fix the bug which

00:30:46,020 --> 00:30:53,700
actually led to a regression in my model

00:30:48,390 --> 00:30:56,340
and over maybe a span of a month I've

00:30:53,700 --> 00:30:58,830
changed my program quite a bit what

00:30:56,340 --> 00:31:00,510
happened when I got this lock file what

00:30:58,830 --> 00:31:02,429
did it happen right piece of people a

00:31:00,510 --> 00:31:03,870
lot of them use Excel spreadsheets or

00:31:02,429 --> 00:31:04,380
like Google especially to track a lot of

00:31:03,870 --> 00:31:07,350
this

00:31:04,380 --> 00:31:11,310
and funny things happen I remember very

00:31:07,350 --> 00:31:13,230
similar to the situation I had ran into

00:31:11,310 --> 00:31:14,520
when I was in college taking physics

00:31:13,230 --> 00:31:16,560
labs

00:31:14,520 --> 00:31:19,290
I remember working with a small team and

00:31:16,560 --> 00:31:22,650
we're doing experiments so I would start

00:31:19,290 --> 00:31:24,840
tracking experiments in success and then

00:31:22,650 --> 00:31:28,290
of course I need to email my colleague

00:31:24,840 --> 00:31:34,680
those experiments so I send them one dot

00:31:28,290 --> 00:31:35,280
the v1 dot XLS v2 v3 final final for

00:31:34,680 --> 00:31:39,890
real this time

00:31:35,280 --> 00:31:44,280
final final final and become a mess

00:31:39,890 --> 00:31:45,360
and the other one is hey now let's say

00:31:44,280 --> 00:31:48,120
you got on the model you're actually

00:31:45,360 --> 00:31:50,580
pretty happy with as a data scientist

00:31:48,120 --> 00:31:52,860
and now you want to deploy in production

00:31:50,580 --> 00:31:54,540
your data scientists you don't really

00:31:52,860 --> 00:31:55,860
know all the production engineering

00:31:54,540 --> 00:31:57,000
stuff you don't know what what SL is

00:31:55,860 --> 00:31:58,470
really mean you don't know how to

00:31:57,000 --> 00:32:00,750
achieve that you don't know what about

00:31:58,470 --> 00:32:02,220
kubernetes or containers you also data

00:32:00,750 --> 00:32:04,620
engineer production engineer for help

00:32:02,220 --> 00:32:07,140
and then this sort of a conversation

00:32:04,620 --> 00:32:08,640
that happens a lot among our customers

00:32:07,140 --> 00:32:10,890
hey here's like something I trained with

00:32:08,640 --> 00:32:12,450
scikit-learn please deploy it and here's

00:32:10,890 --> 00:32:13,470
like something I trained with Sparky's I

00:32:12,450 --> 00:32:15,930
trained with tensorflow

00:32:13,470 --> 00:32:17,760
here's I did it was art now the

00:32:15,930 --> 00:32:19,560
production idea is I have no idea what

00:32:17,760 --> 00:32:21,450
you're talking about I'm really used to

00:32:19,560 --> 00:32:23,040
writing Java or Scala code and I'm

00:32:21,450 --> 00:32:26,190
really good at deploying those to my

00:32:23,040 --> 00:32:28,110
application server all this our pythons

00:32:26,190 --> 00:32:32,370
I could learn smart stuff I don't know

00:32:28,110 --> 00:32:34,170
what they are and the most interesting

00:32:32,370 --> 00:32:36,660
one is hey I just read about something

00:32:34,170 --> 00:32:41,760
interesting on this archive can you also

00:32:36,660 --> 00:32:44,310
deploy this for me now let's take a look

00:32:41,760 --> 00:32:46,230
at what ml flow in a brewery to do all

00:32:44,310 --> 00:32:48,330
right very simple program this is the

00:32:46,230 --> 00:32:51,720
old program but now instead of just

00:32:48,330 --> 00:32:54,090
print you import the ml flow api's and

00:32:51,720 --> 00:32:56,520
then you actually lock them all right

00:32:54,090 --> 00:32:58,520
only does is it calls ml flow API ml

00:32:56,520 --> 00:33:02,370
flow at restores them in the database

00:32:58,520 --> 00:33:03,720
and when you launch an ml flow UI you

00:33:02,370 --> 00:33:07,110
actually give you from a tracking

00:33:03,720 --> 00:33:09,570
component all your experiments so here's

00:33:07,110 --> 00:33:11,460
just a very simple screen visualization

00:33:09,570 --> 00:33:13,860
let's say you track all the experiments

00:33:11,460 --> 00:33:16,020
and now a chair should go in and look at

00:33:13,860 --> 00:33:18,300
hey what is the output for each of the

00:33:16,020 --> 00:33:20,760
experiment what do I get you can

00:33:18,300 --> 00:33:23,280
add nose to it he also tracks your code

00:33:20,760 --> 00:33:26,010
with a git commit and he also integrates

00:33:23,280 --> 00:33:28,140
Delta to give it so one other

00:33:26,010 --> 00:33:30,180
functionality Delta gives you that I

00:33:28,140 --> 00:33:31,950
didn't talk about is time travel because

00:33:30,180 --> 00:33:33,300
we actually saved the transaction log so

00:33:31,950 --> 00:33:35,490
you could go back to refer to any

00:33:33,300 --> 00:33:38,910
specific version of the data in the past

00:33:35,490 --> 00:33:40,350
and by integrating that with ml flow we

00:33:38,910 --> 00:33:42,690
actually allow you to basically

00:33:40,350 --> 00:33:46,770
reproduce your model end-to-end by

00:33:42,690 --> 00:33:48,330
combining Delta data your code itself

00:33:46,770 --> 00:33:50,490
with a git commit hash and all the

00:33:48,330 --> 00:33:53,280
parameters are tracked so now it can

00:33:50,490 --> 00:33:55,730
reproduce everything you can also come

00:33:53,280 --> 00:33:58,080
compare the different runs of the model

00:33:55,730 --> 00:34:02,160
because that's what you usually want to

00:33:58,080 --> 00:34:06,540
visualize in a big experiment the other

00:34:02,160 --> 00:34:09,030
one is the project component of ml flow

00:34:06,540 --> 00:34:11,310
allows you to have offers a standard

00:34:09,030 --> 00:34:13,500
spec the basically allows you to define

00:34:11,310 --> 00:34:15,360
a project with the codependency and the

00:34:13,500 --> 00:34:17,460
configuration and then you can actually

00:34:15,360 --> 00:34:19,560
run the project in a lot of different

00:34:17,460 --> 00:34:21,420
places for example you could run it

00:34:19,560 --> 00:34:22,679
locally was just the mlpro open source

00:34:21,420 --> 00:34:26,100
project you're running on the spark

00:34:22,679 --> 00:34:28,280
cluster and all that needs to be done

00:34:26,100 --> 00:34:30,660
it's just a single line of command and

00:34:28,280 --> 00:34:33,690
here's how a project looks like you

00:34:30,660 --> 00:34:35,100
basically have a Yama file and because a

00:34:33,690 --> 00:34:36,750
lot of data sign is a lot of the

00:34:35,100 --> 00:34:38,550
machine-learning dependencies in python

00:34:36,750 --> 00:34:40,679
we basically allow you to define the

00:34:38,550 --> 00:34:42,540
environment using Conda and you can

00:34:40,679 --> 00:34:45,150
write your Python programs there and

00:34:42,540 --> 00:34:46,679
this becomes a standard container think

00:34:45,150 --> 00:34:48,230
of it almost like a maven but

00:34:46,679 --> 00:34:51,270
specifically built for machine learning

00:34:48,230 --> 00:34:55,830
and you guys should run it pretty much

00:34:51,270 --> 00:34:58,890
everywhere and the last piece of the ml

00:34:55,830 --> 00:35:03,120
flow is the spec for models basically

00:34:58,890 --> 00:35:04,740
for running them and what it does is

00:35:03,120 --> 00:35:07,080
really just a simple wrapper it defines

00:35:04,740 --> 00:35:09,120
a standard API for here are different

00:35:07,080 --> 00:35:10,980
types of models and here's how I can

00:35:09,120 --> 00:35:12,840
actually execute them for example if

00:35:10,980 --> 00:35:14,880
there's a tensor for specific model I

00:35:12,840 --> 00:35:16,590
could just run it was tensor fault

00:35:14,880 --> 00:35:19,290
itself if it's a generic model I could

00:35:16,590 --> 00:35:21,600
just run an arbitrary Python function

00:35:19,290 --> 00:35:23,070
and you can actually deploy for example

00:35:21,600 --> 00:35:25,080
the arbitrary five Python function

00:35:23,070 --> 00:35:27,060
directly in spark itself so it cannot

00:35:25,080 --> 00:35:29,760
your paralyzed your influence inference

00:35:27,060 --> 00:35:31,380
across the cluster of machines even if

00:35:29,760 --> 00:35:31,800
you train your model just using tencel

00:35:31,380 --> 00:35:38,460
always

00:35:31,800 --> 00:35:41,400
on the single note so now with the

00:35:38,460 --> 00:35:42,870
standards back the production engineer

00:35:41,400 --> 00:35:49,620
all they really need to understand is

00:35:42,870 --> 00:35:52,110
hey here's here's a commander in Tehran

00:35:49,620 --> 00:35:55,260
because I'm given an m/l for a project

00:35:52,110 --> 00:36:03,510
and I can just run it across a lot of

00:35:55,260 --> 00:36:05,520
different environments I have so the

00:36:03,510 --> 00:36:07,920
three components tracking projects

00:36:05,520 --> 00:36:10,140
models they're really done to make the

00:36:07,920 --> 00:36:11,880
life of data scientists and also all the

00:36:10,140 --> 00:36:14,730
supporting staff including production

00:36:11,880 --> 00:36:16,950
engineers data engineers easy because we

00:36:14,730 --> 00:36:18,990
sort of try to understand what our data

00:36:16,950 --> 00:36:21,630
science is and all the supporting SAP

00:36:18,990 --> 00:36:25,860
really struggling with everyday and how

00:36:21,630 --> 00:36:27,840
we can make their lives better and m/l

00:36:25,860 --> 00:36:30,600
flow is about a year old we actually

00:36:27,840 --> 00:36:32,520
open source it a year ago was announced

00:36:30,600 --> 00:36:34,770
to spark summit in San Francisco and

00:36:32,520 --> 00:36:38,010
it's already gotten 100 plus

00:36:34,770 --> 00:36:39,690
contributors and I just roped up pi PI

00:36:38,010 --> 00:36:41,490
the Python package in that class at

00:36:39,690 --> 00:36:43,050
night and there's actually a club this

00:36:41,490 --> 00:36:45,810
more than half a million downloads every

00:36:43,050 --> 00:36:47,580
month now a lot of those downloads are

00:36:45,810 --> 00:36:49,920
probably CIC the pipeline to just keep

00:36:47,580 --> 00:36:51,750
catching them but nonetheless is pretty

00:36:49,920 --> 00:36:53,970
impressive for only a one year old

00:36:51,750 --> 00:36:56,370
project it's actually solving some real

00:36:53,970 --> 00:36:57,960
problems for data science is that I

00:36:56,370 --> 00:36:59,820
would say historically I overlooked

00:36:57,960 --> 00:37:02,970
because everybody focused so much on

00:36:59,820 --> 00:37:05,070
just how do I do the machine and a piece

00:37:02,970 --> 00:37:10,310
of it without looking at the surrounding

00:37:05,070 --> 00:37:13,800
infrastructure just to wrap up the talk

00:37:10,310 --> 00:37:16,560
spark was created as a unified analytics

00:37:13,800 --> 00:37:18,750
engine really try to unify Big Data and

00:37:16,560 --> 00:37:22,080
AI but spark taken the approach of how

00:37:18,750 --> 00:37:24,450
do we build the compute art of this

00:37:22,080 --> 00:37:25,890
piece and as we work at more and more

00:37:24,450 --> 00:37:28,040
with different customers and usually

00:37:25,890 --> 00:37:30,300
realize there's a lot of other pieces

00:37:28,040 --> 00:37:32,640
even the ones we thought that we saw

00:37:30,300 --> 00:37:34,650
Proms is not really solve and people are

00:37:32,640 --> 00:37:36,750
struggling with and a lot of a

00:37:34,650 --> 00:37:38,670
responsibility of this industry and also

00:37:36,750 --> 00:37:40,830
data bricks in particular is we need to

00:37:38,670 --> 00:37:43,560
be building understanding the prompts

00:37:40,830 --> 00:37:44,940
users and customers run into providing

00:37:43,560 --> 00:37:46,230
high-level solutions

00:37:44,940 --> 00:37:48,569
they don't have to spend as much time

00:37:46,230 --> 00:37:50,400
fiddling with infrastructure and they

00:37:48,569 --> 00:37:51,180
can focus on their still main specific

00:37:50,400 --> 00:37:53,280
problems

00:37:51,180 --> 00:37:55,430
this requires truce to actually unify a

00:37:53,280 --> 00:37:58,440
lot of different disparate personas

00:37:55,430 --> 00:38:03,480
required using tools to actually unify

00:37:58,440 --> 00:38:06,210
different language techs and go from

00:38:03,480 --> 00:38:08,339
there and the two specific project are

00:38:06,210 --> 00:38:10,770
talk to you about ones Delta like a lot

00:38:08,339 --> 00:38:13,829
of it is about making data ready to

00:38:10,770 --> 00:38:15,650
support the downstream analytics and the

00:38:13,829 --> 00:38:17,849
second ones ml flow it's about making

00:38:15,650 --> 00:38:21,690
managing the lifecycle all the machine

00:38:17,849 --> 00:38:22,650
learning projects all right that's all I

00:38:21,690 --> 00:38:24,839
have

00:38:22,650 --> 00:38:26,490
I can't be here without helping you hey

00:38:24,839 --> 00:38:28,589
we also hiring we have offices in

00:38:26,490 --> 00:38:30,359
Amsterdam San Francisco also a small one

00:38:28,589 --> 00:38:32,849
hangzhou also open to remote

00:38:30,359 --> 00:38:36,119
opportunities and I think I can take a

00:38:32,849 --> 00:38:39,890
little bit of question with the rest of

00:38:36,119 --> 00:38:39,890
the time thank you

00:38:43,530 --> 00:38:54,900
I can't if there's no question that

00:38:53,970 --> 00:39:00,110
means I've explained everything

00:38:54,900 --> 00:39:00,110
perfectly so I I think this one there

00:39:02,240 --> 00:39:10,440
yeah hello hi I had a question about

00:39:06,870 --> 00:39:13,860
Delta Lake yeah so you mentioned that

00:39:10,440 --> 00:39:16,290
there's this sort of I guess compaction

00:39:13,860 --> 00:39:19,170
staff with regards to metadata files is

00:39:16,290 --> 00:39:21,450
that right yeah is there any sort of

00:39:19,170 --> 00:39:23,340
built-in compaction for the data files

00:39:21,450 --> 00:39:26,010
is this something that you need to run

00:39:23,340 --> 00:39:27,600
an out-of-band process or does it just

00:39:26,010 --> 00:39:30,750
come gonna come for free when you use

00:39:27,600 --> 00:39:34,680
Delta yeah so it's a very good question

00:39:30,750 --> 00:39:36,300
we so it's not something that's done

00:39:34,680 --> 00:39:39,660
automatically for you right now although

00:39:36,300 --> 00:39:41,460
we experimenting with it it's very very

00:39:39,660 --> 00:39:43,530
easy to do compaction you could just for

00:39:41,460 --> 00:39:44,910
example schedule a regular job every 30

00:39:43,530 --> 00:39:47,190
minutes and what did the job is

00:39:44,910 --> 00:39:48,890
literally one line is just spark read

00:39:47,190 --> 00:39:52,470
the file in and write the file out and

00:39:48,890 --> 00:39:54,570
do it in a transaction we are

00:39:52,470 --> 00:39:57,150
experimenting with automatically doing

00:39:54,570 --> 00:39:59,100
that directly in a hosted service but

00:39:57,150 --> 00:40:01,230
part of the challenge with compaction is

00:39:59,100 --> 00:40:03,860
depending on data volume saw them

00:40:01,230 --> 00:40:06,600
compaction jobs could be very expensive

00:40:03,860 --> 00:40:08,520
so some of our customers they explicitly

00:40:06,600 --> 00:40:10,200
don't want us to automatically compact

00:40:08,520 --> 00:40:12,180
for them because they're worried about

00:40:10,200 --> 00:40:14,880
the cost it would happen if they don't

00:40:12,180 --> 00:40:17,400
control themselves so there's something

00:40:14,880 --> 00:40:19,350
we're actively looking at but I suspect

00:40:17,400 --> 00:40:22,350
it would be more of a hey you can opt in

00:40:19,350 --> 00:40:24,750
to do automatic compaction but there's a

00:40:22,350 --> 00:40:26,040
way to or maybe you can by default it's

00:40:24,750 --> 00:40:30,470
on but then you can opt out in the

00:40:26,040 --> 00:40:30,470
future just to cover different use cases

00:40:35,420 --> 00:40:44,479
okay if your show I can repeat your

00:40:41,549 --> 00:40:44,479
question if I hear it

00:40:49,440 --> 00:40:55,229
the question is can you compare Delhi

00:40:51,029 --> 00:40:56,459
with iceberg the yeah I think Dell lakes

00:40:55,229 --> 00:40:58,559
just more production-ready

00:40:56,459 --> 00:41:01,049
much more the to try to probably solve

00:40:58,559 --> 00:41:02,339
similar problems in a way Delta Lake has

00:41:01,049 --> 00:41:06,089
been running in production for a long

00:41:02,339 --> 00:41:07,589
time and also more uniquely one of the

00:41:06,089 --> 00:41:11,009
big thing we have focus on is actually

00:41:07,589 --> 00:41:12,959
support streaming and surf incremental

00:41:11,009 --> 00:41:15,660
computation which I don't think iceberg

00:41:12,959 --> 00:41:18,180
does because that's coming from a lot of

00:41:15,660 --> 00:41:22,199
our real requirements I want to see data

00:41:18,180 --> 00:41:24,329
and more real time and that's one big

00:41:22,199 --> 00:41:27,119
difference but the underlying I would

00:41:24,329 --> 00:41:30,380
say at 20,000 feet the underlying

00:41:27,119 --> 00:41:30,380
technology look very similar

00:41:37,430 --> 00:41:44,260
all right I thank you very much all

00:41:42,950 --> 00:41:48,499
right thank you

00:41:44,260 --> 00:41:48,499

YouTube URL: https://www.youtube.com/watch?v=QGj_mxD7Q2I


