Title: Differentiable Functional Programming by Noel Welsh
Publication date: 2018-09-22
Playlist: Scala Days New York 2018
Description: 
	This video was recorded at Scala Days New York 2018
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

More information and the abstract can be found here:
https://na.scaladays.org/schedule/differentiable-functional-programming
Captions: 
	00:00:04,840 --> 00:00:07,880
okay thank you all for coming along to

00:00:07,279 --> 00:00:10,549
my talk

00:00:07,880 --> 00:00:12,590
my name is null Welsh I am a consultant

00:00:10,549 --> 00:00:15,170
at underscore which they global scale

00:00:12,590 --> 00:00:17,900
consultancy today I want to talk to you

00:00:15,170 --> 00:00:20,930
about differentiable programming idea

00:00:17,900 --> 00:00:23,840
deep learning and functional programming

00:00:20,930 --> 00:00:28,010
and how they related so the goals for my

00:00:23,840 --> 00:00:29,570
talk I first need to show you that deep

00:00:28,010 --> 00:00:30,830
learning isn't really so hard you might

00:00:29,570 --> 00:00:32,960
have heard of deep learning it might

00:00:30,830 --> 00:00:34,190
seem like a dark black art to you you

00:00:32,960 --> 00:00:36,500
don't understand how it works I want to

00:00:34,190 --> 00:00:39,500
show you the basic ideas I'm not all

00:00:36,500 --> 00:00:41,120
that complicated I want to show you that

00:00:39,500 --> 00:00:42,830
in fact the basic ideas are very similar

00:00:41,120 --> 00:00:46,909
to what we do in functional programming

00:00:42,830 --> 00:00:48,229
it's all about composition and I want to

00:00:46,909 --> 00:00:50,650
look at some of the algorithms that call

00:00:48,229 --> 00:00:52,519
a groans before the you will leave

00:00:50,650 --> 00:00:53,659
thinking that you could implement it

00:00:52,519 --> 00:00:55,129
yourself I must say you could implement

00:00:53,659 --> 00:00:56,239
a production system yourself that's a

00:00:55,129 --> 00:00:58,640
lot of work but you could certainly

00:00:56,239 --> 00:01:00,470
implement a little tour yourself given a

00:00:58,640 --> 00:01:04,070
day or two and do some useful things

00:01:00,470 --> 00:01:05,269
with it maybe have some fun before we

00:01:04,070 --> 00:01:08,270
get into that let's just talk a little

00:01:05,269 --> 00:01:09,560
bit about our deep learning is so what a

00:01:08,270 --> 00:01:12,020
deep learning useful for well it's

00:01:09,560 --> 00:01:13,430
useful first stuff like this it's always

00:01:12,020 --> 00:01:16,549
great fun it's filled my photos I took

00:01:13,430 --> 00:01:20,600
you in Rome we have Emperor Marcus

00:01:16,549 --> 00:01:24,469
Aurelius as a dog the stylized version

00:01:20,600 --> 00:01:27,409
of statue the dying Gaul and a tree

00:01:24,469 --> 00:01:29,030
we've got a little bit theory so this is

00:01:27,409 --> 00:01:30,409
one of the kind of fun applications of

00:01:29,030 --> 00:01:33,070
deep learning mucking around these

00:01:30,409 --> 00:01:35,420
pictures but there are many more

00:01:33,070 --> 00:01:37,460
commercial applications such as voice

00:01:35,420 --> 00:01:39,649
recognition the thing is in your phone

00:01:37,460 --> 00:01:41,719
that recognizes your voice and image

00:01:39,649 --> 00:01:44,869
recognition that the tags people when

00:01:41,719 --> 00:01:46,999
you're in photos or pulls out street

00:01:44,869 --> 00:01:50,929
signs and so on for Google Maps that's

00:01:46,999 --> 00:01:52,999
all done by deep learning these days so

00:01:50,929 --> 00:01:54,920
that's what deep learning is useful for

00:01:52,999 --> 00:01:56,799
but then what is deep learning what are

00:01:54,920 --> 00:01:59,569
the cool ideas here of deep learning and

00:01:56,799 --> 00:02:01,520
what deep learning is all about is

00:01:59,569 --> 00:02:05,030
supervised learning or parameterize

00:02:01,520 --> 00:02:06,219
functions by gradient descent all right

00:02:05,030 --> 00:02:08,390
simple right

00:02:06,219 --> 00:02:10,340
yeah okay so that's actually like a

00:02:08,390 --> 00:02:11,360
whole bunch of buzzwords although we did

00:02:10,340 --> 00:02:13,810
want to assume instead of simple he said

00:02:11,360 --> 00:02:13,810
simple

00:02:14,220 --> 00:02:21,849
okay for everyone else let's unpack some

00:02:20,110 --> 00:02:23,400
of these our buzzwords and see what they

00:02:21,849 --> 00:02:25,930
all mean

00:02:23,400 --> 00:02:28,239
so there were three I'm gonna look at

00:02:25,930 --> 00:02:29,800
them firstly parameterize functions then

00:02:28,239 --> 00:02:32,130
supervised learning and then gradient

00:02:29,800 --> 00:02:35,530
descent what do I mean by this so

00:02:32,130 --> 00:02:39,130
paralyzed functions we'll start with a

00:02:35,530 --> 00:02:41,500
function you give this function the

00:02:39,130 --> 00:02:43,480
value X you get back the sine of X

00:02:41,500 --> 00:02:45,340
hopefully a very familiar function and

00:02:43,480 --> 00:02:48,100
we can plot a picture of it

00:02:45,340 --> 00:02:49,690
x vs. sine of X and we get something

00:02:48,100 --> 00:02:53,860
that looks like that the familiar old

00:02:49,690 --> 00:02:55,709
sinusoid okay now the many ways we could

00:02:53,860 --> 00:02:58,060
change this we could make it

00:02:55,709 --> 00:02:59,890
sinusoid bigger up and down the

00:02:58,060 --> 00:03:03,010
amplitude where you could change the

00:02:59,890 --> 00:03:05,709
period you can make it tighter we can

00:03:03,010 --> 00:03:08,230
make it looser and there's ways you can

00:03:05,709 --> 00:03:10,630
change the shape that we get and what we

00:03:08,230 --> 00:03:13,480
might call those are parameters that we

00:03:10,630 --> 00:03:15,730
can manipulate so for example here we

00:03:13,480 --> 00:03:17,049
have a parameter which we call theta and

00:03:15,730 --> 00:03:19,540
we choose a particular value of that

00:03:17,049 --> 00:03:20,590
parameter and we get back a function so

00:03:19,540 --> 00:03:22,180
it's what I mean by parameterize

00:03:20,590 --> 00:03:24,370
functions it basically functions

00:03:22,180 --> 00:03:26,430
returning functions something that's

00:03:24,370 --> 00:03:28,840
quite familiar in functional programming

00:03:26,430 --> 00:03:31,959
and in this case what manipulating is

00:03:28,840 --> 00:03:33,579
the period and this it gives us this

00:03:31,959 --> 00:03:37,359
parameter I change this parameter gives

00:03:33,579 --> 00:03:39,790
us a family of functions so we change

00:03:37,359 --> 00:03:41,859
the period so we the period is smaller

00:03:39,790 --> 00:03:43,209
and we get a tighter function we change

00:03:41,859 --> 00:03:45,640
the period so the code is bigger and we

00:03:43,209 --> 00:03:48,609
get this looser function here so our

00:03:45,640 --> 00:03:51,880
choice of parameter I period in this

00:03:48,609 --> 00:03:56,049
case changes the function we get so what

00:03:51,880 --> 00:03:59,769
we're trying to do here alright so on to

00:03:56,049 --> 00:04:01,000
supervised learning not all deep

00:03:59,769 --> 00:04:03,700
learning supervised learning but it's

00:04:01,000 --> 00:04:06,040
the most common paradigm and the idea

00:04:03,700 --> 00:04:08,650
here is about choosing parameters and

00:04:06,040 --> 00:04:11,739
some parameterize function to fit some

00:04:08,650 --> 00:04:13,930
data so I don't mean by that so let's

00:04:11,739 --> 00:04:17,380
say we have some data here points that

00:04:13,930 --> 00:04:19,870
looks suspiciously like a sinusoid maybe

00:04:17,380 --> 00:04:21,519
even generated by one and then what we

00:04:19,870 --> 00:04:24,070
might want to do is say how can we

00:04:21,519 --> 00:04:27,180
choose that parameter that period to

00:04:24,070 --> 00:04:29,610
best match or fit this data

00:04:27,180 --> 00:04:32,430
so here's like one function we could

00:04:29,610 --> 00:04:35,009
choose very small period doesn't look

00:04:32,430 --> 00:04:37,880
like this - well increase the period

00:04:35,009 --> 00:04:42,000
it's better fit that looks pretty good

00:04:37,880 --> 00:04:45,199
that's probably too big and that that's

00:04:42,000 --> 00:04:47,370
definitely too big so we can see that

00:04:45,199 --> 00:04:49,770
changing this parameter gives us a

00:04:47,370 --> 00:04:51,479
different function and different a

00:04:49,770 --> 00:04:54,509
different choice this is going to fit

00:04:51,479 --> 00:04:59,009
the data sort of match up the data a

00:04:54,509 --> 00:05:01,010
different amount so when we come to do

00:04:59,009 --> 00:05:05,370
this on a computer of course we need to

00:05:01,010 --> 00:05:06,780
have some formal way of defining these

00:05:05,370 --> 00:05:08,370
things what does it mean to change the

00:05:06,780 --> 00:05:11,370
parameter how do we actually go about

00:05:08,370 --> 00:05:13,500
changing a parameter wasn't mean to fit

00:05:11,370 --> 00:05:16,020
the data and so we look at these next

00:05:13,500 --> 00:05:19,919
and this is where gradient descent comes

00:05:16,020 --> 00:05:21,660
in so the core idea is gradient descent

00:05:19,919 --> 00:05:24,210
is to algorithmically choose the

00:05:21,660 --> 00:05:26,820
parameters and what we need to do with

00:05:24,210 --> 00:05:29,910
this is firstly define what it means to

00:05:26,820 --> 00:05:32,610
to fit the data how good is a particular

00:05:29,910 --> 00:05:34,500
choice of parameter and the way you do

00:05:32,610 --> 00:05:40,050
this is choose a particular parameter

00:05:34,500 --> 00:05:43,669
here and then draw a line from the data

00:05:40,050 --> 00:05:46,139
to the function and that gives us an

00:05:43,669 --> 00:05:48,000
measurement of error and you can add up

00:05:46,139 --> 00:05:49,590
all these little errors all these these

00:05:48,000 --> 00:05:50,880
distances you normally use square

00:05:49,590 --> 00:05:52,770
distance because just a little bit more

00:05:50,880 --> 00:05:54,150
convenient to work with and you get some

00:05:52,770 --> 00:05:56,250
kind of number that tells you that

00:05:54,150 --> 00:05:59,310
number is small and it's a good fit if a

00:05:56,250 --> 00:06:00,720
number is big it's a bad fit and we have

00:05:59,310 --> 00:06:02,639
lots of different values of this

00:06:00,720 --> 00:06:05,729
parameter of this period we could plot

00:06:02,639 --> 00:06:07,680
out a curve that looks like this and you

00:06:05,729 --> 00:06:09,800
can see there's a nice big dip there

00:06:07,680 --> 00:06:11,820
which would you believe it is it is the

00:06:09,800 --> 00:06:13,680
sinusoid that generated the data a

00:06:11,820 --> 00:06:15,240
little bit around of noise added and you

00:06:13,680 --> 00:06:17,220
can see the harmonics around it a

00:06:15,240 --> 00:06:19,260
reasonably good fit and then as you get

00:06:17,220 --> 00:06:24,780
further away from that choice you get

00:06:19,260 --> 00:06:27,570
worse and worse and so we might think

00:06:24,780 --> 00:06:30,510
about this if we chose some choice of

00:06:27,570 --> 00:06:32,280
period some of that parameter R and V

00:06:30,510 --> 00:06:36,120
later and the way we can improve it is

00:06:32,280 --> 00:06:37,140
we could just roll down hill down to

00:06:36,120 --> 00:06:39,810
some

00:06:37,140 --> 00:06:42,090
minimum we don't have to get the best

00:06:39,810 --> 00:06:43,260
choice but we can certainly imagine your

00:06:42,090 --> 00:06:44,370
money get for the best choice but we

00:06:43,260 --> 00:06:49,380
could certainly prove it just by going

00:06:44,370 --> 00:06:51,480
down here and formally what we're doing

00:06:49,380 --> 00:06:53,160
here is we need to calculate the

00:06:51,480 --> 00:06:54,720
derivative of what is called the loss

00:06:53,160 --> 00:06:57,990
function with respect to these

00:06:54,720 --> 00:06:59,400
parameters so as you may remember from

00:06:57,990 --> 00:07:01,380
calculus if you did calculus the

00:06:59,400 --> 00:07:03,570
derivative is just the slope so that's

00:07:01,380 --> 00:07:04,770
telling us which way downhill is and the

00:07:03,570 --> 00:07:07,230
loss function is just a fancy-schmancy

00:07:04,770 --> 00:07:10,140
term for this distance that we're trying

00:07:07,230 --> 00:07:12,870
to decrease this error that we're trying

00:07:10,140 --> 00:07:14,970
to decrease and we put this in symbolic

00:07:12,870 --> 00:07:17,520
notation we get something like this the

00:07:14,970 --> 00:07:19,050
other data takeaway the sign of the data

00:07:17,520 --> 00:07:20,970
squared or because it's just more

00:07:19,050 --> 00:07:24,390
community work with and then we can do a

00:07:20,970 --> 00:07:25,830
little bit of calculus and you get rid

00:07:24,390 --> 00:07:27,660
of the loss function and then away you

00:07:25,830 --> 00:07:30,420
go

00:07:27,660 --> 00:07:33,720
it details don't really matter that's

00:07:30,420 --> 00:07:36,320
enough to tell us calculate the gradient

00:07:33,720 --> 00:07:39,510
which tells us which way up and down is

00:07:36,320 --> 00:07:40,500
for the contours of parameters and then

00:07:39,510 --> 00:07:42,660
we need to make a change to the

00:07:40,500 --> 00:07:44,340
parameter to improve it so what we're

00:07:42,660 --> 00:07:47,790
just going to do is take a little small

00:07:44,340 --> 00:07:48,810
step in a downhill direction so in we

00:07:47,790 --> 00:07:50,460
calculate the great interest which

00:07:48,810 --> 00:07:53,070
weighs down and then we're just going to

00:07:50,460 --> 00:07:59,490
change the parameter a little bit in

00:07:53,070 --> 00:08:01,170
that direction okay so since this is all

00:07:59,490 --> 00:08:03,030
a very contrived example let's say just

00:08:01,170 --> 00:08:04,680
by chance you happen to choose that

00:08:03,030 --> 00:08:06,930
white dot to start with as your starting

00:08:04,680 --> 00:08:13,620
parameter calculate the gradient take a

00:08:06,930 --> 00:08:15,450
small step you might see the system

00:08:13,620 --> 00:08:18,660
going like I've Illustrated there with a

00:08:15,450 --> 00:08:21,120
pink red dots taking little steps down

00:08:18,660 --> 00:08:24,000
the downward direction until luckily we

00:08:21,120 --> 00:08:25,560
end up right at the the global minimum

00:08:24,000 --> 00:08:27,210
we don't usually end up at the global

00:08:25,560 --> 00:08:30,660
minimum in deep learning and it doesn't

00:08:27,210 --> 00:08:32,330
usually matter really in fact it can be

00:08:30,660 --> 00:08:34,200
done but advantageous not to do that

00:08:32,330 --> 00:08:39,120
that's all stuff we don't need to talk

00:08:34,200 --> 00:08:41,220
about okay so we had that version to

00:08:39,120 --> 00:08:44,430
explain some terms let's get back to

00:08:41,220 --> 00:08:46,170
deep learning I said earlier deep

00:08:44,430 --> 00:08:47,970
learning is supervised learning of

00:08:46,170 --> 00:08:49,470
parameterize functions by gradient

00:08:47,970 --> 00:08:50,670
descent we now looked up all those all

00:08:49,470 --> 00:08:52,650
those ideas mean

00:08:50,670 --> 00:08:54,870
a paralyzed functions it's a function

00:08:52,650 --> 00:08:57,090
returning a function supervised learning

00:08:54,870 --> 00:08:58,860
means getting some data and trying to

00:08:57,090 --> 00:09:01,560
fit that data trying to minimize the

00:08:58,860 --> 00:09:03,180
error and grading descent is a process

00:09:01,560 --> 00:09:06,750
we can use to minimize that area given a

00:09:03,180 --> 00:09:08,790
particular choice of parameter excellent

00:09:06,750 --> 00:09:10,350
so there are a few more choices few more

00:09:08,790 --> 00:09:14,040
decisions we need to make in deep

00:09:10,350 --> 00:09:16,860
learning firstly when you say ok which

00:09:14,040 --> 00:09:18,180
functions are we going to use and the

00:09:16,860 --> 00:09:20,370
choice you make in deep learning is we

00:09:18,180 --> 00:09:22,980
want something which is a function which

00:09:20,370 --> 00:09:24,840
is class of functions that parameterize

00:09:22,980 --> 00:09:27,600
course which are going to be able to

00:09:24,840 --> 00:09:30,720
express reasonably complicated things or

00:09:27,600 --> 00:09:32,460
expressive kasih functions but we also

00:09:30,720 --> 00:09:35,160
want something that's going to run in a

00:09:32,460 --> 00:09:37,680
reasonable amount of time so the choice

00:09:35,160 --> 00:09:40,230
made in deep learning something has well

00:09:37,680 --> 00:09:42,570
suited to GPU acceleration the GPUs are

00:09:40,230 --> 00:09:44,820
always magnitude faster than CPUs as

00:09:42,570 --> 00:09:47,700
long as you fit their programming model

00:09:44,820 --> 00:09:49,950
and in particular what you be doing deep

00:09:47,700 --> 00:09:52,110
learning is we normally use tensor

00:09:49,950 --> 00:09:56,040
multiplication and a non linearity so

00:09:52,110 --> 00:09:57,770
more buzzwords unpack these things see

00:09:56,040 --> 00:10:00,990
what do they mean what a tensors mean

00:09:57,770 --> 00:10:02,400
well tensors are just like a pretentious

00:10:00,990 --> 00:10:03,840
word for multi-dimensional and arrays

00:10:02,400 --> 00:10:05,930
but that's okay because we function

00:10:03,840 --> 00:10:08,910
programmers we love pretentious words

00:10:05,930 --> 00:10:13,650
maaan are duplicative or great stuff and

00:10:08,910 --> 00:10:15,180
here's a new one to use so if you have

00:10:13,650 --> 00:10:17,100
vectors and matrices you can think of

00:10:15,180 --> 00:10:19,680
them as one and two dimensional tensors

00:10:17,100 --> 00:10:23,180
respectively and you can go to three for

00:10:19,680 --> 00:10:25,440
whatever dimensions all just cancer's an

00:10:23,180 --> 00:10:27,930
example of multiplication tensor

00:10:25,440 --> 00:10:29,610
multiplication is well I've chosen

00:10:27,930 --> 00:10:31,470
vector matrix multiplication because

00:10:29,610 --> 00:10:33,240
it's much easier to draw on the screen

00:10:31,470 --> 00:10:34,410
it's in two dimensions or other three

00:10:33,240 --> 00:10:36,990
dimensions which is kind of painful to

00:10:34,410 --> 00:10:38,880
draw you have some kind of input data

00:10:36,990 --> 00:10:41,010
maybe it's a vector three-dimensional

00:10:38,880 --> 00:10:42,750
vector you have some matrix three by

00:10:41,010 --> 00:10:44,100
five dimensions you multiply them

00:10:42,750 --> 00:10:47,970
together and you get an output which is

00:10:44,100 --> 00:10:50,880
a five dimensional vector and the

00:10:47,970 --> 00:10:54,000
parameters here so a matrix is basically

00:10:50,880 --> 00:10:55,320
a function because you put x vector you

00:10:54,000 --> 00:10:58,920
get out of vector you put something in

00:10:55,320 --> 00:11:01,140
you get something out and the numbers

00:10:58,920 --> 00:11:03,360
inside that matrix or the weights as

00:11:01,140 --> 00:11:03,940
they're called in deep learning jargon

00:11:03,360 --> 00:11:05,860
you

00:11:03,940 --> 00:11:08,560
are the parameters that we're going to

00:11:05,860 --> 00:11:10,990
change that changes how that function

00:11:08,560 --> 00:11:13,080
operates okay

00:11:10,990 --> 00:11:16,960
so the other thing was talked about was

00:11:13,080 --> 00:11:19,060
non-linearity and you need this for

00:11:16,960 --> 00:11:21,730
technical reasons if you're a sort of

00:11:19,060 --> 00:11:23,650
station type of person or and applied

00:11:21,730 --> 00:11:24,790
math via has a nonlinear actually the

00:11:23,650 --> 00:11:26,230
whole thing just turns into linear

00:11:24,790 --> 00:11:28,990
regression which is a nice fine

00:11:26,230 --> 00:11:30,610
algorithm but not as expressive as deep

00:11:28,990 --> 00:11:32,560
learning so you need this non-linearity

00:11:30,610 --> 00:11:34,780
a linear function is basically something

00:11:32,560 --> 00:11:37,960
looks like a line and anything doesn't

00:11:34,780 --> 00:11:38,530
look like a straight line will do here's

00:11:37,960 --> 00:11:40,690
an example

00:11:38,530 --> 00:11:43,120
that's called the rectified linear unit

00:11:40,690 --> 00:11:46,150
very commonly used in deep learning it's

00:11:43,120 --> 00:11:48,000
just max of 0 and X so if X is less than

00:11:46,150 --> 00:11:51,160
0 you've got zero straight line

00:11:48,000 --> 00:11:53,530
otherwise you have X looks like that

00:11:51,160 --> 00:11:56,380
okay so it's got a kink in it very

00:11:53,530 --> 00:11:57,340
nonlinear okay putting them together you

00:11:56,380 --> 00:11:59,740
get the whole thing you get a function

00:11:57,340 --> 00:12:02,500
that looks like this you got some input

00:11:59,740 --> 00:12:04,300
X which might might be the vector e saw

00:12:02,500 --> 00:12:06,100
previously you got some weights in some

00:12:04,300 --> 00:12:08,530
kind of tensor you multiply them

00:12:06,100 --> 00:12:09,460
together that gives you some value and

00:12:08,530 --> 00:12:12,910
then you pass this through the

00:12:09,460 --> 00:12:15,280
non-linearity and that's what's called a

00:12:12,910 --> 00:12:19,030
layer in deep learning jargon it's just

00:12:15,280 --> 00:12:22,360
a function it's got some tense operation

00:12:19,030 --> 00:12:23,740
in usually and a non-linearity and then

00:12:22,360 --> 00:12:24,760
putting the deep bit in it's just

00:12:23,740 --> 00:12:27,070
composing a whole bunch of these

00:12:24,760 --> 00:12:28,690
together having chaining functions

00:12:27,070 --> 00:12:31,330
around so you get something like this

00:12:28,690 --> 00:12:33,340
function composition which is something

00:12:31,330 --> 00:12:35,530
that we do all the time to functional

00:12:33,340 --> 00:12:36,850
programming right maybe we should change

00:12:35,530 --> 00:12:38,290
functional programming or deep

00:12:36,850 --> 00:12:44,980
programming and with all be like getting

00:12:38,290 --> 00:12:46,060
awesome VC money ok so one of the

00:12:44,980 --> 00:12:48,520
interesting things we're just happening

00:12:46,060 --> 00:12:50,620
in deep learning is this kind of move

00:12:48,520 --> 00:12:52,330
away just from very simple systems what

00:12:50,620 --> 00:12:54,220
people now start to call differentiable

00:12:52,330 --> 00:12:56,710
programming and there are a few things

00:12:54,220 --> 00:12:59,470
here which are going on first is there's

00:12:56,710 --> 00:13:01,210
software frameworks tend to flow towards

00:12:59,470 --> 00:13:02,460
our you might have heard some of them if

00:13:01,210 --> 00:13:05,140
you haven't doesn't really matter and

00:13:02,460 --> 00:13:06,730
the core idea here is the software is

00:13:05,140 --> 00:13:08,410
automatically calculating the

00:13:06,730 --> 00:13:09,850
derivatives for us and compiling or to

00:13:08,410 --> 00:13:11,500
GPU carried so it's just making a lot

00:13:09,850 --> 00:13:14,380
more accessible a lot faster to write

00:13:11,500 --> 00:13:15,769
one of the these systems and to learn

00:13:14,380 --> 00:13:16,999
these parameters

00:13:15,769 --> 00:13:21,199
and that's a big reason for the success

00:13:16,999 --> 00:13:22,220
of deep learning one thing that deep

00:13:21,199 --> 00:13:24,110
learning models are becoming more

00:13:22,220 --> 00:13:27,139
complex it's becoming more like actual

00:13:24,110 --> 00:13:30,319
programming and models now have

00:13:27,139 --> 00:13:32,480
conditionals and loops in them so this

00:13:30,319 --> 00:13:34,339
is what causes this term differential

00:13:32,480 --> 00:13:37,910
programming to be coined in the last few

00:13:34,339 --> 00:13:40,839
months I think it is it's becoming more

00:13:37,910 --> 00:13:43,009
more like what we do as programmers

00:13:40,839 --> 00:13:45,319
you're constructing a program which you

00:13:43,009 --> 00:13:47,119
think will provide a solution to some

00:13:45,319 --> 00:13:49,249
problem but the nice thing here is that

00:13:47,119 --> 00:13:50,509
part of that program is these weights

00:13:49,249 --> 00:13:52,189
which you're actually setting based on

00:13:50,509 --> 00:13:53,540
the data you're seeing I think it's a

00:13:52,189 --> 00:13:55,429
really interesting thing something that

00:13:53,540 --> 00:13:56,600
actually quite accessible to programmers

00:13:55,429 --> 00:13:59,619
and will become more accessible as it

00:13:56,600 --> 00:14:05,119
becomes more and more like programming

00:13:59,619 --> 00:14:08,209
all right so that is the overview of the

00:14:05,119 --> 00:14:13,220
basic ideas of deep learning paralyzed

00:14:08,209 --> 00:14:17,269
functions gradient descent so on now I

00:14:13,220 --> 00:14:18,949
want to turn to the core algorithm that

00:14:17,269 --> 00:14:20,990
makes this all possible this what's

00:14:18,949 --> 00:14:21,860
called automatic differentiation so

00:14:20,990 --> 00:14:24,230
we've seen we need to calculate

00:14:21,860 --> 00:14:25,339
gradients once we have a gradient to be

00:14:24,230 --> 00:14:28,040
the derivative we can take a little step

00:14:25,339 --> 00:14:29,809
in that direction that's quite simple

00:14:28,040 --> 00:14:32,240
parameterize functions we've seen what a

00:14:29,809 --> 00:14:34,249
good choice there is so cackling the

00:14:32,240 --> 00:14:36,559
gradient is the main thing we want the

00:14:34,249 --> 00:14:40,670
computer to do right so we're talking

00:14:36,559 --> 00:14:42,740
akkadian gradients so firstly I want to

00:14:40,670 --> 00:14:44,929
talk about hard rivers our compositional

00:14:42,740 --> 00:14:47,209
which is a really key idea it makes the

00:14:44,929 --> 00:14:48,470
whole thing work then I want to talk

00:14:47,209 --> 00:14:49,879
about algorithms for calculating the

00:14:48,470 --> 00:14:52,220
gradients and there are two Agron's

00:14:49,879 --> 00:14:53,660
we'll be talking about we've called ford

00:14:52,220 --> 00:14:55,779
mode automatic differentiation and

00:14:53,660 --> 00:14:57,949
reverse mode automatic differentiation

00:14:55,779 --> 00:14:59,779
ford mode is slightly easier to

00:14:57,949 --> 00:15:01,249
understand we spend a bit more time on

00:14:59,779 --> 00:15:03,679
their reverse mode is the thing you're

00:15:01,249 --> 00:15:05,059
more likely to use in practice I'll move

00:15:03,679 --> 00:15:07,669
a little bit more quickly but there is

00:15:05,059 --> 00:15:09,199
code you can go look at afterwards the

00:15:07,669 --> 00:15:13,759
first let's talk about composition of

00:15:09,199 --> 00:15:16,519
derivatives so if function compose what

00:15:13,759 --> 00:15:19,759
I mean by functions composing well we

00:15:16,519 --> 00:15:23,379
can write some kind of this G after air

00:15:19,759 --> 00:15:27,259
for gf you pronounce it just means

00:15:23,379 --> 00:15:28,430
applying G to the output of applying F

00:15:27,259 --> 00:15:31,160
to some inputs

00:15:28,430 --> 00:15:32,240
that's what function composition is and

00:15:31,160 --> 00:15:33,980
the really nice thing about function

00:15:32,240 --> 00:15:35,930
composition is you can build big

00:15:33,980 --> 00:15:37,520
functions out of small functions you

00:15:35,930 --> 00:15:39,440
guys little building blocks and you

00:15:37,520 --> 00:15:41,240
assemble them together and the process

00:15:39,440 --> 00:15:43,330
of assembling them doesn't really change

00:15:41,240 --> 00:15:44,930
the meaning of the individual pieces

00:15:43,330 --> 00:15:46,190
composition the big deal to has

00:15:44,930 --> 00:15:48,050
functional programmers this is why we

00:15:46,190 --> 00:15:51,350
don't like effects so side effects sorry

00:15:48,050 --> 00:15:53,210
the side effects break composition may

00:15:51,350 --> 00:15:55,970
break compositional reasoning and all

00:15:53,210 --> 00:15:59,120
sorts of things I'll rant about that

00:15:55,970 --> 00:16:02,300
much later if anybody wants much depth

00:15:59,120 --> 00:16:08,030
anyway back to derivatives derivatives

00:16:02,300 --> 00:16:11,060
compose as well and you may or may not

00:16:08,030 --> 00:16:12,920
be familiar with the chain rule I'm on

00:16:11,060 --> 00:16:14,780
calculus and we can write up a chain

00:16:12,920 --> 00:16:16,460
rule in this form which tells us that

00:16:14,780 --> 00:16:18,890
the derivative of a function composition

00:16:16,460 --> 00:16:23,960
is the derivative of that outer function

00:16:18,890 --> 00:16:26,840
G composed with F multiplied by the

00:16:23,960 --> 00:16:29,090
derivative of F and what's great about

00:16:26,840 --> 00:16:31,280
this is you notice that they're

00:16:29,090 --> 00:16:33,590
determined brackets doesn't depend on

00:16:31,280 --> 00:16:35,660
the derivative of F so any when you

00:16:33,590 --> 00:16:37,820
compose a few things together they you

00:16:35,660 --> 00:16:39,860
have to combine them so you can

00:16:37,820 --> 00:16:42,230
calculate derivatives independently and

00:16:39,860 --> 00:16:43,940
then combine them so you don't have to

00:16:42,230 --> 00:16:46,280
like assemble one great big program and

00:16:43,940 --> 00:16:48,260
then work out the derivative of it you

00:16:46,280 --> 00:16:50,420
can break it down to small pieces and

00:16:48,260 --> 00:16:51,980
compose them together in the same way

00:16:50,420 --> 00:16:53,180
that you can pose together functions and

00:16:51,980 --> 00:16:58,820
all the other other lovely things we

00:16:53,180 --> 00:17:03,530
compose in Scala code okay so let's talk

00:16:58,820 --> 00:17:07,520
about calculating gradients then all

00:17:03,530 --> 00:17:09,560
right so who did like calculus in high

00:17:07,520 --> 00:17:12,200
school or university or something

00:17:09,560 --> 00:17:13,880
okay ammos turns getting out excellent

00:17:12,200 --> 00:17:14,720
so certainly said calculate degrading

00:17:13,880 --> 00:17:16,670
your post like what I call the

00:17:14,720 --> 00:17:18,200
mathematicians approach which is also

00:17:16,670 --> 00:17:20,209
known as sometimes it's embodied in Chi

00:17:18,200 --> 00:17:22,100
a ssin and what is you'd write down your

00:17:20,209 --> 00:17:24,140
function then you get a great big list

00:17:22,100 --> 00:17:25,370
of rules of differentiation and you just

00:17:24,140 --> 00:17:27,350
keep applying those rules if you can't

00:17:25,370 --> 00:17:29,570
play them anymore and then you've got

00:17:27,350 --> 00:17:31,400
the answer

00:17:29,570 --> 00:17:34,030
you were probably tortured with this if

00:17:31,400 --> 00:17:36,860
you did calculus at some length the

00:17:34,030 --> 00:17:38,210
great thing about this is you get an

00:17:36,860 --> 00:17:39,320
exact solution you have an expression

00:17:38,210 --> 00:17:41,030
that is

00:17:39,320 --> 00:17:44,870
the derivative you can calculate that

00:17:41,030 --> 00:17:47,230
exactly but the thing that's not so

00:17:44,870 --> 00:17:49,700
great about this is firstly that there

00:17:47,230 --> 00:17:52,280
no loops or conditionals have to be like

00:17:49,700 --> 00:17:54,320
an analytic function as mathematicians

00:17:52,280 --> 00:17:55,520
something called these things so you

00:17:54,320 --> 00:17:57,650
can't do these fun things I have talked

00:17:55,520 --> 00:17:59,960
about people doing in deep learning it's

00:17:57,650 --> 00:18:01,190
not really like programming and there's

00:17:59,960 --> 00:18:02,990
another problem which is that the size

00:18:01,190 --> 00:18:04,970
the derivative grows exponentially

00:18:02,990 --> 00:18:07,760
besides the expression this is because

00:18:04,970 --> 00:18:10,040
of things like some product rule which

00:18:07,760 --> 00:18:13,550
is duplicating expressions and/or some

00:18:10,040 --> 00:18:17,000
real progress so the derivative can get

00:18:13,550 --> 00:18:18,320
very big all right you'll say but we're

00:18:17,000 --> 00:18:19,340
not mathematicians it we're programmers

00:18:18,320 --> 00:18:21,890
let's take the programmers approach

00:18:19,340 --> 00:18:22,850
let's hack it of course so maybe in

00:18:21,890 --> 00:18:24,770
characters as well you might see

00:18:22,850 --> 00:18:26,000
something like this saying that you can

00:18:24,770 --> 00:18:27,770
think of the gradient as being take the

00:18:26,000 --> 00:18:29,750
function at a particular point add a

00:18:27,770 --> 00:18:31,340
small amount onto that point and then

00:18:29,750 --> 00:18:32,810
just divide by the same a small amount

00:18:31,340 --> 00:18:34,430
and this will give you a caricature of

00:18:32,810 --> 00:18:36,440
the gradient we're gonna have to

00:18:34,430 --> 00:18:38,810
evaluate the function twice to do this

00:18:36,440 --> 00:18:41,060
it sound like it'd be great we can do

00:18:38,810 --> 00:18:44,090
with any function good stuff but the

00:18:41,060 --> 00:18:47,630
problem is choosing this lines small

00:18:44,090 --> 00:18:49,760
amount this epsilon is hard and the

00:18:47,630 --> 00:18:51,410
error you get depends on epsilon and the

00:18:49,760 --> 00:18:54,410
function in some very hard to determine

00:18:51,410 --> 00:18:55,970
way and you can't really work it out

00:18:54,410 --> 00:18:58,370
beforehand you have to sort of do it

00:18:55,970 --> 00:19:00,440
experimentally so it's not great it's

00:18:58,370 --> 00:19:01,790
not great to get in exact to get error

00:19:00,440 --> 00:19:04,220
that we're not really controlling you

00:19:01,790 --> 00:19:05,480
now calculation of the derivatives so

00:19:04,220 --> 00:19:07,400
we're gonna do of course is your

00:19:05,480 --> 00:19:08,540
animator differentiation approach and

00:19:07,400 --> 00:19:10,580
I've said the two main algorithms

00:19:08,540 --> 00:19:13,730
forward and reverse and evangelized

00:19:10,580 --> 00:19:16,070
gives us an exact solution and it works

00:19:13,730 --> 00:19:19,280
with any function so it's kind like the

00:19:16,070 --> 00:19:20,510
best of both worlds really all right so

00:19:19,280 --> 00:19:22,670
that's enough hype for automating

00:19:20,510 --> 00:19:23,690
different differentiation let's see how

00:19:22,670 --> 00:19:27,400
it actually works

00:19:23,690 --> 00:19:29,870
so we'll start for mode which is simpler

00:19:27,400 --> 00:19:31,790
forward mode automated differentiation

00:19:29,870 --> 00:19:35,720
relies on this concept called dual

00:19:31,790 --> 00:19:37,340
numbers when you first started doing

00:19:35,720 --> 00:19:40,640
calculus you might have done that kind

00:19:37,340 --> 00:19:41,870
of naive infinitesimal calculus adding

00:19:40,640 --> 00:19:43,550
all those little epsilon terms and

00:19:41,870 --> 00:19:44,660
cancelling them out and at some point

00:19:43,550 --> 00:19:46,520
something might've told you you know

00:19:44,660 --> 00:19:48,050
you're not allowed to do that it doesn't

00:19:46,520 --> 00:19:50,090
make sense and then if you did analysis

00:19:48,050 --> 00:19:52,640
you tore down the limits and so forth

00:19:50,090 --> 00:19:53,680
but it turns out with you guys

00:19:52,640 --> 00:19:55,970
to do this kind of infinite time

00:19:53,680 --> 00:19:58,160
infinitesimal stuff and it does make

00:19:55,970 --> 00:20:00,620
sense in a suitable framework and that's

00:19:58,160 --> 00:20:03,140
what we're going to do so we're going to

00:20:00,620 --> 00:20:06,350
take a number a and we can augment it

00:20:03,140 --> 00:20:08,060
with this B epsilon term which is our

00:20:06,350 --> 00:20:10,010
infinitesimal or our measurement of the

00:20:08,060 --> 00:20:12,290
big of the gradient and there's a rule

00:20:10,010 --> 00:20:15,050
of algebra for this algebra and dual

00:20:12,290 --> 00:20:16,520
numbers the epsilon squared is 0 this is

00:20:15,050 --> 00:20:17,810
like complex number where you take a

00:20:16,520 --> 00:20:19,910
normal number and you stick on this

00:20:17,810 --> 00:20:21,320
imaginary part we're doing the same

00:20:19,910 --> 00:20:23,660
thing we're sticking on another part for

00:20:21,320 --> 00:20:26,170
the number and but this case is this

00:20:23,660 --> 00:20:29,870
infinitesimally as an imaginary number

00:20:26,170 --> 00:20:32,630
has different rules for algebra so we

00:20:29,870 --> 00:20:34,550
clean press encourage like so V is our

00:20:32,630 --> 00:20:37,310
value that we're computing and D is our

00:20:34,550 --> 00:20:40,820
derivative right infinitesimal component

00:20:37,310 --> 00:20:43,280
all right then what we need to do is

00:20:40,820 --> 00:20:46,580
need to go and add in rules for basic

00:20:43,280 --> 00:20:48,620
operations and functions so adding

00:20:46,580 --> 00:20:51,140
together two dual numbers just do the

00:20:48,620 --> 00:20:52,760
usual kind of algebra here we add

00:20:51,140 --> 00:20:55,760
together like the number number bits and

00:20:52,760 --> 00:20:57,890
the infinitesimal bits separately and we

00:20:55,760 --> 00:21:01,490
can go anywhere that in code like so

00:20:57,890 --> 00:21:03,610
Plus that values derivatives adding

00:21:01,490 --> 00:21:03,610
together

00:21:04,060 --> 00:21:09,470
multiplying together two dual numbers

00:21:07,210 --> 00:21:10,940
usual rules multiplying together numbers

00:21:09,470 --> 00:21:14,180
so you gotta remember that epsilon

00:21:10,940 --> 00:21:17,510
squared is 0 so we cancel that term out

00:21:14,180 --> 00:21:19,370
we end up with expression shown on the

00:21:17,510 --> 00:21:20,780
board and of course we came in from

00:21:19,370 --> 00:21:29,630
Atlas and carried as well let me get

00:21:20,780 --> 00:21:31,550
that more interesting is what is the

00:21:29,630 --> 00:21:35,390
chain rule expressed in terms of dual

00:21:31,550 --> 00:21:38,450
numbers so if you have a dual number

00:21:35,390 --> 00:21:40,730
which represents some computation

00:21:38,450 --> 00:21:42,290
proceeding this sum we've calculated the

00:21:40,730 --> 00:21:44,680
the derivative of the infinitesimal and

00:21:42,290 --> 00:21:47,660
the value I'm gonna ply to a function

00:21:44,680 --> 00:21:49,400
then what we get is a client's function

00:21:47,660 --> 00:21:52,850
to the the numeric part the number part

00:21:49,400 --> 00:21:55,100
the real part if we like and we get the

00:21:52,850 --> 00:21:57,100
infinitesimal which is the derivative of

00:21:55,100 --> 00:21:58,610
the function at this real part

00:21:57,100 --> 00:22:01,850
multiplied by the

00:21:58,610 --> 00:22:05,300
B the previous derivative this is the

00:22:01,850 --> 00:22:05,950
chain rule for dual numbers only came

00:22:05,300 --> 00:22:09,760
from a net

00:22:05,950 --> 00:22:11,560
like so so here is sign and it looks

00:22:09,760 --> 00:22:15,160
like that something kind of interesting

00:22:11,560 --> 00:22:16,390
about this which is that when we

00:22:15,160 --> 00:22:17,650
calculate the derivative of sign we've

00:22:16,390 --> 00:22:19,720
got two different things we need to know

00:22:17,650 --> 00:22:21,310
about the derivative for sign which is

00:22:19,720 --> 00:22:23,770
cars and we need to know about the

00:22:21,310 --> 00:22:25,150
previous derivative so we might think

00:22:23,770 --> 00:22:28,690
some point can be factored these out

00:22:25,150 --> 00:22:30,910
because this multiplying by D this dot d

00:22:28,690 --> 00:22:32,650
is always going to be the same maybe we

00:22:30,910 --> 00:22:34,540
could factor that bit out of it

00:22:32,650 --> 00:22:36,460
and we'll look at in a bit but right now

00:22:34,540 --> 00:22:37,660
I just want to show you in example there

00:22:36,460 --> 00:22:40,570
just a thought to hold in your head or

00:22:37,660 --> 00:22:42,550
not it doesn't really matter okay say

00:22:40,570 --> 00:22:45,240
it's an example sine squared X very

00:22:42,550 --> 00:22:47,430
exciting function how does the

00:22:45,240 --> 00:22:50,530
derivative computation work in this case

00:22:47,430 --> 00:22:53,530
well the sort of control flow would look

00:22:50,530 --> 00:22:55,150
like this you put X you take the sine of

00:22:53,530 --> 00:22:56,950
it to them and multiply them together

00:22:55,150 --> 00:22:59,200
and you get your results sounds great X

00:22:56,950 --> 00:23:01,570
and when you trace through the jewel

00:22:59,200 --> 00:23:04,630
number you're going to come to close yes

00:23:01,570 --> 00:23:05,860
stop saying X is 1 and the gradient

00:23:04,630 --> 00:23:09,780
derivative is going to be one as well

00:23:05,860 --> 00:23:13,060
it's constant like was X ort of thing

00:23:09,780 --> 00:23:17,440
that's after this value okay let me move

00:23:13,060 --> 00:23:20,400
on to the sines we apply the chain rule

00:23:17,440 --> 00:23:22,900
we saw earlier we get this

00:23:20,400 --> 00:23:25,630
multiply them together by the product

00:23:22,900 --> 00:23:29,110
rule and there's our answer

00:23:25,630 --> 00:23:31,180
and I can copy and paste from a console

00:23:29,110 --> 00:23:33,790
to prove to you this is like next-level

00:23:31,180 --> 00:23:37,260
proof here but yes this is what we get

00:23:33,790 --> 00:23:40,090
when we run it the trace goes through

00:23:37,260 --> 00:23:42,930
now I want to return to manipulation of

00:23:40,090 --> 00:23:46,720
the chain rule any questions on this bit

00:23:42,930 --> 00:23:48,040
good ok what's wrong time to return to

00:23:46,720 --> 00:23:50,080
the chain rule now and talk a little bit

00:23:48,040 --> 00:23:52,030
how give me leap you laid it and get

00:23:50,080 --> 00:23:54,370
something interesting out so you

00:23:52,030 --> 00:23:57,730
remember that the chain rule for dual

00:23:54,370 --> 00:24:00,340
numbers looked like this and what I want

00:23:57,730 --> 00:24:09,570
to point out here is that the bits that

00:24:00,340 --> 00:24:11,789
involve a only involve F as well so

00:24:09,570 --> 00:24:13,080
we don't need to have we could think

00:24:11,789 --> 00:24:14,639
well maybe we can sort of pull out these

00:24:13,080 --> 00:24:16,259
these bits and just have some function

00:24:14,639 --> 00:24:17,700
that you give me a they're the real

00:24:16,259 --> 00:24:21,059
component and I'm going to give you back

00:24:17,700 --> 00:24:22,619
the function applied to that real

00:24:21,059 --> 00:24:26,299
component and its derivative there's

00:24:22,619 --> 00:24:28,499
epsilon there should be at the end here

00:24:26,299 --> 00:24:30,359
can I separate this out and what happens

00:24:28,499 --> 00:24:31,679
if I do and I'm just gonna because this

00:24:30,359 --> 00:24:34,679
notation gonna get a little bit messy

00:24:31,679 --> 00:24:37,950
I'm gonna represent the derivative which

00:24:34,679 --> 00:24:39,659
is a do number by this box so what this

00:24:37,950 --> 00:24:41,369
box means is you got F of a and its

00:24:39,659 --> 00:24:44,879
associated derivative I hope you that's

00:24:41,369 --> 00:24:49,649
clear for context so I can rewrite this

00:24:44,879 --> 00:24:50,879
code and we're going to rewrite this

00:24:49,649 --> 00:24:52,950
expression where something looks like

00:24:50,879 --> 00:24:54,119
this so on the right hand side we've got

00:24:52,950 --> 00:24:56,009
our original start of it's on the left

00:24:54,119 --> 00:24:58,049
hand side we've got some manipulation

00:24:56,009 --> 00:25:01,710
with it and what we want to do is

00:24:58,049 --> 00:25:04,049
somehow we started with a plus B epsilon

00:25:01,710 --> 00:25:06,599
we're gonna pass this through F well

00:25:04,049 --> 00:25:10,109
this one function somehow we want to do

00:25:06,599 --> 00:25:13,619
is multiplying by B epsilon did that we

00:25:10,109 --> 00:25:15,409
have there you want to do some function

00:25:13,619 --> 00:25:18,359
information function is going to do this

00:25:15,409 --> 00:25:20,729
I'm just gonna write a shortcut just

00:25:18,359 --> 00:25:22,169
call this C this value just to make it

00:25:20,729 --> 00:25:25,289
clearer what's going on and you wanted

00:25:22,169 --> 00:25:28,259
to tell me what is going on here so here

00:25:25,289 --> 00:25:30,239
a flat map of wild flat left appears yes

00:25:28,259 --> 00:25:32,309
it is it is flat map ok so there's

00:25:30,239 --> 00:25:34,139
actually a monad here and there isn't

00:25:32,309 --> 00:25:38,999
one out for dual numbers it does meet

00:25:34,139 --> 00:25:42,059
the Monad laws and this monitor

00:25:38,999 --> 00:25:44,669
expresses the chain rule and we can even

00:25:42,059 --> 00:25:46,669
like so flat map and this does that's

00:25:44,669 --> 00:25:49,349
multiplying by D bit for us

00:25:46,669 --> 00:25:52,379
and once we've done that we can then

00:25:49,349 --> 00:25:54,029
implement sine as in terms of flat map

00:25:52,379 --> 00:25:55,710
and we don't have to do that multiplying

00:25:54,029 --> 00:25:59,210
by D bit repeating it every time anymore

00:25:55,710 --> 00:26:00,419
it's taking care of us or us by flat now

00:25:59,210 --> 00:26:01,919
okay

00:26:00,419 --> 00:26:02,999
how many she seemed that mention

00:26:01,919 --> 00:26:06,720
anywhere so that I found that

00:26:02,999 --> 00:26:08,070
interesting let's move on so deep

00:26:06,720 --> 00:26:09,629
learning you need to do the real deep

00:26:08,070 --> 00:26:10,889
learning to generalize tenses by them

00:26:09,629 --> 00:26:12,929
and you shown you that one-dimensional

00:26:10,889 --> 00:26:14,460
case here generalized in two tenses is

00:26:12,929 --> 00:26:15,479
not really any harder but there's a lot

00:26:14,460 --> 00:26:20,309
more where I think it's kind of tedious

00:26:15,479 --> 00:26:21,359
so I haven't done it in this case the

00:26:20,309 --> 00:26:22,669
other thing it's important to know about

00:26:21,359 --> 00:26:23,809
forward motor I have

00:26:22,669 --> 00:26:26,629
really demonstrated hillocks when you're

00:26:23,809 --> 00:26:30,320
working with scalars is that it scales

00:26:26,629 --> 00:26:33,279
the size of the input demand size the

00:26:30,320 --> 00:26:33,279
input dimension yeah

00:26:33,769 --> 00:26:38,960
and often the input can be quite large

00:26:36,919 --> 00:26:41,149
and the output could be like just one

00:26:38,960 --> 00:26:44,659
number a decision yes or no this is a

00:26:41,149 --> 00:26:47,059
dog or a cat or whatever so it's scaling

00:26:44,659 --> 00:26:48,950
inside the intervention is not great so

00:26:47,059 --> 00:26:50,299
let's turn to reverse mode automatic

00:26:48,950 --> 00:26:52,299
differentiation which is going to solve

00:26:50,299 --> 00:26:55,759
some of these problems here I thought

00:26:52,299 --> 00:26:57,259
now the key idea in Reverse mode you

00:26:55,759 --> 00:26:58,789
probably guessed from the name it's all

00:26:57,259 --> 00:27:01,369
about doing things backwards every sort

00:26:58,789 --> 00:27:02,690
of forward mode meant forwards a key

00:27:01,369 --> 00:27:04,100
idea is really that the trainable

00:27:02,690 --> 00:27:07,369
doesn't really care about which order

00:27:04,100 --> 00:27:09,139
you apply things so let me show you what

00:27:07,369 --> 00:27:11,570
I mean by that this is our chain we'll

00:27:09,139 --> 00:27:16,100
written out just just right out expanded

00:27:11,570 --> 00:27:17,869
form and when we did the dual numbers we

00:27:16,100 --> 00:27:21,799
calculated this the inner stuff first

00:27:17,869 --> 00:27:24,519
and then we did the G bit and math

00:27:21,799 --> 00:27:26,690
doesn't care which way you do things so

00:27:24,519 --> 00:27:28,429
what we could do instead was we could do

00:27:26,690 --> 00:27:31,989
this bit first and sort of calculate

00:27:28,429 --> 00:27:34,159
backwards and what does it actually mean

00:27:31,989 --> 00:27:35,840
it means a process like this you're

00:27:34,159 --> 00:27:39,649
going to start with our sine squared X

00:27:35,840 --> 00:27:41,869
again and you would go through it this

00:27:39,649 --> 00:27:45,409
time just calculating the the real

00:27:41,869 --> 00:27:47,539
values and storing them in there

00:27:45,409 --> 00:27:50,239
the end and then we get to the end we

00:27:47,539 --> 00:27:52,340
need to go backwards and calculate these

00:27:50,239 --> 00:27:53,570
derivatives using the equation shown and

00:27:52,340 --> 00:27:55,279
you go back and you go back and you go

00:27:53,570 --> 00:28:01,429
back and you get the result that you'd

00:27:55,279 --> 00:28:05,269
want okay so some complications arise

00:28:01,429 --> 00:28:07,519
here the main complication is you need

00:28:05,269 --> 00:28:09,049
to store the control flow somehow

00:28:07,519 --> 00:28:09,889
because you need to go backwards over it

00:28:09,049 --> 00:28:12,080
so you need to have some kind of

00:28:09,889 --> 00:28:13,850
representation of the control flow so

00:28:12,080 --> 00:28:18,529
you can then calculate backwards down

00:28:13,850 --> 00:28:20,600
this till graph we saw here and you can

00:28:18,529 --> 00:28:24,889
one way of representing control flow is

00:28:20,600 --> 00:28:27,169
as a I'm gonna tell me ma nerd it's am

00:28:24,889 --> 00:28:28,940
honored or you continuations whatever

00:28:27,169 --> 00:28:30,350
other now you can represent a tomorrow

00:28:28,940 --> 00:28:31,849
let's base what I'm on air does have one

00:28:30,350 --> 00:28:34,639
AD represents control flow and we've

00:28:31,849 --> 00:28:35,200
already seen that flat map is the chain

00:28:34,639 --> 00:28:37,420
rule

00:28:35,200 --> 00:28:42,160
so with a different monitor

00:28:37,420 --> 00:28:45,520
implementation we can implement reverse

00:28:42,160 --> 00:28:49,000
mode instead of forward mode and what we

00:28:45,520 --> 00:28:52,240
need to do is store what I called these

00:28:49,000 --> 00:28:54,220
KP values here I've got the continuation

00:28:52,240 --> 00:28:55,870
and this is what allows us to go

00:28:54,220 --> 00:28:57,750
backwards you put a value in it and as

00:28:55,870 --> 00:29:01,480
you goes backwards down the computation

00:28:57,750 --> 00:29:03,160
alright this is a fairly subtle point I

00:29:01,480 --> 00:29:04,870
don't really want to get caught up on

00:29:03,160 --> 00:29:06,460
exactly how it works it takes me a bit

00:29:04,870 --> 00:29:07,930
of time to explain I just want the core

00:29:06,460 --> 00:29:09,880
idea of automated differentiation to go

00:29:07,930 --> 00:29:12,160
on if you want to go in the details

00:29:09,880 --> 00:29:14,380
welcome to talk to me afterwards or go

00:29:12,160 --> 00:29:16,960
have a look at the carriage so once

00:29:14,380 --> 00:29:18,580
we've got this we got this monad this

00:29:16,960 --> 00:29:21,040
continuation how's us to go backwards

00:29:18,580 --> 00:29:22,360
over the control flow we can go ahead

00:29:21,040 --> 00:29:26,620
and put the rest of the things that we

00:29:22,360 --> 00:29:29,320
saw and get ourselves reverse motor

00:29:26,620 --> 00:29:30,790
automatic differentiation and there we

00:29:29,320 --> 00:29:34,390
go we run it and we get the same answer

00:29:30,790 --> 00:29:36,160
as before so you can see that writing

00:29:34,390 --> 00:29:38,350
the program is very straightforward this

00:29:36,160 --> 00:29:39,910
looks like normal code mostly but we get

00:29:38,350 --> 00:29:43,030
this additional calculation of the

00:29:39,910 --> 00:29:44,680
derivative as we go the great thing

00:29:43,030 --> 00:29:46,210
about reverse mode is that as scales and

00:29:44,680 --> 00:29:49,390
the size of the output dimension and

00:29:46,210 --> 00:29:51,640
usually the output is much much smaller

00:29:49,390 --> 00:29:53,440
than input so reverse mode is the

00:29:51,640 --> 00:29:55,410
preferred way of implementing automatic

00:29:53,440 --> 00:30:01,480
differentiation in deep learning

00:29:55,410 --> 00:30:03,250
techniques as I said the code is on

00:30:01,480 --> 00:30:04,810
github don't need to rush your laptop

00:30:03,250 --> 00:30:10,810
now I'll show this this slide at the end

00:30:04,810 --> 00:30:13,660
so you can get it okay right so we have

00:30:10,810 --> 00:30:15,250
talked about this idea of differentiable

00:30:13,660 --> 00:30:19,450
programming and we said differentiable

00:30:15,250 --> 00:30:21,310
programming is automatic differentiation

00:30:19,450 --> 00:30:23,980
we've seen how automate differentiation

00:30:21,310 --> 00:30:26,490
works and it's about composing code

00:30:23,980 --> 00:30:32,890
something that's very familiar to us

00:30:26,490 --> 00:30:34,330
functional programming now what's

00:30:32,890 --> 00:30:35,410
becoming increasingly cases people are

00:30:34,330 --> 00:30:36,580
teaching different reading

00:30:35,410 --> 00:30:39,010
differentiable programming as

00:30:36,580 --> 00:30:42,790
programming so I want to talk about some

00:30:39,010 --> 00:30:45,610
of the things which are happening now or

00:30:42,790 --> 00:30:47,590
in the near future and just how these

00:30:45,610 --> 00:30:49,430
relates to programming languages

00:30:47,590 --> 00:30:52,040
this point we are getting a little bit

00:30:49,430 --> 00:30:55,510
speculative so I'm gonna talk about

00:30:52,040 --> 00:31:00,470
ergonomics correctness and performance

00:30:55,510 --> 00:31:02,540
how they relate to Scala so the first

00:31:00,470 --> 00:31:05,180
thing economics we saw that we could

00:31:02,540 --> 00:31:08,380
write code that looked more or less like

00:31:05,180 --> 00:31:10,090
this and it would our choice of

00:31:08,380 --> 00:31:12,260
automatic differentiation

00:31:10,090 --> 00:31:13,970
differentiation algorithm computer for

00:31:12,260 --> 00:31:18,860
us the value and the derivative and is

00:31:13,970 --> 00:31:20,180
very simple to work with typical systems

00:31:18,860 --> 00:31:23,090
like tensor flow you would write

00:31:20,180 --> 00:31:25,730
something like this which is fairly

00:31:23,090 --> 00:31:28,400
horrendous to work with in a in the

00:31:25,730 --> 00:31:30,830
small scale here it's you can probably

00:31:28,400 --> 00:31:32,870
read it three lines rather than one but

00:31:30,830 --> 00:31:35,150
you can imagine in the large scale it

00:31:32,870 --> 00:31:36,410
gets quite painful to work with so the

00:31:35,150 --> 00:31:38,720
issue here is you're manually

00:31:36,410 --> 00:31:42,740
constructing these these graphs these

00:31:38,720 --> 00:31:44,240
computation graphs expression graphs and

00:31:42,740 --> 00:31:45,950
tends to flow is not very it doesn't

00:31:44,240 --> 00:31:50,270
it's not very nice to use in the first

00:31:45,950 --> 00:31:52,490
place but yes it's definitely a problem

00:31:50,270 --> 00:31:55,250
that people have big nor'easter North

00:31:52,490 --> 00:31:57,440
great and the solution here is really

00:31:55,250 --> 00:32:00,490
better DSL and cost cars great for

00:31:57,440 --> 00:32:03,230
building dsls really seeing this in

00:32:00,490 --> 00:32:05,630
Swift there's a experimental Fork of

00:32:03,230 --> 00:32:09,470
Swift where automatic differentiation is

00:32:05,630 --> 00:32:11,210
built into the language so it constructs

00:32:09,470 --> 00:32:13,070
the graph for you you just write a

00:32:11,210 --> 00:32:14,120
normal code and it'll construct the

00:32:13,070 --> 00:32:18,970
competition graph and run all my

00:32:14,120 --> 00:32:18,970
differentiation and it's it's very nice

00:32:19,720 --> 00:32:24,380
next thing I want to talk about is

00:32:21,590 --> 00:32:27,410
correctness this is quite quite a big

00:32:24,380 --> 00:32:30,680
issue so there are lots of places where

00:32:27,410 --> 00:32:33,460
you can make mistakes and people do make

00:32:30,680 --> 00:32:35,420
mistakes so common example is

00:32:33,460 --> 00:32:37,370
dimensionality of your tensors you've

00:32:35,420 --> 00:32:41,570
got to agree you know when you multiply

00:32:37,370 --> 00:32:44,090
3 by 3 by 3 by 5 matrix you get out of 5

00:32:41,570 --> 00:32:50,150
and you have to get all these dimensions

00:32:44,090 --> 00:32:51,980
to line up another point is that these

00:32:50,150 --> 00:32:54,710
dimensions usually have some kind of

00:32:51,980 --> 00:32:55,650
meaning like if you're processing images

00:32:54,710 --> 00:32:58,110
and

00:32:55,650 --> 00:33:01,410
you probably have like a red green and

00:32:58,110 --> 00:33:04,230
blue channel so that's some meaning

00:33:01,410 --> 00:33:07,680
associated there and it's very common to

00:33:04,230 --> 00:33:10,890
do operations which operate over a

00:33:07,680 --> 00:33:12,810
particular dimension and then you have

00:33:10,890 --> 00:33:13,830
code like this which is kind of thing

00:33:12,810 --> 00:33:16,890
you write intensive flow

00:33:13,830 --> 00:33:18,780
what does access 1 mean is that the

00:33:16,890 --> 00:33:21,420
right axis is that the red channel the

00:33:18,780 --> 00:33:23,430
blue channel I don't know just something

00:33:21,420 --> 00:33:26,730
right so a lot of information that's not

00:33:23,430 --> 00:33:29,760
encoded in the current deep learning

00:33:26,730 --> 00:33:33,360
systems and that leads to bugs so on the

00:33:29,760 --> 00:33:35,820
solution here of course is expressive

00:33:33,360 --> 00:33:39,360
type systems so expressing the

00:33:35,820 --> 00:33:44,790
dimensions of the data in the type

00:33:39,360 --> 00:33:46,950
system doing scala and giving symbolic

00:33:44,790 --> 00:33:50,220
names to these dimensions which you can

00:33:46,950 --> 00:33:52,610
do with type literals which are

00:33:50,220 --> 00:33:55,470
something else well to be news scholar

00:33:52,610 --> 00:33:57,330
but you could do them in inconvenient

00:33:55,470 --> 00:33:59,130
way earlier and that allows you to

00:33:57,330 --> 00:34:01,080
actually check statically that you

00:33:59,130 --> 00:34:03,360
haven't made a mistake in the dimensions

00:34:01,080 --> 00:34:05,310
and allows make so much of a readable

00:34:03,360 --> 00:34:06,750
code because you don't say reduce the

00:34:05,310 --> 00:34:08,790
long dimension 1 you say reduce along

00:34:06,750 --> 00:34:11,399
the red channel or whatever something is

00:34:08,790 --> 00:34:14,880
meaningful to you and that's so an

00:34:11,399 --> 00:34:20,870
example of doing this is this

00:34:14,880 --> 00:34:20,870
experimental package Nexus is very nice

00:34:21,169 --> 00:34:26,250
it's not really a production-ready thing

00:34:23,280 --> 00:34:27,390
but I think it's where we can see a lot

00:34:26,250 --> 00:34:33,240
of stuff in the future going

00:34:27,390 --> 00:34:34,290
cough coming up excuse me one more thing

00:34:33,240 --> 00:34:34,860
I want to talk about here around

00:34:34,290 --> 00:34:38,220
correctness

00:34:34,860 --> 00:34:40,110
is I showed you monitor implementations

00:34:38,220 --> 00:34:43,260
before you can actually write incorrect

00:34:40,110 --> 00:34:44,820
programs using these flat maps and in

00:34:43,260 --> 00:34:46,950
general you can write programs that

00:34:44,820 --> 00:34:48,409
incorrect I calculate the incorrect

00:34:46,950 --> 00:34:51,260
result using automatic differentiation

00:34:48,409 --> 00:34:55,500
so it's not just unique to this flat map

00:34:51,260 --> 00:34:56,880
and the ad by itself is not going to

00:34:55,500 --> 00:34:58,350
automate differentiation by self is not

00:34:56,880 --> 00:35:01,710
going to tell you things that incorrect

00:34:58,350 --> 00:35:03,030
there is other work which is connecting

00:35:01,710 --> 00:35:05,540
linear logic and what's called the

00:35:03,030 --> 00:35:08,270
differential lambda calculus

00:35:05,540 --> 00:35:09,770
two derivatives and that but I don't

00:35:08,270 --> 00:35:11,660
think at this point in time anyone

00:35:09,770 --> 00:35:13,430
really understands exactly how that

00:35:11,660 --> 00:35:17,600
connects to automatic differentiation I

00:35:13,430 --> 00:35:20,570
certainly don't but I I think this might

00:35:17,600 --> 00:35:24,080
be the way to get more correct programs

00:35:20,570 --> 00:35:25,130
out I'm can talk about this later I

00:35:24,080 --> 00:35:26,300
don't want to spend too much time on

00:35:25,130 --> 00:35:29,830
this because the points are fairly

00:35:26,300 --> 00:35:31,670
subtle but if you interested talk later

00:35:29,830 --> 00:35:37,730
finally I want to talk a bit about

00:35:31,670 --> 00:35:39,320
performance now you need compilation for

00:35:37,730 --> 00:35:41,420
performance and use ready to compile to

00:35:39,320 --> 00:35:47,080
GPUs for most situations on all of them

00:35:41,420 --> 00:35:49,340
so these fees some compiler involved

00:35:47,080 --> 00:35:50,870
right now the way this is done I showed

00:35:49,340 --> 00:35:52,370
you like the tensor flow is with these

00:35:50,870 --> 00:35:53,930
embedded D s hours and these are fairly

00:35:52,370 --> 00:35:55,460
horrible to use because you have to

00:35:53,930 --> 00:35:58,120
construct the computation graph mainly

00:35:55,460 --> 00:36:01,130
so the compiler can walk over it and

00:35:58,120 --> 00:36:03,560
work things out for you work out the

00:36:01,130 --> 00:36:07,130
compilation is not a great experience

00:36:03,560 --> 00:36:10,400
and we can do in the future or what

00:36:07,130 --> 00:36:12,170
what's going on now is staging so

00:36:10,400 --> 00:36:15,770
staging any is something that Scala does

00:36:12,170 --> 00:36:17,780
very nicely there's a a tool called

00:36:15,770 --> 00:36:19,490
lightweight much of the staging which

00:36:17,780 --> 00:36:20,900
does it there it is a fork of Scala

00:36:19,490 --> 00:36:22,490
compiler I'm hoping that with what

00:36:20,900 --> 00:36:25,220
they're talking about in dot e staging

00:36:22,490 --> 00:36:27,050
is going to be a key part of the library

00:36:25,220 --> 00:36:28,520
oh sorry the language and the idea of

00:36:27,050 --> 00:36:31,460
staging is basically you have a program

00:36:28,520 --> 00:36:33,440
which writes a program and that program

00:36:31,460 --> 00:36:36,760
itself can write another program so

00:36:33,440 --> 00:36:39,080
we're staging you get to you get to

00:36:36,760 --> 00:36:40,430
create a program which is inner generate

00:36:39,080 --> 00:36:42,950
some more code which codes may be

00:36:40,430 --> 00:36:45,740
compiled into like GPU format much

00:36:42,950 --> 00:36:48,500
easier system to use then just how to

00:36:45,740 --> 00:36:50,510
construct these graphs by hand an

00:36:48,500 --> 00:36:53,540
example of what can be done there is

00:36:50,510 --> 00:36:57,850
this Lantern system which is tr kromm's

00:36:53,540 --> 00:37:01,580
group in Purdue Purdue I believe alright

00:36:57,850 --> 00:37:03,200
so that was just some sort of looking

00:37:01,580 --> 00:37:04,400
ahead at where I think we're going deep

00:37:03,200 --> 00:37:06,110
learning something is moving very fast

00:37:04,400 --> 00:37:08,600
the system's we have right now are

00:37:06,110 --> 00:37:10,640
really quite primitive I think they

00:37:08,600 --> 00:37:12,860
we're gonna see a lot of work in in the

00:37:10,640 --> 00:37:15,720
next few years and making it better it's

00:37:12,860 --> 00:37:17,400
Carla gonna be part of that we'll see

00:37:15,720 --> 00:37:20,040
it really comes down to doing it let's

00:37:17,400 --> 00:37:21,119
get back over the conclusions firstly

00:37:20,040 --> 00:37:23,190
deep learning is all about function

00:37:21,119 --> 00:37:24,660
composition and derivatives and we know

00:37:23,190 --> 00:37:26,330
that function programming is about

00:37:24,660 --> 00:37:28,290
function composition so that's great

00:37:26,330 --> 00:37:29,849
we've seen we can easily express

00:37:28,290 --> 00:37:30,990
derivatives in functional programming in

00:37:29,849 --> 00:37:35,790
fact as the monitor which makes us

00:37:30,990 --> 00:37:37,230
inordinately happy and I think that what

00:37:35,790 --> 00:37:40,109
we see in the features type system

00:37:37,230 --> 00:37:41,760
staging Scala is well positioned to be

00:37:40,109 --> 00:37:45,090
for the next generation of deep planning

00:37:41,760 --> 00:37:46,410
system but it is undoubtedly a lot of

00:37:45,090 --> 00:37:47,820
work and somebody's got to do it and

00:37:46,410 --> 00:37:49,500
it's probably not going to be me though

00:37:47,820 --> 00:37:52,910
if you'd like me to do it and you have a

00:37:49,500 --> 00:37:55,920
big pile of money now let's talk all

00:37:52,910 --> 00:37:58,320
right so that's all I got to say

00:37:55,920 --> 00:37:59,790
hopefully I've given you enough of an

00:37:58,320 --> 00:38:02,430
understanding of how deep learning works

00:37:59,790 --> 00:38:04,260
and you now so feel you can understand

00:38:02,430 --> 00:38:06,420
it you can maybe implement yourself you

00:38:04,260 --> 00:38:09,150
know that automatic differentiation if

00:38:06,420 --> 00:38:10,800
you want the code there isn't github and

00:38:09,150 --> 00:38:13,010
you have any questions that would be

00:38:10,800 --> 00:38:13,010
great

00:38:20,520 --> 00:38:27,460
all right so any questions out there

00:38:25,200 --> 00:38:37,569
I've done such a great job ah there we

00:38:27,460 --> 00:38:40,480
go that's a good question what does map

00:38:37,569 --> 00:38:43,119
correspond to I mmm I guess map

00:38:40,480 --> 00:38:45,880
corresponds corresponds to multiplying

00:38:43,119 --> 00:38:47,670
by a constant like this of operation my

00:38:45,880 --> 00:38:49,329
constant or at least our operation on

00:38:47,670 --> 00:38:53,290
something which is not one of your

00:38:49,329 --> 00:38:54,970
parameterize variables so yeah

00:38:53,290 --> 00:38:57,430
multiplying by like a scalar or thing

00:38:54,970 --> 00:38:59,460
like that or otherwise anything that

00:38:57,430 --> 00:39:02,319
doesn't change the derivative derivative

00:38:59,460 --> 00:39:06,900
so like 2 times X you could implement

00:39:02,319 --> 00:39:06,900

YouTube URL: https://www.youtube.com/watch?v=igRLKYgpHy0


