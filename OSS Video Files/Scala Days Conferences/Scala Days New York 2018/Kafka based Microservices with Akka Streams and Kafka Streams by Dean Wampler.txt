Title: Kafka based Microservices with Akka Streams and Kafka Streams by Dean Wampler
Publication date: 2018-09-21
Playlist: Scala Days New York 2018
Description: 
	This video was recorded at Scala Days New York 2018
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

More information and the abstract can be found here:
https://na.scaladays.org/schedule/kafka-based-microservices-with-akka-streams-and-kafka-streams
Captions: 
	00:00:04,630 --> 00:00:09,010
this is talk about building micro

00:00:06,430 --> 00:00:10,900
services that are based on Kafka is sort

00:00:09,010 --> 00:00:14,679
of the back plane and then using akka

00:00:10,900 --> 00:00:16,270
streams or and maybe Kafka streams I

00:00:14,679 --> 00:00:19,210
want to talk a little bit about the pros

00:00:16,270 --> 00:00:21,010
and cons of each choice you know what I

00:00:19,210 --> 00:00:23,320
like about them what I think you should

00:00:21,010 --> 00:00:26,530
be aware of and I'll even show some code

00:00:23,320 --> 00:00:29,200
examples as usually I put photographs

00:00:26,530 --> 00:00:30,400
that I've taken in the decks that have

00:00:29,200 --> 00:00:32,079
absolutely nothing to do with the

00:00:30,400 --> 00:00:34,090
subject except there's a lot of water

00:00:32,079 --> 00:00:36,310
pictures so you know streams I guess

00:00:34,090 --> 00:00:41,140
these happen to be from Glacier National

00:00:36,310 --> 00:00:43,060
Park up in Montana so let's start by

00:00:41,140 --> 00:00:44,590
putting streaming context a little bit

00:00:43,060 --> 00:00:46,329
but this is actually the same lake as

00:00:44,590 --> 00:00:52,900
the previous picture but earlier in the

00:00:46,329 --> 00:00:54,510
day I read a little a Reilly report that

00:00:52,900 --> 00:00:57,579
you can get for free on our website

00:00:54,510 --> 00:00:59,800
about Oh almost two years ago now that

00:00:57,579 --> 00:01:01,989
kind of goes into the way I see things

00:00:59,800 --> 00:01:04,420
shaping up as far as doing streaming

00:01:01,989 --> 00:01:06,790
data processing as opposed to like

00:01:04,420 --> 00:01:08,710
normal big data batch processing what

00:01:06,790 --> 00:01:10,719
those architectures have to account for

00:01:08,710 --> 00:01:12,909
what they have to support and so a lot

00:01:10,719 --> 00:01:14,740
of what we're talking about is kind of a

00:01:12,909 --> 00:01:17,200
summary of what's in this report if

00:01:14,740 --> 00:01:18,819
you're interested in more details and

00:01:17,200 --> 00:01:22,329
I'll have that link at the up at the end

00:01:18,819 --> 00:01:24,729
and also the slides will be available so

00:01:22,329 --> 00:01:27,100
if we look at the architecture that you

00:01:24,729 --> 00:01:28,810
know I think it's kind of emerging it's

00:01:27,100 --> 00:01:30,159
the best practice architecture if you

00:01:28,810 --> 00:01:31,509
will it looks something like this this

00:01:30,159 --> 00:01:33,399
is a little busy and we're not gonna go

00:01:31,509 --> 00:01:36,369
through all of it the numbers come from

00:01:33,399 --> 00:01:38,979
the report itself but we're gonna focus

00:01:36,369 --> 00:01:41,229
on a couple of areas in this talk or in

00:01:38,979 --> 00:01:43,630
this diagram and that is Kafka as the

00:01:41,229 --> 00:01:45,219
data backplane you know where this kind

00:01:43,630 --> 00:01:48,399
of connecting everything together and

00:01:45,219 --> 00:01:50,079
then these two API Sokka streams and

00:01:48,399 --> 00:01:55,179
Kafka streams that you can embed in your

00:01:50,079 --> 00:01:58,529
microservices to do a data processing so

00:01:55,179 --> 00:01:58,529
first let's talk about why Kafka

00:01:59,549 --> 00:02:05,999
well Kafka is a log oriented system it's

00:02:04,029 --> 00:02:08,440
not really a message queue because the

00:02:05,999 --> 00:02:10,360
consumers do not consume the messages

00:02:08,440 --> 00:02:13,239
which is what a normal queue would do

00:02:10,360 --> 00:02:15,069
when you pop an event instead consumers

00:02:13,239 --> 00:02:15,860
can start at whatever offset they want

00:02:15,069 --> 00:02:18,320
in

00:02:15,860 --> 00:02:19,700
ordered log and start reading they can

00:02:18,320 --> 00:02:21,920
go back and reread if they've made a

00:02:19,700 --> 00:02:24,380
mistake if they've crashed or whatever

00:02:21,920 --> 00:02:26,450
and Kafka keeps track of the lifecycle

00:02:24,380 --> 00:02:28,040
of each of the elements normally it's

00:02:26,450 --> 00:02:30,320
like a timeout and you after a week they

00:02:28,040 --> 00:02:32,000
get deleted something like that and you

00:02:30,320 --> 00:02:34,400
can have as many producers as you want

00:02:32,000 --> 00:02:36,230
you know writing to the log as many

00:02:34,400 --> 00:02:38,590
consumers reading it and so forth and

00:02:36,230 --> 00:02:42,050
it's designed for large scalability for

00:02:38,590 --> 00:02:43,610
partitioning these these topics is the

00:02:42,050 --> 00:02:48,260
way things are organized so that you can

00:02:43,610 --> 00:02:50,660
scale horizontally and so forth so a

00:02:48,260 --> 00:02:52,370
given topic might be a logical thing

00:02:50,660 --> 00:02:54,380
that you're interested in like let's say

00:02:52,370 --> 00:02:56,450
you know credit card transactions if

00:02:54,380 --> 00:02:58,280
you're a bank and then for scale you

00:02:56,450 --> 00:03:01,220
might partition them maybe by something

00:02:58,280 --> 00:03:03,860
like you know zip code or you know the

00:03:01,220 --> 00:03:07,340
customer state that they live in or

00:03:03,860 --> 00:03:09,530
whatever the orders guaranteed in the

00:03:07,340 --> 00:03:11,390
partitions but not per topic so you

00:03:09,530 --> 00:03:12,739
don't get global ordering over the topic

00:03:11,390 --> 00:03:16,280
but you do get ordering in each

00:03:12,739 --> 00:03:18,320
partition it also has some important

00:03:16,280 --> 00:03:21,260
architectural benefits so I drew this

00:03:18,320 --> 00:03:22,820
diagram deliberately bad meaning hard to

00:03:21,260 --> 00:03:24,830
understand it's you know a complete

00:03:22,820 --> 00:03:26,780
bipartite graph where I've got data

00:03:24,830 --> 00:03:29,209
sources on the left and sinks on the

00:03:26,780 --> 00:03:31,430
right and the problem here is not only

00:03:29,209 --> 00:03:32,660
is this hard to comprehend if I'm you

00:03:31,430 --> 00:03:34,940
know sort of the architect trying to

00:03:32,660 --> 00:03:37,160
sort things out but you know suppose

00:03:34,940 --> 00:03:39,140
service one on the right crashes then

00:03:37,160 --> 00:03:40,820
I've suddenly got these broken links

00:03:39,140 --> 00:03:42,799
with all of these sources and I could

00:03:40,820 --> 00:03:45,049
have data loss you know and somehow I've

00:03:42,799 --> 00:03:46,640
got to even if I restart service one

00:03:45,049 --> 00:03:48,410
quickly I have to communicate to

00:03:46,640 --> 00:03:51,320
everybody on the left here's the new

00:03:48,410 --> 00:03:52,700
address to be writing data to and so

00:03:51,320 --> 00:03:55,580
forth there's a lot of problems with

00:03:52,700 --> 00:03:57,650
these kind of direct connections well as

00:03:55,580 --> 00:03:59,630
we all learned you know from computer

00:03:57,650 --> 00:04:02,420
science you can solve any problem with

00:03:59,630 --> 00:04:04,250
another level of indirection so we'll

00:04:02,420 --> 00:04:05,720
just stick Kafka in here and you know

00:04:04,250 --> 00:04:07,430
that solves the problem and it does

00:04:05,720 --> 00:04:09,590
actually solve the problems that I just

00:04:07,430 --> 00:04:11,480
described in a couple of ways one is

00:04:09,590 --> 00:04:13,010
it's conceptually simpler what's going

00:04:11,480 --> 00:04:15,380
on I sort of have this Universal

00:04:13,010 --> 00:04:16,940
pipeline or you know Enterprise Service

00:04:15,380 --> 00:04:19,579
bus if I can call it that that

00:04:16,940 --> 00:04:21,260
everything is flowing through so

00:04:19,579 --> 00:04:24,320
conceptually it's simpler it's also

00:04:21,260 --> 00:04:26,479
simpler like for the api's if I really

00:04:24,320 --> 00:04:29,000
take this to its extreme and do

00:04:26,479 --> 00:04:29,740
everything through Kafka then the only

00:04:29,000 --> 00:04:31,690
API

00:04:29,740 --> 00:04:34,330
you ever have to know is the producer

00:04:31,690 --> 00:04:36,190
and consumer API you know for writing to

00:04:34,330 --> 00:04:38,380
and from Kafka I don't have to

00:04:36,190 --> 00:04:40,360
understand you know any ad-hoc API for

00:04:38,380 --> 00:04:42,340
you know some rest service or whatever

00:04:40,360 --> 00:04:44,319
that's probably going too far but

00:04:42,340 --> 00:04:46,389
nonetheless it has that benefit and then

00:04:44,319 --> 00:04:48,370
the last one is this resiliency argument

00:04:46,389 --> 00:04:50,259
that you know a service one crashes now

00:04:48,370 --> 00:04:53,680
then I'll have data accumulating in

00:04:50,259 --> 00:04:55,330
Kafka you know reliably and durably the

00:04:53,680 --> 00:04:57,250
services on the Left don't have to know

00:04:55,330 --> 00:04:59,289
anything about this problem they can

00:04:57,250 --> 00:05:01,270
keep you know writing as always and then

00:04:59,289 --> 00:05:03,669
when service ones replacement comes up

00:05:01,270 --> 00:05:06,190
it picks up where it left off I can also

00:05:03,669 --> 00:05:08,530
scale these things independently if

00:05:06,190 --> 00:05:10,780
service one is getting overwhelmed maybe

00:05:08,530 --> 00:05:12,400
I can spin up another instance of it to

00:05:10,780 --> 00:05:17,530
you know balance the load that sort of

00:05:12,400 --> 00:05:19,090
thing okay so to summarize it simplifies

00:05:17,530 --> 00:05:21,159
our dependencies it gives us some

00:05:19,090 --> 00:05:22,330
resilience against data loss so you know

00:05:21,159 --> 00:05:24,430
we can have an arbitrary number of

00:05:22,330 --> 00:05:26,740
producers and consumers and we do it

00:05:24,430 --> 00:05:30,520
kind of a simple uniform way of gluing

00:05:26,740 --> 00:05:32,800
everything together but you know that's

00:05:30,520 --> 00:05:34,690
it's sort of the analog of the database

00:05:32,800 --> 00:05:36,520
that you're writing stuff to in fact you

00:05:34,690 --> 00:05:38,020
know some people even talk about Kafka's

00:05:36,520 --> 00:05:39,729
kind of a database and sort of a

00:05:38,020 --> 00:05:41,979
streaming way instead of kind of the

00:05:39,729 --> 00:05:43,479
usual relational way but there's

00:05:41,979 --> 00:05:44,949
obviously stuff you need to do with the

00:05:43,479 --> 00:05:46,419
data once it's there and that's what the

00:05:44,949 --> 00:05:48,759
streaming engines are for what am I

00:05:46,419 --> 00:05:53,560
going to do with these records that I've

00:05:48,759 --> 00:05:55,300
captured now in that report I mentioned

00:05:53,560 --> 00:05:56,979
we actually talked about four of them

00:05:55,300 --> 00:05:58,449
we're really five if you include beam

00:05:56,979 --> 00:06:01,479
and I'll ignore that one for right now

00:05:58,449 --> 00:06:03,639
but I want to contrast with two others

00:06:01,479 --> 00:06:07,330
that you might use first before coming

00:06:03,639 --> 00:06:11,440
back to Kafka and akka and that is spark

00:06:07,330 --> 00:06:13,360
and flink these are basically services

00:06:11,440 --> 00:06:14,800
that you run in your environment at

00:06:13,360 --> 00:06:16,569
least the normal way you deploy them in

00:06:14,800 --> 00:06:18,729
systems like a doop is you have these

00:06:16,569 --> 00:06:20,380
demons running around the cluster and

00:06:18,729 --> 00:06:23,500
you submit work to them and they figure

00:06:20,380 --> 00:06:26,530
out how to break up that work into tasks

00:06:23,500 --> 00:06:28,539
they do all the parallelism for you they

00:06:26,530 --> 00:06:30,909
manage those tasks if anything fails

00:06:28,539 --> 00:06:32,979
they restart them and so forth flink

00:06:30,909 --> 00:06:35,050
works in a very similar way they just do

00:06:32,979 --> 00:06:37,270
a lot of heavy lifting for you and they

00:06:35,050 --> 00:06:39,219
do a uh they do a great job of handling

00:06:37,270 --> 00:06:41,199
very large data sets where you really do

00:06:39,219 --> 00:06:43,390
need the parallelism of a cluster in

00:06:41,199 --> 00:06:45,460
order to to partition them

00:06:43,390 --> 00:06:46,360
and so they're really good systems and

00:06:45,460 --> 00:06:49,000
you know there's just a whole lot going

00:06:46,360 --> 00:06:51,550
on when you submit a job to spark and

00:06:49,000 --> 00:06:53,170
flink is very similar the program that

00:06:51,550 --> 00:06:55,180
you write is called a driver typically

00:06:53,170 --> 00:06:56,710
that's like the main process it's gonna

00:06:55,180 --> 00:06:58,930
run and kind of orchestrate everything

00:06:56,710 --> 00:07:01,570
and then it talks to some cluster

00:06:58,930 --> 00:07:04,510
manager process that knows how to split

00:07:01,570 --> 00:07:05,800
things up into these individual tasks

00:07:04,510 --> 00:07:08,500
that are running under the supervision

00:07:05,800 --> 00:07:10,390
of an executor on each node and on and

00:07:08,500 --> 00:07:12,760
on and on and the other thing you get

00:07:10,390 --> 00:07:14,950
with this is this automatic partitioning

00:07:12,760 --> 00:07:18,220
you also get this pipelining effect so

00:07:14,950 --> 00:07:21,070
if I do you know you read this data set

00:07:18,220 --> 00:07:22,270
and then map and filter and in flat map

00:07:21,070 --> 00:07:24,670
well all of that can be pipelined

00:07:22,270 --> 00:07:26,980
together without running separate tasks

00:07:24,670 --> 00:07:28,810
for each of those steps but then if I do

00:07:26,980 --> 00:07:30,790
a group buyer join well that might

00:07:28,810 --> 00:07:33,130
require data to be shuffled over a

00:07:30,790 --> 00:07:35,560
cluster and it handles all that and then

00:07:33,130 --> 00:07:37,090
starts the next pipeline and so forth so

00:07:35,560 --> 00:07:39,070
just a whole lot going on under the hood

00:07:37,090 --> 00:07:40,600
and it might immediately raise the

00:07:39,070 --> 00:07:42,820
question well you know why wouldn't I

00:07:40,600 --> 00:07:44,470
use this if it turns out I'm not going

00:07:42,820 --> 00:07:46,900
to get a lot of that kind of automatic

00:07:44,470 --> 00:07:48,730
stuff with these other toolkits and the

00:07:46,900 --> 00:07:50,350
reason is that they do impose a fair

00:07:48,730 --> 00:07:53,080
amount of overhead they do have higher

00:07:50,350 --> 00:07:55,630
latency than tools like akka streams and

00:07:53,080 --> 00:07:57,190
Kafka streams and maybe they just don't

00:07:55,630 --> 00:07:59,320
fit the model that you're working and

00:07:57,190 --> 00:08:01,480
maybe you want to glue lots of other

00:07:59,320 --> 00:08:03,550
kinds of code into that streaming

00:08:01,480 --> 00:08:05,770
pipeline that you're writing and it's a

00:08:03,550 --> 00:08:07,810
little harder to do that with oh sorry

00:08:05,770 --> 00:08:09,760
with spark and flink so there are times

00:08:07,810 --> 00:08:12,370
when I think you really do want micro

00:08:09,760 --> 00:08:14,410
services doing your data processing and

00:08:12,370 --> 00:08:16,690
not these kind of you know let's call

00:08:14,410 --> 00:08:18,280
them big ticket systems like this but

00:08:16,690 --> 00:08:20,530
you know at all it'll all depend on what

00:08:18,280 --> 00:08:21,580
you're kind of trying to do actually

00:08:20,530 --> 00:08:23,230
there's another reason it's worth

00:08:21,580 --> 00:08:26,260
mentioning we're I think that the

00:08:23,230 --> 00:08:28,750
balance might tip towards microservices

00:08:26,260 --> 00:08:29,920
over the long term and that is that

00:08:28,750 --> 00:08:31,510
normally when you're doing stream

00:08:29,920 --> 00:08:33,760
processing you're actually working with

00:08:31,510 --> 00:08:36,130
a little bit of data at a time that's

00:08:33,760 --> 00:08:38,650
not always the case you might be joining

00:08:36,130 --> 00:08:40,960
in legacy data historical data and so

00:08:38,650 --> 00:08:43,090
forth but the chances that you actually

00:08:40,960 --> 00:08:46,390
need the parallelism that you're getting

00:08:43,090 --> 00:08:48,130
on this diagram diminishes I think you

00:08:46,390 --> 00:08:49,690
know if you averaged over all the jobs

00:08:48,130 --> 00:08:51,730
you're writing when you're doing things

00:08:49,690 --> 00:08:53,280
a little bit at a time versus capturing

00:08:51,730 --> 00:08:55,210
like data for a week and then suddenly

00:08:53,280 --> 00:08:57,070
processing it all at once

00:08:55,210 --> 00:08:59,440
so that also makes it easier

00:08:57,070 --> 00:09:01,150
do a micro-service approach and the

00:08:59,440 --> 00:09:02,860
final thing I mentioned is that if you

00:09:01,150 --> 00:09:04,720
really know how to run microservice as

00:09:02,860 --> 00:09:07,180
well you've got everything tuned for

00:09:04,720 --> 00:09:09,700
monitoring and management deployment and

00:09:07,180 --> 00:09:12,160
you know your CI CD pipeline and all

00:09:09,700 --> 00:09:14,770
that stuff it's nice to keep using that

00:09:12,160 --> 00:09:16,450
model for everything even with you know

00:09:14,770 --> 00:09:16,900
even if it happens that spark would be

00:09:16,450 --> 00:09:18,550
fine

00:09:16,900 --> 00:09:23,050
let's say but it doesn't quite fit into

00:09:18,550 --> 00:09:24,520
that infrastructure okay so instead

00:09:23,050 --> 00:09:26,320
we'll talk about these streaming

00:09:24,520 --> 00:09:27,910
alternatives and these are really

00:09:26,320 --> 00:09:29,530
libraries that you embed in your

00:09:27,910 --> 00:09:33,220
micro-services whether you've written

00:09:29,530 --> 00:09:34,950
them with akka or orgo or node or well

00:09:33,220 --> 00:09:37,090
actually known to be a little tricky

00:09:34,950 --> 00:09:39,100
actually go with two because these are

00:09:37,090 --> 00:09:41,110
JVM processes but the point being that

00:09:39,100 --> 00:09:43,300
they are designed as libraries that you

00:09:41,110 --> 00:09:46,210
embed in the system they are not systems

00:09:43,300 --> 00:09:51,040
that you throw work at like the other

00:09:46,210 --> 00:09:55,420
tools okay Micra service all the things

00:09:51,040 --> 00:09:57,160
well you know we use Microsoft in C in

00:09:55,420 --> 00:09:59,440
our system and if we want to add latency

00:09:57,160 --> 00:10:01,330
or you know when we don't like the

00:09:59,440 --> 00:10:04,320
reliability of function calls you know

00:10:01,330 --> 00:10:08,140
we'd like things to be less reliable and

00:10:04,320 --> 00:10:10,030
if you accept that argument then let's

00:10:08,140 --> 00:10:12,310
talk about micro services now this

00:10:10,030 --> 00:10:14,830
diagram I came up with to kind of set

00:10:12,310 --> 00:10:16,990
the stage for understanding sort of the

00:10:14,830 --> 00:10:20,080
world view of akka streams versus the

00:10:16,990 --> 00:10:21,730
world view of kafka streams and there's

00:10:20,080 --> 00:10:24,700
really kind of a spectrum of Micra

00:10:21,730 --> 00:10:26,200
services in this context the kinds that

00:10:24,700 --> 00:10:27,580
we're used to writing I think are more

00:10:26,200 --> 00:10:30,190
on the left side they're more about

00:10:27,580 --> 00:10:32,620
event processing you know like login

00:10:30,190 --> 00:10:35,530
user you know authorize credit card add

00:10:32,620 --> 00:10:37,300
item to cart things that sort of implies

00:10:35,530 --> 00:10:38,680
state transitions I'm going to do

00:10:37,300 --> 00:10:40,990
something unique with each of these

00:10:38,680 --> 00:10:42,790
events you know they affect a particular

00:10:40,990 --> 00:10:45,190
customer and I may route them in

00:10:42,790 --> 00:10:47,320
different ways and so forth whereas on

00:10:45,190 --> 00:10:48,610
the right hand side I'm arguing that on

00:10:47,320 --> 00:10:50,080
the other side of the spectrum are

00:10:48,610 --> 00:10:52,600
things that actually are a little more

00:10:50,080 --> 00:10:54,460
anonymous they're more like records of

00:10:52,600 --> 00:10:56,050
data that I'm shoving through the system

00:10:54,460 --> 00:11:01,300
and I can process them a little bit more

00:10:56,050 --> 00:11:02,800
and mass or at least conceptually so you

00:11:01,300 --> 00:11:04,810
know each item has an identity when

00:11:02,800 --> 00:11:05,920
we're talking about an events you know

00:11:04,810 --> 00:11:07,780
the difference between an event the

00:11:05,920 --> 00:11:09,670
message is a message has a sender and

00:11:07,780 --> 00:11:10,279
receiver but it could be encapsulating

00:11:09,670 --> 00:11:13,879
an event

00:11:10,279 --> 00:11:15,589
I often process them uniquely at least

00:11:13,879 --> 00:11:17,810
logically I think about this I might do

00:11:15,589 --> 00:11:20,240
some sort of bulk processing for

00:11:17,810 --> 00:11:22,069
efficiency and often they're used to

00:11:20,240 --> 00:11:26,240
drive session the state machines and

00:11:22,069 --> 00:11:30,889
that sort of thing whereas in the other

00:11:26,240 --> 00:11:32,480
case if this works there goes here I'm

00:11:30,889 --> 00:11:33,949
thinking I'm just gonna shove a lot of

00:11:32,480 --> 00:11:36,170
stuff through I'm gonna do something

00:11:33,949 --> 00:11:39,379
like a sequel query you know group by

00:11:36,170 --> 00:11:42,620
this filter or you know select these

00:11:39,379 --> 00:11:45,860
fields when this clause holds and or

00:11:42,620 --> 00:11:48,559
where cause that sort of thing so I

00:11:45,860 --> 00:11:50,809
think that's important because to me

00:11:48,559 --> 00:11:52,249
acha streams emerged out of the left

00:11:50,809 --> 00:11:54,680
hand side you know it started as this

00:11:52,249 --> 00:11:56,749
great micro service toolkit and then

00:11:54,680 --> 00:11:59,120
akka streams is this you know streaming

00:11:56,749 --> 00:12:03,740
DSL on top of it but it really has that

00:11:59,120 --> 00:12:06,079
mindset of I'm processing events and I'm

00:12:03,740 --> 00:12:08,120
also I need very fine-grained control

00:12:06,079 --> 00:12:09,949
over what I'm doing with this data if

00:12:08,120 --> 00:12:11,600
you went to Collins talked before this

00:12:09,949 --> 00:12:13,459
one he went into a lot of detail about

00:12:11,600 --> 00:12:16,069
some of the cool things you can do with

00:12:13,459 --> 00:12:17,959
akka streams and a lot of it is you know

00:12:16,069 --> 00:12:19,129
a really powerful little operator so you

00:12:17,959 --> 00:12:20,899
can apply to your data

00:12:19,129 --> 00:12:23,540
you know like send the error messages

00:12:20,899 --> 00:12:25,339
this way and you know group things up

00:12:23,540 --> 00:12:27,290
for efficiency and then send them in

00:12:25,339 --> 00:12:28,910
bulk and you handle timeouts and all

00:12:27,290 --> 00:12:30,439
this kind of stuff the sort of thing

00:12:28,910 --> 00:12:32,540
that you really need when you're really

00:12:30,439 --> 00:12:34,879
have tight control over your environment

00:12:32,540 --> 00:12:39,139
and they'd very fine-grained control

00:12:34,879 --> 00:12:40,970
over what's going on whereas Kafka

00:12:39,139 --> 00:12:44,420
streams really emerged out of the micro

00:12:40,970 --> 00:12:46,399
the data centric world in my opinion now

00:12:44,420 --> 00:12:48,529
if you went to Neos keynote yesterday

00:12:46,399 --> 00:12:50,600
Nia so you know one of the early

00:12:48,529 --> 00:12:52,430
engineers on Kafka you know they do talk

00:12:50,600 --> 00:12:54,439
a lot about you know event sourcing

00:12:52,430 --> 00:12:56,509
event processing micro services in the

00:12:54,439 --> 00:12:58,639
same way and you can certainly use Kafka

00:12:56,509 --> 00:13:00,559
streams for all that but if you look at

00:12:58,639 --> 00:13:03,860
the kind of things that they give you as

00:13:00,559 --> 00:13:05,929
easy to use tools to me they really fit

00:13:03,860 --> 00:13:07,939
the model of we're processing data where

00:13:05,929 --> 00:13:09,949
they give you like a sequel query engine

00:13:07,939 --> 00:13:12,079
now and things like that

00:13:09,949 --> 00:13:15,499
so I think that they fit better on the

00:13:12,079 --> 00:13:16,999
right-hand side so let's dive into those

00:13:15,499 --> 00:13:20,410
these two toolkits first we'll talk

00:13:16,999 --> 00:13:20,410
about Kafka streams a little bit

00:13:21,970 --> 00:13:25,389
so some of the things that they give you

00:13:24,039 --> 00:13:27,639
that become important when you're doing

00:13:25,389 --> 00:13:30,579
like aggregations like I want to average

00:13:27,639 --> 00:13:32,919
what's happening over I don't know how

00:13:30,579 --> 00:13:36,220
much money am I making per hour how many

00:13:32,919 --> 00:13:38,859
items have I sold per purse Q per store

00:13:36,220 --> 00:13:42,279
in my online store that sort of thing so

00:13:38,859 --> 00:13:44,649
they have some important concepts like

00:13:42,279 --> 00:13:46,599
the difference between event time and

00:13:44,649 --> 00:13:48,399
processing time event time is what you

00:13:46,599 --> 00:13:50,470
really care about that's logical time

00:13:48,399 --> 00:13:52,749
that's when something really happened

00:13:50,470 --> 00:13:55,209
and against somebody's clock that

00:13:52,749 --> 00:13:56,949
matters whereas processing time happens

00:13:55,209 --> 00:13:59,529
to be that machine over here that's

00:13:56,949 --> 00:14:01,989
doing the analysis and it it could be

00:13:59,529 --> 00:14:03,669
you know in sync with event time on the

00:14:01,989 --> 00:14:06,220
other machines but realistically it's

00:14:03,669 --> 00:14:08,589
always off a little bit and it also it

00:14:06,220 --> 00:14:10,959
might be processing that this data you

00:14:08,589 --> 00:14:12,819
know days or maybe seconds or

00:14:10,959 --> 00:14:14,439
milliseconds after it actually happened

00:14:12,819 --> 00:14:16,449
and that's what the diagram is designed

00:14:14,439 --> 00:14:18,909
to show on the right if I was doing some

00:14:16,449 --> 00:14:21,789
sort of analysis every minute so you

00:14:18,909 --> 00:14:23,079
know statistics of any kind I can't just

00:14:21,789 --> 00:14:24,399
say all right I just hit the minute

00:14:23,079 --> 00:14:26,409
boundary I'm just gonna calculate

00:14:24,399 --> 00:14:29,529
whatever I've got from the last minute

00:14:26,409 --> 00:14:31,749
because I've got stuff arriving late

00:14:29,529 --> 00:14:33,789
stuff arriving at different times just

00:14:31,749 --> 00:14:36,459
because of normal time of flight and a

00:14:33,789 --> 00:14:38,470
network and also for worst case time of

00:14:36,459 --> 00:14:41,049
flight when I have like partitions where

00:14:38,470 --> 00:14:44,259
maybe I won't see the actual data for

00:14:41,049 --> 00:14:45,999
some machine you know hours later what

00:14:44,259 --> 00:14:47,559
do I do about all that so they've

00:14:45,999 --> 00:14:50,019
started thinking about how to handle

00:14:47,559 --> 00:14:52,509
these kinds of things going towards the

00:14:50,019 --> 00:14:55,739
model of giving you accurate analysis of

00:14:52,509 --> 00:14:58,599
data based on these sort of time windows

00:14:55,739 --> 00:15:00,519
the last bullet point this is actually a

00:14:58,599 --> 00:15:02,199
big subject and it's a very important

00:15:00,519 --> 00:15:04,509
subject when you get into more advanced

00:15:02,199 --> 00:15:06,579
streaming scenarios and I talked a

00:15:04,509 --> 00:15:08,199
little bit about it in that report and

00:15:06,579 --> 00:15:10,449
then reference them even better writing

00:15:08,199 --> 00:15:12,309
about it by some guys at Google for

00:15:10,449 --> 00:15:13,869
example Tyler a Cadell being one of them

00:15:12,309 --> 00:15:20,019
who's really gone into this in a lot of

00:15:13,869 --> 00:15:22,209
detail Kafka streams you know made a big

00:15:20,019 --> 00:15:23,949
splashy announcement about this time

00:15:22,209 --> 00:15:26,499
last year that they had implemented

00:15:23,949 --> 00:15:27,909
exactly once processing those of you who

00:15:26,499 --> 00:15:29,499
are pedants would know that that's

00:15:27,909 --> 00:15:31,959
impossible you can never actually do

00:15:29,499 --> 00:15:34,419
exactly once but you can if you set your

00:15:31,959 --> 00:15:37,179
boundaries at a certain level you know

00:15:34,419 --> 00:15:39,429
if you basically do at least once and

00:15:37,179 --> 00:15:42,069
then automatic deduplication then if you

00:15:39,429 --> 00:15:43,569
are exactly once as long as a meteor

00:15:42,069 --> 00:15:46,239
doesn't strike your data center that

00:15:43,569 --> 00:15:48,639
sort of thing so we like to say it's

00:15:46,239 --> 00:15:50,289
effectively once if we want to be really

00:15:48,639 --> 00:15:52,959
careful about it but it's logically

00:15:50,289 --> 00:15:54,220
exactly once and again just as a

00:15:52,959 --> 00:15:56,470
reminder although I think with this

00:15:54,220 --> 00:15:57,519
crowd you get it you know at most once

00:15:56,470 --> 00:15:59,639
is fire-and-forget

00:15:57,519 --> 00:16:01,749
you may never get there but nobody cares

00:15:59,639 --> 00:16:04,749
well you know you might do that with

00:16:01,749 --> 00:16:06,279
some telemetry some log messages at

00:16:04,749 --> 00:16:08,169
least once would be I'm going to keep

00:16:06,279 --> 00:16:10,119
trying until I get an acknowledgment but

00:16:08,169 --> 00:16:12,339
you may end up with multiple copies and

00:16:10,119 --> 00:16:15,429
then exactly once is this sort of a

00:16:12,339 --> 00:16:19,839
unicorn of you only see it once and you

00:16:15,429 --> 00:16:21,339
never miss anything now here's another

00:16:19,839 --> 00:16:23,309
example of where they've thought about

00:16:21,339 --> 00:16:25,779
the data problem they have this

00:16:23,309 --> 00:16:27,939
abstraction called a case dream which is

00:16:25,779 --> 00:16:29,559
really the normal stream that we've kind

00:16:27,939 --> 00:16:31,239
of been talking about with Kafka where I

00:16:29,559 --> 00:16:33,099
just want to see the sequence of events

00:16:31,239 --> 00:16:35,979
you know it's like like an event store

00:16:33,099 --> 00:16:37,509
kind of picture but maybe I don't really

00:16:35,979 --> 00:16:39,039
want to see the whole thing maybe I

00:16:37,509 --> 00:16:40,779
really want to see something more like a

00:16:39,039 --> 00:16:43,209
database I just want to see the last

00:16:40,779 --> 00:16:44,649
value for a key or I want to run up an

00:16:43,209 --> 00:16:47,019
aggregation that I'm going to keep

00:16:44,649 --> 00:16:49,269
updating in place the way we would do

00:16:47,019 --> 00:16:51,789
with a traditional database and they use

00:16:49,269 --> 00:16:52,989
the K table abstraction for this so

00:16:51,789 --> 00:16:54,549
these are really nice features they

00:16:52,989 --> 00:16:56,559
built in to make it easy to do these

00:16:54,549 --> 00:16:58,689
kind of typical things that we do when

00:16:56,559 --> 00:17:04,179
we're analyzing data without having to

00:16:58,689 --> 00:17:05,619
write a lot of boilerplate ourselves and

00:17:04,179 --> 00:17:08,199
but but there are some limitations

00:17:05,619 --> 00:17:10,990
though it Kafka streams only talks to

00:17:08,199 --> 00:17:13,899
Kafka topics or in memory so if you want

00:17:10,990 --> 00:17:16,029
to have it reading a rest interface or

00:17:13,899 --> 00:17:17,679
writing to rest or you know writing to a

00:17:16,029 --> 00:17:19,360
database or something you have to put in

00:17:17,679 --> 00:17:21,970
something else to do it some other

00:17:19,360 --> 00:17:24,069
library or whatever or just you could

00:17:21,970 --> 00:17:25,509
embed the logic in the stream code

00:17:24,069 --> 00:17:27,220
itself that's normally not what you

00:17:25,509 --> 00:17:28,899
really want to do it's really not

00:17:27,220 --> 00:17:30,669
designed to connect to the rest of the

00:17:28,899 --> 00:17:33,850
world in the same way that akka is

00:17:30,669 --> 00:17:35,950
designed to do this with Al pakka and

00:17:33,850 --> 00:17:38,409
the way you do scaling and load

00:17:35,950 --> 00:17:41,859
balancing is really by bipartition it's

00:17:38,409 --> 00:17:43,690
very centric towards concepts and Kafka

00:17:41,859 --> 00:17:45,900
like topics and partitions and that's

00:17:43,690 --> 00:17:48,720
how you scale

00:17:45,900 --> 00:17:51,750
there's a Java API light band actually

00:17:48,720 --> 00:17:53,970
contributed Scala API that's now in the

00:17:51,750 --> 00:17:55,830
project and I'll show an example of that

00:17:53,970 --> 00:17:57,780
in a little bit and as I said a minute

00:17:55,830 --> 00:18:00,660
ago there's even a sequel API so you can

00:17:57,780 --> 00:18:02,790
write sequel queries which is

00:18:00,660 --> 00:18:06,090
unbelievable but actually seek lists

00:18:02,790 --> 00:18:08,640
become like the favorite DSL in

00:18:06,090 --> 00:18:10,650
streaming like everything else turns out

00:18:08,640 --> 00:18:12,720
it's kind of a crazy thing that's

00:18:10,650 --> 00:18:14,430
happening so anyway let's look at an

00:18:12,720 --> 00:18:16,080
example this is sort of a sketch of what

00:18:14,430 --> 00:18:18,300
it is and because we're talking kafka

00:18:16,080 --> 00:18:20,700
streams every little bit of logic will

00:18:18,300 --> 00:18:22,830
read and write Kafka topics in this case

00:18:20,700 --> 00:18:24,809
I'm going to assume I'm ingesting a Ross

00:18:22,830 --> 00:18:27,000
stream of data and I'm going to send it

00:18:24,809 --> 00:18:28,890
into two separate places first I want to

00:18:27,000 --> 00:18:30,720
send it to some machine learning model

00:18:28,890 --> 00:18:33,059
training you maybe I'm looking for

00:18:30,720 --> 00:18:34,620
anomalies or I'm gonna you know do

00:18:33,059 --> 00:18:37,230
recommendations or something like that

00:18:34,620 --> 00:18:38,940
so that'll be one pipeline and then out

00:18:37,230 --> 00:18:41,309
of that model training I'll get periodic

00:18:38,940 --> 00:18:43,920
updates to the model so that it stays

00:18:41,309 --> 00:18:46,200
current and I'm gonna ingest those new

00:18:43,920 --> 00:18:48,390
model parameters and the raw data into

00:18:46,200 --> 00:18:50,700
this model serving bit at the top and

00:18:48,390 --> 00:18:52,920
that would be you know somehow I'll have

00:18:50,700 --> 00:18:55,050
this process that updates my model when

00:18:52,920 --> 00:18:57,000
new parameters come in and then whatever

00:18:55,050 --> 00:18:59,610
happens to be the current model I'll

00:18:57,000 --> 00:19:02,610
score data as it comes through like make

00:18:59,610 --> 00:19:04,770
recommendations flag at a spam whatever

00:19:02,610 --> 00:19:06,870
it is I'm doing and then I can send that

00:19:04,770 --> 00:19:08,520
downstream the scored records and do

00:19:06,870 --> 00:19:10,590
other stuff which I won't really go into

00:19:08,520 --> 00:19:15,090
but just to show you what it might look

00:19:10,590 --> 00:19:17,370
like so I am gonna use this new Scala

00:19:15,090 --> 00:19:18,720
API that light band wrote here's just a

00:19:17,370 --> 00:19:21,330
bunch of details several of my

00:19:18,720 --> 00:19:24,480
colleagues worked on this and you can

00:19:21,330 --> 00:19:26,010
there's also kind of a neat API that we

00:19:24,480 --> 00:19:28,140
we didn't contribute it it doesn't

00:19:26,010 --> 00:19:30,870
really fit into the Kafka project but if

00:19:28,140 --> 00:19:33,630
you use the query ability to ask stuff

00:19:30,870 --> 00:19:36,540
of state you can actually query a stream

00:19:33,630 --> 00:19:39,059
for its state while it's running then

00:19:36,540 --> 00:19:43,380
that this link at the bottom is a tool

00:19:39,059 --> 00:19:46,260
to make that a little easier to do okay

00:19:43,380 --> 00:19:48,450
and the example itself is taken from a

00:19:46,260 --> 00:19:50,220
tutorial that we have on github if

00:19:48,450 --> 00:19:51,960
you're interested in playing around with

00:19:50,220 --> 00:19:54,840
this stuff in more detail obviously I

00:19:51,960 --> 00:19:58,650
have to greatly simplify the code for a

00:19:54,840 --> 00:19:59,710
short talk okay it's a lot of code here

00:19:58,650 --> 00:20:01,600
and I'm just going to walk

00:19:59,710 --> 00:20:03,010
through it carefully well not too

00:20:01,600 --> 00:20:04,720
carefully I'm actually gonna skip over

00:20:03,010 --> 00:20:06,190
some details that aren't that terribly

00:20:04,720 --> 00:20:07,990
important but I'm just a sort of

00:20:06,190 --> 00:20:10,720
hand-wave about some of those at first

00:20:07,990 --> 00:20:13,030
here I'm gonna assume that I'm gonna

00:20:10,720 --> 00:20:15,130
read some Kafka topic called a raw data

00:20:13,030 --> 00:20:17,260
topic and there's the usual thing you do

00:20:15,130 --> 00:20:19,090
to configure where that is you talk to a

00:20:17,260 --> 00:20:20,370
Kafka broker and it tells you where to

00:20:19,090 --> 00:20:24,070
look for it and all that kind of stuff

00:20:20,370 --> 00:20:25,900
we won't go into those details I'm gonna

00:20:24,070 --> 00:20:27,610
need a couple of other topics like the

00:20:25,900 --> 00:20:29,380
the model topic which is where I'll

00:20:27,610 --> 00:20:31,960
write model parameters that when I've

00:20:29,380 --> 00:20:33,670
trained a new model and so forth I'm

00:20:31,960 --> 00:20:35,830
gonna need something that handles those

00:20:33,670 --> 00:20:37,660
updates to the model and it's not

00:20:35,830 --> 00:20:39,460
terribly complicated logic you know

00:20:37,660 --> 00:20:41,500
depends on what toolkit I'm using but

00:20:39,460 --> 00:20:44,020
we'll just abstract over that idea as

00:20:41,500 --> 00:20:45,820
well here the next thing I'll have is

00:20:44,020 --> 00:20:48,130
this model which will be the thing that

00:20:45,820 --> 00:20:49,720
manages so you just sent me a new bunch

00:20:48,130 --> 00:20:51,880
of parameters and now I need to figure

00:20:49,720 --> 00:20:54,190
out what the what kind of model you want

00:20:51,880 --> 00:20:56,560
me to implement I'll spin up an

00:20:54,190 --> 00:20:58,900
implementation of that model and then

00:20:56,560 --> 00:21:01,050
put it into the stream and then we'll

00:20:58,900 --> 00:21:02,860
have a notion of some data record that's

00:21:01,050 --> 00:21:04,420
encapsulate so whatever it is we're

00:21:02,860 --> 00:21:06,700
actually scoring you know whether it's

00:21:04,420 --> 00:21:09,060
credit card transactions or you know

00:21:06,700 --> 00:21:11,620
people surfing on a my website whatever

00:21:09,060 --> 00:21:15,100
and then scored records would just add

00:21:11,620 --> 00:21:18,130
whatever the score is and then finally

00:21:15,100 --> 00:21:20,050
at the bottom in the red box is you know

00:21:18,130 --> 00:21:22,150
just some sort of configuration object

00:21:20,050 --> 00:21:23,740
that you is required in Kafka stream so

00:21:22,150 --> 00:21:25,660
that tells you things like here's where

00:21:23,740 --> 00:21:28,810
you can find the brokers for Kafka those

00:21:25,660 --> 00:21:30,370
are the kind of the master processes you

00:21:28,810 --> 00:21:32,530
know here's the topics I want to use and

00:21:30,370 --> 00:21:34,420
so forth alright so I'm skipping over

00:21:32,530 --> 00:21:36,580
all those details to get to the gist of

00:21:34,420 --> 00:21:39,520
what it's what life is like writing a

00:21:36,580 --> 00:21:40,930
Kafka streams app you would start with

00:21:39,520 --> 00:21:42,790
this stream builder that's like your

00:21:40,930 --> 00:21:45,250
entry point and now the S at the end is

00:21:42,790 --> 00:21:47,680
actually the scala wrapper addition to

00:21:45,250 --> 00:21:49,780
this for the most part the API that we

00:21:47,680 --> 00:21:52,510
wrote we tried very hard to make it just

00:21:49,780 --> 00:21:55,180
a thin veneer over the Java API so that

00:21:52,510 --> 00:21:57,460
conceptually if you know the Java API

00:21:55,180 --> 00:21:59,410
you can easily write with a Scala API

00:21:57,460 --> 00:22:01,540
there's no new concepts added and so

00:21:59,410 --> 00:22:05,220
forth so it's just a very simple change

00:22:01,540 --> 00:22:07,900
at the top to start off with a Scala API

00:22:05,220 --> 00:22:10,270
then you ask the Builder to create two

00:22:07,900 --> 00:22:12,220
kinds of streams for you in this case

00:22:10,270 --> 00:22:13,480
we're going to be reading and writing

00:22:12,220 --> 00:22:15,250
bytes of array

00:22:13,480 --> 00:22:18,309
and just you know it's kind of to take

00:22:15,250 --> 00:22:20,620
the worst case Scala or sorry Kafka

00:22:18,309 --> 00:22:23,620
topics generally speaking are just big

00:22:20,620 --> 00:22:26,110
blobs of bytes there are ways you could

00:22:23,620 --> 00:22:27,610
layer schema on top of them but we're

00:22:26,110 --> 00:22:28,750
just going to go with the worst case and

00:22:27,610 --> 00:22:30,220
manage it ourselves

00:22:28,750 --> 00:22:32,559
so we're going to need two of these

00:22:30,220 --> 00:22:35,290
streams one of which will be the raw

00:22:32,559 --> 00:22:36,760
data that again from that way I guess I

00:22:35,290 --> 00:22:38,679
put the diagram in the bottom that'll be

00:22:36,760 --> 00:22:44,770
the first gray box and then also I'll

00:22:38,679 --> 00:22:46,360
need a stream for my model topics here's

00:22:44,770 --> 00:22:48,520
something again it's going to process

00:22:46,360 --> 00:22:50,770
updates to the model you know we're

00:22:48,520 --> 00:22:52,660
skipping over the details there and and

00:22:50,770 --> 00:22:54,580
similarly I'm gonna have some object

00:22:52,660 --> 00:22:57,070
that encapsulates taking that model

00:22:54,580 --> 00:22:58,720
taking a record and then applying a

00:22:57,070 --> 00:23:02,880
score you know giving me a

00:22:58,720 --> 00:23:05,230
recommendation marking it as spam etc

00:23:02,880 --> 00:23:06,760
and then this is code that probably

00:23:05,230 --> 00:23:08,890
looks a lot like what you're used to if

00:23:06,760 --> 00:23:10,750
you've written any kind of collections

00:23:08,890 --> 00:23:12,910
API code and Scala if you've written

00:23:10,750 --> 00:23:16,240
SPARC code it's very similar etcetera

00:23:12,910 --> 00:23:17,799
where I'm going to start with the model

00:23:16,240 --> 00:23:19,450
that I'm Bill the model stream that I

00:23:17,799 --> 00:23:21,250
built I'm gonna map over the input

00:23:19,450 --> 00:23:25,030
values and what I'm gonna do in this

00:23:21,250 --> 00:23:27,610
first step is parse those bytes into a

00:23:25,030 --> 00:23:29,950
new model and this is actually going to

00:23:27,610 --> 00:23:31,690
return let's see what is it return

00:23:29,950 --> 00:23:34,150
actually oh it returns a record of this

00:23:31,690 --> 00:23:36,160
stuff then I'm going to filter over the

00:23:34,150 --> 00:23:37,720
ones where I successfully parsed into

00:23:36,160 --> 00:23:39,580
the model so you know put in a little

00:23:37,720 --> 00:23:42,250
logic to handle that

00:23:39,580 --> 00:23:45,220
then I'm going to map over those values

00:23:42,250 --> 00:23:47,309
to take the the model parameter set and

00:23:45,220 --> 00:23:50,169
turn it into an actual implementation

00:23:47,309 --> 00:23:52,570
and then the last step is where I'm

00:23:50,169 --> 00:23:54,970
gonna actually process this to set up

00:23:52,570 --> 00:23:57,040
the model in memory so that when I do

00:23:54,970 --> 00:23:59,020
what's sort of like a join later on

00:23:57,040 --> 00:24:03,010
it'll just I'll have my new memory or

00:23:59,020 --> 00:24:04,780
new model in memory the data processing

00:24:03,010 --> 00:24:06,700
stream is very similar it's a little

00:24:04,780 --> 00:24:08,559
simpler in some ways again I've got raw

00:24:06,700 --> 00:24:10,780
bytes coming in I'm gonna have something

00:24:08,559 --> 00:24:13,299
that knows how to parse those bytes I'm

00:24:10,780 --> 00:24:16,179
gonna look for success or failure on

00:24:13,299 --> 00:24:17,590
that process and then if it's if I

00:24:16,179 --> 00:24:19,960
succeeded then I'm gonna take each

00:24:17,590 --> 00:24:22,299
record and I'm going to create a new

00:24:19,960 --> 00:24:24,400
scored record by calling my scoring

00:24:22,299 --> 00:24:26,110
object that we saw towards the top and

00:24:24,400 --> 00:24:28,030
just you know give me a record give me a

00:24:26,110 --> 00:24:29,650
result for this thing

00:24:28,030 --> 00:24:31,570
and then the last line is where I'm

00:24:29,650 --> 00:24:33,520
gonna actually send it to the output to

00:24:31,570 --> 00:24:37,500
is going to write to yet another topic

00:24:33,520 --> 00:24:39,670
in Kafka where I send these results to

00:24:37,500 --> 00:24:41,530
and then the last thing you do you

00:24:39,670 --> 00:24:44,050
actually build up a topology so you call

00:24:41,530 --> 00:24:46,450
this Kafka streams object to take the

00:24:44,050 --> 00:24:48,190
builder to construct a topology of stuff

00:24:46,450 --> 00:24:50,470
in memory that'll actually do this you

00:24:48,190 --> 00:24:52,120
pass it this config object you start it

00:24:50,470 --> 00:24:54,340
running and then you can add a shutdown

00:24:52,120 --> 00:24:56,890
hook if you want to clean everything up

00:24:54,340 --> 00:24:58,750
when you're done so this code I think is

00:24:56,890 --> 00:25:00,460
conceptually pretty simple you know we

00:24:58,750 --> 00:25:02,830
there's a little bit of fluff here and

00:25:00,460 --> 00:25:05,440
the way that the tutorial like handles

00:25:02,830 --> 00:25:07,630
bytes and parses things into models and

00:25:05,440 --> 00:25:09,430
so forth but not too difficult hopefully

00:25:07,630 --> 00:25:11,890
it's it's reasonably comprehensive all

00:25:09,430 --> 00:25:16,060
if you've ever written any Scala a

00:25:11,890 --> 00:25:17,680
collections code before but there are

00:25:16,060 --> 00:25:19,690
some things missing actually and that is

00:25:17,680 --> 00:25:22,870
that the library is doing a really nice

00:25:19,690 --> 00:25:24,640
job talking to to Kafka you know giving

00:25:22,870 --> 00:25:26,680
us these processing primitives

00:25:24,640 --> 00:25:28,660
maybe exactly once if we care about that

00:25:26,680 --> 00:25:30,760
as we said but boy there's a whole lot

00:25:28,660 --> 00:25:32,800
left to do right to actually run micro

00:25:30,760 --> 00:25:34,720
services so you're gonna embed this and

00:25:32,800 --> 00:25:36,190
whatever your favorite library is which

00:25:34,720 --> 00:25:38,650
you know probably is the reactive

00:25:36,190 --> 00:25:41,260
platform from wideband I'm guessing but

00:25:38,650 --> 00:25:43,270
if even if you like another JVM based

00:25:41,260 --> 00:25:45,070
library you would just embed this the

00:25:43,270 --> 00:25:47,110
stream code into that to run your micro

00:25:45,070 --> 00:25:50,500
services and we'll come back to some

00:25:47,110 --> 00:25:51,910
production concerns in a little bit but

00:25:50,500 --> 00:25:57,970
let's go let's now talk about akka

00:25:51,910 --> 00:25:59,860
streams I'm sort of preaching the crowd

00:25:57,970 --> 00:26:01,830
here a little bit or choir rather it

00:25:59,860 --> 00:26:04,630
does implement the reactive streams

00:26:01,830 --> 00:26:08,700
standard so that we get back pressure

00:26:04,630 --> 00:26:10,390
for Fleet for free for flow control okay

00:26:08,700 --> 00:26:12,370
and if you haven't heard of that

00:26:10,390 --> 00:26:14,710
basically the idea is that you have this

00:26:12,370 --> 00:26:16,630
back channel of communication so data is

00:26:14,710 --> 00:26:18,880
flowing left to right and through some

00:26:16,630 --> 00:26:20,650
sort of cue the problem with the queue

00:26:18,880 --> 00:26:22,990
is that if it fills up and it's a

00:26:20,650 --> 00:26:24,970
bounded queue then I have to throw data

00:26:22,990 --> 00:26:26,530
away on the left-hand side if it's an

00:26:24,970 --> 00:26:29,170
unbounded queue I eventually run out of

00:26:26,530 --> 00:26:30,850
memory if my consumers can't keep up so

00:26:29,170 --> 00:26:33,010
how do we solve this problem well we

00:26:30,850 --> 00:26:35,380
have this communication protocol of

00:26:33,010 --> 00:26:38,320
sending information from the consumer

00:26:35,380 --> 00:26:40,270
back upstream to the producer to say I

00:26:38,320 --> 00:26:40,840
can take as much as you can send me

00:26:40,270 --> 00:26:42,400
right now

00:26:40,840 --> 00:26:44,919
it's great just push it as fast as you

00:26:42,400 --> 00:26:46,419
want or you know I'm actually kind of

00:26:44,919 --> 00:26:48,880
slow up here why don't you just send

00:26:46,419 --> 00:26:50,919
them one at a time I'll actually pull so

00:26:48,880 --> 00:26:54,250
he'd actually talk toggles between those

00:26:50,919 --> 00:26:56,710
kind of operations push and pull but it

00:26:54,250 --> 00:26:58,870
means that you can not have to worry

00:26:56,710 --> 00:27:00,309
about having something blow up in the

00:26:58,870 --> 00:27:02,260
middle which is really the key thing is

00:27:00,309 --> 00:27:04,090
it's really a simple idea actually but

00:27:02,260 --> 00:27:07,090
it had never been done before at least

00:27:04,090 --> 00:27:08,710
in our context and it is really powerful

00:27:07,090 --> 00:27:11,169
as a way of building things that

00:27:08,710 --> 00:27:13,149
actually compose because back pressure

00:27:11,169 --> 00:27:14,860
you know propagates through the whole

00:27:13,149 --> 00:27:16,539
system so you can have an entire network

00:27:14,860 --> 00:27:18,610
of these things that won't kill each

00:27:16,539 --> 00:27:20,289
other by you know you know overwhelming

00:27:18,610 --> 00:27:22,330
themselves and then you can make

00:27:20,289 --> 00:27:23,770
strategic decisions so instead of some

00:27:22,330 --> 00:27:25,779
guy in the middle having to make the

00:27:23,770 --> 00:27:28,659
decision about what do I do when I can't

00:27:25,779 --> 00:27:30,940
push any more data in this Q instead

00:27:28,659 --> 00:27:32,830
somebody at the beginning can decide in

00:27:30,940 --> 00:27:34,960
a strategic way all right this is what

00:27:32,830 --> 00:27:36,820
we're gonna do if we run it if we can't

00:27:34,960 --> 00:27:40,000
keep up maybe we'll spin up a whole new

00:27:36,820 --> 00:27:41,799
copy of topology maybe we'll just throw

00:27:40,000 --> 00:27:44,169
data away maybe we'll write data to a

00:27:41,799 --> 00:27:47,710
file and feed it in later we can make

00:27:44,169 --> 00:27:49,419
those decisions strategically so we get

00:27:47,710 --> 00:27:51,429
all this with acha streams because it

00:27:49,419 --> 00:27:53,409
implements this it is part of this big

00:27:51,429 --> 00:27:54,909
akka ecosystem so all the things I just

00:27:53,409 --> 00:27:56,409
mentioned about how am I actually going

00:27:54,909 --> 00:27:58,270
to run this in production when I do all

00:27:56,409 --> 00:28:01,120
these other concerns you know there's a

00:27:58,270 --> 00:28:03,340
whole lot of stuff that we can do even

00:28:01,120 --> 00:28:06,159
if you're using the Kafka stream API on

00:28:03,340 --> 00:28:09,100
top of akka and so forth you can use all

00:28:06,159 --> 00:28:10,990
of these tools for HTTP for persistence

00:28:09,100 --> 00:28:13,090
and so forth and in particular I

00:28:10,990 --> 00:28:15,370
mentioned Kafka streams can only read

00:28:13,090 --> 00:28:17,830
and write coffee topics well with a

00:28:15,370 --> 00:28:23,289
library like al pakka you can connect

00:28:17,830 --> 00:28:26,350
almost anything you want so here's the

00:28:23,289 --> 00:28:27,850
gist of anaka streams app I think I

00:28:26,350 --> 00:28:29,470
don't think anybody a light band would

00:28:27,850 --> 00:28:31,659
disagree with me we hadn't done a great

00:28:29,470 --> 00:28:34,600
job giving you like really easy here's

00:28:31,659 --> 00:28:36,039
the gist kind of documentation so I sort

00:28:34,600 --> 00:28:37,720
of created a little example that I

00:28:36,039 --> 00:28:39,549
thought would at least give you kind of

00:28:37,720 --> 00:28:41,440
the bare-bones minimum thing that you

00:28:39,549 --> 00:28:44,110
would have to write so let's walk

00:28:41,440 --> 00:28:47,049
through this a little bit so you always

00:28:44,110 --> 00:28:48,640
start with a source of some kind here

00:28:47,049 --> 00:28:52,539
I'm just going to do a source of 10

00:28:48,640 --> 00:28:54,280
integers from 1 to 10 and I'm going to

00:28:52,539 --> 00:28:54,580
define some flow that will operate on

00:28:54,280 --> 00:28:55,630
this

00:28:54,580 --> 00:28:57,789
source now I could have more than one

00:28:55,630 --> 00:28:59,500
source here and more than one sink if I

00:28:57,789 --> 00:29:01,350
want but again to keep it simple and

00:28:59,500 --> 00:29:04,240
what I'm going to do is scan my source

00:29:01,350 --> 00:29:06,159
I'm gonna if what scan is actually

00:29:04,240 --> 00:29:08,320
letting me do is accumulation I'm gonna

00:29:06,159 --> 00:29:10,419
start with a count of one and then I'm

00:29:08,320 --> 00:29:12,850
just going to multiply by each number

00:29:10,419 --> 00:29:15,460
and build up a total so it's basically

00:29:12,850 --> 00:29:17,769
like a fold left or something like that

00:29:15,460 --> 00:29:19,659
for those of you pedants out there

00:29:17,769 --> 00:29:20,980
anyway so that's what I'm gonna do I'm

00:29:19,659 --> 00:29:22,899
going to count the numbers from one to

00:29:20,980 --> 00:29:27,820
ten or actually multiply them in this

00:29:22,899 --> 00:29:29,590
case and then I'm gonna run with it

00:29:27,820 --> 00:29:31,480
encapsulate something we'll see a little

00:29:29,590 --> 00:29:34,269
bit later where it materializes this

00:29:31,480 --> 00:29:36,490
thing into actual actors and then I'm

00:29:34,269 --> 00:29:38,850
gonna tell it I want you to just send it

00:29:36,490 --> 00:29:42,220
to print Lin so we're actually going to

00:29:38,850 --> 00:29:43,450
not dump it to anything new like a topic

00:29:42,220 --> 00:29:46,690
in this case we're just going to print

00:29:43,450 --> 00:29:49,480
out the results so it just is source

00:29:46,690 --> 00:29:53,860
flow and sink is sort of the minimum

00:29:49,480 --> 00:29:55,330
thing for a naka stream and this is

00:29:53,860 --> 00:29:57,429
actually defining a graph so you can

00:29:55,330 --> 00:29:58,929
have fairly complicated graphs Colin

00:29:57,429 --> 00:30:01,029
showed some interesting ones that you

00:29:58,929 --> 00:30:05,019
know from practical examples in his talk

00:30:01,029 --> 00:30:06,610
earlier now the diagram here is showing

00:30:05,019 --> 00:30:08,169
one slight difference and I put the

00:30:06,610 --> 00:30:10,360
other one in the bottom just for

00:30:08,169 --> 00:30:12,309
comparison and that is that you know so

00:30:10,360 --> 00:30:14,620
when I have something like an actor

00:30:12,309 --> 00:30:16,510
system under the hood maybe it doesn't

00:30:14,620 --> 00:30:18,789
actually make sense to run everything

00:30:16,510 --> 00:30:20,380
through a Kafka topic because there's

00:30:18,789 --> 00:30:22,929
some overhead there maybe in some cases

00:30:20,380 --> 00:30:25,840
I should just send stuff is a message to

00:30:22,929 --> 00:30:27,370
another actor to do some follow on logic

00:30:25,840 --> 00:30:29,230
and once again I'm not going to show

00:30:27,370 --> 00:30:31,809
anything as an example for that logic

00:30:29,230 --> 00:30:34,090
but this is an option I have with with

00:30:31,809 --> 00:30:35,559
acha streams is that I can balance that

00:30:34,090 --> 00:30:37,690
choice and I'll come back to that choice

00:30:35,559 --> 00:30:39,429
in a little bit but first let me go

00:30:37,690 --> 00:30:41,200
through some code for how this works

00:30:39,429 --> 00:30:44,440
those who you've used akka that will

00:30:41,200 --> 00:30:45,850
recognize a lot of this once again there

00:30:44,440 --> 00:30:48,190
are some things I'm not going to go into

00:30:45,850 --> 00:30:50,799
any detail about but are pretty standard

00:30:48,190 --> 00:30:52,659
things the first block the actor system

00:30:50,799 --> 00:30:54,370
the actor materializer these are the

00:30:52,659 --> 00:30:56,080
sort of things you always do - they're

00:30:54,370 --> 00:30:58,450
they're your entry point to you know

00:30:56,080 --> 00:31:01,330
create this stuff so you start with

00:30:58,450 --> 00:31:02,860
those the model processor and score

00:31:01,330 --> 00:31:04,299
that's basically the same thing that we

00:31:02,860 --> 00:31:06,970
just talked about and just sort of

00:31:04,299 --> 00:31:08,530
abstraction around managing models and

00:31:06,970 --> 00:31:11,770
actually scoring data with

00:31:08,530 --> 00:31:13,660
those models we're going to use kind of

00:31:11,770 --> 00:31:15,790
the same concepts of a record and a

00:31:13,660 --> 00:31:18,700
scored record now you can see I'm just

00:31:15,790 --> 00:31:20,410
going to define a data source and then

00:31:18,700 --> 00:31:22,150
there's this thing on the the two types

00:31:20,410 --> 00:31:24,100
or one it's the record type so it's like

00:31:22,150 --> 00:31:26,320
the schema of the data in this source

00:31:24,100 --> 00:31:28,360
the second item is like a control

00:31:26,320 --> 00:31:30,520
construct it gives me some hooks for

00:31:28,360 --> 00:31:33,970
doing additional things with this stream

00:31:30,520 --> 00:31:38,340
if I want to I kind of glossed over that

00:31:33,970 --> 00:31:40,960
part in the earlier example let's see

00:31:38,340 --> 00:31:43,870
you can see the middle block their

00:31:40,960 --> 00:31:45,850
subscriptions topic so consumer

00:31:43,870 --> 00:31:48,210
subscription I think producers down

00:31:45,850 --> 00:31:52,570
there these are sort of the akka streams

00:31:48,210 --> 00:31:54,820
libraries for talking to Kafka and I

00:31:52,570 --> 00:31:55,840
think that's it for that block all right

00:31:54,820 --> 00:31:58,180
so let's actually go through an example

00:31:55,840 --> 00:31:59,950
so once again we start by creating an

00:31:58,180 --> 00:32:01,990
actor system like we would if we were

00:31:59,950 --> 00:32:04,750
just writing regular actors but we also

00:32:01,990 --> 00:32:06,970
need something that can materialize the

00:32:04,750 --> 00:32:09,130
actual runtime graph that's going to

00:32:06,970 --> 00:32:10,840
process this stuff and we're declaring

00:32:09,130 --> 00:32:13,540
these things implicit because they're

00:32:10,840 --> 00:32:15,430
just carried along as arguments to some

00:32:13,540 --> 00:32:19,690
of these methods but we don't want to

00:32:15,430 --> 00:32:21,460
have to specify them explicitly this

00:32:19,690 --> 00:32:25,480
logic here is basically the same as what

00:32:21,460 --> 00:32:26,560
we had before for Kafka streams but now

00:32:25,480 --> 00:32:28,180
what we're gonna do is slightly

00:32:26,560 --> 00:32:29,860
differently is we're gonna leverage the

00:32:28,180 --> 00:32:33,400
ability to actually define our own

00:32:29,860 --> 00:32:35,980
custom stage this is like our own you

00:32:33,400 --> 00:32:38,560
know controller or operator so I can

00:32:35,980 --> 00:32:40,300
treat this model scoring stage that I'm

00:32:38,560 --> 00:32:42,850
going to create as if it was a regular

00:32:40,300 --> 00:32:45,880
operation that I can apply like map and

00:32:42,850 --> 00:32:47,110
collect and so forth and it you know

00:32:45,880 --> 00:32:50,260
we're not going into a lot of detail

00:32:47,110 --> 00:32:52,420
looks basically like this I extend some

00:32:50,260 --> 00:32:55,270
abstract class you know I'd give it at

00:32:52,420 --> 00:32:56,860
this particular name I define what they

00:32:55,270 --> 00:32:58,630
call inlets and outlets you know these

00:32:56,860 --> 00:33:00,610
are the the data coming in and going out

00:32:58,630 --> 00:33:03,760
and then I define a handler that's gonna

00:33:00,610 --> 00:33:05,650
process each record as it comes in and

00:33:03,760 --> 00:33:07,570
in this case what it's going to do is

00:33:05,650 --> 00:33:09,460
this is what I'm going to use to score

00:33:07,570 --> 00:33:12,610
data so that's scoring record object

00:33:09,460 --> 00:33:14,500
there's a record score object wherever I

00:33:12,610 --> 00:33:16,720
called it it's going to be called to

00:33:14,500 --> 00:33:19,840
construct a new scored record with each

00:33:16,720 --> 00:33:20,410
record I pass in I think there's a typo

00:33:19,840 --> 00:33:21,370
here actually

00:33:20,410 --> 00:33:23,570
oops

00:33:21,370 --> 00:33:26,179
anyway I think you get the gist of it

00:33:23,570 --> 00:33:27,980
it's just a nice mechanism for defining

00:33:26,179 --> 00:33:29,570
her own custom operators which is the

00:33:27,980 --> 00:33:31,669
sort of thing you would do if you wanted

00:33:29,570 --> 00:33:33,110
to give this toolkit to you know machine

00:33:31,669 --> 00:33:34,190
learning researchers who didn't want to

00:33:33,110 --> 00:33:36,860
have to worry about those kind of

00:33:34,190 --> 00:33:38,570
details all right

00:33:36,860 --> 00:33:39,950
I kind of reverse the order a little bit

00:33:38,570 --> 00:33:41,870
previously I talked about the model

00:33:39,950 --> 00:33:43,730
stream first in the data stream here

00:33:41,870 --> 00:33:46,010
we're doing it the other way around but

00:33:43,730 --> 00:33:48,650
very similar with some interesting

00:33:46,010 --> 00:33:50,929
exceptions Nelly akka is the akka geyser

00:33:48,650 --> 00:33:52,130
they're very careful about terminology

00:33:50,929 --> 00:33:55,220
they don't want to tell you something

00:33:52,130 --> 00:33:57,520
that doesn't exist like exactly once so

00:33:55,220 --> 00:34:00,200
they'll say consumer at most one source

00:33:57,520 --> 00:34:02,570
in this case or it could have been at

00:34:00,200 --> 00:34:04,580
least one set cetera and then it's up to

00:34:02,570 --> 00:34:06,530
us to handle B duplication or whatever

00:34:04,580 --> 00:34:09,340
although it with it most once we really

00:34:06,530 --> 00:34:12,350
only would see things once or zero times

00:34:09,340 --> 00:34:14,450
so we're gonna go with that sort of

00:34:12,350 --> 00:34:17,300
reality dose here by going with it most

00:34:14,450 --> 00:34:19,790
once the data consumer settings is like

00:34:17,300 --> 00:34:24,200
my configuration for for how it knows to

00:34:19,790 --> 00:34:26,480
talk to Kafka and so we're gonna then

00:34:24,200 --> 00:34:29,480
subscribe to the data topic again that

00:34:26,480 --> 00:34:32,330
that's the the blue arrow sort of cloudy

00:34:29,480 --> 00:34:34,220
like looking thing in the diagram and

00:34:32,330 --> 00:34:35,929
then we're gonna map over this parse it

00:34:34,220 --> 00:34:38,480
into a record and then we're going to

00:34:35,929 --> 00:34:40,850
use collect which is sort of like filter

00:34:38,480 --> 00:34:43,580
but also to supports transformations so

00:34:40,850 --> 00:34:45,859
the in this case the data record parsed

00:34:43,580 --> 00:34:49,730
bytes will return either a success or

00:34:45,859 --> 00:34:51,409
failure and so what we'll do is collect

00:34:49,730 --> 00:34:53,899
what we're gonna say all right if we got

00:34:51,409 --> 00:34:55,460
a successful parse well we'll just pull

00:34:53,899 --> 00:34:58,430
the data out and that will be the thing

00:34:55,460 --> 00:35:00,170
we send downstream otherwise like any

00:34:58,430 --> 00:35:02,810
good production code we're just gonna

00:35:00,170 --> 00:35:04,970
throw away our error so ignore them okay

00:35:02,810 --> 00:35:08,020
so otherwise you know we shouldn't

00:35:04,970 --> 00:35:09,940
really have a case for the failure

00:35:08,020 --> 00:35:13,460
situation as well

00:35:09,940 --> 00:35:16,040
now the model stream this is really very

00:35:13,460 --> 00:35:17,869
similar to what I said before following

00:35:16,040 --> 00:35:19,490
the example from the tutorial we're

00:35:17,869 --> 00:35:20,780
gonna see we're gonna try to parse the

00:35:19,490 --> 00:35:22,310
bytes all right now we've got

00:35:20,780 --> 00:35:24,500
information about the model if we

00:35:22,310 --> 00:35:26,270
succeeded then we're gonna try to figure

00:35:24,500 --> 00:35:28,820
out which model we want to build the

00:35:26,270 --> 00:35:30,740
reason this logic is there is the

00:35:28,820 --> 00:35:32,570
tutorial supports things like I actually

00:35:30,740 --> 00:35:34,150
want to use a tensor flow model or I

00:35:32,570 --> 00:35:35,589
want to use logistic regression

00:35:34,150 --> 00:35:38,829
and so that's sort of metadata that

00:35:35,589 --> 00:35:40,869
would be embarrass and if that all

00:35:38,829 --> 00:35:43,390
succeeds then I'm gonna build up my new

00:35:40,869 --> 00:35:46,119
model processor set the model towards

00:35:43,390 --> 00:35:47,259
the end there and I'm not gonna do

00:35:46,119 --> 00:35:50,710
anything else with this I'm gonna

00:35:47,259 --> 00:35:52,150
actually as my sink just have it ignore

00:35:50,710 --> 00:35:54,640
it's not going to write anything out I'm

00:35:52,150 --> 00:35:56,289
just going to take this object the

00:35:54,640 --> 00:35:59,019
reason I'm not writing anything out here

00:35:56,289 --> 00:36:01,539
is I've actually circumvented the stream

00:35:59,019 --> 00:36:03,279
if you will with model processor set

00:36:01,539 --> 00:36:05,109
model I'm actually everything I care

00:36:03,279 --> 00:36:07,059
about is actually going to get passed to

00:36:05,109 --> 00:36:09,249
this in-memory object so there's now

00:36:07,059 --> 00:36:11,230
nothing left for me to pass downstream

00:36:09,249 --> 00:36:15,730
so I'm just gonna write to basically a

00:36:11,230 --> 00:36:17,529
no op sink at the end and then the data

00:36:15,730 --> 00:36:20,259
stream is kind of similar to what we had

00:36:17,529 --> 00:36:23,109
before where I'm just going to you know

00:36:20,259 --> 00:36:25,539
hopefully succeed in parsing if I don't

00:36:23,109 --> 00:36:27,369
then I'll just ignore the result and I'm

00:36:25,539 --> 00:36:30,130
gonna run with a producer that will

00:36:27,369 --> 00:36:32,529
actually write to a new Kafka topic so

00:36:30,130 --> 00:36:33,970
the run with will and then a producer

00:36:32,529 --> 00:36:37,930
will give me a sink that since the

00:36:33,970 --> 00:36:39,970
records downstream okay a lot of code

00:36:37,930 --> 00:36:41,200
may be a little hard to follow when I'm

00:36:39,970 --> 00:36:43,239
going through it quickly but hopefully

00:36:41,200 --> 00:36:44,799
get the gist to kind of wrap up I want

00:36:43,239 --> 00:36:47,430
to talk about a few production

00:36:44,799 --> 00:36:50,230
trade-offs that you might think about

00:36:47,430 --> 00:36:51,759
one is how do I'd score this thing well

00:36:50,230 --> 00:36:53,680
there's several things you can do in the

00:36:51,759 --> 00:36:55,210
architecture itself there's there's

00:36:53,680 --> 00:36:56,950
different techniques that have merged in

00:36:55,210 --> 00:36:58,930
the model serving community if I can

00:36:56,950 --> 00:37:00,640
call them that of like delegating work

00:36:58,930 --> 00:37:02,410
to worker nodes we've got all the

00:37:00,640 --> 00:37:05,380
facilities we need and acha to do this

00:37:02,410 --> 00:37:07,210
kind of thing very easily there's also a

00:37:05,380 --> 00:37:09,039
cool idea of I might have several

00:37:07,210 --> 00:37:10,450
different kinds of models and I picked

00:37:09,039 --> 00:37:12,970
the winner you know when I score a

00:37:10,450 --> 00:37:15,039
result I have a metric that says whether

00:37:12,970 --> 00:37:17,640
this result is more likely to be right

00:37:15,039 --> 00:37:19,869
than this result and tricks like that I

00:37:17,640 --> 00:37:22,630
can connect to almost anything with

00:37:19,869 --> 00:37:24,549
alpaca but so I could instead of going

00:37:22,630 --> 00:37:26,619
through Kafka I could just go directly

00:37:24,549 --> 00:37:28,299
through you know web interface like data

00:37:26,619 --> 00:37:30,069
coming right off the internet or

00:37:28,299 --> 00:37:30,489
something if I really wanted to and so

00:37:30,069 --> 00:37:33,009
forth

00:37:30,489 --> 00:37:35,049
I can persist state if I want to keep

00:37:33,009 --> 00:37:37,239
track of the model in case things crash

00:37:35,049 --> 00:37:42,160
and bring it up really fast I get use

00:37:37,239 --> 00:37:43,269
acha persistence for that all right now

00:37:42,160 --> 00:37:45,039
let's talk a little bit about something

00:37:43,269 --> 00:37:46,360
I alluded to a minute ago when would I

00:37:45,039 --> 00:37:48,730
not want to go through

00:37:46,360 --> 00:37:50,290
Africa and when would I want to go

00:37:48,730 --> 00:37:52,330
through Kafka the advantage of like a

00:37:50,290 --> 00:37:54,100
direct actor to actor call is it's

00:37:52,330 --> 00:37:57,130
extremely low latency you know it's a

00:37:54,100 --> 00:37:59,620
little more than a function call very

00:37:57,130 --> 00:38:01,270
minimal IO and memory overhead and I

00:37:59,620 --> 00:38:03,400
don't have to do any marshaling of the

00:38:01,270 --> 00:38:05,950
data to send it across a process

00:38:03,400 --> 00:38:08,320
necessarily that's that's a bit of an

00:38:05,950 --> 00:38:09,970
exaggeration but but it gets rid of a

00:38:08,320 --> 00:38:13,480
lot of the things that happen when I go

00:38:09,970 --> 00:38:15,610
out of process but it is harder to scale

00:38:13,480 --> 00:38:18,520
and there's a slightly more direct

00:38:15,610 --> 00:38:20,080
dependency or connection between the the

00:38:18,520 --> 00:38:21,670
guy that's sending the message and the

00:38:20,080 --> 00:38:24,460
guy receiving it versus the kind of

00:38:21,670 --> 00:38:26,230
abstraction we get going through a Kafka

00:38:24,460 --> 00:38:28,750
topic now that abstract that can be

00:38:26,230 --> 00:38:31,420
abstracted through actor references and

00:38:28,750 --> 00:38:34,180
so forth but it's a little bit more of a

00:38:31,420 --> 00:38:36,760
coupling whereas if I'm going through

00:38:34,180 --> 00:38:38,110
Kafka topic I am taking on higher

00:38:36,760 --> 00:38:40,540
latency you know the whole network

00:38:38,110 --> 00:38:42,160
traffic and queue depth you know if I'm

00:38:40,540 --> 00:38:44,230
going through the messages one at a time

00:38:42,160 --> 00:38:46,330
and I have a very long topic partition

00:38:44,230 --> 00:38:49,960
then you know I get overhead or latency

00:38:46,330 --> 00:38:51,640
from that I have to marshal data I you

00:38:49,960 --> 00:38:53,980
know have all the i/o of going across

00:38:51,640 --> 00:38:56,110
the process but I get a lot of

00:38:53,980 --> 00:38:58,060
flexibility for stability

00:38:56,110 --> 00:39:00,250
you know I'm much more of a firewall

00:38:58,060 --> 00:39:03,850
situation where if a process crashes

00:39:00,250 --> 00:39:05,950
it's unlikely to affect Kafka I can

00:39:03,850 --> 00:39:08,230
easily scale up and scale down more

00:39:05,950 --> 00:39:11,680
easily in some respects and so it has

00:39:08,230 --> 00:39:13,720
its advantages too I get back pressure

00:39:11,680 --> 00:39:15,490
with reactive streams if I'm just doing

00:39:13,720 --> 00:39:16,900
streaming messages between things so

00:39:15,490 --> 00:39:18,790
that's not true if I'm doing actor

00:39:16,900 --> 00:39:21,630
message calls but if I'm using streams

00:39:18,790 --> 00:39:24,040
to communicate then I get back pressure

00:39:21,630 --> 00:39:25,810
but once again I do get a little bit

00:39:24,040 --> 00:39:27,780
more connection of a direct connection

00:39:25,810 --> 00:39:30,670
between sender and receiver from using

00:39:27,780 --> 00:39:32,620
actor messages whereas on the right hand

00:39:30,670 --> 00:39:35,970
side I get a really deep buffer it's not

00:39:32,620 --> 00:39:39,100
quite infinite but it's it does kind of

00:39:35,970 --> 00:39:40,960
mostly solve that problem of why

00:39:39,100 --> 00:39:43,060
reactive streams were invented which is

00:39:40,960 --> 00:39:44,830
I can't have infinite buffers well I

00:39:43,060 --> 00:39:45,730
can't have a nearly infinite buffer if I

00:39:44,830 --> 00:39:47,890
have a big hard drive

00:39:45,730 --> 00:39:50,680
it's basically what it comes down to but

00:39:47,890 --> 00:39:53,410
it's not infinite and I get strong

00:39:50,680 --> 00:39:55,480
decoupling so you have to kind of

00:39:53,410 --> 00:39:57,310
balance when it makes sense to run

00:39:55,480 --> 00:39:58,630
things through Kafka versus when it

00:39:57,310 --> 00:40:00,140
actually makes sense to get better

00:39:58,630 --> 00:40:04,940
efficiencies by keeping

00:40:00,140 --> 00:40:06,350
in process the wrap-up again this is

00:40:04,940 --> 00:40:10,610
kind of summarizing what was in this

00:40:06,350 --> 00:40:12,350
report obviously we're building

00:40:10,610 --> 00:40:13,760
commercial stuff around this and you can

00:40:12,350 --> 00:40:15,440
come talk to me afterwards if you're

00:40:13,760 --> 00:40:17,480
interested in their fast data platform

00:40:15,440 --> 00:40:22,040
and actually iBM is showing it to and in

00:40:17,480 --> 00:40:24,220
their booth and that's it thank you very

00:40:22,040 --> 00:40:24,220
much

00:40:28,430 --> 00:40:32,810
I think we've got five minutes if

00:40:30,560 --> 00:40:41,030
anybody has questions or criticisms

00:40:32,810 --> 00:40:44,080
complaints hi I have actually two

00:40:41,030 --> 00:40:47,900
questions we're using Kafka streams and

00:40:44,080 --> 00:40:49,490
at the beginning we added FS too but

00:40:47,900 --> 00:40:51,860
eventually we saw that we don't need it

00:40:49,490 --> 00:40:54,770
we thought about using caca streams but

00:40:51,860 --> 00:40:57,980
we again found that we don't need it my

00:40:54,770 --> 00:41:00,590
question is which use cases do you think

00:40:57,980 --> 00:41:02,960
with where Kafka streams is just not

00:41:00,590 --> 00:41:04,250
enough that's my first question my

00:41:02,960 --> 00:41:06,410
second question is regarding your

00:41:04,250 --> 00:41:09,820
interactive queries library which you

00:41:06,410 --> 00:41:13,070
embedded alka HTTP strongly with it

00:41:09,820 --> 00:41:14,840
would you think if an interactive query

00:41:13,070 --> 00:41:16,940
which is distributed is a good idea

00:41:14,840 --> 00:41:19,250
because it's like counterintuitive

00:41:16,940 --> 00:41:21,680
because usually what you would like to

00:41:19,250 --> 00:41:23,990
go to the local storage to the rocks to

00:41:21,680 --> 00:41:26,180
be inside your service and not going

00:41:23,990 --> 00:41:28,910
starting hope hopping between services

00:41:26,180 --> 00:41:32,540
so yeah yeah let me pop the stack on the

00:41:28,910 --> 00:41:34,160
questions as far as like so that query

00:41:32,540 --> 00:41:36,950
library mentioned in passing what

00:41:34,160 --> 00:41:38,990
happens is you can ask a Kafka stream

00:41:36,950 --> 00:41:40,790
for its state rather than have to force

00:41:38,990 --> 00:41:42,260
you to write something that writes to a

00:41:40,790 --> 00:41:44,630
database and then you query the database

00:41:42,260 --> 00:41:47,270
the problem is out of the box it only

00:41:44,630 --> 00:41:50,720
lets you talk to a single process so if

00:41:47,270 --> 00:41:52,520
I have a family of Kafka streams apps

00:41:50,720 --> 00:41:55,520
because I've got a bunch of partitions

00:41:52,520 --> 00:41:57,200
one a poor partition then I have to ping

00:41:55,520 --> 00:41:58,880
each one of them so what this library

00:41:57,200 --> 00:42:01,940
does is it makes it easy to aggregate

00:41:58,880 --> 00:42:04,610
that into one big query that's really

00:42:01,940 --> 00:42:05,830
all it does does that make sense I think

00:42:04,610 --> 00:42:07,910
it makes sense for things like

00:42:05,830 --> 00:42:09,890
dashboards or something that maybe

00:42:07,910 --> 00:42:11,750
aren't quite mission-critical I'd be a

00:42:09,890 --> 00:42:13,970
little hesitant to just rely on this

00:42:11,750 --> 00:42:16,730
mechanism if I really needed that state

00:42:13,970 --> 00:42:19,040
to be persisted in a robust way I would

00:42:16,730 --> 00:42:21,280
rather somehow either write it out to

00:42:19,040 --> 00:42:23,870
Kafka or write it to a database

00:42:21,280 --> 00:42:25,700
out-of-band in some way but but I think

00:42:23,870 --> 00:42:28,520
it's really a nice tool for these kinds

00:42:25,700 --> 00:42:31,160
of less mission-critical

00:42:28,520 --> 00:42:32,870
kind of queries of state but I guess I

00:42:31,160 --> 00:42:35,630
think people are actually using it for

00:42:32,870 --> 00:42:37,190
that purpose but it's probably a more

00:42:35,630 --> 00:42:38,070
risky because it's not really designed

00:42:37,190 --> 00:42:40,680
in this

00:42:38,070 --> 00:42:42,480
kind of robust way as far as the

00:42:40,680 --> 00:42:45,090
limitations of kafka streams

00:42:42,480 --> 00:42:47,970
I think Collins talk was a really good

00:42:45,090 --> 00:42:49,800
example earlier of if you really need

00:42:47,970 --> 00:42:52,260
fine-grained control and want very

00:42:49,800 --> 00:42:54,210
precise handling of how things flow

00:42:52,260 --> 00:42:56,010
through the system but want to express

00:42:54,210 --> 00:42:58,410
it in practically a declarative way

00:42:56,010 --> 00:43:00,690
rather than writing a lot of akka code

00:42:58,410 --> 00:43:02,790
it's a fantastic tool to go to akka

00:43:00,690 --> 00:43:04,560
streams for that it really fits that

00:43:02,790 --> 00:43:07,350
model of I do have very tight

00:43:04,560 --> 00:43:09,540
constraints of how things should be

00:43:07,350 --> 00:43:12,270
processed and I want to do very robust

00:43:09,540 --> 00:43:14,130
air handling and error recovery and and

00:43:12,270 --> 00:43:16,770
routing them messages and all that

00:43:14,130 --> 00:43:19,080
whereas I think Kafka streams really

00:43:16,770 --> 00:43:21,720
comes into its own form or let's call

00:43:19,080 --> 00:43:23,550
them mainstream scenarios like I'm just

00:43:21,720 --> 00:43:25,380
gonna do ETL you know I've got log

00:43:23,550 --> 00:43:27,030
messages coming in they're just strings

00:43:25,380 --> 00:43:31,290
I'm gonna parse them into a record

00:43:27,030 --> 00:43:33,540
format for downstream analysis or you

00:43:31,290 --> 00:43:35,130
know I've got a dashboard over here and

00:43:33,540 --> 00:43:36,990
I've got a bunch of topics that I just

00:43:35,130 --> 00:43:39,060
want to watch like you know aggregated

00:43:36,990 --> 00:43:42,210
information like high-water marks or

00:43:39,060 --> 00:43:44,150
average over time you it's really quick

00:43:42,210 --> 00:43:46,860
and easy to knock out those kind of

00:43:44,150 --> 00:43:47,970
applications and kafka streams but they

00:43:46,860 --> 00:43:50,610
don't give you the kind of low-level

00:43:47,970 --> 00:43:52,560
powerful control that you get with akka

00:43:50,610 --> 00:43:55,670
streams so that's sort of an intuitive

00:43:52,560 --> 00:43:59,150
way that I think about the differences

00:43:55,670 --> 00:43:59,150
so anybody else

00:44:06,099 --> 00:44:10,320
I don't see anybody either all right

00:44:09,069 --> 00:44:15,289
thank you very much

00:44:10,320 --> 00:44:15,289

YouTube URL: https://www.youtube.com/watch?v=YFBP9dZlpvM


