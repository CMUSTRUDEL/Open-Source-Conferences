Title: Analyzing Pwned Passwords with Apache Spark by Kelley Robinson
Publication date: 2018-09-22
Playlist: Scala Days New York 2018
Description: 
	This video was recorded at Scala Days New York 2018
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

More information and the abstract can be found here:
https://na.scaladays.org/schedule/analyzing-pwned-passwords-with-apache-spark
Captions: 
	00:00:03,530 --> 00:00:09,559
so a while ago when when Netflix still

00:00:06,830 --> 00:00:11,480
included the the delivery service as

00:00:09,559 --> 00:00:12,679
part of the streaming service I don't

00:00:11,480 --> 00:00:15,460
know if any of you remember this it was

00:00:12,679 --> 00:00:18,050
like you know way back in the old days

00:00:15,460 --> 00:00:19,910
somebody actually hacked my account and

00:00:18,050 --> 00:00:21,350
it was really funny because what

00:00:19,910 --> 00:00:23,029
happened is they didn't change any of

00:00:21,350 --> 00:00:24,080
like the streaming queue because at that

00:00:23,029 --> 00:00:26,930
point there wasn't really that much

00:00:24,080 --> 00:00:29,390
available to stream but what they did is

00:00:26,930 --> 00:00:32,090
they replaced everything in my delivery

00:00:29,390 --> 00:00:36,380
queue with all of Nicolas Cage's

00:00:32,090 --> 00:00:38,059
filmography and they didn't update my

00:00:36,380 --> 00:00:39,710
address and I was like this is really

00:00:38,059 --> 00:00:41,690
really weird I don't know what happened

00:00:39,710 --> 00:00:44,059
maybe it was one of my friends that like

00:00:41,690 --> 00:00:45,260
was trying to troll me more likely it

00:00:44,059 --> 00:00:47,899
was that somebody had like hacked my

00:00:45,260 --> 00:00:50,180
account and was like trying to use it

00:00:47,899 --> 00:00:51,829
just to watch Netflix but that was the

00:00:50,180 --> 00:00:54,680
first time that any of my stuff had ever

00:00:51,829 --> 00:00:55,789
been owned and I just I always remember

00:00:54,680 --> 00:00:58,789
that story now that I think about

00:00:55,789 --> 00:01:00,079
passwords all the time because it it's

00:00:58,789 --> 00:01:01,129
something that happens to a lot of

00:01:00,079 --> 00:01:02,239
people and so you might be wondering

00:01:01,129 --> 00:01:04,850
what does any of this have to do with

00:01:02,239 --> 00:01:06,530
spark and Scala well it turns out that

00:01:04,850 --> 00:01:08,780
there are a lot of these own passwords

00:01:06,530 --> 00:01:11,630
owned by the way spelled with the P is

00:01:08,780 --> 00:01:13,400
the hacker way of spelling owned but you

00:01:11,630 --> 00:01:14,840
keep the peace island if you do that

00:01:13,400 --> 00:01:18,260
just people will think you're cool so

00:01:14,840 --> 00:01:20,000
trust me I but so anyway that that was

00:01:18,260 --> 00:01:21,259
my story about getting owned and it

00:01:20,000 --> 00:01:22,880
turns out that there are a lot of these

00:01:21,259 --> 00:01:25,100
passwords that are available for

00:01:22,880 --> 00:01:26,990
purchase on the internet and we're going

00:01:25,100 --> 00:01:29,720
to be looking at those today using Scala

00:01:26,990 --> 00:01:31,670
and spark spark is a project that's been

00:01:29,720 --> 00:01:33,680
around for about four years

00:01:31,670 --> 00:01:36,500
back when I worked an ad tech we were

00:01:33,680 --> 00:01:38,810
using it to do all of our extract

00:01:36,500 --> 00:01:40,400
transform load our ETL pipelines we were

00:01:38,810 --> 00:01:42,409
using a combination of spark and Scala

00:01:40,400 --> 00:01:44,689
to build those projects and when we got

00:01:42,409 --> 00:01:48,470
started with spark we were on maybe

00:01:44,689 --> 00:01:49,790
spark 1.1 back in around 2015 and so

00:01:48,470 --> 00:01:51,950
it's been really interesting to see how

00:01:49,790 --> 00:01:54,110
the project has developed since then and

00:01:51,950 --> 00:01:56,600
see how the different abstractions have

00:01:54,110 --> 00:01:58,610
gained traction and and developed and

00:01:56,600 --> 00:02:01,250
gained different performance

00:01:58,610 --> 00:02:02,869
improvements over time and so this talk

00:02:01,250 --> 00:02:05,689
today is going to be combining those two

00:02:02,869 --> 00:02:07,369
stories how how we can use spark what we

00:02:05,689 --> 00:02:09,500
can learn from my experience with spark

00:02:07,369 --> 00:02:11,450
maybe how you can get started analyzing

00:02:09,500 --> 00:02:12,440
some some of your own data but we're

00:02:11,450 --> 00:02:15,739
going to be looking at it through the

00:02:12,440 --> 00:02:16,470
lens of own passwords and I got out of

00:02:15,739 --> 00:02:18,210
it I'd talk about

00:02:16,470 --> 00:02:20,010
six months ago these days I'm a

00:02:18,210 --> 00:02:21,570
developer evangelist Twilio where I get

00:02:20,010 --> 00:02:24,540
to work on a lot of fun and interesting

00:02:21,570 --> 00:02:26,460
things so for those of you that don't

00:02:24,540 --> 00:02:28,620
know Tulio is a communications company

00:02:26,460 --> 00:02:30,510
we provide API is that make it easy for

00:02:28,620 --> 00:02:32,400
you to add things like text messaging

00:02:30,510 --> 00:02:35,370
and voice calling into your applications

00:02:32,400 --> 00:02:36,420
and you can do this with all the tools

00:02:35,370 --> 00:02:38,250
languages and frameworks that you

00:02:36,420 --> 00:02:39,300
already know a lot of the stuff that I

00:02:38,250 --> 00:02:41,700
work on has to do with our

00:02:39,300 --> 00:02:45,030
authentication products and so back in

00:02:41,700 --> 00:02:48,060
2015 1200 the-- phone numbers are very

00:02:45,030 --> 00:02:50,190
closely tied to identity and authy is a

00:02:48,060 --> 00:02:52,830
common two-factor authentication and

00:02:50,190 --> 00:02:54,240
identity management solution and so I

00:02:52,830 --> 00:02:56,160
work on a lot of our account security

00:02:54,240 --> 00:02:57,780
products there and spend a lot of time

00:02:56,160 --> 00:03:01,050
thinking about how we can protect our

00:02:57,780 --> 00:03:02,580
identities and online and in the spirit

00:03:01,050 --> 00:03:04,020
of security what I wanted to do with

00:03:02,580 --> 00:03:06,810
this talk was kind of combine those

00:03:04,020 --> 00:03:08,520
things those interests and look at the

00:03:06,810 --> 00:03:10,530
idea of passwords passwords have really

00:03:08,520 --> 00:03:12,750
been the way that we've secured our

00:03:10,530 --> 00:03:14,970
online identities for the last 30 or 40

00:03:12,750 --> 00:03:16,830
years and it's really interesting to see

00:03:14,970 --> 00:03:23,280
how terrible most people's passwords

00:03:16,830 --> 00:03:26,310
still are so what happens is companies

00:03:23,280 --> 00:03:29,220
sites get breached and this data becomes

00:03:26,310 --> 00:03:31,110
available and so we have about 500

00:03:29,220 --> 00:03:33,030
million passwords and credential pairs

00:03:31,110 --> 00:03:35,040
that are available for download for

00:03:33,030 --> 00:03:38,180
doing analysis on and that's the data

00:03:35,040 --> 00:03:38,180
that we're gonna be looking at today

00:03:39,230 --> 00:03:44,250
we're gonna start by introducing apache

00:03:41,459 --> 00:03:46,380
spark look at why people use it and how

00:03:44,250 --> 00:03:48,090
the project has developed I will give a

00:03:46,380 --> 00:03:50,280
little bit of background on the state of

00:03:48,090 --> 00:03:52,380
passwords before we dive into the data

00:03:50,280 --> 00:03:53,910
then I'm going to show you what spark

00:03:52,380 --> 00:03:55,410
looks like in action especially some of

00:03:53,910 --> 00:03:57,840
the newer stuff that's been around in

00:03:55,410 --> 00:03:59,010
the last year or so and finally we'll

00:03:57,840 --> 00:04:01,280
wrap up by talking about the

00:03:59,010 --> 00:04:03,930
implications of big data and security

00:04:01,280 --> 00:04:07,110
alright so let's start by looking at the

00:04:03,930 --> 00:04:09,480
Apache spark project so spark markets

00:04:07,110 --> 00:04:11,850
itself as a fast and unified analytics

00:04:09,480 --> 00:04:14,010
engine for big data and machine learning

00:04:11,850 --> 00:04:15,590
and big data in this case is anything

00:04:14,010 --> 00:04:17,609
that can't fit on a single machine

00:04:15,590 --> 00:04:19,290
machine capabilities are getting bigger

00:04:17,609 --> 00:04:21,330
every day but of course so is the data

00:04:19,290 --> 00:04:22,830
that we're processing and so we're

00:04:21,330 --> 00:04:24,780
talking about data that can't fit on

00:04:22,830 --> 00:04:27,150
your local machine can't fit on a single

00:04:24,780 --> 00:04:30,120
ec2 instance you're going to have data

00:04:27,150 --> 00:04:32,729
spread across different sources

00:04:30,120 --> 00:04:34,830
front files different servers and this

00:04:32,729 --> 00:04:36,660
is how you you're going to need to be

00:04:34,830 --> 00:04:38,340
able to read that and do processing on

00:04:36,660 --> 00:04:40,979
that data in a way that makes sense and

00:04:38,340 --> 00:04:43,440
SPARC is a really useful tool for doing

00:04:40,979 --> 00:04:44,820
that SPARC is generally thought of it as

00:04:43,440 --> 00:04:46,800
an improvement on top of Hadoop and

00:04:44,820 --> 00:04:48,990
MapReduce in fact it was built on top of

00:04:46,800 --> 00:04:51,120
Hadoop and MapReduce and depending on

00:04:48,990 --> 00:04:53,070
what you're doing with it it can be up

00:04:51,120 --> 00:04:54,960
to a hundred times faster than doing the

00:04:53,070 --> 00:04:56,400
same thing in Hadoop which is pretty

00:04:54,960 --> 00:04:57,750
impressive and one of the big reasons

00:04:56,400 --> 00:05:02,880
that a lot of people have moved to this

00:04:57,750 --> 00:05:05,760
project one of the major benefits of

00:05:02,880 --> 00:05:07,440
SPARC is that it has built-in support

00:05:05,760 --> 00:05:08,940
from multiple languages and frameworks

00:05:07,440 --> 00:05:11,570
and so you can see here that it has

00:05:08,940 --> 00:05:13,979
support for Scala and Java and Python

00:05:11,570 --> 00:05:15,539
but it also has these frameworks on top

00:05:13,979 --> 00:05:18,389
of it for doing things like streaming

00:05:15,539 --> 00:05:19,860
data and machine learning and it's

00:05:18,389 --> 00:05:22,080
really been promoting its machine

00:05:19,860 --> 00:05:23,400
learning capabilities lately but as my

00:05:22,080 --> 00:05:24,810
friend Ryan likes to say machine

00:05:23,400 --> 00:05:26,610
learning is just that carrot that you

00:05:24,810 --> 00:05:28,580
dangle in front of data engineers to get

00:05:26,610 --> 00:05:30,960
them to do the actual data engineering

00:05:28,580 --> 00:05:33,780
and I think it's really good that SPARC

00:05:30,960 --> 00:05:35,460
provides the ecosystem to do both things

00:05:33,780 --> 00:05:37,530
because that's going to make your team

00:05:35,460 --> 00:05:38,669
more effective in doing the machine

00:05:37,530 --> 00:05:40,080
learning down the line because you're

00:05:38,669 --> 00:05:41,550
still going to be working within the

00:05:40,080 --> 00:05:43,800
same framework within the same type of

00:05:41,550 --> 00:05:45,840
project ecosystem and that's going to

00:05:43,800 --> 00:05:47,940
make you a lot more capable to get the

00:05:45,840 --> 00:05:53,190
data in a state that you can do the fun

00:05:47,940 --> 00:05:54,930
more interesting analysis on on the data

00:05:53,190 --> 00:05:57,810
engineering side of things SPARC has two

00:05:54,930 --> 00:05:59,789
main abstractions that we'll use and we

00:05:57,810 --> 00:06:02,430
started out with our DD is a resilient

00:05:59,789 --> 00:06:04,020
distributed datasets and those are not

00:06:02,430 --> 00:06:05,849
going away but for a couple of reasons

00:06:04,020 --> 00:06:08,430
people are moving toward more towards

00:06:05,849 --> 00:06:11,220
the data frames now and so data frames

00:06:08,430 --> 00:06:12,560
or data sets are a better option for a

00:06:11,220 --> 00:06:14,820
few reasons

00:06:12,560 --> 00:06:16,289
completely unrelated Lea I just always

00:06:14,820 --> 00:06:18,630
have to point this out that it drives me

00:06:16,289 --> 00:06:20,960
in saying that the camel casing is

00:06:18,630 --> 00:06:23,280
inconsistent on these not is not a typo

00:06:20,960 --> 00:06:26,580
hopefully your IDE will catch that for

00:06:23,280 --> 00:06:28,199
you but in the meantime you definitely

00:06:26,580 --> 00:06:30,360
have to remember that because it will

00:06:28,199 --> 00:06:32,669
drive you and saying if it you're

00:06:30,360 --> 00:06:35,160
anything like me so a little bit more

00:06:32,669 --> 00:06:37,500
about rdd's they are immutable and

00:06:35,160 --> 00:06:39,210
distributed collections on distributed

00:06:37,500 --> 00:06:41,789
data they work really well with

00:06:39,210 --> 00:06:43,560
unstructured data so if you have dry log

00:06:41,789 --> 00:06:44,009
streams that have no structure to them

00:06:43,560 --> 00:06:45,539
already

00:06:44,009 --> 00:06:47,939
these are a good option for processing

00:06:45,539 --> 00:06:49,770
that and they have a very functional API

00:06:47,939 --> 00:06:51,990
that's going to be familiar to a lot of

00:06:49,770 --> 00:06:55,199
Scala developers if you've ever worked

00:06:51,990 --> 00:06:57,809
with Scala extensively the API for SPARC

00:06:55,199 --> 00:07:00,300
on top of rdd's looks very very similar

00:06:57,809 --> 00:07:03,389
and it provides a very familiar mapping

00:07:00,300 --> 00:07:04,649
and reducing interface that you you will

00:07:03,389 --> 00:07:08,129
be familiar with from working with a

00:07:04,649 --> 00:07:09,719
collections library in Scala and you can

00:07:08,129 --> 00:07:12,360
see how it's used in this code example

00:07:09,719 --> 00:07:14,099
to do a basic word count of course as

00:07:12,360 --> 00:07:15,809
the Holden likes to say this is a data

00:07:14,099 --> 00:07:18,689
talk so it has to include a word count

00:07:15,809 --> 00:07:21,899
example rdd's are not going away

00:07:18,689 --> 00:07:23,759
they still have their uses but there

00:07:21,899 --> 00:07:26,789
were some problems with rdd's and a

00:07:23,759 --> 00:07:30,270
problems with the initial iterations of

00:07:26,789 --> 00:07:31,860
spark in the one point X days and early

00:07:30,270 --> 00:07:33,539
on we discovered that users of rdd's

00:07:31,860 --> 00:07:35,189
were doing some unintentionally bad

00:07:33,539 --> 00:07:37,020
things and one of those things that

00:07:35,189 --> 00:07:39,180
people were doing was using this method

00:07:37,020 --> 00:07:41,669
called the group by key and so this is a

00:07:39,180 --> 00:07:42,809
best practices guide that was written by

00:07:41,669 --> 00:07:45,149
data bricks and if you're not familiar

00:07:42,809 --> 00:07:47,699
data bricks is the company that provides

00:07:45,149 --> 00:07:49,979
managed services for running SPARC the

00:07:47,699 --> 00:07:52,050
creators of the SPARC project started

00:07:49,979 --> 00:07:55,080
the data bricks company and so in many

00:07:52,050 --> 00:07:57,029
ways they are the thought leaders they

00:07:55,080 --> 00:07:58,589
are the source of truth for all of the

00:07:57,029 --> 00:08:00,719
things that you would want to know about

00:07:58,589 --> 00:08:02,490
SPARC and so they're publishing a best

00:08:00,719 --> 00:08:04,050
practices guide you definitely want to

00:08:02,490 --> 00:08:05,729
listen to them and so it's interesting

00:08:04,050 --> 00:08:07,740
that they are telling you to avoid an

00:08:05,729 --> 00:08:09,689
API that was provided in the language

00:08:07,740 --> 00:08:12,240
group by key sounds like something would

00:08:09,689 --> 00:08:13,559
that would be very useful to you it

00:08:12,240 --> 00:08:15,479
sounds like something that you would

00:08:13,559 --> 00:08:17,759
need if you're doing any kind of data

00:08:15,479 --> 00:08:20,669
transformations if you're doing anything

00:08:17,759 --> 00:08:22,860
that involves counting aggregation group

00:08:20,669 --> 00:08:25,409
by key especially if you're using an an

00:08:22,860 --> 00:08:27,330
IDE it's a function that may pop up as

00:08:25,409 --> 00:08:29,159
an option and you're like okay that

00:08:27,330 --> 00:08:31,319
sounds kind of nifty but it turns out

00:08:29,159 --> 00:08:33,599
that this is terribly unperformed it's

00:08:31,319 --> 00:08:35,639
really really slow and people weren't

00:08:33,599 --> 00:08:37,979
sure why their code was performing this

00:08:35,639 --> 00:08:39,959
way and so they had to tell people to

00:08:37,979 --> 00:08:41,519
stop using this and that there were

00:08:39,959 --> 00:08:44,760
better options available to them that

00:08:41,519 --> 00:08:46,260
weren't necessarily that discoverable so

00:08:44,760 --> 00:08:47,730
fortunately the project was aware of

00:08:46,260 --> 00:08:50,430
this and they were actively working on

00:08:47,730 --> 00:08:53,069
better ways to make this more accessible

00:08:50,430 --> 00:08:54,600
to people to give them tools that didn't

00:08:53,069 --> 00:08:56,870
have the same kind of pitfalls that you

00:08:54,600 --> 00:08:58,670
would have with something like this

00:08:56,870 --> 00:09:01,760
that's sort of how we ended up with data

00:08:58,670 --> 00:09:03,890
sets and data frames and so sparks

00:09:01,760 --> 00:09:05,990
started by creating the data frames API

00:09:03,890 --> 00:09:07,640
data frames are going to be incredibly

00:09:05,990 --> 00:09:09,410
similar to something like data frames in

00:09:07,640 --> 00:09:11,330
Python or R if you've worked with

00:09:09,410 --> 00:09:13,460
anything like that there are a lot like

00:09:11,330 --> 00:09:15,890
tables if you've ever worked with you

00:09:13,460 --> 00:09:18,560
know CSV files they resemble that kind

00:09:15,890 --> 00:09:20,270
of structure very similarly because they

00:09:18,560 --> 00:09:22,940
come with a structure they come with a

00:09:20,270 --> 00:09:25,010
schema data frames and data sets require

00:09:22,940 --> 00:09:26,900
that you have a a schema and semi

00:09:25,010 --> 00:09:28,339
structured or structured data and

00:09:26,900 --> 00:09:30,110
because of that there's a lot of

00:09:28,339 --> 00:09:32,210
performance improvements that the API

00:09:30,110 --> 00:09:35,240
can make on top of your data for making

00:09:32,210 --> 00:09:37,790
querying it much faster data sets

00:09:35,240 --> 00:09:40,310
specifically are available in Java and

00:09:37,790 --> 00:09:43,610
Scala is a strongly typed option on top

00:09:40,310 --> 00:09:45,400
of data frames and so as Scala

00:09:43,610 --> 00:09:48,830
programmers and as people who like to

00:09:45,400 --> 00:09:51,050
enforce our data and enforce our schemas

00:09:48,830 --> 00:09:57,650
with pipes data sets are a way that you

00:09:51,050 --> 00:09:59,690
can do that with SPARC and because data

00:09:57,650 --> 00:10:02,209
sets effectively require schema they're

00:09:59,690 --> 00:10:05,420
about two times faster than if you're

00:10:02,209 --> 00:10:06,890
doing anything with rdd's so if you're

00:10:05,420 --> 00:10:09,350
working with any kind of structured data

00:10:06,890 --> 00:10:11,630
you definitely want to use this route I

00:10:09,350 --> 00:10:13,910
think the sequel DSL which I'm going to

00:10:11,630 --> 00:10:15,980
show extensively later is one of the

00:10:13,910 --> 00:10:17,660
best things about this is that it allows

00:10:15,980 --> 00:10:21,200
you to work with your data in a very

00:10:17,660 --> 00:10:24,680
familiar way of querying data using a

00:10:21,200 --> 00:10:26,750
sequel like syntax so you'll remember

00:10:24,680 --> 00:10:28,190
this this diagram this ecosystem and the

00:10:26,750 --> 00:10:29,990
fact that SPARC is supported by many

00:10:28,190 --> 00:10:31,270
languages including languages like

00:10:29,990 --> 00:10:33,470
ironpython

00:10:31,270 --> 00:10:35,330
but I do want to point out that Scala

00:10:33,470 --> 00:10:36,980
still has the most robust language API

00:10:35,330 --> 00:10:38,870
and I'm not just saying that to make you

00:10:36,980 --> 00:10:41,390
feel better I know we like to think of

00:10:38,870 --> 00:10:43,490
ourselves as the best but Scala is still

00:10:41,390 --> 00:10:45,830
has the best API for the language

00:10:43,490 --> 00:10:47,570
because it's just been built out more

00:10:45,830 --> 00:10:49,070
the original spark project was written

00:10:47,570 --> 00:10:50,690
in Scala and so that's one of the big

00:10:49,070 --> 00:10:53,660
reasons that they put more investment

00:10:50,690 --> 00:10:55,190
into this API and if you ever need to

00:10:53,660 --> 00:10:56,930
look at anything under the hood you're

00:10:55,190 --> 00:10:58,520
going to be looking at Scala code and so

00:10:56,930 --> 00:11:02,959
I think there's major benefits to being

00:10:58,520 --> 00:11:04,970
able to write Scala on top of spark also

00:11:02,959 --> 00:11:06,230
Scala has these strongly typed datasets

00:11:04,970 --> 00:11:08,440
which is something that's just not going

00:11:06,230 --> 00:11:10,339
to be a thing and something like Python

00:11:08,440 --> 00:11:11,930
so I think there's a

00:11:10,339 --> 00:11:14,949
new advantages that we still have for

00:11:11,930 --> 00:11:17,269
Scala on top of a language like Python

00:11:14,949 --> 00:11:19,970
one of the big deficits bandages early

00:11:17,269 --> 00:11:22,220
on though was that Python was just so

00:11:19,970 --> 00:11:24,230
much slower than Scala and so you can

00:11:22,220 --> 00:11:26,779
see here when we were only had our d DS

00:11:24,230 --> 00:11:29,959
and as an option for abstractions our

00:11:26,779 --> 00:11:32,420
TDS were about half as fast as Scala and

00:11:29,959 --> 00:11:36,079
because of that a lot of teams were

00:11:32,420 --> 00:11:37,819
learning Scala just to use SPARC because

00:11:36,079 --> 00:11:39,680
there was huge performance improvements

00:11:37,819 --> 00:11:41,959
you could get just from switching to

00:11:39,680 --> 00:11:44,389
Scala and instead of having to learn how

00:11:41,959 --> 00:11:46,399
to be a spark performance optimizer

00:11:44,389 --> 00:11:48,259
which if you can avoid it you don't want

00:11:46,399 --> 00:11:53,029
to have to do spark engine optimization

00:11:48,259 --> 00:11:55,430
tuning as much as possible but as you

00:11:53,029 --> 00:11:57,259
can see once data frames came around the

00:11:55,430 --> 00:11:58,879
execution engine is the same for all of

00:11:57,259 --> 00:12:00,499
these there's a translation layer that

00:11:58,879 --> 00:12:03,079
makes the code execution under the hood

00:12:00,499 --> 00:12:04,819
use the same engine and so the the

00:12:03,079 --> 00:12:07,069
actual performance of data frames is

00:12:04,819 --> 00:12:09,079
very very similar across all the

00:12:07,069 --> 00:12:10,519
different languages and I think this is

00:12:09,079 --> 00:12:13,309
going to be really interesting as we

00:12:10,519 --> 00:12:14,660
watch Scala and SPARC and Python and the

00:12:13,309 --> 00:12:16,670
data communities and those different

00:12:14,660 --> 00:12:19,910
languages progressed in the near future

00:12:16,670 --> 00:12:21,949
so this suite was in reference to

00:12:19,910 --> 00:12:24,050
improving accessibility for data

00:12:21,949 --> 00:12:25,579
scientists and Scala and I think he

00:12:24,050 --> 00:12:28,879
really hits it on the head here which is

00:12:25,579 --> 00:12:31,699
that Python and are already have very

00:12:28,879 --> 00:12:34,069
robust communities for data processing I

00:12:31,699 --> 00:12:37,429
think their entry point to Python and R

00:12:34,069 --> 00:12:39,829
is a lot is a lot gentler than it is for

00:12:37,429 --> 00:12:42,110
Scala often and so you find a lot of

00:12:39,829 --> 00:12:44,509
people who are academics some PhD

00:12:42,110 --> 00:12:46,429
students bioinformatics researchers that

00:12:44,509 --> 00:12:48,889
are using languages like Python and R to

00:12:46,429 --> 00:12:51,019
do analysis of of lab data or something

00:12:48,889 --> 00:12:53,449
like that and they don't skip straight

00:12:51,019 --> 00:12:56,050
to Scala because there's already

00:12:53,449 --> 00:12:58,939
packages available like numpy and SCI PI

00:12:56,050 --> 00:13:00,860
that they can use to do their analysis

00:12:58,939 --> 00:13:03,230
and I think Scala currently is lacking a

00:13:00,860 --> 00:13:04,970
lot of those resources and so especially

00:13:03,230 --> 00:13:07,009
with the performance improvements with

00:13:04,970 --> 00:13:08,809
SPARC and Python recently I'm really

00:13:07,009 --> 00:13:11,269
interested to see how the community kind

00:13:08,809 --> 00:13:13,550
of reacts to this and if there's more

00:13:11,269 --> 00:13:16,009
attention that's put on the the Python

00:13:13,550 --> 00:13:17,420
api's to get that up to par with with

00:13:16,009 --> 00:13:18,769
Scala so I definitely think about

00:13:17,420 --> 00:13:23,600
something to watch out for in the next

00:13:18,769 --> 00:13:24,230
year or two so that's some of the basics

00:13:23,600 --> 00:13:25,580
of SPARC

00:13:24,230 --> 00:13:27,020
and where we're at with a project today

00:13:25,580 --> 00:13:28,250
I wanted to spend just a couple of

00:13:27,020 --> 00:13:30,740
minutes talking about the state of

00:13:28,250 --> 00:13:31,700
passwords so we can understand where

00:13:30,740 --> 00:13:34,010
we're coming from when we started

00:13:31,700 --> 00:13:35,720
looking at the data like I mentioned I

00:13:34,010 --> 00:13:38,180
think I spend a lot of time thinking

00:13:35,720 --> 00:13:41,120
about authentication and we have a big

00:13:38,180 --> 00:13:43,010
problem when it comes to passwords and

00:13:41,120 --> 00:13:45,110
that's and that's what people reuse

00:13:43,010 --> 00:13:47,840
passwords and use passwords that are

00:13:45,110 --> 00:13:49,580
short and very easy to guess and then

00:13:47,840 --> 00:13:52,760
they can guess those passwords and hack

00:13:49,580 --> 00:13:54,560
into my Netflix account yeah and it you

00:13:52,760 --> 00:13:55,700
know it's it's problematic because even

00:13:54,560 --> 00:13:57,770
if you don't care if your Netflix

00:13:55,700 --> 00:13:59,660
account gets hacked if you're using or

00:13:57,770 --> 00:14:02,090
if I was using the same password there

00:13:59,660 --> 00:14:05,270
as I was for my email account or my bank

00:14:02,090 --> 00:14:06,920
account you're gonna have a bad time and

00:14:05,270 --> 00:14:09,920
that's because hackers like this guy and

00:14:06,920 --> 00:14:11,240
you know he's legit cuz he in top of all

00:14:09,920 --> 00:14:13,220
that encrypted mess he got the word

00:14:11,240 --> 00:14:14,630
hacked to show up I think that's really

00:14:13,220 --> 00:14:16,640
impressive and he's got the beanie and

00:14:14,630 --> 00:14:18,560
the menacing smile I definitely

00:14:16,640 --> 00:14:23,330
recommend googling stock photos of

00:14:18,560 --> 00:14:25,220
hackers it's it's a good time but but

00:14:23,330 --> 00:14:27,500
you know scary hackers like this guy can

00:14:25,220 --> 00:14:29,240
steal or buy your passwords from hacked

00:14:27,500 --> 00:14:31,460
websites and they can use those in a

00:14:29,240 --> 00:14:34,910
process called credential stuffing to

00:14:31,460 --> 00:14:37,460
try to get access to your financial

00:14:34,910 --> 00:14:39,170
information to your email to purchase

00:14:37,460 --> 00:14:41,060
very expensive Apple products on your

00:14:39,170 --> 00:14:42,830
iCloud account all that kind of fun

00:14:41,060 --> 00:14:45,500
stuff that's going to inconvenience you

00:14:42,830 --> 00:14:47,380
or lose you a lot of money and so you

00:14:45,500 --> 00:14:49,730
might hope that people don't do this

00:14:47,380 --> 00:14:52,160
that people don't have passwords as

00:14:49,730 --> 00:14:54,290
stupid as one two three four five six so

00:14:52,160 --> 00:14:58,640
they don't write it on a sticky note but

00:14:54,290 --> 00:15:00,380
of course they do and so we we happen to

00:14:58,640 --> 00:15:02,570
know that this password one two three

00:15:00,380 --> 00:15:05,870
four or five six has been seen over 20

00:15:02,570 --> 00:15:07,760
million times in data breaches and this

00:15:05,870 --> 00:15:09,410
is according to a website called have I

00:15:07,760 --> 00:15:11,840
been owned how many people have ever

00:15:09,410 --> 00:15:13,730
heard of this website so this is a

00:15:11,840 --> 00:15:15,560
really cool website that you can go to

00:15:13,730 --> 00:15:17,540
and you can put in your email address or

00:15:15,560 --> 00:15:20,180
a password that you use and see if this

00:15:17,540 --> 00:15:22,220
has shown up in a data breach and so

00:15:20,180 --> 00:15:24,230
this is very informative for you because

00:15:22,220 --> 00:15:26,570
if this has happened you know you need

00:15:24,230 --> 00:15:29,120
to change that password you know that

00:15:26,570 --> 00:15:30,620
you need to potentially use different

00:15:29,120 --> 00:15:32,510
passwords on other websites if you were

00:15:30,620 --> 00:15:34,550
using that same pair of credentials for

00:15:32,510 --> 00:15:36,140
other websites that were not only like

00:15:34,550 --> 00:15:37,829
your LinkedIn account which got hacked

00:15:36,140 --> 00:15:39,749
your myspace account which got half

00:15:37,829 --> 00:15:42,509
every account which is basically been

00:15:39,749 --> 00:15:43,769
hacked you probably want to keep track

00:15:42,509 --> 00:15:45,449
of this you can subscribe to this

00:15:43,769 --> 00:15:48,540
website and see if your email ever shows

00:15:45,449 --> 00:15:50,249
up in a data breach and so this is comes

00:15:48,540 --> 00:15:52,739
from a security researcher named Troy

00:15:50,249 --> 00:15:54,540
hunt he is an Australian guy that works

00:15:52,739 --> 00:15:56,819
for Microsoft that rides a jet ski to

00:15:54,540 --> 00:15:58,379
work I don't understand his life but

00:15:56,819 --> 00:16:00,420
anyway he has access to all of these

00:15:58,379 --> 00:16:01,949
millions and millions of credentials and

00:16:00,420 --> 00:16:03,600
he makes them available for people to

00:16:01,949 --> 00:16:05,549
peruse and this is the data that we're

00:16:03,600 --> 00:16:07,049
going to be looking at today and so all

00:16:05,549 --> 00:16:09,600
this data is also available for download

00:16:07,049 --> 00:16:12,029
and so there's like I said a lot of

00:16:09,600 --> 00:16:13,619
these passwords that we can look at and

00:16:12,029 --> 00:16:23,129
so I downloaded all that data and we're

00:16:13,619 --> 00:16:25,019
gonna take a look at that now oh cool so

00:16:23,129 --> 00:16:26,309
what I have here is a pachi Zeppelin

00:16:25,019 --> 00:16:29,459
notebook because anybody worked with

00:16:26,309 --> 00:16:31,860
Zeppelin a couple of people how about

00:16:29,459 --> 00:16:33,480
data bricks does anybody work with data

00:16:31,860 --> 00:16:36,329
bricks Zeppelin is kind of like an open

00:16:33,480 --> 00:16:39,389
source data bricks it's a free project

00:16:36,329 --> 00:16:40,980
that allows you to run Apache spark from

00:16:39,389 --> 00:16:43,049
the interface and it has a spark

00:16:40,980 --> 00:16:44,939
interpreter built into the interface and

00:16:43,049 --> 00:16:45,899
so you don't have to do as much setup as

00:16:44,939 --> 00:16:47,730
you would if you were running this

00:16:45,899 --> 00:16:50,129
directly on your local machine from a

00:16:47,730 --> 00:16:51,839
shell or something like that and so I

00:16:50,129 --> 00:16:53,100
like Zeppelin because it allows me to

00:16:51,839 --> 00:16:54,689
show off the code that I'm going to be

00:16:53,100 --> 00:16:56,670
doing and it also provides that built-in

00:16:54,689 --> 00:16:58,379
spark interpreter if you can see this

00:16:56,670 --> 00:17:00,329
I'm just running this on my localhost

00:16:58,379 --> 00:17:01,799
and so I'm going to be looking up these

00:17:00,329 --> 00:17:03,600
passwords and the first thing that we

00:17:01,799 --> 00:17:06,329
have to do is read in the password data

00:17:03,600 --> 00:17:09,209
and so we can do this using the built in

00:17:06,329 --> 00:17:10,949
spark interpreter spark has built-in

00:17:09,209 --> 00:17:15,539
readers for CSVs

00:17:10,949 --> 00:17:17,159
for json for a few other things so we

00:17:15,539 --> 00:17:22,319
can go ahead and look at the schema of

00:17:17,159 --> 00:17:24,329
this data and we'll see that this is

00:17:22,319 --> 00:17:26,760
coming back with about three columns

00:17:24,329 --> 00:17:28,740
those kind of kind of weird names and so

00:17:26,760 --> 00:17:30,029
we want to go ahead and get the column

00:17:28,740 --> 00:17:32,039
names for those which we can

00:17:30,029 --> 00:17:33,659
conveniently do by setting the option to

00:17:32,039 --> 00:17:37,649
tell this that we want it to read the

00:17:33,659 --> 00:17:39,299
header and so those have more

00:17:37,649 --> 00:17:41,880
interesting headers now so we've got a

00:17:39,299 --> 00:17:43,380
password a hash an account but one other

00:17:41,880 --> 00:17:45,510
thing that I want to do is make that

00:17:43,380 --> 00:17:47,820
count not be a string so that we can

00:17:45,510 --> 00:17:51,020
sort by it and we can do that by forcing

00:17:47,820 --> 00:17:51,020
it to infer the schema

00:17:51,460 --> 00:17:56,960
and once we do that we'll have a more

00:17:53,810 --> 00:17:58,880
useful set of password data so before I

00:17:56,960 --> 00:18:02,150
show this to you though I do want to do

00:17:58,880 --> 00:18:05,810
one more thing and that's take out some

00:18:02,150 --> 00:18:07,430
of these passwords and so it turns out

00:18:05,810 --> 00:18:09,220
that people put a lot of really bad

00:18:07,430 --> 00:18:11,510
words and their passwords and this is a

00:18:09,220 --> 00:18:14,360
professional setting and so I don't want

00:18:11,510 --> 00:18:18,250
to expose those to you and so I have a

00:18:14,360 --> 00:18:18,250
text file that you are not going to see

00:18:18,370 --> 00:18:24,380
and will go ahead and join these

00:18:20,480 --> 00:18:28,010
together and so what we want to do is

00:18:24,380 --> 00:18:31,910
join our passwords with our bad words on

00:18:28,010 --> 00:18:33,970
the the value column so we want to make

00:18:31,910 --> 00:18:41,180
sure that the password if the password

00:18:33,970 --> 00:18:43,370
contains our bad words value column

00:18:41,180 --> 00:18:45,440
values the default column name that it

00:18:43,370 --> 00:18:46,820
will give to a data set and because

00:18:45,440 --> 00:18:48,500
we're reading this in and this only has

00:18:46,820 --> 00:18:50,240
one column that's what it will be and

00:18:48,500 --> 00:18:52,520
the very important thing is that we want

00:18:50,240 --> 00:18:54,620
this to be a left ante join so that

00:18:52,520 --> 00:18:57,380
we're not so that we're excluding these

00:18:54,620 --> 00:19:02,990
from the from the data sense let's go

00:18:57,380 --> 00:19:06,500
ahead and count these so actually that's

00:19:02,990 --> 00:19:09,890
let's do this before we rejoin that so

00:19:06,500 --> 00:19:11,840
let's let's go ahead and count those ok

00:19:09,890 --> 00:19:14,420
so there's there 67 bad words and

00:19:11,840 --> 00:19:17,060
there's about 700,000 passwords and if

00:19:14,420 --> 00:19:18,590
we join these together what do we get I

00:19:17,060 --> 00:19:20,540
just want to make sure that we're not

00:19:18,590 --> 00:19:23,450
including the bad things in here okay so

00:19:20,540 --> 00:19:26,000
we got rid of about you know two hundred

00:19:23,450 --> 00:19:27,950
or twenty twenty thousand at least the

00:19:26,000 --> 00:19:30,770
bad words passwords in here so that's a

00:19:27,950 --> 00:19:32,330
good good for you to know so anyway now

00:19:30,770 --> 00:19:35,330
that we have this I can go ahead and

00:19:32,330 --> 00:19:37,250
sort this we want to do this by the most

00:19:35,330 --> 00:19:39,290
popular passwords and so we'll do that

00:19:37,250 --> 00:19:42,260
by a descending count this is the sequel

00:19:39,290 --> 00:19:45,020
like DSL that spark provides with spark

00:19:42,260 --> 00:19:47,150
sequel and then it also provides this

00:19:45,020 --> 00:19:50,840
built-in show function which i think is

00:19:47,150 --> 00:19:52,730
pretty nifty and so this is us what the

00:19:50,840 --> 00:19:54,470
data looks like you can see the most

00:19:52,730 --> 00:19:57,680
popular passwords things like one two

00:19:54,470 --> 00:20:00,080
three four five six the word QWERTY I

00:19:57,680 --> 00:20:03,440
love you is up there that's that's nice

00:20:00,080 --> 00:20:04,460
but don't use it as your password so as

00:20:03,440 --> 00:20:05,570
you can see these are like

00:20:04,460 --> 00:20:07,310
people start to think that they're

00:20:05,570 --> 00:20:09,500
getting clever down here they're like oh

00:20:07,310 --> 00:20:11,840
maybe if I like mixing the letters with

00:20:09,500 --> 00:20:13,370
the numbers nobody'll know but you're

00:20:11,840 --> 00:20:16,910
not as actually as clever as you think

00:20:13,370 --> 00:20:19,310
you are so just keep that in mind so we

00:20:16,910 --> 00:20:21,320
can see the most popular passwords um oh

00:20:19,310 --> 00:20:23,360
dragon I like that I wonder if Game of

00:20:21,320 --> 00:20:25,460
Thrones had anything to do with that so

00:20:23,360 --> 00:20:26,990
what we can do now is we can start to do

00:20:25,460 --> 00:20:28,700
some different type of analysis on our

00:20:26,990 --> 00:20:30,200
on our passwords and so I want to look

00:20:28,700 --> 00:20:32,540
at what the most common lengths of

00:20:30,200 --> 00:20:33,950
passwords are and that's not in our data

00:20:32,540 --> 00:20:35,480
set but we can use some built-in

00:20:33,950 --> 00:20:37,520
functionality of spark to get that

00:20:35,480 --> 00:20:40,160
information and so we'll go ahead and

00:20:37,520 --> 00:20:41,750
add a column to our data and so we're

00:20:40,160 --> 00:20:43,310
gonna call that the length column I'm

00:20:41,750 --> 00:20:45,980
gonna go ahead and use the built in

00:20:43,310 --> 00:20:47,240
length operator from sparks equal to

00:20:45,980 --> 00:20:51,140
look at the length of our password

00:20:47,240 --> 00:20:53,180
column and so once we have that one of

00:20:51,140 --> 00:20:55,880
the really cool things that we can do is

00:20:53,180 --> 00:20:58,130
go ahead and run raw sequel on top of

00:20:55,880 --> 00:21:00,560
our data frame and this is built into

00:20:58,130 --> 00:21:02,120
Zeppelin sequel or spark comes with

00:21:00,560 --> 00:21:03,770
different interpreters but inside of

00:21:02,120 --> 00:21:05,630
Zeppelin you can say hey I want to

00:21:03,770 --> 00:21:07,490
change this up I want to run raw sequel

00:21:05,630 --> 00:21:09,560
now instead of running the default Scala

00:21:07,490 --> 00:21:13,040
and spark interpreter so you can write

00:21:09,560 --> 00:21:17,660
your sequel query you can do like you

00:21:13,040 --> 00:21:19,370
Atwood would just raw sequel and we're

00:21:17,660 --> 00:21:23,420
going to be selecting from our passwords

00:21:19,370 --> 00:21:29,540
and then we'll group by and order by our

00:21:23,420 --> 00:21:31,970
length and I did not save this as a

00:21:29,540 --> 00:21:36,200
temporary table so we do need to create

00:21:31,970 --> 00:21:37,940
a temporary view so that spark can index

00:21:36,200 --> 00:21:41,500
this and we'll go ahead and name that

00:21:37,940 --> 00:21:41,500
password sorry that's a little cut off

00:21:41,530 --> 00:21:45,670
so I can rerun this and this will create

00:21:43,970 --> 00:21:48,350
a temporary view than the sequel

00:21:45,670 --> 00:21:49,970
interpreter can read from and now we'll

00:21:48,350 --> 00:21:51,680
be able to run this and get some

00:21:49,970 --> 00:21:54,080
information back about the most common

00:21:51,680 --> 00:21:55,700
password lengths and so one of the cool

00:21:54,080 --> 00:21:57,920
things about Zeppelin is now you have

00:21:55,700 --> 00:21:59,450
the table view but you also have some of

00:21:57,920 --> 00:22:01,790
these other built-in views I don't

00:21:59,450 --> 00:22:03,500
really like pie charts but this bar

00:22:01,790 --> 00:22:04,940
chart is pretty useful for getting some

00:22:03,500 --> 00:22:06,110
information about the most common links

00:22:04,940 --> 00:22:08,510
of passwords you can see if they're

00:22:06,110 --> 00:22:10,850
clustered around like the short semi

00:22:08,510 --> 00:22:13,400
mini minimums that we we generally see

00:22:10,850 --> 00:22:14,570
in password input fields like you know

00:22:13,400 --> 00:22:17,520
your password must be at least six

00:22:14,570 --> 00:22:18,540
characters long side note the old

00:22:17,520 --> 00:22:20,390
thing that makes your password more

00:22:18,540 --> 00:22:23,700
secure is generally just make it longer

00:22:20,390 --> 00:22:25,590
so password minimums of six characters

00:22:23,700 --> 00:22:27,390
like almost all of those have been owned

00:22:25,590 --> 00:22:29,040
there's only so many options that you

00:22:27,390 --> 00:22:31,650
can have with a six character password

00:22:29,040 --> 00:22:34,590
so you can see there's 289 million

00:22:31,650 --> 00:22:36,900
passwords in this like truncated data

00:22:34,590 --> 00:22:39,090
set that I'm using for this presentation

00:22:36,900 --> 00:22:39,990
that have been owned so that's pretty

00:22:39,090 --> 00:22:41,820
interesting

00:22:39,990 --> 00:22:43,410
one of the other cool things as you can

00:22:41,820 --> 00:22:45,000
start to do you know you can do where

00:22:43,410 --> 00:22:48,240
clauses in here so you could do where

00:22:45,000 --> 00:22:50,330
password like you know if we wanted to

00:22:48,240 --> 00:22:53,280
say like Scala

00:22:50,330 --> 00:22:55,230
and then one of the very nifty things

00:22:53,280 --> 00:22:57,870
about Zeppelin specifically is then you

00:22:55,230 --> 00:22:59,850
can start to do a variable replacement

00:22:57,870 --> 00:23:01,230
in here and so you can go ahead and do

00:22:59,850 --> 00:23:05,070
something like that like you would with

00:23:01,230 --> 00:23:06,570
a variable in in Scala and then you have

00:23:05,070 --> 00:23:08,160
this field that comes up and so then you

00:23:06,570 --> 00:23:09,840
can go ahead and change this to be you

00:23:08,160 --> 00:23:11,370
know if you wanted to see if that if New

00:23:09,840 --> 00:23:14,310
York could ever showed up in here you

00:23:11,370 --> 00:23:16,320
can start to edit that and then you can

00:23:14,310 --> 00:23:17,730
use this as a dashboard and hand it off

00:23:16,320 --> 00:23:19,470
to people that maybe don't even know

00:23:17,730 --> 00:23:21,780
sequel or you know management that

00:23:19,470 --> 00:23:23,280
doesn't want to write sequel or people

00:23:21,780 --> 00:23:25,110
that are you know the business analysts

00:23:23,280 --> 00:23:26,970
that maybe you're starting to see

00:23:25,110 --> 00:23:28,770
there's a lot of options here for making

00:23:26,970 --> 00:23:29,940
this type of data accessible and that's

00:23:28,770 --> 00:23:32,190
one of the things that I really really

00:23:29,940 --> 00:23:33,660
appreciate about SPARC is that it has

00:23:32,190 --> 00:23:36,720
all these different ways for people to

00:23:33,660 --> 00:23:38,580
start using the same tool to read your

00:23:36,720 --> 00:23:41,100
data and to gain insights from your data

00:23:38,580 --> 00:23:43,140
based on the tool set that they know and

00:23:41,100 --> 00:23:45,420
so I really love that feature about

00:23:43,140 --> 00:23:47,580
about to spark sequel and how you can

00:23:45,420 --> 00:23:49,080
run the raw sequel on top of it do the

00:23:47,580 --> 00:23:50,670
variable replacement and Zeppelin

00:23:49,080 --> 00:23:53,280
there's a lot of cool stuff that you can

00:23:50,670 --> 00:23:54,600
do with that but one of the more

00:23:53,280 --> 00:23:55,950
interesting things about looking at

00:23:54,600 --> 00:23:57,960
password is looking at the actual

00:23:55,950 --> 00:23:59,940
contents of the passwords themselves and

00:23:57,960 --> 00:24:01,800
I think proper nouns are generally a

00:23:59,940 --> 00:24:04,590
good place for us to start with that and

00:24:01,800 --> 00:24:05,820
so there's you know different approaches

00:24:04,590 --> 00:24:08,190
you could take with that you could do

00:24:05,820 --> 00:24:11,910
you know famous people or geographic

00:24:08,190 --> 00:24:15,780
areas I'm gonna look at dog names and so

00:24:11,910 --> 00:24:17,640
I have a JSON file that has some dog

00:24:15,780 --> 00:24:21,800
names in it I'm gonna go ahead and read

00:24:17,640 --> 00:24:21,800
that in and we'll show the CIMA for that

00:24:22,040 --> 00:24:28,020
so the this JSON file has you know two

00:24:25,320 --> 00:24:29,270
two columns that are inferred and so I

00:24:28,020 --> 00:24:32,300
can go ahead

00:24:29,270 --> 00:24:33,770
look at this and if I want to go ahead

00:24:32,300 --> 00:24:35,510
and edit this one of the really cool

00:24:33,770 --> 00:24:37,880
things about the schema inference is

00:24:35,510 --> 00:24:40,130
that if I give max here a favorite food

00:24:37,880 --> 00:24:42,950
and if I say his favorite food is

00:24:40,130 --> 00:24:45,440
chicken and I go ahead and save that

00:24:42,950 --> 00:24:48,470
file spark does expect that JSON files

00:24:45,440 --> 00:24:51,230
are a single line per JSON and so it

00:24:48,470 --> 00:24:52,760
won't read pretty printed JSON but if I

00:24:51,230 --> 00:24:56,390
go ahead and rerun that and print the

00:24:52,760 --> 00:24:59,840
schema even though only one column had

00:24:56,390 --> 00:25:01,310
or one row entry had that variable in it

00:24:59,840 --> 00:25:03,380
it will still infer that that's a

00:25:01,310 --> 00:25:05,750
nullable column that's now available in

00:25:03,380 --> 00:25:07,430
your data set and so you know this this

00:25:05,750 --> 00:25:09,110
data set that I'm using is a very small

00:25:07,430 --> 00:25:10,700
JSON file but you can imagine if you've

00:25:09,110 --> 00:25:12,770
got millions and millions of lines of

00:25:10,700 --> 00:25:14,960
log files if you're using schema

00:25:12,770 --> 00:25:16,790
inference on those you can gain a lot of

00:25:14,960 --> 00:25:18,920
insights into what your data looks like

00:25:16,790 --> 00:25:20,780
if there's new columns that appear if

00:25:18,920 --> 00:25:22,340
there's columns all sudden disappear

00:25:20,780 --> 00:25:24,350
that you no longer have to keep track of

00:25:22,340 --> 00:25:26,720
there's some really useful stuff that

00:25:24,350 --> 00:25:28,040
spark can do for you there to do the

00:25:26,720 --> 00:25:29,390
schema inference so that you don't have

00:25:28,040 --> 00:25:31,340
to keep track of what all of your data

00:25:29,390 --> 00:25:32,810
looks like because I know if your data

00:25:31,340 --> 00:25:36,200
is anything like the data I've had to

00:25:32,810 --> 00:25:37,370
deal with it's terribly unpredictable so

00:25:36,200 --> 00:25:39,770
let's go ahead and see how many people

00:25:37,370 --> 00:25:43,250
use dog names in their passwords let's

00:25:39,770 --> 00:25:47,030
go ahead and join those together so we

00:25:43,250 --> 00:25:49,340
can join the dogs with the passwords and

00:25:47,030 --> 00:25:51,050
this is going to be something similar to

00:25:49,340 --> 00:25:54,310
what we were doing before and so if we

00:25:51,050 --> 00:26:01,520
see like the passwords password column

00:25:54,310 --> 00:26:03,350
if that contains our dog name and we we

00:26:01,520 --> 00:26:07,420
will default to this being an inner join

00:26:03,350 --> 00:26:07,420
on this case and so we can order by

00:26:07,750 --> 00:26:12,620
descending count and in this case I'm

00:26:10,790 --> 00:26:18,140
going to select only the columns that I

00:26:12,620 --> 00:26:23,330
care about which is the name the count

00:26:18,140 --> 00:26:26,150
and we'll throw in the password and so

00:26:23,330 --> 00:26:32,360
now what we can do is go ahead and show

00:26:26,150 --> 00:26:34,820
this and so you can see that charlie is

00:26:32,360 --> 00:26:37,330
a very popular dog amongst its its

00:26:34,820 --> 00:26:39,730
password een owners

00:26:37,330 --> 00:26:41,290
and there's over 15,000 Charlie's that

00:26:39,730 --> 00:26:43,150
have been seen in password data breaches

00:26:41,290 --> 00:26:45,070
but one of the other things that you can

00:26:43,150 --> 00:26:47,350
do is you can start to manipulate your

00:26:45,070 --> 00:26:49,660
data to do some normalization on top of

00:26:47,350 --> 00:26:52,510
n so if you wanted to write a normalized

00:26:49,660 --> 00:26:55,030
function you could use this doing the

00:26:52,510 --> 00:26:58,030
built-in UDF operator and so this is a

00:26:55,030 --> 00:27:00,700
little bit like Scala definition syntax

00:26:58,030 --> 00:27:03,130
a little stranger though and so what

00:27:00,700 --> 00:27:05,110
you'll do is so this is a user-defined

00:27:03,130 --> 00:27:07,000
function in sequel and this will

00:27:05,110 --> 00:27:08,740
translate on top of spark sequel to do a

00:27:07,000 --> 00:27:11,050
lot of operations and transformations

00:27:08,740 --> 00:27:12,760
for you then maybe aren't available as

00:27:11,050 --> 00:27:15,160
built-in functions like something like

00:27:12,760 --> 00:27:18,490
length was and so in our case we're just

00:27:15,160 --> 00:27:20,200
going to call to lowercase on this and

00:27:18,490 --> 00:27:23,530
now what we can do though is we can go

00:27:20,200 --> 00:27:27,520
ahead and say we'll normalize this

00:27:23,530 --> 00:27:31,180
password and this password or this name

00:27:27,520 --> 00:27:33,220
and so then if we run this again we'll

00:27:31,180 --> 00:27:34,900
see a lot more data come back because

00:27:33,220 --> 00:27:37,690
we're starting to look at data that's no

00:27:34,900 --> 00:27:39,370
longer case sensitive and so once we add

00:27:37,690 --> 00:27:41,500
that functionality and all of a sudden

00:27:39,370 --> 00:27:42,910
that amount of owned Charlie's and

00:27:41,500 --> 00:27:44,710
passwords goes up to you know two

00:27:42,910 --> 00:27:48,310
hundred and seventy eight thousand

00:27:44,710 --> 00:27:49,690
passwords that we've seen all right I'm

00:27:48,310 --> 00:27:53,130
gonna stop there and jump back into the

00:27:49,690 --> 00:27:53,130
presentation we can wrap it up

00:27:54,850 --> 00:28:01,090
I think if we've learned anything from

00:27:58,350 --> 00:28:04,240
this so far it's that we we shouldn't

00:28:01,090 --> 00:28:06,520
use dog names and passwords as much as

00:28:04,240 --> 00:28:08,410
we may love our puppies I think it's

00:28:06,520 --> 00:28:10,360
probably not the best way for you to

00:28:08,410 --> 00:28:12,310
start securing your online identities

00:28:10,360 --> 00:28:14,890
because again it goes back to something

00:28:12,310 --> 00:28:18,340
that's really easy to guess using the

00:28:14,890 --> 00:28:21,250
suffix 1-2-3 is incredibly common you

00:28:18,340 --> 00:28:22,600
can see that by looking at the data and

00:28:21,250 --> 00:28:24,520
that's one of the reasons that all those

00:28:22,600 --> 00:28:26,290
password managers or password input

00:28:24,520 --> 00:28:28,570
forms that are like oh and you have to

00:28:26,290 --> 00:28:30,280
require a numeric character is really

00:28:28,570 --> 00:28:32,020
stupid because people use the same

00:28:30,280 --> 00:28:33,970
numeric characters and all of their

00:28:32,020 --> 00:28:36,130
stuff it's like one two three one

00:28:33,970 --> 00:28:38,050
exclamation point admit it you've done

00:28:36,130 --> 00:28:39,880
it if you're not using a password

00:28:38,050 --> 00:28:45,100
manager we're all falling back to the

00:28:39,880 --> 00:28:46,180
same same tropes and but but some of the

00:28:45,100 --> 00:28:47,980
things that I think we've noticed about

00:28:46,180 --> 00:28:50,200
benefits of SPARC include the fact that

00:28:47,980 --> 00:28:52,150
SPARC is incredibly fast its

00:28:50,200 --> 00:28:53,440
be flexible and it gets you up and

00:28:52,150 --> 00:28:55,480
running and a lot of insights very

00:28:53,440 --> 00:28:56,920
quickly and I think one of the things

00:28:55,480 --> 00:28:59,020
that you know might not have come across

00:28:56,920 --> 00:29:00,850
as much in this presentation but usually

00:28:59,020 --> 00:29:02,590
definite benefit of spark is that it is

00:29:00,850 --> 00:29:05,260
proven for large systems there are very

00:29:02,590 --> 00:29:07,270
large companies that are using spark to

00:29:05,260 --> 00:29:08,710
do a lot of analysis Twilio uses a lot

00:29:07,270 --> 00:29:11,050
of spark internally to do our data

00:29:08,710 --> 00:29:13,380
analysis to do you know fraud detection

00:29:11,050 --> 00:29:15,700
and other things around you know

00:29:13,380 --> 00:29:17,560
detecting you know call drops and that

00:29:15,700 --> 00:29:19,780
kind of things all right so there's a

00:29:17,560 --> 00:29:22,240
lot of use cases for spark at really

00:29:19,780 --> 00:29:25,090
really large scale and it's just as easy

00:29:22,240 --> 00:29:28,840
to use once you get it drop and running

00:29:25,090 --> 00:29:31,090
as it is um you know 100 megabytes of

00:29:28,840 --> 00:29:33,610
data as it is you know 100 terabytes of

00:29:31,090 --> 00:29:36,640
data of course

00:29:33,610 --> 00:29:38,200
spark is not without its challenges I

00:29:36,640 --> 00:29:39,970
think one of the things that we run into

00:29:38,200 --> 00:29:42,210
often is that there are very opaque

00:29:39,970 --> 00:29:44,230
error messages that come up with spark

00:29:42,210 --> 00:29:46,450
especially if you're using any of the

00:29:44,230 --> 00:29:48,610
interpreters outside of Scala the error

00:29:46,450 --> 00:29:50,980
messages get wrapped in JVM stack traces

00:29:48,610 --> 00:29:53,200
and so if you're using this Python

00:29:50,980 --> 00:29:55,360
interpreter you're gonna get like JVM

00:29:53,200 --> 00:29:57,310
stack traces wrapped in Python error

00:29:55,360 --> 00:30:00,490
messages wrapped in JVM wrapped in it's

00:29:57,310 --> 00:30:02,140
it's a nightmare and the the error

00:30:00,490 --> 00:30:04,060
messages are getting better every day

00:30:02,140 --> 00:30:06,430
but it's still something that can be

00:30:04,060 --> 00:30:08,170
pretty confusing to look at for the

00:30:06,430 --> 00:30:09,790
first time and track down especially if

00:30:08,170 --> 00:30:11,200
you're running something on a lot of

00:30:09,790 --> 00:30:12,970
different nodes and maybe there was only

00:30:11,200 --> 00:30:15,070
a failure on one of your worker nodes

00:30:12,970 --> 00:30:18,640
and you might not be able to track that

00:30:15,070 --> 00:30:20,590
down very easily operationalizing this

00:30:18,640 --> 00:30:22,030
stuff is can be challenging and I

00:30:20,590 --> 00:30:23,590
included a link on this page and again

00:30:22,030 --> 00:30:25,720
at the end of this notebook about how to

00:30:23,590 --> 00:30:29,260
deploy spark on a cluster which Heather

00:30:25,720 --> 00:30:30,910
Miller wrote a couple months ago there's

00:30:29,260 --> 00:30:33,130
a really good overview of how to do a

00:30:30,910 --> 00:30:34,810
lot of this stuff if you want to do it

00:30:33,130 --> 00:30:36,880
at scale everything I just showed you is

00:30:34,810 --> 00:30:38,500
running on my local machine for the sake

00:30:36,880 --> 00:30:40,390
of making sure that the demo works in

00:30:38,500 --> 00:30:41,770
front of an audience but obviously

00:30:40,390 --> 00:30:44,230
that's not what you're going to be using

00:30:41,770 --> 00:30:45,760
spark for most is the time and so if you

00:30:44,230 --> 00:30:47,740
want to get this up and running on top

00:30:45,760 --> 00:30:49,780
of a on top of a cluster there is

00:30:47,740 --> 00:30:50,830
documentation for how to do that but

00:30:49,780 --> 00:30:53,080
it's something that didn't really exist

00:30:50,830 --> 00:30:54,360
until a couple months ago and so you

00:30:53,080 --> 00:30:57,190
might have some challenges there

00:30:54,360 --> 00:30:59,290
including leave the documentation could

00:30:57,190 --> 00:31:01,450
use some work there there are some

00:30:59,290 --> 00:31:03,580
documents out there that are kiddie and

00:31:01,450 --> 00:31:04,350
updated and have a lot of maybe

00:31:03,580 --> 00:31:05,850
unofficial

00:31:04,350 --> 00:31:07,500
documentation that are still like the

00:31:05,850 --> 00:31:10,650
go-to documentation now that I'll point

00:31:07,500 --> 00:31:14,970
out so this is an example of a stack

00:31:10,650 --> 00:31:16,710
trace so this is I was trying to join

00:31:14,970 --> 00:31:18,929
something and this was just on my local

00:31:16,710 --> 00:31:20,220
machine and there was really long stack

00:31:18,929 --> 00:31:21,929
trace and it goes on for you know a

00:31:20,220 --> 00:31:23,760
couple hundred lines and it's really

00:31:21,929 --> 00:31:27,150
hard to tell what the actual problem was

00:31:23,760 --> 00:31:29,340
here but you know this is the example of

00:31:27,150 --> 00:31:32,520
some of the very opaque error messages

00:31:29,340 --> 00:31:34,650
that you will see in terms of the the

00:31:32,520 --> 00:31:36,750
missing spark documentation the spark

00:31:34,650 --> 00:31:38,549
docs themselves I think are low on

00:31:36,750 --> 00:31:41,070
examples I'm definitely somewhat of the

00:31:38,549 --> 00:31:43,980
learns by a lot by example code but

00:31:41,070 --> 00:31:45,720
y'all suck here has written the mastery

00:31:43,980 --> 00:31:48,270
and Apache spark and the mastering

00:31:45,720 --> 00:31:49,740
Apache sparks equal guides which really

00:31:48,270 --> 00:31:51,270
in my opinion are the missing spark

00:31:49,740 --> 00:31:54,390
documentation and if you're somebody

00:31:51,270 --> 00:31:56,159
that is in the spark ecosystem a lot

00:31:54,390 --> 00:31:57,929
definitely bookmark this because you'll

00:31:56,159 --> 00:32:00,000
find a lot of great examples a lot of

00:31:57,929 --> 00:32:02,220
great how-tos for getting around the

00:32:00,000 --> 00:32:09,809
project that you might not find in the

00:32:02,220 --> 00:32:11,220
official documentation wrapping up I

00:32:09,809 --> 00:32:13,380
want to leave you thinking about some of

00:32:11,220 --> 00:32:15,900
the implications of big data insecurity

00:32:13,380 --> 00:32:18,000
I it's a useful tool for finding

00:32:15,900 --> 00:32:20,220
vulnerabilities for filing for

00:32:18,000 --> 00:32:22,710
discovering fraud in your systems but

00:32:20,220 --> 00:32:24,419
you much as you can use it for gaining

00:32:22,710 --> 00:32:27,900
insights about your data you could also

00:32:24,419 --> 00:32:30,840
be setting yourself up to be the next

00:32:27,900 --> 00:32:35,159
data breach data privacy is on

00:32:30,840 --> 00:32:37,289
everybody's mind I think especially with

00:32:35,159 --> 00:32:38,580
with gdpr that just happened there's

00:32:37,289 --> 00:32:41,220
there's a lot of people that have had

00:32:38,580 --> 00:32:43,650
been thinking about this non-stop for

00:32:41,220 --> 00:32:45,600
the last six months there's a new data

00:32:43,650 --> 00:32:47,309
breach that happens every day with

00:32:45,600 --> 00:32:49,169
Cambridge analytic on all the data

00:32:47,309 --> 00:32:50,760
privacy issues like people are really

00:32:49,169 --> 00:32:53,220
thinking about this more now than ever

00:32:50,760 --> 00:32:54,990
and as much as big data is a useful tool

00:32:53,220 --> 00:32:58,710
we want to be mindful of the way that we

00:32:54,990 --> 00:33:02,039
store our data and how you have access

00:32:58,710 --> 00:33:03,659
to it and what data you are storing you

00:33:02,039 --> 00:33:06,390
know I think it's very easy for us to

00:33:03,659 --> 00:33:08,220
say we want to store everything because

00:33:06,390 --> 00:33:09,720
maybe we'll need it for analytics like

00:33:08,220 --> 00:33:11,880
maybe this will be a useful parameter

00:33:09,720 --> 00:33:13,440
for our machine learning model but

00:33:11,880 --> 00:33:15,270
honestly like the more data that you're

00:33:13,440 --> 00:33:16,940
storing and the more there is potential

00:33:15,270 --> 00:33:19,190
for that to be compromised

00:33:16,940 --> 00:33:20,539
and I think that that you know every

00:33:19,190 --> 00:33:22,999
additional piece of personal information

00:33:20,539 --> 00:33:25,330
that you store especially is setting

00:33:22,999 --> 00:33:27,679
yourself up for a lot more risk

00:33:25,330 --> 00:33:29,840
especially because nobody's security is

00:33:27,679 --> 00:33:32,269
perfect as much as you might invest in

00:33:29,840 --> 00:33:35,769
your security in terms of your OPSEC

00:33:32,269 --> 00:33:38,389
your InfoSec your whatever sec it's it's

00:33:35,769 --> 00:33:40,849
if you have any influence over the

00:33:38,389 --> 00:33:42,649
security at your company including and

00:33:40,849 --> 00:33:45,349
especially if your company is too small

00:33:42,649 --> 00:33:46,820
to have a dedicated security team make

00:33:45,349 --> 00:33:50,599
sure you're storing your passwords in a

00:33:46,820 --> 00:33:52,519
safe and smart way don't roll your own

00:33:50,599 --> 00:33:54,590
crypto use somebody else's crypto

00:33:52,519 --> 00:33:56,899
library there are several that are very

00:33:54,590 --> 00:33:58,700
well proven in Java I know we hate Java

00:33:56,899 --> 00:33:59,840
libraries but use the Java crypto

00:33:58,700 --> 00:34:02,479
libraries

00:33:59,840 --> 00:34:06,679
they've been around for decades and they

00:34:02,479 --> 00:34:08,289
can they can help you out so be the

00:34:06,679 --> 00:34:10,669
security advocate at your company

00:34:08,289 --> 00:34:12,500
because all that password data that we

00:34:10,669 --> 00:34:14,329
looked at was available because those

00:34:12,500 --> 00:34:16,129
passwords got hacked and they were

00:34:14,329 --> 00:34:19,909
stored in a way that did that made it

00:34:16,129 --> 00:34:21,109
easy to back them out or to see them in

00:34:19,909 --> 00:34:24,079
plaintext in a way that they're

00:34:21,109 --> 00:34:27,200
accessible so this suite if you haven't

00:34:24,079 --> 00:34:29,539
seen it was from a t-mobile Australia

00:34:27,200 --> 00:34:33,230
they it was discovered that t-mobile or

00:34:29,539 --> 00:34:35,210
at least the Austria branch was with

00:34:33,230 --> 00:34:37,309
storing passwords in plain text and

00:34:35,210 --> 00:34:39,230
unfortunately the support agent really

00:34:37,309 --> 00:34:42,950
really doubled down on the impossibility

00:34:39,230 --> 00:34:45,289
of that being a problem which i think is

00:34:42,950 --> 00:34:48,649
you know engineers we know these things

00:34:45,289 --> 00:34:54,079
aren't perfect then nobody's security is

00:34:48,649 --> 00:34:55,399
perfect but but don't be like half so so

00:34:54,079 --> 00:34:57,200
that's kind of from the company side of

00:34:55,399 --> 00:34:59,990
things I hope I'm preaching to the choir

00:34:57,200 --> 00:35:02,029
here by the way this guy we've got

00:34:59,990 --> 00:35:06,170
gloves and sunglasses and there's like

00:35:02,029 --> 00:35:09,799
money on the keyboard he's up to

00:35:06,170 --> 00:35:11,390
something good so as a consumer the best

00:35:09,799 --> 00:35:13,069
things that you can do for yourself use

00:35:11,390 --> 00:35:15,980
a password manager these will generate

00:35:13,069 --> 00:35:17,569
long and random and unique passwords for

00:35:15,980 --> 00:35:20,589
the sites that you visit so that even if

00:35:17,569 --> 00:35:22,309
one is compromised you won't have the

00:35:20,589 --> 00:35:24,140
vulnerability you're not opening

00:35:22,309 --> 00:35:25,819
yourself up to the kind of credential

00:35:24,140 --> 00:35:27,770
stuffing that we would see with insecure

00:35:25,819 --> 00:35:29,450
short passwords and turning on

00:35:27,770 --> 00:35:30,770
two-factor authentication wherever it's

00:35:29,450 --> 00:35:34,760
possible

00:35:30,770 --> 00:35:37,790
these two things alone will do a lot to

00:35:34,760 --> 00:35:39,290
secure your online identity security

00:35:37,790 --> 00:35:40,820
through obscurity like most of you are

00:35:39,290 --> 00:35:42,470
probably not going to be the targets of

00:35:40,820 --> 00:35:44,180
hackers unless you're like tweeting

00:35:42,470 --> 00:35:47,420
about the fact that you have 100 Bitcoin

00:35:44,180 --> 00:35:48,860
in coinbase don't do that

00:35:47,420 --> 00:35:50,330
so one of the things that I like to talk

00:35:48,860 --> 00:35:52,670
about is my dad uses a

00:35:50,330 --> 00:35:55,730
password-protected Excel spreadsheet to

00:35:52,670 --> 00:35:57,350
store his passwords yeah I mean it's

00:35:55,730 --> 00:35:58,880
it's kind of funny but like honestly I'm

00:35:57,350 --> 00:36:00,350
pretty proud of him because the

00:35:58,880 --> 00:36:03,440
important thing there is that there's a

00:36:00,350 --> 00:36:05,630
security mindset there he has a an

00:36:03,440 --> 00:36:08,360
inventory of what his passwords are and

00:36:05,630 --> 00:36:10,400
he's thinking about it he's trying to be

00:36:08,360 --> 00:36:13,580
more secure and nobody's gonna break

00:36:10,400 --> 00:36:15,890
into my dad's you know house and and you

00:36:13,580 --> 00:36:17,780
know try to look at his stuff probably

00:36:15,890 --> 00:36:20,210
and so for him that's probably a good

00:36:17,780 --> 00:36:22,400
enough solution you know I'll get him on

00:36:20,210 --> 00:36:23,930
LastPass eventually but I think the

00:36:22,400 --> 00:36:25,820
security mindset is there and that's

00:36:23,930 --> 00:36:27,350
often the biggest hurdle that people

00:36:25,820 --> 00:36:29,210
need to overcome in order to start

00:36:27,350 --> 00:36:33,680
getting into some of these security

00:36:29,210 --> 00:36:35,480
practices so thank you for letting me

00:36:33,680 --> 00:36:37,400
spend some time with you diving into

00:36:35,480 --> 00:36:40,040
spark especially through the lens of own

00:36:37,400 --> 00:36:43,660
passwords I love chatting about this

00:36:40,040 --> 00:36:46,940
stuff so if you want to keep talking or

00:36:43,660 --> 00:36:48,320
have any any ideas for me hopefully I've

00:36:46,940 --> 00:36:50,930
inspired some of you to dig into your

00:36:48,320 --> 00:36:54,200
own data with spark I don't even know

00:36:50,930 --> 00:36:56,630
what this one like there's a wig and a

00:36:54,200 --> 00:36:58,640
skeleton but the hands are normal and I

00:36:56,630 --> 00:37:01,460
think she's like drinking tea anyway

00:36:58,640 --> 00:37:02,510
this is like my alter ego I think so if

00:37:01,460 --> 00:37:04,520
you have big data

00:37:02,510 --> 00:37:06,350
I think spark an especially spark sequel

00:37:04,520 --> 00:37:08,150
are really great tools for doing that

00:37:06,350 --> 00:37:09,770
analysis and for getting your team on

00:37:08,150 --> 00:37:14,270
the same page in terms of the tools that

00:37:09,770 --> 00:37:16,520
you use to digest transform and analyze

00:37:14,270 --> 00:37:19,370
the big data that you have I will post

00:37:16,520 --> 00:37:21,800
these slides after this presentation and

00:37:19,370 --> 00:37:23,270
I have a tutorial for all of this at

00:37:21,800 --> 00:37:26,210
fingers cross will be post on the blog

00:37:23,270 --> 00:37:27,350
our Tullio blog next week come find me

00:37:26,210 --> 00:37:29,810
after this if you have any questions

00:37:27,350 --> 00:37:31,670
about Scala spark or security once again

00:37:29,810 --> 00:37:32,690
my name is Kelly Robinson and thank you

00:37:31,670 --> 00:37:40,430
for listening

00:37:32,690 --> 00:37:45,109
[Applause]

00:37:40,430 --> 00:37:45,109
if there's any questions we have a mic

00:37:48,079 --> 00:37:52,529
I'm just actually really cares because

00:37:50,880 --> 00:37:54,989
they're way more than I was expecting

00:37:52,529 --> 00:37:56,789
the passwords had scala in them could

00:37:54,989 --> 00:37:59,789
you mind showing the most popular ones

00:37:56,789 --> 00:38:01,999
with that real quick so we want to see

00:37:59,789 --> 00:38:04,229
the passwords that have Scala in them

00:38:01,999 --> 00:38:09,890
not not for personal worry of course

00:38:04,229 --> 00:38:09,890
right I mean I mean so we can look at

00:38:19,130 --> 00:38:22,400
so a lot of those were like eight

00:38:20,749 --> 00:38:26,650
characters and so I'm wondering if it's

00:38:22,400 --> 00:38:26,650
like scalable that's it

00:38:34,020 --> 00:38:40,180
Escalade okay I guess a lot of people

00:38:38,560 --> 00:38:44,830
want to put their cars in there and

00:38:40,180 --> 00:38:49,599
they're in their passwords scalawag John

00:38:44,830 --> 00:38:52,680
pretty is that you all right for

00:38:49,599 --> 00:38:56,550
hundreds call developers than you did

00:38:52,680 --> 00:38:56,550

YouTube URL: https://www.youtube.com/watch?v=EaOlOBd7BPM


