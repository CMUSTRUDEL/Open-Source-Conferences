Title: Keeping the “fun” in Apache Spark Datasets and FP by Holden Karau
Publication date: 2018-09-22
Playlist: Scala Days New York 2018
Description: 
	This video was recorded at Scala Days New York 2018
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

More information and the abstract can be found here:
https://na.scaladays.org/schedule/keeping-the-fun-in-apache-spark-datasets-and-fp
Captions: 
	00:00:03,610 --> 00:00:08,470
thank you all for coming my name is

00:00:06,490 --> 00:00:11,080
Holden I'm talking about keeping the fun

00:00:08,470 --> 00:00:13,780
in functional spark datasets and

00:00:11,080 --> 00:00:16,269
functional programming this talk may

00:00:13,780 --> 00:00:21,460
sound happy but I assure you it is

00:00:16,269 --> 00:00:22,900
mostly sad and if anyone saw the same

00:00:21,460 --> 00:00:25,359
talk in Berlin

00:00:22,900 --> 00:00:28,810
thanks for coming I've changed three

00:00:25,359 --> 00:00:30,189
slides so if you were in Berlin you may

00:00:28,810 --> 00:00:32,230
want to go experience one of the other

00:00:30,189 --> 00:00:36,280
wonderful talks and I will not feel at

00:00:32,230 --> 00:00:40,780
all bad about you leaving yeah and happy

00:00:36,280 --> 00:00:43,149
pride its pride month and this week is

00:00:40,780 --> 00:00:45,609
pride in San Francisco we have our March

00:00:43,149 --> 00:00:48,309
and super happy fun times and I am

00:00:45,609 --> 00:00:50,769
wearing this delightful tank top with

00:00:48,309 --> 00:00:53,980
the trans pride logo mashed-up with the

00:00:50,769 --> 00:00:55,989
Scala logo that Heather made and if you

00:00:53,980 --> 00:01:02,079
are looking to give the Scala if you're

00:00:55,989 --> 00:01:05,530
looking to buy Pride Scala stuff heather

00:01:02,079 --> 00:01:07,330
has the shirts there not these but she

00:01:05,530 --> 00:01:10,120
was selling them and you can you can buy

00:01:07,330 --> 00:01:11,620
them from her on the internet and she

00:01:10,120 --> 00:01:14,740
will deliver them in person at the

00:01:11,620 --> 00:01:16,510
conference yeah ok cool

00:01:14,740 --> 00:01:18,760
so name is Holden my preferred pronouns

00:01:16,510 --> 00:01:20,950
are she or her it's tattooed on my wrist

00:01:18,760 --> 00:01:23,590
in case I forget morning's can be a

00:01:20,950 --> 00:01:26,410
little rough I get it

00:01:23,590 --> 00:01:28,150
I'm a developer advocate at Google it's

00:01:26,410 --> 00:01:31,270
all kinds of fun they pay me money to

00:01:28,150 --> 00:01:36,010
work on open-source big data and travel

00:01:31,270 --> 00:01:37,960
so I like this job I am on the SPARC PMC

00:01:36,010 --> 00:01:40,240
which is like being a spark committer

00:01:37,960 --> 00:01:43,140
except it is really really hard to get

00:01:40,240 --> 00:01:45,820
rid of me now so that's kind of awesome

00:01:43,140 --> 00:01:48,480
and I've worked at a whole bunch of

00:01:45,820 --> 00:01:53,350
other places including IBM Alpine

00:01:48,480 --> 00:01:54,490
dataworks Google I worked on some we

00:01:53,350 --> 00:01:55,540
gonna talk about what I worked on last

00:01:54,490 --> 00:01:57,790
time at Google sad

00:01:55,540 --> 00:01:59,500
Foursquare and Amazon I'm a co-author of

00:01:57,790 --> 00:02:02,350
two books I'm actually a third one that

00:01:59,500 --> 00:02:04,660
I don't mention because it's bad but

00:02:02,350 --> 00:02:06,640
high performance spark I got a much

00:02:04,660 --> 00:02:08,530
better royalty deal on than any of the

00:02:06,640 --> 00:02:12,610
other books and I think it is the best

00:02:08,530 --> 00:02:14,260
book for giving me money if anyone has a

00:02:12,610 --> 00:02:17,080
corporate credit card in the audience

00:02:14,260 --> 00:02:19,750
now is a great time to buy several copy

00:02:17,080 --> 00:02:21,700
of it and I'll remind you at the end

00:02:19,750 --> 00:02:24,370
since it is the most important takeaway

00:02:21,700 --> 00:02:27,640
from this talk you can follow me on

00:02:24,370 --> 00:02:29,320
Twitter and SlideShare one of the things

00:02:27,640 --> 00:02:32,710
which I've started doing somewhat

00:02:29,320 --> 00:02:34,360
recently is code review live streams and

00:02:32,710 --> 00:02:38,230
this is if anyone's interested in seeing

00:02:34,360 --> 00:02:39,730
how a project like spark handles our

00:02:38,230 --> 00:02:43,330
community development process

00:02:39,730 --> 00:02:46,030
specifically in code reviews you can

00:02:43,330 --> 00:02:48,490
watch me review people's code live and

00:02:46,030 --> 00:02:50,110
it's fun and you can you can watch this

00:02:48,490 --> 00:02:52,300
and then you can help us review code

00:02:50,110 --> 00:02:53,980
because there are so many people who

00:02:52,300 --> 00:02:55,720
want to write code and the number of

00:02:53,980 --> 00:02:57,190
people who are like yeah you know what

00:02:55,720 --> 00:03:00,520
my open-source contribution is going to

00:02:57,190 --> 00:03:02,470
be reading other people's code is a much

00:03:00,520 --> 00:03:04,390
smaller list somehow and so I hope to

00:03:02,470 --> 00:03:08,070
trick some of you into reviewing spark

00:03:04,390 --> 00:03:10,750
code because there is way too much of it

00:03:08,070 --> 00:03:12,280
and you can watch videos if you want to

00:03:10,750 --> 00:03:17,080
give me feedback on this talk there's a

00:03:12,280 --> 00:03:19,570
forum I see the results of this forum so

00:03:17,080 --> 00:03:23,080
you know but you can also feel that are

00:03:19,570 --> 00:03:26,050
anonymously if you want and as you may

00:03:23,080 --> 00:03:27,850
have guessed by this lovely shirt in

00:03:26,050 --> 00:03:31,150
addition to Who I am professionally I'm

00:03:27,850 --> 00:03:34,120
trans queer Canadian in America on a

00:03:31,150 --> 00:03:37,420
h-1b work visa and it's a real exciting

00:03:34,120 --> 00:03:40,450
time to be here on a work visa and part

00:03:37,420 --> 00:03:42,100
of the leather community and this is not

00:03:40,450 --> 00:03:44,380
particularly related to functional

00:03:42,100 --> 00:03:46,959
programming for example there is no

00:03:44,380 --> 00:03:49,810
secret Canadian monad guide which were

00:03:46,959 --> 00:03:53,170
given at birth you know there's no

00:03:49,810 --> 00:03:55,380
better garbage collector we all use g1g

00:03:53,170 --> 00:03:57,280
see just like just like the rest of you

00:03:55,380 --> 00:03:58,090
but I think it's important to remember

00:03:57,280 --> 00:04:00,070
that we're all from different

00:03:58,090 --> 00:04:02,050
backgrounds and we all look at problems

00:04:00,070 --> 00:04:03,250
in different ways and if we work

00:04:02,050 --> 00:04:04,330
together we can make some really cool

00:04:03,250 --> 00:04:06,070


00:04:04,330 --> 00:04:08,019
but if we spend our time just kind of

00:04:06,070 --> 00:04:10,269
excluding people we're just gonna make

00:04:08,019 --> 00:04:13,019
the same old boring and we're gonna

00:04:10,269 --> 00:04:15,100
burn the world down with big data and

00:04:13,019 --> 00:04:17,560
we're getting some previews of what that

00:04:15,100 --> 00:04:20,200
looks like and it's not good so try and

00:04:17,560 --> 00:04:25,330
not up cool

00:04:20,200 --> 00:04:27,760
and and yes I really like this I thanks

00:04:25,330 --> 00:04:29,420
Heather for doing this it's really nice

00:04:27,760 --> 00:04:31,910
to see Scala

00:04:29,420 --> 00:04:33,620
to be more open and inclusive explicitly

00:04:31,910 --> 00:04:37,960
of people from all kinds of different

00:04:33,620 --> 00:04:39,380
backgrounds so yes kala and Happiness I

00:04:37,960 --> 00:04:41,690
think it's cool

00:04:39,380 --> 00:04:44,120
okay in addition to myself my

00:04:41,690 --> 00:04:47,090
co-presenter is boo

00:04:44,120 --> 00:04:50,120
this is boo bark she also uses she/her

00:04:47,090 --> 00:04:54,320
pronouns she doesn't have a tattoo and

00:04:50,120 --> 00:04:57,770
she is the author of two books however

00:04:54,320 --> 00:04:59,990
they're currently out of print and my

00:04:57,770 --> 00:05:01,880
wife and I are working well I'll see my

00:04:59,990 --> 00:05:04,130
wife because I don't have access to

00:05:01,880 --> 00:05:06,920
industrial grade machines on the next

00:05:04,130 --> 00:05:08,660
edition of printing for this book and I

00:05:06,920 --> 00:05:11,180
will bring it to one of the scholar

00:05:08,660 --> 00:05:12,650
conferences I promise you can follow her

00:05:11,180 --> 00:05:15,680
on Twitter to find out when her book is

00:05:12,650 --> 00:05:18,800
done or mostly to see her retweet my own

00:05:15,680 --> 00:05:21,740
things with the word bark very exciting

00:05:18,800 --> 00:05:23,420
okay so one of the questions which I

00:05:21,740 --> 00:05:26,960
sometimes get asked is why does Google

00:05:23,420 --> 00:05:30,500
cloud care about spark and so many

00:05:26,960 --> 00:05:33,290
exciting things um while we don't use

00:05:30,500 --> 00:05:36,860
spark internally we have our own like

00:05:33,290 --> 00:05:40,150
special fancy tools some of which spark

00:05:36,860 --> 00:05:43,430
perhaps has drawn some inspiration from

00:05:40,150 --> 00:05:45,080
we think spark is really cool because

00:05:43,430 --> 00:05:46,910
it's taught a lot of people these

00:05:45,080 --> 00:05:48,470
concepts and people are writing a lot of

00:05:46,910 --> 00:05:50,540
code on spark and we would love it if

00:05:48,470 --> 00:05:52,040
you ran it on our cloud and that would

00:05:50,540 --> 00:05:54,650
be a way that we would receive money

00:05:52,040 --> 00:05:56,840
from you in exchange for services which

00:05:54,650 --> 00:05:59,090
I'm told is how the modern economy works

00:05:56,840 --> 00:06:01,400
and so this is why my employer cares

00:05:59,090 --> 00:06:03,200
about this and pays me money I am very

00:06:01,400 --> 00:06:06,160
glad that they give me money in exchange

00:06:03,200 --> 00:06:08,360
for spark I hope they continue to do so

00:06:06,160 --> 00:06:11,480
anyways so I'm hoping you're all

00:06:08,360 --> 00:06:14,930
friendly people if you object to

00:06:11,480 --> 00:06:18,350
pictures of cats or stuffed animals this

00:06:14,930 --> 00:06:20,030
is not the talk for you it's all right

00:06:18,350 --> 00:06:21,740
no worries there I'm sure there is

00:06:20,030 --> 00:06:23,600
someone here being very serious and

00:06:21,740 --> 00:06:25,700
businesslike and you can go find that

00:06:23,600 --> 00:06:28,240
talk I'm really curious is there anyone

00:06:25,700 --> 00:06:33,130
here who is new to the scala community

00:06:28,240 --> 00:06:33,130
yeah welcome new friends

00:06:36,410 --> 00:06:41,430
if if no one raises their hands then we

00:06:39,510 --> 00:06:44,700
should be really terrified because that

00:06:41,430 --> 00:06:46,590
will be very bad news and there's been a

00:06:44,700 --> 00:06:47,970
lot of talk about teaching people at

00:06:46,590 --> 00:06:48,840
this conference which i think is is

00:06:47,970 --> 00:06:51,660
lovely and amazing

00:06:48,840 --> 00:06:55,800
um okay cool so I'm gonna talk about

00:06:51,660 --> 00:06:57,720
what spark is super briefly since I

00:06:55,800 --> 00:06:59,280
assume that most people came here

00:06:57,720 --> 00:07:00,780
because they had some background in

00:06:59,280 --> 00:07:03,720
spark but if you don't that's totally

00:07:00,780 --> 00:07:05,790
fine we'll talk about what data sets

00:07:03,720 --> 00:07:07,800
mean for spark instead of rdd's and

00:07:05,790 --> 00:07:10,200
we'll talk about all of the sad things

00:07:07,800 --> 00:07:13,140
that come from the switch from rdd's to

00:07:10,200 --> 00:07:14,820
data sets and we'll talk about some of

00:07:13,140 --> 00:07:17,790
the cool things as well I don't I don't

00:07:14,820 --> 00:07:19,350
want it to just be a sad talk and we'll

00:07:17,790 --> 00:07:22,230
talk about what we can do to make things

00:07:19,350 --> 00:07:24,120
more awesome for future generations and

00:07:22,230 --> 00:07:25,770
you know hopefully I can even some of

00:07:24,120 --> 00:07:28,920
you to do the work that I just do not

00:07:25,770 --> 00:07:30,210
want to bother doing and so I want to be

00:07:28,920 --> 00:07:33,170
clear I'm gonna talk about a lot of

00:07:30,210 --> 00:07:35,310
things that that kind of suck and

00:07:33,170 --> 00:07:37,410
sometimes when I give talks people are

00:07:35,310 --> 00:07:39,660
like wow that sounds really bad what

00:07:37,410 --> 00:07:41,400
should I use instead of spark and it's

00:07:39,660 --> 00:07:43,380
not that you shouldn't use spark or

00:07:41,400 --> 00:07:46,170
spark is especially bad it's that

00:07:43,380 --> 00:07:48,810
everything is terrible some of us just

00:07:46,170 --> 00:07:50,940
bother to talk about it other people

00:07:48,810 --> 00:07:53,130
like put on a much nicer professional

00:07:50,940 --> 00:07:56,100
image and it's not my style

00:07:53,130 --> 00:07:58,460
okay cool so what is spark it's a

00:07:56,100 --> 00:07:59,700
general purpose distributed system

00:07:58,460 --> 00:08:03,030
data-parallel

00:07:59,700 --> 00:08:03,570
and it's built in Scala yay we all love

00:08:03,030 --> 00:08:05,310
Scala

00:08:03,570 --> 00:08:07,860
it's an Apache project which is really

00:08:05,310 --> 00:08:09,570
awesome it means that if like any one of

00:08:07,860 --> 00:08:11,880
the companies behind spark you know

00:08:09,570 --> 00:08:13,260
decides to stop contributing to it you

00:08:11,880 --> 00:08:15,900
don't have to worry about the entire

00:08:13,260 --> 00:08:18,750
community vanishing you know there's

00:08:15,900 --> 00:08:20,220
there some continuity it's faster than

00:08:18,750 --> 00:08:22,080
hadoop mapreduce and I'd like to give

00:08:20,220 --> 00:08:26,700
Map Reduce a big shout out for setting

00:08:22,080 --> 00:08:29,130
the bar down here and like MapReduce

00:08:26,700 --> 00:08:31,560
totally awesome you know for its time

00:08:29,130 --> 00:08:34,169
but it's it's a lot easier to be faster

00:08:31,560 --> 00:08:36,000
than MapReduce and it's good for when

00:08:34,169 --> 00:08:38,010
problems become too big for a single

00:08:36,000 --> 00:08:40,410
machine or when they're likely to become

00:08:38,010 --> 00:08:41,640
too big for a single machine sometimes

00:08:40,410 --> 00:08:44,700
we're working with small datasets

00:08:41,640 --> 00:08:46,260
initially and spark is easy enough at

00:08:44,700 --> 00:08:48,000
handling and when small data sets if

00:08:46,260 --> 00:08:49,560
it's probably going to

00:08:48,000 --> 00:08:51,720
really quickly when you start getting

00:08:49,560 --> 00:08:54,660
customers it can be it can be a good

00:08:51,720 --> 00:08:57,690
thing I'm told customers are useful and

00:08:54,660 --> 00:08:59,700
it has two core abstractions which are

00:08:57,690 --> 00:09:02,160
really confusingly named one of them is

00:08:59,700 --> 00:09:07,700
called resilient distributed datasets

00:09:02,160 --> 00:09:09,930
and another one is called datasets I

00:09:07,700 --> 00:09:12,810
don't know who okay well actually I do

00:09:09,930 --> 00:09:15,840
know who came up with these names um but

00:09:12,810 --> 00:09:17,640
it despite the name datasets is built on

00:09:15,840 --> 00:09:19,350
top of resilient distributed data sets

00:09:17,640 --> 00:09:25,770
and and is in fact resilient and

00:09:19,350 --> 00:09:29,160
distributed yeah I don't know I'm sorry

00:09:25,770 --> 00:09:31,920
okay clicky thing okay so why do people

00:09:29,160 --> 00:09:33,630
come to spark sometimes people are

00:09:31,920 --> 00:09:35,670
MapReduce developers are there any

00:09:33,630 --> 00:09:39,750
MapReduce developers or former MapReduce

00:09:35,670 --> 00:09:42,630
developers in the house yay thank you

00:09:39,750 --> 00:09:44,790
for joining us um and so sometimes it's

00:09:42,630 --> 00:09:48,060
like wow my MapReduce job is gonna take

00:09:44,790 --> 00:09:50,100
like a day I bet I could learn spark in

00:09:48,060 --> 00:09:52,290
that amount of time and sometimes you

00:09:50,100 --> 00:09:53,430
totally can and I think that's kind of

00:09:52,290 --> 00:09:55,530
cool

00:09:53,430 --> 00:09:59,480
the other ones are my data frame doesn't

00:09:55,530 --> 00:10:04,410
fit in memory anymore on the largest

00:09:59,480 --> 00:10:06,540
Google cloud instance or whatever cloud

00:10:04,410 --> 00:10:09,510
provider you happen to use which may or

00:10:06,540 --> 00:10:11,490
may not be Google um and you know maybe

00:10:09,510 --> 00:10:13,830
spark will solve this problem for me and

00:10:11,490 --> 00:10:16,620
the answer is yes it will but at the

00:10:13,830 --> 00:10:19,770
price of everything you hold dear so

00:10:16,620 --> 00:10:22,050
yeah um and the other reason people come

00:10:19,770 --> 00:10:24,510
is there's a fair amount of magic and it

00:10:22,050 --> 00:10:27,240
looks like it's gonna work which is an

00:10:24,510 --> 00:10:31,860
important part and a good first step

00:10:27,240 --> 00:10:33,660
towards working okay cool so what is the

00:10:31,860 --> 00:10:35,730
magic of spark like what what makes

00:10:33,660 --> 00:10:37,530
people really excited about using it so

00:10:35,730 --> 00:10:40,320
automatically distributed functional

00:10:37,530 --> 00:10:41,820
programming that's really cool I

00:10:40,320 --> 00:10:44,130
remember when I was in first year

00:10:41,820 --> 00:10:45,630
University and like being told yeah if

00:10:44,130 --> 00:10:47,430
you write your things this way the

00:10:45,630 --> 00:10:50,010
compiler could do all kinds of cool

00:10:47,430 --> 00:10:53,130
things I mean it doesn't but it could

00:10:50,010 --> 00:10:55,230
and to some degree spark delivers on a

00:10:53,130 --> 00:10:57,660
lot of the cool things the compiler

00:10:55,230 --> 00:10:59,820
could do except it's not the compiler

00:10:57,660 --> 00:11:01,950
and we'll talk about how that impacts us

00:10:59,820 --> 00:11:04,860
later and a lot of those

00:11:01,950 --> 00:11:09,920
is powered by a directed acyclic graph

00:11:04,860 --> 00:11:12,090
or essentially some lines on paper

00:11:09,920 --> 00:11:16,950
except instead of paper they're throwing

00:11:12,090 --> 00:11:18,900
the computer a lot of the names of

00:11:16,950 --> 00:11:20,730
things in spark clearly show its lineage

00:11:18,900 --> 00:11:23,310
it comes from an academic background

00:11:20,730 --> 00:11:25,410
where people had to write papers and get

00:11:23,310 --> 00:11:27,030
PhDs and so there's a lot of things with

00:11:25,410 --> 00:11:29,550
like names where you're like that's

00:11:27,030 --> 00:11:31,710
that's a query plan isn't it and they're

00:11:29,550 --> 00:11:34,860
like no no no no no it is different in

00:11:31,710 --> 00:11:38,700
this important way resiliency is super

00:11:34,860 --> 00:11:41,370
important especially with cloud

00:11:38,700 --> 00:11:45,930
providers if you're running on

00:11:41,370 --> 00:11:47,910
preemptable instances or spot instances

00:11:45,930 --> 00:11:50,190
or whatever you're your provider likes

00:11:47,910 --> 00:11:54,630
to say you will experience failures

00:11:50,190 --> 00:11:58,470
unless you bid super high amounts on hmm

00:11:54,630 --> 00:12:00,690
other cloud company on my cloud company

00:11:58,470 --> 00:12:04,340
you can't bid super high amounts so that

00:12:00,690 --> 00:12:06,780
oh okay um and some other magic

00:12:04,340 --> 00:12:08,550
we had functional programming and it's a

00:12:06,780 --> 00:12:10,590
really great way to trick people into

00:12:08,550 --> 00:12:12,630
learning functional programming because

00:12:10,590 --> 00:12:15,150
people want to build distributed systems

00:12:12,630 --> 00:12:16,700
right they're like I mean we may want to

00:12:15,150 --> 00:12:19,230
build things with functional programming

00:12:16,700 --> 00:12:21,210
because we think it's cool but other

00:12:19,230 --> 00:12:23,100
people have like a business problem they

00:12:21,210 --> 00:12:24,900
have to solve and then we're like by the

00:12:23,100 --> 00:12:27,300
way functional programming will solve

00:12:24,900 --> 00:12:28,770
this for you but we don't tell them it's

00:12:27,300 --> 00:12:31,050
functional programming at the start

00:12:28,770 --> 00:12:33,090
because that makes them scared we tell

00:12:31,050 --> 00:12:34,740
them we have a distributed system and it

00:12:33,090 --> 00:12:41,190
just happens to have immutable data

00:12:34,740 --> 00:12:42,660
types and then soon okay cool and so one

00:12:41,190 --> 00:12:44,160
of the things which which spark has done

00:12:42,660 --> 00:12:47,220
differently than a lot of the other Big

00:12:44,160 --> 00:12:50,190
Data projects is its integrated a whole

00:12:47,220 --> 00:12:51,780
bunch of different things it has two

00:12:50,190 --> 00:12:56,010
separate graph libraries which do not

00:12:51,780 --> 00:12:57,630
work and a third on the way I'm told the

00:12:56,010 --> 00:13:00,030
third one might work this time so I'm

00:12:57,630 --> 00:13:01,890
really excited about that one we have

00:13:00,030 --> 00:13:03,690
two separate machine learning algorithms

00:13:01,890 --> 00:13:07,050
oh sorry libraries with more than two

00:13:03,690 --> 00:13:08,610
algorithms we have two separate

00:13:07,050 --> 00:13:11,730
streaming engines with a third

00:13:08,610 --> 00:13:13,600
integrated recently but the API is the

00:13:11,730 --> 00:13:17,380
same for two of them

00:13:13,600 --> 00:13:20,259
and we also expose interfaces to other

00:13:17,380 --> 00:13:22,180
languages and this will become important

00:13:20,259 --> 00:13:24,040
later on but it's really great to be

00:13:22,180 --> 00:13:26,230
able to get people from a Python

00:13:24,040 --> 00:13:28,060
background to start doing distributed

00:13:26,230 --> 00:13:30,339
functional programming and then just be

00:13:28,060 --> 00:13:32,050
like and you know your code be so much

00:13:30,339 --> 00:13:34,089
faster if you just learned just a little

00:13:32,050 --> 00:13:38,829
bit of Scala you know the first hits

00:13:34,089 --> 00:13:40,149
free and then you know six months later

00:13:38,829 --> 00:13:43,000
they're at a conference talking about

00:13:40,149 --> 00:13:45,310
you know in variant types anyways okay

00:13:43,000 --> 00:13:46,569
so what's part got right for functional

00:13:45,310 --> 00:13:47,350
programming there's some really cool

00:13:46,569 --> 00:13:51,009
things um

00:13:47,350 --> 00:13:53,319
strongly enforced to to most degrees for

00:13:51,009 --> 00:13:55,750
immutable data and this is because it

00:13:53,319 --> 00:13:58,329
uses recompute on failure and if we

00:13:55,750 --> 00:14:00,670
mutate data structures trying to do a

00:13:58,329 --> 00:14:02,709
recompute when we lose the data would

00:14:00,670 --> 00:14:05,860
just result in totally garbage results

00:14:02,709 --> 00:14:08,170
and so this is really cool the exception

00:14:05,860 --> 00:14:10,360
is there's like three escape hatches

00:14:08,170 --> 00:14:13,329
that are built in but they're all really

00:14:10,360 --> 00:14:15,430
well hidden so that's good

00:14:13,329 --> 00:14:17,620
functional operators everyone here

00:14:15,430 --> 00:14:21,130
probably loves map and filter and flat

00:14:17,620 --> 00:14:24,639
map and everyone gets a lambda even Java

00:14:21,130 --> 00:14:28,480
people and it's it solved a business

00:14:24,639 --> 00:14:33,490
need and so these are all things that we

00:14:28,480 --> 00:14:37,389
did well yay go team okay what did we do

00:14:33,490 --> 00:14:39,399
less amazingly so one of the things that

00:14:37,389 --> 00:14:41,439
that spark is really cool with is it it

00:14:39,399 --> 00:14:43,060
takes our lambdas and it it constructs

00:14:41,439 --> 00:14:43,660
closures and it sends them over the

00:14:43,060 --> 00:14:49,360
network

00:14:43,660 --> 00:14:51,819
but closures in Scala we didn't exactly

00:14:49,360 --> 00:14:54,339
foresee that as a thing that people were

00:14:51,819 --> 00:14:55,990
gonna be doing like a lot so we have

00:14:54,339 --> 00:14:59,019
this really high-quality piece of code

00:14:55,990 --> 00:15:01,420
called closure cleaner Scala and if you

00:14:59,019 --> 00:15:02,620
look inside that that is where Cthulhu

00:15:01,420 --> 00:15:04,660
comes out and eats your soul

00:15:02,620 --> 00:15:06,069
I'm nom nom nom them and then disappears

00:15:04,660 --> 00:15:09,399
back again until the next programmer

00:15:06,069 --> 00:15:11,230
comes by and and sometimes this will

00:15:09,399 --> 00:15:13,300
make people think that closures aren't

00:15:11,230 --> 00:15:15,310
as powerful as they really can be

00:15:13,300 --> 00:15:19,180
because we have some limitations inside

00:15:15,310 --> 00:15:20,889
of SPARC and for a lot of people SPARC

00:15:19,180 --> 00:15:22,540
is there sort of introduction to this

00:15:20,889 --> 00:15:24,910
thing and we don't do a good job of

00:15:22,540 --> 00:15:26,480
clearly indicating that like hey you

00:15:24,910 --> 00:15:27,800
can't do this in SPARC

00:15:26,480 --> 00:15:29,810
you could do this in another system

00:15:27,800 --> 00:15:34,870
probably because that makes spark sound

00:15:29,810 --> 00:15:39,829
less cool a lot of things we were like

00:15:34,870 --> 00:15:45,019
man I could add types or I could just

00:15:39,829 --> 00:15:47,060
have a map of strings to strings yeah

00:15:45,019 --> 00:15:50,300
the map of strings to strings

00:15:47,060 --> 00:15:53,779
it's everywhere yay and it's hard to

00:15:50,300 --> 00:15:57,260
debug I I don't how many people were in

00:15:53,779 --> 00:15:59,449
Kelly's talk this morning okay you saw

00:15:57,260 --> 00:16:01,820
those stack traces um and those stock

00:15:59,449 --> 00:16:03,440
traces are delightful and when you start

00:16:01,820 --> 00:16:05,810
running on in a distributed system you

00:16:03,440 --> 00:16:09,139
can get like 400 slightly different

00:16:05,810 --> 00:16:10,699
copies of that stack trace and we don't

00:16:09,139 --> 00:16:12,740
do a good job of telling you like you

00:16:10,699 --> 00:16:16,730
only need to look at one or like whoa

00:16:12,740 --> 00:16:19,310
these are actually really different we

00:16:16,730 --> 00:16:21,350
also added new api's without any types

00:16:19,310 --> 00:16:23,690
because we're like types are overrated

00:16:21,350 --> 00:16:25,760
that was way too much work the first

00:16:23,690 --> 00:16:29,029
time and then after we did that we went

00:16:25,760 --> 00:16:32,660
oh wow maybe maybe we needed those types

00:16:29,029 --> 00:16:35,930
 and so we added them back in so

00:16:32,660 --> 00:16:39,529
what are the new fancy api's that are in

00:16:35,930 --> 00:16:41,750
spark so in in the grand tradition of

00:16:39,529 --> 00:16:44,389
programming the new api's do not work

00:16:41,750 --> 00:16:52,760
yet completely and the old api's are

00:16:44,389 --> 00:16:54,769
deprecated yeah okay um and we we have

00:16:52,760 --> 00:16:56,870
data frames and data sets which we're

00:16:54,769 --> 00:16:58,639
going to talk about a lot and we have a

00:16:56,870 --> 00:17:02,449
new machine learning API where we were

00:16:58,639 --> 00:17:04,400
just like NAFA types and then some

00:17:02,449 --> 00:17:06,470
people over eight and we forgot to make

00:17:04,400 --> 00:17:10,010
it so that you could serve your models

00:17:06,470 --> 00:17:12,230
we made it so you could train them but

00:17:10,010 --> 00:17:15,709
like it turns out once you go from like

00:17:12,230 --> 00:17:18,709
writing papers to trying to do like real

00:17:15,709 --> 00:17:21,470
things that that whole like serving of

00:17:18,709 --> 00:17:23,660
your model is kind of important and

00:17:21,470 --> 00:17:25,040
there's people who sort of hack it up to

00:17:23,660 --> 00:17:29,929
make it work but it's really it's really

00:17:25,040 --> 00:17:31,669
sad and structured streaming is our new

00:17:29,929 --> 00:17:33,559
streaming engine it's actually our new

00:17:31,669 --> 00:17:35,870
streaming API with two different engines

00:17:33,559 --> 00:17:38,659
behind it neither of which work

00:17:35,870 --> 00:17:41,029
completely but it's okay

00:17:38,659 --> 00:17:44,599
one of them might work enough for you

00:17:41,029 --> 00:17:47,359
maybe okay cool so let's focus on data

00:17:44,599 --> 00:17:50,599
sets and data frames so data frames were

00:17:47,359 --> 00:17:52,099
our first like v2 API and we were like

00:17:50,599 --> 00:17:55,099
you know what types are hard everything

00:17:52,099 --> 00:17:58,190
is just going to be a row and rows can

00:17:55,099 --> 00:18:00,049
contain arbitrary things like rows or

00:17:58,190 --> 00:18:02,149
strings or other things but like I don't

00:18:00,049 --> 00:18:05,059
want to know at compile time what's

00:18:02,149 --> 00:18:06,799
going on or even really completely at

00:18:05,059 --> 00:18:10,700
runtime that is gonna be way too much

00:18:06,799 --> 00:18:14,450
work so that went out in like spark 1.6

00:18:10,700 --> 00:18:16,519
in spark to zero we went oh there

00:18:14,450 --> 00:18:19,759
are a lot of people on the eve mailing

00:18:16,519 --> 00:18:21,950
list who odd yes okay okay we were

00:18:19,759 --> 00:18:25,159
always gonna add these types back in and

00:18:21,950 --> 00:18:28,940
now we have types and the the part which

00:18:25,159 --> 00:18:31,220
is kind of cool but kind of sad is it's

00:18:28,940 --> 00:18:34,460
more sequel inspired than functional

00:18:31,220 --> 00:18:35,779
inspired and are there any people in the

00:18:34,460 --> 00:18:37,190
audience which like wake up in the

00:18:35,779 --> 00:18:42,440
morning and are really excited about

00:18:37,190 --> 00:18:45,019
writing a sequel query yeah yeah and

00:18:42,440 --> 00:18:49,970
they're totally are those people but I'm

00:18:45,019 --> 00:18:53,539
not one of them and so hmm I'm selfish

00:18:49,970 --> 00:18:55,249
not like super selfish but I would like

00:18:53,539 --> 00:18:57,979
to see us do things that are not just

00:18:55,249 --> 00:19:01,190
sequel inspired and so we added a bunch

00:18:57,979 --> 00:19:02,330
of functional things after and and you

00:19:01,190 --> 00:19:04,999
can definitely see that there's like

00:19:02,330 --> 00:19:06,320
this API that was like designed together

00:19:04,999 --> 00:19:08,629
and then someone came by and was like

00:19:06,320 --> 00:19:14,149
and I need these six things added on top

00:19:08,629 --> 00:19:16,580
of it yeah okay a whole bunch of things

00:19:14,149 --> 00:19:21,379
that suck let's let's go let's go ahead

00:19:16,580 --> 00:19:23,599
and focus on awesome things yay so it's

00:19:21,379 --> 00:19:27,859
easier to mix functional and relational

00:19:23,599 --> 00:19:30,409
style it's mostly because doing

00:19:27,859 --> 00:19:35,389
relational programming with rdd's was

00:19:30,409 --> 00:19:37,759
just like no so that's that's cool we

00:19:35,389 --> 00:19:40,789
have better serialization which is

00:19:37,759 --> 00:19:46,429
really awesome the java serializer once

00:19:40,789 --> 00:19:48,799
again bar here but you know it's

00:19:46,429 --> 00:19:50,529
actually possibly better than than cryo

00:19:48,799 --> 00:19:52,359
serialization

00:19:50,529 --> 00:19:56,549
and it has some cool things where we can

00:19:52,359 --> 00:19:59,979
do stuff directly on the serialize data

00:19:56,549 --> 00:20:01,960
and it also has the potential for better

00:19:59,979 --> 00:20:05,049
language interoperability and I think

00:20:01,960 --> 00:20:06,669
this is really useful for the scholar

00:20:05,049 --> 00:20:11,679
community because one of the things

00:20:06,669 --> 00:20:14,440
which we kind of suck at is numeric

00:20:11,679 --> 00:20:16,450
computation tools like and and Python

00:20:14,440 --> 00:20:19,119
has and are have such wonderful

00:20:16,450 --> 00:20:22,299
libraries to do really cool numeric

00:20:19,119 --> 00:20:24,969
computation if we could just like steal

00:20:22,299 --> 00:20:27,460
those and like use them from Scala

00:20:24,969 --> 00:20:29,200
without having to rewrite them that

00:20:27,460 --> 00:20:30,669
would be amazing I don't want to have to

00:20:29,200 --> 00:20:35,229
figure out how to call Fortran code from

00:20:30,669 --> 00:20:36,609
Scala you can but it sucks and so they

00:20:35,229 --> 00:20:40,359
will talk about how this works

00:20:36,609 --> 00:20:46,089
um and this graph strongly implies that

00:20:40,359 --> 00:20:48,039
performance is better yay faster hmm

00:20:46,089 --> 00:20:50,109
yeah the first time I ran this the

00:20:48,039 --> 00:20:53,619
results were not like this so I ran it

00:20:50,109 --> 00:20:55,899
again and then I got the right answer so

00:20:53,619 --> 00:20:57,909
essentially this may not apply to

00:20:55,899 --> 00:21:01,179
everyone's data like please please check

00:20:57,909 --> 00:21:03,179
on your own before you trust me and our

00:21:01,179 --> 00:21:06,879
storage is a lot more efficient right

00:21:03,179 --> 00:21:08,499
this is not genuine big data but I was

00:21:06,879 --> 00:21:11,139
running this on my laptop on a flight

00:21:08,499 --> 00:21:14,049
and you can see the RVT version is

00:21:11,139 --> 00:21:16,179
roughly 2x the size of the data set

00:21:14,049 --> 00:21:18,339
version where we have a better

00:21:16,179 --> 00:21:20,139
understanding of what's going on of

00:21:18,339 --> 00:21:22,389
course you can also see that we really

00:21:20,139 --> 00:21:27,909
do not expose that better understanding

00:21:22,389 --> 00:21:30,159
in the UI at all the RVT name is more or

00:21:27,909 --> 00:21:32,889
less unintelligible so that's great and

00:21:30,159 --> 00:21:35,739
yeah it is probably better than crappy

00:21:32,889 --> 00:21:38,469
storage but not a guarantee you know

00:21:35,739 --> 00:21:41,259
talk to your vendor ok

00:21:38,469 --> 00:21:43,690
let's actually mix together functional

00:21:41,259 --> 00:21:46,330
and relational code and the first thing

00:21:43,690 --> 00:21:47,830
that we do is this filter expression and

00:21:46,330 --> 00:21:50,619
one of the things which I love in the

00:21:47,830 --> 00:21:55,839
world is pandas pandas are fuzzy they're

00:21:50,619 --> 00:21:58,869
cute they eat bamboo and if I fly for

00:21:55,839 --> 00:22:01,839
long enough I can go see a panda and so

00:21:58,869 --> 00:22:03,490
DSD filter the happy column is equal to

00:22:01,839 --> 00:22:06,280
true because I do

00:22:03,490 --> 00:22:09,880
want to meet sad pandas that would break

00:22:06,280 --> 00:22:12,250
my heart and in fact the cool thing

00:22:09,880 --> 00:22:16,150
about this is because this filter is

00:22:12,250 --> 00:22:17,920
written in the DSL provided by spark it

00:22:16,150 --> 00:22:21,280
actually understands the filter

00:22:17,920 --> 00:22:24,130
expression and if your data happens to

00:22:21,280 --> 00:22:26,920
be in such a way that it's partition by

00:22:24,130 --> 00:22:29,679
if pandas are happy or not then it will

00:22:26,920 --> 00:22:31,480
only load the data where the pandas are

00:22:29,679 --> 00:22:34,030
happy and this is awesome

00:22:31,480 --> 00:22:36,210
the only thing is like this is not an

00:22:34,030 --> 00:22:38,860
arbitrary expression right if I was like

00:22:36,210 --> 00:22:41,290
interested in pandas that were happy and

00:22:38,860 --> 00:22:45,220
had some other constraints on them it

00:22:41,290 --> 00:22:46,570
will be hard to express here possibly we

00:22:45,220 --> 00:22:49,330
can also see though this reduced

00:22:46,570 --> 00:22:52,960
function that X plus y is a normal Scala

00:22:49,330 --> 00:22:56,380
X plus y so yay I can add two numbers

00:22:52,960 --> 00:23:00,460
together kind of um and so yes we can

00:22:56,380 --> 00:23:03,820
see that we have this mixture where we

00:23:00,460 --> 00:23:05,710
we can take data frames we can apply

00:23:03,820 --> 00:23:06,550
sequel expressions we can make them

00:23:05,710 --> 00:23:08,860
strongly-typed

00:23:06,550 --> 00:23:14,830
and then we can apply arbitrary Scala

00:23:08,860 --> 00:23:15,940
code on top of it and we can we can we

00:23:14,830 --> 00:23:19,600
have our map expression

00:23:15,940 --> 00:23:22,350
everyone loves map yes okay

00:23:19,600 --> 00:23:25,990
way less people love maps than I thought

00:23:22,350 --> 00:23:27,160
and so I was I was happy when I wrote

00:23:25,990 --> 00:23:28,840
this because I was like yeah and this is

00:23:27,160 --> 00:23:30,070
this would be hard to express in sequel

00:23:28,840 --> 00:23:31,510
and someone came up to me was just like

00:23:30,070 --> 00:23:35,020
no oh and you're just really bad at

00:23:31,510 --> 00:23:36,670
sequel and I am but it's it's nice we

00:23:35,020 --> 00:23:38,380
can we can put whatever we want in the

00:23:36,670 --> 00:23:39,730
middle we can we can still have our cool

00:23:38,380 --> 00:23:43,480
functional code we can write lambda

00:23:39,730 --> 00:23:46,510
expressions and be like yeah pay me

00:23:43,480 --> 00:23:48,370
money for functional programming um and

00:23:46,510 --> 00:23:51,370
we can do word count with it that's how

00:23:48,370 --> 00:23:53,440
you know it's genuine big data if we

00:23:51,370 --> 00:23:56,770
were not doing word count how would you

00:23:53,440 --> 00:23:58,000
know this was real um so very exciting

00:23:56,770 --> 00:24:00,790
I'm gonna walk you through word count

00:23:58,000 --> 00:24:04,200
and in fact there are multiple ways to

00:24:00,790 --> 00:24:08,080
do word count with this API oh yes um

00:24:04,200 --> 00:24:12,309
okay so load our data tell SPARC the

00:24:08,080 --> 00:24:15,490
type for compiled time information we

00:24:12,309 --> 00:24:16,950
have our flat map whoo and then we have

00:24:15,490 --> 00:24:19,739
group

00:24:16,950 --> 00:24:23,580
and this cool thing about group-by is

00:24:19,739 --> 00:24:28,320
that it's actually not bad in the old

00:24:23,580 --> 00:24:32,129
spark 1.0 actually ODOT 7 api is grouped

00:24:28,320 --> 00:24:34,830
by key constructs a giant list of all of

00:24:32,129 --> 00:24:36,840
the things with the same key also known

00:24:34,830 --> 00:24:40,739
as it constructs an out of memory

00:24:36,840 --> 00:24:43,139
exception and so the nice thing here is

00:24:40,739 --> 00:24:46,080
group by instead of you really a value

00:24:43,139 --> 00:24:48,359
well sort of eagerly evaluating goodbye

00:24:46,080 --> 00:24:50,759
here instead returns a special data

00:24:48,359 --> 00:24:52,769
structure which goes hey that's really

00:24:50,759 --> 00:24:55,259
cool you wanted me to make this grouping

00:24:52,769 --> 00:24:56,789
tell me what aggregates you want to do

00:24:55,259 --> 00:24:59,669
on this and I'll figure out how to

00:24:56,789 --> 00:25:01,649
pipeline those intelligently and so here

00:24:59,669 --> 00:25:04,019
then we count our words and then we save

00:25:01,649 --> 00:25:10,649
our result out as parquet because we're

00:25:04,019 --> 00:25:12,720
cool yeah ok so that's cool but we can

00:25:10,649 --> 00:25:15,330
already do word count with the RDD api

00:25:12,720 --> 00:25:19,200
what can we do that's really cool with

00:25:15,330 --> 00:25:22,799
the data set api yeah ok cool

00:25:19,200 --> 00:25:24,600
so this cat is awesome um and we're

00:25:22,799 --> 00:25:28,830
gonna compute multiple aggregates at the

00:25:24,600 --> 00:25:31,379
same time oh okay

00:25:28,830 --> 00:25:33,210
this is really exciting if you ever have

00:25:31,379 --> 00:25:35,850
to write this code by hand on the RDD

00:25:33,210 --> 00:25:38,129
api I can assure you when you have to

00:25:35,850 --> 00:25:40,919
write this with the RDD api you end up

00:25:38,129 --> 00:25:43,080
with like X underscore 1 y underscore 2

00:25:40,919 --> 00:25:44,940
and like all kinds of little temporary

00:25:43,080 --> 00:25:46,320
variables and eventually instead of

00:25:44,940 --> 00:25:48,149
computing the average number of hours

00:25:46,320 --> 00:25:49,980
per week you end up competing the

00:25:48,149 --> 00:25:54,059
capital gains divided by a number of

00:25:49,980 --> 00:25:57,210
hours per week and then you make bad

00:25:54,059 --> 00:26:01,230
business decisions because you're like

00:25:57,210 --> 00:26:04,440
wow how people are working a lot I feel

00:26:01,230 --> 00:26:06,179
really bad about myself and and the main

00:26:04,440 --> 00:26:09,059
thing here is like this is a lot easier

00:26:06,179 --> 00:26:11,489
to write it's not particularly

00:26:09,059 --> 00:26:14,239
functional but we can do our cool

00:26:11,489 --> 00:26:17,759
functional stuff on either side of it

00:26:14,239 --> 00:26:19,769
and we can have windows which are also a

00:26:17,759 --> 00:26:22,529
pain in the ass to do with the RDD api

00:26:19,769 --> 00:26:25,139
because with the RDD api have to think

00:26:22,529 --> 00:26:27,179
about how my data is partitioned and

00:26:25,139 --> 00:26:29,149
it's not really super clear what's going

00:26:27,179 --> 00:26:31,639
on and so I can be like hey

00:26:29,149 --> 00:26:34,700
I want the average over the past K and

00:26:31,639 --> 00:26:36,799
next J elements as sorted by some some

00:26:34,700 --> 00:26:38,359
random thing and spark will just compute

00:26:36,799 --> 00:26:40,609
that for me and I don't have to think

00:26:38,359 --> 00:26:43,249
about what's going on and this is really

00:26:40,609 --> 00:26:45,440
awesome once again super easy to do on

00:26:43,249 --> 00:26:48,679
like a single computer really annoying

00:26:45,440 --> 00:26:51,440
to do on a distributed system and so it

00:26:48,679 --> 00:26:54,649
may not seem impressive but trust me it

00:26:51,440 --> 00:26:57,950
kind of is okay and so this is what a

00:26:54,649 --> 00:27:00,859
window operation looks like we can do

00:26:57,950 --> 00:27:04,700
all kinds of things okay

00:27:00,859 --> 00:27:06,619
we can also add UDF's and this is for

00:27:04,700 --> 00:27:08,929
people that really like sequel and so

00:27:06,619 --> 00:27:11,690
that was zero people here so we're gonna

00:27:08,929 --> 00:27:15,289
go ahead and move on and we're gonna

00:27:11,690 --> 00:27:21,080
skip that okay and we can do other fun

00:27:15,289 --> 00:27:23,119
things right like these yeah these three

00:27:21,080 --> 00:27:25,460
you know sort of built in aggregates

00:27:23,119 --> 00:27:27,529
maybe you want to do more than compute

00:27:25,460 --> 00:27:29,149
them in average maths right like maybe

00:27:27,529 --> 00:27:31,669
you want to do something really really

00:27:29,149 --> 00:27:34,909
intense and you can write custom

00:27:31,669 --> 00:27:43,190
aggregates Oh God

00:27:34,909 --> 00:27:45,889
unit unit unit any yeah so you can

00:27:43,190 --> 00:27:51,440
write custom aggregates the types we

00:27:45,889 --> 00:27:54,229
just um yeah we said it with this

00:27:51,440 --> 00:27:57,080
API but it's it's it's okay right you

00:27:54,229 --> 00:28:00,679
you can you can use this API to extend

00:27:57,080 --> 00:28:03,349
it and just hide it in like utils dot

00:28:00,679 --> 00:28:05,629
scala and that'll make yourself feel

00:28:03,349 --> 00:28:07,999
okay about it cuz no one opens utils dot

00:28:05,629 --> 00:28:12,649
Scala we all know that's just where you

00:28:07,999 --> 00:28:14,450
know software goes to die okay we could

00:28:12,649 --> 00:28:17,179
make a functional version of this right

00:28:14,450 --> 00:28:19,669
it would actually not be that hard and

00:28:17,179 --> 00:28:22,849
this is sort of a hint hint for anyone

00:28:19,669 --> 00:28:24,589
who's looking to contribute to spark you

00:28:22,849 --> 00:28:26,659
know there's there's nothing which stops

00:28:24,589 --> 00:28:28,669
you from from making a functional

00:28:26,659 --> 00:28:30,559
version of this and contributing that to

00:28:28,669 --> 00:28:32,330
our API so people don't have to

00:28:30,559 --> 00:28:36,320
understand what a mutable aggregation

00:28:32,330 --> 00:28:38,210
buffer is or return any right like we

00:28:36,320 --> 00:28:40,479
could actually do something smart here

00:28:38,210 --> 00:28:43,000
and you can hide all of the terribleness

00:28:40,479 --> 00:28:46,510
so someone please write this

00:28:43,000 --> 00:28:48,640
want to and the other part of that is

00:28:46,510 --> 00:28:52,410
someone might yell at you but I will be

00:28:48,640 --> 00:28:55,930
with you if you message me okay cool and

00:28:52,410 --> 00:28:59,560
we can use UDF's programmatically this

00:28:55,930 --> 00:29:01,420
is not super exciting but this is

00:28:59,560 --> 00:29:03,250
actually kind of neat because we're

00:29:01,420 --> 00:29:03,820
gonna talk about how to register Python

00:29:03,250 --> 00:29:08,260
UDF's

00:29:03,820 --> 00:29:09,880
and use them from Scala sorry okay we

00:29:08,260 --> 00:29:12,190
have this other thing functions Scala

00:29:09,880 --> 00:29:13,720
and so if you're like hey I don't know

00:29:12,190 --> 00:29:16,030
if I should build this or if I should

00:29:13,720 --> 00:29:17,890
just see if sparks already done this you

00:29:16,030 --> 00:29:19,330
just look in functions at Scala cover

00:29:17,890 --> 00:29:22,060
your eyes when it gets to the type

00:29:19,330 --> 00:29:23,950
declarations and and move through and

00:29:22,060 --> 00:29:27,100
frameless brings type columns to this

00:29:23,950 --> 00:29:30,880
and it's kind of cool but for a slightly

00:29:27,100 --> 00:29:33,180
different price on top of SPARC okay how

00:29:30,880 --> 00:29:36,490
many people care about machine learning

00:29:33,180 --> 00:29:39,610
whoa more than I thought my employer

00:29:36,490 --> 00:29:43,000
will be happy so sparks machine learning

00:29:39,610 --> 00:29:45,910
API is our psychic inspired which are

00:29:43,000 --> 00:29:47,920
Python based and so we went like well

00:29:45,910 --> 00:29:48,930
they don't have types so we don't need

00:29:47,920 --> 00:29:54,640
types

00:29:48,930 --> 00:29:56,680
oops and it's great because machine

00:29:54,640 --> 00:29:59,740
learning jobs always run super quickly

00:29:56,680 --> 00:30:01,360
and definitely don't run for like 24

00:29:59,740 --> 00:30:03,130
hours and then having them fail at the

00:30:01,360 --> 00:30:07,000
last minute because of a type validation

00:30:03,130 --> 00:30:09,100
error right okay so the really sad thing

00:30:07,000 --> 00:30:11,260
is what people end up doing to work with

00:30:09,100 --> 00:30:12,970
spark ml pipelines is you end up like

00:30:11,260 --> 00:30:14,440
sampling your data initially just

00:30:12,970 --> 00:30:19,690
running that through and seeing if it

00:30:14,440 --> 00:30:23,460
works and that's just as a scholar

00:30:19,690 --> 00:30:25,930
programmer it just makes me sad and

00:30:23,460 --> 00:30:28,510
frameless once again has some of this

00:30:25,930 --> 00:30:31,750
with types some other people at a

00:30:28,510 --> 00:30:34,720
company who I should probably not name

00:30:31,750 --> 00:30:38,530
publicly have some code which does this

00:30:34,720 --> 00:30:40,660
without using frameless and be really

00:30:38,530 --> 00:30:42,670
cool if they open sourced it I don't see

00:30:40,660 --> 00:30:44,740
anyone from that company in the audience

00:30:42,670 --> 00:30:46,630
though but if you are you know who you

00:30:44,740 --> 00:30:49,720
are and you can open source this code

00:30:46,630 --> 00:30:55,789
okay um here's how we do machine

00:30:49,720 --> 00:30:57,649
learning everything is strings

00:30:55,789 --> 00:30:59,570
although everything the strings and gets

00:30:57,649 --> 00:31:02,779
turned into doubles behind the scenes so

00:30:59,570 --> 00:31:04,909
that's great and so if you want to build

00:31:02,779 --> 00:31:06,710
a really simple like data pipeline it's

00:31:04,909 --> 00:31:08,809
it's pretty easy doesn't look

00:31:06,710 --> 00:31:10,279
particularly functional we've got all of

00:31:08,809 --> 00:31:13,369
these getters and setters to bring you

00:31:10,279 --> 00:31:14,629
back to the 90s with your javabeans so

00:31:13,369 --> 00:31:19,759
it's it's a really good throwback

00:31:14,629 --> 00:31:21,710
machine learning system and yeah so this

00:31:19,759 --> 00:31:24,259
is not overly important unless you're

00:31:21,710 --> 00:31:25,820
really excited about this but it turns

00:31:24,259 --> 00:31:27,769
out to be pretty simple to actually fit

00:31:25,820 --> 00:31:29,330
your pipelines and then predict the

00:31:27,769 --> 00:31:33,739
results provided that you're willing to

00:31:29,330 --> 00:31:37,279
do it on your spark cluster and do-do-do

00:31:33,739 --> 00:31:39,470
input data do do do and the string

00:31:37,279 --> 00:31:41,210
indexer is not a machine learning stage

00:31:39,470 --> 00:31:42,679
sorry it's not what people would

00:31:41,210 --> 00:31:45,200
classically think of as like a machine

00:31:42,679 --> 00:31:47,419
learning stage but it has to see all of

00:31:45,200 --> 00:31:49,820
the inputs to construct the correct

00:31:47,419 --> 00:31:52,669
labels and so it still has to be fit to

00:31:49,820 --> 00:31:54,889
the data and so even though you might be

00:31:52,669 --> 00:31:56,629
like why do I have to call fit there's

00:31:54,889 --> 00:31:58,519
like nothing exciting happening inside

00:31:56,629 --> 00:32:01,099
of this you haven't even put in naive

00:31:58,519 --> 00:32:02,539
Bayes this this is why we have to call

00:32:01,099 --> 00:32:06,739
fit and then we get some information

00:32:02,539 --> 00:32:09,859
about cats and then we can go ahead and

00:32:06,739 --> 00:32:10,249
add our decision tree on top of it once

00:32:09,859 --> 00:32:13,070
again

00:32:10,249 --> 00:32:16,099
90s throwback and our pipeline now has

00:32:13,070 --> 00:32:17,269
three things inside of it and we can we

00:32:16,099 --> 00:32:20,479
can call fit and then we get a

00:32:17,269 --> 00:32:24,609
completely trained machine learning

00:32:20,479 --> 00:32:27,409
model the only catch is this like

00:32:24,609 --> 00:32:30,349
indexer it turns out at runtime actually

00:32:27,409 --> 00:32:32,029
has a bunch of really bad problems if

00:32:30,349 --> 00:32:34,220
anyone wants to talk about why machine

00:32:32,029 --> 00:32:37,940
learning sucks on spark come and find me

00:32:34,220 --> 00:32:41,629
later when I'm drunk okay cool let's

00:32:37,940 --> 00:32:45,080
focus on more happy things arrow Apache

00:32:41,629 --> 00:32:47,989
arrow is amazing it's integrated in

00:32:45,080 --> 00:32:50,359
spark 2.3 it's not fully integrated

00:32:47,989 --> 00:32:54,440
inside of the spark 2.3 we looked at

00:32:50,359 --> 00:32:59,389
time zones and one oh god oh god no one

00:32:54,440 --> 00:33:02,299
has day time data right yeah lalla and

00:32:59,389 --> 00:33:06,379
then and then we then we made a release

00:33:02,299 --> 00:33:09,139
um but it's really cool and we have

00:33:06,379 --> 00:33:09,650
sorry not we a vendor has a graph which

00:33:09,139 --> 00:33:14,210
implies

00:33:09,650 --> 00:33:19,550
makes park 242 times faster this is a

00:33:14,210 --> 00:33:22,490
lie and not a very convincing one um but

00:33:19,550 --> 00:33:26,270
it can become faster in certain special

00:33:22,490 --> 00:33:29,390
cases and I'm sorry for showing Python

00:33:26,270 --> 00:33:32,000
code at escala conference but we all

00:33:29,390 --> 00:33:36,650
know that Python excels at numeric

00:33:32,000 --> 00:33:38,990
computation and the JVM maybe does not

00:33:36,650 --> 00:33:40,280
excel at numeric computation so if we're

00:33:38,990 --> 00:33:42,230
gonna add two numbers together we're

00:33:40,280 --> 00:33:44,120
gonna do it in Python right we're gonna

00:33:42,230 --> 00:33:47,450
take our earlier word count example and

00:33:44,120 --> 00:33:48,290
we're gonna do our counting that sounds

00:33:47,450 --> 00:33:51,080
like a great idea

00:33:48,290 --> 00:33:53,120
right what could go wrong okay

00:33:51,080 --> 00:33:57,710
foreshadowing is not working out here

00:33:53,120 --> 00:34:00,560
um and we could okay yes so we can take

00:33:57,710 --> 00:34:02,690
those pandas UDF and we can go ahead and

00:34:00,560 --> 00:34:05,270
we can we can call this from Scala

00:34:02,690 --> 00:34:07,280
through a whole bunch of really terrible

00:34:05,270 --> 00:34:09,560
 that is inside of a project called

00:34:07,280 --> 00:34:13,370
sparkling ml and if you go look at

00:34:09,560 --> 00:34:17,030
sparkling ml just don't read startup dot

00:34:13,370 --> 00:34:20,060
pi or the equivalent Java file and it

00:34:17,030 --> 00:34:21,830
looks like a fine project but the cool

00:34:20,060 --> 00:34:25,460
thing is then you can start doing things

00:34:21,830 --> 00:34:27,470
like using Python and LP packages you

00:34:25,460 --> 00:34:29,929
have to write the code for each specific

00:34:27,470 --> 00:34:32,060
package you want to use but you can you

00:34:29,929 --> 00:34:34,159
can write UDF's in Python and start

00:34:32,060 --> 00:34:36,560
using them from Scala and this is really

00:34:34,159 --> 00:34:39,440
convenient because I don't have the time

00:34:36,560 --> 00:34:41,690
to figure out how to do NLP in Java it

00:34:39,440 --> 00:34:43,490
really sucks and the Python people have

00:34:41,690 --> 00:34:46,300
done this for us so we should steal

00:34:43,490 --> 00:34:49,750
their software and pretend it's ours

00:34:46,300 --> 00:34:53,000
but it's open source so that's okay

00:34:49,750 --> 00:34:58,520
maybe okay and we could use it on top of

00:34:53,000 --> 00:35:00,080
streaming except probably don't so we

00:34:58,520 --> 00:35:03,920
have structured streaming new to spark

00:35:00,080 --> 00:35:06,170
2.3 and it wasn't very good

00:35:03,920 --> 00:35:08,060
sorry new in spark $2 it wasn't very

00:35:06,170 --> 00:35:10,370
good and instead of fixing the problem

00:35:08,060 --> 00:35:13,100
we said you know what this needs is a

00:35:10,370 --> 00:35:16,550
new streaming engine complete with a new

00:35:13,100 --> 00:35:19,100
set of bugs and that's what we did in

00:35:16,550 --> 00:35:22,900
spark 2.3 we bought a new execution

00:35:19,100 --> 00:35:22,900
engine in which loses data

00:35:23,150 --> 00:35:31,069
it's a lot faster and this might sound

00:35:29,390 --> 00:35:35,150
strange but there are people for whom

00:35:31,069 --> 00:35:36,559
that is okay and that we have sort of

00:35:35,150 --> 00:35:38,599
two streaming engines we have one that

00:35:36,559 --> 00:35:41,329
is probably right we have one that is

00:35:38,599 --> 00:35:43,369
probably wrong but really fast and

00:35:41,329 --> 00:35:45,799
they're each gonna try and get two

00:35:43,369 --> 00:35:47,509
probably right and kind of fast at some

00:35:45,799 --> 00:35:49,039
point but we can approach it from two

00:35:47,509 --> 00:35:51,289
directions at once inside of the same

00:35:49,039 --> 00:35:56,960
project that's a great use of developer

00:35:51,289 --> 00:35:59,210
time okay um more people about that than

00:35:56,960 --> 00:36:01,910
I expected but it's kind of cool that we

00:35:59,210 --> 00:36:03,140
have this flexibility now and so let's

00:36:01,910 --> 00:36:07,640
look at how to do streaming with this

00:36:03,140 --> 00:36:11,749
API so one of one of the things which

00:36:07,640 --> 00:36:15,200
you have to do with streaming which we

00:36:11,749 --> 00:36:17,569
could in theory fix but we haven't is

00:36:15,200 --> 00:36:21,140
you have to specify the schema of your

00:36:17,569 --> 00:36:22,579
data and this is because what we do for

00:36:21,140 --> 00:36:24,410
schema inference when we're looking at

00:36:22,579 --> 00:36:26,420
data we haven't seen as we sample it and

00:36:24,410 --> 00:36:29,239
then we go yeah this is probably what

00:36:26,420 --> 00:36:31,220
your schema looks like the problem with

00:36:29,239 --> 00:36:33,440
that is with streaming data I can't

00:36:31,220 --> 00:36:35,420
really like go ahead and construct a

00:36:33,440 --> 00:36:38,269
random sample I've got like one batch

00:36:35,420 --> 00:36:40,009
and that batch might only have like

00:36:38,269 --> 00:36:41,989
you're good customers or you're bad

00:36:40,009 --> 00:36:43,309
customers and the next batch is gonna

00:36:41,989 --> 00:36:46,369
show up with fields I didn't know you

00:36:43,309 --> 00:36:49,789
had and then I'm just like oh well I

00:36:46,369 --> 00:36:51,559
used valves everywhere oh dear okay and

00:36:49,789 --> 00:36:55,519
so um for now you have to tell it the

00:36:51,559 --> 00:36:57,470
schema and that's fine but if you do

00:36:55,519 --> 00:36:59,809
this you get back a data set and you can

00:36:57,470 --> 00:37:01,249
do the same word count I showed you

00:36:59,809 --> 00:37:05,210
earlier on top of it

00:37:01,249 --> 00:37:07,160
streaming word count it's streaming big

00:37:05,210 --> 00:37:10,369
data it's cuz they can do word count

00:37:07,160 --> 00:37:13,489
okay alternatively one of the things

00:37:10,369 --> 00:37:16,249
that I believe is that coffee is roughly

00:37:13,489 --> 00:37:17,720
equivalent to happiness but I'm aware

00:37:16,249 --> 00:37:20,779
there are people who disagree with me

00:37:17,720 --> 00:37:21,890
and that is unfortunate I'm also aware

00:37:20,779 --> 00:37:23,900
that if I trained a real machine

00:37:21,890 --> 00:37:27,049
learning model it would probably not

00:37:23,900 --> 00:37:29,749
give me the result I want so instead I'm

00:37:27,049 --> 00:37:31,549
gonna train some crap so I'm gonna group

00:37:29,749 --> 00:37:33,380
people's happiness by the number of

00:37:31,549 --> 00:37:35,509
coffees they've had compute the average

00:37:33,380 --> 00:37:36,549
happiness and then as soon as I get the

00:37:35,509 --> 00:37:39,069
result I want

00:37:36,549 --> 00:37:41,709
to execution because that is the best

00:37:39,069 --> 00:37:45,459
way to run an a/b test and there was

00:37:41,709 --> 00:37:47,739
actually companies that do that okay and

00:37:45,459 --> 00:37:49,479
so that's that's cool and then the main

00:37:47,739 --> 00:37:51,189
thing here is like all of the functional

00:37:49,479 --> 00:37:53,619
code that you wrote and all of the

00:37:51,189 --> 00:37:56,769
relational code that you wrote with the

00:37:53,619 --> 00:37:58,479
few asterisks associated with this will

00:37:56,769 --> 00:38:00,279
kind of just work on top of streaming

00:37:58,479 --> 00:38:03,160
and that's kind of neat to just be able

00:38:00,279 --> 00:38:07,959
to reuse the same types and and

00:38:03,160 --> 00:38:09,400
everything and so it's kind of fun one

00:38:07,959 --> 00:38:12,849
of the things which might happen though

00:38:09,400 --> 00:38:16,599
is because arrow is making Python a lot

00:38:12,849 --> 00:38:18,939
faster and because we're getting a lot

00:38:16,599 --> 00:38:22,599
more relational power inside of spark is

00:38:18,939 --> 00:38:25,150
Scala might start mattering less how

00:38:22,599 --> 00:38:28,959
many people came to the Scala community

00:38:25,150 --> 00:38:31,749
from spark in this audience okay only

00:38:28,959 --> 00:38:34,599
like four that's slower than I expected

00:38:31,749 --> 00:38:37,119
but if you asked this question at like a

00:38:34,599 --> 00:38:39,160
bank or someone who like is willing to

00:38:37,119 --> 00:38:41,890
pay me large sums of money in exchange

00:38:39,160 --> 00:38:44,979
for Scala code the answer is like a lot

00:38:41,890 --> 00:38:46,329
higher and I like money and you may like

00:38:44,979 --> 00:38:47,829
money too

00:38:46,329 --> 00:38:50,739
in exchange for functional programming

00:38:47,829 --> 00:38:53,439
and if if Scala becomes less the de

00:38:50,739 --> 00:38:55,719
facto way to write spark code I will not

00:38:53,439 --> 00:39:01,599
be able to get as much money from people

00:38:55,719 --> 00:39:02,979
and this is a p0 issue and honestly I

00:39:01,599 --> 00:39:04,989
like functional programming

00:39:02,979 --> 00:39:06,999
I would I would rather write functional

00:39:04,989 --> 00:39:08,979
programming for money and then Python

00:39:06,999 --> 00:39:11,289
don't tell the Python people though they

00:39:08,979 --> 00:39:16,959
have me at their conferences too they're

00:39:11,289 --> 00:39:18,939
really sweet but I like okay and right

00:39:16,959 --> 00:39:20,439
sometimes PI SPARC people we used to be

00:39:18,939 --> 00:39:22,269
able to convince them to come join us

00:39:20,439 --> 00:39:24,609
because they write this giant pile of

00:39:22,269 --> 00:39:26,920
code it kind of worked but it was really

00:39:24,609 --> 00:39:29,170
slow and then we were like listen I know

00:39:26,920 --> 00:39:32,019
your codes really slow but if you just

00:39:29,170 --> 00:39:33,609
rewrite this core piece and Scala it'll

00:39:32,019 --> 00:39:35,829
be great and it'll run really fast and

00:39:33,609 --> 00:39:38,019
then they're like well I mean how hard

00:39:35,829 --> 00:39:39,219
could it be there to learn Scala six

00:39:38,019 --> 00:39:40,959
months later they have the answer to

00:39:39,219 --> 00:39:43,809
both their questions and their code runs

00:39:40,959 --> 00:39:46,959
faster or I mean maybe a month and and

00:39:43,809 --> 00:39:49,060
that's cool but but now this isn't gonna

00:39:46,959 --> 00:39:52,510
be enough we're gonna have to offer them

00:39:49,060 --> 00:39:55,630
new shiny things too to convince Python

00:39:52,510 --> 00:39:57,400
people to come join us in our party so

00:39:55,630 --> 00:40:01,900
the the takeaways from this are data

00:39:57,400 --> 00:40:04,150
sets are a functional API kind of with a

00:40:01,900 --> 00:40:06,160
lot of asterisks associated with it and

00:40:04,150 --> 00:40:08,440
there are some things that we can do a

00:40:06,160 --> 00:40:10,240
lot easier with them we can still

00:40:08,440 --> 00:40:13,240
totally sell functional programming

00:40:10,240 --> 00:40:14,890
support contracts to banks and if anyone

00:40:13,240 --> 00:40:19,780
does that you know don't freak out just

00:40:14,890 --> 00:40:22,780
yet however were in this dangerous place

00:40:19,780 --> 00:40:24,370
where we have some important work to do

00:40:22,780 --> 00:40:26,530
to keep functional programming

00:40:24,370 --> 00:40:29,290
competitive with sequel inside of SPARC

00:40:26,530 --> 00:40:32,320
and I never thought I was gonna say that

00:40:29,290 --> 00:40:34,390
in my life like two years ago I was like

00:40:32,320 --> 00:40:35,770
no functional programming is amazing and

00:40:34,390 --> 00:40:37,050
I'm like functional programming is

00:40:35,770 --> 00:40:41,290
amazing oh crap

00:40:37,050 --> 00:40:43,510
where did that come from and so we we

00:40:41,290 --> 00:40:48,100
need to do something to keep ourselves

00:40:43,510 --> 00:40:49,600
relevant oh yeah and the other one is if

00:40:48,100 --> 00:40:50,230
anyone's trying to raise money in San

00:40:49,600 --> 00:40:51,940
Francisco

00:40:50,230 --> 00:40:55,150
you totally can do deep learning on top

00:40:51,940 --> 00:40:55,900
of SPARC don't worry just go pitch it to

00:40:55,150 --> 00:40:57,490
the VCS

00:40:55,900 --> 00:41:00,070
they don't have to read the white paper

00:40:57,490 --> 00:41:02,380
and then your intern can be responsible

00:41:00,070 --> 00:41:06,130
for your deep learning project and cash

00:41:02,380 --> 00:41:09,090
money okay oh I have a bunch of books on

00:41:06,130 --> 00:41:12,100
SPARC um some of these are not my books

00:41:09,090 --> 00:41:14,200
one of them is out-of-date fast data

00:41:12,100 --> 00:41:17,110
processing with spark is the first spark

00:41:14,200 --> 00:41:19,630
book I wrote for SPARC 0.7 and it is

00:41:17,110 --> 00:41:22,810
both really clearly for SPARC 0.7 and

00:41:19,630 --> 00:41:25,090
really clearly the first book I wrote it

00:41:22,810 --> 00:41:27,880
is rough do not buy it unless you just

00:41:25,090 --> 00:41:29,830
want to give me money because as

00:41:27,880 --> 00:41:31,840
corporate ethics training indicated to

00:41:29,830 --> 00:41:34,300
me I'm not supposed to accept $20 bills

00:41:31,840 --> 00:41:36,580
in a brown paper envelope but telling

00:41:34,300 --> 00:41:38,860
people to buy my book is still okay and

00:41:36,580 --> 00:41:40,510
as far as I can tell my boss really does

00:41:38,860 --> 00:41:44,490
not watch my talks because he has not

00:41:40,510 --> 00:41:44,490
mentioned this part to me yet

00:41:44,580 --> 00:41:48,970
alternatively if you want to give me

00:41:46,570 --> 00:41:51,520
money and possibly learn something from

00:41:48,970 --> 00:41:53,710
it you can buy high-performance SPARC

00:41:51,520 --> 00:41:55,870
and it is all about all of the cool

00:41:53,710 --> 00:41:57,880
things that we can do with Scala and

00:41:55,870 --> 00:42:00,130
SPARC together that are really awesome

00:41:57,880 --> 00:42:01,559
and totally faster than the Python

00:42:00,130 --> 00:42:03,390
version

00:42:01,559 --> 00:42:05,279
it was written before Python sparks

00:42:03,390 --> 00:42:07,589
started to get fast but that should not

00:42:05,279 --> 00:42:10,579
stop you from giving me your money or

00:42:07,589 --> 00:42:13,319
more specifically your employer's money

00:42:10,579 --> 00:42:15,359
corporate expense accounts are amazing

00:42:13,319 --> 00:42:18,989
and Jeff Bezos wants to buy an out of

00:42:15,359 --> 00:42:20,729
the newspaper okay um if you want to

00:42:18,989 --> 00:42:22,619
follow me for live streams I will be

00:42:20,729 --> 00:42:24,989
doing a live stream this Friday where I

00:42:22,619 --> 00:42:26,579
review some spark code I normally try

00:42:24,989 --> 00:42:30,900
and do them weekly but my travel

00:42:26,579 --> 00:42:34,559
schedule is entertaining and if you want

00:42:30,900 --> 00:42:37,170
an excuse to go to Europe or stay in New

00:42:34,559 --> 00:42:39,930
York or go to st. Louis they have a

00:42:37,170 --> 00:42:42,210
really wonderful Museum in st. Louis you

00:42:39,930 --> 00:42:44,430
can come join me in these fine cities

00:42:42,210 --> 00:42:47,130
for these very legitimate conferences

00:42:44,430 --> 00:42:49,710
and if your boss asks I will tell him or

00:42:47,130 --> 00:42:52,319
her that it is a very important or them

00:42:49,710 --> 00:42:53,729
sorry a very important conference that

00:42:52,319 --> 00:42:55,680
you should definitely attend for

00:42:53,729 --> 00:42:57,329
business reasons and if you tell me what

00:42:55,680 --> 00:43:01,499
business you're in beforehand I can even

00:42:57,329 --> 00:43:02,880
make a more convincing statement so I

00:43:01,499 --> 00:43:05,029
mean that's pretty much it okay thanks

00:43:02,880 --> 00:43:05,029
bye

00:43:12,280 --> 00:43:17,450
do we have time for questions

00:43:15,100 --> 00:43:19,190
hello we have a couple of minutes a

00:43:17,450 --> 00:43:22,850
couple of questions okay cool yeah if

00:43:19,190 --> 00:43:25,190
anyone has questions I would be happy to

00:43:22,850 --> 00:43:26,810
answer them there's a microphone maybe

00:43:25,190 --> 00:43:31,340
you have a microphone you just talked

00:43:26,810 --> 00:43:32,900
into it yes yes I have to take a picture

00:43:31,340 --> 00:43:38,030
before people leaves from my boss thinks

00:43:32,900 --> 00:43:47,560
this was worth money one question yeah

00:43:38,030 --> 00:43:50,450
questions hello you show a graphic

00:43:47,560 --> 00:43:54,500
comparison of performance between RDD

00:43:50,450 --> 00:43:57,770
and that's a set goodbye yeah I suppose

00:43:54,500 --> 00:44:01,750
that the better performance of the data

00:43:57,770 --> 00:44:05,440
set come from the tunston optimization

00:44:01,750 --> 00:44:09,640
let me go find that graph really quickly

00:44:05,440 --> 00:44:12,410
sorry there's a yeah that one okay so

00:44:09,640 --> 00:44:15,970
the better performance comes from a feud

00:44:12,410 --> 00:44:18,350
oh the one before okay

00:44:15,970 --> 00:44:24,290
well yes show is this the graph later

00:44:18,350 --> 00:44:26,150
that you want it yeah yes so I support

00:44:24,290 --> 00:44:28,220
that the frame is better here because of

00:44:26,150 --> 00:44:32,210
the tungsten and then capital is

00:44:28,220 --> 00:44:35,390
optimization um so okay there are a few

00:44:32,210 --> 00:44:36,890
reasons why it's better so um if you

00:44:35,390 --> 00:44:39,110
look at it there's there's the red bar

00:44:36,890 --> 00:44:42,170
which is group by key which is just

00:44:39,110 --> 00:44:44,570
really bad yeah and spark and then

00:44:42,170 --> 00:44:48,260
reduce by key is reasonably good in

00:44:44,570 --> 00:44:51,290
spark and then the data frame data set

00:44:48,260 --> 00:44:54,200
version and so the performance speed-up

00:44:51,290 --> 00:44:55,700
comes from both it using less memory and

00:44:54,200 --> 00:44:57,710
less network bandwidth because of the

00:44:55,700 --> 00:44:59,300
more efficient storage but the other

00:44:57,710 --> 00:45:02,270
part which is really useful here is

00:44:59,300 --> 00:45:06,800
because we're we have a shuffle stage is

00:45:02,270 --> 00:45:09,200
we can actually the the serialized data

00:45:06,800 --> 00:45:11,030
is directly sortable so if we have an

00:45:09,200 --> 00:45:15,440
external shuffle service it doesn't have

00:45:11,030 --> 00:45:17,590
to deserialize the data to sort it which

00:45:15,440 --> 00:45:20,390
is useful because serialization time

00:45:17,590 --> 00:45:23,450
takes up a non-trivial part of the

00:45:20,390 --> 00:45:28,580
reduce by key implementation and the

00:45:23,450 --> 00:45:31,700
other one is the data frame code path is

00:45:28,580 --> 00:45:33,320
slightly better than the average code

00:45:31,700 --> 00:45:37,850
path with 3d spiky because it's it's

00:45:33,320 --> 00:45:40,070
written to reuse one of the variables

00:45:37,850 --> 00:45:43,250
behind the back so that GC doesn't trash

00:45:40,070 --> 00:45:44,750
quite as much and those those were the

00:45:43,250 --> 00:45:50,660
things that made this a little bit

00:45:44,750 --> 00:45:52,300
faster okay I was about to ask why what

00:45:50,660 --> 00:45:55,640
the other reason we cannot use like

00:45:52,300 --> 00:45:59,960
apply the constant optimization on top

00:45:55,640 --> 00:46:03,050
of the RDD result so that's that's a

00:45:59,960 --> 00:46:05,480
good question and to some degree like

00:46:03,050 --> 00:46:07,880
the tungsten optimization is actually

00:46:05,480 --> 00:46:12,590
running on top of rdd's right like at

00:46:07,880 --> 00:46:15,470
the end of the day data frames more or

00:46:12,590 --> 00:46:18,200
less compile down to two rdd's sort of

00:46:15,470 --> 00:46:23,540
behind your back with a bunch of weird

00:46:18,200 --> 00:46:26,170
 happening in between you could kind

00:46:23,540 --> 00:46:29,660
of use the tungsten serializer if you

00:46:26,170 --> 00:46:32,870
wanted to with rdd's and tried really

00:46:29,660 --> 00:46:35,300
hard but it's it's a bachelor réaliser

00:46:32,870 --> 00:46:36,950
rather than a record serializer so it

00:46:35,300 --> 00:46:40,010
wouldn't directly translate quite as

00:46:36,950 --> 00:46:49,490
well I mean if you really want to do

00:46:40,010 --> 00:46:55,220
that like go ahead I guess but why like

00:46:49,490 --> 00:46:57,620
what why do you is it I don't know why

00:46:55,220 --> 00:46:59,420
you would want to do that I guess is the

00:46:57,620 --> 00:47:02,990
answer you could it would be a lot of

00:46:59,420 --> 00:47:04,820
work and I don't think it would give you

00:47:02,990 --> 00:47:09,100
anything that you would not get from

00:47:04,820 --> 00:47:09,100
just using the data set API itself

00:47:11,690 --> 00:47:19,260
cool um so every time yeah okay cool

00:47:16,440 --> 00:47:21,240
I am not exactly subtle you can come

00:47:19,260 --> 00:47:22,880
find me I have colored hair and the

00:47:21,240 --> 00:47:28,219
stuffed animal

00:47:22,880 --> 00:47:28,219

YouTube URL: https://www.youtube.com/watch?v=gJ7BV6PSFag


