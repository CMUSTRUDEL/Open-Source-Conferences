Title: Compiling like a boss!   by Mirco Dotta & Iulian Dragos
Publication date: 2017-06-28
Playlist: Scala Days 2017
Description: 
	This video was recorded at Scala Days Copenhagen 2017
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

Abstract: 
We all love Scala, but the one aspect we have a hard time accepting are long compile times. It’s not uncommon for a project to experience compilation times of a handful of minutes, if not worse. On top of that, compilation times are unpredictable, depending on a combination of language features, external libraries, and type annotations. A single line change may increase compilation times ten fold.
What can we do? It’s paramount we gain greater insight into the tools and libraries we use. There are also established (anti-)patterns that you should know about, if you fancy to keep compilation times to a minimum. And why not utilizing all cores when compiling? The stock Scala compiler can’t do it, but Triplequote Hydra is here to change that. Sit tight and let’s cut down on compilation time!
Captions: 
	00:00:03,980 --> 00:00:10,809
hello everyone yeah that was okay that

00:00:08,120 --> 00:00:13,610
was a bit love I'm gonna I'll be quiet

00:00:10,809 --> 00:00:17,180
so today we're going to be talking about

00:00:13,610 --> 00:00:22,450
computation time who who has a problem

00:00:17,180 --> 00:00:26,960
with compile time - skele nice all right

00:00:22,450 --> 00:00:29,420
so it's me Eugen draggers we are the

00:00:26,960 --> 00:00:31,520
co-founder of triple quad we've been

00:00:29,420 --> 00:00:33,170
very active in the past ten years in the

00:00:31,520 --> 00:00:37,040
scale community working on different

00:00:33,170 --> 00:00:38,540
mainstream projects and while among

00:00:37,040 --> 00:00:40,490
other things we've also been part of

00:00:38,540 --> 00:00:43,550
light band typesafe and even before that

00:00:40,490 --> 00:00:46,040
scale solutions but we decided to create

00:00:43,550 --> 00:00:49,489
triple quote because our mission is to

00:00:46,040 --> 00:00:51,230
cut down on compile time well as work

00:00:49,489 --> 00:00:55,460
fights due to long compile time in the

00:00:51,230 --> 00:00:58,550
scale community so have you ever felt

00:00:55,460 --> 00:01:00,530
this way we all feel the Scala is such a

00:00:58,550 --> 00:01:03,920
great language we all feel so much

00:01:00,530 --> 00:01:05,089
productive with it but are we have you

00:01:03,920 --> 00:01:06,680
ever thought about all the time that

00:01:05,089 --> 00:01:08,270
you're wasting just waiting for the

00:01:06,680 --> 00:01:09,679
compiler to get back to you so that you

00:01:08,270 --> 00:01:13,130
can you know move on to the next thing

00:01:09,679 --> 00:01:15,409
that you want to do well I definitely

00:01:13,130 --> 00:01:17,810
fell that way and I got to wonder what

00:01:15,409 --> 00:01:19,249
can we do about it so we're not the

00:01:17,810 --> 00:01:20,509
first one thinking about it and it's

00:01:19,249 --> 00:01:23,420
like you know you can go and build

00:01:20,509 --> 00:01:25,579
sophisticated technology to pull you

00:01:23,420 --> 00:01:29,929
back to work once the scalar compiler is

00:01:25,579 --> 00:01:31,789
done with this work but yeah that's you

00:01:29,929 --> 00:01:33,619
know that's not too bad but can we do

00:01:31,789 --> 00:01:37,130
better than that can we actually fix and

00:01:33,619 --> 00:01:40,279
improve us Calla compile times of course

00:01:37,130 --> 00:01:41,659
we think we can and this is one is what

00:01:40,279 --> 00:01:45,289
we're going to be talking about today

00:01:41,659 --> 00:01:47,689
so I start by discussing the state of

00:01:45,289 --> 00:01:49,939
discoloured releases can we say anything

00:01:47,689 --> 00:01:52,849
about compile time as you know new

00:01:49,939 --> 00:01:54,889
releases of Scala are happening and and

00:01:52,849 --> 00:01:57,200
I will spend a bit of time seeing what

00:01:54,889 --> 00:01:59,299
are the tooling available today that can

00:01:57,200 --> 00:02:01,849
help us investigate and detecting those

00:01:59,299 --> 00:02:04,099
compile time in efficiencies and last

00:02:01,849 --> 00:02:06,499
but not least we'll talk about I draw

00:02:04,099 --> 00:02:08,060
which is our product and you know

00:02:06,499 --> 00:02:09,860
awfully you'll agree with us that is the

00:02:08,060 --> 00:02:12,769
free lunch you can just use Hydra and

00:02:09,860 --> 00:02:16,280
reap the benefits of compiling faster

00:02:12,769 --> 00:02:17,480
your Scalla code all right so scale

00:02:16,280 --> 00:02:20,360
releases

00:02:17,480 --> 00:02:23,750
schuyler to the 12 was released back in

00:02:20,360 --> 00:02:26,450
November and many many many great

00:02:23,750 --> 00:02:28,340
features but the one that I really like

00:02:26,450 --> 00:02:30,560
about it is the fact that they they

00:02:28,340 --> 00:02:33,319
target while skele to Trails targets the

00:02:30,560 --> 00:02:35,600
Java Runtime so it actually takes

00:02:33,319 --> 00:02:38,510
advantage of all the goodness ease that

00:02:35,600 --> 00:02:41,110
are available on Java 8 so what does it

00:02:38,510 --> 00:02:44,360
means for us well we got a reduced

00:02:41,110 --> 00:02:46,819
binary footprint when we compiled skala

00:02:44,360 --> 00:02:49,489
code why well because the Scala compiler

00:02:46,819 --> 00:02:51,950
knows is as a different encoding for

00:02:49,489 --> 00:02:54,470
traits and a different coding for

00:02:51,950 --> 00:02:57,160
lambdas and it's using what is available

00:02:54,470 --> 00:02:59,060
in the jeddak well in the Java Runtime

00:02:57,160 --> 00:03:02,390
this is awesome

00:02:59,060 --> 00:03:04,519
and you know it's I'm extremely thankful

00:03:02,390 --> 00:03:07,010
to the light that scaloppine for doing

00:03:04,519 --> 00:03:08,959
that it's it's a great thing but what

00:03:07,010 --> 00:03:11,239
about performance of the scale compiler

00:03:08,959 --> 00:03:13,540
what can we say about if we compare to

00:03:11,239 --> 00:03:16,640
test performance with 211 performance

00:03:13,540 --> 00:03:18,890
well if you talk well at least a couple

00:03:16,640 --> 00:03:21,280
months ago we talked to my saving the

00:03:18,890 --> 00:03:25,010
author of shapeless and for him actually

00:03:21,280 --> 00:03:29,209
combining shapeless tests with 212 was

00:03:25,010 --> 00:03:31,220
lower than with 211 so that that was of

00:03:29,209 --> 00:03:33,079
course it's not ideal but on the other

00:03:31,220 --> 00:03:35,810
side if you talk to the skeleton on

00:03:33,079 --> 00:03:38,180
their benchmarks they seems to have

00:03:35,810 --> 00:03:41,090
better computation times with Scala 212

00:03:38,180 --> 00:03:43,489
which is great so what can we draw from

00:03:41,090 --> 00:03:45,410
that is that it's not easy to tell

00:03:43,489 --> 00:03:48,200
whether Scala is getting any faster as

00:03:45,410 --> 00:03:49,519
the you know as we get new releases but

00:03:48,200 --> 00:03:51,410
the good news about it is that the

00:03:49,519 --> 00:03:53,180
skeleton will acknowledge the problem

00:03:51,410 --> 00:03:55,160
and is working on it is actively working

00:03:53,180 --> 00:03:57,920
on it and are working on automated

00:03:55,160 --> 00:03:59,750
benchmarking so that they can detect if

00:03:57,920 --> 00:04:02,209
there was any regression happening maybe

00:03:59,750 --> 00:04:03,440
in one of the past releases so that you

00:04:02,209 --> 00:04:05,660
know they can they can correct the

00:04:03,440 --> 00:04:07,209
problem now and they also can make sure

00:04:05,660 --> 00:04:09,799
that there's not going to be any other

00:04:07,209 --> 00:04:11,810
regression that goes unnoticed any

00:04:09,799 --> 00:04:14,389
change that let's say brings down

00:04:11,810 --> 00:04:16,340
computation time it will be visible and

00:04:14,389 --> 00:04:20,269
it will be up to them to decide whether

00:04:16,340 --> 00:04:23,000
you know the trade-off is right so it

00:04:20,269 --> 00:04:25,970
kind of leaves a question for us they're

00:04:23,000 --> 00:04:29,270
working on improving computation speed

00:04:25,970 --> 00:04:30,289
of the scalar compiler so we're done we

00:04:29,270 --> 00:04:31,260
don't have to do anything because

00:04:30,289 --> 00:04:32,670
they're going to you know

00:04:31,260 --> 00:04:33,840
they're going to take this problem for

00:04:32,670 --> 00:04:36,840
them and they're going to solve it for

00:04:33,840 --> 00:04:39,000
everyone of course it's not the case we

00:04:36,840 --> 00:04:42,090
can do still plenty of I'd say silly

00:04:39,000 --> 00:04:44,580
things in our code that have a huge

00:04:42,090 --> 00:04:46,440
impact on compile times just think about

00:04:44,580 --> 00:04:49,140
our you know you define a use implicit

00:04:46,440 --> 00:04:50,550
or maybe you define some macros all

00:04:49,140 --> 00:04:53,670
these things of course they're going to

00:04:50,550 --> 00:04:56,010
add up on your compile times and and so

00:04:53,670 --> 00:04:59,070
you want to understand what's happening

00:04:56,010 --> 00:05:02,220
and potentially you want to detect and

00:04:59,070 --> 00:05:06,090
fix inefficiencies that you're adding in

00:05:02,220 --> 00:05:07,560
your code base so what can we do I'd say

00:05:06,090 --> 00:05:09,870
the first thing is that we need to be

00:05:07,560 --> 00:05:12,300
aware of what are the existing tooling

00:05:09,870 --> 00:05:15,750
that can L with the task of well on one

00:05:12,300 --> 00:05:18,540
side detecting inefficiencies and on the

00:05:15,750 --> 00:05:21,030
other side are fixing them and this is

00:05:18,540 --> 00:05:24,870
what we're going to be looking at in in

00:05:21,030 --> 00:05:27,990
the first part of our talk so the scale

00:05:24,870 --> 00:05:29,430
compiler has a bazillion of options I

00:05:27,990 --> 00:05:32,550
don't know if you've ever explored them

00:05:29,430 --> 00:05:35,610
but if you do scala c- x or skeleton -

00:05:32,550 --> 00:05:39,300
why you see many many many options and

00:05:35,610 --> 00:05:41,700
and some of them are extremely useful -

00:05:39,300 --> 00:05:44,340
well to help you bring down compile

00:05:41,700 --> 00:05:46,650
times so there are some really easy ones

00:05:44,340 --> 00:05:49,560
that you can you can use tomorrow or

00:05:46,650 --> 00:05:51,810
today and an immediately spot if you

00:05:49,560 --> 00:05:53,370
have that code so code that is basically

00:05:51,810 --> 00:05:56,100
there but it's not used by your

00:05:53,370 --> 00:05:58,020
application and of course the last code

00:05:56,100 --> 00:06:01,980
you need to compile the faster it would

00:05:58,020 --> 00:06:04,950
be so of course every and each of these

00:06:01,980 --> 00:06:06,810
flags as an overhead as small as it is

00:06:04,950 --> 00:06:09,240
it's still and over it so maybe you

00:06:06,810 --> 00:06:11,190
don't want to run them every time you

00:06:09,240 --> 00:06:12,930
compile locally but I'd say you should

00:06:11,190 --> 00:06:14,460
definitely put them on your CI so that

00:06:12,930 --> 00:06:16,320
at least you you getting information

00:06:14,460 --> 00:06:19,050
maybe the recent code that you can

00:06:16,320 --> 00:06:22,370
remove and have any information on the

00:06:19,050 --> 00:06:24,920
CI I think it's a cost that it's okay

00:06:22,370 --> 00:06:27,390
but I'd say that those are the

00:06:24,920 --> 00:06:30,470
low-hanging fruits are the easy ones to

00:06:27,390 --> 00:06:32,280
catch a related computation speed and

00:06:30,470 --> 00:06:34,200
the other thing I would say is that

00:06:32,280 --> 00:06:37,470
you're probably not going to get 10 20

00:06:34,200 --> 00:06:39,270
30 % are speed up by by enabling those

00:06:37,470 --> 00:06:40,350
flags and removing that code unless you

00:06:39,270 --> 00:06:44,060
have a lot of that code in your

00:06:40,350 --> 00:06:46,820
application so what do you do next

00:06:44,060 --> 00:06:49,340
well the next thing is that you'll have

00:06:46,820 --> 00:06:50,930
to be wearing your detective ID and sort

00:06:49,340 --> 00:06:53,870
of be ready to understand what the

00:06:50,930 --> 00:06:57,590
compiler does understanding well

00:06:53,870 --> 00:07:00,290
actually our Scala treats your code and

00:06:57,590 --> 00:07:01,730
compile it into byte code and there are

00:07:00,290 --> 00:07:03,860
a number of flags that can help you with

00:07:01,730 --> 00:07:05,630
with spotting inefficiencies when it

00:07:03,860 --> 00:07:08,870
comes to compile time the first one I

00:07:05,630 --> 00:07:10,790
would say - verbose so - verbose gives

00:07:08,870 --> 00:07:12,320
you a ton of information that I'd say

00:07:10,790 --> 00:07:14,330
you don't care about when you're looking

00:07:12,320 --> 00:07:16,790
at compile times but there is one thing

00:07:14,330 --> 00:07:19,130
that is extremely useful it gives you

00:07:16,790 --> 00:07:20,750
the compile time by phase your program

00:07:19,130 --> 00:07:23,780
so as you may know the Scala compiler

00:07:20,750 --> 00:07:26,180
has many phases so your program goes

00:07:23,780 --> 00:07:28,480
through many transformations are

00:07:26,180 --> 00:07:31,490
actually 25 in the skeleton fighter and

00:07:28,480 --> 00:07:34,640
what - verbose tells you is how much

00:07:31,490 --> 00:07:37,910
time is it taking every phase you know

00:07:34,640 --> 00:07:40,700
for my compiler for the completion so

00:07:37,910 --> 00:07:42,520
that gives you some initial hints about

00:07:40,700 --> 00:07:45,200
potential problems that you may have

00:07:42,520 --> 00:07:47,450
another one that is really useful is

00:07:45,200 --> 00:07:50,180
this - x print and now you have the

00:07:47,450 --> 00:07:53,630
phase name so what - X spring gives you

00:07:50,180 --> 00:07:56,330
is the abstract syntax tree so the ast

00:07:53,630 --> 00:07:59,090
that is seen by the compiler

00:07:56,330 --> 00:08:00,560
after the given phase so if you don't

00:07:59,090 --> 00:08:02,720
know what are the phase or the phase

00:08:00,560 --> 00:08:04,970
names in the compiler you can just use -

00:08:02,720 --> 00:08:07,250
actual phases and that will list all the

00:08:04,970 --> 00:08:08,780
phases in the compiler and then let's

00:08:07,250 --> 00:08:10,940
say you want to compile you compile one

00:08:08,780 --> 00:08:13,580
source file you'll pass the option - X

00:08:10,940 --> 00:08:15,980
print then let's say typer

00:08:13,580 --> 00:08:17,840
which is type checking and you see what

00:08:15,980 --> 00:08:20,840
the code looks like after the type

00:08:17,840 --> 00:08:24,110
checking phase and I'll tell you a bit

00:08:20,840 --> 00:08:28,760
more about why this is useful in in the

00:08:24,110 --> 00:08:32,270
next slide then if you think implicit

00:08:28,760 --> 00:08:34,790
may be the reason why it's taking too

00:08:32,270 --> 00:08:37,490
much time to compile then - X log

00:08:34,790 --> 00:08:39,080
implicit it gives you a trace of which

00:08:37,490 --> 00:08:42,560
one are the increases that the compiler

00:08:39,080 --> 00:08:44,900
attempted on your on your source on your

00:08:42,560 --> 00:08:46,700
source files but they did not succeed

00:08:44,900 --> 00:08:48,740
of course the more implicit the

00:08:46,700 --> 00:08:51,230
comparator is obtaining the longer it

00:08:48,740 --> 00:08:54,020
will take to compile and last but not

00:08:51,230 --> 00:08:56,390
least if you're using or depending on

00:08:54,020 --> 00:08:57,860
code that uses macros I might be useful

00:08:56,390 --> 00:09:01,310
to know about this

00:08:57,860 --> 00:09:04,100
two micro flags that gives you insights

00:09:01,310 --> 00:09:07,040
into what the competitor is doing during

00:09:04,100 --> 00:09:08,899
micro expansion so in my experience or

00:09:07,040 --> 00:09:11,060
in our experience I would say those are

00:09:08,899 --> 00:09:13,820
the flags that have been extremely

00:09:11,060 --> 00:09:18,260
useful for pinpointing compile-time

00:09:13,820 --> 00:09:20,750
inefficiencies so how do you start well

00:09:18,260 --> 00:09:23,720
let's start by using - they're both on

00:09:20,750 --> 00:09:26,060
your code base get a get a view of your

00:09:23,720 --> 00:09:27,829
compile time by phase so this is

00:09:26,060 --> 00:09:29,720
basically what it would look like of

00:09:27,829 --> 00:09:31,970
course you know you don't get a chart as

00:09:29,720 --> 00:09:33,079
an output you get numbers but they you

00:09:31,970 --> 00:09:36,890
can put it on a chart

00:09:33,079 --> 00:09:41,089
so on the y-axis we have the compiler

00:09:36,890 --> 00:09:42,529
phases and on the XS we have the time

00:09:41,089 --> 00:09:45,140
that it takes for each of these phase

00:09:42,529 --> 00:09:47,180
and here we're basically comparing two

00:09:45,140 --> 00:09:49,190
projects one is the a character module

00:09:47,180 --> 00:09:51,620
and the other is a project that we've

00:09:49,190 --> 00:09:53,470
been working with so we've been doing

00:09:51,620 --> 00:09:56,360
some consulting work with the lambda and

00:09:53,470 --> 00:09:58,519
two of them cut them down compile times

00:09:56,360 --> 00:10:00,890
and this is basically what we've seen

00:09:58,519 --> 00:10:02,899
when we came to well when we arrived to

00:10:00,890 --> 00:10:04,279
the project we were saying that well

00:10:02,899 --> 00:10:06,500
there is one thing that is jumping to

00:10:04,279 --> 00:10:09,920
the eye right there is that huge green

00:10:06,500 --> 00:10:13,630
bar and what do you think is that phase

00:10:09,920 --> 00:10:17,089
does anyone have a guess for that parser

00:10:13,630 --> 00:10:19,420
it's not that it's diaper it's eye

00:10:17,089 --> 00:10:19,420
tracking

00:10:19,570 --> 00:10:25,040
so type checking in that project was

00:10:22,550 --> 00:10:30,649
taking an awful long time he was taking

00:10:25,040 --> 00:10:33,320
32 seconds out of the 55 that you know

00:10:30,649 --> 00:10:36,620
of the old computation time so more than

00:10:33,320 --> 00:10:38,930
50% and you know the reason for that is

00:10:36,620 --> 00:10:41,899
that that project was using some type

00:10:38,930 --> 00:10:42,680
intensive library and of course you know

00:10:41,899 --> 00:10:44,690
it takes time

00:10:42,680 --> 00:10:47,209
well the type checker does has to do

00:10:44,690 --> 00:10:50,269
more work yes to generate things so it's

00:10:47,209 --> 00:10:51,740
definitely a costly operation but you

00:10:50,269 --> 00:10:53,959
know maybe was taking too much time in

00:10:51,740 --> 00:10:55,399
fact it was definitely taking too much

00:10:53,959 --> 00:10:58,910
time and as that you'll be more about

00:10:55,399 --> 00:11:01,250
this in the next slides as well and so

00:10:58,910 --> 00:11:03,170
whenever you see the typer is taking

00:11:01,250 --> 00:11:05,740
more than 50% of the type checking time

00:11:03,170 --> 00:11:08,240
I would probably say even more than 40%

00:11:05,740 --> 00:11:11,610
it might be ok but you should know why

00:11:08,240 --> 00:11:13,079
if you don't then I think it's you're it

00:11:11,610 --> 00:11:15,560
opportunity to investigate and

00:11:13,079 --> 00:11:18,779
understanding what's going on and

00:11:15,560 --> 00:11:20,940
usually if diaper is around 30% as it

00:11:18,779 --> 00:11:24,899
was the as it is the case for the a

00:11:20,940 --> 00:11:28,950
character module I'd say that normal 30%

00:11:24,899 --> 00:11:31,769
is about what you can expect so

00:11:28,950 --> 00:11:34,350
you investigate and what do you focus on

00:11:31,769 --> 00:11:36,750
well they're our usual suspects for long

00:11:34,350 --> 00:11:38,820
complies lines our long type-checking

00:11:36,750 --> 00:11:40,500
time usually type checking as I said and

00:11:38,820 --> 00:11:42,930
what is that takes time well there is

00:11:40,500 --> 00:11:45,860
micros and micro expansions very is

00:11:42,930 --> 00:11:48,120
implicit and there is also I would say

00:11:45,860 --> 00:11:49,740
well sometimes there is a correlation

00:11:48,120 --> 00:11:52,019
between micros and implicit because

00:11:49,740 --> 00:11:54,779
sometimes you can have implicit that are

00:11:52,019 --> 00:11:56,640
implemented with macros so this is

00:11:54,779 --> 00:11:59,250
definitely a case when it can be really

00:11:56,640 --> 00:12:01,589
costly to execute that code to compile

00:11:59,250 --> 00:12:03,000
that code and last but not least there

00:12:01,589 --> 00:12:05,880
there is also type inference as

00:12:03,000 --> 00:12:08,130
marketing said in his keynote there are

00:12:05,880 --> 00:12:10,620
cases where it can be costly to infer

00:12:08,130 --> 00:12:12,420
the type of course there's been many

00:12:10,620 --> 00:12:14,519
many many many improvements to avoid

00:12:12,420 --> 00:12:16,470
those problems but you know there are

00:12:14,519 --> 00:12:18,540
still there are still possibilities when

00:12:16,470 --> 00:12:20,730
it takes time to generate the type and

00:12:18,540 --> 00:12:23,070
and so you can get some easy game and in

00:12:20,730 --> 00:12:25,320
fact I think it was about a month ago or

00:12:23,070 --> 00:12:28,589
two months ago on on Twitter I saw this

00:12:25,320 --> 00:12:30,600
tweet from someone contributing to

00:12:28,589 --> 00:12:33,990
IntelliJ where they say that by putting

00:12:30,600 --> 00:12:36,420
explicit types they got 17% speed up

00:12:33,990 --> 00:12:37,920
just by that so I'm not saying that

00:12:36,420 --> 00:12:40,170
that's what's gonna happen in your code

00:12:37,920 --> 00:12:41,670
base but maybe something that you you

00:12:40,170 --> 00:12:43,470
know you should try maybe just by adding

00:12:41,670 --> 00:12:48,000
explicit types you're going to get some

00:12:43,470 --> 00:12:50,160
real boost that are compiling alright so

00:12:48,000 --> 00:12:52,560
how do you go about identifying problems

00:12:50,160 --> 00:12:55,230
well the first thing is that you want to

00:12:52,560 --> 00:12:57,329
focus your effort on some source file so

00:12:55,230 --> 00:12:59,310
you basically want to know what are the

00:12:57,329 --> 00:13:01,949
source files that are being the more

00:12:59,310 --> 00:13:03,480
costly to compile the bad news about

00:13:01,949 --> 00:13:05,160
that is that there is no tool at the

00:13:03,480 --> 00:13:07,290
moment that can help you with it we're

00:13:05,160 --> 00:13:09,390
actually working on this exact problem

00:13:07,290 --> 00:13:12,480
and using we'll tell you a bit more in

00:13:09,390 --> 00:13:14,699
the second part but if you have a guess

00:13:12,480 --> 00:13:16,620
for what is the source that may be

00:13:14,699 --> 00:13:18,570
taking a lot of time to compile maybe

00:13:16,620 --> 00:13:20,970
because you're using some type intensive

00:13:18,570 --> 00:13:23,550
library in some place well then you know

00:13:20,970 --> 00:13:25,590
you can validate that gas by trying to

00:13:23,550 --> 00:13:27,510
by combining that single sir

00:13:25,590 --> 00:13:29,760
and see how much time it takes in

00:13:27,510 --> 00:13:32,160
isolation if you see that it takes more

00:13:29,760 --> 00:13:34,200
than expected well then that's where you

00:13:32,160 --> 00:13:36,770
should uh you know put your effort and

00:13:34,200 --> 00:13:40,260
investigate so how do you go with that

00:13:36,770 --> 00:13:43,620
well you know you want to print the tree

00:13:40,260 --> 00:13:46,380
off of that source file after typer and

00:13:43,620 --> 00:13:47,910
you want to look at it and see is there

00:13:46,380 --> 00:13:50,640
a huge amount of code that is generated

00:13:47,910 --> 00:13:54,330
that was not in the original source if

00:13:50,640 --> 00:13:58,350
that's the case then macros are at play

00:13:54,330 --> 00:14:02,220
and that's likely the reason why it's

00:13:58,350 --> 00:14:05,130
taking so much or is there any complex

00:14:02,220 --> 00:14:07,560
or long inferred type that's another

00:14:05,130 --> 00:14:09,980
reason why you know that could explain

00:14:07,560 --> 00:14:12,839
why it's taking so long to compile and

00:14:09,980 --> 00:14:15,060
in case none of those are actually an

00:14:12,839 --> 00:14:16,800
explanation you're not using macros then

00:14:15,060 --> 00:14:18,930
I'd say that most likely it's because of

00:14:16,800 --> 00:14:22,260
implicit and if it's because of implicit

00:14:18,930 --> 00:14:24,420
use - X log implicit to get a trace on

00:14:22,260 --> 00:14:27,140
that source file of all many implicit

00:14:24,420 --> 00:14:31,350
have been attempted by the compiler and

00:14:27,140 --> 00:14:34,500
that will tell you why taking so long to

00:14:31,350 --> 00:14:38,220
compile so we're talking about implicit

00:14:34,500 --> 00:14:42,410
resolution who knows how implicit

00:14:38,220 --> 00:14:42,410
resolution is done by the Scala compiler

00:14:42,920 --> 00:14:48,270
okay

00:14:44,089 --> 00:14:52,160
I'd say I don't know maybe 20% so yes

00:14:48,270 --> 00:14:54,960
this is I'd say this is a tricky subject

00:14:52,160 --> 00:14:57,300
but is extremely interesting on one side

00:14:54,960 --> 00:14:59,040
and I definitely think that every

00:14:57,300 --> 00:15:00,960
scallop programmer should understand or

00:14:59,040 --> 00:15:02,430
at least they should have an intuition

00:15:00,960 --> 00:15:04,290
of how this kind of compare does it

00:15:02,430 --> 00:15:09,120
because it plays a huge role on your

00:15:04,290 --> 00:15:11,339
compile times so you want to learn about

00:15:09,120 --> 00:15:13,680
our the Scala compiler does interested

00:15:11,339 --> 00:15:15,510
resolution all you should go and read

00:15:13,680 --> 00:15:17,760
chapter seven in the scale of reference

00:15:15,510 --> 00:15:21,140
the scale of reference is actually very

00:15:17,760 --> 00:15:24,390
readable it's not just made for PhD

00:15:21,140 --> 00:15:26,730
student it can be read by any one of us

00:15:24,390 --> 00:15:28,140
and it gives you a formal definition of

00:15:26,730 --> 00:15:29,430
what the compiler does so you get a

00:15:28,140 --> 00:15:33,120
really good understanding of what's

00:15:29,430 --> 00:15:36,049
happening shall we take a ten minute

00:15:33,120 --> 00:15:39,480
break now we can do that

00:15:36,049 --> 00:15:42,600
good but yeah maybe not let's build an

00:15:39,480 --> 00:15:44,309
intuition of what the compiler does so

00:15:42,600 --> 00:15:45,660
we have this code that I would

00:15:44,309 --> 00:15:48,089
definitely not recommend that you write

00:15:45,660 --> 00:15:50,160
but let's say that we have that show

00:15:48,089 --> 00:15:52,319
method that takes a map and then we're

00:15:50,160 --> 00:15:54,660
calling that method passing just you

00:15:52,319 --> 00:15:57,269
know an employee which is definitely not

00:15:54,660 --> 00:15:58,859
a map so for that to compile we know

00:15:57,269 --> 00:16:00,989
that the compiler is going to look for

00:15:58,859 --> 00:16:03,419
an implicit right if you can't find it

00:16:00,989 --> 00:16:05,429
it will fail we will get a type checking

00:16:03,419 --> 00:16:07,649
error but let's say that it will succeed

00:16:05,429 --> 00:16:10,309
so what is the compiler doing to make

00:16:07,649 --> 00:16:14,549
that dinner to make that statement work

00:16:10,309 --> 00:16:16,649
it will start by looking at the current

00:16:14,549 --> 00:16:21,089
scope it will basically see if there is

00:16:16,649 --> 00:16:23,609
any implicit definition implicit any

00:16:21,089 --> 00:16:25,730
implicit definition yes defining the

00:16:23,609 --> 00:16:29,999
current scope so it's available without

00:16:25,730 --> 00:16:32,759
without prefix it and if there is an

00:16:29,999 --> 00:16:35,399
implicit available that such that

00:16:32,759 --> 00:16:38,489
employee can be transformed in a map of

00:16:35,399 --> 00:16:41,819
ID employee it will apply that implicit

00:16:38,489 --> 00:16:45,239
and basically we'll ill our expression

00:16:41,819 --> 00:16:47,609
so that the code type checks right well

00:16:45,239 --> 00:16:49,949
let's say that it doesn't find it in in

00:16:47,609 --> 00:16:51,779
the current scope do you know what's

00:16:49,949 --> 00:16:54,239
going to be in the next step is it going

00:16:51,779 --> 00:16:59,279
to bail out or is it trying to do some

00:16:54,239 --> 00:17:01,499
some more work it's not going to bail

00:16:59,279 --> 00:17:04,559
out there is something else called the

00:17:01,499 --> 00:17:07,589
implicit scope and we're going to see

00:17:04,559 --> 00:17:09,779
what it is so this is what I told you at

00:17:07,589 --> 00:17:11,519
the beginning it's going to look at the

00:17:09,779 --> 00:17:13,980
at the current scope so everything that

00:17:11,519 --> 00:17:16,260
is available without a prefix if you

00:17:13,980 --> 00:17:18,689
can't find an implicit that works for

00:17:16,260 --> 00:17:21,209
that expression then it will look at the

00:17:18,689 --> 00:17:23,699
implicit scope so what is the implicit

00:17:21,209 --> 00:17:26,549
scope well in this case we had the show

00:17:23,699 --> 00:17:30,059
method that takes a map right a map of

00:17:26,549 --> 00:17:33,269
ID and employees so the implicit scope

00:17:30,059 --> 00:17:37,289
is basically defined by all companion

00:17:33,269 --> 00:17:39,419
object of the target or the color type

00:17:37,289 --> 00:17:41,820
so in this case we have employee which

00:17:39,419 --> 00:17:44,070
is the starting type and we have a map

00:17:41,820 --> 00:17:46,260
as the target type because this show

00:17:44,070 --> 00:17:48,000
method takes a map and what the compiler

00:17:46,260 --> 00:17:49,559
will do is looking at the companion

00:17:48,000 --> 00:17:50,820
object of those types

00:17:49,559 --> 00:17:53,940
to see if there is an implicit

00:17:50,820 --> 00:17:56,490
conversion that that the comparator used

00:17:53,940 --> 00:17:59,249
to transform an employee into a mattify

00:17:56,490 --> 00:18:03,350
the employees if we can find it it will

00:17:59,249 --> 00:18:06,600
apply otherwise um it will just reject

00:18:03,350 --> 00:18:09,690
the statement and it will issue a type

00:18:06,600 --> 00:18:11,519
checking error so in what companion

00:18:09,690 --> 00:18:13,289
object is it going to look well it's

00:18:11,519 --> 00:18:15,539
going to start with map looking at the

00:18:13,289 --> 00:18:17,039
companion object of map not finding

00:18:15,539 --> 00:18:18,779
anything because of course employee is

00:18:17,039 --> 00:18:20,309
our own class so I'm not going to be

00:18:18,779 --> 00:18:23,820
finding something in a library that is

00:18:20,309 --> 00:18:26,249
provided by the Scala library so it's

00:18:23,820 --> 00:18:28,919
going to look into ID in the companion

00:18:26,249 --> 00:18:32,159
object of ID and in the companion object

00:18:28,919 --> 00:18:34,769
of employee but that's not it the

00:18:32,159 --> 00:18:37,139
compiler will also look into map is a

00:18:34,769 --> 00:18:39,299
compound type so it explains other types

00:18:37,139 --> 00:18:43,919
so it will also look into the companion

00:18:39,299 --> 00:18:45,559
object of seek an iterable and so on you

00:18:43,919 --> 00:18:48,299
can see that these are recursive

00:18:45,559 --> 00:18:51,119
algorithm and that should also kind of

00:18:48,299 --> 00:18:52,830
give you in that yeah it takes time just

00:18:51,119 --> 00:18:54,869
because there are many potential types

00:18:52,830 --> 00:18:56,309
and companion object and potentially

00:18:54,869 --> 00:18:58,080
there are many implicit that you will

00:18:56,309 --> 00:19:02,369
have to look at to see if there is one

00:18:58,080 --> 00:19:04,710
that matches good so the takeaway is

00:19:02,369 --> 00:19:10,230
that yes implicit resolution can be

00:19:04,710 --> 00:19:12,090
costly and the tip the one tip that we

00:19:10,230 --> 00:19:14,850
can give you is that try to limit the

00:19:12,090 --> 00:19:17,039
implicit that you have in the scope so

00:19:14,850 --> 00:19:21,059
the one that are available without any

00:19:17,039 --> 00:19:23,369
import and push so what is it like

00:19:21,059 --> 00:19:24,929
general you you make an import you

00:19:23,369 --> 00:19:26,700
import an implicit in the current scope

00:19:24,929 --> 00:19:29,190
then that that's an implicit that is

00:19:26,700 --> 00:19:30,749
available in your source file or maybe

00:19:29,190 --> 00:19:32,309
you define an implicit in a package

00:19:30,749 --> 00:19:34,230
object well then the implicit is

00:19:32,309 --> 00:19:35,759
available to all classes in that package

00:19:34,230 --> 00:19:37,559
right so that's something you should

00:19:35,759 --> 00:19:39,899
avoid so where should you put your

00:19:37,559 --> 00:19:41,879
implicit you should try to push them

00:19:39,899 --> 00:19:43,590
into the implicit scope the reason is

00:19:41,879 --> 00:19:45,869
that you want the scalar compiler to

00:19:43,590 --> 00:19:47,639
look through the implicit in the school

00:19:45,869 --> 00:19:49,649
in the current scope really quickly and

00:19:47,639 --> 00:19:52,230
then just look into the companion object

00:19:49,649 --> 00:19:54,179
which is where your implicit should be

00:19:52,230 --> 00:19:57,990
that's where you want the compiler to

00:19:54,179 --> 00:20:00,869
look for those implicit and you know as

00:19:57,990 --> 00:20:02,279
I guess assented to support this point

00:20:00,869 --> 00:20:03,430
if that when we were at the line not

00:20:02,279 --> 00:20:06,040
working on that project I'm

00:20:03,430 --> 00:20:08,410
before it turned out that they had some

00:20:06,040 --> 00:20:11,080
implicit declared in the companion

00:20:08,410 --> 00:20:12,700
object and not not in the companion

00:20:11,080 --> 00:20:16,210
objects are in the package object so

00:20:12,700 --> 00:20:18,400
those were implicit available to every

00:20:16,210 --> 00:20:20,860
class in those package and they were

00:20:18,400 --> 00:20:23,410
very expensive to compute and actually

00:20:20,860 --> 00:20:25,750
the compiler was attempting to well it

00:20:23,410 --> 00:20:28,480
was computing those implicit many many

00:20:25,750 --> 00:20:30,280
many times and by simply moving those

00:20:28,480 --> 00:20:32,800
implicit from the package object to the

00:20:30,280 --> 00:20:35,890
companion object we brought down compile

00:20:32,800 --> 00:20:37,510
time by 40% with that single change it

00:20:35,890 --> 00:20:41,670
was like literally changing three lines

00:20:37,510 --> 00:20:44,050
of code and and that was a huge boost

00:20:41,670 --> 00:20:45,460
dinner if you want to learn more about

00:20:44,050 --> 00:20:47,260
the word the kind of work that we have

00:20:45,460 --> 00:20:48,580
done as al and I put a link you know

00:20:47,260 --> 00:20:50,170
feel free to explore and then if you

00:20:48,580 --> 00:20:54,060
have questions just tweet us and you

00:20:50,170 --> 00:20:56,770
know we'll be able to tell you more so

00:20:54,060 --> 00:21:00,370
what was happening at Zalando was an

00:20:56,770 --> 00:21:03,670
interplay between opposites and macros

00:21:00,370 --> 00:21:05,140
so macros are great right we love micros

00:21:03,670 --> 00:21:07,780
because they let us reduce the

00:21:05,140 --> 00:21:10,450
boilerplate we don't have to type things

00:21:07,780 --> 00:21:12,370
the micro will do it for us problem is

00:21:10,450 --> 00:21:14,650
that the micro will do it for us every

00:21:12,370 --> 00:21:16,390
time we compile right so if it takes

00:21:14,650 --> 00:21:21,220
time that means that we're going to wait

00:21:16,390 --> 00:21:23,740
every time we're compiling and well you

00:21:21,220 --> 00:21:25,600
know macros can be responsible for 80 90

00:21:23,740 --> 00:21:27,760
percent of compiling time of course that

00:21:25,600 --> 00:21:30,100
depends on what the macros does the

00:21:27,760 --> 00:21:31,480
micro a macro can do anything it can

00:21:30,100 --> 00:21:33,730
trigger type checking it can trigger

00:21:31,480 --> 00:21:37,030
type checking multiple times so just

00:21:33,730 --> 00:21:38,590
imagine you know you're you may

00:21:37,030 --> 00:21:41,590
potentially be spending a lot of time

00:21:38,590 --> 00:21:45,790
just during micro Spanish and type

00:21:41,590 --> 00:21:48,250
checking that much so the takeaway for

00:21:45,790 --> 00:21:50,980
me would be ah not that you shouldn't

00:21:48,250 --> 00:21:53,560
use macros but you should be pragmatic

00:21:50,980 --> 00:21:56,050
about how you use them be aware of the

00:21:53,560 --> 00:21:58,000
cost and you know sometimes you're may

00:21:56,050 --> 00:22:01,480
be better off just maintaining some

00:21:58,000 --> 00:22:06,820
boilerplate instead of you know using

00:22:01,480 --> 00:22:09,220
macros right so it's simple right

00:22:06,820 --> 00:22:11,950
now you know everything about how to

00:22:09,220 --> 00:22:13,990
pinpoint in efficiencies it's a really

00:22:11,950 --> 00:22:15,550
simple job you'll you have these few

00:22:13,990 --> 00:22:16,560
flags easy peasy

00:22:15,550 --> 00:22:19,170
right

00:22:16,560 --> 00:22:22,080
of course it's not you know you saw it

00:22:19,170 --> 00:22:23,790
it takes time probably takes one or two

00:22:22,080 --> 00:22:26,520
days you need to be aware of what the

00:22:23,790 --> 00:22:28,200
compiler does and so now I'm gonna pass

00:22:26,520 --> 00:22:30,150
it to you them and it will tell you oh

00:22:28,200 --> 00:22:31,800
you're not going to have to do all these

00:22:30,150 --> 00:22:33,680
things because either I can do it for

00:22:31,800 --> 00:22:35,790
you Thank You Mirko

00:22:33,680 --> 00:22:38,270
alright so now for something completely

00:22:35,790 --> 00:22:38,270
different

00:22:39,140 --> 00:22:44,520
probably by now we've heard about Hydra

00:22:41,640 --> 00:22:48,090
so this is our project on our vision to

00:22:44,520 --> 00:22:50,670
speed up Scala compilation and Hydra is

00:22:48,090 --> 00:22:53,640
really I would say a fairly natural idea

00:22:50,670 --> 00:22:56,100
that's how we started anyway to try to

00:22:53,640 --> 00:22:58,730
distribute building Scala files or at

00:22:56,100 --> 00:23:01,260
least at the first step to paralyze

00:22:58,730 --> 00:23:03,510
compilation of scholar sources so

00:23:01,260 --> 00:23:05,340
obviously if we take today's

00:23:03,510 --> 00:23:08,190
presentation as an example we're going

00:23:05,340 --> 00:23:09,990
to probably run out of time and that's

00:23:08,190 --> 00:23:11,700
because I had to wait for Mirko to

00:23:09,990 --> 00:23:14,040
finish talking before I could start

00:23:11,700 --> 00:23:16,140
talking as well even though the two

00:23:14,040 --> 00:23:18,990
parts of the presentation are fairly

00:23:16,140 --> 00:23:20,670
different and distinct so wouldn't it be

00:23:18,990 --> 00:23:22,620
much better we could cut the whole

00:23:20,670 --> 00:23:24,450
presentation time in half if both of us

00:23:22,620 --> 00:23:28,920
started talking right after the title

00:23:24,450 --> 00:23:32,010
slide so so taking this natural idea we

00:23:28,920 --> 00:23:34,500
thought well there's two other things

00:23:32,010 --> 00:23:37,170
that I would like from my tooling one is

00:23:34,500 --> 00:23:39,180
to be transparent in the sense I don't

00:23:37,170 --> 00:23:42,950
want to worry about paralyzing I want to

00:23:39,180 --> 00:23:47,130
just be able to turn on a button or a

00:23:42,950 --> 00:23:49,770
slipper a little a little knob and have

00:23:47,130 --> 00:23:51,570
habit working and the same way I can go

00:23:49,770 --> 00:23:53,250
back to my regular single-threaded or

00:23:51,570 --> 00:23:56,280
vanilla color compiler if if for

00:23:53,250 --> 00:23:58,200
whatever reason I have problems and the

00:23:56,280 --> 00:24:00,060
third one is what miracle was just

00:23:58,200 --> 00:24:03,540
talking about is insights I would like

00:24:00,060 --> 00:24:07,140
to know when computation is slow why why

00:24:03,540 --> 00:24:08,850
it is slow and maybe what parts of my

00:24:07,140 --> 00:24:11,190
projects are slow because often they're

00:24:08,850 --> 00:24:14,880
not not all files take the same amount

00:24:11,190 --> 00:24:18,230
of time to compile so while this sounds

00:24:14,880 --> 00:24:20,880
easy we first sat down and thought a bit

00:24:18,230 --> 00:24:22,980
about how how should this work so we can

00:24:20,880 --> 00:24:25,740
distribute files to worker in the clouds

00:24:22,980 --> 00:24:28,620
let's say or several machines and just

00:24:25,740 --> 00:24:30,000
gather the results and this has two hard

00:24:28,620 --> 00:24:30,300
problems they would that we would need

00:24:30,000 --> 00:24:32,370
to

00:24:30,300 --> 00:24:34,830
so the first one is inter file

00:24:32,370 --> 00:24:36,330
dependencies you may send files to

00:24:34,830 --> 00:24:37,080
different workers but they might still

00:24:36,330 --> 00:24:40,100
need each other

00:24:37,080 --> 00:24:42,600
maybe they call in to one another or

00:24:40,100 --> 00:24:45,390
they extend each other there's a file at

00:24:42,600 --> 00:24:47,220
a class and a subclass and so on and the

00:24:45,390 --> 00:24:49,320
second one a distributed system is a

00:24:47,220 --> 00:24:51,510
complex beast so once you go distributed

00:24:49,320 --> 00:24:55,980
you have to worry about a number of

00:24:51,510 --> 00:24:57,960
issues so before we set on or embark on

00:24:55,980 --> 00:25:00,510
such a difficult journey let's see if

00:24:57,960 --> 00:25:03,540
it's worth it so so we did a little bit

00:25:00,510 --> 00:25:06,090
of math so Amdahl's law is a way to

00:25:03,540 --> 00:25:09,120
predict the speed-up you can get from

00:25:06,090 --> 00:25:13,170
paralyzing parts of an algorithm or of a

00:25:09,120 --> 00:25:14,970
program so obviously it's fairly

00:25:13,170 --> 00:25:16,050
impossible to parallelize everything

00:25:14,970 --> 00:25:18,870
unless you have you know an

00:25:16,050 --> 00:25:21,630
embarrassingly parallel problem so we

00:25:18,870 --> 00:25:24,270
have two constants in there P is the

00:25:21,630 --> 00:25:28,380
chunk or percentage of the work that

00:25:24,270 --> 00:25:30,270
could be paralyzed and s is the speed-up

00:25:28,380 --> 00:25:32,460
we can get for the paralyzed or

00:25:30,270 --> 00:25:34,020
parallelizable work so that is roughly

00:25:32,460 --> 00:25:37,380
the number of course we can throw out

00:25:34,020 --> 00:25:40,770
the problem so the speed-up would then

00:25:37,380 --> 00:25:44,250
be bounded by the constant work that we

00:25:40,770 --> 00:25:45,840
cannot paralyze 1 minus P so in the

00:25:44,250 --> 00:25:49,080
ideal case the maximum theoretical

00:25:45,840 --> 00:25:51,450
speed-up really depends on how much we

00:25:49,080 --> 00:25:53,130
need to sort of do for in every worker

00:25:51,450 --> 00:25:57,600
so these are some some values we put

00:25:53,130 --> 00:26:00,120
there if the 50% of the work cannot be

00:25:57,600 --> 00:26:02,640
parallelized and the maximum speed-up I

00:26:00,120 --> 00:26:07,170
can get is 2 times basically the rest

00:26:02,640 --> 00:26:09,560
after the 50% mark takes 0 seconds and

00:26:07,170 --> 00:26:13,140
then I made my computation twice as fast

00:26:09,560 --> 00:26:15,840
and so on so for 10% obviously that that

00:26:13,140 --> 00:26:18,180
means 10 X speed up now the problem we

00:26:15,840 --> 00:26:19,770
had with this is that we kept looking

00:26:18,180 --> 00:26:22,020
and looking but could but couldn't find

00:26:19,770 --> 00:26:26,190
any hardware with an infinite number of

00:26:22,020 --> 00:26:28,560
course we're usually stuck with 4 or 8

00:26:26,190 --> 00:26:30,240
on a laptop so some more let's say

00:26:28,560 --> 00:26:32,600
realistic numbers would be something

00:26:30,240 --> 00:26:37,080
like this what we can typically expect

00:26:32,600 --> 00:26:40,970
so we can we can let's say expect a 2.5

00:26:37,080 --> 00:26:43,590
speed-up if our if we use for course and

00:26:40,970 --> 00:26:44,220
the constant work the part that cannot

00:26:43,590 --> 00:26:50,520
be paralyzed

00:26:44,220 --> 00:26:53,159
is 20% so from our measurements so far

00:26:50,520 --> 00:26:55,860
that's also the area where we stand

00:26:53,159 --> 00:26:57,390
because you can use the law in the other

00:26:55,860 --> 00:26:59,280
direction too if I measure the

00:26:57,390 --> 00:27:02,250
compilation time and I know how many

00:26:59,280 --> 00:27:04,799
course I used I can gather I can extract

00:27:02,250 --> 00:27:08,130
the constant and this constant number is

00:27:04,799 --> 00:27:10,320
actually quite nice because it it works

00:27:08,130 --> 00:27:13,289
for it's a specific number to a project

00:27:10,320 --> 00:27:15,240
it's project specific usually actually

00:27:13,289 --> 00:27:18,120
it's even it's different between main

00:27:15,240 --> 00:27:20,370
sources and tests much lower for test

00:27:18,120 --> 00:27:24,330
and we can use it also to predict

00:27:20,370 --> 00:27:26,220
performance to a certain extent so maybe

00:27:24,330 --> 00:27:28,230
one other interesting thing to notice is

00:27:26,220 --> 00:27:31,409
that even if I double the number of

00:27:28,230 --> 00:27:35,730
course I will not really double my

00:27:31,409 --> 00:27:38,190
speed-up so in case of let's say the

00:27:35,730 --> 00:27:40,200
happy case of only 10% constant work I

00:27:38,190 --> 00:27:42,659
can go from three times faster to four

00:27:40,200 --> 00:27:48,360
point seven times faster by doubling the

00:27:42,659 --> 00:27:50,010
number of course so let's see now we

00:27:48,360 --> 00:27:53,070
said well this is worth it obviously

00:27:50,010 --> 00:27:56,400
since we're here to present it let's see

00:27:53,070 --> 00:28:00,510
how do we do this so we started with one

00:27:56,400 --> 00:28:03,360
machine so Hydra today is it's just

00:28:00,510 --> 00:28:05,039
parallel it's not distributed yet but we

00:28:03,360 --> 00:28:07,860
can still use all the course we have on

00:28:05,039 --> 00:28:10,470
one machine and the granularity at which

00:28:07,860 --> 00:28:14,940
we work so the level of parallelism is a

00:28:10,470 --> 00:28:16,740
single file source files and the way we

00:28:14,940 --> 00:28:18,390
paralyze it is to simply have several

00:28:16,740 --> 00:28:20,520
scholar workers that take the source

00:28:18,390 --> 00:28:22,799
file from source and go all the way to

00:28:20,520 --> 00:28:25,370
bytecode that means that the whole

00:28:22,799 --> 00:28:27,559
pipeline runs in parallel in each worker

00:28:25,370 --> 00:28:30,840
including typer

00:28:27,559 --> 00:28:32,669
and the hard problems while we have to

00:28:30,840 --> 00:28:35,490
let's say if we go a little bit deeper

00:28:32,669 --> 00:28:37,230
than in the first assessment Inter file

00:28:35,490 --> 00:28:38,730
dependencies between workers which we

00:28:37,230 --> 00:28:42,059
still have that problem because we have

00:28:38,730 --> 00:28:45,690
independent workers and there that our

00:28:42,059 --> 00:28:49,049
solution was to to basically duplicate

00:28:45,690 --> 00:28:50,669
some work but make sure that we share as

00:28:49,049 --> 00:28:52,860
much as we can between workers in order

00:28:50,669 --> 00:28:55,380
to keep getting the speed-up so that's a

00:28:52,860 --> 00:28:56,870
little bit vague but I can't get into

00:28:55,380 --> 00:28:59,880
too many details on

00:28:56,870 --> 00:29:03,240
and the second problem is that the Scala

00:28:59,880 --> 00:29:04,980
compiler is very performance aware so so

00:29:03,240 --> 00:29:06,630
there's been a lot of work going in

00:29:04,980 --> 00:29:08,670
Scala C to make it as fast as it can

00:29:06,630 --> 00:29:11,220
possibly be that means that there is

00:29:08,670 --> 00:29:13,530
laziness and there's mutable State for

00:29:11,220 --> 00:29:15,330
good reasons I would say so that also

00:29:13,530 --> 00:29:18,810
makes it harder to parallelize so we

00:29:15,330 --> 00:29:21,480
can't really use shared state safely and

00:29:18,810 --> 00:29:22,950
we don't do that at least we started

00:29:21,480 --> 00:29:25,290
with the share nothing architecture and

00:29:22,950 --> 00:29:27,480
then have very controlled sharing

00:29:25,290 --> 00:29:31,470
between workers in order to still get a

00:29:27,480 --> 00:29:33,930
good speed ups actually just to as a

00:29:31,470 --> 00:29:36,420
parenthesis on the laziness immutable

00:29:33,930 --> 00:29:39,060
state I hear very good news about Scala

00:29:36,420 --> 00:29:40,980
2:12 3 and there's going to be pretty

00:29:39,060 --> 00:29:43,800
pretty nice speed ups in the

00:29:40,980 --> 00:29:45,780
single-threaded case as well which

00:29:43,800 --> 00:29:47,520
obviously is good news for for Hydra as

00:29:45,780 --> 00:29:50,550
well because if we can get down that

00:29:47,520 --> 00:29:52,110
constant factor we can also we can also

00:29:50,550 --> 00:29:56,430
paralyze better so we can get better

00:29:52,110 --> 00:29:58,650
results so how well does it work these

00:29:56,430 --> 00:30:02,190
are a couple of open source projects

00:29:58,650 --> 00:30:06,510
that we measured so specs to current

00:30:02,190 --> 00:30:09,120
test I think we get between 2.2 and 2.5

00:30:06,510 --> 00:30:10,890
speed-up so that's if we go back to the

00:30:09,120 --> 00:30:14,790
table with under slow we're probably

00:30:10,890 --> 00:30:17,630
somewhere between 10 and 20 percent cost

00:30:14,790 --> 00:30:22,020
constant work maybe even less shapeless

00:30:17,630 --> 00:30:23,580
we have the JavaScript core and the JVM

00:30:22,020 --> 00:30:28,970
core so the speed up there are a little

00:30:23,580 --> 00:30:33,240
bit less less good and for specs - and

00:30:28,970 --> 00:30:35,400
what okay yeah it's not on this chart

00:30:33,240 --> 00:30:36,840
but our best speed-up and I didn't put

00:30:35,400 --> 00:30:40,560
it there because it's maybe a little bit

00:30:36,840 --> 00:30:43,410
a typical is for for Scala debugger

00:30:40,560 --> 00:30:45,150
tests that were six times faster so in

00:30:43,410 --> 00:30:47,370
that case the constant speed up is so

00:30:45,150 --> 00:30:49,290
sorry the constant factor was 6% so so

00:30:47,370 --> 00:30:53,850
it tests usually behave a little bit

00:30:49,290 --> 00:30:57,150
better because they they have less inter

00:30:53,850 --> 00:30:59,100
dependencies obviously okay so we talked

00:30:57,150 --> 00:31:00,690
about shadeless but the shapeless itself

00:30:59,100 --> 00:31:02,220
is a fairly small codebase if you look

00:31:00,690 --> 00:31:05,220
at the actual number on the bar chart

00:31:02,220 --> 00:31:08,610
you see that compilation time takes what

00:31:05,220 --> 00:31:09,970
15 - less than 20 seconds or for for the

00:31:08,610 --> 00:31:13,000
JVM one

00:31:09,970 --> 00:31:14,920
than 15 seconds so shapeless itself is

00:31:13,000 --> 00:31:16,930
fast to compile but what everyone is

00:31:14,920 --> 00:31:18,820
suffering from is actually using

00:31:16,930 --> 00:31:20,590
shapeless which means shapeless tests

00:31:18,820 --> 00:31:24,130
would be a more interesting project to

00:31:20,590 --> 00:31:27,100
look at so these are the numbers this is

00:31:24,130 --> 00:31:31,240
all based on 212 1 I would say 212 2 is

00:31:27,100 --> 00:31:34,090
probably the same so on this case we get

00:31:31,240 --> 00:31:38,140
a three time speed-up what I was saying

00:31:34,090 --> 00:31:41,770
test usually have a much better constant

00:31:38,140 --> 00:31:44,050
factor in there obviously what I didn't

00:31:41,770 --> 00:31:46,450
really say is that the more cores you

00:31:44,050 --> 00:31:48,720
throw the problem it doesn't necessarily

00:31:46,450 --> 00:31:54,130
mean that you will continue improving

00:31:48,720 --> 00:31:58,920
the speed up every time because Scala is

00:31:54,130 --> 00:32:01,990
a quite a memory intensive process so

00:31:58,920 --> 00:32:04,270
depending on your hardware you get

00:32:01,990 --> 00:32:06,160
memory bound quickly so the memory bus

00:32:04,270 --> 00:32:07,600
you just get saturated so if you have

00:32:06,160 --> 00:32:09,280
more workers trying to read and write to

00:32:07,600 --> 00:32:13,620
memory you will just make everyone

00:32:09,280 --> 00:32:16,720
slower but or if you have Intel Xeon

00:32:13,620 --> 00:32:19,240
based hardware with very good memory bus

00:32:16,720 --> 00:32:20,440
you can you can go way far away further

00:32:19,240 --> 00:32:22,300
than for course all of these

00:32:20,440 --> 00:32:28,270
measurements are using four cores by the

00:32:22,300 --> 00:32:30,550
way on on a MacBook Pro so i7 also all

00:32:28,270 --> 00:32:34,000
the times we shall we've shown our warm

00:32:30,550 --> 00:32:35,980
compiled times but it's nice to see how

00:32:34,000 --> 00:32:41,110
it behaved on how it would behave on the

00:32:35,980 --> 00:32:44,320
CI where it's all called JDM so warm JVM

00:32:41,110 --> 00:32:45,820
means that you've run your load several

00:32:44,320 --> 00:32:49,180
times so that the java virtual machine

00:32:45,820 --> 00:32:51,460
has finished just-in-time compiling your

00:32:49,180 --> 00:32:54,520
code loading all the class files from

00:32:51,460 --> 00:32:57,910
disk verifying the code and so on so so

00:32:54,520 --> 00:32:59,830
these are the curves we get for for one

00:32:57,910 --> 00:33:01,660
of the one of the projects you can see

00:32:59,830 --> 00:33:03,970
that the speed-up is quite consistent I

00:33:01,660 --> 00:33:06,670
think it gets slightly better on a warm

00:33:03,970 --> 00:33:10,570
JVM but it's still pretty good already

00:33:06,670 --> 00:33:13,390
from from the first run here as a

00:33:10,570 --> 00:33:16,300
parentheses measuring JVM performances

00:33:13,390 --> 00:33:18,220
it's quite a tricky business so I I just

00:33:16,300 --> 00:33:21,070
want to quickly mention that the

00:33:18,220 --> 00:33:22,900
skeleton has a pretty awesome set up now

00:33:21,070 --> 00:33:23,520
for benchmarking Scala and I hope one of

00:33:22,900 --> 00:33:25,290
them will

00:33:23,520 --> 00:33:27,860
at some point give a talk about how to

00:33:25,290 --> 00:33:33,870
properly measure scholarship performance

00:33:27,860 --> 00:33:37,200
because coach good awesome awesome

00:33:33,870 --> 00:33:39,840
thanks so so yeah go go ahead and read

00:33:37,200 --> 00:33:41,970
sets blog post when it comes or Jason's

00:33:39,840 --> 00:33:44,700
well skeletons blog post when it comes

00:33:41,970 --> 00:33:49,620
out and it would help us as well to link

00:33:44,700 --> 00:33:52,920
to it and show it to to to everyone all

00:33:49,620 --> 00:33:54,300
right so how is the Hydra compiler

00:33:52,920 --> 00:33:56,250
working well there's there several

00:33:54,300 --> 00:34:00,480
components in there one of them is a

00:33:56,250 --> 00:34:01,980
Scala C fork we hope that this won't

00:34:00,480 --> 00:34:04,530
stay forever a fork we are pushing

00:34:01,980 --> 00:34:07,860
patches upstream when they make sense

00:34:04,530 --> 00:34:12,750
and I I hope this we will manage to

00:34:07,860 --> 00:34:14,700
unfor kit soon we support 211 and 212 so

00:34:12,750 --> 00:34:16,980
starting with 211 eight so there's

00:34:14,700 --> 00:34:19,619
there's a certain amount of work that's

00:34:16,980 --> 00:34:22,020
sort of in the past but hopefully for

00:34:19,619 --> 00:34:24,990
four to twelve and onwards we can we can

00:34:22,020 --> 00:34:26,879
get rid of the of the fork one thing

00:34:24,990 --> 00:34:29,040
that's maybe good to mention this is a

00:34:26,879 --> 00:34:31,020
drop-in replacement it's not an SBT

00:34:29,040 --> 00:34:33,570
plug-in even though there is an SVT

00:34:31,020 --> 00:34:34,919
plug-in this can work fairly well on the

00:34:33,570 --> 00:34:37,020
command line if you just want to quickly

00:34:34,919 --> 00:34:39,359
test it

00:34:37,020 --> 00:34:41,700
other Forks in particular the type level

00:34:39,359 --> 00:34:44,490
Scala fork should work as well we don't

00:34:41,700 --> 00:34:47,250
support it currently but I don't see any

00:34:44,490 --> 00:34:50,700
reason why if there is demand this would

00:34:47,250 --> 00:34:52,080
be difficult to do and one one question

00:34:50,700 --> 00:34:54,960
we get often is how do we make sure

00:34:52,080 --> 00:34:57,450
you're not breaking our build so so what

00:34:54,960 --> 00:34:59,580
we guarantee is a binary identical

00:34:57,450 --> 00:35:01,500
output you can compare the class files

00:34:59,580 --> 00:35:03,840
that you obtained with Scala c and hydra

00:35:01,500 --> 00:35:07,020
and they should be the same there are

00:35:03,840 --> 00:35:08,970
some sometimes very slight differences

00:35:07,020 --> 00:35:10,830
in the order of methods but absolutely

00:35:08,970 --> 00:35:13,619
everything else is identical and I think

00:35:10,830 --> 00:35:19,920
we can probably get rid of all of those

00:35:13,619 --> 00:35:23,970
as well so the last part of the after

00:35:19,920 --> 00:35:25,080
the three pillars inside this is started

00:35:23,970 --> 00:35:27,930
by saying well if we're going to

00:35:25,080 --> 00:35:29,550
distribute the the build we want to know

00:35:27,930 --> 00:35:33,270
how the workers are doing and sometimes

00:35:29,550 --> 00:35:34,740
workers can get unbalanced and the

00:35:33,270 --> 00:35:36,480
reason why they get unbalanced is that

00:35:34,740 --> 00:35:38,130
all source files are not equal

00:35:36,480 --> 00:35:39,869
some of them can take a very long time

00:35:38,130 --> 00:35:42,359
to compile so you would like to know

00:35:39,869 --> 00:35:44,070
that you know you have a worker that

00:35:42,359 --> 00:35:46,619
takes 200 seconds to compile while

00:35:44,070 --> 00:35:50,490
everything else finished in 30 seconds

00:35:46,619 --> 00:35:52,680
and and we had we had such an issue with

00:35:50,490 --> 00:35:54,480
one of our without one of our clients

00:35:52,680 --> 00:35:57,570
who was saying that I can't get any

00:35:54,480 --> 00:36:00,420
speed up out of Hydra and we we have

00:35:57,570 --> 00:36:03,510
around 190 seconds with Hydra and 200

00:36:00,420 --> 00:36:05,130
something or 2050 with single-threaded

00:36:03,510 --> 00:36:06,960
compiler and then when we sat down and

00:36:05,130 --> 00:36:09,810
looked there was one file that was

00:36:06,960 --> 00:36:12,210
taking 200 seconds to compile or 190

00:36:09,810 --> 00:36:13,920
whatever and that was just one file that

00:36:12,210 --> 00:36:16,380
stayed in one worker and everybody else

00:36:13,920 --> 00:36:18,780
finished you know you know 10 15 20

00:36:16,380 --> 00:36:20,580
seconds so we said ok we have to do

00:36:18,780 --> 00:36:22,920
something about it we have to help the

00:36:20,580 --> 00:36:24,630
users know about this and then naturally

00:36:22,920 --> 00:36:26,490
we moved on to more detailed metrics

00:36:24,630 --> 00:36:30,210
than just you know the title at the time

00:36:26,490 --> 00:36:33,330
so in that particular case we could

00:36:30,210 --> 00:36:35,970
track it down maybe it took us a day to

00:36:33,330 --> 00:36:37,800
find the exact combination of imports

00:36:35,970 --> 00:36:40,830
and macros expansions and so on that

00:36:37,800 --> 00:36:43,710
were taking literally I don't know five

00:36:40,830 --> 00:36:46,230
hundred lines of code file into 40,000

00:36:43,710 --> 00:36:49,770
lines of code so that's that I would say

00:36:46,230 --> 00:36:51,570
quite impressive as a seed so we could

00:36:49,770 --> 00:36:53,910
take it down so what we want to do is to

00:36:51,570 --> 00:36:57,210
get this kind of monitoring or matrix

00:36:53,910 --> 00:36:59,480
and build some sort of - port or UI

00:36:57,210 --> 00:37:01,800
where you can see your builds

00:36:59,480 --> 00:37:03,540
historically as well so you can see well

00:37:01,800 --> 00:37:05,130
last week my build was much faster what

00:37:03,540 --> 00:37:06,990
happened what is the commit that slowed

00:37:05,130 --> 00:37:10,280
it down what is the file that got slower

00:37:06,990 --> 00:37:13,410
or or is there too much GC load maybe

00:37:10,280 --> 00:37:16,800
maybe I just need to give it 8 gigabytes

00:37:13,410 --> 00:37:21,420
of RAM of heap instead of just 6 and

00:37:16,800 --> 00:37:23,820
that's why it's slow so how would you

00:37:21,420 --> 00:37:25,440
well yeah the transplant actually this

00:37:23,820 --> 00:37:28,500
is the last dealer transparency which is

00:37:25,440 --> 00:37:30,750
also the simpler simpler one the way to

00:37:28,500 --> 00:37:33,180
use it if you're using SBT is to simply

00:37:30,750 --> 00:37:35,730
add a compiler plug-in that lets you

00:37:33,180 --> 00:37:37,460
that sort of plugs into the compiled

00:37:35,730 --> 00:37:41,609
tasks and that's all you need to do

00:37:37,460 --> 00:37:44,310
other build tools will come so far we

00:37:41,609 --> 00:37:46,080
didn't do it but I guess if there is if

00:37:44,310 --> 00:37:48,720
there is a lot of demand we can we can

00:37:46,080 --> 00:37:50,160
look into that as well on the command

00:37:48,720 --> 00:37:52,650
line is just another miner

00:37:50,160 --> 00:37:54,390
cpu's flag that you can say all I want

00:37:52,650 --> 00:37:56,539
to use for workers and and that's about

00:37:54,390 --> 00:37:56,539
it

00:37:56,940 --> 00:38:01,740
so so obviously I didn't mention it but

00:37:59,730 --> 00:38:03,900
that Hydra is our business so we're

00:38:01,740 --> 00:38:04,710
trying to build a business on top of

00:38:03,900 --> 00:38:07,140
scala tooling

00:38:04,710 --> 00:38:08,970
how do we doe contribute back to the

00:38:07,140 --> 00:38:11,549
community and this is a thorny issue of

00:38:08,970 --> 00:38:12,960
course because we build on top of what

00:38:11,549 --> 00:38:15,990
the community build and we want to give

00:38:12,960 --> 00:38:18,000
back so we're pushing patches upstream

00:38:15,990 --> 00:38:21,380
we depend on Scala working well and we

00:38:18,000 --> 00:38:24,780
depend on SBT and Zink working well and

00:38:21,380 --> 00:38:27,240
of course once we are where I'll talk

00:38:24,780 --> 00:38:29,460
about the timeline a bit later once we

00:38:27,240 --> 00:38:31,680
reach the general availability we will

00:38:29,460 --> 00:38:34,410
provide open source projects with with

00:38:31,680 --> 00:38:37,890
licenses as well so it will be free to

00:38:34,410 --> 00:38:40,020
use and the last point which I want to

00:38:37,890 --> 00:38:43,470
mention as well is that we will

00:38:40,020 --> 00:38:45,299
implement a sip sip 25 which is about

00:38:43,470 --> 00:38:47,819
adding static fields and methods to

00:38:45,299 --> 00:38:50,670
discolor objects hopefully this will

00:38:47,819 --> 00:38:52,799
make it into Scala 212 3 so it's really

00:38:50,670 --> 00:38:55,289
so now we started working on it the PR

00:38:52,799 --> 00:38:59,910
is not out yet but hopefully it will be

00:38:55,289 --> 00:39:02,849
out soon so the timeline at this point

00:38:59,910 --> 00:39:04,829
and I think until the end of q3 this

00:39:02,849 --> 00:39:08,390
year will start we started a private

00:39:04,829 --> 00:39:11,369
rollout that means we are working with

00:39:08,390 --> 00:39:14,099
with clients who have a relatively large

00:39:11,369 --> 00:39:15,990
build so it takes takes many minutes to

00:39:14,099 --> 00:39:18,059
build and we're working closely with

00:39:15,990 --> 00:39:19,770
them to find all the issues if there are

00:39:18,059 --> 00:39:21,900
issues and help them get the best bidder

00:39:19,770 --> 00:39:23,460
possible and at the same time we're

00:39:21,900 --> 00:39:25,230
working on the insights and metrics that

00:39:23,460 --> 00:39:27,270
I mentioned and trying to put that also

00:39:25,230 --> 00:39:29,970
into into Hydra so you can you can

00:39:27,270 --> 00:39:33,270
basically debug sort of performance

00:39:29,970 --> 00:39:36,510
problems in in your build so towards the

00:39:33,270 --> 00:39:39,750
end of or yeah in q4 we want to actually

00:39:36,510 --> 00:39:41,369
go out and let everyone use it you can

00:39:39,750 --> 00:39:43,890
download and try it out on your build

00:39:41,369 --> 00:39:46,529
and at the same time also starts working

00:39:43,890 --> 00:39:48,480
on the distributed part so when the

00:39:46,529 --> 00:39:52,200
build is too large or too big for one

00:39:48,480 --> 00:39:54,809
machine to be able to use more machines

00:39:52,200 --> 00:39:58,130
maybe you have an Amazon account and you

00:39:54,809 --> 00:40:00,900
want to use AWS and and so on and

00:39:58,130 --> 00:40:03,390
obviously if we have that we'll also

00:40:00,900 --> 00:40:04,650
have a cloud service where you can

00:40:03,390 --> 00:40:06,840
either

00:40:04,650 --> 00:40:10,080
send you're built to to the triple code

00:40:06,840 --> 00:40:15,090
cloud or deploy it on premise and use it

00:40:10,080 --> 00:40:17,880
there I think that that's all we have

00:40:15,090 --> 00:40:19,130
thank you very much if you want to to

00:40:17,880 --> 00:40:21,210
have a look at what we've done

00:40:19,130 --> 00:40:23,700
previously there's a link to the blog

00:40:21,210 --> 00:40:25,440
and if your company is interested in the

00:40:23,700 --> 00:40:28,350
private rollout please come talk to us

00:40:25,440 --> 00:40:32,610
after and we'll be very happy to talk to

00:40:28,350 --> 00:40:41,790
you and try to help Thanks

00:40:32,610 --> 00:40:44,010
is there any questions system I'm

00:40:41,790 --> 00:40:47,430
wondering if you could show us a

00:40:44,010 --> 00:40:49,820
dashboard about that inside or matrix

00:40:47,430 --> 00:40:53,970
how do they look like in a real project

00:40:49,820 --> 00:40:56,190
sure well it's not done yet we have some

00:40:53,970 --> 00:40:57,690
sketches but we didn't put them in the

00:40:56,190 --> 00:41:02,520
in the slides because it's really just

00:40:57,690 --> 00:41:04,260
raw sketches I would I would not venture

00:41:02,520 --> 00:41:05,880
into it we can talk afterwards maybe you

00:41:04,260 --> 00:41:07,980
have some good ideas they will be very

00:41:05,880 --> 00:41:09,510
very happy to hear them so I can I can

00:41:07,980 --> 00:41:11,130
say that what makes it a little bit

00:41:09,510 --> 00:41:13,140
difficult is that it's not just

00:41:11,130 --> 00:41:17,160
timelines we don't want to use something

00:41:13,140 --> 00:41:18,810
like in for for showing spans we have we

00:41:17,160 --> 00:41:21,000
want to add alerts as well basically

00:41:18,810 --> 00:41:23,520
flag potential positions in your source

00:41:21,000 --> 00:41:26,310
that are that are issues that we want to

00:41:23,520 --> 00:41:28,230
look and then dig into that to find more

00:41:26,310 --> 00:41:30,330
info of that at that particular point in

00:41:28,230 --> 00:41:34,650
code so it's going to be a challenge to

00:41:30,330 --> 00:41:37,710
make this easy to use hello

00:41:34,650 --> 00:41:40,050
okay so you mentioned that later on once

00:41:37,710 --> 00:41:41,930
you reach general availability the

00:41:40,050 --> 00:41:45,060
project will be available to open source

00:41:41,930 --> 00:41:47,010
projects yes what would be the status of

00:41:45,060 --> 00:41:51,300
the monitoring part it is going to be

00:41:47,010 --> 00:41:54,030
part of Hydra or it's a separate thing

00:41:51,300 --> 00:41:57,990
I don't yeah it's a good question I

00:41:54,030 --> 00:41:59,790
think that our vision was to have a CI

00:41:57,990 --> 00:42:02,550
version because it makes much more sense

00:41:59,790 --> 00:42:04,860
to have the historic view of your bills

00:42:02,550 --> 00:42:06,540
if you can tie them to a commit because

00:42:04,860 --> 00:42:07,830
that's that's what sort of makes it easy

00:42:06,540 --> 00:42:10,260
to distinguish if you build several

00:42:07,830 --> 00:42:14,730
times on your machine is not going to be

00:42:10,260 --> 00:42:16,650
extremely interesting so so maybe it's

00:42:14,730 --> 00:42:20,190
useful as well for

00:42:16,650 --> 00:42:22,500
for the for the individual version I'm

00:42:20,190 --> 00:42:24,450
I'm not yet sure maybe there maybe there

00:42:22,500 --> 00:42:26,579
would be part of maybe something came

00:42:24,450 --> 00:42:27,990
down because because the idea of the the

00:42:26,579 --> 00:42:29,880
dashboard and UI is to actually have a

00:42:27,990 --> 00:42:33,329
web-based UI where the whole team can

00:42:29,880 --> 00:42:35,569
look at data so I'm not sure how well

00:42:33,329 --> 00:42:45,960
that should work for an individual

00:42:35,569 --> 00:42:47,490
computer user alright well if there are

00:42:45,960 --> 00:42:49,440
no more questions thank you all again

00:42:47,490 --> 00:42:51,490
and you know grab us after the talk if

00:42:49,440 --> 00:42:59,010
you pick up Soviet question

00:42:51,490 --> 00:42:59,010

YouTube URL: https://www.youtube.com/watch?v=QKvzyHroKLA


