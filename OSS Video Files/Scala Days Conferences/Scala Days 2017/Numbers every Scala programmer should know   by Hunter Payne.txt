Title: Numbers every Scala programmer should know   by Hunter Payne
Publication date: 2017-06-06
Playlist: Scala Days 2017
Description: 
	This video was recorded at Scala Days Chicago 2017
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

Abstract: 
Writing highly performant code has always been a challenge for programmers. Techniques change and evolve in response to shifts in underlying hardware. For example, before the early '90s it was faster to XOR a register with itself than to set a register to 0. This is no longer the case. Instead, creating fast code is mostly about improving L2 and L3 cache hit rates, aligning data in memory and allowing hardware attached to the I/O bus to move data to main memory concurrently with the CPU. Now, with the advent of Scala, use of immutable data has been touted as the best practice for how to write concurrent code. In this talk, we'll discuss why using immutability may not always be the right choice. We'll also cover when to follow easier models reliant on immutability, the best times to brave the difficulties of synchronization and locks, and when blocking is the right thing to do, even inside of an actor.
Captions: 
	00:00:03,020 --> 00:00:09,450
as I said my name is Claire pain I'm a

00:00:05,399 --> 00:00:12,059
staff engineer at Credit Karma my

00:00:09,450 --> 00:00:13,740
previous life my previous job was design

00:00:12,059 --> 00:00:16,460
high-performance middleware that did

00:00:13,740 --> 00:00:19,050
millions of events per second per core I

00:00:16,460 --> 00:00:21,660
spent about side microdata doing

00:00:19,050 --> 00:00:23,760
high-performance software and this is

00:00:21,660 --> 00:00:26,220
sort of my ideas that I've sort of

00:00:23,760 --> 00:00:28,740
distilled over 20 years of writing

00:00:26,220 --> 00:00:29,970
software so what are we going to talk

00:00:28,740 --> 00:00:31,679
about well we're just going to talk

00:00:29,970 --> 00:00:33,330
about why do we do latency why don't we

00:00:31,679 --> 00:00:36,780
just you know write something you know

00:00:33,330 --> 00:00:40,500
wrote it works and then Carla de Ojo

00:00:36,780 --> 00:00:43,290
I certainly for that then we will talk

00:00:40,500 --> 00:00:44,730
about a great slide given at a google

00:00:43,290 --> 00:00:47,400
talk about seven years ago called

00:00:44,730 --> 00:00:48,600
numbers every prisoner should know we're

00:00:47,400 --> 00:00:50,550
gonna talk about how to analyze those

00:00:48,600 --> 00:00:53,070
numbers how to utilize those numbers how

00:00:50,550 --> 00:00:59,520
to figure out how to apply those numbers

00:00:53,070 --> 00:01:03,870
to to optimizing our software welcome

00:00:59,520 --> 00:01:05,880
thank you and then we're going to look

00:01:03,870 --> 00:01:08,730
at how to benchmark our own applications

00:01:05,880 --> 00:01:11,540
our own hardware to know how to improve

00:01:08,730 --> 00:01:15,000
the performance of our own applications

00:01:11,540 --> 00:01:16,950
so a relevant one of latency matter well

00:01:15,000 --> 00:01:19,680
why do we do all the extra work for

00:01:16,950 --> 00:01:23,820
example Google those fronts of their

00:01:19,680 --> 00:01:25,110
micro service architecture is actually

00:01:23,820 --> 00:01:27,540
custom hardware they have made

00:01:25,110 --> 00:01:29,730
themselves they actually remade the

00:01:27,540 --> 00:01:31,110
hardware of the computer to better suit

00:01:29,730 --> 00:01:32,700
what they were trying to do because they

00:01:31,110 --> 00:01:34,770
thought latency was that important

00:01:32,700 --> 00:01:37,650
people write programming languages

00:01:34,770 --> 00:01:39,360
designed around latency they reorganized

00:01:37,650 --> 00:01:40,740
the entire company with engineering

00:01:39,360 --> 00:01:42,540
departments around trying to squeeze

00:01:40,740 --> 00:01:44,640
latency why do we do this

00:01:42,540 --> 00:01:47,610
well I'll sensibly we do it because it

00:01:44,640 --> 00:01:49,680
saved hardware which is nice it makes

00:01:47,610 --> 00:01:52,110
our friends in systems operations lives

00:01:49,680 --> 00:01:53,520
a little bit easier which is which is a

00:01:52,110 --> 00:01:55,140
good thing and in a perfect world that

00:01:53,520 --> 00:01:57,479
would be enough but the truth is neither

00:01:55,140 --> 00:01:59,340
those three real reasons the real reason

00:01:57,479 --> 00:02:00,900
is because users hate to wait and if you

00:01:59,340 --> 00:02:02,520
make them wait they will go somewhere

00:02:00,900 --> 00:02:06,180
else and they will take their money with

00:02:02,520 --> 00:02:07,890
you so it's important to note that

00:02:06,180 --> 00:02:11,120
there's both human scale latency and

00:02:07,890 --> 00:02:14,130
machine way of machine level latency

00:02:11,120 --> 00:02:15,030
human scale latency is the amount of

00:02:14,130 --> 00:02:16,920
time humans

00:02:15,030 --> 00:02:19,800
can perceive this is where we're going

00:02:16,920 --> 00:02:21,000
to optimize our applications to machine

00:02:19,800 --> 00:02:22,110
scale latency we're not really going to

00:02:21,000 --> 00:02:24,270
talk about today that would be in

00:02:22,110 --> 00:02:25,470
high-frequency trading or things where

00:02:24,270 --> 00:02:29,940
you are racing against another

00:02:25,470 --> 00:02:31,319
application somehow so what is fast how

00:02:29,940 --> 00:02:33,510
to users perceive fast

00:02:31,319 --> 00:02:36,720
well fast is that's about a hundred

00:02:33,510 --> 00:02:38,010
milliseconds that's fast if you can get

00:02:36,720 --> 00:02:40,050
your latency into the hundreds of

00:02:38,010 --> 00:02:41,970
milliseconds your users will receive

00:02:40,050 --> 00:02:43,860
your applications being instantaneous

00:02:41,970 --> 00:02:45,780
something where they can try something

00:02:43,860 --> 00:02:47,580
and experiment with your website

00:02:45,780 --> 00:02:50,310
interact with your website and it won't

00:02:47,580 --> 00:02:52,560
really cost them too much and they'll

00:02:50,310 --> 00:02:54,300
pick you liked it the best they will

00:02:52,560 --> 00:02:55,950
rate your application in the most

00:02:54,300 --> 00:02:58,470
favorable ways and they will have the

00:02:55,950 --> 00:03:01,080
best response to your application at

00:02:58,470 --> 00:03:02,880
about a second or seconds of latency the

00:03:01,080 --> 00:03:05,519
the application will feel stutter a

00:03:02,880 --> 00:03:08,550
little bit it will feel not exactly

00:03:05,519 --> 00:03:11,430
fluid users will start to get a little

00:03:08,550 --> 00:03:12,780
frustrated they'll know that okay if I

00:03:11,430 --> 00:03:13,980
click on this button I got to wait for a

00:03:12,780 --> 00:03:15,600
second or click on this button for

00:03:13,980 --> 00:03:17,580
weights dragons so I'm going to think

00:03:15,600 --> 00:03:18,930
about which button I should click on and

00:03:17,580 --> 00:03:20,970
I'm going to put more thought into it

00:03:18,930 --> 00:03:22,650
and that's going to sort of cut down on

00:03:20,970 --> 00:03:24,209
the interactivity and if then if your

00:03:22,650 --> 00:03:26,130
latency goes into the tens of seconds

00:03:24,209 --> 00:03:27,540
you've lost them they're going somewhere

00:03:26,130 --> 00:03:28,920
else and they're going to take their

00:03:27,540 --> 00:03:31,200
money with them and their attention

00:03:28,920 --> 00:03:34,950
their eyeballs and whatever else pays

00:03:31,200 --> 00:03:36,690
our salaries so Amazon's found that for

00:03:34,950 --> 00:03:39,540
every hundred milliseconds of latency it

00:03:36,690 --> 00:03:41,579
costs them 1% sales for Amazon that's

00:03:39,540 --> 00:03:45,030
six to eight hundred million dollars a

00:03:41,579 --> 00:03:47,609
year ok so that's why they get to put

00:03:45,030 --> 00:03:49,230
teams and whole programming languages to

00:03:47,609 --> 00:03:51,510
work to try to squeeze this stuff but

00:03:49,230 --> 00:03:53,850
even for smaller organizations that can

00:03:51,510 --> 00:03:55,829
really help Google hadn't ran an

00:03:53,850 --> 00:03:58,049
experiment where they normally Google

00:03:55,829 --> 00:03:59,970
returns ten links per page ten search

00:03:58,049 --> 00:04:01,890
hits per page well they asked users well

00:03:59,970 --> 00:04:03,870
how many search hits do you want us to

00:04:01,890 --> 00:04:05,459
return in the first page and you know

00:04:03,870 --> 00:04:07,380
more is better right so ten is good

00:04:05,459 --> 00:04:07,950
twenty is better thirty is even better

00:04:07,380 --> 00:04:09,299
than that

00:04:07,950 --> 00:04:11,100
so they're like fine all right we'll put

00:04:09,299 --> 00:04:12,930
thirty on there well it turns out what

00:04:11,100 --> 00:04:14,609
happened when they put thirty blinks on

00:04:12,930 --> 00:04:17,070
their front page is the latency of their

00:04:14,609 --> 00:04:19,079
of their search response page went from

00:04:17,070 --> 00:04:21,539
300 milliseconds to 800 milliseconds and

00:04:19,079 --> 00:04:24,210
that alone was enough to cost them 20

00:04:21,539 --> 00:04:26,130
percent of their revenue even if your

00:04:24,210 --> 00:04:28,710
revenue if your latency is very bad just

00:04:26,130 --> 00:04:33,660
making it less bad will have a lot

00:04:28,710 --> 00:04:35,550
impact and Mozilla RAL found that the

00:04:33,660 --> 00:04:37,710
users that got lucky that has the

00:04:35,550 --> 00:04:38,910
fastest latency that your site responded

00:04:37,710 --> 00:04:41,520
those are the ones that are going to be

00:04:38,910 --> 00:04:43,889
pissed into your site so it really shows

00:04:41,520 --> 00:04:45,479
that if you can spend the extra time to

00:04:43,889 --> 00:04:47,610
optimize your software optimize your

00:04:45,479 --> 00:04:49,620
infrastructure reduce your latency it

00:04:47,610 --> 00:04:52,169
has a massive impact on how the users

00:04:49,620 --> 00:04:55,800
respond and how deep that they kind of

00:04:52,169 --> 00:04:57,570
trade in to your site so this talk was

00:04:55,800 --> 00:05:00,360
originally inspired by a talk given by

00:04:57,570 --> 00:05:02,160
Jeffrey Dean of Google entitled building

00:05:00,360 --> 00:05:04,440
software systems at Google and lessons

00:05:02,160 --> 00:05:05,820
learned and about two-thirds of the way

00:05:04,440 --> 00:05:07,560
through that talk that talked was sort

00:05:05,820 --> 00:05:09,630
of a natural history of the evolution of

00:05:07,560 --> 00:05:11,070
Google's infrastructure and it was it

00:05:09,630 --> 00:05:12,240
was pretty interesting but about

00:05:11,070 --> 00:05:14,190
two-thirds of the way through the talk

00:05:12,240 --> 00:05:16,620
was the most interesting thing for me

00:05:14,190 --> 00:05:19,039
which is a slide entitled numbers every

00:05:16,620 --> 00:05:22,770
programmer should know and it put into

00:05:19,039 --> 00:05:24,690
one slide something that I had struggled

00:05:22,770 --> 00:05:28,050
to explain to other developers for

00:05:24,690 --> 00:05:30,180
probably 10 or 15 years at that point I

00:05:28,050 --> 00:05:32,909
could never figure out how to explain

00:05:30,180 --> 00:05:38,340
how to optimize things and this sort of

00:05:32,909 --> 00:05:40,139
put it into focus and I think that the

00:05:38,340 --> 00:05:41,520
reason why they originally made it is to

00:05:40,139 --> 00:05:43,590
settle arguments between Google

00:05:41,520 --> 00:05:46,440
engineers because you know arguments

00:05:43,590 --> 00:05:49,590
between engineers can last you know till

00:05:46,440 --> 00:05:51,780
the heat death of the universe so the

00:05:49,590 --> 00:05:53,789
idea was to come up with a chart where

00:05:51,780 --> 00:05:55,680
you know you look up on this road you

00:05:53,789 --> 00:05:58,440
look over that road you know who's who

00:05:55,680 --> 00:06:00,360
has the right ish answer and you know

00:05:58,440 --> 00:06:02,789
which which technique you should you

00:06:00,360 --> 00:06:04,520
should try to implement and measure so

00:06:02,789 --> 00:06:07,770
let's go ahead and look at the numbers

00:06:04,520 --> 00:06:10,409
so in the center column you see the

00:06:07,770 --> 00:06:14,159
results in 2005 from the benchmarks and

00:06:10,409 --> 00:06:17,250
in the right column next 2017 and see at

00:06:14,159 --> 00:06:19,979
the top you can see various operations

00:06:17,250 --> 00:06:22,169
that you'll often do when you're writing

00:06:19,979 --> 00:06:23,960
a piece of software and they're just

00:06:22,169 --> 00:06:26,669
common operations that you can benchmark

00:06:23,960 --> 00:06:29,250
and then you can identify in your own

00:06:26,669 --> 00:06:31,380
software and you can sort of find you

00:06:29,250 --> 00:06:33,630
know this is roughly how long this could

00:06:31,380 --> 00:06:35,280
take and my hypothesis about the

00:06:33,630 --> 00:06:37,349
original argument this was made to

00:06:35,280 --> 00:06:38,729
settle was whether compression was going

00:06:37,349 --> 00:06:41,039
to speed up the latency of a micro

00:06:38,729 --> 00:06:42,569
service so you can see there's two rows

00:06:41,039 --> 00:06:44,759
right here compress

00:06:42,569 --> 00:06:46,679
kilobyte with zippy and send two

00:06:44,759 --> 00:06:48,719
kilobytes over the network in the data

00:06:46,679 --> 00:06:51,300
center and the interesting thing about

00:06:48,719 --> 00:06:54,300
this for me is that the answer the

00:06:51,300 --> 00:06:56,459
correct answer the most optimum solution

00:06:54,300 --> 00:06:59,399
in 2005 is different from what it would

00:06:56,459 --> 00:07:02,839
have been today so the developer who was

00:06:59,399 --> 00:07:05,399
right and in this particular case

00:07:02,839 --> 00:07:07,229
compression with zippy was much faster

00:07:05,399 --> 00:07:09,569
than sending the data over the network

00:07:07,229 --> 00:07:11,429
so the extra bandwidth you saved by

00:07:09,569 --> 00:07:13,139
compression turns out that actually

00:07:11,429 --> 00:07:14,490
reduced the latency because it reduced

00:07:13,139 --> 00:07:17,039
the amount of bits that the network

00:07:14,490 --> 00:07:18,809
stack had to read and so it turns out

00:07:17,039 --> 00:07:21,539
that the correct answer in 2005 was to

00:07:18,809 --> 00:07:22,949
compress things but today the correct

00:07:21,539 --> 00:07:25,439
answer would have been not to compress

00:07:22,949 --> 00:07:28,740
things turns out CPU time of waste that

00:07:25,439 --> 00:07:33,149
you would waste would not be worth the

00:07:28,740 --> 00:07:35,069
extra latency you would get so that's

00:07:33,149 --> 00:07:36,240
kind of interesting to me that shows

00:07:35,069 --> 00:07:37,860
that you really want to run these

00:07:36,240 --> 00:07:41,009
benchmarks every so often Hardware

00:07:37,860 --> 00:07:42,539
changes and just because the answer was

00:07:41,009 --> 00:07:45,719
correct last year doesn't mean we've

00:07:42,539 --> 00:07:47,430
corrected next year so what are the

00:07:45,719 --> 00:07:50,399
numbers tell us they tell us that CPU

00:07:47,430 --> 00:07:52,849
and RAM is obviously very fast acquiring

00:07:50,399 --> 00:07:54,839
and releasing locks is very very fast

00:07:52,849 --> 00:07:56,939
networking tied inside the data center

00:07:54,839 --> 00:07:59,099
is fast and is getting faster you notice

00:07:56,939 --> 00:08:00,949
that actually had the biggest jump and

00:07:59,099 --> 00:08:05,729
improvement in that 12 year period

00:08:00,949 --> 00:08:09,449
improves 64 times so networks can

00:08:05,729 --> 00:08:11,099
transmit 64 times as much data per unit

00:08:09,449 --> 00:08:12,569
time as they could when those those

00:08:11,099 --> 00:08:14,939
benchmarks were originally taken 12

00:08:12,569 --> 00:08:17,459
years ago they tell us the copying data

00:08:14,939 --> 00:08:19,499
is slow but it's getting faster but it's

00:08:17,459 --> 00:08:22,169
still still very slow in comparison with

00:08:19,499 --> 00:08:24,360
locking it tells us the disks are slow

00:08:22,169 --> 00:08:26,430
SSD is less slow but we are knew that

00:08:24,360 --> 00:08:28,079
and it tells us the networking outside

00:08:26,430 --> 00:08:31,349
the data center is slow and it's not

00:08:28,079 --> 00:08:33,649
changing I'll let you guys guess who's

00:08:31,349 --> 00:08:36,689
responsible for that

00:08:33,649 --> 00:08:39,149
so let's go and revisit that at that

00:08:36,689 --> 00:08:41,399
slide again but this time we're going to

00:08:39,149 --> 00:08:44,039
only look at now because we're not in

00:08:41,399 --> 00:08:45,600
2005 I don't have a time machine you

00:08:44,039 --> 00:08:48,180
don't probably have a time machine

00:08:45,600 --> 00:08:49,800
either so we're going to look at the

00:08:48,180 --> 00:08:51,750
operations we're going to look at the

00:08:49,800 --> 00:08:53,730
order of magnitude into which they're

00:08:51,750 --> 00:08:55,680
particularly this the duration of the

00:08:53,730 --> 00:08:56,490
application fall or a duration of the

00:08:55,680 --> 00:09:01,140
operation

00:08:56,490 --> 00:09:03,270
balls so you can see that you have the

00:09:01,140 --> 00:09:05,490
very slow round trip to the Netherlands

00:09:03,270 --> 00:09:07,260
that's another eight you can see you

00:09:05,490 --> 00:09:08,640
have disk operations those are both in

00:09:07,260 --> 00:09:10,770
the order two four or six that's not

00:09:08,640 --> 00:09:12,630
really surprising and as you get to the

00:09:10,770 --> 00:09:15,240
faster and faster operations you need to

00:09:12,630 --> 00:09:19,200
obviously a lower lower number generally

00:09:15,240 --> 00:09:21,899
you want to analyze any operation any

00:09:19,200 --> 00:09:23,610
operation in a particular order of

00:09:21,899 --> 00:09:27,420
magnitude is roughly equivalent for your

00:09:23,610 --> 00:09:29,160
analysis it's trying to hone in on exact

00:09:27,420 --> 00:09:31,350
numbers and optimizing software is

00:09:29,160 --> 00:09:33,060
probably not going to get you much what

00:09:31,350 --> 00:09:35,010
you're really optimizing is the most

00:09:33,060 --> 00:09:36,630
expensive operations and the most

00:09:35,010 --> 00:09:38,550
expensive operations are the ones that

00:09:36,630 --> 00:09:40,950
appear in the highest order of magnitude

00:09:38,550 --> 00:09:43,380
so even if the operations don't take the

00:09:40,950 --> 00:09:45,029
exact same amount of time if they're in

00:09:43,380 --> 00:09:50,670
the same order of magnitude you can sort

00:09:45,029 --> 00:09:51,930
of analyze them similarly anyway so some

00:09:50,670 --> 00:09:53,850
of you out there probably paying very

00:09:51,930 --> 00:09:55,380
close attention and you saw a couple

00:09:53,850 --> 00:09:58,110
numbers up there that did not make any

00:09:55,380 --> 00:09:59,490
sense they certainly are surprising to

00:09:58,110 --> 00:10:02,700
most people when they first see them in

00:09:59,490 --> 00:10:05,430
this particular case the 500,000

00:10:02,700 --> 00:10:09,060
nanosecond round trip and the 125 now

00:10:05,430 --> 00:10:11,940
set nanosecond speed for ripes so how

00:10:09,060 --> 00:10:15,089
can a right be one for thousands as fast

00:10:11,940 --> 00:10:17,040
as a pain that doesn't seem possible and

00:10:15,089 --> 00:10:19,290
it clearly is what's happening is that

00:10:17,040 --> 00:10:22,649
that right is an amortized amount of

00:10:19,290 --> 00:10:26,190
time it's not actually how much time it

00:10:22,649 --> 00:10:29,310
takes it's actually how it's actually

00:10:26,190 --> 00:10:32,130
the amount of time that it's responsible

00:10:29,310 --> 00:10:33,540
or taking up in the amount of time of

00:10:32,130 --> 00:10:36,360
the amount of data that you can send

00:10:33,540 --> 00:10:37,890
during one round trip so for instance

00:10:36,360 --> 00:10:41,310
are in this particular example are

00:10:37,890 --> 00:10:43,170
requests are 2k and we can send 4,000 of

00:10:41,310 --> 00:10:45,029
those requests every time that we do a

00:10:43,170 --> 00:10:47,520
pain in the amount of time it takes for

00:10:45,029 --> 00:10:49,260
obtained to complete for that network

00:10:47,520 --> 00:10:52,020
package to travel to the other side and

00:10:49,260 --> 00:10:55,110
back again you can send 4,000 of these

00:10:52,020 --> 00:10:56,160
two things to 2k requests so it's

00:10:55,110 --> 00:10:58,589
important to understand the difference

00:10:56,160 --> 00:11:01,860
between latency and throughput and the

00:10:58,589 --> 00:11:03,899
fact that they're not exactly the same

00:11:01,860 --> 00:11:06,870
thing but they're also not exactly

00:11:03,899 --> 00:11:09,000
inversely proportional or uh inverses of

00:11:06,870 --> 00:11:10,210
each other so let's talk about that a

00:11:09,000 --> 00:11:12,370
little bit more

00:11:10,210 --> 00:11:15,040
latency is obviously the amount of time

00:11:12,370 --> 00:11:17,260
it takes for a particular operation to

00:11:15,040 --> 00:11:20,320
complete from start to finish one

00:11:17,260 --> 00:11:22,030
operation throughput is the number of

00:11:20,320 --> 00:11:24,610
events per second you can process

00:11:22,030 --> 00:11:28,000
through so you can think of latency as

00:11:24,610 --> 00:11:29,290
being how long a road is or how fast a

00:11:28,000 --> 00:11:31,030
car is traveling on the road and

00:11:29,290 --> 00:11:33,910
throughput you can think of is the

00:11:31,030 --> 00:11:36,580
number of lanes in the highway how wide

00:11:33,910 --> 00:11:39,570
the road is is it a superhighway or is

00:11:36,580 --> 00:11:45,220
it a dirt path through somebody's field

00:11:39,570 --> 00:11:46,480
so generally when we build software

00:11:45,220 --> 00:11:48,970
infrastructures there's a couple

00:11:46,480 --> 00:11:50,260
different design patterns that we use it

00:11:48,970 --> 00:11:52,750
turns out knowing what you're doing

00:11:50,260 --> 00:11:53,920
knowing are you optimizing for latency

00:11:52,750 --> 00:11:56,680
or throughput is important because that

00:11:53,920 --> 00:11:59,530
sort of tells you which kind of design

00:11:56,680 --> 00:12:01,150
patterns to use a lot of people use

00:11:59,530 --> 00:12:02,800
microservices and they understand the

00:12:01,150 --> 00:12:05,410
benefits that come along with that in

00:12:02,800 --> 00:12:07,180
terms of organization but from a

00:12:05,410 --> 00:12:10,180
technical a technology point of view

00:12:07,180 --> 00:12:11,680
from a computing point of view micro

00:12:10,180 --> 00:12:13,750
services are used when you want to

00:12:11,680 --> 00:12:15,460
squeeze latency when you're not

00:12:13,750 --> 00:12:17,770
squeezing link to use different design

00:12:15,460 --> 00:12:20,890
patterns that i've sort of called data

00:12:17,770 --> 00:12:22,420
pipeline for lack of a better name and

00:12:20,890 --> 00:12:24,130
those are applications that are going to

00:12:22,420 --> 00:12:25,690
be optimized for throughput and you're

00:12:24,130 --> 00:12:27,190
going to design those applications

00:12:25,690 --> 00:12:28,780
differently and you're going to use

00:12:27,190 --> 00:12:30,490
slightly different design techniques

00:12:28,780 --> 00:12:32,530
when you're doing a micro service versus

00:12:30,490 --> 00:12:34,060
the data pipeline so it's important to

00:12:32,530 --> 00:12:36,370
know when you're starting out and

00:12:34,060 --> 00:12:41,410
writing a service which of these two

00:12:36,370 --> 00:12:43,680
goals you're trying to go for so micro

00:12:41,410 --> 00:12:46,900
services as we said are good for

00:12:43,680 --> 00:12:49,690
organizing teams they ought you will

00:12:46,900 --> 00:12:52,180
also tend to when you're writing a micro

00:12:49,690 --> 00:12:54,280
service you'll tend to precache or

00:12:52,180 --> 00:12:57,520
pre-allocate buffers and what have you

00:12:54,280 --> 00:12:59,620
in terms of individual requests so you

00:12:57,520 --> 00:13:01,840
you might if you allocate buffers you're

00:12:59,620 --> 00:13:03,970
going to allocate them as to the size of

00:13:01,840 --> 00:13:07,140
the maximum requested that service can

00:13:03,970 --> 00:13:09,010
handle it also is nice and that it keeps

00:13:07,140 --> 00:13:11,530
developers from doing things they

00:13:09,010 --> 00:13:13,930
probably shouldn't like calling library

00:13:11,530 --> 00:13:15,940
methods that another team wrote that may

00:13:13,930 --> 00:13:19,150
be scheduled for defecation or something

00:13:15,940 --> 00:13:21,370
else along those lines they do have more

00:13:19,150 --> 00:13:25,590
overhead due to you having more teams

00:13:21,370 --> 00:13:25,590
but that's for somebody else

00:13:25,720 --> 00:13:29,110
why data pipelines you do a data

00:13:27,699 --> 00:13:30,399
pipeline when you're trying to get as

00:13:29,110 --> 00:13:32,980
much data through your system as

00:13:30,399 --> 00:13:34,209
possible generally with data pipelines

00:13:32,980 --> 00:13:36,819
you're going to use back pressuring

00:13:34,209 --> 00:13:39,249
techniques you'll pre-allocate buffers

00:13:36,819 --> 00:13:40,959
and other sorts of system resources in

00:13:39,249 --> 00:13:44,499
terms of numbers that are good for the

00:13:40,959 --> 00:13:46,839
hardware for example um you will often

00:13:44,499 --> 00:13:49,959
allocate buffers in blocks of four

00:13:46,839 --> 00:13:51,879
kilobytes each why four kilobytes well

00:13:49,959 --> 00:13:54,879
that's the size of a page in virtual

00:13:51,879 --> 00:13:57,029
memory or a page on disk so it turns out

00:13:54,879 --> 00:14:00,339
that actually malloc you four kilobytes

00:13:57,029 --> 00:14:04,420
4096 bytes is faster than mounting one

00:14:00,339 --> 00:14:06,939
byte the reason for this is that if I'm

00:14:04,420 --> 00:14:10,059
not only managing one byte I have to

00:14:06,939 --> 00:14:12,970
first find a page that is either if

00:14:10,059 --> 00:14:14,709
three or partially free and then I see

00:14:12,970 --> 00:14:16,869
if there's a one byte run on that page

00:14:14,709 --> 00:14:18,610
and then I have to actually update that

00:14:16,869 --> 00:14:21,519
page with the information that says this

00:14:18,610 --> 00:14:24,339
byte is now allocated if I'm allocating

00:14:21,519 --> 00:14:26,350
four bytes by Malecha four by are more

00:14:24,339 --> 00:14:31,329
kilobytes all I have to do is find a new

00:14:26,350 --> 00:14:32,920
page much faster and so so often in data

00:14:31,329 --> 00:14:35,100
pipelines you're taking advantages of

00:14:32,920 --> 00:14:37,389
these sorts of tricks in the hardware

00:14:35,100 --> 00:14:39,610
and that allows you to squeeze your

00:14:37,389 --> 00:14:40,779
throughput you should never do that and

00:14:39,610 --> 00:14:42,850
when you're doing a micro service

00:14:40,779 --> 00:14:47,949
microservices should be sized for one

00:14:42,850 --> 00:14:49,540
request so now that we've talked about

00:14:47,949 --> 00:14:51,220
overall design goals that we should be

00:14:49,540 --> 00:14:53,319
looking at now we'll talk about how you

00:14:51,220 --> 00:14:57,549
hone in on specific parts of your

00:14:53,319 --> 00:14:59,350
application to know what optimized so

00:14:57,549 --> 00:15:02,019
the first thing to realize is something

00:14:59,350 --> 00:15:04,329
that I'm sure you all already know which

00:15:02,019 --> 00:15:06,519
is that not all lines of code are on the

00:15:04,329 --> 00:15:08,199
same number of times you have some code

00:15:06,519 --> 00:15:10,720
that runs it startup you got some code

00:15:08,199 --> 00:15:12,610
that runs a teardown hopefully and then

00:15:10,720 --> 00:15:16,600
you have code that handles individual

00:15:12,610 --> 00:15:18,369
requests a code that's going to be and

00:15:16,600 --> 00:15:19,869
and also on top of that you might have

00:15:18,369 --> 00:15:21,519
code inside the code that handles

00:15:19,869 --> 00:15:24,220
individual requests it runs multiple

00:15:21,519 --> 00:15:26,850
times for requests the most frequent run

00:15:24,220 --> 00:15:29,139
of code would be known as the inner loop

00:15:26,850 --> 00:15:30,579
generally you're trying to optimize the

00:15:29,139 --> 00:15:32,410
inner loop you're trying to pull as much

00:15:30,579 --> 00:15:33,999
operations out of the inner loop you're

00:15:32,410 --> 00:15:36,730
trying to pull any expensive operations

00:15:33,999 --> 00:15:37,540
out of an inner loop as you can and if

00:15:36,730 --> 00:15:40,089
you look closely

00:15:37,540 --> 00:15:42,399
database the actual architecture of

00:15:40,089 --> 00:15:46,060
software the database you'll realize

00:15:42,399 --> 00:15:48,519
that about 90% of it is doesn't touch

00:15:46,060 --> 00:15:50,079
rows ever it never touches the data that

00:15:48,519 --> 00:15:52,690
actually gets returned to you when you

00:15:50,079 --> 00:15:58,000
run a query 90% of the code is actually

00:15:52,690 --> 00:16:01,360
things that are doing doing strange or

00:15:58,000 --> 00:16:03,250
doing novel techniques to pull work out

00:16:01,360 --> 00:16:05,199
of that inner loop there are whole

00:16:03,250 --> 00:16:07,930
components in databases that their only

00:16:05,199 --> 00:16:11,100
job is to figure out how to do one less

00:16:07,930 --> 00:16:13,300
operation per row inside that inner loop

00:16:11,100 --> 00:16:15,940
and that's sort of the ultimate lesson

00:16:13,300 --> 00:16:18,370
in the ultimate evolution of a highly

00:16:15,940 --> 00:16:21,790
optimized architecture in the software

00:16:18,370 --> 00:16:23,769
industry right now so I'm sure everyone

00:16:21,790 --> 00:16:26,980
is familiar with Big O notation and

00:16:23,769 --> 00:16:28,420
complexity analysis if not then go to an

00:16:26,980 --> 00:16:33,910
interview and you will become familiar

00:16:28,420 --> 00:16:36,459
with them so generally you can do the

00:16:33,910 --> 00:16:37,990
same sort of analysis here in this case

00:16:36,459 --> 00:16:39,699
you're looking for operations that are

00:16:37,990 --> 00:16:41,350
in the highest order of magnitude and

00:16:39,699 --> 00:16:43,720
you sort of want to treat them as what

00:16:41,350 --> 00:16:45,160
you're analyzing you only want to look

00:16:43,720 --> 00:16:46,690
at the frequency of the most expensive

00:16:45,160 --> 00:16:48,459
operations you don't care about less

00:16:46,690 --> 00:16:51,880
expensive operations just like they go

00:16:48,459 --> 00:16:54,639
notation and you want to then find the

00:16:51,880 --> 00:16:56,139
most successes application or operation

00:16:54,639 --> 00:16:57,880
that's run the most number of times and

00:16:56,139 --> 00:17:00,279
you want to figure out how to reduce or

00:16:57,880 --> 00:17:03,370
eliminate those calls this is the single

00:17:00,279 --> 00:17:05,559
best way to optimize your application so

00:17:03,370 --> 00:17:08,380
how do you do that what things can you

00:17:05,559 --> 00:17:10,089
do to sort of save or reduce the these

00:17:08,380 --> 00:17:11,770
expensive calls that your application

00:17:10,089 --> 00:17:14,620
might need to make carry out its use

00:17:11,770 --> 00:17:17,980
case well the first one is that you can

00:17:14,620 --> 00:17:19,780
just call the operation once you can

00:17:17,980 --> 00:17:22,659
take it whatever it returned to you and

00:17:19,780 --> 00:17:25,179
you can reuse that thing so an example

00:17:22,659 --> 00:17:27,909
of that would be like a pre allocation

00:17:25,179 --> 00:17:29,799
or caching and pre allocation when you

00:17:27,909 --> 00:17:33,309
pre-allocate buffer Malik is the

00:17:29,799 --> 00:17:35,080
expensive operation and you're using the

00:17:33,309 --> 00:17:36,460
result of it the buffer that it gave you

00:17:35,080 --> 00:17:40,059
and you're going to use it over and over

00:17:36,460 --> 00:17:43,240
and over again you can also make the

00:17:40,059 --> 00:17:45,460
operations less expensive so for

00:17:43,240 --> 00:17:48,010
instance if you're doing parsing you

00:17:45,460 --> 00:17:50,559
might want to use a sax or call back

00:17:48,010 --> 00:17:51,490
style parser instead of a Dom parser the

00:17:50,559 --> 00:17:54,100
reason for that is

00:17:51,490 --> 00:17:56,200
like a said Malik's very expensive Dom's

00:17:54,100 --> 00:17:57,280
are going to require allocating an awful

00:17:56,200 --> 00:18:00,040
lot of memory to hold the entire

00:17:57,280 --> 00:18:01,840
document in memory and whereas a sack

00:18:00,040 --> 00:18:03,940
style parser only needs to hold enough

00:18:01,840 --> 00:18:07,510
memory to hold all the data from the

00:18:03,940 --> 00:18:09,070
root to the leaf and if that's all if

00:18:07,510 --> 00:18:10,720
you have to actually allocate less

00:18:09,070 --> 00:18:13,210
memory and you're reusing that memory

00:18:10,720 --> 00:18:15,700
it's in the caches it stays in the

00:18:13,210 --> 00:18:17,620
caches and it's going to speed up your

00:18:15,700 --> 00:18:20,200
operations compression is another way to

00:18:17,620 --> 00:18:22,090
do it but often data centers networking

00:18:20,200 --> 00:18:25,690
is so fast nowadays it doesn't help so

00:18:22,090 --> 00:18:27,580
much but that's a nice technique you can

00:18:25,690 --> 00:18:30,400
call the expensive operation less times

00:18:27,580 --> 00:18:33,550
with more data so for instance batching

00:18:30,400 --> 00:18:35,440
a bunch of database writes or and this

00:18:33,550 --> 00:18:37,929
is my favorite one you can be tricky in

00:18:35,440 --> 00:18:39,460
which case you don't do the expensive

00:18:37,929 --> 00:18:41,590
operation until after you've returned

00:18:39,460 --> 00:18:43,570
the results to the user so for instance

00:18:41,590 --> 00:18:44,770
metrics collection or user behavioral

00:18:43,570 --> 00:18:47,530
collection this would be a good example

00:18:44,770 --> 00:18:49,240
of that so we have a couple applications

00:18:47,530 --> 00:18:50,530
at Credit Karma till after they return

00:18:49,240 --> 00:18:54,370
the response then they do something else

00:18:50,530 --> 00:18:57,010
and that the user doesn't see that extra

00:18:54,370 --> 00:19:00,010
wait time and honestly if you can do the

00:18:57,010 --> 00:19:00,970
last one that's that's the best one well

00:19:00,010 --> 00:19:03,820
the first one's the best one the last

00:19:00,970 --> 00:19:08,110
one second so let's try to put some of

00:19:03,820 --> 00:19:11,980
the stuff into practice so let's imagine

00:19:08,110 --> 00:19:15,250
we have an example application this

00:19:11,980 --> 00:19:18,340
example application reads some JSON data

00:19:15,250 --> 00:19:20,320
over the network maybe at HTTP it'll

00:19:18,340 --> 00:19:22,090
parse it it'll transform it and it will

00:19:20,320 --> 00:19:25,230
write it to the database or write a row

00:19:22,090 --> 00:19:27,730
to the database should we focus on

00:19:25,230 --> 00:19:29,110
optimizing the networking or the parsing

00:19:27,730 --> 00:19:31,059
or the transformation how should we

00:19:29,110 --> 00:19:32,980
optimize that say we've we've written it

00:19:31,059 --> 00:19:34,660
it's in production it's running but we

00:19:32,980 --> 00:19:37,600
wanted to run more efficiently we wanted

00:19:34,660 --> 00:19:41,590
to run with lower latency so here's a

00:19:37,600 --> 00:19:44,020
couple operation benchmarks of those

00:19:41,590 --> 00:19:45,490
common operations from the previous

00:19:44,020 --> 00:19:48,640
slide you can see we have the four

00:19:45,490 --> 00:19:50,860
different more expensive operations and

00:19:48,640 --> 00:19:54,820
that's roughly how much time they take

00:19:50,860 --> 00:19:56,590
in terms of benchmarking so what do we

00:19:54,820 --> 00:19:59,110
want to do how do we want to optimize

00:19:56,590 --> 00:20:01,350
this application so you want to have an

00:19:59,110 --> 00:20:01,350
idea

00:20:02,770 --> 00:20:11,020
say again yes that's actually correct

00:20:08,040 --> 00:20:12,760
thank you

00:20:11,020 --> 00:20:15,309
never team latency is one round-trip

00:20:12,760 --> 00:20:18,490
plus one right

00:20:15,309 --> 00:20:20,910
the latency of parsing 2k JSON objects

00:20:18,490 --> 00:20:23,380
roughly 10,000 nanoseconds and

00:20:20,910 --> 00:20:26,050
microseconds same thing for the

00:20:23,380 --> 00:20:27,790
transformation and the duration of

00:20:26,050 --> 00:20:30,420
writing the row like we said before is

00:20:27,790 --> 00:20:32,860
it's very slow it's it's 5 milliseconds

00:20:30,420 --> 00:20:34,500
so if you can get away with it the

00:20:32,860 --> 00:20:37,330
correct answer in that case would be to

00:20:34,500 --> 00:20:40,809
batch up your rows and maybe you only do

00:20:37,330 --> 00:20:43,960
one write every 128 requests when one

00:20:40,809 --> 00:20:46,540
one commits every 128 requests in that

00:20:43,960 --> 00:20:49,980
case that would actually us we improve

00:20:46,540 --> 00:20:53,620
your latency 90% you would be able to

00:20:49,980 --> 00:20:55,120
respond in one tenth the time because it

00:20:53,620 --> 00:20:58,090
was far and away the most expensive

00:20:55,120 --> 00:21:00,940
operation and 1% of the people or maybe

00:20:58,090 --> 00:21:02,920
if we do batch up 500 requests and that

00:21:00,940 --> 00:21:06,160
would be to ten to the population well

00:21:02,920 --> 00:21:08,800
they get unlucky their latency is

00:21:06,160 --> 00:21:10,840
getting much higher five milliseconds

00:21:08,800 --> 00:21:14,170
higher in this particular case or five

00:21:10,840 --> 00:21:15,940
and a half but that's but the other

00:21:14,170 --> 00:21:18,340
ninety-nine point eight percent of the

00:21:15,940 --> 00:21:19,780
people get a much faster experience and

00:21:18,340 --> 00:21:21,420
that that might be a really good

00:21:19,780 --> 00:21:23,559
trade-off depending on what you're doing

00:21:21,420 --> 00:21:26,530
so it's important to understand your use

00:21:23,559 --> 00:21:27,850
case it's important to understand what

00:21:26,530 --> 00:21:31,360
trade-offs you're willing to make in

00:21:27,850 --> 00:21:34,929
terms of concurrency or consistency or

00:21:31,360 --> 00:21:36,730
even even accuracy and sometimes

00:21:34,929 --> 00:21:38,710
sacrificing a few of those things will

00:21:36,730 --> 00:21:40,690
open up optimizations that allow you to

00:21:38,710 --> 00:21:42,220
really reduce your latency or really

00:21:40,690 --> 00:21:45,070
Drive Thru foot depending on what you're

00:21:42,220 --> 00:21:46,900
going for so instead of focusing on the

00:21:45,070 --> 00:21:48,190
JSON parsing or transforming that's

00:21:46,900 --> 00:21:50,559
clearly the wrong thing to do you

00:21:48,190 --> 00:21:52,570
optimize the network performance thank

00:21:50,559 --> 00:21:55,330
you for that and this can be done by

00:21:52,570 --> 00:21:56,830
batching the DB writes compression also

00:21:55,330 --> 00:22:00,450
might be something that works in this

00:21:56,830 --> 00:22:02,980
particular case compression works when

00:22:00,450 --> 00:22:04,570
when often if you have specialized

00:22:02,980 --> 00:22:06,550
hardware is doing the compression that

00:22:04,570 --> 00:22:08,800
that will speed things up to the point

00:22:06,550 --> 00:22:11,650
where you can take advantage of it but

00:22:08,800 --> 00:22:14,260
often modern in data center networks are

00:22:11,650 --> 00:22:16,690
so fast that compression is not really

00:22:14,260 --> 00:22:21,159
as beneficial as it used to be

00:22:16,690 --> 00:22:22,509
so I guessed here this is what I think

00:22:21,159 --> 00:22:24,129
that application would look like in

00:22:22,509 --> 00:22:25,359
terms of a load profile and it would

00:22:24,129 --> 00:22:26,619
probably look something some of this we

00:22:25,359 --> 00:22:30,249
have similar applications to Credit

00:22:26,619 --> 00:22:31,690
Karma to look roughly like this and so

00:22:30,249 --> 00:22:33,489
generally when you're running your

00:22:31,690 --> 00:22:35,200
application if it's running at full bore

00:22:33,489 --> 00:22:38,710
if your load testing it you have it

00:22:35,200 --> 00:22:40,379
running right at its peak that exam

00:22:38,710 --> 00:22:43,269
Aksum number of requests it can handle

00:22:40,379 --> 00:22:46,149
you'll get one of these three three

00:22:43,269 --> 00:22:48,639
measurements will always be nearly

00:22:46,149 --> 00:22:51,340
pegged off it will be 90% or higher

00:22:48,639 --> 00:22:52,840
that's your bottleneck that's the

00:22:51,340 --> 00:22:54,429
easiest way to figure out what's

00:22:52,840 --> 00:22:57,070
optimize when you have an application

00:22:54,429 --> 00:22:59,399
you're trying to optimize which of these

00:22:57,070 --> 00:23:02,109
three things are you using almost all of

00:22:59,399 --> 00:23:03,039
if you're not using almost all of these

00:23:02,109 --> 00:23:05,950
three things there's a couple

00:23:03,039 --> 00:23:07,119
possibilities a you may not be throwing

00:23:05,950 --> 00:23:09,879
enough traffic at your particular

00:23:07,119 --> 00:23:12,399
application maybe it just has that much

00:23:09,879 --> 00:23:13,899
extra spare horsepower to go or maybe

00:23:12,399 --> 00:23:18,009
there's something horribly wrong with

00:23:13,899 --> 00:23:19,960
your code but generally you will get a

00:23:18,009 --> 00:23:22,090
picture like this where one of them is

00:23:19,960 --> 00:23:23,169
pegged off nearly scale high and what

00:23:22,090 --> 00:23:25,149
you want to do is you want to try to

00:23:23,169 --> 00:23:27,849
optimize the operations the most heavily

00:23:25,149 --> 00:23:30,450
use that resource so some people might

00:23:27,849 --> 00:23:33,879
look at this graph just stick your

00:23:30,450 --> 00:23:36,039
diagram and say okay you should do

00:23:33,879 --> 00:23:37,659
asynchronous right you should be doing a

00:23:36,039 --> 00:23:39,729
synchro's networking that's what's going

00:23:37,659 --> 00:23:41,529
to improve this this year turns out

00:23:39,729 --> 00:23:43,629
that's not actually correct

00:23:41,529 --> 00:23:45,789
so we benchmarked blocking versus non

00:23:43,629 --> 00:23:47,950
blocking and it bears out some

00:23:45,789 --> 00:23:50,080
interesting results so the first thing

00:23:47,950 --> 00:23:53,159
to note here is that blocking i/o is

00:23:50,080 --> 00:23:56,019
often faster especially for writes

00:23:53,159 --> 00:23:59,259
that's very strange most people do not

00:23:56,019 --> 00:24:01,029
realize that also the reads are almost

00:23:59,259 --> 00:24:04,899
identical speed they're almost

00:24:01,029 --> 00:24:07,359
identically the same amount of time the

00:24:04,899 --> 00:24:09,789
one trick here though is that when you

00:24:07,359 --> 00:24:11,649
run these benchmarks we notice that for

00:24:09,789 --> 00:24:14,799
the block and we expected this but we

00:24:11,649 --> 00:24:16,149
noticed for the blocking i/o the CPU

00:24:14,799 --> 00:24:17,769
would be much higher when we're running

00:24:16,149 --> 00:24:20,109
the blocking benchmarks versus the non

00:24:17,769 --> 00:24:22,359
blocking benchmarks and the reason for

00:24:20,109 --> 00:24:24,580
that is that a synchronous i/o whether

00:24:22,359 --> 00:24:26,769
it's networking or disk what it really

00:24:24,580 --> 00:24:29,320
saves you with CPU it doesn't save you

00:24:26,769 --> 00:24:30,370
disk your networking and that's an

00:24:29,320 --> 00:24:31,570
important importance

00:24:30,370 --> 00:24:33,340
to realize when you're trying to

00:24:31,570 --> 00:24:36,090
optimize these applications because a

00:24:33,340 --> 00:24:38,380
lot of people are confused about that

00:24:36,090 --> 00:24:41,770
they should only use non-blocking when

00:24:38,380 --> 00:24:43,540
the CPU is either overloaded or you want

00:24:41,770 --> 00:24:45,670
to reduce it you have an active reason

00:24:43,540 --> 00:24:47,920
to want to reduce your CPU load that's

00:24:45,670 --> 00:24:49,750
when you use asynchronous and how much

00:24:47,920 --> 00:24:52,660
global shared state you have is sort of

00:24:49,750 --> 00:24:58,750
the feeling on how much use you can get

00:24:52,660 --> 00:25:01,240
out of that extra CPU time so you can

00:24:58,750 --> 00:25:03,340
often the benefits of non-blocking

00:25:01,240 --> 00:25:04,720
shared shared overhead so it's important

00:25:03,340 --> 00:25:07,720
to understand what's happening when you

00:25:04,720 --> 00:25:10,120
when you're using a blocking API alright

00:25:07,720 --> 00:25:13,360
so when you're using at the lowest level

00:25:10,120 --> 00:25:15,970
all i/o is asynchronous when your disc

00:25:13,360 --> 00:25:17,830
receives a new buffer or when you're

00:25:15,970 --> 00:25:20,230
reading from disk and a buffer has been

00:25:17,830 --> 00:25:22,870
filled by the disk hardware that sends

00:25:20,230 --> 00:25:25,630
an interrupt to the CPU when the network

00:25:22,870 --> 00:25:26,710
write to pack it out and those that it's

00:25:25,630 --> 00:25:29,290
the right has completed successfully

00:25:26,710 --> 00:25:31,000
that sends it interrupts to the CPU at

00:25:29,290 --> 00:25:32,890
the lowest level all i/o is

00:25:31,000 --> 00:25:34,360
fundamentally asynchronous what's

00:25:32,890 --> 00:25:37,330
happening is that the folks that wrote

00:25:34,360 --> 00:25:40,870
the OS or blocking api's you're using in

00:25:37,330 --> 00:25:42,960
your language have written the code that

00:25:40,870 --> 00:25:45,910
lose together those events into a

00:25:42,960 --> 00:25:49,540
synchronous stream that lets your API

00:25:45,910 --> 00:25:51,670
work in a blocking way but if say I have

00:25:49,540 --> 00:25:55,660
four or ten or a hundred of these

00:25:51,670 --> 00:25:57,070
blocking writers or readers that what

00:25:55,660 --> 00:25:58,720
that's doing is introducing a tremendous

00:25:57,070 --> 00:26:00,070
amount of overhead when you're doing

00:25:58,720 --> 00:26:03,820
asynchronous programming what you're

00:26:00,070 --> 00:26:06,760
doing is allowing one shared piece of

00:26:03,820 --> 00:26:08,410
software piece of memory to handle all

00:26:06,760 --> 00:26:09,850
of the overheads of all of those

00:26:08,410 --> 00:26:12,130
different write operations that you're

00:26:09,850 --> 00:26:14,860
doing all right and that's why it's

00:26:12,130 --> 00:26:16,480
that's why it saves CPU and that's why

00:26:14,860 --> 00:26:18,690
it doesn't really have an impact on

00:26:16,480 --> 00:26:20,530
networking or disk utilization and the

00:26:18,690 --> 00:26:22,900
surprising thing for most people is that

00:26:20,530 --> 00:26:25,809
usually you can max out the hardware by

00:26:22,900 --> 00:26:28,720
doing blocking operations that surprises

00:26:25,809 --> 00:26:30,760
a lot of people generally you'll also

00:26:28,720 --> 00:26:33,970
max out your CPU at the same time or at

00:26:30,760 --> 00:26:35,710
least come close to it but then that's

00:26:33,970 --> 00:26:39,490
why we have non-blocking so that you can

00:26:35,710 --> 00:26:41,350
save that CPU resource so say somebody

00:26:39,490 --> 00:26:42,460
went and changed our application to

00:26:41,350 --> 00:26:44,430
non-blocking

00:26:42,460 --> 00:26:47,580
this is what we would expect

00:26:44,430 --> 00:26:48,750
the CPU is now gone from 25/3 but our

00:26:47,580 --> 00:26:50,460
application has the exact same

00:26:48,750 --> 00:26:51,960
performance profile that it did before

00:26:50,460 --> 00:26:53,490
in other words it has the same latency

00:26:51,960 --> 00:26:55,350
and handling the same number of requests

00:26:53,490 --> 00:26:58,410
per second so it's important to

00:26:55,350 --> 00:27:00,270
understand when you use asynchronous i/o

00:26:58,410 --> 00:27:02,640
what you're getting out of it what

00:27:00,270 --> 00:27:07,170
you're getting is more CPU not more disk

00:27:02,640 --> 00:27:11,280
or more network so the message is bench

00:27:07,170 --> 00:27:13,020
market Hardware changes over time the

00:27:11,280 --> 00:27:15,000
numbers and I'm reporting are probably

00:27:13,020 --> 00:27:16,740
not the numbers that you'll get because

00:27:15,000 --> 00:27:18,059
the hardware I have it Credit Karma is

00:27:16,740 --> 00:27:20,370
probably not the same as the hardware

00:27:18,059 --> 00:27:23,070
you have wherever you will work and

00:27:20,370 --> 00:27:25,380
certainly and the numbers that every

00:27:23,070 --> 00:27:28,440
programmer should know I wish credit

00:27:25,380 --> 00:27:30,230
Karma's paying times with that love we

00:27:28,440 --> 00:27:32,460
have a really good network but at

00:27:30,230 --> 00:27:34,590
500,000 nanosecond number that's an

00:27:32,460 --> 00:27:37,050
incredible network the Google folks

00:27:34,590 --> 00:27:39,679
really know what they're doing and so

00:27:37,050 --> 00:27:42,059
you have to benchmark it for yourself

00:27:39,679 --> 00:27:43,380
because often you'll be surprised by

00:27:42,059 --> 00:27:47,250
these things and these things change

00:27:43,380 --> 00:27:48,750
over time so for instance you'll see in

00:27:47,250 --> 00:27:52,230
a second when we talk about is instance

00:27:48,750 --> 00:27:55,080
of I've programmed Java for 20 years so

00:27:52,230 --> 00:27:57,240
his instance of is evil as far as most

00:27:55,080 --> 00:27:58,740
Java programmers know but it turns out

00:27:57,240 --> 00:28:00,390
at one point they actually optimized

00:27:58,740 --> 00:28:03,360
that we'll talk about that in a second

00:28:00,390 --> 00:28:06,030
but things change your your programming

00:28:03,360 --> 00:28:07,290
language optimizes certain containers

00:28:06,030 --> 00:28:09,600
certain runtimes

00:28:07,290 --> 00:28:11,429
those things improve over time the OS

00:28:09,600 --> 00:28:13,260
improves over time the hardware improves

00:28:11,429 --> 00:28:14,730
over time these things change so you

00:28:13,260 --> 00:28:17,309
have to benchmark it for yourself when

00:28:14,730 --> 00:28:20,250
you're optimizing these things so we did

00:28:17,309 --> 00:28:22,650
some benchmarks we used jmh which is the

00:28:20,250 --> 00:28:25,350
Java micro benchmark harness which you

00:28:22,650 --> 00:28:27,480
can use from Scala and I did the

00:28:25,350 --> 00:28:31,800
benchmarks on this trusty little laptop

00:28:27,480 --> 00:28:33,059
here and m3 medium and AWS and I report

00:28:31,800 --> 00:28:36,059
through what's being reported are the

00:28:33,059 --> 00:28:38,700
averages of those two the averages of

00:28:36,059 --> 00:28:40,290
those two numbers and you can go and

00:28:38,700 --> 00:28:43,880
download the benchmark yourself and run

00:28:40,290 --> 00:28:48,570
the cell run them yourself and see what

00:28:43,880 --> 00:28:52,050
they tell you about your own hardware so

00:28:48,570 --> 00:28:53,610
we saw the numbers that every programmer

00:28:52,050 --> 00:28:55,110
should know we thought all right what

00:28:53,610 --> 00:28:56,550
are some other benchmarks that we might

00:28:55,110 --> 00:28:58,080
want to do what are some other

00:28:56,550 --> 00:28:59,789
performance numbers

00:28:58,080 --> 00:29:01,620
that we might were curious about

00:28:59,789 --> 00:29:03,750
especially for the scholar language

00:29:01,620 --> 00:29:06,750
what's what is it good at what is it bad

00:29:03,750 --> 00:29:09,330
at so we benchmarked system calls like

00:29:06,750 --> 00:29:11,460
random and current time we benchmark

00:29:09,330 --> 00:29:13,110
some IO operations as you already saw

00:29:11,460 --> 00:29:14,820
we also benchmark some less common i/o

00:29:13,110 --> 00:29:18,059
operations like deleting a file or

00:29:14,820 --> 00:29:19,799
updating its metadata we tried to

00:29:18,059 --> 00:29:22,649
benchmark mysterious things like context

00:29:19,799 --> 00:29:23,519
which is that failed but we did find a

00:29:22,649 --> 00:29:25,380
good number for them

00:29:23,519 --> 00:29:26,970
we've been smart searching large data

00:29:25,380 --> 00:29:29,269
structures and we benchmarks which

00:29:26,970 --> 00:29:32,789
attachments with classes versus integers

00:29:29,269 --> 00:29:34,230
so and we were surprised by some of the

00:29:32,789 --> 00:29:36,690
numbers and not surprised by some of the

00:29:34,230 --> 00:29:38,639
numbers so we weren't really surprised

00:29:36,690 --> 00:29:40,019
to system calls are really fast the

00:29:38,639 --> 00:29:42,750
Linux kernel team is really really good

00:29:40,019 --> 00:29:46,289
they write very efficient code the next

00:29:42,750 --> 00:29:48,750
one the match against six classes in the

00:29:46,289 --> 00:29:51,210
match against six integers that one we

00:29:48,750 --> 00:29:52,649
were very surprised by like I said I

00:29:51,210 --> 00:29:54,659
programmed Java for a long time is

00:29:52,649 --> 00:29:55,980
instance of a slow it's evil never call

00:29:54,659 --> 00:29:59,010
it you're doing something wrong if you

00:29:55,980 --> 00:30:00,990
call it however when you do a switch a

00:29:59,010 --> 00:30:03,630
match statement in Scala and you match I

00:30:00,990 --> 00:30:06,450
guess the class well that actually turn

00:30:03,630 --> 00:30:08,220
in turn will call is instance up and it

00:30:06,450 --> 00:30:10,200
will do it with an if-else structure

00:30:08,220 --> 00:30:11,580
until you get to six items when you get

00:30:10,200 --> 00:30:13,529
to six items it will actually use a jump

00:30:11,580 --> 00:30:16,789
statement turns out that runs at the

00:30:13,529 --> 00:30:19,740
same speed as if else for the most part

00:30:16,789 --> 00:30:22,080
this was very surprising to me I had

00:30:19,740 --> 00:30:24,389
seen some Scala code in a JSON library

00:30:22,080 --> 00:30:26,429
that will remain nameless where they had

00:30:24,389 --> 00:30:28,350
done some very clever Scala tricks so

00:30:26,429 --> 00:30:30,419
that when you match you match to get the

00:30:28,350 --> 00:30:31,799
results of the Dom they had parsed you

00:30:30,419 --> 00:30:33,539
actually matched against integers

00:30:31,799 --> 00:30:35,370
instead classes to try to avoid this

00:30:33,539 --> 00:30:37,289
turns out that we don't have to worry

00:30:35,370 --> 00:30:41,220
about this this is something that we can

00:30:37,289 --> 00:30:43,169
just blithely ignore it's great JVM team

00:30:41,220 --> 00:30:44,760
did a really good job is instance of now

00:30:43,169 --> 00:30:48,389
runs almost as fast as the integer

00:30:44,760 --> 00:30:49,710
comparison so any Scala library where

00:30:48,389 --> 00:30:51,690
you've done some tricks to match against

00:30:49,710 --> 00:30:53,940
objects or integers instead of matching

00:30:51,690 --> 00:30:55,260
to the classes you don't have to do that

00:30:53,940 --> 00:30:57,269
anymore you can just matching his class

00:30:55,260 --> 00:31:00,149
be very naive about it I produces

00:30:57,269 --> 00:31:01,590
cleaner code it'll run just as fast and

00:31:00,149 --> 00:31:04,679
this is something I would not have

00:31:01,590 --> 00:31:07,289
thought at all and I was very surprised

00:31:04,679 --> 00:31:09,450
by this so this is why we benchmark

00:31:07,289 --> 00:31:11,140
because we learn things that we we

00:31:09,450 --> 00:31:13,330
thought had been dogma for you

00:31:11,140 --> 00:31:17,230
on the case of businesses of that have

00:31:13,330 --> 00:31:20,470
been dogma for 15 years for me anyway

00:31:17,230 --> 00:31:21,970
the so context switching we found a good

00:31:20,470 --> 00:31:24,100
number from that from the Intel website

00:31:21,970 --> 00:31:26,650
the other surprising thing for us is

00:31:24,100 --> 00:31:28,330
that list is glacially slow there's only

00:31:26,650 --> 00:31:29,890
one use case where you should ever use

00:31:28,330 --> 00:31:32,049
lists and we'll talk about that in a

00:31:29,890 --> 00:31:32,740
second but don't use lists don't use

00:31:32,049 --> 00:31:34,900
seek

00:31:32,740 --> 00:31:36,549
they're very very slow you can use them

00:31:34,900 --> 00:31:37,480
if the data structure small compares it

00:31:36,549 --> 00:31:39,280
doesn't matter so I can impact

00:31:37,480 --> 00:31:41,920
performance but if you're holding more

00:31:39,280 --> 00:31:44,500
than literally 50 items don't do it

00:31:41,920 --> 00:31:46,720
that's literally the cutoff where list

00:31:44,500 --> 00:31:49,780
starts really trailing off from the

00:31:46,720 --> 00:31:51,220
performance of array in that so we're

00:31:49,780 --> 00:31:53,590
going to dive a little more deeply into

00:31:51,220 --> 00:31:55,030
three of these benchmarks the first one

00:31:53,590 --> 00:31:57,760
we're going to dive more deeply into is

00:31:55,030 --> 00:31:59,260
the recursion and loops benchmark we

00:31:57,760 --> 00:32:02,470
were trying to compare different

00:31:59,260 --> 00:32:04,210
techniques for accomplishing looping

00:32:02,470 --> 00:32:05,919
that you have available to you in Scala

00:32:04,210 --> 00:32:09,520
and scholars of functional language you

00:32:05,919 --> 00:32:11,530
have a lot of them we use factorial is

00:32:09,520 --> 00:32:14,049
sort of our benchmark to test this stuff

00:32:11,530 --> 00:32:15,640
we benchmark loop for our full

00:32:14,049 --> 00:32:18,940
comprehensions recursion tail recursion

00:32:15,640 --> 00:32:20,830
and fold left and we expected these

00:32:18,940 --> 00:32:22,630
results to be roughly equivalent we were

00:32:20,830 --> 00:32:24,220
expected that pretty much all these

00:32:22,630 --> 00:32:27,460
techniques would run in roughly the same

00:32:24,220 --> 00:32:29,950
amount of time we discovered a few

00:32:27,460 --> 00:32:31,960
interesting things so the first is this

00:32:29,950 --> 00:32:34,120
is the code that we use to benchmark

00:32:31,960 --> 00:32:36,730
this stuff or we thought we were going

00:32:34,120 --> 00:32:38,500
to use to benchmark this stuff at the

00:32:36,730 --> 00:32:41,530
top you can see that app tram that's a

00:32:38,500 --> 00:32:43,299
jmh annotation that basically tells jmh

00:32:41,530 --> 00:32:45,760
to run this benchmark multiple times

00:32:43,299 --> 00:32:47,320
with different values for start start as

00:32:45,760 --> 00:32:51,220
simply the size of the problem we're

00:32:47,320 --> 00:32:53,380
working on in this case a benchmark

00:32:51,220 --> 00:32:55,990
indicates that this is a benchmark that

00:32:53,380 --> 00:33:00,100
we're going to run I tried to make the

00:32:55,990 --> 00:33:01,660
code dry so I created a sort of more

00:33:00,100 --> 00:33:04,360
generic implementation that they would

00:33:01,660 --> 00:33:08,080
all call and use the clever implicit

00:33:04,360 --> 00:33:10,900
integral and did a clever little fold

00:33:08,080 --> 00:33:12,820
left at the bottom there to do actually

00:33:10,900 --> 00:33:14,500
that we're actually measuring and I was

00:33:12,820 --> 00:33:16,059
very proud of myself and I thought this

00:33:14,500 --> 00:33:17,830
is very functional code and all the

00:33:16,059 --> 00:33:19,450
folks of Scala days will really be

00:33:17,830 --> 00:33:23,110
impressed by it and unfortunately it's

00:33:19,450 --> 00:33:24,370
wrong I'm sorry

00:33:23,110 --> 00:33:27,070
turns out

00:33:24,370 --> 00:33:30,160
that the Scala compiler has some

00:33:27,070 --> 00:33:31,809
performance bottlenecks in it it turns

00:33:30,160 --> 00:33:33,280
out that maybe someday this would be

00:33:31,809 --> 00:33:38,380
fixed I really hope that they fix this

00:33:33,280 --> 00:33:39,970
stuff but right now when you in integral

00:33:38,380 --> 00:33:42,160
is actually a subclass numeric so this

00:33:39,970 --> 00:33:44,770
applies to numeric and integral when you

00:33:42,160 --> 00:33:48,850
import them in this style and you use

00:33:44,770 --> 00:33:50,950
things like zero in one and plus these

00:33:48,850 --> 00:33:52,870
are the Scala compiler has not figured

00:33:50,950 --> 00:33:55,870
out how to get rid of all the auto

00:33:52,870 --> 00:33:57,370
boxing that comes along with that and so

00:33:55,870 --> 00:34:00,820
it turns out that the right way to do it

00:33:57,370 --> 00:34:03,610
is this which is even shorter it's much

00:34:00,820 --> 00:34:07,300
less dry I basically just had to write

00:34:03,610 --> 00:34:09,760
it four different ways and you can see

00:34:07,300 --> 00:34:11,649
it's basically doing the same thing and

00:34:09,760 --> 00:34:16,120
this is this is what we use to benchmark

00:34:11,649 --> 00:34:17,710
things and so this is what we found we

00:34:16,120 --> 00:34:20,830
found that for and while or basically

00:34:17,710 --> 00:34:23,169
the same speed so for you out there that

00:34:20,830 --> 00:34:25,750
value functional or of pure functional

00:34:23,169 --> 00:34:27,970
I'm sure in this room that's eighty

00:34:25,750 --> 00:34:29,169
ninety percent of you go ahead and use

00:34:27,970 --> 00:34:30,940
your for comprehension that's what you

00:34:29,169 --> 00:34:33,129
want to use that's going to be the

00:34:30,940 --> 00:34:35,290
fastest so there's only again all this

00:34:33,129 --> 00:34:37,120
only applies to performance critical

00:34:35,290 --> 00:34:40,450
sections of your application rest of it

00:34:37,120 --> 00:34:44,980
do whatever you want and so for and

00:34:40,450 --> 00:34:46,570
while roughly the same speed for it when

00:34:44,980 --> 00:34:48,669
you get to lots and lots of iterations

00:34:46,570 --> 00:34:50,950
while we'll slowly pull away from four

00:34:48,669 --> 00:34:53,649
so if you're iterating millions of times

00:34:50,950 --> 00:34:55,240
and maybe you want to use awhile but

00:34:53,649 --> 00:34:58,030
generally those are the two fastest

00:34:55,240 --> 00:34:59,710
things we were surprised that recurs

00:34:58,030 --> 00:35:01,150
that tail recursion was so slow we

00:34:59,710 --> 00:35:02,320
thought tail recursion was going to run

00:35:01,150 --> 00:35:04,780
at the same speed at support

00:35:02,320 --> 00:35:06,280
comprehension turns out that that's not

00:35:04,780 --> 00:35:08,740
really the case it is significantly

00:35:06,280 --> 00:35:12,820
slower after some very strange reason

00:35:08,740 --> 00:35:13,960
fold left was the slowest of all maybe

00:35:12,820 --> 00:35:16,750
someone can find a bug in my

00:35:13,960 --> 00:35:19,240
benchmarking code I am not certain why

00:35:16,750 --> 00:35:20,950
that would be the case it compiled down

00:35:19,240 --> 00:35:22,630
ultimately at the bottom it calls back

00:35:20,950 --> 00:35:26,050
to a while loop so it should be the same

00:35:22,630 --> 00:35:27,070
speed as a while loop the second

00:35:26,050 --> 00:35:28,600
benchmark that we're going to go a

00:35:27,070 --> 00:35:30,730
little deeper into is the collections

00:35:28,600 --> 00:35:32,560
benchmark in this case what we were

00:35:30,730 --> 00:35:36,460
trying to do is measure the speed of

00:35:32,560 --> 00:35:38,200
finding an item in a large collection so

00:35:36,460 --> 00:35:39,820
we measured two different ways to

00:35:38,200 --> 00:35:41,500
searching the collections we use the

00:35:39,820 --> 00:35:44,849
while loop we use defined with the zip

00:35:41,500 --> 00:35:48,520
with index for the map we used to get

00:35:44,849 --> 00:35:49,990
map is sort of cheating we wanted to see

00:35:48,520 --> 00:35:52,150
map against the the other data

00:35:49,990 --> 00:35:54,550
structures though we use different types

00:35:52,150 --> 00:35:59,950
and we use the immutable variants of all

00:35:54,550 --> 00:36:01,210
of these basic Scala collections and

00:35:59,950 --> 00:36:03,460
this is the benchmarking code that we

00:36:01,210 --> 00:36:07,780
use this is a chunk of it this

00:36:03,460 --> 00:36:11,460
particular chunk does be defined with

00:36:07,780 --> 00:36:20,349
the zip with index it's doing it for

00:36:11,460 --> 00:36:22,660
seeks I believe yes and you can download

00:36:20,349 --> 00:36:25,960
all this code on github and find all the

00:36:22,660 --> 00:36:28,960
things that I did wrong I was a joke

00:36:25,960 --> 00:36:30,940
anyway so after we measured it we found

00:36:28,960 --> 00:36:34,839
this we found that list was very very

00:36:30,940 --> 00:36:36,880
slow array was far less slow and map not

00:36:34,839 --> 00:36:38,920
surprisingly with the winner but what

00:36:36,880 --> 00:36:42,400
was interesting to us is that the the

00:36:38,920 --> 00:36:46,390
difference started really early on even

00:36:42,400 --> 00:36:51,310
it's 60 item 64 items the difference was

00:36:46,390 --> 00:36:52,810
an order of magnitude already so the

00:36:51,310 --> 00:36:54,940
break-even number that I've been using

00:36:52,810 --> 00:36:56,829
for when you want to Swit when you want

00:36:54,940 --> 00:36:59,050
to basically ban the use of list was

00:36:56,829 --> 00:37:02,140
about 50 so anytime you get past about

00:36:59,050 --> 00:37:03,730
50 don't use list don't you see at least

00:37:02,140 --> 00:37:06,569
if hits in performance-critical code

00:37:03,730 --> 00:37:09,819
it's not in performance-critical code in

00:37:06,569 --> 00:37:10,869
another and the third and final

00:37:09,819 --> 00:37:13,990
benchmark that we're going to dive a

00:37:10,869 --> 00:37:15,819
little more deeply into is mutability in

00:37:13,990 --> 00:37:17,859
this particular case the task was to

00:37:15,819 --> 00:37:19,569
insert delete or read from a data

00:37:17,859 --> 00:37:21,819
structure we wanted to compare the

00:37:19,569 --> 00:37:25,119
immutable variants to their mutable

00:37:21,819 --> 00:37:27,369
variants we saw the array and map were

00:37:25,119 --> 00:37:29,859
the best from the previous benchmark so

00:37:27,369 --> 00:37:33,069
we doing the mutable and immutable

00:37:29,859 --> 00:37:36,430
variants of array mutable buffer which

00:37:33,069 --> 00:37:37,839
is the global or shrinkable x' array

00:37:36,430 --> 00:37:40,329
thing in scala

00:37:37,839 --> 00:37:43,690
and immutable index weak and we included

00:37:40,329 --> 00:37:48,240
java concurrent hash map fun and also to

00:37:43,690 --> 00:37:51,069
sort of see how locking compares to copy

00:37:48,240 --> 00:37:52,180
so this is a piece of the benchmarking

00:37:51,069 --> 00:37:55,299
code we used

00:37:52,180 --> 00:37:58,150
the weird thing that's that you might

00:37:55,299 --> 00:38:00,789
see up here is that BH : black hole part

00:37:58,150 --> 00:38:02,170
what that is is that's jmh data

00:38:00,789 --> 00:38:04,239
structure that basically tells the

00:38:02,170 --> 00:38:06,699
compiler not to throw away the results

00:38:04,239 --> 00:38:08,469
of this operation so it turns out that

00:38:06,699 --> 00:38:11,079
if you were to run this without that the

00:38:08,469 --> 00:38:13,779
hot spot and it's brand wisdom will

00:38:11,079 --> 00:38:16,390
actually remove your entire read

00:38:13,779 --> 00:38:20,309
operation it will run very fast you get

00:38:16,390 --> 00:38:23,380
very exciting numbers but very wrong

00:38:20,309 --> 00:38:25,150
anyway so these are the results that we

00:38:23,380 --> 00:38:26,829
got so the first thing that was

00:38:25,150 --> 00:38:28,599
surprising is that a mutable map is very

00:38:26,829 --> 00:38:30,699
slow and we're not that surprised that

00:38:28,599 --> 00:38:32,739
it's very slow at inserting or removing

00:38:30,699 --> 00:38:34,269
I mean that's sort of the cost that you

00:38:32,739 --> 00:38:37,029
would pay for using a mutable data

00:38:34,269 --> 00:38:39,749
structure but it's reads we're very slow

00:38:37,029 --> 00:38:42,160
to which was very surprising to us

00:38:39,749 --> 00:38:44,289
compaction is a big problem for arrays

00:38:42,160 --> 00:38:47,259
so the only time that you should ever

00:38:44,289 --> 00:38:48,699
use a big list or a list with a lot of

00:38:47,259 --> 00:38:51,609
items in it is when you're going to do a

00:38:48,699 --> 00:38:52,989
lot of deletes and these aren't and

00:38:51,609 --> 00:38:54,819
you're going to do a lot of deletes that

00:38:52,989 --> 00:38:57,880
aren't from the head or the tail

00:38:54,819 --> 00:39:01,150
especially the tail tail deletes are not

00:38:57,880 --> 00:39:03,400
expensive at all for arrays or data

00:39:01,150 --> 00:39:04,839
structures based on arrays because it's

00:39:03,400 --> 00:39:07,479
not going to do a copy all it's going to

00:39:04,839 --> 00:39:10,749
do is move to the capacity integer down

00:39:07,479 --> 00:39:12,489
one or decrement the capacity number and

00:39:10,749 --> 00:39:13,809
so that's not a big deal but if you're

00:39:12,489 --> 00:39:15,160
removing them from the middle it's

00:39:13,809 --> 00:39:17,920
actually going to copy all those

00:39:15,160 --> 00:39:20,619
elements back down and that becomes a

00:39:17,920 --> 00:39:22,179
huge slowdown so only use lists when

00:39:20,619 --> 00:39:24,729
you're doing lots and lots of random

00:39:22,179 --> 00:39:26,739
deletes and the third thing it was

00:39:24,729 --> 00:39:31,089
interesting to us that reads are

00:39:26,739 --> 00:39:33,609
basically similar but mutable map is the

00:39:31,089 --> 00:39:36,069
fastest considerably faster than

00:39:33,609 --> 00:39:37,569
immutable map and Java concurrent

00:39:36,069 --> 00:39:39,819
hashmaps or splits the difference

00:39:37,569 --> 00:39:41,469
between the two which I found very

00:39:39,819 --> 00:39:43,989
interesting hopefully the Scala team

00:39:41,469 --> 00:39:45,670
will improve the performance of the

00:39:43,989 --> 00:39:47,880
containers in the future I'm certain

00:39:45,670 --> 00:39:51,249
that they can they're very bright people

00:39:47,880 --> 00:39:53,199
and I there's really good tools on the

00:39:51,249 --> 00:39:56,140
JVM for profiling and optimizing these

00:39:53,199 --> 00:39:57,609
things and I think in future releases a

00:39:56,140 --> 00:39:59,949
scholar they'll probably be able to

00:39:57,609 --> 00:40:01,239
correct some of these performance

00:39:59,949 --> 00:40:04,239
problems and some of the immutable data

00:40:01,239 --> 00:40:05,860
structures somebody came up to us and

00:40:04,239 --> 00:40:07,270
said hey there's 212 you know

00:40:05,860 --> 00:40:09,760
211 is very old maybe you're doing

00:40:07,270 --> 00:40:14,230
something wrong so we did 212 and we

00:40:09,760 --> 00:40:16,000
found the same thing anyway so hopefully

00:40:14,230 --> 00:40:18,640
maybe in 230 or 240 mail they'll

00:40:16,000 --> 00:40:20,380
optimize these things so how does all of

00:40:18,640 --> 00:40:23,620
this knowledge help us design software

00:40:20,380 --> 00:40:25,780
well the first thing to know is are you

00:40:23,620 --> 00:40:28,720
writing a piece of code it's designed to

00:40:25,780 --> 00:40:30,610
be high throughput or low latency after

00:40:28,720 --> 00:40:34,060
you identify that you only want to

00:40:30,610 --> 00:40:37,630
optimize and profile inner loops you

00:40:34,060 --> 00:40:39,430
want to look at your disk usage your CPU

00:40:37,630 --> 00:40:41,890
usage and your network usage to figure

00:40:39,430 --> 00:40:44,260
out which one of those is is is the

00:40:41,890 --> 00:40:46,360
highest that's that will tell you which

00:40:44,260 --> 00:40:49,180
kinds of operations are likely causing

00:40:46,360 --> 00:40:51,280
this system slow down or not running as

00:40:49,180 --> 00:40:52,930
fast as it could and then finally when

00:40:51,280 --> 00:40:55,990
you identify those slowest most

00:40:52,930 --> 00:40:58,210
expensive operations you want to reduce

00:40:55,990 --> 00:41:02,350
cache delay or eliminate those expensive

00:40:58,210 --> 00:41:03,880
costs so thank you to the Credit Karma

00:41:02,350 --> 00:41:06,700
folks that gave me so much support and

00:41:03,880 --> 00:41:07,540
the Scala day staff for being such

00:41:06,700 --> 00:41:15,940
wonderful hubs

00:41:07,540 --> 00:41:17,950
thank you question so the question was

00:41:15,940 --> 00:41:20,650
do you have any ideas why a mutable map

00:41:17,950 --> 00:41:23,500
is so much slower no I really don't I

00:41:20,650 --> 00:41:25,930
haven't profiled a mutable map really

00:41:23,500 --> 00:41:30,430
deeply to try to figure out what might

00:41:25,930 --> 00:41:34,030
be the source of these issues hi

00:41:30,430 --> 00:41:36,040
if I had to guess it's because they say

00:41:34,030 --> 00:41:37,780
they tried to make some the basic

00:41:36,040 --> 00:41:41,200
storage container being in that pair

00:41:37,780 --> 00:41:43,150
that couple objects do they use if I had

00:41:41,200 --> 00:41:44,950
to blame something that's what I would

00:41:43,150 --> 00:41:49,630
blame that that's purely an intuitive

00:41:44,950 --> 00:41:50,680
guest back I know no hard facts if

00:41:49,630 --> 00:41:51,580
somebody knows the answers a question

00:41:50,680 --> 00:41:54,730
I'm all yours

00:41:51,580 --> 00:41:58,450
what you said whis and sheikhs did that

00:41:54,730 --> 00:42:00,400
include vector uh anything that's array

00:41:58,450 --> 00:42:02,290
is what you want to go for and anything

00:42:00,400 --> 00:42:07,360
that's based off the linked list style

00:42:02,290 --> 00:42:09,910
list you want to avoid I'm not very good

00:42:07,360 --> 00:42:13,020
at remembering what mass to what I tend

00:42:09,910 --> 00:42:15,760
to just use index seek everywhere oh

00:42:13,020 --> 00:42:17,980
yeah if you look at my github you'll

00:42:15,760 --> 00:42:19,790
you'll you'll see that I actually rehab

00:42:17,980 --> 00:42:21,740
a compiler plug-in recently I did

00:42:19,790 --> 00:42:23,390
you make heavily use of lists but I was

00:42:21,740 --> 00:42:26,690
following the pattern of the original

00:42:23,390 --> 00:42:29,300
Lightman developer there and I was going

00:42:26,690 --> 00:42:30,530
for loose change so I'm not being a

00:42:29,300 --> 00:42:34,310
hypocrite if somebody wants to go my

00:42:30,530 --> 00:42:36,530
github list is really great when you're

00:42:34,310 --> 00:42:37,940
doing lots of deletes and this code was

00:42:36,530 --> 00:42:41,000
doing a lot of recursion and a lot of

00:42:37,940 --> 00:42:42,800
deletes it sometimes it makes sense

00:42:41,000 --> 00:42:46,250
there are cases where lists make sense

00:42:42,800 --> 00:42:48,130
that's absolutely true but most of the

00:42:46,250 --> 00:42:50,470
time when you're in that inner loop

00:42:48,130 --> 00:42:52,910
you're going to want to use an array

00:42:50,470 --> 00:42:55,610
unless like I said you're doing lots and

00:42:52,910 --> 00:42:57,500
lots of random delete if you have

00:42:55,610 --> 00:43:02,060
stackers you like functionality array is

00:42:57,500 --> 00:43:04,250
a really great backer for that so that's

00:43:02,060 --> 00:43:05,960
the question so the question was since

00:43:04,250 --> 00:43:08,860
we're on the JVM with garbage collection

00:43:05,960 --> 00:43:11,750
how do we deal with that impact on on

00:43:08,860 --> 00:43:14,540
latency um the short answer for that is

00:43:11,750 --> 00:43:16,160
that malloc is very expensive and the

00:43:14,540 --> 00:43:18,790
truth is the things that lead to

00:43:16,160 --> 00:43:21,800
performance bottlenecks high latency

00:43:18,790 --> 00:43:24,200
slow throughput are roughly the same

00:43:21,800 --> 00:43:26,840
thing two leads to garbage collection

00:43:24,200 --> 00:43:28,550
taking a long time if you properly

00:43:26,840 --> 00:43:30,260
optimize your application garbage

00:43:28,550 --> 00:43:32,750
collection should not take a long amount

00:43:30,260 --> 00:43:34,870
of time the other thing is that garbage

00:43:32,750 --> 00:43:36,680
collection is sort of like that one of

00:43:34,870 --> 00:43:38,510
applications talked about when we were

00:43:36,680 --> 00:43:42,650
talking about batching the database

00:43:38,510 --> 00:43:43,790
writes and when when we're talking about

00:43:42,650 --> 00:43:45,710
that we were basically saying that

00:43:43,790 --> 00:43:47,480
there's one user out of 100 or 1 user

00:43:45,710 --> 00:43:48,010
out of 500 it's just going to take one

00:43:47,480 --> 00:43:50,600
for the team

00:43:48,010 --> 00:43:53,720
they're their requests just going slower

00:43:50,600 --> 00:43:56,000
than everyone else's and that's probably

00:43:53,720 --> 00:43:58,100
a good trade-off most the time the GC is

00:43:56,000 --> 00:44:00,320
basically the same thing somebody's

00:43:58,100 --> 00:44:02,630
going to get unlucky the way Google

00:44:00,320 --> 00:44:03,800
deals with that just for reference is

00:44:02,630 --> 00:44:06,650
they do everything twice

00:44:03,800 --> 00:44:08,600
and so it's instead of only one of out

00:44:06,650 --> 00:44:10,960
of every 500 getting unlucky you'd

00:44:08,600 --> 00:44:15,380
actually have to get one out of every

00:44:10,960 --> 00:44:21,320
was that 250 thousand five hundred

00:44:15,380 --> 00:44:23,270
squared so yeah if you if you did every

00:44:21,320 --> 00:44:25,550
operation twice then you would you would

00:44:23,270 --> 00:44:27,500
reduce that by a massive amount but even

00:44:25,550 --> 00:44:30,020
at Google there's that one user out of

00:44:27,500 --> 00:44:31,910
four users out of every million to just

00:44:30,020 --> 00:44:32,960
get unlucky and their request is going

00:44:31,910 --> 00:44:33,660
to be a little slower because they hit a

00:44:32,960 --> 00:44:35,580
GC

00:44:33,660 --> 00:44:37,110
but if you did your application right

00:44:35,580 --> 00:44:39,570
that extra overhead is like 30

00:44:37,110 --> 00:44:43,680
milliseconds right if you're getting

00:44:39,570 --> 00:44:47,910
multiple second garbage collections you

00:44:43,680 --> 00:44:49,920
need to work on that and you're probably

00:44:47,910 --> 00:44:52,220
allocating way too much memory yes

00:44:49,920 --> 00:44:52,220
please

00:44:52,320 --> 00:44:59,130
so the easiest answer to the first place

00:44:55,410 --> 00:45:01,610
to look is always run a profiler the JVM

00:44:59,130 --> 00:45:03,780
has a really excellent profiler API

00:45:01,610 --> 00:45:05,550
there's a lot of good tools that make

00:45:03,780 --> 00:45:07,560
use of it some of them are free and

00:45:05,550 --> 00:45:09,660
packaged with the JVM so that's what I

00:45:07,560 --> 00:45:11,400
would start right now evilly not knowing

00:45:09,660 --> 00:45:13,590
anything else about the application the

00:45:11,400 --> 00:45:14,880
other thing I would say is that the kind

00:45:13,590 --> 00:45:16,140
of pipeline that you were describing you

00:45:14,880 --> 00:45:17,940
would have four or five nodes and

00:45:16,140 --> 00:45:19,140
together strung together I was really

00:45:17,940 --> 00:45:21,540
talking about four or five applications

00:45:19,140 --> 00:45:24,270
strung together but you're the same

00:45:21,540 --> 00:45:27,480
concepts and same things still apply so

00:45:24,270 --> 00:45:29,220
in each of those individual nodes in a

00:45:27,480 --> 00:45:32,250
pipeline would still have its own inner

00:45:29,220 --> 00:45:35,730
loop right it's thing where it's doing

00:45:32,250 --> 00:45:36,750
its request in a database kernel what

00:45:35,730 --> 00:45:38,640
you would see there is you'd have

00:45:36,750 --> 00:45:40,710
several EXO's they're called EXO's and

00:45:38,640 --> 00:45:42,240
database parlance but they're the same

00:45:40,710 --> 00:45:44,160
thing that you're describing and they

00:45:42,240 --> 00:45:46,470
have buffers in between them and they

00:45:44,160 --> 00:45:48,950
would each when a so it's called it

00:45:46,470 --> 00:45:52,500
simply runs until its upper is empty and

00:45:48,950 --> 00:45:56,130
then then the next one gets scheduled I

00:45:52,500 --> 00:45:59,400
think similarly you would probably see

00:45:56,130 --> 00:46:00,750
the same sorts of behavior and what

00:45:59,400 --> 00:46:02,880
you're describing as in a database

00:46:00,750 --> 00:46:04,440
kernel where one of the EXO's would run

00:46:02,880 --> 00:46:07,290
much slower than the rest of them and

00:46:04,440 --> 00:46:12,390
you would find that EXO by the fact that

00:46:07,290 --> 00:46:13,740
its input buffer was always full so it

00:46:12,390 --> 00:46:15,630
would be the same sort of thing what

00:46:13,740 --> 00:46:19,080
you're describing you would look for the

00:46:15,630 --> 00:46:22,020
guy who's called the least and the next

00:46:19,080 --> 00:46:24,300
one downstream is likely the problem you

00:46:22,020 --> 00:46:28,290
may claim earlier about a non-blocking

00:46:24,300 --> 00:46:30,990
i/o and and how only if CPU is a

00:46:28,290 --> 00:46:33,990
bottleneck would it impact latency slash

00:46:30,990 --> 00:46:36,390
throughput correct ah but I guess

00:46:33,990 --> 00:46:38,430
frameworks like play use akka and nettie

00:46:36,390 --> 00:46:40,440
precisely for the reason that they can

00:46:38,430 --> 00:46:43,590
free up those thread pools with not

00:46:40,440 --> 00:46:47,460
rocking i/o sure which in turn has an

00:46:43,590 --> 00:46:50,340
implication on on latency slash through

00:46:47,460 --> 00:46:52,290
but it does so so is this a

00:46:50,340 --> 00:46:54,360
contradiction to what you said if so yes

00:46:52,290 --> 00:46:56,940
if not why not no it's not a

00:46:54,360 --> 00:46:59,670
contradiction actually what I said is

00:46:56,940 --> 00:47:03,360
that if if network IO is what your maxed

00:46:59,670 --> 00:47:04,890
out at and you apply asynchronous you're

00:47:03,360 --> 00:47:06,090
not going to improve your application

00:47:04,890 --> 00:47:09,450
because what it's going to do is reduce

00:47:06,090 --> 00:47:11,520
the CPU right you haven't actually saved

00:47:09,450 --> 00:47:14,760
what's going on in the network you

00:47:11,520 --> 00:47:16,710
already had 75% your CPU always empty so

00:47:14,760 --> 00:47:19,440
every time a request came in and mediate

00:47:16,710 --> 00:47:20,970
some CPU time it was always available so

00:47:19,440 --> 00:47:23,160
that's why it doesn't have an impact on

00:47:20,970 --> 00:47:25,830
latency in that particular case that

00:47:23,160 --> 00:47:28,050
being said if you're doing the number of

00:47:25,830 --> 00:47:29,370
i/o requests that that you're doing in

00:47:28,050 --> 00:47:31,980
some of these applications that you

00:47:29,370 --> 00:47:34,800
described they may have a couple

00:47:31,980 --> 00:47:36,900
thousand concurrent writers and that

00:47:34,800 --> 00:47:39,030
might easily overwhelm the CPU the

00:47:36,900 --> 00:47:41,790
overhead that they induce my easily

00:47:39,030 --> 00:47:44,100
overwhelming CPU so in their use cases

00:47:41,790 --> 00:47:45,900
they would want that in addition since

00:47:44,100 --> 00:47:47,010
most the time they're acting as proxies

00:47:45,900 --> 00:47:48,480
in a lot of cases they're acting as

00:47:47,010 --> 00:47:50,250
proxies and sitting on the same machine

00:47:48,480 --> 00:47:52,710
they want to be as efficient as possible

00:47:50,250 --> 00:47:54,480
anyway plus the fact that there are

00:47:52,710 --> 00:47:55,950
third-party applications sort of talking

00:47:54,480 --> 00:47:57,780
about like I'm better than the

00:47:55,950 --> 00:47:59,760
implementation you can do yourself you

00:47:57,780 --> 00:48:04,230
really should do this stuff correctly

00:47:59,760 --> 00:48:05,490
anyway because in their use case where

00:48:04,230 --> 00:48:08,190
they originally wrote this for maybe

00:48:05,490 --> 00:48:09,990
that box was mostly CPU free in which

00:48:08,190 --> 00:48:11,850
case they didn't need to do their

00:48:09,990 --> 00:48:14,100
asynchronous optimization however

00:48:11,850 --> 00:48:15,660
there's use cases that those of us

00:48:14,100 --> 00:48:18,450
probably even in this room have used

00:48:15,660 --> 00:48:19,800
where they needed that CPU free and so

00:48:18,450 --> 00:48:23,450
that's why they would have done it in

00:48:19,800 --> 00:48:23,450
those libraries and those applications

00:48:24,440 --> 00:48:28,710
it's much harder code to write I

00:48:26,370 --> 00:48:29,940
personally prefer writing asynchronous

00:48:28,710 --> 00:48:31,890
i/o especially on the network it's

00:48:29,940 --> 00:48:34,560
actually easy to write in some cases but

00:48:31,890 --> 00:48:37,710
the bugs are so much more subtle and so

00:48:34,560 --> 00:48:39,240
much more difficult to find and and

00:48:37,710 --> 00:48:40,920
really you know this gray hair is

00:48:39,240 --> 00:48:44,330
probably from one of those and this

00:48:40,920 --> 00:48:49,010
one's function difference there you go

00:48:44,330 --> 00:48:54,519
did it alright thank you everybody

00:48:49,010 --> 00:48:54,519

YouTube URL: https://www.youtube.com/watch?v=DXFUn90hudk


