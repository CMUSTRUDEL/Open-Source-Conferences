Title: Extending Spark Machine Learning   Beyond Linear Regression   by Holden Karau
Publication date: 2017-06-07
Playlist: Scala Days 2017
Description: 
	This talk was recorded at Scala Days Chicago, 2017. Follow along on Twitter @scaladays and on the website for more information http://scaladays.org/.

Abstract:
Apache Spark is one of the most popular Big Data tools. This talk introduces & takes a deep dive on Spark's scikit-learn inspired Machine Learning pipelines.

Spark's ML pipelines provide a lot of power, but sometimes the tools you need for your specific problem aren't available yet. By integrating your own data preparation and machine learning tools into Spark's ML pipelines you will be able to take advantage of useful meta-algorithms, like parameter searching.

Even if you don't have your own machine learning algorithms you want to implement, this talk peels back the covers on how the ML APIs are built and can help you make even more awesome ML pipelines and customize Spark models for your needs.

A basic understanding of Spark will make it easier to follow along, but if this is your first Spark talk, this will still be useful and give you a broad understanding of how Spark ML functions (of course since the presenter is an author, if this is your first introduction to Spark she encourages you to buy her book "Learning Spark" & "High Performance Spark").
Captions: 
	00:00:00,000 --> 00:00:04,339
um I guess we're going to get started

00:00:01,829 --> 00:00:07,890
since I think it's supposed to start at

00:00:04,339 --> 00:00:09,900
11:35 and if anyone wants to show up

00:00:07,890 --> 00:00:11,809
they can just trickle in the first few

00:00:09,900 --> 00:00:15,719
slides are not that exciting anyways

00:00:11,809 --> 00:00:20,939
sorry everyone that got here early so

00:00:15,719 --> 00:00:23,039
yeah this is extending spark m.l.a in

00:00:20,939 --> 00:00:24,900
spark we can only do this in Scala right

00:00:23,039 --> 00:00:28,019
now but I figure for a skylit conference

00:00:24,900 --> 00:00:30,119
I don't have to apologize that much if

00:00:28,019 --> 00:00:31,650
there's any secret Python users hanging

00:00:30,119 --> 00:00:34,260
out in the room I'm sorry

00:00:31,650 --> 00:00:38,190
it doesn't work in Python yet for

00:00:34,260 --> 00:00:41,969
reasons which we can talk about over at

00:00:38,190 --> 00:00:44,100
least two shots of whiskey so I'm

00:00:41,969 --> 00:00:45,719
holding my preferred pronouns are she or

00:00:44,100 --> 00:00:49,980
her it's tattooed on my wrist in case

00:00:45,719 --> 00:00:51,960
you forget so try not forget and I'm a

00:00:49,980 --> 00:00:54,629
principal software engineer at IBM spar

00:00:51,960 --> 00:00:56,129
technology Center in San Francisco it's

00:00:54,629 --> 00:00:59,309
really nice they pay me to work on open

00:00:56,129 --> 00:01:02,160
source software and I'm sure they get

00:00:59,309 --> 00:01:04,080
some value from it but I'm mostly

00:01:02,160 --> 00:01:06,119
excited I get to write code and I miss

00:01:04,080 --> 00:01:08,670
for a committer as of this January which

00:01:06,119 --> 00:01:10,260
is really exciting to me because it

00:01:08,670 --> 00:01:11,670
means I can merge my own code some of

00:01:10,260 --> 00:01:14,430
the times without people like

00:01:11,670 --> 00:01:17,640
necessarily noticing and I can do evil

00:01:14,430 --> 00:01:20,900
things but of course evil things in the

00:01:17,640 --> 00:01:22,860
patchy way so always always on brand

00:01:20,900 --> 00:01:25,229
previously I've been at a bunch of other

00:01:22,860 --> 00:01:28,890
companies besides IBM working on sort of

00:01:25,229 --> 00:01:31,470
similar problems I've worked with SPARC

00:01:28,890 --> 00:01:35,009
and many of them but also like Google

00:01:31,470 --> 00:01:36,509
and Foursquare we use different tools if

00:01:35,009 --> 00:01:38,430
you want to follow me on Twitter for a

00:01:36,509 --> 00:01:40,320
Canadian who's kind of really confused

00:01:38,430 --> 00:01:43,290
with why she decided to live in America

00:01:40,320 --> 00:01:45,600
you can find me it's at Holden Cairo I

00:01:43,290 --> 00:01:48,810
do occasionally tweet about computers

00:01:45,600 --> 00:01:51,689
but right now it's mostly sadness I've

00:01:48,810 --> 00:01:53,280
got some slides there on SlideShare the

00:01:51,689 --> 00:01:55,049
slides from today's talk will probably

00:01:53,280 --> 00:01:57,810
go on SlideShare unless I get really

00:01:55,049 --> 00:02:00,420
drunk in which case we'll go on

00:01:57,810 --> 00:02:01,350
SlideShare when someone reminds me if we

00:02:00,420 --> 00:02:03,299
want to pretend to be business

00:02:01,350 --> 00:02:07,890
professionals you can find me on

00:02:03,299 --> 00:02:10,319
LinkedIn and tell me what it's for and

00:02:07,890 --> 00:02:13,379
github has the code examples

00:02:10,319 --> 00:02:15,439
talk the rest of them are and other

00:02:13,379 --> 00:02:19,469
github repo that I have but whatever

00:02:15,439 --> 00:02:21,749
it's pretty funny and like I'm trans and

00:02:19,469 --> 00:02:23,760
queer and Canadian and I consider myself

00:02:21,749 --> 00:02:25,560
a part of the leather community in fact

00:02:23,760 --> 00:02:28,829
I'm going to a lot our conference right

00:02:25,560 --> 00:02:30,299
after this conference and I mean it's

00:02:28,829 --> 00:02:32,310
not super relevant to writing software

00:02:30,299 --> 00:02:34,139
but I feel like a lot of people maybe

00:02:32,310 --> 00:02:36,299
don't know other people like me who are

00:02:34,139 --> 00:02:39,209
visible about who they are and like

00:02:36,299 --> 00:02:41,579
hello we're not maybe as scary as

00:02:39,209 --> 00:02:43,980
television sometimes implies you know I

00:02:41,579 --> 00:02:47,099
make the same stupid scholar mistakes

00:02:43,980 --> 00:02:52,379
that you make I don't have any secret

00:02:47,099 --> 00:02:54,540
powers somebody see oh yeah I'm also on

00:02:52,379 --> 00:03:00,030
an h-1b visa which feels amazing after

00:02:54,540 --> 00:03:02,040
the executive order um anyways um that

00:03:00,030 --> 00:03:04,680
slide is not necessarily endorsed by my

00:03:02,040 --> 00:03:06,689
employer etcetera etcetera but manner so

00:03:04,680 --> 00:03:08,939
my employer is amazing I love them they

00:03:06,689 --> 00:03:11,699
give me money and not only do they give

00:03:08,939 --> 00:03:13,620
me money they give about 25 other no 50

00:03:11,699 --> 00:03:16,349
other developers money to work on spark

00:03:13,620 --> 00:03:20,280
and spark related tools like system ml

00:03:16,349 --> 00:03:21,659
and Apache Tory and it's a lot of fun we

00:03:20,280 --> 00:03:24,840
hang out in San Francisco and re open

00:03:21,659 --> 00:03:26,090
source code we have this graph that goes

00:03:24,840 --> 00:03:29,189
up and to the right

00:03:26,090 --> 00:03:30,900
this graph is why I still get paid so

00:03:29,189 --> 00:03:33,989
that graph will keep going up and to the

00:03:30,900 --> 00:03:36,750
right for a long time or at least I hope

00:03:33,989 --> 00:03:38,519
so so we make a lot of contributions to

00:03:36,750 --> 00:03:40,709
spark and if you're ever considering

00:03:38,519 --> 00:03:44,480
buying something from IBM I don't know

00:03:40,709 --> 00:03:47,030
what it is we sell but please do buy it

00:03:44,480 --> 00:03:49,280
our last quarter was not that amazing I

00:03:47,030 --> 00:03:54,580
think is what the prop said so pleased

00:03:49,280 --> 00:03:57,260
by mainframes or surfaces I don't yeah

00:03:54,580 --> 00:03:59,209
right okay so you all laughed at my

00:03:57,260 --> 00:04:00,470
corny joke so you're clearly nice people

00:03:59,209 --> 00:04:02,930
so we don't have to we don't have to go

00:04:00,470 --> 00:04:05,000
a bad assumption if you object to

00:04:02,930 --> 00:04:07,459
pictures of cats this talk is not for

00:04:05,000 --> 00:04:10,550
you I'm sorry about that

00:04:07,459 --> 00:04:14,900
how many people have actively worked

00:04:10,550 --> 00:04:16,160
with Apache spark huh yeah okay okay and

00:04:14,900 --> 00:04:20,870
now for those of you that didn't raise

00:04:16,160 --> 00:04:22,490
your hand if you like are confident and

00:04:20,870 --> 00:04:24,980
raising your hand to say that you have

00:04:22,490 --> 00:04:26,960
not worked with spark are you willing to

00:04:24,980 --> 00:04:28,370
raise your hand oh wow okay that's still

00:04:26,960 --> 00:04:31,240
a good percentage of people that's

00:04:28,370 --> 00:04:34,550
pretty awesome so we're going to cover

00:04:31,240 --> 00:04:36,260
some of the basics of spark and

00:04:34,550 --> 00:04:38,860
hopefully this will be enough to trick

00:04:36,260 --> 00:04:41,690
you into using spark in your daily work

00:04:38,860 --> 00:04:44,000
this might not be enough to actually

00:04:41,690 --> 00:04:46,280
make you use spark into your daily work

00:04:44,000 --> 00:04:49,040
for that Paco has a really wonderful

00:04:46,280 --> 00:04:50,510
introduction video and I have a book I

00:04:49,040 --> 00:04:55,190
would like everyone to buy several

00:04:50,510 --> 00:05:00,610
copies of and who here is new to our

00:04:55,190 --> 00:05:03,440
community you just go ah yay hello um

00:05:00,610 --> 00:05:04,250
okay so if there's parts of the sides

00:05:03,440 --> 00:05:06,320
which don't make sense

00:05:04,250 --> 00:05:11,479
stop me and we can explain them and it's

00:05:06,320 --> 00:05:13,729
cool welcome to Scala happy fun okay um

00:05:11,479 --> 00:05:14,930
and yeah I've been hoping everyone's

00:05:13,729 --> 00:05:17,180
here because I think machine learning is

00:05:14,930 --> 00:05:19,690
kind of cool if you don't it's probably

00:05:17,180 --> 00:05:21,710
going to be kind of boring but whatever

00:05:19,690 --> 00:05:23,900
we're going to talk about the good stuff

00:05:21,710 --> 00:05:25,370
yeah I'm going to try and trick you into

00:05:23,900 --> 00:05:27,590
publishing your fancy new machine

00:05:25,370 --> 00:05:30,830
learning models on github so I can steal

00:05:27,590 --> 00:05:32,450
them and use them or collaborate with

00:05:30,830 --> 00:05:35,360
you it's not stealing if it's Apache

00:05:32,450 --> 00:05:37,669
licensed so for those people who are new

00:05:35,360 --> 00:05:39,710
to spark it's a general-purpose

00:05:37,669 --> 00:05:42,260
mistreated system it's written in Scala

00:05:39,710 --> 00:05:44,180
it has a really nice API even for the

00:05:42,260 --> 00:05:47,120
Python users we have a really nice API

00:05:44,180 --> 00:05:49,880
we have an AR API but really and nice

00:05:47,120 --> 00:05:53,479
are not adjectives I would describe for

00:05:49,880 --> 00:05:56,180
our API just yet but we're working on it

00:05:53,479 --> 00:05:58,009
so if you're if you're like interested

00:05:56,180 --> 00:06:00,080
in working in spark but not everyone in

00:05:58,009 --> 00:06:02,720
your company is like ready to use Scala

00:06:00,080 --> 00:06:05,150
you can still totally bring spark and

00:06:02,720 --> 00:06:07,009
it's a great Trojan horse for convincing

00:06:05,150 --> 00:06:09,050
your employer to use Scala because you

00:06:07,009 --> 00:06:11,419
can use it in Python but you can do so

00:06:09,050 --> 00:06:14,210
much more in Scala and it's a liquid way

00:06:11,419 --> 00:06:16,400
of tricking people into using Scala it's

00:06:14,210 --> 00:06:18,020
really active and Apache project a lot

00:06:16,400 --> 00:06:19,820
of people find it while they're waiting

00:06:18,020 --> 00:06:21,530
for their eight-hour MapReduce jobs to

00:06:19,820 --> 00:06:23,990
finish and they're like how can I make

00:06:21,530 --> 00:06:25,580
my MapReduce job faster and then they

00:06:23,990 --> 00:06:27,440
find spark and they're like I bet I can

00:06:25,580 --> 00:06:30,650
learn this by the time my MapReduce job

00:06:27,440 --> 00:06:34,099
is finished and sometimes that's true

00:06:30,650 --> 00:06:35,570
not always but at least sometimes the

00:06:34,099 --> 00:06:37,430
other way people find it is they're like

00:06:35,570 --> 00:06:38,360
why is Python running out of memory and

00:06:37,430 --> 00:06:45,229
then they're like oh okay I'll just

00:06:38,360 --> 00:06:47,539
repeat my computer yeah do you do SPARC

00:06:45,229 --> 00:06:49,160
datasets are also sometimes called our

00:06:47,539 --> 00:06:52,400
data frames and earlier versions of

00:06:49,160 --> 00:06:54,380
spark spark has two sort of abstractions

00:06:52,400 --> 00:06:55,639
we have this magical distributed

00:06:54,380 --> 00:06:57,289
collection called the resilient

00:06:55,639 --> 00:06:58,789
distributed data set that you can

00:06:57,289 --> 00:07:00,650
pretend is a magical distributed

00:06:58,789 --> 00:07:02,570
collection until it stops working and

00:07:00,650 --> 00:07:05,240
then you have to find out what the magic

00:07:02,570 --> 00:07:07,010
is and data sets and data frames are

00:07:05,240 --> 00:07:08,659
similar we can pretend they're this

00:07:07,010 --> 00:07:10,810
magical distributed collection that I

00:07:08,659 --> 00:07:13,669
can also run equal expressions on top of

00:07:10,810 --> 00:07:15,229
Intel it stops working and then we have

00:07:13,669 --> 00:07:17,750
to figure out what the details are but

00:07:15,229 --> 00:07:19,699
for the purposes of today's talks we can

00:07:17,750 --> 00:07:22,280
pretend that they're magical happy

00:07:19,699 --> 00:07:23,539
expression and I can run I'm sorry this

00:07:22,280 --> 00:07:25,520
isn't Python

00:07:23,539 --> 00:07:28,039
I can run both things in this like

00:07:25,520 --> 00:07:30,380
little DSL that we have on the DSL was

00:07:28,039 --> 00:07:32,810
the same for the most part between

00:07:30,380 --> 00:07:34,909
languages or we can specify lambda

00:07:32,810 --> 00:07:36,710
expressions or like arbitrary functions

00:07:34,909 --> 00:07:39,050
in Scala and run them over our

00:07:36,710 --> 00:07:41,270
distributed data and that's awesome it

00:07:39,050 --> 00:07:42,409
distributes both your data and the work

00:07:41,270 --> 00:07:44,930
that you're doing on your data across

00:07:42,409 --> 00:07:48,409
your cluster magically for you that's

00:07:44,930 --> 00:07:50,360
pretty cool so what is SPARC machine

00:07:48,409 --> 00:07:52,340
learning SPARC machine learning

00:07:50,360 --> 00:07:55,310
pipelines essentially looked at I could

00:07:52,340 --> 00:07:57,199
learn and one oh wow both people made a

00:07:55,310 --> 00:07:58,810
really good API I'm going to try and

00:07:57,199 --> 00:08:01,699
steal as much of it as I can

00:07:58,810 --> 00:08:03,150
and we didn't succeed in stealing all of

00:08:01,699 --> 00:08:05,810
it which is very

00:08:03,150 --> 00:08:09,960
but we stole a lot of their good ideas

00:08:05,810 --> 00:08:11,940
and so inspired our first attempt at

00:08:09,960 --> 00:08:14,250
machine learning was very much focused

00:08:11,940 --> 00:08:17,160
on just making like individual models

00:08:14,250 --> 00:08:19,020
and you you pass in this RDD which was

00:08:17,160 --> 00:08:21,840
already cleaned up and had all of your

00:08:19,020 --> 00:08:23,870
data like nicely organized and then you

00:08:21,840 --> 00:08:26,280
train your model on it and you went away

00:08:23,870 --> 00:08:27,990
but then it turns out that like that's

00:08:26,280 --> 00:08:29,970
not actually really how anyone to use

00:08:27,990 --> 00:08:32,010
the machine learning right everyone who

00:08:29,970 --> 00:08:33,870
does machine not everyone okay most

00:08:32,010 --> 00:08:36,390
people who do machine learning have

00:08:33,870 --> 00:08:38,310
messy data right like our labels are

00:08:36,390 --> 00:08:40,919
kind of crappy we've got a bunch of null

00:08:38,310 --> 00:08:42,570
columns we have like maybe text rule

00:08:40,919 --> 00:08:45,180
data in the middle of something for some

00:08:42,570 --> 00:08:47,940
reason and so we need to spend a lot of

00:08:45,180 --> 00:08:49,860
time cleaning our data and how we clean

00:08:47,940 --> 00:08:51,500
our data actually can really impact the

00:08:49,860 --> 00:08:54,839
later stages in our pipeline and so

00:08:51,500 --> 00:08:56,880
spark ml says like hey we're going to

00:08:54,839 --> 00:08:58,560
make this thing so that we can put

00:08:56,880 --> 00:09:01,380
together all of the things that you've

00:08:58,560 --> 00:09:03,390
been doing into a single pipeline and

00:09:01,380 --> 00:09:05,310
then we can we can do training on your

00:09:03,390 --> 00:09:07,410
pipeline and you don't have to worry

00:09:05,310 --> 00:09:09,120
about like doing a separate data

00:09:07,410 --> 00:09:12,870
cleaning phase we can do this as an

00:09:09,120 --> 00:09:15,870
integrated component and so that's kind

00:09:12,870 --> 00:09:17,700
of cool the pipeline stages are sort of

00:09:15,870 --> 00:09:21,000
broken into two categories

00:09:17,700 --> 00:09:22,800
there's estimators and transformers and

00:09:21,000 --> 00:09:25,380
estimators are things which have to see

00:09:22,800 --> 00:09:26,820
some data first so we have to train them

00:09:25,380 --> 00:09:29,130
so machine learning models are

00:09:26,820 --> 00:09:30,800
estimators but also things like a string

00:09:29,130 --> 00:09:33,570
and that sir tends to be an estimator

00:09:30,800 --> 00:09:35,750
because we want to make sure that we're

00:09:33,570 --> 00:09:37,920
indexing frequent strings to low numbers

00:09:35,750 --> 00:09:39,540
or essentially we want to make sure

00:09:37,920 --> 00:09:42,720
we're not indexing two strings to the

00:09:39,540 --> 00:09:45,570
same number right we can clearly do like

00:09:42,720 --> 00:09:48,360
hashing and and if we you a hashing

00:09:45,570 --> 00:09:50,250
thing that's clearly a transformer it

00:09:48,360 --> 00:09:52,529
doesn't need to see any data first we

00:09:50,250 --> 00:09:55,950
can just hash our inputs and things are

00:09:52,529 --> 00:09:59,459
fun and our tokenizer in this case is um

00:09:55,950 --> 00:10:02,220
the high quality space tokenizer which

00:09:59,459 --> 00:10:03,930
works really well for english and we're

00:10:02,220 --> 00:10:05,690
in north america so i can pretend it

00:10:03,930 --> 00:10:09,300
works for all of the languages i've seen

00:10:05,690 --> 00:10:11,459
if you're not from North America the

00:10:09,300 --> 00:10:11,740
tokenizer needs a bit of extension you

00:10:11,459 --> 00:10:14,860
can

00:10:11,740 --> 00:10:16,840
communicate work but normally you can

00:10:14,860 --> 00:10:21,580
make a tokenizer which is a transformer

00:10:16,840 --> 00:10:24,010
rather funding estimator yay so we're

00:10:21,580 --> 00:10:26,650
gonna do a stage together we're gonna

00:10:24,010 --> 00:10:28,510
say like I like spark ml pipelines I

00:10:26,650 --> 00:10:29,710
think they're really cool I think I'm

00:10:28,510 --> 00:10:32,080
going to do my machine learning with

00:10:29,710 --> 00:10:33,850
them but I want to feel like a badass

00:10:32,080 --> 00:10:35,740
right I don't want to just put a bunch

00:10:33,850 --> 00:10:37,240
of components off the shelf I want to

00:10:35,740 --> 00:10:39,460
add something new and I want to do

00:10:37,240 --> 00:10:41,110
something cool and I've managed to trick

00:10:39,460 --> 00:10:43,060
my boss into this by saying that I

00:10:41,110 --> 00:10:45,580
really need this special algorithm that

00:10:43,060 --> 00:10:47,800
isn't in spark and it's only going to

00:10:45,580 --> 00:10:49,900
take a month unlike at the one month

00:10:47,800 --> 00:10:51,940
market ah my boss is only one more month

00:10:49,900 --> 00:10:54,520
and I can do that about three times

00:10:51,940 --> 00:10:56,110
before my project gets canceled and in

00:10:54,520 --> 00:10:59,560
that three times I can probably make

00:10:56,110 --> 00:11:01,390
something cool if you don't have that

00:10:59,560 --> 00:11:03,400
time you can just make a just make a

00:11:01,390 --> 00:11:04,840
transformer you just tell you boss you

00:11:03,400 --> 00:11:07,810
need to do some special data cleaning

00:11:04,840 --> 00:11:10,770
and it'll just take you a week but you

00:11:07,810 --> 00:11:13,060
can fit it into sparks a mile pipeline

00:11:10,770 --> 00:11:15,640
and there's a whole bunch of things that

00:11:13,060 --> 00:11:17,320
are actually already available so

00:11:15,640 --> 00:11:19,120
there's a chance that you might not

00:11:17,320 --> 00:11:21,340
actually need to make your own pipeline

00:11:19,120 --> 00:11:25,330
stage right like we've got decision

00:11:21,340 --> 00:11:27,430
trees we have like I mean we have

00:11:25,330 --> 00:11:32,140
logistic regression because yeah fitting

00:11:27,430 --> 00:11:33,130
lines to things like what most people

00:11:32,140 --> 00:11:35,620
want them when I ask for a machine

00:11:33,130 --> 00:11:38,530
learning engineer and we've got like a

00:11:35,620 --> 00:11:40,030
OS which kind of works re OS

00:11:38,530 --> 00:11:43,090
implementations has been improving

00:11:40,030 --> 00:11:45,430
substantially and in addition to sort of

00:11:43,090 --> 00:11:48,010
the classic machine learning models

00:11:45,430 --> 00:11:49,660
inside spark there's spark packages

00:11:48,010 --> 00:11:51,310
where people who have made their own

00:11:49,660 --> 00:11:53,020
machine learning models people who have

00:11:51,310 --> 00:11:55,690
tricked their boss into giving them

00:11:53,020 --> 00:11:57,550
three months to write their code have

00:11:55,690 --> 00:11:59,560
gone there and added their own stuff but

00:11:57,550 --> 00:12:01,120
you're going to add your awesome food

00:11:59,560 --> 00:12:03,790
that inator and we're going to make some

00:12:01,120 --> 00:12:06,370
really fun code together so what does

00:12:03,790 --> 00:12:07,690
our food as inator gonna look like we're

00:12:06,370 --> 00:12:10,030
going to start on our adventure and

00:12:07,690 --> 00:12:11,410
Missy I almost lost Missy right before

00:12:10,030 --> 00:12:13,990
the talk and I was really sad and

00:12:11,410 --> 00:12:15,550
stressed out but I found Nikki and I

00:12:13,990 --> 00:12:18,520
feel like that's a good Goleman for this

00:12:15,550 --> 00:12:20,890
talk you might not agree but it's okay

00:12:18,520 --> 00:12:23,870
so Missy is going on an adventure to

00:12:20,890 --> 00:12:25,880
extend spark machine learning

00:12:23,870 --> 00:12:27,260
she only she doesn't have opposable

00:12:25,880 --> 00:12:30,380
thumbs so it's going to take a little

00:12:27,260 --> 00:12:33,230
bit of time with the you know single

00:12:30,380 --> 00:12:34,700
typing but we're going to get there and

00:12:33,230 --> 00:12:37,010
we're going to start with just a

00:12:34,700 --> 00:12:38,840
transformer because Miffy doesn't have

00:12:37,010 --> 00:12:41,750
five fingers on on each and she's only

00:12:38,840 --> 00:12:44,600
got the only met the hand but we're

00:12:41,750 --> 00:12:46,100
going to build a transformer and this is

00:12:44,600 --> 00:12:48,230
going to be really simple and it's a

00:12:46,100 --> 00:12:49,850
good first step in extending spark ml

00:12:48,230 --> 00:12:54,050
because our estimators are going to

00:12:49,850 --> 00:12:55,850
return transformers and to make a

00:12:54,050 --> 00:12:58,790
transformer work we need to provide this

00:12:55,850 --> 00:13:01,640
thing called transform schema and what

00:12:58,790 --> 00:13:03,470
transform schema is used to do is to

00:13:01,640 --> 00:13:05,420
validate that our input schema is

00:13:03,470 --> 00:13:06,770
reasonable now this is a Skylar

00:13:05,420 --> 00:13:08,660
conference so you'll probably like

00:13:06,770 --> 00:13:11,020
Holden why aren't you using types and

00:13:08,660 --> 00:13:14,390
looks like a really good question and

00:13:11,020 --> 00:13:16,580
it's not for me but unfortunately we we

00:13:14,390 --> 00:13:19,070
have to do this at run time because we

00:13:16,580 --> 00:13:20,510
pass around these data frame objects

00:13:19,070 --> 00:13:24,950
where we don't have the types constraint

00:13:20,510 --> 00:13:26,450
on them and instead of having like

00:13:24,950 --> 00:13:28,070
compile time type checking we're like

00:13:26,450 --> 00:13:29,690
well at least we shouldn't fail in the

00:13:28,070 --> 00:13:31,640
middle of an eight-hour machine learning

00:13:29,690 --> 00:13:33,290
job we should fail at the start of the

00:13:31,640 --> 00:13:35,570
eight-hour machine learning job if we

00:13:33,290 --> 00:13:37,580
can so when the machine learning

00:13:35,570 --> 00:13:41,030
pipeline starts up running we checked

00:13:37,580 --> 00:13:42,260
our inputs are valid and we're going to

00:13:41,030 --> 00:13:44,780
make it so their pipelines are

00:13:42,260 --> 00:13:47,000
configurable and sir pipeline stages are

00:13:44,780 --> 00:13:49,490
configurable so that I can do primers

00:13:47,000 --> 00:13:52,550
that searches across them because I did

00:13:49,490 --> 00:13:56,420
not do very well in statistics in school

00:13:52,550 --> 00:13:59,000
they actually they let me pass because

00:13:56,420 --> 00:14:02,360
they lost my final exam which was like

00:13:59,000 --> 00:14:05,450
yeah I think I did like well on the

00:14:02,360 --> 00:14:07,280
final exam but you know at this point

00:14:05,450 --> 00:14:08,960
who really knows so I'm going to trust

00:14:07,280 --> 00:14:10,310
the computer to do a parameter read

00:14:08,960 --> 00:14:11,900
search more than I'm going to trust

00:14:10,310 --> 00:14:14,300
myself to pick the right l1

00:14:11,900 --> 00:14:17,870
regularization parameter from my linear

00:14:14,300 --> 00:14:20,150
regression model so our simplest

00:14:17,870 --> 00:14:21,290
transformer is word count you might have

00:14:20,150 --> 00:14:23,570
thought you were going to escape the

00:14:21,290 --> 00:14:25,060
word count example and this is even this

00:14:23,570 --> 00:14:27,380
isn't going to be a proper word count

00:14:25,060 --> 00:14:29,360
but it's going to be a really basic word

00:14:27,380 --> 00:14:30,230
count and we're just going to count the

00:14:29,360 --> 00:14:31,760
number of wood

00:14:30,230 --> 00:14:34,970
that I've got in some random thing in

00:14:31,760 --> 00:14:36,470
the input these two random functions we

00:14:34,970 --> 00:14:39,380
have to provide they're not really all

00:14:36,470 --> 00:14:41,030
that important default copy you really

00:14:39,380 --> 00:14:42,920
should have just been the base instance

00:14:41,030 --> 00:14:45,350
of copy but it's not and we don't want

00:14:42,920 --> 00:14:47,240
to break binary compatibility so every

00:14:45,350 --> 00:14:50,930
time you make a new stage you need this

00:14:47,240 --> 00:14:52,880
boilerplate it's a lot of fun

00:14:50,930 --> 00:14:55,910
oh yeah and these transformers are not

00:14:52,880 --> 00:14:57,950
from the Mighty Morphin yeah don't

00:14:55,910 --> 00:14:59,840
they're not that kind of transformers

00:14:57,950 --> 00:15:02,060
so verifying what our input schema is

00:14:59,840 --> 00:15:03,350
reasonable it's pretty basic what we

00:15:02,060 --> 00:15:05,330
need to do is make sure that like

00:15:03,350 --> 00:15:08,180
whatever fields you were expecting to be

00:15:05,330 --> 00:15:10,940
present are present and then we also

00:15:08,180 --> 00:15:13,940
need to update the result so that the

00:15:10,940 --> 00:15:16,430
downstream pipeline stages can make can

00:15:13,940 --> 00:15:17,750
consume our fields right because if we

00:15:16,430 --> 00:15:20,360
didn't add whatever fields we were

00:15:17,750 --> 00:15:22,460
returning to it the next stage in the

00:15:20,360 --> 00:15:23,990
pipeline that might use it wouldn't see

00:15:22,460 --> 00:15:28,190
it and it wouldn't know that it's

00:15:23,990 --> 00:15:30,050
actually safe to run so yeah transform

00:15:28,190 --> 00:15:32,270
schema is used to verify that things

00:15:30,050 --> 00:15:35,950
should work and that works about as well

00:15:32,270 --> 00:15:35,950
as everything where I go should

00:15:36,250 --> 00:15:40,940
sometimes it sails but you know whatever

00:15:38,660 --> 00:15:45,770
it stops enough errors that it's worth

00:15:40,940 --> 00:15:48,050
doing this early on so how do you

00:15:45,770 --> 00:15:49,820
actually like make the pipeline stage do

00:15:48,050 --> 00:15:52,190
something this isn't really all that

00:15:49,820 --> 00:15:54,140
exciting I've got a wood compass just

00:15:52,190 --> 00:15:56,570
going to count the number of woods not

00:15:54,140 --> 00:15:59,210
even going to give me the like words in

00:15:56,570 --> 00:16:02,270
there count but we just call transform

00:15:59,210 --> 00:16:05,030
and I define my UDF and I do whatever I

00:16:02,270 --> 00:16:07,040
want right and our UDS could actually be

00:16:05,030 --> 00:16:09,440
much more complicated and we'll look at

00:16:07,040 --> 00:16:11,480
one of those in a little bit but this is

00:16:09,440 --> 00:16:13,040
clearly we don't have to train our wood

00:16:11,480 --> 00:16:15,020
count model it's going to be able to

00:16:13,040 --> 00:16:16,370
count the words without training so we

00:16:15,020 --> 00:16:19,970
can just directly implement those

00:16:16,370 --> 00:16:21,620
transform function and then we need to

00:16:19,970 --> 00:16:24,050
we need to make it configurable it turns

00:16:21,620 --> 00:16:26,120
out like people for some reason don't

00:16:24,050 --> 00:16:28,610
want to rename their data fields into

00:16:26,120 --> 00:16:31,550
happy pandas as an input column so we

00:16:28,610 --> 00:16:33,080
make it so that it's configurable and we

00:16:31,550 --> 00:16:34,550
have to use this parameter well we don't

00:16:33,080 --> 00:16:35,550
have to but we should use this parameter

00:16:34,550 --> 00:16:37,769
mechanism

00:16:35,550 --> 00:16:40,920
spark because when we use this parameter

00:16:37,769 --> 00:16:42,779
mechanism from inside of spark oven meta

00:16:40,920 --> 00:16:46,529
algorithms that spark has are able to

00:16:42,779 --> 00:16:49,230
work on our parameters and this is this

00:16:46,529 --> 00:16:51,360
is pretty nice if you're like wondering

00:16:49,230 --> 00:16:53,459
what kind of knobs you should provide on

00:16:51,360 --> 00:16:55,079
your new machine learning algorithm you

00:16:53,459 --> 00:16:57,720
can go look at this file called shared

00:16:55,079 --> 00:16:59,670
params dog Scala inside of SPARC and see

00:16:57,720 --> 00:17:03,899
what the common parameters are the

00:16:59,670 --> 00:17:05,790
different models are exporting and

00:17:03,899 --> 00:17:08,959
cross-validation yeah it automagically

00:17:05,790 --> 00:17:11,579
fits our stuff it even works in Python

00:17:08,959 --> 00:17:16,189
but I clearly gave you the trap across

00:17:11,579 --> 00:17:20,730
process whatever and here's an example

00:17:16,189 --> 00:17:24,000
mostly you know it's not that hard to do

00:17:20,730 --> 00:17:25,709
but here I'm only tuning on one

00:17:24,000 --> 00:17:27,750
parameter but we could tune on five

00:17:25,709 --> 00:17:29,070
parameters the only problem is I would

00:17:27,750 --> 00:17:30,390
have like one of those exponential

00:17:29,070 --> 00:17:32,520
curves with the amount of time it was

00:17:30,390 --> 00:17:35,100
going to take and so it gets a little

00:17:32,520 --> 00:17:36,990
sad at some point but if you make your

00:17:35,100 --> 00:17:39,210
machine learning models this way people

00:17:36,990 --> 00:17:41,400
like me who want regular statistics can

00:17:39,210 --> 00:17:44,040
pretend that we are and I would really

00:17:41,400 --> 00:17:46,050
appreciate that and you can make it so

00:17:44,040 --> 00:17:47,850
that I can use my parameter grid and do

00:17:46,050 --> 00:17:51,150
a search and I can search across

00:17:47,850 --> 00:17:52,890
multiple stages at the same time and if

00:17:51,150 --> 00:17:57,030
you do those please reserve a test set

00:17:52,890 --> 00:18:01,470
beforehand otherwise your results are

00:17:57,030 --> 00:18:03,510
kind of meaningless yeah so that's not

00:18:01,470 --> 00:18:05,070
all that exciting that's just making an

00:18:03,510 --> 00:18:07,260
instance that counts the number of words

00:18:05,070 --> 00:18:08,850
we actually want to provide like a

00:18:07,260 --> 00:18:11,190
machine learning model I want to go off

00:18:08,850 --> 00:18:12,870
for three months after I told my boss it

00:18:11,190 --> 00:18:14,910
was going to be one month and make a new

00:18:12,870 --> 00:18:17,070
fancy machine model and I assume you all

00:18:14,910 --> 00:18:20,700
do too so we're going to have

00:18:17,070 --> 00:18:23,700
programming classes did you do and cats

00:18:20,700 --> 00:18:25,260
love it yeah so the difference is

00:18:23,700 --> 00:18:26,910
instead of providing a transform

00:18:25,260 --> 00:18:29,460
function what we need to provide is a

00:18:26,910 --> 00:18:32,280
fit function in this case I'm just

00:18:29,460 --> 00:18:33,809
making a simple string indexer but

00:18:32,280 --> 00:18:35,820
essentially if you were doing knife

00:18:33,809 --> 00:18:38,190
berries or something like you you would

00:18:35,820 --> 00:18:39,419
do your accounts here if you were doing

00:18:38,190 --> 00:18:41,190
something with a gradient descent

00:18:39,419 --> 00:18:43,380
algorithm you would do your gradient

00:18:41,190 --> 00:18:44,190
descent to do your search for the right

00:18:43,380 --> 00:18:46,169
parameters and

00:18:44,190 --> 00:18:48,990
out of your food function here and then

00:18:46,169 --> 00:18:50,279
at the end you return a model on your

00:18:48,990 --> 00:18:54,269
model is going to be like the

00:18:50,279 --> 00:18:57,870
Transformers that we just looked at oh

00:18:54,269 --> 00:19:01,559
that slide is repeated twice sorry so

00:18:57,870 --> 00:19:03,090
the one thing is getting our input

00:19:01,559 --> 00:19:04,590
columns getting the variables that we

00:19:03,090 --> 00:19:09,269
defined with the parameters we just use

00:19:04,590 --> 00:19:11,850
this dollar sign input call syntax if

00:19:09,269 --> 00:19:13,049
you're outside of the input stage and

00:19:11,850 --> 00:19:14,519
for some reason you want to look at what

00:19:13,049 --> 00:19:16,409
the parameters were that were configured

00:19:14,519 --> 00:19:17,820
on the stage like maybe you've written

00:19:16,409 --> 00:19:20,820
it out to disk or you're getting reading

00:19:17,820 --> 00:19:24,210
to you can use our friendly Java Bean

00:19:20,820 --> 00:19:27,330
like functions get input call yay we can

00:19:24,210 --> 00:19:29,519
pretend like we're writing Java 1.2 ok

00:19:27,330 --> 00:19:31,139
no one's excited either friend the

00:19:29,519 --> 00:19:34,379
transformer is back

00:19:31,139 --> 00:19:37,049
so our simple indexer here is the

00:19:34,379 --> 00:19:38,720
transformer we construct it the things

00:19:37,049 --> 00:19:40,649
that we're passing into it they're like

00:19:38,720 --> 00:19:42,320
configurations but since if it's not

00:19:40,649 --> 00:19:44,610
something that the user would set or not

00:19:42,320 --> 00:19:46,740
anything that they were doing over a

00:19:44,610 --> 00:19:48,389
parameter search we just have it as

00:19:46,740 --> 00:19:51,629
standard constructor parameters rather

00:19:48,389 --> 00:19:54,809
than special spark parameters and then

00:19:51,629 --> 00:19:56,730
we have our fancy transform function and

00:19:54,809 --> 00:20:02,039
it's not all that fancy but it you know

00:19:56,730 --> 00:20:04,980
it just doesn't say yeah so we left out

00:20:02,039 --> 00:20:07,830
transform schema in this instance of it

00:20:04,980 --> 00:20:11,070
and we still need transform schema even

00:20:07,830 --> 00:20:13,769
though like we're not going to be doing

00:20:11,070 --> 00:20:15,690
the checking beforehand to make sure

00:20:13,769 --> 00:20:18,210
that this is pipeline is going to

00:20:15,690 --> 00:20:20,370
succeed but we still have to provide

00:20:18,210 --> 00:20:21,600
transform schema in any instance of this

00:20:20,370 --> 00:20:25,740
just because it's part of the signature

00:20:21,600 --> 00:20:27,809
and it's still used you can use a

00:20:25,740 --> 00:20:29,549
classifier estimated base class and

00:20:27,809 --> 00:20:30,720
they'll provide a transform schema you

00:20:29,549 --> 00:20:33,360
have to provide a slightly different

00:20:30,720 --> 00:20:36,929
transform function and some extra type

00:20:33,360 --> 00:20:37,649
information and they do it for us and so

00:20:36,929 --> 00:20:39,269
we're going to build a classifier

00:20:37,649 --> 00:20:41,039
instead yeah we're going to build a

00:20:39,269 --> 00:20:43,049
really simple naive Bayes classifier

00:20:41,039 --> 00:20:44,580
because word count was kind of boring

00:20:43,049 --> 00:20:47,490
and everyone looks like they're falling

00:20:44,580 --> 00:20:48,430
asleep even boo is getting a little

00:20:47,490 --> 00:20:52,720
tired

00:20:48,430 --> 00:20:54,130
okay so we're going to make this we

00:20:52,720 --> 00:20:57,910
specify our input type is going to be

00:20:54,130 --> 00:21:00,700
some vectors we're going to return a

00:20:57,910 --> 00:21:03,100
simple knife base model and pretty

00:21:00,700 --> 00:21:04,390
amazing our train functions have a lot

00:21:03,100 --> 00:21:06,400
of data thoughts inside of it because

00:21:04,390 --> 00:21:10,090
I'm lazy but we'll actually expand those

00:21:06,400 --> 00:21:13,420
dots for people that really want to look

00:21:10,090 --> 00:21:16,170
inside of the dot dot dot and so here we

00:21:13,420 --> 00:21:19,660
we figure out the number of features

00:21:16,170 --> 00:21:22,750
being input so the input to most spark

00:21:19,660 --> 00:21:24,880
machine learning models is a vector in

00:21:22,750 --> 00:21:26,470
that your data frame has a features

00:21:24,880 --> 00:21:28,120
column and your features column will

00:21:26,470 --> 00:21:30,010
normally be a vector and this is

00:21:28,120 --> 00:21:32,530
produced using something called the

00:21:30,010 --> 00:21:35,890
vector assembler but it's just a bunch

00:21:32,530 --> 00:21:40,210
of doubles and so we can go in there and

00:21:35,890 --> 00:21:42,250
pull this out grouped by key normally is

00:21:40,210 --> 00:21:45,220
the devil but in this case it's not

00:21:42,250 --> 00:21:47,140
where is it no it's not the devil here

00:21:45,220 --> 00:21:49,300
because I'm in the happy data set land

00:21:47,140 --> 00:21:51,010
and data sets group by key is safe if

00:21:49,300 --> 00:21:53,680
you're an RDD is grouped by key makes

00:21:51,010 --> 00:21:55,150
you very sad in your job fails but since

00:21:53,680 --> 00:21:57,580
we're working on data sets this is

00:21:55,150 --> 00:21:59,170
actually pretty awesome and then I can

00:21:57,580 --> 00:22:01,780
go ahead and I can get my class counts

00:21:59,170 --> 00:22:06,000
and then I can select the labels and

00:22:01,780 --> 00:22:09,540
features and then I let go and I do some

00:22:06,000 --> 00:22:12,610
comment ratio too like the code is

00:22:09,540 --> 00:22:14,710
pretty alright but we're going to figure

00:22:12,610 --> 00:22:17,080
out the number of documents as well and

00:22:14,710 --> 00:22:19,390
a number of classes just because we're

00:22:17,080 --> 00:22:20,830
training an eye based thing and then

00:22:19,390 --> 00:22:23,140
we're going to go ahead and actually get

00:22:20,830 --> 00:22:24,340
our label counts and this is just a

00:22:23,140 --> 00:22:25,960
really simple aggregate you could

00:22:24,340 --> 00:22:28,690
probably write a much better aggregate

00:22:25,960 --> 00:22:31,120
but this is just a really simple

00:22:28,690 --> 00:22:33,990
aggregate and the important thing is

00:22:31,120 --> 00:22:35,980
like we're doing all of these things in

00:22:33,990 --> 00:22:37,630
transformations on data frames like

00:22:35,980 --> 00:22:39,910
we're not going to collect the data back

00:22:37,630 --> 00:22:41,680
locally and do something with it

00:22:39,910 --> 00:22:43,570
the only exception is if you were doing

00:22:41,680 --> 00:22:46,990
something like gradient descent you

00:22:43,570 --> 00:22:48,490
would collect back sort of your training

00:22:46,990 --> 00:22:51,310
offsets and you would do another

00:22:48,490 --> 00:22:52,740
iteration but since this is really nice

00:22:51,310 --> 00:22:58,049
basic and deterministic

00:22:52,740 --> 00:23:00,840
fine yeah more code on slides not that

00:22:58,049 --> 00:23:02,580
exciting there's probably even typos in

00:23:00,840 --> 00:23:04,020
this code that it's actually the correct

00:23:02,580 --> 00:23:06,210
one is in my github so you should look

00:23:04,020 --> 00:23:09,630
at that one rather than paying too much

00:23:06,210 --> 00:23:12,330
attention to this so what is what is

00:23:09,630 --> 00:23:14,880
using the base trade cadets so this time

00:23:12,330 --> 00:23:17,340
we used a classifier trade it gets us

00:23:14,880 --> 00:23:18,750
free schema validation it gave us some

00:23:17,340 --> 00:23:22,169
free parameters we didn't have to

00:23:18,750 --> 00:23:23,940
specify what our input column was the

00:23:22,169 --> 00:23:27,240
transform function calls our trains

00:23:23,940 --> 00:23:29,039
function the wrong way around or

00:23:27,240 --> 00:23:31,080
whatever and it gives users a better

00:23:29,039 --> 00:23:33,600
understanding of what to expect right

00:23:31,080 --> 00:23:35,159
and that's the main thing if you are

00:23:33,600 --> 00:23:36,840
going to actually go out and build

00:23:35,159 --> 00:23:38,580
something like an iterative machine

00:23:36,840 --> 00:23:40,740
learning algorithm there's this like

00:23:38,580 --> 00:23:43,679
sort of sneaky part inside of data sets

00:23:40,740 --> 00:23:46,470
which is really sad and this is that

00:23:43,679 --> 00:23:49,080
data sets and iterative algorithms are

00:23:46,470 --> 00:23:52,020
like oil and water right now and so if

00:23:49,080 --> 00:23:53,220
you look inside of any of the spark ml

00:23:52,020 --> 00:23:55,320
models which are actually doing

00:23:53,220 --> 00:23:58,559
iterative training the first thing that

00:23:55,320 --> 00:24:00,779
they do is they take the data and they

00:23:58,559 --> 00:24:03,120
switch it into an RTD and then they

00:24:00,779 --> 00:24:05,190
cache it and you're going to want to do

00:24:03,120 --> 00:24:08,730
the same thing too and this is just

00:24:05,190 --> 00:24:11,070
because the the data set api's are

00:24:08,730 --> 00:24:13,289
really nice and wonderful but the data

00:24:11,070 --> 00:24:14,940
set optimizer gets kind of overloaded

00:24:13,289 --> 00:24:18,510
when it gets a lot of really

00:24:14,940 --> 00:24:19,559
identical-looking sub queries and they

00:24:18,510 --> 00:24:21,899
go on for too long

00:24:19,559 --> 00:24:23,760
but the data optimizer is really basic

00:24:21,899 --> 00:24:25,320
and it doesn't get overloaded by that

00:24:23,760 --> 00:24:27,929
and so we just we just switched to using

00:24:25,320 --> 00:24:34,950
our TVs for now and it works out much

00:24:27,929 --> 00:24:37,860
better and I'm well aware this is the

00:24:34,950 --> 00:24:39,690
talk that leads right into lunch and boo

00:24:37,860 --> 00:24:41,909
has found some lunch food and would

00:24:39,690 --> 00:24:44,760
would not want to keep anyone from their

00:24:41,909 --> 00:24:49,169
lunch so we'll wrap up without too much

00:24:44,760 --> 00:24:51,570
excitement but what can we do now that

00:24:49,169 --> 00:24:53,309
we've made our ml pipeline model you

00:24:51,570 --> 00:24:55,230
should put it in an ml pipeline you can

00:24:53,309 --> 00:24:58,200
even actually try putting it in and

00:24:55,230 --> 00:25:00,390
comparing it to an existing spark ml

00:24:58,200 --> 00:25:03,720
pipeline oh sorry an existing spark ml

00:25:00,390 --> 00:25:05,140
stage so if you don't you're like basic

00:25:03,720 --> 00:25:06,820
spark ml linear

00:25:05,140 --> 00:25:09,520
thing and then you've made your new

00:25:06,820 --> 00:25:12,850
fancy like all of your business logic

00:25:09,520 --> 00:25:14,290
whatever stage and you Splott that one

00:25:12,850 --> 00:25:16,690
in you should make sure it actually

00:25:14,290 --> 00:25:18,790
performs better than the default ones

00:25:16,690 --> 00:25:20,110
because a lot of the times the things

00:25:18,790 --> 00:25:23,410
that we spent three months on develop

00:25:20,110 --> 00:25:27,460
work as expected we can do hyper

00:25:23,410 --> 00:25:30,400
parameter tuning yay if we wanted to say

00:25:27,460 --> 00:25:32,310
like use our model for some reason we

00:25:30,400 --> 00:25:35,110
could actually implement save and export

00:25:32,310 --> 00:25:37,870
ml reader and ml writer give us sort of

00:25:35,110 --> 00:25:39,520
the basics for this but you're probably

00:25:37,870 --> 00:25:43,690
just going to want to write your own

00:25:39,520 --> 00:25:46,930
custom export logic because sparks ml

00:25:43,690 --> 00:25:50,820
formats are very exciting with a lot of

00:25:46,930 --> 00:25:50,820
opportunities for future improvement

00:25:51,150 --> 00:25:58,210
because yeah parquet is an amazing

00:25:54,460 --> 00:26:00,550
format for storing coefficients and the

00:25:58,210 --> 00:26:02,020
serving side you can kind of do serving

00:26:00,550 --> 00:26:04,270
inside of spark but you really really

00:26:02,020 --> 00:26:06,250
shouldn't people keep trying to build

00:26:04,270 --> 00:26:07,300
real time serving layers with spark and

00:26:06,250 --> 00:26:10,600
it's not a good idea

00:26:07,300 --> 00:26:13,210
I even had beer with someone last night

00:26:10,600 --> 00:26:14,920
who was trying to do that because of

00:26:13,210 --> 00:26:18,160
business reasons I believe would be a

00:26:14,920 --> 00:26:19,960
good expression and really what you want

00:26:18,160 --> 00:26:22,900
to do is you want to persist it out to

00:26:19,960 --> 00:26:27,850
some other system so if you can save it

00:26:22,900 --> 00:26:30,070
in PMML or whatever format you want and

00:26:27,850 --> 00:26:32,680
yeah okay so you you've made your ml

00:26:30,070 --> 00:26:34,090
model it actually works pretty well you

00:26:32,680 --> 00:26:36,880
put it up on github because you're a

00:26:34,090 --> 00:26:39,490
nice person but now you can publish it

00:26:36,880 --> 00:26:42,280
time mate in central ok and then we can

00:26:39,490 --> 00:26:44,560
put it on spark packages org where you

00:26:42,280 --> 00:26:46,570
can look and find the other ml pipeline

00:26:44,560 --> 00:26:48,480
models who are made by people who are

00:26:46,570 --> 00:26:53,020
like you how hard can it be

00:26:48,480 --> 00:26:56,020
and yeah it's pretty fun if you like

00:26:53,020 --> 00:27:00,190
Python people you can look at how we

00:26:56,020 --> 00:27:02,500
expose with ml models in Python for

00:27:00,190 --> 00:27:06,370
spark it's mostly done using this thing

00:27:02,500 --> 00:27:09,220
called Piper J which is capable of

00:27:06,370 --> 00:27:11,140
summoning Cthulhu if use incorrectly so

00:27:09,220 --> 00:27:13,540
please test your Python bindings there

00:27:11,140 --> 00:27:15,580
are no types to help you out and keep

00:27:13,540 --> 00:27:17,590
you safe so if you want to support

00:27:15,580 --> 00:27:20,080
Python users that's great I love you

00:27:17,590 --> 00:27:23,220
but please please test your Python code

00:27:20,080 --> 00:27:25,600
I mean test your Scala code - but like I

00:27:23,220 --> 00:27:28,169
assume were all good Scala developers

00:27:25,600 --> 00:27:28,169
here already

00:27:28,240 --> 00:27:32,440
and there's a bunch of resources so if

00:27:30,610 --> 00:27:34,330
like those five slides with code on it

00:27:32,440 --> 00:27:37,029
wasn't like the most exciting thing in

00:27:34,330 --> 00:27:38,980
your world that's okay I have an example

00:27:37,029 --> 00:27:41,320
repo it has some sample models in there

00:27:38,980 --> 00:27:43,570
they range from the really simple word

00:27:41,320 --> 00:27:45,190
counting that we looked at to like this

00:27:43,570 --> 00:27:47,590
really simple night vision that we also

00:27:45,190 --> 00:27:49,510
looked at if you want to see like how

00:27:47,590 --> 00:27:50,950
the iterative algorithms are implemented

00:27:49,510 --> 00:27:52,480
inside of spark because you're

00:27:50,950 --> 00:27:55,240
interested in doing something with like

00:27:52,480 --> 00:27:58,120
gradient descent model you can look

00:27:55,240 --> 00:28:01,090
inside of spark itself they use a lot of

00:27:58,120 --> 00:28:02,559
internal API is naturally but the code

00:28:01,090 --> 00:28:04,029
is nice and public and you can just

00:28:02,559 --> 00:28:06,370
rewrite the parts that are calling the

00:28:04,029 --> 00:28:10,060
internal API when you're thinking about

00:28:06,370 --> 00:28:11,429
it I haven't no Riley blog post about it

00:28:10,060 --> 00:28:15,279
[Music]

00:28:11,429 --> 00:28:17,710
might be good I don't know probably the

00:28:15,279 --> 00:28:23,230
other resources are more directly useful

00:28:17,710 --> 00:28:27,190
to you and there's some spark books yay

00:28:23,230 --> 00:28:31,059
most of these book books are written by

00:28:27,190 --> 00:28:32,830
me I think that's true yes but not all

00:28:31,059 --> 00:28:34,899
of them so I don't receive royalties for

00:28:32,830 --> 00:28:36,429
all release books but I will try and get

00:28:34,899 --> 00:28:40,000
you to buy the one where I receive the

00:28:36,429 --> 00:28:42,730
most royalties because I like coffee and

00:28:40,000 --> 00:28:45,520
living in San Francisco is really really

00:28:42,730 --> 00:28:47,110
 expensive one of the books that I

00:28:45,520 --> 00:28:48,940
didn't write is called learning Thai

00:28:47,110 --> 00:28:50,649
spark if you have eyes on friends and

00:28:48,940 --> 00:28:53,320
you're looking to convert them to the

00:28:50,649 --> 00:28:55,450
dark side you can get them learning ice

00:28:53,320 --> 00:28:57,580
bars get them hooked on spark and then

00:28:55,450 --> 00:28:58,779
you know help three months later and be

00:28:57,580 --> 00:29:00,880
like by the way

00:28:58,779 --> 00:29:04,270
Scala will make here oh it's so much

00:29:00,880 --> 00:29:06,010
faster or you know not that you should

00:29:04,270 --> 00:29:08,740
convert your friends so the dark side

00:29:06,010 --> 00:29:11,350
for cookies well you can buy this book

00:29:08,740 --> 00:29:13,690
yeah it's available an early release

00:29:11,350 --> 00:29:15,279
from O'Reilly which means that you can

00:29:13,690 --> 00:29:19,330
give me money for something that doesn't

00:29:15,279 --> 00:29:21,039
exist um I like thinking that I can sell

00:29:19,330 --> 00:29:23,130
that don't exist so it's really hard to

00:29:21,039 --> 00:29:25,270
return something that doesn't exist

00:29:23,130 --> 00:29:26,279
actually it's very easy to return

00:29:25,270 --> 00:29:28,739
something that doesn't exist

00:29:26,279 --> 00:29:30,779
unfortunately but that should not stop

00:29:28,739 --> 00:29:34,019
you from buying it and especially if you

00:29:30,779 --> 00:29:36,809
have pets pets really love the Amazon

00:29:34,019 --> 00:29:39,029
boxes but books come in so if you have a

00:29:36,809 --> 00:29:41,249
corporate expense account I think it is

00:29:39,029 --> 00:29:44,090
a great time to buy several copies of

00:29:41,249 --> 00:29:46,950
this book for everyone that you know

00:29:44,090 --> 00:29:48,599
also like the IBM compliance training

00:29:46,950 --> 00:29:51,330
said that I'm no longer allowed to

00:29:48,599 --> 00:29:54,090
accept $20 bills inside of brown

00:29:51,330 --> 00:29:56,789
envelopes and so instead if you buy this

00:29:54,090 --> 00:29:59,249
book I don't get a $20 bill but I do get

00:29:56,789 --> 00:30:01,289
like a dollar or something like that

00:29:59,249 --> 00:30:04,889
from O'Reilly and so like that can be a

00:30:01,289 --> 00:30:06,809
way of not not bribing me or whatever

00:30:04,889 --> 00:30:11,219
those words are but yet you can give me

00:30:06,809 --> 00:30:14,460
money this way and I've got some other

00:30:11,219 --> 00:30:18,389
talks if anyone's really anyone to San

00:30:14,460 --> 00:30:21,509
Francisco yeah okay so there's data and

00:30:18,389 --> 00:30:23,279
comps next week in San Francisco I'll be

00:30:21,509 --> 00:30:25,589
talking about how to use data frames so

00:30:23,279 --> 00:30:27,899
if like this talk was like I don't know

00:30:25,589 --> 00:30:29,460
what these data frames are then like you

00:30:27,899 --> 00:30:31,769
can come to next week's talk and learn

00:30:29,460 --> 00:30:33,960
about data frames if anyone's looking

00:30:31,769 --> 00:30:38,039
for an excuse to visit Barcelona or

00:30:33,960 --> 00:30:40,830
London or Israel or Copenhagen or I mean

00:30:38,039 --> 00:30:42,179
San Francisco I mean you can come and

00:30:40,830 --> 00:30:44,009
drag me in these places and tell your

00:30:42,179 --> 00:30:45,899
boss that it's very important that you

00:30:44,009 --> 00:30:48,989
go see this talk and I also to back you

00:30:45,899 --> 00:30:50,669
up on that and then we both win because

00:30:48,989 --> 00:30:53,279
your boss thinks but I know what I'm

00:30:50,669 --> 00:30:54,719
doing and you are clearly an industry

00:30:53,279 --> 00:30:57,690
leader as well for needing to go to

00:30:54,719 --> 00:30:59,070
these fancy places okay so that's pretty

00:30:57,690 --> 00:31:01,649
much it

00:30:59,070 --> 00:31:03,539
key thanks bye I figured I would leave

00:31:01,649 --> 00:31:08,249
10 minutes for questions if anyone has

00:31:03,539 --> 00:31:10,950
questions if you don't I would really

00:31:08,249 --> 00:31:13,499
love your feedback on what you're doing

00:31:10,950 --> 00:31:15,239
to test your splice applications and if

00:31:13,499 --> 00:31:16,919
the answer is I'm not testing my spark

00:31:15,239 --> 00:31:19,080
applications I would really like to know

00:31:16,919 --> 00:31:22,679
where you work so I can shrug your

00:31:19,080 --> 00:31:24,629
company shares but more more real

00:31:22,679 --> 00:31:26,009
security I really am interested in

00:31:24,629 --> 00:31:27,929
knowing what people are doing to test

00:31:26,009 --> 00:31:31,469
their distributed systems so that I can

00:31:27,929 --> 00:31:32,849
try and make better tools for people or

00:31:31,469 --> 00:31:36,769
does anyone have any questions about

00:31:32,849 --> 00:31:36,769
extending spark machine learning models

00:31:37,140 --> 00:31:42,670
coffee shops in San Francisco I can also

00:31:40,030 --> 00:31:45,310
answer questions on and boo can answer

00:31:42,670 --> 00:31:48,730
questions on dog treats and Miffy knows

00:31:45,310 --> 00:31:52,390
a lot about flowers or do we just go get

00:31:48,730 --> 00:31:53,860
lunch early okay it sounds like team

00:31:52,390 --> 00:31:55,810
lunch is winning so thank you all for

00:31:53,860 --> 00:31:58,690
coming I hope this will be somewhat

00:31:55,810 --> 00:32:04,880
useful but thanks

00:31:58,690 --> 00:32:04,880

YouTube URL: https://www.youtube.com/watch?v=CAqqd4UqCtY


