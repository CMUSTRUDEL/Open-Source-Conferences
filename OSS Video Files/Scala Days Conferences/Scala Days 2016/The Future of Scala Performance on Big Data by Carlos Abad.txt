Title: The Future of Scala Performance on Big Data by Carlos Abad
Publication date: 2017-01-19
Playlist: Scala Days 2016
Description: 
	This video was recorded at Scala Days Berlin 2016
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

Abstract:
Scala is the fastest expanding language in the datacenter, big data, and cloud computing. However the lack of standardized Big Data Scala benchmarks has hindered extended performance improvements related to the datacenter. In this talk we will present a newly developed Scala Big Data Benchmark based on learnings from TCPx-BB, SparkBench, and genomic (GATK/ADAM) workloads. We'll also introduce the most impacting big data-related performance improvements for Scala developed by our group.
Captions: 
	00:00:01,330 --> 00:00:07,249
hello everyone my name is Carlos Abbott

00:00:04,580 --> 00:00:13,190
and I'm a data center performance

00:00:07,249 --> 00:00:16,430
engineer at Intel please at the end of

00:00:13,190 --> 00:00:19,130
the talk if I sell rating so we can keep

00:00:16,430 --> 00:00:20,720
improving by using your feedback feel

00:00:19,130 --> 00:00:24,320
free to approach me with any feedback

00:00:20,720 --> 00:00:27,439
you have at the end of the talk first of

00:00:24,320 --> 00:00:29,599
all this is the legal disclaimer if you

00:00:27,439 --> 00:00:31,910
have any questions about it please let

00:00:29,599 --> 00:00:36,110
me know i will give you the email from

00:00:31,910 --> 00:00:38,480
my lawyers and and just so you guys know

00:00:36,110 --> 00:00:41,270
if you guys have any questions use the

00:00:38,480 --> 00:00:43,280
talk please just stop me and at the end

00:00:41,270 --> 00:00:45,260
of the side i will answer the question i

00:00:43,280 --> 00:00:47,270
think there are some microphones over

00:00:45,260 --> 00:00:51,559
there but i'm pretty sure i can just

00:00:47,270 --> 00:00:57,559
hear your questions so the first thing

00:00:51,559 --> 00:00:59,300
is about intel and scala i have

00:00:57,559 --> 00:01:03,379
approached several of you doing those

00:00:59,300 --> 00:01:06,110
these 24 last 24 hours and many people

00:01:03,379 --> 00:01:08,570
ask me like wait a second you work for

00:01:06,110 --> 00:01:10,910
intel i thought until they were all in

00:01:08,570 --> 00:01:13,340
the cpu business right they do chips

00:01:10,910 --> 00:01:16,640
right they don't do software not at all

00:01:13,340 --> 00:01:19,970
actually intel has a lot of people

00:01:16,640 --> 00:01:22,450
working in data center technologies we

00:01:19,970 --> 00:01:24,950
are in charge of squeezing the most

00:01:22,450 --> 00:01:27,200
performance out of the hardware it's

00:01:24,950 --> 00:01:29,810
amazing how much you need optimizations

00:01:27,200 --> 00:01:32,150
in the linux kernel to get that those

00:01:29,810 --> 00:01:36,650
cpu features or to use at maximum on

00:01:32,150 --> 00:01:39,260
your cpu moreover intel has been working

00:01:36,650 --> 00:01:42,620
for the last 10 to 15 years in

00:01:39,260 --> 00:01:44,740
optimizing other languages like for

00:01:42,620 --> 00:01:47,360
example Java optimizing the JVM

00:01:44,740 --> 00:01:51,650
optimizing Python we started optimizing

00:01:47,360 --> 00:01:54,760
Python recently know Jas and now one

00:01:51,650 --> 00:01:58,540
year ago I was named the lead

00:01:54,760 --> 00:02:01,220
performance engineer at Intel for Scala

00:01:58,540 --> 00:02:04,760
it's being a very interesting year by

00:02:01,220 --> 00:02:07,460
the way so all just so you guys know all

00:02:04,760 --> 00:02:09,320
the data and all the code we're going to

00:02:07,460 --> 00:02:12,140
show it here it will be published online

00:02:09,320 --> 00:02:14,690
and you guys will be able to grab it

00:02:12,140 --> 00:02:16,850
from github play with it

00:02:14,690 --> 00:02:19,130
expand it if you want if you like

00:02:16,850 --> 00:02:23,180
everything will be open sourced from RN

00:02:19,130 --> 00:02:24,890
to the first thing that I want to talk

00:02:23,180 --> 00:02:27,370
about is what we are going to talk about

00:02:24,890 --> 00:02:31,700
basically I'm going to show up up

00:02:27,370 --> 00:02:34,640
performance well before we go before we

00:02:31,700 --> 00:02:39,710
continue how many people have ever done

00:02:34,640 --> 00:02:43,430
any profiling right right how many of

00:02:39,710 --> 00:02:46,310
you have ever used benchmarks in a

00:02:43,430 --> 00:02:51,460
scholar or any language very good very

00:02:46,310 --> 00:02:54,740
good that's amazing so one of the things

00:02:51,460 --> 00:02:57,520
we started working in Scala we found

00:02:54,740 --> 00:03:00,380
that they are not a lot of good

00:02:57,520 --> 00:03:02,630
benchmarks and I'm going to explain what

00:03:00,380 --> 00:03:05,900
I mean by good and basically i'm going

00:03:02,630 --> 00:03:08,240
to show a benchmark suite that we have

00:03:05,900 --> 00:03:11,180
developed based on bits data work big

00:03:08,240 --> 00:03:13,850
data workloads i'm going to show how we

00:03:11,180 --> 00:03:15,680
have used that benchmark suite to

00:03:13,850 --> 00:03:18,530
optimize the mutable hashmap in a

00:03:15,680 --> 00:03:22,190
scholar and also i'm going to show how

00:03:18,530 --> 00:03:25,730
once the code is open source how you can

00:03:22,190 --> 00:03:28,550
grab that code and expand it with your

00:03:25,730 --> 00:03:31,100
own benchmarks easily and on the

00:03:28,550 --> 00:03:32,900
benchmark suite will the driver of the

00:03:31,100 --> 00:03:36,080
bed front street will live with all the

00:03:32,900 --> 00:03:39,140
complexity for you so why is this dog

00:03:36,080 --> 00:03:42,970
interesting for any of you well if you

00:03:39,140 --> 00:03:44,989
work in Scala performance is obvious

00:03:42,970 --> 00:03:47,660
we're going to show a new benchmark

00:03:44,989 --> 00:03:51,709
suite and methodology for benchmarking

00:03:47,660 --> 00:03:57,380
Scala for big data applications for the

00:03:51,709 --> 00:03:59,450
rest of you big data Scala developers it

00:03:57,380 --> 00:04:02,510
will this is a tool that it will it will

00:03:59,450 --> 00:04:04,870
allow any scholar developer to quickly

00:04:02,510 --> 00:04:07,880
create your own micro benchmark and

00:04:04,870 --> 00:04:15,410
expand this way I will explain more as

00:04:07,880 --> 00:04:17,690
we go in the presentation so do we

00:04:15,410 --> 00:04:19,489
really need a new escala benchmark I'm

00:04:17,690 --> 00:04:21,380
pretty sure they're there are several

00:04:19,489 --> 00:04:24,440
talks here talking about new benchmarks

00:04:21,380 --> 00:04:27,470
right so the thing is that when one year

00:04:24,440 --> 00:04:28,220
ago when we started working on a scala

00:04:27,470 --> 00:04:32,150
we

00:04:28,220 --> 00:04:35,300
found that they are not benchmarking

00:04:32,150 --> 00:04:38,240
escala benchmarks based on big data

00:04:35,300 --> 00:04:41,660
applications we have for example scala

00:04:38,240 --> 00:04:44,090
bench which is either bit outdated and

00:04:41,660 --> 00:04:47,360
it's not based on Big Data workloads at

00:04:44,090 --> 00:04:49,430
all we have also a lot of micro

00:04:47,360 --> 00:04:51,200
benchmarks but those micro benchmark do

00:04:49,430 --> 00:04:57,080
not show reflect any of the information

00:04:51,200 --> 00:04:59,150
from Big Data benchmarks and and also we

00:04:57,080 --> 00:05:01,130
have a spark bench for example for those

00:04:59,150 --> 00:05:04,040
of you who don't know anything about the

00:05:01,130 --> 00:05:06,530
spark spark it's a big data processing

00:05:04,040 --> 00:05:08,810
framework is the most use open source

00:05:06,530 --> 00:05:13,460
big data processing framework out there

00:05:08,810 --> 00:05:15,350
and it doesn't give any information

00:05:13,460 --> 00:05:18,740
about the Scala it's not focused on a

00:05:15,350 --> 00:05:21,020
scholar and moreover when you wanna go

00:05:18,740 --> 00:05:22,790
on run any big data workloads like

00:05:21,020 --> 00:05:24,950
genomic processing you need your own

00:05:22,790 --> 00:05:26,660
cluster and that's a problem right

00:05:24,950 --> 00:05:28,250
because it requires to have an

00:05:26,660 --> 00:05:31,880
infrastructure that you need to rent or

00:05:28,250 --> 00:05:37,010
do you need to have we have simplified

00:05:31,880 --> 00:05:39,650
all of that so for those of you who

00:05:37,010 --> 00:05:43,550
don't know what the spark is apache

00:05:39,650 --> 00:05:46,490
spark is a framework it's a processing

00:05:43,550 --> 00:05:50,630
framework that allows you to grab data

00:05:46,490 --> 00:05:52,370
and the framework itself will deal with

00:05:50,630 --> 00:05:55,210
the complexity of splitting the data in

00:05:52,370 --> 00:05:59,300
your data center processing that data

00:05:55,210 --> 00:06:01,190
aggregating the results at the end with

00:05:59,300 --> 00:06:04,430
the sure many of you know what the spark

00:06:01,190 --> 00:06:06,470
does and nowadays parties you see is

00:06:04,430 --> 00:06:09,919
used for applications like genome

00:06:06,470 --> 00:06:16,580
analysis ecommerce fraud detection is

00:06:09,919 --> 00:06:20,210
reading purely almost purely in Scala so

00:06:16,580 --> 00:06:23,600
what we did is that we grabbed real real

00:06:20,210 --> 00:06:28,240
world workload like genomic processing

00:06:23,600 --> 00:06:31,990
genomic benchmarks and we extracted

00:06:28,240 --> 00:06:35,110
performing information we we synthesized

00:06:31,990 --> 00:06:39,229
lightweight escala benchmarks that

00:06:35,110 --> 00:06:42,070
perform pretty much the same or hit the

00:06:39,229 --> 00:06:45,100
same hot spot at the Astorino

00:06:42,070 --> 00:06:46,540
education but the only the main

00:06:45,100 --> 00:06:48,790
difference here is that instead of

00:06:46,540 --> 00:06:52,780
having your big date application running

00:06:48,790 --> 00:06:55,720
for let's say 24 hours like man anomic

00:06:52,780 --> 00:06:58,720
pipeline was running the other day for

00:06:55,720 --> 00:07:00,640
in my entire spark cluster now I can do

00:06:58,720 --> 00:07:03,790
that in my own laptop and they will get

00:07:00,640 --> 00:07:08,350
the same performance information so the

00:07:03,790 --> 00:07:11,620
difference is huge humongous not only

00:07:08,350 --> 00:07:13,930
how fast you can do that from hours or

00:07:11,620 --> 00:07:16,180
days two minutes but also the resource

00:07:13,930 --> 00:07:17,650
is needed you don't need a cluster

00:07:16,180 --> 00:07:27,100
anymore now you can do that on your own

00:07:17,650 --> 00:07:31,660
laptop incredible right so the way we

00:07:27,100 --> 00:07:35,170
created these benchmarks or benchmark

00:07:31,660 --> 00:07:38,320
suite this benchmark suite is made out

00:07:35,170 --> 00:07:42,250
of a bunch of micro benchmarks at and

00:07:38,320 --> 00:07:47,470
for each micro benchmark we have created

00:07:42,250 --> 00:07:51,250
we run a bunch of workloads real-world

00:07:47,470 --> 00:07:54,190
workload we work with researchers all

00:07:51,250 --> 00:07:56,680
over the wall with from the genomic side

00:07:54,190 --> 00:07:59,500
to ask them a how do you run your

00:07:56,680 --> 00:08:01,480
genomic workloads what do you run can we

00:07:59,500 --> 00:08:04,120
access the same dynamic files can we run

00:08:01,480 --> 00:08:07,000
it in or cluster and then we ran it in

00:08:04,120 --> 00:08:09,100
our own spark cluster and we struck

00:08:07,000 --> 00:08:12,850
performance data that performance data

00:08:09,100 --> 00:08:15,790
give us the places or tell us as the

00:08:12,850 --> 00:08:18,910
places where or application is most used

00:08:15,790 --> 00:08:21,930
the places or escala the scala libraries

00:08:18,910 --> 00:08:27,610
or collections that are most used and

00:08:21,930 --> 00:08:32,710
out of that information we create one

00:08:27,610 --> 00:08:34,900
each one of the micro benchmarks and we

00:08:32,710 --> 00:08:37,660
did that over and over with a bunch of

00:08:34,900 --> 00:08:39,700
workload real real life workloads

00:08:37,660 --> 00:08:41,919
workers that are used nowadays today

00:08:39,700 --> 00:08:47,410
this is not something that we make up

00:08:41,919 --> 00:08:52,260
this is real workload big bench used in

00:08:47,410 --> 00:08:52,260
high performance computing a spar bench

00:08:52,690 --> 00:08:56,790
they are trying to do it industrywide

00:08:54,730 --> 00:09:02,130
but we can say it's industry-wide

00:08:56,790 --> 00:09:05,560
benchmarking adam genomics use worldwide

00:09:02,130 --> 00:09:07,180
so out of that we found a bunch of hot

00:09:05,560 --> 00:09:09,550
message we found a bunch of flavors that

00:09:07,180 --> 00:09:11,770
they were they were getting heat over

00:09:09,550 --> 00:09:13,930
and over that they need to get optimized

00:09:11,770 --> 00:09:22,270
like for example the hashmap the meat of

00:09:13,930 --> 00:09:24,910
a hashmap the meter will twist it and we

00:09:22,270 --> 00:09:27,760
once we had that that and this is

00:09:24,910 --> 00:09:29,890
exactly the particle that you are going

00:09:27,760 --> 00:09:32,380
to get once you go to github on github

00:09:29,890 --> 00:09:36,550
you do already have all those benchmarks

00:09:32,380 --> 00:09:40,600
there for you to download and by running

00:09:36,550 --> 00:09:42,550
the benchmark suite there is a driver

00:09:40,600 --> 00:09:44,260
that will take care of extracting all

00:09:42,550 --> 00:09:46,150
the information all the profiling

00:09:44,260 --> 00:09:48,580
information for you you have to do

00:09:46,150 --> 00:09:51,280
anything if you have Java and Scala

00:09:48,580 --> 00:09:53,050
installed in your system the driver will

00:09:51,280 --> 00:09:55,570
take care of extracting the performance

00:09:53,050 --> 00:09:57,850
information with different java virtual

00:09:55,570 --> 00:09:59,590
machines with differing Scala versions

00:09:57,850 --> 00:10:02,470
if you want to compare scholar versions

00:09:59,590 --> 00:10:06,160
if you if you want to compare if you

00:10:02,470 --> 00:10:09,510
want to compare different flags as well

00:10:06,160 --> 00:10:14,520
and one of the most amazing features

00:10:09,510 --> 00:10:17,020
here is that this benchmark suite is

00:10:14,520 --> 00:10:20,250
time-based profiling and when I meant by

00:10:17,020 --> 00:10:23,770
that nowadays benchmarks as are based on

00:10:20,250 --> 00:10:25,360
repeating a task let's say 10,000 times

00:10:23,770 --> 00:10:29,560
right you repeat that are attached

00:10:25,360 --> 00:10:32,710
10,000 times but problem is that in a

00:10:29,560 --> 00:10:36,700
real-life workload that test will be run

00:10:32,710 --> 00:10:38,880
maybe a trillion times over the over 10

00:10:36,700 --> 00:10:42,190
hours instead of 10 milliseconds and

00:10:38,880 --> 00:10:44,500
that is the main difference here every

00:10:42,190 --> 00:10:47,680
micro benchmarks out they were running

00:10:44,500 --> 00:10:50,560
for 10 milliseconds one second and I can

00:10:47,680 --> 00:10:52,900
tell you that Aeschylus right on top of

00:10:50,560 --> 00:10:54,550
the Java Virtual Machine and after

00:10:52,900 --> 00:10:57,550
several minutes of running and workload

00:10:54,550 --> 00:10:59,950
or seconds they performance profiling

00:10:57,550 --> 00:11:03,040
the performance profile changes and

00:10:59,950 --> 00:11:05,240
that's why we ran the workload not only

00:11:03,040 --> 00:11:08,720
we ran the workload with each

00:11:05,240 --> 00:11:10,730
workload with a jvm warm-up sequence so

00:11:08,720 --> 00:11:12,110
we warm up the Java Virtual Machine for

00:11:10,730 --> 00:11:16,190
you you don't have to worry about that

00:11:12,110 --> 00:11:18,260
at all and once the workday the JVM is

00:11:16,190 --> 00:11:20,990
in a steady state we start recording

00:11:18,260 --> 00:11:22,940
that for as long as you need usually

00:11:20,990 --> 00:11:25,279
five minutes three minutes of data five

00:11:22,940 --> 00:11:27,080
minutes one hour of data and we have

00:11:25,279 --> 00:11:28,459
reached all the performance results for

00:11:27,080 --> 00:11:32,709
you so you'll have to worry about

00:11:28,459 --> 00:11:35,270
anything it's beautiful and simple

00:11:32,709 --> 00:11:38,930
that's what we performance engineer need

00:11:35,270 --> 00:11:42,410
it for a long time beautiful so this is

00:11:38,930 --> 00:11:44,450
the example output I'm really aware that

00:11:42,410 --> 00:11:49,760
people at the back cannot see so that's

00:11:44,450 --> 00:11:52,670
why we have a magnification so the

00:11:49,760 --> 00:11:55,610
driver will run each workload with

00:11:52,670 --> 00:11:58,250
different sizes with different input

00:11:55,610 --> 00:12:01,730
sizes so for example for this hash

00:11:58,250 --> 00:12:05,450
mutable hashmap benchmark it will run

00:12:01,730 --> 00:12:08,029
the input with different sizes of the

00:12:05,450 --> 00:12:11,500
input different input sizes so you can

00:12:08,029 --> 00:12:14,329
see the scaling of your data or

00:12:11,500 --> 00:12:16,040
benchmark with different sizes this is

00:12:14,329 --> 00:12:18,079
really important because you don't know

00:12:16,040 --> 00:12:21,110
how big the service you are creating is

00:12:18,079 --> 00:12:25,010
going to be used in the future right at

00:12:21,110 --> 00:12:27,140
least in big data and not only that we

00:12:25,010 --> 00:12:29,839
can compare different color versions and

00:12:27,140 --> 00:12:33,829
difference java versions did you guys

00:12:29,839 --> 00:12:35,839
know that changing from java 7 to java 8

00:12:33,829 --> 00:12:40,250
gives you five percent improvement in

00:12:35,839 --> 00:12:44,060
the hashmap i do and now you guys do as

00:12:40,250 --> 00:12:45,829
well so now by using this is the type of

00:12:44,060 --> 00:12:48,800
information you guys will get by using

00:12:45,829 --> 00:12:50,540
this benchmark and this benjamin street

00:12:48,800 --> 00:12:52,339
and when you create your own benchmark

00:12:50,540 --> 00:12:54,350
you will be able to see the same

00:12:52,339 --> 00:12:57,589
differences what happens if I change the

00:12:54,350 --> 00:13:03,260
JVM some things are better with a server

00:12:57,589 --> 00:13:06,649
of flags that with other flags so this

00:13:03,260 --> 00:13:08,839
is an example where we have used this

00:13:06,649 --> 00:13:15,470
benchmark Street to optimize the Scala

00:13:08,839 --> 00:13:17,060
so we run the we're on this bench we run

00:13:15,470 --> 00:13:18,710
this venture street and we realize that

00:13:17,060 --> 00:13:21,890
the middle hashmap

00:13:18,710 --> 00:13:24,530
was not scaling me so the performance of

00:13:21,890 --> 00:13:26,840
the middle Hodgman was getting degrade

00:13:24,530 --> 00:13:29,810
with a certain data pattern so we had a

00:13:26,840 --> 00:13:32,000
genomic application Adam that it was

00:13:29,810 --> 00:13:34,490
with his reading ask Allah is run on top

00:13:32,000 --> 00:13:38,720
of a spark and by using the hash map in

00:13:34,490 --> 00:13:40,850
a in different ways in a certain ways

00:13:38,720 --> 00:13:45,560
and within a certain way and we are

00:13:40,850 --> 00:13:49,160
certain data pattern it actually it

00:13:45,560 --> 00:13:52,870
actually clocked the hash map so we

00:13:49,160 --> 00:13:56,390
found that by modifying modifying the

00:13:52,870 --> 00:13:59,450
implementation with opportunistic red

00:13:56,390 --> 00:14:01,460
black tree instead of a linear hashmap

00:13:59,450 --> 00:14:07,720
we change the performance of the hash

00:14:01,460 --> 00:14:10,670
map from linear to logarithmic the whole

00:14:07,720 --> 00:14:13,220
who knows what logarithmic versus linear

00:14:10,670 --> 00:14:20,150
means it's a programmers you don't worry

00:14:13,220 --> 00:14:22,580
is not an interview so yeah so that

00:14:20,150 --> 00:14:25,220
means that as we grow the size of the

00:14:22,580 --> 00:14:27,080
hash map and instead of the performance

00:14:25,220 --> 00:14:30,110
of the half the time to retrieve data

00:14:27,080 --> 00:14:32,060
from the hash map scales with it if it's

00:14:30,110 --> 00:14:34,250
linear it will scale with the number of

00:14:32,060 --> 00:14:36,260
elements in the hashmap if it's

00:14:34,250 --> 00:14:40,550
logarithmic it was not a scale linearly

00:14:36,260 --> 00:14:45,800
it will scale this way so that means

00:14:40,550 --> 00:14:47,360
that instead of ha when the difference

00:14:45,800 --> 00:14:48,950
between having 1 million elements and 10

00:14:47,360 --> 00:14:52,130
million elements is not 10 eggs in

00:14:48,950 --> 00:14:57,830
performance it's much less than that it

00:14:52,130 --> 00:15:01,460
was 10x and this is the example

00:14:57,830 --> 00:15:03,260
benchmark this is an example of how once

00:15:01,460 --> 00:15:06,050
we put the data on github you will be

00:15:03,260 --> 00:15:10,520
able to get that code and expand it with

00:15:06,050 --> 00:15:13,130
your own workload and it's very easy

00:15:10,520 --> 00:15:15,920
actually just by extending the trade

00:15:13,130 --> 00:15:17,630
benchmark that we have created the

00:15:15,920 --> 00:15:19,580
driver automatically recognizes your

00:15:17,630 --> 00:15:22,100
code automatically recognizes the tree

00:15:19,580 --> 00:15:28,520
and by implementing these two methods

00:15:22,100 --> 00:15:31,010
warm up and run by defining them the

00:15:28,520 --> 00:15:32,120
bank run we will be able to use your

00:15:31,010 --> 00:15:34,220
code you

00:15:32,120 --> 00:15:37,640
your benchmark usually Neil micro

00:15:34,220 --> 00:15:39,320
benchmark and ran with different

00:15:37,640 --> 00:15:42,200
configurations this benchmark over and

00:15:39,320 --> 00:15:44,330
over four minutes or hours you now you

00:15:42,200 --> 00:15:45,950
can run your benchmark for minutes and

00:15:44,330 --> 00:15:47,270
it will obtain all the information all

00:15:45,950 --> 00:15:48,920
the profiling information it will do the

00:15:47,270 --> 00:15:51,529
averages it will do that the VA

00:15:48,920 --> 00:15:54,380
deviation a standard deviation it will

00:15:51,529 --> 00:15:56,990
actually obtain garbage collection

00:15:54,380 --> 00:15:59,920
information and profiling for you you

00:15:56,990 --> 00:15:59,920
don't have to worry about that anymore

00:16:01,870 --> 00:16:08,900
just to finish my talk I want to say

00:16:06,560 --> 00:16:13,190
what we have achieved so far we have

00:16:08,900 --> 00:16:16,460
cheese so far it's actually we have been

00:16:13,190 --> 00:16:19,220
able to create this new framework it's

00:16:16,460 --> 00:16:20,900
not only framework but a scallop

00:16:19,220 --> 00:16:22,480
benchmark suite and by using this

00:16:20,900 --> 00:16:27,560
pension street we have been successful

00:16:22,480 --> 00:16:29,750
several times we expect to increase the

00:16:27,560 --> 00:16:33,950
number of micro benchmarks over time as

00:16:29,750 --> 00:16:35,839
we find more and more bottlenecks and we

00:16:33,950 --> 00:16:37,610
would like to call the community to help

00:16:35,839 --> 00:16:39,320
us increasing the micro benchmarks

00:16:37,610 --> 00:16:41,600
increasing the benchmark street

00:16:39,320 --> 00:16:47,170
including more micro venture that they

00:16:41,600 --> 00:16:51,920
think they are useful and they can be

00:16:47,170 --> 00:16:58,450
they can be used for their own it can be

00:16:51,920 --> 00:17:02,029
very useful for them so and that's it

00:16:58,450 --> 00:17:04,810
any anyone has a question maybe I have

00:17:02,029 --> 00:17:04,810
fresh too much

00:17:18,230 --> 00:17:22,530
you hear me oh it will be awesome

00:17:21,030 --> 00:17:25,020
English are some specific about the

00:17:22,530 --> 00:17:27,150
hashmap what do you do is like if you

00:17:25,020 --> 00:17:28,830
are using certain like but pattern that

00:17:27,150 --> 00:17:31,890
you are always hitting the same harsh

00:17:28,830 --> 00:17:33,780
like collisions do you do you know when

00:17:31,890 --> 00:17:35,760
you're extending you know hash map then

00:17:33,780 --> 00:17:37,730
you decide oh I'm switching to the trees

00:17:35,760 --> 00:17:40,170
or how does your optimization works

00:17:37,730 --> 00:17:42,870
really awesome to hear so the

00:17:40,170 --> 00:17:45,810
atomization will be actually we hope to

00:17:42,870 --> 00:17:48,380
push it into the main repo up some point

00:17:45,810 --> 00:17:50,730
in the future close future hopefully and

00:17:48,380 --> 00:17:53,340
basically optimization the way works

00:17:50,730 --> 00:17:55,650
right now is without going into too much

00:17:53,340 --> 00:17:58,830
detail but the hashmap basically right

00:17:55,650 --> 00:18:02,640
now creates a linked list in each one of

00:17:58,830 --> 00:18:04,530
the entries and with that if you are

00:18:02,640 --> 00:18:05,820
really really lucky as the genomic guys

00:18:04,530 --> 00:18:07,770
are really really lucky with their

00:18:05,820 --> 00:18:11,220
hashes because the hashes as you guys

00:18:07,770 --> 00:18:13,800
know genome I didn't notice but genome

00:18:11,220 --> 00:18:15,960
information has four letters and they

00:18:13,800 --> 00:18:19,410
always hashing the hitting the same for

00:18:15,960 --> 00:18:21,600
hashes so only for hash astonishing were

00:18:19,410 --> 00:18:24,270
very big hashmap not only um it's not

00:18:21,600 --> 00:18:26,160
actually had a percent away i'm always

00:18:24,270 --> 00:18:28,590
simplifying i'm super simple super

00:18:26,160 --> 00:18:30,750
oversimplifying but basically idea is

00:18:28,590 --> 00:18:32,870
that because of the data pattern where

00:18:30,750 --> 00:18:35,970
they use the hash map they were actually

00:18:32,870 --> 00:18:39,060
clocking the hashmap much more than they

00:18:35,970 --> 00:18:40,320
should and we found that and they were

00:18:39,060 --> 00:18:42,900
actually saturating a linked list so

00:18:40,320 --> 00:18:44,270
they were by hitting the hash when they

00:18:42,900 --> 00:18:47,310
were always going through a linked list

00:18:44,270 --> 00:18:49,830
so we change that so right now if if the

00:18:47,310 --> 00:18:52,500
linked list gets too long we change that

00:18:49,830 --> 00:18:55,590
link list to a tree a Redback tree a

00:18:52,500 --> 00:18:57,000
self-balancing three so goodbye going

00:18:55,590 --> 00:18:59,040
through three it's much faster than a

00:18:57,000 --> 00:19:01,430
linked list right so that's just how we

00:18:59,040 --> 00:19:01,430
did it

00:19:10,450 --> 00:19:16,400
you know thank you for talk but maybe I

00:19:14,660 --> 00:19:25,280
missed something but where is the link

00:19:16,400 --> 00:19:28,549
to github okay so he will be on my

00:19:25,280 --> 00:19:31,400
Twitter sorry that I forgot included in

00:19:28,549 --> 00:19:35,059
the talk I would have I will just post

00:19:31,400 --> 00:19:39,409
it on get her on Twitter and the second

00:19:35,059 --> 00:19:42,620
question you show the test performance

00:19:39,409 --> 00:19:47,330
test is this test running in a spark in

00:19:42,620 --> 00:19:50,690
Lagos park yeah it is oh it's running in

00:19:47,330 --> 00:19:58,100
a synthetic environment this one and now

00:19:50,690 --> 00:20:01,700
add the code shows the code mm-hmm he's

00:19:58,100 --> 00:20:03,770
gone yeah so you this is an example of

00:20:01,700 --> 00:20:06,440
benchmark and you prepare some data and

00:20:03,770 --> 00:20:09,049
next performs a test and is this test

00:20:06,440 --> 00:20:14,210
running inside the spark vocals park oh

00:20:09,049 --> 00:20:16,580
it distributed spark or lock it or just

00:20:14,210 --> 00:20:19,880
run in into i don't know like synthetic

00:20:16,580 --> 00:20:22,490
environment like GV a metal so umm well

00:20:19,880 --> 00:20:26,720
first of all let me clarify a spark is

00:20:22,490 --> 00:20:27,919
running on top of the JVM yeah but I was

00:20:26,720 --> 00:20:32,030
just kidding or if there was a skinny

00:20:27,919 --> 00:20:35,480
butt too so this is what I meant by this

00:20:32,030 --> 00:20:38,030
is like this is how you once you have

00:20:35,480 --> 00:20:40,940
the hot spots analyzed or exactly from a

00:20:38,030 --> 00:20:42,679
spark you can grab that piece of code of

00:20:40,940 --> 00:20:47,510
hot methods and copy paste that hot

00:20:42,679 --> 00:20:49,309
method into this trade implementing this

00:20:47,510 --> 00:20:52,309
trade and this will be run in a

00:20:49,309 --> 00:20:55,100
synthetic environment but you mentioned

00:20:52,309 --> 00:20:58,700
that it's possible to check for our

00:20:55,100 --> 00:21:01,610
synchronization stuff data communication

00:20:58,700 --> 00:21:05,480
stuff into spark in previous slides

00:21:01,610 --> 00:21:08,000
samson life Sarah yeah synchronization

00:21:05,480 --> 00:21:10,490
oh yeah I mentioned that yes I mentioned

00:21:08,000 --> 00:21:14,390
that and how can i check and measure it

00:21:10,490 --> 00:21:16,610
with this in a synthetic environment in

00:21:14,390 --> 00:21:18,559
a synthetic environment well for that we

00:21:16,610 --> 00:21:20,000
use a thing called Java flight recorder

00:21:18,559 --> 00:21:22,190
so

00:21:20,000 --> 00:21:24,770
once you have instructed your hot

00:21:22,190 --> 00:21:29,660
methods if the hot methods expand that

00:21:24,770 --> 00:21:31,700
into different threads the you will hit

00:21:29,660 --> 00:21:33,560
the same synchronization methods as we

00:21:31,700 --> 00:21:36,170
have seen by the way in some hot methods

00:21:33,560 --> 00:21:38,390
and some benchmarks and Java flight

00:21:36,170 --> 00:21:40,370
recorder is very good at capturing those

00:21:38,390 --> 00:21:44,480
synchronization issues it will show you

00:21:40,370 --> 00:21:47,840
how many times those threads are just

00:21:44,480 --> 00:21:50,030
holding because of synchronization

00:21:47,840 --> 00:21:52,580
issues so that's how does how we obtain

00:21:50,030 --> 00:21:56,090
that data and the last of delegate

00:21:52,580 --> 00:21:58,970
rights it first you analyze the pattern

00:21:56,090 --> 00:22:01,760
of usage and next you are trying to find

00:21:58,970 --> 00:22:03,880
the bottlenecks and next put bottlenecks

00:22:01,760 --> 00:22:06,380
into your benchmark to and optimize it

00:22:03,880 --> 00:22:11,170
okay thank you rather than bottlenecks I

00:22:06,380 --> 00:22:11,170
would like to say hot methods okay

00:22:20,220 --> 00:22:26,870
I guess this is going to be so thank

00:22:23,039 --> 00:22:26,870
everyone we're coming

00:22:48,210 --> 00:22:50,270

YouTube URL: https://www.youtube.com/watch?v=2zciHuH4T_k


