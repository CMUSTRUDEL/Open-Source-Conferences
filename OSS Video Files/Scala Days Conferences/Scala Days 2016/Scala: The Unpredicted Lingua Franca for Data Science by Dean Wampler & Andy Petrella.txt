Title: Scala: The Unpredicted Lingua Franca for Data Science by Dean Wampler & Andy Petrella
Publication date: 2017-01-19
Playlist: Scala Days 2016
Description: 
	This video was recorded at Scala Days Berlin 2016
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

Anstract:
It was true that, until pretty recently, the language of choice to manipulate and to make sense out of the data for Data Scientists was mainly one of Python, R or Matlab. This lead to split in the communities and duplication of efforts in languages offering a similar set functionnaiity. Although, it was foreseen that Julia (for instance) could gather parts of these communities, an unexpected event happened: the amount of available data and the distributed technologies to handle them. Distributed technologies raised out of the blue by data engineer and most of them are using a convenient and easy to deploy platform, the JVM. In this talk, we’ll show how the Data Scientists are now part of an heterogeneous team that has to face many problems and have to work towards a global solution together. This is including a new responsibility to be productive and agile in order to have their work integrated into the platform. This is why technologies like Apache Spark is so important nowadays and is gaining this traction from different communities. And even though some binding are available to legacy languages there, all the creativity in new ways to analyse the data has to be done in scala. So that, the second part of this talk will introduce and summarize all the new methodologies and scientific advances in machine learning done Scala as the main language, rather than others. We’ll demonstrate that all using the right tooling for Data Scientists which is enabling interactivity, live reactivity, charting capabilities and robustness in Scala, something there were still missing from the legacy languages. Hence, the examples will be provided and shown in a fully productive and reproducible environment combining the Spark Notebook and Docker.
Captions: 
	00:00:04,580 --> 00:00:14,199
hello everyone she was like yeah he was

00:00:11,120 --> 00:00:17,119
he doesn't work with my kids another way

00:00:14,199 --> 00:00:20,930
hello everyone as I said I'm very

00:00:17,119 --> 00:00:23,810
pleased to close this day talking about

00:00:20,930 --> 00:00:26,090
data science and especially why Scala is

00:00:23,810 --> 00:00:30,650
most very the favorite language that we

00:00:26,090 --> 00:00:33,710
have to to use in this new area at least

00:00:30,650 --> 00:00:36,489
for the language I'm very pleased to to

00:00:33,710 --> 00:00:39,680
do it with President Snow from line Bend

00:00:36,489 --> 00:00:44,300
and myself from the from the zoo of

00:00:39,680 --> 00:00:46,390
Belgium if you know it on vex regret zoo

00:00:44,300 --> 00:00:50,059
you should go there so I'm giving show

00:00:46,390 --> 00:00:52,460
every day there anyway

00:00:50,059 --> 00:01:01,910
so of course the new emperor who doesn't

00:00:52,460 --> 00:01:02,899
know what in one play yeah okay so I'm

00:01:01,910 --> 00:01:05,149
Andy Petrella

00:01:02,899 --> 00:01:09,560
from the company della fellows in

00:01:05,149 --> 00:01:11,509
Belgium so we mainly do data science we

00:01:09,560 --> 00:01:13,399
do you a lot of spark but we do data

00:01:11,509 --> 00:01:15,200
science and data science I won't expand

00:01:13,399 --> 00:01:18,469
on that right now but this is in anyway

00:01:15,200 --> 00:01:20,060
we do right so light band yeah of course

00:01:18,469 --> 00:01:23,209
you guys know what light bends all about

00:01:20,060 --> 00:01:25,279
and we started working with spark about

00:01:23,209 --> 00:01:28,759
a year ago because it's written in Scala

00:01:25,279 --> 00:01:31,159
and kind of expanding what what we think

00:01:28,759 --> 00:01:33,709
is possible with there's really two

00:01:31,159 --> 00:01:35,569
kinds of people you run into in in this

00:01:33,709 --> 00:01:37,310
big data world there's the data

00:01:35,569 --> 00:01:39,679
engineers who are doing the plumbing

00:01:37,310 --> 00:01:42,009
they're closest to what we typically do

00:01:39,679 --> 00:01:44,929
and then there's the data scientists so

00:01:42,009 --> 00:01:48,619
the data engineers have largely picked

00:01:44,929 --> 00:01:49,579
Scala as the tool of choice in both

00:01:48,619 --> 00:01:52,340
Kafka and

00:01:49,579 --> 00:01:54,319
spark or written along with other tools

00:01:52,340 --> 00:01:56,060
and now we're starting to see data

00:01:54,319 --> 00:02:02,659
scientists take an interest in it as

00:01:56,060 --> 00:02:04,369
well so Jenna three reasons I will cover

00:02:02,659 --> 00:02:06,819
them in two minutes and then it's okay

00:02:04,369 --> 00:02:09,580
so we can fold it and go back home

00:02:06,819 --> 00:02:12,349
no actually the the talk will be in

00:02:09,580 --> 00:02:14,180
several force the first four will be a

00:02:12,349 --> 00:02:17,150
few reasons about why Scala and then we

00:02:14,180 --> 00:02:17,890
will have examples running straight in a

00:02:17,150 --> 00:02:22,300
notebook

00:02:17,890 --> 00:02:24,730
so let's start with the reasons so there

00:02:22,300 --> 00:02:27,130
are three reasons so why I've been as

00:02:24,730 --> 00:02:28,690
recently at strata why the data signs

00:02:27,130 --> 00:02:29,880
need a new language and I said yeah

00:02:28,690 --> 00:02:32,980
actually doesn't need a new language

00:02:29,880 --> 00:02:37,390
first it needs a new platform it needs a

00:02:32,980 --> 00:02:39,580
new runtime so far our MATLAB Python and

00:02:37,390 --> 00:02:41,320
so on were used but actually most of the

00:02:39,580 --> 00:02:41,740
time decent women are not Enterprise

00:02:41,320 --> 00:02:44,620
already

00:02:41,740 --> 00:02:48,850
however the JVM is 20 plus years now

00:02:44,620 --> 00:02:51,400
approved and improved by 10 plus million

00:02:48,850 --> 00:02:53,320
people around so that means that a lot

00:02:51,400 --> 00:02:56,140
of people has focused their intention on

00:02:53,320 --> 00:02:59,050
enterprise ready platform for services

00:02:56,140 --> 00:03:00,610
for tooling for software whatsoever that

00:02:59,050 --> 00:03:04,000
means that JVM is most probably one of

00:03:00,610 --> 00:03:07,540
the best choices around to now taking

00:03:04,000 --> 00:03:10,750
over the data science way max functional

00:03:07,540 --> 00:03:14,020
of course I mean now we are dealing a

00:03:10,750 --> 00:03:15,790
lot with big data or fast data or a lot

00:03:14,020 --> 00:03:18,010
of models and sampling models deep

00:03:15,790 --> 00:03:19,870
learning and so on that means that we

00:03:18,010 --> 00:03:22,030
cannot really enter that onto a single

00:03:19,870 --> 00:03:24,790
machine if we cannot do that into a

00:03:22,030 --> 00:03:26,980
single machine maybe we can take it over

00:03:24,790 --> 00:03:28,450
and then distribute the data and rather

00:03:26,980 --> 00:03:31,299
than taking the data back to the

00:03:28,450 --> 00:03:34,660
computer right it's better to send the

00:03:31,299 --> 00:03:36,519
data send a function sorry to the data

00:03:34,660 --> 00:03:39,220
so we have to write functions and

00:03:36,519 --> 00:03:41,260
serialize them and send them over so

00:03:39,220 --> 00:03:43,959
it's better to do that into a functional

00:03:41,260 --> 00:03:46,299
language so we know that in Java for

00:03:43,959 --> 00:03:48,580
instance is very painful to do it so

00:03:46,299 --> 00:03:50,130
rather than maybe taking Java on the JVM

00:03:48,580 --> 00:03:53,140
is better to take scanner

00:03:50,130 --> 00:03:54,549
because it's simple it's simple so we

00:03:53,140 --> 00:03:57,220
might think okay so there is also

00:03:54,549 --> 00:03:59,709
closure but actually the thing is that

00:03:57,220 --> 00:04:02,500
closure is very far from what the people

00:03:59,709 --> 00:04:05,320
know already if you think about Matei

00:04:02,500 --> 00:04:08,739
right so you want it to test Messrs he

00:04:05,320 --> 00:04:10,540
created spark did eating in in scanner

00:04:08,739 --> 00:04:12,660
because of Julian and also because

00:04:10,540 --> 00:04:15,670
Canada is very simple you can read it

00:04:12,660 --> 00:04:18,820
while you're just a Java or Python

00:04:15,670 --> 00:04:21,040
Deborah is very close to them so this

00:04:18,820 --> 00:04:24,660
these are the three reasons I would use

00:04:21,040 --> 00:04:27,280
when having asked okay why scanner now

00:04:24,660 --> 00:04:29,710
this winning opera will explain a bit

00:04:27,280 --> 00:04:31,180
more detail why scanner why the feature

00:04:29,710 --> 00:04:33,880
of scanner a very

00:04:31,180 --> 00:04:35,590
and for data scientists yeah the way I'm

00:04:33,880 --> 00:04:39,220
going to do this is actually with the

00:04:35,590 --> 00:04:45,490
tool that Andy wrote called the Smart

00:04:39,220 --> 00:04:47,050
Notebook so spark notebook so if you

00:04:45,490 --> 00:04:48,340
come from the data science world you're

00:04:47,050 --> 00:04:50,080
actually used to working in notebooks

00:04:48,340 --> 00:04:52,750
and this is sort of the electronic

00:04:50,080 --> 00:04:54,340
analog of the lab notebooks that you may

00:04:52,750 --> 00:04:57,160
have used when you took chemistry in

00:04:54,340 --> 00:04:59,800
like college or whatever where you you

00:04:57,160 --> 00:05:01,750
intersperse experimental results and

00:04:59,800 --> 00:05:04,180
graphs and you know comments and

00:05:01,750 --> 00:05:05,979
documentation and it's a very nice way

00:05:04,180 --> 00:05:08,650
to work I actually think that developers

00:05:05,979 --> 00:05:11,440
would should actually use these two

00:05:08,650 --> 00:05:13,900
especially in situations where you would

00:05:11,440 --> 00:05:15,580
use the scholar repple to do like

00:05:13,900 --> 00:05:17,289
interactive exploring and you'll see why

00:05:15,580 --> 00:05:19,240
it's a really nice way to actually work

00:05:17,289 --> 00:05:20,470
when you're just basically working with

00:05:19,240 --> 00:05:22,900
the repple which is what it's

00:05:20,470 --> 00:05:24,340
effectively doing so the you know you

00:05:22,900 --> 00:05:27,580
can organize these in directory so we

00:05:24,340 --> 00:05:29,080
have three of them here this one is the

00:05:27,580 --> 00:05:30,970
one that I wrote I actually included a

00:05:29,080 --> 00:05:36,010
PDF so if you go to that link that you

00:05:30,970 --> 00:05:37,449
just saw and you come up to the git repo

00:05:36,010 --> 00:05:39,039
with these notebooks you can actually

00:05:37,449 --> 00:05:41,260
just look at the PDF for this stuff

00:05:39,039 --> 00:05:46,060
without actually running a spark

00:05:41,260 --> 00:05:50,800
notebook there we go I think I launched

00:05:46,060 --> 00:05:52,150
it twice one of these that's it all

00:05:50,800 --> 00:05:54,610
right so what you can see here these are

00:05:52,150 --> 00:05:56,949
cells and each of these cells are

00:05:54,610 --> 00:05:59,110
actually just marked down so you can

00:05:56,949 --> 00:06:00,849
intersperse comments and markdown you

00:05:59,110 --> 00:06:02,440
can actually you know hook in things

00:06:00,849 --> 00:06:04,479
like la tech if you want to do

00:06:02,440 --> 00:06:07,210
mathematics and that kind of stuff and

00:06:04,479 --> 00:06:08,889
there's shift and so then I can evaluate

00:06:07,210 --> 00:06:10,720
these if I do shift enter

00:06:08,889 --> 00:06:13,320
they'll just evaluate each of these

00:06:10,720 --> 00:06:15,310
cells here it's just parsing the

00:06:13,320 --> 00:06:17,470
markdown so it's not too much this

00:06:15,310 --> 00:06:19,900
exciting but we'll see in a second we'll

00:06:17,470 --> 00:06:22,270
actually get to assume some real code so

00:06:19,900 --> 00:06:24,550
we've done this talk several times now -

00:06:22,270 --> 00:06:26,169
as you can see this bullet put list here

00:06:24,550 --> 00:06:28,330
which could be a little bigger let's

00:06:26,169 --> 00:06:33,820
actually make that a little bigger there

00:06:28,330 --> 00:06:35,830
it is okay so when we did us for a data

00:06:33,820 --> 00:06:37,780
audience I tend to describe the Scala

00:06:35,830 --> 00:06:39,940
features but presumably all of you know

00:06:37,780 --> 00:06:41,380
the Scala features already so I'm just

00:06:39,940 --> 00:06:44,440
going to highlight why they're more

00:06:41,380 --> 00:06:44,920
useful in the data context the way this

00:06:44,440 --> 00:06:46,990
notebook

00:06:44,920 --> 00:06:49,480
organized I actually turned it in kind

00:06:46,990 --> 00:06:52,120
of into a very long list of why scowl is

00:06:49,480 --> 00:06:54,880
better than Java but a lot of that stuff

00:06:52,120 --> 00:06:57,460
isn't so important for data science and

00:06:54,880 --> 00:06:59,080
for spark in particular for example you

00:06:57,460 --> 00:07:01,210
really don't write recursive functions

00:06:59,080 --> 00:07:03,250
very often in SPARC because typically

00:07:01,210 --> 00:07:05,110
you have this big system that's you're

00:07:03,250 --> 00:07:07,390
running through your data in parallel

00:07:05,110 --> 00:07:08,800
and you don't actually do recursion much

00:07:07,390 --> 00:07:10,570
but that's a really you know the tail

00:07:08,800 --> 00:07:13,510
recursion optimization is a great

00:07:10,570 --> 00:07:16,090
feature for us in general but it's not

00:07:13,510 --> 00:07:18,820
so useful for SPARC so that's the second

00:07:16,090 --> 00:07:20,440
half of this this notebook here and we

00:07:18,820 --> 00:07:23,470
won't go through that we'll just focus

00:07:20,440 --> 00:07:26,770
on why's Scala is really nice for data

00:07:23,470 --> 00:07:28,600
science alright so if I scroll down a

00:07:26,770 --> 00:07:31,030
little bit the first one out that's

00:07:28,600 --> 00:07:35,080
pretty obvious when you think about it

00:07:31,030 --> 00:07:37,210
is functional programming and that

00:07:35,080 --> 00:07:40,240
includes things like you know the

00:07:37,210 --> 00:07:42,430
immutability of results no side effects

00:07:40,240 --> 00:07:43,930
and things like that and it turns out as

00:07:42,430 --> 00:07:45,850
you'll see in a second if you don't

00:07:43,930 --> 00:07:50,290
already know this it's the most concise

00:07:45,850 --> 00:07:52,180
way to work with data for me big data or

00:07:50,290 --> 00:07:54,580
fast date or whatever term you want to

00:07:52,180 --> 00:07:56,650
use is really the killer app for

00:07:54,580 --> 00:07:58,900
functional programming more so than

00:07:56,650 --> 00:08:00,550
concurrency and I think that turned out

00:07:58,900 --> 00:08:02,550
to be true because more people are

00:08:00,550 --> 00:08:04,420
having to learn how to work with data

00:08:02,550 --> 00:08:06,490
than they're having to learn how to

00:08:04,420 --> 00:08:09,370
write concurrent code we know we always

00:08:06,490 --> 00:08:11,680
seem to find a way to avoid that problem

00:08:09,370 --> 00:08:13,210
with toolkits like akka or whatever but

00:08:11,680 --> 00:08:14,920
this is really forcing people to

00:08:13,210 --> 00:08:17,020
recognize that you know sort of old

00:08:14,920 --> 00:08:18,940
object-oriented thinking isn't optimal

00:08:17,020 --> 00:08:21,190
for the modern world

00:08:18,940 --> 00:08:22,960
so anyway going a little farther here so

00:08:21,190 --> 00:08:24,700
that's the first one and I have some

00:08:22,960 --> 00:08:27,730
sections that I'll just briefly comment

00:08:24,700 --> 00:08:29,500
in today but I describe the other three

00:08:27,730 --> 00:08:32,740
languages that are used with SPARC

00:08:29,500 --> 00:08:35,110
Python R and Java and you know how well

00:08:32,740 --> 00:08:37,450
they support these particular topics and

00:08:35,110 --> 00:08:40,510
really among these Java's getting better

00:08:37,450 --> 00:08:42,700
as we know Python is sort of a mixed bag

00:08:40,510 --> 00:08:45,790
but nothing really feels as functional

00:08:42,700 --> 00:08:47,530
of these four as Scala does and it

00:08:45,790 --> 00:08:48,970
becomes one of these things where you

00:08:47,530 --> 00:08:51,010
really want to get into this mode of

00:08:48,970 --> 00:08:52,300
thinking about transformations of data

00:08:51,010 --> 00:08:54,730
and you don't want to have to like

00:08:52,300 --> 00:08:56,740
context switch and think how do I map

00:08:54,730 --> 00:08:58,520
this to objects how do we map this to

00:08:56,740 --> 00:09:01,160
some weirdness about the Python

00:08:58,520 --> 00:09:02,510
API etc it lets you think in the in the

00:09:01,160 --> 00:09:03,860
idioms that you want to think in and

00:09:02,510 --> 00:09:06,050
then just write them down which is

00:09:03,860 --> 00:09:07,850
really valuable and there's a few a

00:09:06,050 --> 00:09:09,140
sub-bullets here of things that people

00:09:07,850 --> 00:09:11,540
have mentioned to me when I've given

00:09:09,140 --> 00:09:13,910
this talk before how like traits and

00:09:11,540 --> 00:09:16,040
interfaces make it easy to compose code

00:09:13,910 --> 00:09:18,350
and have reasonable behavior and mix in

00:09:16,040 --> 00:09:21,050
composition which benefits big data

00:09:18,350 --> 00:09:23,120
projects as well as normal projects and

00:09:21,050 --> 00:09:25,400
also if you're using the java stream API

00:09:23,120 --> 00:09:26,930
it's a more functional API now but you

00:09:25,400 --> 00:09:29,570
find yourself flipping back and forth

00:09:26,930 --> 00:09:31,220
between that and the old-style Java

00:09:29,570 --> 00:09:33,830
collections and that's also one of those

00:09:31,220 --> 00:09:35,690
things of unnecessary detail that you

00:09:33,830 --> 00:09:39,140
have to keep in your mind as you're

00:09:35,690 --> 00:09:41,060
working with stuff but really when at

00:09:39,140 --> 00:09:42,860
the end of the day Matassa haria when he

00:09:41,060 --> 00:09:44,690
created spark said I'm gonna use Scala

00:09:42,860 --> 00:09:46,760
because I really love this the Scala

00:09:44,690 --> 00:09:48,710
collections they have the right you know

00:09:46,760 --> 00:09:50,360
metaphors for what I'm trying to do so

00:09:48,710 --> 00:09:52,940
let's see what that looks like I'm gonna

00:09:50,360 --> 00:09:55,040
start with a naive algorithm for

00:09:52,940 --> 00:09:57,530
computing primes this is our first

00:09:55,040 --> 00:09:59,240
actual code and you know when I

00:09:57,530 --> 00:10:01,190
sometimes I've selected that I haven't

00:09:59,240 --> 00:10:02,930
evaluated and as soon as I evaluate it

00:10:01,190 --> 00:10:04,460
now I've got this thing was parsed by

00:10:02,930 --> 00:10:10,070
the repple and I have this function I

00:10:04,460 --> 00:10:11,720
can use and here's a kind of a silly

00:10:10,070 --> 00:10:13,520
example but one that will look almost

00:10:11,720 --> 00:10:15,070
identical in spark where I'm just gonna

00:10:13,520 --> 00:10:17,990
take the numbers from one to a hundred

00:10:15,070 --> 00:10:19,610
map over them to create a tuple you know

00:10:17,990 --> 00:10:21,770
with you know the number and whether or

00:10:19,610 --> 00:10:24,050
not it's prime then group by whether

00:10:21,770 --> 00:10:25,670
it's prime sewing's up with two records

00:10:24,050 --> 00:10:27,860
you know for the throughs and the false

00:10:25,670 --> 00:10:30,560
is and then I'll print out a final

00:10:27,860 --> 00:10:32,840
tupple of you know true/false and the

00:10:30,560 --> 00:10:34,700
size it turns out this is something I

00:10:32,840 --> 00:10:37,400
did not know until I did this that

00:10:34,700 --> 00:10:38,870
exactly one-third are not prime and

00:10:37,400 --> 00:10:41,570
one-third are prime when you go exactly

00:10:38,870 --> 00:10:45,050
from one to a hundred inclusive which is

00:10:41,570 --> 00:10:46,730
kind of an interesting result and here's

00:10:45,050 --> 00:10:48,860
what the thing looks like in spark and

00:10:46,730 --> 00:10:50,870
it's nearly identical the three lines in

00:10:48,860 --> 00:10:52,790
the middle that are indented I just

00:10:50,870 --> 00:10:54,650
copied and pasted the only thing that's

00:10:52,790 --> 00:10:56,540
different are I have to set it up in a

00:10:54,650 --> 00:10:58,400
different way you start with this thing

00:10:56,540 --> 00:11:01,160
called a spark context that knows how to

00:10:58,400 --> 00:11:02,690
connect to your cluster and all that you

00:11:01,160 --> 00:11:04,700
take the range and you turn it into

00:11:02,690 --> 00:11:06,980
sparks own data structure called a

00:11:04,700 --> 00:11:09,290
resilient distributed data set with this

00:11:06,980 --> 00:11:11,240
parallelized method then you do the same

00:11:09,290 --> 00:11:12,350
operations that you already know that

00:11:11,240 --> 00:11:14,990
you've already you know learn

00:11:12,350 --> 00:11:16,280
but being a scholar programmer the one

00:11:14,990 --> 00:11:18,800
thing that's different at the end though

00:11:16,280 --> 00:11:20,510
is that these are lazy operations we're

00:11:18,800 --> 00:11:23,390
actually building up an abstract syntax

00:11:20,510 --> 00:11:26,540
tree it's only evaluated when we asked

00:11:23,390 --> 00:11:29,510
for results collect actually takes the

00:11:26,540 --> 00:11:31,370
RTD turns it into a regular Scala

00:11:29,510 --> 00:11:33,530
collection and brings it back into my

00:11:31,370 --> 00:11:37,880
local driver program which is this

00:11:33,530 --> 00:11:40,160
notebook so when I run this I'll get the

00:11:37,880 --> 00:11:41,420
same result hopefully and there it is

00:11:40,160 --> 00:11:44,180
and you can see it did a little bit

00:11:41,420 --> 00:11:46,910
fancier stuff and spark notebook has

00:11:44,180 --> 00:11:49,970
these nice features of when you return

00:11:46,910 --> 00:11:52,100
an RTD it will display in a table format

00:11:49,970 --> 00:11:53,840
and you can do things like you know look

00:11:52,100 --> 00:11:56,000
at histograms which are kind of boring

00:11:53,840 --> 00:12:00,140
at this in this case but interesting

00:11:56,000 --> 00:12:01,490
nonetheless all right all right so I

00:12:00,140 --> 00:12:04,580
mentioned when our TVs are I won't

00:12:01,490 --> 00:12:06,350
belabor that point they are the fancy

00:12:04,580 --> 00:12:08,270
thing that looked like you know vectors

00:12:06,350 --> 00:12:10,340
or something but in fact are distributed

00:12:08,270 --> 00:12:12,110
over I collect a cluster and have some

00:12:10,340 --> 00:12:15,710
resilience properties which is what the

00:12:12,110 --> 00:12:18,140
R is for and you know the API for Python

00:12:15,710 --> 00:12:20,090
is nearly the same I kind of joked that

00:12:18,140 --> 00:12:22,520
if you used a color scheme where curly

00:12:20,090 --> 00:12:24,410
braces were white on white it would look

00:12:22,520 --> 00:12:26,240
like Python code when you write spark

00:12:24,410 --> 00:12:29,180
code it's it's nearly that it's almost

00:12:26,240 --> 00:12:31,550
that exact close but none of the other

00:12:29,180 --> 00:12:33,560
API is for R and Java are nearly as

00:12:31,550 --> 00:12:34,880
concise and elegant and that becomes so

00:12:33,560 --> 00:12:37,450
important when you're trying to build

00:12:34,880 --> 00:12:39,980
non-trivial algorithms and this stuff

00:12:37,450 --> 00:12:41,510
here's an advantage we all know and love

00:12:39,980 --> 00:12:43,760
which is the fact that we can do this

00:12:41,510 --> 00:12:45,470
interpreted in experience we can

00:12:43,760 --> 00:12:48,020
experiment with our data we can run

00:12:45,470 --> 00:12:50,120
interactive queries we can go back to

00:12:48,020 --> 00:12:51,830
previous code in the notebook and edit

00:12:50,120 --> 00:12:53,690
it easily and rerun it that's one thing

00:12:51,830 --> 00:12:56,000
they really love about notebooks versus

00:12:53,690 --> 00:12:58,280
the raw repple but it's the same kind of

00:12:56,000 --> 00:13:01,280
experience that we of course have known

00:12:58,280 --> 00:13:02,840
for a long time in Python and R and we

00:13:01,280 --> 00:13:04,670
are now going to get this in Java you

00:13:02,840 --> 00:13:06,590
can actually download the latest you

00:13:04,670 --> 00:13:08,540
know JDK build for Java 9 and there's

00:13:06,590 --> 00:13:11,480
actually an interpreter in it just kind

00:13:08,540 --> 00:13:15,110
of it only took 20 years having no kind

00:13:11,480 --> 00:13:16,760
of amazing here's something that is you

00:13:15,110 --> 00:13:19,040
know it's sort of this mundane thing we

00:13:16,760 --> 00:13:21,200
take for granted but my gosh it makes so

00:13:19,040 --> 00:13:22,970
much sense when you're writing data

00:13:21,200 --> 00:13:24,440
stuff it's often true that you just want

00:13:22,970 --> 00:13:25,339
to reuse a couple to represent your

00:13:24,440 --> 00:13:29,269
records

00:13:25,339 --> 00:13:31,339
and it's so easy to do this in with

00:13:29,269 --> 00:13:32,689
Scala so this is basically what we've

00:13:31,339 --> 00:13:34,040
been doing I've been just using

00:13:32,689 --> 00:13:36,079
parentheses to build you know two

00:13:34,040 --> 00:13:38,810
element tuples and just forget about it

00:13:36,079 --> 00:13:41,089
I just do it and it's done so it's it's

00:13:38,810 --> 00:13:44,120
just an incredible advantage whereas if

00:13:41,089 --> 00:13:45,860
you use Java you know you don't get this

00:13:44,120 --> 00:13:48,139
at all in the library and spark'd

00:13:45,860 --> 00:13:49,850
actually added its own sort of they call

00:13:48,139 --> 00:13:51,559
it a mutable pair type which is what

00:13:49,850 --> 00:13:53,029
this is you have to do this explicitly

00:13:51,559 --> 00:13:56,180
if you want to work with tuples so it's

00:13:53,029 --> 00:13:58,670
kind of stupid not to have that

00:13:56,180 --> 00:14:00,019
convenience that we just saw the other

00:13:58,670 --> 00:14:01,790
thing that's incredibly important is

00:14:00,019 --> 00:14:04,249
pattern matching there's so many times

00:14:01,790 --> 00:14:05,959
then you have some structure some record

00:14:04,249 --> 00:14:08,569
and you want to rip it apart and

00:14:05,959 --> 00:14:10,879
restructure it or throw things away you

00:14:08,569 --> 00:14:13,249
this is basically the same program we've

00:14:10,879 --> 00:14:14,839
been running or I just rewrote it using

00:14:13,249 --> 00:14:17,300
pattern matching you know kind of a

00:14:14,839 --> 00:14:19,430
trivial example again but nonetheless

00:14:17,300 --> 00:14:22,339
it's so nice that I can now use

00:14:19,430 --> 00:14:23,899
meaningful names for these fields and

00:14:22,339 --> 00:14:25,759
once you know the pattern matching stuff

00:14:23,899 --> 00:14:27,410
as we all know it's just really concise

00:14:25,759 --> 00:14:29,480
to think about what you want to do with

00:14:27,410 --> 00:14:32,750
that record rip it apart put it back

00:14:29,480 --> 00:14:34,160
together and you're done and lo and

00:14:32,750 --> 00:14:37,519
behold we got the same result which is

00:14:34,160 --> 00:14:39,170
kind of amazing and so I'd explain what

00:14:37,519 --> 00:14:41,360
all these pattern matching stuff means

00:14:39,170 --> 00:14:43,339
but this is familiar stuff and I just

00:14:41,360 --> 00:14:46,100
love this too that if you know the exact

00:14:43,339 --> 00:14:47,660
structure of your object you can just

00:14:46,100 --> 00:14:49,879
rip the whole thing apart no matter how

00:14:47,660 --> 00:14:52,220
deeply nested it is and you're not

00:14:49,879 --> 00:14:54,559
limited of course two built-in types

00:14:52,220 --> 00:14:57,620
because of case classes our old friends

00:14:54,559 --> 00:15:01,069
here you know we can do records as case

00:14:57,620 --> 00:15:02,839
objects or case instances rather we get

00:15:01,069 --> 00:15:04,550
all this other stuff for free that we

00:15:02,839 --> 00:15:06,769
love and then we can do pattern matching

00:15:04,550 --> 00:15:09,199
on that so it's pretty common in spark

00:15:06,769 --> 00:15:10,250
like if I'm reading raw text files like

00:15:09,199 --> 00:15:12,379
say log files

00:15:10,250 --> 00:15:14,360
I'll just parse them into a case class

00:15:12,379 --> 00:15:16,519
that represents the record and then I

00:15:14,360 --> 00:15:18,800
can do pattern matching to tease things

00:15:16,519 --> 00:15:21,470
apart like we're doing here so I made up

00:15:18,800 --> 00:15:23,240
this record with Andy and I and we both

00:15:21,470 --> 00:15:26,389
happen to be very young people still as

00:15:23,240 --> 00:15:29,540
you can tell and you know we get these

00:15:26,389 --> 00:15:31,189
you know our ages and whatnot so all

00:15:29,540 --> 00:15:32,990
just by pattern matching ripping things

00:15:31,189 --> 00:15:35,390
apart and we're done what do you mean

00:15:32,990 --> 00:15:38,580
forty what is this

00:15:35,390 --> 00:15:41,070
use this turn yeah you just did it you

00:15:38,580 --> 00:15:43,140
just I just could beat you with off one

00:15:41,070 --> 00:15:46,350
year more anyway alright I'll let it go

00:15:43,140 --> 00:15:47,940
this time and once again this is stuff

00:15:46,350 --> 00:15:50,250
you do not get in these other languages

00:15:47,940 --> 00:15:52,710
so it really makes it very concise to

00:15:50,250 --> 00:15:54,780
work with this code and for me it's just

00:15:52,710 --> 00:15:56,310
that it was yeah when I started doing

00:15:54,780 --> 00:15:57,990
Hadoop consulting it was a horrible

00:15:56,310 --> 00:16:00,330
experience because the tooling was so

00:15:57,990 --> 00:16:02,340
bad and it became fun when I started

00:16:00,330 --> 00:16:06,090
using tools like scalding and which is a

00:16:02,340 --> 00:16:08,210
precursor to spark spark type inference

00:16:06,090 --> 00:16:10,650
is an interesting thing in this case

00:16:08,210 --> 00:16:13,020
it's really for the same reasons that we

00:16:10,650 --> 00:16:15,150
know and love it already that it removes

00:16:13,020 --> 00:16:17,940
the ceremony of inserting all these type

00:16:15,150 --> 00:16:20,460
annotations yet we get type safety and

00:16:17,940 --> 00:16:21,960
we get feedback especially in using the

00:16:20,460 --> 00:16:23,820
repple about what we've just completed

00:16:21,960 --> 00:16:26,430
constructed so I'm sitting here trying

00:16:23,820 --> 00:16:28,080
to learn spark and I don't know quite

00:16:26,430 --> 00:16:30,300
what I got when I did this operation

00:16:28,080 --> 00:16:32,700
here but lo and behold it tells me I

00:16:30,300 --> 00:16:34,920
created an RTD of these two element

00:16:32,700 --> 00:16:36,780
tuples and I just get that information

00:16:34,920 --> 00:16:39,300
right away I found that when I was

00:16:36,780 --> 00:16:41,190
learning the Python API for SPARC I

00:16:39,300 --> 00:16:42,960
would inevitably he's like print out the

00:16:41,190 --> 00:16:44,250
first five elements of everything just

00:16:42,960 --> 00:16:46,590
to figure out what I just constructed

00:16:44,250 --> 00:16:48,270
because there was no feedback so this

00:16:46,590 --> 00:16:49,680
you know is something that we've all

00:16:48,270 --> 00:16:51,830
probably experienced but you really

00:16:49,680 --> 00:16:55,800
noticed this when working with them

00:16:51,830 --> 00:16:58,680
SPARC as well plus we get the extra type

00:16:55,800 --> 00:17:00,480
safety and of course Java gives this

00:16:58,680 --> 00:17:02,760
type safety too but then we have all the

00:17:00,480 --> 00:17:05,100
extra ceremony of adding types so forth

00:17:02,760 --> 00:17:07,140
it also is really convenient to have the

00:17:05,100 --> 00:17:09,930
unification of primitives and types this

00:17:07,140 --> 00:17:11,400
is you know s in contrast to Java where

00:17:09,930 --> 00:17:13,500
I can just say things like list of

00:17:11,400 --> 00:17:16,410
string list of integer or you know int

00:17:13,500 --> 00:17:18,510
float double string and my tuples and I

00:17:16,410 --> 00:17:20,940
don't have to like do these your boxing

00:17:18,510 --> 00:17:23,760
conversions or emanuelly or whatever

00:17:20,940 --> 00:17:27,150
it's just automatically treated in a

00:17:23,760 --> 00:17:28,470
consistent way the second to last point

00:17:27,150 --> 00:17:30,870
is the fact that we can do

00:17:28,470 --> 00:17:32,550
domain-specific language is really nice

00:17:30,870 --> 00:17:34,290
to spark this is a slightly more

00:17:32,550 --> 00:17:37,830
involved example so let me just walk

00:17:34,290 --> 00:17:39,660
through it so we don't have to do the

00:17:37,830 --> 00:17:42,150
import for sequel that we used to be in

00:17:39,660 --> 00:17:44,460
here cuz the sequel to point to the rain

00:17:42,150 --> 00:17:47,010
folder but oh you using the C okay yeah

00:17:44,460 --> 00:17:48,510
you're used to got it got it right yeah

00:17:47,010 --> 00:17:50,760
we've been editing this sort

00:17:48,510 --> 00:17:52,170
parallel lately so I'm gonna use an

00:17:50,760 --> 00:17:54,660
environment variable that actually tell

00:17:52,170 --> 00:17:56,190
us a notebook where I put my data set so

00:17:54,660 --> 00:17:59,640
let's evaluate that quickly

00:17:56,190 --> 00:18:01,740
you know just regular Scala I'm gonna

00:17:59,640 --> 00:18:03,660
write a the equivalent of a sequel query

00:18:01,740 --> 00:18:06,090
and this is amazing API that they've

00:18:03,660 --> 00:18:08,700
added on top of that our DD API where I

00:18:06,090 --> 00:18:11,490
can use this new reference object called

00:18:08,700 --> 00:18:13,350
a spark session I can get a reader and

00:18:11,490 --> 00:18:15,690
then I can just read Jason it'll parse

00:18:13,350 --> 00:18:17,910
it it'll figure out the scheme of the

00:18:15,690 --> 00:18:21,990
records in this case this is data for

00:18:17,910 --> 00:18:24,090
all the airports in North America and

00:18:21,990 --> 00:18:25,380
you know he just parses it and I'm done

00:18:24,090 --> 00:18:27,810
I don't have to think about it anymore

00:18:25,380 --> 00:18:31,550
you can see it's showing the schema here

00:18:27,810 --> 00:18:33,480
and you know obviously it leads to the

00:18:31,550 --> 00:18:36,620
results a little bit to fit in the

00:18:33,480 --> 00:18:38,700
screen so that's kind of a cool thing

00:18:36,620 --> 00:18:40,320
caching is a way of saying I'm going to

00:18:38,700 --> 00:18:42,180
go over the state and repeatedly so I

00:18:40,320 --> 00:18:44,250
want you to hold it in memory and not

00:18:42,180 --> 00:18:46,320
just force me to go back to disk every

00:18:44,250 --> 00:18:48,930
time and then I put in the second

00:18:46,320 --> 00:18:51,510
airports D F which is the object so it

00:18:48,930 --> 00:18:54,360
would print out a nice formatted table

00:18:51,510 --> 00:18:55,800
of the airport's and then I forget how

00:18:54,360 --> 00:18:59,130
many there are something like a thousand

00:18:55,800 --> 00:19:01,980
or something it's pretty big but and so

00:18:59,130 --> 00:19:03,780
this is by default I think 20 and then

00:19:01,980 --> 00:19:05,430
here's the point so we built up to this

00:19:03,780 --> 00:19:08,040
and now we can write the equivalent of a

00:19:05,430 --> 00:19:10,530
sequel query and this API the reason

00:19:08,040 --> 00:19:12,240
that this is the DSL section right this

00:19:10,530 --> 00:19:16,020
API is an idiomatic

00:19:12,240 --> 00:19:18,390
Scala version of an idiomatic - API

00:19:16,020 --> 00:19:21,600
called data frames which was itself

00:19:18,390 --> 00:19:24,210
copied from the our language API called

00:19:21,600 --> 00:19:26,400
data frames so that this is effectively

00:19:24,210 --> 00:19:28,260
a sequel query I'm gonna group by my

00:19:26,400 --> 00:19:30,630
state and country where these airports

00:19:28,260 --> 00:19:32,310
are located I'm gonna count the size of

00:19:30,630 --> 00:19:34,620
each groups and then I'm gonna print

00:19:32,310 --> 00:19:35,880
them out by count descending so you can

00:19:34,620 --> 00:19:38,340
just picture in your head with the

00:19:35,880 --> 00:19:39,930
sequel query you would look like if I

00:19:38,340 --> 00:19:42,720
had time I would go ahead and write it

00:19:39,930 --> 00:19:44,430
out - it should I had it print the

00:19:42,720 --> 00:19:46,950
schema so the schema is going to be

00:19:44,430 --> 00:19:49,110
three elements and then it's going to

00:19:46,950 --> 00:19:52,500
print the first hundred of them and it

00:19:49,110 --> 00:19:54,870
turns out Alaska has the most airports

00:19:52,500 --> 00:19:57,480
of any US state and they're mostly

00:19:54,870 --> 00:19:58,950
little like bush airports for people

00:19:57,480 --> 00:20:01,950
that are flying into the mountains and

00:19:58,950 --> 00:20:02,350
stuff like that so we can write the

00:20:01,950 --> 00:20:03,870
equivalent

00:20:02,350 --> 00:20:06,640
sequel queries there's some amazing

00:20:03,870 --> 00:20:08,049
optimizations behind the scenes but we

00:20:06,640 --> 00:20:09,880
could also either do this with a regular

00:20:08,049 --> 00:20:12,100
sequel query in a string which is

00:20:09,880 --> 00:20:16,900
strongly typed or we can use this

00:20:12,100 --> 00:20:19,000
idiomatic DSL up here to construct DSL

00:20:16,900 --> 00:20:22,360
that you know represents some concept we

00:20:19,000 --> 00:20:24,039
want so this last section 9 is the one

00:20:22,360 --> 00:20:26,530
that talks about things that really are

00:20:24,039 --> 00:20:30,039
benefits for our Scala in general but

00:20:26,530 --> 00:20:31,480
less important for data science and I'll

00:20:30,039 --> 00:20:33,640
let you read that on your own I'll just

00:20:31,480 --> 00:20:37,440
finish with a at the right direction

00:20:33,640 --> 00:20:39,669
here just a few final points about

00:20:37,440 --> 00:20:42,940
disadvantages of Scala just in the

00:20:39,669 --> 00:20:45,490
interest of full disclosure I can get to

00:20:42,940 --> 00:20:49,150
it here I have a lot of points in here

00:20:45,490 --> 00:20:51,039
now as you can see so by the way please

00:20:49,150 --> 00:20:52,630
send pull requests if you have any other

00:20:51,039 --> 00:20:55,570
things you think are better or worse

00:20:52,630 --> 00:20:57,190
about Scala versus Java let me know but

00:20:55,570 --> 00:21:00,640
the disadvantages there's really three a

00:20:57,190 --> 00:21:02,650
list here there's still a lot more tools

00:21:00,640 --> 00:21:05,049
for data science and Python and our

00:21:02,650 --> 00:21:07,210
that's the limitation we need to fix and

00:21:05,049 --> 00:21:08,799
the JVM itself has some issues and I'm

00:21:07,210 --> 00:21:11,500
going to get into this in detail and my

00:21:08,799 --> 00:21:13,780
talk tomorrow like the fact that arrays

00:21:11,500 --> 00:21:15,580
are indexed by integers which means we

00:21:13,780 --> 00:21:17,679
can only have two billion elements and

00:21:15,580 --> 00:21:19,900
if it's a byte array you only get two

00:21:17,679 --> 00:21:22,390
gigabytes of data when you know now you

00:21:19,900 --> 00:21:25,600
can get like a two terabyte Ram instance

00:21:22,390 --> 00:21:27,820
on Amazon so it's really bad that we are

00:21:25,600 --> 00:21:30,100
actually indexing by integers and then

00:21:27,820 --> 00:21:32,500
it turns out as beautiful as the JVM is

00:21:30,100 --> 00:21:34,809
at representing arbitrary objects and

00:21:32,500 --> 00:21:37,480
graphs like some person type I've got

00:21:34,809 --> 00:21:39,640
here it really sucks for garbage

00:21:37,480 --> 00:21:41,559
collection and performance and cache

00:21:39,640 --> 00:21:44,169
awareness and all that stuff when I've

00:21:41,559 --> 00:21:47,049
got billions of these things so sparked

00:21:44,169 --> 00:21:48,820
actually introduced a new encoding for

00:21:47,049 --> 00:21:50,320
objects that are records and I'll get

00:21:48,820 --> 00:21:52,990
into this a lot tomorrow if you're

00:21:50,320 --> 00:21:55,030
interested it turns out we've actually

00:21:52,990 --> 00:21:56,799
found some weird behaviors in the repple

00:21:55,030 --> 00:21:59,049
when you start loading gigabytes of data

00:21:56,799 --> 00:22:01,570
in the repple and i'll also talk about

00:21:59,049 --> 00:22:05,820
that but so those are teasers for

00:22:01,570 --> 00:22:07,929
tomorrow alright great so this was

00:22:05,820 --> 00:22:10,000
regarding these kind of features of

00:22:07,929 --> 00:22:11,590
course but has you said so we are still

00:22:10,000 --> 00:22:15,010
lacking of you know

00:22:11,590 --> 00:22:16,010
Meli tooling and scanner so a tooling

00:22:15,010 --> 00:22:18,830
and models may

00:22:16,010 --> 00:22:21,620
we are lacking of models however now

00:22:18,830 --> 00:22:24,350
it's maybe the good time to port

00:22:21,620 --> 00:22:27,110
existing models to scanner on the JVM

00:22:24,350 --> 00:22:29,540
but also it's a good time to invent new

00:22:27,110 --> 00:22:33,620
models because now we have the the power

00:22:29,540 --> 00:22:35,360
we have the use cases coming and yeah

00:22:33,620 --> 00:22:36,890
there is certainly a lot of different

00:22:35,360 --> 00:22:38,750
ways to solve the same problem into the

00:22:36,890 --> 00:22:43,100
science so it's the right time to come

00:22:38,750 --> 00:22:46,450
up with new solutions so regarding the

00:22:43,100 --> 00:22:48,830
tooling I would say that this is what I

00:22:46,450 --> 00:22:51,800
started with a spy notebook to be honest

00:22:48,830 --> 00:22:52,760
so I'm using spark for three years now

00:22:51,800 --> 00:22:55,460
or something

00:22:52,760 --> 00:22:58,670
since pond5 upon for something like that

00:22:55,460 --> 00:23:01,190
and very quickly I was fed up by using

00:22:58,670 --> 00:23:05,270
my rebel because I couldn't you reuse my

00:23:01,190 --> 00:23:07,940
work easily and I had always to go back

00:23:05,270 --> 00:23:09,770
in the history and so on it was very

00:23:07,940 --> 00:23:11,750
painful and also I didn't had any

00:23:09,770 --> 00:23:14,000
visualization and so on so and I didn't

00:23:11,750 --> 00:23:17,060
want it to go back in my history doing

00:23:14,000 --> 00:23:19,010
Python are again so I studied this

00:23:17,060 --> 00:23:22,000
project to this pine notebook which was

00:23:19,010 --> 00:23:25,040
initially based on the scanner notebook

00:23:22,000 --> 00:23:28,280
that I ported to many so many things

00:23:25,040 --> 00:23:30,830
into it and regarding these examples

00:23:28,280 --> 00:23:32,420
they are actually in this repository so

00:23:30,830 --> 00:23:35,780
you can fork it you can clone it and

00:23:32,420 --> 00:23:37,940
this repository is docker enable it so

00:23:35,780 --> 00:23:39,740
there is a docker file and there is a

00:23:37,940 --> 00:23:43,700
docker which is already published with

00:23:39,740 --> 00:23:47,450
Park 2.00 preview so you can use that

00:23:43,700 --> 00:23:49,700
one and also benefit from the developer

00:23:47,450 --> 00:23:52,760
installation to use directly the preview

00:23:49,700 --> 00:23:56,120
despite 2.00 which would be cut know the

00:23:52,760 --> 00:23:59,450
RC should be cut soon by the way it's

00:23:56,120 --> 00:24:03,470
still an RC wait so the next notebook

00:23:59,450 --> 00:24:05,510
that I'm going to cover is cold white

00:24:03,470 --> 00:24:07,280
spy notebook then so what do we you want

00:24:05,510 --> 00:24:13,000
to use the spark notebook instead MIDI

00:24:07,280 --> 00:24:13,000
holders so I mean to

00:24:13,799 --> 00:24:19,330
so the idea of the spy notebook is to

00:24:16,059 --> 00:24:21,910
fill the gap in their science that means

00:24:19,330 --> 00:24:24,130
that we bring a lot of things coming

00:24:21,910 --> 00:24:27,580
from what I used to use like our studio

00:24:24,130 --> 00:24:30,370
or or ipython at that time but directly

00:24:27,580 --> 00:24:33,040
on the JVM so it's a pretty successful

00:24:30,370 --> 00:24:35,679
project so far we had more time reaching

00:24:33,040 --> 00:24:38,860
now the one thousand and three hundred

00:24:35,679 --> 00:24:40,600
and fifty stars a lot of people is

00:24:38,860 --> 00:24:44,140
involved in the get er channel so we

00:24:40,600 --> 00:24:46,179
have almost 500 percent there now a lot

00:24:44,140 --> 00:24:48,309
of messages or a lot of activity there

00:24:46,179 --> 00:24:51,330
to help people not on your spy notebook

00:24:48,309 --> 00:24:54,490
but also spark related to work in the

00:24:51,330 --> 00:24:57,270
how to work in the in the in a spiral

00:24:54,490 --> 00:25:02,470
notebook so I will do a little bit of

00:24:57,270 --> 00:25:05,350
formatting so I can track my my flow so

00:25:02,470 --> 00:25:07,600
the thing is so why it's one of the

00:25:05,350 --> 00:25:09,400
biggest feature feature of the of the

00:25:07,600 --> 00:25:12,100
spy notebook is that it has the capacity

00:25:09,400 --> 00:25:14,799
to start multiple spark context that

00:25:12,100 --> 00:25:18,130
means that you won't suffer from clashes

00:25:14,799 --> 00:25:20,980
in the classpath why because each time

00:25:18,130 --> 00:25:23,290
you start a new a notebook so you when

00:25:20,980 --> 00:25:25,690
you click essentially on the on the on

00:25:23,290 --> 00:25:29,460
the notebook can you open this page it

00:25:25,690 --> 00:25:32,740
starts a dedicated JVM behind the scene

00:25:29,460 --> 00:25:35,530
so this JVM which is dedicated to this

00:25:32,740 --> 00:25:38,350
specific notebook is actually containing

00:25:35,530 --> 00:25:40,929
a spy driver which can define whatever

00:25:38,350 --> 00:25:43,270
classes whatever packages that you want

00:25:40,929 --> 00:25:46,299
to include into it and that can define

00:25:43,270 --> 00:25:48,610
how it will deal with with the clusters

00:25:46,299 --> 00:25:50,110
so have asking for a few more memory

00:25:48,610 --> 00:25:51,760
than the others for instance and so

00:25:50,110 --> 00:25:54,309
whatsoever so you can tune it the way

00:25:51,760 --> 00:25:56,440
you like it and you want clash with

00:25:54,309 --> 00:25:58,600
other people see you one blow than the

00:25:56,440 --> 00:26:00,549
driver because you have used more memory

00:25:58,600 --> 00:26:02,470
than the others so this is one of the

00:26:00,549 --> 00:26:05,500
biggest feature make which makes its way

00:26:02,470 --> 00:26:08,320
more stable than original or older

00:26:05,500 --> 00:26:09,850
notebooks let's say and of course by

00:26:08,320 --> 00:26:12,549
default when you started is going to

00:26:09,850 --> 00:26:14,020
create for you a few items ID objects

00:26:12,549 --> 00:26:16,900
like the specs session and also of

00:26:14,020 --> 00:26:21,070
course the legacy now and sparse context

00:26:16,900 --> 00:26:22,570
and is areas as see so you have them so

00:26:21,070 --> 00:26:25,660
you can start working working right

00:26:22,570 --> 00:26:28,270
ahead with this part so now a few things

00:26:25,660 --> 00:26:29,320
that means that I'm I like to have

00:26:28,270 --> 00:26:32,050
reproducibility

00:26:29,320 --> 00:26:34,390
into my process so I like to have my

00:26:32,050 --> 00:26:36,340
notebook to be a fully reproducible in

00:26:34,390 --> 00:26:38,410
to other environment that means that I'm

00:26:36,340 --> 00:26:40,030
defining a lot of things within the

00:26:38,410 --> 00:26:42,040
metadata and not the notebook itself

00:26:40,030 --> 00:26:43,840
right in the metadata of the notebooks I

00:26:42,040 --> 00:26:46,330
don't want it we don't want them to pull

00:26:43,840 --> 00:26:48,520
you to my environment so that means that

00:26:46,330 --> 00:26:51,430
here in the crystal in the metadata of

00:26:48,520 --> 00:26:53,980
the my notebook I have defined this spot

00:26:51,430 --> 00:26:57,220
configuration which we defined D for

00:26:53,980 --> 00:26:58,750
parallelism and if I normally should be

00:26:57,220 --> 00:27:01,360
eight because I have eight cores but

00:26:58,750 --> 00:27:03,670
here is four because I've parsed it and

00:27:01,360 --> 00:27:06,610
I've reconfigured the spot configuration

00:27:03,670 --> 00:27:09,490
on the fly based on this metadata also

00:27:06,610 --> 00:27:12,370
you can define your dependencies using

00:27:09,490 --> 00:27:15,670
an SBT like syntax so here for instance

00:27:12,370 --> 00:27:17,560
I've included the Kafka library and you

00:27:15,670 --> 00:27:19,300
it's very small but you don't see it

00:27:17,560 --> 00:27:21,370
that actually I will show it after but

00:27:19,300 --> 00:27:24,010
there is a specific scent act that

00:27:21,370 --> 00:27:26,410
allows us to use underscore in this

00:27:24,010 --> 00:27:28,540
metadata in order to catch all I will

00:27:26,410 --> 00:27:30,670
get back to the tab after so these

00:27:28,540 --> 00:27:32,200
dependencies are downloaded by the spy

00:27:30,670 --> 00:27:33,640
notebook and inject it into the class

00:27:32,200 --> 00:27:35,620
path so you don't have to do anything

00:27:33,640 --> 00:27:38,770
and also they are put into your spine

00:27:35,620 --> 00:27:40,630
two jars configuration variables so they

00:27:38,770 --> 00:27:42,100
will be also shipped to the cluster for

00:27:40,630 --> 00:27:44,620
you so you don't have to do anything

00:27:42,100 --> 00:27:46,240
about it that means that you can then

00:27:44,620 --> 00:27:49,330
use you know for instance the spark

00:27:46,240 --> 00:27:51,220
connector for Cassandra or the calf

00:27:49,330 --> 00:27:53,170
caliber is available out there so I

00:27:51,220 --> 00:27:56,200
didn't have to define anything but in

00:27:53,170 --> 00:27:58,690
the metadata so this can be configured

00:27:56,200 --> 00:28:02,230
using this panel and you see here I have

00:27:58,690 --> 00:28:04,870
used underscore there which is a word

00:28:02,230 --> 00:28:08,740
card for please use the current spot

00:28:04,870 --> 00:28:10,990
version like double you know double a

00:28:08,740 --> 00:28:13,360
person's in SBT means please use the

00:28:10,990 --> 00:28:15,960
current as beat scanner version and the

00:28:13,360 --> 00:28:18,580
score means please the year the current

00:28:15,960 --> 00:28:22,140
scanner spoke versions right and then

00:28:18,580 --> 00:28:22,140
you have the custom configuration

00:28:24,330 --> 00:28:28,810
actually something else which is very

00:28:26,770 --> 00:28:31,060
helpful so I was really quickly fed up

00:28:28,810 --> 00:28:34,810
as well going constantly to the cluster

00:28:31,060 --> 00:28:36,190
right to see my logs right measures you

00:28:34,810 --> 00:28:38,290
have to go to different machines see the

00:28:36,190 --> 00:28:39,399
logs or whatever here actually when you

00:28:38,290 --> 00:28:41,259
do something

00:28:39,399 --> 00:28:44,049
mad at beam Sparkle whatsoever

00:28:41,259 --> 00:28:46,269
everything is locked in the browser so

00:28:44,049 --> 00:28:48,519
you don't have to go to yarn to mess us

00:28:46,269 --> 00:28:49,989
anymore you can just open your browser

00:28:48,519 --> 00:28:52,330
and console and everything it will be

00:28:49,989 --> 00:28:55,299
out there and if you are Affairs you

00:28:52,330 --> 00:28:58,629
will be always you will you will have

00:28:55,299 --> 00:29:00,219
the capacity to reprint dealers using a

00:28:58,629 --> 00:29:04,509
panel that I will show it right after

00:29:00,219 --> 00:29:11,409
and the side pane which is not shown

00:29:04,509 --> 00:29:17,559
right now is actually there you know

00:29:11,409 --> 00:29:19,679
some because there so the side pane

00:29:17,559 --> 00:29:24,669
which doesn't want to show by the way

00:29:19,679 --> 00:29:28,649
everywhere I will Road it because

00:29:24,669 --> 00:29:31,649
actually I did something wrong they were

00:29:28,649 --> 00:29:31,649
yeah

00:29:34,960 --> 00:29:40,809
there and then the sidebar contains a

00:29:38,770 --> 00:29:46,179
few things so when I will create

00:29:40,809 --> 00:29:48,610
variables like here it will retain all

00:29:46,179 --> 00:29:51,789
the information order so if I'm heir and

00:29:48,610 --> 00:29:53,140
I want to know where rest 19 was defined

00:29:51,789 --> 00:29:58,390
I click on it and then it brings me back

00:29:53,140 --> 00:30:00,250
to the cell um I was a very a fan of our

00:29:58,390 --> 00:30:01,960
studio and this type of capacity this

00:30:00,250 --> 00:30:03,820
gap of functionality was available in

00:30:01,960 --> 00:30:05,950
our studio so you can have a list of the

00:30:03,820 --> 00:30:08,020
defined variables but you couldn't get

00:30:05,950 --> 00:30:10,419
back where the developer would be if

00:30:08,020 --> 00:30:12,820
were defined which was very painful at

00:30:10,419 --> 00:30:15,460
the end of a very long experimentation

00:30:12,820 --> 00:30:18,159
but also it contains an error log here

00:30:15,460 --> 00:30:22,750
which allows you to check the logs that

00:30:18,159 --> 00:30:27,610
were errors actually well okay I will

00:30:22,750 --> 00:30:29,470
move a little bit faster to show okay

00:30:27,610 --> 00:30:31,390
the next thing you know it's plotting

00:30:29,470 --> 00:30:34,720
definitely so you want to plot

00:30:31,390 --> 00:30:37,090
information I'm not an artist right but

00:30:34,720 --> 00:30:39,220
I like to have view of my data being

00:30:37,090 --> 00:30:42,340
able to represent you know even slightly

00:30:39,220 --> 00:30:44,110
what my data look like so here I'm just

00:30:42,340 --> 00:30:46,840
creating a few examples based on the

00:30:44,110 --> 00:30:49,029
case class and if I written the variable

00:30:46,840 --> 00:30:51,429
as the last testament or statement of a

00:30:49,029 --> 00:30:54,370
cell so I print what I can print like

00:30:51,429 --> 00:30:57,039
here's a table and a private chart but

00:30:54,370 --> 00:30:59,679
actually these are regular types so I

00:30:57,039 --> 00:31:02,460
can have the table chart you a widget in

00:30:59,679 --> 00:31:04,840
order to print examples and then it

00:31:02,460 --> 00:31:09,220
turns back into the class and then it

00:31:04,840 --> 00:31:11,950
shows the table for the data other

00:31:09,220 --> 00:31:14,260
things so table are fair it's so nice

00:31:11,950 --> 00:31:17,020
but you want to print lines as well so

00:31:14,260 --> 00:31:19,480
here I'm printing the data I want to

00:31:17,020 --> 00:31:21,309
have the x-axis to be the IDS the y-axis

00:31:19,480 --> 00:31:24,700
to be the values and I want to group the

00:31:21,309 --> 00:31:26,169
adapter for advanced features so it

00:31:24,700 --> 00:31:29,080
which is true or false and then I have

00:31:26,169 --> 00:31:31,149
my line charts you get the idea I will

00:31:29,080 --> 00:31:36,250
have that for the Scotch a scatter chart

00:31:31,149 --> 00:31:39,220
and for the bar graph of course so I'm

00:31:36,250 --> 00:31:41,440
basically a mathematician I did a lot of

00:31:39,220 --> 00:31:43,990
graph theory so one of the first feature

00:31:41,440 --> 00:31:46,720
that I have it I've added in in my

00:31:43,990 --> 00:31:48,669
notebook was graph plotting so I have

00:31:46,720 --> 00:31:51,249
created this graph DSL

00:31:48,669 --> 00:31:55,389
you to define northern edges assigning

00:31:51,249 --> 00:31:56,889
them values properties colors size and

00:31:55,389 --> 00:32:00,190
whatever and here actually I'm using

00:31:56,889 --> 00:32:03,070
this inter these this DSL to create

00:32:00,190 --> 00:32:06,429
edges and nodes and connecting them

00:32:03,070 --> 00:32:10,690
based on a DS and then I'm can use the

00:32:06,429 --> 00:32:14,499
graph chart which shows my my points

00:32:10,690 --> 00:32:17,619
linked you know using the edges and you

00:32:14,499 --> 00:32:19,929
can it's a d3 layout chart I can

00:32:17,619 --> 00:32:21,580
increase the font the size of the chart

00:32:19,929 --> 00:32:24,730
but I won't show it right now otherwise

00:32:21,580 --> 00:32:26,919
that we're not run out of time so this

00:32:24,730 --> 00:32:29,739
is a Geo point shot I did all of that

00:32:26,919 --> 00:32:32,019
just special project in my life I love

00:32:29,739 --> 00:32:33,850
just geospatial stuff so the second

00:32:32,019 --> 00:32:35,799
thing I have added for Advent chart was

00:32:33,850 --> 00:32:38,919
the spatial information with lat/long

00:32:35,799 --> 00:32:41,379
for instance and then I can group them

00:32:38,919 --> 00:32:45,730
using the count limit to five okay just

00:32:41,379 --> 00:32:51,129
fetching the last the first state which

00:32:45,730 --> 00:32:53,619
have the most airports then I defined a

00:32:51,129 --> 00:32:57,070
class for it I convert my data frame as

00:32:53,619 --> 00:32:58,659
a data set and I can print it so it will

00:32:57,070 --> 00:33:02,350
looks like that for instance

00:32:58,659 --> 00:33:04,480
but I can also reshape my data set in

00:33:02,350 --> 00:33:06,340
order to say okay so now I want to

00:33:04,480 --> 00:33:08,409
assign a few colors and size to the

00:33:06,340 --> 00:33:10,570
states using the data frame API and

00:33:08,409 --> 00:33:13,210
afterwards I will use a geophone charge

00:33:10,570 --> 00:33:15,820
and say okay the lat/long fills the

00:33:13,210 --> 00:33:19,119
Latin long there are field is the are

00:33:15,820 --> 00:33:21,809
field which means the radius the color

00:33:19,119 --> 00:33:28,269
fruit is the C and if I print it then

00:33:21,809 --> 00:33:29,350
it's gonna fail because I forgot

00:33:28,269 --> 00:33:43,230
something

00:33:29,350 --> 00:33:43,230
oh yeah that's my bad ethics or Apache

00:33:44,290 --> 00:33:53,350
spark Oh shoes and I should be okay

00:33:51,610 --> 00:33:57,070
isn't it yeah

00:33:53,350 --> 00:33:59,040
all right so her now I have my my chart

00:33:57,070 --> 00:34:02,950
with points and stuff like that right

00:33:59,040 --> 00:34:05,170
however so you know lat/long is very

00:34:02,950 --> 00:34:08,350
just facial for dummies right so what we

00:34:05,170 --> 00:34:11,200
want is polygons lines so you can also

00:34:08,350 --> 00:34:13,390
fetch some data from the internet using

00:34:11,200 --> 00:34:15,700
a sedge is actually a context for shell

00:34:13,390 --> 00:34:19,300
so you pass a shell script it downloads

00:34:15,700 --> 00:34:21,310
the data then you can parse it using the

00:34:19,300 --> 00:34:23,740
the tools that I'm providing in widgets

00:34:21,310 --> 00:34:26,110
so you part your JSON file so you get

00:34:23,740 --> 00:34:28,419
back a JSON presentation you can do that

00:34:26,110 --> 00:34:31,570
for another file which will be mainly

00:34:28,419 --> 00:34:33,520
lines I parse it I get the data and then

00:34:31,570 --> 00:34:35,560
I can create a dual chart now don't know

00:34:33,520 --> 00:34:38,140
that your point chart and then I will

00:34:35,560 --> 00:34:40,510
have my polygons there right away I'm

00:34:38,140 --> 00:34:43,390
still using Scala right no genius know

00:34:40,510 --> 00:34:45,730
HTML no whatsoever fully the scanner and

00:34:43,390 --> 00:34:48,370
oh by the way I want to have an extra

00:34:45,730 --> 00:34:50,919
data set to it okay so you can use the

00:34:48,370 --> 00:34:53,350
add an apply function to it and then

00:34:50,919 --> 00:34:55,000
it's gonna update the chart with new

00:34:53,350 --> 00:34:58,780
information into which which are

00:34:55,000 --> 00:35:01,620
basically routes okay you want to create

00:34:58,780 --> 00:35:04,450
fancy charts for enough you can also

00:35:01,620 --> 00:35:06,970
create rudders so here I'm a fan of

00:35:04,450 --> 00:35:08,740
basketball so I created the base San

00:35:06,970 --> 00:35:10,570
Antonio's first data set from a website

00:35:08,740 --> 00:35:13,390
which I'm mentioning and then I'm create

00:35:10,570 --> 00:35:15,580
a rather chart onto it on the different

00:35:13,390 --> 00:35:16,950
fields and then I can see what's going

00:35:15,580 --> 00:35:20,350
on there

00:35:16,950 --> 00:35:23,080
private so it's a funny one but I will

00:35:20,350 --> 00:35:24,790
quickly show it so you can tune the way

00:35:23,080 --> 00:35:26,760
you want to plot things actually in the

00:35:24,790 --> 00:35:29,440
pie chart by dragging and dropping

00:35:26,760 --> 00:35:32,110
features onto the graph parallel

00:35:29,440 --> 00:35:35,050
coordinates a very useful chart that

00:35:32,110 --> 00:35:37,800
allows you to display in two dimensions

00:35:35,050 --> 00:35:41,350
a lot of them and a lot of dimensions

00:35:37,800 --> 00:35:43,780
using error line so you can filter using

00:35:41,350 --> 00:35:47,670
this kind of things there so you can say

00:35:43,780 --> 00:35:50,650
okay so 70 75 percent for free-throw and

00:35:47,670 --> 00:35:52,030
three point three percent and so on and

00:35:50,650 --> 00:35:54,930
then you get to the name here Jonathan

00:35:52,030 --> 00:35:58,380
Simmons and Boris Dao

00:35:54,930 --> 00:36:00,630
time series you know one of the most use

00:35:58,380 --> 00:36:02,580
you must come on you skate that we can

00:36:00,630 --> 00:36:05,220
see around you create a dataset with

00:36:02,580 --> 00:36:07,800
dates and values and you get back a time

00:36:05,220 --> 00:36:12,030
service with dates and the values on the

00:36:07,800 --> 00:36:13,530
y-axis as I said everything is reactive

00:36:12,030 --> 00:36:16,110
so we shown it already

00:36:13,530 --> 00:36:17,850
I've shown it already with update so I

00:36:16,110 --> 00:36:20,250
won't show it here but basically you can

00:36:17,850 --> 00:36:25,080
do it in a in a loop and thread and then

00:36:20,250 --> 00:36:26,700
the graph is updated while you know the

00:36:25,080 --> 00:36:30,120
process is a significant in the

00:36:26,700 --> 00:36:31,710
different threads there anyway I like

00:36:30,120 --> 00:36:33,570
that so I don't know if you know shiny

00:36:31,710 --> 00:36:35,130
but shiny has the capacity to have

00:36:33,570 --> 00:36:37,950
reactive components that they can

00:36:35,130 --> 00:36:41,340
interact with each other and what I have

00:36:37,950 --> 00:36:43,980
in in the spy notebook is this operator

00:36:41,340 --> 00:36:46,800
that allows we just to connect to the

00:36:43,980 --> 00:36:49,200
order and that means that the drop-down

00:36:46,800 --> 00:36:52,050
list is sending events to this observer

00:36:49,200 --> 00:36:55,080
which will update to different widgets

00:36:52,050 --> 00:36:56,910
which is a output and a radar chance so

00:36:55,080 --> 00:36:59,610
it means that if I have my rotochop

00:36:56,910 --> 00:37:01,520
showing all people there I can go to

00:36:59,610 --> 00:37:04,200
Joey Oh

00:37:01,520 --> 00:37:07,620
Ginobili and it shows it and I can get

00:37:04,200 --> 00:37:11,270
back to her because her have a filter if

00:37:07,620 --> 00:37:14,790
it's all I feel I don't filter anything

00:37:11,270 --> 00:37:16,260
synchronization one half time was

00:37:14,790 --> 00:37:18,570
probably actually notebooks are

00:37:16,260 --> 00:37:20,220
synchronized so if one shot is moving or

00:37:18,570 --> 00:37:22,530
displayed on one notebook is gonna be

00:37:20,220 --> 00:37:25,590
displayed in the other tab so everything

00:37:22,530 --> 00:37:27,450
is moving at the same in the same manner

00:37:25,590 --> 00:37:29,570
because everything is connected to the

00:37:27,450 --> 00:37:33,570
same a WebSocket on the on the server

00:37:29,570 --> 00:37:36,360
you can also create new chart type

00:37:33,570 --> 00:37:38,820
within the notebooks if you want to do

00:37:36,360 --> 00:37:40,920
really want to do JavaScript so you can

00:37:38,820 --> 00:37:43,140
do it into the internal notebook here

00:37:40,920 --> 00:37:45,810
it's just a dummy example showing

00:37:43,140 --> 00:37:47,640
progress bars and I explained how to

00:37:45,810 --> 00:37:49,950
create new widgets by the way here I'm

00:37:47,640 --> 00:37:53,490
porting you a few things like I'm using

00:37:49,950 --> 00:37:56,940
extensively type classes in order to to

00:37:53,490 --> 00:37:59,220
have implicit to convert my data types

00:37:56,940 --> 00:38:01,860
to point which is basically a string

00:37:59,220 --> 00:38:04,260
sequence of string and value and then

00:38:01,860 --> 00:38:05,910
sampler is a tab class in order to

00:38:04,260 --> 00:38:09,030
inject

00:38:05,910 --> 00:38:10,230
see for my sampling strategy and here

00:38:09,030 --> 00:38:14,700
I'm connecting scallop with the

00:38:10,230 --> 00:38:17,359
JavaScript word here and when I I can

00:38:14,700 --> 00:38:17,359
create some data

00:38:32,130 --> 00:38:38,430
yeah well actually no way it's I don't

00:38:37,200 --> 00:38:40,560
know why it's lagging but it's lagging

00:38:38,430 --> 00:38:44,520
anyway so anyway I can still consult

00:38:40,560 --> 00:38:46,050
things if it doesn't work well yeah

00:38:44,520 --> 00:38:47,550
sometimes this happens so if there is

00:38:46,050 --> 00:38:49,110
something wrong with the repple I would

00:38:47,550 --> 00:38:53,100
say this sometimes this can happen if

00:38:49,110 --> 00:38:55,440
you execute - too tricky too tricky code

00:38:53,100 --> 00:38:58,620
and sometimes the report gets a little

00:38:55,440 --> 00:39:00,150
bit fuzzy and that you may have this you

00:38:58,620 --> 00:39:02,070
just have to restart the Red Pony and it

00:39:00,150 --> 00:39:05,430
goes fine but it's I'm not going to show

00:39:02,070 --> 00:39:07,230
it a new way sorry about that so the

00:39:05,430 --> 00:39:10,050
thing that I was showing normally was a

00:39:07,230 --> 00:39:13,010
bunch of progress bar evolving with the

00:39:10,050 --> 00:39:15,440
data coming into the threads as well

00:39:13,010 --> 00:39:17,640
latech so lattic is supported with

00:39:15,440 --> 00:39:19,590
interpolation so you can use markdown

00:39:17,640 --> 00:39:20,820
and then you can put some lattic into it

00:39:19,590 --> 00:39:24,930
and then you will be rendered as a

00:39:20,820 --> 00:39:27,720
regular lattic strings so these are a

00:39:24,930 --> 00:39:29,520
few bunch of advantages using something

00:39:27,720 --> 00:39:31,410
like a notebook so you can get a feeling

00:39:29,520 --> 00:39:33,570
about the data plotted the way you want

00:39:31,410 --> 00:39:34,700
to plot it and having a better feeling

00:39:33,570 --> 00:39:37,770
about it

00:39:34,700 --> 00:39:40,770
however the last point this was about

00:39:37,770 --> 00:39:42,930
tooling and now what about available

00:39:40,770 --> 00:39:45,090
models and algorithms because it's okay

00:39:42,930 --> 00:39:49,230
so you can do statistics or written

00:39:45,090 --> 00:39:51,840
ethics using your scala in your your

00:39:49,230 --> 00:39:55,020
your table also whatsoever

00:39:51,840 --> 00:39:57,600
however you need to do data science then

00:39:55,020 --> 00:39:58,650
you have to look for models or you have

00:39:57,600 --> 00:40:00,480
to implement that by the way

00:39:58,650 --> 00:40:02,040
implementing a model is very easy

00:40:00,480 --> 00:40:05,670
general you take the paper you implement

00:40:02,040 --> 00:40:07,860
it generally is like two pages of text

00:40:05,670 --> 00:40:10,680
in the in the in the paper so it's like

00:40:07,860 --> 00:40:12,630
a few hours code but generally you don't

00:40:10,680 --> 00:40:16,650
want to redo it though then you have to

00:40:12,630 --> 00:40:19,010
look in existing libraries and in this

00:40:16,650 --> 00:40:22,110
notebook there I'm listing a few of them

00:40:19,010 --> 00:40:26,940
this is a work in progress though so I

00:40:22,110 --> 00:40:28,620
have listed a few that I'm using time to

00:40:26,940 --> 00:40:33,630
time the most important one was perlier

00:40:28,620 --> 00:40:35,910
h2o ml lib and and deep learning for J

00:40:33,630 --> 00:40:38,520
but in this notebook I'm showing you a

00:40:35,910 --> 00:40:41,910
few snippets using smile for instance

00:40:38,520 --> 00:40:43,460
smile is a very very wide library so it

00:40:41,910 --> 00:40:46,520
has like 98

00:40:43,460 --> 00:40:49,369
today implementation of algorithms and

00:40:46,520 --> 00:40:51,920
models I cited them all here because

00:40:49,369 --> 00:40:54,170
it's very impressive however it's only

00:40:51,920 --> 00:40:55,940
only local right it's a local

00:40:54,170 --> 00:41:01,010
implementation it's very very fast bits

00:40:55,940 --> 00:41:01,660
only local so the list goes on there we

00:41:01,010 --> 00:41:04,670
are

00:41:01,660 --> 00:41:08,920
I've put hey something that I like

00:41:04,670 --> 00:41:08,920
because I like to estimate density and

00:41:09,490 --> 00:41:13,609
distribution so I've put an example from

00:41:12,770 --> 00:41:16,579
4mx

00:41:13,609 --> 00:41:19,430
and and here if you want to secure it so

00:41:16,579 --> 00:41:24,500
I'm downloading a few data the train and

00:41:19,430 --> 00:41:26,450
the test so there is a test and I'm

00:41:24,500 --> 00:41:28,849
converting the data into something that

00:41:26,450 --> 00:41:31,460
I consume into the smiley classification

00:41:28,849 --> 00:41:35,930
max and I apply it and then we can

00:41:31,460 --> 00:41:38,540
consume the error depending for J

00:41:35,930 --> 00:41:42,020
tomorrow I will give a a talk with

00:41:38,540 --> 00:41:45,770
Melanie on how to do an LP with the

00:41:42,020 --> 00:41:48,440
planning for J in spark but the Manning

00:41:45,770 --> 00:41:50,390
is not only disability it's also local

00:41:48,440 --> 00:41:54,109
but here I'm showing a good example

00:41:50,390 --> 00:41:59,030
using an STM and the word were to effect

00:41:54,109 --> 00:42:03,260
model to to do some sentiment analysis

00:41:59,030 --> 00:42:07,069
on text and I put it even in spark using

00:42:03,260 --> 00:42:09,680
this second structure ml lib is off of

00:42:07,069 --> 00:42:11,930
course one of the most important now in

00:42:09,680 --> 00:42:13,940
scanner so it has quite a lot of

00:42:11,930 --> 00:42:15,920
implementation as well but the thing is

00:42:13,940 --> 00:42:17,900
the very difference is that everything

00:42:15,920 --> 00:42:19,970
is well distributed very performant

00:42:17,900 --> 00:42:22,730
scalable so they're very focused on this

00:42:19,970 --> 00:42:26,030
fact having things to be scalable as

00:42:22,730 --> 00:42:27,470
much as possible not instead of two the

00:42:26,030 --> 00:42:29,390
first factor to accept a model

00:42:27,470 --> 00:42:30,790
implementation so this is just showing

00:42:29,390 --> 00:42:33,740
and I'm stealing it from the

00:42:30,790 --> 00:42:36,200
documentation which is very good this is

00:42:33,740 --> 00:42:38,630
just a random forest implementation I

00:42:36,200 --> 00:42:41,030
import algorithm the model some you

00:42:38,630 --> 00:42:43,549
choose to load the data I'm taking some

00:42:41,030 --> 00:42:45,980
data for SVM a initially then I split

00:42:43,549 --> 00:42:47,720
the data to have my training and test I

00:42:45,980 --> 00:42:50,240
defined some a few parameters

00:42:47,720 --> 00:42:52,250
I'm calling this object then trans

00:42:50,240 --> 00:42:54,020
pacifier with information that I want to

00:42:52,250 --> 00:42:56,359
and then finally I can do prediction

00:42:54,020 --> 00:42:57,290
directly on my own deities so it's very

00:42:56,359 --> 00:43:01,670
simple a few

00:42:57,290 --> 00:43:05,480
and you have your daily trains in - you

00:43:01,670 --> 00:43:08,030
have your random forest model trains in

00:43:05,480 --> 00:43:10,370
distributed manner and then you can you

00:43:08,030 --> 00:43:13,430
can show a few information regarding

00:43:10,370 --> 00:43:16,100
tester and so on something that I like

00:43:13,430 --> 00:43:18,770
and I work with them on that so there is

00:43:16,100 --> 00:43:20,900
the Duke University of Paris which is

00:43:18,770 --> 00:43:23,540
now working on spark streaming in order

00:43:20,900 --> 00:43:26,390
to create new algorithms streaming so

00:43:23,540 --> 00:43:29,540
online learning algorithms and this one

00:43:26,390 --> 00:43:31,790
is very interesting I think is jet

00:43:29,540 --> 00:43:35,420
stream which is an adaptation of the

00:43:31,790 --> 00:43:37,430
gasner Network essentially it's a

00:43:35,420 --> 00:43:39,650
self-organizing map actually but that

00:43:37,430 --> 00:43:42,080
works in in the streaming manner that

00:43:39,650 --> 00:43:44,720
means that you can actually detect a new

00:43:42,080 --> 00:43:47,000
cluster on the fly it's not like k-means

00:43:44,720 --> 00:43:48,770
where you have to fix the AI parameter k

00:43:47,000 --> 00:43:50,270
here actually you can detect new

00:43:48,770 --> 00:43:53,270
clusters because actually you are

00:43:50,270 --> 00:43:57,020
folding the space in order to detect new

00:43:53,270 --> 00:44:01,610
clusters based on gr optimization and as

00:43:57,020 --> 00:44:03,860
showing very quickly how to do it using

00:44:01,610 --> 00:44:06,080
the API and also the coder is very

00:44:03,860 --> 00:44:07,600
simple so you fix a few parameters and

00:44:06,080 --> 00:44:13,610
you and you're good to go

00:44:07,600 --> 00:44:15,920
using addy stream to train the model so

00:44:13,610 --> 00:44:18,470
it's a work in progress there are a few

00:44:15,920 --> 00:44:20,900
other libraries that I want to show at

00:44:18,470 --> 00:44:23,080
some point in this notebook h2o which is

00:44:20,900 --> 00:44:25,970
a fantastic one very performant

00:44:23,080 --> 00:44:29,840
figero is that this next one

00:44:25,970 --> 00:44:31,880
Oh scarf for operational research by

00:44:29,840 --> 00:44:35,810
Escala which is more or less linked to

00:44:31,880 --> 00:44:37,730
figure at some point and maybe sysml and

00:44:35,810 --> 00:44:40,010
most probably opti ml which is stanford

00:44:37,730 --> 00:44:46,400
projects to implement a lot of model in

00:44:40,010 --> 00:44:49,280
Scala so again this code is available in

00:44:46,400 --> 00:44:51,620
this repository this clonic for kit

00:44:49,280 --> 00:44:53,180
build a docker if you don't want to

00:44:51,620 --> 00:44:56,810
build a docker right now it's going to

00:44:53,180 --> 00:45:00,290
be anyway available in the in the docker

00:44:56,810 --> 00:45:03,160
hub its publishing there in the

00:45:00,290 --> 00:45:03,160
documentation anyway

00:45:08,490 --> 00:45:13,859
the connection is really poor so I will

00:45:10,560 --> 00:45:15,240
leave it like this and then finally yeah

00:45:13,859 --> 00:45:18,030
I don't have anymore

00:45:15,240 --> 00:45:21,720
connection so there are a lot of

00:45:18,030 --> 00:45:24,900
versatility Canova and they start doing

00:45:21,720 --> 00:45:26,760
the courses in SCANA directly so I know

00:45:24,900 --> 00:45:29,490
if University is doing that I set it to

00:45:26,760 --> 00:45:32,190
two of them the University of Paris and

00:45:29,490 --> 00:45:35,130
some university in Netherlands they are

00:45:32,190 --> 00:45:36,810
at both universities and there is also a

00:45:35,130 --> 00:45:38,970
link to the course if you want to follow

00:45:36,810 --> 00:45:40,770
it and these guys actually using the spy

00:45:38,970 --> 00:45:43,590
notebook for the student to get involved

00:45:40,770 --> 00:45:45,420
in to spark there is some education

00:45:43,590 --> 00:45:49,170
companies there is data science retreat

00:45:45,420 --> 00:45:51,660
here on the floor we're in the light

00:45:49,170 --> 00:45:53,940
band booth there is also the data

00:45:51,660 --> 00:45:56,430
science in cooperation in the US for

00:45:53,940 --> 00:45:59,550
which I am I'm a mentor it's also a

00:45:56,430 --> 00:46:00,960
twelve weeks course introducing

00:45:59,550 --> 00:46:03,180
functional programming disability

00:46:00,960 --> 00:46:06,060
computing and machine learning and on

00:46:03,180 --> 00:46:07,530
get up you will have this it has been

00:46:06,060 --> 00:46:09,510
published yesterday by the way by these

00:46:07,530 --> 00:46:12,359
guys so there are two courses using

00:46:09,510 --> 00:46:14,190
again as my new book in this case to

00:46:12,359 --> 00:46:17,730
explain how to do a machine learning

00:46:14,190 --> 00:46:19,410
using scan and spark and of course check

00:46:17,730 --> 00:46:23,400
this website where there are a lot of

00:46:19,410 --> 00:46:26,490
packages published so this is basically

00:46:23,400 --> 00:46:28,619
it's regarding why scanner is most

00:46:26,490 --> 00:46:31,770
probably the next language for dello

00:46:28,619 --> 00:46:33,600
science with a few arguments by the way

00:46:31,770 --> 00:46:38,660
Lightman and ourselves in the lab fellas

00:46:33,600 --> 00:46:44,250
we are in so please come to us to ask

00:46:38,660 --> 00:46:46,680
what where when and yeah that's it

00:46:44,250 --> 00:46:49,380
you know both talking again tomorrow oh

00:46:46,680 --> 00:46:52,560
yeah we give to we get to walk each

00:46:49,380 --> 00:46:54,600
other tomorrow I think we're over time

00:46:52,560 --> 00:46:56,820
and it's party time so if anybody wants

00:46:54,600 --> 00:46:57,390
to ask us questions offline maybe we

00:46:56,820 --> 00:47:00,540
should do that

00:46:57,390 --> 00:47:02,810
it's 47 talk to me now so thank you very

00:47:00,540 --> 00:47:02,810

YouTube URL: https://www.youtube.com/watch?v=IUBcdbeY2UE


