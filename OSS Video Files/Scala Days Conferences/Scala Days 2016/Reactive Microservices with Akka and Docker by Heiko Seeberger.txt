Title: Reactive Microservices with Akka and Docker by Heiko Seeberger
Publication date: 2017-01-19
Playlist: Scala Days 2016
Description: 
	This video was recorded at Scala Days Berlin 2016
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

Anstract:
Akka is a toolkit for building elastic and resilient distributed systems and Docker makes shipping and running those distributed systems easy as never before. In this talk we will briefly introduce you to the basics of Akka and Docker and then show how you can use these innovative technologies to build and run really Reactive micorservices. Donâ€™t expect too many slides, but be prepared for live demos.
Captions: 
	00:00:04,570 --> 00:00:09,490
all right I think we get started welcome

00:00:08,110 --> 00:00:11,559
everybody to this talk about reactive

00:00:09,490 --> 00:00:14,200
micro services with akka and docker as

00:00:11,559 --> 00:00:15,309
many buzzwords as possible in one short

00:00:14,200 --> 00:00:17,230
title

00:00:15,309 --> 00:00:19,060
my name is haiku I'm working for the

00:00:17,230 --> 00:00:21,580
fantastic company code centric we are a

00:00:19,060 --> 00:00:24,699
sponsor check out our booth if you want

00:00:21,580 --> 00:00:28,960
to talk by the way I'm also the author

00:00:24,699 --> 00:00:30,789
of this little German Scala book so we

00:00:28,960 --> 00:00:36,490
give it away at our booth if you're

00:00:30,789 --> 00:00:39,340
interested so what is reactive who has

00:00:36,490 --> 00:00:40,289
read the reactive manifesto show me your

00:00:39,340 --> 00:00:44,350
hands

00:00:40,289 --> 00:00:47,440
most of you that's great just in a

00:00:44,350 --> 00:00:51,280
nutshell reactive defines a couple of

00:00:47,440 --> 00:00:55,600
traits or qualities which modern systems

00:00:51,280 --> 00:00:57,100
should have and the the most important

00:00:55,600 --> 00:00:59,289
of these traits is responsive which

00:00:57,100 --> 00:01:03,070
means that a system should always

00:00:59,289 --> 00:01:06,070
respond to requests if possible at all

00:01:03,070 --> 00:01:08,140
and apart from a lot of non-functional

00:01:06,070 --> 00:01:11,320
sorry functional requirements there are

00:01:08,140 --> 00:01:14,790
two really important non-functional

00:01:11,320 --> 00:01:18,460
qualities that are required to be

00:01:14,790 --> 00:01:22,659
responsive and that is resilient so a

00:01:18,460 --> 00:01:26,259
system should stay responsive even in

00:01:22,659 --> 00:01:28,950
the face of failure and elastic means

00:01:26,259 --> 00:01:34,420
that the system remains respond

00:01:28,950 --> 00:01:36,130
responsive under load and if you put

00:01:34,420 --> 00:01:38,350
message driven to the whole thing

00:01:36,130 --> 00:01:42,159
underpin it with asynchronous message

00:01:38,350 --> 00:01:44,170
passing you can enable these other

00:01:42,159 --> 00:01:47,170
traits for example because the message

00:01:44,170 --> 00:01:49,420
passing means that you decouple the

00:01:47,170 --> 00:01:51,310
sender from the receiver and you can

00:01:49,420 --> 00:01:56,530
have delegation of failure handling and

00:01:51,310 --> 00:01:59,590
all these cool things next question

00:01:56,530 --> 00:02:02,770
what's a micro service so for me the

00:01:59,590 --> 00:02:05,710
size doesn't matter but for me it's more

00:02:02,770 --> 00:02:12,400
of the UNIX philosophy do one thing

00:02:05,710 --> 00:02:15,750
hopefully do it well and one other

00:02:12,400 --> 00:02:18,819
really important aspect for me is that a

00:02:15,750 --> 00:02:23,920
micro service should be able to react

00:02:18,819 --> 00:02:26,470
or to act autonomously and that means it

00:02:23,920 --> 00:02:32,640
has to be isolated from others it has to

00:02:26,470 --> 00:02:37,629
be self-contained and means to own its

00:02:32,640 --> 00:02:41,769
data and we will see what that means

00:02:37,629 --> 00:02:44,620
with one example later by the way this

00:02:41,769 --> 00:02:48,939
booklet by Johannes Panera has inspired

00:02:44,620 --> 00:02:51,190
a lot of the ideas and terminology I'm

00:02:48,939 --> 00:02:56,440
using here if you haven't read that

00:02:51,190 --> 00:02:59,290
check it out it's a quick read so if you

00:02:56,440 --> 00:03:03,459
have a microservice hmm that doesn't

00:02:59,290 --> 00:03:04,659
really give you too much you need a lot

00:03:03,459 --> 00:03:08,290
of micro services they need to

00:03:04,659 --> 00:03:10,599
collaborate and in this book Yana says 1

00:03:08,290 --> 00:03:12,099
Microsoft's is no micro service they

00:03:10,599 --> 00:03:18,280
come in system and this quote is

00:03:12,099 --> 00:03:22,989
actually stolen from it goes back to

00:03:18,280 --> 00:03:24,160
various until biology people and I think

00:03:22,989 --> 00:03:26,410
it's really important understand that

00:03:24,160 --> 00:03:29,199
for this sort of collaboration we have

00:03:26,410 --> 00:03:31,299
to think about how exactly that would

00:03:29,199 --> 00:03:34,060
happen in a synchronous fashion or

00:03:31,299 --> 00:03:38,889
asynchronously oh yeah let's take a look

00:03:34,060 --> 00:03:43,090
later in detail but for me it is totally

00:03:38,889 --> 00:03:44,889
clear that in order to have autonomy

00:03:43,090 --> 00:03:50,879
those micro services must be isolated

00:03:44,889 --> 00:03:54,129
and you can achieve a solution using

00:03:50,879 --> 00:03:56,319
asynchronous messaging asynchronous

00:03:54,129 --> 00:04:00,449
communication because that decouples

00:03:56,319 --> 00:04:03,250
your services in time so that means that

00:04:00,449 --> 00:04:05,680
you don't have to wait for the other

00:04:03,250 --> 00:04:09,280
service to be available to talk to it

00:04:05,680 --> 00:04:14,829
and you can also contain fader you can

00:04:09,280 --> 00:04:19,299
compartmentalize your system so what

00:04:14,829 --> 00:04:21,639
everyone convey is that one micro

00:04:19,299 --> 00:04:24,490
service should not access another one in

00:04:21,639 --> 00:04:26,710
a synchronous session when serving

00:04:24,490 --> 00:04:30,159
requests because that would totally

00:04:26,710 --> 00:04:31,990
break isolation that would mean no more

00:04:30,159 --> 00:04:34,060
autonomy

00:04:31,990 --> 00:04:38,410
so for erected microservice this is a

00:04:34,060 --> 00:04:39,100
no-go so how does aqua fit into this

00:04:38,410 --> 00:04:42,010
picture

00:04:39,100 --> 00:04:45,940
I think the ACTA model is a really great

00:04:42,010 --> 00:04:50,710
fit for any sort of reactive system

00:04:45,940 --> 00:04:53,250
weather or micro service or not if you

00:04:50,710 --> 00:04:54,460
look at akka it's not a single jar

00:04:53,250 --> 00:04:57,580
anymore

00:04:54,460 --> 00:04:59,290
it it offers you really a lot of high

00:04:57,580 --> 00:05:02,140
level modules for scalability and

00:04:59,290 --> 00:05:06,730
resilience for example you can use our

00:05:02,140 --> 00:05:12,100
KH TDP which has been released in occur

00:05:06,730 --> 00:05:14,020
- 4 - a couple of months ago and that is

00:05:12,100 --> 00:05:16,330
really something you probably need for

00:05:14,020 --> 00:05:20,980
almost every micro service right usually

00:05:16,330 --> 00:05:23,080
they have an HTTP API and there's not a

00:05:20,980 --> 00:05:25,450
module is called akka SSE server sent

00:05:23,080 --> 00:05:28,660
events which which I initially authored

00:05:25,450 --> 00:05:30,730
and this is really great for

00:05:28,660 --> 00:05:34,240
asynchronous event based collaboration

00:05:30,730 --> 00:05:36,420
as we will see later in the example next

00:05:34,240 --> 00:05:39,010
question how does docker fit

00:05:36,420 --> 00:05:43,960
first of all containers provide

00:05:39,010 --> 00:05:47,350
isolation maybe no perfect isolation but

00:05:43,960 --> 00:05:51,490
at least some sort of isolation on on a

00:05:47,350 --> 00:05:55,420
host and in my opinion using containers

00:05:51,490 --> 00:05:59,200
and the Tucker workflow whether well the

00:05:55,420 --> 00:06:02,710
container workflow makes the automation

00:05:59,200 --> 00:06:04,990
of the life cycle a lot easier if you

00:06:02,710 --> 00:06:07,180
look at docker in particular there's a

00:06:04,990 --> 00:06:09,790
lot of tools and infrastructure

00:06:07,180 --> 00:06:11,800
available namely dock compose and and

00:06:09,790 --> 00:06:16,230
duck a cloud you should take our Tucker

00:06:11,800 --> 00:06:19,210
cloud that's pretty pretty awesome ok so

00:06:16,230 --> 00:06:23,230
the rest of the talk I will show you a

00:06:19,210 --> 00:06:25,840
demo it's of course a very simple chat

00:06:23,230 --> 00:06:29,020
application and there will be two

00:06:25,840 --> 00:06:31,870
services there will be a user service to

00:06:29,020 --> 00:06:36,130
identify users and then there will be a

00:06:31,870 --> 00:06:38,500
chat service that needs some information

00:06:36,130 --> 00:06:41,770
from that user service and I want to

00:06:38,500 --> 00:06:44,470
show you how a chat service can get

00:06:41,770 --> 00:06:45,150
access to those information we get there

00:06:44,470 --> 00:06:47,130
step by step

00:06:45,150 --> 00:06:49,530
so I will sit down because I have to do

00:06:47,130 --> 00:06:52,669
some hacking and coding so that's easier

00:06:49,530 --> 00:06:55,020
when sitting I hope that's okay for you

00:06:52,669 --> 00:06:58,830
so the first thing I would like to show

00:06:55,020 --> 00:07:02,790
you is just akka HTTP how to run the

00:06:58,830 --> 00:07:05,940
server so there's two things you need to

00:07:02,790 --> 00:07:07,710
do you need to bind to a socket and you

00:07:05,940 --> 00:07:13,500
need to hand request personal responses

00:07:07,710 --> 00:07:16,710
right and those handlers are implemented

00:07:13,500 --> 00:07:22,080
using akka streams they are a flow which

00:07:16,710 --> 00:07:26,789
is a linear processing pipeline taking

00:07:22,080 --> 00:07:27,960
requests and emitting responses but you

00:07:26,789 --> 00:07:30,210
don't have to really dive into the

00:07:27,960 --> 00:07:34,770
details of the akka streams API there's

00:07:30,210 --> 00:07:38,070
a nice routing DSL which can be used and

00:07:34,770 --> 00:07:42,240
I want to show you how to build a really

00:07:38,070 --> 00:07:45,240
very very simple web service we call it

00:07:42,240 --> 00:07:50,520
the user API so one thing you need to do

00:07:45,240 --> 00:07:54,150
is you use the HTTP api of akka HTTP

00:07:50,520 --> 00:07:56,099
call the bind and handle method with a

00:07:54,150 --> 00:07:59,460
route and you give it an address and a

00:07:56,099 --> 00:08:03,110
port and then you react to either

00:07:59,460 --> 00:08:05,880
successful server binding or a failure

00:08:03,110 --> 00:08:08,550
but that's all you need to get the

00:08:05,880 --> 00:08:14,250
service started the interesting part is

00:08:08,550 --> 00:08:17,520
here in this route and this method apply

00:08:14,250 --> 00:08:20,250
creates the route and currently it

00:08:17,520 --> 00:08:23,610
doesn't really do a lot of things but it

00:08:20,250 --> 00:08:28,229
hopefully shows you the nature or yeah

00:08:23,610 --> 00:08:31,080
the feel of this DSL the directives

00:08:28,229 --> 00:08:34,289
which are used here are a path matching

00:08:31,080 --> 00:08:37,529
directive to match the route path single

00:08:34,289 --> 00:08:39,599
slash path and then we only filter get

00:08:37,529 --> 00:08:43,200
requests of course there's post and

00:08:39,599 --> 00:08:45,060
delete and all the HTTP verbs and if

00:08:43,200 --> 00:08:47,750
such a request comes in we complete it

00:08:45,060 --> 00:08:50,490
which means we create a response with

00:08:47,750 --> 00:08:54,510
hello world great huh

00:08:50,490 --> 00:08:58,079
okay that's how it looks and in order to

00:08:54,510 --> 00:08:58,930
run it on docker I have to show you how

00:08:58,079 --> 00:09:02,170
to create

00:08:58,930 --> 00:09:04,630
images so this can be done with SPT and

00:09:02,170 --> 00:09:08,560
the fantastic expert in a packager

00:09:04,630 --> 00:09:13,300
plugin the guys downstairs from Goethe

00:09:08,560 --> 00:09:16,209
Fogg have genetic Pachter t-shirts go

00:09:13,300 --> 00:09:20,350
and grab them they are awesome and with

00:09:16,209 --> 00:09:24,100
that really smart plugin you just have

00:09:20,350 --> 00:09:26,680
some settings in your project and and

00:09:24,100 --> 00:09:28,930
using the task publish local or publish

00:09:26,680 --> 00:09:31,570
you can really create your docker images

00:09:28,930 --> 00:09:35,620
so let me show you that first of all

00:09:31,570 --> 00:09:39,220
let's take a look at the build dot SBT

00:09:35,620 --> 00:09:43,630
file where you have to put a couple of a

00:09:39,220 --> 00:09:45,010
couple of settings for docker and yeah

00:09:43,630 --> 00:09:46,930
as you can see probably the most

00:09:45,010 --> 00:09:51,880
important one here the base image using

00:09:46,930 --> 00:09:56,709
Java 8 and and with that in place you

00:09:51,880 --> 00:09:58,240
can then do darker publish local and if

00:09:56,709 --> 00:10:08,130
you're familiar with docker you probably

00:09:58,240 --> 00:10:12,520
know these print lines and now we have

00:10:08,130 --> 00:10:18,040
latest version of the Kepler user

00:10:12,520 --> 00:10:21,459
service so let's run it I have a simple

00:10:18,040 --> 00:10:25,750
script let me show you this script that

00:10:21,459 --> 00:10:28,029
essentially just wraps the docker run

00:10:25,750 --> 00:10:31,200
command with a couple of flags that are

00:10:28,029 --> 00:10:34,450
necessary in particular I want to expose

00:10:31,200 --> 00:10:37,720
the port to the host I wanna I wanna

00:10:34,450 --> 00:10:41,350
publish the expose port to the house so

00:10:37,720 --> 00:10:47,800
I can access it so if I do a run gabber

00:10:41,350 --> 00:10:49,959
user the docker containers started I can

00:10:47,800 --> 00:10:54,640
I can check out the log and it looks

00:10:49,959 --> 00:10:57,760
good so it seems to listen so let me

00:10:54,640 --> 00:10:59,860
check out what happens if I if I call

00:10:57,760 --> 00:11:02,079
that oh I did I don't need that I just

00:10:59,860 --> 00:11:04,870
used the loot path and I get my hello

00:11:02,079 --> 00:11:07,649
world so yeah akka is up and running on

00:11:04,870 --> 00:11:07,649
docker

00:11:09,080 --> 00:11:17,120
of course a a proper user service would

00:11:12,860 --> 00:11:23,200
not only say hello world but instead it

00:11:17,120 --> 00:11:28,310
would consume and and create properly

00:11:23,200 --> 00:11:33,730
properly formatted well yeah because the

00:11:28,310 --> 00:11:37,279
responses and usually HTTP uses JSON and

00:11:33,730 --> 00:11:40,250
rhtp has a really neat feature

00:11:37,279 --> 00:11:44,209
it's called marshalling which means you

00:11:40,250 --> 00:11:47,620
can complete your requests with normal

00:11:44,209 --> 00:11:52,820
domain objects we have already seen that

00:11:47,620 --> 00:11:56,810
in our case here the domain object was a

00:11:52,820 --> 00:12:00,170
string but you might have asked yourself

00:11:56,810 --> 00:12:03,769
oh why can I complete a request with a

00:12:00,170 --> 00:12:06,079
string I mean I need a HTTP response

00:12:03,769 --> 00:12:07,910
right so what you really need type wise

00:12:06,079 --> 00:12:10,240
is a to respond to Marshall level

00:12:07,910 --> 00:12:12,769
something that can be marshaled as a

00:12:10,240 --> 00:12:16,700
response and of course strings can be

00:12:12,769 --> 00:12:20,180
marshaled the response will be strict

00:12:16,700 --> 00:12:22,430
entity and the code will be a 200 ok and

00:12:20,180 --> 00:12:29,180
it will be encoding to explain all these

00:12:22,430 --> 00:12:32,779
things if you do Jason marshalling you

00:12:29,180 --> 00:12:36,380
of course have to tell our canoe P how

00:12:32,779 --> 00:12:42,079
to exactly marshal your domain objects

00:12:36,380 --> 00:12:44,750
and akka supports Spray JSON and XML in

00:12:42,079 --> 00:12:47,720
Jackson out of the box and their self

00:12:44,750 --> 00:12:51,410
project which I have started and which

00:12:47,720 --> 00:12:53,899
is a great example of community driven

00:12:51,410 --> 00:12:57,680
project because I didn't write that much

00:12:53,899 --> 00:12:59,510
code it's all the other computers okay

00:12:57,680 --> 00:13:02,930
Sookie Jason supports additional JSON

00:12:59,510 --> 00:13:04,850
libraries like Searcy play Jason and so

00:13:02,930 --> 00:13:07,279
on I personally really like Searcy

00:13:04,850 --> 00:13:08,990
because it has fantastic feature I will

00:13:07,279 --> 00:13:12,470
show you and I'm in a moment to

00:13:08,990 --> 00:13:14,449
automatically create the decoders and

00:13:12,470 --> 00:13:17,660
encode us or the marshalling

00:13:14,449 --> 00:13:20,300
infrastructure which is needed for case

00:13:17,660 --> 00:13:21,080
classes and also for shields trade

00:13:20,300 --> 00:13:23,300
hierarchies

00:13:21,080 --> 00:13:28,580
so that's really pretty awesome so let

00:13:23,300 --> 00:13:32,450
me just show you how a complete user

00:13:28,580 --> 00:13:36,800
service would look like so we are now

00:13:32,450 --> 00:13:39,770
talking to a user repository but the

00:13:36,800 --> 00:13:42,200
most important aspect here is that we're

00:13:39,770 --> 00:13:46,280
using more of the directives to properly

00:13:42,200 --> 00:13:50,750
implement a couple of API features so

00:13:46,280 --> 00:13:55,550
when we get a user's path which is then

00:13:50,750 --> 00:13:59,030
ended we get by completing with a future

00:13:55,550 --> 00:14:02,420
so we ask this is the ask pattern right

00:13:59,030 --> 00:14:05,630
from akka we ask the user posit Ori get

00:14:02,420 --> 00:14:08,990
users and the message we will get back

00:14:05,630 --> 00:14:13,450
is a user's object which is just a set

00:14:08,990 --> 00:14:16,850
of user or wrapping a set of user and

00:14:13,450 --> 00:14:19,070
with that in place we can have complete

00:14:16,850 --> 00:14:21,170
requests we can complete the request

00:14:19,070 --> 00:14:23,930
with a set of users with a set of our

00:14:21,170 --> 00:14:26,570
domain instances and that works because

00:14:23,930 --> 00:14:28,670
of these two imports there's just the

00:14:26,570 --> 00:14:32,890
SUSE import which comes from the arcade

00:14:28,670 --> 00:14:37,190
GP JSON library and you also need this

00:14:32,890 --> 00:14:40,550
auto magic import that makes the all the

00:14:37,190 --> 00:14:43,150
heavy lifting possible to be honest that

00:14:40,550 --> 00:14:46,400
comes at a price there are macros

00:14:43,150 --> 00:14:50,660
working under the hood so the compile

00:14:46,400 --> 00:14:53,600
time will be even worse it will come

00:14:50,660 --> 00:14:55,910
slower you know it's already pretty slow

00:14:53,600 --> 00:14:57,200
Scout compiler so but it's very

00:14:55,910 --> 00:14:59,540
convenient you don't have to doing

00:14:57,200 --> 00:15:01,070
anything to your case classes and if you

00:14:59,540 --> 00:15:07,460
really want you can write those decoding

00:15:01,070 --> 00:15:10,790
encoders manually mmm apart from getting

00:15:07,460 --> 00:15:13,220
users we of course also have the ability

00:15:10,790 --> 00:15:15,620
to post users in order to create new

00:15:13,220 --> 00:15:18,230
users and and here I have something

00:15:15,620 --> 00:15:21,140
which I call a diverging response

00:15:18,230 --> 00:15:23,570
because if I send this add user a

00:15:21,140 --> 00:15:26,660
message to the user repository they're

00:15:23,570 --> 00:15:28,400
two possible outcomes well that was the

00:15:26,660 --> 00:15:31,220
failure case because I get back future

00:15:28,400 --> 00:15:33,320
but in the successful

00:15:31,220 --> 00:15:35,930
I could get a message a response

00:15:33,320 --> 00:15:41,330
username taken or an event user edit

00:15:35,930 --> 00:15:44,420
right so I need to deal with that by by

00:15:41,330 --> 00:15:46,460
matching on the diverging options but

00:15:44,420 --> 00:15:50,570
that can be done with the on success

00:15:46,460 --> 00:15:52,010
directive and yeah I don't want to dive

00:15:50,570 --> 00:15:53,300
into all the details but hopefully you

00:15:52,010 --> 00:15:55,310
can see that there's a couple of

00:15:53,300 --> 00:15:58,040
directors which are really powerful

00:15:55,310 --> 00:16:04,520
which give you a lot of flexibility in

00:15:58,040 --> 00:16:16,010
power to well build your route let me

00:16:04,520 --> 00:16:19,640
just build that again and okay we have

00:16:16,010 --> 00:16:23,140
it we have it no ok now we have it okay

00:16:19,640 --> 00:16:26,390
so let me again start the user service

00:16:23,140 --> 00:16:29,750
take a look at the logs okay that looks

00:16:26,390 --> 00:16:32,180
good and and and now we can do the

00:16:29,750 --> 00:16:34,130
following we can first just try to act

00:16:32,180 --> 00:16:37,820
the current users which gives us back an

00:16:34,130 --> 00:16:40,610
empty array and then we could for

00:16:37,820 --> 00:16:44,000
example create a first user and that

00:16:40,610 --> 00:16:47,720
worked well so if I try to get again I

00:16:44,000 --> 00:16:49,550
get and write with one element and you

00:16:47,720 --> 00:16:52,160
can figure out how the marshaling works

00:16:49,550 --> 00:16:55,540
for case classes it's trivial just

00:16:52,160 --> 00:17:01,070
mapping the parameters the fields two

00:16:55,540 --> 00:17:03,110
keys in this JSON object and if I try to

00:17:01,070 --> 00:17:05,840
you at the same user again we get user

00:17:03,110 --> 00:17:09,620
name taken all these things so akka HTTP

00:17:05,840 --> 00:17:11,960
he takes care of a lot of things content

00:17:09,620 --> 00:17:15,590
type a content negotiation and you even

00:17:11,960 --> 00:17:20,410
get nice States codes and all these

00:17:15,590 --> 00:17:25,250
things okay great now we have a system

00:17:20,410 --> 00:17:27,140
which does what should do is your

00:17:25,250 --> 00:17:28,280
possible to add users and and remove

00:17:27,140 --> 00:17:30,800
users all these things but it's of

00:17:28,280 --> 00:17:34,970
course not yet reactive why because if I

00:17:30,800 --> 00:17:37,490
just do a remove for this container

00:17:34,970 --> 00:17:40,250
sorry it's gone it's no longer available

00:17:37,490 --> 00:17:43,220
so it's not really super reactive it's

00:17:40,250 --> 00:17:49,549
not resilient so we need to take

00:17:43,220 --> 00:17:54,169
here of that and one important piece

00:17:49,549 --> 00:17:57,799
here is the persistence of the state of

00:17:54,169 --> 00:18:00,460
actors I haven't shown you in detail how

00:17:57,799 --> 00:18:03,230
this user repository is implemented but

00:18:00,460 --> 00:18:05,659
it's just an actor right so you send

00:18:03,230 --> 00:18:09,320
messages to it you add user and delete

00:18:05,659 --> 00:18:14,360
user and it has an internal state and

00:18:09,320 --> 00:18:16,610
once you stop that actor or just crash

00:18:14,360 --> 00:18:19,100
the whole system that state is gone and

00:18:16,610 --> 00:18:24,080
the question we have to answer is how

00:18:19,100 --> 00:18:27,230
can we recover how can we restore the

00:18:24,080 --> 00:18:29,659
state when we restart an actor or start

00:18:27,230 --> 00:18:32,990
an actor and the answer is ARCA

00:18:29,659 --> 00:18:36,289
persistence is event a sourcing please

00:18:32,990 --> 00:18:38,990
familiar with event sourcing okay most

00:18:36,289 --> 00:18:41,950
of you so for the others in a nutshell

00:18:38,990 --> 00:18:47,799
event sourcing is really different from

00:18:41,950 --> 00:18:50,659
traditional let's say create an update

00:18:47,799 --> 00:18:52,220
persistence because you never delete

00:18:50,659 --> 00:18:55,789
anything you never overwrite anything

00:18:52,220 --> 00:18:58,100
instead of storing the state you store

00:18:55,789 --> 00:19:01,580
all the state changes which are

00:18:58,100 --> 00:19:03,049
expressed as or model as events so in

00:19:01,580 --> 00:19:05,240
our persistence we would create an event

00:19:03,049 --> 00:19:07,789
for any very command Abela command would

00:19:05,240 --> 00:19:10,280
be add user with a user name which

00:19:07,789 --> 00:19:12,380
hasn't been taken yet okay which is

00:19:10,280 --> 00:19:15,230
still free and then we create an event

00:19:12,380 --> 00:19:17,630
called user edit and then we let the

00:19:15,230 --> 00:19:20,350
journal persist even the journal is the

00:19:17,630 --> 00:19:25,640
abstraction provided by our consistence

00:19:20,350 --> 00:19:30,049
for the database it could be any

00:19:25,640 --> 00:19:32,179
database I would say currently the

00:19:30,049 --> 00:19:33,740
Cassandra plugin for the journal is

00:19:32,179 --> 00:19:36,020
probably the most production-ready this

00:19:33,740 --> 00:19:38,840
was a very good Kafka base one and

00:19:36,020 --> 00:19:41,120
there's a couple of local ones which are

00:19:38,840 --> 00:19:42,380
not that interesting for me because if I

00:19:41,120 --> 00:19:44,780
want to build a reactive system I have

00:19:42,380 --> 00:19:47,539
to really go and build a distributed

00:19:44,780 --> 00:19:50,059
system and therefore I think it was to

00:19:47,539 --> 00:19:51,830
the database is the best option but

00:19:50,059 --> 00:19:55,190
anyway there's a journal abstraction

00:19:51,830 --> 00:19:55,690
when the journal comes back to us here

00:19:55,190 --> 00:19:57,919
and

00:19:55,690 --> 00:20:02,450
acknowledges that even has been

00:19:57,919 --> 00:20:04,909
persisted with then and only then apply

00:20:02,450 --> 00:20:06,770
the event to the actress state that's

00:20:04,909 --> 00:20:10,610
the whole idea here it's pretty simple

00:20:06,770 --> 00:20:14,090
and whenever an actor which the

00:20:10,610 --> 00:20:19,010
persistent actor gets started all the

00:20:14,090 --> 00:20:21,649
events are replayed so it ends up in the

00:20:19,010 --> 00:20:24,470
old state the state in which it was

00:20:21,649 --> 00:20:27,110
before there's so much more to say you

00:20:24,470 --> 00:20:29,380
can do time trials but not replaying all

00:20:27,110 --> 00:20:33,110
the events there's snapshots to avoid

00:20:29,380 --> 00:20:34,640
huge recovery delays because if you have

00:20:33,110 --> 00:20:36,919
to replay millions and millions of

00:20:34,640 --> 00:20:39,500
events that might just take too long so

00:20:36,919 --> 00:20:42,559
there's a lot to say which I don't have

00:20:39,500 --> 00:20:47,690
time for but therefore I just want to

00:20:42,559 --> 00:20:52,010
give you a quick glance into how you

00:20:47,690 --> 00:20:58,580
would how you dried that code so let's

00:20:52,010 --> 00:21:00,409
take a look at the user repository so as

00:20:58,580 --> 00:21:03,230
you can see here we are extending

00:21:00,409 --> 00:21:07,429
persistent actor that's the trait or at

00:21:03,230 --> 00:21:10,490
the clasp we have to extend from and a

00:21:07,429 --> 00:21:14,450
persistence actor has a persistence ID

00:21:10,490 --> 00:21:17,360
that has to be stable that assigns the

00:21:14,450 --> 00:21:19,669
events for this particular actor in the

00:21:17,360 --> 00:21:21,559
journal and and then it has to receive

00:21:19,669 --> 00:21:24,140
methods one is called receive command

00:21:21,559 --> 00:21:28,159
which is just for the normal messages I

00:21:24,140 --> 00:21:30,230
would say that replaces the receive so

00:21:28,159 --> 00:21:32,390
if there's a command which is not

00:21:30,230 --> 00:21:34,970
related to persistence at all like get

00:21:32,390 --> 00:21:38,630
users we can just handle it like we did

00:21:34,970 --> 00:21:41,960
before and if we get a persistence aware

00:21:38,630 --> 00:21:43,700
command like add user remove user for

00:21:41,960 --> 00:21:46,279
example add user is handled here in the

00:21:43,700 --> 00:21:49,789
add user method we first validate the

00:21:46,279 --> 00:21:51,679
command is valid can we add a user so if

00:21:49,789 --> 00:21:54,320
the users already contains the username

00:21:51,679 --> 00:21:58,809
we cannot so that's an unbalanced

00:21:54,320 --> 00:22:02,149
command and we send back in the bad

00:21:58,809 --> 00:22:05,029
confirmation username taken but else we

00:22:02,149 --> 00:22:08,870
invoke the persist method that tells the

00:22:05,029 --> 00:22:10,880
journal to take this event user added

00:22:08,870 --> 00:22:14,360
and store it and only when the turtle

00:22:10,880 --> 00:22:16,850
gets back to us it will invoke this

00:22:14,360 --> 00:22:17,480
callback and in this callback we do two

00:22:16,850 --> 00:22:20,300
things

00:22:17,480 --> 00:22:23,510
first we change the state by invoking

00:22:20,300 --> 00:22:28,360
receive recover receive recovers also

00:22:23,510 --> 00:22:31,700
the method that is invoked for recovery

00:22:28,360 --> 00:22:34,550
therefore we achieve getting into the

00:22:31,700 --> 00:22:36,020
same state again on recovery and after

00:22:34,550 --> 00:22:40,670
that we do some side effects like

00:22:36,020 --> 00:22:42,920
logging and sending messages okay so

00:22:40,670 --> 00:22:44,809
it's important to stand that only after

00:22:42,920 --> 00:22:48,559
successful persistence we apply the

00:22:44,809 --> 00:22:51,530
state internal state changes and that's

00:22:48,559 --> 00:22:55,750
the pattern and once we have that in

00:22:51,530 --> 00:23:01,220
place we can we can give it a try

00:22:55,750 --> 00:23:05,300
so let me first remove the current

00:23:01,220 --> 00:23:10,460
running thingy I also have to start

00:23:05,300 --> 00:23:12,740
Cassandra or some journal which I'm with

00:23:10,460 --> 00:23:16,640
her I'm doing I'm using Cassandra here I

00:23:12,740 --> 00:23:24,020
have to check the log because some was

00:23:16,640 --> 00:23:26,270
really a little slow in starting now it

00:23:24,020 --> 00:23:31,040
looks good now it has started and and

00:23:26,270 --> 00:23:34,000
now let me just run the Gatley user

00:23:31,040 --> 00:23:34,000
again

00:23:39,850 --> 00:23:52,660
looks good okay and then he'll get the

00:23:46,750 --> 00:23:59,110
users empty now let's add a user okay

00:23:52,660 --> 00:24:02,289
add it looks good we can now just remove

00:23:59,110 --> 00:24:10,710
the container let's just simply crash it

00:24:02,289 --> 00:24:14,860
and start it again and see what happens

00:24:10,710 --> 00:24:18,789
if we try to get the users again Oh

00:24:14,860 --> 00:24:21,820
hooray the user has survived the crash

00:24:18,789 --> 00:24:24,700
because we have used persistent actor so

00:24:21,820 --> 00:24:28,539
that's already getting us closer to a

00:24:24,700 --> 00:24:31,030
really reactive micro-service but

00:24:28,539 --> 00:24:33,520
there's more work to do so what we need

00:24:31,030 --> 00:24:38,320
next is okay here I already mentioned

00:24:33,520 --> 00:24:40,059
that if if we really want to have a very

00:24:38,320 --> 00:24:42,070
reactive system we need to distribute it

00:24:40,059 --> 00:24:43,390
we need to have multiple nodes we want

00:24:42,070 --> 00:24:47,799
to use our car remote you want to use

00:24:43,390 --> 00:24:51,429
our cluster and in docker

00:24:47,799 --> 00:24:53,110
you have so-called networks that isolate

00:24:51,429 --> 00:24:57,010
the containers so within a network

00:24:53,110 --> 00:24:59,049
containers can talk to each other and if

00:24:57,010 --> 00:25:04,419
you want to externally communicate you

00:24:59,049 --> 00:25:06,539
need to publish ports and if you go for

00:25:04,419 --> 00:25:10,179
a multi host deployment I highly

00:25:06,539 --> 00:25:12,460
recommend using overlay networks like

00:25:10,179 --> 00:25:15,700
supported by two o'clock or OS and so on

00:25:12,460 --> 00:25:18,970
because then you have like a virtual

00:25:15,700 --> 00:25:21,400
network where those hosts sorry those

00:25:18,970 --> 00:25:23,200
containers can talk to each other you

00:25:21,400 --> 00:25:24,669
don't have to distinguish between public

00:25:23,200 --> 00:25:28,110
and bound address in our remote I can

00:25:24,669 --> 00:25:30,730
remote is a feature where you can have a

00:25:28,110 --> 00:25:33,490
find a dress and a published address

00:25:30,730 --> 00:25:36,880
which is great if you cannot use overlay

00:25:33,490 --> 00:25:38,559
networks but with an overlay network you

00:25:36,880 --> 00:25:42,460
just don't have to worry you don't need

00:25:38,559 --> 00:25:44,590
that nice feature okay so in order to

00:25:42,460 --> 00:25:46,510
become reactive with a cluster we need

00:25:44,590 --> 00:25:48,760
to use the cluster actor ref provider

00:25:46,510 --> 00:25:51,730
akka is distribute by default so

00:25:48,760 --> 00:25:52,890
everything should work in a distorted

00:25:51,730 --> 00:25:54,600
setting

00:25:52,890 --> 00:26:00,240
most of the things you have to do is

00:25:54,600 --> 00:26:03,179
just configuration one thing to note is

00:26:00,240 --> 00:26:05,570
that in order to form a cluster there's

00:26:03,179 --> 00:26:09,350
an approach using so-called seat nodes

00:26:05,570 --> 00:26:12,750
which are so and in the darker case

00:26:09,350 --> 00:26:15,570
those container dresses are not known

00:26:12,750 --> 00:26:19,529
upfront so it's you cannot use seed

00:26:15,570 --> 00:26:22,140
nodes like you would do in a more static

00:26:19,529 --> 00:26:25,289
setting and that's library called

00:26:22,140 --> 00:26:28,409
constructor which I have offered that

00:26:25,289 --> 00:26:30,779
helps with bootstrapping in a cluster so

00:26:28,409 --> 00:26:34,590
if you have a coordination service like

00:26:30,779 --> 00:26:39,960
at CD or zookeeper or console that is

00:26:34,590 --> 00:26:41,820
used by constructor and the existing

00:26:39,960 --> 00:26:45,600
nodes of the cluster are looked up there

00:26:41,820 --> 00:26:49,620
and bootstrapping all the corner cases

00:26:45,600 --> 00:26:51,360
are hopefully covered I don't have the

00:26:49,620 --> 00:26:53,580
time to really show you the details of

00:26:51,360 --> 00:26:56,190
the state machine but anyway that

00:26:53,580 --> 00:27:03,539
library helps with constructing an ARCA

00:26:56,190 --> 00:27:06,830
cluster and with that in place we now

00:27:03,539 --> 00:27:10,740
can use one or more of those high-level

00:27:06,830 --> 00:27:14,760
modules which akka cluster gives you and

00:27:10,740 --> 00:27:17,519
one is the cluster tools library it has

00:27:14,760 --> 00:27:19,980
the cluster singleton implementation and

00:27:17,519 --> 00:27:22,769
that is to run exactly one actor

00:27:19,980 --> 00:27:23,309
instance in the whole cluster well not

00:27:22,769 --> 00:27:27,090
exactly

00:27:23,309 --> 00:27:29,399
at most it's more precise so there's a

00:27:27,090 --> 00:27:31,830
class of singleton manager and that will

00:27:29,399 --> 00:27:35,850
manage the life type lifecycle of this

00:27:31,830 --> 00:27:40,460
singleton actor so if that goes away for

00:27:35,850 --> 00:27:44,220
whatever reason it is started on an

00:27:40,460 --> 00:27:46,500
existing node again so you really don't

00:27:44,220 --> 00:27:48,419
know where this actor is living and

00:27:46,500 --> 00:27:53,370
therefore you need the cluster singleton

00:27:48,419 --> 00:27:57,919
proxy to talk to your actor to your

00:27:53,370 --> 00:28:09,440
singleton actor let me

00:27:57,919 --> 00:28:11,960
I'll show you how to do that so in in

00:28:09,440 --> 00:28:14,480
the app in this class which has a main

00:28:11,960 --> 00:28:17,059
method where we just create an active

00:28:14,480 --> 00:28:21,470
system and then create a route actor

00:28:17,059 --> 00:28:26,059
which does all the other things we we

00:28:21,470 --> 00:28:28,190
create our user repository sorry here it

00:28:26,059 --> 00:28:30,110
is the user repository actor not

00:28:28,190 --> 00:28:33,230
directly as a child actor of the route

00:28:30,110 --> 00:28:35,210
like we did before but instead we use

00:28:33,230 --> 00:28:38,690
the cluster singleton manager which is

00:28:35,210 --> 00:28:42,889
started on every node and therefore via

00:28:38,690 --> 00:28:44,990
communication with its peers knows where

00:28:42,889 --> 00:28:48,919
to create that singleton it's usually

00:28:44,990 --> 00:28:50,659
the it is the oldest node and only if

00:28:48,919 --> 00:28:57,490
that goes away it will create a new one

00:28:50,659 --> 00:28:57,490
okay so um let's see how that works so

00:29:01,690 --> 00:29:13,309
what we can do now is we can run a play

00:29:06,679 --> 00:29:16,850
user with the default port and also with

00:29:13,309 --> 00:29:20,929
another port so what that gives us

00:29:16,850 --> 00:29:30,580
hopefully is that we can access the

00:29:20,929 --> 00:29:41,539
users out right there is no good okay

00:29:30,580 --> 00:29:43,490
Tucker logs Kapler user ah yeah I see so

00:29:41,539 --> 00:29:47,559
I mentioned this nice little library to

00:29:43,490 --> 00:29:50,380
bootstrap the cluster called constructor

00:29:47,559 --> 00:29:54,769
you remember what it needs a

00:29:50,380 --> 00:30:01,399
coordination service so I have to run at

00:29:54,769 --> 00:30:04,639
CD okay let me do that exercise again so

00:30:01,399 --> 00:30:10,480
we have Cassandra we have a CD and now

00:30:04,639 --> 00:30:12,870
I'm trying to start a getler again

00:30:10,480 --> 00:30:12,870
yeah

00:30:20,700 --> 00:30:31,049
go away please okay now I have really

00:30:26,279 --> 00:30:34,850
high confidence that it is working yes

00:30:31,049 --> 00:30:43,970
great so this one is working let's

00:30:34,850 --> 00:30:49,259
create another one and curl it hooray so

00:30:43,970 --> 00:30:52,590
I'll do it again we can either call the

00:30:49,259 --> 00:30:56,489
one or the other and both of these

00:30:52,590 --> 00:30:58,109
running services will give us that one

00:30:56,489 --> 00:31:01,379
user and that is because the user

00:30:58,109 --> 00:31:05,730
repository is really only running once

00:31:01,379 --> 00:31:08,850
in in the cluster and I can so the

00:31:05,730 --> 00:31:13,109
oldest one was the first so I can I can

00:31:08,850 --> 00:31:16,590
kill one of the notes it is kept by user

00:31:13,109 --> 00:31:18,649
zero and still the system should be

00:31:16,590 --> 00:31:21,840
available as you can see here so

00:31:18,649 --> 00:31:31,039
everything is fine class of singleton

00:31:21,840 --> 00:31:33,330
helps with that and now we enter a a

00:31:31,039 --> 00:31:37,470
library and a pattern which is really

00:31:33,330 --> 00:31:39,119
neat for event based collaboration of

00:31:37,470 --> 00:31:42,389
services because what we have seen was a

00:31:39,119 --> 00:31:45,570
single service and as I said you know

00:31:42,389 --> 00:31:48,539
said one Microsoft's no service ah so we

00:31:45,570 --> 00:31:50,489
need those services to color bright and

00:31:48,539 --> 00:31:53,730
so some events I think are a nice way to

00:31:50,489 --> 00:31:55,350
do that I just came to I just noticed

00:31:53,730 --> 00:31:57,059
that I don't know half a year ago

00:31:55,350 --> 00:32:00,059
because usually service and events are

00:31:57,059 --> 00:32:02,999
just for pushing events to the browser

00:32:00,059 --> 00:32:06,450
it's a standardized API and it's

00:32:02,999 --> 00:32:12,119
supported by most most every reasonable

00:32:06,450 --> 00:32:14,879
browser and yeah so it's a very very

00:32:12,119 --> 00:32:17,039
simple protocol completely based on HTTP

00:32:14,879 --> 00:32:19,499
you do a get and then the server

00:32:17,039 --> 00:32:22,739
responds with a response which is kept

00:32:19,499 --> 00:32:29,099
open it's not closed so it's almost like

00:32:22,739 --> 00:32:30,929
long polling at standardized and content

00:32:29,099 --> 00:32:33,590
type has to be text event stream and the

00:32:30,929 --> 00:32:39,260
events themselves are really simple

00:32:33,590 --> 00:32:45,110
new line based text-based things utf-8

00:32:39,260 --> 00:32:47,810
so that is a neat way to communicate or

00:32:45,110 --> 00:32:53,060
to let them collaborate so what I want

00:32:47,810 --> 00:32:57,200
to show you next is how to implement a

00:32:53,060 --> 00:33:00,710
way to publish user events I mean when

00:32:57,200 --> 00:33:05,380
we create a new user or if we remove a

00:33:00,710 --> 00:33:07,490
new user that needs to be published to

00:33:05,380 --> 00:33:10,970
interested parties to other services

00:33:07,490 --> 00:33:14,090
right and that can be done using certain

00:33:10,970 --> 00:33:19,010
events so let me show you that so the

00:33:14,090 --> 00:33:21,920
user API is here and let me just compile

00:33:19,010 --> 00:33:26,330
that so it's getting faster later our

00:33:21,920 --> 00:33:30,290
route which is shown here not only

00:33:26,330 --> 00:33:34,490
contains the users it now also contains

00:33:30,290 --> 00:33:37,010
the users events and this route is a

00:33:34,490 --> 00:33:39,890
little more complicated because in the

00:33:37,010 --> 00:33:43,010
service and event specification the

00:33:39,890 --> 00:33:46,490
client could send a header called last

00:33:43,010 --> 00:33:53,090
event ID which the server then should

00:33:46,490 --> 00:33:58,400
treat in a way to only start pushing

00:33:53,090 --> 00:33:59,750
events from that last event ID and that

00:33:58,400 --> 00:34:02,150
is optional so we have to deal with that

00:33:59,750 --> 00:34:04,340
so but what we are doing here is we go

00:34:02,150 --> 00:34:07,010
to the user posit or II we tell it or we

00:34:04,340 --> 00:34:08,180
ask it to get the user events from the

00:34:07,010 --> 00:34:11,570
sequence number which is which is

00:34:08,180 --> 00:34:13,760
defined by the last event of E and we

00:34:11,570 --> 00:34:18,650
map it to user events usually event is

00:34:13,760 --> 00:34:21,380
just another element or class in our two

00:34:18,650 --> 00:34:24,080
main model as you can see here and then

00:34:21,380 --> 00:34:27,290
we map the user events to server sent

00:34:24,080 --> 00:34:31,220
event which is essentially a case class

00:34:27,290 --> 00:34:36,380
you can see here provided by the akka

00:34:31,220 --> 00:34:38,780
ssv library that in place as you can see

00:34:36,380 --> 00:34:40,490
here we can complete the request so what

00:34:38,780 --> 00:34:47,290
we are really doing here is we complete

00:34:40,490 --> 00:34:47,290
the request with a source of

00:34:47,630 --> 00:34:53,720
with a source of server-sent events if

00:34:50,930 --> 00:34:56,110
you look at the type here oh it's a

00:34:53,720 --> 00:34:59,570
future okay anyway you're just great too

00:34:56,110 --> 00:35:02,450
so we can complete requests with futures

00:34:59,570 --> 00:35:04,310
as you have seen before and in this case

00:35:02,450 --> 00:35:07,460
here when we map it to service an event

00:35:04,310 --> 00:35:10,640
okay as the sea takes care of the

00:35:07,460 --> 00:35:14,330
marshaling and and treating those events

00:35:10,640 --> 00:35:15,970
that when they when they arrive as as as

00:35:14,330 --> 00:35:20,710
needed

00:35:15,970 --> 00:35:20,710
okay so let's check that out

00:35:29,300 --> 00:35:38,200
has been built I run it

00:35:52,730 --> 00:36:02,829
what's that constructor it's not

00:36:00,470 --> 00:36:07,000
creating the system okay I will just

00:36:02,829 --> 00:36:07,000
abort everything huh

00:36:07,750 --> 00:36:11,829
those pesky live demos

00:36:12,220 --> 00:36:22,819
let me run ad CD let me run Cassandra

00:36:17,089 --> 00:36:24,950
and maybe this state of the example is

00:36:22,819 --> 00:36:27,650
broken so I will even move forward to

00:36:24,950 --> 00:36:29,359
the next state and I have tested that

00:36:27,650 --> 00:36:33,650
before so I know it's working

00:36:29,359 --> 00:36:37,309
okay so what what we are getting at now

00:36:33,650 --> 00:36:40,040
is two services collaborating and I will

00:36:37,309 --> 00:36:41,480
show you this how such an event stream

00:36:40,040 --> 00:36:46,040
looks like but the base idea is the

00:36:41,480 --> 00:36:49,329
following if you have two services where

00:36:46,040 --> 00:36:52,160
the user events come from the right side

00:36:49,329 --> 00:36:55,730
your your client your service which is

00:36:52,160 --> 00:36:58,940
consuming them should just well run in a

00:36:55,730 --> 00:37:01,369
loop and and consume the the events as

00:36:58,940 --> 00:37:06,859
they arrive and if the connection breaks

00:37:01,369 --> 00:37:10,180
or for whatever reason is closed it just

00:37:06,859 --> 00:37:14,829
reconnect it uses this last event ID so

00:37:10,180 --> 00:37:20,859
any client of the user service

00:37:14,829 --> 00:37:23,299
interested in user events would would

00:37:20,859 --> 00:37:25,700
collect those and extract the data it

00:37:23,299 --> 00:37:28,250
needs and store the data in its own

00:37:25,700 --> 00:37:31,520
database I mean this is the idea of

00:37:28,250 --> 00:37:35,750
autonomy right a service needs to own

00:37:31,520 --> 00:37:44,160
its its data and in the case of the chat

00:37:35,750 --> 00:37:48,390
service now we have a second

00:37:44,160 --> 00:37:51,210
here and that the chat service also has

00:37:48,390 --> 00:37:55,670
a repository for users so it it takes

00:37:51,210 --> 00:37:58,410
care of storing the users but you cannot

00:37:55,670 --> 00:38:04,349
add users to the chat service directly

00:37:58,410 --> 00:38:06,750
this is done by going to the user events

00:38:04,349 --> 00:38:08,190
from the user service so as you can see

00:38:06,750 --> 00:38:17,520
here the user depository at the chat

00:38:08,190 --> 00:38:21,780
service has a URI which it uses to here

00:38:17,520 --> 00:38:25,349
which it uses to connect to the the user

00:38:21,780 --> 00:38:27,450
service and get the events this is

00:38:25,349 --> 00:38:30,539
abstracted away in this server sent

00:38:27,450 --> 00:38:33,210
events client which does all they have

00:38:30,539 --> 00:38:37,619
lists lifting with the reconnection if

00:38:33,210 --> 00:38:40,140
if if the connection is dropped all

00:38:37,619 --> 00:38:45,240
these things so it's some quite

00:38:40,140 --> 00:38:47,190
interesting stuff with a graph so it's

00:38:45,240 --> 00:38:49,230
it's not a simple flow at the linear

00:38:47,190 --> 00:38:52,049
processing pipeline but akka streams

00:38:49,230 --> 00:38:53,760
used as a cyclic graph pretty

00:38:52,049 --> 00:38:57,630
interesting stuff you can you can take a

00:38:53,760 --> 00:39:00,150
look if you're interested but yeah to

00:38:57,630 --> 00:39:02,130
make that easy to use

00:39:00,150 --> 00:39:04,950
I have abstracted away in this simple

00:39:02,130 --> 00:39:07,049
servant event client and what I'm going

00:39:04,950 --> 00:39:09,900
to show now is the following if I really

00:39:07,049 --> 00:39:13,740
start to of those services so what I'm

00:39:09,900 --> 00:39:19,049
going to do is now I run together user

00:39:13,740 --> 00:39:24,690
and I also run the gap of chat and I

00:39:19,049 --> 00:39:28,529
hope what oh I have to build it hmm ah

00:39:24,690 --> 00:39:28,799
maybe that was my mistake before I don't

00:39:28,529 --> 00:39:32,809
know

00:39:28,799 --> 00:39:32,809
so let me publish local

00:39:37,340 --> 00:39:45,330
okay now both images should have been

00:39:41,220 --> 00:39:50,640
built so I run the user service and I

00:39:45,330 --> 00:39:58,230
run the chat service my script doesn't

00:39:50,640 --> 00:40:04,170
remove the old one that's bad now okay

00:39:58,230 --> 00:40:04,920
docker logs gather chat up and running

00:40:04,170 --> 00:40:07,470
that looks good

00:40:04,920 --> 00:40:11,430
grab a user up and running that looks

00:40:07,470 --> 00:40:14,430
good so let's follow the chat here so

00:40:11,430 --> 00:40:15,930
that's the chat service and that it's

00:40:14,430 --> 00:40:23,180
listening to events of the user service

00:40:15,930 --> 00:40:28,100
so that means if we add a user to the

00:40:23,180 --> 00:40:30,990
user service currently we only have 0

00:40:28,100 --> 00:40:33,780
users because we use restart Cassandra

00:40:30,990 --> 00:40:36,330
now add one we added and can you see it

00:40:33,780 --> 00:40:40,800
here so it was pretty quick huh so this

00:40:36,330 --> 00:40:43,470
is the chat service and it added the

00:40:40,800 --> 00:40:45,570
user with user name one which had been

00:40:43,470 --> 00:40:46,859
added in this other service so let me do

00:40:45,570 --> 00:40:49,680
that again

00:40:46,859 --> 00:40:53,850
it is quick but still so if you look up

00:40:49,680 --> 00:40:55,740
here it takes some time like two seconds

00:40:53,850 --> 00:41:00,420
three seconds so it's a synchronous

00:40:55,740 --> 00:41:04,220
collaboration using events and that

00:41:00,420 --> 00:41:07,800
brings me to the end of the talk

00:41:04,220 --> 00:41:11,609
everything is available online in

00:41:07,800 --> 00:41:13,400
particular the sample application I have

00:41:11,609 --> 00:41:17,400
shown you the code I have shown you

00:41:13,400 --> 00:41:23,970
under my github hi cozy burger

00:41:17,400 --> 00:41:26,940
HT burger account and I think we we have

00:41:23,970 --> 00:41:32,490
time for questions are there any

00:41:26,940 --> 00:41:36,710
questions how do we do that - do we have

00:41:32,490 --> 00:41:36,710
a mic people who have questions

00:41:43,980 --> 00:41:49,420
laughter you can repeat the question

00:41:46,150 --> 00:41:52,200
hello oh no it works so I have a

00:41:49,420 --> 00:41:56,740
question about the different services

00:41:52,200 --> 00:41:59,020
being autonomous do you create a cluster

00:41:56,740 --> 00:42:01,480
for each of those innaka clustering or

00:41:59,020 --> 00:42:03,760
do you have a share cluster with

00:42:01,480 --> 00:42:05,829
multiple sources in them yeah that's the

00:42:03,760 --> 00:42:08,170
question yeah that's a very good

00:42:05,829 --> 00:42:10,660
question so I didn't explain that no so

00:42:08,170 --> 00:42:13,750
each services autonomous and could have

00:42:10,660 --> 00:42:16,480
should have it's it's its own cluster

00:42:13,750 --> 00:42:18,820
because its autonomous it should not be

00:42:16,480 --> 00:42:22,480
in the same cluster I mean

00:42:18,820 --> 00:42:25,150
Anaka cluster the membership to a

00:42:22,480 --> 00:42:26,800
cluster depends on the name of the

00:42:25,150 --> 00:42:29,349
active system so if you name them

00:42:26,800 --> 00:42:31,720
differently like the Java user and the

00:42:29,349 --> 00:42:33,730
Kepler chat then they cannot be in the

00:42:31,720 --> 00:42:35,380
same cluster anyway and I think that's

00:42:33,730 --> 00:42:38,290
that's a good thing that's what you want

00:42:35,380 --> 00:42:41,320
right so each service autonomous deploy

00:42:38,290 --> 00:42:44,490
to one know two three four ten nodes and

00:42:41,320 --> 00:42:44,490
and that's it

00:42:45,359 --> 00:42:48,510
more questions

00:42:48,690 --> 00:42:51,690
hello

00:42:52,180 --> 00:42:57,490
how can you or how would you suggest

00:42:54,099 --> 00:43:00,760
dealing with serialization of the of the

00:42:57,490 --> 00:43:02,500
user model between so from the chat to

00:43:00,760 --> 00:43:06,310
the Goblin you sent the user model from

00:43:02,500 --> 00:43:09,700
one site to the other so with context

00:43:06,310 --> 00:43:11,680
rejection the serialization yes so how

00:43:09,700 --> 00:43:13,800
would you deal with handling the user

00:43:11,680 --> 00:43:16,420
objects that you defined in the one

00:43:13,800 --> 00:43:18,280
application in the other application and

00:43:16,420 --> 00:43:22,030
how would you share that definition or

00:43:18,280 --> 00:43:23,859
yeah yeah so that's a really good

00:43:22,030 --> 00:43:28,119
question it's a really hard problem

00:43:23,859 --> 00:43:31,349
because even if you solve it once things

00:43:28,119 --> 00:43:34,869
might change so if you think about

00:43:31,349 --> 00:43:39,010
serialization of data either to arca

00:43:34,869 --> 00:43:42,400
persistence or to another service to

00:43:39,010 --> 00:43:44,829
consume it you might run into version

00:43:42,400 --> 00:43:46,240
problems if you change your object and

00:43:44,829 --> 00:43:50,349
therefore change the serialization

00:43:46,240 --> 00:43:50,799
format you need to think about that up

00:43:50,349 --> 00:43:52,509
front

00:43:50,799 --> 00:43:54,429
you definitely need to think about that

00:43:52,509 --> 00:43:59,439
often it's very important and I think

00:43:54,429 --> 00:44:01,329
the way to treat it is the way in which

00:43:59,439 --> 00:44:03,429
we always treat like incompatible

00:44:01,329 --> 00:44:06,489
changes so that might be compatible

00:44:03,429 --> 00:44:10,059
changes in JSON if you just add a field

00:44:06,489 --> 00:44:11,829
that should not break the user who

00:44:10,059 --> 00:44:13,059
doesn't even know that field but if you

00:44:11,829 --> 00:44:16,269
rename a field that's a breaking change

00:44:13,059 --> 00:44:20,249
so then you should probably offer a

00:44:16,269 --> 00:44:23,890
second API version to version 3 whatever

00:44:20,249 --> 00:44:27,789
so you would then have to support at

00:44:23,890 --> 00:44:30,640
least for some time two versions of your

00:44:27,789 --> 00:44:32,769
API so in this case the user event

00:44:30,640 --> 00:44:34,809
stream there would be two versions of

00:44:32,769 --> 00:44:36,910
that two streams right one with the old

00:44:34,809 --> 00:44:42,219
and one with new format if they are not

00:44:36,910 --> 00:44:45,779
compatible that's really technically

00:44:42,219 --> 00:44:48,779
it's easy to do but for a product

00:44:45,779 --> 00:44:51,969
lifecycle manager that's a hard thing

00:44:48,779 --> 00:44:53,380
and should be considered upfront so

00:44:51,969 --> 00:44:55,959
would you recommend tools like protobuf

00:44:53,380 --> 00:44:57,369
and three so would you recommend tools

00:44:55,959 --> 00:44:59,319
like prototype birth or something to

00:44:57,369 --> 00:45:02,049
extract away from the versioning that

00:44:59,319 --> 00:45:04,660
gives you some way to maybe maybe I

00:45:02,049 --> 00:45:07,420
don't have a final answer to that but

00:45:04,660 --> 00:45:11,799
definitely definitely some tools like

00:45:07,420 --> 00:45:13,779
that or or other by the way akka does

00:45:11,799 --> 00:45:15,939
offer help with that in particular with

00:45:13,779 --> 00:45:19,239
our consistence where you really want to

00:45:15,939 --> 00:45:21,569
be able to restore the old events from

00:45:19,239 --> 00:45:24,880
last year or maybe from ten years ago

00:45:21,569 --> 00:45:26,199
there's an adapter layer in our copper

00:45:24,880 --> 00:45:29,349
systems which you can and probably

00:45:26,199 --> 00:45:32,079
should use all right so we are running

00:45:29,349 --> 00:45:33,849
out of time Thanks you can catch me down

00:45:32,079 --> 00:45:38,039
at the booth or somewhere enjoy the rest

00:45:33,849 --> 00:45:38,039

YouTube URL: https://www.youtube.com/watch?v=nL4XoH2_Lew


