Title: Deep Learning and NLP with Spark by Andy Petrella and Melanie Warrick
Publication date: 2017-01-19
Playlist: Scala Days 2016
Description: 
	This video was recorded at Scala Days Berlin 2016
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

Anstract:
Deep Learning is taking data science by storm. Unfortunately, most existing solutions aren’t particularly scalable. In this talk, we will show how we can easily implement a Spark­ ready version one of the most complex deep learning models, the Long Short­ Term Memory (LSTM) neural network, widely used in the hardest natural language processing and understanding problems, such as automatic summarization, machine translation, question answering and discourse. We will also show an LSTM demo with interactive, real­time visualizations using the Spark Notebook and Spark Streaming.
Captions: 
	00:00:00,570 --> 00:00:04,319
so my name is Melanie Warrick and this

00:00:02,580 --> 00:00:05,970
is Andy per trellis and we are here to

00:00:04,319 --> 00:00:08,940
talk to you guys about deep learning and

00:00:05,970 --> 00:00:10,650
NLP with spark and specifically if you

00:00:08,940 --> 00:00:14,009
don't know NLP is natural language

00:00:10,650 --> 00:00:16,139
processing so we're going to show you a

00:00:14,009 --> 00:00:17,640
demo and I'm gonna give you just kind of

00:00:16,139 --> 00:00:20,430
an overview a few things before we get

00:00:17,640 --> 00:00:23,550
to the demo first off how many of you

00:00:20,430 --> 00:00:24,750
have done machine learning alright great

00:00:23,550 --> 00:00:25,920
we got a lot of people in the crowd that

00:00:24,750 --> 00:00:28,050
have done machine learning that's

00:00:25,920 --> 00:00:29,369
fantastic well for those of you who have

00:00:28,050 --> 00:00:31,619
and I'm just going to do a real brief

00:00:29,369 --> 00:00:33,649
overview machine learning the way I like

00:00:31,619 --> 00:00:36,210
to explain it is that it is about

00:00:33,649 --> 00:00:38,640
pattern recognition and prediction and

00:00:36,210 --> 00:00:39,809
using algorithms to do this there's a

00:00:38,640 --> 00:00:41,940
number of different types of algorithms

00:00:39,809 --> 00:00:43,469
that are out there the one in particular

00:00:41,940 --> 00:00:45,690
that we're going to go through today is

00:00:43,469 --> 00:00:47,730
known as natural language or known as

00:00:45,690 --> 00:00:49,890
neural nets using natural language

00:00:47,730 --> 00:00:51,300
processing now when we talk about

00:00:49,890 --> 00:00:53,219
machine learning just to give a little

00:00:51,300 --> 00:00:54,809
bit more information you know some great

00:00:53,219 --> 00:00:56,190
examples that exist out there and this

00:00:54,809 --> 00:00:58,289
is actually a really great example that

00:00:56,190 --> 00:01:00,780
exists out there deep in mind which is a

00:00:58,289 --> 00:01:03,960
company that's owned by Google recently

00:01:00,780 --> 00:01:05,640
won against the world champion in go

00:01:03,960 --> 00:01:08,189
they were able to create a program

00:01:05,640 --> 00:01:10,409
called alphago and it beats the world

00:01:08,189 --> 00:01:13,320
champion four out of five games that

00:01:10,409 --> 00:01:15,270
world champion Lisa Dahl was surprised

00:01:13,320 --> 00:01:18,509
what what was the most interesting thing

00:01:15,270 --> 00:01:21,869
about this is that there was moves that

00:01:18,509 --> 00:01:23,549
deepmind's game would play that a human

00:01:21,869 --> 00:01:25,590
would normally write off as being a bad

00:01:23,549 --> 00:01:27,600
direction to go in

00:01:25,590 --> 00:01:30,420
but the alphago system was able to

00:01:27,600 --> 00:01:32,130
assess a larger search space than what

00:01:30,420 --> 00:01:34,049
Lisa Dahl was able to take in and

00:01:32,130 --> 00:01:36,180
understand that it could take hits it

00:01:34,049 --> 00:01:37,350
could take losses losses that you and I

00:01:36,180 --> 00:01:39,780
would think would be a really big

00:01:37,350 --> 00:01:42,390
mistake or or even a small mistake and

00:01:39,780 --> 00:01:45,750
then it knew that those hits would

00:01:42,390 --> 00:01:47,280
actually not derail it and if anything

00:01:45,750 --> 00:01:51,240
give it the opportunity to win bigger

00:01:47,280 --> 00:01:54,210
later on so machine learning it's it's

00:01:51,240 --> 00:01:57,450
taking algorithms it's taking the making

00:01:54,210 --> 00:02:00,140
the ability to adapt and to learn that

00:01:57,450 --> 00:02:02,820
to allow that program to adapt and learn

00:02:00,140 --> 00:02:04,710
another example that I like to share is

00:02:02,820 --> 00:02:06,930
in Switzerland for search-and-rescue

00:02:04,710 --> 00:02:09,810
they are using drones that have been

00:02:06,930 --> 00:02:11,790
trained on images of different hiking

00:02:09,810 --> 00:02:13,500
paths and those drones have actually

00:02:11,790 --> 00:02:16,380
been shown to be able to identify

00:02:13,500 --> 00:02:18,450
anomalies in those paths to help track

00:02:16,380 --> 00:02:20,190
down people who are lost they're

00:02:18,450 --> 00:02:23,330
identifying those anomalies about three

00:02:20,190 --> 00:02:27,390
percent better than humans so far and

00:02:23,330 --> 00:02:28,920
another interesting example most people

00:02:27,390 --> 00:02:30,720
will write off speech recognition right

00:02:28,920 --> 00:02:32,700
now because when we talk about speech

00:02:30,720 --> 00:02:34,590
recognition devices when we look at Siri

00:02:32,700 --> 00:02:36,030
or some of the other examples that are

00:02:34,590 --> 00:02:37,920
out there you know we're like well we

00:02:36,030 --> 00:02:39,870
can't have a conversation with it so we

00:02:37,920 --> 00:02:41,970
write it off right because I mean I've

00:02:39,870 --> 00:02:43,860
watched Star Trek I expect to be able to

00:02:41,970 --> 00:02:45,600
have a conversation with my computer I

00:02:43,860 --> 00:02:50,370
don't understand why it's not at that

00:02:45,600 --> 00:02:52,110
point yet but the reality is it's it may

00:02:50,370 --> 00:02:54,240
not be at that point yet but we are

00:02:52,110 --> 00:02:55,560
getting so much closer I mean we are so

00:02:54,240 --> 00:02:57,420
much farther along than we were even

00:02:55,560 --> 00:02:59,160
just a couple years ago and we didn't

00:02:57,420 --> 00:03:02,310
have some of these devices even when we

00:02:59,160 --> 00:03:04,170
got into the 2000s to begin with then

00:03:02,310 --> 00:03:08,190
was just recently announced by the team

00:03:04,170 --> 00:03:09,959
that created Siri and Viv is this next

00:03:08,190 --> 00:03:11,220
step in what is known as natural

00:03:09,959 --> 00:03:13,260
language processing and speech

00:03:11,220 --> 00:03:16,110
recognition and if you haven't checked

00:03:13,260 --> 00:03:18,570
it out I recommend it it's you can't

00:03:16,110 --> 00:03:21,660
have necessarily a coherent conversation

00:03:18,570 --> 00:03:23,400
with Viviane however you can ask viv

00:03:21,660 --> 00:03:25,980
questions that are much more complex

00:03:23,400 --> 00:03:28,890
than you've ever seen before like show

00:03:25,980 --> 00:03:32,850
me the countries that were above 40

00:03:28,890 --> 00:03:34,410
degrees Celsius on Thursday or show me

00:03:32,850 --> 00:03:36,690
the cities that were above the 40

00:03:34,410 --> 00:03:38,880
degrees Celsius and supposedly viv is

00:03:36,690 --> 00:03:40,370
able to go in and give you that kind of

00:03:38,880 --> 00:03:43,260
nuance in that kind of detail and

00:03:40,370 --> 00:03:45,420
language is hard I mean it's hard enough

00:03:43,260 --> 00:03:47,640
with one language and we have you know

00:03:45,420 --> 00:03:50,549
so many different languages throughout

00:03:47,640 --> 00:03:53,280
the world and different meanings in the

00:03:50,549 --> 00:03:56,610
way the words are put together and and

00:03:53,280 --> 00:03:58,620
the tone of the way the words sound so

00:03:56,610 --> 00:04:01,170
language is hard this is definitely a

00:03:58,620 --> 00:04:03,000
hard problem and when you get into

00:04:01,170 --> 00:04:04,380
natural language processing there's a

00:04:03,000 --> 00:04:06,570
number of ways you can approach how

00:04:04,380 --> 00:04:08,400
you're going to apply it regularly

00:04:06,570 --> 00:04:09,690
you'll see chatbots or you'll see and I

00:04:08,400 --> 00:04:11,370
know some of the folks in speech

00:04:09,690 --> 00:04:13,320
recognition don't like the term tap up

00:04:11,370 --> 00:04:15,510
but you'll see question and answering

00:04:13,320 --> 00:04:16,890
types of solution you may see something

00:04:15,510 --> 00:04:19,260
where you're getting image kabbage

00:04:16,890 --> 00:04:20,669
captioning where you'll have some type

00:04:19,260 --> 00:04:23,520
of image and you're trying to identify

00:04:20,669 --> 00:04:24,900
objects in it or topic modeling is very

00:04:23,520 --> 00:04:26,600
common especially if you're working with

00:04:24,900 --> 00:04:28,400
any kind of recommender engines you

00:04:26,600 --> 00:04:30,680
trying to identify sort of core

00:04:28,400 --> 00:04:34,100
groupings core topics that will go

00:04:30,680 --> 00:04:35,630
across ranges language machine

00:04:34,100 --> 00:04:37,490
translation language translation in

00:04:35,630 --> 00:04:38,960
particular I know we've seen some great

00:04:37,490 --> 00:04:40,970
examples I think skypes got this

00:04:38,960 --> 00:04:42,890
incorporated where you can you know

00:04:40,970 --> 00:04:44,690
speak in one language and have it pretty

00:04:42,890 --> 00:04:46,220
quickly translated into another like

00:04:44,690 --> 00:04:49,550
speaking German and have it translated

00:04:46,220 --> 00:04:51,800
to Chinese and then language generation

00:04:49,550 --> 00:04:54,470
there's some fun examples out there now

00:04:51,800 --> 00:04:56,180
where literally you've got BOTS that can

00:04:54,470 --> 00:04:57,770
you know generate something that's

00:04:56,180 --> 00:05:00,050
almost akin to Shakespeare after it's

00:04:57,770 --> 00:05:02,090
consumed all those Shakespearean text

00:05:00,050 --> 00:05:03,740
but what's more interesting is the

00:05:02,090 --> 00:05:06,980
commercial applications we are already

00:05:03,740 --> 00:05:12,310
seeing that they have BOTS that can

00:05:06,980 --> 00:05:15,080
generate articles news articles now

00:05:12,310 --> 00:05:16,850
before I get to the example a couple

00:05:15,080 --> 00:05:20,000
other things I want to give a give you

00:05:16,850 --> 00:05:22,090
an idea about as I mentioned this is

00:05:20,000 --> 00:05:25,280
about using machine learning and

00:05:22,090 --> 00:05:26,630
specifically naturally or keep talking

00:05:25,280 --> 00:05:28,400
about natural language processing but it

00:05:26,630 --> 00:05:31,010
which is what we're doing but it

00:05:28,400 --> 00:05:33,470
specifically neural nets know neural

00:05:31,010 --> 00:05:34,910
nets it's a very complex topic it takes

00:05:33,470 --> 00:05:36,410
a lot of time to explain and there's

00:05:34,910 --> 00:05:38,270
some great resources out there to delve

00:05:36,410 --> 00:05:40,310
into this in more detail but if you have

00:05:38,270 --> 00:05:41,870
not experienced or looked at neural nets

00:05:40,310 --> 00:05:43,520
in the past I'm gonna try to give you a

00:05:41,870 --> 00:05:45,020
little bit of an understanding if you've

00:05:43,520 --> 00:05:47,840
seen it before then this is gonna be

00:05:45,020 --> 00:05:49,730
pretty basic you have a model that

00:05:47,840 --> 00:05:51,350
you're trying to generate it's almost

00:05:49,730 --> 00:05:53,270
like if you're doing linear regression

00:05:51,350 --> 00:05:55,490
linear regression has a model that

00:05:53,270 --> 00:05:58,010
models pretty basic it's y equals MX

00:05:55,490 --> 00:05:59,180
plus B when you are fitting your model

00:05:58,010 --> 00:06:01,610
to whatever problem you're solving

00:05:59,180 --> 00:06:04,940
you're going to be tweaking that M and

00:06:01,610 --> 00:06:07,280
that B your coefficients now this is a

00:06:04,940 --> 00:06:09,500
model that you're trying to fit this is

00:06:07,280 --> 00:06:11,750
a much more complex model it operates a

00:06:09,500 --> 00:06:13,280
not much higher level of dimension than

00:06:11,750 --> 00:06:15,890
two dimensions like you would do with

00:06:13,280 --> 00:06:18,230
linear regression now this model uses

00:06:15,890 --> 00:06:21,320
linear regression it uses some level let

00:06:18,230 --> 00:06:24,290
level of linear transformation as well

00:06:21,320 --> 00:06:25,280
as non linear transformation if you

00:06:24,290 --> 00:06:27,200
don't know what that means

00:06:25,280 --> 00:06:28,940
spend some time doing research but just

00:06:27,200 --> 00:06:31,400
understand there are some changes that

00:06:28,940 --> 00:06:33,380
it's doing to the data and ultimately

00:06:31,400 --> 00:06:34,730
it's taking whatever data you're feeding

00:06:33,380 --> 00:06:38,330
it and question you're trying to answer

00:06:34,730 --> 00:06:40,430
and trying to make this adapt to that

00:06:38,330 --> 00:06:42,560
question what's happening

00:06:40,430 --> 00:06:44,300
is usually you're bringing in a input

00:06:42,560 --> 00:06:46,700
you break down that input for it to

00:06:44,300 --> 00:06:49,550
consume and every node that I have up

00:06:46,700 --> 00:06:51,200
here that's blue is representing a

00:06:49,550 --> 00:06:53,420
different node that would be doing some

00:06:51,200 --> 00:06:56,000
type of computation and it would be it's

00:06:53,420 --> 00:06:58,040
doing its own fitting to the data so

00:06:56,000 --> 00:06:59,810
it's not gonna look the same in terms of

00:06:58,040 --> 00:07:01,880
the way it fits between every single

00:06:59,810 --> 00:07:03,560
blue node they're all going to be unique

00:07:01,880 --> 00:07:05,690
in the way they look at the the data

00:07:03,560 --> 00:07:08,240
that it's ingesting and when it gets to

00:07:05,690 --> 00:07:10,400
the end it's going to see what is my

00:07:08,240 --> 00:07:12,350
loss now usually when you use neural

00:07:10,400 --> 00:07:13,730
nets or or more traditionally when we've

00:07:12,350 --> 00:07:15,230
been using neural nets and commercial

00:07:13,730 --> 00:07:16,970
applications we've been using it for

00:07:15,230 --> 00:07:19,670
supervised learning which means it's

00:07:16,970 --> 00:07:21,770
labeled data so we know what the answer

00:07:19,670 --> 00:07:23,990
should look like and we can get an error

00:07:21,770 --> 00:07:25,940
and then what we do is we literally go

00:07:23,990 --> 00:07:28,730
back into the net and we're going to

00:07:25,940 --> 00:07:30,140
update every single node based on how

00:07:28,730 --> 00:07:33,680
much we think we need to change it to

00:07:30,140 --> 00:07:35,210
better fit whatever we're solving yeah

00:07:33,680 --> 00:07:37,640
its abstract it's a little kind of

00:07:35,210 --> 00:07:39,440
nuance II like I said there's a lot more

00:07:37,640 --> 00:07:41,450
detail to this but just understand

00:07:39,440 --> 00:07:42,980
you're taking in some data you're doing

00:07:41,450 --> 00:07:44,810
computations on that data and you're

00:07:42,980 --> 00:07:47,510
doing it in multiple places with

00:07:44,810 --> 00:07:49,550
different variations on that computation

00:07:47,510 --> 00:07:51,260
you get to the end you find out what the

00:07:49,550 --> 00:07:53,240
problem is what the error is and you're

00:07:51,260 --> 00:07:55,430
gonna go back and try to adjust this

00:07:53,240 --> 00:07:56,870
model you're doing tweaks and you just

00:07:55,430 --> 00:07:59,270
keep doing that until you get to a point

00:07:56,870 --> 00:08:01,550
where you have a good score so that's a

00:07:59,270 --> 00:08:03,800
neural net roughly now these blue nodes

00:08:01,550 --> 00:08:06,020
this is a very simplistic model now

00:08:03,800 --> 00:08:07,970
technically you can just have one blue

00:08:06,020 --> 00:08:10,220
node one computation node and you've got

00:08:07,970 --> 00:08:12,530
a neural net but really you're going to

00:08:10,220 --> 00:08:14,750
usually have you know potentially

00:08:12,530 --> 00:08:16,220
hundreds thousands of nodes depending on

00:08:14,750 --> 00:08:17,990
the problem you're working with and each

00:08:16,220 --> 00:08:20,660
problem is kind of different in how you

00:08:17,990 --> 00:08:23,960
tackle it and this is also a very basic

00:08:20,660 --> 00:08:25,730
simplistic structure there's some pretty

00:08:23,960 --> 00:08:28,280
well-known established structures for

00:08:25,730 --> 00:08:29,510
certain types of problems and this one

00:08:28,280 --> 00:08:31,520
would be known as your sort of

00:08:29,510 --> 00:08:33,260
feed-forward fully connected structure

00:08:31,520 --> 00:08:35,540
it's very simplistic everything's

00:08:33,260 --> 00:08:36,830
connected to everything else now the

00:08:35,540 --> 00:08:38,510
structure we're going to show you guys

00:08:36,830 --> 00:08:40,940
today or the structure that is used in

00:08:38,510 --> 00:08:43,100
the example today is known as recurrent

00:08:40,940 --> 00:08:44,810
neural Nets now the difference with this

00:08:43,100 --> 00:08:47,360
structure versus what I've just shown

00:08:44,810 --> 00:08:49,850
you is that this structure is allowing

00:08:47,360 --> 00:08:52,850
you to keep track of state what it does

00:08:49,850 --> 00:08:53,590
is it allows the model those each of

00:08:52,850 --> 00:08:56,980
those blue now

00:08:53,590 --> 00:08:59,650
to know what it saw before and it is in

00:08:56,980 --> 00:09:01,750
that way able to say I understand what I

00:08:59,650 --> 00:09:04,270
saw just recently and I can make some

00:09:01,750 --> 00:09:06,700
better decisions because I have a sense

00:09:04,270 --> 00:09:07,990
of what came before this is very crucial

00:09:06,700 --> 00:09:10,240
when you're working with anything that's

00:09:07,990 --> 00:09:12,190
time series related and when you work

00:09:10,240 --> 00:09:14,590
with natural language processing you're

00:09:12,190 --> 00:09:16,480
working with orders of words the order

00:09:14,590 --> 00:09:17,890
of the words actually matters and that's

00:09:16,480 --> 00:09:19,930
something that you want to take into

00:09:17,890 --> 00:09:22,750
account when you're working and trying

00:09:19,930 --> 00:09:24,070
to assess it now recurrent Nets this

00:09:22,750 --> 00:09:24,580
idea has been around for a really long

00:09:24,070 --> 00:09:26,350
time

00:09:24,580 --> 00:09:27,910
in this space I mean actually neural

00:09:26,350 --> 00:09:30,880
nets front frankly have been around

00:09:27,910 --> 00:09:32,260
since the 50s and the recurrent net

00:09:30,880 --> 00:09:33,460
algorithm has been around or this

00:09:32,260 --> 00:09:35,860
structure has been around for a little

00:09:33,460 --> 00:09:37,540
while as well the thing is it was not

00:09:35,860 --> 00:09:39,580
seen as being very useful or valuable

00:09:37,540 --> 00:09:42,730
for the longest time because there's

00:09:39,580 --> 00:09:44,050
some issues with vanishing and exploding

00:09:42,730 --> 00:09:46,150
gradients if you don't know what a

00:09:44,050 --> 00:09:47,710
gradient is just understand that the

00:09:46,150 --> 00:09:49,300
gradient is what you use to take the

00:09:47,710 --> 00:09:51,820
error and go back and make changes to

00:09:49,300 --> 00:09:53,950
your net but ultimately it's got issues

00:09:51,820 --> 00:09:55,720
with what's or what's happening is that

00:09:53,950 --> 00:09:57,880
the computer is having a hard time

00:09:55,720 --> 00:10:00,910
keeping track of either the values get

00:09:57,880 --> 00:10:03,160
way too small or way too large so it's

00:10:00,910 --> 00:10:05,230
been going to zero or going to infinity

00:10:03,160 --> 00:10:07,570
and you're basically unable to go back

00:10:05,230 --> 00:10:10,240
and take errors and propagate them back

00:10:07,570 --> 00:10:13,360
into the net so this is one problem that

00:10:10,240 --> 00:10:15,910
recurrent Nets had and Along Came long

00:10:13,360 --> 00:10:18,250
term long short-term memory the LST ends

00:10:15,910 --> 00:10:20,140
this is a variant on recurrent Nets in

00:10:18,250 --> 00:10:22,090
particular so whenever you hear about

00:10:20,140 --> 00:10:24,250
recurrent Nets being used understand

00:10:22,090 --> 00:10:26,980
that the Ellis TM is what is usually

00:10:24,250 --> 00:10:28,840
being used to solve the problem there's

00:10:26,980 --> 00:10:30,400
so much more to this again I highly

00:10:28,840 --> 00:10:31,630
recommend spending some time researching

00:10:30,400 --> 00:10:33,310
it I just want you to have an

00:10:31,630 --> 00:10:35,260
understanding we're going to use neural

00:10:33,310 --> 00:10:37,540
nets as our model we're specifically

00:10:35,260 --> 00:10:40,120
using Ellis tans to solve this problem

00:10:37,540 --> 00:10:42,850
and that you know this information will

00:10:40,120 --> 00:10:44,970
be useful to further investigate later

00:10:42,850 --> 00:10:48,310
down the line okay a couple other things

00:10:44,970 --> 00:10:51,010
so the great thing about using recurrent

00:10:48,310 --> 00:10:53,410
Nets her Ellis teams is that you can

00:10:51,010 --> 00:10:55,330
have different variations on your inputs

00:10:53,410 --> 00:10:56,620
and your outputs usually when you work

00:10:55,330 --> 00:10:57,820
with any other kind of models you're

00:10:56,620 --> 00:11:00,670
gonna have a standard one-to-one

00:10:57,820 --> 00:11:02,140
relationship that one on the far while

00:11:00,670 --> 00:11:05,230
it gets to all of you guys it's the far

00:11:02,140 --> 00:11:06,460
left so the one to one is what you'll

00:11:05,230 --> 00:11:07,110
have you'll have like an input of an

00:11:06,460 --> 00:11:11,580
image and you

00:11:07,110 --> 00:11:12,960
an output of a title or a description of

00:11:11,580 --> 00:11:15,650
the image or something like that well

00:11:12,960 --> 00:11:18,600
now put of a word to the image let's say

00:11:15,650 --> 00:11:20,190
now a standard one-to-many situation

00:11:18,600 --> 00:11:22,500
would be something like a word Tyvek

00:11:20,190 --> 00:11:24,840
where you are going to take a corpus of

00:11:22,500 --> 00:11:26,700
words and you do a relationship to those

00:11:24,840 --> 00:11:29,010
words so you say I have one word and

00:11:26,700 --> 00:11:31,260
here's a list of all these words that

00:11:29,010 --> 00:11:33,540
are in close relation to that one word

00:11:31,260 --> 00:11:35,070
and that's a well-known model that

00:11:33,540 --> 00:11:38,550
exists out there in terms of giving us

00:11:35,070 --> 00:11:41,730
some similarities between words another

00:11:38,550 --> 00:11:44,190
example of one-to-many that has been

00:11:41,730 --> 00:11:45,810
done in the last couple years and this I

00:11:44,190 --> 00:11:47,610
saw this example actually being

00:11:45,810 --> 00:11:48,930
mentioned yesterday it's called image

00:11:47,610 --> 00:11:50,550
captioning there was a significant

00:11:48,930 --> 00:11:52,020
amount of research that came out from a

00:11:50,550 --> 00:11:54,420
number of the big players in the space

00:11:52,020 --> 00:11:56,520
and what they were able to do is

00:11:54,420 --> 00:11:58,860
literally take images and as you can see

00:11:56,520 --> 00:12:01,230
here not only identify objects and them

00:11:58,860 --> 00:12:03,210
but come up with descriptions come up

00:12:01,230 --> 00:12:05,970
with actual sentences to describe what

00:12:03,210 --> 00:12:08,910
this image is showing you so that is an

00:12:05,970 --> 00:12:10,710
example of your one-to-many what we're

00:12:08,910 --> 00:12:12,690
going to show you today is taking

00:12:10,710 --> 00:12:14,640
several reviews so that would be your

00:12:12,690 --> 00:12:17,370
many to one where we're going to try to

00:12:14,640 --> 00:12:19,320
show you is it a good or a bad review is

00:12:17,370 --> 00:12:21,690
it do we want to go see this movie or do

00:12:19,320 --> 00:12:23,460
we not want to go see this movie other

00:12:21,690 --> 00:12:25,740
examples that exist out there in terms

00:12:23,460 --> 00:12:27,810
of leveraging recurrent Nets especially

00:12:25,740 --> 00:12:29,310
in the NLP space is you may be doing

00:12:27,810 --> 00:12:30,990
language translation like we talked

00:12:29,310 --> 00:12:33,690
about where you would be doing something

00:12:30,990 --> 00:12:35,190
like German to French or you could be

00:12:33,690 --> 00:12:38,400
doing something where you have video and

00:12:35,190 --> 00:12:40,920
each image in that video you want to

00:12:38,400 --> 00:12:42,960
have some type of word associated to it

00:12:40,920 --> 00:12:45,120
so those are just to give you a sense of

00:12:42,960 --> 00:12:47,580
the sequence of sequence and when you're

00:12:45,120 --> 00:12:50,250
thinking about input and output for the

00:12:47,580 --> 00:12:52,490
neural net itself so as I mentioned

00:12:50,250 --> 00:12:54,630
we're going to do sentiment analysis and

00:12:52,490 --> 00:12:56,640
specifically we have a number of reviews

00:12:54,630 --> 00:12:58,140
we're going to put those into the net

00:12:56,640 --> 00:13:02,010
we're going to have it come back and

00:12:58,140 --> 00:13:04,500
tell us good bad not so good really just

00:13:02,010 --> 00:13:06,540
good bad okay and what we're going to

00:13:04,500 --> 00:13:09,630
use or what we are using in this example

00:13:06,540 --> 00:13:11,280
is couple things from the the reason why

00:13:09,630 --> 00:13:13,800
I'm here is because I work with sky mind

00:13:11,280 --> 00:13:16,140
and I'm an engineer with sky mind and we

00:13:13,800 --> 00:13:20,290
produce or we develop deep learning for

00:13:16,140 --> 00:13:23,050
J or DL for J its java-based

00:13:20,290 --> 00:13:25,960
solution in the neural net space that is

00:13:23,050 --> 00:13:27,910
integrates very easily with Scala if

00:13:25,960 --> 00:13:30,250
you've heard of I don't mind saying it

00:13:27,910 --> 00:13:32,050
because I'm work heavily in this space

00:13:30,250 --> 00:13:35,020
if you've heard of done tensorflow or

00:13:32,050 --> 00:13:36,790
cafe or torch we are also a library but

00:13:35,020 --> 00:13:38,170
our difference is that we are based in

00:13:36,790 --> 00:13:40,710
Java and we were able to integrate with

00:13:38,170 --> 00:13:43,720
Scala and we also integrate with SPARC

00:13:40,710 --> 00:13:45,970
we use Canova that's another one of the

00:13:43,720 --> 00:13:47,470
platform's we build out as a way for you

00:13:45,970 --> 00:13:50,080
to convert any kind of data that you're

00:13:47,470 --> 00:13:51,790
trying to pull into the nets and we're

00:13:50,080 --> 00:13:54,070
built off of in D for J and that's

00:13:51,790 --> 00:13:56,350
really our version of numpy if you've

00:13:54,070 --> 00:13:58,360
worked with Python at all but ultimately

00:13:56,350 --> 00:14:00,640
it's a linear algebra library and we've

00:13:58,360 --> 00:14:02,950
recently integrated it where we have it

00:14:00,640 --> 00:14:04,240
pretty much based off of C so all of

00:14:02,950 --> 00:14:06,700
these neural net libraries are

00:14:04,240 --> 00:14:09,460
leveraging C to optimize how you're

00:14:06,700 --> 00:14:11,230
working within memory so in D for J is

00:14:09,460 --> 00:14:12,820
built off of Liban D for J you really

00:14:11,230 --> 00:14:14,500
don't need to know about living D for J

00:14:12,820 --> 00:14:16,450
we handle all of that for you on the

00:14:14,500 --> 00:14:19,810
back end but I'm just making you aware

00:14:16,450 --> 00:14:21,460
so in D for J Canova both support DL for

00:14:19,810 --> 00:14:22,690
J and deal for J is where you're going

00:14:21,460 --> 00:14:24,520
to build out your models

00:14:22,690 --> 00:14:26,260
and that's the information about sky

00:14:24,520 --> 00:14:27,580
mine the other thing to note about us is

00:14:26,260 --> 00:14:29,140
that we've been building this out it's

00:14:27,580 --> 00:14:30,700
open source we're always looking for

00:14:29,140 --> 00:14:33,130
people to contribute so happy to have

00:14:30,700 --> 00:14:36,820
people help out especially with some of

00:14:33,130 --> 00:14:39,070
the work around Scala bindings but our

00:14:36,820 --> 00:14:41,440
goal is working to integrate so we can

00:14:39,070 --> 00:14:44,800
provide this for enterprise solutions

00:14:41,440 --> 00:14:46,720
and from there I'm gonna hand it off to

00:14:44,800 --> 00:14:49,480
Andy he's gonna talk to you about the

00:14:46,720 --> 00:14:51,040
notebook that he's building his or the

00:14:49,480 --> 00:14:57,340
notebook that his company is provided

00:14:51,040 --> 00:15:00,970
and it's gonna be part of this demo I

00:14:57,340 --> 00:15:05,500
don't know one so thanks melody for this

00:15:00,970 --> 00:15:06,970
great intuition to NLP before that just

00:15:05,500 --> 00:15:09,460
a few words about the spaniard book I

00:15:06,970 --> 00:15:12,520
don't know if some of you were the talk

00:15:09,460 --> 00:15:15,940
that I gave yesterday with Dean no one a

00:15:12,520 --> 00:15:18,970
few of you okay are present at the end

00:15:15,940 --> 00:15:21,430
of the of the talk the repo that we use

00:15:18,970 --> 00:15:24,370
in order to present Scylla is the data

00:15:21,430 --> 00:15:28,000
science language and also a few notebook

00:15:24,370 --> 00:15:31,330
that explain what kind of library you

00:15:28,000 --> 00:15:33,820
can use Scala to do machine learning and

00:15:31,330 --> 00:15:36,070
it includes of course yell for J

00:15:33,820 --> 00:15:38,950
with MLM and other thing other things

00:15:36,070 --> 00:15:40,930
but it necessarily for deep learning you

00:15:38,950 --> 00:15:43,330
have to go to deep learning for jay-z is

00:15:40,930 --> 00:15:46,420
the best that I've used so far on the

00:15:43,330 --> 00:15:48,850
JVM so this my notebook is simply web

00:15:46,420 --> 00:15:50,980
page I will show an example to run some

00:15:48,850 --> 00:15:53,490
NLP the LA Fellows is the company that

00:15:50,980 --> 00:15:56,020
I've created a year ago with Xavier

00:15:53,490 --> 00:15:58,330
where we do the last science on data

00:15:56,020 --> 00:16:00,520
science and means that we are doing

00:15:58,330 --> 00:16:03,490
recommendations for data scientist based

00:16:00,520 --> 00:16:06,160
on the information that we can grab into

00:16:03,490 --> 00:16:10,000
the system where the data scientist is

00:16:06,160 --> 00:16:11,710
working on the HIV we are we are doing

00:16:10,000 --> 00:16:16,990
recommendation for the guy that does

00:16:11,710 --> 00:16:19,600
recommendation well so we had a project

00:16:16,990 --> 00:16:23,890
actually with Skyy mind a few a few

00:16:19,600 --> 00:16:27,280
months ago sky mine canonical IBM and

00:16:23,890 --> 00:16:31,200
Vidya and for that we created juju a

00:16:27,280 --> 00:16:34,000
jujube bundle who knows about juju right

00:16:31,200 --> 00:16:37,300
juju is very interesting tool it allows

00:16:34,000 --> 00:16:39,940
you know you to create and package tools

00:16:37,300 --> 00:16:43,570
link them together and deploy the whole

00:16:39,940 --> 00:16:45,460
thing at once onto you know open two

00:16:43,570 --> 00:16:48,790
closers but they are supporting it two

00:16:45,460 --> 00:16:51,340
core OS and other platforms including

00:16:48,790 --> 00:16:53,320
AWS so it's very interesting if you want

00:16:51,340 --> 00:16:54,970
to to build a whole cluster and deploy

00:16:53,320 --> 00:16:57,880
it at once using a simple comment it's

00:16:54,970 --> 00:17:01,180
not so far for terraform to be honest so

00:16:57,880 --> 00:17:03,370
we share with them a juju bumble

00:17:01,180 --> 00:17:05,920
includes the elf 4j so you don't have to

00:17:03,370 --> 00:17:08,170
you know take care of yourself about the

00:17:05,920 --> 00:17:10,120
differences of the approach and so on it

00:17:08,170 --> 00:17:12,640
installs Meadows Park in the spy

00:17:10,120 --> 00:17:14,620
notebook as the as the as the client so

00:17:12,640 --> 00:17:16,209
it looks like that actually so the juju

00:17:14,620 --> 00:17:19,630
will allow you to install all these

00:17:16,209 --> 00:17:23,860
things in one single line onto your

00:17:19,630 --> 00:17:27,220
report to do iws or whatever so this

00:17:23,860 --> 00:17:29,770
blog is explaining what has been done

00:17:27,220 --> 00:17:33,700
there what we've done actually is to do

00:17:29,770 --> 00:17:35,890
some NLP analyzes on Lars in order to

00:17:33,700 --> 00:17:37,590
detect failing nodes on an open star

00:17:35,890 --> 00:17:41,800
cluster

00:17:37,590 --> 00:17:43,030
well so regarding the the notebook that

00:17:41,800 --> 00:17:44,620
I want to show so yeah this is the

00:17:43,030 --> 00:17:46,100
message cluster I would just wanted to

00:17:44,620 --> 00:17:48,850
show that we

00:17:46,100 --> 00:17:52,820
we have done the work in order to have

00:17:48,850 --> 00:17:54,860
GPUs on Messrs so we can schedule also

00:17:52,820 --> 00:17:57,740
GPUs on this message cluster so we can

00:17:54,860 --> 00:18:00,980
leverage that because of course

00:17:57,740 --> 00:18:05,810
therefore J in Oviatt 11 D Lib and D 4j

00:18:00,980 --> 00:18:09,110
behind it is also linking to to CUDA so

00:18:05,810 --> 00:18:10,010
you can leverage the GPU power for for

00:18:09,110 --> 00:18:12,620
your analysis

00:18:10,010 --> 00:18:15,530
well so I'm going to hide a few things

00:18:12,620 --> 00:18:17,450
there to little more room yeah

00:18:15,530 --> 00:18:20,080
so you can you see that maybe I should

00:18:17,450 --> 00:18:26,840
increase a little bit the font something

00:18:20,080 --> 00:18:28,580
no it's okay so the idea here is to do

00:18:26,840 --> 00:18:30,680
two things melis so we want to do an LP

00:18:28,580 --> 00:18:32,980
right so we want to analyze text the

00:18:30,680 --> 00:18:36,590
text that we are going to analyze are

00:18:32,980 --> 00:18:39,080
the IMDB recommendations the reviews

00:18:36,590 --> 00:18:41,780
sorry so we we have a lot of reviews in

00:18:39,080 --> 00:18:43,730
NDB I want to see if it's a positive

00:18:41,780 --> 00:18:46,130
review what's a negative review right

00:18:43,730 --> 00:18:48,830
and but what would be the purpose of

00:18:46,130 --> 00:18:52,130
that is that you can then check if then

00:18:48,830 --> 00:18:56,360
you know the score assigned to a movie

00:18:52,130 --> 00:18:57,650
is actually respecting what are the

00:18:56,360 --> 00:18:58,940
comments in there because there is a

00:18:57,650 --> 00:19:00,680
high chance the comments are more

00:18:58,940 --> 00:19:04,520
important that the stars in themselves

00:19:00,680 --> 00:19:07,700
because the engagement is higher so in

00:19:04,520 --> 00:19:10,730
order to do that so we'll use a

00:19:07,700 --> 00:19:14,240
recurrent Network and specifically the

00:19:10,730 --> 00:19:19,700
STM to to solve the error propagation

00:19:14,240 --> 00:19:22,690
problem and to do so first we'll convert

00:19:19,700 --> 00:19:26,570
text you know the reviews into a space

00:19:22,690 --> 00:19:28,250
which is computationally interesting so

00:19:26,570 --> 00:19:29,330
we can do computation on it and the

00:19:28,250 --> 00:19:31,850
space that we're going to use is the

00:19:29,330 --> 00:19:35,660
word to vac one right as Melanie has

00:19:31,850 --> 00:19:39,080
explained in order to do that so we will

00:19:35,660 --> 00:19:41,000
use something provided by Google Google

00:19:39,080 --> 00:19:43,550
has trained the modern and put it

00:19:41,000 --> 00:19:46,370
publicly available they trying to work

00:19:43,550 --> 00:19:49,250
to fake a word to vac model onto their

00:19:46,370 --> 00:19:51,530
Google News datasets so there are a lot

00:19:49,250 --> 00:19:53,210
of words in there and they have a lot of

00:19:51,530 --> 00:19:57,110
information that means that the space is

00:19:53,210 --> 00:19:59,270
pretty well optimized so this notebook

00:19:57,110 --> 00:19:59,870
will be provided very soon so where it's

00:19:59,270 --> 00:20:02,809
still

00:19:59,870 --> 00:20:05,630
working on it - to clear a few things in

00:20:02,809 --> 00:20:08,030
there but we will share it publicly with

00:20:05,630 --> 00:20:09,650
the repo and so on Melanie you you can

00:20:08,030 --> 00:20:11,900
follow Melanie nightly hall on Twitter

00:20:09,650 --> 00:20:15,410
she will provide all the details about

00:20:11,900 --> 00:20:17,510
it in a few time well so this notebook

00:20:15,410 --> 00:20:21,170
as usual is including a lot of

00:20:17,510 --> 00:20:23,750
dependencies to do to show you what it

00:20:21,170 --> 00:20:26,900
looks like so you can go to the end no I

00:20:23,750 --> 00:20:29,440
didn't let move metadata and there you

00:20:26,900 --> 00:20:32,840
will see that we have imported and e4j

00:20:29,440 --> 00:20:34,970
here deep learning libraries including

00:20:32,840 --> 00:20:38,410
some native staffs in order to be able

00:20:34,970 --> 00:20:43,420
to access the underlying architecture

00:20:38,410 --> 00:20:47,960
and also we have including a Spellbinder

00:20:43,420 --> 00:20:49,880
well so and then the notebook will take

00:20:47,960 --> 00:20:51,740
care of downloading all the holiday

00:20:49,880 --> 00:20:55,130
perón seized from internet and and so on

00:20:51,740 --> 00:20:57,500
so you don't have to care about it so we

00:20:55,130 --> 00:20:59,450
have the dependencies so we have we can

00:20:57,500 --> 00:21:01,190
just you know paste it in up in a Cell

00:20:59,450 --> 00:21:03,140
executed and then they will be imported

00:21:01,190 --> 00:21:05,360
we understand there is a repeal that

00:21:03,140 --> 00:21:07,870
takes care of the imports and that will

00:21:05,360 --> 00:21:10,370
track the evolution of the notebook so

00:21:07,870 --> 00:21:13,280
each cell is actually a line which is

00:21:10,370 --> 00:21:15,470
sent to the server and executes it so

00:21:13,280 --> 00:21:17,750
the first thing that we'll do is that so

00:21:15,470 --> 00:21:23,330
we don't load this this file right and

00:21:17,750 --> 00:21:26,780
this file is 1.5 gig so it's huge

00:21:23,330 --> 00:21:30,440
right and it's huge that in also is

00:21:26,780 --> 00:21:32,000
gonna be local of course I'm not going

00:21:30,440 --> 00:21:33,710
to run the notebook now because it's

00:21:32,000 --> 00:21:35,870
gonna take some time so I have already

00:21:33,710 --> 00:21:37,700
prepared it but the thing is that this

00:21:35,870 --> 00:21:41,059
this information is very interesting

00:21:37,700 --> 00:21:42,770
because this will convert the data set

00:21:41,059 --> 00:21:47,120
that we are analyzing the right space

00:21:42,770 --> 00:21:49,190
and and hence it's gonna be very

00:21:47,120 --> 00:21:52,130
efficient and yeah right so we don't

00:21:49,190 --> 00:21:55,010
download it then we can extract it and

00:21:52,130 --> 00:21:58,760
at some point we are loading it so there

00:21:55,010 --> 00:22:01,520
is a very helpful loader in DL 4j named

00:21:58,760 --> 00:22:04,309
to work vectors eliezer and then you can

00:22:01,520 --> 00:22:06,020
load directly the binary because

00:22:04,309 --> 00:22:08,120
actually Google is providing a binary

00:22:06,020 --> 00:22:11,270
which has a certain shape and this

00:22:08,120 --> 00:22:13,929
loader takes care of the of the of the

00:22:11,270 --> 00:22:16,169
loading efficiently so you just have to

00:22:13,929 --> 00:22:18,460
provide the path to it and that's it

00:22:16,169 --> 00:22:23,559
okay I skip that because it was a

00:22:18,460 --> 00:22:26,940
previous try so now that we have the

00:22:23,559 --> 00:22:28,119
Google model in memory which takes 3.5

00:22:26,940 --> 00:22:31,570
gigabyte

00:22:28,119 --> 00:22:34,119
in memory right because it expanded so

00:22:31,570 --> 00:22:38,590
you have so many words and each word has

00:22:34,119 --> 00:22:39,789
300 dimensions right so it's a 300

00:22:38,590 --> 00:22:41,769
dimension space

00:22:39,789 --> 00:22:43,539
you know where each word is B can be

00:22:41,769 --> 00:22:46,419
placed right it's very very large space

00:22:43,539 --> 00:22:48,369
of course right that means that it's

00:22:46,419 --> 00:22:50,740
very important to have a lot of words in

00:22:48,369 --> 00:22:52,299
your corpus in order to fill the space

00:22:50,740 --> 00:22:54,009
correctly as well otherwise you might

00:22:52,299 --> 00:22:55,809
have a point that will land somewhere

00:22:54,009 --> 00:22:58,299
where there is no much information and

00:22:55,809 --> 00:23:01,690
then the resolution is immediate

00:22:58,299 --> 00:23:04,509
accuracy will be very might be poor at

00:23:01,690 --> 00:23:07,720
that time right so you you want to

00:23:04,509 --> 00:23:12,909
parameterize the the model so has many

00:23:07,720 --> 00:23:16,769
sets is going to be a many to one and to

00:23:12,909 --> 00:23:19,389
do that so we'll try to do it into a a

00:23:16,769 --> 00:23:22,330
let's say batch manner so we'll check

00:23:19,389 --> 00:23:24,759
batches and I will join the different

00:23:22,330 --> 00:23:26,860
the algorithm on the batches and

00:23:24,759 --> 00:23:29,740
aggregate at the end so the batches is

00:23:26,860 --> 00:23:31,840
50 the vector size as I said is 300

00:23:29,740 --> 00:23:34,919
because we have 200 dimensions for each

00:23:31,840 --> 00:23:39,039
word the number of ff-fuck during the

00:23:34,919 --> 00:23:41,820
four for the batches is a 5 and then we

00:23:39,039 --> 00:23:44,350
have something that allows us to to plot

00:23:41,820 --> 00:23:46,929
sorry the functions that we are going to

00:23:44,350 --> 00:23:54,789
use which will be sub sighing soft

00:23:46,929 --> 00:23:56,710
science of Max right now LSD M which is

00:23:54,789 --> 00:23:59,259
the purpose of this side of this token

00:23:56,710 --> 00:24:02,529
support how do you want how would you

00:23:59,259 --> 00:24:04,240
want I'm sorry how would you create such

00:24:02,529 --> 00:24:06,879
layers so now we're talking about layer

00:24:04,240 --> 00:24:11,289
now right so you have this graves Alice

00:24:06,879 --> 00:24:14,230
TM which is a variation of the STM build

00:24:11,289 --> 00:24:18,549
earlier and you can provide the vector

00:24:14,230 --> 00:24:20,740
size which is 300 so we take 200 word

00:24:18,549 --> 00:24:23,259
that have been converted in 200 space

00:24:20,740 --> 00:24:26,510
and then we will change it into 200

00:24:23,259 --> 00:24:34,580
units which will be the

00:24:26,510 --> 00:24:36,980
the number of of time step right and

00:24:34,580 --> 00:24:40,160
from dim type steps so we'll learn you

00:24:36,980 --> 00:24:40,490
know how these steps are linked to each

00:24:40,160 --> 00:24:42,350
other

00:24:40,490 --> 00:24:45,440
using the regular Network and then we

00:24:42,350 --> 00:24:49,700
will output the EDD answer which will be

00:24:45,440 --> 00:24:52,580
either positive or negative of course we

00:24:49,700 --> 00:24:54,200
have a trained data set to do that we'll

00:24:52,580 --> 00:24:57,170
use the max of max for the activation

00:24:54,200 --> 00:24:59,750
and the loss function is the max extent

00:24:57,170 --> 00:25:01,820
which is actually an exclamation of the

00:24:59,750 --> 00:25:07,460
distribution which is very efficient to

00:25:01,820 --> 00:25:09,770
to to to segregate where the derives now

00:25:07,460 --> 00:25:13,250
that we have built the layer the output

00:25:09,770 --> 00:25:15,500
layer using an RNN then we can finally

00:25:13,250 --> 00:25:17,390
build the whole thing so you can use the

00:25:15,500 --> 00:25:17,870
neuron network configuration builder to

00:25:17,390 --> 00:25:20,270
do that

00:25:17,870 --> 00:25:22,790
and we pass several things which will be

00:25:20,270 --> 00:25:26,510
important to to the optimization phase

00:25:22,790 --> 00:25:28,130
first we will use as GG right so because

00:25:26,510 --> 00:25:29,720
we are doing batches then we will use

00:25:28,130 --> 00:25:33,740
this GG in order to optimize our

00:25:29,720 --> 00:25:35,750
algorithm we don't even operation do of

00:25:33,740 --> 00:25:40,250
we will use the proper creation using

00:25:35,750 --> 00:25:45,230
the RMS the realization is using a magic

00:25:40,250 --> 00:25:47,030
number using the l2 which produce usual

00:25:45,230 --> 00:25:49,670
so we might have used the elastic map

00:25:47,030 --> 00:25:51,440
maybe as well there are no we might have

00:25:49,670 --> 00:25:52,550
used the elastic nut as well maybe there

00:25:51,440 --> 00:25:55,790
because there are a lot of dimensions

00:25:52,550 --> 00:25:58,810
anyway the waiting is a via is just a

00:25:55,790 --> 00:26:02,090
good way to initialize the the primer

00:25:58,810 --> 00:26:06,230
weight vector and then finally we'll

00:26:02,090 --> 00:26:09,350
have a I can keep that burning word rate

00:26:06,230 --> 00:26:11,630
is one of the aisle sorry

00:26:09,350 --> 00:26:15,140
the I per primary again it's a magic

00:26:11,630 --> 00:26:17,480
number that works pretty well yeah there

00:26:15,140 --> 00:26:19,190
are the magics there who said that

00:26:17,480 --> 00:26:21,530
machine learning isn't magic just some

00:26:19,190 --> 00:26:24,380
part is magic not really actually there

00:26:21,530 --> 00:26:26,180
are a lot of work that has been done

00:26:24,380 --> 00:26:28,730
before to find this number of course

00:26:26,180 --> 00:26:30,470
then we create the layers and then we

00:26:28,730 --> 00:26:32,300
put them into the model which so we have

00:26:30,470 --> 00:26:34,550
two layers there one layer with on the

00:26:32,300 --> 00:26:36,800
output layer and then finally we said

00:26:34,550 --> 00:26:38,660
that we want to project pepper propagate

00:26:36,800 --> 00:26:39,860
the error of course and then we called

00:26:38,660 --> 00:26:42,470
in order to to

00:26:39,860 --> 00:26:44,899
to add the model then we have this fancy

00:26:42,470 --> 00:26:47,000
JSON representation of the multi-layer

00:26:44,899 --> 00:26:52,429
configuration that we can store if we

00:26:47,000 --> 00:26:54,799
like reload and so on now that we have

00:26:52,429 --> 00:26:58,039
the configuration of course we can build

00:26:54,799 --> 00:26:59,960
la a network onto it so we call the

00:26:58,039 --> 00:27:03,019
multi-layer network builder we

00:26:59,960 --> 00:27:04,880
initialize it we have the model that we

00:27:03,019 --> 00:27:06,769
can now start training but the way we

00:27:04,880 --> 00:27:09,440
want to train it is using is part right

00:27:06,769 --> 00:27:12,500
so we want to leverage the fact that the

00:27:09,440 --> 00:27:14,570
data can be distributed okay to do that

00:27:12,500 --> 00:27:16,519
it's pretty easy actually you can just

00:27:14,570 --> 00:27:20,480
use this party r4j multi-layer

00:27:16,519 --> 00:27:22,399
implementation and you just has to get

00:27:20,480 --> 00:27:25,820
some configuration regarding the way

00:27:22,399 --> 00:27:28,429
you're going to aggregate information in

00:27:25,820 --> 00:27:29,779
distributed manner of course because all

00:27:28,429 --> 00:27:32,299
machine learning is done like that right

00:27:29,779 --> 00:27:33,860
now so you distribute you train locally

00:27:32,299 --> 00:27:36,889
on each partition and then you aggregate

00:27:33,860 --> 00:27:40,100
in each let's say I per step so

00:27:36,889 --> 00:27:42,500
degradation is using average in this

00:27:40,100 --> 00:27:46,549
case pretty common in this case and

00:27:42,500 --> 00:27:48,710
finally we initialize the spider

00:27:46,549 --> 00:27:51,830
fragility that you're passing to spark

00:27:48,710 --> 00:27:55,190
context so you can access the data using

00:27:51,830 --> 00:27:59,500
the right configuration and the net well

00:27:55,190 --> 00:28:02,269
this this is now the data preparation

00:27:59,500 --> 00:28:06,200
where we are getting the information for

00:28:02,269 --> 00:28:09,380
m DB and the m DB data is organized in

00:28:06,200 --> 00:28:11,720
this folder so we have a train and a

00:28:09,380 --> 00:28:14,480
test folder and in this train and test

00:28:11,720 --> 00:28:18,289
folder we have in each of them posts and

00:28:14,480 --> 00:28:20,690
neg folders which contain a bunch of

00:28:18,289 --> 00:28:24,080
files right so for each review we'll

00:28:20,690 --> 00:28:27,950
have a file it's a very bad way to to

00:28:24,080 --> 00:28:31,370
create data set but then we have 12,500

00:28:27,950 --> 00:28:35,149
files with one review single line

00:28:31,370 --> 00:28:37,820
generally so that means that for

00:28:35,149 --> 00:28:39,850
distributed is going to be tough but we

00:28:37,820 --> 00:28:41,870
there are manner they have you know the

00:28:39,850 --> 00:28:45,049
solutions to regular getting everything

00:28:41,870 --> 00:28:46,610
and then distribute the data so I'm

00:28:45,049 --> 00:28:47,750
explaining everything there so you can

00:28:46,610 --> 00:28:49,730
go through the notebook when we

00:28:47,750 --> 00:28:52,010
distribute it and then you can check out

00:28:49,730 --> 00:28:53,330
the mathematical reason and how to do it

00:28:52,010 --> 00:28:55,159
and so on so forth

00:28:53,330 --> 00:28:57,080
but the main idea is that at some point

00:28:55,159 --> 00:29:00,830
we take this information which will be

00:28:57,080 --> 00:29:03,950
converted into 10 times 300 time Android

00:29:00,830 --> 00:29:07,639
so we will have data set per batch 10

00:29:03,950 --> 00:29:10,580
reviews right each word is converted

00:29:07,639 --> 00:29:13,519
into the 300 space so each word is 200

00:29:10,580 --> 00:29:15,889
so 10 300 and then 100 is the number of

00:29:13,519 --> 00:29:19,009
words that we will keep in each review

00:29:15,889 --> 00:29:21,169
right number of word that we will keep

00:29:19,009 --> 00:29:23,720
that is also in the world to vac model

00:29:21,169 --> 00:29:27,529
of course because the world to vac is a

00:29:23,720 --> 00:29:29,690
gigantic map assigning a word to a to a

00:29:27,529 --> 00:29:32,119
space that means that we will take only

00:29:29,690 --> 00:29:36,080
the word which were already present into

00:29:32,119 --> 00:29:38,029
the Google News data set the Google use

00:29:36,080 --> 00:29:40,279
for that stuff and at the end for each

00:29:38,029 --> 00:29:46,419
data set which is packing the each batch

00:29:40,279 --> 00:29:50,059
we will return the value for each libels

00:29:46,419 --> 00:29:52,389
then you will have a ten two times two

00:29:50,059 --> 00:29:56,929
times 100

00:29:52,389 --> 00:29:59,779
sorry to do that easy we have the

00:29:56,929 --> 00:30:01,460
feature so we have this word two Veck we

00:29:59,779 --> 00:30:04,009
get the word factor matrix from the

00:30:01,460 --> 00:30:06,019
token which is a word and then we get a

00:30:04,009 --> 00:30:10,539
vector which is the 300 thing and we

00:30:06,019 --> 00:30:14,179
convert it into an array using the nd 4j

00:30:10,539 --> 00:30:16,220
library this is still using the ante for

00:30:14,179 --> 00:30:19,789
J but I have to mention that there is

00:30:16,220 --> 00:30:22,970
also an g4s layer on top on D for J

00:30:19,789 --> 00:30:26,799
which is the portage of the Java library

00:30:22,970 --> 00:30:29,090
to Scala which a more a dramatic Scala

00:30:26,799 --> 00:30:31,009
library which is really cool because

00:30:29,090 --> 00:30:33,259
then you can leverage the the

00:30:31,009 --> 00:30:37,669
architecture of your computation your

00:30:33,259 --> 00:30:41,090
machine to do some line in algebra in

00:30:37,669 --> 00:30:43,399
Scala worth checking to be honest okay

00:30:41,090 --> 00:30:46,639
so then we we will denote this data set

00:30:43,399 --> 00:30:48,799
we expand it we load it this is just

00:30:46,639 --> 00:30:52,639
showing the file size is pretty long so

00:30:48,799 --> 00:30:56,019
we have one fiber information for per

00:30:52,639 --> 00:30:59,779
review as I said and this is the first

00:30:56,019 --> 00:31:03,649
review so about this movie that honestly

00:30:59,779 --> 00:31:05,740
I don't know who knows this movie at the

00:31:03,649 --> 00:31:08,380
third time no one

00:31:05,740 --> 00:31:11,620
yeah maybe doesn't exist I didn't check

00:31:08,380 --> 00:31:14,679
but maybe I should check anyway actually

00:31:11,620 --> 00:31:16,779
this this woman who is a teacher said

00:31:14,679 --> 00:31:21,250
that she's very happy about this movie

00:31:16,779 --> 00:31:25,299
which is apparently respecting how you

00:31:21,250 --> 00:31:28,000
know college is working or is working in

00:31:25,299 --> 00:31:32,320
the u.s. maybe maybe she created the

00:31:28,000 --> 00:31:34,029
movie I don't know yeah but then we have

00:31:32,320 --> 00:31:35,710
to go through these files and then we

00:31:34,029 --> 00:31:37,690
have to corrugate things and so on so I

00:31:35,710 --> 00:31:40,960
will skip the details this is done into

00:31:37,690 --> 00:31:43,120
this sentiment example iterator and then

00:31:40,960 --> 00:31:45,789
there is something bad I'm sorry about

00:31:43,120 --> 00:31:47,679
it but we are working on that so we are

00:31:45,789 --> 00:31:49,929
doing something on this iterator which

00:31:47,679 --> 00:31:52,870
is iterating over the files and packing

00:31:49,929 --> 00:31:54,700
them into ten examples what we do at

00:31:52,870 --> 00:31:56,830
some point is that we go over the whole

00:31:54,700 --> 00:32:00,580
dataset normally and we load it into an

00:31:56,830 --> 00:32:03,340
iterator which is the buffer and we

00:32:00,580 --> 00:32:07,000
Cyril eyes it so there are two reasons

00:32:03,340 --> 00:32:12,580
why it's done like that the first reason

00:32:07,000 --> 00:32:15,340
is that I mean having one file on the

00:32:12,580 --> 00:32:17,230
cluster you know if you have HDFS that

00:32:15,340 --> 00:32:19,809
means that you will have to reserve a

00:32:17,230 --> 00:32:22,570
block for each file that means it's very

00:32:19,809 --> 00:32:25,210
bad so you lose a lot of space on your

00:32:22,570 --> 00:32:26,950
disk for just one file this is the first

00:32:25,210 --> 00:32:30,640
reason but the most important reason is

00:32:26,950 --> 00:32:32,980
that if you want to convert your reviews

00:32:30,640 --> 00:32:34,750
you know in a distributed manner if you

00:32:32,980 --> 00:32:36,970
have the file distributed that means

00:32:34,750 --> 00:32:39,460
that you have to have your work to vac

00:32:36,970 --> 00:32:44,080
vector present in each machine in the

00:32:39,460 --> 00:32:49,149
cluster right but a word to vac vector

00:32:44,080 --> 00:32:50,799
is 3.5 gigs in memory and that means

00:32:49,149 --> 00:32:53,830
that it will take quite some memory

00:32:50,799 --> 00:32:55,779
image on each executor and for sure now

00:32:53,830 --> 00:32:58,090
we are thinking about how to distribute

00:32:55,779 --> 00:32:59,740
it from the driver on the cluster using

00:32:58,090 --> 00:33:02,590
broadcasts or something like that but it

00:32:59,740 --> 00:33:04,419
might take some time or squeezing a

00:33:02,590 --> 00:33:06,640
little bit of spark appear in order to

00:33:04,419 --> 00:33:09,340
allow this model to be loaded

00:33:06,640 --> 00:33:11,110
distributed manner so this is something

00:33:09,340 --> 00:33:14,380
on which we are working on because this

00:33:11,110 --> 00:33:15,389
is a limitation but there are many ways

00:33:14,380 --> 00:33:19,259
to work

00:33:15,389 --> 00:33:21,029
on it so here the way to do it at least

00:33:19,259 --> 00:33:23,399
to have a bunch of information

00:33:21,029 --> 00:33:25,139
distributed that we take a bunch of them

00:33:23,399 --> 00:33:28,169
and we distributed using the power as

00:33:25,139 --> 00:33:30,809
mentioned on the on the spot context now

00:33:28,169 --> 00:33:34,499
now we have the data set disputed into

00:33:30,809 --> 00:33:36,419
the world to vex space so how do we try

00:33:34,499 --> 00:33:39,359
the model right now so in a disabled man

00:33:36,419 --> 00:33:41,519
run now it's pretty easy so we take the

00:33:39,359 --> 00:33:44,940
spy network with fit data set on to

00:33:41,519 --> 00:33:47,039
dirtylee and that's it it's pretty usual

00:33:44,940 --> 00:33:50,849
in machine learning is that the API Geno

00:33:47,039 --> 00:33:54,269
is is simple when we come part the

00:33:50,849 --> 00:33:56,369
mathematical theory call you know a

00:33:54,269 --> 00:33:59,700
concept behind it's generally the API is

00:33:56,369 --> 00:34:01,619
okay you configure you pass the right I

00:33:59,700 --> 00:34:03,839
prepare a mirror and you train and it's

00:34:01,619 --> 00:34:05,669
done but of course we have to understand

00:34:03,839 --> 00:34:07,829
what's going on behind order to you know

00:34:05,669 --> 00:34:09,179
to interpret correctly what are the

00:34:07,829 --> 00:34:11,700
results and how to configure it

00:34:09,179 --> 00:34:14,099
correctly by the way so you fit the data

00:34:11,700 --> 00:34:18,269
set and you get back what a trained

00:34:14,099 --> 00:34:20,460
network something we can save on you

00:34:18,269 --> 00:34:23,460
know on parque whatever like it's done

00:34:20,460 --> 00:34:26,009
in inspark already and when it is paid

00:34:23,460 --> 00:34:27,869
is saved in in parque you can load it in

00:34:26,009 --> 00:34:30,659
a stream or whatever so in the stream

00:34:27,869 --> 00:34:33,450
you can predict reviews using the

00:34:30,659 --> 00:34:35,579
predict method here I'm just showing as

00:34:33,450 --> 00:34:37,409
a quick example so we do the same thing

00:34:35,579 --> 00:34:39,359
like for the trend data we take all the

00:34:37,409 --> 00:34:43,950
files we are roundish dive into into

00:34:39,359 --> 00:34:46,649
data set ten by ten we serve realize it

00:34:43,950 --> 00:34:50,909
and then we can use the product function

00:34:46,649 --> 00:34:54,509
that will return the predictions which

00:34:50,909 --> 00:34:56,460
will be essentially for each entry in

00:34:54,509 --> 00:35:00,420
the data set if it's a positive or

00:34:56,460 --> 00:35:03,539
negative we can also have access to the

00:35:00,420 --> 00:35:07,200
scores if we want to check a little bit

00:35:03,539 --> 00:35:09,599
in more detail what's what went yeah

00:35:07,200 --> 00:35:12,569
what word you know the initial score on

00:35:09,599 --> 00:35:16,559
to which we have decided if it's a

00:35:12,569 --> 00:35:18,450
positive or negative as I said so there

00:35:16,559 --> 00:35:20,279
are a few limitations and next steps so

00:35:18,450 --> 00:35:23,759
this is one what we are working right

00:35:20,279 --> 00:35:27,660
now before distributing it we have to

00:35:23,759 --> 00:35:28,770
distribute the data initially paralyzing

00:35:27,660 --> 00:35:31,290
the projection on

00:35:28,770 --> 00:35:34,860
the walk to vex pace as Isis is really

00:35:31,290 --> 00:35:37,260
pretty hard and and so this is done

00:35:34,860 --> 00:35:41,210
actually now the predict has a few

00:35:37,260 --> 00:35:50,550
problems on this oh man this is no done

00:35:41,210 --> 00:35:53,100
so wait okay so we are now able to be

00:35:50,550 --> 00:35:55,170
able to you know to do some prediction

00:35:53,100 --> 00:35:56,730
of the relational asset like AMD be in a

00:35:55,170 --> 00:35:58,740
swimming manner and so on and actually

00:35:56,730 --> 00:36:00,330
the code is pretty simple of course

00:35:58,740 --> 00:36:02,270
there are a few limitations because we

00:36:00,330 --> 00:36:06,360
are working in in distributed systems

00:36:02,270 --> 00:36:08,220
but that means also that we have a lot

00:36:06,360 --> 00:36:10,430
of work as a data engineer and data

00:36:08,220 --> 00:36:15,690
scientist in order to have our

00:36:10,430 --> 00:36:17,430
environment ready for for the challenges

00:36:15,690 --> 00:36:21,060
that we will have to face for the coming

00:36:17,430 --> 00:36:22,530
years but I mean yeah we would be bored

00:36:21,060 --> 00:36:27,750
of course if everything we're already

00:36:22,530 --> 00:36:29,460
done right by the way this is the the

00:36:27,750 --> 00:36:33,390
get up folder that I've mentioned

00:36:29,460 --> 00:36:35,310
initially so this is California science

00:36:33,390 --> 00:36:39,860
and there are a few things there that

00:36:35,310 --> 00:36:44,790
you you might want to check that mainly

00:36:39,860 --> 00:36:46,620
in this branch that will merge soon

00:36:44,790 --> 00:36:49,020
there are three notebooks one is

00:36:46,620 --> 00:36:51,540
explaining why SCADA for data science

00:36:49,020 --> 00:36:54,090
why using the spine notebook as a tool

00:36:51,540 --> 00:36:55,980
for data science and what are the

00:36:54,090 --> 00:36:57,900
different libraries that you might want

00:36:55,980 --> 00:37:01,470
to check for doing data science like

00:36:57,900 --> 00:37:02,970
therefore J smile smile I recommend it

00:37:01,470 --> 00:37:05,280
especially if you want to do some

00:37:02,970 --> 00:37:08,760
training or learn machine learning

00:37:05,280 --> 00:37:10,560
because it has 98 models and our core is

00:37:08,760 --> 00:37:13,230
implemented there although they are

00:37:10,560 --> 00:37:16,680
local and and all the libraries that

00:37:13,230 --> 00:37:24,960
have integrated it there and there is by

00:37:16,680 --> 00:37:28,290
the way a docker for it so this docker

00:37:24,960 --> 00:37:31,680
is actually so if you check the tags

00:37:28,290 --> 00:37:35,490
it's prepared with Cassandra SPARC spy

00:37:31,680 --> 00:37:37,170
notebook and SPARC 2.0 so it's already

00:37:35,490 --> 00:37:40,710
prepared for spectral zero so you can

00:37:37,170 --> 00:37:44,849
already use it for for this

00:37:40,710 --> 00:37:48,240
things I'm done with that so there is a

00:37:44,849 --> 00:37:50,160
reference I will leave it to you this

00:37:48,240 --> 00:37:53,070
last part here this is some references

00:37:50,160 --> 00:37:55,140
that I've listed to give you guys one

00:37:53,070 --> 00:37:57,619
kind of a resource where you can go look

00:37:55,140 --> 00:38:00,359
for additional information on RN ins

00:37:57,619 --> 00:38:02,490
also some of the repos that we've

00:38:00,359 --> 00:38:03,570
mentioned and if you're taking photos

00:38:02,490 --> 00:38:07,380
just to let you know if you're on

00:38:03,570 --> 00:38:10,109
Twitter at night owl is where this this

00:38:07,380 --> 00:38:13,830
deck is currently posted the image

00:38:10,109 --> 00:38:16,890
references so I'm at night now Andy's at

00:38:13,830 --> 00:38:20,280
newts sub then my favorite newts sub

00:38:16,890 --> 00:38:21,450
yeah and that's pretty much what we

00:38:20,280 --> 00:38:23,250
wanted to share with you guys today so

00:38:21,450 --> 00:38:26,540
thank you very much for listening and it

00:38:23,250 --> 00:38:26,540
will take questions if you have any

00:38:32,580 --> 00:38:46,840
yes is one so do you want machine

00:38:45,160 --> 00:38:49,630
learning the optimization or the

00:38:46,840 --> 00:38:51,220
monitoring question so there is one

00:38:49,630 --> 00:38:58,290
question behind it we have to pick one

00:38:51,220 --> 00:39:02,290
yes whichever one you want ask go for it

00:38:58,290 --> 00:39:06,010
okay so I take the monitoring one yeah

00:39:02,290 --> 00:39:08,950
when I train models I usually need some

00:39:06,010 --> 00:39:12,130
hours for one epoch of training and so

00:39:08,950 --> 00:39:13,660
monitoring comes apart entire to see the

00:39:12,130 --> 00:39:16,030
learning curve the training error the

00:39:13,660 --> 00:39:19,090
validation errors is that supported by

00:39:16,030 --> 00:39:22,120
your solution yes you can monitor and

00:39:19,090 --> 00:39:23,800
see how the score is improving or not

00:39:22,120 --> 00:39:26,560
improving and there's some additional

00:39:23,800 --> 00:39:28,600
kind of monitoring tools it's set

00:39:26,560 --> 00:39:30,430
iteration listeners is what you would do

00:39:28,600 --> 00:39:31,900
on the model and there's some examples

00:39:30,430 --> 00:39:34,000
we have in the examples to repo that

00:39:31,900 --> 00:39:36,340
I've got in that github repo did use

00:39:34,000 --> 00:39:38,260
this court oration that is nice yeah you

00:39:36,340 --> 00:39:39,910
saw you basically will say when you

00:39:38,260 --> 00:39:41,920
create your network then you'll you'll

00:39:39,910 --> 00:39:43,180
say Network dot set iteration listener

00:39:41,920 --> 00:39:45,220
and there's couple listeners you can

00:39:43,180 --> 00:39:46,480
pass in like there's histograms in

00:39:45,220 --> 00:39:48,910
particular that'll show you that kind of

00:39:46,480 --> 00:39:50,200
information histograms that'll show you

00:39:48,910 --> 00:39:51,780
sort of how are you doing with your

00:39:50,200 --> 00:40:00,040
scores how are you doing with the

00:39:51,780 --> 00:40:02,800
gradients any other questions you can

00:40:00,040 --> 00:40:04,840
ask your other question go for it

00:40:02,800 --> 00:40:07,210
okay let's like the machine learning one

00:40:04,840 --> 00:40:10,330
you mentioned that you were using

00:40:07,210 --> 00:40:12,460
elastic net for regularization which is

00:40:10,330 --> 00:40:15,910
really weird because why would you use

00:40:12,460 --> 00:40:18,880
force paucity on because there are a lot

00:40:15,910 --> 00:40:21,400
of variables you might want maybe to get

00:40:18,880 --> 00:40:23,050
rid of someone know was thinking about

00:40:21,400 --> 00:40:28,020
it isn't you should definitely use

00:40:23,050 --> 00:40:30,550
dropout yeah it works pretty well okay

00:40:28,020 --> 00:40:32,710
well let me just say because I know

00:40:30,550 --> 00:40:35,200
you're asking about sort of in essence

00:40:32,710 --> 00:40:37,420
you're talking around regularization and

00:40:35,200 --> 00:40:40,539
something to note that I didn't get into

00:40:37,420 --> 00:40:42,130
too much with the model but one of the

00:40:40,539 --> 00:40:43,989
challenges with neural nets is that they

00:40:42,130 --> 00:40:45,579
can be over fit easily meaning that

00:40:43,989 --> 00:40:47,859
they're too perfectly

00:40:45,579 --> 00:40:49,599
associated to the training data and they

00:40:47,859 --> 00:40:51,459
don't generalize so if you send it

00:40:49,599 --> 00:40:53,950
something new it's not going to give you

00:40:51,459 --> 00:40:57,880
very good results on that new item so

00:40:53,950 --> 00:41:01,209
you use things like dropout an l2 which

00:40:57,880 --> 00:41:04,299
Andy had mentioned to regularize meaning

00:41:01,209 --> 00:41:07,329
give it still some level of you know not

00:41:04,299 --> 00:41:09,369
you know haziness some level of it's not

00:41:07,329 --> 00:41:11,529
perfectly matched to the data that it

00:41:09,369 --> 00:41:13,359
saw so that it can better generalize to

00:41:11,529 --> 00:41:15,400
other problems and then you had a third

00:41:13,359 --> 00:41:17,799
question oh you really want it because

00:41:15,400 --> 00:41:20,709
it's the hardest one bring it so it has

00:41:17,799 --> 00:41:22,479
to do with optimization if I understood

00:41:20,709 --> 00:41:24,430
it right you sent the training data to

00:41:22,479 --> 00:41:26,859
all the nodes and then you run

00:41:24,430 --> 00:41:28,660
mini-batch so has to create him to send

00:41:26,859 --> 00:41:30,309
on that node that's not the best

00:41:28,660 --> 00:41:32,259
optimization and you probably know that

00:41:30,309 --> 00:41:33,940
but the question I'm getting invited to

00:41:32,259 --> 00:41:36,309
me yeah yeah I mean I'm the distributed

00:41:33,940 --> 00:41:38,190
so I'm running it on single GPUs because

00:41:36,309 --> 00:41:41,469
I have made really bad experience with

00:41:38,190 --> 00:41:44,949
multi chip use our multi-gpu computer

00:41:41,469 --> 00:41:46,989
died rep recipes and distributed was

00:41:44,949 --> 00:41:50,289
always slower so we have 10 nodes with

00:41:46,989 --> 00:41:52,630
one thick GPU and was always really slow

00:41:50,289 --> 00:41:56,619
up to I think a few nodes so I rather

00:41:52,630 --> 00:41:59,680
run 10 experiments concurrently on 10

00:41:56,619 --> 00:42:01,329
single nodes then distributed so but if

00:41:59,680 --> 00:42:03,809
you send all the data to the notes you

00:42:01,329 --> 00:42:06,849
don't really have a distribution right

00:42:03,809 --> 00:42:09,069
wait okay so you're talking about the

00:42:06,849 --> 00:42:12,099
optimization in terms of distributing it

00:42:09,069 --> 00:42:13,989
across multiple GPUs or multiple CPUs

00:42:12,099 --> 00:42:16,150
and if you know you don't know GPUs are

00:42:13,989 --> 00:42:17,859
a different type of chip that's used for

00:42:16,150 --> 00:42:21,640
parallel processing and to speed up the

00:42:17,859 --> 00:42:23,890
training process so you're speaking

00:42:21,640 --> 00:42:25,269
specifically to a technique that's

00:42:23,890 --> 00:42:28,150
applied as distributing it across

00:42:25,269 --> 00:42:30,729
multiple GPUs and the typical approach

00:42:28,150 --> 00:42:33,400
is that you will send the data to all of

00:42:30,729 --> 00:42:35,319
the the GPUs and you'll or no you'll

00:42:33,400 --> 00:42:36,819
split out the data across different GPUs

00:42:35,319 --> 00:42:38,410
and you'll have the model on all of the

00:42:36,819 --> 00:42:39,880
GPUs it's a couple of different ways you

00:42:38,410 --> 00:42:41,979
can do this but the model would go on

00:42:39,880 --> 00:42:44,229
every single one and the data would just

00:42:41,979 --> 00:42:45,559
be split out depends on how much data

00:42:44,229 --> 00:42:47,900
you have

00:42:45,559 --> 00:42:50,510
and how large that data is that you

00:42:47,900 --> 00:42:52,849
would want to use and distribute across

00:42:50,510 --> 00:42:55,220
multiple GPUs it has been shown to to

00:42:52,849 --> 00:42:57,440
actually speed up performance some of

00:42:55,220 --> 00:42:59,569
the I know the larger guys out there

00:42:57,440 --> 00:43:01,490
like Google and Baidu and so forth are

00:42:59,569 --> 00:43:06,319
experimenting and using that type of

00:43:01,490 --> 00:43:08,480
solution so sure if you can feed your

00:43:06,319 --> 00:43:10,460
data all through one system typically

00:43:08,480 --> 00:43:12,680
that's a more solid solution to go with

00:43:10,460 --> 00:43:14,630
to start out with but if you find that

00:43:12,680 --> 00:43:18,079
you want to bring down the number of

00:43:14,630 --> 00:43:19,490
hours days months whatever it is because

00:43:18,079 --> 00:43:21,650
you have so much data that you're trying

00:43:19,490 --> 00:43:23,480
to process that's when you start to

00:43:21,650 --> 00:43:25,220
explore splitting out across multiple

00:43:23,480 --> 00:43:27,650
systems and splitting the data out

00:43:25,220 --> 00:43:29,329
across multiple systems yeah and there

00:43:27,650 --> 00:43:33,619
are other problems with SGD in this

00:43:29,329 --> 00:43:36,170
video the partition sometimes are skewed

00:43:33,619 --> 00:43:38,180
or very often that means that when you

00:43:36,170 --> 00:43:40,520
turn the model that the model is very

00:43:38,180 --> 00:43:42,589
different on each machine because the

00:43:40,520 --> 00:43:45,109
data it holds is very different you know

00:43:42,589 --> 00:43:46,970
that means you might want to first

00:43:45,109 --> 00:43:49,670
shuffle the whole data before doing that

00:43:46,970 --> 00:43:52,280
or you might check as well to splash

00:43:49,670 --> 00:43:54,799
library at the splash library is doing

00:43:52,280 --> 00:43:57,290
bootstrapping on each machine that means

00:43:54,799 --> 00:43:59,450
that it's you know a bit stressing the

00:43:57,290 --> 00:44:02,390
very variability of the data set in

00:43:59,450 --> 00:44:04,220
before before aggregating locally so

00:44:02,390 --> 00:44:05,960
there are a few techniques now and what

00:44:04,220 --> 00:44:07,339
I said we are we are in distributed

00:44:05,960 --> 00:44:10,069
environment now so there are a lot of

00:44:07,339 --> 00:44:11,660
research going on and this is one of the

00:44:10,069 --> 00:44:13,880
you know the most important one

00:44:11,660 --> 00:44:15,559
regarding SGD I would say right and

00:44:13,880 --> 00:44:18,440
actually I want to add something whether

00:44:15,559 --> 00:44:20,480
you you do distribute it or not it's

00:44:18,440 --> 00:44:22,970
pretty common nowadays with neural nets

00:44:20,480 --> 00:44:24,799
to batch the data to send it through and

00:44:22,970 --> 00:44:27,140
to do parameter averaging or gradient

00:44:24,799 --> 00:44:27,589
averaging this is actually a very common

00:44:27,140 --> 00:44:29,420
technique

00:44:27,589 --> 00:44:32,569
the researchers been showing that it's

00:44:29,420 --> 00:44:35,450
it's doing very well and so I'm not sure

00:44:32,569 --> 00:44:37,010
exactly your experience but but I know

00:44:35,450 --> 00:44:38,150
that significant amount of research has

00:44:37,010 --> 00:44:39,710
been done on this and they're showing

00:44:38,150 --> 00:44:43,160
that that's a technique that's pretty

00:44:39,710 --> 00:44:51,230
well accepted in the industry so yeah

00:44:43,160 --> 00:44:53,630
any other questions fantastic yeah all

00:44:51,230 --> 00:44:55,930
right anybody else have questions no

00:44:53,630 --> 00:44:57,990
okay well thank you very much

00:44:55,930 --> 00:44:57,990

YouTube URL: https://www.youtube.com/watch?v=vaiE2yGdJSg


