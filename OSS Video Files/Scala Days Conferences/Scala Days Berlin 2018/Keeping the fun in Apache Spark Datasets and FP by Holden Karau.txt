Title: Keeping the fun in Apache Spark Datasets and FP by Holden Karau
Publication date: 2018-09-20
Playlist: Scala Days Berlin 2018
Description: 
	This video was recorded at Scala Days Berlin 2018
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

More information and the abstract can be found here:
https://eu.scaladays.org/lect-6920-keeping-the-%22fun%22-in-apache-spark%3A-datasets-and-fp.html
Captions: 
	00:00:04,580 --> 00:00:08,210
so yeah I'm gonna be talking about spark

00:00:06,920 --> 00:00:10,490
datasets and how they relate to

00:00:08,210 --> 00:00:11,870
functional programming and this talk is

00:00:10,490 --> 00:00:15,350
probably actually going to be a little

00:00:11,870 --> 00:00:17,570
bit sad despite the title because it

00:00:15,350 --> 00:00:19,490
turns out that well we can keep the fun

00:00:17,570 --> 00:00:22,279
in spark and functional programming

00:00:19,490 --> 00:00:25,610
there's a lot of work that we maybe

00:00:22,279 --> 00:00:30,410
forgot to do that we should do to

00:00:25,610 --> 00:00:32,660
catch-up and and we'll talk about how we

00:00:30,410 --> 00:00:35,630
can do that I'll try and trick people

00:00:32,660 --> 00:00:38,539
into doing my work for me as a proud

00:00:35,630 --> 00:00:39,859
open-source maintainer but also we'll

00:00:38,539 --> 00:00:43,699
focus on like some cool things that you

00:00:39,859 --> 00:00:45,500
can do too so it'll be good yeah so yeah

00:00:43,699 --> 00:00:48,320
my pronouns are she or her I'm a

00:00:45,500 --> 00:00:51,230
developer advocate at Google you can

00:00:48,320 --> 00:00:53,000
find my slides on SlideShare my tweets

00:00:51,230 --> 00:00:56,809
according to the speaker from Twitter

00:00:53,000 --> 00:00:57,620
are not very important we were drinking

00:00:56,809 --> 00:00:59,499
last night

00:00:57,620 --> 00:01:01,280
but you can also follow me on Twitter

00:00:59,499 --> 00:01:03,980
one of the things that I've started

00:01:01,280 --> 00:01:05,930
doing is code review live streams so if

00:01:03,980 --> 00:01:08,090
anyone's interested in seeing more about

00:01:05,930 --> 00:01:12,080
how sort of the open source process

00:01:08,090 --> 00:01:14,600
works in the patchy way when you have

00:01:12,080 --> 00:01:17,390
giant projects and lots of competing

00:01:14,600 --> 00:01:21,130
commercial interests you can watch that

00:01:17,390 --> 00:01:23,630
there and it's not as sad as that sounds

00:01:21,130 --> 00:01:26,060
and if you want to give me feedback on

00:01:23,630 --> 00:01:30,500
this talk I have a little Google survey

00:01:26,060 --> 00:01:33,020
that you can fill out no pressure I I do

00:01:30,500 --> 00:01:37,130
read it so keep that in mind when you

00:01:33,020 --> 00:01:39,500
submit it so I am also trans clear

00:01:37,130 --> 00:01:40,880
Canadian I live in America on a work

00:01:39,500 --> 00:01:43,580
visa that they're debating whether or

00:01:40,880 --> 00:01:46,070
not they want to keep and part of the

00:01:43,580 --> 00:01:47,840
leather community this is not related to

00:01:46,070 --> 00:01:50,180
spark or functional programming there is

00:01:47,840 --> 00:01:53,470
no secret Canadian garbage collector

00:01:50,180 --> 00:01:56,600
that we've been hiding from you we all

00:01:53,470 --> 00:01:57,140
live with G 1 G C and are very happy

00:01:56,600 --> 00:01:59,150
with it

00:01:57,140 --> 00:02:00,140
but I think it's important to remember

00:01:59,150 --> 00:02:02,450
that we're all from different

00:02:00,140 --> 00:02:04,430
backgrounds and if we work together we

00:02:02,450 --> 00:02:05,540
can get through this a lot faster and if

00:02:04,430 --> 00:02:06,830
we spend our time fighting with each

00:02:05,540 --> 00:02:08,420
other we're just gonna burn the world

00:02:06,830 --> 00:02:11,150
down with distributed systems and

00:02:08,420 --> 00:02:12,650
sadness and so we should you know be

00:02:11,150 --> 00:02:13,989
respectful of our colleagues regardless

00:02:12,650 --> 00:02:16,969
of where they're from and

00:02:13,989 --> 00:02:20,450
life will get better um but not

00:02:16,969 --> 00:02:22,760
immediately okay so this is boo she also

00:02:20,450 --> 00:02:24,469
uses she/her pronouns she is the author

00:02:22,760 --> 00:02:27,939
of learning to bark and high-performance

00:02:24,469 --> 00:02:30,470
barking it's currently out of print

00:02:27,939 --> 00:02:32,900
there's been some supply issues because

00:02:30,470 --> 00:02:35,510
they're done in pen and she does not

00:02:32,900 --> 00:02:37,670
have opposable thumbs so we're working

00:02:35,510 --> 00:02:41,209
on that and you can definitely follow

00:02:37,670 --> 00:02:44,180
her on Twitter as well he is not a

00:02:41,209 --> 00:02:47,750
scallop programmer yet working on that

00:02:44,180 --> 00:02:49,670
so um I feel I might as well mention why

00:02:47,750 --> 00:02:53,389
my employer Claire's about SPARC since

00:02:49,670 --> 00:02:55,189
they pay me money and we have to hosted

00:02:53,389 --> 00:03:01,040
services that you can run SPARC on top

00:02:55,189 --> 00:03:04,430
of one with support and one for more

00:03:01,040 --> 00:03:06,650
adventurous people so we have cloud data

00:03:04,430 --> 00:03:08,329
proc and we have google kubernetes

00:03:06,650 --> 00:03:10,730
engine I'm pretty sure that's the name

00:03:08,329 --> 00:03:12,859
of it and you can deploy SPARC on top of

00:03:10,730 --> 00:03:14,299
that and if anyone wants to help me but

00:03:12,859 --> 00:03:16,730
isn't willing to write my software for

00:03:14,299 --> 00:03:18,489
me and you are using SPARC how many

00:03:16,730 --> 00:03:22,549
people are using spark here by the way

00:03:18,489 --> 00:03:25,579
yay if you want to keep us from breaking

00:03:22,549 --> 00:03:27,500
your pipelines you can try out the new

00:03:25,579 --> 00:03:29,449
release candidate and tell us if there's

00:03:27,500 --> 00:03:31,849
any regressions because otherwise you

00:03:29,449 --> 00:03:35,090
find out after we broke your software

00:03:31,849 --> 00:03:37,280
and if you tell us first then we might

00:03:35,090 --> 00:03:39,680
not break it we might still break it

00:03:37,280 --> 00:03:41,299
because we're lazy but there's a good

00:03:39,680 --> 00:03:44,780
chance that we'll fix whatever bug we

00:03:41,299 --> 00:03:46,400
introduce that affects you so I'm hoping

00:03:44,780 --> 00:03:47,810
you're all friendly people it does look

00:03:46,400 --> 00:03:49,699
like there are a lot of spark users in

00:03:47,810 --> 00:03:51,400
the house is there anyone new to the

00:03:49,699 --> 00:03:53,810
scala community like this is their first

00:03:51,400 --> 00:03:57,709
scala conference their first time using

00:03:53,810 --> 00:04:01,400
Scala yay welcome new friends thank you

00:03:57,709 --> 00:04:05,900
for coming to the Scala community we're

00:04:01,400 --> 00:04:07,459
relatively friendly and I'm happy to

00:04:05,900 --> 00:04:10,879
have you I'm always happy to see more

00:04:07,459 --> 00:04:11,329
people here and yeah so that's pretty

00:04:10,879 --> 00:04:13,040
cool

00:04:11,329 --> 00:04:15,409
oh right if you really don't like

00:04:13,040 --> 00:04:19,310
pictures of cats there are many other

00:04:15,409 --> 00:04:21,049
talks available to you this is not the

00:04:19,310 --> 00:04:23,660
one for you

00:04:21,049 --> 00:04:25,310
so I'm gonna talk about what spark is

00:04:23,660 --> 00:04:27,860
for the people who don't know spark just

00:04:25,310 --> 00:04:29,210
super briefly I'll mention why I think

00:04:27,860 --> 00:04:32,000
it's helped drive functional programming

00:04:29,210 --> 00:04:34,400
end of enterprise we'll talk about what

00:04:32,000 --> 00:04:36,379
sparks new api's mean how we can still

00:04:34,400 --> 00:04:38,300
do functional programming with them but

00:04:36,379 --> 00:04:40,789
how it's maybe not going to be as much

00:04:38,300 --> 00:04:43,550
of an on-ramp as we used to see which is

00:04:40,789 --> 00:04:45,050
a little sad we'll talk about some of

00:04:43,550 --> 00:04:46,849
the cool things that I can do with data

00:04:45,050 --> 00:04:49,129
sets which are really awesome that were

00:04:46,849 --> 00:04:51,379
like way too much work to do like on

00:04:49,129 --> 00:04:52,819
rdd's so if you have some problems that

00:04:51,379 --> 00:04:55,130
you've been like putting off solving

00:04:52,819 --> 00:04:57,860
because they were super gnarly this will

00:04:55,130 --> 00:04:58,789
hopefully help you and I forgot to

00:04:57,860 --> 00:05:01,099
mention we were gonna look at some

00:04:58,789 --> 00:05:03,169
machine learning in here just for good

00:05:01,099 --> 00:05:06,139
measure there's no deep learning so San

00:05:03,169 --> 00:05:07,340
Francisco people I'm sorry but I'm in

00:05:06,139 --> 00:05:10,069
Europe so I figured maybe you wanted

00:05:07,340 --> 00:05:11,360
software that worked and we'll talk

00:05:10,069 --> 00:05:13,280
about how we can make this more awesome

00:05:11,360 --> 00:05:14,479
for the people who come after us and and

00:05:13,280 --> 00:05:16,340
make sure they still learn functional

00:05:14,479 --> 00:05:19,520
programming and they aren't seduced by

00:05:16,340 --> 00:05:20,990
the dark side of sequel because I don't

00:05:19,520 --> 00:05:23,389
I want more functional programming nerds

00:05:20,990 --> 00:05:25,520
to hang out with that's the long and

00:05:23,389 --> 00:05:27,380
short of it so spark is a

00:05:25,520 --> 00:05:29,090
general-purpose distributed system for

00:05:27,380 --> 00:05:32,150
parallel data processing what's an

00:05:29,090 --> 00:05:34,280
Apache project and if it's a much faster

00:05:32,150 --> 00:05:36,620
than hadoop mapreduce who do MapReduce

00:05:34,280 --> 00:05:41,029
set the bar down at about where my heel

00:05:36,620 --> 00:05:45,169
is very nice of MapReduce a really great

00:05:41,029 --> 00:05:47,000
innovator but not fast at all so that's

00:05:45,169 --> 00:05:49,940
great I love it when I have really low

00:05:47,000 --> 00:05:51,560
bars to achieve success and it's good

00:05:49,940 --> 00:05:53,449
for when our problems get too big for a

00:05:51,560 --> 00:05:55,400
single machine and has two core

00:05:53,449 --> 00:05:58,550
abstractions one of them is really

00:05:55,400 --> 00:06:01,039
functional and the other one is a little

00:05:58,550 --> 00:06:03,770
less functional but we can we can still

00:06:01,039 --> 00:06:05,090
work with it and have fun

00:06:03,770 --> 00:06:07,460
so for the people who aren't currently

00:06:05,090 --> 00:06:10,430
using spark maybe or go run a MapReduce

00:06:07,460 --> 00:06:12,229
job or a hive job it'll take 16 hours

00:06:10,430 --> 00:06:15,130
and while it's running you can just

00:06:12,229 --> 00:06:18,500
learn spark the book is not that big and

00:06:15,130 --> 00:06:19,370
it does after all it's available by

00:06:18,500 --> 00:06:20,870
prescription here

00:06:19,370 --> 00:06:24,169
talk to your doctor tell them you want

00:06:20,870 --> 00:06:25,520
to learn a new programming tool the

00:06:24,169 --> 00:06:28,370
other reason that people come to spark

00:06:25,520 --> 00:06:30,469
and this is normally for people who are

00:06:28,370 --> 00:06:33,160
in the Python world is they have a data

00:06:30,469 --> 00:06:35,110
frame they try and load it in memory it

00:06:33,160 --> 00:06:36,700
out of memory and then they think the

00:06:35,110 --> 00:06:38,560
solution to that is distributed systems

00:06:36,700 --> 00:06:40,450
later on they are depressed but at that

00:06:38,560 --> 00:06:43,980
point we've already converted them to

00:06:40,450 --> 00:06:48,940
functional programming so it's fine and

00:06:43,980 --> 00:06:51,310
a little bit of magic yeah magic so what

00:06:48,940 --> 00:06:53,020
is the magic of spark so for me the

00:06:51,310 --> 00:06:55,060
first time I saw spark I was like oh hey

00:06:53,020 --> 00:06:56,830
this actually does the things that

00:06:55,060 --> 00:06:58,870
people told me we we could do with

00:06:56,830 --> 00:07:01,090
functional systems that I haven't seen

00:06:58,870 --> 00:07:03,190
people bothering to do right like I can

00:07:01,090 --> 00:07:05,260
take advantage of the fact that my data

00:07:03,190 --> 00:07:07,210
is immutable and SPARC enforces it and

00:07:05,260 --> 00:07:09,220
requires it and then takes advantage of

00:07:07,210 --> 00:07:11,890
the fact that the data is immutable two

00:07:09,220 --> 00:07:14,530
parallel lies the work it has some

00:07:11,890 --> 00:07:16,270
really cool optimizer it actually has

00:07:14,530 --> 00:07:18,340
three different optimizers depending on

00:07:16,270 --> 00:07:20,320
which mode but it's not super important

00:07:18,340 --> 00:07:23,770
and it takes a different approach to

00:07:20,320 --> 00:07:25,540
resiliency and so it recovers from

00:07:23,770 --> 00:07:27,550
failures rather than protecting from

00:07:25,540 --> 00:07:28,840
failures and in the MapReduce world we

00:07:27,550 --> 00:07:30,610
save things out to three different

00:07:28,840 --> 00:07:32,220
computers and we assume we won't have

00:07:30,610 --> 00:07:34,960
three computers fail at the same time

00:07:32,220 --> 00:07:37,510
which that's that works out pretty well

00:07:34,960 --> 00:07:40,600
in the SPARC world we say everything is

00:07:37,510 --> 00:07:42,610
made of Cheetos and sometimes our Tito's

00:07:40,600 --> 00:07:47,860
will blow away but it's okay I can just

00:07:42,610 --> 00:07:49,419
buy more Cheetos mmm but maybe the

00:07:47,860 --> 00:07:52,690
cheeto based analogy that do you have

00:07:49,419 --> 00:07:55,169
Cheetos here okay I don't know why it

00:07:52,690 --> 00:07:57,700
wasn't working then um but essentially

00:07:55,169 --> 00:07:59,230
spark just says hey I'll keep track of

00:07:57,700 --> 00:08:01,270
all of the work you've asked me to do if

00:07:59,230 --> 00:08:03,190
I lose some of it I'll just recompute it

00:08:01,270 --> 00:08:04,690
and it'll be fine and I think it's the

00:08:03,190 --> 00:08:06,580
best way to trick people into learning

00:08:04,690 --> 00:08:07,810
functional programming because you don't

00:08:06,580 --> 00:08:09,730
have to tell them that's what they're

00:08:07,810 --> 00:08:10,990
doing you can go to them and you can be

00:08:09,730 --> 00:08:12,970
like do you want to learn distributed

00:08:10,990 --> 00:08:14,800
systems and do cool machine learning

00:08:12,970 --> 00:08:15,400
problems and people are like that sounds

00:08:14,800 --> 00:08:17,560
like fun

00:08:15,400 --> 00:08:20,040
and then we're like here have a lambda

00:08:17,560 --> 00:08:24,340
and they're like mm-hmm

00:08:20,040 --> 00:08:29,140
okay I'll roll with that

00:08:24,340 --> 00:08:30,790
right and so SPARC is not just like one

00:08:29,140 --> 00:08:32,770
monolithic well it is one giant

00:08:30,790 --> 00:08:34,690
monolithic project and it has so many

00:08:32,770 --> 00:08:37,030
different things inside of it we have

00:08:34,690 --> 00:08:40,390
two separate graph algorithms that do

00:08:37,030 --> 00:08:43,210
not work we have two separate machine

00:08:40,390 --> 00:08:45,370
learning libraries this one's deprecated

00:08:43,210 --> 00:08:46,520
and that one's new and doesn't quite

00:08:45,370 --> 00:08:49,830
work

00:08:46,520 --> 00:08:53,910
we have two separate streaming systems

00:08:49,830 --> 00:08:56,220
this one's deprecated and I never really

00:08:53,910 --> 00:08:58,470
worked and this one has two different

00:08:56,220 --> 00:09:01,050
execution engines neither of which do

00:08:58,470 --> 00:09:02,940
exactly what you want but you can pick

00:09:01,050 --> 00:09:05,730
the trade-offs that are closest to what

00:09:02,940 --> 00:09:08,060
you're willing to accept and pretend

00:09:05,730 --> 00:09:14,790
that it works

00:09:08,060 --> 00:09:16,110
yay okay or not and obviously probably

00:09:14,790 --> 00:09:17,460
most of the people here are gonna work

00:09:16,110 --> 00:09:18,780
with spark and Scala

00:09:17,460 --> 00:09:20,490
but I think one of the nice things about

00:09:18,780 --> 00:09:21,990
it is if you're working with data

00:09:20,490 --> 00:09:23,940
scientists or other people in your

00:09:21,990 --> 00:09:25,680
organization you may not be able to

00:09:23,940 --> 00:09:27,930
immediately convince them to learn Scala

00:09:25,680 --> 00:09:29,490
but you can convince them to learn spark

00:09:27,930 --> 00:09:31,080
with Python because you can be like no

00:09:29,490 --> 00:09:34,380
no as a Python API and that's

00:09:31,080 --> 00:09:36,240
technically true but the Python API

00:09:34,380 --> 00:09:37,890
teaches them functional programming

00:09:36,240 --> 00:09:40,530
concepts and then later you can come

00:09:37,890 --> 00:09:42,600
back and be like wow yeah your code is

00:09:40,530 --> 00:09:44,610
kind of slower than mine here let me

00:09:42,600 --> 00:09:46,530
show you how to rewrite it into Scala it

00:09:44,610 --> 00:09:50,010
stays pretty much the same and they're

00:09:46,530 --> 00:09:51,240
like oh that seems like a win I don't

00:09:50,010 --> 00:09:53,520
mention this part at the Python

00:09:51,240 --> 00:09:55,770
conferences I speak at and so if

00:09:53,520 --> 00:09:57,920
anyone's watching the recording I'm

00:09:55,770 --> 00:10:01,680
sorry

00:09:57,920 --> 00:10:03,270
whatever okay so spark got some cool

00:10:01,680 --> 00:10:05,040
things right for making functional

00:10:03,270 --> 00:10:07,140
programming fun for distributed systems

00:10:05,040 --> 00:10:09,480
it's sort of strongly if forces

00:10:07,140 --> 00:10:12,060
immutable data you can break it but then

00:10:09,480 --> 00:10:13,560
your program doesn't work it has

00:10:12,060 --> 00:10:15,300
functional operators on its distributed

00:10:13,560 --> 00:10:17,580
collections and it has lambdas for

00:10:15,300 --> 00:10:19,350
everyone not just Scala people we very

00:10:17,580 --> 00:10:21,150
strongly encourage Python people they

00:10:19,350 --> 00:10:23,760
have lambdas and all of our examples

00:10:21,150 --> 00:10:25,710
show them how to use this it solved a

00:10:23,760 --> 00:10:27,510
business need there are lots of cool

00:10:25,710 --> 00:10:30,510
functional programming frameworks which

00:10:27,510 --> 00:10:32,490
have existed before spark but many of

00:10:30,510 --> 00:10:34,830
them didn't solve a need that businesses

00:10:32,490 --> 00:10:36,660
thought they have and spark solved a

00:10:34,830 --> 00:10:38,220
business need whether it's real or

00:10:36,660 --> 00:10:39,780
imaginary that lots of businesses

00:10:38,220 --> 00:10:41,700
thought they had they think they have

00:10:39,780 --> 00:10:42,420
big data even if their data fits on a

00:10:41,700 --> 00:10:44,370
floppy disk

00:10:42,420 --> 00:10:47,220
they have big data and they need a

00:10:44,370 --> 00:10:50,070
cluster and so they can run spark on

00:10:47,220 --> 00:10:51,660
their cluster and regardless of whether

00:10:50,070 --> 00:10:53,190
or not this business need was real they

00:10:51,660 --> 00:10:56,460
think they want it and so that's

00:10:53,190 --> 00:10:59,540
important and the the last one is it

00:10:56,460 --> 00:11:01,190
makes it really hard so I don't

00:10:59,540 --> 00:11:05,810
me wrong I still write code with VARs

00:11:01,190 --> 00:11:08,779
I'm not a good person but spark makes it

00:11:05,810 --> 00:11:11,029
hard to use VARs as like global state or

00:11:08,779 --> 00:11:13,279
things which are really bad it makes it

00:11:11,029 --> 00:11:16,009
challenging to do things that are bad

00:11:13,279 --> 00:11:18,050
not impossible because sometimes we all

00:11:16,009 --> 00:11:21,110
have to peel back the curtain and murder

00:11:18,050 --> 00:11:23,720
the person typing on the machine to get

00:11:21,110 --> 00:11:25,279
what we want done but it strongly

00:11:23,720 --> 00:11:28,579
discourages them and makes them really

00:11:25,279 --> 00:11:31,579
awkward which is which is good okay and

00:11:28,579 --> 00:11:33,230
it got a bunch of things not so right

00:11:31,579 --> 00:11:36,170
that makes functional programming with

00:11:33,230 --> 00:11:39,069
spark kind of painful one of the things

00:11:36,170 --> 00:11:43,100
is our approach to serializing closures

00:11:39,069 --> 00:11:45,050
is we have this wonderful piece of

00:11:43,100 --> 00:11:47,269
software called closure cleaner Scala

00:11:45,050 --> 00:11:50,000
and we have an equivalent in Python and

00:11:47,269 --> 00:11:53,630
an R I haven't looked because I'm

00:11:50,000 --> 00:11:56,329
terrified but essentially we take the

00:11:53,630 --> 00:11:59,139
Java serialize closure and then just

00:11:56,329 --> 00:12:04,149
start deleting things from it and it's

00:11:59,139 --> 00:12:09,199
faster most of the time but occasionally

00:12:04,149 --> 00:12:11,060
not so faster and occasionally wrong and

00:12:09,199 --> 00:12:12,740
then people think that closures have

00:12:11,060 --> 00:12:14,930
limitations which they don't really have

00:12:12,740 --> 00:12:16,490
they only have limitations because we're

00:12:14,930 --> 00:12:18,470
doing some crazy things to make their

00:12:16,490 --> 00:12:21,380
software go fast right that if they were

00:12:18,470 --> 00:12:23,000
using closures in another context would

00:12:21,380 --> 00:12:25,069
be totally fine right like referencing

00:12:23,000 --> 00:12:27,139
things in your outer class totally fine

00:12:25,069 --> 00:12:30,350
to do in a normal closure doing that in

00:12:27,139 --> 00:12:32,600
a spark closure makes your life really

00:12:30,350 --> 00:12:34,399
sad and sometimes people can get these

00:12:32,600 --> 00:12:36,579
things confused because to them spark

00:12:34,399 --> 00:12:39,380
and Scala can be hard to separate

00:12:36,579 --> 00:12:41,329
another one is we oh right we like

00:12:39,380 --> 00:12:43,699
strings everywhere for settings right

00:12:41,329 --> 00:12:45,800
everyone loves setting strings right

00:12:43,699 --> 00:12:47,959
it's the best way to convey integer

00:12:45,800 --> 00:12:50,660
information is inside of a string and

00:12:47,959 --> 00:12:52,160
sparks configuration definitely embrace

00:12:50,660 --> 00:12:54,500
that philosophy

00:12:52,160 --> 00:12:56,930
so sometimes we maybe encourage people

00:12:54,500 --> 00:13:01,550
to do things with types which are not so

00:12:56,930 --> 00:13:03,620
right that's cool and it's really hard

00:13:01,550 --> 00:13:05,839
to debug and sometimes it is actually

00:13:03,620 --> 00:13:07,730
scalos fault and other times it's that

00:13:05,839 --> 00:13:09,680
spark gave you 400 copies of the stack

00:13:07,730 --> 00:13:11,380
trace and won't tell you which one is

00:13:09,680 --> 00:13:14,990
important

00:13:11,380 --> 00:13:17,300
it literally will often give you two

00:13:14,990 --> 00:13:19,550
times the number of machines in your

00:13:17,300 --> 00:13:20,930
cluster plus some constant factor number

00:13:19,550 --> 00:13:24,850
of stack traces back when you have an

00:13:20,930 --> 00:13:28,670
exception and this can feel overwhelming

00:13:24,850 --> 00:13:30,830
even for me I when I get 400 sec traces

00:13:28,670 --> 00:13:32,720
even though in the like non lizard part

00:13:30,830 --> 00:13:34,790
of my brain I'm like yeah that's fine

00:13:32,720 --> 00:13:36,740
everything's fine it's the dog sitting

00:13:34,790 --> 00:13:38,690
in the room with fire the lizard part is

00:13:36,740 --> 00:13:41,990
my brain is like no it's on fire there's

00:13:38,690 --> 00:13:44,830
400 stack traces stop now investigate

00:13:41,990 --> 00:13:48,050
and and so that's that's sad and we also

00:13:44,830 --> 00:13:50,150
introduced two new api's and they were

00:13:48,050 --> 00:13:52,820
both introduced by going like how

00:13:50,150 --> 00:13:54,470
important are types anyways and

00:13:52,820 --> 00:13:56,690
unfortunately we found out the answer to

00:13:54,470 --> 00:13:58,970
that question and it turns out they were

00:13:56,690 --> 00:14:01,040
kind of useful and so we just tried to

00:13:58,970 --> 00:14:02,690
shove some types back in really quickly

00:14:01,040 --> 00:14:04,910
well no one's looking and be like yeah

00:14:02,690 --> 00:14:06,320
it's always head types what are you

00:14:04,910 --> 00:14:08,410
talking about that we'll leave here I

00:14:06,320 --> 00:14:11,420
doesn't exist anymore

00:14:08,410 --> 00:14:12,830
and actually we yeah if anyone wants to

00:14:11,420 --> 00:14:16,040
see the data frame deprecation it's

00:14:12,830 --> 00:14:19,130
super hilarious what we did but it

00:14:16,040 --> 00:14:20,810
doesn't work for Java users are there

00:14:19,130 --> 00:14:29,450
actually are there any Java users in the

00:14:20,810 --> 00:14:30,950
house don't be afraid okay so six people

00:14:29,450 --> 00:14:32,320
and one person who doesn't want to admit

00:14:30,950 --> 00:14:35,270
it but their friend thinks they use Java

00:14:32,320 --> 00:14:37,430
and that's fine right and and I think

00:14:35,270 --> 00:14:39,350
this is actually really good because we

00:14:37,430 --> 00:14:41,720
can slowly start to bring the Java

00:14:39,350 --> 00:14:44,450
people over into our world by being like

00:14:41,720 --> 00:14:48,020
I'm just saying word count is like 10

00:14:44,450 --> 00:14:50,990
lines man it's cool come join the dark

00:14:48,020 --> 00:14:54,920
side we have cookies and confusing stack

00:14:50,990 --> 00:14:57,980
traces but think of the cookies okay so

00:14:54,920 --> 00:15:00,050
what are the new api's so we have a

00:14:57,980 --> 00:15:02,720
streaming system with three different

00:15:00,050 --> 00:15:05,270
engines keeps it super exciting really

00:15:02,720 --> 00:15:07,490
hard to debug because no one really

00:15:05,270 --> 00:15:08,720
understands all of them so when it

00:15:07,490 --> 00:15:11,000
doesn't work you try switching the

00:15:08,720 --> 00:15:14,019
engine and now you have a whole new

00:15:11,000 --> 00:15:17,959
stack trace yeah

00:15:14,019 --> 00:15:19,399
okay no excitement um we introduced this

00:15:17,959 --> 00:15:21,410
thing called data frames where we were

00:15:19,399 --> 00:15:23,720
like types types are for suckers you

00:15:21,410 --> 00:15:26,839
know what everyone loves runtime schema

00:15:23,720 --> 00:15:29,180
evaluation and then then no one loved

00:15:26,839 --> 00:15:32,240
that except the Python people but even

00:15:29,180 --> 00:15:36,230
they were like uh I have some feelings

00:15:32,240 --> 00:15:37,699
and we were like well no and a new

00:15:36,230 --> 00:15:39,889
machine learning API because obviously

00:15:37,699 --> 00:15:42,589
machine learning jobs don't take very

00:15:39,889 --> 00:15:43,910
long to run it's totally okay to do no

00:15:42,589 --> 00:15:45,379
type checking in your machine learning

00:15:43,910 --> 00:15:47,149
API and assume all of the fields

00:15:45,379 --> 00:15:49,480
requested are present and fail at the

00:15:47,149 --> 00:15:54,379
very end of a 24 hour model training job

00:15:49,480 --> 00:15:58,639
it was a great decision okay um

00:15:54,379 --> 00:15:59,120
sarcasm - sorry okay so beta frames what

00:15:58,639 --> 00:16:01,310
did we do

00:15:59,120 --> 00:16:05,120
we were like everything is a row that's

00:16:01,310 --> 00:16:08,089
great it was not so great um and we

00:16:05,120 --> 00:16:10,910
actually we made sad decisions we made

00:16:08,089 --> 00:16:12,259
everything sequel inspired and are there

00:16:10,910 --> 00:16:17,209
sequel people in the house who really

00:16:12,259 --> 00:16:21,319
like sequel okay four people will really

00:16:17,209 --> 00:16:24,110
love the data frame v1 API it it feels

00:16:21,319 --> 00:16:26,089
like you're writing sequel in Scala yay

00:16:24,110 --> 00:16:28,490
but without type-checking it's not like

00:16:26,089 --> 00:16:33,800
the fancy sequel like it's just like

00:16:28,490 --> 00:16:39,470
regular sad sequel or not set yeah that

00:16:33,800 --> 00:16:40,819
didn't get better okay we realized that

00:16:39,470 --> 00:16:44,240
we should add some functional operators

00:16:40,819 --> 00:16:45,889
to it um and we did sort of but one of

00:16:44,240 --> 00:16:47,480
the things which we didn't do was add

00:16:45,889 --> 00:16:50,180
them in a way that's easy for people in

00:16:47,480 --> 00:16:52,730
other languages to use and so we can't

00:16:50,180 --> 00:16:54,699
use them as easily to convert the Python

00:16:52,730 --> 00:16:57,019
people to the dark side with cookies

00:16:54,699 --> 00:17:00,079
they're like no it's fine I know sequel

00:16:57,019 --> 00:17:05,600
and I'm like oh but but I have cookies

00:17:00,079 --> 00:17:07,370
for you and they're they walk away but

00:17:05,600 --> 00:17:10,400
they have some cool things about them

00:17:07,370 --> 00:17:12,049
they're a lot faster and they have the

00:17:10,400 --> 00:17:14,150
potential to allow us to interoperate

00:17:12,049 --> 00:17:16,400
with other languages better and this is

00:17:14,150 --> 00:17:18,799
really cool if your desire to rewrite

00:17:16,400 --> 00:17:23,149
your co-workers Python code into Scala

00:17:18,799 --> 00:17:25,579
is about zero you can you can use their

00:17:23,149 --> 00:17:26,610
chunk of like really complex numeric

00:17:25,579 --> 00:17:29,670
computing

00:17:26,610 --> 00:17:31,980
site of Scala and let's kind of neat and

00:17:29,670 --> 00:17:35,309
I have a graph which you should all

00:17:31,980 --> 00:17:40,080
believe and trust as a vendor because

00:17:35,309 --> 00:17:41,940
vendors never lie so we can see here we

00:17:40,080 --> 00:17:44,190
have two RTD operations and this is

00:17:41,940 --> 00:17:46,230
computing I think some average

00:17:44,190 --> 00:17:49,049
information over some arbitrary data set

00:17:46,230 --> 00:17:50,790
and we can see that the data frame 1

00:17:49,049 --> 00:17:55,080
takes much less time and that's good

00:17:50,790 --> 00:17:56,850
yeah faster faster is better the only

00:17:55,080 --> 00:18:01,370
sadness is I had to give up functional

00:17:56,850 --> 00:18:05,280
programming so I sold my soul for like

00:18:01,370 --> 00:18:07,380
maybe 5 minutes and that's like not

00:18:05,280 --> 00:18:08,700
that's not a trade-off that I would make

00:18:07,380 --> 00:18:10,830
but unfortunately it's a trade-off that

00:18:08,700 --> 00:18:13,919
a lot of other people have made and I

00:18:10,830 --> 00:18:15,840
want to steal them back to my side and

00:18:13,919 --> 00:18:17,070
yeah our storage is a lot more efficient

00:18:15,840 --> 00:18:19,020
it turns out when you're competing

00:18:17,070 --> 00:18:20,280
against you have a serialization once

00:18:19,020 --> 00:18:24,510
again it's like competing with MapReduce

00:18:20,280 --> 00:18:26,250
not that hard but we have a nice graph

00:18:24,510 --> 00:18:30,330
which is like yeah fifty percent more

00:18:26,250 --> 00:18:32,220
efficient okay um

00:18:30,330 --> 00:18:33,750
whatever and there are other things that

00:18:32,220 --> 00:18:35,700
we could use in the comparison then

00:18:33,750 --> 00:18:37,770
becomes a lot less impressive it's true

00:18:35,700 --> 00:18:40,260
but it is still like five to ten percent

00:18:37,770 --> 00:18:43,320
more impressive so how do we use these

00:18:40,260 --> 00:18:46,460
things to do terrible sequel stuff but

00:18:43,320 --> 00:18:50,640
still do cool functional code right so

00:18:46,460 --> 00:18:53,640
this is okay we can tell ourselves it's

00:18:50,640 --> 00:18:54,809
a DSL rather than sequel if that helps

00:18:53,640 --> 00:18:57,990
us feel like we're still part of the

00:18:54,809 --> 00:19:01,799
cool kids club so we have this triple

00:18:57,990 --> 00:19:05,809
equals operator so much excitement

00:19:01,799 --> 00:19:08,610
um the great thing is double equals will

00:19:05,809 --> 00:19:10,860
sometimes compile here and then just

00:19:08,610 --> 00:19:12,530
result in crap other times it will not

00:19:10,860 --> 00:19:17,280
compile and that's lovely

00:19:12,530 --> 00:19:19,169
yay yay excitement okay so we can see

00:19:17,280 --> 00:19:22,080
the Select thing that looks a lot like

00:19:19,169 --> 00:19:23,970
sequel right select selecting sequel are

00:19:22,080 --> 00:19:26,610
totally the same thing but that reduced

00:19:23,970 --> 00:19:28,970
function some good happy functional

00:19:26,610 --> 00:19:32,850
programming just like mom used to bake

00:19:28,970 --> 00:19:36,049
so that's cool and yeah so we can put

00:19:32,850 --> 00:19:39,210
arbitrary Scala code into

00:19:36,049 --> 00:19:41,580
data sets and we can we can actually

00:19:39,210 --> 00:19:44,190
keep our nice little map operator yay

00:19:41,580 --> 00:19:46,739
everyone loves map right map is my

00:19:44,190 --> 00:19:48,330
favorite next to flat map who is my real

00:19:46,739 --> 00:19:50,600
favorite because flat map is in word

00:19:48,330 --> 00:19:53,070
count and word count is the best example

00:19:50,600 --> 00:19:54,840
and this is really good because I don't

00:19:53,070 --> 00:19:57,600
ever want to actually have to properly

00:19:54,840 --> 00:20:00,570
learn sequel but I do want my code to

00:19:57,600 --> 00:20:01,679
run faster right and I want to be able

00:20:00,570 --> 00:20:03,570
to work with people who have learnt

00:20:01,679 --> 00:20:05,369
sequel without having to rewrite all of

00:20:03,570 --> 00:20:06,809
their stuff into scala because then i

00:20:05,369 --> 00:20:10,320
probably have to learn sequel in the

00:20:06,809 --> 00:20:13,379
process and we can do word count with it

00:20:10,320 --> 00:20:19,340
yeah everyone loves word count right

00:20:13,379 --> 00:20:22,320
word count one vaguely excited person

00:20:19,340 --> 00:20:26,850
the same person who betrayed his java

00:20:22,320 --> 00:20:30,749
programmer friend so that's cool but

00:20:26,850 --> 00:20:33,119
this is load some arbitrary data selects

00:20:30,749 --> 00:20:36,389
a text field out of it tells spark the

00:20:33,119 --> 00:20:38,549
type and this this parts a little sad

00:20:36,389 --> 00:20:40,950
like we have to explicitly specify our

00:20:38,549 --> 00:20:43,619
type once we've loaded our data but that

00:20:40,950 --> 00:20:45,330
can be pretty much any case class it's

00:20:43,619 --> 00:20:47,730
just because we have a string we don't

00:20:45,330 --> 00:20:49,799
we don't have to define a case class of

00:20:47,730 --> 00:20:51,840
the word string that would be kind of

00:20:49,799 --> 00:20:55,350
redundant and then we can still have our

00:20:51,840 --> 00:20:58,499
fun flat map the only problem is when I

00:20:55,350 --> 00:21:02,669
get to this group by operator I lose all

00:20:58,499 --> 00:21:06,149
of my type information yay types are for

00:21:02,669 --> 00:21:08,399
suckers anyways but the cool thing about

00:21:06,149 --> 00:21:10,980
this is that group by inspark is

00:21:08,399 --> 00:21:14,279
terrible how many people have called

00:21:10,980 --> 00:21:18,989
group by key on rdd's okay keep your

00:21:14,279 --> 00:21:23,309
hand up if it worked one person had it

00:21:18,989 --> 00:21:26,489
work yeah kinda okay so group by key is

00:21:23,309 --> 00:21:28,169
not always breaking your code but when

00:21:26,489 --> 00:21:30,330
you encounter a big data group by key

00:21:28,169 --> 00:21:32,489
will probably bake your code and you're

00:21:30,330 --> 00:21:35,129
probably using spark because of big data

00:21:32,489 --> 00:21:36,450
and now if you have high cardinality

00:21:35,129 --> 00:21:38,369
information in your groups don't

00:21:36,450 --> 00:21:40,529
actually have that much intersection

00:21:38,369 --> 00:21:42,629
group by key is fine it's just part of

00:21:40,529 --> 00:21:44,999
the API which we introduced which 90% of

00:21:42,629 --> 00:21:47,340
the time when it's called results in an

00:21:44,999 --> 00:21:48,809
out of memory exception which to me was

00:21:47,340 --> 00:21:49,180
a sign that this was something we should

00:21:48,809 --> 00:21:51,370
double

00:21:49,180 --> 00:21:53,410
down on and introduced in our new API

00:21:51,370 --> 00:21:57,220
but this time without that of memory

00:21:53,410 --> 00:21:59,760
exceptions and so the nice thing here is

00:21:57,220 --> 00:22:01,660
that this aggregate function actually

00:21:59,760 --> 00:22:03,520
understands what's happening because

00:22:01,660 --> 00:22:05,950
we're writing it in a DSL rather than

00:22:03,520 --> 00:22:07,570
arbitrary Scala code and that's a little

00:22:05,950 --> 00:22:10,210
sad right like I don't have whatever

00:22:07,570 --> 00:22:14,020
code I want here but SPARC knows what's

00:22:10,210 --> 00:22:16,690
happening and it's able to pipeline this

00:22:14,020 --> 00:22:18,100
essentially reduction operator while

00:22:16,690 --> 00:22:19,540
it's grouping the data together and it

00:22:18,100 --> 00:22:22,870
doesn't have to make this like giant

00:22:19,540 --> 00:22:25,780
list and then fail and be sad I can I

00:22:22,870 --> 00:22:27,190
can be happy briefly and I'll get

00:22:25,780 --> 00:22:31,180
another memory except for in another day

00:22:27,190 --> 00:22:34,840
but so how do we do things that were

00:22:31,180 --> 00:22:40,690
hard to do with our disease we put on a

00:22:34,840 --> 00:22:46,900
trash can bag trash bag and are a cat ok

00:22:40,690 --> 00:22:51,250
so this is this impressive to anyone no

00:22:46,900 --> 00:22:53,710
one okay so this is impressive to me and

00:22:51,250 --> 00:22:58,330
this is because doing this on the RDD

00:22:53,710 --> 00:23:00,310
api is really really terrible right like

00:22:58,330 --> 00:23:03,160
this is an incredibly simple concept

00:23:00,310 --> 00:23:05,260
that is really annoying to express with

00:23:03,160 --> 00:23:06,970
reduced by Kean's Bartlett and so it's

00:23:05,260 --> 00:23:08,680
kind of cool I can have my functional

00:23:06,970 --> 00:23:09,850
programming and then when I want to

00:23:08,680 --> 00:23:12,550
compute a bunch of things that are

00:23:09,850 --> 00:23:14,620
comparatively actually simple but really

00:23:12,550 --> 00:23:17,020
annoying to express because I have to

00:23:14,620 --> 00:23:18,640
keep track of like X 1 X 2 X 3 or make

00:23:17,020 --> 00:23:20,470
like a weird case class to keep track of

00:23:18,640 --> 00:23:22,240
all of these different things I can just

00:23:20,470 --> 00:23:27,280
let the computer do that for me

00:23:22,240 --> 00:23:29,140
yay so this is this is the kind of thing

00:23:27,280 --> 00:23:33,340
which is impressive to the people that

00:23:29,140 --> 00:23:35,670
wrote it so it's cool trust me

00:23:33,340 --> 00:23:40,000
the other thing is windowed operations

00:23:35,670 --> 00:23:42,610
so essentially trying to look at some of

00:23:40,000 --> 00:23:44,410
our data over you know plus minus K or J

00:23:42,610 --> 00:23:45,970
right so I'm gonna look at some number

00:23:44,410 --> 00:23:48,130
of Records before some room number of

00:23:45,970 --> 00:23:50,800
Records after I can have some type of

00:23:48,130 --> 00:23:53,230
sorting order this is also really

00:23:50,800 --> 00:23:54,850
annoying to do in SPARC and distributed

00:23:53,230 --> 00:23:57,430
systems in general window operations

00:23:54,850 --> 00:24:00,040
kind of suck they're super easy to do on

00:23:57,430 --> 00:24:01,470
one computer but that they're really

00:24:00,040 --> 00:24:04,200
frustrating to do here

00:24:01,470 --> 00:24:06,600
and so yeah okay so we can see it goes

00:24:04,200 --> 00:24:08,519
the window shifts down I probably should

00:24:06,600 --> 00:24:10,590
have put some results in there but just

00:24:08,519 --> 00:24:14,009
pretend that I was less lazy and filled

00:24:10,590 --> 00:24:15,299
in the second half of this slide and so

00:24:14,009 --> 00:24:17,070
this is what our window spec can look

00:24:15,299 --> 00:24:20,220
like we can say I'm interested in

00:24:17,070 --> 00:24:22,679
knowing the capital gains tax in America

00:24:20,220 --> 00:24:24,929
for people ten years younger than me ten

00:24:22,679 --> 00:24:27,360
years older with some average mostly

00:24:24,929 --> 00:24:29,039
because I want to see like how many more

00:24:27,360 --> 00:24:32,700
stock options I should be asking my boss

00:24:29,039 --> 00:24:34,019
for and I know an average because you

00:24:32,700 --> 00:24:37,350
know people my age might not be

00:24:34,019 --> 00:24:38,970
completely representative but like the

00:24:37,350 --> 00:24:41,309
nice thing is this is so much less work

00:24:38,970 --> 00:24:44,039
the only downside is it's not very

00:24:41,309 --> 00:24:47,190
functional right I'm kind of restricted

00:24:44,039 --> 00:24:48,690
to this average or I mean I'm not

00:24:47,190 --> 00:24:52,710
restricted to just average there's like

00:24:48,690 --> 00:24:53,940
obviously min and Max as well there's

00:24:52,710 --> 00:24:55,919
there's actually a large number of

00:24:53,940 --> 00:24:57,480
built-in aggregate functions but like I

00:24:55,919 --> 00:24:59,399
might have some special business logic

00:24:57,480 --> 00:25:01,470
in this case I would probably replace

00:24:59,399 --> 00:25:03,330
this with max times two so that when I

00:25:01,470 --> 00:25:05,279
go and ask my boss for more money it's

00:25:03,330 --> 00:25:07,379
like yeah the computer says you should

00:25:05,279 --> 00:25:10,409
pay me more and you should not read my

00:25:07,379 --> 00:25:12,049
notebook but pay me more um that's never

00:25:10,409 --> 00:25:14,059
worked but if it works for you

00:25:12,049 --> 00:25:17,429
congratulations

00:25:14,059 --> 00:25:19,139
so that's that's kind of sad um we'll

00:25:17,429 --> 00:25:22,830
skip UDF's their third set we're gonna

00:25:19,139 --> 00:25:24,450
go look at custom aggregates everyone

00:25:22,830 --> 00:25:32,159
loves writing case classes to aggregate

00:25:24,450 --> 00:25:37,019
data right okay zero people so this is

00:25:32,159 --> 00:25:39,659
really really ugly I have the immutable

00:25:37,019 --> 00:25:42,480
aggregation buffer and I return unit and

00:25:39,659 --> 00:25:45,000
I have update and I return unit and they

00:25:42,480 --> 00:25:49,860
have merged and I return unit and I have

00:25:45,000 --> 00:25:51,840
evaluate and I return any oh yeah

00:25:49,860 --> 00:25:56,190
this API definitely shows that we were

00:25:51,840 --> 00:25:59,669
like types what are those that's that's

00:25:56,190 --> 00:26:02,309
unfortunate um so it would actually be

00:25:59,669 --> 00:26:03,870
quite possible to go ahead so we can

00:26:02,309 --> 00:26:05,159
make a user-defined aggregate function

00:26:03,870 --> 00:26:06,870
and then we can go back to our window

00:26:05,159 --> 00:26:08,340
thing wherever the hell that was and

00:26:06,870 --> 00:26:10,379
then we can use our user-defined

00:26:08,340 --> 00:26:12,570
aggregate function here and that's I

00:26:10,379 --> 00:26:15,080
mean it's better than being stuck with

00:26:12,570 --> 00:26:17,730
the built-in options but

00:26:15,080 --> 00:26:21,779
is anyone really excited about filling

00:26:17,730 --> 00:26:22,139
in the definition of this class yeah no

00:26:21,779 --> 00:26:25,139
one

00:26:22,139 --> 00:26:25,740
so essentially because we made the API

00:26:25,139 --> 00:26:29,249
this way

00:26:25,740 --> 00:26:31,110
it's really ugly but it doesn't have to

00:26:29,249 --> 00:26:32,399
be right it's not inherently ugly and

00:26:31,110 --> 00:26:35,879
this is actually one of the things which

00:26:32,399 --> 00:26:38,999
we could fix if people thought it was

00:26:35,879 --> 00:26:41,159
important does anyone think that they

00:26:38,999 --> 00:26:45,230
would prefer a different aggregate maybe

00:26:41,159 --> 00:26:47,730
one that looks like a normal aggregate

00:26:45,230 --> 00:26:52,409
okay does anyone think that enough that

00:26:47,730 --> 00:26:54,720
they want to make a PR dammit oh yay

00:26:52,409 --> 00:26:57,330
okay we're gonna hang out after this um

00:26:54,720 --> 00:26:58,590
and the people who were like yeah I want

00:26:57,330 --> 00:27:01,169
this to be better would you be willing

00:26:58,590 --> 00:27:05,309
to say comment on a PR saying the old

00:27:01,169 --> 00:27:07,230
thing was terrible maybe okay what one

00:27:05,309 --> 00:27:11,450
person will help me and I can create

00:27:07,230 --> 00:27:15,720
some extra github accounts please don't

00:27:11,450 --> 00:27:18,350
damn it I'm not a very good evil person

00:27:15,720 --> 00:27:22,080
but as I share my evil plans on camera

00:27:18,350 --> 00:27:24,029
yeah okay this is also boring so um I

00:27:22,080 --> 00:27:26,100
mentioned that we have a lot of things

00:27:24,029 --> 00:27:27,929
that are built in and it's not just

00:27:26,100 --> 00:27:30,149
aggregates we have like all kinds of

00:27:27,929 --> 00:27:36,179
functions and it's cool everything takes

00:27:30,149 --> 00:27:38,100
in either a string or a column yay and

00:27:36,179 --> 00:27:39,539
the string name it references a column

00:27:38,100 --> 00:27:41,639
that may or may not exist and you'll

00:27:39,539 --> 00:27:43,529
find out at runtime and you can actually

00:27:41,639 --> 00:27:45,749
define columns which don't exist as well

00:27:43,529 --> 00:27:46,679
it takes a little bit more work but I've

00:27:45,749 --> 00:27:50,519
done it by accident

00:27:46,679 --> 00:27:54,360
and so that's really cool and really

00:27:50,519 --> 00:27:58,860
great to try and compute the min of like

00:27:54,360 --> 00:28:00,240
maybe a binary field and then at runtime

00:27:58,860 --> 00:28:02,279
find out that sparks says now I don't

00:28:00,240 --> 00:28:06,779
know how to compare these two and then

00:28:02,279 --> 00:28:10,019
you're just like oh but thankfully the

00:28:06,779 --> 00:28:11,879
the friendless people brings type

00:28:10,019 --> 00:28:13,889
columns and you can actually write

00:28:11,879 --> 00:28:16,499
functions which which check that the

00:28:13,889 --> 00:28:17,999
types are the right types not not even

00:28:16,499 --> 00:28:20,669
just that the types are the right types

00:28:17,999 --> 00:28:22,529
but that the column exists in your input

00:28:20,669 --> 00:28:24,679
data which has some fancy share right

00:28:22,529 --> 00:28:27,539
there

00:28:24,679 --> 00:28:29,190
admittedly like they don't have complete

00:28:27,539 --> 00:28:30,659
one-to-one parity I think with all of

00:28:29,190 --> 00:28:34,529
the functions and functions dot Scala

00:28:30,659 --> 00:28:37,470
but you can add your own and just make a

00:28:34,529 --> 00:28:40,860
custom local maven publish that's normal

00:28:37,470 --> 00:28:43,529
right that's great okay um cool let's

00:28:40,860 --> 00:28:46,559
talk about another API that's sad and we

00:28:43,529 --> 00:28:48,570
can fix maybe so spark ml pipelines are

00:28:46,559 --> 00:28:53,399
spiked it inspired psych it is notably

00:28:48,570 --> 00:28:54,840
in Python and Python notably is not what

00:28:53,399 --> 00:28:56,580
one would consider a strongly typed

00:28:54,840 --> 00:28:59,850
language um you can't add type

00:28:56,580 --> 00:29:02,870
annotations to it but psych it did not

00:28:59,850 --> 00:29:05,460
and neither did we

00:29:02,870 --> 00:29:08,549
and instead what we did is we said I

00:29:05,460 --> 00:29:10,740
love runtime schema checking what I'll

00:29:08,549 --> 00:29:11,070
do is when you ask me to Train on some

00:29:10,740 --> 00:29:13,559
data

00:29:11,070 --> 00:29:16,169
I'll see if that data looks kind of like

00:29:13,559 --> 00:29:17,730
what I expected based on the program are

00:29:16,169 --> 00:29:22,950
remembering to write a function called

00:29:17,730 --> 00:29:26,070
validate and that works at least some of

00:29:22,950 --> 00:29:26,580
the time the only problem is when it

00:29:26,070 --> 00:29:30,600
doesn't

00:29:26,580 --> 00:29:34,950
machine learning jobs often take more

00:29:30,600 --> 00:29:37,679
compute resources than I'm worth right

00:29:34,950 --> 00:29:39,360
especially if I don't own the computers

00:29:37,679 --> 00:29:41,220
I write right like running this on a

00:29:39,360 --> 00:29:43,710
bunch of GPUs it gets really expensive

00:29:41,220 --> 00:29:45,179
and so having this fail for like silly

00:29:43,710 --> 00:29:47,340
type errors is really frustrating

00:29:45,179 --> 00:29:50,279
once again frameless to the rescue blah

00:29:47,340 --> 00:29:54,390
blah blah but yeah we couldn't we can

00:29:50,279 --> 00:29:56,640
see Oh strings we don't even take

00:29:54,390 --> 00:29:59,909
columns here so we don't we have no idea

00:29:56,640 --> 00:30:01,500
if this data exists or not and that's

00:29:59,909 --> 00:30:03,870
great and then I make this pipeline

00:30:01,500 --> 00:30:06,330
which concatenates two things into a

00:30:03,870 --> 00:30:09,419
vector and turns a category into a

00:30:06,330 --> 00:30:11,190
category index and that's essentially

00:30:09,419 --> 00:30:14,070
just because we can't actually handle

00:30:11,190 --> 00:30:15,510
strings uh-huh so we have to turn them

00:30:14,070 --> 00:30:17,580
into integers for you behind your back

00:30:15,510 --> 00:30:22,649
it's okay I'm actually not integers

00:30:17,580 --> 00:30:26,549
floating points which are great for

00:30:22,649 --> 00:30:31,029
comparing categorical features

00:30:26,549 --> 00:30:33,220
um sometimes 1.0 is equal to 1.0 and I'm

00:30:31,029 --> 00:30:36,690
really happy about those times it's the

00:30:33,220 --> 00:30:38,830
other times that I'm less happy about um

00:30:36,690 --> 00:30:40,870
whatever we don't have to worry about

00:30:38,830 --> 00:30:43,630
how this pipeline works too much but

00:30:40,870 --> 00:30:46,299
what matters is no will skip this this

00:30:43,630 --> 00:30:48,460
is sad um what matters is I can train a

00:30:46,299 --> 00:30:52,390
decision tree on top of it world-class

00:30:48,460 --> 00:30:53,799
machine learning we can also actually

00:30:52,390 --> 00:30:54,970
put in deep learning if you're just

00:30:53,799 --> 00:30:58,090
trying to raise money and you don't need

00:30:54,970 --> 00:30:59,559
a real model but you can also swap in

00:30:58,090 --> 00:31:01,360
linear regression or really whatever you

00:30:59,559 --> 00:31:03,100
want and you can just sell spark to

00:31:01,360 --> 00:31:05,019
train this and it all train all of the

00:31:03,100 --> 00:31:07,929
things together and it'll output the

00:31:05,019 --> 00:31:10,960
result the only downside of doing it

00:31:07,929 --> 00:31:13,360
this way is if you're doing it and say

00:31:10,960 --> 00:31:15,970
for example a notebook when it fails you

00:31:13,360 --> 00:31:18,130
you you don't get the partial result

00:31:15,970 --> 00:31:19,840
back you just get the pipeline fitting

00:31:18,130 --> 00:31:21,429
failed and so if you're interactively

00:31:19,840 --> 00:31:23,980
developing in a notebook or a repple

00:31:21,429 --> 00:31:25,690
this is not so much fun but if you're

00:31:23,980 --> 00:31:27,130
doing this like in production putting it

00:31:25,690 --> 00:31:28,419
together like this is much more fun

00:31:27,130 --> 00:31:29,740
because then you don't have to remember

00:31:28,419 --> 00:31:35,649
to chain together a bunch of different

00:31:29,740 --> 00:31:38,769
things so this cat is having a really

00:31:35,649 --> 00:31:41,289
good time because this cat has found out

00:31:38,769 --> 00:31:44,500
about the wonders of Apache aero how

00:31:41,289 --> 00:31:47,049
many people know about Apache aero whoa

00:31:44,500 --> 00:31:50,559
more than I expected um for the rest of

00:31:47,049 --> 00:31:52,720
you hmm I am not paid or endorsed by

00:31:50,559 --> 00:31:57,809
Apache aero but you might get confused

00:31:52,720 --> 00:31:57,809
shortly it's the magical future Oh

00:31:58,029 --> 00:32:05,620
if you trust vendors this is 242 times

00:32:00,820 --> 00:32:11,940
faster it is not but it might be 2 times

00:32:05,620 --> 00:32:14,559
faster and 2 is pretty close to 242 um

00:32:11,940 --> 00:32:18,700
see the earlier comment about floating

00:32:14,559 --> 00:32:21,460
points okay so this this future is

00:32:18,700 --> 00:32:23,529
probably over promised but that should

00:32:21,460 --> 00:32:27,730
not stop us from enjoying some of the

00:32:23,529 --> 00:32:30,029
cool parts of it and this is in Python

00:32:27,730 --> 00:32:35,830
are there any Python users in the house

00:32:30,029 --> 00:32:37,659
don't be afraid to and yeah you were app

00:32:35,830 --> 00:32:40,820
icon

00:32:37,659 --> 00:32:43,279
okay three and some other potentially

00:32:40,820 --> 00:32:45,350
scared people I guess it's but the nice

00:32:43,279 --> 00:32:47,269
thing about this is that python is great

00:32:45,350 --> 00:32:49,730
for numerical computation like adding

00:32:47,269 --> 00:32:52,309
two integers together we all would not

00:32:49,730 --> 00:32:54,019
trust the JVM to do this operations we

00:32:52,309 --> 00:32:56,240
need to go to Python which will shell

00:32:54,019 --> 00:32:58,399
out two Fortran which is where real math

00:32:56,240 --> 00:33:01,639
happens according to some blog post I

00:32:58,399 --> 00:33:04,700
read on the Internet and the cool thing

00:33:01,639 --> 00:33:07,220
about this is that oh okay there was

00:33:04,700 --> 00:33:09,350
actually mm-hmm I changed my slides

00:33:07,220 --> 00:33:10,880
right before this talk and it looks like

00:33:09,350 --> 00:33:12,529
I deleted the part which explained the

00:33:10,880 --> 00:33:14,389
cool part so I'll just do it by hand so

00:33:12,529 --> 00:33:16,519
the cool part about this is that we can

00:33:14,389 --> 00:33:19,880
actually take these pandas UDF's

00:33:16,519 --> 00:33:22,100
and we can use them from Scala we have

00:33:19,880 --> 00:33:23,809
to do a whole bunch of boilerplate or

00:33:22,100 --> 00:33:25,970
just copy and paste it because copy and

00:33:23,809 --> 00:33:28,100
pasted code is great and then we can go

00:33:25,970 --> 00:33:30,590
ahead and we can use our co-workers

00:33:28,100 --> 00:33:32,809
Python code really quickly just does

00:33:30,590 --> 00:33:37,039
anyone work with people who work in non

00:33:32,809 --> 00:33:39,350
jvm languages that's a lot of people how

00:33:37,039 --> 00:33:42,399
many of you enjoy rewriting your

00:33:39,350 --> 00:33:44,960
co-workers code to run on the JVM

00:33:42,399 --> 00:33:49,630
there's like one and a half people who

00:33:44,960 --> 00:33:50,840
are into some weird and that's fine

00:33:49,630 --> 00:33:52,399
whatever

00:33:50,840 --> 00:33:55,039
it's fine if you want to rewrite your

00:33:52,399 --> 00:33:56,960
friend's code just don't tell them but

00:33:55,039 --> 00:33:59,510
the nice thing is we don't have to now

00:33:56,960 --> 00:34:02,419
and we no longer I mean it's not 242

00:33:59,510 --> 00:34:04,429
times faster but the Python code that we

00:34:02,419 --> 00:34:06,470
call is no longer super expensive to

00:34:04,429 --> 00:34:10,220
call we can we can use the bits of

00:34:06,470 --> 00:34:12,740
Python code really inexpensively and no

00:34:10,220 --> 00:34:14,389
we can't use that part for streaming but

00:34:12,740 --> 00:34:16,310
we can use the data set api for

00:34:14,389 --> 00:34:17,750
streaming and that's the part where i

00:34:16,310 --> 00:34:20,119
have two different streaming engines

00:34:17,750 --> 00:34:21,950
that i can turn on and off and will

00:34:20,119 --> 00:34:24,379
actually sometimes override what

00:34:21,950 --> 00:34:26,540
decision i've made based on where I'm

00:34:24,379 --> 00:34:28,429
writing my data out to so I can

00:34:26,540 --> 00:34:30,379
configure it to use one streaming engine

00:34:28,429 --> 00:34:32,419
but if I write my data to a source which

00:34:30,379 --> 00:34:34,909
doesn't support it we just change it

00:34:32,419 --> 00:34:37,069
behind your back it's a really great

00:34:34,909 --> 00:34:38,540
design mechanism that allows you to

00:34:37,069 --> 00:34:42,859
easily replicate your production

00:34:38,540 --> 00:34:46,490
environment locally okay maybe I should

00:34:42,859 --> 00:34:48,079
do less sarcasm so yeah its new

00:34:46,490 --> 00:34:49,139
structured treatment is new to spark to

00:34:48,079 --> 00:34:50,609
zero

00:34:49,139 --> 00:34:52,589
and after that we were like you know

00:34:50,609 --> 00:34:54,299
what I'm getting bored I want to write

00:34:52,589 --> 00:34:56,220
more new software I don't want to fix

00:34:54,299 --> 00:34:58,410
the bugs in the old one so that's why we

00:34:56,220 --> 00:35:01,950
brought the new execution engine in 2.3

00:34:58,410 --> 00:35:04,470
and it's kind of cool and this is what

00:35:01,950 --> 00:35:08,069
it can look like I can compute my

00:35:04,470 --> 00:35:10,049
average happiness for coffees and I can

00:35:08,069 --> 00:35:12,000
do that with the DSL we were looking at

00:35:10,049 --> 00:35:13,859
earlier and I couldn't I can put that

00:35:12,000 --> 00:35:21,329
terrible case class that I wrote earlier

00:35:13,859 --> 00:35:23,700
in here as well it's it's really sad we

00:35:21,329 --> 00:35:27,240
can all sorry we can also use map

00:35:23,700 --> 00:35:30,990
however there's a bunch of random

00:35:27,240 --> 00:35:35,069
functionality inside of the data set

00:35:30,990 --> 00:35:39,750
functional API which will compile but

00:35:35,069 --> 00:35:44,010
not work at runtime on the data sets

00:35:39,750 --> 00:35:47,579
streaming API so if you use this you

00:35:44,010 --> 00:35:52,109
need really good tests otherwise you'll

00:35:47,579 --> 00:35:54,839
be really sad okay um and I want to

00:35:52,109 --> 00:35:58,740
leave us all on a sad note because

00:35:54,839 --> 00:36:00,900
happiness is overrated and admittedly I

00:35:58,740 --> 00:36:02,849
float between Python and Scala so I will

00:36:00,900 --> 00:36:09,150
probably still have a job I hope knock

00:36:02,849 --> 00:36:10,619
on whatever Montay but one of the things

00:36:09,150 --> 00:36:15,210
which is which is kind of scary about

00:36:10,619 --> 00:36:17,789
this slide which implies is 242 times

00:36:15,210 --> 00:36:19,799
faster is that the Python users who used

00:36:17,789 --> 00:36:21,269
to be able to go to and be like it's

00:36:19,799 --> 00:36:23,670
time to rewrite your code to be

00:36:21,269 --> 00:36:25,170
production-ready let's let's put this in

00:36:23,670 --> 00:36:26,099
Scala are just going to be like nah

00:36:25,170 --> 00:36:28,019
that's fine

00:36:26,099 --> 00:36:30,750
it's good enough in Python and that's

00:36:28,019 --> 00:36:32,279
that's cool but then we're losing this

00:36:30,750 --> 00:36:34,740
new source of people that we can bring

00:36:32,279 --> 00:36:36,630
into our community to keep it like happy

00:36:34,740 --> 00:36:39,269
and alive and instead the Python

00:36:36,630 --> 00:36:42,029
community can like steal them I mean

00:36:39,269 --> 00:36:44,910
it's okay either way I get paid but I

00:36:42,029 --> 00:36:46,859
like functional programming and I like

00:36:44,910 --> 00:36:49,440
types so I want them to come to our

00:36:46,859 --> 00:36:50,910
community instead because otherwise I

00:36:49,440 --> 00:36:54,079
have to write a lot of code without any

00:36:50,910 --> 00:36:57,259
types and I'm just gonna be really sad

00:36:54,079 --> 00:37:01,499
what other sad things do I have

00:36:57,259 --> 00:37:03,660
yeah okay so what are what are the main

00:37:01,499 --> 00:37:09,079
takeaways so the main takeaways are yeah

00:37:03,660 --> 00:37:12,329
we do have functional api's sort of in

00:37:09,079 --> 00:37:14,369
datasets we added them after the fact

00:37:12,329 --> 00:37:16,890
and it shows when you try and do

00:37:14,369 --> 00:37:18,960
anything beyond a map right once even if

00:37:16,890 --> 00:37:22,200
we try and do a simple aggregate we end

00:37:18,960 --> 00:37:25,859
up getting delightful things like any

00:37:22,200 --> 00:37:28,230
and mutable row option which are super

00:37:25,859 --> 00:37:29,970
clear and just so it's clear rows can

00:37:28,230 --> 00:37:32,339
contain an arbitrary number of fields

00:37:29,970 --> 00:37:35,279
all of which can be of almost but not

00:37:32,339 --> 00:37:40,470
quite any type and no we don't know

00:37:35,279 --> 00:37:42,029
until we access it it's great um but

00:37:40,470 --> 00:37:43,859
don't worry we can still sell enterprise

00:37:42,029 --> 00:37:47,009
support contracts and training to banks

00:37:43,859 --> 00:37:49,589
because datasets work better in Scala

00:37:47,009 --> 00:37:51,390
for now and we can we can write our

00:37:49,589 --> 00:37:52,769
functional programming against them we

00:37:51,390 --> 00:37:55,319
just really have to hope that they're

00:37:52,769 --> 00:37:57,059
not looking too closely or we have to

00:37:55,319 --> 00:38:00,839
convince them to use frameless which

00:37:57,059 --> 00:38:04,019
that's an option to spark ml uses data

00:38:00,839 --> 00:38:07,920
frames which is the even more untyped

00:38:04,019 --> 00:38:09,599
precursor to datasets and that's like

00:38:07,920 --> 00:38:12,119
data frames which are essentially never

00:38:09,599 --> 00:38:13,950
allowed to contain type information yeah

00:38:12,119 --> 00:38:16,950
I'm sorry data frames are data sets with

00:38:13,950 --> 00:38:19,710
explicitly no type information so enter

00:38:16,950 --> 00:38:21,599
there all hope is lost but unfortunately

00:38:19,710 --> 00:38:24,119
how many people's company is like

00:38:21,599 --> 00:38:27,509
excited about or investigating or

00:38:24,119 --> 00:38:30,960
working with machine learning it's a lot

00:38:27,509 --> 00:38:32,309
of people right and that's kind of sad

00:38:30,960 --> 00:38:35,460
that if we want to do this cool stuff

00:38:32,309 --> 00:38:41,970
for now we we have to give up a lot of

00:38:35,460 --> 00:38:43,319
our cool toys and yeah you you can was

00:38:41,970 --> 00:38:46,369
there anyone from San Francisco trying

00:38:43,319 --> 00:38:48,359
to raise a Series A for their startup

00:38:46,369 --> 00:38:53,220
okay yeah we don't have to cover deep

00:38:48,359 --> 00:38:54,539
learning great yeah cool oh we're

00:38:53,220 --> 00:38:56,190
getting this is the second most

00:38:54,539 --> 00:38:58,259
important slide and the most important

00:38:56,190 --> 00:39:01,529
slide will be coming up soon here is a

00:38:58,259 --> 00:39:04,739
collection of books about spark here is

00:39:01,529 --> 00:39:07,720
the book where I make the most money it

00:39:04,739 --> 00:39:10,339
is also about spark

00:39:07,720 --> 00:39:12,890
it is largely focused on functional

00:39:10,339 --> 00:39:15,380
programming as well it's it's written in

00:39:12,890 --> 00:39:17,029
Scala because it predates the part where

00:39:15,380 --> 00:39:22,450
you could write fast Python code for

00:39:17,029 --> 00:39:25,339
SPARC also because I like Scala more

00:39:22,450 --> 00:39:28,519
Python people I'm sorry please have me

00:39:25,339 --> 00:39:30,230
back at your conference um and you can

00:39:28,519 --> 00:39:32,420
buy it from the scrappy Seattle

00:39:30,230 --> 00:39:35,630
bookstore because Jeff Bezos wants to

00:39:32,420 --> 00:39:37,160
buy another newspaper and one copy of

00:39:35,630 --> 00:39:39,470
high-performance SPARC is about a

00:39:37,160 --> 00:39:42,410
quarter of a cup of coffee in San

00:39:39,470 --> 00:39:43,999
Francisco and you can help an developer

00:39:42,410 --> 00:39:49,190
from turning to a life of enterprise

00:39:43,999 --> 00:39:52,940
support contracts give generously with

00:39:49,190 --> 00:39:54,890
your corporate credit card today with

00:39:52,940 --> 00:39:56,690
your personal credit card I don't don't

00:39:54,890 --> 00:39:58,009
waste your money I mean definitely buy

00:39:56,690 --> 00:39:59,900
like one copy but if you have a

00:39:58,009 --> 00:40:01,670
corporate card it's a great gift for

00:39:59,900 --> 00:40:06,230
customers and people who don't want to

00:40:01,670 --> 00:40:08,150
bother returning it I will be at the

00:40:06,230 --> 00:40:10,190
closing panel right after this if anyone

00:40:08,150 --> 00:40:12,739
wants an excuse to go to London and get

00:40:10,190 --> 00:40:15,829
rained on I will be there next week and

00:40:12,739 --> 00:40:18,380
I will also be in Spain if anyone wants

00:40:15,829 --> 00:40:21,170
an excuse to go to a place with a beach

00:40:18,380 --> 00:40:26,059
I is there a beach in Berlin I don't

00:40:21,170 --> 00:40:29,299
think so oh there is is it nice oh yeah

00:40:26,059 --> 00:40:32,150
okay if anyone wants to go to a possibly

00:40:29,299 --> 00:40:35,569
nicer beach you can join me at J on the

00:40:32,150 --> 00:40:37,849
beach and if you want to go to New York

00:40:35,569 --> 00:40:39,739
Scala days New York I will be back in

00:40:37,849 --> 00:40:43,119
Berlin for Faust backstage and Berlin

00:40:39,739 --> 00:40:46,789
buzzwords and Amsterdam is always fun

00:40:43,119 --> 00:40:49,599
Portland less so but still nicer than

00:40:46,789 --> 00:40:53,480
some other places I've been

00:40:49,599 --> 00:40:55,789
let's not name those okay cool um so I

00:40:53,480 --> 00:40:57,049
think I have time for questions okay

00:40:55,789 --> 00:40:59,779
yeah I've got five minutes left for

00:40:57,049 --> 00:41:02,839
questions hopefully this wasn't too

00:40:59,779 --> 00:41:03,529
depressing I'm sorry yeah there's a

00:41:02,839 --> 00:41:06,759
question in the back

00:41:03,529 --> 00:41:06,759
okay hello

00:41:07,370 --> 00:41:10,480
[Applause]

00:41:10,560 --> 00:41:14,530
I should take a picture of the room I

00:41:13,270 --> 00:41:15,819
should have taken a picture of the room

00:41:14,530 --> 00:41:17,560
in the middle before people started

00:41:15,819 --> 00:41:20,230
leaving but I'll take a picture of the

00:41:17,560 --> 00:41:23,410
room now before everyone leaves so that

00:41:20,230 --> 00:41:26,650
it looks like I do good work if you ever

00:41:23,410 --> 00:41:29,680
talk to my boss at Google I am amazing

00:41:26,650 --> 00:41:32,530
and the word is strongly exceed

00:41:29,680 --> 00:41:34,960
expectations that's that's what they

00:41:32,530 --> 00:41:37,540
want to hear in the review do you have

00:41:34,960 --> 00:41:39,880
the microphone yeah thanks for the talk

00:41:37,540 --> 00:41:43,119
first of all and is there any still

00:41:39,880 --> 00:41:46,000
practical use of RDD looks like data

00:41:43,119 --> 00:41:49,859
frame is a clear winner yeah so that's a

00:41:46,000 --> 00:41:54,609
great question and the answer is sort of

00:41:49,859 --> 00:41:56,290
data sets are often better part of that

00:41:54,609 --> 00:41:58,900
comes from having a really cool

00:41:56,290 --> 00:42:02,440
optimizer one of the downsides of having

00:41:58,900 --> 00:42:05,710
a really cool optimizer is sometimes it

00:42:02,440 --> 00:42:10,000
goes on an adventure and this adventure

00:42:05,710 --> 00:42:11,290
takes longer than your actual job and so

00:42:10,000 --> 00:42:13,000
we have this really awesome optimizer

00:42:11,290 --> 00:42:14,500
that occasionally takes longer than your

00:42:13,000 --> 00:42:15,010
work and so if you've got stuff like

00:42:14,500 --> 00:42:19,060
that

00:42:15,010 --> 00:42:20,800
totally rdd's are better actually one of

00:42:19,060 --> 00:42:22,990
the main cases where rdd's make more

00:42:20,800 --> 00:42:25,200
sense despite us building our new

00:42:22,990 --> 00:42:28,660
machine learning api on top of data sets

00:42:25,200 --> 00:42:30,640
is for machine learning because data

00:42:28,660 --> 00:42:32,800
sets don't handle iterative algorithms

00:42:30,640 --> 00:42:36,700
very well and most of machine learning

00:42:32,800 --> 00:42:38,560
is iterative algorithms so we did a

00:42:36,700 --> 00:42:40,839
really good job with the design decision

00:42:38,560 --> 00:42:43,540
there and there's an open juror to fix

00:42:40,839 --> 00:42:48,520
that issue and it's been open for a few

00:42:43,540 --> 00:42:53,230
years so yeah we're getting right on

00:42:48,520 --> 00:42:57,849
that cool so yeah machine learning and

00:42:53,230 --> 00:42:59,680
some other weird cases question so well

00:42:57,849 --> 00:43:01,180
first of all I want to clarify that I

00:42:59,680 --> 00:43:03,790
think you've probably seen a fakie

00:43:01,180 --> 00:43:06,910
finite PyCon I'm really loyal to the

00:43:03,790 --> 00:43:07,990
scala community okay listen it's fine

00:43:06,910 --> 00:43:10,630
you could have been doing reconnaissance

00:43:07,990 --> 00:43:13,810
i i completely understand and respect

00:43:10,630 --> 00:43:19,079
that and I of course love both Python

00:43:13,810 --> 00:43:19,079
and Scala equally I like scallop better

00:43:19,840 --> 00:43:31,300
oh sorry um so my question is spark also

00:43:26,830 --> 00:43:32,920
offers nice functions for you to do

00:43:31,300 --> 00:43:35,740
machine learning so when do you

00:43:32,920 --> 00:43:37,390
recommend a user to use spark for

00:43:35,740 --> 00:43:39,670
machine learning when do you recommend

00:43:37,390 --> 00:43:41,260
them to use tensorflow or other tools

00:43:39,670 --> 00:43:45,580
for machine learning that's awkward

00:43:41,260 --> 00:43:47,140
because my employer is a glorious

00:43:45,580 --> 00:43:50,110
employer who can do no wrong but is

00:43:47,140 --> 00:43:51,910
occasionally confusing produces

00:43:50,110 --> 00:43:57,520
tensorflow along with the help of the

00:43:51,910 --> 00:44:01,420
community and other valued partners so

00:43:57,520 --> 00:44:03,430
might might sorry my personal view is if

00:44:01,420 --> 00:44:05,200
like tensorflow can be really great for

00:44:03,430 --> 00:44:08,500
transfer learning if you have terabytes

00:44:05,200 --> 00:44:11,470
of data and you're not really sure

00:44:08,500 --> 00:44:14,110
what's going on tensorflow can be really

00:44:11,470 --> 00:44:17,140
awesome I think for most machine

00:44:14,110 --> 00:44:20,170
learning use cases transfer learning is

00:44:17,140 --> 00:44:21,580
perhaps a bit oversold like if I'm if

00:44:20,170 --> 00:44:23,080
I'm doing image recognition and I'm

00:44:21,580 --> 00:44:26,620
looking for dogs and I have a thing that

00:44:23,080 --> 00:44:29,220
can find cats okay if I'm trying to go

00:44:26,620 --> 00:44:33,610
from a thing that can find cats to fraud

00:44:29,220 --> 00:44:35,980
yeah maybe not so good and so I think

00:44:33,610 --> 00:44:39,910
tensorflow for now is is a very specific

00:44:35,980 --> 00:44:41,200
use case and SPARC ml is whenever you

00:44:39,910 --> 00:44:42,640
have too much data to fit on a single

00:44:41,200 --> 00:44:44,860
machine and you want to do some cool

00:44:42,640 --> 00:44:46,080
machine learning with it that is not

00:44:44,860 --> 00:44:48,970
deep learning

00:44:46,080 --> 00:44:50,170
although the SPARC ml people will be

00:44:48,970 --> 00:44:52,600
happy to sell you a deep learning

00:44:50,170 --> 00:44:57,250
solution from one of their valued

00:44:52,600 --> 00:44:58,830
partners I'm not one of the valued

00:44:57,250 --> 00:45:02,170
partners though so like I don't want to

00:44:58,830 --> 00:45:06,130
sorry okay I come here to learn a bit

00:45:02,170 --> 00:45:08,320
about spark so I heard a lot of things

00:45:06,130 --> 00:45:10,240
which are not good and spark so my

00:45:08,320 --> 00:45:10,540
conclusion is if you want to do cool

00:45:10,240 --> 00:45:13,240
stuff

00:45:10,540 --> 00:45:15,970
don't do this spark it's just a message

00:45:13,240 --> 00:45:19,000
that's that's my takeaway today so I'm

00:45:15,970 --> 00:45:21,700
sorry confused so what it is should I

00:45:19,000 --> 00:45:22,330
use yes not obviously should I go to a

00:45:21,700 --> 00:45:25,360
fling

00:45:22,330 --> 00:45:26,560
oh no no okay so I want to be clear like

00:45:25,360 --> 00:45:28,150
these are things which are hard and

00:45:26,560 --> 00:45:30,420
spark but don't worry everything is

00:45:28,150 --> 00:45:30,420
terrible

00:45:30,660 --> 00:45:35,410
no I like I'm serious like everything

00:45:33,550 --> 00:45:37,420
sucks if you go and find like some

00:45:35,410 --> 00:45:39,910
hardcore flink users and you sit down

00:45:37,420 --> 00:45:42,760
with them and you're like what is rough

00:45:39,910 --> 00:45:45,160
they will not shut up I mean neither

00:45:42,760 --> 00:45:47,680
will I for spark right like they're

00:45:45,160 --> 00:45:50,560
these are these are rough points right

00:45:47,680 --> 00:45:54,730
and we we can fix some of them together

00:45:50,560 --> 00:45:56,320
hopefully oh wait yes okay they're there

00:45:54,730 --> 00:45:57,520
you're gonna write that PR it's going to

00:45:56,320 --> 00:45:59,770
be amazing

00:45:57,520 --> 00:46:01,060
and the other parts we can avoid right

00:45:59,770 --> 00:46:02,860
like if you need to do some cool

00:46:01,060 --> 00:46:05,260
functional programming for now you can

00:46:02,860 --> 00:46:07,510
use the RDD api and it'll be a little

00:46:05,260 --> 00:46:13,870
slower but that's alright i'm a cloud

00:46:07,510 --> 00:46:15,640
provider just buy more computers I have

00:46:13,870 --> 00:46:16,570
a bank weekend you can borrow the money

00:46:15,640 --> 00:46:22,810
that's great

00:46:16,570 --> 00:46:28,140
I love banks here your credit cards very

00:46:22,810 --> 00:46:28,140
rarely have charge backs please buy more

00:46:28,860 --> 00:46:33,460
so thank you very much I'm afraid we

00:46:31,690 --> 00:46:35,410
have to close it note so you can get to

00:46:33,460 --> 00:46:37,330
the panel yeah sorry about that

00:46:35,410 --> 00:46:39,070
so yeah you will be at the panel but you

00:46:37,330 --> 00:46:40,600
need some time to get over there and

00:46:39,070 --> 00:46:42,300
well yeah I have to change out of these

00:46:40,600 --> 00:46:44,840
yeah

00:46:42,300 --> 00:46:45,160
thank you very much was a great

00:46:44,840 --> 00:46:48,260
[Applause]

00:46:45,160 --> 00:46:48,260

YouTube URL: https://www.youtube.com/watch?v=-8YMkjb0Oxk


