Title: Differentiable Functional Programming by Noel Welsh
Publication date: 2018-09-20
Playlist: Scala Days Berlin 2018
Description: 
	This video was recorded at Scala Days Berlin 2018
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

More information and the abstract can be found here:
https://eu.scaladays.org/lect-6943-differentiable-functional-programming.html
Captions: 
	00:00:04,610 --> 00:00:09,710
so thank you all for coming along to my

00:00:06,529 --> 00:00:11,959
talk when we got that post lunch lull I

00:00:09,710 --> 00:00:14,389
try to keep you awake at least if not

00:00:11,959 --> 00:00:16,910
entertained for the next 40 minutes or

00:00:14,389 --> 00:00:18,080
so my name is Noah Welsh I'm a

00:00:16,910 --> 00:00:21,080
consultant at under school and I'm

00:00:18,080 --> 00:00:23,240
school we do Scala development training

00:00:21,080 --> 00:00:27,050
mentoring that sort of thing but this

00:00:23,240 --> 00:00:29,900
talk is a bit about something else so my

00:00:27,050 --> 00:00:31,099
goals to talk I ready to show that let's

00:00:29,900 --> 00:00:34,730
talk about deep learning I want to show

00:00:31,099 --> 00:00:36,890
that deep learning isn't so complicated

00:00:34,730 --> 00:00:39,260
isn't so hard at least not from a

00:00:36,890 --> 00:00:40,430
computational point of view from the

00:00:39,260 --> 00:00:42,409
point of view of why it does what it

00:00:40,430 --> 00:00:45,379
does I don't think anybody knows yet but

00:00:42,409 --> 00:00:50,420
at least if you want to implement it it

00:00:45,379 --> 00:00:51,890
isn't that difficult I found when I

00:00:50,420 --> 00:00:53,210
looked into deep learning so something I

00:00:51,890 --> 00:00:55,220
was interesting I want to learn more

00:00:53,210 --> 00:00:56,989
about that is very similar to functional

00:00:55,220 --> 00:00:59,119
programming it really is all about

00:00:56,989 --> 00:01:00,470
function composition and some very

00:00:59,119 --> 00:01:01,940
familiar concepts I hopefully you'll

00:01:00,470 --> 00:01:04,280
come away thinking that it's something

00:01:01,940 --> 00:01:05,409
you too can understand and it's

00:01:04,280 --> 00:01:08,119
something you could implement yourself

00:01:05,409 --> 00:01:09,740
in a few days if you wanted to not a

00:01:08,119 --> 00:01:11,120
production system necessarily but

00:01:09,740 --> 00:01:13,400
certainly a toy system that you can have

00:01:11,120 --> 00:01:16,160
some fun with it's very easy to get

00:01:13,400 --> 00:01:17,690
going with so these talk is really is to

00:01:16,160 --> 00:01:19,340
talk about the computational aspects of

00:01:17,690 --> 00:01:21,830
deep learning let's talk about some

00:01:19,340 --> 00:01:23,990
implementation aspects I'm assuming that

00:01:21,830 --> 00:01:25,580
you don't know huge amount about that so

00:01:23,990 --> 00:01:27,650
we're going over some fairly basic stuff

00:01:25,580 --> 00:01:29,810
and it's not really talked about the

00:01:27,650 --> 00:01:31,540
sort of practice of doing deep learning

00:01:29,810 --> 00:01:34,250
or using deep learning production

00:01:31,540 --> 00:01:37,700
hopefully you saw the talk yesterday if

00:01:34,250 --> 00:01:39,680
you're interested in that aspect so

00:01:37,700 --> 00:01:42,260
let's start off by saying what is deep

00:01:39,680 --> 00:01:44,320
learning this is what deep learning is

00:01:42,260 --> 00:01:46,640
all about plenty fun pictures

00:01:44,320 --> 00:01:47,930
not really but these are some of the

00:01:46,640 --> 00:01:50,180
examples of things you can do with deep

00:01:47,930 --> 00:01:51,979
learning the picture on the left is an

00:01:50,180 --> 00:01:55,970
example of style transfer we have taken

00:01:51,979 --> 00:01:58,040
a picture of a statue and it may look

00:01:55,970 --> 00:02:00,320
kind of stylized using a style of a

00:01:58,040 --> 00:02:03,229
actually a piece of marble and the other

00:02:00,320 --> 00:02:04,909
two pictures are called deep dreams

00:02:03,229 --> 00:02:07,640
which is basically making

00:02:04,909 --> 00:02:09,619
strange-looking pictures out of normal

00:02:07,640 --> 00:02:11,269
pictures so they're quite fun but

00:02:09,619 --> 00:02:13,940
they're also practical aspects is used

00:02:11,269 --> 00:02:15,380
for image recognition all those things

00:02:13,940 --> 00:02:17,510
in Facebook and so forth where it picks

00:02:15,380 --> 00:02:18,480
out who who's in a photo voice

00:02:17,510 --> 00:02:21,030
recognition

00:02:18,480 --> 00:02:23,250
there's lots of tasks that I can be

00:02:21,030 --> 00:02:24,870
prevalent most things here mobile phone

00:02:23,250 --> 00:02:28,650
now it's probably has some kind deep

00:02:24,870 --> 00:02:30,930
learning in there in some ways so that's

00:02:28,650 --> 00:02:33,900
the the use case of it but what is it

00:02:30,930 --> 00:02:35,519
actually where we say that deep learning

00:02:33,900 --> 00:02:36,870
supervised learning not necessary six

00:02:35,519 --> 00:02:38,459
five but for our case the supervised

00:02:36,870 --> 00:02:41,129
learning cause Prem tries functions by

00:02:38,459 --> 00:02:42,720
gradient descent okay it's quite a few

00:02:41,129 --> 00:02:44,730
buzzwords packed into a sentence there

00:02:42,720 --> 00:02:47,190
so they spent the first section of this

00:02:44,730 --> 00:02:49,590
talk unpacking those buzzwords and

00:02:47,190 --> 00:02:52,140
saying what I mean by supervised

00:02:49,590 --> 00:02:53,640
learning of parameterize functions by

00:02:52,140 --> 00:02:56,459
gradient descent what are we talking

00:02:53,640 --> 00:03:00,209
about here so three things I won't

00:02:56,459 --> 00:03:02,549
discuss parameterize functions this type

00:03:00,209 --> 00:03:04,079
of supervised learning and gradient

00:03:02,549 --> 00:03:07,079
descent

00:03:04,079 --> 00:03:09,840
so parameterize functions let's start

00:03:07,079 --> 00:03:13,290
with a a simple normal function the sine

00:03:09,840 --> 00:03:15,120
function you give in a value X and you

00:03:13,290 --> 00:03:17,900
get back the sine of X and you can graph

00:03:15,120 --> 00:03:20,400
that and you get something like this

00:03:17,900 --> 00:03:22,859
okay very familiar to us all right it's

00:03:20,400 --> 00:03:25,620
just a it's a function we like functions

00:03:22,859 --> 00:03:27,209
in functional programming now one of the

00:03:25,620 --> 00:03:29,340
things you can do this for the sine

00:03:27,209 --> 00:03:31,530
function is you can change its shape so

00:03:29,340 --> 00:03:33,840
perhaps you could change the period and

00:03:31,530 --> 00:03:36,840
you can make this a parameter so you

00:03:33,840 --> 00:03:37,440
give me a parameter and you get back a

00:03:36,840 --> 00:03:39,540
function

00:03:37,440 --> 00:03:42,989
give me the period and I'll give you

00:03:39,540 --> 00:03:45,209
back a sign for that particular period

00:03:42,989 --> 00:03:48,180
that you've given me so instead of just

00:03:45,209 --> 00:03:49,560
having one function we now have a family

00:03:48,180 --> 00:03:52,620
of functions which are related by

00:03:49,560 --> 00:03:55,169
changing this parameter the period so we

00:03:52,620 --> 00:03:58,260
can make the period bigger and we get

00:03:55,169 --> 00:03:59,790
the slower slowly moving orange line and

00:03:58,260 --> 00:04:06,209
we can make the period smaller and we

00:03:59,790 --> 00:04:08,400
get the quickly moving blue line okay so

00:04:06,209 --> 00:04:10,260
that's parameterizing and bio mean by

00:04:08,400 --> 00:04:12,569
memorizing function is basically just a

00:04:10,260 --> 00:04:17,000
function returning a function so i

00:04:12,569 --> 00:04:18,989
concept every roll I hope familiar with

00:04:17,000 --> 00:04:21,840
so moving on to supervised learning

00:04:18,989 --> 00:04:23,699
what's the idea here so this is like the

00:04:21,840 --> 00:04:26,190
learning bidding machine learning you

00:04:23,699 --> 00:04:28,409
know deep learning and the idea is that

00:04:26,190 --> 00:04:30,220
we want to choose these parameters on

00:04:28,409 --> 00:04:32,410
our function to

00:04:30,220 --> 00:04:35,350
fit some data let's have an example we

00:04:32,410 --> 00:04:37,690
mean by fitting let's start some data

00:04:35,350 --> 00:04:40,960
I'm sure you can guess what this looks

00:04:37,690 --> 00:04:43,120
like so I say ok I think this data is

00:04:40,960 --> 00:04:44,950
generated by a sine wave now let me try

00:04:43,120 --> 00:04:48,460
to choose a sine wave that's going to

00:04:44,950 --> 00:04:49,570
generate very similar data so maybe and

00:04:48,460 --> 00:04:51,160
we're going to have our parameterize

00:04:49,570 --> 00:04:53,770
function again and we could be changing

00:04:51,160 --> 00:04:56,530
the period to much create different sine

00:04:53,770 --> 00:05:00,700
waves so by staff this one very short

00:04:56,530 --> 00:05:04,060
period not really good fit make speed a

00:05:00,700 --> 00:05:06,250
little bit bigger it's getting better ok

00:05:04,060 --> 00:05:08,530
that looks pretty good and now we're

00:05:06,250 --> 00:05:12,310
getting a bit too big and that's

00:05:08,530 --> 00:05:14,530
definitely big ok so that's the idea of

00:05:12,310 --> 00:05:17,500
supervised learning adjusting your

00:05:14,530 --> 00:05:21,850
parameters to fit some data that you

00:05:17,500 --> 00:05:24,850
have a typical classification task the

00:05:21,850 --> 00:05:25,930
data could be like labels of saying this

00:05:24,850 --> 00:05:27,850
is a picture of a person this is a

00:05:25,930 --> 00:05:31,000
picture of a dog or so on but this would

00:05:27,850 --> 00:05:33,850
do as a simple example and the final bit

00:05:31,000 --> 00:05:36,520
then is about gradient descent so I

00:05:33,850 --> 00:05:38,350
described the idea of supervised

00:05:36,520 --> 00:05:40,300
learning but actually implements it in

00:05:38,350 --> 00:05:41,620
the computer we need to make it a little

00:05:40,300 --> 00:05:44,860
bit more concrete a little bit more

00:05:41,620 --> 00:05:46,419
formalized so the idea here by gradient

00:05:44,860 --> 00:05:49,600
descent is is one method by which a

00:05:46,419 --> 00:05:54,400
computer can algorithmically choose the

00:05:49,600 --> 00:05:56,800
parameters so if go back to example here

00:05:54,400 --> 00:06:00,640
the first thing we need to do is we need

00:05:56,800 --> 00:06:03,400
to formalize quantify how good is this

00:06:00,640 --> 00:06:05,740
particular choice of the parameters and

00:06:03,400 --> 00:06:09,610
one of the ways we can do that is we can

00:06:05,740 --> 00:06:11,020
say well how far away from the function

00:06:09,610 --> 00:06:13,990
we have generated are all the points in

00:06:11,020 --> 00:06:15,460
the data we get those distances and we

00:06:13,990 --> 00:06:18,130
can add them all up and I gives us a

00:06:15,460 --> 00:06:19,750
number and the smaller that number the

00:06:18,130 --> 00:06:22,720
less far away the points are from the

00:06:19,750 --> 00:06:26,200
function the better the function is the

00:06:22,720 --> 00:06:28,660
bigger it is worse the function is and

00:06:26,200 --> 00:06:33,610
we can plot that and we might end up the

00:06:28,660 --> 00:06:35,290
plot like this so we have here this is

00:06:33,610 --> 00:06:37,000
the very good part I can reach it which

00:06:35,290 --> 00:06:39,010
is great I don't have a laser pointer

00:06:37,000 --> 00:06:41,500
that of course is the function actually

00:06:39,010 --> 00:06:42,610
generated the data that's a really good

00:06:41,500 --> 00:06:44,389
fit and you can see there are a few

00:06:42,610 --> 00:06:49,370
other cases which are fairly

00:06:44,389 --> 00:06:52,190
and they relate to harmonics as you

00:06:49,370 --> 00:06:54,050
might expect what we can see from

00:06:52,190 --> 00:06:56,870
looking at this was if we started

00:06:54,050 --> 00:07:00,229
somewhere intuitively if we just chose a

00:06:56,870 --> 00:07:02,600
period some kind of value and we could

00:07:00,229 --> 00:07:04,370
roll downhill and that would get us to

00:07:02,600 --> 00:07:06,169
somewhere better so we might not end up

00:07:04,370 --> 00:07:08,840
at the best place if we started over

00:07:06,169 --> 00:07:10,910
here we might not end up at this best

00:07:08,840 --> 00:07:12,110
spot but at least we would improve and

00:07:10,910 --> 00:07:14,750
maybe choose some different random

00:07:12,110 --> 00:07:18,850
values of this period roll downhill and

00:07:14,750 --> 00:07:22,039
eventually would find this best one and

00:07:18,850 --> 00:07:23,210
speaking mathematically what we want to

00:07:22,039 --> 00:07:24,800
do is we want to calculate the

00:07:23,210 --> 00:07:27,139
derivative of the it's called a loss

00:07:24,800 --> 00:07:29,510
function which is just that error that

00:07:27,139 --> 00:07:32,330
we'd showed earlier with respect to the

00:07:29,510 --> 00:07:34,880
parameters in this case a period and we

00:07:32,330 --> 00:07:38,150
can write they don't formally that's our

00:07:34,880 --> 00:07:40,580
loss function squared error or the

00:07:38,150 --> 00:07:44,270
distance I know this has a square root

00:07:40,580 --> 00:07:45,289
and we take the derivative of it and we

00:07:44,270 --> 00:07:47,530
get a function that looks like that

00:07:45,289 --> 00:07:51,289
doesn't really matter what the form is

00:07:47,530 --> 00:07:54,380
and from this derivative this derivative

00:07:51,289 --> 00:07:56,330
tells us the gradient so what the

00:07:54,380 --> 00:07:57,919
gradient is basically means is the slope

00:07:56,330 --> 00:08:03,590
the gradient tells us which way is

00:07:57,919 --> 00:08:05,150
downhill so we can roll down there so if

00:08:03,590 --> 00:08:06,440
you've done calculus before you even

00:08:05,150 --> 00:08:07,880
know about gradients if you haven't done

00:08:06,440 --> 00:08:09,440
calculus all you need to know is

00:08:07,880 --> 00:08:11,180
basically the gradient is telling us

00:08:09,440 --> 00:08:14,090
about the slope and then we can choose

00:08:11,180 --> 00:08:16,430
as a direction to roll down the final

00:08:14,090 --> 00:08:17,840
thing we need to do is to say exactly

00:08:16,430 --> 00:08:20,810
how are we going to roll down this

00:08:17,840 --> 00:08:22,220
gradient down this slope and all we're

00:08:20,810 --> 00:08:24,740
going to do is just make a little change

00:08:22,220 --> 00:08:27,440
to the parameters in this downward

00:08:24,740 --> 00:08:28,610
direction that we've calculated so we

00:08:27,440 --> 00:08:29,120
might never something that looks like

00:08:28,610 --> 00:08:31,340
this

00:08:29,120 --> 00:08:33,260
let's say we are started at the white

00:08:31,340 --> 00:08:34,940
point at the top there happen to be a

00:08:33,260 --> 00:08:38,209
lucky choice of parameters ended up

00:08:34,940 --> 00:08:40,669
getting us to the minimum and we move

00:08:38,209 --> 00:08:42,169
downhill a little bit count out the

00:08:40,669 --> 00:08:44,300
gradient again move downhill a little

00:08:42,169 --> 00:08:47,660
bit more and so on until we eventually

00:08:44,300 --> 00:08:49,640
end up at the bottom there just by

00:08:47,660 --> 00:08:52,130
making these small adjustments and

00:08:49,640 --> 00:08:55,060
that's an algorithm known as stochastic

00:08:52,130 --> 00:08:55,060
gradient descent

00:08:55,880 --> 00:09:00,600
there are lots of wrinkles you can put

00:08:58,530 --> 00:09:01,740
on this momentum terms and mini batching

00:09:00,600 --> 00:09:04,560
and all this kind of stuff that you can

00:09:01,740 --> 00:09:06,960
do but this is just the essential idea

00:09:04,560 --> 00:09:09,770
just rolling downhill anyone can do that

00:09:06,960 --> 00:09:13,110
especially we had a few things to drink

00:09:09,770 --> 00:09:16,980
so now we know what supervisor learning

00:09:13,110 --> 00:09:20,910
of parameterize functions is let's move

00:09:16,980 --> 00:09:21,510
back to deep learning so so deep

00:09:20,910 --> 00:09:22,890
learning supervised learning

00:09:21,510 --> 00:09:26,760
parameterize functions by gradient

00:09:22,890 --> 00:09:30,180
descent now one of the things that we

00:09:26,760 --> 00:09:32,460
have to choose is which actual functions

00:09:30,180 --> 00:09:35,280
are we going to use so I showed the

00:09:32,460 --> 00:09:37,050
example there of a sine wave and it just

00:09:35,280 --> 00:09:39,240
so happened that the sine wave fit very

00:09:37,050 --> 00:09:40,980
nicely the data that we had but when we

00:09:39,240 --> 00:09:43,530
were working on things like images or

00:09:40,980 --> 00:09:45,600
speech data like that it's not so clear

00:09:43,530 --> 00:09:50,910
what the functions we should be using

00:09:45,600 --> 00:09:52,620
are so deep learning is two more things

00:09:50,910 --> 00:09:54,330
it's a choice of functions that is very

00:09:52,620 --> 00:09:57,000
expressive that can model lots of

00:09:54,330 --> 00:10:00,150
different situations and is the choice

00:09:57,000 --> 00:10:02,100
of functions that are suitable to GPU

00:10:00,150 --> 00:10:03,420
acceleration so this kind of this

00:10:02,100 --> 00:10:05,370
software deep learning software

00:10:03,420 --> 00:10:06,720
framework my tensorflow being really

00:10:05,370 --> 00:10:10,950
important for the adoption so I've made

00:10:06,720 --> 00:10:14,220
it accessible and they made it fast so

00:10:10,950 --> 00:10:17,160
what is that choice then choice in most

00:10:14,220 --> 00:10:18,750
deep learning is a tensor operation

00:10:17,160 --> 00:10:21,210
tense multiplication and a non-linearity

00:10:18,750 --> 00:10:23,720
so I choose two words they're two

00:10:21,210 --> 00:10:27,780
phrases tensor non-linearity

00:10:23,720 --> 00:10:29,640
non-linearity let's unpack those tensor

00:10:27,780 --> 00:10:32,820
is a fancy word for a multi-dimensional

00:10:29,640 --> 00:10:35,040
array so just like you impress your

00:10:32,820 --> 00:10:35,880
friends with modern in the pub huh I was

00:10:35,040 --> 00:10:39,540
working my lab today

00:10:35,880 --> 00:10:41,430
now look at you with all I'm sure you

00:10:39,540 --> 00:10:42,750
can now impressive the word tensor it

00:10:41,430 --> 00:10:46,130
just means basically a multi-dimensional

00:10:42,750 --> 00:10:49,020
right so we have vectors and matrices

00:10:46,130 --> 00:10:50,880
you might know if one or two dimensional

00:10:49,020 --> 00:10:55,710
you have more dimensions you call them

00:10:50,880 --> 00:10:58,160
tensors so here is example of a tense

00:10:55,710 --> 00:11:01,530
operation a vector matrix or

00:10:58,160 --> 00:11:03,480
modification of which the tensor EIJ in

00:11:01,530 --> 00:11:06,120
the more general form this what is a

00:11:03,480 --> 00:11:08,280
simple example it's easy to draw and the

00:11:06,120 --> 00:11:09,000
idea is you get a vector three

00:11:08,280 --> 00:11:10,920
dimensions you more

00:11:09,000 --> 00:11:13,800
by matrix three by five matrix D and add

00:11:10,920 --> 00:11:16,830
a vector of five dimensions and how this

00:11:13,800 --> 00:11:18,870
you do this particular operation you

00:11:16,830 --> 00:11:20,070
probably learned in school if you don't

00:11:18,870 --> 00:11:21,060
remember or you didn't learn it doesn't

00:11:20,070 --> 00:11:23,510
really matter for the points of this

00:11:21,060 --> 00:11:27,210
talk its case the important points are

00:11:23,510 --> 00:11:29,370
we do this multiplication somehow and we

00:11:27,210 --> 00:11:33,690
have varying dimensions coming in and

00:11:29,370 --> 00:11:35,910
out of here the other thing is that

00:11:33,690 --> 00:11:39,300
these matrices are all made up of

00:11:35,910 --> 00:11:41,550
numbers two dimensional array numbers

00:11:39,300 --> 00:11:43,350
and these weights as they're sometimes

00:11:41,550 --> 00:11:45,870
called numbers are going to be the

00:11:43,350 --> 00:11:48,750
parameters that we adjust in our

00:11:45,870 --> 00:11:50,130
parameterize function so we has an input

00:11:48,750 --> 00:11:52,190
which could be whatever our speech

00:11:50,130 --> 00:11:55,410
signal our image whatever it might be

00:11:52,190 --> 00:11:58,980
the parameters I mean that matrix there

00:11:55,410 --> 00:12:03,210
we get some kind of output the other

00:11:58,980 --> 00:12:05,520
part of this is a non linearity so hey

00:12:03,210 --> 00:12:07,050
it doesn't lots of different ones being

00:12:05,520 --> 00:12:09,660
used all it is got to be something

00:12:07,050 --> 00:12:13,140
that's not a straight line basically if

00:12:09,660 --> 00:12:15,450
there's a straight line then stuff won't

00:12:13,140 --> 00:12:17,070
work but here's a simple example for the

00:12:15,450 --> 00:12:18,960
rectified linear unit that's used very

00:12:17,070 --> 00:12:21,330
commonly anything is a very simple

00:12:18,960 --> 00:12:23,370
function you give it a value X and you

00:12:21,330 --> 00:12:26,460
either get zero if X is less than zero

00:12:23,370 --> 00:12:29,430
so your flat line and then you just get

00:12:26,460 --> 00:12:30,330
a straight line X you get X back so the

00:12:29,430 --> 00:12:31,980
whole function is not a straight line

00:12:30,330 --> 00:12:36,210
there are two straight lines it's

00:12:31,980 --> 00:12:39,510
nonlinear and then we might say okay an

00:12:36,210 --> 00:12:41,640
entire what's called a layer in deep

00:12:39,510 --> 00:12:47,240
learning might be this non-linearity

00:12:41,640 --> 00:12:50,070
this value thing of X the input

00:12:47,240 --> 00:12:54,150
multiplied by the weight matrix of

00:12:50,070 --> 00:12:55,710
tension or whatever it is so that's one

00:12:54,150 --> 00:12:57,660
part of it and then the other bit the

00:12:55,710 --> 00:13:00,390
bit that makes deep learning deep is

00:12:57,660 --> 00:13:02,720
basically just composing many of these

00:13:00,390 --> 00:13:05,780
functions so like so

00:13:02,720 --> 00:13:08,310
dah-dah-dah compar compose that whatever

00:13:05,780 --> 00:13:11,190
so in deep learning it's not uncommon to

00:13:08,310 --> 00:13:15,230
have like 30 40 60 layers as they called

00:13:11,190 --> 00:13:17,040
or 60 functions as we might call them

00:13:15,230 --> 00:13:18,870
but it's nothing particularly

00:13:17,040 --> 00:13:20,250
complicated from a computational point

00:13:18,870 --> 00:13:21,000
of view it's just

00:13:20,250 --> 00:13:22,980
Oh

00:13:21,000 --> 00:13:28,350
bunch of multiplications chained

00:13:22,980 --> 00:13:30,450
together with these nonlinearities okay

00:13:28,350 --> 00:13:33,750
so at this point any questions anybody

00:13:30,450 --> 00:13:36,000
has this okay cool

00:13:33,750 --> 00:13:39,450
let's go on so the deep learning is

00:13:36,000 --> 00:13:41,400
going is what is being called

00:13:39,450 --> 00:13:45,660
differentiable programming now let's

00:13:41,400 --> 00:13:48,000
talk a little about that now so the idea

00:13:45,660 --> 00:13:49,200
is firstly you get software which is

00:13:48,000 --> 00:13:51,150
going to automatically compute the

00:13:49,200 --> 00:13:52,820
derivatives and compile this to your GPU

00:13:51,150 --> 00:13:55,080
code where you can run really quickly

00:13:52,820 --> 00:13:56,460
another thing is that we're finding that

00:13:55,080 --> 00:13:58,050
the what people are doing in deep

00:13:56,460 --> 00:14:00,440
learning is becoming more and more like

00:13:58,050 --> 00:14:02,760
what what programmers do in programming

00:14:00,440 --> 00:14:04,050
so the model is getting more complex and

00:14:02,760 --> 00:14:06,930
we're getting things like conditionals

00:14:04,050 --> 00:14:09,810
and loops this is where you get the turn

00:14:06,930 --> 00:14:11,930
differentiable programming so doing deep

00:14:09,810 --> 00:14:15,120
learning is becoming much more like what

00:14:11,930 --> 00:14:16,650
programmers do you have control flow you

00:14:15,120 --> 00:14:18,060
have your meat your good reusable

00:14:16,650 --> 00:14:20,310
components which are these little layers

00:14:18,060 --> 00:14:21,420
and they have structure in them I think

00:14:20,310 --> 00:14:23,730
is I don't call that combination

00:14:21,420 --> 00:14:25,140
convolutional networks and whatever but

00:14:23,730 --> 00:14:26,580
it's becoming like as a programmer when

00:14:25,140 --> 00:14:28,140
you have like an API and you work in

00:14:26,580 --> 00:14:32,430
that API I want to do this I want to

00:14:28,140 --> 00:14:33,690
convert you know a user name to a email

00:14:32,430 --> 00:14:35,130
address or something like that or

00:14:33,690 --> 00:14:36,540
whatever you might be doing it's just

00:14:35,130 --> 00:14:40,730
cutting the same thing for for deep

00:14:36,540 --> 00:14:43,380
learning practitioners you have these

00:14:40,730 --> 00:14:44,640
libraries of layers that you're using

00:14:43,380 --> 00:14:46,800
and you're building the networks that

00:14:44,640 --> 00:14:53,310
are just using these layers or functions

00:14:46,800 --> 00:14:55,250
as we might just call them all right now

00:14:53,310 --> 00:14:58,710
I want to talk about what is one of the

00:14:55,250 --> 00:15:01,560
core techniques in deep learning this

00:14:58,710 --> 00:15:04,200
idea of automatic differentiation so I

00:15:01,560 --> 00:15:06,360
said at the beginning of the talk deep

00:15:04,200 --> 00:15:08,790
learning is about function composition

00:15:06,360 --> 00:15:10,140
and automatic differentiation so we've

00:15:08,790 --> 00:15:11,730
seen how the function composition comes

00:15:10,140 --> 00:15:13,470
in and let's talk about automatic

00:15:11,730 --> 00:15:14,970
differentiation this is getting the

00:15:13,470 --> 00:15:16,970
software to complete the gradients for

00:15:14,970 --> 00:15:20,580
you

00:15:16,970 --> 00:15:21,930
so I think the first point and the most

00:15:20,580 --> 00:15:23,460
important point I'll be discussing is

00:15:21,930 --> 00:15:25,530
that derivatives are compositional in

00:15:23,460 --> 00:15:27,480
the same way that functions are then

00:15:25,530 --> 00:15:29,400
we'll be looking at ways you can compute

00:15:27,480 --> 00:15:31,650
gradients and we focus in particular on

00:15:29,400 --> 00:15:33,900
two algorithms for mode or to make

00:15:31,650 --> 00:15:34,570
differentiation and fairly briefly

00:15:33,900 --> 00:15:38,050
reverse

00:15:34,570 --> 00:15:40,150
okay so let's look at our composition of

00:15:38,050 --> 00:15:42,580
derivatives this is the core idea that

00:15:40,150 --> 00:15:46,660
really makes this deep learning work in

00:15:42,580 --> 00:15:47,860
my opinion so it functions compose let

00:15:46,660 --> 00:15:49,240
start with functions feel familiar what

00:15:47,860 --> 00:15:50,290
do you mean by function composition

00:15:49,240 --> 00:15:52,900
well we've really seen it and we're

00:15:50,290 --> 00:15:54,970
building up the deep learning models

00:15:52,900 --> 00:15:56,110
function composition is just applying a

00:15:54,970 --> 00:15:58,060
function to the output of another

00:15:56,110 --> 00:15:59,710
function and we can write it if we want

00:15:58,060 --> 00:16:03,520
fancy mathematical symbols as this kind

00:15:59,710 --> 00:16:06,220
of circle or we can think of it as can

00:16:03,520 --> 00:16:07,770
you be applied to the output of F so

00:16:06,220 --> 00:16:11,020
it's something that we do all the time

00:16:07,770 --> 00:16:14,290
now the important part about derivatives

00:16:11,020 --> 00:16:15,940
is they also compose and this if you

00:16:14,290 --> 00:16:18,360
remember from calculus is called the

00:16:15,940 --> 00:16:22,540
chain rule and what this is saying is

00:16:18,360 --> 00:16:24,670
the derivative of two functions stuck

00:16:22,540 --> 00:16:26,650
together compose together is a

00:16:24,670 --> 00:16:28,890
derivative of the outer function applied

00:16:26,650 --> 00:16:30,910
to the output of the inner function

00:16:28,890 --> 00:16:33,070
multiplied by the derivative of the

00:16:30,910 --> 00:16:34,590
inner function and this is really

00:16:33,070 --> 00:16:37,510
important because it means we can

00:16:34,590 --> 00:16:40,090
calculate derivatives separately and

00:16:37,510 --> 00:16:42,220
then combine them together if we

00:16:40,090 --> 00:16:43,570
couldn't do that then deep learning

00:16:42,220 --> 00:16:44,800
wouldn't work because we wouldn't be

00:16:43,570 --> 00:16:46,300
able to build it up from small pieces

00:16:44,800 --> 00:16:49,660
would have to treat the whole thing as

00:16:46,300 --> 00:16:52,770
one giant unit so composition of

00:16:49,660 --> 00:17:00,520
derivatives by the chain rule is the

00:16:52,770 --> 00:17:02,470
whole thing that makes this work okay so

00:17:00,520 --> 00:17:06,130
triggers compose but how do we actually

00:17:02,470 --> 00:17:08,050
calculate the gradients now there are

00:17:06,130 --> 00:17:09,450
different approaches here one is what I

00:17:08,050 --> 00:17:12,490
like to call the mathematicians approach

00:17:09,450 --> 00:17:14,770
probably the thing you learned in school

00:17:12,490 --> 00:17:16,810
I was known as simple of differentiation

00:17:14,770 --> 00:17:18,610
and basically you write down your

00:17:16,810 --> 00:17:21,160
equation you get a whole ton of rules

00:17:18,610 --> 00:17:23,320
here a few of them you probably remember

00:17:21,160 --> 00:17:25,000
a whole bunch more and well maybe I

00:17:23,320 --> 00:17:27,700
remember them I didn't remember them

00:17:25,000 --> 00:17:29,650
until I look them up and you basically

00:17:27,700 --> 00:17:30,790
you look over your equation and you're

00:17:29,650 --> 00:17:32,740
going through the rules finding the

00:17:30,790 --> 00:17:36,310
rules that match and applying them until

00:17:32,740 --> 00:17:37,930
you get a derivative what's nice about

00:17:36,310 --> 00:17:39,640
this is it gives you an exact solution

00:17:37,930 --> 00:17:41,620
it's also nice that it'll answer

00:17:39,640 --> 00:17:46,030
questions on your calculus exam if you

00:17:41,620 --> 00:17:47,640
did such things the problem with it is

00:17:46,030 --> 00:17:50,860
that

00:17:47,640 --> 00:17:52,780
you can only use record analytic

00:17:50,860 --> 00:17:55,450
equations things you can write down that

00:17:52,780 --> 00:17:57,160
mathematicians accept so no loops no

00:17:55,450 --> 00:17:58,450
conditionals none of the fun programming

00:17:57,160 --> 00:17:59,800
stuff you have to have the whole sort of

00:17:58,450 --> 00:18:04,150
equated and written down they're not

00:17:59,800 --> 00:18:06,160
really a compositional technique the

00:18:04,150 --> 00:18:07,300
other problem is that the size of the

00:18:06,160 --> 00:18:09,370
derivative is going to grow

00:18:07,300 --> 00:18:11,620
exponentially in the size of the

00:18:09,370 --> 00:18:13,480
equation you started with and that's

00:18:11,620 --> 00:18:14,320
because of things like the chain rule

00:18:13,480 --> 00:18:16,660
when you see the chain rule you

00:18:14,320 --> 00:18:19,530
introduced two derivatives and the

00:18:16,660 --> 00:18:22,660
original function is in there as well so

00:18:19,530 --> 00:18:26,020
it's getting bigger and bigger so it's

00:18:22,660 --> 00:18:27,370
not so great now let's look at the the

00:18:26,020 --> 00:18:29,260
computer scientist approach I shouldn't

00:18:27,370 --> 00:18:30,730
call the programmer to give it more more

00:18:29,260 --> 00:18:32,830
status the computer scientist approach

00:18:30,730 --> 00:18:34,900
the Causby to hack everything up and

00:18:32,830 --> 00:18:36,130
just do it numerically so you're saying

00:18:34,900 --> 00:18:38,020
well I've got a function that takes a

00:18:36,130 --> 00:18:40,330
double I'm gonna take a double and add a

00:18:38,020 --> 00:18:42,640
little a little amount to it and see

00:18:40,330 --> 00:18:45,070
what what do I get I take the difference

00:18:42,640 --> 00:18:46,390
divide by that little amount and that's

00:18:45,070 --> 00:18:51,160
going to give me some approximation to

00:18:46,390 --> 00:18:52,750
the gradient a numerical approach the

00:18:51,160 --> 00:18:54,400
great thing about this is it works with

00:18:52,750 --> 00:18:56,020
any function right you can stick a

00:18:54,400 --> 00:18:59,860
double in you can stick a double that's

00:18:56,020 --> 00:19:03,010
been changed by a small amount the

00:18:59,860 --> 00:19:04,510
problem with it is choosing this small

00:19:03,010 --> 00:19:07,000
amount you're changing that double byte

00:19:04,510 --> 00:19:09,760
is difficult and the error can vary

00:19:07,000 --> 00:19:11,320
wildly depending on your choice is it

00:19:09,760 --> 00:19:13,030
something like a sweet spot which

00:19:11,320 --> 00:19:14,350
already depends on the function so it

00:19:13,030 --> 00:19:16,750
depends a lot on that actually how the

00:19:14,350 --> 00:19:19,660
numeric computation is being done and

00:19:16,750 --> 00:19:21,930
it's not clear what that value should be

00:19:19,660 --> 00:19:24,930
until you've done some experimentation

00:19:21,930 --> 00:19:27,040
so it's not the best way of doing things

00:19:24,930 --> 00:19:28,240
what is the best way of doing things is

00:19:27,040 --> 00:19:30,280
this technique called automatic

00:19:28,240 --> 00:19:34,240
differentiation and it's kind of a

00:19:30,280 --> 00:19:37,170
combination of the symbolic approach and

00:19:34,240 --> 00:19:40,180
that it uses as a base these symbolic

00:19:37,170 --> 00:19:41,830
derivatives and it's also got a bit of

00:19:40,180 --> 00:19:43,810
the numeric approach it exists working

00:19:41,830 --> 00:19:45,970
out the derivative at a particular point

00:19:43,810 --> 00:19:48,400
not giving an equation for the

00:19:45,970 --> 00:19:53,260
derivative which the symbolic approach

00:19:48,400 --> 00:19:54,490
does so two main algorithms for in

00:19:53,260 --> 00:19:56,770
Reverse and it gives you an exact

00:19:54,490 --> 00:19:59,950
solution and I work with any function so

00:19:56,770 --> 00:20:00,410
it's quite nice so let's have a look at

00:19:59,950 --> 00:20:03,190
the

00:20:00,410 --> 00:20:08,480
ford mode which is the simplest way to

00:20:03,190 --> 00:20:10,400
implement this the big idea here is we

00:20:08,480 --> 00:20:14,300
calculate with what are called dual

00:20:10,400 --> 00:20:16,220
numbers what's interesting about your

00:20:14,300 --> 00:20:18,350
numbers they basically define like this

00:20:16,220 --> 00:20:20,500
they are very kind of similar in the

00:20:18,350 --> 00:20:23,630
idea to complex numbers of their algebra

00:20:20,500 --> 00:20:26,540
a little bit different and they're also

00:20:23,630 --> 00:20:28,970
kind of similar to if you ever did like

00:20:26,540 --> 00:20:31,160
naive calculus with infinite decimals

00:20:28,970 --> 00:20:34,340
they work in a very similar way and I'm

00:20:31,160 --> 00:20:35,990
surprising they actually work so we got

00:20:34,340 --> 00:20:38,960
what we do is we're going to represent a

00:20:35,990 --> 00:20:40,760
number a number a with having another

00:20:38,960 --> 00:20:43,190
little component which sort of tags

00:20:40,760 --> 00:20:46,040
along with it which is going to be this

00:20:43,190 --> 00:20:48,770
epsilon the epsilon term which

00:20:46,040 --> 00:20:50,090
represents the derivative of let me have

00:20:48,770 --> 00:20:53,090
this rule whenever you multiply two

00:20:50,090 --> 00:20:56,090
epsilon terms together that you get zero

00:20:53,090 --> 00:20:59,930
so you can represent that in code like

00:20:56,090 --> 00:21:04,060
so V is the value the point actual

00:20:59,930 --> 00:21:07,130
double and D is the derivative so far

00:21:04,060 --> 00:21:09,290
then we add in rules our basic

00:21:07,130 --> 00:21:10,670
operations and our functions and these

00:21:09,290 --> 00:21:14,600
are just the same rules that used in

00:21:10,670 --> 00:21:16,400
like the symbolic differentiation so for

00:21:14,600 --> 00:21:18,500
addition it works just like addition you

00:21:16,400 --> 00:21:20,210
add two numbers together you add the

00:21:18,500 --> 00:21:23,950
kind of a number bit for the real bit

00:21:20,210 --> 00:21:27,920
and you add the derivative bits together

00:21:23,950 --> 00:21:30,770
that like so add the values add the

00:21:27,920 --> 00:21:32,240
derivative multiplication it just

00:21:30,770 --> 00:21:33,770
multiplies out and we apply that rule

00:21:32,240 --> 00:21:36,980
that when you get epsilon squared that's

00:21:33,770 --> 00:21:40,240
0 so that term goes away and if you look

00:21:36,980 --> 00:21:46,250
at the formula of the product rule in

00:21:40,240 --> 00:21:50,710
differentiation it's just this so let me

00:21:46,250 --> 00:21:53,510
kind of code like so and where we go and

00:21:50,710 --> 00:21:56,750
the final bit we need to do is our frame

00:21:53,510 --> 00:21:58,940
the chain rule which says when you've

00:21:56,750 --> 00:22:01,700
got a value and the derivative then you

00:21:58,940 --> 00:22:04,160
get the value of a function at that

00:22:01,700 --> 00:22:07,030
point and the derivative at that point

00:22:04,160 --> 00:22:10,640
multiplied by the derivative of the

00:22:07,030 --> 00:22:12,720
previous step and so here's an

00:22:10,640 --> 00:22:14,490
implementation of that safe

00:22:12,720 --> 00:22:16,860
for the sign function the derivative of

00:22:14,490 --> 00:22:18,300
that is cos and you see we have cos of

00:22:16,860 --> 00:22:20,190
the value multiplied by the previous

00:22:18,300 --> 00:22:25,410
derivative that's basically the chain

00:22:20,190 --> 00:22:26,370
rule wrapped up in there all right so if

00:22:25,410 --> 00:22:28,560
you have a simple function

00:22:26,370 --> 00:22:30,660
let's have sine squared it's just make a

00:22:28,560 --> 00:22:33,330
little bit more complex we can implement

00:22:30,660 --> 00:22:37,170
that like so as you can see it looks

00:22:33,330 --> 00:22:39,180
very similar we'll just run it with a

00:22:37,170 --> 00:22:42,060
jewel number set 1 to be our derivative

00:22:39,180 --> 00:22:44,190
so that we can basically place to start

00:22:42,060 --> 00:22:46,590
with the identity and we get back the

00:22:44,190 --> 00:22:50,670
jewel number saying the value is 0.7 and

00:22:46,590 --> 00:22:52,050
the derivative is 0.9 one of the nice

00:22:50,670 --> 00:22:54,660
things we can do in Scarlets we can

00:22:52,050 --> 00:22:57,090
generalize over functions using doubles

00:22:54,660 --> 00:23:00,480
and using dual numbers we can just say

00:22:57,090 --> 00:23:02,900
define she type classes for these so an

00:23:00,480 --> 00:23:05,760
example in the spy library have this jet

00:23:02,900 --> 00:23:07,650
called jet not sure why though just

00:23:05,760 --> 00:23:09,090
that's another name for jewel number and

00:23:07,650 --> 00:23:11,790
then you can define something like this

00:23:09,090 --> 00:23:15,660
so I method in this case because we

00:23:11,790 --> 00:23:17,910
don't have implicit functions yet and

00:23:15,660 --> 00:23:20,250
then we have a few type classes from spy

00:23:17,910 --> 00:23:22,350
defining operations so trees the

00:23:20,250 --> 00:23:25,950
trigonometric operation the sine and the

00:23:22,350 --> 00:23:30,510
field is giving us the multiplication

00:23:25,950 --> 00:23:32,580
and then because we have implementations

00:23:30,510 --> 00:23:35,370
in spire for that for double those type

00:23:32,580 --> 00:23:38,370
classes we can stick in a double we get

00:23:35,370 --> 00:23:41,550
back just a normal result stick in a jet

00:23:38,370 --> 00:23:43,080
as they're called and we get back the

00:23:41,550 --> 00:23:45,170
result and the derivative which matches

00:23:43,080 --> 00:23:47,880
what we computed in the previous example

00:23:45,170 --> 00:23:50,250
so a very nice technique you might in

00:23:47,880 --> 00:23:51,570
your code if you want to do this allows

00:23:50,250 --> 00:23:52,920
you to choose what you're doing you're

00:23:51,570 --> 00:23:55,320
doing doubles you don't care about

00:23:52,920 --> 00:23:57,290
differentiation you want to use two

00:23:55,320 --> 00:24:02,730
numbers did you care about

00:23:57,290 --> 00:24:04,650
differentiation so for simplicity all

00:24:02,730 --> 00:24:08,370
these examples I've shown so far have

00:24:04,650 --> 00:24:12,600
been using just normal numbers doubles

00:24:08,370 --> 00:24:14,310
reals whenever one-dimensional in the

00:24:12,600 --> 00:24:16,950
real cases for deep learning you need to

00:24:14,310 --> 00:24:21,090
generalize this to tensors to our

00:24:16,950 --> 00:24:24,210
multi-dimensional arrays which is not

00:24:21,090 --> 00:24:25,520
more complex just more tedious so I

00:24:24,210 --> 00:24:27,170
haven't done that here

00:24:25,520 --> 00:24:33,350
examples I won't keep the examples

00:24:27,170 --> 00:24:36,740
relatively clean but my it all goes

00:24:33,350 --> 00:24:38,630
through in the same way right

00:24:36,740 --> 00:24:40,550
one of the things about Ford mode is

00:24:38,630 --> 00:24:42,350
that it scales in the size of the input

00:24:40,550 --> 00:24:45,170
dimension so the dimension of what you

00:24:42,350 --> 00:24:47,120
start with and in a typical deep

00:24:45,170 --> 00:24:48,290
learning context so your input is an

00:24:47,120 --> 00:24:51,230
image that's quite a high dimensional

00:24:48,290 --> 00:24:53,860
thing like if a megapixel images a

00:24:51,230 --> 00:24:55,910
million pixels and each pixel at 2003

00:24:53,860 --> 00:24:58,010
red green and blue you might break him

00:24:55,910 --> 00:25:03,950
red being very three three million

00:24:58,010 --> 00:25:05,450
dimensions or more so often for deep

00:25:03,950 --> 00:25:06,800
learning you take another choice which

00:25:05,450 --> 00:25:09,130
is his reverse mode automatic

00:25:06,800 --> 00:25:09,130
differentiation

00:25:09,250 --> 00:25:14,060
the key insight in Reverse mode is that

00:25:12,650 --> 00:25:16,130
the chain rule doesn't really care which

00:25:14,060 --> 00:25:18,410
order you compute things so here's a

00:25:16,130 --> 00:25:20,840
chain rule again and with ford mode we

00:25:18,410 --> 00:25:22,790
were calculating things from the right

00:25:20,840 --> 00:25:25,490
hand to the left we're saying we've

00:25:22,790 --> 00:25:26,780
already know the derivative of F we've

00:25:25,490 --> 00:25:29,270
calculated a derivative now going to

00:25:26,780 --> 00:25:33,350
multiply it by the derivative of G at

00:25:29,270 --> 00:25:34,160
the value but you can go the other

00:25:33,350 --> 00:25:38,480
direction

00:25:34,160 --> 00:25:39,980
you could go derivative of G multiplied

00:25:38,480 --> 00:25:42,530
by the value of F at the point looking

00:25:39,980 --> 00:25:47,570
at and then multiply by the derivative

00:25:42,530 --> 00:25:49,550
of F and this is inverting the control

00:25:47,570 --> 00:25:51,140
flow because you calculate your result

00:25:49,550 --> 00:25:54,020
going one way and now you need to go

00:25:51,140 --> 00:25:57,200
back the other way to calculate the

00:25:54,020 --> 00:25:59,030
derivative and so to do this you need to

00:25:57,200 --> 00:26:00,980
capture the control flow somehow so the

00:25:59,030 --> 00:26:03,560
way we do that is V R DS continuations

00:26:00,980 --> 00:26:06,730
or we use monads since we love monde ads

00:26:03,560 --> 00:26:09,940
and continuations off of me often use in

00:26:06,730 --> 00:26:12,380
Scala I'll just talk about monads

00:26:09,940 --> 00:26:14,620
the key thing in monad is add the flat

00:26:12,380 --> 00:26:19,130
map now becomes basically the chain rule

00:26:14,620 --> 00:26:21,800
so it looks like at this ad

00:26:19,130 --> 00:26:24,380
what about differentiation tasks and we

00:26:21,800 --> 00:26:26,210
have this value so V is the value just

00:26:24,380 --> 00:26:28,220
like it was in the Ford mode and K is

00:26:26,210 --> 00:26:29,660
now this continued continuation which is

00:26:28,220 --> 00:26:32,510
allow us to go backwards to compute

00:26:29,660 --> 00:26:36,620
things backwards so when we do a flat

00:26:32,510 --> 00:26:38,330
map we construct this continuation which

00:26:36,620 --> 00:26:42,110
is going to go sort of backwards

00:26:38,330 --> 00:26:44,500
that's applying the chain rule there and

00:26:42,110 --> 00:26:48,429
so you construct all of these things

00:26:44,500 --> 00:26:50,570
same old rules that we saw for plus

00:26:48,429 --> 00:26:52,159
multiplication and sign and so on which

00:26:50,570 --> 00:26:54,200
I've just to implement a sign in terms

00:26:52,159 --> 00:26:57,080
of flatmap to show that we can do this

00:26:54,200 --> 00:26:59,899
and then when you at some point you

00:26:57,080 --> 00:27:01,309
after the gradient and it has this big

00:26:59,899 --> 00:27:03,289
chain of functions which are all calling

00:27:01,309 --> 00:27:05,029
each other and you stick a value in the

00:27:03,289 --> 00:27:06,890
top then it goes backwards through the

00:27:05,029 --> 00:27:10,070
control flow versus the control flow

00:27:06,890 --> 00:27:14,409
computing things okay

00:27:10,070 --> 00:27:16,370
it's a little bit more complicated sure

00:27:14,409 --> 00:27:19,070
possibly you don't fully understand at

00:27:16,370 --> 00:27:21,080
this point that's okay there are lots of

00:27:19,070 --> 00:27:23,419
references the main idea is this reverse

00:27:21,080 --> 00:27:26,870
reverse control flow and the fact that

00:27:23,419 --> 00:27:29,779
it's a grism actually exists and what do

00:27:26,870 --> 00:27:31,490
you know when we run it we get the same

00:27:29,779 --> 00:27:32,600
value we did before so just ask the

00:27:31,490 --> 00:27:36,110
gradient we get the great this is

00:27:32,600 --> 00:27:37,580
actually some gradient we did before the

00:27:36,110 --> 00:27:40,309
most important thing about reverse mode

00:27:37,580 --> 00:27:42,230
is that it scales in the size of the

00:27:40,309 --> 00:27:45,260
output dimension and normally in your

00:27:42,230 --> 00:27:46,580
deep learning situation the output is a

00:27:45,260 --> 00:27:47,870
lot smaller than the important input

00:27:46,580 --> 00:27:49,519
might be big image the output could be

00:27:47,870 --> 00:27:51,200
like a little label and number saying

00:27:49,519 --> 00:27:53,899
whether it's a person or a dog or a

00:27:51,200 --> 00:27:57,559
piece of cheese or whatever so that's

00:27:53,899 --> 00:27:59,570
why we prefer reverse mode in most deep

00:27:57,559 --> 00:28:01,250
learning frameworks they afford mode is

00:27:59,570 --> 00:28:03,049
easier to implement and very natural to

00:28:01,250 --> 00:28:08,870
work with if you're not concerned about

00:28:03,049 --> 00:28:10,730
high numbers of dimensions okay so

00:28:08,870 --> 00:28:13,250
that's like the fundamental algorithm

00:28:10,730 --> 00:28:15,279
that works within deep learning now I

00:28:13,250 --> 00:28:18,679
want to talk a little bit more about

00:28:15,279 --> 00:28:24,049
where I see the deep learning going and

00:28:18,679 --> 00:28:25,610
what role Scala may play in that so I

00:28:24,049 --> 00:28:26,779
said that differential programming is

00:28:25,610 --> 00:28:28,250
ought to make differentiation and

00:28:26,779 --> 00:28:30,019
compositional code we've seen that we've

00:28:28,250 --> 00:28:34,730
seen how composition or derivatives

00:28:30,019 --> 00:28:36,529
plays an important part in allowing us

00:28:34,730 --> 00:28:38,029
to do this we've seen the automatic

00:28:36,529 --> 00:28:39,590
differentiation algorithms for computing

00:28:38,029 --> 00:28:44,179
the derivatives and they lean on

00:28:39,590 --> 00:28:45,740
compositionality now one of the other

00:28:44,179 --> 00:28:47,450
big idea is the differential programming

00:28:45,740 --> 00:28:49,610
is we want to treat programming as

00:28:47,450 --> 00:28:51,679
programming and what do we care about in

00:28:49,610 --> 00:28:52,130
programming well some things we care

00:28:51,679 --> 00:28:56,210
about

00:28:52,130 --> 00:28:58,910
Onix how usable is our system sometimes

00:28:56,210 --> 00:29:00,320
we care about correctness as well and we

00:28:58,910 --> 00:29:01,940
also care about performance so let's

00:29:00,320 --> 00:29:05,060
have a little tour see some of the

00:29:01,940 --> 00:29:08,510
things that are coming up here so in

00:29:05,060 --> 00:29:10,880
terms of ergonomics we solve like the

00:29:08,510 --> 00:29:12,590
ford mode differentiation and in the

00:29:10,880 --> 00:29:15,140
reverse mode it was very ergonomic you

00:29:12,590 --> 00:29:17,150
could basically write this and that

00:29:15,140 --> 00:29:20,810
would calculate a derivative if you so

00:29:17,150 --> 00:29:22,520
care to do that so that's a very easy

00:29:20,810 --> 00:29:23,960
system to use it looks just like normal

00:29:22,520 --> 00:29:27,470
programming but you get to compute a

00:29:23,960 --> 00:29:29,770
derivative when you want here is how you

00:29:27,470 --> 00:29:33,530
would express the similar thing in

00:29:29,770 --> 00:29:37,000
tensor flow in Python although the

00:29:33,530 --> 00:29:39,260
bindings for Scala are similar it's

00:29:37,000 --> 00:29:40,610
fairly horrendous I mean this is a very

00:29:39,260 --> 00:29:41,960
small function so you can probably

00:29:40,610 --> 00:29:44,990
understand what's going on there but you

00:29:41,960 --> 00:29:46,640
can see how writing a complicated

00:29:44,990 --> 00:29:52,100
function intensive flow could quickly

00:29:46,640 --> 00:29:54,110
get unpleasant due to this and there's a

00:29:52,100 --> 00:29:55,910
solution here and the solution is better

00:29:54,110 --> 00:29:58,790
dear cells and we can do that very easy

00:29:55,910 --> 00:30:00,680
in Scala for example we've shown that

00:29:58,790 --> 00:30:02,320
just in the previous slides very

00:30:00,680 --> 00:30:04,820
infinite falling back of differentiation

00:30:02,320 --> 00:30:06,440
so I think that's something where most

00:30:04,820 --> 00:30:10,070
EFI we are seeing improvement now and we

00:30:06,440 --> 00:30:11,510
can see seem see more correctness is

00:30:10,070 --> 00:30:15,500
where I think things get really

00:30:11,510 --> 00:30:19,340
interesting and so what are the basic

00:30:15,500 --> 00:30:21,950
correctness requirements a one is that

00:30:19,340 --> 00:30:24,320
the dimensions have to agree when you

00:30:21,950 --> 00:30:26,390
have a three by five matrix you can only

00:30:24,320 --> 00:30:27,740
multiply by vector of dimension three

00:30:26,390 --> 00:30:33,020
and you get out a vector of dimension

00:30:27,740 --> 00:30:34,070
five and these in variant got ahold of

00:30:33,020 --> 00:30:36,440
all throughout your program the

00:30:34,070 --> 00:30:38,870
dimensionality of oliver tensors has to

00:30:36,440 --> 00:30:41,030
match up this is not something as

00:30:38,870 --> 00:30:42,440
typically enforced by the current deep

00:30:41,030 --> 00:30:44,330
learning systems they just had like a

00:30:42,440 --> 00:30:46,070
tensor type which can return serve any

00:30:44,330 --> 00:30:48,560
dimensions this is checked at run time

00:30:46,070 --> 00:30:52,730
which as we know checking the run time

00:30:48,560 --> 00:30:53,960
is a great way to introduce bugs so can

00:30:52,730 --> 00:30:58,340
we do better can you represent these

00:30:53,960 --> 00:31:00,920
types of a compile time the other point

00:30:58,340 --> 00:31:02,540
where types are really important and

00:31:00,920 --> 00:31:03,920
this goes beyond just the dimensionality

00:31:02,540 --> 00:31:05,300
is that each of these dimensions has

00:31:03,920 --> 00:31:08,210
some meaning

00:31:05,300 --> 00:31:10,130
so it's very common again is tensorflow

00:31:08,210 --> 00:31:12,230
example to do operations on are sort of

00:31:10,130 --> 00:31:16,130
by access basis so here I'm reducing

00:31:12,230 --> 00:31:18,410
along axis one what is access one who

00:31:16,130 --> 00:31:20,780
knows you just have to remember should

00:31:18,410 --> 00:31:25,520
actually be reducing cron access to X is

00:31:20,780 --> 00:31:26,840
4 so these primitive api's are just

00:31:25,520 --> 00:31:28,520
leaving this information up to the

00:31:26,840 --> 00:31:31,910
programmer to remember and we know

00:31:28,520 --> 00:31:33,050
that's a great way to make errors so

00:31:31,910 --> 00:31:35,360
what's the solution the solution of

00:31:33,050 --> 00:31:37,130
course is expressive type systems so

00:31:35,360 --> 00:31:40,820
Express the dimensionality in the types

00:31:37,130 --> 00:31:42,500
and give names to those dimensions what

00:31:40,820 --> 00:31:44,330
does it actually mean is one the red

00:31:42,500 --> 00:31:46,370
channel one the green channel one the

00:31:44,330 --> 00:31:47,690
blue channel that's meaningful then I

00:31:46,370 --> 00:31:48,470
can I got much better chance of doing

00:31:47,690 --> 00:31:51,010
the right thing

00:31:48,470 --> 00:31:54,470
and we can use in Scala where we need

00:31:51,010 --> 00:31:56,390
type literals so they called we need

00:31:54,470 --> 00:31:58,850
things like H list but it's all the

00:31:56,390 --> 00:32:00,410
tools are there and here's a project

00:31:58,850 --> 00:32:02,540
that someone is working on a project

00:32:00,410 --> 00:32:04,610
called Nexus which is doing just this

00:32:02,540 --> 00:32:08,300
putting those types in place for a

00:32:04,610 --> 00:32:11,540
better programming experience the final

00:32:08,300 --> 00:32:15,410
thing is performance we need to compile

00:32:11,540 --> 00:32:16,820
the GPU for performance reasons how do

00:32:15,410 --> 00:32:18,470
we do that well the way we do it now as

00:32:16,820 --> 00:32:20,360
we have these embedded DSL this is why

00:32:18,470 --> 00:32:22,100
tensorflow is so awful is because you're

00:32:20,360 --> 00:32:24,350
building up basically its abstract

00:32:22,100 --> 00:32:28,550
syntax tree directly so we can then go

00:32:24,350 --> 00:32:30,230
and walk that the compiler what are we

00:32:28,550 --> 00:32:35,380
going to see in the future where you

00:32:30,230 --> 00:32:37,370
might see languages built for 10 doing

00:32:35,380 --> 00:32:39,230
this type of deep learning stuff

00:32:37,370 --> 00:32:41,480
directly the Swift at the moment right

00:32:39,230 --> 00:32:42,830
now is sort of Swift being integrated in

00:32:41,480 --> 00:32:44,440
the tensor flow where you just write

00:32:42,830 --> 00:32:46,430
normal Swift code and the compiler

00:32:44,440 --> 00:32:50,570
understands how to convert that into

00:32:46,430 --> 00:32:53,330
tensor flow code I'm more interested in

00:32:50,570 --> 00:32:55,730
approaches which don't require sort of

00:32:53,330 --> 00:32:58,160
rebuilding the language completely and

00:32:55,730 --> 00:33:00,260
here I think staging is a great thing

00:32:58,160 --> 00:33:03,980
and so there's another project here by

00:33:00,260 --> 00:33:05,630
tia Crump's actually the student called

00:33:03,980 --> 00:33:08,210
lanten which is building this into scala

00:33:05,630 --> 00:33:10,850
and the idea of staging is you don't

00:33:08,210 --> 00:33:12,590
your program is not one unit which is

00:33:10,850 --> 00:33:16,160
compiler and once you have a program

00:33:12,590 --> 00:33:17,400
which generates a program and so once

00:33:16,160 --> 00:33:19,980
you have that in place

00:33:17,400 --> 00:33:22,230
it's really useful for data science in

00:33:19,980 --> 00:33:23,640
general because often things like the

00:33:22,230 --> 00:33:25,559
dimensions we talked about earlier

00:33:23,640 --> 00:33:27,540
dimension out of your data comes when

00:33:25,559 --> 00:33:28,620
you load the data off the disk you don't

00:33:27,540 --> 00:33:30,390
necessarily know until you start

00:33:28,620 --> 00:33:31,650
processing it so you don't have a

00:33:30,390 --> 00:33:33,000
program that loads the data and says

00:33:31,650 --> 00:33:34,640
okay these are dimensions right the

00:33:33,000 --> 00:33:37,020
program that then uses their dimensions

00:33:34,640 --> 00:33:39,900
or the other thing you can do is you can

00:33:37,020 --> 00:33:43,170
walk over you can walk over your program

00:33:39,900 --> 00:33:45,330
and and generate outputs say for tensor

00:33:43,170 --> 00:33:48,059
flow so I think that's really really

00:33:45,330 --> 00:33:49,200
interesting work so conclusions deep

00:33:48,059 --> 00:33:51,960
learning is about function composition

00:33:49,200 --> 00:33:54,540
and derivatives FP is about a function

00:33:51,960 --> 00:33:56,190
composition nice familiar stuff and we

00:33:54,540 --> 00:33:57,540
can easily express derivatives in this

00:33:56,190 --> 00:34:00,690
competition manner using these two

00:33:57,540 --> 00:34:02,070
algorithms forward and backwards I think

00:34:00,690 --> 00:34:03,120
Scala is got a good chance of being in

00:34:02,070 --> 00:34:04,860
the next generation of deep learning

00:34:03,120 --> 00:34:06,450
systems we're seeing now people are

00:34:04,860 --> 00:34:08,639
ready realizing the shortcomings and

00:34:06,450 --> 00:34:10,260
easting systems but to be honest it's a

00:34:08,639 --> 00:34:12,240
lot of work and nothing happen assigned

00:34:10,260 --> 00:34:13,940
it picks it up and I'm don't think I'm

00:34:12,240 --> 00:34:16,860
the person to pick it up and do it

00:34:13,940 --> 00:34:19,790
anyway there we go thank you for

00:34:16,860 --> 00:34:24,409
attention and have any questions

00:34:19,790 --> 00:34:24,409

YouTube URL: https://www.youtube.com/watch?v=nETDYWAHAfE


