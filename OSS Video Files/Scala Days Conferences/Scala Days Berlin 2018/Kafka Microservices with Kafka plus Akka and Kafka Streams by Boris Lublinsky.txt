Title: Kafka Microservices with Kafka plus Akka and Kafka Streams by Boris Lublinsky
Publication date: 2018-09-20
Playlist: Scala Days Berlin 2018
Description: 
	This video was recorded at Scala Days Berlin 2018
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

Find more information and the abstract here: 
https://eu.scaladays.org/lect-6939-kafka-microservices-with-kafka-plus-akka-and-kafka-streams.html
Captions: 
	00:00:04,890 --> 00:00:11,209
so we're going to talk about streaming

00:00:08,069 --> 00:00:15,660
micro services and the first question is

00:00:11,209 --> 00:00:18,180
worth streaming and as you know the

00:00:15,660 --> 00:00:21,779
majority of the application in general

00:00:18,180 --> 00:00:24,869
the badge but the real life is 3mc and

00:00:21,779 --> 00:00:27,900
we have to adapt to the real life and

00:00:24,869 --> 00:00:30,960
there is another reason why streaming is

00:00:27,900 --> 00:00:32,970
so important is because the majority of

00:00:30,960 --> 00:00:37,440
companies wants to know what is

00:00:32,970 --> 00:00:41,460
happening right now not two days or even

00:00:37,440 --> 00:00:44,129
three hours after the things happened so

00:00:41,460 --> 00:00:47,970
I didn't wrote a book a couple of years

00:00:44,129 --> 00:00:51,479
ago it's a little bit dated but it's

00:00:47,970 --> 00:00:55,350
still very relevant and he is talking

00:00:51,479 --> 00:00:59,909
about different architecture for

00:00:55,350 --> 00:01:03,600
streaming and for fast data so in his

00:00:59,909 --> 00:01:08,520
book he is proposing the overall

00:01:03,600 --> 00:01:13,170
architecture for streaming data and in

00:01:08,520 --> 00:01:14,910
this talk we will only concentrate on a

00:01:13,170 --> 00:01:18,300
couple of things we will talk about

00:01:14,910 --> 00:01:21,150
Kafka as the data backbone and we will

00:01:18,300 --> 00:01:24,870
talk about extremes and Kafka streams

00:01:21,150 --> 00:01:27,810
for implementation of the streaming so

00:01:24,870 --> 00:01:29,790
the first question is why Kafka I mean

00:01:27,810 --> 00:01:35,659
how many people in the audience have

00:01:29,790 --> 00:01:39,530
been using Kafka so far quite a few so

00:01:35,659 --> 00:01:43,040
what is Kafka a lot of people are saying

00:01:39,530 --> 00:01:48,390
Kafka is a messaging system which

00:01:43,040 --> 00:01:53,070
technically is wrong Kafka is a log it's

00:01:48,390 --> 00:01:55,770
a distributed look and in this sense

00:01:53,070 --> 00:01:58,560
it's quite different from the messaging

00:01:55,770 --> 00:02:01,770
system it can be used as a messaging

00:01:58,560 --> 00:02:04,680
system but it will be very wrong to call

00:02:01,770 --> 00:02:06,770
Kafka messaging system and there are

00:02:04,680 --> 00:02:11,159
quite a few differences which are

00:02:06,770 --> 00:02:13,770
important the most important one is in

00:02:11,159 --> 00:02:17,459
messaging system messages are ephemeral

00:02:13,770 --> 00:02:18,569
in Kafka they are stored on disk which

00:02:17,459 --> 00:02:21,450
gives you the cape

00:02:18,569 --> 00:02:24,540
ability of replaying the messages which

00:02:21,450 --> 00:02:29,969
is becoming extremely important the

00:02:24,540 --> 00:02:32,480
second thing is a Kafka created sort of

00:02:29,969 --> 00:02:34,909
revolution in the messaging

00:02:32,480 --> 00:02:37,650
implementation because traditionally a

00:02:34,909 --> 00:02:40,290
messaging system we are keeping track of

00:02:37,650 --> 00:02:43,469
the consumers and this is where the

00:02:40,290 --> 00:02:46,290
limitation on the scalability was Kafka

00:02:43,469 --> 00:02:49,709
is the first one that is not trying to

00:02:46,290 --> 00:02:52,409
keep this track instead every consumer

00:02:49,709 --> 00:02:56,939
is keeping track where in the log he

00:02:52,409 --> 00:03:01,590
ears which basically means that they

00:02:56,939 --> 00:03:04,199
explained the scalability and there can

00:03:01,590 --> 00:03:06,959
be unlimited virtually unlimited amount

00:03:04,199 --> 00:03:10,799
of consumers that can connect to Kafka

00:03:06,959 --> 00:03:13,949
so in Kafka what is happening is the

00:03:10,799 --> 00:03:18,329
producer always writes to the end of the

00:03:13,949 --> 00:03:24,599
log and each consumer is reading from

00:03:18,329 --> 00:03:30,659
the position where he stopped the basic

00:03:24,599 --> 00:03:35,220
thing in Kafka is a topic where in

00:03:30,659 --> 00:03:38,159
reality a topic is not a real thing it

00:03:35,220 --> 00:03:41,970
is a convenience aggregation of multiple

00:03:38,159 --> 00:03:46,019
partitions so the true element of Kafka

00:03:41,970 --> 00:03:48,689
is partition and when people are talking

00:03:46,019 --> 00:03:51,569
about preserving the sequence of the

00:03:48,689 --> 00:03:54,030
messages they are really not topic about

00:03:51,569 --> 00:03:57,389
talking about topics they are talking

00:03:54,030 --> 00:03:59,729
about partitions within the topic within

00:03:57,389 --> 00:04:01,859
the partition the sequence of the

00:03:59,729 --> 00:04:05,280
messages is guaranteed within the

00:04:01,859 --> 00:04:07,919
partition within the topic it's not so

00:04:05,280 --> 00:04:10,310
topic is just nothing else but

00:04:07,919 --> 00:04:16,590
convenience logical aggregation of

00:04:10,310 --> 00:04:20,400
partitions so prior to introduction of

00:04:16,590 --> 00:04:26,580
Kafka and generally prior to

00:04:20,400 --> 00:04:30,810
introduction of messaging this is what

00:04:26,580 --> 00:04:32,070
people had to deal with so if the

00:04:30,810 --> 00:04:35,160
service wanted

00:04:32,070 --> 00:04:38,370
to invoke another service it has to know

00:04:35,160 --> 00:04:43,020
where their other service is located it

00:04:38,370 --> 00:04:45,060
has to know some additional things about

00:04:43,020 --> 00:04:48,360
the service for example what kind of

00:04:45,060 --> 00:04:53,700
requests it accepts is a pure HTTP is a

00:04:48,360 --> 00:04:58,040
RPC something else so basicly without

00:04:53,700 --> 00:04:59,940
having Kafka as a backbone you here the

00:04:58,040 --> 00:05:03,150
web woof

00:04:59,940 --> 00:05:06,870
at heart connections and it's very hard

00:05:03,150 --> 00:05:10,020
to try to figure out what's going on so

00:05:06,870 --> 00:05:13,790
by introducing Kafka this allows to

00:05:10,020 --> 00:05:16,650
simplify the connectivity quite a bit

00:05:13,790 --> 00:05:20,520
because all the messages are going

00:05:16,650 --> 00:05:22,920
through the Kafka and to be more precise

00:05:20,520 --> 00:05:26,550
I'm talking about Kafka but in reality

00:05:22,920 --> 00:05:29,250
it can be carved it can be a privada it

00:05:26,550 --> 00:05:31,290
can be Apache distributed log they're

00:05:29,250 --> 00:05:34,400
all provide similar kind of

00:05:31,290 --> 00:05:38,970
functionality and similar kind of api's

00:05:34,400 --> 00:05:41,910
so why is it important it's better

00:05:38,970 --> 00:05:44,550
decoupling because now you have a

00:05:41,910 --> 00:05:46,740
complete and plural decoupling I because

00:05:44,550 --> 00:05:51,240
Kafka allows you to store messages

00:05:46,740 --> 00:05:53,820
persistently so if your recipient will

00:05:51,240 --> 00:05:56,910
go down it doesn't really matter too

00:05:53,820 --> 00:05:59,610
much because the messages are going to

00:05:56,910 --> 00:06:01,560
be stored in the log and when he will

00:05:59,610 --> 00:06:04,260
come back up

00:06:01,560 --> 00:06:07,500
he will start receiving these messages

00:06:04,260 --> 00:06:11,070
it are simplified dependencies because

00:06:07,500 --> 00:06:13,470
in this case your provider doesn't

00:06:11,070 --> 00:06:16,920
really care who the consumer so I he

00:06:13,470 --> 00:06:19,170
writes to the log and you can add any

00:06:16,920 --> 00:06:22,320
amount of consumers nothing is going to

00:06:19,170 --> 00:06:26,940
change and of course there is a

00:06:22,320 --> 00:06:30,770
simplicity of a single API is for

00:06:26,940 --> 00:06:30,770
majority of communications

00:06:33,430 --> 00:06:40,240
so I let's talk a little bit about okay

00:06:38,080 --> 00:06:43,090
so this is crafter's a bad boy now we

00:06:40,240 --> 00:06:46,259
can start talking about streaming

00:06:43,090 --> 00:06:49,620
engines and there are two approaches

00:06:46,259 --> 00:06:52,930
traditional approach is based on

00:06:49,620 --> 00:06:56,860
MapReduce that became extremely popular

00:06:52,930 --> 00:06:58,780
with the big data and has resolved all

00:06:56,860 --> 00:07:03,009
the fewest implementations of the

00:06:58,780 --> 00:07:07,000
streaming engines for example storm for

00:07:03,009 --> 00:07:11,590
example spark streaming kind of bringing

00:07:07,000 --> 00:07:15,009
the same architecture to streaming so

00:07:11,590 --> 00:07:18,910
this is what is called MapReduce

00:07:15,009 --> 00:07:22,840
trimming styles so stream engines today

00:07:18,910 --> 00:07:28,229
the most popular spark streaming and

00:07:22,840 --> 00:07:34,229
fleeing those are services to which your

00:07:28,229 --> 00:07:37,690
kind of submit your work and they scale

00:07:34,229 --> 00:07:40,599
they do automatic partitioning they do

00:07:37,690 --> 00:07:44,860
failover and recovery they provide a lot

00:07:40,599 --> 00:07:47,440
of things behind the scenes so if we

00:07:44,860 --> 00:07:50,830
look at a spark example what is

00:07:47,440 --> 00:07:52,870
happening is your spark driver talks to

00:07:50,830 --> 00:07:56,370
the cluster manager quarter manager

00:07:52,870 --> 00:08:00,099
decides where to put your executor if

00:07:56,370 --> 00:08:04,090
your scale will start to increase it may

00:08:00,099 --> 00:08:07,690
add it is no amount of executives or you

00:08:04,090 --> 00:08:10,570
might have to manually specify more

00:08:07,690 --> 00:08:13,870
amount of executives in your application

00:08:10,570 --> 00:08:18,009
but it will do all this for you it's

00:08:13,870 --> 00:08:20,889
also do a lot of additional things

00:08:18,009 --> 00:08:23,800
because on the right hand side here's

00:08:20,889 --> 00:08:26,500
how you write your code and now spark

00:08:23,800 --> 00:08:30,009
with its optimizer because it knows the

00:08:26,500 --> 00:08:33,839
full code it will decide what to execute

00:08:30,009 --> 00:08:38,649
where and at which point to do shuffle

00:08:33,839 --> 00:08:41,130
and this is great but it doesn't come

00:08:38,649 --> 00:08:45,910
for free the problem with this approach

00:08:41,130 --> 00:08:46,480
is first of all it becomes the easy of

00:08:45,910 --> 00:08:48,550
the code

00:08:46,480 --> 00:08:50,709
of ownership you have to have a server

00:08:48,550 --> 00:08:54,519
and you have to have a group that

00:08:50,709 --> 00:08:59,430
supports the server it also becomes the

00:08:54,519 --> 00:09:02,050
issue of programming model because every

00:08:59,430 --> 00:09:04,480
server of this type dictates the

00:09:02,050 --> 00:09:07,630
programming model that you have to

00:09:04,480 --> 00:09:11,110
adhere to in order to execute and it's

00:09:07,630 --> 00:09:13,300
wonderful if your application feels well

00:09:11,110 --> 00:09:15,639
into this programming model if it

00:09:13,300 --> 00:09:20,019
doesn't you might be in a big trouble

00:09:15,639 --> 00:09:22,899
it also dictates the deployment model so

00:09:20,019 --> 00:09:25,690
you have to have your server riding you

00:09:22,899 --> 00:09:26,620
have to have to create assembly

00:09:25,690 --> 00:09:28,990
übercharge

00:09:26,620 --> 00:09:34,230
you have to submit your uber jar to the

00:09:28,990 --> 00:09:37,779
server all this becomes not so

00:09:34,230 --> 00:09:39,490
convenient so you have to wait the price

00:09:37,779 --> 00:09:41,649
that you are playing versus the

00:09:39,490 --> 00:09:46,060
advantages that you are getting out of

00:09:41,649 --> 00:09:49,740
this so as a result several years ago

00:09:46,060 --> 00:09:53,440
people started to think about using

00:09:49,740 --> 00:09:58,319
micro services for streaming and one of

00:09:53,440 --> 00:10:03,339
the breakthrough was Jacobs article

00:09:58,319 --> 00:10:07,899
about kafka streams and the whole idea

00:10:03,339 --> 00:10:10,899
behind this Wars there is a certain

00:10:07,899 --> 00:10:12,459
difference between bridge processing and

00:10:10,899 --> 00:10:15,310
streaming processing because bridge

00:10:12,459 --> 00:10:18,639
usually assume a humongous amount of

00:10:15,310 --> 00:10:21,819
data that you have to press to process

00:10:18,639 --> 00:10:24,579
we're in our streaming case typically

00:10:21,819 --> 00:10:28,540
you are dealing with the short small

00:10:24,579 --> 00:10:31,449
amount of data that is relevant for this

00:10:28,540 --> 00:10:35,230
particular moment so people started

00:10:31,449 --> 00:10:37,889
thinking that maybe using application

00:10:35,230 --> 00:10:43,990
server is not always the appropriate

00:10:37,889 --> 00:10:49,149
tool for doing streaming applications so

00:10:43,990 --> 00:10:54,490
we will talk about two most popular

00:10:49,149 --> 00:10:57,010
libraries one of them our disclaimer I

00:10:54,490 --> 00:10:59,920
work for light burn that's why I assume

00:10:57,010 --> 00:11:02,170
that ARCA is the best thing in the world

00:10:59,920 --> 00:11:05,920
at least I'm supposed to think this way

00:11:02,170 --> 00:11:10,329
and there is another very popular

00:11:05,920 --> 00:11:13,839
library which is Kafka streams and I'll

00:11:10,329 --> 00:11:20,399
try to show one example comparing the

00:11:13,839 --> 00:11:24,040
two so there are different

00:11:20,399 --> 00:11:27,609
micro-services here technically they are

00:11:24,040 --> 00:11:32,879
coming from the different are design

00:11:27,609 --> 00:11:35,949
philosophies there is a event-driven I

00:11:32,879 --> 00:11:40,139
philosophy where every element of the

00:11:35,949 --> 00:11:46,029
Attic has its own identity and this

00:11:40,139 --> 00:11:50,519
identity is important because every

00:11:46,029 --> 00:11:50,519
message has to be processed by itself

00:11:51,809 --> 00:12:08,850
very clever animation let me finish this

00:11:54,699 --> 00:12:14,230
first and there is a second type of

00:12:08,850 --> 00:12:17,470
approach that is also popular and this

00:12:14,230 --> 00:12:20,169
is processing messages in bulk where the

00:12:17,470 --> 00:12:23,139
identity of the individual message does

00:12:20,169 --> 00:12:26,829
not matter so much what is important is

00:12:23,139 --> 00:12:37,059
the aggregated data that exists in the

00:12:26,829 --> 00:12:40,149
set of messages so acha streams emerged

00:12:37,059 --> 00:12:43,749
from the event-driven microservices

00:12:40,149 --> 00:12:46,329
because in akka streams every message is

00:12:43,749 --> 00:12:48,789
an individual message and you can route

00:12:46,329 --> 00:12:53,199
it differently you can process it

00:12:48,789 --> 00:12:56,799
differently kafka streams emerge from

00:12:53,199 --> 00:13:00,039
the other side are kind of similar to

00:12:56,799 --> 00:13:03,160
what spark streaming does - what fling

00:13:00,039 --> 00:13:06,519
does are they typically treat not

00:13:03,160 --> 00:13:09,699
individual messages but they treat the

00:13:06,519 --> 00:13:12,249
collection of messaging and messages and

00:13:09,699 --> 00:13:13,040
we'll talk a little bit later about win

00:13:12,249 --> 00:13:15,980
during this

00:13:13,040 --> 00:13:19,370
one of the approaches but what is

00:13:15,980 --> 00:13:23,000
happening in reality these two paradigms

00:13:19,370 --> 00:13:26,630
are merging and technically you can use

00:13:23,000 --> 00:13:29,360
ARCA streams to process bulk and you can

00:13:26,630 --> 00:13:33,319
use Kafka streams alternatively to

00:13:29,360 --> 00:13:38,180
process individual messages so let's

00:13:33,319 --> 00:13:44,290
start from our Kafka streams so Kafka

00:13:38,180 --> 00:13:50,509
streams is fairly new library it hears

00:13:44,290 --> 00:13:55,310
all there the word here is probably not

00:13:50,509 --> 00:13:58,300
important but rather fashionable it has

00:13:55,310 --> 00:14:00,529
all the fashionable stream processing

00:13:58,300 --> 00:14:04,399
semantics are distinguishing between

00:14:00,529 --> 00:14:07,790
even time and processing time winning

00:14:04,399 --> 00:14:11,389
support group within the window I mean

00:14:07,790 --> 00:14:15,440
it's interesting if you look at what is

00:14:11,389 --> 00:14:18,139
happening several years ago Google and

00:14:15,440 --> 00:14:20,839
Apache introduced the application server

00:14:18,139 --> 00:14:23,630
which is called beam and they were

00:14:20,839 --> 00:14:27,260
actually the first one that we are

00:14:23,630 --> 00:14:29,630
defining all the semantics they were the

00:14:27,260 --> 00:14:32,899
first one that came up with distinguish

00:14:29,630 --> 00:14:36,350
me and between the event occurrence time

00:14:32,899 --> 00:14:38,930
and even processing time they were very

00:14:36,350 --> 00:14:42,110
careful about window ink and this is

00:14:38,930 --> 00:14:44,480
extremely important if you are trying to

00:14:42,110 --> 00:14:47,930
use this type of engines to build

00:14:44,480 --> 00:14:50,180
analytical applications several

00:14:47,930 --> 00:14:54,440
conversation that we were trying to help

00:14:50,180 --> 00:14:57,380
with Google glass you will this is great

00:14:54,440 --> 00:15:00,110
but analytical applications are not the

00:14:57,380 --> 00:15:03,069
only one applications that are important

00:15:00,110 --> 00:15:09,100
for streaming so we need to enhance

00:15:03,069 --> 00:15:12,800
these capabilities so the two

00:15:09,100 --> 00:15:17,050
fundamental elements in kafka streams

00:15:12,800 --> 00:15:20,300
are k stream which allow you to record

00:15:17,050 --> 00:15:24,740
transformation and want one mapping and

00:15:20,300 --> 00:15:26,940
it also here's our k table which is a

00:15:24,740 --> 00:15:29,610
key value table

00:15:26,940 --> 00:15:33,210
which allows you to store a value by key

00:15:29,610 --> 00:15:36,960
and is useful for holding a store values

00:15:33,210 --> 00:15:39,660
one of the things the confluent was

00:15:36,960 --> 00:15:42,420
extremely good at is when they have

00:15:39,660 --> 00:15:45,000
introduced these two things they have

00:15:42,420 --> 00:15:48,980
also introduced an extremely important

00:15:45,000 --> 00:15:52,260
notion of our stream table duality which

00:15:48,980 --> 00:15:54,780
obvious to anybody who ever used the

00:15:52,260 --> 00:15:57,510
normal sequel databases because you

00:15:54,780 --> 00:16:02,970
first write to the log and then you have

00:15:57,510 --> 00:16:06,230
the table and they formalize this as

00:16:02,970 --> 00:16:10,670
much as they could and now this becomes

00:16:06,230 --> 00:16:13,500
extremely important notions stream is a

00:16:10,670 --> 00:16:16,530
representation in time of the table and

00:16:13,500 --> 00:16:19,590
table is the collection of the data that

00:16:16,530 --> 00:16:22,470
came through the stream but in all

00:16:19,590 --> 00:16:24,180
fairness they were the first one that we

00:16:22,470 --> 00:16:26,780
were running with this and this became

00:16:24,180 --> 00:16:30,120
very important

00:16:26,780 --> 00:16:34,260
so Kafka streams allows you to read and

00:16:30,120 --> 00:16:37,230
write ooh Kafka topics or in memory or

00:16:34,260 --> 00:16:41,520
in the table this is the nice thing

00:16:37,230 --> 00:16:44,640
about Kafka streams so table api's in

00:16:41,520 --> 00:16:46,680
case we my pies are very similar and

00:16:44,640 --> 00:16:50,310
when you write your code you can mix and

00:16:46,680 --> 00:16:55,200
match declarations on K stream and K

00:16:50,310 --> 00:16:58,760
table he and it does load balancing and

00:16:55,200 --> 00:17:03,750
scale using total partitioning this is

00:16:58,760 --> 00:17:09,060
another important thing that let me stop

00:17:03,750 --> 00:17:12,089
here for a second so the way Kafka works

00:17:09,060 --> 00:17:14,790
is I because Kafka streams is basically

00:17:12,089 --> 00:17:17,640
a Kafka cos humor or Kafka producer so

00:17:14,790 --> 00:17:21,240
it abides by the same rules but the way

00:17:17,640 --> 00:17:24,420
Kafka works is if you have a topic with

00:17:21,240 --> 00:17:27,750
five partitions you can hear five

00:17:24,420 --> 00:17:31,050
consumers each dedicated to a single

00:17:27,750 --> 00:17:34,860
topic due to the single partition of the

00:17:31,050 --> 00:17:38,610
topic if you have less than five then

00:17:34,860 --> 00:17:40,179
what is happening is Kafka will assign

00:17:38,610 --> 00:17:43,360
the some of them more

00:17:40,179 --> 00:17:46,119
non-partisan if you have more than five

00:17:43,360 --> 00:17:48,700
what is going to happen is five are

00:17:46,119 --> 00:17:51,610
going to process each individual

00:17:48,700 --> 00:17:56,679
partition and all the other ones are

00:17:51,610 --> 00:17:59,129
going to be there as hot spares so there

00:17:56,679 --> 00:18:04,720
is this coupling

00:17:59,129 --> 00:18:10,740
kafka streams is leveraging these for

00:18:04,720 --> 00:18:14,700
its scalability Kappa streams are

00:18:10,740 --> 00:18:17,559
confident made a very specific decision

00:18:14,700 --> 00:18:23,259
they had their own API that they are

00:18:17,559 --> 00:18:27,070
providing is Java API and at this point

00:18:23,259 --> 00:18:30,879
it was like burn for the rescue and we

00:18:27,070 --> 00:18:34,509
have created the skullery guys and I

00:18:30,879 --> 00:18:39,059
will talk later today about details of

00:18:34,509 --> 00:18:43,749
this gallery PI's implementation and

00:18:39,059 --> 00:18:49,720
Kafka streams also have the thing which

00:18:43,749 --> 00:18:52,860
is called case equal which yeah I'm film

00:18:49,720 --> 00:18:57,269
right now so I better wash my tongue

00:18:52,860 --> 00:19:01,960
it's kind of a strange thing because

00:18:57,269 --> 00:19:05,559
sequel is widely adopted in many stream

00:19:01,960 --> 00:19:09,070
streaming engines for example spark

00:19:05,559 --> 00:19:11,379
structures dreaming and fleeing but the

00:19:09,070 --> 00:19:14,249
way it works there yes you can use

00:19:11,379 --> 00:19:17,230
sequel is a programming language

00:19:14,249 --> 00:19:18,639
basically sequel allows you to define

00:19:17,230 --> 00:19:22,090
your transformation

00:19:18,639 --> 00:19:25,179
instead of writing code Kafka streams

00:19:22,090 --> 00:19:27,850
decided to implement are there case it

00:19:25,179 --> 00:19:30,129
will slightly differently so you can't

00:19:27,850 --> 00:19:32,889
use sequel in your Kafka streams

00:19:30,129 --> 00:19:35,590
applications k sequel is a separate

00:19:32,889 --> 00:19:39,220
application where you can do sequel and

00:19:35,590 --> 00:19:41,950
nothing else so for me it's a little bit

00:19:39,220 --> 00:19:45,840
strange I would rather see them doing

00:19:41,950 --> 00:19:49,509
the same thing that link is doing and

00:19:45,840 --> 00:19:53,379
we're doing okay on time so flink here's

00:19:49,509 --> 00:19:53,799
this wonderful thing if you know one of

00:19:53,379 --> 00:19:58,960
the big

00:19:53,799 --> 00:20:02,049
just blink users is Alibaba and Alibaba

00:19:58,960 --> 00:20:04,809
basically banned all of the development

00:20:02,049 --> 00:20:06,820
for fling except the sequel so they are

00:20:04,809 --> 00:20:09,549
writing all of their applications in

00:20:06,820 --> 00:20:11,379
sequel this is absolutely amazing we're

00:20:09,549 --> 00:20:14,340
talking about the most modern

00:20:11,379 --> 00:20:16,840
programming languages which is Scala and

00:20:14,340 --> 00:20:19,239
the majority of developers are

00:20:16,840 --> 00:20:22,720
absolutely happy not to know anything

00:20:19,239 --> 00:20:23,559
about this and use 30 years old sequel

00:20:22,720 --> 00:20:27,460
here

00:20:23,559 --> 00:20:29,409
according to Alibaba this is the highest

00:20:27,460 --> 00:20:31,989
productivity that they have average

00:20:29,409 --> 00:20:36,059
shift I'm sorry I'm not doing this on

00:20:31,989 --> 00:20:36,059
purpose it's just amazing to me

00:20:38,129 --> 00:20:47,019
so the example that I will show is the

00:20:43,409 --> 00:20:48,879
example from this time it's my book

00:20:47,019 --> 00:20:51,999
model sorry book

00:20:48,879 --> 00:20:58,899
well I can't used in all the time I have

00:20:51,999 --> 00:21:02,529
to have something of my own and for this

00:20:58,899 --> 00:21:04,509
example we will use of course I work for

00:21:02,529 --> 00:21:08,590
light band we are going to use the new

00:21:04,509 --> 00:21:13,509
Escalade guys that are finally now part

00:21:08,590 --> 00:21:19,590
of Kafka streams distribution and if you

00:21:13,509 --> 00:21:29,159
are interested we also have support in

00:21:19,590 --> 00:21:29,159
Scala and HTTP for queryable state and

00:21:29,639 --> 00:21:40,299
ok so if you want to see the full code

00:21:33,580 --> 00:21:42,850
this is the git repo and we are going to

00:21:40,299 --> 00:21:50,080
give this tutorial next week hopefully

00:21:42,850 --> 00:21:57,669
within this time it Stratton London so

00:21:50,080 --> 00:22:01,299
let's look at the code this is kappa

00:21:57,669 --> 00:22:05,999
stream code which is surprisingly simple

00:22:01,299 --> 00:22:05,999
so first we create the builder

00:22:06,360 --> 00:22:16,270
and this is the newest color api's fresh

00:22:10,480 --> 00:22:25,950
of the press we create the components

00:22:16,270 --> 00:22:29,640
that we need there we do and we

00:22:25,950 --> 00:22:32,800
specified that the data is coming from

00:22:29,640 --> 00:22:41,520
Kafka QC and we are specifying that

00:22:32,800 --> 00:22:45,720
format of the data is byte arrays we are

00:22:41,520 --> 00:22:49,120
doing the model and if you guys know

00:22:45,720 --> 00:22:56,050
spark of link you will see that it's

00:22:49,120 --> 00:22:57,850
very similar functionality rise we do

00:22:56,050 --> 00:23:02,250
the second mapping

00:22:57,850 --> 00:23:06,760
after that we instantiate the streams

00:23:02,250 --> 00:23:10,450
and then we say streams run and we add

00:23:06,760 --> 00:23:14,340
shutdown hook so that it exits cleanly

00:23:10,450 --> 00:23:19,180
that's all it takes to write a simple

00:23:14,340 --> 00:23:23,470
Kafka streams covered so this would be

00:23:19,180 --> 00:23:25,900
wonderful but we have now as we're not

00:23:23,470 --> 00:23:28,120
using the server and we're using library

00:23:25,900 --> 00:23:31,030
we have to deal with all these problems

00:23:28,120 --> 00:23:33,190
what to do with the scalability and I've

00:23:31,030 --> 00:23:35,440
talked a little bit that we can leverage

00:23:33,190 --> 00:23:38,950
the Kafka scalability which is built

00:23:35,440 --> 00:23:43,090
into Kafka would you with failover you

00:23:38,950 --> 00:23:45,490
are completely on your own and would to

00:23:43,090 --> 00:23:50,680
do with restart ability you have to

00:23:45,490 --> 00:23:54,000
write scripts so the moral of this story

00:23:50,680 --> 00:23:57,600
is there is no free lunch you gain more

00:23:54,000 --> 00:23:59,790
flexibility in implementation simplicity

00:23:57,600 --> 00:24:08,410
but you have to worry about

00:23:59,790 --> 00:24:10,990
infrastructure concerns ARCA streams you

00:24:08,410 --> 00:24:13,210
guys have huge multiple presentation in

00:24:10,990 --> 00:24:15,430
the last two days about akka streams

00:24:13,210 --> 00:24:18,480
this is the best thing since sliced

00:24:15,430 --> 00:24:20,130
bread and I'm not just saying this

00:24:18,480 --> 00:24:24,330
because I work for light band I truly

00:24:20,130 --> 00:24:29,070
believe in this well maybe not but it's

00:24:24,330 --> 00:24:32,309
based on reactive streams and it here's

00:24:29,070 --> 00:24:35,040
the back pressure and this is a quick

00:24:32,309 --> 00:24:38,760
refresher of what back pressure is I'm

00:24:35,040 --> 00:24:41,910
sure you all healed in the last two days

00:24:38,760 --> 00:24:45,510
about that pressure so basically the

00:24:41,910 --> 00:24:48,169
idea is you have the bounded queue and

00:24:45,510 --> 00:24:52,140
you can control the speed with which

00:24:48,169 --> 00:24:55,049
producer is producing the new messages

00:24:52,140 --> 00:24:57,840
this is how you basically achieve

00:24:55,049 --> 00:25:00,419
reactive streams and the wonderful thing

00:24:57,840 --> 00:25:04,020
is it composes

00:25:00,419 --> 00:25:06,210
quite nicely so if you have a

00:25:04,020 --> 00:25:11,100
composition of reactive stream you have

00:25:06,210 --> 00:25:14,850
reactive all the way through so acha

00:25:11,100 --> 00:25:17,370
streams is part of the ecosystem which

00:25:14,850 --> 00:25:20,880
also includes our correctors akka

00:25:17,370 --> 00:25:22,440
cluster our HTTP aqua persistence and

00:25:20,880 --> 00:25:27,809
many other pieces

00:25:22,440 --> 00:25:31,080
it also has alpaca for connectivity to

00:25:27,809 --> 00:25:34,500
different sources and I'm happy to say

00:25:31,080 --> 00:25:37,160
we've kind rebooted alpaca development

00:25:34,500 --> 00:25:40,100
we have more people that now are

00:25:37,160 --> 00:25:42,929
dedicated fully to alpaca projects so

00:25:40,100 --> 00:25:47,460
hopefully things will start to move a

00:25:42,929 --> 00:25:52,950
little bit faster and they have very low

00:25:47,460 --> 00:25:56,790
overhead and latency so this is the most

00:25:52,950 --> 00:26:00,140
simplistic akka streams application and

00:25:56,790 --> 00:26:02,850
what I wanted to show here is the

00:26:00,140 --> 00:26:08,850
typical Extremes application has a

00:26:02,850 --> 00:26:12,720
source it has a flow which process the

00:26:08,850 --> 00:26:15,360
data from the source and it has the sync

00:26:12,720 --> 00:26:20,240
which outputs the results of the

00:26:15,360 --> 00:26:24,090
execution so this is the standard

00:26:20,240 --> 00:26:26,940
development paradigm for akka streams

00:26:24,090 --> 00:26:30,870
and one of the things that I find very

00:26:26,940 --> 00:26:32,040
convenient is I can just write through

00:26:30,870 --> 00:26:35,460
my process

00:26:32,040 --> 00:26:37,919
as a graph and then just go and

00:26:35,460 --> 00:26:41,100
implement the picture and this is why I

00:26:37,919 --> 00:26:43,700
think it is so convenient and we work so

00:26:41,100 --> 00:26:43,700
nicely

00:26:45,330 --> 00:26:52,530
so let's look it at the same example

00:26:49,200 --> 00:26:57,690
that were flogged it with Kafka streams

00:26:52,530 --> 00:27:00,330
are but implemented in extremes if you

00:26:57,690 --> 00:27:05,370
look at the code you will hear that it's

00:27:00,330 --> 00:27:09,150
kind of similar more or less the same

00:27:05,370 --> 00:27:13,790
thing and what it hears it hears at the

00:27:09,150 --> 00:27:19,470
ceremony which is necessary to enable

00:27:13,790 --> 00:27:23,690
extremes applications which is defining

00:27:19,470 --> 00:27:29,309
system materializer execution context

00:27:23,690 --> 00:27:34,490
then what you have is you have kind of

00:27:29,309 --> 00:27:39,299
similar definitions one thing here is

00:27:34,490 --> 00:27:48,030
we've been using custom graph stage to

00:27:39,299 --> 00:27:51,210
implement processing logic and the rest

00:27:48,030 --> 00:27:57,929
looks pretty much very similar to what

00:27:51,210 --> 00:28:00,990
we have seen with Kafka streams so there

00:27:57,929 --> 00:28:05,790
is a whole bunch of other concerns that

00:28:00,990 --> 00:28:11,730
we can address because extremes is a

00:28:05,790 --> 00:28:14,280
very rich ecosystem we can scale the

00:28:11,730 --> 00:28:16,919
things quite nicely because in this

00:28:14,280 --> 00:28:19,590
particular example every scoring engine

00:28:16,919 --> 00:28:24,120
is working with the particular model so

00:28:19,590 --> 00:28:28,410
we can integrate extremes with actors

00:28:24,120 --> 00:28:31,080
allowing individual actor to process

00:28:28,410 --> 00:28:34,320
requests of the certain data type and

00:28:31,080 --> 00:28:36,530
the advantage of this is if Kafka

00:28:34,320 --> 00:28:39,660
streams is inheritance the

00:28:36,530 --> 00:28:42,659
single-threaded here by introducing

00:28:39,660 --> 00:28:45,419
actors we can parallel wise and we can

00:28:42,659 --> 00:28:49,889
process different data types in

00:28:45,419 --> 00:28:53,549
which is becoming by far faster we can

00:28:49,889 --> 00:28:57,029
go even further and it reduced a cluster

00:28:53,549 --> 00:29:01,399
so that we can do parallelization across

00:28:57,029 --> 00:29:04,889
multiple machines we can

00:29:01,399 --> 00:29:09,179
Piercy's the execution through a

00:29:04,889 --> 00:29:11,759
persistence I mean in all fairness Kafka

00:29:09,179 --> 00:29:16,019
persistence if you write things in table

00:29:11,759 --> 00:29:21,209
tables are backed up by capital Q's so

00:29:16,019 --> 00:29:24,029
those are one power functionality and we

00:29:21,209 --> 00:29:27,029
also can connect to anything using Cal

00:29:24,029 --> 00:29:31,320
parka I mean the point of this slide is

00:29:27,029 --> 00:29:33,690
that a chi is such a rich ecosystem you

00:29:31,320 --> 00:29:37,469
can do a lot of things by combining

00:29:33,690 --> 00:29:43,129
pieces of the acha ecosystems together

00:29:37,469 --> 00:29:49,559
to build your application this is

00:29:43,129 --> 00:29:52,320
another issue that always comes up in

00:29:49,559 --> 00:29:55,019
this kind of system how do you do

00:29:52,320 --> 00:29:57,479
connectivity between the components at

00:29:55,019 --> 00:30:00,539
which point do you do direct

00:29:57,479 --> 00:30:03,389
connectivity in memory at which point

00:30:00,539 --> 00:30:10,529
you're using Kafka is the intermediary

00:30:03,389 --> 00:30:12,959
and because my role in the company is

00:30:10,529 --> 00:30:13,559
architect my standard answer to

00:30:12,959 --> 00:30:16,619
everything

00:30:13,559 --> 00:30:19,889
is it depends what you're trying to

00:30:16,619 --> 00:30:24,539
achieve well it really depends what can

00:30:19,889 --> 00:30:29,190
I do so if you're doing in memory

00:30:24,539 --> 00:30:34,489
connectivity your overhead is extremely

00:30:29,190 --> 00:30:37,769
low your performance is extremely high

00:30:34,489 --> 00:30:43,649
your application is extremely brittle

00:30:37,769 --> 00:30:49,320
because making changes is hard if you

00:30:43,649 --> 00:30:56,129
are trying to do it through Kafka you

00:30:49,320 --> 00:30:58,409
are losing in latency you are losing in

00:30:56,129 --> 00:30:59,100
processing because you need to marshal

00:30:58,409 --> 00:31:03,620
marshal

00:30:59,100 --> 00:31:08,179
every time when you do the hop but you

00:31:03,620 --> 00:31:11,159
gain a lot I because you can do

00:31:08,179 --> 00:31:14,309
independent scalability of components

00:31:11,159 --> 00:31:17,009
you can slide additional component that

00:31:14,309 --> 00:31:20,159
is using the same data without changing

00:31:17,009 --> 00:31:22,169
put you have already so it really you

00:31:20,159 --> 00:31:31,830
have to evaluate what your main

00:31:22,169 --> 00:31:36,509
criterias and go from there so again if

00:31:31,830 --> 00:31:38,490
you are this is more consideration

00:31:36,509 --> 00:31:43,019
between the two architectures and

00:31:38,490 --> 00:31:45,690
connectivity so this is for the most

00:31:43,019 --> 00:31:48,559
part what I wanted to say which is known

00:31:45,690 --> 00:31:51,779
as I'm waiting here earlier to lunch

00:31:48,559 --> 00:31:53,669
it's always hard to present right before

00:31:51,779 --> 00:31:59,789
lines because people are angry with you

00:31:53,669 --> 00:32:02,370
if you run over time I this is again I'm

00:31:59,789 --> 00:32:05,639
using this to promote what we are doing

00:32:02,370 --> 00:32:11,820
in light behind this is like band First

00:32:05,639 --> 00:32:15,410
Data Platform and I'll be happy to

00:32:11,820 --> 00:32:24,640
answer any questions that you guys have

00:32:15,410 --> 00:32:24,640
[Applause]

00:32:25,740 --> 00:32:29,980
and it looks like people are so hungry

00:32:28,360 --> 00:32:39,310
nobody wants to ask the question no

00:32:29,980 --> 00:32:42,190
there is something there you mentioned

00:32:39,310 --> 00:32:44,830
in passing I think that one of the

00:32:42,190 --> 00:32:48,070
advantages is that ARCA is a rich

00:32:44,830 --> 00:32:52,420
ecosystem and therefore you can also

00:32:48,070 --> 00:32:55,360
combine streams with acha persistence

00:32:52,420 --> 00:32:59,560
can you talk a little bit about how that

00:32:55,360 --> 00:33:03,160
would look in practice I yeah in reality

00:32:59,560 --> 00:33:06,130
you don't combine directly streams with

00:33:03,160 --> 00:33:09,490
akkad persistence what you do is you

00:33:06,130 --> 00:33:11,760
combine streams with actors and then

00:33:09,490 --> 00:33:16,180
through actors here using persistence

00:33:11,760 --> 00:33:18,160
okay ah as Dean would say in my place

00:33:16,180 --> 00:33:21,220
there is nothing that you can't achieve

00:33:18,160 --> 00:33:23,970
with one more level of indirection make

00:33:21,220 --> 00:33:23,970

YouTube URL: https://www.youtube.com/watch?v=dgsolEv-AzU


