Title: High performance Scala systems without tears by Zahari Dichev
Publication date: 2018-09-20
Playlist: Scala Days Berlin 2018
Description: 
	This video was recorded at Scala Days Berlin 2018
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

More information and the abstract can be found here:
https://eu.scaladays.org/lect-7070-high-performance-scala-systems-without-tears.html
Captions: 
	00:00:04,590 --> 00:00:11,430
hello everyone first of all I'm really

00:00:08,519 --> 00:00:13,139
glad to be here today and I've been

00:00:11,430 --> 00:00:17,010
anticipating this event for quite some

00:00:13,139 --> 00:00:20,220
time now and this is the good news the

00:00:17,010 --> 00:00:22,109
bad news however is that it's my first

00:00:20,220 --> 00:00:24,859
talk so if it's not that great I don't

00:00:22,109 --> 00:00:28,199
think you can get your money back but

00:00:24,859 --> 00:00:31,109
nevertheless I'll I'll do my best

00:00:28,199 --> 00:00:35,130
so we're going to be talking about high

00:00:31,109 --> 00:00:38,010
performance today and I'm gonna take you

00:00:35,130 --> 00:00:41,760
through a few topics that we are going

00:00:38,010 --> 00:00:44,040
to go into detail and that are often ly

00:00:41,760 --> 00:00:47,610
considered when we are engineering for

00:00:44,040 --> 00:00:50,630
performance and we're not going to cover

00:00:47,610 --> 00:00:53,100
everything because this deserves a whole

00:00:50,630 --> 00:00:54,900
probably sequence of books like the game

00:00:53,100 --> 00:00:58,500
of Thrones in order to cover all aspects

00:00:54,900 --> 00:01:00,240
of performance so it's we're just going

00:00:58,500 --> 00:01:02,670
to look at the few things that have came

00:01:00,240 --> 00:01:04,350
up frequently when I was dealing with

00:01:02,670 --> 00:01:08,729
such problems and I'm gonna show you

00:01:04,350 --> 00:01:11,549
ways how we can how we can what we can

00:01:08,729 --> 00:01:15,439
do about positively impacting the

00:01:11,549 --> 00:01:19,109
performance of our scholar systems so

00:01:15,439 --> 00:01:21,210
we're gonna also conclude with real-life

00:01:19,109 --> 00:01:22,799
use case because I'm sure you don't

00:01:21,210 --> 00:01:26,429
really want to just look at benchmarks

00:01:22,799 --> 00:01:28,890
and test code that takes these ideas and

00:01:26,429 --> 00:01:30,990
the theories behind what I will be

00:01:28,890 --> 00:01:33,060
talking about today and applies them in

00:01:30,990 --> 00:01:37,049
order to extract a bit more performance

00:01:33,060 --> 00:01:41,369
out of a real component that's in

00:01:37,049 --> 00:01:44,549
production so let's just get started so

00:01:41,369 --> 00:01:47,399
the first thing is excessive object

00:01:44,549 --> 00:01:51,270
instantiation and this is something

00:01:47,399 --> 00:01:54,659
that's fairly a fairly hot topic when it

00:01:51,270 --> 00:01:56,639
comes to performance it's important to

00:01:54,659 --> 00:01:59,490
keep in mind that actual object

00:01:56,639 --> 00:02:02,369
instantiation on the JVM itself costs

00:01:59,490 --> 00:02:05,249
time it creates a lot more work for the

00:02:02,369 --> 00:02:07,649
garbage collector and it can introduce

00:02:05,249 --> 00:02:11,520
systemic pauses in your application

00:02:07,649 --> 00:02:13,439
which introduces non determinism and non

00:02:11,520 --> 00:02:15,510
determinism is bad when we are talking

00:02:13,439 --> 00:02:17,810
about performance because we don't only

00:02:15,510 --> 00:02:21,230
want to have something that runs

00:02:17,810 --> 00:02:23,180
fast sometimes we want to have a system

00:02:21,230 --> 00:02:26,450
that's measurable and that's predictable

00:02:23,180 --> 00:02:28,459
and that we can put hard constraints on

00:02:26,450 --> 00:02:33,950
in terms of its latency and throughput

00:02:28,459 --> 00:02:36,019
requirements so that being said it's

00:02:33,950 --> 00:02:38,750
important to know that this is such an

00:02:36,019 --> 00:02:41,569
important topic that a fair amount of

00:02:38,750 --> 00:02:44,690
systems actually goes through the effort

00:02:41,569 --> 00:02:48,410
of implementing gc3 data structures on

00:02:44,690 --> 00:02:52,220
of heap and there are some commercial

00:02:48,410 --> 00:02:55,370
solutions that try to achieve zero pause

00:02:52,220 --> 00:02:57,290
garbage collection such as the azores in

00:02:55,370 --> 00:03:00,140
and as a matter of fact there was even

00:02:57,290 --> 00:03:03,769
proposal on the JVM to introduce a know

00:03:00,140 --> 00:03:07,130
up garbage collector that will just do

00:03:03,769 --> 00:03:09,290
completely nothing and it didn't it

00:03:07,130 --> 00:03:10,970
still isn't in but it's something that

00:03:09,290 --> 00:03:13,819
as a matter of fact could be quite

00:03:10,970 --> 00:03:16,670
useful in certain class of systems that

00:03:13,819 --> 00:03:20,959
are running out there so that being said

00:03:16,670 --> 00:03:24,290
let's illustrate the impact of object

00:03:20,959 --> 00:03:28,760
allocation on hold paths with a real

00:03:24,290 --> 00:03:31,700
example and it's the all loft extractor

00:03:28,760 --> 00:03:34,549
objects right so extractors you all know

00:03:31,700 --> 00:03:36,890
this they are pretty much an object with

00:03:34,549 --> 00:03:40,640
an on apply method which takes an object

00:03:36,890 --> 00:03:42,890
and tries to tries to give back its

00:03:40,640 --> 00:03:44,989
components and they're really useful in

00:03:42,890 --> 00:03:46,940
pattern matching and partial functions

00:03:44,989 --> 00:03:49,760
and then overall a great tool for

00:03:46,940 --> 00:03:58,220
achieving brevity and expressiveness in

00:03:49,760 --> 00:04:00,109
your code so what can we what can we

00:03:58,220 --> 00:04:02,090
show what can we actually do with

00:04:00,109 --> 00:04:06,910
extractors right well imagine this

00:04:02,090 --> 00:04:09,290
simple example where we have a we have

00:04:06,910 --> 00:04:11,120
case class that looks like this

00:04:09,290 --> 00:04:12,950
so it's a rook I don't actually play

00:04:11,120 --> 00:04:17,090
chess but I just found out that this

00:04:12,950 --> 00:04:17,870
cool example we have a route dice either

00:04:17,090 --> 00:04:20,600
black or white

00:04:17,870 --> 00:04:23,240
and we simply need to go to a board to a

00:04:20,600 --> 00:04:25,190
square board full of rooks and match on

00:04:23,240 --> 00:04:27,320
all black rooks and do something with

00:04:25,190 --> 00:04:30,889
their x and y-coordinates so pretty much

00:04:27,320 --> 00:04:34,999
extract them and just

00:04:30,889 --> 00:04:38,499
sum them up or whatever so a extractor

00:04:34,999 --> 00:04:41,689
object that can do that and give us back

00:04:38,499 --> 00:04:44,810
give us back the coordinates looks a bit

00:04:41,689 --> 00:04:47,870
like this right we have checks whether

00:04:44,810 --> 00:04:50,449
the rook is black and it gives back a

00:04:47,870 --> 00:04:54,800
topo of the x and y coordinates

00:04:50,449 --> 00:04:56,449
otherwise it's a knob right so but the

00:04:54,800 --> 00:04:59,900
interesting thing about this piece of

00:04:56,449 --> 00:05:02,060
codes is that if you use Java P or any

00:04:59,900 --> 00:05:04,039
kind of bytecode viewing tool and take a

00:05:02,060 --> 00:05:06,469
look at the bytecode it's going to look

00:05:04,039 --> 00:05:09,460
like this and if you drill a bit further

00:05:06,469 --> 00:05:12,500
down you're gonna notice that apart from

00:05:09,460 --> 00:05:16,219
instantiating the specialized tuple of

00:05:12,500 --> 00:05:19,729
RIT - you're also creating a some object

00:05:16,219 --> 00:05:23,060
which which is again allocated on the

00:05:19,729 --> 00:05:25,819
heap and introduces a point of

00:05:23,060 --> 00:05:29,419
indirection and also creates work for

00:05:25,819 --> 00:05:32,330
the GC and for all things involved

00:05:29,419 --> 00:05:34,249
around allocation in bookkeeping so can

00:05:32,330 --> 00:05:37,210
we do better than that in this

00:05:34,249 --> 00:05:40,870
particular case turns out yes we can use

00:05:37,210 --> 00:05:45,099
something that was introduced in 2.11

00:05:40,870 --> 00:05:48,919
namely name based extractors to

00:05:45,099 --> 00:05:52,370
eliminate the need of using an option so

00:05:48,919 --> 00:05:54,229
really the requirement that these

00:05:52,370 --> 00:05:56,719
extractor objects the name basic

00:05:54,229 --> 00:06:00,529
extractor objects have is only to return

00:05:56,719 --> 00:06:04,250
an instance of a type that defines two

00:06:00,529 --> 00:06:06,469
methods that is empty and the get and if

00:06:04,250 --> 00:06:09,289
we have to if we want to rewrite our

00:06:06,469 --> 00:06:13,669
extractor to utilize that functionality

00:06:09,289 --> 00:06:16,990
and make use of that feature we can we

00:06:13,669 --> 00:06:21,050
can write something like this right so

00:06:16,990 --> 00:06:25,120
it's it's instead of returning an option

00:06:21,050 --> 00:06:28,849
we're returning an extractor object that

00:06:25,120 --> 00:06:32,240
that defines these two methods and can

00:06:28,849 --> 00:06:35,990
get and uses the fact that it takes an

00:06:32,240 --> 00:06:38,089
inner F it uses it to have a guardian

00:06:35,990 --> 00:06:42,099
value that is know in order to determine

00:06:38,089 --> 00:06:45,230
whether the extraction is empty or not

00:06:42,099 --> 00:06:48,500
so looking at the bytecode

00:06:45,230 --> 00:06:49,820
this particular think you quickly notice

00:06:48,500 --> 00:06:53,750
that again

00:06:49,820 --> 00:06:56,210
the only allocation that happens and is

00:06:53,750 --> 00:06:59,770
in this case the instantiation

00:06:56,210 --> 00:07:04,760
of the tuple of the specialized tuple of

00:06:59,770 --> 00:07:06,860
r82 and but does that even causes a dent

00:07:04,760 --> 00:07:09,830
in our performance that it does this

00:07:06,860 --> 00:07:13,220
matter it turns out yes it actually

00:07:09,830 --> 00:07:15,850
matters significantly I have used SBT

00:07:13,220 --> 00:07:18,560
gmh to run some benchmarks on different

00:07:15,850 --> 00:07:21,710
sizes of chess boards full of black

00:07:18,560 --> 00:07:23,330
rooks and pretty much trying to going

00:07:21,710 --> 00:07:25,670
through the farooq through the full

00:07:23,330 --> 00:07:28,520
board extracting every roof that is

00:07:25,670 --> 00:07:32,900
black and summing up their coordinates

00:07:28,520 --> 00:07:34,580
and looking at the average time in

00:07:32,900 --> 00:07:39,430
milliseconds to perform this operation

00:07:34,580 --> 00:07:43,820
across various various sizes of boards

00:07:39,430 --> 00:07:47,830
it quickly becomes obvious that this

00:07:43,820 --> 00:07:52,370
simple change increases the performance

00:07:47,830 --> 00:07:55,970
quite some and that's all pretty obvious

00:07:52,370 --> 00:07:58,070
from from the results and if you want to

00:07:55,970 --> 00:08:01,490
even go a bit further and analyze these

00:07:58,070 --> 00:08:04,210
kind of things you can use a lot more

00:08:01,490 --> 00:08:08,840
tools that are there on the part of the

00:08:04,210 --> 00:08:11,600
part of the to chain to do so so you can

00:08:08,840 --> 00:08:14,720
you can print your GC application or

00:08:11,600 --> 00:08:17,540
stop time and concurrent time to look at

00:08:14,720 --> 00:08:19,370
your pauses and look at their duration

00:08:17,540 --> 00:08:21,260
of course you have to keep in mind that

00:08:19,370 --> 00:08:23,270
these pauses are not only introduced by

00:08:21,260 --> 00:08:26,450
garbage collection activity but also

00:08:23,270 --> 00:08:29,810
other things as well such as bias lock

00:08:26,450 --> 00:08:31,730
reclamation if there's that happening

00:08:29,810 --> 00:08:34,070
but nevertheless there are tools that

00:08:31,730 --> 00:08:36,770
can show you a lot about what is

00:08:34,070 --> 00:08:38,540
happening with respect to this kind of

00:08:36,770 --> 00:08:42,800
allocation and reclamation activity

00:08:38,540 --> 00:08:46,070
that's going on with the JVM and just to

00:08:42,800 --> 00:08:49,790
show a quick a quick snapshot of what

00:08:46,070 --> 00:08:52,190
things sort of look like on the heap we

00:08:49,790 --> 00:08:54,890
can use Java P to take a look at the

00:08:52,190 --> 00:08:57,649
heap histogram and see that

00:08:54,890 --> 00:08:59,329
for the default implementation with

00:08:57,649 --> 00:09:02,930
option we are indeed

00:08:59,329 --> 00:09:06,370
have at this point in time when the

00:09:02,930 --> 00:09:09,790
snapshot was taken who have significant

00:09:06,370 --> 00:09:13,190
quantity of some objects allocated while

00:09:09,790 --> 00:09:15,769
on the on the name based version of the

00:09:13,190 --> 00:09:18,200
codes there is theories there is only

00:09:15,769 --> 00:09:20,089
tuples and roots and integers and all

00:09:18,200 --> 00:09:21,410
the other stuff that that's that's on

00:09:20,089 --> 00:09:24,890
the heap at the moment but there is no

00:09:21,410 --> 00:09:29,990
sign of some Salo cated naturally so

00:09:24,890 --> 00:09:32,209
that being said again it's important to

00:09:29,990 --> 00:09:34,279
consider these things especially when

00:09:32,209 --> 00:09:40,450
you're running on a very hot hot code

00:09:34,279 --> 00:09:44,510
path because the JVM is great too for

00:09:40,450 --> 00:09:46,190
managing it it gives you quite a lot of

00:09:44,510 --> 00:09:48,410
around the security of managing memory

00:09:46,190 --> 00:09:52,250
and and and there is a lot of work being

00:09:48,410 --> 00:09:54,680
done in that but but it's not for free

00:09:52,250 --> 00:09:58,430
so when you're when you're engineering

00:09:54,680 --> 00:10:00,829
for performance it's important to keep

00:09:58,430 --> 00:10:04,010
these things to a minimum and not only

00:10:00,829 --> 00:10:06,949
that but make sure you do that at the

00:10:04,010 --> 00:10:08,630
correct spot and of your code and of

00:10:06,949 --> 00:10:10,579
your systems and this comes with

00:10:08,630 --> 00:10:13,310
measurements setting up experiments and

00:10:10,579 --> 00:10:15,980
looking at the results of that in order

00:10:13,310 --> 00:10:19,430
to make informed decisions and put the

00:10:15,980 --> 00:10:22,360
effort where it needs to be so that's

00:10:19,430 --> 00:10:24,920
one thing our objects instantiation and

00:10:22,360 --> 00:10:28,910
the excess logical instantiation is a

00:10:24,920 --> 00:10:30,410
pretty pretty close to mine think that

00:10:28,910 --> 00:10:32,540
that comes when when we're thinking

00:10:30,410 --> 00:10:35,149
about performance in the JVM and Scala

00:10:32,540 --> 00:10:40,820
but it's not only that there are other

00:10:35,149 --> 00:10:43,880
things that we need to consider and we

00:10:40,820 --> 00:10:46,160
need to keep in mind that all memory is

00:10:43,880 --> 00:10:49,570
not equal this is something they

00:10:46,160 --> 00:10:53,649
probably that's quite important to

00:10:49,570 --> 00:10:56,329
mention when somebody tells you that an

00:10:53,649 --> 00:10:58,760
access into an array is always constant

00:10:56,329 --> 00:11:00,199
cost it's really not right it really

00:10:58,760 --> 00:11:04,699
depends what part of the array you're

00:11:00,199 --> 00:11:07,310
accessing so that being said why am I

00:11:04,699 --> 00:11:09,490
even talking about this well it has to

00:11:07,310 --> 00:11:09,490
do with

00:11:09,560 --> 00:11:15,810
memory access patterns and algorithms

00:11:12,930 --> 00:11:19,589
for doing that and this all comes down

00:11:15,810 --> 00:11:22,770
to our hardware our CPUs nowadays are

00:11:19,589 --> 00:11:26,010
doing quite a lot more work apart from

00:11:22,770 --> 00:11:27,930
just dealing with a bunch of arithmetic

00:11:26,010 --> 00:11:30,959
operations and you probably all know

00:11:27,930 --> 00:11:34,130
that one of your main jobs is masking

00:11:30,959 --> 00:11:36,480
latency to main memory which is high and

00:11:34,130 --> 00:11:41,250
the way they are doing that is through a

00:11:36,480 --> 00:11:44,300
hierarchy of caches that ensure our CPUs

00:11:41,250 --> 00:11:46,830
progress is not severely hindered by

00:11:44,300 --> 00:11:50,550
just constant fetching data from main

00:11:46,830 --> 00:11:53,310
memory and it is said that a cache miss

00:11:50,550 --> 00:11:56,820
is one of the most prominent performance

00:11:53,310 --> 00:11:59,339
killers on modern hardware and this is

00:11:56,820 --> 00:12:04,860
true across all sorts of problem

00:11:59,339 --> 00:12:08,490
platforms not just the JVM so of course

00:12:04,860 --> 00:12:09,839
it it this is this is the hardware

00:12:08,490 --> 00:12:12,209
engineers have thought about that and

00:12:09,839 --> 00:12:14,610
have provided us with a hearty of caches

00:12:12,209 --> 00:12:17,190
and you all know about the l1 cache you

00:12:14,610 --> 00:12:19,470
know l2 and l3 caches and then what

00:12:17,190 --> 00:12:23,220
their respective latencies are and the

00:12:19,470 --> 00:12:25,230
sizes but if you have to draw this it

00:12:23,220 --> 00:12:28,260
would sort of look a bit like this right

00:12:25,230 --> 00:12:29,910
this is sort of an imaginary memory

00:12:28,260 --> 00:12:32,730
hierarchy or at least how it sort of

00:12:29,910 --> 00:12:35,970
looks on my machine so we have the l1

00:12:32,730 --> 00:12:38,160
cache which is core lock o l2 and l3

00:12:35,970 --> 00:12:41,730
which is shared and it sort of provides

00:12:38,160 --> 00:12:45,740
the main fabric that ensures that your

00:12:41,730 --> 00:12:48,529
cache subsystem is kept here until

00:12:45,740 --> 00:12:51,510
left-to-right you're increasing in size

00:12:48,529 --> 00:12:54,870
but you're also increasing in terms of

00:12:51,510 --> 00:12:56,820
latency and data access costs and then

00:12:54,870 --> 00:13:03,440
you go to main memory and things get

00:12:56,820 --> 00:13:03,440
quite ugly very quickly that being said

00:13:03,800 --> 00:13:10,230
it's probably useful to substitute that

00:13:07,200 --> 00:13:12,810
with an example right so something that

00:13:10,230 --> 00:13:15,240
you probably all off it's going to take

00:13:12,810 --> 00:13:18,420
you back to your high school algebra

00:13:15,240 --> 00:13:21,120
ears it's the matrix transposition

00:13:18,420 --> 00:13:22,470
problem right it's very simple let's say

00:13:21,120 --> 00:13:27,060
we have a square matrix

00:13:22,470 --> 00:13:29,820
that nature and transposed it would look

00:13:27,060 --> 00:13:33,630
like this right and imagine this matrix

00:13:29,820 --> 00:13:36,840
is stored in memory as in a column major

00:13:33,630 --> 00:13:40,680
order and we can't we can't really

00:13:36,840 --> 00:13:43,070
change that right so an algorithm

00:13:40,680 --> 00:13:48,240
probably not the most efficient one to

00:13:43,070 --> 00:13:52,170
transpose this matrix by doing accesses

00:13:48,240 --> 00:13:55,260
in a row major order so going left to

00:13:52,170 --> 00:13:58,350
left right row by row would sort of look

00:13:55,260 --> 00:14:02,940
like this two nested for-loops so I'm

00:13:58,350 --> 00:14:04,530
guilty of that but that would that would

00:14:02,940 --> 00:14:08,220
sort of what it looks like in its

00:14:04,530 --> 00:14:10,410
simplicity right so now imagine this we

00:14:08,220 --> 00:14:12,870
run this piece of code on this kind of

00:14:10,410 --> 00:14:15,860
imaginary computer which is not far away

00:14:12,870 --> 00:14:19,320
from what we have in these puppies here

00:14:15,860 --> 00:14:23,370
and imagine this machine sort of looks

00:14:19,320 --> 00:14:25,860
like one that has just cache and main

00:14:23,370 --> 00:14:26,550
memory and latest main memory is quite

00:14:25,860 --> 00:14:29,490
expensive

00:14:26,550 --> 00:14:32,430
while access to cache are very quick you

00:14:29,490 --> 00:14:34,830
can pretty much consider them free and

00:14:32,430 --> 00:14:38,100
imagine we load data in cache lines

00:14:34,830 --> 00:14:41,400
which is as we do in our modern hardware

00:14:38,100 --> 00:14:44,940
and these cache lines are four elements

00:14:41,400 --> 00:14:46,680
wide for matrix elements whites so for

00:14:44,940 --> 00:14:50,720
long so they can only fit for Long's and

00:14:46,680 --> 00:14:53,070
you have a cache size of just one line

00:14:50,720 --> 00:14:55,170
one cache line so you can store only

00:14:53,070 --> 00:14:57,170
four elements of the matrix in your

00:14:55,170 --> 00:15:00,930
cache so if we run that kind of

00:14:57,170 --> 00:15:04,650
algorithm and represent what visually is

00:15:00,930 --> 00:15:06,630
going to happen it would look a bit like

00:15:04,650 --> 00:15:09,840
this right we have a contiguous array in

00:15:06,630 --> 00:15:16,200
memory storing the matrix in column

00:15:09,840 --> 00:15:19,290
major order and we are accessing rows

00:15:16,200 --> 00:15:20,760
left right and the different colors

00:15:19,290 --> 00:15:25,320
represent the different cache lines by

00:15:20,760 --> 00:15:29,790
the way if if that's not clear and if we

00:15:25,320 --> 00:15:33,360
proceed to transpose this matrix running

00:15:29,790 --> 00:15:34,470
this algorithm we could see that by the

00:15:33,360 --> 00:15:36,840
time with

00:15:34,470 --> 00:15:41,430
we're done with the first row with

00:15:36,840 --> 00:15:45,510
pretty much access for separate cache

00:15:41,430 --> 00:15:48,210
lines and so we have incurred quite a

00:15:45,510 --> 00:15:51,960
lot of evictions on our tiny tiny

00:15:48,210 --> 00:15:56,130
one-line sized cache and naturally we

00:15:51,960 --> 00:15:59,340
with with we've resorted to two main

00:15:56,130 --> 00:16:01,740
memory in that imaginary scenario and

00:15:59,340 --> 00:16:03,690
this is this is not great obviously

00:16:01,740 --> 00:16:05,310
because pretty much if you look at it we

00:16:03,690 --> 00:16:06,900
are accessing and getting a full cache

00:16:05,310 --> 00:16:09,330
line in our caches but we're sort of

00:16:06,900 --> 00:16:11,730
just operating and doing actual work on

00:16:09,330 --> 00:16:13,830
1/4 of it so why do we even do that

00:16:11,730 --> 00:16:16,370
we're throwing most of our data away

00:16:13,830 --> 00:16:18,660
that we're going to be using later on

00:16:16,370 --> 00:16:22,020
when we are for example accessing the

00:16:18,660 --> 00:16:24,900
second row so what can be done about

00:16:22,020 --> 00:16:29,370
this well you can sort of separate your

00:16:24,900 --> 00:16:35,330
matrix into recursively into smaller and

00:16:29,370 --> 00:16:39,720
smaller regions and proceed as follows

00:16:35,330 --> 00:16:43,170
so in that case pretty much your first

00:16:39,720 --> 00:16:45,780
your first the first part of the matrix

00:16:43,170 --> 00:16:51,570
your you're going to be transposing it's

00:16:45,780 --> 00:16:53,760
you're only going to be accessing to two

00:16:51,570 --> 00:16:56,430
separate cache lines in order to to

00:16:53,760 --> 00:16:59,010
transpose one fourth of the matrix so

00:16:56,430 --> 00:17:04,380
simply by changing our access button we

00:16:59,010 --> 00:17:09,689
have effectively sliced our cache misses

00:17:04,380 --> 00:17:11,640
by a factor of two and that's it that

00:17:09,689 --> 00:17:15,420
does this actually have any dent on

00:17:11,640 --> 00:17:18,089
performance yeah it certainly does so

00:17:15,420 --> 00:17:20,579
looking at the again the gem age

00:17:18,089 --> 00:17:22,350
benchmarks you can see that for

00:17:20,579 --> 00:17:24,209
different sizes of matrix and that's

00:17:22,350 --> 00:17:26,130
just a size of the matrix in one

00:17:24,209 --> 00:17:31,650
direction so it's for example the square

00:17:26,130 --> 00:17:34,140
matrix of 2084 by 2084 we can see that

00:17:31,650 --> 00:17:36,270
the cache friendly approach achieves

00:17:34,140 --> 00:17:41,360
quite a lot of performance increase

00:17:36,270 --> 00:17:45,320
compared to the naive access pattern and

00:17:41,360 --> 00:17:47,840
this this comes pretty close to mind

00:17:45,320 --> 00:17:50,210
but if we really want to be sure about

00:17:47,840 --> 00:17:55,730
this kind of weather this is actually

00:17:50,210 --> 00:17:57,530
happening and whether we are whether we

00:17:55,730 --> 00:17:59,810
can benefit from from changing our

00:17:57,530 --> 00:18:02,690
algorithms to to exploit a cash car here

00:17:59,810 --> 00:18:06,110
we can use quite interesting tools like

00:18:02,690 --> 00:18:09,100
intel vtune liquid and purse stat and as

00:18:06,110 --> 00:18:11,750
well as a recently Intel PCM which is

00:18:09,100 --> 00:18:15,080
which is something you can coming from

00:18:11,750 --> 00:18:18,310
Intel you can compile it on your Mac you

00:18:15,080 --> 00:18:22,220
need to disable the kernel protection of

00:18:18,310 --> 00:18:24,830
Mac OS or or whatever but but you can

00:18:22,220 --> 00:18:26,780
get it running and it these tools are

00:18:24,830 --> 00:18:28,730
great because they give you stats around

00:18:26,780 --> 00:18:31,190
your cache hits and misses and the

00:18:28,730 --> 00:18:33,860
ratios apart from other really really

00:18:31,190 --> 00:18:37,730
interesting data just getting directly

00:18:33,860 --> 00:18:39,710
read from the heart the for for the

00:18:37,730 --> 00:18:42,740
motor processor hardware contours on

00:18:39,710 --> 00:18:44,240
your CPU and Intel open PCM run on the

00:18:42,740 --> 00:18:47,360
Mac looks a bit like this so it gives

00:18:44,240 --> 00:18:50,540
you the l2 misses and hits and the

00:18:47,360 --> 00:18:52,280
ratios as well as a lot of other stuff

00:18:50,540 --> 00:18:54,020
that that you can that you can look at

00:18:52,280 --> 00:18:56,840
and for example you can see here that

00:18:54,020 --> 00:18:58,190
for this program that I run I don't even

00:18:56,840 --> 00:19:01,610
actually remember what particular

00:18:58,190 --> 00:19:04,670
program I run here but I can see here

00:19:01,610 --> 00:19:06,950
that on the core 4 I'm incurring

00:19:04,670 --> 00:19:10,730
significant amount of l2 cache misses

00:19:06,950 --> 00:19:13,490
and obviously my l2 hit ratio is quite

00:19:10,730 --> 00:19:16,790
quite low which is sort of the thing

00:19:13,490 --> 00:19:18,350
that can tell you that for sharing is

00:19:16,790 --> 00:19:19,600
happening but we're going to talk about

00:19:18,350 --> 00:19:24,100
that later

00:19:19,600 --> 00:19:28,610
so plotting that plotting this data on

00:19:24,100 --> 00:19:30,950
on a chart for this particular matrix

00:19:28,610 --> 00:19:36,350
multiplication problem a transposition

00:19:30,950 --> 00:19:38,630
problem looks as follows and it is again

00:19:36,350 --> 00:19:41,120
clear that this is probably the most

00:19:38,630 --> 00:19:42,890
prominent performance killer in this in

00:19:41,120 --> 00:19:45,440
this in the naive version it's the fact

00:19:42,890 --> 00:19:48,200
that you're incurring quite a lot of l2

00:19:45,440 --> 00:19:50,870
cache misses and this is a topic that is

00:19:48,200 --> 00:19:53,450
so prominent and performance that there

00:19:50,870 --> 00:19:57,140
is a whole stream of research that is

00:19:53,450 --> 00:19:58,610
focused around the so called cache

00:19:57,140 --> 00:20:01,429
oblivious algorithms

00:19:58,610 --> 00:20:02,360
and these algorithms the name is a bit

00:20:01,429 --> 00:20:05,510
deceiving

00:20:02,360 --> 00:20:08,510
it's cache-oblivious because they don't

00:20:05,510 --> 00:20:11,690
really need or care to know about the

00:20:08,510 --> 00:20:13,549
underlying details of the cache

00:20:11,690 --> 00:20:15,770
hierarchy so the cache line size the

00:20:13,549 --> 00:20:18,860
cache size it's often the depth of the

00:20:15,770 --> 00:20:22,690
caches none of that is needed what these

00:20:18,860 --> 00:20:25,100
algorithms actually resort upon is

00:20:22,690 --> 00:20:27,350
pretty much dividing and conquering the

00:20:25,100 --> 00:20:29,090
problem up to a point where it's small

00:20:27,350 --> 00:20:31,030
enough to fit in cache and then they

00:20:29,090 --> 00:20:33,470
solve it and move to the next part so

00:20:31,030 --> 00:20:37,370
most of these algorithms are recursive

00:20:33,470 --> 00:20:39,350
in nature and using a divide and conquer

00:20:37,370 --> 00:20:42,890
approach and as a matter of fact this is

00:20:39,350 --> 00:20:44,929
actually quite you it's used in systems

00:20:42,890 --> 00:20:47,900
that need to be engineered for

00:20:44,929 --> 00:20:51,350
performance I think like our artery

00:20:47,900 --> 00:20:53,960
actor FK she's using some sort of a

00:20:51,350 --> 00:20:57,730
oblivious cache oblivious caching data

00:20:53,960 --> 00:21:03,679
structure that that is that is in there

00:20:57,730 --> 00:21:05,299
so MIT is quite famous for having a

00:21:03,679 --> 00:21:07,790
whole group of researchers that are

00:21:05,299 --> 00:21:10,190
dedicated to the design in an analysis

00:21:07,790 --> 00:21:12,290
of cache oblivious algorithms and and

00:21:10,190 --> 00:21:14,809
they output quite a lot of papers and if

00:21:12,290 --> 00:21:17,270
you look at the open courseware videos

00:21:14,809 --> 00:21:19,130
there's great videos of explaining some

00:21:17,270 --> 00:21:21,470
of the concepts and doing demos and

00:21:19,130 --> 00:21:23,690
designing algorithms on the board that

00:21:21,470 --> 00:21:24,320
actually take advantage of the cache

00:21:23,690 --> 00:21:26,750
hierarchy

00:21:24,320 --> 00:21:31,040
some examples are matrix multiplication

00:21:26,750 --> 00:21:33,799
such as the Strassen algorithm and frigo

00:21:31,040 --> 00:21:37,880
strands balls for transpositions amongst

00:21:33,799 --> 00:21:43,010
others but I encourage you if you

00:21:37,880 --> 00:21:44,980
actually are facing a problem where you

00:21:43,010 --> 00:21:48,400
are experiencing quite a lot of cache

00:21:44,980 --> 00:21:52,370
misses and that hinders your performance

00:21:48,400 --> 00:21:54,770
it's it's really good to think and go

00:21:52,370 --> 00:21:56,750
out and look at algorithms that are

00:21:54,770 --> 00:21:58,280
already out there that's the tick

00:21:56,750 --> 00:22:01,580
advantage of there the lion cache

00:21:58,280 --> 00:22:03,460
hierarchy and try to apply them to your

00:22:01,580 --> 00:22:08,679
particular problem because this could

00:22:03,460 --> 00:22:08,679
benefit your performance significantly

00:22:08,800 --> 00:22:14,680
so

00:22:10,780 --> 00:22:18,300
that's that another topic that

00:22:14,680 --> 00:22:21,610
frequently comes up and high-performance

00:22:18,300 --> 00:22:23,460
systems it's the one about

00:22:21,610 --> 00:22:26,260
synchronization right and

00:22:23,460 --> 00:22:27,760
synchronization has its price and this

00:22:26,260 --> 00:22:29,770
is actually something that you are all

00:22:27,760 --> 00:22:32,170
aware of right I mean we live in a

00:22:29,770 --> 00:22:33,790
concurrent world so everybody is doing

00:22:32,170 --> 00:22:38,350
concurrency nowadays even if they don't

00:22:33,790 --> 00:22:42,520
know about it but it's really important

00:22:38,350 --> 00:22:44,020
to know and think about what is the

00:22:42,520 --> 00:22:47,560
actual cost of what I'm doing

00:22:44,020 --> 00:22:50,500
so Scala and the JVM in general as well

00:22:47,560 --> 00:22:53,170
as our hardware give us this nice

00:22:50,500 --> 00:22:55,680
toolbox of things that we can use in

00:22:53,170 --> 00:22:58,030
order to write safe and correct

00:22:55,680 --> 00:22:59,830
concurrent algorithms you know we have

00:22:58,030 --> 00:23:01,720
atomic primitives and we have volatiles

00:22:59,830 --> 00:23:03,250
and we have concurrent collections and

00:23:01,720 --> 00:23:05,650
some of them are signed synchronized

00:23:03,250 --> 00:23:07,420
some of them are concurrent and how

00:23:05,650 --> 00:23:12,790
concurrent they are nobody really knows

00:23:07,420 --> 00:23:13,990
but it's it's it's a wide gamut of tools

00:23:12,790 --> 00:23:16,030
that you can use to write your

00:23:13,990 --> 00:23:18,100
algorithms but sometimes it's really

00:23:16,030 --> 00:23:21,570
important to step back and think about

00:23:18,100 --> 00:23:24,930
well am I using the right thing and

00:23:21,570 --> 00:23:28,090
furthermore am I using it to its best

00:23:24,930 --> 00:23:30,060
capabilities and pretty much am i using

00:23:28,090 --> 00:23:32,650
the right tool for the right job here

00:23:30,060 --> 00:23:35,560
because this matters there's a matter of

00:23:32,650 --> 00:23:38,080
fact matters a lot so let's move into

00:23:35,560 --> 00:23:40,810
another example that's kind of a toy

00:23:38,080 --> 00:23:43,660
example so it's kind of a day in the

00:23:40,810 --> 00:23:45,820
life I recently have been working with

00:23:43,660 --> 00:23:50,980
quite a lot of events so only think I

00:23:45,820 --> 00:23:52,960
see these events everywhere so naturally

00:23:50,980 --> 00:23:56,650
that's kind of my example let's imagine

00:23:52,960 --> 00:24:00,610
we have an event lock so it has a writer

00:23:56,650 --> 00:24:02,320
and which appends events in a linear

00:24:00,610 --> 00:24:04,540
fashion to the lock and we have a

00:24:02,320 --> 00:24:08,680
transformer that sort of tails the lock

00:24:04,540 --> 00:24:11,740
and transforms these events in place so

00:24:08,680 --> 00:24:15,670
probably really bad for events or

00:24:11,740 --> 00:24:18,190
systems but yeah of course transformer

00:24:15,670 --> 00:24:20,770
is never a head of the writer that that

00:24:18,190 --> 00:24:23,950
is the constraint you constantly be so

00:24:20,770 --> 00:24:25,399
an interface for that kind of stateful

00:24:23,950 --> 00:24:27,889
event

00:24:25,399 --> 00:24:29,870
looks like this right we have the right

00:24:27,889 --> 00:24:33,470
necks and the transformed necks which

00:24:29,870 --> 00:24:36,529
will pretty much take a function

00:24:33,470 --> 00:24:38,600
and apply it the event and if we if our

00:24:36,529 --> 00:24:40,610
events are represented as ins we can

00:24:38,600 --> 00:24:42,590
represent the states of this event lock

00:24:40,610 --> 00:24:48,080
in that fashion we have the writer

00:24:42,590 --> 00:24:49,940
position and the reader the transformer

00:24:48,080 --> 00:24:56,330
position and we store our events in a

00:24:49,940 --> 00:24:58,399
lock so if we have to make sure that

00:24:56,330 --> 00:25:01,789
this is actually running concurrently

00:24:58,399 --> 00:25:03,590
correctly because we have two separate

00:25:01,789 --> 00:25:06,769
threads one writing and one one

00:25:03,590 --> 00:25:10,220
transforming we can go for and reach for

00:25:06,769 --> 00:25:12,379
the the first thing that that that some

00:25:10,220 --> 00:25:14,090
might very well think of it's just

00:25:12,379 --> 00:25:16,100
synchronize it you know just synchronize

00:25:14,090 --> 00:25:17,659
these both two methods so like the right

00:25:16,100 --> 00:25:19,749
next method it's going to be a

00:25:17,659 --> 00:25:22,399
synchronized method it's going to take

00:25:19,749 --> 00:25:25,490
its gonna check for the constraints and

00:25:22,399 --> 00:25:28,129
then it's just going to write into the

00:25:25,490 --> 00:25:31,070
position incrementing the position as it

00:25:28,129 --> 00:25:34,039
goes and the transform next method is

00:25:31,070 --> 00:25:38,049
similar checks whether we are not kind

00:25:34,039 --> 00:25:40,879
of running ahead and applies the

00:25:38,049 --> 00:25:44,360
function to the event and writes it back

00:25:40,879 --> 00:25:47,899
into place so what can we do with such a

00:25:44,360 --> 00:25:50,149
thing with an event log events well we

00:25:47,899 --> 00:25:52,519
can go and delete all of our events just

00:25:50,149 --> 00:25:55,970
milling them out and you might think

00:25:52,519 --> 00:25:58,970
this is not a good idea but guess what

00:25:55,970 --> 00:26:01,129
GD RP comes around soon so probably

00:25:58,970 --> 00:26:05,600
probably some of some of us will have to

00:26:01,129 --> 00:26:08,779
do that even if they don't want so but

00:26:05,600 --> 00:26:10,309
how does that look in terms of I hope

00:26:08,779 --> 00:26:14,690
nobody does that we there are better

00:26:10,309 --> 00:26:16,669
ways to do that so how does that

00:26:14,690 --> 00:26:22,759
actually look in terms of performance oh

00:26:16,669 --> 00:26:26,629
wow that even moves it's if we if we so

00:26:22,759 --> 00:26:28,730
I went and measured that I did half a

00:26:26,629 --> 00:26:33,679
billion events sort of pipe through

00:26:28,730 --> 00:26:37,370
through this event lock and this is sort

00:26:33,679 --> 00:26:42,090
of the results we get right we have

00:26:37,370 --> 00:26:44,159
north of 13 million opps per second and

00:26:42,090 --> 00:26:48,600
we have significant amount of l2 cache

00:26:44,159 --> 00:26:51,960
misses and only 0.8 instructions per

00:26:48,600 --> 00:26:55,370
cycle so why might that be happening

00:26:51,960 --> 00:26:57,929
well plenty of reasons but

00:26:55,370 --> 00:27:00,419
synchronization costs quite a lot right

00:26:57,929 --> 00:27:03,029
and as a matter of fact there it also

00:27:00,419 --> 00:27:06,710
introduces quite a lot of contention not

00:27:03,029 --> 00:27:10,200
on our data but on the lock itself so

00:27:06,710 --> 00:27:12,120
transformer and and then in writer are

00:27:10,200 --> 00:27:13,950
contending on this on this lock all the

00:27:12,120 --> 00:27:17,490
time thereby introducing quite a lot of

00:27:13,950 --> 00:27:18,299
l2 cache misses can we do that in a

00:27:17,490 --> 00:27:20,610
better way

00:27:18,299 --> 00:27:23,100
yeah first of all we don't really need

00:27:20,610 --> 00:27:25,260
exclusive access here right we have

00:27:23,100 --> 00:27:27,389
threats writing to independent variables

00:27:25,260 --> 00:27:28,919
so only think we need this visibility

00:27:27,389 --> 00:27:32,880
right we need to ensure visibility

00:27:28,919 --> 00:27:35,250
across across threats and so therefore

00:27:32,880 --> 00:27:39,269
naturally we can reserve to vote a

00:27:35,250 --> 00:27:42,120
resort to vote out which as you all know

00:27:39,269 --> 00:27:48,179
it introduces happens before

00:27:42,120 --> 00:27:50,970
relationship on the JVM full fans and it

00:27:48,179 --> 00:27:53,940
ensures that the values that have been

00:27:50,970 --> 00:27:57,330
modified before are written to the l1

00:27:53,940 --> 00:28:00,419
cache contrary to what a lot of

00:27:57,330 --> 00:28:02,279
interviewers might think it doesn't

00:28:00,419 --> 00:28:04,380
really do value to be read from my

00:28:02,279 --> 00:28:10,260
memory it that's not what it does just

00:28:04,380 --> 00:28:12,330
make sure that the the value is visible

00:28:10,260 --> 00:28:14,909
in the anyone questions from then on the

00:28:12,330 --> 00:28:16,799
hardware protocols ensure that this is

00:28:14,909 --> 00:28:22,110
visible across our cache hierarchy and

00:28:16,799 --> 00:28:24,630
to other course as well and so using

00:28:22,110 --> 00:28:27,899
that we can achieve the following

00:28:24,630 --> 00:28:31,289
results we can see that we've

00:28:27,899 --> 00:28:37,610
significantly decreased our l2 cache

00:28:31,289 --> 00:28:40,049
misses and we have we have increased our

00:28:37,610 --> 00:28:41,190
operations per second and we have as a

00:28:40,049 --> 00:28:43,260
matter of fact increased our

00:28:41,190 --> 00:28:46,769
instructions per cycle as well which is

00:28:43,260 --> 00:28:49,559
a really good measure of the use of the

00:28:46,769 --> 00:28:51,650
amount of useful work we are doing why

00:28:49,559 --> 00:28:54,690
is that because

00:28:51,650 --> 00:28:57,270
now we are our CPUs will be busy

00:28:54,690 --> 00:29:03,960
retiring instructions instead of

00:28:57,270 --> 00:29:06,060
spending time waiting on waiting on any

00:29:03,960 --> 00:29:08,450
kind of artists arbitration to happen

00:29:06,060 --> 00:29:11,430
and waiting on fetching data from

00:29:08,450 --> 00:29:14,310
dedication and what not so this this

00:29:11,430 --> 00:29:16,310
will of course mean that it's good

00:29:14,310 --> 00:29:19,620
measure to see that you are actually

00:29:16,310 --> 00:29:22,010
using your time on the CPU a lot better

00:29:19,620 --> 00:29:24,810
so the higher you are pretty much the

00:29:22,010 --> 00:29:30,120
it's it's more useful work that you're

00:29:24,810 --> 00:29:33,870
doing so can we do something else about

00:29:30,120 --> 00:29:38,580
that particular system yes

00:29:33,870 --> 00:29:44,220
turns out we definitely can vote out

00:29:38,580 --> 00:29:48,480
albeit less expensive than locking it's

00:29:44,220 --> 00:29:50,580
still not very cheap so it's as I said

00:29:48,480 --> 00:29:55,890
on x86 I think it reduces the full

00:29:50,580 --> 00:29:58,530
memory fence so it's it's not cheap what

00:29:55,890 --> 00:30:02,630
can we do is we can use this little gem

00:29:58,530 --> 00:30:05,030
in Atomics called lazy set which is

00:30:02,630 --> 00:30:08,700
which is as a matter of fact quite

00:30:05,030 --> 00:30:14,460
respond quite discussion on the JVM

00:30:08,700 --> 00:30:17,250
mailing lists and it's it usually don't

00:30:14,460 --> 00:30:19,020
introduces a memory fence but the

00:30:17,250 --> 00:30:23,430
details around that are quite

00:30:19,020 --> 00:30:25,740
complicated it works well in particular

00:30:23,430 --> 00:30:28,470
cases such as when you have single

00:30:25,740 --> 00:30:31,910
writers and it's effectively a very very

00:30:28,470 --> 00:30:34,050
cheap compared to vote our

00:30:31,910 --> 00:30:39,330
synchronization primitive that that you

00:30:34,050 --> 00:30:41,430
can use in order to ensure and put some

00:30:39,330 --> 00:30:46,110
garden here some visibility that in some

00:30:41,430 --> 00:30:48,650
cases are enough and that being said if

00:30:46,110 --> 00:30:51,570
we change our called codes to use that

00:30:48,650 --> 00:30:54,330
we can see that we have significantly

00:30:51,570 --> 00:30:57,840
increased our operations per second and

00:30:54,330 --> 00:31:00,780
we have as well increased our the useful

00:30:57,840 --> 00:31:02,130
work that we are doing so have brought

00:31:00,780 --> 00:31:03,900
up our instructions per cycle

00:31:02,130 --> 00:31:06,810
significantly as well

00:31:03,900 --> 00:31:10,320
and this is just simply by using this

00:31:06,810 --> 00:31:15,630
particular piece of a functionality on

00:31:10,320 --> 00:31:18,150
the from the atomic library so we still

00:31:15,630 --> 00:31:23,490
have some other problems though that are

00:31:18,150 --> 00:31:27,840
pending in our tool that's direct

00:31:23,490 --> 00:31:31,770
competition of Kafka it's it's really

00:31:27,840 --> 00:31:35,280
the false sharing right we as I

00:31:31,770 --> 00:31:38,330
mentioned earlier l2 misses or like the

00:31:35,280 --> 00:31:41,520
presence of high number of l2 misses s

00:31:38,330 --> 00:31:43,980
kind of makes you think it's kind of the

00:31:41,520 --> 00:31:47,400
smell that leads you to this false

00:31:43,980 --> 00:31:49,590
sharing problem and you have probably

00:31:47,400 --> 00:31:51,660
all heard about the this problem the

00:31:49,590 --> 00:31:53,640
false sharing it's when two threads are

00:31:51,660 --> 00:31:55,740
modifying independent variables that

00:31:53,640 --> 00:32:00,360
happen to live on the same cache line

00:31:55,740 --> 00:32:04,770
and because our because our caches are

00:32:00,360 --> 00:32:09,350
always kept coherent at these cache

00:32:04,770 --> 00:32:15,240
lines when modified are invalidated so

00:32:09,350 --> 00:32:18,420
in this particular case our writer when

00:32:15,240 --> 00:32:21,390
when it writes to the writer position

00:32:18,420 --> 00:32:24,030
variable it will invalidate this cache

00:32:21,390 --> 00:32:26,490
line that that that is shared with the

00:32:24,030 --> 00:32:27,840
reader position and then well with the

00:32:26,490 --> 00:32:30,210
transformer position when the

00:32:27,840 --> 00:32:32,450
transformer goes in the reads that it's

00:32:30,210 --> 00:32:37,410
its own position in order to determine

00:32:32,450 --> 00:32:40,950
well what to do it's gonna go and fetch

00:32:37,410 --> 00:32:47,490
this data from somewhere else but not

00:32:40,950 --> 00:32:50,850
its most close to it a place in l1 cache

00:32:47,490 --> 00:32:52,830
for example and this is this is this is

00:32:50,850 --> 00:32:56,000
not that this is good that this is the

00:32:52,830 --> 00:32:58,590
error because it's it provides us with

00:32:56,000 --> 00:33:02,430
guarantees that our concurrent problems

00:32:58,590 --> 00:33:06,600
will run correctly and what what is

00:33:02,430 --> 00:33:08,940
ensured what what helps that guarantee

00:33:06,600 --> 00:33:11,970
is the underlying hardware protocols

00:33:08,940 --> 00:33:14,760
that are that are implemented in our

00:33:11,970 --> 00:33:16,440
CPUs so that's kind of one of them or a

00:33:14,760 --> 00:33:17,400
variation thereof it's the macey

00:33:16,440 --> 00:33:19,740
protocol

00:33:17,400 --> 00:33:22,440
that's pretty much finite state machine

00:33:19,740 --> 00:33:26,780
putting every cash line into into into

00:33:22,440 --> 00:33:30,510
one of four states and depending on that

00:33:26,780 --> 00:33:32,970
our CPUs know whether they can use

00:33:30,510 --> 00:33:35,070
what's in l1 cache or they need to go

00:33:32,970 --> 00:33:37,020
and snoop on the bus in order to get the

00:33:35,070 --> 00:33:38,940
freshest value from some other core or

00:33:37,020 --> 00:33:41,280
you need to resort to an access to the

00:33:38,940 --> 00:33:42,780
l3 cache which is the costly one that's

00:33:41,280 --> 00:33:46,980
why I mentioned earlier that the l3

00:33:42,780 --> 00:33:51,059
cache is sort of it's sort of the fabric

00:33:46,980 --> 00:33:53,730
that most oftenly is ensuring that

00:33:51,059 --> 00:33:55,200
hearings of our cache system so this is

00:33:53,730 --> 00:34:01,530
what pretty much ensures visibility

00:33:55,200 --> 00:34:04,710
right across threats and it's again it's

00:34:01,530 --> 00:34:07,520
it's it's not cheap so but our

00:34:04,710 --> 00:34:12,030
particular problem and this this case is

00:34:07,520 --> 00:34:17,659
the following right we are our cache

00:34:12,030 --> 00:34:20,550
lines are usually 64 bytes right and

00:34:17,659 --> 00:34:23,250
they're very dependent on our object

00:34:20,550 --> 00:34:28,679
layout so going back to the volatile

00:34:23,250 --> 00:34:31,379
example we are we are we are putting two

00:34:28,679 --> 00:34:33,389
variables side by side and it's very

00:34:31,379 --> 00:34:36,899
very likely that they will end up on the

00:34:33,389 --> 00:34:39,389
same cache line and as I said this means

00:34:36,899 --> 00:34:45,149
that this cache line will be invalidated

00:34:39,389 --> 00:34:48,030
once either the transformer or the

00:34:45,149 --> 00:34:49,919
writer writes to it and this is not

00:34:48,030 --> 00:34:53,370
great because ultimately what's going to

00:34:49,919 --> 00:34:53,820
happen is that our data will be ping

00:34:53,370 --> 00:34:56,940
ponging

00:34:53,820 --> 00:34:59,160
across the l3 cache which incurs

00:34:56,940 --> 00:35:01,550
significant latency instead of being

00:34:59,160 --> 00:35:06,570
kept in there long caches and and and

00:35:01,550 --> 00:35:11,810
and resorting to this really really

00:35:06,570 --> 00:35:14,810
quick sub nano psycho sub nanosecond

00:35:11,810 --> 00:35:18,180
accesses in certain in certain scenarios

00:35:14,810 --> 00:35:20,790
so how can we actually ensure that well

00:35:18,180 --> 00:35:22,410
this is just it's not always the case

00:35:20,790 --> 00:35:25,730
pretty much in the discretion of the

00:35:22,410 --> 00:35:30,770
compiler did it to do to do to

00:35:25,730 --> 00:35:33,530
distribute these things but in order to

00:35:30,770 --> 00:35:37,190
I actually get a further insight into

00:35:33,530 --> 00:35:42,650
what's what's happening is that we can

00:35:37,190 --> 00:35:45,800
we can use to like the SBT Joe from tow

00:35:42,650 --> 00:35:48,620
so that gives us a really nice way of

00:35:45,800 --> 00:35:50,510
looking at the Java object layout of our

00:35:48,620 --> 00:35:53,330
objects and kind of think and reason

00:35:50,510 --> 00:35:56,060
about these things whether we are we

00:35:53,330 --> 00:35:59,090
might potentially introduce such kind of

00:35:56,060 --> 00:36:01,150
false sharing behavior and what can we

00:35:59,090 --> 00:36:05,470
do about that well back in the day

00:36:01,150 --> 00:36:07,700
people would go and use things like

00:36:05,470 --> 00:36:09,470
explicit padding where they would put

00:36:07,700 --> 00:36:12,800
more Long's between between the lines

00:36:09,470 --> 00:36:17,030
just to pass their variables and this is

00:36:12,800 --> 00:36:19,700
and this is this has been done in quite

00:36:17,030 --> 00:36:21,860
a few places such as you know in quite a

00:36:19,700 --> 00:36:24,560
few scenarios like so you can pad your

00:36:21,860 --> 00:36:27,110
slots in an array or like a ring buffer

00:36:24,560 --> 00:36:31,370
or very simply you can pat your head

00:36:27,110 --> 00:36:33,950
until pointers in a queue but thankfully

00:36:31,370 --> 00:36:38,290
nowadays we have a better way to do it

00:36:33,950 --> 00:36:40,670
we have all sorts of concurrent

00:36:38,290 --> 00:36:44,510
libraries that can help us with pairing

00:36:40,670 --> 00:36:49,430
so for example that's a one that I say I

00:36:44,510 --> 00:36:51,140
used it's you can create a long that's

00:36:49,430 --> 00:36:54,500
palette right you cannot create an

00:36:51,140 --> 00:36:56,090
atomic long that is padded and you can

00:36:54,500 --> 00:36:58,460
parrot left right

00:36:56,090 --> 00:37:00,680
to ensure ultimate security because if

00:36:58,460 --> 00:37:02,480
you only pad between the two between the

00:37:00,680 --> 00:37:04,250
two variables

00:37:02,480 --> 00:37:06,650
you're ensuring that they are not gonna

00:37:04,250 --> 00:37:10,460
sort of ensure for sharing

00:37:06,650 --> 00:37:12,290
well cause for sharing to each other

00:37:10,460 --> 00:37:16,430
but you never know where they're

00:37:12,290 --> 00:37:18,380
actually gonna end up and what cache

00:37:16,430 --> 00:37:22,160
line will be right above them for

00:37:18,380 --> 00:37:24,380
example so it's important to Pat them

00:37:22,160 --> 00:37:27,490
both ways it's at least it gives you

00:37:24,380 --> 00:37:30,470
quite a lot more security around

00:37:27,490 --> 00:37:34,670
avoiding false sharing so that being

00:37:30,470 --> 00:37:42,680
said let's look at what this gives us

00:37:34,670 --> 00:37:47,029
right so we have decreased our l2 misses

00:37:42,680 --> 00:37:48,319
and we have increased our operations per

00:37:47,029 --> 00:37:51,529
second one once again

00:37:48,319 --> 00:37:53,720
and of course we have also increased our

00:37:51,529 --> 00:37:57,200
instructions per cycle because now

00:37:53,720 --> 00:38:00,410
you're not wasting cycles on waiting for

00:37:57,200 --> 00:38:01,940
and paying that latency to l3 caches

00:38:00,410 --> 00:38:04,130
we're simply getting the data that's

00:38:01,940 --> 00:38:06,410
close to us and we know that it's not

00:38:04,130 --> 00:38:10,039
that the cache line is not invalidated

00:38:06,410 --> 00:38:13,309
right so we're we're we're decreasing

00:38:10,039 --> 00:38:16,249
the latency to slower levels of the

00:38:13,309 --> 00:38:18,440
cache we are also decreasing something

00:38:16,249 --> 00:38:19,069
that can also be measured but it's a bit

00:38:18,440 --> 00:38:23,779
more involved

00:38:19,069 --> 00:38:25,220
it's the coherency protocol traffic now

00:38:23,779 --> 00:38:26,980
this is more efficient but back in the

00:38:25,220 --> 00:38:29,890
day I think it used to share the same

00:38:26,980 --> 00:38:33,349
the same sort of fabric that that is

00:38:29,890 --> 00:38:35,930
that is with the loads and stores to the

00:38:33,349 --> 00:38:40,400
K subsystem so this could also increase

00:38:35,930 --> 00:38:45,140
your will decrease your performance all

00:38:40,400 --> 00:38:49,460
of that being said it's important to

00:38:45,140 --> 00:38:51,920
look at your use cases and think about

00:38:49,460 --> 00:38:55,339
the concurrency primitives you are using

00:38:51,920 --> 00:38:57,049
and really pay attention to what they

00:38:55,339 --> 00:38:59,450
are doing how they are doing it in the

00:38:57,049 --> 00:39:02,900
hood and if you're not sure measure and

00:38:59,450 --> 00:39:06,289
measure and measure in order to get the

00:39:02,900 --> 00:39:10,430
best insight so that being said let's

00:39:06,289 --> 00:39:14,359
take a look at a real use case so it's

00:39:10,430 --> 00:39:19,549
really something that's implemented

00:39:14,359 --> 00:39:20,119
currently in a live system it's it's in

00:39:19,549 --> 00:39:23,809
Accra

00:39:20,119 --> 00:39:25,489
right so how does the message lifecycle

00:39:23,809 --> 00:39:27,950
looks like well we have a sender that

00:39:25,489 --> 00:39:29,720
kind of sends a message to an actor ref

00:39:27,950 --> 00:39:33,019
and then the dispatcher is responsible

00:39:29,720 --> 00:39:35,359
putting that onto a cue onto the the

00:39:33,019 --> 00:39:37,819
mailbox and this mailbox is run by the

00:39:35,359 --> 00:39:40,369
dispatcher itself which is backed by an

00:39:37,819 --> 00:39:42,920
executor service in most cases that has

00:39:40,369 --> 00:39:44,660
a bunch of threats that so what are the

00:39:42,920 --> 00:39:48,410
types of the dispatchers we have a

00:39:44,660 --> 00:39:50,930
default dispatcher that is sort of a

00:39:48,410 --> 00:39:52,640
default one that you kind of use when

00:39:50,930 --> 00:39:54,859
nothing else specified then we have the

00:39:52,640 --> 00:39:56,000
pinned one and then we have a calling

00:39:54,859 --> 00:39:58,280
three dispatcher which is all

00:39:56,000 --> 00:40:00,380
use for tests and then the executors

00:39:58,280 --> 00:40:04,300
that are most frequently use the for

00:40:00,380 --> 00:40:06,910
joint pool and thread pool executors and

00:40:04,300 --> 00:40:09,740
if we look at the thread pool executors

00:40:06,910 --> 00:40:11,690
it would look like this right we have an

00:40:09,740 --> 00:40:14,870
external component submitting a runnable

00:40:11,690 --> 00:40:16,040
in this case the mailbox relay and then

00:40:14,870 --> 00:40:19,670
we have a bunch of threads that are

00:40:16,040 --> 00:40:21,710
picking up that are picking up the work

00:40:19,670 --> 00:40:23,750
and running it and this is a link

00:40:21,710 --> 00:40:24,980
blocking queue what are some of the

00:40:23,750 --> 00:40:27,830
limitations of that

00:40:24,980 --> 00:40:31,700
well our Hardware underneath ensures

00:40:27,830 --> 00:40:33,980
well or at least tries to may do its

00:40:31,700 --> 00:40:37,820
best in order to ensure cache affinity

00:40:33,980 --> 00:40:42,350
so it would place threads to run on

00:40:37,820 --> 00:40:45,730
course based on whether they run on

00:40:42,350 --> 00:40:49,070
these cores before at least we'll try -

00:40:45,730 --> 00:40:52,330
that being said one of the limitations

00:40:49,070 --> 00:40:56,090
is that this these systems completely

00:40:52,330 --> 00:40:59,090
well don't take that into advantage so

00:40:56,090 --> 00:41:01,340
there is no actor to thread affinity so

00:40:59,090 --> 00:41:04,460
one actor beingness being a stateful

00:41:01,340 --> 00:41:07,640
creature at will as a matter of fact not

00:41:04,460 --> 00:41:09,980
run on the same thread that it used to

00:41:07,640 --> 00:41:11,960
most likely because of the sort of

00:41:09,980 --> 00:41:15,470
producer/consumer nature of these of

00:41:11,960 --> 00:41:19,130
these underlying thread pools that are

00:41:15,470 --> 00:41:20,750
ticking the actor system so this can

00:41:19,130 --> 00:41:23,600
potentially cause quite a lot of CPU

00:41:20,750 --> 00:41:27,580
cache invalidation and also there is no

00:41:23,600 --> 00:41:31,160
there is no actual way to control the

00:41:27,580 --> 00:41:33,260
the details of how this act of the

00:41:31,160 --> 00:41:35,120
thread mapping is happening so an

00:41:33,260 --> 00:41:37,880
alternative proposal that recently got

00:41:35,120 --> 00:41:43,040
well not so recently but yeah got into

00:41:37,880 --> 00:41:45,350
the ARCA is the affinity pool which uses

00:41:43,040 --> 00:41:50,210
a component called the cue selector that

00:41:45,350 --> 00:41:53,750
based on the actor it will director its

00:41:50,210 --> 00:41:56,270
mailbox it will actually go and pick one

00:41:53,750 --> 00:42:00,770
of the cues that is associated with each

00:41:56,270 --> 00:42:02,990
thread and submit it to it so that gives

00:42:00,770 --> 00:42:05,180
you quite a bit of things it's well it

00:42:02,990 --> 00:42:06,860
gives you the affinity and the

00:42:05,180 --> 00:42:09,720
stickiness of an actor to particular

00:42:06,860 --> 00:42:11,190
thread it also gives you the fact that

00:42:09,720 --> 00:42:15,140
you you know compared to a thread pool

00:42:11,190 --> 00:42:19,520
executor for example you're using strip

00:42:15,140 --> 00:42:21,660
strip walk free cues in order to

00:42:19,520 --> 00:42:25,859
distribute the work so you're decreasing

00:42:21,660 --> 00:42:29,670
contention on that work and handing off

00:42:25,859 --> 00:42:31,830
fabric quite a bit currently the default

00:42:29,670 --> 00:42:34,440
queue selector that's implemented is the

00:42:31,830 --> 00:42:36,660
fair distribution cue selector which has

00:42:34,440 --> 00:42:38,820
an adaptive work assignment strategy

00:42:36,660 --> 00:42:41,280
which means that if you have few actors

00:42:38,820 --> 00:42:42,900
it's gonna do sort of explicit mapping

00:42:41,280 --> 00:42:45,090
that is fair so it's going to maintain

00:42:42,900 --> 00:42:47,609
state of like which actor matches to

00:42:45,090 --> 00:42:48,869
which thread and if you have more actors

00:42:47,609 --> 00:42:51,359
it's going to do consistent hashing

00:42:48,869 --> 00:42:53,970
relying on the statistical distribution

00:42:51,359 --> 00:43:01,050
of the disk of fairness of the hash in

00:42:53,970 --> 00:43:03,270
order to to kind of do that so what are

00:43:01,050 --> 00:43:06,150
the advantages less cache hits due to

00:43:03,270 --> 00:43:08,190
temporal locality decreased contention

00:43:06,150 --> 00:43:10,410
and of course customizable queue

00:43:08,190 --> 00:43:13,650
selection and a benchmark that was

00:43:10,410 --> 00:43:17,609
recently and that is in accra that you

00:43:13,650 --> 00:43:22,020
can run on this gives you the following

00:43:17,609 --> 00:43:23,910
results so for certain number of

00:43:22,020 --> 00:43:26,970
throughputs it's it's gonna it's gonna

00:43:23,910 --> 00:43:30,750
actually show you that in certain

00:43:26,970 --> 00:43:33,869
scenarios the affinity poo is increasing

00:43:30,750 --> 00:43:36,990
performance a little bit just because of

00:43:33,869 --> 00:43:39,810
the temporal locality of data so just

00:43:36,990 --> 00:43:41,820
summarize all of that if you're on the

00:43:39,810 --> 00:43:46,800
hot path really make sure you are and

00:43:41,820 --> 00:43:50,700
measure measure everything use SBT gmh

00:43:46,800 --> 00:43:53,270
Java object layout for stats and watch

00:43:50,700 --> 00:43:55,770
out for features of the language that

00:43:53,270 --> 00:43:58,109
increase object instantiation such as

00:43:55,770 --> 00:44:00,260
pattern matching use algorithms and data

00:43:58,109 --> 00:44:03,540
structures that are cache friendly and

00:44:00,260 --> 00:44:05,780
just use efficient concurrency tools but

00:44:03,540 --> 00:44:08,520
try not to roll your own because it's

00:44:05,780 --> 00:44:10,530
it's risky so there are tools like GC to

00:44:08,520 --> 00:44:14,790
socket vertex that can help you with all

00:44:10,530 --> 00:44:16,619
that stuff and these are the resources

00:44:14,790 --> 00:44:18,990
where you can read a lot more about all

00:44:16,619 --> 00:44:21,450
that stuff and the slides and code will

00:44:18,990 --> 00:44:23,369
be uploaded soon as well on my github

00:44:21,450 --> 00:44:24,900
account or thanks everyone

00:44:23,369 --> 00:44:33,420
for the attention

00:44:24,900 --> 00:44:41,940
[Applause]

00:44:33,420 --> 00:44:47,990
think so I don't know whether there is

00:44:41,940 --> 00:44:47,990
time for any questions or sure

00:44:57,850 --> 00:45:08,570
one question there hello you mentioned

00:45:04,430 --> 00:45:12,109
earlier that some interviewers are like

00:45:08,570 --> 00:45:15,440
sure about some some misconception about

00:45:12,109 --> 00:45:17,810
caching and the way the volatile work

00:45:15,440 --> 00:45:21,470
can you elaborate on this a little bit

00:45:17,810 --> 00:45:25,310
more so right did I hear very well so

00:45:21,470 --> 00:45:27,230
the question was about area in the

00:45:25,310 --> 00:45:29,660
beginning of your presentation you

00:45:27,230 --> 00:45:33,290
mentioned something about interviewers

00:45:29,660 --> 00:45:37,070
that like know how the volatile work but

00:45:33,290 --> 00:45:39,920
actually they don't can you like add

00:45:37,070 --> 00:45:42,590
some details here yeah well yeah that's

00:45:39,920 --> 00:45:44,480
why she was a joke but like I remember

00:45:42,590 --> 00:45:46,580
when I was interviewing for Java roles

00:45:44,480 --> 00:45:48,560
like back in the day when I was still in

00:45:46,580 --> 00:45:50,000
school they would like the

00:45:48,560 --> 00:45:51,890
run-of-the-mill question will be like oh

00:45:50,000 --> 00:45:56,869
yes what's vote out can you explain what

00:45:51,890 --> 00:45:59,300
violet iOS and and I have heard these

00:45:56,869 --> 00:46:00,619
explanations for them when I couldn't

00:45:59,300 --> 00:46:03,650
answer because there were times like

00:46:00,619 --> 00:46:05,900
that as well that well you know it

00:46:03,650 --> 00:46:09,280
ensures that you have thread visibility

00:46:05,900 --> 00:46:13,400
and by fetching stuff from my memory and

00:46:09,280 --> 00:46:15,830
that's what I was referring to that you

00:46:13,400 --> 00:46:17,180
know it's it's to me that's not that's

00:46:15,830 --> 00:46:19,460
it's not the whole truth

00:46:17,180 --> 00:46:24,250
right it's it's not ensuring that it

00:46:19,460 --> 00:46:24,250
fetches data from my memory

00:46:29,369 --> 00:46:34,809
okay so and sends it okay you just

00:46:32,049 --> 00:46:37,779
mentioned that yeah just a story he's

00:46:34,809 --> 00:46:39,309
just an animal all right so what well I

00:46:37,779 --> 00:46:44,019
don't think all of them are like that

00:46:39,309 --> 00:46:44,829
it's see I work with Matias he didn't

00:46:44,019 --> 00:46:48,339
ask me that question

00:46:44,829 --> 00:46:52,359
hehe so but yeah

00:46:48,339 --> 00:46:55,119
a deer we get one more question here in

00:46:52,359 --> 00:46:57,519
the front yeah hi if the tools you

00:46:55,119 --> 00:47:00,549
mentioned do they also help me to see

00:46:57,519 --> 00:47:02,859
how my objects layout in memory so I can

00:47:00,549 --> 00:47:06,390
maybe transform them to be more

00:47:02,859 --> 00:47:12,519
efficient object layout in memory yeah I

00:47:06,390 --> 00:47:14,769
think like if you use any kind of more

00:47:12,519 --> 00:47:16,119
sophisticated tools like intel vtune and

00:47:14,769 --> 00:47:18,729
that kind of stuff you can you can look

00:47:16,119 --> 00:47:22,779
at that here but usually these things

00:47:18,729 --> 00:47:24,969
have kind of high learning curve and

00:47:22,779 --> 00:47:31,869
commercial solutions for the most part

00:47:24,969 --> 00:47:33,789
so but yeah yeah they are tools yeah Jol

00:47:31,869 --> 00:47:36,309
is great as I said this is this you can

00:47:33,789 --> 00:47:40,959
reason quite what about the stuff that

00:47:36,309 --> 00:47:44,259
you see there so yeah sorry my go-to to

00:47:40,959 --> 00:47:47,799
to sort of get a real kind of an

00:47:44,259 --> 00:47:50,499
aesthetic what's happening ok so see ya

00:47:47,799 --> 00:47:52,569
so since drinks and food is waiting

00:47:50,499 --> 00:47:56,549
outside well I suggested a all other

00:47:52,569 --> 00:47:56,549

YouTube URL: https://www.youtube.com/watch?v=Zf4QXqIdNF8


