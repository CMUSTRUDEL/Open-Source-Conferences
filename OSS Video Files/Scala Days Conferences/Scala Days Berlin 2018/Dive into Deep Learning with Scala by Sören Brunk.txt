Title: Dive into Deep Learning with Scala by SÃ¶ren Brunk
Publication date: 2018-09-20
Playlist: Scala Days Berlin 2018
Description: 
	This video was recorded at Scala Days Berlin 2018
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

More information and the abstract can be found here:
https://eu.scaladays.org/lect-6912-dive-into-deep-learning-with-scala.html
Captions: 
	00:00:04,850 --> 00:00:09,180
today I want to share with you some of

00:00:07,080 --> 00:00:13,770
the things I've learned there so let's

00:00:09,180 --> 00:00:15,629
dive in a is a new electricity says

00:00:13,770 --> 00:00:19,650
entering a while known deep learning

00:00:15,629 --> 00:00:22,410
pioneer what does he mean by that by the

00:00:19,650 --> 00:00:23,910
end of the 19th century electricity was

00:00:22,410 --> 00:00:25,589
one of the main drivers behind the

00:00:23,910 --> 00:00:27,630
Second Industrial Revolution that

00:00:25,589 --> 00:00:29,730
completely transformed many industries

00:00:27,630 --> 00:00:33,540
like manufacturing communication and

00:00:29,730 --> 00:00:34,800
more and it eventually led to

00:00:33,540 --> 00:00:37,350
electronics and computers and

00:00:34,800 --> 00:00:40,530
electricity literally powers a modern

00:00:37,350 --> 00:00:42,990
life so what Andrew is saying is that

00:00:40,530 --> 00:00:45,270
much like electricity has revolutionized

00:00:42,990 --> 00:00:49,200
all these industries and our lives say I

00:00:45,270 --> 00:00:51,120
will have a similar impact now most of

00:00:49,200 --> 00:00:53,220
the current progress happening in AI is

00:00:51,120 --> 00:00:55,530
coming from a specific sub field and

00:00:53,220 --> 00:00:58,170
that's deep learning and deep learning

00:00:55,530 --> 00:01:00,330
is not just a hype it's moving fast from

00:00:58,170 --> 00:01:02,070
being mostly a research field towards

00:01:00,330 --> 00:01:05,189
massive adoption and all kinds of

00:01:02,070 --> 00:01:07,259
domains so you might be asking yourself

00:01:05,189 --> 00:01:10,320
how is deep learning going to affect me

00:01:07,259 --> 00:01:13,499
as a developer am I going to be replaced

00:01:10,320 --> 00:01:15,359
by some self writing software or on the

00:01:13,499 --> 00:01:17,299
positive side can I benefit from deep

00:01:15,359 --> 00:01:19,590
learning in my day to day work and

00:01:17,299 --> 00:01:20,789
perhaps you've also seen some of the

00:01:19,590 --> 00:01:23,600
cool things you can do with deep

00:01:20,789 --> 00:01:25,890
learning and that got you interested

00:01:23,600 --> 00:01:28,229
perhaps you have a project idea you want

00:01:25,890 --> 00:01:30,630
to try or you just want to play with it

00:01:28,229 --> 00:01:32,340
see what's possible maybe you already

00:01:30,630 --> 00:01:34,469
have some machine learning experience

00:01:32,340 --> 00:01:37,409
but you want to see what's possible with

00:01:34,469 --> 00:01:39,240
Scala whatever the reason I think it's

00:01:37,409 --> 00:01:43,409
an exciting time to get into deep

00:01:39,240 --> 00:01:45,869
learning but getting started can be hard

00:01:43,409 --> 00:01:48,240
there's a lot of jargon a lot of

00:01:45,869 --> 00:01:50,219
different libraries most of them are

00:01:48,240 --> 00:01:52,079
research focused and written in Python

00:01:50,219 --> 00:01:54,959
and deep learning as an engineering

00:01:52,079 --> 00:01:57,630
discipline is still young so it's

00:01:54,959 --> 00:01:59,009
lacking best practices so you might be

00:01:57,630 --> 00:02:01,459
wondering what's a good way to get

00:01:59,009 --> 00:02:03,869
started and whether you need a PhD in

00:02:01,459 --> 00:02:07,799
statistics or machine learning or some

00:02:03,869 --> 00:02:09,720
similar field and perhaps what you're

00:02:07,799 --> 00:02:12,150
hoping for is more something like this a

00:02:09,720 --> 00:02:14,610
nice abstraction that allows you to use

00:02:12,150 --> 00:02:17,180
the technology without having to know

00:02:14,610 --> 00:02:20,269
all the details behind it and

00:02:17,180 --> 00:02:22,189
and for software that usually translates

00:02:20,269 --> 00:02:24,920
into having a library but hides those

00:02:22,189 --> 00:02:27,010
details behind a nice interface couldn't

00:02:24,920 --> 00:02:30,110
we have that for deep learning as well

00:02:27,010 --> 00:02:31,909
well there's no free lunch but the good

00:02:30,110 --> 00:02:33,650
news is you don't have to learn all the

00:02:31,909 --> 00:02:35,900
theory to get started with deep learning

00:02:33,650 --> 00:02:38,569
from for many tasks it's enough to know

00:02:35,900 --> 00:02:41,180
the basics and you can always learn more

00:02:38,569 --> 00:02:42,620
advanced concepts along the way and it's

00:02:41,180 --> 00:02:44,959
totally possible to do deep learning

00:02:42,620 --> 00:02:46,549
with Scala and to help you with that I'm

00:02:44,959 --> 00:02:49,730
going to introduce deep learning in

00:02:46,549 --> 00:02:52,129
three stairs today first I'm gonna show

00:02:49,730 --> 00:02:54,260
you the core building blocks the

00:02:52,129 --> 00:02:56,870
essentials of deep learning then we'll

00:02:54,260 --> 00:03:01,220
look at Scala as a language or building

00:02:56,870 --> 00:03:03,019
deep learning systems and finally we go

00:03:01,220 --> 00:03:05,030
through the actual steps required to

00:03:03,019 --> 00:03:06,590
build such a system and of course we'll

00:03:05,030 --> 00:03:10,849
apply these steps in a simple example

00:03:06,590 --> 00:03:12,590
using Scala so let's plant the first

00:03:10,849 --> 00:03:16,400
stair understand what deep learning is

00:03:12,590 --> 00:03:18,139
at its core when you talk about deep

00:03:16,400 --> 00:03:20,900
learning we usually mean artificial

00:03:18,139 --> 00:03:23,030
neural networks no networks have been

00:03:20,900 --> 00:03:25,040
around for decades but deep learning is

00:03:23,030 --> 00:03:27,500
a newer term that was originally used

00:03:25,040 --> 00:03:29,329
for larger deeper networks but now

00:03:27,500 --> 00:03:30,859
basically all neural networks are kind

00:03:29,329 --> 00:03:32,739
of deep so we use those terms

00:03:30,859 --> 00:03:35,569
interchangeably

00:03:32,739 --> 00:03:38,540
but what is an artificial neural network

00:03:35,569 --> 00:03:41,569
well first of all it's not some all of

00:03:38,540 --> 00:03:44,900
our biological brain it's loosely

00:03:41,569 --> 00:03:47,560
inspired by the brain like airplanes are

00:03:44,900 --> 00:03:50,690
inspired by birds but that's all

00:03:47,560 --> 00:03:52,340
but what what is it then to gain some

00:03:50,690 --> 00:03:56,750
intuition let's look at a few examples

00:03:52,340 --> 00:03:58,459
first computer vision is one of the

00:03:56,750 --> 00:04:00,169
first fields where neural networks have

00:03:58,459 --> 00:04:02,739
been applied successfully back in the

00:04:00,169 --> 00:04:05,479
90s but the big breakthrough came in

00:04:02,739 --> 00:04:07,849
2012 when researchers use deep learning

00:04:05,479 --> 00:04:11,000
and challenge for image recognition

00:04:07,849 --> 00:04:13,549
algorithms and smashed their competition

00:04:11,000 --> 00:04:15,440
and computer vision is still one of the

00:04:13,549 --> 00:04:17,570
most active fields and deep learning

00:04:15,440 --> 00:04:19,900
research with many applications ranging

00:04:17,570 --> 00:04:22,280
from medicine to self-driving vehicles

00:04:19,900 --> 00:04:24,470
here we see an example of object

00:04:22,280 --> 00:04:27,380
detection the inputs are raw pixels of

00:04:24,470 --> 00:04:31,070
an image and the neural network predicts

00:04:27,380 --> 00:04:34,430
a set of bounding boxes together with

00:04:31,070 --> 00:04:36,680
and a confidence score so this

00:04:34,430 --> 00:04:39,560
particular network has clearly

00:04:36,680 --> 00:04:41,270
identified my daughter as a person it's

00:04:39,560 --> 00:04:46,670
also quite confident that she's sitting

00:04:41,270 --> 00:04:48,470
on a motorcycle another very successful

00:04:46,670 --> 00:04:50,960
domain for deep learning is speech

00:04:48,470 --> 00:04:52,840
recognition given audio as input a

00:04:50,960 --> 00:04:55,190
neural network predicts the spoken text

00:04:52,840 --> 00:04:58,100
the most visible application of all

00:04:55,190 --> 00:05:01,070
these voices systems like serial X and

00:04:58,100 --> 00:05:03,650
so on they all use deep learning to

00:05:01,070 --> 00:05:07,940
recognize your voice and the recognition

00:05:03,650 --> 00:05:09,590
part actually works quite well the last

00:05:07,940 --> 00:05:11,780
example I want to show you is machine

00:05:09,590 --> 00:05:13,850
translation which has seen also seen

00:05:11,780 --> 00:05:16,370
tremendous improvements over the past

00:05:13,850 --> 00:05:18,410
years you receive the description from

00:05:16,370 --> 00:05:20,540
the Scala based website translated to

00:05:18,410 --> 00:05:23,570
German using state-of-the-art deep

00:05:20,540 --> 00:05:25,430
learning techniques it's not perfect but

00:05:23,570 --> 00:05:26,600
if you understand German you'll agree

00:05:25,430 --> 00:05:31,430
that it's still a pretty good

00:05:26,600 --> 00:05:33,680
translation so what do all these

00:05:31,430 --> 00:05:36,230
examples have in common well they're

00:05:33,680 --> 00:05:38,330
multiple things they are all tasks that

00:05:36,230 --> 00:05:41,660
are generally easy to solve for humans

00:05:38,330 --> 00:05:43,850
but difficult for computers they all

00:05:41,660 --> 00:05:46,430
deal with unstructured data like images

00:05:43,850 --> 00:05:47,780
audio or text but that doesn't mean the

00:05:46,430 --> 00:05:49,460
deep learning can be applied to

00:05:47,780 --> 00:05:51,340
structured data quite on the contrary

00:05:49,460 --> 00:05:54,470
it's used with great success for

00:05:51,340 --> 00:05:58,580
recommendations and advertising and so

00:05:54,470 --> 00:06:00,620
on but there's another thing and to see

00:05:58,580 --> 00:06:04,480
it more clearly let's overlay our

00:06:00,620 --> 00:06:07,610
examples and remove the differences

00:06:04,480 --> 00:06:10,520
right it's the arrow all the examples

00:06:07,610 --> 00:06:12,560
map some input to an output the input

00:06:10,520 --> 00:06:15,280
and output types on the other hand are

00:06:12,560 --> 00:06:18,380
fairly different we can map images to

00:06:15,280 --> 00:06:21,640
classes raw audio to text or text from

00:06:18,380 --> 00:06:27,760
one language to to another language so

00:06:21,640 --> 00:06:31,190
let's make them generic what do we get

00:06:27,760 --> 00:06:32,930
right a function and it's pure it has no

00:06:31,190 --> 00:06:35,840
side effects and it will always return

00:06:32,930 --> 00:06:37,700
the same result for the same input now

00:06:35,840 --> 00:06:39,740
that might seem obvious but I think it's

00:06:37,700 --> 00:06:41,810
quite important to realise what it means

00:06:39,740 --> 00:06:43,460
in terms of the generalization power of

00:06:41,810 --> 00:06:44,370
neural networks it means that we can use

00:06:43,460 --> 00:06:46,260
them for many

00:06:44,370 --> 00:06:50,820
different tasks in many different

00:06:46,260 --> 00:06:53,610
domains let's be a little bit more

00:06:50,820 --> 00:06:55,800
precise with terminology in machine

00:06:53,610 --> 00:06:57,900
learning a model is something we can use

00:06:55,800 --> 00:06:59,940
to make predictions in other words we

00:06:57,900 --> 00:07:04,350
can apply it to our input to get a

00:06:59,940 --> 00:07:06,870
result back so for instance we could

00:07:04,350 --> 00:07:09,300
have a classifier for images which is an

00:07:06,870 --> 00:07:11,729
instance of a model that map's an input

00:07:09,300 --> 00:07:15,389
image to a class representing the

00:07:11,729 --> 00:07:18,229
content of that image but how do we get

00:07:15,389 --> 00:07:21,060
there how do we implement that function

00:07:18,229 --> 00:07:23,460
and to find out we have to look inside

00:07:21,060 --> 00:07:27,780
the black box to see what what a neural

00:07:23,460 --> 00:07:30,090
network actually does internally neural

00:07:27,780 --> 00:07:31,699
networks operate on so-called tensors

00:07:30,090 --> 00:07:34,560
pendulous are basically just

00:07:31,699 --> 00:07:36,660
multi-dimensional numeric arrays the

00:07:34,560 --> 00:07:38,789
main reason to use tensors is efficiency

00:07:36,660 --> 00:07:41,520
we can reduce most neural network

00:07:38,789 --> 00:07:44,160
operations to matrix multiplications and

00:07:41,520 --> 00:07:45,960
other linear algebra operations and we

00:07:44,160 --> 00:07:47,940
can use highly optimized low-level

00:07:45,960 --> 00:07:50,669
libraries to execute these operations

00:07:47,940 --> 00:07:52,380
fast and in parallel we can also run

00:07:50,669 --> 00:07:56,639
them on GPUs to speed up our

00:07:52,380 --> 00:07:59,099
computations even more so internally a

00:07:56,639 --> 00:08:01,380
model actually takes the tensor as input

00:07:59,099 --> 00:08:04,169
and returns another tensor or sometimes

00:08:01,380 --> 00:08:06,660
a list of tensors what we need is an

00:08:04,169 --> 00:08:09,120
encoder for the input a and a decoder

00:08:06,660 --> 00:08:11,599
for the output B so we can actually

00:08:09,120 --> 00:08:14,400
build a model that goes from A to B in

00:08:11,599 --> 00:08:16,650
general we can encode most data as

00:08:14,400 --> 00:08:19,050
tensors we encoded this binary numbers

00:08:16,650 --> 00:08:21,690
anyway right so for instance an

00:08:19,050 --> 00:08:27,690
uncompressed image already is a 3d

00:08:21,690 --> 00:08:29,280
tensor of RGB values ok now that we've

00:08:27,690 --> 00:08:31,710
talked about our inputs and outputs

00:08:29,280 --> 00:08:34,380
let's have a look at the most basic unit

00:08:31,710 --> 00:08:37,500
of a neural network the artificial

00:08:34,380 --> 00:08:39,060
neuron conceptually you can think of an

00:08:37,500 --> 00:08:41,789
artificial neuron as some kind of

00:08:39,060 --> 00:08:43,440
programmable digital circuit although it

00:08:41,789 --> 00:08:47,220
operates on floating-point numbers

00:08:43,440 --> 00:08:48,990
instead of binary numbers it computes a

00:08:47,220 --> 00:08:52,350
simple mathematical function from its

00:08:48,990 --> 00:08:56,730
inputs and returns a single number first

00:08:52,350 --> 00:08:57,730
it multiplies each input x1 x2 up to xn

00:08:56,730 --> 00:09:00,850
with

00:08:57,730 --> 00:09:02,769
wait and those weights determine how a

00:09:00,850 --> 00:09:06,130
neuron reacts which in inputs they

00:09:02,769 --> 00:09:08,410
decide how important it input is and how

00:09:06,130 --> 00:09:09,940
much its value should contribute to the

00:09:08,410 --> 00:09:13,600
output so they are basically the

00:09:09,940 --> 00:09:15,160
programming of manure and then it

00:09:13,600 --> 00:09:17,110
combines the results of that

00:09:15,160 --> 00:09:19,930
multiplication into a single number but

00:09:17,110 --> 00:09:21,880
just summing them up and finally it

00:09:19,930 --> 00:09:24,639
applies the so called activation

00:09:21,880 --> 00:09:26,380
function to that sum there are different

00:09:24,639 --> 00:09:29,260
activation functions but in general they

00:09:26,380 --> 00:09:32,079
squish the input into some fixed range

00:09:29,260 --> 00:09:33,670
the like between zero and one the

00:09:32,079 --> 00:09:36,430
activation function is important because

00:09:33,670 --> 00:09:38,170
when you when you combine neurons it

00:09:36,430 --> 00:09:40,630
allows us to represent more complex

00:09:38,170 --> 00:09:41,889
nonlinear functions and this essentially

00:09:40,630 --> 00:09:44,529
enables the neural network to do

00:09:41,889 --> 00:09:51,100
conditional logic which makes it so

00:09:44,529 --> 00:09:54,070
powerful now when we or a neural network

00:09:51,100 --> 00:09:56,170
is actually just just a collection of

00:09:54,070 --> 00:09:58,630
these artificial neurons and stacked in

00:09:56,170 --> 00:10:00,430
layers the neurons of each layer take

00:09:58,630 --> 00:10:02,800
the output of the previous layers input

00:10:00,430 --> 00:10:04,959
and compute that function we've seen on

00:10:02,800 --> 00:10:07,240
the previous slide and by doing so each

00:10:04,959 --> 00:10:09,459
layer transforms the original input a

00:10:07,240 --> 00:10:13,029
little bit step by step up to the final

00:10:09,459 --> 00:10:15,430
output it's important to understand that

00:10:13,029 --> 00:10:18,160
each neuron has its own set of weights

00:10:15,430 --> 00:10:21,459
so even the neurons and of a single

00:10:18,160 --> 00:10:23,500
layer here we see is very simple neural

00:10:21,459 --> 00:10:27,279
network consisting of two fully

00:10:23,500 --> 00:10:29,139
connected layers the input layer doesn't

00:10:27,279 --> 00:10:31,420
count because it just represents the

00:10:29,139 --> 00:10:35,130
input values and fully connected means

00:10:31,420 --> 00:10:38,319
that each neuron receives all the inputs

00:10:35,130 --> 00:10:40,149
from the previous layer sorry all the

00:10:38,319 --> 00:10:42,490
outputs from the previous layers input

00:10:40,149 --> 00:10:44,589
and each neuron usually computes a

00:10:42,490 --> 00:10:48,430
different output because it's weights

00:10:44,589 --> 00:10:50,730
differ from the other neurons now in

00:10:48,430 --> 00:10:53,199
practice many networks are much deeper

00:10:50,730 --> 00:10:55,779
which just means they have many more

00:10:53,199 --> 00:10:59,769
layers like for instance the inception

00:10:55,779 --> 00:11:02,620
network for image recognition has I

00:10:59,769 --> 00:11:05,260
think it's 48 layers or something like

00:11:02,620 --> 00:11:07,420
that and the structure and size of our

00:11:05,260 --> 00:11:08,980
layers is called the architecture of the

00:11:07,420 --> 00:11:11,030
neural network it's very important

00:11:08,980 --> 00:11:12,530
because it defines how big the

00:11:11,030 --> 00:11:14,240
a city of the network is how much

00:11:12,530 --> 00:11:17,000
information the network can store and

00:11:14,240 --> 00:11:18,830
how it flows through the network it also

00:11:17,000 --> 00:11:21,170
defines the inputs and outputs the

00:11:18,830 --> 00:11:23,030
signature of the function but it does

00:11:21,170 --> 00:11:25,940
does not define the function logic

00:11:23,030 --> 00:11:31,840
itself that logic is defined by the

00:11:25,940 --> 00:11:35,720
weights but how do we set those weights

00:11:31,840 --> 00:11:38,690
in order to get the function to actually

00:11:35,720 --> 00:11:40,730
compute or the network to actually

00:11:38,690 --> 00:11:45,260
compute what we want how do we program

00:11:40,730 --> 00:11:48,110
the network and the answer is we learn

00:11:45,260 --> 00:11:53,540
them and that's what machine learning is

00:11:48,110 --> 00:11:55,520
all about in traditional programming we

00:11:53,540 --> 00:11:57,500
write rules in some way or another of

00:11:55,520 --> 00:11:59,660
course we use different abstractions

00:11:57,500 --> 00:12:02,660
languages and paradigms but ultimately

00:11:59,660 --> 00:12:04,340
we write rules and we apply those rules

00:12:02,660 --> 00:12:07,340
to some input data in order to get

00:12:04,340 --> 00:12:09,440
answers that works fine in most cases

00:12:07,340 --> 00:12:12,890
but sometimes it doesn't work so well

00:12:09,440 --> 00:12:15,050
because we just don't know how to do it

00:12:12,890 --> 00:12:17,660
recognizing faces for instance seems

00:12:15,050 --> 00:12:20,150
very easy to force humans but if you try

00:12:17,660 --> 00:12:22,280
to develop an algorithm you'll see that

00:12:20,150 --> 00:12:25,310
it's actually very actually a very very

00:12:22,280 --> 00:12:26,660
difficult problem and machine learning

00:12:25,310 --> 00:12:29,090
on the other hand we do not write the

00:12:26,660 --> 00:12:30,770
rules ourselves create a system that can

00:12:29,090 --> 00:12:33,380
learn the rules from so-called training

00:12:30,770 --> 00:12:36,110
data which basically which is basically

00:12:33,380 --> 00:12:38,150
example data with given answers and of

00:12:36,110 --> 00:12:40,190
course we can in turn apply those rules

00:12:38,150 --> 00:12:43,450
to new data to get new answers like we

00:12:40,190 --> 00:12:43,450
do with our handwritten rules

00:12:54,050 --> 00:12:59,550
but how can we create such a system that

00:12:57,120 --> 00:13:01,709
learns from data well there are

00:12:59,550 --> 00:13:04,110
different approaches but what currently

00:13:01,709 --> 00:13:06,720
powers most deep learning system in

00:13:04,110 --> 00:13:08,399
systems in production is a technique

00:13:06,720 --> 00:13:10,290
called supervised learning and

00:13:08,399 --> 00:13:12,600
supervised learning we essentially learn

00:13:10,290 --> 00:13:16,019
from known examples so we train a model

00:13:12,600 --> 00:13:18,029
with examples known examples means some

00:13:16,019 --> 00:13:20,250
input data together with corresponding

00:13:18,029 --> 00:13:22,410
answers called labels so that's our

00:13:20,250 --> 00:13:24,089
training data if for instance we wanted

00:13:22,410 --> 00:13:26,459
to predict whether a photo contains a

00:13:24,089 --> 00:13:28,589
cat or not our training data would look

00:13:26,459 --> 00:13:30,839
something like this our examples are

00:13:28,589 --> 00:13:34,110
photos showing either a cat or something

00:13:30,839 --> 00:13:36,570
different and for each photo we also

00:13:34,110 --> 00:13:39,000
have a label that's one if there's a cat

00:13:36,570 --> 00:13:40,620
in the photo and zero otherwise it's

00:13:39,000 --> 00:13:42,750
important that we have a large enough

00:13:40,620 --> 00:13:46,110
training set and enough examples for

00:13:42,750 --> 00:13:48,930
each class and how large exactly depends

00:13:46,110 --> 00:13:53,579
on the complexity of the problem but

00:13:48,930 --> 00:13:55,320
more so almost almost always better and

00:13:53,579 --> 00:13:57,600
that's why the big tech companies that

00:13:55,320 --> 00:14:01,620
collect all of our data have such an

00:13:57,600 --> 00:14:04,019
advantage in the field but how does this

00:14:01,620 --> 00:14:06,600
training work how can we actually learn

00:14:04,019 --> 00:14:08,399
a function from examples remember that

00:14:06,600 --> 00:14:10,079
the weights determine the function of a

00:14:08,399 --> 00:14:11,579
neural network compute so it's the right

00:14:10,079 --> 00:14:15,029
values of the weight so we want to

00:14:11,579 --> 00:14:17,010
figure out training is an iterative

00:14:15,029 --> 00:14:20,459
process with multiple steps so let's

00:14:17,010 --> 00:14:22,380
look at them one at a time at the first

00:14:20,459 --> 00:14:24,480
step we initialize our weights randomly

00:14:22,380 --> 00:14:26,940
that it will give us a give us a model

00:14:24,480 --> 00:14:31,290
that's pretty bad but we have a starting

00:14:26,940 --> 00:14:32,790
point next we do a forward pass we run

00:14:31,290 --> 00:14:35,430
some training examples through the

00:14:32,790 --> 00:14:38,130
network here just one for illustration

00:14:35,430 --> 00:14:42,120
which gives us a result the result

00:14:38,130 --> 00:14:44,070
that's usually not so good now we

00:14:42,120 --> 00:14:46,140
compare the result with the correct

00:14:44,070 --> 00:14:48,600
value from the training example we

00:14:46,140 --> 00:14:50,699
compute the difference which gives us

00:14:48,600 --> 00:14:53,820
the error the network has made the loss

00:14:50,699 --> 00:14:55,709
and our goal is to minimize that loss

00:14:53,820 --> 00:14:57,660
because the small loss means the

00:14:55,709 --> 00:15:00,180
predictions are close to the training

00:14:57,660 --> 00:15:02,110
values so the loss is our feedback

00:15:00,180 --> 00:15:05,200
signal so and

00:15:02,110 --> 00:15:07,870
important to get it right we can use

00:15:05,200 --> 00:15:10,779
loss as guide to find out how we have to

00:15:07,870 --> 00:15:12,760
change the weights and to improve our

00:15:10,779 --> 00:15:15,550
model through an algorithm called back

00:15:12,760 --> 00:15:17,860
propagation so I'm not going to explain

00:15:15,550 --> 00:15:20,500
this in detail but we basically compute

00:15:17,860 --> 00:15:24,040
the derivative the slope of the loss

00:15:20,500 --> 00:15:27,100
function with respect to each rate you

00:15:24,040 --> 00:15:29,620
might remember from calculus that the

00:15:27,100 --> 00:15:31,899
derivative tells us how a function value

00:15:29,620 --> 00:15:35,740
changes when we change the input a tiny

00:15:31,899 --> 00:15:37,810
bit and here we use that information to

00:15:35,740 --> 00:15:41,320
change the weights a little bit in the

00:15:37,810 --> 00:15:43,120
direction opposite to the slope and that

00:15:41,320 --> 00:15:49,089
way we make the loss a little bit

00:15:43,120 --> 00:15:51,279
smaller and now we repeat those four

00:15:49,089 --> 00:15:53,019
steps and each time we reduce the loss a

00:15:51,279 --> 00:15:55,810
little bit and improve our model until

00:15:53,019 --> 00:15:58,899
we reach a minimum and that method for

00:15:55,810 --> 00:16:01,029
training is called gradient descent so

00:15:58,899 --> 00:16:03,160
the loss function looks very simple here

00:16:01,029 --> 00:16:07,329
but that's just because we only have a

00:16:03,160 --> 00:16:08,980
single a single weight or two weights in

00:16:07,329 --> 00:16:11,470
the example on the right so we can see

00:16:08,980 --> 00:16:14,110
what's going on in reality we have a

00:16:11,470 --> 00:16:16,600
very high dimensional space of thousands

00:16:14,110 --> 00:16:20,019
or even millions of weights and we also

00:16:16,600 --> 00:16:21,850
have more complicated loss functions so

00:16:20,019 --> 00:16:24,220
we might have to deal with all kinds of

00:16:21,850 --> 00:16:26,380
issues to get the training right but

00:16:24,220 --> 00:16:28,269
even for deep complex networks the

00:16:26,380 --> 00:16:31,829
training process is just to run these

00:16:28,269 --> 00:16:33,940
four steps repeatedly so that's that

00:16:31,829 --> 00:16:38,920
essentially how neural networks learn

00:16:33,940 --> 00:16:40,750
their function so why is deep learning

00:16:38,920 --> 00:16:45,209
working so well well why is it often

00:16:40,750 --> 00:16:47,709
better better than other methods and

00:16:45,209 --> 00:16:49,959
probably the most important reason is

00:16:47,709 --> 00:16:52,089
scalability given enough training data

00:16:49,959 --> 00:16:54,640
neural networks can learn very complex

00:16:52,089 --> 00:16:56,800
functions that's why for problems like

00:16:54,640 --> 00:17:01,410
computer vision deep learning has

00:16:56,800 --> 00:17:04,120
outperformed many other methods by far

00:17:01,410 --> 00:17:06,370
this comes at the price though we need a

00:17:04,120 --> 00:17:08,559
lot of training data and we need deeper

00:17:06,370 --> 00:17:10,600
and more computationally challenging

00:17:08,559 --> 00:17:14,220
networks so training very deep networks

00:17:10,600 --> 00:17:14,220
can we can be hard

00:17:14,480 --> 00:17:18,839
and another reason is composition we've

00:17:17,309 --> 00:17:21,569
already seen that neural networks are

00:17:18,839 --> 00:17:23,370
composed from neurons and layers but we

00:17:21,569 --> 00:17:25,079
can also plug together bigger networks

00:17:23,370 --> 00:17:28,649
that have already been trained

00:17:25,079 --> 00:17:30,990
individually and we train them a little

00:17:28,649 --> 00:17:32,880
bit more jointly and then use them like

00:17:30,990 --> 00:17:34,620
in this example which is an image

00:17:32,880 --> 00:17:38,070
caption generator and I think that's

00:17:34,620 --> 00:17:40,289
pretty amazing or we can do something

00:17:38,070 --> 00:17:42,899
called transfer learning we can chop off

00:17:40,289 --> 00:17:45,750
and we replace parts and reuse in front

00:17:42,899 --> 00:17:47,429
for a different task so for instance we

00:17:45,750 --> 00:17:50,730
can train a neural network on images of

00:17:47,429 --> 00:17:52,470
cats and dogs and then remove the

00:17:50,730 --> 00:17:54,870
classification layers at the end and put

00:17:52,470 --> 00:17:56,640
in a car detector instead we need to

00:17:54,870 --> 00:18:00,600
train the classifier a bit but we can

00:17:56,640 --> 00:18:03,659
reuse most of the earlier layers so we

00:18:00,600 --> 00:18:04,890
build small reusable functions and we

00:18:03,659 --> 00:18:08,520
combine them to create something

00:18:04,890 --> 00:18:11,909
powerful and that does sound a lot like

00:18:08,520 --> 00:18:14,149
FP right and it's indeed related deep

00:18:11,909 --> 00:18:16,409
learning is essentially composition of

00:18:14,149 --> 00:18:20,340
differentiable and therefore learnable

00:18:16,409 --> 00:18:23,399
functions and I think you can hear you

00:18:20,340 --> 00:18:30,559
more about that concept tomorrow from

00:18:23,399 --> 00:18:34,200
Noel stop talk right so you may hurt

00:18:30,559 --> 00:18:36,090
those fears about an AI super

00:18:34,200 --> 00:18:39,299
intelligence that will soon destroy

00:18:36,090 --> 00:18:40,799
humanity I wouldn't worry too much we

00:18:39,299 --> 00:18:42,960
aren't there yet the best deep learning

00:18:40,799 --> 00:18:44,700
is some kind of narrow intelligence so

00:18:42,960 --> 00:18:48,149
there's no no higher-level reasoning

00:18:44,700 --> 00:18:50,039
involved and you can see that quite

00:18:48,149 --> 00:18:51,690
nicely in these examples the neural

00:18:50,039 --> 00:18:54,779
network on the left thinks there are

00:18:51,690 --> 00:18:58,649
three real bicycles and the person in

00:18:54,779 --> 00:19:00,870
the image you can even fool neural

00:18:58,649 --> 00:19:03,899
networks on purpose with adversarial

00:19:00,870 --> 00:19:07,470
examples in the middle we see what's

00:19:03,899 --> 00:19:10,789
called an adversarial patch and a neural

00:19:07,470 --> 00:19:14,820
network recognizes this as a toaster

00:19:10,789 --> 00:19:16,620
just look a bit like a toaster right so

00:19:14,820 --> 00:19:18,690
if you place it closely to the banana

00:19:16,620 --> 00:19:21,289
that banana suddenly becomes a toaster

00:19:18,690 --> 00:19:21,289
for the network

00:19:23,590 --> 00:19:28,910
okay so that's it for the deep learning

00:19:26,150 --> 00:19:31,429
foundations and I hope I could give you

00:19:28,910 --> 00:19:33,800
some insight into what neural networks

00:19:31,429 --> 00:19:36,020
are and how they work let's now climb

00:19:33,800 --> 00:19:38,650
the next stair and look at Scala as a

00:19:36,020 --> 00:19:42,140
language for deep learning

00:19:38,650 --> 00:19:44,480
so why Scala building a neural network

00:19:42,140 --> 00:19:46,490
is still more an art than science it

00:19:44,480 --> 00:19:48,260
usually means a lot of prototyping and

00:19:46,490 --> 00:19:51,380
experimentation until we're happy with a

00:19:48,260 --> 00:19:53,510
model so we need a language that allows

00:19:51,380 --> 00:19:55,790
us to express new ideas quickly and

00:19:53,510 --> 00:19:58,910
doesn't get in the way but once we build

00:19:55,790 --> 00:20:01,280
a model that works we of course we want

00:19:58,910 --> 00:20:03,380
to use it in production so we need to

00:20:01,280 --> 00:20:05,570
integrate it into its existing system

00:20:03,380 --> 00:20:07,970
and currently most deeply deep learning

00:20:05,570 --> 00:20:09,559
libraries are written in Python so

00:20:07,970 --> 00:20:12,350
Python is great for data science and

00:20:09,559 --> 00:20:14,179
prototyping but most larger systems are

00:20:12,350 --> 00:20:16,010
written in other languages then these

00:20:14,179 --> 00:20:19,490
systems often run on the JVM

00:20:16,010 --> 00:20:21,320
and that adds mental overhead because we

00:20:19,490 --> 00:20:23,809
have to switch between languages and

00:20:21,320 --> 00:20:26,480
ecosystems and with Scala we can

00:20:23,809 --> 00:20:29,140
actually have the best of both worlds so

00:20:26,480 --> 00:20:33,620
a language that's concise and expressive

00:20:29,140 --> 00:20:35,210
so for rapid prototyping but we can also

00:20:33,620 --> 00:20:36,860
move our model to production with

00:20:35,210 --> 00:20:42,140
confidence because we know in good

00:20:36,860 --> 00:20:43,910
company but there's another reason which

00:20:42,140 --> 00:20:46,750
is very important both for prototyping

00:20:43,910 --> 00:20:49,309
and production and that's type safety so

00:20:46,750 --> 00:20:50,750
debugging neural networks can be quite

00:20:49,309 --> 00:20:54,350
difficult because we often don't know

00:20:50,750 --> 00:20:57,800
whether bad results are caused by our

00:20:54,350 --> 00:20:59,000
input data or architecture or an actual

00:20:57,800 --> 00:21:00,950
mark

00:20:59,000 --> 00:21:02,980
we can't just write a unit test when we

00:21:00,950 --> 00:21:05,150
don't know exactly what we can't expect

00:21:02,980 --> 00:21:07,190
so it's especially important to

00:21:05,150 --> 00:21:09,500
eliminate potential bug sources up front

00:21:07,190 --> 00:21:11,179
and with Scala one way of course is to

00:21:09,500 --> 00:21:14,480
let the compiler help us by providing

00:21:11,179 --> 00:21:16,670
typesafe tensor operation operations so

00:21:14,480 --> 00:21:19,040
for instance instead of having the data

00:21:16,670 --> 00:21:21,500
type and shape of the tensor only

00:21:19,040 --> 00:21:26,090
available at run time like in the

00:21:21,500 --> 00:21:27,980
example above we could try to encode to

00:21:26,090 --> 00:21:30,530
encode them as type information where

00:21:27,980 --> 00:21:33,200
possible so that way and an operation

00:21:30,530 --> 00:21:37,630
can check whether the input tensor

00:21:33,200 --> 00:21:37,630
matches its requirements at compile time

00:21:37,720 --> 00:21:43,070
so in theory Scala is a great fit for

00:21:40,850 --> 00:21:45,410
deep learning but how does reality look

00:21:43,070 --> 00:21:48,260
like like in 2018

00:21:45,410 --> 00:21:50,000
and I'm not gonna lie to you the

00:21:48,260 --> 00:21:52,190
existing Scala libraries are not as

00:21:50,000 --> 00:21:54,440
immature as their Python counterparts

00:21:52,190 --> 00:21:57,169
yet that doesn't mean we can use them

00:21:54,440 --> 00:22:01,039
just don't expect everything to be

00:21:57,169 --> 00:22:02,929
working smoothly out of the box so what

00:22:01,039 --> 00:22:05,780
features should a deep learning library

00:22:02,929 --> 00:22:07,549
provide at least um well we need

00:22:05,780 --> 00:22:09,740
abstractions for tensors and tensor

00:22:07,549 --> 00:22:12,740
operations and we want them to be fast

00:22:09,740 --> 00:22:14,630
so we want GPU support but then we also

00:22:12,740 --> 00:22:16,520
want higher level api so building and

00:22:14,630 --> 00:22:18,350
training neural networks so deep

00:22:16,520 --> 00:22:21,650
learning library should provide building

00:22:18,350 --> 00:22:24,559
blocks for the architecture to create

00:22:21,650 --> 00:22:26,090
and stack our layers and we don't want

00:22:24,559 --> 00:22:28,190
to think about backpropagation

00:22:26,090 --> 00:22:30,679
derivatives or how to do gradient

00:22:28,190 --> 00:22:32,299
descent so a library should implement

00:22:30,679 --> 00:22:34,700
these things and let us focus on the

00:22:32,299 --> 00:22:36,140
task at hand and then there are

00:22:34,700 --> 00:22:38,659
additional features we'd like to have

00:22:36,140 --> 00:22:41,210
like data pre-processing pre-processing

00:22:38,659 --> 00:22:45,890
capabilities or distributed training for

00:22:41,210 --> 00:22:48,020
larger networks so based on this wish

00:22:45,890 --> 00:22:49,760
list I've selected three libraries I

00:22:48,020 --> 00:22:52,370
want to introduce briefly of course

00:22:49,760 --> 00:22:55,490
there's more but to my knowledge these

00:22:52,370 --> 00:22:57,409
are the most developed ones deep

00:22:55,490 --> 00:23:00,289
learning 4j is the oldest and probably

00:22:57,409 --> 00:23:02,390
the most mature library it has rich

00:23:00,289 --> 00:23:06,230
documentation a large community and

00:23:02,390 --> 00:23:09,470
commercial support but we in Java land

00:23:06,230 --> 00:23:11,960
so it's usable but not always fun from a

00:23:09,470 --> 00:23:14,240
scholar perspective there is a native

00:23:11,960 --> 00:23:17,360
scholar API on top of deal 4j called

00:23:14,240 --> 00:23:21,679
scale net and development but it's still

00:23:17,360 --> 00:23:24,169
in an early stage IMAX net is a deep

00:23:21,679 --> 00:23:26,840
learning library with focus on

00:23:24,169 --> 00:23:29,620
scalability it has a Scala API as part

00:23:26,840 --> 00:23:32,929
of the main project so that's nice

00:23:29,620 --> 00:23:35,720
currently the API is a very direct port

00:23:32,929 --> 00:23:38,659
of the Python API though so it's really

00:23:35,720 --> 00:23:40,429
like writing Python in Scala and so

00:23:38,659 --> 00:23:44,480
there's not not much use of the type

00:23:40,429 --> 00:23:46,490
system then we have tensorflow for Scala

00:23:44,480 --> 00:23:49,130
which is the youngest project of the

00:23:46,490 --> 00:23:49,820
three but it has made impressive

00:23:49,130 --> 00:23:52,940
progress

00:23:49,820 --> 00:23:55,340
last year it builds on the low-level

00:23:52,940 --> 00:23:58,790
parts of the popular tensorflow library

00:23:55,340 --> 00:24:00,890
from google and and the author has

00:23:58,790 --> 00:24:05,630
ported large parts of ten to floss

00:24:00,890 --> 00:24:09,200
Python API to Scala and it offers the

00:24:05,630 --> 00:24:12,650
most idiomatic and typesafe Scala API so

00:24:09,200 --> 00:24:16,010
I'll use it in my example you can find a

00:24:12,650 --> 00:24:19,790
closer look at these libraries and some

00:24:16,010 --> 00:24:21,590
example code on my block but please take

00:24:19,790 --> 00:24:26,600
these statements with a bit of caution

00:24:21,590 --> 00:24:30,290
because things are developing so fast so

00:24:26,600 --> 00:24:35,870
enough theory let's now apply what we've

00:24:30,290 --> 00:24:38,000
learned so far so the first thing we

00:24:35,870 --> 00:24:41,570
have to think about is what what's our

00:24:38,000 --> 00:24:45,560
actual business problem right but once

00:24:41,570 --> 00:24:47,330
we have that we should check whether we

00:24:45,560 --> 00:24:51,530
even need deep learning and sometimes

00:24:47,330 --> 00:24:53,030
the simple solution works just fine if

00:24:51,530 --> 00:24:55,190
deep learning seems like the right

00:24:53,030 --> 00:24:57,080
approach try to define the signature of

00:24:55,190 --> 00:24:59,990
our function they want to learn so

00:24:57,080 --> 00:25:03,530
what's your input types input and output

00:24:59,990 --> 00:25:05,330
types what's your a and B and let's do

00:25:03,530 --> 00:25:07,580
this with an example so we are all

00:25:05,330 --> 00:25:11,030
excited about dotty right we all heard

00:25:07,580 --> 00:25:14,240
the keynote keynote yesterday and we

00:25:11,030 --> 00:25:15,830
want to be ready so what we have tools

00:25:14,240 --> 00:25:18,200
like Scala fix to help with the

00:25:15,830 --> 00:25:20,030
transition of code let's assume that

00:25:18,200 --> 00:25:22,670
someone has decided to upgrade the logo

00:25:20,030 --> 00:25:24,860
as well so we want to replace all the

00:25:22,670 --> 00:25:28,790
images containing the Scala logo with

00:25:24,860 --> 00:25:31,540
the dotty version so what we need first

00:25:28,790 --> 00:25:34,610
is an object detector for the Scala logo

00:25:31,540 --> 00:25:38,320
but let's simplify things a little bit

00:25:34,610 --> 00:25:40,820
here by making it just a classifier that

00:25:38,320 --> 00:25:43,840
recognizes whether an image contains the

00:25:40,820 --> 00:25:43,840
scala logo or not

00:25:50,990 --> 00:25:56,400
the next step is to look for existing

00:25:53,700 --> 00:25:58,230
solutions if you're really lucky there's

00:25:56,400 --> 00:26:00,720
already a pre trained model for your

00:25:58,230 --> 00:26:02,190
task that you can just use usually

00:26:00,720 --> 00:26:04,350
that's not the case though

00:26:02,190 --> 00:26:06,150
but sometimes the similar model exists

00:26:04,350 --> 00:26:08,760
like an image recognition model trained

00:26:06,150 --> 00:26:11,490
on other images then you could try

00:26:08,760 --> 00:26:13,620
transfer learning that's especially

00:26:11,490 --> 00:26:16,530
helpful if you don't have much data and

00:26:13,620 --> 00:26:18,600
you want to speed up training or you

00:26:16,530 --> 00:26:21,090
could build and exist on an existing

00:26:18,600 --> 00:26:23,190
architecture and adapt that to your task

00:26:21,090 --> 00:26:25,050
and that could actually work for our

00:26:23,190 --> 00:26:28,710
example but to keep things simple let's

00:26:25,050 --> 00:26:30,360
build something from scratch so for

00:26:28,710 --> 00:26:32,940
supervised learning we need label data

00:26:30,360 --> 00:26:35,660
remember label data are examples with

00:26:32,940 --> 00:26:37,560
answers a cat picture cat label

00:26:35,660 --> 00:26:39,210
depending on your problem there are

00:26:37,560 --> 00:26:41,040
different options to get training data

00:26:39,210 --> 00:26:43,680
perhaps you can reuse some existing

00:26:41,040 --> 00:26:45,720
public data set the good starting point

00:26:43,680 --> 00:26:48,240
here is kaggle a website that holds data

00:26:45,720 --> 00:26:50,850
science competitions or you might do the

00:26:48,240 --> 00:26:53,220
labeling yourself or or you pay someone

00:26:50,850 --> 00:26:54,960
to else to do it

00:26:53,220 --> 00:26:56,820
you usually also have to do some

00:26:54,960 --> 00:26:58,920
pre-processing to get the training data

00:26:56,820 --> 00:27:01,020
in shape so you might have to clean it

00:26:58,920 --> 00:27:04,230
through some normalization resizing and

00:27:01,020 --> 00:27:06,750
so on and you have to vectorize it

00:27:04,230 --> 00:27:09,180
encoded as a tensor so you can actually

00:27:06,750 --> 00:27:11,190
feed it into your network ideally you

00:27:09,180 --> 00:27:12,930
can build some kind of pre-processing

00:27:11,190 --> 00:27:16,350
pipeline and automate as much as

00:27:12,930 --> 00:27:19,530
possible and you also have to split your

00:27:16,350 --> 00:27:23,790
data into training and test data so why

00:27:19,530 --> 00:27:25,680
is that the reason is an important issue

00:27:23,790 --> 00:27:28,200
of machine learning called overfitting

00:27:25,680 --> 00:27:33,000
so that basically means that your model

00:27:28,200 --> 00:27:36,150
just memorizes the the training data but

00:27:33,000 --> 00:27:38,630
what you want your model to do is to

00:27:36,150 --> 00:27:41,340
generalize to work well on unseen data

00:27:38,630 --> 00:27:43,260
instead so to recognize overfitting you

00:27:41,340 --> 00:27:48,210
put aside some part of the label data

00:27:43,260 --> 00:27:49,920
and use it for evaluation so what's a

00:27:48,210 --> 00:27:52,080
good source for Scala log of photos in

00:27:49,920 --> 00:27:54,840
the wild well of course

00:27:52,080 --> 00:27:56,310
Scala days so I've collected about 800

00:27:54,840 --> 00:28:00,410
photos for training from the photo

00:27:56,310 --> 00:28:01,900
collections of previous Scala days and

00:28:00,410 --> 00:28:07,030
pre-processed

00:28:01,900 --> 00:28:09,000
a little bit in manual labor so for

00:28:07,030 --> 00:28:11,890
reading images from disk and do

00:28:09,000 --> 00:28:16,720
pre-processing like resizing and adding

00:28:11,890 --> 00:28:20,230
labels we could use tensor flows data

00:28:16,720 --> 00:28:22,890
set API the the data set API allows us

00:28:20,230 --> 00:28:25,150
to build input pipelines by transforming

00:28:22,890 --> 00:28:27,880
basically transforming collections of

00:28:25,150 --> 00:28:30,490
tensors and it should look very familiar

00:28:27,880 --> 00:28:33,220
if you you know the Scala collections

00:28:30,490 --> 00:28:35,260
API so it has all the Combinator's or

00:28:33,220 --> 00:28:38,020
many of the Combinator's like map

00:28:35,260 --> 00:28:40,180
flatmap and friends the main difference

00:28:38,020 --> 00:28:41,980
is that we work on tensors here and the

00:28:40,180 --> 00:28:44,800
cool thing is that in comparison to the

00:28:41,980 --> 00:28:47,410
Python version which which looks very

00:28:44,800 --> 00:28:51,180
similar its type safe and prevents us

00:28:47,410 --> 00:28:51,180
from from combining things the wrong way

00:28:51,960 --> 00:28:56,620
the next step is to think about the

00:28:54,250 --> 00:28:59,020
architecture remember a neural network

00:28:56,620 --> 00:29:01,660
architecture defines the the capacity of

00:28:59,020 --> 00:29:04,600
the network and also defines how we can

00:29:01,660 --> 00:29:07,000
constrain the search base for finding

00:29:04,600 --> 00:29:08,860
weights during training so what works

00:29:07,000 --> 00:29:13,180
best really depends on the task you're

00:29:08,860 --> 00:29:15,160
trying to solve and and your data you

00:29:13,180 --> 00:29:17,770
can try different architectures just to

00:29:15,160 --> 00:29:20,440
see how they work but for common types

00:29:17,770 --> 00:29:23,740
of data there are major known

00:29:20,440 --> 00:29:27,490
architectures that are known to work

00:29:23,740 --> 00:29:30,160
well for for specific for specific tasks

00:29:27,490 --> 00:29:32,200
and we have already seen fully connected

00:29:30,160 --> 00:29:34,330
neural networks which is the most

00:29:32,200 --> 00:29:37,210
general architecture and often a good

00:29:34,330 --> 00:29:40,000
way to start convolutional neural

00:29:37,210 --> 00:29:42,520
networks are well-suited for all kinds

00:29:40,000 --> 00:29:45,900
of image recognition tasks but they also

00:29:42,520 --> 00:29:49,290
work well for for other domains and

00:29:45,900 --> 00:29:52,840
recurrent neural networks are good for

00:29:49,290 --> 00:29:55,630
variable sequences like text or time

00:29:52,840 --> 00:29:58,000
series data and of course it's possible

00:29:55,630 --> 00:30:00,870
and it's also quite common to combine

00:29:58,000 --> 00:30:03,840
different architectures in one network

00:30:00,870 --> 00:30:06,190
so since we're doing image recognition a

00:30:03,840 --> 00:30:08,910
convolutional neural network or see an

00:30:06,190 --> 00:30:11,380
endless a sensible architecture a

00:30:08,910 --> 00:30:14,890
convolutional a convolutional layer

00:30:11,380 --> 00:30:15,450
learns filters that you run over you

00:30:14,890 --> 00:30:17,610
image

00:30:15,450 --> 00:30:19,500
and each filter computes a value from

00:30:17,610 --> 00:30:22,169
its surrounding pixels and that way it

00:30:19,500 --> 00:30:25,110
can detect local patterns in the image

00:30:22,169 --> 00:30:28,049
and if you stack multiple convolutional

00:30:25,110 --> 00:30:30,510
layers they they can learn to detect

00:30:28,049 --> 00:30:32,389
more and more complex patterns in deeper

00:30:30,510 --> 00:30:35,460
layers like we see how here on the right

00:30:32,389 --> 00:30:38,460
so the first layer detects very simple

00:30:35,460 --> 00:30:41,159
simple patterns like edges the second

00:30:38,460 --> 00:30:44,730
layer already larger with more complex

00:30:41,159 --> 00:30:47,789
patterns the third layer seems to detect

00:30:44,730 --> 00:30:53,549
face parts already and the the fourth

00:30:47,789 --> 00:30:57,000
layer recognizes whole faces so at the

00:30:53,549 --> 00:31:00,690
end we usually have one or more fully

00:30:57,000 --> 00:31:04,580
connected layers that are responsible

00:31:00,690 --> 00:31:06,720
for the actual classification tasks

00:31:04,580 --> 00:31:09,809
building a neural network is basically

00:31:06,720 --> 00:31:12,450
doing three steps repeatedly so first we

00:31:09,809 --> 00:31:14,100
create a model we train it on our

00:31:12,450 --> 00:31:16,139
training data and test how well it

00:31:14,100 --> 00:31:18,360
performs on our separate test data set

00:31:16,139 --> 00:31:21,090
and then we try to improve the

00:31:18,360 --> 00:31:23,909
architecture and we add more layers try

00:31:21,090 --> 00:31:28,220
another type of layer and so on and we

00:31:23,909 --> 00:31:33,120
train and evaluate again see whether our

00:31:28,220 --> 00:31:35,429
changes have improved and we do that

00:31:33,120 --> 00:31:40,289
until hopefully we find something that

00:31:35,429 --> 00:31:42,600
works well it's often useful to start

00:31:40,289 --> 00:31:44,580
very simple and try to build a minimal

00:31:42,600 --> 00:31:47,220
model first and that allows you to

00:31:44,580 --> 00:31:49,139
establish a baseline which is important

00:31:47,220 --> 00:31:51,659
for comparison and error analysis and

00:31:49,139 --> 00:31:54,950
from there you can try a bigger model a

00:31:51,659 --> 00:31:58,380
more specialized architecture and so on

00:31:54,950 --> 00:31:59,610
so let's try that for our example using

00:31:58,380 --> 00:32:03,929
tensorflow Scala

00:31:59,610 --> 00:32:08,360
so first we have to define our input

00:32:03,929 --> 00:32:12,960
layers the shape of our images is your

00:32:08,360 --> 00:32:17,580
250 by 250 with three channels color

00:32:12,960 --> 00:32:19,649
channels and the one for the labels is

00:32:17,580 --> 00:32:22,320
just a simple vector and minus one here

00:32:19,649 --> 00:32:25,110
means just means unknown because that's

00:32:22,320 --> 00:32:27,270
the the batch size how many inputs we

00:32:25,110 --> 00:32:30,030
process at once

00:32:27,270 --> 00:32:32,340
and now comes the actual architecture of

00:32:30,030 --> 00:32:35,010
our Laius and tensile floss gala

00:32:32,340 --> 00:32:38,480
provides set of predefined layer

00:32:35,010 --> 00:32:41,850
constructors through its learn api and

00:32:38,480 --> 00:32:51,540
we can compose multiple layers with the

00:32:41,850 --> 00:32:58,410
composed operator we see here so we

00:32:51,540 --> 00:33:01,580
flatten the input to a one dimensional

00:32:58,410 --> 00:33:04,380
vectors and we cast it to a float

00:33:01,580 --> 00:33:09,390
because that's what the fully connected

00:33:04,380 --> 00:33:11,360
net layers expects and the fully

00:33:09,390 --> 00:33:18,600
connected layer is called linear here

00:33:11,360 --> 00:33:21,390
and we set it to 64 neurons and then

00:33:18,600 --> 00:33:23,490
comes the activation function which is

00:33:21,390 --> 00:33:27,780
called value here and that's known to

00:33:23,490 --> 00:33:29,610
work well in practice and finally the

00:33:27,780 --> 00:33:33,540
output layer which has two neurons one

00:33:29,610 --> 00:33:35,820
for each class we also define a layer

00:33:33,540 --> 00:33:37,290
for the labels because we have to cast

00:33:35,820 --> 00:33:43,640
them to a lung for further processing

00:33:37,290 --> 00:33:46,350
and then we set our loss function and

00:33:43,640 --> 00:33:48,960
cross-entropy is something that works

00:33:46,350 --> 00:33:50,970
well for multiple classes and you can

00:33:48,960 --> 00:33:54,690
see that we use the composed operator

00:33:50,970 --> 00:33:59,460
again here to get a single number as a

00:33:54,690 --> 00:34:01,980
loss the optimizer is responsible for

00:33:59,460 --> 00:34:03,840
updating our weights so here we use

00:34:01,980 --> 00:34:05,970
standard standard gradient descent and

00:34:03,840 --> 00:34:08,340
we also set the learning rate that's how

00:34:05,970 --> 00:34:13,859
much we want to update the weights in

00:34:08,340 --> 00:34:17,250
each iteration and finally we create we

00:34:13,859 --> 00:34:24,119
combine these things into models that we

00:34:17,250 --> 00:34:26,669
can actually train so let's now run

00:34:24,119 --> 00:34:29,310
actual training on the only examples so

00:34:26,669 --> 00:34:31,250
training can take a long time depending

00:34:29,310 --> 00:34:34,050
on the size of your training data and

00:34:31,250 --> 00:34:38,010
the complexity of your network so you

00:34:34,050 --> 00:34:40,520
really want to do that on a GPU except

00:34:38,010 --> 00:34:43,610
for very small networks

00:34:40,520 --> 00:34:47,300
and we monitor how well it works so we

00:34:43,610 --> 00:34:50,030
looked at the loss which should go down

00:34:47,300 --> 00:34:52,220
over time and we also do evaluation from

00:34:50,030 --> 00:34:54,530
time to time to on the test set to see

00:34:52,220 --> 00:34:57,050
how the model works on unseen data and

00:34:54,530 --> 00:34:59,660
as you can see we make good improvements

00:34:57,050 --> 00:35:05,960
but after some time the accuracy starts

00:34:59,660 --> 00:35:08,360
to oscillate around 0.75 and the maximum

00:35:05,960 --> 00:35:12,800
seems to be opened 8 so with that model

00:35:08,360 --> 00:35:14,450
we can predict images predict whether an

00:35:12,800 --> 00:35:17,840
image contains the skala logo or not

00:35:14,450 --> 00:35:21,850
with an accuracy of 80% and that's not

00:35:17,840 --> 00:35:31,730
bad but perhaps we can do better so

00:35:21,850 --> 00:35:35,800
let's use 128 neurons make the model a

00:35:31,730 --> 00:35:41,420
little bit bigger and run training again

00:35:35,800 --> 00:35:47,869
so that works better so let's try even

00:35:41,420 --> 00:35:51,530
more say 512 even better and we could

00:35:47,869 --> 00:35:54,830
probably try more but there's not not

00:35:51,530 --> 00:35:56,960
much gain with with more neurons over

00:35:54,830 --> 00:35:59,930
more leaner layers so let's add a

00:35:56,960 --> 00:36:03,020
convolutional layer instead so a

00:35:59,930 --> 00:36:09,170
convolutional layer is called 2d

00:36:03,020 --> 00:36:11,990
intensive flow Scala and we use 32

00:36:09,170 --> 00:36:15,170
filters here and set the shape of our

00:36:11,990 --> 00:36:19,190
filter to 3 by 3 and we also add a few

00:36:15,170 --> 00:36:20,869
other few other options we add a bias

00:36:19,190 --> 00:36:23,630
which is basically another type of

00:36:20,869 --> 00:36:27,980
weight and again the activation function

00:36:23,630 --> 00:36:31,340
and finally we shrink our image with a

00:36:27,980 --> 00:36:34,160
so called max pool operation so we can

00:36:31,340 --> 00:36:39,830
look at larger parts than single pixels

00:36:34,160 --> 00:36:45,830
in later layers and as you can see when

00:36:39,830 --> 00:36:47,780
we run training again weight when we run

00:36:45,830 --> 00:36:54,160
training again our test accuracy

00:36:47,780 --> 00:36:59,049
increases to 2 0.95 so

00:36:54,160 --> 00:37:01,990
network classifiers 95% of our test

00:36:59,049 --> 00:37:05,109
images correctly and we probably could

00:37:01,990 --> 00:37:05,650
even do better but 95% is fine for us

00:37:05,109 --> 00:37:11,309
here

00:37:05,650 --> 00:37:13,960
so let's actually run the model so

00:37:11,309 --> 00:37:18,309
because of once we're happy of course we

00:37:13,960 --> 00:37:21,789
want to use it so here we see a very

00:37:18,309 --> 00:37:25,030
simple example and it seems to work be

00:37:21,789 --> 00:37:26,799
working quite nicely now I was something

00:37:25,030 --> 00:37:28,539
to show you a live demo here but I

00:37:26,799 --> 00:37:31,000
actually managed to screw up my model

00:37:28,539 --> 00:37:33,339
while trying last minutes improve last

00:37:31,000 --> 00:37:34,720
minute improvements so you have to

00:37:33,339 --> 00:37:38,049
believe me that it works from the

00:37:34,720 --> 00:37:39,539
screenshots but that brings me to my

00:37:38,049 --> 00:37:44,470
next slide

00:37:39,539 --> 00:37:47,829
deployment now that really depends on

00:37:44,470 --> 00:37:51,039
your target environment like cloud or

00:37:47,829 --> 00:37:52,660
even mobile devices but it's important

00:37:51,039 --> 00:37:55,299
to understand that neural networks are

00:37:52,660 --> 00:37:58,030
software artifacts so most of the things

00:37:55,299 --> 00:37:59,980
that apply to regular software also

00:37:58,030 --> 00:38:04,230
apply to new networks we have to think

00:37:59,980 --> 00:38:07,059
about versioning testing maintenance etc

00:38:04,230 --> 00:38:10,420
due to the data-driven nature of machine

00:38:07,059 --> 00:38:11,770
learning monitoring model accuracy is

00:38:10,420 --> 00:38:14,619
especially important

00:38:11,770 --> 00:38:17,770
so if curacy goes down over time that

00:38:14,619 --> 00:38:20,440
might indicate concept drift which means

00:38:17,770 --> 00:38:22,779
that newer input data differs from the

00:38:20,440 --> 00:38:26,380
formal training data and we should do

00:38:22,779 --> 00:38:32,549
regular retraining on fresh labeled data

00:38:26,380 --> 00:38:36,359
so to keep our models up-to-date so

00:38:32,549 --> 00:38:40,270
that's basically it main takeaways

00:38:36,359 --> 00:38:41,020
neural networks are functions and we

00:38:40,270 --> 00:38:42,430
learn them

00:38:41,020 --> 00:38:45,849
we usually learn them from known

00:38:42,430 --> 00:38:48,390
examples training can be challenging

00:38:45,849 --> 00:38:50,829
especially with larger networks and

00:38:48,390 --> 00:38:53,680
Scala is actually a great language for

00:38:50,829 --> 00:38:57,490
deep learning so if you interested give

00:38:53,680 --> 00:39:02,559
it a try use one of the libraries or get

00:38:57,490 --> 00:39:03,930
involved to make it even better thanks

00:39:02,559 --> 00:39:08,600
for listening

00:39:03,930 --> 00:39:08,600

YouTube URL: https://www.youtube.com/watch?v=06sVEKh6bVc


