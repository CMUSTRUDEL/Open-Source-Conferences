Title: Patterns for Streaming Telemetry with Akka Streams by Colin Breck
Publication date: 2018-09-20
Playlist: Scala Days Berlin 2018
Description: 
	This video was recorded at Scala Days Berlin 2018
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

More information and the abstract can be found here:
https://eu.scaladays.org/lect-6934-patterns-for-streaming-telemetry-with-akka-streams.html
Captions: 
	00:00:04,590 --> 00:00:09,870
so I've spent my entire career working

00:00:08,219 --> 00:00:12,150
on software systems that have

00:00:09,870 --> 00:00:15,830
incorporated streaming telemetry in one

00:00:12,150 --> 00:00:18,199
form or another and mainly in the

00:00:15,830 --> 00:00:20,369
applications for the industrial world I

00:00:18,199 --> 00:00:23,550
spent many years working on a time

00:00:20,369 --> 00:00:26,089
series database product for storing

00:00:23,550 --> 00:00:28,679
historical telemetry for for analysis

00:00:26,089 --> 00:00:30,210
I've worked on pub/sub systems for

00:00:28,679 --> 00:00:32,970
sharing telemetry among different

00:00:30,210 --> 00:00:36,120
applications and I've worked on data

00:00:32,970 --> 00:00:37,680
collection agents for control systems

00:00:36,120 --> 00:00:39,810
industrial equipment those kind of

00:00:37,680 --> 00:00:42,090
things we're doing reliable collection

00:00:39,810 --> 00:00:44,010
of telemetry and for the past few years

00:00:42,090 --> 00:00:46,080
I've been working on telemetry systems

00:00:44,010 --> 00:00:49,910
for the monitoring and control of

00:00:46,080 --> 00:00:54,780
distributed iot assets mainly for

00:00:49,910 --> 00:00:56,070
distributed renewable energy the

00:00:54,780 --> 00:00:57,900
telemetry from these systems is

00:00:56,070 --> 00:00:58,920
typically an unbounded stream it's just

00:00:57,900 --> 00:01:00,770
going to go on forever

00:00:58,920 --> 00:01:03,720
unbounded stream of measurements and

00:01:00,770 --> 00:01:05,580
it's usually these systems are designed

00:01:03,720 --> 00:01:07,110
such that that eventual consistency is

00:01:05,580 --> 00:01:08,369
okay but we do need to be careful to

00:01:07,110 --> 00:01:12,270
make sure that the messages are always

00:01:08,369 --> 00:01:14,399
ordered and these these systems usually

00:01:12,270 --> 00:01:16,049
require very reliable data collection

00:01:14,399 --> 00:01:17,909
and very low latency because they're

00:01:16,049 --> 00:01:19,159
being used to make operational decisions

00:01:17,909 --> 00:01:22,229
for the business

00:01:19,159 --> 00:01:24,030
so from these constraints and from these

00:01:22,229 --> 00:01:26,880
requirements some very common patterns

00:01:24,030 --> 00:01:28,679
emerge and that's what I'd like to talk

00:01:26,880 --> 00:01:31,079
about here today is how the acha streams

00:01:28,679 --> 00:01:33,600
API is really powerful for addressing

00:01:31,079 --> 00:01:36,509
these common patterns and it supports

00:01:33,600 --> 00:01:38,999
building extremely reliable scalable and

00:01:36,509 --> 00:01:41,340
resilient systems so this is a very

00:01:38,999 --> 00:01:43,409
introductory talk it's intended to be

00:01:41,340 --> 00:01:46,770
very introductory and it's mainly

00:01:43,409 --> 00:01:48,420
targeted at application developers so if

00:01:46,770 --> 00:01:50,639
you're new to Scala or akka I hope you

00:01:48,420 --> 00:01:53,310
find this talk rather accessible if

00:01:50,639 --> 00:01:54,689
you're familiar with akka already but

00:01:53,310 --> 00:01:56,130
perhaps you're not familiar with the

00:01:54,689 --> 00:01:58,530
streaming API is I hope you find it

00:01:56,130 --> 00:01:59,639
valuable if you're already familiar with

00:01:58,530 --> 00:02:01,679
everything that I talked about here

00:01:59,639 --> 00:02:03,240
today maybe this is the talk for you to

00:02:01,679 --> 00:02:05,399
share with your colleagues who aren't

00:02:03,240 --> 00:02:09,840
yet convinced to use the akka streams

00:02:05,399 --> 00:02:12,900
API and the talks broken into two pieces

00:02:09,840 --> 00:02:15,209
so the in the the first part I want to

00:02:12,900 --> 00:02:18,120
present a motivating example that really

00:02:15,209 --> 00:02:18,569
demonstrates the pitfalls of developing

00:02:18,120 --> 00:02:20,340
these

00:02:18,569 --> 00:02:23,010
systems for streaming telemetry without

00:02:20,340 --> 00:02:25,049
actually using explicit streaming api's

00:02:23,010 --> 00:02:26,639
and then the second part I want to

00:02:25,049 --> 00:02:28,680
explore these common patterns that come

00:02:26,639 --> 00:02:30,150
up again and again and show how they can

00:02:28,680 --> 00:02:32,670
be addressed with the akka streams API

00:02:30,150 --> 00:02:35,000
and then I'll just briefly conclude by

00:02:32,670 --> 00:02:38,189
coming back to the motivating example

00:02:35,000 --> 00:02:40,859
and just a disclaimer that I have to say

00:02:38,189 --> 00:02:42,900
before I begin is that this presentation

00:02:40,859 --> 00:02:44,849
is based on my personal experience and

00:02:42,900 --> 00:02:50,549
personal opinions doesn't necessarily

00:02:44,849 --> 00:02:53,730
represent the my any of my employers ok

00:02:50,549 --> 00:02:56,250
to begin a motivating example so let's

00:02:53,730 --> 00:02:58,349
say we have a fleet of wind turbines

00:02:56,250 --> 00:03:00,419
that we want to collect telemetry from

00:02:58,349 --> 00:03:03,510
so that we can control them or monitor

00:03:00,419 --> 00:03:05,760
them and we want to build a web service

00:03:03,510 --> 00:03:07,829
that those wind turbines are going to

00:03:05,760 --> 00:03:09,480
connect to and upload telemetry and that

00:03:07,829 --> 00:03:11,400
web service will be responsible for

00:03:09,480 --> 00:03:15,139
storing that telemetry in a database

00:03:11,400 --> 00:03:17,879
that can be used by other applications

00:03:15,139 --> 00:03:22,979
and I want to build a proof of concept

00:03:17,879 --> 00:03:25,049
for this system using akka actors and

00:03:22,979 --> 00:03:27,269
I'll start with an actor that's going to

00:03:25,049 --> 00:03:29,849
maintain the connection to the database

00:03:27,269 --> 00:03:33,060
so if you're not familiar with actors

00:03:29,849 --> 00:03:34,500
actors are really just a class and you

00:03:33,060 --> 00:03:36,509
interface with that class through

00:03:34,500 --> 00:03:39,449
message passing it's really fairly

00:03:36,509 --> 00:03:42,419
simple and this database actor is going

00:03:39,449 --> 00:03:45,150
to hold the database client so that it

00:03:42,419 --> 00:03:47,459
can interact with the database and

00:03:45,150 --> 00:03:49,889
whenever it receives a new insert

00:03:47,459 --> 00:03:52,799
message with a new sample from a wind

00:03:49,889 --> 00:03:55,169
turbine it's going to call this insert

00:03:52,799 --> 00:03:57,329
async method on the client to insert

00:03:55,169 --> 00:03:59,069
that sample into the database and for

00:03:57,329 --> 00:04:00,989
ignite for now since this is just a

00:03:59,069 --> 00:04:03,209
prototype proof of concept I'm going to

00:04:00,989 --> 00:04:06,629
ignore our error handling and return to

00:04:03,209 --> 00:04:09,199
it later in the presentation so the

00:04:06,629 --> 00:04:12,750
database actor can be created as follows

00:04:09,199 --> 00:04:15,180
and then it can be used in in this

00:04:12,750 --> 00:04:16,829
WebSocket flow so every wind turbine is

00:04:15,180 --> 00:04:18,590
going to have a WebSocket connection to

00:04:16,829 --> 00:04:20,609
our service for uploading telemetry

00:04:18,590 --> 00:04:22,289
WebSockets are actually implemented

00:04:20,609 --> 00:04:23,960
using the akka streams API which I'll

00:04:22,289 --> 00:04:26,340
return to a little bit later on but

00:04:23,960 --> 00:04:28,260
basically each time we get a new message

00:04:26,340 --> 00:04:30,900
from one of those wind turbines with

00:04:28,260 --> 00:04:32,430
some telemetry we're going to send that

00:04:30,900 --> 00:04:34,380
message to the database act

00:04:32,430 --> 00:04:36,660
we can share that database actor across

00:04:34,380 --> 00:04:38,430
connections and the database actor will

00:04:36,660 --> 00:04:41,850
be responsible for uploading the

00:04:38,430 --> 00:04:44,970
telemetry to the database so it's pretty

00:04:41,850 --> 00:04:47,310
simple so the first problem we run into

00:04:44,970 --> 00:04:48,870
when we start testing this application

00:04:47,310 --> 00:04:50,460
is that we have really poor performance

00:04:48,870 --> 00:04:53,400
because we're sending an individual

00:04:50,460 --> 00:04:55,110
measurement at a time to the database so

00:04:53,400 --> 00:04:58,650
we need to have some kind of grouping in

00:04:55,110 --> 00:05:01,889
order to have efficient writes so we're

00:04:58,650 --> 00:05:04,080
smart programmers so we can extend the

00:05:01,889 --> 00:05:06,330
the database actor to buffer a sequence

00:05:04,080 --> 00:05:09,180
of samples in memory and we'll keep

00:05:06,330 --> 00:05:10,949
track of the count so each time we get a

00:05:09,180 --> 00:05:12,870
new sample we'll just append that to the

00:05:10,949 --> 00:05:15,180
sequence and increment the count by one

00:05:12,870 --> 00:05:17,550
and then once the count reaches a

00:05:15,180 --> 00:05:19,590
thousand then we'll insert a batch of

00:05:17,550 --> 00:05:21,120
telemetry into the database and this

00:05:19,590 --> 00:05:23,729
will give us more efficient writes and

00:05:21,120 --> 00:05:27,120
the insert method looks as follows will

00:05:23,729 --> 00:05:29,580
call this bulk insert method and then

00:05:27,120 --> 00:05:32,120
we'll set the samples to nil and set the

00:05:29,580 --> 00:05:34,229
count to zero after a successful right

00:05:32,120 --> 00:05:35,880
okay so this delivers much better

00:05:34,229 --> 00:05:37,650
performance we're happy with that

00:05:35,880 --> 00:05:39,690
the next problem we run into is that

00:05:37,650 --> 00:05:41,250
message delivery in these systems can be

00:05:39,690 --> 00:05:43,770
can be quite irregular especially if

00:05:41,250 --> 00:05:45,889
telemetry is reported on change rather

00:05:43,770 --> 00:05:48,210
than on some regular interval and

00:05:45,889 --> 00:05:50,460
waiting until we have a batch of a

00:05:48,210 --> 00:05:52,199
thousand samples sometimes introduces

00:05:50,460 --> 00:05:54,419
too much latency into the system we want

00:05:52,199 --> 00:05:58,889
the data to be available in the database

00:05:54,419 --> 00:06:01,020
in near real-time so we can solve this

00:05:58,889 --> 00:06:03,030
problem in the database actor again by

00:06:01,020 --> 00:06:05,159
introducing a timer which is easy to do

00:06:03,030 --> 00:06:07,710
with an actor and we'll just send

00:06:05,159 --> 00:06:10,349
ourselves a tick message once a second

00:06:07,710 --> 00:06:12,030
and when we receive this tick message

00:06:10,349 --> 00:06:13,800
we'll call that insert method in order

00:06:12,030 --> 00:06:15,659
to insert whatever has been buffered up

00:06:13,800 --> 00:06:18,330
to this point in memory into the

00:06:15,659 --> 00:06:24,539
database and then we'll reset the timer

00:06:18,330 --> 00:06:26,310
to expire in another second so we this

00:06:24,539 --> 00:06:28,199
solves a problem in terms of having the

00:06:26,310 --> 00:06:29,699
telemetry in near-real-time in the

00:06:28,199 --> 00:06:32,340
database it's it'll be there within

00:06:29,699 --> 00:06:34,470
roughly one second now but of course we

00:06:32,340 --> 00:06:36,510
worked hard previously to batch our

00:06:34,470 --> 00:06:38,159
writes at a thousand and now we could

00:06:36,510 --> 00:06:39,810
have one of these timers fire right

00:06:38,159 --> 00:06:42,090
after we've just inserted a complete

00:06:39,810 --> 00:06:44,969
batch and of course this bothers us a

00:06:42,090 --> 00:06:47,010
little bit and we can't resist fixing

00:06:44,969 --> 00:06:49,560
it's probably premature optimization but

00:06:47,010 --> 00:06:51,690
let's address it anyway and we can

00:06:49,560 --> 00:06:53,789
address this by introducing another

00:06:51,690 --> 00:06:56,159
variable but of course this is getting a

00:06:53,789 --> 00:06:58,729
little hard to read now so I'll zoom in

00:06:56,159 --> 00:07:01,469
a bit so we could introduce a boolean

00:06:58,729 --> 00:07:03,300
variable that indicates whether we've

00:07:01,469 --> 00:07:07,050
written samples to the database in the

00:07:03,300 --> 00:07:10,050
last interval so when we do have a full

00:07:07,050 --> 00:07:11,729
batch of a thousand we'll set this flush

00:07:10,050 --> 00:07:14,820
variable to false and then when our

00:07:11,729 --> 00:07:16,500
periodic one-second timer expires if

00:07:14,820 --> 00:07:19,320
we've already inserted a batch within

00:07:16,500 --> 00:07:21,060
the last second we'll just skip the

00:07:19,320 --> 00:07:24,570
periodic insert and then reset the

00:07:21,060 --> 00:07:26,099
boolean okay

00:07:24,570 --> 00:07:28,229
another thing we notice when we start

00:07:26,099 --> 00:07:30,330
testing this proof-of-concept at scale

00:07:28,229 --> 00:07:32,370
is that sometimes we can overwhelm the

00:07:30,330 --> 00:07:34,380
database with writes and this starts to

00:07:32,370 --> 00:07:36,570
negatively impact other applications

00:07:34,380 --> 00:07:38,639
that are using it so some somehow we

00:07:36,570 --> 00:07:41,099
need to throttle these requests against

00:07:38,639 --> 00:07:44,669
the database and limit the concurrency

00:07:41,099 --> 00:07:46,669
through some of these dynamics so we can

00:07:44,669 --> 00:07:49,830
solve this by introducing yet another

00:07:46,669 --> 00:07:51,750
mutable variable we're going to keep

00:07:49,830 --> 00:07:54,270
track of another the number of

00:07:51,750 --> 00:07:56,490
outstanding requests and each time we

00:07:54,270 --> 00:07:57,870
initiate a new asynchronous request to

00:07:56,490 --> 00:08:00,510
the database we'll increment this

00:07:57,870 --> 00:08:02,820
variable by one and we'll never let more

00:08:00,510 --> 00:08:05,550
than ten outstanding requests to the

00:08:02,820 --> 00:08:07,110
database occur at any one time of course

00:08:05,550 --> 00:08:09,719
this also means if we want to maintain

00:08:07,110 --> 00:08:13,260
our our batch size of a thousand now we

00:08:09,719 --> 00:08:15,770
need to split that sequence and keep

00:08:13,260 --> 00:08:18,150
track of what's remaining in memory and

00:08:15,770 --> 00:08:20,789
it also means that we need to decrement

00:08:18,150 --> 00:08:23,820
that variable so when the when the

00:08:20,789 --> 00:08:25,770
database insert completes we're going to

00:08:23,820 --> 00:08:28,830
send ourselves a message in order to

00:08:25,770 --> 00:08:30,479
decrement the counter and when we

00:08:28,830 --> 00:08:33,060
received that decrement message will

00:08:30,479 --> 00:08:34,409
decrement the counter by one and if

00:08:33,060 --> 00:08:36,419
there are still more than a thousand

00:08:34,409 --> 00:08:38,010
outstanding events in memory we'll

00:08:36,419 --> 00:08:39,390
insert those into the database and of

00:08:38,010 --> 00:08:43,050
course we can't remember to set that

00:08:39,390 --> 00:08:44,219
magic flush variable to false okay so

00:08:43,050 --> 00:08:45,800
how does how does our proof-of-concept

00:08:44,219 --> 00:08:47,839
look

00:08:45,800 --> 00:08:50,269
we started out with this very simple

00:08:47,839 --> 00:08:51,649
database actor it was easy to get our

00:08:50,269 --> 00:08:54,610
heads around and it looked fairly

00:08:51,649 --> 00:08:57,249
promising and we've ended up with this

00:08:54,610 --> 00:08:59,509
and if i zoom in on this insert method

00:08:57,249 --> 00:09:01,639
at first glance it's really not even

00:08:59,509 --> 00:09:03,410
clear what this is doing in fact I'd

00:09:01,639 --> 00:09:06,920
love if there's a bug in here because it

00:09:03,410 --> 00:09:08,660
would it would kind of prove my point so

00:09:06,920 --> 00:09:10,699
we've seen that the the actor is good at

00:09:08,660 --> 00:09:12,139
managing that mutable state all that

00:09:10,699 --> 00:09:13,429
mutable States in the actor we didn't

00:09:12,139 --> 00:09:15,049
have to sprinkle it throughout of our

00:09:13,429 --> 00:09:16,189
throughout our application and we didn't

00:09:15,049 --> 00:09:17,480
have to make changes throughout our

00:09:16,189 --> 00:09:19,939
application it's all contained within

00:09:17,480 --> 00:09:22,429
the actor that was nice we haven't had

00:09:19,939 --> 00:09:25,279
to worry about thread safety or locking

00:09:22,429 --> 00:09:27,350
that's all been encapsulated within the

00:09:25,279 --> 00:09:29,619
actor and certainly the timer's within

00:09:27,350 --> 00:09:32,059
the actor itself those were easy to use

00:09:29,619 --> 00:09:33,920
but if we take a step back we're having

00:09:32,059 --> 00:09:35,389
to deal with all the the same old

00:09:33,920 --> 00:09:37,879
problems we had to deal with in in

00:09:35,389 --> 00:09:40,459
multi-threaded programs in C++ Java

00:09:37,879 --> 00:09:42,170
c-sharp and in dealing with concurrency

00:09:40,459 --> 00:09:44,019
we're having to manage memory we're

00:09:42,170 --> 00:09:46,249
having to manage flow control and

00:09:44,019 --> 00:09:48,709
certainly the testing of this actor is

00:09:46,249 --> 00:09:51,980
not going to be trivial at all the way

00:09:48,709 --> 00:09:53,959
it's currently structured and I haven't

00:09:51,980 --> 00:09:56,389
even considered error handling yet I

00:09:53,959 --> 00:09:58,639
kind of ignored that and that's going to

00:09:56,389 --> 00:10:00,319
add significant complexity to this actor

00:09:58,639 --> 00:10:02,449
especially if we need to maintain order

00:10:00,319 --> 00:10:05,600
which is often the case in systems that

00:10:02,449 --> 00:10:08,569
stream telemetry and of course we've

00:10:05,600 --> 00:10:10,490
done a good job to throttle the requests

00:10:08,569 --> 00:10:12,470
against the database now but we've done

00:10:10,490 --> 00:10:15,230
nothing to throttle calls against the

00:10:12,470 --> 00:10:18,199
service itself and by default actor

00:10:15,230 --> 00:10:22,360
mailboxes are unbounded so if we have

00:10:18,199 --> 00:10:22,360
thousands of wind turbines

00:10:30,880 --> 00:10:35,870
yeah you get the point so we're gonna

00:10:34,490 --> 00:10:38,149
need to implement some kind of flow

00:10:35,870 --> 00:10:40,520
control in order to you know tell the

00:10:38,149 --> 00:10:42,380
wind turbines to slow down that the

00:10:40,520 --> 00:10:44,330
service itself is overwhelmed and of

00:10:42,380 --> 00:10:47,149
course that's going to add significant

00:10:44,330 --> 00:10:48,800
complexity to this application so I'd

00:10:47,149 --> 00:10:51,470
say that our proof of concept actually

00:10:48,800 --> 00:10:53,450
looks pretty shattered as well as I'd

00:10:51,470 --> 00:10:56,870
say our faith in actors at this point

00:10:53,450 --> 00:10:58,100
especially for streaming workloads so

00:10:56,870 --> 00:11:01,550
let's just want to briefly take a look

00:10:58,100 --> 00:11:04,970
at reactive streams so the reactive

00:11:01,550 --> 00:11:06,470
streams is it's a specification that's

00:11:04,970 --> 00:11:10,459
focused on a synchronous stream

00:11:06,470 --> 00:11:12,290
processing a lot of and it delivers

00:11:10,459 --> 00:11:14,149
non-blocking flow control so a lot of

00:11:12,290 --> 00:11:16,160
systems implement flow control either

00:11:14,149 --> 00:11:18,260
through blocking or polling whereas

00:11:16,160 --> 00:11:19,880
reactive streams is focused on doing

00:11:18,260 --> 00:11:22,820
that in an asynchronous manner that's

00:11:19,880 --> 00:11:24,500
more efficient and what it ends up doing

00:11:22,820 --> 00:11:26,000
is it gives us bounded resource

00:11:24,500 --> 00:11:28,190
constraints and it makes our

00:11:26,000 --> 00:11:30,080
applications run really reliably and

00:11:28,190 --> 00:11:32,240
it's also focused on interoperability

00:11:30,080 --> 00:11:35,870
and operability of libraries and

00:11:32,240 --> 00:11:39,170
interoperability of systems and here's

00:11:35,870 --> 00:11:40,880
an example part of the specification for

00:11:39,170 --> 00:11:42,350
the subscriber you can see it's a it's a

00:11:40,880 --> 00:11:44,450
small interface and this gives a

00:11:42,350 --> 00:11:47,839
sampling of some of the specification

00:11:44,450 --> 00:11:49,640
here and here's an example of a reactive

00:11:47,839 --> 00:11:52,130
streams open source library that I

00:11:49,640 --> 00:11:54,050
contributed a little bit to you can see

00:11:52,130 --> 00:11:56,029
that we need to worry about things like

00:11:54,050 --> 00:12:00,290
variables being null in order to make

00:11:56,029 --> 00:12:01,700
sure that we obey the specification if

00:12:00,290 --> 00:12:04,700
you are writing your own libraries make

00:12:01,700 --> 00:12:06,140
sure you use the TCK out there to make

00:12:04,700 --> 00:12:08,029
sure that you're compliant and these

00:12:06,140 --> 00:12:10,040
kind of things we also need to worry

00:12:08,029 --> 00:12:12,260
about operations being atomic right

00:12:10,040 --> 00:12:13,880
we're not in an actor here so we need to

00:12:12,260 --> 00:12:15,890
make sure that we're performing certain

00:12:13,880 --> 00:12:19,820
operations atomically we also need to

00:12:15,890 --> 00:12:21,650
worry about threads in some cases so I

00:12:19,820 --> 00:12:25,190
think this this is a point that's that's

00:12:21,650 --> 00:12:26,540
often misunderstood people hear about

00:12:25,190 --> 00:12:28,880
reactive streams or they read the

00:12:26,540 --> 00:12:30,170
reactive streams specification they get

00:12:28,880 --> 00:12:32,450
excited because it's the thing that's

00:12:30,170 --> 00:12:34,520
going to solve their problems but it's

00:12:32,450 --> 00:12:36,200
really intended for library developers

00:12:34,520 --> 00:12:38,089
the average application developer

00:12:36,200 --> 00:12:39,860
shouldn't be programming at this level

00:12:38,089 --> 00:12:42,380
of abstraction

00:12:39,860 --> 00:12:45,200
we want to be using a higher level end

00:12:42,380 --> 00:12:48,560
user API and there's a number of those

00:12:45,200 --> 00:12:50,930
API is out there and akka streams API is

00:12:48,560 --> 00:12:54,440
my particular favorite I really really

00:12:50,930 --> 00:12:57,320
love the akka streams API so just

00:12:54,440 --> 00:12:59,540
briefly looking at akka streams API it

00:12:57,320 --> 00:13:01,490
has this notion of sources which are

00:12:59,540 --> 00:13:03,830
elements that only emit messages they

00:13:01,490 --> 00:13:07,580
don't actually have any inputs and they

00:13:03,830 --> 00:13:09,440
have sinks which have messages that come

00:13:07,580 --> 00:13:11,240
in but they don't have any outputs and

00:13:09,440 --> 00:13:13,040
then there's these elements that have

00:13:11,240 --> 00:13:14,630
messages that come in and go out and

00:13:13,040 --> 00:13:16,670
they can perform different

00:13:14,630 --> 00:13:19,520
transformations like mapping filtering

00:13:16,670 --> 00:13:22,310
throttling and so on and it could be

00:13:19,520 --> 00:13:25,070
composed together into flows in simple

00:13:22,310 --> 00:13:27,170
cases like maybe an ETL application or

00:13:25,070 --> 00:13:30,170
in more complex cases where you need to

00:13:27,170 --> 00:13:34,400
merge streams or bifurcate streams they

00:13:30,170 --> 00:13:36,440
can be arranged into graphs and of

00:13:34,400 --> 00:13:38,960
course because the acha streams API

00:13:36,440 --> 00:13:40,880
obeys the reactive stream specification

00:13:38,960 --> 00:13:43,610
there's this asynchronous back pressure

00:13:40,880 --> 00:13:45,650
throughout the pipeline and what that

00:13:43,610 --> 00:13:47,780
means is that the the upstream basically

00:13:45,650 --> 00:13:49,340
can't overwhelm the downstream because

00:13:47,780 --> 00:13:51,020
the upstream doesn't push messages

00:13:49,340 --> 00:13:54,050
downstream the downstream requests

00:13:51,020 --> 00:13:55,700
demand from upstream and this can be

00:13:54,050 --> 00:13:58,810
done asynchronously and it can be

00:13:55,700 --> 00:14:01,910
batched so it's actually quite efficient

00:13:58,810 --> 00:14:05,030
okay part two the presentation I want to

00:14:01,910 --> 00:14:07,700
take a look at a bunch of patterns some

00:14:05,030 --> 00:14:08,990
of which we've already seen that occur

00:14:07,700 --> 00:14:11,090
all the time when you're dealing with

00:14:08,990 --> 00:14:12,950
streaming telemetry and look at how the

00:14:11,090 --> 00:14:14,810
akka streams API can be used to address

00:14:12,950 --> 00:14:17,120
those and I want to provide tiny little

00:14:14,810 --> 00:14:19,010
self-contained examples that hopefully

00:14:17,120 --> 00:14:26,000
are easy to understand as well as a

00:14:19,010 --> 00:14:27,500
visual representation for each one okay

00:14:26,000 --> 00:14:30,320
so the first pattern we've already seen

00:14:27,500 --> 00:14:33,500
which is is grouping messages so that we

00:14:30,320 --> 00:14:35,360
can have efficient writes and this can

00:14:33,500 --> 00:14:37,730
be done with the grouped element from

00:14:35,360 --> 00:14:40,310
the akka streams API and basically what

00:14:37,730 --> 00:14:42,350
it does is you set a group size and it's

00:14:40,310 --> 00:14:44,990
going to batch elements up to the group

00:14:42,350 --> 00:14:47,400
size and then emit a sequence of

00:14:44,990 --> 00:14:49,410
elements downstream

00:14:47,400 --> 00:14:51,540
and in terms of code it looks like the

00:14:49,410 --> 00:14:53,610
following so here's a source that's

00:14:51,540 --> 00:14:56,339
going to tick every ten milliseconds and

00:14:53,610 --> 00:14:58,589
I'm going to construct a random sample

00:14:56,339 --> 00:15:00,900
with the current time and a random value

00:14:58,589 --> 00:15:03,660
and then I'm going to group those in a

00:15:00,900 --> 00:15:06,480
batch of a thousand and insert them into

00:15:03,660 --> 00:15:08,100
the database and run with sync ignore

00:15:06,480 --> 00:15:11,460
because I'm not interested in in the

00:15:08,100 --> 00:15:13,050
results now a problem that we've already

00:15:11,460 --> 00:15:14,640
seen is that sometimes grouping

00:15:13,050 --> 00:15:16,400
introduces too much latency in these

00:15:14,640 --> 00:15:18,450
telemetry systems because we're making

00:15:16,400 --> 00:15:21,029
when we want to make near real-time

00:15:18,450 --> 00:15:25,470
operational decisions with them we need

00:15:21,029 --> 00:15:27,240
to be sensitive to latency so the way to

00:15:25,470 --> 00:15:30,330
do this with our streams is the grouped

00:15:27,240 --> 00:15:32,610
within element and so grouped within can

00:15:30,330 --> 00:15:35,220
be used by just replacing a grouped with

00:15:32,610 --> 00:15:38,700
grouped within and what this is gonna do

00:15:35,220 --> 00:15:41,250
is emit elements downstream in either

00:15:38,700 --> 00:15:43,410
batches of a thousand or whatever's been

00:15:41,250 --> 00:15:46,200
buffered within a hundred milliseconds

00:15:43,410 --> 00:15:48,360
whatever comes first so the batch sizes

00:15:46,200 --> 00:15:51,510
downstream won't won't necessarily be

00:15:48,360 --> 00:15:53,220
uniform anymore but you're gonna you're

00:15:51,510 --> 00:15:56,880
gonna reduce you're gonna have a kind of

00:15:53,220 --> 00:15:58,200
maximum bound on the latency okay

00:15:56,880 --> 00:16:00,900
another common pattern especially that

00:15:58,200 --> 00:16:02,970
happens in IOT is the need for

00:16:00,900 --> 00:16:04,800
disaggregation sometimes you have I of T

00:16:02,970 --> 00:16:06,540
devices that are offline for a while and

00:16:04,800 --> 00:16:09,330
they buffer up a bunch of messages and

00:16:06,540 --> 00:16:12,540
you want to disaggregate those or often

00:16:09,330 --> 00:16:14,580
IOT devices will send aggregate messages

00:16:12,540 --> 00:16:17,400
with samples from many sensors and you

00:16:14,580 --> 00:16:19,620
might want to also disaggregate those so

00:16:17,400 --> 00:16:22,339
let's revisit the the wind turbines and

00:16:19,620 --> 00:16:25,650
let's say it uploads this JSON message

00:16:22,339 --> 00:16:29,459
which has an ID a unique ID for the for

00:16:25,650 --> 00:16:33,180
the asset it has a timestamp and then it

00:16:29,459 --> 00:16:36,650
has this array of measurements one for

00:16:33,180 --> 00:16:40,529
the power one for the rotor speed and

00:16:36,650 --> 00:16:42,529
one for the wind speed and let's say we

00:16:40,529 --> 00:16:45,540
want to take that JSON message and

00:16:42,529 --> 00:16:48,510
disaggregate it into a measurement where

00:16:45,540 --> 00:16:51,360
the ID is the unique ID of the asset the

00:16:48,510 --> 00:16:53,730
timestamp is the timestamp the signal is

00:16:51,360 --> 00:16:56,579
the name of the measurement and then the

00:16:53,730 --> 00:16:58,600
value is the value we want to construct

00:16:56,579 --> 00:17:01,110
one of these for the power

00:16:58,600 --> 00:17:04,620
and one of these for the rotor speed and

00:17:01,110 --> 00:17:07,120
one of these for the wind speed and

00:17:04,620 --> 00:17:09,100
there'll be this parse message that'll

00:17:07,120 --> 00:17:10,449
take in the JSON and emit this sequence

00:17:09,100 --> 00:17:13,120
of measurements and I'll leave that up

00:17:10,449 --> 00:17:14,920
to your imagination so the way to do

00:17:13,120 --> 00:17:17,920
this disaggregation with akka streams

00:17:14,920 --> 00:17:19,570
API is using map can cat and so what map

00:17:17,920 --> 00:17:22,390
can cat does it takes it takes in an

00:17:19,570 --> 00:17:24,400
aggregate has some way of disaggregating

00:17:22,390 --> 00:17:28,600
it and then emitting individual elements

00:17:24,400 --> 00:17:33,190
downstream what this looks like here's

00:17:28,600 --> 00:17:34,570
here's a source with JSON and this is

00:17:33,190 --> 00:17:37,270
our parse function that's going to emit

00:17:34,570 --> 00:17:39,730
a sequence of elements and map can cat

00:17:37,270 --> 00:17:41,950
is going to emit those as individual

00:17:39,730 --> 00:17:43,990
elements downstream then again we can

00:17:41,950 --> 00:17:46,030
group them back up in a batch of a

00:17:43,990 --> 00:17:47,850
thousand and insert them into the

00:17:46,030 --> 00:17:50,440
database

00:17:47,850 --> 00:17:52,750
okay the next pattern I want to look at

00:17:50,440 --> 00:17:54,910
is filtering this again this often

00:17:52,750 --> 00:17:57,730
happens again in IOT applications you

00:17:54,910 --> 00:17:59,230
want to filter out legacy messages or

00:17:57,730 --> 00:18:01,450
messages that you're not interested in

00:17:59,230 --> 00:18:04,230
or messages that are improperly

00:18:01,450 --> 00:18:06,400
formatted that you can't deserialize or

00:18:04,230 --> 00:18:08,800
values that are impossible like a

00:18:06,400 --> 00:18:11,470
battery with a negative state of charge

00:18:08,800 --> 00:18:13,480
something like that it's often really

00:18:11,470 --> 00:18:17,020
important to clean up the telemetry on

00:18:13,480 --> 00:18:18,340
ingest of these applications so there's

00:18:17,020 --> 00:18:21,070
there's a bunch of ways to filter

00:18:18,340 --> 00:18:24,550
streams with the acha streams API the

00:18:21,070 --> 00:18:27,520
first case is filter and what filter

00:18:24,550 --> 00:18:29,080
does is it takes a boolean function so

00:18:27,520 --> 00:18:31,590
here's a here's a source that's going to

00:18:29,080 --> 00:18:34,240
emit the integers from one to ten and

00:18:31,590 --> 00:18:36,610
it's going to filter out the integers

00:18:34,240 --> 00:18:39,100
that are odd and emit the even integers

00:18:36,610 --> 00:18:41,050
downstream so whatever returns true will

00:18:39,100 --> 00:18:43,300
be passed downstream whatever we return

00:18:41,050 --> 00:18:44,200
false for will be filtered out and of

00:18:43,300 --> 00:18:48,000
course this can be written more

00:18:44,200 --> 00:18:51,280
succinctly in this case as follows

00:18:48,000 --> 00:18:54,700
another way to filter streams is using

00:18:51,280 --> 00:18:56,860
collect or collect type so collect takes

00:18:54,700 --> 00:18:59,500
a partial function and it'll filter out

00:18:56,860 --> 00:19:01,780
whatever we don't match on so this is

00:18:59,500 --> 00:19:04,650
the equivalent of the the filter case

00:19:01,780 --> 00:19:04,650
that we just looked at

00:19:05,810 --> 00:19:10,620
probably a little more realistic example

00:19:08,430 --> 00:19:12,810
for collect is if you have say a map

00:19:10,620 --> 00:19:15,900
stage upstream that's mapping the values

00:19:12,810 --> 00:19:19,800
into valid values for the even integers

00:19:15,900 --> 00:19:21,660
and invalid values for the for the odd

00:19:19,800 --> 00:19:23,970
integers and then you can collect the

00:19:21,660 --> 00:19:26,130
valid ones downstream and note that you

00:19:23,970 --> 00:19:27,600
can also you can also transform the

00:19:26,130 --> 00:19:29,040
element you can operate on the

00:19:27,600 --> 00:19:31,320
individual element here or turning it

00:19:29,040 --> 00:19:32,940
into a string so in the filter case it's

00:19:31,320 --> 00:19:34,500
just a boolean and it passes the

00:19:32,940 --> 00:19:36,810
original element downstream in the

00:19:34,500 --> 00:19:39,270
collect you're allowed to operate on the

00:19:36,810 --> 00:19:41,790
element if all you want to do is just

00:19:39,270 --> 00:19:43,890
filter based on type you can use collect

00:19:41,790 --> 00:19:45,540
type so whatever you match on will be

00:19:43,890 --> 00:19:48,690
passed downstream and whatever it

00:19:45,540 --> 00:19:51,570
doesn't match will be filtered out now

00:19:48,690 --> 00:19:53,220
filtering in these in these telemetry

00:19:51,570 --> 00:19:54,510
systems it often feels like the right

00:19:53,220 --> 00:19:57,870
thing to do but when you start running

00:19:54,510 --> 00:19:59,520
production systems usually you find out

00:19:57,870 --> 00:20:01,560
that you don't actually want to filter

00:19:59,520 --> 00:20:03,210
things out you want to inspect those in

00:20:01,560 --> 00:20:05,130
some way you want to increment counters

00:20:03,210 --> 00:20:07,380
you want to log them because you want to

00:20:05,130 --> 00:20:09,240
answer questions like why is this device

00:20:07,380 --> 00:20:11,190
sending me a message that I can't

00:20:09,240 --> 00:20:14,310
deserialize and so that you can address

00:20:11,190 --> 00:20:17,970
that so one way to do this is with

00:20:14,310 --> 00:20:20,160
divert to and what divert to will allow

00:20:17,970 --> 00:20:22,950
you to do is send certain messages to a

00:20:20,160 --> 00:20:26,940
sink and pass the rest of the message

00:20:22,950 --> 00:20:29,040
down rest of the messages downstream so

00:20:26,940 --> 00:20:31,080
as an example here's a logging sink

00:20:29,040 --> 00:20:32,880
that's going to log all of the invalid

00:20:31,080 --> 00:20:35,480
messages so that we could inspect those

00:20:32,880 --> 00:20:38,220
later and maybe take some action and

00:20:35,480 --> 00:20:40,670
then we'll use it with an either monad

00:20:38,220 --> 00:20:43,050
so the left side will be valid

00:20:40,670 --> 00:20:45,180
measurements and the right side will be

00:20:43,050 --> 00:20:48,690
invalid measurements and then we're

00:20:45,180 --> 00:20:50,580
gonna divert all of the invalid messages

00:20:48,690 --> 00:20:52,380
to that error sink so that we can log

00:20:50,580 --> 00:20:54,240
them and then we'll pass all of the

00:20:52,380 --> 00:20:57,930
valid ones downstream so that we can

00:20:54,240 --> 00:20:59,910
operate on them independently and just

00:20:57,930 --> 00:21:02,130
one more thing worth mentioning with

00:20:59,910 --> 00:21:05,310
with all of these patterns that involve

00:21:02,130 --> 00:21:07,680
filtering a lot of these telemetry

00:21:05,310 --> 00:21:10,050
systems will have often have some kind

00:21:07,680 --> 00:21:13,260
of durable queue as part of the pipeline

00:21:10,050 --> 00:21:14,550
something like Apache Kafka and of

00:21:13,260 --> 00:21:15,840
course when you're reading off these

00:21:14,550 --> 00:21:16,970
durable queues you need to commit

00:21:15,840 --> 00:21:18,909
offsets

00:21:16,970 --> 00:21:21,020
and what this what ends up happening is

00:21:18,909 --> 00:21:23,090
even though you want to filter out

00:21:21,020 --> 00:21:25,010
messages in your pipeline often you need

00:21:23,090 --> 00:21:27,200
to send every message all the way

00:21:25,010 --> 00:21:30,820
downstream so that you can commit your

00:21:27,200 --> 00:21:33,830
offsets and these filtering patterns I

00:21:30,820 --> 00:21:35,630
actually heard become a little bit of an

00:21:33,830 --> 00:21:39,830
anti-pattern I think when you're reading

00:21:35,630 --> 00:21:41,360
off of a durable queue okay another

00:21:39,830 --> 00:21:44,059
pattern that we've already seen is

00:21:41,360 --> 00:21:48,260
limiting requests against another

00:21:44,059 --> 00:21:49,789
service or a database so here's the

00:21:48,260 --> 00:21:53,299
example we've been looking at where

00:21:49,789 --> 00:21:56,419
we're we have a synchronous call to the

00:21:53,299 --> 00:21:57,890
database to insert the telemetry if I

00:21:56,419 --> 00:22:01,250
replace that with an asynchronous

00:21:57,890 --> 00:22:02,990
request this is a mistake a lot of

00:22:01,250 --> 00:22:05,510
people that are new to akka streams API

00:22:02,990 --> 00:22:06,950
make its even it's a mistake that I

00:22:05,510 --> 00:22:09,909
still make from time to time because

00:22:06,950 --> 00:22:11,750
this will compile and it'll run and

00:22:09,909 --> 00:22:14,900
essentially what you're doing here is

00:22:11,750 --> 00:22:16,190
you're passing downstream futures but

00:22:14,900 --> 00:22:18,650
the futures haven't necessarily been

00:22:16,190 --> 00:22:20,659
completed yet so essentially what you've

00:22:18,650 --> 00:22:22,909
done is eliminated the back pressure

00:22:20,659 --> 00:22:25,429
this this stream this element is just

00:22:22,909 --> 00:22:27,289
going to emit futures as fast as the

00:22:25,429 --> 00:22:28,309
downstream will accept them so what you

00:22:27,289 --> 00:22:31,100
really want when you're dealing with

00:22:28,309 --> 00:22:33,020
asynchronous methods is to use map async

00:22:31,100 --> 00:22:34,700
and then map async also has a

00:22:33,020 --> 00:22:37,460
parallelism factor which allows you to

00:22:34,700 --> 00:22:40,490
control how many outstanding requests

00:22:37,460 --> 00:22:42,530
are being made at any one time and this

00:22:40,490 --> 00:22:48,200
is as simple as taking the map stage and

00:22:42,530 --> 00:22:49,669
just replacing it with not async when

00:22:48,200 --> 00:22:51,380
what map a sinks going to do is going to

00:22:49,669 --> 00:22:53,929
unwrap it's going to wait for the

00:22:51,380 --> 00:22:55,940
futures to complete and unwrap the value

00:22:53,929 --> 00:22:56,929
that's inside and pass that downstream

00:22:55,940 --> 00:22:58,700
so you don't have to do any of that

00:22:56,929 --> 00:23:00,710
yourself and then in terms of

00:22:58,700 --> 00:23:02,600
controlling the parallelism it's as

00:23:00,710 --> 00:23:04,220
simple as just changing the parallelism

00:23:02,600 --> 00:23:07,039
factor so if we want to go from two to

00:23:04,220 --> 00:23:08,720
four we just change that number it's

00:23:07,039 --> 00:23:11,960
very easy to experiment with the amount

00:23:08,720 --> 00:23:13,730
of parallelism in the system another

00:23:11,960 --> 00:23:16,299
important thing to note about map async

00:23:13,730 --> 00:23:19,039
is that it's going to emit the elements

00:23:16,299 --> 00:23:22,940
downstream in order no matter which

00:23:19,039 --> 00:23:25,159
order the futures complete in and this

00:23:22,940 --> 00:23:28,460
is often in systems that stream

00:23:25,159 --> 00:23:30,620
telemetry very important or events or

00:23:28,460 --> 00:23:34,730
systems you want to maintain the order

00:23:30,620 --> 00:23:37,700
all the way downstream if you don't care

00:23:34,730 --> 00:23:39,920
about order you can change the use map

00:23:37,700 --> 00:23:41,410
async unordered in some cases and what

00:23:39,920 --> 00:23:44,870
this will do is it will omit those

00:23:41,410 --> 00:23:47,300
values from those futures downstream as

00:23:44,870 --> 00:23:50,030
fast as they commit as fast as they

00:23:47,300 --> 00:23:53,480
complete basically so if you have a kind

00:23:50,030 --> 00:23:56,270
of head of line blocking and and you can

00:23:53,480 --> 00:23:58,400
tolerate the messages being out of order

00:23:56,270 --> 00:24:01,070
map async unordered can sometimes

00:23:58,400 --> 00:24:02,840
deliver better performance and it's as

00:24:01,070 --> 00:24:07,730
simple as replacing the map async with

00:24:02,840 --> 00:24:11,690
that basic unordered ok the next pattern

00:24:07,730 --> 00:24:14,780
is throttling so this is a pattern where

00:24:11,690 --> 00:24:17,110
you have a fast upstream and a slower

00:24:14,780 --> 00:24:20,030
downstream that's usually defined by

00:24:17,110 --> 00:24:22,160
some kind of like service limits or SLA

00:24:20,030 --> 00:24:25,910
something like that or you can only have

00:24:22,160 --> 00:24:27,650
so many requests per second and this can

00:24:25,910 --> 00:24:29,840
be done with the throttle element so

00:24:27,650 --> 00:24:32,929
fast upstream and you want to emit at

00:24:29,840 --> 00:24:34,070
some kind of defined rate downstream so

00:24:32,929 --> 00:24:37,070
here's an example that we've been

00:24:34,070 --> 00:24:39,200
looking at a few times this is gonna

00:24:37,070 --> 00:24:40,760
take every 10 seconds or every 10

00:24:39,200 --> 00:24:43,070
milliseconds and generate a random

00:24:40,760 --> 00:24:45,080
sample and then I'm gonna throttle it

00:24:43,070 --> 00:24:47,270
such that I'm only emitting one element

00:24:45,080 --> 00:24:49,970
downstream per second so even though the

00:24:47,270 --> 00:24:52,640
upstream is ticking at every 10

00:24:49,970 --> 00:24:55,160
milliseconds only one message per second

00:24:52,640 --> 00:24:56,990
or is going to be emitted downstream and

00:24:55,160 --> 00:24:59,450
there's a few different throttle modes

00:24:56,990 --> 00:25:02,990
here I'm using shaping which will back

00:24:59,450 --> 00:25:04,730
pressure to the upstream there's also a

00:25:02,990 --> 00:25:07,280
mode where you can fail this stream if

00:25:04,730 --> 00:25:08,600
you exceed the throttle most often when

00:25:07,280 --> 00:25:10,309
you're streaming telemetry though you

00:25:08,600 --> 00:25:13,450
don't want to fail the stream you just

00:25:10,309 --> 00:25:15,890
want to push back and back pressure

00:25:13,450 --> 00:25:18,740
another thing to note about throttle is

00:25:15,890 --> 00:25:20,900
that the if you're dealing with messages

00:25:18,740 --> 00:25:22,790
that are non-uniform you don't

00:25:20,900 --> 00:25:25,910
necessarily have to evenly spaced those

00:25:22,790 --> 00:25:29,900
messages with the throttle you can also

00:25:25,910 --> 00:25:31,820
burst them so here's an example of using

00:25:29,900 --> 00:25:34,280
the burst so what this is going to do is

00:25:31,820 --> 00:25:36,260
allow three messages a second downstream

00:25:34,280 --> 00:25:36,830
but if all three of those messages come

00:25:36,260 --> 00:25:38,990
right away

00:25:36,830 --> 00:25:41,510
it'll just emit all of them downstream

00:25:38,990 --> 00:25:43,810
and this will give you lower latency so

00:25:41,510 --> 00:25:46,870
it'll still respect the total for the

00:25:43,810 --> 00:25:48,580
of time but it'll have lower latency and

00:25:46,870 --> 00:25:52,240
that it'll let messages downstream right

00:25:48,580 --> 00:25:53,860
away okay the next pattern is

00:25:52,240 --> 00:25:55,570
concurrency

00:25:53,860 --> 00:25:57,580
there's actually four planes in this

00:25:55,570 --> 00:25:59,380
picture there's another one in the

00:25:57,580 --> 00:26:00,550
background on the Left I keep this is

00:25:59,380 --> 00:26:03,070
the first picture I picked when I

00:26:00,550 --> 00:26:04,930
thinking of concurrency and I wanted to

00:26:03,070 --> 00:26:08,980
finds like a guy juggling knives or

00:26:04,930 --> 00:26:10,810
something like that but since I picked

00:26:08,980 --> 00:26:12,820
this picture there's been two or three

00:26:10,810 --> 00:26:15,310
near-miss accidents on this runways

00:26:12,820 --> 00:26:19,080
which proves that concurrency is still

00:26:15,310 --> 00:26:19,080
hard this isn't this is in San Francisco

00:26:19,350 --> 00:26:28,780
so understanding that concurrency is

00:26:22,360 --> 00:26:31,570
hard by default most akka streams they

00:26:28,780 --> 00:26:32,890
they they run on top of actors when

00:26:31,570 --> 00:26:35,080
you're just using the akka streams API

00:26:32,890 --> 00:26:36,340
you you don't really need to know that

00:26:35,080 --> 00:26:39,130
there's actors there it's just kind of

00:26:36,340 --> 00:26:40,540
transparent to you one of the cases

00:26:39,130 --> 00:26:41,920
where you do need to understand this is

00:26:40,540 --> 00:26:45,550
when you when you want to introduce

00:26:41,920 --> 00:26:48,190
concurrency so by default actors use

00:26:45,550 --> 00:26:51,700
what's called operator fusing and it the

00:26:48,190 --> 00:26:53,680
stream will run on top of one actor and

00:26:51,700 --> 00:26:56,830
this is often the most efficient way to

00:26:53,680 --> 00:26:59,470
execute the stream but there are some

00:26:56,830 --> 00:27:01,840
workloads where you want the stream to

00:26:59,470 --> 00:27:04,360
run in parallel and the way to do this

00:27:01,840 --> 00:27:06,370
is using async and what it's going to do

00:27:04,360 --> 00:27:09,810
is insert an asynchronous boundary and

00:27:06,370 --> 00:27:12,130
now that stream will run on two actors

00:27:09,810 --> 00:27:13,990
potentially in parallel and it's going

00:27:12,130 --> 00:27:17,470
to use message passing in order to

00:27:13,990 --> 00:27:20,350
communicate between those two actors so

00:27:17,470 --> 00:27:24,970
here's an example of using async in this

00:27:20,350 --> 00:27:27,490
case we're gonna gzip encode a JSON

00:27:24,970 --> 00:27:30,130
string a million times and then gzip

00:27:27,490 --> 00:27:32,260
decode that string same string a million

00:27:30,130 --> 00:27:34,150
times and this will actually execute

00:27:32,260 --> 00:27:37,480
twice as fast assuming you have enough

00:27:34,150 --> 00:27:40,060
course with the async in-between because

00:27:37,480 --> 00:27:43,210
the to map stages can execute in

00:27:40,060 --> 00:27:44,860
parallel and it's also worth noting how

00:27:43,210 --> 00:27:46,240
easy it is to experiment with this

00:27:44,860 --> 00:27:48,130
parallelism here we didn't need to

00:27:46,240 --> 00:27:49,900
introduce threads we didn't need to

00:27:48,130 --> 00:27:51,700
introduce reader writer locks or

00:27:49,900 --> 00:27:54,460
anything like that all we needed to do

00:27:51,700 --> 00:27:55,050
is say async run our benchmarks again or

00:27:54,460 --> 00:27:56,940
runner

00:27:55,050 --> 00:28:01,590
and understand if that delivered better

00:27:56,940 --> 00:28:03,060
performance and I just want to make it

00:28:01,590 --> 00:28:07,950
clear because I already talked about map

00:28:03,060 --> 00:28:10,020
async and and futures that this actually

00:28:07,950 --> 00:28:11,850
doesn't introduce a asynchronous

00:28:10,020 --> 00:28:13,860
boundary so certainly those futures

00:28:11,850 --> 00:28:16,380
might execute on a different thread and

00:28:13,860 --> 00:28:18,330
they might in execute in parallel but

00:28:16,380 --> 00:28:20,780
there's still one actor that's managing

00:28:18,330 --> 00:28:24,810
this stream and there are some cases

00:28:20,780 --> 00:28:26,910
where it's advantageous to still use an

00:28:24,810 --> 00:28:29,670
asynchronous boundary even though your

00:28:26,910 --> 00:28:32,790
stream is using map async to execute

00:28:29,670 --> 00:28:35,490
futures and again this just comes down

00:28:32,790 --> 00:28:39,270
to benchmarking and profiling and your

00:28:35,490 --> 00:28:45,300
individual workload okay the next

00:28:39,270 --> 00:28:46,830
pattern is idle timeouts so the way the

00:28:45,300 --> 00:28:49,590
way to do this is with the idle timeout

00:28:46,830 --> 00:28:51,900
flow stage and the way it works is it

00:28:49,590 --> 00:28:53,490
you set a timeout and if no elements

00:28:51,900 --> 00:28:56,880
have been passed through that stage

00:28:53,490 --> 00:28:58,890
within the time out the the idle timeout

00:28:56,880 --> 00:29:01,680
element will throw an exception and fail

00:28:58,890 --> 00:29:04,980
the stream so where this is useful is in

00:29:01,680 --> 00:29:06,510
reclaiming resources so imagine those

00:29:04,980 --> 00:29:07,620
those wind turbines we're talking about

00:29:06,510 --> 00:29:09,330
earlier if they have a WebSocket

00:29:07,620 --> 00:29:11,490
connection to our service and that

00:29:09,330 --> 00:29:13,080
WebSocket connection goes idle we

00:29:11,490 --> 00:29:14,970
eventually want to reclaim those

00:29:13,080 --> 00:29:17,370
resources an idle timeout is one way to

00:29:14,970 --> 00:29:19,820
do that the other place I've found idle

00:29:17,370 --> 00:29:22,500
timeout really useful in systems that

00:29:19,820 --> 00:29:26,340
stream telemetry is in testing those

00:29:22,500 --> 00:29:28,770
systems often and end up using akka

00:29:26,340 --> 00:29:32,460
streams too it's kind of like a test

00:29:28,770 --> 00:29:34,350
probe throughout the system and maybe

00:29:32,460 --> 00:29:36,570
reading off something like Kafka or a

00:29:34,350 --> 00:29:38,580
pub/sub system and expecting messages

00:29:36,570 --> 00:29:40,110
within a certain time frame and if you

00:29:38,580 --> 00:29:41,850
don't receive those messages within that

00:29:40,110 --> 00:29:44,460
time frame idle timeouts a great way to

00:29:41,850 --> 00:29:48,300
have to assert certain behaviors and

00:29:44,460 --> 00:29:49,980
fail the stream so here's an example of

00:29:48,300 --> 00:29:52,140
using idle time owned this is this

00:29:49,980 --> 00:29:54,720
source is only going to tick every 10

00:29:52,140 --> 00:29:56,850
minutes and generate a random sample and

00:29:54,720 --> 00:29:59,040
then the idle timeout is set at a minute

00:29:56,850 --> 00:30:00,630
so source is taking it 10 minutes idle

00:29:59,040 --> 00:30:03,030
timeouts at 1 minute so the stream is

00:30:00,630 --> 00:30:05,970
going to fail and it's going to fail

00:30:03,030 --> 00:30:07,830
with this timeout exception and say this

00:30:05,970 --> 00:30:09,150
was you know one of those wind turbines

00:30:07,830 --> 00:30:12,000
we could actually

00:30:09,150 --> 00:30:14,970
log a message saying device with certain

00:30:12,000 --> 00:30:19,680
ID has been idle for a certain amount of

00:30:14,970 --> 00:30:21,840
time okay this is another pattern that

00:30:19,680 --> 00:30:23,940
we've already seen is in the motivating

00:30:21,840 --> 00:30:25,440
example is the need for some kind of

00:30:23,940 --> 00:30:26,910
periodic event so this might be

00:30:25,440 --> 00:30:29,520
something like a heartbeat message a

00:30:26,910 --> 00:30:31,680
status message a ping and the way to do

00:30:29,520 --> 00:30:34,170
this with the akka streams API is to to

00:30:31,680 --> 00:30:36,450
generate a source or a source shape that

00:30:34,170 --> 00:30:38,250
emits those periodic messages and then

00:30:36,450 --> 00:30:42,420
you merge them with the rest of the

00:30:38,250 --> 00:30:44,040
stream so here's an example of a source

00:30:42,420 --> 00:30:47,190
that's going to emit a status message

00:30:44,040 --> 00:30:49,320
once a minute and then the stream is

00:30:47,190 --> 00:30:51,060
going to generate these random samples

00:30:49,320 --> 00:30:53,340
again once a second and then we're just

00:30:51,060 --> 00:30:58,440
going to merge that with the status

00:30:53,340 --> 00:31:00,570
message so it's pretty easy to use okay

00:30:58,440 --> 00:31:04,170
finally the last pattern I want to look

00:31:00,570 --> 00:31:05,670
at is watching termination in case you

00:31:04,170 --> 00:31:07,950
don't understand the picture this is a

00:31:05,670 --> 00:31:10,170
bridge in San Francisco the bridge on

00:31:07,950 --> 00:31:11,790
the writes being destroyed over the

00:31:10,170 --> 00:31:13,770
course of a couple years and then bridge

00:31:11,790 --> 00:31:17,430
on the left is being built over the

00:31:13,770 --> 00:31:20,400
course of a couple years so what watch

00:31:17,430 --> 00:31:22,110
termination does is it doesn't impact

00:31:20,400 --> 00:31:24,810
the stream at all it just observes the

00:31:22,110 --> 00:31:27,810
stream and allows messages to flow

00:31:24,810 --> 00:31:30,450
through it and when the stream completes

00:31:27,810 --> 00:31:32,040
you get a future that either completes

00:31:30,450 --> 00:31:34,560
successfully or with a failure and you

00:31:32,040 --> 00:31:36,360
can take some kind of action so you

00:31:34,560 --> 00:31:39,120
basically just get this notification

00:31:36,360 --> 00:31:40,830
that the stream has completed but you

00:31:39,120 --> 00:31:43,050
don't impact it in any other way and

00:31:40,830 --> 00:31:45,030
here's an example of using it this is

00:31:43,050 --> 00:31:48,690
the greeter WebSocket is right out of

00:31:45,030 --> 00:31:50,640
the akka HTTP documentation and it

00:31:48,690 --> 00:31:53,330
basically responds to whatever message

00:31:50,640 --> 00:31:57,270
you send it with hello and your message

00:31:53,330 --> 00:32:00,900
and it can be used in an HTTP server as

00:31:57,270 --> 00:32:03,600
follows so if we want to say write a

00:32:00,900 --> 00:32:06,000
message when this greeter WebSocket

00:32:03,600 --> 00:32:10,020
completes we can watch for termination

00:32:06,000 --> 00:32:11,700
of that stream and when it completes

00:32:10,020 --> 00:32:13,620
we'll either know whether it completed

00:32:11,700 --> 00:32:17,010
successfully or whether it had an

00:32:13,620 --> 00:32:19,320
exception and we can log a message

00:32:17,010 --> 00:32:22,590
okay so these are all the patterns that

00:32:19,320 --> 00:32:24,960
I wanted to look at and in light of

00:32:22,590 --> 00:32:26,910
these patterns let's return to the proof

00:32:24,960 --> 00:32:29,490
of concept that I was constructing

00:32:26,910 --> 00:32:32,220
earlier and if you recall the proof of

00:32:29,490 --> 00:32:34,350
concept was a service that would collect

00:32:32,220 --> 00:32:36,570
telemetry from thousands of wind

00:32:34,350 --> 00:32:38,720
turbines and store them in this database

00:32:36,570 --> 00:32:43,470
and we took an actor/model approach

00:32:38,720 --> 00:32:44,970
originally with this when we would get a

00:32:43,470 --> 00:32:47,070
telemetry message from the wind turbine

00:32:44,970 --> 00:32:49,049
we would send it to this factor that was

00:32:47,070 --> 00:32:51,330
responsible for storing the telemetry in

00:32:49,049 --> 00:32:53,070
the database and the actor we ended up

00:32:51,330 --> 00:32:54,840
with looked like this and was performing

00:32:53,070 --> 00:32:59,850
grouping and throttling and periodic

00:32:54,840 --> 00:33:01,470
events and so on so in light of the akka

00:32:59,850 --> 00:33:03,570
streams API we can actually replace

00:33:01,470 --> 00:33:06,590
everything that that database actor was

00:33:03,570 --> 00:33:09,450
doing with these two lines of code

00:33:06,590 --> 00:33:11,340
grouped within that's gonna group

00:33:09,450 --> 00:33:13,110
messages into a batch of a thousand but

00:33:11,340 --> 00:33:16,380
it's also not gonna wait any longer than

00:33:13,110 --> 00:33:18,299
a second so we'll have low latency to

00:33:16,380 --> 00:33:20,520
the database and it's not going to allow

00:33:18,299 --> 00:33:22,350
any more than ten asynchronous requests

00:33:20,520 --> 00:33:24,600
to the database at any one time so that

00:33:22,350 --> 00:33:27,809
we don't overwhelm the database so

00:33:24,600 --> 00:33:30,960
replace all this code in the actor with

00:33:27,809 --> 00:33:32,730
these two lines and you also note that

00:33:30,960 --> 00:33:35,010
we got rid of all this mutable state

00:33:32,730 --> 00:33:37,650
that we had before that we were managing

00:33:35,010 --> 00:33:39,660
and also note that this has flow control

00:33:37,650 --> 00:33:42,450
which we didn't have before because the

00:33:39,660 --> 00:33:45,299
akka streams has back pressure and uses

00:33:42,450 --> 00:33:47,010
the reactive stream specification this

00:33:45,299 --> 00:33:50,640
is fully flow controlled and we have

00:33:47,010 --> 00:33:51,750
bounded resource constraints now it's

00:33:50,640 --> 00:33:55,080
beyond the scope of this presentation

00:33:51,750 --> 00:33:56,760
here but this also has much easier error

00:33:55,080 --> 00:34:00,090
handling semantics than we would have

00:33:56,760 --> 00:34:01,440
had in the actor previously and note

00:34:00,090 --> 00:34:02,970
that we didn't need to rewrite a whole

00:34:01,440 --> 00:34:06,299
lot of code basically all I did was

00:34:02,970 --> 00:34:07,650
delete code and we basically took the

00:34:06,299 --> 00:34:09,149
business logic that we had previously

00:34:07,650 --> 00:34:13,320
and we just put it in a streaming

00:34:09,149 --> 00:34:15,030
context in terms of testing I don't feel

00:34:13,320 --> 00:34:16,980
I need to test group within their map

00:34:15,030 --> 00:34:19,070
async I trust that the akka team has

00:34:16,980 --> 00:34:21,149
done that for me

00:34:19,070 --> 00:34:23,639
generally all I need to do is test my

00:34:21,149 --> 00:34:25,290
business logic here like like to like

00:34:23,639 --> 00:34:27,000
the parsing function

00:34:25,290 --> 00:34:29,490
which of course is easy to unit test

00:34:27,000 --> 00:34:31,860
that said the akka streams API also has

00:34:29,490 --> 00:34:33,960
a really good test kit where you can

00:34:31,860 --> 00:34:35,909
basically create a source that creates a

00:34:33,960 --> 00:34:38,639
known sequence of messages and then you

00:34:35,909 --> 00:34:40,230
can assert that the stream passes

00:34:38,639 --> 00:34:42,450
through what you would expect you can

00:34:40,230 --> 00:34:48,350
basically test all of your edge cases in

00:34:42,450 --> 00:34:51,149
your stream in a unit test ok summing up

00:34:48,350 --> 00:34:53,520
the akka streams API incorporates these

00:34:51,149 --> 00:34:55,050
these high level streaming constructs so

00:34:53,520 --> 00:34:57,750
it kind of takes reactive stream

00:34:55,050 --> 00:34:59,730
specification and puts it up in a higher

00:34:57,750 --> 00:35:01,590
level for application developers and

00:34:59,730 --> 00:35:03,150
note that these these things are really

00:35:01,590 --> 00:35:04,920
conceptually easy right like something

00:35:03,150 --> 00:35:07,230
like throttling it's conceptually easy

00:35:04,920 --> 00:35:08,730
but we've seen when you Devils kind of

00:35:07,230 --> 00:35:13,590
in the details when you get into the

00:35:08,730 --> 00:35:15,360
implementation akka streams API because

00:35:13,590 --> 00:35:16,740
the reactive stream specification also

00:35:15,360 --> 00:35:18,480
has flow control and that gives us

00:35:16,740 --> 00:35:20,220
bounded resource constraints which makes

00:35:18,480 --> 00:35:22,980
our applications run really reliably

00:35:20,220 --> 00:35:26,400
it's also strongly typed so we have

00:35:22,980 --> 00:35:27,660
compile time safety and it's it really

00:35:26,400 --> 00:35:30,090
delivers great performance and

00:35:27,660 --> 00:35:31,950
reliability from personal experience it

00:35:30,090 --> 00:35:34,230
worked on many many of these systems

00:35:31,950 --> 00:35:37,560
that run in production and they just run

00:35:34,230 --> 00:35:41,480
they just work very responsive to system

00:35:37,560 --> 00:35:45,570
dynamics and very resilient to failures

00:35:41,480 --> 00:35:48,180
so what about actors I think this is

00:35:45,570 --> 00:35:50,280
I've seen a few people kind of have this

00:35:48,180 --> 00:35:52,520
realization at this point is like well

00:35:50,280 --> 00:35:55,110
we thought actors were a great thing

00:35:52,520 --> 00:35:58,560
these streams look way better do we even

00:35:55,110 --> 00:36:02,010
need actors anymore and the answer is is

00:35:58,560 --> 00:36:04,380
yes so if you're if you're kind of

00:36:02,010 --> 00:36:05,910
having that same realization I'd

00:36:04,380 --> 00:36:07,980
encourage you to watch this talk that I

00:36:05,910 --> 00:36:09,750
gave at reactive summit last year which

00:36:07,980 --> 00:36:12,890
is really the the complement to this

00:36:09,750 --> 00:36:16,230
talk actors are still really valuable

00:36:12,890 --> 00:36:19,200
for managing the lifecycle of a stream

00:36:16,230 --> 00:36:21,150
or distributing streaming workloads

00:36:19,200 --> 00:36:24,450
within an akka cluster if your say your

00:36:21,150 --> 00:36:26,340
streaming workload you know is it's too

00:36:24,450 --> 00:36:28,320
big for one machine and you need to

00:36:26,340 --> 00:36:29,610
distribute it in a cluster or you want

00:36:28,320 --> 00:36:32,910
to have it in a cluster for high

00:36:29,610 --> 00:36:35,840
availability actors are really valuable

00:36:32,910 --> 00:36:38,760
for doing that I mean actors are still

00:36:35,840 --> 00:36:40,530
valuable for maintaining state

00:36:38,760 --> 00:36:42,690
and I also think they're really valuable

00:36:40,530 --> 00:36:45,450
from especially in IOT workloads they're

00:36:42,690 --> 00:36:47,910
really valuable from from modeling so

00:36:45,450 --> 00:36:51,570
you can model your IOT assets with kind

00:36:47,910 --> 00:36:53,910
of a digital twin that is the actor so

00:36:51,570 --> 00:36:57,560
if you're interested in more of that I

00:36:53,910 --> 00:36:57,560
would encourage you to watch this talk

00:36:58,010 --> 00:37:02,060
that's me on Twitter if you're into

00:37:00,630 --> 00:37:06,030
Twitter

00:37:02,060 --> 00:37:07,619
that's my blog if if you're interested

00:37:06,030 --> 00:37:10,020
in what I talked about here today in

00:37:07,619 --> 00:37:12,330
written form along with a few more

00:37:10,020 --> 00:37:15,750
examples encourage you to reference

00:37:12,330 --> 00:37:17,490
these two blog articles there's also

00:37:15,750 --> 00:37:20,850
some other things on my blog in terms of

00:37:17,490 --> 00:37:22,230
Scala alka working in teams these kind

00:37:20,850 --> 00:37:24,030
of things I try and write one article a

00:37:22,230 --> 00:37:27,420
month so if you're interested you can

00:37:24,030 --> 00:37:36,869
check that out I believe we have some

00:37:27,420 --> 00:37:40,730
time for for questions yes we do thank

00:37:36,869 --> 00:37:40,730
you very much I just start whatever way

00:37:41,510 --> 00:37:49,080
hi great talk thank you I was wondering

00:37:45,890 --> 00:37:53,510
what your recommendation is for using

00:37:49,080 --> 00:37:56,369
streams are you just using a stream in

00:37:53,510 --> 00:37:58,830
have an application which consists of

00:37:56,369 --> 00:38:00,690
just a single stream or if you have an

00:37:58,830 --> 00:38:04,350
application that contains multiple

00:38:00,690 --> 00:38:06,600
streams how to prevent that you know the

00:38:04,350 --> 00:38:10,530
the rest of the application still

00:38:06,600 --> 00:38:13,500
performs well the stream is also doing

00:38:10,530 --> 00:38:16,830
its stuff you know not if there are a

00:38:13,500 --> 00:38:18,960
lot of events coming in how to actually

00:38:16,830 --> 00:38:21,359
have the system also do some back

00:38:18,960 --> 00:38:26,280
pressure on the stream itself so it can

00:38:21,359 --> 00:38:27,270
also do a lot of stuff right i if

00:38:26,280 --> 00:38:28,040
there's a lot of answers to that

00:38:27,270 --> 00:38:30,780
question

00:38:28,040 --> 00:38:32,790
so part of it depends on whether you

00:38:30,780 --> 00:38:36,150
using streams for say unbounded

00:38:32,790 --> 00:38:39,720
workflows that are constantly busy or if

00:38:36,150 --> 00:38:42,180
you're using streams for like short

00:38:39,720 --> 00:38:44,660
there short live streams like there's no

00:38:42,180 --> 00:38:47,420
reason why you can't take a stream

00:38:44,660 --> 00:38:52,260
streaming approach to database requests

00:38:47,420 --> 00:38:53,850
in fact I think people don't

00:38:52,260 --> 00:38:56,640
imagine enough where they can use

00:38:53,850 --> 00:38:58,260
streams that often you can like reinvent

00:38:56,640 --> 00:39:00,690
most of your workloads in a streaming

00:38:58,260 --> 00:39:02,100
fashion and have that back pressure and

00:39:00,690 --> 00:39:04,740
have these high level streaming

00:39:02,100 --> 00:39:06,690
constructs at your fingertips

00:39:04,740 --> 00:39:08,040
so if you're doing something like making

00:39:06,690 --> 00:39:09,630
a request you have a bunch of short

00:39:08,040 --> 00:39:12,450
requests against a database and you can

00:39:09,630 --> 00:39:13,920
compose those in a streaming way it's

00:39:12,450 --> 00:39:16,020
not that much different I'd say then

00:39:13,920 --> 00:39:19,050
then writing normal application code

00:39:16,020 --> 00:39:21,780
when you have these streaming workloads

00:39:19,050 --> 00:39:24,480
that are running all the time so say

00:39:21,780 --> 00:39:27,240
your congestion pipeline and you're

00:39:24,480 --> 00:39:28,590
consuming a Kafka partition something

00:39:27,240 --> 00:39:31,110
like that you know you're gonna get

00:39:28,590 --> 00:39:33,120
10,000 messages per second and you're

00:39:31,110 --> 00:39:35,250
gonna consume this much CPU all the time

00:39:33,120 --> 00:39:37,620
I think that that's the case where you

00:39:35,250 --> 00:39:39,780
need to partition your streams relative

00:39:37,620 --> 00:39:42,150
to your underlying compute resources and

00:39:39,780 --> 00:39:47,580
be sensitive to you know not packing too

00:39:42,150 --> 00:39:49,470
many streams into one application and

00:39:47,580 --> 00:39:52,080
there's different ways to do that

00:39:49,470 --> 00:39:53,640
actually if you check out my my blog

00:39:52,080 --> 00:39:55,650
this month and I'm gonna write a

00:39:53,640 --> 00:39:57,930
follow-up article next month I talked a

00:39:55,650 --> 00:40:00,720
little bit about how to partition acha

00:39:57,930 --> 00:40:04,200
streams in cases where you are starting

00:40:00,720 --> 00:40:17,670
to use too much compute on on one

00:40:04,200 --> 00:40:19,890
machine questions hi amazing

00:40:17,670 --> 00:40:23,370
presentation by the way I just wanted to

00:40:19,890 --> 00:40:27,420
ask in the demo you showed saving data

00:40:23,370 --> 00:40:29,390
in the map and then map a saying is

00:40:27,420 --> 00:40:33,120
there a reason why this couldn't be

00:40:29,390 --> 00:40:35,760
expressed in the sync fashion to saving

00:40:33,120 --> 00:40:38,490
that because it seems naively as the

00:40:35,760 --> 00:40:42,810
right thing to do yeah could you come on

00:40:38,490 --> 00:40:44,400
but oh yeah please absolutely it in some

00:40:42,810 --> 00:40:47,370
cases it is is it is the right thing to

00:40:44,400 --> 00:40:51,900
do and that's a pattern that that I have

00:40:47,370 --> 00:40:54,780
used I I think more often in not though

00:40:51,900 --> 00:40:57,360
you want to inspect the result of what

00:40:54,780 --> 00:40:58,950
happened in the database and maybe pass

00:40:57,360 --> 00:41:01,100
that to a logging stage or something

00:40:58,950 --> 00:41:04,410
like that and very often in these

00:41:01,100 --> 00:41:05,790
telemetry systems you want to send back

00:41:04,410 --> 00:41:08,190
an acknowledgement to

00:41:05,790 --> 00:41:10,350
device to say hey I got your message you

00:41:08,190 --> 00:41:13,350
don't need to retry or you want to

00:41:10,350 --> 00:41:13,890
commit the offsets to Kafka or something

00:41:13,350 --> 00:41:16,980
like that

00:41:13,890 --> 00:41:19,170
so the you often you kind of think of

00:41:16,980 --> 00:41:20,880
the database as being your sink and the

00:41:19,170 --> 00:41:23,580
end of the stream but it's not actually

00:41:20,880 --> 00:41:25,590
it's not usually the case I'd say you

00:41:23,580 --> 00:41:27,510
usually you want to still have some more

00:41:25,590 --> 00:41:29,190
downstream processing commit offsets

00:41:27,510 --> 00:41:30,990
have acknowledgments and so on and

00:41:29,190 --> 00:41:33,690
that's that's actually a more common

00:41:30,990 --> 00:41:36,270
pattern to say write to a database in an

00:41:33,690 --> 00:41:53,940
in a map async stage and then have more

00:41:36,270 --> 00:41:57,140
elements downstream I am you mention

00:41:53,940 --> 00:42:00,120
about watch termination of a stream but

00:41:57,140 --> 00:42:02,430
how can I for example if I want to shut

00:42:00,120 --> 00:42:04,830
down the stream based on an external

00:42:02,430 --> 00:42:07,110
factor I received a message from

00:42:04,830 --> 00:42:10,230
somewhere else but I still have to

00:42:07,110 --> 00:42:12,060
guarantee that all the message messages

00:42:10,230 --> 00:42:14,790
are incoming the stream are going to be

00:42:12,060 --> 00:42:17,460
processed but it's still being able to

00:42:14,790 --> 00:42:19,710
shut down safely the string based on

00:42:17,460 --> 00:42:22,500
something outside the source yeah

00:42:19,710 --> 00:42:24,750
there's there's ways to do that watch

00:42:22,500 --> 00:42:27,600
that the other presentation I referenced

00:42:24,750 --> 00:42:29,370
from reactive summit I talked about some

00:42:27,600 --> 00:42:38,610
of those details and how to interact in

00:42:29,370 --> 00:42:41,270
that way yeah this is not passing thing

00:42:38,610 --> 00:42:47,160
it won't end all back pressure right now

00:42:41,270 --> 00:42:49,470
face the the stream will yes okay but

00:42:47,160 --> 00:42:51,900
not not the database because it's not

00:42:49,470 --> 00:42:54,120
it's not part of the the sources it

00:42:51,900 --> 00:42:56,430
won't request like when it's ready to

00:42:54,120 --> 00:42:59,040
for more data because you like bulk

00:42:56,430 --> 00:43:02,250
insert into it so it looks like you kind

00:42:59,040 --> 00:43:06,300
of try to manage what the database can

00:43:02,250 --> 00:43:10,500
accept yeah is it possible to improve

00:43:06,300 --> 00:43:13,050
that and like put a database driver part

00:43:10,500 --> 00:43:15,300
of the directive stream so you don't

00:43:13,050 --> 00:43:17,760
have to end all like yeah like booking

00:43:15,300 --> 00:43:19,350
or stuff like that it is your database

00:43:17,760 --> 00:43:21,240
can actually have

00:43:19,350 --> 00:43:23,190
the streams interface then absolutely

00:43:21,240 --> 00:43:25,500
yeah then like if you look at say how a

00:43:23,190 --> 00:43:27,690
WebSocket interacts at the TCP layer

00:43:25,500 --> 00:43:30,360
right that is like back pressure the

00:43:27,690 --> 00:43:32,490
whole way through if your database has

00:43:30,360 --> 00:43:34,860
as a driver that can understand that

00:43:32,490 --> 00:43:39,500
then yeah you you don't need to do

00:43:34,860 --> 00:43:39,500
something as as crude as map a sink but

00:43:39,560 --> 00:43:44,520
what you'll find is often you're you

00:43:42,450 --> 00:43:46,170
know you're making say HTTP calls

00:43:44,520 --> 00:43:48,240
against another service these kind of

00:43:46,170 --> 00:43:51,690
things and often your might your map

00:43:48,240 --> 00:43:55,280
using map async in those cases but but

00:43:51,690 --> 00:43:57,930
yeah absolutely it it doesn't preclude a

00:43:55,280 --> 00:44:00,500
service with a with a proper reactive

00:43:57,930 --> 00:44:00,500

YouTube URL: https://www.youtube.com/watch?v=ilhImUjF53A


