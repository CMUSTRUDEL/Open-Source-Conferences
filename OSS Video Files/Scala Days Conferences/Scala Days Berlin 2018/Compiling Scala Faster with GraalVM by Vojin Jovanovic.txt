Title: Compiling Scala Faster with GraalVM by Vojin Jovanovic
Publication date: 2018-09-20
Playlist: Scala Days Berlin 2018
Description: 
	This video was recorded at Scala Days Berlin 2018
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

More information and the abstract can be found here:
https://eu.scaladays.org/lect-6928-compiling-scala-faster-with-graalvm.html
Captions: 
	00:00:04,590 --> 00:00:13,889
so why I'm here is to compile Scala

00:00:09,120 --> 00:00:17,039
faster so I went on Google and typed in

00:00:13,889 --> 00:00:20,669
Scala compiler is so and this is what

00:00:17,039 --> 00:00:23,730
comes out and the second thing I want to

00:00:20,669 --> 00:00:26,419
talk about is the startup time of Scala

00:00:23,730 --> 00:00:30,810
compiler and other scholar programs

00:00:26,419 --> 00:00:32,040
so before I start with with the

00:00:30,810 --> 00:00:33,660
presentation I would like to introduce

00:00:32,040 --> 00:00:36,239
what we do it or collapse

00:00:33,660 --> 00:00:38,970
it's a project called Grail VM it's

00:00:36,239 --> 00:00:41,190
essentially a virtual machine built on

00:00:38,970 --> 00:00:43,950
top of the JVM so it uses all of the

00:00:41,190 --> 00:00:46,800
runtime components from the JVM but then

00:00:43,950 --> 00:00:49,710
it adds a JIT compiler and a truffle

00:00:46,800 --> 00:00:52,800
framework on top which allow us to run

00:00:49,710 --> 00:00:55,800
multiple languages in graz VM so gravity

00:00:52,800 --> 00:00:58,470
M supports at the moment in Java Scala

00:00:55,800 --> 00:01:02,460
and all other taylean languages and as

00:00:58,470 --> 00:01:06,330
well as Ruby Python jsr and all of the

00:01:02,460 --> 00:01:08,640
language compiled to evolve ian graal vm

00:01:06,330 --> 00:01:10,979
comes with additional tool that we'll

00:01:08,640 --> 00:01:13,409
talk about here it is called native

00:01:10,979 --> 00:01:17,280
image and this tool allows us to build

00:01:13,409 --> 00:01:19,799
native images from any java program

00:01:17,280 --> 00:01:23,600
including the growl VM itself so it

00:01:19,799 --> 00:01:26,340
allows us to run in native code so

00:01:23,600 --> 00:01:29,249
essentially the slogan of the growl VM

00:01:26,340 --> 00:01:31,020
is like run programs faster anywhere so

00:01:29,249 --> 00:01:32,609
what we'll do we run it we can run in

00:01:31,020 --> 00:01:36,329
the open JDK which is what we'll be

00:01:32,609 --> 00:01:39,810
presenting mostly today then we have a

00:01:36,329 --> 00:01:41,460
node.js mode we embed growl a VM into

00:01:39,810 --> 00:01:45,240
the Oracle database in the project

00:01:41,460 --> 00:01:47,789
called Emily as well as MySQL so today

00:01:45,240 --> 00:01:50,459
you can run JavaScript and Python in

00:01:47,789 --> 00:01:52,979
MySQL and then finally you can run it

00:01:50,459 --> 00:01:55,380
standalone so for more information just

00:01:52,979 --> 00:01:57,329
go to Crowell V on the torque and you'll

00:01:55,380 --> 00:02:02,090
figure out more this is the I can give

00:01:57,329 --> 00:02:02,090
three talks just about growl VM so

00:02:02,389 --> 00:02:06,990
because here we are talking about

00:02:04,409 --> 00:02:09,600
compiling Scala faster I'll just show

00:02:06,990 --> 00:02:13,830
you a few of the things which we call

00:02:09,600 --> 00:02:17,160
the compiler in rallyin does to make

00:02:13,830 --> 00:02:19,780
programs faster so

00:02:17,160 --> 00:02:23,530
sorry before I do that so the key

00:02:19,780 --> 00:02:28,060
features of girl-wise grow better than

00:02:23,530 --> 00:02:29,800
other compilers on the JVM so one of the

00:02:28,060 --> 00:02:33,150
main reasons I think is written in Java

00:02:29,800 --> 00:02:35,410
and why is that important is because

00:02:33,150 --> 00:02:37,060
when you write your compiler in Java

00:02:35,410 --> 00:02:40,810
you can easily refactor it use all the

00:02:37,060 --> 00:02:45,370
tooling it's much more modular and in

00:02:40,810 --> 00:02:48,010
the long run it pays off manifold then

00:02:45,370 --> 00:02:51,220
it has modular architecture so it's very

00:02:48,010 --> 00:02:54,310
easy to configure add your own faces and

00:02:51,220 --> 00:02:59,160
change them and it also separates the VM

00:02:54,310 --> 00:03:02,590
from the compiler by essentially

00:02:59,160 --> 00:03:04,810
snippets and provider interfaces so this

00:03:02,590 --> 00:03:07,780
allows us essentially to kind of because

00:03:04,810 --> 00:03:09,880
growl growl came into sky into the

00:03:07,780 --> 00:03:11,380
hotspot later we had to kind of make an

00:03:09,880 --> 00:03:13,510
interface for every single thing we

00:03:11,380 --> 00:03:17,440
require from the VM so the code is

00:03:13,510 --> 00:03:19,840
clearly separated and then it's designed

00:03:17,440 --> 00:03:22,600
for speculative optimizations that's

00:03:19,840 --> 00:03:24,940
what every great compiler does and

00:03:22,600 --> 00:03:28,990
garbage collection and it has aggressive

00:03:24,940 --> 00:03:32,350
optimizations and out of these I'll talk

00:03:28,990 --> 00:03:35,980
just about two of them which which I

00:03:32,350 --> 00:03:39,130
like one is called partial escape

00:03:35,980 --> 00:03:41,140
analysis so essentially when you have

00:03:39,130 --> 00:03:44,590
objects in your methods allocations for

00:03:41,140 --> 00:03:47,290
example this new car here these are

00:03:44,590 --> 00:03:50,920
locations if you do escape analysis

00:03:47,290 --> 00:03:52,540
you're trying to figure out if you can

00:03:50,920 --> 00:03:55,959
remove this allocation and replete

00:03:52,540 --> 00:03:57,610
replace replace all of the usages just

00:03:55,959 --> 00:04:00,760
by the values that you have which is in

00:03:57,610 --> 00:04:05,170
this case horsepower and name and if you

00:04:00,760 --> 00:04:07,660
look at this method in here of the car

00:04:05,170 --> 00:04:10,660
escapes in two places here in two adds

00:04:07,660 --> 00:04:13,300
to cash and in the return car however

00:04:10,660 --> 00:04:16,720
this could be a very unlikely path all

00:04:13,300 --> 00:04:19,359
right so what gravettian tries to do is

00:04:16,720 --> 00:04:20,950
to have absolutely no allocations on the

00:04:19,359 --> 00:04:23,100
paths which are important on the hot

00:04:20,950 --> 00:04:27,280
path so in this case what will happen

00:04:23,100 --> 00:04:29,139
whoops oh yeah so the car essentially we

00:04:27,280 --> 00:04:31,120
will move the location of the car

00:04:29,139 --> 00:04:34,150
to the else branch which is not a likely

00:04:31,120 --> 00:04:37,900
path and then on the on the hot path

00:04:34,150 --> 00:04:41,499
will just help accesses to the fields so

00:04:37,900 --> 00:04:43,210
this is something that into this kind of

00:04:41,499 --> 00:04:44,590
unique to grow and which especially in

00:04:43,210 --> 00:04:46,659
context of Scala which has a lot of

00:04:44,590 --> 00:04:49,029
allocations helps quite a bit and

00:04:46,659 --> 00:04:53,830
removes a bunch of them from the hot

00:04:49,029 --> 00:04:56,889
path another one which I really like is

00:04:53,830 --> 00:04:58,689
called simulate simulation based paths

00:04:56,889 --> 00:05:01,539
of duplication this is essentially a

00:04:58,689 --> 00:05:04,659
general generalization of a of an

00:05:01,539 --> 00:05:07,210
optimization which is called tail tail

00:05:04,659 --> 00:05:08,919
call duplication because it works across

00:05:07,210 --> 00:05:11,279
every merge point so if you look at this

00:05:08,919 --> 00:05:15,580
little code snippet here where we have

00:05:11,279 --> 00:05:18,550
we say if a greater than B P equals a

00:05:15,580 --> 00:05:21,339
and then else P equals two and when we

00:05:18,550 --> 00:05:23,319
divide the length of an array by P so

00:05:21,339 --> 00:05:24,759
division and when you compile it down to

00:05:23,319 --> 00:05:27,669
assembly is a very expensive operation

00:05:24,759 --> 00:05:29,710
and if you have this being a slow path

00:05:27,669 --> 00:05:33,580
and this being a fast path which is

00:05:29,710 --> 00:05:35,740
executed most of the time and we notice

00:05:33,580 --> 00:05:42,120
that in the fast path P gets a type

00:05:35,740 --> 00:05:42,120
which is a constant type always - we can

00:05:42,779 --> 00:05:45,779
sorry

00:05:48,639 --> 00:05:53,389
we can essentially do a strength

00:05:51,619 --> 00:05:55,279
reduction because we know that P is

00:05:53,389 --> 00:05:57,619
always 2 we know that X dot length is

00:05:55,279 --> 00:06:02,089
greater than 2 so we can rewrite X dot

00:05:57,619 --> 00:06:06,649
length divided 2 with excellent shift by

00:06:02,089 --> 00:06:09,769
1 and then however after the merge here

00:06:06,649 --> 00:06:11,509
after the if this X dot length this

00:06:09,769 --> 00:06:13,879
cannot be applied because it applies to

00:06:11,509 --> 00:06:16,449
the both of values so what we do is

00:06:13,879 --> 00:06:18,979
essentially do a cost-benefit analysis

00:06:16,449 --> 00:06:21,739
whether this is worth it and this is how

00:06:18,979 --> 00:06:23,959
it looks so we say benefit is a latency

00:06:21,739 --> 00:06:26,839
of a division - the latency of the shift

00:06:23,959 --> 00:06:28,939
and then times probability of the branch

00:06:26,839 --> 00:06:30,499
and then we can compute the value which

00:06:28,939 --> 00:06:32,659
is in this case quite high because the

00:06:30,499 --> 00:06:34,999
latency of DV is very high and then we

00:06:32,659 --> 00:06:39,109
say because we duplicated code you say

00:06:34,999 --> 00:06:42,349
how many instructions that this add on

00:06:39,109 --> 00:06:46,459
on overall to the compilation and enroll

00:06:42,349 --> 00:06:48,229
all of the nodes are annotated bit and

00:06:46,459 --> 00:06:50,029
estimate how many instructions will they

00:06:48,229 --> 00:06:52,069
cause when the once the compilation is

00:06:50,029 --> 00:06:54,019
done so we use that cost model to

00:06:52,069 --> 00:06:56,509
compute the number of instructions and

00:06:54,019 --> 00:06:59,239
then we get this code in the end

00:06:56,509 --> 00:07:03,409
actually there is a return but in scala

00:06:59,239 --> 00:07:05,809
debts can hurt right and then we get

00:07:03,409 --> 00:07:09,829
coded this one order of magnitude faster

00:07:05,809 --> 00:07:12,309
on the CPUs there's a paper about this

00:07:09,829 --> 00:07:18,099
work which you can look for more details

00:07:12,309 --> 00:07:21,139
a key point is that essentially you

00:07:18,099 --> 00:07:24,129
remove the code inside but then we run

00:07:21,139 --> 00:07:26,809
additional phases that are kind of

00:07:24,129 --> 00:07:28,699
reducing the code size and doing the

00:07:26,809 --> 00:07:30,349
optimizations and then doing the cost so

00:07:28,699 --> 00:07:32,779
we're doing the part of the compilation

00:07:30,349 --> 00:07:36,799
run in one of the phases to figure out

00:07:32,779 --> 00:07:39,079
how much to be safe and there is a few

00:07:36,799 --> 00:07:40,620
more of these which I don't have time to

00:07:39,079 --> 00:07:48,240
present

00:07:40,620 --> 00:07:59,790
so be nice that my machine responds okay

00:07:48,240 --> 00:08:02,130
so let's take a look at how girl does on

00:07:59,790 --> 00:08:03,840
a simple program so here I have a

00:08:02,130 --> 00:08:07,110
program which is doing a streaming

00:08:03,840 --> 00:08:08,610
operations in Java so this is you don't

00:08:07,110 --> 00:08:11,160
have to look at the beginning but

00:08:08,610 --> 00:08:13,070
essentially we have a bunch of like what

00:08:11,160 --> 00:08:15,900
is equivalent of a case class in Scala

00:08:13,070 --> 00:08:17,790
we have a person which has a sex gender

00:08:15,900 --> 00:08:20,640
and so on so it's a bunch of abstraction

00:08:17,790 --> 00:08:23,100
that you use and then we do we have an

00:08:20,640 --> 00:08:25,140
array of persons of a certain size and

00:08:23,100 --> 00:08:27,840
we will do streaming operations so we do

00:08:25,140 --> 00:08:29,340
a filter B's select only male persons

00:08:27,840 --> 00:08:32,850
than with another filter with the height

00:08:29,340 --> 00:08:34,800
we map the int age to int and then

00:08:32,850 --> 00:08:37,950
filter age greater than ten we do and

00:08:34,800 --> 00:08:43,530
everything to get this double so and if

00:08:37,950 --> 00:08:47,820
you take this code and run it on the

00:08:43,530 --> 00:08:49,350
regular JVM we'll get something like

00:08:47,820 --> 00:08:52,290
this so in the beginning we have some

00:08:49,350 --> 00:08:55,260
warm-up and then we get around hundred

00:08:52,290 --> 00:08:57,860
and 80 milliseconds now if you run it

00:08:55,260 --> 00:08:57,860
again

00:09:05,920 --> 00:09:10,160
we get four times or five times faster

00:09:09,740 --> 00:09:15,950
code

00:09:10,160 --> 00:09:18,710
so what Garvey and does here so what

00:09:15,950 --> 00:09:23,630
gravettian does here is analyzes in

00:09:18,710 --> 00:09:26,570
depth there in linings are relevant and

00:09:23,630 --> 00:09:28,340
essentially does a few more tricks to

00:09:26,570 --> 00:09:30,860
convert all of this into a single while

00:09:28,340 --> 00:09:32,390
loop all right so it kind of tastes this

00:09:30,860 --> 00:09:42,650
is an equivalent performance of a while

00:09:32,390 --> 00:09:49,730
loop so okay nice it's faster so I think

00:09:42,650 --> 00:09:51,800
it's the recording gap but so then let's

00:09:49,730 --> 00:09:53,150
take it on the I mean the main problem

00:09:51,800 --> 00:09:55,370
in the one of the main problems in the

00:09:53,150 --> 00:09:57,890
community is the performance of the

00:09:55,370 --> 00:10:00,280
Scala compiler so what I did here is

00:09:57,890 --> 00:10:03,920
they took the Scala to 12-6

00:10:00,280 --> 00:10:06,470
and then I took the compiler suite

00:10:03,920 --> 00:10:08,150
benchmark from the skeleton which is

00:10:06,470 --> 00:10:11,930
awesome and it works really well and

00:10:08,150 --> 00:10:16,840
then I took growl and run it so growl

00:10:11,930 --> 00:10:19,820
has two to two modes there is gravy on

00:10:16,840 --> 00:10:22,190
Community Edition which is based on the

00:10:19,820 --> 00:10:23,990
core compiler which is missing a few of

00:10:22,190 --> 00:10:25,730
the phases and then is then there is

00:10:23,990 --> 00:10:26,870
Enterprise Edition which has all of the

00:10:25,730 --> 00:10:30,260
phases including the ones I showed

00:10:26,870 --> 00:10:32,180
included and before I go on with the

00:10:30,260 --> 00:10:34,370
graphs so in this presentation all of

00:10:32,180 --> 00:10:36,230
the graphs or this are done in the same

00:10:34,370 --> 00:10:39,500
way so on the left side we have the

00:10:36,230 --> 00:10:41,270
speed-up and then on the bottom we have

00:10:39,500 --> 00:10:44,240
the project sphere on on the on the

00:10:41,270 --> 00:10:46,220
x-axis all of the runs were done until

00:10:44,240 --> 00:10:48,260
complete warm-up there is no turbo and

00:10:46,220 --> 00:10:51,620
was done enough in a proper benchmarking

00:10:48,260 --> 00:10:55,340
setup with no no noise so and then we

00:10:51,620 --> 00:10:56,780
can see that these are the core pieces

00:10:55,340 --> 00:10:59,060
of code that we were using so for

00:10:56,780 --> 00:11:05,090
compilation and the Scala seaweed girl

00:10:59,060 --> 00:11:09,640
VM runs essentially 1.47 times faster

00:11:05,090 --> 00:11:11,829
here 1.42 and 1.42 right so so

00:11:09,640 --> 00:11:14,079
and this is this is quite a bit for

00:11:11,829 --> 00:11:18,339
daily development if you save one point

00:11:14,079 --> 00:11:20,500
five on your computation then I this is

00:11:18,339 --> 00:11:22,930
an ideal world all of all of the codes

00:11:20,500 --> 00:11:26,050
and all of the output was in essentially

00:11:22,930 --> 00:11:29,800
in the in memory and there is no SBT

00:11:26,050 --> 00:11:31,570
then I added SBT so I checked out a few

00:11:29,800 --> 00:11:33,279
projects which are kind of known in the

00:11:31,570 --> 00:11:36,399
community so I took shapeless I took

00:11:33,279 --> 00:11:38,230
Scala C and I took akka and then

00:11:36,399 --> 00:11:40,269
essentially this is what we get in in

00:11:38,230 --> 00:11:42,820
real life so if you take a project what

00:11:40,269 --> 00:11:44,440
they did is I took SBT and they enter

00:11:42,820 --> 00:11:46,060
the project and then I say clink on file

00:11:44,440 --> 00:11:48,130
clink of parkland compile until it

00:11:46,060 --> 00:11:51,670
stabilizes and then I compared the

00:11:48,130 --> 00:11:52,930
values 4 the last compilation so and for

00:11:51,670 --> 00:11:55,779
shapeless which is kind of shortly

00:11:52,930 --> 00:11:58,690
compiling we get one point 26 first

00:11:55,779 --> 00:12:00,250
color see 135 and frack of 135 and this

00:11:58,690 --> 00:12:03,790
is with all the noise like the old

00:12:00,250 --> 00:12:06,480
updating and scullery form stuff and so

00:12:03,790 --> 00:12:11,890
on so this is what you essentially get

00:12:06,480 --> 00:12:13,930
if you take your albion so so

00:12:11,890 --> 00:12:15,550
essentially how how do we do that right

00:12:13,930 --> 00:12:17,649
it's very easy so it comes in two

00:12:15,550 --> 00:12:19,750
versions the Community Edition can be

00:12:17,649 --> 00:12:21,160
used freely by anyone which gives around

00:12:19,750 --> 00:12:24,519
ten percent which we've seen in the

00:12:21,160 --> 00:12:27,040
previous slide and essentially just

00:12:24,519 --> 00:12:28,839
download it put it in your you know run

00:12:27,040 --> 00:12:30,399
it on your laptop in your CI wherever

00:12:28,839 --> 00:12:32,829
you like it's fine

00:12:30,399 --> 00:12:35,769
and then for growl VM Enterprise Edition

00:12:32,829 --> 00:12:38,470
it's essentially free evaluation license

00:12:35,769 --> 00:12:40,360
for non production News therefore all

00:12:38,470 --> 00:12:42,550
the individuals I would say compilation

00:12:40,360 --> 00:12:44,740
is a non production news so essentially

00:12:42,550 --> 00:12:47,440
just download it from here and get your

00:12:44,740 --> 00:12:50,050
compiler faster okay so and how do you

00:12:47,440 --> 00:12:53,980
do this well it's very easy you say just

00:12:50,050 --> 00:12:57,100
SBT Java home and you give a path to

00:12:53,980 --> 00:12:59,970
crawl VM okay or you can even put it

00:12:57,100 --> 00:12:59,970
into SBT ops

00:13:00,489 --> 00:13:06,589
yeah so let's just do that

00:13:03,199 --> 00:13:08,779
just real quick to break the ice it's a

00:13:06,589 --> 00:13:21,199
really stupid thing but so here I have

00:13:08,779 --> 00:13:23,419
shapeless so we can do SBT and there you

00:13:21,199 --> 00:13:30,229
go so this is basically on one download

00:13:23,419 --> 00:13:33,259
and and essentially one one argument is

00:13:30,229 --> 00:13:37,729
is dividing you like it's getting you

00:13:33,259 --> 00:13:39,559
30% of compilation time so so and when

00:13:37,729 --> 00:13:41,479
you compile in SBT you will notice that

00:13:39,559 --> 00:13:43,879
for small projects the first compilation

00:13:41,479 --> 00:13:46,489
will be maybe 20% slower than the

00:13:43,879 --> 00:13:48,409
hotspot but then already the second one

00:13:46,489 --> 00:13:51,619
will be faster and all the consecutive

00:13:48,409 --> 00:13:54,259
ones will be faster if you if you take a

00:13:51,619 --> 00:13:56,029
big project like akka even the first one

00:13:54,259 --> 00:13:57,859
will be faster because Akkad compiles

00:13:56,029 --> 00:13:59,989
for long enough time for the VN to warm

00:13:57,859 --> 00:14:05,329
up so it kind of evens out even the

00:13:59,989 --> 00:14:07,209
first one becomes faster yeah so just

00:14:05,329 --> 00:14:10,219
you know take it for a spin

00:14:07,209 --> 00:14:11,869
it hasn't failed a single time in all

00:14:10,219 --> 00:14:16,129
the benchmarks I've done so I will

00:14:11,869 --> 00:14:19,399
consider it stable and also essentially

00:14:16,129 --> 00:14:21,529
if you if you have any problems of

00:14:19,399 --> 00:14:27,669
course report but also it's compilation

00:14:21,529 --> 00:14:27,669
so it's not really important good

00:14:33,380 --> 00:14:39,000
you will also notice that in the first

00:14:36,360 --> 00:14:42,930
compilation your your machine will use

00:14:39,000 --> 00:14:54,570
all up all the calls but then later it

00:14:42,930 --> 00:15:01,590
basically stops so yes I don't this one

00:14:54,570 --> 00:15:04,380
is still from the lab there we go and so

00:15:01,590 --> 00:15:08,160
on every next time it's much faster so

00:15:04,380 --> 00:15:11,160
let's not wait for my laptop good

00:15:08,160 --> 00:15:11,580
so there you go just download it take it

00:15:11,160 --> 00:15:15,600
out

00:15:11,580 --> 00:15:17,760
compile faster and write better code so

00:15:15,600 --> 00:15:21,240
in the second part of the talk I want to

00:15:17,760 --> 00:15:23,220
talk about the problem of startup in

00:15:21,240 --> 00:15:27,960
managed runtimes especially in the JVM

00:15:23,220 --> 00:15:29,940
so essentially if you look at the JVM

00:15:27,960 --> 00:15:32,340
and when we look at the hello world for

00:15:29,940 --> 00:15:34,680
example in Scala the time to just print

00:15:32,340 --> 00:15:36,210
hello world is like 100 milliseconds

00:15:34,680 --> 00:15:38,730
which is in the in computer science

00:15:36,210 --> 00:15:41,240
eternity and then if you look at the

00:15:38,730 --> 00:15:44,400
instructions it didn't like billions and

00:15:41,240 --> 00:15:47,610
it takes a lot of memory for JavaScript

00:15:44,400 --> 00:15:50,250
this is running gracias on the JVM it's

00:15:47,610 --> 00:15:53,820
1.2 seconds to print hello world which

00:15:50,250 --> 00:15:55,920
is an eternity of an eternity so and

00:15:53,820 --> 00:15:57,780
then when we look at the hello world in

00:15:55,920 --> 00:15:59,850
C it takes 5 milliseconds and it's

00:15:57,780 --> 00:16:02,370
mostly because of the oil stuff that's

00:15:59,850 --> 00:16:04,770
running and is there's two hundred two

00:16:02,370 --> 00:16:07,020
thousand instructions so and and why is

00:16:04,770 --> 00:16:09,240
this because the JVM essentially does

00:16:07,020 --> 00:16:11,190
many things when it starts up especially

00:16:09,240 --> 00:16:12,930
for the short programs so first it needs

00:16:11,190 --> 00:16:15,780
to load all the classes verified the

00:16:12,930 --> 00:16:17,490
bytecode then you need to either use

00:16:15,780 --> 00:16:19,320
bytecode interpretation in the case of

00:16:17,490 --> 00:16:22,830
the JVM or baseline compilation which

00:16:19,320 --> 00:16:25,050
adds an additional layer an additional

00:16:22,830 --> 00:16:26,580
overhead and then finally you do

00:16:25,050 --> 00:16:27,810
just-in-time compilation and because

00:16:26,580 --> 00:16:29,550
you're running a short program you just

00:16:27,810 --> 00:16:32,250
compile your stuff when you finish the

00:16:29,550 --> 00:16:37,410
program so you basically just wasted all

00:16:32,250 --> 00:16:39,540
your time on that so and in Guardian we

00:16:37,410 --> 00:16:41,780
do the following we wanted to get rid of

00:16:39,540 --> 00:16:41,780
this

00:16:46,279 --> 00:16:49,860
yeah

00:16:47,550 --> 00:16:53,220
so we build this tool called native

00:16:49,860 --> 00:16:56,040
image and what native image does it

00:16:53,220 --> 00:16:57,480
takes any polyglots JVM programs that

00:16:56,040 --> 00:17:01,140
can be written in Kotlin it can be

00:16:57,480 --> 00:17:04,079
written in scala and in java it takes

00:17:01,140 --> 00:17:05,939
all of those that code in case if you're

00:17:04,079 --> 00:17:08,520
building the if you're building the

00:17:05,939 --> 00:17:09,600
image for the for truffle and all the

00:17:08,520 --> 00:17:11,850
languages that you're running like

00:17:09,600 --> 00:17:13,829
javascript takes that as well it takes

00:17:11,850 --> 00:17:15,900
the vm which we've written which is

00:17:13,829 --> 00:17:19,140
called substrate vm which is also in

00:17:15,900 --> 00:17:21,089
java and then it does points to analysis

00:17:19,140 --> 00:17:22,920
essentially it finds all of the fields

00:17:21,089 --> 00:17:26,280
all of the methods in all of the classes

00:17:22,920 --> 00:17:27,569
that are eatable so it cuts off most of

00:17:26,280 --> 00:17:30,120
the classes that you don't really need

00:17:27,569 --> 00:17:31,890
on your class path and essentially uses

00:17:30,120 --> 00:17:35,520
growl too ahead of time control it into

00:17:31,890 --> 00:17:37,410
the machine code so it can be compiled

00:17:35,520 --> 00:17:39,330
leader into a shared library if you have

00:17:37,410 --> 00:17:42,540
will see this towards the end of the

00:17:39,330 --> 00:17:47,280
talk or into an executable and for now

00:17:42,540 --> 00:17:48,660
we support run pius x and linux but in

00:17:47,280 --> 00:17:52,380
the end of the year we'll support

00:17:48,660 --> 00:17:54,630
Windows as well and iOS so so so

00:17:52,380 --> 00:17:55,980
essentially until probably you know very

00:17:54,630 --> 00:18:00,330
soon it will run on all the platforms

00:17:55,980 --> 00:18:02,370
and then if you take a native image and

00:18:00,330 --> 00:18:04,410
then we take the same programs what we

00:18:02,370 --> 00:18:06,960
get is really essentially hello world

00:18:04,410 --> 00:18:09,600
this color will be comparable to see and

00:18:06,960 --> 00:18:12,540
hear the world in jeaious well not I

00:18:09,600 --> 00:18:18,600
mean it's a bit slower but still not

00:18:12,540 --> 00:18:21,030
eternity so native him building a native

00:18:18,600 --> 00:18:22,679
image it's kind of notoriously hard to

00:18:21,030 --> 00:18:26,940
do ahead of time compilation of

00:18:22,679 --> 00:18:29,820
languages like Java and Scala so in

00:18:26,940 --> 00:18:31,380
order to get our native images fast we

00:18:29,820 --> 00:18:33,480
introduced this mode of compilation

00:18:31,380 --> 00:18:36,270
called profile profile guided

00:18:33,480 --> 00:18:39,270
optimizations so what we do is

00:18:36,270 --> 00:18:41,820
essentially because the girl compiler is

00:18:39,270 --> 00:18:44,190
built was originally built for the JVM

00:18:41,820 --> 00:18:46,890
so all of those all of the phases all of

00:18:44,190 --> 00:18:49,500
the optimizations they want to know

00:18:46,890 --> 00:18:51,179
which which paths in the program are hot

00:18:49,500 --> 00:18:51,950
and which are called so you need to need

00:18:51,179 --> 00:18:55,870
know all the

00:18:51,950 --> 00:18:59,419
probabilities for every branch you take

00:18:55,870 --> 00:19:02,960
so what we do is essentially we build a

00:18:59,419 --> 00:19:06,880
native image in the instrumentation mode

00:19:02,960 --> 00:19:09,799
where we very is an instrument every

00:19:06,880 --> 00:19:11,809
conditional in the code and every method

00:19:09,799 --> 00:19:14,029
call and then we get an instrument

00:19:11,809 --> 00:19:15,860
advisory so this binary is a bit slower

00:19:14,029 --> 00:19:17,330
than the original one but that's not

00:19:15,860 --> 00:19:20,299
important because it's anywhere running

00:19:17,330 --> 00:19:21,980
only before you ship your code so we and

00:19:20,299 --> 00:19:24,010
then you can take relevant workloads and

00:19:21,980 --> 00:19:27,169
then from those you build profiles

00:19:24,010 --> 00:19:28,760
profile files which again feed into the

00:19:27,169 --> 00:19:31,250
native image adjust this time you say

00:19:28,760 --> 00:19:33,260
there's this video and this Indian gives

00:19:31,250 --> 00:19:43,039
you an optimized binary that you can go

00:19:33,260 --> 00:19:45,049
use later in production so it's

00:19:43,039 --> 00:19:53,299
recording this in the show that's

00:19:45,049 --> 00:19:55,429
important yeah so let's take the streams

00:19:53,299 --> 00:19:58,340
that we had here so it's already

00:19:55,429 --> 00:20:00,470
compiled so we can just say to build the

00:19:58,340 --> 00:20:02,659
image it's very easy so this is native

00:20:00,470 --> 00:20:05,000
image it essentially comes with gravy on

00:20:02,659 --> 00:20:09,980
so you just say the name of the main

00:20:05,000 --> 00:20:12,649
class and there we go so these are the

00:20:09,980 --> 00:20:14,450
faces that explode like explain so this

00:20:12,649 --> 00:20:16,340
is the analysis which kind of finds does

00:20:14,450 --> 00:20:18,380
the reach ability then we basically

00:20:16,340 --> 00:20:23,480
compile and then we just try to the

00:20:18,380 --> 00:20:25,940
image and now we can run test stream but

00:20:23,480 --> 00:20:29,240
you see that it's about 2x slower than

00:20:25,940 --> 00:20:39,409
what we had on hotspot so what we can do

00:20:29,240 --> 00:20:40,820
now is though there we go so this will

00:20:39,409 --> 00:20:43,070
add an additional phase to the

00:20:40,820 --> 00:20:45,710
compilation pipe welcome instrument you

00:20:43,070 --> 00:20:48,250
can see it here which adds all the

00:20:45,710 --> 00:20:48,250
instrumentation

00:20:52,240 --> 00:21:03,440
and then we can run that it's even

00:20:59,120 --> 00:21:10,010
slower some ten to fifty percent depends

00:21:03,440 --> 00:21:12,620
on the on the workload and then if you

00:21:10,010 --> 00:21:14,030
can see we will get this EP off it's

00:21:12,620 --> 00:21:16,190
called default because we didn't specify

00:21:14,030 --> 00:21:17,870
but you can also specify the file part

00:21:16,190 --> 00:21:20,000
so you can kind of run multiple

00:21:17,870 --> 00:21:30,170
benchmarks and then just accumulate your

00:21:20,000 --> 00:21:32,120
profiles and then if you do know this

00:21:30,170 --> 00:21:34,540
will no use suck in the profiles which

00:21:32,120 --> 00:21:36,880
we've seen on the on the diagram and

00:21:34,540 --> 00:21:39,710
essentially compile the program faster

00:21:36,880 --> 00:21:41,660
so native image is still not as

00:21:39,710 --> 00:21:44,570
sophisticated as the growl in the

00:21:41,660 --> 00:21:46,880
original mode because when you do JIT

00:21:44,570 --> 00:21:48,410
compilation you can do you can cut off

00:21:46,880 --> 00:21:50,180
branches that are never used which

00:21:48,410 --> 00:21:52,490
you're never allowed to do in the ahead

00:21:50,180 --> 00:21:55,400
of time mode so if you run this we will

00:21:52,490 --> 00:21:57,890
not get as good performance but we will

00:21:55,400 --> 00:22:00,290
basically you know get better

00:21:57,890 --> 00:22:04,220
performance than what you get with valve

00:22:00,290 --> 00:22:06,770
um so we got here about 30 40 percent so

00:22:04,220 --> 00:22:08,900
yeah so this is the basically how PDO is

00:22:06,770 --> 00:22:11,900
used so don't be afraid to use it's very

00:22:08,900 --> 00:22:16,930
simple just you know run one more build

00:22:11,900 --> 00:22:16,930
step before you ship the final binary so

00:22:17,320 --> 00:22:24,170
good and here is just for the yeah this

00:22:22,430 --> 00:22:26,060
is this is how you use it if you have a

00:22:24,170 --> 00:22:28,040
dump file so you can just dump multiple

00:22:26,060 --> 00:22:34,430
runs and then you basically specify them

00:22:28,040 --> 00:22:37,660
here so let's let's see I'm sorry one

00:22:34,430 --> 00:22:43,370
more thing so let's let's try this

00:22:37,660 --> 00:22:47,420
native image tool on I have here this I

00:22:43,370 --> 00:22:50,240
I have made this script it does it's

00:22:47,420 --> 00:22:51,320
called Scala see image don't look at the

00:22:50,240 --> 00:22:53,450
length of the thing but what it

00:22:51,320 --> 00:22:55,910
basically does it gives this class path

00:22:53,450 --> 00:22:59,300
to the Scala compiler I took the latest

00:22:55,910 --> 00:23:01,580
version of 213 it says which methods

00:22:59,300 --> 00:23:04,490
have to be substituted because Scala

00:23:01,580 --> 00:23:07,160
compiler uses invokedynamic in one place

00:23:04,490 --> 00:23:10,220
so this needs to be substituted but this

00:23:07,160 --> 00:23:13,520
can also be removed easily and then it

00:23:10,220 --> 00:23:15,200
says which classes are reflectively

00:23:13,520 --> 00:23:17,840
loaded in the scholar compilers you need

00:23:15,200 --> 00:23:20,270
to kind of tell the tool what is what

00:23:17,840 --> 00:23:22,340
what which reflection which dynamic

00:23:20,270 --> 00:23:28,310
loading is going to happen and then we

00:23:22,340 --> 00:23:30,380
say give it the name scholar see okay so

00:23:28,310 --> 00:23:32,930
let's let's build the native image of

00:23:30,380 --> 00:23:35,180
the scholar compiler so this time we

00:23:32,930 --> 00:23:39,580
will not use PDO because it takes a ton

00:23:35,180 --> 00:23:39,580
of time it takes a ton of time anyhow so

00:23:40,480 --> 00:23:43,520
and then what they also done while this

00:23:43,010 --> 00:23:47,600
is building

00:23:43,520 --> 00:23:48,980
I've written this script which

00:23:47,600 --> 00:23:50,630
essentially calls this call this is

00:23:48,980 --> 00:23:53,330
equivalent of the it's like a hacky

00:23:50,630 --> 00:23:54,230
equivalent of the Scala Bear scripts

00:23:53,330 --> 00:23:57,800
which run Scala

00:23:54,230 --> 00:24:00,350
so it you know passes the class part and

00:23:57,800 --> 00:24:02,930
all the stuff what it needs in addition

00:24:00,350 --> 00:24:05,180
is the Artie the jar that you can see

00:24:02,930 --> 00:24:07,040
here and the reason is that it's not

00:24:05,180 --> 00:24:08,930
present because this is a native image

00:24:07,040 --> 00:24:13,400
so you kind of you need to pass it on a

00:24:08,930 --> 00:24:15,260
class path so essentially for now I just

00:24:13,400 --> 00:24:17,900
built this image for the Scala see the

00:24:15,260 --> 00:24:19,220
latest version but with the release with

00:24:17,900 --> 00:24:22,460
the next release which is at the end of

00:24:19,220 --> 00:24:25,130
the month I will make the versions four

00:24:22,460 --> 00:24:27,290
to twelve and then hopefully we can add

00:24:25,130 --> 00:24:29,750
changes to Scala to thirteen so that we

00:24:27,290 --> 00:24:31,580
don't need these substitutions this one

00:24:29,750 --> 00:24:34,130
substitution actually so so that it you

00:24:31,580 --> 00:24:36,050
can build the image just with a simple

00:24:34,130 --> 00:24:38,870
command line and also I'll publish the

00:24:36,050 --> 00:24:40,700
scripts the link will be provided so for

00:24:38,870 --> 00:24:42,230
the next essentially from the next

00:24:40,700 --> 00:24:45,920
release will be able to build a native

00:24:42,230 --> 00:24:52,340
image of the Scala compiler 212 and 213

00:24:45,920 --> 00:24:55,520
and if you think's colors is compiling

00:24:52,340 --> 00:24:58,360
slowly you should try native image this

00:24:55,520 --> 00:24:58,360
is my daily life

00:25:11,460 --> 00:25:14,670
almost there

00:25:16,840 --> 00:25:22,850
okay and so while this is still

00:25:19,130 --> 00:25:24,770
compiling what we will run here is this

00:25:22,850 --> 00:25:28,310
program which was written by my son he's

00:25:24,770 --> 00:25:31,370
six so he started writing code and it

00:25:28,310 --> 00:25:35,060
does some ASCII art so we will use this

00:25:31,370 --> 00:25:40,850
here for the demo it essentially you

00:25:35,060 --> 00:25:43,930
will see what it has done so don't

00:25:40,850 --> 00:25:47,180
complain about scholar compilation time

00:25:43,930 --> 00:25:48,860
okay so this is the script I've written

00:25:47,180 --> 00:25:51,890
and then we just like a nice daughter

00:25:48,860 --> 00:26:00,230
scholar we say you know we can even find

00:25:51,890 --> 00:26:04,510
that now you can say it's done in 180

00:26:00,230 --> 00:26:09,160
milliseconds and then let's run it I

00:26:04,510 --> 00:26:09,160
have to use to 13-under wise it fails

00:26:09,940 --> 00:26:17,870
and then we say and then if we'd say

00:26:14,720 --> 00:26:21,110
this which in Serbian means summer it

00:26:17,870 --> 00:26:23,600
will print basically I know we already

00:26:21,110 --> 00:26:27,500
put the for loop it will prevent you

00:26:23,600 --> 00:26:30,890
know the sunshine and if it they say

00:26:27,500 --> 00:26:41,240
winter it will print snow and the end

00:26:30,890 --> 00:26:43,490
chimney has smoke okay so then we built

00:26:41,240 --> 00:26:46,820
a native image of the benchmarking suite

00:26:43,490 --> 00:26:48,530
so again we have Scala P we have vector

00:26:46,820 --> 00:26:51,080
and re to us

00:26:48,530 --> 00:26:52,730
so only this is a cold run so we will

00:26:51,080 --> 00:26:55,040
run the benchmark suite essentially only

00:26:52,730 --> 00:26:57,110
for one iteration on hotspot and one

00:26:55,040 --> 00:26:58,880
iteration in the native image and this

00:26:57,110 --> 00:27:01,790
is what we get so for Scala P you get

00:26:58,880 --> 00:27:03,920
like 6x faster because the first

00:27:01,790 --> 00:27:06,860
iteration just takes 10 seconds on on

00:27:03,920 --> 00:27:10,130
hotspot and for smaller projects even

00:27:06,860 --> 00:27:11,750
more so 13 and 9 which essentially if

00:27:10,130 --> 00:27:14,480
you're working on a command line is

00:27:11,750 --> 00:27:16,700
incredible right

00:27:14,480 --> 00:27:21,100
then we also compile compare the native

00:27:16,700 --> 00:27:21,100
image with

00:27:21,810 --> 00:27:27,930
with the warm-up VM and this is what we

00:27:25,230 --> 00:27:30,180
get so on this graph hotspot here is

00:27:27,930 --> 00:27:32,310
essentially a hotspot which doesn't use

00:27:30,180 --> 00:27:34,170
compressed pointers and I put this which

00:27:32,310 --> 00:27:35,940
is not a default modern hotspot and I

00:27:34,170 --> 00:27:37,890
put this intentionally because SVM at

00:27:35,940 --> 00:27:39,210
the moment doesn't support compress

00:27:37,890 --> 00:27:41,640
pointers and this makes a huge

00:27:39,210 --> 00:27:44,880
difference and we are adding them in the

00:27:41,640 --> 00:27:47,400
next month or two so so basically you

00:27:44,880 --> 00:27:50,280
know we will get this gap very soon so

00:27:47,400 --> 00:27:51,780
and then we take a native image and then

00:27:50,280 --> 00:27:53,520
we take hotspot with compress pointers

00:27:51,780 --> 00:27:55,080
and native image that we built from the

00:27:53,520 --> 00:27:57,240
scholar compiler that I'll just show you

00:27:55,080 --> 00:28:00,330
and essentially the native image the

00:27:57,240 --> 00:28:02,250
head of time compile code is comparable

00:28:00,330 --> 00:28:04,920
to hotspot so here is 3% faster

00:28:02,250 --> 00:28:07,230
somewhere is 2% slower but in general

00:28:04,920 --> 00:28:09,420
it's comparable and once we get the

00:28:07,230 --> 00:28:13,620
compressed pointers this gap from here

00:28:09,420 --> 00:28:16,230
will go here basically so so I find this

00:28:13,620 --> 00:28:18,210
quite amazing that an i/o team it is

00:28:16,230 --> 00:28:24,750
faster than the kind of the fastest

00:28:18,210 --> 00:28:26,790
compiler on the JVM at the moment so of

00:28:24,750 --> 00:28:30,020
course because you build ahead of time

00:28:26,790 --> 00:28:33,480
there are some restrictions essentially

00:28:30,020 --> 00:28:36,930
all of the the whole image is built on

00:28:33,480 --> 00:28:38,820
the closed world assumption so all of

00:28:36,930 --> 00:28:42,690
the dynamically loaded classes must be

00:28:38,820 --> 00:28:44,550
known during the image build in some

00:28:42,690 --> 00:28:46,590
cases involved I Namek which generates

00:28:44,550 --> 00:28:49,770
new code or just features the code like

00:28:46,590 --> 00:28:52,560
in the Scala compiler will not work and

00:28:49,770 --> 00:28:54,630
you cannot do things like bytecode

00:28:52,560 --> 00:28:58,170
generation at runtime so you can

00:28:54,630 --> 00:28:59,910
generate any bytecode during the image

00:28:58,170 --> 00:29:01,440
real time but not at runtime so if

00:28:59,910 --> 00:29:03,690
something happens in the static fields

00:29:01,440 --> 00:29:05,970
are in the initializers that's fine but

00:29:03,690 --> 00:29:09,360
later in the in the runtime it's not not

00:29:05,970 --> 00:29:11,970
allowed and then we have quite good

00:29:09,360 --> 00:29:13,470
support for the JDK some of the things

00:29:11,970 --> 00:29:16,140
that we don't care about are still

00:29:13,470 --> 00:29:17,940
missing but you know the community is

00:29:16,140 --> 00:29:19,470
starting to involve and we are you know

00:29:17,940 --> 00:29:21,840
faster and faster fixing them for

00:29:19,470 --> 00:29:26,670
example swing and AWT are still not

00:29:21,840 --> 00:29:28,170
implemented and this is unfortunate for

00:29:26,670 --> 00:29:30,630
the Scala

00:29:28,170 --> 00:29:32,310
compilation because macros are

00:29:30,630 --> 00:29:34,650
dynamically loaded and they're basically

00:29:32,310 --> 00:29:38,190
distributed all over

00:29:34,650 --> 00:29:40,320
so it's very hard to kind of build a

00:29:38,190 --> 00:29:43,110
native image which which would work in

00:29:40,320 --> 00:29:47,280
an SBT setting right because you would

00:29:43,110 --> 00:29:48,780
need to know either build it's actually

00:29:47,280 --> 00:29:50,580
we have a few stuff you possible

00:29:48,780 --> 00:29:52,260
solutions right so you can build a

00:29:50,580 --> 00:29:54,059
custom image for your project with your

00:29:52,260 --> 00:29:57,030
macros and then your developers can use

00:29:54,059 --> 00:30:00,270
it but that's kind of I don't know it

00:29:57,030 --> 00:30:02,460
would require quite some SBT hacking and

00:30:00,270 --> 00:30:04,020
I don't know if it would be feasible in

00:30:02,460 --> 00:30:06,870
the midterm we will definitely support

00:30:04,020 --> 00:30:08,820
some kind of dynamic loading so your

00:30:06,870 --> 00:30:12,990
macros could be compiled separately the

00:30:08,820 --> 00:30:14,700
native image and in the long term we

00:30:12,990 --> 00:30:18,299
will definitely support dynamic class

00:30:14,700 --> 00:30:21,780
loading but this is a longer period so

00:30:18,299 --> 00:30:25,740
in the end we will have Scala C which

00:30:21,780 --> 00:30:28,200
essentially runs fast

00:30:25,740 --> 00:30:30,390
right from the start and it runs faster

00:30:28,200 --> 00:30:32,429
than hotspot and it will be probably

00:30:30,390 --> 00:30:34,980
licensed as free to use because it's a

00:30:32,429 --> 00:30:38,580
native image it's not it's not growl VM

00:30:34,980 --> 00:30:40,559
itself so but if anyone is interested in

00:30:38,580 --> 00:30:42,660
one of these projects kind of you know

00:30:40,559 --> 00:30:49,200
send me an email we can we can work on

00:30:42,660 --> 00:30:52,290
it I didn't have time yeah so and then

00:30:49,200 --> 00:30:55,020
in the last part of the talk I will talk

00:30:52,290 --> 00:30:57,059
about the shared library part of the

00:30:55,020 --> 00:31:00,860
native image and inter interrupt with C

00:30:57,059 --> 00:31:04,500
so essentially what we support in Java

00:31:00,860 --> 00:31:07,559
are annotations to kind of hook into the

00:31:04,500 --> 00:31:09,840
native to the native libraries so we

00:31:07,559 --> 00:31:11,610
have a bunch of here is just for an

00:31:09,840 --> 00:31:14,760
example I want to go into all of them

00:31:11,610 --> 00:31:18,059
but you can essentially you know call C

00:31:14,760 --> 00:31:20,460
functions this is very similar to to

00:31:18,059 --> 00:31:23,210
Scala native so you can you know define

00:31:20,460 --> 00:31:26,040
constants you can define structs

00:31:23,210 --> 00:31:28,350
pointers you can have a context for

00:31:26,040 --> 00:31:30,360
compilation which libraries you're using

00:31:28,350 --> 00:31:34,799
and for example this is our

00:31:30,360 --> 00:31:36,210
implementation of nano time in in SVM so

00:31:34,799 --> 00:31:38,250
we write the whole VM is written

00:31:36,210 --> 00:31:40,230
basically in Java but if you look at it

00:31:38,250 --> 00:31:41,730
it's essentially you say stack value to

00:31:40,230 --> 00:31:43,580
get so this is essentially a stack

00:31:41,730 --> 00:31:46,040
allocation from C

00:31:43,580 --> 00:31:49,010
and then you call C functions and and

00:31:46,040 --> 00:31:51,290
computed alright so and why am i

00:31:49,010 --> 00:31:53,840
mentioning this because in the scholar

00:31:51,290 --> 00:31:56,180
community there is this project called

00:31:53,840 --> 00:32:00,800
Scala native which kind of does the same

00:31:56,180 --> 00:32:04,190
thing it does one thing very well and it

00:32:00,800 --> 00:32:06,230
has essentially asked interface which is

00:32:04,190 --> 00:32:10,340
idiomatic to Scala and it's very

00:32:06,230 --> 00:32:13,430
intuitive to use and essentially Scala

00:32:10,340 --> 00:32:16,310
native can also be implemented by gravia

00:32:13,430 --> 00:32:18,500
so if you take the native image and we

00:32:16,310 --> 00:32:20,390
take the Scala native intrinsics that we

00:32:18,500 --> 00:32:23,150
have and then we just compile them into

00:32:20,390 --> 00:32:26,050
the into the primitives that we have

00:32:23,150 --> 00:32:30,920
here which is not always easy but very

00:32:26,050 --> 00:32:33,650
easier than implementing Scala native we

00:32:30,920 --> 00:32:36,410
can get the same thing basically so and

00:32:33,650 --> 00:32:38,510
it can be also implemented probably just

00:32:36,410 --> 00:32:40,250
by a simple compiler plugin I wouldn't I

00:32:38,510 --> 00:32:44,180
wouldn't think it's more than a month or

00:32:40,250 --> 00:32:46,250
two of work to do that and then what you

00:32:44,180 --> 00:32:48,620
get with this is that you get all of the

00:32:46,250 --> 00:32:53,290
JVM libraries so it are they from Kotlin

00:32:48,620 --> 00:32:55,550
or closure or or Scala itself for free

00:32:53,290 --> 00:32:57,500
and you don't need to rewrite the whole

00:32:55,550 --> 00:33:02,420
JVM ecosystem which is kind of a big

00:32:57,500 --> 00:33:04,880
thing and so what I did is I took the

00:33:02,420 --> 00:33:07,700
Scala native has a benchmarking suite so

00:33:04,880 --> 00:33:09,620
I took the benchmarks from there so

00:33:07,700 --> 00:33:11,870
orange is the native image just a

00:33:09,620 --> 00:33:14,360
vanilla one without video and the red

00:33:11,870 --> 00:33:17,240
one is with video and so this is :

00:33:14,360 --> 00:33:20,780
native without their version of the PDO

00:33:17,240 --> 00:33:23,480
because it's not published yet and it's

00:33:20,780 --> 00:33:27,080
using this image GGC which is supposedly

00:33:23,480 --> 00:33:28,570
the faster GC for Scala native so and if

00:33:27,080 --> 00:33:32,240
you look what native image does

00:33:28,570 --> 00:33:35,480
essentially it outperforms it even in

00:33:32,240 --> 00:33:41,509
the in the open version by 2 to 2 5 X

00:33:35,480 --> 00:33:48,749
and then up to 10 X bit video

00:33:41,509 --> 00:33:50,759
so yeah so in summary we get with Galvin

00:33:48,749 --> 00:33:52,649
we get scholar compilation faster from

00:33:50,759 --> 00:33:54,899
one point three to one point five x and

00:33:52,649 --> 00:33:59,279
I personally believe we can even go

00:33:54,899 --> 00:34:02,070
better and then we can get native images

00:33:59,279 --> 00:34:03,659
of scholar programs fast startup with

00:34:02,070 --> 00:34:07,250
fast startup and low footprint we've

00:34:03,659 --> 00:34:09,750
seen about 10x faster let's start up

00:34:07,250 --> 00:34:12,119
it's also faster than the hot spot JIT

00:34:09,750 --> 00:34:13,950
compiled code and essentially if you

00:34:12,119 --> 00:34:16,589
have a cool tool that you like and it's

00:34:13,950 --> 00:34:19,800
in open source for example skullery for

00:34:16,589 --> 00:34:21,210
more Scala fmt just contact us and I

00:34:19,800 --> 00:34:23,520
think we can get the free license

00:34:21,210 --> 00:34:26,790
basically for the for the PDO and and

00:34:23,520 --> 00:34:28,859
all of the enterprise features so you

00:34:26,790 --> 00:34:33,000
will get this essentially the you know

00:34:28,859 --> 00:34:34,980
fast startup and fast startup and faster

00:34:33,000 --> 00:34:38,220
peak performance which is kind of nice

00:34:34,980 --> 00:34:40,139
and then we have open problems which are

00:34:38,220 --> 00:34:45,119
you know effective support for Scala

00:34:40,139 --> 00:34:46,560
macros in the native image so I'm open

00:34:45,119 --> 00:34:50,280
to discussion I think with this needs to

00:34:46,560 --> 00:34:50,970
be solved and then we we need support

00:34:50,280 --> 00:34:54,300
for the Scala

00:34:50,970 --> 00:34:56,040
native interoperability VC so we can run

00:34:54,300 --> 00:34:57,780
Scala native but we cannot do C

00:34:56,040 --> 00:35:02,190
interrupt because color native all uses

00:34:57,780 --> 00:35:05,630
their own intrinsics and you know I hope

00:35:02,190 --> 00:35:10,020
that in a year or so code to Google will

00:35:05,630 --> 00:35:14,030
will get something like this so thank

00:35:10,020 --> 00:35:14,030
you very much all of the Dems yeah

00:35:19,380 --> 00:35:24,460
so all of the demos will be published

00:35:21,790 --> 00:35:27,310
here I work for Oracle so it takes an

00:35:24,460 --> 00:35:28,900
approval or something so whatever but

00:35:27,310 --> 00:35:32,050
really the next release at the end of

00:35:28,900 --> 00:35:34,330
the month we'll have it all there okay

00:35:32,050 --> 00:35:38,950
thank you very much and I think I opened

00:35:34,330 --> 00:35:45,910
the room for questions right away first

00:35:38,950 --> 00:35:48,940
come first serve so what does he do but

00:35:45,910 --> 00:35:51,910
she doesn't do the things I've shown on

00:35:48,940 --> 00:35:55,300
this well you've seen one of the

00:35:51,910 --> 00:35:57,490
optimizations and as as we not show

00:35:55,300 --> 00:36:00,280
escape analysis partial escape analysis

00:35:57,490 --> 00:36:01,690
in is in both so that but the pod

00:36:00,280 --> 00:36:05,590
duplication for example is there

00:36:01,690 --> 00:36:10,930
inlining is better and a few other small

00:36:05,590 --> 00:36:18,400
things but I think yeah these are the

00:36:10,930 --> 00:36:21,550
key things so I'm curious I I didn't get

00:36:18,400 --> 00:36:24,880
quite where growl VM operates it

00:36:21,550 --> 00:36:26,920
operates on byte codes right mmm so does

00:36:24,880 --> 00:36:29,890
it or does it also in need the source

00:36:26,920 --> 00:36:32,200
code no so so it doesn't it operates all

00:36:29,890 --> 00:36:34,420
isn't byte code so that's that's our

00:36:32,200 --> 00:36:37,600
input so it's basically you know if it

00:36:34,420 --> 00:36:40,090
works on in the open JDK it essentially

00:36:37,600 --> 00:36:42,100
is just like normal hotspot when you run

00:36:40,090 --> 00:36:44,859
it so it parses bytecode and gets the

00:36:42,100 --> 00:36:47,650
assembly out native image also operates

00:36:44,859 --> 00:36:50,580
um just pure by code so it does this

00:36:47,650 --> 00:36:52,780
does it ahead of time so yeah all the

00:36:50,580 --> 00:36:54,430
optimizations you have shown they don't

00:36:52,780 --> 00:36:56,320
actually operate on the source but they

00:36:54,430 --> 00:36:59,340
operate on what happens on bytecode

00:36:56,320 --> 00:36:59,340
level no no

00:37:03,809 --> 00:37:08,859
are there any changes related to

00:37:06,369 --> 00:37:11,470
inlining Koenig amorphic and coal sites

00:37:08,859 --> 00:37:14,109
oh yes but you've seen the streams think

00:37:11,470 --> 00:37:16,319
I didn't do the scholar collections in

00:37:14,109 --> 00:37:18,670
the interest of time but it basically

00:37:16,319 --> 00:37:20,829
it's pretty much the same results

00:37:18,670 --> 00:37:23,650
you know you get for these like small

00:37:20,829 --> 00:37:27,010
snippets of like collections you get two

00:37:23,650 --> 00:37:33,720
to seven eight X depends all right so so

00:37:27,010 --> 00:37:37,390
yes it does help what what exactly

00:37:33,720 --> 00:37:39,400
changed in metamorphic callsigns well

00:37:37,390 --> 00:37:42,039
because you in line more right so when

00:37:39,400 --> 00:37:47,260
you in line more you typically when you

00:37:42,039 --> 00:37:48,609
in line more you get you know when in

00:37:47,260 --> 00:37:50,140
line more you get essentially less

00:37:48,609 --> 00:37:52,660
metamorphic sites right so the problem

00:37:50,140 --> 00:37:54,460
is you know in some cases it will still

00:37:52,660 --> 00:37:56,890
happen right but the more you're in line

00:37:54,460 --> 00:37:59,109
the more you know code you have and the

00:37:56,890 --> 00:38:01,029
less megamorph occur calls are right so

00:37:59,109 --> 00:38:06,700
that's that's kind of the simplest

00:38:01,029 --> 00:38:08,260
answer I can give you what does the rest

00:38:06,700 --> 00:38:11,140
of the tooling look like hey still make

00:38:08,260 --> 00:38:13,510
thread dams heat dumps profiling that

00:38:11,140 --> 00:38:15,220
sort of thing yeah good question thank

00:38:13,510 --> 00:38:16,990
you I I didn't put that or not to

00:38:15,220 --> 00:38:19,089
complicate yeah so you can use gdb to

00:38:16,990 --> 00:38:22,089
debug so when you debug Java it works

00:38:19,089 --> 00:38:24,970
pretty pretty well when you debug Scala

00:38:22,089 --> 00:38:27,460
well you know and it kind of gets to all

00:38:24,970 --> 00:38:29,890
the low-level stuff so it kind of you

00:38:27,460 --> 00:38:32,770
know you can get to your stuff but it

00:38:29,890 --> 00:38:35,109
takes a bit of effort I guess it needs

00:38:32,770 --> 00:38:36,430
someone to to do that one if people

00:38:35,109 --> 00:38:37,930
start using it seriously we would

00:38:36,430 --> 00:38:40,359
probably do that and because we have

00:38:37,930 --> 00:38:43,180
this for gdb we have this kind of Python

00:38:40,359 --> 00:38:44,890
helpers which allow you to interpret the

00:38:43,180 --> 00:38:47,589
language specific stuff because you know

00:38:44,890 --> 00:38:50,500
it's a there is a mismatch between what

00:38:47,589 --> 00:38:53,170
you get from from you know what you get

00:38:50,500 --> 00:38:55,720
from debug info in the image alright so

00:38:53,170 --> 00:38:58,720
and then heap dumps yeah that supported

00:38:55,720 --> 00:39:00,640
profiling is supported code coverage

00:38:58,720 --> 00:39:03,819
will come soon because this PTO feature

00:39:00,640 --> 00:39:06,760
also kind of you know it also tells you

00:39:03,819 --> 00:39:10,510
which branches were executed so yeah so

00:39:06,760 --> 00:39:13,210
it's it you know maybe it's not as so

00:39:10,510 --> 00:39:16,270
the native image part is not as

00:39:13,210 --> 00:39:19,360
as the as the hot spot I would still

00:39:16,270 --> 00:39:21,120
prefer working in on JVM and the

00:39:19,360 --> 00:39:24,160
immediate bill times are quite long and

00:39:21,120 --> 00:39:28,140
the Java part is equivalent basically so

00:39:24,160 --> 00:39:32,740
you have all of the stuff the same yeah

00:39:28,140 --> 00:39:37,840
as far as I know yes as far as I know

00:39:32,740 --> 00:39:44,140
only Java 8 to support right now and it

00:39:37,840 --> 00:39:46,300
so ordered Java 911 yeah so this this

00:39:44,140 --> 00:39:48,220
what I presented is Java 8 we ship a

00:39:46,300 --> 00:39:51,130
modified version of Java 8 which allows

00:39:48,220 --> 00:39:53,950
us to put growl inside with Java 9 the

00:39:51,130 --> 00:39:56,740
JVM CI which is essentially the compiler

00:39:53,950 --> 00:39:58,780
interface comes with with the VM so yes

00:39:56,740 --> 00:40:02,140
this will be running on on you know

00:39:58,780 --> 00:40:05,170
you're pure regular job you can just add

00:40:02,140 --> 00:40:07,570
you can just add you know a bit of a

00:40:05,170 --> 00:40:09,940
class path for your growl compiler jars

00:40:07,570 --> 00:40:14,560
and that's it so I will be able to use

00:40:09,940 --> 00:40:16,180
the Java 9 runtime jars yes ok yes you

00:40:14,560 --> 00:40:18,700
can use them even now but it's I think

00:40:16,180 --> 00:40:20,650
we don't still ship it with Java 9 but

00:40:18,700 --> 00:40:26,050
it generally we do tests on them and it

00:40:20,650 --> 00:40:28,120
all works yeah thank you we see a good

00:40:26,050 --> 00:40:30,640
performance improvement for the JVM

00:40:28,120 --> 00:40:32,830
based languages but what about and seems

00:40:30,640 --> 00:40:35,170
likely to use using the bytecode to the

00:40:32,830 --> 00:40:37,330
source and uses a static analyzers ahead

00:40:35,170 --> 00:40:40,630
of time compilation but what about the

00:40:37,330 --> 00:40:42,250
dynamic languages not just Ruby what is

00:40:40,630 --> 00:40:47,190
about departments yeah good good

00:40:42,250 --> 00:40:50,740
questions so javascript is the same as

00:40:47,190 --> 00:40:52,090
v8 if you don't take regular expressions

00:40:50,740 --> 00:40:54,730
into account that you're working on now

00:40:52,090 --> 00:40:57,000
it's even number one percent faster on

00:40:54,730 --> 00:40:59,740
like a do mean of all the benchmarks

00:40:57,000 --> 00:41:01,930
ruby and other languages because v8 has

00:40:59,740 --> 00:41:05,140
a big team working behind it right so so

00:41:01,930 --> 00:41:07,630
it's super optimized so other languages

00:41:05,140 --> 00:41:11,920
are very faster so ruby is 4x faster

00:41:07,630 --> 00:41:14,200
than the JRuby implementation and our is

00:41:11,920 --> 00:41:16,630
like 5x I think and then

00:41:14,200 --> 00:41:20,410
python is also very faster so so

00:41:16,630 --> 00:41:21,990
basically LLVM is a bit slower than when

00:41:20,410 --> 00:41:24,420
you compile it with the reality of

00:41:21,990 --> 00:41:27,270
it is unfortunate I believe we can be

00:41:24,420 --> 00:41:29,730
faster there as well but basically you

00:41:27,270 --> 00:41:31,619
know it's faster it's slower it's at

00:41:29,730 --> 00:41:35,040
warm-up we haven't worked on warm-up so

00:41:31,619 --> 00:41:37,530
we ate has a super good interpreter and

00:41:35,040 --> 00:41:40,650
the compiler is tuned for for starting

00:41:37,530 --> 00:41:42,960
of like fast in our case it's it's kind

00:41:40,650 --> 00:41:44,790
of it's you can use it in a shell

00:41:42,960 --> 00:41:47,910
scripting mode you get pissed I get

00:41:44,790 --> 00:41:52,950
pissed when use it's also basically only

00:41:47,910 --> 00:41:56,580
in the server mode at the moment okay in

00:41:52,950 --> 00:41:58,920
Java 9 is a compiler interface

00:41:56,580 --> 00:42:00,180
implemented with Enterprise Edition or

00:41:58,920 --> 00:42:03,060
Community Edition

00:42:00,180 --> 00:42:05,850
well it's the licensing thing all right

00:42:03,060 --> 00:42:07,320
so so essentially the compiler interface

00:42:05,850 --> 00:42:09,720
is the interface and then you can put

00:42:07,320 --> 00:42:12,869
you know whichever Edition you you want

00:42:09,720 --> 00:42:15,000
right so so for non production use like

00:42:12,869 --> 00:42:16,830
compilation for example you can put the

00:42:15,000 --> 00:42:18,330
Enterprise Edition and otherwise you can

00:42:16,830 --> 00:42:21,540
just take the Community Edition which

00:42:18,330 --> 00:42:23,400
also gives speed ups for Scala and it's

00:42:21,540 --> 00:42:26,100
also ready used in Twitter they use it

00:42:23,400 --> 00:42:27,720
in production so there is a great talk

00:42:26,100 --> 00:42:29,280
by Chris telling her it will be in the

00:42:27,720 --> 00:42:31,530
Scala days in New York you can take a

00:42:29,280 --> 00:42:35,750
look at that and his experiences from

00:42:31,530 --> 00:42:39,750
there a lot of the time people put

00:42:35,750 --> 00:42:42,270
resources on the class path and and I

00:42:39,750 --> 00:42:44,280
try to use resources on the class path

00:42:42,270 --> 00:42:46,640
of native image but that can't work

00:42:44,280 --> 00:42:50,150
because class loaders not supported with

00:42:46,640 --> 00:42:53,270
W time for him to support that use case

00:42:50,150 --> 00:42:56,730
no it actually as long as you know stuff

00:42:53,270 --> 00:42:58,320
ahead of time it's all works so it will

00:42:56,730 --> 00:43:00,600
it at the moment we have resources

00:42:58,320 --> 00:43:03,050
supported as long as you know them while

00:43:00,600 --> 00:43:05,640
you build your image you cannot do it

00:43:03,050 --> 00:43:07,470
later but yeah we will probably support

00:43:05,640 --> 00:43:09,630
this in but this is not kind of a

00:43:07,470 --> 00:43:12,570
near-future thing because we have bigger

00:43:09,630 --> 00:43:14,460
fish to fry at the moment but seems to

00:43:12,570 --> 00:43:19,530
be I mean yes indeed in the Indian real

00:43:14,460 --> 00:43:23,460
supported but not at the moment so any

00:43:19,530 --> 00:43:25,820
more questions okay there's one study

00:43:23,460 --> 00:43:25,820
right

00:43:26,320 --> 00:43:35,150
does growl create a lot more assembly

00:43:30,740 --> 00:43:38,680
code and see - is that the problem it

00:43:35,150 --> 00:43:41,540
does create more code I haven't seen I

00:43:38,680 --> 00:43:43,460
haven't seen that it's a problem I think

00:43:41,540 --> 00:43:47,450
I said I think this paranoia about

00:43:43,460 --> 00:43:49,910
generating assembly code is it's a bit

00:43:47,450 --> 00:43:52,250
old-school I think it's not I mean I on

00:43:49,910 --> 00:43:55,250
laptops it works on several machines I

00:43:52,250 --> 00:43:57,320
haven't experienced any issues so but

00:43:55,250 --> 00:43:59,690
and we are also not we are not in the

00:43:57,320 --> 00:44:01,460
mode where you generate recklessly

00:43:59,690 --> 00:44:04,940
assembly just to get every bit of

00:44:01,460 --> 00:44:07,430
performance so so so we are quite

00:44:04,940 --> 00:44:09,050
conservative both the the inlining and

00:44:07,430 --> 00:44:11,180
the third duplication essentially all

00:44:09,050 --> 00:44:14,270
duplication phases are very conservative

00:44:11,180 --> 00:44:16,730
about that so so yes he generates more

00:44:14,270 --> 00:44:19,100
but not like the you know orders of

00:44:16,730 --> 00:44:20,630
magnitude and also interesting what I

00:44:19,100 --> 00:44:22,250
found interesting is also if you I was

00:44:20,630 --> 00:44:24,860
playing with the parameter tuning for

00:44:22,250 --> 00:44:26,570
the Scala compiler and then I was even

00:44:24,860 --> 00:44:28,460
generating like two times more code than

00:44:26,570 --> 00:44:30,260
three times more called and the

00:44:28,460 --> 00:44:32,330
performance went up for only by three

00:44:30,260 --> 00:44:34,490
percent and so so it's kind of day I

00:44:32,330 --> 00:44:36,260
think the aligner is doing a very good

00:44:34,490 --> 00:44:38,780
job in finding what really needs to be

00:44:36,260 --> 00:44:40,250
in mind and and another thing I find

00:44:38,780 --> 00:44:42,020
interesting is that for example this pod

00:44:40,250 --> 00:44:44,720
duplication although it should always

00:44:42,020 --> 00:44:46,820
duplicate code in some benchmarks it

00:44:44,720 --> 00:44:53,810
even like reduces the code size by far

00:44:46,820 --> 00:44:56,680
so okay thank you very much I think one

00:44:53,810 --> 00:44:56,680
more question okay

00:44:57,320 --> 00:45:06,090
you're the bad guy givin people away

00:45:00,240 --> 00:45:09,120
from the beer here so I have a question

00:45:06,090 --> 00:45:11,040
about your benchmark results against

00:45:09,120 --> 00:45:13,950
scanner native mm-hmm so you said that

00:45:11,040 --> 00:45:16,410
in some cases you go to up almost 10x

00:45:13,950 --> 00:45:19,350
come here to scare a native and from

00:45:16,410 --> 00:45:21,210
Dennis measurements and benchmarks he

00:45:19,350 --> 00:45:23,550
says is two times slower than hot spot

00:45:21,210 --> 00:45:25,940
so that would means that you're around

00:45:23,550 --> 00:45:29,910
four to five times faster than hot spot

00:45:25,940 --> 00:45:32,460
correct that's not maybe I can rerun the

00:45:29,910 --> 00:45:35,820
benchmarks but what I did is I I how

00:45:32,460 --> 00:45:36,780
random is I took the SBT and I took at

00:45:35,820 --> 00:45:39,050
the machine that they do the

00:45:36,780 --> 00:45:44,670
benchmarking on so everything the same I

00:45:39,050 --> 00:45:48,920
said I did SBT as with the benchmarks

00:45:44,670 --> 00:45:53,040
slash run and then from ourselves I did

00:45:48,920 --> 00:45:55,260
native image basically of the main class

00:45:53,040 --> 00:46:01,530
of the benchmarks and to run and then I

00:45:55,260 --> 00:46:04,800
took the values out and that's it so but

00:46:01,530 --> 00:46:10,200
I will rerun that yeah okay and update

00:46:04,800 --> 00:46:12,750
the slides if it's not correct yes okay

00:46:10,200 --> 00:46:14,510
thank you very much going yes and thank

00:46:12,750 --> 00:46:18,239
you very much everyone

00:46:14,510 --> 00:46:18,239

YouTube URL: https://www.youtube.com/watch?v=2HpTHGDrqcQ


