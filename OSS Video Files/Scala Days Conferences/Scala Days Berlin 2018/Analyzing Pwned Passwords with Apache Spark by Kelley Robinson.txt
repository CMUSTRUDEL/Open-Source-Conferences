Title: Analyzing Pwned Passwords with Apache Spark by Kelley Robinson
Publication date: 2018-09-20
Playlist: Scala Days Berlin 2018
Description: 
	This video was recorded at Scala Days Berlin 2018
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

More information and the abstract can be found here:
https://eu.scaladays.org/lect-6948-analyzing-pwned-passwords-with-apache-spark.html
Captions: 
	00:00:04,660 --> 00:00:08,920
good morning everyone I'm really excited

00:00:07,150 --> 00:00:10,780
to be here Scala days is one of my

00:00:08,920 --> 00:00:13,209
favorite conferences so thank you for

00:00:10,780 --> 00:00:15,160
having me back like he said my name is

00:00:13,209 --> 00:00:18,070
Kelly Robinson and I have spent most of

00:00:15,160 --> 00:00:19,600
my career working in Scala including

00:00:18,070 --> 00:00:22,359
about three years doing data engineering

00:00:19,600 --> 00:00:24,939
with Apache spark and we were doing that

00:00:22,359 --> 00:00:28,359
at an advertising technology company we

00:00:24,939 --> 00:00:31,480
I started there in about 2015 and at

00:00:28,359 --> 00:00:34,240
that point we were using spark maybe 1.1

00:00:31,480 --> 00:00:36,250
and so it was a really really early

00:00:34,240 --> 00:00:37,600
project at that point and it's been

00:00:36,250 --> 00:00:40,690
really interesting to see how the

00:00:37,600 --> 00:00:42,940
projects developed since then as you can

00:00:40,690 --> 00:00:44,320
imagine ad tech has a a lot of data so I

00:00:42,940 --> 00:00:46,390
spent most of my time at that company

00:00:44,320 --> 00:00:49,059
writing our data pipelines for our

00:00:46,390 --> 00:00:51,039
extract transform load or ETL pipelines

00:00:49,059 --> 00:00:55,000
and all of that was doing done doing

00:00:51,039 --> 00:00:56,559
using Scala and spark and it's really

00:00:55,000 --> 00:00:58,480
fascinating for me to see how many

00:00:56,559 --> 00:01:00,309
people are interested in spark at this

00:00:58,480 --> 00:01:02,019
point especially since the project has

00:01:00,309 --> 00:01:03,579
really only been GA for about three or

00:01:02,019 --> 00:01:05,580
four years so quick show of hands who

00:01:03,579 --> 00:01:09,000
here actively uses spark

00:01:05,580 --> 00:01:11,380
who here has like dabbled with spark

00:01:09,000 --> 00:01:15,159
alright cool and who here has written no

00:01:11,380 --> 00:01:17,080
spark oh wow so good balance of people

00:01:15,159 --> 00:01:19,240
this is gonna be a great talk for all of

00:01:17,080 --> 00:01:21,009
you it introduces some fundamentals of

00:01:19,240 --> 00:01:22,689
spark and it also gets into some of the

00:01:21,009 --> 00:01:24,280
new abstractions that even if you've

00:01:22,689 --> 00:01:27,850
been working with spark for a few years

00:01:24,280 --> 00:01:29,619
you might not have seen yet so I got out

00:01:27,850 --> 00:01:31,390
of AB Tech about 6 months ago these days

00:01:29,619 --> 00:01:33,729
I am a developer evangelist at Tullio

00:01:31,390 --> 00:01:36,340
where I get to work on a lot of cool and

00:01:33,729 --> 00:01:37,930
interesting things for those of you that

00:01:36,340 --> 00:01:40,240
are not familiar Twilio is a

00:01:37,930 --> 00:01:42,369
communications company so we make api's

00:01:40,240 --> 00:01:44,079
that allow you to add things like text

00:01:42,369 --> 00:01:45,310
messaging and voice calling video chat

00:01:44,079 --> 00:01:48,329
and things like two-factor

00:01:45,310 --> 00:01:52,149
authentication into your applications

00:01:48,329 --> 00:01:54,430
with identity being so closely tied to

00:01:52,149 --> 00:01:56,619
phone numbers Trulia acquired

00:01:54,430 --> 00:01:57,670
of--they in 2015 which most of you may

00:01:56,619 --> 00:01:59,770
know is a two-factor authentication

00:01:57,670 --> 00:02:01,329
provider and this is the kind of stuff

00:01:59,770 --> 00:02:03,610
that I work on every day I work on our

00:02:01,329 --> 00:02:04,840
account security products and this is

00:02:03,610 --> 00:02:06,579
really the last I'm gonna be talking

00:02:04,840 --> 00:02:08,759
about Twilio in this talk but since I

00:02:06,579 --> 00:02:12,190
spend so much of my day thinking about

00:02:08,759 --> 00:02:14,580
authentication and identity and how we

00:02:12,190 --> 00:02:16,930
can secure our online identities and

00:02:14,580 --> 00:02:17,780
passwords are really the biggest way

00:02:16,930 --> 00:02:20,480
that we've done that and

00:02:17,780 --> 00:02:22,069
thirty or forty years so when I was

00:02:20,480 --> 00:02:23,209
writing this talk I needed some data to

00:02:22,069 --> 00:02:24,280
dig through that was going to be a

00:02:23,209 --> 00:02:27,110
little bit more interesting than

00:02:24,280 --> 00:02:28,130
advertising clickstream data I don't

00:02:27,110 --> 00:02:30,590
know about you but that's not the most

00:02:28,130 --> 00:02:32,630
fascinating thing to me and fortunately

00:02:30,590 --> 00:02:35,120
or unfortunately we have about 500

00:02:32,630 --> 00:02:36,890
million breached credentials that we can

00:02:35,120 --> 00:02:40,580
dig through and so that's the stuff that

00:02:36,890 --> 00:02:42,140
we're gonna be looking at today we're

00:02:40,580 --> 00:02:44,390
going to start by introducing Apache

00:02:42,140 --> 00:02:47,360
spark look at why people are using it

00:02:44,390 --> 00:02:48,980
and how the project has developed I'll

00:02:47,360 --> 00:02:51,730
give a bit of background on the state of

00:02:48,980 --> 00:02:54,380
passwords before we dive into the data

00:02:51,730 --> 00:02:57,290
then I'm gonna show you how spark works

00:02:54,380 --> 00:02:58,640
with a little bit of live code and then

00:02:57,290 --> 00:03:00,440
finally we'll spend some time talking

00:02:58,640 --> 00:03:02,410
about the implications of the

00:03:00,440 --> 00:03:05,120
intersection of big data and security

00:03:02,410 --> 00:03:08,510
but let's start by looking at the apache

00:03:05,120 --> 00:03:10,580
spark project so spark markets markets

00:03:08,510 --> 00:03:13,069
itself as a fast unified analytics

00:03:10,580 --> 00:03:15,860
engine for big data and machine learning

00:03:13,069 --> 00:03:17,720
it provides a functional interface for

00:03:15,860 --> 00:03:19,910
doing a lot of the computations on top

00:03:17,720 --> 00:03:21,350
of that and it does that with languages

00:03:19,910 --> 00:03:24,049
like Scala which is why we're all here

00:03:21,350 --> 00:03:27,980
but it also has api's for writing and

00:03:24,049 --> 00:03:29,600
python sequel r and more it's generally

00:03:27,980 --> 00:03:31,130
thought of as an improvement on top of

00:03:29,600 --> 00:03:34,150
Hadoop and MapReduce in fact it was

00:03:31,130 --> 00:03:35,989
built on top of Hadoop and MapReduce and

00:03:34,150 --> 00:03:37,850
depending on what you're doing it's

00:03:35,989 --> 00:03:39,260
about a hundred times faster than Hadoop

00:03:37,850 --> 00:03:41,570
and MapReduce which is why so many

00:03:39,260 --> 00:03:42,920
people are moving to SPARC now any kind

00:03:41,570 --> 00:03:45,470
of Big Data job that they were running

00:03:42,920 --> 00:03:49,040
five years ago can now be done in SPARC

00:03:45,470 --> 00:03:49,910
and a fraction of the time when we talk

00:03:49,040 --> 00:03:51,560
about Big Data

00:03:49,910 --> 00:03:53,600
I mean data that can't fit on a single

00:03:51,560 --> 00:03:55,640
machine though machines are getting

00:03:53,600 --> 00:03:57,109
bigger every single day so thanks to

00:03:55,640 --> 00:03:59,780
Gary Bernhardt we know that there are

00:03:57,109 --> 00:04:01,609
Amazon instances that are enormous now

00:03:59,780 --> 00:04:02,870
so maybe you don't even need SPARC we

00:04:01,609 --> 00:04:06,739
can all just go home write a Python

00:04:02,870 --> 00:04:07,820
script end of talk just kidding so there

00:04:06,739 --> 00:04:08,959
are some companies that of course are

00:04:07,820 --> 00:04:11,420
doing this kind of stuff at petabyte

00:04:08,959 --> 00:04:13,790
scale and as our machine capabilities

00:04:11,420 --> 00:04:15,440
grow so does our data we are all very

00:04:13,790 --> 00:04:17,390
concerned with data these days and we're

00:04:15,440 --> 00:04:20,450
storing more and more of it even with EU

00:04:17,390 --> 00:04:21,620
regulations being what they are but I

00:04:20,450 --> 00:04:23,300
just want to include this because when

00:04:21,620 --> 00:04:24,979
people ask me about big data and tools

00:04:23,300 --> 00:04:27,470
for big data one of the things that I

00:04:24,979 --> 00:04:28,100
often say is like how big is your data

00:04:27,470 --> 00:04:30,350
action

00:04:28,100 --> 00:04:32,090
if you're dealing with even several you

00:04:30,350 --> 00:04:33,560
know dozens of gigabytes of data there's

00:04:32,090 --> 00:04:38,030
a good chance that you can run that on a

00:04:33,560 --> 00:04:39,800
single machine one of the major benefits

00:04:38,030 --> 00:04:41,300
of SPARC though is that it has built-in

00:04:39,800 --> 00:04:43,250
support for multiple languages and

00:04:41,300 --> 00:04:44,780
extensions and so I mentioned some of

00:04:43,250 --> 00:04:46,090
the languages that it supports again

00:04:44,780 --> 00:04:48,620
we're all familiar with Scala

00:04:46,090 --> 00:04:50,660
but it also has built-in libraries for

00:04:48,620 --> 00:04:52,850
things like streaming data and machine

00:04:50,660 --> 00:04:54,620
learning and it's really promoting its

00:04:52,850 --> 00:04:56,840
capabilities for machine learning lately

00:04:54,620 --> 00:04:59,660
an AI the SPARC summit that's coming up

00:04:56,840 --> 00:05:01,160
is like the SPARC and AI summit and so

00:04:59,660 --> 00:05:03,500
they're really really leaning heavily

00:05:01,160 --> 00:05:05,330
into that but as my friend Ryan likes to

00:05:03,500 --> 00:05:06,950
say machine learning is just the carrot

00:05:05,330 --> 00:05:09,350
that you dangle in front of engineers to

00:05:06,950 --> 00:05:10,790
get them to do data engineering and so

00:05:09,350 --> 00:05:13,100
it's written one of those things that in

00:05:10,790 --> 00:05:15,710
order to do that kind of analysis you

00:05:13,100 --> 00:05:17,240
really have to clean your data first you

00:05:15,710 --> 00:05:19,550
have to get your data in a format that's

00:05:17,240 --> 00:05:21,080
going to be useful to you and that's

00:05:19,550 --> 00:05:23,120
where Sparkle is going to come in in

00:05:21,080 --> 00:05:24,710
both fronts and that's one of the things

00:05:23,120 --> 00:05:26,330
that I really appreciate about SPARC is

00:05:24,710 --> 00:05:28,310
even though they are investing in the

00:05:26,330 --> 00:05:29,900
machine learning to lean it also is a

00:05:28,310 --> 00:05:35,090
really good platform for doing the data

00:05:29,900 --> 00:05:36,920
engineering side of things there are two

00:05:35,090 --> 00:05:38,030
main abstractions for SPARC and so these

00:05:36,920 --> 00:05:39,380
are some of the things that you're going

00:05:38,030 --> 00:05:41,930
to be dealing with and every piece of

00:05:39,380 --> 00:05:44,300
smart code that you write we started

00:05:41,930 --> 00:05:47,300
with our Dedes or resilient distributed

00:05:44,300 --> 00:05:49,340
datasets and those are not going away I

00:05:47,300 --> 00:05:51,350
want to make that clear but they're

00:05:49,340 --> 00:05:54,470
moving towards something called data

00:05:51,350 --> 00:05:56,510
frames and data sets and for several

00:05:54,470 --> 00:05:58,310
reasons these are going to be preferred

00:05:56,510 --> 00:05:59,710
nowadays and we'll get into some of

00:05:58,310 --> 00:06:01,970
those reasons in just a little bit

00:05:59,710 --> 00:06:04,400
completely unrelated lis I just have to

00:06:01,970 --> 00:06:06,470
rant about the fact that they were not

00:06:04,400 --> 00:06:09,620
consistent with a camel casing of these

00:06:06,470 --> 00:06:12,020
this is not a typo the F is capitalized

00:06:09,620 --> 00:06:14,030
in the axes lowercase so I can never

00:06:12,020 --> 00:06:17,120
remember which ones which thank God for

00:06:14,030 --> 00:06:19,150
IDs but still it's an inconsistency that

00:06:17,120 --> 00:06:22,190
drives me mad every single day

00:06:19,150 --> 00:06:23,330
the rdd's let's talk a little bit more

00:06:22,190 --> 00:06:24,860
about what those do so they are

00:06:23,330 --> 00:06:27,410
distributed collections with functional

00:06:24,860 --> 00:06:29,960
operators and they have a functional API

00:06:27,410 --> 00:06:31,430
for working with them that's going to be

00:06:29,960 --> 00:06:33,290
very familiar to a lot of Scala

00:06:31,430 --> 00:06:36,169
developers and so you can see how we're

00:06:33,290 --> 00:06:37,900
doing a count with a flat map Map Reduce

00:06:36,169 --> 00:06:39,940
function here

00:06:37,900 --> 00:06:41,620
and the code example that we're using

00:06:39,940 --> 00:06:43,509
here is going to be doing that basic

00:06:41,620 --> 00:06:45,070
reduce method and that looks a lot like

00:06:43,509 --> 00:06:48,639
Scala code even though a lot of these

00:06:45,070 --> 00:06:50,800
ApS are specific to the RDD like I

00:06:48,639 --> 00:06:54,699
mentioned rdd's aren't going away they

00:06:50,800 --> 00:06:56,410
still have their use cases but there was

00:06:54,699 --> 00:06:58,720
some problems that we had with our D DS

00:06:56,410 --> 00:07:00,400
and even though SPARC was easy to use

00:06:58,720 --> 00:07:03,070
people were running into these

00:07:00,400 --> 00:07:05,350
roadblocks occasionally and doing some

00:07:03,070 --> 00:07:06,490
unintentionally very bad things and one

00:07:05,350 --> 00:07:09,009
of the big things that people were doing

00:07:06,490 --> 00:07:11,110
was using methods like group by key and

00:07:09,009 --> 00:07:13,840
it turns out that this is really slow

00:07:11,110 --> 00:07:15,100
and terribly not performant and there's

00:07:13,840 --> 00:07:18,490
usually a better way to get what you

00:07:15,100 --> 00:07:20,380
need but this is from a data breaks best

00:07:18,490 --> 00:07:22,240
practices guide data bricks for those of

00:07:20,380 --> 00:07:24,639
you that aren't familiar is the company

00:07:22,240 --> 00:07:26,710
that kind of maintains the SPARC project

00:07:24,639 --> 00:07:29,410
the original creators of the SPARC

00:07:26,710 --> 00:07:31,990
project that was created at UC Berkeley

00:07:29,410 --> 00:07:34,330
as a graduate level studies project of

00:07:31,990 --> 00:07:36,280
some kind and then they took that put it

00:07:34,330 --> 00:07:38,020
in the Apache Incubator and now have a

00:07:36,280 --> 00:07:40,150
company around it that offers some

00:07:38,020 --> 00:07:42,490
managed services so data bricks is a

00:07:40,150 --> 00:07:44,710
pretty authoritative guide to things

00:07:42,490 --> 00:07:45,940
that you would do with SPARC and this is

00:07:44,710 --> 00:07:48,250
from their best practice this guy that

00:07:45,940 --> 00:07:50,080
they say don't use this API that we gave

00:07:48,250 --> 00:07:51,820
you and so this is one of the faults

00:07:50,080 --> 00:07:54,039
that you see with things like rdd's is

00:07:51,820 --> 00:07:56,409
that they're slow they offer api's that

00:07:54,039 --> 00:07:58,300
might not be the best thing for you and

00:07:56,409 --> 00:07:59,590
the team was very aware of this and they

00:07:58,300 --> 00:08:01,840
were working on ways to make it better

00:07:59,590 --> 00:08:03,820
but it's really easy for programmers to

00:08:01,840 --> 00:08:05,560
fall into this trap especially in a

00:08:03,820 --> 00:08:08,260
language like Scala where you're

00:08:05,560 --> 00:08:10,659
generally working with an IDE and so you

00:08:08,260 --> 00:08:12,220
know you're in IntelliJ you hit that and

00:08:10,659 --> 00:08:13,900
you see this method available and it

00:08:12,220 --> 00:08:16,150
sounds like something really useful and

00:08:13,900 --> 00:08:17,349
there's no information to you that this

00:08:16,150 --> 00:08:20,860
is not something that you might want to

00:08:17,349 --> 00:08:23,260
use so what they are encouraging people

00:08:20,860 --> 00:08:25,510
to do now is instead use this data sets

00:08:23,260 --> 00:08:28,000
abstraction and they started this by

00:08:25,510 --> 00:08:29,800
developing the data frames API and so if

00:08:28,000 --> 00:08:32,140
you're familiar at all with data frames

00:08:29,800 --> 00:08:35,349
in a language like Python pandas or R

00:08:32,140 --> 00:08:37,479
this is going to be very similar to that

00:08:35,349 --> 00:08:39,580
it was modeled on top of that and so

00:08:37,479 --> 00:08:41,830
this is the SPARC equivalent to

00:08:39,580 --> 00:08:45,100
something like that and then data sets

00:08:41,830 --> 00:08:47,170
were developed on top of that for Scala

00:08:45,100 --> 00:08:50,079
and Java specifically to offer a

00:08:47,170 --> 00:08:50,570
strongly typed API on top of the data

00:08:50,079 --> 00:08:54,110
frames

00:08:50,570 --> 00:08:56,330
API and so this abstraction layer we all

00:08:54,110 --> 00:08:57,950
love types in our skull programs and so

00:08:56,330 --> 00:09:00,680
this is going to offer you some compile

00:08:57,950 --> 00:09:02,030
time safety in addition to some of the

00:09:00,680 --> 00:09:05,180
speed benefits that you get with the

00:09:02,030 --> 00:09:07,250
data frames abstraction and this API

00:09:05,180 --> 00:09:09,740
specifically works with structured or

00:09:07,250 --> 00:09:12,380
semi structured data and it contains a

00:09:09,740 --> 00:09:14,750
lot of optimizations that make the data

00:09:12,380 --> 00:09:18,170
on the operations on it much faster than

00:09:14,750 --> 00:09:20,680
if you're working with rdd's and because

00:09:18,170 --> 00:09:22,400
data sets effectively require a schema

00:09:20,680 --> 00:09:24,470
depending on what you're doing they're

00:09:22,400 --> 00:09:25,880
about two times faster than rdd's which

00:09:24,470 --> 00:09:28,430
is pretty big if you're doing a lot of

00:09:25,880 --> 00:09:29,750
operations across your data so if you're

00:09:28,430 --> 00:09:31,400
working with any kind of structured data

00:09:29,750 --> 00:09:34,040
you definitely want to be looking at the

00:09:31,400 --> 00:09:36,260
data frames or the data sets API they

00:09:34,040 --> 00:09:38,180
also provide a sequel or a sequel like

00:09:36,260 --> 00:09:39,860
DSL and you can see that in this code

00:09:38,180 --> 00:09:41,600
example we're doing a group by method

00:09:39,860 --> 00:09:43,010
and a count on this and so that's going

00:09:41,600 --> 00:09:45,620
to be familiar if you're familiar with

00:09:43,010 --> 00:09:47,540
sequel which sequel is pretty much the

00:09:45,620 --> 00:09:49,310
universal data processing language all

00:09:47,540 --> 00:09:50,600
the best data scientists I know are

00:09:49,310 --> 00:09:54,140
really good with sequel whether they

00:09:50,600 --> 00:09:56,870
want to be or not and you can even use

00:09:54,140 --> 00:09:58,040
raw sequel on top of a spark which is

00:09:56,870 --> 00:10:01,700
really cool and I'm going to show you

00:09:58,040 --> 00:10:03,380
that a little bit later so you'll

00:10:01,700 --> 00:10:05,810
remember this ecosystem the fact that

00:10:03,380 --> 00:10:09,200
SPARC supports many languages including

00:10:05,810 --> 00:10:10,610
languages like RN Python for this

00:10:09,200 --> 00:10:12,620
audience though I do want to point out

00:10:10,610 --> 00:10:14,210
that Scala still has the most robust

00:10:12,620 --> 00:10:16,130
language API I probably don't need to

00:10:14,210 --> 00:10:18,440
convince you of this or that you need to

00:10:16,130 --> 00:10:20,570
use Scala but it's way faster when

00:10:18,440 --> 00:10:22,940
you're dealing with rdd's there's a huge

00:10:20,570 --> 00:10:24,650
performance gain there it has strongly

00:10:22,940 --> 00:10:26,300
typed datasets while something like

00:10:24,650 --> 00:10:28,220
Python giving that it's a dynamically

00:10:26,300 --> 00:10:30,740
typed language is pretty much only

00:10:28,220 --> 00:10:33,110
always going to have the data frames

00:10:30,740 --> 00:10:35,540
I know Python 3 is adding some types but

00:10:33,110 --> 00:10:37,370
I think that the data sets API is still

00:10:35,540 --> 00:10:39,620
going to be most beneficial in languages

00:10:37,370 --> 00:10:41,450
like Scala and it's also almost all

00:10:39,620 --> 00:10:42,860
Scala under the hood for SPARC and so if

00:10:41,450 --> 00:10:44,330
you really need to dig into the source

00:10:42,860 --> 00:10:46,670
code which I've needed to do

00:10:44,330 --> 00:10:48,950
occasionally you're going to be looking

00:10:46,670 --> 00:10:50,660
at spark or Scala code under the hood if

00:10:48,950 --> 00:10:52,490
you need to extend the language at all

00:10:50,660 --> 00:10:56,990
at the library there's ways that you can

00:10:52,490 --> 00:10:58,820
do that with Scala so one of the big

00:10:56,990 --> 00:11:00,800
disadvantages it early on is that Python

00:10:58,820 --> 00:11:02,510
was just so much slower than Scala and

00:11:00,800 --> 00:11:04,010
so you would have a lot of data engineer

00:11:02,510 --> 00:11:06,199
and python engineers that would learn

00:11:04,010 --> 00:11:06,860
Scala just to improve the speed of their

00:11:06,199 --> 00:11:09,050
programs

00:11:06,860 --> 00:11:11,029
dean Wampler if you're familiar with him

00:11:09,050 --> 00:11:13,730
has a very popular tutorial that's

00:11:11,029 --> 00:11:15,680
called just enough Scala for SPARC and I

00:11:13,730 --> 00:11:17,420
think that's really great it's been the

00:11:15,680 --> 00:11:19,370
gateway drug for a lot of people to get

00:11:17,420 --> 00:11:21,649
into Scala I think there's a lot of

00:11:19,370 --> 00:11:22,699
people that learn just enough Scala so

00:11:21,649 --> 00:11:24,350
that they could do the things that they

00:11:22,699 --> 00:11:26,690
need because then you don't have to get

00:11:24,350 --> 00:11:28,940
into SPARC engine optimization tuning

00:11:26,690 --> 00:11:30,980
which trust me if you can avoid SPARC

00:11:28,940 --> 00:11:33,170
engine optimization tuning you want to

00:11:30,980 --> 00:11:35,810
sometimes learning Scala is easier than

00:11:33,170 --> 00:11:37,399
learning how to tune spark which may say

00:11:35,810 --> 00:11:39,949
something about the the program itself

00:11:37,399 --> 00:11:41,510
but one of the interesting things that's

00:11:39,949 --> 00:11:44,570
good to point out here is that with the

00:11:41,510 --> 00:11:46,790
dataframes API Python is just as fast as

00:11:44,570 --> 00:11:47,930
Scala and so moving forward this is

00:11:46,790 --> 00:11:49,730
going to be really interesting to see

00:11:47,930 --> 00:11:51,920
how the language continues to develop

00:11:49,730 --> 00:11:54,380
because there's so many optimizations

00:11:51,920 --> 00:11:56,779
and Python now that there's not that

00:11:54,380 --> 00:12:00,589
same motivation to move to Scala as

00:11:56,779 --> 00:12:01,970
there was previously and so I thought

00:12:00,589 --> 00:12:03,949
the street was really interesting and

00:12:01,970 --> 00:12:06,470
it's in reference to improving

00:12:03,949 --> 00:12:09,740
accessibility of SPARC and Scala for

00:12:06,470 --> 00:12:11,540
data scientists and Cameron you know

00:12:09,740 --> 00:12:13,699
says there's a lot of low-hanging fruit

00:12:11,540 --> 00:12:15,949
here for data visualization plotting

00:12:13,699 --> 00:12:17,959
libraries and he's right that Python and

00:12:15,949 --> 00:12:20,029
R are lightyears ahead when it comes to

00:12:17,959 --> 00:12:23,240
this and the default language is for

00:12:20,029 --> 00:12:24,709
people like you know biologists or any

00:12:23,240 --> 00:12:27,199
type of other person that's doing a lot

00:12:24,709 --> 00:12:29,420
of computational data analysis a lot of

00:12:27,199 --> 00:12:31,790
times they go to our Python and I think

00:12:29,420 --> 00:12:33,560
in order for Scala to be an accessible

00:12:31,790 --> 00:12:35,360
alternative to that we really have to

00:12:33,560 --> 00:12:36,889
work on the tooling and make it a lot

00:12:35,360 --> 00:12:38,060
more accessible with the visualizations

00:12:36,889 --> 00:12:40,940
and the type of things that we're

00:12:38,060 --> 00:12:42,620
offering to that community so I'm really

00:12:40,940 --> 00:12:44,990
excited and interested to see where the

00:12:42,620 --> 00:12:47,360
data community takes this even as a

00:12:44,990 --> 00:12:49,010
Scala engineer and all the benefits that

00:12:47,360 --> 00:12:51,079
I just mentioned I have a hunch that

00:12:49,010 --> 00:12:53,420
Python might win out here just because

00:12:51,079 --> 00:12:55,880
they had such a huge head start with

00:12:53,420 --> 00:12:58,639
this and matei who is the creator of

00:12:55,880 --> 00:13:01,190
SPARC at last year's Scala by the bay

00:12:58,639 --> 00:13:03,290
his key know about SPARC at a Scala

00:13:01,190 --> 00:13:05,240
conference he all of his code samples

00:13:03,290 --> 00:13:08,510
were in Python and so I think that might

00:13:05,240 --> 00:13:10,550
tell you something so that's some of the

00:13:08,510 --> 00:13:12,260
basics of SPARC and where we're at with

00:13:10,550 --> 00:13:14,420
the project today I wanted to spend just

00:13:12,260 --> 00:13:15,949
a couple of minutes talking about the

00:13:14,420 --> 00:13:16,360
state of password since it's going to

00:13:15,949 --> 00:13:19,630
info

00:13:16,360 --> 00:13:21,220
our data analysis like I mentioned I

00:13:19,630 --> 00:13:23,920
spend a lot of time thinking about

00:13:21,220 --> 00:13:25,630
authentication and we have a big problem

00:13:23,920 --> 00:13:28,149
and that's that people reuse passwords

00:13:25,630 --> 00:13:31,269
and they use passwords that are short

00:13:28,149 --> 00:13:32,740
and easy to guess and the problem is is

00:13:31,269 --> 00:13:34,779
like even if you don't care if your

00:13:32,740 --> 00:13:36,610
myspace account gets hacked if you were

00:13:34,779 --> 00:13:38,560
using the same set of credentials as you

00:13:36,610 --> 00:13:40,870
were for your myspace account as you are

00:13:38,560 --> 00:13:42,459
for your email or your bank account

00:13:40,870 --> 00:13:43,899
you're gonna have a bad time

00:13:42,459 --> 00:13:45,940
and you're gonna have a bad time because

00:13:43,899 --> 00:13:47,620
hackers like this guy and you know this

00:13:45,940 --> 00:13:49,329
guy is legit because on top of all that

00:13:47,620 --> 00:13:52,660
encrypted text you got the word hacked

00:13:49,329 --> 00:13:54,279
to read I love stock photos of hackers

00:13:52,660 --> 00:13:58,360
trust me and Google it you won't regret

00:13:54,279 --> 00:14:00,190
it but guys like this will buy your

00:13:58,360 --> 00:14:01,570
leaked credentials and implement them

00:14:00,190 --> 00:14:04,180
across the web and a practice known as

00:14:01,570 --> 00:14:05,589
credential stuffing and people are still

00:14:04,180 --> 00:14:07,180
doing this because they can make it a

00:14:05,589 --> 00:14:08,230
lot of money doing this you know they

00:14:07,180 --> 00:14:10,510
can transfer money out of your bank

00:14:08,230 --> 00:14:12,670
accounts they can get access to your

00:14:10,510 --> 00:14:13,990
email and buy things for themselves and

00:14:12,670 --> 00:14:16,990
all of that fun stuff that you don't

00:14:13,990 --> 00:14:18,640
want to happen and so the reason that we

00:14:16,990 --> 00:14:20,260
have a problem with passwords is because

00:14:18,640 --> 00:14:21,640
this credential stuffing is so common

00:14:20,260 --> 00:14:25,180
and people are using and reusing

00:14:21,640 --> 00:14:26,620
passwords all over the place you might

00:14:25,180 --> 00:14:27,730
think that nobody does this nobody

00:14:26,620 --> 00:14:29,709
writes a password on a sticky note

00:14:27,730 --> 00:14:32,589
nobody has a password as silly as one

00:14:29,709 --> 00:14:35,589
two three four five six but of course

00:14:32,589 --> 00:14:37,690
there are people that still do in fact

00:14:35,589 --> 00:14:40,560
this password one two three four five

00:14:37,690 --> 00:14:42,850
six has been seen over 20 million times

00:14:40,560 --> 00:14:44,890
this is according to the site have I

00:14:42,850 --> 00:14:47,380
been owned a project from security

00:14:44,890 --> 00:14:50,199
researcher Troy hunt sidenote

00:14:47,380 --> 00:14:52,420
pwned with a P is the hacker way of

00:14:50,199 --> 00:14:56,199
spelling owned and if you keep the piece

00:14:52,420 --> 00:14:58,329
silent people will think you're cool so

00:14:56,199 --> 00:15:00,190
Troy hunt has access to password data

00:14:58,329 --> 00:15:02,140
from hundreds of password breaches and

00:15:00,190 --> 00:15:03,760
he provides an interface for seeing if

00:15:02,140 --> 00:15:05,500
your password has been included or

00:15:03,760 --> 00:15:07,390
password or your email has been included

00:15:05,500 --> 00:15:09,190
in one of those data breaches so I

00:15:07,390 --> 00:15:10,480
encourage you to check out the site you

00:15:09,190 --> 00:15:12,370
might find some things that you need to

00:15:10,480 --> 00:15:13,630
change and this is the data that I'm

00:15:12,370 --> 00:15:22,440
going to be using today to show off

00:15:13,630 --> 00:15:22,440
Apache spark so let me just switch over

00:15:28,100 --> 00:15:32,580
oops

00:15:29,339 --> 00:15:34,020
okay so I'm using a tool called Zeppelin

00:15:32,580 --> 00:15:35,850
here Zeppelin is kind of like a Jupiter

00:15:34,020 --> 00:15:38,279
notebook it has a built in spark

00:15:35,850 --> 00:15:40,080
interpreter though and so Zeppelin is a

00:15:38,279 --> 00:15:41,580
really good way to set up spark locally

00:15:40,080 --> 00:15:43,830
on your machine it's also a really good

00:15:41,580 --> 00:15:45,420
way for me to demo things for you today

00:15:43,830 --> 00:15:46,500
I'm running this on localhost so

00:15:45,420 --> 00:15:49,709
hopefully I won't have to rely on

00:15:46,500 --> 00:15:51,180
conference Wi-Fi like I said it has the

00:15:49,709 --> 00:15:52,860
built-in spark interpreter great for

00:15:51,180 --> 00:15:54,660
ad-hoc analysis it has some built in

00:15:52,860 --> 00:15:56,790
visualizations and so that's one of the

00:15:54,660 --> 00:15:58,410
things that I think Scala and spark is

00:15:56,790 --> 00:16:00,959
lacking and this starts to solve that

00:15:58,410 --> 00:16:02,580
problem if you've ever used data bricks

00:16:00,959 --> 00:16:04,770
Zeppelin is kind of like an open-source

00:16:02,580 --> 00:16:07,589
alternative to that data bricks is not

00:16:04,770 --> 00:16:09,540
cheap and so for my purposes I was using

00:16:07,589 --> 00:16:10,890
the open-source alternative if anybody

00:16:09,540 --> 00:16:13,920
here works at data bricks and wants to

00:16:10,890 --> 00:16:16,200
give me credit come find me and for the

00:16:13,920 --> 00:16:17,910
sake of time I've already done some an

00:16:16,200 --> 00:16:20,550
hour I've already done some joining of

00:16:17,910 --> 00:16:23,100
the data so what is in the the own

00:16:20,550 --> 00:16:25,110
password data is hashes of passwords

00:16:23,100 --> 00:16:26,670
because it turns out that people store a

00:16:25,110 --> 00:16:28,770
lot of personal information in their

00:16:26,670 --> 00:16:30,300
passwords and so Troy hunt didn't want

00:16:28,770 --> 00:16:33,240
to make those all accessible and a

00:16:30,300 --> 00:16:36,180
downloadable file and so it's a sha-1

00:16:33,240 --> 00:16:37,589
hash of passwords and a count of the

00:16:36,180 --> 00:16:39,870
number of times that that password has

00:16:37,589 --> 00:16:41,520
been seen in a data breach because

00:16:39,870 --> 00:16:44,160
there's only so much interesting things

00:16:41,520 --> 00:16:45,510
we can look at with sha-1 hashes I've

00:16:44,160 --> 00:16:48,000
gone ahead and joined that with some

00:16:45,510 --> 00:16:49,950
known common passwords and so I think we

00:16:48,000 --> 00:16:51,899
have about 700 passwords and so I'm

00:16:49,950 --> 00:16:54,720
gonna go ahead and read those in these

00:16:51,899 --> 00:16:58,230
are in a CSV file and so we can go ahead

00:16:54,720 --> 00:17:00,060
and grab that and we'll go ahead and

00:16:58,230 --> 00:17:01,890
count the number of those that there are

00:17:00,060 --> 00:17:05,010
like I said I think there should be

00:17:01,890 --> 00:17:07,319
about 700,000 go ahead and save that to

00:17:05,010 --> 00:17:09,240
a variable you'll notice here that spark

00:17:07,319 --> 00:17:13,309
has a built in CSV reader which is

00:17:09,240 --> 00:17:16,020
pretty handy and so we can use that to

00:17:13,309 --> 00:17:17,790
go ahead and read in this data once I

00:17:16,020 --> 00:17:20,429
print the schema you'll see that it has

00:17:17,790 --> 00:17:22,589
some columns it's already interpreted

00:17:20,429 --> 00:17:25,050
those columns to be these you know built

00:17:22,589 --> 00:17:27,209
in variable names but the interesting

00:17:25,050 --> 00:17:29,490
thing is that we have a header on this

00:17:27,209 --> 00:17:32,429
file we just have to tell SPARC that it

00:17:29,490 --> 00:17:33,990
needs to look for that so it's just one

00:17:32,429 --> 00:17:36,000
line we can go ahead and get the head

00:17:33,990 --> 00:17:37,920
names that are there and then we have a

00:17:36,000 --> 00:17:39,900
data frame that has this information in

00:17:37,920 --> 00:17:42,510
the headers already and so we can start

00:17:39,900 --> 00:17:44,429
to do analysis on that the other thing

00:17:42,510 --> 00:17:48,809
that you might notice is that our count

00:17:44,429 --> 00:17:50,880
here is a string and counts you know

00:17:48,809 --> 00:17:53,670
usually our numeric and so we want to

00:17:50,880 --> 00:17:56,250
tell it to infer the schema and then

00:17:53,670 --> 00:17:58,800
that'll allow us to have the correct

00:17:56,250 --> 00:18:01,080
types that we want and so now we have a

00:17:58,800 --> 00:18:02,760
count as an integer this is pretty cool

00:18:01,080 --> 00:18:05,520
so we've already got some interesting

00:18:02,760 --> 00:18:08,309
data frame to look at before I show you

00:18:05,520 --> 00:18:10,530
the contents of that data I have to do

00:18:08,309 --> 00:18:12,510
something real quick and that's because

00:18:10,530 --> 00:18:14,250
people put a lot of bad words and their

00:18:12,510 --> 00:18:15,450
passwords this is a professional

00:18:14,250 --> 00:18:18,450
environment I don't want those things

00:18:15,450 --> 00:18:22,200
showing up so we'll go ahead and read in

00:18:18,450 --> 00:18:23,550
those this is just from a text file that

00:18:22,200 --> 00:18:26,760
I have I'm not going to show you what's

00:18:23,550 --> 00:18:30,660
in here you can use your imagination so

00:18:26,760 --> 00:18:34,580
we can go ahead and count these so this

00:18:30,660 --> 00:18:38,910
is a we wanted to count those and then

00:18:34,580 --> 00:18:42,360
count our bad words and I think there's

00:18:38,910 --> 00:18:44,429
about 65 yeah 67 bad words and you'll

00:18:42,360 --> 00:18:45,990
see there's 700,000 passwords

00:18:44,429 --> 00:18:48,210
let's join these together and see how

00:18:45,990 --> 00:18:50,160
many passwords we get rid of so we can

00:18:48,210 --> 00:18:51,660
go ahead and do the join like you would

00:18:50,160 --> 00:18:54,480
in a sequel and so we're using the

00:18:51,660 --> 00:18:56,100
sequel like DSL to do this and so we can

00:18:54,480 --> 00:18:58,800
join our bad words and we're going to do

00:18:56,100 --> 00:19:04,140
that on our if our password column

00:18:58,800 --> 00:19:05,460
contains any of these bad words and so

00:19:04,140 --> 00:19:07,620
we're gonna be looking for the value

00:19:05,460 --> 00:19:09,750
value is the default column name and so

00:19:07,620 --> 00:19:11,910
for this bad words data set you'll see

00:19:09,750 --> 00:19:13,590
that this is a data set of string and so

00:19:11,910 --> 00:19:15,690
it has one column and it interpreted

00:19:13,590 --> 00:19:17,160
that to be a string value and then the

00:19:15,690 --> 00:19:18,929
very important thing is that we want to

00:19:17,160 --> 00:19:20,700
tell it that this is a left

00:19:18,929 --> 00:19:22,080
Antti join which means leave out

00:19:20,700 --> 00:19:24,720
everything that's in this thing that

00:19:22,080 --> 00:19:27,150
we're joining it with and so now if we

00:19:24,720 --> 00:19:31,350
do this count hopefully we can see that

00:19:27,150 --> 00:19:33,630
there are about 20,000 less passwords

00:19:31,350 --> 00:19:35,280
and the data set we were working with so

00:19:33,630 --> 00:19:38,730
that's kind of interesting do with that

00:19:35,280 --> 00:19:40,410
information what you will so now that we

00:19:38,730 --> 00:19:42,570
have this we can go ahead and show our

00:19:40,410 --> 00:19:45,419
most common passwords and we're going to

00:19:42,570 --> 00:19:48,500
be doing this by ordering by the

00:19:45,419 --> 00:19:48,500
descending count

00:19:48,749 --> 00:19:53,830
and we have a built-in method called

00:19:51,879 --> 00:19:56,649
show which will do this in a table view

00:19:53,830 --> 00:19:58,989
so that we can see the passwords the

00:19:56,649 --> 00:20:01,840
most common passwords and so this is

00:19:58,989 --> 00:20:06,129
exactly what you would expect numeric

00:20:01,840 --> 00:20:09,190
sequences the word QWERTY abc123 all

00:20:06,129 --> 00:20:13,809
these very obvious passwords I love you

00:20:09,190 --> 00:20:15,519
oh that's nice but still you know if you

00:20:13,809 --> 00:20:17,679
have any of these your passwords maybe

00:20:15,519 --> 00:20:19,389
it's okay I think the the common example

00:20:17,679 --> 00:20:21,220
that I hear is if it's your Pizza login

00:20:19,389 --> 00:20:23,499
maybe you don't care if anybody gets

00:20:21,220 --> 00:20:24,999
access to that but you can also think

00:20:23,499 --> 00:20:26,559
about the fact that even with your Pizza

00:20:24,999 --> 00:20:29,259
login somebody can get access to your

00:20:26,559 --> 00:20:31,119
home address somebody can get access to

00:20:29,259 --> 00:20:34,659
your phone number and those are things

00:20:31,119 --> 00:20:37,269
that people can use to hack you so keep

00:20:34,659 --> 00:20:39,070
your password strong that's my one thing

00:20:37,269 --> 00:20:40,029
that I want to leave you with here so

00:20:39,070 --> 00:20:42,039
the other thing that I want to show you

00:20:40,029 --> 00:20:43,840
here is so there are our data frame for

00:20:42,039 --> 00:20:46,480
passwords is still a data frame it's not

00:20:43,840 --> 00:20:48,070
our strongly typed data set and so we

00:20:46,480 --> 00:20:50,950
can go ahead and make that a data set

00:20:48,070 --> 00:20:52,749
and by declaring a case class and so

00:20:50,950 --> 00:20:54,570
I'll name is like password hash and

00:20:52,749 --> 00:20:58,480
we're gonna have a password which is a

00:20:54,570 --> 00:21:00,759
string and the hash is also going to be

00:20:58,480 --> 00:21:03,789
a string and then our count is going to

00:21:00,759 --> 00:21:05,710
be an int and then all you need to do in

00:21:03,789 --> 00:21:11,649
order to tell it that this is the type

00:21:05,710 --> 00:21:13,389
that you want is declare that and then

00:21:11,649 --> 00:21:16,090
once we have this we'll have a strongly

00:21:13,389 --> 00:21:17,710
typed data set of password hash and this

00:21:16,090 --> 00:21:19,710
is useful because then we can go ahead

00:21:17,710 --> 00:21:24,970
and do things like filter on the types

00:21:19,710 --> 00:21:28,480
so we could filter on anything that has

00:21:24,970 --> 00:21:29,889
a count greater than 5,000 and this is

00:21:28,480 --> 00:21:31,149
going to be really useful depending on

00:21:29,889 --> 00:21:34,119
the type of analysis that you're doing

00:21:31,149 --> 00:21:36,609
because this is going to start catching

00:21:34,119 --> 00:21:38,499
things at compile time instead of

00:21:36,609 --> 00:21:39,820
catching things at runtime and so the

00:21:38,499 --> 00:21:42,909
alternative for doing something like

00:21:39,820 --> 00:21:45,720
this in spark if you don't have the data

00:21:42,909 --> 00:21:48,369
set as you're doing that kind of column

00:21:45,720 --> 00:21:51,309
access that you saw me do with this this

00:21:48,369 --> 00:21:53,169
notation and so that's how you would

00:21:51,309 --> 00:21:56,019
access a column and so you can also do

00:21:53,169 --> 00:21:57,879
things like you could do a where Clause

00:21:56,019 --> 00:21:59,380
on this and so you can write some raw

00:21:57,879 --> 00:22:01,300
sequel in here

00:21:59,380 --> 00:22:03,520
so you can do where count is greater

00:22:01,300 --> 00:22:06,520
than 5000 but if you accidentally

00:22:03,520 --> 00:22:08,950
misspelled that as ko nut you know

00:22:06,520 --> 00:22:10,210
you're going to get a runtime error when

00:22:08,950 --> 00:22:12,610
you're running this in your code as

00:22:10,210 --> 00:22:14,200
opposed to doing it you know in with

00:22:12,610 --> 00:22:16,630
this method you would get the the

00:22:14,200 --> 00:22:17,950
compile time safety and so that's one of

00:22:16,630 --> 00:22:20,140
the things that is the benefit of doing

00:22:17,950 --> 00:22:22,660
having data sets in your scholar code

00:22:20,140 --> 00:22:24,400
for stuff like that all right so that's

00:22:22,660 --> 00:22:25,840
we've got our data loaded in and now we

00:22:24,400 --> 00:22:27,340
can look at some of the things with the

00:22:25,840 --> 00:22:29,470
data one of the things that I want to

00:22:27,340 --> 00:22:30,670
look at here is the most common lengths

00:22:29,470 --> 00:22:32,620
of passwords because I think that's

00:22:30,670 --> 00:22:34,660
pretty interesting and we're gonna do

00:22:32,620 --> 00:22:37,060
something first to give all the most

00:22:34,660 --> 00:22:39,340
common links to our data and so I'm

00:22:37,060 --> 00:22:41,740
gonna go ahead and add a column you can

00:22:39,340 --> 00:22:44,440
do this with spark would spark sequel

00:22:41,740 --> 00:22:46,240
and so I'm gonna call this column Len

00:22:44,440 --> 00:22:49,030
and then we can use the built in

00:22:46,240 --> 00:22:52,480
function for length to get our length of

00:22:49,030 --> 00:22:55,420
our password column and then we can go

00:22:52,480 --> 00:22:57,190
ahead and add that and then once we have

00:22:55,420 --> 00:22:59,580
this we can do something cool which is

00:22:57,190 --> 00:23:02,560
running raw sequel on top of our data

00:22:59,580 --> 00:23:04,360
let me make that a little bit bigger and

00:23:02,560 --> 00:23:06,640
so what you can do here is write the

00:23:04,360 --> 00:23:09,100
sequel like you would expect to and so

00:23:06,640 --> 00:23:11,950
I'm going to select our length the sum

00:23:09,100 --> 00:23:13,600
of our counts and we're gonna be

00:23:11,950 --> 00:23:18,040
selecting this from our passwords table

00:23:13,600 --> 00:23:22,690
and then we're going to group by an

00:23:18,040 --> 00:23:24,760
order by our length and that's because I

00:23:22,690 --> 00:23:31,210
didn't save this and so you have to

00:23:24,760 --> 00:23:34,960
create a temp you and give it a name let

00:23:31,210 --> 00:23:37,180
me put that on the new line and this is

00:23:34,960 --> 00:23:39,430
to help it do some indexing so that you

00:23:37,180 --> 00:23:41,320
can run the raw sequel query on top of

00:23:39,430 --> 00:23:43,240
it and so this is being done with

00:23:41,320 --> 00:23:44,770
Zeppelin and the spark engine and so now

00:23:43,240 --> 00:23:46,870
we can run this and it will give us some

00:23:44,770 --> 00:23:49,210
information about what our most common

00:23:46,870 --> 00:23:50,860
lengths are and the really cool thing

00:23:49,210 --> 00:23:53,290
about this is that it comes with some

00:23:50,860 --> 00:23:55,510
built-in visualizations and so you can

00:23:53,290 --> 00:23:57,490
see things here like it offers a bar

00:23:55,510 --> 00:23:58,720
chart a pie chart

00:23:57,490 --> 00:24:01,090
I don't know when pie charts are ever

00:23:58,720 --> 00:24:02,530
good but you can think when scatter

00:24:01,090 --> 00:24:04,300
plots might be necessary

00:24:02,530 --> 00:24:05,800
I think our bar chart for this data is

00:24:04,300 --> 00:24:07,840
pretty interesting you can see that our

00:24:05,800 --> 00:24:09,760
six and eight character passwords are

00:24:07,840 --> 00:24:11,890
the most common this makes sense to me

00:24:09,760 --> 00:24:12,590
since a lot of password minimums are six

00:24:11,890 --> 00:24:14,000
and eight care

00:24:12,590 --> 00:24:17,450
and so that's where you're going to see

00:24:14,000 --> 00:24:19,700
a lot of people adding in those password

00:24:17,450 --> 00:24:21,440
lengths some other cool things that you

00:24:19,700 --> 00:24:24,340
can do with this is you can add in where

00:24:21,440 --> 00:24:27,950
clauses so you could do where password

00:24:24,340 --> 00:24:30,470
like well do Berlin

00:24:27,950 --> 00:24:32,029
and so you could do this like with all

00:24:30,470 --> 00:24:33,200
the Rossy qulity you could imagine but

00:24:32,029 --> 00:24:34,909
then one of my favorite things about

00:24:33,200 --> 00:24:38,659
Zeppelin is that it does this cool thing

00:24:34,909 --> 00:24:40,669
where you can do these placeholders and

00:24:38,659 --> 00:24:42,020
so you basically give it a variable name

00:24:40,669 --> 00:24:43,909
and then you have this password field

00:24:42,020 --> 00:24:45,590
that comes up and then you could give

00:24:43,909 --> 00:24:47,120
this report to somebody on your you know

00:24:45,590 --> 00:24:48,710
management team or somebody that doesn't

00:24:47,120 --> 00:24:50,390
want to write any code or doesn't know

00:24:48,710 --> 00:24:52,159
sequel and then you could have the enum

00:24:50,390 --> 00:24:55,880
then replace this with something like

00:24:52,159 --> 00:24:57,350
Kelly or Scala a lot of people that use

00:24:55,880 --> 00:24:59,240
Kelly in their passwords it's very

00:24:57,350 --> 00:25:01,010
exciting I don't think those eight

00:24:59,240 --> 00:25:02,690
character passwords are the word Scala

00:25:01,010 --> 00:25:05,120
unless all of you are writing Scala one

00:25:02,690 --> 00:25:06,919
two three is your passwords all 8000 of

00:25:05,120 --> 00:25:08,630
you I think that's pretty pretty

00:25:06,919 --> 00:25:11,380
interesting I bet those are all Martin's

00:25:08,630 --> 00:25:13,549
passwords don't tell them I said that

00:25:11,380 --> 00:25:14,750
but yeah so that's pretty interesting I

00:25:13,549 --> 00:25:16,549
think this goes back to some of the

00:25:14,750 --> 00:25:19,669
really great things that we want to

00:25:16,549 --> 00:25:21,980
encourage about about our data

00:25:19,669 --> 00:25:23,779
processing and tools moving forward is

00:25:21,980 --> 00:25:26,149
that these visualization tools for any

00:25:23,779 --> 00:25:28,580
kind of data processing are super super

00:25:26,149 --> 00:25:31,580
awesome and as much as we can continue

00:25:28,580 --> 00:25:33,320
to develop these we should alright so

00:25:31,580 --> 00:25:35,029
we've looked at the length of passwords

00:25:33,320 --> 00:25:36,770
one of the other things that I want to

00:25:35,029 --> 00:25:38,960
take a look at is kind of the contents

00:25:36,770 --> 00:25:40,010
of the passwords themselves and there's

00:25:38,960 --> 00:25:42,830
a couple of different ways you can look

00:25:40,010 --> 00:25:44,720
at this I think looking at proper nouns

00:25:42,830 --> 00:25:46,429
is a really obvious thing that you can

00:25:44,720 --> 00:25:49,760
attack and you could do that with like

00:25:46,429 --> 00:25:51,860
celebrity names or geo locations I'm

00:25:49,760 --> 00:25:55,429
gonna look at doc names because I love

00:25:51,860 --> 00:25:58,610
dogs and so I have a JSON file with some

00:25:55,429 --> 00:26:04,340
common dog names and we'll go ahead and

00:25:58,610 --> 00:26:06,169
read that in and so this will have some

00:26:04,340 --> 00:26:09,649
information in it I forgot to save this

00:26:06,169 --> 00:26:11,419
file so this is the the dog JSON file

00:26:09,649 --> 00:26:13,549
that we have and I think this is really

00:26:11,419 --> 00:26:16,190
interesting because I have this one line

00:26:13,549 --> 00:26:18,409
that has chicken or Max's favorite food

00:26:16,190 --> 00:26:21,620
in there and so just because that one

00:26:18,409 --> 00:26:24,919
line has food in it it realized that the

00:26:21,620 --> 00:26:26,000
schema had that property and so it

00:26:24,919 --> 00:26:27,710
inferred that

00:26:26,000 --> 00:26:30,710
had those three columns and if I take

00:26:27,710 --> 00:26:32,620
that out it's a corrupt record because

00:26:30,710 --> 00:26:35,540
SPARC expects used to all be on one line

00:26:32,620 --> 00:26:37,910
so if I take that out and rerun it it'll

00:26:35,540 --> 00:26:40,010
know that there's only two keys in there

00:26:37,910 --> 00:26:41,600
that we're expecting and this is really

00:26:40,010 --> 00:26:43,460
useful because especially if you're

00:26:41,600 --> 00:26:45,950
dealing with hundreds of millions of log

00:26:43,460 --> 00:26:48,800
lines every hour whatever your data

00:26:45,950 --> 00:26:50,540
scale is it's really hard to have any

00:26:48,800 --> 00:26:52,460
kind of idea about the consistency of

00:26:50,540 --> 00:26:53,840
what's in that data and so this is one

00:26:52,460 --> 00:26:55,760
of the really brilliant ways that spark

00:26:53,840 --> 00:26:57,320
comes up with to do that data inference

00:26:55,760 --> 00:26:59,150
and the schema inference for you

00:26:57,320 --> 00:27:00,740
you don't have to worry about that as

00:26:59,150 --> 00:27:02,270
much as you otherwise would have and

00:27:00,740 --> 00:27:04,610
maybe you're still going to have your

00:27:02,270 --> 00:27:06,800
own encoders and decoders that you can

00:27:04,610 --> 00:27:08,660
perform business logic and have some

00:27:06,800 --> 00:27:10,160
idea of your expectations of the data

00:27:08,660 --> 00:27:12,620
but this is also a really interesting

00:27:10,160 --> 00:27:15,650
way that you can help inform what those

00:27:12,620 --> 00:27:17,930
encoders contain so you can build those

00:27:15,650 --> 00:27:20,750
more smartly and maybe you get some

00:27:17,930 --> 00:27:23,600
error identification or notifications

00:27:20,750 --> 00:27:25,640
and in your tool set about if you're

00:27:23,600 --> 00:27:28,730
getting data back that you didn't expect

00:27:25,640 --> 00:27:30,470
other than just suppressing those so I

00:27:28,730 --> 00:27:31,820
think that's pretty cool now that we

00:27:30,470 --> 00:27:33,790
have our dogs we can go ahead and see

00:27:31,820 --> 00:27:36,020
how many passwords contain our dog names

00:27:33,790 --> 00:27:38,810
so let's go ahead and join those

00:27:36,020 --> 00:27:40,430
together again and this is going to look

00:27:38,810 --> 00:27:42,740
pretty similar to the join that we did

00:27:40,430 --> 00:27:49,520
before we're going to look at if our

00:27:42,740 --> 00:27:50,690
password contains the dog name but this

00:27:49,520 --> 00:27:52,580
time we're not going to give it a

00:27:50,690 --> 00:27:54,530
specific join type because the default

00:27:52,580 --> 00:27:57,890
join is going to be an inner join and

00:27:54,530 --> 00:27:59,600
that's what we want to do and so any

00:27:57,890 --> 00:28:06,680
guesses what the most popular dog names

00:27:59,600 --> 00:28:08,420
and passwords are we'll go ahead and

00:28:06,680 --> 00:28:09,620
order that by descending count and I'm

00:28:08,420 --> 00:28:11,900
just going to go ahead and select the

00:28:09,620 --> 00:28:18,110
fields that we care about which is the

00:28:11,900 --> 00:28:21,010
password the name and the count and

00:28:18,110 --> 00:28:21,010
we'll go ahead and show that

00:28:21,430 --> 00:28:28,070
and so Charlie Charlie is our most loved

00:28:25,130 --> 00:28:29,270
dog name apparently that's that's good

00:28:28,070 --> 00:28:32,030
for Charlie probably not good for

00:28:29,270 --> 00:28:34,100
Charlie's owners you will notice that

00:28:32,030 --> 00:28:36,020
this did an exact string match on this

00:28:34,100 --> 00:28:37,820
and so if we wanted to be looking at

00:28:36,020 --> 00:28:38,990
lowercase names there's a way that we

00:28:37,820 --> 00:28:41,179
can do that

00:28:38,990 --> 00:28:43,370
that's what something called a UDF's or

00:28:41,179 --> 00:28:45,470
user-defined functions and so we want to

00:28:43,370 --> 00:28:47,240
define one that's lower case we used a

00:28:45,470 --> 00:28:48,890
built in spark function earlier for the

00:28:47,240 --> 00:28:50,690
length lower case is not a built in

00:28:48,890 --> 00:28:52,940
spark function and so we can go ahead

00:28:50,690 --> 00:28:55,059
and define that ourselves the syntax for

00:28:52,940 --> 00:28:57,230
this is a little different than Scala

00:28:55,059 --> 00:28:59,780
definition syntax but it should look

00:28:57,230 --> 00:29:01,970
familiar enough to you and so we'll take

00:28:59,780 --> 00:29:05,600
in our string and then all we need to do

00:29:01,970 --> 00:29:09,700
is lowercase that and now we have a

00:29:05,600 --> 00:29:13,850
lowercase UDF and we can go ahead and

00:29:09,700 --> 00:29:16,370
add that here and now I think the

00:29:13,850 --> 00:29:19,190
results are pretty similar but you will

00:29:16,370 --> 00:29:20,510
see even more passwords because people

00:29:19,190 --> 00:29:24,710
are more likely to type in lowercase

00:29:20,510 --> 00:29:26,900
things so 278,000 people using Charlie

00:29:24,710 --> 00:29:30,080
Charlie the dogs maybe that's a kid too

00:29:26,900 --> 00:29:32,900
in their passwords so that's pretty

00:29:30,080 --> 00:29:34,100
interesting I'm gonna stop this section

00:29:32,900 --> 00:29:36,550
here because there's a couple other

00:29:34,100 --> 00:29:38,870
things that I want to get through still

00:29:36,550 --> 00:29:44,510
let me make sure that this is going to

00:29:38,870 --> 00:29:47,000
look ok beautiful alright if nothing

00:29:44,510 --> 00:29:48,230
else I think that we can all agree not

00:29:47,000 --> 00:29:50,600
to include our dog names and our

00:29:48,230 --> 00:29:52,250
passwords this entire talk was just an

00:29:50,600 --> 00:29:53,720
excuse and every talk I give is an

00:29:52,250 --> 00:29:56,000
excuse to for me to put a dog rights

00:29:53,720 --> 00:29:57,470
tweet in there I did modify this one but

00:29:56,000 --> 00:30:02,480
this is the best account please go

00:29:57,470 --> 00:30:03,980
follow it so you know these including

00:30:02,480 --> 00:30:06,290
proper nouns and names and your

00:30:03,980 --> 00:30:08,690
passwords is not as clever as you might

00:30:06,290 --> 00:30:10,910
think it's a very common way for people

00:30:08,690 --> 00:30:12,320
to remember their passwords but there's

00:30:10,910 --> 00:30:15,140
other ways that you can create memorable

00:30:12,320 --> 00:30:17,170
passwords that are not as insecure as is

00:30:15,140 --> 00:30:19,940
something like including your dog name

00:30:17,170 --> 00:30:21,350
in all seriousness know some of the

00:30:19,940 --> 00:30:22,850
major benefits of SPARC and some of the

00:30:21,350 --> 00:30:26,150
things that I hope is treated with that

00:30:22,850 --> 00:30:28,250
demo is that it's not only fast and

00:30:26,150 --> 00:30:31,580
flexible but it's really useful for

00:30:28,250 --> 00:30:33,080
quick data exploration and I love how

00:30:31,580 --> 00:30:34,850
quick it is for data exploration and

00:30:33,080 --> 00:30:36,590
having worked on combination data

00:30:34,850 --> 00:30:39,590
science data engineering teams before

00:30:36,590 --> 00:30:41,690
this tool is incredible for having a

00:30:39,590 --> 00:30:43,730
common language between those two

00:30:41,690 --> 00:30:45,080
functions and if anybody's worked in

00:30:43,730 --> 00:30:47,809
those functions before you might know

00:30:45,080 --> 00:30:50,210
how difficult it is for you to work with

00:30:47,809 --> 00:30:51,620
data scientists if they don't speak the

00:30:50,210 --> 00:30:53,660
same language as you and that's

00:30:51,620 --> 00:30:55,580
metaphoric and literal because if

00:30:53,660 --> 00:30:57,620
they're writing all their models and are

00:30:55,580 --> 00:30:59,330
and Python and your entire data

00:30:57,620 --> 00:31:01,610
infrastructure is in Scala it can be

00:30:59,330 --> 00:31:03,410
really hard to translate that over and

00:31:01,610 --> 00:31:06,110
so if you can get on the same platform

00:31:03,410 --> 00:31:08,240
it's a huge step in the direction of

00:31:06,110 --> 00:31:10,460
making your team more productive overall

00:31:08,240 --> 00:31:13,220
I think that's one of the big advantages

00:31:10,460 --> 00:31:15,500
that SPARC is going to have is that it

00:31:13,220 --> 00:31:17,380
provides that toolset for doing both of

00:31:15,500 --> 00:31:19,910
those things together

00:31:17,380 --> 00:31:22,160
of course SPARC is not without

00:31:19,910 --> 00:31:24,080
challenges I do want to highlight the

00:31:22,160 --> 00:31:25,910
challenges and operationalizing since

00:31:24,080 --> 00:31:27,679
it's relatively easy for me to stand

00:31:25,910 --> 00:31:29,510
this up the Zeppelin instance up on my

00:31:27,679 --> 00:31:31,700
local machine if you're doing that on a

00:31:29,510 --> 00:31:36,440
100 node cluster it might be a little

00:31:31,700 --> 00:31:37,700
bit harder but the whole benefit of

00:31:36,440 --> 00:31:39,590
working with something like SPARC is

00:31:37,700 --> 00:31:42,050
that you can run it on distributed data

00:31:39,590 --> 00:31:43,850
it's just not as easy to do that as what

00:31:42,050 --> 00:31:45,559
I just showed you and that's it's still

00:31:43,850 --> 00:31:47,360
a young project the tooling is still

00:31:45,559 --> 00:31:48,500
being developed and I do want to

00:31:47,360 --> 00:31:50,870
highlight some of the things that people

00:31:48,500 --> 00:31:52,730
are doing to make this easier for one

00:31:50,870 --> 00:31:55,130
Heather Miller just wrote a blog post on

00:31:52,730 --> 00:31:56,540
how to launch SPARC on a cluster and so

00:31:55,130 --> 00:31:58,670
that's linked in the slides and I will

00:31:56,540 --> 00:32:00,050
post these later and this is really

00:31:58,670 --> 00:32:01,970
useful because that didn't really exist

00:32:00,050 --> 00:32:04,100
before last month when Heather wrote

00:32:01,970 --> 00:32:06,230
that post and so there's other things

00:32:04,100 --> 00:32:08,510
that people are doing to improve the

00:32:06,230 --> 00:32:11,540
operation of operationalizing

00:32:08,510 --> 00:32:12,830
the opaque error messages holding Caro

00:32:11,540 --> 00:32:15,290
gave a really good talk

00:32:12,830 --> 00:32:19,600
I see holding in the back last week at

00:32:15,290 --> 00:32:21,710
PyCon about how to debug PI SPARC and

00:32:19,600 --> 00:32:24,020
Python developers have any more

00:32:21,710 --> 00:32:25,520
challenging journey than scala

00:32:24,020 --> 00:32:28,429
developers because they've got Python

00:32:25,520 --> 00:32:30,580
stack traces nested in Java strat traces

00:32:28,429 --> 00:32:33,620
nested in python stack traces

00:32:30,580 --> 00:32:35,890
unfortunately we only have Java stack

00:32:33,620 --> 00:32:39,230
traces but it still can be a little

00:32:35,890 --> 00:32:40,760
challenging and included this error

00:32:39,230 --> 00:32:43,820
message so you can see what some of

00:32:40,760 --> 00:32:45,050
these stack traces look like we can

00:32:43,820 --> 00:32:46,490
spend a little bit of time on this this

00:32:45,050 --> 00:32:48,980
is an error message from something I was

00:32:46,490 --> 00:32:50,330
running on my own machine and so you can

00:32:48,980 --> 00:32:52,700
imagine when you start running this on

00:32:50,330 --> 00:32:55,520
large clusters you get all these you

00:32:52,700 --> 00:32:57,590
know node executor lost errors that are

00:32:55,520 --> 00:32:59,900
really challenging to debug if you don't

00:32:57,590 --> 00:33:02,150
know the right place to look and there's

00:32:59,900 --> 00:33:04,490
work being done every day to make these

00:33:02,150 --> 00:33:05,370
a little bit better but this is still

00:33:04,490 --> 00:33:09,000
something that's a

00:33:05,370 --> 00:33:10,440
big challenge of working with spark and

00:33:09,000 --> 00:33:11,610
with regards to documentation I

00:33:10,440 --> 00:33:12,990
definitely want to call out the

00:33:11,610 --> 00:33:15,000
documentation that yacht zach has

00:33:12,990 --> 00:33:17,400
compiled and so this is not part of the

00:33:15,000 --> 00:33:19,800
official spark documentation but this

00:33:17,400 --> 00:33:21,810
dock is the master in apache spark and

00:33:19,800 --> 00:33:24,780
he has a companion mastering apache

00:33:21,810 --> 00:33:27,030
spark sequel guide and so this is really

00:33:24,780 --> 00:33:28,710
really awesome stuff as soon as I found

00:33:27,030 --> 00:33:30,780
this it was immediately bookmarked and

00:33:28,710 --> 00:33:33,600
some of that syntax that you saw me

00:33:30,780 --> 00:33:35,160
writing with like the dollar sign for

00:33:33,600 --> 00:33:37,020
accessing columns the way that you

00:33:35,160 --> 00:33:38,790
compare things and where clause is

00:33:37,020 --> 00:33:40,440
there's like triple equals that getting

00:33:38,790 --> 00:33:42,840
introduced none of that is super

00:33:40,440 --> 00:33:44,190
intuitive unless you've been introduced

00:33:42,840 --> 00:33:45,840
to the documentation from somewhere else

00:33:44,190 --> 00:33:48,930
and this is one of the really really

00:33:45,840 --> 00:33:51,270
good guides for doing that if you're

00:33:48,930 --> 00:33:52,920
part of the SPARC community working on

00:33:51,270 --> 00:33:55,440
the documentation is always a great way

00:33:52,920 --> 00:34:00,750
to become more familiar with it and I

00:33:55,440 --> 00:34:01,920
encourage you to help out there finally

00:34:00,750 --> 00:34:03,720
I want to leave you thinking about some

00:34:01,920 --> 00:34:05,430
of the security implications of working

00:34:03,720 --> 00:34:06,930
with big data I think this whole talk is

00:34:05,430 --> 00:34:09,000
kind of revolved around these things a

00:34:06,930 --> 00:34:11,250
little bit but there's a lot of things

00:34:09,000 --> 00:34:14,790
that we need to be considerate of in

00:34:11,250 --> 00:34:17,910
2018 it's big data is a really useful

00:34:14,790 --> 00:34:19,470
tool for finding bad things and it's a

00:34:17,910 --> 00:34:21,330
useful tool for gaining insights into

00:34:19,470 --> 00:34:22,620
your business but you want to keep in

00:34:21,330 --> 00:34:27,630
mind that you could be part of the next

00:34:22,620 --> 00:34:30,690
data breach so this Dilbert is almost

00:34:27,630 --> 00:34:32,700
too real a little too painful data

00:34:30,690 --> 00:34:36,540
privacy is on everybody's mind lately

00:34:32,700 --> 00:34:38,100
and whether that's gdpr which as I say

00:34:36,540 --> 00:34:39,690
that I see those of you that are

00:34:38,100 --> 00:34:41,940
cringing cuz that's all you have to

00:34:39,690 --> 00:34:43,710
think about GDP are obviously we're in

00:34:41,940 --> 00:34:45,930
Europe right now most of you are

00:34:43,710 --> 00:34:48,120
probably very familiar the data privacy

00:34:45,930 --> 00:34:51,120
law in the EU that goes into effect on

00:34:48,120 --> 00:34:52,590
May 25th coming right up and this is why

00:34:51,120 --> 00:34:56,040
you've been getting all of those privacy

00:34:52,590 --> 00:34:57,420
policy change emails lately so all of

00:34:56,040 --> 00:34:59,310
these companies are doing all this work

00:34:57,420 --> 00:35:01,290
to become compliant and that involves

00:34:59,310 --> 00:35:02,580
updating their privacy policies you see

00:35:01,290 --> 00:35:04,830
companies that are just straight up

00:35:02,580 --> 00:35:07,320
shutting down because all their business

00:35:04,830 --> 00:35:09,720
was relying on selling your data and now

00:35:07,320 --> 00:35:10,860
they can't do that anymore so I think

00:35:09,720 --> 00:35:12,180
it's really interesting to see the

00:35:10,860 --> 00:35:14,250
evolution of things that are happening

00:35:12,180 --> 00:35:16,260
all the data breaches that are being

00:35:14,250 --> 00:35:18,410
revealed right now part of that is in

00:35:16,260 --> 00:35:20,270
order to be compliant with GDP

00:35:18,410 --> 00:35:23,090
you have to reveal the data breaches

00:35:20,270 --> 00:35:24,470
within like 72 hours or something and so

00:35:23,090 --> 00:35:27,710
people are getting out in front of that

00:35:24,470 --> 00:35:30,050
in order to maybe be lost in the noise

00:35:27,710 --> 00:35:32,500
maybe that's their goal I don't know our

00:35:30,050 --> 00:35:34,640
strategy is to wear them down right and

00:35:32,500 --> 00:35:37,520
so you know we can think about things

00:35:34,640 --> 00:35:39,920
like gdpr cambridge analytic i the

00:35:37,520 --> 00:35:42,080
Equifax hack these things are happening

00:35:39,920 --> 00:35:43,580
so often now that it's really hard to

00:35:42,080 --> 00:35:46,640
forget about the implications of

00:35:43,580 --> 00:35:48,800
security and privacy and we really can't

00:35:46,640 --> 00:35:50,359
separate for security from development

00:35:48,800 --> 00:35:53,150
especially when we're working with any

00:35:50,359 --> 00:35:54,619
kind of data so as engineers we need to

00:35:53,150 --> 00:35:58,460
be mindful of the data that you store

00:35:54,619 --> 00:36:00,470
and how we're accessing it and that's

00:35:58,460 --> 00:36:02,840
because nobody security is perfect and

00:36:00,470 --> 00:36:05,330
so if you have any influence over the

00:36:02,840 --> 00:36:06,950
security at your company including if

00:36:05,330 --> 00:36:09,440
your company is too small to have a

00:36:06,950 --> 00:36:11,420
dedicated security team make sure you're

00:36:09,440 --> 00:36:14,119
storing passwords and other personally

00:36:11,420 --> 00:36:16,040
identifiable information securely and be

00:36:14,119 --> 00:36:18,380
an advocate for not storing more data

00:36:16,040 --> 00:36:19,970
than it's necessary sometimes executives

00:36:18,380 --> 00:36:21,800
come in and say store all the data it

00:36:19,970 --> 00:36:23,390
might be useful for analytics later but

00:36:21,800 --> 00:36:24,500
honestly that can cause you more

00:36:23,390 --> 00:36:26,480
headaches down the line and that's

00:36:24,500 --> 00:36:28,490
something that security experts often

00:36:26,480 --> 00:36:31,609
recommend it's simply not storing more

00:36:28,490 --> 00:36:33,290
data than you need all of that password

00:36:31,609 --> 00:36:34,820
data that we looked at is accessible

00:36:33,290 --> 00:36:37,700
because it was stored in a way that made

00:36:34,820 --> 00:36:40,100
either decrypting it or accessing an

00:36:37,700 --> 00:36:42,710
available in plaintext and so those are

00:36:40,100 --> 00:36:44,780
all data breaches that are unfortunately

00:36:42,710 --> 00:36:46,520
now available to us because there was

00:36:44,780 --> 00:36:49,490
not great security around the way that

00:36:46,520 --> 00:36:51,350
data was stored and so this tweet if you

00:36:49,490 --> 00:36:53,359
haven't seen it it was revealed that

00:36:51,350 --> 00:36:55,130
t-mobile or at least t-mobile Austria

00:36:53,359 --> 00:36:57,859
was story in plain text password data

00:36:55,130 --> 00:36:59,630
and unfortunately the support agent

00:36:57,859 --> 00:37:01,430
really really doubled down on the

00:36:59,630 --> 00:37:03,470
impossibility of that being a problem

00:37:01,430 --> 00:37:08,000
which you don't want to do you don't

00:37:03,470 --> 00:37:09,730
want your company to be like hath so

00:37:08,000 --> 00:37:11,960
that's from the company side of things

00:37:09,730 --> 00:37:13,369
from the consumer side of things it

00:37:11,960 --> 00:37:15,350
might be preaching to the choir and an

00:37:13,369 --> 00:37:18,470
audience like this but please use a

00:37:15,350 --> 00:37:21,350
password manager you can use something

00:37:18,470 --> 00:37:23,210
like LastPass or one password turn on

00:37:21,350 --> 00:37:26,000
two-factor authentication and wherever

00:37:23,210 --> 00:37:28,820
possible use an app like offi instead of

00:37:26,000 --> 00:37:29,900
SMS 2fa because it turns out that SMS

00:37:28,820 --> 00:37:32,000
2fa is also not

00:37:29,900 --> 00:37:34,730
terribly secure and those two things

00:37:32,000 --> 00:37:36,650
alone will do an enormous amount to

00:37:34,730 --> 00:37:39,320
protect your identity and secure your

00:37:36,650 --> 00:37:40,670
online presence and as engineers one of

00:37:39,320 --> 00:37:42,710
the things that we can also do is

00:37:40,670 --> 00:37:44,750
encourage our friends and family to use

00:37:42,710 --> 00:37:46,520
tools like this - these are consumer

00:37:44,750 --> 00:37:48,110
applications that are really accessible

00:37:46,520 --> 00:37:50,270
to anyone regardless of their technical

00:37:48,110 --> 00:37:52,130
background and this is something that we

00:37:50,270 --> 00:37:53,960
can do is encourage our parents and our

00:37:52,130 --> 00:37:56,750
siblings and our aunts and uncles all to

00:37:53,960 --> 00:37:58,910
use these tools my dad uses a

00:37:56,750 --> 00:38:01,730
password-protected Excel spreadsheet to

00:37:58,910 --> 00:38:04,250
store his passwords honestly I'm like go

00:38:01,730 --> 00:38:05,780
dad that's pretty good because the

00:38:04,250 --> 00:38:08,420
important part there's the security

00:38:05,780 --> 00:38:10,970
mindset is there and that's often the

00:38:08,420 --> 00:38:12,500
hardest hurdle to overcome is that

00:38:10,970 --> 00:38:14,870
people just don't think it's a problem

00:38:12,500 --> 00:38:17,090
and so we'll get him using LastPass soon

00:38:14,870 --> 00:38:22,160
enough but I'm pretty proud of this the

00:38:17,090 --> 00:38:23,240
steps he's taken already so thank you

00:38:22,160 --> 00:38:25,010
for letting me spend some time with you

00:38:23,240 --> 00:38:27,920
this morning and diving into spark

00:38:25,010 --> 00:38:29,660
through the lens of own passwords I love

00:38:27,920 --> 00:38:31,130
chatting about this stuff so hopefully

00:38:29,660 --> 00:38:34,370
I've inspired some of you to dig into

00:38:31,130 --> 00:38:34,760
your own data with spark if you have big

00:38:34,370 --> 00:38:37,610
data

00:38:34,760 --> 00:38:39,560
I think spark and sequel are great tools

00:38:37,610 --> 00:38:42,140
for doing a lot of powerful analysis

00:38:39,560 --> 00:38:43,880
like I mentioned I'm really excited to

00:38:42,140 --> 00:38:46,610
see how the project continues to develop

00:38:43,880 --> 00:38:48,740
and what people will use it for and I've

00:38:46,610 --> 00:38:50,390
also decided this lady hacker skeleton

00:38:48,740 --> 00:38:52,700
is my alter ego so that's why I like

00:38:50,390 --> 00:38:54,200
closing with this slide I will post

00:38:52,700 --> 00:38:55,940
these slides to my Twitter right after

00:38:54,200 --> 00:38:57,680
I'm done here and I'm gonna have a

00:38:55,940 --> 00:39:00,440
tutorial ready on the Twilio blog pretty

00:38:57,680 --> 00:39:01,700
soon to show you what I did come find me

00:39:00,440 --> 00:39:05,180
after this if you want to talk about

00:39:01,700 --> 00:39:06,890
Scala spark or security I'll be hanging

00:39:05,180 --> 00:39:09,710
out here for a little bit or I'm also at

00:39:06,890 --> 00:39:11,750
the Twilio booth back by room five once

00:39:09,710 --> 00:39:13,610
again my name is Kelly Robinson please

00:39:11,750 --> 00:39:15,730
use a password manager and thank you for

00:39:13,610 --> 00:39:15,730
listening

00:39:19,829 --> 00:39:24,809
thank you very much Kelly um I think we

00:39:22,990 --> 00:39:33,549
have some time for questions now are

00:39:24,809 --> 00:39:45,730
they only want to ask something okay

00:39:33,549 --> 00:39:50,049
that's what I would like to ask how they

00:39:45,730 --> 00:39:54,220
type in further in the jeiza file when

00:39:50,049 --> 00:39:56,710
you process it with this Park how stark

00:39:54,220 --> 00:40:02,589
does type inference from JSON files it

00:39:56,710 --> 00:40:05,410
is based on them not Jason sorry CSV

00:40:02,589 --> 00:40:07,089
files that's a good question I don't

00:40:05,410 --> 00:40:09,039
know fully the mechanics of how that

00:40:07,089 --> 00:40:11,339
works under the hood if anybody here

00:40:09,039 --> 00:40:17,109
knows and wants to chime in Holden

00:40:11,339 --> 00:40:19,680
Holden knows so I will let her take that

00:40:17,109 --> 00:40:19,680
answer

00:40:26,769 --> 00:40:30,529
this is my strategy for doing Q&A as I

00:40:29,569 --> 00:40:35,269
make somebody else in the audience

00:40:30,529 --> 00:40:37,579
answer yeah so this actually changes

00:40:35,269 --> 00:40:40,700
between different versions of spark but

00:40:37,579 --> 00:40:43,490
it samples the rose and it looks at the

00:40:40,700 --> 00:40:44,930
the columns if you don't have infer

00:40:43,490 --> 00:40:46,730
schema set to true it just assumes

00:40:44,930 --> 00:40:48,880
everything's a string but if you haven't

00:40:46,730 --> 00:40:51,440
first schema such as true it will do

00:40:48,880 --> 00:40:53,779
some basic sampling and you can

00:40:51,440 --> 00:40:55,940
configure the sampling if the inferred

00:40:53,779 --> 00:40:58,819
schema is garbage but normally it works

00:40:55,940 --> 00:41:00,140
out okay for CSV files for JSON it's

00:40:58,819 --> 00:41:02,240
normally the one where you have to tune

00:41:00,140 --> 00:41:12,529
the sampling if you want to see the code

00:41:02,240 --> 00:41:15,349
come find me any more questions one on

00:41:12,529 --> 00:41:21,140
the front okay can you wait for that

00:41:15,349 --> 00:41:23,720
Mike did he say a bit about clustering

00:41:21,140 --> 00:41:28,609
in Apache spark what do you mean like

00:41:23,720 --> 00:41:29,660
how to deploy out of cluster or so what

00:41:28,609 --> 00:41:31,549
I'm gonna point you to Heather's

00:41:29,660 --> 00:41:33,109
tutorial there you basically have to

00:41:31,549 --> 00:41:35,180
have some kind of cluster management

00:41:33,109 --> 00:41:36,289
tool and so you need machines that

00:41:35,180 --> 00:41:37,819
you're going to be able to process the

00:41:36,289 --> 00:41:41,259
data on and then you need something like

00:41:37,819 --> 00:41:44,599
yarn or may sews or some kind of

00:41:41,259 --> 00:41:47,299
resource manager that's going to work

00:41:44,599 --> 00:41:49,490
with the the master node to be able to

00:41:47,299 --> 00:41:52,730
delegate processes to all the worker

00:41:49,490 --> 00:41:54,140
nodes and so the the way that we were

00:41:52,730 --> 00:41:56,779
running that at my last company was

00:41:54,140 --> 00:41:59,029
using Amazon's EMR which is elastic

00:41:56,779 --> 00:42:01,430
MapReduce and that's a tool that you can

00:41:59,029 --> 00:42:03,109
run with node or mates those to do some

00:42:01,430 --> 00:42:04,910
of that stuff you know there's there's

00:42:03,109 --> 00:42:06,769
tools for out there there for this too

00:42:04,910 --> 00:42:08,150
and there's also enterprise solutions

00:42:06,769 --> 00:42:09,650
that allow you to do this and so things

00:42:08,150 --> 00:42:11,269
like data bricks allow you to do this

00:42:09,650 --> 00:42:13,400
without having to do all the cluster

00:42:11,269 --> 00:42:14,960
management yourself they will do that

00:42:13,400 --> 00:42:16,990
for you and that's kind of their value

00:42:14,960 --> 00:42:16,990

YouTube URL: https://www.youtube.com/watch?v=BRp-PY5SyK0


