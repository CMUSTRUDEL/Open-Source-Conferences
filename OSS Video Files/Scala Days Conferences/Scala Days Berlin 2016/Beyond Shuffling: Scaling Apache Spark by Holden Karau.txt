Title: Beyond Shuffling: Scaling Apache Spark by Holden Karau
Publication date: 2016-07-22
Playlist: Scala Days Berlin 2016
Description: 
	This video was recorded at Scala Days Berlin 2016
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

Abstract:
This session will cover our & community experiences scaling Spark jobs to large datasets and the resulting best practices along with code snippets to illustrate.
 
The planned topics are:
* Using Spark counters for performance investigation
* Spark collects a large number of statistics about our code, but how often do we really look at them? We will cover how to investigate performance issues and figure out where to best spend our time using both counters and the UI.
* Working with Key/Value Data
* Replacing groupByKey for awesomeness
groupByKey makes it too easy to accidently collect individual records which are too large to process. We will talk about how to replace it in different common cases with more memory efficient operations.
* Effective caching & checkpointing
* Being able to reuse previously computed RDDs without recomputing can substantially reduce execution time. Choosing when to cache, checkpoint, or what storage level to use can have a huge performance impact.
* Considerations for noisy clusters
* Functional transformations with Spark Datasets
*How to have the some of benefits of Sparkâ€™s DataFrames while still having the ability to work with arbitrary Scala code
Captions: 
	00:00:01,839 --> 00:00:08,530
so thank you all for coming to beyond

00:00:04,840 --> 00:00:11,830
shuffling I'm Holden and the the cute

00:00:08,530 --> 00:00:15,910
bunny is a fluffy bunny and her name is

00:00:11,830 --> 00:00:18,900
bun bun and my dog over here is named

00:00:15,910 --> 00:00:22,480
boo boo the dog

00:00:18,900 --> 00:00:24,669
so I'm holdin my pronouns are she or her

00:00:22,480 --> 00:00:27,669
it's tattooed on my wrist in case you

00:00:24,669 --> 00:00:30,009
forget just just check that I'm a

00:00:27,669 --> 00:00:32,980
principal engineer at IBM's spark

00:00:30,009 --> 00:00:34,210
Technology Center in San Francisco so if

00:00:32,980 --> 00:00:36,340
I fall asleep in the middle of my talk

00:00:34,210 --> 00:00:40,480
just poke me back awake and I'm a little

00:00:36,340 --> 00:00:42,610
jet-lagged and my boss asked me to say

00:00:40,480 --> 00:00:46,030
that iBM is a founding member of the

00:00:42,610 --> 00:00:47,650
Scala Center as well and and we're

00:00:46,030 --> 00:00:49,960
really excited with all of the really

00:00:47,650 --> 00:00:52,450
cool things that are happening around

00:00:49,960 --> 00:00:53,890
the Scala Center and spark and sort of

00:00:52,450 --> 00:00:55,510
how we can be involved with some of that

00:00:53,890 --> 00:00:57,610
and bringing bringing a lot of cool

00:00:55,510 --> 00:01:00,690
stuff that's happening in Scala and

00:00:57,610 --> 00:01:03,700
helping helping that go with spark

00:01:00,690 --> 00:01:06,749
previously I've been at Alpine data

00:01:03,700 --> 00:01:09,219
bricks Google of Foursquare and Amazon

00:01:06,749 --> 00:01:12,009
doing sort of a mixture of big data and

00:01:09,219 --> 00:01:14,710
search and recommendation problems I'm a

00:01:12,009 --> 00:01:16,299
co-author of the learning spark book and

00:01:14,710 --> 00:01:18,840
to get people to actually ask questions

00:01:16,299 --> 00:01:22,390
the first person who asks me a question

00:01:18,840 --> 00:01:24,999
at the end of the talk will get the

00:01:22,390 --> 00:01:27,479
English version of learning spark it's a

00:01:24,999 --> 00:01:30,609
little out of date but you know it's fun

00:01:27,479 --> 00:01:32,350
if you want to follow me on Twitter you

00:01:30,609 --> 00:01:35,109
can go right ahead it's mostly pictures

00:01:32,350 --> 00:01:38,109
of stuffed animals and random complaints

00:01:35,109 --> 00:01:39,880
about programming languages occasionally

00:01:38,109 --> 00:01:40,530
stuff about spark is is thrown in for

00:01:39,880 --> 00:01:44,229
good measure

00:01:40,530 --> 00:01:45,819
I've got a SlideShare slides from

00:01:44,229 --> 00:01:48,459
today's talk and most of my previous

00:01:45,819 --> 00:01:49,719
talks will be up on SlideShare if you

00:01:48,459 --> 00:01:51,520
want to follow me on github there's

00:01:49,719 --> 00:01:54,009
there's a bunch of random spark related

00:01:51,520 --> 00:01:55,299
projects and I also have videos from a

00:01:54,009 --> 00:01:58,659
lot of my other spark Talks

00:01:55,299 --> 00:02:00,009
they're there up there as well and I'll

00:01:58,659 --> 00:02:03,549
probably throw out some references to

00:02:00,009 --> 00:02:05,469
them too so I'm gonna start with sort of

00:02:03,549 --> 00:02:07,810
what my assumptions are about you all as

00:02:05,469 --> 00:02:09,909
people and if I get a lot of really

00:02:07,810 --> 00:02:12,610
blank faces we'll we'll switch things up

00:02:09,909 --> 00:02:16,430
a bit as we go but we'll talk about sort

00:02:12,610 --> 00:02:17,900
of how to reuse our data and spark if

00:02:16,430 --> 00:02:20,180
considerations from when we're working

00:02:17,900 --> 00:02:22,360
with key value data and sort of what key

00:02:20,180 --> 00:02:24,830
skew means for for working in spark

00:02:22,360 --> 00:02:27,850
we'll also look at how spark sequel can

00:02:24,830 --> 00:02:30,830
help us make really fast spark programs

00:02:27,850 --> 00:02:33,140
it's it's really great then we'll also

00:02:30,830 --> 00:02:35,380
talk about sort of iterator to iterator

00:02:33,140 --> 00:02:38,090
transformations inside of map partitions

00:02:35,380 --> 00:02:39,770
and I'm briefly going to talk about how

00:02:38,090 --> 00:02:42,190
to test spark code and sort of how to

00:02:39,770 --> 00:02:45,020
validate your spark code and sort of

00:02:42,190 --> 00:02:46,940
this this sort of leads into some of the

00:02:45,020 --> 00:02:49,850
counters and stuff that spark collects

00:02:46,940 --> 00:02:51,620
on our job as we're running that can be

00:02:49,850 --> 00:02:56,240
useful in just general debugging as well

00:02:51,620 --> 00:02:57,470
as testing and validation or if you know

00:02:56,240 --> 00:02:59,420
you zoned out you can just think of us

00:02:57,470 --> 00:03:01,910
as going from this very sad kind of

00:02:59,420 --> 00:03:04,730
angry cat that's wet to this very happy

00:03:01,910 --> 00:03:06,320
unicorn we're gonna go from ohms to to

00:03:04,730 --> 00:03:08,750
the happy spark land where there are no

00:03:06,320 --> 00:03:11,630
out of memory exceptions and it's a very

00:03:08,750 --> 00:03:14,510
happy place so I'm hoping you're all

00:03:11,630 --> 00:03:17,000
nice people and you don't mind pictures

00:03:14,510 --> 00:03:18,770
of cats and you may or may not like

00:03:17,000 --> 00:03:20,980
pictures of David Hasselhoff how many

00:03:18,770 --> 00:03:22,160
people like pictures of David Hasselhoff

00:03:20,980 --> 00:03:25,010
okay

00:03:22,160 --> 00:03:27,350
some people that's good I at least one

00:03:25,010 --> 00:03:31,490
picture of David Hasselhoff and lots of

00:03:27,350 --> 00:03:33,530
pictures of cats the the more serious

00:03:31,490 --> 00:03:35,890
one is how many people know or have

00:03:33,530 --> 00:03:41,650
worked in Apache spark like you've you

00:03:35,890 --> 00:03:44,000
wonderful okay this is this is very good

00:03:41,650 --> 00:03:45,770
how many people want to scale their

00:03:44,000 --> 00:03:47,630
spark jobs you want to make things go

00:03:45,770 --> 00:03:49,820
faster our handle larger datasets is

00:03:47,630 --> 00:03:52,250
that also okay great so you did come to

00:03:49,820 --> 00:03:53,510
the right talk that's wonderful and I'm

00:03:52,250 --> 00:03:54,920
hoping you don't mind a grab-bag of

00:03:53,510 --> 00:03:57,770
topics at this point though really

00:03:54,920 --> 00:03:59,510
that's probably not gonna change you're

00:03:57,770 --> 00:04:02,720
sort of stuck with it we're gonna be

00:03:59,510 --> 00:04:04,970
focused mostly just on on Quora spark as

00:04:02,720 --> 00:04:07,280
well as sort of the sequel and data

00:04:04,970 --> 00:04:09,620
frames and data sets that are now in

00:04:07,280 --> 00:04:12,110
spark 1:6 this doesn't really dive

00:04:09,620 --> 00:04:13,430
deeply into streaming or machine

00:04:12,110 --> 00:04:15,620
learning or any of the really sort of

00:04:13,430 --> 00:04:18,650
application-specific libraries that are

00:04:15,620 --> 00:04:19,760
built on spark if you're interested in

00:04:18,650 --> 00:04:22,169
those things there's some other great

00:04:19,760 --> 00:04:25,120
talks that I can recommend

00:04:22,169 --> 00:04:28,120
so that's sort of my pitch for you to

00:04:25,120 --> 00:04:30,100
stay for the rest of the talk and not go

00:04:28,120 --> 00:04:32,410
and get coffee that this cat very much

00:04:30,100 --> 00:04:33,699
wants to go and get some coffee but it's

00:04:32,410 --> 00:04:35,650
okay I'll understand if you want to

00:04:33,699 --> 00:04:37,030
leave in the middle to get coffee I'm

00:04:35,650 --> 00:04:41,080
gonna take some break stay at coffee

00:04:37,030 --> 00:04:45,760
myself so we're gonna start with the the

00:04:41,080 --> 00:04:47,470
really standard example and this example

00:04:45,760 --> 00:04:49,479
looks pretty reasonable

00:04:47,470 --> 00:04:51,760
just like presumably how this file

00:04:49,479 --> 00:04:54,160
cabinet looked to that that cat it

00:04:51,760 --> 00:04:55,750
looked pretty reasonable and safe but in

00:04:54,160 --> 00:04:58,570
reality it's it's actually pretty bad

00:04:55,750 --> 00:05:04,389
right we've got our our really standard

00:04:58,570 --> 00:05:06,370
word count example done wrong but it's

00:05:04,389 --> 00:05:07,990
there and it looks okay and then we also

00:05:06,370 --> 00:05:11,350
have another one where we're looking for

00:05:07,990 --> 00:05:15,210
all of the error messages in in our

00:05:11,350 --> 00:05:17,800
input data and so this is you know a

00:05:15,210 --> 00:05:19,630
really basic common thing that we would

00:05:17,800 --> 00:05:22,479
do and it looks pretty reasonable in

00:05:19,630 --> 00:05:24,070
this code right like in unless hue or

00:05:22,479 --> 00:05:25,479
you're explicitly looking for it it

00:05:24,070 --> 00:05:27,550
probably looks like this code is is

00:05:25,479 --> 00:05:30,099
reasonable and and the thing is it will

00:05:27,550 --> 00:05:31,750
actually probably run successfully it's

00:05:30,099 --> 00:05:32,950
just we could do this a lot better so

00:05:31,750 --> 00:05:35,860
let's let's look at some of those things

00:05:32,950 --> 00:05:37,510
we could do the first is we have the

00:05:35,860 --> 00:05:39,970
same input data that we're using

00:05:37,510 --> 00:05:42,820
multiple times and we haven't explicitly

00:05:39,970 --> 00:05:44,080
cached our input data and this is this

00:05:42,820 --> 00:05:46,210
is important

00:05:44,080 --> 00:05:47,740
sparks lazy evaluation is really great

00:05:46,210 --> 00:05:50,860
because it lets us build up this this

00:05:47,740 --> 00:05:52,479
really cool graph and then spark is able

00:05:50,860 --> 00:05:55,270
to do a lot of really cool optimizations

00:05:52,479 --> 00:05:58,810
using this graph but the sort of

00:05:55,270 --> 00:06:02,800
downside of this approach is that it

00:05:58,810 --> 00:06:04,810
only gets to understand our program up

00:06:02,800 --> 00:06:07,060
to the point where we have an action and

00:06:04,810 --> 00:06:09,490
our action is something like saving it

00:06:07,060 --> 00:06:11,260
out to disk or you know counting the

00:06:09,490 --> 00:06:14,050
record something that forces spark to

00:06:11,260 --> 00:06:15,760
evaluate it and spark can't see forward

00:06:14,050 --> 00:06:17,650
in our program to tell if we're gonna be

00:06:15,760 --> 00:06:19,630
reusing the same data multiple times

00:06:17,650 --> 00:06:21,190
right it's not like a compiler it

00:06:19,630 --> 00:06:23,860
doesn't have a whole program view of

00:06:21,190 --> 00:06:25,180
what's going on so we need to help spark

00:06:23,860 --> 00:06:27,030
out and we need to tell it when we're

00:06:25,180 --> 00:06:29,199
going to be using data multiple times

00:06:27,030 --> 00:06:30,640
and so there's a few different options

00:06:29,199 --> 00:06:32,740
to do this right we can cache it in

00:06:30,640 --> 00:06:34,510
memory and that's really good if your

00:06:32,740 --> 00:06:37,090
data is pretty small caching a memory

00:06:34,510 --> 00:06:38,650
fine if your data is bigger you might

00:06:37,090 --> 00:06:41,200
want to just cash it in memory and

00:06:38,650 --> 00:06:43,780
fallback to disk when you fall you know

00:06:41,200 --> 00:06:45,280
once there's too much contention and

00:06:43,780 --> 00:06:47,710
this can get a bit tricky if you're

00:06:45,280 --> 00:06:49,240
running in a shared environment so if

00:06:47,710 --> 00:06:51,730
you've over in a notebook environment

00:06:49,240 --> 00:06:54,430
like data Brooks cloud and you have a

00:06:51,730 --> 00:06:55,960
bunch of people on the same context the

00:06:54,430 --> 00:06:57,670
data that you cache could actually be

00:06:55,960 --> 00:06:59,890
evicted by other people's caching

00:06:57,670 --> 00:07:01,990
actions and this can lead to sort of

00:06:59,890 --> 00:07:04,120
wonky performance which which doesn't

00:07:01,990 --> 00:07:07,420
really make sense unless you go and look

00:07:04,120 --> 00:07:09,490
at the cache page how many people run on

00:07:07,420 --> 00:07:12,040
a shared cluster you're not the only

00:07:09,490 --> 00:07:13,840
people on your cluster okay so most

00:07:12,040 --> 00:07:17,230
people here are very lucky or have very

00:07:13,840 --> 00:07:19,120
big budgets and that's awesome that's

00:07:17,230 --> 00:07:22,060
great for the people that aren't on

00:07:19,120 --> 00:07:23,920
those dedicated clusters one thing that

00:07:22,060 --> 00:07:27,070
can happen is you can get your jobs

00:07:23,920 --> 00:07:28,630
evicted and this is this is just sort of

00:07:27,070 --> 00:07:31,000
a fact of life of working in a shared

00:07:28,630 --> 00:07:32,770
cluster and it's not the end of the

00:07:31,000 --> 00:07:35,110
world because SPARC is is resilient

00:07:32,770 --> 00:07:38,590
right it's it's very easy it's very

00:07:35,110 --> 00:07:40,780
capable of handling our evictions but

00:07:38,590 --> 00:07:42,880
the problem is the fictions intermixed

00:07:40,780 --> 00:07:45,970
with the caching can can give us some

00:07:42,880 --> 00:07:47,890
really bad performance and and mostly

00:07:45,970 --> 00:07:50,170
with what this happens is if we lose

00:07:47,890 --> 00:07:52,090
like half of our executor zhh there's no

00:07:50,170 --> 00:07:53,620
way the cache data that we've we've

00:07:52,090 --> 00:07:55,360
cached on those machines is actually

00:07:53,620 --> 00:07:57,190
going to be there right like we've lost

00:07:55,360 --> 00:07:58,660
so many machines we are gonna have to

00:07:57,190 --> 00:08:01,870
recompute our data and that's that's

00:07:58,660 --> 00:08:03,670
just a thing if you're on a noisy

00:08:01,870 --> 00:08:06,190
cluster it might be worth it to

00:08:03,670 --> 00:08:07,840
checkpoint your data and checkpointing

00:08:06,190 --> 00:08:09,640
is a really sort of heavy-handed

00:08:07,840 --> 00:08:11,710
operation right it's it's sort of the

00:08:09,640 --> 00:08:14,410
MapReduce model or you take your data

00:08:11,710 --> 00:08:16,030
and you write it out to HDFS or whatever

00:08:14,410 --> 00:08:19,150
persistent storage system that you're

00:08:16,030 --> 00:08:21,310
using and then you you go from there and

00:08:19,150 --> 00:08:22,900
this way you know even if you lose half

00:08:21,310 --> 00:08:25,780
your cluster you aren't going to have to

00:08:22,900 --> 00:08:27,280
recompute the data and it's not like the

00:08:25,780 --> 00:08:29,290
recomputing the data is something that

00:08:27,280 --> 00:08:32,200
you have to do SPARC will do it for you

00:08:29,290 --> 00:08:34,120
but it will slow down your job so

00:08:32,200 --> 00:08:36,190
checkpointing can be useful for people

00:08:34,120 --> 00:08:39,040
in shared contentious cluster

00:08:36,190 --> 00:08:42,160
environments but if you do checkpoint

00:08:39,040 --> 00:08:45,070
always persist first because otherwise

00:08:42,160 --> 00:08:46,720
checkpointing is really designed for the

00:08:45,070 --> 00:08:48,430
situation where your lineage graph has

00:08:46,720 --> 00:08:49,660
gotten really really large

00:08:48,430 --> 00:08:52,090
like you've now a whole bunch of

00:08:49,660 --> 00:08:54,460
transformations and so checkpointing

00:08:52,090 --> 00:08:57,490
will actually throw out this list

00:08:54,460 --> 00:08:59,920
lineage graph and it'll also not bother

00:08:57,490 --> 00:09:01,840
keeping the data around in memory so as

00:08:59,920 --> 00:09:03,670
soon as you checkpoint it actually just

00:09:01,840 --> 00:09:05,530
you know writes it up to disk and gets

00:09:03,670 --> 00:09:07,510
rid of everything and that's probably

00:09:05,530 --> 00:09:09,790
not what you want you probably actually

00:09:07,510 --> 00:09:11,800
want to write it out to disk but keep it

00:09:09,790 --> 00:09:14,170
in memory and just fall back to the

00:09:11,800 --> 00:09:17,350
thing that's in HDFS in case you lose

00:09:14,170 --> 00:09:20,920
your clusters or sorry lose your notice

00:09:17,350 --> 00:09:22,630
in your cluster the other thing with

00:09:20,920 --> 00:09:25,150
which we were doing in our word count

00:09:22,630 --> 00:09:29,380
example is group by key and group by key

00:09:25,150 --> 00:09:31,570
is pretty much the devil and and this

00:09:29,380 --> 00:09:34,060
can happen group by key can be safe if

00:09:31,570 --> 00:09:36,970
we have like absolutely no key skew and

00:09:34,060 --> 00:09:39,070
most of our keys are unique right

00:09:36,970 --> 00:09:42,250
in that case group by key is is pretty

00:09:39,070 --> 00:09:43,390
reasonable but I think as for people

00:09:42,250 --> 00:09:45,910
coming from an engineering background

00:09:43,390 --> 00:09:48,040
we're not always used to thinking what

00:09:45,910 --> 00:09:50,440
the distribution of the data that we're

00:09:48,040 --> 00:09:51,820
processing looks like for people coming

00:09:50,440 --> 00:09:53,620
from a data engineering or a data

00:09:51,820 --> 00:09:57,090
science background you're maybe more

00:09:53,620 --> 00:09:59,200
used to sort of having that mental model

00:09:57,090 --> 00:10:00,760
but there's there's a really good chance

00:09:59,200 --> 00:10:03,280
that even if you think you don't have

00:10:00,760 --> 00:10:05,410
key skew after you've transformed your

00:10:03,280 --> 00:10:07,330
data a few times you you may have

00:10:05,410 --> 00:10:09,280
accidentally brought like a null value

00:10:07,330 --> 00:10:10,930
in as one of your keys and then you're

00:10:09,280 --> 00:10:16,150
gonna end up with this this giant amount

00:10:10,930 --> 00:10:18,070
of key skew so yeah so why do we care so

00:10:16,150 --> 00:10:20,680
when our keys aren't evenly distributed

00:10:18,070 --> 00:10:22,840
group by key will explode but that's not

00:10:20,680 --> 00:10:24,580
really all that hard to make happen like

00:10:22,840 --> 00:10:26,890
we can make group by key explode in a

00:10:24,580 --> 00:10:29,410
lot of perfectly reasonable situations

00:10:26,890 --> 00:10:32,020
but even if we don't have the situation

00:10:29,410 --> 00:10:34,450
where something explodes if we have key

00:10:32,020 --> 00:10:35,950
skew we can really easily get into the

00:10:34,450 --> 00:10:37,990
situation where we have unbalanced

00:10:35,950 --> 00:10:39,970
partitions or we end up with the

00:10:37,990 --> 00:10:41,980
partition which is beyond the the

00:10:39,970 --> 00:10:44,680
partition limit and the shuffle of two

00:10:41,980 --> 00:10:47,650
gigs or even if you know that doesn't

00:10:44,680 --> 00:10:49,450
fail the uneven sharding will give us

00:10:47,650 --> 00:10:51,310
stragglers so we'll end up with you know

00:10:49,450 --> 00:10:54,910
some machines which just have to process

00:10:51,310 --> 00:10:56,590
so much more data that you know they

00:10:54,910 --> 00:11:01,730
take twice as long as the rest of our

00:10:56,590 --> 00:11:06,139
jobs so so group by key is pretty evil

00:11:01,730 --> 00:11:07,850
like this cat yeah pretty evil and what

00:11:06,139 --> 00:11:10,300
it does is it groups all of the records

00:11:07,850 --> 00:11:13,430
with the same key into a single record

00:11:10,300 --> 00:11:15,380
even if right afterwards we perform some

00:11:13,430 --> 00:11:17,899
type of reduction right so a pretty

00:11:15,380 --> 00:11:20,690
common pattern is to group your data

00:11:17,899 --> 00:11:22,639
together and then compute some sort of

00:11:20,690 --> 00:11:26,480
summary statistic on your on your group

00:11:22,639 --> 00:11:28,130
data if we're in sequel everything's

00:11:26,480 --> 00:11:31,220
okay when you win group your data

00:11:28,130 --> 00:11:33,320
together in sequel it actually delays

00:11:31,220 --> 00:11:34,279
the computation and actually waits to

00:11:33,320 --> 00:11:37,250
see what you're doing

00:11:34,279 --> 00:11:38,600
but in regular spark on rdd's when you

00:11:37,250 --> 00:11:41,300
group your data together with group by

00:11:38,600 --> 00:11:44,089
key it it doesn't have that ability to

00:11:41,300 --> 00:11:45,889
see what you're doing next so it ends up

00:11:44,089 --> 00:11:49,370
looking like this and it's very sad and

00:11:45,889 --> 00:11:51,050
our cat gets quite wet on our record is

00:11:49,370 --> 00:11:53,959
is clearly too big to fit on a single

00:11:51,050 --> 00:11:56,360
machine and for people that are

00:11:53,959 --> 00:11:58,570
wondering this fake sample data I like

00:11:56,360 --> 00:12:01,160
to pretend is handlebar mustaches

00:11:58,570 --> 00:12:04,699
ordered by zip code and nine four one

00:12:01,160 --> 00:12:07,670
one zero is the the hipster zip code in

00:12:04,699 --> 00:12:09,350
the city that I'm from so it has too

00:12:07,670 --> 00:12:14,240
many handlebar mustaches to fit on a

00:12:09,350 --> 00:12:15,889
single machine so the correct way so

00:12:14,240 --> 00:12:19,699
what I mean this is the bad way sorry

00:12:15,889 --> 00:12:21,350
this is group by keyword count and the

00:12:19,699 --> 00:12:23,029
the actual normal way to do it is to

00:12:21,350 --> 00:12:27,019
just do this this really simple reduced

00:12:23,029 --> 00:12:29,870
by key right and when we do reduce by

00:12:27,019 --> 00:12:32,990
key spark is actually able to go and do

00:12:29,870 --> 00:12:34,579
sort of a map side reduction and that's

00:12:32,990 --> 00:12:36,260
pretty awesome so essentially before it

00:12:34,579 --> 00:12:38,240
even begins to transfer data over the

00:12:36,260 --> 00:12:40,310
network its able to reduce the data

00:12:38,240 --> 00:12:42,380
locally and then transfer only the

00:12:40,310 --> 00:12:43,579
reduced data over the network and then

00:12:42,380 --> 00:12:46,069
even then it doesn't have to create

00:12:43,579 --> 00:12:48,500
these giant records even if the map size

00:12:46,069 --> 00:12:50,930
reduction doesn't give us anything so

00:12:48,500 --> 00:12:53,959
this is a really simple example of group

00:12:50,930 --> 00:12:55,490
by key on some Python files its

00:12:53,959 --> 00:12:57,829
kilobytes of data because I actually

00:12:55,490 --> 00:12:59,300
wanted the job to succeed but if you

00:12:57,829 --> 00:13:01,940
were working on real data you could just

00:12:59,300 --> 00:13:04,940
replace the K with a G or a T and then

00:13:01,940 --> 00:13:06,740
it would be exactly the same thing and

00:13:04,940 --> 00:13:08,959
we can see it has a shuffled read of 48

00:13:06,740 --> 00:13:10,880
kilobytes and if we did reduce by key

00:13:08,959 --> 00:13:13,029
we'd actually just have a shuffle read

00:13:10,880 --> 00:13:14,529
of only 11 Caleb

00:13:13,029 --> 00:13:18,880
and that's that's a pretty big

00:13:14,529 --> 00:13:20,260
difference so as a general rule if you

00:13:18,880 --> 00:13:21,730
find yourself working in spark and you

00:13:20,260 --> 00:13:23,769
find yourself writing a group by key

00:13:21,730 --> 00:13:25,000
followed by a map values there's a

00:13:23,769 --> 00:13:27,810
really good chance that you want to be

00:13:25,000 --> 00:13:31,000
using reduce by key or aggregate by key

00:13:27,810 --> 00:13:33,459
you get a free map site reduction and as

00:13:31,000 --> 00:13:36,010
a benefit your job doesn't explode which

00:13:33,459 --> 00:13:39,990
is pretty awesome and you get to be

00:13:36,010 --> 00:13:39,990
happy like this cat inside of some bread

00:13:40,560 --> 00:13:44,949
but there is a catch right like group

00:13:42,760 --> 00:13:47,589
like he can be safe some of the time

00:13:44,949 --> 00:13:50,199
it's it's not like just there to make

00:13:47,589 --> 00:13:53,829
your life miserable if you really truly

00:13:50,199 --> 00:13:57,670
have mostly unique keys it's it's pretty

00:13:53,829 --> 00:14:00,850
safe right so you can do this if you if

00:13:57,670 --> 00:14:02,410
you did the readme if you write in

00:14:00,850 --> 00:14:04,930
something where most of the rewards were

00:14:02,410 --> 00:14:07,240
unique and then to do group by key it

00:14:04,930 --> 00:14:08,889
would be okay right and this can

00:14:07,240 --> 00:14:11,649
actually I think this is part of what

00:14:08,889 --> 00:14:13,990
makes group by key kind of the devil is

00:14:11,649 --> 00:14:16,510
it looks okay in our sample test data

00:14:13,990 --> 00:14:18,339
but once we start throwing it at actual

00:14:16,510 --> 00:14:21,220
real-world problems where we have

00:14:18,339 --> 00:14:25,180
missing values if that's when it starts

00:14:21,220 --> 00:14:27,370
to actually explode well we can get

00:14:25,180 --> 00:14:29,740
explosions with key skew without having

00:14:27,370 --> 00:14:32,620
to use group by key right just shuffling

00:14:29,740 --> 00:14:34,680
our data can cause the problem so if we

00:14:32,620 --> 00:14:37,180
return to our handlebar mustache example

00:14:34,680 --> 00:14:39,699
and we we just want to sort this data

00:14:37,180 --> 00:14:42,930
this can be this can be pretty bad right

00:14:39,699 --> 00:14:47,079
so while our records are still

00:14:42,930 --> 00:14:50,170
individually quite small spark isn't

00:14:47,079 --> 00:14:52,029
able to do the split intelligently

00:14:50,170 --> 00:14:55,209
because the key is the same and when we

00:14:52,029 --> 00:14:57,310
need to me need to sort spark just isn't

00:14:55,209 --> 00:15:00,160
able to split if the key isn't changing

00:14:57,310 --> 00:15:02,829
so if you have all of your records with

00:15:00,160 --> 00:15:04,540
one key and here it's it's the mission

00:15:02,829 --> 00:15:07,029
but there's a very good chance that

00:15:04,540 --> 00:15:10,089
you're like one key that has all of the

00:15:07,029 --> 00:15:13,410
records is going to be null at least in

00:15:10,089 --> 00:15:15,880
80% of the datasets I've worked with you

00:15:13,410 --> 00:15:19,389
know you're still gonna have really bad

00:15:15,880 --> 00:15:20,860
performance even if it succeeds so we

00:15:19,389 --> 00:15:22,480
can just add some junk to the end of our

00:15:20,860 --> 00:15:25,390
key and it's really simple and I'll let

00:15:22,480 --> 00:15:25,939
spark do Intelligence splitting and it's

00:15:25,390 --> 00:15:27,439
it's

00:15:25,939 --> 00:15:29,689
actually quite smart right likes part

00:15:27,439 --> 00:15:31,720
samples the data for us and and actually

00:15:29,689 --> 00:15:35,269
comes up with a reasonable set of splits

00:15:31,720 --> 00:15:37,849
when we sort our data which actually

00:15:35,269 --> 00:15:39,649
incidentally means that sort by key is

00:15:37,849 --> 00:15:41,689
like this kind of weird quasi

00:15:39,649 --> 00:15:43,639
transformation quasi action because it

00:15:41,689 --> 00:15:45,559
actually samples your data and that

00:15:43,639 --> 00:15:49,029
involves forcing a computation of your

00:15:45,559 --> 00:15:52,309
data and then it actually does its

00:15:49,029 --> 00:15:54,589
actual shuffle after that so the the

00:15:52,309 --> 00:15:57,379
traditional transformation action

00:15:54,589 --> 00:15:59,449
dichotomy isn't quite as clear as it's

00:15:57,379 --> 00:16:02,569
necessarily made out to be in all of the

00:15:59,449 --> 00:16:04,129
books but yeah so we can we can just add

00:16:02,569 --> 00:16:07,459
some junk to our key and it works pretty

00:16:04,129 --> 00:16:10,249
well and actually and here's our first

00:16:07,459 --> 00:16:12,709
David Hasselhoff I'm the in the top

00:16:10,249 --> 00:16:15,519
corner I think or at least what Google

00:16:12,709 --> 00:16:18,679
Images told me was David Hasselhoff and

00:16:15,519 --> 00:16:21,079
we've got some fire too for good measure

00:16:18,679 --> 00:16:24,229
so their shuffle actually has a bit of

00:16:21,079 --> 00:16:25,849
cool magic inside of it and it's sort of

00:16:24,229 --> 00:16:28,039
an artifact of the way how the shuffle

00:16:25,849 --> 00:16:29,599
is implemented in spark when we shuffle

00:16:28,039 --> 00:16:32,359
our data sparked actually creates these

00:16:29,599 --> 00:16:34,849
temporary shuffle files and now I mean

00:16:32,359 --> 00:16:38,029
when you hear temporary files you don't

00:16:34,849 --> 00:16:40,220
normally think awesome performance you

00:16:38,029 --> 00:16:42,589
normally think really shitty performance

00:16:40,220 --> 00:16:44,899
and in general that's that's actually

00:16:42,589 --> 00:16:46,669
probably true but for the shuffle files

00:16:44,899 --> 00:16:48,199
they tend to actually just stay in the

00:16:46,669 --> 00:16:51,559
OS page cache because they're not all

00:16:48,199 --> 00:16:53,809
that big but we can actually reuse the

00:16:51,559 --> 00:16:56,449
shuffle files so if we have some really

00:16:53,809 --> 00:16:58,459
complex shuffle we don't need to

00:16:56,449 --> 00:17:01,629
explicitly cache our data at that point

00:16:58,459 --> 00:17:05,240
right if we just keep that RDD around we

00:17:01,629 --> 00:17:07,339
continue to reuse that rdd's shuffle

00:17:05,240 --> 00:17:09,709
files and this gives us a pretty similar

00:17:07,339 --> 00:17:14,240
performance characteristic too as if we

00:17:09,709 --> 00:17:16,759
had cached it to disk so this can

00:17:14,240 --> 00:17:19,610
actually be quite good the only sort of

00:17:16,759 --> 00:17:24,139
downside of this is these shuffle files

00:17:19,610 --> 00:17:26,689
aren't really intelligently managed so

00:17:24,139 --> 00:17:29,000
Sparx caching is is quite smart right it

00:17:26,689 --> 00:17:32,360
uses the the pretty standard LRU caching

00:17:29,000 --> 00:17:35,119
algorithm but for shuffle files it just

00:17:32,360 --> 00:17:37,730
leaves them there until the garbage

00:17:35,119 --> 00:17:39,059
collection on your driver program is

00:17:37,730 --> 00:17:40,830
triggered

00:17:39,059 --> 00:17:42,870
and if your driver program has a whole

00:17:40,830 --> 00:17:44,820
bunch of memory assigned to it that

00:17:42,870 --> 00:17:46,410
really there's no relationship to the

00:17:44,820 --> 00:17:48,990
amount of disk space that you have on

00:17:46,410 --> 00:17:51,390
your workers so one of the things which

00:17:48,990 --> 00:17:53,309
you might find yourself encountering if

00:17:51,390 --> 00:17:56,600
you start keeping a whole bunch of RTD

00:17:53,309 --> 00:17:58,770
references around is that you know

00:17:56,600 --> 00:18:00,660
you've got all of these shuffle files

00:17:58,770 --> 00:18:03,030
and you should you need to clean them up

00:18:00,660 --> 00:18:05,820
and you can do this by triggering a GC

00:18:03,030 --> 00:18:08,250
event on your worker program just

00:18:05,820 --> 00:18:10,950
manually triggering the GC event or if

00:18:08,250 --> 00:18:13,289
you feel like using sparks internal API

00:18:10,950 --> 00:18:15,510
s and lying about being inside of

00:18:13,289 --> 00:18:17,280
package org Apache spark there's there's

00:18:15,510 --> 00:18:21,990
another way to clean up shuffle files in

00:18:17,280 --> 00:18:23,760
there but it'll probably change in

00:18:21,990 --> 00:18:26,280
between each version so be prepared to

00:18:23,760 --> 00:18:27,299
rewrite your code several times you

00:18:26,280 --> 00:18:29,039
probably don't want to do that you

00:18:27,299 --> 00:18:35,570
probably just want to trigger a GC event

00:18:29,039 --> 00:18:38,429
it's it's much easier so cat more cats

00:18:35,570 --> 00:18:42,230
so iterator to iterator transformations

00:18:38,429 --> 00:18:45,299
so how many people use map partitions

00:18:42,230 --> 00:18:48,030
okay cool a good number of people so map

00:18:45,299 --> 00:18:50,280
partitions is really great it gives us a

00:18:48,030 --> 00:18:53,039
lot of control and this is this is

00:18:50,280 --> 00:18:55,500
really useful if we're doing things like

00:18:53,039 --> 00:18:57,120
setting up database connections or

00:18:55,500 --> 00:18:59,070
essentially just doing crazy things

00:18:57,120 --> 00:19:00,690
inside of our spark jobs right like

00:18:59,070 --> 00:19:02,700
maybe we're making a PRNG

00:19:00,690 --> 00:19:04,860
for each partition or we're doing

00:19:02,700 --> 00:19:08,190
something a little outside of the norm

00:19:04,860 --> 00:19:10,770
right the one problem with map

00:19:08,190 --> 00:19:13,320
partitions is that it also trusts us to

00:19:10,770 --> 00:19:14,909
not be idiots and when people trust me

00:19:13,320 --> 00:19:18,750
not to be an idiot that's normally when

00:19:14,909 --> 00:19:21,840
my code fails because I'm not super

00:19:18,750 --> 00:19:24,299
attentive the default transformations

00:19:21,840 --> 00:19:27,900
are actually implemented using the

00:19:24,299 --> 00:19:30,299
mapped partitions RDD so map filter flat

00:19:27,900 --> 00:19:33,210
map many of these are implemented using

00:19:30,299 --> 00:19:37,320
the same RDD that map partitions is in

00:19:33,210 --> 00:19:39,510
but if we accidentally implicitly

00:19:37,320 --> 00:19:42,840
convert our iterator into a list or

00:19:39,510 --> 00:19:44,970
really pretty much anything we can force

00:19:42,840 --> 00:19:47,159
part to load our entire partition into

00:19:44,970 --> 00:19:49,710
memory and then we've also made it so

00:19:47,159 --> 00:19:52,290
that spark can't spill to disk as it's

00:19:49,710 --> 00:19:52,800
processing the partition and this is

00:19:52,290 --> 00:19:54,660
this is on

00:19:52,800 --> 00:19:56,640
fortunate because sometimes you actually

00:19:54,660 --> 00:19:58,530
want to process more data per partition

00:19:56,640 --> 00:20:00,180
than is reasonable to hold in memory

00:19:58,530 --> 00:20:02,610
right you actually want to be able to

00:20:00,180 --> 00:20:04,770
spill the disk as you're going so when

00:20:02,610 --> 00:20:06,800
you're using map partitions it's really

00:20:04,770 --> 00:20:09,150
important to be careful that you don't

00:20:06,800 --> 00:20:10,830
implicitly or otherwise convert your

00:20:09,150 --> 00:20:15,150
iterator into something which forces

00:20:10,830 --> 00:20:16,980
spark to keep it in memory so the TLDR

00:20:15,150 --> 00:20:19,620
and here's our second david hasselhoff

00:20:16,980 --> 00:20:22,650
at least I think this is what Google

00:20:19,620 --> 00:20:24,450
Images told me doing the splits the TLDR

00:20:22,650 --> 00:20:26,880
is be careful when you're using map

00:20:24,450 --> 00:20:28,620
partitions because it can explode and

00:20:26,880 --> 00:20:32,730
then you'll have to do the splits in

00:20:28,620 --> 00:20:34,740
front of a fiery building yeah sorry for

00:20:32,730 --> 00:20:37,230
the david hasselhoff jokes it seems like

00:20:34,740 --> 00:20:41,100
there's two people that like them well

00:20:37,230 --> 00:20:43,800
maybe three because I like them okay

00:20:41,100 --> 00:20:46,680
sorry we're gonna move on to spark

00:20:43,800 --> 00:20:49,650
sequel which has decidedly less David

00:20:46,680 --> 00:20:51,480
Hasselhoff but how many people are

00:20:49,650 --> 00:20:54,810
actually already using data frames or

00:20:51,480 --> 00:20:56,370
data sets okay cool that's that's a

00:20:54,810 --> 00:20:57,930
that's a good percentage for the for

00:20:56,370 --> 00:21:00,300
those of you that aren't already using

00:20:57,930 --> 00:21:02,910
data frames or data sets I'm gonna go

00:21:00,300 --> 00:21:06,150
into saleswomen mode even though I

00:21:02,910 --> 00:21:07,770
receive zero dollars for getting you to

00:21:06,150 --> 00:21:11,040
use spark sequel but I think you really

00:21:07,770 --> 00:21:12,690
should be considering it if you're okay

00:21:11,040 --> 00:21:14,850
with some of the constraints that spark

00:21:12,690 --> 00:21:17,070
sequel provides it gives you a whole

00:21:14,850 --> 00:21:20,790
bunch of power for free it gives you

00:21:17,070 --> 00:21:22,920
really superfast serializers and not

00:21:20,790 --> 00:21:25,380
only are the serializers fast there are

00:21:22,920 --> 00:21:28,290
also more space efficient the results

00:21:25,380 --> 00:21:31,530
are much more compact and the serialized

00:21:28,290 --> 00:21:32,910
values can even have certain operations

00:21:31,530 --> 00:21:35,340
performed directly on the serialized

00:21:32,910 --> 00:21:37,700
values so if you remember Hadoop

00:21:35,340 --> 00:21:39,990
sortable from back in your Hadoop days

00:21:37,700 --> 00:21:41,760
you can essentially like sort your data

00:21:39,990 --> 00:21:43,710
without having to deserialize and we

00:21:41,760 --> 00:21:44,220
serialize your data which is pretty

00:21:43,710 --> 00:21:46,230
useful

00:21:44,220 --> 00:21:49,860
right like serialization is is fairly

00:21:46,230 --> 00:21:51,840
expensive just checking are there any

00:21:49,860 --> 00:21:55,220
heretics in the room that used Python or

00:21:51,840 --> 00:21:58,710
R and and I am one of those people

00:21:55,220 --> 00:22:01,260
sometimes okay there's two we should be

00:21:58,710 --> 00:22:04,710
friends or at least keep the rest of

00:22:01,260 --> 00:22:06,580
them from murdering me but for people

00:22:04,710 --> 00:22:10,360
that occasionally work in Python

00:22:06,580 --> 00:22:13,330
or non jvm languages data frames are

00:22:10,360 --> 00:22:15,850
incredibly useful because working with

00:22:13,330 --> 00:22:18,610
rdd's in non jvm languages pays this

00:22:15,850 --> 00:22:22,750
really expensive serialization cost it's

00:22:18,610 --> 00:22:25,360
pretty young ungodly so yeah why is

00:22:22,750 --> 00:22:27,720
sparks evil good and I'm gonna get some

00:22:25,360 --> 00:22:27,720
coffee

00:22:32,250 --> 00:22:36,909
all right so sparks people is is really

00:22:35,110 --> 00:22:39,789
good because it has this really

00:22:36,909 --> 00:22:41,799
efficient representation it's also great

00:22:39,789 --> 00:22:44,409
because the cached representation gives

00:22:41,799 --> 00:22:46,120
us a calmer Kashuk representation so if

00:22:44,409 --> 00:22:48,159
we only need to work on subsets of our

00:22:46,120 --> 00:22:51,190
data at a time we don't have to transfer

00:22:48,159 --> 00:22:53,110
huge amounts of it around another really

00:22:51,190 --> 00:22:55,360
awesome thing is the ability to push

00:22:53,110 --> 00:22:57,250
down operations to the datastore level

00:22:55,360 --> 00:22:59,200
and we can always do this when we're

00:22:57,250 --> 00:23:01,210
working with rdd's but we have to like

00:22:59,200 --> 00:23:03,370
do it ourselves and remember to do it

00:23:01,210 --> 00:23:06,700
ourselves and that's a lot of work and

00:23:03,370 --> 00:23:08,490
I'm pretty lazy so if you load your data

00:23:06,700 --> 00:23:10,600
and then you immediately filter it

00:23:08,490 --> 00:23:12,130
there's there's a good chance that the

00:23:10,600 --> 00:23:14,340
datastore that you're reading from might

00:23:12,130 --> 00:23:17,380
actually be able to handle that filter

00:23:14,340 --> 00:23:18,970
more efficiently than spark can and with

00:23:17,380 --> 00:23:20,980
sports sequel we don't have to like

00:23:18,970 --> 00:23:23,200
manually think through is the filter

00:23:20,980 --> 00:23:26,470
that I can push down or not it'll just

00:23:23,200 --> 00:23:28,059
figure it out for us and really the the

00:23:26,470 --> 00:23:29,529
more awesome thing that that makes Park

00:23:28,059 --> 00:23:31,179
sequel really fast is it can tell the

00:23:29,529 --> 00:23:33,639
difference between what we're asking it

00:23:31,179 --> 00:23:35,440
to do right in most of SPARC we're

00:23:33,639 --> 00:23:38,590
giving it arbitrary Scala lambda

00:23:35,440 --> 00:23:40,419
expressions and right now spart just

00:23:38,590 --> 00:23:42,519
treats those as sort of arbitrary blobs

00:23:40,419 --> 00:23:44,740
and doesn't do any introspection on them

00:23:42,519 --> 00:23:47,169
but with spark sequel we provide it with

00:23:44,740 --> 00:23:49,179
these types equal expressions and it's

00:23:47,169 --> 00:23:51,309
actually able to look at them figure out

00:23:49,179 --> 00:23:55,630
what the hell we're asking it to do and

00:23:51,309 --> 00:23:57,580
do intelligent optimizations and so how

00:23:55,630 --> 00:23:59,769
much faster can it be it can be

00:23:57,580 --> 00:24:01,899
incredibly faster if our baseline is

00:23:59,769 --> 00:24:03,669
group by T pretty much everything is

00:24:01,899 --> 00:24:05,049
better than group by ki but that's like

00:24:03,669 --> 00:24:07,539
saying everything's better than the tire

00:24:05,049 --> 00:24:10,960
fire so it's it's not a very high bar

00:24:07,539 --> 00:24:13,210
but it's it's even better than reduced

00:24:10,960 --> 00:24:14,860
by ki right and and reduced by ki is

00:24:13,210 --> 00:24:17,139
normally what we would replace our

00:24:14,860 --> 00:24:20,260
terribly written group by key piece of

00:24:17,139 --> 00:24:22,630
code with our data frame operation

00:24:20,260 --> 00:24:24,670
can use to be faster even once the JIT

00:24:22,630 --> 00:24:26,920
has had a chance to kick in and optimize

00:24:24,670 --> 00:24:28,660
our lambda expression we can see it

00:24:26,920 --> 00:24:31,180
continues to be faster for future

00:24:28,660 --> 00:24:33,160
iterations and this is because not only

00:24:31,180 --> 00:24:35,950
is the generated code really efficient

00:24:33,160 --> 00:24:38,200
but are on the wire representation of

00:24:35,950 --> 00:24:40,060
our data is is much smaller so our

00:24:38,200 --> 00:24:45,130
shuffling and network costs are really

00:24:40,060 --> 00:24:48,100
low but with every good thing right like

00:24:45,130 --> 00:24:49,960
all of this awesome performance there is

00:24:48,100 --> 00:24:51,700
there has to be some catches right at

00:24:49,960 --> 00:24:53,590
some point it's going to explode and

00:24:51,700 --> 00:24:58,060
bite us in places where we would rather

00:24:53,590 --> 00:24:59,650
not be bitten and for sports sequel this

00:24:58,060 --> 00:25:02,260
is pretty much if you have an iterative

00:24:59,650 --> 00:25:03,640
algorithm it's gonna explode it's not

00:25:02,260 --> 00:25:04,990
gonna work very well so if you're trying

00:25:03,640 --> 00:25:06,760
to do some machine learning on top of

00:25:04,990 --> 00:25:09,670
spark sequel you're gonna have a bad

00:25:06,760 --> 00:25:12,310
time there's some hacks that we can do

00:25:09,670 --> 00:25:14,920
and I'll show one of them some push

00:25:12,310 --> 00:25:17,320
downs don't always work and this is

00:25:14,920 --> 00:25:19,510
unfortunate so while I said we don't

00:25:17,320 --> 00:25:21,250
have to think about it occasionally we

00:25:19,510 --> 00:25:24,100
do have to think about which which push

00:25:21,250 --> 00:25:26,620
downs we care about and crazily enough

00:25:24,100 --> 00:25:29,680
the default shuffle size is ridiculously

00:25:26,620 --> 00:25:32,080
small it's 200 partitions but it's okay

00:25:29,680 --> 00:25:33,700
we can we can just bump that up and if

00:25:32,080 --> 00:25:35,650
you're working in spark by now you've

00:25:33,700 --> 00:25:38,410
figured out that a lot of the default

00:25:35,650 --> 00:25:41,250
configuration values are not things for

00:25:38,410 --> 00:25:44,740
you you want to be using something else

00:25:41,250 --> 00:25:46,060
so the lineage explosion problem for

00:25:44,740 --> 00:25:48,340
when you're doing an iterative algorithm

00:25:46,060 --> 00:25:50,560
on top of data frames is really easy to

00:25:48,340 --> 00:25:52,450
fix it's just kind of weird that we have

00:25:50,560 --> 00:25:54,430
to fix it this way we can essentially

00:25:52,450 --> 00:25:57,490
round-trip our data through an RDD and

00:25:54,430 --> 00:25:59,350
then spark steeples optimizer just like

00:25:57,490 --> 00:26:02,050
isn't able to go up above this and it

00:25:59,350 --> 00:26:04,330
just looks at this RDD picture and and

00:26:02,050 --> 00:26:07,360
tries to only optimize the smaller query

00:26:04,330 --> 00:26:08,800
plan and this works really well this is

00:26:07,360 --> 00:26:12,780
actually what's done inside of graph

00:26:08,800 --> 00:26:15,940
frames has anyone using graph frames

00:26:12,780 --> 00:26:18,430
okay cool well if you were this is this

00:26:15,940 --> 00:26:20,350
is how graph frames handle the the large

00:26:18,430 --> 00:26:22,470
lineage problem with data frames inside

00:26:20,350 --> 00:26:25,090
of sparkly they just cut the lineage and

00:26:22,470 --> 00:26:27,460
it means the optimizer can't do some

00:26:25,090 --> 00:26:29,680
certain awesome things but it also means

00:26:27,460 --> 00:26:32,380
that our code actually runs and and

00:26:29,680 --> 00:26:34,910
that's probably more important

00:26:32,380 --> 00:26:41,150
how many people are using the data set

00:26:34,910 --> 00:26:44,390
API three okay cool so this is exciting

00:26:41,150 --> 00:26:47,030
I really like sparks equal I like data

00:26:44,390 --> 00:26:49,730
frames but part of the reason why I like

00:26:47,030 --> 00:26:51,920
Scala is I actually really like strongly

00:26:49,730 --> 00:26:55,070
typed languages and data frames are just

00:26:51,920 --> 00:26:58,610
this blob of data which has runtime type

00:26:55,070 --> 00:27:01,190
information which is useful but not why

00:26:58,610 --> 00:27:03,590
I like Scala right and data sets

00:27:01,190 --> 00:27:06,440
actually give me compile time type

00:27:03,590 --> 00:27:07,700
information among many other things but

00:27:06,440 --> 00:27:10,340
I think the compile time type

00:27:07,700 --> 00:27:12,440
information is really really important

00:27:10,340 --> 00:27:17,480
how many people enjoy writing

00:27:12,440 --> 00:27:21,920
user-defined functions for hive that is

00:27:17,480 --> 00:27:23,690
no one and the first laughs yeah okay

00:27:21,920 --> 00:27:25,160
yeah there was once someone that claimed

00:27:23,690 --> 00:27:29,630
to like it but I think they were

00:27:25,160 --> 00:27:31,190
probably drunk so data sets get us away

00:27:29,630 --> 00:27:33,020
from having to write user-defined

00:27:31,190 --> 00:27:35,360
functions and they actually just they

00:27:33,020 --> 00:27:37,190
let us write the code that we're used to

00:27:35,360 --> 00:27:38,840
writing we can provide lambda is when we

00:27:37,190 --> 00:27:40,370
want to and then we can provide sequel

00:27:38,840 --> 00:27:42,170
expressions when we're it's easier to

00:27:40,370 --> 00:27:44,030
write sequel and we don't have to have

00:27:42,170 --> 00:27:47,210
this mental model of like mixing the two

00:27:44,030 --> 00:27:49,820
it just takes a care of it for us if

00:27:47,210 --> 00:27:51,740
you're working in spark 1:6 right now

00:27:49,820 --> 00:27:54,860
and because spark 200 is and out you

00:27:51,740 --> 00:27:56,570
probably are you will have to change

00:27:54,860 --> 00:27:59,840
your code if you start using data sets

00:27:56,570 --> 00:28:02,660
because the API is changing for spark -

00:27:59,840 --> 00:28:04,460
oh that being said the API for pretty

00:28:02,660 --> 00:28:06,110
much everything is changing anyway is

00:28:04,460 --> 00:28:08,690
for spark - OH

00:28:06,110 --> 00:28:12,590
at least in small ways so I don't think

00:28:08,690 --> 00:28:14,330
this is too big of a deal so here's an

00:28:12,590 --> 00:28:17,810
example actually let's just skip ahead

00:28:14,330 --> 00:28:21,230
to the annotated example of how to use a

00:28:17,810 --> 00:28:23,900
mixture of sequel type expressions with

00:28:21,230 --> 00:28:27,140
functional type expressions and this

00:28:23,900 --> 00:28:29,720
isn't super exciting and there's there's

00:28:27,140 --> 00:28:31,700
an artifact here because I actually

00:28:29,720 --> 00:28:34,100
wanted my example to compile and spark

00:28:31,700 --> 00:28:35,660
1:6 where I have to take my data set and

00:28:34,100 --> 00:28:38,630
turn it into a data frame and then back

00:28:35,660 --> 00:28:40,850
but in spark - oh we wouldn't have to

00:28:38,630 --> 00:28:43,100
actually go to a data frame and come

00:28:40,850 --> 00:28:45,320
back or or specify whether what our type

00:28:43,100 --> 00:28:46,910
was there it would just magically work

00:28:45,320 --> 00:28:50,660
and we could intermix these two really

00:28:46,910 --> 00:28:52,940
nicely we can also do functional style

00:28:50,660 --> 00:28:56,150
Maps and now we've got boo again who is

00:28:52,940 --> 00:28:57,950
not David Hasselhoff but is very cute so

00:28:56,150 --> 00:29:01,940
we can intermix functional style Maps

00:28:57,950 --> 00:29:04,460
and the data set types work out for us

00:29:01,940 --> 00:29:07,130
right we're taking in a data set of this

00:29:04,460 --> 00:29:10,340
case class called raw Panda and I'm

00:29:07,130 --> 00:29:11,930
doing some random arbitrary lambda

00:29:10,340 --> 00:29:14,420
function and I'm getting back a data set

00:29:11,930 --> 00:29:15,950
of doubles and this is really cool

00:29:14,420 --> 00:29:18,380
because I get all of the really awesome

00:29:15,950 --> 00:29:20,330
bits of sparks equal without having to

00:29:18,380 --> 00:29:23,270
sort of constrict myself to sequel

00:29:20,330 --> 00:29:26,060
expressions right because I don't really

00:29:23,270 --> 00:29:29,540
like writing sequel expressions compared

00:29:26,060 --> 00:29:31,760
to writing lambda functions I'm actually

00:29:29,540 --> 00:29:36,320
pretty bad with sequel so I really like

00:29:31,760 --> 00:29:38,240
this so switching gears entirely let's

00:29:36,320 --> 00:29:40,790
let's look a bit at how to validate our

00:29:38,240 --> 00:29:43,100
spark jobs everyone here already tests

00:29:40,790 --> 00:29:44,630
their spark jobs right is there anyone

00:29:43,100 --> 00:29:48,980
in the room who doesn't test their spark

00:29:44,630 --> 00:29:50,600
jobs good okay is there anyone in the

00:29:48,980 --> 00:29:54,740
room that doesn't validate their spark

00:29:50,600 --> 00:29:58,040
jobs okay at least two people how many

00:29:54,740 --> 00:29:59,540
people in the room don't know what I'm

00:29:58,040 --> 00:30:04,130
saying when I'm saying validate your

00:29:59,540 --> 00:30:05,510
spark jobs Hey okay there we go that's

00:30:04,130 --> 00:30:16,970
more what I was expecting

00:30:05,510 --> 00:30:19,670
okay lovely lovely I need some coffee so

00:30:16,970 --> 00:30:22,610
we're all good software people right we

00:30:19,670 --> 00:30:24,380
know we should test our code and we also

00:30:22,610 --> 00:30:26,900
know that our tests aren't always enough

00:30:24,380 --> 00:30:28,940
right because we can have our entire

00:30:26,900 --> 00:30:31,100
test suite pass and we actually just

00:30:28,940 --> 00:30:33,860
produce garbage results right now

00:30:31,100 --> 00:30:35,120
hopefully our tests catch everything but

00:30:33,860 --> 00:30:37,040
there's a good chance that your tests

00:30:35,120 --> 00:30:39,080
aren't able to cover all of the myriad

00:30:37,040 --> 00:30:43,130
of ways the real world is able to

00:30:39,080 --> 00:30:44,840
 up it is just ridiculous the kinds

00:30:43,130 --> 00:30:47,570
of things that you will show up in your

00:30:44,840 --> 00:30:51,310
data right you'll have names which are

00:30:47,570 --> 00:30:54,140
somehow encoded as the string and null

00:30:51,310 --> 00:30:56,540
you'll have like just all sorts of

00:30:54,140 --> 00:30:57,260
random crazy stuff and it will change

00:30:56,540 --> 00:30:59,149
right

00:30:57,260 --> 00:31:01,519
your upstream data providers

00:30:59,149 --> 00:31:03,440
you know they're giving you data in one

00:31:01,519 --> 00:31:05,570
format today and tomorrow they might

00:31:03,440 --> 00:31:08,269
just drop a field or rename a field or

00:31:05,570 --> 00:31:10,849
even worse repurpose a field right they

00:31:08,269 --> 00:31:14,749
might decide well we have this field for

00:31:10,849 --> 00:31:16,190
for the users income but I'm gonna I'm

00:31:14,749 --> 00:31:20,239
gonna change it to some of some sort of

00:31:16,190 --> 00:31:21,589
categorical data and your tests aren't

00:31:20,239 --> 00:31:22,849
going to be able to catch all of the

00:31:21,589 --> 00:31:24,889
things that are gonna change in the real

00:31:22,849 --> 00:31:28,369
world so I think it's really important

00:31:24,889 --> 00:31:30,440
to introspect our spark jobs while

00:31:28,369 --> 00:31:32,839
they're running to try and figure out if

00:31:30,440 --> 00:31:35,179
they're performing reasonably so that

00:31:32,839 --> 00:31:36,769
when our spark job is done we actually

00:31:35,179 --> 00:31:39,259
know if we can trust the results that

00:31:36,769 --> 00:31:41,389
it's produced right because it's way

00:31:39,259 --> 00:31:43,580
better to not push out a new

00:31:41,389 --> 00:31:45,019
recommendation model and it is to push

00:31:43,580 --> 00:31:46,909
out a recommendation model that

00:31:45,019 --> 00:31:49,580
recommends hugely inappropriate things

00:31:46,909 --> 00:31:51,710
to people and I say this is someone that

00:31:49,580 --> 00:31:52,759
has pushed out a recommendation model

00:31:51,710 --> 00:31:54,799
that has recommended hugely

00:31:52,759 --> 00:31:57,080
inappropriate things to people that was

00:31:54,799 --> 00:31:58,639
a really terrible 2:00 a.m. phone call I

00:31:57,080 --> 00:32:00,889
don't want to have another one of those

00:31:58,639 --> 00:32:03,169
it's not good and my tests weren't going

00:32:00,889 --> 00:32:04,729
to be able to catch that so there's a

00:32:03,169 --> 00:32:07,849
whole bunch of things we can do to sort

00:32:04,729 --> 00:32:09,739
of validate our spark jobs we can

00:32:07,849 --> 00:32:11,450
validate if we're having to drop records

00:32:09,739 --> 00:32:14,179
especially if you're working with JSON

00:32:11,450 --> 00:32:15,830
data there's a good chance that you know

00:32:14,179 --> 00:32:17,509
some of your records aren't going to

00:32:15,830 --> 00:32:19,489
match whatever schema you were expecting

00:32:17,509 --> 00:32:21,109
and your you need to drop some of them

00:32:19,489 --> 00:32:22,460
and that's not the end of the world

00:32:21,109 --> 00:32:24,469
right you don't want to stop your

00:32:22,460 --> 00:32:26,719
program just because you encountered one

00:32:24,469 --> 00:32:29,359
invalid record but if you encountered

00:32:26,719 --> 00:32:30,919
like 90% invalid records there's a good

00:32:29,359 --> 00:32:33,830
chance you really don't want to continue

00:32:30,919 --> 00:32:35,719
and something has gone pretty wrong if

00:32:33,830 --> 00:32:39,200
your job normally takes an hour to run

00:32:35,719 --> 00:32:41,359
and it completed in 20 minutes today I

00:32:39,200 --> 00:32:44,210
mean either you upgraded your cluster or

00:32:41,359 --> 00:32:45,859
something has gone kind of wrong and you

00:32:44,210 --> 00:32:48,289
should probably double-check before you

00:32:45,859 --> 00:32:50,330
deploy your model to production and

00:32:48,289 --> 00:32:53,419
start like trading on the stock market

00:32:50,330 --> 00:32:55,099
with it so there's there's a lot of

00:32:53,419 --> 00:32:57,649
really easy simple rules we can write

00:32:55,099 --> 00:32:59,779
right we can say like my execution time

00:32:57,649 --> 00:33:02,450
should be similar to my execution time

00:32:59,779 --> 00:33:04,309
over the past year we can say that the

00:33:02,450 --> 00:33:07,460
number of users should be monotonically

00:33:04,309 --> 00:33:09,589
increasing or if we're at a company that

00:33:07,460 --> 00:33:11,329
isn't doing so well the number of users

00:33:09,589 --> 00:33:12,570
should not be be decreasing by more than

00:33:11,329 --> 00:33:14,250
50 percent per day

00:33:12,570 --> 00:33:19,530
and if it is I'm gonna be interviewing

00:33:14,250 --> 00:33:22,020
anyway so it doesn't matter yeah so we

00:33:19,530 --> 00:33:24,330
can use accumulators to encode some of

00:33:22,020 --> 00:33:26,910
this information it's a pretty

00:33:24,330 --> 00:33:28,890
convenient way of keeping track of sort

00:33:26,910 --> 00:33:31,170
of incidental things inside of your

00:33:28,890 --> 00:33:34,410
transformations right you don't want to

00:33:31,170 --> 00:33:36,000
always return an option from everything

00:33:34,410 --> 00:33:38,760
to indicate the potential of a failed

00:33:36,000 --> 00:33:40,680
record and count at each stage but you

00:33:38,760 --> 00:33:42,810
do want to get an idea of you know if

00:33:40,680 --> 00:33:46,110
you've encountered an exceptional state

00:33:42,810 --> 00:33:49,770
the problem is accumulators are kind of

00:33:46,110 --> 00:33:53,700
flaky and it's mostly the interplay

00:33:49,770 --> 00:33:56,460
between caching and node failures where

00:33:53,700 --> 00:33:59,700
it can get really really funky but if

00:33:56,460 --> 00:34:01,470
you use relative rules it's pretty ok so

00:33:59,700 --> 00:34:04,680
because it generally tends to me to

00:34:01,470 --> 00:34:06,360
recompute a chunk of data at a time but

00:34:04,680 --> 00:34:08,130
essentially don't write rules that are

00:34:06,360 --> 00:34:11,220
like if I encounter a thousand records

00:34:08,130 --> 00:34:13,700
that are invalid stop that's not a good

00:34:11,220 --> 00:34:16,620
rule right a job that says if more than

00:34:13,700 --> 00:34:19,320
k percent of my data is invalid stop

00:34:16,620 --> 00:34:22,560
because it might accidentally recompute

00:34:19,320 --> 00:34:24,810
some of your data several times so this

00:34:22,560 --> 00:34:27,120
is this is a really simple example where

00:34:24,810 --> 00:34:28,440
we create two accumulators and we keep

00:34:27,120 --> 00:34:31,290
track of the number of things that we're

00:34:28,440 --> 00:34:33,600
throwing out and we say you know if bad

00:34:31,290 --> 00:34:36,179
dot value is higher than ten percent of

00:34:33,600 --> 00:34:39,750
our okay value stop we we don't want to

00:34:36,179 --> 00:34:41,700
use our results in production if you

00:34:39,750 --> 00:34:44,130
think this is cool there's there's an

00:34:41,700 --> 00:34:46,440
early stage project to make it easier to

00:34:44,130 --> 00:34:48,870
write these rules especially the ones

00:34:46,440 --> 00:34:52,650
that look at a historic data so how your

00:34:48,870 --> 00:34:54,750
jobs performed in the past year or you

00:34:52,650 --> 00:34:56,790
know six months or whatever but it's

00:34:54,750 --> 00:34:59,370
very early stage and you shouldn't use

00:34:56,790 --> 00:35:02,250
it in production but I think it can be

00:34:59,370 --> 00:35:04,770
kind of a neat project and I also have a

00:35:02,250 --> 00:35:07,380
PR out in SPARC to try and make a human

00:35:04,770 --> 00:35:09,000
as reasonable so we don't have to limit

00:35:07,380 --> 00:35:13,590
ourselves to these relative rules

00:35:09,000 --> 00:35:16,260
it's SPARC PR one one one zero five and

00:35:13,590 --> 00:35:18,150
the JIRA is one two four six nine if

00:35:16,260 --> 00:35:20,820
anyone happens to be sort of interested

00:35:18,150 --> 00:35:22,800
in seeing why accumulators behave kind

00:35:20,820 --> 00:35:24,270
of weird and what we can do about this

00:35:22,800 --> 00:35:28,560
these are two two

00:35:24,270 --> 00:35:30,270
things so for now some of the most

00:35:28,560 --> 00:35:32,190
common validation rules that people use

00:35:30,270 --> 00:35:34,620
on their jobs are just checking the file

00:35:32,190 --> 00:35:37,470
sizes of their outputs checking the

00:35:34,620 --> 00:35:39,330
execution time checking the file sizes

00:35:37,470 --> 00:35:41,340
of their inputs the number of Records

00:35:39,330 --> 00:35:43,290
read these are all internal counters

00:35:41,340 --> 00:35:45,480
this bar keeps track of that we can we

00:35:43,290 --> 00:35:47,160
can access and the spark validator

00:35:45,480 --> 00:35:49,050
package shows you how to do this if

00:35:47,160 --> 00:35:50,580
you're if you're so inclined

00:35:49,050 --> 00:35:52,620
but you shouldn't actually use the

00:35:50,580 --> 00:35:57,990
validator package we should just look at

00:35:52,620 --> 00:35:59,640
it and copy the parts of it you want and

00:35:57,990 --> 00:36:03,420
yeah so here's an example of using the

00:35:59,640 --> 00:36:05,490
validator package it's a thing on a

00:36:03,420 --> 00:36:08,030
personal note I think it's okay to have

00:36:05,490 --> 00:36:11,250
validation rules that sometimes misfire

00:36:08,030 --> 00:36:12,870
just like don't throw away the data just

00:36:11,250 --> 00:36:15,630
keep it and don't push it to production

00:36:12,870 --> 00:36:18,000
so you can investigate what's gone on

00:36:15,630 --> 00:36:19,920
and then manually figure out if it's

00:36:18,000 --> 00:36:21,420
okay and you know if it's it's a

00:36:19,920 --> 00:36:23,820
reasonable time to add an exception

00:36:21,420 --> 00:36:25,230
maybe you you know your company did

00:36:23,820 --> 00:36:27,930
something terrible and you lost half

00:36:25,230 --> 00:36:29,730
your users yesterday so this is actually

00:36:27,930 --> 00:36:37,890
expected that your job processing time

00:36:29,730 --> 00:36:40,710
went down let me do it on time oh cool I

00:36:37,890 --> 00:36:42,690
don't have a lot of time how many people

00:36:40,710 --> 00:36:44,700
really care about actually everyone was

00:36:42,690 --> 00:36:46,680
already testing their spark code right

00:36:44,700 --> 00:36:49,970
everyone promises me that they were

00:36:46,680 --> 00:36:55,830
already testing their spark code okay

00:36:49,970 --> 00:37:02,520
lovely but see if this No damn it stupid

00:36:55,830 --> 00:37:04,680
Google okay yeah if anyone really cares

00:37:02,520 --> 00:37:06,090
about testing it'll be in the slides and

00:37:04,680 --> 00:37:08,850
I would love to chat with people about

00:37:06,090 --> 00:37:10,680
testing especially there's some really

00:37:08,850 --> 00:37:14,520
cool stuff everyone here knows about

00:37:10,680 --> 00:37:15,870
Scala check right yes okay good if you

00:37:14,520 --> 00:37:17,520
don't know about Scala check come talk

00:37:15,870 --> 00:37:21,180
to me it's awesome you should be using

00:37:17,520 --> 00:37:23,550
it with spark it's really cool but let's

00:37:21,180 --> 00:37:25,860
go ahead and and sort of just wrap up

00:37:23,550 --> 00:37:27,330
and do some questions if you care about

00:37:25,860 --> 00:37:30,330
testing there's a bunch of really cool

00:37:27,330 --> 00:37:32,160
libraries I have a talk on YouTube I

00:37:30,330 --> 00:37:33,810
should have put the link in there it's

00:37:32,160 --> 00:37:36,630
more focused on testing spark and

00:37:33,810 --> 00:37:37,800
there's some blog posts there's a bunch

00:37:36,630 --> 00:37:39,990
of really good spark

00:37:37,800 --> 00:37:42,930
resources I think the programming guide

00:37:39,990 --> 00:37:45,120
is often overlooked a lot of people seem

00:37:42,930 --> 00:37:47,670
to just work with their IDE and and look

00:37:45,120 --> 00:37:50,190
at what it's tab completing - but if you

00:37:47,670 --> 00:37:52,080
actually read the the API descriptions

00:37:50,190 --> 00:37:53,400
it'll give you an idea of what things

00:37:52,080 --> 00:37:55,740
are gonna give you really terrible

00:37:53,400 --> 00:37:57,330
performance like group by key or things

00:37:55,740 --> 00:37:59,670
which implicitly collect some of the

00:37:57,330 --> 00:38:01,530
data back to the driver so I think it's

00:37:59,670 --> 00:38:04,050
worth taking the time to read the docs

00:38:01,530 --> 00:38:06,060
for the things you're using kay has some

00:38:04,050 --> 00:38:08,460
really interesting research for more

00:38:06,060 --> 00:38:09,990
researching people about sort of

00:38:08,460 --> 00:38:12,660
figuring out spark performance stuff in

00:38:09,990 --> 00:38:15,180
general and where those things are I'm

00:38:12,660 --> 00:38:16,560
also gonna try and do an online office

00:38:15,180 --> 00:38:18,960
hours where you can bring your spark

00:38:16,560 --> 00:38:20,430
questions and I'll answer them and you

00:38:18,960 --> 00:38:22,230
can follow me on Twitter and I'll tweet

00:38:20,430 --> 00:38:24,720
about them once I have a chance to like

00:38:22,230 --> 00:38:28,590
sit down in the same city for a period

00:38:24,720 --> 00:38:30,180
of greater than 48 hours cool oh yeah

00:38:28,590 --> 00:38:32,190
there's some books you should buy them

00:38:30,180 --> 00:38:35,070
don't buy that out-of-date one it's not

00:38:32,190 --> 00:38:36,090
very good anyways I wrote it and I'm

00:38:35,070 --> 00:38:39,090
telling you not to buy it

00:38:36,090 --> 00:38:40,860
so really really yeah don't buy the

00:38:39,090 --> 00:38:42,240
first edition of fascinator processing

00:38:40,860 --> 00:38:44,490
with spark if you want to buy any of

00:38:42,240 --> 00:38:47,280
these other books that's cool

00:38:44,490 --> 00:38:49,530
and I get money from some of these books

00:38:47,280 --> 00:38:54,000
so it's very cool and you should buy

00:38:49,530 --> 00:38:56,790
several copies especially if you happen

00:38:54,000 --> 00:38:59,030
to have a corporate expense account

00:38:56,790 --> 00:39:03,090
I recommend high-performance spark as

00:38:59,030 --> 00:39:05,760
the gift of the season your dog wants

00:39:03,090 --> 00:39:08,700
one as does your cat and your car could

00:39:05,760 --> 00:39:11,100
also use a copy right now it's PDF only

00:39:08,700 --> 00:39:16,470
but you can still buy several PDFs I

00:39:11,100 --> 00:39:18,510
promise it's it's an early release we'll

00:39:16,470 --> 00:39:20,640
be adding more chapters as we go it's

00:39:18,510 --> 00:39:23,100
focused on making high-performance spark

00:39:20,640 --> 00:39:24,720
applications so helping you go from sort

00:39:23,100 --> 00:39:28,020
of just understanding the API at its

00:39:24,720 --> 00:39:29,130
basics to how to make stuff go fast if

00:39:28,020 --> 00:39:30,600
you don't want to buy the early release

00:39:29,130 --> 00:39:34,470
and you just want to give me your email

00:39:30,600 --> 00:39:37,200
address to spam or selectively contact I

00:39:34,470 --> 00:39:38,750
will let you know when it's done and if

00:39:37,200 --> 00:39:41,610
you go to high-performance super calm

00:39:38,750 --> 00:39:44,340
I'll send out an email when everything's

00:39:41,610 --> 00:39:46,740
ready there's a bunch of cool spark

00:39:44,340 --> 00:39:48,810
videos but you all probably already know

00:39:46,740 --> 00:39:51,000
this so we'll skip that and we'll do

00:39:48,810 --> 00:39:51,809
questions for the two heretics in the

00:39:51,000 --> 00:39:54,839
odd

00:39:51,809 --> 00:39:56,489
that use Python please get in touch with

00:39:54,839 --> 00:40:00,859
me I'm trying to do some crazy with

00:39:56,489 --> 00:40:03,779
Python in spark I'm sorry everyone else

00:40:00,859 --> 00:40:05,969
I've gone partially to the dark side but

00:40:03,779 --> 00:40:09,059
they had cookies so it was really hard

00:40:05,969 --> 00:40:10,529
to resist but give me your PI spark

00:40:09,059 --> 00:40:12,869
UDF's and I'll try and make them run

00:40:10,529 --> 00:40:14,819
fast and if you care about software

00:40:12,869 --> 00:40:17,249
testing and you don't hate surveys I

00:40:14,819 --> 00:40:18,869
would love to hear your feedback on what

00:40:17,249 --> 00:40:20,819
you're doing to test your spark

00:40:18,869 --> 00:40:23,419
applications so that I can make better

00:40:20,819 --> 00:40:25,829
libraries for testing spark applications

00:40:23,419 --> 00:40:27,719
and now we'll just we'll just do some

00:40:25,829 --> 00:40:30,119
questions until people come and kick us

00:40:27,719 --> 00:40:32,880
out of this room it looks like no one

00:40:30,119 --> 00:40:35,969
really likes the that that question

00:40:32,880 --> 00:40:39,179
e-link thing so we'll just do hands up

00:40:35,969 --> 00:40:45,109
questions and you can get this book it's

00:40:39,179 --> 00:40:49,759
a book made of paper as opposed to a PDF

00:40:45,109 --> 00:40:49,759
come on someone's got a one okay yeah

00:40:51,739 --> 00:40:56,399
thank you for a talk

00:40:53,459 --> 00:40:59,279
the question is can we benefit from

00:40:56,399 --> 00:41:03,299
using external shuffle services if if we

00:40:59,279 --> 00:41:05,759
do does it mean that all they are today

00:41:03,299 --> 00:41:08,880
is gonna be this I realized before going

00:41:05,759 --> 00:41:14,069
to external service and will it be more

00:41:08,880 --> 00:41:17,849
performant apply so the external service

00:41:14,069 --> 00:41:20,130
is is fast if you're if you're working

00:41:17,849 --> 00:41:21,809
with sparks equal actually your your

00:41:20,130 --> 00:41:25,439
data type doesn't need to be

00:41:21,809 --> 00:41:27,119
deserialized to do a sort on because the

00:41:25,439 --> 00:41:31,139
the data is actually just sort of all

00:41:27,119 --> 00:41:33,539
directly so it depends sort of actually

00:41:31,139 --> 00:41:35,489
what your data is how it's going to

00:41:33,539 --> 00:41:37,439
behave with the the external shuffle

00:41:35,489 --> 00:41:41,299
service but yeah but you probably want

00:41:37,439 --> 00:41:41,299
to be using it and you get a book

00:41:46,960 --> 00:41:53,960
and I don't have any more prizes I can I

00:41:51,470 --> 00:41:54,410
might let me see here what's in my

00:41:53,960 --> 00:42:01,039
backpack

00:41:54,410 --> 00:42:04,849
ah does anyone really like okay if

00:42:01,039 --> 00:42:08,509
anyone really likes Hello Kitty I have

00:42:04,849 --> 00:42:09,980
some Hello Kitty buttons if no one likes

00:42:08,509 --> 00:42:12,319
Hello Kitty I'm keeping these from

00:42:09,980 --> 00:42:14,150
myself but if anyone likes Hello Kitty

00:42:12,319 --> 00:42:17,359
and wants to ask a second question I can

00:42:14,150 --> 00:42:19,190
give you my these limited edition will

00:42:17,359 --> 00:42:22,160
probably not actually a limited edition

00:42:19,190 --> 00:42:27,890
I can give you these mass-produced San

00:42:22,160 --> 00:42:29,269
Francisco Hello Kitty buttons or no okay

00:42:27,890 --> 00:42:31,369
no more questions okay cool

00:42:29,269 --> 00:42:33,859
thank you all for coming you've been

00:42:31,369 --> 00:42:36,529
wonderful I'm sorry there wasn't as much

00:42:33,859 --> 00:42:39,700
David Hasselhoff as I had planned next

00:42:36,529 --> 00:42:39,700

YouTube URL: https://www.youtube.com/watch?v=0KGGa9qX9nw


