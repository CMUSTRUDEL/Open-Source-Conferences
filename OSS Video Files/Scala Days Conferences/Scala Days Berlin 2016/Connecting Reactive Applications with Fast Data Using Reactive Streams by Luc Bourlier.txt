Title: Connecting Reactive Applications with Fast Data Using Reactive Streams by Luc Bourlier
Publication date: 2016-07-22
Playlist: Scala Days Berlin 2016
Description: 
	This video was recorded at Scala Days Berlin 2016
Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

Anstract:
A reactive application doesn't live in isolation, it has to connect with the other components, and try to keep the whole system reactive.
Spark, as an element of a Fast Data architecture, is one of these components to connect to. With the addition of the back pressure support to Spark Streaming in Spark 1.5, and the other characteristics of Spark, it is simpler than before to fully integrate it in a reactive system. This talk with describe the streaming model in Spark, its support of back pressure, and show, in a demo, how to use Reactive Streams to integrate Spark Streaming and a reactive application in a reactive system.
Captions: 
	00:00:01,750 --> 00:00:08,410
good afternoon everyone so I think we

00:00:05,350 --> 00:00:10,990
can start so I am looking for a I'm from

00:00:08,410 --> 00:00:13,420
light band I think from not right now

00:00:10,990 --> 00:00:16,000
you should all know what we are and what

00:00:13,420 --> 00:00:17,770
we are former form of type safe if we

00:00:16,000 --> 00:00:21,039
still don't know we have a boost in the

00:00:17,770 --> 00:00:24,310
expression also just come see us and

00:00:21,039 --> 00:00:26,769
today I will speak about connecting our

00:00:24,310 --> 00:00:29,740
reactive application and fast data so

00:00:26,769 --> 00:00:32,169
the title is a bit but well versed Baldy

00:00:29,740 --> 00:00:34,030
so I will go through the different ideas

00:00:32,169 --> 00:00:35,860
of the title first to make sure that we

00:00:34,030 --> 00:00:39,040
we understand what we speak about and

00:00:35,860 --> 00:00:41,649
then describe what can be the program's

00:00:39,040 --> 00:00:43,570
of connecting the big data first data

00:00:41,649 --> 00:00:47,640
with reactive applications and see how

00:00:43,570 --> 00:00:51,039
we can solve solve the problem okay oh

00:00:47,640 --> 00:00:54,489
so on slide of intro so regular

00:00:51,039 --> 00:00:56,710
applications who still don't know what

00:00:54,489 --> 00:01:03,219
is reactive manifesto and reactive

00:00:56,710 --> 00:01:07,420
applications okay so yeah so just go

00:01:03,219 --> 00:01:09,460
rave last by Collison reactor my Festool

00:01:07,420 --> 00:01:11,950
is a description of what we think that

00:01:09,460 --> 00:01:14,470
reactive application should be so being

00:01:11,950 --> 00:01:16,990
a astok responsive resilient and mrs.

00:01:14,470 --> 00:01:19,990
driven so you can you can be resilient

00:01:16,990 --> 00:01:22,479
to any kind of program or kind of RAM

00:01:19,990 --> 00:01:24,130
kind of situations a resilient to the

00:01:22,479 --> 00:01:29,229
volume of the tire we see resilient to

00:01:24,130 --> 00:01:32,350
any failure and and be able to respond

00:01:29,229 --> 00:01:34,750
to to to to send respond in a timely

00:01:32,350 --> 00:01:37,900
manner Matt manner even if your system

00:01:34,750 --> 00:01:40,659
is decorated so yeah so that's great

00:01:37,900 --> 00:01:43,780
education so fast data i'm pretty sure

00:01:40,659 --> 00:01:46,420
that people are less noisy ball of fur

00:01:43,780 --> 00:01:49,810
zeta at this conference so let's stop

00:01:46,420 --> 00:01:51,640
back from Big Data big Gators a big data

00:01:49,810 --> 00:01:54,820
movement started sometime in the middle

00:01:51,640 --> 00:01:57,310
of the 2000s when the big entire company

00:01:54,820 --> 00:01:59,079
started to have too much data to be able

00:01:57,310 --> 00:02:01,630
to manage them with the traditional

00:01:59,079 --> 00:02:04,719
tools so what we speak about big data

00:02:01,630 --> 00:02:07,149
when you your data doesn't fit in one

00:02:04,719 --> 00:02:09,519
machine so it's 11 characteristic or it

00:02:07,149 --> 00:02:11,440
doesn't work well in classical database

00:02:09,519 --> 00:02:14,620
even if it's a working on multiple

00:02:11,440 --> 00:02:18,670
machines the wait was solved

00:02:14,620 --> 00:02:20,799
so the basic idea as well published by

00:02:18,670 --> 00:02:23,530
google in two different papers when

00:02:20,799 --> 00:02:25,060
describing a way to create this weighted

00:02:23,530 --> 00:02:28,120
file system which is resilient and

00:02:25,060 --> 00:02:30,549
redundant working on cheap hardware

00:02:28,120 --> 00:02:32,799
community hardware and my produce which

00:02:30,549 --> 00:02:37,269
is a way to execute operation on this

00:02:32,799 --> 00:02:39,879
type of this we titre system so fast it

00:02:37,269 --> 00:02:44,200
are the idea of fast data basically you

00:02:39,879 --> 00:02:46,450
go we go for about 10 years and and

00:02:44,200 --> 00:02:50,290
people won't want to apply the same

00:02:46,450 --> 00:02:51,970
ideas of as big data but on instead of

00:02:50,290 --> 00:02:54,849
using your data that you have some

00:02:51,970 --> 00:02:57,430
Western in membrane in big cluster you

00:02:54,849 --> 00:02:59,409
want to do the same kind of a portion on

00:02:57,430 --> 00:03:01,090
data which is in coming from you

00:02:59,409 --> 00:03:04,720
directly from your web services or any

00:03:01,090 --> 00:03:09,010
application you have and with the idea

00:03:04,720 --> 00:03:12,250
that to get a really fast update of

00:03:09,010 --> 00:03:15,280
what's happening in your system so one

00:03:12,250 --> 00:03:18,280
logical way that singles in the company

00:03:15,280 --> 00:03:20,109
is you start you're likely you are lucky

00:03:18,280 --> 00:03:21,910
enough that you your managers there you

00:03:20,109 --> 00:03:24,190
go ahead you can do some big Gator so

00:03:21,910 --> 00:03:26,470
you you have access to all the data that

00:03:24,190 --> 00:03:28,389
your company i created over time and you

00:03:26,470 --> 00:03:30,340
start to do analysis on it and you get

00:03:28,389 --> 00:03:33,549
some interesting information about i

00:03:30,340 --> 00:03:36,129
don't know why i mean our friend or some

00:03:33,549 --> 00:03:38,769
sort of service is going down or signal

00:03:36,129 --> 00:03:40,750
at some interesting information and your

00:03:38,769 --> 00:03:43,090
managers is great well that's still cool

00:03:40,750 --> 00:03:44,829
data can we get like data about what's

00:03:43,090 --> 00:03:47,680
appealing now and you start to turn

00:03:44,829 --> 00:03:50,169
every your your your nose every day and

00:03:47,680 --> 00:03:52,239
every day we every morning we can check

00:03:50,169 --> 00:03:55,540
that what happened the day before and

00:03:52,239 --> 00:03:57,190
it's it's nice but I mean you manager

00:03:55,540 --> 00:03:59,709
always want more and manager says well

00:03:57,190 --> 00:04:01,750
can we just shot a shot to bits a cycle

00:03:59,709 --> 00:04:03,370
because if we know we defer that

00:04:01,750 --> 00:04:07,239
something is going wrong we can fix it

00:04:03,370 --> 00:04:09,099
so we start to get re and when we when

00:04:07,239 --> 00:04:11,709
you move from historical data to Delhi

00:04:09,099 --> 00:04:13,599
usually you can do it far away you can

00:04:11,709 --> 00:04:15,160
still run in during the day whatever

00:04:13,599 --> 00:04:18,639
data you receives a dead before it's

00:04:15,160 --> 00:04:21,160
fine when you go to re updates usually

00:04:18,639 --> 00:04:22,780
you need more than just the one off one

00:04:21,160 --> 00:04:24,400
hour of data you need data from the last

00:04:22,780 --> 00:04:26,620
twenty first we were to get some good

00:04:24,400 --> 00:04:28,250
and edges and you start to get tricky to

00:04:26,620 --> 00:04:30,860
process the data for the

00:04:28,250 --> 00:04:33,440
from 24 hours in 100 it might invite mr.

00:04:30,860 --> 00:04:35,390
t to go Trick Trick tricky so you may

00:04:33,440 --> 00:04:38,360
start to switch a bit you're the way you

00:04:35,390 --> 00:04:40,520
do it and after a while your manager say

00:04:38,360 --> 00:04:42,530
tells you i mean we want to know as soon

00:04:40,520 --> 00:04:44,570
as possible when says any program and

00:04:42,530 --> 00:04:47,750
then you get to fast data when you need

00:04:44,570 --> 00:04:52,130
to have a streaming online to to process

00:04:47,750 --> 00:04:54,320
the data and and and to surpass any

00:04:52,130 --> 00:04:56,000
program or any specific things that are

00:04:54,320 --> 00:04:58,460
changing that you need to know so that's

00:04:56,000 --> 00:05:02,900
how a lot o in lot of case we get from

00:04:58,460 --> 00:05:06,140
from big data too fast data so other

00:05:02,900 --> 00:05:07,820
side light Bend i'm almost everybody

00:05:06,140 --> 00:05:10,760
between reactive applications and

00:05:07,820 --> 00:05:14,690
sinking of big data are working with

00:05:10,760 --> 00:05:19,850
park and spa streaming a few reason for

00:05:14,690 --> 00:05:22,510
that one which is not not not to to

00:05:19,850 --> 00:05:26,300
pasta is Sparky's return in skala and

00:05:22,510 --> 00:05:28,790
the way you program spark it looks a lot

00:05:26,300 --> 00:05:30,650
like the way you program the scatter

00:05:28,790 --> 00:05:33,080
correction you do the map you have the

00:05:30,650 --> 00:05:35,300
same same flat map say goodbye the

00:05:33,080 --> 00:05:36,860
semantic behind it it might be slightly

00:05:35,300 --> 00:05:39,850
different but the way you write your

00:05:36,860 --> 00:05:41,990
code it's exactly the same as if you do

00:05:39,850 --> 00:05:44,330
work on Scylla correction so it seemed

00:05:41,990 --> 00:05:50,120
very simple to pick up for fall scallop

00:05:44,330 --> 00:05:52,190
developers and so the big the big the

00:05:50,120 --> 00:05:55,160
big on plants of big data is Hadoop

00:05:52,190 --> 00:05:58,570
which is implanting the basic MapReduce

00:05:55,160 --> 00:06:02,300
model vampiros pharrell works very well

00:05:58,570 --> 00:06:04,630
but after some time it reaches on forums

00:06:02,300 --> 00:06:08,540
or some limitations modern programs and

00:06:04,630 --> 00:06:10,520
the limitations are mostly that it's not

00:06:08,540 --> 00:06:14,169
real good at doing iterative algorithm

00:06:10,520 --> 00:06:16,880
and nowadays the big big big thing is a

00:06:14,169 --> 00:06:19,220
machine learning algorithm and machine

00:06:16,880 --> 00:06:21,560
learning algorithm are always about

00:06:19,220 --> 00:06:24,860
iteration over and over on the send data

00:06:21,560 --> 00:06:27,940
to reduce so if to reduce the volume of

00:06:24,860 --> 00:06:32,570
data to some information which is usable

00:06:27,940 --> 00:06:36,110
so Spock is very good to do that and and

00:06:32,570 --> 00:06:38,210
also so after I've been being solving

00:06:36,110 --> 00:06:40,460
some some of the reason why I dupe was

00:06:38,210 --> 00:06:41,169
not very good at it relative computation

00:06:40,460 --> 00:06:43,689
there

00:06:41,169 --> 00:06:49,180
to step further and know when you do as

00:06:43,689 --> 00:06:51,370
Parker when you create a spice rub when

00:06:49,180 --> 00:06:53,620
you when you good your spark application

00:06:51,370 --> 00:06:57,150
what you just described is a graph of a

00:06:53,620 --> 00:06:59,830
passion to to execute over some data and

00:06:57,150 --> 00:07:01,870
spark take your graph and recompile it

00:06:59,830 --> 00:07:03,669
in such a way that it went very fast so

00:07:01,870 --> 00:07:06,909
in the last lecture no spark as they did

00:07:03,669 --> 00:07:09,009
a big amount of work to to take your

00:07:06,909 --> 00:07:10,449
modelo execution of the model of what

00:07:09,009 --> 00:07:12,580
you want to do algorithm you want to do

00:07:10,449 --> 00:07:15,370
on your picket on your data and to

00:07:12,580 --> 00:07:16,779
compile it and and optimize it it's such

00:07:15,370 --> 00:07:24,639
a way that it ran really well on the

00:07:16,779 --> 00:07:26,289
crystals so yeah so that's it it won't

00:07:24,639 --> 00:07:29,009
be able really useful merge with it's a

00:07:26,289 --> 00:07:33,370
it's the description of the basic way of

00:07:29,009 --> 00:07:35,319
a spark application works so you have a

00:07:33,370 --> 00:07:37,569
crisp optimizer which is something that

00:07:35,319 --> 00:07:40,060
just manage your cluster and allow you

00:07:37,569 --> 00:07:43,240
to you can ask the customer measure I

00:07:40,060 --> 00:07:46,810
need I have a job I won't enroll 64

00:07:43,240 --> 00:07:49,449
calls and Tony gig 2004 memory too and

00:07:46,810 --> 00:07:51,759
my right bank job as a cluster manager

00:07:49,449 --> 00:07:53,289
will give you this resources and after

00:07:51,759 --> 00:07:56,259
that it was a driver program which is

00:07:53,289 --> 00:07:59,469
with your application we will we'll send

00:07:56,259 --> 00:08:01,689
like directly to the executor to the

00:07:59,469 --> 00:08:03,279
nodes on your cluster the work to be

00:08:01,689 --> 00:08:06,279
executed which would be executed on the

00:08:03,279 --> 00:08:08,800
data which is in your Cristero and and

00:08:06,279 --> 00:08:12,490
and you can get the data back on your

00:08:08,800 --> 00:08:15,069
rival or push it push it in the next

00:08:12,490 --> 00:08:19,659
data store for further analysis or sing

00:08:15,069 --> 00:08:21,789
like that so Spock is great and spark is

00:08:19,659 --> 00:08:24,610
so great that they saw that we could

00:08:21,789 --> 00:08:26,499
also use whatever spark not only for big

00:08:24,610 --> 00:08:31,120
bad job but to do it or so on on

00:08:26,499 --> 00:08:34,329
streaming and the way they went about it

00:08:31,120 --> 00:08:39,459
is to go with the idea of mini batching

00:08:34,329 --> 00:08:41,649
so the idea of me batching is to run the

00:08:39,459 --> 00:08:44,589
same type of operation that you do in

00:08:41,649 --> 00:08:47,230
spark normal but on reasonable amount of

00:08:44,589 --> 00:08:49,660
data and what is mini batch is is

00:08:47,230 --> 00:08:52,360
whatever data you received in a set

00:08:49,660 --> 00:08:53,980
interval so you define your spark job to

00:08:52,360 --> 00:08:55,779
say I want to earn every

00:08:53,980 --> 00:08:59,050
whatever once they want five so um let's

00:08:55,779 --> 00:09:02,110
say five seconds and doing five seconds

00:08:59,050 --> 00:09:03,310
spark we receive data coming from your

00:09:02,110 --> 00:09:06,160
application some stream of data

00:09:03,310 --> 00:09:07,990
somewhere storage raggedly keep it

00:09:06,160 --> 00:09:10,329
wicked resilient and everything needed

00:09:07,990 --> 00:09:13,660
and at the end of the five second if we

00:09:10,329 --> 00:09:15,279
start the job which will compute some

00:09:13,660 --> 00:09:19,660
analysis of all the data you receive it

00:09:15,279 --> 00:09:21,279
with faxing on and yeah and then the

00:09:19,660 --> 00:09:24,730
great thing with sputtering is we can

00:09:21,279 --> 00:09:30,180
basically yet you created good tried to

00:09:24,730 --> 00:09:32,860
run on on on on the data you had stored

00:09:30,180 --> 00:09:34,930
statically somewhere you can run the LFE

00:09:32,860 --> 00:09:37,420
the same code on the on the swimming

00:09:34,930 --> 00:09:40,300
later so that's a good thing and to be

00:09:37,420 --> 00:09:41,920
to be even more powerful eyes additional

00:09:40,300 --> 00:09:45,310
portion which are specific to swimming

00:09:41,920 --> 00:09:48,820
because so as I said you you execute a

00:09:45,310 --> 00:09:51,399
normal big data job on some very small

00:09:48,820 --> 00:09:53,380
set of data which might not be enough

00:09:51,399 --> 00:09:55,449
data to be relevant so you can say to

00:09:53,380 --> 00:09:57,790
spark swimming too in fact every five so

00:09:55,449 --> 00:10:00,010
go on run my computation over the last

00:09:57,790 --> 00:10:02,680
five minutes and see that and you can

00:10:00,010 --> 00:10:04,980
also keep state over time to do stupid

00:10:02,680 --> 00:10:08,319
things like one stupid so simple things

00:10:04,980 --> 00:10:09,610
like things and am off request for

00:10:08,319 --> 00:10:11,170
different sure where you get an seng

00:10:09,610 --> 00:10:16,149
that so you can just do all those kind

00:10:11,170 --> 00:10:18,310
of things so yes at what I explain each

00:10:16,149 --> 00:10:24,329
interval you get the data and then you

00:10:18,310 --> 00:10:26,680
run a spa computation on the data so

00:10:24,329 --> 00:10:29,670
write your application is mostly about

00:10:26,680 --> 00:10:33,430
resilient to all kind of things and

00:10:29,670 --> 00:10:37,899
without directly walking on Android team

00:10:33,430 --> 00:10:41,380
fsdo spark and sputtering implement most

00:10:37,899 --> 00:10:44,019
of what's in a directive mephisto so it

00:10:41,380 --> 00:10:48,279
has support to recover for all kind of

00:10:44,019 --> 00:10:51,550
kind of videos and totally specifically

00:10:48,279 --> 00:10:55,389
at sparks trimming and fast data it

00:10:51,550 --> 00:10:56,889
recovers without program if you for

00:10:55,389 --> 00:11:00,040
application dies also denies that it

00:10:56,889 --> 00:11:01,660
recovers and we reprocess and connect

00:11:00,040 --> 00:11:04,930
back to the

00:11:01,660 --> 00:11:08,350
the police of data and ask him to if you

00:11:04,930 --> 00:11:10,000
use the right produce of data I said to

00:11:08,350 --> 00:11:11,890
please we start at a specific point

00:11:10,000 --> 00:11:13,750
where I was ever stopped and and

00:11:11,890 --> 00:11:20,590
continue to process it off of that and

00:11:13,750 --> 00:11:24,070
those is also some support to to my age

00:11:20,590 --> 00:11:26,080
accessory militar so that's what what I

00:11:24,070 --> 00:11:27,430
would mostly show you as a big program

00:11:26,080 --> 00:11:29,860
as you put a short break programs

00:11:27,430 --> 00:11:31,840
because it's when you need coordination

00:11:29,860 --> 00:11:35,140
with between systems everything else

00:11:31,840 --> 00:11:37,270
bottom edge by itself if you're on spark

00:11:35,140 --> 00:11:38,800
on some custom hydro it can one note

00:11:37,270 --> 00:11:40,540
dies I carry the crystal magic

00:11:38,800 --> 00:11:42,880
automatically we start another node and

00:11:40,540 --> 00:11:44,800
if some data has been lost it can be

00:11:42,880 --> 00:11:47,440
recreated or we covered from some

00:11:44,800 --> 00:11:49,510
checkpoints so most of everything is

00:11:47,440 --> 00:11:51,040
managed by spark itself internally but

00:11:49,510 --> 00:11:53,470
if you speak a good high volume of data

00:11:51,040 --> 00:11:55,390
then you need to have some more cycrin

00:11:53,470 --> 00:11:58,660
session with outside wat so with what we

00:11:55,390 --> 00:12:03,190
will see yeah so just to show you we go

00:11:58,660 --> 00:12:05,770
for a demo and to do the demo I will use

00:12:03,190 --> 00:12:11,650
my inter symbol raspberry pie crust I'll

00:12:05,770 --> 00:12:13,690
so why do I use my pie crust I'll a few

00:12:11,650 --> 00:12:15,790
reason one it's fun it's real fun to

00:12:13,690 --> 00:12:19,240
play with these guys another one is I do

00:12:15,790 --> 00:12:21,910
like to what I want to work is on on on

00:12:19,240 --> 00:12:23,710
pushing the limit of a machine so if you

00:12:21,910 --> 00:12:26,310
do that on your local machine with

00:12:23,710 --> 00:12:29,230
virtual machines or the curls you will

00:12:26,310 --> 00:12:31,750
it will be huge mess because everybody's

00:12:29,230 --> 00:12:33,580
is fighting for the same resources so

00:12:31,750 --> 00:12:36,010
doing it on on real hardware or

00:12:33,580 --> 00:12:38,380
something like a cloud it sits with way

00:12:36,010 --> 00:12:41,050
more useful and as I want to push the

00:12:38,380 --> 00:12:42,700
limit of the hardware or Wi-Fi there is

00:12:41,050 --> 00:12:44,470
mud where you don't need to push much to

00:12:42,700 --> 00:12:46,750
get to the limit so its face it was

00:12:44,470 --> 00:12:49,630
simple and if you to get a CT machine

00:12:46,750 --> 00:12:55,500
with a nice big focus and i gave a giddy

00:12:49,630 --> 00:12:59,610
of memory and whatever so the demo i

00:12:55,500 --> 00:13:02,680
have a free applications in my neighbor

00:12:59,610 --> 00:13:03,880
only two in fact on this slide I one

00:13:02,680 --> 00:13:06,250
application which is just apple

00:13:03,880 --> 00:13:08,530
application which gather some data and

00:13:06,250 --> 00:13:10,300
some about the were systems i will show

00:13:08,530 --> 00:13:14,860
you just to know what how the system is

00:13:10,300 --> 00:13:17,260
going i have so active application

00:13:14,860 --> 00:13:19,000
it's a gas stream application I don't

00:13:17,260 --> 00:13:21,610
know if sighs talk about like a string

00:13:19,000 --> 00:13:23,290
in this conference but I guest room is

00:13:21,610 --> 00:13:25,240
quite a really nice if you want to

00:13:23,290 --> 00:13:27,730
process data in the swimming fashion and

00:13:25,240 --> 00:13:29,860
you create a graph of how you want to a

00:13:27,730 --> 00:13:31,930
nice raw data and you get something

00:13:29,860 --> 00:13:34,030
which is which follows the reactive ma

00:13:31,930 --> 00:13:37,000
Fisto and so you can integrate

00:13:34,030 --> 00:13:39,370
interactive application so in my reactor

00:13:37,000 --> 00:13:41,890
application of the first part which is

00:13:39,370 --> 00:13:44,260
the random number generator which I can

00:13:41,890 --> 00:13:47,080
tweet to say I can set your num num gaw

00:13:44,260 --> 00:13:50,080
giotto I want to get another phone for

00:13:47,080 --> 00:13:52,240
4000 number per second and generally

00:13:50,080 --> 00:13:54,990
thought try to give me that and then as

00:13:52,240 --> 00:13:57,340
some some so I guess ask a stream

00:13:54,990 --> 00:13:59,500
configuration to be able to to do

00:13:57,340 --> 00:14:02,710
computation of Pi using on dum dum girls

00:13:59,500 --> 00:14:05,170
and slicing the random numbers in

00:14:02,710 --> 00:14:07,020
different ways to kind of see if I get

00:14:05,170 --> 00:14:09,910
different values of pies and singer that

00:14:07,020 --> 00:14:11,590
order later I cannot merge it and send

00:14:09,910 --> 00:14:14,500
it to sputtering for to do scenario

00:14:11,590 --> 00:14:15,700
analysis on it mostly basically our good

00:14:14,500 --> 00:14:18,070
is the result and what are the

00:14:15,700 --> 00:14:20,500
differences and how I'll flip doing as a

00:14:18,070 --> 00:14:25,090
result depending of how I managed to get

00:14:20,500 --> 00:14:28,390
cake you'll my x value so let's check

00:14:25,090 --> 00:14:32,020
that so at the screen just to show you

00:14:28,390 --> 00:14:38,380
that I have a spark crystal with just

00:14:32,020 --> 00:14:40,360
two to occur nodes so online by my 55

00:14:38,380 --> 00:14:41,650
node clusters are one node which is kind

00:14:40,360 --> 00:14:43,140
of the gateway which also running the

00:14:41,650 --> 00:14:46,360
spark master doesn't really much

00:14:43,140 --> 00:14:48,730
processing processing as a walker are

00:14:46,360 --> 00:14:52,000
doing only Spock Spock walk because they

00:14:48,730 --> 00:14:55,330
need as much as you can so as my small

00:14:52,000 --> 00:14:57,790
player application which just tell me

00:14:55,330 --> 00:15:00,400
the formation of goods not at all so i

00:14:57,790 --> 00:15:04,300
can change where you ask I can vary if I

00:15:00,400 --> 00:15:07,120
check out the update zor zor rate I

00:15:04,300 --> 00:15:08,650
won't I get some values of x so it's

00:15:07,120 --> 00:15:10,750
mostly to just see that my system is

00:15:08,650 --> 00:15:13,690
alive and that my computation of pi is

00:15:10,750 --> 00:15:19,300
pretty bad and if I'm a shown up with

00:15:13,690 --> 00:15:21,790
Spock job I'm morning and that's DUI for

00:15:19,300 --> 00:15:24,460
sparks trimming itself I just refresh

00:15:21,790 --> 00:15:25,990
which is it's it's not a wreck TV I it

00:15:24,460 --> 00:15:27,080
doesn't refresh it doesn't do anything

00:15:25,990 --> 00:15:31,610
it's

00:15:27,080 --> 00:15:33,260
quite annoying so I top I get the gist

00:15:31,610 --> 00:15:35,690
input wait our mini message I receive

00:15:33,260 --> 00:15:39,620
which is normally three times as a rate

00:15:35,690 --> 00:15:42,680
of unknown number of the random number

00:15:39,620 --> 00:15:44,060
generator scheduling the air with

00:15:42,680 --> 00:15:47,060
something which will be interesting to

00:15:44,060 --> 00:15:49,610
see when we get in bad bad situation and

00:15:47,060 --> 00:15:52,070
how long it takes to process so in this

00:15:49,610 --> 00:15:55,310
configuration I asked to open my job

00:15:52,070 --> 00:15:58,040
every second and y know it takes two

00:15:55,310 --> 00:16:00,230
hundred and fifty second on average to

00:15:58,040 --> 00:16:03,380
process the data so I'm fine I mean it

00:16:00,230 --> 00:16:05,840
text so yeah so one more thing I need to

00:16:03,380 --> 00:16:08,990
sell both sputtering it's so gather data

00:16:05,840 --> 00:16:11,600
and in fact it doesn't run the job it

00:16:08,990 --> 00:16:13,070
scheduled a job to be run it says now

00:16:11,600 --> 00:16:16,700
that I will die I want to run

00:16:13,070 --> 00:16:18,050
computation aunties data and so I'm

00:16:16,700 --> 00:16:20,030
possessive time is fine and everything

00:16:18,050 --> 00:16:24,530
goes well so let's try to push it a bit

00:16:20,030 --> 00:16:27,170
more so let's go for a 1000 message

00:16:24,530 --> 00:16:31,220
passing on so my right automatically

00:16:27,170 --> 00:16:35,390
update normally should stabilize at some

00:16:31,220 --> 00:16:36,650
point I mean it it's there is a deceit

00:16:35,390 --> 00:16:37,790
that meteorites right it's utterly

00:16:36,650 --> 00:16:41,000
indifferent of this thing that that

00:16:37,790 --> 00:16:43,180
generates the values so now sometimes a

00:16:41,000 --> 00:16:46,460
 on second one is not quite a line

00:16:43,180 --> 00:16:48,410
and no more record but it still

00:16:46,460 --> 00:16:51,470
basically three times the same execution

00:16:48,410 --> 00:16:56,210
time went a bit higher if we fresh it

00:16:51,470 --> 00:16:59,390
while I'd like 1250 before and where we

00:16:56,210 --> 00:17:02,720
get over 6,600 Michigan so we are still

00:16:59,390 --> 00:17:08,240
fine was still in in the in the less

00:17:02,720 --> 00:17:12,040
than one second so let's double that so

00:17:08,240 --> 00:17:14,510
right update tab rise and we are good

00:17:12,040 --> 00:17:16,339
and we can see also that I mean I'm

00:17:14,510 --> 00:17:18,620
sending way more numbers so as a

00:17:16,339 --> 00:17:20,990
computation is done with unknown

00:17:18,620 --> 00:17:22,880
completion of pies done with random

00:17:20,990 --> 00:17:25,220
number all the more numbers are i send

00:17:22,880 --> 00:17:26,810
the better pyar get I mean Steen hood

00:17:25,220 --> 00:17:30,350
great i have like two dissimilar good

00:17:26,810 --> 00:17:32,360
it's not regret execution time is still

00:17:30,350 --> 00:17:34,430
a bit a bit below once the gonzo were

00:17:32,360 --> 00:17:39,370
still fine so let's double that again

00:17:34,430 --> 00:17:43,790
just oh okay let me just check one thing

00:17:39,370 --> 00:17:50,180
if the wrong demo let's restart this one

00:17:43,790 --> 00:17:55,970
nipple it's okay okay just we started it

00:17:50,180 --> 00:17:58,490
too because the difference is when we

00:17:55,970 --> 00:18:02,080
get to two too much data and I was

00:17:58,490 --> 00:18:02,080
running the wrong part of the day move

00:18:03,040 --> 00:18:08,000
so yeah so I guess trimming is basically

00:18:05,870 --> 00:18:09,730
right now waiting for for Spock

00:18:08,000 --> 00:18:16,090
streaming to start to pick up and

00:18:09,730 --> 00:18:16,090
understand data to it so let's refresh

00:18:16,990 --> 00:18:24,280
okay let's get back to two thousand

00:18:20,120 --> 00:18:29,780
requests plus 2000 number per second

00:18:24,280 --> 00:18:33,260
okay and yeah so execution time is still

00:18:29,780 --> 00:18:35,540
on the oven signal so we are fine and

00:18:33,260 --> 00:18:37,850
with a smaller timeline timeline we can

00:18:35,540 --> 00:18:40,580
see a bit more the valuation of time and

00:18:37,850 --> 00:18:46,450
you can see the oops noble funny man

00:18:40,580 --> 00:18:49,880
getting in so let's go 2011 passing and

00:18:46,450 --> 00:18:52,400
my own room number generator be nice if

00:18:49,880 --> 00:18:57,140
was not going too high okay it's over

00:18:52,400 --> 00:19:00,110
over over 4,000 but we can see here that

00:18:57,140 --> 00:19:03,290
execution time is 1.5 string on

00:19:00,110 --> 00:19:06,530
basically and this number that we can

00:19:03,290 --> 00:19:08,840
don't we look at before it's a 3500

00:19:06,530 --> 00:19:11,870
which what or and it's growing and it's

00:19:08,840 --> 00:19:16,250
growing fast and what does it mean ok so

00:19:11,870 --> 00:19:18,260
it we get back to the streaming you I

00:19:16,250 --> 00:19:21,140
and you see that scheduling delay is

00:19:18,260 --> 00:19:23,270
growing very fast so scheduling delay is

00:19:21,140 --> 00:19:26,030
at the end of the tenth of all you say I

00:19:23,270 --> 00:19:28,760
want to compute I want to compute my

00:19:26,030 --> 00:19:31,130
analysis on this data but the previous

00:19:28,760 --> 00:19:33,200
analysis is not finished yet so you just

00:19:31,130 --> 00:19:39,200
get it scheduled and to be wrong later

00:19:33,200 --> 00:19:42,230
so in a few cycles um yeah I'm getting

00:19:39,200 --> 00:19:44,510
to wipe like 15 second yeah so I'm

00:19:42,230 --> 00:19:47,980
trying to compute every second some data

00:19:44,510 --> 00:19:50,780
I'm ready the 52nd late on idiot I get

00:19:47,980 --> 00:19:51,470
and if I get it to an wrong girl

00:19:50,780 --> 00:19:54,860
religious

00:19:51,470 --> 00:19:56,780
and never stuck so I would just make it

00:19:54,860 --> 00:20:03,080
stop a bit in fact so we can see that

00:19:56,780 --> 00:20:05,510
our and if I if I let it grow we start

00:20:03,080 --> 00:20:09,350
to feel the spark a store memory why it

00:20:05,510 --> 00:20:12,080
keeps the data to be processed and this

00:20:09,350 --> 00:20:14,539
has to be filled up and spark is fine

00:20:12,080 --> 00:20:16,610
Spock start to cash this data on disk

00:20:14,539 --> 00:20:19,970
which makes a ver sister events raw

00:20:16,610 --> 00:20:21,830
because you start to in addition to just

00:20:19,970 --> 00:20:23,870
keep it in memory you stop to cash in on

00:20:21,830 --> 00:20:25,340
disk so you get through processing which

00:20:23,870 --> 00:20:28,070
is similar you are scaling delay which

00:20:25,340 --> 00:20:30,740
is good even higher and when you get a

00:20:28,070 --> 00:20:32,630
bit of displaced or whatever Oh spy

00:20:30,740 --> 00:20:35,440
doesn't crash because puck almost never

00:20:32,630 --> 00:20:37,730
crash it will stop processing and you

00:20:35,440 --> 00:20:39,500
likely you won't post some data will

00:20:37,730 --> 00:20:45,080
likely be lower standard post at some

00:20:39,500 --> 00:20:48,590
point so let's again so i just changed

00:20:45,080 --> 00:20:50,419
before my last piece of torque I change

00:20:48,590 --> 00:20:52,760
the condom numbers right or two just

00:20:50,419 --> 00:20:55,730
pushing 10 message so gone so almost

00:20:52,760 --> 00:20:57,470
nothing and i'm still in the process of

00:20:55,730 --> 00:20:59,630
recovering from the delay ad before I'm

00:20:57,470 --> 00:21:03,650
still processing the old data I still

00:20:59,630 --> 00:21:07,100
not back in a in a way yeah so you see

00:21:03,650 --> 00:21:09,380
you can see that yeah so it took some

00:21:07,100 --> 00:21:11,720
time for to go to apply to the peak of

00:21:09,380 --> 00:21:16,130
the program and then to recover at some

00:21:11,720 --> 00:21:22,100
point so what can we do about that they

00:21:16,130 --> 00:21:24,860
move like pressure so the way we would

00:21:22,100 --> 00:21:26,720
get singh walk about managing too much

00:21:24,860 --> 00:21:29,419
volume in our active applications and

00:21:26,720 --> 00:21:31,549
specifically in a in application in an

00:21:29,419 --> 00:21:34,429
application which are using as a cast

00:21:31,549 --> 00:21:37,340
 it's you use back pressure like

00:21:34,429 --> 00:21:39,770
pressure is very simple idea is you have

00:21:37,340 --> 00:21:42,200
a customer which start to get overloaded

00:21:39,770 --> 00:21:44,570
and it just tells the producer i cannot

00:21:42,200 --> 00:21:47,179
handle that much please please send it

00:21:44,570 --> 00:21:50,480
off a slower that's real just the basic

00:21:47,179 --> 00:21:54,710
idea and the classic example so if you

00:21:50,480 --> 00:21:56,840
ever root at the tcp level API every

00:21:54,710 --> 00:22:00,200
time there's a frame which is sent on

00:21:56,840 --> 00:22:02,270
tcp at the gcb between each side the

00:22:00,200 --> 00:22:04,399
side sending the frame tell the other

00:22:02,270 --> 00:22:05,130
side this is how much before space

00:22:04,399 --> 00:22:08,160
arrive

00:22:05,130 --> 00:22:10,080
so this is really back pressure and if

00:22:08,160 --> 00:22:12,150
you start to have enough too much a

00:22:10,080 --> 00:22:15,960
service that it can process fast enough

00:22:12,150 --> 00:22:19,530
that I incoming and if and the before I

00:22:15,960 --> 00:22:21,900
get full then you get some interesting

00:22:19,530 --> 00:22:24,810
in error exception on the on your put

00:22:21,900 --> 00:22:26,550
your sauce side that add need to look it

00:22:24,810 --> 00:22:29,220
up again but it's it's something that

00:22:26,550 --> 00:22:32,070
doesn't that is not quite clear why it's

00:22:29,220 --> 00:22:36,360
not working anymore so TCP does a back

00:22:32,070 --> 00:22:38,610
pressure so the idea is going to kind of

00:22:36,360 --> 00:22:41,490
use the same thing you know to be able

00:22:38,610 --> 00:22:44,400
to do we want to do back pressure in the

00:22:41,490 --> 00:22:46,950
system to be able to not have the case

00:22:44,400 --> 00:22:49,640
when spark is also learning bye-bye data

00:22:46,950 --> 00:22:52,730
so question in part so a long time ago

00:22:49,640 --> 00:22:56,130
in sparkline meaning like nine months

00:22:52,730 --> 00:22:59,040
maybe a year there was no nothing very

00:22:56,130 --> 00:23:01,050
good to do a conjunction support to do

00:22:59,040 --> 00:23:02,700
back pressure you could set you could

00:23:01,050 --> 00:23:05,070
just set a fight right saying that I

00:23:02,700 --> 00:23:08,700
don't want more than so many messages

00:23:05,070 --> 00:23:11,280
passing on which is totally in its isn't

00:23:08,700 --> 00:23:13,380
not regret if one of your not dies your

00:23:11,280 --> 00:23:15,330
number might be too high because you

00:23:13,380 --> 00:23:17,340
cannot process fast enough and if you

00:23:15,330 --> 00:23:18,870
just add more resources to the cursor

00:23:17,340 --> 00:23:20,400
saying that I want to do more but you

00:23:18,870 --> 00:23:22,410
can still not do more because you are

00:23:20,400 --> 00:23:26,070
limited yourself to so many messages per

00:23:22,410 --> 00:23:27,240
second so 1.5 light bent and with the

00:23:26,070 --> 00:23:30,930
help of data bricks which is a

00:23:27,240 --> 00:23:33,300
commercial company behind the spark we

00:23:30,930 --> 00:23:36,350
worked on on edede back pressure in an

00:23:33,300 --> 00:23:38,820
inch back so instead of having a steak a

00:23:36,350 --> 00:23:41,580
static right limit we haven't done any

00:23:38,820 --> 00:23:43,860
correct limit which is estimated at

00:23:41,580 --> 00:23:46,200
every cycle every interval and this

00:23:43,860 --> 00:23:48,900
information in send up the chain to the

00:23:46,200 --> 00:23:51,120
receiver so Zoe sivak ends and then tell

00:23:48,900 --> 00:23:56,010
it it's probably it it's an producer of

00:23:51,120 --> 00:23:59,870
data to please slow down so the way we

00:23:56,010 --> 00:24:02,790
estimate the rights is fairly simple is

00:23:59,870 --> 00:24:05,160
every time there's an execution on a

00:24:02,790 --> 00:24:08,010
mini batch at each interval we know how

00:24:05,160 --> 00:24:11,190
many elements were in the data our long

00:24:08,010 --> 00:24:13,290
it took to execute the three huge

00:24:11,190 --> 00:24:15,150
operation and also if there was any

00:24:13,290 --> 00:24:16,710
scheduling delay and from that we can

00:24:15,150 --> 00:24:19,020
just do a simple division

00:24:16,710 --> 00:24:21,120
we get a number of a lemon pasta one

00:24:19,020 --> 00:24:23,370
that we can process every where nobody

00:24:21,120 --> 00:24:26,490
element per second we can receive and

00:24:23,370 --> 00:24:28,710
process without any program but just

00:24:26,490 --> 00:24:30,630
doing this simple equation just tell us

00:24:28,710 --> 00:24:33,000
what is a theoretical limit and

00:24:30,630 --> 00:24:34,679
sometimes things change and maybe we do

00:24:33,000 --> 00:24:36,510
get some scheduling did at some point

00:24:34,679 --> 00:24:38,640
the system for some reason slow down a

00:24:36,510 --> 00:24:41,370
bit maybe one don't when node died and

00:24:38,640 --> 00:24:44,010
was restarted and so the worse this time

00:24:41,370 --> 00:24:47,250
can always under the right I'm wilt of

00:24:44,010 --> 00:24:50,630
elements what we want but at some point

00:24:47,250 --> 00:24:52,679
it was raw so I did add it did

00:24:50,630 --> 00:24:54,779
accumulate some schedule a some

00:24:52,679 --> 00:24:57,240
scheduling delay so we need also to be

00:24:54,779 --> 00:24:58,940
able to manage to scales so the way we

00:24:57,240 --> 00:25:01,350
manage that is to use in fact

00:24:58,940 --> 00:25:05,340
proportional integrated develop

00:25:01,350 --> 00:25:07,620
algorithm a PID for short which is able

00:25:05,340 --> 00:25:10,710
to manage the fact that some time the

00:25:07,620 --> 00:25:12,450
value you you give you give to the guide

00:25:10,710 --> 00:25:14,250
the value that the system is computing

00:25:12,450 --> 00:25:20,700
might not be quite the right one I need

00:25:14,250 --> 00:25:24,210
to add to any do to change slightly

00:25:20,700 --> 00:25:27,330
value to see a 22 to manage additional

00:25:24,210 --> 00:25:30,270
parameters initial problems this is

00:25:27,330 --> 00:25:34,289
decent this algorithm is nothing new and

00:25:30,270 --> 00:25:37,440
some dress something we used the most

00:25:34,289 --> 00:25:40,140
nada the most famous case that tis

00:25:37,440 --> 00:25:42,299
agonism is loose is to to manage cargo

00:25:40,140 --> 00:25:44,940
ships huge cargo ships when you start to

00:25:42,299 --> 00:25:47,039
turn the wheel at some point so ship

00:25:44,940 --> 00:25:49,529
will turn but if you don't purge the

00:25:47,039 --> 00:25:51,659
wheel back soon enough it will overturn

00:25:49,529 --> 00:25:54,690
so they use this time for algorithm to

00:25:51,659 --> 00:25:57,149
make sure that you get to the limit and

00:25:54,690 --> 00:25:59,520
you basically stage the limit and never

00:25:57,149 --> 00:26:01,320
overshoot too much so it's exactly what

00:25:59,520 --> 00:26:02,730
we wanted to make sure that we we are

00:26:01,320 --> 00:26:06,779
always at the limit of how much we can

00:26:02,730 --> 00:26:11,159
process but never overshoot too much so

00:26:06,779 --> 00:26:15,510
it's very simple to uh to to configure

00:26:11,159 --> 00:26:16,710
just to configuration information and if

00:26:15,510 --> 00:26:19,409
we want to speak calf calf calf calf

00:26:16,710 --> 00:26:25,399
calf calf calf so we get regret for fast

00:26:19,409 --> 00:26:29,720
data information so this is a graph of

00:26:25,399 --> 00:26:33,590
running the same the same test case

00:26:29,720 --> 00:26:36,260
with a system we without dynamic weight

00:26:33,590 --> 00:26:39,140
limit read limit or with the immigrated

00:26:36,260 --> 00:26:41,450
right limit so what we at the bottom is

00:26:39,140 --> 00:26:44,240
by the amount of data that is been sent

00:26:41,450 --> 00:26:46,250
so there's a spike of data which we send

00:26:44,240 --> 00:26:49,130
twice a mood of data and it starts to be

00:26:46,250 --> 00:26:52,280
too much so it takes too long to process

00:26:49,130 --> 00:26:56,480
so we start to commit delay in the case

00:26:52,280 --> 00:26:58,580
with the dynamic rate limiting so it

00:26:56,480 --> 00:27:00,470
overshoots a bit but it did take that

00:26:58,580 --> 00:27:03,950
it's over shooting and I some delay

00:27:00,470 --> 00:27:06,560
which is a which is I did but it it

00:27:03,950 --> 00:27:13,310
reduces delay over time to get something

00:27:06,560 --> 00:27:17,180
which is stable so it fits almost all

00:27:13,310 --> 00:27:20,510
good except that as I still some some

00:27:17,180 --> 00:27:22,820
limitation of what we did it's the

00:27:20,510 --> 00:27:25,160
computer shirt so the basic estimation

00:27:22,820 --> 00:27:28,160
is done on the assumption that it's that

00:27:25,160 --> 00:27:30,320
execution is linear for every every

00:27:28,160 --> 00:27:34,010
record in your data it takes the same

00:27:30,320 --> 00:27:36,650
time to possess it which is not always

00:27:34,010 --> 00:27:39,350
true sometimes some values maybe take

00:27:36,650 --> 00:27:40,910
more time or your goals i might be a

00:27:39,350 --> 00:27:43,450
slightly ex-partner sure so if you have

00:27:40,910 --> 00:27:46,580
not the tight x multi to process it is

00:27:43,450 --> 00:27:49,130
not all your program because the idea

00:27:46,580 --> 00:27:52,940
will manage the case and and gives you

00:27:49,130 --> 00:27:54,590
so to a stable value but it's a bit like

00:27:52,940 --> 00:27:56,630
this example this example we do

00:27:54,590 --> 00:28:00,050
overshoot even if we are able to compute

00:27:56,630 --> 00:28:03,500
correctly it's because with example it's

00:28:00,050 --> 00:28:05,330
using a not too smart receiver that when

00:28:03,500 --> 00:28:08,360
you get to the limit it's slow down a

00:28:05,330 --> 00:28:10,720
bit so it's why we got chasing so so

00:28:08,360 --> 00:28:15,110
yeah so there's a few limitation which

00:28:10,720 --> 00:28:17,300
impractical are annoying but not not

00:28:15,110 --> 00:28:21,910
breaking the system so it's kind of

00:28:17,300 --> 00:28:25,450
something to know so yeah rocky frames

00:28:21,910 --> 00:28:27,980
so I've Rory active application which is

00:28:25,450 --> 00:28:29,900
compatible egg which is which follows

00:28:27,980 --> 00:28:32,300
right give me festo which as a full

00:28:29,900 --> 00:28:34,040
support for back pressure I have spark

00:28:32,300 --> 00:28:36,410
streaming which I can enable back

00:28:34,040 --> 00:28:38,570
pressure on it so now I need a way to

00:28:36,410 --> 00:28:41,600
connect the two together and reactive

00:28:38,570 --> 00:28:44,500
stream is exactly that so r active

00:28:41,600 --> 00:28:48,790
stream is a we shot

00:28:44,500 --> 00:28:53,540
to spec library whatever a set of

00:28:48,790 --> 00:28:56,030
classes which has meant to connect to

00:28:53,540 --> 00:28:58,940
system which have flow control back

00:28:56,030 --> 00:29:01,220
pressure so if I can just show you the

00:28:58,940 --> 00:29:03,020
web page so that directive that rack

00:29:01,220 --> 00:29:06,920
ustream.com is a description of the page

00:29:03,020 --> 00:29:09,170
and and this is really the API of the of

00:29:06,920 --> 00:29:11,030
the of the interfaces that are defined

00:29:09,170 --> 00:29:12,980
and in fact there's only three

00:29:11,030 --> 00:29:16,370
interesting one processor is just a

00:29:12,980 --> 00:29:18,800
combination of the two two of those ipi

00:29:16,370 --> 00:29:20,480
and two in two tours i7 methods to a

00:29:18,800 --> 00:29:24,290
grant so it's fairly simple to a plant

00:29:20,480 --> 00:29:26,570
but it always looks like my thing but it

00:29:24,290 --> 00:29:29,330
allows you to connect system that no the

00:29:26,570 --> 00:29:32,540
buyback partial a boot flow control ok

00:29:29,330 --> 00:29:37,210
let's get back to the demo which I

00:29:32,540 --> 00:29:40,390
didn't really reset which is I'm not in

00:29:37,210 --> 00:29:45,980
my timing for my demo is like with weird

00:29:40,390 --> 00:29:48,490
so let's restart erasing and enable back

00:29:45,980 --> 00:29:51,800
pressure and interconnection between a

00:29:48,490 --> 00:29:59,770
spot swimming and my application and see

00:29:51,800 --> 00:29:59,770
how we work how it does it start

00:30:02,919 --> 00:30:10,210
come on faster Sparky so yeah I said if

00:30:08,590 --> 00:30:13,480
some time when I do protection for Spock

00:30:10,210 --> 00:30:15,429
Spock is for big data it sometimes takes

00:30:13,480 --> 00:30:17,950
time to small things because it doesn't

00:30:15,429 --> 00:30:21,909
do s'mores and it does big it up so it

00:30:17,950 --> 00:30:24,249
takes time to start n stuff so what was

00:30:21,909 --> 00:30:25,960
I configured to so as configure to send

00:30:24,249 --> 00:30:33,460
10 message for second which is nothing

00:30:25,960 --> 00:30:34,950
so let's get back to the 1000 and so

00:30:33,460 --> 00:30:39,820
yeah so the number of elements

00:30:34,950 --> 00:30:42,940
agro-processing time is whatever testing

00:30:39,820 --> 00:30:45,759
to beat higher than before now it's fine

00:30:42,940 --> 00:30:48,669
and we don't ask it in July because we

00:30:45,759 --> 00:30:53,320
don't have too much data we get back to

00:30:48,669 --> 00:30:56,679
the two thousand and we should still be

00:30:53,320 --> 00:31:00,399
fine because it's still on the other the

00:30:56,679 --> 00:31:03,100
rate of the maximum you can handle we

00:31:00,399 --> 00:31:08,220
are just under one second but we are

00:31:03,100 --> 00:31:10,570
still fine and so when i switch to 4000

00:31:08,220 --> 00:31:13,539
so we switch to photos and it goes for

00:31:10,570 --> 00:31:15,639
24 thousand and in fact the rate i see

00:31:13,539 --> 00:31:19,539
as a random number generator is around

00:31:15,639 --> 00:31:24,519
2000 2500 so this is really showing that

00:31:19,539 --> 00:31:27,570
the full the full chain from spark

00:31:24,519 --> 00:31:30,879
streaming so if i get back to the

00:31:27,570 --> 00:31:33,789
faltering ashram a copy of it so sparse

00:31:30,879 --> 00:31:35,769
swimming get the data and it computer

00:31:33,789 --> 00:31:37,869
magic and under and and in this

00:31:35,769 --> 00:31:40,629
connection it says how much did I

00:31:37,869 --> 00:31:42,249
calendar which NTC formation is patch

00:31:40,629 --> 00:31:44,109
all the way back to the random number

00:31:42,249 --> 00:31:47,470
generator and the random number

00:31:44,109 --> 00:31:51,279
generator decide what to do because it

00:31:47,470 --> 00:31:53,590
cannot crush enough information so in my

00:31:51,279 --> 00:31:55,989
small demo what I do is I just drop

00:31:53,590 --> 00:31:58,359
values as ice too many values i just

00:31:55,989 --> 00:32:00,580
drove values which is something that's

00:31:58,359 --> 00:32:02,529
in a real real use case it's something

00:32:00,580 --> 00:32:06,850
that it is possible if what you are

00:32:02,529 --> 00:32:09,399
doing is processing rugs and if you need

00:32:06,850 --> 00:32:11,169
in addition to our which which is done

00:32:09,399 --> 00:32:13,809
most of the time in system like that is

00:32:11,169 --> 00:32:14,919
you process a lot as far as the apostles

00:32:13,809 --> 00:32:16,869
allowed

00:32:14,919 --> 00:32:20,230
is likely and the same time just tell

00:32:16,869 --> 00:32:21,909
them for further analysis so if you have

00:32:20,230 --> 00:32:24,309
a lot of rugs you you may want to just

00:32:21,909 --> 00:32:26,830
drop in photogs and maybe even one

00:32:24,309 --> 00:32:29,350
England because if you start to get too

00:32:26,830 --> 00:32:31,119
much too much data what you still want

00:32:29,350 --> 00:32:33,369
to be able to process or else you want

00:32:31,119 --> 00:32:35,259
to know if there is any program so it is

00:32:33,369 --> 00:32:37,929
one solution that if you detect that you

00:32:35,259 --> 00:32:40,059
cannot push data fast enough you can

00:32:37,929 --> 00:32:41,499
just start to drop data if if you know

00:32:40,059 --> 00:32:44,859
that some data which is less important

00:32:41,499 --> 00:32:46,419
Oh another solution which is working or

00:32:44,859 --> 00:32:49,570
you are also your application can just

00:32:46,419 --> 00:32:51,279
see oh you left to to mature at

00:32:49,570 --> 00:32:55,359
different rates but if you if you museum

00:32:51,279 --> 00:32:57,279
or the rate of of data you try to send

00:32:55,359 --> 00:32:59,980
and the right of data you are able to

00:32:57,279 --> 00:33:02,529
send at this level you can see Oh sparks

00:32:59,980 --> 00:33:05,499
rooming is not fast enough what can I do

00:33:02,529 --> 00:33:07,210
can just restart on unit and then this

00:33:05,499 --> 00:33:09,279
vacuum it should be faster sputtering

00:33:07,210 --> 00:33:13,809
sputtering it's very linear scale aboard

00:33:09,279 --> 00:33:18,850
so you're good with that ok I'll get

00:33:13,809 --> 00:33:21,820
back to the right side yeah so that's

00:33:18,850 --> 00:33:23,529
basically what i have so you show is

00:33:21,820 --> 00:33:25,840
part in your application and you won't

00:33:23,529 --> 00:33:29,320
make sure that you are covering most of

00:33:25,840 --> 00:33:31,539
the case you can push up form sputtering

00:33:29,320 --> 00:33:33,519
the formation that sparks we might not

00:33:31,539 --> 00:33:35,769
be fast enough to process the data and

00:33:33,519 --> 00:33:41,340
then make smart decision about what you

00:33:35,769 --> 00:33:41,340
can do in a specific case ok thanks

00:33:48,099 --> 00:34:01,579
we are getting with Christian yeah so my

00:33:59,450 --> 00:34:04,700
question is when you connect the

00:34:01,579 --> 00:34:07,309
reactive application to spark yeah do

00:34:04,700 --> 00:34:09,769
you have any examples of how you do that

00:34:07,309 --> 00:34:14,899
or are there some ready-made connectors

00:34:09,769 --> 00:34:19,179
and so I'm using a homegrown because it

00:34:14,899 --> 00:34:21,500
was simpler for me with guys there's a

00:34:19,179 --> 00:34:24,109
implementation of practice stream of

00:34:21,500 --> 00:34:26,240
rtcp that exists which is doing more

00:34:24,109 --> 00:34:28,039
than what I'm doing which which is allow

00:34:26,240 --> 00:34:29,899
you to send even multiple streams on

00:34:28,039 --> 00:34:32,299
over all the same TCP connection and the

00:34:29,899 --> 00:34:35,029
backpressure information also all the

00:34:32,299 --> 00:34:37,250
connection I don't remember the name if

00:34:35,029 --> 00:34:38,389
you go to the reactive swim project you

00:34:37,250 --> 00:34:41,589
should go to find out information about

00:34:38,389 --> 00:34:41,589
it okay

00:34:45,179 --> 00:34:50,159
hello I have a question here that's

00:34:48,270 --> 00:34:54,990
right yeah how is a back pressure

00:34:50,159 --> 00:34:57,329
implemented in spark to dot oh wow spark

00:34:54,990 --> 00:34:59,520
to the door still a spark streaming so

00:34:57,329 --> 00:35:02,849
you can see use the sputtering but spike

00:34:59,520 --> 00:35:06,270
swimming is using the old the louisville

00:35:02,849 --> 00:35:08,280
rd dipi and spoke to dot o is pushing

00:35:06,270 --> 00:35:12,420
more for the higher level that I friend

00:35:08,280 --> 00:35:14,400
at the State API and on the data friend

00:35:12,420 --> 00:35:17,220
that has eight API is a new streaming

00:35:14,400 --> 00:35:19,410
system which calls free streaming which

00:35:17,220 --> 00:35:23,220
why now has no if no idea of back

00:35:19,410 --> 00:35:26,309
pressure so in 20 structural framing we

00:35:23,220 --> 00:35:28,530
will be still be experimental and not

00:35:26,309 --> 00:35:29,670
even support we were stream of data the

00:35:28,530 --> 00:35:33,510
only thing that you will go to support

00:35:29,670 --> 00:35:35,579
is a fact that it's looking at new

00:35:33,510 --> 00:35:37,619
element new information which is having

00:35:35,579 --> 00:35:40,290
a full del Sol possessing new files that

00:35:37,619 --> 00:35:45,150
are landing in the folder so i spoke

00:35:40,290 --> 00:35:46,950
with with TD the many product of a spot

00:35:45,150 --> 00:35:50,160
of sweet OH screaming at that are weeks

00:35:46,950 --> 00:35:53,540
and right now they don't even have a

00:35:50,160 --> 00:35:56,190
clear view of what will be the API for

00:35:53,540 --> 00:35:59,160
more custom receiver so it's still

00:35:56,190 --> 00:36:01,559
something which is on flex and there

00:35:59,160 --> 00:36:06,869
won't be any real answer before 21 we be

00:36:01,559 --> 00:36:08,700
phones back to one okay thank you um the

00:36:06,869 --> 00:36:10,200
standard or what does it say in the

00:36:08,700 --> 00:36:12,809
definition of back pressure when you

00:36:10,200 --> 00:36:15,150
handle multiple consumers like I got one

00:36:12,809 --> 00:36:17,670
consumer who is too slow the other one

00:36:15,150 --> 00:36:19,650
is way fast enough you can handle it but

00:36:17,670 --> 00:36:21,420
you apply the back pressure up to the

00:36:19,650 --> 00:36:23,670
source yes so you do apprise the back

00:36:21,420 --> 00:36:25,349
pressure by default by super importation

00:36:23,670 --> 00:36:27,720
you implement back pressure up to the

00:36:25,349 --> 00:36:30,390
source and then as I said my thing is

00:36:27,720 --> 00:36:33,900
you may want to measure it lower

00:36:30,390 --> 00:36:37,740
somewhere lower in in the chain in the

00:36:33,900 --> 00:36:39,299
intergraph and do your smart resolution

00:36:37,740 --> 00:36:41,849
of the fact that the system is too slow

00:36:39,299 --> 00:36:46,099
I'd look at the G junction point I just

00:36:41,849 --> 00:36:46,099
a Saudi junkies junction point

00:36:48,200 --> 00:36:54,980
I thanks for talk I just want to check

00:36:52,220 --> 00:36:57,560
is your source code open available to a

00:36:54,980 --> 00:37:09,589
I need to push it I will be on Scylla

00:36:57,560 --> 00:37:11,660
keytab s KY Luc okay thank you could you

00:37:09,589 --> 00:37:13,430
could you show the source code of where

00:37:11,660 --> 00:37:16,640
of the point where you're handing over

00:37:13,430 --> 00:37:19,010
the data from acha streams to spark

00:37:16,640 --> 00:37:22,369
streaming oh this is the interface where

00:37:19,010 --> 00:37:26,030
you use the the reactive interface I

00:37:22,369 --> 00:37:35,390
think yeah yeah so yeah okay let's see

00:37:26,030 --> 00:37:40,369
if I can waste is good i ck have it done

00:37:35,390 --> 00:37:49,750
big bucket I to find my car again there

00:37:40,369 --> 00:37:49,750
we go well is my code spagna moose right

00:37:51,339 --> 00:38:09,440
yeah by parties such as yeah so says

00:37:55,339 --> 00:38:13,460
where I can yes so what I have is yeah

00:38:09,440 --> 00:38:15,950
so spark so the way to to create new or

00:38:13,460 --> 00:38:19,400
receiver in spark make trying make

00:38:15,950 --> 00:38:23,270
bigger the way to create a new receiver

00:38:19,400 --> 00:38:25,790
a specific receiver in spark is to to

00:38:23,270 --> 00:38:28,910
create a receiver input streams which is

00:38:25,790 --> 00:38:34,280
the basic API of of custom receivers in

00:38:28,910 --> 00:38:36,859
spark so I what I created is a receiver

00:38:34,280 --> 00:38:43,490
in spark that when spark a skid to

00:38:36,859 --> 00:38:46,550
connect to something equate equator

00:38:43,490 --> 00:38:47,720
equator subscribers all in interactive

00:38:46,550 --> 00:38:49,490
in a

00:38:47,720 --> 00:38:50,960
yeah in reactive stream you have to be

00:38:49,490 --> 00:38:54,109
sure and subscribe us subscribe us

00:38:50,960 --> 00:38:56,630
subscribe to the Grisha and what I do is

00:38:54,109 --> 00:39:00,530
I do have a publisher of data which is

00:38:56,630 --> 00:39:01,849
my connection my to TCP two to the right

00:39:00,530 --> 00:39:06,070
ear application which is on the phone

00:39:01,849 --> 00:39:08,720
box and I just quit the link in the Java

00:39:06,070 --> 00:39:14,420
in Jerusalem inside I create the link

00:39:08,720 --> 00:39:16,520
between between my OS severe and Angel

00:39:14,420 --> 00:39:20,780
publisher which is already connected to

00:39:16,520 --> 00:39:24,650
the to the to the end point of the right

00:39:20,780 --> 00:39:31,760
ear application itself and I mean just

00:39:24,650 --> 00:39:35,140
so what I get on the ride it yeah so

00:39:31,760 --> 00:39:37,700
what I get from spark is update of the

00:39:35,140 --> 00:39:39,980
number of message passing old I can

00:39:37,700 --> 00:39:42,890
receive again yeah spot says it can

00:39:39,980 --> 00:39:45,020
receive and I have some some custom code

00:39:42,890 --> 00:39:49,700
which is a bit ugly but I like to play

00:39:45,020 --> 00:39:51,710
with it which transformed this idea of

00:39:49,700 --> 00:39:53,990
requests pastor God in something which

00:39:51,710 --> 00:39:57,140
makes sense in the interactive stream

00:39:53,990 --> 00:40:00,800
applique like this frame API because

00:39:57,140 --> 00:40:04,160
rectus frame API is just the subscriber

00:40:00,800 --> 00:40:05,630
tears a producer I can i can now at this

00:40:04,160 --> 00:40:07,490
point i can receive one other one

00:40:05,630 --> 00:40:10,160
message and then later i said at this

00:40:07,490 --> 00:40:11,900
point i cannot one more we see more one

00:40:10,160 --> 00:40:13,400
on one more message and singer that so

00:40:11,900 --> 00:40:16,520
that's that my connection between the

00:40:13,400 --> 00:40:21,010
two and then i have relative stream over

00:40:16,520 --> 00:40:23,180
tcp implementation which connect on the

00:40:21,010 --> 00:40:24,710
Equestria application inside to make the

00:40:23,180 --> 00:40:33,920
connection between everything and send

00:40:24,710 --> 00:40:36,470
the backpressure information over so

00:40:33,920 --> 00:40:42,770
does the sparks I does it support

00:40:36,470 --> 00:40:45,080
ssl/tls no in fact laughs so this is a

00:40:42,770 --> 00:40:47,720
the SS air support cas will be in the

00:40:45,080 --> 00:40:54,170
right key stream implantation of our tcp

00:40:47,720 --> 00:40:56,450
you use the connection i do on on on on

00:40:54,170 --> 00:40:58,700
spark is really inside spark inside the

00:40:56,450 --> 00:41:03,290
receiver which is one buys back and then

00:40:58,700 --> 00:41:06,860
so basically you you change you chain a

00:41:03,290 --> 00:41:09,770
chain reactive stream elements so our

00:41:06,860 --> 00:41:11,450
registry which is my my right to

00:41:09,770 --> 00:41:13,280
application which is racing to assist

00:41:11,450 --> 00:41:17,390
reactive string component the component

00:41:13,280 --> 00:41:20,270
which is a reactive stream of rtcp which

00:41:17,390 --> 00:41:24,560
in my case is really simple and just do

00:41:20,270 --> 00:41:27,110
over tcp the more feature one I don't

00:41:24,560 --> 00:41:28,760
know if it support areas and then agenda

00:41:27,110 --> 00:41:34,330
of the connection between this connector

00:41:28,760 --> 00:41:34,330
and like a spider manga turf

00:41:39,000 --> 00:41:47,920
hello how does back pressure works when

00:41:44,680 --> 00:41:54,690
the bottleneck is not the processing but

00:41:47,920 --> 00:42:01,290
the network it depends of what use for

00:41:54,690 --> 00:42:09,640
the as a transfer was a network so in

00:42:01,290 --> 00:42:11,619
trying to think so yes beautiful Kate I

00:42:09,640 --> 00:42:14,109
don't know I want super tunities in fact

00:42:11,619 --> 00:42:17,260
it's a it's a really good question of in

00:42:14,109 --> 00:42:24,880
my simple and protection it might be a

00:42:17,260 --> 00:42:26,440
program so it let me think no yeah so in

00:42:24,880 --> 00:42:29,470
my simple amputation is not a program

00:42:26,440 --> 00:42:32,440
because I start to ask for more elements

00:42:29,470 --> 00:42:34,900
when when I received in your fin amounts

00:42:32,440 --> 00:42:36,520
so if the network is too slow and I

00:42:34,900 --> 00:42:39,549
don't receive enough elements I don't

00:42:36,520 --> 00:42:41,380
ask for more so I mean the back way show

00:42:39,549 --> 00:42:43,359
is always walking like that you receive

00:42:41,380 --> 00:42:45,369
some data you have basically some prefer

00:42:43,359 --> 00:42:48,430
size we know you can use and you ask for

00:42:45,369 --> 00:42:50,890
monuments so in my invitation yeah it

00:42:48,430 --> 00:42:52,480
basically I will not ask for more data

00:42:50,890 --> 00:42:55,359
because I haven't receive all the data I

00:42:52,480 --> 00:42:59,470
can I can order yet but you you might

00:42:55,359 --> 00:43:02,020
still get in trouble because if if you

00:42:59,470 --> 00:43:05,500
start to fill up your tcpdf else then

00:43:02,020 --> 00:43:09,220
you producer cannot push to the TCP it

00:43:05,500 --> 00:43:13,049
will it will it will wait to rock so the

00:43:09,220 --> 00:43:16,349
implantation of the I give the stream

00:43:13,049 --> 00:43:18,730
API over TCP need to be done quickly

00:43:16,349 --> 00:43:20,819
thanks everybody and have a good

00:43:18,730 --> 00:43:20,819
afternoon

00:43:22,830 --> 00:43:24,890

YouTube URL: https://www.youtube.com/watch?v=K4X3XotD_6M


