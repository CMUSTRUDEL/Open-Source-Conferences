Title: Scala and the JVM as a Big Data Platform   Lessons from Apache Spark   by Dean Wampler
Publication date: 2016-07-22
Playlist: Scala Days Berlin 2016
Description: 
	This video was recorded at Scala Days Berlin 2016
follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org 

Abstract:
Spark is implemented in Scala and its user-facing Scala API is very similar to Scala's own Collections API. The power and concision of this API are bringing many developers to Scala. The core abstractions in Spark have created a flexible, extensible platform for applications like streaming, SQL queries, machine learning, and more.
 
Scala's uptake reflects the following advantages over Java:
A pragmatic balance of object-oriented and functional programming.
An interpreter mode, which allows the same sort of exploratory programming that Data Scientists have enjoyed with Python and other languages. Scala-centric "Notebooks" are also now available.
A rich Collections library that enables composition of operations for concise, powerful code.
Tuples are naturally expressed in Scala and very convenient for working with data.
Pattern Matching makes data "deconstruction" fast and intuitive.
Type inference provides safety, feedback to the developer, yet minimal, manual typing of actual type signatures.
Scala idioms lend themselves to the construction of small domain specific languages, which are useful for building libraries that are concise and intuitive for domain experts.
There are disadvantages, too, which we'll discuss.

Spark, like almost all open-source, Big Data tools, leverages the JVM, which is an excellent, general-purpose platform for scalable computing. However, its management of objects is suboptimal for high-performance data crunching. The way objects are organized in memory and the subsequent impact that has on garbage collection can be improved for the special case of Big Data. Hence, the Spark project has recently started a project called "Tungsten" to build internal optimizations using the following techniques:
* Custom data layouts that use memory very efficiently with cache-awareness.
* Manual memory management, both on-heap and off-heap, to minimize garbage and GC pressure.
* Code generation to create optimal implementations of certain, heavily-used expressions from user code.
This talk discusses the strengths and weaknesses of Scala and the JVM for Big Data, Spark in particular, and how we might improve both to make them better tools for our needs.
Captions: 
	00:00:05,590 --> 00:00:11,299
my name is Dean wampler this is a kind

00:00:08,719 --> 00:00:13,309
of my experiences with using spark and

00:00:11,299 --> 00:00:14,600
not with just my experiences but what

00:00:13,309 --> 00:00:17,570
what a lot of us have learned about

00:00:14,600 --> 00:00:21,890
strengths and weaknesses of the JVM and

00:00:17,570 --> 00:00:23,420
Scala that I always put my photography

00:00:21,890 --> 00:00:24,800
in these things so if you're bored with

00:00:23,420 --> 00:00:26,840
the content you can at least look at

00:00:24,800 --> 00:00:28,789
pretty picture sir hopefully this is

00:00:26,840 --> 00:00:31,280
actually Olympic National Park which is

00:00:28,789 --> 00:00:33,649
in the northwest corner of the

00:00:31,280 --> 00:00:36,340
continental US west of Seattle and

00:00:33,649 --> 00:00:39,880
southwest of British Columbia or

00:00:36,340 --> 00:00:43,610
Vancouver and British Columbia Canada

00:00:39,880 --> 00:00:44,930
okay you can actually get this talk and

00:00:43,610 --> 00:00:47,360
actually there's a longer version of

00:00:44,930 --> 00:00:48,829
this talk at my vanity website polyglot

00:00:47,360 --> 00:00:50,559
programming comm and of course they'll

00:00:48,829 --> 00:00:52,820
be on the conference site as well and

00:00:50,559 --> 00:00:54,410
obviously I work for light band it's my

00:00:52,820 --> 00:00:57,380
job to figure out what we should do with

00:00:54,410 --> 00:00:59,899
spark and other big data tools as maybe

00:00:57,380 --> 00:01:02,090
you may know a lot of big data tools

00:00:59,899 --> 00:01:05,899
adopted spark is their core language

00:01:02,090 --> 00:01:07,670
like spark adopted Scala rather like

00:01:05,899 --> 00:01:09,679
spark and Kafka and it's actually

00:01:07,670 --> 00:01:12,709
driving adoption of Scala right now

00:01:09,679 --> 00:01:14,659
which is kind of cool so Bert first let

00:01:12,709 --> 00:01:17,659
me briefly talk about spark so how many

00:01:14,659 --> 00:01:18,679
of you've actually used spark ok great

00:01:17,659 --> 00:01:19,999
most of you so I'll go through this

00:01:18,679 --> 00:01:21,529
fairly quickly because this should be

00:01:19,999 --> 00:01:23,119
mostly review but just to give you a

00:01:21,529 --> 00:01:24,979
sense of it for those of you that have

00:01:23,119 --> 00:01:27,499
never dealt with it before this is a

00:01:24,979 --> 00:01:30,770
areas famous for rain forests which is

00:01:27,499 --> 00:01:32,749
in really big trees so it's basically a

00:01:30,770 --> 00:01:35,240
distributed computing engine for the JVM

00:01:32,749 --> 00:01:37,159
is designed for data centric computation

00:01:35,240 --> 00:01:38,990
where you write a job and as you'll see

00:01:37,159 --> 00:01:41,060
the abstractions look just like the

00:01:38,990 --> 00:01:44,240
Scala collections at least if you're

00:01:41,060 --> 00:01:46,099
using the Scala API and yet it's

00:01:44,240 --> 00:01:47,659
actually distributed over a cluster so

00:01:46,099 --> 00:01:49,310
you can work with you know terabytes to

00:01:47,659 --> 00:01:51,590
petabytes of data with reasonable

00:01:49,310 --> 00:01:55,490
performance as long as your clusters big

00:01:51,590 --> 00:01:56,840
enough instead of vectors and lists and

00:01:55,490 --> 00:01:58,399
things like that that we typically work

00:01:56,840 --> 00:02:00,950
with there's this thing they called it

00:01:58,399 --> 00:02:02,989
resilient distributed data set which is

00:02:00,950 --> 00:02:05,959
a collection that's partitioned over the

00:02:02,989 --> 00:02:07,819
cluster and so it runs in parallel when

00:02:05,959 --> 00:02:09,560
it's processing each of these partitions

00:02:07,819 --> 00:02:12,260
and it's resilient in the sense that

00:02:09,560 --> 00:02:14,330
it's actually computing the graph of

00:02:12,260 --> 00:02:16,160
processing steps that you've defined so

00:02:14,330 --> 00:02:18,320
if it loses a node

00:02:16,160 --> 00:02:20,480
or another way loose is a partition it

00:02:18,320 --> 00:02:23,540
can go back and reconstruct it so that's

00:02:20,480 --> 00:02:25,580
where the resilient comes in and it

00:02:23,540 --> 00:02:28,010
actually not only supports Scala in a

00:02:25,580 --> 00:02:30,200
Java API but because data scientists

00:02:28,010 --> 00:02:32,810
tend to still prefer Python and are

00:02:30,200 --> 00:02:34,610
despite the arguments that Andy Petrella

00:02:32,810 --> 00:02:37,130
and I made yesterday

00:02:34,610 --> 00:02:39,740
it also supports Python an R or sequel

00:02:37,130 --> 00:02:43,430
and we'll see quick examples of that as

00:02:39,740 --> 00:02:45,800
well and importantly for all of us that

00:02:43,430 --> 00:02:47,330
are you mean this is like table stakes

00:02:45,800 --> 00:02:49,490
now for most of us but being able to

00:02:47,330 --> 00:02:51,380
work interactively with code is really

00:02:49,490 --> 00:02:53,420
nice but it's also very important when

00:02:51,380 --> 00:02:54,950
you're doing interactive exploration of

00:02:53,420 --> 00:02:57,950
data and if you've ever used like a

00:02:54,950 --> 00:02:59,900
sequel environment to you know look at

00:02:57,950 --> 00:03:01,640
data that you've got in a database it's

00:02:59,900 --> 00:03:04,160
the same experience it's really crucial

00:03:01,640 --> 00:03:05,870
not to have to write and compile and run

00:03:04,160 --> 00:03:10,150
a program every time you want to do a

00:03:05,870 --> 00:03:13,190
query or some other interactive step and

00:03:10,150 --> 00:03:15,590
because of these repple properties it

00:03:13,190 --> 00:03:17,750
supports these nice user interface is

00:03:15,590 --> 00:03:19,520
called notebooks where this is actually

00:03:17,750 --> 00:03:21,650
a screenshot from one we went through my

00:03:19,520 --> 00:03:24,740
talk yesterday where you can intermix

00:03:21,650 --> 00:03:26,840
like markdown documentation actual code

00:03:24,740 --> 00:03:28,610
that's run in the repple you can graph

00:03:26,840 --> 00:03:30,140
results and all that kind of stuff and

00:03:28,610 --> 00:03:31,850
there's several of them out there now

00:03:30,140 --> 00:03:34,370
this happened to be spark notebook which

00:03:31,850 --> 00:03:37,790
is Java centric but Jupiter is one of

00:03:34,370 --> 00:03:40,070
the sort of pioneers maybe not quite a

00:03:37,790 --> 00:03:44,150
pioneer but one of the more popular ones

00:03:40,070 --> 00:03:45,800
that was based on Python all right so

00:03:44,150 --> 00:03:47,270
with that let's look at an example it's

00:03:45,800 --> 00:03:49,100
one of my favorite ones because it

00:03:47,270 --> 00:03:51,290
really shows how concise you can make

00:03:49,100 --> 00:03:53,990
code and yet do something non-trivial in

00:03:51,290 --> 00:03:55,340
spark it's the inverted index this is

00:03:53,990 --> 00:03:56,690
sort of what you would do first if you

00:03:55,340 --> 00:03:59,240
wanted to build a search engine you

00:03:56,690 --> 00:04:01,340
suppose you have like a corpus of

00:03:59,240 --> 00:04:03,440
documents or something like the internet

00:04:01,340 --> 00:04:05,750
or you know the World Wide Web you want

00:04:03,440 --> 00:04:07,760
to index it so that you start with say a

00:04:05,750 --> 00:04:10,130
reference to a location and then the

00:04:07,760 --> 00:04:11,930
contents and invert it so that you end

00:04:10,130 --> 00:04:14,120
up with an index of all the words that

00:04:11,930 --> 00:04:16,640
are found at each place and then a list

00:04:14,120 --> 00:04:19,010
of locations and counts so the counts

00:04:16,640 --> 00:04:21,049
are important because obviously you know

00:04:19,010 --> 00:04:23,390
if I'm searching for the word like spark

00:04:21,049 --> 00:04:24,979
I'd like to find the pages on the

00:04:23,390 --> 00:04:26,510
internet that really talk about spark

00:04:24,979 --> 00:04:28,340
and I just mentioned in passing

00:04:26,510 --> 00:04:29,490
so counts are a good measure of

00:04:28,340 --> 00:04:33,030
relevance you know

00:04:29,490 --> 00:04:35,069
approximate measure and this this is the

00:04:33,030 --> 00:04:37,470
whole program the entire thing that you

00:04:35,069 --> 00:04:39,419
would might write in the repple using

00:04:37,470 --> 00:04:41,190
Scala and I'll actually go through this

00:04:39,419 --> 00:04:42,870
blow it up a little bit so you can read

00:04:41,190 --> 00:04:47,550
it but it's kind of cool you can put a

00:04:42,870 --> 00:04:48,780
whole thing on a slide actually I'll

00:04:47,550 --> 00:04:49,979
come back to that in a little bit but

00:04:48,780 --> 00:04:53,370
first let me just talk about the

00:04:49,979 --> 00:04:55,400
structure of it let me back at one thing

00:04:53,370 --> 00:04:58,080
so I chose to the color scheme

00:04:55,400 --> 00:04:59,849
specifically to emphasize that you know

00:04:58,080 --> 00:05:02,190
functional transformations are really

00:04:59,849 --> 00:05:03,870
the key here so they're in red so all

00:05:02,190 --> 00:05:06,300
the red stuff or things that are doing

00:05:03,870 --> 00:05:09,210
common operations like mapping flat

00:05:06,300 --> 00:05:11,069
mapping filtering reduced by kiyose is

00:05:09,210 --> 00:05:13,080
like a optimized group by which

00:05:11,069 --> 00:05:17,039
hopefully group bys familiar from sequel

00:05:13,080 --> 00:05:18,690
and so forth so you have these nodes of

00:05:17,039 --> 00:05:20,460
processing that you're defining this

00:05:18,690 --> 00:05:22,830
I've sort of captured them schematically

00:05:20,460 --> 00:05:24,630
on the right and once you understand all

00:05:22,830 --> 00:05:27,270
of these little tools in your toolbox

00:05:24,630 --> 00:05:28,979
like like for Scala collections then you

00:05:27,270 --> 00:05:31,199
can crank this stuff out really quickly

00:05:28,979 --> 00:05:33,419
so the first time I wrote this example

00:05:31,199 --> 00:05:35,250
it took me maybe 30 minutes to write it

00:05:33,419 --> 00:05:37,710
because I knew all of those steps and

00:05:35,250 --> 00:05:40,080
you know you get you get experience you

00:05:37,710 --> 00:05:41,639
know decomposing a problem into you know

00:05:40,080 --> 00:05:43,860
sequence of steps and then you can crank

00:05:41,639 --> 00:05:45,840
stuff out quickly and not get caught up

00:05:43,860 --> 00:05:48,330
in the boilerplate of you know more

00:05:45,840 --> 00:05:49,979
verbose Java or whatever but what's

00:05:48,330 --> 00:05:51,840
actually happening is you're defining a

00:05:49,979 --> 00:05:54,270
dataflow of steps the directed acyclic

00:05:51,840 --> 00:05:55,800
graph to be specific this is linear but

00:05:54,270 --> 00:05:58,530
I could easily have you know sort of

00:05:55,800 --> 00:06:01,380
branching and joining of data

00:05:58,530 --> 00:06:03,870
it's lazily defined so nothing actually

00:06:01,380 --> 00:06:06,300
happens until I say give me some results

00:06:03,870 --> 00:06:08,069
in this case right back to the you know

00:06:06,300 --> 00:06:09,990
my file system and then it actually

00:06:08,069 --> 00:06:13,440
figures out how to run this across your

00:06:09,990 --> 00:06:15,690
cluster but the API looks very much like

00:06:13,440 --> 00:06:17,520
the Scala API in terms of I'm passing

00:06:15,690 --> 00:06:19,919
the anonymous functions to map and

00:06:17,520 --> 00:06:24,630
filter and so forth so that's really

00:06:19,919 --> 00:06:26,340
nice the other thing it does it tries to

00:06:24,630 --> 00:06:27,810
be very performant so rather than

00:06:26,340 --> 00:06:29,880
generating you know I said we could be

00:06:27,810 --> 00:06:31,469
doing like petabytes of data right and I

00:06:29,880 --> 00:06:33,750
wouldn't want to instantiate a new

00:06:31,469 --> 00:06:36,240
petabyte collection on each one of those

00:06:33,750 --> 00:06:38,130
steps these are immutable objects these

00:06:36,240 --> 00:06:40,469
are Dedes but it doesn't do that it

00:06:38,130 --> 00:06:42,900
actually only materializes things at the

00:06:40,469 --> 00:06:45,000
end of stages so called what

00:06:42,900 --> 00:06:46,650
a stage well when I'm doing mapping and

00:06:45,000 --> 00:06:48,840
filtering and flatten mapping that kind

00:06:46,650 --> 00:06:51,150
of stuff I can work in isolation with

00:06:48,840 --> 00:06:53,220
each partition I don't have to get data

00:06:51,150 --> 00:06:55,710
between partitions if I'm just mapping

00:06:53,220 --> 00:06:57,990
over each record but if I do something

00:06:55,710 --> 00:06:59,520
like a group by or a join then I have to

00:06:57,990 --> 00:07:01,979
bring together things that share the

00:06:59,520 --> 00:07:04,110
same key and that triggers a new stage

00:07:01,979 --> 00:07:06,240
as they call it but all the things that

00:07:04,110 --> 00:07:07,710
are still bright those are the things

00:07:06,240 --> 00:07:10,020
where I'm going to materialize some

00:07:07,710 --> 00:07:11,729
result in the intermediate case I'll

00:07:10,020 --> 00:07:13,620
write some temporary files and then

00:07:11,729 --> 00:07:16,500
those will be sucked over the cluster to

00:07:13,620 --> 00:07:19,020
the appropriate nodes or I'll you know

00:07:16,500 --> 00:07:20,729
produce my final output in some sense so

00:07:19,020 --> 00:07:23,370
it's fairly efficient about taking that

00:07:20,729 --> 00:07:26,010
graph of processing and synthesizing

00:07:23,370 --> 00:07:31,350
relatively efficient JVM code that

00:07:26,010 --> 00:07:32,880
you're gonna actually run and once you

00:07:31,350 --> 00:07:34,919
have a good core like this then you can

00:07:32,880 --> 00:07:37,889
layer on top some powerful higher-level

00:07:34,919 --> 00:07:39,419
api's like sequel so you can actually

00:07:37,889 --> 00:07:40,680
you know if you think of I always would

00:07:39,419 --> 00:07:42,360
like to think of sequel is like a

00:07:40,680 --> 00:07:44,310
restricted functional programming

00:07:42,360 --> 00:07:46,380
language it's you know based on set

00:07:44,310 --> 00:07:47,820
theory roughly speaking you know we

00:07:46,380 --> 00:07:50,070
don't have anonymous functions and all

00:07:47,820 --> 00:07:52,050
that kind of stuff maybe we have used to

00:07:50,070 --> 00:07:54,449
define functions which sort of faked the

00:07:52,050 --> 00:07:56,460
same thing but it gives us the idea that

00:07:54,449 --> 00:07:58,289
we can you know declarative lily state

00:07:56,460 --> 00:08:00,240
what we want in terms of what kind of

00:07:58,289 --> 00:08:01,949
data we want to see what transforms we

00:08:00,240 --> 00:08:04,320
want to perform our data we want to

00:08:01,949 --> 00:08:06,840
create and then let the runtime optimize

00:08:04,320 --> 00:08:08,550
how that's actually computed and really

00:08:06,840 --> 00:08:10,199
all of the performance optimizations

00:08:08,550 --> 00:08:12,060
that have been happening in spark many

00:08:10,199 --> 00:08:14,159
of which I'll talk about in this talk

00:08:12,060 --> 00:08:16,349
are happening in the context of

00:08:14,159 --> 00:08:18,419
optimizing sequel the other advantage

00:08:16,349 --> 00:08:21,120
for you know doing it that way versus at

00:08:18,419 --> 00:08:23,070
the RDD level is that sequel is a more

00:08:21,120 --> 00:08:24,930
constrained problem you're not trying to

00:08:23,070 --> 00:08:26,669
solve you know boil the ocean or

00:08:24,930 --> 00:08:28,020
whatever metaphor you want to use you're

00:08:26,669 --> 00:08:29,280
trying to solve a more narrow problem

00:08:28,020 --> 00:08:33,599
and that makes it easier to optimize

00:08:29,280 --> 00:08:35,400
data frames this is a word from the

00:08:33,599 --> 00:08:37,740
actual name the class uses it called a

00:08:35,400 --> 00:08:40,289
data frame but it this this API is

00:08:37,740 --> 00:08:43,169
inspired by a Python API called data

00:08:40,289 --> 00:08:45,630
frames which is actually a copy of a our

00:08:43,169 --> 00:08:47,040
language API called data frames and

00:08:45,630 --> 00:08:48,330
that's where the name comes from but I

00:08:47,040 --> 00:08:51,990
like to think of it basically it's just

00:08:48,330 --> 00:08:53,490
sequel and this is what it would look

00:08:51,990 --> 00:08:54,990
like if you wanted to actually write

00:08:53,490 --> 00:08:56,460
this let's see I think I'm drilling into

00:08:54,990 --> 00:08:58,920
this yeah here we go okay so

00:08:56,460 --> 00:09:00,660
I start off by creating a spark context

00:08:58,920 --> 00:09:02,880
that's how you always enter your program

00:09:00,660 --> 00:09:04,560
it knows like the master argument here

00:09:02,880 --> 00:09:06,600
is where the cluster is and things like

00:09:04,560 --> 00:09:08,100
that and then you wrap that in this

00:09:06,600 --> 00:09:10,530
thing called a sequel context that

00:09:08,100 --> 00:09:13,230
brings in this sequel abstraction or

00:09:10,530 --> 00:09:14,760
data frame abstraction and then there's

00:09:13,230 --> 00:09:17,070
some nice facilities where if I'm

00:09:14,760 --> 00:09:19,740
storing data in what happens to be a

00:09:17,070 --> 00:09:21,690
very popular format called parquet it

00:09:19,740 --> 00:09:24,480
gives you column storage and compression

00:09:21,690 --> 00:09:25,440
and some other advantages then I can

00:09:24,480 --> 00:09:27,540
just say alright I want to read this

00:09:25,440 --> 00:09:28,950
parquet data and the scheme is carried

00:09:27,540 --> 00:09:31,650
with it so I know exactly what the

00:09:28,950 --> 00:09:33,300
records are and I'm actually loading

00:09:31,650 --> 00:09:35,700
this is a data that you can get off the

00:09:33,300 --> 00:09:38,790
Internet's it's actually the all the

00:09:35,700 --> 00:09:42,510
public or rather a commercial flights in

00:09:38,790 --> 00:09:45,090
North America from like 2000 actually I

00:09:42,510 --> 00:09:46,470
think it goes back to 1985 in fact but

00:09:45,090 --> 00:09:47,880
anyway it's kind of a fun dataset to

00:09:46,470 --> 00:09:50,820
play with so I'm gonna load the the

00:09:47,880 --> 00:09:52,440
actual flights the actual planes you

00:09:50,820 --> 00:09:54,750
know like the tail number of each you

00:09:52,440 --> 00:09:56,760
know 747 or whatever that's out there

00:09:54,750 --> 00:09:58,680
and I'm gonna register these as

00:09:56,760 --> 00:10:01,140
temporary tables in sort of the same way

00:09:58,680 --> 00:10:02,790
I might do a sequel and caching is this

00:10:01,140 --> 00:10:04,590
is where the abstraction leaks I need to

00:10:02,790 --> 00:10:06,180
tell SPARC look keep this data in memory

00:10:04,590 --> 00:10:08,100
and don't just keep going back to the

00:10:06,180 --> 00:10:10,890
hard disk to get it because I'm gonna

00:10:08,100 --> 00:10:12,690
keep going over and over this data and

00:10:10,890 --> 00:10:14,850
then I can embed sequel queries if I

00:10:12,690 --> 00:10:16,650
want like this I mean this is string Lee

00:10:14,850 --> 00:10:18,180
type programming which we're told it's

00:10:16,650 --> 00:10:19,830
evil right but it's actually really

00:10:18,180 --> 00:10:21,060
great when you're just doing interactive

00:10:19,830 --> 00:10:24,450
stuff because you'll know right away if

00:10:21,060 --> 00:10:25,560
you screwed up the query or well

00:10:24,450 --> 00:10:27,800
actually what am I doing here I'm

00:10:25,560 --> 00:10:30,300
actually joining the flight data and the

00:10:27,800 --> 00:10:32,430
plane data on the tail number and then

00:10:30,300 --> 00:10:34,140
limiting to the first hundred so I get

00:10:32,430 --> 00:10:36,050
more metadata let's say about the

00:10:34,140 --> 00:10:38,100
airplane that actually did this flight

00:10:36,050 --> 00:10:40,290
but I could also do this with this

00:10:38,100 --> 00:10:42,720
programmatic API this a little bit more

00:10:40,290 --> 00:10:44,340
typesafe but there are some issues with

00:10:42,720 --> 00:10:45,990
it that aren't perfect from our point of

00:10:44,340 --> 00:10:47,970
view with Scala developers but

00:10:45,990 --> 00:10:49,500
nevertheless it's it's a way that's a

00:10:47,970 --> 00:10:52,890
little bit safer than just string lee

00:10:49,500 --> 00:10:54,060
type sequel queries now there's

00:10:52,890 --> 00:10:55,980
something interesting about this

00:10:54,060 --> 00:10:58,950
expression at the bottom and i blow it

00:10:55,980 --> 00:11:01,080
up on the next slide so I notice what I

00:10:58,950 --> 00:11:02,160
did I did flights join planes that you

00:11:01,080 --> 00:11:04,530
could think of what the equivalent

00:11:02,160 --> 00:11:06,420
sequel would be and the join criteria is

00:11:04,530 --> 00:11:08,790
take the column that's the tail number

00:11:06,420 --> 00:11:10,600
and flights and I want those to be equal

00:11:08,790 --> 00:11:13,030
to the column or tail

00:11:10,600 --> 00:11:15,250
were in the plane's data set and notice

00:11:13,030 --> 00:11:17,170
the triple equals this sort of looks

00:11:15,250 --> 00:11:18,790
like an anonymous function which is what

00:11:17,170 --> 00:11:21,340
we're used to with things like map and

00:11:18,790 --> 00:11:24,610
flatmap and all that but it's actually a

00:11:21,340 --> 00:11:26,800
mini DSL for expressing this concept

00:11:24,610 --> 00:11:29,770
that it turns into an abstract syntax

00:11:26,800 --> 00:11:31,540
tree of the spark level and then so now

00:11:29,770 --> 00:11:33,940
they know exactly what I'm doing and

00:11:31,540 --> 00:11:35,740
they can do optimizations based on this

00:11:33,940 --> 00:11:37,750
expression whereas if these were

00:11:35,740 --> 00:11:39,250
anonymous functions that I was passing

00:11:37,750 --> 00:11:42,040
the ones that were used to every day

00:11:39,250 --> 00:11:44,440
that becomes completely opaque to the

00:11:42,040 --> 00:11:46,090
library like spark so spark can't do any

00:11:44,440 --> 00:11:49,570
optimizations if this were anonymous

00:11:46,090 --> 00:11:51,880
functions but because of this little

00:11:49,570 --> 00:11:54,460
mini DSL that's constructing an object

00:11:51,880 --> 00:11:55,840
that represents the join criteria it can

00:11:54,460 --> 00:11:57,880
actually do some aggressive

00:11:55,840 --> 00:12:02,200
optimizations about how this will be run

00:11:57,880 --> 00:12:04,480
when I actually do this query so that

00:12:02,200 --> 00:12:06,100
turns out to be pretty important and it

00:12:04,480 --> 00:12:09,100
turns out when they introduce this new

00:12:06,100 --> 00:12:10,660
data frame sequel API this is what

00:12:09,100 --> 00:12:13,600
happened with the performance numbers

00:12:10,660 --> 00:12:16,240
the the blue is the previous results for

00:12:13,600 --> 00:12:18,430
the RDD api and notice that python was

00:12:16,240 --> 00:12:20,710
significantly slower than scala even

00:12:18,430 --> 00:12:22,750
though python and r are actually getting

00:12:20,710 --> 00:12:24,760
translated into byte code immediately

00:12:22,750 --> 00:12:27,040
that was enough overhead that it kind of

00:12:24,760 --> 00:12:28,780
sucked but you know not maybe not too

00:12:27,040 --> 00:12:30,520
bad if you were a data scientist new

00:12:28,780 --> 00:12:33,010
Python and didn't want to switch to

00:12:30,520 --> 00:12:33,910
Scala you know maybe it was okay but

00:12:33,010 --> 00:12:35,650
notice what happened when they

00:12:33,910 --> 00:12:37,750
introduced data frames suddenly the

00:12:35,650 --> 00:12:39,850
performance became equivalent there's

00:12:37,750 --> 00:12:42,700
still some small issues in performance

00:12:39,850 --> 00:12:44,890
but the reason is because they more

00:12:42,700 --> 00:12:46,570
aggressively get to bytecode and they're

00:12:44,890 --> 00:12:48,220
doing a bunch of optimizations we'll

00:12:46,570 --> 00:12:50,230
discuss so that's where they get like

00:12:48,220 --> 00:12:52,420
the doubling of performance right here

00:12:50,230 --> 00:12:56,470
but then also for Python it becomes

00:12:52,420 --> 00:12:58,480
equivalent to a Scala performance this

00:12:56,470 --> 00:13:00,610
was kind of a breakthrough really in Big

00:12:58,480 --> 00:13:04,720
Data if you go back to the bad old days

00:13:00,610 --> 00:13:07,210
of its predecessor MapReduce your data

00:13:04,720 --> 00:13:09,460
scientists might model stuff an R or

00:13:07,210 --> 00:13:11,590
Python or whatever and then they would

00:13:09,460 --> 00:13:13,690
toss their models over a partition to a

00:13:11,590 --> 00:13:15,730
bunch of Java developers and pray that

00:13:13,690 --> 00:13:17,080
they actually translated it to Java

00:13:15,730 --> 00:13:19,960
correctly and didn't make mistakes

00:13:17,080 --> 00:13:21,460
really horrible way to work but now in

00:13:19,960 --> 00:13:23,590
principle you could just deploy these

00:13:21,460 --> 00:13:24,490
Python models or whatever it is you're

00:13:23,590 --> 00:13:26,230
building and

00:13:24,490 --> 00:13:29,529
would just work and give you excellent

00:13:26,230 --> 00:13:31,089
performance the numbers are actually far

00:13:29,529 --> 00:13:32,770
better like the release of spark that's

00:13:31,089 --> 00:13:34,839
coming out this summer is dropping this

00:13:32,770 --> 00:13:38,140
by like a factor of 10 you know 10

00:13:34,839 --> 00:13:41,290
better and we'll see why as we get to it

00:13:38,140 --> 00:13:43,300
here in a few minutes this is just a

00:13:41,290 --> 00:13:45,610
laundry list of the functions on this

00:13:43,300 --> 00:13:47,399
data frame object that represents the

00:13:45,610 --> 00:13:49,810
sequel queries that you're writing and

00:13:47,399 --> 00:13:51,580
if you just scan a few of the yellow

00:13:49,810 --> 00:13:53,860
names you'll see things that should be

00:13:51,580 --> 00:13:55,690
familiar from sequel like group buy and

00:13:53,860 --> 00:13:57,790
select and all that because basically

00:13:55,690 --> 00:14:04,060
it's a an abstraction around you know

00:13:57,790 --> 00:14:05,770
the relational model now the the other

00:14:04,060 --> 00:14:07,899
thing that they introduced was a way of

00:14:05,770 --> 00:14:09,310
doing streaming and stream processing

00:14:07,899 --> 00:14:11,950
happens to be the other new hotness

00:14:09,310 --> 00:14:14,110
right now like if you've heard of Kafka

00:14:11,950 --> 00:14:17,320
it sits on a rocket ship the way spark

00:14:14,110 --> 00:14:19,899
was two years ago but it turns out spark

00:14:17,320 --> 00:14:21,550
started out as a batch mode system where

00:14:19,899 --> 00:14:23,350
I'd have all my data somewhere and I'd

00:14:21,550 --> 00:14:25,450
run this big job that would just stream

00:14:23,350 --> 00:14:27,339
you know go through it all streams the

00:14:25,450 --> 00:14:29,470
wrong word and you know do some

00:14:27,339 --> 00:14:31,600
processing and then finish but when

00:14:29,470 --> 00:14:33,399
streaming got popular they came up with

00:14:31,600 --> 00:14:35,260
a clever hack they said well you know

00:14:33,399 --> 00:14:37,120
actually this is a batch system but it's

00:14:35,260 --> 00:14:38,950
pretty efficient so what if we just

00:14:37,120 --> 00:14:40,930
captured data in you know time

00:14:38,950 --> 00:14:42,970
increments like down to a second or so

00:14:40,930 --> 00:14:44,740
and then ran a little batch job over

00:14:42,970 --> 00:14:46,540
each of those intervals and that's what

00:14:44,740 --> 00:14:49,149
this really is so d stream is

00:14:46,540 --> 00:14:50,529
discretized stream this is actually

00:14:49,149 --> 00:14:52,089
going away eventually because it turns

00:14:50,529 --> 00:14:54,670
out there are a lot of limitations that

00:14:52,089 --> 00:14:56,320
true streaming engines support that they

00:14:54,670 --> 00:14:59,140
can't but nevertheless it gives us

00:14:56,320 --> 00:15:01,420
ability to do like very sophisticated

00:14:59,140 --> 00:15:04,300
stuff where I don't need sub-second

00:15:01,420 --> 00:15:06,820
latency I can maybe take 10 seconds or a

00:15:04,300 --> 00:15:09,130
second to do something but I don't want

00:15:06,820 --> 00:15:12,940
to wait hours or whatever until the next

00:15:09,130 --> 00:15:14,709
big batch job kicks off and so basically

00:15:12,940 --> 00:15:16,750
what happens is for each of these time

00:15:14,709 --> 00:15:19,329
intervals you'll capture whatever events

00:15:16,750 --> 00:15:21,940
came in like over a socket or from Kafka

00:15:19,329 --> 00:15:24,250
or whatever and stuff those into an RDD

00:15:21,940 --> 00:15:26,529
and then do whatever job you want over

00:15:24,250 --> 00:15:28,060
the RTD if you've ever used the lambda

00:15:26,529 --> 00:15:31,000
architecture where you combine like a

00:15:28,060 --> 00:15:33,220
streaming engine and a batch engine and

00:15:31,000 --> 00:15:35,740
try to join the results together this

00:15:33,220 --> 00:15:37,660
solves that design problem pretty nicely

00:15:35,740 --> 00:15:39,640
because you can have one tool

00:15:37,660 --> 00:15:41,410
that you write your logic in and then

00:15:39,640 --> 00:15:43,330
whether you wrap it in a stream or wrap

00:15:41,410 --> 00:15:45,430
it in a batch job determines how it's

00:15:43,330 --> 00:15:47,830
executed but you're not implementing the

00:15:45,430 --> 00:15:51,450
same logic twice in different tools for

00:15:47,830 --> 00:15:53,560
those different kinds of compute models

00:15:51,450 --> 00:15:54,610
and then just for completeness there's

00:15:53,560 --> 00:15:56,500
two other higher-level

00:15:54,610 --> 00:16:00,880
libraries one is the machine learning

00:15:56,500 --> 00:16:02,440
library called ml Lib that is the best

00:16:00,880 --> 00:16:04,720
part about it really is it's got some

00:16:02,440 --> 00:16:07,840
low-level primitives like a good linear

00:16:04,720 --> 00:16:10,020
algebra library but it's it's not a very

00:16:07,840 --> 00:16:11,770
comprehensive set of higher level

00:16:10,020 --> 00:16:13,990
algorithms that you find a machine

00:16:11,770 --> 00:16:15,820
learning and I think they're evolving

00:16:13,990 --> 00:16:17,650
towards a model of just providing the

00:16:15,820 --> 00:16:19,810
primitives and then letting third

00:16:17,650 --> 00:16:22,980
parties provide the custom libraries

00:16:19,810 --> 00:16:25,300
like deep learning neural networks

00:16:22,980 --> 00:16:27,640
random forests stuff like that if you've

00:16:25,300 --> 00:16:29,590
heard of those things and also there's a

00:16:27,640 --> 00:16:31,540
graph library with the idea being that

00:16:29,590 --> 00:16:33,430
if I have dated it nicely fits a graph

00:16:31,540 --> 00:16:35,650
like here's an example of PageRank from

00:16:33,430 --> 00:16:37,840
Wikipedia you know your social network

00:16:35,650 --> 00:16:40,030
on LinkedIn or Twitter are also examples

00:16:37,840 --> 00:16:42,070
then you can represent that data as a

00:16:40,030 --> 00:16:44,620
graph and use graph algorithms you know

00:16:42,070 --> 00:16:46,000
like you know nearest neighbors called

00:16:44,620 --> 00:16:48,940
connected components and that kind of

00:16:46,000 --> 00:16:50,200
stuff to compute it but that's all I'm

00:16:48,940 --> 00:16:51,880
gonna say about those this is my

00:16:50,200 --> 00:16:54,400
favorite photo actually from this whole

00:16:51,880 --> 00:16:56,410
thing this this is a green thing is a

00:16:54,400 --> 00:16:58,720
rope that's about this big it's one of

00:16:56,410 --> 00:16:59,280
those ropes you used to tie ships to

00:16:58,720 --> 00:17:01,480
docks

00:16:59,280 --> 00:17:05,380
unfortunately this beach was covered in

00:17:01,480 --> 00:17:07,210
detritus and it was mostly debris from

00:17:05,380 --> 00:17:09,810
Japanese fishing vessels after the

00:17:07,210 --> 00:17:13,030
tsunami you know that was what in 2011

00:17:09,810 --> 00:17:15,550
but anyway so there's this big rope and

00:17:13,030 --> 00:17:17,500
then one of my hiking partners there in

00:17:15,550 --> 00:17:18,910
the in the middle so but anyway now

00:17:17,500 --> 00:17:21,130
we've got this sort of background about

00:17:18,910 --> 00:17:23,560
what SPARC is all about let's go down

00:17:21,130 --> 00:17:26,410
and see you know what does it mean in

00:17:23,560 --> 00:17:28,180
terms of using the JVM for all this well

00:17:26,410 --> 00:17:30,160
of course the JVM was picked for all of

00:17:28,180 --> 00:17:32,290
these tools because you know we just had

00:17:30,160 --> 00:17:34,390
a lot of experience running very big JVM

00:17:32,290 --> 00:17:36,610
was running lots of them and the DevOps

00:17:34,390 --> 00:17:37,870
problem was sort of solved mostly we

00:17:36,610 --> 00:17:39,940
just knew how to run this stuff so it

00:17:37,870 --> 00:17:42,280
was a good platform and we could get a

00:17:39,940 --> 00:17:43,980
lot of job of developers - if that's the

00:17:42,280 --> 00:17:46,870
kind of people we wanted to hire for it

00:17:43,980 --> 00:17:48,940
and we've got a ton of tools we now have

00:17:46,870 --> 00:17:50,830
several languages not just Scala but

00:17:48,940 --> 00:17:52,779
closure which is mostly invisible

00:17:50,830 --> 00:17:55,269
unfortunately in this space it kind of

00:17:52,779 --> 00:17:57,850
like to see more of it and then a lot of

00:17:55,269 --> 00:17:58,450
libraries and here's some of the my my

00:17:57,850 --> 00:18:00,850
favorite

00:17:58,450 --> 00:18:02,890
Scala based libraries acha of course

00:18:00,850 --> 00:18:05,590
that you probably know about it breeze

00:18:02,890 --> 00:18:06,820
is a linear algebra library and so forth

00:18:05,590 --> 00:18:09,190
I won't go through all of them and

00:18:06,820 --> 00:18:11,500
developer tools like eclipse intelligent

00:18:09,190 --> 00:18:15,279
and because of this we've had this big

00:18:11,500 --> 00:18:17,260
ecosystem emerge on the JVM including

00:18:15,279 --> 00:18:19,600
some Scala based libraries the ones at

00:18:17,260 --> 00:18:21,100
the top and Kafka and you know Hadoop

00:18:19,600 --> 00:18:23,919
which is you know sort of the gold

00:18:21,100 --> 00:18:25,029
standard in a way for big data databases

00:18:23,919 --> 00:18:27,309
I just you know I could have put a

00:18:25,029 --> 00:18:31,299
easily double or triple the number of

00:18:27,309 --> 00:18:32,710
icons on this slide however there are

00:18:31,299 --> 00:18:34,299
some imperfections there's some things

00:18:32,710 --> 00:18:39,100
that have caused problems for us and I

00:18:34,299 --> 00:18:41,500
want to talk about that next well one of

00:18:39,100 --> 00:18:43,210
the practical issues is that if you're

00:18:41,500 --> 00:18:45,429
doing data science there's just a lot

00:18:43,210 --> 00:18:47,289
more libraries available in the Python

00:18:45,429 --> 00:18:50,559
at our communities for us you know

00:18:47,289 --> 00:18:53,019
complex statistics machine learning

00:18:50,559 --> 00:18:55,090
models and so forth we're catching up in

00:18:53,019 --> 00:18:56,710
Scala but it's probably going to be you

00:18:55,090 --> 00:19:00,279
know a long time before there's any sort

00:18:56,710 --> 00:19:05,169
of parody a garbage collection turns out

00:19:00,279 --> 00:19:07,179
to be a major problem in these big data

00:19:05,169 --> 00:19:09,850
computations and let's actually explore

00:19:07,179 --> 00:19:11,320
that a little bit well it turns out that

00:19:09,850 --> 00:19:13,000
most of the time when you're doing big

00:19:11,320 --> 00:19:14,950
data stuff you're typically running

00:19:13,000 --> 00:19:16,899
heaps that are ten to hundreds of

00:19:14,950 --> 00:19:19,299
gigabytes in size and some of you may

00:19:16,899 --> 00:19:23,260
know you can now get two terabytes of

00:19:19,299 --> 00:19:25,090
RAM on single instances on Amazon so

00:19:23,260 --> 00:19:26,860
we're and so that's just gonna push

00:19:25,090 --> 00:19:28,899
these numbers higher because when you

00:19:26,860 --> 00:19:30,549
have you know terabytes to petabytes of

00:19:28,899 --> 00:19:32,590
data you know you have very little

00:19:30,549 --> 00:19:36,370
memory taken by your code and most of it

00:19:32,590 --> 00:19:39,070
is just storing data but this is not a

00:19:36,370 --> 00:19:41,409
very common size for heaps for the kind

00:19:39,070 --> 00:19:43,149
of Micra services or whatever buzzword

00:19:41,409 --> 00:19:44,649
you want to use the sort of things we've

00:19:43,149 --> 00:19:46,059
been writing for years outside of the

00:19:44,649 --> 00:19:47,980
big data world so we're it's kind of new

00:19:46,059 --> 00:19:50,230
territory and we're pushing the limits

00:19:47,980 --> 00:19:53,559
of what's possible and it turns out that

00:19:50,230 --> 00:19:55,690
typically the way rdd's and data frames

00:19:53,559 --> 00:19:57,610
are used especially if you start caching

00:19:55,690 --> 00:20:00,100
them is that you skew the kind of

00:19:57,610 --> 00:20:02,200
traditional picture of how memory is

00:20:00,100 --> 00:20:04,299
going to be laid out and you can lead to

00:20:02,200 --> 00:20:06,729
extremely long garbage collection pauses

00:20:04,299 --> 00:20:10,269
there's a Hadoop distributed file system

00:20:06,729 --> 00:20:12,159
that's JVM based and the longest garbage

00:20:10,269 --> 00:20:15,549
collection pause I've ever heard of was

00:20:12,159 --> 00:20:17,349
ten hours when a Hadoop file system I

00:20:15,549 --> 00:20:19,989
think was at Facebook basically went

00:20:17,349 --> 00:20:21,789
nuts and 10/10 spent ten hours trying to

00:20:19,989 --> 00:20:25,269
collect garbage so this is really bad

00:20:21,789 --> 00:20:27,249
obviously if you're really interested in

00:20:25,269 --> 00:20:28,929
what's possible to tune spark there's a

00:20:27,249 --> 00:20:31,869
great blog post on the data bricks

00:20:28,929 --> 00:20:33,580
website and I just included a few of the

00:20:31,869 --> 00:20:36,399
options it was actually written by some

00:20:33,580 --> 00:20:38,440
guys from Intel that discussed how what

00:20:36,399 --> 00:20:40,059
they recommended for tuning I have to

00:20:38,440 --> 00:20:41,859
say for those who taking photos of the

00:20:40,059 --> 00:20:43,539
slide this is actually going to be

00:20:41,859 --> 00:20:45,579
obsolete because of the re-engineering

00:20:43,539 --> 00:20:47,769
they're doing inside spark but

00:20:45,579 --> 00:20:49,149
nevertheless it's it's a great article

00:20:47,769 --> 00:20:52,119
for understanding like how to do

00:20:49,149 --> 00:20:55,059
profiling of the garbage collector if

00:20:52,119 --> 00:20:56,649
you really want to read it the other

00:20:55,059 --> 00:20:58,479
problem though it's sort of related is

00:20:56,649 --> 00:21:01,899
really the way that Java object model

00:20:58,479 --> 00:21:03,969
works the the model is extremely

00:21:01,899 --> 00:21:06,129
flexible and it's worked really well for

00:21:03,969 --> 00:21:08,200
the general case of building stuff but

00:21:06,129 --> 00:21:10,119
it just kind of breaks down when we get

00:21:08,200 --> 00:21:12,429
to Big Data where I have like say

00:21:10,119 --> 00:21:15,249
billions of the same thing over and over

00:21:12,429 --> 00:21:17,079
and over again you know just records and

00:21:15,249 --> 00:21:19,989
it's kind of beneficent to so you might

00:21:17,079 --> 00:21:21,070
see like this and this is an example I

00:21:19,989 --> 00:21:24,339
took from a data bricks

00:21:21,070 --> 00:21:26,109
talk about this subject know ABCD you

00:21:24,339 --> 00:21:28,629
know the minimum storage if I just

00:21:26,109 --> 00:21:31,959
assume utf-8 is gonna be 4 bytes right

00:21:28,629 --> 00:21:34,690
but in fact it's 48 bytes in the Java

00:21:31,959 --> 00:21:37,450
object model and that's includes a 12 by

00:21:34,690 --> 00:21:38,950
Tedder that every object carries 8 bytes

00:21:37,450 --> 00:21:40,719
for the hash code because they always

00:21:38,950 --> 00:21:42,729
compute the hash code for it for an

00:21:40,719 --> 00:21:44,950
object so that it's ready in case you

00:21:42,729 --> 00:21:47,950
need it which you may never need it if

00:21:44,950 --> 00:21:50,070
you have trillions of these things 20

00:21:47,950 --> 00:21:52,629
bytes have overhead for the array itself

00:21:50,070 --> 00:21:54,489
that's actually holding this data and

00:21:52,629 --> 00:21:57,879
then eight bytes for each of the

00:21:54,489 --> 00:22:00,639
characters which are actually utf-16 so

00:21:57,879 --> 00:22:02,950
not terribly efficient again not so much

00:22:00,639 --> 00:22:04,929
of a big deal if I have maybe thousands

00:22:02,950 --> 00:22:07,149
of strings but if I'm up in the billions

00:22:04,929 --> 00:22:08,369
of strings then it really makes makes a

00:22:07,149 --> 00:22:10,979
big difference

00:22:08,369 --> 00:22:14,169
the other problem though is the way

00:22:10,979 --> 00:22:16,269
memory is laid out again this is great

00:22:14,169 --> 00:22:18,789
for flexibility in the in the

00:22:16,269 --> 00:22:21,039
a general case but really bad for the

00:22:18,789 --> 00:22:23,049
specific case of records where I'd like

00:22:21,039 --> 00:22:25,779
things contiguous each of these lines

00:22:23,049 --> 00:22:27,940
here is a cache miss so if I'm trying to

00:22:25,779 --> 00:22:30,399
read this data into my CPU to do some

00:22:27,940 --> 00:22:33,789
sort of whatever over it I'm gonna

00:22:30,399 --> 00:22:36,219
probably miss you not find like third

00:22:33,789 --> 00:22:38,440
and zeroth and so forth in the cache and

00:22:36,219 --> 00:22:40,149
I'll have to you know go out to memory

00:22:38,440 --> 00:22:42,580
again so it's terribly inefficient to

00:22:40,149 --> 00:22:47,309
have data spread out like this again you

00:22:42,580 --> 00:22:49,570
know over millions of occurrences and

00:22:47,309 --> 00:22:51,789
here's an example for just an arbitrary

00:22:49,570 --> 00:22:54,489
object that you know like a person type

00:22:51,789 --> 00:22:56,200
that where maybe I can you know in line

00:22:54,489 --> 00:22:57,700
these the age of the person because

00:22:56,200 --> 00:22:59,049
that's a primitive but I've got yet

00:22:57,700 --> 00:23:01,570
another reference off on the heap

00:22:59,049 --> 00:23:03,070
somewhere for the name the address is

00:23:01,570 --> 00:23:05,950
yet another object with its own

00:23:03,070 --> 00:23:08,019
references and so forth again nice for

00:23:05,950 --> 00:23:11,019
flexibility but really bad when I've got

00:23:08,019 --> 00:23:12,459
billions of them and even hash maps are

00:23:11,019 --> 00:23:14,529
a problem even though there are a very

00:23:12,459 --> 00:23:16,389
useful data structure that each hash

00:23:14,529 --> 00:23:19,950
bucket is once again a linked list of

00:23:16,389 --> 00:23:19,950
stuff so I've got just lots of lines

00:23:21,029 --> 00:23:24,279
okay and we're going to come back to the

00:23:22,929 --> 00:23:26,799
solution in a minute I'm just setting up

00:23:24,279 --> 00:23:28,509
some of the problems one of the things

00:23:26,799 --> 00:23:32,049
they discovered through experimentation

00:23:28,509 --> 00:23:33,579
was that spark jobs tend to be CPU bound

00:23:32,049 --> 00:23:36,219
and this is actually also something

00:23:33,579 --> 00:23:39,729
that's new the the previous MapReduce

00:23:36,219 --> 00:23:41,950
API tended to be i/o bound but for

00:23:39,729 --> 00:23:45,039
you're doing various experimental and

00:23:41,950 --> 00:23:46,799
analytical studies of this some people

00:23:45,039 --> 00:23:49,089
at Berkeley University of Berkeley

00:23:46,799 --> 00:23:51,399
discovered that you know in general

00:23:49,089 --> 00:23:52,769
improving network i/o you can only

00:23:51,399 --> 00:23:55,269
expect a few percentage points

00:23:52,769 --> 00:23:57,190
improvement disk i/o which is you know

00:23:55,269 --> 00:23:58,959
also a well-known bottleneck actually

00:23:57,190 --> 00:24:00,579
wasn't that significant in other words

00:23:58,959 --> 00:24:01,899
fixing those problems is not going to

00:24:00,579 --> 00:24:04,539
give you orders of magnitude improvement

00:24:01,899 --> 00:24:06,729
right so you know obviously you need to

00:24:04,539 --> 00:24:08,589
know what problem you should optimize to

00:24:06,729 --> 00:24:10,599
fix if you really want to get dramatic

00:24:08,589 --> 00:24:12,070
improvements so it means we need to do

00:24:10,599 --> 00:24:14,729
something about what's happening in the

00:24:12,070 --> 00:24:17,649
CPU when we're actually processing stuff

00:24:14,729 --> 00:24:19,239
well it's worth maybe taking a step to

00:24:17,649 --> 00:24:21,039
the side a little bit and asking all

00:24:19,239 --> 00:24:23,919
right well why is this flipped why have

00:24:21,039 --> 00:24:25,450
we gone from i/o bound to CPU bound and

00:24:23,919 --> 00:24:28,450
a lot of it reflects the the evolution

00:24:25,450 --> 00:24:28,840
of hardware since you know 2000 or so

00:24:28,450 --> 00:24:31,720
when

00:24:28,840 --> 00:24:33,640
produce ruled the land we have extremely

00:24:31,720 --> 00:24:35,770
fast networks now and the networks are

00:24:33,640 --> 00:24:37,960
actually faster than old spinning hard

00:24:35,770 --> 00:24:39,640
drives in a lot of cases as far as

00:24:37,960 --> 00:24:41,260
throughput if you don't have a lot of

00:24:39,640 --> 00:24:42,970
contention or whatever depending how you

00:24:41,260 --> 00:24:45,010
structure the network and of course

00:24:42,970 --> 00:24:49,360
solid-state drives are noticeably faster

00:24:45,010 --> 00:24:51,220
too so we've reduced i/o or we've

00:24:49,360 --> 00:24:53,409
improved i/o performance but we've also

00:24:51,220 --> 00:24:55,270
gotten smarter about how we use it so

00:24:53,409 --> 00:24:56,919
for example I mentioned that park'

00:24:55,270 --> 00:24:59,020
format one of the things that you can do

00:24:56,919 --> 00:25:00,880
with parquet is do predicate push down

00:24:59,020 --> 00:25:03,039
so if I'm doing a query where I only

00:25:00,880 --> 00:25:05,140
want to see you know a range of records

00:25:03,039 --> 00:25:07,630
I can actually have the parkade driver

00:25:05,140 --> 00:25:09,880
only return to me the stuff that fits

00:25:07,630 --> 00:25:11,500
within that range and not return

00:25:09,880 --> 00:25:14,620
everything that I'll then just turn

00:25:11,500 --> 00:25:16,480
around and throw away we so and we also

00:25:14,620 --> 00:25:18,190
cache stuff in memory more effectively

00:25:16,480 --> 00:25:19,720
so there's less round-tripping to hard

00:25:18,190 --> 00:25:22,270
drives that was a big problem in Map

00:25:19,720 --> 00:25:24,580
Reduce and I just I guess I just

00:25:22,270 --> 00:25:25,809
mentioned part K and how it well so one

00:25:24,580 --> 00:25:28,630
of the other ways park K improves

00:25:25,809 --> 00:25:30,789
performance is by storing by column if I

00:25:28,630 --> 00:25:33,010
have say hundreds of columns in a record

00:25:30,789 --> 00:25:34,960
but I'm only asking for five am I'm

00:25:33,010 --> 00:25:37,450
going to read the disk blocks for those

00:25:34,960 --> 00:25:39,159
five columns and not just read in all of

00:25:37,450 --> 00:25:41,289
the records and then throw away most of

00:25:39,159 --> 00:25:43,510
the data and similarly the columns are

00:25:41,289 --> 00:25:49,510
compressed by default so that'll also

00:25:43,510 --> 00:25:51,250
optimize i/o okay and we're actually all

00:25:49,510 --> 00:25:53,980
of this is forcing more computation on

00:25:51,250 --> 00:25:56,049
the CPU to to shift the balance like

00:25:53,980 --> 00:25:57,850
using serialization more often to get

00:25:56,049 --> 00:25:59,740
stuff around the cluster using

00:25:57,850 --> 00:26:01,809
compression more often and decompression

00:25:59,740 --> 00:26:04,120
of course and then doing a lot more

00:26:01,809 --> 00:26:06,159
stuff with joins and group bys as

00:26:04,120 --> 00:26:07,750
opposed to pre aggregating data so that

00:26:06,159 --> 00:26:11,169
we end up doing a lot more calculations

00:26:07,750 --> 00:26:13,720
of hash codes so we're gonna have to fix

00:26:11,169 --> 00:26:15,580
the CPU and that mostly means improving

00:26:13,720 --> 00:26:18,429
algorithms and improving the use of

00:26:15,580 --> 00:26:20,080
memory so obviously you can they always

00:26:18,429 --> 00:26:22,210
say you know pick the best algorithm and

00:26:20,080 --> 00:26:25,090
not this be too naive about it but it

00:26:22,210 --> 00:26:27,669
turns out in modern architectures memory

00:26:25,090 --> 00:26:30,970
utilization and especially optimizing

00:26:27,669 --> 00:26:33,159
cache performance is really crucial so

00:26:30,970 --> 00:26:35,049
that created this or led to the creation

00:26:33,159 --> 00:26:37,030
of this project called tungsten which

00:26:35,049 --> 00:26:38,640
was an initiative over several releases

00:26:37,030 --> 00:26:41,230
of spark to improve the performance

00:26:38,640 --> 00:26:42,100
again at the dataframe sequel level

00:26:41,230 --> 00:26:45,039
where it's um working

00:26:42,100 --> 00:26:46,870
strain problem as opposed to the RVT api

00:26:45,039 --> 00:26:49,270
which is now seeing more is kind of like

00:26:46,870 --> 00:26:56,980
a an assembly language for spark if you

00:26:49,270 --> 00:26:59,169
will so the goals were these I don't

00:26:56,980 --> 00:27:00,700
know why I disappeared anyway first we

00:26:59,169 --> 00:27:02,440
want to reduce the number of references

00:27:00,700 --> 00:27:04,240
so we want to get rid of as many of

00:27:02,440 --> 00:27:06,669
these lines as we can so we get better

00:27:04,240 --> 00:27:08,919
cache performance and we also want to

00:27:06,669 --> 00:27:10,390
improve how we layout stuff so that we

00:27:08,919 --> 00:27:12,549
get rid of some of that overhead we

00:27:10,390 --> 00:27:14,799
discussed that we don't really want like

00:27:12,549 --> 00:27:19,539
an array that takes 48 instead of 4

00:27:14,799 --> 00:27:21,760
bytes and if we do that if we combine

00:27:19,539 --> 00:27:23,559
some of these objects in some way into

00:27:21,760 --> 00:27:26,559
bigger chunks and that means we have

00:27:23,559 --> 00:27:28,179
fewer objects to manage by the garbage

00:27:26,559 --> 00:27:29,740
collector and they're just bigger so

00:27:28,179 --> 00:27:32,230
you're deleting one object will

00:27:29,740 --> 00:27:34,000
accomplish a lot more and once again if

00:27:32,230 --> 00:27:35,770
everything is contiguous then we'll

00:27:34,000 --> 00:27:38,350
probably have it or at least be more

00:27:35,770 --> 00:27:40,299
likely to have it in the cache line that

00:27:38,350 --> 00:27:43,360
the CPU is already loaded when we're

00:27:40,299 --> 00:27:45,490
trying to process it and the other thing

00:27:43,360 --> 00:27:47,740
we want to do is eliminate the overhead

00:27:45,490 --> 00:27:49,179
of evaluated expression so for example I

00:27:47,740 --> 00:27:51,220
write this sequel query where I'm gonna

00:27:49,179 --> 00:27:52,990
in this contrived example I'm going to

00:27:51,220 --> 00:27:56,260
add two let's say integer fields

00:27:52,990 --> 00:27:58,419
together you know the naive way of doing

00:27:56,260 --> 00:28:00,610
this would be parse this sequel query

00:27:58,419 --> 00:28:02,289
and then have some built-in operators

00:28:00,610 --> 00:28:04,840
that are maybe you know like using a

00:28:02,289 --> 00:28:06,940
class hierarchy with virtual functions

00:28:04,840 --> 00:28:08,770
and so forth to actually evaluate the

00:28:06,940 --> 00:28:10,960
expression but once again do this

00:28:08,770 --> 00:28:12,460
billions of times and that all this sort

00:28:10,960 --> 00:28:15,340
of overhead that I've listed here

00:28:12,460 --> 00:28:17,530
becomes a noticeable penalty so can we

00:28:15,340 --> 00:28:19,120
actually eliminate this kind of stuff

00:28:17,530 --> 00:28:22,179
and the secret will be to actually

00:28:19,120 --> 00:28:23,320
generate custom byte code so let's look

00:28:22,179 --> 00:28:27,010
at what they actually did

00:28:23,320 --> 00:28:28,690
and it's mostly finished now the last

00:28:27,010 --> 00:28:29,289
steps that I know of we're coming out

00:28:28,690 --> 00:28:31,419
this summer

00:28:29,289 --> 00:28:33,700
one is they introduced a new encoding

00:28:31,419 --> 00:28:35,770
for objects this is kind of cool

00:28:33,700 --> 00:28:37,960
actually the type is called compact row

00:28:35,770 --> 00:28:39,100
if you look in the spark source code but

00:28:37,960 --> 00:28:41,409
instead of having all of these

00:28:39,100 --> 00:28:43,240
references around and keeping things

00:28:41,409 --> 00:28:45,850
like hash codes and stuff that we don't

00:28:43,240 --> 00:28:47,710
really need most of the time it starts

00:28:45,850 --> 00:28:49,570
off with a bit field that represents

00:28:47,710 --> 00:28:52,179
whether the value is null and if the

00:28:49,570 --> 00:28:53,740
value happens to be null and an implicit

00:28:52,179 --> 00:28:54,220
here is that I know what the schema is

00:28:53,740 --> 00:28:56,500
of the

00:28:54,220 --> 00:28:58,330
data if the values no then there won't

00:28:56,500 --> 00:29:01,240
be anything about it and the rest of the

00:28:58,330 --> 00:29:03,490
object but if it's not null then I've

00:29:01,240 --> 00:29:05,280
got these 8 bytes per values you know

00:29:03,490 --> 00:29:08,230
organized by how the fields are laid out

00:29:05,280 --> 00:29:10,059
if it's something like a long or I can

00:29:08,230 --> 00:29:12,970
inline it then it's just the values in

00:29:10,059 --> 00:29:15,070
lined in this values section but if it's

00:29:12,970 --> 00:29:16,929
not if it's say a string or something

00:29:15,070 --> 00:29:19,090
then it's that eight bytes will be a

00:29:16,929 --> 00:29:21,010
pointer into the variable length section

00:29:19,090 --> 00:29:22,630
of the array at the end so again this is

00:29:21,010 --> 00:29:25,030
like a byte array and they're just

00:29:22,630 --> 00:29:27,250
putting this stuff in line when they can

00:29:25,030 --> 00:29:30,039
it's all gonna hopefully fit in a cache

00:29:27,250 --> 00:29:31,390
line or at least most of it will and the

00:29:30,039 --> 00:29:33,520
other cool thing about computing

00:29:31,390 --> 00:29:35,080
hashcode and equals is I can just rip

00:29:33,520 --> 00:29:37,539
through the bytes I don't have to do any

00:29:35,080 --> 00:29:39,789
fancy computation for those I just

00:29:37,539 --> 00:29:42,340
ripped through the bytes and I can tell

00:29:39,789 --> 00:29:45,070
you know maybe by the fifth bit that

00:29:42,340 --> 00:29:46,929
these are not equal and the hash code is

00:29:45,070 --> 00:29:50,860
very fast to calculate too so those

00:29:46,929 --> 00:29:52,600
things are also sped up so if you

00:29:50,860 --> 00:29:54,100
compare what we had before was something

00:29:52,600 --> 00:29:56,169
we have now then we're gonna have much

00:29:54,100 --> 00:29:58,630
better cache performance actually much

00:29:56,169 --> 00:30:00,610
more efficient storage and not carrying

00:29:58,630 --> 00:30:04,390
a lot of stuff like you know a hash code

00:30:00,610 --> 00:30:07,150
value that we don't really need they

00:30:04,390 --> 00:30:10,570
also introduced a custom implementation

00:30:07,150 --> 00:30:12,400
for maps that basically use but

00:30:10,570 --> 00:30:14,320
basically hash maps where they're just

00:30:12,400 --> 00:30:16,510
gonna have this one single page of

00:30:14,320 --> 00:30:19,270
memory where they basically line up the

00:30:16,510 --> 00:30:20,980
key value pairs sequentially so that

00:30:19,270 --> 00:30:22,539
once again I have one big object I'm at

00:30:20,980 --> 00:30:25,840
managing not a bunch of little linked

00:30:22,539 --> 00:30:28,390
lists kind of things and that gives us

00:30:25,840 --> 00:30:31,289
you know this better in memory layout

00:30:28,390 --> 00:30:33,549
cache performance the whole whole bit

00:30:31,289 --> 00:30:36,130
and then finally they're doing they're

00:30:33,549 --> 00:30:37,990
using Sun Miss unsafe the infamous API

00:30:36,130 --> 00:30:40,419
that lets basically gives you malloc and

00:30:37,990 --> 00:30:41,710
free we've ever done C programming so

00:30:40,419 --> 00:30:44,530
that we can do some of these allocations

00:30:41,710 --> 00:30:46,179
off heap when when that seems to be the

00:30:44,530 --> 00:30:48,190
smart thing to do and they don't always

00:30:46,179 --> 00:30:50,020
do this but there are cases where it's

00:30:48,190 --> 00:30:52,419
actually better to just reserve the heap

00:30:50,020 --> 00:30:54,460
for more general purpose things and just

00:30:52,419 --> 00:30:59,350
have big pages of memory that are

00:30:54,460 --> 00:31:01,360
managed themselves the last one is this

00:30:59,350 --> 00:31:03,750
expression overhead so what they're what

00:31:01,360 --> 00:31:06,220
they've done up to this point is they've

00:31:03,750 --> 00:31:08,620
generate custom byte code for sub

00:31:06,220 --> 00:31:10,960
expressions and queries like so

00:31:08,620 --> 00:31:12,220
and in 2.0 which is coming out this

00:31:10,960 --> 00:31:13,750
summer they're actually gonna generate

00:31:12,220 --> 00:31:15,549
bytecode for the whole query and it

00:31:13,750 --> 00:31:17,049
turns out they're seen like an order of

00:31:15,549 --> 00:31:20,650
magnitude improvement in performance

00:31:17,049 --> 00:31:22,210
over big queries by generating bytecode

00:31:20,650 --> 00:31:25,210
for the whole thing so it's kind of

00:31:22,210 --> 00:31:27,700
dramatic and then you this is a great

00:31:25,210 --> 00:31:30,039
example to me maybe in the extreme of an

00:31:27,700 --> 00:31:31,570
abstraction versus the implementation if

00:31:30,039 --> 00:31:33,610
you have a narrow well-defined

00:31:31,570 --> 00:31:35,320
abstraction that hopefully if into leaky

00:31:33,610 --> 00:31:37,659
then it gives you a lot of room to do

00:31:35,320 --> 00:31:41,649
optimizations under the hood including

00:31:37,659 --> 00:31:43,240
completely bypassing Scala Java the JVM

00:31:41,649 --> 00:31:46,299
and doing something really extremely

00:31:43,240 --> 00:31:48,850
customized okay

00:31:46,299 --> 00:31:51,640
so a few other things that are kind of

00:31:48,850 --> 00:31:53,860
annoying missing features in Java one is

00:31:51,640 --> 00:31:55,779
that we don't have value types although

00:31:53,860 --> 00:31:57,490
that is possibly gonna come in a future

00:31:55,779 --> 00:31:59,080
release of Java now this is something we

00:31:57,490 --> 00:32:00,549
already know from Scala because we have

00:31:59,080 --> 00:32:03,010
a limited form of this now

00:32:00,549 --> 00:32:05,230
where if and I came up with an example

00:32:03,010 --> 00:32:06,909
like suppose I have timestamp objects

00:32:05,230 --> 00:32:10,720
and I want nice methods for like doing

00:32:06,909 --> 00:32:12,610
arithmetic on timestamps but all I

00:32:10,720 --> 00:32:14,320
really have as far as state is like a

00:32:12,610 --> 00:32:16,690
single field that happens to be a

00:32:14,320 --> 00:32:19,179
primitive well it's kind of stupid to

00:32:16,690 --> 00:32:21,190
encode a class that I heap allocate and

00:32:19,179 --> 00:32:23,169
all this stuff when in the bytecode all

00:32:21,190 --> 00:32:25,090
I really need to do is keep that you

00:32:23,169 --> 00:32:28,450
know single primitive that I could push

00:32:25,090 --> 00:32:30,580
on the stack and not heap allocate so

00:32:28,450 --> 00:32:34,330
we've got this in Scala like I say if

00:32:30,580 --> 00:32:36,309
you have case class extend any vowel

00:32:34,330 --> 00:32:37,480
then you'll get that optimization but

00:32:36,309 --> 00:32:41,460
we'd really like this to happen

00:32:37,480 --> 00:32:41,460
automatically at the JVM level

00:32:41,620 --> 00:32:45,070
another thing that maybe a lot of you

00:32:43,450 --> 00:32:47,529
don't know is that long operations

00:32:45,070 --> 00:32:49,600
aren't actually atomic in the JVM spec

00:32:47,529 --> 00:32:51,940
so it's possible you could get you know

00:32:49,600 --> 00:32:54,220
basically synchronization errors from

00:32:51,940 --> 00:32:57,250
doing it long arithmetic so you have to

00:32:54,220 --> 00:32:59,200
be careful about this rather annoying

00:32:57,250 --> 00:33:00,970
thing for data is we don't have unsigned

00:32:59,200 --> 00:33:02,890
types so there's no way in there in are

00:33:00,970 --> 00:33:05,409
great type system to enforce that this

00:33:02,890 --> 00:33:06,520
is illegal and we have to just do a

00:33:05,409 --> 00:33:08,740
check that we're not trying to do

00:33:06,520 --> 00:33:11,590
factorial of minus one and that has

00:33:08,740 --> 00:33:14,110
another implication which is that arrays

00:33:11,590 --> 00:33:15,820
are indexed by integers which means and

00:33:14,110 --> 00:33:17,649
because we don't have unsigned integers

00:33:15,820 --> 00:33:19,960
we can only have two billion elements in

00:33:17,649 --> 00:33:21,770
an array and if it's a byte array like

00:33:19,960 --> 00:33:23,360
we're using for serializing objects

00:33:21,770 --> 00:33:26,060
to send over the network then we're

00:33:23,360 --> 00:33:27,140
limited to two gigabytes once again in a

00:33:26,060 --> 00:33:29,570
world where we're getting to the

00:33:27,140 --> 00:33:32,120
terabyte size ram and and the

00:33:29,570 --> 00:33:33,770
corresponding eaves this actually turns

00:33:32,120 --> 00:33:36,200
out to be a huge problem and I want to

00:33:33,770 --> 00:33:38,840
show you an example that kind of

00:33:36,200 --> 00:33:41,630
illustrates what's going on here so this

00:33:38,840 --> 00:33:43,370
is a real Sparks shell session and when

00:33:41,630 --> 00:33:44,840
you start the spark repple if you're not

00:33:43,370 --> 00:33:47,270
using a notebook it really is just the

00:33:44,840 --> 00:33:49,700
Scala ripple if you're using Scala

00:33:47,270 --> 00:33:51,650
versus Python let's say so I'm going to

00:33:49,700 --> 00:33:54,170
declare a number that represents 1.1

00:33:51,650 --> 00:33:57,590
billion and then I'm going to fill up an

00:33:54,170 --> 00:33:59,960
array with that many elements this will

00:33:57,590 --> 00:34:01,490
be an array of shorts there's this nice

00:33:59,960 --> 00:34:05,300
little actually do I have a build here

00:34:01,490 --> 00:34:07,520
let's see ok I guess I don't there's a

00:34:05,300 --> 00:34:09,350
nice little utility in SPARC where I can

00:34:07,520 --> 00:34:11,149
estimate the size of the array so I'm

00:34:09,350 --> 00:34:13,490
using that to figure out what I would

00:34:11,149 --> 00:34:18,620
expect that it's about 2.2 gigabytes in

00:34:13,490 --> 00:34:20,600
size now if you have say reference data

00:34:18,620 --> 00:34:22,550
that you want all of your partitions to

00:34:20,600 --> 00:34:24,740
be able to see like I'm gonna do some

00:34:22,550 --> 00:34:26,570
sort of look up against that reference

00:34:24,740 --> 00:34:28,790
data but I don't want to treat it as an

00:34:26,570 --> 00:34:30,770
RDD itself there's this notion of a

00:34:28,790 --> 00:34:32,659
broadcast variable where I could pull

00:34:30,770 --> 00:34:35,149
that in memory and then broadcast it

00:34:32,659 --> 00:34:36,950
around the cluster and then use it as in

00:34:35,149 --> 00:34:38,659
this example at the bottom that'll get

00:34:36,950 --> 00:34:39,710
to here in a second so that's what I'm

00:34:38,659 --> 00:34:42,620
gonna do here I'm going to take that

00:34:39,710 --> 00:34:45,500
array I just declared and broadcast it

00:34:42,620 --> 00:34:47,899
around my cluster now I put in this

00:34:45,500 --> 00:34:50,060
estimate of bead to emphasize the point

00:34:47,899 --> 00:34:52,820
that be this broadcast variable is a

00:34:50,060 --> 00:34:56,690
reference to something huge but B itself

00:34:52,820 --> 00:34:59,480
is tiny ok so there's some what's

00:34:56,690 --> 00:35:02,870
happening here now in this final line is

00:34:59,480 --> 00:35:06,080
I'm going to take integers from 0 up to

00:35:02,870 --> 00:35:08,240
but not including 100,000 parallelized

00:35:06,080 --> 00:35:10,370
turns them into an RDD although that's

00:35:08,240 --> 00:35:12,080
kind of it's almost a trivial point and

00:35:10,370 --> 00:35:14,270
then I'm gonna do something silly really

00:35:12,080 --> 00:35:17,240
or trivial anyway which so I'm gonna map

00:35:14,270 --> 00:35:19,970
over all those indices and get the value

00:35:17,240 --> 00:35:23,050
of my broadcast variable and then you

00:35:19,970 --> 00:35:26,000
know get the I fell iment out of it and

00:35:23,050 --> 00:35:28,100
for what it's worth it turns out that

00:35:26,000 --> 00:35:29,930
this will be a giant array of zeros if

00:35:28,100 --> 00:35:31,340
you recall I created the array and just

00:35:29,930 --> 00:35:33,830
put zeros in there so it's extremely

00:35:31,340 --> 00:35:35,920
trivial example but it illustrates what

00:35:33,830 --> 00:35:37,720
goes wrong a couple thing

00:35:35,920 --> 00:35:40,990
to know if you don't know this about

00:35:37,720 --> 00:35:43,000
spark but in my repple here it's it's

00:35:40,990 --> 00:35:45,580
it's actually running the parallelized

00:35:43,000 --> 00:35:48,310
method and it's calling the map method

00:35:45,580 --> 00:35:50,590
but that's returning this sort of

00:35:48,310 --> 00:35:53,020
abstract object that represents that

00:35:50,590 --> 00:35:54,790
node in the data flow but nothing's

00:35:53,020 --> 00:35:59,290
actually running on clot on the cluster

00:35:54,790 --> 00:36:01,810
yet until I say go but this anonymous

00:35:59,290 --> 00:36:03,820
function has to be serializable because

00:36:01,810 --> 00:36:07,000
that I'm going to send over the cluster

00:36:03,820 --> 00:36:09,040
when I actually run this map step so

00:36:07,000 --> 00:36:11,410
this has to be a serializable object and

00:36:09,040 --> 00:36:14,140
this trips up spark users all the time

00:36:11,410 --> 00:36:17,500
they'll an inappropriately capture

00:36:14,140 --> 00:36:19,150
something that isn't serializable well

00:36:17,500 --> 00:36:21,370
what do you think should happen here

00:36:19,150 --> 00:36:23,500
well you know B is small so and it's

00:36:21,370 --> 00:36:25,660
just a little reference thing so it

00:36:23,500 --> 00:36:28,030
ought to be okay and it's not big in any

00:36:25,660 --> 00:36:31,990
way but when I actually run this this is

00:36:28,030 --> 00:36:34,080
what happens it goes boom and it throws

00:36:31,990 --> 00:36:36,880
an exception out of memory error

00:36:34,080 --> 00:36:38,530
requested array size exceeds VM limit

00:36:36,880 --> 00:36:42,000
that basically means I tried to allocate

00:36:38,530 --> 00:36:45,790
an array bigger than two gigabytes but

00:36:42,000 --> 00:36:48,790
it begs the question where's the array

00:36:45,790 --> 00:36:50,710
that will actually I lied to you it's

00:36:48,790 --> 00:36:52,780
not actually that byte array that's the

00:36:50,710 --> 00:36:56,470
problem it's actually the repple itself

00:36:52,780 --> 00:36:58,450
it's the problem because it turns out

00:36:56,470 --> 00:37:01,030
SPARC actually does handle very big

00:36:58,450 --> 00:37:03,550
broadcast array so I have some customers

00:37:01,030 --> 00:37:05,650
in New York that routinely load like 40

00:37:03,550 --> 00:37:08,140
gigabytes of data into the repple and

00:37:05,650 --> 00:37:10,300
then broadcast it around the cluster and

00:37:08,140 --> 00:37:12,070
that actually works fine because SPARC

00:37:10,300 --> 00:37:13,690
internally will break it up into small

00:37:12,070 --> 00:37:15,640
arrays and then reassemble on the other

00:37:13,690 --> 00:37:18,250
side it's annoying they have to do that

00:37:15,640 --> 00:37:21,820
but they do handle that it's actually a

00:37:18,250 --> 00:37:23,380
problem the Scala repple so if we look

00:37:21,820 --> 00:37:26,680
at the stack trace and I'd pull that a

00:37:23,380 --> 00:37:29,110
few of the interesting bits here it was

00:37:26,680 --> 00:37:30,760
thrown when I called that map method and

00:37:29,110 --> 00:37:32,800
I hadn't actually you know I said that

00:37:30,760 --> 00:37:34,390
it's a lazy thing right so this happened

00:37:32,800 --> 00:37:38,970
immediately when I called the map method

00:37:34,390 --> 00:37:38,970
not when I tried to actually run the job

00:37:39,510 --> 00:37:44,380
so there's this method under here that

00:37:42,310 --> 00:37:46,240
tries to tell if things are serializable

00:37:44,380 --> 00:37:48,100
and what it does is it actually C

00:37:46,240 --> 00:37:50,890
realises the object and then catches

00:37:48,100 --> 00:37:53,830
exception if it fails but if we drill

00:37:50,890 --> 00:37:55,360
into what's actually happening yes it is

00:37:53,830 --> 00:37:57,550
actually failing as it tries to

00:37:55,360 --> 00:38:00,460
serialize the byte array but the

00:37:57,550 --> 00:38:02,110
question is you know why is it copying

00:38:00,460 --> 00:38:03,760
an array I said there's there's no array

00:38:02,110 --> 00:38:05,050
in that anonymous function it was just

00:38:03,760 --> 00:38:06,850
that little reference to a broadcast

00:38:05,050 --> 00:38:09,930
variable so where's the array

00:38:06,850 --> 00:38:14,410
coming from pardon my french

00:38:09,930 --> 00:38:16,840
there is no array in this expression it

00:38:14,410 --> 00:38:18,460
turns out it is this array that we

00:38:16,840 --> 00:38:20,370
started off with in the repple even

00:38:18,460 --> 00:38:22,600
though I'm not trying to use it directly

00:38:20,370 --> 00:38:24,850
I'm accessing it through the broadcast

00:38:22,600 --> 00:38:27,550
variable which means that all on the

00:38:24,850 --> 00:38:29,080
other side of my cluster it will you

00:38:27,550 --> 00:38:31,060
know pull it out of memory but it won't

00:38:29,080 --> 00:38:32,350
try to just serialize it now and throw

00:38:31,060 --> 00:38:34,960
it over there or at least that's what I

00:38:32,350 --> 00:38:37,180
would thought was happening but in fact

00:38:34,960 --> 00:38:41,170
that is what's happening and here's why

00:38:37,180 --> 00:38:42,550
so when you write code like this and

00:38:41,170 --> 00:38:44,220
here I've compressed it so that we can

00:38:42,550 --> 00:38:47,260
see it all in one space

00:38:44,220 --> 00:38:49,090
well the Scala compiler actually does in

00:38:47,260 --> 00:38:51,550
order to satisfy the requirements of the

00:38:49,090 --> 00:38:55,450
JVM as it wraps each of those lines in a

00:38:51,550 --> 00:38:56,830
synthesized class and then it and it

00:38:55,450 --> 00:38:59,650
nests them if you've ever looked at

00:38:56,830 --> 00:39:02,500
what's dumped from a like a long session

00:38:59,650 --> 00:39:03,880
you'll have this huge nesting of these

00:39:02,500 --> 00:39:07,780
objects that are wrapping all the

00:39:03,880 --> 00:39:11,260
previous expressions so when I do this

00:39:07,780 --> 00:39:15,070
I'm closing over be in this closure now

00:39:11,260 --> 00:39:17,500
and B is now a field inside this outer

00:39:15,070 --> 00:39:20,560
class I think of course I left out a lot

00:39:17,500 --> 00:39:22,720
of details but because B is a field now

00:39:20,560 --> 00:39:24,970
it sucks in the entire instance of the

00:39:22,720 --> 00:39:27,730
class and that happens to include the

00:39:24,970 --> 00:39:29,530
original array and that's why it's not

00:39:27,730 --> 00:39:32,410
serializable because they really did try

00:39:29,530 --> 00:39:35,500
to serialize the whole array this just

00:39:32,410 --> 00:39:38,350
confuses the hell out of new people to

00:39:35,500 --> 00:39:40,060
scala and and to spark and it's

00:39:38,350 --> 00:39:41,470
something that we should not have so

00:39:40,060 --> 00:39:42,250
we're actually investigating whether we

00:39:41,470 --> 00:39:44,170
can fix this

00:39:42,250 --> 00:39:45,640
re-engineer the repple to be better for

00:39:44,170 --> 00:39:47,290
these big data scenarios you know you

00:39:45,640 --> 00:39:49,030
could say this is a bug in the repple

00:39:47,290 --> 00:39:50,440
but really it's actually pushing the

00:39:49,030 --> 00:39:52,330
repple beyond what it was originally

00:39:50,440 --> 00:39:53,980
designed for I don't think Martin ever

00:39:52,330 --> 00:39:56,080
thought people would try to put like 40

00:39:53,980 --> 00:39:59,560
gigabyte arrays in the repple and why

00:39:56,080 --> 00:40:01,430
would you fry so there but there just

00:39:59,560 --> 00:40:03,050
quickly there are some

00:40:01,430 --> 00:40:04,760
we do for time pretty good there are

00:40:03,050 --> 00:40:06,170
some workarounds and I'll just briefly

00:40:04,760 --> 00:40:08,809
mention what they are and then we'll be

00:40:06,170 --> 00:40:10,849
done one is if you declare this

00:40:08,809 --> 00:40:13,010
transient in the usual way that avoids

00:40:10,849 --> 00:40:14,540
serializing over something if istic if

00:40:13,010 --> 00:40:18,609
it's marked as transient but you know

00:40:14,540 --> 00:40:20,990
it's weird to do this in the rebel right

00:40:18,609 --> 00:40:23,150
the other thing you can do is the old

00:40:20,990 --> 00:40:25,369
trick is just encapsulating stuff like

00:40:23,150 --> 00:40:29,240
data in this case inside a separate

00:40:25,369 --> 00:40:31,099
object so that the data object and then

00:40:29,240 --> 00:40:33,980
do this trick too you have to declare a

00:40:31,099 --> 00:40:36,349
variable that grabs the get B in this

00:40:33,980 --> 00:40:38,089
case and then you won't serialize over

00:40:36,349 --> 00:40:40,099
the data object you'll just serialize

00:40:38,089 --> 00:40:42,079
over the work object in this case and

00:40:40,099 --> 00:40:44,300
that way we won't serialize the array

00:40:42,079 --> 00:40:47,240
like once again once you learn this

00:40:44,300 --> 00:40:50,869
stuff it works fine but it's kind of not

00:40:47,240 --> 00:40:54,440
a very unintuitive so this is what we

00:40:50,869 --> 00:40:58,190
have to do and that's pretty much it so

00:40:54,440 --> 00:40:59,990
some conclusions Scala is actually

00:40:58,190 --> 00:41:04,460
adoption it's being driven a lot by

00:40:59,990 --> 00:41:06,200
SPARC right now but I think it's

00:41:04,460 --> 00:41:07,910
actually moving too fast the project is

00:41:06,200 --> 00:41:09,859
accumulating a lot of technical debt in

00:41:07,910 --> 00:41:11,960
my opinion and I'm a little worried that

00:41:09,859 --> 00:41:14,599
eventually we'll catch up to that but

00:41:11,960 --> 00:41:17,119
we'll see I'd also like to see some of

00:41:14,599 --> 00:41:19,220
the stuff that that SPARC introduced

00:41:17,119 --> 00:41:21,619
work its way back into Scala collections

00:41:19,220 --> 00:41:24,440
like some of these convenience methods

00:41:21,619 --> 00:41:26,000
for working with key value pairs and I

00:41:24,440 --> 00:41:29,930
just grabbed a list of some of them here

00:41:26,000 --> 00:41:30,740
that are pretty nice maybe we should

00:41:29,930 --> 00:41:32,630
work some of these tungsten

00:41:30,740 --> 00:41:34,130
optimizations back into the spark

00:41:32,630 --> 00:41:36,230
collections you know it may not make

00:41:34,130 --> 00:41:38,059
sense you always have to be careful like

00:41:36,230 --> 00:41:40,069
the Scala parallel collections actually

00:41:38,059 --> 00:41:41,420
didn't turn out to be that good or that

00:41:40,069 --> 00:41:43,430
useful because the overhead of

00:41:41,420 --> 00:41:45,559
parallelization often overwhelmed the

00:41:43,430 --> 00:41:47,030
advantages of it so you'd have to be

00:41:45,559 --> 00:41:49,190
careful of this is actually a benefit

00:41:47,030 --> 00:41:51,319
but it would be nice if maybe we could

00:41:49,190 --> 00:41:54,349
get the at least the object encoding as

00:41:51,319 --> 00:41:56,599
an option for maybe the JVM level you

00:41:54,349 --> 00:41:58,510
know that that linear encoding for

00:41:56,599 --> 00:42:00,470
better cache line performance and stuff

00:41:58,510 --> 00:42:02,089
and I would love to see some of these

00:42:00,470 --> 00:42:04,760
other things fix that I mentioned like

00:42:02,089 --> 00:42:07,760
be able to index arrays by Long's get

00:42:04,760 --> 00:42:10,150
real true value types and true unsigned

00:42:07,760 --> 00:42:12,470
types but those are maybe less crucial

00:42:10,150 --> 00:42:14,329
once again you can get this talk at my

00:42:12,470 --> 00:42:15,360
vanity website probably glad programming

00:42:14,329 --> 00:42:19,180
duck

00:42:15,360 --> 00:42:21,420
and I've got a white paper if you if you

00:42:19,180 --> 00:42:24,160
like reading white papers from vendors

00:42:21,420 --> 00:42:26,020
that goes into our view of fast data a

00:42:24,160 --> 00:42:28,590
little bit more and then that's it so

00:42:26,020 --> 00:42:28,590
thank you very much

00:42:33,770 --> 00:42:38,420
I think we've got to question two

00:42:36,260 --> 00:42:42,860
minutes for questions if there's one

00:42:38,420 --> 00:42:44,150
over here in the beginning of your talk

00:42:42,860 --> 00:42:47,620
you mentioned there are three

00:42:44,150 --> 00:42:51,470
representation of data data set in spark

00:42:47,620 --> 00:42:53,750
RDD data frame and streaming and then

00:42:51,470 --> 00:42:56,000
you mentioned the graph library and

00:42:53,750 --> 00:42:58,160
machine learning library so does those

00:42:56,000 --> 00:43:00,530
library take all three representations

00:42:58,160 --> 00:43:03,080
as input yeah that's a really good

00:43:00,530 --> 00:43:04,880
question so it's actually a problem that

00:43:03,080 --> 00:43:06,800
they don't right now so for example if

00:43:04,880 --> 00:43:08,750
you're working with streaming it would

00:43:06,800 --> 00:43:10,580
really be nice if streams are oriented

00:43:08,750 --> 00:43:12,230
around data frames but what typically

00:43:10,580 --> 00:43:14,660
happens is and I'm talking about what's

00:43:12,230 --> 00:43:17,960
happening as of before 2.0 this is

00:43:14,660 --> 00:43:21,230
actually all changing over the next six

00:43:17,960 --> 00:43:22,700
months and spark but in a streaming case

00:43:21,230 --> 00:43:24,980
you'd have to pull your data in as an

00:43:22,700 --> 00:43:27,020
RDD and then parse it with like the

00:43:24,980 --> 00:43:27,530
park' parser and data frames and so

00:43:27,020 --> 00:43:30,230
forth

00:43:27,530 --> 00:43:32,870
so it's not well integrated the ability

00:43:30,230 --> 00:43:34,940
to use data frames versus skipping our

00:43:32,870 --> 00:43:36,800
TVs when you're working with these other

00:43:34,940 --> 00:43:39,170
libraries and in fact all of them are

00:43:36,800 --> 00:43:41,750
changing the ML library is being

00:43:39,170 --> 00:43:45,040
replaced effectively by a data frame

00:43:41,750 --> 00:43:47,990
based library called ml for confusion

00:43:45,040 --> 00:43:49,640
and then grab the graphics library is a

00:43:47,990 --> 00:43:51,650
bit orphaned at the moment but there's a

00:43:49,640 --> 00:43:53,330
new one called graph frames it's based

00:43:51,650 --> 00:43:56,210
on data frames that will probably become

00:43:53,330 --> 00:43:57,770
the true graph library so they are

00:43:56,210 --> 00:43:59,450
moving in that direction but right now

00:43:57,770 --> 00:44:02,630
it's a bit awkward to use data frames

00:43:59,450 --> 00:44:08,600
with these higher-level API so anybody

00:44:02,630 --> 00:44:11,450
else up front here wait for the mic

00:44:08,600 --> 00:44:13,250
there if you don't mind is any of these

00:44:11,450 --> 00:44:17,330
optimizations something that could be

00:44:13,250 --> 00:44:19,760
interesting of coming in to Scala as or

00:44:17,330 --> 00:44:21,770
even JVM you just mention it a little

00:44:19,760 --> 00:44:24,410
bit that you would wish have wishes to

00:44:21,770 --> 00:44:25,970
the array VM to give contacts there yeah

00:44:24,410 --> 00:44:28,580
I I don't know any reason why you

00:44:25,970 --> 00:44:30,620
wouldn't want to do this unless for some

00:44:28,580 --> 00:44:33,710
reason the performance actually wasn't

00:44:30,620 --> 00:44:36,020
enough benefit to justify all the extra

00:44:33,710 --> 00:44:39,350
engineering you had to do but I'd like

00:44:36,020 --> 00:44:40,780
to like to believe that making the Scala

00:44:39,350 --> 00:44:42,650
collections use some of these

00:44:40,780 --> 00:44:45,740
optimizations would make them much

00:44:42,650 --> 00:44:47,210
faster but I think it more the problem

00:44:45,740 --> 00:44:48,079
so I'd really like to fix first would be

00:44:47,210 --> 00:44:50,420
like the repple

00:44:48,079 --> 00:44:54,099
and send things like that I think I saw

00:44:50,420 --> 00:44:58,029
another hand or maybe it was scratching

00:44:54,099 --> 00:44:58,029

YouTube URL: https://www.youtube.com/watch?v=J5Gu012T22U


