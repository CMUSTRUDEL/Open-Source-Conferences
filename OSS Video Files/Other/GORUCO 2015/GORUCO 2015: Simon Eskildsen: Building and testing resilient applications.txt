Title: GORUCO 2015: Simon Eskildsen: Building and testing resilient applications
Publication date: 2020-01-23
Playlist: GORUCO 2015
Description: 
	@Sirupsen
Drives fail, databases crash, fibers get cut and unindexed queries hit production. Do you know how your application reacts to those events? Are they covered by tests? What about the failures you haven't even thought of? To avoid cascading failures applications must adopt general patterns to defend against misbehaving dependencies, including themselves. This talk covers the resiliency techniques Shopify has successfully put into production at scale, and how we write tests to ensure we don't reintroduce single points of failure. You'll walk away from this talk equipped with the tools to make your applications resilient and to better sleep at night.

 Talk given at GORUCO 2015: http://goruco.com
Captions: 
	00:00:13,980 --> 00:00:21,689
my name is Simon I work for Shopify with

00:00:17,400 --> 00:00:24,630
an H I work on the side reliability

00:00:21,689 --> 00:00:27,630
performance and infrastructure and what

00:00:24,630 --> 00:00:30,630
I want to talk about today is how we

00:00:27,630 --> 00:00:32,550
created a big application and it has

00:00:30,630 --> 00:00:35,699
many moving parts and these parts fail

00:00:32,550 --> 00:00:37,530
but somehow the entire thing manages to

00:00:35,699 --> 00:00:38,910
stay up most of the time and we've

00:00:37,530 --> 00:00:41,010
learned a lot from this and what I want

00:00:38,910 --> 00:00:42,629
to talk about today is give you some

00:00:41,010 --> 00:00:44,760
resources from what we've learned and

00:00:42,629 --> 00:00:49,829
equip we equip you with some vocabulary

00:00:44,760 --> 00:00:52,859
to reason about these things so Shopify

00:00:49,829 --> 00:00:55,469
is a company that helps people sell

00:00:52,859 --> 00:00:59,399
things so we make commerce easy for

00:00:55,469 --> 00:01:00,930
people to sell things on online or in

00:00:59,399 --> 00:01:02,910
their brick-and-mortar stores on

00:01:00,930 --> 00:01:06,390
Pinterest on facebook and really

00:01:02,910 --> 00:01:10,380
anywhere and we're becoming a big

00:01:06,390 --> 00:01:13,050
company we have a lot of money going

00:01:10,380 --> 00:01:14,610
through us we're a large application we

00:01:13,050 --> 00:01:17,130
have a lot of developers pushing code

00:01:14,610 --> 00:01:22,229
every day at employing many times and as

00:01:17,130 --> 00:01:25,380
we all know more money more problems so

00:01:22,229 --> 00:01:27,179
what I want to talk about today is when

00:01:25,380 --> 00:01:28,770
we're building these large systems and

00:01:27,179 --> 00:01:32,160
nowadays really built building

00:01:28,770 --> 00:01:34,140
distributed systems is the default we're

00:01:32,160 --> 00:01:35,910
all starting to use the cloud more and

00:01:34,140 --> 00:01:37,440
more which means that we're using all

00:01:35,910 --> 00:01:39,569
this hardware and all these component

00:01:37,440 --> 00:01:42,539
components like routing and network from

00:01:39,569 --> 00:01:44,280
that we can't control at all so we have

00:01:42,539 --> 00:01:45,929
a new reality where we have to build

00:01:44,280 --> 00:01:47,940
system from a lot of components that we

00:01:45,929 --> 00:01:50,369
don't control and these components can

00:01:47,940 --> 00:01:52,819
fail and this is getting even more

00:01:50,369 --> 00:01:55,709
applicable with things like darker

00:01:52,819 --> 00:01:56,910
microservices and these new

00:01:55,709 --> 00:01:58,349
architectures where more and more

00:01:56,910 --> 00:02:00,869
components are introduced being on the

00:01:58,349 --> 00:02:02,789
rise and that means that your job is now

00:02:00,869 --> 00:02:07,200
also to tame the relationship between

00:02:02,789 --> 00:02:09,209
all these different services and so what

00:02:07,200 --> 00:02:11,430
I want to talk about today is how you

00:02:09,209 --> 00:02:14,069
make that reliable and this has been the

00:02:11,430 --> 00:02:16,530
biggest win for my team in all the time

00:02:14,069 --> 00:02:18,030
that I've been here for two years we now

00:02:16,530 --> 00:02:19,980
have confidence in what happens when the

00:02:18,030 --> 00:02:22,319
different components fail when they

00:02:19,980 --> 00:02:23,879
become slow and we have awareness of

00:02:22,319 --> 00:02:25,349
everything much more about what's going

00:02:23,879 --> 00:02:27,150
on and we're able to reason about the

00:02:25,349 --> 00:02:32,989
system this means that we sleep

00:02:27,150 --> 00:02:35,849
better and we're paste a lot less and

00:02:32,989 --> 00:02:38,370
one of the things that and one of the

00:02:35,849 --> 00:02:41,879
biggest parts of my job is preparing for

00:02:38,370 --> 00:02:45,209
Black Friday and Cyber Monday this is a

00:02:41,879 --> 00:02:47,159
crazy event for us where we have a lot

00:02:45,209 --> 00:02:49,620
more traffic coming in and some of our

00:02:47,159 --> 00:02:52,019
stores do flash sales on top of the hour

00:02:49,620 --> 00:02:54,030
where they might double our traffic or

00:02:52,019 --> 00:02:55,799
more where we have hundreds of thousands

00:02:54,030 --> 00:02:59,250
of customers coming in within the same

00:02:55,799 --> 00:03:01,799
minute and every year around this time

00:02:59,250 --> 00:03:03,569
my team starts talking about what we

00:03:01,799 --> 00:03:06,450
need to do to prepare for Black Friday

00:03:03,569 --> 00:03:08,909
and Cyber Monday this year and last year

00:03:06,450 --> 00:03:10,530
around this time we were seeing a lot of

00:03:08,909 --> 00:03:12,959
embarrassing failures things were

00:03:10,530 --> 00:03:14,430
failing left and right and sometimes the

00:03:12,959 --> 00:03:16,290
entire system went down even when

00:03:14,430 --> 00:03:19,079
something that seems to be trivial

00:03:16,290 --> 00:03:21,060
trivial in the big picture failed we

00:03:19,079 --> 00:03:22,609
didn't have a great overview of all the

00:03:21,060 --> 00:03:27,510
relationship between all these services

00:03:22,609 --> 00:03:30,629
so we sat down a team of five to seven

00:03:27,510 --> 00:03:34,440
people and fought long and hard about

00:03:30,629 --> 00:03:35,819
how to tackle this so that's what I'm

00:03:34,440 --> 00:03:37,739
going to talk about for the rest of this

00:03:35,819 --> 00:03:40,230
talk I'm going to talk about resiliency

00:03:37,739 --> 00:03:42,780
and resiliency is the practice of

00:03:40,230 --> 00:03:44,730
building a system from many unreliable

00:03:42,780 --> 00:03:46,980
components that is reliable as a whole

00:03:44,730 --> 00:03:50,250
if you have a single service that is

00:03:46,980 --> 00:03:52,319
failing or is slow it can't compromise

00:03:50,250 --> 00:03:55,709
the availability or performance of the

00:03:52,319 --> 00:03:58,290
entire system you need to have loosely

00:03:55,709 --> 00:04:00,750
coupled things that can act on it on its

00:03:58,290 --> 00:04:03,989
own to have a reliable large

00:04:00,750 --> 00:04:07,489
infrastructure and if you don't do this

00:04:03,989 --> 00:04:10,669
you're up time and will suffer under the

00:04:07,489 --> 00:04:12,989
faith of the micro service equation

00:04:10,669 --> 00:04:16,199
where if you have some amount of

00:04:12,989 --> 00:04:17,699
services d your availability decreases

00:04:16,199 --> 00:04:19,709
exponentially as you're adding more

00:04:17,699 --> 00:04:22,109
services even if you have something like

00:04:19,709 --> 00:04:24,449
four nines with 10 to 100 services this

00:04:22,109 --> 00:04:26,370
really really quickly corbels down two

00:04:24,449 --> 00:04:28,409
days of downtime per year now this is a

00:04:26,370 --> 00:04:30,389
very rough estimate and you're very

00:04:28,409 --> 00:04:33,690
likely not in this situation but this is

00:04:30,389 --> 00:04:37,050
really the worst case scenario and as

00:04:33,690 --> 00:04:37,889
anyone who's ever seen a an exponential

00:04:37,050 --> 00:04:39,180
graph that looks something like this

00:04:37,889 --> 00:04:40,529
very very quickly as you're adding

00:04:39,180 --> 00:04:43,859
services

00:04:40,529 --> 00:04:47,699
using your overall availability and

00:04:43,859 --> 00:04:50,489
really on your system is only as as weak

00:04:47,699 --> 00:04:53,339
as your weakest sink or as strong as

00:04:50,489 --> 00:04:54,689
your weakest single point of failure so

00:04:53,339 --> 00:04:56,129
if you're not aware of this and you're

00:04:54,689 --> 00:04:57,269
not thinking about the overall

00:04:56,129 --> 00:04:59,159
resistance resilience you have the

00:04:57,269 --> 00:05:01,559
system as you're adding services adding

00:04:59,159 --> 00:05:05,519
components leads to decrease decrease in

00:05:01,559 --> 00:05:08,039
availability and many of us might think

00:05:05,519 --> 00:05:09,389
well we have a big monorail it's fine we

00:05:08,039 --> 00:05:10,859
have the big monolithic application

00:05:09,389 --> 00:05:12,479
doesn't have that have any external

00:05:10,859 --> 00:05:14,969
dependencies we're good we're really

00:05:12,479 --> 00:05:18,329
that's lying to ourselves Shopify is a

00:05:14,969 --> 00:05:20,459
big proud monolith you might say and but

00:05:18,329 --> 00:05:22,739
we still have tons of dependencies we

00:05:20,459 --> 00:05:24,749
have baked relational beta stores we

00:05:22,739 --> 00:05:26,459
have tons of key value stores we're

00:05:24,749 --> 00:05:29,309
talking to payment gateways we're

00:05:26,459 --> 00:05:31,679
talking to api's we're sending email

00:05:29,309 --> 00:05:34,199
three tongs CRM systems there are easily

00:05:31,679 --> 00:05:35,999
tens if not over 100 dependencies

00:05:34,199 --> 00:05:37,799
externally to us that we don't always

00:05:35,999 --> 00:05:39,649
have control over and each one of these

00:05:37,799 --> 00:05:41,459
can compromise the system of the

00:05:39,649 --> 00:05:43,799
performance of the entire system if

00:05:41,459 --> 00:05:47,429
you're not careful so now we're going to

00:05:43,799 --> 00:05:49,919
talk about fall backs so when you're

00:05:47,429 --> 00:05:51,749
cheering one of these services there you

00:05:49,919 --> 00:05:55,019
have some some data that you want out of

00:05:51,749 --> 00:05:56,999
it for example if you browse the netflix

00:05:55,019 --> 00:05:59,369
page you get an overview of all your

00:05:56,999 --> 00:06:01,829
titles and its title has some star

00:05:59,369 --> 00:06:04,379
rating now is that service that stirs

00:06:01,829 --> 00:06:06,089
that serving the star rating is down you

00:06:04,379 --> 00:06:07,829
can do two things you can fail the

00:06:06,089 --> 00:06:09,419
entire page which is the default in

00:06:07,829 --> 00:06:10,529
high-level language like Ruby because it

00:06:09,419 --> 00:06:12,869
will raise an exception trying to

00:06:10,529 --> 00:06:15,359
connect to it or you can have a

00:06:12,869 --> 00:06:16,949
secondary reasonable behavior which is

00:06:15,359 --> 00:06:19,379
just falling back to something like five

00:06:16,949 --> 00:06:21,469
gray stars or something like that that's

00:06:19,379 --> 00:06:26,459
much better because we can still browse

00:06:21,469 --> 00:06:28,109
everything and we're not we're not we're

00:06:26,459 --> 00:06:31,229
not compromising the entire system just

00:06:28,109 --> 00:06:34,289
because that one service is down and let

00:06:31,229 --> 00:06:37,349
me take a more nearby example for us we

00:06:34,289 --> 00:06:39,659
have stores that sell sneakers in this

00:06:37,349 --> 00:06:41,399
example and when we're rendering the

00:06:39,659 --> 00:06:45,059
store it's made of a lot of different

00:06:41,399 --> 00:06:47,219
services I might have a service that the

00:06:45,059 --> 00:06:48,899
search we might have one that stores the

00:06:47,219 --> 00:06:51,539
sessions we might have a data store for

00:06:48,899 --> 00:06:53,279
carts we have a CDN dependency and my

00:06:51,539 --> 00:06:54,000
sequel shard wear the shorts job is

00:06:53,279 --> 00:06:55,320
stored on

00:06:54,000 --> 00:06:58,140
and there might be many more

00:06:55,320 --> 00:06:59,970
dependencies in this so imagine if one

00:06:58,140 --> 00:07:01,560
of these dependencies fail all of these

00:06:59,970 --> 00:07:04,020
are somewhat of haagen all to the entire

00:07:01,560 --> 00:07:06,960
page for example if we kill the session

00:07:04,020 --> 00:07:08,580
page by default in rails if we kill the

00:07:06,960 --> 00:07:10,950
store where the sessions are stored

00:07:08,580 --> 00:07:13,110
except if it's stored in a cookie you

00:07:10,950 --> 00:07:15,840
will just get an HTTP 500 it will raise

00:07:13,110 --> 00:07:18,570
an exception and the customer will see

00:07:15,840 --> 00:07:21,300
something like this but that breaks the

00:07:18,570 --> 00:07:24,270
principle we have one service that now

00:07:21,300 --> 00:07:26,730
made the entire application 500 even

00:07:24,270 --> 00:07:28,230
though it's not daddy it's not that

00:07:26,730 --> 00:07:30,780
important to the storefront customers

00:07:28,230 --> 00:07:32,880
can still browse and so on even though

00:07:30,780 --> 00:07:34,500
the session storage is down so that

00:07:32,880 --> 00:07:36,540
might look something like this well if

00:07:34,500 --> 00:07:38,880
the session storage goes down we just

00:07:36,540 --> 00:07:40,470
sign the user out they can still browse

00:07:38,880 --> 00:07:42,690
the storefront they can still do

00:07:40,470 --> 00:07:44,340
checkouts they can still add things to

00:07:42,690 --> 00:07:47,120
their cart they can still do anything

00:07:44,340 --> 00:07:50,130
except associate orders with their

00:07:47,120 --> 00:07:52,110
account this is great the customers are

00:07:50,130 --> 00:07:53,730
happy they can still do check out the

00:07:52,110 --> 00:07:55,200
merchants are happy because they don't

00:07:53,730 --> 00:07:56,490
even notice this downtime the

00:07:55,200 --> 00:07:57,900
infrastructure team is happy because

00:07:56,490 --> 00:08:00,450
when this failure happens and we get

00:07:57,900 --> 00:08:01,979
paged we're less stressed out because we

00:08:00,450 --> 00:08:03,450
know exactly we know that the

00:08:01,979 --> 00:08:05,669
application is coping successfully to

00:08:03,450 --> 00:08:07,290
this and the I dish you might go through

00:08:05,669 --> 00:08:09,390
every single one of the dependencies and

00:08:07,290 --> 00:08:11,940
make sure that this page is acting well

00:08:09,390 --> 00:08:13,440
when one of them are down for example

00:08:11,940 --> 00:08:16,860
let's say that the card service is down

00:08:13,440 --> 00:08:18,000
you could just make sure that you can't

00:08:16,860 --> 00:08:19,380
add something to the card but still

00:08:18,000 --> 00:08:20,520
browse this store that's a great first

00:08:19,380 --> 00:08:22,740
revision but you could do something

00:08:20,520 --> 00:08:25,169
extremely sophisticated where you're

00:08:22,740 --> 00:08:27,000
adding the card items to a local storage

00:08:25,169 --> 00:08:28,890
in JavaScript and in persisting it when

00:08:27,000 --> 00:08:31,620
did persistence card persistence layer

00:08:28,890 --> 00:08:36,539
is back you can do very very advanced

00:08:31,620 --> 00:08:38,250
fallbacks if you need to so the code

00:08:36,539 --> 00:08:41,099
might look something like this we've all

00:08:38,250 --> 00:08:43,080
written code like this where we get a

00:08:41,099 --> 00:08:44,880
user and we fetch it out of the session

00:08:43,080 --> 00:08:46,560
layer and again if you're using cookies

00:08:44,880 --> 00:08:48,060
this doesn't really apply but if you're

00:08:46,560 --> 00:08:49,620
storing it is something like Redis and

00:08:48,060 --> 00:08:53,430
something that's where this user's not

00:08:49,620 --> 00:08:55,890
stored dis applies so the problem here

00:08:53,430 --> 00:08:57,330
is that if you can't access the session

00:08:55,890 --> 00:08:59,460
layer this code will raise an exception

00:08:57,330 --> 00:09:02,580
to give the 500 the first case I shows

00:08:59,460 --> 00:09:04,200
before so the more resilient code would

00:09:02,580 --> 00:09:06,320
look something like this it rescues the

00:09:04,200 --> 00:09:08,060
error and returns back

00:09:06,320 --> 00:09:10,250
and the great thing is that a template

00:09:08,060 --> 00:09:12,170
already does this it already checks if

00:09:10,250 --> 00:09:14,560
there's a user and if there's no user it

00:09:12,170 --> 00:09:16,970
just shows the user has signed out so

00:09:14,560 --> 00:09:19,370
everything else just work we only had it

00:09:16,970 --> 00:09:22,370
two had to add two lines of code to make

00:09:19,370 --> 00:09:24,020
this way more resilient and you can do

00:09:22,370 --> 00:09:26,030
this with a lot of other things you just

00:09:24,020 --> 00:09:27,770
return an empty empty data structure

00:09:26,030 --> 00:09:31,430
when the data stores down is this much

00:09:27,770 --> 00:09:32,840
more reasonable so now the great thing

00:09:31,430 --> 00:09:36,860
in the Ruby community is that you will

00:09:32,840 --> 00:09:40,100
ask how do we test this and the first

00:09:36,860 --> 00:09:42,230
thing you might do is do a mock so you

00:09:40,100 --> 00:09:45,080
grab your memcache driver and you try

00:09:42,230 --> 00:09:48,530
them to mock out and raise some errors

00:09:45,080 --> 00:09:51,350
and simulate a problem but this very

00:09:48,530 --> 00:09:55,760
very easily n sub covering as many boxes

00:09:51,350 --> 00:09:57,080
is uncovering so and you're writing one

00:09:55,760 --> 00:09:58,550
for each different client you're running

00:09:57,080 --> 00:10:00,980
writing one for my sequel read as

00:09:58,550 --> 00:10:03,020
memcache elasticsearch what you have and

00:10:00,980 --> 00:10:04,490
it's really hard to do this at the right

00:10:03,020 --> 00:10:06,490
layer we did this to start with and

00:10:04,490 --> 00:10:09,230
easily became a hundred lines too

00:10:06,490 --> 00:10:11,300
closely mimic the real behavior behavior

00:10:09,230 --> 00:10:13,970
of the driver now you can take the other

00:10:11,300 --> 00:10:16,190
extreme and do it in production kill

00:10:13,970 --> 00:10:17,840
nodes slow them down do something like

00:10:16,190 --> 00:10:21,080
the chaos monkey which I'm going to talk

00:10:17,840 --> 00:10:23,300
a bit about later but that doesn't

00:10:21,080 --> 00:10:25,010
really my inner rubios is not really

00:10:23,300 --> 00:10:26,630
happy with that solution either I want

00:10:25,010 --> 00:10:29,060
to be able to test this I wanted to read

00:10:26,630 --> 00:10:31,430
be reproducible and I want see I to run

00:10:29,060 --> 00:10:33,020
with all these tests making sure that

00:10:31,430 --> 00:10:36,470
I'm not regressing the resiliency

00:10:33,020 --> 00:10:39,080
guarantees that i had two weeks ago so

00:10:36,470 --> 00:10:40,730
there didn't really exist anything that

00:10:39,080 --> 00:10:42,710
did the middle ground and the middle

00:10:40,730 --> 00:10:45,710
ground is to try and emulate this at the

00:10:42,710 --> 00:10:48,500
TCP layer what if we can slow down all

00:10:45,710 --> 00:10:50,540
these connections and simulate simulate

00:10:48,500 --> 00:10:53,420
fares at the TCP level that means that

00:10:50,540 --> 00:10:57,050
we don't have to we don't have to write

00:10:53,420 --> 00:10:58,070
a client every a failing client mock for

00:10:57,050 --> 00:10:59,510
every single driver and we're not

00:10:58,070 --> 00:11:00,830
testing in production and we can run

00:10:59,510 --> 00:11:03,620
this proxy in all development

00:11:00,830 --> 00:11:05,090
environments and on CI the problem was

00:11:03,620 --> 00:11:09,050
that this proxy didn't exist so we had

00:11:05,090 --> 00:11:11,870
to build it so Shopify build a proxy

00:11:09,050 --> 00:11:14,360
called toxic proxy and it's a proxy that

00:11:11,870 --> 00:11:16,640
allows you to apply toxic stew different

00:11:14,360 --> 00:11:18,990
channels on this proxy it's at the TCP

00:11:16,640 --> 00:11:20,580
level so instead of connecting directly

00:11:18,990 --> 00:11:24,060
my sequel you connect to my cycle

00:11:20,580 --> 00:11:25,350
through this proxy and then via HTTP you

00:11:24,060 --> 00:11:26,580
can simulate a different failures you

00:11:25,350 --> 00:11:28,680
can tell it now it's slow now

00:11:26,580 --> 00:11:30,420
connections warm clothes and so on and

00:11:28,680 --> 00:11:32,550
so on and we found a ton of bucks and

00:11:30,420 --> 00:11:34,110
with this we found bucks in rails we

00:11:32,550 --> 00:11:37,470
found bucks and different drivers we

00:11:34,110 --> 00:11:39,570
found bugs in our own code and this was

00:11:37,470 --> 00:11:41,730
way superior to using mocks and every

00:11:39,570 --> 00:11:44,399
developer is now running this and it's

00:11:41,730 --> 00:11:46,440
running on CI before every every before

00:11:44,399 --> 00:11:51,330
the test pass and we even went as far to

00:11:46,440 --> 00:11:52,980
create a page in our admin bar where our

00:11:51,330 --> 00:11:54,060
developers can tinker with all the

00:11:52,980 --> 00:11:56,850
different connections slow them down

00:11:54,060 --> 00:12:00,209
kill them and apply all these these

00:11:56,850 --> 00:12:02,670
toxic stew the proxies so now we can go

00:12:00,209 --> 00:12:06,120
back to what we talked about before this

00:12:02,670 --> 00:12:08,850
resiliency of the session storage so we

00:12:06,120 --> 00:12:11,310
write a test we tell toxic proxy kill

00:12:08,850 --> 00:12:13,140
the session storage within this block it

00:12:11,310 --> 00:12:15,330
sends an HTTP call to talk to you proxy

00:12:13,140 --> 00:12:17,070
tells it to kill that connection then

00:12:15,330 --> 00:12:18,330
you do all your code you get the front

00:12:17,070 --> 00:12:20,580
page and make sure there's a success

00:12:18,330 --> 00:12:22,230
maybe you're checking for a flash that

00:12:20,580 --> 00:12:26,070
says that the session storage is

00:12:22,230 --> 00:12:28,920
currently down and this works great but

00:12:26,070 --> 00:12:31,320
now that we've talked about both fall

00:12:28,920 --> 00:12:34,020
backs the plan B of all of these

00:12:31,320 --> 00:12:36,270
different functionalities and we've

00:12:34,020 --> 00:12:37,560
talked about tests but it can be really

00:12:36,270 --> 00:12:39,890
really hard to get an overview of

00:12:37,560 --> 00:12:42,690
whether you're resilient as a whole all

00:12:39,890 --> 00:12:43,770
there are many many different different

00:12:42,690 --> 00:12:45,690
dependencies between your different

00:12:43,770 --> 00:12:47,459
sections of your applications and all

00:12:45,690 --> 00:12:50,850
your components so how do we get an

00:12:47,459 --> 00:12:53,910
overview so what we did was we came up

00:12:50,850 --> 00:12:56,940
with a resiliency matrix we have out of

00:12:53,910 --> 00:12:58,230
the out of the right we have the

00:12:56,940 --> 00:13:00,270
different sections in power application

00:12:58,230 --> 00:13:02,399
this might be check out the storefront

00:13:00,270 --> 00:13:03,720
the administration panel and on the left

00:13:02,399 --> 00:13:06,870
we have all the different dependencies

00:13:03,720 --> 00:13:09,089
that make up these different sections so

00:13:06,870 --> 00:13:11,700
our my sequel our caching tier our

00:13:09,089 --> 00:13:13,829
searched here message queue external

00:13:11,700 --> 00:13:17,160
external gateways and so on and so on

00:13:13,829 --> 00:13:19,680
and read here indicates that when the

00:13:17,160 --> 00:13:22,940
component on the left is down the

00:13:19,680 --> 00:13:26,150
section on the on the right is

00:13:22,940 --> 00:13:28,829
unavailable or available or degraded and

00:13:26,150 --> 00:13:31,230
we started filling this out with toxic

00:13:28,829 --> 00:13:32,339
proxy and we were shocked at what this

00:13:31,230 --> 00:13:33,930
looked like we had

00:13:32,339 --> 00:13:35,850
we paid much attention to this and it

00:13:33,930 --> 00:13:38,040
just groaned this app over 10 years and

00:13:35,850 --> 00:13:41,490
it was it looked like somewhat of a

00:13:38,040 --> 00:13:43,199
nightmare so what we did was we sat down

00:13:41,490 --> 00:13:45,600
and we wrote a test for each one of

00:13:43,199 --> 00:13:47,370
these cells and some of these were still

00:13:45,600 --> 00:13:50,279
500 some of these were successes and

00:13:47,370 --> 00:13:54,529
then the mission of the team just became

00:13:50,279 --> 00:13:57,509
to flip all of these 500 to success and

00:13:54,529 --> 00:13:59,970
as we went through this it wasn't only

00:13:57,509 --> 00:14:03,120
in our code there were problems we found

00:13:59,970 --> 00:14:05,550
a couple of bugs in rails and this is

00:14:03,120 --> 00:14:07,649
one of them where there's a middleware

00:14:05,550 --> 00:14:10,189
that is Stan she ate security cash and

00:14:07,649 --> 00:14:12,360
when that happens it grabs a connection

00:14:10,189 --> 00:14:16,230
but if there is no connection it tries

00:14:12,360 --> 00:14:17,759
to establish one so this means that even

00:14:16,230 --> 00:14:21,180
though you have a page that doesn't

00:14:17,759 --> 00:14:22,889
access your database it still fails

00:14:21,180 --> 00:14:24,480
because this middleware doesn't know

00:14:22,889 --> 00:14:26,850
anything about whether the page is going

00:14:24,480 --> 00:14:28,769
to access the data to you later on so if

00:14:26,850 --> 00:14:31,769
you right now have an app that requires

00:14:28,769 --> 00:14:33,689
active record and you have a page that

00:14:31,769 --> 00:14:36,540
doesn't access my sequel when my sig was

00:14:33,689 --> 00:14:38,040
down you can't access that page this is

00:14:36,540 --> 00:14:40,350
still a bug in master we had three

00:14:38,040 --> 00:14:41,790
monkey patched it and I'm a terrible

00:14:40,350 --> 00:14:44,040
terrible person for not submitting this

00:14:41,790 --> 00:14:46,470
upstream yet but there's a couple things

00:14:44,040 --> 00:14:47,730
that need to be addressed in rails like

00:14:46,470 --> 00:14:49,439
there needs to be a general exception

00:14:47,730 --> 00:14:50,759
from all the drivers when you can't

00:14:49,439 --> 00:14:53,459
connect to something before it's going

00:14:50,759 --> 00:14:54,990
to be addressed at the right layer so

00:14:53,459 --> 00:14:56,309
the problem is with this line of code

00:14:54,990 --> 00:14:58,679
where it's trying to establish a

00:14:56,309 --> 00:15:01,949
connection if it if it's not already

00:14:58,679 --> 00:15:03,600
there so let's take another example so

00:15:01,949 --> 00:15:05,189
we have a product and the product has

00:15:03,600 --> 00:15:07,110
tags and for whatever reason these tacks

00:15:05,189 --> 00:15:08,970
are stored in a Redis instance and now

00:15:07,110 --> 00:15:11,189
looking at the session code from before

00:15:08,970 --> 00:15:13,230
you can start to connect the dots and

00:15:11,189 --> 00:15:16,230
how could we make this resilient well

00:15:13,230 --> 00:15:18,660
let's just do us before we rescue the

00:15:16,230 --> 00:15:19,949
register and return an MP or a template

00:15:18,660 --> 00:15:21,420
copes with this just fine it just

00:15:19,949 --> 00:15:23,249
iterates over an empty array doesn't

00:15:21,420 --> 00:15:25,759
show tags customers are happy we're

00:15:23,249 --> 00:15:29,040
happy it doesn't really matter too much

00:15:25,759 --> 00:15:30,540
but if you're when as your database or

00:15:29,040 --> 00:15:33,809
not your database but your code base

00:15:30,540 --> 00:15:35,850
grows this you start to scatter this

00:15:33,809 --> 00:15:37,709
information all over and very quickly

00:15:35,850 --> 00:15:39,420
you leak the abstraction of the

00:15:37,709 --> 00:15:41,189
resiliency layer all over the code and

00:15:39,420 --> 00:15:43,649
you sprinkle here all these rescues all

00:15:41,189 --> 00:15:44,480
around so what we started doing instead

00:15:43,649 --> 00:15:47,000
is building these

00:15:44,480 --> 00:15:48,500
decorator's these data data structures

00:15:47,000 --> 00:15:52,190
that we were storing and secondary data

00:15:48,500 --> 00:15:53,510
data stores we provided Brazilian

00:15:52,190 --> 00:15:55,490
abstractions around and there we only

00:15:53,510 --> 00:15:56,750
needed a handful of these and this men's

00:15:55,490 --> 00:15:58,190
that eighty percent of the time our

00:15:56,750 --> 00:15:59,870
developers don't have to care about the

00:15:58,190 --> 00:16:03,500
resiliency layer it's already done for

00:15:59,870 --> 00:16:04,790
them and when you're in only when you're

00:16:03,500 --> 00:16:07,880
introducing a new dependencies do you

00:16:04,790 --> 00:16:09,769
have to add another one of these so

00:16:07,880 --> 00:16:11,449
that's the first part where we're

00:16:09,769 --> 00:16:16,180
talking about fall backs and we're

00:16:11,449 --> 00:16:19,399
talking about building resilient code

00:16:16,180 --> 00:16:21,170
but really the majority of the time on

00:16:19,399 --> 00:16:25,160
my team wasn't really spent on fall

00:16:21,170 --> 00:16:30,529
backs and on writing tests but on

00:16:25,160 --> 00:16:32,899
dealing with slow slow components this

00:16:30,529 --> 00:16:34,720
was a lot lot harder than any of than

00:16:32,899 --> 00:16:37,190
anything else I've talked about here

00:16:34,720 --> 00:16:39,170
because these petitions and network

00:16:37,190 --> 00:16:42,230
partitions and slowness and latency take

00:16:39,170 --> 00:16:43,850
very interesting forms sometimes the

00:16:42,230 --> 00:16:46,040
entire thing is slow because there's an

00:16:43,850 --> 00:16:48,709
i/o problem on the nape data node but

00:16:46,040 --> 00:16:50,420
other times only some of your machines

00:16:48,709 --> 00:16:51,920
or experienced failure experiencing

00:16:50,420 --> 00:16:53,389
failure maybe amazon is having a problem

00:16:51,920 --> 00:16:54,829
with one of its core routers and you

00:16:53,389 --> 00:16:58,730
happen to have a couple of services on

00:16:54,829 --> 00:17:00,529
them you don't know and we can

00:16:58,730 --> 00:17:02,899
illustrate this with a very very simple

00:17:00,529 --> 00:17:05,299
thing it's called Biddle slaw and is

00:17:02,899 --> 00:17:07,910
from queueing theory which Nadia talked

00:17:05,299 --> 00:17:10,189
about earlier and really all it says is

00:17:07,910 --> 00:17:12,860
that if your response time goes up your

00:17:10,189 --> 00:17:15,140
throughput goes down and that makes

00:17:12,860 --> 00:17:17,179
sense we can we can we can illustrate

00:17:15,140 --> 00:17:19,970
this with an example we have a web

00:17:17,179 --> 00:17:23,329
server in this case a unicorn and it can

00:17:19,970 --> 00:17:25,280
only serve one request at a time and now

00:17:23,329 --> 00:17:27,740
one of our data stores is slow it's just

00:17:25,280 --> 00:17:30,320
not responding but it's not rejecting

00:17:27,740 --> 00:17:32,600
right away either so someone is hitting

00:17:30,320 --> 00:17:34,250
this page he's hitting / impacted and

00:17:32,600 --> 00:17:36,830
he's hitting that time out of half a

00:17:34,250 --> 00:17:38,390
second to the data store but while we're

00:17:36,830 --> 00:17:41,450
waiting for the timeout another request

00:17:38,390 --> 00:17:44,240
comes in it's going to slice okay which

00:17:41,450 --> 00:17:45,230
take 20 milliseconds that's the response

00:17:44,240 --> 00:17:48,130
time that we've done although our

00:17:45,230 --> 00:17:50,690
capacity planning according to so that

00:17:48,130 --> 00:17:53,350
that person is just waiting behind the

00:17:50,690 --> 00:17:57,110
impact one and there's another one and

00:17:53,350 --> 00:17:58,520
another one and this one is the slow one

00:17:57,110 --> 00:18:00,410
and very very quickly

00:17:58,520 --> 00:18:02,270
all of your different workers starts to

00:18:00,410 --> 00:18:06,590
have backed up requests that are both

00:18:02,270 --> 00:18:07,970
that are slow and fast so again we're

00:18:06,590 --> 00:18:11,090
breaking the principle from the start

00:18:07,970 --> 00:18:13,070
where we let allow slow component to let

00:18:11,090 --> 00:18:15,650
all the other components fail and all

00:18:13,070 --> 00:18:18,440
the other interactions even though we

00:18:15,650 --> 00:18:19,970
have a timeout in place and we can

00:18:18,440 --> 00:18:22,070
illustrate another problem with timeouts

00:18:19,970 --> 00:18:23,480
with this example we have a simple

00:18:22,070 --> 00:18:25,400
architecture where we have a load

00:18:23,480 --> 00:18:28,220
balancer and it's distributing load

00:18:25,400 --> 00:18:29,960
between a couple of services and they're

00:18:28,220 --> 00:18:32,540
talking to you some data stores in the

00:18:29,960 --> 00:18:35,180
back end and an external API and they're

00:18:32,540 --> 00:18:37,670
very fast and everything is great but

00:18:35,180 --> 00:18:39,110
now this data data storage that we're

00:18:37,670 --> 00:18:41,060
accessing extremely frequently this

00:18:39,110 --> 00:18:42,950
could be something like a Redis bumps

00:18:41,060 --> 00:18:44,600
its response time from about point two

00:18:42,950 --> 00:18:46,640
milliseconds to two milliseconds and

00:18:44,600 --> 00:18:48,320
this does this seems pretty innocent and

00:18:46,640 --> 00:18:49,550
it seems innocent enough that we don't

00:18:48,320 --> 00:18:53,000
have a timeout of two milliseconds

00:18:49,550 --> 00:18:54,230
that's crazy low it doesn't even that

00:18:53,000 --> 00:18:55,760
doesn't even account for things like

00:18:54,230 --> 00:18:57,950
network level transmits that take

00:18:55,760 --> 00:19:01,900
hundreds of milliseconds so you wouldn't

00:18:57,950 --> 00:19:04,940
have a time out of that low either and

00:19:01,900 --> 00:19:06,440
this ten tenfold increase in latency

00:19:04,940 --> 00:19:08,750
from that data storage frequently

00:19:06,440 --> 00:19:11,540
accessed from everywhere else also

00:19:08,750 --> 00:19:13,790
results in the throughput dropping by an

00:19:11,540 --> 00:19:15,730
order of magnitude this follows from the

00:19:13,790 --> 00:19:18,140
little little slaw that I showed before

00:19:15,730 --> 00:19:21,860
so timeouts are really not good enough

00:19:18,140 --> 00:19:24,110
once you're unsure at scale your

00:19:21,860 --> 00:19:26,570
response time suffers you suffer under

00:19:24,110 --> 00:19:28,220
littles law and setting the timeouts

00:19:26,570 --> 00:19:29,870
right is extremely problematic they're

00:19:28,220 --> 00:19:31,040
either too high and you get the problem

00:19:29,870 --> 00:19:33,590
with the unicorn and all the back

00:19:31,040 --> 00:19:35,210
requests or they're way too low and they

00:19:33,590 --> 00:19:36,440
don't account for all the outliers you

00:19:35,210 --> 00:19:38,690
might have customers that have a lot

00:19:36,440 --> 00:19:40,520
more data than other people than your

00:19:38,690 --> 00:19:41,840
other customers and quote requests to

00:19:40,520 --> 00:19:44,030
them for them might take a couple

00:19:41,840 --> 00:19:46,160
milliseconds more but you don't want to

00:19:44,030 --> 00:19:48,200
completely cut them off to solve this

00:19:46,160 --> 00:19:50,630
problem either so timeouts are not good

00:19:48,200 --> 00:19:53,420
enough we need ways to fail faster than

00:19:50,630 --> 00:19:56,930
simple timeouts so just a great book

00:19:53,420 --> 00:19:59,120
called release it and it describes among

00:19:56,930 --> 00:20:00,830
others a mother among other things some

00:19:59,120 --> 00:20:02,630
heuristics on failing fast and building

00:20:00,830 --> 00:20:04,160
really really good software I definitely

00:20:02,630 --> 00:20:06,320
recommend reading this book if you find

00:20:04,160 --> 00:20:08,150
this interesting so it describes at

00:20:06,320 --> 00:20:10,760
least two hristos for failing faster

00:20:08,150 --> 00:20:11,700
than timeouts it describes circuit

00:20:10,760 --> 00:20:14,010
breakers

00:20:11,700 --> 00:20:16,679
and it describes bulkheads and these are

00:20:14,010 --> 00:20:19,460
two different patterns that complement

00:20:16,679 --> 00:20:24,450
each other pretty well on failing fast

00:20:19,460 --> 00:20:26,730
so circuit breakers essentially is a

00:20:24,450 --> 00:20:28,649
mechanism where you make the heuristic

00:20:26,730 --> 00:20:30,149
that if you have done a couple of

00:20:28,649 --> 00:20:32,700
requests to it to a data store or

00:20:30,149 --> 00:20:34,919
service and the couple on these requests

00:20:32,700 --> 00:20:36,720
in the past have failed then it's very

00:20:34,919 --> 00:20:39,720
likely that in the future they will also

00:20:36,720 --> 00:20:41,610
fail at least for some amount of time so

00:20:39,720 --> 00:20:45,240
what do you do is every time you do a

00:20:41,610 --> 00:20:46,649
call to your to your data store or

00:20:45,240 --> 00:20:48,779
something like that or another service

00:20:46,649 --> 00:20:50,850
you first asked is the circuit open if

00:20:48,779 --> 00:20:53,190
the circuit is open you instantly

00:20:50,850 --> 00:20:55,289
trigger an exception if not you go ahead

00:20:53,190 --> 00:20:58,500
with the with the requests to the back

00:20:55,289 --> 00:21:00,210
end so the dad request fail if it fail

00:20:58,500 --> 00:21:02,340
you market failure and if enough of

00:21:00,210 --> 00:21:05,820
these failure figures happened within

00:21:02,340 --> 00:21:07,139
some sliding window then you open the

00:21:05,820 --> 00:21:09,570
circuit at all the future requests will

00:21:07,139 --> 00:21:11,130
fail if the drive drive it didn't feel

00:21:09,570 --> 00:21:12,809
well then you're Marcus success and I

00:21:11,130 --> 00:21:15,269
think not if enough success has happened

00:21:12,809 --> 00:21:17,370
within the time frame then you close the

00:21:15,269 --> 00:21:19,710
circuit again and then you enter call

00:21:17,370 --> 00:21:22,019
and return back to the client so the way

00:21:19,710 --> 00:21:23,730
did you close the circuit again after

00:21:22,019 --> 00:21:25,710
some time is that after some time out

00:21:23,730 --> 00:21:27,510
let's say 20 seconds you allow a single

00:21:25,710 --> 00:21:29,549
request to go through if that request

00:21:27,510 --> 00:21:30,870
went through successfully you make the

00:21:29,549 --> 00:21:32,549
heuristic that okay like we'll do a

00:21:30,870 --> 00:21:34,710
couple more looks good now and then you

00:21:32,549 --> 00:21:37,260
close the circuit forever so this is a

00:21:34,710 --> 00:21:39,990
really simple heuristic that helps a lot

00:21:37,260 --> 00:21:41,669
when your components are already failing

00:21:39,990 --> 00:21:43,799
you give them a little bit of room to

00:21:41,669 --> 00:21:45,750
recover if you have if you ship an

00:21:43,799 --> 00:21:47,669
unindexed curator production for example

00:21:45,750 --> 00:21:50,220
your databases are extremely overloaded

00:21:47,669 --> 00:21:52,139
and maybe your dbas can't even SSH in or

00:21:50,220 --> 00:21:54,690
something similar they can or can

00:21:52,139 --> 00:21:56,279
recover the problem in some other way so

00:21:54,690 --> 00:21:58,200
this is great it feels fast off there's

00:21:56,279 --> 00:21:59,909
several timeouts this arista cos pretty

00:21:58,200 --> 00:22:02,370
easy to understand it's reasonable in

00:21:59,909 --> 00:22:04,230
most cases the problem though is that if

00:22:02,370 --> 00:22:06,389
your timeout is high let's say your

00:22:04,230 --> 00:22:09,510
timeout is something like 20 30 seconds

00:22:06,389 --> 00:22:12,750
then this doesn't this doesn't help you

00:22:09,510 --> 00:22:14,460
quite too much into start because you

00:22:12,750 --> 00:22:16,799
will have something like three to four

00:22:14,460 --> 00:22:18,870
requests making up about a minute for

00:22:16,799 --> 00:22:20,250
all these circuits to trigger and during

00:22:18,870 --> 00:22:21,720
that time you're completely unavailable

00:22:20,250 --> 00:22:22,919
so you have one to two minutes where

00:22:21,720 --> 00:22:25,440
you're down if your timeouts are high

00:22:22,919 --> 00:22:26,909
enough or even more so we need

00:22:25,440 --> 00:22:29,279
better way to reason about how many

00:22:26,909 --> 00:22:32,370
beings are accessing a data store at

00:22:29,279 --> 00:22:34,679
once or a service and this is where

00:22:32,370 --> 00:22:36,269
bulkheads come in what you do with

00:22:34,679 --> 00:22:39,210
bulkheads is that every time you're

00:22:36,269 --> 00:22:41,610
requesting you're requesting a service

00:22:39,210 --> 00:22:44,070
or a back-end data store you have to

00:22:41,610 --> 00:22:46,470
acquire a ticket so we get a request in

00:22:44,070 --> 00:22:48,480
to one of our applications and it grabs

00:22:46,470 --> 00:22:50,039
a ticket and then cures the data store

00:22:48,480 --> 00:22:52,559
now at this time this data store has

00:22:50,039 --> 00:22:53,970
become slow and it's not responding gap

00:22:52,559 --> 00:22:57,149
back to the client a decline is now

00:22:53,970 --> 00:22:59,159
waiting for that timeout so the next the

00:22:57,149 --> 00:23:02,429
next request comes in and another ticket

00:22:59,159 --> 00:23:05,429
is taken and now and that one is hanging

00:23:02,429 --> 00:23:07,440
that request is hanging as well so when

00:23:05,429 --> 00:23:10,440
we get the third request in to the third

00:23:07,440 --> 00:23:12,029
application it's rejected right away at

00:23:10,440 --> 00:23:13,679
that layer at the ticket layer it can't

00:23:12,029 --> 00:23:17,220
acquire ticket so it's failing right

00:23:13,679 --> 00:23:19,110
away and that means we're unlocking this

00:23:17,220 --> 00:23:20,820
worker to do useful work it can now do

00:23:19,110 --> 00:23:23,100
work that's communicating with the other

00:23:20,820 --> 00:23:25,440
backends and this solves the problem

00:23:23,100 --> 00:23:26,909
from before as well with the unicorn

00:23:25,440 --> 00:23:30,029
with all these requests accessing

00:23:26,909 --> 00:23:32,100
different different different services

00:23:30,029 --> 00:23:33,720
doing the requests where you're freeing

00:23:32,100 --> 00:23:35,580
up the worker to work on useful things

00:23:33,720 --> 00:23:37,470
because you can reason about how many

00:23:35,580 --> 00:23:40,500
workers at once are curing the two

00:23:37,470 --> 00:23:43,110
different data stores and you can really

00:23:40,500 --> 00:23:45,779
think of bulkheads as a way of seeing a

00:23:43,110 --> 00:23:48,330
thread pool and many of you might

00:23:45,779 --> 00:23:49,980
already be using a thread pool and might

00:23:48,330 --> 00:23:52,259
not even have realized that it has these

00:23:49,980 --> 00:23:55,799
benefits and the great thing about it is

00:23:52,259 --> 00:23:57,419
that it ensures that I control to it

00:23:55,799 --> 00:24:00,480
ensures that you know exactly how many

00:23:57,419 --> 00:24:03,240
things are curing a back-end service at

00:24:00,480 --> 00:24:04,440
once it feels faster than the circuit

00:24:03,240 --> 00:24:06,750
breakers when the timeouts are high

00:24:04,440 --> 00:24:08,159
because in the case where I showed

00:24:06,750 --> 00:24:09,690
before if you didn't have the tickets

00:24:08,159 --> 00:24:11,100
all of them would have to trigger their

00:24:09,690 --> 00:24:12,659
circuit breakers or you would have a

00:24:11,100 --> 00:24:14,399
global circuit breaker but that has a

00:24:12,659 --> 00:24:15,870
lot of problems like synchronizing state

00:24:14,399 --> 00:24:19,559
which you have to be resilient to as

00:24:15,870 --> 00:24:21,360
well so these three patterns really

00:24:19,559 --> 00:24:23,399
timeout circuit breakers and bulkheads

00:24:21,360 --> 00:24:25,740
all complement each other really well

00:24:23,399 --> 00:24:27,509
and they all stack on top of each other

00:24:25,740 --> 00:24:30,389
and make your application even more

00:24:27,509 --> 00:24:31,980
resilient to these failures so you might

00:24:30,389 --> 00:24:34,049
ask yourself this this sounds pretty

00:24:31,980 --> 00:24:36,509
advanced do i do i really need this and

00:24:34,049 --> 00:24:38,160
you probably don't if you haven't seen

00:24:36,509 --> 00:24:41,010
these problems yet

00:24:38,160 --> 00:24:43,830
and you might not meet this but now

00:24:41,010 --> 00:24:45,360
you're equipped to to recognize these

00:24:43,830 --> 00:24:46,560
problems in production and you have the

00:24:45,360 --> 00:24:49,590
solution student and its really really

00:24:46,560 --> 00:24:51,330
easy to use but these problems didn't

00:24:49,590 --> 00:24:53,700
appear didn't surface for us for for

00:24:51,330 --> 00:24:55,200
many years but if you are seeing these

00:24:53,700 --> 00:25:00,150
problems in production I would

00:24:55,200 --> 00:25:01,890
definitely look into it so we

00:25:00,150 --> 00:25:03,390
implemented we implemented a library for

00:25:01,890 --> 00:25:05,130
those at this we call it Semyon and

00:25:03,390 --> 00:25:06,780
netflix implemented their own library

00:25:05,130 --> 00:25:08,220
which they call history and they've done

00:25:06,780 --> 00:25:10,320
some extensive documentation on

00:25:08,220 --> 00:25:12,480
resiliency twitter has a library as well

00:25:10,320 --> 00:25:14,130
and if you have a lot of services i

00:25:12,480 --> 00:25:15,390
definitely recommend looking into these

00:25:14,130 --> 00:25:18,360
libraries they have great read maize

00:25:15,390 --> 00:25:21,380
grade wikis and just all these toolkits

00:25:18,360 --> 00:25:23,520
for failing it faster than timeouts

00:25:21,380 --> 00:25:25,650
Netflix build this thing called the

00:25:23,520 --> 00:25:27,210
simian army and what the simian army

00:25:25,650 --> 00:25:29,130
does is it's a collection of these

00:25:27,210 --> 00:25:31,230
monkey scripts that kill your servers in

00:25:29,130 --> 00:25:33,420
production slowed him down artificially

00:25:31,230 --> 00:25:36,240
and in some cases kill entire regions at

00:25:33,420 --> 00:25:38,160
once this is really useful for testing

00:25:36,240 --> 00:25:39,990
and production whether all these all

00:25:38,160 --> 00:25:41,490
these all these resiliency patterns that

00:25:39,990 --> 00:25:45,390
you apply to actually work whether your

00:25:41,490 --> 00:25:47,780
fallback fullbacks work and so on so in

00:25:45,390 --> 00:25:50,310
the end you start climbing this this

00:25:47,780 --> 00:25:52,410
resiliency ladder of maturity as your

00:25:50,310 --> 00:25:53,610
application grows you start somewhere at

00:25:52,410 --> 00:25:56,250
the bottom where you haven't really done

00:25:53,610 --> 00:25:57,900
anything and slowly you're maturing your

00:25:56,250 --> 00:26:00,150
application along with the product and

00:25:57,900 --> 00:26:01,740
you start adding talk to proxy tests

00:26:00,150 --> 00:26:03,500
build out a matrix of the different

00:26:01,740 --> 00:26:05,850
dependencies you provide

00:26:03,500 --> 00:26:08,610
application-specific fallbacks like the

00:26:05,850 --> 00:26:10,380
session one I showed before or carts you

00:26:08,610 --> 00:26:14,250
start looking at the resiliency patterns

00:26:10,380 --> 00:26:16,590
for slow backends and data stores you

00:26:14,250 --> 00:26:19,080
might start doing practice days where

00:26:16,590 --> 00:26:21,300
you kill kill notes yourself and check

00:26:19,080 --> 00:26:24,240
how the entire system appears I know

00:26:21,300 --> 00:26:25,770
that Google has a like an RPG game where

00:26:24,240 --> 00:26:27,840
they sit down and have a game master

00:26:25,770 --> 00:26:29,850
that says imagine if these services

00:26:27,840 --> 00:26:31,320
failed or became slow what would happen

00:26:29,850 --> 00:26:32,910
and they sit in a room and argue about

00:26:31,320 --> 00:26:34,850
this and there are a lot of things to

00:26:32,910 --> 00:26:36,990
make a lot of ways to make this fun and

00:26:34,850 --> 00:26:38,760
once you're confident enough with died

00:26:36,990 --> 00:26:41,010
you start adopting some of these scripts

00:26:38,760 --> 00:26:43,740
in production and let them kill notes

00:26:41,010 --> 00:26:45,480
and slowed him down artificially even at

00:26:43,740 --> 00:26:46,500
3am when you're really confident and at

00:26:45,480 --> 00:26:47,880
some point you get to the point where

00:26:46,500 --> 00:26:49,760
you can kill entire data centers and

00:26:47,880 --> 00:26:54,840
you're fine

00:26:49,760 --> 00:26:58,140
so the final remarks here are I would

00:26:54,840 --> 00:26:59,880
recommend anyone to sit down and draw

00:26:58,140 --> 00:27:01,410
every Cillian C matrix for application

00:26:59,880 --> 00:27:03,090
this doesn't take more than an afternoon

00:27:01,410 --> 00:27:05,490
it's really really simple to write tests

00:27:03,090 --> 00:27:06,840
for Botox a proxy and it gives you a lot

00:27:05,490 --> 00:27:08,580
more confidence in what your

00:27:06,840 --> 00:27:11,190
applications and all its dependencies

00:27:08,580 --> 00:27:12,750
and failure patents are and not everyone

00:27:11,190 --> 00:27:14,610
needs to circuit breakers and bulkheads

00:27:12,750 --> 00:27:17,100
but certainly I think everyone can

00:27:14,610 --> 00:27:19,080
benefit from looking at fallbacks and

00:27:17,100 --> 00:27:21,990
looking at drawing out their resiliency

00:27:19,080 --> 00:27:24,270
matrix and be really careful when you're

00:27:21,990 --> 00:27:26,970
introducing new dependencies and new

00:27:24,270 --> 00:27:28,470
services because very very easily you

00:27:26,970 --> 00:27:30,510
end up building the same model if you

00:27:28,470 --> 00:27:33,630
had before with a ton of services with

00:27:30,510 --> 00:27:35,130
really rusty bagpipes in between be

00:27:33,630 --> 00:27:36,360
careful when you're introducing new

00:27:35,130 --> 00:27:37,890
services because it might actually

00:27:36,360 --> 00:27:41,370
decrease your availability in the long

00:27:37,890 --> 00:27:42,960
run and we wrote a lot more

00:27:41,370 --> 00:27:45,450
documentation on what we've learned from

00:27:42,960 --> 00:27:46,799
resiliency in the readme zuv Semyon

00:27:45,450 --> 00:27:49,679
which is the library for failing to pass

00:27:46,799 --> 00:27:52,020
in ruby we we've written a lot of

00:27:49,679 --> 00:27:53,909
documentation for tocsy proxy and we

00:27:52,020 --> 00:27:58,679
wrote I wrote a blog post as well that

00:27:53,909 --> 00:28:00,799
gives an overview like this talk thank

00:27:58,679 --> 00:28:00,799
you

00:28:17,490 --> 00:28:19,550

YouTube URL: https://www.youtube.com/watch?v=ev0KpoACieo


