Title: GORUCO 2015: Melinda Seckington: Un-Artificial Intelligence
Publication date: 2020-01-23
Playlist: GORUCO 2015
Description: 
	@mseckington

HAL, Skynet, KITTâ€¦ we've always been intrigued by artificial intelligence, but have you ever stopped to considered the un-artificial? Most developers are familiar with the basics of AI: how do you make a computer, an algorithm, a system learn something? How do you model real world problems in such a way that an artificial mind can process them? What most don't realize though is that the same principles can be applied to people. This talk looks at some of the theories behind how machines learn versus how people learn, and maps it to real life examples of how specifically our users learn their way around interfaces and how designers and developers apply learning methodologies in their day-to-day actions.

Talk given at GORUCO 2015: http://goruco.com
Captions: 
	00:00:14,160 --> 00:00:19,430
hi everyone so I'm a member and i'll be

00:00:17,310 --> 00:00:22,950
talking about on artificial intelligence

00:00:19,430 --> 00:00:25,650
so ever since the industrial revolution

00:00:22,950 --> 00:00:29,400
we've had a fascination with stories

00:00:25,650 --> 00:00:31,830
about AI just thinking of recent movies

00:00:29,400 --> 00:00:35,760
we've got all these different variations

00:00:31,830 --> 00:00:41,129
of the same theme and how does actually

00:00:35,760 --> 00:00:44,280
having a eyes change our world now I'm a

00:00:41,129 --> 00:00:46,649
huge movie geek but for my day job I'm a

00:00:44,280 --> 00:00:49,860
developer at futurelearn and we're a

00:00:46,649 --> 00:00:52,230
london-based startup and it's a social

00:00:49,860 --> 00:00:54,270
learning platform so we work together

00:00:52,230 --> 00:00:57,350
with universities and cultural

00:00:54,270 --> 00:01:00,329
institutions to deliver online courses

00:00:57,350 --> 00:01:02,789
now what that means though is that our

00:01:00,329 --> 00:01:05,789
team is encouraged to learn more about

00:01:02,789 --> 00:01:08,759
the fury and the principles of pedagogy

00:01:05,789 --> 00:01:12,810
and how to build great learning

00:01:08,759 --> 00:01:15,899
experiences now my own background is in

00:01:12,810 --> 00:01:18,929
AI I did it back in University and what

00:01:15,899 --> 00:01:21,390
I realized from learning about human

00:01:18,929 --> 00:01:24,750
learning experiences is that how

00:01:21,390 --> 00:01:27,030
machines or the artificial learn is very

00:01:24,750 --> 00:01:30,450
very similar to how people or the

00:01:27,030 --> 00:01:32,069
unofficial learn so that's what I'll be

00:01:30,450 --> 00:01:34,130
talking about here explaining some

00:01:32,069 --> 00:01:36,750
concepts from AI and link them to

00:01:34,130 --> 00:01:38,580
unofficial intelligence so before we can

00:01:36,750 --> 00:01:40,649
look at artificial and are the

00:01:38,580 --> 00:01:45,860
artificial intelligence we need to

00:01:40,649 --> 00:01:48,539
define intelligence how do we define

00:01:45,860 --> 00:01:51,300
yeah what will make something or someone

00:01:48,539 --> 00:01:53,819
intelligence so I did what every geek

00:01:51,300 --> 00:01:56,679
would do and I looked it up in the

00:01:53,819 --> 00:02:00,319
dungeons and dragons manual

00:01:56,679 --> 00:02:03,079
now the parts about wizards and spells

00:02:00,319 --> 00:02:05,390
aren't really relevant to us here but

00:02:03,079 --> 00:02:07,850
these parts are how well you character

00:02:05,390 --> 00:02:11,150
learns and reasons and having a wide

00:02:07,850 --> 00:02:14,360
assortment of skills so here's another

00:02:11,150 --> 00:02:16,430
proper definition so intelligence is not

00:02:14,360 --> 00:02:18,860
just about having the knowledge or

00:02:16,430 --> 00:02:21,620
skills it's about knowing how to obtain

00:02:18,860 --> 00:02:24,560
them how to reason about them and how to

00:02:21,620 --> 00:02:26,360
use them so what do you mean with

00:02:24,560 --> 00:02:27,739
artificial intelligence well actually we

00:02:26,360 --> 00:02:29,810
use the phrase for two different things

00:02:27,739 --> 00:02:32,690
on one hand it's the actual intelligence

00:02:29,810 --> 00:02:34,640
of machines or software but we also use

00:02:32,690 --> 00:02:36,680
a term for the actual research field

00:02:34,640 --> 00:02:39,470
looking at creating intelligence within

00:02:36,680 --> 00:02:41,120
machines and within that field we

00:02:39,470 --> 00:02:44,000
basically have four different approaches

00:02:41,120 --> 00:02:46,430
as to how to implement a I these are

00:02:44,000 --> 00:02:47,900
kind of the four main areas and today

00:02:46,430 --> 00:02:50,359
we're only looking at one of them

00:02:47,900 --> 00:02:52,519
systems that act rationally the system

00:02:50,359 --> 00:02:54,940
is rational if it does the right thing

00:02:52,519 --> 00:02:57,530
how do we define what the right thing is

00:02:54,940 --> 00:03:00,709
so this is very much the concept of

00:02:57,530 --> 00:03:02,630
intelligent intelligent agents today an

00:03:00,709 --> 00:03:04,940
intelligent agent there's one that acts

00:03:02,630 --> 00:03:08,540
to achieve the best outcome in all

00:03:04,940 --> 00:03:11,540
situations so you can create this simple

00:03:08,540 --> 00:03:14,959
law reasonably simple diagram of a

00:03:11,540 --> 00:03:17,480
simple reflex agents so an agent lives

00:03:14,959 --> 00:03:19,459
in an environment and has sensors with

00:03:17,480 --> 00:03:20,930
which it can observe the world and it

00:03:19,459 --> 00:03:23,120
has effect salesmen which can take

00:03:20,930 --> 00:03:25,400
actions on the world and it basically

00:03:23,120 --> 00:03:27,829
creates a state of what the world is

00:03:25,400 --> 00:03:31,430
like now and then has a set of if-then

00:03:27,829 --> 00:03:33,709
rules and together these two are used to

00:03:31,430 --> 00:03:36,530
determine what action the agents next

00:03:33,709 --> 00:03:38,959
takes so when we're looking at the

00:03:36,530 --> 00:03:41,359
unofficial we can apply the same diagram

00:03:38,959 --> 00:03:44,840
our sensors are basically our five

00:03:41,359 --> 00:03:48,049
senses taste touch etc and our vectors

00:03:44,840 --> 00:03:50,450
are whatever we use to take actions on

00:03:48,049 --> 00:03:53,900
the world so our voice our hands our

00:03:50,450 --> 00:03:55,790
movements and this becomes most apparent

00:03:53,900 --> 00:03:58,069
when looking at the research of puffs

00:03:55,790 --> 00:03:59,840
law so he was a Russian physiologist

00:03:58,069 --> 00:04:02,030
known for his work in classical

00:03:59,840 --> 00:04:05,180
conditioning where he basically trained

00:04:02,030 --> 00:04:07,970
dogs to associate the sound of a buzzer

00:04:05,180 --> 00:04:11,090
with food

00:04:07,970 --> 00:04:14,330
now these are my two cats Casey and

00:04:11,090 --> 00:04:16,880
dusty and they really really get

00:04:14,330 --> 00:04:19,340
annoying when they're hungry and so I

00:04:16,880 --> 00:04:23,810
Ford I try to do the same thing as per

00:04:19,340 --> 00:04:25,700
fluid could I actually train them soda

00:04:23,810 --> 00:04:27,740
started out like normal cats whenever

00:04:25,700 --> 00:04:29,750
they smelled food they jump up and

00:04:27,740 --> 00:04:32,210
rushed in kitchen knowing that they'd

00:04:29,750 --> 00:04:34,280
soon get to eat and I then started

00:04:32,210 --> 00:04:38,240
training them with a standard iphone

00:04:34,280 --> 00:04:40,880
alarm and I basically only stand up and

00:04:38,240 --> 00:04:43,340
feed them if that alarm went off and if

00:04:40,880 --> 00:04:46,880
they heard it so eventually they started

00:04:43,340 --> 00:04:48,320
associating that sound with food it was

00:04:46,880 --> 00:04:50,630
a bit of a field experiment though

00:04:48,320 --> 00:04:52,400
because it didn't mean that they stopped

00:04:50,630 --> 00:04:55,820
being hungry to wrestle the time so

00:04:52,400 --> 00:04:57,350
there is still pretty annoying but even

00:04:55,820 --> 00:05:00,380
now and it's about three four years

00:04:57,350 --> 00:05:03,110
later they'll still recognize that I

00:05:00,380 --> 00:05:05,870
phone alarm whenever it's used in a TV

00:05:03,110 --> 00:05:08,030
show or in the movie and they'll rush to

00:05:05,870 --> 00:05:11,050
the kitchen expecting that they would

00:05:08,030 --> 00:05:13,340
get food which can get pretty annoying

00:05:11,050 --> 00:05:14,950
because there's quite a lot of movies

00:05:13,340 --> 00:05:18,950
out there that still use that bloody

00:05:14,950 --> 00:05:20,750
iphone alarm so we're not that different

00:05:18,950 --> 00:05:23,470
from cats and we use these same

00:05:20,750 --> 00:05:25,760
principles on ourselves to form habits

00:05:23,470 --> 00:05:28,400
so each morning when I hear my alarm

00:05:25,760 --> 00:05:31,460
clock I know that means i need to wake

00:05:28,400 --> 00:05:33,500
up and get out of bed and as a developer

00:05:31,460 --> 00:05:36,530
I know that when I see failing tests I

00:05:33,500 --> 00:05:38,870
should fix them and and these are very

00:05:36,530 --> 00:05:41,360
very simplified loops but how do we

00:05:38,870 --> 00:05:44,150
actually learn these new rules about

00:05:41,360 --> 00:05:46,790
what actions to take how do we learn new

00:05:44,150 --> 00:05:50,030
things it's against a bit more of a

00:05:46,790 --> 00:05:51,710
complex diagram of a learning agents so

00:05:50,030 --> 00:05:54,979
just like before we have sensors and

00:05:51,710 --> 00:05:59,380
effectors in which we can observe and

00:05:54,979 --> 00:06:01,800
take action on the environments and and

00:05:59,380 --> 00:06:04,750
but when now we have a couple more

00:06:01,800 --> 00:06:07,180
elements and the main element here is

00:06:04,750 --> 00:06:09,430
the performance element and this is

00:06:07,180 --> 00:06:12,040
basically the entire agent that you just

00:06:09,430 --> 00:06:14,230
saw before it has again that bit that

00:06:12,040 --> 00:06:16,660
can create a state of the world it has

00:06:14,230 --> 00:06:18,970
if-then rules but now we have a couple

00:06:16,660 --> 00:06:21,130
more components that can influence and

00:06:18,970 --> 00:06:25,060
change what this performance element

00:06:21,130 --> 00:06:27,940
does so the main thing we're interested

00:06:25,060 --> 00:06:30,910
in is the learning elements so what this

00:06:27,940 --> 00:06:32,860
does is that based on observations that

00:06:30,910 --> 00:06:36,880
it gets from the critic about past

00:06:32,860 --> 00:06:39,700
actions and the environment it's gets

00:06:36,880 --> 00:06:41,680
feedback and then can make changes to

00:06:39,700 --> 00:06:45,270
the performance elements so this decide

00:06:41,680 --> 00:06:47,560
this changes the way we make decisions

00:06:45,270 --> 00:06:49,930
so how does it learn though what actions

00:06:47,560 --> 00:06:51,940
to what changes to make and this is

00:06:49,930 --> 00:06:54,670
where learning algorithms come in so

00:06:51,940 --> 00:06:56,890
there are a couple of known learning

00:06:54,670 --> 00:07:00,010
algorithms and only highlight a couple

00:06:56,890 --> 00:07:01,750
of the more familiar ones here and to

00:07:00,010 --> 00:07:04,030
the supervised learning and in this case

00:07:01,750 --> 00:07:06,040
the feedback is basically all up grunts

00:07:04,030 --> 00:07:08,470
so you get a bunch of label training

00:07:06,040 --> 00:07:10,780
data and basically trying to infer the

00:07:08,470 --> 00:07:13,840
rules for which input belongs to which

00:07:10,780 --> 00:07:16,720
label then it's unsupervised learning

00:07:13,840 --> 00:07:19,480
and in this case the feedback is just a

00:07:16,720 --> 00:07:22,030
bunch of data so the learning algorithm

00:07:19,480 --> 00:07:24,820
has to identify the patterns or the

00:07:22,030 --> 00:07:27,670
structure from the inputs even though

00:07:24,820 --> 00:07:29,080
there's no specific output values and

00:07:27,670 --> 00:07:31,210
then it's reinforcement learning and

00:07:29,080 --> 00:07:33,070
rather than having correctly labeled

00:07:31,210 --> 00:07:35,050
data in this case the feedback with

00:07:33,070 --> 00:07:37,510
reinforcement learning is actual proper

00:07:35,050 --> 00:07:39,430
feedback so the agent makes a decision

00:07:37,510 --> 00:07:41,950
and then get the feedback whether it's

00:07:39,430 --> 00:07:44,410
right or wrong so it's much more general

00:07:41,950 --> 00:07:45,820
but at the same time it the agent has to

00:07:44,410 --> 00:07:47,860
have a much better understanding of how

00:07:45,820 --> 00:07:50,410
the environment works and it needs

00:07:47,860 --> 00:07:54,100
something or someone to tell it whether

00:07:50,410 --> 00:07:56,110
it's right or wrong and in the same way

00:07:54,100 --> 00:07:58,540
that machines learn through different

00:07:56,110 --> 00:07:59,800
types of algorithms humans basically

00:07:58,540 --> 00:08:03,550
learned from different types of

00:07:59,800 --> 00:08:05,800
activities so these are this is an

00:08:03,550 --> 00:08:08,320
overview of the 16 different types of

00:08:05,800 --> 00:08:10,000
learning activities and on a highlight

00:08:08,320 --> 00:08:11,770
just a couple of them here because we

00:08:10,000 --> 00:08:12,249
don't have time to go over all 16 of

00:08:11,770 --> 00:08:14,349
them

00:08:12,249 --> 00:08:17,019
but you kind of but all of these

00:08:14,349 --> 00:08:19,569
different ones appear when you're trying

00:08:17,019 --> 00:08:21,339
to learn something so the first thing I

00:08:19,569 --> 00:08:23,319
want to tell about we'll talk about is

00:08:21,339 --> 00:08:26,289
delivered and in this case learners are

00:08:23,319 --> 00:08:28,209
presented with information so this is

00:08:26,289 --> 00:08:30,759
again very similar to supervised

00:08:28,209 --> 00:08:32,259
learning you're presented with the

00:08:30,759 --> 00:08:35,560
content which contains all the

00:08:32,259 --> 00:08:38,289
information that you need and as

00:08:35,560 --> 00:08:40,000
developers we do this when trying to

00:08:38,289 --> 00:08:42,219
learn something new it basically read

00:08:40,000 --> 00:08:44,529
books we watched videos we attend

00:08:42,219 --> 00:08:47,769
conferences it's all about consuming

00:08:44,529 --> 00:08:49,269
content the next one I want to talk

00:08:47,769 --> 00:08:51,759
about is conversational and

00:08:49,269 --> 00:08:53,860
collaborative so here it's about

00:08:51,759 --> 00:08:56,410
learning through conversing with others

00:08:53,860 --> 00:08:58,870
and by constructing a shared

00:08:56,410 --> 00:09:01,029
understanding so this is very much like

00:08:58,870 --> 00:09:03,399
unsupervised learning where the learning

00:09:01,029 --> 00:09:06,490
comes from the structure of what is

00:09:03,399 --> 00:09:08,170
being learned and again it's developers

00:09:06,490 --> 00:09:10,269
we do this when pairing with others

00:09:08,170 --> 00:09:12,970
together we form a share it on the

00:09:10,269 --> 00:09:15,279
standing of what we're working on and

00:09:12,970 --> 00:09:17,920
then it's assessing and in this case is

00:09:15,279 --> 00:09:19,629
about receiving constructive feedback so

00:09:17,920 --> 00:09:22,000
this is very much like reinforcement

00:09:19,629 --> 00:09:24,939
learning we get the feedback and from

00:09:22,000 --> 00:09:26,259
the feedback you learn and as developers

00:09:24,939 --> 00:09:28,059
RIA gehen do the same thing basically

00:09:26,259 --> 00:09:31,180
the pull request we learn from the

00:09:28,059 --> 00:09:33,759
feedback we get from others so again the

00:09:31,180 --> 00:09:35,709
overview of 16 of the learning

00:09:33,759 --> 00:09:37,779
activities and I just wanted you all to

00:09:35,709 --> 00:09:39,910
take a very brief moment to just think

00:09:37,779 --> 00:09:43,779
about the last time you actively learned

00:09:39,910 --> 00:09:45,759
something when was the last time what

00:09:43,779 --> 00:09:49,149
what type of activity activities did you

00:09:45,759 --> 00:09:52,209
actually do so what makes us different

00:09:49,149 --> 00:09:55,360
from machines well for starters we are

00:09:52,209 --> 00:09:58,240
very contextual we know what we're

00:09:55,360 --> 00:09:59,800
learning in what situation unlike

00:09:58,240 --> 00:10:02,350
machines we're not really bound to one

00:09:59,800 --> 00:10:03,850
purpose or one domain we're then

00:10:02,350 --> 00:10:06,100
constantly learning we don't have an off

00:10:03,850 --> 00:10:07,720
switch for learning we might not be

00:10:06,100 --> 00:10:09,459
consciously learning new skills or

00:10:07,720 --> 00:10:11,589
knowledge but we process everything

00:10:09,459 --> 00:10:14,620
that's around us whereas machines are

00:10:11,589 --> 00:10:17,199
very much safe based and then probably

00:10:14,620 --> 00:10:18,910
Nora knowledge we have a huge backlog of

00:10:17,199 --> 00:10:20,529
other things we know and we can make

00:10:18,910 --> 00:10:23,190
associations between the different types

00:10:20,529 --> 00:10:24,450
of information we find but

00:10:23,190 --> 00:10:27,480
this is something that not necessarily

00:10:24,450 --> 00:10:29,100
other people might have picked up on and

00:10:27,480 --> 00:10:31,740
next to that we are emotional we attach

00:10:29,100 --> 00:10:34,200
value to certain skills information and

00:10:31,740 --> 00:10:37,410
experiences and we need that emotion to

00:10:34,200 --> 00:10:40,320
make proper decisions and finally with

00:10:37,410 --> 00:10:42,630
social we've learned from others and we

00:10:40,320 --> 00:10:46,100
need that social interaction like here

00:10:42,630 --> 00:10:48,990
at a conference to gain better learning

00:10:46,100 --> 00:10:50,790
so we started creating machines that

00:10:48,990 --> 00:10:53,640
have all these different abilities but

00:10:50,790 --> 00:10:55,560
not all of them combined and machines

00:10:53,640 --> 00:10:58,290
need to be able to do all of these in a

00:10:55,560 --> 00:11:01,590
generalized way before we consider them

00:10:58,290 --> 00:11:03,780
learning the way we as humans do and I

00:11:01,590 --> 00:11:05,850
don't think right at parole I'm not the

00:11:03,780 --> 00:11:07,980
first to say this but I think we will

00:11:05,850 --> 00:11:10,890
have artificial intelligence within this

00:11:07,980 --> 00:11:12,990
century and that's not going to be the

00:11:10,890 --> 00:11:15,030
type of AI that is pulling out downfall

00:11:12,990 --> 00:11:17,580
and causing the end of the world but

00:11:15,030 --> 00:11:20,070
rather will have machines that can learn

00:11:17,580 --> 00:11:22,740
and reason about skills and knowledge as

00:11:20,070 --> 00:11:24,990
humans do and I think that's when things

00:11:22,740 --> 00:11:28,050
really get interesting because in a

00:11:24,990 --> 00:11:30,660
world where humans and machines learn

00:11:28,050 --> 00:11:33,960
the same way does that mean they'll

00:11:30,660 --> 00:11:35,790
learn together well as schools become

00:11:33,960 --> 00:11:39,090
places where both humans and machines

00:11:35,790 --> 00:11:42,030
learn so when we're thinking of web

00:11:39,090 --> 00:11:45,150
development of the future think about

00:11:42,030 --> 00:11:48,890
this well what we develop for humans

00:11:45,150 --> 00:11:51,990
also actually be used for machines and

00:11:48,890 --> 00:11:57,440
what we develop for humans us actually

00:11:51,990 --> 00:11:57,440
work for machines thanks for listening

00:12:06,480 --> 00:12:08,540
you

00:12:16,200 --> 00:12:18,260

YouTube URL: https://www.youtube.com/watch?v=7Y1Bv2BJDLs


