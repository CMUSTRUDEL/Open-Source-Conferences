Title: GRCon19 - Streaming with DPDK: Raising the Throughput Ceiling with Drivers in Use.. by Alex Williams
Publication date: 2019-11-05
Playlist: GRCon 2019
Description: 
	Streaming with DPDK: Raising the Throughput Ceiling with Drivers in Use Space by Alex Williams

Software latency is the biggest impediment to high-speed streaming from a computer, and expensive kernel interactions are the primary source. System calls and buffer copying add to processing time, and the kernel’s scheduler can cause large latency spikes. Consequently, engineers have compensated by adding large buffers to the other side. Is there another way?

Enter projects like the Data Plane Development Kit (DPDK). By implementing the driver outside the kernel, DPDK enables our applications to reduce processing and take control of scheduling. In this presentation, we look at how DPDK addresses the latency problem and how we used the technology in UHD to create our own network stack and blast through our throughput ceiling.

Why now? The recently-released N320 offers the widest per-channel bandwidth of any USRP developed so far. At 250 MSPS per channel, the stream could not be sustained with our previous architecture’s throughput over the 10 GbE link. With technologies like DPDK, we can enable higher, more reliable throughput and spend fewer resources on buffering for those high-bandwidth applications. However, for GNU Radio to access this level of performance, it will necessitate a more efficient use of resources, including a revamp of the scheduler to use some of the same ideas.
Captions: 
	00:00:15,100 --> 00:00:19,540
okay so I'm Alex Williams I work for

00:00:18,520 --> 00:00:22,480
national assurance as a software

00:00:19,540 --> 00:00:24,190
engineer so one of the things that we're

00:00:22,480 --> 00:00:26,440
all constantly pursuing is higher

00:00:24,190 --> 00:00:28,420
bandwidth and it's starting to get

00:00:26,440 --> 00:00:33,489
rather difficult to move that data

00:00:28,420 --> 00:00:35,940
reliably to our our systems so one of

00:00:33,489 --> 00:00:38,230
the things that I've been working on and

00:00:35,940 --> 00:00:40,360
I've been studying is like how do we

00:00:38,230 --> 00:00:43,900
move this data across it in that fashion

00:00:40,360 --> 00:00:45,550
fashion and it turns out that a lot of

00:00:43,900 --> 00:00:49,780
our interactions with the kernel are

00:00:45,550 --> 00:00:53,769
kind of slowing us up and so what I've

00:00:49,780 --> 00:00:57,400
done is taken a framework called DB DK

00:00:53,769 --> 00:00:58,989
the data plane development kit and used

00:00:57,400 --> 00:01:02,220
it to help accelerate our network

00:00:58,989 --> 00:01:05,560
processing with drivers and user space

00:01:02,220 --> 00:01:10,170
so I should say that I I work a lot with

00:01:05,560 --> 00:01:13,450
UHD and so a lot of the examples and

00:01:10,170 --> 00:01:17,290
what I've done all pertains to it but

00:01:13,450 --> 00:01:18,820
the challenges and the strategies for

00:01:17,290 --> 00:01:21,850
handling them kind of form design

00:01:18,820 --> 00:01:24,460
patterns that are common to other places

00:01:21,850 --> 00:01:28,960
and so for like gee the goon or radio

00:01:24,460 --> 00:01:32,460
scheduler similar ideas will help out so

00:01:28,960 --> 00:01:35,229
what I'll do first is I'll talk about

00:01:32,460 --> 00:01:37,960
how the user stream in network mode now

00:01:35,229 --> 00:01:40,420
and what's kind of limiting us then

00:01:37,960 --> 00:01:43,600
we'll talk about some technologies to

00:01:40,420 --> 00:01:45,640
help get around that and we'll introduce

00:01:43,600 --> 00:01:48,340
DP DK talk about the work that I've done

00:01:45,640 --> 00:01:53,320
benchmark results and the impact on

00:01:48,340 --> 00:01:56,200
gonna radio so how does UHD currently

00:01:53,320 --> 00:02:00,189
work so basically the architecture is

00:01:56,200 --> 00:02:03,310
kind of like this you've got your your

00:02:00,189 --> 00:02:05,710
radio right there it's receiving its

00:02:03,310 --> 00:02:09,490
data or 10 Gigabit Ethernet and the

00:02:05,710 --> 00:02:11,380
application is sending it to the

00:02:09,490 --> 00:02:13,330
streamer api's which there's like a

00:02:11,380 --> 00:02:15,160
lower-level one where we get a transport

00:02:13,330 --> 00:02:17,860
and we have to talk to the kernel to go

00:02:15,160 --> 00:02:19,360
over since you know transmissions are

00:02:17,860 --> 00:02:21,160
kind of bursty we'll have a packet

00:02:19,360 --> 00:02:23,350
buffer to smooth that out and the radio

00:02:21,160 --> 00:02:27,400
will consume it at kind of a constant

00:02:23,350 --> 00:02:28,459
rate so it works kind of in software

00:02:27,400 --> 00:02:31,450
then

00:02:28,459 --> 00:02:34,310
to make this all kind of happen right

00:02:31,450 --> 00:02:38,510
for RTX you'd have your buffer you'd

00:02:34,310 --> 00:02:43,099
create your samples you'd then maybe

00:02:38,510 --> 00:02:47,060
keep your buffer over and wait for UHT

00:02:43,099 --> 00:02:48,799
to do its thing you HD will take that

00:02:47,060 --> 00:02:51,049
buffer or will look to see is there

00:02:48,799 --> 00:02:53,360
space in that and the packets at the

00:02:51,049 --> 00:02:54,920
buffer downstream and if there is oh I

00:02:53,360 --> 00:02:56,840
can send it out I'll hand it off to the

00:02:54,920 --> 00:02:58,760
colonel and finally it gets out and

00:02:56,840 --> 00:03:02,629
there's a number of copies in there on

00:02:58,760 --> 00:03:04,430
its way to its destination so this it

00:03:02,629 --> 00:03:11,060
kind of takes a bit to get to where we

00:03:04,430 --> 00:03:15,379
need it to go so in terms of like time

00:03:11,060 --> 00:03:17,709
and how things work you HD and you keep

00:03:15,379 --> 00:03:23,180
in mind uses credit based flow control

00:03:17,709 --> 00:03:24,799
so as we tried to send packets down we

00:03:23,180 --> 00:03:28,730
have to get a message back that says

00:03:24,799 --> 00:03:31,640
okay there's space you can send more to

00:03:28,730 --> 00:03:35,030
the packet buffer so in a way that miss

00:03:31,640 --> 00:03:37,430
might work is you would add in it kind

00:03:35,030 --> 00:03:40,310
of learn okay my buffer size is this

00:03:37,430 --> 00:03:43,549
much downstream I can use this much of

00:03:40,310 --> 00:03:45,500
it I'll send my command to start

00:03:43,549 --> 00:03:47,660
streaming sometime in the future I'll

00:03:45,500 --> 00:03:49,940
fill the buffer and then the radio can

00:03:47,660 --> 00:03:51,799
start started streaming and you'll see

00:03:49,940 --> 00:03:56,780
it kind of does it at a constant rate

00:03:51,799 --> 00:03:58,340
and just takes it out meanwhile so the

00:03:56,780 --> 00:04:00,680
radio and the packet buffer will send

00:03:58,340 --> 00:04:01,040
its flow controller response will handle

00:04:00,680 --> 00:04:05,269
it

00:04:01,040 --> 00:04:06,769
it'll HD will say okay I can send more

00:04:05,269 --> 00:04:09,049
there's more there and hopefully it'll

00:04:06,769 --> 00:04:11,930
arrive in time so that we don't get it

00:04:09,049 --> 00:04:14,269
under run so with this you can kind of

00:04:11,930 --> 00:04:17,620
see we need enough buffering to cover

00:04:14,269 --> 00:04:17,620
whatever is the worst case latency

00:04:20,500 --> 00:04:24,949
unfortunately transmissions are kind of

00:04:23,300 --> 00:04:27,740
bursty and we don't have complete

00:04:24,949 --> 00:04:30,380
control over the whole machine so you

00:04:27,740 --> 00:04:31,880
can get a lot of jitter induced by the

00:04:30,380 --> 00:04:36,080
scheduler and other things that are

00:04:31,880 --> 00:04:38,539
happening at the time and for example

00:04:36,080 --> 00:04:40,099
it's kind of a cheesy one but you could

00:04:38,539 --> 00:04:42,110
say well what if we got this flow

00:04:40,099 --> 00:04:42,540
control response and oh the OS need to

00:04:42,110 --> 00:04:44,480
check for

00:04:42,540 --> 00:04:47,070
up days and we just can't do our thing

00:04:44,480 --> 00:04:49,050
you love what you see is we couldn't

00:04:47,070 --> 00:04:51,150
actually process that response in time

00:04:49,050 --> 00:04:52,950
and then we couldn't send the packet

00:04:51,150 --> 00:04:55,470
until it was too late got or underflow

00:04:52,950 --> 00:04:59,670
and then we had some junk send out over

00:04:55,470 --> 00:05:01,470
the radio so one way to kind of help

00:04:59,670 --> 00:05:02,610
with this would be to say well okay it's

00:05:01,470 --> 00:05:04,470
just huh

00:05:02,610 --> 00:05:05,700
just need more buffers and need to fill

00:05:04,470 --> 00:05:08,700
it up more but there's a cost to that

00:05:05,700 --> 00:05:12,690
you end up with higher delays then all

00:05:08,700 --> 00:05:17,940
the time to get to your radio and

00:05:12,690 --> 00:05:20,220
there's more hardware required so let's

00:05:17,940 --> 00:05:22,440
take a little bit of a deeper look at

00:05:20,220 --> 00:05:26,940
like where's all this coming from what's

00:05:22,440 --> 00:05:28,470
happening with scheduling so for a

00:05:26,940 --> 00:05:31,320
scheduler you have like some set of

00:05:28,470 --> 00:05:33,270
tasks and a queue and it'll be there

00:05:31,320 --> 00:05:34,830
trying to decide what should I run on

00:05:33,270 --> 00:05:36,420
all these cores that I've got and

00:05:34,830 --> 00:05:37,860
there's various metrics for talking

00:05:36,420 --> 00:05:40,890
about how it performs

00:05:37,860 --> 00:05:44,220
there's fairness which is like how

00:05:40,890 --> 00:05:49,140
equally am i sharing it and then latency

00:05:44,220 --> 00:05:51,300
to like get to the end of your task and

00:05:49,140 --> 00:05:54,060
threads can be pulled off a core in

00:05:51,300 --> 00:05:56,730
various ways one of them would be

00:05:54,060 --> 00:05:58,760
voluntarily where it just sort of needs

00:05:56,730 --> 00:06:01,470
to wait on something and it says okay

00:05:58,760 --> 00:06:03,510
I'm not running here so something else

00:06:01,470 --> 00:06:06,210
can go there another one that might be

00:06:03,510 --> 00:06:07,590
involuntarily where the scheduler maybe

00:06:06,210 --> 00:06:12,210
has a timer interrupt and says yep

00:06:07,590 --> 00:06:14,430
Europe you can't run anymore and both of

00:06:12,210 --> 00:06:17,940
those cases though you can end up with a

00:06:14,430 --> 00:06:21,000
situation where you don't get back on

00:06:17,940 --> 00:06:23,510
your core when you're ready because

00:06:21,000 --> 00:06:25,940
you're waiting for that other thread

00:06:23,510 --> 00:06:29,190
you're waiting for that other thread to

00:06:25,940 --> 00:06:33,390
to come off first and so there's always

00:06:29,190 --> 00:06:35,220
these delays then for your responses so

00:06:33,390 --> 00:06:38,130
one thing to also keep in mind then is

00:06:35,220 --> 00:06:40,980
that Linux is default scheduler is the

00:06:38,130 --> 00:06:43,830
completely fair scheduler so it's made

00:06:40,980 --> 00:06:45,540
to share a CPU core relatively fairly

00:06:43,830 --> 00:06:49,470
they say completely fair but that's not

00:06:45,540 --> 00:06:51,150
exactly true and it doesn't really know

00:06:49,470 --> 00:06:54,330
what you're trying to do it kind of

00:06:51,150 --> 00:06:55,660
guessing and as a result your network

00:06:54,330 --> 00:06:57,280
activity your

00:06:55,660 --> 00:06:58,720
you're processing it might not

00:06:57,280 --> 00:07:00,700
understand that this needs to be higher

00:06:58,720 --> 00:07:05,590
priority and when the event comes in it

00:07:00,700 --> 00:07:07,810
should schedule it right away and that's

00:07:05,590 --> 00:07:09,220
kind of an explanation of why that

00:07:07,810 --> 00:07:11,140
thread that's waiting for the flow

00:07:09,220 --> 00:07:13,650
control response it might then wait to

00:07:11,140 --> 00:07:17,350
to get to it

00:07:13,650 --> 00:07:19,600
on top of that there can be issues where

00:07:17,350 --> 00:07:21,730
you have an application that creates a

00:07:19,600 --> 00:07:24,010
bunch of threads but there aren't enough

00:07:21,730 --> 00:07:29,530
course to handle it like sagen your

00:07:24,010 --> 00:07:31,900
radio you a UHD also actually creates a

00:07:29,530 --> 00:07:33,460
bunch of threads too and you kind of see

00:07:31,900 --> 00:07:34,780
there a list of some of them that it has

00:07:33,460 --> 00:07:36,760
and most of these spend their time

00:07:34,780 --> 00:07:39,310
sleeping but they do have to wake up

00:07:36,760 --> 00:07:41,680
periodically and do their thing and so

00:07:39,310 --> 00:07:43,150
every time you have another thread that

00:07:41,680 --> 00:07:47,260
need to do something it competes for

00:07:43,150 --> 00:07:50,080
that core time and it also invites the

00:07:47,260 --> 00:07:51,820
scheduler to come in and say I couldn't

00:07:50,080 --> 00:07:53,650
let you run right now let's just let

00:07:51,820 --> 00:07:58,030
this other thing happen and it doesn't

00:07:53,650 --> 00:08:00,100
know okay what's high-priority again the

00:07:58,030 --> 00:08:05,710
kernel defaults are just not optimized

00:08:00,100 --> 00:08:07,930
for real-time systems right here that's

00:08:05,710 --> 00:08:12,330
kind of gives you an idea of how well we

00:08:07,930 --> 00:08:16,660
do with the defaults that we've got and

00:08:12,330 --> 00:08:18,490
the usual kernel based Network stack and

00:08:16,660 --> 00:08:22,240
you can see that we're kind of hit

00:08:18,490 --> 00:08:26,200
roughly half of the best performance for

00:08:22,240 --> 00:08:29,290
like in three twenty four th so how can

00:08:26,200 --> 00:08:32,830
we do better this can't really be all we

00:08:29,290 --> 00:08:35,050
can do right well there are various

00:08:32,830 --> 00:08:38,140
technologies out there that give us a

00:08:35,050 --> 00:08:40,210
way to inform the colonel that of what

00:08:38,140 --> 00:08:43,479
it is we're trying to do and maybe take

00:08:40,210 --> 00:08:45,040
more control back from it so there are

00:08:43,479 --> 00:08:46,990
ways to a lot of them are called kernel

00:08:45,040 --> 00:08:51,940
bypass but sometimes it's actually just

00:08:46,990 --> 00:08:57,040
a way to work together with what's there

00:08:51,940 --> 00:08:58,420
so a strategy that we might have okay we

00:08:57,040 --> 00:09:00,790
have all this context switching

00:08:58,420 --> 00:09:02,740
happening because we're doing Nick

00:09:00,790 --> 00:09:04,990
transactions and used to go into BSD

00:09:02,740 --> 00:09:05,620
sockets and go into the kernel driver

00:09:04,990 --> 00:09:07,570
and she

00:09:05,620 --> 00:09:09,490
at all and we don't need to do that

00:09:07,570 --> 00:09:11,710
maybe we can take control of the knick

00:09:09,490 --> 00:09:15,900
and and just control it from our

00:09:11,710 --> 00:09:18,580
application and user space also we could

00:09:15,900 --> 00:09:20,500
inform it of what's a high priority and

00:09:18,580 --> 00:09:24,010
what core is maybe we should use for

00:09:20,500 --> 00:09:27,279
certain things and so there are actually

00:09:24,010 --> 00:09:30,850
API to do all this we have the affinity

00:09:27,279 --> 00:09:32,950
control API is where you can say there's

00:09:30,850 --> 00:09:36,400
a mask that you have you can run a

00:09:32,950 --> 00:09:39,390
particular thread on the core and you

00:09:36,400 --> 00:09:41,820
can say nothing else should run on that

00:09:39,390 --> 00:09:44,920
and then you have high priority

00:09:41,820 --> 00:09:47,470
scheduling classes so anything in this

00:09:44,920 --> 00:09:49,720
class will run above anything that

00:09:47,470 --> 00:09:52,210
happens in in the completely fair

00:09:49,720 --> 00:09:54,370
scheduler class it says those two

00:09:52,210 --> 00:09:56,980
together you can effectively take

00:09:54,370 --> 00:09:59,440
control of CPU cores there's another

00:09:56,980 --> 00:10:03,550
thing on top where you have like a boot

00:09:59,440 --> 00:10:05,830
argument and you can isolate a CPU where

00:10:03,550 --> 00:10:11,410
the mask only matches that one CPU and

00:10:05,830 --> 00:10:13,930
that goes even further so that helps us

00:10:11,410 --> 00:10:15,730
take control of scheduling then there's

00:10:13,930 --> 00:10:19,450
the drivers and user space part and we

00:10:15,730 --> 00:10:21,480
have a few technologies here one is the

00:10:19,450 --> 00:10:24,700
user space i/o and with this you can

00:10:21,480 --> 00:10:27,040
make available like your device

00:10:24,700 --> 00:10:29,260
registers and user space it can do GMA

00:10:27,040 --> 00:10:31,089
you can receive interrupts the only

00:10:29,260 --> 00:10:34,240
downside is that there aren't really any

00:10:31,089 --> 00:10:37,570
protections for where that DMA is going

00:10:34,240 --> 00:10:39,760
to and so they came up then with came up

00:10:37,570 --> 00:10:42,279
with virtual function IO which then

00:10:39,760 --> 00:10:44,500
brings in an iommu if your system

00:10:42,279 --> 00:10:46,150
supports it and that will provide like

00:10:44,500 --> 00:10:50,170
page mappings so that you can't go to

00:10:46,150 --> 00:10:52,930
places that it shouldn't and then

00:10:50,170 --> 00:10:54,640
there's V fi o.4 mediated devices and

00:10:52,930 --> 00:10:58,420
this was kind of special it's for a case

00:10:54,640 --> 00:11:00,310
where you might have a device that you

00:10:58,420 --> 00:11:03,250
might want some of your control to still

00:11:00,310 --> 00:11:06,100
remain in the kernel but maybe you want

00:11:03,250 --> 00:11:10,180
to give applications access to DMA

00:11:06,100 --> 00:11:11,980
engines and that would be useful for if

00:11:10,180 --> 00:11:14,440
you have say multiple applications that

00:11:11,980 --> 00:11:16,030
are kind of trying to share a device or

00:11:14,440 --> 00:11:17,860
if you have

00:11:16,030 --> 00:11:19,300
you want to give out DMA engines to

00:11:17,860 --> 00:11:20,770
different VMs

00:11:19,300 --> 00:11:25,510
that's specifically what that was

00:11:20,770 --> 00:11:29,350
actually made for so if we combine all

00:11:25,510 --> 00:11:31,390
these things together and get some

00:11:29,350 --> 00:11:33,610
secret sauce from Intel it Mellanox and

00:11:31,390 --> 00:11:35,800
some of the others you come up with data

00:11:33,610 --> 00:11:38,230
playing development kit and what this is

00:11:35,800 --> 00:11:41,470
is sort of a snit working framework for

00:11:38,230 --> 00:11:43,120
a fast packet processing it kind of

00:11:41,470 --> 00:11:45,490
provides the raw interface to all the

00:11:43,120 --> 00:11:49,330
network devices so you don't get all the

00:11:45,490 --> 00:11:53,550
niceties like TCP and UDP and even ARP

00:11:49,330 --> 00:11:56,710
that's all stuff that I had to implement

00:11:53,550 --> 00:12:00,100
but it doesn't mean that you get to make

00:11:56,710 --> 00:12:02,620
it as small as you need and I have to do

00:12:00,100 --> 00:12:04,390
only the things that you want on top of

00:12:02,620 --> 00:12:06,850
that there are some other niceties so

00:12:04,390 --> 00:12:10,830
they have and some API is to interact

00:12:06,850 --> 00:12:10,830
with those affinity control and

00:12:10,890 --> 00:12:15,910
real-time scheduling classes I mentioned

00:12:13,420 --> 00:12:17,770
earlier and some other like data

00:12:15,910 --> 00:12:19,900
structures and memory managers and

00:12:17,770 --> 00:12:23,340
things of that sort to keep you from

00:12:19,900 --> 00:12:23,340
sleeping if that's what you want to do

00:12:23,370 --> 00:12:29,680
and lastly on that note it is fairly

00:12:27,790 --> 00:12:32,560
widely supported you do need a NIC that

00:12:29,680 --> 00:12:34,930
is has a driver for it but it's not hard

00:12:32,560 --> 00:12:36,640
to find and the biggest ones are gonna

00:12:34,930 --> 00:12:42,010
be Intel Mellanox but there's other

00:12:36,640 --> 00:12:45,280
vendors too on top of that ok so then

00:12:42,010 --> 00:12:48,400
the work that I did on top it's called

00:12:45,280 --> 00:12:52,840
you HTTP TK but it's a special name

00:12:48,400 --> 00:12:54,370
there so you can see that it was a

00:12:52,840 --> 00:12:57,010
conventional stack you end up having

00:12:54,370 --> 00:12:58,570
just UHD there and then there's a whole

00:12:57,010 --> 00:13:00,610
bunch that's implemented in the kernel

00:12:58,570 --> 00:13:04,780
before it gets the network card before

00:13:00,610 --> 00:13:07,030
you HTTP TK we have our driver and user

00:13:04,780 --> 00:13:10,150
space and it's only for an it just to

00:13:07,030 --> 00:13:11,410
set up your your memory mappings that it

00:13:10,150 --> 00:13:13,330
goes to the kernel the rest of the time

00:13:11,410 --> 00:13:17,320
it goes directly to the network card and

00:13:13,330 --> 00:13:19,150
you have much reduced latency I did an

00:13:17,320 --> 00:13:21,220
old minimal network stack on top

00:13:19,150 --> 00:13:24,700
I'd only supports exactly what we need

00:13:21,220 --> 00:13:27,490
which is UDP and ARP and IP and we had

00:13:24,700 --> 00:13:28,650
some zero copy operation where we would

00:13:27,490 --> 00:13:32,100
only pass along

00:13:28,650 --> 00:13:33,450
pointers to dia Mabel buffers and I cut

00:13:32,100 --> 00:13:37,820
out that copy that you would have

00:13:33,450 --> 00:13:37,820
between the colonel and your application

00:13:38,300 --> 00:13:44,190
in terms of scheduling then we had to

00:13:41,250 --> 00:13:46,020
come up with a better way to to figure

00:13:44,190 --> 00:13:50,310
out how to show that our networking

00:13:46,020 --> 00:13:51,990
activity was high priority so we have a

00:13:50,310 --> 00:13:55,560
little threading model here where we can

00:13:51,990 --> 00:13:57,570
put certain ports on a configurable

00:13:55,560 --> 00:14:00,180
number of i/o threads and then we'll

00:13:57,570 --> 00:14:03,060
assign IO threads to specific cores and

00:14:00,180 --> 00:14:06,000
that IO thread will take it 100% that

00:14:03,060 --> 00:14:07,980
CPU will never sleep but as a

00:14:06,000 --> 00:14:09,780
consequence you will be able to react to

00:14:07,980 --> 00:14:16,200
anything that comes in as fast as

00:14:09,780 --> 00:14:18,420
possible and so this replaces sort of

00:14:16,200 --> 00:14:20,070
the UDP transport with kind of our

00:14:18,420 --> 00:14:21,270
cheddar streaming layered on top and

00:14:20,070 --> 00:14:23,610
there were no changes at all of the

00:14:21,270 --> 00:14:26,040
streamer API you can use this with just

00:14:23,610 --> 00:14:27,600
a configuration file that describes the

00:14:26,040 --> 00:14:30,180
things that you would have in like

00:14:27,600 --> 00:14:32,880
network manager because we can't use the

00:14:30,180 --> 00:14:34,620
system utilities to assign IP addresses

00:14:32,880 --> 00:14:37,230
anymore we have to have it in our our

00:14:34,620 --> 00:14:39,270
own configuration file and the little

00:14:37,230 --> 00:14:42,330
device argument and then it's right

00:14:39,270 --> 00:14:43,920
there and available to use including

00:14:42,330 --> 00:14:49,220
your own configuration for which cores

00:14:43,920 --> 00:14:51,420
you want to actually use so performance

00:14:49,220 --> 00:14:53,190
well we could achieve full rate

00:14:51,420 --> 00:14:56,550
streaming to in 50 mega samples per

00:14:53,190 --> 00:15:01,170
second two channels to each device no

00:14:56,550 --> 00:15:03,090
problem we ran a little bit of a test to

00:15:01,170 --> 00:15:05,880
show kind of what the difference was in

00:15:03,090 --> 00:15:08,120
the latency so we measure this from the

00:15:05,880 --> 00:15:11,640
output of the our export on the radio

00:15:08,120 --> 00:15:15,390
going back to the device and then

00:15:11,640 --> 00:15:18,060
sending a TX packet out and getting it

00:15:15,390 --> 00:15:21,650
measuring at the point on the TX part of

00:15:18,060 --> 00:15:25,650
the radio and for the using the kernel

00:15:21,650 --> 00:15:27,450
I'd be about 300 microseconds DVD K we

00:15:25,650 --> 00:15:29,250
reduced it all the way down to 80 and

00:15:27,450 --> 00:15:32,250
keep in mind this is also going through

00:15:29,250 --> 00:15:35,570
the DDC and ID you see and a bunch of

00:15:32,250 --> 00:15:35,570
other blocks on its way there

00:15:37,070 --> 00:15:41,770
that gets much better so for good new

00:15:39,140 --> 00:15:45,470
radio then we'll add that on top and

00:15:41,770 --> 00:15:48,740
performance improves but it turns out

00:15:45,470 --> 00:15:51,800
radio has some bottlenecks so for

00:15:48,740 --> 00:15:54,110
similar work these are two different

00:15:51,800 --> 00:15:57,050
examples that are basically signal

00:15:54,110 --> 00:15:58,850
generators you can see that would get

00:15:57,050 --> 00:16:00,620
kind of reduced performance for it so

00:15:58,850 --> 00:16:03,980
the X waveforms sorry I should say is

00:16:00,620 --> 00:16:06,290
the example that's just UHD only UHD SiC

00:16:03,980 --> 00:16:11,110
Jin is the one that uses good new radio

00:16:06,290 --> 00:16:13,910
to implement it was a cig source block

00:16:11,110 --> 00:16:15,140
what happens is it kind of gets reduced

00:16:13,910 --> 00:16:18,920
in performance because we ended up

00:16:15,140 --> 00:16:26,000
making a ton of threads and you get a

00:16:18,920 --> 00:16:28,160
lot of migration between cores yeah so

00:16:26,000 --> 00:16:30,950
the thread per block scheduler in

00:16:28,160 --> 00:16:32,180
particular may be a likely bottleneck I

00:16:30,950 --> 00:16:35,780
know we've been talking about this a lot

00:16:32,180 --> 00:16:37,250
already during this conference but it

00:16:35,780 --> 00:16:40,730
does replicate more threads than

00:16:37,250 --> 00:16:42,730
available CPU cores we get kind of a lot

00:16:40,730 --> 00:16:46,940
of switching a lot of threads moving

00:16:42,730 --> 00:16:49,480
around because it just assigns threads

00:16:46,940 --> 00:16:52,460
discuss what that is about signs threads

00:16:49,480 --> 00:16:56,210
whenever a core is available and then

00:16:52,460 --> 00:16:58,910
you end up with cold caches cold like

00:16:56,210 --> 00:17:00,170
branch predictor is called TLB ease all

00:16:58,910 --> 00:17:05,030
these sorts of things that then have to

00:17:00,170 --> 00:17:07,040
be loaded all over again and then kind

00:17:05,030 --> 00:17:09,650
of funky some of these blocks have

00:17:07,040 --> 00:17:11,839
really tiny workloads and they still get

00:17:09,650 --> 00:17:14,600
their own threads and all this freaking

00:17:11,839 --> 00:17:17,480
switching kind of just invites latency

00:17:14,600 --> 00:17:19,280
spikes so what are the things we can do

00:17:17,480 --> 00:17:22,670
about it well Marcus actually went over

00:17:19,280 --> 00:17:23,990
a lot of this already but you know we

00:17:22,670 --> 00:17:25,880
could partition the graph but of course

00:17:23,990 --> 00:17:27,589
our grind your look granularity maybe

00:17:25,880 --> 00:17:30,740
let's not have so many threads it's just

00:17:27,589 --> 00:17:33,140
not useful and then we could take those

00:17:30,740 --> 00:17:35,120
sub graphs and put them you know on

00:17:33,140 --> 00:17:39,170
individual threads assign them to cores

00:17:35,120 --> 00:17:40,760
maybe we do some pinning there's some

00:17:39,170 --> 00:17:42,500
ability here to auto adapt but we should

00:17:40,760 --> 00:17:44,750
probably always have a way for the user

00:17:42,500 --> 00:17:48,970
to specify because nobody knows the

00:17:44,750 --> 00:17:48,970
application better than the user of them

00:17:49,559 --> 00:17:53,710
and this question slide I had here but I

00:17:52,570 --> 00:17:55,990
think has become pretty clear as we

00:17:53,710 --> 00:17:59,500
talked that yes we are interested in the

00:17:55,990 --> 00:18:02,590
higher data rates and in terms of

00:17:59,500 --> 00:18:04,660
handling processing GPUs and FPGAs and

00:18:02,590 --> 00:18:06,040
the whole heterogeneous computing thing

00:18:04,660 --> 00:18:11,530
is very important to us and we'll keep

00:18:06,040 --> 00:18:14,470
working on it so any questions so is

00:18:11,530 --> 00:18:16,590
this something I can download now it's

00:18:14,470 --> 00:18:18,490
available now and if so are there

00:18:16,590 --> 00:18:22,240
application notes or something on how I

00:18:18,490 --> 00:18:25,330
can implement it into your HDMI machine

00:18:22,240 --> 00:18:28,390
yep it's all open source the it's been

00:18:25,330 --> 00:18:31,000
in 3.14 since it was released I think

00:18:28,390 --> 00:18:34,570
three not Fortunato when we released in

00:18:31,000 --> 00:18:42,460
320 and in our manual and on our

00:18:34,570 --> 00:18:44,919
knowledge base um so I've been playing

00:18:42,460 --> 00:18:46,600
around with the PDK myself and is that

00:18:44,919 --> 00:18:49,330
actually true you can't have an i/o

00:18:46,600 --> 00:18:52,870
thread that is not consuming a full CPU

00:18:49,330 --> 00:18:56,130
core because at low rates obviously

00:18:52,870 --> 00:18:58,630
that's kind of a waste right right so

00:18:56,130 --> 00:19:00,250
that's why it's an option because you

00:18:58,630 --> 00:19:01,990
might not want to use it for your low

00:19:00,250 --> 00:19:04,330
rate thing this is more for a high rate

00:19:01,990 --> 00:19:06,220
case where you know you're going to have

00:19:04,330 --> 00:19:09,309
data constantly coming in so there's no

00:19:06,220 --> 00:19:10,690
reason to switch out in a similar case

00:19:09,309 --> 00:19:13,150
you might want to have options for the

00:19:10,690 --> 00:19:14,650
can your radio scheduler where maybe

00:19:13,150 --> 00:19:16,900
sometimes you want to have switching

00:19:14,650 --> 00:19:20,380
sometimes you don't cuz maybe power is

00:19:16,900 --> 00:19:23,440
more important than being constantly

00:19:20,380 --> 00:19:26,410
ready the other question I have is the

00:19:23,440 --> 00:19:28,960
lock free ring exchange buffer right um

00:19:26,410 --> 00:19:31,240
is that your creation or aesthetic um

00:19:28,960 --> 00:19:33,370
does that come with the PDK or whatever

00:19:31,240 --> 00:19:37,120
thread building blocks that particular

00:19:33,370 --> 00:19:38,830
one was a creation from DP DK and then I

00:19:37,120 --> 00:19:39,580
added some way queues and such to deal

00:19:38,830 --> 00:19:42,940
with timeouts

00:19:39,580 --> 00:19:45,940
cool thanks did you find any influence

00:19:42,940 --> 00:19:49,770
from the Spector or melt on medications

00:19:45,940 --> 00:19:49,770
implemented in a caller and last year

00:19:50,520 --> 00:19:56,210
with the mitigations

00:19:53,620 --> 00:20:01,580
especially regarding task-switching

00:19:56,210 --> 00:20:05,000
you find a lot of accused power if you

00:20:01,580 --> 00:20:08,480
have to do a context switch and such

00:20:05,000 --> 00:20:10,940
stuff right so I imagine that would make

00:20:08,480 --> 00:20:11,990
the UDP or the Colonel case even worse

00:20:10,940 --> 00:20:13,760
than it is now

00:20:11,990 --> 00:20:16,130
so then that would be another argument

00:20:13,760 --> 00:20:18,310
to use DP DK because you do less of the

00:20:16,130 --> 00:20:21,110
context switching but I haven't

00:20:18,310 --> 00:20:23,560
specifically looked at what the impact

00:20:21,110 --> 00:20:29,380
is there

00:20:23,560 --> 00:20:29,380

YouTube URL: https://www.youtube.com/watch?v=6Ll0-H8IgyY


