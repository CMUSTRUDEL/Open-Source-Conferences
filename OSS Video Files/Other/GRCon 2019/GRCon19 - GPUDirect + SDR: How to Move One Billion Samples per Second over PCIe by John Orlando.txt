Title: GRCon19 - GPUDirect + SDR: How to Move One Billion Samples per Second over PCIe by John Orlando
Publication date: 2019-11-05
Playlist: GRCon 2019
Description: 
	GPUDirect + SDR: How to Move One Billion Samples per Second over PCIe by John Orlando

We are at an interesting time with SDR system development: channel bandwidths are pushing from hundreds of MHz to GHz, channel counts continue to increase, and computational components such as CPUs, FPGAs, and GPUs are rapidly evolving. Software developers can easily exhaust the signal processing resources of a general purpose CPU, and are turning to GPUs more and more to perform the task at hand. The core challenge of getting digitized radio samples into a GPU still typically involves a CPU in the middle doing nothing more than shuttling samples around. This is not only inefficient, but it will ultimately lead to a bottleneck in the system for wide bandwidth signal processing tasks.

This talk will discuss the work that Epiq Solutions has been tackling to integrate Nvidia’s GPUDirect framework for use in transporting digitized radio samples from an SDR directly into a GPU over PCIe. Leveraging GPUDirect alleviates the burden on the CPU while simultaneously increasing the achievable throughput by a substantial margin. Real world benchmarks leveraging the PCIe DMA driver utilized by Epiq’s Sidekiq SDR cards will be presented, including transport throughput benchmarks for the Nvidia Quadro P2000 GPU as well as an Nvidia Xavier GPU platform.
Captions: 
	00:00:14,490 --> 00:00:18,750
yeah as Martin said my name is John

00:00:16,320 --> 00:00:21,570
Orlando the CEO and co-founder and epic

00:00:18,750 --> 00:00:23,369
solutions thrilled to be another sponsor

00:00:21,570 --> 00:00:25,050
here this year with the new radio crew

00:00:23,369 --> 00:00:26,640
we're gonna be giving a talk I'm gonna

00:00:25,050 --> 00:00:29,220
be giving a talk today kind of on the

00:00:26,640 --> 00:00:30,630
shoulders of giants of our team back at

00:00:29,220 --> 00:00:32,279
our headquarters in Chicago that does a

00:00:30,630 --> 00:00:33,989
lot of work on our psychic line of radio

00:00:32,279 --> 00:00:35,580
cards in particular we're gonna be

00:00:33,989 --> 00:00:38,550
talking about something called GPU

00:00:35,580 --> 00:00:41,280
direct and a way to be able to move 1

00:00:38,550 --> 00:00:43,860
million samples or more over PCI Express

00:00:41,280 --> 00:00:44,910
and into a GPU and some of the some of

00:00:43,860 --> 00:00:46,230
the research we've been doing in that

00:00:44,910 --> 00:00:48,780
space in kind of the direction we're

00:00:46,230 --> 00:00:50,960
headed so quick company background about

00:00:48,780 --> 00:00:53,460
us we are celebrating our 10th year

00:00:50,960 --> 00:00:54,960
being in business this year delivering

00:00:53,460 --> 00:00:57,299
very very small form-factor

00:00:54,960 --> 00:01:00,210
software-defined radio cards to a whole

00:00:57,299 --> 00:01:01,860
host of different customers and use

00:01:00,210 --> 00:01:03,689
cases if you're familiar with our

00:01:01,860 --> 00:01:04,979
sidekick cards they come in mini PCIe

00:01:03,689 --> 00:01:06,630
and MDOT two and a whole bunch other

00:01:04,979 --> 00:01:08,850
flavor stop by our booth if you want to

00:01:06,630 --> 00:01:11,040
take a peek at them see some of them

00:01:08,850 --> 00:01:13,770
operational we've got a team of 40 folks

00:01:11,040 --> 00:01:15,750
and growing I'll put my plug here and at

00:01:13,770 --> 00:01:17,790
the end for we're hiring so if you like

00:01:15,750 --> 00:01:20,390
to make cool radio things come and talk

00:01:17,790 --> 00:01:23,820
to us but let's get right into it

00:01:20,390 --> 00:01:25,770
standard scr problem has three main

00:01:23,820 --> 00:01:28,410
components right at a very high level

00:01:25,770 --> 00:01:32,420
people have some sort of antenna some

00:01:28,410 --> 00:01:32,420
sort of our front-end and data converter

00:01:33,020 --> 00:01:39,060
and a compute element there's lots of

00:01:37,860 --> 00:01:40,350
debates around what the right way is to

00:01:39,060 --> 00:01:42,270
do this I'm not going to get into the

00:01:40,350 --> 00:01:44,850
religious debates around this what we're

00:01:42,270 --> 00:01:48,690
gonna focus on primarily is how data

00:01:44,850 --> 00:01:50,790
moves from that data converter into your

00:01:48,690 --> 00:01:52,590
compute L iment and in some sense kind

00:01:50,790 --> 00:01:53,909
of which computer elements make sense to

00:01:52,590 --> 00:01:56,909
be using for different kinds of

00:01:53,909 --> 00:01:58,770
applications so that's the super 10,000

00:01:56,909 --> 00:02:01,020
foot of view we've been spending a lot

00:01:58,770 --> 00:02:03,390
of time looking at as the number of

00:02:01,020 --> 00:02:05,250
channels increase as the bandwidth of

00:02:03,390 --> 00:02:07,080
these channels increase we have a

00:02:05,250 --> 00:02:09,030
problem that needs to be solved and that

00:02:07,080 --> 00:02:11,580
problem is nothing is getting easier in

00:02:09,030 --> 00:02:13,319
this world of how many samples how much

00:02:11,580 --> 00:02:16,110
data do you need to be able to consume

00:02:13,319 --> 00:02:19,769
back in the days of old-school cellular

00:02:16,110 --> 00:02:21,959
2g GSM things were easy you can do it

00:02:19,769 --> 00:02:24,389
you can gobble up a couple of megabytes

00:02:21,959 --> 00:02:27,749
per second on almost anything as we went

00:02:24,389 --> 00:02:28,340
to 3G and 4G and that variance of 802 11

00:02:27,749 --> 00:02:31,160
and now we've

00:02:28,340 --> 00:02:33,560
5gn are kind of at the state of affairs

00:02:31,160 --> 00:02:35,900
you have 400 megahertz wide channels you

00:02:33,560 --> 00:02:38,090
have multiple antennas you've got a

00:02:35,900 --> 00:02:41,150
veritable crapload of data that you have

00:02:38,090 --> 00:02:44,060
to deal with so when we think about

00:02:41,150 --> 00:02:46,610
compute elements compute options for

00:02:44,060 --> 00:02:47,930
these this kind of data flow you really

00:02:46,610 --> 00:02:49,400
have three cases people consider

00:02:47,930 --> 00:02:52,340
typically right everybody knows about

00:02:49,400 --> 00:02:55,430
doing this on GPS and a CPU you also

00:02:52,340 --> 00:02:59,239
have GPUs and you also have FPGAs I have

00:02:55,430 --> 00:03:00,920
the kind of original DSP like not as in

00:02:59,239 --> 00:03:03,140
digital signal processing but digital

00:03:00,920 --> 00:03:05,140
signal processor chips also as an option

00:03:03,140 --> 00:03:07,519
up there but primarily I would suspect

00:03:05,140 --> 00:03:09,650
90% of the people in this room are using

00:03:07,519 --> 00:03:12,739
one of those three main categories for

00:03:09,650 --> 00:03:14,480
further signal processing and if we're

00:03:12,739 --> 00:03:16,630
pragmatic about how we actually want to

00:03:14,480 --> 00:03:19,370
accomplish tasks like building radios

00:03:16,630 --> 00:03:21,290
there's a whole lot of software

00:03:19,370 --> 00:03:22,849
engineers out in the world there is a

00:03:21,290 --> 00:03:24,380
smaller number of software engineers

00:03:22,849 --> 00:03:26,150
that understand signal processing there

00:03:24,380 --> 00:03:27,739
is an even smaller number of FPGA

00:03:26,150 --> 00:03:29,390
engineers and then you know there's

00:03:27,739 --> 00:03:32,030
little itty-bitty sliver of folks that

00:03:29,390 --> 00:03:33,890
can do FPGA signal processing at the end

00:03:32,030 --> 00:03:35,359
of the day and that's a log scale and

00:03:33,890 --> 00:03:36,829
this is not actual data this is

00:03:35,359 --> 00:03:39,230
completely made up just after being in

00:03:36,829 --> 00:03:41,239
the industry for about 20 years so when

00:03:39,230 --> 00:03:44,810
we think about what we could do to be

00:03:41,239 --> 00:03:46,730
able to take advantage of the software

00:03:44,810 --> 00:03:48,799
folks we have in the universe and solve

00:03:46,730 --> 00:03:50,569
signal processing problems the right

00:03:48,799 --> 00:03:52,910
spot to kind of focus on sits in this

00:03:50,569 --> 00:03:55,519
middle category as a company we do lots

00:03:52,910 --> 00:03:57,319
and FPGAs but it is an incredibly hard

00:03:55,519 --> 00:03:59,090
to recruit and find good talent that our

00:03:57,319 --> 00:04:01,519
FPGA jockeys and signal processing

00:03:59,090 --> 00:04:02,750
jockeys and I see heads nodding so good

00:04:01,519 --> 00:04:05,900
I'm glad to hear we're not the only ones

00:04:02,750 --> 00:04:07,850
doing that so this is going to be a talk

00:04:05,900 --> 00:04:09,680
kind of about using GPUs and what they

00:04:07,850 --> 00:04:10,970
can enable but not so much on the hey

00:04:09,680 --> 00:04:13,040
I'm gonna go do a bunch of signal

00:04:10,970 --> 00:04:15,620
processing kernels in my GPU it's all

00:04:13,040 --> 00:04:18,070
about how can you get data into the GPU

00:04:15,620 --> 00:04:20,600
in the most effective manner possible

00:04:18,070 --> 00:04:22,190
the Eureka moment here is kind of

00:04:20,600 --> 00:04:23,720
tongue-in-cheek right at the end of the

00:04:22,190 --> 00:04:25,880
day people have been doing work with

00:04:23,720 --> 00:04:28,010
GPUs and signal processing for years

00:04:25,880 --> 00:04:29,930
there was some work published as it

00:04:28,010 --> 00:04:33,500
relates to the new radio back in 2011

00:04:29,930 --> 00:04:35,900
for being able to use a GPU for doing

00:04:33,500 --> 00:04:38,090
the computational portion of a signal

00:04:35,900 --> 00:04:40,159
processing block but you had to get the

00:04:38,090 --> 00:04:41,780
data into the CPU and then the CPU had

00:04:40,159 --> 00:04:43,700
to shuttle it down to the GP

00:04:41,780 --> 00:04:45,440
and then you had to get it back and if

00:04:43,700 --> 00:04:48,050
you're like our team you've probably

00:04:45,440 --> 00:04:50,840
found that that shuttling activity can

00:04:48,050 --> 00:04:52,430
seriously kill any benefit you have by

00:04:50,840 --> 00:04:54,620
using a GPU there are cases where that's

00:04:52,430 --> 00:04:57,080
not that's not necessarily the the way

00:04:54,620 --> 00:04:59,240
it goes but commonly that is a that is

00:04:57,080 --> 00:05:03,020
the case so the classic architecture

00:04:59,240 --> 00:05:05,990
here let's see does this yeah well so

00:05:03,020 --> 00:05:06,950
the classic architecture here there we

00:05:05,990 --> 00:05:08,690
go perfect

00:05:06,950 --> 00:05:09,980
you'll have your analog signal coming in

00:05:08,690 --> 00:05:12,380
you'll go through a front end into your

00:05:09,980 --> 00:05:13,970
data converters you will have some form

00:05:12,380 --> 00:05:16,580
of digitized interface going into your

00:05:13,970 --> 00:05:18,920
FPGA maybe that's jazzy - oh 4 maybe

00:05:16,580 --> 00:05:21,800
that's something else that FPGA will

00:05:18,920 --> 00:05:24,830
typically package up those samples move

00:05:21,800 --> 00:05:27,380
them typically over PCI Express or some

00:05:24,830 --> 00:05:29,300
other interface into a host CPU where

00:05:27,380 --> 00:05:31,970
they will be DMA direct memory access

00:05:29,300 --> 00:05:34,580
into system Ram and then to get them to

00:05:31,970 --> 00:05:36,260
the CPU to get them down to a GPU for

00:05:34,580 --> 00:05:38,840
processing the CPU will have to pull

00:05:36,260 --> 00:05:41,660
them out and push them back over PCI

00:05:38,840 --> 00:05:42,800
Express most typically and down to a GPU

00:05:41,660 --> 00:05:44,750
so this is kind of the classic

00:05:42,800 --> 00:05:46,670
architecture for how a lot of systems

00:05:44,750 --> 00:05:49,400
work when you're talking about having a

00:05:46,670 --> 00:05:51,650
GPU in the mix our question was can we

00:05:49,400 --> 00:05:53,660
do better can we increase throughput if

00:05:51,650 --> 00:05:55,280
we can by how much we were interested in

00:05:53,660 --> 00:05:58,460
understanding kind of the bounding boxes

00:05:55,280 --> 00:06:00,410
here so with that I'm going to talk a

00:05:58,460 --> 00:06:03,740
little bit about this thing called GPU

00:06:00,410 --> 00:06:06,320
direct from Nvidia so what is GPU direct

00:06:03,740 --> 00:06:08,810
it was developed by Nvidia back in 2013

00:06:06,320 --> 00:06:11,000
their basic premise was we want to have

00:06:08,810 --> 00:06:12,860
a mechanism that lets you move data

00:06:11,000 --> 00:06:16,730
regardless of what kind of data it is

00:06:12,860 --> 00:06:18,410
into a GPU over PCI Express without

00:06:16,730 --> 00:06:21,200
having to have the CPU play any

00:06:18,410 --> 00:06:23,480
significant role in that process so

00:06:21,200 --> 00:06:25,640
Nvidia rolled this out

00:06:23,480 --> 00:06:27,350
it's kind of gotten some traction over

00:06:25,640 --> 00:06:30,110
the course of time but not a ton of

00:06:27,350 --> 00:06:33,020
press the the idea here is there's kind

00:06:30,110 --> 00:06:36,260
of a software driver that allows allows

00:06:33,020 --> 00:06:38,930
PCIe devices to target DMA transactions

00:06:36,260 --> 00:06:40,970
directly into the GPU memory through the

00:06:38,930 --> 00:06:42,770
PCI Express route complex without ever

00:06:40,970 --> 00:06:46,310
having to go up to the CPU and back down

00:06:42,770 --> 00:06:48,200
if you're familiar with doing NVIDIA GPU

00:06:46,310 --> 00:06:49,520
based programming CUDA is a typical way

00:06:48,200 --> 00:06:52,520
you would be building computational

00:06:49,520 --> 00:06:54,350
kernels down on the GPU and if you can

00:06:52,520 --> 00:06:55,430
kind of make all of this work at the end

00:06:54,350 --> 00:06:56,840
of the day the CPU

00:06:55,430 --> 00:06:59,510
sets things up and gets the hell out of

00:06:56,840 --> 00:07:00,740
the way which is nice in theory it's

00:06:59,510 --> 00:07:02,060
more efficient you should be able to get

00:07:00,740 --> 00:07:04,640
more throughput you should be able to

00:07:02,060 --> 00:07:06,470
use fewer CPU cycles in the mix you

00:07:04,640 --> 00:07:08,420
should be able to lower your system

00:07:06,470 --> 00:07:11,000
power consumption because the CPU isn't

00:07:08,420 --> 00:07:13,760
doing as much and generally winning

00:07:11,000 --> 00:07:17,330
which which we like so it would take the

00:07:13,760 --> 00:07:19,400
architecture from this to this where you

00:07:17,330 --> 00:07:21,590
are doing this very efficient path going

00:07:19,400 --> 00:07:24,080
from the radio front end into the FPGA a

00:07:21,590 --> 00:07:25,610
brief moment inside the CPU but only

00:07:24,080 --> 00:07:28,460
into the route complex and then back

00:07:25,610 --> 00:07:30,380
down to the GPU to let the GPU do the

00:07:28,460 --> 00:07:33,560
thing that it's good at and crunching on

00:07:30,380 --> 00:07:35,030
the samples so what do you need to be

00:07:33,560 --> 00:07:36,680
able to accomplish this we need a couple

00:07:35,030 --> 00:07:39,200
things first and foremost you need a

00:07:36,680 --> 00:07:41,990
radio that knows how to emit samples

00:07:39,200 --> 00:07:43,910
over PCI Express we happen to make

00:07:41,990 --> 00:07:45,620
radios that emit samples over PCI

00:07:43,910 --> 00:07:49,360
Express the one most appropriate here

00:07:45,620 --> 00:07:53,060
are psychic x4 card which is a vida 57

00:07:49,360 --> 00:07:56,360
compliant FPGA mezzanine card it has to

00:07:53,060 --> 00:07:58,430
of analog devices a DRV 9009 RF ICS

00:07:56,360 --> 00:08:00,380
which is their most recent RFIC they've

00:07:58,430 --> 00:08:02,530
introduced into the market so on this

00:08:00,380 --> 00:08:05,180
one card you get a four by four my mo

00:08:02,530 --> 00:08:08,840
capability that gives you either four

00:08:05,180 --> 00:08:10,820
200 megahertz wide channels or up to 800

00:08:08,840 --> 00:08:12,560
megahertz wide instantaneous bandwidth

00:08:10,820 --> 00:08:14,270
that you can consume it's got an

00:08:12,560 --> 00:08:15,590
integrated pre-select filters supported

00:08:14,270 --> 00:08:17,930
by your standard api if you want more

00:08:15,590 --> 00:08:19,400
details come find us the idea here

00:08:17,930 --> 00:08:21,920
though is you would take that card plug

00:08:19,400 --> 00:08:24,290
it into some sort of carrier whether

00:08:21,920 --> 00:08:27,350
it's a commercially commercial carrier

00:08:24,290 --> 00:08:29,810
for a standard PCI Express slot or into

00:08:27,350 --> 00:08:32,090
something like a 3u VPX card for more

00:08:29,810 --> 00:08:33,740
ruggedized deployments the combination

00:08:32,090 --> 00:08:35,480
of the x4 card and one of the two

00:08:33,740 --> 00:08:38,240
carriers then goes into a host system

00:08:35,480 --> 00:08:41,150
and voila you've got gobs of bandwidth

00:08:38,240 --> 00:08:44,450
going over PCI Express into some host

00:08:41,150 --> 00:08:46,220
system to then be processed so as a as a

00:08:44,450 --> 00:08:47,420
backdrop that kind of sets the stage we

00:08:46,220 --> 00:08:49,280
have a bunch of different other radio

00:08:47,420 --> 00:08:50,870
cards this is a bit of an eyesore but

00:08:49,280 --> 00:08:53,060
all the ones along the top and our

00:08:50,870 --> 00:08:54,710
sidekick family have PCI Express support

00:08:53,060 --> 00:08:56,540
the slides will be available if you're

00:08:54,710 --> 00:08:58,790
into the details come you can check them

00:08:56,540 --> 00:09:00,830
out later the other thing you need is

00:08:58,790 --> 00:09:04,280
the compute element side right you need

00:09:00,830 --> 00:09:06,770
a GPU in the mix and you need some sort

00:09:04,280 --> 00:09:07,970
of CPU to kind of set things up the two

00:09:06,770 --> 00:09:09,260
main systems that we were looking at

00:09:07,970 --> 00:09:12,860
here was a standard

00:09:09,260 --> 00:09:15,680
our standard x86 motherboard with NVIDIA

00:09:12,860 --> 00:09:19,130
Quadro p2000 graphics card plugged into

00:09:15,680 --> 00:09:21,860
it and a core i7 CPU as well as an

00:09:19,130 --> 00:09:25,520
nvidia xavier platform which is their

00:09:21,860 --> 00:09:27,680
most recent kind of low-power GPU CPU

00:09:25,520 --> 00:09:29,720
combination so both of these platforms

00:09:27,680 --> 00:09:32,120
end up being fairly good for doing some

00:09:29,720 --> 00:09:33,410
evaluation of GPU direct so we decided

00:09:32,120 --> 00:09:36,650
that kind of made sense one other point

00:09:33,410 --> 00:09:38,870
to note here again it is necessary for

00:09:36,650 --> 00:09:40,640
GPU directs to work to have the GPU in

00:09:38,870 --> 00:09:42,230
the radio on the same pci express root

00:09:40,640 --> 00:09:44,450
complex that's taking care of you

00:09:42,230 --> 00:09:47,900
implicitly in the Nvidia xavier platform

00:09:44,450 --> 00:09:49,640
by by design so roughly how it works if

00:09:47,900 --> 00:09:51,740
you're familiar with DMA it's almost

00:09:49,640 --> 00:09:54,020
exactly like you would expect it to be

00:09:51,740 --> 00:09:56,480
the CPU is responsible for pinning down

00:09:54,020 --> 00:09:58,460
a block of memory inside of the GPU with

00:09:56,480 --> 00:10:00,440
kind of reserving it the CPU is

00:09:58,460 --> 00:10:02,930
responsible for telling the FPGA in the

00:10:00,440 --> 00:10:05,840
radio card hey here's this physical

00:10:02,930 --> 00:10:07,340
address space in the in the GPU that way

00:10:05,840 --> 00:10:10,070
I want you to be able to target your DMA

00:10:07,340 --> 00:10:11,870
transactions to CPU gets the hell out of

00:10:10,070 --> 00:10:13,940
the way and the FPGA can then start

00:10:11,870 --> 00:10:16,910
dumping data over DMA directly into the

00:10:13,940 --> 00:10:19,280
GPU while the CPU for the most part sits

00:10:16,910 --> 00:10:20,720
back GPU crunches the data like a

00:10:19,280 --> 00:10:23,330
champion then you go to step four and

00:10:20,720 --> 00:10:25,370
you can kind of repeat this loop so our

00:10:23,330 --> 00:10:27,920
question was great theories awesome does

00:10:25,370 --> 00:10:29,330
it work we set out to figure out what

00:10:27,920 --> 00:10:31,460
kind of difference it actually makes so

00:10:29,330 --> 00:10:34,310
we went through and updated our current

00:10:31,460 --> 00:10:36,260
PCI Express DMA driver that works with

00:10:34,310 --> 00:10:38,660
our psychic cards to support GPU direct

00:10:36,260 --> 00:10:40,610
we built two hardware platforms exactly

00:10:38,660 --> 00:10:42,350
the ones I described to you before ie a

00:10:40,610 --> 00:10:44,900
commercial off-the-shelf a gigabyte

00:10:42,350 --> 00:10:47,770
motherboard PCIe carrier cart with our

00:10:44,900 --> 00:10:51,290
with our radio on it and then a Quadro

00:10:47,770 --> 00:10:52,340
GPU card from Nvidia as well as an exam

00:10:51,290 --> 00:10:53,900
for meeeeee

00:10:52,340 --> 00:10:56,000
to be able to see how well this works

00:10:53,900 --> 00:10:58,070
both of these run Linux GPU Direct does

00:10:56,000 --> 00:10:59,270
not work in Windows currently there's a

00:10:58,070 --> 00:11:01,190
little bit of talk about trying to make

00:10:59,270 --> 00:11:03,020
that happen but for the most part this

00:11:01,190 --> 00:11:06,380
is a this is all Linux all the time

00:11:03,020 --> 00:11:08,330
and we wanted to see how it would go so

00:11:06,380 --> 00:11:11,090
we have two test scenarios right the old

00:11:08,330 --> 00:11:13,400
way we're gonna go FPGA to CPU to Ram to

00:11:11,090 --> 00:11:15,950
CPU to GPU and then the new way we're

00:11:13,400 --> 00:11:17,780
gonna go FPGA to GPU and just for kicks

00:11:15,950 --> 00:11:19,730
we'll do a simple computational chronal

00:11:17,780 --> 00:11:22,170
in the GPU again this is not about is

00:11:19,730 --> 00:11:23,850
the GPU efficient at processing

00:11:22,170 --> 00:11:25,829
this is all about understanding the

00:11:23,850 --> 00:11:27,510
throughput benefit of getting data over

00:11:25,829 --> 00:11:32,060
GPU direct versus using a more true

00:11:27,510 --> 00:11:34,500
traditional method so the test results

00:11:32,060 --> 00:11:36,510
we went through and set up our first

00:11:34,500 --> 00:11:37,889
system platform number one and we did

00:11:36,510 --> 00:11:39,420
the old way where you have to go all the

00:11:37,889 --> 00:11:42,180
way up to the CPU and come back down and

00:11:39,420 --> 00:11:44,010
we were measuring just north of 1700

00:11:42,180 --> 00:11:46,440
megabytes per second didn't seem too bad

00:11:44,010 --> 00:11:47,910
reasonable amount of data going through

00:11:46,440 --> 00:11:50,850
and then we got all excited and we fired

00:11:47,910 --> 00:11:53,070
up the GPU direct way and it was a

00:11:50,850 --> 00:11:55,320
lopping seventeen hundred and thirty

00:11:53,070 --> 00:11:57,389
three megabytes per second for exactly

00:11:55,320 --> 00:12:00,180
you guessed it one percent performance

00:11:57,389 --> 00:12:02,970
improvement so we sat there for a while

00:12:00,180 --> 00:12:05,160
and we said that doesn't seem quite

00:12:02,970 --> 00:12:07,110
right what could we be doing wrong here

00:12:05,160 --> 00:12:08,220
either GPU Direct sucks so there's

00:12:07,110 --> 00:12:09,899
something else about it that we're

00:12:08,220 --> 00:12:11,160
missing it turns out there was something

00:12:09,899 --> 00:12:12,180
else about that we were missing there's

00:12:11,160 --> 00:12:13,829
a bunch of head-scratching

00:12:12,180 --> 00:12:17,100
we stared at the platform a little bit

00:12:13,829 --> 00:12:18,389
we ran some more tests and of course you

00:12:17,100 --> 00:12:19,380
got to go check the hardware at the end

00:12:18,389 --> 00:12:21,180
of the day to really figure out what's

00:12:19,380 --> 00:12:23,250
going on so let's look at this we

00:12:21,180 --> 00:12:27,480
originally had this loaded up with a PCI

00:12:23,250 --> 00:12:29,040
Express Gen 2 by 4 DMA engine which

00:12:27,480 --> 00:12:31,800
means we have four lanes a PCI Express

00:12:29,040 --> 00:12:34,529
run at Gen 2 speeds if we do a little

00:12:31,800 --> 00:12:37,620
bit of math each Gen 2 PCI Express Lane

00:12:34,529 --> 00:12:40,110
gives you five gigabits per second each

00:12:37,620 --> 00:12:43,440
we have four lanes so we get 20 gigabits

00:12:40,110 --> 00:12:45,660
per second through that path there is a

00:12:43,440 --> 00:12:49,529
p10 decoding on the lane so you actually

00:12:45,660 --> 00:12:52,769
reduce that by 6 by 80% by 20% down to

00:12:49,529 --> 00:12:54,600
16 Giga bits per second a real-world PCI

00:12:52,769 --> 00:12:57,510
bus utilization to kind of expect this

00:12:54,600 --> 00:12:58,800
maybe 85% efficient source oh and does

00:12:57,510 --> 00:13:01,560
anybody want to guess if you divide

00:12:58,800 --> 00:13:04,260
thirteen point six by eight bits how

00:13:01,560 --> 00:13:06,569
many megabytes per second that is that

00:13:04,260 --> 00:13:09,209
is exactly almost 1700 megabytes per

00:13:06,569 --> 00:13:11,550
second so what does this mean it means

00:13:09,209 --> 00:13:13,440
that we effectively had the PCI Express

00:13:11,550 --> 00:13:16,470
bus as the limiting factor in this first

00:13:13,440 --> 00:13:18,240
test scenario so we say great for these

00:13:16,470 --> 00:13:20,639
cases where you're in this range there

00:13:18,240 --> 00:13:22,680
actually isn't any throughput benefit to

00:13:20,639 --> 00:13:25,410
using GPU direct unless you want to have

00:13:22,680 --> 00:13:27,180
your CPU restored CPU set aside to do

00:13:25,410 --> 00:13:30,569
some additional processing the path

00:13:27,180 --> 00:13:33,300
going FPGA to CPU to RAM back to the CPU

00:13:30,569 --> 00:13:35,640
back down to the GPU or going right from

00:13:33,300 --> 00:13:38,120
FPGA to GPU same

00:13:35,640 --> 00:13:41,550
but that you can achieve so what now

00:13:38,120 --> 00:13:44,190
well we said let's take another look at

00:13:41,550 --> 00:13:46,529
this try take two what if we go PCI

00:13:44,190 --> 00:13:48,300
Express gen3 we bump it up to eight

00:13:46,529 --> 00:13:50,370
lanes and we see what kind of difference

00:13:48,300 --> 00:13:52,620
that makes the motherboard actually

00:13:50,370 --> 00:13:54,930
supports up to Gen 3 by 16 but we wanted

00:13:52,620 --> 00:13:57,209
to go with by 8 if you're not a PCI

00:13:54,930 --> 00:13:59,880
Express jockey Gen 3 gets you to 8

00:13:57,209 --> 00:14:01,980
gigabits per second per Lane we have 8

00:13:59,880 --> 00:14:04,560
lanes so we've got 64 gigabits per

00:14:01,980 --> 00:14:07,649
second of throughput it uses a different

00:14:04,560 --> 00:14:10,140
line coding we have 128 B 130 B which

00:14:07,649 --> 00:14:12,029
actually means we can we only drop down

00:14:10,140 --> 00:14:15,769
the overall throughput about 63 gigabits

00:14:12,029 --> 00:14:18,240
per second it should be plenty right and

00:14:15,769 --> 00:14:20,880
we redo the tests and we hope that

00:14:18,240 --> 00:14:21,450
sanity is restored so take to drumroll

00:14:20,880 --> 00:14:24,170
please

00:14:21,450 --> 00:14:26,670
test one the old way we're now up to 3.2

00:14:24,170 --> 00:14:28,980
gigabytes per second not too bad

00:14:26,670 --> 00:14:31,140
definite improvement that we see and of

00:14:28,980 --> 00:14:33,720
course the GPU direct way ah that's what

00:14:31,140 --> 00:14:35,250
we're looking for so now we're at 4.7

00:14:33,720 --> 00:14:37,440
just under four point seven gigabytes

00:14:35,250 --> 00:14:39,899
per second there's about a net 45

00:14:37,440 --> 00:14:42,329
percent throughput improvement in being

00:14:39,899 --> 00:14:43,829
able to move data from the radio into

00:14:42,329 --> 00:14:46,230
the GPU compared to what I would

00:14:43,829 --> 00:14:48,000
consider the old standard way of not

00:14:46,230 --> 00:14:50,250
using GPU direct so that's actually

00:14:48,000 --> 00:14:51,810
pretty important from our view sanity is

00:14:50,250 --> 00:14:52,890
kind of restored here we were happy to

00:14:51,810 --> 00:14:55,410
see that improvement we weren't exactly

00:14:52,890 --> 00:14:58,500
sure what to expect in doing these tests

00:14:55,410 --> 00:15:02,250
and more importantly as I go to the next

00:14:58,500 --> 00:15:03,540
oh there we go more importantly we

00:15:02,250 --> 00:15:06,120
wanted to see what it looked like when

00:15:03,540 --> 00:15:07,829
we went to the Xavier platform to see

00:15:06,120 --> 00:15:10,350
what it looked what it was able to do so

00:15:07,829 --> 00:15:12,480
we did test one the old way up to the

00:15:10,350 --> 00:15:14,640
CPU and back down a little bit lower not

00:15:12,480 --> 00:15:17,250
unexpected it's lower power Xavier has

00:15:14,640 --> 00:15:18,959
ARM cores versus x86 we were seeing just

00:15:17,250 --> 00:15:21,420
north of two gigabytes per second and

00:15:18,959 --> 00:15:22,880
then we did the GPU direct way and it

00:15:21,420 --> 00:15:25,350
turns out it is almost an identical

00:15:22,880 --> 00:15:27,930
percentage improvement throughput for

00:15:25,350 --> 00:15:29,730
Xavier compared to the x86 total is

00:15:27,930 --> 00:15:32,839
still lower but in terms of percentage

00:15:29,730 --> 00:15:35,339
improvement 46 percent versus 45 percent

00:15:32,839 --> 00:15:36,540
so we were pretty happy to see that

00:15:35,339 --> 00:15:40,320
those numbers seemed to line up with

00:15:36,540 --> 00:15:42,690
things with things here so the answer

00:15:40,320 --> 00:15:44,880
here is yes Virginia you can move more

00:15:42,690 --> 00:15:48,230
than a billion samples per second over

00:15:44,880 --> 00:15:52,310
PCI Express directly into a GPU using

00:15:48,230 --> 00:15:56,000
if you direct and RDMA 46 77 megabytes

00:15:52,310 --> 00:15:57,290
per second equates to about 1.1 7 Giga

00:15:56,000 --> 00:16:00,410
samples per second

00:15:57,290 --> 00:16:02,750
assuming you have 16-bit I 16-bit queue

00:16:00,410 --> 00:16:04,910
that is enough to digitize 1 gigahertz

00:16:02,750 --> 00:16:06,830
of RF spectrum and get it into a GPU

00:16:04,910 --> 00:16:08,840
without the without the CPU having to be

00:16:06,830 --> 00:16:11,060
substantially involved as best we can

00:16:08,840 --> 00:16:14,090
tell SDR is based on PCI Express radios

00:16:11,060 --> 00:16:16,550
give you the most efficient scalable low

00:16:14,090 --> 00:16:18,380
power transport way out there on the

00:16:16,550 --> 00:16:20,060
market today in terms of being able to

00:16:18,380 --> 00:16:21,170
extend to these bandwidths and these

00:16:20,060 --> 00:16:23,690
channel counts that are going to matter

00:16:21,170 --> 00:16:26,510
and and GPU Direct is kind of a natural

00:16:23,690 --> 00:16:27,830
fit in this space but that question of

00:16:26,510 --> 00:16:29,990
doesn't matter is pretty important

00:16:27,830 --> 00:16:31,940
because if you only care about 10

00:16:29,990 --> 00:16:33,620
megahertz of spectrum or 20 megahertz of

00:16:31,940 --> 00:16:35,750
spectrum it probably makes no difference

00:16:33,620 --> 00:16:37,970
to you unless you have a boatload of

00:16:35,750 --> 00:16:40,220
processing you want to do in the GPU to

00:16:37,970 --> 00:16:41,600
kind of push things forward but if you

00:16:40,220 --> 00:16:43,460
care about single channel bandwidths

00:16:41,600 --> 00:16:46,490
that are like in these gigahertz Plus

00:16:43,460 --> 00:16:49,250
ranges if you care about my mode 2 by 2

00:16:46,490 --> 00:16:50,720
4 by 4 more than that where each channel

00:16:49,250 --> 00:16:52,970
bandwidth is going to be hundreds of

00:16:50,720 --> 00:16:54,560
megahertz potentially or if you just

00:16:52,970 --> 00:16:56,930
want to keep your CPU cycles for

00:16:54,560 --> 00:16:58,490
something else or do your just your

00:16:56,930 --> 00:17:00,290
lagea jockey and that's where you want

00:16:58,490 --> 00:17:01,760
to do your processing then yes GPU

00:17:00,290 --> 00:17:03,980
direct does matter and it kind of

00:17:01,760 --> 00:17:05,900
matters big time in terms of the overall

00:17:03,980 --> 00:17:08,420
throughput it may be the only way to get

00:17:05,900 --> 00:17:09,920
the results desired that you want the

00:17:08,420 --> 00:17:12,620
other nice thing that working with GPU

00:17:09,920 --> 00:17:14,540
is not specific to GPU direct but having

00:17:12,620 --> 00:17:16,060
GPUs in the mix gives you a fairly nice

00:17:14,540 --> 00:17:18,440
path for being able to interconnect

00:17:16,060 --> 00:17:20,120
processing nodes together I long for the

00:17:18,440 --> 00:17:22,130
day when there's a way to kind of have a

00:17:20,120 --> 00:17:23,840
flow graph and properly deploy the

00:17:22,130 --> 00:17:26,060
signal processing across all these

00:17:23,840 --> 00:17:27,530
different nodes whether you know with

00:17:26,060 --> 00:17:29,390
art the work that our F NOC is doing

00:17:27,530 --> 00:17:31,040
with FPGA is I think is right in line

00:17:29,390 --> 00:17:32,630
with this and kind of parallel doing

00:17:31,040 --> 00:17:34,580
something similar with GPUs from a

00:17:32,630 --> 00:17:36,140
higher-level thing would be would be

00:17:34,580 --> 00:17:37,490
super awesome and video seems to be

00:17:36,140 --> 00:17:39,140
pushing pretty hard in some of these

00:17:37,490 --> 00:17:40,700
spaces for how to scale this and

00:17:39,140 --> 00:17:42,650
especially as the band widths go up and

00:17:40,700 --> 00:17:44,690
up I think it'll be a big deal so we'll

00:17:42,650 --> 00:17:47,390
be pushing GPU direct support in lips I

00:17:44,690 --> 00:17:50,570
could come in fourth quarter of 2019 the

00:17:47,390 --> 00:17:51,710
two initial versions the two initial

00:17:50,570 --> 00:17:53,150
radios that are going to kind of get

00:17:51,710 --> 00:17:55,640
most of the focus is going to be psychic

00:17:53,150 --> 00:17:57,230
x4 this one that has 4 200 mega watt

00:17:55,640 --> 00:17:59,480
channels or you can look at an aggregate

00:17:57,230 --> 00:18:00,890
of 800 megahertz instantaneously that

00:17:59,480 --> 00:18:01,470
makes the most sense and probably where

00:18:00,890 --> 00:18:03,720
the most Ben

00:18:01,470 --> 00:18:06,120
it would be realized as a second tier

00:18:03,720 --> 00:18:07,230
our psychic stretch that a little bit

00:18:06,120 --> 00:18:09,480
has been talked about through the

00:18:07,230 --> 00:18:12,480
lightning talks earlier it's our more

00:18:09,480 --> 00:18:15,600
recent offering single single receive

00:18:12,480 --> 00:18:18,870
single transmit in a 22 by a TM to carb

00:18:15,600 --> 00:18:20,220
form factor only 61 point 44 mega

00:18:18,870 --> 00:18:22,770
samples per second but even at those

00:18:20,220 --> 00:18:24,659
rates you could seriously you could

00:18:22,770 --> 00:18:25,980
potentially peg a CPU if you were really

00:18:24,659 --> 00:18:27,780
trying to crunch through all that data

00:18:25,980 --> 00:18:29,250
so there still may be some benefit even

00:18:27,780 --> 00:18:31,760
with a narrower bandwidth and being able

00:18:29,250 --> 00:18:33,860
to make this work with a with GPU

00:18:31,760 --> 00:18:36,270
finally some areas for exploration

00:18:33,860 --> 00:18:38,549
you're still doing DM a descriptor

00:18:36,270 --> 00:18:40,289
management so the CPU is still involved

00:18:38,549 --> 00:18:42,210
a little bit here there's some

00:18:40,289 --> 00:18:44,820
back-and-forth on the best ways to

00:18:42,210 --> 00:18:47,130
handle that other things of concise a of

00:18:44,820 --> 00:18:49,950
question the optimal data transport size

00:18:47,130 --> 00:18:51,570
GPUs like big blocks at data right now

00:18:49,950 --> 00:18:53,400
we're still kind of working with what's

00:18:51,570 --> 00:18:55,860
standard and PCI Express DMA blocks

00:18:53,400 --> 00:18:58,350
which is for K bytes probably the

00:18:55,860 --> 00:19:00,480
biggest one is how the metadata for IQ

00:18:58,350 --> 00:19:02,549
streams get handled a lot of it when

00:19:00,480 --> 00:19:05,460
we're transporting our data typically we

00:19:02,549 --> 00:19:07,020
have a header that is a precedes the IQ

00:19:05,460 --> 00:19:09,000
with things like timestamps and gain

00:19:07,020 --> 00:19:10,110
values things specific to the radio if

00:19:09,000 --> 00:19:11,640
you want to check to see if you have

00:19:10,110 --> 00:19:13,830
timestamp gaps or you want to assess the

00:19:11,640 --> 00:19:15,929
gain the GPU will need to be the thing

00:19:13,830 --> 00:19:17,280
that actually does that and possibly the

00:19:15,929 --> 00:19:19,919
way around this is just to kind of give

00:19:17,280 --> 00:19:21,600
it raw IQ data and worry about the the

00:19:19,919 --> 00:19:22,980
metadata kind of out-of-band and assume

00:19:21,600 --> 00:19:25,500
that the GPU will be able to consume

00:19:22,980 --> 00:19:27,059
things and keep up with it and then of

00:19:25,500 --> 00:19:29,070
course integration with GPU frameworks

00:19:27,059 --> 00:19:31,500
or something from the miter folks called

00:19:29,070 --> 00:19:33,240
photon that is hopefully going to get

00:19:31,500 --> 00:19:34,799
some more kind of light a day out there

00:19:33,240 --> 00:19:36,780
in terms of what's going on with that

00:19:34,799 --> 00:19:38,850
this could be a great fit for that and

00:19:36,780 --> 00:19:42,750
last but not least this is all very

00:19:38,850 --> 00:19:44,820
Nvidia specific and CUDA specific if you

00:19:42,750 --> 00:19:46,970
are an open CL guy or a girl and

00:19:44,820 --> 00:19:49,289
interested in doing work in that space

00:19:46,970 --> 00:19:51,840
there may be some ways to make it work

00:19:49,289 --> 00:19:54,480
there's something called direct GMA or

00:19:51,840 --> 00:19:56,250
bus addressable memory that could

00:19:54,480 --> 00:19:58,620
provide a lot of the same advantages if

00:19:56,250 --> 00:20:01,260
you google around open CL and PCI

00:19:58,620 --> 00:20:03,539
Express and DMA there is not a whole ton

00:20:01,260 --> 00:20:05,940
out there the last active work that we

00:20:03,539 --> 00:20:07,350
could find was several years back so we

00:20:05,940 --> 00:20:09,270
need to do some more exploring to see if

00:20:07,350 --> 00:20:11,370
we're just not our Google foo is not

00:20:09,270 --> 00:20:13,169
sufficiently strong or if there are

00:20:11,370 --> 00:20:14,970
really people interested in doing this

00:20:13,169 --> 00:20:18,179
the exact same ideas should work

00:20:14,970 --> 00:20:19,289
and should be viable so I'd like to give

00:20:18,179 --> 00:20:21,030
a quick shout out to some of the mitre

00:20:19,289 --> 00:20:22,799
folks that helped fund a portion of this

00:20:21,030 --> 00:20:25,200
development work as well to our psychic

00:20:22,799 --> 00:20:27,690
team that did the that did the heavy

00:20:25,200 --> 00:20:29,700
lifting and to all our customers that

00:20:27,690 --> 00:20:32,220
continue to use our psychic cards and

00:20:29,700 --> 00:20:33,780
deploy them all over the world and I

00:20:32,220 --> 00:20:35,429
said at the beginning of the talk that I

00:20:33,780 --> 00:20:37,110
would be revisiting this one more time

00:20:35,429 --> 00:20:38,820
if you're at the top of your game and

00:20:37,110 --> 00:20:43,080
you like building hardware and software

00:20:38,820 --> 00:20:44,340
and SCR is your thing we are hiring and

00:20:43,080 --> 00:20:46,620
would love to come talk we've got a

00:20:44,340 --> 00:20:48,960
booth over in the in the center here and

00:20:46,620 --> 00:20:51,720
or send us an email a jobs at Epic

00:20:48,960 --> 00:21:03,440
solutions so with that that is

00:20:51,720 --> 00:21:06,690
everything we got time for questions

00:21:03,440 --> 00:21:08,460
hey John great talk so you kind of

00:21:06,690 --> 00:21:09,510
mentioned that when when things even now

00:21:08,460 --> 00:21:11,789
you're like oh it doesn't really matter

00:21:09,510 --> 00:21:13,049
for certain cases one or the other did

00:21:11,789 --> 00:21:14,580
you do any power measurements because

00:21:13,049 --> 00:21:15,990
I'm wondering if it saves you a sane

00:21:14,580 --> 00:21:18,000
amount of power even if you see the same

00:21:15,990 --> 00:21:21,120
performance yeah it will say it will

00:21:18,000 --> 00:21:22,289
absolutely save you power and the two

00:21:21,120 --> 00:21:25,440
things we want to spend a little bit

00:21:22,289 --> 00:21:27,240
more time exploring is how much power

00:21:25,440 --> 00:21:29,850
does it really end up saving you can the

00:21:27,240 --> 00:21:31,289
CPU drop to a lower power state within

00:21:29,850 --> 00:21:32,880
you know before it has to do some

00:21:31,289 --> 00:21:35,190
descriptor management what does that

00:21:32,880 --> 00:21:36,840
equate to and then we gave kind of a pre

00:21:35,190 --> 00:21:38,520
briefing to some customers earlier this

00:21:36,840 --> 00:21:40,350
morning on this and they were asking

00:21:38,520 --> 00:21:42,750
about you know like memory bandwidth

00:21:40,350 --> 00:21:44,520
like how much more do they really have

00:21:42,750 --> 00:21:46,830
free on the CPU for doing other things

00:21:44,520 --> 00:21:49,230
while the GPU is chunking away so their

00:21:46,830 --> 00:21:50,250
areas for exploration I'm pretty

00:21:49,230 --> 00:21:51,840
confident that there are some

00:21:50,250 --> 00:21:55,919
non-trivial savings there we don't have

00:21:51,840 --> 00:22:00,150
them quantified yet though were you able

00:21:55,919 --> 00:22:03,390
to succeed in this endeavor with open

00:22:00,150 --> 00:22:05,340
information or did you have to sign we

00:22:03,390 --> 00:22:07,260
do not have to sign any NDA's we started

00:22:05,340 --> 00:22:10,590
with what Nvidia publicly has published

00:22:07,260 --> 00:22:12,179
if you google GPU directs you'll see

00:22:10,590 --> 00:22:14,400
some of the pictures I had up there and

00:22:12,179 --> 00:22:19,559
a bunch of information and we're able to

00:22:14,400 --> 00:22:21,870
make it go so how do you like how often

00:22:19,559 --> 00:22:24,120
does the CPU have to interfere to update

00:22:21,870 --> 00:22:25,649
you know inform the GPU that hey that

00:22:24,120 --> 00:22:27,419
the may transfer is complete you can

00:22:25,649 --> 00:22:28,380
that that is an excellent question

00:22:27,419 --> 00:22:29,730
yeah and that's that's what

00:22:28,380 --> 00:22:32,070
the areas right now we're getting to the

00:22:29,730 --> 00:22:34,320
larger transport blocks will make a

00:22:32,070 --> 00:22:36,390
difference right now you're basically

00:22:34,320 --> 00:22:38,670
saying hey every time we have completed

00:22:36,390 --> 00:22:39,840
some number of DMA transactions we need

00:22:38,670 --> 00:22:41,070
to be able to know and we need to be

00:22:39,840 --> 00:22:43,260
able to signal and say all right you

00:22:41,070 --> 00:22:44,610
need to be able to the GPU is right it's

00:22:43,260 --> 00:22:47,400
ready to go right because that that's

00:22:44,610 --> 00:22:48,870
still piece the beauty of DMA is that

00:22:47,400 --> 00:22:50,550
nothing has to know about it something

00:22:48,870 --> 00:22:52,380
has to know about it something has to

00:22:50,550 --> 00:22:54,390
kind of keep pace with it we've looked

00:22:52,380 --> 00:22:56,160
at a couple of things we've looked at a

00:22:54,390 --> 00:22:57,960
couple ways to try to help I guess

00:22:56,160 --> 00:22:59,430
reduce that load and there's a balance

00:22:57,960 --> 00:23:01,350
between how much data you're willing to

00:22:59,430 --> 00:23:03,150
potentially miss and how often you

00:23:01,350 --> 00:23:04,290
really want to be able to inform the GPU

00:23:03,150 --> 00:23:06,780
that like you're good you can take the

00:23:04,290 --> 00:23:08,190
next block here and go with it so so

00:23:06,780 --> 00:23:08,880
there's that balance is definitely there

00:23:08,190 --> 00:23:12,980
it's going to end up being very

00:23:08,880 --> 00:23:15,690
application specific yep that's

00:23:12,980 --> 00:23:18,120
incredibly impressive but I would really

00:23:15,690 --> 00:23:21,840
like a long piece of fiber between my

00:23:18,120 --> 00:23:23,430
antennas ma SDR and my ugly data

00:23:21,840 --> 00:23:25,740
processing which I prefer to have an

00:23:23,430 --> 00:23:27,960
affair and a cage for a way how can you

00:23:25,740 --> 00:23:30,570
integrate this with 100 gig link or

00:23:27,960 --> 00:23:33,480
something in between so there is work

00:23:30,570 --> 00:23:37,800
that we've seen with folks trying to

00:23:33,480 --> 00:23:40,920
figure out how to plumb 10 gig 40 gig

00:23:37,800 --> 00:23:42,810
100 gig in a GPU direct right and you

00:23:40,920 --> 00:23:45,090
end up playing some similar games in

00:23:42,810 --> 00:23:46,830
other words imagine your network your

00:23:45,090 --> 00:23:48,690
kernel network stack where you're trying

00:23:46,830 --> 00:23:50,940
to allocate buffers that are in the GPU

00:23:48,690 --> 00:23:52,350
so normally when data is flowing in

00:23:50,940 --> 00:23:54,270
you're trying to feed it in from your

00:23:52,350 --> 00:23:57,270
network card you're actually trying to

00:23:54,270 --> 00:23:59,550
push it down to the GPU the little bit

00:23:57,270 --> 00:24:00,900
we looked at it it felt very clunky now

00:23:59,550 --> 00:24:03,780
there's no doubt about a thorough net is

00:24:00,900 --> 00:24:06,600
everywhere like networking is the future

00:24:03,780 --> 00:24:08,790
that there is a I think there is a

00:24:06,600 --> 00:24:10,410
argument to say that it's worth trying

00:24:08,790 --> 00:24:13,740
to figure out how you can do that in a

00:24:10,410 --> 00:24:17,340
more effective way from our view I mean

00:24:13,740 --> 00:24:19,620
from our view it does not feel like the

00:24:17,340 --> 00:24:20,970
most effective efficient way to actually

00:24:19,620 --> 00:24:22,770
get data into the gym and this is

00:24:20,970 --> 00:24:24,570
exactly what PCI Express was meant for

00:24:22,770 --> 00:24:26,100
right I would love to see people that

00:24:24,570 --> 00:24:27,630
are moving PCI Express over longer

00:24:26,100 --> 00:24:29,970
cables but I don't I think that's a kind

00:24:27,630 --> 00:24:31,170
of a long a long haul as well the other

00:24:29,970 --> 00:24:33,930
piece that we're really trying to push

00:24:31,170 --> 00:24:35,190
for is you have to put the processing

00:24:33,930 --> 00:24:36,900
near the antenna right there the other

00:24:35,190 --> 00:24:38,490
separation here is just to say what can

00:24:36,900 --> 00:24:41,370
we do to make that work or to figure out

00:24:38,490 --> 00:24:42,240
how that topology is a little bit more a

00:24:41,370 --> 00:24:43,679
little bit better

00:24:42,240 --> 00:24:45,630
to the problems and not necessarily

00:24:43,679 --> 00:24:47,910
having to dump everything everything

00:24:45,630 --> 00:24:49,559
back very impressive

00:24:47,910 --> 00:24:52,770
I'm just curious did you guys ever try

00:24:49,559 --> 00:24:56,820
to saturate the entire PCI bus go big

00:24:52,770 --> 00:24:58,800
Greg also a great question we this was

00:24:56,820 --> 00:25:01,740
as far as we took it in the time that we

00:24:58,800 --> 00:25:04,800
had available to it what we suspect is

00:25:01,740 --> 00:25:06,840
that we're probably running into how

00:25:04,800 --> 00:25:08,550
fast running the FPGA fabric that's

00:25:06,840 --> 00:25:09,840
interfacing to the DMA like there's

00:25:08,550 --> 00:25:11,070
gonna be this point where that's running

00:25:09,840 --> 00:25:12,900
at a couple hundred megahertz or

00:25:11,070 --> 00:25:14,429
whatever it is and that is going to be

00:25:12,900 --> 00:25:17,490
the thing that's actually driving it

00:25:14,429 --> 00:25:19,950
that logic is certainly capable of going

00:25:17,490 --> 00:25:23,580
faster on the FPGA side of the DMA

00:25:19,950 --> 00:25:24,600
engine side it just takes more we're

00:25:23,580 --> 00:25:25,920
getting in that zone where it's gonna

00:25:24,600 --> 00:25:27,510
we're gonna have to be real careful to

00:25:25,920 --> 00:25:29,550
be able to scale that up to say 300

00:25:27,510 --> 00:25:31,500
megahertz or 400 megahertz logic across

00:25:29,550 --> 00:25:33,000
a you know kind of a parallel buses

00:25:31,500 --> 00:25:36,390
before it hits the Saudis for actually

00:25:33,000 --> 00:25:38,179
getting over PCI Express okay and I

00:25:36,390 --> 00:25:41,070
think there's also no more questions

00:25:38,179 --> 00:25:42,900
so with that well I'll say thank you

00:25:41,070 --> 00:25:43,370
again thanks guys for talking in for

00:25:42,900 --> 00:25:47,270
sponsoring

00:25:43,370 --> 00:25:47,270

YouTube URL: https://www.youtube.com/watch?v=uROtYMuuksM


