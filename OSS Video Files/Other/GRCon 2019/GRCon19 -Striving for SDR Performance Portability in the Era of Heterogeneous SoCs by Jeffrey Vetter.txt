Title: GRCon19 -Striving for SDR Performance Portability in the Era of Heterogeneous SoCs by Jeffrey Vetter
Publication date: 2019-11-05
Playlist: GRCon 2019
Description: 
	Striving for SDR Performance Portability in the Era of Heterogeneous SoCs by Jeffrey Vetter, Mehmet Belviranli, Seyong Lee, Roberto Gioiosa, Abdel-Kareem Moadi

Future heterogeneous DSSoCs will be extraordinarily complex in terms of processors, memory hierarchies, and interconnection networks. To manage this complexity, architects, system software designers, and application developers need design and programming technologies to be flexible, accurate, efficient, and productive. Recently, our team has started to explore the mapping of GNU Radio to various heterogeneous SoCs in order to understand how programming technologies can support this goal of making this SDR framework performance portable. Using our software stack, we are porting several SDR applications to GPUs from NVIDIA, AMD, and ARM, and to NVIDIA Xavier SoCs, Qualcomm Snapdragon, and Xilinx Zynq devices. Our current approach uses a directive-based programming model and a new intelligent runtime scheduler to port and execute the workflows. The initial directive-based approach uses OpenACC as it is a portable, open programming model for heterogeneous systems. This compiler generates tasks that are then queued and scheduled by our intelligent runtime scheduler. Initial performance results appear promising; however, more automation will further broad deployment. Also, we have developed a host of tools to examine and profile SDR workflows and modules. Specifically, these analysis tools enable automated characterization of the behavioral and computational features of GNU Radio blocks and workflows. The static tools in GNR-tools help developers to create ontologies and queries to classify GNR modules based on custom scenarios. The dynamic toolset provides automated profiling capabilities of GNR workflows and presents detailed statistics on how components in a given software defined radio application perform. GNR-tools also produces graph-based representation of the analyzed data and provides powerful visualization options to filter and display the information obtained from the static and dynamic tools. Our software is available as open source software and will be made available to the community.
Captions: 
	00:00:14,670 --> 00:00:20,530
well good afternoon everyone I'm leading

00:00:18,010 --> 00:00:23,350
a team Oak Ridge except next generation

00:00:20,530 --> 00:00:26,520
architectures and Pathfinder emerging

00:00:23,350 --> 00:00:28,630
technologies for variety of sponsors and

00:00:26,520 --> 00:00:33,270
what I'm going to talk about today is

00:00:28,630 --> 00:00:35,650
some recent efforts we have start but

00:00:33,270 --> 00:00:38,079
before I dive into that however I'd like

00:00:35,650 --> 00:00:40,990
to talk a little bit about the recent

00:00:38,079 --> 00:00:44,530
trends in and I do this because there's

00:00:40,990 --> 00:00:46,120
a lot of doubt in at least the VOE

00:00:44,530 --> 00:00:48,280
community about where computer

00:00:46,120 --> 00:00:49,840
architectures are headed and so I'm

00:00:48,280 --> 00:00:54,100
going to spend a couple minutes talking

00:00:49,840 --> 00:00:56,170
about that with the goal of convincing

00:00:54,100 --> 00:01:00,130
you there's almost a state of crisis in

00:00:56,170 --> 00:01:03,100
software systems and applications at

00:01:00,130 --> 00:01:06,250
this point and that is because everyone

00:01:03,100 --> 00:01:09,310
is having right specialized code for

00:01:06,250 --> 00:01:12,340
individual architectures right even in

00:01:09,310 --> 00:01:14,140
VOE in many other places that's that's a

00:01:12,340 --> 00:01:18,250
big challenge we need to address right

00:01:14,140 --> 00:01:20,140
now our project is the coronal cosmic

00:01:18,250 --> 00:01:24,159
project is sponsored by the

00:01:20,140 --> 00:01:26,289
ess/sot program and we're looking at

00:01:24,159 --> 00:01:29,049
several of these challenges first one is

00:01:26,289 --> 00:01:30,670
really how do we even model these codes

00:01:29,049 --> 00:01:34,749
to know what architectures appropriate

00:01:30,670 --> 00:01:37,920
the second one is how do we write code

00:01:34,749 --> 00:01:40,210
this performance work right so we can

00:01:37,920 --> 00:01:42,670
legitimately run it on a couple of

00:01:40,210 --> 00:01:45,999
different architectures at least and see

00:01:42,670 --> 00:01:48,310
reasonable performance third is this

00:01:45,999 --> 00:01:50,560
idea of intelligent scheduling now you

00:01:48,310 --> 00:01:54,249
know when an smtp it won't be for SMB

00:01:50,560 --> 00:01:56,319
scheduling is relatively simple right

00:01:54,249 --> 00:01:57,880
our argument is that you need an

00:01:56,319 --> 00:01:59,649
intelligent scheduler because that

00:01:57,880 --> 00:02:01,929
intelligent schedule is going to have to

00:01:59,649 --> 00:02:04,509
know that the GPU to provide 10x

00:02:01,929 --> 00:02:06,729
performance on specific kernel

00:02:04,509 --> 00:02:08,890
rather than just scheduling if they're

00:02:06,729 --> 00:02:09,569
playing and so what we're doing is

00:02:08,890 --> 00:02:11,879
looking at

00:02:09,569 --> 00:02:15,959
intelligent scheduling systems try to

00:02:11,879 --> 00:02:18,689
try to help that and then finally this

00:02:15,959 --> 00:02:21,120
is you can think of this is future

00:02:18,689 --> 00:02:22,530
proofing but we want to target a wide

00:02:21,120 --> 00:02:25,500
variety of existing and future

00:02:22,530 --> 00:02:29,700
architectures both within PS sock as

00:02:25,500 --> 00:02:32,040
well as outside that some people are

00:02:29,700 --> 00:02:33,540
looking at agile chip development if you

00:02:32,040 --> 00:02:37,019
look at the Turing award lectures

00:02:33,540 --> 00:02:38,519
earlier this year you know they're

00:02:37,019 --> 00:02:40,109
talking about Angela chip develop

00:02:38,519 --> 00:02:49,730
they're talking about new chips coming

00:02:40,109 --> 00:02:53,189
out you know it's a big challenge so

00:02:49,730 --> 00:02:56,609
look at making trench some of this I'm

00:02:53,189 --> 00:02:58,439
sure many of you know but we know that a

00:02:56,609 --> 00:03:00,030
lot of our contemporary devices are

00:02:58,439 --> 00:03:01,439
approaching fundamental limits and there

00:03:00,030 --> 00:03:04,620
are a lot of papers out there that talk

00:03:01,439 --> 00:03:08,549
about the end of Dennard scaling Moore's

00:03:04,620 --> 00:03:10,980
law and so that's you know physically we

00:03:08,549 --> 00:03:14,220
know that that's also that way but but I

00:03:10,980 --> 00:03:16,319
think the more important thing is what

00:03:14,220 --> 00:03:20,909
it's doing to the business of boundaries

00:03:16,319 --> 00:03:22,319
and you look at this investments that

00:03:20,909 --> 00:03:25,139
the different companies are having to

00:03:22,319 --> 00:03:26,340
make Samsung for example is claiming

00:03:25,139 --> 00:03:29,340
they have to invest one hundred and

00:03:26,340 --> 00:03:31,709
fifteen billion dollars over the next

00:03:29,340 --> 00:03:36,470
ten years just to think about that

00:03:31,709 --> 00:03:36,470
number you know that about that's about

00:03:43,400 --> 00:03:49,709
that's in the hope they get it right and

00:03:46,409 --> 00:03:56,129
then we'll buy those products and you

00:03:49,709 --> 00:03:58,650
also see the number of well so this is

00:03:56,129 --> 00:04:00,299
how the impact on at least our art

00:03:58,650 --> 00:04:03,540
environment we're seeing a lot of

00:04:00,299 --> 00:04:05,669
acquisitions and other other business

00:04:03,540 --> 00:04:07,290
activity where companies are being

00:04:05,669 --> 00:04:10,319
bought changing roadmaps

00:04:07,290 --> 00:04:12,419
merging and and I would argue at least

00:04:10,319 --> 00:04:15,509
part of that is due to this complexity

00:04:12,419 --> 00:04:17,969
and cost that has to be invested and the

00:04:15,509 --> 00:04:20,310
inherent risk in that where they may get

00:04:17,969 --> 00:04:23,279
it wrong they may you know and don't

00:04:20,310 --> 00:04:25,710
like intelligent where ten nanometer

00:04:23,279 --> 00:04:28,169
didn't work out as well one time as they

00:04:25,710 --> 00:04:30,090
thought it would and so you have a deep

00:04:28,169 --> 00:04:36,210
taking starting to take big chunks of

00:04:30,090 --> 00:04:39,630
market share so you know I'm not here

00:04:36,210 --> 00:04:41,880
just to be pessimistic but I'd like to

00:04:39,630 --> 00:04:43,740
think of this as a basically a sixth

00:04:41,880 --> 00:04:45,529
wave of computing it to go back to rate

00:04:43,740 --> 00:04:48,750
curve balls book and you look at

00:04:45,529 --> 00:04:50,639
computing over the last century I really

00:04:48,750 --> 00:04:54,840
like this graph because it shows the

00:04:50,639 --> 00:04:57,300
major bucks in computing an integrated

00:04:54,840 --> 00:04:59,190
circuit one at the end is where we have

00:04:57,300 --> 00:05:08,669
been we've really benefited from that

00:04:59,190 --> 00:05:12,210
you just download and that's changing

00:05:08,669 --> 00:05:13,770
and so big picture this is something

00:05:12,210 --> 00:05:16,229
that I have to deal with for the next

00:05:13,770 --> 00:05:22,229
ten years at least probably and we've

00:05:16,229 --> 00:05:25,169
already started it's a great time to be

00:05:22,229 --> 00:05:27,930
a computer scientist so you promise to

00:05:25,169 --> 00:05:31,919
work on it mean opportunities there it's

00:05:27,930 --> 00:05:34,199
it's a good area to be so what's gonna

00:05:31,919 --> 00:05:36,449
happen during this so these are my three

00:05:34,199 --> 00:05:39,419
predictions just to kind of give an

00:05:36,449 --> 00:05:41,460
overview the first ones really optimized

00:05:39,419 --> 00:05:44,070
the software and expose new parallelism

00:05:41,460 --> 00:05:44,990
so India for example this is something

00:05:44,070 --> 00:05:50,060
we've been doing

00:05:44,990 --> 00:05:51,650
mpi opening others for a long time so we

00:05:50,060 --> 00:05:54,349
think a lot of other people don't start

00:05:51,650 --> 00:06:02,630
doing that - looking more carefully at

00:05:54,349 --> 00:06:05,240
their code understanding parallel ok the

00:06:02,630 --> 00:06:07,310
other thing is another the next big

00:06:05,240 --> 00:06:09,080
thing the new shiny object is the

00:06:07,310 --> 00:06:10,699
emergent technologies right so

00:06:09,080 --> 00:06:14,030
everyone's heard about quantum computing

00:06:10,699 --> 00:06:15,610
and there's a lot of a lot of funding a

00:06:14,030 --> 00:06:17,630
lot of initiatives for that right now

00:06:15,610 --> 00:06:19,880
the recent report from the National

00:06:17,630 --> 00:06:23,150
Academy of Engineering suggests that

00:06:19,880 --> 00:06:24,889
it'll be at least a decade before RSA

00:06:23,150 --> 00:06:27,020
factoring because he's threatened by

00:06:24,889 --> 00:06:28,820
quantum computing so there's a lot of

00:06:27,020 --> 00:06:32,000
engineering and a lot of challenges

00:06:28,820 --> 00:06:36,620
there neuromorphic computing is in the

00:06:32,000 --> 00:06:39,020
same boat their research 21 now we have

00:06:36,620 --> 00:06:41,210
advanced digital where carbon nanotubes

00:06:39,020 --> 00:06:44,720
have made some inroads just last week

00:06:41,210 --> 00:06:47,050
one of the other DARPA projects and the

00:06:44,720 --> 00:06:49,520
RI program announced they've created a

00:06:47,050 --> 00:06:52,099
16-bit processor running on carbon

00:06:49,520 --> 00:06:55,130
nanotubes right so it's very low

00:06:52,099 --> 00:06:57,710
frequency but it's interesting progress

00:06:55,130 --> 00:06:59,690
right and then finally the emerging

00:06:57,710 --> 00:07:05,830
memory devices what I mean by that is

00:06:59,690 --> 00:07:05,830
all you have things like flash drives in

00:07:05,860 --> 00:07:10,900
all those things are really making

00:07:08,870 --> 00:07:10,900
progress

00:07:12,130 --> 00:07:17,030
so finally here in the middle was this

00:07:14,659 --> 00:07:19,430
idea of architectural specialization and

00:07:17,030 --> 00:07:21,620
this is really the idea of using the

00:07:19,430 --> 00:07:23,360
CMOS we have today we're really

00:07:21,620 --> 00:07:26,150
specializing it for workloads

00:07:23,360 --> 00:07:28,729
and that means you're not just having a

00:07:26,150 --> 00:07:31,190
bigger cache or there may be a little

00:07:28,729 --> 00:07:33,560
faster or more cores

00:07:31,190 --> 00:07:37,039
we've actually take specialized

00:07:33,560 --> 00:07:38,940
functionality and put it on a chip it

00:07:37,039 --> 00:07:41,580
also means integrating

00:07:38,940 --> 00:07:43,740
these performances may not have to shift

00:07:41,580 --> 00:07:46,530
data back and forth between your devices

00:07:43,740 --> 00:07:50,190
and it really means paying attention to

00:07:46,530 --> 00:07:53,490
memory and storage as well and so that's

00:07:50,190 --> 00:07:56,010
what we're seeing across the board for

00:07:53,490 --> 00:08:00,300
the most part if you look at Google and

00:07:56,010 --> 00:08:02,790
Amazon and Facebook and on a lot of

00:08:00,300 --> 00:08:05,220
other processors that's what's happening

00:08:02,790 --> 00:08:08,400
right now they're hot chips for example

00:08:05,220 --> 00:08:10,200
of a semiconductor conference there are

00:08:08,400 --> 00:08:12,810
a lot of announcements about specialized

00:08:10,200 --> 00:08:15,720
SOC s for machine learning autonomous

00:08:12,810 --> 00:08:19,350
driving and many other areas plus you

00:08:15,720 --> 00:08:21,840
have things like GPUs obviously and and

00:08:19,350 --> 00:08:24,330
you know the mobile market has really

00:08:21,840 --> 00:08:26,520
led this trip if you look at this you

00:08:24,330 --> 00:08:28,380
know the iPhone 11 was just announced we

00:08:26,520 --> 00:08:30,390
don't have a chart for the iPhone 11 the

00:08:28,380 --> 00:08:32,700
ads but but if you just look at out

00:08:30,390 --> 00:08:36,890
those processors they've been growing

00:08:32,700 --> 00:08:40,410
dramatically because they've identified

00:08:36,890 --> 00:08:48,360
specific functionality that have a power

00:08:40,410 --> 00:08:49,950
performance improvement that they and

00:08:48,360 --> 00:08:52,260
then finally it was this idea of the

00:08:49,950 --> 00:08:55,830
risk by the ego system and so that's

00:08:52,260 --> 00:08:58,080
again you know that's growing rapidly I

00:08:55,830 --> 00:09:00,480
think they're expecting almost 2,000

00:08:58,080 --> 00:09:03,390
scientists and engineers at diversified

00:09:00,480 --> 00:09:05,280
of Summit this fall and it's open source

00:09:03,390 --> 00:09:08,930
you can go download a pair log for

00:09:05,280 --> 00:09:08,930
various cores and

00:09:10,540 --> 00:09:16,280
so just to close the circle on this so

00:09:15,020 --> 00:09:18,860
this transition period we're talking

00:09:16,280 --> 00:09:20,480
about is going to be disrupted there's a

00:09:18,860 --> 00:09:22,340
lot of opportunity are a lot of pitfalls

00:09:20,480 --> 00:09:25,970
if you look at this chart I've adapted

00:09:22,340 --> 00:09:28,160
for my quickly removing computing the

00:09:25,970 --> 00:09:30,790
point of it is the the darker colored

00:09:28,160 --> 00:09:33,860
cells really mean you have to rethink

00:09:30,790 --> 00:09:36,260
everything in the stack right or in that

00:09:33,860 --> 00:09:37,900
layer of the stack the things the lower

00:09:36,260 --> 00:09:40,580
layer like the new carbon energy

00:09:37,900 --> 00:09:42,350
transistor we may be able to cover open

00:09:40,580 --> 00:09:44,900
high but something like quantum

00:09:42,350 --> 00:09:46,810
computing under allure of computing you

00:09:44,900 --> 00:09:53,030
may need to redesign your entire

00:09:46,810 --> 00:09:53,960
algorithm and so we we've been seeing

00:09:53,030 --> 00:09:56,650
this at Overton

00:09:53,960 --> 00:10:00,020
so for the past eight or nine years

00:09:56,650 --> 00:10:02,480
we've seen a movement toward graphics

00:10:00,020 --> 00:10:05,660
processors in our large-scale systems

00:10:02,480 --> 00:10:07,370
and unfortunately at Oak Ridge right now

00:10:05,660 --> 00:10:13,120
we have the number one system on the top

00:10:07,370 --> 00:10:18,830
500 it has 27,000 voltage you can use it

00:10:13,120 --> 00:10:20,900
it draws almost 10 megawatts and it has

00:10:18,830 --> 00:10:22,580
these tensor Forbes on it so if you're

00:10:20,900 --> 00:10:23,150
thinking about machine learning in these

00:10:22,580 --> 00:10:26,210
days

00:10:23,150 --> 00:10:31,340
you think about a peak of almost 3.3

00:10:26,210 --> 00:10:33,200
Excise now the question is does your

00:10:31,340 --> 00:10:34,309
album really make use of those and I

00:10:33,200 --> 00:10:36,269
would argue

00:10:34,309 --> 00:10:38,489
specialized architecture that was

00:10:36,269 --> 00:10:44,279
created for machine learning and we need

00:10:38,489 --> 00:10:46,529
ways program that works and make the

00:10:44,279 --> 00:10:50,009
next system we have also going to be a

00:10:46,529 --> 00:10:52,559
GPU based system and it is it is a AMD

00:10:50,009 --> 00:10:57,600
GPU based system and so it's what would

00:10:52,559 --> 00:11:07,649
be about 1 and 1/2 X applause and focus

00:10:57,600 --> 00:11:09,299
primarily and so just if you look at all

00:11:07,649 --> 00:11:10,919
of the architectures we have there are 3

00:11:09,299 --> 00:11:33,179
main trends the first one is the

00:11:10,919 --> 00:11:38,850
heterogeneous course el capitan on the

00:11:33,179 --> 00:11:45,299
far right provided by Creator the choice

00:11:38,850 --> 00:11:47,220
of accelerator so the other thing is

00:11:45,299 --> 00:11:53,609
that memory hierarchies are changing

00:11:47,220 --> 00:11:56,699
rapidly as well as and so just to

00:11:53,609 --> 00:12:00,139
summarize on that you know we're seeing

00:11:56,699 --> 00:12:02,910
a lot of these effects in Yui and the

00:12:00,139 --> 00:12:07,730
applications are primarily climate and

00:12:02,910 --> 00:12:11,069
other types of large large teams right

00:12:07,730 --> 00:12:13,279
recently Dartmouth started the

00:12:11,069 --> 00:12:16,499
domain-specific system ownership program

00:12:13,279 --> 00:12:18,989
to address similar challenges but for

00:12:16,499 --> 00:12:20,970
streaming types of applications and this

00:12:18,989 --> 00:12:23,850
includes things like vision and

00:12:20,970 --> 00:12:27,339
software-defined radio these are the

00:12:23,850 --> 00:12:30,739
five performance right now this program

00:12:27,339 --> 00:12:33,589
Program Manager is Rhonda we've heard

00:12:30,739 --> 00:12:34,970
problem earlier but we're trying to

00:12:33,589 --> 00:12:39,470
address some of the challenges of

00:12:34,970 --> 00:12:41,899
heterogeneous processing with this I'm

00:12:39,470 --> 00:12:46,429
going to talk about our project called

00:12:41,899 --> 00:12:54,350
cosmic and so you know it's a whirlwind

00:12:46,429 --> 00:12:57,769
tour there's a lot of things to cover so

00:12:54,350 --> 00:13:00,470
think big picture so I come to you and

00:12:57,769 --> 00:13:03,290
say I've got a billion transistors what

00:13:00,470 --> 00:13:06,350
do you want on the chip how did you

00:13:03,290 --> 00:13:07,579
decide that and so the first thing we're

00:13:06,350 --> 00:13:11,230
trying to do is come up with the

00:13:07,579 --> 00:13:14,209
methodologies to look at SDR

00:13:11,230 --> 00:13:16,399
applications and then decide what needs

00:13:14,209 --> 00:13:19,009
to be migrated in the hardware of the

00:13:16,399 --> 00:13:22,160
chip and we're doing that with a

00:13:19,009 --> 00:13:24,499
technique called ontology ontologies is

00:13:22,160 --> 00:13:26,660
really a way to break up all

00:13:24,499 --> 00:13:32,059
functionality you can have in something

00:13:26,660 --> 00:13:34,509
like GNR suite or ot suite and look at

00:13:32,059 --> 00:13:37,009
it and understand what functionality

00:13:34,509 --> 00:13:41,059
necessary and then you can map that onto

00:13:37,009 --> 00:13:43,309
different accelerators for example the

00:13:41,059 --> 00:13:45,860
second thing is the actual chip design

00:13:43,309 --> 00:13:47,779
and three of the programs

00:13:45,860 --> 00:13:50,149
I'm sorry three of the projects in the

00:13:47,779 --> 00:13:52,399
PS socket is building jobs they're

00:13:50,149 --> 00:13:54,259
building chips were expecting the senior

00:13:52,399 --> 00:13:59,209
chips come out of this program will have

00:13:54,259 --> 00:14:00,830
to write write software for and then

00:13:59,209 --> 00:14:08,839
those chips and we should

00:14:00,830 --> 00:14:11,990
and the programming system can also take

00:14:08,839 --> 00:14:15,820
advantage of that ontology because in

00:14:11,990 --> 00:14:15,820
the end up you need to understand what

00:14:16,029 --> 00:14:23,329
well as the data movement runtime

00:14:21,410 --> 00:14:26,300
scheduling the same way for two indents

00:14:23,329 --> 00:14:28,850
versus farce or something like a random

00:14:26,300 --> 00:14:31,700
memory access all those things influence

00:14:28,850 --> 00:14:33,410
the downstream software stack decisions

00:14:31,700 --> 00:14:35,180
that you're making and then at the end

00:14:33,410 --> 00:14:37,610
we have what we're calling the

00:14:35,180 --> 00:14:40,190
performance way to die and what that

00:14:37,610 --> 00:14:41,959
does is this really allow you to see an

00:14:40,190 --> 00:14:42,829
information back now why is that

00:14:41,959 --> 00:14:45,470
important

00:14:42,829 --> 00:14:48,470
it gets back to this runtime schedule so

00:14:45,470 --> 00:14:50,269
if you look for runtime scheduler it

00:14:48,470 --> 00:14:51,800
really needs to understand how long it's

00:14:50,269 --> 00:14:57,140
going to take to finish a task for this

00:14:51,800 --> 00:14:59,480
kind of computer and that's really hard

00:14:57,140 --> 00:15:02,570
to do in this case you can do things

00:14:59,480 --> 00:15:04,519
like count flops count cycle but we're

00:15:02,570 --> 00:15:06,920
trying to understand how we could use

00:15:04,519 --> 00:15:08,660
that in perspective layer feed

00:15:06,920 --> 00:15:13,100
information back and talk about where to

00:15:08,660 --> 00:15:17,209
schedule the next pass we've broken this

00:15:13,100 --> 00:15:18,640
down into several basic categories that

00:15:17,209 --> 00:15:21,649
I'm going to talk about briefly

00:15:18,640 --> 00:15:24,230
applications that compiler geocell layer

00:15:21,649 --> 00:15:26,089
the runtime system and then the

00:15:24,230 --> 00:15:28,070
performance modeling level the

00:15:26,089 --> 00:15:29,750
performance modeling is really the glue

00:15:28,070 --> 00:15:32,029
that pulls this all together

00:15:29,750 --> 00:15:33,860
and the reason that's true is because it

00:15:32,029 --> 00:15:36,970
gives you foresight it gives you a way

00:15:33,860 --> 00:15:41,270
to predict what the next computation

00:15:36,970 --> 00:15:44,899
runtime as well as which device

00:15:41,270 --> 00:15:46,520
that's performance so just to give you

00:15:44,899 --> 00:15:50,149
an idea the hardware we're looking at

00:15:46,520 --> 00:15:52,130
right now we want to look at many so

00:15:50,149 --> 00:15:54,470
we're targeting all these devices right

00:15:52,130 --> 00:15:56,680
now we have some of that we're looking

00:15:54,470 --> 00:16:03,740
at with six folders in it the

00:15:56,680 --> 00:16:08,149
workstation and the radeon arm under x2

00:16:03,740 --> 00:16:12,500
node we're also running on an FPGA

00:16:08,149 --> 00:16:13,490
graphics ten more interestingly I want

00:16:12,500 --> 00:16:17,029
to get back to this

00:16:13,490 --> 00:16:19,279
xaviar which is just a motive and as you

00:16:17,029 --> 00:16:22,070
see on the right this was made primarily

00:16:19,279 --> 00:16:25,580
for autonomous driving and other types

00:16:22,070 --> 00:16:29,029
of tasks but it includes an inference

00:16:25,580 --> 00:16:32,240
engine the vision processor includes

00:16:29,029 --> 00:16:35,980
both GPUs and our board and so all of

00:16:32,240 --> 00:16:39,950
those are are in that one association

00:16:35,980 --> 00:16:41,810
also you've got Qualcomm Snapdragon and

00:16:39,950 --> 00:16:43,610
so another another very very popular

00:16:41,810 --> 00:16:46,329
chip I'm sure there's there's some of

00:16:43,610 --> 00:16:49,550
these floating around it has a similarly

00:16:46,329 --> 00:16:55,390
complex organization in terms of the

00:16:49,550 --> 00:16:55,390
functionality it has its own AI engine

00:16:59,110 --> 00:17:05,120
and so that's the applications that I'm

00:17:03,920 --> 00:17:06,860
sorry that's the hardware we were

00:17:05,120 --> 00:17:08,510
looking at I want to say a little bit

00:17:06,860 --> 00:17:11,089
about the applications since you're

00:17:08,510 --> 00:17:16,579
you're the target application area any

00:17:11,089 --> 00:17:19,910
of this but what we're looking at doing

00:17:16,579 --> 00:17:21,860
right now is having something like the

00:17:19,910 --> 00:17:24,140
xaviar running on each side running the

00:17:21,860 --> 00:17:28,550
workflow and being able to have it in

00:17:24,140 --> 00:17:32,900
the end workflow with both of these

00:17:28,550 --> 00:17:34,580
see and we're using GNR to do that in

00:17:32,900 --> 00:17:39,500
this example we're just using the Wi-Fi

00:17:34,580 --> 00:17:42,920
protocol and again the goal was not to

00:17:39,500 --> 00:17:45,320
come up with a new SDR workflow it's to

00:17:42,920 --> 00:17:47,150
make sure we can accelerate those

00:17:45,320 --> 00:17:51,620
individual modules some of the

00:17:47,150 --> 00:17:55,550
technologies that I just described so

00:17:51,620 --> 00:17:57,530
since we were new we went to the GN our

00:17:55,550 --> 00:18:01,340
meeting last year in Henderson which was

00:17:57,530 --> 00:18:03,650
a lot harder but we wanted to try the

00:18:01,340 --> 00:18:06,440
profile and understand kind of

00:18:03,650 --> 00:18:09,320
everything that went into radio just

00:18:06,440 --> 00:18:10,640
from a clean slate perspective so we

00:18:09,320 --> 00:18:13,460
went in and started doing a lot of

00:18:10,640 --> 00:18:18,470
application profile we've built some

00:18:13,460 --> 00:18:24,980
tools allow us to profile make those

00:18:18,470 --> 00:18:26,630
available this will be really important

00:18:24,980 --> 00:18:29,570
when we're looking at you know what

00:18:26,630 --> 00:18:32,240
accelerators put on the chip and what

00:18:29,570 --> 00:18:36,440
pieces of code we need to accelerate we

00:18:32,240 --> 00:18:39,200
have several different scripts and not

00:18:36,440 --> 00:18:41,930
just an entire or everything you have

00:18:39,200 --> 00:18:44,050
basically a period in our repo this

00:18:41,930 --> 00:18:46,940
includes creating block level ontology

00:18:44,050 --> 00:18:50,450
it grabs a bunch of basic properties out

00:18:46,940 --> 00:18:52,370
of all of those modules things like some

00:18:50,450 --> 00:18:54,560
reports and the data types and all of

00:18:52,370 --> 00:18:56,830
that so you can organize it and look at

00:18:54,560 --> 00:18:59,240
it the second one is really a flow graph

00:18:56,830 --> 00:19:01,700
characterization and what it does is it

00:18:59,240 --> 00:19:04,250
automatically runs authors and generates

00:19:01,700 --> 00:19:05,419
profile they give you some idea of like

00:19:04,250 --> 00:19:08,179
what's coming

00:19:05,419 --> 00:19:10,759
intense about that module and then

00:19:08,179 --> 00:19:13,519
finally there's really a design space

00:19:10,759 --> 00:19:15,889
exploration where you can run 13 blocks

00:19:13,519 --> 00:19:18,859
and get some idea of what the

00:19:15,889 --> 00:19:24,559
scalability functionality is different

00:19:18,859 --> 00:19:32,509
types of devices and there's our our URL

00:19:24,559 --> 00:19:34,009
and slightly available the other thing

00:19:32,509 --> 00:19:35,959
that we did was look at proximity

00:19:34,009 --> 00:19:38,479
analysis and so what this means is we

00:19:35,959 --> 00:19:41,179
looked across a subset of all the

00:19:38,479 --> 00:19:44,690
workflows to identify modules that

00:19:41,179 --> 00:19:48,200
appear near each other in terms of being

00:19:44,690 --> 00:19:50,089
connected many times now this is at

00:19:48,200 --> 00:19:53,179
first light it's just something trivial

00:19:50,089 --> 00:19:55,190
but the idea is that that if you have

00:19:53,179 --> 00:19:57,639
functionality you want to put hardware

00:19:55,190 --> 00:20:00,679
you may be able to take some of these

00:19:57,639 --> 00:20:02,509
connected components and really use them

00:20:00,679 --> 00:20:06,259
together and actually provide one

00:20:02,509 --> 00:20:10,219
component to do that Hardware activity

00:20:06,259 --> 00:20:13,209
and it very clearly identifies what some

00:20:10,219 --> 00:20:21,379
of these components are that's in the

00:20:13,209 --> 00:20:23,809
URL then so moving on looking at Aspen's

00:20:21,379 --> 00:20:25,369
the performance modeling tool now again

00:20:23,809 --> 00:20:27,259
why do we need that we need to be able

00:20:25,369 --> 00:20:28,700
to figure out what the components are

00:20:27,259 --> 00:20:32,299
how they're connected what their

00:20:28,700 --> 00:20:33,859
performance characteristics are so Aspen

00:20:32,299 --> 00:20:37,129
is something we created about nine years

00:20:33,859 --> 00:20:39,589
ago it's performance modeling notation

00:20:37,129 --> 00:20:43,759
and basically you have some source code

00:20:39,589 --> 00:20:45,739
you can I just source code with compiler

00:20:43,759 --> 00:20:48,330
and create an asymptote you can write

00:20:45,739 --> 00:20:50,310
the asymptotes out you know in editor

00:20:48,330 --> 00:20:52,020
max or something and then once you have

00:20:50,310 --> 00:20:54,090
that you can use for a lot of different

00:20:52,020 --> 00:20:56,790
things and that can be design space

00:20:54,090 --> 00:20:59,820
exploration or feedback to your runtime

00:20:56,790 --> 00:21:03,210
system and we've demonstrated that in

00:20:59,820 --> 00:21:07,610
multiple pages we're trying to adapt it

00:21:03,210 --> 00:21:11,130
now for SDR so what does that look like

00:21:07,610 --> 00:21:13,920
so this is the new radio flow graph and

00:21:11,130 --> 00:21:16,380
Aspen remodel conversion that we've done

00:21:13,920 --> 00:21:18,600
so far what you see on the left hand

00:21:16,380 --> 00:21:23,070
side there is really a high-level view

00:21:18,600 --> 00:21:25,140
of the overall workflow right and that

00:21:23,070 --> 00:21:34,650
pulls together the components and then

00:21:25,140 --> 00:21:36,900
parameters and allows you to so the Box

00:21:34,650 --> 00:21:40,380
on the right shows you what the output

00:21:36,900 --> 00:21:43,110
or the performance model is for this one

00:21:40,380 --> 00:21:44,610
of those specific modules and that's

00:21:43,110 --> 00:21:46,020
important because that really gives you

00:21:44,610 --> 00:21:51,980
an indication of what the computational

00:21:46,020 --> 00:21:51,980
complexity to be right and in many cases

00:21:58,760 --> 00:22:04,260
this is really informative if you have

00:22:01,260 --> 00:22:06,030
one of these files for everyone it

00:22:04,260 --> 00:22:10,620
really gives you a way to figure out

00:22:06,030 --> 00:22:18,630
what the compute ratio memory intensity

00:22:10,620 --> 00:22:21,390
of the last one is complexity

00:22:18,630 --> 00:22:23,970
everybody's type LS HW at one time or

00:22:21,390 --> 00:22:25,860
another our you know our architectures

00:22:23,970 --> 00:22:29,090
are getting far more complex than that

00:22:25,860 --> 00:22:31,530
and so what what we're creating is a

00:22:29,090 --> 00:22:32,760
abstraction model basically let you

00:22:31,530 --> 00:22:35,400
write out with a different component

00:22:32,760 --> 00:22:38,580
soft and innocent X here is you know it

00:22:35,400 --> 00:22:40,110
exists but somewhat arbitrary we're

00:22:38,580 --> 00:22:42,300
trying to come up with a way to describe

00:22:40,110 --> 00:22:44,550
the different functionality in these

00:22:42,300 --> 00:22:54,390
devices so that will

00:22:44,550 --> 00:22:57,090
the math of schedule 1 so the compiler

00:22:54,390 --> 00:22:59,310
and programming system is one of the

00:22:57,090 --> 00:23:02,610
things we've been working on for several

00:22:59,310 --> 00:23:08,850
years what we do there is we take an

00:23:02,610 --> 00:23:10,740
approach that level so that the compiler

00:23:08,850 --> 00:23:13,380
can actually generate code for that

00:23:10,740 --> 00:23:15,270
target and then hand it off to a

00:23:13,380 --> 00:23:19,320
back-end system for just-in-time

00:23:15,270 --> 00:23:22,410
compiling and so the compiler we have

00:23:19,320 --> 00:23:25,170
this called open are the important thing

00:23:22,410 --> 00:23:28,440
to realize is with open are it really

00:23:25,170 --> 00:23:30,810
tastes open ACC directives or over them

00:23:28,440 --> 00:23:32,570
before offload directive and can

00:23:30,810 --> 00:23:34,980
generate a high-level intermediate

00:23:32,570 --> 00:23:39,710
representation for those of you familiar

00:23:34,980 --> 00:23:43,140
with LLB Mir LOV Mir is a very low of

00:23:39,710 --> 00:23:45,930
this ir is much much higher and what it

00:23:43,140 --> 00:23:48,990
captures are things like nested loops

00:23:45,930 --> 00:23:52,530
captures kernel launches it captures

00:23:48,990 --> 00:23:54,840
data transfers and so with that we can

00:23:52,530 --> 00:23:58,380
pass that off to things like our auto

00:23:54,840 --> 00:24:00,870
tuner in order to find out what the

00:23:58,380 --> 00:24:03,000
perfect mapping it for a specific

00:24:00,870 --> 00:24:06,020
architecture and then we generate that

00:24:03,000 --> 00:24:06,020
from the back end

00:24:12,260 --> 00:24:17,660
what are some of the targets we

00:24:14,000 --> 00:24:20,000
currently support right now we support

00:24:17,660 --> 00:24:23,000
all these and most of them are through

00:24:20,000 --> 00:24:26,990
OpenCL you know there's a lot of a lot

00:24:23,000 --> 00:24:30,169
of people questioning OpenCL we find it

00:24:26,990 --> 00:24:34,070
horrible it's almost everywhere there's

00:24:30,169 --> 00:24:36,590
a lot of a lot of potential in OpenCL

00:24:34,070 --> 00:24:39,049
just for that purpose and so we've been

00:24:36,590 --> 00:24:57,110
focusing on that we're recently even

00:24:39,049 --> 00:24:59,419
looking at generating code automatic and

00:24:57,110 --> 00:25:01,760
so this is just an example of the

00:24:59,419 --> 00:25:03,950
performance analysis mentioned earlier

00:25:01,760 --> 00:25:07,250
we took a few of the blocks and run them

00:25:03,950 --> 00:25:08,990
and found out what the actual you know

00:25:07,250 --> 00:25:12,169
performance was with different buffer

00:25:08,990 --> 00:25:17,799
sizes of these individual GNR blocks and

00:25:12,169 --> 00:25:17,799
this is generated by our fire compiler

00:25:18,640 --> 00:25:25,410
and this it also tells you a little bit

00:25:21,830 --> 00:25:32,880
about the actual performance predicted

00:25:25,410 --> 00:25:34,680
the compiler itself so moving on to

00:25:32,880 --> 00:25:37,740
runtime system I just wanted to give

00:25:34,680 --> 00:25:39,930
some highlights here the runtime system

00:25:37,740 --> 00:25:42,180
as I mentioned earlier is really

00:25:39,930 --> 00:25:49,740
important and I think over the past

00:25:42,180 --> 00:25:51,990
decade or more really fast but also

00:25:49,740 --> 00:25:54,630
really similar and so what we're

00:25:51,990 --> 00:25:56,790
thinking just having a real-time

00:25:54,630 --> 00:25:57,930
scheduler that looks like this and this

00:25:56,790 --> 00:26:00,860
is something that we're actually

00:25:57,930 --> 00:26:07,830
building but it's really a framework

00:26:00,860 --> 00:26:10,260
programming extremely many different

00:26:07,830 --> 00:26:12,480
application feed them through this

00:26:10,260 --> 00:26:14,550
intelligent scheduler intelligent

00:26:12,480 --> 00:26:21,450
scheduler has multiple queues it can

00:26:14,550 --> 00:26:23,340
take those and it has things like that

00:26:21,450 --> 00:26:25,170
and then see so it can manage those

00:26:23,340 --> 00:26:33,900
dependencies as well as the data

00:26:25,170 --> 00:26:36,840
movement again the actual past some

00:26:33,900 --> 00:26:39,030
cells would be generated by the compiler

00:26:36,840 --> 00:26:41,130
they look right down kind of like OpenCL

00:26:39,030 --> 00:26:41,820
tasks or slightly higher levels and

00:26:41,130 --> 00:26:50,310
OpenCL

00:26:41,820 --> 00:26:53,550
but they get fed to this and this is an

00:26:50,310 --> 00:26:55,770
example of SDR so we've got multiple

00:26:53,550 --> 00:26:57,750
workflows here I can just pick one which

00:26:55,770 --> 00:26:59,180
is the one in the back you can think of

00:26:57,750 --> 00:27:02,030
getting some

00:26:59,180 --> 00:27:04,160
parallel is about pipelining and then

00:27:02,030 --> 00:27:05,720
you can get some pastoralism out of the

00:27:04,160 --> 00:27:07,190
different modules you have in the graph

00:27:05,720 --> 00:27:10,220
and then each one of those may have

00:27:07,190 --> 00:27:12,680
absolutely failed you stack on top of

00:27:10,220 --> 00:27:17,590
that multiple workloads running on this

00:27:12,680 --> 00:27:17,590
device and that's where the scheduling

00:27:25,150 --> 00:27:32,510
the current system we have looks like

00:27:28,460 --> 00:27:36,020
this I know it's got a lot of detail set

00:27:32,510 --> 00:27:39,700
but our compiler can generate again a

00:27:36,020 --> 00:27:43,310
variety of code OpenCL

00:27:39,700 --> 00:27:45,080
Crudup and it can then lower it and hand

00:27:43,310 --> 00:27:47,930
it off to the runtime system once the

00:27:45,080 --> 00:28:03,950
filer generates that code along with the

00:27:47,930 --> 00:28:09,110
dependency description and right now for

00:28:03,950 --> 00:28:14,750
all of these we have OpenCL working on

00:28:09,110 --> 00:28:17,030
that but right now we recognize the

00:28:14,750 --> 00:28:20,690
tendency so across those devices you can

00:28:17,030 --> 00:28:28,220
have dependency and the system itself

00:28:20,690 --> 00:28:34,370
the runtime system itself it should be

00:28:28,220 --> 00:28:36,440
start executing this is a kind of a

00:28:34,370 --> 00:28:38,810
different view this is what we see when

00:28:36,440 --> 00:28:42,230
you're actually starting to execute you

00:28:38,810 --> 00:28:44,660
have you know ready cube task scheduler

00:28:42,230 --> 00:28:46,400
that path scheduler you know you can

00:28:44,660 --> 00:28:48,560
make choice as many different ways and

00:28:46,400 --> 00:28:51,140
this reporter was getting to earlier

00:28:48,560 --> 00:28:53,900
where you have a very simple scheduling

00:28:51,140 --> 00:28:56,570
policies such as round-robin just you

00:28:53,900 --> 00:28:57,770
know the first thing with you there are

00:28:56,570 --> 00:29:00,270
a lot of things you can do there

00:28:57,770 --> 00:29:02,770
there's profile there

00:29:00,270 --> 00:29:05,140
the oncologists kitchen their

00:29:02,770 --> 00:29:06,280
performance models like Aspen and we're

00:29:05,140 --> 00:29:08,800
not quite ready yet

00:29:06,280 --> 00:29:10,480
to be able to evaluate all those but

00:29:08,800 --> 00:29:12,760
that's really the next step in what

00:29:10,480 --> 00:29:16,600
we're doing and so you want to be able

00:29:12,760 --> 00:29:19,030
to have all those events from SDR

00:29:16,600 --> 00:29:21,910
application community and then be able

00:29:19,030 --> 00:29:30,250
amount to a specific accelerator based

00:29:21,910 --> 00:29:32,500
on initially as I mentioned earlier this

00:29:30,250 --> 00:29:36,720
past Hitler also has to know how many

00:29:32,500 --> 00:29:36,720
confuse are for each one of those and

00:29:43,800 --> 00:29:49,750
finally the introspective layer that I

00:29:47,950 --> 00:29:51,970
mentioned earlier is something that

00:29:49,750 --> 00:29:54,100
we're you know we started out really not

00:29:51,970 --> 00:29:57,280
having an introspective layer in our

00:29:54,100 --> 00:29:59,050
entire system we're adding that this

00:29:57,280 --> 00:30:01,870
employs things like just using the

00:29:59,050 --> 00:30:05,350
performance and power counters even use

00:30:01,870 --> 00:30:07,720
things like a temperature sensor to feed

00:30:05,350 --> 00:30:11,200
that back and perhaps and complete that

00:30:07,720 --> 00:30:12,730
in the runtime system decision now it's

00:30:11,200 --> 00:30:14,410
questionable whether you can do that

00:30:12,730 --> 00:30:18,510
effectively at all because there's all

00:30:14,410 --> 00:30:21,419
kinds of challenges like the aliasing

00:30:18,510 --> 00:30:30,360
close that quick enough but but that's

00:30:21,419 --> 00:30:34,260
something that we're so just to finish

00:30:30,360 --> 00:30:38,790
up so you know I hope I convinced you

00:30:34,260 --> 00:30:40,679
there are a lot of opportunity you know

00:30:38,790 --> 00:30:41,130
it's going to get worse before it gets

00:30:40,679 --> 00:30:44,990
better

00:30:41,130 --> 00:30:48,419
take my word there's a lot of challenges

00:30:44,990 --> 00:30:51,630
coming into architecture the software

00:30:48,419 --> 00:30:54,030
systems really are not ready and we need

00:30:51,630 --> 00:30:55,799
ways to focus on performance portability

00:30:54,030 --> 00:31:00,900
so you don't have to rewrite every tenet

00:30:55,799 --> 00:31:03,059
for every platform our cosmic project is

00:31:00,900 --> 00:31:06,440
really looking at design and programming

00:31:03,059 --> 00:31:09,090
challenges for SDR and we're focusing on

00:31:06,440 --> 00:31:11,820
things I mentioned early performance

00:31:09,090 --> 00:31:15,419
modeling performance support of the

00:31:11,820 --> 00:31:17,790
compilation intelligent scheduling and

00:31:15,419 --> 00:31:19,350
then you know it's not enough to get

00:31:17,790 --> 00:31:22,350
excellent performance from one

00:31:19,350 --> 00:31:24,059
architecture what we have to do is be

00:31:22,350 --> 00:31:25,830
able to pick this code up and run it

00:31:24,059 --> 00:31:28,169
across you know five or ten

00:31:25,830 --> 00:31:29,940
architectures and yet 80% of the

00:31:28,169 --> 00:31:35,100
performance you would if you had to do

00:31:29,940 --> 00:31:37,490
it manually with the next and so that's

00:31:35,100 --> 00:31:37,490
what

00:31:37,570 --> 00:31:41,360
so with that I thank you for your

00:31:39,950 --> 00:31:43,620
attention and you have been answering

00:31:41,360 --> 00:31:50,300
questions

00:31:43,620 --> 00:31:50,300

YouTube URL: https://www.youtube.com/watch?v=Y5wk1WnAeq4


