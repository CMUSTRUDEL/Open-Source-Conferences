Title: JUC U.S. East 2015 - Automating a Big Data Platform with Jenkins
Publication date: 2020-01-23
Playlist: JUC U.S East 2015
Description: 
	By: Sander Kieft, Sanoma 

Sanoma is the largest publisher in The Netherlands and Finland, running some of the largest websites and mobile applications in those regions. Since 2010, Sanoma has been using Hadoop to process big data and gain insights into their products, customers and advertisers. After using traditional ETL tools for data ingestion and process management, Sanoma moved to Jenkins in 2012 to automate the big data infrastructure. This presentation is about the past, present and future of the Sanoma Data Platform and the role Jenkins plays within it. Attendees will get a brief introduction of the challenges involved with big data and the way they have been tackled at Sanoma. Sander will also touch on the pains and joys he and his team get from Jenkins.
Captions: 
	00:00:15,240 --> 00:00:21,250
yeah thanks for coming to

00:00:17,830 --> 00:00:26,770
this talk it's about automating big data

00:00:21,250 --> 00:00:29,410
platform with with Jenkins and it's it's

00:00:26,770 --> 00:00:31,540
classified as being a non-technical talk

00:00:29,410 --> 00:00:33,640
I would say it's like a less technical

00:00:31,540 --> 00:00:37,890
talk there is some slides with codes but

00:00:33,640 --> 00:00:40,210
it's yeah it's just for illustration and

00:00:37,890 --> 00:00:42,510
we need to talk about the past present

00:00:40,210 --> 00:00:45,940
and future of the big data platform we

00:00:42,510 --> 00:00:48,190
we built and the role Jenkins place in

00:00:45,940 --> 00:00:53,790
that in in serving the the Big Data

00:00:48,190 --> 00:00:56,979
elephant about me I'm Sonic if some

00:00:53,790 --> 00:01:00,549
manager for Oklahoma components within

00:00:56,979 --> 00:01:02,790
within cinema and one of those

00:01:00,549 --> 00:01:07,479
components is is the big data platform

00:01:02,790 --> 00:01:09,369
and cinema is a publishing company we

00:01:07,479 --> 00:01:11,830
publish about the hundreds consumer

00:01:09,369 --> 00:01:16,270
magazines and the two largest newspapers

00:01:11,830 --> 00:01:18,420
in in Finland and we have like five

00:01:16,270 --> 00:01:23,260
television stations both in nawlins and

00:01:18,420 --> 00:01:24,940
I'm Finland and about 200 websites under

00:01:23,260 --> 00:01:27,010
weights like the largest new site in the

00:01:24,940 --> 00:01:30,880
Netherlands watch sport or loss price

00:01:27,010 --> 00:01:33,760
comparison and we have about 100 mobile

00:01:30,880 --> 00:01:39,610
applications and these are generating

00:01:33,760 --> 00:01:41,830
like a lot of data and in the past we

00:01:39,610 --> 00:01:44,560
ran all this infrastructure and did all

00:01:41,830 --> 00:01:48,190
the analyses on like the systems that

00:01:44,560 --> 00:01:49,600
provided the data because already back

00:01:48,190 --> 00:01:51,880
in the day the data from the web

00:01:49,600 --> 00:01:54,850
analytics system and advertising systems

00:01:51,880 --> 00:01:58,120
were too big to to be able to handle in

00:01:54,850 --> 00:01:59,920
a traditional bi bi system so we got

00:01:58,120 --> 00:02:03,220
like the high level aggregates combined

00:01:59,920 --> 00:02:07,900
them in Excel and then use that for for

00:02:03,220 --> 00:02:11,019
reporting but somewhere in 2010 a use

00:02:07,900 --> 00:02:13,269
case came along for starting to combine

00:02:11,019 --> 00:02:17,180
this this data and we've been toying

00:02:13,269 --> 00:02:22,640
around to distinguish called Hadoop and

00:02:17,180 --> 00:02:24,920
in 2010 also a simpler way of accessing

00:02:22,640 --> 00:02:28,129
that data came available which was high

00:02:24,920 --> 00:02:31,040
it's like a sequel layer on top of on

00:02:28,129 --> 00:02:33,889
top of a dupe so we start to this

00:02:31,040 --> 00:02:38,590
project to combine the data from those

00:02:33,889 --> 00:02:40,639
various from those various sources and

00:02:38,590 --> 00:02:43,700
to give you a bit of an introduction

00:02:40,639 --> 00:02:46,959
about about big data by the way who's

00:02:43,700 --> 00:02:49,790
familiar with with big data and Hadoop

00:02:46,959 --> 00:02:51,919
ok that's good i put in this life

00:02:49,790 --> 00:02:56,090
because yesterday at the drinks i was

00:02:51,919 --> 00:02:58,099
done to a few people and yeah they

00:02:56,090 --> 00:03:00,379
pointed out or maybe do some sort of

00:02:58,099 --> 00:03:06,560
introduction because maybe not everyone

00:03:00,379 --> 00:03:08,480
is familiar and what we wanted from from

00:03:06,560 --> 00:03:11,150
the daily we we had a few particular use

00:03:08,480 --> 00:03:13,489
case we want to solve but we knew that

00:03:11,150 --> 00:03:16,310
there was in the day that a lot more of

00:03:13,489 --> 00:03:19,549
value but which is that just didn't know

00:03:16,310 --> 00:03:23,389
how to extract it yet so we really

00:03:19,549 --> 00:03:25,430
bought into this idea with in science

00:03:23,389 --> 00:03:29,060
one man's noise is the other man signal

00:03:25,430 --> 00:03:31,129
so what some people consider to be

00:03:29,060 --> 00:03:33,169
garbage in the data other people can

00:03:31,129 --> 00:03:36,859
extract value out of and maybe not today

00:03:33,169 --> 00:03:39,439
but maybe in in two months time they

00:03:36,859 --> 00:03:43,040
think hey this is brilliant decision the

00:03:39,439 --> 00:03:45,979
data we can leverage that so with a

00:03:43,040 --> 00:03:49,939
traditional system the way it goes you

00:03:45,979 --> 00:03:52,189
get all this data you refine it and you

00:03:49,939 --> 00:03:56,359
put it in your operational data mart so

00:03:52,189 --> 00:03:59,000
see that's bottled water the rest goes

00:03:56,359 --> 00:04:01,579
to tape but usually it goes to waste

00:03:59,000 --> 00:04:05,720
it's it's not being not being used at

00:04:01,579 --> 00:04:09,590
all with big data you capture all the

00:04:05,720 --> 00:04:12,109
data you you collect and you store it it

00:04:09,590 --> 00:04:14,359
was called like a data lake and from

00:04:12,109 --> 00:04:16,519
this data lake you can get to refined

00:04:14,359 --> 00:04:18,739
bottles of water but it also allows you

00:04:16,519 --> 00:04:21,919
to go back to this lake with your

00:04:18,739 --> 00:04:24,740
buckets take a scoop and and take it

00:04:21,919 --> 00:04:27,590
home for for further analysis and maybe

00:04:24,740 --> 00:04:35,540
feel like another more too

00:04:27,590 --> 00:04:37,699
datawarehouse out of that so when starts

00:04:35,540 --> 00:04:39,290
data becoming big data it's a bit of a

00:04:37,699 --> 00:04:41,090
marketing slide or at least it's used a

00:04:39,290 --> 00:04:43,580
lot in in selling your big data

00:04:41,090 --> 00:04:46,160
solutions but there's like the three

00:04:43,580 --> 00:04:48,949
fees it's about volume it's about

00:04:46,160 --> 00:04:51,050
variety and it's about velocity we're

00:04:48,949 --> 00:04:55,040
volume being like the sheer amount of

00:04:51,050 --> 00:04:57,620
data that you have to process or or to

00:04:55,040 --> 00:04:59,419
get access to the variety is both the

00:04:57,620 --> 00:05:01,639
number of sources but also like the

00:04:59,419 --> 00:05:05,540
various formats of the data being in

00:05:01,639 --> 00:05:08,870
documents or images or video or like XML

00:05:05,540 --> 00:05:13,580
files and velocity is like the rate at

00:05:08,870 --> 00:05:15,440
which the data comes in so how fast you

00:05:13,580 --> 00:05:19,310
have to process the data to be able to

00:05:15,440 --> 00:05:22,850
to capture the insights from it and

00:05:19,310 --> 00:05:25,039
usually it's not a single thing of these

00:05:22,850 --> 00:05:26,889
fees that make your problem big big data

00:05:25,039 --> 00:05:31,220
problem it's usually like a combination

00:05:26,889 --> 00:05:34,100
because also force anima are volumes why

00:05:31,220 --> 00:05:41,120
it's especially I think for American

00:05:34,100 --> 00:05:43,039
market it's like not that big variety we

00:05:41,120 --> 00:05:45,560
have quite a lot of sources but like the

00:05:43,039 --> 00:05:49,970
formats are pretty strict the velocity

00:05:45,560 --> 00:05:51,889
that is a thing for us because we want

00:05:49,970 --> 00:05:54,380
to provide these these metrics in

00:05:51,889 --> 00:05:57,590
real-time do like user recommendations

00:05:54,380 --> 00:06:00,950
in in real time so we felt that like the

00:05:57,590 --> 00:06:03,979
combination of these made a really a big

00:06:00,950 --> 00:06:05,660
data solution rights right for us and in

00:06:03,979 --> 00:06:08,919
being a marketing slide nowadays there's

00:06:05,660 --> 00:06:12,620
much more fees that that has been added

00:06:08,919 --> 00:06:15,110
and what's like the definition it's like

00:06:12,620 --> 00:06:16,970
this is the one we we use it's like

00:06:15,110 --> 00:06:18,289
structures and unstructured data which

00:06:16,970 --> 00:06:20,180
can take valuable information that

00:06:18,289 --> 00:06:23,330
cannot be processed by a traditional

00:06:20,180 --> 00:06:26,389
system or at least against like

00:06:23,330 --> 00:06:28,400
acceptable cost and time because if you

00:06:26,389 --> 00:06:31,580
have money to spare you can just buy a

00:06:28,400 --> 00:06:33,710
really big Oracle or IBM or teradata

00:06:31,580 --> 00:06:38,210
appliance and dump all your data and

00:06:33,710 --> 00:06:39,590
your problem can be solved but if you're

00:06:38,210 --> 00:06:42,830
working with

00:06:39,590 --> 00:06:46,370
as we do with like a lot of data but in

00:06:42,830 --> 00:06:48,770
itself per row of data not really

00:06:46,370 --> 00:06:51,080
valuable it usually makes it really hard

00:06:48,770 --> 00:06:58,340
to justify the cost for for such a big

00:06:51,080 --> 00:07:01,010
appliance and then about the variety we

00:06:58,340 --> 00:07:02,630
don't have a lot of relational input

00:07:01,010 --> 00:07:04,430
data we do have a lot of like

00:07:02,630 --> 00:07:08,750
semi-structured data so being locked

00:07:04,430 --> 00:07:10,280
files and we have very little flack

00:07:08,750 --> 00:07:13,160
unstructured data we have some of our

00:07:10,280 --> 00:07:16,190
content that we do analyze on on a big

00:07:13,160 --> 00:07:18,200
data platform but the majority for our

00:07:16,190 --> 00:07:23,210
sources is it's like semi semi

00:07:18,200 --> 00:07:25,250
structured then the biggest difference

00:07:23,210 --> 00:07:29,300
between using a relational database

00:07:25,250 --> 00:07:31,160
system and a big data system is the

00:07:29,300 --> 00:07:33,920
difference on when to apply a schema

00:07:31,160 --> 00:07:37,310
with a relational database system you

00:07:33,920 --> 00:07:40,550
use schema alright so when you insert

00:07:37,310 --> 00:07:42,230
the data it's inserted using a certain

00:07:40,550 --> 00:07:44,510
schema you define all the fields up

00:07:42,230 --> 00:07:46,820
front the data is mapped to those fields

00:07:44,510 --> 00:07:49,090
if stored in a binary representation of

00:07:46,820 --> 00:07:52,250
that and it's also reducing that way

00:07:49,090 --> 00:07:54,950
with big data it usually works different

00:07:52,250 --> 00:07:57,440
you just put on the data in the format

00:07:54,950 --> 00:07:59,710
it's originating and then you map the

00:07:57,440 --> 00:08:02,150
schema on top of that when you read it

00:07:59,710 --> 00:08:08,020
which gives you a lot more flexibility

00:08:02,150 --> 00:08:08,020
to evolve that schema over over time and

00:08:08,110 --> 00:08:12,470
like we said maybe there's like some of

00:08:10,700 --> 00:08:14,780
the fields in the lock that you don't

00:08:12,470 --> 00:08:17,740
initially expose because you don't see

00:08:14,780 --> 00:08:20,420
any value but over time you can still

00:08:17,740 --> 00:08:22,220
capture their data by changing the

00:08:20,420 --> 00:08:28,010
schema putting those additional fields

00:08:22,220 --> 00:08:29,840
in and leveraging those um and big data

00:08:28,010 --> 00:08:32,240
is about the difference between scale up

00:08:29,840 --> 00:08:33,890
and scale out where it's like these

00:08:32,240 --> 00:08:36,020
appliances or your relational database

00:08:33,890 --> 00:08:39,610
system it's usually like a server and

00:08:36,020 --> 00:08:43,760
storage usually honest and device or

00:08:39,610 --> 00:08:45,920
network attached storage and when your

00:08:43,760 --> 00:08:48,260
data volumes grow or you need more

00:08:45,920 --> 00:08:50,210
performance you scale the solution you

00:08:48,260 --> 00:08:51,610
just buy a bigger surfer you're just my

00:08:50,210 --> 00:08:56,390
bigger song

00:08:51,610 --> 00:09:00,110
but it becomes much more expensive

00:08:56,390 --> 00:09:03,860
quickly whereas with Hadoop you start

00:09:00,110 --> 00:09:05,810
with one server with local storage I'd

00:09:03,860 --> 00:09:08,530
like another one we had like another one

00:09:05,810 --> 00:09:11,930
and instead of like growing one server

00:09:08,530 --> 00:09:14,150
you grow the number of servers with

00:09:11,930 --> 00:09:16,610
keeping like the price of the server's

00:09:14,150 --> 00:09:18,950
down or at least they're like what's

00:09:16,610 --> 00:09:21,590
called commodity hardware we're not

00:09:18,950 --> 00:09:25,580
using desktop computers but we just buy

00:09:21,590 --> 00:09:32,690
the the most cost-effective dell

00:09:25,580 --> 00:09:36,460
machines that we can then can find so to

00:09:32,690 --> 00:09:38,900
let to harness that all that power

00:09:36,460 --> 00:09:42,500
Hadoop which is created and that's like

00:09:38,900 --> 00:09:45,320
one of the de facto platforms associated

00:09:42,500 --> 00:09:47,270
with with big data and Hadoop came out

00:09:45,320 --> 00:09:50,750
of two papers that originally have been

00:09:47,270 --> 00:09:53,990
written by Google the GFS paper and the

00:09:50,750 --> 00:09:56,060
MapReduce paper gfs describing their way

00:09:53,990 --> 00:09:58,580
of the Google file system of their

00:09:56,060 --> 00:10:01,570
distributed file system for building

00:09:58,580 --> 00:10:04,640
their web indexes and MapReduce

00:10:01,570 --> 00:10:06,200
described an algorithm functional

00:10:04,640 --> 00:10:09,020
programming using the map and reduce

00:10:06,200 --> 00:10:11,240
with a map and the full operation for

00:10:09,020 --> 00:10:17,240
processing data and being a functional

00:10:11,240 --> 00:10:19,910
is much easier to to distribute and some

00:10:17,240 --> 00:10:22,880
guys at Yahoo implemented that made it

00:10:19,910 --> 00:10:27,080
open source and that's what started then

00:10:22,880 --> 00:10:29,450
the yeah the big data movement if you

00:10:27,080 --> 00:10:32,510
want to call it like that and over the

00:10:29,450 --> 00:10:36,860
years a lot more programming way chef

00:10:32,510 --> 00:10:39,110
have been added so mainly going to talk

00:10:36,860 --> 00:10:41,330
today about Hadoop just like the core

00:10:39,110 --> 00:10:43,930
being the distributed file system and

00:10:41,330 --> 00:10:47,150
the distributed processing and hive

00:10:43,930 --> 00:10:49,280
being like a sequel layer on top of

00:10:47,150 --> 00:10:56,030
Hadoop so you can just use equal to

00:10:49,280 --> 00:10:58,910
query your your data and when we set out

00:10:56,030 --> 00:11:01,210
to build this we felt that not only have

00:10:58,910 --> 00:11:02,630
to scale this technically but also

00:11:01,210 --> 00:11:06,050
organizationally

00:11:02,630 --> 00:11:08,780
we needed to provide a way for everyone

00:11:06,050 --> 00:11:10,850
in the organization to leverage like the

00:11:08,780 --> 00:11:12,710
data that's that's on to do so we need

00:11:10,850 --> 00:11:16,280
to introduce some level of self service

00:11:12,710 --> 00:11:20,110
so not everyone had to come to us to

00:11:16,280 --> 00:11:25,040
build to build the to build a solution

00:11:20,110 --> 00:11:27,260
but yeah that required some some more

00:11:25,040 --> 00:11:29,840
thinking and yeah being from Amsterdam

00:11:27,260 --> 00:11:33,320
this is also what we consider a self

00:11:29,840 --> 00:11:36,470
surface so if you look at like the the

00:11:33,320 --> 00:11:42,470
flow of data from the source to to the

00:11:36,470 --> 00:11:46,190
inside we started off with looking at

00:11:42,470 --> 00:11:49,880
like the last bit of the of the of this

00:11:46,190 --> 00:11:53,120
chain and really opening up the way that

00:11:49,880 --> 00:11:55,310
the business analysts could use their

00:11:53,120 --> 00:11:58,250
the data for building their reports and

00:11:55,310 --> 00:12:01,280
getting their insights and we will take

00:11:58,250 --> 00:12:05,750
care of the first bit so everything is

00:12:01,280 --> 00:12:08,990
great we started out building this we

00:12:05,750 --> 00:12:12,470
selected the tool for it which was quick

00:12:08,990 --> 00:12:15,050
fuse like a memory analytics then our

00:12:12,470 --> 00:12:18,710
data and data processing on Hadoop with

00:12:15,050 --> 00:12:21,890
high 44 like easy access to to the data

00:12:18,710 --> 00:12:24,620
and all we needed was like something to

00:12:21,890 --> 00:12:26,480
glue this together and typically what

00:12:24,620 --> 00:12:28,880
you do with like a bi project is

00:12:26,480 --> 00:12:34,310
introduced ETL tool where it stands for

00:12:28,880 --> 00:12:36,590
like extract transform loved and back

00:12:34,310 --> 00:12:39,350
then they were like two in the market

00:12:36,590 --> 00:12:43,250
that had connectors to to Hadoop so we

00:12:39,350 --> 00:12:45,440
were able to interact with we do we pick

00:12:43,250 --> 00:12:49,000
one of them and it's like a graphical

00:12:45,440 --> 00:12:51,530
look for generation language which

00:12:49,000 --> 00:12:53,980
allows you to drag and drop certain

00:12:51,530 --> 00:12:59,210
process season and define it and

00:12:53,980 --> 00:13:02,720
initially well that that works with the

00:12:59,210 --> 00:13:05,080
initial flow gets the data on the only

00:13:02,720 --> 00:13:05,080
cluster

00:13:06,910 --> 00:13:15,320
sorry but when you start growing this

00:13:12,850 --> 00:13:17,990
the flowers start to get more complex

00:13:15,320 --> 00:13:19,399
they're harder to debug and the biggest

00:13:17,990 --> 00:13:23,810
problem because we're working with big

00:13:19,399 --> 00:13:25,760
data the the performance is important

00:13:23,810 --> 00:13:28,399
because like within 24 hours you have to

00:13:25,760 --> 00:13:29,839
do the processing for 24 hours of data

00:13:28,399 --> 00:13:33,500
otherwise like the next thing you can't

00:13:29,839 --> 00:13:35,630
can't keep up and then you have to like

00:13:33,500 --> 00:13:39,019
paralyzed some of the of the work so

00:13:35,630 --> 00:13:41,720
that it can can run okay and can come in

00:13:39,019 --> 00:13:44,149
simultaneously so that much then your

00:13:41,720 --> 00:13:46,040
job starts looking like this and you can

00:13:44,149 --> 00:13:49,639
imagine that now ending like the single

00:13:46,040 --> 00:13:56,839
field in all of these operations it's

00:13:49,639 --> 00:14:00,560
like a hell of a job so um what you

00:13:56,839 --> 00:14:01,970
start doing you start introducing some

00:14:00,560 --> 00:14:04,430
shell scripts to do the heavy lifting

00:14:01,970 --> 00:14:10,730
for you taking away like all of the

00:14:04,430 --> 00:14:12,680
power of the of the ETL tool um so this

00:14:10,730 --> 00:14:14,829
part of the project failed dramatically

00:14:12,680 --> 00:14:16,790
we had this really interesting

00:14:14,829 --> 00:14:19,010
components like a dupe and click view

00:14:16,790 --> 00:14:21,620
which provided a lot of value in itself

00:14:19,010 --> 00:14:23,990
but like the integrations and working

00:14:21,620 --> 00:14:26,209
with ETL tool didn't fit our our use

00:14:23,990 --> 00:14:29,089
case it didn't scale technically it

00:14:26,209 --> 00:14:31,699
didn't skill organizationally and we

00:14:29,089 --> 00:14:33,589
needed a slightly different mindset I

00:14:31,699 --> 00:14:36,589
think we approached it too much as being

00:14:33,589 --> 00:14:42,890
like a bi project instead of being at a

00:14:36,589 --> 00:14:46,370
development project so we had to do a

00:14:42,890 --> 00:14:51,470
reset because all of these sources keep

00:14:46,370 --> 00:14:56,630
the kept changing and resetting a map

00:14:51,470 --> 00:15:01,149
that we we also need to rethink like the

00:14:56,630 --> 00:15:04,430
this the organizational scalability and

00:15:01,149 --> 00:15:06,800
then we met this guy this guy is Russell

00:15:04,430 --> 00:15:12,500
journey he wrote a book about about

00:15:06,800 --> 00:15:15,260
agile data and what he said was that you

00:15:12,500 --> 00:15:17,980
should make the entire stack self

00:15:15,260 --> 00:15:20,710
surface so not only the last bits of

00:15:17,980 --> 00:15:22,930
doing the reporting but you should open

00:15:20,710 --> 00:15:35,380
up the entire system to everyone that

00:15:22,930 --> 00:15:37,690
wants to wants to use it and and we

00:15:35,380 --> 00:15:39,850
found that initially a bit challenging

00:15:37,690 --> 00:15:41,560
but we bought into the idea of opening

00:15:39,850 --> 00:15:45,250
much more to the stack and gradually

00:15:41,560 --> 00:15:49,180
moving forward in the way we exposed the

00:15:45,250 --> 00:15:51,430
data but the tools we had didn't fit

00:15:49,180 --> 00:15:53,920
that or at least the ETL tool we had to

00:15:51,430 --> 00:15:58,440
do it didn't fit that shall we needed

00:15:53,920 --> 00:16:00,550
new glue and then looking at at the

00:15:58,440 --> 00:16:04,150
capabilities we need from the from the

00:16:00,550 --> 00:16:06,070
GTL tool we need like processing we need

00:16:04,150 --> 00:16:09,130
like scheduling be like data quality

00:16:06,070 --> 00:16:12,940
data lineage versioning and annotation

00:16:09,130 --> 00:16:18,480
and what can provide these these things

00:16:12,940 --> 00:16:18,480
so we looked at a few options we used

00:16:19,069 --> 00:16:23,159
commercially supported our source

00:16:20,970 --> 00:16:29,039
product initially we looked at usually

00:16:23,159 --> 00:16:31,139
some commercial ETL tools and we looked

00:16:29,039 --> 00:16:33,720
at some some options from like the Big

00:16:31,139 --> 00:16:39,809
Data community which we felt were not

00:16:33,720 --> 00:16:41,819
mature enough and then we we did the

00:16:39,809 --> 00:16:45,119
spike with it we tried like three

00:16:41,819 --> 00:16:46,709
options and like benchmark time against

00:16:45,119 --> 00:16:51,239
each other how fast is was to integrate

00:16:46,709 --> 00:16:55,319
and from that came that this was the

00:16:51,239 --> 00:16:59,069
fastest to to implement it was like a

00:16:55,319 --> 00:17:04,559
bash script reading in the the files

00:16:59,069 --> 00:17:07,799
from a ftp location and putting them on

00:17:04,559 --> 00:17:10,740
the on the big data cluster and then

00:17:07,799 --> 00:17:12,779
doing the processing there but there was

00:17:10,740 --> 00:17:16,399
a slight problem here there was this

00:17:12,779 --> 00:17:20,850
loop for the non-technical people and

00:17:16,399 --> 00:17:23,129
activating this loop 10,000 times takes

00:17:20,850 --> 00:17:26,429
takes a long time and it's due to the

00:17:23,129 --> 00:17:30,720
three hub commandos that are there at

00:17:26,429 --> 00:17:33,120
the at the bottom they all take like at

00:17:30,720 --> 00:17:35,639
least a half seconds to to start up so

00:17:33,120 --> 00:17:39,509
then I trading visual will take too long

00:17:35,639 --> 00:17:42,059
so we looked at an alternative and we

00:17:39,509 --> 00:17:44,759
found also a scripting language but

00:17:42,059 --> 00:17:46,799
running straight on the JVM so reducing

00:17:44,759 --> 00:17:50,220
the startup time and be able to execute

00:17:46,799 --> 00:17:53,700
much much faster it looks like Python

00:17:50,220 --> 00:18:01,409
but it was on the JVM so this is this is

00:17:53,700 --> 00:18:04,350
jayden it gives you like all the powers

00:18:01,409 --> 00:18:08,039
of a scripting language and the

00:18:04,350 --> 00:18:11,460
accessibility of of Java and interacting

00:18:08,039 --> 00:18:14,039
with them with a platform so that's all

00:18:11,460 --> 00:18:15,570
like the processing part but now still

00:18:14,039 --> 00:18:18,600
we needed something for scheduling and

00:18:15,570 --> 00:18:20,340
there we already had Jenkins in place

00:18:18,600 --> 00:18:25,049
because we were building like some of

00:18:20,340 --> 00:18:28,980
the jobs which are in Java with Jenkins

00:18:25,049 --> 00:18:31,280
and Jenkins already it's like the

00:18:28,980 --> 00:18:33,770
scheduling is this monitoring it emails

00:18:31,280 --> 00:18:36,050
you when something fails it has this

00:18:33,770 --> 00:18:38,090
nice dashboard we can put our monitoring

00:18:36,050 --> 00:18:40,940
wall it is really nice integration with

00:18:38,090 --> 00:18:44,930
the first inning control and this was

00:18:40,940 --> 00:18:47,710
really a tool for developers and we can

00:18:44,930 --> 00:18:53,510
also open this up to like other teams

00:18:47,710 --> 00:18:55,550
using to get a job scheduled on on the

00:18:53,510 --> 00:18:58,730
platform so this was really a natural

00:18:55,550 --> 00:19:06,020
choice for us moving moving this to to

00:18:58,730 --> 00:19:08,450
Jenkins so then looking at those

00:19:06,020 --> 00:19:11,150
capabilities for processing we have bash

00:19:08,450 --> 00:19:13,850
and jayden for scheduling we use Jenkins

00:19:11,150 --> 00:19:16,850
versioning our culture port story

00:19:13,850 --> 00:19:19,940
material and for annotation we're

00:19:16,850 --> 00:19:26,210
commenting the code instead of printing

00:19:19,940 --> 00:19:28,640
out and highlighting the steps so what

00:19:26,210 --> 00:19:30,800
do we use for four processes we try to

00:19:28,640 --> 00:19:34,880
make these independent steps moving the

00:19:30,800 --> 00:19:37,940
data one stage to the next and all these

00:19:34,880 --> 00:19:40,070
steps have a central role for for

00:19:37,940 --> 00:19:42,710
Jenkins because Jenkins is kicking off

00:19:40,070 --> 00:19:44,810
like all these separate separate steps

00:19:42,710 --> 00:19:48,500
so this is like the extract getting the

00:19:44,810 --> 00:19:52,100
data from a source system usually FTP or

00:19:48,500 --> 00:19:54,860
our API it loads the data puts it

00:19:52,100 --> 00:19:57,440
straight on on a dupe and this is what

00:19:54,860 --> 00:20:01,160
helps like the scalability because this

00:19:57,440 --> 00:20:03,830
is as fast as it as it gets I like the

00:20:01,160 --> 00:20:05,870
next step a processing the data is

00:20:03,830 --> 00:20:08,210
happening on a dupe and there you really

00:20:05,870 --> 00:20:10,070
have the power of the cluster and being

00:20:08,210 --> 00:20:12,470
able to run it in then in parallel so

00:20:10,070 --> 00:20:14,330
now your ETL tool or jenkins is not a

00:20:12,470 --> 00:20:17,660
bottleneck anymore it just kicks off the

00:20:14,330 --> 00:20:22,060
job have it run on them on the big data

00:20:17,660 --> 00:20:25,310
cluster I like the last step Jenkins

00:20:22,060 --> 00:20:27,890
triggers the loads to do click few to

00:20:25,310 --> 00:20:30,220
expose the data to the to the desperate

00:20:27,890 --> 00:20:30,220
users

00:20:34,950 --> 00:20:41,140
sorry

00:20:37,370 --> 00:20:41,140
yum it's okay

00:20:48,179 --> 00:20:54,780
yep yep yep

00:21:02,270 --> 00:21:05,510
yep yep

00:21:11,110 --> 00:21:17,600
okay yeah so the question was about

00:21:13,970 --> 00:21:19,789
loading the loading the script so what's

00:21:17,600 --> 00:21:23,149
the description used in from Jenkins

00:21:19,789 --> 00:21:25,460
that's indeed its HD item so giant

00:21:23,149 --> 00:21:27,740
Coates gets the data from the shores and

00:21:25,460 --> 00:21:30,549
puts it on the cluster then on Hadoop

00:21:27,740 --> 00:21:35,019
your various ways of processing the data

00:21:30,549 --> 00:21:38,389
a bit dependent on the on the complexity

00:21:35,019 --> 00:21:42,830
we use either MapReduce code written in

00:21:38,389 --> 00:21:45,440
Java that's yeah you like implement the

00:21:42,830 --> 00:21:47,960
map and reduce function or like various

00:21:45,440 --> 00:21:50,749
various steps of that there are also

00:21:47,960 --> 00:21:54,759
like higher-level frameworks that make

00:21:50,749 --> 00:21:54,759
it easier to develop just like cascading

00:21:56,740 --> 00:22:01,789
which gives you some higher-level

00:21:58,460 --> 00:22:06,460
operators which still boils down to two

00:22:01,789 --> 00:22:08,690
MapReduce cult then there's also a

00:22:06,460 --> 00:22:11,929
scholar wrapper on top of that which is

00:22:08,690 --> 00:22:18,110
called scolding we used mainly our data

00:22:11,929 --> 00:22:20,509
scientists use that and so that's for

00:22:18,110 --> 00:22:21,980
jobs with a higher complexity for like

00:22:20,509 --> 00:22:24,879
the simpler jobs you can use Hadoop

00:22:21,980 --> 00:22:31,399
streaming which they can still use

00:22:24,879 --> 00:22:33,919
Python or bash or whatever and when it's

00:22:31,399 --> 00:22:36,019
like really simple you can immediately

00:22:33,919 --> 00:22:39,470
use hive which is like the sequel

00:22:36,019 --> 00:22:42,499
language to use the process and so is if

00:22:39,470 --> 00:22:45,499
it's already like a CSV format you can

00:22:42,499 --> 00:22:48,169
just put a external table from hive on

00:22:45,499 --> 00:22:53,029
top of that and just use equal insert

00:22:48,169 --> 00:22:55,809
into to move its and process it does

00:22:53,029 --> 00:22:55,809
that answer the question

00:23:15,810 --> 00:23:24,240
yep the question was if with our DBMS

00:23:21,120 --> 00:23:26,040
sequel is the only way to interact but

00:23:24,240 --> 00:23:32,550
with Hadoop you have like multiple ways

00:23:26,040 --> 00:23:36,690
of processing the data I'd like with

00:23:32,550 --> 00:23:40,770
MapReduce codes or with streaming or

00:23:36,690 --> 00:23:42,480
there's also pick which like a simple

00:23:40,770 --> 00:23:44,340
script lace scripting language on top of

00:23:42,480 --> 00:23:49,680
the loop or high if it's there like chic

00:23:44,340 --> 00:23:52,740
oh yeah so then what levels of

00:23:49,680 --> 00:23:54,960
self-service do we do we now have so

00:23:52,740 --> 00:23:57,270
like the initial step so like to extract

00:23:54,960 --> 00:23:59,670
that's done with with Titan and that

00:23:57,270 --> 00:24:02,730
brings like the raw data in its native

00:23:59,670 --> 00:24:05,010
form on to Hadoop that's what we expose

00:24:02,730 --> 00:24:07,770
to the data scientists they write their

00:24:05,010 --> 00:24:10,260
jobs in cascading or in scolding and

00:24:07,770 --> 00:24:12,360
they also execute that using Jenkins so

00:24:10,260 --> 00:24:14,400
they scheduled their jobs using Jenkins

00:24:12,360 --> 00:24:17,100
and there we have the business analysts

00:24:14,400 --> 00:24:22,530
they used like a more refined form of

00:24:17,100 --> 00:24:26,340
the data where we create like normalized

00:24:22,530 --> 00:24:29,790
stable names normalize the fields but

00:24:26,340 --> 00:24:33,180
this is still not a mobile data

00:24:29,790 --> 00:24:38,060
warehouse what you typically have like

00:24:33,180 --> 00:24:38,060
in a star schema or or in a data Mart

00:24:39,880 --> 00:24:59,960
that is happening by by some of the

00:24:43,309 --> 00:25:02,360
other teams they one sec and they create

00:24:59,960 --> 00:25:10,970
these these data Mart's in in like sauce

00:25:02,360 --> 00:25:12,740
or in Business Objects so what do we use

00:25:10,970 --> 00:25:15,230
for like providing is the self service

00:25:12,740 --> 00:25:18,260
one of those tools is is you which is

00:25:15,230 --> 00:25:21,500
the Hadoop user experience it's usually

00:25:18,260 --> 00:25:23,570
interface on top of a dupe exposing like

00:25:21,500 --> 00:25:26,960
a lot of these these functionalities and

00:25:23,570 --> 00:25:28,929
mainly exposing like like I've so users

00:25:26,960 --> 00:25:34,220
have an interactive way of typing Mexico

00:25:28,929 --> 00:25:40,460
including the data other tools like take

00:25:34,220 --> 00:25:42,409
few already mentioned the other one more

00:25:40,460 --> 00:25:46,070
for like data sciences like our our

00:25:42,409 --> 00:25:55,429
studio which enables the data sciences

00:25:46,070 --> 00:25:58,669
do a predictive analysis really trading

00:25:55,429 --> 00:26:01,760
am a classification model or doing

00:25:58,669 --> 00:26:04,820
clustering and then using that into some

00:26:01,760 --> 00:26:07,429
of the real time real time systems and

00:26:04,820 --> 00:26:11,659
jenkins think is for for scheduling

00:26:07,429 --> 00:26:17,240
everything and with scheduling we really

00:26:11,659 --> 00:26:21,080
find it practical to use the dashing

00:26:17,240 --> 00:26:24,470
scheduling which tells your jobs to run

00:26:21,080 --> 00:26:26,809
in a certain time so we define it should

00:26:24,470 --> 00:26:32,870
run in a certain hour but you don't

00:26:26,809 --> 00:26:38,210
really care at what point in that in

00:26:32,870 --> 00:26:41,560
that in that hour giving a really nice

00:26:38,210 --> 00:26:49,600
way of distributing the load on the

00:26:41,560 --> 00:27:00,310
on the Jenkins server the other thing we

00:26:49,600 --> 00:27:01,690
use from from Jenkins sorry sir I think

00:27:00,310 --> 00:27:06,250
we use from Jenkins our lease

00:27:01,690 --> 00:27:09,880
parameterize builds which allows users

00:27:06,250 --> 00:27:12,400
to just enter a few parameters and then

00:27:09,880 --> 00:27:15,280
run Rena build so they can request like

00:27:12,400 --> 00:27:17,860
their that crystal report asking for

00:27:15,280 --> 00:27:21,250
certain date range or asking for certain

00:27:17,860 --> 00:27:25,150
customer or a certain website so they

00:27:21,250 --> 00:27:27,730
just plug in the numbers hit the build

00:27:25,150 --> 00:27:30,730
button and then that job is ran with

00:27:27,730 --> 00:27:32,380
those parameters we initially had really

00:27:30,730 --> 00:27:34,510
high hopes for this because we felt this

00:27:32,380 --> 00:27:38,650
like a really nice way for for use to

00:27:34,510 --> 00:27:43,810
interact but currently it's not used as

00:27:38,650 --> 00:27:46,060
much as we as we thought and that's

00:27:43,810 --> 00:27:51,580
mainly because we introduced like this

00:27:46,060 --> 00:27:54,870
this other way of interacting from from

00:27:51,580 --> 00:28:00,880
click view directly where users define a

00:27:54,870 --> 00:28:04,810
a query and then click view triggers the

00:28:00,880 --> 00:28:07,960
Jenkins API the job is it's loaded from

00:28:04,810 --> 00:28:10,270
from from click view it's being executed

00:28:07,960 --> 00:28:13,290
on the cluster and then a day as we

00:28:10,270 --> 00:28:16,300
export it it is somewhat of a workaround

00:28:13,290 --> 00:28:18,190
but it's mainly because because of this

00:28:16,300 --> 00:28:23,770
this performs much better than pulling

00:28:18,190 --> 00:28:27,160
the data from a few from from gloopy to

00:28:23,770 --> 00:28:30,880
a few it's more performance to to push

00:28:27,160 --> 00:28:33,280
it for some reason and this is where we

00:28:30,880 --> 00:28:36,240
also use like the throttle current build

00:28:33,280 --> 00:28:38,830
plugin from from Hadoop the former

00:28:36,240 --> 00:28:44,110
Jenkins to lessen the load on the on the

00:28:38,830 --> 00:28:46,690
cluster like the main plugins we we use

00:28:44,110 --> 00:28:48,610
in in Jenkins is like the mailer for

00:28:46,690 --> 00:28:51,120
sending out the output the held up

00:28:48,610 --> 00:28:54,740
connectivity for getting the access to

00:28:51,120 --> 00:28:56,790
to the users add to the data scientists

00:28:54,740 --> 00:28:58,800
in combination with the matrix

00:28:56,790 --> 00:29:02,130
authorization strategy which gives you

00:28:58,800 --> 00:29:04,710
the flexibility to assign the roles to

00:29:02,130 --> 00:29:08,040
the to the projects and obviously the

00:29:04,710 --> 00:29:13,170
material and you get plugins and the

00:29:08,040 --> 00:29:16,350
green balls so how does this this look

00:29:13,170 --> 00:29:18,450
together we have like the sources we

00:29:16,350 --> 00:29:20,880
have Jenkins as a central central

00:29:18,450 --> 00:29:23,300
component with with Titan getting all

00:29:20,880 --> 00:29:26,700
the data from the sources on to the dupe

00:29:23,300 --> 00:29:29,309
scheduling the scheduled exports the

00:29:26,700 --> 00:29:30,900
scheduled loads to click view and and we

00:29:29,309 --> 00:29:33,750
have hive and you for more of the

00:29:30,900 --> 00:29:36,000
exploratory work and there's an option

00:29:33,750 --> 00:29:40,200
to export that directly to click view or

00:29:36,000 --> 00:29:43,800
directly to to our server we're moving

00:29:40,200 --> 00:29:45,240
more and more towards like real time we

00:29:43,800 --> 00:29:46,830
ever out logging infrastructure

00:29:45,240 --> 00:29:50,280
implementing like all the sites for

00:29:46,830 --> 00:29:53,760
getting the events and then driving like

00:29:50,280 --> 00:30:00,300
recommendations we use Kafka and storm

00:29:53,760 --> 00:30:03,270
for that and that's like added at the

00:30:00,300 --> 00:30:05,970
bottom joke the nice thing is that all

00:30:03,270 --> 00:30:08,160
the data we capture goes to Hadoop so we

00:30:05,970 --> 00:30:11,429
could always do a replay of the data we

00:30:08,160 --> 00:30:13,679
collected and it's then fats to like the

00:30:11,429 --> 00:30:16,230
real time process for for doing like the

00:30:13,679 --> 00:30:18,950
recommendations and this is what now

00:30:16,230 --> 00:30:21,540
that's called like a lambda architecture

00:30:18,950 --> 00:30:23,790
and then the the models that the data

00:30:21,540 --> 00:30:27,990
science is built in our can be exported

00:30:23,790 --> 00:30:30,590
and an executed in real time for for the

00:30:27,990 --> 00:30:35,670
recommendations or like classification

00:30:30,590 --> 00:30:38,340
so what's up our current state our

00:30:35,670 --> 00:30:41,760
current state is that we exposed like

00:30:38,340 --> 00:30:46,340
all the light blue as really being self

00:30:41,760 --> 00:30:50,630
surface the first part can be used if

00:30:46,340 --> 00:30:54,210
the fella purse want want to extract

00:30:50,630 --> 00:30:56,220
their their own source they have access

00:30:54,210 --> 00:30:58,530
to Jenkins if X to the culture story

00:30:56,220 --> 00:31:00,470
they can write it themselves it's it

00:30:58,530 --> 00:31:03,960
rarely happens it's usually what we do

00:31:00,470 --> 00:31:08,260
but from from there onwards

00:31:03,960 --> 00:31:14,320
people are are using are doing it

00:31:08,260 --> 00:31:16,330
themselves quite quite often so we now

00:31:14,320 --> 00:31:19,960
have jenkins and anne-jade on a gaff

00:31:16,330 --> 00:31:21,370
gaya storm Institute recently and in the

00:31:19,960 --> 00:31:26,440
beginning of this year we moved to yarn

00:31:21,370 --> 00:31:29,440
which is allows for more processing

00:31:26,440 --> 00:31:33,910
paradigms on top of Hadoop instead of

00:31:29,440 --> 00:31:38,200
just doing doing MapReduce so what are

00:31:33,910 --> 00:31:40,720
the use cases we we do or what are

00:31:38,200 --> 00:31:43,810
people doing with our platform mainly

00:31:40,720 --> 00:31:45,520
it's like combining the data sources are

00:31:43,810 --> 00:31:47,020
combining web analytics with a

00:31:45,520 --> 00:31:50,260
behavioral targeting system with our

00:31:47,020 --> 00:31:52,300
advertising system we're also doing a be

00:31:50,260 --> 00:31:55,870
testing and analyzing the results that

00:31:52,300 --> 00:31:57,670
come from come from that we we also buy

00:31:55,870 --> 00:32:04,210
traffic for some of our sites we have to

00:31:57,670 --> 00:32:08,500
optimize on on the ROI of that we we

00:32:04,210 --> 00:32:13,180
provide like an ad auction so customers

00:32:08,500 --> 00:32:16,120
can buy or can bit on at space that's

00:32:13,180 --> 00:32:18,250
that we that we serve for that we do

00:32:16,120 --> 00:32:20,470
surprise optimizations we do ad

00:32:18,250 --> 00:32:23,220
targeting so that's classifying users

00:32:20,470 --> 00:32:27,610
today this person's interesting cars

00:32:23,220 --> 00:32:29,350
will show car ads and we will face

00:32:27,610 --> 00:32:35,890
recommendations and also saw some shirts

00:32:29,350 --> 00:32:39,190
optimizations so still a main use cases

00:32:35,890 --> 00:32:41,860
for reporting and it's increasingly

00:32:39,190 --> 00:32:44,350
shifting to to data science they're also

00:32:41,860 --> 00:32:47,260
like a further down in the stack it's

00:32:44,350 --> 00:32:49,780
like the common cinema a big data

00:32:47,260 --> 00:32:55,270
platform used in the in the in the two

00:32:49,780 --> 00:32:58,960
countries we reflect it 250 daily

00:32:55,270 --> 00:33:03,480
dashboard users and a 40 analysts really

00:32:58,960 --> 00:33:06,850
work on on Hadoop itself and in Jenkins

00:33:03,480 --> 00:33:09,670
for these + source systems and then

00:33:06,850 --> 00:33:12,590
under 25 different sources which results

00:33:09,670 --> 00:33:17,480
in like it to 400 tables in

00:33:12,590 --> 00:33:20,390
in hive the the core team is actually

00:33:17,480 --> 00:33:22,100
pretty small this one product down and

00:33:20,390 --> 00:33:27,049
three developers they provide like the

00:33:22,100 --> 00:33:30,860
platform Jenkins and they get like the

00:33:27,049 --> 00:33:32,720
sources on the platform and they support

00:33:30,860 --> 00:33:35,270
like the data scientist the business

00:33:32,720 --> 00:33:37,370
analysts and there's a guy supporting

00:33:35,270 --> 00:33:40,460
and supporting click view so

00:33:37,370 --> 00:33:46,429
architectural support curve platform is

00:33:40,460 --> 00:33:49,760
55 notes 650 terabytes of proficient

00:33:46,429 --> 00:33:54,289
storage and we run about 3,000 jobs a

00:33:49,760 --> 00:33:59,390
day and some information about the

00:33:54,289 --> 00:34:06,020
letter notes so what's what's next for

00:33:59,390 --> 00:34:07,610
us we have some some challenges and one

00:34:06,020 --> 00:34:10,669
of them is security which we feel

00:34:07,610 --> 00:34:14,320
security is always an issue you should

00:34:10,669 --> 00:34:17,179
also always be bi or your toes and

00:34:14,320 --> 00:34:21,470
really look in in how you can can

00:34:17,179 --> 00:34:26,720
improve improve that and Hadoop

00:34:21,470 --> 00:34:30,950
especially is harder to protect piece of

00:34:26,720 --> 00:34:35,119
infrastructure then maybe like a web web

00:34:30,950 --> 00:34:37,570
site the infrastructure we're currently

00:34:35,119 --> 00:34:41,589
running on premise or this little

00:34:37,570 --> 00:34:43,909
echolocation owning the our own hardware

00:34:41,589 --> 00:34:45,919
we've done that since the beginning and

00:34:43,909 --> 00:34:48,349
that mainly has to do with like the

00:34:45,919 --> 00:34:50,599
price point a public cloud is really

00:34:48,349 --> 00:34:53,450
nice gives you a lot of flexibility but

00:34:50,599 --> 00:34:55,310
comes at like higher costs so running

00:34:53,450 --> 00:34:58,760
the same infrastructure at like Amazon

00:34:55,310 --> 00:35:03,109
with like two times as expensive so

00:34:58,760 --> 00:35:04,790
we're really trying to to balance what

00:35:03,109 --> 00:35:07,280
we do in our own infrastructure what we

00:35:04,790 --> 00:35:09,980
do in public clouds I want to improve

00:35:07,280 --> 00:35:12,830
like the integrations and we want to

00:35:09,980 --> 00:35:15,859
improve some of the Jenkins stuff

00:35:12,830 --> 00:35:18,560
because for us Jenks is still a single

00:35:15,859 --> 00:35:21,030
point of failure we have some issues

00:35:18,560 --> 00:35:26,070
with like big cult repositories

00:35:21,030 --> 00:35:29,040
and that's because like those 3,000 jobs

00:35:26,070 --> 00:35:33,750
that we run originated from like I'd say

00:35:29,040 --> 00:35:36,060
500 jobs in in Jenkins which almost all

00:35:33,750 --> 00:35:39,210
check out like the shame cause report

00:35:36,060 --> 00:35:41,820
story we had once someone that by

00:35:39,210 --> 00:35:44,070
accident checked in some test data which

00:35:41,820 --> 00:35:46,260
turned out to be I think two or three

00:35:44,070 --> 00:35:52,520
gigabytes but then having that checked

00:35:46,260 --> 00:35:57,560
out 500 times costs up man some issues

00:35:52,520 --> 00:36:00,390
the the way Jenkins upgrades it's not

00:35:57,560 --> 00:36:02,370
not really convenient for us because it

00:36:00,390 --> 00:36:04,110
has to shut down that in combination

00:36:02,370 --> 00:36:09,090
would like to run the long running jobs

00:36:04,110 --> 00:36:11,400
is it's not so practical because that

00:36:09,090 --> 00:36:13,890
there's jobs that run for like 14 hours

00:36:11,400 --> 00:36:17,460
and when you absolutely like and

00:36:13,890 --> 00:36:19,920
upgrades yeah you're you either have to

00:36:17,460 --> 00:36:21,420
break off the job or wait until that job

00:36:19,920 --> 00:36:22,950
finally completed and have try to

00:36:21,420 --> 00:36:26,130
restart which can then be in the middle

00:36:22,950 --> 00:36:27,690
of the night and then yeah because the

00:36:26,130 --> 00:36:29,940
single point of failure want to increase

00:36:27,690 --> 00:36:37,610
the redundancy maybe you to do some more

00:36:29,940 --> 00:36:41,730
build slaves so to wrap up everything

00:36:37,610 --> 00:36:45,870
the main takeaways duping big data open

00:36:41,730 --> 00:36:48,240
a lot of opportunities for for your for

00:36:45,870 --> 00:36:52,260
your company there's a lot of data you

00:36:48,240 --> 00:36:54,090
can you can more easily access and use

00:36:52,260 --> 00:36:58,560
for for driving improvements in your

00:36:54,090 --> 00:37:01,950
product or new insights you can easily

00:36:58,560 --> 00:37:06,150
capture more data and start using it

00:37:01,950 --> 00:37:08,460
later also in like a iterative way you

00:37:06,150 --> 00:37:10,560
can get stars really quickly with like a

00:37:08,460 --> 00:37:13,530
simple use case but start capturing a

00:37:10,560 --> 00:37:18,300
lot more data and then in time sorry

00:37:13,530 --> 00:37:22,830
expanding on on the usage I do require

00:37:18,300 --> 00:37:26,220
developers and developers like using

00:37:22,830 --> 00:37:29,790
development tools and like Jenkins is a

00:37:26,220 --> 00:37:32,400
really good match for that and Jenkins

00:37:29,790 --> 00:37:34,060
might not be the best solution and it's

00:37:32,400 --> 00:37:36,430
not a real natural fit for

00:37:34,060 --> 00:37:40,420
big data but it's a good enough solution

00:37:36,430 --> 00:37:45,070
at least for us and it's also source the

00:37:40,420 --> 00:37:49,090
problem well enough for for now and like

00:37:45,070 --> 00:37:51,520
I said you can start small with with big

00:37:49,090 --> 00:37:56,070
data so that's like the my main

00:37:51,520 --> 00:37:56,070
takeaways are there any questions

00:38:02,530 --> 00:38:12,400
sure you have to come over like what

00:38:15,280 --> 00:38:20,750
yeah oh yeah you can obviously use

00:38:18,170 --> 00:38:22,760
Chrome but that doesn't have any

00:38:20,750 --> 00:38:24,440
dependency management that doesn't have

00:38:22,760 --> 00:38:28,610
like the monitoring that doesn't have

00:38:24,440 --> 00:38:30,830
like the nice reporting wall and yet it

00:38:28,610 --> 00:38:33,500
requires your developers to go and edit

00:38:30,830 --> 00:38:37,190
like the wrong file this gives you like

00:38:33,500 --> 00:38:41,300
a nice web interface you can enter their

00:38:37,190 --> 00:38:44,480
and their jobs there so yeah I think

00:38:41,300 --> 00:38:48,010
this was a more friendly the one of the

00:38:44,480 --> 00:38:51,020
things chrome does not have is the the

00:38:48,010 --> 00:38:53,150
hash scheduling which is like really

00:38:51,020 --> 00:38:55,370
really important because if you're

00:38:53,150 --> 00:38:57,830
running like 500 jobs and some of them

00:38:55,370 --> 00:38:59,870
you run hourly you really want to spread

00:38:57,830 --> 00:39:01,970
out that loud and not being there

00:38:59,870 --> 00:39:04,550
manually picking like you can run at

00:39:01,970 --> 00:39:06,910
five minutes past the hour you can run

00:39:04,550 --> 00:39:12,280
10 minutes past the hour the H

00:39:06,910 --> 00:39:12,280
scheduling takes care of that for you

00:39:19,150 --> 00:39:24,770
the question is what are the

00:39:22,059 --> 00:39:30,050
dependencies we really try to limit the

00:39:24,770 --> 00:39:33,050
dependencies because if you really

00:39:30,050 --> 00:39:35,650
design the jobs to be independent and we

00:39:33,050 --> 00:39:38,839
just put the data to like the next stage

00:39:35,650 --> 00:39:44,359
you can have all the jobs independently

00:39:38,839 --> 00:39:47,210
like monitoring the the yeah their

00:39:44,359 --> 00:39:49,760
inputs directory and just process what

00:39:47,210 --> 00:39:52,700
data is there which gives you a lot more

00:39:49,760 --> 00:39:54,800
options to do reruns of your of your

00:39:52,700 --> 00:39:56,960
data whereas if you really make abuse

00:39:54,800 --> 00:40:02,210
dependency graph of that it becomes much

00:39:56,960 --> 00:40:05,809
harder to to manage too many cells so we

00:40:02,210 --> 00:40:07,250
do use that but we try to limit it as

00:40:05,809 --> 00:40:10,630
much as possible and really make them

00:40:07,250 --> 00:40:10,630
like atomic steps

00:40:15,480 --> 00:40:23,490
you can you can use the dependency graph

00:40:20,020 --> 00:40:23,490
plugin that's a

00:40:48,450 --> 00:40:54,910
the gap is in the redundancy so being at

00:40:52,030 --> 00:40:56,980
a single point of failure yeah maybe you

00:40:54,910 --> 00:41:02,740
can go support it and go for like the

00:40:56,980 --> 00:41:08,790
master master set up but we have not

00:41:02,740 --> 00:41:13,000
looked into that debt enough I think

00:41:08,790 --> 00:41:15,270
some of the dependencies are in of the

00:41:13,000 --> 00:41:19,660
the short comics are in like the updates

00:41:15,270 --> 00:41:21,220
and having to restart with it like all

00:41:19,660 --> 00:41:23,020
the all the updates and not having like

00:41:21,220 --> 00:41:25,980
a separation between like the

00:41:23,020 --> 00:41:29,170
coordinator role and like the executors

00:41:25,980 --> 00:41:30,730
in some sense because that would be like

00:41:29,170 --> 00:41:33,940
really convenient to just have these

00:41:30,730 --> 00:41:35,710
build slaves from autonomously and just

00:41:33,940 --> 00:41:39,670
report back on like the job site which

00:41:35,710 --> 00:41:47,920
allows you to just reboot the the master

00:41:39,670 --> 00:41:52,620
or the scheduling service some other

00:41:47,920 --> 00:41:52,620
shortcomings see what was there

00:41:56,880 --> 00:42:04,930
they're so the the big repositories now

00:42:03,130 --> 00:42:08,130
we're for every job we like checking out

00:42:04,930 --> 00:42:10,420
the full report story and it has some

00:42:08,130 --> 00:42:14,590
minimal framework so it is like the

00:42:10,420 --> 00:42:19,950
entire thing yeah maybe having an option

00:42:14,590 --> 00:42:25,270
to to refer like I already checked out

00:42:19,950 --> 00:42:32,880
report story would be an improvement by

00:42:25,270 --> 00:42:35,560
then yeah I think that's moving like the

00:42:32,880 --> 00:42:37,870
dependencies to like a data level

00:42:35,560 --> 00:42:39,070
instead of like a job level that could

00:42:37,870 --> 00:42:41,230
be interesting that's also like what

00:42:39,070 --> 00:42:44,470
some of the other companies are doing so

00:42:41,230 --> 00:42:49,210
like LinkedIn is building azkaban to

00:42:44,470 --> 00:42:51,250
solve this problem and Spotify build

00:42:49,210 --> 00:42:53,100
Luigi it's a lot of sort of

00:42:51,250 --> 00:42:55,900
contributions from some other companies

00:42:53,100 --> 00:42:57,580
and they're mainly focused on like these

00:42:55,900 --> 00:42:59,850
data dependencies and building like the

00:42:57,580 --> 00:43:04,870
dependency graph for for those processes

00:42:59,850 --> 00:43:10,320
and if we really try to build the

00:43:04,870 --> 00:43:15,790
autonomous steps so we we have yeah well

00:43:10,320 --> 00:43:18,270
we have less need for for that the

00:43:15,790 --> 00:43:18,270
starts to question

00:43:22,290 --> 00:43:30,790
other questions okay and thank you for

00:43:26,770 --> 00:43:34,540
your time and slide from the

00:43:30,790 --> 00:43:39,120
organization you can leave your feedback

00:43:34,540 --> 00:43:39,120
somewhere in the app okay thanks

00:43:48,760 --> 00:43:50,820

YouTube URL: https://www.youtube.com/watch?v=rldLDWf49no


