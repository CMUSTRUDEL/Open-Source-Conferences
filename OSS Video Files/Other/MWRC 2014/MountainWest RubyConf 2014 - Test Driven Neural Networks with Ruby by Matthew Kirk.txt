Title: MountainWest RubyConf 2014 - Test Driven Neural Networks with Ruby by Matthew Kirk
Publication date: 2020-01-24
Playlist: MWRC 2014
Description: 
	Neural networks are an excellent way of mapping past observations to a functional model. Many researchers have been able to build tools to recognize handwriting, or even jaundice detection. While Neural Networks are powerful they still are somewhat of a mystery to many. This talk aims to explain neural networks in a test driven way. We'll write tests first and go through how to build a neural network to determine what language a sentence is. By the end of this talk you'll know how to build neural networks with tests!

Help us caption & translate this video!

http://amara.org/v/FG2I/
Captions: 
	00:00:24,980 --> 00:00:33,630
who remembers email before gmail who

00:00:30,810 --> 00:00:37,170
remembers the massive amount of spam

00:00:33,630 --> 00:00:40,830
that we used to get before gmail still

00:00:37,170 --> 00:00:43,920
do yes we do I remember when I switched

00:00:40,830 --> 00:00:47,280
from having my excite email account to

00:00:43,920 --> 00:00:49,199
having gmail it was like entering a new

00:00:47,280 --> 00:00:51,180
haven where I didn't have to spend all

00:00:49,199 --> 00:00:53,100
of my day marking things as spam or not

00:00:51,180 --> 00:01:00,260
spam and deleting things out of my inbox

00:00:53,100 --> 00:01:00,260
it was wonderful who remembers these

00:01:00,530 --> 00:01:06,090
when I was a kid listen to my favorite

00:01:04,140 --> 00:01:09,000
radio station and a have a cassette in

00:01:06,090 --> 00:01:10,619
the tuner waiting for that opportune

00:01:09,000 --> 00:01:15,360
moment where I could run across the room

00:01:10,619 --> 00:01:18,060
and hit record about after six hours or

00:01:15,360 --> 00:01:20,729
so I'd have maybe 20 minutes of a good

00:01:18,060 --> 00:01:23,130
mix tape of course I no longer have to

00:01:20,729 --> 00:01:26,909
do that because I have spotify i have

00:01:23,130 --> 00:01:28,380
pandora on my phone it's really amazing

00:01:26,909 --> 00:01:30,960
because i don't have to spend any time

00:01:28,380 --> 00:01:33,990
making mixtapes anymore although they

00:01:30,960 --> 00:01:36,600
are kind of fun to make what do these

00:01:33,990 --> 00:01:39,720
two things have in common they're both

00:01:36,600 --> 00:01:42,570
using data to solve a problem to make

00:01:39,720 --> 00:01:44,520
our lives much easier pandora makes our

00:01:42,570 --> 00:01:48,570
life easier gmail makes our life easier

00:01:44,520 --> 00:01:49,920
spam filtering really so today I'm here

00:01:48,570 --> 00:01:51,720
to issue every single one of you a

00:01:49,920 --> 00:01:54,780
challenge and that is somewhere in this

00:01:51,720 --> 00:01:56,549
room there is somebody who's going to

00:01:54,780 --> 00:01:58,920
make the next gmail they're going to

00:01:56,549 --> 00:02:02,490
make the next Pandora or maybe it's a

00:01:58,920 --> 00:02:06,150
group of people data as we all know it

00:02:02,490 --> 00:02:07,350
is a big deal data big data whatever you

00:02:06,150 --> 00:02:10,259
want to call it it's a bunch of

00:02:07,350 --> 00:02:14,540
marketing humble jumbo but it is a big

00:02:10,259 --> 00:02:14,540
deal data is the new bacon

00:02:14,800 --> 00:02:23,710
I really like this shirt I don't

00:02:20,740 --> 00:02:29,830
actually eat bacon but this is so

00:02:23,710 --> 00:02:31,450
awesome but you might be saying that I

00:02:29,830 --> 00:02:33,970
am at the wrong conference i'm not a

00:02:31,450 --> 00:02:37,300
java 1 i'm not at PyCon i'm not a

00:02:33,970 --> 00:02:39,670
closure conch this is not a big data

00:02:37,300 --> 00:02:41,380
conference this is a ruby conference and

00:02:39,670 --> 00:02:47,980
i must be in the wrong place because

00:02:41,380 --> 00:02:49,840
ruby is not a big data language but i am

00:02:47,980 --> 00:02:52,360
going to agree with what been pretty

00:02:49,840 --> 00:02:55,450
much pointed out before this talk which

00:02:52,360 --> 00:02:57,700
is that ruby has lots of tools ruby has

00:02:55,450 --> 00:03:02,410
plenty of tools whether it's in c

00:02:57,700 --> 00:03:07,630
libraries regular Ruby Ruby gems Java

00:03:02,410 --> 00:03:10,060
through JRuby ruby has many tools but

00:03:07,630 --> 00:03:12,970
unfortunately a lot of us don't know how

00:03:10,060 --> 00:03:16,540
to go about solving data type problems

00:03:12,970 --> 00:03:19,570
because data science machine learning is

00:03:16,540 --> 00:03:21,670
kind of a big freaking mess if you were

00:03:19,570 --> 00:03:24,400
to open up a journal on machine learning

00:03:21,670 --> 00:03:26,380
you would see lots and lots of comments

00:03:24,400 --> 00:03:29,410
about things that most of us don't

00:03:26,380 --> 00:03:31,060
understand because honestly most of the

00:03:29,410 --> 00:03:36,400
academics are trying to understand it

00:03:31,060 --> 00:03:39,700
themselves on top of that Ruby is not

00:03:36,400 --> 00:03:43,330
about complex math matts created Ruby

00:03:39,700 --> 00:03:49,930
for our happiness complex math was not

00:03:43,330 --> 00:03:51,820
created for our happiness but that can

00:03:49,930 --> 00:03:54,010
actually be in our favor because Ruby is

00:03:51,820 --> 00:03:59,860
such a wonderful language I personally

00:03:54,010 --> 00:04:03,910
love working with Ruby next slide come

00:03:59,860 --> 00:04:09,310
on and my computer is freezing how

00:04:03,910 --> 00:04:14,080
wonderful is that it must be the complex

00:04:09,310 --> 00:04:16,419
math now this next slide that is

00:04:14,080 --> 00:04:18,489
supposed to come up which my computer

00:04:16,419 --> 00:04:23,560
really is freezing how wonderful is that

00:04:18,489 --> 00:04:26,320
is talking about Who I am my name is

00:04:23,560 --> 00:04:28,389
Matthew Kirk I have been doing data

00:04:26,320 --> 00:04:31,569
science type things machine

00:04:28,389 --> 00:04:33,999
learning for eight years a long time

00:04:31,569 --> 00:04:35,680
before I even realized that it was even

00:04:33,999 --> 00:04:38,770
called machine learning I learned it as

00:04:35,680 --> 00:04:41,669
operations research a long time ago and

00:04:38,770 --> 00:04:46,499
I've been doing Ruby for five years I

00:04:41,669 --> 00:04:50,189
really love both I love Ruby and I love

00:04:46,499 --> 00:04:55,360
complex math I like machine learning

00:04:50,189 --> 00:04:57,729
let's try this come on there we go on

00:04:55,360 --> 00:05:00,069
top of that I'm actually writing a book

00:04:57,729 --> 00:05:02,949
about using machine learning with Ruby

00:05:00,069 --> 00:05:05,080
and just talk that I'm going to be

00:05:02,949 --> 00:05:09,069
giving today is a chapter out of the

00:05:05,080 --> 00:05:11,860
book today we're going to cover feed

00:05:09,069 --> 00:05:16,539
forward neural networks doesn't that

00:05:11,860 --> 00:05:21,789
sound exciting come on I want some more

00:05:16,539 --> 00:05:25,449
out of the audience all right neural

00:05:21,789 --> 00:05:28,060
networks is an extremely vast subject so

00:05:25,449 --> 00:05:31,270
we're going to condense it quite a bit

00:05:28,060 --> 00:05:32,110
because this is 30 minutes long and so

00:05:31,270 --> 00:05:33,550
we're only going to talk about

00:05:32,110 --> 00:05:35,620
feed-forward neural networks there's

00:05:33,550 --> 00:05:38,199
plenty more that you can learn about but

00:05:35,620 --> 00:05:39,580
let's just focus on one thing we're

00:05:38,199 --> 00:05:41,139
going to go through an example of how to

00:05:39,580 --> 00:05:43,870
classify strings to a particular

00:05:41,139 --> 00:05:47,199
language and we're going to do it in a

00:05:43,870 --> 00:05:49,330
test driven fashion it just approved you

00:05:47,199 --> 00:05:50,949
that I'm not making things up we're

00:05:49,330 --> 00:05:56,439
going to actually demo it at the very

00:05:50,949 --> 00:05:59,050
end so neural networks neural networks

00:05:56,439 --> 00:06:00,639
are a supervised learning method which

00:05:59,050 --> 00:06:03,129
you remember from the last talk is

00:06:00,639 --> 00:06:05,469
basically the idea that you have data

00:06:03,129 --> 00:06:07,149
points that have particular labels and

00:06:05,469 --> 00:06:10,060
you want to learn something off of that

00:06:07,149 --> 00:06:13,360
a particular pattern neural networks are

00:06:10,060 --> 00:06:15,189
really interesting because you really

00:06:13,360 --> 00:06:18,399
don't have a lot of restriction you can

00:06:15,189 --> 00:06:19,719
map just about anything with them so

00:06:18,399 --> 00:06:21,819
that's why i like to call them kind of a

00:06:19,719 --> 00:06:24,009
sledgehammer they're really good about

00:06:21,819 --> 00:06:28,839
taking any functional approximation and

00:06:24,009 --> 00:06:30,969
just making it happen if anybody has

00:06:28,839 --> 00:06:32,979
ever done computer science most likely

00:06:30,969 --> 00:06:34,930
you've heard about neural networks a

00:06:32,979 --> 00:06:38,500
little bit and some of you probably have

00:06:34,930 --> 00:06:41,080
seen a graph like this anybody all right

00:06:38,500 --> 00:06:42,249
so some of you neural networks are split

00:06:41,080 --> 00:06:45,999
up into three different

00:06:42,249 --> 00:06:48,999
layers now you have the out input layer

00:06:45,999 --> 00:06:52,749
which is simply just the inputs these

00:06:48,999 --> 00:06:55,599
are zero to one binary type inputs true

00:06:52,749 --> 00:06:57,249
false type inputs really it's pretty

00:06:55,599 --> 00:07:00,309
simple to know it's just the input of

00:06:57,249 --> 00:07:02,199
what you're looking at the thing that

00:07:00,309 --> 00:07:05,319
confuses people the most would have to

00:07:02,199 --> 00:07:06,969
be this hidden layer whereby we're

00:07:05,319 --> 00:07:10,539
adding complexity to this particular

00:07:06,969 --> 00:07:12,429
function and we can't see what's going

00:07:10,539 --> 00:07:16,059
on in there it's more or less like a

00:07:12,429 --> 00:07:18,669
private method of this function we don't

00:07:16,059 --> 00:07:21,849
see it we can't observe it it's just

00:07:18,669 --> 00:07:24,399
there and we know it's there and lastly

00:07:21,849 --> 00:07:28,539
we have the output which again is just

00:07:24,399 --> 00:07:31,259
an output out of this function now

00:07:28,539 --> 00:07:35,199
inside of these neural networks

00:07:31,259 --> 00:07:37,360
conceptually we have neurons and if any

00:07:35,199 --> 00:07:39,189
of you have ever taken biology we all

00:07:37,360 --> 00:07:41,289
have neural networks in our brains and

00:07:39,189 --> 00:07:43,569
we have little neurons that are talking

00:07:41,289 --> 00:07:46,329
with other neurons it's the same idea

00:07:43,569 --> 00:07:49,179
with artificial neural networks except

00:07:46,329 --> 00:07:52,769
that artificial neurons which are these

00:07:49,179 --> 00:07:56,739
little pieces are just a weighted sum

00:07:52,769 --> 00:07:58,149
now x1 x2 weighted together and then

00:07:56,739 --> 00:07:59,709
wrapped in this function that doesn't

00:07:58,149 --> 00:08:02,860
mean anything right it doesn't make any

00:07:59,709 --> 00:08:06,599
sense so I think a better analogy is

00:08:02,860 --> 00:08:09,759
thinking about logic neural networks and

00:08:06,599 --> 00:08:12,429
perceptrons any of those types of things

00:08:09,759 --> 00:08:15,729
was based on this idea of threshold

00:08:12,429 --> 00:08:17,639
logic where you could input data and you

00:08:15,729 --> 00:08:21,149
can determine whether is true or false

00:08:17,639 --> 00:08:24,009
very similar to a digital logic gate or

00:08:21,149 --> 00:08:27,579
boolean logic where you're saying this

00:08:24,009 --> 00:08:30,669
and that very simple but neural networks

00:08:27,579 --> 00:08:34,209
takes it a little bit further with fuzzy

00:08:30,669 --> 00:08:37,120
logic no longer do things have to be 0

00:08:34,209 --> 00:08:42,209
or 1 it can be kinda true or kind of

00:08:37,120 --> 00:08:45,339
false it's a range between 0 and 1 this

00:08:42,209 --> 00:08:47,019
is extremely powerful because no longer

00:08:45,339 --> 00:08:50,439
does things have to be black or white

00:08:47,019 --> 00:08:53,769
you can really bask in the gray area and

00:08:50,439 --> 00:08:55,800
so this is an extremely powerful piece

00:08:53,769 --> 00:08:58,660
of neural networks

00:08:55,800 --> 00:09:01,720
but really this analogy is probably the

00:08:58,660 --> 00:09:04,030
best idea to get us understanding what

00:09:01,720 --> 00:09:07,600
this particular neuron does it takes two

00:09:04,030 --> 00:09:10,480
inputs it adds them together in a

00:09:07,600 --> 00:09:12,730
particular logical way just like this

00:09:10,480 --> 00:09:16,980
digital logic gate would and then

00:09:12,730 --> 00:09:19,990
outputs the signal to the next neuron

00:09:16,980 --> 00:09:22,600
now if we were to go back you'll notice

00:09:19,990 --> 00:09:27,940
that we have this weighted sum wrapped

00:09:22,600 --> 00:09:34,090
in a function now let's assume that we

00:09:27,940 --> 00:09:38,620
have x1 is say 20,000 and x2 is 50,000

00:09:34,090 --> 00:09:40,810
and the w's are just both ones it's

00:09:38,620 --> 00:09:44,050
going to make a very big number come out

00:09:40,810 --> 00:09:46,090
and really that's not between zero and

00:09:44,050 --> 00:09:49,750
one so all this function does which is

00:09:46,090 --> 00:09:52,690
called an activation function this it

00:09:49,750 --> 00:09:55,720
takes any input and makes it between

00:09:52,690 --> 00:09:58,630
zero and one now these are a lot of

00:09:55,720 --> 00:10:00,310
different funny mathematical terms that

00:09:58,630 --> 00:10:01,810
I don't expect anybody to take away from

00:10:00,310 --> 00:10:04,720
this because it doesn't really matter

00:10:01,810 --> 00:10:08,680
just realize that there's the learning

00:10:04,720 --> 00:10:10,150
curve the bell curve line threshold but

00:10:08,680 --> 00:10:14,170
the most important thing to take out of

00:10:10,150 --> 00:10:16,930
this is really the learning curve now we

00:10:14,170 --> 00:10:19,990
all know that when we start learning

00:10:16,930 --> 00:10:22,270
something we suck at it so we start at

00:10:19,990 --> 00:10:25,090
the bottom and then we get better and

00:10:22,270 --> 00:10:27,550
then we plateau that is the learning

00:10:25,090 --> 00:10:30,550
curve that's all the sigmoid function

00:10:27,550 --> 00:10:33,730
and the Elliot function does it takes a

00:10:30,550 --> 00:10:37,300
big number like 50,000 it makes it

00:10:33,730 --> 00:10:41,470
closer to 1 whereas negative 50,000

00:10:37,300 --> 00:10:46,200
would be closer to 0 so all it's doing

00:10:41,470 --> 00:10:46,200
is making sure that it's true or false

00:10:46,470 --> 00:10:51,520
now there's one piece that have left out

00:10:49,750 --> 00:10:53,020
of this and I'm kind of hand waving a

00:10:51,520 --> 00:10:54,490
lot of this because we only have 30

00:10:53,020 --> 00:10:59,170
minutes to talk about neural networks

00:10:54,490 --> 00:11:03,550
and that is finding the weights in these

00:10:59,170 --> 00:11:05,740
sums now you could brute force that and

00:11:03,550 --> 00:11:07,780
that would take an inordinate amount of

00:11:05,740 --> 00:11:08,870
time or you could do it much more

00:11:07,780 --> 00:11:10,610
intelligently

00:11:08,870 --> 00:11:13,250
people have come up with something

00:11:10,610 --> 00:11:15,800
called training algorithms quick prop

00:11:13,250 --> 00:11:18,620
are proud back prop our prop is the

00:11:15,800 --> 00:11:21,140
fastest but what you need to know is

00:11:18,620 --> 00:11:24,770
that really what we're doing is we're

00:11:21,140 --> 00:11:27,230
trying to minimize air and the way all

00:11:24,770 --> 00:11:30,230
of these algorithms do it is simply

00:11:27,230 --> 00:11:32,000
something called gradient descent so

00:11:30,230 --> 00:11:35,660
imagine that I was at the top of a hill

00:11:32,000 --> 00:11:37,190
in the middle of a fog really really

00:11:35,660 --> 00:11:42,529
thick fog and there was a tree in front

00:11:37,190 --> 00:11:44,360
of me and somehow I'm the aim guy and I

00:11:42,529 --> 00:11:47,360
want to get to the base camp right

00:11:44,360 --> 00:11:49,610
because that's all I care about at that

00:11:47,360 --> 00:11:51,589
particular moment I can't see anything

00:11:49,610 --> 00:11:55,180
in front of me all I can see is right

00:11:51,589 --> 00:11:58,640
what's in front of a right so

00:11:55,180 --> 00:12:01,430
intuitively what i would do is I'd look

00:11:58,640 --> 00:12:04,400
around me and I'd see which direction is

00:12:01,430 --> 00:12:07,430
going down the steepest and then walk

00:12:04,400 --> 00:12:09,410
that direction and then again keep doing

00:12:07,430 --> 00:12:11,839
that until eventually I hit the bottom

00:12:09,410 --> 00:12:14,390
of the valley that's what all of these

00:12:11,839 --> 00:12:18,200
algorithms do except the bottom of the

00:12:14,390 --> 00:12:23,270
valley is air this air rate is pretty

00:12:18,200 --> 00:12:25,520
much given a model how often do we

00:12:23,270 --> 00:12:28,959
expect it to be wrong we want to

00:12:25,520 --> 00:12:34,120
minimize this air so that our model

00:12:28,959 --> 00:12:34,120
effectively models the data that we have

00:12:35,500 --> 00:12:42,470
that is just a tip of the iceberg neural

00:12:39,320 --> 00:12:44,810
networks is an extremely dense subject

00:12:42,470 --> 00:12:47,180
you can get into RBF networks deep

00:12:44,810 --> 00:12:51,380
learning cyclical networks there is

00:12:47,180 --> 00:12:54,050
tomes of volumes of written things about

00:12:51,380 --> 00:12:55,940
neural networks it's crazy but that is

00:12:54,050 --> 00:12:59,270
feed for old feed-forward neural

00:12:55,940 --> 00:13:02,540
networks in a nutshell very simply it's

00:12:59,270 --> 00:13:04,700
just a complicated weighted sum so that

00:13:02,540 --> 00:13:07,010
all you get out at the end of it is a

00:13:04,700 --> 00:13:11,810
true or false type statement it's a

00:13:07,010 --> 00:13:13,250
fuzzy logic type of a function but since

00:13:11,810 --> 00:13:14,810
we're at an actual programming

00:13:13,250 --> 00:13:18,560
conference I thought I would focus more

00:13:14,810 --> 00:13:21,590
on implementation what can you really do

00:13:18,560 --> 00:13:26,260
with a neural network

00:13:21,590 --> 00:13:29,600
and to preface this example one day

00:13:26,260 --> 00:13:32,360
being the avid foreign language student

00:13:29,600 --> 00:13:35,240
that I am I went to google translate and

00:13:32,360 --> 00:13:37,160
i started typing in a German word and lo

00:13:35,240 --> 00:13:39,560
and behold Google and all of its

00:13:37,160 --> 00:13:42,080
infinite wisdom said would you like to

00:13:39,560 --> 00:13:45,500
translate this from German and of course

00:13:42,080 --> 00:13:47,870
yes I did now thinking about this I

00:13:45,500 --> 00:13:50,960
wanted to figure out how on earth does

00:13:47,870 --> 00:13:54,140
Google do this it doesn't read my mind

00:13:50,960 --> 00:13:59,480
obviously because i don't have an API on

00:13:54,140 --> 00:14:02,030
that and I mean I suppose they could

00:13:59,480 --> 00:14:04,070
they could do some sort of a stem hash

00:14:02,030 --> 00:14:06,230
look up you could build an enormous

00:14:04,070 --> 00:14:09,500
distributed hash and look up stems

00:14:06,230 --> 00:14:11,990
that's probably what Google is doing but

00:14:09,500 --> 00:14:15,530
I wanted to figure out what's a really

00:14:11,990 --> 00:14:18,470
fast way of classifying something like

00:14:15,530 --> 00:14:22,340
gesund which is a German stem into

00:14:18,470 --> 00:14:24,980
German and of course since this is a

00:14:22,340 --> 00:14:30,110
talk about neural networks we're going

00:14:24,980 --> 00:14:32,690
to use neural networks let specifically

00:14:30,110 --> 00:14:35,300
I wanted to bring this down this example

00:14:32,690 --> 00:14:38,600
a little bit down into very specifically

00:14:35,300 --> 00:14:43,370
about what will classify and it's only

00:14:38,600 --> 00:14:46,580
these Latin languages now I could have

00:14:43,370 --> 00:14:49,040
thrown in Chinese and Japanese and Hindi

00:14:46,580 --> 00:14:52,100
and many other languages but that would

00:14:49,040 --> 00:14:55,910
kind of be cheating because when you see

00:14:52,100 --> 00:14:58,220
a Chinese character its Chinese so those

00:14:55,910 --> 00:15:00,230
kinds of languages aren't as difficult

00:14:58,220 --> 00:15:02,060
but these languages like English German

00:15:00,230 --> 00:15:04,820
Polish Swedish Finnish they all have

00:15:02,060 --> 00:15:09,680
Latin characters a little bit of funny

00:15:04,820 --> 00:15:11,990
umlauts and O's and whatever so specific

00:15:09,680 --> 00:15:16,580
how would we do this how are we going to

00:15:11,990 --> 00:15:19,000
do this the first step in all machine

00:15:16,580 --> 00:15:23,360
learning problems is to collect data

00:15:19,000 --> 00:15:25,370
data is really the most important thing

00:15:23,360 --> 00:15:26,630
in machine learning because without data

00:15:25,370 --> 00:15:30,310
you wouldn't be able to learn anything

00:15:26,630 --> 00:15:32,510
at all machine learning is all about

00:15:30,310 --> 00:15:34,520
inductive reasoning it's all about

00:15:32,510 --> 00:15:37,070
taking the data that we have

00:15:34,520 --> 00:15:39,880
and determining a particular pattern out

00:15:37,070 --> 00:15:43,190
of it so data is really important and

00:15:39,880 --> 00:15:47,780
again I wanted to make it easy on myself

00:15:43,190 --> 00:15:49,340
so I just used the most translated book

00:15:47,780 --> 00:15:51,020
in the world which happens to be the

00:15:49,340 --> 00:15:52,670
Bible because you can get it for free

00:15:51,020 --> 00:15:55,040
there's no copyright infringement

00:15:52,670 --> 00:15:57,920
nothing like that I'm not doing anything

00:15:55,040 --> 00:16:01,820
special except downloading the chapters

00:15:57,920 --> 00:16:03,980
out of Matthew in acts but this data is

00:16:01,820 --> 00:16:10,820
freely accessible so there's a really

00:16:03,980 --> 00:16:13,250
good corpus of data of tagged text but

00:16:10,820 --> 00:16:16,040
as you can remember neural networks are

00:16:13,250 --> 00:16:18,320
about zeros and ones and text has

00:16:16,040 --> 00:16:22,760
nothing to do with zeros or ones so what

00:16:18,320 --> 00:16:25,520
exactly are we going to do we could do a

00:16:22,760 --> 00:16:28,940
lot of different things with text we can

00:16:25,520 --> 00:16:31,490
split it up into stems words frequencies

00:16:28,940 --> 00:16:34,070
most likely Google is probably using

00:16:31,490 --> 00:16:38,510
stems because that's the right way to do

00:16:34,070 --> 00:16:40,250
it and it's probably more accurate but

00:16:38,510 --> 00:16:43,100
what if we could do it with frequency of

00:16:40,250 --> 00:16:45,260
letters just based purely off of the

00:16:43,100 --> 00:16:50,240
amount of times that a letter shows up

00:16:45,260 --> 00:16:52,820
in a sentence now looking at the data I

00:16:50,240 --> 00:16:55,850
just extracted out the frequency of

00:16:52,820 --> 00:16:57,890
letters sorted across all these

00:16:55,850 --> 00:17:00,320
different languages you'll see that

00:16:57,890 --> 00:17:03,950
there is somewhat of a characteristic

00:17:00,320 --> 00:17:07,280
now it's not perfect it's not like we

00:17:03,950 --> 00:17:10,490
can just fit a line to it but there's

00:17:07,280 --> 00:17:13,160
something there and I don't know if any

00:17:10,490 --> 00:17:16,819
of you remember this show but there was

00:17:13,160 --> 00:17:19,490
a show called ghost rider and

00:17:16,819 --> 00:17:21,799
I remember this episode really well

00:17:19,490 --> 00:17:24,319
where they crack the code because they

00:17:21,799 --> 00:17:27,139
realize that English has a lot of ease

00:17:24,319 --> 00:17:32,779
in it well this is pretty much the same

00:17:27,139 --> 00:17:34,880
thing you a lot of languages have these

00:17:32,779 --> 00:17:37,940
characteristics to them in that polish

00:17:34,880 --> 00:17:39,860
has a lots of Y's and Z's finish has a

00:17:37,940 --> 00:17:42,970
lot of everything in it for some reason

00:17:39,860 --> 00:17:46,610
a lot of vowels they use a lot of owls

00:17:42,970 --> 00:17:48,470
English of course uses a lot of ease but

00:17:46,610 --> 00:17:50,919
this is useful because we can actually

00:17:48,470 --> 00:17:54,110
use this there is something there that

00:17:50,919 --> 00:17:56,600
maybe we can't intuitively do in our

00:17:54,110 --> 00:18:01,549
brains but we could build something to

00:17:56,600 --> 00:18:03,470
learn from this particular data so now

00:18:01,549 --> 00:18:06,799
we're at the point where we actually

00:18:03,470 --> 00:18:09,470
need to build a neural net and a lot of

00:18:06,799 --> 00:18:12,350
people would start loading up IRB and

00:18:09,470 --> 00:18:15,740
screwing around with many different gems

00:18:12,350 --> 00:18:18,019
like T learn or Ruby a and n which

00:18:15,740 --> 00:18:20,179
stands for artificial neural nets and

00:18:18,019 --> 00:18:22,509
that's perfectly fine you can spike

00:18:20,179 --> 00:18:25,100
things but I personally believe that

00:18:22,509 --> 00:18:26,870
test driven development applied to

00:18:25,100 --> 00:18:29,649
machine learning is a really great

00:18:26,870 --> 00:18:32,330
approach because when you're approaching

00:18:29,649 --> 00:18:34,700
scientific type problems you're writing

00:18:32,330 --> 00:18:36,740
down a hypothesis testing against it and

00:18:34,700 --> 00:18:39,110
then iterating over and over and over

00:18:36,740 --> 00:18:42,919
it's very similar to the red green

00:18:39,110 --> 00:18:46,250
refactor cycle that everybody knows so

00:18:42,919 --> 00:18:50,240
if we were to test drive test yeah

00:18:46,250 --> 00:18:51,590
test-driven develop neural nets we would

00:18:50,240 --> 00:18:54,070
really need to test a few different

00:18:51,590 --> 00:18:56,779
things and that is testing the seams

00:18:54,070 --> 00:19:02,450
testing whether our model over or under

00:18:56,779 --> 00:19:04,789
fits the data now the model conceptually

00:19:02,450 --> 00:19:06,620
will look something like this I couldn't

00:19:04,789 --> 00:19:09,950
fit it all on to this slide because

00:19:06,620 --> 00:19:13,850
there's fifty four input nodes whereby

00:19:09,950 --> 00:19:15,860
there's 54 unique characters across all

00:19:13,850 --> 00:19:18,649
of these different languages there's a

00:19:15,860 --> 00:19:21,559
bunch of hidden units imagine 40 or so

00:19:18,649 --> 00:19:25,909
and then six languages which are just

00:19:21,559 --> 00:19:28,340
bits testing the seams which we need to

00:19:25,909 --> 00:19:31,330
start with is really focused on testing

00:19:28,340 --> 00:19:31,330
this part right here

00:19:32,090 --> 00:19:38,490
so to make very quick pseudocode which

00:19:36,060 --> 00:19:42,030
i'll actually show you in just a second

00:19:38,490 --> 00:19:45,120
i just wanted to point out that the

00:19:42,030 --> 00:19:47,670
overall test would be making sure that

00:19:45,120 --> 00:19:49,740
what you're inputting is correct making

00:19:47,670 --> 00:19:52,590
sure that everything adds up to one so

00:19:49,740 --> 00:19:55,560
for instance for every vector they

00:19:52,590 --> 00:19:59,370
should add up to one so think of it as

00:19:55,560 --> 00:20:02,570
each node that comes in the character

00:19:59,370 --> 00:20:07,500
nodes should be a probability of that

00:20:02,570 --> 00:20:11,310
particular character so if you had a

00:20:07,500 --> 00:20:16,590
sentence of a be a would be 50 and B

00:20:11,310 --> 00:20:20,640
would be fifty fifty percent so that's

00:20:16,590 --> 00:20:24,590
testing the seems very simply now

00:20:20,640 --> 00:20:28,590
secondly we need to test for

00:20:24,590 --> 00:20:33,590
underfitting and the idea for this is

00:20:28,590 --> 00:20:36,840
that a lot of a lot of the times

00:20:33,590 --> 00:20:41,250
mathematical models will either under

00:20:36,840 --> 00:20:44,460
fit or over fit data and what's what's

00:20:41,250 --> 00:20:48,300
hard about neural networks is that it

00:20:44,460 --> 00:20:51,510
has this internal rate of air so as you

00:20:48,300 --> 00:20:53,490
remember from the the little slide where

00:20:51,510 --> 00:20:55,400
the guys walking down the valley we're

00:20:53,490 --> 00:20:58,110
trying to minimize the air but

00:20:55,400 --> 00:21:02,400
unfortunately what ends up happening it

00:20:58,110 --> 00:21:05,760
is over iterations let me get to it over

00:21:02,400 --> 00:21:07,860
iterations where a neural net Wiz

00:21:05,760 --> 00:21:09,420
iterating and trying to get better and

00:21:07,860 --> 00:21:11,820
better and better well we'll end up

00:21:09,420 --> 00:21:15,210
happening is is the air the internal air

00:21:11,820 --> 00:21:18,510
will get that will keep lowering but the

00:21:15,210 --> 00:21:21,630
real air will do this little polynomial

00:21:18,510 --> 00:21:24,930
type thing now the reason for that is

00:21:21,630 --> 00:21:30,060
because over time the neural net will

00:21:24,930 --> 00:21:32,130
just keep remembering things so instead

00:21:30,060 --> 00:21:34,260
of teaching it all of the inputs and all

00:21:32,130 --> 00:21:35,700
of the outputs which we could do we

00:21:34,260 --> 00:21:38,250
could just load everything into memory

00:21:35,700 --> 00:21:39,570
and make this massive decision tree what

00:21:38,250 --> 00:21:41,370
we want to do is we want to make sure

00:21:39,570 --> 00:21:43,779
that we're minimizing both of these

00:21:41,370 --> 00:21:45,820
errors and the way you do that is

00:21:43,779 --> 00:21:48,070
or something called cross validation and

00:21:45,820 --> 00:21:51,249
cross validation is a very simple idea

00:21:48,070 --> 00:21:53,499
where you have a data set you split it

00:21:51,249 --> 00:21:59,769
into two parts where you have validation

00:21:53,499 --> 00:22:01,809
and training and the training set is

00:21:59,769 --> 00:22:04,059
what you trained the neural network with

00:22:01,809 --> 00:22:09,639
and validation is just how you calculate

00:22:04,059 --> 00:22:11,259
a real air so that's a very simple thing

00:22:09,639 --> 00:22:13,269
to do you just split the data into two

00:22:11,259 --> 00:22:15,039
pieces and then you calculate the real

00:22:13,269 --> 00:22:20,259
air as opposed to what the neural net is

00:22:15,039 --> 00:22:22,359
telling you the air is and that would

00:22:20,259 --> 00:22:24,729
simply look something like this for each

00:22:22,359 --> 00:22:27,519
language it trains cross validates with

00:22:24,729 --> 00:22:28,869
a five percent error rate or below now

00:22:27,519 --> 00:22:31,659
the important thing to point out here is

00:22:28,869 --> 00:22:33,580
that it's not important what the air

00:22:31,659 --> 00:22:36,249
rate is it's just important to write

00:22:33,580 --> 00:22:38,169
down your first assumption so that when

00:22:36,249 --> 00:22:41,979
you actually get all of the code working

00:22:38,169 --> 00:22:45,960
you can tweak or you can raise up the

00:22:41,979 --> 00:22:48,369
air because you don't care anymore and

00:22:45,960 --> 00:22:50,169
lastly I wanted to give one little plug

00:22:48,369 --> 00:22:53,259
to something called Occam's razor and

00:22:50,169 --> 00:22:56,349
that is this idea that if a neural

00:22:53,259 --> 00:22:59,379
network takes 50,000 iterations to

00:22:56,349 --> 00:23:02,769
complete it's probably not a very good

00:22:59,379 --> 00:23:06,399
model Occam's razor is all about the

00:23:02,769 --> 00:23:08,739
simplest solution being the best now

00:23:06,399 --> 00:23:13,809
implicitly we can kind of test for this

00:23:08,739 --> 00:23:16,269
weather in test speed but on the other

00:23:13,809 --> 00:23:19,469
hand we can also set a max iteration in

00:23:16,269 --> 00:23:24,729
a neural net and that's what I've done

00:23:19,469 --> 00:23:27,659
so that's a lot of concepts in theory so

00:23:24,729 --> 00:23:35,139
let's actually look at some real code

00:23:27,659 --> 00:23:38,469
all right wow this is okay so let's look

00:23:35,139 --> 00:23:40,269
at the seam test really quick basically

00:23:38,469 --> 00:23:44,499
what i'm doing here is I have all of

00:23:40,269 --> 00:23:49,599
these characters a through z this is a

00:23:44,499 --> 00:23:53,139
really nasty character which is the it's

00:23:49,599 --> 00:23:55,580
a utf-8 space which I'll get to in a

00:23:53,139 --> 00:23:58,470
second about how nasty that is

00:23:55,580 --> 00:24:00,300
here's a bunch of umlauts and funny

00:23:58,470 --> 00:24:03,150
polish characters characters I don't

00:24:00,300 --> 00:24:05,700
care about really what we're doing down

00:24:03,150 --> 00:24:08,340
here is we're asserting that these

00:24:05,700 --> 00:24:10,530
vectors that I'm making must have a

00:24:08,340 --> 00:24:14,880
through z I want everything to be

00:24:10,530 --> 00:24:19,290
downcast I want them to be unique and I

00:24:14,880 --> 00:24:22,140
want them to sum up to 1 now getting

00:24:19,290 --> 00:24:24,800
back to the whole this utf-8 space

00:24:22,140 --> 00:24:27,840
character being such an annoyance

00:24:24,800 --> 00:24:29,130
without a seam test without the test I

00:24:27,840 --> 00:24:33,270
would have probably never found it

00:24:29,130 --> 00:24:34,710
because it wouldn't have notified me but

00:24:33,270 --> 00:24:36,690
all of a sudden when I was running this

00:24:34,710 --> 00:24:39,000
test and I was looking at the output it

00:24:36,690 --> 00:24:41,310
was saying oh one of your unique keys is

00:24:39,000 --> 00:24:44,310
a space and I'm thinking that is total

00:24:41,310 --> 00:24:46,230
crap like well I don't want a space but

00:24:44,310 --> 00:24:52,110
having the same test totally saved me in

00:24:46,230 --> 00:24:56,340
that really this is a simple simple

00:24:52,110 --> 00:24:59,460
simple class now getting into the cross

00:24:56,340 --> 00:25:02,280
validation test what I do here is I

00:24:59,460 --> 00:25:06,060
split the data into the Matthew verses

00:25:02,280 --> 00:25:08,190
and the axe versus I make this little

00:25:06,060 --> 00:25:10,980
helper function which compares the

00:25:08,190 --> 00:25:15,350
network with another text file I compare

00:25:10,980 --> 00:25:24,960
the boat both of them and then I run it

00:25:15,350 --> 00:25:25,890
now let's actually run that now I wanted

00:25:24,960 --> 00:25:28,560
to actually show you what this looks

00:25:25,890 --> 00:25:31,550
like in a production type setting

00:25:28,560 --> 00:25:34,410
because what's really awesome about this

00:25:31,550 --> 00:25:37,410
using a neural net is how ridiculously

00:25:34,410 --> 00:25:40,440
fast it is it's pretty much like taking

00:25:37,410 --> 00:25:42,780
this o of n thing to 0 of 1 but even

00:25:40,440 --> 00:25:47,190
faster than 0 of 1 because it's not a

00:25:42,780 --> 00:25:51,570
very big hash it's just a weighted sum

00:25:47,190 --> 00:25:57,750
and that's all it is so here's my little

00:25:51,570 --> 00:26:03,450
web app and I type in gesund and German

00:25:57,750 --> 00:26:06,000
shows up y'all robbing I think I think

00:26:03,450 --> 00:26:08,580
that's a Swedish word i want to say i

00:26:06,000 --> 00:26:11,100
don't i don't know Swedish

00:26:08,580 --> 00:26:13,919
so I loaded these up these are just

00:26:11,100 --> 00:26:16,260
little snippets out of I don't speak

00:26:13,919 --> 00:26:20,480
Finnish either but supposedly that's

00:26:16,260 --> 00:26:24,750
finish and let's see some German

00:26:20,480 --> 00:26:27,059
Norwegian like how crumbs trying to

00:26:24,750 --> 00:26:32,039
translate this for me that was really

00:26:27,059 --> 00:26:33,659
nice of them Polish Swedish and the

00:26:32,039 --> 00:26:35,429
eternal question that I wanted to throw

00:26:33,659 --> 00:26:42,799
in here is what language the Swedish

00:26:35,429 --> 00:26:48,210
Chef actually speaks English English

00:26:42,799 --> 00:26:49,860
Polish is whooping corn he he seems to

00:26:48,210 --> 00:26:53,100
speak a lot of English from what I've

00:26:49,860 --> 00:26:54,570
seen and some Norwegian finish so i

00:26:53,100 --> 00:27:00,659
think i think the answer to that

00:26:54,570 --> 00:27:01,799
question is that he's multicultural but

00:27:00,659 --> 00:27:03,649
there you have it what's really

00:27:01,799 --> 00:27:06,269
interesting about this and i'll show you

00:27:03,649 --> 00:27:09,269
actually right here is that every time

00:27:06,269 --> 00:27:11,220
it's it's actually hitting this and it

00:27:09,269 --> 00:27:14,100
takes absolutely no time to run this

00:27:11,220 --> 00:27:16,200
because it's just a weighted sum all it

00:27:14,100 --> 00:27:18,029
is is just taking zeros and ones waiting

00:27:16,200 --> 00:27:25,470
them together and then outputting an

00:27:18,029 --> 00:27:30,720
answer it's extremely fast okay so let's

00:27:25,470 --> 00:27:33,960
get back to this so I wanted to put this

00:27:30,720 --> 00:27:35,970
slide up here go to github you can

00:27:33,960 --> 00:27:37,980
actually download all of the code for

00:27:35,970 --> 00:27:39,659
this language predictor you can go sign

00:27:37,980 --> 00:27:42,210
up for the book that I'm writing about

00:27:39,659 --> 00:27:44,100
machine learning at that website you can

00:27:42,210 --> 00:27:47,700
talk to me on twitter if you don't like

00:27:44,100 --> 00:27:50,279
talking to people in real life there's

00:27:47,700 --> 00:27:55,649
some information about that so go ahead

00:27:50,279 --> 00:27:59,760
and get that down in conclusion this

00:27:55,649 --> 00:28:01,980
really is just the beginning neural Nets

00:27:59,760 --> 00:28:05,220
is an extremely vast subject machine

00:28:01,980 --> 00:28:08,039
learning is a vast subject but it's so

00:28:05,220 --> 00:28:10,260
important that we learn this stuff a lot

00:28:08,039 --> 00:28:12,690
of us are writing rails apps and we're

00:28:10,260 --> 00:28:15,779
making lots of data and we can actually

00:28:12,690 --> 00:28:17,510
end up making really cool tools just

00:28:15,779 --> 00:28:21,360
based purely off of machine learning

00:28:17,510 --> 00:28:22,260
it's it's a very exciting field and I

00:28:21,360 --> 00:28:24,240
full

00:28:22,260 --> 00:28:27,530
we encourage every one of you to learn

00:28:24,240 --> 00:28:27,530
more about it thank you

00:29:00,300 --> 00:29:02,360

YouTube URL: https://www.youtube.com/watch?v=mWZ1SPgx80g


