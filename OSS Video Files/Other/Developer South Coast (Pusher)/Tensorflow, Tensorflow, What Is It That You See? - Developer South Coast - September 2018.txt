Title: Tensorflow, Tensorflow, What Is It That You See? - Developer South Coast - September 2018
Publication date: 2018-10-11
Playlist: Developer South Coast
Description: 
	Presented by Ashic Mahtab.

Machines are doing things today that were unthinkable even a few years ago. From computer vision, to understanding human conversation, to translating text, or even generating art; they're encroaching on domains previously thought to be strictly human realms. The technology behind this is actually fairly old; neural networks have been around for decades. However, modern hardware capabilities have made them technology feasible to such a degree that they can now drive cars.

Tensorflow is Google's open source framework for deep learning. It makes it relatively straightforward to apply these techniques. In this session, we'll cover the basics of image recognition, and use Jupyter Notebooks, and Tensorflow to apply it.... with a dash of GPU processing to boot.

Key takeaways

You'll learn how machines can be endowed cognition.
You'll also see the basics of using Tensorflow.

_

About Pusher Sessions:

We're bringing the meetup to you. With Sessions, you can watch recordings of top-notch talks from developer meetups -- wherever and whenever you want.

Meetups are a great way to learn from our peers and to keep up with the latest trends and technologies. As developers ourselves, we at Pusher wanted to bring this great content to more people... So we built Sessions. On Sessions, you can watch talks that interest you and subscribe to be notified when new content gets added.

If you run a meetup and want to get involved, kindly get in touch.

_

About Pusher:

Pusher is a hosted service with APIs, developer tools and open source libraries that greatly simplify integrating real-time functionality into web and mobile applications. 

Pusher will automatically scale when required, removing all the pain of setting up and maintaining a secure, real-time infrastructure. 

Pusher is already trusted to do so by thousands of developers and companies like GitHub, MailChimp, the Financial Times, Buffer and many more. 

Getting started takes just a few seconds: simply go to pusher.com and create a free account. Happy hacking!
Captions: 
	00:00:00,030 --> 00:00:05,130
so today we will be talking about deep

00:00:03,240 --> 00:00:09,000
learning so forget the title is just to

00:00:05,130 --> 00:00:10,620
get you hooked we'll look at some types

00:00:09,000 --> 00:00:12,450
of Likert but it's mostly going to be

00:00:10,620 --> 00:00:16,529
about deep learning Basics around deep

00:00:12,450 --> 00:00:19,949
learning associated stuff the stock was

00:00:16,529 --> 00:00:23,130
first delivered on 5th of November there

00:00:19,949 --> 00:00:26,090
have been changes the slides or that

00:00:23,130 --> 00:00:30,390
date on that slide has not been updated

00:00:26,090 --> 00:00:32,009
anyhow so deep learning do any of you

00:00:30,390 --> 00:00:34,890
guys use some form of deep learning

00:00:32,009 --> 00:00:38,399
framework do you are you familiar with

00:00:34,890 --> 00:00:41,489
neural networks do you use neural

00:00:38,399 --> 00:00:45,239
networks for anything ok cool so this is

00:00:41,489 --> 00:00:48,809
just a right level of talk it is very

00:00:45,239 --> 00:00:49,980
introductory I don't know if we've got

00:00:48,809 --> 00:00:52,110
enough to fill the whole thing but

00:00:49,980 --> 00:00:55,410
hopefully we can go into details when

00:00:52,110 --> 00:00:57,090
and where necessary we'll start by

00:00:55,410 --> 00:00:59,430
looking at what deep deep learning is

00:00:57,090 --> 00:01:01,469
neural networks some tensorflow

00:00:59,430 --> 00:01:04,559
and then different types of neural

00:01:01,469 --> 00:01:06,960
networks and some concepts that go just

00:01:04,559 --> 00:01:08,400
below the surface so we don't just do

00:01:06,960 --> 00:01:16,380
something like word count and say hey

00:01:08,400 --> 00:01:19,549
look your lobes right so when you hear

00:01:16,380 --> 00:01:19,549
about deep learning what do you think

00:01:22,500 --> 00:01:30,090
you know network with multiple hidden

00:01:24,820 --> 00:01:30,090
layers cool where would you use it

00:01:30,360 --> 00:01:36,750
recognition of generalized concepts from

00:01:34,030 --> 00:01:42,250
specific items so this is a picture of

00:01:36,750 --> 00:01:45,579
something that's a very good use case if

00:01:42,250 --> 00:01:50,200
we were to generalize that further the

00:01:45,579 --> 00:01:52,960
idea is that to make deep learning work

00:01:50,200 --> 00:01:58,869
you will need lots of data and you will

00:01:52,960 --> 00:02:00,399
need lots of computational power and if

00:01:58,869 --> 00:02:01,689
you don't have lots of data and you

00:02:00,399 --> 00:02:04,869
don't have a complex problem then deep

00:02:01,689 --> 00:02:07,179
learning is not thing for you if you

00:02:04,869 --> 00:02:08,979
have small amounts of data it is

00:02:07,179 --> 00:02:10,360
possible to get good results not from

00:02:08,979 --> 00:02:18,489
deep learning but from traditional

00:02:10,360 --> 00:02:20,590
statistical approaches because who

00:02:18,489 --> 00:02:22,720
hundreds of examples thousands of

00:02:20,590 --> 00:02:26,489
examples not in the millions of millions

00:02:22,720 --> 00:02:29,139
of stuff yeah

00:02:26,489 --> 00:02:30,459
so I mean there are statistical models I

00:02:29,139 --> 00:02:32,530
can work with hundred data points and

00:02:30,459 --> 00:02:35,680
still give you fairly good good results

00:02:32,530 --> 00:02:37,120
or a few thousand data points if you can

00:02:35,680 --> 00:02:41,410
compute something with traditional

00:02:37,120 --> 00:02:43,180
methods on a single machine do it

00:02:41,410 --> 00:02:46,780
there's no reason to go into the whole

00:02:43,180 --> 00:02:51,180
big data thing unless traditional

00:02:46,780 --> 00:02:51,180
approaches are not working for you okay

00:02:51,540 --> 00:02:59,109
right now one of the areas where or

00:02:56,769 --> 00:03:01,530
rather one category of areas where deep

00:02:59,109 --> 00:03:03,519
learning is proving to do really well is

00:03:01,530 --> 00:03:07,510
solving things that previously only

00:03:03,519 --> 00:03:10,959
humans could do so as I said recognizing

00:03:07,510 --> 00:03:15,130
images we'll look at some more examples

00:03:10,959 --> 00:03:21,099
of that kind of like understanding text

00:03:15,130 --> 00:03:23,709
or being able to create artwork things

00:03:21,099 --> 00:03:25,269
that machine's when we think of what

00:03:23,709 --> 00:03:27,400
machines can do we think that this can't

00:03:25,269 --> 00:03:29,799
happen but these are areas where deep

00:03:27,400 --> 00:03:32,939
learning is actually producing results

00:03:29,799 --> 00:03:32,939
and we'll look at some examples

00:03:34,230 --> 00:03:37,750
the

00:03:35,980 --> 00:03:41,610
except of neural networks has been

00:03:37,750 --> 00:03:44,409
around for a long time decades right

00:03:41,610 --> 00:03:45,849
back then in the 70s 80s there wasn't

00:03:44,409 --> 00:03:50,349
enough computational power to make this

00:03:45,849 --> 00:03:53,440
production great at least not generally

00:03:50,349 --> 00:03:55,750
and the other thing was that each

00:03:53,440 --> 00:03:59,230
different industry who tried to apply it

00:03:55,750 --> 00:04:00,790
use their own framework it was like well

00:03:59,230 --> 00:04:03,160
now we have Microsoft building their one

00:04:00,790 --> 00:04:04,690
Facebook building their one and what not

00:04:03,160 --> 00:04:08,530
there is some level of collaboration

00:04:04,690 --> 00:04:11,890
which is good but previously what would

00:04:08,530 --> 00:04:13,810
happen is that say the electronics

00:04:11,890 --> 00:04:15,250
industry would come up with their own

00:04:13,810 --> 00:04:16,840
thing

00:04:15,250 --> 00:04:18,459
people who focused on image recognition

00:04:16,840 --> 00:04:20,859
they'd come up with their own thing and

00:04:18,459 --> 00:04:22,600
there was no opportunity to share the

00:04:20,859 --> 00:04:24,789
tech across platform so people were like

00:04:22,600 --> 00:04:27,400
rebuilding the same things over and over

00:04:24,789 --> 00:04:29,860
again even with the limited

00:04:27,400 --> 00:04:32,229
computational power they had so it was

00:04:29,860 --> 00:04:34,240
quite impractical some of the changes

00:04:32,229 --> 00:04:39,520
that we're seeing now is frameworks like

00:04:34,240 --> 00:04:41,680
tensorflow pi torch these are well these

00:04:39,520 --> 00:04:45,580
share terminology they share design

00:04:41,680 --> 00:04:47,289
principles and shared learnings plus

00:04:45,580 --> 00:04:50,770
they provide the technology in terms of

00:04:47,289 --> 00:04:53,169
Python libraries and whatnot that apply

00:04:50,770 --> 00:04:56,229
cross discipline so it doesn't matter if

00:04:53,169 --> 00:04:59,620
you are trying to project election

00:04:56,229 --> 00:05:03,099
outcomes or predict stock trades stock

00:04:59,620 --> 00:05:05,889
prices or production in a plant or

00:05:03,099 --> 00:05:07,660
something right the same tools can be

00:05:05,889 --> 00:05:10,030
used in all of these scenarios

00:05:07,660 --> 00:05:11,770
they'll be applied differently but you

00:05:10,030 --> 00:05:13,030
won't have to relearn a framework just

00:05:11,770 --> 00:05:19,200
because you're crossing a domain

00:05:13,030 --> 00:05:19,200
boundary so

00:05:19,830 --> 00:05:25,770
that brings us to neural networks are

00:05:22,800 --> 00:05:29,890
people familiar with something like that

00:05:25,770 --> 00:05:33,040
okay it's a very simplistic thing where

00:05:29,890 --> 00:05:37,420
we have sorry I have a hideous cough bad

00:05:33,040 --> 00:05:40,750
for the video right so the idea is that

00:05:37,420 --> 00:05:42,310
we have noticed I receive input they do

00:05:40,750 --> 00:05:44,530
some calculation look at what

00:05:42,310 --> 00:05:47,350
calculation that is in a moment and they

00:05:44,530 --> 00:05:50,260
emit some output these outputs go to

00:05:47,350 --> 00:05:51,670
subsequent notes they do the same thing

00:05:50,260 --> 00:05:54,700
and there's layers and layers of these

00:05:51,670 --> 00:05:56,860
until finally we get to some sort of

00:05:54,700 --> 00:06:00,810
output okay

00:05:56,860 --> 00:06:05,830
collectively this is called a perceptron

00:06:00,810 --> 00:06:08,050
people often use the terminology of well

00:06:05,830 --> 00:06:10,930
the human brain when thinking about how

00:06:08,050 --> 00:06:15,120
neural networks work that's not

00:06:10,930 --> 00:06:15,120
necessarily the most accurate of things

00:06:16,290 --> 00:06:21,460
neurons in our brain we think work kind

00:06:19,630 --> 00:06:23,170
of similarly isn't they receive input

00:06:21,460 --> 00:06:27,010
they do some stuff and they relay on

00:06:23,170 --> 00:06:29,110
that but but there are many things about

00:06:27,010 --> 00:06:31,240
our brain that we don't know there are

00:06:29,110 --> 00:06:34,420
competing theories there's the notion of

00:06:31,240 --> 00:06:36,100
a grandmother cell with the idea that it

00:06:34,420 --> 00:06:41,620
can behave differently in different

00:06:36,100 --> 00:06:44,430
contexts and based on well what they're

00:06:41,620 --> 00:06:46,570
connected to they can they can actually

00:06:44,430 --> 00:06:48,820
exhibit different behavior rather than

00:06:46,570 --> 00:06:56,200
work the same way every single time like

00:06:48,820 --> 00:06:58,420
perceptrons do so so the notion of the

00:06:56,200 --> 00:07:01,960
artificial neural network or neural

00:06:58,420 --> 00:07:03,340
networks for deep learning instead of

00:07:01,960 --> 00:07:05,080
the biological model is better to think

00:07:03,340 --> 00:07:06,640
of it as a mathematical model and the

00:07:05,080 --> 00:07:10,300
mathematical model is extremely simple

00:07:06,640 --> 00:07:14,170
okay what we have here the cursor shows

00:07:10,300 --> 00:07:17,740
up on the screen as well so we have an

00:07:14,170 --> 00:07:20,350
input we have a weight and we are doing

00:07:17,740 --> 00:07:22,300
a weighted sum so you can see we

00:07:20,350 --> 00:07:25,540
multiply the weight by the input for

00:07:22,300 --> 00:07:27,910
each of the inputs we sum them together

00:07:25,540 --> 00:07:32,780
and we call a function and that function

00:07:27,910 --> 00:07:34,910
can vary for different results

00:07:32,780 --> 00:07:36,530
but that's kind of the model of a neural

00:07:34,910 --> 00:07:38,420
network oh sorry that's kind of the

00:07:36,530 --> 00:07:43,600
model for a perceptron in a neural

00:07:38,420 --> 00:07:43,600
network okay questions around it

00:07:51,590 --> 00:07:56,479
there are exceptions we'll look into

00:07:52,880 --> 00:07:58,729
later yeah when you use the mathematical

00:07:56,479 --> 00:08:01,729
terms there - a little rusty that's a

00:07:58,729 --> 00:08:05,210
sum of things is that a literal addition

00:08:01,729 --> 00:08:08,800
or is that some way of combining that's

00:08:05,210 --> 00:08:11,479
very formal addition okay

00:08:08,800 --> 00:08:14,650
the thing with neural networks is that

00:08:11,479 --> 00:08:20,030
they do extremely simple computations

00:08:14,650 --> 00:08:23,180
many many times many many many many

00:08:20,030 --> 00:08:25,250
parallels and they figure out the

00:08:23,180 --> 00:08:27,500
weights what we try to do is figure out

00:08:25,250 --> 00:08:29,479
the best way to apply at each layer and

00:08:27,500 --> 00:08:32,360
that's that's where the training comes

00:08:29,479 --> 00:08:34,430
in okay you talked earlier about how you

00:08:32,360 --> 00:08:37,669
want a large amount of computational

00:08:34,430 --> 00:08:40,159
power yeah is that for the training of

00:08:37,669 --> 00:08:43,750
the neuron net or for the use of it or

00:08:40,159 --> 00:08:47,120
bugs the train train this is definitely

00:08:43,750 --> 00:08:49,459
in some sense I might serialize my

00:08:47,120 --> 00:08:51,260
resulting net and deploy it onto a much

00:08:49,459 --> 00:08:52,610
lesser piece of hardware and use it

00:08:51,260 --> 00:08:55,459
there yeah with some success

00:08:52,610 --> 00:08:57,290
yeah that's common across most machine

00:08:55,459 --> 00:09:03,020
learning things anyway the training is

00:08:57,290 --> 00:09:04,490
what takes we'll see we do afford

00:09:03,020 --> 00:09:07,670
propagation and then we go back to

00:09:04,490 --> 00:09:10,640
record rotation yeah actually we might

00:09:07,670 --> 00:09:12,230
not see if we haven't seen by the end of

00:09:10,640 --> 00:09:13,970
the talk then we'll have time to discuss

00:09:12,230 --> 00:09:15,980
them and we'll go into more detail there

00:09:13,970 --> 00:09:19,399
but this this has literally a weighted

00:09:15,980 --> 00:09:22,760
sum okay so input x1 is multiplied by

00:09:19,399 --> 00:09:24,740
weight w1 input X 2 is multiplied by

00:09:22,760 --> 00:09:26,870
weight W 2 input X 3 is multiplied by

00:09:24,740 --> 00:09:29,959
weight W 3 these are then sum together

00:09:26,870 --> 00:09:32,390
and a function is fault and that gives

00:09:29,959 --> 00:09:35,680
us our output Y it's as simple as that

00:09:32,390 --> 00:09:38,810
there's no more complications to us okay

00:09:35,680 --> 00:09:39,260
right so first of all just a quick

00:09:38,810 --> 00:09:44,770
question

00:09:39,260 --> 00:09:44,770
anybody have any idea what this is okay

00:09:50,880 --> 00:09:55,180
yes so although it's called regression

00:09:53,560 --> 00:09:57,670
logistic regression is actually a

00:09:55,180 --> 00:10:00,090
classification method so you have two or

00:09:57,670 --> 00:10:03,790
more actually you can have multiple

00:10:00,090 --> 00:10:05,380
potential classes like say you're trying

00:10:03,790 --> 00:10:08,860
to either identify digits from

00:10:05,380 --> 00:10:10,600
handwriting so you have ten classes and

00:10:08,860 --> 00:10:13,330
you're trying to predict what the

00:10:10,600 --> 00:10:15,760
probability is that a certain number or

00:10:13,330 --> 00:10:21,520
digit is one two three four whatever

00:10:15,760 --> 00:10:26,730
okay so the model for logistic

00:10:21,520 --> 00:10:30,880
regression is quite simple we have a

00:10:26,730 --> 00:10:34,780
weight matrix we have an input matrix we

00:10:30,880 --> 00:10:36,370
add a constant and we get some output if

00:10:34,780 --> 00:10:38,140
there are curves your inputs you can

00:10:36,370 --> 00:10:41,200
take squares of inputs and whatnot but

00:10:38,140 --> 00:10:49,030
at its basics it's a linear equation

00:10:41,200 --> 00:10:52,710
that's why so in terms of here what we

00:10:49,030 --> 00:10:55,270
get is a weighted sum plus a constant

00:10:52,710 --> 00:10:57,880
we'll ignore the constant for now

00:10:55,270 --> 00:11:00,850
because you can Center your input so

00:10:57,880 --> 00:11:04,260
that that becomes zero but that gives

00:11:00,850 --> 00:11:08,110
you an output Y right

00:11:04,260 --> 00:11:10,060
you take that output Y and you run it

00:11:08,110 --> 00:11:15,550
through this function this function is

00:11:10,060 --> 00:11:19,420
called softmax okay why it's called

00:11:15,550 --> 00:11:22,330
softmax less important but if actually

00:11:19,420 --> 00:11:27,880
what it does is instead of its having a

00:11:22,330 --> 00:11:30,100
zero one it has a slope and the benefit

00:11:27,880 --> 00:11:32,350
of something having a slope is that you

00:11:30,100 --> 00:11:34,480
can then take a derivative if you can

00:11:32,350 --> 00:11:36,880
take a derivative then you can run

00:11:34,480 --> 00:11:39,400
optimization on it like gradient descent

00:11:36,880 --> 00:11:41,280
and whatnot okay so whenever we see

00:11:39,400 --> 00:11:44,830
something that's like straight lines

00:11:41,280 --> 00:11:47,170
it's and we're trying to use it in a in

00:11:44,830 --> 00:11:49,840
a learning construct then it's better to

00:11:47,170 --> 00:11:51,340
have instead of a kind of sudden rise or

00:11:49,840 --> 00:11:54,640
sudden drop score function it's better

00:11:51,340 --> 00:11:59,500
to have a kind of gradual growth and

00:11:54,640 --> 00:12:01,180
what this does is it takes our output

00:11:59,500 --> 00:12:04,360
from this equation

00:12:01,180 --> 00:12:09,160
and it gives us kind of like in a step

00:12:04,360 --> 00:12:11,170
function with a fervent the output of

00:12:09,160 --> 00:12:15,850
this is that we will have probabilities

00:12:11,170 --> 00:12:19,420
so over here we had schools let's say WX

00:12:15,850 --> 00:12:22,540
plus B if it were to one point zero zero

00:12:19,420 --> 00:12:25,660
point one these would be scores or

00:12:22,540 --> 00:12:27,280
otherwise known as low jets if you take

00:12:25,660 --> 00:12:30,600
the legit and you call this softmax

00:12:27,280 --> 00:12:33,870
function then what you get is

00:12:30,600 --> 00:12:36,730
probabilities and the nice things about

00:12:33,870 --> 00:12:38,590
probabilities is that if you have 0.7

00:12:36,730 --> 00:12:40,660
point two and point one then you can say

00:12:38,590 --> 00:12:44,110
this is this this particular input

00:12:40,660 --> 00:12:46,360
belongs to the first loss the second one

00:12:44,110 --> 00:12:48,610
or the third one even gives you a degree

00:12:46,360 --> 00:12:50,560
sorry I'm losing the connection between

00:12:48,610 --> 00:12:52,480
the previous light in this one okay

00:12:50,560 --> 00:12:55,960
previous slide you seem to have one

00:12:52,480 --> 00:12:59,560
output but they seem very universe ah

00:12:55,960 --> 00:13:00,790
so remember here this is a matrix right

00:12:59,560 --> 00:13:02,650
so again you've got like a capital A

00:13:00,790 --> 00:13:04,570
there yeah and you're reducing that to

00:13:02,650 --> 00:13:06,010
some set of numbers or a matrix of

00:13:04,570 --> 00:13:10,570
numbers which has been going through

00:13:06,010 --> 00:13:13,330
function producing why yeah why itself

00:13:10,570 --> 00:13:21,490
will have three right so this will be a

00:13:13,330 --> 00:13:24,100
score for everybody yeah so the reason

00:13:21,490 --> 00:13:26,620
for this is because you have X's and

00:13:24,100 --> 00:13:28,330
inputs and you have three weights right

00:13:26,620 --> 00:13:30,340
the first weight second weight in the

00:13:28,330 --> 00:13:32,440
third weight so the weights will

00:13:30,340 --> 00:13:35,080
multiply with the inputs giving you a

00:13:32,440 --> 00:13:36,670
higher value for the first one a lower

00:13:35,080 --> 00:13:39,550
value for the middle one and a tiny

00:13:36,670 --> 00:13:42,520
value for the last one now two one and

00:13:39,550 --> 00:13:44,590
point one are also kind of if he is and

00:13:42,520 --> 00:13:45,820
like okay this is 2 this is 1 so it

00:13:44,590 --> 00:13:49,390
could be either the first one or the

00:13:45,820 --> 00:13:53,440
second one right I mean okay not really

00:13:49,390 --> 00:13:55,720
but if you look at those values just

00:13:53,440 --> 00:13:58,360
numerically because in terms of scale

00:13:55,720 --> 00:14:00,460
this is saying to one and zero almost

00:13:58,360 --> 00:14:04,120
okay but then when you apply softmax to

00:14:00,460 --> 00:14:06,550
it the probabilities kind of are drawn

00:14:04,120 --> 00:14:09,460
out much better so here this 2 becomes

00:14:06,550 --> 00:14:13,910
0.7 which is 70% probability which is

00:14:09,460 --> 00:14:17,550
much more differentiated than say 2 & 1

00:14:13,910 --> 00:14:21,270
so if we go back to our first function

00:14:17,550 --> 00:14:25,500
over here that was our weighted sum that

00:14:21,270 --> 00:14:28,800
gave us our Y and this F was our softmax

00:14:25,500 --> 00:14:31,740
and the resultant thing we got was the

00:14:28,800 --> 00:14:35,910
final output which is a vector of

00:14:31,740 --> 00:14:37,860
probabilities okay it's not really yet a

00:14:35,910 --> 00:14:41,250
vector of probabilities because what we

00:14:37,860 --> 00:14:44,610
then do is call a function on it's

00:14:41,250 --> 00:14:50,520
called a categorical cross entropy okay

00:14:44,610 --> 00:14:51,450
and what this does is basically I won't

00:14:50,520 --> 00:14:53,070
go into the mass for this but

00:14:51,450 --> 00:14:54,270
effectively what it does is whatever has

00:14:53,070 --> 00:14:57,480
the highest probability gets the 1 and

00:14:54,270 --> 00:14:58,950
other ones get 0 very simplistic but I

00:14:57,480 --> 00:15:00,420
just call that's what it's doing okay

00:14:58,950 --> 00:15:01,800
so instead of having point seven point

00:15:00,420 --> 00:15:03,870
two and point one to deal with you

00:15:01,800 --> 00:15:10,980
basically get that this is an a this is

00:15:03,870 --> 00:15:15,090
not a B and there's no C right

00:15:10,980 --> 00:15:17,160
so the benefits of a linear what we

00:15:15,090 --> 00:15:19,590
looked at was a linear model right the

00:15:17,160 --> 00:15:21,030
benefits of a linear model is that GPUs

00:15:19,590 --> 00:15:21,630
can calculate these things extremely

00:15:21,030 --> 00:15:24,120
fast

00:15:21,630 --> 00:15:26,370
the reason GPUs exist is because they

00:15:24,120 --> 00:15:30,030
can do these calculations very fast okay

00:15:26,370 --> 00:15:34,160
they multiply matrices your inputs you

00:15:30,030 --> 00:15:37,530
all GP have you guys worked with shaders

00:15:34,160 --> 00:15:39,630
you have cool that's back in the day my

00:15:37,530 --> 00:15:43,800
university project wasn't shaders it's

00:15:39,630 --> 00:15:45,150
like man that was cool stuff but yeah so

00:15:43,800 --> 00:15:46,650
the instructions you sent your GPU are

00:15:45,150 --> 00:15:48,810
almost always entirely in terms of

00:15:46,650 --> 00:15:51,120
matrices and vectors okay and what it

00:15:48,810 --> 00:15:53,570
does very well very fast is multiplied

00:15:51,120 --> 00:15:57,630
them together and give you some Outlook

00:15:53,570 --> 00:16:01,260
GPUs tend to have a lot of low power

00:15:57,630 --> 00:16:03,600
pours and this is what they excel in

00:16:01,260 --> 00:16:07,770
this is dedicated hardware just to do

00:16:03,600 --> 00:16:11,370
this okay and as such GPUs are very very

00:16:07,770 --> 00:16:15,000
good at calculating the result of a

00:16:11,370 --> 00:16:17,210
linear model okay it's also stable as

00:16:15,000 --> 00:16:19,920
then it doesn't go radically you know

00:16:17,210 --> 00:16:22,700
it's like this and then suddenly goes

00:16:19,920 --> 00:16:27,070
through you have a kind of range bound

00:16:22,700 --> 00:16:28,900
level in which we're dealing with

00:16:27,070 --> 00:16:31,240
and the other thing is if you have a

00:16:28,900 --> 00:16:34,780
linear model then the derivative of that

00:16:31,240 --> 00:16:37,150
is going to be a constant okay so y

00:16:34,780 --> 00:16:41,610
equals MX plus B the derivative is going

00:16:37,150 --> 00:16:41,610
to be web axle okay

00:16:42,630 --> 00:16:49,420
the cons of a linear model is basically

00:16:46,780 --> 00:16:53,560
the number of parameters you have okay

00:16:49,420 --> 00:16:55,930
having lots of parameters has a number

00:16:53,560 --> 00:16:59,680
of problems that we'll get into a bit

00:16:55,930 --> 00:17:01,330
later but at its basic it's the more

00:16:59,680 --> 00:17:03,880
parameters you have in your model the

00:17:01,330 --> 00:17:05,620
more things you need to train the notion

00:17:03,880 --> 00:17:08,320
of training a machine learning model is

00:17:05,620 --> 00:17:10,180
basically taking those things like

00:17:08,320 --> 00:17:16,959
weights and finding the optimal value

00:17:10,180 --> 00:17:22,030
and in our case you have n inputs K

00:17:16,959 --> 00:17:23,350
outputs this is something you're going

00:17:22,030 --> 00:17:24,880
to have to Train this is something

00:17:23,350 --> 00:17:28,690
you're going to have to train and the

00:17:24,880 --> 00:17:31,690
model now has n plus 1 times K

00:17:28,690 --> 00:17:37,150
parameters so this is one downside of

00:17:31,690 --> 00:17:39,790
having that's the other problem is that

00:17:37,150 --> 00:17:41,200
it's linear so it's a relationship

00:17:39,790 --> 00:17:44,620
between your inputs and outputs are

00:17:41,200 --> 00:17:46,660
linear then great it works if there's

00:17:44,620 --> 00:17:49,480
like curves and volts like most real

00:17:46,660 --> 00:17:50,080
physical data then linear models don't

00:17:49,480 --> 00:17:52,810
work very well

00:17:50,080 --> 00:17:54,850
now if you apply this function the

00:17:52,810 --> 00:17:57,820
result is also going to be effectively

00:17:54,850 --> 00:18:00,220
linear as in when we say linear we don't

00:17:57,820 --> 00:18:02,770
necessarily mean it's going to be

00:18:00,220 --> 00:18:04,750
exactly a straight line we mean that in

00:18:02,770 --> 00:18:06,490
terms of the input and the output the

00:18:04,750 --> 00:18:08,650
degree of the input so we're not taking

00:18:06,490 --> 00:18:10,870
squares of the input we're not thinking

00:18:08,650 --> 00:18:12,700
about relationships between the inputs

00:18:10,870 --> 00:18:15,760
so for example if you have two

00:18:12,700 --> 00:18:18,010
dimensions like say I don't know a

00:18:15,760 --> 00:18:19,150
person's age in a person's salary you're

00:18:18,010 --> 00:18:20,440
not considering relationship between

00:18:19,150 --> 00:18:24,820
them you're treating them as independent

00:18:20,440 --> 00:18:31,060
inputs that's what a linear model is

00:18:24,820 --> 00:18:33,370
about I guess and this is basically what

00:18:31,060 --> 00:18:35,920
they're saying is that if you have x1

00:18:33,370 --> 00:18:38,920
and x2 is two separate things and you

00:18:35,920 --> 00:18:40,950
can have many many more of these in your

00:18:38,920 --> 00:18:42,870
model that's still a linear model

00:18:40,950 --> 00:18:45,630
but if you have any interaction terms

00:18:42,870 --> 00:18:47,130
where inputs tend to be the

00:18:45,630 --> 00:18:49,410
multiplication of the two for example

00:18:47,130 --> 00:18:56,160
then those sorts of interactions can't

00:18:49,410 --> 00:18:58,080
be captured in a linear right till now

00:18:56,160 --> 00:19:01,620
what we said there was very little deep

00:18:58,080 --> 00:19:04,470
learning this is all basic stats yeah so

00:19:01,620 --> 00:19:07,890
one of the inventions

00:19:04,470 --> 00:19:11,280
I guess not really inventions but one of

00:19:07,890 --> 00:19:14,850
the things that really made deep

00:19:11,280 --> 00:19:16,260
learning take off because it started

00:19:14,850 --> 00:19:18,810
doing really well for nonlinear

00:19:16,260 --> 00:19:21,360
functions okay now if your non-linear

00:19:18,810 --> 00:19:24,030
function is very curvy that's still very

00:19:21,360 --> 00:19:28,760
difficult to optimize okay but this

00:19:24,030 --> 00:19:28,760
little thing will the reticle it's unit

00:19:29,870 --> 00:19:35,820
it turns out it performs surprisingly

00:19:32,850 --> 00:19:37,730
well when you have lots of data okay all

00:19:35,820 --> 00:19:40,590
it is is basically it's a function

00:19:37,730 --> 00:19:43,080
that's up until a certain value it gives

00:19:40,590 --> 00:19:48,360
an output of zero and then it becomes a

00:19:43,080 --> 00:19:51,030
linear okay and it turns out that a lot

00:19:48,360 --> 00:19:53,870
of non-linearity can be captured by

00:19:51,030 --> 00:19:56,100
repeatedly applying the same thing and

00:19:53,870 --> 00:20:00,110
when you're training neural networks you

00:19:56,100 --> 00:20:03,060
are repeatedly playing the same thing so

00:20:00,110 --> 00:20:05,130
it was more of kind of like hey let's do

00:20:03,060 --> 00:20:09,180
this it sure maybe it'll give us

00:20:05,130 --> 00:20:12,750
something and they did and it did and

00:20:09,180 --> 00:20:13,530
values are one of when you deal when you

00:20:12,750 --> 00:20:15,570
want to introduce some sort of

00:20:13,530 --> 00:20:18,480
non-linearity in your module this is

00:20:15,570 --> 00:20:21,000
this is one of the things that work ok

00:20:18,480 --> 00:20:23,730
very nicely and because they're so

00:20:21,000 --> 00:20:25,500
simple this is still very fast on GPUs

00:20:23,730 --> 00:20:27,630
you're not calculating squares you're

00:20:25,500 --> 00:20:30,600
not multiplying stuff together it's

00:20:27,630 --> 00:20:33,180
still basically do a check is the value

00:20:30,600 --> 00:20:36,240
greater than play in this case zero yes

00:20:33,180 --> 00:20:47,070
then calculate the linear value

00:20:36,240 --> 00:20:49,860
otherwise just take zero okay and the

00:20:47,070 --> 00:20:53,520
deep part of deep neural networks comes

00:20:49,860 --> 00:20:54,870
from basically creating these in layers

00:20:53,520 --> 00:20:56,760
many many layers

00:20:54,870 --> 00:21:00,390
after the other and more more MORE so

00:20:56,760 --> 00:21:02,400
for example we have a linear model here

00:21:00,390 --> 00:21:04,470
where you have a linear layer we're

00:21:02,400 --> 00:21:06,630
doing a weighted sum but then we're

00:21:04,470 --> 00:21:07,980
adding a bit of London area C and then

00:21:06,630 --> 00:21:09,809
you can do another one and then you can

00:21:07,980 --> 00:21:11,670
do another one and you can do another

00:21:09,809 --> 00:21:14,880
one and you can make it a lot more

00:21:11,670 --> 00:21:23,990
complex then simply say you know

00:21:14,880 --> 00:21:27,230
multiply by W and that'd be it's adding

00:21:23,990 --> 00:21:29,910
non-linearity to your model regardless

00:21:27,230 --> 00:21:32,760
yes so one of the things that new and

00:21:29,910 --> 00:21:35,670
that words do and this is actually kind

00:21:32,760 --> 00:21:37,110
of might sound surprising because in

00:21:35,670 --> 00:21:38,340
traditional statistics one of the key

00:21:37,110 --> 00:21:40,410
things that you do is basically feature

00:21:38,340 --> 00:21:41,880
engineering you spend a lot of time

00:21:40,410 --> 00:21:44,250
trying to figure out what your features

00:21:41,880 --> 00:21:47,520
are you then multiply with things with

00:21:44,250 --> 00:21:49,800
more weird things to trying to like you

00:21:47,520 --> 00:21:51,780
multiply age with salary and and then

00:21:49,800 --> 00:21:53,730
try to try to see if you can generate a

00:21:51,780 --> 00:21:56,730
feature from that and see if that's

00:21:53,730 --> 00:22:00,150
related to some sort of output and that

00:21:56,730 --> 00:22:01,920
takes a lot of time right some deep

00:22:00,150 --> 00:22:03,870
learning in seriously literally use the

00:22:01,920 --> 00:22:06,030
tagline that architecture engineering is

00:22:03,870 --> 00:22:08,309
the new feature engineering the notion

00:22:06,030 --> 00:22:10,830
of architecture is this not your

00:22:08,309 --> 00:22:15,900
software architecture okay the idea is

00:22:10,830 --> 00:22:20,580
that given enough data your neural

00:22:15,900 --> 00:22:22,350
network can learn the features you've

00:22:20,580 --> 00:22:26,179
got your inputs you've got your outputs

00:22:22,350 --> 00:22:30,260
if you build a relatively good model and

00:22:26,179 --> 00:22:32,820
then you can watch the experiment there

00:22:30,260 --> 00:22:36,080
and you train and what not but what

00:22:32,820 --> 00:22:39,059
happens is that the network itself

00:22:36,080 --> 00:22:41,370
learns and that's the deep learning part

00:22:39,059 --> 00:22:43,610
it learns what things to emphasize on

00:22:41,370 --> 00:22:48,960
and what things it needs to ignore and

00:22:43,610 --> 00:22:51,210
you as a you know session or data

00:22:48,960 --> 00:22:53,160
engineer or day scientist you don't have

00:22:51,210 --> 00:22:55,740
to figure out those things to a certain

00:22:53,160 --> 00:22:58,500
extent it can still help okay I'm not

00:22:55,740 --> 00:23:02,460
giving it tons and tons of relevant

00:22:58,500 --> 00:23:04,600
stuff but then it fairly quickly learns

00:23:02,460 --> 00:23:08,560
how to ignore things that don't matter

00:23:04,600 --> 00:23:10,450
so if we give it the appropriate kind of

00:23:08,560 --> 00:23:13,090
neural net with the deep learning by

00:23:10,450 --> 00:23:15,310
having a huge amount of list of people

00:23:13,090 --> 00:23:18,160
with ten different attributes amongst

00:23:15,310 --> 00:23:19,780
them one of which is a goal of maximum

00:23:18,160 --> 00:23:22,840
salary that they happen to be getting

00:23:19,780 --> 00:23:26,470
paid and we train it through on this it

00:23:22,840 --> 00:23:29,140
will determine which of those attributes

00:23:26,470 --> 00:23:31,240
is it's all relevant yes and discard

00:23:29,140 --> 00:23:32,890
some of them yes but you won't notice

00:23:31,240 --> 00:23:35,290
you won't know it's actually done yes so

00:23:32,890 --> 00:23:37,180
that's one of the downsides of neural

00:23:35,290 --> 00:23:42,090
networks is that they work very much

00:23:37,180 --> 00:23:44,530
like black boxes that there are some

00:23:42,090 --> 00:23:46,020
well even ongoing research into trying

00:23:44,530 --> 00:23:48,730
to make them more understandable and

00:23:46,020 --> 00:23:53,470
give you more feedback as to how they're

00:23:48,730 --> 00:23:55,390
working but yes in that case I've done

00:23:53,470 --> 00:23:59,350
that if I fed a huge amount of data

00:23:55,390 --> 00:24:00,970
through and you know that has determined

00:23:59,350 --> 00:24:03,340
for itself that these particular factors

00:24:00,970 --> 00:24:07,410
just not relevant to the goal or

00:24:03,340 --> 00:24:10,000
maximize this yeah how can we use that

00:24:07,410 --> 00:24:14,920
you have there trained up model as your

00:24:10,000 --> 00:24:16,570
output of a new person yeah with fresh

00:24:14,920 --> 00:24:20,920
data that it's never seen before

00:24:16,570 --> 00:24:23,860
yeah and I'm gonna ask you to predict

00:24:20,920 --> 00:24:25,630
the salary in this case yeah without

00:24:23,860 --> 00:24:27,820
that piece of information present it

00:24:25,630 --> 00:24:29,650
will give me hopefully a fairly accurate

00:24:27,820 --> 00:24:32,290
answer based on its learning that it's

00:24:29,650 --> 00:24:36,450
done so far it will do so using the

00:24:32,290 --> 00:24:36,450
historical learning that it's done yes

00:24:37,989 --> 00:24:44,419
okay so when we say learning what we're

00:24:41,809 --> 00:24:46,820
effectively saying is that where the

00:24:44,419 --> 00:24:50,329
network will teach itself the optimal

00:24:46,820 --> 00:24:52,309
value of say W and B and it will have

00:24:50,329 --> 00:24:54,769
these constants so when you're trying to

00:24:52,309 --> 00:24:56,719
predict what the new input is just doing

00:24:54,769 --> 00:24:59,799
a forward run so does it actually become

00:24:56,719 --> 00:25:02,329
more efficient so on the first few

00:24:59,799 --> 00:25:03,769
thousand tries or whatever it may well

00:25:02,329 --> 00:25:06,200
have been analyzing everything that was

00:25:03,769 --> 00:25:08,329
going in yeah but when it comes to

00:25:06,200 --> 00:25:10,700
deploying the the fully trained network

00:25:08,329 --> 00:25:11,869
someplace it will not just be more

00:25:10,700 --> 00:25:13,579
efficient because it's done learning it

00:25:11,869 --> 00:25:14,709
will do so because it will bypass the

00:25:13,579 --> 00:25:21,259
part of it the analysis that

00:25:14,709 --> 00:25:23,839
over-involved kind of but not not really

00:25:21,259 --> 00:25:26,570
cuz in terms of predicting what it

00:25:23,839 --> 00:25:27,859
basically does is it will literally run

00:25:26,570 --> 00:25:29,329
the same computation it's not

00:25:27,859 --> 00:25:31,999
intelligent or clever in that respect

00:25:29,329 --> 00:25:34,429
right it will basically take the weights

00:25:31,999 --> 00:25:38,359
it's learned multiply the inputs right

00:25:34,429 --> 00:25:40,399
if it has deemed that a feature is

00:25:38,359 --> 00:25:48,889
unnecessary the weight for that input is

00:25:40,399 --> 00:25:50,479
going to be zero or very low okay this

00:25:48,889 --> 00:25:52,909
is abstract o'clock that certainly

00:25:50,479 --> 00:25:55,249
functional languages can not bother to

00:25:52,909 --> 00:25:56,450
multiply by zero because they look at it

00:25:55,249 --> 00:25:58,549
and go well I know what that's going to

00:25:56,450 --> 00:25:59,239
be I won't even bother following that

00:25:58,549 --> 00:26:01,759
branch

00:25:59,239 --> 00:26:04,309
so actually we combine the learned

00:26:01,759 --> 00:26:06,739
knowledge with some of the optimizations

00:26:04,309 --> 00:26:08,929
that things do potentially you do

00:26:06,739 --> 00:26:10,559
actually sort of seal off branches of

00:26:08,929 --> 00:26:14,679
computation

00:26:10,559 --> 00:26:17,890
you could but considering that a lot of

00:26:14,679 --> 00:26:19,900
this happens just in parallel anyway so

00:26:17,890 --> 00:26:22,390
doing it for one feature versus ten

00:26:19,900 --> 00:26:23,410
features it's not more work because

00:26:22,390 --> 00:26:25,570
everything's happening on the GP on

00:26:23,410 --> 00:26:27,760
probably it is more work it's a wasted

00:26:25,570 --> 00:26:29,679
work but it's it's not I mean you

00:26:27,760 --> 00:26:37,320
wouldn't be saving too much in terms of

00:26:29,679 --> 00:26:40,299
computation but who knows okay right so

00:26:37,320 --> 00:26:43,840
the values here what it would do is for

00:26:40,299 --> 00:26:46,330
example non-linearity as in if there's

00:26:43,840 --> 00:26:49,150
no linear relationship between input one

00:26:46,330 --> 00:26:50,860
less input and output or even there's

00:26:49,150 --> 00:26:53,710
interaction terms between two different

00:26:50,860 --> 00:26:57,610
features then that's what values will

00:26:53,710 --> 00:26:58,960
capture right when you will look at some

00:26:57,610 --> 00:27:01,510
tensor flow code when we look at it

00:26:58,960 --> 00:27:03,280
you'll see that you're not figuring out

00:27:01,510 --> 00:27:06,600
much in terms of inserting a rally you

00:27:03,280 --> 00:27:09,250
just say I want to renew here okay and

00:27:06,600 --> 00:27:11,530
it puts a value it's not your job to

00:27:09,250 --> 00:27:16,500
Train it because the network itself will

00:27:11,530 --> 00:27:21,460
train so when when it goes drink right

00:27:16,500 --> 00:27:23,200
so tensor flow is well Google's deep

00:27:21,460 --> 00:27:25,240
learning toolkits and whatnot is very

00:27:23,200 --> 00:27:33,220
good it's also hideously painful to code

00:27:25,240 --> 00:27:36,909
for yeah there are frameworks on top of

00:27:33,220 --> 00:27:39,600
tensor flow TF learn being one they make

00:27:36,909 --> 00:27:42,159
programming with tensor flow easier and

00:27:39,600 --> 00:27:44,350
cooler it's not that there are any less

00:27:42,159 --> 00:27:47,190
efficient than tensor flow they're just

00:27:44,350 --> 00:27:49,419
think of them as library functions or

00:27:47,190 --> 00:27:50,890
working with sensor for as opposed to

00:27:49,419 --> 00:27:52,929
you know you could do the same things

00:27:50,890 --> 00:27:54,669
low-level by yourself you wanted to or

00:27:52,929 --> 00:28:04,570
you could use yarn and make your job

00:27:54,669 --> 00:28:07,110
easier okay so this is kind of how we

00:28:04,570 --> 00:28:10,799
load data in tensor flow with Python I

00:28:07,110 --> 00:28:13,030
mean most people can read Python write

00:28:10,799 --> 00:28:16,870
executable pseudo good no no no okay so

00:28:13,030 --> 00:28:19,630
M nest is a data set that comes as part

00:28:16,870 --> 00:28:22,270
of a lot of machine learning framework

00:28:19,630 --> 00:28:25,020
things it's in our it's in

00:28:22,270 --> 00:28:29,350
tensorflow as well so emesis basically a

00:28:25,020 --> 00:28:31,450
hand-written digit library so basically

00:28:29,350 --> 00:28:33,790
you have tons of digits that have been

00:28:31,450 --> 00:28:35,110
written by hand and they're labeled as

00:28:33,790 --> 00:28:38,440
you know one two three four five six

00:28:35,110 --> 00:28:41,230
seven eight nine and zero the goal of

00:28:38,440 --> 00:28:43,960
your project when working with n list is

00:28:41,230 --> 00:28:48,250
to accurately and correctly predict what

00:28:43,960 --> 00:28:51,250
a digit is okay here what we're

00:28:48,250 --> 00:28:58,210
basically doing is we're loading the

00:28:51,250 --> 00:29:01,030
sample data set into training of inputs

00:28:58,210 --> 00:29:04,720
training outputs test inputs and test

00:29:01,030 --> 00:29:06,760
outputs so we're separating our data

00:29:04,720 --> 00:29:08,230
that's been given to training set we'll

00:29:06,760 --> 00:29:12,640
train our model on our training set and

00:29:08,230 --> 00:29:15,630
then we will test it on a test set not

00:29:12,640 --> 00:29:15,630
doing validation okay

00:29:16,090 --> 00:29:21,370
this light is slightly less interesting

00:29:18,760 --> 00:29:24,820
all it's basically doing is taking the

00:29:21,370 --> 00:29:26,830
data that's there and printing it into

00:29:24,820 --> 00:29:28,960
an image so this is what our input data

00:29:26,830 --> 00:29:30,610
actually looks like okay the function is

00:29:28,960 --> 00:29:33,540
not important what our input data looks

00:29:30,610 --> 00:29:37,870
like it's a two-dimensional array where

00:29:33,540 --> 00:29:40,300
certain bits are lit up and other bits

00:29:37,870 --> 00:29:42,100
are not lit up okay it's black and white

00:29:40,300 --> 00:29:44,950
that's what our input looks like this is

00:29:42,100 --> 00:29:49,330
what's given into the array to our input

00:29:44,950 --> 00:29:56,350
and from here we are supposed to predict

00:29:49,330 --> 00:30:01,930
that that number is seven okay so in TF

00:29:56,350 --> 00:30:03,400
learn this is actually a model so ignore

00:30:01,930 --> 00:30:05,890
that that's a framework thing but you

00:30:03,400 --> 00:30:07,360
always do that as a first line it makes

00:30:05,890 --> 00:30:12,310
it clears out any variables from your

00:30:07,360 --> 00:30:14,740
previous room kind of next thing we're

00:30:12,310 --> 00:30:17,950
saying is that we are creating a neural

00:30:14,740 --> 00:30:22,360
network with one layer which is our

00:30:17,950 --> 00:30:26,440
input data okay does everybody

00:30:22,360 --> 00:30:28,419
understand that bit okay

00:30:26,440 --> 00:30:29,740
then what we're doing and look at the

00:30:28,419 --> 00:30:33,159
lovely mutation any functional

00:30:29,740 --> 00:30:34,269
programmers here I'm actually a

00:30:33,159 --> 00:30:37,629
functional programmer as a functional

00:30:34,269 --> 00:30:39,820
but yeah we're mutating stuff you deal

00:30:37,629 --> 00:30:40,990
with it we're not really mutating we're

00:30:39,820 --> 00:30:45,000
just reassigning this could have been

00:30:40,990 --> 00:30:47,110
called Matt to net three or whatever so

00:30:45,000 --> 00:30:49,450
the first thing we're doing is we're

00:30:47,110 --> 00:30:51,100
taking the input data and we're creating

00:30:49,450 --> 00:30:52,620
a one layer neural network that's our

00:30:51,100 --> 00:30:56,830
first lab that's our input layer okay

00:30:52,620 --> 00:30:59,799
then we are adding a hidden layer by

00:30:56,830 --> 00:31:02,519
saying we want to fully connected layer

00:30:59,799 --> 00:31:06,070
the input to that is the previous one

00:31:02,519 --> 00:31:10,330
the activation function that we have is

00:31:06,070 --> 00:31:14,279
a value so the F that we're doing

00:31:10,330 --> 00:31:14,279
instead of softmax it's gonna be earlier

00:31:14,340 --> 00:31:21,970
and we want it to have 128 our input has

00:31:19,299 --> 00:31:25,659
64 or the output of the first layer is

00:31:21,970 --> 00:31:28,840
going to have 128 so we're increasing in

00:31:25,659 --> 00:31:30,129
that case we could have made that 32 we

00:31:28,840 --> 00:31:31,360
could have made that 512 it's entirely

00:31:30,129 --> 00:31:33,190
up to us what we do okay

00:31:31,360 --> 00:31:34,539
that's our definition of the

00:31:33,190 --> 00:31:36,789
architecture we're saying the first

00:31:34,539 --> 00:31:40,690
layer is 64 how many other is the next

00:31:36,789 --> 00:31:43,629
next leg in half we take that and we add

00:31:40,690 --> 00:31:46,330
another layer and let's say we have that

00:31:43,629 --> 00:31:51,220
one is going to have 32 and then for the

00:31:46,330 --> 00:31:54,759
final one we're gonna have 10 as the

00:31:51,220 --> 00:31:57,100
output because 10 digits right and for

00:31:54,759 --> 00:31:59,379
the final one we can do different types

00:31:57,100 --> 00:32:00,399
of activation functions internally but

00:31:59,379 --> 00:32:01,090
for the final one we're going to use

00:32:00,399 --> 00:32:03,070
sample softmax

00:32:01,090 --> 00:32:04,600
because at the end of the day what we're

00:32:03,070 --> 00:32:07,240
interested in is the probability of it

00:32:04,600 --> 00:32:09,100
being 0 1 so that F we talked about that

00:32:07,240 --> 00:32:12,970
X F is basically that third parameter

00:32:09,100 --> 00:32:14,740
okay so what's your input network how

00:32:12,970 --> 00:32:23,409
many outputs you want and what's your

00:32:14,740 --> 00:32:26,500
activation function quite nice right so

00:32:23,409 --> 00:32:31,179
that is effectively our definition of

00:32:26,500 --> 00:32:34,509
the network that's defined the next

00:32:31,179 --> 00:32:37,059
thing we're saying is that we're going

00:32:34,509 --> 00:32:39,369
to run a regression we are going to use

00:32:37,059 --> 00:32:40,269
stochastic gradient descent yes it's a

00:32:39,369 --> 00:32:42,969
magic term and

00:32:40,269 --> 00:32:44,830
use abbreviations quite a lot and

00:32:42,969 --> 00:32:49,119
learning rate is going to be point zero

00:32:44,830 --> 00:32:51,039
one and the loss function we're going to

00:32:49,119 --> 00:32:52,629
use this categorical cross entropy so

00:32:51,039 --> 00:32:56,589
the thing we applied at the end of the F

00:32:52,629 --> 00:33:01,419
to get zero or one kind of how you

00:32:56,589 --> 00:33:03,909
calculate loss effectively let's ignore

00:33:01,419 --> 00:33:06,459
this is like sorry this is basically

00:33:03,909 --> 00:33:07,989
what we do for any sort of machine

00:33:06,459 --> 00:33:12,759
learning so what are using scikit-learn

00:33:07,989 --> 00:33:14,049
you will have an element of this okay so

00:33:12,759 --> 00:33:15,519
what we're saying is that we're going to

00:33:14,049 --> 00:33:17,409
take this network and we are going to

00:33:15,519 --> 00:33:20,159
run a regression it's probably better

00:33:17,409 --> 00:33:22,419
not to call this nets but hey whatever

00:33:20,159 --> 00:33:25,029
and then what we're saying is we take

00:33:22,419 --> 00:33:27,190
this output from regression and we tell

00:33:25,029 --> 00:33:30,659
TF learn to create a deep neural network

00:33:27,190 --> 00:33:34,899
and that is going to be our model okay

00:33:30,659 --> 00:33:37,419
imagine having to create this with even

00:33:34,899 --> 00:33:41,169
basic Python or anything that would take

00:33:37,419 --> 00:33:42,879
like weeks okay that'll take weeks this

00:33:41,169 --> 00:33:46,299
is why these toolkits exist these

00:33:42,879 --> 00:33:49,659
problems are so common that you're I

00:33:46,299 --> 00:33:53,139
mean if you're using deep neural Nets

00:33:49,659 --> 00:33:55,599
your concern should be these things your

00:33:53,139 --> 00:33:58,769
concern should never ever be you know

00:33:55,599 --> 00:33:58,769
how these things are implemented

00:34:16,200 --> 00:34:20,740
interesting enough for the majority of

00:34:19,089 --> 00:34:23,669
things you don't need that deep of the

00:34:20,740 --> 00:34:28,329
network to get satisfactory performance

00:34:23,669 --> 00:34:30,970
okay that said we'll we'll look at some

00:34:28,329 --> 00:34:33,460
examples and yes they can get pretty

00:34:30,970 --> 00:34:37,960
deep if you're looking to win

00:34:33,460 --> 00:34:41,079
competitions they'll get very deep will

00:34:37,960 --> 00:34:43,179
also we may cover this when we go to

00:34:41,079 --> 00:34:45,970
overfitting but having a too deep can be

00:34:43,179 --> 00:34:49,809
counterproductive your network might get

00:34:45,970 --> 00:34:51,819
too good at learning your data it's a

00:34:49,809 --> 00:34:54,759
trade-off so the next thing we do we

00:34:51,819 --> 00:34:57,369
have our model basically fit it by

00:34:54,759 --> 00:34:58,779
calling model dot fit surprisingly and

00:34:57,369 --> 00:35:00,640
what that does is it takes your training

00:34:58,779 --> 00:35:02,890
data takes your training output it says

00:35:00,640 --> 00:35:05,859
how much of the data you want to hold

00:35:02,890 --> 00:35:09,339
that as a validation set so metric this

00:35:05,859 --> 00:35:13,359
is for logging output while you're

00:35:09,339 --> 00:35:16,240
baptized is what your epoch is and what

00:35:13,359 --> 00:35:18,009
it does is it then goes and does the

00:35:16,240 --> 00:35:20,230
forward run back propagation forward run

00:35:18,009 --> 00:35:25,319
back propagation until the model has

00:35:20,230 --> 00:35:29,470
been trained up and when does it know

00:35:25,319 --> 00:35:31,480
when at each iteration it can improve by

00:35:29,470 --> 00:35:35,049
point zero one for example here right

00:35:31,480 --> 00:35:37,150
and at each iteration it knows how good

00:35:35,049 --> 00:35:47,500
it's doing or knots based on whatever it

00:35:37,150 --> 00:35:50,410
loss function is so once we have our

00:35:47,500 --> 00:35:53,440
trained up model what we can then do is

00:35:50,410 --> 00:35:56,079
basically use our model to predict on

00:35:53,440 --> 00:35:58,900
our test set test input and then we

00:35:56,079 --> 00:36:01,059
measure the accuracy to say you know how

00:35:58,900 --> 00:36:05,230
accurate this is once okay so this is

00:36:01,059 --> 00:36:07,390
very similar to what you'd be doing in

00:36:05,230 --> 00:36:09,910
any sort of machine learning library so

00:36:07,390 --> 00:36:11,650
whether that be scikit-learn the notions

00:36:09,910 --> 00:36:14,440
of model fed predictions calculating

00:36:11,650 --> 00:36:16,720
accuracy it's exactly the same steps

00:36:14,440 --> 00:36:18,700
what tensorflow and TF learned is giving

00:36:16,720 --> 00:36:19,240
you is basically a very friendly toolkit

00:36:18,700 --> 00:36:20,650
by

00:36:19,240 --> 00:36:24,849
which you can take those concepts and

00:36:20,650 --> 00:36:30,990
apply them to deep learning or apply

00:36:24,849 --> 00:36:36,369
deep learning to those programs right so

00:36:30,990 --> 00:36:40,480
one problem with deep learning is that

00:36:36,369 --> 00:36:43,930
it learns too well in fact very early on

00:36:40,480 --> 00:36:47,020
one of the biggest challenges with

00:36:43,930 --> 00:36:48,160
neural networks is that they learn it so

00:36:47,020 --> 00:36:50,410
well that they basically memorize the

00:36:48,160 --> 00:36:52,119
data so it works very well on the

00:36:50,410 --> 00:36:54,010
training later as soon as you give a

00:36:52,119 --> 00:36:56,260
data that it hasn't seen it falls over

00:36:54,010 --> 00:36:59,050
because it's like you know completely

00:36:56,260 --> 00:37:02,589
tuned to your data so what we try to do

00:36:59,050 --> 00:37:06,520
is we prevent it from learning too well

00:37:02,589 --> 00:37:09,780
so there are some ways of doing that the

00:37:06,520 --> 00:37:14,140
first one is called l2 regularization

00:37:09,780 --> 00:37:15,910
things like you know regularization is

00:37:14,140 --> 00:37:18,190
kind of it's the same with machine

00:37:15,910 --> 00:37:21,190
learning you you kind of with things

00:37:18,190 --> 00:37:24,190
like random for us you know you have to

00:37:21,190 --> 00:37:25,930
stop you can't keep on having a get

00:37:24,190 --> 00:37:27,790
better and better and better because if

00:37:25,930 --> 00:37:31,030
it does it over later

00:37:27,790 --> 00:37:32,770
what l2 regularization does it's some

00:37:31,030 --> 00:37:34,900
math there but I won't go into the mask

00:37:32,770 --> 00:37:37,359
the the simple thing is that it

00:37:34,900 --> 00:37:40,330
penalizes for weights so the more

00:37:37,359 --> 00:37:43,810
weights you have the more penalty you

00:37:40,330 --> 00:37:46,300
get so if your model is going to learn

00:37:43,810 --> 00:37:49,720
with say you know two weights and have

00:37:46,300 --> 00:37:52,270
such two features and it achieves kind

00:37:49,720 --> 00:37:55,240
of similar results with twenty features

00:37:52,270 --> 00:37:57,280
then l2 regularization will penalize the

00:37:55,240 --> 00:38:00,160
second model and reward the first model

00:37:57,280 --> 00:38:03,550
or not reward it will penalize the first

00:38:00,160 --> 00:38:07,109
model less okay so if you can make do

00:38:03,550 --> 00:38:09,040
relatively well with fewer features as

00:38:07,109 --> 00:38:11,800
opposed to having lots and lots of

00:38:09,040 --> 00:38:14,589
features then altro regularization will

00:38:11,800 --> 00:38:15,940
kind of give you that and the benefit of

00:38:14,589 --> 00:38:21,130
that is that because it's using less

00:38:15,940 --> 00:38:22,960
features it tends to not over fit your

00:38:21,130 --> 00:38:25,150
data because if you keep letting it

00:38:22,960 --> 00:38:27,970
going it will just kind of completely

00:38:25,150 --> 00:38:29,650
fit all of your input data and your kind

00:38:27,970 --> 00:38:32,800
of curve would in fact we look like you

00:38:29,650 --> 00:38:35,050
know very very half of the type of

00:38:32,800 --> 00:38:38,560
jumpy Cerf whereas this kind of applies

00:38:35,050 --> 00:38:41,050
the sort of smoothing because the model

00:38:38,560 --> 00:38:43,330
hand will have to achieve the best value

00:38:41,050 --> 00:38:45,010
possible and the more features it tries

00:38:43,330 --> 00:38:47,080
to do the more kind of sharp turns it

00:38:45,010 --> 00:38:49,060
tries to take on your curve the more

00:38:47,080 --> 00:38:51,490
gets penalized so at some point it

00:38:49,060 --> 00:38:53,230
reaches equilibrium where it goes like

00:38:51,490 --> 00:38:54,610
okay this is good enough I'm going to

00:38:53,230 --> 00:38:57,010
have some loss I'm gonna accept that

00:38:54,610 --> 00:39:00,670
loss because if I try to reduce loss to

00:38:57,010 --> 00:39:01,690
zero then I will not generalize one of

00:39:00,670 --> 00:39:04,120
the biggest problems in machine learning

00:39:01,690 --> 00:39:07,330
is generalizing models so regularization

00:39:04,120 --> 00:39:11,200
is one thing the other thing and this is

00:39:07,330 --> 00:39:15,070
like only four well I guess works for

00:39:11,200 --> 00:39:17,400
neural nets is dropouts so what it does

00:39:15,070 --> 00:39:22,660
is that as a stage it randomly drops

00:39:17,400 --> 00:39:24,490
half the connections okay so basically

00:39:22,660 --> 00:39:28,090
what it tells us your model is whatever

00:39:24,490 --> 00:39:30,370
it learns it has to kind of get

00:39:28,090 --> 00:39:33,060
corroborated corroborating evidence from

00:39:30,370 --> 00:39:36,820
multiple sources before accepting it and

00:39:33,060 --> 00:39:41,040
and so just something saying one thing I

00:39:36,820 --> 00:39:44,050
mean if you see some news on Infowars

00:39:41,040 --> 00:39:49,120
then you think it's fake if you see the

00:39:44,050 --> 00:39:53,220
same thing being repeated on BBC CNN Fox

00:39:49,120 --> 00:39:55,930
News I don't know but a lot of sources

00:39:53,220 --> 00:39:58,570
then you have more trust in it well this

00:39:55,930 --> 00:40:01,330
is effectively doing is for any news

00:39:58,570 --> 00:40:03,580
item it's randomly taking out half of

00:40:01,330 --> 00:40:07,030
the sources and only letting the model

00:40:03,580 --> 00:40:08,590
learn if the remaining sources do kind

00:40:07,030 --> 00:40:11,050
of have similar inputs to it

00:40:08,590 --> 00:40:12,340
okay so it makes your model robust

00:40:11,050 --> 00:40:16,360
towards you know

00:40:12,340 --> 00:40:18,700
zooks and sudden things happening and as

00:40:16,360 --> 00:40:20,790
such it prevents overfitting and dropout

00:40:18,700 --> 00:40:23,770
is actually one of the most effective

00:40:20,790 --> 00:40:25,630
techniques for regularization in neural

00:40:23,770 --> 00:40:28,300
nets works surprisingly well

00:40:25,630 --> 00:40:29,650
and if you you can drop different

00:40:28,300 --> 00:40:32,680
percentages it doesn't have to be half

00:40:29,650 --> 00:40:36,220
if you drop half then later on when you

00:40:32,680 --> 00:40:37,270
try to just sum you don't have to do

00:40:36,220 --> 00:40:40,810
certain calculations so it's more

00:40:37,270 --> 00:40:44,550
efficient to actually drop 50% then like

00:40:40,810 --> 00:40:47,800
70 percent or 20 percent over

00:40:44,550 --> 00:40:49,600
okay great so the next thing we're going

00:40:47,800 --> 00:40:51,550
to look at well we'll look at some

00:40:49,600 --> 00:40:54,220
categories of neural networks first one

00:40:51,550 --> 00:40:56,800
is convolution in your legs right so um

00:40:54,220 --> 00:40:59,830
have you guys seen any movies where you

00:40:56,800 --> 00:41:02,320
see like this thing like robot or

00:40:59,830 --> 00:41:04,660
something that scans people with laser

00:41:02,320 --> 00:41:06,520
beams right so that's basically what

00:41:04,660 --> 00:41:09,930
convolution in your no sir okay

00:41:06,520 --> 00:41:14,200
and surprisingly those things are not

00:41:09,930 --> 00:41:19,800
completely made up because that's how

00:41:14,200 --> 00:41:23,560
you actually do image recognition so

00:41:19,800 --> 00:41:25,690
we're going to how they work I think

00:41:23,560 --> 00:41:29,710
we'll talk about is system invariance

00:41:25,690 --> 00:41:31,630
okay that means is that let's say you'll

00:41:29,710 --> 00:41:34,090
have some attributes that regardless of

00:41:31,630 --> 00:41:36,040
those some things are going to hold or

00:41:34,090 --> 00:41:40,540
not hold so for example we've got lots

00:41:36,040 --> 00:41:43,720
of color for the first one you can say

00:41:40,540 --> 00:41:45,580
this this is a this is a this is a even

00:41:43,720 --> 00:41:50,230
though they have different colors okay

00:41:45,580 --> 00:41:51,970
similarly this is a cat image oh brother

00:41:50,230 --> 00:41:54,300
this is an image of a cat and this is an

00:41:51,970 --> 00:41:57,190
image of a cat okay they've been rotated

00:41:54,300 --> 00:41:59,680
translated so they've been rotated this

00:41:57,190 --> 00:42:01,240
is like 45 degrees that way that's 45

00:41:59,680 --> 00:42:03,880
degrees that way they've been translated

00:42:01,240 --> 00:42:06,550
this is the top left this is the bottom

00:42:03,880 --> 00:42:08,050
right but any good image recognition

00:42:06,550 --> 00:42:10,690
thing should be able to say that this is

00:42:08,050 --> 00:42:12,490
a cat and this is a cat right okay so

00:42:10,690 --> 00:42:13,990
when we say one it's all where

00:42:12,490 --> 00:42:17,619
invariance we mean that's regardless of

00:42:13,990 --> 00:42:18,700
these translations rotations these two

00:42:17,619 --> 00:42:24,130
artifacts are going to be the same thing

00:42:18,700 --> 00:42:26,950
so well convolutional neural networks

00:42:24,130 --> 00:42:29,770
are is that their neural networks that

00:42:26,950 --> 00:42:33,160
share their parameters across space so

00:42:29,770 --> 00:42:34,960
any parameters that are for this cat are

00:42:33,160 --> 00:42:39,910
going to apply to this cat even though

00:42:34,960 --> 00:42:42,280
there have been rotated slightly so what

00:42:39,910 --> 00:42:45,910
a convolution is is like let's say this

00:42:42,280 --> 00:42:47,650
this whole thing is an image the

00:42:45,910 --> 00:42:52,270
convolution is basically a subset of

00:42:47,650 --> 00:42:55,590
that image okay so what we do let me

00:42:52,270 --> 00:42:55,590
just check if that slide is

00:42:56,940 --> 00:43:05,050
we're getting deep okay right so what we

00:43:00,880 --> 00:43:06,910
do is we go let's say our image is this

00:43:05,050 --> 00:43:09,910
big and our convolution is this big

00:43:06,910 --> 00:43:12,490
right we go one at a time like this

00:43:09,910 --> 00:43:14,290
across the whole of the image and we

00:43:12,490 --> 00:43:18,810
extract some outputs from each of those

00:43:14,290 --> 00:43:21,089
okay our first image starts off as a big

00:43:18,810 --> 00:43:23,500
thin thing

00:43:21,089 --> 00:43:28,240
thin as in it can have three features

00:43:23,500 --> 00:43:33,869
maybe four features rgba okay so the

00:43:28,240 --> 00:43:33,869
base image our input layer it can be

00:43:34,020 --> 00:43:42,369
what's called I can have four features

00:43:37,780 --> 00:43:44,440
RGB n a what we'll do is we'll take a

00:43:42,369 --> 00:43:46,180
subset of that image as a first

00:43:44,440 --> 00:43:48,960
convolution and take some output but

00:43:46,180 --> 00:43:51,970
that output is going to have a fewer

00:43:48,960 --> 00:43:54,250
total number of outputs so instead of

00:43:51,970 --> 00:43:56,619
all of the pixels in the image that will

00:43:54,250 --> 00:43:59,589
have a subset of that but it will the

00:43:56,619 --> 00:44:02,319
output will be thicker or fatter as in

00:43:59,589 --> 00:44:06,970
it will have more features so instead of

00:44:02,319 --> 00:44:09,339
having RGB and a it will have some sort

00:44:06,970 --> 00:44:11,829
of thing that will additional outputs

00:44:09,339 --> 00:44:16,210
basically okay so we're going from one

00:44:11,829 --> 00:44:21,369
big thing to a smaller layer that is

00:44:16,210 --> 00:44:23,470
fatter and we keep on doing that so from

00:44:21,369 --> 00:44:26,170
the first one we have the second one

00:44:23,470 --> 00:44:29,530
then we have the third one we have the

00:44:26,170 --> 00:44:32,050
fourth one and then you'll see that as

00:44:29,530 --> 00:44:34,750
we go along we can feed the end result

00:44:32,050 --> 00:44:40,960
to a classifier and that's what

00:44:34,750 --> 00:44:42,430
convolutions are okay skipping over some

00:44:40,960 --> 00:44:43,859
details but effectively that's what

00:44:42,430 --> 00:44:47,200
we're doing that's that that's how

00:44:43,859 --> 00:44:51,750
computers recognize images they scan the

00:44:47,200 --> 00:44:54,130
base image you know square by square and

00:44:51,750 --> 00:44:55,990
generate some outputs from that and then

00:44:54,130 --> 00:44:57,670
they analyze that and that and that and

00:44:55,990 --> 00:45:00,060
then finally and the result of a

00:44:57,670 --> 00:45:00,060
classifier

00:45:03,020 --> 00:45:10,680
so in terms of stride stride basicly

00:45:07,290 --> 00:45:12,119
means how much you're jumping from when

00:45:10,680 --> 00:45:14,810
you're scanning the image so let's say

00:45:12,119 --> 00:45:16,980
if kind of like this you're doing like

00:45:14,810 --> 00:45:19,500
along this dimension you're taking that

00:45:16,980 --> 00:45:21,660
that that how far you're progressing

00:45:19,500 --> 00:45:23,310
free trick it's kind of an example if

00:45:21,660 --> 00:45:26,910
this phone were tablet and this would be

00:45:23,310 --> 00:45:29,670
a better example what pulling basically

00:45:26,910 --> 00:45:31,470
means is that from those things you

00:45:29,670 --> 00:45:34,520
you're doing some sort of aggregation

00:45:31,470 --> 00:45:37,650
like a max or a min or something okay

00:45:34,520 --> 00:45:41,579
less important but but the key thing is

00:45:37,650 --> 00:45:44,700
that an actual image recognition thing

00:45:41,579 --> 00:45:48,960
is kind of built up like this so you

00:45:44,700 --> 00:45:50,849
have a base image you have a convolution

00:45:48,960 --> 00:45:52,380
layer you have a max pooling layer which

00:45:50,849 --> 00:45:54,750
means for each convolution you're taking

00:45:52,380 --> 00:45:56,670
the max value you have another

00:45:54,750 --> 00:45:57,960
convolution layer you have maximum layer

00:45:56,670 --> 00:45:59,430
you have a fully connected layer fully

00:45:57,960 --> 00:46:01,859
connect layer and then finally feed it

00:45:59,430 --> 00:46:04,020
took a classifier okay

00:46:01,859 --> 00:46:05,880
the details of the convolution are less

00:46:04,020 --> 00:46:07,619
important here but if you look at the

00:46:05,880 --> 00:46:09,300
similarities between this and the code

00:46:07,619 --> 00:46:11,460
that we had where we just went fully

00:46:09,300 --> 00:46:18,660
connected fully connected fully

00:46:11,460 --> 00:46:20,339
connected and then a soft max rate means

00:46:18,660 --> 00:46:26,060
all the inputs are connected to old

00:46:20,339 --> 00:46:30,980
outputs should have mentioned that okay

00:46:26,060 --> 00:46:35,010
so here previously we had a very basic

00:46:30,980 --> 00:46:37,170
linear regression model right a linear

00:46:35,010 --> 00:46:39,450
model that could try to protect some

00:46:37,170 --> 00:46:41,700
stuff now what we have is a model that

00:46:39,450 --> 00:46:43,980
has two layers of convolutions each with

00:46:41,700 --> 00:46:46,829
max pooling and then doing exactly what

00:46:43,980 --> 00:46:48,329
we were doing before and this is how

00:46:46,829 --> 00:46:51,720
long your networks can get deeper and

00:46:48,329 --> 00:46:53,970
deeper and deeper Yan laocoon anybody

00:46:51,720 --> 00:46:57,890
heard of en Lagoon he's basically had of

00:46:53,970 --> 00:47:00,000
he's a guy who started facebook's i

00:46:57,890 --> 00:47:04,890
stuff

00:47:00,000 --> 00:47:06,420
head of department currently and so that

00:47:04,890 --> 00:47:08,430
there's an image recognition compute

00:47:06,420 --> 00:47:10,410
computer competition that happens every

00:47:08,430 --> 00:47:12,660
year and people try to make better and

00:47:10,410 --> 00:47:13,849
better models in order to be the best

00:47:12,660 --> 00:47:15,620
classifier

00:47:13,849 --> 00:47:18,730
well not

00:47:15,620 --> 00:47:22,970
a few years ago in 2012 the winner was

00:47:18,730 --> 00:47:25,640
Alex Chris that's key and his network

00:47:22,970 --> 00:47:28,940
was Alex net the projector isn't doing

00:47:25,640 --> 00:47:32,180
that image that love is justice but you

00:47:28,940 --> 00:47:33,710
can see it like these networks they have

00:47:32,180 --> 00:47:49,070
like hundreds and hundreds of theirs

00:47:33,710 --> 00:47:50,810
okay they get quite complicated they may

00:47:49,070 --> 00:47:53,240
have more input sets it's not just

00:47:50,810 --> 00:48:01,880
letters it's like image recognition so

00:47:53,240 --> 00:48:03,650
like recognizing pigs and animals and

00:48:01,880 --> 00:48:06,950
yep I mean every year the models get

00:48:03,650 --> 00:48:10,190
better there I mean if you just look at

00:48:06,950 --> 00:48:12,320
your phone and you know what your friend

00:48:10,190 --> 00:48:14,630
can do now and versus say two years ago

00:48:12,320 --> 00:48:24,350
you can kind of get an idea of how it's

00:48:14,630 --> 00:48:25,670
progressing okay so I guess because we

00:48:24,350 --> 00:48:27,350
didn't go into details of convolutions

00:48:25,670 --> 00:48:31,100
going to deserves this kind of heart but

00:48:27,350 --> 00:48:33,590
the idea is that from K by K we end up

00:48:31,100 --> 00:48:35,780
with a one by one convolution for things

00:48:33,590 --> 00:48:38,980
and one by one convolutions can be

00:48:35,780 --> 00:48:42,100
useful for adding some level of ERC

00:48:38,980 --> 00:48:44,390
weren't going to details of exactly how

00:48:42,100 --> 00:48:46,280
not really important in this case but

00:48:44,390 --> 00:48:48,740
it's worth mentioning this this is

00:48:46,280 --> 00:48:49,700
what's called an inception model because

00:48:48,740 --> 00:48:51,760
everybody likes to hear the word

00:48:49,700 --> 00:48:55,270
inception

00:48:51,760 --> 00:48:55,270
what is Inception

00:48:57,950 --> 00:49:04,590
it's cool move you in in your lyrics

00:49:02,820 --> 00:49:07,770
well what it basically means is that you

00:49:04,590 --> 00:49:08,940
take something like a 505 convolutions

00:49:07,770 --> 00:49:11,720
we were three composition one other one

00:49:08,940 --> 00:49:12,870
convolution and just stack them together

00:49:11,720 --> 00:49:14,490
okay

00:49:12,870 --> 00:49:16,440
so in this case we're taking one-by-one

00:49:14,490 --> 00:49:19,230
convolutions of the raw image we're

00:49:16,440 --> 00:49:20,820
doing an average pooling so from this

00:49:19,230 --> 00:49:22,920
one by one we're then taking a five by

00:49:20,820 --> 00:49:24,960
five we're taking the output of that and

00:49:22,920 --> 00:49:26,730
plugging it here we're taking the one by

00:49:24,960 --> 00:49:28,250
one and putting it there we're taking it

00:49:26,730 --> 00:49:30,240
three by three and plugging it there

00:49:28,250 --> 00:49:32,670
we're just increasing the thickness

00:49:30,240 --> 00:49:34,970
you'll notice that all of these things

00:49:32,670 --> 00:49:38,120
what they're doing is starting from some

00:49:34,970 --> 00:49:40,950
big rectangular thing and gradually

00:49:38,120 --> 00:49:44,670
shifting towards smaller fatter things

00:49:40,950 --> 00:49:46,680
until you you're increasing depth and

00:49:44,670 --> 00:49:50,060
you're reducing surface area that's

00:49:46,680 --> 00:49:50,060
that's that's the goal

00:49:51,240 --> 00:49:59,100
when you send a thin class sorry a fat

00:49:56,220 --> 00:50:02,100
kind of small surface area thing to a

00:49:59,100 --> 00:50:04,290
classifier it performs better than if

00:50:02,100 --> 00:50:06,060
you were just maybe I'll just miss the

00:50:04,290 --> 00:50:08,850
convolution taste but if it seems like

00:50:06,060 --> 00:50:12,240
they're kind of narrowing of focus now

00:50:08,850 --> 00:50:16,020
if I start off with a big picture with

00:50:12,240 --> 00:50:18,270
cat in the corner yep hoping that we

00:50:16,020 --> 00:50:20,280
would deduce over time actually I want

00:50:18,270 --> 00:50:21,870
to focus on this area for the

00:50:20,280 --> 00:50:24,390
classification but doesn't that mean

00:50:21,870 --> 00:50:26,850
that I have to effectively take a sample

00:50:24,390 --> 00:50:29,190
size so as you say a smaller rectangular

00:50:26,850 --> 00:50:31,050
surface and there is my example but then

00:50:29,190 --> 00:50:32,430
through all I know because I haven't

00:50:31,050 --> 00:50:33,690
analyzed the image yet there might be

00:50:32,430 --> 00:50:34,890
one over here one over here so I've got

00:50:33,690 --> 00:50:37,260
to do that all over the place and then

00:50:34,890 --> 00:50:39,420
decide that some of these just don't

00:50:37,260 --> 00:50:41,520
matter they're statistically unlike you

00:50:39,420 --> 00:50:43,860
have got anything in them right so if I

00:50:41,520 --> 00:50:45,780
find two yeah are you saying I'm not

00:50:43,860 --> 00:50:50,700
really going to stack those in serial

00:50:45,780 --> 00:50:54,150
going through yeah kind of it's kind of

00:50:50,700 --> 00:50:54,420
like what's it cools in in machine

00:50:54,150 --> 00:51:01,250
learning

00:50:54,420 --> 00:51:03,680
Oh God towards this comple-- confrim at

00:51:01,250 --> 00:51:06,930
[Music]

00:51:03,680 --> 00:51:08,400
you you the idea is that you're

00:51:06,930 --> 00:51:10,820
effectively getting multiple models to

00:51:08,400 --> 00:51:15,690
predict the same thing and then take

00:51:10,820 --> 00:51:18,330
okay okay but what you get yes well you

00:51:15,690 --> 00:51:21,390
get I mean let's imagine a 3x3 grid the

00:51:18,330 --> 00:51:24,150
bottom left corner of which is a cat the

00:51:21,390 --> 00:51:26,850
rest of the sky ah don't don't you get

00:51:24,150 --> 00:51:29,220
eight sets of sky and there can't be a

00:51:26,850 --> 00:51:31,110
cat yeah I'll stop you though because

00:51:29,220 --> 00:51:32,610
what happens is that the first layer it

00:51:31,110 --> 00:51:34,920
just understands lines it doesn't

00:51:32,610 --> 00:51:39,810
understand the shape okay

00:51:34,920 --> 00:51:42,090
delay after that puts the lines together

00:51:39,810 --> 00:51:51,390
and understands basic shapes like

00:51:42,090 --> 00:51:55,350
triangles and whatnot and as the layers

00:51:51,390 --> 00:51:57,240
progress the end result over when we get

00:51:55,350 --> 00:52:00,000
to this part is basically is it a cat or

00:51:57,240 --> 00:52:01,890
not a cat over here it might be the face

00:52:00,000 --> 00:52:08,880
of a face of a cat if we had more layers

00:52:01,890 --> 00:52:10,320
let's say Alexis thing yeah they're good

00:52:08,880 --> 00:52:12,030
so over here for example it might be

00:52:10,320 --> 00:52:14,550
Katherine Alton no cat over here it

00:52:12,030 --> 00:52:17,790
might be a face of the cat over here

00:52:14,550 --> 00:52:20,520
might be lips ears whiskers and whatnot

00:52:17,790 --> 00:52:23,040
over here it could be triangles and

00:52:20,520 --> 00:52:25,710
circles and ellipses over here it could

00:52:23,040 --> 00:52:27,300
be just straight lines and gradually at

00:52:25,710 --> 00:52:36,780
each layer it adds more understanding

00:52:27,300 --> 00:52:40,880
that's right so the next thing we'll

00:52:36,780 --> 00:52:43,830
look at is recurrent neural networks so

00:52:40,880 --> 00:52:48,020
whereas convolutional networks are

00:52:43,830 --> 00:52:52,170
invariant in terms of you know

00:52:48,020 --> 00:52:57,450
transformations recurrent networks have

00:52:52,170 --> 00:53:00,440
an element of time so the idea is that

00:52:57,450 --> 00:53:04,290
in addition to having inputs and outputs

00:53:00,440 --> 00:53:05,880
each one if actually has a feed into the

00:53:04,290 --> 00:53:09,330
next one this is time this is not

00:53:05,880 --> 00:53:12,510
features so x1 and x2 are not two

00:53:09,330 --> 00:53:15,030
different features this is x1 at T 1

00:53:12,510 --> 00:53:16,110
this is X 2 at T 2 or the same in

00:53:15,030 --> 00:53:18,540
Twitter T 2 okay

00:53:16,110 --> 00:53:21,240
the idea is that in addition to output

00:53:18,540 --> 00:53:25,040
you feed something more that you pass on

00:53:21,240 --> 00:53:30,089
to your self in the next letter

00:53:25,040 --> 00:53:34,500
next iteration so kind of like memory

00:53:30,089 --> 00:53:36,210
will get so is that purely abstract this

00:53:34,500 --> 00:53:38,040
is one second more than the previous

00:53:36,210 --> 00:53:39,299
thing you looked at or is it the result

00:53:38,040 --> 00:53:44,099
of the previous thing in conjunction

00:53:39,299 --> 00:53:45,809
with that knowledge so because for

00:53:44,099 --> 00:53:47,190
example if you have a video and you're

00:53:45,809 --> 00:53:48,960
looking at the individual frames of that

00:53:47,190 --> 00:53:51,180
video several of them might appear to be

00:53:48,960 --> 00:53:55,380
blurry on their own but taken in context

00:53:51,180 --> 00:53:57,180
with the others you know you can kind of

00:53:55,380 --> 00:53:59,430
not not the blurry beat not the blurry

00:53:57,180 --> 00:54:03,240
bit but let's say in other word foreign

00:53:59,430 --> 00:54:04,980
and our sequence models so basically the

00:54:03,240 --> 00:54:12,150
sequencing of things becomes important

00:54:04,980 --> 00:54:14,130
that that's that's okay so basically

00:54:12,150 --> 00:54:16,589
what you have is a level of feedback so

00:54:14,130 --> 00:54:18,530
previously your output was based on your

00:54:16,589 --> 00:54:22,740
input and your weight and that was it

00:54:18,530 --> 00:54:25,530
now your output is based on your input

00:54:22,740 --> 00:54:28,079
your weight and what happened before the

00:54:25,530 --> 00:54:29,790
last time okay now the problem with that

00:54:28,079 --> 00:54:33,869
is that your gradients can tend to

00:54:29,790 --> 00:54:36,660
explode or it can tend to come down to

00:54:33,869 --> 00:54:40,819
zero because you're reinforcing the same

00:54:36,660 --> 00:54:43,049
thing over and over and over again okay

00:54:40,819 --> 00:54:44,609
the way we get around exploring

00:54:43,049 --> 00:54:46,349
gradients is very easy is that you have

00:54:44,609 --> 00:54:50,369
a maximum clip so if it reaches a

00:54:46,349 --> 00:54:54,569
certain value then you just clip it for

00:54:50,369 --> 00:54:56,790
the other one we have notions of what

00:54:54,569 --> 00:54:58,530
they call long short-term memory so why

00:54:56,790 --> 00:55:02,160
they call it long short term memory or

00:54:58,530 --> 00:55:04,920
lsdm it is it's basically the memory you

00:55:02,160 --> 00:55:09,359
have is that you don't let it go on for

00:55:04,920 --> 00:55:11,369
a long long time but you have a limited

00:55:09,359 --> 00:55:13,500
amount of time it's very limited with an

00:55:11,369 --> 00:55:18,240
amount of memory that you can remember

00:55:13,500 --> 00:55:20,040
much later so think of it like if you

00:55:18,240 --> 00:55:21,420
think how your own memory works

00:55:20,040 --> 00:55:23,970
you may remember something from

00:55:21,420 --> 00:55:26,609
childhood right maybe you want to play

00:55:23,970 --> 00:55:28,290
in a park or something right and you can

00:55:26,609 --> 00:55:30,420
remember like a five second clip of

00:55:28,290 --> 00:55:31,680
something that happened okay you don't

00:55:30,420 --> 00:55:32,910
remember what happened before it you

00:55:31,680 --> 00:55:35,010
don't remember what happened after it

00:55:32,910 --> 00:55:36,750
certainly not what everything that

00:55:35,010 --> 00:55:37,500
happened subsequently you just remember

00:55:36,750 --> 00:55:38,940
those you know two

00:55:37,500 --> 00:55:43,080
three minutes that's something a long

00:55:38,940 --> 00:55:44,580
term short duration yes so that's how

00:55:43,080 --> 00:55:47,430
you go around a vanishing gradient

00:55:44,580 --> 00:55:55,350
problem also the explosion it was both

00:55:47,430 --> 00:55:57,360
space it takes care of us so L STM's and

00:55:55,350 --> 00:55:58,230
RL ends in terms of regularization this

00:55:57,360 --> 00:56:00,090
is kind of more of a round

00:55:58,230 --> 00:56:02,460
implementation but l two regression

00:56:00,090 --> 00:56:04,680
works quite well dropout you have to be

00:56:02,460 --> 00:56:07,200
careful as in if you do drop out on

00:56:04,680 --> 00:56:09,990
inputs that's fine if you do drop outs

00:56:07,200 --> 00:56:12,510
you can't do any drop outs from the past

00:56:09,990 --> 00:56:14,250
value because then the meaning

00:56:12,510 --> 00:56:16,440
completely differs as in you're not

00:56:14,250 --> 00:56:17,970
dropping input sets you're dropping what

00:56:16,440 --> 00:56:22,530
you told yourself from from the past

00:56:17,970 --> 00:56:25,320
which in bed okay will not go into beam

00:56:22,530 --> 00:56:30,180
search because that's interesting we'll

00:56:25,320 --> 00:56:32,910
look at some applications of yeah that's

00:56:30,180 --> 00:56:35,490
impossible to see look at some

00:56:32,910 --> 00:56:37,320
applications of Arlen's so in terms of

00:56:35,490 --> 00:56:38,760
convolution neural networks there are

00:56:37,320 --> 00:56:40,920
many applications but they primarily

00:56:38,760 --> 00:56:43,050
tend to be around image recognition so

00:56:40,920 --> 00:56:45,660
identifying cats and dogs that's like

00:56:43,050 --> 00:56:47,400
the hello world of deep learning you

00:56:45,660 --> 00:56:50,670
have a set of cats and a set of dogs and

00:56:47,400 --> 00:56:51,990
you have to say which is which but

00:56:50,670 --> 00:56:56,280
ireland's can do some pretty cool stuff

00:56:51,990 --> 00:57:00,630
okay we can't read this but take my word

00:56:56,280 --> 00:57:04,950
for it this text is actually very nicely

00:57:00,630 --> 00:57:06,900
summarized by the bottom so this blurb

00:57:04,950 --> 00:57:12,540
is summarized by this blur you can't see

00:57:06,900 --> 00:57:15,020
either so but it is a pretty tip let's

00:57:12,540 --> 00:57:15,020
see if this works

00:57:21,940 --> 00:57:37,400
[Applause]

00:57:34,280 --> 00:57:43,760
right so that was actually entirely

00:57:37,400 --> 00:57:46,220
generated by our network yeah and that's

00:57:43,760 --> 00:57:48,170
some music that you can find on YouTube

00:57:46,220 --> 00:57:50,540
that's basically people who've built

00:57:48,170 --> 00:57:52,700
neural networks that are generating

00:57:50,540 --> 00:57:54,530
music they they were given samples in

00:57:52,700 --> 00:57:57,050
this particular case they actually used

00:57:54,530 --> 00:58:01,490
Beatles music I personally think it's

00:57:57,050 --> 00:58:03,590
more like Beach Boys but hey we and

00:58:01,490 --> 00:58:05,690
that's what it produced it wasn't like

00:58:03,590 --> 00:58:07,340
humans helped helped it in any way they

00:58:05,690 --> 00:58:12,590
just gave us samples and said hey go go

00:58:07,340 --> 00:58:15,620
make another one okay another one that

00:58:12,590 --> 00:58:26,870
we can hopefully look at if I remember

00:58:15,620 --> 00:58:30,770
my can't have this there okay yeah we're

00:58:26,870 --> 00:58:33,170
here right okay so next thing is this is

00:58:30,770 --> 00:58:37,250
from some assignments that we were doing

00:58:33,170 --> 00:58:38,960
was part of a Udacity course that is not

00:58:37,250 --> 00:58:42,440
important but in fact we what we're

00:58:38,960 --> 00:58:45,860
doing is we are feeding a we're building

00:58:42,440 --> 00:58:49,640
a new lab there and then we're training

00:58:45,860 --> 00:58:53,540
it on the text from a lot of Simpsons

00:58:49,640 --> 00:59:00,370
episodes okay and then we are telling it

00:58:53,540 --> 00:59:00,370
to generate some output

00:59:16,280 --> 00:59:22,160
yeah the TV script is nonsensical in

00:59:20,690 --> 00:59:24,170
many ways the Simpsons are nonsensical

00:59:22,160 --> 00:59:27,849
but this was this was basically a very

00:59:24,170 --> 00:59:30,680
simple model that was kind of used in

00:59:27,849 --> 00:59:31,760
teaching neural networks to people who

00:59:30,680 --> 00:59:33,920
have never used recurrent neural

00:59:31,760 --> 00:59:36,980
networks before and that's that's the

00:59:33,920 --> 00:59:39,109
level of output it actually got and if

00:59:36,980 --> 00:59:42,050
you think about it yeah okay it's kind

00:59:39,109 --> 00:59:44,570
of random in terms of overall context

00:59:42,050 --> 00:59:46,400
but the sentences they kind of do make

00:59:44,570 --> 00:59:47,480
sense they're not completely you know

00:59:46,400 --> 00:59:52,099
weird

00:59:47,480 --> 00:59:55,670
so text processing is one area where you

00:59:52,099 --> 00:59:57,890
know it's actually quite useful you'll

00:59:55,670 --> 01:00:00,770
see these are not actually copied and

00:59:57,890 --> 01:00:02,720
pasted or cut and pasted excerpts from

01:00:00,770 --> 01:00:04,400
whatever it was fed it was actually

01:00:02,720 --> 01:00:14,990
trained on it and it actually generated

01:00:04,400 --> 01:00:16,910
these things that's the whole NLP is a

01:00:14,990 --> 01:00:21,650
whole different field altogether there's

01:00:16,910 --> 01:00:22,940
things like vectorization you can go at

01:00:21,650 --> 01:00:25,640
the end of day you convert it to vectors

01:00:22,940 --> 01:00:27,770
okay so one of the things let's say you

01:00:25,640 --> 01:00:29,359
have a word would say five characters it

01:00:27,770 --> 01:00:31,670
could be that you take

01:00:29,359 --> 01:00:34,310
it's called shingling basically you take

01:00:31,670 --> 01:00:36,260
three characters that becomes an

01:00:34,310 --> 01:00:38,420
identifier and then next few characters

01:00:36,260 --> 01:00:39,440
becomes another identifier and third

01:00:38,420 --> 01:00:43,240
three characters becomes another

01:00:39,440 --> 01:00:46,520
identifier okay so your five letter word

01:00:43,240 --> 01:00:49,640
becomes one shingle to shingle three

01:00:46,520 --> 01:00:52,250
shingles okay each one can have a zero

01:00:49,640 --> 01:00:55,040
in this case will be one one one okay

01:00:52,250 --> 01:00:58,099
you'll have many many shingles and then

01:00:55,040 --> 01:00:59,839
other words will have you know 0 1 0 and

01:00:58,099 --> 01:01:02,589
whatnot but then it is going to be

01:00:59,839 --> 01:01:02,589
become vectorized

01:01:03,210 --> 01:01:13,349
yeah I mean that's one of the more

01:01:11,609 --> 01:01:16,410
advanced models the very most basic

01:01:13,349 --> 01:01:18,839
thing is bag of words as then you just

01:01:16,410 --> 01:01:23,730
say a word count as in this document has

01:01:18,839 --> 01:01:26,460
this many words sorry has these words

01:01:23,730 --> 01:01:28,430
this many times and then you add things

01:01:26,460 --> 01:01:31,020
like sequencing you are chingling

01:01:28,430 --> 01:01:33,390
shingling usually results in things

01:01:31,020 --> 01:01:35,400
being massive so you then have to hash

01:01:33,390 --> 01:01:37,980
them into buckets so that you can

01:01:35,400 --> 01:01:40,020
compute them reasonably NLP is like a

01:01:37,980 --> 01:01:42,650
really really interesting field because

01:01:40,020 --> 01:01:49,200
if you think images have high dimension

01:01:42,650 --> 01:01:54,599
NLP has much much higher dimension right

01:01:49,200 --> 01:01:58,410
so script generation language

01:01:54,599 --> 01:02:04,280
translation it should be fun who knows

01:01:58,410 --> 01:02:06,630
French okay so this is basically I'll

01:02:04,280 --> 01:02:08,460
give you the links to these or tweet it

01:02:06,630 --> 01:02:11,040
out you can look at the source code for

01:02:08,460 --> 01:02:12,270
all of these as in what is doing and you

01:02:11,040 --> 01:02:14,520
can see how how the model actually

01:02:12,270 --> 01:02:15,960
trains itself right for example three

01:02:14,520 --> 01:02:17,460
pubs zero you can see how many batches

01:02:15,960 --> 01:02:19,079
is processed what the training accuracy

01:02:17,460 --> 01:02:21,150
is what the losses and you can see the

01:02:19,079 --> 01:02:24,750
loss gradually going down down down it's

01:02:21,150 --> 01:02:27,960
down to 10% down to 6% down to 4% down

01:02:24,750 --> 01:02:30,480
to 2% and whatnot okay I think it comes

01:02:27,960 --> 01:02:34,380
down to 1.5% so at the end of the day

01:02:30,480 --> 01:02:40,230
after all that training it we give it

01:02:34,380 --> 01:02:43,650
two things to translates Jesus yeah so

01:02:40,230 --> 01:02:45,990
we say he saw a old yellow truck I don't

01:02:43,650 --> 01:02:48,359
know French but I am told that this is

01:02:45,990 --> 01:02:49,980
fairly accurate right this is quite

01:02:48,359 --> 01:02:55,829
interesting auto-encoders basically is

01:02:49,980 --> 01:02:58,079
kind of like a feedback loop and they

01:02:55,829 --> 01:02:59,640
can be used to well not really a

01:02:58,079 --> 01:03:01,200
feedback loop but it's actually what

01:02:59,640 --> 01:03:05,069
they do is they fill in missing data

01:03:01,200 --> 01:03:06,630
okay the train itself to fill in missile

01:03:05,069 --> 01:03:08,400
missing data and somebody had the bright

01:03:06,630 --> 01:03:11,069
idea that hang on if it's filling in for

01:03:08,400 --> 01:03:15,299
missing data we should we may be able to

01:03:11,069 --> 01:03:16,620
make blurry images sharp and there's

01:03:15,299 --> 01:03:19,290
actually a website now cool left

01:03:16,620 --> 01:03:21,960
enhanced or i/o and if you go and submit

01:03:19,290 --> 01:03:25,430
your horrible holiday photo it actually

01:03:21,960 --> 01:03:28,170
makes it sharper I don't know if it's

01:03:25,430 --> 01:03:31,800
visible but you can see that this image

01:03:28,170 --> 01:03:36,420
is quite hazy right and the sharpness

01:03:31,800 --> 01:03:38,130
here is noticeably increased yeah this

01:03:36,420 --> 01:03:39,990
thing hasn't been trained on what that

01:03:38,130 --> 01:03:42,660
place looks like it's been trained to

01:03:39,990 --> 01:03:45,150
recognize where things are blurry and

01:03:42,660 --> 01:03:48,440
how it can be filled in what do you get

01:03:45,150 --> 01:03:51,030
a sample days of yourself doing that

01:03:48,440 --> 01:03:52,380
generates it okay I'll do the same thing

01:03:51,030 --> 01:03:55,950
about the English the French translation

01:03:52,380 --> 01:03:57,450
because French is very easy because you

01:03:55,950 --> 01:03:59,520
have like lots of translations too

01:03:57,450 --> 01:04:00,930
so basically in that case we're not

01:03:59,520 --> 01:04:02,310
using any external model in that case so

01:04:00,930 --> 01:04:03,750
you can look go and look at the code we

01:04:02,310 --> 01:04:05,820
just fed it some paragraphs of English

01:04:03,750 --> 01:04:09,920
text and their French translations and

01:04:05,820 --> 01:04:16,640
it's gone and generated that I just see

01:04:09,920 --> 01:04:16,640
so much of that data because when you go

01:04:26,930 --> 01:04:32,550
right you give it a sample image but you

01:04:29,880 --> 01:04:35,130
don't give it the same image not wait

01:04:32,550 --> 01:04:37,770
for it to know at the same time oh thank

01:04:35,130 --> 01:04:41,250
you thank you very even oh yes again

01:04:37,770 --> 01:04:43,800
record rather than make yeah actually

01:04:41,250 --> 01:04:45,690
that's that's how they for free images

01:04:43,800 --> 01:04:49,470
that's exactly idea it's kind of similar

01:04:45,690 --> 01:04:51,240
to how you make false on networks for

01:04:49,470 --> 01:04:55,320
software that needs to be you know

01:04:51,240 --> 01:04:57,210
realign to network bulbs right style

01:04:55,320 --> 01:04:58,410
transfer isn't one of the cool things

01:04:57,210 --> 01:05:01,650
this is actually a website where you can

01:04:58,410 --> 01:05:03,840
do this and you basically give it a

01:05:01,650 --> 01:05:05,820
picture and you give it another picture

01:05:03,840 --> 01:05:08,850
and it takes the style of one picture

01:05:05,820 --> 01:05:10,770
and superimpose it on the other not

01:05:08,850 --> 01:05:13,770
really superimposition really applies

01:05:10,770 --> 01:05:15,510
that style it's quite cool you can go

01:05:13,770 --> 01:05:18,720
and play around with it yourself the

01:05:15,510 --> 01:05:20,520
code for it is actually out there for

01:05:18,720 --> 01:05:22,470
when you want to like you know we're

01:05:20,520 --> 01:05:24,180
talking about Gary shorts and political

01:05:22,470 --> 01:05:27,800
statements if you want to like

01:05:24,180 --> 01:05:27,800
superimpose cabbages on to cabbages

01:05:28,680 --> 01:05:33,310
that's pretty good I mean look at that

01:05:31,180 --> 01:05:44,710
like look at that look at that and then

01:05:33,310 --> 01:05:46,930
look at that it's right one of the

01:05:44,710 --> 01:05:48,970
biggest advances in recent times is

01:05:46,930 --> 01:05:54,420
actually gams generative a visceral

01:05:48,970 --> 01:05:56,590
networks and how cans work is that

01:05:54,420 --> 01:05:59,050
there's a neural network there's

01:05:56,590 --> 01:06:02,109
generating stuff and another neural

01:05:59,050 --> 01:06:03,730
network that is discriminating stuff so

01:06:02,109 --> 01:06:08,470
it's basically two neural networks who

01:06:03,730 --> 01:06:10,600
fight against each other and what the

01:06:08,470 --> 01:06:14,530
generative Network does is starting from

01:06:10,600 --> 01:06:18,520
random blurb it just generates data okay

01:06:14,530 --> 01:06:21,240
and what the discriminator does is given

01:06:18,520 --> 01:06:24,910
some real data and the inputs from this

01:06:21,240 --> 01:06:27,250
it decides whether the thing that's been

01:06:24,910 --> 01:06:33,280
generated looks like the real thing or

01:06:27,250 --> 01:06:36,369
it's a fake they're competing this guy

01:06:33,280 --> 01:06:40,270
is trying to create fraud stuff fake

01:06:36,369 --> 01:06:43,390
news this guy is trying to identify fake

01:06:40,270 --> 01:06:46,359
stuff the fake news as soon as this guy

01:06:43,390 --> 01:06:49,150
gets better this guy has to kind of get

01:06:46,359 --> 01:06:50,530
better after that as soon as this guy

01:06:49,150 --> 01:06:53,170
gets better this guy has to do better

01:06:50,530 --> 01:06:54,850
it's like a competitively they get

01:06:53,170 --> 01:06:57,700
better and better and better until what

01:06:54,850 --> 01:07:00,609
happens is that the things that are

01:06:57,700 --> 01:07:04,240
being generated are so much like real

01:07:00,609 --> 01:07:15,510
data that is it's very difficult to

01:07:04,240 --> 01:07:19,090
separate the two even by humans okay so

01:07:15,510 --> 01:07:21,790
Stan gun is one such model and this is

01:07:19,090 --> 01:07:23,470
actually a real application now you can

01:07:21,790 --> 01:07:28,450
go through our website and you can type

01:07:23,470 --> 01:07:31,930
in descriptions okay so if you were to

01:07:28,450 --> 01:07:35,080
type in this small bird has a white

01:07:31,930 --> 01:07:39,550
breast light gray head and black wings

01:07:35,080 --> 01:07:41,170
and tail the can will go and grow you

01:07:39,550 --> 01:07:45,339
this image this is

01:07:41,170 --> 01:07:47,680
not an image search okay it's not doing

01:07:45,339 --> 01:07:50,859
this from a library of images it is

01:07:47,680 --> 01:07:55,030
growing a picture of a bird with that

01:07:50,859 --> 01:07:56,890
description yeah well I said the things

01:07:55,030 --> 01:07:59,500
that people could do and machines could

01:07:56,890 --> 01:08:01,420
never do this is like Ganz are one of

01:07:59,500 --> 01:08:05,910
the main areas where this is happening

01:08:01,420 --> 01:08:09,990
it's it's kind of almost scary with with

01:08:05,910 --> 01:08:09,990
what what's being done with these things

01:08:11,430 --> 01:08:14,950
there's also fix to fix that's also

01:08:14,380 --> 01:08:16,480
quite cute

01:08:14,950 --> 01:08:21,130
basically what you do is you do a doodle

01:08:16,480 --> 01:08:22,839
and it makes cuts out of it again this

01:08:21,130 --> 01:08:25,660
is not an image search this is literally

01:08:22,839 --> 01:08:31,660
that image is generated based on what's

01:08:25,660 --> 01:08:37,060
been drawn here just search for edges to

01:08:31,660 --> 01:08:38,679
cuts and it sounds fun but you'll

01:08:37,060 --> 01:08:40,480
actually see that even some web games

01:08:38,679 --> 01:08:42,520
these days they have things where you

01:08:40,480 --> 01:08:44,529
can basically draw a line and that's so

01:08:42,520 --> 01:08:46,239
like previously we had lemmings where

01:08:44,529 --> 01:08:48,250
you like click and basically go like

01:08:46,239 --> 01:08:49,870
okay walk along this bridge or you know

01:08:48,250 --> 01:08:52,089
do this and do that now you can

01:08:49,870 --> 01:08:56,429
literally draw a line and and you the

01:08:52,089 --> 01:09:01,350
characters kind of walk along this any

01:08:56,429 --> 01:09:04,089
cartoon faces who's got a cartoon avatar

01:09:01,350 --> 01:09:05,370
there you go that's actually again kind

01:09:04,089 --> 01:09:09,160
of action

01:09:05,370 --> 01:09:15,160
so yeah you feed it a photo and you get

01:09:09,160 --> 01:09:18,310
back this is actually interesting cycle

01:09:15,160 --> 01:09:20,410
gun this was kind of remarkable the

01:09:18,310 --> 01:09:26,620
reason this was remarkable was that see

01:09:20,410 --> 01:09:28,239
I do have a video for this no I don't so

01:09:26,620 --> 01:09:31,060
what it was given was basically a lot of

01:09:28,239 --> 01:09:35,469
pictures of horses and zebras

01:09:31,060 --> 01:09:38,949
it was then given a video of the horse

01:09:35,469 --> 01:09:42,160
running around a field and it generated

01:09:38,949 --> 01:09:45,880
a video of a zebra running around a

01:09:42,160 --> 01:09:49,799
field this is actually so good let's see

01:09:45,880 --> 01:09:49,799
if I can actually find it on YouTube

01:09:51,060 --> 01:09:55,109
yeah I mean just Cuba these days

01:10:00,820 --> 01:10:16,630
yep so that that right one is generated

01:10:05,920 --> 01:10:17,590
by computer that's some weird on

01:10:16,630 --> 01:10:24,790
YouTube anyway

01:10:17,590 --> 01:10:28,300
and yeah right one practical use case

01:10:24,790 --> 01:10:30,760
for this is actually in generating

01:10:28,300 --> 01:10:33,850
training data so this is actually used

01:10:30,760 --> 01:10:36,100
for a lot of gesture control type things

01:10:33,850 --> 01:10:38,650
to identify software that identifies

01:10:36,100 --> 01:10:39,970
where you're looking okay now it's

01:10:38,650 --> 01:10:42,340
incredibly wasteful to actually have

01:10:39,970 --> 01:10:45,220
humans look that way that way that way

01:10:42,340 --> 01:10:46,780
that way yeah and capturing images and

01:10:45,220 --> 01:10:50,260
use that as training data so what it

01:10:46,780 --> 01:10:51,730
does is basically you have a you capture

01:10:50,260 --> 01:10:54,160
some of that data and you capture a lot

01:10:51,730 --> 01:10:58,150
of people and you let the can generate

01:10:54,160 --> 01:10:59,740
the remainder of your training data so

01:10:58,150 --> 01:11:02,800
that another machine can train itself on

01:10:59,740 --> 01:11:06,730
recognizing how where you're looking how

01:11:02,800 --> 01:11:09,670
do you trying the generative the correct

01:11:06,730 --> 01:11:11,350
generation of dice well it just goes and

01:11:09,670 --> 01:11:18,550
generates right eventually becomes

01:11:11,350 --> 01:11:20,110
correct just they do so instead of a

01:11:18,550 --> 01:11:21,910
human trying to figure out when the

01:11:20,110 --> 01:11:24,070
models good there's another neural

01:11:21,910 --> 01:11:25,930
network who's yeah but if I start with a

01:11:24,070 --> 01:11:27,610
million samples right and I feed that

01:11:25,930 --> 01:11:30,040
into a generative set and i get

01:11:27,610 --> 01:11:31,870
eventually ten millions samples those

01:11:30,040 --> 01:11:34,000
extra nine million samples are

01:11:31,870 --> 01:11:36,070
derivative of my first set of what makes

01:11:34,000 --> 01:11:41,200
it any better to train the discriminator

01:11:36,070 --> 01:11:44,470
based off the ten data the generator

01:11:41,200 --> 01:11:46,540
starts off with random fuzz so sorry

01:11:44,470 --> 01:11:49,000
training the discriminator from the ten

01:11:46,540 --> 01:11:52,600
million rather than just a while since i

01:11:49,000 --> 01:11:54,730
was generated from the one no so if you

01:11:52,600 --> 01:11:55,590
look at the generator it doesn't have

01:11:54,730 --> 01:11:59,860
real data as input

01:11:55,590 --> 01:12:03,310
none at all not at all or at least not

01:11:59,860 --> 01:12:05,320
the set is being tested at least not the

01:12:03,310 --> 01:12:08,290
set is being trained on you could give

01:12:05,320 --> 01:12:10,480
it some other data but sorry not the set

01:12:08,290 --> 01:12:14,140
that is being discriminated on that

01:12:10,480 --> 01:12:14,770
sounds weird not the input data for the

01:12:14,140 --> 01:12:20,890
discrimination

01:12:14,770 --> 01:12:23,860
that's never shown two together yes so

01:12:20,890 --> 01:12:26,680
the feedback it gets is yep this is I

01:12:23,860 --> 01:12:30,989
think this is real news I think this is

01:12:26,680 --> 01:12:30,989
fake news that's the only single day

01:12:37,469 --> 01:12:40,469
right

01:12:45,120 --> 01:12:50,470
imitation learning is that's another use

01:12:48,310 --> 01:12:54,760
case it's basically like where systems

01:12:50,470 --> 01:12:57,370
try to mimic other systems you can see

01:12:54,760 --> 01:12:58,750
how cans can be useful that way particle

01:12:57,370 --> 01:13:00,550
acceleration is also kind of a

01:12:58,750 --> 01:13:03,610
interesting application so it's really

01:13:00,550 --> 01:13:05,680
they use Gans to generate have you guys

01:13:03,610 --> 01:13:10,030
ever heard of vertical Monte Carlo

01:13:05,680 --> 01:13:11,830
simulations basically you just sort of

01:13:10,030 --> 01:13:14,170
random chitin and see what sticks kind

01:13:11,830 --> 01:13:16,570
of okay

01:13:14,170 --> 01:13:19,210
particle accelerators kind of if you

01:13:16,570 --> 01:13:20,710
were to run the experiments for each and

01:13:19,210 --> 01:13:22,480
every one they'd become primitively

01:13:20,710 --> 01:13:26,140
expensive so what they do is they use

01:13:22,480 --> 01:13:27,550
the models to figure out areas that are

01:13:26,140 --> 01:13:30,270
of promise and then narrow down their

01:13:27,550 --> 01:13:36,400
areas of research and gammas are helping

01:13:30,270 --> 01:13:40,290
good luck CAPTCHAs and how CAPTCHAs will

01:13:36,400 --> 01:13:44,800
capture breakers they're also kind of

01:13:40,290 --> 01:13:50,460
based on Kansas these days and this was

01:13:44,800 --> 01:13:55,960
actually our last experiment for that

01:13:50,460 --> 01:13:58,720
particular course the idea was that we

01:13:55,960 --> 01:14:01,660
generated again and we gave it a bunch

01:13:58,720 --> 01:14:04,210
of human images well first digits and

01:14:01,660 --> 01:14:07,120
whatnot but the end of day towards the

01:14:04,210 --> 01:14:09,310
end we gave it a bunch of human images

01:14:07,120 --> 01:14:15,790
to train on I don't know if we're

01:14:09,310 --> 01:14:17,200
showing them no we're not okay and over

01:14:15,790 --> 01:14:19,780
many many iterations

01:14:17,200 --> 01:14:22,960
you can see that the face is so this is

01:14:19,780 --> 01:14:27,070
the output of the generator right from

01:14:22,960 --> 01:14:29,080
random noise effectively almost and then

01:14:27,070 --> 01:14:31,150
gradually it gets worse it gets worse

01:14:29,080 --> 01:14:34,330
and gets worse and then it's starting to

01:14:31,150 --> 01:14:38,410
get better better better better better

01:14:34,330 --> 01:14:40,620
and it starts resembling some human

01:14:38,410 --> 01:14:40,620
faces

01:14:47,239 --> 01:14:53,570
not high resolution HD but I mean it's a

01:14:49,699 --> 01:14:54,949
what 32 by 32 emits so to do this

01:14:53,570 --> 01:14:56,599
there's somebody basically so it

01:14:54,949 --> 01:14:58,519
generates noise emission you go that's

01:14:56,599 --> 01:14:59,659
not really a cubic face then generate

01:14:58,519 --> 01:15:02,150
something else it looks a little bit

01:14:59,659 --> 01:15:04,610
more light so you said yes yes yes no no

01:15:02,150 --> 01:15:06,260
no and you're just sending it along a

01:15:04,610 --> 01:15:20,959
path where you eventually is creating

01:15:06,260 --> 01:15:25,280
stuff like that okay what's our

01:15:20,959 --> 01:15:29,749
traditions what I'm just so so so

01:15:25,280 --> 01:15:32,300
remember so one reason for starting from

01:15:29,749 --> 01:15:33,800
say random noise is that it has no bias

01:15:32,300 --> 01:15:36,559
towards what a human looks like it

01:15:33,800 --> 01:15:38,269
doesn't know is generating humans right

01:15:36,559 --> 01:15:40,610
the fact that is generating humans is

01:15:38,269 --> 01:15:45,110
because the discriminator was told that

01:15:40,610 --> 01:15:46,789
the target looks like this as long as

01:15:45,110 --> 01:15:48,530
vaguely similar to any one of those

01:15:46,789 --> 01:16:00,800
discourage is gonna go like yeah you're

01:15:48,530 --> 01:16:02,360
making improvements what input you're

01:16:00,800 --> 01:16:05,559
feeding to this community if you start

01:16:02,360 --> 01:16:09,340
feeding input with humans with two noses

01:16:05,559 --> 01:16:13,010
to the discriminator then it will say

01:16:09,340 --> 01:16:14,749
yeah I mean you'll you'll see the

01:16:13,010 --> 01:16:17,869
previously I got worse before it got

01:16:14,749 --> 01:16:24,889
better and certain hydrations is gonna

01:16:17,869 --> 01:16:26,780
try other things and informed yes this

01:16:24,889 --> 01:16:30,530
is better than what you had before yeah

01:16:26,780 --> 01:16:33,860
well this is worse at what point will it

01:16:30,530 --> 01:16:35,030
then be considering its outputs I think

01:16:33,860 --> 01:16:37,159
so so towards the end of it there's a

01:16:35,030 --> 01:16:40,429
lot of them which we would recognize as

01:16:37,159 --> 01:16:42,800
facial in in the pyramids right yeah but

01:16:40,429 --> 01:16:44,900
the first two or three were nothing like

01:16:42,800 --> 01:16:47,630
that and yet I would still have told it

01:16:44,900 --> 01:16:48,800
this is more like a human so at what

01:16:47,630 --> 01:16:50,449
point will it look back on its history

01:16:48,800 --> 01:16:52,309
and you'll actually know that got me

01:16:50,449 --> 01:16:54,289
where I am so that man bulk of what

01:16:52,309 --> 01:16:56,179
produces him but actually that wasn't

01:16:54,289 --> 01:17:00,500
too cute

01:16:56,179 --> 01:17:02,510
so how does it I guess a closer or

01:17:00,500 --> 01:17:04,100
further away and then finally yes and no

01:17:02,510 --> 01:17:15,710
is there a distinction between that kind

01:17:04,100 --> 01:17:17,870
of feedback well you start off with

01:17:15,710 --> 01:17:19,159
something that's just fuzz so I say no

01:17:17,870 --> 01:17:21,050
that's not human

01:17:19,159 --> 01:17:22,820
the next randomly generated thing has

01:17:21,050 --> 01:17:25,040
buzzed with a greater density where a

01:17:22,820 --> 01:17:27,969
face or a nose might be in the middle

01:17:25,040 --> 01:17:30,710
and whilst I say yes that is better and

01:17:27,969 --> 01:17:32,150
I'm improving the algorithm from this

01:17:30,710 --> 01:17:34,730
point onwards going down that that's

01:17:32,150 --> 01:17:37,760
what a path it's not it shouldn't be

01:17:34,730 --> 01:17:39,350
used as an example of actual success if

01:17:37,760 --> 01:17:43,330
that makes any sense so it is it's

01:17:39,350 --> 01:17:45,770
better but it is not right and that's

01:17:43,330 --> 01:17:47,989
one of the things community is not

01:17:45,770 --> 01:17:51,199
static it's also improving at the same

01:17:47,989 --> 01:17:53,900
time right so for example you could

01:17:51,199 --> 01:17:55,550
start feeding it more training data as

01:17:53,900 --> 01:17:58,580
things go on and get things more

01:17:55,550 --> 01:18:00,230
accurate and one so the storage itself

01:17:58,580 --> 01:18:03,020
has to be evaluated as well right

01:18:00,230 --> 01:18:05,239
normally they evaluate each other as an

01:18:03,020 --> 01:18:06,679
if if the disre mate to see is that too

01:18:05,239 --> 01:18:08,360
many things are being okay

01:18:06,679 --> 01:18:12,140
then a notice that it needs to make

01:18:08,360 --> 01:18:16,550
itself you know more stringent in terms

01:18:12,140 --> 01:18:17,989
of quality control and at that time the

01:18:16,550 --> 01:18:20,960
generator starts failing again and it

01:18:17,989 --> 01:18:23,179
makes itself better so they don't let

01:18:20,960 --> 01:18:30,010
each other have a have an easy time of

01:18:23,179 --> 01:18:32,870
it well if they settle on an equilibrium

01:18:30,010 --> 01:18:35,750
then what does that mean does that mean

01:18:32,870 --> 01:18:40,429
that a lot of things being generated a

01:18:35,750 --> 01:18:44,360
being passed or failed or that said I

01:18:40,429 --> 01:18:46,040
mean Ganzer X so in terms of

01:18:44,360 --> 01:18:48,290
computational research Ganzer it's to

01:18:46,040 --> 01:18:49,670
mean you like within the last four or

01:18:48,290 --> 01:18:51,710
five years this is like open research

01:18:49,670 --> 01:18:52,790
right now okay and a lot of these real

01:18:51,710 --> 01:18:55,010
applications that we're talking about

01:18:52,790 --> 01:18:56,180
come from games so they're incredibly

01:18:55,010 --> 01:18:59,470
useful

01:18:56,180 --> 01:19:10,150
their feedback from the Ganges from Mesa

01:18:59,470 --> 01:19:12,200
this romance is yeah that's real I guess

01:19:10,150 --> 01:19:16,390
that's question free and good fella

01:19:12,200 --> 01:19:20,750
he's the guy who invented scholar cat

01:19:16,390 --> 01:19:27,680
story come from two very eyes each other

01:19:20,750 --> 01:19:32,210
and I don't know if they were using guns

01:19:27,680 --> 01:19:42,410
but could be knows we're far away from

01:19:32,210 --> 01:19:44,030
what's the Skynet's I mean the state of

01:19:42,410 --> 01:19:47,530
the Arts in terms of artificial

01:19:44,030 --> 01:19:49,400
intelligence is basically maths it's not

01:19:47,530 --> 01:19:51,410
consciousness or anything else yeah

01:19:49,400 --> 01:19:56,000
right libraries tensorflow you have

01:19:51,410 --> 01:19:58,790
learned chaos at that time yes now high

01:19:56,000 --> 01:20:00,920
torch is very interesting the reason why

01:19:58,790 --> 01:20:03,950
torch is interesting is because well

01:20:00,920 --> 01:20:05,540
tazza flow is somewhat session-based

01:20:03,950 --> 01:20:09,590
and the black box once it starts running

01:20:05,540 --> 01:20:13,340
a high torch tries to be more pythonic

01:20:09,590 --> 01:20:16,220
and tries to help you in terms of what a

01:20:13,340 --> 01:20:17,840
gold adapt the network as things are

01:20:16,220 --> 01:20:20,330
being trained okay high torches they're

01:20:17,840 --> 01:20:24,460
kind of friendly a programming model the

01:20:20,330 --> 01:20:28,010
problem was before pi torch I was like

01:20:24,460 --> 01:20:29,420
it was good to experiment with but then

01:20:28,010 --> 01:20:31,610
you'd have to take whatever model you've

01:20:29,420 --> 01:20:33,860
created and convert it into chaos or

01:20:31,610 --> 01:20:36,920
something that was more low-level for

01:20:33,860 --> 01:20:39,440
execution in production with patrasche

01:20:36,920 --> 01:20:40,880
1.0 what they've actually done is they

01:20:39,440 --> 01:20:43,790
wrapped the second phase so you can

01:20:40,880 --> 01:20:45,710
basically take your nice Python code and

01:20:43,790 --> 01:20:48,650
just run that in production which is

01:20:45,710 --> 01:20:50,780
incredibly powerful to do they were

01:20:48,650 --> 01:20:54,410
supposed to release patrols 1.0 at the

01:20:50,780 --> 01:20:59,600
end of summer there is a high-touch

01:20:54,410 --> 01:21:00,920
conference on the 2nd of october so no

01:20:59,600 --> 01:21:03,650
definite they haven't announced anything

01:21:00,920 --> 01:21:06,230
yet but the good money could be on them

01:21:03,650 --> 01:21:08,000
releasing it on the 2nd of october the

01:21:06,230 --> 01:21:10,010
pied roach is worth looking into as well

01:21:08,000 --> 01:21:12,920
as Kevlar

01:21:10,010 --> 01:21:15,079
I don't really code tensorflow low-level

01:21:12,920 --> 01:21:18,050
I'm typically used TF learn some people

01:21:15,079 --> 01:21:19,909
use chaos but simply because the amount

01:21:18,050 --> 01:21:21,199
of work you have to do for tensorflow is

01:21:19,909 --> 01:21:25,909
incredible whereas if you have to learn

01:21:21,199 --> 01:21:27,940
makes it simpler there's a kind of

01:21:25,909 --> 01:21:31,940
debugging tool called tensor boards

01:21:27,940 --> 01:21:34,099
where you can basically when your things

01:21:31,940 --> 01:21:36,889
are executing you can go in and see how

01:21:34,099 --> 01:21:38,420
things are performing a complaint of

01:21:36,889 --> 01:21:41,119
neural networks is that their black

01:21:38,420 --> 01:21:44,210
boxes and they're very hard to reason

01:21:41,119 --> 01:21:45,559
about even with channel board they're

01:21:44,210 --> 01:21:48,320
still hard to reason about but at least

01:21:45,559 --> 01:21:51,579
they you can see if certain weights have

01:21:48,320 --> 01:21:53,599
got stuck or not and that sort of stuff

01:21:51,579 --> 01:21:56,409
so if you're using sense of learn

01:21:53,599 --> 01:21:58,489
support this is worth looking into

01:21:56,409 --> 01:22:02,599
implications where is this going as I

01:21:58,489 --> 01:22:04,519
said this is not Skynet by any means the

01:22:02,599 --> 01:22:07,280
bigger concern is AI taking over jobs

01:22:04,519 --> 01:22:08,960
and everything else realistically

01:22:07,280 --> 01:22:10,429
probably creating more jobs as well as

01:22:08,960 --> 01:22:16,730
getting rid of all jobs

01:22:10,429 --> 01:22:19,579
I mean jobs evolve how many people code

01:22:16,730 --> 01:22:23,340
ASP don't know 1.1 how many people could

01:22:19,579 --> 01:22:29,310
asp.net 1.1 now man

01:22:23,340 --> 01:22:31,619
I can avoid it well London banks but

01:22:29,310 --> 01:22:33,989
really you'll see a lot of kind of jobs

01:22:31,619 --> 01:22:35,520
that humans do will actually go away but

01:22:33,989 --> 01:22:38,400
at the same time a lot of jobs will be

01:22:35,520 --> 01:22:39,840
created because well nobody knew true I

01:22:38,400 --> 01:22:42,330
mean and then today these things are

01:22:39,840 --> 01:22:44,489
built by humans not like machines

01:22:42,330 --> 01:22:49,520
themselves when that happens then when

01:22:44,489 --> 01:22:49,520
Skynet might be a you know right

01:22:50,960 --> 01:22:55,180

YouTube URL: https://www.youtube.com/watch?v=akA3OG9ths4


