Title: Diving into Elasticsearch with .NET - Dot Net South West - October 2017
Publication date: 2018-02-02
Playlist: Dot Net South West
Description: 
	Elasticsearch is a real-time, distributed, scalable and highly available search and analytics engine used by thousands of companies worldwide for a multitude of use cases, including search, data exploration, analytics and security information and event management.

In this talk, we'll get up and running with Elasticsearch from a .NET perspective, exploring its search and analytics capabilities through the Stackoverflow and .NET Core telemetry data sets. Hopefully you'll find the answers you're looking for and come away with new ways to splice and dice your own data.

About Russ Cam:

Russ Cam is a Software Engineer for Elastic, the company behind Elasticsearch and the Elastic Stack, a suite of open source projects focused on scalability and ease-of-use that help you make sense of your data. Within Elastic, he's part of the Microsoft team that looks after all things Microsoft related including the Elasticsearch .NET clients, Windows and Azure integrations.
He has a keen interest in distributed systems, cloud computing and software design, patterns and practices and enjoys contributing to open source projects, particularly when he has an itch to scratch!

_____________________________________________________________

Pusher is a hosted service with APIs, developer tools and open source libraries that greatly simplify integrating real-time functionality into web and mobile applications. 

Pusher will automatically scale when required, removing all the pain of setting up and maintaining a secure, real-time infrastructure. 

Pusher is already trusted to do so by thousands of developers and companies like GitHub, MailChimp, the Financial Times, Buffer and many more. 

Getting started takes just a few seconds: simply go to pusher.com and create a free account. Happy hacking!
Captions: 
	00:00:00,000 --> 00:00:03,600
a little bit about myself my name is

00:00:01,530 --> 00:00:06,029
Russ cam I'm originally from here but

00:00:03,600 --> 00:00:08,700
now I live in Sydney and just over here

00:00:06,029 --> 00:00:11,400
on holidays and thought I'd sign up and

00:00:08,700 --> 00:00:13,019
succumb to her to a talk because I don't

00:00:11,400 --> 00:00:14,820
think anyone's been down this this area

00:00:13,019 --> 00:00:18,240
so you know I thought I'd come and talk

00:00:14,820 --> 00:00:20,670
to you about elasticsearch with.net so

00:00:18,240 --> 00:00:22,730
the agenda for tonight really is to give

00:00:20,670 --> 00:00:25,439
you an introduction to the elastic stack

00:00:22,730 --> 00:00:27,300
get up and running with the.net clients

00:00:25,439 --> 00:00:29,010
you know how do you connect how to use

00:00:27,300 --> 00:00:31,800
it what's the kind of idioms and

00:00:29,010 --> 00:00:33,090
conventions that are in the client then

00:00:31,800 --> 00:00:35,820
have a little look at indexing and

00:00:33,090 --> 00:00:38,460
mapping search and analysis which is

00:00:35,820 --> 00:00:40,710
kind of really that the takeaway bit I

00:00:38,460 --> 00:00:42,390
would say of this particular talk this

00:00:40,710 --> 00:00:45,300
talk is kind of geared more around

00:00:42,390 --> 00:00:48,450
search use cases than other particular

00:00:45,300 --> 00:00:49,800
use cases of elastic search and then

00:00:48,450 --> 00:00:51,840
finally having a little look at

00:00:49,800 --> 00:00:56,370
aggregation sir so being able to group

00:00:51,840 --> 00:00:58,770
group and summarize data so who's

00:00:56,370 --> 00:01:02,609
actually using elastic search here so

00:00:58,770 --> 00:01:04,049
far okay who's played with it okay a few

00:01:02,609 --> 00:01:08,790
people have played with who's using it

00:01:04,049 --> 00:01:12,869
in anger who's using it in anger with

00:01:08,790 --> 00:01:18,000
the.net client okay I will avoid those

00:01:12,869 --> 00:01:20,520
guys so yeah a little bit of the history

00:01:18,000 --> 00:01:24,119
of elastic search so for a long long

00:01:20,520 --> 00:01:25,759
time elastic search has been used with

00:01:24,119 --> 00:01:28,340
Cabana which is like the the

00:01:25,759 --> 00:01:30,180
visualization the web basically the web

00:01:28,340 --> 00:01:32,520
application you can build dashboards

00:01:30,180 --> 00:01:34,229
based on the data and elastic search and

00:01:32,520 --> 00:01:35,850
then it's also been used with logstash

00:01:34,229 --> 00:01:37,979
as well and logstash

00:01:35,850 --> 00:01:39,960
name is a little bit of a misnomer but

00:01:37,979 --> 00:01:42,180
it's essentially aware of ingesting data

00:01:39,960 --> 00:01:44,100
into elastic search and those three

00:01:42,180 --> 00:01:48,420
things together have been known for a

00:01:44,100 --> 00:01:50,340
long time as the ELQ stack so little

00:01:48,420 --> 00:01:52,649
overt or little yeah about two years ago

00:01:50,340 --> 00:01:54,450
two and a half years ago we actually

00:01:52,649 --> 00:01:56,579
added another product to the stack his

00:01:54,450 --> 00:01:58,890
were called beats and I'll go into what

00:01:56,579 --> 00:02:00,509
those are in a second and then we were

00:01:58,890 --> 00:02:03,360
kind of thinking well how does beats fit

00:02:00,509 --> 00:02:06,840
into ELQ you know is it Claire is it

00:02:03,360 --> 00:02:08,670
Belk is it ELQ B so we couldn't really

00:02:06,840 --> 00:02:12,480
sort of come up with a good name for it

00:02:08,670 --> 00:02:13,830
so Sacchi the elastic stack so up the

00:02:12,480 --> 00:02:15,720
top there we have

00:02:13,830 --> 00:02:18,060
Cabana which is basically your window

00:02:15,720 --> 00:02:20,630
into the data and elasticsearch allows

00:02:18,060 --> 00:02:23,730
you to build visualizations dashboards

00:02:20,630 --> 00:02:26,220
run out hot queries have other plugins

00:02:23,730 --> 00:02:28,110
in there for doing machine learning and

00:02:26,220 --> 00:02:29,820
then at the heart of the stack is

00:02:28,110 --> 00:02:31,890
elasticsearch which is the search

00:02:29,820 --> 00:02:33,750
analytics engine that's where you store

00:02:31,890 --> 00:02:36,120
your data that's we query all your data

00:02:33,750 --> 00:02:36,690
then on the ingestion side we have

00:02:36,120 --> 00:02:40,350
logstash

00:02:36,690 --> 00:02:43,020
which is like an i-best describe it as

00:02:40,350 --> 00:02:45,750
an ETL pipelining tool so you'll run it

00:02:43,020 --> 00:02:47,670
centrally somewhere you're able to set a

00:02:45,750 --> 00:02:50,610
bunch of different inputs so like file

00:02:47,670 --> 00:02:53,610
inputs inputs from RabbitMQ inputs from

00:02:50,610 --> 00:02:56,190
Kafka all of the you know the well used

00:02:53,610 --> 00:02:58,590
ones have inputs for them then you can

00:02:56,190 --> 00:03:01,560
define a bunch of filters that basically

00:02:58,590 --> 00:03:03,660
say how you you modify that data so it

00:03:01,560 --> 00:03:05,910
could be things like enriching data if

00:03:03,660 --> 00:03:09,090
you're you're ingesting IP addresses it

00:03:05,910 --> 00:03:11,190
could be getting the GOI geo data for

00:03:09,090 --> 00:03:12,690
those IP addresses and then finally

00:03:11,190 --> 00:03:14,400
there's outputs and one of those you can

00:03:12,690 --> 00:03:16,020
send to elasticsearch you can send it

00:03:14,400 --> 00:03:18,450
back too far you can send it's a single

00:03:16,020 --> 00:03:22,020
server you know are numerous different

00:03:18,450 --> 00:03:23,670
places where beats fits into this so

00:03:22,020 --> 00:03:27,270
logstash you'd kind of run centrally

00:03:23,670 --> 00:03:29,340
beats are very much designed to do one

00:03:27,270 --> 00:03:31,560
thing and do it well so there's a family

00:03:29,340 --> 00:03:33,720
of beats and basically those sit on the

00:03:31,560 --> 00:03:35,610
edge servers they sit on the edge nodes

00:03:33,720 --> 00:03:37,080
and they do one particular thing it

00:03:35,610 --> 00:03:39,150
could for example be monitoring a

00:03:37,080 --> 00:03:41,070
directory and telling their files that

00:03:39,150 --> 00:03:42,870
come into there and then just sending

00:03:41,070 --> 00:03:45,180
those over to log stash or sending them

00:03:42,870 --> 00:03:48,000
to elasticsearch so there's ones for a

00:03:45,180 --> 00:03:51,000
far beat metric beat for getting Linux

00:03:48,000 --> 00:03:54,030
metrics like Windows event logs there's

00:03:51,000 --> 00:03:56,430
a whole whole whole family of them on

00:03:54,030 --> 00:03:59,160
the commercial side so these are all

00:03:56,430 --> 00:04:01,590
open source you can take those you can

00:03:59,160 --> 00:04:03,110
run them they're all under Apache 2 you

00:04:01,590 --> 00:04:06,060
can do whatever you like with them a

00:04:03,110 --> 00:04:08,100
subsidy fine on the commercial side and

00:04:06,060 --> 00:04:10,530
basically as a company how we kind of

00:04:08,100 --> 00:04:13,050
make some money is that we have these

00:04:10,530 --> 00:04:15,209
x-pac offerings now basically all they

00:04:13,050 --> 00:04:17,400
do is they extend the open-source

00:04:15,209 --> 00:04:20,370
version there's no freemium premium

00:04:17,400 --> 00:04:22,350
model here these extend the open-source

00:04:20,370 --> 00:04:24,990
offering so they provide additional

00:04:22,350 --> 00:04:27,280
things such as security orthocenter

00:04:24,990 --> 00:04:29,550
keishon LDAP authentication

00:04:27,280 --> 00:04:31,870
user access control role-based security

00:04:29,550 --> 00:04:34,810
document level security field level

00:04:31,870 --> 00:04:36,610
security alerting so being able to lurk

00:04:34,810 --> 00:04:38,620
when data's coming in when certain

00:04:36,610 --> 00:04:40,689
thresholds are may be here send a

00:04:38,620 --> 00:04:41,639
message the slack send a message to

00:04:40,689 --> 00:04:44,620
HipChat

00:04:41,639 --> 00:04:47,319
send an email put it into another lock

00:04:44,620 --> 00:04:49,030
index in elasticsearch send it to a log

00:04:47,319 --> 00:04:51,490
bar that kind of stuff

00:04:49,030 --> 00:04:54,009
monitoring being able to monitor the

00:04:51,490 --> 00:04:55,810
entire stack so metrics coming out of

00:04:54,009 --> 00:04:58,090
Cabana metrics coming out of

00:04:55,810 --> 00:05:01,240
elasticsearch so what does the jvm heat

00:04:58,090 --> 00:05:03,069
look like what does that the CPU look

00:05:01,240 --> 00:05:04,960
like on there what's the search rate

00:05:03,069 --> 00:05:07,060
like what's the indexing rate light and

00:05:04,960 --> 00:05:09,479
getting those kind of metrics then a

00:05:07,060 --> 00:05:12,129
couple of newer ones here as well graph

00:05:09,479 --> 00:05:13,990
being able to understand and infer

00:05:12,129 --> 00:05:16,090
relationships between documents in

00:05:13,990 --> 00:05:18,819
elasticsearch it's really really cool

00:05:16,090 --> 00:05:20,590
that's one of my favorites if I've got

00:05:18,819 --> 00:05:23,229
time I can probably give a downer of it

00:05:20,590 --> 00:05:24,909
if not grab me afterwards and we can go

00:05:23,229 --> 00:05:27,219
through that and then finally machine

00:05:24,909 --> 00:05:28,990
learning so this this element here isn't

00:05:27,219 --> 00:05:32,860
the big wide world of machine learning

00:05:28,990 --> 00:05:34,240
this is very focused one specific use

00:05:32,860 --> 00:05:37,120
case for machine learning right now

00:05:34,240 --> 00:05:40,419
which is unsupervised machine learning

00:05:37,120 --> 00:05:42,129
for anomaly detection essentially so a

00:05:40,419 --> 00:05:44,379
lot of people use elastic search for a

00:05:42,129 --> 00:05:45,969
time series data and they want to

00:05:44,379 --> 00:05:48,000
understand anomalies within that data

00:05:45,969 --> 00:05:50,740
that's where machine learning comes in

00:05:48,000 --> 00:05:53,740
all of that stuff we offer on a

00:05:50,740 --> 00:05:56,020
software-as-a-service on elastic cloud

00:05:53,740 --> 00:05:58,539
that runs on AWS

00:05:56,020 --> 00:06:02,020
it runs on GCP it doesn't run on Azure

00:05:58,539 --> 00:06:04,629
yet unfortunately but that no doubt that

00:06:02,020 --> 00:06:06,550
will be coming and then the second thing

00:06:04,629 --> 00:06:10,300
for elastic cloud is we actually offer

00:06:06,550 --> 00:06:11,860
elastic cloud enterprise which is the

00:06:10,300 --> 00:06:15,370
software that we use to run elastic

00:06:11,860 --> 00:06:17,409
cloud on on AWS except you can take that

00:06:15,370 --> 00:06:19,839
and run it on your own private servers

00:06:17,409 --> 00:06:21,699
so you can spin up clusters using the

00:06:19,839 --> 00:06:23,650
same orchestration that we use for

00:06:21,699 --> 00:06:25,839
elastic cloud and use that internally

00:06:23,650 --> 00:06:28,180
and that's I'm not a massive fan of the

00:06:25,839 --> 00:06:30,069
name unless that cloud enterprise but

00:06:28,180 --> 00:06:32,439
it's kind of geared more towards that

00:06:30,069 --> 00:06:34,930
the big corporations that are spinning

00:06:32,439 --> 00:06:37,529
up you know hundreds of clusters which

00:06:34,930 --> 00:06:37,529
does happen

00:06:37,650 --> 00:06:41,639
so what is elasticsearch so probably

00:06:40,110 --> 00:06:44,280
about half of you had already used it

00:06:41,639 --> 00:06:47,510
before which is pretty cool

00:06:44,280 --> 00:06:49,830
but essentially elasticsearch is a

00:06:47,510 --> 00:06:51,960
distributed search analytics engine

00:06:49,830 --> 00:06:54,330
that's pretty much in a nutshell but

00:06:51,960 --> 00:06:56,190
what does that actually mean so there's

00:06:54,330 --> 00:06:57,660
a few things to it and we can kind of

00:06:56,190 --> 00:07:00,090
break them down into these three areas

00:06:57,660 --> 00:07:02,820
so it's distributed and scalable it's

00:07:00,090 --> 00:07:04,860
resilient so there are mechanisms in

00:07:02,820 --> 00:07:07,380
there to ensure that the cluster may

00:07:04,860 --> 00:07:10,949
remains operable when certain scenarios

00:07:07,380 --> 00:07:12,960
happen it's highly available so in the

00:07:10,949 --> 00:07:15,150
event that one of those nodes within the

00:07:12,960 --> 00:07:17,220
cluster actually goes down you still

00:07:15,150 --> 00:07:19,590
have all of your data it still operates

00:07:17,220 --> 00:07:24,090
in some capacity and you haven't lost

00:07:19,590 --> 00:07:26,460
anything you ability to do multi-tenancy

00:07:24,090 --> 00:07:28,380
so an elastic search you index documents

00:07:26,460 --> 00:07:30,360
into lastik search they go into an index

00:07:28,380 --> 00:07:32,610
you can have multiple indices you can

00:07:30,360 --> 00:07:34,199
search across multiple indices you can

00:07:32,610 --> 00:07:36,090
search across multiple types although

00:07:34,199 --> 00:07:37,680
types are going to be going away in the

00:07:36,090 --> 00:07:40,500
next version so I won't say much about

00:07:37,680 --> 00:07:42,690
those but you have the ability to

00:07:40,500 --> 00:07:45,300
multi-talent there and particularly with

00:07:42,690 --> 00:07:47,310
security as well you can also ensure

00:07:45,300 --> 00:07:51,120
that people only have access to the data

00:07:47,310 --> 00:07:53,550
that they should and it's able to deal

00:07:51,120 --> 00:07:56,520
with structured and unstructured data so

00:07:53,550 --> 00:07:59,370
some people prefer to use it in the in

00:07:56,520 --> 00:08:01,710
the fashion of a unified logging format

00:07:59,370 --> 00:08:03,750
so unifying what metrics look like what

00:08:01,710 --> 00:08:06,389
log files look like to say from Apache

00:08:03,750 --> 00:08:08,400
and unifying those into a common format

00:08:06,389 --> 00:08:10,410
and then indexing those into a lastik

00:08:08,400 --> 00:08:12,990
search but you don't necessarily have to

00:08:10,410 --> 00:08:15,000
do that if you're indexing data into

00:08:12,990 --> 00:08:17,639
indices and elastic search you can still

00:08:15,000 --> 00:08:19,320
search across those across common fields

00:08:17,639 --> 00:08:23,280
across different fields targeted at

00:08:19,320 --> 00:08:26,699
specific indices it's very developer

00:08:23,280 --> 00:08:29,699
friendly so this is how I originally got

00:08:26,699 --> 00:08:31,949
interested in elastic search was I think

00:08:29,699 --> 00:08:33,630
it was around 2012 I started playing

00:08:31,949 --> 00:08:36,450
with it just for a bit of fun bit of

00:08:33,630 --> 00:08:38,520
research and just thought it was really

00:08:36,450 --> 00:08:41,430
cool and really interesting and was

00:08:38,520 --> 00:08:43,289
amazed by the speed of it and just kind

00:08:41,430 --> 00:08:45,000
of went from there so one of the things

00:08:43,289 --> 00:08:48,450
that makes it very easy to get started

00:08:45,000 --> 00:08:50,120
with is people often refer to ASCII

00:08:48,450 --> 00:08:54,060
Milus

00:08:50,120 --> 00:08:57,720
not quite correct I prefer to refer to

00:08:54,060 --> 00:08:59,519
as an inferred or implied schema so the

00:08:57,720 --> 00:09:01,860
first document that you index into

00:08:59,519 --> 00:09:04,050
elasticsearch if you haven't explicitly

00:09:01,860 --> 00:09:05,790
told elasticsearch about the mappings

00:09:04,050 --> 00:09:07,949
for those documents about the schema for

00:09:05,790 --> 00:09:12,209
them it will infer the schema from that

00:09:07,949 --> 00:09:14,249
first document it sees which is useful

00:09:12,209 --> 00:09:16,110
if particularly if you're ingesting new

00:09:14,249 --> 00:09:19,110
data that can be a really useful feature

00:09:16,110 --> 00:09:21,660
for search use cases not so much and

00:09:19,110 --> 00:09:23,759
we'll see why a little bit later it's a

00:09:21,660 --> 00:09:27,329
juice and rest api essentially I think

00:09:23,759 --> 00:09:30,059
there's now a hundred and one hundred

00:09:27,329 --> 00:09:33,600
and sixty odd api's I believe and so

00:09:30,059 --> 00:09:36,600
everything is over HTTP JSON based so

00:09:33,600 --> 00:09:39,870
JSON documents indexing those JSON back

00:09:36,600 --> 00:09:41,999
and this client libraries for pretty

00:09:39,870 --> 00:09:44,879
much every single language you can think

00:09:41,999 --> 00:09:48,089
of so a dotnet as well there's actually

00:09:44,879 --> 00:09:51,329
two for dotnet but yeah all of that the

00:09:48,089 --> 00:09:53,459
well-known language is that you know and

00:09:51,329 --> 00:09:55,949
it's open source as I mentioned before

00:09:53,459 --> 00:09:58,709
so you can take their stein load it

00:09:55,949 --> 00:10:01,889
start playing with it be impressed like

00:09:58,709 --> 00:10:04,680
I was and yeah put it to some good use

00:10:01,889 --> 00:10:07,740
cases the other awesome thing about it

00:10:04,680 --> 00:10:10,050
is not only is it able to scale so we

00:10:07,740 --> 00:10:15,300
have customers running very very large

00:10:10,050 --> 00:10:16,559
clusters I was on a call with a customer

00:10:15,300 --> 00:10:20,279
recently and they had a four hundred

00:10:16,559 --> 00:10:22,889
node cluster and you have heard of

00:10:20,279 --> 00:10:24,990
bigger ones out there in the field but

00:10:22,889 --> 00:10:27,839
it's still very much near real time for

00:10:24,990 --> 00:10:29,850
search across all of that data which and

00:10:27,839 --> 00:10:31,559
when I say near real time I'm talking

00:10:29,850 --> 00:10:33,029
usually under a second

00:10:31,559 --> 00:10:35,160
like I'm a four hundred node cluster

00:10:33,029 --> 00:10:37,740
under a second result you get results

00:10:35,160 --> 00:10:41,699
back which is is pretty pretty

00:10:37,740 --> 00:10:44,009
impressive able to do for text search so

00:10:41,699 --> 00:10:47,009
feed them back into the the unstructured

00:10:44,009 --> 00:10:49,740
search side of things so able to perform

00:10:47,009 --> 00:10:53,040
searches across unstructured blobs of

00:10:49,740 --> 00:10:54,980
text essentially canned or pseudo

00:10:53,040 --> 00:10:57,779
aggregations so you can combine the two

00:10:54,980 --> 00:10:59,939
it's very useful for e-commerce sites

00:10:57,779 --> 00:11:01,679
for example where you need to perform a

00:10:59,939 --> 00:11:03,620
search and show results but also show

00:11:01,679 --> 00:11:06,800
some facets on the side there

00:11:03,620 --> 00:11:09,110
so for other like categories say of data

00:11:06,800 --> 00:11:11,390
that's coming back and is able to do

00:11:09,110 --> 00:11:13,430
geospatial data as well so you can

00:11:11,390 --> 00:11:15,500
combine geospatial with full text with

00:11:13,430 --> 00:11:18,130
structured search and combine all of

00:11:15,500 --> 00:11:21,770
those together for a very comprehensive

00:11:18,130 --> 00:11:25,190
search strategy and obviously it's

00:11:21,770 --> 00:11:27,560
multilingual here as well so able to

00:11:25,190 --> 00:11:28,670
index various different languages and

00:11:27,560 --> 00:11:30,260
there are other some some other

00:11:28,670 --> 00:11:32,480
interesting things you can do around

00:11:30,260 --> 00:11:34,640
indexing different languages and using

00:11:32,480 --> 00:11:38,180
it to classify new languages as well

00:11:34,640 --> 00:11:39,770
so given this document I have no idea

00:11:38,180 --> 00:11:42,530
what language it's written in being able

00:11:39,770 --> 00:11:43,970
to index it and using one of the another

00:11:42,530 --> 00:11:45,590
feature in elasticsearch which I won't

00:11:43,970 --> 00:11:47,600
talk about tonight but using percolation

00:11:45,590 --> 00:11:50,570
you can get it to identify that kind

00:11:47,600 --> 00:11:53,450
what language that document is and then

00:11:50,570 --> 00:11:55,580
maybe tag it when you send it in that's

00:11:53,450 --> 00:11:59,480
kind of what it is a very very brief

00:11:55,580 --> 00:12:01,940
overview of elasticsearch so you run it

00:11:59,480 --> 00:12:04,550
in a cluster and a cluster consists of a

00:12:01,940 --> 00:12:06,350
bunch of elastic search nodes so here in

00:12:04,550 --> 00:12:09,740
my example here we have a three node

00:12:06,350 --> 00:12:11,300
cluster and when we index into that

00:12:09,740 --> 00:12:13,730
cluster we're going to be creating an

00:12:11,300 --> 00:12:16,550
index and for this particular index I've

00:12:13,730 --> 00:12:19,010
said I would like you to create three

00:12:16,550 --> 00:12:20,960
shards for this in for this particular

00:12:19,010 --> 00:12:23,290
index so all of the documents are going

00:12:20,960 --> 00:12:26,480
to be split across these three shards

00:12:23,290 --> 00:12:29,060
elastic search will assign a shot to

00:12:26,480 --> 00:12:32,980
each of the individual nodes here so the

00:12:29,060 --> 00:12:36,260
P here is for a primary primary shot and

00:12:32,980 --> 00:12:38,420
I can also then set in the settings to

00:12:36,260 --> 00:12:40,790
say I also want a replica shot so for

00:12:38,420 --> 00:12:41,960
each primary shot here we can also have

00:12:40,790 --> 00:12:44,030
a replica shot

00:12:41,960 --> 00:12:45,830
so as documents are coming in to elastic

00:12:44,030 --> 00:12:47,540
search and I'm saying go into this index

00:12:45,830 --> 00:12:49,910
they're going to go into one of these

00:12:47,540 --> 00:12:52,310
primary shards elastic search is going

00:12:49,910 --> 00:12:54,410
to replicate that document over to the

00:12:52,310 --> 00:12:56,480
corresponding replica elastic search

00:12:54,410 --> 00:12:59,090
also balances where those shot where

00:12:56,480 --> 00:13:01,130
those replicas go in in comparison to

00:12:59,090 --> 00:13:03,050
the primary shards so it will never

00:13:01,130 --> 00:13:06,980
assign a replica shard on to a node that

00:13:03,050 --> 00:13:09,320
has the same primary shard for obvious

00:13:06,980 --> 00:13:12,590
reasons which will be explained now so

00:13:09,320 --> 00:13:14,750
if we were to lose know to what what we

00:13:12,590 --> 00:13:17,000
would end up finding is that this one

00:13:14,750 --> 00:13:19,149
replica shard here will end up becoming

00:13:17,000 --> 00:13:21,740
being promoted to a primary chard and

00:13:19,149 --> 00:13:23,000
the cloak the cluster still remains

00:13:21,740 --> 00:13:25,370
operable because we still have a

00:13:23,000 --> 00:13:27,709
complete collection of shards there and

00:13:25,370 --> 00:13:29,810
then once if that other node is was then

00:13:27,709 --> 00:13:33,319
to come back afterwards elasticsearch

00:13:29,810 --> 00:13:35,750
will automatically assign shards onto it

00:13:33,319 --> 00:13:38,120
and you know you would have your your

00:13:35,750 --> 00:13:40,670
redundancy back again there as well so

00:13:38,120 --> 00:13:44,329
that's kind of the way it operates it's

00:13:40,670 --> 00:13:46,730
a primary backup replication model which

00:13:44,329 --> 00:13:49,209
is very well known in distributed

00:13:46,730 --> 00:13:49,209
systems

00:13:50,079 --> 00:13:55,220
so why elasticsearch and why not simple

00:13:53,120 --> 00:13:57,439
so let's just sort of take a little

00:13:55,220 --> 00:13:59,389
journey here and see if we can sort of

00:13:57,439 --> 00:14:02,149
understand why elasticsearch is a good

00:13:59,389 --> 00:14:04,399
fit and sequel is great for certain

00:14:02,149 --> 00:14:07,100
stuff but maybe not for research in

00:14:04,399 --> 00:14:09,279
analytics use cases so we want to search

00:14:07,100 --> 00:14:11,870
in sequel and we're looking for Meeta

00:14:09,279 --> 00:14:14,389
happening in bristol tonight about

00:14:11,870 --> 00:14:17,720
elastic searching net so how would we

00:14:14,389 --> 00:14:20,000
typically go and look for those or how

00:14:17,720 --> 00:14:22,759
would we look for things around how to

00:14:20,000 --> 00:14:24,470
use elastic search with dotnet so we

00:14:22,759 --> 00:14:26,480
might sort of form a sequel query like

00:14:24,470 --> 00:14:28,040
this and they're like yes you know we

00:14:26,480 --> 00:14:30,290
get the one result back we're looking

00:14:28,040 --> 00:14:31,759
for but we may have missed the loads and

00:14:30,290 --> 00:14:35,500
loads of documents that are related to

00:14:31,759 --> 00:14:38,420
dotnet related to elastic search and

00:14:35,500 --> 00:14:41,360
related to other things to do with the

00:14:38,420 --> 00:14:43,160
elastic stack one particular problem

00:14:41,360 --> 00:14:45,559
that we see here is that our particular

00:14:43,160 --> 00:14:48,230
database here is case-sensitive as well

00:14:45,559 --> 00:14:50,300
so we're like okay we can fix that by

00:14:48,230 --> 00:14:52,819
just lowering the title and then

00:14:50,300 --> 00:14:54,709
comparing against the lower case the

00:14:52,819 --> 00:14:57,009
other problem we have here now is that

00:14:54,709 --> 00:14:59,779
this is only ever going to match

00:14:57,009 --> 00:15:01,399
questions here that that you know

00:14:59,779 --> 00:15:03,920
correspond to how do I use elastic

00:15:01,399 --> 00:15:05,990
search we don't know exactly so we've

00:15:03,920 --> 00:15:08,870
heard of this other thing in in sequel

00:15:05,990 --> 00:15:10,970
is like queries so we suddenly change it

00:15:08,870 --> 00:15:12,920
to like query and we're now looking for

00:15:10,970 --> 00:15:15,290
things that contain elastic search word

00:15:12,920 --> 00:15:17,240
net we're thinking okay we're doing

00:15:15,290 --> 00:15:19,040
pretty well here you know we're getting

00:15:17,240 --> 00:15:21,559
back more results than we thought we

00:15:19,040 --> 00:15:23,629
thought we would but the other problem

00:15:21,559 --> 00:15:25,189
here we see now is that we're only

00:15:23,629 --> 00:15:27,529
looking for things that contain exactly

00:15:25,189 --> 00:15:30,529
elastic search word net so we're

00:15:27,529 --> 00:15:32,329
thinking yeah you haven't got me yeah I

00:15:30,529 --> 00:15:34,759
can still sort this out so we split it

00:15:32,329 --> 00:15:37,279
up and we say we're lower equals like

00:15:34,759 --> 00:15:39,350
elasticsearch in Laura's like.net and

00:15:37,279 --> 00:15:41,209
now you know we're getting back more

00:15:39,350 --> 00:15:44,629
more and more and we're feeling kind of

00:15:41,209 --> 00:15:46,040
happy happy with it and the boss comes

00:15:44,629 --> 00:15:47,600
to us and says well actually I'm only

00:15:46,040 --> 00:15:49,910
really interested in the things in the

00:15:47,600 --> 00:15:51,740
last three months thinking okay well

00:15:49,910 --> 00:15:54,079
that's this easily solvable you know

00:15:51,740 --> 00:15:59,779
where creation date was between you know

00:15:54,079 --> 00:16:00,920
1st of july and 1st of october so we're

00:15:59,779 --> 00:16:02,990
lonely what things are the last few

00:16:00,920 --> 00:16:04,339
months what you know we don't only want

00:16:02,990 --> 00:16:07,129
things in the last three months we want

00:16:04,339 --> 00:16:09,470
things overall time but if it's happened

00:16:07,129 --> 00:16:12,110
in the last three months I want it to be

00:16:09,470 --> 00:16:14,029
more relevant than the next thing so

00:16:12,110 --> 00:16:15,620
kind of scratching our heads a little

00:16:14,029 --> 00:16:18,769
bit and we're thinking well how the hell

00:16:15,620 --> 00:16:20,870
are we going to solve that we sort of

00:16:18,769 --> 00:16:23,870
come up with this concept of ordering

00:16:20,870 --> 00:16:25,370
buy and preferring more recent but we we

00:16:23,870 --> 00:16:28,009
sort of start writing the user-defined

00:16:25,370 --> 00:16:29,749
function for that we go sort of halfway

00:16:28,009 --> 00:16:32,420
through and the boss comes up comes up

00:16:29,749 --> 00:16:35,120
to us again and says like you know like

00:16:32,420 --> 00:16:37,129
things more recent but also like those

00:16:35,120 --> 00:16:40,100
questions that have a higher score to

00:16:37,129 --> 00:16:42,889
also be more relevant as well so we sort

00:16:40,100 --> 00:16:45,019
of combine that with our order by idea

00:16:42,889 --> 00:16:47,779
and we're really sort of scratching our

00:16:45,019 --> 00:16:50,120
heads here and you know in the end we

00:16:47,779 --> 00:16:52,309
just eventually you know we realized

00:16:50,120 --> 00:16:55,100
that you know sequel is really not there

00:16:52,309 --> 00:16:57,290
it's not the tool for this for

00:16:55,100 --> 00:16:59,420
performing this job it will get you some

00:16:57,290 --> 00:17:02,779
of the way there as soon as you need to

00:16:59,420 --> 00:17:05,720
do anything more complex sequel is not

00:17:02,779 --> 00:17:08,480
that all databases are good for database

00:17:05,720 --> 00:17:11,089
problems elastic search is good for

00:17:08,480 --> 00:17:15,709
search and analytics problems and we'll

00:17:11,089 --> 00:17:17,179
see why that is the case shortly that's

00:17:15,709 --> 00:17:19,370
kind of an overview of elastic search

00:17:17,179 --> 00:17:21,169
any questions I should have said feel

00:17:19,370 --> 00:17:24,829
free to just you know raise your hand

00:17:21,169 --> 00:17:26,600
shout out whatever that's fine I try not

00:17:24,829 --> 00:17:28,820
to go down the implied knowledge route

00:17:26,600 --> 00:17:32,020
so if anything I saying doesn't make

00:17:28,820 --> 00:17:35,450
sense then just shout and I'll try and

00:17:32,020 --> 00:17:37,909
reword it in a way that does so yeah

00:17:35,450 --> 00:17:40,490
elastic search with net we have two

00:17:37,909 --> 00:17:42,980
clients so if elastic search net is the

00:17:40,490 --> 00:17:44,510
low-level client its dependency free has

00:17:42,980 --> 00:17:46,430
no dependencies on other new

00:17:44,510 --> 00:17:50,000
packages which makes a lot of people

00:17:46,430 --> 00:17:53,350
happy there has all of the elasticsearch

00:17:50,000 --> 00:17:56,020
api is a mapped in the.net client and

00:17:53,350 --> 00:17:59,780
you deal with something called post data

00:17:56,020 --> 00:18:02,840
post data people some people love it

00:17:59,780 --> 00:18:04,910
some people hate it it's a general type

00:18:02,840 --> 00:18:07,760
that is able to handle a bunch of

00:18:04,910 --> 00:18:11,690
different primitives for you being able

00:18:07,760 --> 00:18:13,580
to index a lot in interact with

00:18:11,690 --> 00:18:15,980
elasticsearch so for example you can use

00:18:13,580 --> 00:18:18,590
strings json strings you can use

00:18:15,980 --> 00:18:22,460
anonymous types you can use byte arrays

00:18:18,590 --> 00:18:24,890
you can use streams with post data so

00:18:22,460 --> 00:18:26,930
the intention with the low-level client

00:18:24,890 --> 00:18:29,840
is they're basically not getting your

00:18:26,930 --> 00:18:32,660
way just to give you the bare-bones API

00:18:29,840 --> 00:18:35,330
implementation for you to work with then

00:18:32,660 --> 00:18:37,820
we have nest and nest is the high level

00:18:35,330 --> 00:18:40,040
client and it represents all of the

00:18:37,820 --> 00:18:43,100
requests and responses to elasticsearch

00:18:40,040 --> 00:18:45,200
as strong types so easy discoverability

00:18:43,100 --> 00:18:48,140
there hopefully if there's documentation

00:18:45,200 --> 00:18:49,160
there for it and it has a dependency on

00:18:48,140 --> 00:18:51,410
JSON net

00:18:49,160 --> 00:18:54,740
so whereas elasticsearch net has its own

00:18:51,410 --> 00:18:57,950
small internal JSON serializer this one

00:18:54,740 --> 00:19:01,280
requires JSON net which is both good and

00:18:57,950 --> 00:19:03,020
bad it's good because it's familiar to a

00:19:01,280 --> 00:19:04,730
lot of people it's bad because obviously

00:19:03,020 --> 00:19:06,410
sometimes it can clash with other

00:19:04,730 --> 00:19:09,860
people's dependencies if they're not on

00:19:06,410 --> 00:19:12,650
the same versions and it has a bunch of

00:19:09,860 --> 00:19:16,040
advanced features as well so it has the

00:19:12,650 --> 00:19:17,270
concept of covariant search results so

00:19:16,040 --> 00:19:21,260
if you're searching across multiple

00:19:17,270 --> 00:19:23,840
indices and you have a representation

00:19:21,260 --> 00:19:26,690
representation that maps to a particular

00:19:23,840 --> 00:19:28,550
Poco particular c-sharp OCO you have and

00:19:26,690 --> 00:19:30,320
you're searching across multiple indices

00:19:28,550 --> 00:19:32,710
that have those different pocos in them

00:19:30,320 --> 00:19:35,480
you can still return a collection of

00:19:32,710 --> 00:19:37,400
covariant results there obviously you

00:19:35,480 --> 00:19:38,900
need some kind of relationship to them

00:19:37,400 --> 00:19:41,450
some lowest-common-denominator

00:19:38,900 --> 00:19:43,760
it may be object you know like returning

00:19:41,450 --> 00:19:45,950
a list of object or it may be you know

00:19:43,760 --> 00:19:47,600
some interface but the actual types in

00:19:45,950 --> 00:19:50,120
there will be deserialized to the types

00:19:47,600 --> 00:19:52,460
that you tell us to so we can handle

00:19:50,120 --> 00:19:54,980
covariant search results it does other

00:19:52,460 --> 00:19:57,380
things such as retries and connection

00:19:54,980 --> 00:19:58,279
fell overs so if you're running against

00:19:57,380 --> 00:19:59,929
a particular astok

00:19:58,279 --> 00:20:02,330
search cluster and you seed it with the

00:19:59,929 --> 00:20:04,759
three IP addresses for the nodes that

00:20:02,330 --> 00:20:06,950
are in your cluster it will fall over

00:20:04,759 --> 00:20:08,929
to those nodes and round-robin requests

00:20:06,950 --> 00:20:11,179
over those nodes for you and it does

00:20:08,929 --> 00:20:12,619
other things such as sniffing the state

00:20:11,179 --> 00:20:15,739
of the cluster so if you have a cluster

00:20:12,619 --> 00:20:17,570
that's say grown to six nodes now it's

00:20:15,739 --> 00:20:19,340
able to sniff that bring back the

00:20:17,570 --> 00:20:22,639
details of those six nodes and then run

00:20:19,340 --> 00:20:25,639
against those six nodes then so there's

00:20:22,639 --> 00:20:28,849
quite a fair bit of smarts in there that

00:20:25,639 --> 00:20:30,830
you don't need to write yourself so

00:20:28,849 --> 00:20:34,099
before we get started with search crews

00:20:30,830 --> 00:20:37,099
just kind of jump out of here and jump

00:20:34,099 --> 00:20:38,719
in into some examples instead so this

00:20:37,099 --> 00:20:40,969
particular one is looking at the stack

00:20:38,719 --> 00:20:44,809
overflow data which I would love to show

00:20:40,969 --> 00:20:52,700
you if it was on that screen one second

00:20:44,809 --> 00:20:54,889
let's just duplicate this hey okay is

00:20:52,700 --> 00:20:57,320
that big enough at the back can you you

00:20:54,889 --> 00:20:58,909
read that yep okay so yeah we're just

00:20:57,320 --> 00:21:01,940
dealing with the stack overflow data

00:20:58,909 --> 00:21:04,070
here so if any if no one's looked at it

00:21:01,940 --> 00:21:06,409
before you basically have the concept of

00:21:04,070 --> 00:21:08,690
a post type which has some general

00:21:06,409 --> 00:21:11,419
fields and then you have a question type

00:21:08,690 --> 00:21:15,440
and you have an answer type which both

00:21:11,419 --> 00:21:18,409
derive from a post field so question has

00:21:15,440 --> 00:21:19,999
a certain fields such as title body of

00:21:18,409 --> 00:21:23,059
the question well which is common to

00:21:19,999 --> 00:21:25,039
both questions are answers an answer is

00:21:23,059 --> 00:21:27,080
related to a question as well

00:21:25,039 --> 00:21:29,210
which is what the parent ID here

00:21:27,080 --> 00:21:31,339
represents so these are the these are

00:21:29,210 --> 00:21:34,909
the types that we're dealing with and

00:21:31,339 --> 00:21:37,399
we've mapped so if we were to get

00:21:34,909 --> 00:21:39,529
started with the low-level client what

00:21:37,399 --> 00:21:43,009
we can do here is just send in a search

00:21:39,529 --> 00:21:44,749
request here to the posts index so we're

00:21:43,009 --> 00:21:46,429
targeting the post index and we're

00:21:44,749 --> 00:21:48,169
targeting the question type in there

00:21:46,429 --> 00:21:51,469
because in this particular index we have

00:21:48,169 --> 00:21:53,269
both questions and answers and we can

00:21:51,469 --> 00:21:55,789
send in this anonymous type here and

00:21:53,269 --> 00:21:58,759
this anonymous type really just is the

00:21:55,789 --> 00:22:00,679
same representation for a query that you

00:21:58,759 --> 00:22:03,559
would have in JSON expressed as an

00:22:00,679 --> 00:22:05,599
anomalous type and what we can do is we

00:22:03,559 --> 00:22:07,190
can run that and we probably won't get

00:22:05,599 --> 00:22:09,379
anything back because I don't think I've

00:22:07,190 --> 00:22:10,370
started elasticsearch yet and I haven't

00:22:09,379 --> 00:22:14,600
know so

00:22:10,370 --> 00:22:18,650
to start that as well so I have all of

00:22:14,600 --> 00:22:21,020
the stack overflow data since eternity

00:22:18,650 --> 00:22:21,740
since the start of time indexed here on

00:22:21,020 --> 00:22:24,200
my laptop

00:22:21,740 --> 00:22:27,290
hopefully I'll hold up I think it's

00:22:24,200 --> 00:22:30,170
about sixty gig of data 60 Giga

00:22:27,290 --> 00:22:33,620
questions and answers so let's run this

00:22:30,170 --> 00:22:36,050
one again see what comes back okay so we

00:22:33,620 --> 00:22:38,660
got some results back and we were just

00:22:36,050 --> 00:22:40,640
looking for questions that have in the

00:22:38,660 --> 00:22:43,850
body of the question this should never

00:22:40,640 --> 00:22:48,050
happen and what we can what we can see

00:22:43,850 --> 00:22:50,570
here is a massive wall of JSON ice

00:22:48,050 --> 00:22:53,270
obviously not very obviously not very

00:22:50,570 --> 00:22:58,309
readable for us so yeah that's that's

00:22:53,270 --> 00:23:02,690
brilliant Russ and so what we've

00:22:58,309 --> 00:23:04,910
specified here is this generic type here

00:23:02,690 --> 00:23:07,520
is the type that we expect to get back

00:23:04,910 --> 00:23:09,380
from elasticsearch so in on the response

00:23:07,520 --> 00:23:11,600
here we have some usual details around

00:23:09,380 --> 00:23:13,670
status codes whether there was an

00:23:11,600 --> 00:23:15,620
exception on the elasticsearch side but

00:23:13,670 --> 00:23:17,600
the body part here is essentially gonna

00:23:15,620 --> 00:23:19,309
be that string that we've asked for and

00:23:17,600 --> 00:23:22,190
then all I'm doing is just dumping it

00:23:19,309 --> 00:23:23,780
out to the to the result window there so

00:23:22,190 --> 00:23:26,030
that was a big wall of text that's not

00:23:23,780 --> 00:23:29,030
very useful so if we just skip forward

00:23:26,030 --> 00:23:31,040
to the next one and I'll show you a

00:23:29,030 --> 00:23:32,660
slightly more complicated example so

00:23:31,040 --> 00:23:34,910
we're still using the low-level client

00:23:32,660 --> 00:23:37,040
here except that rather than just

00:23:34,910 --> 00:23:39,140
dumping it out straight to the window

00:23:37,040 --> 00:23:41,240
what we're going to do is set up an

00:23:39,140 --> 00:23:43,460
action here that's going to be cooled

00:23:41,240 --> 00:23:45,230
whenever a request is made and a

00:23:43,460 --> 00:23:47,600
response is returned so when the

00:23:45,230 --> 00:23:49,490
response is returned this particular

00:23:47,600 --> 00:23:51,800
action delegate inside of here is going

00:23:49,490 --> 00:23:53,030
to run so if there's a request body here

00:23:51,800 --> 00:23:54,530
we're just going to write that out if

00:23:53,030 --> 00:23:56,270
there's a response there we're also

00:23:54,530 --> 00:23:57,830
going to write that out it just makes it

00:23:56,270 --> 00:23:59,570
a little bit easier to see exactly

00:23:57,830 --> 00:24:03,200
what's going on if we run the same

00:23:59,570 --> 00:24:05,750
request as before we can see now we have

00:24:03,200 --> 00:24:07,280
a bit more form to you know to the

00:24:05,750 --> 00:24:09,320
results we can it's bit easier to

00:24:07,280 --> 00:24:11,270
understand what's going on so this is

00:24:09,320 --> 00:24:13,760
the request that went through so as a

00:24:11,270 --> 00:24:15,470
post to the posts index to the question

00:24:13,760 --> 00:24:19,130
type there and hitting the underscore

00:24:15,470 --> 00:24:21,350
search API endpoint and these were the

00:24:19,130 --> 00:24:23,720
results that came back so it's a 133

00:24:21,350 --> 00:24:26,330
milliseconds it was a 200

00:24:23,720 --> 00:24:28,549
we can start to see a bit more other

00:24:26,330 --> 00:24:30,080
than that massive wall of text there you

00:24:28,549 --> 00:24:34,340
start to see a bit more structure to

00:24:30,080 --> 00:24:36,530
what comes back so we have here details

00:24:34,340 --> 00:24:38,530
around the shards that were hit in the

00:24:36,530 --> 00:24:41,480
cluster and then inside of the hits

00:24:38,530 --> 00:24:44,020
object here we have total number of

00:24:41,480 --> 00:24:47,750
documents that match that so there's 616

00:24:44,020 --> 00:24:49,640
documents let me bump that a bit a 616

00:24:47,750 --> 00:24:52,100
documents that match this would never

00:24:49,640 --> 00:24:55,190
happen in the body and the max score

00:24:52,100 --> 00:24:57,140
assigned to those is this and these are

00:24:55,190 --> 00:25:00,320
the first ten results inside of this

00:24:57,140 --> 00:25:03,289
hits so each object inside of here is

00:25:00,320 --> 00:25:05,480
what's known as hits metadata and in

00:25:03,289 --> 00:25:07,309
there you get the index that came from

00:25:05,480 --> 00:25:09,169
the type that it was the ID of the

00:25:07,309 --> 00:25:11,539
document the score given to it and

00:25:09,169 --> 00:25:14,210
inside of the source here is the

00:25:11,539 --> 00:25:16,669
original verbatim JSON document that was

00:25:14,210 --> 00:25:18,559
sent to elasticsearch so when you send a

00:25:16,669 --> 00:25:20,720
document to elasticsearch in index it

00:25:18,559 --> 00:25:23,539
you can say whether it should keep

00:25:20,720 --> 00:25:25,309
around the original source document so

00:25:23,539 --> 00:25:27,230
when it does index it you have a few

00:25:25,309 --> 00:25:29,570
different options you can say yes I want

00:25:27,230 --> 00:25:31,460
you to index it for search in which case

00:25:29,570 --> 00:25:32,900
it will go into a particular data

00:25:31,460 --> 00:25:35,570
structure which I'll talk about a bit

00:25:32,900 --> 00:25:37,700
later you can say whether it should

00:25:35,570 --> 00:25:39,770
store the original JSON document so that

00:25:37,700 --> 00:25:41,929
you can get that original document back

00:25:39,770 --> 00:25:43,429
or you can say do neither of those

00:25:41,929 --> 00:25:45,650
things in which case it makes elastic

00:25:43,429 --> 00:25:47,120
search completely useless but you're

00:25:45,650 --> 00:25:50,120
probably going to do one of those things

00:25:47,120 --> 00:25:52,400
now use usually for search case it cases

00:25:50,120 --> 00:25:55,669
it's useful to keep the source around as

00:25:52,400 --> 00:25:57,740
well if you ever need to re indents if

00:25:55,669 --> 00:26:00,140
you don't have the source there you're

00:25:57,740 --> 00:26:02,030
going from maybe some other source again

00:26:00,140 --> 00:26:04,520
so you usually want to keep it around

00:26:02,030 --> 00:26:08,539
unless you have a very good reason not

00:26:04,520 --> 00:26:10,429
to so that's a slightly more complicated

00:26:08,539 --> 00:26:13,370
example with the low-level client let's

00:26:10,429 --> 00:26:17,120
get going with the high-level client so

00:26:13,370 --> 00:26:19,789
just just jump back a second here

00:26:17,120 --> 00:26:22,280
so the low-level client we've passed it

00:26:19,789 --> 00:26:24,950
in some configuration settings here so

00:26:22,280 --> 00:26:28,070
we said I want you to pretty print the

00:26:24,950 --> 00:26:30,710
JSON coming in and coming out disable

00:26:28,070 --> 00:26:32,570
direct streaming which is essentially I

00:26:30,710 --> 00:26:34,039
don't want you to write straight to the

00:26:32,570 --> 00:26:35,780
request stream I don't want you to

00:26:34,039 --> 00:26:37,730
deserialize straight from the response

00:26:35,780 --> 00:26:38,120
stream I want you to buffer those two

00:26:37,730 --> 00:26:40,309
things

00:26:38,120 --> 00:26:42,169
so that they're made then available to

00:26:40,309 --> 00:26:43,909
my action delegate here so I can print

00:26:42,169 --> 00:26:45,470
them out if you decide not to do that

00:26:43,909 --> 00:26:47,299
you won't get any request or response

00:26:45,470 --> 00:26:49,850
bodies but you'll still get the stats

00:26:47,299 --> 00:26:52,010
around what happened in the request and

00:26:49,850 --> 00:26:55,190
we nude up the elastic a low-level

00:26:52,010 --> 00:26:57,620
client here so skipping forward to nest

00:26:55,190 --> 00:27:00,320
client here we're using a slightly

00:26:57,620 --> 00:27:02,210
different connection pool so as I

00:27:00,320 --> 00:27:04,549
mentioned earlier you could we concede

00:27:02,210 --> 00:27:05,929
it here with three nodes to say these

00:27:04,549 --> 00:27:07,730
four three nodes are running in our

00:27:05,929 --> 00:27:11,299
cluster these are our connection

00:27:07,730 --> 00:27:14,000
settings if no index is defined when I

00:27:11,299 --> 00:27:17,149
make a particular API call then use this

00:27:14,000 --> 00:27:19,279
index as the default index name if you

00:27:17,149 --> 00:27:21,620
see a user type post type question type

00:27:19,279 --> 00:27:23,840
answer type use these particular index

00:27:21,620 --> 00:27:25,640
names so that means we don't need to

00:27:23,840 --> 00:27:27,580
define them on every single request and

00:27:25,640 --> 00:27:29,659
response I sorry on every single request

00:27:27,580 --> 00:27:32,390
and there are other options here for

00:27:29,659 --> 00:27:34,669
basic auth global headers additional

00:27:32,390 --> 00:27:38,659
query string parameters this one down

00:27:34,669 --> 00:27:41,510
the bottom here determines how nest is

00:27:38,659 --> 00:27:45,620
going to serialize Poco property names

00:27:41,510 --> 00:27:47,690
so by default nest camel cases property

00:27:45,620 --> 00:27:49,940
names as it sends them to elasticsearch

00:27:47,690 --> 00:27:52,370
so JSON document they'll all be camel

00:27:49,940 --> 00:27:55,250
cased property names there but this

00:27:52,370 --> 00:27:57,679
particular option here on configuration

00:27:55,250 --> 00:28:00,230
settings you can change that if you want

00:27:57,679 --> 00:28:03,740
to and then we simply pass the settings

00:28:00,230 --> 00:28:05,390
to the client so we can do a similar

00:28:03,740 --> 00:28:07,490
thing as we did before with the

00:28:05,390 --> 00:28:09,740
low-level client we can set up an action

00:28:07,490 --> 00:28:11,480
delegate and catch it and now we can

00:28:09,740 --> 00:28:13,460
make a request here with the the

00:28:11,480 --> 00:28:15,649
high-level client so the way the

00:28:13,460 --> 00:28:18,169
high-level client works is there are

00:28:15,649 --> 00:28:21,230
actually two different api's here

00:28:18,169 --> 00:28:23,090
there's a fluent API for those that are

00:28:21,230 --> 00:28:25,840
happy with lambda expressions and

00:28:23,090 --> 00:28:29,809
heavily nested lambda expressions and

00:28:25,840 --> 00:28:31,450
then there are those for using more of a

00:28:29,809 --> 00:28:35,029
kind of object builder object

00:28:31,450 --> 00:28:36,679
initializer syntax as we call it so same

00:28:35,029 --> 00:28:38,929
query as before but now with the

00:28:36,679 --> 00:28:41,270
high-level client so we can see here we

00:28:38,929 --> 00:28:44,600
have member expressions here to get to

00:28:41,270 --> 00:28:45,440
the body of a question and this is our

00:28:44,600 --> 00:28:48,250
query here

00:28:45,440 --> 00:28:51,050
and we can dump that when I again and

00:28:48,250 --> 00:28:53,030
whoa what's happened here

00:28:51,050 --> 00:28:55,670
so index name is null for the given type

00:28:53,030 --> 00:28:57,620
and no default index is set so on this

00:28:55,670 --> 00:29:01,010
in our particular setup here we haven't

00:28:57,620 --> 00:29:02,540
defined what index this the client

00:29:01,010 --> 00:29:06,020
should go and look in for the question

00:29:02,540 --> 00:29:07,730
types so that's that's easily fixed for

00:29:06,020 --> 00:29:14,300
our particular request here we can just

00:29:07,730 --> 00:29:16,790
specify index posts and then run that

00:29:14,300 --> 00:29:18,620
one again or we could have setup a

00:29:16,790 --> 00:29:21,650
convention on our connection settings

00:29:18,620 --> 00:29:24,610
and have that have that work as well so

00:29:21,650 --> 00:29:27,770
all I've done here is just dump out the

00:29:24,610 --> 00:29:29,510
let's zoom in on those that's all I've

00:29:27,770 --> 00:29:31,640
done here is just dumped out the bodies

00:29:29,510 --> 00:29:40,790
of the questions here from Stack

00:29:31,640 --> 00:29:42,770
Overflow so jumping on oh yeah sorry

00:29:40,790 --> 00:29:44,330
yeah so that was with the lambda

00:29:42,770 --> 00:29:46,580
expressions this is where the object

00:29:44,330 --> 00:29:49,460
initialize the syntax so we can build a

00:29:46,580 --> 00:29:51,680
search request looking at question types

00:29:49,460 --> 00:29:54,200
and use the match phrase query which is

00:29:51,680 --> 00:29:56,330
what we were using before and we can

00:29:54,200 --> 00:29:57,650
simply then pass this search request to

00:29:56,330 --> 00:29:59,270
the client and this is going to give us

00:29:57,650 --> 00:30:02,270
exactly the same results as we saw

00:29:59,270 --> 00:30:06,980
before but no pretty table because I'm

00:30:02,270 --> 00:30:09,080
not building a table there so that's

00:30:06,980 --> 00:30:11,930
kind of how you get started research and

00:30:09,080 --> 00:30:14,780
I'll go through some of the capabilities

00:30:11,930 --> 00:30:16,910
in a little bit but you kind of need to

00:30:14,780 --> 00:30:19,880
get data into elasticsearch first right

00:30:16,910 --> 00:30:21,740
in order to be able to search it so

00:30:19,880 --> 00:30:25,160
elastic search has a whole set of cred a

00:30:21,740 --> 00:30:27,400
P is to be able to do that so give you

00:30:25,160 --> 00:30:30,290
an example here we have a question type

00:30:27,400 --> 00:30:32,600
and we can just index that question type

00:30:30,290 --> 00:30:37,910
into elastic search and what you'll see

00:30:32,600 --> 00:30:40,520
come back here is so just checking where

00:30:37,910 --> 00:30:42,800
the index exists and deleting it if it

00:30:40,520 --> 00:30:45,470
does so that that was these two cools

00:30:42,800 --> 00:30:47,300
here and then we're just putting this

00:30:45,470 --> 00:30:49,790
particular document into elastic search

00:30:47,300 --> 00:30:53,540
and we've got a 201 response back so 201

00:30:49,790 --> 00:30:55,880
created HTTP response and it gives us

00:30:53,540 --> 00:30:57,590
some details around exactly what

00:30:55,880 --> 00:30:59,690
happened there that's great for an

00:30:57,590 --> 00:31:01,610
individual document but usually we're

00:30:59,690 --> 00:31:04,909
talking about thousands and millions of

00:31:01,610 --> 00:31:07,309
documents so we actually have a ball

00:31:04,909 --> 00:31:10,220
kpi as well so we can send through a

00:31:07,309 --> 00:31:12,169
bunch of a collection of questions

00:31:10,220 --> 00:31:15,109
collection of answers all through room

00:31:12,169 --> 00:31:17,479
one request at a time so here we're just

00:31:15,109 --> 00:31:19,460
doing something very similar to what we

00:31:17,479 --> 00:31:21,470
were doing before except this time we're

00:31:19,460 --> 00:31:23,779
defining the index up front so we're

00:31:21,470 --> 00:31:25,669
creating the index we're saying inside

00:31:23,779 --> 00:31:27,529
of this index I want you to map a couple

00:31:25,669 --> 00:31:31,009
of types there's a reason for doing that

00:31:27,529 --> 00:31:33,200
here in our particular case you can see

00:31:31,009 --> 00:31:34,999
that where we have a collection of posts

00:31:33,200 --> 00:31:38,749
here with two questions and an answer

00:31:34,999 --> 00:31:41,210
this quiz answer here is related to this

00:31:38,749 --> 00:31:43,519
question here so we have a parent-child

00:31:41,210 --> 00:31:45,559
relationship there between the questions

00:31:43,519 --> 00:31:47,179
and the answers but we just want to

00:31:45,559 --> 00:31:50,149
index those all into the same index

00:31:47,179 --> 00:31:52,070
anyway so we need to we need to give

00:31:50,149 --> 00:31:55,340
elasticsearch a bit of information here

00:31:52,070 --> 00:31:56,989
we need to say that our answer type has

00:31:55,340 --> 00:31:58,869
a parent and the parent of an answer

00:31:56,989 --> 00:32:02,059
type is this question type here the

00:31:58,869 --> 00:32:04,940
automap here I will explain very very

00:32:02,059 --> 00:32:07,159
shortly but essentially we create our

00:32:04,940 --> 00:32:09,739
index using this default index name

00:32:07,159 --> 00:32:11,539
which is just a string and we're saying

00:32:09,739 --> 00:32:14,119
we want to map questions we want to map

00:32:11,539 --> 00:32:16,039
answers then we provide a bunch of posts

00:32:14,119 --> 00:32:19,460
which is our two questions and answer

00:32:16,039 --> 00:32:22,009
and then we call the bulk API so in our

00:32:19,460 --> 00:32:24,109
bulky API here I'm having to specify

00:32:22,009 --> 00:32:26,029
index all over the place here because I

00:32:24,109 --> 00:32:28,729
don't want to overwrite that 60 gig

00:32:26,029 --> 00:32:30,799
questions and answers' index by accident

00:32:28,729 --> 00:32:32,779
so I'm being very explicit around the

00:32:30,799 --> 00:32:36,320
index I'm targeting here just to

00:32:32,779 --> 00:32:38,840
demonstrate to you so in our bulk API

00:32:36,320 --> 00:32:41,570
call we we have a couple of extensions

00:32:38,840 --> 00:32:43,970
here for you to be able to say given

00:32:41,570 --> 00:32:45,799
each item in that collection how it

00:32:43,970 --> 00:32:48,950
should be mapped sorry how it should be

00:32:45,799 --> 00:32:52,279
indexed so we're just looking inside of

00:32:48,950 --> 00:32:54,619
this lambda expression here and we're

00:32:52,279 --> 00:32:57,169
just saying if it's an answer then use

00:32:54,619 --> 00:33:01,519
the parent ID to specify the parent for

00:32:57,169 --> 00:33:04,249
that at that document and then get the

00:33:01,519 --> 00:33:06,169
type of the document because that's

00:33:04,249 --> 00:33:08,119
going to be the type it it's represented

00:33:06,169 --> 00:33:10,039
as within the index which is going to

00:33:08,119 --> 00:33:13,039
have questions and answers and then just

00:33:10,039 --> 00:33:14,259
pass it the document here as well still

00:33:13,039 --> 00:33:17,630
with me so far

00:33:14,259 --> 00:33:19,940
yep cool okay

00:33:17,630 --> 00:33:21,530
so here is us defining our index with

00:33:19,940 --> 00:33:23,000
all of the mappings we're going to skip

00:33:21,530 --> 00:33:26,180
that for the moment because there's a

00:33:23,000 --> 00:33:29,420
lot of information there let's just move

00:33:26,180 --> 00:33:32,230
this to the side here is our bulk

00:33:29,420 --> 00:33:35,600
request though so the bulk requests are

00:33:32,230 --> 00:33:38,150
the line delimited JSON so it's

00:33:35,600 --> 00:33:40,160
basically new line delimited and that

00:33:38,150 --> 00:33:41,810
the layer of them is essentially the the

00:33:40,160 --> 00:33:43,400
bulk operation that you want to perform

00:33:41,810 --> 00:33:45,920
and then if there's a document

00:33:43,400 --> 00:33:48,440
associated with that bulk operation it

00:33:45,920 --> 00:33:51,200
will be on the next line so we can see

00:33:48,440 --> 00:33:53,270
here is a create and then all of this is

00:33:51,200 --> 00:33:54,950
on the next line there's another create

00:33:53,270 --> 00:33:57,860
operation here for the second question

00:33:54,950 --> 00:33:59,720
and then here for the answer we have

00:33:57,860 --> 00:34:02,240
another create operation where we

00:33:59,720 --> 00:34:04,700
specified the parent to be one which is

00:34:02,240 --> 00:34:07,550
this question here and then what we see

00:34:04,700 --> 00:34:10,370
we get back from the bulk API is we get

00:34:07,550 --> 00:34:12,919
back a a collection of items where each

00:34:10,370 --> 00:34:15,080
item in there represents each of the

00:34:12,919 --> 00:34:17,360
items that was in our bulk a bulk API

00:34:15,080 --> 00:34:21,919
call so we can see each one of these

00:34:17,360 --> 00:34:24,679
succeeded 201 201 201 so you can have in

00:34:21,919 --> 00:34:26,570
a bulk operation some operations may

00:34:24,679 --> 00:34:27,879
fail that doesn't mean the entire thing

00:34:26,570 --> 00:34:29,990
fails it just means that those

00:34:27,879 --> 00:34:36,320
individuals have failed and you might

00:34:29,990 --> 00:34:41,270
want to retry them so that's the bulk

00:34:36,320 --> 00:34:43,639
API and you would use that essentially

00:34:41,270 --> 00:34:45,560
to get all of your documents into lastic

00:34:43,639 --> 00:34:48,260
search if you were running purely with

00:34:45,560 --> 00:34:50,690
the.net client we have some helpers as

00:34:48,260 --> 00:34:54,230
well for being able to set up and I

00:34:50,690 --> 00:34:56,720
observable to say here's what I want you

00:34:54,230 --> 00:34:58,760
to do here's a lazy enumerated

00:34:56,720 --> 00:35:01,040
collection of documents I want you to

00:34:58,760 --> 00:35:04,790
index just go and do that thing and then

00:35:01,040 --> 00:35:06,800
just tell me each time you're grabbing a

00:35:04,790 --> 00:35:08,390
bunch of those documents and index in

00:35:06,800 --> 00:35:11,690
them and just just tell me when you've

00:35:08,390 --> 00:35:13,460
completed so some helpers there that

00:35:11,690 --> 00:35:16,490
we've built on top to be able to do that

00:35:13,460 --> 00:35:18,770
kind of stuff and also update documents

00:35:16,490 --> 00:35:23,030
of course you know it wouldn't be code

00:35:18,770 --> 00:35:25,670
if you couldn't update it'd be crude so

00:35:23,030 --> 00:35:27,830
yeah we can just index a question here

00:35:25,670 --> 00:35:30,110
and then we can actually just send in an

00:35:27,830 --> 00:35:31,250
anonymous type with the properties that

00:35:30,110 --> 00:35:34,190
we want to update

00:35:31,250 --> 00:35:36,380
and then we can call the update API here

00:35:34,190 --> 00:35:37,400
so we want to update a question type but

00:35:36,380 --> 00:35:40,190
we're only going to give you an

00:35:37,400 --> 00:35:41,960
anonymous type for the for the

00:35:40,190 --> 00:35:44,720
properties that we want to update and

00:35:41,960 --> 00:35:45,830
then we can go and grab the the document

00:35:44,720 --> 00:35:49,280
at the very end as well

00:35:45,830 --> 00:35:52,460
so if we're to run this this or a

00:35:49,280 --> 00:35:55,340
regular affair it's all pretty casual

00:35:52,460 --> 00:35:57,890
stuff pretty usual stuff but we can see

00:35:55,340 --> 00:36:01,490
at the end here our document has been

00:35:57,890 --> 00:36:09,290
updated okay let's get on to some fun

00:36:01,490 --> 00:36:13,460
stuff okay so onto mappings that was

00:36:09,290 --> 00:36:16,610
basically indexing bulk updating sorry

00:36:13,460 --> 00:36:18,890
bulk indexing and updating just get onto

00:36:16,610 --> 00:36:20,750
some fun stuff instead this is one of

00:36:18,890 --> 00:36:21,860
the things that trips many many people

00:36:20,750 --> 00:36:24,170
up so when you're dealing with

00:36:21,860 --> 00:36:26,570
elasticsearch it has this nature of

00:36:24,170 --> 00:36:28,820
being able to infer a schema from the

00:36:26,570 --> 00:36:30,830
first document it sees but for search

00:36:28,820 --> 00:36:31,660
use cases they're searching probably not

00:36:30,830 --> 00:36:34,730
what you want

00:36:31,660 --> 00:36:39,140
99 in fact I'd say probably all of the

00:36:34,730 --> 00:36:42,220
time you don't want that because the way

00:36:39,140 --> 00:36:45,170
in which the data is indexed is going to

00:36:42,220 --> 00:36:47,540
determine how you can search it and you

00:36:45,170 --> 00:36:49,700
know usually or you have a rough idea of

00:36:47,540 --> 00:36:51,500
how you want to search it so you want to

00:36:49,700 --> 00:36:53,570
be able to you want to index data in a

00:36:51,500 --> 00:36:55,730
particular way that satisfies your

00:36:53,570 --> 00:36:58,250
search needs and this is where mapping

00:36:55,730 --> 00:37:00,440
comes in so at the point of creating an

00:36:58,250 --> 00:37:03,560
index you can apply a mapping to an

00:37:00,440 --> 00:37:05,390
index or if the index is already created

00:37:03,560 --> 00:37:08,270
but hasn't seen any documents yet you

00:37:05,390 --> 00:37:10,750
can give a mapping for a document so in

00:37:08,270 --> 00:37:15,320
this particular case as I said earlier

00:37:10,750 --> 00:37:17,840
we can actually we can within the client

00:37:15,320 --> 00:37:19,970
instead of elasticsearch saying okay

00:37:17,840 --> 00:37:21,620
this looks like a string or map it as

00:37:19,970 --> 00:37:24,020
this particular thing this looks like a

00:37:21,620 --> 00:37:26,240
date our map is this particular thing we

00:37:24,020 --> 00:37:29,480
can actually from the client side based

00:37:26,240 --> 00:37:31,220
on the the structure of your poco say

00:37:29,480 --> 00:37:32,720
this thing should be this and this thing

00:37:31,220 --> 00:37:35,060
should be that and explicitly tell

00:37:32,720 --> 00:37:37,370
elasticsearch that that is the case and

00:37:35,060 --> 00:37:40,370
this is what this auto mapping function

00:37:37,370 --> 00:37:44,450
does here so running auto map on this on

00:37:40,370 --> 00:37:48,590
this question type here is going to

00:37:44,450 --> 00:37:51,590
do some certain things so for example it

00:37:48,590 --> 00:37:54,500
takes this title field here and it's

00:37:51,590 --> 00:37:56,570
going to map it as a text data type now

00:37:54,500 --> 00:37:59,090
a text data type is the one you use for

00:37:56,570 --> 00:38:00,920
performing full-text search that

00:37:59,090 --> 00:38:05,020
undergoes analysis which I'll talk about

00:38:00,920 --> 00:38:08,810
a bit but it also Maps this title type

00:38:05,020 --> 00:38:11,390
as a keyword data type now a keyword

00:38:08,810 --> 00:38:13,340
data type in elasticsearch is the one

00:38:11,390 --> 00:38:16,970
that you use when you want to index that

00:38:13,340 --> 00:38:19,190
piece of data verbatim exactly as as it

00:38:16,970 --> 00:38:21,830
exists in the document and that's really

00:38:19,190 --> 00:38:23,420
really useful for aggregations so say

00:38:21,830 --> 00:38:25,190
for example in a case of Stack Overflow

00:38:23,420 --> 00:38:27,230
you have questions that have a

00:38:25,190 --> 00:38:29,180
collection of tags on them and you then

00:38:27,230 --> 00:38:31,100
you want to aggregate on the tags to say

00:38:29,180 --> 00:38:32,750
okay how many questions are there with

00:38:31,100 --> 00:38:35,240
c-sharp what are all that what are the

00:38:32,750 --> 00:38:37,460
top five tags in elasticsearch you would

00:38:35,240 --> 00:38:39,110
probably index the tags as a keyword

00:38:37,460 --> 00:38:43,670
data types that you can then aggregate

00:38:39,110 --> 00:38:45,500
on them and we can just go down here and

00:38:43,670 --> 00:38:47,240
we can see the other choices that it's

00:38:45,500 --> 00:38:49,010
made here so we can see actually for all

00:38:47,240 --> 00:38:51,530
of the string properties that we have

00:38:49,010 --> 00:38:54,110
the client has sent through a mapping

00:38:51,530 --> 00:38:56,750
that map's it as a text data type but

00:38:54,110 --> 00:38:58,880
also map's it as a keyword data type

00:38:56,750 --> 00:39:01,040
this mapping only exists in

00:38:58,880 --> 00:39:03,170
elasticsearch obviously doesn't exist on

00:39:01,040 --> 00:39:05,720
the poco de poco he has the one property

00:39:03,170 --> 00:39:07,370
but elasticsearch has a notion of a it

00:39:05,720 --> 00:39:08,900
being mapped as a text data type and

00:39:07,370 --> 00:39:11,000
also as a keyword data type which means

00:39:08,900 --> 00:39:16,910
that we can query our query on it in

00:39:11,000 --> 00:39:18,230
those two ways as well so text data type

00:39:16,910 --> 00:39:21,530
is the one you'd use for full-text

00:39:18,230 --> 00:39:23,360
search and keyword data type is one that

00:39:21,530 --> 00:39:26,900
you would typically use for two things

00:39:23,360 --> 00:39:29,120
for aggregations and for sorting where

00:39:26,900 --> 00:39:31,940
you want to sort on the exact value as

00:39:29,120 --> 00:39:34,760
it exists because what happens is if you

00:39:31,940 --> 00:39:37,460
run aggregations on a text data type

00:39:34,760 --> 00:39:39,710
field on a full text mapping essentially

00:39:37,460 --> 00:39:41,990
or if you run sorting on there you're

00:39:39,710 --> 00:39:45,590
actually going to be sorting on the

00:39:41,990 --> 00:39:47,690
results of the analysis on the text so

00:39:45,590 --> 00:39:49,370
it could actually end up being split

00:39:47,690 --> 00:39:51,890
into multiple tokens and you're running

00:39:49,370 --> 00:39:54,980
analytics on multiple tokens rather than

00:39:51,890 --> 00:39:56,630
the individual you know its strengths

00:39:54,980 --> 00:39:59,030
essentially

00:39:56,630 --> 00:40:04,640
or that will become clear as mud in a

00:39:59,030 --> 00:40:07,460
bit so that's the sky auto-mapping that

00:40:04,640 --> 00:40:08,870
you have in the client there's actually

00:40:07,460 --> 00:40:11,120
a couple of other ways that you can map

00:40:08,870 --> 00:40:13,730
as well and usually people tend to use

00:40:11,120 --> 00:40:18,380
these things in conjunction there's a

00:40:13,730 --> 00:40:21,440
concept of attribute mapping so here we

00:40:18,380 --> 00:40:23,360
can say our user type should be a

00:40:21,440 --> 00:40:25,910
developer type with an elastic search

00:40:23,360 --> 00:40:29,000
and the ID property for a user should

00:40:25,910 --> 00:40:31,930
actually be the display name so nest has

00:40:29,000 --> 00:40:34,960
a convention and this is actually

00:40:31,930 --> 00:40:37,550
exclusive to the nest client only

00:40:34,960 --> 00:40:40,310
whereby if you're poco has an ID

00:40:37,550 --> 00:40:43,460
property on it the ID property will be

00:40:40,310 --> 00:40:47,060
used as the ID for the document so you

00:40:43,460 --> 00:40:49,190
can send through a poco poco without any

00:40:47,060 --> 00:40:51,440
ID property and elasticsearch will also

00:40:49,190 --> 00:40:54,290
generate an ID for it but if you have an

00:40:51,440 --> 00:40:56,540
ID field case-insensitive then I lost it

00:40:54,290 --> 00:40:58,940
then nest client will use that as the ID

00:40:56,540 --> 00:41:03,590
for the document which is really useful

00:40:58,940 --> 00:41:06,320
right so we have prop attributes here

00:41:03,590 --> 00:41:08,180
for all of the different types of data

00:41:06,320 --> 00:41:10,640
types that you can map things as so I

00:41:08,180 --> 00:41:13,670
can say here for website URL I want you

00:41:10,640 --> 00:41:15,980
to map you as a text a data type and use

00:41:13,670 --> 00:41:21,290
the whitespace analyzer and I talked

00:41:15,980 --> 00:41:23,420
about analyzers in a second so here and

00:41:21,290 --> 00:41:25,400
in this particular example for this user

00:41:23,420 --> 00:41:27,650
we've Auto maps so we've done what we

00:41:25,400 --> 00:41:30,260
did before which is basically say hey

00:41:27,650 --> 00:41:31,760
nest figure out what the mappings should

00:41:30,260 --> 00:41:34,460
be for each of the properties on this

00:41:31,760 --> 00:41:36,440
poco but then using properties here we

00:41:34,460 --> 00:41:38,630
can override any of those inferred

00:41:36,440 --> 00:41:41,630
mappings as well so we can say actually

00:41:38,630 --> 00:41:44,150
for this age field I want you to use a

00:41:41,630 --> 00:41:46,430
byte rather than an integer which is how

00:41:44,150 --> 00:41:48,560
it's represented on the poco and for

00:41:46,430 --> 00:41:51,260
this badge type which is actually a

00:41:48,560 --> 00:41:53,930
collection of badges where each badge

00:41:51,260 --> 00:41:56,240
looks like this I want you to map it as

00:41:53,930 --> 00:41:57,670
a nested type now there are two

00:41:56,240 --> 00:42:01,340
different ways in which you can map

00:41:57,670 --> 00:42:02,960
collections on pocos if you do nothing

00:42:01,340 --> 00:42:05,050
it will be mapped as what's called an

00:42:02,960 --> 00:42:09,440
object type and an object type

00:42:05,050 --> 00:42:10,160
essentially is you can query on

00:42:09,440 --> 00:42:13,070
individual

00:42:10,160 --> 00:42:15,050
Fields within an individual document in

00:42:13,070 --> 00:42:17,360
the collection but that's usually not

00:42:15,050 --> 00:42:19,850
that useful what you usually tend to

00:42:17,360 --> 00:42:22,250
want to do is maybe do cross querying

00:42:19,850 --> 00:42:24,160
across the individual fields within an

00:42:22,250 --> 00:42:26,450
individual item in the collection

00:42:24,160 --> 00:42:28,880
because what happens behind the scenes

00:42:26,450 --> 00:42:30,850
if you use an object type is that in

00:42:28,880 --> 00:42:32,960
fact all of the fields from all of the

00:42:30,850 --> 00:42:35,510
individual items in the collection get

00:42:32,960 --> 00:42:37,880
flattened out into just the set of

00:42:35,510 --> 00:42:40,160
fields that exist in an individual

00:42:37,880 --> 00:42:42,020
document so you lose the association

00:42:40,160 --> 00:42:44,030
between where the value came from and

00:42:42,020 --> 00:42:46,550
which particular instance which means

00:42:44,030 --> 00:42:48,680
that you then lose the cross association

00:42:46,550 --> 00:42:50,870
between any of the individual property

00:42:48,680 --> 00:42:53,900
values on an individual document in that

00:42:50,870 --> 00:42:57,590
collection so nested types will allow

00:42:53,900 --> 00:43:01,130
you to overcome that limitation and you

00:42:57,590 --> 00:43:06,860
can then query on individual items in

00:43:01,130 --> 00:43:10,940
that collection you can also map using a

00:43:06,860 --> 00:43:13,370
visitor pattern as well so you we can

00:43:10,940 --> 00:43:15,170
pass here for example this keyword

00:43:13,370 --> 00:43:19,190
visitor so that whilst we're mapping

00:43:15,170 --> 00:43:21,620
this user we can say I want you to auto

00:43:19,190 --> 00:43:25,190
map so in further mappings here but

00:43:21,620 --> 00:43:27,290
again if you come across a property type

00:43:25,190 --> 00:43:29,720
of string then I want you to map it as a

00:43:27,290 --> 00:43:31,730
keyword property only so we can pass in

00:43:29,720 --> 00:43:34,250
a visitor to say how we want the

00:43:31,730 --> 00:43:37,520
mappings to go through so we can see the

00:43:34,250 --> 00:43:39,560
result of that here is that all of our

00:43:37,520 --> 00:43:42,050
string fields are mapped there's keyword

00:43:39,560 --> 00:43:44,750
types that's usually very useful for log

00:43:42,050 --> 00:43:46,490
analytics use cases where you typically

00:43:44,750 --> 00:43:49,970
just want the values represented as

00:43:46,490 --> 00:43:52,280
keywords you did you tend to not want to

00:43:49,970 --> 00:43:54,680
search on them in the same way in which

00:43:52,280 --> 00:44:02,360
you want to usually search on bodies of

00:43:54,680 --> 00:44:06,230
text so cool that was a lot of talking

00:44:02,360 --> 00:44:09,710
so we'll just jump back in here to int

00:44:06,230 --> 00:44:11,120
here into the slides so there's a whole

00:44:09,710 --> 00:44:13,970
bunch of search queries that you have

00:44:11,120 --> 00:44:16,670
available full-text search queries which

00:44:13,970 --> 00:44:19,940
we saw briefly there the match phrases a

00:44:16,670 --> 00:44:22,520
is a full text search query we also have

00:44:19,940 --> 00:44:24,500
term level queries which

00:44:22,520 --> 00:44:26,119
very different to how full-text search

00:44:24,500 --> 00:44:28,099
queries works the full-text search

00:44:26,119 --> 00:44:31,520
queries undergo this process of analysis

00:44:28,099 --> 00:44:33,470
index time and also a query time which

00:44:31,520 --> 00:44:36,470
I'll talk about in a sec term level

00:44:33,470 --> 00:44:38,630
queries actually undergo no analysis on

00:44:36,470 --> 00:44:41,450
the query so what you send into the

00:44:38,630 --> 00:44:46,430
query has to basically match exactly

00:44:41,450 --> 00:44:48,680
what is in what is in the index the span

00:44:46,430 --> 00:44:51,020
level queries which is finding a word

00:44:48,680 --> 00:44:53,119
here within the text and say another

00:44:51,020 --> 00:44:55,580
word in the text and what the distances

00:44:53,119 --> 00:44:58,010
between them there's a whole family of

00:44:55,580 --> 00:45:02,089
span queries to do that kind of intro

00:44:58,010 --> 00:45:03,890
text search compound queries so it's

00:45:02,089 --> 00:45:05,990
great I can do full text is great I can

00:45:03,890 --> 00:45:08,180
do term level how about being able to

00:45:05,990 --> 00:45:10,730
combine all of those into a you know

00:45:08,180 --> 00:45:12,320
compound query that this is where this

00:45:10,730 --> 00:45:14,210
is where these come in so typically

00:45:12,320 --> 00:45:16,700
boolean queries are the ones where you

00:45:14,210 --> 00:45:19,010
can combine full text with term level

00:45:16,700 --> 00:45:21,650
with other full text and that's how you

00:45:19,010 --> 00:45:23,810
typically build up a search strategy

00:45:21,650 --> 00:45:26,900
there's other compound queries there as

00:45:23,810 --> 00:45:29,500
well things like more like this which I

00:45:26,900 --> 00:45:32,210
have a have an example of geo queries

00:45:29,500 --> 00:45:34,040
kind of name says it all really being

00:45:32,210 --> 00:45:37,040
able to find things that exist within a

00:45:34,040 --> 00:45:38,930
polygon and geo jason polygon being able

00:45:37,040 --> 00:45:41,119
to find a distance between this thing

00:45:38,930 --> 00:45:44,270
and that thing or show me stuff within a

00:45:41,119 --> 00:45:45,790
particular radius and then obviously you

00:45:44,270 --> 00:45:49,190
can still combine it with all of these

00:45:45,790 --> 00:45:51,070
joining queries so I briefly talked

00:45:49,190 --> 00:45:53,240
about nested types there so that

00:45:51,070 --> 00:45:56,240
internally is represented as a join

00:45:53,240 --> 00:45:59,690
between that the root document and the

00:45:56,240 --> 00:46:01,700
the nested documents there but we also

00:45:59,690 --> 00:46:03,290
have the ability to perform parent-child

00:46:01,700 --> 00:46:06,260
as I showed you with the questions and

00:46:03,290 --> 00:46:08,119
answers and specialized queries which

00:46:06,260 --> 00:46:12,170
are just the different beast altogether

00:46:08,119 --> 00:46:15,800
so we will we will touch on that very

00:46:12,170 --> 00:46:18,020
very shortly but you essentially have a

00:46:15,800 --> 00:46:20,540
multitude of tools available to you

00:46:18,020 --> 00:46:22,520
unlike for example in sequel server

00:46:20,540 --> 00:46:25,099
every everything you can do in sequel

00:46:22,520 --> 00:46:27,050
server you can do an elastic search the

00:46:25,099 --> 00:46:30,320
typical queries you're used to running I

00:46:27,050 --> 00:46:33,020
would typically fall into what would be

00:46:30,320 --> 00:46:35,300
term level queries in here so finding

00:46:33,020 --> 00:46:36,110
this exact thing or finding a prefix of

00:46:35,300 --> 00:46:38,630
finding a bar

00:46:36,110 --> 00:46:40,400
card or finding a regex using a regex

00:46:38,630 --> 00:46:42,950
you can perform all of those in

00:46:40,400 --> 00:46:45,620
elasticsearch so here's a here's a very

00:46:42,950 --> 00:46:46,970
quick example so we're just going to

00:46:45,620 --> 00:46:49,040
create an index we're going to stick two

00:46:46,970 --> 00:46:51,560
documents in their title header use

00:46:49,040 --> 00:46:53,780
elastic search we're all about Cabana

00:46:51,560 --> 00:46:55,490
got bulk index those and then we're just

00:46:53,780 --> 00:46:57,530
going to search using a match query

00:46:55,490 --> 00:46:59,540
which is one of those full text queries

00:46:57,530 --> 00:47:02,870
I was talking about and we're going to

00:46:59,540 --> 00:47:06,800
search for elastic search cabana and if

00:47:02,870 --> 00:47:10,790
we run this one and we look at you look

00:47:06,800 --> 00:47:14,300
at the results we get back both

00:47:10,790 --> 00:47:15,980
documents as we would expect all about

00:47:14,300 --> 00:47:19,520
Cabana gets comes back with a score of

00:47:15,980 --> 00:47:21,080
four interesting how do I use elastic

00:47:19,520 --> 00:47:23,030
search redneck comes back with the score

00:47:21,080 --> 00:47:25,090
of two so we can see her she that the

00:47:23,030 --> 00:47:27,980
all about Cabana document came back

00:47:25,090 --> 00:47:30,920
ahead of the how do I use elastic search

00:47:27,980 --> 00:47:32,900
with net why might that be the case

00:47:30,920 --> 00:47:33,650
there because one of them contains

00:47:32,900 --> 00:47:35,690
Cabana

00:47:33,650 --> 00:47:37,550
one of them contains elastic search you

00:47:35,690 --> 00:47:40,750
know it matched on both of those what's

00:47:37,550 --> 00:47:43,010
what's kind of affecting this scoring

00:47:40,750 --> 00:47:45,530
well there's there's something I should

00:47:43,010 --> 00:47:47,870
clue you into here actually which does

00:47:45,530 --> 00:47:50,180
affect the way in which scores work and

00:47:47,870 --> 00:47:52,850
that's to do with the length of the

00:47:50,180 --> 00:47:54,890
field that were targeting so the field

00:47:52,850 --> 00:47:57,560
head I use elastic search word net is

00:47:54,890 --> 00:48:00,440
longer than all about Cabana which means

00:47:57,560 --> 00:48:02,810
that finding Cabana in all about Cabana

00:48:00,440 --> 00:48:06,770
versus finding elastic search in had I

00:48:02,810 --> 00:48:08,420
use elastic search read net its proper

00:48:06,770 --> 00:48:10,340
you know that the term frequencies are

00:48:08,420 --> 00:48:13,760
the same for both of them but this one

00:48:10,340 --> 00:48:15,560
contains more words this or sorry this

00:48:13,760 --> 00:48:17,480
one contains less words sorry all about

00:48:15,560 --> 00:48:19,040
Cabana so therefore if we find it in

00:48:17,480 --> 00:48:22,490
there we should give it a higher score

00:48:19,040 --> 00:48:25,280
and that that's such a good norms so

00:48:22,490 --> 00:48:27,410
this normalization factor within elastic

00:48:25,280 --> 00:48:30,380
search we can disable that if we want to

00:48:27,410 --> 00:48:32,420
which I'm going to do now and I'm just

00:48:30,380 --> 00:48:34,640
using a visitor to do it so when in

00:48:32,420 --> 00:48:36,500
anytime you see a text property just

00:48:34,640 --> 00:48:38,330
disable the norms just set norms to

00:48:36,500 --> 00:48:41,690
false in there and if we run that one

00:48:38,330 --> 00:48:48,200
again and we have a look at the results

00:48:41,690 --> 00:48:49,670
we can see here now that the scores for

00:48:48,200 --> 00:48:52,460
each of them they're different

00:48:49,670 --> 00:48:54,019
than what they were before but we can

00:48:52,460 --> 00:48:55,970
see that the scores here for the two

00:48:54,019 --> 00:48:58,849
documents are exactly the same so we

00:48:55,970 --> 00:49:00,940
disabled norms so it's no longer taking

00:48:58,849 --> 00:49:04,579
the length of the field into account

00:49:00,940 --> 00:49:07,700
which is typically I found typically

00:49:04,579 --> 00:49:11,510
that's usually what you want to do the

00:49:07,700 --> 00:49:15,410
intention there is to favor things such

00:49:11,510 --> 00:49:16,700
as titles over bodies of text that's

00:49:15,410 --> 00:49:18,799
that's the intention so if you're

00:49:16,700 --> 00:49:20,630
targeting multiple fields give more

00:49:18,799 --> 00:49:22,130
weight to the shorter fields than the

00:49:20,630 --> 00:49:25,359
longer ones because the chance of

00:49:22,130 --> 00:49:27,740
finding the probability of that that

00:49:25,359 --> 00:49:31,069
token being more relevant in the short

00:49:27,740 --> 00:49:34,039
field you know is higher than existing

00:49:31,069 --> 00:49:39,349
in a bigger body of text that's the

00:49:34,039 --> 00:49:41,119
intention anyway so let's go on to the

00:49:39,349 --> 00:49:44,359
term level query here so this is a not

00:49:41,119 --> 00:49:45,980
analyzed query and if we run this one so

00:49:44,359 --> 00:49:48,349
we're just indexing the same documents

00:49:45,980 --> 00:49:51,769
as before except I'm not showing it now

00:49:48,349 --> 00:49:53,390
I'm just believe me it's happening and

00:49:51,769 --> 00:49:56,809
we're just running this term level query

00:49:53,390 --> 00:49:57,680
we get no results back why might that be

00:49:56,809 --> 00:50:01,430
the case

00:49:57,680 --> 00:50:03,349
oh it's actually because the title field

00:50:01,430 --> 00:50:05,509
that we're targeting is being analyzed

00:50:03,349 --> 00:50:07,880
when it's being indexed so it's been

00:50:05,509 --> 00:50:10,309
it's a text data type and it's being

00:50:07,880 --> 00:50:12,559
analyzed and what we're doing here is

00:50:10,309 --> 00:50:15,109
we're sending through a case sensitive

00:50:12,559 --> 00:50:17,900
elastic search here which isn't matching

00:50:15,109 --> 00:50:19,640
the way in which the tête the original

00:50:17,900 --> 00:50:23,359
text from the document when we indexed

00:50:19,640 --> 00:50:24,890
has been analyzed so if we look at the

00:50:23,359 --> 00:50:31,849
next one if we send it through lower

00:50:24,890 --> 00:50:33,950
case here we got a result so we got how

00:50:31,849 --> 00:50:35,569
do I use elastic search with net so we

00:50:33,950 --> 00:50:38,900
can you know what's kind of going on

00:50:35,569 --> 00:50:42,049
here so as I said before term level

00:50:38,900 --> 00:50:46,039
queries they the actual input to the

00:50:42,049 --> 00:50:48,589
query undergoes no analysis so whatever

00:50:46,039 --> 00:50:50,869
you send to a term level query it's

00:50:48,589 --> 00:50:52,579
going to look exactly for that in the

00:50:50,869 --> 00:50:54,500
inverted index when it's querying

00:50:52,579 --> 00:50:57,710
elasticsearch and because our original

00:50:54,500 --> 00:51:01,430
data in our original document underwent

00:50:57,710 --> 00:51:03,390
analysis where we indexed there it's it

00:51:01,430 --> 00:51:05,069
should be clear hopefully that the toe

00:51:03,390 --> 00:51:08,309
Ekans that have been produced as a

00:51:05,069 --> 00:51:10,890
result of that analysis are probably not

00:51:08,309 --> 00:51:12,000
they don't contain any casing because we

00:51:10,890 --> 00:51:13,950
saw a match for a lowercase

00:51:12,000 --> 00:51:16,799
elasticsearch but we didn't see a match

00:51:13,950 --> 00:51:19,529
for uppercase elasticsearch if we were

00:51:16,799 --> 00:51:21,450
to target the key keyword field here of

00:51:19,529 --> 00:51:23,730
the title because we've indexed it as

00:51:21,450 --> 00:51:26,250
both text data type and keyword data

00:51:23,730 --> 00:51:27,690
type so we can use this handy suffix

00:51:26,250 --> 00:51:29,549
here so we can still use a lambda

00:51:27,690 --> 00:51:31,440
expression to say so I get the title

00:51:29,549 --> 00:51:35,190
field but I want you to look at the

00:51:31,440 --> 00:51:37,470
keyword field of the title and we send

00:51:35,190 --> 00:51:41,250
through exactly what the title was we

00:51:37,470 --> 00:51:45,000
should see a result there which we do

00:51:41,250 --> 00:51:48,079
there so that's term love that's kind of

00:51:45,000 --> 00:51:52,319
full text queries term level queries

00:51:48,079 --> 00:51:56,670
here's a geo query so say for example

00:51:52,319 --> 00:51:58,470
I'm looking for where in this list of

00:51:56,670 --> 00:52:00,990
suburbs which are Australian suburbs

00:51:58,470 --> 00:52:03,240
where does this point exist in which

00:52:00,990 --> 00:52:04,680
suburb does this point exist so this is

00:52:03,240 --> 00:52:05,849
just looking at all of the Australian

00:52:04,680 --> 00:52:08,519
suburbs which I think there are eight

00:52:05,849 --> 00:52:11,700
thousand six hundred of and just tell me

00:52:08,519 --> 00:52:13,500
where that geo coordinate is so it comes

00:52:11,700 --> 00:52:19,430
back and it's where I live which is

00:52:13,500 --> 00:52:23,519
manly three and we can then also say

00:52:19,430 --> 00:52:27,660
given a a geo shake that is already

00:52:23,519 --> 00:52:30,390
indexed show me other geo ships that are

00:52:27,660 --> 00:52:33,539
indexed that intersect with that geo

00:52:30,390 --> 00:52:35,430
shape that is indexed so we can use this

00:52:33,539 --> 00:52:37,769
geo index shape here and we can say

00:52:35,430 --> 00:52:39,420
given the suburb suburb with this ID

00:52:37,769 --> 00:52:42,059
which is the one we saw in the previous

00:52:39,420 --> 00:52:43,920
query manly show me there basically show

00:52:42,059 --> 00:52:46,349
me the surrounding suburbs show me the

00:52:43,920 --> 00:52:50,640
suburbs that lie next to that suburb and

00:52:46,349 --> 00:52:53,400
then we also then want to exclude manly

00:52:50,640 --> 00:52:58,259
itself from the results there so if we

00:52:53,400 --> 00:53:00,569
were to run that query queenscliff north

00:52:58,259 --> 00:53:02,279
manly manly ville Fairlight those are

00:53:00,569 --> 00:53:04,470
the surrounding suburbs so I can show

00:53:02,279 --> 00:53:07,230
you that's the case here's Manley here

00:53:04,470 --> 00:53:13,380
his Fairlight his Queenscliff is manly

00:53:07,230 --> 00:53:15,990
Vale that's geo queries more like this

00:53:13,380 --> 00:53:17,160
so this falls into that specialized

00:53:15,990 --> 00:53:19,799
field that I was talking

00:53:17,160 --> 00:53:21,779
so given a document or given just the

00:53:19,799 --> 00:53:23,730
piece of text show me other documents

00:53:21,779 --> 00:53:26,009
that exist within elasticsearch which

00:53:23,730 --> 00:53:29,789
are like this one so there's a quite a

00:53:26,009 --> 00:53:32,490
few heuristics and algorithms that

00:53:29,789 --> 00:53:35,970
determine that likeness most of them are

00:53:32,490 --> 00:53:37,559
based on text based likeness but if we

00:53:35,970 --> 00:53:40,440
were to say look for a Stack Overflow

00:53:37,559 --> 00:53:42,480
questions that have the text of reduced

00:53:40,440 --> 00:53:45,299
shards in elasticsearch in the title and

00:53:42,480 --> 00:53:49,200
we want at least one of these terms to

00:53:45,299 --> 00:53:52,049
be in the title then we can get the

00:53:49,200 --> 00:54:01,380
results back from those and if we just

00:53:52,049 --> 00:54:02,789
zoom in we can see here that Oh we're

00:54:01,380 --> 00:54:04,799
gonna we're going to be able to move yet

00:54:02,789 --> 00:54:07,769
we are so we can see here the titles

00:54:04,799 --> 00:54:10,069
here reducing number shards full cluster

00:54:07,769 --> 00:54:13,740
restart we can see these all related to

00:54:10,069 --> 00:54:16,349
elasticsearch by by the tags we can also

00:54:13,740 --> 00:54:19,740
we can also see that they're all roughly

00:54:16,349 --> 00:54:25,039
related to shards or to sharding in some

00:54:19,740 --> 00:54:32,490
way so that was a more like this query

00:54:25,039 --> 00:54:35,940
and we all skip that one in the essence

00:54:32,490 --> 00:54:38,809
of time so function score query here is

00:54:35,940 --> 00:54:42,480
another one that's very very useful so

00:54:38,809 --> 00:54:44,609
we can do some things such as say show

00:54:42,480 --> 00:54:47,160
me documents which are like this other

00:54:44,609 --> 00:54:49,049
one but maybe we want to take features

00:54:47,160 --> 00:54:50,940
of the particular documents that would

00:54:49,049 --> 00:54:53,519
match we want to take features of those

00:54:50,940 --> 00:54:56,700
and use them to influence the score of

00:54:53,519 --> 00:55:00,630
of that particular document so we can

00:54:56,700 --> 00:55:02,730
say here so it's it's rather nested you

00:55:00,630 --> 00:55:05,430
can see that nest itself gets rather

00:55:02,730 --> 00:55:08,460
nested in lambda expressions but

00:55:05,430 --> 00:55:11,099
hopefully this if you lay it out in this

00:55:08,460 --> 00:55:13,440
particular way it tends to mimic exactly

00:55:11,099 --> 00:55:17,519
what the JSON structure looks like for a

00:55:13,440 --> 00:55:19,230
query so here we're just saying so

00:55:17,519 --> 00:55:21,240
source includes only bring back the

00:55:19,230 --> 00:55:23,490
title score and creation date for each

00:55:21,240 --> 00:55:25,859
particular document and in our query

00:55:23,490 --> 00:55:28,259
then we want to perform a function score

00:55:25,859 --> 00:55:29,970
query so the query that we want to run

00:55:28,259 --> 00:55:30,540
is the more like this query that we just

00:55:29,970 --> 00:55:32,100
ran

00:55:30,540 --> 00:55:35,040
which has reduced number shards in

00:55:32,100 --> 00:55:38,190
lastik search but then we want to apply

00:55:35,040 --> 00:55:41,070
some functions to the results there as

00:55:38,190 --> 00:55:42,840
they're coming back and those are here

00:55:41,070 --> 00:55:45,900
in this particular example we want to

00:55:42,840 --> 00:55:47,820
say anything that any question that's

00:55:45,900 --> 00:55:51,300
happened in the last hundred eighty two

00:55:47,820 --> 00:55:53,310
days the last six months and anything

00:55:51,300 --> 00:55:55,500
that's within the last three years we

00:55:53,310 --> 00:56:01,820
want to decay the score for them at a

00:55:55,500 --> 00:56:05,880
gradual which yes so that would be a

00:56:01,820 --> 00:56:08,550
linear decay so anything that is is

00:56:05,880 --> 00:56:10,890
after six months up to three years we

00:56:08,550 --> 00:56:12,960
want to reduce the score for it so if

00:56:10,890 --> 00:56:16,560
it's older we want to give it a lower

00:56:12,960 --> 00:56:19,320
score so the premise here is that newer

00:56:16,560 --> 00:56:21,120
questions in Stack Overflow are probably

00:56:19,320 --> 00:56:23,280
more relevant than older questions in

00:56:21,120 --> 00:56:24,810
Stack Overflow so we can influence the

00:56:23,280 --> 00:56:27,450
score there by taking the creation date

00:56:24,810 --> 00:56:28,980
of the question this field value factor

00:56:27,450 --> 00:56:30,750
is another function that's going to run

00:56:28,980 --> 00:56:34,170
which is going to say take the score of

00:56:30,750 --> 00:56:38,760
that question and so long as it's

00:56:34,170 --> 00:56:42,120
greater than zero we want to apply a 1.5

00:56:38,760 --> 00:56:45,480
fracture factor to it so take the take

00:56:42,120 --> 00:56:48,540
the score and take the square root here

00:56:45,480 --> 00:56:51,300
actually and and apply a factor of 1.5

00:56:48,540 --> 00:56:53,460
so higher scoring questions we want also

00:56:51,300 --> 00:56:55,350
want to be more relevant so these two

00:56:53,460 --> 00:56:58,530
functions are going to also be running

00:56:55,350 --> 00:57:00,570
in combination with it matching more

00:56:58,530 --> 00:57:02,040
like this so we're going to take the

00:57:00,570 --> 00:57:04,350
results of all of those things together

00:57:02,040 --> 00:57:06,240
and here we're going to multiply the

00:57:04,350 --> 00:57:08,760
score of all of them to give a final

00:57:06,240 --> 00:57:09,680
score to a particular document does that

00:57:08,760 --> 00:57:12,750
make sense

00:57:09,680 --> 00:57:17,880
so if we have a look if we have a look

00:57:12,750 --> 00:57:20,490
at this one back here sorry let's jump

00:57:17,880 --> 00:57:27,950
to this one here which I think shows us

00:57:20,490 --> 00:57:27,950
some details so let me just zoom in here

00:57:28,460 --> 00:57:33,750
we can see that the results just from

00:57:31,380 --> 00:57:35,610
running a more like this query we can

00:57:33,750 --> 00:57:37,650
see there's a whole bunch of questions

00:57:35,610 --> 00:57:40,350
here that actually have not very high

00:57:37,650 --> 00:57:42,900
scores on them so they might be poorly

00:57:40,350 --> 00:57:43,829
worded questions they might be very very

00:57:42,900 --> 00:57:45,809
specific

00:57:43,829 --> 00:57:49,079
questions rather than general questions

00:57:45,809 --> 00:57:51,020
about elasticsearch so we want to

00:57:49,079 --> 00:57:54,930
basically get some more relevant

00:57:51,020 --> 00:57:57,900
questions in the in the results here so

00:57:54,930 --> 00:58:02,010
if we were to run our function score

00:57:57,900 --> 00:58:08,460
query and check out the results of this

00:58:02,010 --> 00:58:10,589
one we can see here now how our

00:58:08,460 --> 00:58:13,770
functions have affected the results here

00:58:10,589 --> 00:58:15,569
so we have some higher scoring we have

00:58:13,770 --> 00:58:19,470
some higher scoring questions coming

00:58:15,569 --> 00:58:21,510
back as being more relevant and yeah

00:58:19,470 --> 00:58:24,770
there's an influence of more recent

00:58:21,510 --> 00:58:27,780
there so these particular ones here

00:58:24,770 --> 00:58:30,059
they're probably more relevant for the

00:58:27,780 --> 00:58:32,400
text-based and we can we can there's

00:58:30,059 --> 00:58:34,500
another API to understand the breakdown

00:58:32,400 --> 00:58:37,170
of a particular score which we could use

00:58:34,500 --> 00:58:39,869
here to understand why this one here

00:58:37,170 --> 00:58:43,170
that's four years old appears higher

00:58:39,869 --> 00:58:45,089
than this one we can see you know just

00:58:43,170 --> 00:58:48,569
eyeballing this probably because this

00:58:45,089 --> 00:58:50,760
one has a higher score and the title is

00:58:48,569 --> 00:58:53,150
probably more like the title that we

00:58:50,760 --> 00:58:53,150
asked for

00:58:54,500 --> 00:58:59,400
that's function score queries so it's a

00:58:57,180 --> 00:59:04,200
specialized type of query very very

00:58:59,400 --> 00:59:06,930
useful so compound queries is a boolean

00:59:04,200 --> 00:59:09,710
query so we want to look for documents

00:59:06,930 --> 00:59:12,750
that match with cabaña in a title or

00:59:09,710 --> 00:59:14,730
match with elasticsearch in the title if

00:59:12,750 --> 00:59:16,410
they must match with elasticsearch then

00:59:14,730 --> 00:59:20,040
give give them a slightly higher

00:59:16,410 --> 00:59:22,859
boosting six boost their score by 2 here

00:59:20,040 --> 00:59:24,240
and then we also want to filter so we

00:59:22,859 --> 00:59:26,069
only want to look for questions here

00:59:24,240 --> 00:59:28,799
that do have a score greater than zero

00:59:26,069 --> 00:59:30,720
so any negative score questions we're

00:59:28,799 --> 00:59:32,910
not interested in so we can combine all

00:59:30,720 --> 00:59:35,099
of those things in so gabbana or

00:59:32,910 --> 00:59:36,900
elasticsearch in a title and it has to

00:59:35,099 --> 00:59:38,760
have a score greater than zero that's

00:59:36,900 --> 00:59:40,890
essentially a boolean query that's how

00:59:38,760 --> 00:59:44,000
we combine a collection of queries into

00:59:40,890 --> 00:59:48,599
one and we can combine them in and ORS

00:59:44,000 --> 00:59:51,809
not and filter configurations filters

00:59:48,599 --> 00:59:54,900
here queries that run within the filter

00:59:51,809 --> 00:59:56,920
section here do not undergo a scouring

00:59:54,900 --> 00:59:59,260
phase so tip

00:59:56,920 --> 01:00:01,870
with unstructured search where you might

00:59:59,260 --> 01:00:04,210
do for example range queries or does it

01:00:01,870 --> 01:00:06,130
match this thing exactly you typically

01:00:04,210 --> 01:00:07,600
don't have scores for those things there

01:00:06,130 --> 01:00:09,820
are predicates there over match or they

01:00:07,600 --> 01:00:12,040
don't you typically won't want to put

01:00:09,820 --> 01:00:13,960
those into the filter part of a boolean

01:00:12,040 --> 01:00:18,750
clause if you're combining them with

01:00:13,960 --> 01:00:20,980
other queries nest has a very very nice

01:00:18,750 --> 01:00:23,800
shorthand for being able to specify

01:00:20,980 --> 01:00:26,710
boolean queries by over overloading the

01:00:23,800 --> 01:00:30,010
operators on queries so we can or these

01:00:26,710 --> 01:00:32,160
two queries together so am ghosts yes a

01:00:30,010 --> 01:00:35,830
match query here or this match query and

01:00:32,160 --> 01:00:38,980
plus so plus unary operator here is used

01:00:35,830 --> 01:00:44,230
to specify filter query this is exactly

01:00:38,980 --> 01:00:46,300
the same query as we saw before and this

01:00:44,230 --> 01:00:49,030
one here oh yeah it just shows off

01:00:46,300 --> 01:00:52,810
covariant search results so we can ask

01:00:49,030 --> 01:00:55,210
to search it on the posts on posts we're

01:00:52,810 --> 01:00:58,420
interested in questions and answers and

01:00:55,210 --> 01:01:00,250
we can perform an async/await search for

01:00:58,420 --> 01:01:02,530
anything that matches async/await in the

01:01:00,250 --> 01:01:04,960
body and we can see the results here

01:01:02,530 --> 01:01:10,320
that we get back questions and answers

01:01:04,960 --> 01:01:13,240
in that results collection cool okay so

01:01:10,320 --> 01:01:20,530
that was a very very well win tour of

01:01:13,240 --> 01:01:22,150
search queries I'm gonna skip through

01:01:20,530 --> 01:01:23,770
this very very quickly because I can see

01:01:22,150 --> 01:01:26,110
it's very hot in here

01:01:23,770 --> 01:01:28,420
I talked basically around text data

01:01:26,110 --> 01:01:31,330
types undergoing analysis what was I

01:01:28,420 --> 01:01:34,630
blathering on about essentially they

01:01:31,330 --> 01:01:36,850
undergo analysis using an analyzer so

01:01:34,630 --> 01:01:39,700
what does an analyzer do it takes in a

01:01:36,850 --> 01:01:42,730
stream of characters it runs them

01:01:39,700 --> 01:01:44,320
through 0 more character filters it then

01:01:42,730 --> 01:01:46,450
runs that through a tokenizer which

01:01:44,320 --> 01:01:48,640
tokenized is that text into individual

01:01:46,450 --> 01:01:51,070
terms and then it can run those

01:01:48,640 --> 01:01:53,710
individual tokens through 0 or more

01:01:51,070 --> 01:01:55,300
token filters and at the end of it you

01:01:53,710 --> 01:01:57,610
get a collection of tokens which are

01:01:55,300 --> 01:01:59,620
indexed into the inverted index and when

01:01:57,610 --> 01:02:01,480
you perform a search on a text data type

01:01:59,620 --> 01:02:04,510
field you're going to be hitting the

01:02:01,480 --> 01:02:06,700
inverted index and your query will also

01:02:04,510 --> 01:02:08,320
undergo analysis and the tokens produce

01:02:06,700 --> 01:02:10,359
from that will be compared to the tokens

01:02:08,320 --> 01:02:13,099
produce the index time

01:02:10,359 --> 01:02:14,750
so the standard analyzer is the one by

01:02:13,099 --> 01:02:16,400
default if you don't specify any

01:02:14,750 --> 01:02:18,890
particular analyzer it's going to use a

01:02:16,400 --> 01:02:22,250
standard one which doesn't have any

01:02:18,890 --> 01:02:24,049
character filters it has a tokenizer

01:02:22,250 --> 01:02:26,779
which does which is called the standard

01:02:24,049 --> 01:02:28,880
tokenizer funnily enough which undergoes

01:02:26,779 --> 01:02:32,269
Unicode text segmentation algorithm

01:02:28,880 --> 01:02:34,190
which is has very different as a complex

01:02:32,269 --> 01:02:36,799
set of heuristics for how it determines

01:02:34,190 --> 01:02:39,410
where to split words so it will split on

01:02:36,799 --> 01:02:41,450
whitespace it may split on a dot if it's

01:02:39,410 --> 01:02:43,849
at the end of a word but not if the dot

01:02:41,450 --> 01:02:46,819
exists in the middle and various other

01:02:43,849 --> 01:02:49,970
heuristics so it generally does a pretty

01:02:46,819 --> 01:02:52,460
smart job for Western languages for

01:02:49,970 --> 01:02:54,500
Eastern languages it's not that useful

01:02:52,460 --> 01:02:57,470
because they have very very different

01:02:54,500 --> 01:03:00,970
rules around how they form words and

01:02:57,470 --> 01:03:03,740
usually they compound things together so

01:03:00,970 --> 01:03:05,930
standard analyzers usually no good you

01:03:03,740 --> 01:03:07,460
want to use a specialized analyzer for

01:03:05,930 --> 01:03:09,769
that stuff and then it goes through

01:03:07,460 --> 01:03:11,779
token filters standard token filter

01:03:09,769 --> 01:03:13,220
doesn't do anything lower case token

01:03:11,779 --> 01:03:15,859
filter substantive token fill as a

01:03:13,220 --> 01:03:18,559
placeholder for future things the lower

01:03:15,859 --> 01:03:19,910
case token filter lower cases so we saw

01:03:18,559 --> 01:03:22,490
there elasticsearch remembers

01:03:19,910 --> 01:03:24,170
capitalized didn't match the standard

01:03:22,490 --> 01:03:26,509
analyzer because the standard analyzer

01:03:24,170 --> 01:03:28,880
lower cases things and then a stop token

01:03:26,509 --> 01:03:30,559
filter can remove stop words so it can

01:03:28,880 --> 01:03:33,319
take out those common words like there

01:03:30,559 --> 01:03:35,990
is and which are of no use for search

01:03:33,319 --> 01:03:39,289
generally and you want to strip those

01:03:35,990 --> 01:03:41,390
out and we can also build analyzers to

01:03:39,289 --> 01:03:44,690
perform things such as stemming limit

01:03:41,390 --> 01:03:47,750
ization in order to take for example

01:03:44,690 --> 01:03:50,079
running runner runs and stem those down

01:03:47,750 --> 01:03:52,220
to the to the route form of the word

01:03:50,079 --> 01:03:54,920
that's essentially what we can do with

01:03:52,220 --> 01:03:57,890
analysis we have full control over how

01:03:54,920 --> 01:04:00,470
we decide that that data is analyzed to

01:03:57,890 --> 01:04:02,269
give you a quick example if we were to

01:04:00,470 --> 01:04:04,579
say F sharp is clearly the superior

01:04:02,269 --> 01:04:06,500
language if we were to run that through

01:04:04,579 --> 01:04:08,000
the standard analyzer the first of all

01:04:06,500 --> 01:04:11,269
the tokenizer is going to split into

01:04:08,000 --> 01:04:13,759
these tokens so we can see that the hash

01:04:11,269 --> 01:04:16,660
the pound has disappeared so as our

01:04:13,759 --> 01:04:19,309
stars around V it's lost some emphasis

01:04:16,660 --> 01:04:20,869
we can see the standard token filter as

01:04:19,309 --> 01:04:21,619
I said as a placeholder it isn't doing

01:04:20,869 --> 01:04:23,660
anything here

01:04:21,619 --> 01:04:26,710
to the tokens but the lowercase token

01:04:23,660 --> 01:04:29,210
filter seeking a lowercase ever fin and

01:04:26,710 --> 01:04:31,940
then the stop where a token filter in

01:04:29,210 --> 01:04:34,549
the standard analyzer default at G

01:04:31,940 --> 01:04:37,819
doesn't remove stop words you can derive

01:04:34,549 --> 01:04:40,789
yeah it made trick you but you can

01:04:37,819 --> 01:04:44,299
derive a an analyzer from the standard

01:04:40,789 --> 01:04:48,019
analyzer and say hey remove English stop

01:04:44,299 --> 01:04:50,509
words please I think it should remove

01:04:48,019 --> 01:04:52,839
them by default personally but that's a

01:04:50,509 --> 01:04:55,460
discussion to be had with the team and

01:04:52,839 --> 01:04:57,230
then all of those tokens go into the

01:04:55,460 --> 01:05:00,230
inverted index so for that particular

01:04:57,230 --> 01:05:02,569
field which is title we can see the

01:05:00,230 --> 01:05:04,940
terms that have the result of analysis

01:05:02,569 --> 01:05:09,049
and then the document IDs from which

01:05:04,940 --> 01:05:11,180
they came and just to jump very very

01:05:09,049 --> 01:05:15,410
quickly because I realize time is flying

01:05:11,180 --> 01:05:20,089
by if we were to just skip over all of

01:05:15,410 --> 01:05:23,420
these and just jump into analysis this

01:05:20,089 --> 01:05:25,460
is essentially what how you would define

01:05:23,420 --> 01:05:28,099
an analyzer so at the point of creating

01:05:25,460 --> 01:05:29,989
this index here we can specify the

01:05:28,099 --> 01:05:31,999
number of shards that it should have so

01:05:29,989 --> 01:05:34,999
for demonstration purposes I only need

01:05:31,999 --> 01:05:38,589
one know replicas and then under

01:05:34,999 --> 01:05:41,690
analysis I can specify character filters

01:05:38,589 --> 01:05:43,789
analyzers and token filters so let's

01:05:41,690 --> 01:05:45,559
just take a very very quick walk through

01:05:43,789 --> 01:05:47,359
here character filters are going to

01:05:45,559 --> 01:05:50,150
strip individual characters from the

01:05:47,359 --> 01:05:51,980
token stream so we set one up here can

01:05:50,150 --> 01:05:53,900
call it programming language it's just

01:05:51,980 --> 01:05:55,609
going to call this function and all that

01:05:53,900 --> 01:05:57,499
function is going to do is it's just

01:05:55,609 --> 01:06:00,140
going to take anything that starts with

01:05:57,499 --> 01:06:02,720
C sharp F sharp F sharp G sharp F sharp

01:06:00,140 --> 01:06:05,299
a sharp I've never heard of that K sharp

01:06:02,720 --> 01:06:08,930
G sharp it's just going to take that and

01:06:05,299 --> 01:06:11,210
and change the hash to sharp so that we

01:06:08,930 --> 01:06:12,049
don't lose it when we send it through

01:06:11,210 --> 01:06:13,849
the tokenizer

01:06:12,049 --> 01:06:17,390
because we want people to be able to

01:06:13,849 --> 01:06:19,579
search with C sharp with the hash but

01:06:17,390 --> 01:06:22,910
still be able to find C sharp results

01:06:19,579 --> 01:06:25,160
and not see results as they would if we

01:06:22,910 --> 01:06:27,950
just let it run through the standard

01:06:25,160 --> 01:06:30,440
tokenizer then what we're going to do is

01:06:27,950 --> 01:06:32,119
set up a token filter here so we're

01:06:30,440 --> 01:06:35,520
going to call it our better synonym

01:06:32,119 --> 01:06:37,800
token filter so any time it sees so

01:06:35,520 --> 01:06:40,440
period better or greater it's also going

01:06:37,800 --> 01:06:42,780
to introduce the synonym for that one as

01:06:40,440 --> 01:06:45,540
well so these are all bi-directional

01:06:42,780 --> 01:06:47,640
synonyms so superior better greater any

01:06:45,540 --> 01:06:49,470
of those will also have the synonym for

01:06:47,640 --> 01:06:52,230
that and then we're just going to define

01:06:49,470 --> 01:06:54,330
two analyzers here two custom analyzers

01:06:52,230 --> 01:06:58,560
that use those pieces one we've called

01:06:54,330 --> 01:07:00,570
HTML which uses a HTML strip a character

01:06:58,560 --> 01:07:02,580
filter which is an inbuilt one in in

01:07:00,570 --> 01:07:04,530
elasticsearch which just takes out all

01:07:02,580 --> 01:07:06,450
of the tags because usually if you're

01:07:04,530 --> 01:07:08,820
searching on a field that has you know

01:07:06,450 --> 01:07:10,440
HTML tags in it you don't want those in

01:07:08,820 --> 01:07:12,900
there you know you want a strip all of

01:07:10,440 --> 01:07:14,690
those ok and we're also going to use our

01:07:12,900 --> 01:07:16,530
programming language character filter

01:07:14,690 --> 01:07:17,370
we're just going to use the standard

01:07:16,530 --> 01:07:18,840
tokenizer

01:07:17,370 --> 01:07:21,450
because it does a pretty good job with

01:07:18,840 --> 01:07:23,070
that Unicode text segmentation and then

01:07:21,450 --> 01:07:25,530
we're going to use our token filter as

01:07:23,070 --> 01:07:26,910
the standard lower case use a stop one

01:07:25,530 --> 01:07:29,370
and then we're going to use our better

01:07:26,910 --> 01:07:31,380
synonym and then this one's really just

01:07:29,370 --> 01:07:35,040
the same as this one here except it

01:07:31,380 --> 01:07:37,230
doesn't strip the HTML tags and there's

01:07:35,040 --> 01:07:41,280
a reason why we want might want to do

01:07:37,230 --> 01:07:44,040
that here on our question type when

01:07:41,280 --> 01:07:45,780
we're mapping it we're for the body for

01:07:44,040 --> 01:07:48,480
the body field we're going to say use

01:07:45,780 --> 01:07:51,000
the HTML one so strip HTML tags when

01:07:48,480 --> 01:07:53,760
we're indexing but when we're searching

01:07:51,000 --> 01:07:55,380
use this analyzer instead so when you

01:07:53,760 --> 01:07:57,390
sir when you're searching on the site

01:07:55,380 --> 01:07:59,460
you're not going to put in HTML tags as

01:07:57,390 --> 01:08:02,130
you search so there's no need to do that

01:07:59,460 --> 01:08:03,810
HTML tag stripping so we can use this

01:08:02,130 --> 01:08:06,300
other analyzer that doesn't need to do

01:08:03,810 --> 01:08:09,690
that so if we just run if we just run

01:08:06,300 --> 01:08:11,430
this very quickly so we can see the

01:08:09,690 --> 01:08:17,520
mapping that was generated from my

01:08:11,430 --> 01:08:19,200
function yeah so we can see that one

01:08:17,520 --> 01:08:21,000
generated from the function and this is

01:08:19,200 --> 01:08:23,760
the mapping for the question type and

01:08:21,000 --> 01:08:26,460
yet it succeeded so if we go to the next

01:08:23,760 --> 01:08:29,580
slide here and then we just run use the

01:08:26,460 --> 01:08:31,529
analyze API and just see what F sharp is

01:08:29,580 --> 01:08:33,960
clearly the superior language comes out

01:08:31,529 --> 01:08:35,370
with the standard analyzer and then we

01:08:33,960 --> 01:08:38,400
see what comes out with our expand

01:08:35,370 --> 01:08:40,440
analyzer that we just created and just

01:08:38,400 --> 01:08:43,850
compare the tokens that we get out for

01:08:40,440 --> 01:08:45,830
both of those so

01:08:43,850 --> 01:08:47,540
can see the standard analyser F is

01:08:45,830 --> 01:08:49,100
clearly the superior language and we can

01:08:47,540 --> 01:08:51,350
see with our expand one which has our

01:08:49,100 --> 01:08:53,180
synonym token filter in there we also

01:08:51,350 --> 01:08:53,900
have the better greater synonyms in

01:08:53,180 --> 01:08:55,880
there as well

01:08:53,900 --> 01:08:58,190
so someone looking for f-sharp is the

01:08:55,880 --> 01:09:00,320
better language is probably going to

01:08:58,190 --> 01:09:06,440
come across our question if it had the

01:09:00,320 --> 01:09:10,460
superior language in it I'm gonna fly

01:09:06,440 --> 01:09:14,060
through aggregations because they're

01:09:10,460 --> 01:09:16,400
kind of cool yep so this was all that

01:09:14,060 --> 01:09:18,020
was all searched it's very very useful

01:09:16,400 --> 01:09:19,760
for lots of things but you can also get

01:09:18,020 --> 01:09:22,130
your data so metrics being able to

01:09:19,760 --> 01:09:25,910
aggregate individual metrics sums

01:09:22,130 --> 01:09:28,070
averages mins max that kind of thing so

01:09:25,910 --> 01:09:30,350
individual values getting those out of

01:09:28,070 --> 01:09:33,290
their buckets or an interesting one so

01:09:30,350 --> 01:09:35,840
just basically dropping things into

01:09:33,290 --> 01:09:37,250
buckets based on say date range so if

01:09:35,840 --> 01:09:38,900
it's in the last week he goes in this

01:09:37,250 --> 01:09:41,720
bucket if then the week before goes in

01:09:38,900 --> 01:09:43,550
that bucket and with bucket aggregations

01:09:41,720 --> 01:09:45,140
we can also sub a core gate as well so

01:09:43,550 --> 01:09:47,030
we can put things in a bucket here and

01:09:45,140 --> 01:09:49,070
then sub sub aggregate those buckets

01:09:47,030 --> 01:09:54,950
those buckets and we can go n levels

01:09:49,070 --> 01:09:56,840
deep in buckets geo so we can do geo

01:09:54,950 --> 01:10:00,950
aggregations so stuff that falls within

01:09:56,840 --> 01:10:03,290
5 meters 10 meters 15 meters we can do

01:10:00,950 --> 01:10:05,750
that kind of stuff we can build where it

01:10:03,290 --> 01:10:07,970
falls on a geo hash grid we can also

01:10:05,750 --> 01:10:10,430
perform geo centroid aggregations which

01:10:07,970 --> 01:10:12,680
are really an extension of geo hash ones

01:10:10,430 --> 01:10:15,080
so if things fall within to a grid and

01:10:12,680 --> 01:10:18,140
that grid is a representation of what

01:10:15,080 --> 01:10:21,620
the world looks like then say things

01:10:18,140 --> 01:10:24,620
happening in this particular part of a

01:10:21,620 --> 01:10:27,680
grid we might want to Center those to

01:10:24,620 --> 01:10:29,180
the average of an overall area so just

01:10:27,680 --> 01:10:31,340
move those particular points or

01:10:29,180 --> 01:10:33,350
geocentric it's much easier to show with

01:10:31,340 --> 01:10:34,820
a demonstration but I probably don't

01:10:33,350 --> 01:10:36,560
have time for that and matrix

01:10:34,820 --> 01:10:39,440
aggregations which is essentially build

01:10:36,560 --> 01:10:40,910
a mate matrix from a particular field

01:10:39,440 --> 01:10:43,690
within a document and give you

01:10:40,910 --> 01:10:46,730
statistics about that particular field

01:10:43,690 --> 01:10:50,510
making sense of data have a looking

01:10:46,730 --> 01:10:53,450
cabaña just have a look at that very

01:10:50,510 --> 01:10:57,530
quickly so I index the all of the data

01:10:53,450 --> 01:11:00,070
in the dotnet quart element telemetry

01:10:57,530 --> 01:11:03,110
so anyone else have a look at that data

01:11:00,070 --> 01:11:05,090
no so they really sticky on the tooling

01:11:03,110 --> 01:11:07,310
what was happening there and they've

01:11:05,090 --> 01:11:10,550
released these in tab separated value

01:11:07,310 --> 01:11:12,560
files so I just wrote a very simple log

01:11:10,550 --> 01:11:15,350
stash configuration happy to share that

01:11:12,560 --> 01:11:17,150
afterwards to index that data into

01:11:15,350 --> 01:11:20,780
elasticsearch and then build this

01:11:17,150 --> 01:11:22,700
visualization here so if this was this I

01:11:20,780 --> 01:11:24,410
mean this is static data here so this is

01:11:22,700 --> 01:11:26,810
never going to change

01:11:24,410 --> 01:11:29,300
but you can build these dashboards on

01:11:26,810 --> 01:11:31,190
data and have these dashboards update

01:11:29,300 --> 01:11:32,930
real time limit as that data is coming

01:11:31,190 --> 01:11:34,850
in so we've just built some pretty

01:11:32,930 --> 01:11:37,760
charts all of these charts are built in

01:11:34,850 --> 01:11:39,800
to Cabana and we can click on these and

01:11:37,760 --> 01:11:41,420
when we click on a particular area all

01:11:39,800 --> 01:11:45,700
the other charts are going to refresh

01:11:41,420 --> 01:11:45,700
based on our particular choice there

01:11:49,000 --> 01:11:54,350
that was really a very very brief

01:11:51,950 --> 01:11:59,540
overview and I realize I've gone way way

01:11:54,350 --> 01:12:01,190
over time this typically happens but

01:11:59,540 --> 01:12:04,220
yeah hopefully that's given you a flavor

01:12:01,190 --> 01:12:06,560
of the kind of searches you can do and

01:12:04,220 --> 01:12:09,410
the kind of things you can also

01:12:06,560 --> 01:12:11,180
aggregate on I'm happy to chat for

01:12:09,410 --> 01:12:13,580
however long you want to spend

01:12:11,180 --> 01:12:17,420
afterwards going through particular

01:12:13,580 --> 01:12:21,140
issues you might be facing just wanted

01:12:17,420 --> 01:12:24,230
to call out this new Search app so we

01:12:21,140 --> 01:12:26,870
have a repository where we do a

01:12:24,230 --> 01:12:29,780
walk-through of building out a dotnet

01:12:26,870 --> 01:12:32,120
core web application that uses elastic

01:12:29,780 --> 01:12:34,340
search to perform search on you get

01:12:32,120 --> 01:12:39,740
packages because we all know the new get

01:12:34,340 --> 01:12:41,390
search is good right and in fact we can

01:12:39,740 --> 01:12:45,440
go in here we can go and hit the site if

01:12:41,390 --> 01:12:54,730
we want to go and just go to news search

01:12:45,440 --> 01:13:00,580
CH it should be up it's on assure so I

01:12:54,730 --> 01:13:03,500
have to give it a nudge I'm on Wi-Fi a

01:13:00,580 --> 01:13:06,050
there we go and we can go and search for

01:13:03,500 --> 01:13:08,780
things here so all of the data here is

01:13:06,050 --> 01:13:12,790
entirely powered by elastic search so

01:13:08,780 --> 01:13:12,790
for us going to search for elastics

01:13:13,600 --> 01:13:18,500
yeah you can have a play with that and

01:13:16,490 --> 01:13:21,290
we can page through data here we can

01:13:18,500 --> 01:13:23,390
bring back 50 packages and we can click

01:13:21,290 --> 01:13:26,150
on each individual one of these and we

01:13:23,390 --> 01:13:28,400
can see that this query score here we

01:13:26,150 --> 01:13:31,040
have some facets or aggregations down

01:13:28,400 --> 01:13:33,680
the side here as well so Microsoft seem

01:13:31,040 --> 01:13:35,960
to be doing pretty well generally for

01:13:33,680 --> 01:13:37,790
packages on there because we probably

01:13:35,960 --> 01:13:41,030
have a few different versions for dotnet

01:13:37,790 --> 01:13:46,190
core doing X and then various other

01:13:41,030 --> 01:13:48,530
strands so yeah please go check that out

01:13:46,190 --> 01:13:50,920
and have a look it's a very

01:13:48,530 --> 01:13:53,120
comprehensive walkthrough there of

01:13:50,920 --> 01:13:55,430
dealing with different kinds of queries

01:13:53,120 --> 01:13:57,410
function score queries match queries

01:13:55,430 --> 01:13:59,810
query string queries and it walks

01:13:57,410 --> 01:14:02,630
through some of the pros cons that

01:13:59,810 --> 01:14:05,060
trades off the pitfalls that kind of

01:14:02,630 --> 01:14:07,070
stuff so highly recommend on that

01:14:05,060 --> 01:14:10,040
particular branch as well go and have a

01:14:07,070 --> 01:14:12,770
look at that some other resources if you

01:14:10,040 --> 01:14:14,690
are very very interested we have the

01:14:12,770 --> 01:14:16,190
reference online so obviously a

01:14:14,690 --> 01:14:18,770
reference for all of the versions we've

01:14:16,190 --> 01:14:20,840
completed but also next to the reference

01:14:18,770 --> 01:14:22,700
we also have the definitive guide online

01:14:20,840 --> 01:14:25,010
so you can buy it as a book currently

01:14:22,700 --> 01:14:28,490
but the book is based on elasticsearch

01:14:25,010 --> 01:14:31,010
1.3 and we're on elasticsearch 563 right

01:14:28,490 --> 01:14:34,490
now about to release about to release

01:14:31,010 --> 01:14:37,940
elasticsearch 6 so the definitive guide

01:14:34,490 --> 01:14:40,370
online is currently at version along

01:14:37,940 --> 01:14:42,230
with elasticsearch 2 it's in the process

01:14:40,370 --> 01:14:44,750
of being updated for six because there's

01:14:42,230 --> 01:14:48,080
a lot of changes but you have a read of

01:14:44,750 --> 01:14:50,390
that it's a really great book to get

01:14:48,080 --> 01:14:52,640
started with stuff don't know clients

01:14:50,390 --> 01:14:56,420
there go and have a look far some issues

01:14:52,640 --> 01:14:59,390
open pr's we will of be ours just have a

01:14:56,420 --> 01:15:01,610
play with it and yeah follow any issues

01:14:59,390 --> 01:15:04,130
there that you come across as well very

01:15:01,610 --> 01:15:06,050
open to talking about you know problems

01:15:04,130 --> 01:15:08,660
or improvements that can be made to the

01:15:06,050 --> 01:15:10,730
client and then the documentation for

01:15:08,660 --> 01:15:12,770
the client exists on the elastic site as

01:15:10,730 --> 01:15:15,800
well and that's fairly fairly

01:15:12,770 --> 01:15:17,690
comprehensive it's generated entirely

01:15:15,800 --> 01:15:20,300
from our test so it doesn't drift we use

01:15:17,690 --> 01:15:23,390
Rosalind to walk the AST and generate

01:15:20,300 --> 01:15:27,470
the documentation from that

01:15:23,390 --> 01:15:29,390
finally one more thing we do have our

01:15:27,470 --> 01:15:31,700
user conference our global user

01:15:29,390 --> 01:15:34,250
conference in San Francisco February

01:15:31,700 --> 01:15:36,890
27th so much the first I am NOT trying

01:15:34,250 --> 01:15:39,950
to sell this to you yg I am because it's

01:15:36,890 --> 01:15:41,830
awesome but I wanted to call out the

01:15:39,950 --> 01:15:44,330
fact that there's a call for papers here

01:15:41,830 --> 01:15:47,930
31st of October so if you have some

01:15:44,330 --> 01:15:50,240
interesting use cases already please

01:15:47,930 --> 01:15:52,700
submit a paper for this elasticsearch

01:15:50,240 --> 01:15:55,670
will obviously you know help you get

01:15:52,700 --> 01:15:57,080
there and accommodation so you know if

01:15:55,670 --> 01:15:59,060
you have some interesting use cases

01:15:57,080 --> 01:16:00,950
please please do come and Shera

01:15:59,060 --> 01:16:03,410
share them with us we've had some really

01:16:00,950 --> 01:16:08,210
cool people talk from you know uber

01:16:03,410 --> 01:16:09,620
tinder NASA United States Air Force a

01:16:08,210 --> 01:16:12,020
bunch of different people have talked

01:16:09,620 --> 01:16:13,970
about various different things and it's

01:16:12,020 --> 01:16:16,810
usually really interesting stuff that I

01:16:13,970 --> 01:16:19,340
I would never think to use the stack for

01:16:16,810 --> 01:16:21,440
so yeah thank you very much hopefully

01:16:19,340 --> 01:16:22,190
that was that was useful for you thank

01:16:21,440 --> 01:16:30,630
you

01:16:22,190 --> 01:16:30,630

YouTube URL: https://www.youtube.com/watch?v=8G2Vcdcm8WU


