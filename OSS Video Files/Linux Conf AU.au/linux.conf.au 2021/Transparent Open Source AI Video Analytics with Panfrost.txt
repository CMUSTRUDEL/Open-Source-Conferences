Title: Transparent Open Source AI Video Analytics with Panfrost
Publication date: 2021-01-31
Playlist: linux.conf.au 2021
Description: 
	Aaron Boxer, Marcus Edel

https://lca2021.linux.org.au/schedule/presentation/53/

AI-powered video analytics is one of the most challenging applications of
AI to edge devices, given the edge's power, compute, and memory limitations.
This area is currently dominated by NVidia Deep Stream, which suffers from:

1. vendor-lock-in from CUDA language and NVidia hardware

2. lack of transparency into low level tensor operations and algorithms due to
closed source drivers and libraries.

Can we give freedom of choice back to AI multimedia developers ?

Can we build a pure open source stack, running from application to ML framework
down to GPU driver, which allows complete transparency into the ML inference
workflow ?

The new Panfrost open source driver for Mali GPUs is solving this problem
on the edge by enabling a fast and efficient machine learning stack
running pure open source. Combining this with TensorFlow Lite and GStreamer,
we get a powerful open source AI stack for video analytics. And because
the stack is open from top to bottom, we get visibility into the complete
inference process, allowing us to better understand and explain how an 
analytic model makes its predictions.

The ability to explain how a model infers it's results (explainability) is an
increasingly desirable ML feature, particularly in applications that have an impact
on privacy, such as video facial recognition. Explainability allows us to build
ethical and trustworthy ML systems known to be free from bias. 
Closed source blobs and libraries interfere with explainability by hiding
crucial computations from view. 

In this talk, we will walk through the process of building an AI-driven multimedia
pipeline on top of a completely open source inference stack:
open source GPU driver, machine learning framework and machine learning models.
We will share what we have learned about optimizing these models to run fast
on resource-constrained hardware such as the Rockchip RK3399. And we will discuss
how this completely open stack is a critical component of ethical and trustworthy
video analytics.

linux.conf.au is a conference about the Linux operating system, and all aspects of the thriving ecosystem of Free and Open Source Software that has grown up around it. Run since 1999, in a different Australian or New Zealand city each year, by a team of local volunteers, LCA invites more than 500 people to learn from the people who shape the future of Open Source. For more information on the conference see https://linux.conf.au/

Produced by Next Day Video Australia: https://nextdayvideo.com.au

#linux.conf.au #linux #foss #opensource

Sun Jan 24 14:25:00 2021 at Pia Andrews Conservatory
Captions: 
	00:00:10,820 --> 00:00:14,060
[Music]

00:00:15,120 --> 00:00:20,640
everyone it is great to have you back

00:00:18,080 --> 00:00:21,439
you are about to hear from aaron boxer

00:00:20,640 --> 00:00:25,039
and marcus

00:00:21,439 --> 00:00:25,840
adele now marcus's camera is having some

00:00:25,039 --> 00:00:27,920
problems

00:00:25,840 --> 00:00:30,480
so but marcus is in this talk and you

00:00:27,920 --> 00:00:32,480
will be hearing from him soon

00:00:30,480 --> 00:00:34,640
aaron and marcus have found yet another

00:00:32,480 --> 00:00:35,040
computing domain whose dominant tooling

00:00:34,640 --> 00:00:37,600
isn't

00:00:35,040 --> 00:00:38,960
open source namely the nvidia deep

00:00:37,600 --> 00:00:41,600
stream stack

00:00:38,960 --> 00:00:42,800
and this is used for ai powered video

00:00:41,600 --> 00:00:45,039
analysis

00:00:42,800 --> 00:00:45,920
but there are efforts to cast an open

00:00:45,039 --> 00:00:48,559
source light

00:00:45,920 --> 00:00:49,680
into this domain though aaron and marcus

00:00:48,559 --> 00:00:52,559
are going to tell us

00:00:49,680 --> 00:00:54,160
about panfrost an open source driver for

00:00:52,559 --> 00:00:56,960
the mali gpus

00:00:54,160 --> 00:00:57,280
and how this can can be combined with

00:00:56,960 --> 00:01:00,559
other

00:00:57,280 --> 00:01:01,440
open source components now they are

00:01:00,559 --> 00:01:03,440
happy to take

00:01:01,440 --> 00:01:05,519
questions so if you can write your

00:01:03,440 --> 00:01:07,680
questions into the chat channel

00:01:05,519 --> 00:01:09,520
and where if we've got time we're going

00:01:07,680 --> 00:01:11,280
to run through them

00:01:09,520 --> 00:01:13,520
to use up our time at the end of the

00:01:11,280 --> 00:01:14,320
talk otherwise they are happy to jump

00:01:13,520 --> 00:01:16,400
into the

00:01:14,320 --> 00:01:17,680
chat to talk with you about your

00:01:16,400 --> 00:01:20,960
questions

00:01:17,680 --> 00:01:23,600
marcus and aaron take it away

00:01:20,960 --> 00:01:25,280
thank you very much um so hello and

00:01:23,600 --> 00:01:27,280
welcome to our talk

00:01:25,280 --> 00:01:29,200
transparent open source video analytics

00:01:27,280 --> 00:01:31,520
with panfrost i'm aaron

00:01:29,200 --> 00:01:33,119
and this is marcus and fortunately his

00:01:31,520 --> 00:01:35,360
uh closed source

00:01:33,119 --> 00:01:36,479
webcam has decided to not cooperate

00:01:35,360 --> 00:01:38,880
tonight but

00:01:36,479 --> 00:01:41,439
together we're going to tell you about

00:01:38,880 --> 00:01:44,640
how we built a purely open source

00:01:41,439 --> 00:01:46,960
video analytics pipeline using pan frost

00:01:44,640 --> 00:01:48,240
and we're also going to talk about how

00:01:46,960 --> 00:01:50,640
we can gain insight

00:01:48,240 --> 00:01:53,200
into the predictions that this kind of

00:01:50,640 --> 00:01:56,240
pipeline can make

00:01:53,200 --> 00:01:59,520
so to begin i'm going to give a very

00:01:56,240 --> 00:02:00,880
quick overview of object detection and

00:01:59,520 --> 00:02:03,280
the ai field

00:02:00,880 --> 00:02:05,360
it's a very vast field and growing

00:02:03,280 --> 00:02:08,080
rapidly but we're going to

00:02:05,360 --> 00:02:09,679
stick to some of the areas that relate

00:02:08,080 --> 00:02:12,080
to video analytics

00:02:09,679 --> 00:02:13,840
so here on the slide we have four

00:02:12,080 --> 00:02:17,120
different types of object detection

00:02:13,840 --> 00:02:20,160
tasks moving from easier to

00:02:17,120 --> 00:02:21,520
more difficult from left to right so the

00:02:20,160 --> 00:02:24,080
first

00:02:21,520 --> 00:02:25,680
task is called classification where

00:02:24,080 --> 00:02:28,480
we're given a video

00:02:25,680 --> 00:02:30,400
frame or an image and we want to

00:02:28,480 --> 00:02:32,879
determine the most important object

00:02:30,400 --> 00:02:34,239
in that image in this case it's a cat

00:02:32,879 --> 00:02:37,120
and this we just have to label that

00:02:34,239 --> 00:02:40,160
image as a cat moving to the next

00:02:37,120 --> 00:02:43,280
uh task is localization where we're able

00:02:40,160 --> 00:02:45,519
to put a bounding box around that cat

00:02:43,280 --> 00:02:47,280
and moving to the right again we have

00:02:45,519 --> 00:02:49,440
now have cats and dogs

00:02:47,280 --> 00:02:50,800
and we're able to identify them oh and

00:02:49,440 --> 00:02:53,040
there's a duck

00:02:50,800 --> 00:02:54,720
and put bounding boxes around all those

00:02:53,040 --> 00:02:58,480
objects and finally on the far

00:02:54,720 --> 00:02:59,360
right it's a segmentation where we can

00:02:58,480 --> 00:03:01,680
put

00:02:59,360 --> 00:03:03,599
an outline around each important object

00:03:01,680 --> 00:03:05,599
in the scene

00:03:03,599 --> 00:03:06,800
one one interesting thing to note about

00:03:05,599 --> 00:03:09,760
object detection is

00:03:06,800 --> 00:03:11,440
up until let's say 20 years ago it was

00:03:09,760 --> 00:03:13,599
generally believed that it would take a

00:03:11,440 --> 00:03:14,560
very long time maybe a hundred years for

00:03:13,599 --> 00:03:16,800
a computer

00:03:14,560 --> 00:03:17,760
to match a human at object detection

00:03:16,800 --> 00:03:19,680
just because

00:03:17,760 --> 00:03:21,440
our visual system is very good at

00:03:19,680 --> 00:03:22,879
detecting objects and we

00:03:21,440 --> 00:03:24,720
are it's been evolving for hundreds of

00:03:22,879 --> 00:03:28,000
millions of years and yet

00:03:24,720 --> 00:03:29,680
as of around five years ago we now have

00:03:28,000 --> 00:03:32,000
computer systems that can beat the

00:03:29,680 --> 00:03:34,480
average human at detecting objects so

00:03:32,000 --> 00:03:35,599
we've moved very fast in a very short

00:03:34,480 --> 00:03:39,840
period of time

00:03:35,599 --> 00:03:41,680
and let's find out how this came about

00:03:39,840 --> 00:03:44,239
the current approach to ai is based on a

00:03:41,680 --> 00:03:46,959
philosophy called connectionism

00:03:44,239 --> 00:03:48,159
and that uses the mammalian nervous

00:03:46,959 --> 00:03:50,000
system

00:03:48,159 --> 00:03:51,440
as a model for building an intelligent

00:03:50,000 --> 00:03:53,120
system so here

00:03:51,440 --> 00:03:55,519
in the slide we have two neurons

00:03:53,120 --> 00:03:58,000
connected via synapse

00:03:55,519 --> 00:03:59,360
and the neuron on the left has inputs

00:03:58,000 --> 00:04:02,000
from other neurons

00:03:59,360 --> 00:04:03,599
and those inputs affect the voltage in

00:04:02,000 --> 00:04:04,239
that particular neuron when the voltage

00:04:03,599 --> 00:04:05,840
reaches

00:04:04,239 --> 00:04:07,760
a certain level called the activation

00:04:05,840 --> 00:04:09,840
potential it will fire

00:04:07,760 --> 00:04:10,799
its signal down the synapse to its

00:04:09,840 --> 00:04:14,000
connected

00:04:10,799 --> 00:04:15,439
neuron and when that neuron reaches its

00:04:14,000 --> 00:04:18,079
activation potential

00:04:15,439 --> 00:04:19,600
it will fire so each neuron individually

00:04:18,079 --> 00:04:20,720
is relatively simple but when you get

00:04:19,600 --> 00:04:23,280
billions of neurons

00:04:20,720 --> 00:04:24,080
connected together and also when you can

00:04:23,280 --> 00:04:25,520
change

00:04:24,080 --> 00:04:26,800
the connections between the neurons

00:04:25,520 --> 00:04:27,840
based on your environment when you can

00:04:26,800 --> 00:04:29,360
learn

00:04:27,840 --> 00:04:32,080
then you get an intelligent system and

00:04:29,360 --> 00:04:35,120
that's how our brains work so

00:04:32,080 --> 00:04:38,160
connectionism abstracts this

00:04:35,120 --> 00:04:41,360
biological system to a computer

00:04:38,160 --> 00:04:42,880
model called a deep learning network so

00:04:41,360 --> 00:04:45,600
the neurons

00:04:42,880 --> 00:04:46,800
that we saw are now those nodes those

00:04:45,600 --> 00:04:49,120
around

00:04:46,800 --> 00:04:50,080
circles and instead of voltages we now

00:04:49,120 --> 00:04:53,360
have numbers

00:04:50,080 --> 00:04:55,440
flowing through the network uh so

00:04:53,360 --> 00:04:56,639
on the left there are input nodes where

00:04:55,440 --> 00:04:58,720
the

00:04:56,639 --> 00:05:00,720
the data comes in then there are hidden

00:04:58,720 --> 00:05:03,360
nodes uh that receive

00:05:00,720 --> 00:05:04,400
those numbers and pass them down the

00:05:03,360 --> 00:05:06,400
network and then on the

00:05:04,400 --> 00:05:08,320
on the right there are the output nodes

00:05:06,400 --> 00:05:10,880
that give us our result

00:05:08,320 --> 00:05:12,720
and since there are uh three hidden

00:05:10,880 --> 00:05:15,759
layers between the input and the output

00:05:12,720 --> 00:05:18,560
we call it a deep learning network

00:05:15,759 --> 00:05:19,360
um important to note that the arrows

00:05:18,560 --> 00:05:22,560
between

00:05:19,360 --> 00:05:24,479
their neurons for each arrow there's an

00:05:22,560 --> 00:05:26,880
associated weight which is just a number

00:05:24,479 --> 00:05:27,919
and that gets multiplied that multiplies

00:05:26,880 --> 00:05:31,120
the output

00:05:27,919 --> 00:05:34,320
of the um the neuron coming in

00:05:31,120 --> 00:05:38,160
and um he can

00:05:34,320 --> 00:05:40,080
excuse me we can change the value

00:05:38,160 --> 00:05:41,919
of those weights and that's how the

00:05:40,080 --> 00:05:42,560
model actually responds to the input

00:05:41,919 --> 00:05:44,960
data

00:05:42,560 --> 00:05:45,759
those those weights get modified and

00:05:44,960 --> 00:05:47,280
there's also

00:05:45,759 --> 00:05:49,680
something called an activation function

00:05:47,280 --> 00:05:50,960
which affects the output of one neuron

00:05:49,680 --> 00:05:53,199
coming into the next

00:05:50,960 --> 00:05:55,680
and that adds some non-linearity to the

00:05:53,199 --> 00:05:57,680
system because if it was just a linear

00:05:55,680 --> 00:05:59,919
output then we would just have a large

00:05:57,680 --> 00:06:01,440
linear system of equations which is

00:05:59,919 --> 00:06:03,199
uh doesn't really capture some of the

00:06:01,440 --> 00:06:04,840
more interesting tasks

00:06:03,199 --> 00:06:07,759
in the world which which have

00:06:04,840 --> 00:06:11,919
non-linearities

00:06:07,759 --> 00:06:14,960
and so once we have a model set up

00:06:11,919 --> 00:06:16,080
we um want to train the model with data

00:06:14,960 --> 00:06:18,560
that we've already

00:06:16,080 --> 00:06:19,360
um that we already stand we know what

00:06:18,560 --> 00:06:21,120
the model

00:06:19,360 --> 00:06:23,360
should output for the training data so

00:06:21,120 --> 00:06:26,000
we feed the training data into the

00:06:23,360 --> 00:06:26,400
nodes and we adjust the weights uh based

00:06:26,000 --> 00:06:28,720
on

00:06:26,400 --> 00:06:30,000
what the the model outputs for the

00:06:28,720 --> 00:06:33,039
initial data

00:06:30,000 --> 00:06:34,639
so we want to

00:06:33,039 --> 00:06:36,000
iterate and adjust the weights and

00:06:34,639 --> 00:06:36,800
hopefully those weights are going to

00:06:36,000 --> 00:06:39,759
converge

00:06:36,800 --> 00:06:41,199
to some fixed points and once it does

00:06:39,759 --> 00:06:42,960
that then the model is considered

00:06:41,199 --> 00:06:45,759
trained it's learned the patterns

00:06:42,960 --> 00:06:46,639
uh from the training data and is ready

00:06:45,759 --> 00:06:48,080
for

00:06:46,639 --> 00:06:51,520
new data that we've never seen before

00:06:48,080 --> 00:06:54,960
this is typically done on a discrete gpu

00:06:51,520 --> 00:06:57,280
because it requires a lot of horsepower

00:06:54,960 --> 00:06:58,880
once the model is trained we're ready

00:06:57,280 --> 00:07:00,960
for inference and that's when we

00:06:58,880 --> 00:07:02,639
we receive data that we've never seen

00:07:00,960 --> 00:07:03,919
before and

00:07:02,639 --> 00:07:06,560
if the model has been trained well

00:07:03,919 --> 00:07:08,479
enough then the outputs for the new data

00:07:06,560 --> 00:07:10,800
should match the patterns that is

00:07:08,479 --> 00:07:13,520
learned from the training data

00:07:10,800 --> 00:07:15,199
and when you do inference typically it

00:07:13,520 --> 00:07:17,759
doesn't require as much compute

00:07:15,199 --> 00:07:19,039
resources and so this is more suitable

00:07:17,759 --> 00:07:23,360
for a low power

00:07:19,039 --> 00:07:26,160
edge device which is what we focused on

00:07:23,360 --> 00:07:26,639
and so we'll talk a little bit about

00:07:26,160 --> 00:07:30,560
running

00:07:26,639 --> 00:07:33,840
ai on an edge device so

00:07:30,560 --> 00:07:36,960
typically an edge device is

00:07:33,840 --> 00:07:38,479
low power low memory low compute and so

00:07:36,960 --> 00:07:40,160
when you're trying to run machine

00:07:38,479 --> 00:07:42,000
learning on these devices

00:07:40,160 --> 00:07:44,080
it's very important to be as efficient

00:07:42,000 --> 00:07:45,919
as possible and so we have to do a

00:07:44,080 --> 00:07:46,560
couple of things to the network that

00:07:45,919 --> 00:07:49,599
we've

00:07:46,560 --> 00:07:51,919
designed to make it uh efficient and

00:07:49,599 --> 00:07:53,039
those tasks are called pruning and

00:07:51,919 --> 00:07:55,919
quantization

00:07:53,039 --> 00:07:56,560
so if we go back to our deep learning

00:07:55,919 --> 00:07:58,400
network

00:07:56,560 --> 00:08:00,240
we see all the arrows connecting the

00:07:58,400 --> 00:08:01,280
nodes we can actually remove some of

00:08:00,240 --> 00:08:03,919
those arrows

00:08:01,280 --> 00:08:05,680
and prune the network and maintain the

00:08:03,919 --> 00:08:06,479
accuracy at the same time so whenever we

00:08:05,680 --> 00:08:07,520
remove

00:08:06,479 --> 00:08:09,520
an error we're reducing our

00:08:07,520 --> 00:08:11,039
computational load

00:08:09,520 --> 00:08:12,960
the second thing we can do is call

00:08:11,039 --> 00:08:16,240
quantization where

00:08:12,960 --> 00:08:20,000
we take the weights that are um that are

00:08:16,240 --> 00:08:22,160
associated with each arrow and if it's

00:08:20,000 --> 00:08:24,319
stored as a 32-bit number we can

00:08:22,160 --> 00:08:25,680
actually store it let's say a 16 or

00:08:24,319 --> 00:08:28,000
8-bit

00:08:25,680 --> 00:08:29,360
and that reduces our memory requirements

00:08:28,000 --> 00:08:32,000
and it also allows us

00:08:29,360 --> 00:08:33,599
if we have vector hardware we can we can

00:08:32,000 --> 00:08:35,440
actually operate on multiple

00:08:33,599 --> 00:08:37,200
weights with the same number of cpu

00:08:35,440 --> 00:08:37,839
cycles so four eight bit weights instead

00:08:37,200 --> 00:08:41,120
of one

00:08:37,839 --> 00:08:44,000
32-bit weight so those two

00:08:41,120 --> 00:08:46,160
activities will help us get our

00:08:44,000 --> 00:08:48,160
efficiency

00:08:46,160 --> 00:08:50,080
up while maintaining the same level of

00:08:48,160 --> 00:08:52,720
accuracy

00:08:50,080 --> 00:08:55,360
now we can talk about the hardware

00:08:52,720 --> 00:08:58,160
that's available for this type of

00:08:55,360 --> 00:08:59,760
or working on the edge it's not it's

00:08:58,160 --> 00:09:03,920
kind of limited at the moment

00:08:59,760 --> 00:09:06,640
a popular solution is the nvidia tegra

00:09:03,920 --> 00:09:08,800
that you see over pictured over here the

00:09:06,640 --> 00:09:09,839
drawback of course is that it's closed

00:09:08,800 --> 00:09:12,480
source

00:09:09,839 --> 00:09:13,839
and um there's vendor lock in with cuda

00:09:12,480 --> 00:09:15,680
library

00:09:13,839 --> 00:09:17,200
which is the language that only works

00:09:15,680 --> 00:09:19,440
works only with nvidia

00:09:17,200 --> 00:09:21,279
cards so if you want to customize your

00:09:19,440 --> 00:09:22,320
model with some custom operations

00:09:21,279 --> 00:09:25,279
you'd have to write that in cuda and

00:09:22,320 --> 00:09:28,800
then you're stuck with cuda systems

00:09:25,279 --> 00:09:31,839
it's also rather expensive so

00:09:28,800 --> 00:09:34,320
at calabro we wanted to build

00:09:31,839 --> 00:09:35,440
an open source stack that would free the

00:09:34,320 --> 00:09:38,720
developers from

00:09:35,440 --> 00:09:39,920
this lock-in and provide more choice in

00:09:38,720 --> 00:09:45,040
the marketplace

00:09:39,920 --> 00:09:48,000
and so this is where pan frost comes in

00:09:45,040 --> 00:09:49,040
and pan frost is an open source driver

00:09:48,000 --> 00:09:52,160
for arm

00:09:49,040 --> 00:09:54,399
mali gpus

00:09:52,160 --> 00:09:57,200
and it's fully upstreamed into mesa

00:09:54,399 --> 00:09:58,640
which is the linux graphics subsystem

00:09:57,200 --> 00:10:01,839
team lead is our very own alyssa

00:09:58,640 --> 00:10:04,320
rosensweig who's also at calabra

00:10:01,839 --> 00:10:05,519
so some of the features that pan frost

00:10:04,320 --> 00:10:08,320
has

00:10:05,519 --> 00:10:09,200
made it appealing for this project

00:10:08,320 --> 00:10:12,720
particularly

00:10:09,200 --> 00:10:13,120
it's recently supported gles 3.2 glas is

00:10:12,720 --> 00:10:15,279
the

00:10:13,120 --> 00:10:16,480
embedded version of the opengl graphics

00:10:15,279 --> 00:10:20,079
language

00:10:16,480 --> 00:10:21,360
and gles 3.2 supports compute shaders

00:10:20,079 --> 00:10:23,920
which are small

00:10:21,360 --> 00:10:26,240
programs that you can schedule from user

00:10:23,920 --> 00:10:27,440
space and schedule to run on the gpu

00:10:26,240 --> 00:10:30,640
itself

00:10:27,440 --> 00:10:32,399
so um here's a pathway for running our

00:10:30,640 --> 00:10:36,000
compute intensive

00:10:32,399 --> 00:10:38,640
neural network on the mali gpu

00:10:36,000 --> 00:10:39,680
and we'll see how that uh is important

00:10:38,640 --> 00:10:42,399
in a bit

00:10:39,680 --> 00:10:43,760
uh in terms of the hardware we chose the

00:10:42,399 --> 00:10:45,519
rock by four

00:10:43,760 --> 00:10:47,440
which is a very nice system it's got

00:10:45,519 --> 00:10:50,480
four core cpu

00:10:47,440 --> 00:10:54,880
four core mali gpu and

00:10:50,480 --> 00:10:57,440
four gigabytes of uh ddr ram

00:10:54,880 --> 00:10:58,320
and these systems unfortunately run a

00:10:57,440 --> 00:11:02,800
little

00:10:58,320 --> 00:11:05,120
hot so uh here's marcus's cooling system

00:11:02,800 --> 00:11:07,040
cooling solution um the the chip is

00:11:05,120 --> 00:11:07,680
buried somewhere underneath that huge

00:11:07,040 --> 00:11:10,399
fan

00:11:07,680 --> 00:11:12,320
um but don't try this at home um the

00:11:10,399 --> 00:11:13,279
alternative is just to use a large

00:11:12,320 --> 00:11:16,480
heatsink

00:11:13,279 --> 00:11:18,240
instead of the fan so

00:11:16,480 --> 00:11:19,680
so pan frost is in place we have the

00:11:18,240 --> 00:11:22,720
hardware the next

00:11:19,680 --> 00:11:24,800
uh level up the stack is a

00:11:22,720 --> 00:11:26,160
tool kit to manage the neural network

00:11:24,800 --> 00:11:29,680
manage the training

00:11:26,160 --> 00:11:31,680
and manage uh storage and of the actual

00:11:29,680 --> 00:11:35,440
model so we

00:11:31,680 --> 00:11:35,920
uh chose tensorflow lite to manage that

00:11:35,440 --> 00:11:38,480
level

00:11:35,920 --> 00:11:39,200
for a couple of reasons tensorflow lite

00:11:38,480 --> 00:11:42,160
is uh

00:11:39,200 --> 00:11:43,920
based on google's tensorflow library and

00:11:42,160 --> 00:11:47,040
it's designed for

00:11:43,920 --> 00:11:50,560
edge devices like mobile iot

00:11:47,040 --> 00:11:53,040
and they focus on android and ios

00:11:50,560 --> 00:11:54,959
but we were able to tweak it a little

00:11:53,040 --> 00:11:56,639
bit and we could get it to work with pan

00:11:54,959 --> 00:11:58,959
frost

00:11:56,639 --> 00:11:59,760
one notable feature for tensorflow lite

00:11:58,959 --> 00:12:02,800
is they have

00:11:59,760 --> 00:12:04,720
uh delegates which are ways of

00:12:02,800 --> 00:12:07,279
offloading the compute

00:12:04,720 --> 00:12:08,959
typically tensorflow lite will run on

00:12:07,279 --> 00:12:11,360
the cpu or the dsp

00:12:08,959 --> 00:12:12,560
of your device but with the delegate you

00:12:11,360 --> 00:12:15,839
can offload compute

00:12:12,560 --> 00:12:18,480
into the gpu or onto a second dsp

00:12:15,839 --> 00:12:20,079
and fortunately for us there is a gles

00:12:18,480 --> 00:12:22,639
3.2 delegate

00:12:20,079 --> 00:12:23,760
and so now we have a pathway from

00:12:22,639 --> 00:12:26,639
tensorflow lite

00:12:23,760 --> 00:12:28,480
to schedule all that compute on the gpu

00:12:26,639 --> 00:12:30,399
and that saves power and gives us better

00:12:28,480 --> 00:12:32,959
performance

00:12:30,399 --> 00:12:34,160
so we have the driver and we have the

00:12:32,959 --> 00:12:37,360
neural network

00:12:34,160 --> 00:12:39,200
layer the next layer in the stack

00:12:37,360 --> 00:12:42,079
is the multimedia framework to actually

00:12:39,200 --> 00:12:45,279
handle the video and to do that we chose

00:12:42,079 --> 00:12:47,040
to use g streamer

00:12:45,279 --> 00:12:48,959
g streamer is everyone's favorite open

00:12:47,040 --> 00:12:52,399
source multimedia framework

00:12:48,959 --> 00:12:55,760
it's graph based so you can

00:12:52,399 --> 00:12:58,639
create very complex bespoke

00:12:55,760 --> 00:13:00,320
pipelines for video and audio it

00:12:58,639 --> 00:13:02,560
supports a lot of hardware a lot of

00:13:00,320 --> 00:13:04,240
hardware and software codecs currently

00:13:02,560 --> 00:13:06,320
it does has no support

00:13:04,240 --> 00:13:07,839
for neural nets so we want to fill this

00:13:06,320 --> 00:13:10,959
gap

00:13:07,839 --> 00:13:13,600
to do that we had to get a grip

00:13:10,959 --> 00:13:14,800
on the different the many different

00:13:13,600 --> 00:13:15,760
neural network toolkits that are

00:13:14,800 --> 00:13:19,120
available

00:13:15,760 --> 00:13:20,800
tensorflow pi torch ammo pack

00:13:19,120 --> 00:13:22,720
we didn't want to have to manage that

00:13:20,800 --> 00:13:24,399
complexity but fortunately

00:13:22,720 --> 00:13:26,240
there is something called onyx which is

00:13:24,399 --> 00:13:28,320
open neural network exchange

00:13:26,240 --> 00:13:29,279
which sits on top of your favorite

00:13:28,320 --> 00:13:31,519
neural network

00:13:29,279 --> 00:13:32,639
library and provides a common format and

00:13:31,519 --> 00:13:34,880
set of operators so

00:13:32,639 --> 00:13:36,000
you can train your system on tensorflow

00:13:34,880 --> 00:13:37,760
for example

00:13:36,000 --> 00:13:40,079
and then you can do the inference on a

00:13:37,760 --> 00:13:44,160
different library and it all

00:13:40,079 --> 00:13:46,800
uh works using onyx so we decided to

00:13:44,160 --> 00:13:48,959
write our element based on onyx rather

00:13:46,800 --> 00:13:51,680
than a particular toolkit

00:13:48,959 --> 00:13:52,720
and so here's our design for our gst

00:13:51,680 --> 00:13:55,040
neural net

00:13:52,720 --> 00:13:57,279
class in gstreamer it's based on the

00:13:55,040 --> 00:14:00,639
video filter class

00:13:57,279 --> 00:14:02,560
it's it's a fairly simple uh design that

00:14:00,639 --> 00:14:05,360
the filter class will just take a video

00:14:02,560 --> 00:14:07,360
buffer do something do some computation

00:14:05,360 --> 00:14:08,399
and then either change the actual video

00:14:07,360 --> 00:14:11,680
frame or

00:14:08,399 --> 00:14:15,120
attach the metadata to the frame

00:14:11,680 --> 00:14:18,240
so um in our case our gst neural

00:14:15,120 --> 00:14:21,519
will receive a video buffer it will

00:14:18,240 --> 00:14:25,040
pass it through onyx uh after it

00:14:21,519 --> 00:14:27,839
um downsamples the buffer and the

00:14:25,040 --> 00:14:30,160
our the ai model will then make its

00:14:27,839 --> 00:14:33,199
prediction which gets passed back

00:14:30,160 --> 00:14:33,920
to our element and then the neural net

00:14:33,199 --> 00:14:37,120
will attach

00:14:33,920 --> 00:14:40,399
a custom meta object for metadata

00:14:37,120 --> 00:14:42,000
with the label and the overlay and then

00:14:40,399 --> 00:14:44,000
passes that to our optional

00:14:42,000 --> 00:14:46,000
overlay component which converts that

00:14:44,000 --> 00:14:48,399
into a composition meta

00:14:46,000 --> 00:14:49,839
which in gstreamer the gstreamer world

00:14:48,399 --> 00:14:52,160
is a very efficient way of

00:14:49,839 --> 00:14:53,040
creating an overlay that could then be

00:14:52,160 --> 00:14:56,880
composited

00:14:53,040 --> 00:14:59,920
if you want to display the video so

00:14:56,880 --> 00:15:02,240
um that's that's our design for the

00:14:59,920 --> 00:15:03,040
g streamer level so now we have the

00:15:02,240 --> 00:15:06,160
hardware

00:15:03,040 --> 00:15:09,199
the driver the toolkit tensorflow lite

00:15:06,160 --> 00:15:12,560
and the video the multimedia framework

00:15:09,199 --> 00:15:16,000
uh using g streamer and here's a little

00:15:12,560 --> 00:15:19,519
demo to show you our results

00:15:16,000 --> 00:15:23,199
and as you can see this is pre-covered

00:15:19,519 --> 00:15:24,959
no social distancing was wandering

00:15:23,199 --> 00:15:27,839
freely about the streets

00:15:24,959 --> 00:15:29,360
uh but you can see that the uh accuracy

00:15:27,839 --> 00:15:30,720
is quite good this is running on the

00:15:29,360 --> 00:15:32,800
rock pie

00:15:30,720 --> 00:15:33,839
um it has some issues with baby

00:15:32,800 --> 00:15:35,120
carriages which is things it's

00:15:33,839 --> 00:15:38,160
motorcycles

00:15:35,120 --> 00:15:40,320
so um we might wonder why it's

00:15:38,160 --> 00:15:42,320
making that prediction and um that's

00:15:40,320 --> 00:15:43,519
actually the second half of the talk

00:15:42,320 --> 00:15:45,920
which marcus will talk about

00:15:43,519 --> 00:15:49,600
explainability how we can explain

00:15:45,920 --> 00:15:53,199
how uh the system made that prediction

00:15:49,600 --> 00:15:56,160
and here's a summary of our results we

00:15:53,199 --> 00:16:00,639
used a model called optimize yolo

00:15:56,160 --> 00:16:03,360
and you can see that um accuracy is 83

00:16:00,639 --> 00:16:05,360
and it's running quite nicely on gpu so

00:16:03,360 --> 00:16:09,040
we're very very happy with

00:16:05,360 --> 00:16:12,000
the that result that we got

00:16:09,040 --> 00:16:12,399
and so um next we're going to actually

00:16:12,000 --> 00:16:14,959
talk

00:16:12,399 --> 00:16:15,519
about uh for example why that baby

00:16:14,959 --> 00:16:17,120
carriage

00:16:15,519 --> 00:16:18,720
how to get insight into why a baby

00:16:17,120 --> 00:16:19,120
carriage would look like a motorcycle to

00:16:18,720 --> 00:16:21,680
a

00:16:19,120 --> 00:16:22,480
neural net and that is explainability

00:16:21,680 --> 00:16:26,320
and marcus

00:16:22,480 --> 00:16:29,759
is going to tell you about that

00:16:26,320 --> 00:16:30,480
take it away thanks aaron that's right

00:16:29,759 --> 00:16:32,639
so the

00:16:30,480 --> 00:16:34,560
second part of the talk is going to be

00:16:32,639 --> 00:16:36,000
about explainability for machine

00:16:34,560 --> 00:16:38,639
learning models

00:16:36,000 --> 00:16:40,320
and to be clear from the very beginning

00:16:38,639 --> 00:16:43,040
we are not going to go into much

00:16:40,320 --> 00:16:44,320
details about specific deep learning

00:16:43,040 --> 00:16:46,639
techniques

00:16:44,320 --> 00:16:49,600
i think a lot of academic works in this

00:16:46,639 --> 00:16:51,839
area touches on these specifics

00:16:49,600 --> 00:16:53,920
we are much more concerned about the the

00:16:51,839 --> 00:16:56,320
data regarding human behavior

00:16:53,920 --> 00:16:57,920
is someone gonna pay their bill if

00:16:56,320 --> 00:17:00,399
someone gonna default

00:16:57,920 --> 00:17:02,399
back on their loan as someone you know

00:17:00,399 --> 00:17:04,000
turn from one phone company to another

00:17:02,399 --> 00:17:05,839
phone company

00:17:04,000 --> 00:17:08,319
does the phrase recognition

00:17:05,839 --> 00:17:09,039
implementation identify the right group

00:17:08,319 --> 00:17:12,240
of people

00:17:09,039 --> 00:17:12,640
or is the object detection model picking

00:17:12,240 --> 00:17:14,880
up

00:17:12,640 --> 00:17:16,160
the right features um those kinds of

00:17:14,880 --> 00:17:18,480
problems

00:17:16,160 --> 00:17:20,000
and yeah there's a lot of good things

00:17:18,480 --> 00:17:22,799
going on when it comes to

00:17:20,000 --> 00:17:24,720
explainability but there's also a lot of

00:17:22,799 --> 00:17:27,280
bad and ugly things going on

00:17:24,720 --> 00:17:29,280
uh when it comes to explain a certain

00:17:27,280 --> 00:17:32,559
type of predictive model so i'm

00:17:29,280 --> 00:17:35,360
i'm going to start from a very broad

00:17:32,559 --> 00:17:35,679
non-technical perspective and slowly

00:17:35,360 --> 00:17:38,720
move

00:17:35,679 --> 00:17:41,840
into details and yeah if you have

00:17:38,720 --> 00:17:43,919
any kind of clarification question as

00:17:41,840 --> 00:17:46,720
as we going along just put it in a chat

00:17:43,919 --> 00:17:49,760
and we are trying to to clear it up

00:17:46,720 --> 00:17:51,360
okay so um yeah what what is machine

00:17:49,760 --> 00:17:54,320
learning explainability

00:17:51,360 --> 00:17:55,120
so assuming you know what uh machine

00:17:54,320 --> 00:17:57,360
learning is

00:17:55,120 --> 00:17:58,160
you know machine learning is this um sub

00:17:57,360 --> 00:18:01,200
discipline

00:17:58,160 --> 00:18:04,240
of ai that that learned these um

00:18:01,200 --> 00:18:07,280
very complex functions from from data

00:18:04,240 --> 00:18:09,440
um that's typically trying to predict or

00:18:07,280 --> 00:18:12,799
classify or even discriminate between

00:18:09,440 --> 00:18:15,520
different types of um phenomena right

00:18:12,799 --> 00:18:16,960
so yeah the the problem is that these

00:18:15,520 --> 00:18:19,840
functions become

00:18:16,960 --> 00:18:21,200
very very complex and difficult to

00:18:19,840 --> 00:18:24,000
interpret difficult

00:18:21,200 --> 00:18:24,400
to explain to your boss how they work

00:18:24,000 --> 00:18:27,039
and

00:18:24,400 --> 00:18:28,000
when i say difficult i mean borderline

00:18:27,039 --> 00:18:30,960
possible

00:18:28,000 --> 00:18:33,039
and yeah so they they become difficult

00:18:30,960 --> 00:18:35,440
to explain to co-workers how they work

00:18:33,039 --> 00:18:36,160
difficult to your customers how they

00:18:35,440 --> 00:18:38,559
work

00:18:36,160 --> 00:18:39,440
so they tend to be very very accurate

00:18:38,559 --> 00:18:42,480
predictors but

00:18:39,440 --> 00:18:44,799
very difficult to explain and yeah

00:18:42,480 --> 00:18:47,280
there are a lot of the definitions um

00:18:44,799 --> 00:18:50,000
what machine learning explainability is

00:18:47,280 --> 00:18:52,240
um the one here shown is one of the

00:18:50,000 --> 00:18:53,679
these symbols and also one of my my

00:18:52,240 --> 00:18:57,120
favorite quotes

00:18:53,679 --> 00:19:00,080
about machine learning explainability so

00:18:57,120 --> 00:19:01,200
yeah we are looking for for a way to

00:19:00,080 --> 00:19:04,320
present or

00:19:01,200 --> 00:19:06,559
explain to a human being what um complex

00:19:04,320 --> 00:19:07,679
mathematical function that's been

00:19:06,559 --> 00:19:10,640
trained

00:19:07,679 --> 00:19:10,960
from from data is doing and if you have

00:19:10,640 --> 00:19:13,520
not

00:19:10,960 --> 00:19:15,360
read um thoroughly rigorous science of

00:19:13,520 --> 00:19:17,679
interpreting machine learning

00:19:15,360 --> 00:19:18,400
um i mean it's it's not even a long long

00:19:17,679 --> 00:19:20,799
paper

00:19:18,400 --> 00:19:22,000
i i highly recommend reading it it's

00:19:20,799 --> 00:19:24,640
very approachable

00:19:22,000 --> 00:19:25,919
even for say the business analyst

00:19:24,640 --> 00:19:28,000
community

00:19:25,919 --> 00:19:29,039
would probably be able to get something

00:19:28,000 --> 00:19:31,039
out of it

00:19:29,039 --> 00:19:33,360
and of course collaborator is not the

00:19:31,039 --> 00:19:35,360
only one thinking about machine learning

00:19:33,360 --> 00:19:37,600
explainability

00:19:35,360 --> 00:19:38,960
in fact there's a large group of

00:19:37,600 --> 00:19:42,240
academics that work

00:19:38,960 --> 00:19:44,720
under the acronym fed fairness

00:19:42,240 --> 00:19:45,520
accountability transparency and machine

00:19:44,720 --> 00:19:48,000
learning

00:19:45,520 --> 00:19:48,880
and they have a great web page that goes

00:19:48,000 --> 00:19:51,840
into a lot of

00:19:48,880 --> 00:19:52,799
details about what explainability is

00:19:51,840 --> 00:19:55,120
exactly

00:19:52,799 --> 00:19:57,760
why it's important so yeah i i would

00:19:55,120 --> 00:20:00,880
urge you to to check that out and then

00:19:57,760 --> 00:20:01,520
um another group of researchers um in

00:20:00,880 --> 00:20:04,880
this field

00:20:01,520 --> 00:20:06,000
is um from from darpa so yeah it see

00:20:04,880 --> 00:20:08,799
easy military

00:20:06,000 --> 00:20:10,159
military basically and yeah i i guess

00:20:08,799 --> 00:20:12,799
you can imagine why the

00:20:10,159 --> 00:20:15,600
military is especially interested in

00:20:12,799 --> 00:20:18,799
machine learning explainability

00:20:15,600 --> 00:20:21,919
and um yeah they call their program

00:20:18,799 --> 00:20:23,520
psy or explain it by ai and yeah they

00:20:21,919 --> 00:20:25,720
have a nice website as well

00:20:23,520 --> 00:20:27,520
that talks about some of their

00:20:25,720 --> 00:20:31,600
unclassified um goals

00:20:27,520 --> 00:20:31,600
as well um next slide please

00:20:32,640 --> 00:20:36,000
okay so yeah now might the question come

00:20:35,360 --> 00:20:38,880
up though

00:20:36,000 --> 00:20:40,320
uh why why should we even care about

00:20:38,880 --> 00:20:42,559
explaining the ai

00:20:40,320 --> 00:20:43,600
and yeah this one also one of my

00:20:42,559 --> 00:20:45,520
favorite quotes

00:20:43,600 --> 00:20:47,039
maybe it's a little bit not to clear but

00:20:45,520 --> 00:20:49,600
i i think it's a really

00:20:47,039 --> 00:20:50,080
it's really important it's not just

00:20:49,600 --> 00:20:53,440
about

00:20:50,080 --> 00:20:55,440
helping banks use more accurate

00:20:53,440 --> 00:20:56,960
predictive models even though that's

00:20:55,440 --> 00:21:00,000
basically what the

00:20:56,960 --> 00:21:03,039
commercial drive is about in general um

00:21:00,000 --> 00:21:05,679
artificial intelligence promises us

00:21:03,039 --> 00:21:06,640
more convenience organization

00:21:05,679 --> 00:21:09,120
optimization

00:21:06,640 --> 00:21:10,080
in our day-to-day lives and these

00:21:09,120 --> 00:21:12,000
techniques

00:21:10,080 --> 00:21:14,080
or technologies become more and more

00:21:12,000 --> 00:21:15,919
important in our lives

00:21:14,080 --> 00:21:17,200
we will probably have more questions

00:21:15,919 --> 00:21:20,159
about how they work

00:21:17,200 --> 00:21:20,880
especially we would have questions about

00:21:20,159 --> 00:21:24,080
when they go

00:21:20,880 --> 00:21:26,080
wrong um if they send us to jail or or

00:21:24,080 --> 00:21:28,640
don't let our kids into college

00:21:26,080 --> 00:21:29,280
so there's a very very important social

00:21:28,640 --> 00:21:32,400
aspect

00:21:29,280 --> 00:21:35,039
to those kind of problems now and

00:21:32,400 --> 00:21:37,760
yeah we we are working for an open

00:21:35,039 --> 00:21:40,320
source consultancy and and we are mostly

00:21:37,760 --> 00:21:40,880
uh interested in helping others use

00:21:40,320 --> 00:21:44,000
those

00:21:40,880 --> 00:21:46,640
uh very complex predictors that's our

00:21:44,000 --> 00:21:48,640
professional goal but i think it's very

00:21:46,640 --> 00:21:49,200
very important to think about the the

00:21:48,640 --> 00:21:51,600
way

00:21:49,200 --> 00:21:53,600
these artificial intelligence systems

00:21:51,600 --> 00:21:54,240
will be impacting our day-to-day lives

00:21:53,600 --> 00:21:56,559
and

00:21:54,240 --> 00:21:58,720
yeah we certainly want to know how they

00:21:56,559 --> 00:22:01,760
are making decisions as they become

00:21:58,720 --> 00:22:05,520
more and more relevant in our lives and

00:22:01,760 --> 00:22:08,240
yeah i i think another aspect of um

00:22:05,520 --> 00:22:09,760
those problems that's very important um

00:22:08,240 --> 00:22:12,480
i guess we have all seen

00:22:09,760 --> 00:22:13,039
something that recently um next like

00:22:12,480 --> 00:22:16,720
please

00:22:13,039 --> 00:22:19,039
um yeah they they hadn't an

00:22:16,720 --> 00:22:22,559
accident because something was not

00:22:19,039 --> 00:22:26,799
classified or even misclassified

00:22:22,559 --> 00:22:29,600
next slide please there are that jail

00:22:26,799 --> 00:22:30,080
or parole decisions being made because

00:22:29,600 --> 00:22:32,240
people

00:22:30,080 --> 00:22:33,600
type in the the wrong number into a

00:22:32,240 --> 00:22:35,520
black box model

00:22:33,600 --> 00:22:37,440
and it's letting yeah dangerous people

00:22:35,520 --> 00:22:38,240
go free it's keeping innocent people in

00:22:37,440 --> 00:22:40,960
prison

00:22:38,240 --> 00:22:42,720
um who don't deserve to be there um next

00:22:40,960 --> 00:22:45,919
slide please

00:22:42,720 --> 00:22:48,720
um yeah we we are doing a lot of bad

00:22:45,919 --> 00:22:51,200
medical diagnosis and screening because

00:22:48,720 --> 00:22:53,679
for example the the algorithms

00:22:51,200 --> 00:22:55,280
um are dependent on factors that are not

00:22:53,679 --> 00:22:58,559
allowed to be there like like

00:22:55,280 --> 00:23:00,880
words within an x-ray image so um the

00:22:58,559 --> 00:23:02,480
the model depends on those rather than

00:23:00,880 --> 00:23:05,679
the actual image itself

00:23:02,480 --> 00:23:05,679
um next like this

00:23:06,080 --> 00:23:10,159
um yeah another example we are we are

00:23:08,240 --> 00:23:13,600
back making that loans

00:23:10,159 --> 00:23:14,400
and credit decisions based on um faulty

00:23:13,600 --> 00:23:17,600
informations

00:23:14,400 --> 00:23:18,799
um so yeah it once again it will be very

00:23:17,600 --> 00:23:21,440
important to be

00:23:18,799 --> 00:23:23,120
uh so that we are able to debug those

00:23:21,440 --> 00:23:27,679
systems to understand

00:23:23,120 --> 00:23:29,520
if somehow someone and tampered with it

00:23:27,679 --> 00:23:32,480
but it's very very difficult to do that

00:23:29,520 --> 00:23:33,120
without having deep insight into the

00:23:32,480 --> 00:23:35,840
internal

00:23:33,120 --> 00:23:36,159
mechanisms of this system itself whether

00:23:35,840 --> 00:23:37,679
the

00:23:36,159 --> 00:23:39,679
system has been tampered with whether

00:23:37,679 --> 00:23:40,720
the inputs of these systems are being

00:23:39,679 --> 00:23:43,679
tampered with

00:23:40,720 --> 00:23:45,039
or the output of the systems are being

00:23:43,679 --> 00:23:47,600
tampered with

00:23:45,039 --> 00:23:48,320
so yeah i think there's both very

00:23:47,600 --> 00:23:51,279
important

00:23:48,320 --> 00:23:53,600
social and commercial motivations for

00:23:51,279 --> 00:23:56,400
those kinds of problems

00:23:53,600 --> 00:23:56,400
next slide please

00:23:57,039 --> 00:24:01,360
so yeah so why why haven't we be doing

00:24:00,400 --> 00:24:04,720
explainable

00:24:01,360 --> 00:24:08,400
ai right from from the start um

00:24:04,720 --> 00:24:09,039
next slide please well it's it's simple

00:24:08,400 --> 00:24:11,600
because

00:24:09,039 --> 00:24:12,799
it's it's difficult right um i'm i'm

00:24:11,600 --> 00:24:16,240
going to talk about

00:24:12,799 --> 00:24:18,799
um three reasons why i feel is difficult

00:24:16,240 --> 00:24:21,279
um that i've run into my undertakings

00:24:18,799 --> 00:24:22,240
into the direction but you may run into

00:24:21,279 --> 00:24:24,720
other problems

00:24:22,240 --> 00:24:27,039
um because i i think it's a fairly fraud

00:24:24,720 --> 00:24:30,640
and difficult problem to solve

00:24:27,039 --> 00:24:32,400
so yeah um yeah so so machine learning

00:24:30,640 --> 00:24:34,960
algorithms create

00:24:32,400 --> 00:24:37,520
functions that are interestingly combine

00:24:34,960 --> 00:24:41,039
and recombine variables until they

00:24:37,520 --> 00:24:44,080
are interest interacting in a very very

00:24:41,039 --> 00:24:46,000
sophisticated ways so one goal of

00:24:44,080 --> 00:24:48,559
machine learning explainability

00:24:46,000 --> 00:24:51,360
is to disagree predictions to form

00:24:48,559 --> 00:24:54,240
something that is called a reason codes

00:24:51,360 --> 00:24:55,279
so one example if you if you're ever

00:24:54,240 --> 00:24:58,000
been turned down

00:24:55,279 --> 00:25:00,400
for for credit card or even looked at

00:24:58,000 --> 00:25:02,559
your credit report the the credit rating

00:25:00,400 --> 00:25:05,279
agency or the credit lender

00:25:02,559 --> 00:25:06,080
they have to give you i think in the us

00:25:05,279 --> 00:25:08,720
it's

00:25:06,080 --> 00:25:10,320
five reasons why they turned you down

00:25:08,720 --> 00:25:14,080
and those are called

00:25:10,320 --> 00:25:16,720
reason codes and yeah those are related

00:25:14,080 --> 00:25:18,480
to your input variables that went into

00:25:16,720 --> 00:25:20,640
their credit scoring model

00:25:18,480 --> 00:25:21,760
that's basically a bad box um that

00:25:20,640 --> 00:25:24,720
decided you

00:25:21,760 --> 00:25:26,559
wouldn't probably uh pay your bills so

00:25:24,720 --> 00:25:28,720
if you ever turn down for for credit

00:25:26,559 --> 00:25:30,799
card you might see something like

00:25:28,720 --> 00:25:32,960
um your length of credit isn't long

00:25:30,799 --> 00:25:34,080
enough your savings account balance

00:25:32,960 --> 00:25:36,880
isn't high enough

00:25:34,080 --> 00:25:37,520
um things like this but it doesn't say

00:25:36,880 --> 00:25:40,720
the the

00:25:37,520 --> 00:25:43,679
sophisticated interactions between um

00:25:40,720 --> 00:25:44,720
your savings account um your length of

00:25:43,679 --> 00:25:47,760
credit history

00:25:44,720 --> 00:25:48,559
um your your debt to income ratio and

00:25:47,760 --> 00:25:51,360
five other

00:25:48,559 --> 00:25:52,159
revenues um why why they they turned you

00:25:51,360 --> 00:25:54,960
down

00:25:52,159 --> 00:25:56,960
so we have to break it down into simple

00:25:54,960 --> 00:26:00,400
terms and those are called

00:25:56,960 --> 00:26:03,760
reason codes and um yeah that's sort of

00:26:00,400 --> 00:26:06,400
fundamentally the odds why the uh the

00:26:03,760 --> 00:26:07,679
way machine learning algorithms work

00:26:06,400 --> 00:26:09,600
machine learning algorithms

00:26:07,679 --> 00:26:10,880
interestingly consider high degree

00:26:09,600 --> 00:26:13,919
interactions between

00:26:10,880 --> 00:26:15,679
input variables and so disagreeing a

00:26:13,919 --> 00:26:18,320
predictions into a single feature

00:26:15,679 --> 00:26:21,360
contribution is a difficult thing to do

00:26:18,320 --> 00:26:23,919
and yeah often even questionable

00:26:21,360 --> 00:26:26,240
so yeah one one thing to remember is

00:26:23,919 --> 00:26:27,360
that a lot of these approaches that we

00:26:26,240 --> 00:26:30,640
will talk about

00:26:27,360 --> 00:26:31,520
are approximate and some are very

00:26:30,640 --> 00:26:35,200
approximate

00:26:31,520 --> 00:26:35,200
um next slide please

00:26:35,520 --> 00:26:40,919
um yeah okay another reason why i find

00:26:39,279 --> 00:26:43,440
machine learning algorithms

00:26:40,919 --> 00:26:46,000
explainability to be difficult is

00:26:43,440 --> 00:26:47,200
illustrated here it's a little bit

00:26:46,000 --> 00:26:50,159
advanced but

00:26:47,200 --> 00:26:51,200
um i will see we i i think we will get

00:26:50,159 --> 00:26:54,320
it

00:26:51,200 --> 00:26:56,720
so when i train a good old-fashioned

00:26:54,320 --> 00:26:58,720
linear model it's called a convex

00:26:56,720 --> 00:27:01,440
optimization problem and

00:26:58,720 --> 00:27:02,400
what that means is given my input data

00:27:01,440 --> 00:27:05,360
and some

00:27:02,400 --> 00:27:06,000
numeric value some combinations of those

00:27:05,360 --> 00:27:07,919
parameters

00:27:06,000 --> 00:27:09,840
and your input data will lead to the

00:27:07,919 --> 00:27:12,400
lowest possible error state

00:27:09,840 --> 00:27:13,679
and you can see in the the image on the

00:27:12,400 --> 00:27:15,840
the left

00:27:13,679 --> 00:27:18,240
there's only one possible lowest error

00:27:15,840 --> 00:27:19,039
state and consequently only one best

00:27:18,240 --> 00:27:23,039
model

00:27:19,039 --> 00:27:26,640
it's not quite good but it's

00:27:23,039 --> 00:27:29,039
not far from wrong actually now

00:27:26,640 --> 00:27:30,880
when when i go into the the machine

00:27:29,039 --> 00:27:33,679
learning world i can actually have

00:27:30,880 --> 00:27:34,000
many good models for the same data set

00:27:33,679 --> 00:27:36,640
and

00:27:34,000 --> 00:27:38,640
this is a well-known phenomenon and

00:27:36,640 --> 00:27:42,000
sometimes referred to

00:27:38,640 --> 00:27:45,679
as the multiplicity of good models

00:27:42,000 --> 00:27:48,480
so even for for well good understand

00:27:45,679 --> 00:27:50,640
understood data set there there's many

00:27:48,480 --> 00:27:52,399
potentially even an infinite number of

00:27:50,640 --> 00:27:53,919
machine learning models that can give

00:27:52,399 --> 00:27:56,799
you a good predictions

00:27:53,919 --> 00:27:59,679
so when i go to to explain one of those

00:27:56,799 --> 00:28:02,080
models it's very crucial to to remember

00:27:59,679 --> 00:28:04,799
i'm just explaining one

00:28:02,080 --> 00:28:05,679
we we happen to be using um the one

00:28:04,799 --> 00:28:08,159
model out of

00:28:05,679 --> 00:28:10,159
many many good models for this pro this

00:28:08,159 --> 00:28:12,960
problem i'm just explaining one

00:28:10,159 --> 00:28:14,159
so the the essence here is even if i'm

00:28:12,960 --> 00:28:16,960
able to explain

00:28:14,159 --> 00:28:17,600
a certain kind of model i can directly

00:28:16,960 --> 00:28:20,880
transfer

00:28:17,600 --> 00:28:24,399
that knowledge to to the next one um

00:28:20,880 --> 00:28:27,919
even if i i built on a model that

00:28:24,399 --> 00:28:31,840
i i really under understood well

00:28:27,919 --> 00:28:31,840
that's next flight split

00:28:32,080 --> 00:28:37,120
um the the the third reason um why i

00:28:35,120 --> 00:28:38,480
think machine learning explainability is

00:28:37,120 --> 00:28:41,679
is difficult it's

00:28:38,480 --> 00:28:43,679
much more fundamental um

00:28:41,679 --> 00:28:45,600
we all have seen that the field of

00:28:43,679 --> 00:28:48,559
machine learning is growing rapidly

00:28:45,600 --> 00:28:50,000
and producing a wide variety of learning

00:28:48,559 --> 00:28:51,200
algorithms for for different

00:28:50,000 --> 00:28:54,000
applications

00:28:51,200 --> 00:28:56,640
um and indeed the ultimate value of

00:28:54,000 --> 00:28:59,760
those algorithms is to a great extent

00:28:56,640 --> 00:29:01,600
um yeah judged by their success in

00:28:59,760 --> 00:29:03,279
solving real world problems and

00:29:01,600 --> 00:29:06,320
therefore um

00:29:03,279 --> 00:29:08,480
algorithm replication and application to

00:29:06,320 --> 00:29:11,840
new trust are crucial to the

00:29:08,480 --> 00:29:13,760
progress of the field however few

00:29:11,840 --> 00:29:16,559
machine learning researchers or

00:29:13,760 --> 00:29:16,880
companies currently publish the software

00:29:16,559 --> 00:29:19,279
um

00:29:16,880 --> 00:29:21,039
source code or maybe even more

00:29:19,279 --> 00:29:24,720
importantly the data

00:29:21,039 --> 00:29:27,520
associated with the paper or software

00:29:24,720 --> 00:29:29,440
and actually here as aaron already

00:29:27,520 --> 00:29:30,640
mentioned we we can see that for the

00:29:29,440 --> 00:29:32,960
whole software stack

00:29:30,640 --> 00:29:34,480
um currently the the landscape of

00:29:32,960 --> 00:29:36,480
machine learning software

00:29:34,480 --> 00:29:39,120
and hardware is um throughout the

00:29:36,480 --> 00:29:42,080
dominated by by nvidia and cuder

00:29:39,120 --> 00:29:44,000
um yeah nvidia gpus have become enriched

00:29:42,080 --> 00:29:46,960
and dominant and and also

00:29:44,000 --> 00:29:49,919
it's it's possible to go out and buy an

00:29:46,960 --> 00:29:52,640
amd gpu and use opencl based or

00:29:49,919 --> 00:29:53,360
rock and based linear algebra libraries

00:29:52,640 --> 00:29:56,320
it's

00:29:53,360 --> 00:29:57,679
an nvidia's excellent cuter linear

00:29:56,320 --> 00:30:00,880
algebra support like

00:29:57,679 --> 00:30:03,760
um coolblast or qdn that

00:30:00,880 --> 00:30:04,960
yeah have propelled nvidia to the market

00:30:03,760 --> 00:30:08,320
leadership

00:30:04,960 --> 00:30:08,640
but yeah we like to to see that changed

00:30:08,320 --> 00:30:11,039
in

00:30:08,640 --> 00:30:13,840
the future by showing it's possible to

00:30:11,039 --> 00:30:16,960
build upon a complete open source deck

00:30:13,840 --> 00:30:20,000
um complete open source video driver

00:30:16,960 --> 00:30:21,200
open source software and also open um

00:30:20,000 --> 00:30:23,279
open data

00:30:21,200 --> 00:30:24,640
but yeah the the lack of openly

00:30:23,279 --> 00:30:27,760
available algorithm

00:30:24,640 --> 00:30:30,720
implementation is a major obstacle um to

00:30:27,760 --> 00:30:32,080
scientific progress uh in and beyond our

00:30:30,720 --> 00:30:34,640
our community

00:30:32,080 --> 00:30:36,080
but yeah we we believe that open source

00:30:34,640 --> 00:30:39,600
can play a very important

00:30:36,080 --> 00:30:42,640
role in removing that obstacle

00:30:39,600 --> 00:30:42,640
um next slide please

00:30:42,880 --> 00:30:47,919
yeah so those are the three reasons why

00:30:46,159 --> 00:30:50,080
we think machine learning explainability

00:30:47,919 --> 00:30:52,159
is difficult of course

00:30:50,080 --> 00:30:53,919
there are other issues which make

00:30:52,159 --> 00:30:56,799
machine learning explanability

00:30:53,919 --> 00:30:57,919
difficult but yeah again those are the

00:30:56,799 --> 00:31:00,880
major issues we

00:30:57,919 --> 00:31:02,320
identified for for the cog so now the

00:31:00,880 --> 00:31:04,480
question might come up

00:31:02,320 --> 00:31:05,679
how can we do this how can we do

00:31:04,480 --> 00:31:09,039
explainable ai

00:31:05,679 --> 00:31:10,960
in real life um well i think one of the

00:31:09,039 --> 00:31:13,360
the most important ways

00:31:10,960 --> 00:31:14,399
um to start is by understanding your

00:31:13,360 --> 00:31:16,159
data and

00:31:14,399 --> 00:31:18,000
there are a lot of ways to understand

00:31:16,159 --> 00:31:21,120
data of course

00:31:18,000 --> 00:31:22,960
i i i mentioned that in the the reason

00:31:21,120 --> 00:31:24,480
number three why machine learning

00:31:22,960 --> 00:31:28,080
explainability is difficult

00:31:24,480 --> 00:31:31,360
you can only do that step if the data is

00:31:28,080 --> 00:31:33,600
available or even open source next like

00:31:31,360 --> 00:31:33,600
this

00:31:34,320 --> 00:31:39,760
so one particular way um is through

00:31:37,760 --> 00:31:43,360
data visualization so there's a lot of

00:31:39,760 --> 00:31:46,399
ways to visually understand data

00:31:43,360 --> 00:31:49,519
these are just two of my favorites

00:31:46,399 --> 00:31:51,360
um i i really much like the 2d

00:31:49,519 --> 00:31:51,840
projection that you are seeing on the

00:31:51,360 --> 00:31:55,519
left

00:31:51,840 --> 00:31:56,480
because here i have projected a 28 times

00:31:55,519 --> 00:32:00,159
00:31:56,480 --> 00:32:03,279
dimensional data set down onto

00:32:00,159 --> 00:32:05,600
just two dimensions so that gives me a

00:32:03,279 --> 00:32:08,960
way to actually see and maybe understand

00:32:05,600 --> 00:32:12,480
things in my data such as hierarchy

00:32:08,960 --> 00:32:14,559
sparsity clusters outliners

00:32:12,480 --> 00:32:16,320
and all of those things would be

00:32:14,559 --> 00:32:18,399
impacting my model and

00:32:16,320 --> 00:32:19,440
and things i would expect my model to

00:32:18,399 --> 00:32:22,799
learn uh

00:32:19,440 --> 00:32:24,240
if it did a good job so basically we

00:32:22,799 --> 00:32:26,399
start at the

00:32:24,240 --> 00:32:28,000
very beginning and say i need to get

00:32:26,399 --> 00:32:30,480
some understanding of my data

00:32:28,000 --> 00:32:31,120
so i can check that my model understands

00:32:30,480 --> 00:32:34,559
my data

00:32:31,120 --> 00:32:36,640
at least as well as i do now

00:32:34,559 --> 00:32:38,159
the the graph on the the right has a lot

00:32:36,640 --> 00:32:40,640
of different names

00:32:38,159 --> 00:32:42,000
but i i just call it the correlation

00:32:40,640 --> 00:32:44,399
graph which helps us

00:32:42,000 --> 00:32:45,600
understand the the relationships in the

00:32:44,399 --> 00:32:49,039
dataset better

00:32:45,600 --> 00:32:50,240
so the the graph on the left is about

00:32:49,039 --> 00:32:52,480
structure we hope

00:32:50,240 --> 00:32:54,480
we hope a model would learn the the

00:32:52,480 --> 00:32:56,480
graph on the right is about

00:32:54,480 --> 00:32:58,399
understanding relationships that a model

00:32:56,480 --> 00:33:00,399
would learn and i i

00:32:58,399 --> 00:33:01,679
i really like both of those graphs

00:33:00,399 --> 00:33:04,080
because they put

00:33:01,679 --> 00:33:05,600
a high dimensional information into just

00:33:04,080 --> 00:33:09,200
two dimensions so

00:33:05,600 --> 00:33:11,120
um we can just look at it and

00:33:09,200 --> 00:33:12,640
another thing i like about the the

00:33:11,120 --> 00:33:15,519
correlation graph is

00:33:12,640 --> 00:33:16,640
that i can see high dimensional um

00:33:15,519 --> 00:33:19,600
relationships

00:33:16,640 --> 00:33:21,600
so for example i can identify groups of

00:33:19,600 --> 00:33:24,720
variables that are highly related

00:33:21,600 --> 00:33:25,039
to to each other and yeah those are the

00:33:24,720 --> 00:33:27,679
the

00:33:25,039 --> 00:33:28,960
relationship that we would hope a

00:33:27,679 --> 00:33:33,440
well-trained

00:33:28,960 --> 00:33:36,399
machine learning model would pick up

00:33:33,440 --> 00:33:36,399
next slide please

00:33:37,360 --> 00:33:41,039
another technique uh for for data

00:33:39,679 --> 00:33:44,080
visualization is um

00:33:41,039 --> 00:33:46,080
saliency or or saliency maps and and

00:33:44,080 --> 00:33:48,960
saliency maps have been getting

00:33:46,080 --> 00:33:49,519
a lot of attentions lately especially in

00:33:48,960 --> 00:33:52,480
the

00:33:49,519 --> 00:33:54,480
computer vision field they are a popular

00:33:52,480 --> 00:33:56,240
visualization tool for for gaining

00:33:54,480 --> 00:33:58,480
insights into

00:33:56,240 --> 00:34:00,159
why a deep learning model made an

00:33:58,480 --> 00:34:03,279
individual decision

00:34:00,159 --> 00:34:03,840
such as classifying an image and major

00:34:03,279 --> 00:34:06,559
papers

00:34:03,840 --> 00:34:09,520
such as dueling decree and and

00:34:06,559 --> 00:34:12,079
adversarial examples for for zns

00:34:09,520 --> 00:34:12,960
um use um saliency maps in order to

00:34:12,079 --> 00:34:15,040
convey

00:34:12,960 --> 00:34:16,399
uh where the the models are focusing

00:34:15,040 --> 00:34:19,040
their attention on

00:34:16,399 --> 00:34:20,480
and and sentencing maps are usually

00:34:19,040 --> 00:34:23,520
rendered as a heat map

00:34:20,480 --> 00:34:25,760
where the the hotness corresponds to

00:34:23,520 --> 00:34:26,639
two regions that have a big impact on

00:34:25,760 --> 00:34:28,800
the

00:34:26,639 --> 00:34:31,119
model's final decision and they are

00:34:28,800 --> 00:34:33,359
helpful for example when you are

00:34:31,119 --> 00:34:35,280
frustrated by your model incorrectly

00:34:33,359 --> 00:34:38,800
classifying a certain data

00:34:35,280 --> 00:34:41,119
point um yeah because you can

00:34:38,800 --> 00:34:43,119
take a look at the input features that

00:34:41,119 --> 00:34:47,280
led to that decision

00:34:43,119 --> 00:34:50,320
but the the problem is that silency

00:34:47,280 --> 00:34:50,960
if the image is misclassified you can

00:34:50,320 --> 00:34:53,119
tell

00:34:50,960 --> 00:34:54,879
where the the network is looking but it

00:34:53,119 --> 00:34:57,680
doesn't tell you anything about

00:34:54,879 --> 00:34:58,160
what the net network is actually doing

00:34:57,680 --> 00:35:01,119
and

00:34:58,160 --> 00:35:03,359
yeah there's a great story i i like to

00:35:01,119 --> 00:35:04,000
tell because it's a great illustration

00:35:03,359 --> 00:35:07,119
of how

00:35:04,000 --> 00:35:08,960
a i get the the wrong idea um what

00:35:07,119 --> 00:35:12,400
problem we are trying to solve

00:35:08,960 --> 00:35:12,400
um next like please

00:35:12,640 --> 00:35:16,480
yeah and and the story is that

00:35:15,359 --> 00:35:19,440
researchers at

00:35:16,480 --> 00:35:22,079
the um university of tubing trained the

00:35:19,440 --> 00:35:24,480
newer network to to recognize images

00:35:22,079 --> 00:35:25,440
and then had it to point out which part

00:35:24,480 --> 00:35:27,599
of the images

00:35:25,440 --> 00:35:29,359
were the most important for its decision

00:35:27,599 --> 00:35:31,200
and and when they asked it to highlight

00:35:29,359 --> 00:35:32,640
the most important pixels for the

00:35:31,200 --> 00:35:35,760
category tange

00:35:32,640 --> 00:35:36,720
which is kind of a fish this is what it

00:35:35,760 --> 00:35:39,280
highlighted

00:35:36,720 --> 00:35:40,720
human fingers against green background

00:35:39,280 --> 00:35:42,880
and

00:35:40,720 --> 00:35:44,720
yeah so the question might be why was it

00:35:42,880 --> 00:35:46,560
looking for human fingers when it was

00:35:44,720 --> 00:35:48,960
supposed to be looking for fish

00:35:46,560 --> 00:35:50,079
well it turns out that most of the

00:35:48,960 --> 00:35:52,720
detention pictures

00:35:50,079 --> 00:35:53,200
that were used to trend in your network

00:35:52,720 --> 00:35:55,440
um

00:35:53,200 --> 00:35:56,400
where from from people holding the fish

00:35:55,440 --> 00:35:59,760
as a trophy

00:35:56,400 --> 00:36:00,480
so um yeah the network doesn't have any

00:35:59,760 --> 00:36:02,720
context

00:36:00,480 --> 00:36:04,880
for what the tang actually looks like it

00:36:02,720 --> 00:36:05,920
just assumes the the fingers are part of

00:36:04,880 --> 00:36:08,800
the official

00:36:05,920 --> 00:36:10,320
so yeah just one example that shows um

00:36:08,800 --> 00:36:13,200
saliency could be helpful

00:36:10,320 --> 00:36:14,880
and yet also shows that uh yeah how

00:36:13,200 --> 00:36:15,920
important it is to inspect and clean

00:36:14,880 --> 00:36:19,119
your data

00:36:15,920 --> 00:36:22,400
um next slide please um

00:36:19,119 --> 00:36:23,760
another way um uh for for um

00:36:22,400 --> 00:36:25,839
inspecting your data is called

00:36:23,760 --> 00:36:28,000
perturbation and and the method has been

00:36:25,839 --> 00:36:30,480
around for for many many years now

00:36:28,000 --> 00:36:31,920
and the idea is that we like to

00:36:30,480 --> 00:36:34,240
understand the importance of

00:36:31,920 --> 00:36:34,960
certain features by masking out the

00:36:34,240 --> 00:36:37,440
input

00:36:34,960 --> 00:36:39,680
for example here we have a castle and

00:36:37,440 --> 00:36:42,079
you would then measure what happens if

00:36:39,680 --> 00:36:42,880
you would mask out some of some parts of

00:36:42,079 --> 00:36:44,960
the image

00:36:42,880 --> 00:36:47,440
and in the the first two examples you

00:36:44,960 --> 00:36:49,839
can see that the model is able to to

00:36:47,440 --> 00:36:52,960
classify the image as a castle

00:36:49,839 --> 00:36:54,800
so we could derive that there are not

00:36:52,960 --> 00:36:57,119
so many important informations for the

00:36:54,800 --> 00:36:59,599
classification uh where in the the

00:36:57,119 --> 00:37:01,760
example at the bottom the model may not

00:36:59,599 --> 00:37:04,320
be able to classify the image as a class

00:37:01,760 --> 00:37:05,359
a castle anymore so we could derive okay

00:37:04,320 --> 00:37:08,079
there's some

00:37:05,359 --> 00:37:09,440
important piece um in that region for

00:37:08,079 --> 00:37:12,800
the the classification

00:37:09,440 --> 00:37:15,040
next slide please um

00:37:12,800 --> 00:37:16,320
yeah another way for uh for

00:37:15,040 --> 00:37:19,359
explainability i

00:37:16,320 --> 00:37:21,440
um could be practiced by by simply

00:37:19,359 --> 00:37:22,000
training explainable models i'm not

00:37:21,440 --> 00:37:24,480
going through

00:37:22,000 --> 00:37:26,079
all the links on the slide but every

00:37:24,480 --> 00:37:28,240
link on this page is

00:37:26,079 --> 00:37:30,800
open source software that is capable of

00:37:28,240 --> 00:37:34,160
training accurate and explainable models

00:37:30,800 --> 00:37:36,000
so yeah no excuses um a lot of things to

00:37:34,160 --> 00:37:38,480
try and in broad classes

00:37:36,000 --> 00:37:40,400
um these are decision trees monotonic

00:37:38,480 --> 00:37:43,280
gradient boosting machines

00:37:40,400 --> 00:37:44,160
um also rule-based models are very

00:37:43,280 --> 00:37:47,520
powerful

00:37:44,160 --> 00:37:50,079
and another model is super sparse and

00:37:47,520 --> 00:37:50,880
linear integer models yeah very powerful

00:37:50,079 --> 00:37:53,520
models

00:37:50,880 --> 00:37:54,880
besides neural networks next slide

00:37:53,520 --> 00:37:57,520
please

00:37:54,880 --> 00:37:58,560
um yeah one one last method i'd like to

00:37:57,520 --> 00:38:00,640
mention

00:37:58,560 --> 00:38:01,680
is is called layer wise relevance

00:38:00,640 --> 00:38:04,160
propagation

00:38:01,680 --> 00:38:05,760
um that is pretty similar to the idea i

00:38:04,160 --> 00:38:07,520
mentioned before but is

00:38:05,760 --> 00:38:09,839
specifically designed for for newer

00:38:07,520 --> 00:38:12,400
networks where the idea is to use

00:38:09,839 --> 00:38:12,960
structure to simplify the explanation

00:38:12,400 --> 00:38:15,520
problem

00:38:12,960 --> 00:38:17,359
so instead of instead of explaining the

00:38:15,520 --> 00:38:19,839
whole model at once

00:38:17,359 --> 00:38:20,800
you you split up your model into simpler

00:38:19,839 --> 00:38:24,480
functions

00:38:20,800 --> 00:38:28,240
and which are way easier to explain

00:38:24,480 --> 00:38:30,320
and use the the lrp

00:38:28,240 --> 00:38:32,480
divide and conquer idea to to come

00:38:30,320 --> 00:38:33,680
decompose the model and explain the

00:38:32,480 --> 00:38:36,079
simpler functions and

00:38:33,680 --> 00:38:37,040
aggregate that information in a

00:38:36,079 --> 00:38:40,720
meaningful way

00:38:37,040 --> 00:38:43,599
later um next slide please

00:38:40,720 --> 00:38:43,839
um so yeah here are my recommendations

00:38:43,599 --> 00:38:46,880
if

00:38:43,839 --> 00:38:49,520
if you're doing ai in in real life

00:38:46,880 --> 00:38:49,920
so just in general consider deployment

00:38:49,520 --> 00:38:51,839
um

00:38:49,920 --> 00:38:53,680
in in most cases your model works really

00:38:51,839 --> 00:38:56,000
well on your laptop but

00:38:53,680 --> 00:38:58,000
if you move the model from your laptop

00:38:56,000 --> 00:38:59,200
onto your big secure server or your

00:38:58,000 --> 00:39:02,240
embedded system

00:38:59,200 --> 00:39:04,560
um strange things happen um i've seen a

00:39:02,240 --> 00:39:08,079
lot of bugs in the internals um

00:39:04,560 --> 00:39:11,040
the model will get one row wrong to get

00:39:08,079 --> 00:39:12,560
10 rows right so the the explanation for

00:39:11,040 --> 00:39:15,040
for the road that

00:39:12,560 --> 00:39:16,720
is the model gets wrong is it's

00:39:15,040 --> 00:39:19,520
incredibly useful

00:39:16,720 --> 00:39:20,800
um also um random data attacks so we

00:39:19,520 --> 00:39:22,720
just take our system

00:39:20,800 --> 00:39:24,800
over the weekend and just expose them

00:39:22,720 --> 00:39:27,119
constantly to random data

00:39:24,800 --> 00:39:28,160
and and we will find all kinds of

00:39:27,119 --> 00:39:30,640
interesting things

00:39:28,160 --> 00:39:31,599
so i highly recommend that technique as

00:39:30,640 --> 00:39:34,800
well

00:39:31,599 --> 00:39:38,960
um exactly um

00:39:34,800 --> 00:39:42,079
and in here we we um we used all

00:39:38,960 --> 00:39:44,240
all the ideas i i mentioned above so

00:39:42,079 --> 00:39:45,280
um to better understand our machine

00:39:44,240 --> 00:39:48,000
learning model

00:39:45,280 --> 00:39:50,240
um which not only enables us to to train

00:39:48,000 --> 00:39:53,440
a model on our desktop and later do

00:39:50,240 --> 00:39:57,040
to deploy it onto an embedded

00:39:53,440 --> 00:39:58,960
device like the the pi but um since we

00:39:57,040 --> 00:40:02,160
reused the transparent and

00:39:58,960 --> 00:40:04,640
explainable model we could identify and

00:40:02,160 --> 00:40:05,760
avoid a lot of bugs in the internal

00:40:04,640 --> 00:40:08,319
implementation

00:40:05,760 --> 00:40:10,560
um so the kernels and um the the data

00:40:08,319 --> 00:40:13,280
loading process um that's

00:40:10,560 --> 00:40:15,200
yeah and that's not that not only

00:40:13,280 --> 00:40:18,240
increased the model performance but

00:40:15,200 --> 00:40:18,720
also resulted in a much more explainable

00:40:18,240 --> 00:40:22,000
model

00:40:18,720 --> 00:40:25,839
and transparent model

00:40:22,000 --> 00:40:25,839
next slide please

00:40:27,760 --> 00:40:34,560
um yeah excellent okay um

00:40:31,359 --> 00:40:36,319
thank you very much marcus and thank you

00:40:34,560 --> 00:40:38,319
everyone for joining us we really

00:40:36,319 --> 00:40:40,960
enjoyed presenting we hope you enjoyed

00:40:38,319 --> 00:40:42,000
our talk um if you want to know a little

00:40:40,960 --> 00:40:43,680
bit more about

00:40:42,000 --> 00:40:45,520
uh what we're doing at collaborator

00:40:43,680 --> 00:40:46,480
there's a blog post from marcus on our

00:40:45,520 --> 00:40:49,760
site there's also

00:40:46,480 --> 00:40:51,440
twitter and if you have if you have a

00:40:49,760 --> 00:40:53,359
few minutes and you'd like to ask

00:40:51,440 --> 00:40:55,440
questions please join us for the q a

00:40:53,359 --> 00:40:56,480
afterwards thank you again and have a

00:40:55,440 --> 00:40:59,839
great conference

00:40:56,480 --> 00:41:00,640
bye that was great we do have a few

00:40:59,839 --> 00:41:03,839
minutes for

00:41:00,640 --> 00:41:04,400
questions and we do have uh at least one

00:41:03,839 --> 00:41:06,960
question

00:41:04,400 --> 00:41:08,720
from the audience and i've got a few

00:41:06,960 --> 00:41:10,240
more more dummy questions

00:41:08,720 --> 00:41:12,640
so let's jump into the first one from

00:41:10,240 --> 00:41:14,160
the audience it comes from will brown

00:41:12,640 --> 00:41:15,839
not the will brown who we had as a

00:41:14,160 --> 00:41:18,400
presenter just before but

00:41:15,839 --> 00:41:19,200
uh will brown from the core team will's

00:41:18,400 --> 00:41:22,079
question is

00:41:19,200 --> 00:41:24,000
in the late 90s there was an uptake in

00:41:22,079 --> 00:41:27,440
fractal image recognition

00:41:24,000 --> 00:41:30,160
based largely on irving vitamins work

00:41:27,440 --> 00:41:32,480
has fractal image recognition being

00:41:30,160 --> 00:41:35,680
surpassed or is it still being used

00:41:32,480 --> 00:41:38,000
in part

00:41:35,680 --> 00:41:39,520
interesting that's a that's a good

00:41:38,000 --> 00:41:43,280
question um

00:41:39,520 --> 00:41:46,800
i'm pretty sure it's still used in

00:41:43,280 --> 00:41:49,440
in some areas but yeah nowadays

00:41:46,800 --> 00:41:50,640
um it's it's all about um deep neural

00:41:49,440 --> 00:41:53,839
networks right

00:41:50,640 --> 00:41:55,760
so um yeah they dominate

00:41:53,839 --> 00:41:57,920
the whole pipeline from from data

00:41:55,760 --> 00:42:01,839
loading uh up to to

00:41:57,920 --> 00:42:03,599
um yeah the actual model

00:42:01,839 --> 00:42:05,440
cool um you were talking about

00:42:03,599 --> 00:42:07,359
explainability and

00:42:05,440 --> 00:42:09,440
when i was listening to your talk i was

00:42:07,359 --> 00:42:12,079
abstractly wondering

00:42:09,440 --> 00:42:13,359
who is the target audience for these

00:42:12,079 --> 00:42:15,440
kind of explanations

00:42:13,359 --> 00:42:17,839
and what kind of technical knowledge are

00:42:15,440 --> 00:42:20,640
you anticipating that they will have

00:42:17,839 --> 00:42:22,480
for example can you explain this to me

00:42:20,640 --> 00:42:24,160
as if i'm a five-year-old

00:42:22,480 --> 00:42:28,560
or are you explaining it to a business

00:42:24,160 --> 00:42:28,560
so they can you know leverage or improve

00:42:28,720 --> 00:42:32,480
um yeah i mean personally i i think

00:42:31,760 --> 00:42:34,800
everyone

00:42:32,480 --> 00:42:35,920
should at least think uh think about

00:42:34,800 --> 00:42:39,040
explainability

00:42:35,920 --> 00:42:42,000
because yeah it's super important um

00:42:39,040 --> 00:42:44,480
and if if you have some um general

00:42:42,000 --> 00:42:49,599
understanding of

00:42:44,480 --> 00:42:51,839
linear algebra you should be ready to go

00:42:49,599 --> 00:42:53,839
but yeah i think everyone should should

00:42:51,839 --> 00:42:58,160
at least look at least once

00:42:53,839 --> 00:43:01,280
into explainable ai

00:42:58,160 --> 00:43:04,640
cool um another question that

00:43:01,280 --> 00:43:07,839
i've got um you showed us some

00:43:04,640 --> 00:43:09,599
two-dimensional projections

00:43:07,839 --> 00:43:11,680
and they looked fantastic in fact it

00:43:09,599 --> 00:43:12,560
blew my mind that you could go from 28

00:43:11,680 --> 00:43:14,560
dimensions

00:43:12,560 --> 00:43:16,400
down to two dimensions but i wonder are

00:43:14,560 --> 00:43:18,000
there like three dimensional graphs

00:43:16,400 --> 00:43:19,599
on projections that are useful or

00:43:18,000 --> 00:43:22,160
practical because we're seeing

00:43:19,599 --> 00:43:23,359
you know virtual reality kind of goals

00:43:22,160 --> 00:43:26,160
and and

00:43:23,359 --> 00:43:26,800
that kind of stuff yeah yeah yeah

00:43:26,160 --> 00:43:29,760
definitely

00:43:26,800 --> 00:43:30,960
um there's one more method called um

00:43:29,760 --> 00:43:34,720
teen say that is

00:43:30,960 --> 00:43:37,920
um used um all the time

00:43:34,720 --> 00:43:39,359
um that's for for three dimensions um

00:43:37,920 --> 00:43:42,000
but here

00:43:39,359 --> 00:43:44,560
in most cases i'm i'm just starting with

00:43:42,000 --> 00:43:47,280
two dimensions because it's easier to do

00:43:44,560 --> 00:43:48,480
visualize and go over two to three

00:43:47,280 --> 00:43:51,040
dimensions here

00:43:48,480 --> 00:43:51,680
yeah all right one of your slides was a

00:43:51,040 --> 00:43:54,000
bunch of

00:43:51,680 --> 00:43:54,720
open source projects that i think you

00:43:54,000 --> 00:43:56,960
said had

00:43:54,720 --> 00:43:59,520
explainable models is it possible to

00:43:56,960 --> 00:44:01,040
provide that slide and its links to the

00:43:59,520 --> 00:44:04,240
chat after this talk

00:44:01,040 --> 00:44:07,280
of course great fantastic well

00:44:04,240 --> 00:44:08,400
we've only got one more minute to go um

00:44:07,280 --> 00:44:10,240
i don't think we've

00:44:08,400 --> 00:44:12,400
got any questions that have been passed

00:44:10,240 --> 00:44:14,000
through to me so we might start wrapping

00:44:12,400 --> 00:44:15,760
it up there thank you so much

00:44:14,000 --> 00:44:17,520
that was a great talk i really

00:44:15,760 --> 00:44:20,800
appreciate it

00:44:17,520 --> 00:44:22,240
thank you as as i understand it at least

00:44:20,800 --> 00:44:24,319
one of you are coming from

00:44:22,240 --> 00:44:26,079
canada which is a really different time

00:44:24,319 --> 00:44:29,440
zone than what we have in

00:44:26,079 --> 00:44:29,839
australia so we we very much appreciate

00:44:29,440 --> 00:44:33,280
that

00:44:29,839 --> 00:44:35,520
you've come all that way um

00:44:33,280 --> 00:44:37,119
for those of us in australia it's now

00:44:35,520 --> 00:44:41,440
time to have a cuppa

00:44:37,119 --> 00:44:44,800
otherwise known as afternoon tea

00:44:41,440 --> 00:44:46,319
we are going to be back oh

00:44:44,800 --> 00:44:48,160
pause one moment we've got 20 seconds

00:44:46,319 --> 00:44:50,400
for a quick last question from

00:44:48,160 --> 00:44:53,200
jan schmidt have you looked at nn

00:44:50,400 --> 00:44:56,480
streamer any thoughts

00:44:53,200 --> 00:44:58,480
um yes we did look at and streamer um

00:44:56,480 --> 00:44:59,599
which is further along in the actual

00:44:58,480 --> 00:45:01,599
implementation but

00:44:59,599 --> 00:45:03,040
um we had some issues just with the

00:45:01,599 --> 00:45:06,160
quality of the

00:45:03,040 --> 00:45:07,040
code and we wanted to actually design

00:45:06,160 --> 00:45:09,920
something that would

00:45:07,040 --> 00:45:11,599
um have input from the upstream for the

00:45:09,920 --> 00:45:12,319
actual design process and then streamers

00:45:11,599 --> 00:45:14,400
already

00:45:12,319 --> 00:45:16,240
quite far along in that regard so that's

00:45:14,400 --> 00:45:18,800
why we decided to

00:45:16,240 --> 00:45:19,680
design our own rather than reuse that

00:45:18,800 --> 00:45:22,160
framework

00:45:19,680 --> 00:45:23,520
great all right well let's wrap it up

00:45:22,160 --> 00:45:26,720
there we will be back

00:45:23,520 --> 00:45:29,119
um at 3 45 p.m in

00:45:26,720 --> 00:45:30,800
australia melbourne time zone which is

00:45:29,119 --> 00:45:32,400
in about 30 minutes

00:45:30,800 --> 00:45:35,280
and when we get back we've got another

00:45:32,400 --> 00:45:37,040
ai talk charles martin is going to

00:45:35,280 --> 00:45:39,359
explain

00:45:37,040 --> 00:45:40,640
how about his efforts to use machine

00:45:39,359 --> 00:45:45,680
learning to create

00:45:40,640 --> 00:45:53,839
new musical instruments see you all then

00:45:45,680 --> 00:45:53,839

YouTube URL: https://www.youtube.com/watch?v=vXZmx2aYXwQ


