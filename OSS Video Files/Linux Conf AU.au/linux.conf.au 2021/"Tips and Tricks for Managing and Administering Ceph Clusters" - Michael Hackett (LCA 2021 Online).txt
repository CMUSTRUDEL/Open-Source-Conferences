Title: "Tips and Tricks for Managing and Administering Ceph Clusters" - Michael Hackett (LCA 2021 Online)
Publication date: 2021-01-30
Playlist: linux.conf.au 2021
Description: 
	Michael Hackett

https://lca2021.linux.org.au/schedule/presentation/88/

A quick overview on the Ceph project and improvements made to managing and administering Ceph clusters in our latest release. We will review the Ceph Dashboard as well as improvements made to automation and management of the Ceph cluster. We will touch on the new Cephadm managment component in Octopus and improvements from past management tools.

linux.conf.au is a conference about the Linux operating system, and all aspects of the thriving ecosystem of Free and Open Source Software that has grown up around it. Run since 1999, in a different Australian or New Zealand city each year, by a team of local volunteers, LCA invites more than 500 people to learn from the people who shape the future of Open Source. For more information on the conference see https://linux.conf.au/

Produced by NDV: https://youtube.com/channel/UCQ7dFBzZGlBvtU2hCecsBBg?sub_confirmation=1

#linux.conf.au #linux #foss #opensource

Sat Jan 23 11:15:00 2021 at Blemings Labs
Captions: 
	00:00:10,820 --> 00:00:14,060
[Music]

00:00:15,519 --> 00:00:19,840
the system administration miniconf

00:00:17,440 --> 00:00:20,560
and our second presentation of the

00:00:19,840 --> 00:00:22,400
morning or

00:00:20,560 --> 00:00:23,680
the evening and we have now reached the

00:00:22,400 --> 00:00:26,240
us in

00:00:23,680 --> 00:00:28,800
our presenters so we are into evening

00:00:26,240 --> 00:00:32,079
rather than the middle of the night

00:00:28,800 --> 00:00:34,800
up now talking is uh michael

00:00:32,079 --> 00:00:37,600
talking about administrating cf clusters

00:00:34,800 --> 00:00:37,600
over to you michael

00:00:37,680 --> 00:00:41,520
thank you very much um as in the

00:00:40,239 --> 00:00:43,200
introduction we're going to be going

00:00:41,520 --> 00:00:44,640
over a couple of things regarding

00:00:43,200 --> 00:00:46,239
tips and tricks for managing and

00:00:44,640 --> 00:00:48,879
administering uh

00:00:46,239 --> 00:00:48,879
seth clusters

00:00:49,520 --> 00:00:53,360
it's just a real quick background on me

00:00:51,600 --> 00:00:56,079
on uh why i think i

00:00:53,360 --> 00:00:56,640
am able to give this talk to everybody

00:00:56,079 --> 00:00:58,879
today

00:00:56,640 --> 00:01:01,920
i've been working storage over 15 years

00:00:58,879 --> 00:01:04,239
specifically seth with five years

00:01:01,920 --> 00:01:06,080
i work for red hat currently in their

00:01:04,239 --> 00:01:09,520
product experience department

00:01:06,080 --> 00:01:10,960
in which i um i work

00:01:09,520 --> 00:01:12,960
hand in hand with our product management

00:01:10,960 --> 00:01:14,799
and engineering teams and support

00:01:12,960 --> 00:01:16,159
uh supporting rsf product so i do a lot

00:01:14,799 --> 00:01:17,360
of the pushing in regards to getting

00:01:16,159 --> 00:01:20,000
fixes out to our

00:01:17,360 --> 00:01:20,799
our customers that that um that that

00:01:20,000 --> 00:01:24,640
require these

00:01:20,799 --> 00:01:24,640
these these needed enhancements or bug

00:01:24,840 --> 00:01:29,200
fixes

00:01:26,080 --> 00:01:30,960
so why are we all here today um a couple

00:01:29,200 --> 00:01:33,040
of prerequisites on this talk

00:01:30,960 --> 00:01:34,560
where where there's there's no deep

00:01:33,040 --> 00:01:36,240
knowledge required here for seth

00:01:34,560 --> 00:01:38,479
uh the only requirement is that that you

00:01:36,240 --> 00:01:39,119
have a a want to want to know more about

00:01:38,479 --> 00:01:41,759
ceph and

00:01:39,119 --> 00:01:42,960
and have a deep deep need to kind of get

00:01:41,759 --> 00:01:44,640
a general understanding of what we have

00:01:42,960 --> 00:01:46,240
available for for software-defined

00:01:44,640 --> 00:01:49,520
storage with seth

00:01:46,240 --> 00:01:51,920
i'm going to be covering um cepha adm

00:01:49,520 --> 00:01:53,759
and and usage basics around sephadm uh

00:01:51,920 --> 00:01:55,520
but in particular the deployment of a

00:01:53,759 --> 00:01:56,880
ceph cluster using ceph adm

00:01:55,520 --> 00:01:58,799
we'll also take a look at the the new

00:01:56,880 --> 00:02:00,799
embedded ceph dashboard as

00:01:58,799 --> 00:02:02,560
uh in my opinion it's really cool and

00:02:00,799 --> 00:02:05,520
one of the the great usability new

00:02:02,560 --> 00:02:07,040
usability features with ceph

00:02:05,520 --> 00:02:08,640
a highlight here is we're not going to

00:02:07,040 --> 00:02:10,640
be covering anything with rooksef

00:02:08,640 --> 00:02:12,400
or the ceph operator for kubernetes i

00:02:10,640 --> 00:02:14,560
want to call that out because uh

00:02:12,400 --> 00:02:15,440
with kubernetes being the the hot topic

00:02:14,560 --> 00:02:16,560
now um

00:02:15,440 --> 00:02:19,280
i just want to highlight none of that's

00:02:16,560 --> 00:02:19,280
going to be covered here

00:02:21,440 --> 00:02:27,200
so what is cdm so in short uh cefadm

00:02:25,120 --> 00:02:29,680
deploys and manages the ceph cluster

00:02:27,200 --> 00:02:31,680
by connecting the hosts from the manager

00:02:29,680 --> 00:02:33,920
via ssh

00:02:31,680 --> 00:02:35,680
um this allows us to add or remove or

00:02:33,920 --> 00:02:38,640
update staff naming containers

00:02:35,680 --> 00:02:39,440
all using this this one tool set and we

00:02:38,640 --> 00:02:41,200
don't rely on

00:02:39,440 --> 00:02:42,720
any external configuration or

00:02:41,200 --> 00:02:43,360
orchestration tools like we did in the

00:02:42,720 --> 00:02:47,920
past

00:02:43,360 --> 00:02:50,560
like sephancibal rook or or salt

00:02:47,920 --> 00:02:51,920
cephadm is also a tool that's actually

00:02:50,560 --> 00:02:53,120
going to manage the full life cycle of

00:02:51,920 --> 00:02:54,800
your ceph cluster

00:02:53,120 --> 00:02:56,640
starting off from the the beginning by

00:02:54,800 --> 00:02:57,760
bootstrapping the installation of the

00:02:56,640 --> 00:03:00,080
tinyceph cluster

00:02:57,760 --> 00:03:01,599
a single node and then using the ceph

00:03:00,080 --> 00:03:02,319
orchestration interface which we'll get

00:03:01,599 --> 00:03:05,840
into in the

00:03:02,319 --> 00:03:07,519
the next couple slides uh you might

00:03:05,840 --> 00:03:09,360
you might be familiar with these as as

00:03:07,519 --> 00:03:11,200
the day two aspect of ceph

00:03:09,360 --> 00:03:12,959
um or day two aspect of any system

00:03:11,200 --> 00:03:14,239
administration and we're gonna use that

00:03:12,959 --> 00:03:16,319
to expand the cluster

00:03:14,239 --> 00:03:18,080
to start adding hosts and provisioning

00:03:16,319 --> 00:03:21,200
other daemons like monitors and

00:03:18,080 --> 00:03:24,319
osds and and your client daemons

00:03:21,200 --> 00:03:26,319
and services that use the cluster

00:03:24,319 --> 00:03:27,519
the cefadm can be used either via the

00:03:26,319 --> 00:03:29,200
command line interface

00:03:27,519 --> 00:03:30,959
or once installed you can actually use

00:03:29,200 --> 00:03:32,080
the dashboard in order and once

00:03:30,959 --> 00:03:33,360
installed in bootstrap you can actually

00:03:32,080 --> 00:03:35,200
use the dashboard to

00:03:33,360 --> 00:03:37,920
to just click and add hosts and and

00:03:35,200 --> 00:03:41,519
different services as as you choose via

00:03:37,920 --> 00:03:44,480
the graphical interface um seph8m is new

00:03:41,519 --> 00:03:45,519
in the octopus releases f and and we

00:03:44,480 --> 00:03:47,519
don't currently

00:03:45,519 --> 00:03:51,120
support any other lower versions than

00:03:47,519 --> 00:03:51,120
octopus this f80m

00:03:55,840 --> 00:04:00,879
so what's the goal here why why was cedm

00:03:58,879 --> 00:04:02,959
developed

00:04:00,879 --> 00:04:04,319
the main goal here was to provide a

00:04:02,959 --> 00:04:06,000
fully featured

00:04:04,319 --> 00:04:08,239
well-maintained installation and

00:04:06,000 --> 00:04:10,720
management tool that can be used for

00:04:08,239 --> 00:04:12,480
anybody not running ceph in kubernetes

00:04:10,720 --> 00:04:14,239
basically we're we're saying anybody

00:04:12,480 --> 00:04:17,040
because the aspect of this tool

00:04:14,239 --> 00:04:18,479
is simplicity and and and quickness

00:04:17,040 --> 00:04:19,199
getting a stuff cluster up and running

00:04:18,479 --> 00:04:21,519
quickly

00:04:19,199 --> 00:04:24,960
and getting the damage deployed in an

00:04:21,519 --> 00:04:26,320
equal quicker fat quick fashion

00:04:24,960 --> 00:04:28,479
so so some of the goals that were set

00:04:26,320 --> 00:04:31,280
forth with this tool is to deploy an

00:04:28,479 --> 00:04:33,280
entire ceph cluster inside containers

00:04:31,280 --> 00:04:34,960
why are we using containers so the

00:04:33,280 --> 00:04:36,800
reason we chose to use containers is

00:04:34,960 --> 00:04:38,720
because it's going to simplify

00:04:36,800 --> 00:04:40,240
management and dependency of all the

00:04:38,720 --> 00:04:40,840
package burdens across multiple

00:04:40,240 --> 00:04:44,560
different

00:04:40,840 --> 00:04:46,560
distributions rel ubuntu suse

00:04:44,560 --> 00:04:47,759
multiple different distributions package

00:04:46,560 --> 00:04:51,280
management is also

00:04:47,759 --> 00:04:51,280
set with a single container

00:04:52,000 --> 00:04:55,360
pushing towards a tight integration with

00:04:53,759 --> 00:04:59,520
with the orchestrator api

00:04:55,360 --> 00:05:01,520
setforge the orchestrator interface

00:04:59,520 --> 00:05:02,880
has evolved along with the development

00:05:01,520 --> 00:05:04,320
of cephadm

00:05:02,880 --> 00:05:06,160
in order to match all of the

00:05:04,320 --> 00:05:07,440
functionality and and

00:05:06,160 --> 00:05:09,360
features that we actually put in the

00:05:07,440 --> 00:05:10,000
ceph rook so it's actually a very close

00:05:09,360 --> 00:05:15,199
resemblance

00:05:10,000 --> 00:05:15,199
between seth rook and and ceph adm

00:05:16,240 --> 00:05:19,600
like i said before no dependency on

00:05:17,840 --> 00:05:21,600
management tools um

00:05:19,600 --> 00:05:24,320
the reason why we did this i mean i mean

00:05:21,600 --> 00:05:26,400
cephas salt is great sensible is great

00:05:24,320 --> 00:05:28,720
but the reason why this was done is is

00:05:26,400 --> 00:05:29,919
is when when we make sef depend on these

00:05:28,720 --> 00:05:31,120
types of tools

00:05:29,919 --> 00:05:32,639
it means that there's more than one

00:05:31,120 --> 00:05:34,400
piece of software that the user is

00:05:32,639 --> 00:05:38,000
actually going to have to learn

00:05:34,400 --> 00:05:40,400
um i mean ceph as a whole is is

00:05:38,000 --> 00:05:41,919
is is a large part of it is a great

00:05:40,400 --> 00:05:44,880
amount of learning but when we

00:05:41,919 --> 00:05:46,479
we stack on having to learn uh ansible

00:05:44,880 --> 00:05:48,320
in order to deploy a stuff cluster that

00:05:46,479 --> 00:05:49,520
can be too much of a burden for for some

00:05:48,320 --> 00:05:51,680
system admins

00:05:49,520 --> 00:05:52,639
so the goal here is to to give a single

00:05:51,680 --> 00:05:54,000
management tool

00:05:52,639 --> 00:05:56,160
for management and deployment of stuff

00:05:54,000 --> 00:05:58,080
clusters

00:05:56,160 --> 00:05:59,600
like i stated previously minimum os

00:05:58,080 --> 00:06:01,840
dependencies basically the only

00:05:59,600 --> 00:06:03,120
requirements for cdm from a dependency

00:06:01,840 --> 00:06:06,080
standpoint on the hosts

00:06:03,120 --> 00:06:08,639
are python 3 lvm any type of container

00:06:06,080 --> 00:06:11,360
runtime service like pod mana docker

00:06:08,639 --> 00:06:13,840
any any modern linux distribution will

00:06:11,360 --> 00:06:13,840
actually do

00:06:14,479 --> 00:06:17,759
isolating clusters from each other is is

00:06:16,639 --> 00:06:19,520
another big goal here

00:06:17,759 --> 00:06:21,120
the reason why we we have this is

00:06:19,520 --> 00:06:23,039
because in the past if you you were

00:06:21,120 --> 00:06:24,000
using sephansible or say self-deployed

00:06:23,039 --> 00:06:25,759
to deploy

00:06:24,000 --> 00:06:27,759
ceph clusters and doing multiple ceph

00:06:25,759 --> 00:06:29,199
clusters there were

00:06:27,759 --> 00:06:31,120
multiple issues you would encounter

00:06:29,199 --> 00:06:32,400
there the use of cephadmin actually

00:06:31,120 --> 00:06:35,520
allows us to

00:06:32,400 --> 00:06:38,080
to separate clusters into separate areas

00:06:35,520 --> 00:06:39,759
and actually shell into certain clusters

00:06:38,080 --> 00:06:41,440
and manage those clusters all from the

00:06:39,759 --> 00:06:44,000
same node so multiple clusters on a

00:06:41,440 --> 00:06:46,479
single node

00:06:44,000 --> 00:06:47,919
automated upgrades is also great because

00:06:46,479 --> 00:06:49,039
we're pushing that sef is actually going

00:06:47,919 --> 00:06:50,720
to own a deployment

00:06:49,039 --> 00:06:52,479
and own a responsibility for upgrading

00:06:50,720 --> 00:06:53,680
the cluster in a safe and automated

00:06:52,479 --> 00:06:56,160
fashion

00:06:53,680 --> 00:06:57,759
and then finally um pushing for an easy

00:06:56,160 --> 00:06:59,280
migration from our legacy deployment

00:06:57,759 --> 00:07:02,880
tools like self-deploy

00:06:59,280 --> 00:07:04,639
and sephansible um in this fadm

00:07:02,880 --> 00:07:06,479
so basically there will be a a

00:07:04,639 --> 00:07:08,400
conversion period on these upgrades

00:07:06,479 --> 00:07:10,000
where cephadm will be able to take over

00:07:08,400 --> 00:07:11,280
an existing cluster that was deployed on

00:07:10,000 --> 00:07:17,840
a legacy tool

00:07:11,280 --> 00:07:17,840
to be managed by ceph8m

00:07:29,440 --> 00:07:32,880
so just running through a quick install

00:07:31,120 --> 00:07:34,639
here i want to show you that it really

00:07:32,880 --> 00:07:36,880
is that simple

00:07:34,639 --> 00:07:38,319
as i stated to install a seph cluster

00:07:36,880 --> 00:07:40,800
within a

00:07:38,319 --> 00:07:43,120
five to ten minute time frame basically

00:07:40,800 --> 00:07:45,440
i like to call it a three-step process

00:07:43,120 --> 00:07:47,520
mainly because a lot of these these

00:07:45,440 --> 00:07:49,759
these steps documented here are

00:07:47,520 --> 00:07:51,199
that i'm showing in this slide are are

00:07:49,759 --> 00:07:52,639
prerequisites and and could be done

00:07:51,199 --> 00:07:54,639
before step cluster

00:07:52,639 --> 00:07:55,680
but basically it's it's basically

00:07:54,639 --> 00:07:59,599
fetching a script

00:07:55,680 --> 00:08:01,520
bootstrapping your nodes

00:07:59,599 --> 00:08:02,879
adding each one of the new hosts into

00:08:01,520 --> 00:08:04,800
that

00:08:02,879 --> 00:08:06,639
into your cluster via the orchestrator

00:08:04,800 --> 00:08:09,680
and then adding your osds

00:08:06,639 --> 00:08:11,280
it's as simple as that bootstrapping is

00:08:09,680 --> 00:08:13,360
basically just a process of creating a

00:08:11,280 --> 00:08:16,000
monitor and manager daemon on the

00:08:13,360 --> 00:08:17,280
on the the the single new um on the

00:08:16,000 --> 00:08:21,759
single node

00:08:17,280 --> 00:08:24,479
we generate a key we pass that key to um

00:08:21,759 --> 00:08:26,639
into into a a pub file in that csef we

00:08:24,479 --> 00:08:29,360
write a minimal configuration file

00:08:26,639 --> 00:08:32,479
we copy the uh the secret key from for

00:08:29,360 --> 00:08:34,560
our client admin key into etsy sef

00:08:32,479 --> 00:08:36,560
and and we spin up the monitor container

00:08:34,560 --> 00:08:38,640
in the manager container

00:08:36,560 --> 00:08:40,479
this this this default bootstrap

00:08:38,640 --> 00:08:42,080
behavior that i show you in this slide

00:08:40,479 --> 00:08:43,519
and these in these six steps is actually

00:08:42,080 --> 00:08:44,880
going to work for the majority of our

00:08:43,519 --> 00:08:46,399
users

00:08:44,880 --> 00:08:48,000
as soon as you run through this at this

00:08:46,399 --> 00:08:49,920
point you can access the cluster v or

00:08:48,000 --> 00:08:53,680
shell container from ceph adm

00:08:49,920 --> 00:08:57,120
or or or or basically just just run the

00:08:53,680 --> 00:08:57,120
script from inside the directory

00:09:03,600 --> 00:09:09,839
so in those six steps this is just going

00:09:05,120 --> 00:09:09,839
to be a basic overview of how this works

00:09:10,480 --> 00:09:13,839
this is after completion of running your

00:09:11,920 --> 00:09:15,680
bootstrap script um

00:09:13,839 --> 00:09:17,519
this this occurs once you install the

00:09:15,680 --> 00:09:19,760
script you run yourself your

00:09:17,519 --> 00:09:21,120
your your ceph adm command to deploy the

00:09:19,760 --> 00:09:23,120
bootstrap

00:09:21,120 --> 00:09:25,760
you literally have a cluster in about

00:09:23,120 --> 00:09:27,040
two minutes with a single mod manager

00:09:25,760 --> 00:09:28,480
and a dashboard daemon running and at

00:09:27,040 --> 00:09:29,600
this point you can actually log into the

00:09:28,480 --> 00:09:32,959
to the ceph dashboard

00:09:29,600 --> 00:09:32,959
and actually view yourself cluster

00:09:34,560 --> 00:09:37,920
we have our ssh connection to interact

00:09:36,720 --> 00:09:39,200
with the secondary host which we're

00:09:37,920 --> 00:09:40,240
going to be adding which is basically

00:09:39,200 --> 00:09:43,680
just using a

00:09:40,240 --> 00:09:45,760
asaf adm command add host

00:09:43,680 --> 00:09:47,920
and we have a another mod manager

00:09:45,760 --> 00:09:49,760
deployed

00:09:47,920 --> 00:09:51,279
adding a third all you need is basically

00:09:49,760 --> 00:09:53,519
the host name and

00:09:51,279 --> 00:09:54,560
it to add that that node into the

00:09:53,519 --> 00:09:56,080
cluster

00:09:54,560 --> 00:09:57,920
the the nodes will automatically get

00:09:56,080 --> 00:10:01,519
added as monitor nodes

00:09:57,920 --> 00:10:03,200
due to the fact that we we we have a

00:10:01,519 --> 00:10:04,640
a system limit set when we add these

00:10:03,200 --> 00:10:06,720
nodes the default is five

00:10:04,640 --> 00:10:08,800
so the first five nodes added will go in

00:10:06,720 --> 00:10:09,680
as monitors you can edit that to limit

00:10:08,800 --> 00:10:11,200
it to three

00:10:09,680 --> 00:10:12,320
um if you if you wanna if you're going

00:10:11,200 --> 00:10:14,079
to have a smaller cluster and only

00:10:12,320 --> 00:10:16,480
require three monitors

00:10:14,079 --> 00:10:17,680
so that's step two basically out of at

00:10:16,480 --> 00:10:19,440
out of the additional nodes which come

00:10:17,680 --> 00:10:20,959
in as monitors and then step three

00:10:19,440 --> 00:10:23,120
it's basically going in and adding in

00:10:20,959 --> 00:10:26,000
your osd's it's basically the

00:10:23,120 --> 00:10:28,160
the same adm command but um you can go

00:10:26,000 --> 00:10:30,560
in and use an activate all command

00:10:28,160 --> 00:10:31,440
which means basically that's fadm will

00:10:30,560 --> 00:10:32,959
go in um

00:10:31,440 --> 00:10:34,560
sorry seth orchestrator we'll go in and

00:10:32,959 --> 00:10:35,120
look at all the devices available on

00:10:34,560 --> 00:10:37,839
these

00:10:35,120 --> 00:10:41,360
on these nodes and go in and deploy osds

00:10:37,839 --> 00:10:41,360
against every single one of these nodes

00:10:43,680 --> 00:10:47,279
if you wanted more on ceph8m i suggest

00:10:45,600 --> 00:10:47,839
you you look through the documentation

00:10:47,279 --> 00:10:49,519
it's

00:10:47,839 --> 00:10:52,480
it's it's some vast documentation will

00:10:49,519 --> 00:10:52,480
give you great background

00:10:56,079 --> 00:11:00,720
so what is the ceph dashboard

00:10:59,120 --> 00:11:02,720
i talked a bit about this in in the in

00:11:00,720 --> 00:11:04,079
the cef adm install um and the reason

00:11:02,720 --> 00:11:05,920
why i brought it up is because by

00:11:04,079 --> 00:11:07,360
default the the ceph dashboard gets

00:11:05,920 --> 00:11:10,320
installed automatically when you

00:11:07,360 --> 00:11:10,320
bootstrap a cluster

00:11:10,480 --> 00:11:14,800
we this is a a web-based management tool

00:11:13,120 --> 00:11:16,399
which was is largely cleaned up than

00:11:14,800 --> 00:11:17,040
what we had in the in our past asperger

00:11:16,399 --> 00:11:18,880
revisions

00:11:17,040 --> 00:11:20,560
if if anybody that's been around with

00:11:18,880 --> 00:11:22,079
seth for a long time remembers the the

00:11:20,560 --> 00:11:24,880
calamari dashboard

00:11:22,079 --> 00:11:25,519
um this is a vast improvement over that

00:11:24,880 --> 00:11:28,800
including the

00:11:25,519 --> 00:11:31,360
the ability to to to manage and

00:11:28,800 --> 00:11:33,760
and add demons and and things of that

00:11:31,360 --> 00:11:33,760
nature

00:11:37,279 --> 00:11:40,320
so this is a slide that talks a a lot

00:11:39,680 --> 00:11:41,680
into

00:11:40,320 --> 00:11:43,920
basically how the dashboard works but

00:11:41,680 --> 00:11:45,839
but simply put if if you deploy a ceph

00:11:43,920 --> 00:11:47,600
cluster on octopus

00:11:45,839 --> 00:11:49,200
you're going with fadm you'll get the

00:11:47,600 --> 00:11:50,639
sap dashboard by default

00:11:49,200 --> 00:11:52,320
unless you specify with an argument

00:11:50,639 --> 00:11:54,160
during the cluster install to basically

00:11:52,320 --> 00:11:55,519
not install the dashboard

00:11:54,160 --> 00:11:57,760
the dashboard is going to be running

00:11:55,519 --> 00:11:59,600
inside the cef manager

00:11:57,760 --> 00:12:01,519
um and we'll use other ceph manager

00:11:59,600 --> 00:12:03,120
modules like prometheus and grafana for

00:12:01,519 --> 00:12:04,880
scraping data and putting it into the

00:12:03,120 --> 00:12:06,399
graphs on the dashboard

00:12:04,880 --> 00:12:08,000
prometheus and grafana will be running

00:12:06,399 --> 00:12:10,959
in containerized services

00:12:08,000 --> 00:12:12,800
on your nodes so they will be um

00:12:10,959 --> 00:12:14,639
there'll be containerized services in

00:12:12,800 --> 00:12:16,720
there it's like your your alert manager

00:12:14,639 --> 00:12:18,399
which will push alerts towards the

00:12:16,720 --> 00:12:20,320
the dashboard and then also your

00:12:18,399 --> 00:12:24,959
prometheus for scraping each one of the

00:12:20,320 --> 00:12:27,120
the data points on the nodes

00:12:24,959 --> 00:12:29,200
the the the documentation for it for the

00:12:27,120 --> 00:12:29,920
dashboard is is fantastic and and i

00:12:29,200 --> 00:12:31,920
suggest you

00:12:29,920 --> 00:12:32,959
you have a look at it um if you're

00:12:31,920 --> 00:12:35,680
interested in

00:12:32,959 --> 00:12:36,240
in more of what the dashboard can do um

00:12:35,680 --> 00:12:38,560
because

00:12:36,240 --> 00:12:39,920
it's it it's a large improvement over

00:12:38,560 --> 00:12:42,959
what we had previously

00:12:39,920 --> 00:12:46,880
in regards to user management as as well

00:12:42,959 --> 00:12:46,880
as what's available inside the dashboard

00:12:49,120 --> 00:12:52,720
with the time i have left i wanted to

00:12:50,639 --> 00:12:55,839
run through a a couple of these

00:12:52,720 --> 00:12:57,040
these dashboard slides um because one of

00:12:55,839 --> 00:12:59,680
the biggest complaints we

00:12:57,040 --> 00:13:00,800
we get with seth um working support for

00:12:59,680 --> 00:13:03,360
a number of years

00:13:00,800 --> 00:13:06,000
on on on the surf product is is it's

00:13:03,360 --> 00:13:07,920
tough to troubleshoot um

00:13:06,000 --> 00:13:09,519
the the dashboard really shines in this

00:13:07,920 --> 00:13:12,720
area uh it

00:13:09,519 --> 00:13:15,600
it also helps uh alleviate problem nodes

00:13:12,720 --> 00:13:17,279
or problem disks inside clusters

00:13:15,600 --> 00:13:19,360
this is the host overall performance

00:13:17,279 --> 00:13:23,360
view which is basically an aggregate

00:13:19,360 --> 00:13:25,600
of cpu busy ram utilization

00:13:23,360 --> 00:13:26,399
and and things along those lines for the

00:13:25,600 --> 00:13:28,959
the

00:13:26,399 --> 00:13:31,360
entire host environment on inside your

00:13:28,959 --> 00:13:31,360
cluster

00:13:31,440 --> 00:13:34,639
you can actually drill down in each one

00:13:33,040 --> 00:13:36,480
of your hosts on your cluster

00:13:34,639 --> 00:13:38,720
and and get further details in regards

00:13:36,480 --> 00:13:41,199
to the number of osds for example

00:13:38,720 --> 00:13:42,160
the raw capacity for those osds on the

00:13:41,199 --> 00:13:45,199
disks

00:13:42,160 --> 00:13:46,959
um a popular um

00:13:45,199 --> 00:13:49,360
panels for for our customers have

00:13:46,959 --> 00:13:51,839
actually been a lot of the network work

00:13:49,360 --> 00:13:53,920
uh the network panels um as you can see

00:13:51,839 --> 00:13:55,040
we we actually show drop rate and an

00:13:53,920 --> 00:13:57,920
error rate on on

00:13:55,040 --> 00:13:59,360
on the nics for the the the specified

00:13:57,920 --> 00:14:01,440
public networks for

00:13:59,360 --> 00:14:02,480
um for yourself cluster so when

00:14:01,440 --> 00:14:05,199
troubleshooting

00:14:02,480 --> 00:14:06,560
flapping osds for example this is a this

00:14:05,199 --> 00:14:08,240
could be a

00:14:06,560 --> 00:14:09,839
a great pain in order to go and look at

00:14:08,240 --> 00:14:12,079
for those types of troubles

00:14:09,839 --> 00:14:13,760
also throughput by disks is great so you

00:14:12,079 --> 00:14:17,839
can look for any type of hot spots

00:14:13,760 --> 00:14:17,839
on on a node in your cluster

00:14:18,480 --> 00:14:22,639
pool performance stats is is also fairly

00:14:20,880 --> 00:14:22,959
important important as you can see one

00:14:22,639 --> 00:14:26,240
of the

00:14:22,959 --> 00:14:28,240
the main panels here um top 15

00:14:26,240 --> 00:14:30,160
client iops by pool so so you can listen

00:14:28,240 --> 00:14:31,680
for those those noisy neighbor clients

00:14:30,160 --> 00:14:34,000
um which which are basically hammering

00:14:31,680 --> 00:14:34,480
pools which if you're sharing crush rule

00:14:34,000 --> 00:14:36,560
sets

00:14:34,480 --> 00:14:38,399
inside your cef cluster uh could be

00:14:36,560 --> 00:14:40,000
impacting other other um

00:14:38,399 --> 00:14:42,240
other clients in the cluster so this is

00:14:40,000 --> 00:14:45,279
a a another great um

00:14:42,240 --> 00:14:46,959
area in the dashboard to look at

00:14:45,279 --> 00:14:48,880
and then one of my personal favorites

00:14:46,959 --> 00:14:50,880
being being a nrgw

00:14:48,880 --> 00:14:52,079
specialist is is the is the rgw

00:14:50,880 --> 00:14:54,959
performance stats

00:14:52,079 --> 00:14:56,480
um these are improving with with every

00:14:54,959 --> 00:14:58,800
update we do to the dashboard

00:14:56,480 --> 00:15:01,839
uh due to the large of a large amount of

00:14:58,800 --> 00:15:01,839
object users out there

00:15:02,240 --> 00:15:06,720
and when when investigating any type of

00:15:04,240 --> 00:15:08,639
rgw performance issues this is a great

00:15:06,720 --> 00:15:10,800
area in the dashboard to use i also want

00:15:08,639 --> 00:15:12,800
to highlight your sync performance

00:15:10,800 --> 00:15:14,240
tab here this is for multi-site where

00:15:12,800 --> 00:15:17,680
you can actually see the

00:15:14,240 --> 00:15:19,519
the the sync rate between your sites uh

00:15:17,680 --> 00:15:21,120
that's been difficult in the past to to

00:15:19,519 --> 00:15:23,279
determine um

00:15:21,120 --> 00:15:25,040
using a rados gateway admin command in

00:15:23,279 --> 00:15:25,519
order to do that and having this graph

00:15:25,040 --> 00:15:28,000
is

00:15:25,519 --> 00:15:30,480
is is is helpful in that troubleshooting

00:15:28,000 --> 00:15:30,480
standpoint

00:15:32,240 --> 00:15:36,000
thank you very much i i hope i um push

00:15:34,880 --> 00:15:39,120
some

00:15:36,000 --> 00:15:40,800
um ideas towards sef and if you uh have

00:15:39,120 --> 00:15:42,160
any further information on it please

00:15:40,800 --> 00:15:44,000
look to the the links that i put inside

00:15:42,160 --> 00:15:46,079
the talk

00:15:44,000 --> 00:15:47,759
thank you very much and we will try and

00:15:46,079 --> 00:15:49,040
get those slides up as soon as possible

00:15:47,759 --> 00:15:51,519
so that you can have a look at

00:15:49,040 --> 00:15:52,560
the links directly and hopefully michael

00:15:51,519 --> 00:15:54,399
will have a chance to

00:15:52,560 --> 00:15:56,399
jump into the chat and vinulus and

00:15:54,399 --> 00:15:59,040
answer any questions that you might have

00:15:56,399 --> 00:15:59,680
thank you for the talk michael um we now

00:15:59,040 --> 00:16:02,480
have a break

00:15:59,680 --> 00:16:04,399
and then hopefully our next speaker

00:16:02,480 --> 00:16:07,440
federico will be talking to us

00:16:04,399 --> 00:16:16,880
about raspberry pi clusters thank you

00:16:07,440 --> 00:16:16,880

YouTube URL: https://www.youtube.com/watch?v=uMxubYvbXFE


