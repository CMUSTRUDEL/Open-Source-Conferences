Title: "Don’t eat my data - 30+ years of storage war stories" - Steven Ellis (LCA 2020)
Publication date: 2020-01-14
Playlist: linux.conf.au 2020
Description: 
	Steven Ellis

https://lca2020.linux.org.au/schedule/presentation/109/

We are in an era of explosive data growth, with everything from cat pictures to NASA’s incredible archive of space photography, and this all needs to be stored and catalogued in a reliable and easily accessible form. How can we safely store/backup/manage this data across  a diverse range personal devices, never mind the challenges of enterprise class scale out storage, with the occasional (rain) cloud along the way?

We’ll start off with a bit of history on data storage formats to set the scene, and then dig into some stories from the last 35+ years of Steven’s own journey in IT, with some names changed to protect the “innocent”.

Themes covered will include
    Getting physical
    Bits, Bytes, Blocks and Blobs, with a file or three
    Performance sanity tests
    Hardware Raid - pain or pleasure?
    File-system selection and why checksums matter
    Going mad with MDADM (part 2)
    DRBDont
    HA NFS aren't the droids you're looking for
    Vacuum Cleaners and Dehumidifiers
    Raid is not a backup, and erasure coding isn’t about data deletion
    The cloud is just someone else’s SAN
    When physical disks trump flash storage

linux.conf.au is a conference about the Linux operating system, and all aspects of the thriving ecosystem of Free and Open Source Software that has grown up around it. Run since 1999, in a different Australian or New Zealand city each year, by a team of local volunteers, LCA invites more than 500 people to learn from the people who shape the future of Open Source. For more information on the conference see https://linux.conf.au/

Produced by NDV: https://youtube.com/channel/UCQ7dFBzZGlBvtU2hCecsBBg?sub_confirmation=1

#linux.conf.au #linux #foss #opensource

Mon Jan 13 16:55:00 2020 at Arena
Captions: 
	00:00:00,060 --> 00:00:04,980
okay microphones on awesome Wow I'm

00:00:03,030 --> 00:00:06,870
amazed to see so many of you here as I'm

00:00:04,980 --> 00:00:09,510
the last thing standing between you and

00:00:06,870 --> 00:00:12,150
a cold beverage or the Linux Australia

00:00:09,510 --> 00:00:16,190
AGM depending on what you're up to this

00:00:12,150 --> 00:00:19,500
evening I updated the timeframe on this

00:00:16,190 --> 00:00:21,449
shortly before coming here which is kind

00:00:19,500 --> 00:00:24,600
of really scary it makes me feel

00:00:21,449 --> 00:00:26,880
incredibly old but hopefully you'll get

00:00:24,600 --> 00:00:31,590
the context of the the journey as we

00:00:26,880 --> 00:00:32,489
walk through this this afternoon so

00:00:31,590 --> 00:00:35,760
we're going to do a little bit of

00:00:32,489 --> 00:00:37,170
history and a couple of Technology

00:00:35,760 --> 00:00:38,730
overview some of that I'm going to skip

00:00:37,170 --> 00:00:40,649
through quite quickly for the purposes

00:00:38,730 --> 00:00:43,050
of time but I've included those slides

00:00:40,649 --> 00:00:44,040
just for completeness and then we're

00:00:43,050 --> 00:00:46,530
going to get into a few war stories

00:00:44,040 --> 00:00:49,620
where the names have been our first

00:00:46,530 --> 00:00:50,850
occator to protect the innocent and what

00:00:49,620 --> 00:00:53,219
I'm going to try and leave you with are

00:00:50,850 --> 00:00:55,050
a few tips and tricks a couple of things

00:00:53,219 --> 00:00:57,000
that you can use in your day job a

00:00:55,050 --> 00:00:58,559
couple of questions that you can ask

00:00:57,000 --> 00:01:01,289
your various project managers and

00:00:58,559 --> 00:01:03,570
project leads and customers to try and

00:01:01,289 --> 00:01:06,030
avoid some of the missteps I've hit or

00:01:03,570 --> 00:01:09,950
my customers have hit or I've seen in

00:01:06,030 --> 00:01:12,420
projects over the years so as I said I

00:01:09,950 --> 00:01:14,100
stepped back and tried to decide where

00:01:12,420 --> 00:01:17,070
to begin with this where does my journey

00:01:14,100 --> 00:01:20,070
with digital data begin and it begins

00:01:17,070 --> 00:01:23,070
with mice clicker works

00:01:20,070 --> 00:01:25,320
the humble cassette tape how many others

00:01:23,070 --> 00:01:27,990
here started their journey in computing

00:01:25,320 --> 00:01:30,540
with the humble cassette tape you know I

00:01:27,990 --> 00:01:32,820
started computing with the zx81 and then

00:01:30,540 --> 00:01:34,890
moved on to a Sinclair spectrum so most

00:01:32,820 --> 00:01:36,900
of my early digital data existed on one

00:01:34,890 --> 00:01:39,180
of these things which seems kind of

00:01:36,900 --> 00:01:42,689
anacott a bit of an anachronism these

00:01:39,180 --> 00:01:45,600
days but surprisingly tape is still

00:01:42,689 --> 00:01:48,000
quite popular in the enterprise but

00:01:45,600 --> 00:01:50,750
where do we as humans start this is an

00:01:48,000 --> 00:01:54,780
incredible pool of data through

00:01:50,750 --> 00:01:57,600
artifacts that have outlasted millennia

00:01:54,780 --> 00:01:59,810
things like cave paintings or

00:01:57,600 --> 00:02:02,159
[Music]

00:01:59,810 --> 00:02:04,320
hieroglyphics although both of these

00:02:02,159 --> 00:02:07,140
have suffered data loss and bit rot over

00:02:04,320 --> 00:02:10,259
the years along the way we've seen

00:02:07,140 --> 00:02:12,540
various generations of disk devices and

00:02:10,259 --> 00:02:13,380
it's hard to believe the sheer amount of

00:02:12,540 --> 00:02:15,780
storage we

00:02:13,380 --> 00:02:18,900
now fit on a modern hard drive compared

00:02:15,780 --> 00:02:21,960
with those legacy devices which leads

00:02:18,900 --> 00:02:24,810
into some of the technologies that that

00:02:21,960 --> 00:02:26,370
we use to talk to these devices and a

00:02:24,810 --> 00:02:28,530
couple I want to point out are the ones

00:02:26,370 --> 00:02:30,750
that we would consider legacy today that

00:02:28,530 --> 00:02:33,030
were prevalent 10-15 years ago things

00:02:30,750 --> 00:02:34,770
like scuzzy and IDE which we don't see

00:02:33,030 --> 00:02:38,580
in the enterprise so much anymore at

00:02:34,770 --> 00:02:40,710
least in terms of an interconnect the

00:02:38,580 --> 00:02:43,290
true enterprise level were starting to

00:02:40,710 --> 00:02:45,150
see why the top ssin of pci based

00:02:43,290 --> 00:02:47,940
protocols but they're filtered into

00:02:45,150 --> 00:02:51,120
consumer and prosumer devices for

00:02:47,940 --> 00:02:54,360
example my laptop here has a nvme device

00:02:51,120 --> 00:02:57,330
that is capable of around 3,000

00:02:54,360 --> 00:02:59,850
megabytes a second whereas back when I

00:02:57,330 --> 00:03:03,480
gave a presentation at LCA in 2010

00:02:59,850 --> 00:03:06,120
around going mad with MDA DM and as some

00:03:03,480 --> 00:03:08,010
of my earlier adventures with storage I

00:03:06,120 --> 00:03:10,530
had a hard drive that was capable

00:03:08,010 --> 00:03:12,690
hundred megabytes a second and shortly

00:03:10,530 --> 00:03:15,510
through the talk it started to produce

00:03:12,690 --> 00:03:17,340
approximately 0.1 megabytes a second as

00:03:15,510 --> 00:03:21,030
I suffered hardware failure was

00:03:17,340 --> 00:03:22,350
presenting about rate so hopefully I'm

00:03:21,030 --> 00:03:25,320
gonna have better look with the flash

00:03:22,350 --> 00:03:26,910
device in my laptop today data

00:03:25,320 --> 00:03:29,400
protection techniques as I mentioned

00:03:26,910 --> 00:03:31,500
raid already could that there really

00:03:29,400 --> 00:03:32,640
should be more laptops available with

00:03:31,500 --> 00:03:33,990
raid we're going to dig into some of

00:03:32,640 --> 00:03:35,130
these a little bit deeper later but

00:03:33,990 --> 00:03:38,790
again these slides are here for

00:03:35,130 --> 00:03:43,170
completeness and I've also thrown these

00:03:38,790 --> 00:03:47,640
in because you know within organizations

00:03:43,170 --> 00:03:49,050
as sometimes you need to make it clear

00:03:47,640 --> 00:03:54,780
what the difference is between things

00:03:49,050 --> 00:03:56,459
like Sam versus Nass often the project

00:03:54,780 --> 00:03:58,440
teams the project donors and developers

00:03:56,459 --> 00:04:01,320
just don't care and particularly today

00:03:58,440 --> 00:04:02,940
in modern environments many of your

00:04:01,320 --> 00:04:04,380
development teams don't care because all

00:04:02,940 --> 00:04:05,910
they're interested in is cloud all

00:04:04,380 --> 00:04:08,160
they're interested in is object the cool

00:04:05,910 --> 00:04:10,709
kids deal with objects you know they the

00:04:08,160 --> 00:04:14,220
adoption of s3 type protocols and Swift

00:04:10,709 --> 00:04:18,359
has become massive inside development

00:04:14,220 --> 00:04:19,920
teams today and then we have the various

00:04:18,359 --> 00:04:22,140
data protection techniques that we all

00:04:19,920 --> 00:04:25,800
rely upon her providing business

00:04:22,140 --> 00:04:27,150
continuity raid has been dominant for so

00:04:25,800 --> 00:04:29,639
many years in the data set

00:04:27,150 --> 00:04:31,830
and today it's still extremely common to

00:04:29,639 --> 00:04:33,900
see our enterprise service deploy with

00:04:31,830 --> 00:04:35,490
raid one storage just for the base

00:04:33,900 --> 00:04:37,380
operating system whilst the rest of the

00:04:35,490 --> 00:04:43,740
environment may still be coming off sand

00:04:37,380 --> 00:04:46,260
or using a raid 5 or raid 6 in scale out

00:04:43,740 --> 00:04:47,940
storage or hyper scale storage we're

00:04:46,260 --> 00:04:50,610
seeing a shift towards technology like a

00:04:47,940 --> 00:04:53,460
razer coding and immediately want to

00:04:50,610 --> 00:04:54,900
talk about erasure coding with a new

00:04:53,460 --> 00:04:56,760
project team they start thinking about

00:04:54,900 --> 00:04:58,590
was this all about data deletion because

00:04:56,760 --> 00:05:00,300
of the use of the word erasure what it's

00:04:58,590 --> 00:05:02,729
really about data protection techniques

00:05:00,300 --> 00:05:04,710
we move away from raid so often I

00:05:02,729 --> 00:05:07,050
initially described it as raid on

00:05:04,710 --> 00:05:09,570
steroids you know in arrays you're coded

00:05:07,050 --> 00:05:11,130
4 plus 2 we can allow for 2 disks to

00:05:09,570 --> 00:05:13,350
fail where you've got a six disk array

00:05:11,130 --> 00:05:15,630
it's kind of similar in a way to kind of

00:05:13,350 --> 00:05:18,690
a raid 6 implementation except it isn't

00:05:15,630 --> 00:05:21,270
under the hood the algorithms used to

00:05:18,690 --> 00:05:23,039
deploy this are quite different with 8

00:05:21,270 --> 00:05:25,470
plus 3 we can suffer a three-disc

00:05:23,039 --> 00:05:28,199
failure we're defining an environment

00:05:25,470 --> 00:05:30,180
designed to suffer failure but what's

00:05:28,199 --> 00:05:32,700
really nice about arrays your coding is

00:05:30,180 --> 00:05:34,740
it isn't just about data in the chassis

00:05:32,700 --> 00:05:37,950
you can arrays your code crush chassis

00:05:34,740 --> 00:05:40,410
across the data center which allows for

00:05:37,950 --> 00:05:42,539
a higher degree of resiliency and

00:05:40,410 --> 00:05:45,510
redundancy and the really neat thing is

00:05:42,539 --> 00:05:48,360
you can scale +1 I might start with 8

00:05:45,510 --> 00:05:52,110
plus 3 and 11 chassis x' or 11 disks and

00:05:48,360 --> 00:05:54,479
then scale 212 213 214 and rebalance

00:05:52,110 --> 00:05:57,060
that erasure coded array I'm still

00:05:54,479 --> 00:06:00,180
running an 8 + 3 model but now running

00:05:57,060 --> 00:06:02,280
across 16 disks there's some incredible

00:06:00,180 --> 00:06:08,849
flexibility around the arranger coded

00:06:02,280 --> 00:06:11,460
approach compression and deduplication

00:06:08,849 --> 00:06:12,810
are very hot topics in the enterprise as

00:06:11,460 --> 00:06:15,090
well you know we've got all those

00:06:12,810 --> 00:06:17,639
gazillions of cat pictures that can be

00:06:15,090 --> 00:06:20,940
compressed down or deduplicated down to

00:06:17,639 --> 00:06:22,380
a much smaller data set and then we've

00:06:20,940 --> 00:06:24,360
got the various vendors that your

00:06:22,380 --> 00:06:26,160
various terms your storage vendors will

00:06:24,360 --> 00:06:27,750
talk to you in terms of you know how

00:06:26,160 --> 00:06:29,520
much raw storage versus effective

00:06:27,750 --> 00:06:33,330
storage and this becomes really critical

00:06:29,520 --> 00:06:35,840
whether we talk about either you know

00:06:33,330 --> 00:06:37,469
decimal gigabytes or true gigabytes

00:06:35,840 --> 00:06:39,539
because what is the actual

00:06:37,469 --> 00:06:40,650
representation of your storage what is

00:06:39,539 --> 00:06:42,270
it really worth

00:06:40,650 --> 00:06:43,710
and sometimes you can get really tied up

00:06:42,270 --> 00:06:45,600
in knots when you actually get this up

00:06:43,710 --> 00:06:47,220
to super scale and someone starts

00:06:45,600 --> 00:06:49,470
discovering they're missing several

00:06:47,220 --> 00:06:50,910
hundreds of terabytes of data because

00:06:49,470 --> 00:06:53,300
they're using the wrong model to

00:06:50,910 --> 00:06:55,590
calculate their available disk capacity

00:06:53,300 --> 00:06:57,330
so we're going to start with a few the

00:06:55,590 --> 00:07:00,509
wall story so first of all can I fix

00:06:57,330 --> 00:07:02,130
this for you so as I said thirty five

00:07:00,509 --> 00:07:04,410
years ago my journey started with the

00:07:02,130 --> 00:07:06,479
humble cassette tape and in order to fix

00:07:04,410 --> 00:07:08,430
most of my early data problems I had to

00:07:06,479 --> 00:07:10,590
use a pencil now this is a fairly

00:07:08,430 --> 00:07:12,060
familiar meme on the Internet and if

00:07:10,590 --> 00:07:13,740
this doesn't make sense you come and buy

00:07:12,060 --> 00:07:16,260
me a drink later and I'll dig into

00:07:13,740 --> 00:07:17,910
greater detail but for those of us even

00:07:16,260 --> 00:07:20,520
with the humble mixtape that you wanted

00:07:17,910 --> 00:07:22,800
to save but more importantly it was that

00:07:20,520 --> 00:07:25,229
program had spent four hours typing in

00:07:22,800 --> 00:07:29,039
out of a computer magazine or in later

00:07:25,229 --> 00:07:31,620
years the code I was writing myself in

00:07:29,039 --> 00:07:32,940
fact those days my backup mechanism was

00:07:31,620 --> 00:07:35,190
a twin cassette deck to make an

00:07:32,940 --> 00:07:38,070
immediate copy of my you know important

00:07:35,190 --> 00:07:40,500
computer programs but where I want to

00:07:38,070 --> 00:07:45,630
drive home is how important is to be

00:07:40,500 --> 00:07:48,440
aware of Aging and legacy storage be it

00:07:45,630 --> 00:07:53,039
floppy disks hard drives optical devices

00:07:48,440 --> 00:07:55,530
flash drives or tape devices you know

00:07:53,039 --> 00:07:58,229
bit rot can occur on a wide range of

00:07:55,530 --> 00:08:01,320
these do any of you remember the phase

00:07:58,229 --> 00:08:03,419
where there was a massive amount of bit

00:08:01,320 --> 00:08:06,449
rot on optical disks particularly

00:08:03,419 --> 00:08:07,949
printed audio CDs you know Philips and

00:08:06,449 --> 00:08:11,130
phonograms spent an incredible amount of

00:08:07,949 --> 00:08:12,449
money replacing CDs that were assumed to

00:08:11,130 --> 00:08:15,210
last for a hundred years and were

00:08:12,449 --> 00:08:18,599
actually rusting within two to three

00:08:15,210 --> 00:08:21,360
years I've seen a lot of flash device

00:08:18,599 --> 00:08:22,830
failure particularly USB type keys be

00:08:21,360 --> 00:08:24,780
either physical failure because people

00:08:22,830 --> 00:08:27,690
misusing them was just the flash device

00:08:24,780 --> 00:08:29,340
goes mould is a really big problem with

00:08:27,690 --> 00:08:32,339
your tape devices and on your old-school

00:08:29,340 --> 00:08:34,080
legacy floppy disks and then you've got

00:08:32,339 --> 00:08:36,029
all these old interfaces which can

00:08:34,080 --> 00:08:39,719
really hurt you if you no longer have an

00:08:36,029 --> 00:08:42,599
IDE adapter to read that old IDE backup

00:08:39,719 --> 00:08:44,970
that you happen to have lying around and

00:08:42,599 --> 00:08:46,950
then of course there's the legacy file

00:08:44,970 --> 00:08:48,390
systems that you can no longer read or

00:08:46,950 --> 00:08:52,110
are really hard because the goner and

00:08:48,390 --> 00:08:54,100
removed support from the Linux kernel so

00:08:52,110 --> 00:08:56,600
a few data recovery tips

00:08:54,100 --> 00:08:57,950
isopropyl alcohol lint-free cloths are

00:08:56,600 --> 00:08:59,990
really useful if you're going through a

00:08:57,950 --> 00:09:02,930
big stack of CDs trying to recover data

00:08:59,990 --> 00:09:05,150
because nothing worse than suddenly

00:09:02,930 --> 00:09:07,010
discovering your CD drive or your DVD

00:09:05,150 --> 00:09:08,540
drive your blu-ray drive no longer works

00:09:07,010 --> 00:09:13,730
because it's all the innards are clogged

00:09:08,540 --> 00:09:16,580
up I like using USB based dongles DVDs

00:09:13,730 --> 00:09:18,410
CD drives floppy drives ID and SAS or

00:09:16,580 --> 00:09:21,860
adapters because that way it can

00:09:18,410 --> 00:09:24,200
hot-swap devices on and off and then I'd

00:09:21,860 --> 00:09:27,230
like to use something like a an Asus an

00:09:24,200 --> 00:09:29,390
or a USB a large USB external drive for

00:09:27,230 --> 00:09:31,130
initial archive I'll go into that a

00:09:29,390 --> 00:09:33,020
little bit more detail later tools

00:09:31,130 --> 00:09:36,980
things like system rescue CD ultimate

00:09:33,020 --> 00:09:38,390
boot CD DD rescues awesome LZ dough peel

00:09:36,980 --> 00:09:40,910
similar tools they're capable of really

00:09:38,390 --> 00:09:44,780
fast data compression minimal overhead

00:09:40,910 --> 00:09:46,550
some big benefits the moment you start

00:09:44,780 --> 00:09:49,460
throwing a big disk image you know

00:09:46,550 --> 00:09:52,130
terabytes of data a gzip or B zip or

00:09:49,460 --> 00:09:56,630
similar tools you just going to burn CPU

00:09:52,130 --> 00:09:58,850
and time LZ a test disk and photo record

00:09:56,630 --> 00:10:01,880
awesome forgetting really deep and dirty

00:09:58,850 --> 00:10:05,180
for data recovery a local author based

00:10:01,880 --> 00:10:08,000
in Auckland a few years ago reached out

00:10:05,180 --> 00:10:09,650
to meet her friend because her hard

00:10:08,000 --> 00:10:13,580
drive had failed with her latest

00:10:09,650 --> 00:10:14,720
manuscript nine months of work since I

00:10:13,580 --> 00:10:16,640
her backups weren't working because

00:10:14,720 --> 00:10:20,930
she'd never tested her backup recovery

00:10:16,640 --> 00:10:24,380
was working so thanks to those tools we

00:10:20,930 --> 00:10:26,360
managed to get every single character of

00:10:24,380 --> 00:10:29,150
a manuscript back and she was back up

00:10:26,360 --> 00:10:30,740
and running within a week right so some

00:10:29,150 --> 00:10:33,590
of these tools are truly awesome with

00:10:30,740 --> 00:10:35,780
data recovery so when the original media

00:10:33,590 --> 00:10:37,340
has failed USB dongles sometimes don't

00:10:35,780 --> 00:10:40,910
cut it particularly when you go really

00:10:37,340 --> 00:10:42,980
badly behave ideal SATA Drive there's

00:10:40,910 --> 00:10:44,060
just Klunk Klunk Klunk away and you're

00:10:42,980 --> 00:10:45,710
desperately trying to get the data off

00:10:44,060 --> 00:10:48,950
so you may need to go and borrow some

00:10:45,710 --> 00:10:51,470
old hardware I recommend having some

00:10:48,950 --> 00:10:53,630
live OS images at that point when you're

00:10:51,470 --> 00:10:56,990
doing data recovery I actually have a

00:10:53,630 --> 00:11:00,500
small USB key that's capable of booting

00:10:56,990 --> 00:11:03,080
on both UEFI and BIOS space hardware

00:11:00,500 --> 00:11:06,110
with a whole range of data recovery

00:11:03,080 --> 00:11:07,600
tools that I use regularly and I said

00:11:06,110 --> 00:11:10,180
earlier always paid

00:11:07,600 --> 00:11:12,430
copy of the original media DD rescue is

00:11:10,180 --> 00:11:14,380
great whether it's a flash device or

00:11:12,430 --> 00:11:16,240
spinning disc because it will go

00:11:14,380 --> 00:11:18,370
backwards and forwards and isolate

00:11:16,240 --> 00:11:21,699
problem bits within the disk that you

00:11:18,370 --> 00:11:23,560
can't recover then use your recovered

00:11:21,699 --> 00:11:25,870
image or a snapshot of that recovered

00:11:23,560 --> 00:11:27,819
image to do the next phase of your data

00:11:25,870 --> 00:11:29,980
recovery and park that disk or flash

00:11:27,819 --> 00:11:31,360
device out of the way because if you

00:11:29,980 --> 00:11:35,199
just keep using it it's going to

00:11:31,360 --> 00:11:40,600
deteriorate even more I've on one

00:11:35,199 --> 00:11:43,389
occasion had a disc where every time we

00:11:40,600 --> 00:11:45,579
ran DD rescue we got approximately two

00:11:43,389 --> 00:11:47,589
or three percent less data recovery on

00:11:45,579 --> 00:11:53,199
each iteration as the disk just

00:11:47,589 --> 00:11:55,660
progressively failed come on and this is

00:11:53,199 --> 00:11:58,180
one area where hard drives from flash

00:11:55,660 --> 00:12:01,110
devices the technologies like trim and

00:11:58,180 --> 00:12:04,509
discard are really important on flash on

00:12:01,110 --> 00:12:06,279
flash SSD or nvme devices in order to

00:12:04,509 --> 00:12:09,069
provide we're leveling and improve the

00:12:06,279 --> 00:12:10,269
longevity of the device but it means

00:12:09,069 --> 00:12:12,160
that when you do a delete it's

00:12:10,269 --> 00:12:14,199
effectively a true delete the data is

00:12:12,160 --> 00:12:15,519
really hard to recover at that point

00:12:14,199 --> 00:12:17,519
unless you've got very specialized

00:12:15,519 --> 00:12:19,300
tooling that can get inside the firmware

00:12:17,519 --> 00:12:21,160
whereas when you're doing it on a

00:12:19,300 --> 00:12:23,529
traditional hard drive you accidentally

00:12:21,160 --> 00:12:25,480
delete some data as long as you can pull

00:12:23,529 --> 00:12:27,459
the mains really quickly and you haven't

00:12:25,480 --> 00:12:29,889
started writing new data you can

00:12:27,459 --> 00:12:32,470
actually go and use tools like photorec

00:12:29,889 --> 00:12:38,230
etc to go and probe the disk and find

00:12:32,470 --> 00:12:39,160
those missing data files come on my

00:12:38,230 --> 00:12:41,410
clickers not happy

00:12:39,160 --> 00:12:44,800
always remember raid is not a backup

00:12:41,410 --> 00:12:46,449
mechanism and there's many ways of

00:12:44,800 --> 00:12:48,939
implementing raid you know the

00:12:46,449 --> 00:12:51,399
proprietary or in kernel drivers or for

00:12:48,939 --> 00:12:53,050
the hardware raid controllers were less

00:12:51,399 --> 00:12:54,850
prevalent now things like the fake raid

00:12:53,050 --> 00:12:57,610
devices are talked a bit about that in

00:12:54,850 --> 00:13:01,209
Wellington in 2010 and things like MDA

00:12:57,610 --> 00:13:02,529
DM LVM base raids at FS raid btrfs raid

00:13:01,209 --> 00:13:04,329
how many different software raid

00:13:02,529 --> 00:13:07,089
implementations do we actually need but

00:13:04,329 --> 00:13:10,060
there's plenty of options out there so

00:13:07,089 --> 00:13:12,790
back in 2010 I did a talk go mad with

00:13:10,060 --> 00:13:16,300
MDA DM I was trying to pinpoint an issue

00:13:12,790 --> 00:13:18,610
around raid it failure a software raid

00:13:16,300 --> 00:13:20,649
failure and in the end it was an issue a

00:13:18,610 --> 00:13:21,640
conflict between the HB a or the raid

00:13:20,649 --> 00:13:24,220
adapter

00:13:21,640 --> 00:13:26,860
and the chipset on the PCI bus and we

00:13:24,220 --> 00:13:29,620
were actually seeing data loss or data

00:13:26,860 --> 00:13:31,839
corruption bits being flipped on the PCI

00:13:29,620 --> 00:13:34,000
bus in this case of it I've actually

00:13:31,839 --> 00:13:36,459
been running XFS as the file system and

00:13:34,000 --> 00:13:38,680
had detected it sooner because XFS has

00:13:36,459 --> 00:13:40,390
checked something and it would have

00:13:38,680 --> 00:13:42,130
notified me there was some consistency

00:13:40,390 --> 00:13:45,370
issues with the file system instead it

00:13:42,130 --> 00:13:47,079
was ext3 at the time and in the end I

00:13:45,370 --> 00:13:49,420
resolved it by just simply moving

00:13:47,079 --> 00:13:52,450
everything to a different motherboard it

00:13:49,420 --> 00:13:54,190
was a painful thing to diagnose since

00:13:52,450 --> 00:13:56,890
then I've had other adventures with raid

00:13:54,190 --> 00:13:58,720
controllers so in this case we had a

00:13:56,890 --> 00:14:01,029
hardware RAID controller failure where

00:13:58,720 --> 00:14:04,180
we had no spare compatible hardware it

00:14:01,029 --> 00:14:07,390
was the only version of that Hardware we

00:14:04,180 --> 00:14:08,680
actually had inside the organization so

00:14:07,390 --> 00:14:11,019
we're starting to look at trade me and

00:14:08,680 --> 00:14:13,380
eBay to try and get a controller that we

00:14:11,019 --> 00:14:16,300
could actually try and recover the data

00:14:13,380 --> 00:14:19,510
what happened was while we were doing

00:14:16,300 --> 00:14:22,750
that we got an old Linux box parade just

00:14:19,510 --> 00:14:26,350
straight HBA no RAID controller plugged

00:14:22,750 --> 00:14:29,579
the disks in Linux booted discovered the

00:14:26,350 --> 00:14:32,260
raid metadata reassembled the raid array

00:14:29,579 --> 00:14:34,060
thanks to DM raid and we're able to

00:14:32,260 --> 00:14:37,720
backup the data on to some new hardware

00:14:34,060 --> 00:14:38,980
and move on so the the capabilities in

00:14:37,720 --> 00:14:41,850
the Linux kernel sometimes just

00:14:38,980 --> 00:14:47,620
outstanding amazing and saved my life

00:14:41,850 --> 00:14:48,820
part 3 a failed raid 5 array now if any

00:14:47,620 --> 00:14:53,620
of you have any of these particular

00:14:48,820 --> 00:14:56,160
Marvel PCIe SATA HBAs they're not very

00:14:53,620 --> 00:14:59,290
good I'm trying not to use bad language

00:14:56,160 --> 00:15:00,850
they are dreadful there's a bunch of

00:14:59,290 --> 00:15:04,329
bugs Ehlers linked at the bottom of this

00:15:00,850 --> 00:15:08,050
but under high i/o which can be just as

00:15:04,329 --> 00:15:10,480
simple as running the raid set check it

00:15:08,050 --> 00:15:12,160
will start producing DMA errors and then

00:15:10,480 --> 00:15:15,940
suddenly drive drives are being booted

00:15:12,160 --> 00:15:17,680
from your raid array so one early one

00:15:15,940 --> 00:15:19,810
Sunday morning while it was performing

00:15:17,680 --> 00:15:26,290
the raid set chair two drives got

00:15:19,810 --> 00:15:28,240
boosted from the array ouch solution we

00:15:26,290 --> 00:15:30,970
changed the HBA which looked at the raid

00:15:28,240 --> 00:15:32,829
set looked at its health now we could go

00:15:30,970 --> 00:15:35,120
into recovery mode but at this point we

00:15:32,829 --> 00:15:39,350
were looking at trying to recover

00:15:35,120 --> 00:15:40,879
9 terabytes of data 8 terabytes of data

00:15:39,350 --> 00:15:43,879
it would take a while to do a full

00:15:40,879 --> 00:15:45,589
recovery would after work through this

00:15:43,879 --> 00:15:48,920
but we looked in the detail of the raid

00:15:45,589 --> 00:15:52,100
set using the MDA DM tools and it turns

00:15:48,920 --> 00:15:55,759
out one disk had a completely different

00:15:52,100 --> 00:15:58,040
event state than the others to had the

00:15:55,759 --> 00:16:00,980
same event level and one was only

00:15:58,040 --> 00:16:03,259
different by one number so we actually

00:16:00,980 --> 00:16:05,930
did a Force assemble looked at the

00:16:03,259 --> 00:16:09,160
degraded array check the LVN metadata

00:16:05,930 --> 00:16:11,660
check the underlying file systems

00:16:09,160 --> 00:16:14,240
virtual machine snapshots all the data

00:16:11,660 --> 00:16:17,209
that was stored there and it came up as

00:16:14,240 --> 00:16:18,800
consistent we did a comparison later

00:16:17,209 --> 00:16:20,899
with a backup or it meant we were able

00:16:18,800 --> 00:16:23,600
to recover the environment in ours

00:16:20,899 --> 00:16:27,279
rather than taking best part of a day

00:16:23,600 --> 00:16:27,279
trying to do a full data asset recovery

00:16:27,970 --> 00:16:35,180
going faster how fast is your disk now

00:16:32,269 --> 00:16:36,800
these are two of my favorite tools well

00:16:35,180 --> 00:16:38,170
there's three tools here booked to

00:16:36,800 --> 00:16:40,999
toolsets

00:16:38,170 --> 00:16:44,480
HD parm honesty is my favorites and as

00:16:40,999 --> 00:16:46,730
you check B it's a HD and SSD or a sand

00:16:44,480 --> 00:16:48,759
mount it's piece of storage it could be

00:16:46,730 --> 00:16:51,290
a flash device it doesn't really matter

00:16:48,759 --> 00:16:53,529
but just a sanity check that you're

00:16:51,290 --> 00:16:56,420
actually giving the I ops you expect

00:16:53,529 --> 00:16:58,699
I've seen cases with sand mounds where

00:16:56,420 --> 00:17:00,829
we're fluctuating between hundreds of

00:16:58,699 --> 00:17:02,360
megabytes a second to tens of megabytes

00:17:00,829 --> 00:17:06,140
a second which means that we've got an

00:17:02,360 --> 00:17:09,199
issue with the the San fabric or there's

00:17:06,140 --> 00:17:11,539
some issue around io allocation in one

00:17:09,199 --> 00:17:14,089
particular case we were consistently

00:17:11,539 --> 00:17:18,679
getting sub 10 megabytes a second off a

00:17:14,089 --> 00:17:20,959
Tier one enterprise Sam that's not good

00:17:18,679 --> 00:17:23,149
that's not healthy particularly when

00:17:20,959 --> 00:17:25,640
you're trying to run higher IOT P

00:17:23,149 --> 00:17:28,659
workloads or virtual machines offer and

00:17:25,640 --> 00:17:31,039
it just happened that that set the wrong

00:17:28,659 --> 00:17:34,610
allocation for our workloads and we were

00:17:31,039 --> 00:17:36,169
in the lowest performance trf3 read F 3

00:17:34,610 --> 00:17:37,970
write a great for testing your flash

00:17:36,169 --> 00:17:39,679
storage devices are making sure that

00:17:37,970 --> 00:17:41,750
they actually behave as you expected

00:17:39,679 --> 00:17:43,940
that they have the correct size that you

00:17:41,750 --> 00:17:46,640
expect them to that they're not a dumb

00:17:43,940 --> 00:17:49,160
device where someone's faking the size

00:17:46,640 --> 00:17:51,410
and it's a 2 gigabyte disk

00:17:49,160 --> 00:17:53,540
to be a 16-gigabyte this but they're

00:17:51,410 --> 00:17:59,840
also great for doing quick speed tests

00:17:53,540 --> 00:18:05,000
of how USB devices are behaving clusters

00:17:59,840 --> 00:18:06,740
clusters when you're dealing with hey

00:18:05,000 --> 00:18:07,940
chase storage requirements usually

00:18:06,740 --> 00:18:12,130
people start talking about I need a

00:18:07,940 --> 00:18:14,750
storage cluster for a particular need

00:18:12,130 --> 00:18:16,580
there was a time not so long ago where I

00:18:14,750 --> 00:18:19,460
had considerably more hair and what

00:18:16,580 --> 00:18:21,500
little I had wasn't grey first

00:18:19,460 --> 00:18:22,370
understand the use cases and the RTOS

00:18:21,500 --> 00:18:23,780
rpoS

00:18:22,370 --> 00:18:25,910
what the performance requirements

00:18:23,780 --> 00:18:28,640
latency requirements otherwise things

00:18:25,910 --> 00:18:30,290
don't quite pan out as you expect again

00:18:28,640 --> 00:18:32,410
I said earlier raid is not a backup

00:18:30,290 --> 00:18:35,060
mechanism dr is not a backup mechanism

00:18:32,410 --> 00:18:37,490
h.a is not a backup mechanism they're

00:18:35,060 --> 00:18:41,240
just simply matters for allowing for

00:18:37,490 --> 00:18:44,210
business continuity and you know high

00:18:41,240 --> 00:18:46,280
availability but you know overall you

00:18:44,210 --> 00:18:48,710
can't go and recover to an older version

00:18:46,280 --> 00:18:50,330
of a file that's amazing the meetings

00:18:48,710 --> 00:18:52,130
I've been in where a customer has said

00:18:50,330 --> 00:18:54,320
oh yeah we've got backup sorted because

00:18:52,130 --> 00:18:57,260
we do hey CheY or we do dr to another

00:18:54,320 --> 00:18:58,460
site and also fully understand the

00:18:57,260 --> 00:19:00,710
difference between active active active

00:18:58,460 --> 00:19:03,440
passive because that can really hurt you

00:19:00,710 --> 00:19:05,000
and always always remember a cluster of

00:19:03,440 --> 00:19:05,450
two is simply a problem waiting to

00:19:05,000 --> 00:19:07,730
happen

00:19:05,450 --> 00:19:09,530
if you don't have a third quorum note if

00:19:07,730 --> 00:19:10,850
you do not have an arbiter which is

00:19:09,530 --> 00:19:13,360
becoming common for some

00:19:10,850 --> 00:19:16,690
software-defined storage technologies

00:19:13,360 --> 00:19:21,620
you're going to get bitten at some point

00:19:16,690 --> 00:19:26,840
so hey CheY NFS have any of you

00:19:21,620 --> 00:19:30,440
implemented AG n FS yeah tried yes good

00:19:26,840 --> 00:19:32,860
right active active is really hard and

00:19:30,440 --> 00:19:35,240
painful because you need an underlying

00:19:32,860 --> 00:19:38,330
cluster aware file system to kind of do

00:19:35,240 --> 00:19:39,830
it properly and if you use them like GFS

00:19:38,330 --> 00:19:41,650
to which is one of the approaches

00:19:39,830 --> 00:19:44,510
performance kind of falls off a cliff

00:19:41,650 --> 00:19:46,700
active passive you can actually do with

00:19:44,510 --> 00:19:49,730
shared storage of a fiber channel FCoE I

00:19:46,700 --> 00:19:53,270
scuzzy or some other method don't do

00:19:49,730 --> 00:19:56,780
over NFS don't try and do a qi NFS with

00:19:53,270 --> 00:19:58,730
an NFS back-end life really is too short

00:19:56,780 --> 00:20:01,049
to try and attempt this thing and I've

00:19:58,730 --> 00:20:02,789
seen someone do it

00:20:01,049 --> 00:20:04,559
so what you do then is you make sure

00:20:02,789 --> 00:20:06,990
that a given partition that you're

00:20:04,559 --> 00:20:08,429
exporting is only active on one node you

00:20:06,990 --> 00:20:11,760
use things like pacemaker to allow

00:20:08,429 --> 00:20:15,149
failover you set up a VIP awesome it

00:20:11,760 --> 00:20:18,419
kind of works it will work but

00:20:15,149 --> 00:20:19,409
understand your application workloads so

00:20:18,419 --> 00:20:23,010
here we are we've got an application

00:20:19,409 --> 00:20:26,760
service that's really latency rative it

00:20:23,010 --> 00:20:29,789
has a sub five millisecond latency due

00:20:26,760 --> 00:20:33,539
to network issues we would regularly see

00:20:29,789 --> 00:20:36,269
that exceeded regardless of how well the

00:20:33,539 --> 00:20:38,130
h NFS environment was working we would

00:20:36,269 --> 00:20:41,429
exceed that five millisecond latency for

00:20:38,130 --> 00:20:46,590
a storage request and the application

00:20:41,429 --> 00:20:48,360
service would fall over so fail also if

00:20:46,590 --> 00:20:50,370
you're doing the managed NFS failover

00:20:48,360 --> 00:20:51,870
because we're going to put node one into

00:20:50,370 --> 00:20:54,120
maintenance and do a managed failover

00:20:51,870 --> 00:20:56,970
because we're doing patching you have to

00:20:54,120 --> 00:21:04,769
allow 30 seconds for things to cleanly

00:20:56,970 --> 00:21:07,440
failover fail scaling two nodes couldn't

00:21:04,769 --> 00:21:08,909
cope with the workload we we started

00:21:07,440 --> 00:21:10,769
with all the workloads on one node and

00:21:08,909 --> 00:21:12,899
then we start with the mixed model where

00:21:10,769 --> 00:21:14,760
these mount points on node a would fail

00:21:12,899 --> 00:21:16,500
to node B and vice versa

00:21:14,760 --> 00:21:18,539
you're now building interesting

00:21:16,500 --> 00:21:21,419
technical debt into your environment and

00:21:18,539 --> 00:21:23,490
the customer this particular occasion

00:21:21,419 --> 00:21:25,380
wasn't keen on automation

00:21:23,490 --> 00:21:27,570
they preferred operational busywork

00:21:25,380 --> 00:21:30,299
because they didn't trust automated

00:21:27,570 --> 00:21:31,889
tools to manage some of this and then we

00:21:30,299 --> 00:21:34,590
had to scale up to three nodes which

00:21:31,889 --> 00:21:36,510
made the things even more complex so

00:21:34,590 --> 00:21:38,159
these workloads on node a would fail to

00:21:36,510 --> 00:21:40,529
be these ones on B would fail to see

00:21:38,159 --> 00:21:43,559
these ones on C would fail to a and so

00:21:40,529 --> 00:21:46,320
forth simply to deal with the growth we

00:21:43,559 --> 00:21:49,380
did look at other technologies like

00:21:46,320 --> 00:21:52,590
Gloucester and the issue here was that

00:21:49,380 --> 00:21:54,330
we the only protocol they were to allow

00:21:52,590 --> 00:21:57,840
us to use was NFS and we still hit some

00:21:54,330 --> 00:22:00,360
of the NFS issues so it didn't actually

00:21:57,840 --> 00:22:03,630
provide the outcome that the client

00:22:00,360 --> 00:22:06,240
wanted and the entire environment had to

00:22:03,630 --> 00:22:09,210
be virtualized so we were running a

00:22:06,240 --> 00:22:11,070
highly available NFS cluster on top of a

00:22:09,210 --> 00:22:13,500
virtual infrastructure that wasn't

00:22:11,070 --> 00:22:16,020
actually highly available

00:22:13,500 --> 00:22:17,760
and once you do this you get no live

00:22:16,020 --> 00:22:19,560
migration because in order for this to

00:22:17,760 --> 00:22:21,960
work you have to turn live migration off

00:22:19,560 --> 00:22:24,090
on the instances so we couldn't meet

00:22:21,960 --> 00:22:27,960
some of the customers other SL A's

00:22:24,090 --> 00:22:31,740
requirements and objectives DRB don't

00:22:27,960 --> 00:22:33,980
I've just just don't okay because

00:22:31,740 --> 00:22:37,260
typically maintenance is a nightmare

00:22:33,980 --> 00:22:39,390
fail back always test fail back whenever

00:22:37,260 --> 00:22:41,130
you do anything like this it's amazing

00:22:39,390 --> 00:22:43,260
how many projects I've seen going we're

00:22:41,130 --> 00:22:45,750
all good we've tested failover but we've

00:22:43,260 --> 00:22:48,150
never ever tested fail back

00:22:45,750 --> 00:22:49,290
cluster - remember I said earlier is a

00:22:48,150 --> 00:22:54,570
problem waiting to happen

00:22:49,290 --> 00:22:57,330
and stone if you ever been a stone is

00:22:54,570 --> 00:22:58,710
Hell where it's just cyclic why we went

00:22:57,330 --> 00:23:00,480
to do maintenance and we could never

00:22:58,710 --> 00:23:04,070
ever get one load up without trying to

00:23:00,480 --> 00:23:06,330
kill the other yeah been there done that

00:23:04,070 --> 00:23:07,920
multiple single points of failure or

00:23:06,330 --> 00:23:10,860
what you often hit with these kind of

00:23:07,920 --> 00:23:11,940
projects so understand the requirements

00:23:10,860 --> 00:23:13,740
and then look at the existing

00:23:11,940 --> 00:23:15,900
infrastructure especially things like

00:23:13,740 --> 00:23:17,010
San arrays that you may be having to

00:23:15,900 --> 00:23:19,380
leverage because the environments

00:23:17,010 --> 00:23:23,910
virtualized associated network

00:23:19,380 --> 00:23:25,890
infrastructure we had a project we

00:23:23,910 --> 00:23:29,010
thought everything was nicely redundant

00:23:25,890 --> 00:23:31,280
and unavailable but actually everything

00:23:29,010 --> 00:23:36,240
went through the same course which and

00:23:31,280 --> 00:23:40,020
when we put high i/o workload on it it

00:23:36,240 --> 00:23:42,000
took down the course which and what is

00:23:40,020 --> 00:23:43,350
the real cost of the solution because

00:23:42,000 --> 00:23:46,500
otherwise these things just turn into

00:23:43,350 --> 00:23:48,300
crazy busy work and actually build up a

00:23:46,500 --> 00:23:49,440
very high project cost whereas you might

00:23:48,300 --> 00:23:51,150
actually be better off going and

00:23:49,440 --> 00:23:52,500
strong-arming your traditional san

00:23:51,150 --> 00:23:59,790
vendor and having them provide a

00:23:52,500 --> 00:24:01,290
solution or capability but often there's

00:23:59,790 --> 00:24:02,880
a statement that the existing storage

00:24:01,290 --> 00:24:05,960
infrastructure isn't reliably an offer

00:24:02,880 --> 00:24:08,700
doesn't meet those are Tio's our POS etc

00:24:05,960 --> 00:24:10,650
but then the overlay this requirement as

00:24:08,700 --> 00:24:12,480
a Sudhir Leo this holds you know

00:24:10,650 --> 00:24:13,950
solution has to be virtualized and the

00:24:12,480 --> 00:24:15,600
virtual infrastructure relies on the

00:24:13,950 --> 00:24:17,820
traditional San or on the existing

00:24:15,600 --> 00:24:19,500
network that you can do some clever

00:24:17,820 --> 00:24:21,060
things to kind of track yourself away

00:24:19,500 --> 00:24:23,370
from that like sticking some disks

00:24:21,060 --> 00:24:25,320
inside your hypervisors and then passing

00:24:23,370 --> 00:24:26,850
those through but then you'll usually

00:24:25,320 --> 00:24:27,299
get someone in the organisation say oh

00:24:26,850 --> 00:24:28,830
no

00:24:27,299 --> 00:24:31,080
but we have to drive my grow everything

00:24:28,830 --> 00:24:33,809
and then you fall into yet another trap

00:24:31,080 --> 00:24:36,169
to be honest one of your greatest assets

00:24:33,809 --> 00:24:38,730
at this point is a really big whiteboard

00:24:36,169 --> 00:24:40,350
plan things out and then stop when litte

00:24:38,730 --> 00:24:41,909
bit red marks through the things that

00:24:40,350 --> 00:24:45,389
are single points of failure for the

00:24:41,909 --> 00:24:47,519
customer I'll start looking at

00:24:45,389 --> 00:24:49,200
software-defined storage both bluster

00:24:47,519 --> 00:24:52,159
and Safa are a widely supported

00:24:49,200 --> 00:24:54,840
open-source projects

00:24:52,159 --> 00:24:56,759
Gloucester can run on virtual or bare

00:24:54,840 --> 00:24:59,730
metal it's very very simple to implement

00:24:56,759 --> 00:25:02,129
and supports ifs NFS P NFS and the

00:24:59,730 --> 00:25:04,499
Gloucester fuse driver seth has a

00:25:02,129 --> 00:25:06,749
broader range of capabilities and can

00:25:04,499 --> 00:25:08,999
really do super scale and I'm actually

00:25:06,749 --> 00:25:11,489
giving a talk tomorrow about the work

00:25:08,999 --> 00:25:13,679
we've done around rock and Ceph for

00:25:11,489 --> 00:25:15,539
containerized management of Ceph and

00:25:13,679 --> 00:25:21,090
kind of the data operational side of

00:25:15,539 --> 00:25:25,470
safe ah now what the fsck what else can

00:25:21,090 --> 00:25:28,109
we kept dust and humidity we had a small

00:25:25,470 --> 00:25:31,739
machine room that got resized we had to

00:25:28,109 --> 00:25:33,239
pour in a storeroom and they came in and

00:25:31,739 --> 00:25:34,980
they put in a new drywall and they

00:25:33,239 --> 00:25:36,989
covered the racks with dust sheets and

00:25:34,980 --> 00:25:39,330
they sanded everything and they took the

00:25:36,989 --> 00:25:41,309
dust sheets away and we came in on

00:25:39,330 --> 00:25:43,470
Monday and we had to vacuum out every

00:25:41,309 --> 00:25:45,570
server we had to wipe down the machine

00:25:43,470 --> 00:25:47,009
room there was dust everywhere and over

00:25:45,570 --> 00:25:51,049
the course of approximately nine months

00:25:47,009 --> 00:25:53,549
every single hard drive failed because

00:25:51,049 --> 00:25:55,859
plaster dust can get really fine and

00:25:53,549 --> 00:26:00,350
it's amazing how far inside a disk it

00:25:55,859 --> 00:26:04,769
can get humidity the aircon unit

00:26:00,350 --> 00:26:08,659
separate location leaked on a carpeted

00:26:04,769 --> 00:26:11,789
machine room yeah these things do happen

00:26:08,659 --> 00:26:13,409
and the temporary aircon unit couldn't

00:26:11,789 --> 00:26:15,299
cope with it and in fact I remember now

00:26:13,409 --> 00:26:18,090
there was a secondary impact was the

00:26:15,299 --> 00:26:20,789
temporary aircon unit was wired to a

00:26:18,090 --> 00:26:24,570
plug that wasn't active at weekends

00:26:20,789 --> 00:26:27,149
because of some stupid timeout so at the

00:26:24,570 --> 00:26:29,899
weekend and the carpet wasn't entirely

00:26:27,149 --> 00:26:32,759
dry the room turned into a giant sauna

00:26:29,899 --> 00:26:35,820
so over approximately a six-month period

00:26:32,759 --> 00:26:38,460
we had to replace every goddamn hard

00:26:35,820 --> 00:26:40,299
drive and in fact I've heard reports

00:26:38,460 --> 00:26:42,639
subsequently and none

00:26:40,299 --> 00:26:46,509
of the chassis started to rust out a

00:26:42,639 --> 00:26:49,419
year or two later so Dustin humidity our

00:26:46,509 --> 00:26:51,700
killers avoid them at all costs so these

00:26:49,419 --> 00:26:53,979
days with IOT sensors you can pretty

00:26:51,700 --> 00:26:55,359
much rig your data center to detect a

00:26:53,979 --> 00:26:57,399
lot of these things all you little small

00:26:55,359 --> 00:26:59,229
you know corporate sites that still have

00:26:57,399 --> 00:27:03,989
some equipment on-premise highly

00:26:59,229 --> 00:27:08,859
recommend it expect the unexpected

00:27:03,989 --> 00:27:11,619
this was a doozy corrupted LVM so he had

00:27:08,859 --> 00:27:14,019
some enterprise systems consuming

00:27:11,619 --> 00:27:19,419
multiple loans office and into a single

00:27:14,019 --> 00:27:21,070
volume group under LVM fibre channel one

00:27:19,419 --> 00:27:23,169
of the fibre channel loans had actually

00:27:21,070 --> 00:27:26,739
been allocated to two systems by

00:27:23,169 --> 00:27:31,029
accidents there's no partition table

00:27:26,739 --> 00:27:32,859
present so the team managing the other

00:27:31,029 --> 00:27:34,659
system assumed they had a nice clean

00:27:32,859 --> 00:27:37,509
disk and partitioned it and started

00:27:34,659 --> 00:27:43,229
using it and it was great until our

00:27:37,509 --> 00:27:49,869
database fell over our primary Tijuana

00:27:43,229 --> 00:27:51,549
customer-facing database when so because

00:27:49,869 --> 00:27:54,190
that particular loan was in the middle

00:27:51,549 --> 00:27:57,450
of the volume group somewhere and was

00:27:54,190 --> 00:27:59,709
actually being used by a bunch of LVS

00:27:57,450 --> 00:28:03,519
that's a really interesting one to

00:27:59,709 --> 00:28:05,679
diagnose what the hell went wrong and in

00:28:03,519 --> 00:28:09,399
fact we took out the other system when

00:28:05,679 --> 00:28:11,259
we try to diagnose our problem so

00:28:09,399 --> 00:28:13,509
recommendation personally is to always

00:28:11,259 --> 00:28:16,419
put a partition table on I know some

00:28:13,509 --> 00:28:18,519
operations teams around here often don't

00:28:16,419 --> 00:28:20,529
do that one with the dealing with you

00:28:18,519 --> 00:28:21,909
know scaling out volume groups because

00:28:20,529 --> 00:28:24,669
they just want to consume the whole disk

00:28:21,909 --> 00:28:27,849
but as a protection mechanism I always

00:28:24,669 --> 00:28:29,049
put a partition table now this was

00:28:27,849 --> 00:28:30,789
another great one we were seeing a

00:28:29,049 --> 00:28:34,059
corrupted file system on a virtual

00:28:30,789 --> 00:28:36,639
machine the XFS file system was showing

00:28:34,059 --> 00:28:39,609
regular consistency checks consistency

00:28:36,639 --> 00:28:42,039
issues and rebooting that particular VM

00:28:39,609 --> 00:28:43,599
was inconsistent it would come up it

00:28:42,039 --> 00:28:45,399
would be fine it would run great and

00:28:43,599 --> 00:28:46,839
then as part of trying to diagnose a

00:28:45,399 --> 00:28:48,099
root cause the problem would reboot it

00:28:46,839 --> 00:28:50,639
and if things would get really weird

00:28:48,099 --> 00:28:52,690
again and then we started to notice

00:28:50,639 --> 00:28:53,710
another virtual machine was having

00:28:52,690 --> 00:28:56,230
problems

00:28:53,710 --> 00:28:58,630
this was getting really odd where we

00:28:56,230 --> 00:29:00,970
suffering a memory issue where we

00:28:58,630 --> 00:29:05,470
suffering a hardware failure maybe the

00:29:00,970 --> 00:29:06,580
CPU was going on the the hypervisor know

00:29:05,470 --> 00:29:09,850
that virtual machines on a different

00:29:06,580 --> 00:29:13,330
hypervisor so turns out there was poor

00:29:09,850 --> 00:29:15,850
LUN grouping again off the backend

00:29:13,330 --> 00:29:18,610
storage virtual guests are hosted on KVM

00:29:15,850 --> 00:29:21,610
there's a bunch of Long's mapped into

00:29:18,610 --> 00:29:23,410
the hypervisor but it turns out some of

00:29:21,610 --> 00:29:28,300
those loans have been also mapped into

00:29:23,410 --> 00:29:31,320
some guests and due to the way LV M

00:29:28,300 --> 00:29:34,090
works is we had a weird race condition

00:29:31,320 --> 00:29:37,540
because all the VG's had the same name

00:29:34,090 --> 00:29:42,790
all the LVS had the same name so we had

00:29:37,540 --> 00:29:46,510
slash Linux VG slash var slash Linux VG

00:29:42,790 --> 00:29:48,280
slash data and occasionally a couple of

00:29:46,510 --> 00:29:51,400
the systems would be cross mounting each

00:29:48,280 --> 00:29:56,860
other's data and var partitions at the

00:29:51,400 --> 00:29:59,230
same time if you are building off

00:29:56,860 --> 00:30:01,060
templates in a virtual environment I

00:29:59,230 --> 00:30:03,460
highly recommend you make sure you mount

00:30:01,060 --> 00:30:06,370
by UUID and as part of your deployment

00:30:03,460 --> 00:30:11,320
process make sure the injects unique EU

00:30:06,370 --> 00:30:12,640
IDs when you do the provisioning in this

00:30:11,320 --> 00:30:15,040
particular case we went through a whole

00:30:12,640 --> 00:30:18,070
world of pain trying to diagnose the

00:30:15,040 --> 00:30:20,320
issue I actually was able to spot it

00:30:18,070 --> 00:30:23,170
because it actually managed to cause it

00:30:20,320 --> 00:30:24,400
a few years earlier at home it's amazing

00:30:23,170 --> 00:30:25,690
what you can do in your own lab on your

00:30:24,400 --> 00:30:27,550
own time and with your own degree of

00:30:25,690 --> 00:30:28,270
stupidity but I've actually had this

00:30:27,550 --> 00:30:30,370
happen before

00:30:28,270 --> 00:30:36,370
and I'm literally looking at logs going

00:30:30,370 --> 00:30:38,920
no no no not again so yeah or expect the

00:30:36,370 --> 00:30:42,520
unexpected so what next where are we

00:30:38,920 --> 00:30:45,100
going in this ecosystem so they say

00:30:42,520 --> 00:30:47,380
everything my clickers not very happy

00:30:45,100 --> 00:30:48,550
everything old is new again so we all

00:30:47,380 --> 00:30:51,850
know we're always going to need more

00:30:48,550 --> 00:30:53,590
storage and the fact is simple factors

00:30:51,850 --> 00:30:54,490
in the enterprise hard drives aren't

00:30:53,590 --> 00:30:56,170
dead yet

00:30:54,490 --> 00:30:58,810
all right spinning Russ still has its

00:30:56,170 --> 00:31:01,960
place in its price point but you know at

00:30:58,810 --> 00:31:04,360
some point the TCO 4 flash will drop

00:31:01,960 --> 00:31:05,950
below that of spinning disk and if you

00:31:04,360 --> 00:31:07,450
want to see an example of where that may

00:31:05,950 --> 00:31:11,080
happen sooner rather than later

00:31:07,450 --> 00:31:12,520
look at ETS FF this is a hyper dense

00:31:11,080 --> 00:31:14,740
flash storage platform you can

00:31:12,520 --> 00:31:17,950
effectively put a petabyte of storage in

00:31:14,740 --> 00:31:19,750
1u so this is a look at ruler format it

00:31:17,950 --> 00:31:22,360
was originally called the ruler was an

00:31:19,750 --> 00:31:24,100
Intel format and now there's a refined

00:31:22,360 --> 00:31:29,500
version that a bunch of vendors are

00:31:24,100 --> 00:31:31,930
adopting this is PCIe interconnect on

00:31:29,500 --> 00:31:35,980
the end but it's a ruler shape form

00:31:31,930 --> 00:31:39,490
factor really quite cool and that's with

00:31:35,980 --> 00:31:40,960
today's storage densities but these

00:31:39,490 --> 00:31:43,420
things are estimated to have a

00:31:40,960 --> 00:31:46,300
Enterprise production life of 15 years

00:31:43,420 --> 00:31:48,340
if used correctly versus having to

00:31:46,300 --> 00:31:50,860
replace spinning discs every 3 or 5

00:31:48,340 --> 00:31:53,140
years so you told cost of ownership

00:31:50,860 --> 00:31:54,970
suddenly starts to shift particularly

00:31:53,140 --> 00:31:59,530
when you take parent and and cooling

00:31:54,970 --> 00:32:01,090
into consideration if you've been to LCA

00:31:59,530 --> 00:32:02,580
before you know we've had a lot of talks

00:32:01,090 --> 00:32:05,860
about things like persistent memory

00:32:02,580 --> 00:32:08,620
persistent memory at the RAM level and

00:32:05,860 --> 00:32:10,780
persistent memory type devices at the

00:32:08,620 --> 00:32:12,790
storage level things like octane from

00:32:10,780 --> 00:32:16,690
Intel is a good example where you can

00:32:12,790 --> 00:32:19,180
actually have memory that persists

00:32:16,690 --> 00:32:22,600
through a reboot and then the cloud

00:32:19,180 --> 00:32:23,860
players are consistently innovating

00:32:22,600 --> 00:32:25,870
because they want us to spend more money

00:32:23,860 --> 00:32:27,760
and I think our job in terms of the open

00:32:25,870 --> 00:32:29,470
source communities to out-innovate them

00:32:27,760 --> 00:32:32,140
and come up with more interesting ways

00:32:29,470 --> 00:32:36,280
for us to either perform analytics and

00:32:32,140 --> 00:32:38,160
use storage or find new use cases but

00:32:36,280 --> 00:32:41,850
the one thing I'll leave you from is

00:32:38,160 --> 00:32:45,190
something when I was researching some

00:32:41,850 --> 00:32:50,290
some of the futures around this is a lot

00:32:45,190 --> 00:32:53,260
of the pundits say no this isn't dead or

00:32:50,290 --> 00:32:55,630
rather this isn't dead it's still by far

00:32:53,260 --> 00:32:57,700
the most common enterprise backup medium

00:32:55,630 --> 00:32:59,590
and whilst you may think you're backing

00:32:57,700 --> 00:33:02,260
up to the cloud there's a very good

00:32:59,590 --> 00:33:05,290
chance your cloud somewhere is still

00:33:02,260 --> 00:33:11,070
backing up to tape it's a tape ain't

00:33:05,290 --> 00:33:13,480
dead yet so thank you for your time

00:33:11,070 --> 00:33:17,040
hopefully I've given you some tips and

00:33:13,480 --> 00:33:17,040
tricks are there any questions

00:33:18,970 --> 00:33:22,660
it's very quiet

00:33:27,780 --> 00:33:41,460
and there we go thanks for that what do

00:33:35,850 --> 00:33:46,500
you use to back up at home I you work

00:33:41,460 --> 00:33:48,890
for my personal data I use a cycle of

00:33:46,500 --> 00:33:55,590
very very large USB three hard drives

00:33:48,890 --> 00:33:57,540
that I rotate through for large data and

00:33:55,590 --> 00:33:59,580
then other data I'll also have an

00:33:57,540 --> 00:34:01,260
encrypted and that's all encrypted so I

00:33:59,580 --> 00:34:02,670
use looks encryption on the hard drive

00:34:01,260 --> 00:34:05,040
so if the hard drive ever gets stolen

00:34:02,670 --> 00:34:06,660
they just see a partition saying this is

00:34:05,040 --> 00:34:08,070
Steve Alice's hard drive do not touch

00:34:06,660 --> 00:34:11,340
and then the rest of the data is

00:34:08,070 --> 00:34:13,500
encrypted or I do encrypted copies out

00:34:11,340 --> 00:34:15,060
into certain public cloud resources so

00:34:13,500 --> 00:34:16,590
multiple copies in multiple places

00:34:15,060 --> 00:34:20,310
depending on the criticality of the

00:34:16,590 --> 00:34:22,320
information um I've never heard of a

00:34:20,310 --> 00:34:24,379
ratio coding before this is a new thing

00:34:22,320 --> 00:34:26,790
could you spend 10 seconds on that all

00:34:24,379 --> 00:34:27,990
sorry I didn't hear that erasure coding

00:34:26,790 --> 00:34:29,460
so we're going to be quite there you

00:34:27,990 --> 00:34:31,710
were talking about a razer coding for

00:34:29,460 --> 00:34:33,870
raid replacement on steroids

00:34:31,710 --> 00:34:38,970
i've never heard of this before never

00:34:33,870 --> 00:34:41,220
heard of erasure coding so that's a hole

00:34:38,970 --> 00:34:45,600
so how many of you have never heard of

00:34:41,220 --> 00:34:47,190
erasure coding before okay erasure code

00:34:45,600 --> 00:34:49,950
is incredibly popular in the Software

00:34:47,190 --> 00:34:51,780
Defined ecosystem I could drop I've

00:34:49,950 --> 00:34:54,060
dropped our whole deck just talking

00:34:51,780 --> 00:34:56,220
about that there's some really good

00:34:54,060 --> 00:34:59,820
write-ups on dating sites like storage

00:34:56,220 --> 00:35:01,980
review Wikipedia's architect articles

00:34:59,820 --> 00:35:05,580
reasonably good the Ceph documentation

00:35:01,980 --> 00:35:07,470
has some really good references but

00:35:05,580 --> 00:35:13,500
erasure coding is effectively a way to

00:35:07,470 --> 00:35:17,280
stripe data and these stripe data across

00:35:13,500 --> 00:35:21,870
multiple drives designed specifically

00:35:17,280 --> 00:35:24,030
around recovery and a failure set

00:35:21,870 --> 00:35:26,550
management so I'll say I am happy to

00:35:24,030 --> 00:35:29,400
suffer a certain level of failure which

00:35:26,550 --> 00:35:32,250
is your plus M or plus n or whatever you

00:35:29,400 --> 00:35:33,930
want to however you want to factor it so

00:35:32,250 --> 00:35:36,210
I'm happy to suffer our failure rate of

00:35:33,930 --> 00:35:39,650
three but the nice thing by Razer code

00:35:36,210 --> 00:35:42,600
is it's designed to work across

00:35:39,650 --> 00:35:44,820
systems so when we deal with something

00:35:42,600 --> 00:35:46,410
like SEF and we do a rager coding we

00:35:44,820 --> 00:35:48,270
don't do raid in an array and then or

00:35:46,410 --> 00:35:51,780
Asia code we arrays your code at the

00:35:48,270 --> 00:35:53,400
disk level across the entire array so

00:35:51,780 --> 00:35:55,770
you can actually just take out an entire

00:35:53,400 --> 00:35:57,810
and take out have a disk fail and

00:35:55,770 --> 00:35:59,520
replace it or have a server fail and

00:35:57,810 --> 00:36:01,619
replace it and the raid will heal the

00:35:59,520 --> 00:36:04,140
array will heal itself and it's heal

00:36:01,619 --> 00:36:06,030
performances way above typical raid

00:36:04,140 --> 00:36:08,820
technologies it's usually much faster at

00:36:06,030 --> 00:36:10,619
healing and behaving so it's quite nice

00:36:08,820 --> 00:36:12,930
and elegant but it does allow for very

00:36:10,619 --> 00:36:17,910
elastic kind of +1 scaling depending on

00:36:12,930 --> 00:36:20,100
the implementation a lot of proprietary

00:36:17,910 --> 00:36:26,220
storage vendors have their own variants

00:36:20,100 --> 00:36:30,390
on a razor coding under the hood there's

00:36:26,220 --> 00:36:32,700
lots now hi I'm Ben I'm a FreeBSD user I

00:36:30,390 --> 00:36:39,720
just wanted to ask apart from licensing

00:36:32,700 --> 00:36:40,800
what do you think it was Oedipus you did

00:36:39,720 --> 00:36:46,590
see who I work for

00:36:40,800 --> 00:36:48,390
I can't probably answer that Linux wrote

00:36:46,590 --> 00:36:54,090
a really interesting piece about it

00:36:48,390 --> 00:36:56,670
recently I'll defer to him and I won't

00:36:54,090 --> 00:37:00,450
use btrfs because I've already had too

00:36:56,670 --> 00:37:03,570
many experiments in losing my data hello

00:37:00,450 --> 00:37:06,060
my name is Amy thank you for that really

00:37:03,570 --> 00:37:07,680
fascinating talk I'm interested in

00:37:06,060 --> 00:37:10,170
something you mentioned earlier about

00:37:07,680 --> 00:37:14,060
data decay and I was wondering should

00:37:10,170 --> 00:37:18,119
data decay or should we store it forever

00:37:14,060 --> 00:37:21,840
there's a chap NAT talking to new is X

00:37:18,119 --> 00:37:23,220
O'Reilly he's a he's based in New

00:37:21,840 --> 00:37:25,470
Zealand and he gave a talk

00:37:23,220 --> 00:37:27,300
a number of years ago for the privacy

00:37:25,470 --> 00:37:29,550
Commission and he said should actually

00:37:27,300 --> 00:37:31,950
be a statute of limitations on your

00:37:29,550 --> 00:37:33,810
social media presence you know should

00:37:31,950 --> 00:37:38,369
there be a point in time where all your

00:37:33,810 --> 00:37:40,500
social media missteps expire naturally

00:37:38,369 --> 00:37:43,200
yes it's really hard an awful lot of

00:37:40,500 --> 00:37:46,380
data that we create today will live

00:37:43,200 --> 00:37:48,950
forever if the various cloud players

00:37:46,380 --> 00:37:52,640
have their way

00:37:48,950 --> 00:37:55,280
but also a critical thing often in the

00:37:52,640 --> 00:37:57,220
enterprise's secure data destruction so

00:37:55,280 --> 00:37:59,570
there's some really cool tools around

00:37:57,220 --> 00:38:02,420
taking an existing storage device and

00:37:59,570 --> 00:38:03,830
truly deleting it rewriting it to make

00:38:02,420 --> 00:38:08,660
sure that it's almost impossible to

00:38:03,830 --> 00:38:11,870
recover even whether you know you know

00:38:08,660 --> 00:38:15,590
intense digital forensic devices so and

00:38:11,870 --> 00:38:21,730
we're things like gdpr and some other

00:38:15,590 --> 00:38:24,050
data restrictions around geography or

00:38:21,730 --> 00:38:26,030
you know credit card information or the

00:38:24,050 --> 00:38:27,110
uses of personal information there's an

00:38:26,030 --> 00:38:30,170
awful lot of stuff going on around

00:38:27,110 --> 00:38:33,230
protecting data or hiding data of who's

00:38:30,170 --> 00:38:35,510
decoding data or another one is trying

00:38:33,230 --> 00:38:38,990
to make sure data is cleanse so that I

00:38:35,510 --> 00:38:40,670
was talking to someone who works at the

00:38:38,990 --> 00:38:43,670
UN and one of their things is they want

00:38:40,670 --> 00:38:45,770
to provide cleanse datasets so that

00:38:43,670 --> 00:38:47,620
things are properly anonymized so you

00:38:45,770 --> 00:38:50,900
can perform analysis without giving away

00:38:47,620 --> 00:38:57,530
too much about who the people are that's

00:38:50,900 --> 00:38:59,450
really important yeah I'm here for the

00:38:57,530 --> 00:39:01,490
rest of the week I'm also giving a talk

00:38:59,450 --> 00:39:05,540
tomorrow in the container mini Kampf

00:39:01,490 --> 00:39:09,710
about rook's eff so thanks for your time

00:39:05,540 --> 00:39:09,710

YouTube URL: https://www.youtube.com/watch?v=PUuTUxnqX0g


