Title: "Synchronised Playback with GStreamer" - Arun Raghavan (LCA 2020)
Publication date: 2020-01-14
Playlist: linux.conf.au 2020
Description: 
	Arun Raghavan

https://lca2020.linux.org.au/schedule/presentation/171/

GStreamer is flexible a multimedia framework used to build various different sorts of systems that deal with audio and video.

One aspect of the framework is the infrastructure for implementing synchronised media playback across devices. This finds use in various scenarios ranging from video walls to synchronised media displays to multiroom audio systems.

This talk will explore the GstSyncServer library, which provides a flexible, high-level API for building such systems. This will include examples of how it could be used, the design of the API, how it may be used to implement custom synchronisation mechanisms, and areas where contributors could get involved.

linux.conf.au is a conference about the Linux operating system, and all aspects of the thriving ecosystem of Free and Open Source Software that has grown up around it. Run since 1999, in a different Australian or New Zealand city each year, by a team of local volunteers, LCA invites more than 500 people to learn from the people who shape the future of Open Source. For more information on the conference see https://linux.conf.au/

Produced by NDV: https://youtube.com/channel/UCQ7dFBzZGlBvtU2hCecsBBg?sub_confirmation=1

#linux.conf.au #linux #foss #opensource

Mon Jan 13 11:20:00 2020 at Room 5
Captions: 
	00:00:04,310 --> 00:00:09,540
okay welcome back so now we've got Erin

00:00:07,770 --> 00:00:13,170
talking about synchronized playback in

00:00:09,540 --> 00:00:15,240
GStreamer if the opportunity arises to

00:00:13,170 --> 00:00:17,029
ask questions could you indicate you

00:00:15,240 --> 00:00:19,949
wish to and I'll bring the mic to you

00:00:17,029 --> 00:00:21,990
that saves the speaker having to repeat

00:00:19,949 --> 00:00:25,740
your question for the AV recording and

00:00:21,990 --> 00:00:27,140
the streaming so thanks for that good

00:00:25,740 --> 00:00:31,380
thanks

00:00:27,140 --> 00:00:35,040
alright everyone I'm idil Raghavan here

00:00:31,380 --> 00:00:37,620
from Bangalore India I work on both

00:00:35,040 --> 00:00:39,300
audio and gstreamer so if you use Linux

00:00:37,620 --> 00:00:43,079
and your audio is broken it's almost

00:00:39,300 --> 00:00:46,530
certainly not my fault so today I'll be

00:00:43,079 --> 00:00:51,120
talking a little bit about synchronized

00:00:46,530 --> 00:00:53,160
Network playback with gstreamer so quick

00:00:51,120 --> 00:00:58,559
show of hands who's familiar with

00:00:53,160 --> 00:01:00,149
GStreamer already ok reasonably large

00:00:58,559 --> 00:01:01,320
number of people but it's about half of

00:01:00,149 --> 00:01:04,610
you so I'm going to just walk through a

00:01:01,320 --> 00:01:08,460
very quick introduction of GStreamer

00:01:04,610 --> 00:01:11,070
what GStreamer is is a multimedia

00:01:08,460 --> 00:01:14,820
framework it's basically a library that

00:01:11,070 --> 00:01:18,119
you can use to build your application if

00:01:14,820 --> 00:01:20,220
it deals with audio or video or both it

00:01:18,119 --> 00:01:22,200
is pipeline based and I'll just walk

00:01:20,220 --> 00:01:25,920
through that in a second and it is

00:01:22,200 --> 00:01:29,610
modular so how G's FEMA has written is

00:01:25,920 --> 00:01:33,000
that there is a plug-in system plugins

00:01:29,610 --> 00:01:34,920
provide elements which provide small

00:01:33,000 --> 00:01:38,159
bits of functionality that you string

00:01:34,920 --> 00:01:42,090
together and and build your application

00:01:38,159 --> 00:01:45,740
using that right so if you wanted to do

00:01:42,090 --> 00:01:48,630
if you wanted to playback an mp3 file

00:01:45,740 --> 00:01:50,670
what you might do is use the file source

00:01:48,630 --> 00:01:54,119
element which will read data from the

00:01:50,670 --> 00:01:58,079
file system provide that to an mp3

00:01:54,119 --> 00:02:01,170
decoder element which will wrap some mp3

00:01:58,079 --> 00:02:04,649
decoding library because those mp3

00:02:01,170 --> 00:02:06,750
frames provide PCM data which you might

00:02:04,649 --> 00:02:10,349
want to convert using the audio convert

00:02:06,750 --> 00:02:11,700
element into or whatever format can be

00:02:10,349 --> 00:02:15,810
played on your

00:02:11,700 --> 00:02:18,800
systems so that might be pulseaudio also

00:02:15,810 --> 00:02:22,019
on linux or on windows you'd use

00:02:18,800 --> 00:02:24,530
whatever direct sound or wave out or

00:02:22,019 --> 00:02:28,319
whatever that is we have support for iOS

00:02:24,530 --> 00:02:29,700
Mac OS Android as well right so the

00:02:28,319 --> 00:02:32,580
audience thing can be a platform

00:02:29,700 --> 00:02:34,560
dependent sake so all of these elements

00:02:32,580 --> 00:02:36,780
put together with some additional

00:02:34,560 --> 00:02:39,599
bookkeeping is basically a pipeline

00:02:36,780 --> 00:02:44,160
right slightly more complicated example

00:02:39,599 --> 00:02:46,080
that involves our audio and video so you

00:02:44,160 --> 00:02:48,930
might have audio and video in a Matroska

00:02:46,080 --> 00:02:50,579
file so you similar to before you might

00:02:48,930 --> 00:02:54,560
have a file source that reads from the

00:02:50,579 --> 00:02:57,180
file system gives that data to

00:02:54,560 --> 00:02:59,940
demultiplexer element which splits up

00:02:57,180 --> 00:03:02,280
the audio and video stream and these

00:02:59,940 --> 00:03:04,019
with them separately and like before we

00:03:02,280 --> 00:03:05,970
have one part of the pipeline dealing

00:03:04,019 --> 00:03:07,950
with audio decoding it and playing it

00:03:05,970 --> 00:03:09,599
back using a sync another part of it

00:03:07,950 --> 00:03:11,940
decoding video and playing it back with

00:03:09,599 --> 00:03:13,860
a sync right and this is again the whole

00:03:11,940 --> 00:03:16,280
thing together is our gstreamer pipeline

00:03:13,860 --> 00:03:20,010
and you can imagine now that

00:03:16,280 --> 00:03:22,079
constructing any sort of audio/video

00:03:20,010 --> 00:03:24,780
processing pipeline should be fairly

00:03:22,079 --> 00:03:26,730
simple if we have the elements that we

00:03:24,780 --> 00:03:29,720
need so if you wanted to build something

00:03:26,730 --> 00:03:32,010
to capture audio and capture your video

00:03:29,720 --> 00:03:34,590
compress it and send it out over the

00:03:32,010 --> 00:03:36,630
Internet to talk to somebody else over

00:03:34,590 --> 00:03:38,220
video call that would also be possible

00:03:36,630 --> 00:03:40,980
with gstreamer and multiple such

00:03:38,220 --> 00:03:46,200
applications are possible with gstreamer

00:03:40,980 --> 00:03:48,650
right so that's enough about G Sima what

00:03:46,200 --> 00:03:51,030
we're here to talk about today is

00:03:48,650 --> 00:03:52,709
synchronized network playback right and

00:03:51,030 --> 00:03:54,359
why would you want to do that there are

00:03:52,709 --> 00:03:57,239
multiple different sorts of applications

00:03:54,359 --> 00:04:01,250
that you could conceive of so one are

00:03:57,239 --> 00:04:04,139
one possible use is multi-room audio

00:04:01,250 --> 00:04:06,930
like you might get with a Sonos or

00:04:04,139 --> 00:04:09,690
google home system where you have one or

00:04:06,930 --> 00:04:11,639
more speakers in a room or in multiple

00:04:09,690 --> 00:04:15,180
rooms in your house and you want to play

00:04:11,639 --> 00:04:19,530
a song on all of these speakers at the

00:04:15,180 --> 00:04:22,849
same time another application that finds

00:04:19,530 --> 00:04:24,870
use in various kinds of places including

00:04:22,849 --> 00:04:27,090
art installation

00:04:24,870 --> 00:04:28,919
advertising displays all sorts of things

00:04:27,090 --> 00:04:32,310
our video walls where you might have

00:04:28,919 --> 00:04:34,320
multiple multiple displaying units that

00:04:32,310 --> 00:04:36,960
you want to put together in an array and

00:04:34,320 --> 00:04:38,460
make one big screen off right so that's

00:04:36,960 --> 00:04:39,990
another place where you might have each

00:04:38,460 --> 00:04:42,000
of those displays connected to a

00:04:39,990 --> 00:04:43,860
separate computer these things are

00:04:42,000 --> 00:04:45,870
connected together and they're playing

00:04:43,860 --> 00:04:49,800
different parts of the same video all at

00:04:45,870 --> 00:04:52,169
the same time we also have cases where

00:04:49,800 --> 00:04:54,419
in cars and planes you might have

00:04:52,169 --> 00:04:55,860
different displays that are playing back

00:04:54,419 --> 00:04:57,720
the same audio and video stream and

00:04:55,860 --> 00:04:59,669
these also need to be synchronized right

00:04:57,720 --> 00:05:02,940
and the idea with all of these is that

00:04:59,669 --> 00:05:04,860
you've got a single single stream that

00:05:02,940 --> 00:05:06,389
you're playing at the same time and you

00:05:04,860 --> 00:05:11,910
don't want these to be out of sync you

00:05:06,389 --> 00:05:14,550
want them to be playing together so I'm

00:05:11,910 --> 00:05:17,669
gonna take a quick detour into how how

00:05:14,550 --> 00:05:21,570
synchronization is done using gstreamer

00:05:17,669 --> 00:05:24,510
this is not actually the core of this

00:05:21,570 --> 00:05:27,479
talk if you want to dig into more

00:05:24,510 --> 00:05:30,389
details there was a talk in adults here

00:05:27,479 --> 00:05:33,270
in 2016 by Sebastian that talks about

00:05:30,389 --> 00:05:36,660
the actual mechanism itself but to

00:05:33,270 --> 00:05:38,940
briefly go through what is involved in

00:05:36,660 --> 00:05:42,150
this process right there are three

00:05:38,940 --> 00:05:44,190
aspects of sync that you need to deal

00:05:42,150 --> 00:05:46,560
with to have synchronized blame across

00:05:44,190 --> 00:05:48,720
the network the first thing is having

00:05:46,560 --> 00:05:51,960
the media accessible to all your notes

00:05:48,720 --> 00:05:54,990
right and this can be done in multiple

00:05:51,960 --> 00:05:58,139
ways you could have say an HTTP server

00:05:54,990 --> 00:06:00,000
somewhere that's serving up the media to

00:05:58,139 --> 00:06:01,650
all your nodes or you could just have

00:06:00,000 --> 00:06:03,570
all the nodes somehow synchronize the

00:06:01,650 --> 00:06:06,330
files and have them available locally or

00:06:03,570 --> 00:06:08,370
you could have use something like our

00:06:06,330 --> 00:06:11,520
TSP you could use not ESP server to make

00:06:08,370 --> 00:06:14,070
media available to all your nodes so

00:06:11,520 --> 00:06:17,220
that's the first part the second part is

00:06:14,070 --> 00:06:19,740
timing and this is in some sense the

00:06:17,220 --> 00:06:23,810
most crucial part crucial piece of the

00:06:19,740 --> 00:06:27,630
problem right so if you remember the the

00:06:23,810 --> 00:06:29,700
pipeline that we showed where you have

00:06:27,630 --> 00:06:31,279
audio and video playing right even on a

00:06:29,700 --> 00:06:33,409
on a single system

00:06:31,279 --> 00:06:34,579
we do need to synchronize audio and

00:06:33,409 --> 00:06:35,719
video so if you're playing audio and

00:06:34,579 --> 00:06:37,339
video together you want to make sure

00:06:35,719 --> 00:06:39,079
that it's synchronized so you have lip

00:06:37,339 --> 00:06:40,069
sync and when somebody's saying

00:06:39,079 --> 00:06:43,219
something it looks like they're actually

00:06:40,069 --> 00:06:45,139
saying it how we manage this in a

00:06:43,219 --> 00:06:47,449
gstreamer pipeline running on a single

00:06:45,139 --> 00:06:50,119
machine is that the pipeline has the

00:06:47,449 --> 00:06:54,049
notion of a clock right this clock is

00:06:50,119 --> 00:06:57,679
what all arm rendering in the sings is

00:06:54,049 --> 00:07:02,089
synchronized to so usually what happens

00:06:57,679 --> 00:07:04,459
is you have you have buffers that are

00:07:02,089 --> 00:07:06,559
flowing from the file or to the audio

00:07:04,459 --> 00:07:09,169
and video paths of the pipeline and

00:07:06,559 --> 00:07:11,299
these buffers at my instant so they so

00:07:09,169 --> 00:07:12,619
in the in the simplest case you might

00:07:11,299 --> 00:07:15,559
imagine that the time stamp

00:07:12,619 --> 00:07:18,469
start from 0 and progress say if the

00:07:15,559 --> 00:07:20,269
first buffer is 100 milliseconds long

00:07:18,469 --> 00:07:21,860
the first time stamp would be 0 and the

00:07:20,269 --> 00:07:24,110
second buffer would have a time stamp of

00:07:21,860 --> 00:07:26,799
100 milliseconds and so on and so forth

00:07:24,110 --> 00:07:30,019
and when the buffer reaches the sync it

00:07:26,799 --> 00:07:32,539
the pipeline has some notion of playback

00:07:30,019 --> 00:07:34,129
started at time equals T so at time

00:07:32,539 --> 00:07:36,469
equals T I should render the first

00:07:34,129 --> 00:07:38,089
buffer and then at time equals T plus

00:07:36,469 --> 00:07:40,159
100 milliseconds I should render the

00:07:38,089 --> 00:07:43,129
second buffer and so on and so forth so

00:07:40,159 --> 00:07:46,069
we synchronized at each of the things to

00:07:43,129 --> 00:07:48,229
to the pipeline clock and since all the

00:07:46,069 --> 00:07:50,409
sayings are are playing to the same

00:07:48,229 --> 00:07:52,669
pipeline clock we can make sure that

00:07:50,409 --> 00:07:56,479
playback is synchronized within the

00:07:52,669 --> 00:07:59,029
pipeline right usually if you're talking

00:07:56,479 --> 00:08:02,229
about local playback the the clock is

00:07:59,029 --> 00:08:04,759
provided by the audio device itself so

00:08:02,229 --> 00:08:07,369
the audio is being rendered at whatever

00:08:04,759 --> 00:08:08,899
rate the audio device is playing out

00:08:07,369 --> 00:08:11,029
audio and the video is also being

00:08:08,899 --> 00:08:14,329
rendered at the same rate as the audio

00:08:11,029 --> 00:08:18,110
device so that's how we guarantee

00:08:14,329 --> 00:08:20,599
synchronization there but what gstreamer

00:08:18,110 --> 00:08:23,059
allows you to do also in addition to

00:08:20,599 --> 00:08:25,999
using the audio clock in your pipeline

00:08:23,059 --> 00:08:28,639
is to provide a clock externally so you

00:08:25,999 --> 00:08:30,739
can the clock in gstreamer is just an

00:08:28,639 --> 00:08:33,740
abstraction can represent any time

00:08:30,739 --> 00:08:35,930
source so so these female lets you just

00:08:33,740 --> 00:08:37,870
say here's the time source please render

00:08:35,930 --> 00:08:40,490
your the audio and video are

00:08:37,870 --> 00:08:44,910
synchronized to this time source right

00:08:40,490 --> 00:08:47,130
and now that time source could be

00:08:44,910 --> 00:08:48,900
the audio clock as we said before it

00:08:47,130 --> 00:08:50,730
could be the system clock the system

00:08:48,900 --> 00:08:52,620
real-time clock it could also be a

00:08:50,730 --> 00:08:54,090
network clock right and that is the

00:08:52,620 --> 00:08:56,970
mechanism that we use to synchronize

00:08:54,090 --> 00:08:59,250
multiple pipelines across different

00:08:56,970 --> 00:09:03,270
machines to the same clock so we use a

00:08:59,250 --> 00:09:04,770
network clock that and we have multiple

00:09:03,270 --> 00:09:07,680
implementations of network clocks we

00:09:04,770 --> 00:09:10,500
have one based on NTP one based on PTP

00:09:07,680 --> 00:09:14,580
both of these are Network time protocols

00:09:10,500 --> 00:09:17,070
for clocks to synchronize and and you

00:09:14,580 --> 00:09:20,580
can also have a GC Minette clock which

00:09:17,070 --> 00:09:24,510
is an NTP like implementation that that

00:09:20,580 --> 00:09:26,760
we have in gstreamer just to back up I

00:09:24,510 --> 00:09:30,660
realize I didn't explain why we need to

00:09:26,760 --> 00:09:33,120
synchronize but basically each each

00:09:30,660 --> 00:09:35,640
machine that you have has a little

00:09:33,120 --> 00:09:37,740
florestal running in it and no two

00:09:35,640 --> 00:09:40,350
crystals oscillate at the same rate that

00:09:37,740 --> 00:09:42,510
means that each machine that you have in

00:09:40,350 --> 00:09:47,430
your network has a slightly different

00:09:42,510 --> 00:09:49,950
idea of how much time has elapsed in one

00:09:47,430 --> 00:09:52,460
second or one what one second means so

00:09:49,950 --> 00:09:56,750
we need all these different machines to

00:09:52,460 --> 00:10:02,160
have one clock source that defines what

00:09:56,750 --> 00:10:07,020
what the rate of time is and have them

00:10:02,160 --> 00:10:10,110
all basically run their own pipelines

00:10:07,020 --> 00:10:13,740
against that clock right so so that's

00:10:10,110 --> 00:10:16,170
basically the second part of that we

00:10:13,740 --> 00:10:17,790
need for network playback in a

00:10:16,170 --> 00:10:18,960
synchronized way so this is media this

00:10:17,790 --> 00:10:20,550
timing and this finally there's some

00:10:18,960 --> 00:10:24,420
control information that we need to

00:10:20,550 --> 00:10:26,400
distribute to all the nodes so one very

00:10:24,420 --> 00:10:28,170
obvious thing that we've already alluded

00:10:26,400 --> 00:10:30,390
to is if you're going to start playback

00:10:28,170 --> 00:10:32,640
then you need to tell all the nodes that

00:10:30,390 --> 00:10:35,580
playback has started at this time so

00:10:32,640 --> 00:10:37,740
that they can start playback against

00:10:35,580 --> 00:10:40,350
that reference time right and because

00:10:37,740 --> 00:10:42,720
you have a shared clock as long as all

00:10:40,350 --> 00:10:44,690
the nodes share that same clock they

00:10:42,720 --> 00:10:46,830
have the same idea of what that time is

00:10:44,690 --> 00:10:50,250
you might also want to share other

00:10:46,830 --> 00:10:52,380
control information such as I'm going to

00:10:50,250 --> 00:10:55,050
start playback now or stop or pause or

00:10:52,380 --> 00:10:56,850
seek or all this all these bits right so

00:10:55,050 --> 00:10:58,180
if you have these three pieces in place

00:10:56,850 --> 00:11:01,750
you can set up a

00:10:58,180 --> 00:11:05,820
system where you have multiple nodes

00:11:01,750 --> 00:11:10,120
that are there are being being media at

00:11:05,820 --> 00:11:14,740
in a synchronized way right now if you

00:11:10,120 --> 00:11:17,800
dig into into Sebastian stock from 2016

00:11:14,740 --> 00:11:19,959
this does require a fair amount of

00:11:17,800 --> 00:11:22,480
in-depth knowledge of GStreamer so you

00:11:19,959 --> 00:11:24,399
need to understand some gstreamer

00:11:22,480 --> 00:11:26,380
concepts before you're able to write

00:11:24,399 --> 00:11:30,459
such a system and we found ourselves

00:11:26,380 --> 00:11:31,959
writing this a few times and basically

00:11:30,459 --> 00:11:37,500
thought to ourselves can we make this

00:11:31,959 --> 00:11:43,120
easier for for users of gstreamer right

00:11:37,500 --> 00:11:47,440
so based on this the GSD sync server

00:11:43,120 --> 00:11:49,930
project was born right it's basically a

00:11:47,440 --> 00:11:52,990
library to allow you to write

00:11:49,930 --> 00:11:56,970
applications that need to do this sort

00:11:52,990 --> 00:12:00,010
of synchronized network playback right

00:11:56,970 --> 00:12:02,050
the things that it does it does not

00:12:00,010 --> 00:12:06,130
provide a way for you to share media

00:12:02,050 --> 00:12:08,830
across across your nodes right the

00:12:06,130 --> 00:12:11,140
reason for this is that we found that

00:12:08,830 --> 00:12:13,959
enough there are enough different use

00:12:11,140 --> 00:12:15,850
cases that people have that will be

00:12:13,959 --> 00:12:19,209
difficult to write something that covers

00:12:15,850 --> 00:12:22,959
everything and then these things are

00:12:19,209 --> 00:12:26,770
fairly standard outside of what we would

00:12:22,959 --> 00:12:28,330
be able to provide so providing an HTTP

00:12:26,770 --> 00:12:30,520
server that serves up a bunch of media

00:12:28,330 --> 00:12:32,620
is fairly simple or providing a

00:12:30,520 --> 00:12:35,080
synchronization mechanism for people for

00:12:32,620 --> 00:12:37,300
the nodes to just say are saying or

00:12:35,080 --> 00:12:39,850
whatever media to their individual nodes

00:12:37,300 --> 00:12:41,589
is something that you will need to solve

00:12:39,850 --> 00:12:44,410
depending on your specific application

00:12:41,589 --> 00:12:48,630
so that's not covered by this library

00:12:44,410 --> 00:12:52,300
what we do take care of is the clock the

00:12:48,630 --> 00:12:54,730
clock and timing related things for all

00:12:52,300 --> 00:12:56,770
your nodes and we've provided a way to

00:12:54,730 --> 00:13:02,320
provide or to send controls across

00:12:56,770 --> 00:13:04,329
across the various nodes right as the

00:13:02,320 --> 00:13:06,220
name suggests it's a client-server based

00:13:04,329 --> 00:13:09,360
architecture so what you do is you run a

00:13:06,220 --> 00:13:11,699
little server that deals with

00:13:09,360 --> 00:13:15,059
telling all the nodes what to play and

00:13:11,699 --> 00:13:16,679
providing pause play kind of controls

00:13:15,059 --> 00:13:19,649
and you have clients which are the

00:13:16,679 --> 00:13:24,869
actual nodes that do any video display

00:13:19,649 --> 00:13:29,720
or plain mac of audio ok so the server

00:13:24,869 --> 00:13:32,730
but takes care of a few basic things you

00:13:29,720 --> 00:13:35,069
provide a playlist which is just a set

00:13:32,730 --> 00:13:37,259
of you arise that you want to play as

00:13:35,069 --> 00:13:40,079
long as each of the nodes knows how to

00:13:37,259 --> 00:13:42,239
open that URI you're good to go right

00:13:40,079 --> 00:13:45,089
there's a control mechanism so you can

00:13:42,239 --> 00:13:47,489
as I said before are do things like play

00:13:45,089 --> 00:13:49,799
or pause you can provide transformations

00:13:47,489 --> 00:13:53,759
and the reason you want to be able to do

00:13:49,799 --> 00:13:55,559
do that is for example in the in the

00:13:53,759 --> 00:13:59,129
example we had with video walls right

00:13:55,559 --> 00:14:00,720
you might have say a 2x2 grid of

00:13:59,129 --> 00:14:03,449
displays that you want to treat as one

00:14:00,720 --> 00:14:06,420
large screen you are streaming the same

00:14:03,449 --> 00:14:09,029
video file to each of these clients but

00:14:06,420 --> 00:14:10,619
each client is going to apply a

00:14:09,029 --> 00:14:11,309
transformation to the video before

00:14:10,619 --> 00:14:14,850
entering it

00:14:11,309 --> 00:14:17,819
so in this 4x4 grid the top-left display

00:14:14,850 --> 00:14:23,489
will need to divide the basically crop

00:14:17,819 --> 00:14:27,569
out the video divided by 4 play only the

00:14:23,489 --> 00:14:28,980
top left then the top right this level

00:14:27,569 --> 00:14:30,629
show the top right of the video and so

00:14:28,980 --> 00:14:38,189
forth you might also want to do a bit of

00:14:30,629 --> 00:14:40,309
cropping to a come to to basically deal

00:14:38,189 --> 00:14:42,989
with the bezels on the display so that

00:14:40,309 --> 00:14:44,610
it looks like it's one big display and

00:14:42,989 --> 00:14:46,649
not just divided by four so you want to

00:14:44,610 --> 00:14:50,160
account for the bezels so that it looks

00:14:46,649 --> 00:14:53,730
continuous right and we have some

00:14:50,160 --> 00:14:57,239
mechanism to notify the server when a

00:14:53,730 --> 00:14:59,220
client joins or leaves right so API vice

00:14:57,239 --> 00:15:02,339
the idea is that it's supposed to be

00:14:59,220 --> 00:15:05,699
very simple so we you create a GSD sync

00:15:02,339 --> 00:15:09,329
server object this is all based on G

00:15:05,699 --> 00:15:12,899
object which is object system for C that

00:15:09,329 --> 00:15:17,100
we use in gstreamer and other projects

00:15:12,899 --> 00:15:20,039
based around norm right so it's fairly

00:15:17,100 --> 00:15:22,830
straightforward you create a new sync

00:15:20,039 --> 00:15:25,830
server object you provide it a play

00:15:22,830 --> 00:15:28,020
list you started and that's pretty much

00:15:25,830 --> 00:15:30,420
it and then there are some controls like

00:15:28,020 --> 00:15:33,420
setting it to paused and unpause it

00:15:30,420 --> 00:15:36,450
right on the client side it's even

00:15:33,420 --> 00:15:37,950
simpler you basically need to just more

00:15:36,450 --> 00:15:41,910
or less point to a server address and

00:15:37,950 --> 00:15:46,860
that's that right we do provide a way to

00:15:41,910 --> 00:15:48,990
sort of customize how you render audio

00:15:46,860 --> 00:15:52,230
or video because especially on embedded

00:15:48,990 --> 00:15:54,990
platforms you might want to say use say

00:15:52,230 --> 00:15:57,390
an accelerated video sync so that your

00:15:54,990 --> 00:15:59,970
display is smooth or you might want to

00:15:57,390 --> 00:16:02,580
make some special controls to create a

00:15:59,970 --> 00:16:05,550
fullscreen window on this on the system

00:16:02,580 --> 00:16:08,550
that you are on so the soap laman is

00:16:05,550 --> 00:16:10,080
basically a high level element that

00:16:08,550 --> 00:16:12,510
GStreamer provides that you can give a

00:16:10,080 --> 00:16:14,700
custom audio sync or video sing to so

00:16:12,510 --> 00:16:17,400
you just if you want to control this you

00:16:14,700 --> 00:16:19,070
can create a Plamen element and provide

00:16:17,400 --> 00:16:22,860
it to the client and we'll use that

00:16:19,070 --> 00:16:26,970
plane to render render the audio and the

00:16:22,860 --> 00:16:28,860
video right and the API is quite simple

00:16:26,970 --> 00:16:30,900
you create a new client

00:16:28,860 --> 00:16:33,300
when you're creating it you point it to

00:16:30,900 --> 00:16:35,880
the server whatever address you provide

00:16:33,300 --> 00:16:37,140
will be used and then you just start it

00:16:35,880 --> 00:16:38,940
and it will automatically get

00:16:37,140 --> 00:16:42,680
information from the server and start

00:16:38,940 --> 00:16:45,720
rendering it right that's pretty much it

00:16:42,680 --> 00:16:48,180
this somewhat ugly diagram kind of shows

00:16:45,720 --> 00:16:50,760
what what's going on there so you have a

00:16:48,180 --> 00:16:52,440
server the server is just it's a program

00:16:50,760 --> 00:16:54,270
so it can be running on any of the nodes

00:16:52,440 --> 00:16:56,730
that's also running a client right and

00:16:54,270 --> 00:16:59,460
the client is running so you're

00:16:56,730 --> 00:17:01,320
instantiating a GSD sync server object

00:16:59,460 --> 00:17:04,020
on the server or the SE sync client

00:17:01,320 --> 00:17:06,270
object on each client and there's this

00:17:04,020 --> 00:17:08,550
control plane that's taking care of all

00:17:06,270 --> 00:17:12,720
the messages and everything onto the

00:17:08,550 --> 00:17:18,000
hood right any questions so far anything

00:17:12,720 --> 00:17:19,980
that didn't make sense okay good so

00:17:18,000 --> 00:17:22,050
let's keep going and then maybe at the

00:17:19,980 --> 00:17:24,180
end of the request if you can get to

00:17:22,050 --> 00:17:27,270
that so this is pretty much all you need

00:17:24,180 --> 00:17:29,520
if you just want to create an

00:17:27,270 --> 00:17:31,980
application that does synchronize

00:17:29,520 --> 00:17:35,280
playback you basically just need to use

00:17:31,980 --> 00:17:35,809
the sync server and same client objects

00:17:35,280 --> 00:17:41,720
and that

00:17:35,809 --> 00:17:44,179
very much it right but we found that not

00:17:41,720 --> 00:17:46,820
everybody like people want more control

00:17:44,179 --> 00:17:49,179
sometimes over the actual way in which

00:17:46,820 --> 00:17:52,509
we send control messages out right the

00:17:49,179 --> 00:17:55,190
default that you saw where we have this

00:17:52,509 --> 00:17:58,549
this control part and the network in

00:17:55,190 --> 00:18:00,769
between we have a TCP server running on

00:17:58,549 --> 00:18:03,350
the server and clients connect to that

00:18:00,769 --> 00:18:06,769
server and exchange information using a

00:18:03,350 --> 00:18:08,570
custom protocol but there are

00:18:06,769 --> 00:18:11,299
application that might want to implement

00:18:08,570 --> 00:18:14,269
their own transport for these control

00:18:11,299 --> 00:18:18,860
messages so for that we basically allow

00:18:14,269 --> 00:18:20,629
you to override the control interface by

00:18:18,860 --> 00:18:22,669
providing a control server and a control

00:18:20,629 --> 00:18:25,190
client an implementation of a controller

00:18:22,669 --> 00:18:26,629
and a control client interface right so

00:18:25,190 --> 00:18:30,470
if you're implementing you control

00:18:26,629 --> 00:18:33,440
server of your own you basically prove

00:18:30,470 --> 00:18:35,779
you need to provide some way to say hey

00:18:33,440 --> 00:18:39,440
listen on this address as you make sctv

00:18:35,779 --> 00:18:42,710
based or UDP based you need to provide

00:18:39,440 --> 00:18:45,110
some way to synchronize sync info object

00:18:42,710 --> 00:18:47,509
so it's a well-defined object you need a

00:18:45,110 --> 00:18:49,940
way to serialize and send this out over

00:18:47,509 --> 00:18:51,499
the network right it's some control for

00:18:49,940 --> 00:18:55,100
starting and stopping and you need to

00:18:51,499 --> 00:18:57,049
provide a mechanism to notify the the

00:18:55,100 --> 00:19:00,590
user of this interface that clients and

00:18:57,049 --> 00:19:02,119
joined or left if you're in the

00:19:00,590 --> 00:19:05,149
implement in the client side again you

00:19:02,119 --> 00:19:07,309
need to provide a way to say hey this is

00:19:05,149 --> 00:19:09,320
the address of the control server that

00:19:07,309 --> 00:19:13,159
you provided here this is a way to

00:19:09,320 --> 00:19:15,950
deserialize that sync information that

00:19:13,159 --> 00:19:19,669
needs to be provided to the GST sync

00:19:15,950 --> 00:19:22,039
client and way to start and stop and the

00:19:19,669 --> 00:19:23,659
way to identify the client right and the

00:19:22,039 --> 00:19:25,190
way we want the reason we want to have

00:19:23,659 --> 00:19:27,710
this kind of identification and

00:19:25,190 --> 00:19:29,690
configuration for clients is so that you

00:19:27,710 --> 00:19:31,369
can have the server can dynamically

00:19:29,690 --> 00:19:33,139
adjust what it's doing based on the

00:19:31,369 --> 00:19:35,779
clients that join right so if you have a

00:19:33,139 --> 00:19:38,539
video wall maybe you can configure what

00:19:35,779 --> 00:19:40,549
the transformations are based on whether

00:19:38,539 --> 00:19:42,320
there are four clients or nine clients

00:19:40,549 --> 00:19:43,879
or 16 clients and that would allow you

00:19:42,320 --> 00:19:47,029
to determine the size of the screen or

00:19:43,879 --> 00:19:48,980
in a in a multi-room audio setup you may

00:19:47,029 --> 00:19:50,690
if you have just one speaker you

00:19:48,980 --> 00:19:52,130
everything to it but if you have two

00:19:50,690 --> 00:19:54,049
speakers and you know one of them is the

00:19:52,130 --> 00:19:57,230
left and right channel you might want to

00:19:54,049 --> 00:19:59,390
treat it as a stereo pair and play left

00:19:57,230 --> 00:20:01,820
to one and right to another right so

00:19:59,390 --> 00:20:05,690
this mechanism is meant to allow you to

00:20:01,820 --> 00:20:11,270
do that right and that's pretty much it

00:20:05,690 --> 00:20:13,309
I realized I almost don't know so if you

00:20:11,270 --> 00:20:15,230
want to make a simple application

00:20:13,309 --> 00:20:18,679
there's a GST sync server a sync client

00:20:15,230 --> 00:20:22,220
object if you want to have control over

00:20:18,679 --> 00:20:25,940
how messages are sent back and forth you

00:20:22,220 --> 00:20:27,830
have that we need to still merge this

00:20:25,940 --> 00:20:30,290
into the gstreamer repository this is

00:20:27,830 --> 00:20:32,380
been pending or me finding the time as

00:20:30,290 --> 00:20:35,540
well as getting some feedback on the API

00:20:32,380 --> 00:20:37,340
we need to provide more controls and

00:20:35,540 --> 00:20:39,020
more examples and there are some things

00:20:37,340 --> 00:20:41,299
within GStreamer that we can do to get

00:20:39,020 --> 00:20:46,880
tighter sync it's very decent right now

00:20:41,299 --> 00:20:49,010
but we can probably do better and yeah

00:20:46,880 --> 00:20:51,020
that's it there's a little demo with

00:20:49,010 --> 00:20:54,080
that very shaky video the video on the

00:20:51,020 --> 00:20:56,120
left is running off my laptop the one on

00:20:54,080 --> 00:21:03,020
the right is running I believe of an

00:20:56,120 --> 00:21:07,309
Odroid board and you'll see that when

00:21:03,020 --> 00:21:09,950
they start they they'll be the video on

00:21:07,309 --> 00:21:11,660
the left will start before the video on

00:21:09,950 --> 00:21:14,540
the right but they'll be in sync

00:21:11,660 --> 00:21:17,870
soon after it's playing tears of Steel

00:21:14,540 --> 00:21:21,260
the open blender foundation video which

00:21:17,870 --> 00:21:24,790
will start soon yeah that's it

00:21:21,260 --> 00:21:24,790
thank you for listening

00:21:25,620 --> 00:21:31,750

YouTube URL: https://www.youtube.com/watch?v=1CVjxmGgj2k


