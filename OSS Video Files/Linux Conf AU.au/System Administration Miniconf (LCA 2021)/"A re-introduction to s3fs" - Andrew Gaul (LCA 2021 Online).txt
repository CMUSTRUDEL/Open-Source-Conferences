Title: "A re-introduction to s3fs" - Andrew Gaul (LCA 2021 Online)
Publication date: 2021-02-18
Playlist: System Administration Miniconf (LCA 2021)
Description: 
	Andrew Gaul

https://lca2021.linux.org.au/schedule/presentation/83/

S3 file systems are a popular interface to object storage despite their leaky abstractions and performance pitfalls.  In this talk we will explore s3fs, one of the most popular FUSE file systems, and when it is an appropriate solution.  We will compare it with NFS and also discuss how s3fs has evolved over the last ten years.

linux.conf.au is a conference about the Linux operating system, and all aspects of the thriving ecosystem of Free and Open Source Software that has grown up around it. Run since 1999, in a different Australian or New Zealand city each year, by a team of local volunteers, LCA invites more than 500 people to learn from the people who shape the future of Open Source. For more information on the conference see https://linux.conf.au/

Produced by Next Day Video Australia: https://nextdayvideo.com.au

#linux.conf.au #linux #foss #opensource

Sat Jan 23 16:15:00 2021 at Blemings Labs
Captions: 
	00:00:10,820 --> 00:00:14,060
[Music]

00:00:15,280 --> 00:00:20,240
the system administration mini conf

00:00:17,600 --> 00:00:21,039
and we have three talks left and the

00:00:20,240 --> 00:00:24,000
mini conf

00:00:21,039 --> 00:00:26,359
and up now is andrew gall giving us a

00:00:24,000 --> 00:00:30,400
reintroduction to

00:00:26,359 --> 00:00:32,320
s3fs over to you andrew hi

00:00:30,400 --> 00:00:34,239
uh yeah i am going to be reintroducing

00:00:32,320 --> 00:00:37,360
svfs i am one of the maintainers

00:00:34,239 --> 00:00:40,399
of this file system um so

00:00:37,360 --> 00:00:42,000
just a brief overview uh s3fs is a fuse

00:00:40,399 --> 00:00:43,280
file system that allows posix

00:00:42,000 --> 00:00:46,239
applications which means

00:00:43,280 --> 00:00:50,000
linux or mac os or ubsd whatever to

00:00:46,239 --> 00:00:52,719
mount s3 object storage as if it's a

00:00:50,000 --> 00:00:54,239
local file system so fuse runs in user

00:00:52,719 --> 00:00:55,280
space not in kernel space which is why

00:00:54,239 --> 00:00:56,640
it's portable across

00:00:55,280 --> 00:00:58,960
all this implement all the different

00:00:56,640 --> 00:00:59,840
operating systems uh it was created in

00:00:58,960 --> 00:01:01,600
00:00:59,840 --> 00:01:04,000
uh around the time estrella was

00:01:01,600 --> 00:01:06,320
introduced uh had a couple of wilderness

00:01:04,000 --> 00:01:07,760
years after that uh but has had pretty

00:01:06,320 --> 00:01:10,320
steady maintainership for the last seven

00:01:07,760 --> 00:01:12,880
or eight years now um

00:01:10,320 --> 00:01:15,040
it's compatible with um amazon s3 is the

00:01:12,880 --> 00:01:17,759
most famous uh s3 but there are many

00:01:15,040 --> 00:01:19,040
implementations um tens or 100 if

00:01:17,759 --> 00:01:20,640
depending on account

00:01:19,040 --> 00:01:22,080
um so it's really compatible with

00:01:20,640 --> 00:01:25,119
everything including

00:01:22,080 --> 00:01:26,960
s3 like partial s3 implementations uh it

00:01:25,119 --> 00:01:29,040
has many uh packages available

00:01:26,960 --> 00:01:30,400
most linux distributions uh you can just

00:01:29,040 --> 00:01:32,240
install it um

00:01:30,400 --> 00:01:33,920
apt-get install or whatever uh it does

00:01:32,240 --> 00:01:36,479
exist on mac os as well

00:01:33,920 --> 00:01:37,680
um and it works well for several use

00:01:36,479 --> 00:01:39,360
cases

00:01:37,680 --> 00:01:40,720
but not not all of them which is what

00:01:39,360 --> 00:01:43,200
we're going to explore in this talk

00:01:40,720 --> 00:01:44,640
sgfs is a leaky abstraction and it has

00:01:43,200 --> 00:01:47,200
some data consistency and performance

00:01:44,640 --> 00:01:47,200
pitfalls

00:01:47,920 --> 00:01:54,799
sreefs usage is really simple you

00:01:51,280 --> 00:01:57,920
you configure your your s3 credentials

00:01:54,799 --> 00:01:59,439
and then you just run s3fs bucket

00:01:57,920 --> 00:02:01,040
and a mount point and then you can

00:01:59,439 --> 00:02:01,680
interact with it as if it's a local file

00:02:01,040 --> 00:02:03,280
system

00:02:01,680 --> 00:02:04,799
so in this example let's say we write

00:02:03,280 --> 00:02:05,200
the file foo and then we can read the

00:02:04,799 --> 00:02:06,719
file

00:02:05,200 --> 00:02:08,640
we can stack the file and see it has the

00:02:06,719 --> 00:02:10,879
correct metadata um

00:02:08,640 --> 00:02:12,080
and the the good part about s3fs is that

00:02:10,879 --> 00:02:15,280
you can interact with

00:02:12,080 --> 00:02:17,680
files um through other s3 applications

00:02:15,280 --> 00:02:19,280
and so here we're using the aws cli and

00:02:17,680 --> 00:02:21,120
we can see that foo exists

00:02:19,280 --> 00:02:23,760
it's four bytes as we expect and we can

00:02:21,120 --> 00:02:27,040
round trip the data

00:02:23,760 --> 00:02:29,680
so s3fs works well in some situations

00:02:27,040 --> 00:02:31,200
um so first of all when when posix

00:02:29,680 --> 00:02:33,120
compatibility is required

00:02:31,200 --> 00:02:35,200
um if you you don't need fast

00:02:33,120 --> 00:02:38,000
compatibility please don't use scrfs

00:02:35,200 --> 00:02:38,879
use a native s3 application it does

00:02:38,000 --> 00:02:41,040
support most

00:02:38,879 --> 00:02:42,160
of the operations that you would expect

00:02:41,040 --> 00:02:44,319
but it can't support

00:02:42,160 --> 00:02:45,440
hard links or atomic renames of files or

00:02:44,319 --> 00:02:48,480
directories due to

00:02:45,440 --> 00:02:51,040
the underlying s3 limitations

00:02:48,480 --> 00:02:52,560
so esri is really good in kind of mixed

00:02:51,040 --> 00:02:54,640
environments when you need some posix

00:02:52,560 --> 00:02:57,040
and you have some other s3

00:02:54,640 --> 00:02:59,599
either data producers or consumers uh

00:02:57,040 --> 00:03:01,280
uses the the normal s3 object format

00:02:59,599 --> 00:03:03,760
uh it stores kind of the extra posix

00:03:01,280 --> 00:03:05,200
bits and s3 user metadata like uids and

00:03:03,760 --> 00:03:08,400
permissions

00:03:05,200 --> 00:03:11,040
um s3 workspace srfs works best for um

00:03:08,400 --> 00:03:11,920
for sequential read and write operations

00:03:11,040 --> 00:03:14,480
usually kind of

00:03:11,920 --> 00:03:16,239
bulky data on larger files and i would

00:03:14,480 --> 00:03:19,040
define larger files as something

00:03:16,239 --> 00:03:19,840
that would range from five megabytes to

00:03:19,040 --> 00:03:22,560
um

00:03:19,840 --> 00:03:23,200
five gigabytes or or more um srfs does

00:03:22,560 --> 00:03:24,879
work

00:03:23,200 --> 00:03:26,640
uh for files up to five terabytes

00:03:24,879 --> 00:03:28,000
although it has some uh limitations when

00:03:26,640 --> 00:03:31,519
writing those that we'll talk about

00:03:28,000 --> 00:03:32,799
on a subsequent slide um similarly like

00:03:31,519 --> 00:03:35,200
s reference doesn't work well

00:03:32,799 --> 00:03:35,920
for directories that have um lots and

00:03:35,200 --> 00:03:38,080
lots of

00:03:35,920 --> 00:03:39,760
uh um single directions with lots of

00:03:38,080 --> 00:03:41,840
files so i would say like

00:03:39,760 --> 00:03:43,040
tens of thousands of files is manageable

00:03:41,840 --> 00:03:43,519
but when you get into the millions or

00:03:43,040 --> 00:03:46,319
sometimes

00:03:43,519 --> 00:03:47,680
billions surface just can't um it

00:03:46,319 --> 00:03:50,720
basically can't issue a reader

00:03:47,680 --> 00:03:52,480
uh so that's um it's not going to work

00:03:50,720 --> 00:03:54,000
for you for performance reasons

00:03:52,480 --> 00:03:55,840
and and generally we would think about

00:03:54,000 --> 00:03:57,599
sdfs working well when you have high

00:03:55,840 --> 00:03:58,239
bandwidth and low latency to the s3

00:03:57,599 --> 00:04:00,720
server

00:03:58,239 --> 00:04:02,000
so um for example my bucket is in the

00:04:00,720 --> 00:04:04,480
united states and i'm in tokyo and

00:04:02,000 --> 00:04:05,599
that's a little little slow for me

00:04:04,480 --> 00:04:08,560
just from the round trips from all the

00:04:05,599 --> 00:04:08,560
http requests

00:04:08,720 --> 00:04:12,400
so um the downside of s35s is that

00:04:10,879 --> 00:04:13,760
positive compatibility introduces

00:04:12,400 --> 00:04:14,799
overhead that you wouldn't have with the

00:04:13,760 --> 00:04:17,840
native application

00:04:14,799 --> 00:04:19,919
and you can kind of think about

00:04:17,840 --> 00:04:20,880
the analogy of hammering a square peg

00:04:19,919 --> 00:04:22,720
into a round hole

00:04:20,880 --> 00:04:24,400
like it'll work with sufficient force

00:04:22,720 --> 00:04:27,199
and time but it's not

00:04:24,400 --> 00:04:28,080
natural so one of the reasons for that

00:04:27,199 --> 00:04:31,040
is that reader

00:04:28,080 --> 00:04:32,000
is a pretty expensive operation so in

00:04:31,040 --> 00:04:34,800
addition to

00:04:32,000 --> 00:04:35,520
reader issuing a list objects request an

00:04:34,800 --> 00:04:37,840
s3

00:04:35,520 --> 00:04:39,440
it has to issue a head request for each

00:04:37,840 --> 00:04:41,360
object to translate that and get the

00:04:39,440 --> 00:04:44,000
stat file to return that to return

00:04:41,360 --> 00:04:46,080
so this is ruinously expensive for that

00:04:44,000 --> 00:04:47,919
example of millions of files

00:04:46,080 --> 00:04:49,680
uh another pitfall is that there's a

00:04:47,919 --> 00:04:51,520
random writes will amplify

00:04:49,680 --> 00:04:54,160
which means that if you write one byte

00:04:51,520 --> 00:04:54,880
um sriffs actually has to write five

00:04:54,160 --> 00:04:57,360
megabytes

00:04:54,880 --> 00:04:57,919
to satisfy the um the api requirements

00:04:57,360 --> 00:05:01,039
of the

00:04:57,919 --> 00:05:04,080
s3 multi-part upload

00:05:01,039 --> 00:05:05,520
request so if you're doing lots of small

00:05:04,080 --> 00:05:07,440
modifications like a database you will

00:05:05,520 --> 00:05:09,360
not be happy

00:05:07,440 --> 00:05:10,479
and another kind of interesting area is

00:05:09,360 --> 00:05:12,479
that uploading

00:05:10,479 --> 00:05:13,680
updating metadata like if you're doing a

00:05:12,479 --> 00:05:16,880
change mod x

00:05:13,680 --> 00:05:18,800
um plus x on an object uh

00:05:16,880 --> 00:05:20,720
it actually copies all the data on the

00:05:18,800 --> 00:05:22,560
s3 server i mean srfs doesn't see that

00:05:20,720 --> 00:05:22,960
but the s3 server is working very hard

00:05:22,560 --> 00:05:26,160
to

00:05:22,960 --> 00:05:28,080
to move all that stuff around um

00:05:26,160 --> 00:05:29,600
so we we do recommend use native

00:05:28,080 --> 00:05:32,880
applications when available

00:05:29,600 --> 00:05:33,840
uh aws cly is much faster than ls or

00:05:32,880 --> 00:05:36,240
fine for example

00:05:33,840 --> 00:05:38,320
uh rclone has native s3 support which is

00:05:36,240 --> 00:05:40,320
better than doing an rsync onto an s3fs

00:05:38,320 --> 00:05:42,560
mount point uh a lot of applications do

00:05:40,320 --> 00:05:45,360
have negative s3 support like um

00:05:42,560 --> 00:05:46,560
procona xp stream uh it's better to do

00:05:45,360 --> 00:05:48,720
that uh

00:05:46,560 --> 00:05:50,080
do that natively instead of doing xp

00:05:48,720 --> 00:05:53,280
stream to a file locally and then

00:05:50,080 --> 00:05:55,600
copying it

00:05:53,280 --> 00:05:56,319
so s3fs does have some other downsides

00:05:55,600 --> 00:05:57,600
uh it has

00:05:56,319 --> 00:05:59,440
really primitive multi-client

00:05:57,600 --> 00:06:01,199
coordination so if you're familiar with

00:05:59,440 --> 00:06:03,759
nfs version 2

00:06:01,199 --> 00:06:06,000
it lacked a lot of the strong semantics

00:06:03,759 --> 00:06:07,919
that we enjoy in more modern versions

00:06:06,000 --> 00:06:10,000
and so it's spfs is kind of similar to

00:06:07,919 --> 00:06:11,759
that so it has what's called close to

00:06:10,000 --> 00:06:14,639
open data consistency

00:06:11,759 --> 00:06:15,039
uh and this means that when a when a s3s

00:06:14,639 --> 00:06:18,639
client

00:06:15,039 --> 00:06:21,280
is writing to a file and sjf won't

00:06:18,639 --> 00:06:22,720
won't write that to the s3 server and

00:06:21,280 --> 00:06:24,720
it's not exposed to the other clients

00:06:22,720 --> 00:06:26,160
until the file the file descriptor is

00:06:24,720 --> 00:06:29,280
closed or

00:06:26,160 --> 00:06:31,759
you do an fsync and so

00:06:29,280 --> 00:06:33,039
this is this is bad because multiple

00:06:31,759 --> 00:06:35,440
clients may cache

00:06:33,039 --> 00:06:36,319
stale data and you have inconsistencies

00:06:35,440 --> 00:06:38,080
there

00:06:36,319 --> 00:06:39,759
um and this this gets a little bit worse

00:06:38,080 --> 00:06:42,560
for even metadata um

00:06:39,759 --> 00:06:43,520
because reader is so expensive s3fs will

00:06:42,560 --> 00:06:46,560
cache it for a pretty

00:06:43,520 --> 00:06:49,199
generous uh 15 minutes to avoid um

00:06:46,560 --> 00:06:50,639
capturing that again and so as you might

00:06:49,199 --> 00:06:53,919
expect i i notify doesn't work

00:06:50,639 --> 00:06:53,919
for that s3 best as well

00:06:54,240 --> 00:06:58,240
uh there's a wrinkle to s3

00:06:56,800 --> 00:07:00,479
implementations themselves

00:06:58,240 --> 00:07:02,400
um so traditional file systems give you

00:07:00,479 --> 00:07:04,400
what's called strong consistency

00:07:02,400 --> 00:07:05,440
and that means if you were to take a

00:07:04,400 --> 00:07:07,440
file

00:07:05,440 --> 00:07:08,880
and write one version of the data to it

00:07:07,440 --> 00:07:09,919
and then overwrite that file the second

00:07:08,880 --> 00:07:12,479
version of that data

00:07:09,919 --> 00:07:13,759
that when you read that file you will

00:07:12,479 --> 00:07:14,560
always get the second version of the

00:07:13,759 --> 00:07:16,160
data

00:07:14,560 --> 00:07:18,319
and this is that what traditional file

00:07:16,160 --> 00:07:19,599
systems guarantee uh amazon recently

00:07:18,319 --> 00:07:21,280
started guaranteeing that

00:07:19,599 --> 00:07:22,800
last month which is really great for

00:07:21,280 --> 00:07:24,880
users uh but

00:07:22,800 --> 00:07:26,720
traditionally s3 implementations uh only

00:07:24,880 --> 00:07:28,479
guarantee eventual consistency

00:07:26,720 --> 00:07:30,080
and so this means if you echo food or

00:07:28,479 --> 00:07:33,039
file in a echo bar to a file

00:07:30,080 --> 00:07:33,440
when you catch file you may get foo or

00:07:33,039 --> 00:07:35,120
bar

00:07:33,440 --> 00:07:36,880
for some implementation defined period

00:07:35,120 --> 00:07:38,720
of time this is because

00:07:36,880 --> 00:07:40,880
most s3 implementations are distributed

00:07:38,720 --> 00:07:42,000
systems and to have the availability

00:07:40,880 --> 00:07:43,599
that they're going for they give up

00:07:42,000 --> 00:07:45,680
consistency

00:07:43,599 --> 00:07:47,120
um i did measure this in the real world

00:07:45,680 --> 00:07:49,199
a long time ago um i

00:07:47,120 --> 00:07:51,360
i was able to see that this happened one

00:07:49,199 --> 00:07:52,400
out of 1000 times in the worst case

00:07:51,360 --> 00:07:54,960
so this is one of these things that

00:07:52,400 --> 00:07:55,520
happens sometimes but not often enough

00:07:54,960 --> 00:07:57,360
for you to

00:07:55,520 --> 00:07:59,039
to notice it regularly and they'll just

00:07:57,360 --> 00:08:00,879
seem like strange semantics

00:07:59,039 --> 00:08:02,240
and furthermore caching can hide this

00:08:00,879 --> 00:08:04,160
behavior which um

00:08:02,240 --> 00:08:05,840
maybe is good for for you as a user but

00:08:04,160 --> 00:08:07,360
it makes it even harder to

00:08:05,840 --> 00:08:08,960
validate whether your application works

00:08:07,360 --> 00:08:10,639
when there's eventual consistency

00:08:08,960 --> 00:08:11,919
so uh you you want to make sure that

00:08:10,639 --> 00:08:13,280
your forklift can tolerate these

00:08:11,919 --> 00:08:16,160
semantics since they're not the normal

00:08:13,280 --> 00:08:16,160
posix semantics

00:08:16,879 --> 00:08:20,800
sjfs performance can be good out of box

00:08:19,599 --> 00:08:23,680
it's actually tuned

00:08:20,800 --> 00:08:24,560
for lower memory systems and desktops

00:08:23,680 --> 00:08:27,280
laptops

00:08:24,560 --> 00:08:29,039
raspberry pies and low memory containers

00:08:27,280 --> 00:08:30,800
but you can easily double performance by

00:08:29,039 --> 00:08:33,599
changing some of those defaults

00:08:30,800 --> 00:08:34,719
um one of them is for reader one of the

00:08:33,599 --> 00:08:36,880
slowest operations

00:08:34,719 --> 00:08:38,159
is multi-rec max it's a number of

00:08:36,880 --> 00:08:40,000
parallel head requests

00:08:38,159 --> 00:08:41,839
that reader can issue at the same time i

00:08:40,000 --> 00:08:44,000
believe that defaults to 20

00:08:41,839 --> 00:08:44,959
and you should be able to get uh linear

00:08:44,000 --> 00:08:48,480
performance very

00:08:44,959 --> 00:08:51,200
recently that um

00:08:48,480 --> 00:08:52,800
dash o use cache is important so most

00:08:51,200 --> 00:08:54,320
most of your base users have kind of a

00:08:52,800 --> 00:08:55,760
streaming performance they're going for

00:08:54,320 --> 00:08:57,600
or they're doing one pass over

00:08:55,760 --> 00:09:00,399
over data either reader reading or

00:08:57,600 --> 00:09:02,800
writing for those use cases that

00:09:00,399 --> 00:09:05,040
are using the same data over and over

00:09:02,800 --> 00:09:06,320
again you you want to turn on caching

00:09:05,040 --> 00:09:08,080
and the reason this isn't set on by

00:09:06,320 --> 00:09:08,800
default is it can use a lot of space in

00:09:08,080 --> 00:09:12,320
slash temp

00:09:08,800 --> 00:09:15,040
which is can be precious to some users

00:09:12,320 --> 00:09:16,240
the next three flags control how srfs is

00:09:15,040 --> 00:09:19,680
able to write data

00:09:16,240 --> 00:09:21,600
to um s3 so parallel count is the number

00:09:19,680 --> 00:09:22,240
of parallel put requests by default this

00:09:21,600 --> 00:09:24,320
is five

00:09:22,240 --> 00:09:25,920
uh some other clients use ten or more uh

00:09:24,320 --> 00:09:27,040
this is one of the easier ways to double

00:09:25,920 --> 00:09:28,720
performance

00:09:27,040 --> 00:09:30,160
um it's kind of a little bit in conflict

00:09:28,720 --> 00:09:32,560
with multi-part

00:09:30,160 --> 00:09:34,399
size so you you kind of you when you're

00:09:32,560 --> 00:09:37,440
writing lots of data you want to have

00:09:34,399 --> 00:09:39,200
as many parallel put requests going at

00:09:37,440 --> 00:09:42,000
the same time to use the entire uh

00:09:39,200 --> 00:09:43,680
pipe um you don't want to set the

00:09:42,000 --> 00:09:47,360
multi-request multi-part request too

00:09:43,680 --> 00:09:48,959
too big or too small um to um

00:09:47,360 --> 00:09:50,800
even the connections be long-lived and

00:09:48,959 --> 00:09:53,920
you you kind of want

00:09:50,800 --> 00:09:56,880
usually uh one connection um

00:09:53,920 --> 00:09:58,080
you you want to kind of pick uh the the

00:09:56,880 --> 00:10:00,959
lowest number that gives you

00:09:58,080 --> 00:10:02,320
all the parallel counts uh used or

00:10:00,959 --> 00:10:03,760
highest number of all the powerpoints

00:10:02,320 --> 00:10:05,360
used

00:10:03,760 --> 00:10:06,800
max 30 data is kind of an interesting

00:10:05,360 --> 00:10:09,839
recent addition um

00:10:06,800 --> 00:10:12,000
this is um is the amount of

00:10:09,839 --> 00:10:13,920
temporary data that sjfs will hold on to

00:10:12,000 --> 00:10:17,760
before it will flush to s3

00:10:13,920 --> 00:10:20,480
so s3 is a immutable object store and so

00:10:17,760 --> 00:10:21,760
the way s3fs is able to have mutable

00:10:20,480 --> 00:10:23,360
files on top of that

00:10:21,760 --> 00:10:24,880
is it buffers things locally and it'll

00:10:23,360 --> 00:10:28,560
periodically flush that

00:10:24,880 --> 00:10:30,320
and we'll do a multi-part upload

00:10:28,560 --> 00:10:32,800
which can be some combination of new

00:10:30,320 --> 00:10:34,959
data and copy data from the server

00:10:32,800 --> 00:10:36,399
so if you're creating very very big

00:10:34,959 --> 00:10:37,120
files like the five terabyte example

00:10:36,399 --> 00:10:38,720
from earlier

00:10:37,120 --> 00:10:40,800
you want to set this as high as possible

00:10:38,720 --> 00:10:44,320
to avoid the number of rewrites

00:10:40,800 --> 00:10:44,320
that will be done by the server

00:10:44,800 --> 00:10:47,600
so there's been a lot of changes in

00:10:46,079 --> 00:10:49,839
recent years uh if you go to stack

00:10:47,600 --> 00:10:51,839
overflow from 2013 or 2014 you'll see a

00:10:49,839 --> 00:10:52,560
lot of users upset about especially bugs

00:10:51,839 --> 00:10:54,399
in the system

00:10:52,560 --> 00:10:55,760
and and so this has been my my focus is

00:10:54,399 --> 00:10:57,040
fixing a lot of the

00:10:55,760 --> 00:10:59,440
concurrency data corruption and

00:10:57,040 --> 00:11:00,480
possibility uh issues that were in there

00:10:59,440 --> 00:11:02,959
um so if you've

00:11:00,480 --> 00:11:04,480
experienced like poor um poor

00:11:02,959 --> 00:11:05,600
correctness before please please try

00:11:04,480 --> 00:11:06,880
again uh we're

00:11:05,600 --> 00:11:08,959
we're much better place than we were six

00:11:06,880 --> 00:11:10,560
years ago uh we've backed that up with

00:11:08,959 --> 00:11:12,399
much more robust testing to prevent

00:11:10,560 --> 00:11:14,560
progressions uh we're really in a much

00:11:12,399 --> 00:11:16,480
better place now

00:11:14,560 --> 00:11:18,160
we've had a lot of users contribute um

00:11:16,480 --> 00:11:18,880
packages to all the major distributions

00:11:18,160 --> 00:11:21,200
that was a

00:11:18,880 --> 00:11:22,959
kind of focus of a couple years ago more

00:11:21,200 --> 00:11:25,040
recently we're getting into performance

00:11:22,959 --> 00:11:26,399
um especially there's kind of like

00:11:25,040 --> 00:11:27,120
everyday performance we've made read

00:11:26,399 --> 00:11:30,000
write and read

00:11:27,120 --> 00:11:30,320
faster but we're starting to look at

00:11:30,000 --> 00:11:33,440
much

00:11:30,320 --> 00:11:35,360
larger files now this is important to um

00:11:33,440 --> 00:11:37,120
some users especially machine learning

00:11:35,360 --> 00:11:38,240
community and but other other people as

00:11:37,120 --> 00:11:40,240
well

00:11:38,240 --> 00:11:42,480
so a key one that was kind of

00:11:40,240 --> 00:11:44,959
foundational for some future ones was um

00:11:42,480 --> 00:11:46,880
partial updates to to objects via

00:11:44,959 --> 00:11:50,240
server-side copies

00:11:46,880 --> 00:11:52,880
before uh 2019 if you wanted to

00:11:50,240 --> 00:11:54,160
append a single byte to a file it would

00:11:52,880 --> 00:11:55,040
actually have to copy the whole file

00:11:54,160 --> 00:11:58,160
locally and then

00:11:55,040 --> 00:11:59,519
sync that whole thing back to s3 now

00:11:58,160 --> 00:12:01,200
it'll actually just issue a server side

00:11:59,519 --> 00:12:02,000
copy for the first n bytes and append

00:12:01,200 --> 00:12:04,720
that one byte

00:12:02,000 --> 00:12:05,839
to the new object this enabled the

00:12:04,720 --> 00:12:07,920
subsequent

00:12:05,839 --> 00:12:09,519
optimization for writing files larger

00:12:07,920 --> 00:12:11,519
than local storage

00:12:09,519 --> 00:12:13,440
so now we can create things larger than

00:12:11,519 --> 00:12:16,639
temporary storage and we flush every

00:12:13,440 --> 00:12:16,639
five gigabytes i believe

00:12:17,120 --> 00:12:21,600
a couple odds and ends of pitfalls we

00:12:19,920 --> 00:12:22,399
have a lot of reports from users where

00:12:21,600 --> 00:12:26,240
update dv

00:12:22,399 --> 00:12:27,839
is um is indexing

00:12:26,240 --> 00:12:29,279
their s3 container which is not what

00:12:27,839 --> 00:12:31,920
they expected especially when they have

00:12:29,279 --> 00:12:33,360
multiple sjfs clients on many machines

00:12:31,920 --> 00:12:36,959
this is easy to work around by adding

00:12:33,360 --> 00:12:38,560
prune paths to updatedb.conf

00:12:36,959 --> 00:12:40,240
s3fs does use temporary storage and

00:12:38,560 --> 00:12:40,639
slash temp and especially for users

00:12:40,240 --> 00:12:43,519
creating

00:12:40,639 --> 00:12:44,639
lots and lots of big files uh it can uh

00:12:43,519 --> 00:12:47,200
consume a lot of space

00:12:44,639 --> 00:12:47,920
you you can kind of influence this with

00:12:47,200 --> 00:12:50,959
with the

00:12:47,920 --> 00:12:52,560
actual insured disk free flag

00:12:50,959 --> 00:12:54,000
but it's just kind of a limitation of

00:12:52,560 --> 00:12:55,760
this uh mutable

00:12:54,000 --> 00:12:57,360
file system on top of immutable object

00:12:55,760 --> 00:12:58,720
store

00:12:57,360 --> 00:13:00,720
uh there's there have been a lot of

00:12:58,720 --> 00:13:02,800
people that have had permission problems

00:13:00,720 --> 00:13:03,760
um old versions of srafs did not have

00:13:02,800 --> 00:13:06,560
the correct defaults

00:13:03,760 --> 00:13:07,120
uh when the metadata headers weren't

00:13:06,560 --> 00:13:08,560
present

00:13:07,120 --> 00:13:10,800
we do have the correct defaults today

00:13:08,560 --> 00:13:12,399
although there's still

00:13:10,800 --> 00:13:13,600
teething issues with with people that

00:13:12,399 --> 00:13:15,120
have either created things with

00:13:13,600 --> 00:13:20,079
different uids and different systems

00:13:15,120 --> 00:13:22,240
and so you can override that with oumask

00:13:20,079 --> 00:13:23,839
a natural comparison people make is uh

00:13:22,240 --> 00:13:25,200
s3fs against nfs

00:13:23,839 --> 00:13:26,720
um these are actually very different

00:13:25,200 --> 00:13:27,360
systems they're both good for different

00:13:26,720 --> 00:13:29,279
reasons

00:13:27,360 --> 00:13:31,279
uh nfs is a better choice that when you

00:13:29,279 --> 00:13:32,160
need multi-client coordinations or

00:13:31,279 --> 00:13:34,160
you're making lots of small

00:13:32,160 --> 00:13:34,800
modifications to files or you're listing

00:13:34,160 --> 00:13:37,680
very large

00:13:34,800 --> 00:13:39,279
directories um nfs just has a better

00:13:37,680 --> 00:13:41,760
protocol to deal with this kind of thing

00:13:39,279 --> 00:13:44,000
so you'll you'll see that the nfs has

00:13:41,760 --> 00:13:46,880
strong semantics via leases instead of

00:13:44,000 --> 00:13:47,199
srfs has just timeouts that it uses and

00:13:46,880 --> 00:13:48,639
the

00:13:47,199 --> 00:13:50,240
right granularity can be as small as a

00:13:48,639 --> 00:13:52,160
single byte for nfs before

00:13:50,240 --> 00:13:53,360
versus safes requires writing five

00:13:52,160 --> 00:13:55,199
megabytes

00:13:53,360 --> 00:13:57,040
uh reader is much better for nfs because

00:13:55,199 --> 00:14:00,320
a single rpc instead of uh

00:13:57,040 --> 00:14:03,120
at ofn rbc's uh but uh

00:14:00,320 --> 00:14:04,639
sugarfest's favor um the actual s3 cost

00:14:03,120 --> 00:14:07,839
at least on amazon can be

00:14:04,639 --> 00:14:09,839
15 times cheaper uh this is really sffs

00:14:07,839 --> 00:14:10,720
is really a good solution for bulk data

00:14:09,839 --> 00:14:13,120
and of course if you're trying to

00:14:10,720 --> 00:14:15,040
interoperate with other sd applications

00:14:13,120 --> 00:14:19,680
it's uh better to do that through

00:14:15,040 --> 00:14:22,240
s3fs uh there are uh alternatives

00:14:19,680 --> 00:14:22,800
um there's a couple of s3 alternatives

00:14:22,240 --> 00:14:26,079
goofy's

00:14:22,800 --> 00:14:27,199
as a very good um well-tuned performance

00:14:26,079 --> 00:14:29,440
s3 file system

00:14:27,199 --> 00:14:30,320
uh gives up some posix compatibility to

00:14:29,440 --> 00:14:32,399
achieve that

00:14:30,320 --> 00:14:33,760
which um you know works for some people

00:14:32,399 --> 00:14:36,560
and i'm glad

00:14:33,760 --> 00:14:37,519
that this exists uh s3ql is a little bit

00:14:36,560 --> 00:14:39,360
more interesting

00:14:37,519 --> 00:14:40,560
um it's more of a local file system that

00:14:39,360 --> 00:14:42,480
uses sql lite

00:14:40,560 --> 00:14:44,000
locally to store all metadata and has

00:14:42,480 --> 00:14:44,880
its own object format so it's not

00:14:44,000 --> 00:14:48,240
interoperable

00:14:44,880 --> 00:14:49,760
with other um s3 applications i gave a

00:14:48,240 --> 00:14:50,000
talk about this at ohio linux fest if

00:14:49,760 --> 00:14:51,199
you

00:14:50,000 --> 00:14:53,440
are considering these alternatives

00:14:51,199 --> 00:14:54,639
please look into that um

00:14:53,440 --> 00:14:56,880
and it's just worth saying that there

00:14:54,639 --> 00:14:58,880
are other clouds and other protocols

00:14:56,880 --> 00:15:02,240
you may want to use those fuse file

00:14:58,880 --> 00:15:04,959
systems if you're on those clouds

00:15:02,240 --> 00:15:06,880
uh just to wrap up sfs provides a lot of

00:15:04,959 --> 00:15:09,600
posix compatibility uh using a

00:15:06,880 --> 00:15:10,639
s3 object store this this probably

00:15:09,600 --> 00:15:12,800
imposes overhead

00:15:10,639 --> 00:15:13,920
please use estate applications when it's

00:15:12,800 --> 00:15:16,480
appropriate

00:15:13,920 --> 00:15:18,240
uh there's been a couple of important

00:15:16,480 --> 00:15:19,680
features and fixes in recent years so if

00:15:18,240 --> 00:15:21,440
you weren't happy before please please

00:15:19,680 --> 00:15:23,040
try again

00:15:21,440 --> 00:15:24,800
and if you're still unhappy please try

00:15:23,040 --> 00:15:26,399
tuning the performance via the flags

00:15:24,800 --> 00:15:28,160
that i suggest earlier

00:15:26,399 --> 00:15:31,040
and finally consider the alternatives

00:15:28,160 --> 00:15:32,399
when your use case demands it

00:15:31,040 --> 00:15:34,399
uh so i think we're going to be taking

00:15:32,399 --> 00:15:36,000
questions uh in the chat room after the

00:15:34,399 --> 00:15:38,480
fact

00:15:36,000 --> 00:15:39,680
yes that's correct um there are already

00:15:38,480 --> 00:15:43,120
some questions waiting for you

00:15:39,680 --> 00:15:45,440
in the venulis um chat room alongside

00:15:43,120 --> 00:15:47,360
the video

00:15:45,440 --> 00:15:48,959
thank you very much for the talk um it's

00:15:47,360 --> 00:15:49,839
really good to see that this is an

00:15:48,959 --> 00:15:52,880
option when

00:15:49,839 --> 00:15:54,160
your use case actually needs it we now

00:15:52,880 --> 00:15:55,839
have a ten minute break

00:15:54,160 --> 00:15:57,360
and then we're back with the talk from

00:15:55,839 --> 00:16:05,839
matthew

00:15:57,360 --> 00:16:05,839

YouTube URL: https://www.youtube.com/watch?v=esT28l4NcSM


