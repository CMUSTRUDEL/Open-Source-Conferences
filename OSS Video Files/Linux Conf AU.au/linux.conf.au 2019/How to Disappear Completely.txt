Title: How to Disappear Completely
Publication date: 2020-01-09
Playlist: linux.conf.au 2019
Description: 
	Lilly Ryan

https://2019.linux.conf.au/schedule/presentation/250/

If you’ve ever wanted an invisibility cloak, this talk is for you.

Cameras peer at us from street corners, from phones, from the dashes of passing cars, and from shopping mall advertisements. Even when you’re not posing for a selfie, you might end up in the background of someone else’s picture while you’re out at dinner — and if that photograph is uploaded to a social media site, facial recognition can pick you out of the lineup.

Machines are getting better at recognising our faces — but if you want to sneak through the streets of your city like a young wizard in the corridors after midnight, there is still hope.

This talk provides an overview of the latest urban camouflage technology and how to deploy it in order to foil facial recognition. During this session you’ll learn the ways machines “see” us, and how to play with them in order to become invisible. We’ll dive into the ethics of biometric identification, the way current legislation affects this technology, and what is needed for facial recognition technology to be both useful and just.

linux.conf.au is a conference about the Linux operating system, and all aspects of the thriving ecosystem of Free and Open Source Software that has grown up around it. Run since 1999, in a different Australian or New Zealand city each year, by a team of local volunteers, LCA invites more than 500 people to learn from the people who shape the future of Open Source. For more information on the conference see https://linux.conf.au/

#linux.conf.au #linux #foss #opensource
Captions: 
	00:00:02,959 --> 00:00:10,830
hello no it is working it must be

00:00:09,420 --> 00:00:13,469
because everyone stopped talking

00:00:10,830 --> 00:00:15,360
so they must have heard me um hi

00:00:13,469 --> 00:00:18,000
everybody welcome back again to the

00:00:15,360 --> 00:00:19,560
privacy security identity mini

00:00:18,000 --> 00:00:23,189
conference the one with the longest name

00:00:19,560 --> 00:00:26,369
we're winning that so up next we've got

00:00:23,189 --> 00:00:28,349
Lily Ryan who is a pen tester Python

00:00:26,369 --> 00:00:31,349
Wrangler and recovering historian from

00:00:28,349 --> 00:00:33,450
Melbourne more importantly she has two

00:00:31,349 --> 00:00:43,350
adorable greyhounds and is one of the

00:00:33,450 --> 00:00:45,210
conference's many knitters Lily does an

00:00:43,350 --> 00:00:46,950
amazing talk where she wears a wizard

00:00:45,210 --> 00:00:51,030
robe and has a wand and she's not doing

00:00:46,950 --> 00:00:57,079
that today instead she's gonna tell us

00:00:51,030 --> 00:00:59,910
how to disappear completely hi folks

00:00:57,079 --> 00:01:01,500
yeah this one we're all good everyone

00:00:59,910 --> 00:01:03,960
can hear me everyone at the back yes

00:01:01,500 --> 00:01:06,990
good I've got a thumbs down and a thumbs

00:01:03,960 --> 00:01:10,200
up and now I'm confused go to assume

00:01:06,990 --> 00:01:12,869
that because you laughed it was fine my

00:01:10,200 --> 00:01:14,900
name is Lily Ryan and today I am going

00:01:12,869 --> 00:01:18,600
to teach you how to disappear completely

00:01:14,900 --> 00:01:21,450
you can find me online as Atticus or on

00:01:18,600 --> 00:01:23,250
Twitter as Atticus underscore au because

00:01:21,450 --> 00:01:26,820
someone is squatting on the other name

00:01:23,250 --> 00:01:29,369
and has been a few years in previous

00:01:26,820 --> 00:01:30,689
life I was a medieval historian and a

00:01:29,369 --> 00:01:32,490
linguist and a graphic designer

00:01:30,689 --> 00:01:34,290
but these days is there an said I'm a

00:01:32,490 --> 00:01:36,930
pen tester and my job is to push the

00:01:34,290 --> 00:01:39,119
boundaries of systems the systems that I

00:01:36,930 --> 00:01:40,829
like to interrogate the most of the

00:01:39,119 --> 00:01:42,960
social systems that govern how and when

00:01:40,829 --> 00:01:45,329
technologies used and I'm especially

00:01:42,960 --> 00:01:47,310
interested in the non-consensual use of

00:01:45,329 --> 00:01:49,020
Technology and public spaces and how we

00:01:47,310 --> 00:01:50,670
can make these users more visible to the

00:01:49,020 --> 00:01:53,670
public and how we can challenge them

00:01:50,670 --> 00:01:55,680
when we find them so today we're diving

00:01:53,670 --> 00:01:57,930
into one of the more thorny and

00:01:55,680 --> 00:02:00,630
increasingly prevalent examples of

00:01:57,930 --> 00:02:03,180
non-consensual public tech use facial

00:02:00,630 --> 00:02:04,680
recognition technology over the next

00:02:03,180 --> 00:02:06,930
half an hour I'll show you how this tech

00:02:04,680 --> 00:02:09,450
works where it gets used and how it can

00:02:06,930 --> 00:02:11,819
be broken and we'll also talk about why

00:02:09,450 --> 00:02:13,500
it's important to keep being critical of

00:02:11,819 --> 00:02:15,690
facial recognition technology

00:02:13,500 --> 00:02:17,790
even though legislation like the gdpr

00:02:15,690 --> 00:02:20,490
promises to make it easier for this tech

00:02:17,790 --> 00:02:23,930
to not be used without your informed

00:02:20,490 --> 00:02:26,520
consent so let's start with the

00:02:23,930 --> 00:02:29,280
science-fiction version of facial

00:02:26,520 --> 00:02:31,020
recognition so this is the intro for the

00:02:29,280 --> 00:02:33,410
American television series personal

00:02:31,020 --> 00:02:35,910
interest there's a lot of high-tech

00:02:33,410 --> 00:02:37,470
sci-fi face detecting going on you know

00:02:35,910 --> 00:02:39,930
in connecting faces with identities

00:02:37,470 --> 00:02:41,850
we're interpreting moods linking phone

00:02:39,930 --> 00:02:44,250
call metadata all of that stuff it's

00:02:41,850 --> 00:02:46,830
really great fiction but reality has

00:02:44,250 --> 00:02:48,060
already stepped over this threshold so

00:02:46,830 --> 00:02:52,080
you might have seen this clip online

00:02:48,060 --> 00:02:53,610
before in September 2017 this clip of

00:02:52,080 --> 00:02:55,400
traffic surveillance cameras at a

00:02:53,610 --> 00:02:58,170
Chinese intersection surfaced online

00:02:55,400 --> 00:02:59,790
because we live in the era of fake news

00:02:58,170 --> 00:03:01,350
it's easy to be skeptical about all

00:02:59,790 --> 00:03:04,590
these short clips that claim to be

00:03:01,350 --> 00:03:06,390
unveiling this near futuristic tech but

00:03:04,590 --> 00:03:09,180
this is actually real software running

00:03:06,390 --> 00:03:11,550
on a live system and here's a better

00:03:09,180 --> 00:03:12,900
screen capture of that same software the

00:03:11,550 --> 00:03:14,220
labels are in English this time instead

00:03:12,900 --> 00:03:16,260
of in Mandarin but they are still

00:03:14,220 --> 00:03:19,049
probably too small for you to read so

00:03:16,260 --> 00:03:21,780
I'll tell you what it's doing it's

00:03:19,049 --> 00:03:23,250
detecting the basics of the things that

00:03:21,780 --> 00:03:25,769
are in the video stream in front of it

00:03:23,250 --> 00:03:28,500
is this thing a car is it a bicycle is

00:03:25,769 --> 00:03:31,739
it a pedestrian is it a truck if it's a

00:03:28,500 --> 00:03:33,480
car what color is it you can also see it

00:03:31,739 --> 00:03:38,040
highlighting the path that everybody's

00:03:33,480 --> 00:03:39,630
taking as well and if it thinks it sees

00:03:38,040 --> 00:03:41,760
it a pedestrian it will then start

00:03:39,630 --> 00:03:44,970
describing what gender it perceives that

00:03:41,760 --> 00:03:47,220
person to be the approximate age for the

00:03:44,970 --> 00:03:50,070
person what they might be wearing as it

00:03:47,220 --> 00:03:51,720
passes it address that kind of thing and

00:03:50,070 --> 00:03:54,239
having access to this kind of capability

00:03:51,720 --> 00:03:56,400
makes it really easy to track somebody

00:03:54,239 --> 00:03:57,870
across a busy city even in China where

00:03:56,400 --> 00:04:00,060
there's a population of over a billion

00:03:57,870 --> 00:04:01,440
people and you can see it the size of

00:04:00,060 --> 00:04:04,110
the screen where the software is pulling

00:04:01,440 --> 00:04:05,760
out a lot of still images it'll use

00:04:04,110 --> 00:04:08,970
those and store them to compare them to

00:04:05,760 --> 00:04:10,350
images later on for reference and since

00:04:08,970 --> 00:04:12,450
time which is the company that makes

00:04:10,350 --> 00:04:14,970
this software have another products

00:04:12,450 --> 00:04:17,850
called sense face which does exactly

00:04:14,970 --> 00:04:20,010
what it says on the tin cents face picks

00:04:17,850 --> 00:04:21,570
up live streams from cameras at places

00:04:20,010 --> 00:04:23,250
where people will pass through with

00:04:21,570 --> 00:04:25,950
their faces clearly visible to the

00:04:23,250 --> 00:04:27,330
stream screen that's places like

00:04:25,950 --> 00:04:29,520
escalators

00:04:27,330 --> 00:04:31,259
train station ticket barriers their

00:04:29,520 --> 00:04:33,569
software will match the features from

00:04:31,259 --> 00:04:36,210
the faces that they see with the faces

00:04:33,569 --> 00:04:37,949
that they already have on file and the

00:04:36,210 --> 00:04:39,509
Chinese government requires an ID card

00:04:37,949 --> 00:04:41,729
for every citizen over the age of 18

00:04:39,509 --> 00:04:44,099
which is also what this technology was

00:04:41,729 --> 00:04:45,690
trained on so there's not only a

00:04:44,099 --> 00:04:48,180
ready-made government database to

00:04:45,690 --> 00:04:50,009
compare this to but also it's almost

00:04:48,180 --> 00:04:51,599
certainly seen every single one of these

00:04:50,009 --> 00:04:53,520
faces before so its results are

00:04:51,599 --> 00:04:56,250
extremely accurate because this is its

00:04:53,520 --> 00:04:57,810
training set it can be really easy to

00:04:56,250 --> 00:04:59,940
look at this kind of stuff and dismiss

00:04:57,810 --> 00:05:02,129
it out of hand because we hear a lot

00:04:59,940 --> 00:05:03,539
that the general attitude of the Chinese

00:05:02,129 --> 00:05:04,879
public to things like privacy and

00:05:03,539 --> 00:05:07,259
surveillance are fundamentally different

00:05:04,879 --> 00:05:09,900
to the attitudes to those things in

00:05:07,259 --> 00:05:12,780
other countries but systems like this

00:05:09,900 --> 00:05:14,280
absolutely get used in other places the

00:05:12,780 --> 00:05:16,889
United States and particularly gives us

00:05:14,280 --> 00:05:19,080
a number of egregious examples and this

00:05:16,889 --> 00:05:20,460
isn't just because Silicon Valley keeps

00:05:19,080 --> 00:05:22,080
coming up with new things that it

00:05:20,460 --> 00:05:25,259
decides to embed facial recognition

00:05:22,080 --> 00:05:27,090
technology into but also because of the

00:05:25,259 --> 00:05:28,500
diversity of the state by state laws in

00:05:27,090 --> 00:05:32,400
that country it's led to some really

00:05:28,500 --> 00:05:34,199
extreme use cases one among the many

00:05:32,400 --> 00:05:36,270
things that Amazon's been criticized for

00:05:34,199 --> 00:05:38,819
recently is for selling cheap access and

00:05:36,270 --> 00:05:40,380
consulting services to local US law

00:05:38,819 --> 00:05:43,440
enforcement agencies that want to use

00:05:40,380 --> 00:05:45,000
its recognition API and at a time when

00:05:43,440 --> 00:05:46,949
people are unjustly targeted by law

00:05:45,000 --> 00:05:49,020
enforcement in the US and many other

00:05:46,949 --> 00:05:51,270
places every day because just the color

00:05:49,020 --> 00:05:53,099
of their skin it's really troubling to

00:05:51,270 --> 00:05:55,080
learn but yet another closed source

00:05:53,099 --> 00:05:57,300
proprietary algorithm is being mixed

00:05:55,080 --> 00:06:01,500
into law enforcement processes without

00:05:57,300 --> 00:06:04,080
any kind of apparent oversight and it's

00:06:01,500 --> 00:06:05,639
partly because of things like this that

00:06:04,080 --> 00:06:07,080
a lot of the loudest critical and

00:06:05,639 --> 00:06:09,719
ethical research into facial recognition

00:06:07,080 --> 00:06:13,740
technology also comes from the United

00:06:09,719 --> 00:06:16,080
States this is joy Balham weanie she is

00:06:13,740 --> 00:06:18,000
a researcher at MIT Media labs she is

00:06:16,080 --> 00:06:20,099
one hell of a poet and she's campaigning

00:06:18,000 --> 00:06:22,409
really hard for more transparency into

00:06:20,099 --> 00:06:27,180
around how accurate facial recognition

00:06:22,409 --> 00:06:29,310
tech can be in 2015 well and when he

00:06:27,180 --> 00:06:32,069
founded the algorithmic Justice League

00:06:29,310 --> 00:06:33,150
to raise awareness of the social biases

00:06:32,069 --> 00:06:36,419
that are built into facial recognition

00:06:33,150 --> 00:06:38,159
tech and she started it because her own

00:06:36,419 --> 00:06:40,500
research projects into facial

00:06:38,159 --> 00:06:40,920
recognition technology and augmented

00:06:40,500 --> 00:06:42,450
reality

00:06:40,920 --> 00:06:44,520
were extremely difficult for her to

00:06:42,450 --> 00:06:46,320
finish because everything that she tried

00:06:44,520 --> 00:06:48,090
was full of this proprietary algorithm

00:06:46,320 --> 00:06:50,670
that had been trained on heavily died

00:06:48,090 --> 00:06:52,650
heavily by its datasets that had failed

00:06:50,670 --> 00:06:56,010
to recognize her dark skin as a human

00:06:52,650 --> 00:06:57,870
face so in order to complete her work

00:06:56,010 --> 00:06:59,730
she had to result to holding a white

00:06:57,870 --> 00:07:01,320
mask over her face or to get one of her

00:06:59,730 --> 00:07:03,000
lighter skinned colleagues to perform

00:07:01,320 --> 00:07:06,840
the experiment for her because the

00:07:03,000 --> 00:07:09,570
datasets weren't picking her up so in

00:07:06,840 --> 00:07:11,520
December 2017 under the umbrella of the

00:07:09,570 --> 00:07:13,590
algorithmic Justice League Walla Meany

00:07:11,520 --> 00:07:15,180
published a comparative study of the

00:07:13,590 --> 00:07:18,150
accuracy of three commercially available

00:07:15,180 --> 00:07:21,150
facial recognition api's she looked at

00:07:18,150 --> 00:07:23,580
face plus plus Microsoft Azure face API

00:07:21,150 --> 00:07:25,350
and IBM's Watson and she found that

00:07:23,580 --> 00:07:28,110
there was an overall trend towards

00:07:25,350 --> 00:07:30,870
accuracy but only if you were a lighter

00:07:28,110 --> 00:07:33,030
skinned male presenting person and there

00:07:30,870 --> 00:07:34,980
were many many more false matches if you

00:07:33,030 --> 00:07:36,780
were a darker skinned or a female

00:07:34,980 --> 00:07:39,900
presenting person or especially if you

00:07:36,780 --> 00:07:42,000
were both of those things her work also

00:07:39,900 --> 00:07:44,250
highlighted how just how opaque a lot of

00:07:42,000 --> 00:07:46,350
these algorithms are so while everyone's

00:07:44,250 --> 00:07:47,880
running around copy writing and closing

00:07:46,350 --> 00:07:49,590
off the algorithms in the datasets that

00:07:47,880 --> 00:07:52,500
make up their secret sauce facial

00:07:49,590 --> 00:07:55,350
recognition systems almost nobody is

00:07:52,500 --> 00:07:57,870
actually independently auditing any of

00:07:55,350 --> 00:08:00,150
these things most of the statistics that

00:07:57,870 --> 00:08:03,210
we have about how accurate any given

00:08:00,150 --> 00:08:05,490
facial recognition API is come from the

00:08:03,210 --> 00:08:09,030
companies that sell them this is not

00:08:05,490 --> 00:08:11,220
science this is marketing and when this

00:08:09,030 --> 00:08:13,080
kind of tech gets sold to third parties

00:08:11,220 --> 00:08:14,610
then used to make decisions about who

00:08:13,080 --> 00:08:16,590
gets to be arrested and who should be

00:08:14,610 --> 00:08:19,590
detained there is really no recourse for

00:08:16,590 --> 00:08:22,620
any kind of justice and it's not just

00:08:19,590 --> 00:08:24,390
bias based on skin color either some of

00:08:22,620 --> 00:08:25,860
you may have seen this study late last

00:08:24,390 --> 00:08:27,720
year pair of researchers from Stanford

00:08:25,860 --> 00:08:30,060
University published a study that

00:08:27,720 --> 00:08:31,560
claimed to be able to use facial

00:08:30,060 --> 00:08:33,540
recognition to detect whether or not

00:08:31,560 --> 00:08:36,750
somebody was homosexual or heterosexual

00:08:33,540 --> 00:08:38,220
I yelled so loudly when I read this

00:08:36,750 --> 00:08:40,679
paper that one of my neighbors came over

00:08:38,220 --> 00:08:44,169
to see if I was okay

00:08:40,679 --> 00:08:46,689
because while the algorithm absolutely

00:08:44,169 --> 00:08:48,420
does take pictures of human faces and it

00:08:46,689 --> 00:08:52,029
assigns them a rating of gay or straight

00:08:48,420 --> 00:08:54,790
the way the algorithms underneath was

00:08:52,029 --> 00:08:58,149
set up completely ignored the idea that

00:08:54,790 --> 00:09:00,129
anybody might be bisexual or asexual or

00:08:58,149 --> 00:09:02,800
that gender exists on a spectrum instead

00:09:00,129 --> 00:09:05,290
of as a strict binary and yet all over

00:09:02,800 --> 00:09:08,589
the news people were talking about this

00:09:05,290 --> 00:09:11,170
stuff as some kind of magic scientific

00:09:08,589 --> 00:09:13,480
facial recognition gaydar instead of

00:09:11,170 --> 00:09:16,230
questioning what really is pseudo

00:09:13,480 --> 00:09:18,910
scientific phrenological nonsense

00:09:16,230 --> 00:09:20,439
nobody really questioned it mostly

00:09:18,910 --> 00:09:23,410
because it used phrases like facial

00:09:20,439 --> 00:09:24,639
recognition and machine learning and so

00:09:23,410 --> 00:09:28,300
of course it must be cutting-edge and

00:09:24,639 --> 00:09:29,649
accurate but it isn't just the law

00:09:28,300 --> 00:09:31,749
enforcement agencies and their

00:09:29,649 --> 00:09:33,970
headline-grabbing researchers that use

00:09:31,749 --> 00:09:35,730
this kind of tech facial recognition is

00:09:33,970 --> 00:09:38,230
used in marketing and advertising

00:09:35,730 --> 00:09:40,720
contexts far more often than anything

00:09:38,230 --> 00:09:42,990
else and many times even with less

00:09:40,720 --> 00:09:46,660
public scrutiny and awareness and

00:09:42,990 --> 00:09:48,699
oversight this for example is the

00:09:46,660 --> 00:09:50,769
maintenance interface of a digital

00:09:48,699 --> 00:09:53,230
advertising screen from a shopping mall

00:09:50,769 --> 00:09:55,480
in Canberra in Australia this was taken

00:09:53,230 --> 00:09:57,129
in October 2017 you can see it there

00:09:55,480 --> 00:09:59,920
detecting the faces of the two people in

00:09:57,129 --> 00:10:01,839
front of it in Australia it is very

00:09:59,920 --> 00:10:04,360
common for digital advertising screens

00:10:01,839 --> 00:10:05,980
to do this a lot of them are equipped

00:10:04,360 --> 00:10:08,049
with cameras there right up the top of

00:10:05,980 --> 00:10:09,699
the little monolith looking thing and

00:10:08,049 --> 00:10:11,889
they're pre-installed with tracking

00:10:09,699 --> 00:10:14,379
software this one's running software by

00:10:11,889 --> 00:10:16,179
a French company called qui VD who they

00:10:14,379 --> 00:10:20,350
specialize in commercial applications of

00:10:16,179 --> 00:10:22,389
facial recognition so the Westfield

00:10:20,350 --> 00:10:24,279
corporation which operates the shopping

00:10:22,389 --> 00:10:26,769
mall where that particular screen was

00:10:24,279 --> 00:10:29,230
found runs many other shopping malls all

00:10:26,769 --> 00:10:31,230
across Australia and New Zealand and

00:10:29,230 --> 00:10:33,999
Europe and North and South America and

00:10:31,230 --> 00:10:36,459
using additive using advertising screens

00:10:33,999 --> 00:10:38,679
like that one they detect what they

00:10:36,459 --> 00:10:40,899
think is your age and your gender and

00:10:38,679 --> 00:10:42,639
your mood and then they correlate this

00:10:40,899 --> 00:10:43,839
information with data from your

00:10:42,639 --> 00:10:45,610
smartphone paying off their

00:10:43,839 --> 00:10:47,709
complimentary Wi-Fi routers all around

00:10:45,610 --> 00:10:50,620
the malls to make a real-time map of

00:10:47,709 --> 00:10:52,660
where people are what advertising they

00:10:50,620 --> 00:10:54,730
should be showing to these people

00:10:52,660 --> 00:10:56,680
tracking suspicious-looking kinds of

00:10:54,730 --> 00:10:58,510
people for security incident purposes

00:10:56,680 --> 00:11:01,900
and to work out whether they can sell

00:10:58,510 --> 00:11:03,760
everybody everything else when this made

00:11:01,900 --> 00:11:06,010
news in Australia in 2017

00:11:03,760 --> 00:11:07,630
what's field was contacted for comment

00:11:06,010 --> 00:11:10,800
and a spokeswoman for Westfield said

00:11:07,630 --> 00:11:13,540
that they do this stuff to connect

00:11:10,800 --> 00:11:19,210
customers with retailers in a more

00:11:13,540 --> 00:11:21,700
meaningful way I'm not actually sure

00:11:19,210 --> 00:11:22,990
what that sentence means because there's

00:11:21,700 --> 00:11:25,090
someone who's walking around that kind

00:11:22,990 --> 00:11:28,210
of mall I don't feel a special

00:11:25,090 --> 00:11:29,920
relationship to anybody especially if I

00:11:28,210 --> 00:11:31,570
don't know that this kind of traffic is

00:11:29,920 --> 00:11:34,030
happening connection is a two-way thing

00:11:31,570 --> 00:11:38,290
and if I'm not participating it's not a

00:11:34,030 --> 00:11:39,880
connection but Westfield isn't the only

00:11:38,290 --> 00:11:41,770
retail operator who does this kind of

00:11:39,880 --> 00:11:44,560
thing inside of shopping environments so

00:11:41,770 --> 00:11:46,270
in May last year foodstuffs the parent

00:11:44,560 --> 00:11:47,590
company who runs the new world in the

00:11:46,270 --> 00:11:49,900
Foursquare supermarkets in New Zealand

00:11:47,590 --> 00:11:51,190
among other things they got a lot of

00:11:49,900 --> 00:11:53,380
press when it came out that they were

00:11:51,190 --> 00:11:55,060
using facial recognition technology in

00:11:53,380 --> 00:11:57,460
their stores to try to detect

00:11:55,060 --> 00:11:59,290
shoplifters it only came out because

00:11:57,460 --> 00:12:00,910
they used this check to apprehend a

00:11:59,290 --> 00:12:02,650
visitor to one of their supermarkets who

00:12:00,910 --> 00:12:03,930
turned out wasn't actually the person

00:12:02,650 --> 00:12:06,370
that they were after

00:12:03,930 --> 00:12:08,380
so this cutting-edge technology got a

00:12:06,370 --> 00:12:10,120
lot of bad press they were being used to

00:12:08,380 --> 00:12:12,340
persecute for being used to persecute

00:12:10,120 --> 00:12:14,440
people for crimes which clearly didn't

00:12:12,340 --> 00:12:15,790
work very well but even more than that

00:12:14,440 --> 00:12:18,190
people were angry that this kind of

00:12:15,790 --> 00:12:19,780
technology was in use every time they

00:12:18,190 --> 00:12:23,470
stopped by to get a loaf of bread and

00:12:19,780 --> 00:12:24,700
they hadn't been informed about it even

00:12:23,470 --> 00:12:26,290
churches have been getting in on the

00:12:24,700 --> 00:12:29,710
facial-recognition action in the last

00:12:26,290 --> 00:12:32,050
few years there is a whole subset of the

00:12:29,710 --> 00:12:33,880
industry dedicated to making it easy to

00:12:32,050 --> 00:12:37,600
know just who has and hasn't been to

00:12:33,880 --> 00:12:42,700
chat with God this week it is what I

00:12:37,600 --> 00:12:43,900
look it up at its core facial

00:12:42,700 --> 00:12:46,570
recognition tech is just trying to

00:12:43,900 --> 00:12:49,570
answer this one question who is this

00:12:46,570 --> 00:12:51,340
particular person the problems come when

00:12:49,570 --> 00:12:51,910
we overlay different meanings on that

00:12:51,340 --> 00:12:54,340
answer

00:12:51,910 --> 00:12:57,280
the question of who is this can be

00:12:54,340 --> 00:12:58,540
either benign or threatening when we

00:12:57,280 --> 00:13:00,820
look at it through the lens of law

00:12:58,540 --> 00:13:03,070
enforcement or marketing or religion or

00:13:00,820 --> 00:13:05,680
sexuality or any number of other

00:13:03,070 --> 00:13:06,760
applications sometimes it makes life

00:13:05,680 --> 00:13:09,100
convenient for

00:13:06,760 --> 00:13:11,560
often it comes at the expense of being

00:13:09,100 --> 00:13:13,810
invisibly surveilled without being told

00:13:11,560 --> 00:13:15,700
about it and depending on who you are

00:13:13,810 --> 00:13:18,820
where you are in the world where you are

00:13:15,700 --> 00:13:20,800
in life and how political factors work

00:13:18,820 --> 00:13:23,370
for or against you at any given point in

00:13:20,800 --> 00:13:26,920
time the thing that was convenient today

00:13:23,370 --> 00:13:28,420
can be threatening tomorrow but without

00:13:26,920 --> 00:13:29,680
questioning the convenience about

00:13:28,420 --> 00:13:30,910
pushing the boundaries and the

00:13:29,680 --> 00:13:32,980
assumptions that we make about this

00:13:30,910 --> 00:13:36,640
technology the infrastructure will

00:13:32,980 --> 00:13:39,100
remainder facial recognition tech is

00:13:36,640 --> 00:13:40,720
often marketed as something that helps

00:13:39,100 --> 00:13:44,290
us to trust that a person is who they

00:13:40,720 --> 00:13:46,390
say they are but given how given what we

00:13:44,290 --> 00:13:48,430
know about how many different facial

00:13:46,390 --> 00:13:51,160
recognition products there are and how

00:13:48,430 --> 00:13:53,500
little of them are independently audited

00:13:51,160 --> 00:13:55,120
at all you know how much we kick how

00:13:53,500 --> 00:13:58,830
much can we trust that facial

00:13:55,120 --> 00:14:01,450
recognition itself is who it says it is

00:13:58,830 --> 00:14:03,130
how much can we trust that the facial

00:14:01,450 --> 00:14:06,130
recognition Texas used in the spaces

00:14:03,130 --> 00:14:09,990
that we inhabit won't one day be used to

00:14:06,130 --> 00:14:13,510
turn us algorithmic biases around on us

00:14:09,990 --> 00:14:16,510
the good news for a given value of good

00:14:13,510 --> 00:14:18,310
is that plenty of people from artists to

00:14:16,510 --> 00:14:20,290
academics to fashion designers have

00:14:18,310 --> 00:14:21,910
given thought to how to circumvent these

00:14:20,290 --> 00:14:24,700
technologies and we can do this

00:14:21,910 --> 00:14:26,320
ourselves as well to do it properly we

00:14:24,700 --> 00:14:28,570
first have to understand how facial

00:14:26,320 --> 00:14:29,740
recognition works like we've just

00:14:28,570 --> 00:14:32,050
established facial recognition

00:14:29,740 --> 00:14:33,300
technology is a broad umbrella term that

00:14:32,050 --> 00:14:35,290
covers a lot of proprietary

00:14:33,300 --> 00:14:37,720
implementations algorithms and

00:14:35,290 --> 00:14:40,029
approaches as well as a handful of open

00:14:37,720 --> 00:14:41,230
source solutions but although every

00:14:40,029 --> 00:14:42,690
company's product looks a little

00:14:41,230 --> 00:14:45,580
different under the hood they're all

00:14:42,690 --> 00:14:48,339
operating using roughly the same kinds

00:14:45,580 --> 00:14:51,089
of steps the three steps that they use

00:14:48,339 --> 00:14:55,000
our detection face print creation and

00:14:51,089 --> 00:14:56,620
recognition before a system can

00:14:55,000 --> 00:14:59,050
recognize a face it needs to know that

00:14:56,620 --> 00:15:01,300
it is looking at a face this is what

00:14:59,050 --> 00:15:03,100
detection does traditional computer

00:15:01,300 --> 00:15:05,470
vision facial detection is all about the

00:15:03,100 --> 00:15:07,510
patterns maps of light and dark parts of

00:15:05,470 --> 00:15:09,070
an image and the pattern formed by the

00:15:07,510 --> 00:15:12,279
eyes the nose and the mouth which is

00:15:09,070 --> 00:15:14,860
quite a distinctive thing this is a

00:15:12,279 --> 00:15:16,690
representation of the Viola Jones object

00:15:14,860 --> 00:15:19,180
detection framework which is a fairly

00:15:16,690 --> 00:15:19,930
used fairly commonly used system for

00:15:19,180 --> 00:15:22,330
detecting face

00:15:19,930 --> 00:15:24,520
in an image if the text patterns of dark

00:15:22,330 --> 00:15:26,860
and light relative to the patterns next

00:15:24,520 --> 00:15:29,680
to them and it refines this model over a

00:15:26,860 --> 00:15:31,450
few iterations it's pretty crude pattern

00:15:29,680 --> 00:15:33,610
matching needs a full-frontal image of a

00:15:31,450 --> 00:15:34,990
face in order it'll work effectively but

00:15:33,610 --> 00:15:37,300
it does the work fairly well with these

00:15:34,990 --> 00:15:40,570
limitations this is what most phone

00:15:37,300 --> 00:15:42,310
cameras will use to detect faces and put

00:15:40,570 --> 00:15:43,480
a little box around them it's cheap it

00:15:42,310 --> 00:15:46,720
really doesn't take much processing

00:15:43,480 --> 00:15:49,180
power to do this the next step is face

00:15:46,720 --> 00:15:50,920
print creation which helps to get helps

00:15:49,180 --> 00:15:52,480
the system's get a more accurate map of

00:15:50,920 --> 00:15:55,510
a human face from a two-dimensional

00:15:52,480 --> 00:15:57,100
image traditional fake facial

00:15:55,510 --> 00:15:59,080
recognition tech often does this by

00:15:57,100 --> 00:16:01,029
creating a mesh this measures things

00:15:59,080 --> 00:16:03,430
like the distance between your eyes and

00:16:01,029 --> 00:16:05,260
the width of your nose and depth of the

00:16:03,430 --> 00:16:08,709
eye sockets the shape of the cheekbones

00:16:05,260 --> 00:16:10,420
the length of the jawline often it

00:16:08,709 --> 00:16:12,940
begins with a generic model of a human

00:16:10,420 --> 00:16:14,560
face which is trained on lots of

00:16:12,940 --> 00:16:16,779
different photographs of people and then

00:16:14,560 --> 00:16:18,279
kind of averaged out and then the system

00:16:16,779 --> 00:16:20,050
is placed over your face which has

00:16:18,279 --> 00:16:22,720
already been detected by the system and

00:16:20,050 --> 00:16:24,459
the dot points are adjusted to fit the

00:16:22,720 --> 00:16:27,130
edges of the dark and light patterns of

00:16:24,459 --> 00:16:28,810
your face and then this model is refined

00:16:27,130 --> 00:16:31,089
by relating the dots to one another

00:16:28,810 --> 00:16:33,730
which creates a mesh of your face that

00:16:31,089 --> 00:16:34,959
can track you as you tilt and move when

00:16:33,730 --> 00:16:37,540
you aren't looking directly at the

00:16:34,959 --> 00:16:38,580
camera but as I said this isn't the only

00:16:37,540 --> 00:16:40,959
way to do it

00:16:38,580 --> 00:16:43,060
newer methods take different approaches

00:16:40,959 --> 00:16:46,300
this is what the iPhones true depth

00:16:43,060 --> 00:16:48,970
camera sees the iPhone projects an array

00:16:46,300 --> 00:16:50,709
of 30,000 infrared dots over your face

00:16:48,970 --> 00:16:52,720
to get a real-time sense of depth and

00:16:50,709 --> 00:16:55,089
this is just one frame of data from

00:16:52,720 --> 00:16:57,520
those senses they take when it takes a

00:16:55,089 --> 00:16:59,230
capture it takes many many frames of

00:16:57,520 --> 00:17:01,029
data in real time and compares them to

00:16:59,230 --> 00:17:04,420
each other to get a fairly accurate 3d

00:17:01,029 --> 00:17:05,860
map of your whole face once the system

00:17:04,420 --> 00:17:09,250
has a map of the face it can try to

00:17:05,860 --> 00:17:10,600
recognize in recognition connects all of

00:17:09,250 --> 00:17:12,760
this info with the other data that the

00:17:10,600 --> 00:17:15,730
system already has about you in the case

00:17:12,760 --> 00:17:17,589
of the iPhone face ID you have to

00:17:15,730 --> 00:17:19,089
register your face first and then it

00:17:17,589 --> 00:17:22,959
stores that information to compare all

00:17:19,089 --> 00:17:24,910
the new data against then you can have

00:17:22,959 --> 00:17:26,920
like a numerical representation of a

00:17:24,910 --> 00:17:28,720
mesh pattern compared against other mesh

00:17:26,920 --> 00:17:31,390
patterns in different database like like

00:17:28,720 --> 00:17:32,710
a government database some recognition

00:17:31,390 --> 00:17:33,120
approaches are a little bit different

00:17:32,710 --> 00:17:34,919
though

00:17:33,120 --> 00:17:37,140
some of these earlier steps might be

00:17:34,919 --> 00:17:39,120
skipped in favor of a different machine

00:17:37,140 --> 00:17:41,669
learning approach which happens after it

00:17:39,120 --> 00:17:44,220
detects a face for example Facebook's

00:17:41,669 --> 00:17:46,169
facial recognition algorithm is called

00:17:44,220 --> 00:17:47,520
deep face which I thought was

00:17:46,169 --> 00:17:50,070
ill-advised but there you have it

00:17:47,520 --> 00:17:51,539
and this is what it uses in the

00:17:50,070 --> 00:17:53,309
jurisdictions where it's allowed to at

00:17:51,539 --> 00:17:55,020
any rate to order a tag you and your

00:17:53,309 --> 00:17:56,940
friends in images when you upload them

00:17:55,020 --> 00:17:59,130
to the site and this method is

00:17:56,940 --> 00:18:00,929
apparently 97% accurate although again

00:17:59,130 --> 00:18:02,760
that is according to Facebook themselves

00:18:00,929 --> 00:18:06,809
which so that's marketing and it's not

00:18:02,760 --> 00:18:08,850
actually science another approach which

00:18:06,809 --> 00:18:11,730
is used by things like the Python face

00:18:08,850 --> 00:18:14,039
recognition library uses a pre trained

00:18:11,730 --> 00:18:15,419
deep convolutional neural network to do

00:18:14,039 --> 00:18:15,929
the detecting and they're recognizing in

00:18:15,419 --> 00:18:18,120
the face

00:18:15,929 --> 00:18:20,340
it'll take still photographs of people's

00:18:18,120 --> 00:18:23,100
faces do a bit of mesh creation and then

00:18:20,340 --> 00:18:25,440
skew them until the system has some kind

00:18:23,100 --> 00:18:27,690
of average a bit like what the face

00:18:25,440 --> 00:18:30,899
recognition library has done to this

00:18:27,690 --> 00:18:33,179
picture will ferrel up here and then the

00:18:30,899 --> 00:18:35,130
neural network takes 128 separate

00:18:33,179 --> 00:18:37,799
measurements of the face these

00:18:35,130 --> 00:18:39,179
measurements correspond to we we don't

00:18:37,799 --> 00:18:41,159
actually know what they correspond to

00:18:39,179 --> 00:18:42,750
because the neural network kind of

00:18:41,159 --> 00:18:44,789
decides for itself what it wants to

00:18:42,750 --> 00:18:47,309
measure and however it does that

00:18:44,789 --> 00:18:50,279
whatever happens it comes up with these

00:18:47,309 --> 00:18:51,779
mysterious 128 numbers and then these

00:18:50,279 --> 00:18:53,760
measurements are unique enough to be

00:18:51,779 --> 00:18:55,799
used to identify people when the camera

00:18:53,760 --> 00:18:57,029
sees them again because the network can

00:18:55,799 --> 00:19:00,330
run the same calculations and then

00:18:57,029 --> 00:19:01,559
cross-correlate i've tested this library

00:19:00,330 --> 00:19:04,770
out by training on a picture of myself

00:19:01,559 --> 00:19:06,029
and it was really spooky how quickly it

00:19:04,770 --> 00:19:07,770
would recognize me even when I was

00:19:06,029 --> 00:19:10,350
wearing a mask or a fake beard or

00:19:07,770 --> 00:19:12,929
pulling a silly face which leads me to

00:19:10,350 --> 00:19:16,559
my next point how do we mess with these

00:19:12,929 --> 00:19:18,600
things this depends on whether you want

00:19:16,559 --> 00:19:19,679
to mess with facial detection and stop

00:19:18,600 --> 00:19:20,789
this at the source so it doesn't

00:19:19,679 --> 00:19:23,490
actually know that there's a person

00:19:20,789 --> 00:19:25,380
there or faceprint creation so that the

00:19:23,490 --> 00:19:27,270
record of your face that's created now

00:19:25,380 --> 00:19:28,590
doesn't match whatever your face looks

00:19:27,270 --> 00:19:31,080
like in a few seconds or a few minutes

00:19:28,590 --> 00:19:33,630
or whether you want to mess with facial

00:19:31,080 --> 00:19:35,100
recognition or you try to stop a system

00:19:33,630 --> 00:19:38,190
from recognizing that this is you

00:19:35,100 --> 00:19:39,659
specifically and the focus of the

00:19:38,190 --> 00:19:42,390
adversarial methods that I'm about to

00:19:39,659 --> 00:19:44,700
discuss is on the physical layer that is

00:19:42,390 --> 00:19:46,350
how the facial recognition systems

00:19:44,700 --> 00:19:48,510
can be attacked by a person standing in

00:19:46,350 --> 00:19:49,920
front of a camera if you do further

00:19:48,510 --> 00:19:51,450
reading on this topic you'll find that

00:19:49,920 --> 00:19:54,330
there are also attacks on the digital

00:19:51,450 --> 00:19:56,250
layer which is by altering recognition

00:19:54,330 --> 00:19:58,050
information in a database so that it

00:19:56,250 --> 00:20:00,420
presents false information when it's

00:19:58,050 --> 00:20:02,790
consulted for example and I find this

00:20:00,420 --> 00:20:05,520
part just as fascinating but I only have

00:20:02,790 --> 00:20:07,980
half an hour and you're all gonna want

00:20:05,520 --> 00:20:09,510
your dinner at some point if you also

00:20:07,980 --> 00:20:11,610
want to go off on a tangent about this

00:20:09,510 --> 00:20:13,770
there are also fields of study about

00:20:11,610 --> 00:20:15,750
other biometric recognition attacks like

00:20:13,770 --> 00:20:17,820
gait analysis iris scanning fingerprints

00:20:15,750 --> 00:20:20,610
spoofing etc I'd encourage you to look

00:20:17,820 --> 00:20:22,860
that up later with those caveats let's

00:20:20,610 --> 00:20:24,300
get invisible here are some of the

00:20:22,860 --> 00:20:30,930
things that people have tried for many

00:20:24,300 --> 00:20:33,120
years to avoid to avoid detection this

00:20:30,930 --> 00:20:34,790
is one of the most traditional facial

00:20:33,120 --> 00:20:37,710
recognition attack techniques of all

00:20:34,790 --> 00:20:39,600
obscuring the whole face or the majority

00:20:37,710 --> 00:20:41,970
of the face using a mask or balaclava

00:20:39,600 --> 00:20:47,550
this is a look mostly favored by hackers

00:20:41,970 --> 00:20:49,920
and stock photos but but some facial

00:20:47,550 --> 00:20:52,890
recognition systems can absolutely be

00:20:49,920 --> 00:20:54,390
fooled by masks even in 2019 you can

00:20:52,890 --> 00:20:55,980
still trick some of them with masks made

00:20:54,390 --> 00:20:59,010
out of printed pictures of your targets

00:20:55,980 --> 00:21:00,900
face the iPhone seems to be the gold

00:20:59,010 --> 00:21:02,580
standard for facial recognition security

00:21:00,900 --> 00:21:04,320
but Android which is still the most

00:21:02,580 --> 00:21:06,510
popular smartphone operating system in

00:21:04,320 --> 00:21:10,020
the world has a variety of problems with

00:21:06,510 --> 00:21:11,070
this so last month Forbes conducted an

00:21:10,020 --> 00:21:12,650
experiment where they were able to

00:21:11,070 --> 00:21:14,700
bypass the facial recognition

00:21:12,650 --> 00:21:17,070
authentication feature on many common

00:21:14,700 --> 00:21:19,560
Android phones by what's called a

00:21:17,070 --> 00:21:20,790
presentation attack which is whether

00:21:19,560 --> 00:21:23,070
device hour locks when it's shown a

00:21:20,790 --> 00:21:25,650
representation or an image of this

00:21:23,070 --> 00:21:29,730
person that it's expecting to see they

00:21:25,650 --> 00:21:32,160
did this for an LG g7 think think thank

00:21:29,730 --> 00:21:34,470
you I'm not sure if anyone has one you

00:21:32,160 --> 00:21:37,920
can correct me later a samsung s9 a

00:21:34,470 --> 00:21:41,160
Samsung Note 8 and a one plus six using

00:21:37,920 --> 00:21:43,200
a 3d printed head their article on these

00:21:41,160 --> 00:21:44,910
findings prompted every single one of

00:21:43,200 --> 00:21:46,200
these companies to release statement

00:21:44,910 --> 00:21:48,840
saying that biometric authentication

00:21:46,200 --> 00:21:51,570
wasn't really recommended as a strong

00:21:48,840 --> 00:21:52,980
smartphone security measure and that

00:21:51,570 --> 00:21:54,540
they encourage the use of pins or

00:21:52,980 --> 00:21:57,570
passphrases for better security

00:21:54,540 --> 00:21:58,740
in fact in May last year Mashable proved

00:21:57,570 --> 00:22:00,420
that the 1 plus 6

00:21:58,740 --> 00:22:01,890
smartphone face unlock function could be

00:22:00,420 --> 00:22:03,990
fooled by someone just holding up a

00:22:01,890 --> 00:22:06,929
picture of a printed out photograph of

00:22:03,990 --> 00:22:10,200
the target and if that's not enough in

00:22:06,929 --> 00:22:11,910
late 2017 Microsoft's Windows 10 hello

00:22:10,200 --> 00:22:14,429
face authentication system was

00:22:11,910 --> 00:22:17,160
discovered to be vulnerable to the exact

00:22:14,429 --> 00:22:18,990
same attack Microsoft has patched this

00:22:17,160 --> 00:22:20,429
so of course none of us have machines

00:22:18,990 --> 00:22:22,380
that are vulnerable to this now because

00:22:20,429 --> 00:22:28,140
we all run software updates as soon as

00:22:22,380 --> 00:22:29,580
they're available right yep good another

00:22:28,140 --> 00:22:31,710
traditional technique for avoiding

00:22:29,580 --> 00:22:33,590
recognition favored by many celebrities

00:22:31,710 --> 00:22:35,550
over the years is obscuring the eyes

00:22:33,590 --> 00:22:38,250
your eyes in the bridge of your nose

00:22:35,550 --> 00:22:40,170
provide a really rich collection of

00:22:38,250 --> 00:22:41,780
unique data points that are treated in

00:22:40,170 --> 00:22:43,980
the most traditional face maps

00:22:41,780 --> 00:22:45,690
sunglasses cover these things therefore

00:22:43,980 --> 00:22:47,850
making it kind of hard to identify you

00:22:45,690 --> 00:22:50,280
but this doesn't always fool the facial

00:22:47,850 --> 00:22:51,540
detection step again it really depends

00:22:50,280 --> 00:22:55,110
on the software that's running under the

00:22:51,540 --> 00:22:57,230
hood making weird faces is another way

00:22:55,110 --> 00:23:00,000
that facial recognition sometimes fooled

00:22:57,230 --> 00:23:01,140
some software can't recognize you if you

00:23:00,000 --> 00:23:03,360
aren't using a neutral expression

00:23:01,140 --> 00:23:05,520
Airport smart gates fall into this

00:23:03,360 --> 00:23:07,440
category and I get tripped up by it a

00:23:05,520 --> 00:23:08,970
lot because you know being from this

00:23:07,440 --> 00:23:10,500
part of the world means that I like a

00:23:08,970 --> 00:23:11,940
lot of you have to travel a very long

00:23:10,500 --> 00:23:14,220
way to get anywhere else

00:23:11,940 --> 00:23:15,960
and I definitely do not look the same as

00:23:14,220 --> 00:23:19,230
my passport picture after I've been in

00:23:15,960 --> 00:23:20,760
transit for 28 hours the number of

00:23:19,230 --> 00:23:24,980
airport smart gates that have failed to

00:23:20,760 --> 00:23:28,380
recognize me after that has been nonzero

00:23:24,980 --> 00:23:30,420
in the realm of specialized anonymity

00:23:28,380 --> 00:23:32,250
fashion we have the paparazzi scarf

00:23:30,420 --> 00:23:34,830
which was created by a guy called Seif

00:23:32,250 --> 00:23:37,380
Siddiqui this scarf project was inspired

00:23:34,830 --> 00:23:39,450
after a bicycle reflect a ruined flash

00:23:37,380 --> 00:23:40,890
photograph that he was taking and he

00:23:39,450 --> 00:23:43,650
realized it would be kind of convenient

00:23:40,890 --> 00:23:45,320
in some cases to want to ruin that

00:23:43,650 --> 00:23:47,970
particular photograph

00:23:45,320 --> 00:23:49,770
apparently Paris Hilton has been seen

00:23:47,970 --> 00:23:51,240
with one but in case you are thinking of

00:23:49,770 --> 00:23:53,309
getting one just be warned that the

00:23:51,240 --> 00:23:56,070
scarf in this picture will cost you 680

00:23:53,309 --> 00:23:59,150
New Zealand dollars which is why Paris

00:23:56,070 --> 00:24:01,740
Hilton has one and I don't

00:23:59,150 --> 00:24:04,740
but for everyday people who don't have

00:24:01,740 --> 00:24:06,120
680 bucks to throw away on one scarf you

00:24:04,740 --> 00:24:08,610
can get yourself one of these this is

00:24:06,120 --> 00:24:11,940
the paparazzi visor they became briefly

00:24:08,610 --> 00:24:14,100
popular in late 2017 it's advertised as

00:24:11,940 --> 00:24:16,830
a way to protect your face from the Sun

00:24:14,100 --> 00:24:20,100
and also from Annoying cameras and much

00:24:16,830 --> 00:24:21,720
like about clobbering most of your face

00:24:20,100 --> 00:24:25,560
although from all the reports I've heard

00:24:21,720 --> 00:24:27,120
it is also very annoying to wear but

00:24:25,560 --> 00:24:29,310
don't worry this is slightly less

00:24:27,120 --> 00:24:32,130
annoying adversarial fashion reflector

00:24:29,310 --> 00:24:34,640
calls these work along the same lines as

00:24:32,130 --> 00:24:36,510
the paparazzi scarf but for your face

00:24:34,640 --> 00:24:39,270
reflected cools the sunglasses with

00:24:36,510 --> 00:24:40,680
reflective decoration on them they work

00:24:39,270 --> 00:24:42,510
with visible light and with infrared

00:24:40,680 --> 00:24:44,430
light and they have the added benefit of

00:24:42,510 --> 00:24:53,100
being a little less fashionably jarring

00:24:44,430 --> 00:24:59,730
to wear than the paparazzi Visor I don't

00:24:53,100 --> 00:25:01,890
even do it once a while but any survey

00:24:59,730 --> 00:25:03,060
of facial recognition attacks would be

00:25:01,890 --> 00:25:05,490
incomplete if I don't mention Adam

00:25:03,060 --> 00:25:07,140
Harvey's work hobby is an artist who's

00:25:05,490 --> 00:25:09,810
based in Berlin he's mostly known for

00:25:07,140 --> 00:25:12,750
his CV Da'Ville work that's this is a

00:25:09,810 --> 00:25:14,040
computer vision attempt - at the it's a

00:25:12,750 --> 00:25:16,410
computer vision attempt at this old

00:25:14,040 --> 00:25:18,360
warfare tactic of dazzle camouflage so

00:25:16,410 --> 00:25:20,880
d2 for a quick history lesson because

00:25:18,360 --> 00:25:23,550
this is the top being given by me dazzle

00:25:20,880 --> 00:25:25,440
camouflage was an experimental form of

00:25:23,550 --> 00:25:27,960
painting used on ships in World War one

00:25:25,440 --> 00:25:30,630
it wasn't intended to conceal the ship

00:25:27,960 --> 00:25:32,520
at all it's very obviously there but it

00:25:30,630 --> 00:25:34,740
made it more difficult to estimate

00:25:32,520 --> 00:25:36,720
things about the ship like how fast it

00:25:34,740 --> 00:25:39,780
was going its distance from you which

00:25:36,720 --> 00:25:42,240
way it was facing and that way it made

00:25:39,780 --> 00:25:44,670
the ships harder for enemies to fire out

00:25:42,240 --> 00:25:46,230
or predict things about and the desert

00:25:44,670 --> 00:25:48,510
camouflage method achieved this as you

00:25:46,230 --> 00:25:51,210
can see here by using huge blocks of

00:25:48,510 --> 00:25:52,920
color in irregular patterns on ships to

00:25:51,210 --> 00:25:56,550
break up their outline and all their

00:25:52,920 --> 00:25:58,740
features in just the same way CV dazzle

00:25:56,550 --> 00:26:00,120
will not make you invisible to other

00:25:58,740 --> 00:26:01,590
human beings that certainly will not

00:26:00,120 --> 00:26:03,500
make you invisible unless you're at a

00:26:01,590 --> 00:26:06,150
sci-fi convention or at a fashion show

00:26:03,500 --> 00:26:08,160
but to computers it will make it a lot

00:26:06,150 --> 00:26:10,590
harder for some of them and particularly

00:26:08,160 --> 00:26:11,360
the more older ones to recognise your

00:26:10,590 --> 00:26:13,040
face

00:26:11,360 --> 00:26:14,810
and it does this more or less the same

00:26:13,040 --> 00:26:16,520
way they did it in World War one they

00:26:14,810 --> 00:26:17,960
break up a lot of the outlines of the

00:26:16,520 --> 00:26:19,910
things that the algorithms are designed

00:26:17,960 --> 00:26:21,820
to detect mostly by messing with the

00:26:19,910 --> 00:26:24,290
patterns of light and dark

00:26:21,820 --> 00:26:26,750
Harvey is also responsible for the hyper

00:26:24,290 --> 00:26:29,600
phase project which is focused on making

00:26:26,750 --> 00:26:30,980
the parts of the image around a face you

00:26:29,600 --> 00:26:33,140
know your atmospheric stuff more

00:26:30,980 --> 00:26:35,720
confusing for computers instead of just

00:26:33,140 --> 00:26:38,840
focusing on the face itself so patterns

00:26:35,720 --> 00:26:40,520
like this one aim to saturate the facial

00:26:38,840 --> 00:26:42,500
recognition algorithm my facial

00:26:40,520 --> 00:26:44,180
detection algorithm in particular with a

00:26:42,500 --> 00:26:46,580
bunch of patterns that it really wants

00:26:44,180 --> 00:26:48,410
to interpret its faces and therefore it

00:26:46,580 --> 00:26:50,030
will stop it from recognizing actual

00:26:48,410 --> 00:26:51,740
faces that are present in the same image

00:26:50,030 --> 00:26:53,960
so the idea is that you could get a

00:26:51,740 --> 00:26:55,490
shirt printed up like this and wear it

00:26:53,960 --> 00:26:59,560
down the street and cause account a

00:26:55,490 --> 00:26:59,560
bunch of computers to really freak out

00:27:00,370 --> 00:27:04,820
outside of the realms of art in fashion

00:27:02,750 --> 00:27:06,830
though university researchers have also

00:27:04,820 --> 00:27:09,020
been trying hard to work around facial

00:27:06,830 --> 00:27:11,060
recognition tech most of these methods

00:27:09,020 --> 00:27:12,800
are still pretty experimental but some

00:27:11,060 --> 00:27:14,830
of them seem to be effective and more of

00:27:12,800 --> 00:27:17,630
them being developed all the time

00:27:14,830 --> 00:27:19,820
something that I haven't seen done for

00:27:17,630 --> 00:27:21,470
facial recognition quite so much yet but

00:27:19,820 --> 00:27:23,090
which is really having an impact on

00:27:21,470 --> 00:27:26,570
other forms of machine learning based

00:27:23,090 --> 00:27:28,370
computer vision is stickers a research

00:27:26,570 --> 00:27:30,320
team at Google discovered that these

00:27:28,370 --> 00:27:31,730
trippy looking stickers will completely

00:27:30,320 --> 00:27:33,860
mess with a neural network trying to

00:27:31,730 --> 00:27:35,450
detect things in a photo in this case

00:27:33,860 --> 00:27:37,010
the presence of this sticker makes the

00:27:35,450 --> 00:27:43,490
neural network think that this banana is

00:27:37,010 --> 00:27:45,560
actually a toaster I have not seen this

00:27:43,490 --> 00:27:47,090
tried with human faces yet and it's

00:27:45,560 --> 00:27:48,410
definitely not going to stop facial

00:27:47,090 --> 00:27:50,360
recognition techniques that are bought

00:27:48,410 --> 00:27:52,100
built on more traditional ideas about

00:27:50,360 --> 00:27:55,280
patterns of light and dark and that kind

00:27:52,100 --> 00:27:56,660
of thing or a mesh creation by if anyone

00:27:55,280 --> 00:27:59,150
is in the mood to conduct some research

00:27:56,660 --> 00:28:01,460
on this there's a good chance that some

00:27:59,150 --> 00:28:05,090
kinds of experimental new neural network

00:28:01,460 --> 00:28:07,220
driven facial recognition tech

00:28:05,090 --> 00:28:08,690
and like several of the other options

00:28:07,220 --> 00:28:09,770
that we've discussed it is clearly

00:28:08,690 --> 00:28:11,570
something that will challenge ideas

00:28:09,770 --> 00:28:14,990
about fashion further and this can only

00:28:11,570 --> 00:28:17,120
be a good thing this is the one I'm most

00:28:14,990 --> 00:28:19,550
excited about the in a paper that was

00:28:17,120 --> 00:28:21,200
published in March last year some

00:28:19,550 --> 00:28:22,880
researchers from Fudan University and

00:28:21,200 --> 00:28:25,110
Shanghai experimented with the effects

00:28:22,880 --> 00:28:27,420
of infrared light on facial

00:28:25,110 --> 00:28:29,640
recognition and detection systems this

00:28:27,420 --> 00:28:31,799
team was able to demonstrate that by

00:28:29,640 --> 00:28:33,720
beaming patches of infrared light onto

00:28:31,799 --> 00:28:35,790
your face in different ways it can make

00:28:33,720 --> 00:28:37,920
some facial recognition systems actually

00:28:35,790 --> 00:28:39,809
think that you were someone else they

00:28:37,920 --> 00:28:41,040
successfully got one system to recognize

00:28:39,809 --> 00:28:42,809
a member of their team has four

00:28:41,040 --> 00:28:46,590
different people using this method

00:28:42,809 --> 00:28:47,790
including Moby for some reason they did

00:28:46,590 --> 00:28:49,140
this with a small array of infrared

00:28:47,790 --> 00:28:51,510
lights that are mounted on the brim of a

00:28:49,140 --> 00:28:53,010
cap as you can see here the lights were

00:28:51,510 --> 00:28:55,890
programmed to shine on the face of the

00:28:53,010 --> 00:28:57,390
person wearing the Hat and there would

00:28:55,890 --> 00:28:59,070
be in different patterns these patterns

00:28:57,390 --> 00:29:00,690
were enough to convince the Machine

00:28:59,070 --> 00:29:04,049
looking at the image that this person

00:29:00,690 --> 00:29:05,910
was a different person I really like

00:29:04,049 --> 00:29:07,470
this solution particularly because the

00:29:05,910 --> 00:29:09,419
cap can be programmed to do whatever you

00:29:07,470 --> 00:29:10,890
want the infrared light won't bother you

00:29:09,419 --> 00:29:12,600
that much and when you're done with

00:29:10,890 --> 00:29:14,520
messing with the facial recognition tech

00:29:12,600 --> 00:29:16,140
you can just kind of take it off and put

00:29:14,520 --> 00:29:18,210
it away instead of removing a lot of

00:29:16,140 --> 00:29:22,020
makeup or looking super suspicious with

00:29:18,210 --> 00:29:24,270
a visor down over your face academic

00:29:22,020 --> 00:29:26,130
research and fashion and art are all

00:29:24,270 --> 00:29:28,260
working together to come up with newer

00:29:26,130 --> 00:29:29,850
and stranger ways to defeat the newer

00:29:28,260 --> 00:29:32,370
and stranger algorithms that are being

00:29:29,850 --> 00:29:34,470
deployed but honestly the most effective

00:29:32,370 --> 00:29:36,809
way to combat facial recognition system

00:29:34,470 --> 00:29:39,179
this has nothing to do with facing the

00:29:36,809 --> 00:29:42,690
cameras at all and all to do with facing

00:29:39,179 --> 00:29:43,890
other people the fact that the gdpr is

00:29:42,690 --> 00:29:45,540
now in effect means that a lot of

00:29:43,890 --> 00:29:47,340
companies in Europe in particular as

00:29:45,540 --> 00:29:49,169
well as companies that do business in

00:29:47,340 --> 00:29:51,690
Europe are now forced to consider

00:29:49,169 --> 00:29:52,890
transparency of usage and informed

00:29:51,690 --> 00:29:55,770
consent when it comes to deploying

00:29:52,890 --> 00:29:57,840
facial recognition tech but even though

00:29:55,770 --> 00:29:59,640
the gdpr might make it more obvious to

00:29:57,840 --> 00:30:01,980
you that you are being surveilled it

00:29:59,640 --> 00:30:03,840
still doesn't do anything to guarantee

00:30:01,980 --> 00:30:07,169
that the underlying algorithms are based

00:30:03,840 --> 00:30:08,610
on anything that's an unbiased or that

00:30:07,169 --> 00:30:10,770
the accuracy of any of this tech has

00:30:08,610 --> 00:30:13,740
been independently audited does not do

00:30:10,770 --> 00:30:16,590
that there's also the fact that in a

00:30:13,740 --> 00:30:18,870
global economy non GDP are based tech

00:30:16,590 --> 00:30:20,610
will absolutely still be produced and

00:30:18,870 --> 00:30:22,559
sold and get used everywhere including

00:30:20,610 --> 00:30:26,100
in Europe by people who think that

00:30:22,559 --> 00:30:27,510
they're not going to get caught there

00:30:26,100 --> 00:30:29,010
was also the fact that the Australian

00:30:27,510 --> 00:30:31,919
government in between bouts of

00:30:29,010 --> 00:30:34,050
infighting is still considering things

00:30:31,919 --> 00:30:35,370
like the identity matching services bill

00:30:34,050 --> 00:30:37,440
which has been sitting before the House

00:30:35,370 --> 00:30:38,050
of Representatives since early last year

00:30:37,440 --> 00:30:42,010
and

00:30:38,050 --> 00:30:43,300
around 2:00 it's been busy apparently so

00:30:42,010 --> 00:30:47,500
this bill aims to draw together a

00:30:43,300 --> 00:30:48,520
database of all the people all the faces

00:30:47,500 --> 00:30:50,670
all the pictures of people from

00:30:48,520 --> 00:30:52,420
passports from driver's licenses from

00:30:50,670 --> 00:30:54,220
criminal records

00:30:52,420 --> 00:30:56,050
immigration data other documents and

00:30:54,220 --> 00:30:57,550
then to match that with information that

00:30:56,050 --> 00:31:00,640
the government has about our identities

00:30:57,550 --> 00:31:02,350
which is usually quite a lot then and it

00:31:00,640 --> 00:31:03,880
wants to give peated buttons Department

00:31:02,350 --> 00:31:06,730
of Home Affairs access to this whole

00:31:03,880 --> 00:31:08,740
capability and they literally call this

00:31:06,730 --> 00:31:11,230
tech the capability with a capital C

00:31:08,740 --> 00:31:14,190
which does not sound cartoonishly evil

00:31:11,230 --> 00:31:14,190
in the slightest

00:31:14,250 --> 00:31:19,300
then there's the assistance in access

00:31:16,510 --> 00:31:21,370
bill which Ben spoke about earlier so

00:31:19,300 --> 00:31:22,600
this legislation was brushed through and

00:31:21,370 --> 00:31:24,550
is completely reprehensible and

00:31:22,600 --> 00:31:26,380
unconsidered Manor last month it means

00:31:24,550 --> 00:31:28,120
that any Australian or anyone living in

00:31:26,380 --> 00:31:30,100
Australia can be asked by the Australian

00:31:28,120 --> 00:31:33,760
Government to create backdoors in almost

00:31:30,100 --> 00:31:35,740
any software or face jail time we really

00:31:33,760 --> 00:31:38,050
can't expect realistic algorithmic

00:31:35,740 --> 00:31:39,520
transparency or any kind of provable

00:31:38,050 --> 00:31:42,910
accuracy in facial recognition

00:31:39,520 --> 00:31:46,090
technology any line of code to

00:31:42,910 --> 00:31:47,200
arbitrarily be at state secret and for

00:31:46,090 --> 00:31:49,330
rogue versions of facial recognition

00:31:47,200 --> 00:31:52,870
technology to be deployed to random

00:31:49,330 --> 00:31:54,700
devices all over the place but even

00:31:52,870 --> 00:31:56,980
though algorithmic transparency is still

00:31:54,700 --> 00:31:59,260
a complete pipe dream striving for it

00:31:56,980 --> 00:32:01,270
matters now more than ever in a world

00:31:59,260 --> 00:32:03,400
where we have the assistance in access

00:32:01,270 --> 00:32:07,000
bill Donald Trump is President of the

00:32:03,400 --> 00:32:09,580
United States every everyday things just

00:32:07,000 --> 00:32:12,220
seem to get weirder and worse for

00:32:09,580 --> 00:32:14,110
basically everybody we need to continue

00:32:12,220 --> 00:32:16,000
to ask questions and to talk to each

00:32:14,110 --> 00:32:18,910
other and to be alert to the uses and

00:32:16,000 --> 00:32:20,920
abuses of biased attack we need to

00:32:18,910 --> 00:32:22,210
challenge the fancy statistics where we

00:32:20,920 --> 00:32:24,700
see when it comes to facial recognition

00:32:22,210 --> 00:32:27,490
accuracy and ask ourselves if what we're

00:32:24,700 --> 00:32:30,340
hearing is marketing or independently

00:32:27,490 --> 00:32:33,160
verified fact because right now most of

00:32:30,340 --> 00:32:34,210
it definitely isn't unbiased and most of

00:32:33,160 --> 00:32:37,750
it definitely is inaccurate for

00:32:34,210 --> 00:32:39,490
everybody it's also almost impossible to

00:32:37,750 --> 00:32:41,680
know which facial recognition systems

00:32:39,490 --> 00:32:43,690
can be trusted to be socially and

00:32:41,680 --> 00:32:47,860
ethically responsible my guess would be

00:32:43,690 --> 00:32:50,110
probably zero of them as a final note I

00:32:47,860 --> 00:32:51,830
am NOT here to make the case that facial

00:32:50,110 --> 00:32:54,080
recognition technology has

00:32:51,830 --> 00:32:55,820
place in the world there are cases like

00:32:54,080 --> 00:32:58,669
an international border crossings where

00:32:55,820 --> 00:33:00,380
it can sometimes be useful and their

00:32:58,669 --> 00:33:02,960
cases like unlocking an iPhone where it

00:33:00,380 --> 00:33:04,669
can be really convenient but there are

00:33:02,960 --> 00:33:06,409
also cases where recognition is

00:33:04,669 --> 00:33:09,019
performed on us and used to draw

00:33:06,409 --> 00:33:10,909
conclusions about us that we currently

00:33:09,019 --> 00:33:12,769
don't have any real ways and

00:33:10,909 --> 00:33:15,799
meaningfully interrogate and that we

00:33:12,769 --> 00:33:18,620
can't necessarily trust and I'm here to

00:33:15,799 --> 00:33:20,809
urge caution in where these things have

00:33:18,620 --> 00:33:24,470
deployed and urge for the creation of

00:33:20,809 --> 00:33:26,690
more skepticism and more openness better

00:33:24,470 --> 00:33:28,639
datasets and it clearer ways to know

00:33:26,690 --> 00:33:31,340
whose technology is being used under the

00:33:28,639 --> 00:33:33,529
hood and if we're all aware that facial

00:33:31,340 --> 00:33:36,320
recognition is going to be invisibly

00:33:33,529 --> 00:33:38,480
performed on us in the absence of change

00:33:36,320 --> 00:33:39,980
from the top down I would encourage you

00:33:38,480 --> 00:33:45,909
all to think about how you might become

00:33:39,980 --> 00:33:45,909
invisible to it in response thank you

00:33:51,650 --> 00:33:53,710

YouTube URL: https://www.youtube.com/watch?v=LOulCAz4S0M


