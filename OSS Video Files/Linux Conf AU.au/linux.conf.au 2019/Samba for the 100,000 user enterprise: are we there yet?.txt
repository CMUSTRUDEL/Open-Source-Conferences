Title: Samba for the 100,000 user enterprise: are we there yet?
Publication date: 2020-01-09
Playlist: linux.conf.au 2019
Description: 
	Andrew Bartlett

https://2019.linux.conf.au/schedule/presentation/204/

For a number of years Andrew Bartlett has given presentations along the lines of 'the road to 100k users', and this so in part, this is another update on that long road.  

But it is also a milestone, because in the past year Samba has successfully added 124,000 users to the DB in a 4-hour benchmark, re-targeted to LMDB and successfully stood up a second replicated domain controller at the 100,000 user scale. 

Beyond scale, Andrew Bartlett will also update the audience on the many other new features coming up in Samba 4.10.

linux.conf.au is a conference about the Linux operating system, and all aspects of the thriving ecosystem of Free and Open Source Software that has grown up around it. Run since 1999, in a different Australian or New Zealand city each year, by a team of local volunteers, LCA invites more than 500 people to learn from the people who shape the future of Open Source. For more information on the conference see https://linux.conf.au/

#linux.conf.au #linux #foss #opensource
Captions: 
	00:00:02,000 --> 00:00:09,540
okay we now have two talks about samba

00:00:06,839 --> 00:00:20,300
back to back the first by one of the

00:00:09,540 --> 00:00:23,100
Samba core developers and remodelers so

00:00:20,300 --> 00:00:25,260
I've been talking many many years about

00:00:23,100 --> 00:00:27,720
Samba at the at the scale of 100,000

00:00:25,260 --> 00:00:29,730
user enterprise something a scale much

00:00:27,720 --> 00:00:32,130
larger than I ever expected to be say

00:00:29,730 --> 00:00:34,590
Brady to be PC nap but we really are

00:00:32,130 --> 00:00:35,670
making some good progress so I'm

00:00:34,590 --> 00:00:37,200
wondering are we there yet

00:00:35,670 --> 00:00:39,809
this is the team that I work with a

00:00:37,200 --> 00:00:42,690
catalyst catalyst has really supported

00:00:39,809 --> 00:00:45,149
this effort really well and we've got

00:00:42,690 --> 00:00:46,739
five seventeen members now our catalyst

00:00:45,149 --> 00:00:50,010
and I've got a team of eight including

00:00:46,739 --> 00:00:52,680
Alessandra my p.m. so we work out of

00:00:50,010 --> 00:00:54,510
Wellington and most of the work I'm

00:00:52,680 --> 00:00:57,870
talking about here is stuff that we've

00:00:54,510 --> 00:01:01,649
actually spearheaded as part of the same

00:00:57,870 --> 00:01:04,710
a team first then we'll talk about some

00:01:01,649 --> 00:01:07,170
of their other achievements I could talk

00:01:04,710 --> 00:01:08,909
a whole slide deck on moving to get laid

00:01:07,170 --> 00:01:10,140
but I've just got one slide on this so

00:01:08,909 --> 00:01:13,590
we've managed to move some of the same

00:01:10,140 --> 00:01:15,299
project to get lab we had tried to use

00:01:13,590 --> 00:01:17,700
github as a way of improving contributor

00:01:15,299 --> 00:01:20,369
engagement we hoped it would maybe have

00:01:17,700 --> 00:01:22,590
sis admins or our other users step up a

00:01:20,369 --> 00:01:24,840
little bit to doing a bit of working

00:01:22,590 --> 00:01:26,310
with Samba but turned out the Samba team

00:01:24,840 --> 00:01:28,710
members wouldn't use it and then it

00:01:26,310 --> 00:01:30,750
became the platform that people were

00:01:28,710 --> 00:01:34,610
told to use but God ignore it on that

00:01:30,750 --> 00:01:37,470
wasn't very good so get lab is open core

00:01:34,610 --> 00:01:39,570
so we use kilo lab calm as a hosted

00:01:37,470 --> 00:01:42,060
solution for that and we couldn't export

00:01:39,570 --> 00:01:43,259
the open source version if we want but

00:01:42,060 --> 00:01:46,110
we actually have up take the Samba team

00:01:43,259 --> 00:01:49,619
is using it which in particularly using

00:01:46,110 --> 00:01:51,390
it for get lab CI so samba school test

00:01:49,619 --> 00:01:55,079
which is run in parallel on the get lab

00:01:51,390 --> 00:01:57,570
CI infrastructure we've got get lab comm

00:01:55,079 --> 00:02:00,180
provides free CI services and then the

00:01:57,570 --> 00:02:02,009
Samba team run some services on rec

00:02:00,180 --> 00:02:04,110
space with a credit they generously

00:02:02,009 --> 00:02:07,530
provide that's the eye that allows our

00:02:04,110 --> 00:02:09,599
developers to help us produce software

00:02:07,530 --> 00:02:11,550
faster with better turnarounds and less

00:02:09,599 --> 00:02:14,010
put your patch in it failed the test or

00:02:11,550 --> 00:02:15,750
you know go back try again

00:02:14,010 --> 00:02:17,700
and that makes us fast with getting a

00:02:15,750 --> 00:02:19,080
development which helps us then spend

00:02:17,700 --> 00:02:22,290
more of our time trying to move on

00:02:19,080 --> 00:02:23,610
important things like scale but we still

00:02:22,290 --> 00:02:24,930
do all of our final build a merge on

00:02:23,610 --> 00:02:27,510
same draw card we because we're a

00:02:24,930 --> 00:02:29,190
project that started 92 we are still

00:02:27,510 --> 00:02:30,170
pretty old and conservative about how we

00:02:29,190 --> 00:02:32,550
do things

00:02:30,170 --> 00:02:33,750
another new thing that we've achieved in

00:02:32,550 --> 00:02:36,870
the last little while and sambar is

00:02:33,750 --> 00:02:39,420
moving to Python 3 so Samba 4.10 which

00:02:36,870 --> 00:02:41,880
is due out in March will support Python

00:02:39,420 --> 00:02:43,680
3 by default the build system we'd be

00:02:41,880 --> 00:02:46,230
Python 3 the Baha'i some bindings

00:02:43,680 --> 00:02:47,760
generated or or python 3 the previous

00:02:46,230 --> 00:02:49,700
versions had partial support the number

00:02:47,760 --> 00:02:53,040
of the bindings could be generated but

00:02:49,700 --> 00:02:54,480
the whole build system and the complete

00:02:53,040 --> 00:02:55,890
actual running of actually director you

00:02:54,480 --> 00:02:58,500
could domain controller and path of 3

00:02:55,890 --> 00:03:00,090
wasn't available so that's changed real

00:02:58,500 --> 00:03:01,620
light will apparently have Python 3 in

00:03:00,090 --> 00:03:04,560
if they ever get it out the door

00:03:01,620 --> 00:03:06,690
but rel 7 only as Python to see sees in

00:03:04,560 --> 00:03:11,910
this flagged a world that I just hate

00:03:06,690 --> 00:03:14,280
and Red Hat why did you do this to us so

00:03:11,910 --> 00:03:16,890
we're still going to build with Python 2

00:03:14,280 --> 00:03:18,569
7 for a pure pile server we're not going

00:03:16,890 --> 00:03:21,329
to get rid of that very soon because we

00:03:18,569 --> 00:03:23,430
know that it's really hard to get modern

00:03:21,329 --> 00:03:24,540
software on all these enterprise pieces

00:03:23,430 --> 00:03:27,209
of equipment that people in system

00:03:24,540 --> 00:03:28,560
running modern same run anyway it's a

00:03:27,209 --> 00:03:31,230
requirement so we are going to keep that

00:03:28,560 --> 00:03:33,959
going but the and we're going to have a

00:03:31,230 --> 00:03:37,139
dual release the next release ember 14

00:03:33,959 --> 00:03:38,760
will still work on python 2 and Python 3

00:03:37,139 --> 00:03:40,709
but we're going to be getting rid of the

00:03:38,760 --> 00:03:44,430
Python 2 support as soon as I can get

00:03:40,709 --> 00:03:45,630
some consensus but here's what I was

00:03:44,430 --> 00:03:49,829
really here to talk about which is the

00:03:45,630 --> 00:03:51,359
goal of having 100,000 users 120 120

00:03:49,829 --> 00:03:53,310
thousand users hundred thousand

00:03:51,359 --> 00:03:55,440
computers the whole scales we're talking

00:03:53,310 --> 00:03:59,430
about this so 300,000 user like objects

00:03:55,440 --> 00:04:02,400
in the domain but also we're looking for

00:03:59,430 --> 00:04:03,900
smaller scales that people send in quite

00:04:02,400 --> 00:04:07,069
casually deploy a same with the main on

00:04:03,900 --> 00:04:09,239
50,000 users that are telling University

00:04:07,069 --> 00:04:10,859
when I've heard of this I thought you

00:04:09,239 --> 00:04:13,889
have no idea how badly that would have

00:04:10,859 --> 00:04:15,630
worked two years ago but while some

00:04:13,889 --> 00:04:16,979
we're working on other things scale and

00:04:15,630 --> 00:04:18,660
you know being ready for the hundred

00:04:16,979 --> 00:04:22,860
thousand user enterprise is far more

00:04:18,660 --> 00:04:24,720
than just does it work with this many

00:04:22,860 --> 00:04:27,360
users there are things that matter at

00:04:24,720 --> 00:04:27,900
that scale you really need decent audit

00:04:27,360 --> 00:04:30,180
logging

00:04:27,900 --> 00:04:31,590
you really need decent auditing of

00:04:30,180 --> 00:04:32,790
changes of that scale because you're not

00:04:31,590 --> 00:04:38,039
going to notice them sort of

00:04:32,790 --> 00:04:40,139
individually so we've done a number of

00:04:38,039 --> 00:04:41,880
things I've told this slide has been up

00:04:40,139 --> 00:04:43,380
for a few years in the as I've given

00:04:41,880 --> 00:04:46,169
this talk about how we've improved the

00:04:43,380 --> 00:04:48,990
connection processing in Samba so we've

00:04:46,169 --> 00:04:50,340
moved from a number of our components

00:04:48,990 --> 00:04:53,789
being locked into running in a single

00:04:50,340 --> 00:04:55,979
process to being setting up a forking

00:04:53,789 --> 00:04:59,820
model a bit like the old Apache forking

00:04:55,979 --> 00:05:01,740
model which I then also preformed model

00:04:59,820 --> 00:05:04,860
names are similarly stolen from from the

00:05:01,740 --> 00:05:07,080
Apache Price project for handling our

00:05:04,860 --> 00:05:09,180
LDAP server and now we're also managing

00:05:07,080 --> 00:05:11,250
the important net logon server which is

00:05:09,180 --> 00:05:12,750
used for until my authentication of

00:05:11,250 --> 00:05:14,430
which there's still too much on me

00:05:12,750 --> 00:05:16,560
networks as well as KDC which is the

00:05:14,430 --> 00:05:18,330
Kerberos key distribution center so

00:05:16,560 --> 00:05:19,949
we're improving the parts of Samba that

00:05:18,330 --> 00:05:21,479
we know are bottlenecks we're allowing

00:05:19,949 --> 00:05:22,949
them to run in multiple processes or

00:05:21,479 --> 00:05:26,220
there's better scaling across the CPUs

00:05:22,949 --> 00:05:28,169
on the domain controllers we've also

00:05:26,220 --> 00:05:30,710
introduced some process limits for the

00:05:28,169 --> 00:05:33,240
standard per connection football model

00:05:30,710 --> 00:05:37,710
try and provide some little bit of

00:05:33,240 --> 00:05:39,330
control over how well that explodes I'm

00:05:37,710 --> 00:05:40,349
moving fairly fast because I love slides

00:05:39,330 --> 00:05:42,300
and there's also a lot of things that

00:05:40,349 --> 00:05:44,250
we've actually moved through I would

00:05:42,300 --> 00:05:47,250
like some questions at the end or or

00:05:44,250 --> 00:05:49,380
ever as we go though we also done audit

00:05:47,250 --> 00:05:50,880
logging so as I say Auto logging starts

00:05:49,380 --> 00:05:52,470
to matter when you're at this scale and

00:05:50,880 --> 00:05:54,270
we've had people say that the best

00:05:52,470 --> 00:05:56,310
format for us to ingest those audit logs

00:05:54,270 --> 00:05:57,539
is not human readable bits of text that

00:05:56,310 --> 00:05:58,710
then have to be parsed out with the

00:05:57,539 --> 00:06:01,530
world's most horrible regular

00:05:58,710 --> 00:06:03,570
expressions but actually JSON I got trot

00:06:01,530 --> 00:06:04,979
we had some with one of our lovely Samba

00:06:03,570 --> 00:06:06,599
team members troll us on the mailing

00:06:04,979 --> 00:06:08,460
list and translate we are we moving to

00:06:06,599 --> 00:06:10,520
JavaScript what's all this - no it's

00:06:08,460 --> 00:06:14,400
just a really good well accepted

00:06:10,520 --> 00:06:15,479
representation for logs and we probably

00:06:14,400 --> 00:06:17,970
shouldn't have even bothered with the

00:06:15,479 --> 00:06:19,979
human readable form because no humans

00:06:17,970 --> 00:06:21,870
were actually reading it and the JSON

00:06:19,979 --> 00:06:24,900
actually was you know it could be could

00:06:21,870 --> 00:06:26,370
be actually parse fairly well and you

00:06:24,900 --> 00:06:28,520
know we doubled the amount of effort we

00:06:26,370 --> 00:06:31,250
had to put into the into the logging

00:06:28,520 --> 00:06:33,720
once I had fine-grained password policy

00:06:31,250 --> 00:06:35,070
without users to have different policies

00:06:33,720 --> 00:06:36,930
previous is one policy for the whole

00:06:35,070 --> 00:06:38,400
domain and now we can go and say

00:06:36,930 --> 00:06:39,690
administrators have these policies or

00:06:38,400 --> 00:06:41,700
this particular account has a lacks

00:06:39,690 --> 00:06:43,140
password policy perhaps it's a service

00:06:41,700 --> 00:06:46,380
account that's not really a service

00:06:43,140 --> 00:06:49,380
account things like that that need you

00:06:46,380 --> 00:06:50,760
know maybe you've got a desk a Welcome

00:06:49,380 --> 00:06:52,890
Desk account or something that needs to

00:06:50,760 --> 00:06:57,360
have a well known password so this one

00:06:52,890 --> 00:06:58,980
else asked to basically worked to a

00:06:57,360 --> 00:07:02,750
larger organization where it's not just

00:06:58,980 --> 00:07:06,120
one size fits all now um

00:07:02,750 --> 00:07:07,740
any of you do backups these backups

00:07:06,120 --> 00:07:12,600
these backups are part of your part of

00:07:07,740 --> 00:07:15,300
your job yeah so how many of you backup

00:07:12,600 --> 00:07:20,280
your database instances by just tearing

00:07:15,300 --> 00:07:22,590
the database server yeah so yeah not so

00:07:20,280 --> 00:07:24,450
not so good so we did have a shell

00:07:22,590 --> 00:07:26,460
script which did attempt to lock the

00:07:24,450 --> 00:07:29,820
database but as the davis is in four

00:07:26,460 --> 00:07:32,970
files it locked them one second third

00:07:29,820 --> 00:07:34,290
and so it was knocking and consistent

00:07:32,970 --> 00:07:35,820
snapshots so we've managing to make to

00:07:34,290 --> 00:07:37,530
take a consistent snapshot of the whole

00:07:35,820 --> 00:07:40,200
database by locking the whole database

00:07:37,530 --> 00:07:42,510
and then reading it let's have an online

00:07:40,200 --> 00:07:44,550
backup so near Samba when it's an active

00:07:42,510 --> 00:07:46,230
directory DC it has a replication

00:07:44,550 --> 00:07:47,970
protocol between their main controller

00:07:46,230 --> 00:07:50,760
so we use that protocol to do the backup

00:07:47,970 --> 00:07:52,590
that allows us to download the whole

00:07:50,760 --> 00:07:55,500
database in a form that's less likely to

00:07:52,590 --> 00:07:57,870
be corrupt because the replication

00:07:55,500 --> 00:07:59,880
protocol by the process of pulling it

00:07:57,870 --> 00:08:01,860
out of the records and doing it tends to

00:07:59,880 --> 00:08:03,570
get a nice of you of the database than

00:08:01,860 --> 00:08:04,770
just raw records so we have two

00:08:03,570 --> 00:08:07,830
different ways of back in the database

00:08:04,770 --> 00:08:10,290
up allow us to make sure that we get

00:08:07,830 --> 00:08:11,910
either a sort of sanitized copy or the

00:08:10,290 --> 00:08:13,260
forensic copy of what was on your disk

00:08:11,910 --> 00:08:14,460
which maybe will be very useful for

00:08:13,260 --> 00:08:17,190
trying to work out what went really

00:08:14,460 --> 00:08:19,470
wrong we also have a restore tool

00:08:17,190 --> 00:08:24,930
because anyone actually restore from

00:08:19,470 --> 00:08:27,330
backups so this we do recommend that if

00:08:24,930 --> 00:08:29,700
you do have a partial Samba domain that

00:08:27,330 --> 00:08:31,380
if you want to restore that you just go

00:08:29,700 --> 00:08:33,360
and say okay turn off the things they're

00:08:31,380 --> 00:08:35,400
broken start named domain controllers

00:08:33,360 --> 00:08:37,800
from the working once that's all good

00:08:35,400 --> 00:08:39,810
but if you're down to nothing then we

00:08:37,800 --> 00:08:40,830
have an authority restore it removes all

00:08:39,810 --> 00:08:43,470
the records the other domain controllers

00:08:40,830 --> 00:08:46,650
and says this is the only DC that has

00:08:43,470 --> 00:08:48,990
ever existed and I'm starting from here

00:08:46,650 --> 00:08:51,240
this allows you to then rebuild the

00:08:48,990 --> 00:08:53,070
network without oh it's looking for

00:08:51,240 --> 00:08:54,660
these dead things that were removed but

00:08:53,070 --> 00:08:55,500
not removed it makes it much easier to

00:08:54,660 --> 00:08:58,530
rebuild it in

00:08:55,500 --> 00:08:59,760
catastrophic restore scenario and also

00:08:58,530 --> 00:09:01,380
make sure that those other domain

00:08:59,760 --> 00:09:03,690
controllers that might not have actually

00:09:01,380 --> 00:09:05,280
been powered off don't suddenly resync

00:09:03,690 --> 00:09:08,730
into the network and cause total

00:09:05,280 --> 00:09:09,960
replication chaos now the other thing

00:09:08,730 --> 00:09:11,220
that's really important if you're going

00:09:09,960 --> 00:09:14,490
to be working these scholars have got to

00:09:11,220 --> 00:09:16,530
create a lab domain now how many of you

00:09:14,490 --> 00:09:18,390
are really good I probably do Liam can

00:09:16,530 --> 00:09:20,670
manage it but anyone else can create a

00:09:18,390 --> 00:09:22,650
totally isolated level to network that

00:09:20,670 --> 00:09:24,900
can still get to the to the mirrors for

00:09:22,650 --> 00:09:26,280
installing the packages and you know and

00:09:24,900 --> 00:09:29,250
won't talk at all to the rest of your

00:09:26,280 --> 00:09:31,740
production network I'm not seeing very

00:09:29,250 --> 00:09:33,720
many hands so basically we had advised

00:09:31,740 --> 00:09:35,940
customers to do this and they always

00:09:33,720 --> 00:09:38,520
just they merged again and it was a

00:09:35,940 --> 00:09:40,440
disaster so what we do is we rename the

00:09:38,520 --> 00:09:41,970
domain in the same processor where we

00:09:40,440 --> 00:09:44,160
restore it and change the important keys

00:09:41,970 --> 00:09:46,080
if I can't talk we rename them so they

00:09:44,160 --> 00:09:47,580
can coexist on the network this will

00:09:46,080 --> 00:09:49,470
actually create a lab domain with all

00:09:47,580 --> 00:09:50,700
the users from production possibly the

00:09:49,470 --> 00:09:53,520
passwords or not depending on which

00:09:50,700 --> 00:09:55,410
options you said but not the name of

00:09:53,520 --> 00:09:57,000
production so then you can then go and

00:09:55,410 --> 00:10:01,380
test your things do your upgrades and

00:09:57,000 --> 00:10:03,030
then try it again in production what's

00:10:01,380 --> 00:10:04,680
it been working on ad operation at scale

00:10:03,030 --> 00:10:05,670
so we've got traffic replay tool I've

00:10:04,680 --> 00:10:07,589
talked about this a number of

00:10:05,670 --> 00:10:10,100
conferences in the past we can pre

00:10:07,589 --> 00:10:13,710
create realistic databases we can

00:10:10,100 --> 00:10:15,210
simulate to to get the traffic and or

00:10:13,710 --> 00:10:17,010
even ramp up the traffic based on

00:10:15,210 --> 00:10:20,070
recording traffic seen on your network

00:10:17,010 --> 00:10:22,560
and then replaying it this also has is

00:10:20,070 --> 00:10:24,060
also able to operate against windows so

00:10:22,560 --> 00:10:25,470
we're actually able to benchmark Samba

00:10:24,060 --> 00:10:27,089
on Windows with the same traffic load

00:10:25,470 --> 00:10:28,950
which is really interesting because we

00:10:27,089 --> 00:10:32,910
gain there it's not not as fast as

00:10:28,950 --> 00:10:34,230
Windows but it's it's not too bad once I

00:10:32,910 --> 00:10:35,610
managed to get replication and working

00:10:34,230 --> 00:10:37,050
at scale it turned out that filling the

00:10:35,610 --> 00:10:38,790
database was enough proof that we could

00:10:37,050 --> 00:10:40,920
operate at 100,000 users you couldn't

00:10:38,790 --> 00:10:43,950
replicate a second replica so it fixed

00:10:40,920 --> 00:10:45,630
that little things but basically by stir

00:10:43,950 --> 00:10:47,190
by actually drawing and trying at this

00:10:45,630 --> 00:10:48,240
scale we are finding the bottlenecks and

00:10:47,190 --> 00:10:49,790
finding that they're actually quite

00:10:48,240 --> 00:10:52,170
fixable

00:10:49,790 --> 00:10:53,400
another issue at scale is into forest

00:10:52,170 --> 00:10:55,710
Trust's we want to be able to have

00:10:53,400 --> 00:10:57,030
multiple domains so this is an area that

00:10:55,710 --> 00:10:58,500
Stefan Mehta marker has been working on

00:10:57,030 --> 00:11:00,810
it's been possible for a little while to

00:10:58,500 --> 00:11:05,490
trust other forests it's still one

00:11:00,810 --> 00:11:07,320
domain perform a terribly much unless

00:11:05,490 --> 00:11:09,000
you're really versed in AD but these are

00:11:07,320 --> 00:11:11,460
limitations that sample works with

00:11:09,000 --> 00:11:12,810
but we can actually have decent trust

00:11:11,460 --> 00:11:14,550
between the forests that handle some

00:11:12,810 --> 00:11:16,440
group membership crossing them where

00:11:14,550 --> 00:11:21,240
were each step we make this work a

00:11:16,440 --> 00:11:22,980
little bit more we've also got

00:11:21,240 --> 00:11:24,450
replication Diagnostics because at this

00:11:22,980 --> 00:11:26,370
kind of scale you need to be able to

00:11:24,450 --> 00:11:28,200
know where things are failing with your

00:11:26,370 --> 00:11:30,150
replication we've got human readable

00:11:28,200 --> 00:11:31,740
text we've got real inspiration from the

00:11:30,150 --> 00:11:35,970
safe project from last year's talk about

00:11:31,740 --> 00:11:39,300
what tools saying how your project is

00:11:35,970 --> 00:11:41,070
working should look like and then JSON

00:11:39,300 --> 00:11:44,910
representation for ingesting into log

00:11:41,070 --> 00:11:46,560
tools we've improved Sambor tool so that

00:11:44,910 --> 00:11:48,720
we've got new sub commands for arcs for

00:11:46,560 --> 00:11:51,420
organizational unit management computer

00:11:48,720 --> 00:11:54,240
management and D and improve the DNS

00:11:51,420 --> 00:11:55,590
handling there's an as we found the

00:11:54,240 --> 00:11:57,990
number of group memberships and the

00:11:55,590 --> 00:11:59,310
distribution of groups like I have 110

00:11:57,990 --> 00:12:00,900
thousand user group was a really

00:11:59,310 --> 00:12:03,480
important property for working out the

00:12:00,900 --> 00:12:05,460
scalability factor of a Samba in the

00:12:03,480 --> 00:12:07,860
large domain we now produce a tool that

00:12:05,460 --> 00:12:09,900
will tell us in a simple summary what

00:12:07,860 --> 00:12:11,220
your domain looks like because we were

00:12:09,900 --> 00:12:13,470
just guessing and our customers couldn't

00:12:11,220 --> 00:12:16,700
even tell us sort of what they'd look

00:12:13,470 --> 00:12:16,700
like in terms of the shape of that curve

00:12:17,990 --> 00:12:21,600
but here comes the biggest thing that

00:12:20,130 --> 00:12:23,280
we've done towards scalability is we've

00:12:21,600 --> 00:12:26,220
moved from a 32-bit database to a new

00:12:23,280 --> 00:12:28,080
64-bit database we're concerned that for

00:12:26,220 --> 00:12:30,660
gear by database just would be filled

00:12:28,080 --> 00:12:32,190
too quickly they were all that we'd be

00:12:30,660 --> 00:12:34,740
seeing all the time repacking to get us

00:12:32,190 --> 00:12:37,680
back under to gig and have enough space

00:12:34,740 --> 00:12:39,270
for a transaction so none of our

00:12:37,680 --> 00:12:41,460
customers wanted to hit the hard limit

00:12:39,270 --> 00:12:43,770
of full gig and then start development

00:12:41,460 --> 00:12:46,170
so we chose emt-b it's used by open LDAP

00:12:43,770 --> 00:12:48,300
and pretty much did what it said on the

00:12:46,170 --> 00:12:50,220
team it taught us a lot more about our

00:12:48,300 --> 00:12:51,839
locking than actually problems in LM DB

00:12:50,220 --> 00:12:53,339
so we're actually really happy so you

00:12:51,839 --> 00:12:55,200
found and fixed a number of locking

00:12:53,339 --> 00:12:58,920
issues by introducing a new key value

00:12:55,200 --> 00:13:01,140
layer into samba previously we had the

00:12:58,920 --> 00:13:02,730
idea of a different back-end but it

00:13:01,140 --> 00:13:04,860
would take LDAP like structures so we

00:13:02,730 --> 00:13:06,900
ended up adding another layer in there

00:13:04,860 --> 00:13:08,430
that was key value structures and then

00:13:06,900 --> 00:13:11,190
said tell you basically value database

00:13:08,430 --> 00:13:12,600
lndhu is key value database and then put

00:13:11,190 --> 00:13:14,010
that extra layer in actually work pretty

00:13:12,600 --> 00:13:15,570
well with a lot of tests to lock down

00:13:14,010 --> 00:13:18,750
the semantics we actually got something

00:13:15,570 --> 00:13:21,900
which were pretty happy with until we

00:13:18,750 --> 00:13:22,830
hit locking even the prototypes found

00:13:21,900 --> 00:13:24,480
that we were missing locking

00:13:22,830 --> 00:13:28,890
now some of this is we fix quite a while

00:13:24,480 --> 00:13:31,950
ago for 747s now 18 months old but we

00:13:28,890 --> 00:13:33,750
hit more locking we had issues where we

00:13:31,950 --> 00:13:35,040
found out we were unlocking databases in

00:13:33,750 --> 00:13:36,330
the wrong order and so you could see the

00:13:35,040 --> 00:13:39,450
sequence number before you could see the

00:13:36,330 --> 00:13:41,220
data which then meant that you then told

00:13:39,450 --> 00:13:43,970
everyone about the new change the data

00:13:41,220 --> 00:13:45,240
wasn't visible yeah not good not good

00:13:43,970 --> 00:13:48,660
anyway

00:13:45,240 --> 00:13:50,220
we've found the fix those and then we

00:13:48,660 --> 00:13:51,110
hit issues around maximum key size we

00:13:50,220 --> 00:13:54,180
knew this one was coming

00:13:51,110 --> 00:13:55,230
LM DB has a key size limit of 511 bytes

00:13:54,180 --> 00:13:57,390
but we were putting our full

00:13:55,230 --> 00:13:59,070
distinguished name in now distinguished

00:13:57,390 --> 00:14:00,329
names most of them would never be that

00:13:59,070 --> 00:14:03,390
large but we know someone's going to try

00:14:00,329 --> 00:14:05,070
and stretch the limits so I riri work

00:14:03,390 --> 00:14:06,810
the database to instead use a good word

00:14:05,070 --> 00:14:10,500
for the key and then have a separate D

00:14:06,810 --> 00:14:12,750
into key lookup and then we looked at

00:14:10,500 --> 00:14:14,430
our indexes and said that's going to be

00:14:12,750 --> 00:14:16,200
awkward there can be very very long but

00:14:14,430 --> 00:14:18,120
actually why don't we just truncate them

00:14:16,200 --> 00:14:19,709
usually it'll match and the rest of time

00:14:18,120 --> 00:14:22,860
will just scan it seems to work well

00:14:19,709 --> 00:14:24,779
enough we've been measuring performance

00:14:22,860 --> 00:14:26,459
actually on an old AMD Athlon because

00:14:24,779 --> 00:14:27,930
bit like the free software mera group

00:14:26,459 --> 00:14:31,589
the way you get hardware at catalyst did

00:14:27,930 --> 00:14:33,180
you get the castoffs but also traffic

00:14:31,589 --> 00:14:35,459
replaying the cloud and adding users

00:14:33,180 --> 00:14:38,610
groups in my workstation so we went

00:14:35,459 --> 00:14:39,480
around the petrovic replay ah super

00:14:38,610 --> 00:14:41,640
simple form it's lost

00:14:39,480 --> 00:14:43,320
oh no but actually we found that

00:14:41,640 --> 00:14:44,399
basically we had an interception layer

00:14:43,320 --> 00:14:46,230
that was meant to be handling network

00:14:44,399 --> 00:14:48,240
traffic and also was handling all the

00:14:46,230 --> 00:14:49,740
right calls to the database and just

00:14:48,240 --> 00:14:51,450
looking up the link list to work out is

00:14:49,740 --> 00:14:55,500
this a socket that's a network socket

00:14:51,450 --> 00:14:58,470
was taking time anyway baby our 10%

00:14:55,500 --> 00:15:00,570
slower we don't think it's we think only

00:14:58,470 --> 00:15:02,399
to be a scale better but you know

00:15:00,570 --> 00:15:05,940
there's TDP it's actually done good

00:15:02,399 --> 00:15:07,230
database after all these years we use

00:15:05,940 --> 00:15:10,500
the traffic replay and they came out

00:15:07,230 --> 00:15:11,699
about even and then we went and written

00:15:10,500 --> 00:15:13,050
this benchmark which I've been running

00:15:11,699 --> 00:15:14,970
for a while which is just sort of film

00:15:13,050 --> 00:15:16,649
the database how long how many users can

00:15:14,970 --> 00:15:25,170
you fill in and with all the F syncs

00:15:16,649 --> 00:15:28,230
turned off in half the wood we don't

00:15:25,170 --> 00:15:30,240
need this what ok but actually FC was

00:15:28,230 --> 00:15:33,949
still turned on so a few patches later

00:15:30,240 --> 00:15:37,250
we made it also honor they don't f sync

00:15:33,949 --> 00:15:39,510
and we base a pudding

00:15:37,250 --> 00:15:44,670
124,000 users in that benchmark sort of

00:15:39,510 --> 00:15:47,130
run so it's not really LMD bu is much

00:15:44,670 --> 00:15:48,660
much faster than this but actually what

00:15:47,130 --> 00:15:50,570
it's really showing us is that the

00:15:48,660 --> 00:15:52,680
common factor between it which is the

00:15:50,570 --> 00:15:55,920
module stack that sits above all is that

00:15:52,680 --> 00:15:57,540
still where all the slowness is so you

00:15:55,920 --> 00:15:59,190
know going to a lightning-fast database

00:15:57,540 --> 00:16:01,200
underneath doesn't give us you know ten

00:15:59,190 --> 00:16:03,840
times the performance it just makes

00:16:01,200 --> 00:16:05,880
things a little bit better so this is

00:16:03,840 --> 00:16:08,220
produced this scatter graph just to dry

00:16:05,880 --> 00:16:10,170
so the lines fan out because they go to

00:16:08,220 --> 00:16:13,290
what some of the users are in one two

00:16:10,170 --> 00:16:13,620
and three groups it's just but it's

00:16:13,290 --> 00:16:15,600
interesting

00:16:13,620 --> 00:16:17,340
it's a lot higher jumping in the latency

00:16:15,600 --> 00:16:18,360
if they'll only be on the right we don't

00:16:17,340 --> 00:16:19,920
really know what that's about it's

00:16:18,360 --> 00:16:21,330
presumably rebalancing the tree it's got

00:16:19,920 --> 00:16:24,300
a very different database structure the

00:16:21,330 --> 00:16:26,400
way TV works it's not so bad

00:16:24,300 --> 00:16:28,380
we address the customers need for scale

00:16:26,400 --> 00:16:29,370
we've set the database limit at six gig

00:16:28,380 --> 00:16:31,170
at the moment that's a compile time

00:16:29,370 --> 00:16:32,940
constant we don't have to it just

00:16:31,170 --> 00:16:34,560
changes how much virtual memory is being

00:16:32,940 --> 00:16:36,750
used people got really freaked out when

00:16:34,560 --> 00:16:40,080
they saw you know 100 gigabytes and more

00:16:36,750 --> 00:16:42,360
processors so I was trying to at the

00:16:40,080 --> 00:16:44,430
code in I can change a constant later

00:16:42,360 --> 00:16:46,560
with our people screaming but we open up

00:16:44,430 --> 00:16:48,960
new opportunities we can also start

00:16:46,560 --> 00:16:50,640
using Ellen DB sub databases which would

00:16:48,960 --> 00:16:51,930
be still into one giant lock which is

00:16:50,640 --> 00:16:54,900
actually what we need and would avoid

00:16:51,930 --> 00:16:56,730
our okay lock this lock these unlock

00:16:54,900 --> 00:16:59,400
them in the right order kind of game

00:16:56,730 --> 00:17:01,320
that we currently play and once the

00:16:59,400 --> 00:17:03,570
potentially L DB has a concept of an

00:17:01,320 --> 00:17:05,850
ordered walkthrough the keys and that

00:17:03,570 --> 00:17:08,730
could allow us to do range indexes which

00:17:05,850 --> 00:17:10,740
could be really useful saw some sharp

00:17:08,730 --> 00:17:13,829
edges LD be has different locking

00:17:10,740 --> 00:17:17,089
behavior there's no exclusive access

00:17:13,829 --> 00:17:17,089
so database operations

00:17:18,320 --> 00:17:21,980
can't we can't go and be put into

00:17:20,300 --> 00:17:25,100
maintenance mode you know we know that

00:17:21,980 --> 00:17:32,480
we're the only ones able to access it at

00:17:25,100 --> 00:17:34,550
all far as the sparse it's I don't

00:17:32,480 --> 00:17:36,590
really like these you know here's a file

00:17:34,550 --> 00:17:39,410
the maximum size it's ever going to be

00:17:36,590 --> 00:17:40,910
but it's not filled and so other things

00:17:39,410 --> 00:17:45,140
filling the disk and then make this

00:17:40,910 --> 00:17:46,880
really go badly TDB has always filled

00:17:45,140 --> 00:17:49,580
its files and administrators are used to

00:17:46,880 --> 00:17:50,360
that so I'm not quite sure how we're

00:17:49,580 --> 00:17:52,220
going to handle that

00:17:50,360 --> 00:17:54,170
I'm not sure whether it will really end

00:17:52,220 --> 00:17:55,940
up bothering our administrators but I've

00:17:54,170 --> 00:17:58,480
kind of like DF to represent how much

00:17:55,940 --> 00:18:00,800
time after we filled the database

00:17:58,480 --> 00:18:03,770
they're also not extended automatically

00:18:00,800 --> 00:18:05,600
which is they've gained different TTB so

00:18:03,770 --> 00:18:10,430
just we need some real-world experience

00:18:05,600 --> 00:18:12,620
from out from our deployments there's

00:18:10,430 --> 00:18:14,150
still stuff to do for support for a

00:18:12,620 --> 00:18:16,850
hundred thousand users is really tasks

00:18:14,150 --> 00:18:19,310
for the the September release of Samba

00:18:16,850 --> 00:18:20,750
we've got to improve subtree renames I

00:18:19,310 --> 00:18:23,180
mean one of our big clients is the

00:18:20,750 --> 00:18:25,000
French government and in my experience

00:18:23,180 --> 00:18:27,230
government organisations love

00:18:25,000 --> 00:18:30,860
reorganizing so I really like to be able

00:18:27,230 --> 00:18:32,330
to allow them to do that we're gonna

00:18:30,860 --> 00:18:34,010
change the pack format in the database

00:18:32,330 --> 00:18:35,120
to avoid reading data that won't be

00:18:34,010 --> 00:18:39,290
returned because that current pack

00:18:35,120 --> 00:18:41,270
format is a you know you actually end up

00:18:39,290 --> 00:18:43,070
have to read the whole amount of

00:18:41,270 --> 00:18:45,280
information about a user in order to

00:18:43,070 --> 00:18:47,120
actually read anything about the user

00:18:45,280 --> 00:18:50,000
and we're gonna improve the memory

00:18:47,120 --> 00:18:51,290
management so that we allocate less

00:18:50,000 --> 00:18:53,660
memory when we're returning data

00:18:51,290 --> 00:18:56,180
internally that may not when it's not

00:18:53,660 --> 00:18:56,710
required to be individually allocated so

00:18:56,180 --> 00:19:00,050
we're there yet

00:18:56,710 --> 00:19:01,760
probably I'm looking forward to the next

00:19:00,050 --> 00:19:03,170
two samba releases because we've got

00:19:01,760 --> 00:19:04,730
still got a good amount of our project

00:19:03,170 --> 00:19:05,450
to go with our customers are paying for

00:19:04,730 --> 00:19:06,890
this

00:19:05,450 --> 00:19:09,140
but real-world feedback could be really

00:19:06,890 --> 00:19:11,510
valuable so we really appreciate any

00:19:09,140 --> 00:19:12,980
feedback from any samba users no matter

00:19:11,510 --> 00:19:14,480
what the scale is because it just tells

00:19:12,980 --> 00:19:17,210
us things edge cases that we hadn't

00:19:14,480 --> 00:19:18,680
thought about finally please become an

00:19:17,210 --> 00:19:20,600
official Conservancy supporter the

00:19:18,680 --> 00:19:23,000
software freedom Conservancy keeps Samba

00:19:20,600 --> 00:19:27,650
with a legal home so they are really

00:19:23,000 --> 00:19:28,910
worth supporting and also this is

00:19:27,650 --> 00:19:30,680
catalyst these are some of the

00:19:28,910 --> 00:19:31,500
technologies and I know that even more

00:19:30,680 --> 00:19:35,520
because they might

00:19:31,500 --> 00:19:38,420
I told but in the last 10 seconds that I

00:19:35,520 --> 00:19:38,420
have are there any questions

00:19:45,580 --> 00:19:52,149
you take a couple of questions while

00:19:47,260 --> 00:19:55,360
Karen sits up high just a quick question

00:19:52,149 --> 00:19:56,980
we use CTD B for a load balancing is

00:19:55,360 --> 00:20:01,390
this gonna be transparent moving over to

00:19:56,980 --> 00:20:03,370
the new database or is it these changes

00:20:01,390 --> 00:20:05,080
are just across the the Active Directory

00:20:03,370 --> 00:20:06,820
side of the house thank you

00:20:05,080 --> 00:20:15,840
and thankfully doesn't the databases

00:20:06,820 --> 00:20:15,840
nearly these sighs more questions

00:20:24,500 --> 00:20:30,320
were you able to use any of the LLVM

00:20:26,800 --> 00:20:33,140
sanitizers else an and such to help you

00:20:30,320 --> 00:20:35,900
with the lock the locking issues or just

00:20:33,140 --> 00:20:40,460
different semantics and you couldn't we

00:20:35,900 --> 00:20:42,380
just we would diagnose them with mostly

00:20:40,460 --> 00:20:44,330
just picking back traces on the locks

00:20:42,380 --> 00:20:46,100
and chase them that way

00:20:44,330 --> 00:20:47,360
st. game figuring out which one was

00:20:46,100 --> 00:20:49,400
locked and then tracing all the

00:20:47,360 --> 00:20:51,890
processes there's some good guides on

00:20:49,400 --> 00:20:52,790
the web about some LS locks and things

00:20:51,890 --> 00:20:54,110
was very helpful

00:20:52,790 --> 00:20:56,540
but tell us who was waiting for a

00:20:54,110 --> 00:21:04,610
blocked lock and things so that's how we

00:20:56,540 --> 00:21:14,890
ended up doing it time for one more

00:21:04,610 --> 00:21:14,890

YouTube URL: https://www.youtube.com/watch?v=KEyzrnr2UhU


