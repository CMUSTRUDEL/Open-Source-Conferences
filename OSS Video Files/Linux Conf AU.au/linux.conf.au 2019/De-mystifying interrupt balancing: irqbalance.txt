Title: De-mystifying interrupt balancing: irqbalance
Publication date: 2020-01-09
Playlist: linux.conf.au 2019
Description: 
	PJ (Peter) Waskiewicz

https://2019.linux.conf.au/schedule/presentation/102/

irqbalance is a userspace daemon that has been available for a number of years.  Its main purpose is to make decisions on how to best balance interrupts within a system.  Getting this "right" can be a challenge, since it needs to run on enterprise-level systems all the way to a mobile device.  Taking things like NUMA locality, bus hierarchy, shared interrupts, cache locality, all into consideration, irqbalance is often misunderstood with how it works, what it can do, and what it cannot do.

As co-maintainer of irqbalance, this talk is intended to pull the curtain back on how this whole framework operates.  What pieces of the Linux kernel are involved in balancing interrupts, where policies are enforced, current best-known configurations, and future ideas to try and improve operation, are all things targeted with this talk.

linux.conf.au is a conference about the Linux operating system, and all aspects of the thriving ecosystem of Free and Open Source Software that has grown up around it. Run since 1999, in a different Australian or New Zealand city each year, by a team of local volunteers, LCA invites more than 500 people to learn from the people who shape the future of Open Source. For more information on the conference see https://linux.conf.au/

#linux.conf.au #linux #foss #opensource
Captions: 
	00:00:02,480 --> 00:00:12,719
hello everyone welcome to day three of

00:00:06,810 --> 00:00:16,350
Linux Kampf day you today the talk will

00:00:12,719 --> 00:00:19,650
be given by mr. Peter Boscovich and the

00:00:16,350 --> 00:00:21,720
talk is titled demystifying interrupt

00:00:19,650 --> 00:00:25,099
handling and sorry demystifying

00:00:21,720 --> 00:00:27,449
interrupt balancing irq balance so

00:00:25,099 --> 00:00:29,760
demystifying interrupt balancing irq

00:00:27,449 --> 00:00:45,149
balance PJ Peter wasowicz

00:00:29,760 --> 00:00:47,789
please and just to let you know I

00:00:45,149 --> 00:00:49,710
expected a much smaller turnout I am NOT

00:00:47,789 --> 00:00:51,300
Steven Ross that giving the F trace

00:00:49,710 --> 00:00:53,280
tutorial so thank you for coming to this

00:00:51,300 --> 00:00:56,100
that's a one that I actually wanted to

00:00:53,280 --> 00:00:58,199
see so hopefully I'll be able to get to

00:00:56,100 --> 00:00:59,760
see it on the video and watching waiting

00:00:58,199 --> 00:01:04,890
for the exodus of people that were

00:00:59,760 --> 00:01:06,780
expecting F trace here so this is why I

00:01:04,890 --> 00:01:08,670
hope that you guys are here

00:01:06,780 --> 00:01:11,880
this talk is something that I've been

00:01:08,670 --> 00:01:14,850
thinking about giving for a number of

00:01:11,880 --> 00:01:18,330
years because of the number of questions

00:01:14,850 --> 00:01:20,869
that I get over time about in a rapidity

00:01:18,330 --> 00:01:24,630
and how desire key balance actually work

00:01:20,869 --> 00:01:28,950
so to start with some more fundamental

00:01:24,630 --> 00:01:30,150
questions I wanted to talk first at what

00:01:28,950 --> 00:01:31,500
does it actually mean to balance

00:01:30,150 --> 00:01:34,439
interrupts what does that actually mean

00:01:31,500 --> 00:01:38,009
on a system and what kind of systems do

00:01:34,439 --> 00:01:39,600
we try to balance interrupts on and all

00:01:38,009 --> 00:01:42,570
of the different scenarios that can that

00:01:39,600 --> 00:01:45,000
can happen there and it's always good to

00:01:42,570 --> 00:01:46,560
know why you're doing something so we'll

00:01:45,000 --> 00:01:49,320
talk about why is interrupt balancing

00:01:46,560 --> 00:01:52,520
important talk about some scenarios that

00:01:49,320 --> 00:01:55,979
people run into in enterprise on mobile

00:01:52,520 --> 00:01:58,979
and how they apply to this this thing

00:01:55,979 --> 00:02:01,799
that we're trying to define today and

00:01:58,979 --> 00:02:03,210
then things are not rainbows and

00:02:01,799 --> 00:02:05,040
unicorns all the time when you try to do

00:02:03,210 --> 00:02:08,099
things you may have the best intentions

00:02:05,040 --> 00:02:10,920
that turn into nightmares to debug so

00:02:08,099 --> 00:02:13,000
we'll look at if you actually do try to

00:02:10,920 --> 00:02:15,280
do some good interrupt balancing

00:02:13,000 --> 00:02:16,840
why does it hurt you sometimes how many

00:02:15,280 --> 00:02:18,280
of you in here have actually run irq

00:02:16,840 --> 00:02:19,450
balance and just rip your hair out

00:02:18,280 --> 00:02:22,030
trying to figure out why it did the

00:02:19,450 --> 00:02:24,040
wrong thing so that's why you're here so

00:02:22,030 --> 00:02:26,500
hopefully when you when you leave here

00:02:24,040 --> 00:02:28,960
you won't rip your hair out as much and

00:02:26,500 --> 00:02:33,010
I can actually get some better better

00:02:28,960 --> 00:02:35,500
performance then I wanted to talk about

00:02:33,010 --> 00:02:39,580
a little bit more about how policies are

00:02:35,500 --> 00:02:41,650
actually applied in Linux specifically

00:02:39,580 --> 00:02:43,090
around interrupt balancing I get a lot

00:02:41,650 --> 00:02:44,920
of common questions so this is where

00:02:43,090 --> 00:02:46,900
we're trying to pull the curtain back on

00:02:44,920 --> 00:02:48,840
why are we doing things in iraqi balance

00:02:46,900 --> 00:02:51,040
and maybe not inside the kernel itself

00:02:48,840 --> 00:02:54,250
then we actually get to talk about iraqi

00:02:51,040 --> 00:02:56,020
balance how does it work what is the

00:02:54,250 --> 00:02:58,330
data that it's actually pulling out of

00:02:56,020 --> 00:02:59,830
the kernel what's exposed via the kernel

00:02:58,330 --> 00:03:02,320
and how does it actually collate all

00:02:59,830 --> 00:03:04,450
that together to make the decisions that

00:03:02,320 --> 00:03:07,270
it's making whether or not they're good

00:03:04,450 --> 00:03:08,920
or poor decisions what are the

00:03:07,270 --> 00:03:10,510
challenges that challenges that we have

00:03:08,920 --> 00:03:12,850
what are some of the improvements that

00:03:10,510 --> 00:03:15,340
we've been making over the years to try

00:03:12,850 --> 00:03:19,269
to give a bit more flexibility with our

00:03:15,340 --> 00:03:20,769
key balance more knobs to tune it so

00:03:19,269 --> 00:03:22,840
that maybe things that you have not seen

00:03:20,769 --> 00:03:24,100
or you looked at the man page into said

00:03:22,840 --> 00:03:26,830
I have really no idea how that's

00:03:24,100 --> 00:03:28,360
supposed to work so you ignore it so

00:03:26,830 --> 00:03:30,970
hopefully we'll will kind of clarify

00:03:28,360 --> 00:03:32,200
that a bit today and talk about some

00:03:30,970 --> 00:03:34,540
more of the future improvements that

00:03:32,200 --> 00:03:35,970
we're thinking about and that's where

00:03:34,540 --> 00:03:38,890
I'd like to open it up to discussion

00:03:35,970 --> 00:03:41,200
after that for see if anyone else has

00:03:38,890 --> 00:03:46,360
heartburn that they want to talk about

00:03:41,200 --> 00:03:49,090
or questions about irq Mellon's so to

00:03:46,360 --> 00:03:52,000
start with as Cody said I'm PJ wasowicz

00:03:49,090 --> 00:03:54,430
I have been working in Linux networking

00:03:52,000 --> 00:03:57,370
for over 12 years specifically in the

00:03:54,430 --> 00:03:59,320
kernel both in the network stack and

00:03:57,370 --> 00:04:01,989
then also on a number of the high speed

00:03:59,320 --> 00:04:06,640
network drivers for Intel the i-x GB

00:04:01,989 --> 00:04:08,590
drivers the iPhone ee ice drivers and so

00:04:06,640 --> 00:04:11,620
with that I've been focusing on a lot of

00:04:08,590 --> 00:04:13,840
performance and scalability when we

00:04:11,620 --> 00:04:16,709
tried to start scaling say the 10

00:04:13,840 --> 00:04:18,880
gigabit NICs back like 8 to 10 years ago

00:04:16,709 --> 00:04:20,769
interrupt balancing was one of the first

00:04:18,880 --> 00:04:22,330
and foremost problems that we had to

00:04:20,769 --> 00:04:26,620
solve after we solved the multi Q

00:04:22,330 --> 00:04:28,240
problem in the kernel so this area

00:04:26,620 --> 00:04:30,100
in high-speed networking is very very

00:04:28,240 --> 00:04:34,300
relevant to the interrupt balancing work

00:04:30,100 --> 00:04:35,680
that I've been also doing to support

00:04:34,300 --> 00:04:38,710
some of that I've also worked in the

00:04:35,680 --> 00:04:41,260
interrupt core to expose some of these

00:04:38,710 --> 00:04:43,840
knobs to make IR key balance a little

00:04:41,260 --> 00:04:45,850
less dumb and we can all argue as to

00:04:43,840 --> 00:04:47,830
what level of dumbness irq balance has

00:04:45,850 --> 00:04:49,240
achieved today and I can say that

00:04:47,830 --> 00:04:52,479
because I am a co maintainer of irq

00:04:49,240 --> 00:04:56,110
balance so we are here to try to improve

00:04:52,479 --> 00:04:58,300
it a little bit more about me I like to

00:04:56,110 --> 00:05:01,200
race motorcycles I like driving over

00:04:58,300 --> 00:05:04,120
things in the woods in my Jeep and

00:05:01,200 --> 00:05:07,540
recently got married which will kind of

00:05:04,120 --> 00:05:09,760
put those two things in jeopardy but

00:05:07,540 --> 00:05:12,850
yeah if there's me at the track and then

00:05:09,760 --> 00:05:17,370
me driving over a dead stump we did not

00:05:12,850 --> 00:05:19,660
harm anything in this picture okay so

00:05:17,370 --> 00:05:21,700
when I say balancing interrupts what

00:05:19,660 --> 00:05:24,729
does that actually mean to you

00:05:21,700 --> 00:05:28,090
and what are we trying to actually

00:05:24,729 --> 00:05:30,910
achieve right so what's driving the

00:05:28,090 --> 00:05:32,560
needs for this and one of the big things

00:05:30,910 --> 00:05:35,800
is that we have this multi core system

00:05:32,560 --> 00:05:39,340
war I I call it the core wars are alive

00:05:35,800 --> 00:05:41,169
and well we have Intel you know saying I

00:05:39,340 --> 00:05:42,610
can put more cores on a die and then AMD

00:05:41,169 --> 00:05:45,820
says well I could do better than that

00:05:42,610 --> 00:05:47,950
and then we've got armed 64 systems over

00:05:45,820 --> 00:05:51,940
there saying I've got 32 cores on a die

00:05:47,950 --> 00:05:55,360
as well so we have all of these cores

00:05:51,940 --> 00:05:57,880
that need work to do and when we have

00:05:55,360 --> 00:05:59,889
lots and lots of i/o devices we need to

00:05:57,880 --> 00:06:02,229
feed those cores right so we have to aim

00:05:59,889 --> 00:06:04,780
interrupts at different cores in some

00:06:02,229 --> 00:06:06,789
kind of same fashion to try to get the

00:06:04,780 --> 00:06:09,130
best use out of all of these extra CPUs

00:06:06,789 --> 00:06:10,750
that we have things get even more

00:06:09,130 --> 00:06:13,900
complicated when we talk about multi

00:06:10,750 --> 00:06:17,440
socket systems where you have you know

00:06:13,900 --> 00:06:19,330
two to four to eight sockets all with

00:06:17,440 --> 00:06:21,190
their own PCI Express Lanes for i/o

00:06:19,330 --> 00:06:23,310
devices to come in we're gonna get into

00:06:21,190 --> 00:06:26,710
all of this with with Numa locality I

00:06:23,310 --> 00:06:29,260
think that's actually my next bullet so

00:06:26,710 --> 00:06:31,419
the memory subsystems have also been

00:06:29,260 --> 00:06:33,400
getting more complex the topologies are

00:06:31,419 --> 00:06:35,500
looking a little different you know back

00:06:33,400 --> 00:06:38,500
in the day you had you know your PCI

00:06:35,500 --> 00:06:40,150
slot connected to your i/o hub and

00:06:38,500 --> 00:06:43,150
everything was kind of equated

00:06:40,150 --> 00:06:46,419
to memory these days we have PCI

00:06:43,150 --> 00:06:48,639
controllers are now in the on this

00:06:46,419 --> 00:06:51,790
actual CPU die right so on the Intel

00:06:48,639 --> 00:06:53,500
systems they're in the in the uncor so

00:06:51,790 --> 00:06:55,570
you have all of your PCI Express Lanes

00:06:53,500 --> 00:06:57,550
coming out of each CPU socket and you

00:06:55,570 --> 00:06:59,020
have your Neumann domains your memory

00:06:57,550 --> 00:07:01,650
controllers hanging off of each CPU

00:06:59,020 --> 00:07:04,870
socket so you don't have an equidistant

00:07:01,650 --> 00:07:06,639
hop to memory if you happen to be

00:07:04,870 --> 00:07:08,860
connected physically in a PCI Express

00:07:06,639 --> 00:07:10,449
slot to one socket but all of your

00:07:08,860 --> 00:07:13,389
memory is on the other sockets Neumann

00:07:10,449 --> 00:07:15,639
domain so how do you balance interrupts

00:07:13,389 --> 00:07:17,919
to make that more efficient if it's even

00:07:15,639 --> 00:07:24,639
possible so these are things that we

00:07:17,919 --> 00:07:28,330
also have to take into consideration IO

00:07:24,639 --> 00:07:31,720
devices have kind of had this explosion

00:07:28,330 --> 00:07:35,260
of the ability to generate lots and lots

00:07:31,720 --> 00:07:37,680
of I own right so network devices back

00:07:35,260 --> 00:07:40,930
when we started doing this work in say

00:07:37,680 --> 00:07:42,610
2007-2008 we had two queues we had two

00:07:40,930 --> 00:07:44,289
receiving two arcs queues and man we

00:07:42,610 --> 00:07:46,750
couldn't believe what we could do with

00:07:44,289 --> 00:07:48,130
that parallelism was was was mind

00:07:46,750 --> 00:07:50,229
boggling

00:07:48,130 --> 00:07:52,690
and then we upgraded to sixteen queues

00:07:50,229 --> 00:07:55,620
each and then we went and went whole hog

00:07:52,690 --> 00:07:58,750
and got 128 queues receive and transmit

00:07:55,620 --> 00:08:01,000
and now how do you start pairing out

00:07:58,750 --> 00:08:02,349
those I out I owe queues this is where

00:08:01,000 --> 00:08:04,840
virtualization came in and started

00:08:02,349 --> 00:08:07,900
getting queues dedicated to VMs and now

00:08:04,840 --> 00:08:09,520
we have containers and now our network

00:08:07,900 --> 00:08:11,919
devices have tens of thousands of queue

00:08:09,520 --> 00:08:15,190
pairs right so I think the current

00:08:11,919 --> 00:08:18,820
Mellanox cards out there have 16,000

00:08:15,190 --> 00:08:20,919
transmit 16,000 receive queues and if

00:08:18,820 --> 00:08:23,580
they're all generating i/o and causing

00:08:20,919 --> 00:08:26,020
interrupts to fire on a lot of these

00:08:23,580 --> 00:08:29,919
queues how do you actually spread those

00:08:26,020 --> 00:08:32,680
out to make the best use of them storage

00:08:29,919 --> 00:08:35,200
devices have always sort of lagged

00:08:32,680 --> 00:08:37,779
behind how network devices have evolved

00:08:35,200 --> 00:08:38,979
in terms of parallelism so SATA

00:08:37,779 --> 00:08:42,370
controllers for a long time we're just

00:08:38,979 --> 00:08:43,240
you know single queue in and out they

00:08:42,370 --> 00:08:46,420
started to become a little more

00:08:43,240 --> 00:08:49,270
parallelized nvme by the spec actually

00:08:46,420 --> 00:08:50,860
defines multiple send and completion

00:08:49,270 --> 00:08:52,840
queues and now they have multiple

00:08:50,860 --> 00:08:56,160
interrupt sources as well

00:08:52,840 --> 00:09:02,380
and that leads me into the next point

00:08:56,160 --> 00:09:04,180
all of this put together is kind of made

00:09:02,380 --> 00:09:05,980
even harder to deal with because now

00:09:04,180 --> 00:09:08,170
your i/o devices don't generate just one

00:09:05,980 --> 00:09:10,240
interrupt right back in the day you had

00:09:08,170 --> 00:09:12,250
your legacy interrupts then we moved to

00:09:10,240 --> 00:09:13,540
MSI interrupts or messaged signal

00:09:12,250 --> 00:09:15,520
interrupts which are a single interrupts

00:09:13,540 --> 00:09:18,010
source IO devices can now generate

00:09:15,520 --> 00:09:22,000
thousands of interrupts in parallel

00:09:18,010 --> 00:09:23,440
right with MSI X so the question is how

00:09:22,000 --> 00:09:25,000
do we actually properly spread all of

00:09:23,440 --> 00:09:26,890
these things out to all of the CPUs so

00:09:25,000 --> 00:09:32,730
that we're not varying one CPU with all

00:09:26,890 --> 00:09:32,730
of the work everyone's still awake now

00:09:32,940 --> 00:09:39,510
all they heard was the groans so I'm

00:09:35,230 --> 00:09:42,330
going to assume yes so given all of that

00:09:39,510 --> 00:09:44,980
why is it actually an important task and

00:09:42,330 --> 00:09:50,050
that previous slide really focused on

00:09:44,980 --> 00:09:51,670
scalability right scalability is kind of

00:09:50,050 --> 00:09:54,610
the natural default here right if you

00:09:51,670 --> 00:09:56,560
have you know 25 Gigabit Nick or 100

00:09:54,610 --> 00:09:58,300
gigabit Nick and you have multiple

00:09:56,560 --> 00:10:00,580
interrupts lots and lots of CPUs you

00:09:58,300 --> 00:10:02,530
want to fan them out maybe use our

00:10:00,580 --> 00:10:05,020
assess receive side scaling on the Nick

00:10:02,530 --> 00:10:07,600
to spread your flows out or you're using

00:10:05,020 --> 00:10:10,300
maybe a RFS advanced receive flow

00:10:07,600 --> 00:10:13,180
steering any way that you can figure out

00:10:10,300 --> 00:10:15,010
how to fan out your your actual dmas and

00:10:13,180 --> 00:10:17,920
the Nick will generate interrupts that

00:10:15,010 --> 00:10:19,240
then have to be serviced by CPUs so we

00:10:17,920 --> 00:10:22,480
want to make use of those CPUs by

00:10:19,240 --> 00:10:25,150
spreading out the load in a lot of the

00:10:22,480 --> 00:10:27,130
tests that we did over the years the

00:10:25,150 --> 00:10:29,140
best way to get the best performance is

00:10:27,130 --> 00:10:31,240
to pair up queues so you'd have a

00:10:29,140 --> 00:10:34,000
receive and a transmit queue that kind

00:10:31,240 --> 00:10:38,170
of get married together and they have a

00:10:34,000 --> 00:10:39,880
single interrupt and you try to figure

00:10:38,170 --> 00:10:42,670
out how your flows that you're running

00:10:39,880 --> 00:10:44,530
on your CPUs your network flows would be

00:10:42,670 --> 00:10:46,270
pinned to a specific core that you

00:10:44,530 --> 00:10:48,160
happen to have that interrupts firing on

00:10:46,270 --> 00:10:50,920
if you're able to do this and get this

00:10:48,160 --> 00:10:53,500
beautiful silo you get really really

00:10:50,920 --> 00:10:55,360
good cache locality on the CPUs and your

00:10:53,500 --> 00:10:59,560
performance is about as good as it's

00:10:55,360 --> 00:11:01,240
gonna get and if someone asks a question

00:10:59,560 --> 00:11:03,430
at the end why can't we just make our

00:11:01,240 --> 00:11:04,480
queue balanced do that I would love to

00:11:03,430 --> 00:11:06,930
have a chat with you to figure out how

00:11:04,480 --> 00:11:06,930
to do that

00:11:07,319 --> 00:11:10,769
and as I said this this is very very

00:11:09,149 --> 00:11:12,689
important for Network scalability right

00:11:10,769 --> 00:11:15,660
to be able to have this this cache

00:11:12,689 --> 00:11:18,389
locality to scale nvme is definitely

00:11:15,660 --> 00:11:19,829
knocking on the door now on running into

00:11:18,389 --> 00:11:24,529
some of the same scalability issues

00:11:19,829 --> 00:11:24,529
without having a interrupt alignment

00:11:24,689 --> 00:11:32,459
the next part is a little bit different

00:11:27,689 --> 00:11:33,929
this is kind of a more special case with

00:11:32,459 --> 00:11:37,230
cache efficiency so it's a different

00:11:33,929 --> 00:11:39,389
type of cache efficiency for locality if

00:11:37,230 --> 00:11:42,809
you had a work load a custom workload

00:11:39,389 --> 00:11:44,819
that you were trying to keep within a

00:11:42,809 --> 00:11:46,619
shared cache right so if you know about

00:11:44,819 --> 00:11:49,170
caching domains you have l1 which is

00:11:46,619 --> 00:11:50,550
typically one to one with CPUs your l2

00:11:49,170 --> 00:11:52,800
caches are typically shared across a

00:11:50,550 --> 00:11:54,839
couple different cores depending on the

00:11:52,800 --> 00:11:57,329
model of the CPU and then as you go

00:11:54,839 --> 00:11:59,730
deeper and deeper in the cache you have

00:11:57,329 --> 00:12:01,649
much more CPUs that are sharing one one

00:11:59,730 --> 00:12:05,160
of the caches so if you had a workload

00:12:01,649 --> 00:12:06,929
that wanted to have threads within two

00:12:05,160 --> 00:12:09,629
cores or four cores that are on the same

00:12:06,929 --> 00:12:12,449
shared cache you can balance the

00:12:09,629 --> 00:12:14,129
interrupts to fire on any of the CPUs

00:12:12,449 --> 00:12:15,629
that are in that shared cache so you

00:12:14,129 --> 00:12:17,369
maintain cache locality within your

00:12:15,629 --> 00:12:19,920
application again this is a very very

00:12:17,369 --> 00:12:21,209
special use case I mention it because I

00:12:19,920 --> 00:12:22,860
our key balance gives you the knob to

00:12:21,209 --> 00:12:27,299
hang yourself with if you're trying to

00:12:22,860 --> 00:12:30,720
do this so memory efficiency this is a

00:12:27,299 --> 00:12:34,740
this is another one I had mentioned the

00:12:30,720 --> 00:12:36,929
the Numa crosstalk so if I have a

00:12:34,740 --> 00:12:38,699
network device that is physically

00:12:36,929 --> 00:12:42,749
connected through PCI Express to one CPU

00:12:38,699 --> 00:12:45,029
socket and I have all of my memory also

00:12:42,749 --> 00:12:47,189
allocated on that socket but all of the

00:12:45,029 --> 00:12:50,040
interrupts for that PCI Express device

00:12:47,189 --> 00:12:51,470
are aimed at the other socket CPUs every

00:12:50,040 --> 00:12:54,149
single interrupt is going to cause

00:12:51,470 --> 00:12:58,199
across Numa traffic so you're going to

00:12:54,149 --> 00:13:01,019
be crossing the CPUs interconnect and we

00:12:58,199 --> 00:13:02,610
have seen worst case scenario you know

00:13:01,019 --> 00:13:06,569
synthetic tests where we can bury the

00:13:02,610 --> 00:13:08,790
qpi on even existing systems today for

00:13:06,569 --> 00:13:10,920
skylake which are blazing fast we can

00:13:08,790 --> 00:13:13,679
completely saturate that link if we

00:13:10,920 --> 00:13:14,999
screw this up on purpose and I our key

00:13:13,679 --> 00:13:18,449
balance kind of screws it up and makes

00:13:14,999 --> 00:13:19,740
it pretty painful and then the last one

00:13:18,449 --> 00:13:21,870
is power efficient

00:13:19,740 --> 00:13:23,280
right so if we want to balance

00:13:21,870 --> 00:13:25,620
interrupts it's not always about

00:13:23,280 --> 00:13:27,630
scalability it could be trying to

00:13:25,620 --> 00:13:29,790
collapse interrupts down to a certain

00:13:27,630 --> 00:13:32,040
subset of chords right as long as

00:13:29,790 --> 00:13:34,800
there's an interrupt tied to a CPU and a

00:13:32,040 --> 00:13:36,920
can fire on that CPU we will never allow

00:13:34,800 --> 00:13:39,480
that CPU to drop down at deep sea States

00:13:36,920 --> 00:13:42,090
and potentially allow a whole package to

00:13:39,480 --> 00:13:45,060
go offline so we're keeping caches hot

00:13:42,090 --> 00:13:47,370
we're keepin CPUs hot so on the laptop

00:13:45,060 --> 00:13:50,610
settings or even in data centers where

00:13:47,370 --> 00:13:51,720
you want to particularly in cloud where

00:13:50,610 --> 00:13:54,090
you want to spin things up very very

00:13:51,720 --> 00:13:55,980
quickly get all your CPU is going and

00:13:54,090 --> 00:13:57,570
then if the workloads drop you want to

00:13:55,980 --> 00:13:59,490
migrate all your workload on to one

00:13:57,570 --> 00:14:02,460
socket potentially to allow a whole

00:13:59,490 --> 00:14:04,260
bunch of other sockets to go offline so

00:14:02,460 --> 00:14:07,710
this is why you would also want to

00:14:04,260 --> 00:14:09,600
balance interrupts so the question is

00:14:07,710 --> 00:14:11,220
how are all of these related and the

00:14:09,600 --> 00:14:13,620
answer is they're not really related at

00:14:11,220 --> 00:14:16,770
all scalability is one thing you want to

00:14:13,620 --> 00:14:19,290
go big and wide and you know siloed and

00:14:16,770 --> 00:14:21,300
parallel cache efficiency is very very

00:14:19,290 --> 00:14:22,860
tight that if you have all this

00:14:21,300 --> 00:14:24,180
parallelism going on around that you're

00:14:22,860 --> 00:14:27,410
gonna be thrashing the crap out of your

00:14:24,180 --> 00:14:29,550
cache memory efficiency is kind of

00:14:27,410 --> 00:14:30,930
related to scalability but then power

00:14:29,550 --> 00:14:34,020
efficiency is on the other end of the

00:14:30,930 --> 00:14:35,600
spectrum so these are all of the issues

00:14:34,020 --> 00:14:37,680
that irq balance has to try to deal with

00:14:35,600 --> 00:14:40,680
so hopefully now you have a little bit

00:14:37,680 --> 00:14:43,620
of sympathy for it and if not that's

00:14:40,680 --> 00:14:46,140
okay so let's just run through a couple

00:14:43,620 --> 00:14:46,920
scenarios these are kind of synthetic

00:14:46,140 --> 00:14:50,310
pie-in-the-sky

00:14:46,920 --> 00:14:53,220
scenarios your target system wants power

00:14:50,310 --> 00:14:54,240
efficiency so like a laptop but all of

00:14:53,220 --> 00:14:56,790
the interrupts are assigned to each and

00:14:54,240 --> 00:14:59,070
every CPU core right worst-case scenario

00:14:56,790 --> 00:15:00,660
interrupts keep the CPS and caches from

00:14:59,070 --> 00:15:04,770
dropping in to see these dates so you

00:15:00,660 --> 00:15:06,840
fail scenario two you wanted maximum

00:15:04,770 --> 00:15:08,940
performance and scalability but the

00:15:06,840 --> 00:15:10,320
thing that I bet most of you in this

00:15:08,940 --> 00:15:12,300
room will raise your hand on if I say

00:15:10,320 --> 00:15:13,950
how often have you seen a system where

00:15:12,300 --> 00:15:17,880
all of your interrupts on this massively

00:15:13,950 --> 00:15:19,650
parallel system or on CPUs zero yeah so

00:15:17,880 --> 00:15:21,510
that sucks because now you have

00:15:19,650 --> 00:15:23,730
head-of-line blocking for every single

00:15:21,510 --> 00:15:26,430
interrupt on every device is waiting for

00:15:23,730 --> 00:15:28,470
CPU to zero to re-enable an interrupt so

00:15:26,430 --> 00:15:32,160
it can enter up again and go into that I

00:15:28,470 --> 00:15:35,990
is our the inter-service routine

00:15:32,160 --> 00:15:40,500
and now you have completely major system

00:15:35,990 --> 00:15:44,040
1cor waiting to have a bunch of cores do

00:15:40,500 --> 00:15:45,720
a little bit of work right so you have

00:15:44,040 --> 00:15:48,150
to wait for CPU zero to process all the

00:15:45,720 --> 00:15:50,880
interrupts and you fail and then the

00:15:48,150 --> 00:15:52,380
third scenario is if you want a mix of

00:15:50,880 --> 00:15:55,950
both performance and scalability but

00:15:52,380 --> 00:16:00,840
want great power efficiency as they say

00:15:55,950 --> 00:16:02,520
in Ireland good luck okay so just to

00:16:00,840 --> 00:16:04,820
give a little bit more context around

00:16:02,520 --> 00:16:07,170
irq contexts it's a really terrible pun

00:16:04,820 --> 00:16:08,790
this is important to understand when we

00:16:07,170 --> 00:16:10,290
talk about interrupts in the kernel that

00:16:08,790 --> 00:16:12,210
you're trying to balance because there

00:16:10,290 --> 00:16:14,670
are different two different contexts or

00:16:12,210 --> 00:16:15,930
main contexts we have Hardware

00:16:14,670 --> 00:16:17,910
interrupts right these are the things

00:16:15,930 --> 00:16:22,620
we've been referring to in the i/o

00:16:17,910 --> 00:16:23,700
devices they run in hard irq context in

00:16:22,620 --> 00:16:25,560
the kernel

00:16:23,700 --> 00:16:28,830
these include legacy interrupts so if

00:16:25,560 --> 00:16:32,130
you know anything about PCI devices or

00:16:28,830 --> 00:16:34,290
even ISO devices you'll see the int a or

00:16:32,130 --> 00:16:37,530
in B C or D pin interrupts these are

00:16:34,290 --> 00:16:40,320
called legacy this is an homage back to

00:16:37,530 --> 00:16:42,630
the old iron QL line days if you've ever

00:16:40,320 --> 00:16:44,760
screwed up your jumper settings on like

00:16:42,630 --> 00:16:46,620
your Sound Blaster card

00:16:44,760 --> 00:16:51,000
don't let the non-grey hair fool you

00:16:46,620 --> 00:16:52,200
I've run into this problem a lot even

00:16:51,000 --> 00:16:54,600
when you have the floppy disk to

00:16:52,200 --> 00:16:57,360
reprogram the virtual jumpers as to

00:16:54,600 --> 00:16:59,670
where your interrupts actually fired and

00:16:57,360 --> 00:17:01,770
then in PCI we introduced MSI or the

00:16:59,670 --> 00:17:04,140
messaged single interrupt so this is a

00:17:01,770 --> 00:17:06,870
non shareable interrupt so every device

00:17:04,140 --> 00:17:08,160
actually gets its own interrupt line and

00:17:06,870 --> 00:17:11,910
this is really just a PCI Express

00:17:08,160 --> 00:17:16,680
message and then MSI X this is many msi

00:17:11,910 --> 00:17:20,449
interrupts on a device this layout on a

00:17:16,680 --> 00:17:20,449
system is exposed via proc interrupts

00:17:21,110 --> 00:17:25,770
then there are software interrupts and

00:17:23,790 --> 00:17:27,900
these are generated by the kernel these

00:17:25,770 --> 00:17:31,050
run in soft irq context they are still

00:17:27,900 --> 00:17:33,330
considered an exception level thing so

00:17:31,050 --> 00:17:35,700
the same rules for interrupts apply to

00:17:33,330 --> 00:17:40,050
software cues as in you can't sleep

00:17:35,700 --> 00:17:41,370
inside of an interrupt well I teach a

00:17:40,050 --> 00:17:43,950
class on how to write device drivers

00:17:41,370 --> 00:17:47,340
back home and at a local university and

00:17:43,950 --> 00:17:49,590
a student actually asked me you can't

00:17:47,340 --> 00:17:51,240
sleep in one well you can sleep in and

00:17:49,590 --> 00:17:52,710
interrupt but I don't recommend you do

00:17:51,240 --> 00:17:54,510
it because you will have a kernel crash

00:17:52,710 --> 00:17:57,500
every time that is different and you'll

00:17:54,510 --> 00:18:00,300
be infuriated as what what's going wrong

00:17:57,500 --> 00:18:01,620
if you want to hear more about a horror

00:18:00,300 --> 00:18:03,390
story that took us three months to

00:18:01,620 --> 00:18:07,400
actually find a bug that that was this

00:18:03,390 --> 00:18:10,560
you can pull me aside outside anyways

00:18:07,400 --> 00:18:11,910
software interrupts are generally used

00:18:10,560 --> 00:18:14,400
for Network receive processing that's

00:18:11,910 --> 00:18:18,060
really the bulk of where software IQs

00:18:14,400 --> 00:18:20,160
run in the kernel this is the polling

00:18:18,060 --> 00:18:22,950
mechanism known as nappy if anyone has

00:18:20,160 --> 00:18:24,600
heard of that it is known as the new API

00:18:22,950 --> 00:18:28,470
and it's been around for about fourteen

00:18:24,600 --> 00:18:31,860
years and these software interrupts are

00:18:28,470 --> 00:18:35,420
exposed via proc soft I recuse and I'll

00:18:31,860 --> 00:18:37,830
talk a little bit more about that so

00:18:35,420 --> 00:18:39,410
just to kind of clarify a little bit

00:18:37,830 --> 00:18:41,970
more just some background information

00:18:39,410 --> 00:18:45,060
hired hard iron queues always run higher

00:18:41,970 --> 00:18:47,460
than software accuse and you must have

00:18:45,060 --> 00:18:49,110
interrupts disabled on the CPU right if

00:18:47,460 --> 00:18:50,910
hardware interrupt shows up you better

00:18:49,110 --> 00:18:52,440
make sure that that interrupt is

00:18:50,910 --> 00:18:54,870
disabled on the CPU because if a

00:18:52,440 --> 00:18:56,550
software interrupt tries to fire you

00:18:54,870 --> 00:18:58,950
will typically have a different type of

00:18:56,550 --> 00:19:01,770
interrupt fire shortly thereafter which

00:18:58,950 --> 00:19:04,140
is a machine check delivered via nmi or

00:19:01,770 --> 00:19:08,730
non-maskable interrupt and you have some

00:19:04,140 --> 00:19:10,380
problems so it's a bit of an eye chart

00:19:08,730 --> 00:19:12,750
and I apologize and I am gonna turn

00:19:10,380 --> 00:19:14,310
around and look at this this is what a

00:19:12,750 --> 00:19:16,220
snapshot of proc interrupts looks like

00:19:14,310 --> 00:19:19,650
if you have never seen it on the

00:19:16,220 --> 00:19:21,750
leftmost column is the interrupt number

00:19:19,650 --> 00:19:25,740
and then every other column after that

00:19:21,750 --> 00:19:28,380
is one column per CPU so this is from an

00:19:25,740 --> 00:19:30,450
eight thread fork or a thread system I

00:19:28,380 --> 00:19:33,540
decided to take the snapshot from this

00:19:30,450 --> 00:19:36,030
and not my dual sockets curly skylake

00:19:33,540 --> 00:19:37,860
with a hundred twelve cores because then

00:19:36,030 --> 00:19:39,750
there would be a hundred twelve columns

00:19:37,860 --> 00:19:41,070
that I would have to put in like two

00:19:39,750 --> 00:19:45,030
point font and it would still wrap about

00:19:41,070 --> 00:19:48,030
four times but what this represents is

00:19:45,030 --> 00:19:50,400
on the rightmost side is the actual

00:19:48,030 --> 00:19:54,090
interrupt that was created with the

00:19:50,400 --> 00:19:55,440
request irq function call inside of a

00:19:54,090 --> 00:19:57,610
driver these are hardware interrupts and

00:19:55,440 --> 00:19:59,800
this is showing a count of

00:19:57,610 --> 00:20:03,760
each CPU where that interrupts fired

00:19:59,800 --> 00:20:06,670
since the system last booted okay so

00:20:03,760 --> 00:20:08,560
I'll give you time later to look at this

00:20:06,670 --> 00:20:12,100
if you if you have a burning desire to

00:20:08,560 --> 00:20:17,320
watch this video again interrupt 124 is

00:20:12,100 --> 00:20:20,350
my en should be at 0 but it's a E&P 0s

00:20:17,320 --> 00:20:21,190
31 f6 that is my one gig device and it

00:20:20,350 --> 00:20:23,080
has one interrupt

00:20:21,190 --> 00:20:26,320
it's an msi interrupts and you can see

00:20:23,080 --> 00:20:28,300
that this this very large number on cpu

00:20:26,320 --> 00:20:31,300
6 indicates that all of the interrupts

00:20:28,300 --> 00:20:35,860
right now are pinned to cpu 6 for that

00:20:31,300 --> 00:20:38,350
one device if we look at proct soft iron

00:20:35,860 --> 00:20:39,700
queues it's a little different these are

00:20:38,350 --> 00:20:42,670
on the left side these are all the

00:20:39,700 --> 00:20:45,430
different software cues that the kernel

00:20:42,670 --> 00:20:47,410
currently has if you want to learn about

00:20:45,430 --> 00:20:48,640
the RC you saw fire to you mr. Paul is

00:20:47,410 --> 00:20:51,720
right here and he has a talk right after

00:20:48,640 --> 00:20:54,310
mine I highly recommend going to that

00:20:51,720 --> 00:20:57,520
the one that I'm more interested in is

00:20:54,310 --> 00:21:00,460
the net rx this is that receive side

00:20:57,520 --> 00:21:02,260
nappy polling software interrupt and you

00:21:00,460 --> 00:21:06,250
can see on CPU 6 this number is a lot

00:21:02,260 --> 00:21:09,460
larger than the other CPUs talk a little

00:21:06,250 --> 00:21:11,380
bit more about this here in a second so

00:21:09,460 --> 00:21:13,570
there are some differences with how

00:21:11,380 --> 00:21:16,180
affinity works with with hardware and

00:21:13,570 --> 00:21:17,470
software interrupts as I said hardware

00:21:16,180 --> 00:21:20,710
interrupts are generated by the endpoint

00:21:17,470 --> 00:21:24,580
devices they come across as a PCI

00:21:20,710 --> 00:21:26,290
Express message then that puts the CPU

00:21:24,580 --> 00:21:28,570
into an exception handler looks it up in

00:21:26,290 --> 00:21:30,820
the a pic the programmable interrupt

00:21:28,570 --> 00:21:32,320
controller and finds where that

00:21:30,820 --> 00:21:34,990
registered interrupt service routine is

00:21:32,320 --> 00:21:37,360
and then calls it and if you've never

00:21:34,990 --> 00:21:39,040
seen a PCI Express message there is a

00:21:37,360 --> 00:21:41,230
memory right in here that's going across

00:21:39,040 --> 00:21:43,840
the bus this is from a PCI Express

00:21:41,230 --> 00:21:48,610
analyzer this is what's happening on the

00:21:43,840 --> 00:21:50,620
actual bus to generate and interrupt in

00:21:48,610 --> 00:21:51,880
contrast software interrupts they are

00:21:50,620 --> 00:21:55,120
scheduled via the kernel as I said

00:21:51,880 --> 00:21:58,210
there's a do soft irq function call that

00:21:55,120 --> 00:22:00,970
you have in kernel core these are run

00:21:58,210 --> 00:22:03,460
out of the soft I er QD threads so if

00:22:00,970 --> 00:22:06,160
you did a process list of PS you see

00:22:03,460 --> 00:22:08,350
these case off tire QD slash 0 through

00:22:06,160 --> 00:22:10,510
whatever and that whatever is the number

00:22:08,350 --> 00:22:11,260
of cores you have on your system on it's

00:22:10,510 --> 00:22:12,370
in brackets

00:22:11,260 --> 00:22:14,020
Colonel thread that's actually running

00:22:12,370 --> 00:22:18,520
the software queues on that particular

00:22:14,020 --> 00:22:20,230
CPU and that is a very very important

00:22:18,520 --> 00:22:22,360
point that software interrupts are

00:22:20,230 --> 00:22:24,670
raised on the CPU of the kernel thread

00:22:22,360 --> 00:22:25,900
that is currently running on that CPU so

00:22:24,670 --> 00:22:28,330
if you have the kernel thread running in

00:22:25,900 --> 00:22:32,250
like CPU four and you do a software EQ

00:22:28,330 --> 00:22:35,620
that software EQ will fire on CPU four

00:22:32,250 --> 00:22:38,890
so the the subtlety here is for the

00:22:35,620 --> 00:22:41,440
hardware interrupts there is a another

00:22:38,890 --> 00:22:45,400
knob that irq balance makes use of proc

00:22:41,440 --> 00:22:47,860
irq and then the hardware irq number SNP

00:22:45,400 --> 00:22:50,050
affinity this is only applicable to

00:22:47,860 --> 00:22:51,130
hardware interrupts so if you're

00:22:50,050 --> 00:22:52,060
thinking well I've got hardware

00:22:51,130 --> 00:22:53,380
interrupts that I'm trying to balance

00:22:52,060 --> 00:22:55,120
and then I've got these software queues

00:22:53,380 --> 00:22:56,980
kind of running over here and you're

00:22:55,120 --> 00:22:58,300
telling me that my network drivers and

00:22:56,980 --> 00:22:59,980
DEP network devices are using software

00:22:58,300 --> 00:23:02,590
or higher queues to do the bulk of their

00:22:59,980 --> 00:23:04,030
work and this is the only knob that I've

00:23:02,590 --> 00:23:09,340
got how do i Affinia ties those other

00:23:04,030 --> 00:23:11,650
things and we'll get to that so this is

00:23:09,340 --> 00:23:16,390
just a quick listing of what that proc

00:23:11,650 --> 00:23:18,130
irq and then in this case 149 this is an

00:23:16,390 --> 00:23:20,830
MSI X vector on an eye for TE driver

00:23:18,130 --> 00:23:23,380
there are a few hooks here the two

00:23:20,830 --> 00:23:24,970
important ones to look at our SP

00:23:23,380 --> 00:23:27,790
affinity you can see that it is writable

00:23:24,970 --> 00:23:30,130
by route this is how we change the

00:23:27,790 --> 00:23:32,560
affinity for this particular interrupt

00:23:30,130 --> 00:23:36,040
vector and then there's this affinity

00:23:32,560 --> 00:23:37,930
hint hook that's also hanging off but

00:23:36,040 --> 00:23:40,270
that's also only read-only this is

00:23:37,930 --> 00:23:44,320
something that's exposed and can be

00:23:40,270 --> 00:23:48,040
programmed to by the driver okay so how

00:23:44,320 --> 00:23:49,570
can interrupt balancing go wrong this is

00:23:48,040 --> 00:23:51,610
this is always fun to talk about

00:23:49,570 --> 00:23:53,710
so device drivers when they allocate

00:23:51,610 --> 00:23:56,170
memory they by default pretty much

00:23:53,710 --> 00:24:00,460
always allocate now on the closest Numa

00:23:56,170 --> 00:24:02,110
node so we used fmk malloc we can

00:24:00,460 --> 00:24:04,960
override this with an underscore node

00:24:02,110 --> 00:24:08,890
and force a certain limit node and

00:24:04,960 --> 00:24:10,810
that's usually fraught with danger but

00:24:08,890 --> 00:24:13,060
the proximity of where this Numa node is

00:24:10,810 --> 00:24:15,640
can be affected by the PCIe root complex

00:24:13,060 --> 00:24:18,850
as I mentioned before we have PCIe root

00:24:15,640 --> 00:24:20,560
complexes on each CPU socket but you

00:24:18,850 --> 00:24:22,330
really should look at what your vendor

00:24:20,560 --> 00:24:24,490
did when they built the motherboard and

00:24:22,330 --> 00:24:25,040
where those PCI Express Lanes are

00:24:24,490 --> 00:24:27,050
electrically

00:24:25,040 --> 00:24:30,740
coming out of the sockets a lot of the

00:24:27,050 --> 00:24:32,600
1u pizza box servers you may have two

00:24:30,740 --> 00:24:34,160
sockets but only one of those sockets

00:24:32,600 --> 00:24:36,440
lanes are actually physically connected

00:24:34,160 --> 00:24:38,480
to the PCI Express slots so you have a

00:24:36,440 --> 00:24:43,460
whole bunch of PCIe lanes on another CPU

00:24:38,480 --> 00:24:44,810
that are hooked up to nothing the memory

00:24:43,460 --> 00:24:47,360
that we're allocating are the DMA

00:24:44,810 --> 00:24:50,110
buffers and the descriptor rings and

00:24:47,360 --> 00:24:52,070
where this gets into like a problem is

00:24:50,110 --> 00:24:54,620
again referring to this worst case

00:24:52,070 --> 00:24:56,210
scenario that I mentioned before if you

00:24:54,620 --> 00:24:57,920
have applications getting scheduled on

00:24:56,210 --> 00:25:00,920
different cores on different sockets

00:24:57,920 --> 00:25:03,530
that allocate say for networking user

00:25:00,920 --> 00:25:05,330
space socket buffers it could be on a

00:25:03,530 --> 00:25:06,890
different Numa no than the NIC and it

00:25:05,330 --> 00:25:09,020
can be even worse if you're in this case

00:25:06,890 --> 00:25:11,000
where you're allocating on a Numa know

00:25:09,020 --> 00:25:12,890
that you're physically connected to

00:25:11,000 --> 00:25:14,660
through PCI Express but on a different

00:25:12,890 --> 00:25:19,910
socket and then your applications oh and

00:25:14,660 --> 00:25:21,880
and and then hilarity ensues so some

00:25:19,910 --> 00:25:24,380
some diagrams cuz pictures are good and

00:25:21,880 --> 00:25:26,750
this is a very very simple picture right

00:25:24,380 --> 00:25:29,000
this is a very old picture of what does

00:25:26,750 --> 00:25:30,950
good interrupt balancing what is what is

00:25:29,000 --> 00:25:33,430
it what should it look like and here we

00:25:30,950 --> 00:25:38,750
have an ingress egress you know rx txq

00:25:33,430 --> 00:25:41,180
and we have this this the silo that I

00:25:38,750 --> 00:25:45,200
refer to of cache locality and cache

00:25:41,180 --> 00:25:46,730
goodness up to a CPU core what good and

00:25:45,200 --> 00:25:48,520
interrupt balancing on the whole system

00:25:46,730 --> 00:25:51,080
level topology should really look like

00:25:48,520 --> 00:25:53,690
and this is really kind of hard to parse

00:25:51,080 --> 00:25:55,390
and I apologize if anyone can actually

00:25:53,690 --> 00:25:57,620
look and see all these little numbers

00:25:55,390 --> 00:26:00,010
this was taken from a patent application

00:25:57,620 --> 00:26:05,650
that I had on our key balancing about

00:26:00,010 --> 00:26:10,490
nine years ago so it is with use of mine

00:26:05,650 --> 00:26:14,030
but it does show how does the each MSI X

00:26:10,490 --> 00:26:15,710
vector get applied to multiple cue pairs

00:26:14,030 --> 00:26:17,810
and how do they move around on the CPUs

00:26:15,710 --> 00:26:20,300
in a dynamic way this was this was

00:26:17,810 --> 00:26:24,470
around how to do dynamic power

00:26:20,300 --> 00:26:25,610
efficiency so but it still applies and

00:26:24,470 --> 00:26:27,620
then there's what does interrupt

00:26:25,610 --> 00:26:30,350
balancing really look like if you just

00:26:27,620 --> 00:26:32,920
kind of haphazardly go and and it's

00:26:30,350 --> 00:26:32,920
pretty much like that

00:26:36,450 --> 00:26:41,140
okay okay

00:26:37,780 --> 00:26:44,110
yes it's apparently this is a 50 lane

00:26:41,140 --> 00:26:45,520
wide road in Beijing I didn't even know

00:26:44,110 --> 00:26:49,030
they made them that big but apparently

00:26:45,520 --> 00:26:51,250
they do okay so now to our your key

00:26:49,030 --> 00:26:52,900
balance so the main flow of operation

00:26:51,250 --> 00:26:57,520
how does actually how does it actually

00:26:52,900 --> 00:26:58,750
work so it does run in userspace right

00:26:57,520 --> 00:26:59,290
that is a that is an important

00:26:58,750 --> 00:27:02,920
distinction

00:26:59,290 --> 00:27:05,559
it is privileged we saw that the proc ir

00:27:02,920 --> 00:27:08,290
q RQ naam S&P affinity file is only

00:27:05,559 --> 00:27:11,950
writable via route so it must run his

00:27:08,290 --> 00:27:14,260
route and we have information exposed

00:27:11,950 --> 00:27:16,059
from the kernel to determine all of

00:27:14,260 --> 00:27:18,040
these various system topologies right

00:27:16,059 --> 00:27:20,830
we've been talking about CPU layouts and

00:27:18,040 --> 00:27:22,210
and cache shared levels and and Numa

00:27:20,830 --> 00:27:25,450
nodes and how are they connected to all

00:27:22,210 --> 00:27:27,100
of the CPUs so there are a number of

00:27:25,450 --> 00:27:31,600
locations that I ricky balance will go

00:27:27,100 --> 00:27:33,100
and query the kernel on every every

00:27:31,600 --> 00:27:34,870
cycle when it's going through to see if

00:27:33,100 --> 00:27:38,110
anything changed right if you plug the

00:27:34,870 --> 00:27:40,090
CPU or pull memory out so it gets its

00:27:38,110 --> 00:27:44,020
CPU and cache configuration and layout

00:27:40,090 --> 00:27:47,320
from sis devices systems CPU it gets its

00:27:44,020 --> 00:27:50,410
Numa configuration from that path there

00:27:47,320 --> 00:27:53,590
and proc interrupts that we looked at

00:27:50,410 --> 00:27:57,160
that is the list of all of the allocated

00:27:53,590 --> 00:27:58,870
and also online interrupts so a driver

00:27:57,160 --> 00:28:01,120
may not be up and running yet so you may

00:27:58,870 --> 00:28:03,940
have not done the old school if config

00:28:01,120 --> 00:28:06,610
eat zero up but the driver may be loaded

00:28:03,940 --> 00:28:08,230
and ready to go you just may have not

00:28:06,610 --> 00:28:10,330
brought linked up that interrupt will be

00:28:08,230 --> 00:28:14,440
online and ready to go and it'll show up

00:28:10,330 --> 00:28:16,809
here it just won't be doing anything and

00:28:14,440 --> 00:28:18,700
then affinity hints per interrupt if

00:28:16,809 --> 00:28:23,260
they are exposed and we're going to get

00:28:18,700 --> 00:28:25,000
to that here in a slider - this may be

00:28:23,260 --> 00:28:27,730
populated by the driver and it may not

00:28:25,000 --> 00:28:30,429
be so if it is Iorek you balance will

00:28:27,730 --> 00:28:32,380
use it and there is a scan loop and

00:28:30,429 --> 00:28:34,360
right now that default is every 10

00:28:32,380 --> 00:28:36,160
seconds it is changeable via command

00:28:34,360 --> 00:28:39,880
line I do not have the man page

00:28:36,160 --> 00:28:42,730
somewhere in here figure you can go read

00:28:39,880 --> 00:28:47,650
the man page and I'll keep it read the

00:28:42,730 --> 00:28:51,530
man page not the the typical acronyms

00:28:47,650 --> 00:28:54,100
so I Ricky balance then takes all of

00:28:51,530 --> 00:28:57,230
this data it will do a whole bunch of

00:28:54,100 --> 00:28:59,600
backflips and limbo and all this stuff

00:28:57,230 --> 00:29:01,940
to merge the Numa configuration with the

00:28:59,600 --> 00:29:04,810
CPU topology that is actually a lot of

00:29:01,940 --> 00:29:07,280
the the core code when it's processing

00:29:04,810 --> 00:29:11,450
and it builds this entire system

00:29:07,280 --> 00:29:13,940
topology map internally takes all of

00:29:11,450 --> 00:29:15,530
this what the interrupts load is where

00:29:13,940 --> 00:29:17,300
are the interrupts firing on which cpus

00:29:15,530 --> 00:29:20,750
and which interrupts are more busy than

00:29:17,300 --> 00:29:22,070
others and compares with any command

00:29:20,750 --> 00:29:23,930
line options that were passed to it as

00:29:22,070 --> 00:29:26,740
to how it's supposed to handle all of

00:29:23,930 --> 00:29:31,480
this data and then spits out an answer

00:29:26,740 --> 00:29:31,480
and that answer is usually not correct

00:29:31,690 --> 00:29:35,570
if it needs to go ahead and make any

00:29:34,340 --> 00:29:38,210
changes it goes and writes the new

00:29:35,570 --> 00:29:40,190
affinities out to this file this is the

00:29:38,210 --> 00:29:43,730
thing that actually goes and pokes at

00:29:40,190 --> 00:29:45,880
the a pic reprogramming rewrites the a

00:29:43,730 --> 00:29:51,170
pic values for those interrupts numbers

00:29:45,880 --> 00:29:53,210
to fire on different CPUs and just to

00:29:51,170 --> 00:29:54,560
reiterate it does not move the software

00:29:53,210 --> 00:29:57,380
cues that were scheduled from hardware

00:29:54,560 --> 00:29:59,450
interrupts so alluding back to the

00:29:57,380 --> 00:30:02,000
snappy thing for networking if you have

00:29:59,450 --> 00:30:03,770
a soft interrupts firing that was

00:30:02,000 --> 00:30:05,510
scheduled via a hardware interrupt and

00:30:03,770 --> 00:30:08,780
remember the software eq is run on the

00:30:05,510 --> 00:30:10,130
CPU that they were scheduled on so if

00:30:08,780 --> 00:30:12,800
you move your hardware interrupts over

00:30:10,130 --> 00:30:14,360
to a different cpu that hardware

00:30:12,800 --> 00:30:16,430
interrupts has to fire again on the

00:30:14,360 --> 00:30:18,350
different cpu to reschedule that soft

00:30:16,430 --> 00:30:21,560
irq on the different cpu this is a

00:30:18,350 --> 00:30:23,450
common issue that I get on scalability

00:30:21,560 --> 00:30:26,450
questions on the irq balance mailing

00:30:23,450 --> 00:30:28,070
lists this thing ran and I have this you

00:30:26,450 --> 00:30:31,190
know huge network card that's always

00:30:28,070 --> 00:30:34,610
under load and I rebalance interrupts

00:30:31,190 --> 00:30:36,950
and nothing changes and then I ask well

00:30:34,610 --> 00:30:38,330
did you change it through SNP affinity

00:30:36,950 --> 00:30:40,580
directly well yeah and that didn't fix

00:30:38,330 --> 00:30:44,720
it either so if you know anything about

00:30:40,580 --> 00:30:46,340
nappy nappy actually has to have no work

00:30:44,720 --> 00:30:48,590
to do to get back out of polling mode

00:30:46,340 --> 00:30:51,050
and go into hardware interrupt mode so

00:30:48,590 --> 00:30:52,940
this isn't one of those things that you

00:30:51,050 --> 00:30:55,220
you magically change this value and then

00:30:52,940 --> 00:30:58,400
things go yeah Paul

00:30:55,220 --> 00:31:09,350
I like stupid questions because cuz then

00:30:58,400 --> 00:31:11,450
hopefully they're easy to answer so the

00:31:09,350 --> 00:31:13,970
question is could there be an interface

00:31:11,450 --> 00:31:16,760
between our key balance and the nappy

00:31:13,970 --> 00:31:27,110
interface in the kernel - - feedback -

00:31:16,760 --> 00:31:30,920
move that is a great suggestion never

00:31:27,110 --> 00:31:34,520
thought of it and I like it there's a

00:31:30,920 --> 00:31:35,900
github page for higher key balance I'm

00:31:34,520 --> 00:31:37,490
not gonna say submit patches because

00:31:35,900 --> 00:31:39,980
that's the easy one but there actually

00:31:37,490 --> 00:31:43,220
is a Neil hormon is the the other

00:31:39,980 --> 00:31:45,680
maintainer he's at Red Hat we have a

00:31:43,220 --> 00:31:48,470
issue section to open up a bug or a

00:31:45,680 --> 00:31:49,970
tracker and usually when they open up we

00:31:48,470 --> 00:31:53,680
jump right on them that's that's a

00:31:49,970 --> 00:31:56,180
really good suggestion thank you Paul

00:31:53,680 --> 00:31:57,830
now is this where you're gonna be able

00:31:56,180 --> 00:31:59,750
to use the same interface for software

00:31:57,830 --> 00:32:04,490
interrupts things for the RCO software

00:31:59,750 --> 00:32:05,950
accused and move them around I was

00:32:04,490 --> 00:32:08,810
hoping that there's this thing called

00:32:05,950 --> 00:32:15,410
kernel or get kernel.org that I can

00:32:08,810 --> 00:32:17,180
submit Patras again okay okay so now to

00:32:15,410 --> 00:32:19,370
move a little bit more to the policy

00:32:17,180 --> 00:32:21,080
enforcement right so a lot of the work

00:32:19,370 --> 00:32:23,360
around irq balance in the last few years

00:32:21,080 --> 00:32:25,280
has been around how do we enforce policy

00:32:23,360 --> 00:32:27,350
but a lot of the questions I also get

00:32:25,280 --> 00:32:30,110
are why doesn't interrupt balancing just

00:32:27,350 --> 00:32:32,960
happen inside the kernel right and my

00:32:30,110 --> 00:32:34,850
personal take is I've actually tried to

00:32:32,960 --> 00:32:37,220
submit patches numerous times to allow

00:32:34,850 --> 00:32:38,630
the kernel or the device drivers to

00:32:37,220 --> 00:32:41,240
balance their own interrupts and that

00:32:38,630 --> 00:32:42,680
gets shot down because policy is always

00:32:41,240 --> 00:32:46,730
an entertaining conversation to have

00:32:42,680 --> 00:32:49,310
with kernel developers I agree with the

00:32:46,730 --> 00:32:50,900
concept at a certain level that user

00:32:49,310 --> 00:32:52,790
space should be making decisions on

00:32:50,900 --> 00:32:54,590
policy as to how the system works

00:32:52,790 --> 00:32:56,240
and the kernel should be responsible for

00:32:54,590 --> 00:33:01,340
providing all the information for that

00:32:56,240 --> 00:33:03,290
policy to be reasonably applied and you

00:33:01,340 --> 00:33:06,050
can see the success of irq balances

00:33:03,290 --> 00:33:08,390
policy that maybe we don't have enough

00:33:06,050 --> 00:33:09,320
information coming out of the kernel we

00:33:08,390 --> 00:33:10,700
may never have enough

00:33:09,320 --> 00:33:11,960
information coming out of the kernel

00:33:10,700 --> 00:33:14,360
because we may not get scheduler

00:33:11,960 --> 00:33:18,230
information based on where applications

00:33:14,360 --> 00:33:20,179
are running and yeah so I just kind of

00:33:18,230 --> 00:33:22,759
said that that the policy lives in user

00:33:20,179 --> 00:33:23,899
space we need the knobs we need the

00:33:22,759 --> 00:33:28,419
knobs to be able to get the right

00:33:23,899 --> 00:33:31,340
information to make the right calls and

00:33:28,419 --> 00:33:33,259
that is the common question is why

00:33:31,340 --> 00:33:35,049
doesn't the kernel just balance it if it

00:33:33,259 --> 00:33:37,130
already has all the information it knows

00:33:35,049 --> 00:33:38,659
where the memory is being allocated

00:33:37,130 --> 00:33:41,000
which Numa nodes and knows where it

00:33:38,659 --> 00:33:43,129
allocated its queue resources it knows

00:33:41,000 --> 00:33:44,960
where it well it allocates the

00:33:43,129 --> 00:33:46,340
interrupts and it should just say I want

00:33:44,960 --> 00:33:48,139
them right there because that's where I

00:33:46,340 --> 00:33:50,799
allocated all of the system memory for

00:33:48,139 --> 00:33:53,659
that ring that I want to Affinia ties to

00:33:50,799 --> 00:33:55,309
and the answer is see the first bullet

00:33:53,659 --> 00:33:56,330
point policy is always an entertaining

00:33:55,309 --> 00:33:59,690
conversation to have with kernel

00:33:56,330 --> 00:34:03,009
developers seriously though a few years

00:33:59,690 --> 00:34:05,269
back Christoph Helbig and I plumbers

00:34:03,009 --> 00:34:06,919
started talking about this as he was

00:34:05,269 --> 00:34:09,470
working on the nvme patches and

00:34:06,919 --> 00:34:10,970
realizing that well crap I can't Afeni

00:34:09,470 --> 00:34:13,069
ties my interrupts on this multi cue

00:34:10,970 --> 00:34:15,770
device well that's just stupid

00:34:13,069 --> 00:34:17,510
and then fast rewind about eight years

00:34:15,770 --> 00:34:19,069
before when I submitted the same patches

00:34:17,510 --> 00:34:21,889
to do it for networking and Christoph

00:34:19,069 --> 00:34:34,970
shot it down saying we don't do policy

00:34:21,889 --> 00:34:36,950
there was a question up there yeah so so

00:34:34,970 --> 00:34:40,550
so you're saying that yes there there

00:34:36,950 --> 00:34:42,079
are exceptions to policy being applied

00:34:40,550 --> 00:34:45,829
in the kernel versus user space and that

00:34:42,079 --> 00:34:47,960
is that is accurate this is one that we

00:34:45,829 --> 00:34:50,419
are actively trying to work on to to

00:34:47,960 --> 00:34:54,290
allow we do have a little bit of wiggle

00:34:50,419 --> 00:34:56,960
room but but we still can't apply in

00:34:54,290 --> 00:35:00,170
actual affinity directly to the irq core

00:34:56,960 --> 00:35:01,430
from a driver yet and I use yet because

00:35:00,170 --> 00:35:05,450
because one of these days those patches

00:35:01,430 --> 00:35:07,730
will go in okay thanks so how do we Bend

00:35:05,450 --> 00:35:09,230
irq balance to our will everyone's

00:35:07,730 --> 00:35:10,430
solution probably is killer our key

00:35:09,230 --> 00:35:12,710
balance and manually place the

00:35:10,430 --> 00:35:14,089
interrupts this is the typical thing I

00:35:12,710 --> 00:35:15,920
hear people do or if they don't even

00:35:14,089 --> 00:35:18,380
install it and there's this neat little

00:35:15,920 --> 00:35:19,550
set proc irq affinity script that has

00:35:18,380 --> 00:35:22,850
been running around the internet for

00:35:19,550 --> 00:35:25,370
years that has basically

00:35:22,850 --> 00:35:27,110
what we wrote and it just kind of you

00:35:25,370 --> 00:35:30,290
know this it went viral

00:35:27,110 --> 00:35:32,030
I guess is the the term today this is

00:35:30,290 --> 00:35:34,010
this is something that that works for

00:35:32,030 --> 00:35:36,890
some some things right but what happens

00:35:34,010 --> 00:35:39,590
if your system topology changes what

00:35:36,890 --> 00:35:41,750
happens if a workload fires up and

00:35:39,590 --> 00:35:43,730
starts hammering on a CPU that you

00:35:41,750 --> 00:35:46,460
didn't expect and now you have no

00:35:43,730 --> 00:35:50,050
flexibility to try to rebalance the

00:35:46,460 --> 00:35:53,690
interrupts to to accommodate that

00:35:50,050 --> 00:35:56,440
solution to is pray and that usually

00:35:53,690 --> 00:35:59,090
ends up with that lovely traffic jam

00:35:56,440 --> 00:36:01,940
solution 3 so this is getting into some

00:35:59,090 --> 00:36:04,340
more of the techniques that people are

00:36:01,940 --> 00:36:05,810
using today there are command-line

00:36:04,340 --> 00:36:07,070
options too hierarchy balance to ban

00:36:05,810 --> 00:36:09,710
certain higher queues this has been

00:36:07,070 --> 00:36:11,150
around for a bit and when you ban those

00:36:09,710 --> 00:36:14,740
interrupts then you can manually place

00:36:11,150 --> 00:36:19,460
those yourself so it's a a bit more

00:36:14,740 --> 00:36:21,020
robust than solution 1 so this will

00:36:19,460 --> 00:36:23,270
allow our queue balance to still move

00:36:21,020 --> 00:36:24,590
the other things around but the problem

00:36:23,270 --> 00:36:27,410
is it might be moving those things

00:36:24,590 --> 00:36:29,210
around and it can't sometimes make the

00:36:27,410 --> 00:36:31,010
right decisions because you won't let it

00:36:29,210 --> 00:36:33,650
balance other ones away if something

00:36:31,010 --> 00:36:35,270
gets hot right so this is still a

00:36:33,650 --> 00:36:38,120
slightly static configuration with a

00:36:35,270 --> 00:36:40,310
little bit of fluidity and then there's

00:36:38,120 --> 00:36:43,100
solution for this is a relatively new

00:36:40,310 --> 00:36:46,160
thing that we've added called policy

00:36:43,100 --> 00:36:50,990
scripts this is a programmatic way to

00:36:46,160 --> 00:36:53,450
define the topology so we suggest where

00:36:50,990 --> 00:36:54,890
we want all of the interrupts we allow

00:36:53,450 --> 00:36:57,430
other in our interrupts to move around

00:36:54,890 --> 00:36:59,930
and it's a very very nice way to

00:36:57,430 --> 00:37:08,600
override global settings that I our key

00:36:59,930 --> 00:37:11,660
balance currently has yeah so here's the

00:37:08,600 --> 00:37:13,940
the hint policy option this is something

00:37:11,660 --> 00:37:19,910
that is is you can give it this shell

00:37:13,940 --> 00:37:22,880
script or a policy script thing and this

00:37:19,910 --> 00:37:25,670
is a global setting and this is where I

00:37:22,880 --> 00:37:28,700
was saying that the okay now I remember

00:37:25,670 --> 00:37:31,220
what this bullet is this is where the

00:37:28,700 --> 00:37:33,800
driver is trying to expose its actual

00:37:31,220 --> 00:37:36,390
suggested affinity it's a hint and it

00:37:33,800 --> 00:37:38,190
needs to call irq set affinity hint

00:37:36,390 --> 00:37:42,869
that is that affinity hint thing if the

00:37:38,190 --> 00:37:44,759
driver does not expose that then irq

00:37:42,869 --> 00:37:47,329
balance is going to make a decision for

00:37:44,759 --> 00:37:49,529
you or you manually place this interrupt

00:37:47,329 --> 00:37:51,269
so the policy scripts are nice because

00:37:49,529 --> 00:37:54,599
it doesn't require the driver to have to

00:37:51,269 --> 00:37:56,249
do that right so you can specify I want

00:37:54,599 --> 00:38:00,779
to hint this thing over to this other

00:37:56,249 --> 00:38:02,970
CPU and I can define it in these key

00:38:00,779 --> 00:38:05,999
value pairs there's more information on

00:38:02,970 --> 00:38:07,950
it and the man page and you can specify

00:38:05,999 --> 00:38:09,960
the policy of those interrupts to be at

00:38:07,950 --> 00:38:12,900
the pneuma level at a certain cache

00:38:09,960 --> 00:38:14,759
level so you can be at the LLC you can

00:38:12,900 --> 00:38:17,039
have it at the l2 level the l1 level

00:38:14,759 --> 00:38:19,950
which would not be a great idea or at

00:38:17,039 --> 00:38:21,989
the physical core level so this gives

00:38:19,950 --> 00:38:24,359
you much finer grained control and we've

00:38:21,989 --> 00:38:25,619
seen some people use this and actually

00:38:24,359 --> 00:38:28,140
had some pretty good success with it

00:38:25,619 --> 00:38:31,739
they've been happy and I like trying to

00:38:28,140 --> 00:38:34,499
make people happy so this is this is

00:38:31,739 --> 00:38:36,660
where we're at today so where are we

00:38:34,499 --> 00:38:39,749
trying to go next and Paul had a great

00:38:36,660 --> 00:38:43,680
suggestion to solve the software EQ

00:38:39,749 --> 00:38:46,819
rebalancing issue but those knobs that I

00:38:43,680 --> 00:38:48,960
was mentioning that we would better need

00:38:46,819 --> 00:38:50,910
one of the things we don't have is

00:38:48,960 --> 00:38:53,069
scheduler feedback and that's one of the

00:38:50,910 --> 00:38:55,559
things I don't think we would get but

00:38:53,069 --> 00:38:57,450
knowing where applications are starting

00:38:55,559 --> 00:38:59,700
and where they're getting scheduled so

00:38:57,450 --> 00:39:03,119
that we can not have to set tasks and

00:38:59,700 --> 00:39:05,489
force them onto a CPU if irq balance can

00:39:03,119 --> 00:39:08,369
get something dynamic we can try to move

00:39:05,489 --> 00:39:10,019
interrupts around to move with network

00:39:08,369 --> 00:39:15,690
flows that are moving around between

00:39:10,019 --> 00:39:17,999
different cores this goes to the policy

00:39:15,690 --> 00:39:20,119
allow drivers to actually have better

00:39:17,999 --> 00:39:22,019
control of replacing their interrupts

00:39:20,119 --> 00:39:23,910
drivers know where they're allocating

00:39:22,019 --> 00:39:26,819
memory and they know where it would be

00:39:23,910 --> 00:39:28,349
best suited for an interrupt to fire so

00:39:26,819 --> 00:39:30,650
we should we should give them better

00:39:28,349 --> 00:39:32,670
control stay tuned

00:39:30,650 --> 00:39:34,559
some of the other improvements that

00:39:32,670 --> 00:39:38,400
we've been working on and this feeds

00:39:34,559 --> 00:39:39,480
into the Internet of Things theme I had

00:39:38,400 --> 00:39:41,249
to slip it in here somewhere

00:39:39,480 --> 00:39:43,710
eirick you balance has always been

00:39:41,249 --> 00:39:46,230
focused on more PCIe devices and PCI

00:39:43,710 --> 00:39:48,210
devices however a lot of the arm

00:39:46,230 --> 00:39:50,099
platforms have been starting to use this

00:39:48,210 --> 00:39:53,759
we've been finding that

00:39:50,099 --> 00:39:55,710
pcie devices don't get quite processed

00:39:53,759 --> 00:39:58,140
correctly by IR key balance I know

00:39:55,710 --> 00:39:59,369
that's a shocker so we've been getting

00:39:58,140 --> 00:40:03,089
some patches from the community to help

00:39:59,369 --> 00:40:06,089
out which is great and I think this is

00:40:03,089 --> 00:40:07,799
where we can really really benefit all

00:40:06,089 --> 00:40:09,930
of these are like really archaic

00:40:07,799 --> 00:40:11,729
command-line tools it would be nice to

00:40:09,930 --> 00:40:14,249
have some kind of like there is an ir

00:40:11,729 --> 00:40:16,670
key balance UI it is in the ir q balance

00:40:14,249 --> 00:40:19,470
tree it is not maintained by neil and i

00:40:16,670 --> 00:40:21,329
so I would like to see something around

00:40:19,470 --> 00:40:23,940
this to just make it easier for people

00:40:21,329 --> 00:40:25,319
to actually configure things so that

00:40:23,940 --> 00:40:26,339
they think that they configured

00:40:25,319 --> 00:40:27,779
something correctly and then they're

00:40:26,339 --> 00:40:30,869
like why the hell didn't that work right

00:40:27,779 --> 00:40:34,140
so so I think that's a that's an area

00:40:30,869 --> 00:40:37,470
that we can really improve on so to sum

00:40:34,140 --> 00:40:38,849
up so you can alway cup now interrupts

00:40:37,470 --> 00:40:40,769
balancing is not a simple operation

00:40:38,849 --> 00:40:42,839
right there's a lot of reasons that we

00:40:40,769 --> 00:40:46,349
want to try to interrupt interrupts and

00:40:42,839 --> 00:40:50,460
the reason may not be what happens on

00:40:46,349 --> 00:40:52,259
the actual reality side right in order

00:40:50,460 --> 00:40:53,849
to do this though there is a need for a

00:40:52,259 --> 00:40:55,049
complete system and topology and

00:40:53,849 --> 00:40:58,259
visibility to balance everything

00:40:55,049 --> 00:41:00,269
correctly these use cases right between

00:40:58,259 --> 00:41:02,299
scalability and power efficiency right

00:41:00,269 --> 00:41:04,469
there they're two bookends and

00:41:02,299 --> 00:41:06,539
understanding those are really really

00:41:04,469 --> 00:41:10,140
critical to having correct interrupt

00:41:06,539 --> 00:41:12,119
balancing right so I kind of threw these

00:41:10,140 --> 00:41:13,920
out and then just want to kind of tease

00:41:12,119 --> 00:41:15,509
your noodle a little bit how does this

00:41:13,920 --> 00:41:20,160
apply to virtual machines and virtual

00:41:15,509 --> 00:41:24,839
CPUs and pass-through Hardware not well

00:41:20,160 --> 00:41:29,119
how's that so with that I'd like to open

00:41:24,839 --> 00:41:29,119
it up to questions for a few minutes yes

00:41:35,779 --> 00:41:40,319
sorry so the answer to my question is

00:41:38,670 --> 00:41:42,930
are there going to be it's too hard or

00:41:40,319 --> 00:41:46,819
it's very simple so my previous employer

00:41:42,930 --> 00:41:49,440
who I won't name made a VoIP hardware

00:41:46,819 --> 00:41:53,339
for every ISDN connection it needs to

00:41:49,440 --> 00:41:55,349
generate 60 interrupts a second with rl6

00:41:53,339 --> 00:41:57,509
kernels when you look at proc interrupts

00:41:55,349 --> 00:41:59,099
it smeared beautifully over all that the

00:41:57,509 --> 00:42:03,210
driver was multi-threaded it all worked

00:41:59,099 --> 00:42:06,720
in which was a 2.6 kernel in real 7

00:42:03,210 --> 00:42:09,690
it's like 66% or on CPU one and the rest

00:42:06,720 --> 00:42:12,710
is smeared of what changed do you know

00:42:09,690 --> 00:42:14,930
so I don't know in that particular case

00:42:12,710 --> 00:42:18,000
it can be a lot of different things

00:42:14,930 --> 00:42:20,250
there were some interrupt balance irq

00:42:18,000 --> 00:42:24,720
balance changes so by default we were

00:42:20,250 --> 00:42:26,520
using affinity hint that was with

00:42:24,720 --> 00:42:28,380
anti-occupy literally setting the the

00:42:26,520 --> 00:42:31,230
process affinity to all if they want to

00:42:28,380 --> 00:42:33,660
run so it would run the kernel would

00:42:31,230 --> 00:42:36,750
randomly allocate it without IQ balance

00:42:33,660 --> 00:42:38,070
at all so I went down this path so my

00:42:36,750 --> 00:42:44,670
answer to that without looking at the

00:42:38,070 --> 00:42:45,990
code and see what changed is I I can I

00:42:44,670 --> 00:42:49,680
can assure you that the interrupts core

00:42:45,990 --> 00:42:51,660
did not have any hooks to allow us to do

00:42:49,680 --> 00:42:52,890
that programmatically that I was aware

00:42:51,660 --> 00:42:56,520
of because I had to go and write the

00:42:52,890 --> 00:42:59,460
other stuff yeah you talked at the

00:42:56,520 --> 00:43:02,880
beginning about for scaling at large

00:42:59,460 --> 00:43:07,350
particularly large network loads the

00:43:02,880 --> 00:43:10,200
need to spread across cause I've heard

00:43:07,350 --> 00:43:13,050
other people mention that actually if

00:43:10,200 --> 00:43:14,880
you've got a high CPU load you may

00:43:13,050 --> 00:43:18,150
actually want to dedicate a core to

00:43:14,880 --> 00:43:21,000
processing all your interrupts what are

00:43:18,150 --> 00:43:23,640
they what are the circumstances or the

00:43:21,000 --> 00:43:24,960
variables under which you would decide

00:43:23,640 --> 00:43:26,190
to go one way or the other

00:43:24,960 --> 00:43:29,490
yeah that's a good that's a good

00:43:26,190 --> 00:43:30,810
question so so the the very high level

00:43:29,490 --> 00:43:32,850
will hand wavy simple answer is it

00:43:30,810 --> 00:43:34,440
depends on the workload right so if you

00:43:32,850 --> 00:43:37,320
have lots and lots of small packets that

00:43:34,440 --> 00:43:41,700
are generating lots of interrupts nappy

00:43:37,320 --> 00:43:42,840
doesn't really ever kick in very well so

00:43:41,700 --> 00:43:44,520
you're gonna have a much higher

00:43:42,840 --> 00:43:46,200
interrupt rate so in that case you may

00:43:44,520 --> 00:43:49,950
want to collapse them down to like say 8

00:43:46,200 --> 00:43:51,950
CPUs for bulk transfers it really

00:43:49,950 --> 00:43:55,470
depends on how much data you're moving

00:43:51,950 --> 00:43:58,200
and if you want to keep your your cores

00:43:55,470 --> 00:44:00,810
wide it also has to do with network

00:43:58,200 --> 00:44:03,810
hardware so some vendors they'll use our

00:44:00,810 --> 00:44:07,500
assess receive side scaling to blow the

00:44:03,810 --> 00:44:08,850
flows out and some of them fall over at

00:44:07,500 --> 00:44:10,980
a certain point of the number of cues

00:44:08,850 --> 00:44:13,800
that they bring online I know the Intel

00:44:10,980 --> 00:44:16,530
hardware the 80 to $5.99 if you went

00:44:13,800 --> 00:44:17,880
above 16 receive cues

00:44:16,530 --> 00:44:22,530
it's became a little marginal at that

00:44:17,880 --> 00:44:24,840
point yes Paul one of the fun things

00:44:22,530 --> 00:44:26,970
we've had with our Cu and some academic

00:44:24,840 --> 00:44:29,160
things is virtualization which you

00:44:26,970 --> 00:44:32,490
mentioned and that's one way you can

00:44:29,160 --> 00:44:33,690
sleep in an Arab handler is is that

00:44:32,490 --> 00:44:35,010
something that you have some way of

00:44:33,690 --> 00:44:41,730
taking account of or you're looking at

00:44:35,010 --> 00:44:44,730
it all so irq balance we the simple

00:44:41,730 --> 00:44:46,290
answer is no because once we're once

00:44:44,730 --> 00:44:48,210
we're inside of the the virtual machine

00:44:46,290 --> 00:44:49,800
we really don't we don't see a

00:44:48,210 --> 00:44:51,950
difference right we see an interrupt

00:44:49,800 --> 00:44:54,680
vector and it's at the kernels

00:44:51,950 --> 00:44:57,480
discretion as to how its going to handle

00:44:54,680 --> 00:44:59,160
sleeping we can balance and interrupt

00:44:57,480 --> 00:45:03,600
but it may not be a physical interrupt

00:44:59,160 --> 00:45:04,680
if it is an SR iov device of EF they are

00:45:03,600 --> 00:45:07,290
but they'll be going through the X to a

00:45:04,680 --> 00:45:10,140
pick instead and we can balance them

00:45:07,290 --> 00:45:11,960
within the the virtual machine but where

00:45:10,140 --> 00:45:16,050
they physically land after that and

00:45:11,960 --> 00:45:18,780
however the translation happens I think

00:45:16,050 --> 00:45:20,070
that's all we have time for could I get

00:45:18,780 --> 00:45:24,800
you please give a round of applause for

00:45:20,070 --> 00:45:24,800
Peter thank you

00:45:26,770 --> 00:45:28,830

YouTube URL: https://www.youtube.com/watch?v=hjMWVrqrt2U


