Title: "Linux Testing made better with DATA" - Nageswara Sastry (LCA 2021 Online)
Publication date: 2021-02-18
Playlist: Kernel Miniconf (LCA 2021)
Description: 
	Nageswara Sastry

https://lca2021.linux.org.au/schedule/presentation/103/

Data is fuel to any product transformation. Any useful Insights derived from data helps in reducing unnecessary work, doing things faster with more accuracy and efficiency. Linux Testing creates different types of data. The objective of this talk would be to present about how we can use data collected from code coverage to identify redundant test cases, reduce the number of test cases to execute, create dynamic test suite and prioritise test cases. We will show case significant reduction in time and effort for test engineers, developers as well as efficient hardware usage in the mentioned use cases.

Created a simple solution using ctags, code coverage data. In this solution used test case/suite reduction technique named code coverage method, this method helps in reducing 99% of the test cases with out effecting the bug identification capability. From the initial runs in our environment seen 93.5% reduction of test cases. Though the solution is simple, but effective in identifying redundant test cases, reducing number of test cases to execute, what tests to improve, which parts of the code needs new test cases.

linux.conf.au is a conference about the Linux operating system, and all aspects of the thriving ecosystem of Free and Open Source Software that has grown up around it. Run since 1999, in a different Australian or New Zealand city each year, by a team of local volunteers, LCA invites more than 500 people to learn from the people who shape the future of Open Source. For more information on the conference see https://linux.conf.au/

Produced by Next Day Video Australia: https://nextdayvideo.com.au

#linux.conf.au #linux #foss #opensource

Sat Jan 23 15:45:00 2021 at Tux Theatre
Captions: 
	00:00:10,820 --> 00:00:14,060
[Music]

00:00:16,000 --> 00:00:19,039
um

00:00:16,320 --> 00:00:19,520
up next we've got uh nagasurua sastry

00:00:19,039 --> 00:00:22,240
who

00:00:19,520 --> 00:00:24,560
is a software engineer at the ibm linux

00:00:22,240 --> 00:00:26,800
technology center in india

00:00:24,560 --> 00:00:28,320
who has an extensive background in linux

00:00:26,800 --> 00:00:30,480
test development

00:00:28,320 --> 00:00:32,719
he'll be speaking to us today on linux

00:00:30,480 --> 00:00:35,120
testing made better with data

00:00:32,719 --> 00:00:36,640
um due to the uh constraints of the time

00:00:35,120 --> 00:00:38,399
slot we're going to ask that you keep

00:00:36,640 --> 00:00:41,280
your questions to the

00:00:38,399 --> 00:00:41,680
colonel miniconf boss channel on venulis

00:00:41,280 --> 00:00:43,520
um

00:00:41,680 --> 00:00:45,760
and that way we'll be able to get to

00:00:43,520 --> 00:00:49,440
them after the talk is over

00:00:45,760 --> 00:00:52,399
and aggressor your the floor is yours

00:00:49,440 --> 00:00:54,879
thank you andrew yeah i'll start

00:00:52,399 --> 00:00:54,879
presenting

00:00:56,160 --> 00:01:00,480
hi all uh welcome welcome to the session

00:00:58,320 --> 00:01:01,039
named uh linux testing made better with

00:01:00,480 --> 00:01:03,520
data

00:01:01,039 --> 00:01:05,280
this is nagishwara from linux technology

00:01:03,520 --> 00:01:06,880
center ibm india

00:01:05,280 --> 00:01:08,799
data is fuel to any product

00:01:06,880 --> 00:01:11,200
transformation any useful

00:01:08,799 --> 00:01:13,040
insights derived from data helps in

00:01:11,200 --> 00:01:15,600
reducing unnecessary work

00:01:13,040 --> 00:01:17,600
doing things much faster with more

00:01:15,600 --> 00:01:20,400
accuracy and efficiency

00:01:17,600 --> 00:01:22,400
the objective of this talk is to present

00:01:20,400 --> 00:01:26,320
how to utilize the data collected from

00:01:22,400 --> 00:01:27,680
the next testing process so um for the

00:01:26,320 --> 00:01:29,520
next couple of minutes i'll take you

00:01:27,680 --> 00:01:30,479
through the agenda that is uh the

00:01:29,520 --> 00:01:32,640
problem statement

00:01:30,479 --> 00:01:33,759
what do we aim to achieve how did we

00:01:32,640 --> 00:01:36,640
achieve this and

00:01:33,759 --> 00:01:36,640
some results

00:01:37,200 --> 00:01:41,040
here is the problem statement current uh

00:01:39,759 --> 00:01:43,759
regression test speeds

00:01:41,040 --> 00:01:44,560
um are static what i meant by static

00:01:43,759 --> 00:01:46,320
here

00:01:44,560 --> 00:01:47,680
regression test fields defined in

00:01:46,320 --> 00:01:49,920
continuous regression

00:01:47,680 --> 00:01:50,720
framework that is here they are not

00:01:49,920 --> 00:01:52,960
going to change

00:01:50,720 --> 00:01:55,840
dynamically according to the patch or

00:01:52,960 --> 00:01:59,680
patch it what we wanted to test

00:01:55,840 --> 00:02:02,640
and turn around time for one iteration

00:01:59,680 --> 00:02:04,399
of running these regulation test feeds

00:02:02,640 --> 00:02:05,840
ranging from three hours to 12 hours at

00:02:04,399 --> 00:02:08,560
this moment

00:02:05,840 --> 00:02:09,920
based on the regression test which you

00:02:08,560 --> 00:02:12,800
select

00:02:09,920 --> 00:02:13,360
and we are not answering uh some of the

00:02:12,800 --> 00:02:16,080
questions

00:02:13,360 --> 00:02:16,959
at this moment how well we tested a

00:02:16,080 --> 00:02:19,599
particular

00:02:16,959 --> 00:02:21,520
piece of code what's the code coverage

00:02:19,599 --> 00:02:22,160
percentage and what is the code coverage

00:02:21,520 --> 00:02:25,200
efficiency

00:02:22,160 --> 00:02:27,040
of a test case which part of the code is

00:02:25,200 --> 00:02:29,360
not having test cases

00:02:27,040 --> 00:02:31,040
turnaround time for uh the path set

00:02:29,360 --> 00:02:35,519
already discussed so

00:02:31,040 --> 00:02:40,080
um with all these problems myself and

00:02:35,519 --> 00:02:43,599
an intern we started solving this

00:02:40,080 --> 00:02:46,480
what do we aim to achieve here so

00:02:43,599 --> 00:02:48,480
for a particular patch or path set run

00:02:46,480 --> 00:02:51,120
only the required test cases

00:02:48,480 --> 00:02:51,519
this improves turnaround time for the

00:02:51,120 --> 00:02:54,640
test

00:02:51,519 --> 00:02:55,040
cases uh results to be published and we

00:02:54,640 --> 00:02:57,280
have

00:02:55,040 --> 00:03:00,080
confidence that we are not going to miss

00:02:57,280 --> 00:03:03,920
any known bugs

00:03:00,080 --> 00:03:05,760
like and uh we are going to create um

00:03:03,920 --> 00:03:09,280
dynamic test suite

00:03:05,760 --> 00:03:11,200
and give insights to the testers

00:03:09,280 --> 00:03:13,360
like what are the areas of the source

00:03:11,200 --> 00:03:15,440
code need more test cases

00:03:13,360 --> 00:03:16,720
coverage efficiency of the test case

00:03:15,440 --> 00:03:19,519
this helps

00:03:16,720 --> 00:03:21,840
what test cases need to be improved and

00:03:19,519 --> 00:03:24,799
what are the new source files are added

00:03:21,840 --> 00:03:26,319
by that testers can write new test cases

00:03:24,799 --> 00:03:28,159
this whole process can

00:03:26,319 --> 00:03:29,760
save lots of systems as well as

00:03:28,159 --> 00:03:31,519
immediate time

00:03:29,760 --> 00:03:33,599
linking the test cases to the source

00:03:31,519 --> 00:03:36,080
code

00:03:33,599 --> 00:03:38,560
is done by running the test cases and

00:03:36,080 --> 00:03:41,120
collecting the source code coverage

00:03:38,560 --> 00:03:46,080
this data helps in deriving many useful

00:03:41,120 --> 00:03:49,360
insights with very less complexity

00:03:46,080 --> 00:03:52,239
okay before jumping into the

00:03:49,360 --> 00:03:52,720
actual project uh i want to cover some

00:03:52,239 --> 00:03:56,000
of the

00:03:52,720 --> 00:03:57,920
uh method what we used here so

00:03:56,000 --> 00:04:00,319
there are many different methods which

00:03:57,920 --> 00:04:02,319
reduce the test cases but

00:04:00,319 --> 00:04:04,640
out of which we picked up the coverage

00:04:02,319 --> 00:04:06,480
based method using this method

00:04:04,640 --> 00:04:08,799
the reduction rate of test case can

00:04:06,480 --> 00:04:10,159
reach up to 99

00:04:08,799 --> 00:04:13,040
though the number of test cases are

00:04:10,159 --> 00:04:15,760
reduced from thousands to couple of tens

00:04:13,040 --> 00:04:16,959
there is no reduction in fault finding

00:04:15,760 --> 00:04:19,280
capacity

00:04:16,959 --> 00:04:21,280
the procedure of this method has two

00:04:19,280 --> 00:04:22,240
simple steps that is calculate the code

00:04:21,280 --> 00:04:24,560
coverage

00:04:22,240 --> 00:04:26,240
and take the decision based on the code

00:04:24,560 --> 00:04:28,400
coverage value

00:04:26,240 --> 00:04:30,479
based on the code coverage value we are

00:04:28,400 --> 00:04:33,199
going to take the decisions like

00:04:30,479 --> 00:04:34,240
what to do with the test case when the

00:04:33,199 --> 00:04:36,960
code coverage is

00:04:34,240 --> 00:04:37,600
at a particular value these decisions

00:04:36,960 --> 00:04:40,720
helps

00:04:37,600 --> 00:04:42,639
in creating the required dynamic uh test

00:04:40,720 --> 00:04:43,440
suite creation and prioritizing the test

00:04:42,639 --> 00:04:45,759
cases

00:04:43,440 --> 00:04:46,720
eliminating their redundant test cases

00:04:45,759 --> 00:04:49,680
and there are

00:04:46,720 --> 00:04:50,080
many other methods from research or from

00:04:49,680 --> 00:04:53,120
the

00:04:50,080 --> 00:04:55,440
industry like program slicing

00:04:53,120 --> 00:04:57,199
genetic algorithm greedy algorithm fuzzy

00:04:55,440 --> 00:05:00,320
logic hybrid method

00:04:57,199 --> 00:05:02,560
clustering all these methods

00:05:00,320 --> 00:05:03,600
take more time to implement they are

00:05:02,560 --> 00:05:05,680
complex

00:05:03,600 --> 00:05:08,000
have less fault detection capabilities

00:05:05,680 --> 00:05:11,120
needs time to train models

00:05:08,000 --> 00:05:14,240
and some of uh the methods need more

00:05:11,120 --> 00:05:17,360
experimentation to customize

00:05:14,240 --> 00:05:20,400
so how did we

00:05:17,360 --> 00:05:23,039
achieve this so here is the

00:05:20,400 --> 00:05:24,000
uh overview of the whole project it

00:05:23,039 --> 00:05:26,080
divided into

00:05:24,000 --> 00:05:27,919
two different parts creation of the

00:05:26,080 --> 00:05:28,800
databases that is source and then the

00:05:27,919 --> 00:05:33,600
test

00:05:28,800 --> 00:05:35,360
and then passing the patch or path set

00:05:33,600 --> 00:05:36,960
but that we are going to get the

00:05:35,360 --> 00:05:40,400
required output

00:05:36,960 --> 00:05:42,720
so first uh the source database creation

00:05:40,400 --> 00:05:44,560
for the source database creation we used

00:05:42,720 --> 00:05:46,639
c tags that is a

00:05:44,560 --> 00:05:48,240
tool which helps in adding the code

00:05:46,639 --> 00:05:52,400
maintenance so

00:05:48,240 --> 00:05:55,199
this ctax creates an index file

00:05:52,400 --> 00:05:55,840
after running on the linux source code

00:05:55,199 --> 00:05:59,440
then

00:05:55,840 --> 00:06:01,120
passing this c index file what we

00:05:59,440 --> 00:06:02,800
picked up is the function name and then

00:06:01,120 --> 00:06:04,240
the file name and then stored it in the

00:06:02,800 --> 00:06:08,000
database

00:06:04,240 --> 00:06:10,319
then the test case database

00:06:08,000 --> 00:06:11,600
so here what we are doing is for each

00:06:10,319 --> 00:06:14,560
test feed

00:06:11,600 --> 00:06:16,400
run each test case and collect the

00:06:14,560 --> 00:06:20,240
functional coverage of the

00:06:16,400 --> 00:06:23,680
test cases then this data is

00:06:20,240 --> 00:06:25,199
passed um the tool used here is the

00:06:23,680 --> 00:06:27,360
g-code tool

00:06:25,199 --> 00:06:29,840
so the data from the g-code tool is

00:06:27,360 --> 00:06:32,720
passed and put into the database

00:06:29,840 --> 00:06:33,120
with uh different fields like the file

00:06:32,720 --> 00:06:35,199
name

00:06:33,120 --> 00:06:37,280
and then the function name along with

00:06:35,199 --> 00:06:40,000
the coverage and then the test case

00:06:37,280 --> 00:06:41,759
by that we can link the test cases with

00:06:40,000 --> 00:06:44,880
uh the source code

00:06:41,759 --> 00:06:46,880
and um out of these two this uh

00:06:44,880 --> 00:06:50,160
takes more time the test database

00:06:46,880 --> 00:06:50,160
creation takes more time

00:06:51,280 --> 00:06:55,520
and this is the key for all the insights

00:06:54,479 --> 00:06:58,479
what we are going to

00:06:55,520 --> 00:06:59,360
draw from it and then we are going to

00:06:58,479 --> 00:07:02,479
pass the

00:06:59,360 --> 00:07:03,680
patch or patch set from here we are

00:07:02,479 --> 00:07:06,960
going to extract

00:07:03,680 --> 00:07:09,520
the file names which are modified

00:07:06,960 --> 00:07:10,960
are added using this patch then we are

00:07:09,520 --> 00:07:13,280
going to

00:07:10,960 --> 00:07:14,000
check whether these files are present in

00:07:13,280 --> 00:07:16,720
the source

00:07:14,000 --> 00:07:17,360
database or not why this important

00:07:16,720 --> 00:07:20,960
because

00:07:17,360 --> 00:07:22,319
um tester we need to inform the testers

00:07:20,960 --> 00:07:25,440
about the new file

00:07:22,319 --> 00:07:26,960
when the file from the patch set is not

00:07:25,440 --> 00:07:29,280
available here

00:07:26,960 --> 00:07:31,440
then that means it is a new file and

00:07:29,280 --> 00:07:33,440
then the tester will be informed about

00:07:31,440 --> 00:07:35,520
the addition of new file from the patch

00:07:33,440 --> 00:07:37,039
and he can prepare the test cases for

00:07:35,520 --> 00:07:39,360
that

00:07:37,039 --> 00:07:41,120
and in the case of the existing file we

00:07:39,360 --> 00:07:43,120
are going to query the database

00:07:41,120 --> 00:07:44,800
and then retrieve the test cases that

00:07:43,120 --> 00:07:48,560
needs to be run

00:07:44,800 --> 00:07:50,639
and then given as an output

00:07:48,560 --> 00:07:52,160
the whole method uh explained we

00:07:50,639 --> 00:07:54,960
automated using jenkins

00:07:52,160 --> 00:07:55,680
and then after preparation of the two

00:07:54,960 --> 00:07:59,039
databases

00:07:55,680 --> 00:08:02,400
like source and then test

00:07:59,039 --> 00:08:05,199
simply pass the patch url um to the job

00:08:02,400 --> 00:08:07,759
and we get output as the what test

00:08:05,199 --> 00:08:09,919
stresses need to be done

00:08:07,759 --> 00:08:10,879
here i'll show you the source database

00:08:09,919 --> 00:08:14,160
schema

00:08:10,879 --> 00:08:16,879
just we have the tag that

00:08:14,160 --> 00:08:18,400
and here is the example uh record from

00:08:16,879 --> 00:08:22,319
the source database

00:08:18,400 --> 00:08:22,319
function name uh colon the file name

00:08:22,560 --> 00:08:26,319
and then here is the test database

00:08:25,280 --> 00:08:28,639
schema

00:08:26,319 --> 00:08:29,919
here we are taking the tag coverage

00:08:28,639 --> 00:08:33,680
percentage and then the

00:08:29,919 --> 00:08:35,519
test name the tag will look like

00:08:33,680 --> 00:08:36,800
the example record will look like this

00:08:35,519 --> 00:08:38,479
the file name colon

00:08:36,800 --> 00:08:41,839
the function name coverage percentage

00:08:38,479 --> 00:08:41,839
and then the test case name

00:08:42,640 --> 00:08:46,080
okay we have the data how we are going

00:08:44,800 --> 00:08:49,200
to identify

00:08:46,080 --> 00:08:50,160
the redundant test cases uh by looking

00:08:49,200 --> 00:08:53,440
at

00:08:50,160 --> 00:08:54,080
a particular tag with filename.c and

00:08:53,440 --> 00:08:56,640
then the function

00:08:54,080 --> 00:08:58,399
name and then if you see the code

00:08:56,640 --> 00:09:00,080
coverage percentage for a particular

00:08:58,399 --> 00:09:03,519
test case

00:09:00,080 --> 00:09:05,519
uh based on the data here we are going

00:09:03,519 --> 00:09:06,399
to identify the redundant test cases in

00:09:05,519 --> 00:09:09,680
this case uh

00:09:06,399 --> 00:09:13,040
example if you see all these test cases

00:09:09,680 --> 00:09:16,320
are having 85 percentage approximately

00:09:13,040 --> 00:09:20,560
with the exit vmax ops function

00:09:16,320 --> 00:09:22,560
so to cover 85 percentage

00:09:20,560 --> 00:09:24,959
no need to run all these test cases but

00:09:22,560 --> 00:09:26,800
simply run one test case is sufficient

00:09:24,959 --> 00:09:30,399
like that we can eliminate the redundant

00:09:26,800 --> 00:09:30,399
test cases that saves time

00:09:30,800 --> 00:09:37,839
okay how to know what test cases to be

00:09:34,839 --> 00:09:41,279
improved

00:09:37,839 --> 00:09:43,200
okay querying for a particular tag

00:09:41,279 --> 00:09:45,200
like filename dot c and then the

00:09:43,200 --> 00:09:47,120
function name with the highest code

00:09:45,200 --> 00:09:48,959
coverage along with the test case name

00:09:47,120 --> 00:09:50,959
will give

00:09:48,959 --> 00:09:52,320
the test case is covering that

00:09:50,959 --> 00:09:55,600
particular function at

00:09:52,320 --> 00:09:56,320
what level so based on the number we are

00:09:55,600 --> 00:09:58,399
going to

00:09:56,320 --> 00:09:59,839
uh take addition of improving that

00:09:58,399 --> 00:10:01,920
particular test case

00:09:59,839 --> 00:10:03,120
and after uh improving the test case

00:10:01,920 --> 00:10:06,079
next time also we can

00:10:03,120 --> 00:10:09,760
see how much the test case got improved

00:10:06,079 --> 00:10:09,760
uh in terms of the code coverage

00:10:10,880 --> 00:10:16,160
okay here are the results i have

00:10:16,560 --> 00:10:20,880
we uh in our environment we have

00:10:19,200 --> 00:10:23,200
approximately thousand test cases

00:10:20,880 --> 00:10:24,480
and after eliminating all the redundant

00:10:23,200 --> 00:10:27,360
test cases

00:10:24,480 --> 00:10:28,079
um we figured out like okay 129 test

00:10:27,360 --> 00:10:31,120
cases

00:10:28,079 --> 00:10:32,480
are necessary to get the same code

00:10:31,120 --> 00:10:34,240
coverage as

00:10:32,480 --> 00:10:35,519
uh we are getting it for the thousand

00:10:34,240 --> 00:10:37,360
test cases

00:10:35,519 --> 00:10:39,680
so no need to run the thousand test

00:10:37,360 --> 00:10:42,079
cases for so many hours

00:10:39,680 --> 00:10:45,120
by simply running 129 test cases will

00:10:42,079 --> 00:10:48,480
give the exact same code coverage

00:10:45,120 --> 00:10:51,600
and uh from uh

00:10:48,480 --> 00:10:54,720
observing important uh files uh

00:10:51,600 --> 00:10:57,600
like some we have picked up some 673 and

00:10:54,720 --> 00:11:00,560
then out of which 361 files having 54

00:10:57,600 --> 00:11:03,519
percentage of code coverage

00:11:00,560 --> 00:11:04,480
with 100 of code coverage with the case

00:11:03,519 --> 00:11:05,839
self test

00:11:04,480 --> 00:11:08,000
that is kernel self test and then

00:11:05,839 --> 00:11:11,040
avocado test

00:11:08,000 --> 00:11:12,240
so there is no improvement for these

00:11:11,040 --> 00:11:14,160
files or

00:11:12,240 --> 00:11:15,839
the test cases which are running to

00:11:14,160 --> 00:11:20,480
cover and then

00:11:15,839 --> 00:11:23,279
um we have a 312 files out of 673

00:11:20,480 --> 00:11:24,720
the core coverage is ranging from 96

00:11:23,279 --> 00:11:27,920
percent to 4 percentage

00:11:24,720 --> 00:11:29,279
so these are the files or test cases

00:11:27,920 --> 00:11:33,040
related to them

00:11:29,279 --> 00:11:36,480
are important to

00:11:33,040 --> 00:11:39,120
for the improvement or prioritizing the

00:11:36,480 --> 00:11:39,120
test cases

00:11:40,839 --> 00:11:44,720
etc

00:11:42,160 --> 00:11:45,440
so we have the disclaimer chart here and

00:11:44,720 --> 00:11:47,519
the message

00:11:45,440 --> 00:11:49,839
what i wanted to give here is with

00:11:47,519 --> 00:11:52,720
careful collection and analysis of data

00:11:49,839 --> 00:11:53,519
we can get more useful insights by that

00:11:52,720 --> 00:11:56,079
we can make

00:11:53,519 --> 00:11:58,079
uh linux testing better so we should

00:11:56,079 --> 00:12:00,720
make a deliberate practice of collecting

00:11:58,079 --> 00:12:01,440
data and methods to analyze the same by

00:12:00,720 --> 00:12:04,240
that

00:12:01,440 --> 00:12:06,399
we can get more useful insights to make

00:12:04,240 --> 00:12:07,920
uh the linux better

00:12:06,399 --> 00:12:10,320
thanks a lot for the opportunity to

00:12:07,920 --> 00:12:11,920
speak at the next world conference

00:12:10,320 --> 00:12:14,160
if you have any questions we can take

00:12:11,920 --> 00:12:14,160
now

00:12:16,800 --> 00:12:20,639
we do have a couple of minutes before

00:12:18,480 --> 00:12:21,680
the time slot ends so if anyone does

00:12:20,639 --> 00:12:23,600
have any questions

00:12:21,680 --> 00:12:25,839
uh please post it in the tux theater

00:12:23,600 --> 00:12:25,839
chat

00:12:50,839 --> 00:12:53,839
so

00:13:14,079 --> 00:13:17,200
okay it doesn't sound like we have any

00:13:15,680 --> 00:13:20,639
questions right at the moment

00:13:17,200 --> 00:13:23,600
um if anyone has anything they'd like to

00:13:20,639 --> 00:13:24,800
ask that they think of later uh head

00:13:23,600 --> 00:13:25,920
over head on over to the colonel

00:13:24,800 --> 00:13:28,399
miniconf boss

00:13:25,920 --> 00:13:29,760
channel which you can join on the left

00:13:28,399 --> 00:13:32,560
hand side of

00:13:29,760 --> 00:13:34,959
venulis and ask your questions there

00:13:32,560 --> 00:13:37,360
actually we just had a question come in

00:13:34,959 --> 00:13:39,440
um question from michael is there any

00:13:37,360 --> 00:13:42,399
way we can measure path coverage rather

00:13:39,440 --> 00:13:42,399
than line coverage

00:13:42,959 --> 00:13:49,120
okay i haven't explored that way because

00:13:46,079 --> 00:13:49,680
this is the kind of poc i have done we

00:13:49,120 --> 00:13:52,480
have

00:13:49,680 --> 00:13:53,360
taken the functional coverage not the

00:13:52,480 --> 00:14:03,839
path coverage

00:13:53,360 --> 00:14:03,839
but we can definitely work for that

00:14:10,639 --> 00:14:15,120
okay it doesn't look like we've got any

00:14:12,079 --> 00:14:18,160
more questions um so

00:14:15,120 --> 00:14:18,639
thank you nagathora uh for your talk

00:14:18,160 --> 00:14:20,720
today

00:14:18,639 --> 00:14:22,079
and if anyone has further questions head

00:14:20,720 --> 00:14:24,800
on over to the colonel miniconf

00:14:22,079 --> 00:14:26,639
channel and ask it there thank you

00:14:24,800 --> 00:14:30,320
thanks a lot

00:14:26,639 --> 00:14:32,399
we'll be taking a 10

00:14:30,320 --> 00:14:32,399

YouTube URL: https://www.youtube.com/watch?v=K95UAOAJ4hs


