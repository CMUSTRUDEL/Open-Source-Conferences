Title: Princiya Sequeira: Web based 3D Camera Simulator for Perceptual Computing - JSConf.Asia 2015
Publication date: 2015-12-19
Playlist: JSConf.Asia 2015
Description: 
	This talk is about a web based solution to simulate 3D cameras for prototyping, developing and testing perceptual computing, augmented reality applications without using a physical device. The simulator mimics all of the hardware features of a typical 3D camera like that of Intel’s RealSense or Microsoft’s Kinect. The solution is purely based on JavaScript and easily integrates with the 3D camera’s SDKs. Also, it is platform independent. 

Using this simulator, people can conduct workshops, seminars for a larger audience who would be intending to learn a new technology, but wouldn't be having the required hardware (RealSense / Kinect / Leap Motion). All the sci-fi stuff from movies is no more a fiction, and perceptual computing has made all of this transform into reality. So when all of this is happening, I would love to introduce before the audience the immense possibilities which could be explored through JavaScript and web.

Princiya is a technology and start-up enthusiast. JavaScript is her second love, food comes first ;). She likes coding, attending conferences, learning new things, meeting new people. Interested in visualizations, able to hack D3.js to death! An active member and JavaScript study group co-ordinator for WomenWhoCode Bangalore Chapter, Data Expert (Visualizations) for DataKind Bangalore Chapter. Currently she works as a Product Engineer for Guru-G Learning Labs, an ed-tech start-up, based out of Bangalore, India.

JSConf.Asia - Red Dot Design Museum, Singapore - 19 November 2015.

Source: http://2015.jsconf.asia
Slides: https://docs.google.com/presentation/d/1iA0m4JyN1wfRmW9UuVWwW9gnG9nWFmJV4IzcMtn24ps/edit?usp=sharing

License: For reuse of this video under a more permissive license please get in touch with us. The speakers retain the copyright for their performances.
Captions: 
	00:00:07,760 --> 00:00:17,220
so imagine this you grab your device

00:00:12,349 --> 00:00:20,789
open an app do some doodles midair watch

00:00:17,220 --> 00:00:24,359
your art appear on the screen is this

00:00:20,789 --> 00:00:27,720
the future no it's right around the

00:00:24,359 --> 00:00:30,420
corner and when you get to learn to

00:00:27,720 --> 00:00:34,980
build something like this you will call

00:00:30,420 --> 00:00:41,210
all of this amazing I am Prince iam and

00:00:34,980 --> 00:00:41,210
I love JavaScript so

00:00:41,380 --> 00:00:47,500
I am from Bangalore and I work for all

00:00:45,460 --> 00:00:50,620
education startup called guru ji

00:00:47,500 --> 00:00:55,180
learning labs I work on lodges building

00:00:50,620 --> 00:00:58,600
back in solutions yeah I participated

00:00:55,180 --> 00:01:01,780
the early morning today in this Jays

00:00:58,600 --> 00:01:04,979
party I was not sure because it was

00:01:01,780 --> 00:01:08,170
before my talk and I was already nervous

00:01:04,979 --> 00:01:12,180
but yes thanks to y'all lovely audience

00:01:08,170 --> 00:01:12,180
I think I just got lucky

00:01:12,190 --> 00:01:20,080
ok so the way we interacted with

00:01:16,420 --> 00:01:24,369
computers on a large scale was stuck in

00:01:20,080 --> 00:01:28,680
place for roughly 20 years from mouse -

00:01:24,369 --> 00:01:34,030
keyboard to joystick it is game over

00:01:28,680 --> 00:01:38,050
today it is the era of gestures today's

00:01:34,030 --> 00:01:41,259
gamers can do everything from slice and

00:01:38,050 --> 00:01:44,950
dice and fruit ninja to quest for a

00:01:41,259 --> 00:01:47,890
dragon and Skyrim from these early

00:01:44,950 --> 00:01:51,729
successes in 3d gesture-based

00:01:47,890 --> 00:01:58,060
input we now see high growth market

00:01:51,729 --> 00:02:01,270
emerging across multiple industries so

00:01:58,060 --> 00:02:05,590
this is what I am going to boil with for

00:02:01,270 --> 00:02:08,700
the next 30 minutes so by the end of

00:02:05,590 --> 00:02:12,280
this talk the such as computing

00:02:08,700 --> 00:02:15,970
augmented reality virtual reality will

00:02:12,280 --> 00:02:18,400
no more be alien to Yan any technology

00:02:15,970 --> 00:02:21,070
enthusiasts can start getting their

00:02:18,400 --> 00:02:25,540
hands dirty with these latest cool

00:02:21,070 --> 00:02:28,980
technology you get to learn to build the

00:02:25,540 --> 00:02:32,850
next generation of natural immersive

00:02:28,980 --> 00:02:37,090
intuitive apps hands and finger tracking

00:02:32,850 --> 00:02:41,410
facial recognition gesture analysis 3d

00:02:37,090 --> 00:02:44,320
scanning the list goes on and on now let

00:02:41,410 --> 00:02:48,010
me tell you the best thing you wouldn't

00:02:44,320 --> 00:02:50,859
need a hardware that is a 3d camera or a

00:02:48,010 --> 00:02:51,640
motion sensing device to test out the

00:02:50,859 --> 00:02:55,660
SDK

00:02:51,640 --> 00:02:58,360
Auto Bell a hello one sample I shall

00:02:55,660 --> 00:03:02,740
tell you some secret to test the SDK

00:02:58,360 --> 00:03:05,530
merely through JavaScript go and scold

00:03:02,740 --> 00:03:11,440
3d objects explore outer space or gain

00:03:05,530 --> 00:03:14,470
some magical powers okay so let's get

00:03:11,440 --> 00:03:18,970
into the basics for the stock 3d camera

00:03:14,470 --> 00:03:22,660
a type of camera with two or more lenses

00:03:18,970 --> 00:03:26,650
with separate image sensors a film frame

00:03:22,660 --> 00:03:29,680
for each lens 3d cameras give you the

00:03:26,650 --> 00:03:31,780
visual ability to perceive the world and

00:03:29,680 --> 00:03:36,640
the distance of an object in

00:03:31,780 --> 00:03:40,950
three-dimensional perceptual computing

00:03:36,640 --> 00:03:43,480
the ability for a computer to recognize

00:03:40,950 --> 00:03:46,570
what is going around it

00:03:43,480 --> 00:03:49,390
more specifically the computer can

00:03:46,570 --> 00:03:52,620
perceive the environment and the users

00:03:49,390 --> 00:03:52,620
in that environment

00:03:52,980 --> 00:03:58,690
augmented reality direct or indirect

00:03:57,070 --> 00:04:01,120
view of a physical real-world

00:03:58,690 --> 00:04:04,480
environment whose elements are augmented

00:04:01,120 --> 00:04:08,650
by computer-generated sensory inputs

00:04:04,480 --> 00:04:11,290
such as sound video graphics etc here a

00:04:08,650 --> 00:04:15,430
view of reality is modified by a

00:04:11,290 --> 00:04:18,160
computer as a result the technology

00:04:15,430 --> 00:04:20,410
functions by enhancing one's current

00:04:18,160 --> 00:04:24,850
perception of reality

00:04:20,410 --> 00:04:27,960
by contrast virtual reality replaces the

00:04:24,850 --> 00:04:31,030
real world with a stimulated one

00:04:27,960 --> 00:04:34,210
augmentation is conventionally in real

00:04:31,030 --> 00:04:42,340
time and in semantic context with

00:04:34,210 --> 00:04:44,770
environmental elements so we need to

00:04:42,340 --> 00:04:49,180
know more about these motion sensing

00:04:44,770 --> 00:04:53,010
input devices here are the few ones

00:04:49,180 --> 00:04:55,900
which I have tested out for the SDKs

00:04:53,010 --> 00:04:58,750
statutory warning here these are just

00:04:55,900 --> 00:05:02,600
listed in alphabetical order I am NOT

00:04:58,750 --> 00:05:04,490
representing any of these companies so

00:05:02,600 --> 00:05:08,240
virtual computing augmented reality

00:05:04,490 --> 00:05:10,490
virtual reality isn't new devices like

00:05:08,240 --> 00:05:13,520
these intel's realsense leap motion

00:05:10,490 --> 00:05:17,660
Microsoft Kinect has been around for a

00:05:13,520 --> 00:05:22,700
while you can control these devices with

00:05:17,660 --> 00:05:25,940
a wave a wink swipe and a smile with

00:05:22,700 --> 00:05:29,780
these you can scan real life things like

00:05:25,940 --> 00:05:34,300
a piece of art a child's toy all your

00:05:29,780 --> 00:05:38,420
own face and create a digital 3d version

00:05:34,300 --> 00:05:41,420
the technology behind these is around a

00:05:38,420 --> 00:05:45,800
webcam style added as a peripheral

00:05:41,420 --> 00:05:49,250
device it enables users to control and

00:05:45,800 --> 00:05:52,370
interact with their computer without the

00:05:49,250 --> 00:05:55,190
need for a game controller through a

00:05:52,370 --> 00:05:58,700
natural user interface using gestures

00:05:55,190 --> 00:06:02,390
and spoken commands these devices

00:05:58,700 --> 00:06:05,780
feature an RGB camera depth sensor and

00:06:02,390 --> 00:06:08,060
multi array microphone this technology

00:06:05,780 --> 00:06:11,720
could vary across devices because leap

00:06:08,060 --> 00:06:16,720
motion doesn't support wise or facial

00:06:11,720 --> 00:06:16,720
recognition Kinect and real sense does

00:06:17,230 --> 00:06:24,260
so let me tell you the motivation behind

00:06:20,360 --> 00:06:26,360
this idea few months back I was

00:06:24,260 --> 00:06:29,120
preparing for my talk on perceptual

00:06:26,360 --> 00:06:32,350
computing and I was supposed to give a

00:06:29,120 --> 00:06:36,800
demo on intel realsense JavaScript SDK

00:06:32,350 --> 00:06:39,350
this SDK works only on Windows two days

00:06:36,800 --> 00:06:43,580
before my talk windows in my laptop

00:06:39,350 --> 00:06:48,200
crashed yes I did not have a backup of

00:06:43,580 --> 00:06:51,280
this code and panic mode turned on this

00:06:48,200 --> 00:06:53,450
is when I thought of this web simulator

00:06:51,280 --> 00:06:55,100
by the way when I was preparing these

00:06:53,450 --> 00:06:57,680
slides I used Google Docs

00:06:55,100 --> 00:07:00,740
I didn't want any sort of laptop mushafs

00:06:57,680 --> 00:07:02,780
hacking in a foreign country I also

00:07:00,740 --> 00:07:05,120
thought that would be nice to conduct

00:07:02,780 --> 00:07:07,610
workshops seminars for a larger audience

00:07:05,120 --> 00:07:10,490
who would be intending to learn a new

00:07:07,610 --> 00:07:13,380
technology but wouldn't be having the

00:07:10,490 --> 00:07:16,530
required hardware C or real sense

00:07:13,380 --> 00:07:21,480
motion or a Kinect that is well this web

00:07:16,530 --> 00:07:24,570
simulator is very helpful when idea

00:07:21,480 --> 00:07:27,020
clicked and started wondering how is it

00:07:24,570 --> 00:07:29,850
that an external motion sensing device

00:07:27,020 --> 00:07:33,240
interacts with the computer natural

00:07:29,850 --> 00:07:37,650
question all these 3d cameras are

00:07:33,240 --> 00:07:41,400
connected through a USB port and the 3d

00:07:37,650 --> 00:07:46,050
camera SDKs access the 3d camera through

00:07:41,400 --> 00:07:48,770
the USB port okay so far so good we have

00:07:46,050 --> 00:07:51,870
one thing in common that is the USB port

00:07:48,770 --> 00:07:53,550
now all these SDKs have one thing

00:07:51,870 --> 00:07:56,760
another thing in common for that

00:07:53,550 --> 00:08:00,330
applications that is JavaScript is the

00:07:56,760 --> 00:08:02,760
primary language and in addition there

00:08:00,330 --> 00:08:06,030
is another thing common which is used to

00:08:02,760 --> 00:08:10,400
transmit the data from the camera to the

00:08:06,030 --> 00:08:14,610
computer and this is done using

00:08:10,400 --> 00:08:18,240
WebSockets so this is my secret these

00:08:14,610 --> 00:08:21,210
SDKs create a WebSocket which is part of

00:08:18,240 --> 00:08:26,550
the whole application and the secret

00:08:21,210 --> 00:08:30,690
behind the simulator is creating nodejs

00:08:26,550 --> 00:08:34,560
based WebSocket server which replaces

00:08:30,690 --> 00:08:36,900
the default one used by the SDKs that's

00:08:34,560 --> 00:08:40,020
it that's the secret so far the

00:08:36,900 --> 00:08:42,300
simulator for my simulator I create a

00:08:40,020 --> 00:08:45,300
noches WebSocket streaming server and

00:08:42,300 --> 00:08:51,360
replace it with the default one which is

00:08:45,300 --> 00:08:54,420
used by the SDKs so let's try to

00:08:51,360 --> 00:08:57,270
understand what a WebSocket is the

00:08:54,420 --> 00:08:59,910
explanation might seem too technical so

00:08:57,270 --> 00:09:04,440
to put it simple data can be sent

00:08:59,910 --> 00:09:07,920
between client and server the WebSocket

00:09:04,440 --> 00:09:11,490
specification was developed as part of

00:09:07,920 --> 00:09:17,790
the html5 initiative and this introduced

00:09:11,490 --> 00:09:19,760
the WebSocket JavaScript interface here

00:09:17,790 --> 00:09:23,150
is some code for a WebSocket

00:09:19,760 --> 00:09:25,100
implementation to connect to an endpoint

00:09:23,150 --> 00:09:27,710
just create

00:09:25,100 --> 00:09:31,190
a new WebSocket instance the first line

00:09:27,710 --> 00:09:35,060
they're providing the new object with

00:09:31,190 --> 00:09:38,800
the URL that represents the endpoint to

00:09:35,060 --> 00:09:42,650
which you wish to connect as shown here

00:09:38,800 --> 00:09:44,300
one thing to notice here is that to

00:09:42,650 --> 00:09:48,170
whoever is new to WebSockets

00:09:44,300 --> 00:09:51,860
a WebSocket connection is established by

00:09:48,170 --> 00:09:54,740
upgrading from the HTTP protocol to the

00:09:51,860 --> 00:09:57,020
WebSockets protocol during the initial

00:09:54,740 --> 00:10:00,350
handshake between the client and the

00:09:57,020 --> 00:10:03,980
server which is why the URL begins with

00:10:00,350 --> 00:10:08,120
the ws and not whether HTTP for secure

00:10:03,980 --> 00:10:11,840
it would be WSS before connecting to an

00:10:08,120 --> 00:10:15,440
endpoint and sending a message you can

00:10:11,840 --> 00:10:17,840
associate a series of event listeners to

00:10:15,440 --> 00:10:21,200
handle each face of the connection

00:10:17,840 --> 00:10:23,900
lifecycle as shown here so we have a

00:10:21,200 --> 00:10:27,710
bunch of event listeners like an open on

00:10:23,900 --> 00:10:30,950
message on clothes on error as well we

00:10:27,710 --> 00:10:33,980
can include that now to send a message

00:10:30,950 --> 00:10:34,490
to the server from the client simply

00:10:33,980 --> 00:10:37,430
called

00:10:34,490 --> 00:10:40,250
send the last orbit the last code

00:10:37,430 --> 00:10:43,070
snippet here and provide the content you

00:10:40,250 --> 00:10:46,160
wish to deliver after sending the

00:10:43,070 --> 00:10:49,580
message call close to terminate the

00:10:46,160 --> 00:10:52,880
connection as you can see it really

00:10:49,580 --> 00:10:55,640
couldn't be much easier just do this

00:10:52,880 --> 00:11:02,840
WebSocket implementation and we can test

00:10:55,640 --> 00:11:05,780
out the 3d camera SDKs so I am uploaded

00:11:02,840 --> 00:11:06,590
the code here and I will share my slice

00:11:05,780 --> 00:11:09,470
shortly

00:11:06,590 --> 00:11:12,560
the last windows crashed with a teach me

00:11:09,470 --> 00:11:16,040
this lesson and since then whenever I've

00:11:12,560 --> 00:11:19,220
been working I make a backup but my

00:11:16,040 --> 00:11:21,080
github reputation is quite bad I am sort

00:11:19,220 --> 00:11:22,900
of a new one to the open this

00:11:21,080 --> 00:11:28,190
contribution

00:11:22,900 --> 00:11:29,900
yeah I have told myself that I need to

00:11:28,190 --> 00:11:31,820
be active and do more sort of these

00:11:29,900 --> 00:11:33,040
contributions so coming back to this

00:11:31,820 --> 00:11:35,139
code I have worked

00:11:33,040 --> 00:11:37,779
demos uploaded for real sense and leap

00:11:35,139 --> 00:11:44,079
motion and I'll upload the code related

00:11:37,779 --> 00:11:45,970
to connect shortly so to pick a few good

00:11:44,079 --> 00:11:47,740
snippets which could be the starting

00:11:45,970 --> 00:11:51,790
point for y'all to get started

00:11:47,740 --> 00:11:54,569
couple of things here each SDK has its

00:11:51,790 --> 00:11:58,360
own comfortable ways of receiving data

00:11:54,569 --> 00:12:01,089
leap motion what it does is captures the

00:11:58,360 --> 00:12:04,269
frames from the controller writes the

00:12:01,089 --> 00:12:08,500
JSON data to a file and sends this JSON

00:12:04,269 --> 00:12:10,569
output over WebSockets so the JSON data

00:12:08,500 --> 00:12:15,430
is pretty big and it looks something

00:12:10,569 --> 00:12:18,100
like this it's a huge one so Lee portion

00:12:15,430 --> 00:12:19,180
what it does is captures the frames from

00:12:18,100 --> 00:12:22,300
the controller the leap motion

00:12:19,180 --> 00:12:25,029
controller writes this JSON data to a

00:12:22,300 --> 00:12:28,420
file and sends this JSON output over

00:12:25,029 --> 00:12:31,389
WebSockets so a leap motion expects a

00:12:28,420 --> 00:12:33,970
pre-processed json input whereas

00:12:31,389 --> 00:12:37,779
realsense on the other hand expects the

00:12:33,970 --> 00:12:40,810
raw image frames over WebSockets the SDK

00:12:37,779 --> 00:12:43,209
then processes this data and performs

00:12:40,810 --> 00:12:46,510
facial gesture recognition algorithms

00:12:43,209 --> 00:12:48,850
which are part of the SDK the difference

00:12:46,510 --> 00:12:52,389
to be noted here is that leap motion

00:12:48,850 --> 00:12:54,670
expects the WebSocket server to process

00:12:52,389 --> 00:12:57,940
the raw data and generate meaningful

00:12:54,670 --> 00:13:01,149
JSON output like this whereas realsense

00:12:57,940 --> 00:13:03,930
takes only the raw data from web socket

00:13:01,149 --> 00:13:07,360
and the SDK then processes this raw data

00:13:03,930 --> 00:13:13,149
the complex facial gesture algorithms

00:13:07,360 --> 00:13:16,630
are part of the SDK independent SDKs so

00:13:13,149 --> 00:13:20,260
this is from realsense dot JS file after

00:13:16,630 --> 00:13:23,050
you have downloaded the realsense sdk so

00:13:20,260 --> 00:13:24,490
you can search for socket in the file

00:13:23,050 --> 00:13:25,930
you can search for the keyword socket

00:13:24,490 --> 00:13:29,110
and the file and you'll find something

00:13:25,930 --> 00:13:31,000
like this so here we have socket URL

00:13:29,110 --> 00:13:32,740
which is an array and it tries

00:13:31,000 --> 00:13:35,019
connecting to these a bunch of for

00:13:32,740 --> 00:13:37,569
WebSocket URLs this is how you make a

00:13:35,019 --> 00:13:40,630
WebSocket server call as we already

00:13:37,569 --> 00:13:44,410
discussed so instead of HTTP it would be

00:13:40,630 --> 00:13:48,940
our WS and then you can associate any

00:13:44,410 --> 00:13:50,529
port number the last few lines I am

00:13:48,940 --> 00:13:52,269
showing you all the WebSocket methods

00:13:50,529 --> 00:13:56,050
and the event handlers so this is

00:13:52,269 --> 00:13:59,589
exactly what appears in the SDK on open

00:13:56,050 --> 00:14:02,110
on message on error on closed special

00:13:59,589 --> 00:14:05,350
thing to note here is binary type which

00:14:02,110 --> 00:14:07,660
is set to array buffer this needs to be

00:14:05,350 --> 00:14:10,750
done for WebSockets when you saw it sent

00:14:07,660 --> 00:14:13,509
raw data because the real cells expects

00:14:10,750 --> 00:14:20,980
the raw data over the web sockets which

00:14:13,509 --> 00:14:23,319
is then processed by the SDK this is

00:14:20,980 --> 00:14:26,220
what you'll get from leap dodges after

00:14:23,319 --> 00:14:28,649
you have downloaded the leap motion SDK

00:14:26,220 --> 00:14:32,500
pretty much similar to the previous one

00:14:28,649 --> 00:14:34,990
expect except the data comes pre

00:14:32,500 --> 00:14:37,420
processed that is in JSON format here

00:14:34,990 --> 00:14:40,000
and no array buffer setting is needed

00:14:37,420 --> 00:14:41,709
for WebSockets so as I told this is this

00:14:40,000 --> 00:14:44,050
could be the main difference while

00:14:41,709 --> 00:14:47,529
trying out the different SDKs the

00:14:44,050 --> 00:14:50,100
realsense expects raw data which you why

00:14:47,529 --> 00:14:55,300
you'll have to set the array buffer type

00:14:50,100 --> 00:14:59,550
to binary and the leap motion uses the

00:14:55,300 --> 00:14:59,550
JSON output pre-process to JSON output

00:15:00,360 --> 00:15:10,720
so this is how you are right on OGS

00:15:06,279 --> 00:15:12,670
WebSocket server now after no J's put

00:15:10,720 --> 00:15:15,220
javascript firmly on the back end and

00:15:12,670 --> 00:15:16,689
angularjs made us realized that client

00:15:15,220 --> 00:15:18,689
business logic needs to be moved

00:15:16,689 --> 00:15:21,069
entirely on the client-side

00:15:18,689 --> 00:15:24,459
possibilities for JavaScript were

00:15:21,069 --> 00:15:28,600
endless javascript phones robots and

00:15:24,459 --> 00:15:31,269
whatnot here uh no J's based WebSocket

00:15:28,600 --> 00:15:34,300
server has been implemented for this web

00:15:31,269 --> 00:15:37,180
simulator the port number as you'll see

00:15:34,300 --> 00:15:39,639
and the second line there is changeable

00:15:37,180 --> 00:15:42,040
we need to replace this value with the

00:15:39,639 --> 00:15:44,170
port number used by the SDK so the leaf

00:15:42,040 --> 00:15:46,000
motion on default the port number was

00:15:44,170 --> 00:15:48,790
six four three seven or six four three

00:15:46,000 --> 00:15:52,569
six or intel's realsense it was the five

00:15:48,790 --> 00:15:54,400
digit number the clients per a represent

00:15:52,569 --> 00:15:57,400
us the SDKs

00:15:54,400 --> 00:15:59,950
the connect to this web server this is a

00:15:57,400 --> 00:16:02,590
client-server model so Aaron

00:15:59,950 --> 00:16:07,600
this server is written in node.js and

00:16:02,590 --> 00:16:10,450
the clients are one of the 3d camera SDK

00:16:07,600 --> 00:16:10,990
so architecture looks something like

00:16:10,450 --> 00:16:12,600
this

00:16:10,990 --> 00:16:15,760
it's a client-server architecture

00:16:12,600 --> 00:16:18,490
wherein the server is this noches

00:16:15,760 --> 00:16:21,190
WebSocket server and the clients could

00:16:18,490 --> 00:16:23,980
be the SDKs plus it needs to be one of

00:16:21,190 --> 00:16:27,880
the browser which actually detects the

00:16:23,980 --> 00:16:30,600
native camera now in html5 we can access

00:16:27,880 --> 00:16:35,170
the native camera using getusermedia and

00:16:30,600 --> 00:16:36,820
so once you turn on this sdk you so the

00:16:35,170 --> 00:16:40,200
server will be listening it's a node.js

00:16:36,820 --> 00:16:44,680
server and you open a HTML browser

00:16:40,200 --> 00:16:46,530
wherein you have some code which will be

00:16:44,680 --> 00:16:50,380
the client which has the getusermedia

00:16:46,530 --> 00:16:52,480
which captures the user flames that is

00:16:50,380 --> 00:16:55,000
which captures the frames from the

00:16:52,480 --> 00:16:57,310
native camera and then transmits this

00:16:55,000 --> 00:17:00,040
binary data for your sense or rough

00:16:57,310 --> 00:17:03,580
processes it has a JSON and sends it to

00:17:00,040 --> 00:17:06,580
the WebSocket server so since this is a

00:17:03,580 --> 00:17:09,010
client-server architecture this camera

00:17:06,580 --> 00:17:11,440
client sends data to the web socket and

00:17:09,010 --> 00:17:13,870
this web socket in turn sends this data

00:17:11,440 --> 00:17:16,300
to the client to the next client which

00:17:13,870 --> 00:17:19,540
is our SDK that is it could be the real

00:17:16,300 --> 00:17:21,220
sense or the SDK so that's all you

00:17:19,540 --> 00:17:24,070
capture the input from the native camera

00:17:21,220 --> 00:17:27,360
process it and give it to the SDK so

00:17:24,070 --> 00:17:30,670
once these SDKs get the data it's now

00:17:27,360 --> 00:17:33,760
quite easy to test these SDKs go through

00:17:30,670 --> 00:17:37,750
the code or do some manipulations use

00:17:33,760 --> 00:17:40,540
some gestures or use some keyboard

00:17:37,750 --> 00:17:43,450
controls instead of gestures one thing

00:17:40,540 --> 00:17:45,850
is this simulator right now doesn't

00:17:43,450 --> 00:17:48,100
replace the holes or Hardware simulator

00:17:45,850 --> 00:17:51,160
that is it's not able to identify depth

00:17:48,100 --> 00:17:53,440
but if somebody gives you or what

00:17:51,160 --> 00:17:55,270
exactly the depth information comes we

00:17:53,440 --> 00:17:56,860
can just tweak these values and then

00:17:55,270 --> 00:17:58,990
maybe lights we can build some other

00:17:56,860 --> 00:18:02,170
applications say or just your controls

00:17:58,990 --> 00:18:05,290
not your head or like wave and then

00:18:02,170 --> 00:18:06,210
change ppts or do some cool animations

00:18:05,290 --> 00:18:08,520
on the sly

00:18:06,210 --> 00:18:12,450
and so on that is exactly what sleep

00:18:08,520 --> 00:18:14,460
motion or the real sense does so now

00:18:12,450 --> 00:18:16,200
we're coming back to my code I need to

00:18:14,460 --> 00:18:18,150
assemble all the code and make this as a

00:18:16,200 --> 00:18:19,890
universal simulator right now it's three

00:18:18,150 --> 00:18:25,800
different versions sitting for like real

00:18:19,890 --> 00:18:28,950
sense if motion or Microsoft Kinect so

00:18:25,800 --> 00:18:32,010
lastly you can go and Skutt some 3d

00:18:28,950 --> 00:18:37,830
objects explore outer space and gain

00:18:32,010 --> 00:18:43,620
some magical powers okay

00:18:37,830 --> 00:18:46,410
I still have some time and if you all

00:18:43,620 --> 00:18:49,560
have any questions y'all can ask me I

00:18:46,410 --> 00:18:51,510
would be happy to answer here or I could

00:18:49,560 --> 00:18:55,110
take it offline give you show you the

00:18:51,510 --> 00:18:58,890
demo what I have you could also tweet me

00:18:55,110 --> 00:19:01,200
at Lindsey and ask or yeah I would thank

00:18:58,890 --> 00:19:03,480
Jess can't Asia for giving me this

00:19:01,200 --> 00:19:06,870
wonderful opportunity to come here and

00:19:03,480 --> 00:19:11,550
present here I hope I didn't bore y'all

00:19:06,870 --> 00:19:13,500
I was not sure what to be shown on the

00:19:11,550 --> 00:19:16,260
slides or how much of the demo needs to

00:19:13,500 --> 00:19:18,390
be shown so just to prevent any shots of

00:19:16,260 --> 00:19:20,580
mushafs I don't have working demo but

00:19:18,390 --> 00:19:21,960
yes I have a working demo but I've not

00:19:20,580 --> 00:19:24,570
included part of the presentation

00:19:21,960 --> 00:19:27,330
because the gestures of svehla getting

00:19:24,570 --> 00:19:30,600
to Ori when it came to the light but yes

00:19:27,330 --> 00:19:32,760
I have some cool just obeys the demos or

00:19:30,600 --> 00:19:35,220
like swiping based demos y'all can come

00:19:32,760 --> 00:19:36,330
and check it out I'll be present here

00:19:35,220 --> 00:19:39,870
today tomorrow

00:19:36,330 --> 00:19:42,990
please come and feel free to bother me I

00:19:39,870 --> 00:19:47,660
would love to and yeah any questions

00:19:42,990 --> 00:19:47,660

YouTube URL: https://www.youtube.com/watch?v=vgWonzhZIww


