Title: Governing Artificial Intelligence Bias: International Women's Day - JS Monthly - March 2020
Publication date: 2020-03-10
Playlist: JS Monthly London
Description: 
	The impact of being a Women behind the Technology // Daphne Coates
_

About Pusher Sessions:

We're bringing the meetup to you. With Sessions, you can watch recordings of top-notch talks from developer meetups -- wherever and whenever you want.

Meetups are a great way to learn from our peers and to keep up with the latest trends and technologies. As developers ourselves, we at Pusher wanted to bring this great content to more people... So we built Sessions. On Sessions, you can watch talks that interest you and subscribe to be notified when new content gets added.

If you run a meetup and want to get involved, kindly get in touch.

_

About Pusher:

Pusher is a hosted service with APIs, developer tools and open source libraries that greatly simplify integrating real-time functionality into web and mobile applications. 

Pusher will automatically scale when required, removing all the pain of setting up and maintaining a secure, real-time infrastructure. 

Pusher is already trusted to do so by thousands of developers and companies like GitHub, MailChimp, the Financial Times, Buffer and many more. 

Getting started takes just a few seconds: simply go to pusher.com and create a free account. Happy hacking!
Captions: 
	00:00:00,659 --> 00:00:05,580
so today I want to talk to you about

00:00:02,689 --> 00:00:07,410
artificial intelligence and the impact

00:00:05,580 --> 00:00:09,030
that is having on all of your lives some

00:00:07,410 --> 00:00:10,920
of it you may know about some of it you

00:00:09,030 --> 00:00:13,170
may not know about and how we can

00:00:10,920 --> 00:00:17,340
mitigate and govern the bias that we see

00:00:13,170 --> 00:00:19,050
in our day-to-day lives so the question

00:00:17,340 --> 00:00:21,170
I have for everyone here is does AI

00:00:19,050 --> 00:00:27,029
discriminate against women

00:00:21,170 --> 00:00:29,699
hey shocker so one of the statistics

00:00:27,029 --> 00:00:31,830
that really got me into this in the

00:00:29,699 --> 00:00:34,320
first place was that men are six times

00:00:31,830 --> 00:00:36,660
more likely to receive targeted ads for

00:00:34,320 --> 00:00:39,059
higher paying and executive jobs than

00:00:36,660 --> 00:00:41,070
women and I was like hold on a second

00:00:39,059 --> 00:00:42,930
I'm missing out on opportunities here

00:00:41,070 --> 00:00:45,719
that I don't even know I'm missing out

00:00:42,930 --> 00:00:48,390
on in the first place you know how is

00:00:45,719 --> 00:00:50,250
this fair you probably might have seen

00:00:48,390 --> 00:00:52,440
this one in the news regarding Amazon

00:00:50,250 --> 00:00:54,510
where they built a hiring AI that

00:00:52,440 --> 00:00:55,980
systematically discriminated against

00:00:54,510 --> 00:00:59,699
women you could have something like

00:00:55,980 --> 00:01:01,829
women's Chess Club captain CV out denied

00:00:59,699 --> 00:01:05,010
and that was because it was built on

00:01:01,829 --> 00:01:08,010
past successful hires second shocker

00:01:05,010 --> 00:01:10,530
they were all men therefore how can we

00:01:08,010 --> 00:01:13,290
build an inclusive and diverse hiring

00:01:10,530 --> 00:01:15,360
system if it's based off past data where

00:01:13,290 --> 00:01:18,350
all the successful hires were from one

00:01:15,360 --> 00:01:23,390
category so the second question does AI

00:01:18,350 --> 00:01:28,650
discriminate against everyone yes

00:01:23,390 --> 00:01:30,270
not just women so if we look at other

00:01:28,650 --> 00:01:31,439
categories I'm going to start with women

00:01:30,270 --> 00:01:35,040
cuz obviously we're talking about

00:01:31,439 --> 00:01:36,960
International Women's Day but if we look

00:01:35,040 --> 00:01:39,030
at this more generally lenders have been

00:01:36,960 --> 00:01:40,770
found to charge up to three point five

00:01:39,030 --> 00:01:43,530
percent higher interest rates to

00:01:40,770 --> 00:01:46,020
minorities which again is based on past

00:01:43,530 --> 00:01:47,490
data but what does that have to do on

00:01:46,020 --> 00:01:49,350
your credit score in your credit rating

00:01:47,490 --> 00:01:50,909
shouldn't it be based on things like the

00:01:49,350 --> 00:01:55,350
age of your account and your ability to

00:01:50,909 --> 00:01:58,380
repay another thing to do with hiring is

00:01:55,350 --> 00:02:00,960
that people with European American names

00:01:58,380 --> 00:02:03,030
with 50% more likely to get a job

00:02:00,960 --> 00:02:04,020
interview than those with African

00:02:03,030 --> 00:02:06,060
American names

00:02:04,020 --> 00:02:09,509
despite the Seavey's being completely

00:02:06,060 --> 00:02:12,750
identical again inherent bias and

00:02:09,509 --> 00:02:13,740
discrimination so the question I then

00:02:12,750 --> 00:02:16,560
ask is will

00:02:13,740 --> 00:02:18,740
why is this happening and this is all

00:02:16,560 --> 00:02:21,540
down to something called bias and

00:02:18,740 --> 00:02:22,410
unfortunately it's an inherently human

00:02:21,540 --> 00:02:25,140
construct

00:02:22,410 --> 00:02:28,050
we all have bias we all have prejudice

00:02:25,140 --> 00:02:30,000
and we will make cognitive shortcuts and

00:02:28,050 --> 00:02:32,850
assumptions to make decisions

00:02:30,000 --> 00:02:34,470
that's just how we operate the problem

00:02:32,850 --> 00:02:36,480
is is that when we're trying to build

00:02:34,470 --> 00:02:39,270
artificial intelligence and sort of

00:02:36,480 --> 00:02:41,310
visualizing this from a human brain into

00:02:39,270 --> 00:02:44,400
an artificial intelligence system we

00:02:41,310 --> 00:02:46,290
translate those biases so we can define

00:02:44,400 --> 00:02:48,510
it as an unintended algorithmic

00:02:46,290 --> 00:02:51,060
preference derived from behaviors

00:02:48,510 --> 00:02:53,180
reflecting implicit prejudice reflecting

00:02:51,060 --> 00:02:56,490
in unfair illegal or unethical

00:02:53,180 --> 00:02:58,890
implications so the second problem this

00:02:56,490 --> 00:03:02,130
then says is well how do we translate

00:02:58,890 --> 00:03:05,070
human constructs into an artificially

00:03:02,130 --> 00:03:07,590
intelligent system one of them that we

00:03:05,070 --> 00:03:10,320
can look at is fairness again an

00:03:07,590 --> 00:03:12,360
inherently human concept how do you tell

00:03:10,320 --> 00:03:15,060
a system what's fair and what's not fair

00:03:12,360 --> 00:03:16,860
so I'm not going to go into any of this

00:03:15,060 --> 00:03:18,810
but just to show you that when you're

00:03:16,860 --> 00:03:21,840
creating something that's human and

00:03:18,810 --> 00:03:23,520
translating it into a machine you have

00:03:21,840 --> 00:03:26,250
to define what you mean by these

00:03:23,520 --> 00:03:28,470
constructs are we looking at including

00:03:26,250 --> 00:03:30,900
sensitive attributes are we excluding

00:03:28,470 --> 00:03:33,240
them does a counterfactual value replace

00:03:30,900 --> 00:03:36,450
the sensitive attribute we have to now

00:03:33,240 --> 00:03:38,010
work at defining how we take human

00:03:36,450 --> 00:03:41,730
things and put them into an intelligent

00:03:38,010 --> 00:03:43,560
system so today I wanted to briefly go

00:03:41,730 --> 00:03:46,950
over four different areas from where

00:03:43,560 --> 00:03:48,930
bias can stem from in the datasets with

00:03:46,950 --> 00:03:51,680
all of you here who are programmers you

00:03:48,930 --> 00:03:56,430
are all responsible for this issue

00:03:51,680 --> 00:03:58,830
algorithmic bias and black box AI so if

00:03:56,430 --> 00:04:00,840
we start with dataset bias and we look

00:03:58,830 --> 00:04:04,370
at one of the tests that was conducted a

00:04:00,840 --> 00:04:06,720
web crawler ought automatically index

00:04:04,370 --> 00:04:08,280
840 billion words from the internet

00:04:06,720 --> 00:04:11,520
bearing in mind the Internet is

00:04:08,280 --> 00:04:13,290
user-generated content so conducting a

00:04:11,520 --> 00:04:15,540
word embedding Association test that

00:04:13,290 --> 00:04:16,440
looks at how close the two words are

00:04:15,540 --> 00:04:19,020
linked together

00:04:16,440 --> 00:04:20,670
it found that managed programmer as

00:04:19,020 --> 00:04:23,160
women is the homemaker

00:04:20,670 --> 00:04:25,110
again we made this internet this is

00:04:23,160 --> 00:04:27,030
where we're getting our training data

00:04:25,110 --> 00:04:29,460
from again

00:04:27,030 --> 00:04:33,120
a picture of google translate into

00:04:29,460 --> 00:04:35,940
Turkish we put in he is a nurse she is a

00:04:33,120 --> 00:04:38,250
doctor Google Translate obviously is fed

00:04:35,940 --> 00:04:39,930
on training data from the internet that

00:04:38,250 --> 00:04:40,970
come out as she is a nurse he is a

00:04:39,930 --> 00:04:43,860
doctor

00:04:40,970 --> 00:04:46,290
clearly you can see here that the data

00:04:43,860 --> 00:04:48,300
sets that are systems are being based on

00:04:46,290 --> 00:04:50,580
if they're being taken from the web

00:04:48,300 --> 00:04:53,400
which is user-generated content it's

00:04:50,580 --> 00:04:56,760
going to have inherent biases systemic

00:04:53,400 --> 00:04:59,340
from our past so where else can we get

00:04:56,760 --> 00:05:01,260
data set bias from like I said based on

00:04:59,340 --> 00:05:04,560
past data with the Amazon the example

00:05:01,260 --> 00:05:06,630
past hires how old is our data how

00:05:04,560 --> 00:05:08,490
public is it was it for your specific

00:05:06,630 --> 00:05:11,669
purpose of your project or did it have a

00:05:08,490 --> 00:05:13,770
wider reach how much diversity have you

00:05:11,669 --> 00:05:16,980
got in your data how representative is

00:05:13,770 --> 00:05:19,530
it of today's population how much data

00:05:16,980 --> 00:05:22,530
integrity do you have and how accurate

00:05:19,530 --> 00:05:25,080
are your sources and also how much data

00:05:22,530 --> 00:05:27,780
do you have per sensitive attribute and

00:05:25,080 --> 00:05:29,610
then I will make assumptions if it

00:05:27,780 --> 00:05:31,620
doesn't have enough data to make an

00:05:29,610 --> 00:05:33,479
informed decision it will make

00:05:31,620 --> 00:05:35,970
assumptions based on the data that you

00:05:33,479 --> 00:05:38,070
have the most about predominantly if

00:05:35,970 --> 00:05:40,620
we're going Vijender it will be male

00:05:38,070 --> 00:05:43,020
especially for visual recognition women

00:05:40,620 --> 00:05:47,580
of color something that a eye isn't very

00:05:43,020 --> 00:05:51,240
good at recognizing talking of that ibm'

00:05:47,580 --> 00:05:54,090
not a shameless plug but something that

00:05:51,240 --> 00:05:57,120
we're doing to combat this is creating a

00:05:54,090 --> 00:05:59,729
diversity and faces data set where we

00:05:57,120 --> 00:06:02,220
have created 1 million annotated human

00:05:59,729 --> 00:06:04,110
facial images just as a sort of starter

00:06:02,220 --> 00:06:06,870
for tend to say look we're trying to

00:06:04,110 --> 00:06:09,479
create a dataset that is representative

00:06:06,870 --> 00:06:11,310
of today's population and is diverse

00:06:09,479 --> 00:06:16,140
enough to ensure that we don't have

00:06:11,310 --> 00:06:17,430
these biases in our training data so the

00:06:16,140 --> 00:06:19,490
second part we're gonna have a look at

00:06:17,430 --> 00:06:23,070
is programmer bias

00:06:19,490 --> 00:06:25,350
so within IBM I have run a couple of

00:06:23,070 --> 00:06:29,640
workshops and they have all had the same

00:06:25,350 --> 00:06:33,300
output I ask people please draw me a

00:06:29,640 --> 00:06:36,300
shoe no more context just a shoe let's

00:06:33,300 --> 00:06:39,060
have a guess what we have here so the

00:06:36,300 --> 00:06:41,940
men tend to draw trainers

00:06:39,060 --> 00:06:45,470
sooo shoes whereas on the women's side

00:06:41,940 --> 00:06:49,080
we have a whole array of things heels

00:06:45,470 --> 00:06:51,630
pumps sandals but what this really

00:06:49,080 --> 00:06:53,430
illustrates to me every time is that if

00:06:51,630 --> 00:06:55,919
you have a programming group that isn't

00:06:53,430 --> 00:06:58,200
diverse you're only going to get one

00:06:55,919 --> 00:07:00,660
worldview you're only going to get one

00:06:58,200 --> 00:07:02,220
version of the truth so how do you know

00:07:00,660 --> 00:07:04,740
that if you're creating an AI that

00:07:02,220 --> 00:07:06,900
detects shoes if you've got a very

00:07:04,740 --> 00:07:08,700
narrow-minded programming group they're

00:07:06,900 --> 00:07:10,740
not going to include attributes that

00:07:08,700 --> 00:07:12,900
would link to the heels all the pumps or

00:07:10,740 --> 00:07:14,490
the sandals and this is very prevalent

00:07:12,900 --> 00:07:16,950
in a lot of systems that can't

00:07:14,490 --> 00:07:19,470
understand the nuances between things

00:07:16,950 --> 00:07:20,820
like a wallet in a purse small things

00:07:19,470 --> 00:07:23,300
that you would only understand if you

00:07:20,820 --> 00:07:25,979
have that diverse development team so

00:07:23,300 --> 00:07:28,310
cognitively without us intentionally

00:07:25,979 --> 00:07:30,900
realizing it we are embedding our own

00:07:28,310 --> 00:07:33,030
prejudice and assumptions and cognitive

00:07:30,900 --> 00:07:36,900
shortcuts into our artificially

00:07:33,030 --> 00:07:39,330
intelligent systems to back this up

00:07:36,900 --> 00:07:41,780
the UK which is also responsible for

00:07:39,330 --> 00:07:45,300
this because we have less than 10% of

00:07:41,780 --> 00:07:47,669
female software engineers which is why

00:07:45,300 --> 00:07:50,100
I'm such an advocate for anyone who is

00:07:47,669 --> 00:07:52,350
interested in this industry to come

00:07:50,100 --> 00:07:54,270
together in one way or another so that

00:07:52,350 --> 00:07:56,220
we can have more diverse development

00:07:54,270 --> 00:07:58,530
teams and we can make sure that data is

00:07:56,220 --> 00:08:02,760
representative of the population of

00:07:58,530 --> 00:08:04,950
today so then companies when I tell them

00:08:02,760 --> 00:08:06,240
you need to be more ethical they go well

00:08:04,950 --> 00:08:08,460
I don't care it doesn't make me any more

00:08:06,240 --> 00:08:11,280
money and I say well actually it can if

00:08:08,460 --> 00:08:14,310
you're creating these hiring AIS for

00:08:11,280 --> 00:08:16,500
example which are inherently bias if you

00:08:14,310 --> 00:08:17,910
don't have racial diversity then you're

00:08:16,500 --> 00:08:21,210
not going to get higher financial

00:08:17,910 --> 00:08:24,210
returns by as much as 35% companies with

00:08:21,210 --> 00:08:26,460
below average diversity receives 73%

00:08:24,210 --> 00:08:28,680
less revenues from innovation if that's

00:08:26,460 --> 00:08:31,410
not enough to show you why you need hire

00:08:28,680 --> 00:08:33,060
diverse people but also make sure you

00:08:31,410 --> 00:08:34,530
have diverse development teams then I'm

00:08:33,060 --> 00:08:36,029
not really sure what else you need

00:08:34,530 --> 00:08:37,760
because at the end of the day the bottom

00:08:36,029 --> 00:08:40,650
line is business

00:08:37,760 --> 00:08:43,740
so the third category is algorithmic

00:08:40,650 --> 00:08:47,130
bias now I wanted to illustrate this

00:08:43,740 --> 00:08:48,810
through an example of a CV typically

00:08:47,130 --> 00:08:50,790
when you fire your CV into a hiring

00:08:48,810 --> 00:08:53,130
algorithm they might be looking at all

00:08:50,790 --> 00:08:56,190
of these factors race

00:08:53,130 --> 00:08:58,470
age sex buzzwords and all of that

00:08:56,190 --> 00:09:00,540
inherently will lead to a bias hiring

00:08:58,470 --> 00:09:03,540
decision so a lot of companies come to

00:09:00,540 --> 00:09:06,030
me and they go I've got a solution I'll

00:09:03,540 --> 00:09:09,540
just get rid of the sensitive attributes

00:09:06,030 --> 00:09:14,520
and it will not be bias it's not as easy

00:09:09,540 --> 00:09:16,440
as a lot of data that you'll find on a

00:09:14,520 --> 00:09:18,810
CV that you might not specifically be

00:09:16,440 --> 00:09:21,030
looking for the AI will take into an

00:09:18,810 --> 00:09:24,360
account so for example at the top here

00:09:21,030 --> 00:09:27,630
you've got a rough address the AI can

00:09:24,360 --> 00:09:30,180
then make inferences because it's

00:09:27,630 --> 00:09:31,890
designed to fit a regression curve and

00:09:30,180 --> 00:09:35,250
whatever success principles you have

00:09:31,890 --> 00:09:37,020
defined for it it will assume so purely

00:09:35,250 --> 00:09:39,180
from your address alone we can infer

00:09:37,020 --> 00:09:41,700
your ethnicity because ethnic groups

00:09:39,180 --> 00:09:43,860
tend to congregate in different areas so

00:09:41,700 --> 00:09:45,990
we know where you live we can then infer

00:09:43,860 --> 00:09:48,360
your financial background what kind of

00:09:45,990 --> 00:09:50,400
area do you live in but then we can also

00:09:48,360 --> 00:09:53,310
rank that against the success of past

00:09:50,400 --> 00:09:55,830
hires if past hires have all come from a

00:09:53,310 --> 00:09:58,620
certain area and been successful the AI

00:09:55,830 --> 00:10:00,480
will then look at you as less successful

00:09:58,620 --> 00:10:02,610
if you come from an area that not many

00:10:00,480 --> 00:10:04,830
people have previously come from why

00:10:02,610 --> 00:10:06,960
should when you live be a disadvantage

00:10:04,830 --> 00:10:09,720
no one cares it's about you and your

00:10:06,960 --> 00:10:13,470
skills probably mr. AI won't see it like

00:10:09,720 --> 00:10:15,660
that so we have to make action now to be

00:10:13,470 --> 00:10:18,390
able to go this isn't the only way to

00:10:15,660 --> 00:10:21,030
solve this problem excluding sensitive

00:10:18,390 --> 00:10:22,470
attributes isn't enough anymore we need

00:10:21,030 --> 00:10:25,230
to make sure that we look for these

00:10:22,470 --> 00:10:27,690
small nuances and say what else can an

00:10:25,230 --> 00:10:30,720
AI infer from something as simple as

00:10:27,690 --> 00:10:32,220
your address so like I said which

00:10:30,720 --> 00:10:35,370
variables are included in your

00:10:32,220 --> 00:10:37,440
decision-making but also how much are

00:10:35,370 --> 00:10:39,780
they waited to what extent did any

00:10:37,440 --> 00:10:41,760
variables constitute to a decision being

00:10:39,780 --> 00:10:44,400
made so if we look at something like

00:10:41,760 --> 00:10:46,500
what's an open scale we now have the

00:10:44,400 --> 00:10:49,350
ability to see which actually is

00:10:46,500 --> 00:10:51,090
contributed to a decision being made so

00:10:49,350 --> 00:10:53,580
here we've got one for a loan being

00:10:51,090 --> 00:10:56,160
approved or denied and you can see that

00:10:53,580 --> 00:10:58,650
the applicants age country and account

00:10:56,160 --> 00:11:00,840
age we're on the positive side and what

00:10:58,650 --> 00:11:03,000
waiting and percentage they had - alone

00:11:00,840 --> 00:11:05,339
being approved versus their account

00:11:03,000 --> 00:11:06,819
balance and account status now I'm not

00:11:05,339 --> 00:11:09,039
saying that these are

00:11:06,819 --> 00:11:11,410
I don't know why what country you're in

00:11:09,039 --> 00:11:13,539
would necessarily wait more than your

00:11:11,410 --> 00:11:16,269
account status but you can see just for

00:11:13,539 --> 00:11:18,489
argument's sake how much this can have

00:11:16,269 --> 00:11:21,189
and how much further we can dig into a

00:11:18,489 --> 00:11:23,049
model to look at its integrity accuracy

00:11:21,189 --> 00:11:26,559
and ultimately transparency and

00:11:23,049 --> 00:11:28,569
accountability so the final part I

00:11:26,559 --> 00:11:30,970
wanted to look at our black box a eyes a

00:11:28,569 --> 00:11:33,009
lot of AIS that were historically built

00:11:30,970 --> 00:11:35,799
we can't dig into those we are we are

00:11:33,009 --> 00:11:38,229
too far gone in a lot of senses because

00:11:35,799 --> 00:11:41,829
these models are so far advanced that

00:11:38,229 --> 00:11:44,889
they can write their own code we haven't

00:11:41,829 --> 00:11:47,169
built transparency mechanisms in yet so

00:11:44,889 --> 00:11:49,689
a lot of it is process re-engineering

00:11:47,169 --> 00:11:53,379
we're working backwards to see where the

00:11:49,689 --> 00:11:55,359
bias stems from so a simple picture of

00:11:53,379 --> 00:11:57,579
what a neural network could look like if

00:11:55,359 --> 00:11:58,809
you see in the middle no one has a clue

00:11:57,579 --> 00:12:01,899
what's going on in there

00:11:58,809 --> 00:12:04,660
so it is our job to work backwards and

00:12:01,899 --> 00:12:07,119
say where is the buyer stemming from and

00:12:04,660 --> 00:12:09,729
why what tools can we use like deep

00:12:07,119 --> 00:12:11,859
visualization tool kits we can look at

00:12:09,729 --> 00:12:14,139
these and go in each layer of the neural

00:12:11,859 --> 00:12:17,079
network which pixels light up how does

00:12:14,139 --> 00:12:20,559
an AI know what is a difference between

00:12:17,079 --> 00:12:23,169
this pixel and this pixel so to

00:12:20,559 --> 00:12:26,350
illustrate that I have an AI here that

00:12:23,169 --> 00:12:28,059
categorizes trees I ask it what these

00:12:26,350 --> 00:12:30,609
trees are it tells me it's an elm tree

00:12:28,059 --> 00:12:32,559
it's a pine tree it's an elm tree it's a

00:12:30,609 --> 00:12:36,069
pine tree and I go fantastic it's so

00:12:32,559 --> 00:12:37,809
accurate how does it know this you would

00:12:36,069 --> 00:12:40,779
think as a logical human that it's

00:12:37,809 --> 00:12:43,419
looked at the bark or it's looked at the

00:12:40,779 --> 00:12:47,799
color of the leaves or it's got pine

00:12:43,419 --> 00:12:49,600
cones the a I unfortunately knew the

00:12:47,799 --> 00:12:51,759
difference between an elm tree and a

00:12:49,600 --> 00:12:57,809
pine tree because of the birds in the

00:12:51,759 --> 00:13:01,119
branches an AI is not a logical human so

00:12:57,809 --> 00:13:04,299
this is just a way to visualize the fact

00:13:01,119 --> 00:13:06,399
that and then I will make an accurate

00:13:04,299 --> 00:13:09,309
decision of what we think is correct but

00:13:06,399 --> 00:13:11,519
we need to test it to say why is it

00:13:09,309 --> 00:13:15,639
making these decisions what attributes

00:13:11,519 --> 00:13:17,859
constitute the output decision is it the

00:13:15,639 --> 00:13:19,209
branches and the bark or is it that

00:13:17,859 --> 00:13:19,620
there's birds in the trees and like I

00:13:19,209 --> 00:13:21,420
said

00:13:19,620 --> 00:13:23,850
can go through each layer of a neural

00:13:21,420 --> 00:13:26,310
network and see at which point are the

00:13:23,850 --> 00:13:28,170
pixels light up and then be able to see

00:13:26,310 --> 00:13:29,760
it kept lighting up on the birds and I

00:13:28,170 --> 00:13:32,100
would know that's how it comes to choose

00:13:29,760 --> 00:13:34,710
this decision we can then work through

00:13:32,100 --> 00:13:36,750
things like back propagation to see okay

00:13:34,710 --> 00:13:38,370
how do we reverse engineer this and

00:13:36,750 --> 00:13:41,730
ensure that it understands the

00:13:38,370 --> 00:13:44,070
differences between the two trees so to

00:13:41,730 --> 00:13:46,500
sort of sum up why should we bother with

00:13:44,070 --> 00:13:49,050
AI ethics this is a very scathing

00:13:46,500 --> 00:13:51,480
overview of some of the issues that we

00:13:49,050 --> 00:13:53,220
face that IO has been called when the

00:13:51,480 --> 00:13:54,570
greatest human rights challenges of the

00:13:53,220 --> 00:13:56,370
21st century if you're being

00:13:54,570 --> 00:13:59,400
discriminated against and can't get

00:13:56,370 --> 00:14:01,830
alone then this definitely is one if we

00:13:59,400 --> 00:14:04,350
don't do something now then we will

00:14:01,830 --> 00:14:06,810
continue to entrench societal prejudice

00:14:04,350 --> 00:14:09,750
in a sort of self reinforcing feedback

00:14:06,810 --> 00:14:12,779
loop as AI becomes more prevalent it's

00:14:09,750 --> 00:14:15,089
still bias that will entrench all of the

00:14:12,779 --> 00:14:17,520
new training data in our present day and

00:14:15,089 --> 00:14:21,270
most of you here will have been affected

00:14:17,520 --> 00:14:23,279
by some form of AI on a daily basis and

00:14:21,270 --> 00:14:25,140
I will also look to refine not only our

00:14:23,279 --> 00:14:27,180
society but our businesses and our

00:14:25,140 --> 00:14:29,160
government so realistically if we don't

00:14:27,180 --> 00:14:31,740
act and provide some form of regulation

00:14:29,160 --> 00:14:35,070
now we're going to be entrenching this

00:14:31,740 --> 00:14:38,100
further into our futures so a final sort

00:14:35,070 --> 00:14:40,050
of word for me to not just the women

00:14:38,100 --> 00:14:42,990
here but everyone involved in a ice

00:14:40,050 --> 00:14:45,270
creation or output sees drive the

00:14:42,990 --> 00:14:46,760
conversation work within your

00:14:45,270 --> 00:14:49,440
development teams push the

00:14:46,760 --> 00:14:52,680
representative that be with your data or

00:14:49,440 --> 00:14:54,810
yourselves define success for everyone

00:14:52,680 --> 00:14:57,630
don't define success for what you

00:14:54,810 --> 00:15:00,150
believe it is define success within a

00:14:57,630 --> 00:15:03,120
system so there is equal outcomes for

00:15:00,150 --> 00:15:05,850
everyone look at how you define fairness

00:15:03,120 --> 00:15:08,029
as a technical construct not just a

00:15:05,850 --> 00:15:10,770
human one and ensure that you have

00:15:08,029 --> 00:15:12,779
accountability and transparency built-in

00:15:10,770 --> 00:15:15,209
throughout because that is the only way

00:15:12,779 --> 00:15:19,050
that we can ensure procedural regularity

00:15:15,209 --> 00:15:23,790
and that we can monitor and orderly

00:15:19,050 --> 00:15:27,270
systems as we move forward a very last

00:15:23,790 --> 00:15:28,829
final plug from me I actually managed to

00:15:27,270 --> 00:15:31,700
get a paper published regarding

00:15:28,829 --> 00:15:34,640
governing AI bias in IBM systems journal

00:15:31,700 --> 00:15:38,610
yeah

00:15:34,640 --> 00:15:38,610
[Applause]

00:15:39,660 --> 00:15:45,450
this looks at building a maturity

00:15:43,300 --> 00:15:48,280
framework to go into an organization

00:15:45,450 --> 00:15:50,530
give them all of the different tools and

00:15:48,280 --> 00:15:53,200
knowledge they need to mitigate bias

00:15:50,530 --> 00:15:55,180
right from the business offset going how

00:15:53,200 --> 00:15:56,650
do we put bias on the agenda is it in

00:15:55,180 --> 00:15:58,180
our strategy is it in our sprint

00:15:56,650 --> 00:16:00,400
development plan have we got a device

00:15:58,180 --> 00:16:02,890
development team right through to the

00:16:00,400 --> 00:16:04,180
data the algorithm and then at the end

00:16:02,890 --> 00:16:06,760
looking at the testing and the

00:16:04,180 --> 00:16:08,770
accountability mechanisms so if anyone's

00:16:06,760 --> 00:16:10,630
interested please either message me on

00:16:08,770 --> 00:16:13,570
LinkedIn or find me tonight and I can

00:16:10,630 --> 00:16:16,630
provide you with a copy if you wanted to

00:16:13,570 --> 00:16:19,330
read more so thank you so much everyone

00:16:16,630 --> 00:16:20,260
you've been a fab audience and have a

00:16:19,330 --> 00:16:25,299
good evening

00:16:20,260 --> 00:16:25,299

YouTube URL: https://www.youtube.com/watch?v=9lc2jHUArNI


