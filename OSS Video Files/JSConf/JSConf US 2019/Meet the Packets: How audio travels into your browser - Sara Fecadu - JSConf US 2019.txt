Title: Meet the Packets: How audio travels into your browser - Sara Fecadu - JSConf US 2019
Publication date: 2019-09-11
Playlist: JSConf US 2019
Description: 
	It's 2019, audio packets from streaming services are swirling all around us. Yet, as JavaScript enthusiasts, we are often so focused on the application layer that we miss out on all the quirks that exist in the layers below.

Let's journey from the browser down to the physical layer (and back up) to see how a single audio file travels over the wires and into our lives. This talk will look at audio codecs, compression techniques, and the use of TCP congestion control algorithms (like BBR) to help manage packet loss.
Captions: 
	00:00:00,160 --> 00:00:01,160
Meet the Packets: How audio travels into your browser

00:00:01,160 --> 00:00:02,160
Sara Fecadu KATIE: Hello. Welcome back. So, I keep forgetting

00:00:02,160 --> 00:00:03,160
to do this and I apologize. But the big announcement right now is that the swag is ready. But do

00:00:03,160 --> 00:00:04,160
not go get swag now because we're about to have a really awesome talk by Sara Fecadu.

00:00:04,160 --> 00:00:05,830
I asked Sara for a fun fact and her fun fact was that she makes  bakes a mean cookie which

00:00:05,830 --> 00:00:06,830
unfortunately we can't all indulge in. So, as a follow up question, I said what prompted

00:00:06,830 --> 00:00:07,830
you write this talk about an audio API. And she said, well, I had spent a year building

00:00:07,830 --> 00:00:08,830
a checkout form and I just couldn't stand to look at it or think about it anymore and

00:00:08,830 --> 00:00:09,830
I had to do something different. Which I think is something that literally all you have us

00:00:09,830 --> 00:00:10,830
can probably identify really strongly with. So, anyways, Sara is gonna come up and talk

00:00:10,830 --> 00:00:11,830
to us about the audio API. So, give it up for Sara.

00:00:11,830 --> 00:00:12,830
[ Applause ] SARA: Hello. See if I can get my computer

00:00:12,830 --> 00:00:13,830
started here. Okay. Welcome to my talk. Meet the packets. If not everyone has realized,

00:00:13,830 --> 00:00:14,830
it's a play off meet the parents. I spent a lot of time working on that.

00:00:14,830 --> 00:00:15,830
[ Laughter ] Let's see here. One second. Gonna progress?

00:00:15,830 --> 00:00:16,830
No. Okay. We're gonna do it without the clicker. So, this will be interesting. As Katie said,

00:00:16,830 --> 00:00:17,830
my name  oh. My whole slide deck isn't progressing. Okay. One second. There we go. Okay. Thank

00:00:17,830 --> 00:00:18,830
you for coming to talk. As Katie said, my name is Sara Fecadu. I am from Seattle, Washington.

00:00:18,830 --> 00:00:19,830
And I don't have a ton of hobbies besides making cookies and listening to a lot of podcasts.

00:00:19,830 --> 00:00:22,429
And by day I'm a software developer at Nordstrom. And Nordstrom is a clothing retailer founded

00:00:22,429 --> 00:00:27,539
in 1901. While people don't usually associate 100 year old companies with tech, we have

00:00:27,539 --> 00:00:32,599
a thriving tech org working on innovative ways to get you what you need and feel your

00:00:32,599 --> 00:00:41,890
best. And a year ago I was hired on to do a rewrite of Nordstrom.com's redux. And as

00:00:41,890 --> 00:00:50,640
of last May, we have been taking 100% of customer orders. Now, why am I talking about audio

00:00:50,640 --> 00:00:58,430
streaming? Katie may have taken my joke here, but the answer is: Form fields. Our checkout

00:00:58,430 --> 00:01:04,980
UI has 22 form fields. And they come in different groupings for different reasons. But many

00:01:04,980 --> 00:01:09,990
of my waking moments over the past year have been spent thinking about these form fields.

00:01:09,990 --> 00:01:16,450
And I just wanted to do anything else. So, I was sitting on my couch one night reading

00:01:16,450 --> 00:01:22,440
a book on packet analysis, like one does, and watching a YouTube video. And I thought

00:01:22,440 --> 00:01:28,430
to myself, how does that work? Like, on the packet level, how does audio video streaming

00:01:28,430 --> 00:01:35,040
work? So, to answer the larger question, I started small with: What is audio streaming?

00:01:35,040 --> 00:01:41,030
And audio streaming is the act of sending audio files over the network. And this talk

00:01:41,030 --> 00:01:45,890
will be about on demand audio streaming. Now, the major difference between on demand streaming

00:01:45,890 --> 00:01:50,600
and live streaming, is with on demand streaming we need all of the packets to get across the

00:01:50,600 --> 00:01:55,310
wire. Whereas with live streaming, you may be more interested in keeping them up with

00:01:55,310 --> 00:02:01,950
the event and a certain amount of packet loss is acceptable. Over the past few months, I

00:02:01,950 --> 00:02:08,011
learned that audio streaming, even when limited to on demand, is as wide a subject as it is

00:02:08,011 --> 00:02:15,400
deep. I have picked three topics that exemplify what audio streaming is. Why it's hard and

00:02:15,400 --> 00:02:21,220
how to get started yourself. And we will talk about audio streaming protocols, TCP congestion

00:02:21,220 --> 00:02:28,840
control and client players. Audio streaming protocols give us a stand how to encode, segment

00:02:28,840 --> 00:02:35,730
and ship your code to the client. TCP congestion control handles congestion on the TCP layer

00:02:35,730 --> 00:02:42,980
of the stack. And it is relevant with on demand audio streaming because we're shipping larger

00:02:42,980 --> 00:02:49,440
audio files and we need every single packet to make its way to the client to play audio.

00:02:49,440 --> 00:02:55,481
A client player is any network connected device with a play and pause button. So, this could

00:02:55,481 --> 00:03:02,120
be your phone, your TV, your laptop, et cetera. And client players not only allow us to play

00:03:02,120 --> 00:03:08,010
our audio, but when paired with modern audio streaming protocols, they hold a lot of decision

00:03:08,010 --> 00:03:14,650
making power. Well, audio streaming protocols are the heart of audio streaming. And today

00:03:14,650 --> 00:03:19,560
we'll talk about adaptive bitrate streaming it &s it benefits and how to convert your

00:03:19,560 --> 00:03:25,410
own audio files to work with two popular audio streaming protocols. Before we get started,

00:03:25,410 --> 00:03:32,060
I wanted to go over some terms that will come up. A codec encodes data and uses compression

00:03:32,060 --> 00:03:38,460
techniques to get the highest quality for the smallest footprint. Encoding and trans

00:03:38,460 --> 00:03:47,450
coding is converting it from one type to another. Trans coding can convert from digital to digital.

00:03:47,450 --> 00:03:57,270
And then move from analog to other digital files. Bitrate is how many bits it takes to

00:03:57,270 --> 00:04:02,290
encode a second of audio. And this number usually refers to the quality of the audio

00:04:02,290 --> 00:04:10,230
file. When I think of playing music on the Internet, I think of an HTML5 audio tag with

00:04:10,230 --> 00:04:15,849
a source attribute set to the path of my audio file. And this is a perfectly reasonable way

00:04:15,849 --> 00:04:22,500
to do it. You can request and receive a single file containing an entire song. And it would

00:04:22,500 --> 00:04:27,561
be referred to as progressive streaming and the major benefit here is you only have one

00:04:27,561 --> 00:04:34,430
file to deal with. But let's say, for instance, you have a user and they have a slow network

00:04:34,430 --> 00:04:41,230
connection and they can't download your one file. They're stuck. So, adaptive bitrate

00:04:41,230 --> 00:04:47,300
streaming aims to solve this problem by encoding your audio in multiple bitrates and allowing

00:04:47,300 --> 00:04:51,850
the client player to decide which quality is best for the user to listen to your audio

00:04:51,850 --> 00:04:59,940
uninterrupted. This allows more users to access your audio. But it does add a layer of operational

00:04:59,940 --> 00:05:07,260
complexity because now you've got a lot more work on moving parts. The audio streaming

00:05:07,260 --> 00:05:14,280
protocols we'll talk about not only average adaptive bitrate streaming, but also use HTTP

00:05:14,280 --> 00:05:19,320
web servers. They do this by encoding the file, segmenting they will, placing them on

00:05:19,320 --> 00:05:25,870
a web server and then once requested, partial audio files are sent to the client one at

00:05:25,870 --> 00:05:33,700
a time. Here is the secret to our modern audio streaming protocols is it's more of a series

00:05:33,700 --> 00:05:40,051
of downloads than it really is a stream. But we'll refer to it as streaming anyway. The

00:05:40,051 --> 00:05:46,240
two most popular audio streaming protocols today are HTTP lye streaming, or HLS, and

00:05:46,240 --> 00:05:56,180
dynamic adaptive streaming over HTTP, MPEG DASH. It was created by Apple to support streaming

00:05:56,180 --> 00:06:06,229
to mobile devices and it is default on all Mac OS and Apple devices. And MPEG DASH was

00:06:06,229 --> 00:06:13,410
a direct alternative to HLS. It was created by the forum who want to make MPEG DASH the

00:06:13,410 --> 00:06:25,639
international streaming. Let's look at them side by side. HLS takes the MPC, AAC, AC 3,

00:06:25,639 --> 00:06:35,760
or EC 3, encodes them into fragmented MP4 files. Those segmented files are in a play

00:06:35,760 --> 00:06:42,010
list. If you have multiple bitrate streams, each stream will be in a media play list and

00:06:42,010 --> 00:06:49,230
all of your media play lists will be in a master play list. With MPEG DASH, it is agnostic,

00:06:49,230 --> 00:07:01,009
in theory you can convert any into MPEG DASH. It will be fragmented into a fragmented MP4

00:07:01,009 --> 00:07:08,389
file. That will be displayed in an XML manifest file called a media presentation description.

00:07:08,389 --> 00:07:14,901
Okay. We've talked about what files will be used and what they'll be segmented into, but

00:07:14,901 --> 00:07:20,040
how do you get it there? You've got this audio file. What tools allow you to convert the

00:07:20,040 --> 00:07:26,710
audio file? Well, you've got options. But most of these options are paid options. Except

00:07:26,710 --> 00:07:32,910
for FFmpeg. Which is an open source demand line tool that among other things allows you

00:07:32,910 --> 00:07:41,540
to convert audio files to be HLS or MPEG DASH. However, I founded learning curve for FFmpeg

00:07:41,540 --> 00:07:52,460
to be pretty steep. And a lot of the documentation for HLS and MPEG DASH were for video streams.

00:07:52,460 --> 00:08:00,100
Instead I used Amazon elastic trans coder. It's an AWS offering that converts files of

00:08:00,100 --> 00:08:05,789
one type to another. In our case, we're taking an audio file and converting it to be used

00:08:05,789 --> 00:08:15,040
with HLS and MPEG DASH. It's pretty much plug and play. You tell Amazon elastic trans coder

00:08:15,040 --> 00:08:20,030
what type of files you have and what type of files you want and it outputs the stream

00:08:20,030 --> 00:08:25,560
for you. And even though it's easy to use, it's not a free service. So, if you were going

00:08:25,560 --> 00:08:30,910
to be converting a lot of files, it may be worth your time to learn more about an open

00:08:30,910 --> 00:08:37,829
source alternative like MPEG DASH. My workflow when working with Amazon Elastic Transcoder

00:08:37,829 --> 00:08:48,110
was to upload to an AWS object store. I told Amazon Elastic Transcoder where my audio file

00:08:48,110 --> 00:08:55,920
was and what settings I needed it to convert my audio files to. And Amazon Elastic Transcoder

00:08:55,920 --> 00:09:04,661
output my streams into that same S3 bucket. And I downloaded them for us to explore. This

00:09:04,661 --> 00:09:09,920
is the basic set of files you would get with an HLS stream. And it kind of looks like a

00:09:09,920 --> 00:09:16,089
lot. But we're going to break it down into four groups. In the top left, the master play

00:09:16,089 --> 00:09:22,470
list. In our case, we have two bitrate streams represented and they will be linked out from

00:09:22,470 --> 00:09:28,029
the master play list. And then in the top right you'll see those media play lists which

00:09:28,029 --> 00:09:35,720
have each bitrate stream. And those will contain all of our links to our transport stream files

00:09:35,720 --> 00:09:42,470
which are the fragmented audio files represented in both the bottom left and the bottom right.

00:09:42,470 --> 00:09:48,119
On the bottom right we have our 64K bitrate stream segmented audio files. And in the bottom,

00:09:48,119 --> 00:09:52,860
oh. Did I get that backwards? I'm not really good at right and left. But in the bottom

00:09:52,860 --> 00:09:57,829
section you'll have your fragmented audio files. We'll take a closer look at those so

00:09:57,829 --> 00:10:03,449
you can see really what's in it. This is the entirety of the HLS master play list. It contains

00:10:03,449 --> 00:10:09,529
information about the specific bitrate streams and links out to those media play lists that

00:10:09,529 --> 00:10:16,529
represent the streams themselves. Let's look at the 64K bitrate stream media playlist.

00:10:16,529 --> 00:10:21,720
It has even more information about the stream including caching information, the target

00:10:21,720 --> 00:10:27,769
duration of each segmented audio file, and most importantly, links out to our transport

00:10:27,769 --> 00:10:34,610
streams. This is what one of those fragmented audio times looks like. And there's something

00:10:34,610 --> 00:10:40,339
a little interesting going on here. If you'll notice, it's color coded and I kept trying

00:10:40,339 --> 00:10:46,839
to figure out why. But then I realized a transport stream has the file extension .ts. And something

00:10:46,839 --> 00:10:56,550
else has the file extension .ts, TypeScript. Ignore the colors. It's just a binary coded

00:10:56,550 --> 00:11:04,869
file. Now our MPEG DASH audio stream has fewer files and looks more manageable. But it's

00:11:04,869 --> 00:11:10,790
similar. We have our media presentation description, which is an XML manifest file which contains

00:11:10,790 --> 00:11:17,600
all of our information about the stream. Then below we have our two segmented audio files.

00:11:17,600 --> 00:11:23,829
All of the segments are encapsulated in a single file, but within them there are segments.

00:11:23,829 --> 00:11:28,161
That's why there are fewer files in the MPEG DASH audio stream than in the other audio

00:11:28,161 --> 00:11:36,160
stream. Look at the description. See a lot of stuff here. But there are three important

00:11:36,160 --> 00:11:42,589
elements. All bitrate streams are represented in a representation tag. And then all bitrate

00:11:42,589 --> 00:11:49,720
streams are enclosed in an adaptation set. Within the representation tag, we do have

00:11:49,720 --> 00:11:55,699
our URL to our audio files. And taking a look at one of those audio files we'll see if looks

00:11:55,699 --> 00:12:01,889
fairly similar to the segmented audio file we saw with HLS. Minus the color coding because

00:12:01,889 --> 00:12:07,949
it's a .MP4 versus .TS. visual studio is not confused in this case.

00:12:07,949 --> 00:12:14,750
Earlier we talked about progressive streaming which is streaming an entire audio file in

00:12:14,750 --> 00:12:21,850
one two. We used an audio element and a source attribute with the path of our audio file.

00:12:21,850 --> 00:12:26,420
With MPEG DASH and HLS, it's very similar. But instead of having the path to our audio

00:12:26,420 --> 00:12:34,139
file, we have the path to the master play list for HLS or media presentation description

00:12:34,139 --> 00:12:40,040
for MPEG DASH. We're going to take a hard left here and we're gonna talk about the second

00:12:40,040 --> 00:12:48,019
topic in my talk. Which is TCP congestion control. And TCP is a transport layer protocol

00:12:48,019 --> 00:12:52,869
and it has mechanisms in both its sender and receiver which are defined by the operating

00:12:52,869 --> 00:12:58,989
systems of each to react to and hopefully avoid congestion when sending packets over

00:12:58,989 --> 00:13:08,009
the wire. And they are called TCP congestion control. And today we talk about packet loss

00:13:08,009 --> 00:13:16,459
congestion control and why it isn't so great. And more specific, the congestion window and

00:13:16,459 --> 00:13:23,339
duplicate acknowledgment in packet loss based congestion control. Before we get started,

00:13:23,339 --> 00:13:28,629
somewhere terms, bandwidth is the rate at which data can be sent. And throughput is

00:13:28,629 --> 00:13:34,519
the rate at which data can be received. The congestion window is a TCP variable that defines

00:13:34,519 --> 00:13:41,059
the amount of data that can be sent before the acknowledgment is received by the sender.

00:13:41,059 --> 00:13:45,660
Let's say you have a user who has requested your audio file from the server. Your audio

00:13:45,660 --> 00:13:51,689
packets travel down the network stack, across the physical layer, up the data link layer

00:13:51,689 --> 00:13:57,829
in the network layer and arrives at the transport layer and unfortunately there's congestion

00:13:57,829 --> 00:14:02,989
right before we reached our destination. Now, traffic congestion and network congestion

00:14:02,989 --> 00:14:08,249
have very similar beginnings. Either too many cars or too many packets have entered the

00:14:08,249 --> 00:14:15,751
roadway and there's nowhere for them to go. With traffic, you have to wait it out. Luckily

00:14:15,751 --> 00:14:23,989
for us, TCP congestion control allows them to flow over the wire, even during congestion.

00:14:23,989 --> 00:14:29,670
And before we get to the specifics of these TCP congestion control algorithms, let's talk

00:14:29,670 --> 00:14:36,139
about the TCP happy path. We're going to start with a single packet sent from the sender

00:14:36,139 --> 00:14:42,179
to the receiver flowing through the receiver's buffer. And being acknowledged by the receiver

00:14:42,179 --> 00:14:47,549
and having an acknowledgment packet sent back to the requester. We talked about the congestion

00:14:47,549 --> 00:14:54,930
window, the amount of data before a sender receives an acknowledgment. Another way of

00:14:54,930 --> 00:15:00,519
thinking about the congestion window is as a sending rate. As the sender receives acknowledgments,

00:15:00,519 --> 00:15:07,670
the congestion window grows. And as the receiver's buffers fill and they drop all excess packets,

00:15:07,670 --> 00:15:13,150
the sender responds by shrinking the congestion window. A second way of thinking about the

00:15:13,150 --> 00:15:21,129
congestion window is as a bucket. And as packet loss occurs, the bucket shrinks. And as acknowledgments

00:15:21,129 --> 00:15:28,049
are received by the sender, the bucket gross. There's a slight oversight in the bucket explanation

00:15:28,049 --> 00:15:32,660
in that the receiver has no way of telling the sender that it is dropping packets due

00:15:32,660 --> 00:15:39,589
to congestion. But one option the sender does have is to send a duplicate acknowledgment.

00:15:39,589 --> 00:15:49,930
And a duplicate is if they're trying to send out of order packets. They send one, two and

00:15:49,930 --> 00:15:54,470
three. For the purposes of our example, the receiver's not going to process them right

00:15:54,470 --> 00:16:03,040
away. So, that when we send packet four, it's full and it has nowhere to go. So, packet

00:16:03,040 --> 00:16:11,419
four dropped due to congestion. And they move on to process packet one, send an acknowledgment,

00:16:11,419 --> 00:16:19,040
send for packet two and for three. However, when it looks at packet five, it says I can't

00:16:19,040 --> 00:16:25,529
process you because this would be an out of order packet. drops packet five and sends

00:16:25,529 --> 00:16:35,919
back for three. The sender is tipped off that it needs to sends packets four and five again.

00:16:35,919 --> 00:16:42,129
So, a more truthful version of the bucket metaphor would be that the congestion window

00:16:42,129 --> 00:16:48,850
shrinks as old acknowledgments are received by the sender. And the bucket window grows

00:16:48,850 --> 00:16:55,839
as new acknowledgments are sent by the sender. The first b congestion control algorithms

00:16:55,839 --> 00:17:03,529
were written in the 1980s and the most recent were a couple years ago. We will talk about

00:17:03,529 --> 00:17:17,170
TCP Reno and BBR. TCP Reno is the classic. And BBR was created by Google engineers a

00:17:17,170 --> 00:17:25,650
few years ago to address issues that they saw when using packet based algorithms. TCP

00:17:25,650 --> 00:17:32,760
Reno starts with a congestion period where it's set at some rate increasing by. It's

00:17:32,760 --> 00:17:39,540
set at some value, excuse me, increasing by some rate. And as the sender receives acknowledgments,

00:17:39,540 --> 00:17:46,850
the congestion window grows by one. And as the sender adds packets, it is divided by

00:17:46,850 --> 00:17:54,850
some rate. I have chosen path. So, it's divided by two. And the main issue with TCP Reno is

00:17:54,850 --> 00:18:00,350
that it assumes that small amounts of packet loss are congestion. And in a world where

00:18:00,350 --> 00:18:05,600
the sender doesn't know the state of the receiver's buffer and the receiver is unable to tell

00:18:05,600 --> 00:18:11,180
the sender that it has room left to process packets, you have an Internet moving at a

00:18:11,180 --> 00:18:18,610
fraction of the capacity. In 2016, BBR was created to help you get the most out of your

00:18:18,610 --> 00:18:26,140
Internet connection. It looks for the place where sending rate is equal to bandwidth.

00:18:26,140 --> 00:18:33,530
In theory, you should be able to send to the receiver and move on to the application without

00:18:33,530 --> 00:18:39,420
any queuing. Some companies have reported positive outcomes when using BBR in their

00:18:39,420 --> 00:18:45,130
production systems. Firstly, it only has to be implemented in the senders side and is

00:18:45,130 --> 00:18:54,410
in Linux operating systems with kernel 4.9 or higher. And they found BBR increased bandwidth

00:18:54,410 --> 00:19:02,280
for the low bandwidth users for 10 15%, and the bandwidth for their median group 5 7.

00:19:02,280 --> 00:19:08,510
Additionally, users in Latin America and Asia saw additional increases. But is it a fair

00:19:08,510 --> 00:19:16,040
algorithm? Fairness, or using your fair share of bandwidth is the goal of every TCP control

00:19:16,040 --> 00:19:22,110
algorithm. And in experiments in Google and Spotify, they found that BBR was able to co

00:19:22,110 --> 00:19:30,620
exist with congestion control algorithms like TCP Reno or QBIC. However, some researchers

00:19:30,620 --> 00:19:37,450
found that BBR's initial start algorithm pushed QBIC spenders back to where they couldn't

00:19:37,450 --> 00:19:42,940
reestablish their fair share of bandwidth. And this is an issue currently being look

00:19:42,940 --> 00:19:50,600
the at both in and outside of Google. We've reached the final section in this talk. And

00:19:50,600 --> 00:19:56,870
so far we've talked about how audio files are processed to be streamed and issues that

00:19:56,870 --> 00:20:03,160
may occur as they travel to devices. We'll wrap up by talking about the role of the client

00:20:03,160 --> 00:20:10,110
player and how to create your own audio strings. Now, I'm a pretty big fan of Spotify and I

00:20:10,110 --> 00:20:15,190
use it regularly. But have you ever looked at what's being sent back from the web server

00:20:15,190 --> 00:20:22,010
to create those audio streams? This should look pretty familiar to what we were looking

00:20:22,010 --> 00:20:27,530
at with our segmented audio files with HLS and MPEG DASH. But when I first saw these,

00:20:27,530 --> 00:20:33,870
I did not have this context. And I kept thinking, do I need to write some client side JavaScript

00:20:33,870 --> 00:20:40,480
to get this to play on the Internet? Is there an NPM package I can use? Or is there something

00:20:40,480 --> 00:20:46,230
simple and obvious going on here that's going right over my head. And luckily for me and

00:20:46,230 --> 00:20:52,200
hopefully everyone who writes JavaScript for the web, there is. Because HLS and MPEG DASH

00:20:52,200 --> 00:20:57,550
handed over a lot of responsibility to the clients that process their streams. And this

00:20:57,550 --> 00:21:05,380
not only includes picking the correct quality of audio to play, but it also includes allowing

00:21:05,380 --> 00:21:11,230
elements like the audio element to process segmented audio files without any modification.

00:21:11,230 --> 00:21:17,870
And most browsers do this by leveraging the media sources extension API and the encrypted

00:21:17,870 --> 00:21:25,230
media extensions API. Additionally, libraries like HLS.JS and Dash.JS are available while

00:21:25,230 --> 00:21:33,700
cross browser support is low. As a side note, if you need to support iOS Safari, you need

00:21:33,700 --> 00:21:39,990
HLS. But with most other browsers, you have options. So, it would have been really fun

00:21:39,990 --> 00:21:46,000
to reverse engineer Spotify's audio player. But I got tired of reading their minified

00:21:46,000 --> 00:21:53,160
code. So, I decided to make my own audio player. And I started with a cassette that I found

00:21:53,160 --> 00:21:59,900
from a box of cassettes. And I chose it because it has the words "Map squad" written on it.

00:21:59,900 --> 00:22:06,560
And I used my iPhone's voice memo application to record the audio so the quality is so so

00:22:06,560 --> 00:22:13,580
at best. But it works. And you can try it right now. But maybe wait until the end of

00:22:13,580 --> 00:22:20,370
the talk because I want to show you how it's made. The entire application is a single in

00:22:20,370 --> 00:22:28,490
docs.HTML file with an audio element in the body. When loaded into the browser, the immediately

00:22:28,490 --> 00:22:36,480
invoked function runs, the init function. And at the top, we define the audio that's

00:22:36,480 --> 00:22:44,450
equal to our audio element. Next web see if the media sources extension API is supported

00:22:44,450 --> 00:22:51,380
in our browser. If it is, we will assume we can use dash.JS to enable MPEG DASH in most

00:22:51,380 --> 00:23:01,860
browsers. Pass it to the dash.JS media player. And when the player is initialized, our audio

00:23:01,860 --> 00:23:08,450
will be loaded with it. If the media sources extension API is not available, we're going

00:23:08,450 --> 00:23:15,160
to assume we're using iOS Safari and we need to have an HLS stream. We will do this by

00:23:15,160 --> 00:23:20,890
setting the source attribute of our audio element to the master playlist or the past

00:23:20,890 --> 00:23:30,020
to our master playlist. And that  this file is all you need to stream audio to most browsers

00:23:30,020 --> 00:23:36,160
in 2019. If you want to try it out in the browser for yourself, or you want to create

00:23:36,160 --> 00:23:38,910
your own audio streams, please feel free to fork 24 repo. Thank you.

00:23:38,910 --> 00:23:39,910
[ Applause ] KATIE: I'm sorry. I think that scared me more

00:23:39,910 --> 00:23:40,910
than it scared you. Thank you so much, Sara. Can you believe that is the first talk she

00:23:40,910 --> 00:23:41,910
has ever given at a conference? Yes. Amazing. All right. So, we have about a  a 15 minute

00:23:41,910 --> 00:23:42,910
break right now. So, go out and pick up your swag bags. And we'll see you back here at

00:23:42,910 --> 00:23:43,910
3:00. Patricia Ruiz Realini is talking about the importance of your local library. Which

00:23:43,910 --> 00:23:44,910
is pretty cool because I hang out at the library. We'll see you back here at 3:00. No, wait.

00:23:44,910 --> 00:23:44,912

YouTube URL: https://www.youtube.com/watch?v=x3sdyyzdeCY


