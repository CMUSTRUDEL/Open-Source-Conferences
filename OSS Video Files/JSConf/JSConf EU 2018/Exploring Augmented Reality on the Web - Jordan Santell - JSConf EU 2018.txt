Title: Exploring Augmented Reality on the Web - Jordan Santell - JSConf EU 2018
Publication date: 2018-06-13
Playlist: JSConf EU 2018
Description: 
	Augmented reality is enabling all new experiences, interaction patterns, and technology. As the web begins to adopt AR capabilities, this new medium is now available to web developers. If you know JavaScript, you can develop AR experiences that are enhancements to an existing app, fun bite-size demos, or explore completely uncharted territory.

We’ll explore what’s possible right now with the process of making an AR experience on the web, including practical use-cases of “augmenting” current web applications, as well as handling progressive enhancement of AR on unsupported platforms. This exploration will take us to where we seem to be heading in terms of AR as a medium on the web platform.

OMG JSConf EU is coming back in 2019 https://2019.jsconf.eu/
Captions: 
	00:00:09,500 --> 00:00:11,349
Hello.

00:00:11,349 --> 00:00:16,330
My name is Jordan Santell from Google daydream WebXR team.

00:00:16,330 --> 00:00:21,400
I'm here to share about our augmented reality on the world.

00:00:21,400 --> 00:00:31,410
We're exploring how AR and VR will act on the web.

00:00:31,410 --> 00:00:33,070
What is augmented reality?

00:00:33,070 --> 00:00:38,850
Pokemon Go, filters, you know augmented reality.

00:00:38,850 --> 00:00:44,649
But it's the come positing of the digital world on the real world.

00:00:44,649 --> 00:00:47,260
It's similar to VA.

00:00:47,260 --> 00:00:57,059
If you take AR and everything is in the virtual world, all opaque, then that becomes VR.

00:00:57,059 --> 00:01:02,629
So, immersive computing is technology that operates on human immersion.

00:01:02,629 --> 00:01:06,210
When that's on the web, we call it the immersive web.

00:01:06,210 --> 00:01:12,500
So, we have been dreaming a lot about the future of the immersive web five to ten years

00:01:12,500 --> 00:01:13,659
from now.

00:01:13,659 --> 00:01:20,829
And I'm excited to share with you the story of our foundational block of work for this.

00:01:20,829 --> 00:01:22,909
Bringing augmented reality to the web.

00:01:22,909 --> 00:01:27,570
And I'll also talk a little bit about what can be coming up in the future.

00:01:27,570 --> 00:01:32,520
What can be going on in ten years from now and the challenges it brings and why this

00:01:32,520 --> 00:01:37,229
is an exciting time to jump on in new medium.

00:01:37,229 --> 00:01:46,960
Last summer Apple released AR kit and Google, AR core, these are for iOS and Android devices.

00:01:46,960 --> 00:01:53,619
Enabling mobile developers to create augmented reality applications and provides real world

00:01:53,619 --> 00:01:58,610
understanding so developing can create content in order to interact with the real world.

00:01:58,610 --> 00:02:04,120
So, developers having easy access is only one half of the equation.

00:02:04,120 --> 00:02:09,250
The other half is do users actually have access to AR capable devices?

00:02:09,250 --> 00:02:15,560
Previously this was limited to dedicated headsets, expensive dedicated headsets and less stable

00:02:15,560 --> 00:02:16,560
platforms.

00:02:16,560 --> 00:02:23,990
But these new forms, AR Core and AR Kit only require censors like gyro scope, accelerometer,

00:02:23,990 --> 00:02:28,660
RGB camera, things you can find on most more than smartphones.

00:02:28,660 --> 00:02:30,820
No special hardware is needed.

00:02:30,820 --> 00:02:36,540
There are around 600 million AR Core and Kit devices.

00:02:36,540 --> 00:02:44,590
It's the perfect time to start bringing augment the reality to the web.

00:02:44,590 --> 00:02:50,280
And there's like 200 megabytes of animated GIFs here, so, it might be a little slow.

00:02:50,280 --> 00:02:55,770
We built some prototype browsers, web AR on AR Kit and Core.

00:02:55,770 --> 00:02:56,770
Great names.

00:02:56,770 --> 00:03:03,440
And these are hackie web views we use to expose features from AR kit, AR Core to web development.

00:03:03,440 --> 00:03:12,750
First, we needed to identify which features we needed to create these experiences.

00:03:12,750 --> 00:03:18,560
We identified three high level requirements we would need to bring to the web.

00:03:18,560 --> 00:03:21,860
First off, we need to know where the device is in 3D space.

00:03:21,860 --> 00:03:27,320
Its position and orientation in order to sync our virtual world on top of the real world.

00:03:27,320 --> 00:03:29,310
VR has similar needs.

00:03:29,310 --> 00:03:31,490
You have to track it there.

00:03:31,490 --> 00:03:36,209
We were able to leverage a lot of the web VR API here.

00:03:36,209 --> 00:03:41,650
Sometimes this is referred to as six stuff, or six degrees of freedom.

00:03:41,650 --> 00:03:49,210
And that's referring to tracking three axis of position and three axis of orientation.

00:03:49,210 --> 00:03:52,700
We need to expose the camera stream and intrinsics.

00:03:52,700 --> 00:03:57,290
Intrinsics in this case are things like the field of view and perspective of a camera.

00:03:57,290 --> 00:04:02,180
Again, in order to sync our virtual world with the real world.

00:04:02,180 --> 00:04:06,590
Mobile platforms only allow one process to access the camera at a time.

00:04:06,590 --> 00:04:12,350
Since our AR platform is using the camera, we can't use get user media, for example.

00:04:12,350 --> 00:04:17,160
We have to have another way to expose this information.

00:04:17,160 --> 00:04:22,970
And finally, we need to be able to understand the world around us in order to interact with

00:04:22,970 --> 00:04:24,450
the real world.

00:04:24,450 --> 00:04:32,150
So, there is this broad description, the catch all term for the ability to interpret the

00:04:32,150 --> 00:04:33,150
real world.

00:04:33,150 --> 00:04:39,190
So, for example, finding surfaces to place an object on it or estimating light in an

00:04:39,190 --> 00:04:40,190
environment.

00:04:40,190 --> 00:04:45,500
So, you can see the brave line up, get scared when the lights go out.

00:04:45,500 --> 00:04:48,870
So, these three components are MVP feature set.

00:04:48,870 --> 00:04:53,280
And we also, of course, had some goals for this experiment.

00:04:53,280 --> 00:04:56,520
First off, general occupy exploration.

00:04:56,520 --> 00:05:01,480
We were going to look into the ergonomics or feasibility of different features that

00:05:01,480 --> 00:05:02,810
we do expose.

00:05:02,810 --> 00:05:11,230
So, for example, earlier prototypes used Tango smartphones with infrared depth cameras on

00:05:11,230 --> 00:05:12,230
them.

00:05:12,230 --> 00:05:13,330
Similar to a Kinect.

00:05:13,330 --> 00:05:22,140
So, this gave us around 30,0003D points per frame to position in the real world and

00:05:22,140 --> 00:05:24,070
use that in our experiments.

00:05:24,070 --> 00:05:31,670
So, needless to say, pushing 30,0003D points from the platform to web content and sometimes

00:05:31,670 --> 00:05:35,240
in this case to the GPU was not ideal.

00:05:35,240 --> 00:05:41,630
So, we moved our initial API from point clouds was difficult to use in a performant way.

00:05:41,630 --> 00:05:44,230
Wees want to discover what's possible.

00:05:44,230 --> 00:05:46,290
What kinds of things we can make.

00:05:46,290 --> 00:05:51,830
With post tracking in the camera feed, we can do things like build portals.

00:05:51,830 --> 00:05:57,660
And since this is the web, we can use physics libraries and service detection to make an

00:05:57,660 --> 00:05:59,110
AR mic drop.

00:05:59,110 --> 00:06:06,600
We have had to build tools as we're developing these experiences to debug what we're doing.

00:06:06,600 --> 00:06:12,320
It was very difficult before we had these tools to do things like visualize the surfaces.

00:06:12,320 --> 00:06:15,290
So, we know are we finding a plane?

00:06:15,290 --> 00:06:16,540
Are we miss something did I break something?

00:06:16,540 --> 00:06:20,460
What's going on here?

00:06:20,460 --> 00:06:23,070
And we also want to get developers excited about these features.

00:06:23,070 --> 00:06:27,530
We wanted developers to bug visor vendors and say, we want this.

00:06:27,530 --> 00:06:28,850
Please implement this.

00:06:28,850 --> 00:06:31,400
And convince organizations to check it out.

00:06:31,400 --> 00:06:32,730
Maybe this is something we can use.

00:06:32,730 --> 00:06:36,480
And see what core ideas people came up with.

00:06:36,480 --> 00:06:42,110
So, concurrently, while working on these prototypes, folks from a handful of browsers that you

00:06:42,110 --> 00:06:45,260
see up there, we were working on web VR2.0.

00:06:45,260 --> 00:06:54,940
And it was renamed to WebXR to support virtual reality and augmented reality experiences.

00:06:54,940 --> 00:07:01,419
There's now an implementation of WebXR inside of Chrome canary, I think the latest one,

00:07:01,419 --> 00:07:03,060
behind a flag.

00:07:03,060 --> 00:07:10,040
And all currently support VR, all these ones, have pledged to support the WebXR Spec as

00:07:10,040 --> 00:07:13,770
well.

00:07:13,770 --> 00:07:19,050
After about nine months of prototyping and working with standards, we now have a working

00:07:19,050 --> 00:07:21,060
version in a real browser.

00:07:21,060 --> 00:07:22,680
So, kind of, asterisk.

00:07:22,680 --> 00:07:28,700
So, when I wrote this proposal for JSConf EU months ago, I was kind of I had no idea

00:07:28,700 --> 00:07:32,520
what state this would be in by now.

00:07:32,520 --> 00:07:33,669
But I got pretty close.

00:07:33,669 --> 00:07:35,250
It's about like two weeks away.

00:07:35,250 --> 00:07:38,480
We're going through the reveal process right now.

00:07:38,480 --> 00:07:43,690
And who knew that shipping code to a billion plus users was difficult?

00:07:43,690 --> 00:07:47,960
So, it would be behind multiple flags.

00:07:47,960 --> 00:07:51,699
And the API will change for sure between several versions.

00:07:51,699 --> 00:07:54,180
It's very in flux.

00:07:54,180 --> 00:08:00,860
So, now that we have these features enabled in a real browser, we were able to show this

00:08:00,860 --> 00:08:03,870
off at Google IO a few weeks back.

00:08:03,870 --> 00:08:08,050
We have this educational site showcasing the statue from Mexico.

00:08:08,050 --> 00:08:15,580
It's kind of like this kind of a Wikipedia type article with a model on there.

00:08:15,580 --> 00:08:18,760
And we wanted to show off the progressive enhancement concept.

00:08:18,760 --> 00:08:20,210
Something very webby.

00:08:20,210 --> 00:08:25,230
So, if you're viewing this on desktop or mobile browser, you can move the statue around with

00:08:25,230 --> 00:08:26,790
mouse and keyword.

00:08:26,790 --> 00:08:30,590
And see the WebGL content and viewers.

00:08:30,590 --> 00:08:36,590
But on a browser that supports AR features with WebXR, you can place it in the world

00:08:36,590 --> 00:08:37,940
around you.

00:08:37,940 --> 00:08:41,650
I swear the performance is better than this GIF looks.

00:08:41,650 --> 00:08:48,700
You can place this life size statue and these annotations you can interact with.

00:08:48,700 --> 00:08:49,960
Click on them.

00:08:49,960 --> 00:08:56,170
And since it's just the web, render with HTML and CSS.

00:08:56,170 --> 00:09:00,890
All the same web tools you're used to.

00:09:00,890 --> 00:09:05,730
When we were showing this off in the web booth, there's a web booth and an Android booth at

00:09:05,730 --> 00:09:08,110
Google IO.

00:09:08,110 --> 00:09:12,780
I was at the Android booth, so we had a lot of Android developers check it out.

00:09:12,780 --> 00:09:16,380
And their reaction was they didn't quite believe this was the web.

00:09:16,380 --> 00:09:23,350
So, it was a lot of great feedback and confirmation that, yes, the web can performantly run WebGL

00:09:23,350 --> 00:09:26,580
experiences with AR.

00:09:26,580 --> 00:09:31,550
So, I mentioned earlier, understanding.

00:09:31,550 --> 00:09:36,830
So, the initial installation of augmented reality in Chrome only implements one set

00:09:36,830 --> 00:09:38,270
of see understanding.

00:09:38,270 --> 00:09:39,700
A hit test.

00:09:39,700 --> 00:09:46,460
You can array out from the device and interact with the real world, some surface, a collision.

00:09:46,460 --> 00:09:50,040
And this returns a 3D point if found.

00:09:50,040 --> 00:09:58,370
So, commonly in applications, you have something that traces real world surfaces and displace

00:09:58,370 --> 00:10:06,650
an indicator to the user indicating there's a space found here or no surface.

00:10:06,650 --> 00:10:08,660
And where that collision occurred.

00:10:08,660 --> 00:10:13,610
So, how we implement this radical behavior is on every frame we cast out array from the

00:10:13,610 --> 00:10:17,890
device and see if we get a hit.

00:10:17,890 --> 00:10:22,040
If it hits the surface, we change the radical to provide feedback.

00:10:22,040 --> 00:10:25,460
In this use case, we say, hey, we found a surface.

00:10:25,460 --> 00:10:27,770
Tap again to place a model.

00:10:27,770 --> 00:10:34,350
So, I'm going to show off some example code of implementing this radical.

00:10:34,350 --> 00:10:37,710
And I don't want to get too deep into this at all for several reasons.

00:10:37,710 --> 00:10:39,451
One, it's not in Canary yet.

00:10:39,451 --> 00:10:44,670
And there will be better resources to come than me talking about it.

00:10:44,670 --> 00:10:49,000
Like a Code Lab and other documentation.

00:10:49,000 --> 00:10:54,020
This code is the same as if you're doing a VR experience with WebXR.

00:10:54,020 --> 00:10:57,080
We can request a device from navigator XR.

00:10:57,080 --> 00:11:02,450
And we need to create an XR presentation context from a Canvas element.

00:11:02,450 --> 00:11:06,170
This is similar to getting a WebGL context from Canvas.

00:11:06,170 --> 00:11:11,170
And this is visible to the user and injected into the DOM.

00:11:11,170 --> 00:11:16,250
And as a big caveat, there's absolutely no error handling here that should be done.

00:11:16,250 --> 00:11:20,920
So, then you also request a session on that XR device.

00:11:20,920 --> 00:11:23,010
Passing in our XL presentation context.

00:11:23,010 --> 00:11:27,830
So, this request session is the part that's not fully spec壇 out for augmented reality

00:11:27,830 --> 00:11:28,830
yet.

00:11:28,830 --> 00:11:34,589
Ideally in the future this is where you would request specific augmented reality features.

00:11:34,589 --> 00:11:39,590
So, you can kind of react to different environments, different platforms can support different

00:11:39,590 --> 00:11:41,300
features.

00:11:41,300 --> 00:11:46,470
And right now, once this lands, if you enable the augmented reality flag, everything's always

00:11:46,470 --> 00:11:47,470
AR.

00:11:47,470 --> 00:11:49,650
The camera feed will always be wondering.

00:11:49,650 --> 00:11:52,730
And the underlying AR platform is always running.

00:11:52,730 --> 00:11:58,250
There's battery implications and privacy implications for using these features.

00:11:58,250 --> 00:12:02,680
So, that will change very soon.

00:12:02,680 --> 00:12:05,339
So, this is a lot of code right here, I know.

00:12:05,339 --> 00:12:06,800
So, it's a lot of boilerplate.

00:12:06,800 --> 00:12:10,420
This will all be abstracted away behind libraries.

00:12:10,420 --> 00:12:15,230
But really quickly, we need to set up another Canvas that we write the WebGL commands to.

00:12:15,230 --> 00:12:21,120
So, this gets moved over to the XL presentation context we injected into the DOM.

00:12:21,120 --> 00:12:23,250
This is the weirdest part of WebXR.

00:12:23,250 --> 00:12:25,440
You have two canvass.

00:12:25,440 --> 00:12:31,420
One that you write to that is not in the DOM, and one that displays that content that's

00:12:31,420 --> 00:12:32,420
in the DOM.

00:12:32,420 --> 00:12:35,370
So, it's very didn't than other kind of 3D stuff.

00:12:35,370 --> 00:12:39,649
So, things of note here, we create a frame of reference.

00:12:39,649 --> 00:12:40,649
And for our session.

00:12:40,649 --> 00:12:45,360
And then we start a request animation frame loop using our session's request animation

00:12:45,360 --> 00:12:46,360
frame.

00:12:46,360 --> 00:12:50,930
You have to hook into the native device in order to sync everything properly.

00:12:50,930 --> 00:12:54,970
So, similar to this is, again, we're working with 3D.

00:12:54,970 --> 00:13:01,680
So, this is similar to other kind of WebGL 3, JS development where we have a render loop.

00:13:01,680 --> 00:13:08,830
So, a render loop we get our devices and that contains like a matrix representing our device's

00:13:08,830 --> 00:13:10,720
position and orientation.

00:13:10,720 --> 00:13:12,440
And then we render every frame.

00:13:12,440 --> 00:13:17,090
Our virtual content that matches perfectly with our real world.

00:13:17,090 --> 00:13:20,730
And then we call this radical update function.

00:13:20,730 --> 00:13:27,250
So, this radical is a custom GS class and is like a disk on the ground that can trace

00:13:27,250 --> 00:13:28,250
surfaces.

00:13:28,250 --> 00:13:32,080
And in every frame, update function.

00:13:32,080 --> 00:13:36,070
I'm glossing over the details here.

00:13:36,070 --> 00:13:41,160
And we have an origin point and a direction vector representing this line coming out of

00:13:41,160 --> 00:13:42,269
our device.

00:13:42,269 --> 00:13:47,310
And we have a variable hit.

00:13:47,310 --> 00:13:50,920
And that's an array of all the hits that we've found.

00:13:50,920 --> 00:13:54,950
And it's easy to not find any hits if we're in low lighting and there's no kind of discernible

00:13:54,950 --> 00:13:57,860
feature on the surfaces we're looking at.

00:13:57,860 --> 00:13:59,660
If we found a hit, great.

00:13:59,660 --> 00:14:01,959
Continue the 3D position and orientation.

00:14:01,959 --> 00:14:04,220
And use that to position our radical.

00:14:04,220 --> 00:14:09,670
Maybe we can fade it out or hide it completely.

00:14:09,670 --> 00:14:14,830
So, no worries if that didn't completely make sense.

00:14:14,830 --> 00:14:20,420
The WebXR Spec is difficult to use, especially if you're not familiar with 3D on the web.

00:14:20,420 --> 00:14:25,850
I wanted to share the high level concepts and show what it looks like.

00:14:25,850 --> 00:14:30,800
And there will definitely be libraries that will abstract this away.

00:14:30,800 --> 00:14:33,800
So, what's next in the near future?

00:14:33,800 --> 00:14:41,540
Like I said, sometime in the next few weeks AR will land in Canary behind a flag.

00:14:41,540 --> 00:14:46,980
And with this flag, like I said, augmented reality will always be on.

00:14:46,980 --> 00:14:48,850
And we want this to be more fine tuned.

00:14:48,850 --> 00:14:53,330
So, with the request session API will have to be fleshed out.

00:14:53,330 --> 00:14:57,190
And right now, we're only supporting hit tests.

00:14:57,190 --> 00:15:01,760
That abstraction over the underlying service or mesh data that the AR platform provides.

00:15:01,760 --> 00:15:09,560
But in the future, we would like to have point clouds, services, mesh, light estimation as

00:15:09,560 --> 00:15:16,480
well as things like anchors which are abstract 3D points that you can place around the world.

00:15:16,480 --> 00:15:24,730
Over time, as the system understands the scene better, that will update and become more accurate.

00:15:24,730 --> 00:15:26,680
And right now, we're hand coding all this XL stuff.

00:15:26,680 --> 00:15:28,510
There's a lot of rough edges.

00:15:28,510 --> 00:15:30,230
The spec is still under development.

00:15:30,230 --> 00:15:33,460
And we're making tools internally as needed.

00:15:33,460 --> 00:15:39,120
So, libraries like 3JS currently support some XR use cases.

00:15:39,120 --> 00:15:42,399
And in the future, we imagine this would also support AR.

00:15:42,399 --> 00:15:48,560
And if you're familiar with Mozilla's A frame web library, after our prototype browsers,

00:15:48,560 --> 00:15:52,560
someone a week later made an AR component that worked great.

00:15:52,560 --> 00:15:56,050
So, that community is very proactive.

00:15:56,050 --> 00:16:01,310
We imagine all these libraries will support creating AR for a better experience rather

00:16:01,310 --> 00:16:05,899
than writing using the API directly.

00:16:05,899 --> 00:16:11,519
So, what I'm talking about right now is mostly mobile AR.

00:16:11,519 --> 00:16:17,130
And this provides a lot of immediate solutions for things like shopping, education and entertainment.

00:16:17,130 --> 00:16:22,470
And this is only the beginning as the future gets much more interesting and weird.

00:16:22,470 --> 00:16:27,440
So, that's, you know, the future in the next few months or a year.

00:16:27,440 --> 00:16:30,269
What about the next five years?

00:16:30,269 --> 00:16:35,029
A little further out we can imagine computer vision and machine learning being used with

00:16:35,029 --> 00:16:36,260
augmented reality.

00:16:36,260 --> 00:16:40,899
Mozilla has been working on their own prototype browsers using AR.

00:16:40,899 --> 00:16:47,100
And they have been doing some research with computer vision within WebXR.

00:16:47,100 --> 00:16:52,089
And something that's been talked about a lot in the AR industry is this concept of the

00:16:52,089 --> 00:16:53,990
AR cloud, or cloud anchors.

00:16:53,990 --> 00:16:58,209
Which lets multiple users share their virtual world.

00:16:58,209 --> 00:17:04,319
This feature is currently a course and works for iOS and Android.

00:17:04,319 --> 00:17:07,949
But this will be critical to have also on the web.

00:17:07,949 --> 00:17:13,199
Unclear if it will be different services or part of the native API.

00:17:13,199 --> 00:17:14,199
No idea.

00:17:14,199 --> 00:17:20,409
But this is necessary and critical for our user immersive content.

00:17:20,409 --> 00:17:22,939
And hardware is making a lot of progress.

00:17:22,939 --> 00:17:30,489
So, with the latest generation of VR headsets, these R and 1 headsets, tracking for VR is

00:17:30,489 --> 00:17:33,289
much more accessible now.

00:17:33,289 --> 00:17:37,379
Rather than needing a whole room set up where you're installing I'm not sure what they

00:17:37,379 --> 00:17:45,080
call it like lasers to track your movement and also like an expensive gaming computer,

00:17:45,080 --> 00:17:46,080
essentially.

00:17:46,080 --> 00:17:52,090
So, we're getting closer to these, like, smaller accessible rather affordable, comparatively,

00:17:52,090 --> 00:17:55,730
headsets that people can use.

00:17:55,730 --> 00:18:10,870
AR headsets like HoloLens and others, they're out and quite expensive and specialized hardware.

00:18:10,870 --> 00:18:16,909
And in this head mounted display AR world, those will be supporting browsers.

00:18:16,909 --> 00:18:19,700
So, the specification has to work for mobile.

00:18:19,700 --> 00:18:22,940
But it's got to have the head mounted displays.

00:18:22,940 --> 00:18:27,669
And the head mounted displays will have paradigms.

00:18:27,669 --> 00:18:35,669
The HoloLens has pretty good support for hand gestures to interact with content as well

00:18:35,669 --> 00:18:37,580
as voice commands.

00:18:37,580 --> 00:18:40,720
So, we seem to be trending towards that.

00:18:40,720 --> 00:18:45,720
Like a limited physical interface and using more computer vision machine learning to interact

00:18:45,720 --> 00:18:47,840
with this virtual environment.

00:18:47,840 --> 00:18:55,169
So, further out in the future again, this is looking at trends, making informed guesses.

00:18:55,169 --> 00:18:57,779
But at the end of the day, they're still guesses.

00:18:57,779 --> 00:19:00,570
We have no idea.

00:19:00,570 --> 00:19:06,899
Head mounted displays will evolve into hip glasses with these AR capabilities.

00:19:06,899 --> 00:19:08,419
Years out.

00:19:08,419 --> 00:19:13,750
And we have these socially acceptable headsets and we can anchor content in the real world,

00:19:13,750 --> 00:19:18,830
we can reframe what immersive computing will look like in the future.

00:19:18,830 --> 00:19:21,720
Contextualized information that is highly scalable.

00:19:21,720 --> 00:19:23,809
Discovered serendipitously.

00:19:23,809 --> 00:19:28,409
Instantly accessible, secure and interoperable across devices.

00:19:28,409 --> 00:19:32,400
And that sounds a lot like the web.

00:19:32,400 --> 00:19:36,750
Imagine walking down the street as contextually relevant content is appearing.

00:19:36,750 --> 00:19:44,330
We're executing content, that's what the web is for.

00:19:44,330 --> 00:19:49,820
And in this future our immersive user agents have even more responsibility.

00:19:49,820 --> 00:19:55,289
It's necessary for users to be able to control and filter that content.

00:19:55,289 --> 00:19:58,830
Especially since content can be obstructing our vision when we're crossing the street,

00:19:58,830 --> 00:19:59,830
for example.

00:19:59,830 --> 00:20:02,970
So, you know, that's pretty important.

00:20:02,970 --> 00:20:04,440
We need to get that right.

00:20:04,440 --> 00:20:07,729
So, these are just some trends based off of technology.

00:20:07,729 --> 00:20:14,559
But in the future, society will drive immersive computing more so than technological possibilities.

00:20:14,559 --> 00:20:19,340
Cultures will define the relationships with immersive technology based off of the social

00:20:19,340 --> 00:20:22,510
intimacy and privacy values.

00:20:22,510 --> 00:20:27,799
Usage will be influenced by state's legislation or indifference on data ownership, net neutrality

00:20:27,799 --> 00:20:30,279
and corporate regulations.

00:20:30,279 --> 00:20:35,200
A society's economic equality will determine access to these immersive technologies and

00:20:35,200 --> 00:20:38,649
therefore its creators, directions and applications.

00:20:38,649 --> 00:20:44,330
And the outcome of each group's sociopolitical decisions will define their relationship with

00:20:44,330 --> 00:20:46,080
this technology.

00:20:46,080 --> 00:20:50,639
We can have some societies comfortable with ubiquitous always on, always recording head

00:20:50,639 --> 00:20:53,539
mounted displays in public areas.

00:20:53,539 --> 00:21:00,210
And others as needed, education and training.

00:21:00,210 --> 00:21:06,759
And they are all intertwined and it's important to consider all of this within the context

00:21:06,759 --> 00:21:08,780
of society.

00:21:08,780 --> 00:21:11,539
So, there are a lot of unknowns.

00:21:11,539 --> 00:21:14,049
Some are exciting, some are terrifying.

00:21:14,049 --> 00:21:18,480
And within these unknowns, that's where we must exert our influence.

00:21:18,480 --> 00:21:22,399
We must design a future that puts human first, not existing power structures.

00:21:22,399 --> 00:21:24,000
And it's important to us.

00:21:24,000 --> 00:21:28,881
And to tech critics, sci fi records and groups underrepresented in tech and ask, how can

00:21:28,881 --> 00:21:31,950
this be used for evil?

00:21:31,950 --> 00:21:36,419
This ubiquitous always on technology has massive privacy implications.

00:21:36,419 --> 00:21:42,220
And this trend of cloud computing could contribute to the surveillance state.

00:21:42,220 --> 00:21:46,779
Maybe we can steer towards own device computing.

00:21:46,779 --> 00:21:55,830
This is being shopped around to police departments around the world for facial recognition purposes.

00:21:55,830 --> 00:22:05,029
This could threaten civil liberties until or unless our laws catch up with technology.

00:22:05,029 --> 00:22:09,759
Unchecked capitalism could result in hyper consumerism unless we have the ability to

00:22:09,759 --> 00:22:13,049
control this content.

00:22:13,049 --> 00:22:17,129
And if this technology isn't accessible to all, then we can further increase the digital

00:22:17,129 --> 00:22:21,610
divide separating between those who have and have not.

00:22:21,610 --> 00:22:28,470
So, we must constantly be vigilant as technologists and steer this towards good.

00:22:28,470 --> 00:22:32,020
Patricia had a great talk yesterday on ethics.

00:22:32,020 --> 00:22:34,429
All that applies here as well.

00:22:34,429 --> 00:22:38,679
But the fun parts, there's a lot to be excited about.

00:22:38,679 --> 00:22:40,979
So, we are in uncharted lands.

00:22:40,979 --> 00:22:44,759
We're learning about this medium as it's being created and as we go.

00:22:44,759 --> 00:22:51,059
So, Apple, Microsoft, Google, have all published these UX guidelines and they're just scratching

00:22:51,059 --> 00:22:54,659
the surface of what's possible now and in the future.

00:22:54,659 --> 00:23:00,190
You could invent the next gesture that is as ubiquitous as scrolling and clicking and

00:23:00,190 --> 00:23:03,759
interacting with these virtual objects.

00:23:03,759 --> 00:23:08,889
And when we're talking about the web, we have access to hundreds of thousands of npm packages

00:23:08,889 --> 00:23:12,609
that we can use and mix and match in our products.

00:23:12,609 --> 00:23:17,649
And there are no immersive technology libraries currently.

00:23:17,649 --> 00:23:24,730
So, you could create the next JQuery or Lodash or React in these environments.

00:23:24,730 --> 00:23:30,990
I was going to say there are no killer apps for VR or AR, but I played Beat Saber a couple

00:23:30,990 --> 00:23:32,320
weeks ago.

00:23:32,320 --> 00:23:35,210
It was great.

00:23:35,210 --> 00:23:41,419
But what's getting people to jump into virtual and augmented reality?

00:23:41,419 --> 00:23:44,529
To get over the initial friction?

00:23:44,529 --> 00:23:51,849
Since personal computing is a multi disciplinary field, I think we will see a lot of new collaborations.

00:23:51,849 --> 00:23:56,940
Like a fashion designer and a historian or a machine learning expert and a social worker.

00:23:56,940 --> 00:24:01,259
Or in this case, an audio engineer and a Jedi.

00:24:01,259 --> 00:24:07,679
So, it's a super exciting time to start these new collaborations as engineers and designers.

00:24:07,679 --> 00:24:12,460
Matt has a great article on AR first applications.

00:24:12,460 --> 00:24:18,229
And something that stuck with me here is with new mediums we mostly just port the previous

00:24:18,229 --> 00:24:21,110
or older mediums into it.

00:24:21,110 --> 00:24:26,419
When mobile was the new thing, we mostly just had tiny websites on our phones.

00:24:26,419 --> 00:24:33,539
It wasn't until we started leveraging location or constant access and always connected devices

00:24:33,539 --> 00:24:39,950
that we truly leveraged the platform with things like Twitter and Lyft.

00:24:39,950 --> 00:24:41,840
And same with augmented reality.

00:24:41,840 --> 00:24:48,279
A lot of augmented reality applications are just putting 2D content around the world.

00:24:48,279 --> 00:24:50,700
A lot of it's like, this could have been a website.

00:24:50,700 --> 00:24:53,159
This would have been easier.

00:24:53,159 --> 00:25:00,879
So, we're still discovering what are the applications that truly leverage this new medium?

00:25:00,879 --> 00:25:05,570
And that's it's still important to play around which explore and have fun with this

00:25:05,570 --> 00:25:06,570
medium.

00:25:06,570 --> 00:25:13,419
So, before I'm done, I'll leave a few links, the immersive web community group is on GitHub.

00:25:13,419 --> 00:25:17,220
You can follow along with the spec.

00:25:17,220 --> 00:25:23,369
And Mozilla's A frame is a great technology that uses web components to create VR and

00:25:23,369 --> 00:25:24,629
augmented reality scenes.

00:25:24,629 --> 00:25:30,179
You don't have to be familiar with 3D or WebGL or any that have stuff.

00:25:30,179 --> 00:25:36,359
And the immersive web weekly is a newsletter I started a few weeks ago that's summarizing

00:25:36,359 --> 00:25:43,239
all that's happening in this fast moving space.

00:25:43,239 --> 00:25:47,090
Be on the lookout for AR landing in Chrome Canary very soon.

00:25:47,090 --> 00:25:48,849
And hope you all explore the future.

00:25:48,849 --> 00:25:49,349

YouTube URL: https://www.youtube.com/watch?v=bqmT0_w6_qc


