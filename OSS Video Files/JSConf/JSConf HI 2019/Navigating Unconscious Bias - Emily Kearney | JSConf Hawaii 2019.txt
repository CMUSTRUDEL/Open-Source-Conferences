Title: Navigating Unconscious Bias - Emily Kearney | JSConf Hawaii 2019
Publication date: 2019-06-24
Playlist: JSConf HI 2019
Description: 
	Emily Kearney discusses unconscious bias and provides a suite of tools and mental models to help identify and overcome biases in a 100% empowerment, 0% guilt trip talk.

JSConf Hawaii is returning in 2020. Learn more at https://www.jsconfhi.com/
Captions: 
	00:00:06,580 --> 00:00:09,720
I'm Emily and in today's talk we're

00:00:08,889 --> 00:00:12,120
going to talk about

00:00:09,720 --> 00:00:13,440
is an unconscious bias and how it

00:00:12,120 --> 00:00:15,770
affects folks like us and tech

00:00:13,440 --> 00:00:20,340
workspaces and what we can do about it

00:00:15,770 --> 00:00:23,430
great so just a little bit more about me

00:00:20,340 --> 00:00:25,740
my pronouns are she her hers and I'm a

00:00:23,430 --> 00:00:27,689
PhD candidate at UC Berkeley

00:00:25,740 --> 00:00:29,310
I'm studying chocolate and how humans

00:00:27,689 --> 00:00:31,890
can save it so I know very important

00:00:29,310 --> 00:00:33,300
work but well my work doesn't have me

00:00:31,890 --> 00:00:35,190
playing around in JavaScript all that

00:00:33,300 --> 00:00:38,010
much I do do a lot of data science and

00:00:35,190 --> 00:00:40,649
RN Python but today my day job doesn't

00:00:38,010 --> 00:00:42,300
matter instead we're going to talk about

00:00:40,649 --> 00:00:44,280
something that affects everyone in this

00:00:42,300 --> 00:00:47,940
room because we are all humans and

00:00:44,280 --> 00:00:49,470
that's unconscious bias so who here has

00:00:47,940 --> 00:00:51,980
gone to some sort of diversity training

00:00:49,470 --> 00:00:55,410
that they felt was a big waste of time

00:00:51,980 --> 00:00:57,210
yeah I definitely have and as it turns

00:00:55,410 --> 00:00:59,610
out studies have shown that a bad

00:00:57,210 --> 00:01:02,280
training can actually have a negative

00:00:59,610 --> 00:01:04,259
effect on people so they can come out of

00:01:02,280 --> 00:01:05,939
a bad training with more hostile

00:01:04,259 --> 00:01:07,860
attitudes towards people who are

00:01:05,939 --> 00:01:10,259
different from them and so at the

00:01:07,860 --> 00:01:12,030
unconscious bias project where most of

00:01:10,259 --> 00:01:14,220
us are either in grad school or have

00:01:12,030 --> 00:01:15,600
PhDs in STEM fields we've always been

00:01:14,220 --> 00:01:18,300
really careful to only promote

00:01:15,600 --> 00:01:20,040
evidence-based approaches we also want

00:01:18,300 --> 00:01:22,710
the information we provide to empower

00:01:20,040 --> 00:01:24,240
you instead of leading you even more

00:01:22,710 --> 00:01:26,190
deflated about the current state of the

00:01:24,240 --> 00:01:29,490
world that's why our motto is a hundred

00:01:26,190 --> 00:01:31,770
percent oops yeah a hundred percent

00:01:29,490 --> 00:01:33,810
empowerment zero percent guilt trip and

00:01:31,770 --> 00:01:35,940
today I'm going to teach you all tools

00:01:33,810 --> 00:01:38,180
to reduce the effect of unconscious bias

00:01:35,940 --> 00:01:41,220
and I want you to leave this conference

00:01:38,180 --> 00:01:43,049
as unconscious bias warriors our

00:01:41,220 --> 00:01:45,479
ambassadors that are dedicated to making

00:01:43,049 --> 00:01:47,220
a positive impact in your circle so that

00:01:45,479 --> 00:01:50,729
tech becomes more and more diverse

00:01:47,220 --> 00:01:52,619
inclusive and equitable so I'll be going

00:01:50,729 --> 00:01:55,020
over a lot of tools and resources today

00:01:52,619 --> 00:01:57,180
in my talk but don't worry if you miss

00:01:55,020 --> 00:01:58,770
something you'll be able to access all

00:01:57,180 --> 00:02:01,140
of the peer reviewed literature and all

00:01:58,770 --> 00:02:04,200
of the other resources I mentioned on

00:02:01,140 --> 00:02:05,939
our website so finally before we jump

00:02:04,200 --> 00:02:07,950
into the good stuff I want to mention

00:02:05,939 --> 00:02:10,739
that I'll be touching on issues that are

00:02:07,950 --> 00:02:12,330
deeply personal and some of the examples

00:02:10,739 --> 00:02:14,280
that I may present could be triggering

00:02:12,330 --> 00:02:15,900
to some of you in the audience but I

00:02:14,280 --> 00:02:18,209
present them in the hopes of having a

00:02:15,900 --> 00:02:21,030
frank conversation about why they are

00:02:18,209 --> 00:02:23,520
harmful and what we can take what steps

00:02:21,030 --> 00:02:25,700
we can take as individuals and by Stan

00:02:23,520 --> 00:02:27,720
to make the world a better place so

00:02:25,700 --> 00:02:29,790
unfortunately I won't really have any

00:02:27,720 --> 00:02:31,500
time for questions from the audience up

00:02:29,790 --> 00:02:32,790
here on stage but I encourage you to

00:02:31,500 --> 00:02:34,170
come find me if you have questions

00:02:32,790 --> 00:02:36,330
concerns or comments about the

00:02:34,170 --> 00:02:39,510
presentation and so with that let's

00:02:36,330 --> 00:02:41,490
start with a definition so we need to be

00:02:39,510 --> 00:02:43,950
on the same page about what bias is and

00:02:41,490 --> 00:02:46,680
biases prejudice in favor of or against

00:02:43,950 --> 00:02:49,410
one group person or thing compared with

00:02:46,680 --> 00:02:51,960
another and usually a way considered to

00:02:49,410 --> 00:02:54,180
be unfair so unconscious or implicit

00:02:51,960 --> 00:02:55,920
bias is bias that you have even though

00:02:54,180 --> 00:02:59,310
you don't consciously agree with it or

00:02:55,920 --> 00:03:01,290
even know that it's there and most

00:02:59,310 --> 00:03:03,240
people don't want to be racist or sexist

00:03:01,290 --> 00:03:05,190
but a lot of us are unconsciously biased

00:03:03,240 --> 00:03:06,780
against certain groups anyway and this

00:03:05,190 --> 00:03:08,940
starts with negative stereotypes from

00:03:06,780 --> 00:03:11,820
the world around us what we see in TV

00:03:08,940 --> 00:03:13,860
shows in movies here on the radio read

00:03:11,820 --> 00:03:15,690
in books see on the internet and even

00:03:13,860 --> 00:03:18,030
pick up from our friends and family and

00:03:15,690 --> 00:03:19,950
so even if we don't unconsciously agree

00:03:18,030 --> 00:03:21,330
with these negative stereotypes they can

00:03:19,950 --> 00:03:23,490
still worm their way into our

00:03:21,330 --> 00:03:25,950
unconscious and influence how we think

00:03:23,490 --> 00:03:28,140
and act so I'd like to call attention to

00:03:25,950 --> 00:03:30,270
a few examples of negative stereotypes

00:03:28,140 --> 00:03:32,790
from American culture to show you what

00:03:30,270 --> 00:03:34,680
I'm talking about men are seen as

00:03:32,790 --> 00:03:36,959
breadwinners and women as caregivers

00:03:34,680 --> 00:03:39,390
women can be beautiful or intelligent

00:03:36,959 --> 00:03:41,820
but not both African Americans are

00:03:39,390 --> 00:03:43,709
commonly portrayed as loud and biased

00:03:41,820 --> 00:03:45,930
can even come from simple associations

00:03:43,709 --> 00:03:48,209
of patterns that we notice unconsciously

00:03:45,930 --> 00:03:51,360
like that custodians are often racial

00:03:48,209 --> 00:03:53,580
minorities and before stereotypes even

00:03:51,360 --> 00:03:55,650
lead to bias stereotypes themselves are

00:03:53,580 --> 00:03:57,330
damaging being stereotyped has been

00:03:55,650 --> 00:03:59,760
found to lead to short-term aggression

00:03:57,330 --> 00:04:01,920
inability to focus which results in

00:03:59,760 --> 00:04:04,350
decrease performance and overeating

00:04:01,920 --> 00:04:06,930
people who regularly encounter the

00:04:04,350 --> 00:04:08,820
threat of being judged by a negative

00:04:06,930 --> 00:04:11,850
stereotype are more likely to have

00:04:08,820 --> 00:04:14,070
hypertension be depressed and to rate

00:04:11,850 --> 00:04:16,020
their own health more poorly and being

00:04:14,070 --> 00:04:18,359
on the receiving end of positive bias or

00:04:16,020 --> 00:04:20,100
positive stereotypes hurts too implying

00:04:18,359 --> 00:04:22,049
someone is good at math because they are

00:04:20,100 --> 00:04:23,940
Asian might make the receiver of the

00:04:22,049 --> 00:04:26,910
comment dislike the speaker and invoke

00:04:23,940 --> 00:04:29,250
feelings of anger but when stereotypes

00:04:26,910 --> 00:04:31,470
lead to unconscious bias about who can

00:04:29,250 --> 00:04:33,780
thrive in science and tech this can be

00:04:31,470 --> 00:04:36,630
even this can have even more damaging

00:04:33,780 --> 00:04:38,340
effects by creating barriers to entering

00:04:36,630 --> 00:04:41,310
STEM fields and this is the so-called

00:04:38,340 --> 00:04:43,260
pipeline problem and folks start being

00:04:41,310 --> 00:04:45,600
affected by this bias at a very early

00:04:43,260 --> 00:04:47,430
age and it happens around the world so

00:04:45,600 --> 00:04:49,890
this paper is actually from Germany and

00:04:47,430 --> 00:04:52,020
I'll be presenting a few more papers and

00:04:49,890 --> 00:04:53,490
then in this section here the

00:04:52,020 --> 00:04:54,720
researchers showed that teachers have

00:04:53,490 --> 00:04:56,850
both conscious and unconscious

00:04:54,720 --> 00:04:58,770
stereotypes about boys being better at

00:04:56,850 --> 00:05:00,810
math and girls being better at languages

00:04:58,770 --> 00:05:02,520
and those stereotypes influence whether

00:05:00,810 --> 00:05:04,500
the teachers recommend boys and girls

00:05:02,520 --> 00:05:06,720
for science and technology schools

00:05:04,500 --> 00:05:08,940
versus language oriented schools and

00:05:06,720 --> 00:05:12,510
this bias continues from grade school

00:05:08,940 --> 00:05:14,360
through graduate school but the pipeline

00:05:12,510 --> 00:05:16,770
is only the beginning of the problem

00:05:14,360 --> 00:05:18,660
once marginalized people get their foot

00:05:16,770 --> 00:05:22,530
in the door unconscious bias leads to

00:05:18,660 --> 00:05:24,480
barriers to rising in stem so study

00:05:22,530 --> 00:05:26,460
after study has found evidence of bias

00:05:24,480 --> 00:05:28,170
against almost any marginalized group

00:05:26,460 --> 00:05:29,970
you can think of in terms of who gets

00:05:28,170 --> 00:05:32,700
called back for an interview and as we

00:05:29,970 --> 00:05:34,500
all know jobs are pretty important and

00:05:32,700 --> 00:05:36,720
this type of bias occurs even when that

00:05:34,500 --> 00:05:38,700
discrimination is illegal in that area

00:05:36,720 --> 00:05:41,280
so in the United States there's bias

00:05:38,700 --> 00:05:43,230
against openly gay men and bias against

00:05:41,280 --> 00:05:45,180
people with disabilities even with the

00:05:43,230 --> 00:05:48,240
disability would not have affected the

00:05:45,180 --> 00:05:50,670
person's ability to perform the job in

00:05:48,240 --> 00:05:52,860
the United States and China being a

00:05:50,670 --> 00:05:54,420
parent hurt job prospects for women but

00:05:52,860 --> 00:05:56,820
it was actually the opposite for men

00:05:54,420 --> 00:05:59,250
being a father helped job prospects and

00:05:56,820 --> 00:06:01,440
one last example bias was found against

00:05:59,250 --> 00:06:04,140
people with Arabic sounding names upon

00:06:01,440 --> 00:06:06,750
both among both American and Dutch

00:06:04,140 --> 00:06:09,090
reviewers so imagine experiencing these

00:06:06,750 --> 00:06:10,920
biases every time you apply to a job or

00:06:09,090 --> 00:06:12,950
when you want to move up within your own

00:06:10,920 --> 00:06:16,020
company or switch to a different team

00:06:12,950 --> 00:06:18,150
and imagine how much harder that might

00:06:16,020 --> 00:06:20,400
be or how much worse that bias might be

00:06:18,150 --> 00:06:22,890
if you're an Arabic lesbian mother in a

00:06:20,400 --> 00:06:24,660
wheelchair and studies have actually

00:06:22,890 --> 00:06:27,180
looked specifically at this type of

00:06:24,660 --> 00:06:29,220
intersection for instance between gender

00:06:27,180 --> 00:06:31,470
and race so women and racial minorities

00:06:29,220 --> 00:06:33,690
are both less likely to be given the

00:06:31,470 --> 00:06:35,640
benefit of the doubt and the problem is

00:06:33,690 --> 00:06:37,980
only worse if you're both a woman and a

00:06:35,640 --> 00:06:39,810
racial minority so when women of color

00:06:37,980 --> 00:06:41,580
demonstrate leadership and competence

00:06:39,810 --> 00:06:43,290
there are certain is's perceived

00:06:41,580 --> 00:06:46,200
negatively and the data shows that they

00:06:43,290 --> 00:06:48,150
get pushed back for it so even when

00:06:46,200 --> 00:06:49,200
you're successful unsupportive work

00:06:48,150 --> 00:06:51,600
environments are

00:06:49,200 --> 00:06:54,330
reason for leading stem in the next few

00:06:51,600 --> 00:06:55,980
slides I'll be focusing on women but

00:06:54,330 --> 00:07:00,420
these trends are representative of most

00:06:55,980 --> 00:07:02,490
underrepresented groups in tech so let

00:07:00,420 --> 00:07:04,050
me be frank women do leave stem at

00:07:02,490 --> 00:07:06,390
greater rates than they leave the non

00:07:04,050 --> 00:07:07,980
stem workforce the orange line that you

00:07:06,390 --> 00:07:09,600
see on this screen is women being

00:07:07,980 --> 00:07:12,420
retained in non STEM fields

00:07:09,600 --> 00:07:14,760
now let's look at stem as you can see

00:07:12,420 --> 00:07:17,520
the retention is much much worse and

00:07:14,760 --> 00:07:19,200
less than 20% of the women who leave

00:07:17,520 --> 00:07:21,540
STEM fields are also leaving the

00:07:19,200 --> 00:07:24,750
workforce for instance like taking care

00:07:21,540 --> 00:07:26,820
of family more than 80% of the women

00:07:24,750 --> 00:07:28,590
leaving the stem workforce are just

00:07:26,820 --> 00:07:30,960
getting jobs in other fields they're

00:07:28,590 --> 00:07:32,790
going to non STEM fields

00:07:30,960 --> 00:07:35,490
so let's look at a different control and

00:07:32,790 --> 00:07:38,130
focus in on just the tech industry when

00:07:35,490 --> 00:07:40,260
you compare the attrition rates of of

00:07:38,130 --> 00:07:43,110
women to men in tech it's more than

00:07:40,260 --> 00:07:47,880
twice as high 41% of women leave tech

00:07:43,110 --> 00:07:49,860
versus just 17% of men and on top of the

00:07:47,880 --> 00:07:51,240
well-documented pay gap that exists

00:07:49,860 --> 00:07:53,460
there are many cultural cues that a

00:07:51,240 --> 00:07:55,710
company can give to signal that an

00:07:53,460 --> 00:07:58,470
employee is not valued which in turn

00:07:55,710 --> 00:08:00,870
encourages that employee to leave one is

00:07:58,470 --> 00:08:02,670
feedback in performance reviews in this

00:08:00,870 --> 00:08:04,410
study the majority of women received

00:08:02,670 --> 00:08:06,450
reviews with a negative unhelpful

00:08:04,410 --> 00:08:08,430
feedback while the majority of men

00:08:06,450 --> 00:08:10,410
received only constructive feedback and

00:08:08,430 --> 00:08:12,000
the negative feedback for women was

00:08:10,410 --> 00:08:14,610
about personality in ways that reflect

00:08:12,000 --> 00:08:17,570
common unconscious biases like telling

00:08:14,610 --> 00:08:20,400
women to pipe down or be less abrasive

00:08:17,570 --> 00:08:22,590
so together these barriers keep some

00:08:20,400 --> 00:08:24,770
groups underrepresented in stem and tech

00:08:22,590 --> 00:08:27,150
and reinforce pre-existing stereotypes

00:08:24,770 --> 00:08:29,160
creating what we've dubbed the positive

00:08:27,150 --> 00:08:32,220
feedback loop of negative stereotypes

00:08:29,160 --> 00:08:33,750
and there's so so much more

00:08:32,220 --> 00:08:35,640
peer-reviewed literature out there that

00:08:33,750 --> 00:08:38,490
shows that unconscious bias is real and

00:08:35,640 --> 00:08:40,800
it is impacting diversity if you'd like

00:08:38,490 --> 00:08:42,240
to explore more please head over to our

00:08:40,800 --> 00:08:43,800
website where you can find all of the

00:08:42,240 --> 00:08:46,800
papers that I mentioned in the stock as

00:08:43,800 --> 00:08:49,260
well as hundreds others about almost

00:08:46,800 --> 00:08:52,680
every aspect of bias its effect and what

00:08:49,260 --> 00:08:55,350
we can do about it ok so let's take a

00:08:52,680 --> 00:08:57,300
breath because that was a lot of bad

00:08:55,350 --> 00:08:58,890
news and I know some of you might be

00:08:57,300 --> 00:09:01,020
feeling a little low at this point in

00:08:58,890 --> 00:09:02,390
the talk but the good news is that we're

00:09:01,020 --> 00:09:03,890
only about half way through

00:09:02,390 --> 00:09:06,020
that means we have a lot of time to talk

00:09:03,890 --> 00:09:07,460
about solutions so even though

00:09:06,020 --> 00:09:10,310
stereotypes come from our environment

00:09:07,460 --> 00:09:11,840
that doesn't make us powerless happily

00:09:10,310 --> 00:09:13,760
there's enough studies out there that we

00:09:11,840 --> 00:09:16,600
know how to reduce negative effects of

00:09:13,760 --> 00:09:19,070
bias and reliably reduce our biases too

00:09:16,600 --> 00:09:21,050
so thoughts are actually our first line

00:09:19,070 --> 00:09:22,880
of defense when confronting bias so I'm

00:09:21,050 --> 00:09:24,200
going to teach you two techniques that

00:09:22,880 --> 00:09:26,600
can help you decrease your own

00:09:24,200 --> 00:09:28,160
unconscious bias but before we start

00:09:26,600 --> 00:09:30,680
training we need to cover some

00:09:28,160 --> 00:09:33,590
prerequisites in order for these

00:09:30,680 --> 00:09:35,450
techniques to be effective for you you

00:09:33,590 --> 00:09:37,670
need to be motivated to overcome your

00:09:35,450 --> 00:09:39,860
unconscious bias hopefully the positive

00:09:37,670 --> 00:09:43,070
loop of negative stereo bat stereotypes

00:09:39,860 --> 00:09:44,810
helps to give you that motivation you

00:09:43,070 --> 00:09:46,850
then need to take steps to become aware

00:09:44,810 --> 00:09:48,380
of your bias and why it exists and we'll

00:09:46,850 --> 00:09:49,760
explore how to recognize your own bias

00:09:48,380 --> 00:09:51,710
and the first technique that I teach you

00:09:49,760 --> 00:09:54,170
but you can also do it through more

00:09:51,710 --> 00:09:56,450
technical ways by taking an implicit

00:09:54,170 --> 00:09:59,330
association test at harvard's project

00:09:56,450 --> 00:10:01,880
implicit website you'll need to learn

00:09:59,330 --> 00:10:03,440
the to detect the subtle influence of

00:10:01,880 --> 00:10:05,270
bias and your everyday surroundings

00:10:03,440 --> 00:10:06,860
which helps you know when to act and

00:10:05,270 --> 00:10:09,620
then you have to practice these

00:10:06,860 --> 00:10:11,270
strategies to reduce your bias that you

00:10:09,620 --> 00:10:13,430
know what to do in that moment when you

00:10:11,270 --> 00:10:15,350
recognize it and yes you do have to

00:10:13,430 --> 00:10:16,070
practice without practice there will be

00:10:15,350 --> 00:10:18,290
no change

00:10:16,070 --> 00:10:20,900
okay so let's learn our first strategy I

00:10:18,290 --> 00:10:22,550
want you to take a look at this picture

00:10:20,900 --> 00:10:24,500
and think to yourself what is your first

00:10:22,550 --> 00:10:26,150
reaction to it if I asked you to

00:10:24,500 --> 00:10:27,230
describe who these people are and what

00:10:26,150 --> 00:10:30,140
they were doing what would you think

00:10:27,230 --> 00:10:32,030
what would you say did you assume that

00:10:30,140 --> 00:10:34,550
each of these people are engineers or

00:10:32,030 --> 00:10:37,310
designers beginners or experienced

00:10:34,550 --> 00:10:40,220
straight or queer disabled or not

00:10:37,310 --> 00:10:42,470
communicating well or poorly when I

00:10:40,220 --> 00:10:44,930
first saw this picture I thought young

00:10:42,470 --> 00:10:45,500
and experienced interns and that's my

00:10:44,930 --> 00:10:47,570
bias

00:10:45,500 --> 00:10:49,700
so once you've acknowledged that your

00:10:47,570 --> 00:10:51,470
own biases are informing the way you see

00:10:49,700 --> 00:10:53,710
people you can choose to replace any

00:10:51,470 --> 00:10:55,700
biased aspect of your thoughts or your

00:10:53,710 --> 00:10:59,690
reaction with a different less

00:10:55,700 --> 00:11:02,150
stereotypical scenario and this is what

00:10:59,690 --> 00:11:03,680
we call breaking down the stereotype the

00:11:02,150 --> 00:11:06,470
idea here is to consciously recognize

00:11:03,680 --> 00:11:08,270
that I thought that you had came from a

00:11:06,470 --> 00:11:10,730
stereotype and instead of leaving that

00:11:08,270 --> 00:11:12,840
bias buried in your subconscious bring

00:11:10,730 --> 00:11:14,430
it to the surface and then replace it

00:11:12,840 --> 00:11:16,860
so in this case I can choose to

00:11:14,430 --> 00:11:18,870
recognize my biased reaction and think

00:11:16,860 --> 00:11:22,320
CEOs entrepreneurs

00:11:18,870 --> 00:11:24,180
leaders experienced okay so let's move

00:11:22,320 --> 00:11:26,490
on to our second strategy which is

00:11:24,180 --> 00:11:28,500
increasing opportunities for contact and

00:11:26,490 --> 00:11:31,290
it is probably one of the most important

00:11:28,500 --> 00:11:33,089
strategies that you'll find because it

00:11:31,290 --> 00:11:35,310
tends to help with all aspects of

00:11:33,089 --> 00:11:37,020
breaking down our biases it can make us

00:11:35,310 --> 00:11:39,000
confront our basic assumptions

00:11:37,020 --> 00:11:40,710
it helps us empathize with individuals

00:11:39,000 --> 00:11:43,320
who are different from us and it can

00:11:40,710 --> 00:11:45,779
give us non-stereotypical x' that we can

00:11:43,320 --> 00:11:49,410
continuously use to fight our own bias

00:11:45,779 --> 00:11:51,720
and there's many ways to do this even if

00:11:49,410 --> 00:11:53,370
you aren't a social butterfly so you can

00:11:51,720 --> 00:11:55,980
try going to a diverse conference like

00:11:53,370 --> 00:11:57,690
this one you could follow folks like the

00:11:55,980 --> 00:12:00,330
other amazing speakers that you've heard

00:11:57,690 --> 00:12:01,980
on Twitter you can contribute to an open

00:12:00,330 --> 00:12:03,900
source project with diverse maintained

00:12:01,980 --> 00:12:06,120
errs or you could mentor someone new or

00:12:03,900 --> 00:12:07,830
find a mentor for yourself whatever

00:12:06,120 --> 00:12:09,839
feels like a good step for you that's

00:12:07,830 --> 00:12:12,270
enough and I'm not going to lie

00:12:09,839 --> 00:12:14,760
increasing opportunities for contact can

00:12:12,270 --> 00:12:16,500
be scary sometimes too so if you

00:12:14,760 --> 00:12:18,750
struggle with this like I tend to do

00:12:16,500 --> 00:12:21,360
it's always a good move to listen and be

00:12:18,750 --> 00:12:23,220
genuinely curious Twitter and blogs can

00:12:21,360 --> 00:12:24,960
be an awesome start because you can

00:12:23,220 --> 00:12:26,460
follow along and get a sense of what

00:12:24,960 --> 00:12:29,400
someone might or might not want to talk

00:12:26,460 --> 00:12:31,500
about it also helps to be open to

00:12:29,400 --> 00:12:33,810
hearing things you didn't expect reading

00:12:31,500 --> 00:12:36,089
body language cues and believing what

00:12:33,810 --> 00:12:38,490
people say about their experiences even

00:12:36,089 --> 00:12:40,080
if it seems contradictory to what -

00:12:38,490 --> 00:12:42,720
something that you thought you knew and

00:12:40,080 --> 00:12:44,700
if someone gives you some hard feedback

00:12:42,720 --> 00:12:46,920
or it just doesn't have time to talk

00:12:44,700 --> 00:12:49,830
try and stay humble other people are the

00:12:46,920 --> 00:12:50,550
main character in their own story so as

00:12:49,830 --> 00:12:52,589
a group

00:12:50,550 --> 00:12:54,000
let's send an intention to use the rest

00:12:52,589 --> 00:12:56,100
of this conference to meet new people

00:12:54,000 --> 00:12:57,720
and help increase our opportunities for

00:12:56,100 --> 00:12:59,880
contact with groups that are

00:12:57,720 --> 00:13:01,800
underrepresented in our workplaces and

00:12:59,880 --> 00:13:04,350
let's try and maintain those connections

00:13:01,800 --> 00:13:07,709
after this conference even if it is just

00:13:04,350 --> 00:13:09,720
following a new person on Twitter so to

00:13:07,709 --> 00:13:11,610
wrap up this section unconsciously

00:13:09,720 --> 00:13:15,060
biased thoughts are a habit and like any

00:13:11,610 --> 00:13:16,860
habit they can be broken mostly you can

00:13:15,060 --> 00:13:18,900
never be quite cured of unconscious bias

00:13:16,860 --> 00:13:21,420
but over time the strategies to reduce

00:13:18,900 --> 00:13:23,100
bias become habits themselves and it

00:13:21,420 --> 00:13:23,870
will feel easier to quash unconscious

00:13:23,100 --> 00:13:26,480
bias when it

00:13:23,870 --> 00:13:28,220
sup again so let's shift away from our

00:13:26,480 --> 00:13:30,170
internal struggles and dive into how we

00:13:28,220 --> 00:13:31,790
can respond to bias in our environments

00:13:30,170 --> 00:13:34,760
we've been working with an amazing

00:13:31,790 --> 00:13:36,529
cartoonist - RIA Theresa Oh born to

00:13:34,760 --> 00:13:38,870
illustrate real scenarios that come up

00:13:36,529 --> 00:13:40,400
and how we can react when they do so we

00:13:38,870 --> 00:13:42,350
have cartoon showing bias against white

00:13:40,400 --> 00:13:44,120
women people of color bias against

00:13:42,350 --> 00:13:46,640
people with disabilities against queer

00:13:44,120 --> 00:13:48,380
folks and so on but for this talk I

00:13:46,640 --> 00:13:51,140
picked just a couple of cartoons that I

00:13:48,380 --> 00:13:53,450
hope are less emotionally charged than

00:13:51,140 --> 00:13:56,270
others but still useful for practicing

00:13:53,450 --> 00:13:58,850
these techniques so who's heard this one

00:13:56,270 --> 00:14:02,930
before she's really smart but I wish she

00:13:58,850 --> 00:14:04,880
wasn't so bossy so I have and I may have

00:14:02,930 --> 00:14:06,980
thought it one or two times to remember

00:14:04,880 --> 00:14:10,130
that we are all biased and we can even

00:14:06,980 --> 00:14:12,320
be biased against our own group so let's

00:14:10,130 --> 00:14:14,390
identify this bias this cartoon

00:14:12,320 --> 00:14:16,190
showcases a common bias against women in

00:14:14,390 --> 00:14:18,250
leadership roles so what are your

00:14:16,190 --> 00:14:20,690
options when someone says this to you

00:14:18,250 --> 00:14:23,360
instead of labeling the person who made

00:14:20,690 --> 00:14:24,800
the comment as racist or sexist inciting

00:14:23,360 --> 00:14:26,839
a reflection on the comment through four

00:14:24,800 --> 00:14:28,940
simple strategies will help to highlight

00:14:26,839 --> 00:14:31,430
the problematic nature of it and reflect

00:14:28,940 --> 00:14:34,339
and encourage a change in behavior

00:14:31,430 --> 00:14:36,140
rather than an argument so our first

00:14:34,339 --> 00:14:38,510
strategy is just to observe the problem

00:14:36,140 --> 00:14:39,380
instead of saying staying silent or

00:14:38,510 --> 00:14:40,640
moving on

00:14:39,380 --> 00:14:42,770
observe that the comment was

00:14:40,640 --> 00:14:46,190
stereotypical and hurtful something like

00:14:42,770 --> 00:14:47,779
ouch that's a bit harsh or you could

00:14:46,190 --> 00:14:50,540
counter a microaggression with a micro

00:14:47,779 --> 00:14:52,790
affection she's not bossy she's a leader

00:14:50,540 --> 00:14:54,529
a positive comment like this can help

00:14:52,790 --> 00:14:56,510
weaken the negative stereotype for those

00:14:54,529 --> 00:14:58,070
around you and without being too

00:14:56,510 --> 00:15:00,800
confrontational or derail in the

00:14:58,070 --> 00:15:03,110
conversation our bystander could also

00:15:00,800 --> 00:15:04,490
reflect the comment back which would

00:15:03,110 --> 00:15:05,839
make the speaker do the work of

00:15:04,490 --> 00:15:08,150
challenging their own assumptions

00:15:05,839 --> 00:15:11,330
something like how come you never called

00:15:08,150 --> 00:15:13,700
guys bossy and finally he could simply

00:15:11,330 --> 00:15:15,860
label the stereotype that's in play hey

00:15:13,700 --> 00:15:18,500
I think that's reinforcing a hurtful

00:15:15,860 --> 00:15:20,510
gender stereotype or as a friend said to

00:15:18,500 --> 00:15:23,500
me when I was practice talk a few days

00:15:20,510 --> 00:15:26,510
ago bias much bro

00:15:23,500 --> 00:15:28,130
okay so let's practice in this cartoon a

00:15:26,510 --> 00:15:30,080
manager is just finishing up a meeting

00:15:28,130 --> 00:15:32,360
and asking one of his newer team members

00:15:30,080 --> 00:15:35,150
does your wife take care care of the

00:15:32,360 --> 00:15:36,920
kids or do you do daycare sorry let me

00:15:35,150 --> 00:15:37,410
repeat that does your wife take care of

00:15:36,920 --> 00:15:40,350
the kids

00:15:37,410 --> 00:15:42,779
or do you do daycare as an opportunity

00:15:40,350 --> 00:15:44,430
to meet people right now I would like

00:15:42,779 --> 00:15:47,129
you to form groups of two or three make

00:15:44,430 --> 00:15:49,470
sure no one is left out and identify the

00:15:47,129 --> 00:15:51,600
stereotype or types in this cartoon and

00:15:49,470 --> 00:15:53,910
then brainstorm a few responses of your

00:15:51,600 --> 00:15:55,230
own I'll give you a little bit more than

00:15:53,910 --> 00:15:58,800
a minute to do that and I'm actually

00:15:55,230 --> 00:16:00,930
going to ask visnu to come up and play

00:15:58,800 --> 00:16:03,810
some lilting ukulele tunes to keep you

00:16:00,930 --> 00:16:06,870
call while you are brainstorming and

00:16:03,810 --> 00:16:09,209
talking so when you hear his music stop

00:16:06,870 --> 00:16:10,720
please turn your attention back up on

00:16:09,209 --> 00:16:14,249
stage to me

00:16:10,720 --> 00:16:14,249
[Music]

00:16:20,680 --> 00:16:51,470
[Music]

00:16:54,100 --> 00:18:07,460
[Music]

00:18:12,070 --> 00:18:18,980
great wow that was a lot of talking and

00:18:16,700 --> 00:18:23,510
I'm so glad that all of you have so much

00:18:18,980 --> 00:18:26,690
to say about this so let's review what

00:18:23,510 --> 00:18:30,590
we just went over and had great ideas

00:18:26,690 --> 00:18:32,390
about so there are actually two biases

00:18:30,590 --> 00:18:34,250
in this situation and I hope some of you

00:18:32,390 --> 00:18:35,780
picked up on that so the first is that

00:18:34,250 --> 00:18:38,090
dads don't play an active role in

00:18:35,780 --> 00:18:41,470
parenting and the second is that dads

00:18:38,090 --> 00:18:44,470
must be in a heterosexual relationship

00:18:41,470 --> 00:18:44,470
yeah

00:18:46,280 --> 00:18:52,820
i I couldn't Oh monogamy

00:18:50,420 --> 00:18:55,700
yeah you know could be in a polyamorous

00:18:52,820 --> 00:18:57,680
relationship too okay so let's review

00:18:55,700 --> 00:19:00,350
some of the possible responses that our

00:18:57,680 --> 00:19:01,940
dad could use in this situation the team

00:19:00,350 --> 00:19:03,920
member could observe the problem by

00:19:01,940 --> 00:19:06,470
saying actually my husband and I share

00:19:03,920 --> 00:19:08,270
parenting responsibilities equally he

00:19:06,470 --> 00:19:09,680
could also use positive redirection I

00:19:08,270 --> 00:19:12,620
take pride in being the primary

00:19:09,680 --> 00:19:13,760
caretaker of my children he could also

00:19:12,620 --> 00:19:16,670
transfer the work

00:19:13,760 --> 00:19:18,980
why are those the only options and

00:19:16,670 --> 00:19:21,230
finally he could label the stereotype as

00:19:18,980 --> 00:19:23,330
a gay father who manages our family's

00:19:21,230 --> 00:19:25,460
childcare my situation is only unusual

00:19:23,330 --> 00:19:31,880
for those who hold unfair gender

00:19:25,460 --> 00:19:33,650
binaries so that one might be a little

00:19:31,880 --> 00:19:35,390
wordy but I'm sure you can come up with

00:19:33,650 --> 00:19:38,390
some that you would use in that

00:19:35,390 --> 00:19:40,430
situation too so we want all of you to

00:19:38,390 --> 00:19:42,560
feel represented by our cartoons so feel

00:19:40,430 --> 00:19:44,870
free to share your own real experiences

00:19:42,560 --> 00:19:46,550
of bias with us through our website and

00:19:44,870 --> 00:19:48,380
maybe they will be made into a cartoon

00:19:46,550 --> 00:19:52,010
to teach others to be more empathetic

00:19:48,380 --> 00:19:54,460
humans and to born the great is actually

00:19:52,010 --> 00:19:57,050
the cartoonists Instagram and Twitter

00:19:54,460 --> 00:20:00,410
handle so if you want to tweet at her

00:19:57,050 --> 00:20:02,060
feel free so in parallel with personal

00:20:00,410 --> 00:20:03,140
adjustments that you can make to your

00:20:02,060 --> 00:20:04,880
words and thoughts there's also

00:20:03,140 --> 00:20:06,740
important work that can be done at an

00:20:04,880 --> 00:20:09,050
organizational level through putting

00:20:06,740 --> 00:20:10,990
good policies in place now I bet some of

00:20:09,050 --> 00:20:13,990
you might be resisting this idea of it

00:20:10,990 --> 00:20:17,600
after all Tech is a meritocracy right

00:20:13,990 --> 00:20:19,490
well nope exactly the opposite the fact

00:20:17,600 --> 00:20:21,410
that the tech sector celebrates the idea

00:20:19,490 --> 00:20:23,930
of innate talent is actually evidence

00:20:21,410 --> 00:20:25,370
that tech is not a meritocracy this

00:20:23,930 --> 00:20:27,980
study shows that when you focus on just

00:20:25,370 --> 00:20:29,900
coding or engineering instead of

00:20:27,980 --> 00:20:31,880
including holistic values like teamwork

00:20:29,900 --> 00:20:34,250
transparency and consistency in

00:20:31,880 --> 00:20:36,200
performance reviews then manager level

00:20:34,250 --> 00:20:37,940
folks with unconscious bias are more

00:20:36,200 --> 00:20:40,340
likely to review their team members with

00:20:37,940 --> 00:20:43,700
that bias regardless of how awesome a

00:20:40,340 --> 00:20:46,130
person's code is so you can think of

00:20:43,700 --> 00:20:47,720
good policies as the bumpers in bowling

00:20:46,130 --> 00:20:49,220
without them people with more

00:20:47,720 --> 00:20:51,620
unconscious bias will throw more gutter

00:20:49,220 --> 00:20:53,870
balls by unintentionally acting with

00:20:51,620 --> 00:20:55,850
their biases on the other hand good

00:20:53,870 --> 00:20:57,710
policies can guide people's behavior in

00:20:55,850 --> 00:20:59,179
the direction that you want it to go if

00:20:57,710 --> 00:21:01,219
you consciously ask them

00:20:59,179 --> 00:21:04,460
towards an environment that includes and

00:21:01,219 --> 00:21:06,080
recognizes everyone's unique talent so

00:21:04,460 --> 00:21:08,149
many conversations about policies

00:21:06,080 --> 00:21:09,619
usually start with hiring how to get

00:21:08,149 --> 00:21:11,419
more diverse candidates into the

00:21:09,619 --> 00:21:13,219
recruiting pipeline how to get more

00:21:11,419 --> 00:21:15,320
through the interview process and how to

00:21:13,219 --> 00:21:18,169
have more have more of them accept

00:21:15,320 --> 00:21:20,029
offers and hiring really is an important

00:21:18,169 --> 00:21:22,369
place to start but it's only one small

00:21:20,029 --> 00:21:24,589
piece of the puzzle so once you have

00:21:22,369 --> 00:21:28,249
employees on staff there's a wide range

00:21:24,589 --> 00:21:29,929
of of ways you can provide inclusive

00:21:28,249 --> 00:21:32,049
support that meet the different needs

00:21:29,929 --> 00:21:34,940
and working styles of different people

00:21:32,049 --> 00:21:36,769
might have and when people on your team

00:21:34,940 --> 00:21:38,929
get into the habit of valuing each other

00:21:36,769 --> 00:21:40,549
as unique individuals that internal

00:21:38,929 --> 00:21:42,379
culture can actually translate into

00:21:40,549 --> 00:21:45,979
building more equitable less biased

00:21:42,379 --> 00:21:48,080
products for your users so we like to

00:21:45,979 --> 00:21:49,789
say that policies should come from

00:21:48,080 --> 00:21:51,499
overarching principles and one of the

00:21:49,789 --> 00:21:54,259
most important is aiming for inclusion

00:21:51,499 --> 00:21:55,940
and equity not just diversity we haven't

00:21:54,259 --> 00:21:57,649
really defined those terms yet so here's

00:21:55,940 --> 00:22:00,379
how I think about it I think if

00:21:57,649 --> 00:22:02,869
diversity is about being about headcount

00:22:00,379 --> 00:22:04,460
and representation as an example do you

00:22:02,869 --> 00:22:07,369
do user research with people with

00:22:04,460 --> 00:22:09,529
disabilities equity is about resources

00:22:07,369 --> 00:22:11,269
and opportunity do you compensate people

00:22:09,529 --> 00:22:12,799
with disabilities at a fair and just

00:22:11,269 --> 00:22:14,839
level considering their unique

00:22:12,799 --> 00:22:16,700
perspective their investment and giving

00:22:14,839 --> 00:22:18,229
you that feedback and the extra value

00:22:16,700 --> 00:22:20,479
that they're adding to your product an

00:22:18,229 --> 00:22:22,279
inclusion is the hardest to measure it's

00:22:20,479 --> 00:22:24,799
about how marginalized people feel about

00:22:22,279 --> 00:22:26,899
your culture do people with disabilities

00:22:24,799 --> 00:22:29,119
come away feeling like valued partners

00:22:26,899 --> 00:22:30,919
do you create an environment where they

00:22:29,119 --> 00:22:34,539
can provide honest feedback and you

00:22:30,919 --> 00:22:37,609
listen and act on their feedback so

00:22:34,539 --> 00:22:39,279
let's give you some examples to improve

00:22:37,609 --> 00:22:41,330
the inclusion and equity within your

00:22:39,279 --> 00:22:43,549
organization you can work at many levels

00:22:41,330 --> 00:22:46,759
so you could write more inclusive job

00:22:43,549 --> 00:22:48,289
descriptions a cool - cool tool here is

00:22:46,759 --> 00:22:49,759
text EO which I think one of our

00:22:48,289 --> 00:22:52,009
previous speakers actually works at

00:22:49,759 --> 00:22:53,479
right now which points out masculine and

00:22:52,009 --> 00:22:55,429
feminine word ease and helps you shift

00:22:53,479 --> 00:22:57,440
the balance in your job description so

00:22:55,429 --> 00:22:59,809
that everyone feels comfortable applying

00:22:57,440 --> 00:23:02,299
you could also expand benefits beyond

00:22:59,809 --> 00:23:04,249
just young single employees one example

00:23:02,299 --> 00:23:07,190
is allowing flexible hours for a child

00:23:04,249 --> 00:23:09,489
or elder care or you can make make

00:23:07,190 --> 00:23:11,629
remote work an option for everyone

00:23:09,489 --> 00:23:12,559
you should also account for power

00:23:11,629 --> 00:23:14,870
dynamics

00:23:12,559 --> 00:23:16,429
during conflict resolution this can

00:23:14,870 --> 00:23:18,409
relieve some pressure from marginalized

00:23:16,429 --> 00:23:21,409
people who may feel trapped or powerless

00:23:18,409 --> 00:23:23,539
if there's a dispute so focusing on

00:23:21,409 --> 00:23:25,190
diversity without also working on

00:23:23,539 --> 00:23:27,110
inclusion and equity can create a

00:23:25,190 --> 00:23:29,480
situation where you get diverse people

00:23:27,110 --> 00:23:30,679
in the door take your metrics up but

00:23:29,480 --> 00:23:32,269
they encounter high levels of

00:23:30,679 --> 00:23:34,580
unconscious bias on their team and

00:23:32,269 --> 00:23:36,440
they'll be quicker to leave if you make

00:23:34,580 --> 00:23:38,840
inclusion equity and bias reduction

00:23:36,440 --> 00:23:40,429
priorities to marginalized people may be

00:23:38,840 --> 00:23:43,370
more likely to stay on your team and

00:23:40,429 --> 00:23:46,429
recruit others to join and with that

00:23:43,370 --> 00:23:48,799
let's wrap up so yes unconscious bias is

00:23:46,429 --> 00:23:51,200
a real problem and yes ways to reduce

00:23:48,799 --> 00:23:52,820
unconscious bias are real - and no

00:23:51,200 --> 00:23:55,190
matter what role we play in different

00:23:52,820 --> 00:23:57,049
situations or different organizations we

00:23:55,190 --> 00:23:59,269
each have options for what to say in the

00:23:57,049 --> 00:24:01,309
moment when we see or experience bias

00:23:59,269 --> 00:24:03,350
and what policies we can advocate for

00:24:01,309 --> 00:24:06,080
that can guide everyone towards less

00:24:03,350 --> 00:24:08,450
bias and even though most of us here

00:24:06,080 --> 00:24:11,119
have unconscious bias those biases don't

00:24:08,450 --> 00:24:13,639
own us with dedication we can practice

00:24:11,119 --> 00:24:15,559
breaking that bias habit and becoming

00:24:13,639 --> 00:24:18,440
closer the people around us and closer

00:24:15,559 --> 00:24:20,240
to the people that we want to be so if

00:24:18,440 --> 00:24:22,429
you like more info please come talk to

00:24:20,240 --> 00:24:25,970
me or contact us online or go to our

00:24:22,429 --> 00:24:27,049
website you BP is always super excited

00:24:25,970 --> 00:24:29,029
to speak to new people and new

00:24:27,049 --> 00:24:31,009
organizations about what they can do to

00:24:29,029 --> 00:24:33,320
improve their culture and decrease the

00:24:31,009 --> 00:24:35,299
affective bias in their workplaces we

00:24:33,320 --> 00:24:37,249
are a volunteer driven nonprofit so if

00:24:35,299 --> 00:24:39,590
you'd like to get involved you can visit

00:24:37,249 --> 00:24:41,179
our website or chat with me again and

00:24:39,590 --> 00:24:43,009
you can also just jump right in by

00:24:41,179 --> 00:24:45,619
joining our weekly virtual meetings

00:24:43,009 --> 00:24:47,480
using the join ubp link so thank you so

00:24:45,619 --> 00:24:49,070
much and let's work together to weed out

00:24:47,480 --> 00:24:51,490
unconscious bias in all of our

00:24:49,070 --> 00:24:51,490
workplaces

00:24:53,770 --> 00:24:58,280
you

00:24:55,050 --> 00:24:58,280

YouTube URL: https://www.youtube.com/watch?v=VGcqkF2XDgA


