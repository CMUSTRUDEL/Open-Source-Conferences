Title: HI and AI - Kyle Oba | JSConf Hawaii 2019
Publication date: 2019-06-16
Playlist: JSConf HI 2019
Description: 
	Kyle Oba discusses artificial intelligence and privacy in the digital age.

JSConf Hawaii is returning in 2020. Learn more at https://www.jsconfhi.com/
Captions: 
	00:00:05,779 --> 00:00:10,910
good morning hello my name is Kyle Oba

00:00:08,920 --> 00:00:12,980
thank you to all the organizers and the

00:00:10,910 --> 00:00:14,359
volunteers for you know this has been an

00:00:12,980 --> 00:00:16,070
amazing experience so far I'm very

00:00:14,359 --> 00:00:19,490
excited to be here so you can probably

00:00:16,070 --> 00:00:21,500
tell I'm a little bit nervous so I know

00:00:19,490 --> 00:00:23,089
I'm just gonna dive right in alright so

00:00:21,500 --> 00:00:26,089
I work at a company called patio Cola we

00:00:23,089 --> 00:00:27,529
work on local local problems with local

00:00:26,089 --> 00:00:29,329
clients when you try to find local

00:00:27,529 --> 00:00:30,800
solutions as much as possible or a

00:00:29,329 --> 00:00:32,960
research and design and development

00:00:30,800 --> 00:00:35,690
company My partner and I started this

00:00:32,960 --> 00:00:39,710
company before moving to Hawaii but

00:00:35,690 --> 00:00:41,480
she's actually from Hawaii so that's

00:00:39,710 --> 00:00:43,700
where this whole like local little

00:00:41,480 --> 00:00:45,710
centered design and research comes from

00:00:43,700 --> 00:00:48,020
we partner with other researchers and

00:00:45,710 --> 00:00:49,580
designers and programmers but enough

00:00:48,020 --> 00:00:55,450
about me

00:00:49,580 --> 00:01:00,220
I'm sorry you can't see my slides Oh No

00:00:55,450 --> 00:01:00,220
should I just try to restart that okay

00:01:06,420 --> 00:01:18,450
okay cool thank you thank you that's my

00:01:14,370 --> 00:01:19,080
presentation keynote is the thing it

00:01:18,450 --> 00:01:22,710
used to work

00:01:19,080 --> 00:01:25,619
okay so cool alright so diving right in

00:01:22,710 --> 00:01:26,820
so the unprecedented so Shoshana Zubov

00:01:25,619 --> 00:01:28,740
just recently published a book called

00:01:26,820 --> 00:01:30,119
the age of surveillance capitalism I'm

00:01:28,740 --> 00:01:31,590
not gonna pretend to have read the whole

00:01:30,119 --> 00:01:35,790
thing because it's like over 700 pages

00:01:31,590 --> 00:01:37,409
and I'm a busy person but yeah so

00:01:35,790 --> 00:01:38,970
there's this idea they unprecedented and

00:01:37,409 --> 00:01:41,820
in her own words the unprecedent is

00:01:38,970 --> 00:01:42,690
necessarily unrecognizable right so when

00:01:41,820 --> 00:01:44,580
we encounter something that's

00:01:42,690 --> 00:01:45,630
unprecedented we automatically interpret

00:01:44,580 --> 00:01:47,940
it through the lenses that we are

00:01:45,630 --> 00:01:50,280
familiar with or the familiar categories

00:01:47,940 --> 00:01:52,080
thereby rendering invisible precisely

00:01:50,280 --> 00:01:53,550
which is unprecedented when I read this

00:01:52,080 --> 00:01:55,830
I was thinking wow this is amazing

00:01:53,550 --> 00:01:58,770
I admittedly was going to Center this

00:01:55,830 --> 00:02:01,950
talk all around context but I really

00:01:58,770 --> 00:02:05,640
like her idea here and so if you were

00:02:01,950 --> 00:02:08,880
living or visiting Hawaii about a year

00:02:05,640 --> 00:02:12,150
ago exactly almost you would have

00:02:08,880 --> 00:02:13,500
received this on your phone it's all

00:02:12,150 --> 00:02:15,840
caps so you know it's important

00:02:13,500 --> 00:02:17,730
ballistic missile threat in bounds of

00:02:15,840 --> 00:02:19,920
Hawaii seek immediate shelter and then

00:02:17,730 --> 00:02:22,829
if you were you're like is that real it

00:02:19,920 --> 00:02:24,570
says this is not a drill which you know

00:02:22,829 --> 00:02:26,070
usually I'm people are drilling me at

00:02:24,570 --> 00:02:29,370
8:00 in the morning on Saturday but

00:02:26,070 --> 00:02:31,769
whatever yeah so from my own experience

00:02:29,370 --> 00:02:34,380
you know I I saw this you know it was

00:02:31,769 --> 00:02:36,329
Saturday morning I just kind of calmly

00:02:34,380 --> 00:02:38,850
like doing stuff in the kitchen look at

00:02:36,329 --> 00:02:44,250
my phone and I guess I'm gonna die right

00:02:38,850 --> 00:02:45,510
and you know usually when I see you know

00:02:44,250 --> 00:02:48,540
we're all gonna die but whatever you

00:02:45,510 --> 00:02:49,769
know but when I see this type of message

00:02:48,540 --> 00:02:51,570
on my phone and maybe some of you got

00:02:49,769 --> 00:02:54,360
this last night but it's like there's a

00:02:51,570 --> 00:02:57,600
flood coming maybe you know there's a

00:02:54,360 --> 00:02:59,670
hurricane on its way maybe maybe you

00:02:57,600 --> 00:03:02,040
should care maybe right and so that's

00:02:59,670 --> 00:03:04,829
kind of the lens that I used to

00:03:02,040 --> 00:03:06,959
interpret this message which is kind of

00:03:04,829 --> 00:03:08,940
like what Zubov is saying it was totally

00:03:06,959 --> 00:03:11,160
the wrong lens to use it was just a lens

00:03:08,940 --> 00:03:13,980
I was familiar to me so I immediately

00:03:11,160 --> 00:03:16,739
started filling my water filter in the

00:03:13,980 --> 00:03:18,780
kitchen with water which seems

00:03:16,739 --> 00:03:19,770
reasonable but like when you but then

00:03:18,780 --> 00:03:21,990
like you know 38

00:03:19,770 --> 00:03:23,610
later when they say just kidding just

00:03:21,990 --> 00:03:25,980
just you didn't that's not real I really

00:03:23,610 --> 00:03:28,860
like had time to evaluate like my own

00:03:25,980 --> 00:03:31,560
behavior and and it really didn't make

00:03:28,860 --> 00:03:35,220
any sense I mean like if we got hit by a

00:03:31,560 --> 00:03:38,640
missile as pouring water I mean I don't

00:03:35,220 --> 00:03:40,830
you know didn't really matter okay cool

00:03:38,640 --> 00:03:43,620
so that's the idea of the unprecedent

00:03:40,830 --> 00:03:46,380
how we require these lenses in order to

00:03:43,620 --> 00:03:47,520
interpret new things and so obviously

00:03:46,380 --> 00:03:49,500
this talk it's a little bit about

00:03:47,520 --> 00:03:51,870
artificial intelligence which is kind of

00:03:49,500 --> 00:03:54,450
this weird broad term so like houses

00:03:51,870 --> 00:03:56,610
houses relevant to us today

00:03:54,450 --> 00:03:59,790
so as builders and designers as a lot of

00:03:56,610 --> 00:04:03,210
us are we need to create that new lens

00:03:59,790 --> 00:04:05,220
for ourselves in order to interpret that

00:04:03,210 --> 00:04:08,760
which is unprecedented to us and to the

00:04:05,220 --> 00:04:10,110
community or into society yeah one of

00:04:08,760 --> 00:04:11,400
the common themes to a lot of the work

00:04:10,110 --> 00:04:14,070
that we do is that we'd like to try to

00:04:11,400 --> 00:04:16,200
find the unrelatable things that are

00:04:14,070 --> 00:04:17,760
invisible things that are undefined and

00:04:16,200 --> 00:04:21,299
try to make them more relatable visible

00:04:17,760 --> 00:04:22,260
and articulate them more fully and it's

00:04:21,299 --> 00:04:23,880
only through the creation of these new

00:04:22,260 --> 00:04:26,820
lenses that we can bend further these

00:04:23,880 --> 00:04:29,520
discussions with our communities so as

00:04:26,820 --> 00:04:32,220
an example of that I wanted to take you

00:04:29,520 --> 00:04:34,260
all through a project that we did with

00:04:32,220 --> 00:04:36,660
the Honolulu Museum of Art which is just

00:04:34,260 --> 00:04:39,270
down the street and I highly recommend

00:04:36,660 --> 00:04:42,060
it so about the time that that alert

00:04:39,270 --> 00:04:43,770
came out we had the opportunity to do

00:04:42,060 --> 00:04:45,810
this project with the museum and we

00:04:43,770 --> 00:04:47,640
called it a design intervention and we

00:04:45,810 --> 00:04:50,700
wanted to sort of forward the discussion

00:04:47,640 --> 00:04:52,320
of surveillance technologies so at the

00:04:50,700 --> 00:04:54,870
time they had this program called

00:04:52,320 --> 00:04:56,130
classified that was produced by the

00:04:54,870 --> 00:04:58,850
Doris Duke theatre which is in the

00:04:56,130 --> 00:05:01,050
museum which was to highlight screening

00:04:58,850 --> 00:05:02,580
well highlights of the of the program

00:05:01,050 --> 00:05:05,669
were screening of films including lower

00:05:02,580 --> 00:05:07,410
Patras citizen for documentary about

00:05:05,669 --> 00:05:10,050
Edward Snowden and there was also a

00:05:07,410 --> 00:05:11,580
panel discussion so the panel discussion

00:05:10,050 --> 00:05:13,830
took place on you know just one

00:05:11,580 --> 00:05:15,900
particular day involved Kate Crawford

00:05:13,830 --> 00:05:17,580
one of the cofounders of the AI now

00:05:15,900 --> 00:05:18,840
Institute revver Paglen and artist who

00:05:17,580 --> 00:05:21,180
works with surveillance technologies

00:05:18,840 --> 00:05:24,090
Laura Patris the director Edward Snowden

00:05:21,180 --> 00:05:26,910
actually Skyped in and Ben Weisner who's

00:05:24,090 --> 00:05:28,680
his ACLU attorney and it was moderated

00:05:26,910 --> 00:05:30,300
by asan ilahe another artist who works

00:05:28,680 --> 00:05:33,090
with surveillance technologies or

00:05:30,300 --> 00:05:33,960
surveillance in general and so we were

00:05:33,090 --> 00:05:36,479
invited to sort of

00:05:33,960 --> 00:05:39,000
come and create an experience and so we

00:05:36,479 --> 00:05:40,530
dubbed it a design intervention to

00:05:39,000 --> 00:05:43,650
complement what was happening in the

00:05:40,530 --> 00:05:46,979
panel so so they the museum called it to

00:05:43,650 --> 00:05:48,270
my profile tour I'm not sure how I feel

00:05:46,979 --> 00:05:49,650
about that name but you know that it's

00:05:48,270 --> 00:05:51,509
stuck so that's what it is it's a my

00:05:49,650 --> 00:05:53,910
poster my profile took my profile choice

00:05:51,509 --> 00:05:57,350
it's a little it's a little 2004 a name

00:05:53,910 --> 00:06:01,050
I think a little bit but it's cool and

00:05:57,350 --> 00:06:03,030
so the purpose of our of our design

00:06:01,050 --> 00:06:05,820
intervention was to sort of raise raise

00:06:03,030 --> 00:06:07,440
awareness about things in Google that

00:06:05,820 --> 00:06:09,750
can go wrong so the fal ability of face

00:06:07,440 --> 00:06:13,199
detection face recognition technology

00:06:09,750 --> 00:06:16,080
and problematic data privacy issues so

00:06:13,199 --> 00:06:18,030
this is like about a year ago so we

00:06:16,080 --> 00:06:19,020
weren't well I'll just leave it at that

00:06:18,030 --> 00:06:20,190
so we were totally sure what was gonna

00:06:19,020 --> 00:06:22,590
happen with those issues but we knew

00:06:20,190 --> 00:06:23,970
they were problematic anyway so this was

00:06:22,590 --> 00:06:26,789
this was mentioned reduce public to

00:06:23,970 --> 00:06:29,070
specific technologies and the use cases

00:06:26,789 --> 00:06:32,460
of machine learning learning in the same

00:06:29,070 --> 00:06:34,650
giant amorphous AI thing and to replace

00:06:32,460 --> 00:06:36,150
sort of those marketing buzzwords with

00:06:34,650 --> 00:06:38,430
some a little bit more articulate

00:06:36,150 --> 00:06:41,580
discussion and specific discussion about

00:06:38,430 --> 00:06:44,010
these issues all right so what did we

00:06:41,580 --> 00:06:44,789
build so we built individual offices I

00:06:44,010 --> 00:06:46,530
don't know

00:06:44,789 --> 00:06:49,380
elevator pitch that we built into the

00:06:46,530 --> 00:06:52,680
individual eyes tours based on guests

00:06:49,380 --> 00:06:54,930
faces match to our currently on display

00:06:52,680 --> 00:06:55,470
in a museum okay and I know a lot of you

00:06:54,930 --> 00:06:58,409
are thinking

00:06:55,470 --> 00:07:01,080
isn't that what Google did in their app

00:06:58,409 --> 00:07:03,360
for Google arts and culture and now I'm

00:07:01,080 --> 00:07:05,159
not lying when I tell you this and you

00:07:03,360 --> 00:07:08,729
know that that's what Liars say but

00:07:05,159 --> 00:07:10,470
anyway we they really least there at

00:07:08,729 --> 00:07:12,120
like two weeks before we went live and

00:07:10,470 --> 00:07:13,500
it was just sort of uncanny I actually

00:07:12,120 --> 00:07:16,380
didn't find out about their app until

00:07:13,500 --> 00:07:18,690
after we after the panel and I was like

00:07:16,380 --> 00:07:21,180
oh wow that that's disappointing but at

00:07:18,690 --> 00:07:22,320
the same time it was really interesting

00:07:21,180 --> 00:07:25,500
because they were doing something very

00:07:22,320 --> 00:07:27,620
very similar I can I'm so I'm gonna kind

00:07:25,500 --> 00:07:29,849
of walk you through how we did it and

00:07:27,620 --> 00:07:32,070
some of the differences but I think it

00:07:29,849 --> 00:07:33,330
was an uncanny coincidence and I think a

00:07:32,070 --> 00:07:35,250
lot of it had to do with how good

00:07:33,330 --> 00:07:36,630
certain types of Technology were like

00:07:35,250 --> 00:07:39,000
face recognition and detection we're

00:07:36,630 --> 00:07:44,159
getting at about that time like just and

00:07:39,000 --> 00:07:45,510
how easy it was to apply so all right so

00:07:44,159 --> 00:07:47,630
cool to my probe out here how did it

00:07:45,510 --> 00:07:50,030
work so step one you had to opt in

00:07:47,630 --> 00:07:51,890
not everybody had to do this as you can

00:07:50,030 --> 00:07:53,660
imagine at a panel discussion where

00:07:51,890 --> 00:07:55,130
Edward Snowden is skyping in in the

00:07:53,660 --> 00:07:57,470
state of Hawaii where he used to work

00:07:55,130 --> 00:07:59,540
it's a little bit weird to have somebody

00:07:57,470 --> 00:08:01,340
ask you to take a picture of you before

00:07:59,540 --> 00:08:04,130
you go into the panel so you had to opt

00:08:01,340 --> 00:08:06,500
in and then if you did opt in we took a

00:08:04,130 --> 00:08:07,940
picture of you and it said hey this is

00:08:06,500 --> 00:08:12,320
the picture we're taking would you like

00:08:07,940 --> 00:08:13,490
to opt in if you did we kind of labeled

00:08:12,320 --> 00:08:15,980
you in the wheel world we gave you a

00:08:13,490 --> 00:08:18,110
sticker and a number on there it's sort

00:08:15,980 --> 00:08:22,610
of like sort of a symbolic like your

00:08:18,110 --> 00:08:24,740
this is happening to you yeah I mean you

00:08:22,610 --> 00:08:26,270
know let's be fair not every website

00:08:24,740 --> 00:08:28,010
does this okay well they don't give you

00:08:26,270 --> 00:08:31,100
a sticker well actually the stickers are

00:08:28,010 --> 00:08:32,570
a thing but anyway okay oh yeah

00:08:31,100 --> 00:08:35,479
incidentally ask me later I asked you

00:08:32,570 --> 00:08:38,090
should give up all right cool so the

00:08:35,479 --> 00:08:40,490
four-step customized tour so the panel

00:08:38,090 --> 00:08:42,289
was an hour long so before you walked in

00:08:40,490 --> 00:08:44,510
we took your picture when you walked out

00:08:42,289 --> 00:08:45,740
we handed you a customized tour so there

00:08:44,510 --> 00:08:47,510
was a lot that went into sort of

00:08:45,740 --> 00:08:49,670
printing and formatting and laying these

00:08:47,510 --> 00:08:51,350
things out ahead of time so that only

00:08:49,670 --> 00:08:55,760
the live customized content would be

00:08:51,350 --> 00:08:56,990
applied cool so the process so I know

00:08:55,760 --> 00:08:58,970
this is a little small for you to see

00:08:56,990 --> 00:09:01,700
I'm sorry about that this is sort of

00:08:58,970 --> 00:09:03,920
like a version from the version of what

00:09:01,700 --> 00:09:06,260
we put out in a handout and so basically

00:09:03,920 --> 00:09:08,120
the top part of it is stuff that

00:09:06,260 --> 00:09:11,330
happened ahead of time and the bottom

00:09:08,120 --> 00:09:15,010
part is stuff that happened on site that

00:09:11,330 --> 00:09:17,900
day and I'll walk you through that so

00:09:15,010 --> 00:09:19,610
step one of I think almost every one of

00:09:17,900 --> 00:09:21,140
these machine learning slash you know

00:09:19,610 --> 00:09:22,970
whatever you want to call it artificial

00:09:21,140 --> 00:09:24,860
intelligence projects is data collection

00:09:22,970 --> 00:09:26,600
right or getting data from somewhere and

00:09:24,860 --> 00:09:28,160
so as you can imagine when you're

00:09:26,600 --> 00:09:29,960
working with a museum who hasn't really

00:09:28,160 --> 00:09:31,760
done this type of thing before data

00:09:29,960 --> 00:09:34,370
collection is hard data collection is

00:09:31,760 --> 00:09:37,310
hard I in my experience no matter what

00:09:34,370 --> 00:09:38,990
project I'm working on so we had to walk

00:09:37,310 --> 00:09:41,060
through the entire museum find out where

00:09:38,990 --> 00:09:42,560
all the art was write down all the

00:09:41,060 --> 00:09:44,450
numbers associated with that art and

00:09:42,560 --> 00:09:46,490
figure out what they were called what

00:09:44,450 --> 00:09:49,370
room they were in and then get a photo

00:09:46,490 --> 00:09:53,030
of everything that was that was a lot of

00:09:49,370 --> 00:09:54,380
work and then we sort of processed the

00:09:53,030 --> 00:09:56,330
images of the art as well as the

00:09:54,380 --> 00:09:58,010
sculpture so anything that they had that

00:09:56,330 --> 00:09:59,570
was going to be on this display at that

00:09:58,010 --> 00:10:00,990
time that wasn't in sort of like a

00:09:59,570 --> 00:10:03,930
special exhibition where

00:10:00,990 --> 00:10:07,380
allowed to photograph the art we pass it

00:10:03,930 --> 00:10:09,540
through a face detection neural network

00:10:07,380 --> 00:10:10,890
which is essentially just says hey I

00:10:09,540 --> 00:10:13,020
think there's a face here and I think

00:10:10,890 --> 00:10:16,680
it's inside of this rectangle and then

00:10:13,020 --> 00:10:18,060
you can highlight that or crop it out so

00:10:16,680 --> 00:10:19,800
in this case we just cropped out the

00:10:18,060 --> 00:10:22,709
faces and then we used a second neural

00:10:19,800 --> 00:10:24,089
network which was a landmark prediction

00:10:22,709 --> 00:10:25,740
neural network which essentially says

00:10:24,089 --> 00:10:27,330
hey we're I think the eyebrows are

00:10:25,740 --> 00:10:29,310
probably you know based on you telling

00:10:27,330 --> 00:10:31,410
me this is a face I think this is where

00:10:29,310 --> 00:10:35,160
the eyebrows are the eyes the nose the

00:10:31,410 --> 00:10:37,620
mouth the jaw etc and the image on the

00:10:35,160 --> 00:10:39,690
bottom there which is me I just wanted

00:10:37,620 --> 00:10:41,700
to throw in here that although we did a

00:10:39,690 --> 00:10:45,750
lot of this in Python because the

00:10:41,700 --> 00:10:48,060
beginning of every web project using

00:10:45,750 --> 00:10:51,779
machine learning is to install Python on

00:10:48,060 --> 00:10:53,130
your machine this is something that you

00:10:51,779 --> 00:10:54,930
can actually do in the web so I wanted

00:10:53,130 --> 00:10:59,010
to put this link here this is an example

00:10:54,930 --> 00:11:01,290
that runs in ml 5j s and is rendered

00:10:59,010 --> 00:11:03,810
with p5.js or you know sort of like put

00:11:01,290 --> 00:11:05,579
into the browser with p5.js it's super

00:11:03,810 --> 00:11:07,589
cool so if you're like if you if you

00:11:05,579 --> 00:11:08,670
love javascript you can do all this

00:11:07,589 --> 00:11:10,740
stuff from the comfort of your

00:11:08,670 --> 00:11:12,870
JavaScript but just remember you have to

00:11:10,740 --> 00:11:14,940
install Python because that's just

00:11:12,870 --> 00:11:17,430
required but don't be afraid of Python

00:11:14,940 --> 00:11:18,329
it's just executable pseudo code all

00:11:17,430 --> 00:11:22,860
right

00:11:18,329 --> 00:11:24,959
next let's see that's a joke for the

00:11:22,860 --> 00:11:27,930
Python people all right so this prepares

00:11:24,959 --> 00:11:30,839
us for numerical comfort comparison so

00:11:27,930 --> 00:11:32,089
and we can actually pass that set of

00:11:30,839 --> 00:11:34,860
landmarks through a face recognition

00:11:32,089 --> 00:11:36,360
neural network weather then essentially

00:11:34,860 --> 00:11:38,430
render that image into a bunch of

00:11:36,360 --> 00:11:40,529
numbers right like big vector all right

00:11:38,430 --> 00:11:42,420
and so now that we have sort of a face

00:11:40,529 --> 00:11:44,220
and we know where the landmarks are

00:11:42,420 --> 00:11:46,470
we've turned it into a bunch of numbers

00:11:44,220 --> 00:11:47,910
we can start matching it to things okay

00:11:46,470 --> 00:11:49,740
so like on the so beast a ball that

00:11:47,910 --> 00:11:50,279
stuff away and then on the day of the

00:11:49,740 --> 00:11:52,350
event

00:11:50,279 --> 00:11:55,890
people come in we take their picture

00:11:52,350 --> 00:11:59,910
right and then we find the landmarks on

00:11:55,890 --> 00:12:02,190
their face and then we can then render

00:11:59,910 --> 00:12:06,870
them into numbers and then match them to

00:12:02,190 --> 00:12:08,910
the art that they're similar to cool and

00:12:06,870 --> 00:12:11,339
then since we have facial scary facial

00:12:08,910 --> 00:12:12,720
recognition software we can then say hey

00:12:11,339 --> 00:12:14,430
do you want your tor

00:12:12,720 --> 00:12:16,110
oh we know who you are so here's

00:12:14,430 --> 00:12:19,440
- right so that was another thing we

00:12:16,110 --> 00:12:21,540
added so it wasn't just about matching

00:12:19,440 --> 00:12:25,260
faces to art we also did a number of

00:12:21,540 --> 00:12:26,760
other things so this is the tour folded

00:12:25,260 --> 00:12:28,470
up that we handed to them so it has

00:12:26,760 --> 00:12:30,589
their face it has their landmarks and it

00:12:28,470 --> 00:12:32,730
has a number of super problematic

00:12:30,589 --> 00:12:35,450
classifications that we said we're

00:12:32,730 --> 00:12:38,339
attributes of you based on your face and

00:12:35,450 --> 00:12:39,720
this was not meant to be like true it

00:12:38,339 --> 00:12:41,370
was meant to be like super problematic

00:12:39,720 --> 00:12:43,500
and something that people should

00:12:41,370 --> 00:12:47,250
question so special thanks to Kyle

00:12:43,500 --> 00:12:48,810
McDonald who is an artist who trained

00:12:47,250 --> 00:12:50,910
the neural network to work with these

00:12:48,810 --> 00:12:53,760
classifications on the labeled faces of

00:12:50,910 --> 00:12:55,380
the wild data set it's another one of

00:12:53,760 --> 00:12:59,310
things where you you can pop that thing

00:12:55,380 --> 00:13:01,380
into anyway I won't go into it so these

00:12:59,310 --> 00:13:03,149
labels were we're super problematic and

00:13:01,380 --> 00:13:04,470
I think in some cases kind of insulting

00:13:03,149 --> 00:13:07,140
it might tell you you're not attractive

00:13:04,470 --> 00:13:09,029
right and some it might tell you you

00:13:07,140 --> 00:13:11,220
look tired you know stuff like that and

00:13:09,029 --> 00:13:12,800
you know that's not something you want

00:13:11,220 --> 00:13:16,820
to hear from computer or anybody but

00:13:12,800 --> 00:13:19,560
nonetheless computers will do that and

00:13:16,820 --> 00:13:21,480
um so the next page we actually this is

00:13:19,560 --> 00:13:24,360
the stuff that was besides the labels

00:13:21,480 --> 00:13:25,890
was custom generated so here we were

00:13:24,360 --> 00:13:27,390
able to say okay we think you look like

00:13:25,890 --> 00:13:29,160
this painting we think you look like

00:13:27,390 --> 00:13:30,930
this sculpture and then based on

00:13:29,160 --> 00:13:32,640
everything that was in the frame when

00:13:30,930 --> 00:13:34,800
you took your photo we match it we're

00:13:32,640 --> 00:13:37,440
matching you using a different neural

00:13:34,800 --> 00:13:38,910
network that does object matching and

00:13:37,440 --> 00:13:41,820
I'll go into that a little bit later and

00:13:38,910 --> 00:13:42,330
then because of your classification that

00:13:41,820 --> 00:13:44,190
we gave you

00:13:42,330 --> 00:13:45,300
we're gonna say this piece of art is

00:13:44,190 --> 00:13:47,490
something you might be interested in

00:13:45,300 --> 00:13:50,040
based on what was strongest in your

00:13:47,490 --> 00:13:52,529
classifications and then whoop and then

00:13:50,040 --> 00:13:54,630
we created a path through other pieces

00:13:52,529 --> 00:13:56,130
of art that's sort of like provided this

00:13:54,630 --> 00:14:00,170
sort of like trail that you could follow

00:13:56,130 --> 00:14:02,880
from one our piece of art to the next

00:14:00,170 --> 00:14:04,650
this was an educational project so what

00:14:02,880 --> 00:14:06,690
we really wanted to do was to provide

00:14:04,650 --> 00:14:09,270
people a way to question the technology

00:14:06,690 --> 00:14:11,850
and have it be personal so that people

00:14:09,270 --> 00:14:14,100
could have a further more articulate

00:14:11,850 --> 00:14:15,300
discussion in the community so we also

00:14:14,100 --> 00:14:17,700
wanted to highlight where our

00:14:15,300 --> 00:14:20,850
technologies failed I'll go into that a

00:14:17,700 --> 00:14:23,040
little bit and then also we provided

00:14:20,850 --> 00:14:25,020
details about how the artwork was chosen

00:14:23,040 --> 00:14:27,510
a very technical level if people wanted

00:14:25,020 --> 00:14:28,890
to get them out as well

00:14:27,510 --> 00:14:31,170
so cool so one of the things that we

00:14:28,890 --> 00:14:33,120
really wanted to do was to say you know

00:14:31,170 --> 00:14:36,360
what are the ways that we can use to

00:14:33,120 --> 00:14:38,310
find art that we can match to people and

00:14:36,360 --> 00:14:39,450
so one of the techniques that we came

00:14:38,310 --> 00:14:41,390
across was something called reverse

00:14:39,450 --> 00:14:44,370
image search and so reverse image search

00:14:41,390 --> 00:14:47,880
essentially you take an image you use an

00:14:44,370 --> 00:14:50,130
object detection neural network and you

00:14:47,880 --> 00:14:53,460
kind of short-circuit it to provide you

00:14:50,130 --> 00:14:55,500
with numbers instead of names of things

00:14:53,460 --> 00:14:57,000
and then with those numbers you can sort

00:14:55,500 --> 00:14:59,730
of compare them to other objects and

00:14:57,000 --> 00:15:01,200
find in the data set other objects that

00:14:59,730 --> 00:15:05,220
look the same so I'll just kind of let

00:15:01,200 --> 00:15:06,420
that let that picture clean itself one

00:15:05,220 --> 00:15:08,550
of the object neural the neural networks

00:15:06,420 --> 00:15:10,740
are used was called vgg 16 it's 16

00:15:08,550 --> 00:15:12,180
layers you slice off the last layer that

00:15:10,740 --> 00:15:14,520
basically tells you what the thing is

00:15:12,180 --> 00:15:17,030
and you're left with it with a giant

00:15:14,520 --> 00:15:19,890
vector for every object that's numbers

00:15:17,030 --> 00:15:21,650
here's another example of finding stuff

00:15:19,890 --> 00:15:24,000
that's similar to the vase on the left

00:15:21,650 --> 00:15:25,320
so once you've got these numbers you

00:15:24,000 --> 00:15:27,600
basically have coordinates in this

00:15:25,320 --> 00:15:31,410
multi-dimensional space and you can use

00:15:27,600 --> 00:15:34,290
traditional distance algorithms to see

00:15:31,410 --> 00:15:36,240
what's close to what and then once you

00:15:34,290 --> 00:15:37,680
do something called PCA analysis or

00:15:36,240 --> 00:15:39,900
principal component analysis you can

00:15:37,680 --> 00:15:42,120
slam you know four thousand dimensions

00:15:39,900 --> 00:15:45,240
down into two dimensions using this

00:15:42,120 --> 00:15:47,700
algorithm which essentially maintains

00:15:45,240 --> 00:15:50,370
closeness of points in multi-dimensional

00:15:47,700 --> 00:15:52,230
space in the two dimensional translation

00:15:50,370 --> 00:15:53,550
and then once you're in two dimensions

00:15:52,230 --> 00:15:55,620
you can use traditional graphing

00:15:53,550 --> 00:15:57,810
algorithm or graph algorithms to find

00:15:55,620 --> 00:16:00,900
shortest paths like you would to find a

00:15:57,810 --> 00:16:02,610
route through a city so in this case we

00:16:00,900 --> 00:16:05,280
have a picture of a person you may have

00:16:02,610 --> 00:16:06,690
heard of him and the and a vase on the

00:16:05,280 --> 00:16:08,700
right hand side and we tried to find

00:16:06,690 --> 00:16:10,140
items in the museum collection that

00:16:08,700 --> 00:16:12,150
would sort of like transition from one

00:16:10,140 --> 00:16:14,450
to the next I'm not sure how convincing

00:16:12,150 --> 00:16:17,340
this is I kind of think it's convincing

00:16:14,450 --> 00:16:19,830
maybe it's like bias on me for selecting

00:16:17,340 --> 00:16:22,680
this but anyway you kind of get the

00:16:19,830 --> 00:16:26,490
picture this is how we selected art so

00:16:22,680 --> 00:16:29,610
lessons learned we had to try a lot of

00:16:26,490 --> 00:16:31,920
different ways to try to find things

00:16:29,610 --> 00:16:34,020
that would work - Matt - convincingly

00:16:31,920 --> 00:16:35,970
match you to something that's in the art

00:16:34,020 --> 00:16:37,350
right - a piece of art and once we got

00:16:35,970 --> 00:16:38,760
to this point we're like yeah that's

00:16:37,350 --> 00:16:41,180
pretty good Zuckerberg looks like that

00:16:38,760 --> 00:16:44,240
guy so okay we're like

00:16:41,180 --> 00:16:49,880
we're done alright but there's also

00:16:44,240 --> 00:16:51,950
mistakes that are made and I should have

00:16:49,880 --> 00:16:54,080
put like a warning or something on the

00:16:51,950 --> 00:16:55,520
previous slide so that we started out

00:16:54,080 --> 00:16:57,200
with this thing called the opencv

00:16:55,520 --> 00:16:59,330
classifier to find faces and it turns

00:16:57,200 --> 00:17:01,010
out it's not that good or at least it

00:16:59,330 --> 00:17:02,360
was good for a while but then when you

00:17:01,010 --> 00:17:03,860
know these convolutional neural networks

00:17:02,360 --> 00:17:06,110
came out we ended up using one byte

00:17:03,860 --> 00:17:09,650
called D live which hasn't I put that as

00:17:06,110 --> 00:17:10,940
a Python API it got much better but we

00:17:09,650 --> 00:17:12,860
were getting results like this and I I

00:17:10,940 --> 00:17:17,120
just I mean it's like automatic

00:17:12,860 --> 00:17:19,010
screenshots save right also privacy

00:17:17,120 --> 00:17:20,810
policy this is one of the least the

00:17:19,010 --> 00:17:22,970
lesser successful parts of the project

00:17:20,810 --> 00:17:25,670
was we put a giant wall sized terms and

00:17:22,970 --> 00:17:28,090
conditions on the wall to remind people

00:17:25,670 --> 00:17:30,530
that we were taking their images and

00:17:28,090 --> 00:17:32,600
making conclusions about them and

00:17:30,530 --> 00:17:34,490
recording it on a computer and and

00:17:32,600 --> 00:17:36,050
basically saying all the horrible things

00:17:34,490 --> 00:17:37,640
we could do with that information and

00:17:36,050 --> 00:17:38,660
then at the very end which they liked

00:17:37,640 --> 00:17:40,250
but we're not gonna do and we're gonna

00:17:38,660 --> 00:17:42,410
believe your data in 30 days but you

00:17:40,250 --> 00:17:43,700
should really think about this and I

00:17:42,410 --> 00:17:45,230
don't know what it is about putting

00:17:43,700 --> 00:17:47,030
terms and conditions in front of people

00:17:45,230 --> 00:17:49,460
but they don't want to read them so

00:17:47,030 --> 00:17:50,630
almost nobody read this which is a

00:17:49,460 --> 00:17:53,570
little disappointing for our graphic

00:17:50,630 --> 00:17:55,220
designer that we work with but it did

00:17:53,570 --> 00:17:56,990
teach this lesson that it was really

00:17:55,220 --> 00:18:00,680
incumbent on us to make this more

00:17:56,990 --> 00:18:03,350
approachable and consumable another

00:18:00,680 --> 00:18:06,110
super difficult thing to deal with was

00:18:03,350 --> 00:18:08,120
using the classifications and labeling

00:18:06,110 --> 00:18:11,810
the data that exists in the Y or exists

00:18:08,120 --> 00:18:13,340
currently right if you look at this it's

00:18:11,810 --> 00:18:15,260
probably too small for you to see but if

00:18:13,340 --> 00:18:17,270
you look at this up close you'll see

00:18:15,260 --> 00:18:20,980
that there's only four ethnicities

00:18:17,270 --> 00:18:25,880
available in the label faces in the wild

00:18:20,980 --> 00:18:27,650
attribute classifiers black white Asian

00:18:25,880 --> 00:18:30,830
and Indian I thought Asians and Indians

00:18:27,650 --> 00:18:32,630
were kind of but whatever and you know

00:18:30,830 --> 00:18:34,190
we're in Hawaii we're doing this project

00:18:32,630 --> 00:18:35,780
in Hawaii there's a lot of Southeast

00:18:34,190 --> 00:18:36,920
Asians right if you just say Asia and

00:18:35,780 --> 00:18:41,120
you're like I don't know and there's

00:18:36,920 --> 00:18:43,460
like it it's everybody almost and yeah

00:18:41,120 --> 00:18:44,690
and there's no Hawaiian right so you

00:18:43,460 --> 00:18:46,520
know there's so you kind of like leaving

00:18:44,690 --> 00:18:48,740
out a giant portion of the population

00:18:46,520 --> 00:18:50,510
and then also I'd probably not need to

00:18:48,740 --> 00:18:52,810
go into it but you know art collections

00:18:50,510 --> 00:18:55,530
that you're using this for also have a

00:18:52,810 --> 00:18:57,990
bias

00:18:55,530 --> 00:18:59,070
kind of taking a wider view you know I

00:18:57,990 --> 00:19:01,140
found this when we were doing our

00:18:59,070 --> 00:19:02,700
research there's obvious bias here I

00:19:01,140 --> 00:19:05,840
don't need to go into it but the dataset

00:19:02,700 --> 00:19:08,669
has a bias the engineers have a bias

00:19:05,840 --> 00:19:11,309
there's bias out there people anyway and

00:19:08,669 --> 00:19:12,990
then so this is Hawaii

00:19:11,309 --> 00:19:14,640
Hawaii's weird and this is why it's my

00:19:12,990 --> 00:19:16,500
concern why is super unique in the

00:19:14,640 --> 00:19:17,730
United States we're number one in terms

00:19:16,500 --> 00:19:20,160
of the percentage of non-white

00:19:17,730 --> 00:19:22,230
population even looking at this chart

00:19:20,160 --> 00:19:24,720
there's no category for Hawaiians

00:19:22,230 --> 00:19:28,380
there's also no category for mixed mixed

00:19:24,720 --> 00:19:30,780
race or ethnicity so so why is that

00:19:28,380 --> 00:19:33,179
right because the bar graph tool didn't

00:19:30,780 --> 00:19:35,370
really allow for that and then also

00:19:33,179 --> 00:19:37,830
locally our Police Department just

00:19:35,370 --> 00:19:39,660
bought 1200 axial on body cameras which

00:19:37,830 --> 00:19:42,030
sounds a lot like letting my axe on body

00:19:39,660 --> 00:19:44,010
cameras and I'm a little worried about

00:19:42,030 --> 00:19:45,510
that because you know

00:19:44,010 --> 00:19:48,360
excellent has an AI division

00:19:45,510 --> 00:19:50,160
headquartered in Scottsdale Arizona and

00:19:48,360 --> 00:19:51,120
you know if bias are the thing maybe

00:19:50,160 --> 00:19:53,700
this is something we should be concerned

00:19:51,120 --> 00:19:57,210
about why looks very different than

00:19:53,700 --> 00:19:59,490
Scottsdale Arizona and a worthwhile so I

00:19:57,210 --> 00:20:00,660
think in the words of so I'm gonna leave

00:19:59,490 --> 00:20:04,140
you with a couple of coats so we're

00:20:00,660 --> 00:20:05,730
worthwhile what's what exercise would be

00:20:04,140 --> 00:20:07,290
to delete the word technology from

00:20:05,730 --> 00:20:10,850
vocabulary in order to see how quickly

00:20:07,290 --> 00:20:13,950
capitalism's objectives are exposed okay

00:20:10,850 --> 00:20:16,530
marketing is powerful right AI I believe

00:20:13,950 --> 00:20:18,330
is one of those terms which if not more

00:20:16,530 --> 00:20:21,270
clearly defined it's essentially just

00:20:18,330 --> 00:20:22,830
marketing right and AI means anything

00:20:21,270 --> 00:20:26,190
that's all powerful and inevitable and

00:20:22,830 --> 00:20:27,570
as long as we we allow that term to be

00:20:26,190 --> 00:20:29,460
just thrown around without properly

00:20:27,570 --> 00:20:31,710
defining it we're basically giving in to

00:20:29,460 --> 00:20:33,570
this sort of desire to just say it's

00:20:31,710 --> 00:20:35,520
inevitable it's all powerful it's gonna

00:20:33,570 --> 00:20:36,929
happen let's just let it right so

00:20:35,520 --> 00:20:38,010
where's it so there's an opportunity for

00:20:36,929 --> 00:20:40,410
us and I'm gonna just kind of go through

00:20:38,010 --> 00:20:42,299
this real quick but this is actually

00:20:40,410 --> 00:20:45,690
happening like people are getting

00:20:42,299 --> 00:20:48,059
tickets in China because their picture

00:20:45,690 --> 00:20:49,679
goes across the intersection right we

00:20:48,059 --> 00:20:54,809
don't really have to let that happen

00:20:49,679 --> 00:20:56,370
but we do also this quote by having

00:20:54,809 --> 00:20:58,410
anymore is also inefficiency is

00:20:56,370 --> 00:21:00,360
precisely what shelters us from the APA

00:20:58,410 --> 00:21:02,640
inhumanity of Taylorism and market

00:21:00,360 --> 00:21:03,720
fundamentalism so when it efficiency is

00:21:02,640 --> 00:21:05,520
the result of the deliberative

00:21:03,720 --> 00:21:06,360
commitment by a democratically run

00:21:05,520 --> 00:21:08,220
community

00:21:06,360 --> 00:21:09,600
no need to eliminate it even if the

00:21:08,220 --> 00:21:12,290
latest technologies can accomplish that

00:21:09,600 --> 00:21:15,480
in no time I love this guy's books I

00:21:12,290 --> 00:21:17,550
highly recommend them essentially

00:21:15,480 --> 00:21:19,440
sometimes in efficiencies a good thing

00:21:17,550 --> 00:21:21,690
we should think about it one think about

00:21:19,440 --> 00:21:23,880
it when we eliminate it and then finally

00:21:21,690 --> 00:21:25,860
so technology is not and never can be a

00:21:23,880 --> 00:21:28,140
thing in itself isolated from economies

00:21:25,860 --> 00:21:29,730
and it kind of economics and society

00:21:28,140 --> 00:21:33,150
this means that technological

00:21:29,730 --> 00:21:38,460
inevitability does not exist and I can't

00:21:33,150 --> 00:21:41,670
I love this quote it's it's everything

00:21:38,460 --> 00:21:43,530
that that that we love about the work

00:21:41,670 --> 00:21:44,670
that we do is learning about the

00:21:43,530 --> 00:21:46,340
communities and learning about the

00:21:44,670 --> 00:21:48,480
technology that they use and

00:21:46,340 --> 00:21:50,010
understanding when it's appropriate to

00:21:48,480 --> 00:21:51,420
apply the technology and when it's

00:21:50,010 --> 00:21:55,800
appropriate to step back and just sort

00:21:51,420 --> 00:21:57,930
of listen so I encourage you all to take

00:21:55,800 --> 00:22:01,440
this you know what did I end on a high

00:21:57,930 --> 00:22:03,090
note so today to take what I've said

00:22:01,440 --> 00:22:05,220
it's like go forth to your local

00:22:03,090 --> 00:22:08,970
communities and see how you can create

00:22:05,220 --> 00:22:10,350
your own lens and use your lens to shape

00:22:08,970 --> 00:22:14,340
the future with these technologies

00:22:10,350 --> 00:22:15,310
because none of this is inevitable so

00:22:14,340 --> 00:22:22,280
thank you very much

00:22:15,310 --> 00:22:22,280

YouTube URL: https://www.youtube.com/watch?v=AGtDAO_a9Mk


