Title: Naveed Ihsanullah: Parallelism experiments in JavaScript | JSConf US 2015
Publication date: 2015-06-17
Playlist: JSConf US 2015
Description: 
	With the amazing performance of modern single threaded JavaScript how can we catch up in parallelism? Today’s hardware provides specialized instructions that can operate on data in parallel and provides multiple execution units that can run code in parallel. The single threaded nature of classic JavaScript cannot take advantage of these resources. When quad-core smartphones are already available today that leaves a lot of performance potential on the table. I will share work we are doing to extend JavaScript with flexible and powerful primitives for parallelism that will unlock new performance opportunities to the Web. Let’s explore how native code concepts like shared memory and execution synchronization could work in JavaScript. With great power comes great responsibility so I will touch on some mitigation strategies we have in place to make sure tomorrow’s web applications stay well behaved as they use all the horsepower your hardware can provide.

Transcript: https://gist.github.com/voodootikigod/e146469b95b7c25962cd
Captions: 
	00:00:14,880 --> 00:00:15,880
That was a very kind introduction.

00:00:15,880 --> 00:00:16,880
Thank you very much.

00:00:16,880 --> 00:00:17,880
So how is everyone doing at JSConf?

00:00:17,880 --> 00:00:18,880
I heard I have no competition at the beach.

00:00:18,880 --> 00:00:19,880
Everyone is here.

00:00:19,880 --> 00:00:20,880
Full house.

00:00:20,880 --> 00:00:21,880
Thank you for coming by.

00:00:21,880 --> 00:00:22,880
So yeah.

00:00:22,880 --> 00:00:23,880
Like I said, I want to talk to you about concurrency and

00:00:23,880 --> 00:00:24,880
parallelism in JavaScript.

00:00:24,880 --> 00:00:25,880
Maybe a brief introduction is warranted.

00:00:25,880 --> 00:00:26,880
I do lead the JS team, and for the last several years, we've been thinking about

00:00:26,880 --> 00:00:30,590
this, these problems in JavaScript, and how we can address that.

00:00:30,590 --> 00:00:35,180
I would like to share some of the ways we've come around to improving concurrency

00:00:35,180 --> 00:00:36,290
and parallelism in JS.

00:00:36,290 --> 00:00:37,290
But before we...

00:00:37,290 --> 00:00:40,270
The string is a little tight.

00:00:40,270 --> 00:00:44,890
Before we go down that road, let's actually talk about some definitions.

00:00:44,890 --> 00:00:49,360
Concurrency and parallelism often come up together but they're not the same thing.

00:00:49,360 --> 00:00:52,160
The way I think about it is...

00:00:52,160 --> 00:00:55,190
Concurrency is how do you make an application more usable?

00:00:55,190 --> 00:00:59,400
And here we have two queues of people and a single revolving

00:00:59,400 --> 00:01:00,620
door.

00:01:00,620 --> 00:01:05,239
You can come up with some logic to have them take turns going through

00:01:05,239 --> 00:01:07,009
there, but it doesn't matter what you do.

00:01:07,009 --> 00:01:10,690
You have one whole queue go through first or have them take turns

00:01:10,690 --> 00:01:16,600
like that -- some sort of multitasking to utilize the resource and make sure

00:01:16,600 --> 00:01:17,600
everything happens.

00:01:17,600 --> 00:01:21,299
And examples inside the running the program or

00:01:21,299 --> 00:01:27,740
application might be pre-emptive multitasking, Windows 95, or async

00:01:27,740 --> 00:01:30,799
programming that we're very familiar with.

00:01:30,799 --> 00:01:37,100
Parallelism is all about how you can be faster by leveraging or exploiting

00:01:37,100 --> 00:01:38,939
hardware availability.

00:01:38,939 --> 00:01:42,510
So in this case, we now have two revolving doors,

00:01:42,510 --> 00:01:46,179
and we can theoretically hopefully go twice as fast, because there's

00:01:46,179 --> 00:01:48,840
no contention and complex in my examples.

00:01:48,840 --> 00:01:50,780
But the real world is not like that.

00:01:50,780 --> 00:01:53,679
The real world has complexity, and the reason why concurrency

00:01:53,679 --> 00:01:59,009
and parallelism are often mixed together is because in the real world, it's

00:01:59,009 --> 00:02:02,850
really hard to separate the different aspects.

00:02:02,850 --> 00:02:05,409
And you have to be very sensitive to the way you take care of your

00:02:05,409 --> 00:02:08,929
application to avoid deadlocks and races.

00:02:08,929 --> 00:02:13,870
So now that we talked about those definitions, I want to explain why improving

00:02:13,870 --> 00:02:15,480
this is actually important in JavaScript.

00:02:15,480 --> 00:02:18,900
And to do that, let's talk about the evolution of the web

00:02:18,900 --> 00:02:19,900
application.

00:02:19,900 --> 00:02:24,780
Back in the day, not quite the '80s, but way, way back, we all

00:02:24,780 --> 00:02:27,150
had a homepage that looked something like this.

00:02:27,150 --> 00:02:28,150
I'm sorry.

00:02:28,150 --> 00:02:30,440
Mine is under construction at the moment, but maybe it will

00:02:30,440 --> 00:02:32,500
evolve a little.

00:02:32,500 --> 00:02:36,060
Static html and GIFs can only take us so far.

00:02:36,060 --> 00:02:38,250
In 1995, though, something really big happened.

00:02:38,250 --> 00:02:41,830
I think most of us know what this is?

00:02:41,830 --> 00:02:44,730
Any guesses?

00:02:44,730 --> 00:02:46,210
I think I heard it.

00:02:46,210 --> 00:02:47,210
Yes.

00:02:47,210 --> 00:02:49,270
JavaScript was invented, and it was good.

00:02:49,270 --> 00:02:50,400
The universe of...

00:02:50,400 --> 00:02:54,250
I think I came to the right venue to make that

00:02:54,250 --> 00:02:55,420
comment.

00:02:55,420 --> 00:02:59,640
The universe of the modern interactive web was really born.

00:02:59,640 --> 00:03:04,450
And homepages were never the same again.

00:03:04,450 --> 00:03:08,900
So in the next 20 years, I think the home pages and websites

00:03:08,900 --> 00:03:11,850
have evolved past Windows alert.

00:03:11,850 --> 00:03:17,120
And now you have desktop applications that have actually gone non-native.

00:03:17,120 --> 00:03:22,490
Here we have an example of Excel, and most of the Microsoft Office suite is now available

00:03:22,490 --> 00:03:25,040
as a web application.

00:03:25,040 --> 00:03:29,260
JavaScript and the web platform is actually capable of supporting even

00:03:29,260 --> 00:03:30,920
virtual reality technology.

00:03:30,920 --> 00:03:34,120
Here we have a frame or a couple of frames from

00:03:34,120 --> 00:03:40,620
the Polar Sea Project that MozVR is working on.

00:03:40,620 --> 00:03:45,690
So today's JavaScript is really vast.

00:03:45,690 --> 00:03:49,190
We track most of the modern browsers, their performance, and how

00:03:49,190 --> 00:03:50,410
they're doing over time.

00:03:50,410 --> 00:03:54,020
And here you can see that we're all getting a lot

00:03:54,020 --> 00:03:57,430
faster.

00:03:57,430 --> 00:04:00,061
And that's continued to be true for other factors as well, because

00:04:00,061 --> 00:04:04,190
we're inventing other technologies, like (inaudible), that allow us

00:04:04,190 --> 00:04:08,500
to bypass some of the inherent bottle necks in regular JavaScript.

00:04:08,500 --> 00:04:11,650
In this slide...

00:04:11,650 --> 00:04:14,040
The bottom line is native.

00:04:14,040 --> 00:04:16,070
C++ compiled code.

00:04:16,070 --> 00:04:20,130
And JS running on Firefox is only 2X of that.

00:04:20,130 --> 00:04:22,039
We have techniques in certain workloads that can

00:04:22,039 --> 00:04:24,580
get much closer to native.

00:04:24,580 --> 00:04:25,940
1.5X.

00:04:25,940 --> 00:04:29,660
So if JavaScript is so fast, why do we have a problem?

00:04:29,660 --> 00:04:36,750
Well, to illustrate that, I have a snapshot here of my Windows desktop.

00:04:36,750 --> 00:04:42,250
Right now you can see that the desktop doesn't seem to be doing a whole

00:04:42,250 --> 00:04:43,250
lot.

00:04:43,250 --> 00:04:47,440
We're running at 5% CPU utilization, and I have a ton of applications

00:04:47,440 --> 00:04:52,440
open, including Firefox with, I believe, at the moment, 30 tabs.

00:04:52,440 --> 00:04:55,680
So I open one more tab with a really intensive JavaScript

00:04:55,680 --> 00:04:57,180
application.

00:04:57,180 --> 00:05:03,690
And you'll see that the PerfMon seems to be indicating something.

00:05:03,690 --> 00:05:06,900
Those of you who are not familiar with Windows and PerfMon, each of

00:05:06,900 --> 00:05:14,090
these squares represents a CPU core and how much work it's doing.

00:05:14,090 --> 00:05:19,919
In this illustration, cores three and four -- they're virtual cores, but we'll

00:05:19,919 --> 00:05:24,780
not worry about that detail -- cores three and four are doing a lot of work,

00:05:24,780 --> 00:05:28,780
but the rest of the CPU is mostly just sitting there, and overall utilization

00:05:28,780 --> 00:05:32,000
is at 24%.

00:05:32,000 --> 00:05:33,000
Out of a possible 100.

00:05:33,000 --> 00:05:34,000
Right?

00:05:34,000 --> 00:05:37,590
So the computer is still 75% idle.

00:05:37,590 --> 00:05:39,130
Why does this matter?

00:05:39,130 --> 00:05:43,270
Well, the gigahertz race is effectively at an end.

00:05:43,270 --> 00:05:46,490
It's hard to make CPUs faster than they are now.

00:05:46,490 --> 00:05:50,330
Power and heat have stopped our ability to innovate there.

00:05:50,330 --> 00:05:53,740
So for future hardware, the way we're scaling is

00:05:53,740 --> 00:05:57,389
by adding additional cores or execution units.

00:05:57,389 --> 00:06:00,419
Your modern smartphone typically has two cores.

00:06:00,419 --> 00:06:04,040
Some of them are even quad-core enabled.

00:06:04,040 --> 00:06:09,919
Laptops and desktops have four to eight virtual course and workstations can have 4 to 20 cores

00:06:09,919 --> 00:06:14,660
per CPU, and they may have multiple CPUs on their motherboards.

00:06:14,660 --> 00:06:19,590
So right here I've only talked about the parallelism aspect, how to do work at the

00:06:19,590 --> 00:06:21,020
same time.

00:06:21,020 --> 00:06:23,490
But concurrency has its issues too.

00:06:23,490 --> 00:06:27,139
We're always waiting on something in a JavaScript application.

00:06:27,139 --> 00:06:31,350
Waiting on events, on the user, on the cache, and of course

00:06:31,350 --> 00:06:35,110
always waiting on the internet to do something.

00:06:35,110 --> 00:06:40,660
So improving parallelism and improving concurrency can really make a big

00:06:40,660 --> 00:06:43,170
difference to a host of applications.

00:06:43,170 --> 00:06:48,730
Some of the application types are of course games, AI, rendering, code generation,

00:06:48,730 --> 00:06:53,199
and each of these specific application types have their own domain-specific

00:06:53,199 --> 00:06:56,290
way of dealing with these constructs.

00:06:56,290 --> 00:06:59,350
But something that -- if we do solve this problem, we want to

00:06:59,350 --> 00:07:03,180
preserve it, if possible.

00:07:03,180 --> 00:07:08,210
While looking at this problem and investigating different ways to deal with it, web workers

00:07:08,210 --> 00:07:09,210
are around.

00:07:09,210 --> 00:07:10,530
Web workers do promote parallelism.

00:07:10,530 --> 00:07:12,280
You can do something on a thread.

00:07:12,280 --> 00:07:15,110
They tend to be very heavy threads.

00:07:15,110 --> 00:07:16,229
But it works.

00:07:16,229 --> 00:07:21,160
The real issue for saturating a core doesn't seem to be spinning up another thread.

00:07:21,160 --> 00:07:22,920
It's communication between that.

00:07:22,920 --> 00:07:26,320
And coordination is really about concurrency.

00:07:26,320 --> 00:07:28,280
But concurrency does exist on the web today.

00:07:28,280 --> 00:07:31,130
Exists in JavaScript.

00:07:31,130 --> 00:07:36,010
We've had at this point parallel JavaScript -- post message

00:07:36,010 --> 00:07:37,010
will always be around.

00:07:37,010 --> 00:07:38,389
It's very useful.

00:07:38,389 --> 00:07:41,950
And then the buffered transfer semantics.

00:07:41,950 --> 00:07:44,260
So let's dive into each one of these and see where they

00:07:44,260 --> 00:07:45,800
may have some limitations.

00:07:45,800 --> 00:07:51,600
Parallel JS, for those who are not familiar with it, it was an experimental

00:07:51,600 --> 00:07:55,330
effort to see if we can bring some of the semantics of MapReduce to

00:07:55,330 --> 00:08:01,139
JavaScript, and enable a whole slew of application types.

00:08:01,139 --> 00:08:03,699
It was a casual API, but it really took some sophistication to

00:08:03,699 --> 00:08:07,301
use it, so the average programmer couldn't make that much out of it, and it

00:08:07,301 --> 00:08:10,770
had some really high implementation costs in the engine, so it was hard to support

00:08:10,770 --> 00:08:13,360
going forward.

00:08:13,360 --> 00:08:15,650
PostMessage obviously...

00:08:15,650 --> 00:08:17,050
Around and incredibly useful.

00:08:17,050 --> 00:08:21,169
But it also has its own set of issues.

00:08:21,169 --> 00:08:22,389
Dependent on the event loop.

00:08:22,389 --> 00:08:26,990
The performance, which we'll go into a little bit more later, just isn't there,

00:08:26,990 --> 00:08:28,390
and you cannot share state.

00:08:28,390 --> 00:08:30,990
You're transferring state over.

00:08:30,990 --> 00:08:32,050
Really one at a time.

00:08:32,050 --> 00:08:35,950
And if you care about the response, you have to transfer a new state back.

00:08:35,950 --> 00:08:39,630
It's possible with postMessage to transfer a buffer entirely by

00:08:39,630 --> 00:08:45,009
reference, and that solves some subsets of problems, but you still

00:08:45,009 --> 00:08:47,650
can't work on the same data at the same time.

00:08:47,650 --> 00:08:52,190
This really is a bottleneck, a coordination bottleneck, and it's something

00:08:52,190 --> 00:08:54,720
that we want to address.

00:08:54,720 --> 00:08:57,149
So while looking at different possible ways of

00:08:57,149 --> 00:09:01,610
handling this, it sort of became apparent that obviously native has

00:09:01,610 --> 00:09:03,149
handled this already.

00:09:03,149 --> 00:09:07,560
The operating system libraries have a slew of constructs

00:09:07,560 --> 00:09:10,690
that allow us to solve concurrent problems.

00:09:10,690 --> 00:09:11,690
Solve them well.

00:09:11,690 --> 00:09:15,850
And solve them in ways that each individual application can tailor for its

00:09:15,850 --> 00:09:17,190
own needs.

00:09:17,190 --> 00:09:21,630
So that helped us come up with some design considerations for what our

00:09:21,630 --> 00:09:23,360
ideal solution may look like.

00:09:23,360 --> 00:09:25,580
Of course, we want native-like performance.

00:09:25,580 --> 00:09:27,560
That's the Holy Grail benchmark.

00:09:27,560 --> 00:09:29,430
Let's be as fast as we can be.

00:09:29,430 --> 00:09:33,060
We don't want to be dependent on the main event loop, if

00:09:33,060 --> 00:09:34,390
possible.

00:09:34,390 --> 00:09:38,660
Because everyone uses the event loop, and that -- as Simon

00:09:38,660 --> 00:09:42,310
pointed out, can lead to some jankiness.

00:09:42,310 --> 00:09:43,630
Implementation versatility.

00:09:43,630 --> 00:09:45,810
What I mean by that is...

00:09:45,810 --> 00:09:50,950
Every problem has probably an ideal answer.

00:09:50,950 --> 00:09:53,490
And so you want to allow the developer to come

00:09:53,490 --> 00:09:56,760
up with an answer that makes the most sense for them, instead of being

00:09:56,760 --> 00:10:02,010
constrained by a really high level set of utilities that force them

00:10:02,010 --> 00:10:06,320
to talk in a particular vocabulary.

00:10:06,320 --> 00:10:10,760
We want to support -- this one may seem a little arbitrary, but we'll

00:10:10,760 --> 00:10:15,360
explain more in the future -- we want to support the algorithms and

00:10:15,360 --> 00:10:18,250
applications that are based on threads and pthreads.

00:10:18,250 --> 00:10:21,740
For a long time, threads have been around, and there's a lot

00:10:21,740 --> 00:10:24,000
of good computer science research on them.

00:10:24,000 --> 00:10:25,060
How to use them really effectively.

00:10:25,060 --> 00:10:27,670
And all those algorithms have a lot of value.

00:10:27,670 --> 00:10:32,899
Not every single application, not every single problem can benefit from those algorithms.

00:10:32,899 --> 00:10:36,120
But it is possible to directly apply them with minimal changes.

00:10:36,120 --> 00:10:38,480
It does seem like a win for the user.

00:10:38,480 --> 00:10:40,720
So it's something that we would like to support.

00:10:40,720 --> 00:10:45,450
And finally, support the extensible web philosophy.

00:10:45,450 --> 00:10:49,890
Anyone out there familiar with the extensible web philosophy?

00:10:49,890 --> 00:10:52,190
Or have read the extensible web manifesto?

00:10:52,190 --> 00:10:53,560
A couple of people?

00:10:53,560 --> 00:10:57,200
It's worth explaining a little bit here.

00:10:57,200 --> 00:10:59,271
I'm sure every single person will have their own take on it,

00:10:59,271 --> 00:11:04,529
and I certainly invite you to go to the extensible web manifesto website, to

00:11:04,529 --> 00:11:06,380
read about it in more detail.

00:11:06,380 --> 00:11:10,649
But at a high level, it's thought that the best

00:11:10,649 --> 00:11:14,230
innovation is going to come from developers, come from you guys.

00:11:14,230 --> 00:11:18,650
Sometimes standards bodies and browser developers can

00:11:18,650 --> 00:11:23,430
spend too much time iterating an idea, trying to come up with something perfect,

00:11:23,430 --> 00:11:27,750
and by the time it comes out to where the developers can use it, we

00:11:27,750 --> 00:11:29,399
didn't really hit the mark.

00:11:29,399 --> 00:11:32,180
So it seems like it's a better model for us to provide

00:11:32,180 --> 00:11:36,580
low level primitives that you can then generate and iterate on quickly

00:11:36,580 --> 00:11:41,600
in JS, instead of for us to provide rich high level implementations that

00:11:41,600 --> 00:11:46,750
are surfaced, and then you don't have a lot of flexibility to tailor

00:11:46,750 --> 00:11:51,040
that to the way you want to do work.

00:11:51,040 --> 00:11:55,550
So taking all this together, we came up with shared memory for

00:11:55,550 --> 00:11:57,170
JavaScript.

00:11:57,170 --> 00:11:59,880
And it looks something like this.

00:11:59,880 --> 00:12:03,390
So this font might be a little bit small for some of the people in the back

00:12:03,390 --> 00:12:04,390
to read.

00:12:04,390 --> 00:12:07,020
I know you guys in the front can only read the first line.

00:12:07,020 --> 00:12:12,360
So I would like to thank Lars Hanson for actually putting this document together.

00:12:12,360 --> 00:12:16,170
Obviously it's incredibly detailed, and there's a link at the end, so

00:12:16,170 --> 00:12:18,170
you can take a look at it yourself.

00:12:18,170 --> 00:12:20,950
But I think this might be one of those times where an example

00:12:20,950 --> 00:12:24,360
might be more beneficial than going through all the pages.

00:12:24,360 --> 00:12:26,029
So what I would like to do is...

00:12:26,029 --> 00:12:28,690
Build an example.

00:12:28,690 --> 00:12:30,709
The incrementing worker.

00:12:30,709 --> 00:12:33,410
And what I want to do here is implement that same

00:12:33,410 --> 00:12:35,800
example in a couple of different ways.

00:12:35,800 --> 00:12:39,040
First with postMessage, and then with using the shared

00:12:39,040 --> 00:12:43,170
memory techniques that we just came up with, and you can get an idea of what

00:12:43,170 --> 00:12:47,230
that work flow would look like, and what the trade-offs might be.

00:12:47,230 --> 00:12:48,740
So...

00:12:48,740 --> 00:12:50,880
First we're going to do this with postMessage, and what I did

00:12:50,880 --> 00:12:53,649
here was build a sequence diagram so you can really understand exactly

00:12:53,649 --> 00:12:56,890
what I'm trying to do.

00:12:56,890 --> 00:12:57,890
Create a worker.

00:12:57,890 --> 00:13:02,690
Brief that worker to tell us it's ready, and then we'll start working.

00:13:02,690 --> 00:13:08,330
And the goal of this incrementing worker -- really all it's going to do is talk in

00:13:08,330 --> 00:13:13,370
some kind of mechanism channel back to the master or the thread, passing

00:13:13,370 --> 00:13:16,550
an integer each time and incrementing it when it gets an integer from

00:13:16,550 --> 00:13:17,830
the master.

00:13:17,830 --> 00:13:22,209
They're going to reach some count, in this case I used 100,000,

00:13:22,209 --> 00:13:28,550
and when that message passing is done, the master will display some results.

00:13:28,550 --> 00:13:31,230
So the simplest scenario here is...

00:13:31,230 --> 00:13:33,070
The master says -- hey, start at zero.

00:13:33,070 --> 00:13:34,510
Go for 100,000 times.

00:13:34,510 --> 00:13:37,370
At the end, the worker will -- and the master will

00:13:37,370 --> 00:13:39,340
have a variable that says 100,000.

00:13:39,340 --> 00:13:42,750
Not rocket science, but hopefully enough to illustrate some points.

00:13:42,750 --> 00:13:46,350
Here's an implementation of the master side.

00:13:46,350 --> 00:13:51,240
It's code and it's small, and I think it's worth calling out some lines here.

00:13:51,240 --> 00:13:59,810
So we agree on 100,000 iterations, and the master posts to the worker to

00:13:59,810 --> 00:14:02,040
start at zero.

00:14:02,040 --> 00:14:06,000
The worker and the master will talk inside this callback, and

00:14:06,000 --> 00:14:08,080
the worker's date is going to come back as event data.

00:14:08,080 --> 00:14:11,050
It's going to be unpackaged.

00:14:11,050 --> 00:14:14,460
When some condition has met the terminal condition, it will

00:14:14,460 --> 00:14:15,460
end.

00:14:15,460 --> 00:14:20,130
Otherwise the master will pass the integer right back to the worker.

00:14:20,130 --> 00:14:22,700
On the worker's side, it's even simpler.

00:14:22,700 --> 00:14:27,540
The worker -- all it does is unpack the master's integer, increments it,

00:14:27,540 --> 00:14:30,610
and sends it right back on its way.

00:14:30,610 --> 00:14:32,209
So how does that work?

00:14:32,209 --> 00:14:33,300
How fast does that work?

00:14:33,300 --> 00:14:37,860
Well, for postMessage in this implementation on this laptop, I get

00:14:37,860 --> 00:14:40,709
about 54,000 messages per second.

00:14:40,709 --> 00:14:42,850
I think the example is pretty simple.

00:14:42,850 --> 00:14:47,779
There's probably a variety of these that you use all the time just for solving

00:14:47,779 --> 00:14:51,390
message passing techniques between the worker and the master.

00:14:51,390 --> 00:14:58,490
Let's do that again, using shared memory, specifically using shared Int32 array.

00:14:58,490 --> 00:15:03,990
So there's going to be some new constructs here, coming out of the shared

00:15:03,990 --> 00:15:08,070
memory document that I would like to call out.

00:15:08,070 --> 00:15:10,339
This is actually too small for even me to read.

00:15:10,339 --> 00:15:11,610
Make that bigger.

00:15:11,610 --> 00:15:14,720
Going to make sure I'm on the same page with all of you.

00:15:14,720 --> 00:15:16,390
So the first line I'm highlighting here is shared

00:15:16,390 --> 00:15:17,390
array buffer.

00:15:17,390 --> 00:15:24,579
This is one of the core pieces of the concurrency technique that

00:15:24,579 --> 00:15:25,660
we've come up with.

00:15:25,660 --> 00:15:30,529
Which allows us to share memory.

00:15:30,529 --> 00:15:32,050
Basically you specify a side.

00:15:32,050 --> 00:15:38,240
And here is a syncretic object that's actually layered on top of what actually exists

00:15:38,240 --> 00:15:39,589
in the spec.

00:15:39,589 --> 00:15:42,950
But what's in the spec is a little too low level to cleanly

00:15:42,950 --> 00:15:45,850
show it in a presentation.

00:15:45,850 --> 00:15:47,600
The purpose of this is basically -- it's a waitable

00:15:47,600 --> 00:15:48,600
object.

00:15:48,600 --> 00:15:54,260
It's going to allow for the master and the worker to coordinate.

00:15:54,260 --> 00:15:58,620
Using postMessage, the only use of postMessage in this example, we're passing

00:15:58,620 --> 00:16:02,660
the shared buffer to the worker, but because it's a shared buffer,

00:16:02,660 --> 00:16:06,340
the master will be able to use it as well.

00:16:06,340 --> 00:16:09,770
So this is a shared Int32 array.

00:16:09,770 --> 00:16:13,350
That's a view on the shared buffer.

00:16:13,350 --> 00:16:16,160
We're basically saying -- at this address, inside the shared memory

00:16:16,160 --> 00:16:17,380
region, it's an integer.

00:16:17,380 --> 00:16:18,660
Treat it like an integer.

00:16:18,660 --> 00:16:22,019
Give me integer semantics on it.

00:16:22,019 --> 00:16:26,900
And here, inside the callback, using the syncretic, the master is

00:16:26,900 --> 00:16:31,220
saying, after incrementing the buffer, the master is saying -- all right, I'm

00:16:31,220 --> 00:16:32,220
done.

00:16:32,220 --> 00:16:33,389
So the worker will wake up.

00:16:33,389 --> 00:16:38,250
Now, in this example, unlike the example from before, the master is also incrementing

00:16:38,250 --> 00:16:39,940
the counts.

00:16:39,940 --> 00:16:44,180
The only real reason for doing that is -- this loop, the

00:16:44,180 --> 00:16:47,060
master will do nothing other than saying I'm done, without doing any real work.

00:16:47,060 --> 00:16:50,481
So I wanted him to do something to justify the existence of that loop, other

00:16:50,481 --> 00:16:53,430
than the wall for the worker to continue.

00:16:53,430 --> 00:16:58,110
Otherwise the worker would count to 1,000 instantly and be done

00:16:58,110 --> 00:17:02,269
without the master and the worker going back and forth.

00:17:02,269 --> 00:17:06,089
So here on the worker side we have once again that waitable

00:17:06,089 --> 00:17:14,040
object, the syncretic, and the shared Int32 view on the shared array

00:17:14,040 --> 00:17:15,040
buffer.

00:17:15,040 --> 00:17:21,630
And on the loop side, we wait for the master to be done, increment the buffer,

00:17:21,630 --> 00:17:23,630
and then say we're done.

00:17:23,630 --> 00:17:28,870
So it looks nothing like the postMessage version.

00:17:28,870 --> 00:17:32,420
It's incredibly specific to integer-only.

00:17:32,420 --> 00:17:35,660
PostMessage you can pass in anything.

00:17:35,660 --> 00:17:39,330
But it does function, and it functions incredibly fast.

00:17:39,330 --> 00:17:42,230
Where before we had 54,000 messages per second,

00:17:42,230 --> 00:17:44,460
this time we have 6 million.

00:17:44,460 --> 00:17:46,200
Over 6 million messages per second.

00:17:46,200 --> 00:17:48,420
I think we can all agree that I cheated a little bit.

00:17:48,420 --> 00:17:49,420
Right?

00:17:49,420 --> 00:17:51,500
So unlike the postMessage model, where I can pass

00:17:51,500 --> 00:17:57,570
anything in and do anything I want with it, where it's synchronized for me,

00:17:57,570 --> 00:18:00,890
and where I have constructs where I don't have to worry about locking, all

00:18:00,890 --> 00:18:04,130
that sort of nitty-gritty low level stuff was exposed in the shared

00:18:04,130 --> 00:18:07,130
memory version, so it's not really a drop-in replacement for

00:18:07,130 --> 00:18:08,130
postMessage.

00:18:08,130 --> 00:18:10,260
I think I can do better, and I'm going to try to do better

00:18:10,260 --> 00:18:11,680
in this second version.

00:18:11,680 --> 00:18:14,430
Once again, because performance isn't everything.

00:18:14,430 --> 00:18:18,510
Sometimes ergonomics of the implementation matter.

00:18:18,510 --> 00:18:22,611
So here what I do is -- and I'm hiding a lot of the details, but what I do

00:18:22,611 --> 00:18:28,220
differently here is I'm creating a new object that wraps a sender and

00:18:28,220 --> 00:18:32,330
receiver, a channel sender and a channel receiver, and all this is built on

00:18:32,330 --> 00:18:35,549
top of the same primitives that you saw before.

00:18:35,549 --> 00:18:38,840
And if we look inside our loop now, it's a lot clearer.

00:18:38,840 --> 00:18:41,050
And it maps much, much more directly, one to

00:18:41,050 --> 00:18:42,669
one, with postMessage.

00:18:42,669 --> 00:18:47,010
We can basically treat the send like the post to the

00:18:47,010 --> 00:18:50,640
worker, and the receive like the post from the worker.

00:18:50,640 --> 00:18:52,169
There's no locks exposed here.

00:18:52,169 --> 00:18:55,481
There's no waitable objects that you can screw up, by

00:18:55,481 --> 00:18:58,790
signaling at the wrong time.

00:18:58,790 --> 00:19:02,360
And the worker side is just as clean, just as

00:19:02,360 --> 00:19:03,360
simple.

00:19:03,360 --> 00:19:04,360
And I think very, very easy to read.

00:19:04,360 --> 00:19:10,890
Arguably even easier to read than the native postMessage implementation.

00:19:10,890 --> 00:19:13,830
With receive, increment, and send.

00:19:13,830 --> 00:19:15,930
So how does this perform?

00:19:15,930 --> 00:19:17,840
Still a lot better than postMessage.

00:19:17,840 --> 00:19:26,490
And it's a huge trade-off for using these high level constructs, versus

00:19:26,490 --> 00:19:28,760
shared Int32 array.

00:19:28,760 --> 00:19:29,760
But the choice is yours.

00:19:29,760 --> 00:19:30,760
Right?

00:19:30,760 --> 00:19:33,030
The choice for having high level constructs or low level

00:19:33,030 --> 00:19:37,250
performance is something that this implementation allows you to have.

00:19:37,250 --> 00:19:40,040
So going back to our design criteria, let's see how we did.

00:19:40,040 --> 00:19:41,890
Did we get native--like performance?

00:19:41,890 --> 00:19:45,380
Well, I didn't directly compare it against native

00:19:45,380 --> 00:19:52,370
C++, but we're 5X faster than postMessage.

00:19:52,370 --> 00:19:53,929
I think we did pretty well there.

00:19:53,929 --> 00:19:56,260
We'll come back to that later.

00:19:56,260 --> 00:19:59,130
How about not dependent on the main thread event loop?

00:19:59,130 --> 00:20:04,830
Aside from the shared array buffer, we're not dependent

00:20:04,830 --> 00:20:08,679
on the event loop at all.

00:20:08,679 --> 00:20:10,200
Versatility, I give myself a check.

00:20:10,200 --> 00:20:12,780
We have two implementations, one really fast and really

00:20:12,780 --> 00:20:17,440
specific and one general purpose that's still reasonably fast.

00:20:17,440 --> 00:20:21,250
Do we support the extensible web philosophy?

00:20:21,250 --> 00:20:24,330
The fact that you can have these multiple implementations

00:20:24,330 --> 00:20:28,320
and our iterations were done in JavaScript instead of rebuilding the browser

00:20:28,320 --> 00:20:31,820
each time, I think I can give myself a check for that as well.

00:20:31,820 --> 00:20:34,590
And the final one is -- do you support applications and

00:20:34,590 --> 00:20:36,870
algorithms based on threads and pthreads?

00:20:36,870 --> 00:20:39,170
We didn't really talk about that at all here, so I'm going to

00:20:39,170 --> 00:20:42,350
table that again for a little bit later in the discussion.

00:20:42,350 --> 00:20:45,820
So we agree at least on three of five?

00:20:45,820 --> 00:20:48,500
Maybe three and a half of five?

00:20:48,500 --> 00:20:50,360
Anyone?

00:20:50,360 --> 00:20:52,230
Okay.

00:20:52,230 --> 00:20:53,680
Hard graders out there in the audience.

00:20:53,680 --> 00:20:54,680
Okay.

00:20:54,680 --> 00:20:55,680
So...

00:20:55,680 --> 00:21:00,789
I have a couple of demos, and they're live demos.

00:21:00,789 --> 00:21:02,410
So we'll see how that goes.

00:21:02,410 --> 00:21:06,150
This is against a version of Nightly with one private patch

00:21:06,150 --> 00:21:07,150
applied.

00:21:07,150 --> 00:21:12,090
So one of these demos will work as is, in a Nightly version

00:21:12,090 --> 00:21:13,460
of Firefox.

00:21:13,460 --> 00:21:15,870
The other one is not quite ready yet.

00:21:15,870 --> 00:21:18,669
The first one that I would like to show you is...

00:21:18,669 --> 00:21:22,760
An implementation of the Mandelbrot algorithm,

00:21:22,760 --> 00:21:25,210
visualized in JavaScript.

00:21:25,210 --> 00:21:27,080
Let's go out to my Nightly.

00:21:27,080 --> 00:21:35,250
I don't know why that's here, but we'll ignore it.

00:21:35,250 --> 00:21:41,100
So here we have Mandelbrot in JS.

00:21:41,100 --> 00:21:42,590
And you're going to do the usual thing.

00:21:42,590 --> 00:21:43,590
Zoom around.

00:21:43,590 --> 00:21:45,070
By the way, this is the first demo that I ever

00:21:45,070 --> 00:21:47,320
showed my wife, who's in the audience.

00:21:47,320 --> 00:21:51,140
Something that I actually made possible on the web.

00:21:51,140 --> 00:21:54,240
So she was so excited to see this, thinking now she's

00:21:54,240 --> 00:21:55,830
going to understand what I do for work.

00:21:55,830 --> 00:22:00,260
After I was done showing her this, she has absolutely no idea what I do for work.

00:22:00,260 --> 00:22:04,670
So hopefully this will go over a little bit better for everybody else.

00:22:04,670 --> 00:22:07,470
Right now the default on this is to use one to four threads.

00:22:07,470 --> 00:22:10,429
I'm going to reduce it to one thread, and just show

00:22:10,429 --> 00:22:15,120
that our CPU utilization is...

00:22:15,120 --> 00:22:17,160
That number is...

00:22:17,160 --> 00:22:18,160
Not exactly right.

00:22:18,160 --> 00:22:20,190
What it's showing is out of 800...

00:22:20,190 --> 00:22:21,570
8 cores.

00:22:21,570 --> 00:22:25,650
So 100% would be full utilization of one core.

00:22:25,650 --> 00:22:28,020
So to show that a little bit more clearly, let's actually go

00:22:28,020 --> 00:22:30,480
to this, which I have running in the background.

00:22:30,480 --> 00:22:33,799
I don't know how familiar you are with the htop application.

00:22:33,799 --> 00:22:35,840
It's similar to the Perfmon I was using on Windows.

00:22:35,840 --> 00:22:42,429
You can see in the top left corner there are eight effective CPU

00:22:42,429 --> 00:22:46,480
cores on this machine, and right now it looks like four of them are kind of

00:22:46,480 --> 00:22:50,680
doing something, and overall CPU utilization is around 100% out of a

00:22:50,680 --> 00:22:53,750
possible 800%.

00:22:53,750 --> 00:22:55,150
So back to our demo.

00:22:55,150 --> 00:23:00,299
Let's change this to use four threads and increase the number of times that

00:23:00,299 --> 00:23:03,640
this thing updates from 50 to 500.

00:23:03,640 --> 00:23:09,950
Now, my frame rates are dropping, because of the update interval, but increasing, because

00:23:09,950 --> 00:23:10,980
I added more threads.

00:23:10,980 --> 00:23:14,400
What I really want to show here is...

00:23:14,400 --> 00:23:21,340
That it's possible for us to really make this computer come to a halt.

00:23:21,340 --> 00:23:26,721
So it's not very often that you can run a JavaScript application that pegs the CPU on

00:23:26,721 --> 00:23:28,220
a modern computer.

00:23:28,220 --> 00:23:32,560
And if you were up here, and I don't know if we can pick

00:23:32,560 --> 00:23:33,660
this up...

00:23:33,660 --> 00:23:37,750
The fans on this laptop are now at 100%.

00:23:37,750 --> 00:23:40,930
So yay!

00:23:40,930 --> 00:23:44,539
I came up with the most inefficient heater in

00:23:44,539 --> 00:23:48,990
the whole world.

00:23:48,990 --> 00:23:50,000
Hopefully this thing will...

00:23:50,000 --> 00:23:51,000
Shut down politely.

00:23:51,000 --> 00:23:52,070
I really don't know what that is.

00:23:52,070 --> 00:23:53,730
I'm just going to ignore it.

00:23:53,730 --> 00:23:57,400
Ask you to do the same.

00:23:57,400 --> 00:24:03,390
Go back to our presentation.

00:24:03,390 --> 00:24:08,280
On a different machine, with 16 effective cores, I actually captured the performance

00:24:08,280 --> 00:24:10,700
of that demo, so I could illustrate it here.

00:24:10,700 --> 00:24:14,660
And in that capture, I actually enabled something called SIMD,

00:24:14,660 --> 00:24:15,660
which had a blue bar.

00:24:15,660 --> 00:24:16,660
So just ignore that right now.

00:24:16,660 --> 00:24:19,580
If you guys are really interested in that, you can find me afterwards

00:24:19,580 --> 00:24:21,600
and we can talk more about SIMD.

00:24:21,600 --> 00:24:26,230
But the grey bars are the performance normalized against one core

00:24:26,230 --> 00:24:29,900
performance on that Mandelbrot example.

00:24:29,900 --> 00:24:36,500
You can see we have perfect CPU scaling across all 16 cores.

00:24:36,500 --> 00:24:42,230
Adding a core added about 70% more processing power, and that's processing that we could

00:24:42,230 --> 00:24:46,919
use for really awesome game demos like we saw in the presentations prior to

00:24:46,919 --> 00:24:50,470
this one.

00:24:50,470 --> 00:24:53,870
So the next demo I would like to show...

00:24:53,870 --> 00:24:56,559
Something I'm really, really excited by.

00:24:56,559 --> 00:24:59,020
And in fact, something we only just got completed.

00:24:59,020 --> 00:25:00,659
So hopefully this works.

00:25:00,659 --> 00:25:05,950
Like I said, I'm running a private build of Firefox.

00:25:05,950 --> 00:25:10,600
So I'll take a sip of water for good luck.

00:25:10,600 --> 00:25:13,590
This is the Unity WebGL benchmark.

00:25:13,590 --> 00:25:19,810
People out there familiar with Unity technology as a company?

00:25:19,810 --> 00:25:21,980
Pretty good percentage of the audience.

00:25:21,980 --> 00:25:24,669
For those who are not, Unity technologies makes a

00:25:24,669 --> 00:25:27,110
3D game engine called Unity.

00:25:27,110 --> 00:25:33,470
It's one of the most popular licensed 3D game engines in the world, if not the most popular

00:25:33,470 --> 00:25:38,580
one, and it's a company that I think does really, really exciting things.

00:25:38,580 --> 00:25:44,620
So for us to be able to work with them on enabling their benchmark, using our

00:25:44,620 --> 00:25:47,300
shared memory, was really exciting.

00:25:47,300 --> 00:25:51,010
Like I said, it's something we only got done Friday of last week.

00:25:51,010 --> 00:25:52,940
So we'll see how that goes.

00:25:52,940 --> 00:25:55,060
Back to my nightly.

00:25:55,060 --> 00:25:58,960
And let's start this up.

00:25:58,960 --> 00:26:05,490
These are default four cores.

00:26:05,490 --> 00:26:07,760
So...

00:26:07,760 --> 00:26:12,850
As you can see right away, we have a text rendering bug on the first

00:26:12,850 --> 00:26:15,049
screen, which we'll ignore.

00:26:15,049 --> 00:26:19,650
I'm also going to uncheck the first two demos, because they're graphically maybe not

00:26:19,650 --> 00:26:21,070
as interesting as some of the other ones.

00:26:21,070 --> 00:26:22,470
Let this go.

00:26:22,470 --> 00:26:28,340
So this benchmark is much, much more than a WebGL benchmark.

00:26:28,340 --> 00:26:32,680
What it is, actually, is testing the entire Unity 3D engine, and

00:26:32,680 --> 00:26:35,320
it's all running in the browser.

00:26:35,320 --> 00:26:40,180
Basically automatically ported from C++, very minimal changes on the Unity side.

00:26:40,180 --> 00:26:44,850
All the changes were in our shared memory and in Firefox itself.

00:26:44,850 --> 00:26:46,690
Here we have a bunch of dancing bears.

00:26:46,690 --> 00:26:50,169
And what's more exciting than skinned dancing bears?

00:26:50,169 --> 00:26:52,690
So like I said, it's much more than WebGL.

00:26:52,690 --> 00:26:55,650
They have AI running.

00:26:55,650 --> 00:26:56,980
Physics.

00:26:56,980 --> 00:26:58,299
Particles.

00:26:58,299 --> 00:26:59,630
Skinning.

00:26:59,630 --> 00:27:05,130
And also their job system, which allows for execution

00:27:05,130 --> 00:27:06,370
of threads.

00:27:06,370 --> 00:27:07,970
Threaded content.

00:27:07,970 --> 00:27:12,159
I think it was much prettier than any demo I

00:27:12,159 --> 00:27:13,159
could write.

00:27:13,159 --> 00:27:15,669
I'll let it go one more...

00:27:15,669 --> 00:27:20,030
Oh, yes, have to wait for the snowman in the middle of the flurries.

00:27:20,030 --> 00:27:26,830
Like I said, the reason why I'm really excited by this is...

00:27:26,830 --> 00:27:30,820
The Unity 3D engine is a massive code base.

00:27:30,820 --> 00:27:35,471
Enabling that -- basically automatically ported over directly from C++ on

00:27:35,471 --> 00:27:41,820
top of this is an incredible validation of the technology we put together.

00:27:41,820 --> 00:27:45,210
And that it's working at all is amazing.

00:27:45,210 --> 00:27:46,210
But I think...

00:27:46,210 --> 00:27:47,210
I'll show here...

00:27:47,210 --> 00:27:49,470
Let me stop this.

00:27:49,470 --> 00:27:51,789
Oops.

00:27:51,789 --> 00:27:56,440
Too many.

00:27:56,440 --> 00:27:57,871
The performance is really quite good too.

00:27:57,871 --> 00:28:01,799
I'm going to focus on just the top half of the slide.

00:28:01,799 --> 00:28:03,960
Those are all the tests that I was running on the

00:28:03,960 --> 00:28:04,960
left side.

00:28:04,960 --> 00:28:08,070
And what we had is the blue bar is native performance.

00:28:08,070 --> 00:28:10,480
We seem to have an anomaly on the first test.

00:28:10,480 --> 00:28:13,520
The blue bar is shorter than anything running in JavaScript.

00:28:13,520 --> 00:28:15,580
So I'm sure it's something I did.

00:28:15,580 --> 00:28:17,270
And we'll look at that later.

00:28:17,270 --> 00:28:21,169
But for all the other tests, green is normalized against one core

00:28:21,169 --> 00:28:22,210
performance.

00:28:22,210 --> 00:28:24,360
Yellow is four cores.

00:28:24,360 --> 00:28:26,809
And then eight -- red is eight.

00:28:26,809 --> 00:28:28,000
Reddish-orange.

00:28:28,000 --> 00:28:31,390
So you can see as we added four and then eight cores, we

00:28:31,390 --> 00:28:37,090
came much closer to native performance on a bunch of these tests.

00:28:37,090 --> 00:28:40,480
We're still behind on some of the other ones.

00:28:40,480 --> 00:28:42,179
Obviously there's always going to be work to do.

00:28:42,179 --> 00:28:47,270
And not everything is going to scale as well in JS on cores as

00:28:47,270 --> 00:28:49,360
other tests.

00:28:49,360 --> 00:28:52,750
So I've covered this a little bit, but yeah, this is actually

00:28:52,750 --> 00:28:53,750
design criteria.

00:28:53,750 --> 00:28:54,750
How we did.

00:28:54,750 --> 00:28:55,890
We talked about native-like performance.

00:28:55,890 --> 00:28:57,799
Now we can say -- yes, check.

00:28:57,799 --> 00:28:59,159
We do have native-like performance.

00:28:59,159 --> 00:29:01,370
Especially on certain work loads.

00:29:01,370 --> 00:29:07,420
We're almost where we are in native, on the Unity benchmark.

00:29:07,420 --> 00:29:09,930
Support algorithms and applications on pthreads.

00:29:09,930 --> 00:29:14,460
It's a direct port of the pthread implementation of Unity.

00:29:14,460 --> 00:29:17,500
And we were able to make it with almost no changes to the code

00:29:17,500 --> 00:29:18,500
base.

00:29:18,500 --> 00:29:20,150
I'd like to give myself a pat on the back.

00:29:20,150 --> 00:29:24,310
Think I deserve it?

00:29:24,310 --> 00:29:26,390
Thank you.

00:29:26,390 --> 00:29:30,840
So I talked about some of the reasons why the Unity test

00:29:30,840 --> 00:29:32,360
is significant.

00:29:32,360 --> 00:29:35,230
Not everyone is a game dev, not everyone cares that much

00:29:35,230 --> 00:29:36,230
about games.

00:29:36,230 --> 00:29:37,320
I personally love them.

00:29:37,320 --> 00:29:40,580
I think they're not just fun to play.

00:29:40,580 --> 00:29:45,350
They're fun to develop, and they really challenge software developers and hardware

00:29:45,350 --> 00:29:46,350
developers.

00:29:46,350 --> 00:29:49,450
So I think it's a fantastic test of any infrastructure.

00:29:49,450 --> 00:29:54,100
And we were able to make all this work -- the benchmark is not just functional.

00:29:54,100 --> 00:29:57,410
But we were able to make it work fast.

00:29:57,410 --> 00:30:03,760
So Yuca Yolanki did most of the implementation work on

00:30:03,760 --> 00:30:07,860
making the Unity benchmark functional.

00:30:07,860 --> 00:30:12,600
He worked on the (inaudible) side, and he worked on the JS side.

00:30:12,600 --> 00:30:14,100
He's a Mozillian.

00:30:14,100 --> 00:30:18,820
I asked the team for why is this particular thing significant to them,

00:30:18,820 --> 00:30:23,070
and he offered up this quote, which I would like to share with everyone.

00:30:23,070 --> 00:30:26,460
With shared memory, the web lifts an important implementation with shared

00:30:26,460 --> 00:30:33,100
execution that it had compared to native.

00:30:33,100 --> 00:30:36,960
Shared memory is not comparable to a library call that can be emulated or polyfilled.

00:30:36,960 --> 00:30:40,970
I'm sorry for all the people who are asking this question -- can we polyfill this?

00:30:40,970 --> 00:30:41,970
We cannot.

00:30:41,970 --> 00:30:44,150
But it's a fundamental concept of parallel execution

00:30:44,150 --> 00:30:45,150
architectures.

00:30:45,150 --> 00:30:47,980
I'm not sure if it can get any bigger than this.

00:30:47,980 --> 00:30:53,130
He's a very emphatic guy and he's very excited, but I think he really did capture

00:30:53,130 --> 00:30:57,309
the significance of what we accomplished here.

00:30:57,309 --> 00:31:00,700
So what's next for shared memory and for the work

00:31:00,700 --> 00:31:01,700
that we've done here?

00:31:01,700 --> 00:31:03,340
Well, shared memory is in Nightly right now.

00:31:03,340 --> 00:31:06,940
As on JS, for those that are...

00:31:06,940 --> 00:31:12,290
Can I get another show of hands -- are people familiar with (inaudible)

00:31:12,290 --> 00:31:13,290
JS?

00:31:13,290 --> 00:31:15,080
Most of the room.

00:31:15,080 --> 00:31:18,700
So it's a low level subset of JS that we put together

00:31:18,700 --> 00:31:21,600
that you can really optimize for, and it's fantastic for certain

00:31:21,600 --> 00:31:24,470
workloads, especially cross compilation ones.

00:31:24,470 --> 00:31:31,970
So it's not going to move out of Nightly, until we standardize it.

00:31:31,970 --> 00:31:33,850
But we are in talks to standardize it.

00:31:33,850 --> 00:31:38,490
The API, the set of documents you saw that Lars put together, still subject

00:31:38,490 --> 00:31:39,490
to change.

00:31:39,490 --> 00:31:41,169
We're getting feedback from a lot of sources inside Mozilla and

00:31:41,169 --> 00:31:42,169
out.

00:31:42,169 --> 00:31:45,620
But the good news is Google has actually announced fairly recently

00:31:45,620 --> 00:31:47,690
that they're going to start implementing this too.

00:31:47,690 --> 00:31:51,110
So hopefully a year from now we're going to see this in applications

00:31:51,110 --> 00:31:52,650
built against it everywhere.

00:31:52,650 --> 00:31:56,570
I have a bunch of links that nobody can read,

00:31:56,570 --> 00:32:04,090
but I think there'll be some opportunities to share this presentation later.

00:32:04,090 --> 00:32:07,510
So hopefully you can catch up on any details here that you may have.

00:32:07,510 --> 00:32:08,720
And you're always welcome to find me.

00:32:08,720 --> 00:32:09,759

YouTube URL: https://www.youtube.com/watch?v=h_M_uscOKJM


