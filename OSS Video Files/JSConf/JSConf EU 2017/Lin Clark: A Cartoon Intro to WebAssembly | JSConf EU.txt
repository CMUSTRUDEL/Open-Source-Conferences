Title: Lin Clark: A Cartoon Intro to WebAssembly | JSConf EU
Publication date: 2017-05-16
Playlist: JSConf EU 2017
Description: 
	http://2017.jsconf.eu/speakers/lin-clark-a-cartoon-intro-to-webassembly.html

WebAssembly is fast. It’s being called “the future of the web”. It’s speed and potential have major browser vendors working together to make it a reality. And it’s on it’s way—the MVP hit multiple browsers in October of last year.

But what makes it fast? Starting from the basics, this talk will walk you through what WebAssembly is, and then why it’s fast.
Captions: 
	00:00:03,139 --> 00:00:12,799
Lin Clark - A Cartoon Intro to WebAssembly

00:00:12,799 --> 00:00:15,500
>> All right.

00:00:15,500 --> 00:00:32,015
We are going to wait for a few people to get settled and continue on with our next talk.

00:00:32,015 --> 00:00:33,015
How many people use various different ways to get their information about how things

00:00:33,015 --> 00:00:34,015
work on the internet?

00:00:34,015 --> 00:00:35,015
I know I use comics pretty often, I find them accessible, and I've read Lin Clark's comics.

00:00:35,015 --> 00:00:36,015
Let's hear it for Lin!

00:00:36,015 --> 00:00:37,015
[Applause].

00:00:37,015 --> 00:00:38,015
LIN: Thank you, and hi, everyone.

00:00:38,015 --> 00:00:39,015
I'm Lin Clark and I make code cartoons.

00:00:39,015 --> 00:00:40,015
I work at Mozilla.

00:00:40,015 --> 00:00:45,930
The things like the Rust programming language and Servo and WebAssembly which is what I'm

00:00:45,930 --> 00:00:48,190
going to be talking about today.

00:00:48,190 --> 00:00:55,140
Since this is JSConf, I'm guessing most of you are JavaScript developers, so you know

00:00:55,140 --> 00:00:58,700
that, in JavaScript circles today, there's a lot of hype about WebAssembly.

00:00:58,700 --> 00:01:04,250
People are talking about how blazingly fast it is and how it is going to completely change

00:01:04,250 --> 00:01:06,770
the way we do web development.

00:01:06,770 --> 00:01:10,900
But a lot of these conversations don't go into details about exactly what it is about

00:01:10,900 --> 00:01:15,710
WebAssembly that makes it fast and I hear this kind of rhetoric, but I don't hear the

00:01:15,710 --> 00:01:17,540
details to back it up.

00:01:17,540 --> 00:01:19,579
The inner sceptic in me comes out.

00:01:19,579 --> 00:01:23,390
In this talk, I don't want to tell you about how fast WebAssembly is going to be, but I

00:01:23,390 --> 00:01:28,299
want to help you understand what it is about WebAssembly what makes it fast and in what

00:01:28,299 --> 00:01:31,840
circumstances it is fast.

00:01:31,840 --> 00:01:36,070
But first, what is WebAssembly?

00:01:36,070 --> 00:01:40,400
WebAssembly is a way to run programming languages other than JavaScript in your web pages.

00:01:40,400 --> 00:01:44,909
So, in the past, when you wanted to run code on a web page, you had to use JavaScript.

00:01:44,909 --> 00:01:49,750
If you wanted to change the DOM in response to an event or run a calculation, you were

00:01:49,750 --> 00:01:52,080
using JavaScript.

00:01:52,080 --> 00:01:55,939
With WebAssembly it will be possible to do these things with other languages besides

00:01:55,939 --> 00:01:58,520
JavaScript.

00:01:58,520 --> 00:02:02,380
So when people say that WebAssembly is fast, what they're comparing it to is JavaScript

00:02:02,380 --> 00:02:04,820
- that's the apples to apples comparison.

00:02:04,820 --> 00:02:10,740
Now, I don't want to imply it is an either or decision, you're going to be using WebAssembly

00:02:10,740 --> 00:02:13,800
or you're going to be using JavaScript.

00:02:13,800 --> 00:02:19,190
We think that people will be using these two hand in hand in their applications, but it

00:02:19,190 --> 00:02:24,170
is useful to compare the two, so that you understand what this improved performance

00:02:24,170 --> 00:02:29,050
of code running on the web could mean.

00:02:29,050 --> 00:02:33,410
In order to understand this, let's look at a little bit of performance history of code

00:02:33,410 --> 00:02:36,080
running on the web.

00:02:36,080 --> 00:02:40,870
JavaScript was created in 1995, and it wasn't designed to be fast.

00:02:40,870 --> 00:02:48,819
There are a number of features in JavaScript that make it hard to make it fast, and - types

00:02:48,819 --> 00:02:54,629
where you have a string or an integer, you don't know, that even at runtime, that variable

00:02:54,629 --> 00:02:56,650
could change.

00:02:56,650 --> 00:03:00,810
But these features also make it easy for developers to get up and running with JavaScript really

00:03:00,810 --> 00:03:04,700
quickly, so JavaScript developers accepted this trade-off.

00:03:04,700 --> 00:03:08,340
They accepted that their code was going to run a little bit slower because of this ease

00:03:08,340 --> 00:03:10,629
of use.

00:03:10,629 --> 00:03:16,170
And for the first decade of JavaScript, that was true, that JavaScript was pretty slow,

00:03:16,170 --> 00:03:19,230
and then the browsers started get more competitive.

00:03:19,230 --> 00:03:25,549
And about 2008, a period started called the Performance Wars, the browser vendors started

00:03:25,549 --> 00:03:29,940
improving their JSLint engines to make things of faster.

00:03:29,940 --> 00:03:35,260
The technique they used was introducing JIT compilers to the JavaScript engine, and I

00:03:35,260 --> 00:03:38,730
will explain more about that later.

00:03:38,730 --> 00:03:43,220
Let's look at the impact that the JIT compilers had.

00:03:43,220 --> 00:03:47,650
With the introduction of the JITs, you see an inflection point in the performance of

00:03:47,650 --> 00:03:48,650
JavaScript.

00:03:48,650 --> 00:03:53,790
All of a sudden, JavaScript code was running about ten times faster than it had previously.

00:03:53,790 --> 00:04:00,519
And these performance improvements continued over the next decade.

00:04:00,519 --> 00:04:01,709
With this improved performance.

00:04:01,709 --> 00:04:06,620
You start seeing JavaScript being used for things that you never expected like Node and

00:04:06,620 --> 00:04:09,760
Electron.

00:04:09,760 --> 00:04:13,819
These new applications are possible because of this improvement in performance, because

00:04:13,819 --> 00:04:20,290
of this inflection point ten years ago that we have the applications that we do today.

00:04:20,290 --> 00:04:24,110
That's why it's interesting that we may be approaching another one of these inflection

00:04:24,110 --> 00:04:33,340
points and the speed of code running on the web with WebAssembly [sound cut] to do this,

00:04:33,340 --> 00:04:39,140
I need to explain a little bit where JavaScript spends its time today.

00:04:39,140 --> 00:04:45,140
Here's a diagram of where the JS engine spends its time for an hypothetical app.

00:04:45,140 --> 00:04:48,410
Any app will be different.

00:04:48,410 --> 00:04:50,710
We can use it to build up a mental model.

00:04:50,710 --> 00:04:56,170
You may have seen diagrams like this one before and be confused why there are fewer categories

00:04:56,170 --> 00:04:57,170
in this one.

00:04:57,170 --> 00:05:01,010
I've condensed the number of categories so that is it easier to talk about it.

00:05:01,010 --> 00:05:08,610
These categories are parsing, compiling and optimising, re-optimising, executing the code,

00:05:08,610 --> 00:05:10,590
and garbage collection.

00:05:10,590 --> 00:05:15,430
Now, let's look at what this diagram would look like for WebAssembly.

00:05:15,430 --> 00:05:19,730
You will notice that some of the bars are shorter, and some are missing.

00:05:19,730 --> 00:05:27,200
In this talk, I want to explain what WebAssembly changes, how it makes the amount of time that

00:05:27,200 --> 00:05:33,040
the engine spends in these tasks shorter or gets rid of them altogether.

00:05:33,040 --> 00:05:40,440
But first, let's look at where JS engines would be if we had not introduced the JIT.

00:05:40,440 --> 00:05:44,070
In the early days of JavaScript, this diagram would have looked more like this.

00:05:44,070 --> 00:05:47,870
There was parsing, running the code, and garbage collection.

00:05:47,870 --> 00:05:50,360
We're maybe an execution bar shorter.

00:05:50,360 --> 00:05:56,250
What made that rub faster was the introduction of a JIT, the overhead it added, the compiling

00:05:56,250 --> 00:05:58,810
and optimising.

00:05:58,810 --> 00:06:03,150
Now with WebAssembly, we want to make these bars even shorter in in order to see how we

00:06:03,150 --> 00:06:07,590
can do that, we are going to need to dive into the work that the JIT does.

00:06:07,590 --> 00:06:13,380
I'm going into a quick crashing course of Just In Time compilers.

00:06:13,380 --> 00:06:15,910
This is an overview.

00:06:15,910 --> 00:06:19,680
Different engines have different architectures and those architectures have changed over

00:06:19,680 --> 00:06:24,660
time but most apply to most of them right now.

00:06:24,660 --> 00:06:26,810
This is be review for some of you but I will be quick.

00:06:26,810 --> 00:06:31,300
I want to make sure we are all up to speed on this.

00:06:31,300 --> 00:06:34,810
When you're developing, you have a goal and a problem.

00:06:34,810 --> 00:06:40,200
Your goal is that you want to tell the computer what to do.

00:06:40,200 --> 00:06:44,790
The problem is that you speak a human language and the machine speaks a machine language.

00:06:44,790 --> 00:06:49,270
Even if you don't think of JavaScript as a human language, it really is.

00:06:49,270 --> 00:06:56,300
Because it's been designed for human cognition, not for machine cognition.

00:06:56,300 --> 00:06:59,090
I think of this like the movie Arrival.

00:06:59,090 --> 00:07:03,330
We have aliens and humans trying to communicate with one another.

00:07:03,330 --> 00:07:09,440
It's not as easy as translating word-for-word from one language to the other because the

00:07:09,440 --> 00:07:13,120
two groups actually have different ways of seeing the world, and that's true of humans

00:07:13,120 --> 00:07:14,460
and machines too.

00:07:14,460 --> 00:07:20,090
I will explain more about the differences in the way we think later, but let's look

00:07:20,090 --> 00:07:23,650
at the process of translating.

00:07:23,650 --> 00:07:27,220
In programming, there are generally two ways of translating.

00:07:27,220 --> 00:07:30,460
You can either use an interpreter or a compiler.

00:07:30,460 --> 00:07:35,830
With an interpreter, the translation happens pretty much on the fly, line-by-line.

00:07:35,830 --> 00:07:42,020
A compiler, on the other hand, doesn't translate on the fly.

00:07:42,020 --> 00:07:47,860
It takes time ahead of time to create that translation and then hand it off.

00:07:47,860 --> 00:07:52,360
There are pros and cons to each of these ways of handling this translation.

00:07:52,360 --> 00:07:56,240
So, for an interpreter, some of the pros are that it is quick to get up and running.

00:07:56,240 --> 00:07:59,460
You get that immediate feedback loop.

00:07:59,460 --> 00:08:03,081
So an interpreter seems like a natural fit for something like JavaScript where you want

00:08:03,081 --> 00:08:05,940
the developer to see their progress really quickly.

00:08:05,940 --> 00:08:11,810
And that's why, in the beginning, browsers used JavaScript interpreters, but the trade-off

00:08:11,810 --> 00:08:15,370
is that, when you're doing something like a loop where you have to run the same code

00:08:15,370 --> 00:08:20,340
over and over again, you're doing that translation over and over again.

00:08:20,340 --> 00:08:23,120
The compiler has opposite trade-offs.

00:08:23,120 --> 00:08:26,360
It takes a little bit more time to start up because it has to go through that compilation

00:08:26,360 --> 00:08:27,460
step ahead of time.

00:08:27,460 --> 00:08:31,300
But then you don't incur that translation cost in loops where you're running the code

00:08:31,300 --> 00:08:32,990
over and over again.

00:08:32,990 --> 00:08:38,620
And another difference is that interpreters are running during the execution of the code,

00:08:38,620 --> 00:08:42,520
so they can't take too much time to actually think about how the machine thinks and what

00:08:42,520 --> 00:08:46,110
the optimal way to communicate with the machine is.

00:08:46,110 --> 00:08:49,080
Since compilers are working ahead of time, they can take that little bit of extra time

00:08:49,080 --> 00:08:51,570
and think about how best to communicate with the machine.

00:08:51,570 --> 00:08:56,400
You will hear that referred to as optimisation.

00:08:56,400 --> 00:09:00,800
To get the best of both worlds, browsers mixed compilers in.

00:09:00,800 --> 00:09:08,110
They added a new part of the JavaScript engine called a monitor or a profiler.

00:09:08,110 --> 00:09:10,840
The monitor watches the code as it runs.

00:09:10,840 --> 00:09:15,320
It keeps track of things, like how often a function has been executed.

00:09:15,320 --> 00:09:20,040
At first, the monitor just runs everything through the interpreter.

00:09:20,040 --> 00:09:25,770
If the same function is run a few times, that function is called "warm".

00:09:25,770 --> 00:09:30,210
As a function warms up, it gets it off the baseline compiler to create a compiled version

00:09:30,210 --> 00:09:31,940
of it.

00:09:31,940 --> 00:09:37,170
The baseline compiler will do it in chunks.

00:09:37,170 --> 00:09:41,130
Each operation in the function is going to be compiled to one or more Stubbs.

00:09:41,130 --> 00:09:45,200
So, for example, the plus and equals sign will be an operation.

00:09:45,200 --> 00:09:49,400
The compiler would create a stub for that and the stub would be specific to whatever

00:09:49,400 --> 00:09:53,110
types are being used on either side of that operator.

00:09:53,110 --> 00:10:01,260
So, if the sum in the array element here were integers, it would compile to integer addition.

00:10:01,260 --> 00:10:06,270
If the monitor has set operation again with the same variable time, so with integers again,

00:10:06,270 --> 00:10:09,270
it pulls out the stub it has and uses that.

00:10:09,270 --> 00:10:15,070
If it runs into operation with different variable types, it will create another stub and store

00:10:15,070 --> 00:10:17,150
that one as well.

00:10:17,150 --> 00:10:22,220
As the code runs, more baseline Stubbs for more operations will be filled in and this

00:10:22,220 --> 00:10:26,450
will save on translation time and help speed, up.

00:10:26,450 --> 00:10:29,810
Like I mentioned, there is more a compiler can do.

00:10:29,810 --> 00:10:33,240
It can take some time thinking about how the machine thinks go and how best to think with

00:10:33,240 --> 00:10:37,590
the machine.

00:10:37,590 --> 00:10:43,210
The baseline compiler will make some optimisations, but it doesn't want to take up too much time

00:10:43,210 --> 00:10:47,080
because the code is executing at the same time.

00:10:47,080 --> 00:10:51,890
But if the code is really hot, if it has been run a whole bunch, then it can be worthwhile

00:10:51,890 --> 00:10:55,950
to go through and take the time to make that optimisation.

00:10:55,950 --> 00:11:00,830
So, when a part of the code is very hot, the monitor will send it to the optimising compiler

00:11:00,830 --> 00:11:05,910
and this will create another even faster version of that function.

00:11:05,910 --> 00:11:10,040
In order to make the faster version of the function, the optimising compiler has to make

00:11:10,040 --> 00:11:12,280
some assumptions.

00:11:12,280 --> 00:11:15,760
For example, if it can assume that all of the objects that are created by a particular

00:11:15,760 --> 00:11:21,440
constructor had the same shape, so the object has the same property names, and they've been

00:11:21,440 --> 00:11:27,510
added in the same order, then it can cut some corners based on that.

00:11:27,510 --> 00:11:32,120
So the optimising compiler uses the information that the monitor has been gathering to make

00:11:32,120 --> 00:11:33,120
these judgments.

00:11:33,120 --> 00:11:37,220
If something has been true for all previous passes through the code, then it assumes it's

00:11:37,220 --> 00:11:40,220
going to continue to be true.

00:11:40,220 --> 00:11:44,730
Of course, with JavaScript there are never any guarantees.

00:11:44,730 --> 00:11:50,131
You could have 99 objects that all have the same shape but then the 100th object has a

00:11:50,131 --> 00:11:54,220
different property, or a property has been deleted on it.

00:11:54,220 --> 00:11:58,230
So the compiled code needs to check before it runs to see whether the assumptions are

00:11:58,230 --> 00:12:01,910
valid, and if they are, then the compiled code runs.

00:12:01,910 --> 00:12:08,170
But if not, the JIT assumes it made the wrong assumptions and trashes the optimised code.

00:12:08,170 --> 00:12:13,900
At this point the it goes back to the compiled version and this is called de-optimisation

00:12:13,900 --> 00:12:17,290
or bailing out.

00:12:17,290 --> 00:12:21,620
Usually optimising compilers will save you time, they will actually make the code run

00:12:21,620 --> 00:12:23,230
faster.

00:12:23,230 --> 00:12:29,500
But if you have code that keeps gets optimised and then gets bailed out on and then gets

00:12:29,500 --> 00:12:34,500
optimised again, if you get into the cycles, it can actually take more time than it would

00:12:34,500 --> 00:12:38,730
have just running through the baseline compiled version of the videoed.

00:12:38,730 --> 00:12:43,030
So a jot of JITs will keep track of how many times they've tried to optimise a function,

00:12:43,030 --> 00:12:48,610
and if it keeps not working out, then will he will mark it as don't even try optimising

00:12:48,610 --> 00:12:51,960
this again.

00:12:51,960 --> 00:12:54,029
So that is the JIT in a nutshell.

00:12:54,029 --> 00:12:57,940
Code starts off running in an interpreter and the monitor collects information about

00:12:57,940 --> 00:12:58,940
it.

00:12:58,940 --> 00:13:03,100
Then it will send code off to be compiled depending how often that part of the code

00:13:03,100 --> 00:13:06,100
is being run.

00:13:06,100 --> 00:13:11,251
Now that we understand more about the work that the JavaScript engine is doing, let's

00:13:11,251 --> 00:13:17,240
look at ways to maybe make this execution go a little faster.

00:13:17,240 --> 00:13:21,650
One way would be to get rid of some of the overhead, so we can move some of this ahead

00:13:21,650 --> 00:13:24,050
of time.

00:13:24,050 --> 00:13:26,520
But in order to do that, we would need to get rid of the dynamic types.

00:13:26,520 --> 00:13:30,990
If we are going to be optimising ahead of time, we need the types to be explicit in

00:13:30,990 --> 00:13:35,790
the code, because we aren't going to be monitoring it at runtime and see what types are running

00:13:35,790 --> 00:13:36,790
through it.

00:13:36,790 --> 00:13:41,550
These dynamic types that can change at runtime are a problem.

00:13:41,550 --> 00:13:47,520
I already suggested that is what made JavaScript successful: the dynamic types help developers

00:13:47,520 --> 00:13:49,790
get up and running quickly.

00:13:49,790 --> 00:13:54,250
Why we would want to change something that made JavaScript successful?

00:13:54,250 --> 00:13:58,210
I want to be clear here that we don't have to change anything in JavaScript to take advantage

00:13:58,210 --> 00:14:03,170
of the benefits of WebAssembly, but there is a change that's already happening which

00:14:03,170 --> 00:14:08,730
we can take advantage of, and that is the move towards modularity.

00:14:08,730 --> 00:14:13,910
Over the past few years, both with PHM and the 2015 module expect, JavaScript has become

00:14:13,910 --> 00:14:22,100
a more methodised ecosystem, and the nice thing about modules is they provide boundaries.

00:14:22,100 --> 00:14:25,540
You don't really need to know about the inner details of a module that you're depending

00:14:25,540 --> 00:14:30,960
on, so these modules, they could compiled ahead of time using a language that doesn't

00:14:30,960 --> 00:14:35,440
have these flexible types that JavaScript does, and it wouldn't affect how you code.

00:14:35,440 --> 00:14:42,480
Take, for example, React which has a lot of different consumers.

00:14:42,480 --> 00:14:48,180
The React core team has already been working on making their reconciliation algorithm faster.

00:14:48,180 --> 00:14:55,130
An option for them would be to write the new reconciliation algorithm and something like

00:14:55,130 --> 00:14:59,340
C and then compile it ahead of time.

00:14:59,340 --> 00:15:04,220
But as long as they keep the API the same, consumers of React actually wouldn't notice

00:15:04,220 --> 00:15:05,220
this.

00:15:05,220 --> 00:15:10,630
When they update the code, the only thing they would notice is any performance improvements.

00:15:10,630 --> 00:15:14,550
So this is what WebAssembly does: it makes it possible for library authors and application

00:15:14,550 --> 00:15:19,150
developers to code in languages that are more consistently conformant, but then to have

00:15:19,150 --> 00:15:23,160
that code run on the web like JavaScript does and to integrate with existing JavaScript.

00:15:23,160 --> 00:15:28,310
This means that you will be able to benefit from WebAssembly without having to understand

00:15:28,310 --> 00:15:34,000
it or why it's fast, but I always find it more rewarding when I do understand that stuff.

00:15:34,000 --> 00:15:38,870
So I'm going ahead and walk you through how WebAssembly works.

00:15:38,870 --> 00:15:42,350
In order to do that, I'm going to have through another crash course, this time in assembly

00:15:42,350 --> 00:15:44,279
and compilers.

00:15:44,279 --> 00:15:50,540
I talked about how communicating with the machine is like communicating with an alien.

00:15:50,540 --> 00:15:54,310
I want to take a look now at how that alien brain works.

00:15:54,310 --> 00:15:58,360
How the communication that is coming into it gets parsed and understood.

00:15:58,360 --> 00:16:06,310
There is a part of this alien brain that is dedicated to the thinking - like adding, subtracting,

00:16:06,310 --> 00:16:07,310
and logic.

00:16:07,310 --> 00:16:10,430
There is also a part of the brain near that which is the short-term memory.

00:16:10,430 --> 00:16:14,480
Those parts are pretty close together in the same part of the brain.

00:16:14,480 --> 00:16:20,080
Then there are some longer-term memory.

00:16:20,080 --> 00:16:22,810
These different parts have different names.

00:16:22,810 --> 00:16:29,050
So the part that does the thinking is the earth, medic, and logic unit, the ALIO.

00:16:29,050 --> 00:16:30,730
The short-term memory, those are called registers.

00:16:30,730 --> 00:16:36,120
That is encapsulated in the central processing unit, or the CPO.

00:16:36,120 --> 00:16:41,330
The longer term memory, that's random access memory or RAM.

00:16:41,330 --> 00:16:46,660
Each part of the short-term memory has a name and this makes it easy for the brain to understand

00:16:46,660 --> 00:16:51,070
what it should be working on at any given time.

00:16:51,070 --> 00:16:53,750
The sentence is in machine instructions.

00:16:53,750 --> 00:17:00,810
When a sentence gets into the brain, it gets split in a way that means different things.

00:17:00,810 --> 00:17:05,430
The way the sentence will be split up will be very specific to the wiring of this particular

00:17:05,430 --> 00:17:06,939
brain.

00:17:06,939 --> 00:17:10,740
For example, this brain might take the fourth through the tenth bit and pipe it through

00:17:10,740 --> 00:17:16,730
the ALIO and based on where there are ones and zeroes, the ALIO will figure out what

00:17:16,730 --> 00:17:21,240
it is supposed to do for this instruction.

00:17:21,240 --> 00:17:26,160
Then the brain would take the next two chunks to figure out what it needs to do that operation

00:17:26,160 --> 00:17:32,290
on and these will be the addresses of registers.

00:17:32,290 --> 00:17:36,980
You will see I've been adding annotations above the machine code here, which makes it

00:17:36,980 --> 00:17:40,070
easier for us as humans to know what is going on with this machine code.

00:17:40,070 --> 00:17:44,600
That is what the assembly is - symbolic machine code.

00:17:44,600 --> 00:17:50,910
It is a way of human beings being able to read and understand machine code.

00:17:50,910 --> 00:17:55,290
You can see here there is a one-to-one relationship between the assembly and the machine code

00:17:55,290 --> 00:17:58,060
for this machine.

00:17:58,060 --> 00:18:01,010
Something you might have figured out from that is that you actually have a different

00:18:01,010 --> 00:18:06,930
kind of assembly for each kind of wiring you have for a machine.

00:18:06,930 --> 00:18:10,290
Any time that you have a different architecture inside of a machine, any time there's a different

00:18:10,290 --> 00:18:15,540
kind of brain in the machine, there's a good chance it will have its own assembly.

00:18:15,540 --> 00:18:18,990
So we're not talking about the targets this translation just being one thing, just being

00:18:18,990 --> 00:18:24,260
one kind of machine code, it is many different kinds of machine code.

00:18:24,260 --> 00:18:28,890
Just as we speak different languages as humans, machines speak different languages.

00:18:28,890 --> 00:18:35,530
So, if we are talking human to alien translation, you may be going from English or Russian,

00:18:35,530 --> 00:18:43,170
or Mandarin to alien language A or alien language B. In programming terms, this is like going

00:18:43,170 --> 00:18:46,970
from C plus, CLL or Rust to ARM.

00:18:46,970 --> 00:18:53,170
If you want to go down to the high-level programming languages down to assembly languages, you're

00:18:53,170 --> 00:18:57,960
going to have to create a whole bunk of different translators.

00:18:57,960 --> 00:18:59,910
That would be pretty inefficient.

00:18:59,910 --> 00:19:02,990
Most compilers put at least one layer in between.

00:19:02,990 --> 00:19:07,270
The compiler will take the high-lex programming language and translate it down to something

00:19:07,270 --> 00:19:14,230
that's not quite as high level but not as low level as machine code.

00:19:14,230 --> 00:19:17,480
And this is called an intermediate representation.

00:19:17,480 --> 00:19:20,990
The compiler will take any one of the higher level programming languages and go down to

00:19:20,990 --> 00:19:25,750
the single intermediate representation and go from the intermediate representation to

00:19:25,750 --> 00:19:28,390
any one of the assembly languages.

00:19:28,390 --> 00:19:31,730
The thing that goes from the higher level programming language to the intermediate representation

00:19:31,730 --> 00:19:36,380
is called the front-end; anything that goes from the intermediate representation down

00:19:36,380 --> 00:19:41,370
to the assembly is called the back-end.

00:19:41,370 --> 00:19:45,000
Now where does WebAssembly fit into this picture?

00:19:45,000 --> 00:19:52,050
You might think that it is one of these target assembly languages which is kind of true except

00:19:52,050 --> 00:19:56,660
that each one of those languages corresponded to a particular architecture, and when you're

00:19:56,660 --> 00:20:00,360
delivering code across the web, you don't actually know what architecture you're going

00:20:00,360 --> 00:20:02,270
to be running on.

00:20:02,270 --> 00:20:05,500
So WebAssembly's a little bit different from normal assembly.

00:20:05,500 --> 00:20:12,600
It is a machine language for a conceptual machine, not an actual physical machine.

00:20:12,600 --> 00:20:16,950
Once the browser downloads the WebAssembly, it can make the short hop between the WebAssembly

00:20:16,950 --> 00:20:22,450
code and the actual assembly code for that particular architecture.

00:20:22,450 --> 00:20:26,560
Let's walk through the tools that a developer of a library like React would use to make

00:20:26,560 --> 00:20:29,730
their code WebAssembly.

00:20:29,730 --> 00:20:34,240
The compiler has a lot of work to go into it for web assembly called LLVM.

00:20:34,240 --> 00:20:39,250
There are a number of different frontends and backends.

00:20:39,250 --> 00:20:48,120
If we wanted to go from C to WebAssembly we might use Clang taking us down to the representation,

00:20:48,120 --> 00:20:54,420
and once the code is in the intermediate representation, LLVM can do some optimisation for us because

00:20:54,420 --> 00:20:56,560
it understands it at that point.

00:20:56,560 --> 00:21:00,510
Then we want to go from the intermediate representation down to WebAssembly.

00:21:00,510 --> 00:21:07,800
There is a process that will go all the way from LLVM to WebAssembly but you might not

00:21:07,800 --> 00:21:16,480
want to use it until it's fully finished and there is another tool which has a FA Cup finished

00:21:16,480 --> 00:21:21,350
WebAssembly backend, using a fork of LLVM under the hood.

00:21:21,350 --> 00:21:26,429
Even when the LLVM back-up is done, you might want to use some script in to compile your

00:21:26,429 --> 00:21:27,429
code at present.

00:21:27,429 --> 00:21:33,640
It can be useful to pack in useful libraries, things like a file system that works on top

00:21:33,640 --> 00:21:36,230
of index.db.

00:21:36,230 --> 00:21:43,670
Regardless of whether you're using LLVM or sciptum together, the end resulted is .wazm

00:21:43,670 --> 00:21:46,900
for web assembly.

00:21:46,900 --> 00:21:51,790
This can be loaded in JavaScript.

00:21:51,790 --> 00:21:55,540
Right now, the way that you load it in JavaScript is a little bit complicated.

00:21:55,540 --> 00:21:57,660
We're making that easier.

00:21:57,660 --> 00:22:03,500
Webpack has plans to work on it and other module owners plan to work on it.

00:22:03,500 --> 00:22:09,429
Once the browser has a built-in module support, WebAssembly can use that too.

00:22:09,429 --> 00:22:13,980
It should be as easy as loading a JavaScript module.

00:22:13,980 --> 00:22:16,650
When I say that, though, I should add a caveat.

00:22:16,650 --> 00:22:21,250
Loading a WebAssembly module should be as he's an as loading a JavaScript 1 but working

00:22:21,250 --> 00:22:25,590
with it is going to be a little bit different.

00:22:25,590 --> 00:22:30,050
Let's say you're calling a WebAssembly function from JavaScript, and this is the JavaScript

00:22:30,050 --> 00:22:31,230
function.

00:22:31,230 --> 00:22:35,190
And this is the WebAssembly function.

00:22:35,190 --> 00:22:39,620
Functions in WebAssembly can only take WebAssembly types as parameters, and at the moment, that's

00:22:39,620 --> 00:22:45,880
numbers, so integers, floats, that's what you're working with.

00:22:45,880 --> 00:22:49,920
That's different from regular JavaScript modules.

00:22:49,920 --> 00:22:52,640
And the same restriction applies to return values as well.

00:22:52,640 --> 00:22:57,120
But what if you want to be able to return a string?

00:22:57,120 --> 00:22:59,110
You can't do it.

00:22:59,110 --> 00:23:04,290
For any data types that are more complex, you need to put them in the WebAssembly module's

00:23:04,290 --> 00:23:07,430
memory.

00:23:07,430 --> 00:23:09,100
So this memory is an array buffer.

00:23:09,100 --> 00:23:12,960
It is just a JavaScript object that simulates a heap.

00:23:12,960 --> 00:23:17,750
The integers that get passed back and forth can be used kind of like pointers into this

00:23:17,750 --> 00:23:19,360
heap.

00:23:19,360 --> 00:23:23,410
So the C code can use that to write to the memory as if it were an address and then the

00:23:23,410 --> 00:23:27,330
JavaScript can use that number to figure out the array index that it needs to pull the

00:23:27,330 --> 00:23:30,210
value from.

00:23:30,210 --> 00:23:34,230
It's likely that anybody who's developing a WebAssembly module for developers is going

00:23:34,230 --> 00:23:38,320
to create a wrapper around it so you don't actually need to know about that.

00:23:38,320 --> 00:23:42,950
I think it helps to understand the performance characteristics, understanding how the memory

00:23:42,950 --> 00:23:44,070
works.

00:23:44,070 --> 00:23:50,620
What I want to do now is go back to the diagram and look at what it is about WebAssembly that

00:23:50,620 --> 00:23:52,490
can make things run faster.

00:23:52,490 --> 00:23:57,810
So, first off, this isn't actually shown in the diagram, but it can take less time to

00:23:57,810 --> 00:24:04,590
download WebAssembly than JavaScript because it is more compact.

00:24:04,590 --> 00:24:10,190
It was designed specifically to be compact, and it can also be translated into a binary

00:24:10,190 --> 00:24:15,520
form, even though JavaScript is pretty small, if you have equivalent code in WebAssembly,

00:24:15,520 --> 00:24:19,580
it is likely it will be smaller.

00:24:19,580 --> 00:24:22,580
Parsing takes less time than JavaScript too.

00:24:22,580 --> 00:24:27,070
JavaScript needs to be parsed from the source into an abstract syntax tree and then usually

00:24:27,070 --> 00:24:35,760
converted into an intermediate engine called bytecode.

00:24:35,760 --> 00:24:37,500
WebAssembly is already a bytecode.

00:24:37,500 --> 00:24:44,850
It just needs to be decoded from that binary version, and decoding is faster than parsing.

00:24:44,850 --> 00:24:48,169
Compiling takes less time, because a lot of it has been done ahead of time before the

00:24:48,169 --> 00:24:50,750
file was even put up to the server.

00:24:50,750 --> 00:24:54,110
Plus the compiler doesn't have to dominate pile those multiple baseline Stubbs that it

00:24:54,110 --> 00:24:56,750
was doing before for the dynamic types.

00:24:56,750 --> 00:25:04,110
And you don't get into the optimise and de-optimisation cycles that you did with the JIT.

00:25:04,110 --> 00:25:09,429
Running your code is fast because many of the optimisations that JIT makes to JavaScript

00:25:09,429 --> 00:25:12,990
just aren't necessary with WebAssembly.

00:25:12,990 --> 00:25:17,309
Plus WebAssembly itself provides many instructions that are just faster.

00:25:17,309 --> 00:25:22,211
Human programmers don't need to program WebAssembly directly so that means its designers can create

00:25:22,211 --> 00:25:27,919
something closer to how machines think so depending on what kind of code your code is

00:25:27,919 --> 00:25:34,790
doing, these instructions can run anywhere to 800 per cent faster.

00:25:34,790 --> 00:25:40,280
As for bar engage collection, the manuals now use memory management.

00:25:40,280 --> 00:25:41,280
This is likely to change.

00:25:41,280 --> 00:25:43,640
I will explain more about that later.

00:25:43,640 --> 00:25:47,910
For now, you don't need to worry about garbage collection.

00:25:47,910 --> 00:25:51,850
So what is the status of WebAssembly right now?

00:25:51,850 --> 00:25:55,740
In late February, the browser vendors announced that WebAssembly was ready to ship all by

00:25:55,740 --> 00:25:58,179
defaults in browsers.

00:25:58,179 --> 00:26:02,820
We started ship it on by default in Firefox the next week, and then Chrome did the week

00:26:02,820 --> 00:26:07,560
after that, and it's in preview versions in Edge and Safari.

00:26:07,560 --> 00:26:12,220
With this, developers can start shipping WebAssembly code for.

00:26:12,220 --> 00:26:17,230
For earlier versions of browsers that don't support WebAssembly, you can ship down an

00:26:17,230 --> 00:26:18,230
as JS version.

00:26:18,230 --> 00:26:20,179
It is the precursor to web assembly.

00:26:20,179 --> 00:26:23,210
It is fully JS.

00:26:23,210 --> 00:26:27,860
What is in browsers is the MVP - the minimum viable product.

00:26:27,860 --> 00:26:31,799
The MVP doesn't contain all the features that the community group wants but with it, WebAssembly

00:26:31,799 --> 00:26:33,919
is reasonably fast and usable.

00:26:33,919 --> 00:26:37,760
However, it should get even faster in the future through a combination of fixes in the

00:26:37,760 --> 00:26:40,160
engines and new features in the spec.

00:26:40,160 --> 00:26:45,100
For example, a fix that needs to happen in Firefox specifically is that currently calling

00:26:45,100 --> 00:26:50,340
a web assembly function in JS code is slower than it needs to be because of something called

00:26:50,340 --> 00:26:51,340
trampolining.

00:26:51,340 --> 00:26:56,910
Instead of the JIT knowing how to deal with WebAssembly code it has to go through a transfer

00:26:56,910 --> 00:27:01,559
function controlling from JavaScript to WebAssembly.

00:27:01,559 --> 00:27:06,590
This is a lot slower than it would if the JIT knew how to handle this function itself.

00:27:06,590 --> 00:27:07,590
Slower is relative.

00:27:07,590 --> 00:27:09,940
We're only talking nanoseconds here.

00:27:09,940 --> 00:27:14,110
But if you have lots of back-and-forth communication between WebAssembly and JavaScript, you can

00:27:14,110 --> 00:27:16,790
notice that.

00:27:16,790 --> 00:27:20,010
So that's the kind of fix that you can expect in the engine.

00:27:20,010 --> 00:27:24,130
As for the spec, there are a number of features that are coming soon.

00:27:24,130 --> 00:27:27,990
One that is expected reasonably soon is threading.

00:27:27,990 --> 00:27:31,110
One way to speed up the code is to make it possible for different parts of the code to

00:27:31,110 --> 00:27:36,130
run at the same time in parallel, but this can staples backfire since the overhead of

00:27:36,130 --> 00:27:41,140
communication between threads can take up more than time it would have to just run that

00:27:41,140 --> 00:27:42,700
all sequentially.

00:27:42,700 --> 00:27:47,300
But if you share memory between the threads, it reduces this overhead.

00:27:47,300 --> 00:27:52,650
To do this, WebAssembly will use the new shared array buffer that's being shipped?

00:27:52,650 --> 00:27:54,070
Browsers shortly.

00:27:54,070 --> 00:27:58,679
Once that is in place in browsers, the community group can start specifying how WebAssembly

00:27:58,679 --> 00:28:00,440
will use it.

00:28:00,440 --> 00:28:05,030
On feature in a needs to be standardised is direct ARM access.

00:28:05,030 --> 00:28:12,890
Currently, there's no way to interact it the DOM doing element.htmls.

00:28:12,890 --> 00:28:19,400
Instead, you have to go through JS to set that value.

00:28:19,400 --> 00:28:24,370
The community group is currently working on adding DOM support, though.

00:28:24,370 --> 00:28:30,630
One last feature that has a lot of folks excited is integration with the browser's bar engage

00:28:30,630 --> 00:28:31,630
collection.

00:28:31,630 --> 00:28:36,910
So, today, you can ship down your own garbage collector with code if you want to but is

00:28:36,910 --> 00:28:45,620
slow for a few reasons and the community group isn't making it possible for WebAssembly code

00:28:45,620 --> 00:28:51,490
to be used with just the built-in GC which is a highly optimised one that the browsers

00:28:51,490 --> 00:28:55,340
have been working on, so it will run fast and you will have that integration.

00:28:55,340 --> 00:28:58,710
Unfortunately, that's all I have time talk about today, so I'm going to have to wrap

00:28:58,710 --> 00:29:05,059
it up I had a fantastic technical review on this from Luke Wagner.

00:29:05,059 --> 00:29:11,530
He is the person who came up with the way to add types in azm.js and did a lot of the

00:29:11,530 --> 00:29:13,080
work to push WebAssembly forward.

00:29:13,080 --> 00:29:19,790
He is with us today and will be doing a Q and A about WebAssembly in the Mozilla space

00:29:19,790 --> 00:29:20,790
about lunch.

00:29:20,790 --> 00:29:26,559
Feel free to ask us questions, orb you can ask on Twitter or we will both be at the party

00:29:26,559 --> 00:29:27,559
tonight.

00:29:27,559 --> 00:29:29,570
Thank you to him.

00:29:29,570 --> 00:29:30,570
And thank you all for listening.

00:29:30,570 --> 00:29:31,570
[Cheering].

00:29:31,570 --> 00:29:32,570
[Applause].

00:29:32,570 --> 00:29:33,570
>> Thank you for the fantastic talk.

00:29:33,570 --> 00:29:34,570
We're going to continue right on to the next talk.

00:29:34,570 --> 00:29:35,570
If anybody wants to move to the side track, there is going to be a great talk about sharing

00:29:35,570 --> 00:29:37,120
is caring, patterns for JavaScript library design, and give us a minute to set up and

00:29:37,120 --> 00:29:39,640

YouTube URL: https://www.youtube.com/watch?v=HktWin_LPf4


