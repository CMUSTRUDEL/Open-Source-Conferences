Title: Emily Gorcenski: The Ethics of the Internet of Things | JSConf EU 2017
Publication date: 2017-05-14
Playlist: JSConf EU 2017
Description: 
	http://2017.jsconf.eu/speakers/emily-gorcenski-the-ethics-of-the-internet-of-things.html

The Internet of Things is quickly entering our lives with stunning feats of innovationâ€“and sometimes absurdity. Internet connectivity in household and industrial devices truly holds a great deal of promise, but is the pace of innovation too fast and is the design praxis sufficiently rigorous? Is JavaScript well-suited to control devices that can cause material and physical harm? By way of case study, this talk explores ethical issues with IoT technologies and transfers lessons learned from biomedical, automotive, and aerospace engineering industries.
Captions: 
	00:00:03,100 --> 00:00:05,303
Emily Gorcenski - The Ethics of the Internet of Things

00:00:29,890 --> 00:00:30,900
EMILY: Thank you all very much.

00:00:30,900 --> 00:00:34,789
I'm going to talk about the ethics of the Internet of Things and I promise I'm not going

00:00:34,789 --> 00:00:37,960
to lecture you too much.

00:00:37,960 --> 00:00:38,960
I'm Emily Gorcenski.

00:00:38,960 --> 00:00:40,309
I'm on Twitter.

00:00:40,309 --> 00:00:43,440
I say there things sometimes.

00:00:43,440 --> 00:00:50,859
I like the Internet of Things and the landscape it is creating.

00:00:50,859 --> 00:00:53,859
So why am I talking about the Internet of Things at a JavaScript conference?

00:00:53,859 --> 00:00:56,839
Why am I talking about ethics at a JavaScript conference?

00:00:56,839 --> 00:01:02,020
It's because I've given this talk a few times and I joke that every time I give it, I can

00:01:02,020 --> 00:01:06,979
give a brand new talk because there are so many issues, there are so many failures, and

00:01:06,979 --> 00:01:11,659
bugs, and security issues that come up so freakily, so that if I just focus on case

00:01:11,659 --> 00:01:15,509
studies, it will be a brand new talk every time.

00:01:15,509 --> 00:01:19,170
This time, I've decided that I don't want to give this talk any more.

00:01:19,170 --> 00:01:23,630
I want you all to be able to give this talk, so I want to talk a little bit more generally

00:01:23,630 --> 00:01:28,430
about why ethics matters and why is should matter to you as a JavaScript developer and

00:01:28,430 --> 00:01:38,070
how we can put technology into all sorts of devices and services where it doesn't normally

00:01:38,070 --> 00:01:40,200
belong.

00:01:40,200 --> 00:01:43,790
It's almost impossible to do an ethics talk without getting into heavy stuff.

00:01:43,790 --> 00:01:45,369
There are contents warnings about this talk.

00:01:45,369 --> 00:01:51,770
We will talk frankly about some incidents that resulted in injuries and death of people.

00:01:51,770 --> 00:01:56,520
A discussion of a specific instance of sexual assault and an image of raw meat.

00:01:56,520 --> 00:02:04,020
If that spooks you out, that will be maybe about ten minutes into the talk.

00:02:04,020 --> 00:02:05,020
Who am I?

00:02:05,020 --> 00:02:07,100
I have a little bit of a confession.

00:02:07,100 --> 00:02:08,789
I'm kind of an imposter here.

00:02:08,789 --> 00:02:16,800
I'm not a JavaScript developer.

00:02:16,800 --> 00:02:21,950
I'm a data scientist and I'm trained as a mathematician but also an engineer.

00:02:21,950 --> 00:02:27,140
I went to school for aeronautical engineering and mechanical engineering, and I have worked

00:02:27,140 --> 00:02:33,490
in my career in the aerospace, biotech, and now I work in the finance industries.

00:02:33,490 --> 00:02:39,190
And what these industries have in common is that they're all heavily regulated, and most

00:02:39,190 --> 00:02:44,410
of the people working in them subscribe to a professional code of ethics either through

00:02:44,410 --> 00:02:53,620
an independent society or some other organisation that guides what ethical conduct mean.

00:02:53,620 --> 00:02:57,180
Here I am, I worked in defence, health care, and banking, and I'm going to talk to you

00:02:57,180 --> 00:03:02,120
about ethics - buckle in!

00:03:02,120 --> 00:03:04,880
When I talk about the Internet of Things, what is it that I mean?

00:03:04,880 --> 00:03:06,510
It's kind of a wishy-warranty definition.

00:03:06,510 --> 00:03:11,540
We might think about smart fridges or smart cars, that sort of thing.

00:03:11,540 --> 00:03:15,210
I like to think of it as putting the internet where it doesn't normally belong.

00:03:15,210 --> 00:03:17,700
So it could be smart appliances.

00:03:17,700 --> 00:03:24,540
I think of Uber as an IoT taxi.

00:03:24,540 --> 00:03:30,400
When we look at ethics of that, we have to look at the entire scope of what are we doing

00:03:30,400 --> 00:03:32,640
with our technology and what are we connecting?

00:03:32,640 --> 00:03:38,760
The difference isn't that these devices and products or services haven't been computerised,

00:03:38,760 --> 00:03:44,150
it is we're letting the consumer have connectivity to what is going on.

00:03:44,150 --> 00:03:49,540
So, if you're a JavaScript developer, maybe you want an bread machine so you can hack

00:03:49,540 --> 00:03:52,060
your code while the bread bakes.

00:03:52,060 --> 00:03:58,450
This is important because IoT products are the next level of convenience optimisation.

00:03:58,450 --> 00:04:03,390
We've spent the last 30 years optimising products for convenience, and there's not much more

00:04:03,390 --> 00:04:07,090
competitive advantage that you can get in a refrigerator nowadays.

00:04:07,090 --> 00:04:11,280
So if you don't have a competitive advantage with a non-connected device, you have to go

00:04:11,280 --> 00:04:12,280
connected.

00:04:12,280 --> 00:04:17,950
This is also important for people whose livelihoods are affected by disability.

00:04:17,950 --> 00:04:24,250
You might be concerned about the surveillance capabilities of IoT devices or the horrible

00:04:24,250 --> 00:04:29,940
things that Uber has been accused of doing, but if you're not able to get around or don't

00:04:29,940 --> 00:04:35,400
live in a place where there is an easy taxi service and you have other needs, something

00:04:35,400 --> 00:04:38,470
like Uber is a life -saver and changes your life.

00:04:38,470 --> 00:04:49,540
We can't write it off as an absurdity, say IoT is frivolous, we have the internet Twitter

00:04:49,540 --> 00:04:54,630
account and there are a lot of misses and there but there are a lot of good things that

00:04:54,630 --> 00:04:57,180
come out of IoT as well.

00:04:57,180 --> 00:05:06,310
When I talk about ethics, what is it I mean when I say that word?

00:05:06,310 --> 00:05:07,870
You've probably seen this diagram.

00:05:07,870 --> 00:05:12,440
The framing goes there is one Nobel Laureate tied to five set of tracks and another tied

00:05:12,440 --> 00:05:13,990
to another.

00:05:13,990 --> 00:05:18,630
Somehow, you have been put in the position of pulling the lever.

00:05:18,630 --> 00:05:22,880
This is a really popular problem on the internet right now because it feels like something

00:05:22,880 --> 00:05:26,900
we can solve with category theory if we just abstract it enough, and, two, it really makes

00:05:26,900 --> 00:05:30,470
for some dank memes.

00:05:30,470 --> 00:05:35,720
The thing about the trolley problem is the trolley problem wasn't a problem for trolleys,

00:05:35,720 --> 00:05:39,290
so why is it a problem for self-driving cars?

00:05:39,290 --> 00:05:47,670
We love to frame things as puzzles to solve - that is our nature as developers and engineers.

00:05:47,670 --> 00:05:52,150
In tech, we don't actually face ethical dilemmas that often.

00:05:52,150 --> 00:05:56,480
Ethical dilemmas happen when there are two competing ethical frameworks, and an action

00:05:56,480 --> 00:06:02,120
that you take cannot not be in violation of at least one of them.

00:06:02,120 --> 00:06:06,330
What I think is fascinating about JavaScript is that the JavaScript community is responsible

00:06:06,330 --> 00:06:12,169
for what I consider to be the most fascinating true ethical dilemma in a decade of technology.

00:06:12,169 --> 00:06:14,110
I will get to you on that later.

00:06:14,110 --> 00:06:17,790
Some might know what I'm talking about already.

00:06:17,790 --> 00:06:22,500
The issue of technology is that often we just don't act with ethics.

00:06:22,500 --> 00:06:28,020
I don't mean this as an indictment saying you're bad, unethical, immoral human beings.

00:06:28,020 --> 00:06:33,960
There are some companies out there that will get a side eye right now, but I mean in our

00:06:33,960 --> 00:06:36,950
industry, we don't have a professional code.

00:06:36,950 --> 00:06:40,480
There are some societies that you can join, but raise your hand if you're a member of

00:06:40,480 --> 00:06:42,639
like ACM or I888.

00:06:42,639 --> 00:06:49,210
There are a few out there but it is not the majority.

00:06:49,210 --> 00:06:54,450
In practice, ethics are about things: they are about the analysis of harm and the mitigation

00:06:54,450 --> 00:06:55,450
of risk.

00:06:55,450 --> 00:07:01,501
So, when we talk about acting ethically, especially in something like research ethics, what we

00:07:01,501 --> 00:07:05,990
are doing is we're not trying to eliminate the possibility of somebody getting hurt,

00:07:05,990 --> 00:07:11,820
but we are trying to understand all of the ways that somebody might be hurt by our technology,

00:07:11,820 --> 00:07:17,120
and we're looking for what actions we can take to mitigate the chance of that happening,

00:07:17,120 --> 00:07:23,400
to mitigate the severity of it when it happens, and to provide remediation when it inevitably

00:07:23,400 --> 00:07:24,400
does.

00:07:24,400 --> 00:07:28,730
So this is what we need to bid up as our ethical framework when we are developing technology

00:07:28,730 --> 00:07:31,650
particularly for IoT.

00:07:31,650 --> 00:07:34,639
So harm can happen in three ways: ment first is through malfeasance.

00:07:34,639 --> 00:07:37,940
This is the most common topic in IoT.

00:07:37,940 --> 00:07:38,940
This is security.

00:07:38,940 --> 00:07:42,090
This is people talking about hacking.

00:07:42,090 --> 00:07:48,730
When the Miri botnet had a DS attack last fall, it was the biggest issue, the big I

00:07:48,730 --> 00:07:54,270
est DOS attack witnessed happened through IoT devices that were unsecured and you know

00:07:54,270 --> 00:07:57,620
that IoT security is in a pretty abysmal state right now.

00:07:57,620 --> 00:08:04,050
When this happened, the timing of it and the way that it was structured gave a lot of people

00:08:04,050 --> 00:08:09,360
a lot of concerns that it was a precursor to an attack on the US presidential election,

00:08:09,360 --> 00:08:14,729
and that would be an attempt to influence the outcome of that election as it turns out,

00:08:14,729 --> 00:08:19,180
the fears were unfounded - we managed to screw that one up all by ourselves - because I don't

00:08:19,180 --> 00:08:21,310
want to talk about security in this talk.

00:08:21,310 --> 00:08:30,621
First, I can't cover everything; succeeded, the other ways that harm can happen, if we

00:08:30,621 --> 00:08:35,329
address those, we address the security issues.

00:08:35,329 --> 00:08:37,779
Failures are failures in bugs and software.

00:08:37,779 --> 00:08:43,430
Cases happen when a device is operating under normal operating circumstances, but it gets

00:08:43,430 --> 00:08:46,670
put into a condition that we did not predict as developers.

00:08:46,670 --> 00:08:55,639
It is also worth mentioning that sometimes we like to treat ... difference except for

00:08:55,639 --> 00:08:57,649
semantics.

00:08:57,649 --> 00:09:00,550
So a great example of this, this was on Twitter.

00:09:00,550 --> 00:09:09,160
This poor gentleman, Andrew, had an IoT water cooler, and his TLS certificate expired, which

00:09:09,160 --> 00:09:13,420
led to some blocking code which meant that a hardware interlock failed and he has water

00:09:13,420 --> 00:09:15,939
all over his house.

00:09:15,939 --> 00:09:17,470
This is a real issue, right?

00:09:17,470 --> 00:09:19,339
This is a problem.

00:09:19,339 --> 00:09:26,050
If a TLS certificate expire in a web service, like in a web space, and we forget the TLS

00:09:26,050 --> 00:09:32,240
certificate and we have a blocking code, we have ops people to deal with that.

00:09:32,240 --> 00:09:38,879
We can't treat IoT devices like cattle any more, we have to treat them like pets that

00:09:38,879 --> 00:09:43,190
live in people's homes and get very, very angry when they don't get fed.

00:09:43,190 --> 00:09:53,610
One day, if we're not careful, we are going to put JavaScript into, I don't know, an IoT

00:09:53,610 --> 00:09:58,050
kettle and light somebody's house on fire because "undefined" is not a function.

00:09:58,050 --> 00:10:06,390
I stole that joke, by the way which is total payback because he didn't save me MPM socks!

00:10:06,390 --> 00:10:08,800
This is something that I did, I'm very proud of, this.

00:10:08,800 --> 00:10:13,410
I did this a couple of years ago am I did full screen on this if I can, if I can figure

00:10:13,410 --> 00:10:19,899
out where my mouse is - there we go.

00:10:19,899 --> 00:10:25,499
This is a Microsoft band - and I'm not picking on Microsoft at all here - and this is a piece

00:10:25,499 --> 00:10:27,870
of raw chicken.

00:10:27,870 --> 00:10:32,480
I didn't do horrible to a chicken like have a zombie chicken out there, this is a piece

00:10:32,480 --> 00:10:35,459
of meat that I bought from the grocery score.

00:10:35,459 --> 00:10:40,819
It is reading a heart rate of 120 beats per minute!

00:10:40,819 --> 00:10:43,179
[Laughter].

00:10:43,179 --> 00:10:48,699
In the real world, sensors are messy, they're noisy, they're imperfect.

00:10:48,699 --> 00:10:51,990
And so, when we are designing for IoT, we have to take this into consideration.

00:10:51,990 --> 00:10:57,279
It is absurd that you can read a heart rate off a piece of chicken breast, but this has

00:10:57,279 --> 00:11:01,760
deep, deep ramifications for a lot of things.

00:11:01,760 --> 00:11:06,829
For one, there are colleges out there that are mandating students wear FitBits.

00:11:06,829 --> 00:11:13,189
There are employers out there that have health insurance incentive programs for doing this.

00:11:13,189 --> 00:11:18,980
If you're following what is happening with American health care, now we have this issue

00:11:18,980 --> 00:11:23,730
where we have surveillance devices that are monitoring our health and can report on pre-existing

00:11:23,730 --> 00:11:24,839
conditions.

00:11:24,839 --> 00:11:30,540
This is not actually hypothetical, this is something that's really happened.

00:11:30,540 --> 00:11:42,970
Let me get out of full-screen mode if I can.

00:11:42,970 --> 00:11:49,230
2015, a woman was visiting a co-worker.

00:11:49,230 --> 00:11:52,970
She pulled police to report a sexual assault.

00:11:52,970 --> 00:11:57,209
When the police investigated they found her fit bit, and with her permission, they analysed

00:11:57,209 --> 00:11:59,930
the data.

00:11:59,930 --> 00:12:06,389
When they analysed the data, not only did they drop the investigation into her claims,

00:12:06,389 --> 00:12:10,459
but they turned around and they charged her with making false statements to the police,

00:12:10,459 --> 00:12:19,579
and, last year, she completed guilty to those charges and was convicted and put on probation.

00:12:19,579 --> 00:12:25,490
The prosecuting attorney said that the FitBit data sealed the deal.

00:12:25,490 --> 00:12:30,139
I can pull 120 beats per minute off a piece of raw chicken and a woman's life is ruined

00:12:30,139 --> 00:12:35,959
because nobody at FitBit stood up and said no, our devices are not this accurate.

00:12:35,959 --> 00:12:41,630
You can't do that.

00:12:41,630 --> 00:12:45,319
Our devices bear false witness against us, and they can.

00:12:45,319 --> 00:12:51,699
The problem is there is no regulation, quality assurance or standards for how we build them.

00:12:51,699 --> 00:12:53,089
We just ship code.

00:12:53,089 --> 00:12:54,410
We ship hardware.

00:12:54,410 --> 00:12:58,100
We innovate, fast, fast, fast.

00:12:58,100 --> 00:13:03,850
We don't ask ourselves what kinds of harm can happen when this goes wrong?

00:13:03,850 --> 00:13:06,680
And this is happening increasingly often.

00:13:06,680 --> 00:13:10,519
These devices are being used in criminal and civil investigations.

00:13:10,519 --> 00:13:15,379
Just last week, CNN reported that a man is being charged with murder of his wife based

00:13:15,379 --> 00:13:22,019
on the FitBit data saying she had travelled a certain distance and that distance didn't

00:13:22,019 --> 00:13:25,420
correlate with his story.

00:13:25,420 --> 00:13:30,249
Anybody that is knitted, for example, while wearing one will know that it will record

00:13:30,249 --> 00:13:34,449
steps while you're sitting on your couch.

00:13:34,449 --> 00:13:37,959
How can we let this happen?

00:13:37,959 --> 00:13:42,209
How can we let this information affect people's lives?

00:13:42,209 --> 00:13:46,649
Another incident: a smart water metre was used in a murder investigation last year.

00:13:46,649 --> 00:13:53,189
In the same investigation, they also filed a warrant for Amazon Echo data.

00:13:53,189 --> 00:13:55,649
The question is: who's going to go to jail?

00:13:55,649 --> 00:14:00,139
Who's going to get put on probation when a device makes false statements to the police?

00:14:00,139 --> 00:14:04,610
Moreover, say something happens, say something breaks and somebody gets hurt or somebody

00:14:04,610 --> 00:14:05,610
gets killed?

00:14:05,610 --> 00:14:09,350
Who's going to be liable if that device causes an accident?

00:14:09,350 --> 00:14:12,639
Is it the owner?

00:14:12,639 --> 00:14:14,860
Is it the developer?

00:14:14,860 --> 00:14:17,069
The company that made it?

00:14:17,069 --> 00:14:21,269
And this seems like it should be a settled question, but it is actually not.

00:14:21,269 --> 00:14:23,329
And this is has already happened.

00:14:23,329 --> 00:14:27,730
In this frame, you will see the vehicle, the white vehicle on the right is a Google self-driving

00:14:27,730 --> 00:14:33,269
car, and this image is a still image - this is a screen capture from a video taken from

00:14:33,269 --> 00:14:42,499
the dashcam of a municipal bus in Mountain View California.

00:14:42,499 --> 00:14:47,240
That Google SUV is about to pull out in front of the bus and get in an accident.

00:14:47,240 --> 00:14:49,439
Thankfully, nobody was hurt in this.

00:14:49,439 --> 00:14:52,239
There were no injuries, just a fender-bender.

00:14:52,239 --> 00:14:57,379
This is the first time that the self-driving car has ever been found responsible for causing

00:14:57,379 --> 00:15:00,809
an accident.

00:15:00,809 --> 00:15:01,899
Google fessed up to.

00:15:01,899 --> 00:15:03,589
They said, "You know what?

00:15:03,589 --> 00:15:05,449
Our bad.

00:15:05,449 --> 00:15:08,550
We will take care of the damages."

00:15:08,550 --> 00:15:14,180
And they investigated what happened, and they concluded that the car predicted that the

00:15:14,180 --> 00:15:18,550
bus would yield to us because we were ahead of it.

00:15:18,550 --> 00:15:23,699
Okay, now Google is in a position right now where they want to ship self-driving cars,

00:15:23,699 --> 00:15:27,660
so of course they're going to assume liability for this because they don't want to test it

00:15:27,660 --> 00:15:29,470
in court.

00:15:29,470 --> 00:15:32,109
But we can't rely on that as we go into the IoT future.

00:15:32,109 --> 00:15:39,179
We can't rely on benevolent corporations to assume liability once this goes out at scale.

00:15:39,179 --> 00:15:43,160
By the way, even if Google was right this is still going to be an historical moment

00:15:43,160 --> 00:15:47,720
because, if the bus had yielded to the vehicle, it would be the first time that a municipal

00:15:47,720 --> 00:15:54,839
bus has ever yielded!

00:15:54,839 --> 00:15:59,480
A few years ago, there's a judge in San Francisco, and this is part of a research project, not

00:15:59,480 --> 00:16:06,069
part of a case, he looked into the question of whether autonomous systems fall under existing

00:16:06,069 --> 00:16:07,649
theories of liability.

00:16:07,649 --> 00:16:15,449
And in looking into it, he found that vehicles that make our own decisions, that use things

00:16:15,449 --> 00:16:23,519
like neural nets that use adaptive self-adjusting control systems, smart systems, if you will,

00:16:23,519 --> 00:16:28,639
devising their own means to attain a task may not be subject to liability under any

00:16:28,639 --> 00:16:31,749
existing theories of tort.

00:16:31,749 --> 00:16:35,850
And this has huge implications.

00:16:35,850 --> 00:16:42,850
Because if you buy a normal refrigerator and it breaks, you can say, "Hey, manufacturer,

00:16:42,850 --> 00:16:44,019
you're responsible for that break."

00:16:44,019 --> 00:16:49,250
If you buy a coffee-maker and it burns down your house because of a defective unit, you

00:16:49,250 --> 00:16:55,689
can get out safely and then you recover damages from the company and your insurance company

00:16:55,689 --> 00:16:57,009
takes care of it.

00:16:57,009 --> 00:17:00,990
There's a whole ethical framework that's built up around this.

00:17:00,990 --> 00:17:03,910
And there's a legal structure that's there as well.

00:17:03,910 --> 00:17:07,809
Obviously, self-driving cars are going to be safer, they're going to save lives.

00:17:07,809 --> 00:17:09,620
That is a very important thing.

00:17:09,620 --> 00:17:10,699
We want to save lives.

00:17:10,699 --> 00:17:12,900
We want the roads to be better.

00:17:12,900 --> 00:17:17,829
But the number of lives saved is not the only term in our ethical calculus.

00:17:17,829 --> 00:17:21,079
We have to look at what happened when people get injured?

00:17:21,079 --> 00:17:22,689
How are they taken care of?

00:17:22,689 --> 00:17:27,720
How are they able to pay the medical bills or get back to work, or miss work biochemical

00:17:27,720 --> 00:17:34,470
they are recovering but still be able to pay rent and afford food?

00:17:34,470 --> 00:17:41,070
So the question about this is what does this mean for us as developers?

00:17:41,070 --> 00:17:44,970
Like, does this give us a free pass?

00:17:44,970 --> 00:17:47,519
We're not liable for IoT devices.

00:17:47,519 --> 00:17:48,860
That means we can ship, right?

00:17:48,860 --> 00:17:49,880
We can do whatever.

00:17:49,880 --> 00:17:57,370
Let's just innovate the hell out of everything until something breaks, and there is a precedent,

00:17:57,370 --> 00:17:58,370
right?

00:17:58,370 --> 00:18:00,750
Is that really the legacy that we want to leave behind?

00:18:00,750 --> 00:18:05,090
Do we want to leave the legacy behind of we did it because we could and we didn't give

00:18:05,090 --> 00:18:07,760
a damn about who we hurt?

00:18:07,760 --> 00:18:12,940
Some companies are doing this.

00:18:12,940 --> 00:18:18,330
Some companies are actually still working in the space where they just want to innovate,

00:18:18,330 --> 00:18:22,850
and they just want to build things, and ship things, and they will deal with the consequences

00:18:22,850 --> 00:18:23,850
later.

00:18:23,850 --> 00:18:27,169
But you have to ask yourself: do I want to be responsible for that?

00:18:27,169 --> 00:18:28,750
That's what ethics is all about.

00:18:28,750 --> 00:18:34,200
Now, I said that the JavaScript community had one of the most fascinating ethical incidents

00:18:34,200 --> 00:18:41,130
in technology in a while, and that's the left-pad incident.

00:18:41,130 --> 00:18:44,790
I don't know why it is pink, but whatever.

00:18:44,790 --> 00:18:49,040
When Ashley talked about left-pad last night, it was really fascinating because she focused

00:18:49,040 --> 00:18:51,220
on a lot of the backlash.

00:18:51,220 --> 00:18:53,990
She said the internet blew up when left pad happened.

00:18:53,990 --> 00:18:59,889
People were angry about a lot of things, the way the JavaScript community developed a small-module

00:18:59,889 --> 00:19:04,190
system and maybe it is wrong or maybe it is right and there's a lot of argument back and

00:19:04,190 --> 00:19:10,120
forth and friendships were lost or damaged in this incident.

00:19:10,120 --> 00:19:17,299
And what people didn't realise was that the reason for all of this anger and acrimony

00:19:17,299 --> 00:19:22,130
was because the left-pad incident actually exposed what is a true ethical dilemma, and

00:19:22,130 --> 00:19:29,830
we just didn't see the forest for the trees at the moment, because left-pad had two competing

00:19:29,830 --> 00:19:33,050
ethical decisions.

00:19:33,050 --> 00:19:39,230
The first is the hacker culture ethic, that openness is the most important thing, that

00:19:39,230 --> 00:19:46,549
openness is a virtue, and that the ability to control your code is tantamount to being

00:19:46,549 --> 00:19:48,990
a hanger, to being an open-source developer.

00:19:48,990 --> 00:19:52,740
Sure, other people can fork it, but you're going to choose tout it.

00:19:52,740 --> 00:19:57,260
When the left-has the broke all his modules and broke a bunch of stuff on the internet,

00:19:57,260 --> 00:20:03,059
MPM had a competing framework that they have a responsibility to the people who use their

00:20:03,059 --> 00:20:04,059
product.

00:20:04,059 --> 00:20:06,659
They have a responsibility as engineers.

00:20:06,659 --> 00:20:08,169
They also value openness.

00:20:08,169 --> 00:20:13,139
They're an open-source community.

00:20:13,139 --> 00:20:19,870
So this was a very hard decision, and that's why there were so many heads being butt over

00:20:19,870 --> 00:20:22,470
the decision that was made.

00:20:22,470 --> 00:20:26,909
Could you imagine what would have happened if this didn't happen in 2016 when it mostly

00:20:26,909 --> 00:20:28,559
affected the web?

00:20:28,559 --> 00:20:35,409
But rather in, I don't know, 2018, 2020 when MPM is running on people's cars, people's

00:20:35,409 --> 00:20:37,399
refrigerators?

00:20:37,399 --> 00:20:42,110
Somebody pulls down a module, and now, all of a sudden, you're driving on the highway

00:20:42,110 --> 00:20:46,720
at 70 miles an hour and your car shuts off and something goes wrong.

00:20:46,720 --> 00:20:49,620
You think that couldn't ever happen.

00:20:49,620 --> 00:20:55,039
Nobody would ever actually do live deployments on a car running an IoT device running in

00:20:55,039 --> 00:20:56,039
the field.

00:20:56,039 --> 00:20:59,919
Please, like we're doing it in production systems right now.

00:20:59,919 --> 00:21:02,390
IoT security is a mess.

00:21:02,390 --> 00:21:04,750
We're doing this rapid innovation pace.

00:21:04,750 --> 00:21:07,030
Of course there's going to be issues.

00:21:07,030 --> 00:21:11,500
And you don't want to be the person that's responsible for somebody's refrigerator going

00:21:11,500 --> 00:21:14,669
out and they lose all their food or maybe they lose their important medicine that needs

00:21:14,669 --> 00:21:15,669
refrigeration.

00:21:15,669 --> 00:21:19,539
You don't want to be responsible for that - or maybe you do.

00:21:19,539 --> 00:21:23,121
Maybe you think that the virtue of openness is the more important ethic and that is a

00:21:23,121 --> 00:21:25,769
true ethical dilemma.

00:21:25,769 --> 00:21:28,190
So what do we do as jeers?

00:21:28,190 --> 00:21:32,220
What are the takeaways that we can have when we talk about ethics?

00:21:32,220 --> 00:21:39,370
This is kind of why I wanted to not give this talk and rather empower you to be able to

00:21:39,370 --> 00:21:48,110
give this talk, because there are actionable things that we can do as engineers, as developers,

00:21:48,110 --> 00:21:51,120
to make our workspaces better, to act with more ethics.

00:21:51,120 --> 00:21:54,640
The first would be to set expectations with your boss.

00:21:54,640 --> 00:22:00,770
If you know what pressures your boss has, if they ask you to do something you don't

00:22:00,770 --> 00:22:08,010
feel comfortable with, you need to know, can I go to my boss and I don't feel comfortable

00:22:08,010 --> 00:22:09,010
with this?

00:22:09,010 --> 00:22:13,059
Can you go to your manager and say, "I have concerns over this?"

00:22:13,059 --> 00:22:16,120
Do you know what process will happen if you do that?

00:22:16,120 --> 00:22:18,250
That is an important thing.

00:22:18,250 --> 00:22:23,009
You also have to be prepared to say no.

00:22:23,009 --> 00:22:31,809
If somebody comes to you and says, "Hey, I need you to build in this method that sends

00:22:31,809 --> 00:22:38,460
tracking data on somebody's heart rate back to our server in real time," are you comfortable

00:22:38,460 --> 00:22:39,760
doing that?

00:22:39,760 --> 00:22:41,180
Maybe you're not.

00:22:41,180 --> 00:22:45,149
But do you know how to refuse an order?

00:22:45,149 --> 00:22:51,049
Are you willing to refuse an order, to put your career in jeopardy for doing so if it

00:22:51,049 --> 00:22:54,580
goes against something that you believe in?

00:22:54,580 --> 00:22:59,139
You need also to be able to hold frank discussions with your co-workers about what this means.

00:22:59,139 --> 00:23:04,520
I work in finance, and I'm a data scientist, so we have a vast amount of data on people,

00:23:04,520 --> 00:23:10,799
and a vast amount of capability to do things with that data, and so, we talk often, my

00:23:10,799 --> 00:23:17,640
team, about the implications of what we are doing when we record customer data, when we

00:23:17,640 --> 00:23:19,710
record information about their finances.

00:23:19,710 --> 00:23:23,240
We talk all the time, like what are you not willing to do?

00:23:23,240 --> 00:23:25,850
What are we legally obligated to do?

00:23:25,850 --> 00:23:32,529
In finance, we have legal obligations in terms of reporting fraud and in terms of looking

00:23:32,529 --> 00:23:34,549
for money-laundering, for example.

00:23:34,549 --> 00:23:36,490
So we have to talk about these things with each other.

00:23:36,490 --> 00:23:41,140
And, as engineers, you should be able to talk frankly with your co-workers like, "I don't

00:23:41,140 --> 00:23:43,230
like where this is going."

00:23:43,230 --> 00:23:45,440
How do we make sure it doesn't go there?

00:23:45,440 --> 00:23:50,350
How do we make sure it stays on the safe side and not the dangerous side?

00:23:50,350 --> 00:23:54,809
The most important thing is to know your limit.

00:23:54,809 --> 00:23:58,700
It's to know when you're willing to talk away.

00:23:58,700 --> 00:24:02,809
Because tech is really lucrative and we have a lot of privilege.

00:24:02,809 --> 00:24:04,980
We have a lot of privilege in tech.

00:24:04,980 --> 00:24:09,910
Even just look around the space that we are in, this is a remarkable conference in a remarkable

00:24:09,910 --> 00:24:15,799
space, and there are amenities and all sorts of decadence here.

00:24:15,799 --> 00:24:18,769
Not all industries are like this.

00:24:18,769 --> 00:24:24,289
What is your limit where you would be willing to say, "I can no longer in good conscience

00:24:24,289 --> 00:24:27,090
continue to do this"?

00:24:27,090 --> 00:24:29,179
And go do something different.

00:24:29,179 --> 00:24:33,850
If you don't know what that limit is, you're not going to discover that you're over it

00:24:33,850 --> 00:24:36,299
until it is too late.

00:24:36,299 --> 00:24:37,299
That's all I have.

00:24:37,299 --> 00:24:38,299
Thank you very much.

00:24:38,299 --> 00:24:39,299
[Applause].

00:24:39,299 --> 00:24:40,299
[Cheering].

00:24:40,299 --> 00:24:41,299
>> Thank you, Emily.

00:24:41,299 --> 00:24:42,299
That was a spectacular talk.

00:24:42,299 --> 00:24:43,299
I think you touched on a lot of really important points for the current ethical questions we

00:24:43,299 --> 00:24:44,299
should be asking.

00:24:44,299 --> 00:24:45,299
We're going to get set up with the next speaker.

00:24:45,299 --> 00:24:45,799

YouTube URL: https://www.youtube.com/watch?v=xLL7Fo_em2E


