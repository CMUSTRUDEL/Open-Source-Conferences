Title: Nidin Vinayakan: Shared Memory & Parallel Programming with JavaScript | JSConf EU 2017
Publication date: 2017-05-26
Playlist: JSConf EU 2017
Description: 
	http://2017.jsconf.eu/speakers/nidin-vinayakan-shared-memory-parallel-programming-with-javascript.html

Early look in to the upcoming JavaScript features SharedArrayBuffer and Atomics. I will show you, how to craft complex applications using shared memory and gain massive performance boost by utilizing all available CPU cores for parallel computation on same data. I will also teach you how to write safe code while using shared memory and avoiding deadlocks of web workers. I will introduce a custom TypeScript like language called TurboScript for easy parallel programming which will generate efficient asm.js like JavaScript. Finally I will do a live demo of an app created using shared memory and TurboScript “A Global Illumination Ray Tracing Renderer for Three.js”
Captions: 
	00:00:03,100 --> 00:00:06,640
Nidin Vinayakan - Shared Memory & Parallel Programming with JavaScript

00:00:06,640 --> 00:00:22,400
>> So we just had a panel discussion on the TV39 which is writing the living standard

00:00:22,400 --> 00:00:27,740
of JavaScript, and one of the features that came out in the most recent spec was shared

00:00:27,740 --> 00:00:29,310
array buffers and shared memory.

00:00:29,310 --> 00:00:31,100
Please give a warm welcome to Nidin.

00:00:31,100 --> 00:00:32,100
[Applause].

00:00:32,100 --> 00:00:33,100
NIDIN: Good evening, everyone.

00:00:33,100 --> 00:00:34,100
My name is Nidin Vinayakan.

00:00:34,100 --> 00:00:35,579
I'm from India.

00:00:35,579 --> 00:00:38,649
I'm mainly working on web technologies.

00:00:38,649 --> 00:00:47,519
Right now, you can find me on Twitter - maybe some of you know me, like this thumbnail.

00:00:47,519 --> 00:00:50,239
I'm on Twitter.

00:00:50,239 --> 00:00:58,230
Right now, I'm working in a start-up company and we are trying to sell cars online, so

00:00:58,230 --> 00:01:01,769
you can buy cars online with a mouse click.

00:01:01,769 --> 00:01:05,920
If you don't like it, you can return it - yes.

00:01:05,920 --> 00:01:13,080
So you will know we have web workers, and it is enabled mull think threading at some

00:01:13,080 --> 00:01:21,340
extent, so, for example, if you have a memory in worker 1, and you have a memory in worker

00:01:21,340 --> 00:01:28,049
2, if you want to serve this data, you can serve one bit by one using force - that's

00:01:28,049 --> 00:01:31,180
not the efficient way.

00:01:31,180 --> 00:01:37,109
Another thing you can actually post is the memory to the worker 2, then it will clone,

00:01:37,109 --> 00:01:42,890
which actually copies the memory 1, and the memory block and creates a new memory, so,

00:01:42,890 --> 00:01:48,109
in the memory, it will take like two blocks of memory.

00:01:48,109 --> 00:01:51,649
Another thing is you can transfer this memory.

00:01:51,649 --> 00:01:56,360
The problem is that it is fast, there is no cloning, just transfer the memory.

00:01:56,360 --> 00:02:03,369
The problem it is de-attached from the CPU1 and no more axis.

00:02:03,369 --> 00:02:11,830
If you have an object which is crazy, it will be serialised and be serialised during this

00:02:11,830 --> 00:02:19,040
clone which is expensive for a real time shared memory - for the part of the programming,

00:02:19,040 --> 00:02:21,640
this is not an option.

00:02:21,640 --> 00:02:23,530
Yes, there's good news.

00:02:23,530 --> 00:02:36,280
We are now at ES18, and there is a new feature - shared memory and Atomics by Lars T Hansen,

00:02:36,280 --> 00:02:45,900
so we have a new RPA or buffer - so, when you create a buffer it will allocate a memory

00:02:45,900 --> 00:02:51,070
block in the RAM and you can work with it, but the problem is you cannot share it.

00:02:51,070 --> 00:02:56,870
To share the RA buffer is the same but the underlying memories shared between all the

00:02:56,870 --> 00:02:58,920
workers.

00:02:58,920 --> 00:03:03,580
So the API is similar.

00:03:03,580 --> 00:03:12,360
It's the already shared between workers, so just post it, so will transfer the reference

00:03:12,360 --> 00:03:17,630
to this memory to all the workers, and all the workers can act from the same memory,

00:03:17,630 --> 00:03:28,700
read and write to this memory.

00:03:28,700 --> 00:03:30,700
Let's do some executions.

00:03:30,700 --> 00:03:36,060
We're actually counting 200 million numbers.

00:03:36,060 --> 00:03:40,920
How long does this program take?

00:03:40,920 --> 00:03:43,700
Time for 200 million iterations.

00:03:43,700 --> 00:03:46,260
How can we make it faster?

00:03:46,260 --> 00:03:50,890
Run iterations parallel.

00:03:50,890 --> 00:03:58,520
It is hypothetical how we can run it in two CPUs.

00:03:58,520 --> 00:04:00,080
We are expecting still above two.

00:04:00,080 --> 00:04:01,920
Actually, it is slower.

00:04:01,920 --> 00:04:02,920
Why?

00:04:02,920 --> 00:04:20,459
Because we forgot this little thing because the CPU has its own cache and there is the

00:04:20,459 --> 00:04:28,030
L1 cache and if you want to access the memory to copy to L1 cache, so if you're doing a

00:04:28,030 --> 00:04:33,760
computation only in one thread it is faster because all the memories are immediately available

00:04:33,760 --> 00:04:37,460
in the L1 cache so the accessing is really fast.

00:04:37,460 --> 00:04:44,590
If you want to share operation between workers, you need to go through these things.

00:04:44,590 --> 00:04:55,710
Also there's another set of problems, because it can cause data data arrays, then relevant

00:04:55,710 --> 00:05:08,560
to CPU1, maybe CPU 2 has already fetched this data and implemented by one and it writes

00:05:08,560 --> 00:05:10,200
to the memory.

00:05:10,200 --> 00:05:12,950
But CPU1 doesn't know it.

00:05:12,950 --> 00:05:16,530
What will CPU 1 do?

00:05:16,530 --> 00:05:19,300
Increment by one and write to the memory.

00:05:19,300 --> 00:05:28,500
At the end, you will get only one, but should get two, because there is no co-ordination

00:05:28,500 --> 00:05:30,910
between CPU 1 and CPU2.

00:05:30,910 --> 00:05:35,410
For that, we new a new set of APIs, called Atomics.

00:05:35,410 --> 00:05:41,860
We can actually look at one particular line of memory access.

00:05:41,860 --> 00:05:48,290
During the log, CPU cannot reading or write that operation - CP U2.

00:05:48,290 --> 00:05:55,200
We lock the memory, we load it and we added and stored it to this memory and released

00:05:55,200 --> 00:05:56,200
the log.

00:05:56,200 --> 00:06:02,200
Then CP U2 can implement and read and write to this memory using log - or without log.

00:06:02,200 --> 00:06:07,630
If it is not logging, the same problem might happen.

00:06:07,630 --> 00:06:23,740
The new API atomics, only available in ... so we cannot do ... use when you do this Atomic

00:06:23,740 --> 00:06:29,610
add in 3201, at the end, you get the expected result.

00:06:29,610 --> 00:06:45,590
Let's do two small ... . This 

00:06:45,590 --> 00:06:50,550
is our symbol program, counting, so threats count in single thread.

00:06:50,550 --> 00:06:57,460
So take around 400 milliseconds to count these 200 million numbers.

00:06:57,460 --> 00:06:59,669
Then count in parallel.

00:06:59,669 --> 00:07:03,630
It takes longer.

00:07:03,630 --> 00:07:07,970
The - it it's not 200 million.

00:07:07,970 --> 00:07:10,560
If you run it again, it is different.

00:07:10,560 --> 00:07:16,020
Each time you run, it is different data because there's a data race.

00:07:16,020 --> 00:07:21,930
It is not coordinated, they will write depending on the hardware, depending on the cycle, how

00:07:21,930 --> 00:07:23,320
the CPU is busy.

00:07:23,320 --> 00:07:28,110
The CPU will schedule this in sections independently.

00:07:28,110 --> 00:07:32,270
We can run with Atomics.

00:07:32,270 --> 00:07:39,510
It is quite slow because it is doing its kind of serial because it is logging the memory

00:07:39,510 --> 00:07:47,139
and adding it, so during that lock state, the CPU1 cannot add it, so, yes.

00:07:47,139 --> 00:07:52,669
You will get the expected result, but it takes like 13 seconds.

00:07:52,669 --> 00:07:54,150
That's ridiculous.

00:07:54,150 --> 00:07:58,650
It is actually very, very slower than single thread.

00:07:58,650 --> 00:08:17,060
That problem is is I will show the code.

00:08:17,060 --> 00:08:25,500
Formerly accessing this Atomics add any time, like 200 million times using Atomics.add because

00:08:25,500 --> 00:08:28,540
it is logging 200 million times.

00:08:28,540 --> 00:08:36,500
That's very, very slow.

00:08:36,500 --> 00:08:40,769
Like the simple optimisation.

00:08:40,769 --> 00:08:46,380
This is very fast.

00:08:46,380 --> 00:08:49,010
You also get the expected result.

00:08:49,010 --> 00:08:54,829
Then, when you run this single trip again, so, when you run a single trip again, it is

00:08:54,829 --> 00:09:00,550
actually much faster now because it is actually V8 is optimising, it is 50 per cent faster

00:09:00,550 --> 00:09:04,500
now and then you come back with these 400.

00:09:04,500 --> 00:09:13,709
So, in the optimised version, what I'm doing is, because accessing memory is expensive.

00:09:13,709 --> 00:09:24,899
When you distribute some data, so, I'm actually using a local count, then I'm actually counting

00:09:24,899 --> 00:09:32,010
locally, because I have eight threads, and I tribute this 200 million counting to the

00:09:32,010 --> 00:09:36,459
eight threads, so 200 million divided by eight.

00:09:36,459 --> 00:09:41,559
So, in this loop, it only uses the registers, like this log.

00:09:41,559 --> 00:09:50,260
The count is a variable so it will store it to the CPU register, then during in foreloop,

00:09:50,260 --> 00:09:54,819
the access and implementing it's very fast.

00:09:54,819 --> 00:10:04,329
After counting the local news, only using one Atomic.add, that's why it's faster.

00:10:04,329 --> 00:10:12,720
If you're really doing any real power programming, yes, the Atomics is very good, but you should

00:10:12,720 --> 00:10:33,110
not do it if you don't use it, or if you don't really need to synchronise the data.

00:10:33,110 --> 00:10:50,980
So I was working on a GGS render engine so, using web gel it is very hard to mimic the

00:10:50,980 --> 00:10:52,360
physics of light.

00:10:52,360 --> 00:11:01,490
It is very hard to mimic these global illumination, and realistic shadows, and soft shadows, so

00:11:01,490 --> 00:11:13,339
I develop a CPU version of red racing engine using shared memory.

00:11:13,339 --> 00:11:26,290
This is a standard testing model, stand for dragon.

00:11:26,290 --> 00:11:30,050
The Stanford dragon.

00:11:30,050 --> 00:11:38,889
I need to built the tree, �is building a parallel, 2.8 milliseconds and I can start

00:11:38,889 --> 00:11:40,029
rat racing.

00:11:40,029 --> 00:11:41,029
Okay.

00:11:41,029 --> 00:11:46,350
It is actually rat racing in real time in JavaScript.

00:11:46,350 --> 00:11:54,649
So, it is actually splitting the scene into, or imagine to eight styles, and it is rat

00:11:54,649 --> 00:11:58,089
racing eight styles at a time.

00:11:58,089 --> 00:12:02,339
For JavaScript, that is quite fast in this case.

00:12:02,339 --> 00:12:10,980
I can actually turn around.

00:12:10,980 --> 00:12:36,620
When you toggle the versions, you can see the shadows are very nice, and also there

00:12:36,620 --> 00:12:43,079
is colour bleeding here, the red one, and also green colours here, which is very hard

00:12:43,079 --> 00:12:54,860
to mimic using web gel which is a rasterising web - it is flattening all the 3D data into

00:12:54,860 --> 00:12:55,959
2D.

00:12:55,959 --> 00:13:03,939
In this rat racing, we have a shooting race into the scene, and I'm intersecting into

00:13:03,939 --> 00:13:09,809
the 3D model and depending on the normality of the flags and try to find out the accurate

00:13:09,809 --> 00:13:10,809
colour.

00:13:10,809 --> 00:13:16,639
So I left it running longer, we would get a realistic image.

00:13:16,639 --> 00:13:34,459
So there is another three.js demo scene, it is opt data with materials and textures.

00:13:34,459 --> 00:13:38,850
So, this is vanilla JavaScript.

00:13:38,850 --> 00:13:44,029
And it was really tough developing something like this, because I have only shared memories,

00:13:44,029 --> 00:13:45,949
and it is a linear memory.

00:13:45,949 --> 00:13:52,499
Like there is no structure, or I cannot copy objects, so I need to put everything into

00:13:52,499 --> 00:13:53,499
shared memory.

00:13:53,499 --> 00:13:59,610
It is very linear, so I need to write objects into this memory, so doing it manually is

00:13:59,610 --> 00:14:13,939
like, it is not sailable, and it's not - if there is a bug, it is hard to detect.

00:14:13,939 --> 00:14:17,470
I have an object, and the object has some properties.

00:14:17,470 --> 00:14:25,299
I cannot write or read directly to the shared memory, because it is a JavaScript object.

00:14:25,299 --> 00:14:34,819
What I need to do is serialise the kind of end-code this object into normal binaries

00:14:34,819 --> 00:14:41,889
like object ID and property value, so writing directly into this memory.

00:14:41,889 --> 00:14:50,630
It is really not optimal, because for a little better project, it is not an option.

00:14:50,630 --> 00:14:53,470
So I need something like okay an object.

00:14:53,470 --> 00:14:58,730
I'm using Typescript now so I have an object, and I need to do this automatically.

00:14:58,730 --> 00:15:02,329
What is the solution?

00:15:02,329 --> 00:15:17,370
I didn't find any good solution, so I created a compiler, called Turbo - when we create

00:15:17,370 --> 00:15:26,199
a task in the script, it will be a shared memory and then a pointer, so I can work with

00:15:26,199 --> 00:15:29,110
the pointer.

00:15:29,110 --> 00:15:33,389
So yeah, the turbo script looked like this.

00:15:33,389 --> 00:15:40,730
So, for example, it is a vector, and the vector has x, y, z, so, for you, it looks like normal

00:15:40,730 --> 00:15:42,480
typescript.

00:15:42,480 --> 00:15:50,720
The only difference is you need to delete this allocated memory manually, like C++ so

00:15:50,720 --> 00:15:52,429
it can create this object.

00:15:52,429 --> 00:15:57,550
Here I'm creating this new vector, and I'm exporting this function.

00:15:57,550 --> 00:16:02,199
When you export a function, it will actually export, it will create a module and export

00:16:02,199 --> 00:16:08,049
this specific function from the JavaScript.

00:16:08,049 --> 00:16:14,790
It will create a new vector and return a pointer, and later, you create two vectors, and you

00:16:14,790 --> 00:16:16,300
use this function to add it.

00:16:16,300 --> 00:16:23,079
It will add a new vector, then if you are done with that vector, destroy this vector,

00:16:23,079 --> 00:16:30,290
otherwise it will leak the memory, and he will die!

00:16:30,290 --> 00:16:46,790
Turbo script is a subset of typescript.

00:16:46,790 --> 00:16:52,449
It can come back to normal JavaScript.

00:16:52,449 --> 00:16:59,639
In WebAssembly, it is not really useful, because WebAssembly doesn't have shared memory, so

00:16:59,639 --> 00:17:03,899
WebAssembly 2 releases, I can really test the performance.

00:17:03,899 --> 00:17:10,742
I can compile the same code to two or three different targets, normal like JavaScript,

00:17:10,742 --> 00:17:19,000
and have some - it will be a really nice tool to compare how these three things are performing.

00:17:19,000 --> 00:17:28,049
It's still a work in programme West, so I'm trying to implement it.

00:17:28,049 --> 00:17:38,100
So a typescript like JavaScript, some combinations, in the future it is not supporting WebAssembly

00:17:38,100 --> 00:17:44,929
or in order to support all the JavaScript features, I need to re-implement all the JavaScript

00:17:44,929 --> 00:17:47,679
original machine inside WebAssembly.

00:17:47,679 --> 00:17:50,799
That's not doable for now.

00:17:50,799 --> 00:17:56,071
I can actually compile some functions into the WebAssembly and other functions into the

00:17:56,071 --> 00:18:04,130
normal JavaScript and like connect together, for example, DOM access things I can come

00:18:04,130 --> 00:18:06,669
back to the normal JavaScript, so it's easy.

00:18:06,669 --> 00:18:12,549
Maybe some hard functions, number-crunching like that.

00:18:12,549 --> 00:18:21,269
Then, you can utilise this WebAssembly, because normally when I talk about normal JavaScript

00:18:21,269 --> 00:18:26,480
developers, they're not interested in WebAssembly because right now, own C++ can compare to

00:18:26,480 --> 00:18:31,960
WebAssembly and when you talk about WebAssembly they don't know what is it, and they can't

00:18:31,960 --> 00:18:33,530
really use it.

00:18:33,530 --> 00:18:40,040
You can still write an expression in text format which is really not scaleable or it

00:18:40,040 --> 00:18:41,040
is very low.

00:18:41,040 --> 00:18:46,820
It is meant for debugging and no-one is writing a real big project in it.

00:18:46,820 --> 00:18:56,269
So this is also a good tool for normal JavaScript developers because its syntax is similar.

00:18:56,269 --> 00:19:01,360
So merge mark, and I'm useful normal JavaScript.

00:19:01,360 --> 00:19:08,440
I need to - I mention I need to create a vector for race racing which is KD3.

00:19:08,440 --> 00:19:18,100
To build it, it will take a little bit more time because it's actually splitting the 3D

00:19:18,100 --> 00:19:21,309
space into different small, small pieces.

00:19:21,309 --> 00:19:32,220
Normally, without TurboScript, I build once in the main thread or copy these objects through

00:19:32,220 --> 00:19:38,960
both messages and then to actually clone and the performance was low, and it will take

00:19:38,960 --> 00:19:43,260
more than one minute just to initialise this Standford Dragon.

00:19:43,260 --> 00:19:47,090
Right now, I'm using turbo script.

00:19:47,090 --> 00:19:59,610
It will compile and at the end get written a pointer to this 3D the workers has immediately

00:19:59,610 --> 00:20:01,350
access to the KD3.

00:20:01,350 --> 00:20:06,789
It can stop rat-racing.

00:20:06,789 --> 00:20:28,259
I will show you some syntax.

00:20:28,259 --> 00:20:51,030
There is an RAX sample in turbo script.

00:20:51,030 --> 00:20:57,780
You can also export the class, so you export something like this.

00:20:57,780 --> 00:21:06,360
We will open it so it is easy to read.

00:21:06,360 --> 00:21:12,580
So it will declare like this, like data new and data search.

00:21:12,580 --> 00:21:17,190
From the JavaScript, you can use data new so it will create a new instance of that data

00:21:17,190 --> 00:21:19,559
class.

00:21:19,559 --> 00:21:24,380
And one problem is I'm working on it, I'm working on a wrapper for these turbo classes

00:21:24,380 --> 00:21:28,139
so from JavaScript, you can access like instance.properties.

00:21:28,139 --> 00:21:35,809
Right now, you can't access properties, but you can access methods, so you can abscess

00:21:35,809 --> 00:21:54,880
set method, or if you want to access a V1, so you need to, like, catch V1, you need to

00:21:54,880 --> 00:22:00,200
do something like this to access the property, because when you - you're getting a pointer,

00:22:00,200 --> 00:22:07,700
because it is in 32 number, you cannot dot it because it is a pointer.

00:22:07,700 --> 00:22:09,779
Up need to do like this.

00:22:09,779 --> 00:22:15,730
But I'm planning like a wrapper function around the turbo object so it can actually export

00:22:15,730 --> 00:22:21,510
automatically a JavaScript wrapper, then you can use normal things, but I don't know how

00:22:21,510 --> 00:22:27,450
fast because it will create an object, maybe for a bigger objects like if you have a queue,

00:22:27,450 --> 00:22:34,669
for example, a big scene of millions of triangles, you can create wrapper objects for the main

00:22:34,669 --> 00:22:39,419
route objects like a scene, so from the scene, you can access a lot of the properties.

00:22:39,419 --> 00:22:43,840
That will be useful.

00:22:43,840 --> 00:22:44,950
Yes.

00:22:44,950 --> 00:22:55,720
And also, I'm using this wasm tool, I can format this text format and debug what is

00:22:55,720 --> 00:22:57,529
going on in my assembly.

00:22:57,529 --> 00:22:59,240
It is easy for debugging.

00:22:59,240 --> 00:23:08,080
Also, I have a log, so to limit all the data like what bytes are retaining, so it is easy

00:23:08,080 --> 00:23:18,919
for debugging, everything in the log file, but it is formatted.

00:23:18,919 --> 00:23:24,660
And yes, any more questions right now?

00:23:24,660 --> 00:23:27,210
No.

00:23:27,210 --> 00:23:29,760
Okay.

00:23:29,760 --> 00:23:37,410
Yes, that's it.

00:23:37,410 --> 00:23:39,960
[Applause].

00:23:39,960 --> 00:23:50,179
>> Thank you so much, Nidin, for the excellent talk.

00:23:50,179 --> 00:23:51,950
We have a break scheduled now.

00:23:51,950 --> 00:23:55,419
15 minutes, and then the last two talks of the conference.

00:23:55,419 --> 00:23:57,520
We also have our last community event.

00:23:57,520 --> 00:24:00,220
If you want to go and check out another { live : JS } performance and a meet-and-greet session

00:24:00,220 --> 00:24:01,220
with the representatives.

00:24:01,220 --> 00:24:02,220
So, one more talk there, and then two more talks here.

00:24:02,220 --> 00:24:02,221

YouTube URL: https://www.youtube.com/watch?v=vvqfmskTIjE


