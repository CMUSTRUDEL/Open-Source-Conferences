Title: Lian Li: Machine Learning with Node.js - JSUnconf 2016
Publication date: 2016-06-02
Playlist: JSUnconf 2016
Description: 
	Slides: https://slidr.io/Chimney42/machine-learning-with-synaptic

In this talk, I'll be telling you about:

* how neural networks do their magic
* how I implemented a simple neural network
* how to check the performance of the algorithm
* how to tackle the error rate
Captions: 
	00:00:12,379 --> 00:00:18,599
thank you guys I was here last year

00:00:15,629 --> 00:00:21,720
giving my first ever talk a lightning

00:00:18,599 --> 00:00:24,390
talks around last J Hong Kong so I'm

00:00:21,720 --> 00:00:26,130
really glad that I can be here again

00:00:24,390 --> 00:00:29,490
this year and give this talk to you guys

00:00:26,130 --> 00:00:34,290
and yeah I'm late to the gym no party

00:00:29,490 --> 00:00:37,980
I'm going concluding this party today so

00:00:34,290 --> 00:00:41,190
let's start that's me I'm a developer

00:00:37,980 --> 00:00:43,890
Jim do I am NOT a machine learning

00:00:41,190 --> 00:00:47,510
expert or a data scientist data engineer

00:00:43,890 --> 00:00:50,369
I'm neither of those I just did that

00:00:47,510 --> 00:00:52,830
project in my free time because I wanted

00:00:50,369 --> 00:00:54,180
to so if you have specific machine

00:00:52,830 --> 00:00:57,320
learning questions please don't direct

00:00:54,180 --> 00:01:01,140
them to me ask your local data engineer

00:00:57,320 --> 00:01:05,339
you can also reach me on twitter at

00:01:01,140 --> 00:01:08,070
jimmy 40 to get up and if anyone's still

00:01:05,339 --> 00:01:12,090
on Google+ just check if I'm there too I

00:01:08,070 --> 00:01:14,910
think so okay so let's start with a

00:01:12,090 --> 00:01:17,370
funny quote this is actually a former

00:01:14,910 --> 00:01:21,240
football player and he said I never

00:01:17,370 --> 00:01:24,210
predict anything and I never will so

00:01:21,240 --> 00:01:26,870
neither am i I already put the

00:01:24,210 --> 00:01:31,770
predictions up for this match today

00:01:26,870 --> 00:01:34,530
today tomorrow yesterday I put them on

00:01:31,770 --> 00:01:36,720
the back side of this wall I think and I

00:01:34,530 --> 00:01:38,760
will be updating the results as they

00:01:36,720 --> 00:01:43,050
come in I think the game will start in

00:01:38,760 --> 00:01:46,920
15 minutes so let's see how well my

00:01:43,050 --> 00:01:49,590
neural network actually does things okay

00:01:46,920 --> 00:01:51,690
so first things first we have to

00:01:49,590 --> 00:01:54,900
understand neural networks and this will

00:01:51,690 --> 00:01:56,760
conclude some math but I hope that most

00:01:54,900 --> 00:01:57,420
of you will get it because it's really

00:01:56,760 --> 00:02:00,720
not that hard

00:01:57,420 --> 00:02:03,860
but let's look at neurons first those

00:02:00,720 --> 00:02:07,170
are naturally occurring neurons - and

00:02:03,860 --> 00:02:09,959
the sneaky fuzzy stuff to the left are

00:02:07,170 --> 00:02:13,470
dendrites dendrites received the input

00:02:09,959 --> 00:02:15,569
then the neuron does stuff with it sends

00:02:13,470 --> 00:02:17,969
a signal to the next dendrite via the

00:02:15,569 --> 00:02:20,790
axon and then the whole thing is done

00:02:17,969 --> 00:02:22,060
again and the very interesting thing

00:02:20,790 --> 00:02:24,849
about a neuron

00:02:22,060 --> 00:02:26,800
is that neurons could do which to one

00:02:24,849 --> 00:02:28,540
thing can learn to do another thing so

00:02:26,800 --> 00:02:31,720
let's say those are neurons for seeing

00:02:28,540 --> 00:02:34,030
and then the person gets blind for some

00:02:31,720 --> 00:02:36,640
reason so those neurons aren't useless

00:02:34,030 --> 00:02:39,310
now because they can learn to hear for

00:02:36,640 --> 00:02:42,220
example and they will just get some

00:02:39,310 --> 00:02:43,959
input do some stuff and then just send

00:02:42,220 --> 00:02:46,170
another signal to the next neuron and it

00:02:43,959 --> 00:02:49,930
doesn't matter really what the signal is

00:02:46,170 --> 00:02:52,420
so the thing how it how neurons do that

00:02:49,930 --> 00:02:54,730
is by experience so they they get a lot

00:02:52,420 --> 00:02:56,830
of input over time and they there's a

00:02:54,730 --> 00:03:00,220
lot of repetition and iteration and by

00:02:56,830 --> 00:03:03,180
by that neurons learn to see the pattern

00:03:00,220 --> 00:03:06,400
or the logic behind things

00:03:03,180 --> 00:03:09,400
so what neurons do need is a variety of

00:03:06,400 --> 00:03:13,239
training training examples and that's

00:03:09,400 --> 00:03:15,670
the most important thing so let's look

00:03:13,239 --> 00:03:18,519
at an artificial Network that is a

00:03:15,670 --> 00:03:21,450
simple multi-layer network we have three

00:03:18,519 --> 00:03:24,700
layers here the first is the input layer

00:03:21,450 --> 00:03:27,880
that's where we receive our input then

00:03:24,700 --> 00:03:29,350
we have a hidden layer I just chose to

00:03:27,880 --> 00:03:31,989
have one hidden layer it doesn't really

00:03:29,350 --> 00:03:34,570
matter or you can just try it out what

00:03:31,989 --> 00:03:38,290
works for you and then we have an output

00:03:34,570 --> 00:03:40,930
layer so in in this graph the arrows

00:03:38,290 --> 00:03:45,549
would be our dendrites or more axons and

00:03:40,930 --> 00:03:49,600
the circles would be our neurons okay so

00:03:45,549 --> 00:03:51,220
let's look at it more closely so in the

00:03:49,600 --> 00:03:54,069
first layer we get our input like I said

00:03:51,220 --> 00:03:55,870
we call those X for the purpose of

00:03:54,069 --> 00:04:01,090
things so we have three different inputs

00:03:55,870 --> 00:04:02,769
two different features X 1 2 X 3 and if

00:04:01,090 --> 00:04:06,489
we look at the first neuron or the first

00:04:02,769 --> 00:04:10,750
layer we see that it puts out 3

00:04:06,489 --> 00:04:12,850
different values from the neuron so this

00:04:10,750 --> 00:04:15,160
is the math part I was talking about I

00:04:12,850 --> 00:04:17,049
put a legend to the side so you remember

00:04:15,160 --> 00:04:20,650
what I'm talking about so X 1 would be

00:04:17,049 --> 00:04:22,930
our input and theta 1 1 1 is just a

00:04:20,650 --> 00:04:25,180
weight so theta in this case is a matrix

00:04:22,930 --> 00:04:26,860
if you don't know what a matrix is just

00:04:25,180 --> 00:04:29,710
imagine it as being some kind of

00:04:26,860 --> 00:04:32,169
JavaScript object and the 1 1 1 would be

00:04:29,710 --> 00:04:35,020
the keys to access the value behind so I

00:04:32,169 --> 00:04:40,569
could just say x1 times

00:04:35,020 --> 00:04:42,970
or whatever pony unicorn and so then we

00:04:40,569 --> 00:04:46,360
send another signal x1 times another

00:04:42,970 --> 00:04:49,000
wait and then another and the way that

00:04:46,360 --> 00:04:51,819
we choose theta here the weight is

00:04:49,000 --> 00:04:53,440
actually that for our initial run for

00:04:51,819 --> 00:04:56,349
the first iteration we just choose it

00:04:53,440 --> 00:04:58,780
randomly just we don't know what the way

00:04:56,349 --> 00:05:02,380
it would be and we just initiated

00:04:58,780 --> 00:05:04,000
randomly okay so if we look at the first

00:05:02,380 --> 00:05:06,940
neuron the hidden layer and the second

00:05:04,000 --> 00:05:09,160
layer then we see we get three different

00:05:06,940 --> 00:05:13,210
features multiplied with three different

00:05:09,160 --> 00:05:15,550
weights so we get X 1 times something X

00:05:13,210 --> 00:05:17,650
2 times something and X 3 times

00:05:15,550 --> 00:05:20,919
something so this neuron actually gets

00:05:17,650 --> 00:05:24,550
all the input with some weight and then

00:05:20,919 --> 00:05:27,789
it just sums it all up so it just adds

00:05:24,550 --> 00:05:28,479
the three input values and then it does

00:05:27,789 --> 00:05:30,580
something

00:05:28,479 --> 00:05:32,830
activate there's in fact of an

00:05:30,580 --> 00:05:34,479
activation function and that is the

00:05:32,830 --> 00:05:36,970
magic that's happening so there are

00:05:34,479 --> 00:05:38,440
different activation functions I won't

00:05:36,970 --> 00:05:41,770
go into them because that takes a lot of

00:05:38,440 --> 00:05:44,680
math important to know it's just that we

00:05:41,770 --> 00:05:47,889
sum up all the input and then do stuff

00:05:44,680 --> 00:05:51,250
with it and then get to our new value

00:05:47,889 --> 00:05:52,990
which would be a 1/2 but again this is

00:05:51,250 --> 00:05:57,099
just a value in a matrix doesn't really

00:05:52,990 --> 00:05:59,320
matter what it says so the same thing

00:05:57,099 --> 00:06:00,370
actually also happens in our last layer

00:05:59,320 --> 00:06:03,310
now output layer

00:06:00,370 --> 00:06:07,539
we receive our input so we get now have

00:06:03,310 --> 00:06:10,830
a 1 to a 2 2 and a 3 2 and then again

00:06:07,539 --> 00:06:15,280
some Thetas again initially random

00:06:10,830 --> 00:06:19,930
randomly initialized and then we get our

00:06:15,280 --> 00:06:21,759
last value a 1 3 which is just the sum

00:06:19,930 --> 00:06:26,139
of everything we have and then we

00:06:21,759 --> 00:06:28,810
activate it with something ok so what we

00:06:26,139 --> 00:06:31,060
have in the end is we have values we do

00:06:28,810 --> 00:06:33,820
stuff with it get new values to do stuff

00:06:31,060 --> 00:06:36,490
again then get a third value that is the

00:06:33,820 --> 00:06:39,190
result of our hypothesis so our

00:06:36,490 --> 00:06:42,789
hypothesis is kind of like the model

00:06:39,190 --> 00:06:44,500
that we try to build to represent the

00:06:42,789 --> 00:06:48,200
logic or the pattern we're trying to

00:06:44,500 --> 00:06:51,800
learn and so what age theta of X

00:06:48,200 --> 00:06:56,150
means is like you put excess in like X 1

00:06:51,800 --> 00:06:59,360
X 2 X 3 would be 3 values in a vector or

00:06:56,150 --> 00:07:02,480
an array if you want and then so the

00:06:59,360 --> 00:07:05,270
function says this is the logic that I

00:07:02,480 --> 00:07:07,760
think it's gonna be and then we get our

00:07:05,270 --> 00:07:10,340
result this would be a numerical value

00:07:07,760 --> 00:07:13,280
probably and it's ideally really close

00:07:10,340 --> 00:07:17,750
to the reality and the reality is called

00:07:13,280 --> 00:07:20,200
Y in this case ok so there was really a

00:07:17,750 --> 00:07:23,420
lot so let's pause with a funny quote

00:07:20,200 --> 00:07:25,670
this is by George EP box and he says all

00:07:23,420 --> 00:07:34,640
models are wrong but some of them are

00:07:25,670 --> 00:07:36,890
useful ok so let's say we did all that

00:07:34,640 --> 00:07:39,380
stuff with our neurons and become fewer

00:07:36,890 --> 00:07:42,890
all that those things with their magic

00:07:39,380 --> 00:07:45,110
activation function and now we have a

00:07:42,890 --> 00:07:47,390
result but the result is actually

00:07:45,110 --> 00:07:50,390
probably not that good probably not that

00:07:47,390 --> 00:07:52,340
close to reality because we initialize

00:07:50,390 --> 00:07:59,540
data randomly so we just said just to

00:07:52,340 --> 00:08:01,400
random stuff whether for this case which

00:07:59,540 --> 00:08:06,020
is called supervised learning because we

00:08:01,400 --> 00:08:06,530
have we have a output that we know is

00:08:06,020 --> 00:08:09,590
the truth

00:08:06,530 --> 00:08:12,470
so we supervise our machine while it

00:08:09,590 --> 00:08:15,050
learns and supervised learning is used

00:08:12,470 --> 00:08:17,360
for for example quota for betting places

00:08:15,050 --> 00:08:20,270
the P win quota there is one where there

00:08:17,360 --> 00:08:24,050
are supervised learning and also that is

00:08:20,270 --> 00:08:27,830
a really nice example as housing price

00:08:24,050 --> 00:08:29,990
predictions where you say this is the

00:08:27,830 --> 00:08:32,210
square meters and this is a number of

00:08:29,990 --> 00:08:35,060
rooms and this is the price amount of

00:08:32,210 --> 00:08:37,550
money that I have to pay for and so you

00:08:35,060 --> 00:08:40,040
train your neural network with stuff

00:08:37,550 --> 00:08:43,610
that you know is the truth

00:08:40,040 --> 00:08:45,580
there's also unsupervised learning that

00:08:43,610 --> 00:08:48,200
is for example the Netflix

00:08:45,580 --> 00:08:51,110
recommendation system so there is no

00:08:48,200 --> 00:08:54,500
real answer that we the people who build

00:08:51,110 --> 00:08:56,030
it know about we just say probably

00:08:54,500 --> 00:08:59,720
there's some pattern here maybe you can

00:08:56,030 --> 00:09:02,140
try to figure it out and clustering is

00:08:59,720 --> 00:09:05,510
one of the most often used

00:09:02,140 --> 00:09:07,970
methods to do unsupervised learning but

00:09:05,510 --> 00:09:11,240
I won't be talking about that only to be

00:09:07,970 --> 00:09:12,950
talking about supervised learning okay

00:09:11,240 --> 00:09:17,000
so like I said we initialize data

00:09:12,950 --> 00:09:19,070
randomly and this could be an example of

00:09:17,000 --> 00:09:23,000
our training data so we have four

00:09:19,070 --> 00:09:26,660
training examples let's just say this is

00:09:23,000 --> 00:09:29,540
housing prices so x1 would be the number

00:09:26,660 --> 00:09:31,130
of I don't know square meters and the

00:09:29,540 --> 00:09:33,350
other one would be the number of rooms

00:09:31,130 --> 00:09:37,399
which is weird but doesn't matter

00:09:33,350 --> 00:09:39,770
so XS are the inputs Y is the result

00:09:37,399 --> 00:09:41,660
that we know it is and H of X would be

00:09:39,770 --> 00:09:44,779
the result that our neural network just

00:09:41,660 --> 00:09:46,640
gave us so as you can see it's off of

00:09:44,779 --> 00:09:49,850
course because we just did it randomly

00:09:46,640 --> 00:09:53,089
and the question now becomes is how do

00:09:49,850 --> 00:09:58,360
we actually learn the correct patterns

00:09:53,089 --> 00:10:02,180
or the correct weights for our theta so

00:09:58,360 --> 00:10:06,860
what we can do is calculate the cost so

00:10:02,180 --> 00:10:10,339
we have our output 60 and our y which is

00:10:06,860 --> 00:10:13,790
43 and the cost would be the difference

00:10:10,339 --> 00:10:15,680
between those two and this cost is also

00:10:13,790 --> 00:10:18,680
described described by another function

00:10:15,680 --> 00:10:20,900
it's called J of theta doesn't really

00:10:18,680 --> 00:10:22,610
matter what you call it it just means

00:10:20,900 --> 00:10:26,300
this calculates the difference between

00:10:22,610 --> 00:10:30,980
our result and the actual thing that the

00:10:26,300 --> 00:10:33,050
reality okay so what we do now is we use

00:10:30,980 --> 00:10:36,560
a very complicated mathematical function

00:10:33,050 --> 00:10:38,029
and then we take our cost and then

00:10:36,560 --> 00:10:41,180
propagate back through our network

00:10:38,029 --> 00:10:44,450
that's called back propagation and try

00:10:41,180 --> 00:10:46,339
to change the values for theta in a way

00:10:44,450 --> 00:10:49,100
that minimizes the cost because we

00:10:46,339 --> 00:10:51,170
wanted the error to be small as small as

00:10:49,100 --> 00:10:55,760
possible so we try to minimize J of

00:10:51,170 --> 00:10:58,100
theta and the way that we do that with

00:10:55,760 --> 00:11:00,920
synaptic it's called gradient descent

00:10:58,100 --> 00:11:03,649
there are other ways and other functions

00:11:00,920 --> 00:11:06,800
to actually do that but I will only talk

00:11:03,649 --> 00:11:08,750
about gradient descent for now so this

00:11:06,800 --> 00:11:09,570
is very complicated so I'm trying to

00:11:08,750 --> 00:11:12,240
make

00:11:09,570 --> 00:11:14,339
in a very basic way so let's say this is

00:11:12,240 --> 00:11:16,829
a graphical representation of our cost

00:11:14,339 --> 00:11:20,100
function we have theta 1 we have theta 2

00:11:16,829 --> 00:11:23,399
and then we have a circle which is stay

00:11:20,100 --> 00:11:25,410
of theta so what that means is that all

00:11:23,399 --> 00:11:28,980
the combinations of theta 1 and theta 2

00:11:25,410 --> 00:11:31,829
that are on the circle have the same

00:11:28,980 --> 00:11:36,470
value for theta J of theta so let's say

00:11:31,829 --> 00:11:40,920
the the the the bottom dot there is

00:11:36,470 --> 00:11:43,079
theta 1 2 and theta 2 1 that's not a

00:11:40,920 --> 00:11:44,459
very good example but so there those

00:11:43,079 --> 00:11:46,860
will have the same value for J of theta

00:11:44,459 --> 00:11:50,720
or like the upper point where theta 1

00:11:46,860 --> 00:11:53,579
would be 2 and theta 2 would be 4 maybe

00:11:50,720 --> 00:11:56,610
okay and then if we calculate J of theta

00:11:53,579 --> 00:11:59,100
4 a lot of combinations of theta 1 and

00:11:56,610 --> 00:12:01,079
theta 2 we have something that looks

00:11:59,100 --> 00:12:03,180
like this and the way you have to look

00:12:01,079 --> 00:12:05,940
at it is actually that there is a third

00:12:03,180 --> 00:12:08,220
dimension going into the back which is

00:12:05,940 --> 00:12:10,769
there of theta so the value of J of

00:12:08,220 --> 00:12:14,510
theta becomes smaller to the back so

00:12:10,769 --> 00:12:17,880
imagine you're looking into a funnel now

00:12:14,510 --> 00:12:20,459
we can see here that where the Red Dot

00:12:17,880 --> 00:12:22,019
is that would represent the minimum so

00:12:20,459 --> 00:12:24,769
where J of theta would be the smallest

00:12:22,019 --> 00:12:27,750
and this is what we want to reach and

00:12:24,769 --> 00:12:30,360
what gradient descent does is now that

00:12:27,750 --> 00:12:32,610
let's say the gray dot I hope you can

00:12:30,360 --> 00:12:35,160
see it yes and the way dot is where we

00:12:32,610 --> 00:12:38,910
come out initially and we understand

00:12:35,160 --> 00:12:41,610
basically checks for the values of theta

00:12:38,910 --> 00:12:43,620
1 and theta 2 that will get us closer to

00:12:41,610 --> 00:12:45,779
the minimum doesn't have to go there

00:12:43,620 --> 00:12:49,860
directly sometimes it will wander around

00:12:45,779 --> 00:12:52,019
a little bit but eventually it will try

00:12:49,860 --> 00:12:53,699
to reach the global minimum it doesn't

00:12:52,019 --> 00:12:57,389
happen all the time sometimes we just

00:12:53,699 --> 00:13:01,050
wander around it but you you will reach

00:12:57,389 --> 00:13:03,120
the vicinity of it and the size of the

00:13:01,050 --> 00:13:05,010
steps actually is called the learning

00:13:03,120 --> 00:13:06,990
rate you can change that rate you can

00:13:05,010 --> 00:13:09,000
make the size as bigger or smaller and

00:13:06,990 --> 00:13:11,040
usually the learning rate gets smaller

00:13:09,000 --> 00:13:12,689
the closer you are to the minimum so

00:13:11,040 --> 00:13:15,899
what you can't really see here is that

00:13:12,689 --> 00:13:21,260
the steps will get smaller the more you

00:13:15,899 --> 00:13:21,260
approach the red dot okay

00:13:21,310 --> 00:13:26,030
that was a lot of theory so let's

00:13:24,620 --> 00:13:29,450
implement something and you will see

00:13:26,030 --> 00:13:31,970
it's really not that hard so I use

00:13:29,450 --> 00:13:33,860
synaptic because synaptic does all that

00:13:31,970 --> 00:13:35,210
stuff that I showed you already and I

00:13:33,860 --> 00:13:38,990
don't really have to care about it it's

00:13:35,210 --> 00:13:41,690
a framework to do exactly - then do

00:13:38,990 --> 00:13:43,400
stuff with the neural network and it

00:13:41,690 --> 00:13:45,770
gives us everything we need to build a

00:13:43,400 --> 00:13:47,780
simple network and we have an architect

00:13:45,770 --> 00:13:50,450
and the architect will build or a

00:13:47,780 --> 00:13:53,210
network and then we have a trainer and

00:13:50,450 --> 00:13:55,610
that will train our network with the e

00:13:53,210 --> 00:13:59,480
training examples that are not included

00:13:55,610 --> 00:14:02,480
we have to provide them - for our neural

00:13:59,480 --> 00:14:04,790
network and we can make predictions with

00:14:02,480 --> 00:14:10,250
our trained network by calling the

00:14:04,790 --> 00:14:13,690
function activate ok code I will give

00:14:10,250 --> 00:14:13,690
you a moment to parse it

00:14:21,440 --> 00:14:28,129
so I have two objects there one is the

00:14:25,579 --> 00:14:31,190
historic match it has an input array

00:14:28,129 --> 00:14:33,110
those are market values for the team I

00:14:31,190 --> 00:14:35,600
just thought that market values are a

00:14:33,110 --> 00:14:38,269
pretty good approximation of how strong

00:14:35,600 --> 00:14:39,620
a team is so I just chose that one but

00:14:38,269 --> 00:14:42,800
feel free to choose anything else you

00:14:39,620 --> 00:14:44,269
want and so the first is for the home

00:14:42,800 --> 00:14:47,480
team and the second value is for the

00:14:44,269 --> 00:14:50,509
relay team and if you always provide the

00:14:47,480 --> 00:14:52,250
home team first then the numeral network

00:14:50,509 --> 00:14:53,990
will also learn from that because that

00:14:52,250 --> 00:14:56,660
is also a pattern that it will pick up

00:14:53,990 --> 00:14:58,850
if there is something like a home team

00:14:56,660 --> 00:15:01,670
advantage then maybe our new network

00:14:58,850 --> 00:15:05,240
will learn that and then we have an

00:15:01,670 --> 00:15:07,100
output that is a vector or an array

00:15:05,240 --> 00:15:09,949
where we have basically three

00:15:07,100 --> 00:15:11,990
classifications so the first value would

00:15:09,949 --> 00:15:14,750
mean the home team win if it's a one

00:15:11,990 --> 00:15:16,819
over the zero no the second value says

00:15:14,750 --> 00:15:19,459
that it is a draw and the third value

00:15:16,819 --> 00:15:21,079
would be that the away team wins so we

00:15:19,459 --> 00:15:23,240
actually we do three types of

00:15:21,079 --> 00:15:25,550
classifications one for each of the

00:15:23,240 --> 00:15:27,740
possible outcomes and in this example

00:15:25,550 --> 00:15:30,680
the home team won because there's a one

00:15:27,740 --> 00:15:32,120
as the first value then we have the

00:15:30,680 --> 00:15:34,639
match in the future which basically

00:15:32,120 --> 00:15:36,829
looks exactly the same just except that

00:15:34,639 --> 00:15:38,509
it doesn't have an output that's logical

00:15:36,829 --> 00:15:42,259
because we want our annual network to

00:15:38,509 --> 00:15:45,500
give us the output so then we have our

00:15:42,259 --> 00:15:47,720
training set that is consistent of the

00:15:45,500 --> 00:15:49,899
historic matches and we have our network

00:15:47,720 --> 00:15:52,490
that is trained by the architect

00:15:49,899 --> 00:15:55,040
perceptron actually means the smallest

00:15:52,490 --> 00:15:57,949
possible network which is just one

00:15:55,040 --> 00:15:59,410
neuron input and output but it doesn't

00:15:57,949 --> 00:16:01,910
really matter it could also say in it

00:15:59,410 --> 00:16:06,889
it's just the way that we build the

00:16:01,910 --> 00:16:11,209
network and so we give the perceptron

00:16:06,889 --> 00:16:14,300
the number of nodes per layer so first

00:16:11,209 --> 00:16:17,990
is two because we have two input values

00:16:14,300 --> 00:16:19,670
I chose two hidden layers of six nodes

00:16:17,990 --> 00:16:22,059
but it doesn't really matter you can

00:16:19,670 --> 00:16:25,160
just try it out actually that's what I

00:16:22,059 --> 00:16:27,500
also did I just try out what would seem

00:16:25,160 --> 00:16:29,180
to work best and then we have three

00:16:27,500 --> 00:16:33,380
nodes four output layer because we have

00:16:29,180 --> 00:16:34,930
three values in our output then we

00:16:33,380 --> 00:16:37,810
trained our trainer

00:16:34,930 --> 00:16:39,580
the training set and I just chose a

00:16:37,810 --> 00:16:41,589
learning rate that you can see there

00:16:39,580 --> 00:16:43,870
there's learning rate the steps that we

00:16:41,589 --> 00:16:46,360
take in gradient descent and that is

00:16:43,870 --> 00:16:49,600
also something I just figured out with

00:16:46,360 --> 00:16:51,610
some trial and error and then there is a

00:16:49,600 --> 00:16:53,410
number of iterations so you say I want

00:16:51,610 --> 00:16:55,209
to iterate through my whole training set

00:16:53,410 --> 00:16:59,470
a hundred thousand times in this case

00:16:55,209 --> 00:17:02,470
and after we trained or trainer or

00:16:59,470 --> 00:17:05,949
network we can actually make predictions

00:17:02,470 --> 00:17:09,870
like I said before we give our network

00:17:05,949 --> 00:17:15,100
the activations or the input with the

00:17:09,870 --> 00:17:18,670
function called activate and this is

00:17:15,100 --> 00:17:21,280
what the output could look like so here

00:17:18,670 --> 00:17:24,640
you can see the activations the inputs

00:17:21,280 --> 00:17:27,880
the market values and then the output

00:17:24,640 --> 00:17:31,120
and as you can see there's no output

00:17:27,880 --> 00:17:34,390
like 1 0 0 0 0 1 this is a probability

00:17:31,120 --> 00:17:37,000
distribution so all the values added up

00:17:34,390 --> 00:17:38,830
in this array will come to one so

00:17:37,000 --> 00:17:40,870
there's a hundred percent possibility

00:17:38,830 --> 00:17:43,570
that one of those outcomes will happen

00:17:40,870 --> 00:17:46,510
and then there's a distribution of how

00:17:43,570 --> 00:17:51,240
likely it is so for the first one you

00:17:46,510 --> 00:17:52,420
can see that the team who has 320 1.15

00:17:51,240 --> 00:17:55,840
euro

00:17:52,420 --> 00:18:01,630
Paulie millions of euros will more

00:17:55,840 --> 00:18:03,760
likely win than the other team ok this

00:18:01,630 --> 00:18:13,580
is a lot of code and it's also a thinker

00:18:03,760 --> 00:18:19,080
so take your time everyone got that

00:18:13,580 --> 00:18:20,760
good okay so now we have our network we

00:18:19,080 --> 00:18:23,220
have our output it looks pretty good so

00:18:20,760 --> 00:18:27,570
far but how do we know how well it

00:18:23,220 --> 00:18:30,720
performs I will explain some error rates

00:18:27,570 --> 00:18:32,520
that I personally focused on but as when

00:18:30,720 --> 00:18:35,880
I try to learn how to improve my

00:18:32,520 --> 00:18:38,550
algorithm but feel free to think come up

00:18:35,880 --> 00:18:39,980
with your own errors it's just what I

00:18:38,550 --> 00:18:42,510
did

00:18:39,980 --> 00:18:47,460
so there's something called the synaptic

00:18:42,510 --> 00:18:52,770
data error and that is already provided

00:18:47,460 --> 00:18:54,960
with synaptic and I call it data error

00:18:52,770 --> 00:18:57,230
because it just did the name is arrow

00:18:54,960 --> 00:19:00,570
and I call it on the data object and

00:18:57,230 --> 00:19:04,020
what we do here is we add a schedule to

00:19:00,570 --> 00:19:07,530
our trainer and we tell them that every

00:19:04,020 --> 00:19:13,230
ten thousandth iteration I want to log

00:19:07,530 --> 00:19:17,910
the current data error to the console so

00:19:13,230 --> 00:19:20,940
this is bad okay and this could be the

00:19:17,910 --> 00:19:22,860
output and in this case you can see

00:19:20,940 --> 00:19:24,630
actually that the error actually goes

00:19:22,860 --> 00:19:26,790
down a little bit over time and over

00:19:24,630 --> 00:19:31,080
iterations and that is really really

00:19:26,790 --> 00:19:32,820
good to see and also 18 percent is also

00:19:31,080 --> 00:19:34,560
pretty good because we only give it our

00:19:32,820 --> 00:19:38,130
market values and as seems to perform

00:19:34,560 --> 00:19:39,630
pretty good but the error doesn't really

00:19:38,130 --> 00:19:43,140
tell us anything it doesn't really

00:19:39,630 --> 00:19:47,520
translate to anything that you can then

00:19:43,140 --> 00:19:49,470
interpret so I thought I need to display

00:19:47,520 --> 00:19:52,140
a different error that was more of an

00:19:49,470 --> 00:19:55,380
intuitive representation of how well my

00:19:52,140 --> 00:19:59,100
algorithm actually performed so what I

00:19:55,380 --> 00:20:02,730
did then is come up with something

00:19:59,100 --> 00:20:06,020
called the classification error so give

00:20:02,730 --> 00:20:06,020
you a moment to parse this

00:20:08,870 --> 00:20:17,400
okay what i'm doing here is in our do

00:20:13,350 --> 00:20:20,040
function for every 10,000 iteration i

00:20:17,400 --> 00:20:22,800
want to actually make the prediction

00:20:20,040 --> 00:20:24,870
because now i only first i only have the

00:20:22,800 --> 00:20:27,870
probabilities but i don't actually say

00:20:24,870 --> 00:20:29,910
so the home team's gonna win so this is

00:20:27,870 --> 00:20:32,700
what i'm doing here the predict from

00:20:29,910 --> 00:20:35,820
probability function that you can see

00:20:32,700 --> 00:20:38,780
actually just looks for the key where

00:20:35,820 --> 00:20:41,760
the value is the biggest in the array so

00:20:38,780 --> 00:20:44,400
if the home team won this would be 0 if

00:20:41,760 --> 00:20:48,830
it was a draw would be 1 and the

00:20:44,400 --> 00:20:52,140
weighting 1 would be 2 and then i just

00:20:48,830 --> 00:20:54,120
look just make the prediction actually

00:20:52,140 --> 00:20:57,450
and then do the same thing and then

00:20:54,120 --> 00:21:00,720
compare whether the prediction actually

00:20:57,450 --> 00:21:02,970
met the expectation count up my errors

00:21:00,720 --> 00:21:06,390
and then divided through the length of

00:21:02,970 --> 00:21:08,730
the training set and what we actually

00:21:06,390 --> 00:21:12,240
see here is that the error rate is much

00:21:08,730 --> 00:21:14,730
higher with 45% and this is actually

00:21:12,240 --> 00:21:17,580
what i had suspected and i will probably

00:21:14,730 --> 00:21:19,350
tell you later why and also you can see

00:21:17,580 --> 00:21:23,220
that it doesn't continuously go down it

00:21:19,350 --> 00:21:27,060
just seems to hover around 44% but i

00:21:23,220 --> 00:21:28,920
still thought this really wasn't telling

00:21:27,060 --> 00:21:31,410
me how well my algorithm algorithm

00:21:28,920 --> 00:21:32,970
performs because i make those

00:21:31,410 --> 00:21:36,090
predictions with the training set so i

00:21:32,970 --> 00:21:37,620
already know about the data that i get

00:21:36,090 --> 00:21:39,690
in and then i make a prediction from it

00:21:37,620 --> 00:21:41,940
doesn't tell me anything about how well

00:21:39,690 --> 00:21:45,240
my algorithm performs for future

00:21:41,940 --> 00:21:47,310
predictions so then i came up with a

00:21:45,240 --> 00:21:51,150
thing called the cross-validation error

00:21:47,310 --> 00:21:54,990
and what i do there is actually first

00:21:51,150 --> 00:21:58,110
split the data set into a training set

00:21:54,990 --> 00:22:00,210
and then a cross validation set and the

00:21:58,110 --> 00:22:02,520
reason i do that is because i want the

00:22:00,210 --> 00:22:04,590
training data to only train my neural

00:22:02,520 --> 00:22:07,320
network and then I take the cross

00:22:04,590 --> 00:22:09,720
rotation data I act like it would be the

00:22:07,320 --> 00:22:13,200
future and then check how well

00:22:09,720 --> 00:22:15,690
algorithm performs so I'm doing the

00:22:13,200 --> 00:22:17,070
exact same thing as I did before just I

00:22:15,690 --> 00:22:18,690
make the predictions on the

00:22:17,070 --> 00:22:23,400
cross-validation set and not on the

00:22:18,690 --> 00:22:27,930
training set okay so this is the output

00:22:23,400 --> 00:22:30,450
and you can see that it seemed to

00:22:27,930 --> 00:22:31,890
perform even worse a little bit but also

00:22:30,450 --> 00:22:36,000
this is something that I already

00:22:31,890 --> 00:22:38,340
suspected would happen and what's weird

00:22:36,000 --> 00:22:40,140
is that it seems like the error rate

00:22:38,340 --> 00:22:42,900
goes up for at first and then goes down

00:22:40,140 --> 00:22:49,550
and then up again just like the arrow we

00:22:42,900 --> 00:22:52,350
saw before so the question now becomes

00:22:49,550 --> 00:22:53,610
why what what does this mean what what

00:22:52,350 --> 00:22:57,780
do these arrow rates

00:22:53,610 --> 00:23:00,300
tell me actually so the thing is if we

00:22:57,780 --> 00:23:02,670
have one example where our result would

00:23:00,300 --> 00:23:04,980
be this probability distribution and

00:23:02,670 --> 00:23:08,780
then we have the actual result which

00:23:04,980 --> 00:23:12,270
would be this 0 0 1 so they're waiting 1

00:23:08,780 --> 00:23:16,170
what the data error gives us is called

00:23:12,270 --> 00:23:20,910
the mean squared error and the squared

00:23:16,170 --> 00:23:22,590
error is this so what this means is the

00:23:20,910 --> 00:23:24,660
probability distribution would be a

00:23:22,590 --> 00:23:28,560
vector and we measure the we get the

00:23:24,660 --> 00:23:32,010
length of the vector and then to the

00:23:28,560 --> 00:23:35,940
power of 2 which would come up to 0.98

00:23:32,010 --> 00:23:38,100
about 0.9 and then the mean means we

00:23:35,940 --> 00:23:40,020
take all training examples not just the

00:23:38,100 --> 00:23:43,230
one and then calculate the average

00:23:40,020 --> 00:23:46,950
squared error so probably the average

00:23:43,230 --> 00:23:50,310
would be somewhere close to 0.98

00:23:46,950 --> 00:23:54,290
let's say it's that now for a

00:23:50,310 --> 00:23:59,580
classification error we only have one

00:23:54,290 --> 00:24:01,650
because you can either be a hundred

00:23:59,580 --> 00:24:03,000
percent wrong or a hundred percent right

00:24:01,650 --> 00:24:05,100
there's no in between there's no

00:24:03,000 --> 00:24:07,410
probabilities so in this case our error

00:24:05,100 --> 00:24:09,540
would be just one and also for a

00:24:07,410 --> 00:24:13,320
cross-validation error it would be one

00:24:09,540 --> 00:24:15,990
but still it's it's logical that our

00:24:13,320 --> 00:24:17,720
would perform worse with data it doesn't

00:24:15,990 --> 00:24:24,210
know about it's just not in the

00:24:17,720 --> 00:24:27,540
experienced realm of the algorithm okay

00:24:24,210 --> 00:24:29,790
so what does it actually mean when my

00:24:27,540 --> 00:24:32,610
cross-validation error is bigger or

00:24:29,790 --> 00:24:35,460
greater than my classification error so

00:24:32,610 --> 00:24:38,790
that could mean that we have over fitted

00:24:35,460 --> 00:24:42,780
or model or have high variance and I'm

00:24:38,790 --> 00:24:46,290
trying to explain that let's say this is

00:24:42,780 --> 00:24:49,530
our training data so this one is X 1 and

00:24:46,290 --> 00:24:52,530
X 2 we have two features and this is our

00:24:49,530 --> 00:24:55,680
training data and now our model is the

00:24:52,530 --> 00:24:58,860
line there so the model describes the

00:24:55,680 --> 00:25:00,780
training data pretty good but maybe it's

00:24:58,860 --> 00:25:03,440
not that good for future predictions

00:25:00,780 --> 00:25:06,510
because it's really it just follows this

00:25:03,440 --> 00:25:10,950
exact model and it doesn't really

00:25:06,510 --> 00:25:13,200
generalize very well and that is called

00:25:10,950 --> 00:25:16,380
overhead over fit or high variance and

00:25:13,200 --> 00:25:18,770
the other thing the other extreme would

00:25:16,380 --> 00:25:21,990
be if you have the same training set and

00:25:18,770 --> 00:25:24,810
this would be a model it's a straight

00:25:21,990 --> 00:25:27,240
line it's it doesn't fit so well on the

00:25:24,810 --> 00:25:29,520
existing training data but it might

00:25:27,240 --> 00:25:32,430
perform better with future predictions

00:25:29,520 --> 00:25:37,620
than the one on the left and that is

00:25:32,430 --> 00:25:41,580
called under fit or high bias so what we

00:25:37,620 --> 00:25:44,040
maybe can do is try to get more data or

00:25:41,580 --> 00:25:47,130
more features so either get more

00:25:44,040 --> 00:25:49,110
historical data go to the last year the

00:25:47,130 --> 00:25:50,130
year before that also get the data from

00:25:49,110 --> 00:25:52,650
I don't know

00:25:50,130 --> 00:25:56,460
Premier League or the Super League or

00:25:52,650 --> 00:25:59,730
whatever or we can do some or we can get

00:25:56,460 --> 00:26:02,370
more features like the match day or the

00:25:59,730 --> 00:26:07,670
table position or I don't know that a

00:26:02,370 --> 00:26:10,260
number of sexy coaches in the team and

00:26:07,670 --> 00:26:13,950
then there's this other thing called

00:26:10,260 --> 00:26:16,500
regularization and that is basically

00:26:13,950 --> 00:26:20,970
another parameter that we add to our

00:26:16,500 --> 00:26:21,770
cost function ya know to the gradient

00:26:20,970 --> 00:26:26,059
descent function

00:26:21,770 --> 00:26:29,240
and what does actually does is trying to

00:26:26,059 --> 00:26:31,580
keep the Thetas as small or big as

00:26:29,240 --> 00:26:33,650
possible it depends really on what you

00:26:31,580 --> 00:26:38,270
value you choose for your regularization

00:26:33,650 --> 00:26:39,530
parameter if you think about the model I

00:26:38,270 --> 00:26:42,410
showed you before

00:26:39,530 --> 00:26:44,660
keeping the theta small would mean to

00:26:42,410 --> 00:26:46,850
straighten out the line and making a big

00:26:44,660 --> 00:26:48,710
would mean to curve the line more so

00:26:46,850 --> 00:26:51,070
this is how you could counteract

00:26:48,710 --> 00:26:54,679
overfitting or underfitting

00:26:51,070 --> 00:26:56,240
and also the hovering that we saw before

00:26:54,679 --> 00:26:58,910
that the error rate seemed to hover

00:26:56,240 --> 00:27:01,400
around a number but never seem to get

00:26:58,910 --> 00:27:03,110
anywhere could could mean that we

00:27:01,400 --> 00:27:05,990
actually already reached our minimum

00:27:03,110 --> 00:27:07,820
it's just not very optimal so maybe it's

00:27:05,990 --> 00:27:11,720
just we're not that we didn't describe

00:27:07,820 --> 00:27:19,250
our feature pretty good pretty well so

00:27:11,720 --> 00:27:22,340
hmm sorry what I actually did is I chose

00:27:19,250 --> 00:27:24,770
to go with more features and I want to

00:27:22,340 --> 00:27:26,270
show you what happened so I this is for

00:27:24,770 --> 00:27:29,090
the market values you can see the error

00:27:26,270 --> 00:27:32,350
rates it's kind of basically what we saw

00:27:29,090 --> 00:27:35,750
before and now I have two five features

00:27:32,350 --> 00:27:41,540
matched a the market values and then the

00:27:35,750 --> 00:27:43,340
table positions and now we can try to

00:27:41,540 --> 00:27:45,080
interpret there seems to be something

00:27:43,340 --> 00:27:47,450
that is going there was a little bit

00:27:45,080 --> 00:27:50,900
better other things seem to be a little

00:27:47,450 --> 00:27:53,360
bit worse but I personally decided that

00:27:50,900 --> 00:27:55,429
the variation in numbers is just too

00:27:53,360 --> 00:27:57,890
little to really read anything into it

00:27:55,429 --> 00:28:00,559
so the only thing that that really told

00:27:57,890 --> 00:28:02,960
me was that investing time and maybe

00:28:00,559 --> 00:28:04,490
money in getting more features might not

00:28:02,960 --> 00:28:06,530
be that beneficial because it just

00:28:04,490 --> 00:28:08,840
doesn't have that effect on the error

00:28:06,530 --> 00:28:11,840
rates that I hoped it would be I would

00:28:08,840 --> 00:28:13,610
have so my next step would just be to

00:28:11,840 --> 00:28:19,690
try get more data and see what it does

00:28:13,610 --> 00:28:21,830
to my error rate so another funny quote

00:28:19,690 --> 00:28:24,290
prediction is very difficult especially

00:28:21,830 --> 00:28:25,910
about the future and as you saw not even

00:28:24,290 --> 00:28:28,130
for the future just for the faked future

00:28:25,910 --> 00:28:30,550
just for the present it's not that easy

00:28:28,130 --> 00:28:30,550
at all

00:28:32,679 --> 00:28:40,100
okay so now you hopefully know how

00:28:36,679 --> 00:28:41,510
neural networks do their magic and how

00:28:40,100 --> 00:28:43,309
to implement a machine learning

00:28:41,510 --> 00:28:45,110
algorithm with synaptic and you don't

00:28:43,309 --> 00:28:49,100
even have to do all the math stuff just

00:28:45,110 --> 00:28:50,660
go and do stuff then you might have

00:28:49,100 --> 00:28:53,480
learned how to check the performance of

00:28:50,660 --> 00:28:56,690
your algorithm and how to interpret your

00:28:53,480 --> 00:28:58,730
error rates so before I said I would

00:28:56,690 --> 00:29:02,320
make a prediction for this matchday and

00:28:58,730 --> 00:29:05,570
I did I already gave this talk yesterday

00:29:02,320 --> 00:29:09,380
ahjuma so they can actually vouch that I

00:29:05,570 --> 00:29:10,460
made this prediction and I got the first

00:29:09,380 --> 00:29:12,620
game right already

00:29:10,460 --> 00:29:15,169
that's awesome like I said I had the

00:29:12,620 --> 00:29:16,790
predictions over there and you can check

00:29:15,169 --> 00:29:19,130
for yourself how well my algorithm

00:29:16,790 --> 00:29:21,080
actually performs I'm using data from

00:29:19,130 --> 00:29:24,410
the last four years of the bundesliga so

00:29:21,080 --> 00:29:25,820
it's really not that good depending on

00:29:24,410 --> 00:29:31,280
the error rate it's probably worse than

00:29:25,820 --> 00:29:33,740
a coin toss maybe okay if you want to

00:29:31,280 --> 00:29:35,650
know more the first link would be to the

00:29:33,740 --> 00:29:39,610
repository it's called positronic brain

00:29:35,650 --> 00:29:44,419
every Star Trek watcher should laugh now

00:29:39,610 --> 00:29:46,580
okay no one fine the second link is to

00:29:44,419 --> 00:29:49,190
the slides I will also tweet them and

00:29:46,580 --> 00:29:50,270
then you can just check it out sorry for

00:29:49,190 --> 00:29:53,809
the slight mess up I don't know what

00:29:50,270 --> 00:29:56,720
happened there oh sorry and then there's

00:29:53,809 --> 00:29:58,490
this funny page called coding games

00:29:56,720 --> 00:30:00,290
where it's actually super fun you can

00:29:58,490 --> 00:30:02,480
code for fun there and they have a

00:30:00,290 --> 00:30:04,490
machine learning section now and there's

00:30:02,480 --> 00:30:06,380
stuff you can read about and there's

00:30:04,490 --> 00:30:09,620
also the link the second one to synaptic

00:30:06,380 --> 00:30:13,130
the framework so check it out and highly

00:30:09,620 --> 00:30:15,980
recommend it and finally I want to thank

00:30:13,130 --> 00:30:17,390
my master was not here he's actually my

00:30:15,980 --> 00:30:20,890
boyfriend and he helped me with

00:30:17,390 --> 00:30:23,450
implementing a lot of data pausing stuff

00:30:20,890 --> 00:30:26,270
transfer mark we crawl our data from

00:30:23,450 --> 00:30:29,809
their page and I don't really ask them I

00:30:26,270 --> 00:30:31,520
just did also I want to thank Jim drew

00:30:29,809 --> 00:30:33,679
for providing me the time and the

00:30:31,520 --> 00:30:35,179
resources to actually do that and of

00:30:33,679 --> 00:30:37,270
course all of you for your interest

00:30:35,179 --> 00:30:39,330
thanks a lot

00:30:37,270 --> 00:30:39,330

YouTube URL: https://www.youtube.com/watch?v=M5glN6XjDv8


