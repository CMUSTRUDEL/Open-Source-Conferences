Title: Nikolay Matvienko - Node js applications diagnostics under the hood
Publication date: 2018-04-11
Playlist: JSConf Australia 2018
Description: 
	Based on my experience in diagnosing and troubleshooting highload e-commerce Node.js app in production, Iâ€™ve identified three stages in the Node.js diagnostic history. Iâ€™d like to briefly tell this path and show where we are now and tell in details "How to debug and tracing Node.js apps on different levels, and how to track performance problems and memory leaks inside Node.js apps in 2018":

Production troubleshooting: I show and explain the strategy of creation and debugging core dump using llnode, node-report and X-Transaction-ID in our company with real-life examples.
Memory leaks: I show how to identify memory leaks, memory holders using V8 GC tracing, llnode debugging + gencore tool with real-life examples.
Performance: I show how to use 0x flame graph to find performance bottlenecks, how identify Event Loop lags, GC slow work and what can give Async Hooks for that with real-life examples.
Captions: 
	00:00:09,170 --> 00:00:15,300
hi everybody my name is Nikolai I am

00:00:13,110 --> 00:00:16,430
from Russia st. Petersburg and I will

00:00:15,300 --> 00:00:19,420
tell about

00:00:16,430 --> 00:00:21,950
application Diagnostics under the hood

00:00:19,420 --> 00:00:23,450
first of all sorry from English instead

00:00:21,950 --> 00:00:27,080
of improving heat I spent a lot of time

00:00:23,450 --> 00:00:29,240
under the hood with debuggers I work at

00:00:27,080 --> 00:00:31,369
grid dynamics and full stack developer

00:00:29,240 --> 00:00:33,920
and in the retail domain we do digital

00:00:31,369 --> 00:00:36,770
transformation for logic a u.s.

00:00:33,920 --> 00:00:38,809
ecommerce companies using non GS and in

00:00:36,770 --> 00:00:40,820
order to create our session successful

00:00:38,809 --> 00:00:43,879
we devote a lot of time to diagnostics

00:00:40,820 --> 00:00:45,470
and today not just no.6 has a working

00:00:43,879 --> 00:00:49,220
group that deals with performance

00:00:45,470 --> 00:00:52,340
profiling tracing memory analysis and

00:00:49,220 --> 00:00:53,780
post-mortem debuggers and to understand

00:00:52,340 --> 00:00:57,079
where we are now in Diagnostics I

00:00:53,780 --> 00:01:00,230
suggest you look at as a timeline as a

00:00:57,079 --> 00:01:03,350
history way I put the appearance of

00:01:00,230 --> 00:01:05,239
significant tools and highlighted

00:01:03,350 --> 00:01:07,940
several stage the first stage of course

00:01:05,239 --> 00:01:11,119
starts from 2009 where Naja's was

00:01:07,940 --> 00:01:12,500
presented at GS con cool but the part of

00:01:11,119 --> 00:01:15,470
people instruments appeared before

00:01:12,500 --> 00:01:18,890
synergist so as a sister profiler system

00:01:15,470 --> 00:01:20,659
profiler dtrace pair of instruments and

00:01:18,890 --> 00:01:23,780
they still in use

00:01:20,659 --> 00:01:27,080
after the nigiri appearance began to

00:01:23,780 --> 00:01:29,659
appear tools from the first stage keep

00:01:27,080 --> 00:01:31,640
down memory Road not inspector and

00:01:29,659 --> 00:01:34,130
basically those tools will develop

00:01:31,640 --> 00:01:36,470
effective in development and not in

00:01:34,130 --> 00:01:39,260
production and also do you remember

00:01:36,470 --> 00:01:41,270
there was a fork IGS which world more

00:01:39,260 --> 00:01:44,090
appeal is that not just itself so

00:01:41,270 --> 00:01:47,360
community was divided but since 2015

00:01:44,090 --> 00:01:49,549
IGS and not just you need uniting the

00:01:47,360 --> 00:01:51,740
fourth version community has a clear

00:01:49,549 --> 00:01:54,140
retro of development and by this point

00:01:51,740 --> 00:01:57,290
not already used in production by large

00:01:54,140 --> 00:02:00,590
companies such as eBay Alibaba Amazon

00:01:57,290 --> 00:02:04,220
and by our customers as well and here is

00:02:00,590 --> 00:02:07,460
a need for debugging and profiling tools

00:02:04,220 --> 00:02:10,160
in production will create flame graphs

00:02:07,460 --> 00:02:11,090
for profiling and C++ debuggers were

00:02:10,160 --> 00:02:14,269
adapted for

00:02:11,090 --> 00:02:16,160
Nagi as debugging so only in the second

00:02:14,269 --> 00:02:18,890
stage we've got the ability to debug and

00:02:16,160 --> 00:02:21,560
profile in production but we were there

00:02:18,890 --> 00:02:28,400
in the first stage already and it was

00:02:21,560 --> 00:02:29,120
funny and painful so from the 2005 17 I

00:02:28,400 --> 00:02:32,510
have highlighted

00:02:29,120 --> 00:02:34,340
the first stage on the timeline which is

00:02:32,510 --> 00:02:37,069
mostly focused on improving Nadia's

00:02:34,340 --> 00:02:40,700
performance tracing in internal

00:02:37,069 --> 00:02:44,000
components and improving the quality of

00:02:40,700 --> 00:02:47,360
instruments here is a significant

00:02:44,000 --> 00:02:50,360
tradition to turbofan appearance of a

00:02:47,360 --> 00:02:52,700
sink hooks so looks like the death today

00:02:50,360 --> 00:02:55,060
we have a lot of useful heat tools but

00:02:52,700 --> 00:02:57,980
most of them still are external and

00:02:55,060 --> 00:03:02,840
require a better functionality with the

00:02:57,980 --> 00:03:05,480
less performance overhead and summarize

00:03:02,840 --> 00:03:07,310
the pass in Diagnostics not GS was

00:03:05,480 --> 00:03:09,709
mostly focused on performance and there

00:03:07,310 --> 00:03:11,750
were reasons for that we often didn't

00:03:09,709 --> 00:03:14,209
have enough functionality and each time

00:03:11,750 --> 00:03:15,680
we transition to new version of Nijs we

00:03:14,209 --> 00:03:18,170
faced it with the fact that we lose

00:03:15,680 --> 00:03:20,359
support of many useful tools so we had

00:03:18,170 --> 00:03:22,790
to choose either we go with a new

00:03:20,359 --> 00:03:24,829
version which is faster but we lose

00:03:22,790 --> 00:03:28,640
support of useful tools all we stay with

00:03:24,829 --> 00:03:31,909
old slow but with the tools and other

00:03:28,640 --> 00:03:35,829
applications not just applications web

00:03:31,909 --> 00:03:38,030
UI began for front-end and microservices

00:03:35,829 --> 00:03:40,700
allocated in the center of large

00:03:38,030 --> 00:03:43,250
enterprise architecture where integrate

00:03:40,700 --> 00:03:47,389
we use a multiple systems such as CDM

00:03:43,250 --> 00:03:50,599
different databases services and problem

00:03:47,389 --> 00:03:52,359
can come here from anywhere and in the

00:03:50,599 --> 00:03:56,000
days of high load such as Black Friday

00:03:52,359 --> 00:03:58,370
when company make a huge profit each

00:03:56,000 --> 00:04:01,519
code each meet each mistake costs

00:03:58,370 --> 00:04:03,549
thousand dollars per minute and if there

00:04:01,519 --> 00:04:06,680
is an error in Nadia's application is

00:04:03,549 --> 00:04:08,870
necessary to choose a strategy and tools

00:04:06,680 --> 00:04:11,180
as soon as possible and prepare the hard

00:04:08,870 --> 00:04:13,280
fix and today I will show you how to do

00:04:11,180 --> 00:04:16,250
that in certain examples in case of

00:04:13,280 --> 00:04:19,340
errors performance loss and memory leaks

00:04:16,250 --> 00:04:21,650
in production so let's go to the

00:04:19,340 --> 00:04:24,139
debugging there are several reasons for

00:04:21,650 --> 00:04:26,300
debugging in production uncut exception

00:04:24,139 --> 00:04:28,070
kill your process physical reproducible

00:04:26,300 --> 00:04:30,889
errors and you don't have enough

00:04:28,070 --> 00:04:32,680
information from your logs and and

00:04:30,889 --> 00:04:35,210
actually you cannot lock everything

00:04:32,680 --> 00:04:37,940
production environment features and the

00:04:35,210 --> 00:04:41,540
error does not reproduce locally and you

00:04:37,940 --> 00:04:43,430
don't have a time let's

00:04:41,540 --> 00:04:47,330
moving to the simple example we have a

00:04:43,430 --> 00:04:50,210
controller which call service at work we

00:04:47,330 --> 00:04:51,860
use the REST API and the error if thrown

00:04:50,210 --> 00:04:54,440
in the controller and application

00:04:51,860 --> 00:04:56,630
crushed and here is a simple

00:04:54,440 --> 00:04:57,890
implementation of product reservation we

00:04:56,630 --> 00:05:00,500
have a product controller with the

00:04:57,890 --> 00:05:02,480
reserved method where user profile and

00:05:00,500 --> 00:05:05,690
reverse account data I extracted from

00:05:02,480 --> 00:05:08,180
the cookies and since some user doesn't

00:05:05,690 --> 00:05:11,080
have reverse account application Thrones

00:05:08,180 --> 00:05:15,170
and error your application crush and

00:05:11,080 --> 00:05:17,390
sauce it's bad but if it sells a flag

00:05:15,170 --> 00:05:19,880
about an architect section in case of

00:05:17,390 --> 00:05:24,110
uncut exception Nijs who create quorum

00:05:19,880 --> 00:05:26,270
which he is a whole which is dump of

00:05:24,110 --> 00:05:28,310
whole memory of the process that

00:05:26,270 --> 00:05:30,740
contains heap dump and stack trace

00:05:28,310 --> 00:05:32,540
before the crash after that your

00:05:30,740 --> 00:05:36,290
application restarts you get access to

00:05:32,540 --> 00:05:38,990
production to the dump and read it with

00:05:36,290 --> 00:05:42,650
the debugger the most relevant today is

00:05:38,990 --> 00:05:45,110
LLL not plug in for l-ltp debugger so

00:05:42,650 --> 00:05:47,600
and to find the error we need to do just

00:05:45,110 --> 00:05:48,860
three simple steps so we need to get the

00:05:47,600 --> 00:05:51,670
stack trace and finds the last

00:05:48,860 --> 00:05:53,960
JavaScript operation on function and

00:05:51,670 --> 00:05:56,750
read the source code of this function

00:05:53,960 --> 00:05:59,870
and get the input parameters request and

00:05:56,750 --> 00:06:02,990
the response so let's repeat it with the

00:05:59,870 --> 00:06:05,240
core dumps we have a core dump and to

00:06:02,990 --> 00:06:08,560
read it we need to run ll node and

00:06:05,240 --> 00:06:12,170
specify the path to the dump after that

00:06:08,560 --> 00:06:15,710
in debugger session we get access to

00:06:12,170 --> 00:06:19,280
reach the trace with the common two 8bt

00:06:15,710 --> 00:06:21,080
which is a back trace and that here we

00:06:19,280 --> 00:06:25,130
need to find the last J's function which

00:06:21,080 --> 00:06:27,200
is store it in 6 frames frame and here

00:06:25,130 --> 00:06:30,320
we see this is a reserved method of our

00:06:27,200 --> 00:06:31,660
product controller and we have addresses

00:06:30,320 --> 00:06:34,400
to this object

00:06:31,660 --> 00:06:37,070
incoming message which is a which is a

00:06:34,400 --> 00:06:40,430
request response and even to the

00:06:37,070 --> 00:06:42,140
function source code in the memory so we

00:06:40,430 --> 00:06:45,170
need to read the source code of our

00:06:42,140 --> 00:06:47,570
functions or less just gets address and

00:06:45,170 --> 00:06:50,030
using command v8 inspect prints source

00:06:47,570 --> 00:06:52,190
with a dress on function we get the

00:06:50,030 --> 00:06:54,710
source code of this function out of the

00:06:52,190 --> 00:06:55,190
memory after the reading source code

00:06:54,710 --> 00:06:57,080
letter

00:06:55,190 --> 00:07:01,250
Jim back to the start race and get the

00:06:57,080 --> 00:07:04,550
address of our request and extract from

00:07:01,250 --> 00:07:06,860
the memory and here it is so this is our

00:07:04,550 --> 00:07:09,370
request we did it with the common to

00:07:06,860 --> 00:07:12,710
eight inspect and use it address of

00:07:09,370 --> 00:07:15,740
request and after that we can repeat

00:07:12,710 --> 00:07:18,680
this common together cookies to figure

00:07:15,740 --> 00:07:21,290
out which profile doesn't have rewards

00:07:18,680 --> 00:07:26,090
and wipe our application across so

00:07:21,290 --> 00:07:28,850
that's pretty simple but it was a good

00:07:26,090 --> 00:07:31,280
stack trace informative decrees and

00:07:28,850 --> 00:07:34,400
because we had all necessary information

00:07:31,280 --> 00:07:37,520
we had order of last functions with

00:07:34,400 --> 00:07:41,480
addresses to the request response or all

00:07:37,520 --> 00:07:43,460
object but sometimes a framework error

00:07:41,480 --> 00:07:45,940
handler or incorrect customer error

00:07:43,460 --> 00:07:54,230
handling makes a trace uninformative

00:07:45,940 --> 00:07:57,530
consider this at the next example so

00:07:54,230 --> 00:07:59,360
here instead of pass exactly and rewards

00:07:57,530 --> 00:08:01,760
ID we accidentally positive whole

00:07:59,360 --> 00:08:03,770
rewards object and now the error is

00:08:01,760 --> 00:08:06,230
wrong in the surveys in a synchronous

00:08:03,770 --> 00:08:08,900
mode and we don't have a cache block

00:08:06,230 --> 00:08:12,169
here to catch the error and to catch it

00:08:08,900 --> 00:08:14,660
we use a subscription to an hello to

00:08:12,169 --> 00:08:17,480
rejection event where we will do process

00:08:14,660 --> 00:08:20,960
abort to crash we use to get the dump

00:08:17,480 --> 00:08:21,740
and when we try to gather stack trace of

00:08:20,960 --> 00:08:24,290
this dump

00:08:21,740 --> 00:08:26,510
we will not find any find anniversary

00:08:24,290 --> 00:08:31,100
information so we don't have requests

00:08:26,510 --> 00:08:33,530
respawns and needed functions so we

00:08:31,100 --> 00:08:35,630
don't have a stack trace and in this

00:08:33,530 --> 00:08:38,089
case we need to you work just with the

00:08:35,630 --> 00:08:41,060
heap dump with the memory and I'll show

00:08:38,089 --> 00:08:43,190
you how to do this first of all we need

00:08:41,060 --> 00:08:47,930
to get the requests from the memory and

00:08:43,190 --> 00:08:50,240
we can find objects by the type using

00:08:47,930 --> 00:08:53,210
common fine J's instances and specify in

00:08:50,240 --> 00:08:55,640
comic message so those are requests in

00:08:53,210 --> 00:08:59,780
the dump in the memory but while there

00:08:55,640 --> 00:09:02,720
are so many and which request crushes

00:08:59,780 --> 00:09:05,390
the process with all other requests that

00:09:02,720 --> 00:09:07,220
will there inside the event loop and how

00:09:05,390 --> 00:09:09,920
to find it

00:09:07,220 --> 00:09:12,259
and in this case we use a unique request

00:09:09,920 --> 00:09:16,129
ID which we store in the header even

00:09:12,259 --> 00:09:20,649
extraction ID to find the right one and

00:09:16,129 --> 00:09:25,399
we can get this value from our logs and

00:09:20,649 --> 00:09:27,439
find which object has has a reference to

00:09:25,399 --> 00:09:30,709
the string to this value with the

00:09:27,439 --> 00:09:34,009
comment find revs - - drink and we have

00:09:30,709 --> 00:09:37,069
a headers object so let's find the

00:09:34,009 --> 00:09:40,579
parent object of this header and using

00:09:37,069 --> 00:09:43,850
command v8 find revs value and others of

00:09:40,579 --> 00:09:49,069
headers we found the incoming message as

00:09:43,850 --> 00:09:51,589
a request that we need hi Dave so that

00:09:49,069 --> 00:09:53,870
was easy let's go deeper and imagine

00:09:51,589 --> 00:09:56,060
that this data is not enough on us for

00:09:53,870 --> 00:09:59,240
us and we need to know

00:09:56,060 --> 00:10:01,399
local variables from other other

00:09:59,240 --> 00:10:05,149
functions for example from the reserve

00:10:01,399 --> 00:10:07,610
service where was request REST API and

00:10:05,149 --> 00:10:09,290
request database and results we'll

00:10:07,610 --> 00:10:12,350
assign it to the same local variable

00:10:09,290 --> 00:10:15,170
data and we don't have any objects

00:10:12,350 --> 00:10:18,769
referencing to this data how to find it

00:10:15,170 --> 00:10:22,779
but we can find this object by values of

00:10:18,769 --> 00:10:26,269
its properties for example by reverse ID

00:10:22,779 --> 00:10:29,720
user ID or by even by the name of some

00:10:26,269 --> 00:10:32,779
property for example reverse and store

00:10:29,720 --> 00:10:36,079
so the main idea is here that we can

00:10:32,779 --> 00:10:39,019
find the objects not only from the last

00:10:36,079 --> 00:10:40,850
function before the crash we can get

00:10:39,019 --> 00:10:43,220
them out of memory from all other

00:10:40,850 --> 00:10:45,050
functions with high probability that

00:10:43,220 --> 00:10:47,089
were executed before the crash from

00:10:45,050 --> 00:10:49,939
sample from middleware from policies

00:10:47,089 --> 00:10:54,829
from controller or other storage service

00:10:49,939 --> 00:10:56,569
and storage and it helps a lot and to

00:10:54,829 --> 00:10:59,180
get more information we can use an odd

00:10:56,569 --> 00:11:01,009
report and in fact the search for an

00:10:59,180 --> 00:11:03,500
error it's like a data to investigation

00:11:01,009 --> 00:11:06,230
where after generated before the crash

00:11:03,500 --> 00:11:09,139
report will say you what happened with

00:11:06,230 --> 00:11:11,809
your process by JavaScript and C++ stack

00:11:09,139 --> 00:11:14,420
trace where it was and under what

00:11:11,809 --> 00:11:19,040
conditions by worker of garbage

00:11:14,420 --> 00:11:21,670
collector memory and CPU and logs will

00:11:19,040 --> 00:11:25,190
give you valuable hints

00:11:21,670 --> 00:11:27,440
but core dump is not a silver bullet and

00:11:25,190 --> 00:11:28,220
to create core Dom on each error is not

00:11:27,440 --> 00:11:30,020
a good idea

00:11:28,220 --> 00:11:32,000
otherwise the picture will be like this

00:11:30,020 --> 00:11:35,210
we have a lot of errors in production

00:11:32,000 --> 00:11:40,510
and you know the spell not a bottle and

00:11:35,210 --> 00:11:40,510
cut exception so let the debugging begin

00:11:41,020 --> 00:11:45,290
for each error is created core dump

00:11:43,700 --> 00:11:49,540
one core dump took her down freak or

00:11:45,290 --> 00:11:53,060
damn a lot a lot a lot of core dumps and

00:11:49,540 --> 00:11:55,820
bam and your boss is waiting for you and

00:11:53,060 --> 00:11:57,890
he asked she asked hey buddy will lose

00:11:55,820 --> 00:12:01,220
money how much time you need to solve

00:11:57,890 --> 00:12:05,420
this issue and you think him where is my

00:12:01,220 --> 00:12:08,420
dump so I call the situation core dump

00:12:05,420 --> 00:12:12,470
flood where is the problem we need to

00:12:08,420 --> 00:12:14,600
have we need to follow to error handle

00:12:12,470 --> 00:12:17,090
best practices to have a central asset

00:12:14,600 --> 00:12:19,310
error handler with error selection logic

00:12:17,090 --> 00:12:21,440
for example I have a hearty producible

00:12:19,310 --> 00:12:23,690
error but I don't want to generate

00:12:21,440 --> 00:12:26,540
hundreds of identical core dumps and

00:12:23,690 --> 00:12:28,760
look for them among other all I don't

00:12:26,540 --> 00:12:31,850
want to create core dump and kill the

00:12:28,760 --> 00:12:32,570
process every time on error when I can

00:12:31,850 --> 00:12:35,780
handle it

00:12:32,570 --> 00:12:39,320
and in this case I use the gen core and

00:12:35,780 --> 00:12:41,900
error registry which is a custom error

00:12:39,320 --> 00:12:45,440
selection logic which makes decision

00:12:41,900 --> 00:12:49,310
should it create core dump a was for

00:12:45,440 --> 00:12:51,620
this error dump craters already or is

00:12:49,310 --> 00:12:58,220
this a heart irreducible error and if it

00:12:51,620 --> 00:13:00,070
should we use a gem core that makes fork

00:12:58,220 --> 00:13:03,590
of you of your main process and

00:13:00,070 --> 00:13:06,290
terminate it with the dump and our main

00:13:03,590 --> 00:13:09,440
process continue serving traffic without

00:13:06,290 --> 00:13:12,850
accident so that's pretty cool enough

00:13:09,440 --> 00:13:16,340
and let's reduce the whole algorithm

00:13:12,850 --> 00:13:21,470
what last time so if the error is not

00:13:16,340 --> 00:13:24,730
cut not just application generates core

00:13:21,470 --> 00:13:26,720
dump not report generates report and

00:13:24,730 --> 00:13:29,350
application restarts the continue

00:13:26,720 --> 00:13:32,060
serving traffic but if the error was cut

00:13:29,350 --> 00:13:34,040
error handler pulled the available

00:13:32,060 --> 00:13:36,770
information to a log

00:13:34,040 --> 00:13:39,710
and error selection logic make decision

00:13:36,770 --> 00:13:43,460
should it crush the process should have

00:13:39,710 --> 00:13:45,920
generate a core dump and if not it

00:13:43,460 --> 00:13:49,130
continue serving traffic and handles the

00:13:45,920 --> 00:13:51,530
error and if it should it use GM core to

00:13:49,130 --> 00:13:53,750
make a process fork to terminate it with

00:13:51,530 --> 00:13:57,380
the dump and our main process continue

00:13:53,750 --> 00:14:01,790
serving traffic so after that we just

00:13:57,380 --> 00:14:05,870
connect to production get local look and

00:14:01,790 --> 00:14:09,200
our core down to the bucket we ll not is

00:14:05,870 --> 00:14:11,810
it so my suggestions follow to error

00:14:09,200 --> 00:14:13,880
handler best practices call abort only

00:14:11,810 --> 00:14:17,270
in one place in central hazard error

00:14:13,880 --> 00:14:21,710
handler usage angkor do not kill your

00:14:17,270 --> 00:14:23,980
main process and use a non report to

00:14:21,710 --> 00:14:28,670
have more information in case of

00:14:23,980 --> 00:14:31,100
unexpected crush track your ID using

00:14:28,670 --> 00:14:33,820
request IDs for example X request ID

00:14:31,100 --> 00:14:38,150
from engineering or Zipkin headers and

00:14:33,820 --> 00:14:45,680
avoid core dump flood that was about

00:14:38,150 --> 00:14:47,300
debugging in production so let's go to

00:14:45,680 --> 00:14:50,030
the performance profiling and search for

00:14:47,300 --> 00:14:53,690
memory leaks and let's investigate the

00:14:50,030 --> 00:14:56,120
performance loss and memory leaks at an

00:14:53,690 --> 00:14:58,940
example of well real situation when

00:14:56,120 --> 00:15:02,840
after the one of the religious we had a

00:14:58,940 --> 00:15:07,610
huge performance decline just before the

00:15:02,840 --> 00:15:10,060
Black Friday Eve and to do to figure out

00:15:07,610 --> 00:15:12,230
was where as a problem we can start with

00:15:10,060 --> 00:15:15,890
performance profiling using internal

00:15:12,230 --> 00:15:18,560
profiler of Nijs and tick processor to

00:15:15,890 --> 00:15:22,160
find the longest separation but there is

00:15:18,560 --> 00:15:24,650
a there is a good graphical

00:15:22,160 --> 00:15:28,130
representation of profile at trace its

00:15:24,650 --> 00:15:30,980
attacks its call it flame growth and the

00:15:28,130 --> 00:15:33,380
main idea of the feed to find the

00:15:30,980 --> 00:15:35,510
wireless block because the wider block

00:15:33,380 --> 00:15:38,030
is them often it was during is a

00:15:35,510 --> 00:15:41,110
sampling CPU so the long gate what

00:15:38,030 --> 00:15:45,440
performative and on high-level overview

00:15:41,110 --> 00:15:47,420
we can find that application spends 10

00:15:45,440 --> 00:15:51,950
percent on the retrieval data from 8

00:15:47,420 --> 00:15:54,380
I 20% unload operation with data and the

00:15:51,950 --> 00:15:58,570
longest block is a server-side page

00:15:54,380 --> 00:16:01,310
rendering it's about 65% of time and

00:15:58,570 --> 00:16:04,760
it's a lot because it can't East consist

00:16:01,310 --> 00:16:08,690
from custom helpers partials and tea

00:16:04,760 --> 00:16:10,940
plates templates with logic and if we

00:16:08,690 --> 00:16:18,050
scroll up we will see which operations

00:16:10,940 --> 00:16:22,700
are on CPU here is handlebars and is low

00:16:18,050 --> 00:16:26,810
- Mary utils and let's go deeper to low

00:16:22,700 --> 00:16:28,790
- merge stacks and if we zoom in we will

00:16:26,810 --> 00:16:33,020
see so there are two white blocks of

00:16:28,790 --> 00:16:36,920
flow - base merge operation base merge

00:16:33,020 --> 00:16:40,190
deep clone the for each cycle and for

00:16:36,920 --> 00:16:42,560
data processing of product matter so we

00:16:40,190 --> 00:16:47,230
found that our helpers logic in team

00:16:42,560 --> 00:16:51,590
played a lot of cycles low - utils

00:16:47,230 --> 00:16:52,310
our performance is the cause of lowest

00:16:51,590 --> 00:16:55,640
performance

00:16:52,310 --> 00:16:59,420
it's our bottlenecks and to improve that

00:16:55,640 --> 00:17:04,640
we can we did performance profiler with

00:16:59,420 --> 00:17:07,220
a pair of 0 x pcc tools and what we did

00:17:04,640 --> 00:17:10,280
we reduce the punitive operations since

00:17:07,220 --> 00:17:14,510
it blocks our they block our main

00:17:10,280 --> 00:17:16,910
process long cycles decent Parslow -

00:17:14,510 --> 00:17:20,530
object merge and remove it logic from

00:17:16,910 --> 00:17:23,870
buting place and helpers especially

00:17:20,530 --> 00:17:26,630
custom halters and gradually implemented

00:17:23,870 --> 00:17:28,430
server-side rendering and after those

00:17:26,630 --> 00:17:33,080
all improvements all the server-side

00:17:28,430 --> 00:17:36,850
rendering expands only about 15% of main

00:17:33,080 --> 00:17:40,340
time and we reduce it wild blood and

00:17:36,850 --> 00:17:45,770
increase it our synchrony of application

00:17:40,340 --> 00:17:48,380
execution so my suggestions performance

00:17:45,770 --> 00:17:51,290
should be part of requirements every

00:17:48,380 --> 00:17:54,410
time if you would like to add some

00:17:51,290 --> 00:17:56,270
library third party library or your

00:17:54,410 --> 00:17:59,360
library you need to measure performance

00:17:56,270 --> 00:18:00,980
before and after every time collect the

00:17:59,360 --> 00:18:03,799
measurement result or heap

00:18:00,980 --> 00:18:06,520
and profile on different environments on

00:18:03,799 --> 00:18:11,090
stage environments and maneet or

00:18:06,520 --> 00:18:14,120
diagnostic tools performance impact but

00:18:11,090 --> 00:18:15,950
misfortune never comes alone and with

00:18:14,120 --> 00:18:18,590
performance loss we have a problem with

00:18:15,950 --> 00:18:21,200
his memory consumption and there are

00:18:18,590 --> 00:18:23,059
many ways how to define memory leaks we

00:18:21,200 --> 00:18:27,169
can do it with application monitoring

00:18:23,059 --> 00:18:31,490
tools like an solid New Relic DTrace

00:18:27,169 --> 00:18:35,030
perf compare heap dump but I suggest you

00:18:31,490 --> 00:18:36,650
to focus on excessive garbage collection

00:18:35,030 --> 00:18:37,250
since it makes negative impact on

00:18:36,650 --> 00:18:44,929
performance

00:18:37,250 --> 00:18:48,530
look at this nearly graph the GC rate

00:18:44,929 --> 00:18:51,020
grows in a new space young generation

00:18:48,530 --> 00:18:53,980
and to get more information about

00:18:51,020 --> 00:18:59,559
garbage collection work we can trace it

00:18:53,980 --> 00:19:03,290
using tracers GC flag of Nadia's and

00:18:59,559 --> 00:19:05,630
here we see that not said memory

00:19:03,290 --> 00:19:08,360
consumption grows after each collection

00:19:05,630 --> 00:19:13,280
and interval between collection is too

00:19:08,360 --> 00:19:17,770
short that means that it's difficult to

00:19:13,280 --> 00:19:20,179
find to find them in the memory so and

00:19:17,770 --> 00:19:23,780
garbage collection performs very

00:19:20,179 --> 00:19:27,320
frequently and by the duration of

00:19:23,780 --> 00:19:31,580
garbage collection we see there are 16

00:19:27,320 --> 00:19:34,340
16 milliseconds 17 and the more than one

00:19:31,580 --> 00:19:37,360
default millisecond so it that means

00:19:34,340 --> 00:19:40,010
that garbage collection spend more time

00:19:37,360 --> 00:19:42,650
together objects in the memory because

00:19:40,010 --> 00:19:45,410
of complex topology complex graph of

00:19:42,650 --> 00:19:49,730
objects in the memory and it spend more

00:19:45,410 --> 00:19:52,610
time to kalam the memory and how to

00:19:49,730 --> 00:19:55,360
track which operations require memory

00:19:52,610 --> 00:19:58,610
allocation and as we know Naja's

00:19:55,360 --> 00:20:02,150
profiling result receipts tax it's a mix

00:19:58,610 --> 00:20:05,330
of JavaScript function and native v8

00:20:02,150 --> 00:20:08,059
functions so why not just keep track way

00:20:05,330 --> 00:20:11,330
back from garbage collection functions

00:20:08,059 --> 00:20:13,740
calls to JavaScript so let's look at a

00:20:11,330 --> 00:20:17,220
simple implementation of flame graph

00:20:13,740 --> 00:20:22,740
and on the left side where CMS helper

00:20:17,220 --> 00:20:24,690
did insert it use string replies which

00:20:22,740 --> 00:20:30,750
required memory allocation from you one

00:20:24,690 --> 00:20:32,940
use drink and after that since memory

00:20:30,750 --> 00:20:35,940
allocation was failed garbage collection

00:20:32,940 --> 00:20:39,600
called collec garbage and what we can do

00:20:35,940 --> 00:20:43,200
we can find scavenge objects law look at

00:20:39,600 --> 00:20:46,169
text representation command and figure

00:20:43,200 --> 00:20:48,710
out by way back which JavaScript

00:20:46,169 --> 00:20:53,010
function required memory allocation and

00:20:48,710 --> 00:20:57,270
clean at lamps memory so and same as

00:20:53,010 --> 00:20:59,970
helper is a problem the same example

00:20:57,270 --> 00:21:02,940
with the product matter which did the

00:20:59,970 --> 00:21:06,539
map of big array and since it required

00:21:02,940 --> 00:21:08,010
memory for new items and garbage

00:21:06,539 --> 00:21:11,610
collection called

00:21:08,010 --> 00:21:13,649
scavenge object and allocation step so

00:21:11,610 --> 00:21:19,770
as I said we would use it

00:21:13,649 --> 00:21:23,429
our mappers clones be good JavaScript

00:21:19,770 --> 00:21:27,779
cycles and custom helpers and in the

00:21:23,429 --> 00:21:30,090
result after that our memory can doesn't

00:21:27,779 --> 00:21:32,399
doesn't grow and garbage collection is

00:21:30,090 --> 00:21:35,970
little more than one default millisecond

00:21:32,399 --> 00:21:39,750
but we think it's ok and the same

00:21:35,970 --> 00:21:43,470
problem was in in all generation where

00:21:39,750 --> 00:21:46,860
he is a long garbage collection and to

00:21:43,470 --> 00:21:49,470
find miss it closures or timers usually

00:21:46,860 --> 00:21:50,909
we use the heap dump we compare them but

00:21:49,470 --> 00:21:53,159
I'll show you how to do the same with

00:21:50,909 --> 00:21:56,820
the core dump in case of fatal error

00:21:53,159 --> 00:21:59,429
when you get the core dump so what we

00:21:56,820 --> 00:22:01,679
need to do we need to get a report of

00:21:59,429 --> 00:22:04,230
all JavaScript object inside the memory

00:22:01,679 --> 00:22:08,640
we can do it with comment we ate find

00:22:04,230 --> 00:22:10,529
objects GS objects and incoming mass

00:22:08,640 --> 00:22:13,409
large of incoming messages look

00:22:10,529 --> 00:22:16,890
suspicious so let's figure out where

00:22:13,409 --> 00:22:19,320
those requests allocated so we need to

00:22:16,890 --> 00:22:22,409
get all of them from the memory choose

00:22:19,320 --> 00:22:25,740
the first one for example and let take a

00:22:22,409 --> 00:22:26,490
look where are these request is located

00:22:25,740 --> 00:22:29,160
so

00:22:26,490 --> 00:22:33,510
this is a reference from server responds

00:22:29,160 --> 00:22:35,429
to requests and from one big array so

00:22:33,510 --> 00:22:37,950
let's figure out where is this array

00:22:35,429 --> 00:22:40,920
located and this comment we ate find

00:22:37,950 --> 00:22:43,230
revs value and address of this array we

00:22:40,920 --> 00:22:45,690
can find the parent object and this is

00:22:43,230 --> 00:22:49,740
profile middleware with the request

00:22:45,690 --> 00:22:54,600
property so we found which object holds

00:22:49,740 --> 00:22:57,210
memory my suggestions controls the

00:22:54,600 --> 00:22:59,970
lifecycle of objects so you need to know

00:22:57,210 --> 00:23:03,360
where your object was created and where

00:22:59,970 --> 00:23:05,910
to it will be removed trace work of

00:23:03,360 --> 00:23:07,559
garbage collector and if you have if

00:23:05,910 --> 00:23:09,960
you're not sure about the behavior of

00:23:07,559 --> 00:23:13,980
JavaScript behavior maybe in case of

00:23:09,960 --> 00:23:18,059
closures you can download and use d8

00:23:13,980 --> 00:23:20,610
which is the debug 8 4 we ate and check

00:23:18,059 --> 00:23:23,040
the JavaScript behavior called

00:23:20,610 --> 00:23:28,620
JavaScript manually or read a memory

00:23:23,040 --> 00:23:31,590
usage test using leakage and create heap

00:23:28,620 --> 00:23:32,690
dump and core down to investigate and

00:23:31,590 --> 00:23:38,640
find the problem

00:23:32,690 --> 00:23:41,280
so finally we lose performance with

00:23:38,640 --> 00:23:43,830
growth of functionality until it became

00:23:41,280 --> 00:23:47,070
critical for us and after all in all

00:23:43,830 --> 00:23:49,470
those improvements we achieved in

00:23:47,070 --> 00:23:53,730
increasing our performance by half from

00:23:49,470 --> 00:23:55,470
the original baseline and to prevent a

00:23:53,730 --> 00:23:58,740
situation like this one you can choose

00:23:55,470 --> 00:24:00,179
my tips my suggestions because not GS

00:23:58,740 --> 00:24:03,809
was created as a fast performance

00:24:00,179 --> 00:24:07,190
platform and we as end users should

00:24:03,809 --> 00:24:10,910
develop fast performance applications

00:24:07,190 --> 00:24:10,910
that's all thank you

00:24:12,820 --> 00:24:14,880
you

00:24:21,830 --> 00:24:23,890

YouTube URL: https://www.youtube.com/watch?v=y2PRfNsY28w


