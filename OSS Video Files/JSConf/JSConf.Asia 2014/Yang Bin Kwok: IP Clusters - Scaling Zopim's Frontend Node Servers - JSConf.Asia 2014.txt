Title: Yang Bin Kwok: IP Clusters - Scaling Zopim's Frontend Node Servers - JSConf.Asia 2014
Publication date: 2014-12-17
Playlist: JSConf.Asia 2014
Description: 
	Realtime applications are easy with Node, but how do you scale out when the load hits? Zopim shares the lessons learnt at scale, as well as their approach to enterprise class software of a simplified network topology eschewing dedicated load balancers and firewalls.

Yang Bin is a co-founder of Zopim and currently works on simplifying the distribution of realtime data across their POPs and the frontend. One day, he hopes to open source Zopimâ€™s web application framework, but in the meantime he dabbles in image processing, visualizations and chinchillas.

JSConf.Asia is the JavaScript, web and mobile developer conference for Asia. Amara Sanctuary, Singapore - 20 + 21 November 2014.

Source: http://2014.jsconf.asia/#speakers
Project link: http://github.com/zopim/ipcluster

License: For reuse of this video under a more permissive license please get in touch with us. The speakers retain the copyright for their performances.
Captions: 
	00:00:14,299 --> 00:00:23,580
hi everyone today my talk will be my

00:00:20,760 --> 00:00:26,849
talk will be on the scaling lesson is

00:00:23,580 --> 00:00:28,710
that took him learn over the past 45

00:00:26,849 --> 00:00:32,309
years using the GSN production in

00:00:28,710 --> 00:00:34,739
problem and the second part my talk will

00:00:32,309 --> 00:00:36,630
be on IP cluster which is a small

00:00:34,739 --> 00:00:39,420
library that we have open we have just

00:00:36,630 --> 00:00:45,180
open source yesterday to help us with

00:00:39,420 --> 00:00:47,160
scaling no services so I'm young bin my

00:00:45,180 --> 00:00:49,710
friends just call you ID i'm a

00:00:47,160 --> 00:00:53,960
co-founder also pin so we is the best

00:00:49,710 --> 00:00:57,480
live chat company in the world it seems

00:00:53,960 --> 00:00:59,550
150,000 websites we get 3 billion

00:00:57,480 --> 00:01:03,059
impressions on our chat widget month and

00:00:59,550 --> 00:01:08,070
a pic we see 500,000 concurrent

00:01:03,059 --> 00:01:10,350
connections so in two thousand Canada

00:01:08,070 --> 00:01:12,600
doing nov 9 and 10 we decided that we

00:01:10,350 --> 00:01:18,900
wanted to rewrite our dashboard after

00:01:12,600 --> 00:01:21,060
three years from flex to html5 so at a

00:01:18,900 --> 00:01:23,909
point flag set have been really good to

00:01:21,060 --> 00:01:27,150
us because this off all the cross

00:01:23,909 --> 00:01:29,159
browser issues and they have TCP socket

00:01:27,150 --> 00:01:33,060
support so we could have a chance of

00:01:29,159 --> 00:01:36,090
survival for four orders codon streaming

00:01:33,060 --> 00:01:38,070
data but we were getting running the

00:01:36,090 --> 00:01:40,070
morn problems because we couldn't it was

00:01:38,070 --> 00:01:43,829
difficult hire people at new flex and

00:01:40,070 --> 00:01:47,540
also it was extremely difficult to

00:01:43,829 --> 00:01:52,049
customize so designers are not happy so

00:01:47,540 --> 00:01:55,020
we decided to do everything in html5

00:01:52,049 --> 00:01:56,969
again and apply sauna lessons that we

00:01:55,020 --> 00:01:58,770
learned pitting our dashboard so for

00:01:56,969 --> 00:02:01,200
instance in the new version of our

00:01:58,770 --> 00:02:04,909
dashboard we decided that the entire

00:02:01,200 --> 00:02:09,750
application state would be based on

00:02:04,909 --> 00:02:11,610
buying the body that so in the Flex

00:02:09,750 --> 00:02:13,060
version we had usually have made an API

00:02:11,610 --> 00:02:15,849
call in a setup fiction

00:02:13,060 --> 00:02:18,340
to put data in and that was very tedious

00:02:15,849 --> 00:02:19,750
and it meant that some parts of

00:02:18,340 --> 00:02:22,870
application that when you if you know

00:02:19,750 --> 00:02:25,450
about it to set this up you had hot

00:02:22,870 --> 00:02:27,810
problems like when agents are entered

00:02:25,450 --> 00:02:31,989
and so on they do not reflect on you I

00:02:27,810 --> 00:02:34,300
so we decided that we should write a set

00:02:31,989 --> 00:02:36,030
of not GNE servers that will help to

00:02:34,300 --> 00:02:38,769
translate the data from our back-end

00:02:36,030 --> 00:02:42,430
into a format next month I just over by

00:02:38,769 --> 00:02:46,060
our front end so note was a good fit for

00:02:42,430 --> 00:02:50,250
us at a time it was a very familiar

00:02:46,060 --> 00:02:53,080
environment and you can think of our our

00:02:50,250 --> 00:02:57,580
mediator service as we call them as a

00:02:53,080 --> 00:03:00,280
kind of dynamic CDN which which means we

00:02:57,580 --> 00:03:02,140
kind of cash or dynamic information in

00:03:00,280 --> 00:03:04,150
our servers around the world so

00:03:02,140 --> 00:03:05,709
distributing also was wrong we also give

00:03:04,150 --> 00:03:12,040
us additional benefits that lower

00:03:05,709 --> 00:03:16,420
latency is to our end users and being

00:03:12,040 --> 00:03:20,560
able to handle the load for different

00:03:16,420 --> 00:03:27,220
parts of the world so no Jess really

00:03:20,560 --> 00:03:28,870
rocked it was really fast so it looks

00:03:27,220 --> 00:03:31,000
good but the problem is of course number

00:03:28,870 --> 00:03:33,970
communities a pressure model which is

00:03:31,000 --> 00:03:36,700
the typical no problems which is a

00:03:33,970 --> 00:03:38,230
guys single credit and we have memory

00:03:36,700 --> 00:03:42,130
leaks in our applications they are to

00:03:38,230 --> 00:03:43,959
track down so all problems in computer

00:03:42,130 --> 00:03:47,799
science and software play of redirection

00:03:43,959 --> 00:03:50,380
right so easy way out interest to buy a

00:03:47,799 --> 00:03:53,470
couple of load balancers proxies SSL

00:03:50,380 --> 00:03:54,489
terminators etc but because we will

00:03:53,470 --> 00:03:56,380
start off with in have that kind of

00:03:54,489 --> 00:03:58,660
money so we had to be a bit more

00:03:56,380 --> 00:04:03,100
creative with how we try to schedule our

00:03:58,660 --> 00:04:06,970
our server so these are some of the

00:04:03,100 --> 00:04:10,180
things that we try obviously the beauty

00:04:06,970 --> 00:04:12,069
in cluster module which works well from

00:04:10,180 --> 00:04:15,190
hitch to be repairs but because we had

00:04:12,069 --> 00:04:17,109
our service was live chat we had

00:04:15,190 --> 00:04:19,690
persistent connection with our users so

00:04:17,109 --> 00:04:21,640
that you can stream the data and this

00:04:19,690 --> 00:04:24,940
really breaks long polling connections

00:04:21,640 --> 00:04:26,710
because each additional each each time

00:04:24,940 --> 00:04:28,240
the long four finishers

00:04:26,710 --> 00:04:29,199
the browser will connect to a different

00:04:28,240 --> 00:04:31,960
server and then you lose your whole

00:04:29,199 --> 00:04:33,759
application state so there was out at a

00:04:31,960 --> 00:04:35,500
time there was a web sockets for ngx

00:04:33,759 --> 00:04:40,259
body was at a party module and was not

00:04:35,500 --> 00:04:47,800
very live we looked at his a proxy and

00:04:40,259 --> 00:04:50,440
start so history proxy is is a kind of

00:04:47,800 --> 00:04:51,789
dumb proxy so the biggest problem

00:04:50,440 --> 00:04:54,250
hitters proxy was that there was no we

00:04:51,789 --> 00:04:57,759
could get our client IP address in our

00:04:54,250 --> 00:04:59,680
application because we're using SSL and

00:04:57,759 --> 00:05:02,289
he proceeded as i understand ssl alright

00:04:59,680 --> 00:05:05,949
so we looked at stat which is the ssl

00:05:02,289 --> 00:05:08,229
terminator so what I means is your

00:05:05,949 --> 00:05:11,759
browser makes an ssl connection to start

00:05:08,229 --> 00:05:14,169
start on reps that ssl encryption and

00:05:11,759 --> 00:05:17,710
makes it normal TZ connection to your

00:05:14,169 --> 00:05:18,970
server so you don't have to end ssl and

00:05:17,710 --> 00:05:23,560
all the decoding or the decryption

00:05:18,970 --> 00:05:25,270
standbys by the most start so the

00:05:23,560 --> 00:05:27,880
application as much if you do its own

00:05:25,270 --> 00:05:30,400
thing and you don't have to handle it

00:05:27,880 --> 00:05:35,590
you can handle anything from you as well

00:05:30,400 --> 00:05:37,270
so when i start for a while but and in

00:05:35,590 --> 00:05:38,740
the next slide I will talk about how we

00:05:37,270 --> 00:05:42,370
start the problem trying to get the

00:05:38,740 --> 00:05:45,729
client IP address from stuff we also

00:05:42,370 --> 00:05:49,900
look at forwarding soccer file

00:05:45,729 --> 00:05:53,380
descriptors in Linux you can send the

00:05:49,900 --> 00:05:55,900
file descriptor between processors so

00:05:53,380 --> 00:05:57,849
for tcp doesn't work because you could

00:05:55,900 --> 00:06:00,880
just send the farthest planet process

00:05:57,849 --> 00:06:04,210
and and we can write off the 40 script

00:06:00,880 --> 00:06:05,440
but we were using SSL so that kind of

00:06:04,210 --> 00:06:07,479
poke the whole thing because esta has

00:06:05,440 --> 00:06:14,949
this really opaque state that you can't

00:06:07,479 --> 00:06:18,070
stand over socket so let me show you how

00:06:14,949 --> 00:06:22,050
we managed to capture our client padres

00:06:18,070 --> 00:06:24,610
so we were very happy that we were using

00:06:22,050 --> 00:06:27,460
not because we could do things like this

00:06:24,610 --> 00:06:32,169
which is basically heck the note HTTP

00:06:27,460 --> 00:06:35,080
server so that what we did was start at

00:06:32,169 --> 00:06:37,690
this option to after unwraps the

00:06:35,080 --> 00:06:39,580
encryption for the connection since the

00:06:37,690 --> 00:06:40,000
IP address on the client we fill in the

00:06:39,580 --> 00:06:42,280
first four

00:06:40,000 --> 00:06:43,870
bites of the tcp stream so what I'm

00:06:42,280 --> 00:06:46,300
trying boo here is we are trying to

00:06:43,870 --> 00:06:49,350
intercept that initial connection even

00:06:46,300 --> 00:06:53,710
before the HTTP server gets it read off

00:06:49,350 --> 00:06:55,930
the IP address and then pass it back to

00:06:53,710 --> 00:06:58,600
the notes to note so you should be

00:06:55,930 --> 00:07:01,300
server to do it's a healer passing it on

00:06:58,600 --> 00:07:04,180
and others modulus ave he has to do so

00:07:01,300 --> 00:07:07,530
as you can see what we did let me go

00:07:04,180 --> 00:07:11,260
people we get a list of existing

00:07:07,530 --> 00:07:14,230
connections on a tcp server and one of

00:07:11,260 --> 00:07:16,360
these connection handlers well this

00:07:14,230 --> 00:07:20,620
connection event handlers would be the

00:07:16,360 --> 00:07:24,220
HTTP header process so we intersect the

00:07:20,620 --> 00:07:29,860
connection and on one receiving any data

00:07:24,220 --> 00:07:34,510
we attempt to oops we attempt to read

00:07:29,860 --> 00:07:37,620
off the first few bites that mix up the

00:07:34,510 --> 00:07:40,840
client IP address and sets it on the

00:07:37,620 --> 00:07:43,990
socket connection and then pass through

00:07:40,840 --> 00:07:47,850
the remaining data to the original

00:07:43,990 --> 00:07:49,840
handler for this soccer so this works

00:07:47,850 --> 00:07:53,760
it's a bit complicated actually because

00:07:49,840 --> 00:07:59,470
I kind of thicken up or error handling

00:07:53,760 --> 00:08:02,350
right and but it worked for for us for

00:07:59,470 --> 00:08:07,780
for quite a while and so we thought

00:08:02,350 --> 00:08:12,880
through this for maybe half a year sound

00:08:07,780 --> 00:08:14,380
the other problems in general when you

00:08:12,880 --> 00:08:17,050
when you talk about this kind of

00:08:14,380 --> 00:08:19,870
multi-layer infrastructure where you

00:08:17,050 --> 00:08:21,850
have proxies SSL two meters is that you

00:08:19,870 --> 00:08:23,979
get buffer bill which is when all the

00:08:21,850 --> 00:08:26,740
different components in the stack buffer

00:08:23,979 --> 00:08:28,300
a little bit of data and so from an

00:08:26,740 --> 00:08:29,979
application you can tell how much data

00:08:28,300 --> 00:08:31,150
is being buffered by dresses so how much

00:08:29,979 --> 00:08:33,669
room is still sorting our network in

00:08:31,150 --> 00:08:39,310
whether your client has received that

00:08:33,669 --> 00:08:43,330
they are not so after about six months

00:08:39,310 --> 00:08:46,270
or rather for six months while we're

00:08:43,330 --> 00:08:48,760
using stuff we occasionally got problems

00:08:46,270 --> 00:08:51,560
we start just dies and recent god is

00:08:48,760 --> 00:08:54,850
inexplicable errors that

00:08:51,560 --> 00:08:58,960
that what's wrong need not because of

00:08:54,850 --> 00:09:01,730
something I start was doing and the ssl

00:08:58,960 --> 00:09:05,750
the tcp handler in node could not handle

00:09:01,730 --> 00:09:12,890
this so we continue to look for a better

00:09:05,750 --> 00:09:14,480
solution soft our scaling problem so let

00:09:12,890 --> 00:09:20,510
me stay that we thank you give you guys

00:09:14,480 --> 00:09:23,570
a primer on mac bus so you I guess you

00:09:20,510 --> 00:09:25,220
have study familiar with IP addresses

00:09:23,570 --> 00:09:29,450
and a net mass that looks like this so

00:09:25,220 --> 00:09:32,480
II my night 1685 to 128 is the IP

00:09:29,450 --> 00:09:35,000
address and / 26 refers to the network

00:09:32,480 --> 00:09:37,820
at this IP address belongs to so what is

00:09:35,000 --> 00:09:42,440
essentially means is that the first if

00:09:37,820 --> 00:09:44,750
you click the first 26 bits of this IP

00:09:42,440 --> 00:09:48,110
address that is the network of this IP

00:09:44,750 --> 00:09:51,650
address so this is a way for now engines

00:09:48,110 --> 00:09:54,220
to partition that works into site into

00:09:51,650 --> 00:10:00,050
cyber challenge manageable chunks and

00:09:54,220 --> 00:10:03,080
true for initiative process like you

00:10:00,050 --> 00:10:05,089
could route data you can roll back at

00:10:03,080 --> 00:10:11,170
space on which network you are coming

00:10:05,089 --> 00:10:14,080
from which now we are going to so we

00:10:11,170 --> 00:10:16,880
figured out that we could actually abuse

00:10:14,080 --> 00:10:19,790
net masks by matching on the least

00:10:16,880 --> 00:10:23,720
significant bits so when we measure the

00:10:19,790 --> 00:10:26,210
most images are in a typical way that

00:10:23,720 --> 00:10:28,460
people use that mass if we did load

00:10:26,210 --> 00:10:30,260
balancing based on that what the problem

00:10:28,460 --> 00:10:31,339
we will find is that you have some

00:10:30,260 --> 00:10:33,980
networks where you have a lot of users

00:10:31,339 --> 00:10:35,480
and sometimes very very very easy so if

00:10:33,980 --> 00:10:38,060
you try and do load balancing that just

00:10:35,480 --> 00:10:39,620
because you just end up with this

00:10:38,060 --> 00:10:41,510
service they have to sell a lot of

00:10:39,620 --> 00:10:44,360
traffic because I don't know very good

00:10:41,510 --> 00:10:47,300
like a China is in one small neck

00:10:44,360 --> 00:10:49,250
block so if you got actually we could

00:10:47,300 --> 00:10:51,110
match on at least if you can beat and in

00:10:49,250 --> 00:10:53,600
this way we get some sort of random

00:10:51,110 --> 00:10:55,940
nurse when we're trying to route our

00:10:53,600 --> 00:10:58,160
users and at the same time because it's

00:10:55,940 --> 00:11:00,860
based on IP address so for long pulling

00:10:58,160 --> 00:11:04,520
such different connections we still go

00:11:00,860 --> 00:11:05,100
back to the same so so in fact the

00:11:04,520 --> 00:11:08,110
leaner

00:11:05,100 --> 00:11:10,930
now if you'll Ahmad you that's allow you

00:11:08,110 --> 00:11:15,490
to match arbitrary numbers addresses

00:11:10,930 --> 00:11:17,980
addresses so if you see there's a

00:11:15,490 --> 00:11:19,240
bloodbath 2000 doctrine that is the

00:11:17,980 --> 00:11:24,910
networks that we're using so what we're

00:11:19,240 --> 00:11:28,480
saying is match the last two bits of the

00:11:24,910 --> 00:11:31,870
IP address and that is the net block I

00:11:28,480 --> 00:11:34,240
mean so called the slot then the client

00:11:31,870 --> 00:11:37,270
belongs to so what we do here is for

00:11:34,240 --> 00:11:39,880
each of these blocks we brought it we

00:11:37,270 --> 00:11:43,839
Rida right into one of our worker

00:11:39,880 --> 00:11:46,410
processes and this has to be a it has to

00:11:43,839 --> 00:11:52,600
be a power of two processors because of

00:11:46,410 --> 00:11:55,150
how the mass works so we deploy this

00:11:52,600 --> 00:11:58,270
solution and it worked really well for

00:11:55,150 --> 00:12:00,820
us we could scale the number of worker

00:11:58,270 --> 00:12:03,310
processors by adjusting the number of

00:12:00,820 --> 00:12:05,170
bits what you mentioned in this case we

00:12:03,310 --> 00:12:07,089
are matching to bit so they are four

00:12:05,170 --> 00:12:09,660
workers but if we had like a 30-foot

00:12:07,089 --> 00:12:14,260
core machine and you match five bits and

00:12:09,660 --> 00:12:17,860
half that number of other processors so

00:12:14,260 --> 00:12:19,330
this worked very well oh no we still get

00:12:17,860 --> 00:12:22,480
we see at some remaining issues which

00:12:19,330 --> 00:12:25,690
which is our application our loan

00:12:22,480 --> 00:12:28,360
application has some memory leaks and

00:12:25,690 --> 00:12:31,150
over time it builds up n as you know

00:12:28,360 --> 00:12:32,709
we're not kids about 1 to 2 gigabytes of

00:12:31,150 --> 00:12:34,330
memory starts to really slow down the

00:12:32,709 --> 00:12:37,350
process to do garbage collection on time

00:12:34,330 --> 00:12:40,180
so what we did was once no hits around

00:12:37,350 --> 00:12:43,120
almost manual process one key byte of

00:12:40,180 --> 00:12:46,000
memory usage we will we will restart yes

00:12:43,120 --> 00:12:47,560
that particular worker it wasn't so bad

00:12:46,000 --> 00:12:49,390
in a sense that because unique

00:12:47,560 --> 00:12:51,640
disconnects the user that users they

00:12:49,390 --> 00:12:53,200
were connected with which is maybe one

00:12:51,640 --> 00:12:55,060
over eight of our reasons connected to

00:12:53,200 --> 00:12:59,910
the particular server but you are so it

00:12:55,060 --> 00:12:59,910
was a good experience for our users so

00:13:00,600 --> 00:13:07,089
and ok and and we also had some human

00:13:05,470 --> 00:13:09,700
errors when we were attending the

00:13:07,089 --> 00:13:12,310
parties rules because we generated em

00:13:09,700 --> 00:13:14,230
manually and when we wanted to change

00:13:12,310 --> 00:13:18,010
the number of slots the number of

00:13:14,230 --> 00:13:22,230
workers for server it and we

00:13:18,010 --> 00:13:27,070
updated iptables for free it would prick

00:13:22,230 --> 00:13:30,120
so we decided to write a new likely to

00:13:27,070 --> 00:13:33,370
help us handily handle all this iptables

00:13:30,120 --> 00:13:36,460
manipulation and at the same time help

00:13:33,370 --> 00:13:37,600
us manage worker processors we're

00:13:36,460 --> 00:13:40,090
starting them when you are using flash

00:13:37,600 --> 00:13:42,100
memory restarting and when they are when

00:13:40,090 --> 00:13:44,470
they are running out off when they are

00:13:42,100 --> 00:13:48,670
taking too long to respond and and so on

00:13:44,470 --> 00:13:51,100
so these are some of the features that

00:13:48,670 --> 00:13:54,190
our library has given in right now so

00:13:51,100 --> 00:13:57,910
it's Monica you use it like the transfer

00:13:54,190 --> 00:13:59,050
value in nodejs he has place to work

00:13:57,910 --> 00:14:03,040
every time and so what it means is that

00:13:59,050 --> 00:14:04,360
when we want Q of a worker we don't we

00:14:03,040 --> 00:14:06,820
don't ask you it but what we do is that

00:14:04,360 --> 00:14:08,380
we changed i became a rule respona canal

00:14:06,820 --> 00:14:09,850
water resolve new work at first and

00:14:08,380 --> 00:14:12,250
change hi Petey Lorusso right points in

00:14:09,850 --> 00:14:16,300
the new worker and let over continue to

00:14:12,250 --> 00:14:19,000
finish servicing the requested that they

00:14:16,300 --> 00:14:21,010
are still outstanding so they're also

00:14:19,000 --> 00:14:24,220
allows us to do hot curvy road so when

00:14:21,010 --> 00:14:26,410
we have a new deploy we can just make a

00:14:24,220 --> 00:14:29,020
whole bunch of new workers and point

00:14:26,410 --> 00:14:31,780
iptables to use the use the new

00:14:29,020 --> 00:14:35,920
processors so Batman who refers to the

00:14:31,780 --> 00:14:38,530
fact that our new processors thus the

00:14:35,920 --> 00:14:41,890
soccer object ADC is the actual soccer

00:14:38,530 --> 00:14:43,510
that gets connected of client so things

00:14:41,890 --> 00:14:45,760
like doing events they work reliably

00:14:43,510 --> 00:14:48,430
because you don't go to another way of

00:14:45,760 --> 00:14:51,850
proxy and obviously a client IP address

00:14:48,430 --> 00:14:54,780
is there early on the socket three menus

00:14:51,850 --> 00:14:59,230
of this MPI Monica too busy which posed

00:14:54,780 --> 00:15:00,670
the note event loop and determines and

00:14:59,230 --> 00:15:03,610
try to determine if it's taking too long

00:15:00,670 --> 00:15:06,760
to go to the event which which implies

00:15:03,610 --> 00:15:08,770
that the no processor is a new processes

00:15:06,760 --> 00:15:13,000
under too much loot and is spending all

00:15:08,770 --> 00:15:17,400
this time Henry depends so then how was

00:15:13,000 --> 00:15:21,040
that helps us to detect work instead

00:15:17,400 --> 00:15:24,370
have run into some kind of problem and

00:15:21,040 --> 00:15:26,790
they are just general servicing requests

00:15:24,370 --> 00:15:26,790
fast enough

00:15:27,930 --> 00:15:33,420
so as I said so that sort of master

00:15:31,770 --> 00:15:36,899
managers or the other different workers

00:15:33,420 --> 00:15:38,610
and we use unix dominicans to

00:15:36,899 --> 00:15:41,100
communicate between a master and the

00:15:38,610 --> 00:15:43,080
workers the workers select the listening

00:15:41,100 --> 00:15:45,330
pots that be a decent amount owed is

00:15:43,080 --> 00:15:46,560
what we do is that we get the operating

00:15:45,330 --> 00:15:49,320
system to do it so you pick a random

00:15:46,560 --> 00:15:51,089
port and workers report the partner

00:15:49,320 --> 00:15:53,070
listening to the master and master will

00:15:51,089 --> 00:15:56,100
set up the IP table should make sure

00:15:53,070 --> 00:15:58,080
that you points to the right places the

00:15:56,100 --> 00:16:00,690
Muslims are looking above means of

00:15:58,080 --> 00:16:01,830
managing other masters so we in just in

00:16:00,690 --> 00:16:04,589
case there's a button while saying you

00:16:01,830 --> 00:16:06,660
to Cuba or to run in another master I

00:16:04,589 --> 00:16:08,640
because that is designed so that if you

00:16:06,660 --> 00:16:11,220
run another copy of a muscle it will

00:16:08,640 --> 00:16:16,709
queue of the 01 and manage and tip over

00:16:11,220 --> 00:16:20,850
a single cuz i manage them properly so

00:16:16,709 --> 00:16:25,020
we retire workers when so in a capacitor

00:16:20,850 --> 00:16:27,920
we set a memory limit per worker and so

00:16:25,020 --> 00:16:30,630
when workers approach the limit we will

00:16:27,920 --> 00:16:35,279
Chris we will let them kind of shut down

00:16:30,630 --> 00:16:38,610
gracefully by by redirecting all the new

00:16:35,279 --> 00:16:40,709
requests to new servers basis of class

00:16:38,610 --> 00:16:44,550
also a cluster men really mean for the

00:16:40,709 --> 00:16:47,220
entire cluster so when you approach like

00:16:44,550 --> 00:16:49,620
the limit of the more memories of the

00:16:47,220 --> 00:16:57,150
entire struggle has we will find the

00:16:49,620 --> 00:17:00,959
odors retire worker and cute so we have

00:16:57,150 --> 00:17:04,920
open source IP castles so this is all

00:17:00,959 --> 00:17:09,689
this our project page and now I will run

00:17:04,920 --> 00:17:20,670
us through an example of IP cluster

00:17:09,689 --> 00:17:27,390
usage so this is a typical route I'm

00:17:20,670 --> 00:17:31,620
WebSocket based application we start a

00:17:27,390 --> 00:17:38,620
web socket and on a connection we send a

00:17:31,620 --> 00:17:41,010
current every 100 milliseconds so

00:17:38,620 --> 00:17:41,010
slick

00:18:07,710 --> 00:18:10,159
oops

00:18:24,320 --> 00:18:31,330
give it over the demo happens every time

00:18:38,830 --> 00:18:48,820
get out of the skip the demo and run

00:18:45,070 --> 00:18:53,769
true let me just run to how we how you

00:18:48,820 --> 00:18:57,220
want to use IP cluster and as in how how

00:18:53,769 --> 00:18:59,529
can we use IP castor and not a Fraggle

00:18:57,220 --> 00:19:01,960
as this thing weighs so much so if you

00:18:59,529 --> 00:19:07,929
notice this on right side is the same

00:19:01,960 --> 00:19:10,510
application but below we at IP cluster

00:19:07,929 --> 00:19:13,600
support so it's assume it's as simple as

00:19:10,510 --> 00:19:16,000
um it's actually familiar with if after

00:19:13,600 --> 00:19:19,000
me with the cluster module in node is

00:19:16,000 --> 00:19:24,730
actually similar you require the IP

00:19:19,000 --> 00:19:26,380
custom module set up the seller some

00:19:24,730 --> 00:19:29,010
settings photo so this is the socket

00:19:26,380 --> 00:19:31,510
that the master and worker processes

00:19:29,010 --> 00:19:33,789
communicate over it will set IP address

00:19:31,510 --> 00:19:35,769
because otherwise we wouldn't know how

00:19:33,789 --> 00:19:39,610
to setup you table fools so you can a

00:19:35,769 --> 00:19:43,809
master you just set up master and sorry

00:19:39,610 --> 00:19:46,870
let me get a worker you just start yours

00:19:43,809 --> 00:19:50,169
your application as phenomenal except

00:19:46,870 --> 00:19:53,649
this time you don't specify a listening

00:19:50,169 --> 00:19:58,149
port we let we let the operating system

00:19:53,649 --> 00:20:01,299
pick apart and then we will put the pot

00:19:58,149 --> 00:20:11,130
to the master and that's it the master

00:20:01,299 --> 00:20:11,130
vendor or a PTO rules so let me

00:20:24,210 --> 00:20:27,210
what

00:20:56,070 --> 00:21:01,380
okay so let me show you all IP tables

00:21:06,550 --> 00:21:15,470
so I just listed all the rules in the ne

00:21:11,210 --> 00:21:25,270
T table for for this machine and you

00:21:15,470 --> 00:21:32,840
know this is nothing here so our now run

00:21:25,270 --> 00:21:38,570
the select application support and we

00:21:32,840 --> 00:21:40,970
see that we have three rules no 10 in

00:21:38,570 --> 00:21:44,000
fall so what happened was that s each

00:21:40,970 --> 00:21:45,950
worker was fun we reported the pot day

00:21:44,000 --> 00:21:48,440
listening to to the to the master and

00:21:45,950 --> 00:21:54,290
master set up the IP t was for that for

00:21:48,440 --> 00:21:56,870
that particular slot so it always waits

00:21:54,290 --> 00:21:58,970
until that worker report support before

00:21:56,870 --> 00:22:01,130
creating iptables room so what this

00:21:58,970 --> 00:22:03,050
means is that we will never create a PK

00:22:01,130 --> 00:22:09,350
value before the worker is ready to

00:22:03,050 --> 00:22:18,860
accept connections so let's see whether

00:22:09,350 --> 00:22:24,700
it works this time I'm sorry I think

00:22:18,860 --> 00:22:36,170
I've song ever so short a time but I

00:22:24,700 --> 00:22:37,400
guess from the yeah I guess from I guess

00:22:36,170 --> 00:22:44,720
one crow is pretty straightforward how

00:22:37,400 --> 00:22:48,440
it works yeah so let me go back to the

00:22:44,720 --> 00:22:49,910
future so to help us with our memory

00:22:48,440 --> 00:22:52,340
leak issues in the future we are

00:22:49,910 --> 00:22:55,400
planning to automate automated hippie

00:22:52,340 --> 00:22:58,250
things about so the good thing about

00:22:55,400 --> 00:23:00,020
article says that because we can we know

00:22:58,250 --> 00:23:02,930
when a worker starts we can take a

00:23:00,020 --> 00:23:05,180
snapshot of the hip and when the worker

00:23:02,930 --> 00:23:07,490
is retired we can wait until every

00:23:05,180 --> 00:23:09,440
single ass connection has has been

00:23:07,490 --> 00:23:11,540
disconnected before we take another hip

00:23:09,440 --> 00:23:16,700
and if the different so in this way we

00:23:11,540 --> 00:23:19,130
can find memory leaks there's also it's

00:23:16,700 --> 00:23:19,700
been hard to see the health of the

00:23:19,130 --> 00:23:22,610
cluster

00:23:19,700 --> 00:23:24,080
is fine time so some kind of dash

00:23:22,610 --> 00:23:28,960
thought would be nice to be able to see

00:23:24,080 --> 00:23:32,539
what the cluster is currently doing so

00:23:28,960 --> 00:23:35,539
um is a shout out to my team and team

00:23:32,539 --> 00:23:40,010
who is one of our developers and has

00:23:35,539 --> 00:23:54,139
worked a lot on up your cluster yeah

00:23:40,010 --> 00:24:03,700
that's all so I could we have let's say

00:23:54,139 --> 00:24:03,700
one question maybe for young bin yes

00:24:05,590 --> 00:24:10,820
hello I'm just wondering it you have

00:24:08,659 --> 00:24:13,880
have you ever tried to use unless site

00:24:10,820 --> 00:24:15,950
youÃ­re DS RSS install so that you can

00:24:13,880 --> 00:24:17,899
sync synchronize a session between

00:24:15,950 --> 00:24:21,980
multiple clients so I mean you don't

00:24:17,899 --> 00:24:24,919
really needs a sticky station ask foster

00:24:21,980 --> 00:24:27,649
I mean for all the note and also have

00:24:24,919 --> 00:24:30,080
you tried to use a something new

00:24:27,649 --> 00:24:33,340
protocol which is a proxy for taco which

00:24:30,080 --> 00:24:38,840
is allow you to load balance between

00:24:33,340 --> 00:24:39,860
multiply and even GPU TCP so let me let

00:24:38,840 --> 00:24:43,299
me see all your questions right the

00:24:39,860 --> 00:24:46,970
first version so why not why not use a

00:24:43,299 --> 00:24:49,130
very special session store yeah and we

00:24:46,970 --> 00:24:51,289
are why you need so you can match the

00:24:49,130 --> 00:24:53,539
income connection with the provides

00:24:51,289 --> 00:24:55,370
instant connection yeah so this meet you

00:24:53,539 --> 00:24:57,590
don't even eat the sticky six and as the

00:24:55,370 --> 00:24:59,980
first place so sort of thing is we

00:24:57,590 --> 00:25:01,880
actually a really kind of hyper session

00:24:59,980 --> 00:25:06,710
management system which is our back-end

00:25:01,880 --> 00:25:08,480
services so essentially for the all

00:25:06,710 --> 00:25:10,840
mediators we could reconnect with

00:25:08,480 --> 00:25:13,070
another mediator end we can still

00:25:10,840 --> 00:25:16,460
rÃ©pondre back to the same session but a

00:25:13,070 --> 00:25:19,340
problem is that there is a lot of state

00:25:16,460 --> 00:25:22,399
in our front-end service because we are

00:25:19,340 --> 00:25:26,149
we are normalizing the data structure

00:25:22,399 --> 00:25:28,429
into a into a global streaming standard

00:25:26,149 --> 00:25:31,309
for our financial service so let's not

00:25:28,429 --> 00:25:32,800
stay involved and while we can you can

00:25:31,309 --> 00:25:35,170
reconnect or difference over it takes

00:25:32,800 --> 00:25:39,160
a couple seconds to get all that state

00:25:35,170 --> 00:25:42,940
or set up again so we this I mean that's

00:25:39,160 --> 00:25:45,720
why we have we try not to have to prove

00:25:42,940 --> 00:25:45,720

YouTube URL: https://www.youtube.com/watch?v=xYjWlrNN1Qw


