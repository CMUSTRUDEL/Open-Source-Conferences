Title: 10 things I learned making the fastest JS server runtime in the world by Paulo Lopes | JSConf EU
Publication date: 2019-08-05
Playlist: JSConf EU 2019
Description: 
	This presentation is about server performance, which means that no time in the world would be enough to cover it all. Hopefully, I can share with you the top

#10 things I’ve learned while putting JavaScript on the top of the server side benchmarks.

You will learn about runtimes and engines, how some are more capable than others, and sometimes the obvious choice is not always the right one…

This talk is about thinking outside of the box, being creative and don’t take anything for granted. We will debunk myths about native code vs script or RAM usage, it’s going to be fast! I promise!

https://2019.jsconf.eu/paulo-lopes/10-things-i-learned-making-the-fastest-js-server-runtime-in-the-world.html
Captions: 
	00:00:10,889 --> 00:00:11,889
Thank you.

00:00:11,889 --> 00:00:12,940
Thank you, everyone.

00:00:12,940 --> 00:00:17,460
Well, there's one thing I've learned working with my team that I would like to share and

00:00:17,460 --> 00:00:27,019
that I will never forget is that we know that writing fast applications makes our users

00:00:27,019 --> 00:00:29,250
and our customers happy.

00:00:29,250 --> 00:00:31,269
So, who doesn't want to write fast code?

00:00:31,269 --> 00:00:32,509
Raise your hand.

00:00:32,509 --> 00:00:33,509
Naw.

00:00:33,509 --> 00:00:35,059
That's interesting.

00:00:35,059 --> 00:00:39,840
So, before we start, I have a couple of questions that I need to ask.

00:00:39,840 --> 00:00:44,230
And then I'll see what the Internet tells us.

00:00:44,230 --> 00:00:49,570
So, the first question is, if you go on your favorite web browser and your favorite search

00:00:49,570 --> 00:00:53,149
engine and you type "Is JavaScript fast?

00:00:53,149 --> 00:00:55,150
How fast is JavaScript?"

00:00:55,150 --> 00:01:00,890
Probably get something like this that says, and I'm just quoting," Under the right circumstances

00:01:00,890 --> 00:01:01,960
it is very fast.

00:01:01,960 --> 00:01:05,780
Actually, as fast as C."

00:01:05,780 --> 00:01:11,160
If you search again, another result would be, "Why is it so fast?

00:01:11,160 --> 00:01:17,030
It is because as soon as you understand event loop and how it processes requests, you realize

00:01:17,030 --> 00:01:18,680
it's so fast."

00:01:18,680 --> 00:01:23,750
You start to see a pattern, because fast is because it's fast.

00:01:23,750 --> 00:01:29,960
And you keep going on and you get stuff like, "How can it be so fast since it's a single

00:01:29,960 --> 00:01:31,570
thread?"

00:01:31,570 --> 00:01:38,049
And the answer, like in this example is because it's lightweight.

00:01:38,049 --> 00:01:43,850
We keep going and then you find this interesting question."

00:01:43,850 --> 00:01:46,340
How fast is it compared to Java?"

00:01:46,340 --> 00:01:52,241
Well, because most recruiters think that Java and JavaScript is the same thing, which is

00:01:52,241 --> 00:01:53,360
kind of interesting.

00:01:53,360 --> 00:02:00,570
So, if you look around on the Internet, you see JS is and shines when it comes to a huge

00:02:00,570 --> 00:02:05,159
amount of short connections.

00:02:05,159 --> 00:02:11,220
And finally, I could be all day showing Google results.

00:02:11,220 --> 00:02:13,230
But what does it make faster than Java?

00:02:13,230 --> 00:02:20,510
Well, and the answer is because the sync ecosystem is more than 50,000 modules written in asynchronous

00:02:20,510 --> 00:02:21,510
style.

00:02:21,510 --> 00:02:24,040
It's kind of a strange answer to the question.

00:02:24,040 --> 00:02:33,170
But giving all these questions, we need to ask, do we trust the Internet?

00:02:33,170 --> 00:02:37,660
Like the Internet is full of stories, and like some Game of Thrones characters said

00:02:37,660 --> 00:02:40,519
a few weeks ago, stories connect people.

00:02:40,519 --> 00:02:44,030
However, stories are not exact science.

00:02:44,030 --> 00:02:51,150
And above all, they should not drive us as a software engineer.

00:02:51,150 --> 00:02:56,349
So, my interpretation is, do I trust the Internet?

00:02:56,349 --> 00:02:57,349
No.

00:02:57,349 --> 00:02:58,349
I don't.

00:02:58,349 --> 00:02:59,530
And why?

00:02:59,530 --> 00:03:05,800
Because I am a software engineer, as you can if you don't know who invented who coined

00:03:05,800 --> 00:03:12,230
term you can go in our exhibition hall and there's an explanation there who did it.

00:03:12,230 --> 00:03:19,909
And you see that if you look on the dictionary for software engineering, it says engineer

00:03:19,909 --> 00:03:24,450
is the application of science and mathematics by which the properties of matter and the

00:03:24,450 --> 00:03:28,370
sources of energy and nature are made useful to people.

00:03:28,370 --> 00:03:35,900
So, as a software engineer, we should apply science and mathematics to solve our problems.

00:03:35,900 --> 00:03:40,720
So, going back to the question, is JavaScript fast?

00:03:40,720 --> 00:03:43,379
We need we must be able to reproduce a problem.

00:03:43,379 --> 00:03:46,560
We must be able to explain the results.

00:03:46,560 --> 00:03:48,400
And reproduce the results.

00:03:48,400 --> 00:03:53,450
So, I think the right answer is, is JavaScript fast?

00:03:53,450 --> 00:03:54,450
I don't know.

00:03:54,450 --> 00:03:57,599
From these results, it's not clear.

00:03:57,599 --> 00:04:01,430
So, starting now with the main topic.

00:04:01,430 --> 00:04:04,440
Like when I was planning the talk, I needed a title.

00:04:04,440 --> 00:04:09,020
So, I ended up with so things I learned make the fastest JavaScript server runtime in the

00:04:09,020 --> 00:04:10,020
world.

00:04:10,020 --> 00:04:14,150
I carefully decided to pick the word "Server."

00:04:14,150 --> 00:04:19,739
Going to Wikipedia for a definition, a server is a computer in the network of users that

00:04:19,739 --> 00:04:23,940
is used to provide services to other computers in the network.

00:04:23,940 --> 00:04:31,950
So, what I'm about to tell you is not about command line applications or lambdas.

00:04:31,950 --> 00:04:33,850
It's about long running processes.

00:04:33,850 --> 00:04:37,080
So, we need to also define what is fast.

00:04:37,080 --> 00:04:43,700
So, because when I say fast, I don't mean I'm fast because I put my server on a race

00:04:43,700 --> 00:04:46,410
car and the car running around.

00:04:46,410 --> 00:04:47,410
No.

00:04:47,410 --> 00:04:51,200
What I mean by fast is we need to obtain a common set of metrics.

00:04:51,200 --> 00:04:59,599
And for this, I'm using what the site reliability engineering has found out.

00:04:59,599 --> 00:05:04,550
So, if don't know anything about site reliability engineering, there's this interesting link

00:05:04,550 --> 00:05:06,159
with nice books from Google.

00:05:06,159 --> 00:05:08,589
And Google has one of the biggest teams on SRE.

00:05:08,589 --> 00:05:13,720
And SRE has identified five golden signals.

00:05:13,720 --> 00:05:22,420
So, golden signals are critical to the monitoring teams to monitor their systems and identify

00:05:22,420 --> 00:05:27,750
problems before they become really big problems.

00:05:27,750 --> 00:05:31,080
So, there are many metrics to monitor.

00:05:31,080 --> 00:05:40,410
But this team this team this SRE team showed that rate errors and latency, saturation utilization

00:05:40,410 --> 00:05:46,710
contain virtually everything you need to know about what's going on and where.

00:05:46,710 --> 00:05:51,860
Getting the signals is quite challenging and relies on a lot of the tools and services

00:05:51,860 --> 00:05:54,180
you have at your disposal.

00:05:54,180 --> 00:05:58,480
But for now, I'm just considering rate as in request per second.

00:05:58,480 --> 00:06:01,190
Errors like in errors per second, of course.

00:06:01,190 --> 00:06:05,159
And latency as in like response time including waiting and queuing.

00:06:05,159 --> 00:06:08,330
So, focusing on wait, errors and latency.

00:06:08,330 --> 00:06:10,209
I'm focusing on the software.

00:06:10,209 --> 00:06:14,849
I'm not focusing on the hardware or in the operating system.

00:06:14,849 --> 00:06:25,020
So, a typical server application has a wellset wellknown set of characteristics.

00:06:25,020 --> 00:06:33,910
We need to know how the application behaves and only once we understand that, we can talk

00:06:33,910 --> 00:06:34,910
about it.

00:06:34,910 --> 00:06:36,120
So, what is a server application?

00:06:36,120 --> 00:06:41,820
So, my definition of server application is a long running process that should be deployed

00:06:41,820 --> 00:06:45,409
on a cloud or in bare metal.

00:06:45,409 --> 00:06:47,450
And it should be attached to a fast network.

00:06:47,450 --> 00:06:49,750
Otherwise the network becomes your bottleneck.

00:06:49,750 --> 00:06:54,140
And, of course, should have enough CPU and memory.

00:06:54,140 --> 00:06:58,990
So, your application is not strained by your hardware.

00:06:58,990 --> 00:07:04,909
So, a longrunning process has different characteristics from a shortrunning process, of course.

00:07:04,909 --> 00:07:12,330
So, in a longrunning process, the startup and warming up is not really relevant in the

00:07:12,330 --> 00:07:16,650
fullspan life cycle of the server because it's a very tiny moment.

00:07:16,650 --> 00:07:20,890
Again, this isn't true if you're talking about web applications on your browser.

00:07:20,890 --> 00:07:25,310
Because you want to be as fast as possible because that's what's drives the happiness

00:07:25,310 --> 00:07:26,990
of your users.

00:07:26,990 --> 00:07:31,680
So, now we need to define our two major things.

00:07:31,680 --> 00:07:34,250
Our two benchmark things.

00:07:34,250 --> 00:07:38,479
Most Internet articles will tell you how fast something is.

00:07:38,479 --> 00:07:43,730
But most of the time when you read the whole article, you see some graphs, really nice

00:07:43,730 --> 00:07:44,730
graphs.

00:07:44,730 --> 00:07:49,529
But the information about how the tests will perform and how the results were obtained

00:07:49,529 --> 00:07:51,110
is needed.

00:07:51,110 --> 00:07:54,260
From an engineering perspective, this is incorrect.

00:07:54,260 --> 00:07:57,969
We should be able to reproduce the test and the results.

00:07:57,969 --> 00:08:03,870
In a lab that gives you more or less exactly the same results of course.

00:08:03,870 --> 00:08:13,019
On top of that, the experiment was we need to confirm that the results are not biased.

00:08:13,019 --> 00:08:22,140
So, when I write a benchmark, I don't want to make it be my friend and enemy of the others.

00:08:22,140 --> 00:08:24,430
It needs to be fair.

00:08:24,430 --> 00:08:26,919
And writing benchmarks, of course, is hard.

00:08:26,919 --> 00:08:33,840
Because first every benchmark you write will never represent a real-world use case.

00:08:33,840 --> 00:08:37,760
It's always like a tiny subset that doesn't really represent your application.

00:08:37,760 --> 00:08:42,900
So, you need to get into conclusions from just looking at the tiny bit of your life

00:08:42,900 --> 00:08:44,060
cycle.

00:08:44,060 --> 00:08:50,370
So, getting peers to review your code can be really hard to find.

00:08:50,370 --> 00:08:59,580
And getting peers to that are willing to review what you wrote is even harder.

00:08:59,580 --> 00:09:07,070
So, what I'm trying to tell is that benchmarking is hard.

00:09:07,070 --> 00:09:17,700
And, however, there is a very popular benchmark out there that is called the tech and power

00:09:17,700 --> 00:09:19,820
frameworks benchmark.

00:09:19,820 --> 00:09:23,710
Why is this benchmark so interesting to me?

00:09:23,710 --> 00:09:26,380
Well, to me it's like taking power.

00:09:26,380 --> 00:09:32,410
A benchmark shows you the true nature of open source.

00:09:32,410 --> 00:09:34,260
It has more than 500 contributors.

00:09:34,260 --> 00:09:41,570
So, more than 500 different people have contributed to tests and reviewed the tests.

00:09:41,570 --> 00:09:45,000
There are more than 3,000 merged pull requests.

00:09:45,000 --> 00:09:54,670
So, lots of people spend time reviewing or adding new tests to the framework.

00:09:54,670 --> 00:09:57,340
And they already have more than 10,000 commits.

00:09:57,340 --> 00:10:01,460
So, it shows that it's kind of a big project.

00:10:01,460 --> 00:10:05,130
It's not something that someone just planned in the weekend.

00:10:05,130 --> 00:10:08,520
Oh, I want to check on my framework, how it works.

00:10:08,520 --> 00:10:13,610
No, it's something that has been growing steadily for the last couple of years.

00:10:13,610 --> 00:10:19,050
And it already tests more than 630 different frameworks.

00:10:19,050 --> 00:10:22,350
And these frameworks are written in different languages.

00:10:22,350 --> 00:10:29,200
So, this makes my life easier because I don't need to invent my own benchmark.

00:10:29,200 --> 00:10:30,430
I don't need to explain it.

00:10:30,430 --> 00:10:35,550
I can just use it to prove what I want to say.

00:10:35,550 --> 00:10:40,700
So, if you want to have the link, this is like their GitHub repo.

00:10:40,700 --> 00:10:45,370
And from the GitHub repo you can get to the main website, of course.

00:10:45,370 --> 00:10:49,770
And as I said, there are like 630 different frameworks.

00:10:49,770 --> 00:10:57,930
So, if I would try to print on the screen how it looks right now, well, it wouldn't

00:10:57,930 --> 00:10:58,930
fit on the screen.

00:10:58,930 --> 00:11:01,470
So, what I did, I just rotated my screen.

00:11:01,470 --> 00:11:03,610
I took a screenshot.

00:11:03,610 --> 00:11:05,820
And don't worry about the size.

00:11:05,820 --> 00:11:07,570
It's not really relevant.

00:11:07,570 --> 00:11:15,700
What I'm trying to say is that there are lots of frameworks that are already being tested.

00:11:15,700 --> 00:11:21,650
And the quick question I want to ask the audience is, like, can you spot the best result for

00:11:21,650 --> 00:11:25,290
the JavaScript framework on this graph?

00:11:25,290 --> 00:11:27,520
So, probably you cannot because it's very small.

00:11:27,520 --> 00:11:29,640
So, I have here a helper.

00:11:29,640 --> 00:11:35,660
You'll find that as shocking as it can be, the first entry for JavaScript ranks at number

00:11:35,660 --> 00:11:37,560
89.

00:11:37,560 --> 00:11:45,530
Which performs at about 22.7% of the performance of the best result.

00:11:45,530 --> 00:11:55,970
So, if you look at this, and think, well, we all have this idea that JavaScript is fast,

00:11:55,970 --> 00:12:00,550
but results prove things wrong.

00:12:00,550 --> 00:12:03,560
That it's not as fast as we think it is.

00:12:03,560 --> 00:12:08,210
So, what we need to do is that we need to look under the hood.

00:12:08,210 --> 00:12:12,470
So, before we can do any optimization, we need to understand what's going on.

00:12:12,470 --> 00:12:16,970
And we shouldn't jump into conclusions and start tweaking the code of the benchmark.

00:12:16,970 --> 00:12:19,540
Because otherwise we are just yak shaving.

00:12:19,540 --> 00:12:21,650
And you're not really looking into the problem.

00:12:21,650 --> 00:12:25,300
You're just trying to mitigate what could be the cause.

00:12:25,300 --> 00:12:29,070
So, instead of this, we need to take a scientific approach.

00:12:29,070 --> 00:12:33,210
And if you haven't learned anything about profiling in other applications, I would recommend

00:12:33,210 --> 00:12:38,310
for you to look at the tutorial on the NodeJS website on profiling.

00:12:38,310 --> 00:12:46,910
So, just to give you like in a nutshell the information from the from this tutorial.

00:12:46,910 --> 00:12:53,610
That if you look at one of the tests of the benchmark, which is a very simple return,

00:12:53,610 --> 00:12:57,540
hello world string from an HTTP server.

00:12:57,540 --> 00:13:03,320
The best result that you could that you saw on the benchmark was implemented like this.

00:13:03,320 --> 00:13:05,720
So, it uses the cluster module.

00:13:05,720 --> 00:13:12,290
The cluster module will fork the node process for the number of CPUs that the environment

00:13:12,290 --> 00:13:13,290
has.

00:13:13,290 --> 00:13:20,650
And then it uses the express server to set the content type and send the response.

00:13:20,650 --> 00:13:21,820
Okay.

00:13:21,820 --> 00:13:27,670
Probably the express is not the most performant library out there.

00:13:27,670 --> 00:13:30,120
But this is just for illustration.

00:13:30,120 --> 00:13:37,440
So, once we do this and we do profiling, we get a flame graph.

00:13:37,440 --> 00:13:42,020
So, flame graphs are really interesting too when we're talking about performance because

00:13:42,020 --> 00:13:50,550
they give you a visual explanation on where your CPU time is spent.

00:13:50,550 --> 00:13:55,690
The width of the bars or the coloring doesn't really matter.

00:13:55,690 --> 00:14:01,170
The coloring is just to give it make it nice and it's called flame graphs because usually

00:14:01,170 --> 00:14:04,440
we paint it from red to yellow like a flame.

00:14:04,440 --> 00:14:11,360
But what is important to notice is that as you go from bottom up, you see where the code

00:14:11,360 --> 00:14:16,400
is spending most time on your CPU.

00:14:16,400 --> 00:14:23,131
So, if you observe this, you basically what the flame graph is telling you is that there

00:14:23,131 --> 00:14:31,860
is a very tiny piece on the top where JavaScript code is being spent on.

00:14:31,860 --> 00:14:35,870
And then there's lots of time where it's spent on native.

00:14:35,870 --> 00:14:43,430
And native means the Node bindings, V8 will leave for the sync IO and also for the event

00:14:43,430 --> 00:14:44,870
loop.

00:14:44,870 --> 00:14:53,050
So, once we start trying to optimize this, the code, we end up like trying to optimize

00:14:53,050 --> 00:14:54,770
just the tip of the iceberg.

00:14:54,770 --> 00:14:56,350
You cannot optimize everything.

00:14:56,350 --> 00:15:04,680
Because most of the time and if I would go back most of the time here is spent on native

00:15:04,680 --> 00:15:05,680
code.

00:15:05,680 --> 00:15:07,880
So, you're just optimizing the tip of the iceberg.

00:15:07,880 --> 00:15:11,860
So, this makes you think, right?

00:15:11,860 --> 00:15:12,860
This is interesting.

00:15:12,860 --> 00:15:14,920
What can we do about this?

00:15:14,920 --> 00:15:23,580
If you ask yourself, what is the first thing that comes on your mind when I say, JavaScript

00:15:23,580 --> 00:15:24,920
engine?

00:15:24,920 --> 00:15:26,970
Most of you will say V8.

00:15:26,970 --> 00:15:33,470
So, if you look at the mission statement of the V8 project, it reads something like speed

00:15:33,470 --> 00:15:38,630
up real world performance for more than JavaScript and enable developers to build a faster future

00:15:38,630 --> 00:15:39,630
web.

00:15:39,630 --> 00:15:42,920
So, performance on V8 is great.

00:15:42,920 --> 00:15:45,000
But there are more engines out there.

00:15:45,000 --> 00:15:51,190
So, if you look at the table, not an authority on JavaScript engines.

00:15:51,190 --> 00:15:54,620
Just lists the compatibility of ES6 across many.

00:15:54,620 --> 00:16:01,300
There you can see engines like ChakraCore, SpiderMonkey, Safari.

00:16:01,300 --> 00:16:07,910
And there was a new one added last year, crowdjs.

00:16:07,910 --> 00:16:14,920
So, what my experiment was all about is that, well, I should try other engines.

00:16:14,920 --> 00:16:20,560
Because if most of the CPU is spent on native, probably I should look into engines that handle

00:16:20,560 --> 00:16:24,020
this JavaScript runtime in a different way.

00:16:24,020 --> 00:16:26,470
So, I decided to look into rawjs.

00:16:26,470 --> 00:16:35,000
So, raw JS is an extension of the Java machine that supports more languages and execution

00:16:35,000 --> 00:16:36,000
models.

00:16:36,000 --> 00:16:41,839
So, the project includes a new hey performance compiler called Raw because as you know the

00:16:41,839 --> 00:16:47,010
most difficult thing in computer science and science is naming things.

00:16:47,010 --> 00:16:50,750
You all it also Graal because it is interesting.

00:16:50,750 --> 00:16:58,430
And the objective of Graal is to improve the performance of the machine on any language.

00:16:58,430 --> 00:17:04,730
And another goal is to allow free form mixing of any programming language in a single program.

00:17:04,730 --> 00:17:07,380
So, it allows you to do polyglot programming.

00:17:07,380 --> 00:17:16,900
So, on the same program you can use Java, Scala, Ruby, Rust, C++.

00:17:16,900 --> 00:17:21,890
And what's interesting about this is that because it's a new project and it's all up

00:17:21,890 --> 00:17:29,040
to date, they offer a modern JavaScript runtime based on ES2019, ES2020, which isn't released

00:17:29,040 --> 00:17:32,340
yet but they already implemented most of the features.

00:17:32,340 --> 00:17:35,610
And the ultimate goal is a very fast server.

00:17:35,610 --> 00:17:38,190
But I don't want to change my programming language.

00:17:38,190 --> 00:17:39,970
I want to stay on JavaScript.

00:17:39,970 --> 00:17:47,530
So, if I look at the definition of rawjs on their website, their goals are to execute

00:17:47,530 --> 00:17:50,120
JavaScript code with the best possible performance.

00:17:50,120 --> 00:17:54,860
They have full support for the latest ES specification.

00:17:54,860 --> 00:18:02,920
And the fast interoperability with all the languages on either on the JVM or the language

00:18:02,920 --> 00:18:12,050
supported by Graal like Ruby, Python and R. There is research around this because this

00:18:12,050 --> 00:18:17,750
project, although it was open sourced last year, it's been running for more than eight

00:18:17,750 --> 00:18:20,820
years behind closed doors.

00:18:20,820 --> 00:18:28,130
It's just been opened now because now they feel that it's like in a real stable mature

00:18:28,130 --> 00:18:29,560
project.

00:18:29,560 --> 00:18:38,060
So, the people working and researching on this have already shown that the engine is

00:18:38,060 --> 00:18:44,000
slightly better or on par with V8 for just pure language benchmarks.

00:18:44,000 --> 00:18:46,450
And you can read more about the paper there.

00:18:46,450 --> 00:18:52,970
So, and although you can even run like unmodified Node applications on it because it just allows

00:18:52,970 --> 00:19:00,419
you to just replace V8 from Node, I need to formulate a hypothesis.

00:19:00,419 --> 00:19:09,020
What if we create a project that I would call agnostic for X that first will replace V8

00:19:09,020 --> 00:19:10,390
with a project.

00:19:10,390 --> 00:19:15,460
Second, will replace the Eclipse vortex.

00:19:15,460 --> 00:19:21,000
Will replace the V8 with a Graal compiler.

00:19:21,000 --> 00:19:23,020
Will not have Node bindings.

00:19:23,020 --> 00:19:25,180
It will have text definitions.

00:19:25,180 --> 00:19:28,530
This will be discarded at runtime.

00:19:28,530 --> 00:19:32,230
The code that you don't run is the best code, it's the fastest.

00:19:32,230 --> 00:19:33,960
You don't need to run it.

00:19:33,960 --> 00:19:41,270
And offer a basic JS and loader.

00:19:41,270 --> 00:19:45,680
And basic compatibility and allows you to develop and profile the application with the

00:19:45,680 --> 00:19:49,360
tools you already know like the Chrome DevTools.

00:19:49,360 --> 00:19:59,640
So, if we were going to implement the previous example that I showed with Node and express

00:19:59,640 --> 00:20:07,520
using this new style, this is how the old express code would look like.

00:20:07,520 --> 00:20:10,770
I guess it's not that hard to understand what's happening here.

00:20:10,770 --> 00:20:17,030
The important thing here to notice is that the library I chose, vortex, by default uses

00:20:17,030 --> 00:20:21,710
all the available cores on your machine so you don't need to use a cluster module to

00:20:21,710 --> 00:20:22,880
do forks.

00:20:22,880 --> 00:20:25,470
This is all handled behind the scenes for you.

00:20:25,470 --> 00:20:34,179
And Vortex provides us an optimized sync IO build on to have of an open source project

00:20:34,179 --> 00:20:37,930
used by big names like Google, Twitter, Netflix just to name a few.

00:20:37,930 --> 00:20:45,280
If you want to test this, first thing, well, you need to install a very simple application

00:20:45,280 --> 00:20:50,559
called ES4XPM short for project manager.

00:20:50,559 --> 00:20:52,410
We cannot run Node directly.

00:20:52,410 --> 00:20:54,640
Need to run through ES first.

00:20:54,640 --> 00:20:57,510
If I show you, this is how it looks.

00:20:57,510 --> 00:21:01,670
If I create a project.

00:21:01,670 --> 00:21:05,710
I can make it like with a new module syntax.

00:21:05,710 --> 00:21:11,120
And I recorded this so because I'm afraid that I wouldn't have enough time.

00:21:11,120 --> 00:21:12,880
So, just have a couple of dependencies.

00:21:12,880 --> 00:21:15,980
This is pure npm stuff.

00:21:15,980 --> 00:21:21,670
I just use Vortex and web because I want to do a web application.

00:21:21,670 --> 00:21:25,670
So, I create and you can even do like the ES6 modules.

00:21:25,670 --> 00:21:26,670
I can say, okay.

00:21:26,670 --> 00:21:33,790
My home page is a function that I will export that will just say, hello.

00:21:33,790 --> 00:21:37,179
Hello from Vortex Plus ES4X.

00:21:37,179 --> 00:21:40,780
And, of course, now I need to bootstrap a server.

00:21:40,780 --> 00:21:44,850
I get my index, which is like my main application.

00:21:44,850 --> 00:21:51,940
And again, just import some code from the vortex library.

00:21:51,940 --> 00:21:58,610
I now import my route from the module I just created.

00:21:58,610 --> 00:22:00,160
Route.

00:22:00,160 --> 00:22:05,040
And now I just bootstrap the bootstrap my application.

00:22:05,040 --> 00:22:07,830
So, I create the router.

00:22:07,830 --> 00:22:10,000
It's kind of the same idea as the express server.

00:22:10,000 --> 00:22:15,940
So, I now create the router a route on home.

00:22:15,940 --> 00:22:20,450
And I just paste my callback.

00:22:20,450 --> 00:22:23,640
And now I create the server.

00:22:23,640 --> 00:22:28,400
I specify who will handle my server request.

00:22:28,400 --> 00:22:30,120
Who will be my router?

00:22:30,120 --> 00:22:33,080
And I start listening on port 8080.

00:22:33,080 --> 00:22:37,220
Same hello message.

00:22:37,220 --> 00:22:41,360
So, I'm running.

00:22:41,360 --> 00:22:50,929
So, now I can just install starting to make npm and doesn't really matter.

00:22:50,929 --> 00:22:52,990
There are a couple of utilities.

00:22:52,990 --> 00:22:57,790
I can quickly get all of my application running on VSCode.

00:22:57,790 --> 00:23:00,590
You can see it's already bugging.

00:23:00,590 --> 00:23:01,640
I can put a break point.

00:23:01,640 --> 00:23:07,820
And if I put a break point and now make an HTTP request, you see that the request there

00:23:07,820 --> 00:23:08,820
is stopped.

00:23:08,820 --> 00:23:15,640
And what's interesting here to see is that due to the nature of GraalVM, you can see

00:23:15,640 --> 00:23:22,340
on the debugger, the code from the Java side and the code that you wrote.

00:23:22,340 --> 00:23:23,850
Everything is optimized.

00:23:23,850 --> 00:23:32,100
So, the expectation is that once you write code in this way, your user code plus your

00:23:32,100 --> 00:23:38,350
runtime plus your interop plus your engine plus your IO libraries plus the whole world

00:23:38,350 --> 00:23:39,420
that runs your application.

00:23:39,420 --> 00:23:42,220
In this case, the Graal JDK.

00:23:42,220 --> 00:23:45,690
It will all be optimized by Graal.

00:23:45,690 --> 00:23:46,950
Not just the script itself.

00:23:46,950 --> 00:23:52,070
Not just optimizing the tip of the iceberg, you're optimizing everything.

00:23:52,070 --> 00:23:59,810
To test it, I submitted to the tech and power implementation using this project.

00:23:59,810 --> 00:24:04,670
It was reviewed and got accepted and this is how things are.

00:24:04,670 --> 00:24:07,210
This is like the CI builds.

00:24:07,210 --> 00:24:14,730
You see now ES4X is ranking on number five which brings JavaScript from number 86 if

00:24:14,730 --> 00:24:24,250
I'm not mistaken to number five in simple database query test that gets results.

00:24:24,250 --> 00:24:27,810
And ranked number six when doing multiple queries.

00:24:27,810 --> 00:24:32,419
So, you see the parallel loading and testing.

00:24:32,419 --> 00:24:39,260
So, if I have to compare now this experiment with all the frameworks that were already

00:24:39,260 --> 00:24:42,350
on the benchmark, this is how it compares.

00:24:42,350 --> 00:24:51,049
So, when working with JSON we see that the results give you like two times better results

00:24:51,049 --> 00:24:53,490
than the previous best result.

00:24:53,490 --> 00:24:57,980
When going on a post base, doing one query, it's three and a halftimes better.

00:24:57,980 --> 00:25:03,210
But if you have to be fair, testing for the best previous one running on Postpress, it's

00:25:03,210 --> 00:25:07,760
six times better doing multiple queries.

00:25:07,760 --> 00:25:09,870
There's lots of concurrency going on.

00:25:09,870 --> 00:25:13,390
It's still two and a halftimes better than the previous one.

00:25:13,390 --> 00:25:21,010
And doing data updates where the query is really the issue, it's like five times better

00:25:21,010 --> 00:25:22,120
than the previous one.

00:25:22,120 --> 00:25:30,679
To put this in numbers if you think about request response, you see that IO is better,

00:25:30,679 --> 00:25:32,090
of course.

00:25:32,090 --> 00:25:35,300
I'm not talking about a very small improvement, tiny improvements.

00:25:35,300 --> 00:25:38,640
I'm talking about huge numbers.

00:25:38,640 --> 00:25:45,060
So, the final tip is that optimization is like a neverending job.

00:25:45,060 --> 00:25:51,799
So, for example, we could get better results if we used an enterprise edition of Graal

00:25:51,799 --> 00:25:54,980
instead of the open source edition.

00:25:54,980 --> 00:25:58,070
That gives you like 20% better performance.

00:25:58,070 --> 00:26:01,380
And because it's optimization, it's a neverending job.

00:26:01,380 --> 00:26:07,860
You need to rinse and repeat and go just like that.

00:26:07,860 --> 00:26:14,590
So, the key points I want to give is that there's nothing wrong with JavaScript.

00:26:14,590 --> 00:26:16,780
JavaScript can be fast.

00:26:16,780 --> 00:26:21,680
And probably you don't need to switch to Go, Rust, whatever, because you're having performance

00:26:21,680 --> 00:26:22,680
issues.

00:26:22,680 --> 00:26:27,929
You can still if you dare to experiment, you can still remain on JavaScript.

00:26:27,929 --> 00:26:33,720
So, if you want to learn more you can.

00:26:33,720 --> 00:26:41,400
Either find me on Twitter, GitHub, the source code forum on GitHub.

00:26:41,400 --> 00:26:45,490
And if there are any questions, you can catch me later.

00:26:45,490 --> 00:26:51,000
Thank you.

00:26:51,000 --> 00:26:56,290

YouTube URL: https://www.youtube.com/watch?v=-npTuvzflh4


