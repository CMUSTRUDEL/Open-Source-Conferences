Title: The Case for Augmented Reality on the Web by Marley Rafson | JSConf EU 2019
Publication date: 2019-07-29
Playlist: JSConf EU 2019
Description: 
	Augmented reality is already making its way into everyday browsers! This talk will dive into what that might mean for the traditional web developer, and why developing immersive experiences makes so much sense on the web, even in the face of native alternatives. We will cover topics like off-the-shelf web technologies, performance, and privacy all in the context of augmented reality.

https://2019.jsconf.eu/marley-rafson/the-case-for-augmented-reality-on-the-web.html
Captions: 
	00:00:14,160 --> 00:00:15,160
[ Applause ] MARLEY: Thank you.

00:00:15,160 --> 00:00:16,160
And hello.

00:00:16,160 --> 00:00:17,160
Today I'll be talking to you about the case for augmented reality on the web.

00:00:17,160 --> 00:00:18,160
I'm Marley Rafson.

00:00:18,160 --> 00:00:19,210
I'm a software engineer at Google on the web XR team.

00:00:19,210 --> 00:00:21,470
My Twitter handle is mprafson.

00:00:21,470 --> 00:00:25,870
Love to talk letter.

00:00:25,870 --> 00:00:31,680
The two things today with the web and augmented reality.

00:00:31,680 --> 00:00:36,250
I believe that the combination of these two things, the whole is greater than the sum

00:00:36,250 --> 00:00:38,000
of its parts situation.

00:00:38,000 --> 00:00:43,579
I think that the web will gain from augmented reality, and things that augmented reality

00:00:43,579 --> 00:00:46,700
will certainly gain from the web.

00:00:46,700 --> 00:00:55,739
So, to begin, let's start by getting everyone on the same page and let's define augmented

00:00:55,739 --> 00:00:56,739
reality.

00:00:56,739 --> 00:01:01,289
It's the mixing of computergenerated content with the real world.

00:01:01,289 --> 00:01:06,750
To me, we can now add contextualized information into the world around us.

00:01:06,750 --> 00:01:11,750
So, for example, you're trying to make a coffee at the coffee machine, and you have the instructions

00:01:11,750 --> 00:01:14,780
overlaid on top of the coffee machine.

00:01:14,780 --> 00:01:22,530
And implication of augmented reality is you can interact with 3D content in a 3D space.

00:01:22,530 --> 00:01:27,170
So, the canonical example would be you're trying to buy a piece of furniture and you're

00:01:27,170 --> 00:01:31,160
able to put it into your room before you buy it.

00:01:31,160 --> 00:01:33,840
Let's look at what augmented reality looks like today.

00:01:33,840 --> 00:01:36,200
Today this is smartphone AR.

00:01:36,200 --> 00:01:40,140
This is run using a smart phone and basically has some understanding of the world around

00:01:40,140 --> 00:01:44,560
it allowing virtual content to interact with that world too.

00:01:44,560 --> 00:01:48,070
We have augmented reality on headsets.

00:01:48,070 --> 00:01:51,730
Your hands would be free, and you would be free to interact with the content using your

00:01:51,730 --> 00:01:52,850
hands.

00:01:52,850 --> 00:01:58,509
And the third example that I think is less talked about, but I find compelling is audiobased

00:01:58,509 --> 00:02:00,240
augmented reality.

00:02:00,240 --> 00:02:03,020
You're seeing Bose frames.

00:02:03,020 --> 00:02:07,560
It's a pair of sunglasses that have speakers that shoot audio into your ears.

00:02:07,560 --> 00:02:15,980
With this example, with audio AR, you would have that experience where you're at a coffee

00:02:15,980 --> 00:02:17,950
machine and you don't know what to do.

00:02:17,950 --> 00:02:21,129
But instead of seeing the information, it's read off to you.

00:02:21,129 --> 00:02:26,200
And it I would also like to say if you hear me say AR, that's just short for augmented

00:02:26,200 --> 00:02:28,080
reality.

00:02:28,080 --> 00:02:33,029
So, this talk, as I mentioned, it's also about the web.

00:02:33,029 --> 00:02:36,019
Let's talk about where those two things intersect.

00:02:36,019 --> 00:02:38,439
That's what we'll call the immersive web.

00:02:38,439 --> 00:02:44,109
And the immersive web is the use of augmented reality, virtual reality and other 3D technologies

00:02:44,109 --> 00:02:45,510
on the web.

00:02:45,510 --> 00:02:47,079
It's this whole spectrum of things.

00:02:47,079 --> 00:02:52,379
It can be more augmented reality than virtual reality and doesn't quite matter.

00:02:52,379 --> 00:02:56,209
And you'll hear me say XR.

00:02:56,209 --> 00:02:58,430
And that really just stands for wild card reality.

00:02:58,430 --> 00:03:04,699
So, it could be virtual, it could be augmented reality and some combination of the two.

00:03:04,699 --> 00:03:08,150
So, today what the immersive web looks like is something like this.

00:03:08,150 --> 00:03:13,559
We have seen progressive enhancements of sites we use every day.

00:03:13,559 --> 00:03:19,190
The left two examples, Facebook 3D posts and they're incorporated directly into the newsfeed.

00:03:19,190 --> 00:03:23,810
So, that first example is a 3D model that you can interact with on the newsfeed.

00:03:23,810 --> 00:03:30,379
The second is using portrait mode depth information to delightfully play with these photos with

00:03:30,379 --> 00:03:32,859
a bit of depth added.

00:03:32,859 --> 00:03:38,689
And the third example is from Wikipedia who about a year ago started supporting 3D models

00:03:38,689 --> 00:03:40,599
in their articles.

00:03:40,599 --> 00:03:46,769
So, if you're not a large company and you don't have a whole engineering team dedicated

00:03:46,769 --> 00:03:51,849
to building this model viewing experience, there's a solution to that now.

00:03:51,849 --> 00:03:59,620
So, recently Google, a team at Google open sourced a web component for viewing models.

00:03:59,620 --> 00:04:01,099
It's called Model viewer.

00:04:01,099 --> 00:04:02,579
It's fantastic.

00:04:02,579 --> 00:04:06,469
Using HTML, you can add into your side around it.

00:04:06,469 --> 00:04:10,400
That's an example with the astronaut which we know and love.

00:04:10,400 --> 00:04:14,569
And something that's great about the modelviewer team is they're thinking about accessibility

00:04:14,569 --> 00:04:15,569
early.

00:04:15,569 --> 00:04:22,210
So, they've already incorporated alt text into the component.

00:04:22,210 --> 00:04:26,699
So, let's say you have these 3D models on your website.

00:04:26,699 --> 00:04:29,870
How do you actually get it into augmented reality?

00:04:29,870 --> 00:04:36,639
So, right now what we've seen is implementations of native system viewers that allow for augmented

00:04:36,639 --> 00:04:37,710
reality.

00:04:37,710 --> 00:04:42,240
This is not running in a web browser, but this is slick and tight integration with the

00:04:42,240 --> 00:04:43,240
web.

00:04:43,240 --> 00:04:48,310
So, you would have this search result, so it's integrated into search with Google.

00:04:48,310 --> 00:04:54,180
So, you can tap the button and then it intents into this native viewer and then you can place

00:04:54,180 --> 00:05:02,270
it in augmented reality is the same exists on iOS and it's similarly intense.

00:05:02,270 --> 00:05:07,830
And so, what I just said there was that we're using native viewers to do augmented reality.

00:05:07,830 --> 00:05:12,990
But we would like to do that in the browser itself.

00:05:12,990 --> 00:05:17,580
That's where the WebXR device API comes in.

00:05:17,580 --> 00:05:19,990
It exposes lowlevel sensory information.

00:05:19,990 --> 00:05:25,270
So, the camera pose, if you can place an object on the floor.

00:05:25,270 --> 00:05:26,669
And that's under development.

00:05:26,669 --> 00:05:29,289
It's open so we can all contribute to it.

00:05:29,289 --> 00:05:34,280
And you can find that on GitHub.

00:05:34,280 --> 00:05:38,250
And with those examples that I just shared, that's all talking about bringing immersive

00:05:38,250 --> 00:05:40,349
technologies into the web.

00:05:40,349 --> 00:05:43,909
But we can also bring the web into immersive technologies.

00:05:43,909 --> 00:05:46,969
Two examples of XR browsers.

00:05:46,969 --> 00:05:55,370
And you will see the Helio browser placing images into the world around you and then

00:05:55,370 --> 00:05:59,560
Firefox Reality, which is a VR browser.

00:05:59,560 --> 00:06:03,770
So, I think the immersive web today is fantastic.

00:06:03,770 --> 00:06:08,590
I think it's adding information, it's making it more delightful to browse the web.

00:06:08,590 --> 00:06:13,110
But when we're developing these technologies, it's important to see where we're going next

00:06:13,110 --> 00:06:16,199
and to start thinking about that earlier rather than later.

00:06:16,199 --> 00:06:20,880
So, when I think about a future of augmented reality, I start by thinking about how can

00:06:20,880 --> 00:06:22,610
this assist me?

00:06:22,610 --> 00:06:27,919
So, I think about maybe I'm leaving work one day and I'm wearing these lightweight AR goggles,

00:06:27,919 --> 00:06:29,199
I'm walking down the street.

00:06:29,199 --> 00:06:30,610
I'm looking for dinner.

00:06:30,610 --> 00:06:38,180
So, as I look at the restaurants on the street, I'm seeing Yelp data populating with the stars

00:06:38,180 --> 00:06:41,070
rating and the types of menu.

00:06:41,070 --> 00:06:43,810
And maybe I also want to see photos of the food.

00:06:43,810 --> 00:06:47,610
So, Instagram information's coming too.

00:06:47,610 --> 00:06:48,659
And I like Pokemon.

00:06:48,659 --> 00:06:53,780
So, maybe while I'm doing all of this, I'm catching Pokemon, playing Pokemon Go.

00:06:53,780 --> 00:06:58,560
And when I get to the train station, I'll have information populated for me telling

00:06:58,560 --> 00:07:01,949
me which train to take so I can get home fastest.

00:07:01,949 --> 00:07:07,080
And as I mentioned earlier, I probably don't want to be restricted to doing this on just

00:07:07,080 --> 00:07:08,740
one headset.

00:07:08,740 --> 00:07:12,020
Maybe I want headphones another day from a different company.

00:07:12,020 --> 00:07:13,870
Who knows?

00:07:13,870 --> 00:07:16,080
Maybe I just want to use my smartphone.

00:07:16,080 --> 00:07:21,300
So, when I'm thinking of this vision of augmented reality going forward, I start to see some

00:07:21,300 --> 00:07:24,689
interesting things and patterns that I would like to call out here.

00:07:24,689 --> 00:07:28,520
So, the first is that is inherently uses web content.

00:07:28,520 --> 00:07:33,520
I want access to all the things that I already use when I'm browsing the web.

00:07:33,520 --> 00:07:40,069
Another thing is it's this really interesting composition of different types of information.

00:07:40,069 --> 00:07:42,520
So, that could be models or 2D pages.

00:07:42,520 --> 00:07:47,849
And we're going to have to interact with them in a way that's esthetic and delightful and

00:07:47,849 --> 00:07:50,080
still makes me want to use it.

00:07:50,080 --> 00:07:54,969
And onethird trend that I start to see it we're really interacting with the world around

00:07:54,969 --> 00:07:55,969
us.

00:07:55,969 --> 00:08:01,169
I could be places information on to a building or on to a tree or on passing by cars.

00:08:01,169 --> 00:08:07,580
And so, when I start to think about this vision that I want and where we are today, which

00:08:07,580 --> 00:08:12,919
is mostly placing 3D models, I think there is a lot of work to go.

00:08:12,919 --> 00:08:17,600
And where I see a lot of that work coming into play is with a user agent.

00:08:17,600 --> 00:08:25,629
So, the definition from W3C of a user agent is a user agent is any software that retrieves,

00:08:25,629 --> 00:08:29,599
renders and facilitates end user interaction with web content.

00:08:29,599 --> 00:08:33,320
So, today that's usually just your browser.

00:08:33,320 --> 00:08:38,620
So, you get a lot of things when you're browsing using any of the modern browsers.

00:08:38,620 --> 00:08:40,810
It could be Chrome, it could be Firefox.

00:08:40,810 --> 00:08:42,510
It helps you do safe browsing.

00:08:42,510 --> 00:08:47,300
It renders the HTML pages for you and things like that.

00:08:47,300 --> 00:08:53,050
So, to understand how a user agent will play out with this added dimension for augmented

00:08:53,050 --> 00:08:57,449
reality, let's start to break down this definition of user agent.

00:08:57,449 --> 00:09:00,100
So, I'll go in reverse order.

00:09:00,100 --> 00:09:05,970
Start with a user agent facilitates end user interaction with web content.

00:09:05,970 --> 00:09:12,250
So, what I believe is that the user should always be in control of how information is

00:09:12,250 --> 00:09:13,590
presented to them.

00:09:13,590 --> 00:09:19,000
So, on a modern web browser, that's preferences like default text sizing.

00:09:19,000 --> 00:09:21,480
That's contrast ratios.

00:09:21,480 --> 00:09:24,330
And Chrome extensions and things like that.

00:09:24,330 --> 00:09:29,660
But when you think about augmented reality, we have this whole new dimension.

00:09:29,660 --> 00:09:32,180
It's really immersive.

00:09:32,180 --> 00:09:34,290
It's, you know?

00:09:34,290 --> 00:09:39,321
And so, we need to start to think about other ways that we want to have safe browsing and

00:09:39,321 --> 00:09:41,000
we want the user to be in control.

00:09:41,000 --> 00:09:45,000
One example, think about physical proximity.

00:09:45,000 --> 00:09:50,500
You have content that's being rendered possibly close to you and I should be able to say I'm

00:09:50,500 --> 00:09:54,870
not comfortable with anything rendering closer than 5 meters away from me.

00:09:54,870 --> 00:09:58,610
Or I think it's compelling to think about this in terms of sound.

00:09:58,610 --> 00:10:03,130
I probably would never want to allow someone to whisper into my ear as I'm walking down

00:10:03,130 --> 00:10:04,730
the street.

00:10:04,730 --> 00:10:10,680
so, a lot of these considerations come into play when we're talking about augmented reality.

00:10:10,680 --> 00:10:14,870
And in order to accommodate these things for the years, we're going to need an intervention

00:10:14,870 --> 00:10:16,750
point for the information.

00:10:16,750 --> 00:10:21,440
So, the user agent knows what's being asked of it to do this.

00:10:21,440 --> 00:10:28,459
If we think of declarative content that we know and love, that would be HTML and CSS.

00:10:28,459 --> 00:10:33,279
I see this future where there's declarative content that you're able to add into the world

00:10:33,279 --> 00:10:41,620
around you so you could specify things like I would like float left and pin to a building.

00:10:41,620 --> 00:10:46,060
Or you could say, I want the depth to be 5 meters from the user.

00:10:46,060 --> 00:10:51,800
But it's up to the user agent to decide whether or not it can actually honor that request.

00:10:51,800 --> 00:10:57,490
And it's in that way that it can advocate for the user.

00:10:57,490 --> 00:11:05,279
So, another thing that declarative content offers is that is provides semantic understanding

00:11:05,279 --> 00:11:06,300
for the user agent.

00:11:06,300 --> 00:11:12,230
So, what that means is that the user agent has a view into what's being asking to rendered

00:11:12,230 --> 00:11:14,490
or presented to the use.

00:11:14,490 --> 00:11:18,980
I find this example extremely lie lighting of what I'm talking about.

00:11:18,980 --> 00:11:23,029
If we look at the left side, you're seeing the web.

00:11:23,029 --> 00:11:30,470
If given this canvas to render the object like on the web today, given an XY location,

00:11:30,470 --> 00:11:32,750
you can only see the color.

00:11:32,750 --> 00:11:36,570
I see a black pixel, a white pixel, a gray pixel.

00:11:36,570 --> 00:11:37,960
But nothing more than that.

00:11:37,960 --> 00:11:41,759
But with semantic understanding of what's actually in the scene, we have a much more

00:11:41,759 --> 00:11:45,170
rich understanding of the model the and the content that we're placing.

00:11:45,170 --> 00:11:50,230
So, if you look at the right side, you can see I'm looking at the windshield.

00:11:50,230 --> 00:11:52,720
And it has a material that's class.

00:11:52,720 --> 00:11:54,360
And it's transparent.

00:11:54,360 --> 00:11:57,350
You can also see things like the tire and stuff like that.

00:11:57,350 --> 00:12:02,900
And so, you can imagine with screenreaders or accessible or just translating this into

00:12:02,900 --> 00:12:07,080
soundbased AR, you can actually talk about what the user is seeing.

00:12:07,080 --> 00:12:13,490
So, you could say I see a van with black tires and white car metal paint.

00:12:13,490 --> 00:12:19,579
So, the next part of the user agent that I'll talk about is rendering.

00:12:19,579 --> 00:12:25,120
So, the user agent is response for rendering to the browser.

00:12:25,120 --> 00:12:29,160
So, modern web rendering emergencies are amazing.

00:12:29,160 --> 00:12:30,800
They're fantastic.

00:12:30,800 --> 00:12:33,120
That's blank, echo, WebKit, things like that.

00:12:33,120 --> 00:12:38,810
When you think about what they asked to do, it's mostly 2D and it's textbased.

00:12:38,810 --> 00:12:45,170
They're fantastic at rendering pages like the verge at 60 frames per second.

00:12:45,170 --> 00:12:50,600
And some of you might know and some of you might not, but the web already does performant

00:12:50,600 --> 00:12:51,600
3D rendering.

00:12:51,600 --> 00:12:58,860
That's great and provided to the user through a Canvas element and uses WebGL to create

00:12:58,860 --> 00:13:00,580
those graphics.

00:13:00,580 --> 00:13:05,240
What that might look like today is this, this is not augmented or virtual reality.

00:13:05,240 --> 00:13:10,660
But it's just rendering in a web page using a canvas and WebGL.

00:13:10,660 --> 00:13:17,810
So, for the rendering needs of augmented reality, thinking back to the vision is this complex

00:13:17,810 --> 00:13:21,709
composition and layout dynamics of content.

00:13:21,709 --> 00:13:25,860
So, we're gonna probably need to build a rendering engine that can handle that from the ground

00:13:25,860 --> 00:13:26,860
up.

00:13:26,860 --> 00:13:35,850
Whereas the modern rendering engines were optimized for 2D.

00:13:35,850 --> 00:13:40,139
And so, even though I just said that we probably want this authorized from the ground up and

00:13:40,139 --> 00:13:43,470
that we don't want to use Canvas elements, we tried it.

00:13:43,470 --> 00:13:50,480
So, my team experimented on this internal prototype, air web view.

00:13:50,480 --> 00:13:58,519
It's a set of libraries that enable AR through native apps and web view.

00:13:58,519 --> 00:14:05,220
It's a lightweight browse their you can embed into a native app.

00:14:05,220 --> 00:14:12,230
The technical understanding of what we've done is that we've web rendered and using

00:14:12,230 --> 00:14:17,389
that to do than we have tracking, AR Core and ARKit.

00:14:17,389 --> 00:14:20,889
And we combine that with a natively rendered camera feed.

00:14:20,889 --> 00:14:25,080
Let's look at what that looks like.

00:14:25,080 --> 00:14:29,540
You look at the example, we have a stormtrooper and it's convincingly rendered into the space

00:14:29,540 --> 00:14:31,470
around you.

00:14:31,470 --> 00:14:36,460
You need to make sure that the content that's rendered moves at the same time as a native

00:14:36,460 --> 00:14:37,790
camera feed.

00:14:37,790 --> 00:14:40,240
Because if it's off, it completely breaks the illusion.

00:14:40,240 --> 00:14:44,649
So, we have this transparent web view, it's rendering the content, and this becomes a

00:14:44,649 --> 00:14:46,690
difficult technical problem.

00:14:46,690 --> 00:14:49,120
Because if you're familiar, and don't worry if you're not.

00:14:49,120 --> 00:14:52,510
You have a render loop that's running from the browser.

00:14:52,510 --> 00:14:56,240
But then you also have a render loop that's running natively, and you also have the update

00:14:56,240 --> 00:14:58,120
loop for the native tracking.

00:14:58,120 --> 00:15:01,930
So, it's figuring out a way to get all of these things working together in a performant

00:15:01,930 --> 00:15:03,360
way.

00:15:03,360 --> 00:15:08,860
And the advantages of AR web view is it's lightweight.

00:15:08,860 --> 00:15:11,250
It's only 60 kilobytes added to your app.

00:15:11,250 --> 00:15:12,449
It's crossplatform.

00:15:12,449 --> 00:15:16,759
So, you write code once and it works on both iOS and Android.

00:15:16,759 --> 00:15:21,060
It's embeddable into native apps and you can pull down content and code on demand.

00:15:21,060 --> 00:15:29,390
And that last part, to highlight the advantages of that is that you can make changes to this

00:15:29,390 --> 00:15:32,970
augmented reality experience that's independent from your native app code.

00:15:32,970 --> 00:15:38,750
You don't have to wait for an app release to change your AR experience.

00:15:38,750 --> 00:15:43,820
So, one use of AR web view in action was at Google IO this year.

00:15:43,820 --> 00:15:50,170
If you were there, you would have seen this experience that allowed you to orient yourself

00:15:50,170 --> 00:15:53,060
once you're on site at the conference.

00:15:53,060 --> 00:15:57,639
Using augmented reality, you could see which way you might want to go next.

00:15:57,639 --> 00:16:01,010
And so, this experience is literally just a web page.

00:16:01,010 --> 00:16:06,449
And these white text that say water station and eats market are just actually HTML elements

00:16:06,449 --> 00:16:08,850
styled with 3D transforms.

00:16:08,850 --> 00:16:15,029
So, again, AR web view, it's a prototype, it's internal.

00:16:15,029 --> 00:16:18,319
We will a lot of fun and learned a lot with it.

00:16:18,319 --> 00:16:25,399
So, this last aspect of the user agent that I'll talk about is that the user agent retrieves

00:16:25,399 --> 00:16:27,689
content on behalf of the user.

00:16:27,689 --> 00:16:30,230
And this part is so fascinating to me.

00:16:30,230 --> 00:16:35,810
So, if you think back to this vision of augmented reality, you're walking down the street.

00:16:35,810 --> 00:16:39,889
And you look at a restaurant and it knows what you want is the menu information.

00:16:39,889 --> 00:16:42,230
And it knows what you want is the Instagram photos.

00:16:42,230 --> 00:16:47,189
And so, if I was doing this today on my smartphone, I would go to the restaurant.

00:16:47,189 --> 00:16:48,649
I would see the name.

00:16:48,649 --> 00:16:52,720
I would type it into maybe Google search, who knows.

00:16:52,720 --> 00:16:54,870
And then I would access the information that I wanted.

00:16:54,870 --> 00:17:00,290
But what we have with augmented reality is this fundamental paradigm shift.

00:17:00,290 --> 00:17:06,150
Now the user agent is querying on my behalf and should know what I want to see.

00:17:06,150 --> 00:17:12,000
That's very interesting if in a camerafirst browser environment, the user receives content

00:17:12,000 --> 00:17:16,710
rather than querying for it.

00:17:16,710 --> 00:17:22,500
That means that the user gets it over the surfacing.

00:17:22,500 --> 00:17:25,070
This is an interesting question off the bat.

00:17:25,070 --> 00:17:29,430
We can think about modern questions like echo chambers and stuff like that.

00:17:29,430 --> 00:17:34,880
Manage imagine you're in an augmented reality immersive world and you don't know when there's

00:17:34,880 --> 00:17:36,990
content that you're not seeing.

00:17:36,990 --> 00:17:43,150
Or you should have a way to know and have ways to set preferences on what you do want

00:17:43,150 --> 00:17:44,150
to see.

00:17:44,150 --> 00:17:47,300
And I think that there is kind of this pessimistic view of that.

00:17:47,300 --> 00:17:52,270
But I think there's a lot of advantages of having this intermediary between you and the

00:17:52,270 --> 00:17:53,470
information.

00:17:53,470 --> 00:17:57,820
An example that I like to think Imagine you're a coffee drinker.

00:17:57,820 --> 00:18:01,850
You wake up, you want to wear your AR glasses on the way to work.

00:18:01,850 --> 00:18:06,390
You haven't had your glass of coffee, you want the bare minimum.

00:18:06,390 --> 00:18:10,650
Until you get to the coffee shop, then you are ready to get the other things coming.

00:18:10,650 --> 00:18:19,470
In that way, the user agent having discretion is an advantage.

00:18:19,470 --> 00:18:22,680
And so, as you start to hear these things about augmented reality and maybe this future

00:18:22,680 --> 00:18:25,400
where everybody is wearing headsets.

00:18:25,400 --> 00:18:32,430
You might be like me yesterday in Conrad electronics thinking, that's a lot of cameras.

00:18:32,430 --> 00:18:35,380
So, it's true.

00:18:35,380 --> 00:18:40,160
A camerafirst future requires a lot of cameras.

00:18:40,160 --> 00:18:42,350
And it's not just augmented reality that's facing these things.

00:18:42,350 --> 00:18:47,680
It would be things like selfdriving cars where they have six censors, eight, a ton of sensors

00:18:47,680 --> 00:18:54,090
looking at the world around it as they're processing.

00:18:54,090 --> 00:18:59,990
What that means is we're capturing basic life everything that happens on the sidewalk.

00:18:59,990 --> 00:19:02,630
Anything that someone wearing one of these headsets can see.

00:19:02,630 --> 00:19:08,630
And so, as we try to build towards this augmented reality feature, we need to have privacy and

00:19:08,630 --> 00:19:13,560
surveillance top of mind when thinking about worldscale AR.

00:19:13,560 --> 00:19:18,760
So, at face value, I think one of the first things you would think about in the context

00:19:18,760 --> 00:19:24,880
of privacy is that these cameras are capturing RGB data about the world around you.

00:19:24,880 --> 00:19:25,880
Which is just a picture.

00:19:25,880 --> 00:19:28,740
So, what happens with that picture data?

00:19:28,740 --> 00:19:32,470
Is your device computing it on device?

00:19:32,470 --> 00:19:33,960
Is it going up to the cloud?

00:19:33,960 --> 00:19:35,600
Who owns that data on the cloud?

00:19:35,600 --> 00:19:37,050
Is it going to your private cloud?

00:19:37,050 --> 00:19:40,600
And then when you think about secondary data.

00:19:40,600 --> 00:19:45,670
So, that's information that's extracted from this camera image which is how augmented reality

00:19:45,670 --> 00:19:46,670
works.

00:19:46,670 --> 00:19:49,280
You can imagine if you're building meshes around you.

00:19:49,280 --> 00:19:54,080
You live in an apartment building with a distinct statue outside.

00:19:54,080 --> 00:19:55,990
You walk out.

00:19:55,990 --> 00:20:02,830
Your headset creates a mesh of this statue and someone could know exactly where that

00:20:02,830 --> 00:20:04,870
statue is and locate you.

00:20:04,870 --> 00:20:10,450
Even if they don't have the camera data, they can understand that from the derivative information

00:20:10,450 --> 00:20:15,220
that's used for augmented reality.

00:20:15,220 --> 00:20:19,680
And what I think, though, is that user user agents being more powerful is actually an

00:20:19,680 --> 00:20:21,460
advantage in this scenario.

00:20:21,460 --> 00:20:25,520
Because if we can move that information away from the developer and closer to a trusted

00:20:25,520 --> 00:20:27,700
user agent, that's a positive thing.

00:20:27,700 --> 00:20:33,690
But we'll need something like open source code to actually know that we're being that

00:20:33,690 --> 00:20:36,670
we can trust our user agents.

00:20:36,670 --> 00:20:46,530
So, to me, augmented reality and the web is an amazing partnership.

00:20:46,530 --> 00:20:50,760
The web will gain a whole new access for understanding information.

00:20:50,760 --> 00:20:51,950
It will be more contextualized.

00:20:51,950 --> 00:20:53,850
It will be ergonomic.

00:20:53,850 --> 00:20:58,130
And for augmented reality, the web for sure is an important piece.

00:20:58,130 --> 00:21:03,050
It will provide the content and I love the webby principles that we use today.

00:21:03,050 --> 00:21:08,220
I think as we look forward to a future with augmented reality, we need to keep those webby

00:21:08,220 --> 00:21:09,990
principles in line.

00:21:09,990 --> 00:21:17,470
If you are interested in using the resources today, check out the resources.

00:21:17,470 --> 00:21:24,980
Look at the model viewer and see if adding 3D models increases use and context to your

00:21:24,980 --> 00:21:25,980
information.

00:21:25,980 --> 00:21:30,320
And also, start talking to us about the WebXR device API.

00:21:30,320 --> 00:21:34,970
Everyone's opinion matters and it will be great for the ecosystem overall.

00:21:34,970 --> 00:21:41,510
So, to me if the web plus augmented reality is an open ecosystem of rich, contextualized

00:21:41,510 --> 00:21:51,000
information, then I'm excited and I hope that you all are too.

00:21:51,000 --> 00:21:52,000

YouTube URL: https://www.youtube.com/watch?v=Jk4tEMBJleE


