Title: Using ML Node.JS - London Node User Group - July 2019
Publication date: 2019-08-06
Playlist: London Node User Group
Description: 
	Using ML Node.JS
Maximilian Berkman

An overview of ML in JS and what I was able to do with it.
How and why would use Machine Learning in NodeJS.
Iâ€™ll go over some general concepts in ML and then dive into how I leveraged limdu and created a multi-label classifier (called ac-learn) and how you can do the same. All while seeing how fun and amazing it can be.
The talk would be focused on giving you the pointers and tools to solve problems where ML is usually the best solution as well as how you can hit the ground running.

About
I'm a Computer Science placement student working at BWC in Project Management, Web Dev and various other things (e.g. GDPR and IT support). I've been working with JS for about 7 years and with NodeJS for about 3 years and simply love almost everything about it.
Twitter: @Berkmann18

_

About Pusher Sessions:

We're bringing the meetup to you. With Sessions, you can watch recordings of top-notch talks from developer meetups -- wherever and whenever you want.

Meetups are a great way to learn from our peers and to keep up with the latest trends and technologies. As developers ourselves, we at Pusher wanted to bring this great content to more people... So we built Sessions. On Sessions, you can watch talks that interest you and subscribe to be notified when new content gets added.

If you run a meetup and want to get involved, kindly get in touch.

_

About Pusher:

Pusher is a hosted service with APIs, developer tools and open source libraries that greatly simplify integrating real-time functionality into web and mobile applications. 

Pusher will automatically scale when required, removing all the pain of setting up and maintaining a secure, real-time infrastructure. 

Pusher is already trusted to do so by thousands of developers and companies like GitHub, MailChimp, the Financial Times, Buffer and many more. 

Getting started takes just a few seconds: simply go to pusher.com and create a free account. Happy hacking!
Captions: 
	00:00:00,030 --> 00:00:08,130
so my talk is going to be in machine

00:00:03,210 --> 00:00:10,920
learning in ojs so my name is Maximilian

00:00:08,130 --> 00:00:12,840
I'm a computer scientist student at

00:00:10,920 --> 00:00:15,809
voyeur hallway currently doing

00:00:12,840 --> 00:00:18,270
placements as IIT internal Berkman one

00:00:15,809 --> 00:00:21,359
solace in web development project

00:00:18,270 --> 00:00:26,310
management's gdpr compliance and Archer

00:00:21,359 --> 00:00:28,949
supports I'm not a data scientists so

00:00:26,310 --> 00:00:31,949
why I got into machine learning well I

00:00:28,949 --> 00:00:34,920
was interested in bioethics and malware

00:00:31,949 --> 00:00:37,079
so Microsoft when at University and I

00:00:34,920 --> 00:00:41,300
had problem to solve on the open source

00:00:37,079 --> 00:00:44,550
project being the all contributors CLI

00:00:41,300 --> 00:00:46,350
violence or there's a femoral libraries

00:00:44,550 --> 00:00:48,469
and tools most of which are for deep

00:00:46,350 --> 00:00:53,039
learning or neural networks in general

00:00:48,469 --> 00:00:54,559
it can be easy to implement or how

00:00:53,039 --> 00:00:58,980
depending on what you're trying to do

00:00:54,559 --> 00:01:02,340
you don't need to know if I turn or even

00:00:58,980 --> 00:01:05,970
MATLAB to basically get your machine

00:01:02,340 --> 00:01:08,640
learning model up and running the mouse

00:01:05,970 --> 00:01:11,700
you've seen papers machine learnings

00:01:08,640 --> 00:01:14,880
lectures and all that stuff isn't real

00:01:11,700 --> 00:01:16,740
quiet but nevertheless is helpful as

00:01:14,880 --> 00:01:22,259
visuals are things were quite a bit of

00:01:16,740 --> 00:01:25,560
mouse background so what is ml ml stands

00:01:22,259 --> 00:01:28,409
for machine learning don't mature so I

00:01:25,560 --> 00:01:30,329
just famous carrot which is a computer

00:01:28,409 --> 00:01:33,270
program is said to learn some expense e

00:01:30,329 --> 00:01:37,500
with respect to some costs of tasks T

00:01:33,270 --> 00:01:39,540
and fashion piece if the specimens are

00:01:37,500 --> 00:01:43,770
in task T measured by P impressed with

00:01:39,540 --> 00:01:46,170
expands e in simple language machine

00:01:43,770 --> 00:01:48,479
learning is a field in which human maids

00:01:46,170 --> 00:01:51,180
algorithms have an ability to learn by

00:01:48,479 --> 00:01:55,950
themselves and predict the future for

00:01:51,180 --> 00:01:56,430
unseen data so any of its AI so that's a

00:01:55,950 --> 00:01:58,350
thing

00:01:56,430 --> 00:02:01,229
a sequel a lot of people still

00:01:58,350 --> 00:02:04,549
struggling with in differentiating so AI

00:02:01,229 --> 00:02:06,960
support fields where the NC making

00:02:04,549 --> 00:02:10,679
machine smart intelligence and the

00:02:06,960 --> 00:02:13,140
artificial intelligence name mmm as you

00:02:10,679 --> 00:02:13,709
can see in the graph here is a subset of

00:02:13,140 --> 00:02:17,280
it

00:02:13,709 --> 00:02:21,049
with deep learning bangers of say some

00:02:17,280 --> 00:02:23,370
types and models you'll be using or

00:02:21,049 --> 00:02:26,010
encountering when you do machine

00:02:23,370 --> 00:02:29,310
learning is supervised which is

00:02:26,010 --> 00:02:31,799
essentially where you have no knowing

00:02:29,310 --> 00:02:33,810
data will basically labeled where you

00:02:31,799 --> 00:02:35,969
know dancer like a teacher's who knows

00:02:33,810 --> 00:02:39,480
what's right what's wrong or you have

00:02:35,969 --> 00:02:41,340
classification regression basically for

00:02:39,480 --> 00:02:43,379
texts and numbers respectively

00:02:41,340 --> 00:02:45,720
you have unsupervised learning where you

00:02:43,379 --> 00:02:47,939
don't know the answer but you want your

00:02:45,720 --> 00:02:51,150
machine learning model to basically find

00:02:47,939 --> 00:02:54,720
the structure or basically what's common

00:02:51,150 --> 00:02:58,859
between each data elements the third one

00:02:54,720 --> 00:03:00,299
being reinforcement learning wish as the

00:02:58,859 --> 00:03:03,510
image here suggests is where you have

00:03:00,299 --> 00:03:06,150
agents like a robot that does certain

00:03:03,510 --> 00:03:08,849
set of actions in an environment say a

00:03:06,150 --> 00:03:10,889
map or like a game and then you the

00:03:08,849 --> 00:03:14,760
person are you going to be rewarding it

00:03:10,889 --> 00:03:19,109
or punishing it depending on how it

00:03:14,760 --> 00:03:21,120
performed that said actions then on to

00:03:19,109 --> 00:03:23,400
the deep learning so deep learning is a

00:03:21,120 --> 00:03:26,790
bit special because it can be done in a

00:03:23,400 --> 00:03:31,439
supervised or unsupervised way with no

00:03:26,790 --> 00:03:33,359
networks and you have semi-supervised

00:03:31,439 --> 00:03:35,430
learning where Sheltie named success is

00:03:33,359 --> 00:03:37,889
basically a mix of supervised and

00:03:35,430 --> 00:03:40,650
unsupervised learning the only

00:03:37,889 --> 00:03:43,979
difference is that you will have

00:03:40,650 --> 00:03:45,959
typically most of your data without any

00:03:43,979 --> 00:03:49,769
labels so you'd be like an enterprise

00:03:45,959 --> 00:03:52,079
learning approach except you have some

00:03:49,769 --> 00:03:55,769
of you data with a label say I don't

00:03:52,079 --> 00:04:00,239
know element that has the answers like a

00:03:55,769 --> 00:04:04,109
class or the price or whether the output

00:04:00,239 --> 00:04:07,739
is the last one is online learning so

00:04:04,109 --> 00:04:10,709
this one can be used in a supervised or

00:04:07,739 --> 00:04:13,590
enterprise approach the only difference

00:04:10,709 --> 00:04:16,829
compared to the traditional surveillance

00:04:13,590 --> 00:04:20,370
was approaches is that it's a learning a

00:04:16,829 --> 00:04:22,139
whole data set of training elements it

00:04:20,370 --> 00:04:25,080
was going to be looking at one element

00:04:22,139 --> 00:04:27,320
and then learning from this and going on

00:04:25,080 --> 00:04:32,270
Parliament instead of going

00:04:27,320 --> 00:04:34,010
whole set and then learning from it what

00:04:32,270 --> 00:04:36,500
would you use machine learning in

00:04:34,010 --> 00:04:39,620
JavaScript well the good things are you

00:04:36,500 --> 00:04:42,170
can use it directly in the browser as

00:04:39,620 --> 00:04:44,300
well as in noisy environments it's

00:04:42,170 --> 00:04:46,750
easier to integrate in code bases

00:04:44,300 --> 00:04:50,840
written JavaScript or even typescript

00:04:46,750 --> 00:04:53,390
and the online learning is muy cooperate

00:04:50,840 --> 00:04:55,760
it has its basic essentially just some

00:04:53,390 --> 00:04:58,460
JavaScript lines of codes that you run

00:04:55,760 --> 00:05:01,310
within your web app code base or your

00:04:58,460 --> 00:05:05,630
modern app for anything that basically

00:05:01,310 --> 00:05:08,750
you want a JavaScript system it's easier

00:05:05,630 --> 00:05:12,530
to honor smash learning without having

00:05:08,750 --> 00:05:15,470
to know or even use Python or MATLAB or

00:05:12,530 --> 00:05:18,620
any other classical machine learning

00:05:15,470 --> 00:05:21,560
languages and frameworks the bad things

00:05:18,620 --> 00:05:24,710
about it though is that there's a lack

00:05:21,560 --> 00:05:29,890
of maturity and completeness in the

00:05:24,710 --> 00:05:33,170
libraries you have basically because

00:05:29,890 --> 00:05:35,660
most of them are quite young compared to

00:05:33,170 --> 00:05:38,420
the Python nor alternatives most

00:05:35,660 --> 00:05:40,990
efficient faster more efficient namely

00:05:38,420 --> 00:05:45,020
because they were around for longer and

00:05:40,990 --> 00:05:48,260
they were experimented and child.and a

00:05:45,020 --> 00:05:52,490
lot more and then being usually more

00:05:48,260 --> 00:05:54,470
major you will be reinventing the wheel

00:05:52,490 --> 00:05:56,450
on some concept but that's usually not a

00:05:54,470 --> 00:05:58,190
bad thing but you see of something you

00:05:56,450 --> 00:06:00,830
need to bear in mind when you use

00:05:58,190 --> 00:06:02,990
machine learning JavaScript and the last

00:06:00,830 --> 00:06:06,920
thing is that some of the algorithms you

00:06:02,990 --> 00:06:10,910
need like LSA so that sense semantic

00:06:06,920 --> 00:06:16,880
analysis or adaboost and available yet

00:06:10,910 --> 00:06:19,880
or at all in JavaScript so my vision to

00:06:16,880 --> 00:06:21,770
also display a lot especially in deep in

00:06:19,880 --> 00:06:24,470
the deep learning and no network side

00:06:21,770 --> 00:06:25,370
wins but some of the ones are fused

00:06:24,470 --> 00:06:28,280
arlindo

00:06:25,370 --> 00:06:30,590
for classification on my learning energy

00:06:28,280 --> 00:06:32,840
Jas which seems to be the most popular

00:06:30,590 --> 00:06:34,720
one which also can do a lot of things

00:06:32,840 --> 00:06:37,910
like clustering in unsupervised learning

00:06:34,720 --> 00:06:39,539
there are things like LD a for dimension

00:06:37,910 --> 00:06:42,869
reduction

00:06:39,539 --> 00:06:46,589
20s who who some of you my a few tin

00:06:42,869 --> 00:06:50,189
like a a ijs workshop there's other

00:06:46,589 --> 00:06:54,989
things like mark of Jas went for chairs

00:06:50,189 --> 00:06:57,599
for reinforcement learning and so on so

00:06:54,989 --> 00:07:00,449
further ado I'm going to do a

00:06:57,599 --> 00:07:03,689
demonstration using on library are made

00:07:00,449 --> 00:07:06,829
on top of Lynda choose over multi-class

00:07:03,689 --> 00:07:10,049
classification problem I was facing to

00:07:06,829 --> 00:07:12,869
trying to automate or contribute to CLI

00:07:10,049 --> 00:07:15,799
process on the node.js you're going to

00:07:12,869 --> 00:07:26,629
first basically get the learner so

00:07:15,799 --> 00:07:30,619
that's then you're going to create your

00:07:26,629 --> 00:07:33,799
learning instance so basically the

00:07:30,619 --> 00:07:36,629
object that it's going to be

00:07:33,799 --> 00:07:38,149
incorporating all the datasets training

00:07:36,629 --> 00:07:42,149
sites and all the things that will be

00:07:38,149 --> 00:07:46,019
allowing it to train and evaluate its

00:07:42,149 --> 00:07:48,899
knowledge on any new data hook no data

00:07:46,019 --> 00:07:53,129
so for the purpose of this mmm just make

00:07:48,899 --> 00:07:59,009
you use the default parameters so a

00:07:53,129 --> 00:08:02,369
salon comes with data set of about 486

00:07:59,009 --> 00:08:06,989
Lausanne checked of data then github

00:08:02,369 --> 00:08:09,659
labels so this like announcement bug do

00:08:06,989 --> 00:08:12,269
not merge and so on basically like what

00:08:09,659 --> 00:08:15,659
you see in issues for requests and such

00:08:12,269 --> 00:08:20,519
and it's going to be initializing

00:08:15,659 --> 00:08:24,719
training sets of about 70% of data and

00:08:20,519 --> 00:08:26,849
the validation sets now get in into

00:08:24,719 --> 00:08:29,369
later on and the training sites so

00:08:26,849 --> 00:08:31,619
that's essentially what it looks like so

00:08:29,369 --> 00:08:34,699
you have all the data with inputs being

00:08:31,619 --> 00:08:43,189
the hit of label and the output being

00:08:34,699 --> 00:08:47,279
the answer which is one of twenty hates

00:08:43,189 --> 00:08:50,699
or cosh with labels here so that's what

00:08:47,279 --> 00:08:52,280
the young specification uses to classify

00:08:50,699 --> 00:08:54,620
contributions on

00:08:52,280 --> 00:09:02,000
we put or anything where you can apply

00:08:54,620 --> 00:09:04,580
the what we do specification here you

00:09:02,000 --> 00:09:09,080
can see the classifier so that part is

00:09:04,580 --> 00:09:11,870
what comes directly from limiter which

00:09:09,080 --> 00:09:16,070
is essentially unknown network that uses

00:09:11,870 --> 00:09:21,440
T we now offer them so we'll further ado

00:09:16,070 --> 00:09:22,790
I'm going to basically train it so this

00:09:21,440 --> 00:09:26,320
library comes with the option of just

00:09:22,790 --> 00:09:29,270
training it with we call our training

00:09:26,320 --> 00:09:30,830
function but usually machine learning

00:09:29,270 --> 00:09:33,650
it's better to do what we call a

00:09:30,830 --> 00:09:36,280
cross-validation so what cost vibration

00:09:33,650 --> 00:09:40,400
is that you going to be taking a

00:09:36,280 --> 00:09:41,780
training set so 70% of data and a

00:09:40,400 --> 00:09:45,140
validation set together

00:09:41,780 --> 00:09:48,970
so basically going to be making them and

00:09:45,140 --> 00:09:52,280
on a certain amount of iterations so

00:09:48,970 --> 00:09:54,590
it's usually five so you outed for the

00:09:52,280 --> 00:09:58,000
default option of five folds so

00:09:54,590 --> 00:10:00,310
basically five loops where each time the

00:09:58,000 --> 00:10:03,740
validation items are going to be

00:10:00,310 --> 00:10:05,870
different from a look to another so I'm

00:10:03,740 --> 00:10:09,290
going to be training it and then what's

00:10:05,870 --> 00:10:12,770
it's going to be doing is it's going to

00:10:09,290 --> 00:10:17,000
be showing some statistics that coming

00:10:12,770 --> 00:10:19,460
from the of the modules of lambda the

00:10:17,000 --> 00:10:24,560
top one being macro average what it is

00:10:19,460 --> 00:10:27,940
is imagining you have several classes

00:10:24,560 --> 00:10:30,589
and when you use like modern two classes

00:10:27,940 --> 00:10:33,860
micro averages will be basically the

00:10:30,589 --> 00:10:37,250
average of all the statistics such as

00:10:33,860 --> 00:10:41,630
accuracy so how well did the model

00:10:37,250 --> 00:10:44,510
performed of all position how many of

00:10:41,630 --> 00:10:47,900
the data predicted as a certain label

00:10:44,510 --> 00:10:50,930
say how many of the predictions weight

00:10:47,900 --> 00:10:53,750
said oh this is a bug label how many of

00:10:50,930 --> 00:10:57,890
those were correct recall how many of

00:10:53,750 --> 00:10:59,720
the layer in the case of bob labels how

00:10:57,890 --> 00:11:03,230
many of the public was with quickly

00:10:59,720 --> 00:11:06,620
recalls as bob label and then you have

00:11:03,230 --> 00:11:09,260
f1 score which is before the harmonics

00:11:06,620 --> 00:11:12,290
some of both so in cases where you have

00:11:09,260 --> 00:11:14,709
classifiers which says the exact same

00:11:12,290 --> 00:11:16,850
answer every time say you give a hundred

00:11:14,709 --> 00:11:19,040
different answers and you'd say other

00:11:16,850 --> 00:11:20,959
bug every time you might have like a

00:11:19,040 --> 00:11:24,110
high recall by the expression might be

00:11:20,959 --> 00:11:26,930
like below zero so really crap so if one

00:11:24,110 --> 00:11:28,670
score is usually what you see if it's so

00:11:26,930 --> 00:11:30,829
close to here then it's really bad if

00:11:28,670 --> 00:11:34,190
it's close to one which in this case is

00:11:30,829 --> 00:11:36,920
about 90% so it's pretty good then it's

00:11:34,190 --> 00:11:39,740
reliable the next one is micro average

00:11:36,920 --> 00:11:42,860
so that's when you look at every single

00:11:39,740 --> 00:11:46,040
predictions alone so you have like qu+

00:11:42,860 --> 00:11:49,040
which is where a label was correctly

00:11:46,040 --> 00:11:53,750
predicted as what the actual label is to

00:11:49,040 --> 00:11:55,730
negative weight said like when it's

00:11:53,750 --> 00:11:58,220
looking at bugs and say okay this level

00:11:55,730 --> 00:12:01,779
is not a bug and it's enough not to be a

00:11:58,220 --> 00:12:04,640
bug label false positives it's

00:12:01,779 --> 00:12:07,579
essentially where it says okay this is a

00:12:04,640 --> 00:12:10,300
bug label but it's not so it's so it's

00:12:07,579 --> 00:12:12,709
gotten a false positive in multi class

00:12:10,300 --> 00:12:15,339
which is a bit different than in binary

00:12:12,709 --> 00:12:18,529
classification which is a lot easier

00:12:15,339 --> 00:12:21,019
false negatives is when he failed to

00:12:18,529 --> 00:12:23,029
recognize something as a bug or code or

00:12:21,019 --> 00:12:25,399
whichever depending on which label

00:12:23,029 --> 00:12:27,800
you're looking at at the moment and then

00:12:25,399 --> 00:12:29,420
you have more statistics like having

00:12:27,800 --> 00:12:32,329
gained lost but I'm not going to get

00:12:29,420 --> 00:12:38,320
into those ones next thing so now you

00:12:32,329 --> 00:12:41,269
change your learn in recognizing

00:12:38,320 --> 00:12:43,190
validations and test and training

00:12:41,269 --> 00:12:46,550
elements but what about your test

00:12:43,190 --> 00:12:49,130
elements or any unseen data well I'm

00:12:46,550 --> 00:12:51,320
going to show you an example so there's

00:12:49,130 --> 00:12:54,260
an eval function which is basically

00:12:51,320 --> 00:12:57,410
going to take all the elements in each

00:12:54,260 --> 00:12:59,560
way in little set so about 15% of the

00:12:57,410 --> 00:13:02,209
data and it's going to be basically

00:12:59,560 --> 00:13:05,209
evaluating it saying so what you see

00:13:02,209 --> 00:13:06,579
here is countries we have talked so

00:13:05,209 --> 00:13:09,560
that's the country and then you have

00:13:06,579 --> 00:13:11,329
statistics for it some of the sample

00:13:09,560 --> 00:13:14,990
portions or how much of it were on the

00:13:11,329 --> 00:13:16,710
dataset how much of true positives false

00:13:14,990 --> 00:13:18,720
negatives and so on accurate

00:13:16,710 --> 00:13:21,450
see and then you have the confusion

00:13:18,720 --> 00:13:23,760
matrix which is essentially a two-by-two

00:13:21,450 --> 00:13:26,630
array where you have three positive

00:13:23,760 --> 00:13:31,680
false positives false negatives and and

00:13:26,630 --> 00:13:34,500
false negatives and then you have more

00:13:31,680 --> 00:13:37,740
general starts like how many items there

00:13:34,500 --> 00:13:39,420
was in test so in this case 273 how many

00:13:37,740 --> 00:13:43,170
of the operations required so in this

00:13:39,420 --> 00:13:44,550
case 44 and how many / incorrect 229 and

00:13:43,170 --> 00:13:50,480
you have the classes here if you want to

00:13:44,550 --> 00:13:55,500
look at you can also get some the

00:13:50,480 --> 00:14:00,840
confusion matrix such a sexy alone so

00:13:55,500 --> 00:14:02,370
I'm going to say mm-hmm and the

00:14:00,840 --> 00:14:04,590
coefficient matrix a legit you see it

00:14:02,370 --> 00:14:09,000
visually as you see in coefficient

00:14:04,590 --> 00:14:13,520
matrices so has this Queen might not be

00:14:09,000 --> 00:14:16,950
big enough but it ends about 28 by 28

00:14:13,520 --> 00:14:19,130
grids to display I'm going to split in

00:14:16,950 --> 00:14:19,130
two

00:14:31,530 --> 00:14:36,760
so here that's for those of you who

00:14:34,870 --> 00:14:41,110
never seen a country matrix essentially

00:14:36,760 --> 00:14:44,290
where you have on one on the first

00:14:41,110 --> 00:14:46,720
column you have all the labels which

00:14:44,290 --> 00:14:49,000
represent the actual labels that are

00:14:46,720 --> 00:14:51,730
basically in datasets with the answers

00:14:49,000 --> 00:14:55,510
and on the top one is that what the

00:14:51,730 --> 00:14:57,370
model predicted the diagonal is usually

00:14:55,510 --> 00:14:59,650
what the true positives or the correct

00:14:57,370 --> 00:15:02,020
predictions are so green ones are the

00:14:59,650 --> 00:15:05,740
correct ones yellow are basically what

00:15:02,020 --> 00:15:08,080
you should have said oh that's a blog

00:15:05,740 --> 00:15:11,050
okay and and it's actually a blog by

00:15:08,080 --> 00:15:13,480
18th on any probably because there's no

00:15:11,050 --> 00:15:17,560
much data in the training sets you'll be

00:15:13,480 --> 00:15:19,180
able to guess then you have red ones

00:15:17,560 --> 00:15:21,790
here we shall know in the diagonals so

00:15:19,180 --> 00:15:23,290
that's where you have either false

00:15:21,790 --> 00:15:24,970
positive or false negative

00:15:23,290 --> 00:15:28,420
depending on whether you're looking at

00:15:24,970 --> 00:15:30,640
from the ideas or if you're looking from

00:15:28,420 --> 00:15:33,100
the maintenance perspective which

00:15:30,640 --> 00:15:37,980
changes depend on whether it's a false

00:15:33,100 --> 00:15:42,070
positive or false negative which is not

00:15:37,980 --> 00:15:47,310
that hard when you look at to label also

00:15:42,070 --> 00:15:51,400
basically pioneer classifications and

00:15:47,310 --> 00:15:53,050
with this you can also get like the

00:15:51,400 --> 00:15:56,170
across a little fix that I calculated

00:15:53,050 --> 00:16:01,360
based on all the numbers here so if you

00:15:56,170 --> 00:16:08,980
want to get the micro accuracy or if you

00:16:01,360 --> 00:16:12,310
just want you sure that's like some of

00:16:08,980 --> 00:16:14,430
the relevant and important that this

00:16:12,310 --> 00:16:17,680
actually will usually look at when you

00:16:14,430 --> 00:16:22,510
evaluate a machine learning model so you

00:16:17,680 --> 00:16:25,270
have total to say how many elements to

00:16:22,510 --> 00:16:28,960
Falls as before then you have the

00:16:25,270 --> 00:16:31,000
accuracy position we call an F 1 so

00:16:28,960 --> 00:16:33,550
that's macro you can also change if you

00:16:31,000 --> 00:16:37,390
have maker which is usually default as

00:16:33,550 --> 00:16:39,460
in this case of this library the data

00:16:37,390 --> 00:16:42,500
that he is quite imbalanced and I'll

00:16:39,460 --> 00:16:44,540
explain and I'll show you why in a bit

00:16:42,500 --> 00:16:48,020
so you will usually use the micro which

00:16:44,540 --> 00:16:49,910
usually accounts for imbalances the

00:16:48,020 --> 00:16:53,420
third option is where you have weighted

00:16:49,910 --> 00:16:56,480
so what weighted is it's basically where

00:16:53,420 --> 00:16:59,150
you're going to be doing the same I

00:16:56,480 --> 00:17:01,940
protest macro except going to be

00:16:59,150 --> 00:17:05,030
encountered taking check out how many

00:17:01,940 --> 00:17:07,610
items of each classes are presented data

00:17:05,030 --> 00:17:11,750
sets which means that it will be like

00:17:07,610 --> 00:17:14,300
not favoring like the biggest class over

00:17:11,750 --> 00:17:17,000
the one who wishes belly in there and

00:17:14,300 --> 00:17:21,410
that's it's quite similar stack so

00:17:17,000 --> 00:17:24,410
around sixty percent each so it's going

00:17:21,410 --> 00:17:28,220
good now what if we wanted to see the

00:17:24,410 --> 00:17:37,610
actual charts so the processor that's

00:17:28,220 --> 00:17:41,420
one code so here you can see so in blue

00:17:37,610 --> 00:17:45,020
you have the training set so validations

00:17:41,420 --> 00:17:48,260
green and Tessa so that big one is no so

00:17:45,020 --> 00:17:50,390
no is not a mistake it's actually

00:17:48,260 --> 00:17:54,530
intentional it was just a way to

00:17:50,390 --> 00:18:00,350
represent labels that can't fit in any

00:17:54,530 --> 00:18:02,570
of the contribution labels here as in a

00:18:00,350 --> 00:18:05,030
little issue you might have things like

00:18:02,570 --> 00:18:07,100
in progress which are completely

00:18:05,030 --> 00:18:09,020
reluctant or don't mean anything toward

00:18:07,100 --> 00:18:12,140
if it's a code or a bug fix or

00:18:09,020 --> 00:18:14,750
documentation translation design or

00:18:12,140 --> 00:18:16,670
anything here and here you can see what

00:18:14,750 --> 00:18:19,160
they were patterns so some of them are

00:18:16,670 --> 00:18:23,990
really low but that's because things

00:18:19,160 --> 00:18:26,180
like contents sonic finding event

00:18:23,990 --> 00:18:28,160
organization of things you might not be

00:18:26,180 --> 00:18:31,520
is here in github because that's usually

00:18:28,160 --> 00:18:34,280
like in Evan white meats so calm slack

00:18:31,520 --> 00:18:37,760
or any other platforms which might not

00:18:34,280 --> 00:18:40,700
be available when you look at the guitar

00:18:37,760 --> 00:18:46,660
people alone because that labels from

00:18:40,700 --> 00:18:51,299
get up yeah that's pretty much it

00:18:46,660 --> 00:18:51,299

YouTube URL: https://www.youtube.com/watch?v=s6pbe3tUcJ0


