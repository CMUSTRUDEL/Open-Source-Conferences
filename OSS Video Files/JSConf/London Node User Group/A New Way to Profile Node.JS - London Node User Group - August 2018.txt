Title: A New Way to Profile Node.JS - London Node User Group - August 2018
Publication date: 2018-08-30
Playlist: London Node User Group
Description: 
	It’s been weeks and the organisation you work for seems to be slowly turning against you. At least that’s what it feels like. User experience is poor because of slow API’s, sales are being missed, performance-linked SEO heuristics are causing a drop in page ranking. Mobile users have all but given up. Operations have reported that a critical Node.js service owned by your team is spinning at[masked]% CPU, and all parts of the application dependent on the service are experiencing intermittent slowdowns or in some cases, complete unavailability. What are you going to do now?

About the speaker:

David Mark Clements is a Node.js performance specialist and author of Node Cookbook. He is currently serving as Principal Architect with nearForm. David has been coding, speaking and writing about Node.js since Node 0.4 and has worked with frontend JavaScript for 20 years. Of note among David’s open source contributions is Pino, the fastest Node.js logger available (co-authored with Matteo Collina) and 0x, a JavaScript stack profiling tool. David’s book about Node.js, the aforementioned Node Cookbook, is now in its third edition.

@davidmarkclem (nearForm)

_

About Pusher Sessions:

We're bringing the meetup to you. With Sessions, you can watch recordings of top-notch talks from developer meetups -- wherever and whenever you want.

Meetups are a great way to learn from our peers and to keep up with the latest trends and technologies. As developers ourselves, we at Pusher wanted to bring this great content to more people... So we built Sessions. On Sessions, you can watch talks that interest you and subscribe to be notified when new content gets added.

If you run a meetup and want to get involved, kindly get in touch.

_

About Pusher:

Pusher is a hosted service with APIs, developer tools and open source libraries that greatly simplify integrating real-time functionality into web and mobile applications. 

Pusher will automatically scale when required, removing all the pain of setting up and maintaining a secure, real-time infrastructure. 

Pusher is already trusted to do so by thousands of developers and companies like GitHub, MailChimp, the Financial Times, Buffer and many more. 

Getting started takes just a few seconds: simply go to pusher.com and create a free account. Happy hacking!
Captions: 
	00:00:00,030 --> 00:00:13,099
so who's who's working with node in here

00:00:05,029 --> 00:00:16,800
good good right crowd then okay who's

00:00:13,099 --> 00:00:22,199
who's ever had a production issue with

00:00:16,800 --> 00:00:24,269
node all right okay cool

00:00:22,199 --> 00:00:28,920
so there's those that haven't as those

00:00:24,269 --> 00:00:32,489
that are going to so I'm my name's Dave

00:00:28,920 --> 00:00:33,840
Clements I'm with near forum

00:00:32,489 --> 00:00:40,980
found out today that we actually

00:00:33,840 --> 00:00:43,440
sponsored this so you're welcome so yeah

00:00:40,980 --> 00:00:47,370
we as pretty much everyone else in here

00:00:43,440 --> 00:00:50,700
likes to say we are hiring we're looking

00:00:47,370 --> 00:00:54,510
for no developers lot people in here

00:00:50,700 --> 00:00:56,640
doing node both for remote work and work

00:00:54,510 --> 00:00:58,550
here in London and if you fancy there's

00:00:56,640 --> 00:01:02,160
work in New York for you too

00:00:58,550 --> 00:01:08,180
so you know contour to me or Sean if you

00:01:02,160 --> 00:01:13,260
like tall dark and Irish okay so

00:01:08,180 --> 00:01:15,420
speaking of production issues some

00:01:13,260 --> 00:01:17,810
people put their hands up they felt this

00:01:15,420 --> 00:01:22,140
they felt this pressure they felt this

00:01:17,810 --> 00:01:24,869
situation they you get to a point

00:01:22,140 --> 00:01:27,240
sometimes where some some kind of

00:01:24,869 --> 00:01:30,710
reduction instant happens or something's

00:01:27,240 --> 00:01:35,509
deployed and things start to lock down

00:01:30,710 --> 00:01:39,659
and when when the sales traffic drops

00:01:35,509 --> 00:01:43,200
people get angry so what are you doing

00:01:39,659 --> 00:01:44,790
this situation anyone I know you're not

00:01:43,200 --> 00:01:48,540
supposed to ask questions but you can

00:01:44,790 --> 00:01:50,340
answer my questions what would you do in

00:01:48,540 --> 00:01:52,350
a case where things were just going so

00:01:50,340 --> 00:01:53,970
slow that production

00:01:52,350 --> 00:01:59,189
I'd like that your sales traffic's

00:01:53,970 --> 00:02:02,600
dropping quick fix your on-the-spot guy

00:01:59,189 --> 00:02:02,600
is you're all losing money right now

00:02:02,899 --> 00:02:08,970
turn it on and on perfect yeah perfect

00:02:07,409 --> 00:02:11,580
London response I did this in New York

00:02:08,970 --> 00:02:13,260
and I got classic New York sarcastic

00:02:11,580 --> 00:02:17,269
responses and then this is like a

00:02:13,260 --> 00:02:20,549
different version of that for for London

00:02:17,269 --> 00:02:22,680
you you add more service right you add

00:02:20,549 --> 00:02:24,629
more instances you have to scale up very

00:02:22,680 --> 00:02:26,370
quickly and you need to do it now if you

00:02:24,629 --> 00:02:30,360
if you're using a cloud deploy you're

00:02:26,370 --> 00:02:33,209
gonna use that elastic scaling quality

00:02:30,360 --> 00:02:35,629
to to solve the problem quickly but now

00:02:33,209 --> 00:02:38,519
the business is spending more money and

00:02:35,629 --> 00:02:44,840
we need to ask the question why is it

00:02:38,519 --> 00:02:48,870
slow so the answer to that question is

00:02:44,840 --> 00:02:52,530
another question which is where is the

00:02:48,870 --> 00:02:54,269
bottleneck and when we asked the

00:02:52,530 --> 00:02:57,060
question where is the bottleneck we want

00:02:54,269 --> 00:02:59,609
to figure out is the bottleneck internal

00:02:57,060 --> 00:03:01,560
to a process or is it external is it

00:02:59,609 --> 00:03:04,200
somewhere else in the infrastructure so

00:03:01,560 --> 00:03:05,760
I think the reason there was a lot of

00:03:04,200 --> 00:03:07,680
silence around what do you do in this

00:03:05,760 --> 00:03:09,810
this production situation is because a

00:03:07,680 --> 00:03:12,739
lot of people when they're working on

00:03:09,810 --> 00:03:15,959
node are working in a layer within a

00:03:12,739 --> 00:03:17,579
stack and there's other people

00:03:15,959 --> 00:03:20,599
responsible for keeping things running

00:03:17,579 --> 00:03:24,840
and spinning up more servers right but

00:03:20,599 --> 00:03:27,150
what can happen is sometimes the your

00:03:24,840 --> 00:03:30,209
team or your project that sits in the

00:03:27,150 --> 00:03:32,310
middle here after when someone's asking

00:03:30,209 --> 00:03:36,090
why is it slow someone will be asking is

00:03:32,310 --> 00:03:38,549
node slow is your project slow and then

00:03:36,090 --> 00:03:41,220
you have to try to figure out and proof

00:03:38,549 --> 00:03:44,910
is the bottleneck in this entire system

00:03:41,220 --> 00:03:47,880
in my application or is it external to

00:03:44,910 --> 00:03:50,040
my application and bonus points if it is

00:03:47,880 --> 00:03:55,230
external can I figure out where in the

00:03:50,040 --> 00:03:57,359
stack it is so how did you find

00:03:55,230 --> 00:03:59,730
bottleneck because if you can if you can

00:03:57,359 --> 00:04:01,349
find a bottleneck you can prove whether

00:03:59,730 --> 00:04:06,769
the bottlenecks in your application or

00:04:01,349 --> 00:04:08,760
somewhere up some upstream API etc so

00:04:06,769 --> 00:04:10,979
first thing is you have a performance

00:04:08,760 --> 00:04:13,440
issue and something happens in

00:04:10,979 --> 00:04:14,099
production some died maybe the server's

00:04:13,440 --> 00:04:16,019
don't

00:04:14,099 --> 00:04:18,539
crash but maybe there's some Diagnostics

00:04:16,019 --> 00:04:21,120
come back from I don't know maybe you've

00:04:18,539 --> 00:04:23,250
got a new relic in there or from health

00:04:21,120 --> 00:04:24,720
checks whatever it is you get some

00:04:23,250 --> 00:04:27,210
Diagnostics but they're coarse-grained

00:04:24,720 --> 00:04:30,150
they're polluted very hard to figure out

00:04:27,210 --> 00:04:31,800
what's going out on in that situation so

00:04:30,150 --> 00:04:34,979
the next thing that you typically would

00:04:31,800 --> 00:04:36,780
do is you try to reproduce this on a

00:04:34,979 --> 00:04:38,610
production like environment a kind of

00:04:36,780 --> 00:04:40,370
staging environment that you can play

00:04:38,610 --> 00:04:45,030
with that you can run traffic through

00:04:40,370 --> 00:04:47,550
that you can simulate load on and to

00:04:45,030 --> 00:04:51,780
simulate loads first tool I'm going to

00:04:47,550 --> 00:04:53,220
be introducing is Auto cannon so also

00:04:51,780 --> 00:04:56,910
cannons anyone heard of Apache bench

00:04:53,220 --> 00:04:59,570
before yeah yeah cuckoo there's another

00:04:56,910 --> 00:05:04,949
one called w RK w RK - you heard of that

00:04:59,570 --> 00:05:09,000
no okay so these are these are HTTP load

00:05:04,949 --> 00:05:10,259
testers both written in C both quite

00:05:09,000 --> 00:05:13,620
difficult to set up especially in

00:05:10,259 --> 00:05:16,289
Windows autocannons written in node it

00:05:13,620 --> 00:05:19,949
can actually saturate your server 10%

00:05:16,289 --> 00:05:23,250
more than Apache venture w RK and it's

00:05:19,949 --> 00:05:25,949
it's as I say just written in node so

00:05:23,250 --> 00:05:27,870
you can just an NPM install Auto key

00:05:25,949 --> 00:05:29,159
autocannon it will work on windows and

00:05:27,870 --> 00:05:31,860
everything so it's actually brought into

00:05:29,159 --> 00:05:34,259
the node core project as they as a load

00:05:31,860 --> 00:05:37,409
testing tool because it's so easy to get

00:05:34,259 --> 00:05:38,970
it working on Windows as well so you

00:05:37,409 --> 00:05:40,289
would have maybe a staging environment

00:05:38,970 --> 00:05:43,770
and then you could simulate load with

00:05:40,289 --> 00:05:44,070
Apache bench or autocannon or something

00:05:43,770 --> 00:05:46,949
like that

00:05:44,070 --> 00:05:48,090
but your Diagnostics are still fairly

00:05:46,949 --> 00:05:50,130
coarse grained you're really just

00:05:48,090 --> 00:05:53,639
getting you know requests per second

00:05:50,130 --> 00:05:55,620
averages and and so forth back so the

00:05:53,639 --> 00:05:58,080
next thing you do once you've tried to

00:05:55,620 --> 00:06:01,259
try to diagnose the situation in

00:05:58,080 --> 00:06:04,849
production and maybe honed in on an area

00:06:01,259 --> 00:06:06,570
in the system is you reproduce on

00:06:04,849 --> 00:06:08,009
development box it's not a live

00:06:06,570 --> 00:06:09,990
environment you're trying to isolate the

00:06:08,009 --> 00:06:11,190
scenario in your case if you're doing

00:06:09,990 --> 00:06:13,650
this on your development boss is going

00:06:11,190 --> 00:06:16,849
to be a node process if you're working

00:06:13,650 --> 00:06:18,840
with node and then you want to have

00:06:16,849 --> 00:06:23,340
fine-grained Diagnostics where you're

00:06:18,840 --> 00:06:27,029
going to use special flags like tray

00:06:23,340 --> 00:06:27,960
soft trace deopt tracing lining stuff

00:06:27,029 --> 00:06:30,780
like that to

00:06:27,960 --> 00:06:32,940
to try to figure out possibly where in

00:06:30,780 --> 00:06:35,970
the process you you might have a

00:06:32,940 --> 00:06:38,550
bottleneck and then and then you know

00:06:35,970 --> 00:06:40,110
this is this is this is actually the way

00:06:38,550 --> 00:06:42,990
that myself and my colleague matera

00:06:40,110 --> 00:06:45,900
Kalina work is we just stare at the

00:06:42,990 --> 00:06:48,840
screen for a really long time trying to

00:06:45,900 --> 00:06:53,400
figure out what's going on you've

00:06:48,840 --> 00:06:55,349
probably been there right and Matteo who

00:06:53,400 --> 00:06:58,139
who made the original version of this he

00:06:55,349 --> 00:07:01,259
has a dog Mateos got he's an Italian

00:06:58,139 --> 00:07:03,990
he's got curly hair brown eyes and he's

00:07:01,259 --> 00:07:05,130
got a dog called poo Fatih and there's

00:07:03,990 --> 00:07:08,430
some Italians in the room they probably

00:07:05,130 --> 00:07:10,199
know that Smurfette is and it's got

00:07:08,430 --> 00:07:12,720
brown eyes and curly hair and basically

00:07:10,199 --> 00:07:15,680
looks like his child but he'll take her

00:07:12,720 --> 00:07:18,900
for a walk and then he'll have

00:07:15,680 --> 00:07:20,130
epiphanies me I like to take long

00:07:18,900 --> 00:07:23,820
showers there you go

00:07:20,130 --> 00:07:25,560
mental image and and then some magic

00:07:23,820 --> 00:07:27,360
happens in our animal brains at that

00:07:25,560 --> 00:07:29,909
point and we get to perform an

00:07:27,360 --> 00:07:32,220
application however what we want to do

00:07:29,909 --> 00:07:34,710
is near form what we've been working on

00:07:32,220 --> 00:07:37,979
is optimizing that magical question mark

00:07:34,710 --> 00:07:40,199
step and trying to make that take the

00:07:37,979 --> 00:07:42,539
the pattern matching that we're really

00:07:40,199 --> 00:07:43,889
doing in our in our minds when we're

00:07:42,539 --> 00:07:45,449
trying to figure out where the

00:07:43,889 --> 00:07:47,520
bottleneck is and where the problem is

00:07:45,449 --> 00:07:50,820
and turn that into tooling and

00:07:47,520 --> 00:07:53,550
visualizations the the surface those

00:07:50,820 --> 00:07:55,190
patterns faster and and make it much

00:07:53,550 --> 00:07:59,009
easier to prove where the bottleneck is

00:07:55,190 --> 00:08:01,080
so no clinics installable with npm you

00:07:59,009 --> 00:08:02,099
can just do dash G clinic whole thing

00:08:01,080 --> 00:08:05,400
installs and this will give you three

00:08:02,099 --> 00:08:09,440
tools the three tools are clinic doctor

00:08:05,400 --> 00:08:12,690
clinic bubble prof and clinic flame

00:08:09,440 --> 00:08:15,750
clinic doctor what it does is your sort

00:08:12,690 --> 00:08:18,060
of issue GP for a no process it will

00:08:15,750 --> 00:08:20,759
collect metrics by injecting probes and

00:08:18,060 --> 00:08:23,190
then it assesses health with a set of

00:08:20,759 --> 00:08:27,870
heuristics that we developed with a

00:08:23,190 --> 00:08:31,259
statistician yes I've had problems with

00:08:27,870 --> 00:08:35,940
that word in the past who's basically

00:08:31,259 --> 00:08:37,529
using statistics algorithms to map the

00:08:35,940 --> 00:08:39,180
the pattern matching I was talking about

00:08:37,529 --> 00:08:41,460
that we do with our brains into an

00:08:39,180 --> 00:08:43,260
algorithm that can automatically did

00:08:41,460 --> 00:08:46,170
TEKT issues with a no process and

00:08:43,260 --> 00:08:47,490
categorize them for you and then from

00:08:46,170 --> 00:08:48,960
that clinic doctor creates

00:08:47,490 --> 00:08:51,090
recommendations I'm going to show you

00:08:48,960 --> 00:08:55,020
I'm going to live Deut live hack or this

00:08:51,090 --> 00:08:57,870
in a second clinic flame was is based on

00:08:55,020 --> 00:09:00,630
a tool that I wrote called zero X and

00:08:57,870 --> 00:09:02,640
this collects metrics by CPU sampling

00:09:00,630 --> 00:09:05,850
and the thing that it tracks is

00:09:02,640 --> 00:09:09,330
something we refer to as top of stack

00:09:05,850 --> 00:09:12,030
frequency so essentially you're taking a

00:09:09,330 --> 00:09:15,210
millisecond slice of whatever the the

00:09:12,030 --> 00:09:17,610
cool stack is on the CPU at a given time

00:09:15,210 --> 00:09:21,500
and that cool stack cannae can be

00:09:17,610 --> 00:09:23,640
translated to include not just the C

00:09:21,500 --> 00:09:24,990
functions that are on that stack but

00:09:23,640 --> 00:09:26,790
also the JavaScript functions that are

00:09:24,990 --> 00:09:29,790
on that stack now when you get into

00:09:26,790 --> 00:09:32,460
JavaScript land if over successive

00:09:29,790 --> 00:09:34,410
samples you have a higher ratio of a

00:09:32,460 --> 00:09:36,180
function being the function that's

00:09:34,410 --> 00:09:38,850
currently being executed which means at

00:09:36,180 --> 00:09:42,690
the top of the stack in high ratio to

00:09:38,850 --> 00:09:44,670
other functions then it's said to have a

00:09:42,690 --> 00:09:47,130
high top of start frequency and it's

00:09:44,670 --> 00:09:49,950
probably it's going to be a hot function

00:09:47,130 --> 00:09:51,540
a function that's taking more time to

00:09:49,950 --> 00:09:54,060
execute because it's not calling other

00:09:51,540 --> 00:09:57,630
functions after it and flame grass

00:09:54,060 --> 00:09:59,160
visualize this by visualizing all of the

00:09:57,630 --> 00:10:02,340
stacks in aggregate which you're going

00:09:59,160 --> 00:10:06,000
to see in a second and coloring the top

00:10:02,340 --> 00:10:10,310
of stack blocks of frames with a high

00:10:06,000 --> 00:10:13,080
top of stack frequency a deeper hue of

00:10:10,310 --> 00:10:15,840
from from a spectrum of orange to red so

00:10:13,080 --> 00:10:17,850
when you get red blocks that's that's

00:10:15,840 --> 00:10:20,040
the pattern that you're looking for as a

00:10:17,850 --> 00:10:21,630
human so you look at the graph and you

00:10:20,040 --> 00:10:23,520
see red blocks and you go ok that's well

00:10:21,630 --> 00:10:25,680
my bottlenecks are which we'll see which

00:10:23,520 --> 00:10:28,200
we'll see shortly as well and finally

00:10:25,680 --> 00:10:31,140
clinic bubble profits at all that

00:10:28,200 --> 00:10:33,350
nephilim developed recently that

00:10:31,140 --> 00:10:36,150
introduces a new visualization for

00:10:33,350 --> 00:10:38,660
visualizing asynchronous activity so

00:10:36,150 --> 00:10:41,070
where as clinic flame is for identifying

00:10:38,660 --> 00:10:44,250
synchronous blocking functions within a

00:10:41,070 --> 00:10:47,660
process a typical problem when you're

00:10:44,250 --> 00:10:51,930
working with node clinic bubble proof

00:10:47,660 --> 00:10:53,889
works of visualizing IO waiting for IO

00:10:51,930 --> 00:10:56,980
and what's happening

00:10:53,889 --> 00:11:01,569
in that in that period of time so this

00:10:56,980 --> 00:11:03,040
this is useful for bottlenecks outside

00:11:01,569 --> 00:11:05,319
of your application rather than

00:11:03,040 --> 00:11:07,839
bottlenecks within your application so

00:11:05,319 --> 00:11:11,499
Baba profit collects metrics using async

00:11:07,839 --> 00:11:13,629
hooks which is a fairly new API it's

00:11:11,499 --> 00:11:16,059
just about settle down and no ten it's

00:11:13,629 --> 00:11:17,499
not quite right I don't think but it's

00:11:16,059 --> 00:11:20,410
good enough that we can use it for

00:11:17,499 --> 00:11:24,100
development tools the async hooks module

00:11:20,410 --> 00:11:26,470
will basically give you stats on every

00:11:24,100 --> 00:11:28,600
async action that happens in node core

00:11:26,470 --> 00:11:31,059
as you're running a know process and

00:11:28,600 --> 00:11:34,600
also you can instrument other libraries

00:11:31,059 --> 00:11:36,879
so for instance MongoDB it can I think

00:11:34,600 --> 00:11:40,029
is instrumented with async hooks as well

00:11:36,879 --> 00:11:41,769
so you know what asynchronous activity

00:11:40,029 --> 00:11:44,739
is occurring and it can get better over

00:11:41,769 --> 00:11:47,499
time so this creates bubble grass okay

00:11:44,739 --> 00:11:49,179
when you go to nf2 ie4 slash interact if

00:11:47,499 --> 00:11:51,989
you want to check out the actual

00:11:49,179 --> 00:11:53,379
fertilisers I'm going to be creating

00:11:51,989 --> 00:11:55,749
okay

00:11:53,379 --> 00:11:58,089
damn I ruined it you shouldn't have seen

00:11:55,749 --> 00:12:03,879
that yet so I'm gonna start our process

00:11:58,089 --> 00:12:05,439
node one and I'm gonna first of all the

00:12:03,879 --> 00:12:07,419
first thing I do is establish a baseline

00:12:05,439 --> 00:12:10,809
by putting it under load with the tool I

00:12:07,419 --> 00:12:12,369
mentioned earlier so auto cannon I'm

00:12:10,809 --> 00:12:15,100
gonna give it two connections because

00:12:12,369 --> 00:12:17,769
it's known to be problematic for five

00:12:15,100 --> 00:12:20,889
seconds and I'm just gonna hit it with

00:12:17,769 --> 00:12:28,149
with HTTP Chris or five seconds two

00:12:20,889 --> 00:12:31,959
connections okay so right here we see we

00:12:28,149 --> 00:12:34,929
have an average request per second of 12

00:12:31,959 --> 00:12:41,980
12 requests per second is is that fast

00:12:34,929 --> 00:12:43,329
enough yes that's correct

00:12:41,980 --> 00:12:45,579
it does depend on what you're doing

00:12:43,329 --> 00:12:47,589
however 12 requests per second is you

00:12:45,579 --> 00:12:49,059
know we're getting kind of down the low

00:12:47,589 --> 00:12:52,149
end of like what could we possibly be

00:12:49,059 --> 00:12:53,699
doing that we would be it depends on how

00:12:52,149 --> 00:12:57,069
much money you're willing to spend on

00:12:53,699 --> 00:12:58,359
extra servers and yes of course it

00:12:57,069 --> 00:12:59,499
depends on what you're doing but when

00:12:58,359 --> 00:13:00,339
you're getting down to that low of a

00:12:59,499 --> 00:13:01,959
number

00:13:00,339 --> 00:13:03,399
maybe you should be using something

00:13:01,959 --> 00:13:05,230
other than node if it's going to be very

00:13:03,399 --> 00:13:07,210
compute heavy processing because gonna

00:13:05,230 --> 00:13:10,720
it's definitely gonna do better on

00:13:07,210 --> 00:13:12,640
synchronous tasks but is the bottleneck

00:13:10,720 --> 00:13:14,860
synchronous is it external or is it

00:13:12,640 --> 00:13:19,750
internal well we can find out very

00:13:14,860 --> 00:13:23,380
quickly by using no doctor so if I stop

00:13:19,750 --> 00:13:24,970
this guy up here stop this guy up here

00:13:23,380 --> 00:13:27,540
why didn't he stop the first time I

00:13:24,970 --> 00:13:32,140
don't like it when it disobeys me okay

00:13:27,540 --> 00:13:33,340
it's a weird thing to say so what i'm

00:13:32,140 --> 00:13:35,830
doing here is i'm gonna use clinic

00:13:33,340 --> 00:13:39,820
doctor and when you use apply a flagged

00:13:35,830 --> 00:13:41,670
cord on port what this allows is when it

00:13:39,820 --> 00:13:46,870
when the process that i'm about to run

00:13:41,670 --> 00:13:49,420
opens a port it will trigger the command

00:13:46,870 --> 00:13:52,720
within the that's assigned to the on

00:13:49,420 --> 00:13:54,370
port flag so what i'm doing here is

00:13:52,720 --> 00:13:58,630
exactly three commands in one so clinic

00:13:54,370 --> 00:14:00,670
doctor starts up it instruments the the

00:13:58,630 --> 00:14:03,010
process that i'm telling it to run which

00:14:00,670 --> 00:14:06,010
is node one and then when that process

00:14:03,010 --> 00:14:09,370
opens its port which is 3000 it will

00:14:06,010 --> 00:14:12,580
execute this command so in one command i

00:14:09,370 --> 00:14:14,050
can i can essentially find out what's

00:14:12,580 --> 00:14:19,060
going on in my process and in if there

00:14:14,050 --> 00:14:21,250
if it has any issues and this opens the

00:14:19,060 --> 00:14:23,890
doctor display so we see we've got CP

00:14:21,250 --> 00:14:25,990
usage memory usage if only delay active

00:14:23,890 --> 00:14:28,450
handles might have handles are like

00:14:25,990 --> 00:14:31,570
sockets that are open and so forth and

00:14:28,450 --> 00:14:35,440
it says that it's got this the CPU usage

00:14:31,570 --> 00:14:38,130
graph highlighted in red and it says the

00:14:35,440 --> 00:14:41,230
top it's detected a potential io issue

00:14:38,130 --> 00:14:44,020
the reason for that is that the cpu is

00:14:41,230 --> 00:14:45,340
idle too much of the time now this this

00:14:44,020 --> 00:14:47,470
graph is a little bit misleading because

00:14:45,340 --> 00:14:48,610
we're seeing a lot of spikes those

00:14:47,470 --> 00:14:51,880
spikes are actually coming from the

00:14:48,610 --> 00:14:53,080
sampling we've just released this i'm

00:14:51,880 --> 00:14:55,060
not showing it right now because I

00:14:53,080 --> 00:14:57,010
didn't want to update but we've just

00:14:55,060 --> 00:14:59,710
released a new version of doctor that

00:14:57,010 --> 00:15:01,570
uses tends to fly tends to flow with a

00:14:59,710 --> 00:15:04,780
new algorithm written biostatistician

00:15:01,570 --> 00:15:07,090
that flattens this graph by a planning

00:15:04,780 --> 00:15:10,840
algorithm that sort of removes the extra

00:15:07,090 --> 00:15:12,880
noise so the next when you install it

00:15:10,840 --> 00:15:14,830
you probably will see a more flat CPU

00:15:12,880 --> 00:15:20,290
usage graph but nevertheless it has

00:15:14,830 --> 00:15:21,010
detected the i/o issue and down here we

00:15:20,290 --> 00:15:23,500
have recommended

00:15:21,010 --> 00:15:25,330
and it will actually go into great

00:15:23,500 --> 00:15:27,790
detail on on how you can handle it and

00:15:25,330 --> 00:15:30,880
mitigate it in different things but the

00:15:27,790 --> 00:15:33,190
main point here is that to diagnose it

00:15:30,880 --> 00:15:35,380
instructs to say use clinic bubble prof

00:15:33,190 --> 00:15:37,270
because it's an i/o issue it means that

00:15:35,380 --> 00:15:39,130
something's happening outside the

00:15:37,270 --> 00:15:41,080
process rather than inside the process

00:15:39,130 --> 00:15:42,550
so we want to see what's going on with

00:15:41,080 --> 00:15:46,660
the asynchronous activity using Bob

00:15:42,550 --> 00:15:48,730
probe so what I can do is I can take

00:15:46,660 --> 00:15:51,670
that same command that around the clinic

00:15:48,730 --> 00:15:59,170
doctor and just replace doctor with

00:15:51,670 --> 00:16:01,450
bubble do the same thing put an under

00:15:59,170 --> 00:16:08,170
load for five seconds two connections

00:16:01,450 --> 00:16:12,130
and it creates this bad boy so this is a

00:16:08,170 --> 00:16:16,240
little bit too to consume visually but

00:16:12,130 --> 00:16:17,710
essentially the bubbles are represent

00:16:16,240 --> 00:16:19,500
the aggregated synchronous and

00:16:17,710 --> 00:16:25,600
asynchronous time that a particular

00:16:19,500 --> 00:16:28,630
group was was seen operating and by and

00:16:25,600 --> 00:16:30,610
the groups are based on the way the

00:16:28,630 --> 00:16:33,250
async hooks are used by a library or by

00:16:30,610 --> 00:16:35,080
it by node core so faster fire is a

00:16:33,250 --> 00:16:36,610
really fast web framework that's why

00:16:35,080 --> 00:16:38,950
that's right at the top there so if Asif

00:16:36,610 --> 00:16:43,270
is being used to serve it's similar to

00:16:38,950 --> 00:16:45,700
Express so happy and then we see we've

00:16:43,270 --> 00:16:46,750
got a j/s cursor over here and

00:16:45,700 --> 00:16:49,000
we've got something else to do with

00:16:46,750 --> 00:16:52,480
 Jace over here if I hover over

00:16:49,000 --> 00:16:55,690
this line it says that 94% of the

00:16:52,480 --> 00:16:57,610
profile run time 4.9 seconds async

00:16:55,690 --> 00:17:00,370
operations were pending from this group

00:16:57,610 --> 00:17:02,550
it says a similar thing over here so

00:17:00,370 --> 00:17:05,079
what I can do is I can click that line

00:17:02,550 --> 00:17:07,300
and it breaks it into further two lines

00:17:05,079 --> 00:17:09,220
I click the next one and it gives me a

00:17:07,300 --> 00:17:13,720
stack trace an asynchronous stack trace

00:17:09,220 --> 00:17:16,720
with the origin point of this particular

00:17:13,720 --> 00:17:20,829
asynchronous activity given to us so

00:17:16,720 --> 00:17:27,670
this is one j/s line 11 column 7 so if I

00:17:20,829 --> 00:17:30,700
go to one j/s line 11 column 7 we see

00:17:27,670 --> 00:17:33,480
that the what it's calling is fine so

00:17:30,700 --> 00:17:36,100
this is a a MongoDB

00:17:33,480 --> 00:17:37,779
collection and it's calling fine so

00:17:36,100 --> 00:17:40,210
that's going to start triggering some

00:17:37,779 --> 00:17:41,830
asynchronous activity and what what its

00:17:40,210 --> 00:17:45,520
basic was it's basically telling us is

00:17:41,830 --> 00:17:48,059
that it's spending a lot of time for 4.9

00:17:45,520 --> 00:17:51,909
seconds out of 5 seconds in aggregate

00:17:48,059 --> 00:17:53,500
waiting for that MongoDB response so

00:17:51,909 --> 00:17:55,320
that tells us that the problem isn't in

00:17:53,500 --> 00:18:01,090
our process the problem is somewhere

00:17:55,320 --> 00:18:05,230
maybe with MongoDB so what why don't we

00:18:01,090 --> 00:18:08,320
take a look at as my Blue Peter earlier

00:18:05,230 --> 00:18:10,210
attempt why don't we take a look at the

00:18:08,320 --> 00:18:13,630
ruble and open the repple in an and run

00:18:10,210 --> 00:18:16,890
that command so I'm gonna use the it's

00:18:13,630 --> 00:18:19,659
an M is basically this delivers the last

00:18:16,890 --> 00:18:27,070
piece of Jason in all the NPM modules so

00:18:19,659 --> 00:18:30,480
it's a big database so modules is the

00:18:27,070 --> 00:18:34,659
collection and then I can take the

00:18:30,480 --> 00:18:39,159
command that we're running here and run

00:18:34,659 --> 00:18:41,830
the same command in the repple and that

00:18:39,159 --> 00:18:44,230
gives us the response cool thing that

00:18:41,830 --> 00:18:47,260
 gives you is the ability to do and

00:18:44,230 --> 00:18:50,230
explain so it can tell you what that

00:18:47,260 --> 00:18:52,330
command is doing in if we look

00:18:50,230 --> 00:18:55,539
down here it says that the the plan is

00:18:52,330 --> 00:18:57,549
coal scan which means that it's got to

00:18:55,539 --> 00:18:58,510
go through all of the keys in the

00:18:57,549 --> 00:19:02,230
database anyone who's work with

00:18:58,510 --> 00:19:09,070
databases knows you need a index thank

00:19:02,230 --> 00:19:10,630
you so let's see if oh sure that's not

00:19:09,070 --> 00:19:17,710
how that you do it clear on this let's

00:19:10,630 --> 00:19:22,880
see if if there's any other collections

00:19:17,710 --> 00:19:26,220
available Oh check it out

00:19:22,880 --> 00:19:29,520
so we have we have an index version of

00:19:26,220 --> 00:19:33,870
that collection so the solution here may

00:19:29,520 --> 00:19:38,070
just be to index the database so in to

00:19:33,870 --> 00:19:39,600
j/s I'm going to go from DB collection

00:19:38,070 --> 00:19:46,620
modules to deeper collections modules

00:19:39,600 --> 00:19:53,120
index as the only change so back to the

00:19:46,620 --> 00:19:57,690
start run the second one autocannon

00:19:53,120 --> 00:20:02,190
localhost oh sorry two connections five

00:19:57,690 --> 00:20:07,529
seconds localhost three thousand let's

00:20:02,190 --> 00:20:09,539
see what effect this has had if any not

00:20:07,529 --> 00:20:12,360
a great one but it's improved it's

00:20:09,539 --> 00:20:13,440
improved I think that usually when I do

00:20:12,360 --> 00:20:14,850
this it goes from two requests per

00:20:13,440 --> 00:20:16,590
second to a hundred requests per second

00:20:14,850 --> 00:20:18,750
for some reason that first time round

00:20:16,590 --> 00:20:22,950
was twelve requests per second ruining

00:20:18,750 --> 00:20:28,940
my talk man not contrived enough okay

00:20:22,950 --> 00:20:35,970
let's try it with a higher amount of

00:20:28,940 --> 00:20:42,149
connections oh so a little bit better

00:20:35,970 --> 00:20:43,890
okay so okay let's see what I can do now

00:20:42,149 --> 00:20:45,720
is I'll run that same doctor commanders

00:20:43,890 --> 00:20:48,570
before higher amount of connections

00:20:45,720 --> 00:20:54,210
cause clearly you can handle it and see

00:20:48,570 --> 00:20:59,130
if no - and also sockets their ports

00:20:54,210 --> 00:21:00,240
open that's why it's done that so let's

00:20:59,130 --> 00:21:01,799
see if there's anything else wrong with

00:21:00,240 --> 00:21:07,679
the process now that we've fixed that

00:21:01,799 --> 00:21:10,970
problem so now that we've we've solved

00:21:07,679 --> 00:21:13,440
the i/o issue it's exposed another issue

00:21:10,970 --> 00:21:15,210
in this case it's the event loop delays

00:21:13,440 --> 00:21:16,890
everyone under like if everyone's

00:21:15,210 --> 00:21:20,309
working note and JavaScript I assume you

00:21:16,890 --> 00:21:21,840
understand the the event loop right and

00:21:20,309 --> 00:21:24,210
blocking the event loop is a really bad

00:21:21,840 --> 00:21:25,320
thing especially in node more so than in

00:21:24,210 --> 00:21:27,380
the browser unless you're doing like

00:21:25,320 --> 00:21:30,899
animation or gaming stuff in the browser

00:21:27,380 --> 00:21:32,610
Minh node you tend to have one path

00:21:30,899 --> 00:21:34,020
that's used a lot and you want it to

00:21:32,610 --> 00:21:35,460
handle a lot of connections so if you're

00:21:34,020 --> 00:21:39,570
blocking those connections pile

00:21:35,460 --> 00:21:41,789
up and you end up in trouble so this is

00:21:39,570 --> 00:21:44,700
this is now because the i/o issue has

00:21:41,789 --> 00:21:46,590
been resolved the the actual work that

00:21:44,700 --> 00:21:48,299
the process needs to do gets done but

00:21:46,590 --> 00:21:50,309
it's we're seeing that there's a

00:21:48,299 --> 00:21:54,950
synchronous bottleneck now so the

00:21:50,309 --> 00:21:54,950
recommendation is run clinic flame so

00:21:55,429 --> 00:22:02,000
same exact command instead of doctor

00:21:59,039 --> 00:22:02,000
flame

00:22:12,980 --> 00:22:20,820
so what this gives us is a flame graph

00:22:17,820 --> 00:22:24,690
and I can I'm gonna press merge to make

00:22:20,820 --> 00:22:27,120
it even clearer merge takes frames that

00:22:24,690 --> 00:22:28,650
are the equivalent from a JavaScript

00:22:27,120 --> 00:22:31,170
perspective but not from a C perspective

00:22:28,650 --> 00:22:33,930
i optimize frames and optimize frames

00:22:31,170 --> 00:22:36,240
lunges them together so we can see that

00:22:33,930 --> 00:22:39,150
we've got stacks call it functions

00:22:36,240 --> 00:22:41,100
calling functions the width is how long

00:22:39,150 --> 00:22:42,300
it was seen on CPU but the most

00:22:41,100 --> 00:22:44,370
important metric which I mentioned

00:22:42,300 --> 00:22:46,170
earlier is top of stack frequency and

00:22:44,370 --> 00:22:48,990
the top of stack frequency here is very

00:22:46,170 --> 00:22:50,820
high 71% on top of stack and that's why

00:22:48,990 --> 00:22:52,620
that block is super red and it's

00:22:50,820 --> 00:22:54,990
actually the same function over here it

00:22:52,620 --> 00:23:00,300
has a suspicious name it's named compute

00:22:54,990 --> 00:23:03,510
magic let's take a look at that so

00:23:00,300 --> 00:23:04,710
compute magic is doing stuff that we

00:23:03,510 --> 00:23:06,750
really don't need to be doing and

00:23:04,710 --> 00:23:08,490
sometimes you do find this in in code

00:23:06,750 --> 00:23:10,590
that's deployed there's just code in

00:23:08,490 --> 00:23:12,270
there that doesn't need to be there and

00:23:10,590 --> 00:23:14,940
in this case we have a very easy

00:23:12,270 --> 00:23:16,500
solution we just remove the code if we

00:23:14,940 --> 00:23:19,470
route all code everything would be super

00:23:16,500 --> 00:23:21,059
fast but so we're going to just remove

00:23:19,470 --> 00:23:22,920
compute magic in three days is just

00:23:21,059 --> 00:23:24,179
essentially the same thing but with the

00:23:22,920 --> 00:23:26,130
core to compute magic and the compute

00:23:24,179 --> 00:23:27,809
magic function removed see it's sitting

00:23:26,130 --> 00:23:35,870
in the the request path slowing things

00:23:27,809 --> 00:23:35,870
down so let's see how that's done

00:23:43,909 --> 00:23:48,379
oh cool thought 1,300 requests per

00:23:46,669 --> 00:23:50,109
second it's pretty good a good a good

00:23:48,379 --> 00:23:53,690
improvement from 12 requests per second

00:23:50,109 --> 00:23:57,799
and we can we could run doctor one more

00:23:53,690 --> 00:24:00,099
time just to see don't forget to close

00:23:57,799 --> 00:24:00,099
it

00:24:10,090 --> 00:24:15,290
and doctors not found any other problems

00:24:14,030 --> 00:24:17,600
there's probably more optimization you

00:24:15,290 --> 00:24:20,030
could do but in terms of big walloping

00:24:17,600 --> 00:24:22,370
problems this this process is now doing

00:24:20,030 --> 00:24:24,110
what it should be doing so it wasn't me

00:24:22,370 --> 00:24:26,360
is referring to the fact that I didn't

00:24:24,110 --> 00:24:29,780
do all of this work on my own

00:24:26,360 --> 00:24:31,340
I did I did do the majority of Xerox and

00:24:29,780 --> 00:24:33,200
write the the recommendations for bobble

00:24:31,340 --> 00:24:34,910
and so forth but there is a team

00:24:33,200 --> 00:24:36,770
basically there's a there's a team of

00:24:34,910 --> 00:24:38,210
people behind this andreas Madson he's a

00:24:36,770 --> 00:24:43,190
node core contributor he's a

00:24:38,210 --> 00:24:46,250
statistician I was talking about joy joy

00:24:43,190 --> 00:24:48,200
she's a designer that created the logos

00:24:46,250 --> 00:24:50,030
and the different things Connor are our

00:24:48,200 --> 00:24:52,280
product guy matera cleaner I mentioned

00:24:50,030 --> 00:24:55,430
earlier see what I mean

00:24:52,280 --> 00:24:58,250
and then Matthias booze is known as

00:24:55,430 --> 00:25:00,680
McIntosh online very smart programmer

00:24:58,250 --> 00:25:02,390
Alan helped us with our front-end stuff

00:25:00,680 --> 00:25:04,280
Camille who's here with me in London now

00:25:02,390 --> 00:25:08,150
working on a project that we'd love to

00:25:04,280 --> 00:25:09,720
hire you for as well and myself thanks

00:25:08,150 --> 00:25:14,059
everyone appreciate it

00:25:09,720 --> 00:25:14,059

YouTube URL: https://www.youtube.com/watch?v=tq3mqrV49l8


