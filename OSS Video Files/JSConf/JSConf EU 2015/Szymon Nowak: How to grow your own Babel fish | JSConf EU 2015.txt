Title: Szymon Nowak: How to grow your own Babel fish | JSConf EU 2015
Publication date: 2015-10-13
Playlist: JSConf EU 2015
Description: 
	In “The Hitchhiker’s Guide to the Galaxy” the Babel fish is an animal, which “if you stick one in your ear, you can instantly understand anything said to you in any form of language”. In this talk I’ll show you how easy it is to write a web app that does exactly that. I’ll also discuss some of the current Web Speech API limitations that hopefully will be removed in the near future to make the web even more powerful platform for real-time communication. And you won’t have to stick anything into your ear!

Intro music by @halfbyte
Captions: 
	00:00:21,090 --> 00:00:23,290
Hi everyone.

00:00:23,290 --> 00:00:28,940
Today I'll tell you how to grow your own Babel Fish.

00:00:28,940 --> 00:00:33,910
So it turns out that the metaphor is not really that original, especially if you've

00:00:33,910 --> 00:00:41,570
seen Sam stock from IBM Watson, but I'll briefly describe

00:00:41,570 --> 00:00:44,000
what the Babel Fish is.

00:00:44,000 --> 00:00:48,100
Babel Fish is unfortunately a fictional animal from Hitchhiker's Guide

00:00:48,100 --> 00:00:52,780
to the Galaxy by Douglas Adams, and according to the author

00:00:52,780 --> 00:00:56,390
it is a small yellow leech-like, and probably the

00:00:56,390 --> 00:00:57,989
oddest thing in the universe.

00:00:57,989 --> 00:01:06,380
It feeds on brainwave energy and long story short, it is a universal translator.

00:01:06,380 --> 00:01:08,710
The main problem is you actually have to stick one

00:01:08,710 --> 00:01:12,460
into your ear to make it work, but once you do it, you'll

00:01:12,460 --> 00:01:16,420
understand anything that is said to you in any language.

00:01:16,420 --> 00:01:18,720
If you haven't read the book, I really recommend

00:01:18,720 --> 00:01:20,950
you do.

00:01:20,950 --> 00:01:24,060
Okay, so moving on.

00:01:24,060 --> 00:01:28,200
About two years ago I become interested in WebRTC.

00:01:28,200 --> 00:01:33,840
Thomas will give a talk later today, so I'll describe it really briefly.

00:01:33,840 --> 00:01:36,560
It is an API that allows you to create peer-to-peer connections

00:01:36,560 --> 00:01:40,590
between devices, not necessarily just browsers, and you

00:01:40,590 --> 00:01:43,100
can basically send anything you want through such

00:01:43,100 --> 00:01:47,619
a connection: audio, video, or data.

00:01:47,619 --> 00:01:50,490
About a year ago I get a talk about interesting applications

00:01:50,490 --> 00:01:54,020
of WebRTC which was basically showing what stuff you

00:01:54,020 --> 00:01:58,819
can build with it, and I was looking for some applications

00:01:58,819 --> 00:02:01,430
to show, and the thing is that for data, there

00:02:01,430 --> 00:02:03,479
are lots of really interesting applications.

00:02:03,479 --> 00:02:08,929
I mean, WebRTC doesn't -- I mean the protocol is not based

00:02:08,929 --> 00:02:13,910
on TCP, but rather UDP, so we can do very low working,

00:02:13,910 --> 00:02:16,240
you can do file sharing because you have a direct connection

00:02:16,240 --> 00:02:20,739
between two devices, you don't have to send a file like

00:02:20,739 --> 00:02:22,570
for example in Dropbox.

00:02:22,570 --> 00:02:26,990
Maybe some of you have even heard my application called shadrock(?) which

00:02:26,990 --> 00:02:31,629
is like airdrop, which ironically doesn't work on

00:02:31,629 --> 00:02:37,810
safari, but you can actually do crazy stuff with the data

00:02:37,810 --> 00:02:41,599
translator, like implement protocol in JavaScript, which

00:02:41,599 --> 00:02:47,469
works in browser, and that's exactly what WebRTC does.

00:02:47,469 --> 00:02:51,090
But in the case of audio and video, things are boring.

00:02:51,090 --> 00:02:56,299
We have Skype for actually quite some time, and, you

00:02:56,299 --> 00:02:59,989
know, WebRTC does exactly the same but it is not really

00:02:59,989 --> 00:03:02,750
that interesting, right, so I didn't have anything to

00:03:02,750 --> 00:03:06,049
show and I decided to write my own application with a

00:03:06,049 --> 00:03:09,950
small technical twist, and that's what I'd like to show

00:03:09,950 --> 00:03:10,950
you today.

00:03:10,950 --> 00:03:11,950
Okay.

00:03:11,950 --> 00:03:12,950
So it is time for the demo.

00:03:12,950 --> 00:03:22,439
So just sit back, relax and enjoy.

00:03:22,439 --> 00:04:08,540
demo played) Okay, so 

00:04:08,540 --> 00:04:35,200
that's it.

00:04:35,200 --> 00:05:01,870
[applause].

00:05:01,870 --> 00:05:03,250
Thank you.

00:05:03,250 --> 00:05:07,830
So as you can see it is basically a universal translator as well, just like Babel

00:05:07,830 --> 00:05:09,199
Fish.

00:05:09,199 --> 00:05:11,430
The thing is that on the left-hand side of your

00:05:11,430 --> 00:05:15,830
video, yeah, left, you can select the language you're speaking.

00:05:15,830 --> 00:05:18,300
On the right-hand side you can see what the language

00:05:18,300 --> 00:05:20,750
the other person speaks, in and there is one picture

00:05:20,750 --> 00:05:23,099
that I haven't really shown, is that if the speech

00:05:23,099 --> 00:05:25,650
recognition is completely messed up you can type the message

00:05:25,650 --> 00:05:28,090
here and it will also be translated and sent to

00:05:28,090 --> 00:05:30,660
the other person.

00:05:30,660 --> 00:05:33,699
So it looks like kind of cool, right, but it is

00:05:33,699 --> 00:05:34,699
actually nothing new.

00:05:34,699 --> 00:05:39,910
This is a screenshot from a Skype translator by Microsoft.

00:05:39,910 --> 00:05:44,570
It was released in December last year and it does exact the same thing.

00:05:44,570 --> 00:05:47,781
It does it even better because in my application you

00:05:47,781 --> 00:05:50,440
have to press the button to turn the speech recognition

00:05:50,440 --> 00:05:52,290
on.

00:05:52,290 --> 00:05:55,360
I later describe why it is necessary.

00:05:55,360 --> 00:05:58,020
In this application you can just speak all the time,

00:05:58,020 --> 00:06:03,509
and it will all the time recognise what you're saying.

00:06:03,509 --> 00:06:06,320
There are some advantages over Skype.

00:06:06,320 --> 00:06:10,099
The most obvious one is that it works in the browser.

00:06:10,099 --> 00:06:13,050
The Skype application is obviously an application that

00:06:13,050 --> 00:06:16,270
works only on windows.

00:06:16,270 --> 00:06:23,440
It uses open standards for WebRTC, Skype is some proprietary protocols, but to be fair,

00:06:23,440 --> 00:06:25,500
Microsoft is currently working on a version of Skype that

00:06:25,500 --> 00:06:28,479
will work in the browser it will use in the next version

00:06:28,479 --> 00:06:32,099
of WebRTC, I'm not sure about Skype translator,

00:06:32,099 --> 00:06:35,220
but at least a normal version of Skype should work

00:06:35,220 --> 00:06:37,270
in the browser.

00:06:37,270 --> 00:06:39,590
But the most important thing I want to say is

00:06:39,590 --> 00:06:44,180
that really, the first working version of it took one

00:06:44,180 --> 00:06:45,919
day to write.

00:06:45,919 --> 00:06:52,830
And it really shows how powerful today's browsers are and how powerful the APIs are.

00:06:52,830 --> 00:06:56,169
And it is really not that hard to write something like

00:06:56,169 --> 00:06:57,169
that.

00:06:57,169 --> 00:07:01,910
So hopefully you're all wondering how it works.

00:07:01,910 --> 00:07:04,550
So I'll go over it step by step, warning there will be

00:07:04,550 --> 00:07:08,340
some code ahead, but not much fortunately so the first

00:07:08,340 --> 00:07:11,100
thing is obviously send audio and video, and it is

00:07:11,100 --> 00:07:16,419
done via WebRTC, so it is unfortunately not the simplest

00:07:16,419 --> 00:07:19,650
and easiest API, so I am using a library to make

00:07:19,650 --> 00:07:21,250
it easier.

00:07:21,250 --> 00:07:25,370
The library at the time is called simple WebRTC, and

00:07:25,370 --> 00:07:28,169
using it looks like this.

00:07:28,169 --> 00:07:32,610
So that's like less than ten lines of code and that's all you actually

00:07:32,610 --> 00:07:37,560
need to make your own Skype, or very basic version of Skype

00:07:37,560 --> 00:07:39,450
application.

00:07:39,450 --> 00:07:41,099
So I'll go it quickly.

00:07:41,099 --> 00:07:46,900
First, you need to instantiate it, then you just pass IDs of

00:07:46,900 --> 00:07:51,120
DOM elements where you want to display your video, and

00:07:51,120 --> 00:07:54,659
video for other people.

00:07:54,659 --> 00:07:59,319
You wait for ready to call event, which is when your browser gets access to your camera

00:07:59,319 --> 00:08:04,250
and microphone, and then you have to call to join

00:08:04,250 --> 00:08:05,990
the room, which will connect you to all the other peers

00:08:05,990 --> 00:08:06,990
in the same room.

00:08:06,990 --> 00:08:09,300
That's actually the most complicated part if

00:08:09,300 --> 00:08:14,240
you'll use WebRTC directly, and that's just one call in

00:08:14,240 --> 00:08:16,569
simple WebRTC library.

00:08:16,569 --> 00:08:17,960
Yeah, that's it.

00:08:17,960 --> 00:08:20,759
We're actually down to the audio and video part.

00:08:20,759 --> 00:08:26,500
So, moving on to a bit more interesting part, that is speech recognition.

00:08:26,500 --> 00:08:31,439
It is done using web speech recognition API, and using it looks

00:08:31,439 --> 00:08:33,469
very similar to the code that we've just seen.

00:08:33,469 --> 00:08:37,270
Again, you have to instantiate an object, you wait for a result

00:08:37,270 --> 00:08:40,550
event which will be triggered whenever the speech recognition

00:08:40,550 --> 00:08:44,440
actually has some results, and finally you go to the

00:08:44,440 --> 00:08:48,500
start that will ask for your microphone and start speech

00:08:48,500 --> 00:08:50,530
recognition.

00:08:50,530 --> 00:08:57,090
Okay, so the next step is that speech recognition gives us a transcript of what

00:08:57,090 --> 00:09:00,200
you just said, so we need to send it to Google Translate

00:09:00,200 --> 00:09:01,200
to actually translate it.

00:09:01,200 --> 00:09:03,780
It is just done using plain code.

00:09:03,780 --> 00:09:08,210
So no closed here.

00:09:08,210 --> 00:09:11,890
Once you have the original transcript and it is translation, you can send it to the

00:09:11,890 --> 00:09:13,200
other person.

00:09:13,200 --> 00:09:15,251
And the cool thing is that there is WebRTC data transfer

00:09:15,251 --> 00:09:20,080
for that, so you don't need any servers, so normally

00:09:20,080 --> 00:09:23,040
you'd send it using Ajax or web sockets or something

00:09:23,040 --> 00:09:26,220
like that, but in the case of WebRTC you have direct

00:09:26,220 --> 00:09:27,940
connection with the other person.

00:09:27,940 --> 00:09:31,530
We don't need any server for that.

00:09:31,530 --> 00:09:36,130
So, using the data channels, this is really easy.

00:09:36,130 --> 00:09:41,080
You have just one to send directly to all, you specific the channel name, payload name

00:09:41,080 --> 00:09:43,410
which is basically what you're sending, and the data

00:09:43,410 --> 00:09:44,410
itself.

00:09:44,410 --> 00:09:49,020
Yeah, that's it.

00:09:49,020 --> 00:09:54,510
So the last step is, so we already have the original end translator transcript

00:09:54,510 --> 00:09:57,620
on the other side, and now it is time to read it out loud

00:09:57,620 --> 00:10:01,260
and we use again API for that.

00:10:01,260 --> 00:10:05,320
It is called synth API, it looks like that.

00:10:05,320 --> 00:10:12,600
You have this really strange object, I just have to instantiate, the language, the message

00:10:12,600 --> 00:10:18,580
itself, and then you have this, and then you have

00:10:18,580 --> 00:10:22,580
this speak method where you have to just pass this object

00:10:22,580 --> 00:10:28,590
to, and that's actually, as you can see, hopefully

00:10:28,590 --> 00:10:33,370
the individual steps of it are really simple,

00:10:33,370 --> 00:10:34,951
in the real application there is a bit more code, for

00:10:34,951 --> 00:10:37,900
example, for call handling for handling language switches

00:10:37,900 --> 00:10:42,080
and so on, but yeah, that's actually the core of it.

00:10:42,080 --> 00:10:44,850
There is not really that much code in the application.

00:10:44,850 --> 00:10:48,330
If anyone is interested in the actual code, I'll give the

00:10:48,330 --> 00:10:54,011
link to the code at the end of the talk.

00:10:54,011 --> 00:10:58,480
The second part, I'd like to give you a very brief introduction into

00:10:58,480 --> 00:11:03,190
web speech API, I just actually wanted to mention two

00:11:03,190 --> 00:11:04,780
things.

00:11:04,780 --> 00:11:07,170
So the first thing is that web speech API recognition

00:11:07,170 --> 00:11:09,170
has two modes, the first one is one shot, and

00:11:09,170 --> 00:11:11,640
the second one is continuous.

00:11:11,640 --> 00:11:15,070
In the one shot mode, in the speech recognition, the algorhythmic will stop when

00:11:15,070 --> 00:11:21,800
you want, which is great for giving commands and that's

00:11:21,800 --> 00:11:24,640
the mode used in my application.

00:11:24,640 --> 00:11:29,420
The other mode, which is more interesting, is the continuous mode and basically

00:11:29,420 --> 00:11:31,430
it will even if you make a pause, it will just

00:11:31,430 --> 00:11:34,240
continue to work, at least in theory.

00:11:34,240 --> 00:11:38,450
So this one is great for dictating stuff, or for my application, with

00:11:38,450 --> 00:11:42,570
me at least, where users -- two users have a conversation.

00:11:42,570 --> 00:11:46,380
Unfortunately it doesn't really work that well.

00:11:46,380 --> 00:11:49,880
And I'll tell why really soon.

00:11:49,880 --> 00:11:54,690
The second thing is that according to the spec, the API itself is like

00:11:54,690 --> 00:11:59,420
agnostic of the underlying speech recognition implementation,

00:11:59,420 --> 00:12:01,690
so it basically doesn't implement -- it doesn't

00:12:01,690 --> 00:12:04,660
specify how the speech recognition should work.

00:12:04,660 --> 00:12:08,490
And you can have speech recognition on server, or on the client.

00:12:08,490 --> 00:12:11,670
So in Chrome, it uses Google services, so it is

00:12:11,670 --> 00:12:17,180
on the server, but it could be done locally.

00:12:17,180 --> 00:12:23,000
So while the speech API is really awesome, it has some issues.

00:12:23,000 --> 00:12:28,420
So if you'd like to experiment with it, I'll just, you know,

00:12:28,420 --> 00:12:31,190
briefly describe what the issues are, so at least

00:12:31,190 --> 00:12:34,540
you know what to expect.

00:12:34,540 --> 00:12:38,670
So the first issue is that the speech recognition will stop if you talk for too

00:12:38,670 --> 00:12:41,700
long, I'm not sure what the exact limit is, but if you talk

00:12:41,700 --> 00:12:46,970
for a minute or over a minute, it will stop.

00:12:46,970 --> 00:12:51,790
You'll get abort event, and yeah, that's actually -- the second

00:12:51,790 --> 00:12:55,260
issue is actually quite the opposite.

00:12:55,260 --> 00:12:58,300
It will stop if you don't talk for as long.

00:12:58,300 --> 00:13:03,000
I'm talking about the continuous mode which should handle pauses, but if you will

00:13:03,000 --> 00:13:05,920
make long enough pause, it will just stop.

00:13:05,920 --> 00:13:08,770
Again, you'll get abort event, and there is a work around these

00:13:08,770 --> 00:13:09,770
issues.

00:13:09,770 --> 00:13:12,040
I mean, you can wait for the abort event and

00:13:12,040 --> 00:13:16,410
just call start again, but unfortunately, in some cases,

00:13:16,410 --> 00:13:19,380
after you call start, it will abort again and it will

00:13:19,380 --> 00:13:23,870
end up with this, which makes the application unstable

00:13:23,870 --> 00:13:26,550
and actually that's the reason why my application doesn't

00:13:26,550 --> 00:13:30,590
use the continuous mode.

00:13:30,590 --> 00:13:32,960
So the most important issue, I think, is that web

00:13:32,960 --> 00:13:35,740
speech API doesn't work with WebRTC media streams, so

00:13:35,740 --> 00:13:40,050
the thing is the specification was created probably

00:13:40,050 --> 00:13:43,370
before WebRTC application, so that's why it doesn't

00:13:43,370 --> 00:13:46,970
mention anything about media streams, and it means that

00:13:46,970 --> 00:13:50,520
the web speech API can only recognise input from your

00:13:50,520 --> 00:13:54,480
own mic, nothing else, really, and that causes a lot of

00:13:54,480 --> 00:13:55,480
issues.

00:13:55,480 --> 00:13:57,500
So the first thing is that, for example, in my

00:13:57,500 --> 00:14:00,800
application, which is WebRTC, and web speech recognition

00:14:00,800 --> 00:14:03,690
API, it will ask you twice for your mic, which is really

00:14:03,690 --> 00:14:05,080
confusing for users.

00:14:05,080 --> 00:14:09,920
So once it will ask for WebRTC, the second time with the web speech RPI, but

00:14:09,920 --> 00:14:12,990
the most important thing is, like I said, it can only

00:14:12,990 --> 00:14:14,790
recognise what you're saying.

00:14:14,790 --> 00:14:21,590
You can't get remote audio stream from another person and just pass it into

00:14:21,590 --> 00:14:24,710
web speech recognition object, right.

00:14:24,710 --> 00:14:29,990
So if you could do it, you could basically just write an extension or

00:14:29,990 --> 00:14:34,170
plug into any web application and perform the whole recognition

00:14:34,170 --> 00:14:37,680
of whatever any other person has said on your

00:14:37,680 --> 00:14:39,580
computer, which would be really cool.

00:14:39,580 --> 00:14:42,540
Another thing is I'm not sure if you're aware, but using WebRTC you

00:14:42,540 --> 00:14:45,110
can actually make phone calls or make or receive phone

00:14:45,110 --> 00:14:46,230
calls.

00:14:46,230 --> 00:14:51,720
You have to use gateway service, but it is possible,

00:14:51,720 --> 00:14:54,690
and if you could pass like remote audio to speech

00:14:54,690 --> 00:14:58,050
recognition, you could actually, for example, make an application

00:14:58,050 --> 00:15:01,410
where you just call somebody, take audio from them and

00:15:01,410 --> 00:15:03,830
just pass it for speech recognition locally.

00:15:03,830 --> 00:15:07,280
So it will be pretty good for, for example, people with

00:15:07,280 --> 00:15:09,390
hearing loss, which could basically create a transcript

00:15:09,390 --> 00:15:12,030
on the phone call on the fly.

00:15:12,030 --> 00:15:17,740
So yeah, unfortunately it doesn't work, but like I said, this web speech

00:15:17,740 --> 00:15:19,690
spec doesn't really mention anything about it,

00:15:19,690 --> 00:15:24,000
but actually it would be really easy to extend the specification

00:15:24,000 --> 00:15:25,890
without breaking it down.

00:15:25,890 --> 00:15:27,690
That's it.

00:15:27,690 --> 00:15:32,810
You just have to pass audio stream to this speech recognition

00:15:32,810 --> 00:15:36,070
object and it could just work.

00:15:36,070 --> 00:15:38,450
So that's actually what Firefox is doing right now.

00:15:38,450 --> 00:15:44,180
They're extending, it has been at least, extending this API a bit without breaking

00:15:44,180 --> 00:15:49,700
it actually, I just was told that it should work

00:15:49,700 --> 00:15:54,680
in Firefox 2.5, so hopefully it will sync soon.

00:15:54,680 --> 00:15:57,400
And like the most obvious reason why this API is not really

00:15:57,400 --> 00:15:59,930
more popular, it works only in Chrome.

00:15:59,930 --> 00:16:05,870
And the issue is that it is not even like all chrome, desktop and mobile.

00:16:05,870 --> 00:16:08,880
The issue is that for example there is a small bug in

00:16:08,880 --> 00:16:11,360
mobile version of Chrome which doesn't allow you

00:16:11,360 --> 00:16:15,570
to use speech recognition if you're WebRTC at the same time.

00:16:15,570 --> 00:16:20,200
So unfortunately my app won't work on android.

00:16:20,200 --> 00:16:23,150
But what can we do about these issues?

00:16:23,150 --> 00:16:26,850
I mean, the first is we can ask them nicely to update

00:16:26,850 --> 00:16:31,570
the spec, ask Google to fix the implementation, or we could

00:16:31,570 --> 00:16:34,220
ask another browser mender to actually implement

00:16:34,220 --> 00:16:35,730
the API.

00:16:35,730 --> 00:16:38,310
And while the speech recognition itself is really

00:16:38,310 --> 00:16:41,570
complicated problem, the technology is not really an

00:16:41,570 --> 00:16:44,140
issue here.

00:16:44,140 --> 00:16:49,610
Personal assistance or virtual assistance is really popular lately, and it turns other

00:16:49,610 --> 00:16:52,540
that every browser actually has some implementation of

00:16:52,540 --> 00:16:53,600
it.

00:16:53,600 --> 00:16:59,190
So Google obviously has Google Now, Apple has

00:16:59,190 --> 00:17:04,200
Siri, Firefox actually doesn't have anything, but they are

00:17:04,200 --> 00:17:08,940
using a library called Pocket Sphinx, which does speech

00:17:08,940 --> 00:17:12,990
recognition locally on the device.

00:17:12,990 --> 00:17:16,500
But there is also another way.

00:17:16,500 --> 00:17:23,140
We don't really need to wait for a browser vendors, so you probably remember this one.

00:17:23,140 --> 00:17:24,140
Right?

00:17:24,140 --> 00:17:26,660
So all the time I'm talking about making implementation

00:17:26,660 --> 00:17:30,610
of the API, but it is actually possible to create

00:17:30,610 --> 00:17:33,590
a virtual library that will have exactly the same API

00:17:33,590 --> 00:17:39,039
as web speech API and just would use some third-party

00:17:39,039 --> 00:17:43,029
service to perform the speech recognition, or do it

00:17:43,029 --> 00:17:44,029
locally.

00:17:44,029 --> 00:17:45,809
So again, like I said, the speech recognition

00:17:45,809 --> 00:17:49,590
is like really complex and hard problem, but again,

00:17:49,590 --> 00:17:51,720
as it turns out, there are plenty of companies that are

00:17:51,720 --> 00:17:54,360
actually happy to write.

00:17:54,360 --> 00:18:00,769
So Facebook has recently announced Facebook M assistant, yet another -- they

00:18:00,769 --> 00:18:03,779
also obviously have technology for speech recognition.

00:18:03,779 --> 00:18:07,629
There is another assistant called hound by SoundHound.

00:18:07,629 --> 00:18:10,410
I'm not sure if you have heard about it but it looks

00:18:10,410 --> 00:18:17,279
promising, BaiDu has announced one recently, there are

00:18:17,279 --> 00:18:22,879
some other companies that do speech recognition, obviously,

00:18:22,879 --> 00:18:27,010
and there is also we've just seen a demo, there

00:18:27,010 --> 00:18:30,200
is IBM Watson which also has API performing speech

00:18:30,200 --> 00:18:31,200
recognition.

00:18:31,200 --> 00:18:35,629
Finally, there is an interesting project called Pocket

00:18:35,629 --> 00:18:40,200
Sphinx JS which actually uses the library that is used

00:18:40,200 --> 00:18:45,080
by Firefox and it compiles it using scripts into

00:18:45,080 --> 00:18:48,610
JavaScript and this actually provides the fully

00:18:48,610 --> 00:18:52,659
functional speech recognition library in your browser

00:18:52,659 --> 00:18:55,140
and actually in any browser that supports WebRTC.

00:18:55,140 --> 00:19:01,759
It doesn't have to support speech recognition.

00:19:01,759 --> 00:19:05,720
So that's actually it.

00:19:05,720 --> 00:19:11,610
I kind of hope that you saw that, you know, the whole implementation wasn't

00:19:11,610 --> 00:19:12,809
really that hard.

00:19:12,809 --> 00:19:14,429
Each step was really easy.

00:19:14,429 --> 00:19:18,299
You can see we have today's browsers are really powerful

00:19:18,299 --> 00:19:20,960
and I strongly encourage you to maybe instead of writing

00:19:20,960 --> 00:19:25,750
yet another framework just try to experiment with them.

00:19:25,750 --> 00:19:29,730
Also I kind of hope that once Firefox actually releases

00:19:29,730 --> 00:19:33,490
a version that of their browser that has a web speech

00:19:33,490 --> 00:19:42,450
implementation API, I mean web speech API implementation, it will just become more popular.

00:19:42,450 --> 00:19:46,559
My name is size a Szymon and I work with these

00:19:46,559 --> 00:19:47,559
companies.

00:19:47,559 --> 00:19:48,559
That's all.

00:19:48,559 --> 00:19:48,561

YouTube URL: https://www.youtube.com/watch?v=ltMCrW9JCEE


