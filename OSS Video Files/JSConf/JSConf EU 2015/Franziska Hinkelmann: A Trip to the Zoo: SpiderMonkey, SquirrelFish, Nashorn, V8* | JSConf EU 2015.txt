Title: Franziska Hinkelmann: A Trip to the Zoo: SpiderMonkey, SquirrelFish, Nashorn, V8* | JSConf EU 2015
Publication date: 2015-10-19
Playlist: JSConf EU 2015
Description: 
	Do you know how JavaScript engines work and why they are so blazingly fast?

Learn about the fundamentals like abstract syntax tree, opcodes, and just-in-time compilation. JavaScript code can be almost as fast as native C++ code. How do engines accomplish this? Lets look into optimization techniques including hidden classes, ahead-of-time compilation, and single instruction multiple data computations, which, if applied correctly, will give your code a turbo boost.

Understand the inner workings of JavaScript engines and learn by examples how to write faster code.

*SpiderMonkey, SquirrelFish (Nitro), Nashorn, and V8 are JavaScript engines for Firefox, Safari, JVM, and Chrome.

Intro music by @halfbyte
Captions: 
	00:00:24,640 --> 00:00:29,230
Hello, so we'll be doing something a little different because I am going to take you on

00:00:29,230 --> 00:00:37,370
a trip to the zoo. It's a zoo with Monkeys, with rhinos and an aquarium, more specifically

00:00:37,370 --> 00:00:50,480
there will be Spider Monkey, Rhino, and SquirrelFish, I'll be talking about JavaScript engines.

00:00:50,480 --> 00:00:57,010
JavaScript has got incredibly fast we look at how JavaScript engines work and optimisation

00:00:57,010 --> 00:01:02,010
techniques, I'll show you a few examples of how you can write code that takes advantage

00:01:02,010 --> 00:01:08,410
of those optimisations rather than working again them, I won't focus on the differences

00:01:08,410 --> 00:01:13,350
between the engines, instead we will look at what they have in common, the optimisation

00:01:13,350 --> 00:01:16,720
examples work independently of the browser choice.

00:01:16,720 --> 00:01:25,320
So, a little bit of history, SpiderMonkey is the JavaScript engine that was first, the

00:01:25,320 --> 00:01:32,390
very first JavaScript engine, written in 1995 back then used in Netscape communicator and

00:01:32,390 --> 00:01:38,360
after several rewrites that's the engine we have in Firefox today.

00:01:38,360 --> 00:01:46,000
Of course SpiderMonkey comes with Monkey friends, trace money JagerMonkey, TraceMonkey and IonMonkey,

00:01:46,000 --> 00:01:56,100
they help SpiderMonkey took be even faster. Then in 2002 the squirrel fish was written,

00:01:56,100 --> 00:02:10,970
for safari and webkit, we also have rhinos, it's a word play, it's a Java script written

00:02:10,970 --> 00:02:23,650
in oracle, so the next animal, I'm sorry, engine, it's V 8, it's the JavaScript engine

00:02:23,650 --> 00:02:33,680
in Chrome and it came out in 2009, Microsoft jscript or Chakra was also one of the older

00:02:33,680 --> 00:02:44,040
ones that came in 1986, that's what we have in Internet Explorer and Edge. With V 8 and

00:02:44,040 --> 00:02:52,290
jscript I ran out of animals, that's the end of our trip to the zoo, sadly for the rest

00:02:52,290 --> 00:02:58,900
of the talk there won't be any more animals, let's look at the technical side.

00:02:58,900 --> 00:03:03,540
As I already mentioned the engines have all been rewritten several times and that was

00:03:03,540 --> 00:03:10,680
mainly ton for performance, we want fast JavaScript, we want the websites to be fast and responsive,

00:03:10,680 --> 00:03:17,580
we need good performance. How fast are we? here are some benchmarks over the last ten

00:03:17,580 --> 00:03:25,099
years, the SunSpider benchmark, as you can see the score gets higher, fee got faster

00:03:25,099 --> 00:03:33,019
and faster. Ok, are we fast yet, we got faster, there is a website, 'Are we faster.com', you

00:03:33,019 --> 00:03:39,891
see similar pictures, another benchmark, different versions of Chrome and Firefox, as you can

00:03:39,891 --> 00:03:44,450
see they got faster, massive performance improvements.

00:03:44,450 --> 00:03:52,420
All right. So, yes we are faster, but we are faster compared to an older version of ourselves,

00:03:52,420 --> 00:03:59,409
does it actually mean we are fast? We are faster than 20 years ago, ok, but are we fast?

00:03:59,409 --> 00:04:05,190
Let's compare to something where we know that it really is fast.

00:04:05,190 --> 00:04:13,590
Let's have a race between C++ and JavaScript, we know C++ is fast. So, the contenders are

00:04:13,590 --> 00:04:21,340
C++ and JavaScript and the race track are prime numbers. A basic example I want to compute

00:04:21,340 --> 00:04:28,690
the first 25,000 prime numbers and see how fast the JavaScript compared to C++.

00:04:28,690 --> 00:04:35,640
So, on the left you have C++ and on the right you have JavaScript, don't worry, you don't

00:04:35,640 --> 00:04:40,210
need to read thorough the algorithm I just want to give you a quick idea, it's fairly

00:04:40,210 --> 00:04:46,510
short, it easily fits on the page and the C++ and JavaScript code, they do the same

00:04:46,510 --> 00:04:48,110
things, same algorithm.

00:04:48,110 --> 00:05:00,470
Ok, so when I run this, with C++, I first printout the 25,000 prime number, which is

00:05:00,470 --> 00:05:05,110
that one, to make sure my algorithm is correct, it took me a little bit over 6 seconds, 6.2,

00:05:05,110 --> 00:05:14,730
seconds, so how did JavaScript do? So I ran this running D 8 the debugging shell for d8

00:05:14,730 --> 00:05:23,000
makes it really easy to time it, I ran the prime number example in JavaScript, I get

00:05:23,000 --> 00:05:32,030
the correct answer so that's good. It's barely slower than C++, 6.7 seconds. So if you do

00:05:32,030 --> 00:05:40,969
the maths, JavaScript is less than 14% slower than C++. I think this is really amazing,

00:05:40,969 --> 00:05:47,080
those are amazing numbers, because what's so special about this, why is it surprising

00:05:47,080 --> 00:05:53,360
that JavaScript can calculate prime numbers almost as fast as C++. Well C++

00:05:53,360 --> 00:05:59,700
is statically typed, before you run your code you have to specify for every variable what

00:05:59,700 --> 00:06:07,080
type it is, so even at compile time the compiler knows exactly what it will get, knows it will

00:06:07,080 --> 00:06:14,450
get an integer, string or what class it gets. Before run time you can optimise the machine

00:06:14,450 --> 00:06:20,370
code that you get and then you get really fast code once you actually execute it.

00:06:20,370 --> 00:06:24,500
JavaScript is dynamically typed, you don't have this information, only at run time do

00:06:24,500 --> 00:06:30,830
we know what we get, we can't generate optimised code ahead of time.

00:06:30,830 --> 00:06:40,210
Just to make sure here, did compile C++ with -03 optimisation flag, if you don't optimise

00:06:40,210 --> 00:06:46,630
your code when compiling, JavaScript would be way faster than C++ version of this, it's

00:06:46,630 --> 00:06:55,130
ahead of time optimised code in C++ and it's just a little bit faster than JavaScript.

00:06:55,130 --> 00:07:01,050
Ok, so in the next 20 minutes we will look at what JavaScript engines are doing to get

00:07:01,050 --> 00:07:06,560
to this massive performance.

00:07:06,560 --> 00:07:15,300
So a little bit of background, your classic compiler has four components, lLexer, the

00:07:15,300 --> 00:07:24,600
parser, it takes tokens and generates the - the translator translates that tree into

00:07:24,600 --> 00:07:33,570
byte code and then the account code is interpreted. There is basically two main types of compilers,

00:07:33,570 --> 00:07:44,471
ahead of time compilation and ‘just in time’ compilation. When it's ahead of time, before

00:07:44,471 --> 00:07:51,029
you run it, you compile everything if possible, you optimise the machine code that you get

00:07:51,029 --> 00:07:57,160
and then you run it. Obviously C++ is ahead of time. Why do I say obviously, you have

00:07:57,160 --> 00:08:03,479
to do two separate steps, you call the compiler that gives you an executable, then in the

00:08:03,479 --> 00:08:12,959
second step you call the compiler, so clear separation. So the benefit of a ahead of time

00:08:12,959 --> 00:08:19,880
compilation you can take your time to optimise this code, to get really could machine code

00:08:19,880 --> 00:08:23,800
so when you run it it's very fast.

00:08:23,800 --> 00:08:29,280
JavaScript on the other hand uses a 'Just in time' compiler, so we're not compiling

00:08:29,280 --> 00:08:34,820
everything up front ahead of time, for once it would not be great if you opened a website

00:08:34,820 --> 00:08:39,300
and you have to wait for all the JavaScript code to compile before anything starts and

00:08:39,300 --> 00:08:46,060
also since it's dynamically typed we could not generate ahead of time good machine code.

00:08:46,060 --> 00:08:52,260
So we do it, just in time. This is sometimes also called 'Lazy compilation' we only compile

00:08:52,260 --> 00:08:55,000
what we need as we want to execute it.

00:08:55,000 --> 00:09:03,450
Ok this is good, we don't have a lot of information ahead of time so we do just in time compilation,

00:09:03,450 --> 00:09:08,080
but usually just in time compilation is very slow, you don't have the big picture, you

00:09:08,080 --> 00:09:13,140
can't optimise, so the machine code you get does not run very fast.

00:09:13,140 --> 00:09:19,890
So how do we get the almost C++ performance if we are using just in time compilation?

00:09:19,890 --> 00:09:28,190
Well, the basic concept that all modern JavaScript engines use are hidden classes. So we don't

00:09:28,190 --> 00:09:39,460
have hidden classes, at least until ECMA script, but behind the scenes the JavaScript engines

00:09:39,460 --> 00:09:47,060
assign a type or hidden class to any object. So, integers and strings obviously have different

00:09:47,060 --> 00:09:54,751
types of different classes, also objects have hidden classes. Going

00:09:54,751 --> 00:09:59,800
to use this example to show you how the engine in the background assigns the hidden classes.

00:09:59,800 --> 00:10:05,170
I've constructed a function for a point that takes two arguments and assigns them to x

00:10:05,170 --> 00:10:18,370
and y , when I instantiate a new point a, my engine creates a point object, it's a pointer

00:10:18,370 --> 00:10:24,770
and now when I look at the constructor, the engine says, "Ok, so this point object has

00:10:24,770 --> 00:10:36,820
a hidden class, let's call it z0", so my object has class z0, I go through and assign x , the

00:10:36,820 --> 00:10:44,450
engine is saying ok, we’ll take Z 0 and add an x to it, as the first member, I'm getting

00:10:44,450 --> 00:10:52,180
a new hidden class, C 1, it's the hidden class that comes out of Z 0 if you had member x

00:10:52,180 --> 00:10:57,740
to it, Z 1, opposite zero you have the value for x .

00:10:57,740 --> 00:11:08,470
Ok, if you go on in the constructor we do the same things for y , we have C 1 and we

00:11:08,470 --> 00:11:20,790
add y to it, from the 1 we get another hidden class and C 2, where x and y in. Point object

00:11:20,790 --> 00:11:28,030
now has hidden class C 2 which we get by adding an x and then a y . If we instantiate another

00:11:28,030 --> 00:11:37,670
point the compiler goes through the same steps but can reuse C 1 and C 2, in the end both

00:11:37,670 --> 00:11:43,390
A and B will have hidden class C 2, which makes sense, they are very similar objects

00:11:43,390 --> 00:11:50,930
generated the same way and we probably want them treated the same way later at run time.

00:11:50,930 --> 00:11:56,730
If we change the order of assigning x and y for example, or if we assign another variable,

00:11:56,730 --> 00:12:03,600
then the two objects would not have the same hidden class, the order is important and then

00:12:03,600 --> 00:12:12,850
also the same variables. If you are not sure if two objects have the same hidden class

00:12:12,850 --> 00:12:21,190
you can easily check. If you pass in the flag, allow native syntax, then you can call a map

00:12:21,190 --> 00:12:26,550
to check if two objects really have the same hidden class. If that was confusing with the

00:12:26,550 --> 00:12:30,290
order or if you are not sure that two things there are same, this is how you can check.

00:12:30,290 --> 00:12:37,940
I instantiate A and B, they have an x and a y and have the same hidden class. Now if

00:12:37,940 --> 00:12:43,340
I had another variable to A, they don't have the same hidden class any more.

00:12:43,340 --> 00:12:49,820
So, in practice that means we want the same objects to have the same hidden class so it's

00:12:49,820 --> 00:12:55,780
good practice to always instantiate all your members in the constructor function. So rather

00:12:55,780 --> 00:13:00,960
do this, we're in the constructor function you said x and y , then this.

00:13:00,960 --> 00:13:09,470
So, initial all objects and functions.

00:13:09,470 --> 00:13:15,360
The compiler is always keeping track of stuff and adding hidden classes how does that make

00:13:15,360 --> 00:13:24,590
it faster. The hidden classes there are basis for in line caching. So in line caching is

00:13:24,590 --> 00:13:37,100
what gives us a big performance boost, what it does is we're caching function, so if we

00:13:37,100 --> 00:13:43,730
have a function do something of A, when we want to execute it we compile it into a machine

00:13:43,730 --> 00:13:50,050
code and into machine code for anything that looks like an A, anything that has the hidden

00:13:50,050 --> 00:13:55,880
class of A, now we cache the machine code, the next time we come across, do something,

00:13:55,880 --> 00:14:02,010
rather than compile it again for anyone that looks like a B, we just check if A and B are

00:14:02,010 --> 00:14:07,550
similar, if they have the same hidden class we can use the code we already generated.

00:14:07,550 --> 00:14:12,660
If A and B have the same hidden class we don't have to do all the same work, only if A and

00:14:12,660 --> 00:14:18,290
B are different in the sense they don't have the same hidden class, we have to recompile

00:14:18,290 --> 00:14:19,290
and

00:14:19,290 --> 00:14:26,020
generate the machine instructions and then probably cache that version for later use.

00:14:26,020 --> 00:14:31,420
What does this mean in practice? I have a silly function twice here, takes an argument

00:14:31,420 --> 00:14:45,240
A and runs A+ A, if I call twice on the string I expect the string with concat - a million

00:14:45,240 --> 00:14:56,380
times in the loop, I'm called in randomly on a integer or string.

00:14:56,380 --> 00:15:02,280
Now for a million times it takes about six seconds, I'm changing this a little bit, instead

00:15:02,280 --> 00:15:08,190
of twice I'm making two functions, twice string and twice int, they are exactly the same functions

00:15:08,190 --> 00:15:14,690
they get an argument and return A+ A, the intention with the name, I call twice string

00:15:14,690 --> 00:15:22,130
only on strings and twice only on integers. Then again in the loop a million times I randomly

00:15:22,130 --> 00:15:29,350
call twice string on A or twice on 4. So the output module, the randomness is exactly the

00:15:29,350 --> 00:15:39,060
same of my programme before, just calling A+ A, I call two separate functions.

00:15:39,060 --> 00:15:47,450
Let's look at how long this takes, the first example took five seconds, sorry, six seconds,

00:15:47,450 --> 00:15:53,070
but this one with the two functions it only takes five seconds. So we have about 20% speed

00:15:53,070 --> 00:15:59,610
up, it's not great, but it's a speed up so that's good.

00:15:59,610 --> 00:16:04,779
In the first example where we only had one function, that function is called a polymorphic

00:16:04,779 --> 00:16:10,370
function, because the parameter it gets come in different shapes, polymorphic, they come

00:16:10,370 --> 00:16:18,399
as int or string. In the second example the faster one both functions are considered monomorphic

00:16:18,399 --> 00:16:24,910
because they always get the same input the twice string always only got strings so the

00:16:24,910 --> 00:16:30,190
takeaway, monomorphic functions are better than polymorphic function don't switch up

00:16:30,190 --> 00:16:31,899
the type of your parameters.

00:16:31,899 --> 00:16:45,560
Here's a disclaimer though I am talking about optimisation, premature optimisation is the

00:16:45,560 --> 00:16:55,660
root of all evil. Don't go home and change all your polymorphic functions. In the example

00:16:55,660 --> 00:17:02,520
I just had I ran it a million times, I got about a second faster, so probably your application

00:17:02,520 --> 00:17:08,380
is not calling anything a million times, and if you go and change your polymorphic function

00:17:08,380 --> 00:17:14,260
you probably won't see any speed up overall. The only thing you might happen you introduce

00:17:14,260 --> 00:17:25,959
bugs by typos or other mistakes. It's always when you talk about optimisation, don't blindly

00:17:25,959 --> 00:17:38,540
optimise. Our JavaScript engine the compiler is already a just-in-time compiler already,

00:17:38,540 --> 00:17:46,910
that's not quite the speed we saw in the C++ example. So, the trick that all modern JavaScript

00:17:46,910 --> 00:17:53,300
engines use is that they have at least 2 compilers, they have us regular just-in-time compiler

00:17:53,300 --> 00:18:00,360
and an optimised just-in-time compiler so what happens just before we go through our

00:18:00,360 --> 00:18:06,600
tokens, abstract syntax, bytecode then pass it on to just-in-time compiler, which does

00:18:06,600 --> 00:18:12,230
it’s work but now it's profiling and when it realises that it's running a function a

00:18:12,230 --> 00:18:17,920
lot of times it says ooh this is a hot function because we're running it a lot, and the optimised

00:18:17,920 --> 00:18:22,580
just-in-time compiler should take over. So for any function that’s been run a lot the

00:18:22,580 --> 00:18:29,090
optimising just-in-time compiler generates faster machine code for that.

00:18:29,090 --> 00:18:34,559
So anything that's run a lot can be executed faster. But you see there’s a back arrow

00:18:34,559 --> 00:18:38,000
going back from the optimised to regular just-in-time

00:18:38,000 --> 00:18:46,790
compiler, well this happens if the optimised compiler has read the machine code for a function,

00:18:46,790 --> 00:18:51,540
all of a sudden we call this function with a different hidden class then we can't use

00:18:51,540 --> 00:18:59,520
the machine code and instead we fall back to the old slower just-in-time compiler.

00:18:59,520 --> 00:19:08,120
So in V8 the optimised compiler is called Crankshaft added in 2010 in Microsoft Chakra.

00:19:08,120 --> 00:19:14,730
They call it a full just-in-time compiler, SquirrelFish has not one but 2 just-in-time

00:19:14,730 --> 00:19:25,490
compilers: DFG and FTL. It's faster than light and SpiderMonkey has IronMonkey which helps

00:19:25,490 --> 00:19:31,330
for better performance. Another thing that a lot of compilers, do they leave out that

00:19:31,330 --> 00:19:38,840
step of translating into bytecode. That gives you a little bit of performance.

00:19:38,840 --> 00:19:44,280
Let's look at this twice example again this time with the optimised compiler turned on

00:19:44,280 --> 00:19:49,870
so minus minus crankshaft, that's the default option. Earlier on I had it explicitly turned

00:19:49,870 --> 00:19:56,030
off. Let's have the optimisation on, the same examples as before, the polymorphic and monomorphic

00:19:56,030 --> 00:20:04,460
functions, let's see how fast that is. We went from 6 and 5 seconds down to 1 second,

00:20:04,460 --> 00:20:09,480
that's already good that's a 5, 6 times speedup but for the monomorphic function we see an

00:20:09,480 --> 00:20:19,059
even better speedup, we're down to 90 milliseconds now. So with modern engines if you by default

00:20:19,059 --> 00:20:25,050
use the optimised just-in-time compiler this really makes it faster.

00:20:25,050 --> 00:20:31,240
We can nicely see what's going on here, when we call the monomorphic example the faster

00:20:31,240 --> 00:20:39,340
one with minus minus trace optimisation, then you see we're optimising twice string because

00:20:39,340 --> 00:20:44,540
twice string is always called with a string, and the optimising compiler can now handle

00:20:44,540 --> 00:20:51,450
this and the same for twice int. The other example was the polymorphic function we don't

00:20:51,450 --> 00:21:01,780
see optimisation. They are for random and the polymorphic example crankshaft never really

00:21:01,780 --> 00:21:11,140
kicks in so we don't have massive speedup. There's another neat thing, if you ran twice

00:21:11,140 --> 00:21:17,470
always on an integer we always call it on 4 and any after 10,000 loop iterations we

00:21:17,470 --> 00:21:25,150
call it on a string. Then we see the following. So, since we're always calling it on an integer

00:21:25,150 --> 00:21:30,580
we can optimise it so optimising twice but then we get to 10,000 once we're running it

00:21:30,580 --> 00:21:36,030
just once with a string but that forces the compiler to throw out this optimised version

00:21:36,030 --> 00:21:40,250
of twice when it's called with an integer because all of a sudden you cannot use that

00:21:40,250 --> 00:21:44,040
machine code you generated.

00:21:44,040 --> 00:21:52,750
So take away, monomorphic functions are better than polymorphic functions. A few more things

00:21:52,750 --> 00:22:00,130
to avoid are try/catch statements, anything with eval, so V8 says I am not optimising

00:22:00,130 --> 00:22:07,680
that at all. SquirrelMonkey can do it a little bit but only the simple examples, avoid "with",

00:22:07,680 --> 00:22:15,910
don't do switch statements with more than 128 cases they can't be optimised or maintained.

00:22:15,910 --> 00:22:22,740
{laughter} If you have 4 in loops keep your keys local so don't forget var in front of

00:22:22,740 --> 00:22:30,640
it. If you use any of that that will really slow down your code because you can't use

00:22:30,640 --> 00:22:37,570
the optimising compiler. So here's an example of the Fibonacci series, Jennifer already

00:22:37,570 --> 00:22:43,890
mentioned the series earlier, it's a sequence of number that starts off with 11, you get

00:22:43,890 --> 00:22:50,680
the next number always by adding the last 2 numbers. So the algorithm is for the Fibonacci

00:22:50,680 --> 00:22:57,080
number of n, you just have to add the Fibonacci number of n-1 plus the Fibonacci number of

00:22:57,080 --> 00:23:04,960
n-2. We have this nice recursive algorithm, and I stop the recursion when n is smaller

00:23:04,960 --> 00:23:05,960
than 2

00:23:05,960 --> 00:23:11,550
because the first 2 entries are 1 and 1. So we wrote the algorithm but for some reason

00:23:11,550 --> 00:23:18,170
we left an eval 2+2 in there but it's after the statement, it really shouldn't change

00:23:18,170 --> 00:23:29,630
anything, but now when I run this code n=35 I can see that the compiler says optimisation.

00:23:29,630 --> 00:23:34,440
Reason function calls eval. Just because I had the online in there that didn't change

00:23:34,440 --> 00:23:39,480
anything about the algorithm, we can't optimise it, it takes about 4 seconds.

00:23:39,480 --> 00:23:46,299
Ok let's go ahead and fix that remove that line, it's after the return so it really doesn't

00:23:46,299 --> 00:23:57,270
change the code, let's run it again. So we went from 4 seconds to 200 milliseconds, and

00:23:57,270 --> 00:24:03,350
if you put the flag trace optimisation you see the Fibonacci function a nice recursive

00:24:03,350 --> 00:24:09,059
function always called on integers works perfectly fine with the optimising compiler so from

00:24:09,059 --> 00:24:21,120
4 seconds to 200 milliseconds that's a big speedup, but remember this year, don't prematurely

00:24:21,120 --> 00:24:26,190
optimise. Let's look at this example again.

00:24:26,190 --> 00:24:35,800
So for every n we have to add the Fibonacci number of n-1 and n-2. That means if we increase

00:24:35,800 --> 00:24:45,429
n by 1 we're doubling the effort so we have exponential complexity. No, that's not what

00:24:45,429 --> 00:24:52,330
we want. This example is good because we can really easily change the algorithm, so a simple

00:24:52,330 --> 00:24:56,860
trick is just remember the Fibonacci numbers you have already calculated by saving them

00:24:56,860 --> 00:25:06,100
in array a, when you have calculated one use that instead of recalculating it all the time.

00:25:06,100 --> 00:25:15,880
So we have linear complexity instead of exponential complexity, let's look at how fast this is.

00:25:15,880 --> 00:25:22,490
So the exponential example when we delete eval was down to 200 milliseconds or 180,

00:25:22,490 --> 00:25:35,740
the linear example I didn't even bother to take eval out, that only took 67 milliseconds.

00:25:35,740 --> 00:25:40,631
So remember when you want to do optimisation, don't blindly optimise for the compiler, make

00:25:40,631 --> 00:25:49,720
sure you know your bottle neck and fix your algorithms or whatever else is wrong.

00:25:49,720 --> 00:25:56,030
As you saw on this example, JavaScript is almost as fast as native C++ code which is

00:25:56,030 --> 00:26:02,850
amazing considering that JavaScript is a dynamically type language where you can do ahead-of-time

00:26:02,850 --> 00:26:10,490
complication, and since the compilers rely heavily on hidden classes, and inline caching

00:26:10,490 --> 00:26:19,040
your application works best if it's statically typed in nature. Try to avoid polymorphic

00:26:19,040 --> 00:26:25,230
function, make several functions that are monomorphic instead. Use structure initialisation

00:26:25,230 --> 00:26:30,250
that you make sure all your objects do have the same hidden class, if they are the same

00:26:30,250 --> 00:26:36,530
kind of object, also avoid things like eval so you don't stop the optimising compiler.

00:26:36,530 --> 00:26:42,070
Thank you, if you have any questions please come find me I look forward to talking to

00:26:42,070 --> 00:26:42,250

YouTube URL: https://www.youtube.com/watch?v=sloddfX9jLE


