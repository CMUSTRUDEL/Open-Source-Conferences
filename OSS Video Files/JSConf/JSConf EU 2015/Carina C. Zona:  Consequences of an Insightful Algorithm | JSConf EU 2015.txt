Title: Carina C. Zona:  Consequences of an Insightful Algorithm | JSConf EU 2015
Publication date: 2015-10-07
Playlist: JSConf EU 2015
Description: 
	We have ethical responsibilities when coding. We’re able to extract remarkably precise intuitions about an individual. But do we have a right to know what they didn’t consent to share, even when they willingly shared the data that leads us there? A major retailer’s data-driven marketing accidentally revealed to a teen’s family that she was pregnant. Eek.

What are our obligations to people who did not expect themselves to be so intimately known without sharing directly? How do we mitigate against unintended outcomes? For instance, an activity tracker carelessly revealed users’ sexual activity data to search engines. A social network’s algorithm accidentally triggered painful memories for grieving families who’d recently experienced death of their child and other loved ones.

We design software for humans. Balancing human needs and business specs can be tough. It’s crucial that we learn how to build in systematic empathy.

Intro music by @halfbyte
Captions: 
	00:00:27,890 --> 00:00:33,100
Wow! Okay, that was great. That's never happened before. (Laughing) can you do me a favor and

00:00:33,100 --> 00:00:38,470
bring me the water. I left that with you. All right, this is called county queenses

00:00:38,470 --> 00:00:43,710
of an insightful algorithm. The talk is for empathetic coding. We're going to delve into

00:00:43,710 --> 00:00:51,860
specific examples and and in that spirit I want to start with raccoon tent warning, I'm

00:00:51,860 --> 00:00:59,220
going to deal with a number of examples that are sensitive top Inc.s Greek PTSD, fertility,

00:00:59,220 --> 00:01:04,720
racial profiling, con receivation camps sexual history concept and assault. While these are

00:01:04,720 --> 00:01:09,280
not the major point of the talk, in about ten minutes we'll get into examples so you

00:01:09,280 --> 00:01:17,060
have a bit of time to decide if it's not right moment for you. Algorithms impose consequences

00:01:17,060 --> 00:01:25,039
on people all the time. We're able to extract remarkably precise insights about an individual,

00:01:25,039 --> 00:01:30,229
but do we have a right to know what they didn't consent to share? Even when they're willingly

00:01:30,229 --> 00:01:36,850
sharing the data that leads us there. How do we even mitigate against unintended consequences

00:01:36,850 --> 00:01:43,810
like these? Let's start by just thinking about what is an algorithm, defined step‑by‑step

00:01:43,810 --> 00:01:52,369
set of operations predictably arriving at an outcome. Predictivefully is pivotal here.

00:01:52,369 --> 00:01:58,229
We're talking ability algorithm of computer science, patterns of instructions that are

00:01:58,229 --> 00:02:07,689
articulate in the code or in formulas. But you could also think of algorithms as being

00:02:07,689 --> 00:02:11,700
something in just ordinary every day life, patterns of instructions that can be articulate

00:02:11,700 --> 00:02:25,441
in the all sorts of ways such as a map or a recipe or even C ‑‑ you can Lasly define

00:02:25,441 --> 00:02:32,530
it as Al governorrisms as fast trainable artificial gnarl networks. A technology been around for

00:02:32,530 --> 00:02:38,760
a while since '80s in theirretcal scale and confined to academia. In the past few year

00:02:38,760 --> 00:02:46,370
this is' been big advance in a variety of ways that make deep learning for extracting

00:02:46,370 --> 00:02:54,260
insights out of Big Data out of construction deployment. Opening up a lot offed possibilities.

00:02:54,260 --> 00:02:57,870
In particular it's an approach to building and training article official neural networks

00:02:57,870 --> 00:03:03,800
you can think of them as decision making black boxes. What does that mean, essentially we

00:03:03,800 --> 00:03:09,860
have some inputs, is this an array might be representing words concepts, Octobers, any

00:03:09,860 --> 00:03:15,550
number of things, execution running a series of functions repeatedly and layers that get

00:03:15,550 --> 00:03:21,440
more and more recease in their analysis, output our predictions of properties that might be

00:03:21,440 --> 00:03:26,120
useful for drawing intuition about future data set as long as they're similar to the

00:03:26,120 --> 00:03:31,129
original training data set. That allows the us to do some incredible things behavioral

00:03:31,129 --> 00:03:37,170
prediction, the facial identification, sentiment analysis, and things as extreme as self driving

00:03:37,170 --> 00:03:42,480
cars which Google is already using this stuff for a number of other companies are as well.

00:03:42,480 --> 00:03:46,610
So there's a lot of practical applications if that's already intriguing you you can check

00:03:46,610 --> 00:03:58,159
out C Convnetjs, it's a great opportunity to explore and experiment. So deep learning

00:03:58,159 --> 00:04:05,269
relies on and ANN's automated discovery. And it applies those discoveries to intuitions

00:04:05,269 --> 00:04:12,790
about future inputs. There's aquavit, every flaw or assumption in that training data set

00:04:12,790 --> 00:04:18,159
or original functions is going to have unrecognized influence on the Gordon and Maureen and the

00:04:18,159 --> 00:04:22,729
outcomes they generate. We're going to take a closer look at that in a minute, I want

00:04:22,729 --> 00:04:31,280
to give you a neat example of what an ANN is like. This is Mario. It's an ANN that teaches

00:04:31,280 --> 00:04:37,870
itself how to play super Mario world. It start was no clue whatsoever, all it does is manipulate

00:04:37,870 --> 00:04:45,290
numbers and notice that sometimes things happen. Over 24 hour period it learns movement and

00:04:45,290 --> 00:04:51,659
play via a purely self training session in which it engages in those hours of experimentation

00:04:51,659 --> 00:04:55,719
each time learn ago little bit more about the patterns and identifiable them and using

00:04:55,719 --> 00:05:03,710
them to make predictions for next layer. And speaking of games, let's play one. It looks

00:05:03,710 --> 00:05:11,290
a little bit like Bingo, it's called data mining fail. Insightful algorithms are full

00:05:11,290 --> 00:05:16,170
of pitfalls by looking at case studies it's an opportunity to explore some of the pitfalls

00:05:16,170 --> 00:05:20,600
on this particular board. So are you ready? Yeah.

00:05:20,600 --> 00:05:27,030
All right, here we go. In the retail sector the second trimester of pregnancy is known

00:05:27,030 --> 00:05:33,159
as the holy grail. The reason is that it is one of the few times in life where consumers

00:05:33,159 --> 00:05:39,380
spending habits product loyalties, brand loyalties are all kind of thrown up in the air, everything

00:05:39,380 --> 00:05:43,960
is subject for an opportunity to change. And for retailers this is an incredible moment,

00:05:43,960 --> 00:05:51,779
an opportunity to capture a consumer for potentially the rest of their lives and family. Target

00:05:51,779 --> 00:05:55,660
managed to come up several years ago with a predictive algorithm that was good at identifying

00:05:55,660 --> 00:06:02,880
customers that were in the second trimester. They started sending out add seculars to the

00:06:02,880 --> 00:06:11,120
targeted people full of stuff related pregnancy, babies, a funny thing happened one day a man

00:06:11,120 --> 00:06:17,930
came into the store and he was really angry. He's yelling at the manager, how dare you

00:06:17,930 --> 00:06:24,370
send this to my teenage daughter are you trying to tell her to have sex. Manager, you know,

00:06:24,370 --> 00:06:28,120
like he's not in charge of this, this is a huge national change, he apologizes, the guy

00:06:28,120 --> 00:06:33,990
goes home he comes back the following day and says, I you an apology, I talked to my

00:06:33,990 --> 00:06:41,830
daughter it turns out there were things I did not know and she is in fact pregnant.

00:06:41,830 --> 00:06:50,149
So, target was right. But they were also wrong. What they found were that a lot of women were

00:06:50,149 --> 00:06:55,180
not okay with having their privacy violate in the this way. And the way they describe

00:06:55,180 --> 00:07:03,580
od it is, some women React badly. Which is an interesting moral judgment on that. So,

00:07:03,580 --> 00:07:12,450
they came up with a change in plan. The new one was, now adds still go out to the same

00:07:12,450 --> 00:07:17,459
people, same adds except they're couched among other adds that seem completely unrelated

00:07:17,459 --> 00:07:21,960
so that their perception is that you know it's completely random bunch of adds that

00:07:21,960 --> 00:07:25,430
they just happen to get that have some things that are relevant to their life. And the reason

00:07:25,430 --> 00:07:31,010
they do this. This is a quote "as long as a pregnant women thinks she hasn't been spied

00:07:31,010 --> 00:07:40,700
on, as long as we don't spook her, it works.". (Laughing) so algorithms aren't just about

00:07:40,700 --> 00:07:45,900
the outputs, it's about how we use them and how we abuse them. This is one of the examples

00:07:45,900 --> 00:07:54,300
of ways that we can have all the math right and still be wrong. Shutterfully likewise

00:07:54,300 --> 00:07:59,770
was trying to predict things related pregnancy, in this case what they were predicting was

00:07:59,770 --> 00:08:05,279
recent childbirth. Congratulations on your new bundle of joy, time to write thank you

00:08:05,279 --> 00:08:11,620
cards to all the people that came to your lovely party. Some of the people who got these

00:08:11,620 --> 00:08:17,080
said, well, I haven't really been pregnant seeing as how I'm male. (Laughing) and others

00:08:17,080 --> 00:08:22,770
had different responses. Thanks, shutterfly for the congratulations on my new bundle of

00:08:22,770 --> 00:08:32,550
joy, I'm horribly infertile, but hey, I'm aadopting a kitten, so ... I lost a baby in

00:08:32,550 --> 00:08:39,650
November, who would have been due this week, it was like hitting a wall all over again.

00:08:39,650 --> 00:08:45,300
Shutterfly's response was the intent of the e‑mail was to target customers who recently

00:08:45,300 --> 00:08:52,930
had a baby. Well, yes, that's true. That's not an apology. That is a statement that that

00:08:52,930 --> 00:09:03,220
was what they wanted to do. They failed at it. False positives can be very meaningful.

00:09:03,220 --> 00:09:09,800
Few months ago mark Zuckerberg excitedly announced he's going to be a father soon. He wrote on

00:09:09,800 --> 00:09:16,300
Facebook about a series of miscarriages that he and his wife had felt with as a couple.

00:09:16,300 --> 00:09:20,950
This is part of what he had to say. He said, you feel so hopeful when you learn you're

00:09:20,950 --> 00:09:26,280
going to have a child. You start imagining who they'll become, and dreaming of hopes

00:09:26,280 --> 00:09:37,760
for their future. You start making plans. And then they're gone. It's a lonely experience.

00:09:37,760 --> 00:09:43,611
Facebook in review has ‑‑ Facebook year in review has been around for a while. This

00:09:43,611 --> 00:09:48,510
past year what they did was much more automated putting the post from the past year that they

00:09:48,510 --> 00:09:53,370
felt were particularly big and important and memorable for you and throwing them back to

00:09:53,370 --> 00:09:57,260
you at the end of the year to enjoy all over again. What they failed to take into account

00:09:57,260 --> 00:10:04,550
is our lives oconstantly changing in the course of a year many of us have job changes, relationship

00:10:04,550 --> 00:10:09,120
change, our life circumstances. All sorts of things and some of those mean that not

00:10:09,120 --> 00:10:16,400
every memory stays the joyous one that it once was.Er reck Meyer coined the term inadvertent

00:10:16,400 --> 00:10:22,790
algorithmic cruelty. The result of code that works in the overwhelming majority of cases

00:10:22,790 --> 00:10:30,200
but doesn't take other use cases into account. So why does he get to name it? Well because

00:10:30,200 --> 00:10:37,551
he's one of the people it happened to. This is a picture ofmy daughter who is dead. Who

00:10:37,551 --> 00:10:44,400
died this year. The year in review add keeps coming up in my feed, rotating through different

00:10:44,400 --> 00:10:50,520
fun and fabulous backgrounds as if celebrating her death. And there's no obvious way to stop

00:10:50,520 --> 00:10:59,000
it. Eric calls on us to increase awareness of and consideration of the failure modes,

00:10:59,000 --> 00:11:04,720
the educations, the worst case scenarios, and I hope that we can do some of that today

00:11:04,720 --> 00:11:09,490
and that you're going to carry it forward to others W that in mind hear's my first recommendation

00:11:09,490 --> 00:11:19,310
for all of us to think about. Be humble. We cannot actually Intuit inner state, emotions,

00:11:19,310 --> 00:11:33,230
private subjectivity. Not yet. Any way. (Scenario) when fitbit started out, it had a sex tracker.

00:11:33,230 --> 00:11:38,180
You know, quantified self, let's quantified everything, it counts as exercise. Right.

00:11:38,180 --> 00:11:54,650
There was a wrinkle, the wrinkle was that it defaulted to public. (Laughing) you all

00:11:54,650 --> 00:12:03,530
want a fitbit now don't you?! All right, so first of all I appreciate the vigorous effort.

00:12:03,530 --> 00:12:10,070
(Laughing) secondly, I also am a certified sex educator and I'm look oing at the four

00:12:10,070 --> 00:12:22,730
hours and I just tonight know whether to congratulate or be concerned. (Laughing) fitbit users were

00:12:22,730 --> 00:12:27,010
unwitting libraries sharing details of their sex lives with the whole world, it was on

00:12:27,010 --> 00:12:33,900
Google. That's because it was set public by default. And this is one of the things that

00:12:33,900 --> 00:12:39,310
we have to be thinking about an algorithm is not just about crunching numbers our patterns,

00:12:39,310 --> 00:12:46,300
prepredictable reproducible actions, this was really unthinking decision to not evaluate

00:12:46,300 --> 00:12:51,351
how different data sets might be differently treated, differently considered. Different

00:12:51,351 --> 00:12:55,300
amounts of privacy in our lives just because you want to share with all your friends your

00:12:55,300 --> 00:13:00,590
competition over how many steps you've taken or how many runs you've done doesn't mean

00:13:00,590 --> 00:13:02,726
that everything in your life is meant to be a public DOM petition as well. This was ‑‑

00:13:02,726 --> 00:13:16,790
public competition as this was a algorithm for UX it was really a fail here. Most of

00:13:16,790 --> 00:13:24,960
us use internal opt tools, it's mandatory right. Performance tuning, business metrics,

00:13:24,960 --> 00:13:30,640
a lot of something, Uber is called God view. If you're a gamer you're already suspecting

00:13:30,640 --> 00:13:37,150
what this implies. Uber did not limit access to admins and not restrict it to operational

00:13:37,150 --> 00:13:43,060
use alone, workers could freely identify had any passenger and monitor the person's movements

00:13:43,060 --> 00:13:51,880
even drivers were welcome to Bruce through Ubers customer trip records. Meanwhile managers

00:13:51,880 --> 00:13:57,930
felt free to abuse God view for non‑operational purposes such as stocking celebrity ride in

00:13:57,930 --> 00:14:04,190
real‑time showing it as party entertainment. To is show you how horrifying God view is,

00:14:04,190 --> 00:14:16,760
here's code expert. This is so nobly inappropriate. mean, seriously. Auto play true? Okay. And

00:14:16,760 --> 00:14:21,250
then of course there's the other choice. Background image, that's pretty telling as well as to

00:14:21,250 --> 00:14:27,660
what their intent was for this. The research group at dating site okay cupid used to Blog

00:14:27,660 --> 00:14:34,050
all the time things they were learning about aggregating their data and Blog showing insights

00:14:34,050 --> 00:14:40,940
into simple ways that okay cupid users could use that data site well to date better. Uber

00:14:40,940 --> 00:14:46,640
used to Blog about its day to too. There's a critical difference in that Uber's approach

00:14:46,640 --> 00:14:53,880
to it was not about improving customers' experience of a ride service, it was about invading people's

00:14:53,880 --> 00:14:59,990
privacy for the sake of judging and shaming and stalking them. These are not predictable

00:14:59,990 --> 00:15:08,820
consequences of signing up for an account to take a ride. Galling add words is an interesting

00:15:08,820 --> 00:15:15,330
study done at Harvard a few years ago, what they did was took two sets of names, one that

00:15:15,330 --> 00:15:20,151
is strongly correlated with black people and one that's correlated with white people. So

00:15:20,151 --> 00:15:24,581
for instance first name like Latonya would be something that's highly correlated lated

00:15:24,581 --> 00:15:31,070
with black women and something like Jill would be highly correlated with a white woman. And

00:15:31,070 --> 00:15:34,850
then what they did was they matched up the first names a with the real last names of

00:15:34,850 --> 00:15:41,830
professors and did some searches on add words for those names. And what they found is that

00:15:41,830 --> 00:15:48,200
a black identifying name was 25 percent more likely to result in the an add that implied

00:15:48,200 --> 00:15:58,360
that that person had an arrest record. So for example adds like these. And I think it's

00:15:58,360 --> 00:16:04,220
important to note here, Ad word algorithm is focused on predicting what we'll click

00:16:04,220 --> 00:16:11,230
on. That's it. It's not interested in whether anyone was arrested. That's not it's point,

00:16:11,230 --> 00:16:16,640
the real world isn't important. It's whole job is to figure out what motivates us to

00:16:16,640 --> 00:16:22,610
click. Which Ad template we're going to respond to. Based on what it knows about us individually

00:16:22,610 --> 00:16:28,149
and collectively what it knows about other users before us, what we're see inning these

00:16:28,149 --> 00:16:35,370
Ads is our collective bias at work and being reflected back to us and then being reinforced

00:16:35,370 --> 00:16:42,000
every time it's presented again and we click on it again. This is a feedback loop. Data

00:16:42,000 --> 00:16:48,750
is generated by people. It's not objective. It's constrained by our tunnel vision it replicates

00:16:48,750 --> 00:17:03,080
our flaws, it echos our preconceptions. Image recognition is hard. And you remember it wasn't

00:17:03,080 --> 00:17:12,089
so long ago we had things like this. (Laughing) I photo helpfully helpfully detecting faces

00:17:12,089 --> 00:17:19,659
in baked goods. It's funny, sure. It's a harmless mistake, it's a false positive that's easy

00:17:19,659 --> 00:17:26,100
to chuckle at. Some are less funny, such as in this next photo. Flicker classified it

00:17:26,100 --> 00:17:33,580
as essentially children's playground equipment. For those who are not familiar that's ash

00:17:33,580 --> 00:17:45,380
wits. This was in May ‑‑ ‑‑ a month later Google photos mistagged this person

00:17:45,380 --> 00:17:55,340
as an animal. Sorry that was actually flicker again, apologize, it also tagged him as an

00:17:55,340 --> 00:18:02,059
ape. Google photos a month after that tagged someone as a gorilla. You'll notice a common

00:18:02,059 --> 00:18:09,180
theme here, black skin. How does that happen? Well, maybe some of it is just like those

00:18:09,180 --> 00:18:14,721
Ad words our bias being reflected back, there's also other answers for one of those you're

00:18:14,721 --> 00:18:23,560
going to have go all the way back to the 1950s when color film stock was first being developed,

00:18:23,560 --> 00:18:30,830
created it was optimized for white skin to get as much detame out of white skin as possible.

00:18:30,830 --> 00:18:37,190
And for decades labs were given these, they were scall called Shirly cards they were used

00:18:37,190 --> 00:18:45,400
to calibrate developers to make sure they were accurately producing details and colors,

00:18:45,400 --> 00:18:51,080
for decades every film stock was designed to optimize for white skin and to ignore black

00:18:51,080 --> 00:18:57,520
skin. And black skin, to this day still has a very hard time getting a nice accurate well

00:18:57,520 --> 00:19:03,420
exposed photo. When we started moving to digital sensors the obvious thing to do is to replicate

00:19:03,420 --> 00:19:11,059
the experience people were already having. If we have sensors that are radically different

00:19:11,059 --> 00:19:16,790
we would all complain about how terrible and faulty our cameras were. What we have here

00:19:16,790 --> 00:19:25,420
is repeatuation is racial an mouse from thed 50s, we have decades of the data sets that

00:19:25,420 --> 00:19:31,350
are contaminated with noise. And so when we have these kind of misclassifications it's

00:19:31,350 --> 00:19:37,651
easy to look at it and say, Hmm mistake or Hmm racism. It goes deeper than that, these

00:19:37,651 --> 00:19:42,390
are hard problems to solve, Big Data if we throw enough data at it any problem can be

00:19:42,390 --> 00:19:45,780
corrected is what we think. Where are you going to find the data that is an easy corrector

00:19:45,780 --> 00:19:58,750
for this? A firm is a rather unusual credit lending company. They focus on lending for

00:19:58,750 --> 00:20:05,100
certain small consumer purchases and make a rather interesting set of criteria, the

00:20:05,100 --> 00:20:10,050
basis for lending. Essentially just provide a few thing, name, e‑mail, mobile phone

00:20:10,050 --> 00:20:16,410
number, birthday and last four digit of identification number and then from there it starts evaluating

00:20:16,410 --> 00:20:21,520
behavioral factors even were you given that much. Things like how long you take to fill

00:20:21,520 --> 00:20:26,880
in that little form. What time ‑‑ how much time it takes you to remember stuff.

00:20:26,880 --> 00:20:30,260
And then if it needs more information it goes out and looks at your social accounts including

00:20:30,260 --> 00:20:38,310
GitHub. Which is already starting to replicate privilege in the real world because only 2 percent

00:20:38,310 --> 00:20:42,840
of women ‑‑ sorry only 2 percent of Open Source cent toes are women. So that means

00:20:42,840 --> 00:20:48,840
GitHub is inevitably going to be biased towards finding men, and if you're making the criteria

00:20:48,840 --> 00:20:54,320
for lending participation on a site like GitHub automatically it's always going to be biased

00:20:54,320 --> 00:21:00,410
against a lot of women. Think about how much time it takes to remember something. Who might

00:21:00,410 --> 00:21:07,540
need more time? Someone with, say a cognitive processing disorder. Someone who's older.

00:21:07,540 --> 00:21:15,390
All sorts of biases built into the supposedly objective algorithm. UK researchers my shopping

00:21:15,390 --> 00:21:22,120
cart abandonment rate is high because my toddler grabs my visa card and runs off screaming,

00:21:22,120 --> 00:21:28,020
mine, mine, mine. All right, sometimes it's not that we're inattentive because we somehow

00:21:28,020 --> 00:21:33,390
are a bad credit risk, sometimes we're inattentive because other things distract us. This is

00:21:33,390 --> 00:21:38,540
a person who's clock is invisibly ticking and losing opportunities that are financial

00:21:38,540 --> 00:21:44,790
as a result of it. A firm analyzes those social media accounts, they're not the only one,

00:21:44,790 --> 00:21:49,680
there are a number of other companies using similar models. In 2012, in fact Germany's

00:21:49,680 --> 00:21:58,929
biggest credit rating agency considered evaluating Facebook relationships. More recently Facebook

00:21:58,929 --> 00:22:05,020
pushes further down that line making credit decisions about you based on the unrelated

00:22:05,020 --> 00:22:17,340
credit history of your Facebook friends. Okay. What? So are they unaware that friends doesn't

00:22:17,340 --> 00:22:24,990
equal Facebook friend? Like we got to tell Facebook about this. Here's an algorithm with

00:22:24,990 --> 00:22:31,010
potential to deeply intrude on and alter personal relationships. To just to prevent that algorithm

00:22:31,010 --> 00:22:39,910
from financially shaming and pun ishing them. This is a huge consequence. Data is not objective.

00:22:39,910 --> 00:22:47,950
It always has bias, it's inherent at minimum from how it was collected and interpreted.

00:22:47,950 --> 00:22:56,250
A firm says that it's algorithm uses 70,000 factors to reach it's conclusions. They say

00:22:56,250 --> 00:23:00,620
they don't even know what all of them are. All right, so how do we know how many have

00:23:00,620 --> 00:23:05,300
potential for discriminatory outcomes then. How would anyone of them now. If thaw don't

00:23:05,300 --> 00:23:13,030
know how would the consumer do anything about mistakes? Forget about bias, just simple error.

00:23:13,030 --> 00:23:18,841
Rationals for the algorithm can only be seen from inside that black box. So let's take

00:23:18,841 --> 00:23:29,780
a look at it. I took a photo from inside a really, really black box. (Laughing) that's

00:23:29,780 --> 00:23:35,950
what we can see. Making lending desessions inside a black box isn't a radical new business

00:23:35,950 --> 00:23:44,650
model, it's a regression. What is disrupting is oversight and regulation. Right now we're

00:23:44,650 --> 00:23:53,559
in an arms race. Facebook, Google, happening, Microsoft, Yahoo. Baidu, IBM, AT&T Twitter,

00:23:53,559 --> 00:23:59,430
so many companies are making big bets on deep learning, some are already deploying, for

00:23:59,430 --> 00:24:05,940
the moment, quality varies but we have to remember that deep learning is all about iteratively

00:24:05,940 --> 00:24:13,910
drawing intuitions at extremely fine grained levels. And what that means is that they're

00:24:13,910 --> 00:24:18,730
continuously getting more precise in their correctness but also more damaging in their

00:24:18,730 --> 00:24:25,120
wrongness. And that's a dilemma that we have to take seriously as developers. Because underlying

00:24:25,120 --> 00:24:31,370
actions, influence, outcomes and influence consequences. They have underlying assumptions

00:24:31,370 --> 00:24:36,660
about meaning, about accuracy, about the world in which data has been generated, about how

00:24:36,660 --> 00:24:47,210
code should assign meaning to to data. We care about getting this stuff right (To Data)

00:24:47,210 --> 00:24:52,860
the question is how do we flip the paradigm? Well, we can do a few things like taking some

00:24:52,860 --> 00:24:58,200
lessons from professional ethicists because it turns out that's a thing. It turns out

00:24:58,200 --> 00:25:03,440
our profession has professional ethicists, who knew. These are a few that I've adapted

00:25:03,440 --> 00:25:10,280
from the association from computer machinery and a few other sources. We need to consider

00:25:10,280 --> 00:25:17,049
decisions impact, potential impacts on others. For instance how might a false positive affect

00:25:17,049 --> 00:25:23,080
someone, like those shutterfly customers, how might a false negative affect someone,

00:25:23,080 --> 00:25:27,630
have we built in resource for someone to easily get our conclusions corrected when we're wrong

00:25:27,630 --> 00:25:34,190
about them. We need to be able to project the likelihood of consequences to others and

00:25:34,190 --> 00:25:40,030
to minimize negative consequences to others, and yes, I keep hammering on two others, we're

00:25:40,030 --> 00:25:45,730
pretty good at taking care of ourselves. We have to be honest and trustworthy. Not just

00:25:45,730 --> 00:25:48,690
because those are the right things to do, but because we need to be able to lean on

00:25:48,690 --> 00:25:53,950
it when we make mistakes, we need to be able to buyback trust because we've earn it, because

00:25:53,950 --> 00:25:59,840
we can say maya cull pa without that destroying us. We need to provide others with the full

00:25:59,840 --> 00:26:06,400
disclosure of limitations and call attention to signs of risk of harm to them. And here's

00:26:06,400 --> 00:26:12,309
a big one, we have to be visionary about counterrerring bias, we have to be visionaries about creating

00:26:12,309 --> 00:26:20,581
more than one way to counteract it. To counteract bias data, bias analysis, bias impacts. And

00:26:20,581 --> 00:26:27,740
here's the really big one. We have to be able to anticipate diverse ways to screw up. When

00:26:27,740 --> 00:26:33,510
teams are charged with defining data collections use and anal circumstance any time those are

00:26:33,510 --> 00:26:37,850
less diverse than the intended user base, we're going to keep on failing them, just

00:26:37,850 --> 00:26:47,120
like this. We have to have decision making authority in the hands of highly diverse teams,

00:26:47,120 --> 00:26:54,170
highly. What does that mean? Culture fit is the antithesis of diversity, it's superficial

00:26:54,170 --> 00:27:00,120
variations being allowed to exist as long as their unique perspective is sure pressed

00:27:00,120 --> 00:27:07,800
the purpose of culture fit is to avoid disruption of group think. UniI did mixal variety is

00:27:07,800 --> 00:27:15,200
not diversity either. Diversity is widely varied on as many areas as possible. Different

00:27:15,200 --> 00:27:21,880
assumptions, different experiences, until you get to the point where there's no such

00:27:21,880 --> 00:27:30,050
thing as a majority you can't find it. We need to cultivate, inform consent. What that

00:27:30,050 --> 00:27:35,820
means is we ask for permission with the default being no and explain the consequences of a

00:27:35,820 --> 00:27:40,900
yes. Focus on the many people that eerily want to share themselves and enthusiastically

00:27:40,900 --> 00:27:49,429
give consent and want to be served better by that. And we have the audit outcomes constantly,

00:27:49,429 --> 00:27:53,760
the reason is going back to the black box, if we can't be sure what's happening inside

00:27:53,760 --> 00:27:59,920
of it. We need to be able to look at the outcomes. This is used a lot in checking for housing

00:27:59,920 --> 00:28:04,410
discrimination and job discrimination. So you put in two inputs that are exactly the

00:28:04,410 --> 00:28:10,080
same on every criteria but one. One that should not have any bias introduced and if the outcome

00:28:10,080 --> 00:28:15,570
on those divers at all, we know we have an problem with the algorithm. So this is something

00:28:15,570 --> 00:28:19,290
we constantly have to be looking for. For instance when Google photos made that mistake

00:28:19,290 --> 00:28:24,630
with classifying people as gorillas, as soon as they saw that problem on flicker a month

00:28:24,630 --> 00:28:37,350
earlier they should of been dog auditing, do we have a the same problem? And why? Because

00:28:37,350 --> 00:28:46,730
photo of a big black box. And that's why we also have to commit to data transparency and

00:28:46,730 --> 00:28:53,870
algorithmic transparency. And I do mean both because I recognize that these are hard decisions

00:28:53,870 --> 00:28:58,940
to have internally. They truly are. But I also think that, you know, it wasn't that

00:28:58,940 --> 00:29:03,460
long ago that we were fighting for legitimacy of Open Source in our professional Toolkit.

00:29:03,460 --> 00:29:09,820
We push back, we were right, we're profession falls, we won because we know what we're doing

00:29:09,820 --> 00:29:14,910
and we made a good argument. We know that transparency is crucial for drawing insights

00:29:14,910 --> 00:29:19,970
that are general when and useful. So argue for increasing transparency because it's for

00:29:19,970 --> 00:29:27,300
a better product. Cleaner features, fewer bugs stronger tests happier users public trust.

00:29:27,300 --> 00:29:34,100
That's the argument. Because we want to build stuff that matters. We're hired for more than

00:29:34,100 --> 00:29:39,720
just to write code, we're hired as professionals that apply expertise and judgment about how

00:29:39,720 --> 00:29:45,150
to solve problems. That's who we really are. We're not code monkeys, we're people that

00:29:45,150 --> 00:29:49,610
think about how to solve problems. Our role is to be opinionuated about how to make code

00:29:49,610 --> 00:29:56,640
serve the problemtion based well. We can advocate. When we're asked to write code that presumed

00:29:56,640 --> 00:30:01,950
to Intuit people as internal life and act on those assumptions as professionals we have

00:30:01,950 --> 00:30:08,150
to be people's proxies, we have to be their advocates, say no on their behalf to using

00:30:08,150 --> 00:30:15,280
their data in ways that they have not enthusiastically and knowingly consented to. Saying no to uncritically

00:30:15,280 --> 00:30:21,470
reproducing systems that were based to begin with. Say no to writing code that imposes

00:30:21,470 --> 00:30:30,430
on authorized consequences on to their lives. In short, refuse to play along. Thank you.

00:30:30,430 --> 00:30:30,970

YouTube URL: https://www.youtube.com/watch?v=znwWYR1mzzw


