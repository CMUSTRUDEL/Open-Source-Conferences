Title: Jenna Zeigen: On How Your Brain is Conspiring Against You Making Good Software - JSConf Iceland 2016
Publication date: 2016-09-20
Playlist: JSConf Iceland 2016
Description: 
	If there's anything that decades of psychology research have shown us, it's that human cognition is full of bias and fallacy. Even smart software engineers are not immune to being humans. In fact, there's so many things keeping us from being the best developers we could be, preventing us from planning our work effectively to assembling the best teams to being productive in that open office.
Captions: 
	00:00:14,330 --> 00:00:20,099
hi everyone and welcome to my talk on

00:00:17,760 --> 00:00:24,300
how your brain is conspiring against you

00:00:20,099 --> 00:00:25,980
making good software so uh as was

00:00:24,300 --> 00:00:28,800
already mentioned I'm an engineering

00:00:25,980 --> 00:00:30,359
manager at digital ocean which is a new

00:00:28,800 --> 00:00:32,160
york-based company that tries to take

00:00:30,359 --> 00:00:36,449
the cognitive load out of managing your

00:00:32,160 --> 00:00:40,760
infrastructure i'm also an organizer at

00:00:36,449 --> 00:00:44,720
a vampire j/s which is a jazz family

00:00:40,760 --> 00:00:46,890
conference in manhattan and that's uh

00:00:44,720 --> 00:00:50,519
organizing a conference is a lot of work

00:00:46,890 --> 00:00:52,860
a lot of things go into it so just take

00:00:50,519 --> 00:00:54,180
a few minutes or a few seconds to give

00:00:52,860 --> 00:00:58,820
round of applause to all the organizers

00:00:54,180 --> 00:00:58,820
and volunteers cuz yeah they're awesome

00:01:02,629 --> 00:01:08,100
so yeah I'm also zeigen vector on

00:01:05,820 --> 00:01:10,140
Twitter so if you want to tweet

00:01:08,100 --> 00:01:11,909
questions at me while I'm up here try to

00:01:10,140 --> 00:01:14,159
answer them as best as I can once I'm

00:01:11,909 --> 00:01:16,350
not up here anymore I've also tweeted

00:01:14,159 --> 00:01:19,500
out the link to my slides that interests

00:01:16,350 --> 00:01:22,140
you right now so before I was a

00:01:19,500 --> 00:01:23,939
JavaScript engineer and way before I was

00:01:22,140 --> 00:01:25,979
an engineering manager I studied

00:01:23,939 --> 00:01:29,159
cognitive science in University

00:01:25,979 --> 00:01:31,619
cognitive science of talks about a lot

00:01:29,159 --> 00:01:34,259
of things like thinking decision-making

00:01:31,619 --> 00:01:36,299
attention vision consciousness and my

00:01:34,259 --> 00:01:38,369
personal specialty human language

00:01:36,299 --> 00:01:40,679
processing but if you were to ask you to

00:01:38,369 --> 00:01:43,409
sum up what I learned in all those four

00:01:40,679 --> 00:01:46,109
years into one concise sentence it would

00:01:43,409 --> 00:01:48,240
be that humans are predictably

00:01:46,109 --> 00:01:49,979
irrational this predictably irrational

00:01:48,240 --> 00:01:52,259
phrase was coined by a psychologist

00:01:49,979 --> 00:01:54,240
named Dan Ariely who in fact has a book

00:01:52,259 --> 00:01:57,119
by that title but what does this mean

00:01:54,240 --> 00:01:58,920
that we are predictably irrational it

00:01:57,119 --> 00:02:01,200
means that we employ predictable tricks

00:01:58,920 --> 00:02:03,210
patterns and shortcuts that allow us to

00:02:01,200 --> 00:02:05,819
make more efficient decisions to have

00:02:03,210 --> 00:02:08,640
what's called cognitive economy but this

00:02:05,819 --> 00:02:10,979
fast thinking often causes us to deviate

00:02:08,640 --> 00:02:13,740
from rationality in our judgments to

00:02:10,979 --> 00:02:15,390
make what to other eminent scientists in

00:02:13,740 --> 00:02:17,610
the field Amos Tversky and Daniel

00:02:15,390 --> 00:02:20,790
Kahneman have have called severe and

00:02:17,610 --> 00:02:22,709
systematic errors and these errors are

00:02:20,790 --> 00:02:23,370
called cognitive biases which is a

00:02:22,709 --> 00:02:26,430
phrase that you

00:02:23,370 --> 00:02:29,099
might have heard of a few times but this

00:02:26,430 --> 00:02:31,500
is a JavaScript conference and not a

00:02:29,099 --> 00:02:32,790
cognitive science conference so what

00:02:31,500 --> 00:02:35,640
does this have to do with code right

00:02:32,790 --> 00:02:38,659
well my job as an engineering manager is

00:02:35,640 --> 00:02:42,989
to make sure that my team which is

00:02:38,659 --> 00:02:45,720
comprised of humans is happy getting

00:02:42,989 --> 00:02:48,930
their work done getting along with each

00:02:45,720 --> 00:02:51,510
other growing as individuals and as

00:02:48,930 --> 00:02:53,599
programmers and it's also my job to grow

00:02:51,510 --> 00:02:57,209
my team to make it the most successful

00:02:53,599 --> 00:03:00,629
my team is like my product so I've

00:02:57,209 --> 00:03:03,090
started to think back to my cognitive

00:03:00,629 --> 00:03:04,829
science years and I'm trying to figure

00:03:03,090 --> 00:03:07,950
out especially how these cognitive

00:03:04,829 --> 00:03:10,409
biases and other quicksilver mushy human

00:03:07,950 --> 00:03:12,810
brains affect us as we're making

00:03:10,409 --> 00:03:15,209
software in the environment that were

00:03:12,810 --> 00:03:17,730
making the software in because software

00:03:15,209 --> 00:03:21,480
isn't just about code it's also about

00:03:17,730 --> 00:03:25,349
humans software isn't software if

00:03:21,480 --> 00:03:26,970
there's no one using it I think so yeah

00:03:25,349 --> 00:03:29,459
I try to think about these cognitive

00:03:26,970 --> 00:03:31,680
biases because even the most calculating

00:03:29,459 --> 00:03:33,419
software engineers is a human and

00:03:31,680 --> 00:03:35,579
therefore subject to these cognitive

00:03:33,419 --> 00:03:36,630
biases no matter how much they think

00:03:35,579 --> 00:03:40,440
that they are immune to the

00:03:36,630 --> 00:03:44,910
rationalities of humankind so I'm going

00:03:40,440 --> 00:03:47,579
to talk about these things and first I'm

00:03:44,910 --> 00:03:49,530
going to talk about logic so one of the

00:03:47,579 --> 00:03:52,859
fundamental underpinnings of programming

00:03:49,530 --> 00:03:54,510
is logic in programming we have f

00:03:52,859 --> 00:03:57,060
statements all statements bullion's

00:03:54,510 --> 00:04:00,150
ternary operators but computers can only

00:03:57,060 --> 00:04:02,280
do what humans tell them to do and it

00:04:00,150 --> 00:04:05,730
turns out that humans are not very good

00:04:02,280 --> 00:04:08,040
logical thinkers what are the tools that

00:04:05,730 --> 00:04:10,440
psychologists use to test this is called

00:04:08,040 --> 00:04:13,620
the categorical syllogism we have one

00:04:10,440 --> 00:04:15,540
right up here on the screen so a

00:04:13,620 --> 00:04:17,760
categorical syllogism is a logical

00:04:15,540 --> 00:04:19,889
argument with two premises and a

00:04:17,760 --> 00:04:22,349
conclusion and scientists will ask

00:04:19,889 --> 00:04:24,570
humans hey does this conclusion follow

00:04:22,349 --> 00:04:26,940
from the premises so I'll give you a few

00:04:24,570 --> 00:04:29,240
seconds to answer that question for

00:04:26,940 --> 00:04:29,240
yourself

00:04:31,630 --> 00:04:36,250
so when this was presented to

00:04:34,240 --> 00:04:38,800
participants eighty-one percent of them

00:04:36,250 --> 00:04:41,290
said yep the conclusion follows from the

00:04:38,800 --> 00:04:43,600
premises ten percent said something else

00:04:41,290 --> 00:04:47,820
wrong and only nine percent of people

00:04:43,600 --> 00:04:50,260
got it right um so that's not great and

00:04:47,820 --> 00:04:52,570
reasoning about complex if-else

00:04:50,260 --> 00:04:55,270
statements isn't much better so a

00:04:52,570 --> 00:04:57,700
psychologist damn place in came up with

00:04:55,270 --> 00:05:00,130
the task involving four cards in which

00:04:57,700 --> 00:05:01,540
every card has a letter on one side and

00:05:00,130 --> 00:05:03,460
a number on the other side and

00:05:01,540 --> 00:05:05,830
participants were asked to evaluate this

00:05:03,460 --> 00:05:08,020
rule if a card has a vowel on one side

00:05:05,830 --> 00:05:10,390
it has an even number on the other and

00:05:08,020 --> 00:05:11,920
then asked the participants what cards

00:05:10,390 --> 00:05:15,970
need to be flipped over to make sure

00:05:11,920 --> 00:05:21,310
that this real checks out so formulate

00:05:15,970 --> 00:05:24,160
your own answer so only four percent of

00:05:21,310 --> 00:05:27,280
people got the right answer four percent

00:05:24,160 --> 00:05:29,470
which is to flip only the a and the

00:05:27,280 --> 00:05:31,150
seven over if you're stumped about why

00:05:29,470 --> 00:05:33,250
this is right remember that the rule

00:05:31,150 --> 00:05:36,940
says nothing about what has to be on the

00:05:33,250 --> 00:05:39,370
other side of consonants so what does

00:05:36,940 --> 00:05:41,380
this mean for testing our code properly

00:05:39,370 --> 00:05:44,650
right if we can't come up with questions

00:05:41,380 --> 00:05:46,720
to test rules that we hold to be true

00:05:44,650 --> 00:05:48,640
how are you going to test our code and

00:05:46,720 --> 00:05:52,300
what type of bugs are gonna arise from

00:05:48,640 --> 00:05:54,330
this right but luckily there's some hope

00:05:52,300 --> 00:05:57,100
which is that we get much better about

00:05:54,330 --> 00:05:58,990
reasoning about concrete examples so

00:05:57,100 --> 00:06:01,930
some other scientists adapted way since

00:05:58,990 --> 00:06:04,510
for card task to test a concrete rule

00:06:01,930 --> 00:06:06,850
which was that if a person is drinking

00:06:04,510 --> 00:06:08,650
beer they must be over 19 years old and

00:06:06,850 --> 00:06:10,480
in this case seventy-three percent of

00:06:08,650 --> 00:06:15,520
people got the right answer so an

00:06:10,480 --> 00:06:18,370
improvement but like still not great so

00:06:15,520 --> 00:06:20,950
when we inevitably make a mistake in our

00:06:18,370 --> 00:06:23,770
code logical or otherwise because after

00:06:20,950 --> 00:06:25,390
all we are humans we're gonna have to

00:06:23,770 --> 00:06:27,250
end up debugging our code and as

00:06:25,390 --> 00:06:29,260
computer science elaborate e brian

00:06:27,250 --> 00:06:31,360
kernighan said debugging is twice as

00:06:29,260 --> 00:06:34,360
hard as writing the program in the first

00:06:31,360 --> 00:06:37,030
place so why exactly is debugging so

00:06:34,360 --> 00:06:39,820
hard I think there's some cognitive bias

00:06:37,030 --> 00:06:42,370
ezel will help or not helping us to

00:06:39,820 --> 00:06:44,900
debug well one of them is a confirmation

00:06:42,370 --> 00:06:47,419
bias which is that we have a tendency

00:06:44,900 --> 00:06:49,100
to search for interpret favor and

00:06:47,419 --> 00:06:51,380
remember information in a way that

00:06:49,100 --> 00:06:53,060
confirms our pre-existing beliefs or

00:06:51,380 --> 00:06:55,460
hypotheses we also give

00:06:53,060 --> 00:06:58,490
disproportionately less consideration to

00:06:55,460 --> 00:07:01,400
alternative possibilities so an

00:06:58,490 --> 00:07:06,410
experiment to test this Wayson did it

00:07:01,400 --> 00:07:08,270
again and so for instance they gave a

00:07:06,410 --> 00:07:11,000
trio participants a trade-off numbers

00:07:08,270 --> 00:07:12,560
like 246 and told the participants that

00:07:11,000 --> 00:07:14,479
the trio couldn't form to a specific

00:07:12,560 --> 00:07:16,430
rule and their job was to figure out

00:07:14,479 --> 00:07:18,740
what the rule was by asking if other

00:07:16,430 --> 00:07:20,449
trios also informed the rule in this

00:07:18,740 --> 00:07:21,979
case is actually pretty simple that the

00:07:20,449 --> 00:07:24,320
numbers just have to be in a sending

00:07:21,979 --> 00:07:26,000
order but people ended up having trouble

00:07:24,320 --> 00:07:27,560
especially when they asked only

00:07:26,000 --> 00:07:29,539
confirming questions and not

00:07:27,560 --> 00:07:31,940
disconfirming questions if you were to

00:07:29,539 --> 00:07:36,830
say like oh I got the rule it's like

00:07:31,940 --> 00:07:38,000
does 6 8 10 apply like yep you would

00:07:36,830 --> 00:07:39,320
probably just keep going down the same

00:07:38,000 --> 00:07:42,050
path but if you're like I'm gonna try

00:07:39,320 --> 00:07:44,180
something different one to a million and

00:07:42,050 --> 00:07:46,039
they said yeah that also it works then

00:07:44,180 --> 00:07:50,930
you have more insight into what the rule

00:07:46,039 --> 00:07:52,910
actually is but we tend to scrutinize

00:07:50,930 --> 00:07:55,639
disconfirming evidence we try to find

00:07:52,910 --> 00:07:58,490
flaws or ambiguities in in this evidence

00:07:55,639 --> 00:08:00,800
blaming say when you're like when it

00:07:58,490 --> 00:08:02,449
relates to code a bug in the library or

00:08:00,800 --> 00:08:04,280
it was someone else's code that has the

00:08:02,449 --> 00:08:06,710
bug in it and even when the facts are

00:08:04,280 --> 00:08:08,630
clearly stacked up against the belief

00:08:06,710 --> 00:08:10,789
that we are holding true we often throw

00:08:08,630 --> 00:08:13,729
out those disconfirming facts especially

00:08:10,789 --> 00:08:16,360
when the issue is emotionally charged

00:08:13,729 --> 00:08:19,610
such as when your code has a bug in it

00:08:16,360 --> 00:08:22,729
we also tend to be rigid in how we

00:08:19,610 --> 00:08:24,440
approach problems so if you write the

00:08:22,729 --> 00:08:27,199
wrote the code you're going to be rigid

00:08:24,440 --> 00:08:28,610
and stinking thinking through the the

00:08:27,199 --> 00:08:30,440
problem and it's going to be hard to

00:08:28,610 --> 00:08:32,479
think outside the box and squash the bug

00:08:30,440 --> 00:08:35,000
especially if it's on a conceptual level

00:08:32,479 --> 00:08:36,650
I know I'm guilty of substituting the

00:08:35,000 --> 00:08:38,690
single equals four triple equals and

00:08:36,650 --> 00:08:40,580
even that type of bug is hard to find so

00:08:38,690 --> 00:08:43,820
when the bug is on a more conceptual

00:08:40,580 --> 00:08:46,610
level it's that much harder to find and

00:08:43,820 --> 00:08:48,770
squash and we even sometimes tend to

00:08:46,610 --> 00:08:51,470
block problem solutions based on past

00:08:48,770 --> 00:08:53,060
experiences because we have an inability

00:08:51,470 --> 00:08:55,459
to see problems from a fresh perspective

00:08:53,060 --> 00:08:57,900
I think a great way to get around this

00:08:55,459 --> 00:09:00,840
is to pair on squishing bugs so

00:08:57,900 --> 00:09:04,680
you get that extra set of eyes when

00:09:00,840 --> 00:09:06,420
you're particularly stuck another thing

00:09:04,680 --> 00:09:08,520
that's so frustrating about debugging is

00:09:06,420 --> 00:09:10,410
that sometimes it feels like there's no

00:09:08,520 --> 00:09:12,030
end in sight but you're just searching

00:09:10,410 --> 00:09:14,670
and searching like why is this not

00:09:12,030 --> 00:09:16,950
working it's frustrating then there's no

00:09:14,670 --> 00:09:18,840
light at the end of the tunnel and this

00:09:16,950 --> 00:09:20,850
is because we often have no idea when

00:09:18,840 --> 00:09:22,830
we're going to solve a problem even the

00:09:20,850 --> 00:09:25,500
30 seconds before we end up solving it

00:09:22,830 --> 00:09:27,450
and so to test the scientists give

00:09:25,500 --> 00:09:29,610
subjects in sight problems kind of like

00:09:27,450 --> 00:09:33,270
word problem brain teaser type things

00:09:29,610 --> 00:09:35,550
and as they solve the problem the

00:09:33,270 --> 00:09:37,470
scientists ask them as the participants

00:09:35,550 --> 00:09:38,970
to rate their progress in terms of

00:09:37,470 --> 00:09:41,460
warmth so how close do you think you are

00:09:38,970 --> 00:09:45,300
solving this initially they gave ratings

00:09:41,460 --> 00:09:47,550
of one to two and then their warmest

00:09:45,300 --> 00:09:49,380
reading abruptly spiked when they got an

00:09:47,550 --> 00:09:53,280
answer whether or not it was correct or

00:09:49,380 --> 00:09:54,600
incorrect breaks are also more important

00:09:53,280 --> 00:09:56,700
than you think when you're trying to

00:09:54,600 --> 00:09:59,670
solve a problem and I know when I'm

00:09:56,700 --> 00:10:01,500
trying to squash a bug i'm like dead set

00:09:59,670 --> 00:10:03,270
on trying to solve that no breaks until

00:10:01,500 --> 00:10:04,890
i finish it but breaks are actually

00:10:03,270 --> 00:10:06,420
really good and scientists think that

00:10:04,890 --> 00:10:09,030
this is because it helps you forget

00:10:06,420 --> 00:10:10,680
misleading hints so to test this like

00:10:09,030 --> 00:10:12,570
how it just gave participants some

00:10:10,680 --> 00:10:16,050
puzzles to solve as well as misleading

00:10:12,570 --> 00:10:17,400
hints the control group had a minute

00:10:16,050 --> 00:10:19,320
without breaks to try and solve the

00:10:17,400 --> 00:10:21,480
problem and the experimental group had

00:10:19,320 --> 00:10:24,240
30 seconds or interrupted for a short

00:10:21,480 --> 00:10:25,800
bit and then had another 30 seconds the

00:10:24,240 --> 00:10:27,750
interrupted group actually did better

00:10:25,800 --> 00:10:29,430
and it was found as a group were

00:10:27,750 --> 00:10:31,470
actually less likely to remember the

00:10:29,430 --> 00:10:35,430
misleading clues suggesting that maybe

00:10:31,470 --> 00:10:39,180
these two things were correlated a

00:10:35,430 --> 00:10:41,010
creativity is also a kind of you know

00:10:39,180 --> 00:10:43,860
kind of like feels like it's involved in

00:10:41,010 --> 00:10:46,890
squashing a bug or in problem solving

00:10:43,860 --> 00:10:48,360
but what exactly is creativity well

00:10:46,890 --> 00:10:50,820
scientists think it's really actually

00:10:48,360 --> 00:10:52,610
nothing special it's just when the sum

00:10:50,820 --> 00:10:55,230
of all your experiences memories

00:10:52,610 --> 00:10:56,520
training and motivation come together in

00:10:55,230 --> 00:11:00,600
the right way to give you the right

00:10:56,520 --> 00:11:02,430
tools to solve the problem all right so

00:11:00,600 --> 00:11:04,530
now that we are writing our own code and

00:11:02,430 --> 00:11:07,110
finding the bugs in it and maybe there's

00:11:04,530 --> 00:11:08,940
some people helping us we've entered

00:11:07,110 --> 00:11:11,790
into this realm of working with other

00:11:08,940 --> 00:11:14,250
humans and maybe even trying to read and

00:11:11,790 --> 00:11:16,260
other people's code but this is hard

00:11:14,250 --> 00:11:18,660
because again we tend to think about

00:11:16,260 --> 00:11:20,370
problems in fixed ways when you're

00:11:18,660 --> 00:11:21,630
reading someone's code kind of like

00:11:20,370 --> 00:11:23,580
you're getting into their head right

00:11:21,630 --> 00:11:26,220
you're like you're seeing how they solve

00:11:23,580 --> 00:11:27,720
the problem and that is unlikely to be

00:11:26,220 --> 00:11:30,090
exactly the same way that you would have

00:11:27,720 --> 00:11:32,220
solved the problem therefore it's harder

00:11:30,090 --> 00:11:33,690
to understand what they're doing nefer

00:11:32,220 --> 00:11:37,800
makes it that much harder to read the

00:11:33,690 --> 00:11:40,130
code um also we tend to use other

00:11:37,800 --> 00:11:43,350
people's code when we're using libraries

00:11:40,130 --> 00:11:45,390
but y'all might have heard of the not

00:11:43,350 --> 00:11:46,980
invented here syndrome which is that

00:11:45,390 --> 00:11:51,570
we're kind of averse to using things

00:11:46,980 --> 00:11:54,000
that we didn't write and cognitive

00:11:51,570 --> 00:11:57,530
scientists have called this the IKEA

00:11:54,000 --> 00:11:59,970
effect it's a fairly new phenomenon so

00:11:57,530 --> 00:12:02,520
scientists ask participants to assemble

00:11:59,970 --> 00:12:05,580
ikea furniture build lego contraptions

00:12:02,520 --> 00:12:07,500
and fold origami and then they were

00:12:05,580 --> 00:12:09,630
asked how much money they would pay for

00:12:07,500 --> 00:12:12,390
their creations as well as for experts

00:12:09,630 --> 00:12:15,540
creations participants were willing to

00:12:12,390 --> 00:12:17,970
pay the reasonably safe amount of money

00:12:15,540 --> 00:12:20,520
for their finished amateurish

00:12:17,970 --> 00:12:23,070
dilapidated paper cranes as they were

00:12:20,520 --> 00:12:27,750
for beautifully folded professionally

00:12:23,070 --> 00:12:30,030
folded origami cranes and while this

00:12:27,750 --> 00:12:34,410
particular experiment was mainly about

00:12:30,030 --> 00:12:36,990
money I think it's it's very to

00:12:34,410 --> 00:12:39,590
extrapolate to look value in general

00:12:36,990 --> 00:12:44,100
like how much we value other people's

00:12:39,590 --> 00:12:45,420
code versus how much we value our own so

00:12:44,100 --> 00:12:47,700
like that's why we don't want to use

00:12:45,420 --> 00:12:51,720
that library that has been battle tested

00:12:47,700 --> 00:12:53,790
and seeing the bugs have been fixed

00:12:51,720 --> 00:12:55,950
through lovely open-source processes

00:12:53,790 --> 00:12:57,360
that have been used in in ways that may

00:12:55,950 --> 00:13:01,350
be similar or different from the way

00:12:57,360 --> 00:13:02,850
that you're going to use it so working

00:13:01,350 --> 00:13:05,190
with other people maybe you're on a

00:13:02,850 --> 00:13:07,470
software team and therefore you probably

00:13:05,190 --> 00:13:09,450
have to do sprint planning or something

00:13:07,470 --> 00:13:10,890
or like tell someone how long it's going

00:13:09,450 --> 00:13:13,170
to take you to finish that bit of the

00:13:10,890 --> 00:13:14,880
project you know my job as an

00:13:13,170 --> 00:13:16,410
engineering manager is to make sure that

00:13:14,880 --> 00:13:18,420
my people get their work done in a

00:13:16,410 --> 00:13:19,950
reasonable amount of time but there's

00:13:18,420 --> 00:13:21,720
something working against me which is

00:13:19,950 --> 00:13:23,340
that we're really bad at making

00:13:21,720 --> 00:13:25,080
predictions about how much time it's

00:13:23,340 --> 00:13:26,510
going to take to do something

00:13:25,080 --> 00:13:29,400
and this is called the planning fallacy

00:13:26,510 --> 00:13:32,130
so an example of this was some

00:13:29,400 --> 00:13:33,960
psychologists asked 37's like students

00:13:32,130 --> 00:13:35,670
how long they thought it was going to

00:13:33,960 --> 00:13:38,640
take them to finish their senior theses

00:13:35,670 --> 00:13:40,950
and the average estimate was about 34

00:13:38,640 --> 00:13:42,930
days but the actual average time that it

00:13:40,950 --> 00:13:46,410
took these people to finish their papers

00:13:42,930 --> 00:13:48,150
was about 56 days with only about thirty

00:13:46,410 --> 00:13:50,820
percent of people finishing in the time

00:13:48,150 --> 00:13:53,450
that they predicted this phenomenon

00:13:50,820 --> 00:13:56,070
occurs regardless of if people know that

00:13:53,450 --> 00:13:58,350
past tasks that were similar also took

00:13:56,070 --> 00:14:00,150
them longer than they expected didn't

00:13:58,350 --> 00:14:02,790
really matter and this also really only

00:14:00,150 --> 00:14:05,280
happens for our own tasks whether we're

00:14:02,790 --> 00:14:07,230
working by ourselves or in a group for

00:14:05,280 --> 00:14:09,030
other other tasks to other people's

00:14:07,230 --> 00:14:09,990
tasks we show a pessimistic bias we

00:14:09,030 --> 00:14:13,170
think it's going to take them longer

00:14:09,990 --> 00:14:15,570
than they say and this is part of a

00:14:13,170 --> 00:14:18,210
larger bias called the optimism bias

00:14:15,570 --> 00:14:19,680
which is that we think that bad things

00:14:18,210 --> 00:14:23,190
are more likely to happen to other

00:14:19,680 --> 00:14:25,620
people than to us this is seen in 00

00:14:23,190 --> 00:14:28,230
wide array of situations not just

00:14:25,620 --> 00:14:30,690
planning our software projects so people

00:14:28,230 --> 00:14:32,550
tend to think that there are like less

00:14:30,690 --> 00:14:35,280
susceptible to crime than other people

00:14:32,550 --> 00:14:39,360
or less susceptible to the whims of the

00:14:35,280 --> 00:14:43,020
stock market all sorts of things so yeah

00:14:39,360 --> 00:14:45,690
we're not really great at that we're

00:14:43,020 --> 00:14:49,200
also often so worried about wasting time

00:14:45,690 --> 00:14:52,050
that we've already spent and invested

00:14:49,200 --> 00:14:53,520
resources in that we don't consider like

00:14:52,050 --> 00:14:55,680
the cost that it's going to take to

00:14:53,520 --> 00:14:58,950
continue doing something that's like

00:14:55,680 --> 00:15:02,040
slogging so this is called the sunk cost

00:14:58,950 --> 00:15:03,900
fallacy and I think it's contributing to

00:15:02,040 --> 00:15:07,530
us wasting a lot of time and things that

00:15:03,900 --> 00:15:09,480
are just taking a really long time so

00:15:07,530 --> 00:15:10,950
once you've been through that sprint

00:15:09,480 --> 00:15:12,480
planning meeting that took like three

00:15:10,950 --> 00:15:15,180
hours you probably have to go back to

00:15:12,480 --> 00:15:16,860
your desk and start working maybe in an

00:15:15,180 --> 00:15:18,450
open office so the person like right

00:15:16,860 --> 00:15:21,090
next to you is on a Google hangout with

00:15:18,450 --> 00:15:24,360
a person right next to them so there's a

00:15:21,090 --> 00:15:26,610
lot of noise and you know this isn't

00:15:24,360 --> 00:15:28,200
really the best for productivity but in

00:15:26,610 --> 00:15:29,790
the grand scheme of things we're

00:15:28,200 --> 00:15:31,890
actually pretty good at filtering out

00:15:29,790 --> 00:15:35,970
unwanted stimuli and this is called

00:15:31,890 --> 00:15:37,830
selective attention scientists like to

00:15:35,970 --> 00:15:38,940
study selective attention by doing

00:15:37,830 --> 00:15:41,430
what's called a dicot

00:15:38,940 --> 00:15:45,030
listing task with shadowing so

00:15:41,430 --> 00:15:47,640
participants have to to speech dreams or

00:15:45,030 --> 00:15:50,490
sound streams going into into both of

00:15:47,640 --> 00:15:52,530
their ears so in one ear participants

00:15:50,490 --> 00:15:54,510
will will get a stream of speech that

00:15:52,530 --> 00:15:57,060
they're supposed to repeat back and in

00:15:54,510 --> 00:15:59,070
the other ear they get speech music

00:15:57,060 --> 00:16:02,280
something they're not supposed to pay

00:15:59,070 --> 00:16:03,960
attention to that and when questioned

00:16:02,280 --> 00:16:07,500
afterwards participants generally have

00:16:03,960 --> 00:16:08,820
very little recollection of any semantic

00:16:07,500 --> 00:16:11,850
content that though they weren't

00:16:08,820 --> 00:16:15,570
shadowing so the purple well that

00:16:11,850 --> 00:16:17,730
contrast is off so yeah like a switch

00:16:15,570 --> 00:16:21,060
from English to German goes unnoticed

00:16:17,730 --> 00:16:22,980
like you could tell them things that

00:16:21,060 --> 00:16:26,430
they don't want to hear they won't

00:16:22,980 --> 00:16:28,920
remember but physical attributes of

00:16:26,430 --> 00:16:31,110
sound are are remembered so people

00:16:28,920 --> 00:16:34,710
generally are able to recall if the

00:16:31,110 --> 00:16:36,420
unattended channel is speech or not so

00:16:34,710 --> 00:16:37,770
like they're in your office there could

00:16:36,420 --> 00:16:39,540
be like music playing you'll know that

00:16:37,770 --> 00:16:40,710
there's music playing but maybe not

00:16:39,540 --> 00:16:42,270
listen to the lyrics but when you're

00:16:40,710 --> 00:16:43,860
listening to the lyrics you're probably

00:16:42,270 --> 00:16:47,600
not going to be able to focus that much

00:16:43,860 --> 00:16:50,550
on writing code but if there's something

00:16:47,600 --> 00:16:52,470
salient in the and the speedstream that

00:16:50,550 --> 00:16:54,510
they're not listening to like their name

00:16:52,470 --> 00:16:56,310
or some like not safe for work words

00:16:54,510 --> 00:16:59,940
people usually notice them and this is

00:16:56,310 --> 00:17:01,710
called the cocktail party effect so this

00:16:59,940 --> 00:17:04,079
suggests that selective attention

00:17:01,710 --> 00:17:06,180
requires both ignoring like active

00:17:04,079 --> 00:17:09,120
ignoring and active paying attention and

00:17:06,180 --> 00:17:10,890
so just because we're good at selective

00:17:09,120 --> 00:17:14,010
attention doesn't mean it's good for us

00:17:10,890 --> 00:17:16,079
we're putting in like active mental

00:17:14,010 --> 00:17:17,280
effort to blocking out things that we

00:17:16,079 --> 00:17:19,500
don't want to be paying attention to

00:17:17,280 --> 00:17:22,079
which means that there are less mental

00:17:19,500 --> 00:17:24,540
resources going towards the stuff that

00:17:22,079 --> 00:17:26,400
we actually need to be doing so we can

00:17:24,540 --> 00:17:29,280
only pay attention to so many things at

00:17:26,400 --> 00:17:31,110
once mostly one thing and we have a

00:17:29,280 --> 00:17:32,580
limited supply of mental resources and

00:17:31,110 --> 00:17:34,380
we often don't have the budget to do

00:17:32,580 --> 00:17:35,970
more than one thing at once so you're

00:17:34,380 --> 00:17:37,620
unlikely going to be able to keep

00:17:35,970 --> 00:17:39,090
writing that difficult algorithm if

00:17:37,620 --> 00:17:41,190
you're trying to listen in to that

00:17:39,090 --> 00:17:43,020
conversation happening 10 feet away

00:17:41,190 --> 00:17:45,120
between your engineering manager and

00:17:43,020 --> 00:17:47,520
your product manager that if the project

00:17:45,120 --> 00:17:48,780
you're working on that with that

00:17:47,520 --> 00:17:50,700
algorithm is going to get scrapped

00:17:48,780 --> 00:17:51,440
tomorrow not going to be able to

00:17:50,700 --> 00:17:54,610
concentrate

00:17:51,440 --> 00:17:57,049
and this is because we are sometimes

00:17:54,610 --> 00:17:58,850
helpless to the processing power of our

00:17:57,049 --> 00:18:01,039
brain because our brain just does things

00:17:58,850 --> 00:18:04,009
automatically in an attempt to be

00:18:01,039 --> 00:18:06,710
efficient so if you had an email account

00:18:04,009 --> 00:18:09,350
any time around the early 2000s this

00:18:06,710 --> 00:18:11,330
might look familiar and this is actually

00:18:09,350 --> 00:18:13,879
real science it's called the Stroop task

00:18:11,330 --> 00:18:17,480
and if you're not familiar with it the

00:18:13,879 --> 00:18:20,000
idea is that you're supposed to say the

00:18:17,480 --> 00:18:22,759
color that the word is written in not

00:18:20,000 --> 00:18:24,440
the color not the word itself and the

00:18:22,759 --> 00:18:27,679
leading theory about why this occurs is

00:18:24,440 --> 00:18:29,389
called automaticity so all reading is an

00:18:27,679 --> 00:18:30,590
automatic committable process that we

00:18:29,389 --> 00:18:34,940
don't really have to think about very

00:18:30,590 --> 00:18:37,820
much but recognizing a color isn't

00:18:34,940 --> 00:18:39,409
automatic process so while reading

00:18:37,820 --> 00:18:41,870
doesn't need controlled attention it

00:18:39,409 --> 00:18:43,990
uses enough resources to reduce the

00:18:41,870 --> 00:18:49,909
amount that we can then dedicate to

00:18:43,990 --> 00:18:53,809
saying what what color the ink is all

00:18:49,909 --> 00:18:55,340
right so as I said before software is

00:18:53,809 --> 00:18:57,559
actually about people and this lovely

00:18:55,340 --> 00:19:01,370
phrase comes from the amazing people at

00:18:57,559 --> 00:19:02,860
and yet and so when I heard this it kind

00:19:01,370 --> 00:19:04,700
of like changed the way I thought about

00:19:02,860 --> 00:19:06,740
software and about engineering

00:19:04,700 --> 00:19:08,090
management like it's not just about the

00:19:06,740 --> 00:19:10,190
code again I said this in the beginning

00:19:08,090 --> 00:19:13,480
it's also about the people and without

00:19:10,190 --> 00:19:16,309
humans was supported trading software um

00:19:13,480 --> 00:19:21,970
so now we're gonna start to get to

00:19:16,309 --> 00:19:21,970
villach human side of all this right so

00:19:22,090 --> 00:19:25,940
something that's getting in the way of

00:19:23,929 --> 00:19:28,730
us being productive on our teams I think

00:19:25,940 --> 00:19:31,279
is that relatively unskilled people

00:19:28,730 --> 00:19:33,289
think that they are better at tasks than

00:19:31,279 --> 00:19:36,169
they actually are this is called the

00:19:33,289 --> 00:19:38,600
dunning-kruger effect and a funny story

00:19:36,169 --> 00:19:40,909
it was inspired by the case of a dude

00:19:38,600 --> 00:19:43,129
who robbed two banks after covering his

00:19:40,909 --> 00:19:45,620
face with lemon juice since lemon juice

00:19:43,129 --> 00:19:47,029
can can be used as invisible ink he

00:19:45,620 --> 00:19:49,700
thought it would cause his face to be

00:19:47,029 --> 00:19:54,529
invisible on the security cameras which

00:19:49,700 --> 00:19:56,750
is like unfortunate i guess um and this

00:19:54,529 --> 00:19:59,059
is part of a larger phenomenon called

00:19:56,750 --> 00:20:01,159
blue Sri superiority which is that we

00:19:59,059 --> 00:20:04,190
tend to overestimate our own skills and

00:20:01,159 --> 00:20:05,309
abilities so we think that were like

00:20:04,190 --> 00:20:06,690
higher

00:20:05,309 --> 00:20:08,759
elegance we're going to be better at

00:20:06,690 --> 00:20:11,610
performing tasks and tests we think that

00:20:08,759 --> 00:20:14,220
we have like more desirable traits and

00:20:11,610 --> 00:20:16,860
characteristics than other people but

00:20:14,220 --> 00:20:18,990
sort of example of this was in a survey

00:20:16,860 --> 00:20:22,409
of faculty of the University of Nebraska

00:20:18,990 --> 00:20:25,289
sixty-eight percent of these teachers

00:20:22,409 --> 00:20:27,509
rated themselves within the top 25% for

00:20:25,289 --> 00:20:28,950
teaching ability and more than ninety

00:20:27,509 --> 00:20:32,690
percent or you to themselves as above

00:20:28,950 --> 00:20:35,879
average but like that doesn't math uh

00:20:32,690 --> 00:20:37,590
like fifty percent of people have to be

00:20:35,879 --> 00:20:41,429
below average that's just how it works

00:20:37,590 --> 00:20:43,529
and like well I'm like all of you are

00:20:41,429 --> 00:20:49,470
below average in something like I'm

00:20:43,529 --> 00:20:50,309
below average in height uh like doesn't

00:20:49,470 --> 00:20:52,679
mean that you're bad at what you're

00:20:50,309 --> 00:20:54,480
doing just below average so like how is

00:20:52,679 --> 00:20:57,960
this getting in our way right so you're

00:20:54,480 --> 00:20:59,429
not gonna make good software if you

00:20:57,960 --> 00:21:01,470
don't realize how much more you have to

00:20:59,429 --> 00:21:03,360
learn if there's ways you can make your

00:21:01,470 --> 00:21:04,980
software better but you're not really

00:21:03,360 --> 00:21:07,080
open to learning them because you think

00:21:04,980 --> 00:21:11,159
you're the best your software isn't

00:21:07,080 --> 00:21:12,690
going to be good um and there so there's

00:21:11,159 --> 00:21:15,210
the flip side of this which is that

00:21:12,690 --> 00:21:17,220
skilled people often underestimate their

00:21:15,210 --> 00:21:20,369
abilities and think that tasks that are

00:21:17,220 --> 00:21:22,169
easy for them are easy for others so if

00:21:20,369 --> 00:21:23,639
you've been doing this for a while you

00:21:22,169 --> 00:21:25,740
might forget what it was like to be a

00:21:23,639 --> 00:21:28,499
beginner you're not gonna have empathy

00:21:25,740 --> 00:21:30,509
for the beginners on your team that are

00:21:28,499 --> 00:21:31,860
actually integral to your team they're

00:21:30,509 --> 00:21:34,200
going to ask questions that are

00:21:31,860 --> 00:21:35,460
important to making good software going

00:21:34,200 --> 00:21:39,570
to ask things that you're not really

00:21:35,460 --> 00:21:41,610
thinking about anymore so if you if you

00:21:39,570 --> 00:21:42,869
dismiss their questions they're not

00:21:41,610 --> 00:21:44,700
going to ask them any more and then your

00:21:42,869 --> 00:21:47,190
software isn't going to be as good as it

00:21:44,700 --> 00:21:49,139
could be but this this can also manifest

00:21:47,190 --> 00:21:51,690
in a different way and if you attended

00:21:49,139 --> 00:21:53,399
the last talk an imposter syndrome you

00:21:51,690 --> 00:21:56,759
know what impostor syndrome is the the

00:21:53,399 --> 00:21:59,100
short of it is that people sometimes

00:21:56,759 --> 00:22:01,080
think that their accomplishments and

00:21:59,100 --> 00:22:03,600
their achievements are the result of

00:22:01,080 --> 00:22:05,009
luck timing deception like they pulled

00:22:03,600 --> 00:22:07,740
the wool over someone's eyes they don't

00:22:05,009 --> 00:22:10,049
deserve what they have when in fact they

00:22:07,740 --> 00:22:12,029
probably do and some studies suggest

00:22:10,049 --> 00:22:14,519
that this is particularly common among

00:22:12,029 --> 00:22:15,360
high-achieving women but across the

00:22:14,519 --> 00:22:18,059
board even

00:22:15,360 --> 00:22:24,870
of the best programmers I know suffer

00:22:18,059 --> 00:22:26,520
from imposter syndrome so we're working

00:22:24,870 --> 00:22:28,710
with people and we're working with the

00:22:26,520 --> 00:22:30,120
team so how do we assemble the best team

00:22:28,710 --> 00:22:32,250
and how are these cognitive biases

00:22:30,120 --> 00:22:34,230
getting in the way of us assembling the

00:22:32,250 --> 00:22:35,670
best team possible I think one of the

00:22:34,230 --> 00:22:37,470
things getting in the way is that we

00:22:35,670 --> 00:22:41,280
tend to favor members of our own in

00:22:37,470 --> 00:22:42,750
group so and this is getting in the way

00:22:41,280 --> 00:22:45,990
especially with those like referral

00:22:42,750 --> 00:22:48,059
bonuses right you you get a lot of money

00:22:45,990 --> 00:22:49,290
for referring people in your social

00:22:48,059 --> 00:22:52,679
circles that you think you're good

00:22:49,290 --> 00:22:54,900
programmers but if our social social

00:22:52,679 --> 00:22:56,700
circles tend to look like us you're just

00:22:54,900 --> 00:22:59,070
going to refer them so if i were to

00:22:56,700 --> 00:23:00,480
refer people who looked like me my team

00:22:59,070 --> 00:23:04,110
would just be a bunch of white women and

00:23:00,480 --> 00:23:08,400
that's not good either so we want to not

00:23:04,110 --> 00:23:09,750
do that so another thing getting in our

00:23:08,400 --> 00:23:12,480
way is called the fundamental

00:23:09,750 --> 00:23:14,700
attribution error so people have a

00:23:12,480 --> 00:23:16,860
tendency to attribute situations to

00:23:14,700 --> 00:23:19,080
other people's character rather than to

00:23:16,860 --> 00:23:21,630
external factors so a classic example of

00:23:19,080 --> 00:23:23,970
this is like you're driving down the

00:23:21,630 --> 00:23:25,559
street being a law abiding and driving

00:23:23,970 --> 00:23:27,150
the right speed limit and then all of a

00:23:25,559 --> 00:23:29,429
sudden someone like rushes past you and

00:23:27,150 --> 00:23:31,049
you're like ah what a jerk they're like

00:23:29,429 --> 00:23:32,640
breaking all the laws and they're

00:23:31,049 --> 00:23:36,030
causing a dangerous situation on the

00:23:32,640 --> 00:23:37,320
road but maybe in fact they're driving a

00:23:36,030 --> 00:23:39,210
loved one to the hospital and they're

00:23:37,320 --> 00:23:40,950
they're not really thinking about laws

00:23:39,210 --> 00:23:43,290
they're just thinking about their loved

00:23:40,950 --> 00:23:44,640
one in the back seats so if you were in

00:23:43,290 --> 00:23:47,340
that case you would know you would know

00:23:44,640 --> 00:23:48,600
to have empathy for that person and it

00:23:47,340 --> 00:23:54,630
has nothing to do with they're not like

00:23:48,600 --> 00:23:56,669
a jerk actually so this also happens on

00:23:54,630 --> 00:23:58,710
a larger scale something called the

00:23:56,669 --> 00:24:01,500
group attribution error so people have a

00:23:58,710 --> 00:24:03,419
tendency to believe that attributes of a

00:24:01,500 --> 00:24:06,630
group member reflects the entire group

00:24:03,419 --> 00:24:11,070
so I thought this was well explained by

00:24:06,630 --> 00:24:17,040
this XKCD cartoon so unloved on the

00:24:11,070 --> 00:24:19,260
right you see two dudes and it's

00:24:17,040 --> 00:24:22,620
actually on the left nah I'm below

00:24:19,260 --> 00:24:26,520
average in which direction is which so

00:24:22,620 --> 00:24:28,290
on one side you see two dudes one who

00:24:26,520 --> 00:24:31,110
apparently is not great at math

00:24:28,290 --> 00:24:35,460
and his friend is saying why do you suck

00:24:31,110 --> 00:24:38,430
at math on the right we see a dude and a

00:24:35,460 --> 00:24:42,000
woman and the woman apparently is not

00:24:38,430 --> 00:24:46,050
good at math and the guy is saying wow

00:24:42,000 --> 00:24:49,200
girls suck at math so why does this why

00:24:46,050 --> 00:24:51,780
is this kind of true um but studies have

00:24:49,200 --> 00:24:53,970
shown that the the group attribution

00:24:51,780 --> 00:24:55,560
error is stronger and perceptions of

00:24:53,970 --> 00:24:58,860
groups that are viewed as more too

00:24:55,560 --> 00:25:00,330
similar to once owned and this doesn't

00:24:58,860 --> 00:25:03,690
really happen with perceptions of our

00:25:00,330 --> 00:25:06,030
own groups so decisions made by people

00:25:03,690 --> 00:25:08,540
in our own group are the result of

00:25:06,030 --> 00:25:11,040
structural constraints but decisions of

00:25:08,540 --> 00:25:15,420
other groups are the result of their

00:25:11,040 --> 00:25:17,670
attitudes so intact perhaps it's not

00:25:15,420 --> 00:25:20,550
that certain groups of people don't want

00:25:17,670 --> 00:25:22,560
to be programmers maybe just maybe there

00:25:20,550 --> 00:25:28,080
are structural forces making it harder

00:25:22,560 --> 00:25:29,670
for them to do so something else that I

00:25:28,080 --> 00:25:32,520
think is getting in our way is called

00:25:29,670 --> 00:25:34,740
the availability heuristic and this like

00:25:32,520 --> 00:25:36,270
the popular example of this is that like

00:25:34,740 --> 00:25:39,390
why people are so afraid of plane

00:25:36,270 --> 00:25:42,450
crashes which is like we hear only about

00:25:39,390 --> 00:25:46,080
plane crashes and not successful plane

00:25:42,450 --> 00:25:47,910
trips so there are those examples of

00:25:46,080 --> 00:25:49,380
unsuccessful plane trips are more

00:25:47,910 --> 00:25:52,830
salient in our mind and therefore like

00:25:49,380 --> 00:25:55,500
the plane is going to crash so I think

00:25:52,830 --> 00:25:57,480
this applies to to our program and

00:25:55,500 --> 00:25:59,550
communities and that like if we are

00:25:57,480 --> 00:26:01,830
surrounded by people who all look a

00:25:59,550 --> 00:26:03,600
certain way and if the the prominent

00:26:01,830 --> 00:26:05,310
members of our community the main

00:26:03,600 --> 00:26:07,620
contributors to open source and the

00:26:05,310 --> 00:26:08,670
people who we see on stage I'll look a

00:26:07,620 --> 00:26:10,680
certain way we're going to start to

00:26:08,670 --> 00:26:12,150
think that there's the people in our

00:26:10,680 --> 00:26:14,100
community and those are the only people

00:26:12,150 --> 00:26:15,660
in our community and those are the only

00:26:14,100 --> 00:26:17,570
people who can contribute to open source

00:26:15,660 --> 00:26:20,220
and the only people who have good ideas

00:26:17,570 --> 00:26:24,390
to talk about on stage which is not

00:26:20,220 --> 00:26:26,400
necessarily true something else that's

00:26:24,390 --> 00:26:28,290
also getting in our way is called the

00:26:26,400 --> 00:26:30,990
representativeness heuristic which is

00:26:28,290 --> 00:26:32,550
that essentially that we find we think

00:26:30,990 --> 00:26:34,560
that categories are relatively

00:26:32,550 --> 00:26:36,360
homogenous so if someone is a software

00:26:34,560 --> 00:26:37,650
engineer and expect them to have the

00:26:36,360 --> 00:26:39,690
traits that we associate with the

00:26:37,650 --> 00:26:42,130
software engineer and vice versa and

00:26:39,690 --> 00:26:44,440
this heuristic leads to several by

00:26:42,130 --> 00:26:48,340
sees such as something called base rate

00:26:44,440 --> 00:26:49,840
neglect so diversity in kahana man ran

00:26:48,340 --> 00:26:52,930
an experiment in which they gave

00:26:49,840 --> 00:26:55,630
descriptions of a group of people so 100

00:26:52,930 --> 00:26:58,060
people and 70 are lawyers and thirty

00:26:55,630 --> 00:27:00,220
percent are 30-year engineers and they

00:26:58,060 --> 00:27:02,590
found that participants often said that

00:27:00,220 --> 00:27:04,630
someone who sounded like an engineer was

00:27:02,590 --> 00:27:06,460
more likely to be an engineer even

00:27:04,630 --> 00:27:08,530
though the probability was against that

00:27:06,460 --> 00:27:12,670
because there were 70 lawyers and 30

00:27:08,530 --> 00:27:14,620
engineers I've seen this come true at

00:27:12,670 --> 00:27:16,360
programming meetups both it's built

00:27:14,620 --> 00:27:18,490
happened to me and to my friends where

00:27:16,360 --> 00:27:20,170
we're at a programming meet up it's very

00:27:18,490 --> 00:27:22,810
likely that people at programming

00:27:20,170 --> 00:27:23,770
meetups are programmers but people have

00:27:22,810 --> 00:27:27,670
come up to him in like are you a

00:27:23,770 --> 00:27:29,320
designer are you a recruiter like no I'm

00:27:27,670 --> 00:27:36,130
at a programming meetup of a programmer

00:27:29,320 --> 00:27:37,210
it's like not good Oh so um might it be

00:27:36,130 --> 00:27:39,820
able to tell them kind of talking about

00:27:37,210 --> 00:27:42,520
diversity here um so like why is

00:27:39,820 --> 00:27:45,880
diversity important for assembling good

00:27:42,520 --> 00:27:47,710
team and creating good software well so

00:27:45,880 --> 00:27:49,810
i mentioned that creativity is all about

00:27:47,710 --> 00:27:53,170
collective is about associative memory

00:27:49,810 --> 00:27:56,670
so when you have a diverse team you're

00:27:53,170 --> 00:28:00,180
going to have a wider array of of

00:27:56,670 --> 00:28:03,070
experiences of backgrounds of interests

00:28:00,180 --> 00:28:05,620
and therefore you're going to have

00:28:03,070 --> 00:28:06,550
better collective associative memory and

00:28:05,620 --> 00:28:07,840
therefore you're going to be more

00:28:06,550 --> 00:28:09,610
creative you're going to find better

00:28:07,840 --> 00:28:12,550
solutions to problems that's going to

00:28:09,610 --> 00:28:15,940
lead to better software so what can we

00:28:12,550 --> 00:28:18,130
do about all of this right so first off

00:28:15,940 --> 00:28:21,060
like don't feel bad your brains are like

00:28:18,130 --> 00:28:25,030
literally programmed to do this because

00:28:21,060 --> 00:28:27,850
like back in the day we were just

00:28:25,030 --> 00:28:30,760
supposed to like escape predators but

00:28:27,850 --> 00:28:33,460
now we are sitting at computers thinking

00:28:30,760 --> 00:28:35,520
all day so it's a little different so

00:28:33,460 --> 00:28:37,240
like the quicker that you can think the

00:28:35,520 --> 00:28:40,330
quicker you're going to get away from

00:28:37,240 --> 00:28:43,090
that tiger that's chasing you and you're

00:28:40,330 --> 00:28:45,490
going to have more cognitive economy so

00:28:43,090 --> 00:28:48,460
this thinking fast system is called

00:28:45,490 --> 00:28:50,830
system one in the literature and the

00:28:48,460 --> 00:28:52,150
thinking slow system is called system to

00:28:50,830 --> 00:28:55,210
in the literature and if you might have

00:28:52,150 --> 00:28:55,690
read a book or heard of a book by this

00:28:55,210 --> 00:28:57,549
title

00:28:55,690 --> 00:29:00,639
making fast and slow by Daniel Kahneman

00:28:57,549 --> 00:29:02,860
so like what can we do about this right

00:29:00,639 --> 00:29:05,169
like we're as I said we are literally

00:29:02,860 --> 00:29:07,299
sitting there and thinking our job is to

00:29:05,169 --> 00:29:09,549
think and we usually take a really long

00:29:07,299 --> 00:29:12,340
time to think through very hard problems

00:29:09,549 --> 00:29:14,049
so if we just take that same amount of

00:29:12,340 --> 00:29:16,330
time and apply them to things that we

00:29:14,049 --> 00:29:17,889
might not think about as much just take

00:29:16,330 --> 00:29:20,649
another hour to make those hard

00:29:17,889 --> 00:29:23,080
decisions then we're probably going to

00:29:20,649 --> 00:29:24,580
end up making better software that more

00:29:23,080 --> 00:29:28,470
humans can use and we're going to end up

00:29:24,580 --> 00:29:32,110
making these things with happier humans

00:29:28,470 --> 00:29:34,149
so thanks for listening that's me and

00:29:32,110 --> 00:29:36,700
all my emoji friends you can find my

00:29:34,149 --> 00:29:41,700
slides at that link tweet at me it's

00:29:36,700 --> 00:29:41,700

YouTube URL: https://www.youtube.com/watch?v=XNfpnCLbRmc


