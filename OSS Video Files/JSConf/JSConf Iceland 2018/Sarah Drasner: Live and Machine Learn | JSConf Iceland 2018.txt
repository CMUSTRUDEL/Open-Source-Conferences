Title: Sarah Drasner: Live and Machine Learn | JSConf Iceland 2018
Publication date: 2018-04-06
Playlist: JSConf Iceland 2018
Description: 
	https://2018.jsconf.is/speakers/sarah-drasner/

The life we live online increasingly informs the way we live offline as well. Businesses live and die through algorithms like SEO, humans are sorted in government systems, and we make large, life-governing decisions through what is shown to us on the web: home buying, where to live, what to eat, and who we're in contact with regularly. The first shift we as web developers saw was people living and learning on the web more and more, which excited us. But as we start to automate those tasks through machine learning algorithms, a lot of us have trepidation. We know systems have flaws, what are the political and social consequences?

In this talk we'll explore this paradigm shift and some of it's dangers, but we'll also talk about the good impacts technology can bring. Helping people who need it, automating tasks for humans with disabilities, communication for emergency services: the possibilities for positive influence are endless. We'll explore just some of the tools that are out there, how with a little creativity, we can use these technologies for good. We as developers have a voice and chance to make a difference.
Captions: 
	00:00:00,380 --> 00:00:04,529
all right it's the last talk of the day

00:00:02,700 --> 00:00:06,420
you guys made it and you know the first

00:00:04,529 --> 00:00:08,010
thing I want to do is to thank the MCS

00:00:06,420 --> 00:00:10,260
and the organizers for having such a

00:00:08,010 --> 00:00:16,920
great conference so give them a round of

00:00:10,260 --> 00:00:19,470
applause all right now I'm gonna ask

00:00:16,920 --> 00:00:21,390
something of you everyone in this room

00:00:19,470 --> 00:00:23,820
I'd appreciate it if you'd take your

00:00:21,390 --> 00:00:30,480
phones out and hold them in your hands

00:00:23,820 --> 00:00:33,180
you can do it okay everybody pretty much

00:00:30,480 --> 00:00:35,510
okay now pass your phone to the person

00:00:33,180 --> 00:00:35,510
on your right

00:00:44,060 --> 00:00:52,650
okay look at that phone feel it

00:00:48,410 --> 00:00:54,420
recognize it but you almost can't do

00:00:52,650 --> 00:00:57,510
that right because you're so distracted

00:00:54,420 --> 00:01:01,320
thinking about your phone in someone

00:00:57,510 --> 00:01:04,110
else's hands oh it's awful it's just

00:01:01,320 --> 00:01:06,090
awful and the thing that I want to bring

00:01:04,110 --> 00:01:08,190
up here is that the reason that we feel

00:01:06,090 --> 00:01:12,630
this way is that our phones are now an

00:01:08,190 --> 00:01:15,119
extension of ourselves these phones are

00:01:12,630 --> 00:01:15,960
become who we are in our connection to

00:01:15,119 --> 00:01:17,850
that data

00:01:15,960 --> 00:01:19,890
Pete smart makes this point when he

00:01:17,850 --> 00:01:21,270
talks about designing for human-computer

00:01:19,890 --> 00:01:22,890
interaction which we're gonna talk a

00:01:21,270 --> 00:01:24,990
little about a little bit today when he

00:01:22,890 --> 00:01:27,960
talks about our relationship with

00:01:24,990 --> 00:01:29,550
technology but we're gonna talk about it

00:01:27,960 --> 00:01:31,440
with a different stint we're not gonna

00:01:29,550 --> 00:01:33,690
talk about the device itself but how we

00:01:31,440 --> 00:01:35,460
relate to these technological advances

00:01:33,690 --> 00:01:42,270
so you can you can pass the phones back

00:01:35,460 --> 00:01:44,099
you know all right so there are a lot of

00:01:42,270 --> 00:01:46,229
different takes about living our lives

00:01:44,099 --> 00:01:48,330
increasingly online and if you're in

00:01:46,229 --> 00:01:50,399
this room chances are as we started to

00:01:48,330 --> 00:01:52,020
adopt things digitally you got excited

00:01:50,399 --> 00:01:53,849
we were thinking about all the things

00:01:52,020 --> 00:01:56,160
that we could build all the things that

00:01:53,849 --> 00:02:00,569
we can make and curiosity for how we

00:01:56,160 --> 00:02:03,149
could represent ourselves online so

00:02:00,569 --> 00:02:06,030
first we have geo cities which is like a

00:02:03,149 --> 00:02:07,950
bastion of Internet explosion I mean I

00:02:06,030 --> 00:02:10,259
can't make something that's good nobody

00:02:07,950 --> 00:02:12,540
can like that that's just it we reached

00:02:10,259 --> 00:02:13,980
the pinnacle right at the beginning then

00:02:12,540 --> 00:02:16,050
we in myspace which a lot of people

00:02:13,980 --> 00:02:17,550
customized everyone was friends with Tom

00:02:16,050 --> 00:02:20,069
and actually in putting this talk

00:02:17,550 --> 00:02:22,950
together I revisited mine which was a

00:02:20,069 --> 00:02:25,380
time capsule two terrible bangs weird

00:02:22,950 --> 00:02:27,630
old art pictures and weird pictures of

00:02:25,380 --> 00:02:30,690
me in a bathroom I don't know it was a

00:02:27,630 --> 00:02:33,959
face some people might have started

00:02:30,690 --> 00:02:35,310
developing with Dragon Ball Z and some

00:02:33,959 --> 00:02:37,650
people started because they wanted to be

00:02:35,310 --> 00:02:41,239
leet hackers so good at programming that

00:02:37,650 --> 00:02:41,239
their fingers were literally on fire

00:02:41,420 --> 00:02:46,470
whatever our interests were the Internet

00:02:44,129 --> 00:02:48,990
allowed us to explore and share our

00:02:46,470 --> 00:02:52,000
lives and express ourselves in creative

00:02:48,990 --> 00:02:54,250
bizarre interesting

00:02:52,000 --> 00:02:59,110
and unique ways that only a collectivity

00:02:54,250 --> 00:03:02,980
of humans could possibly imagine but the

00:02:59,110 --> 00:03:05,380
only constant is change and soon we had

00:03:02,980 --> 00:03:07,330
so many ways of being represented online

00:03:05,380 --> 00:03:09,520
and so many people were online that we

00:03:07,330 --> 00:03:11,530
needed to automate these tasks we needed

00:03:09,520 --> 00:03:14,380
something that could analyze build

00:03:11,530 --> 00:03:16,480
manage and sort for us and as things

00:03:14,380 --> 00:03:19,210
become more complicated what we realize

00:03:16,480 --> 00:03:21,970
is all of our lives become summations of

00:03:19,210 --> 00:03:24,310
these digital models we become pieces of

00:03:21,970 --> 00:03:28,150
someone else's algorithms which means we

00:03:24,310 --> 00:03:30,400
could be summarized in a single way so

00:03:28,150 --> 00:03:32,350
people are starting to use search

00:03:30,400 --> 00:03:34,120
engines rather than libraries or

00:03:32,350 --> 00:03:36,160
teachers to make sense of the world

00:03:34,120 --> 00:03:38,230
we're inhabiting these inaccuracies

00:03:36,160 --> 00:03:40,090
start to be reflected in what we learn

00:03:38,230 --> 00:03:43,510
what we believe in they become our

00:03:40,090 --> 00:03:45,160
societies and our education and tech can

00:03:43,510 --> 00:03:47,709
be super magical in this way right

00:03:45,160 --> 00:03:50,230
that's such a cool thing that we can all

00:03:47,709 --> 00:03:53,680
be here together for the same reason in

00:03:50,230 --> 00:03:56,290
Iceland but just like any other software

00:03:53,680 --> 00:03:59,739
we write the more magic it contains the

00:03:56,290 --> 00:04:01,720
more it can fail I'm Sara Dresner known

00:03:59,739 --> 00:04:06,880
on Twitter as Sara IDO and I work for

00:04:01,720 --> 00:04:15,100
Microsoft yeah and today's talk is

00:04:06,880 --> 00:04:16,359
called live and machine learning so in

00:04:15,100 --> 00:04:17,440
this talk we're gonna explore the

00:04:16,359 --> 00:04:19,299
paradigm shift that machine learning

00:04:17,440 --> 00:04:21,160
brings and we're gonna do that through

00:04:19,299 --> 00:04:22,510
talking about how much you need learnt

00:04:21,160 --> 00:04:24,370
machine learning works at a really high

00:04:22,510 --> 00:04:26,470
level we don't have a ton of time to go

00:04:24,370 --> 00:04:29,110
into it but we're also going to talk

00:04:26,470 --> 00:04:31,210
about challenges to democracy through

00:04:29,110 --> 00:04:32,650
image machine learning we're also gonna

00:04:31,210 --> 00:04:34,810
talk about machine learning for good

00:04:32,650 --> 00:04:36,040
machine learning does a lot of really

00:04:34,810 --> 00:04:38,560
cool things and we're gonna talk about

00:04:36,040 --> 00:04:40,810
those two and then we're also gonna talk

00:04:38,560 --> 00:04:43,180
about some of the practical tech that we

00:04:40,810 --> 00:04:44,440
all know so not necessarily machine

00:04:43,180 --> 00:04:45,940
learning but how we can make a

00:04:44,440 --> 00:04:49,180
difference with some of the things that

00:04:45,940 --> 00:04:50,410
we know today so let's talk at a very

00:04:49,180 --> 00:04:52,090
high level about what machine learning

00:04:50,410 --> 00:04:54,580
is and we'll kick it off with a quote by

00:04:52,090 --> 00:04:56,500
the fella who coined the term machine

00:04:54,580 --> 00:04:57,910
learning gives computers the ability to

00:04:56,500 --> 00:04:59,550
learn without being explicitly

00:04:57,910 --> 00:05:02,229
programmed

00:04:59,550 --> 00:05:05,260
okay so we're programmers we're familiar

00:05:02,229 --> 00:05:05,870
with if-then statements so what if these

00:05:05,260 --> 00:05:07,400
come

00:05:05,870 --> 00:05:12,080
Editions for the if-then statements

00:05:07,400 --> 00:05:14,270
become complex I mean really complex if

00:05:12,080 --> 00:05:16,850
you're building a program to understand

00:05:14,270 --> 00:05:18,980
that this is a pug and this is a kitten

00:05:16,850 --> 00:05:21,590
you don't necessarily want to program an

00:05:18,980 --> 00:05:24,020
if-statement for every single pixel here

00:05:21,590 --> 00:05:26,000
or the possibility for every pixel for

00:05:24,020 --> 00:05:28,910
every single pug photo I mean that's not

00:05:26,000 --> 00:05:31,040
how our brains work right so how do you

00:05:28,910 --> 00:05:34,040
program it well you want to teach it a

00:05:31,040 --> 00:05:35,930
certain pug leanness you want your

00:05:34,040 --> 00:05:39,560
teacher to let you you want your model

00:05:35,930 --> 00:05:41,240
to learn the essence of the Pug so in

00:05:39,560 --> 00:05:42,980
truth there are a few ways of doing this

00:05:41,240 --> 00:05:45,020
sometimes like the example above we can

00:05:42,980 --> 00:05:46,850
tag a set of data and your model will

00:05:45,020 --> 00:05:48,740
use this supervised data set to

00:05:46,850 --> 00:05:50,690
distinguish between these classes and

00:05:48,740 --> 00:05:53,330
this will help it learn better this is

00:05:50,690 --> 00:05:55,490
called supervised machine learning but

00:05:53,330 --> 00:05:57,350
there are other times where we don't

00:05:55,490 --> 00:05:59,000
really know the answer so if I asked a

00:05:57,350 --> 00:06:00,230
computer what those pugs would look like

00:05:59,000 --> 00:06:02,810
if they were bred together with

00:06:00,230 --> 00:06:06,110
stegosauruses well that's not really a

00:06:02,810 --> 00:06:07,850
thing so there's nothing we can check

00:06:06,110 --> 00:06:11,150
against and that's when we'll use an

00:06:07,850 --> 00:06:12,680
unsupervised process but let's dive into

00:06:11,150 --> 00:06:14,300
supervised process because it's the

00:06:12,680 --> 00:06:16,670
easiest to explain and the time we have

00:06:14,300 --> 00:06:18,500
and it's also pretty common and in truth

00:06:16,670 --> 00:06:20,750
even within supervised machine learning

00:06:18,500 --> 00:06:22,510
there are thousands of algorithms that

00:06:20,750 --> 00:06:24,500
you could use and among those

00:06:22,510 --> 00:06:27,560
convolutional neural networks are

00:06:24,500 --> 00:06:29,750
probably the most popular but genetic is

00:06:27,560 --> 00:06:32,060
the easiest to understand so we're gonna

00:06:29,750 --> 00:06:35,120
use that to kind of discuss things on a

00:06:32,060 --> 00:06:37,670
high level today so let's explore a

00:06:35,120 --> 00:06:40,280
simple toy model in order to make a pug

00:06:37,670 --> 00:06:42,620
kitten sorter what we do is build an

00:06:40,280 --> 00:06:45,200
algorithm that builds algorithms and

00:06:42,620 --> 00:06:48,050
we'll build many of these and they'll do

00:06:45,200 --> 00:06:51,140
their best to sort these and do their

00:06:48,050 --> 00:06:52,850
best sorting and at first it will be bad

00:06:51,140 --> 00:06:55,940
at it they won't just be bad at it

00:06:52,850 --> 00:06:57,890
they'll be terrible but we have another

00:06:55,940 --> 00:07:00,080
function that will check this and

00:06:57,890 --> 00:07:02,960
they'll check it against that tag data

00:07:00,080 --> 00:07:05,900
and for every kitten in pug scenario so

00:07:02,960 --> 00:07:07,700
the ones that get it wrong are thrown

00:07:05,900 --> 00:07:10,460
away and the ones that get it right

00:07:07,700 --> 00:07:12,950
circle back to the Builder and then the

00:07:10,460 --> 00:07:15,920
Builder starts using those to create new

00:07:12,950 --> 00:07:18,500
builders now this might seem hacky and

00:07:15,920 --> 00:07:19,550
imperfect and that's because it is if we

00:07:18,500 --> 00:07:21,800
did this one

00:07:19,550 --> 00:07:25,520
twice or three times it would fail but

00:07:21,800 --> 00:07:27,229
we do it thousands of times and we don't

00:07:25,520 --> 00:07:29,479
do it for a small amount of conditions

00:07:27,229 --> 00:07:31,070
either we build up all of the gradients

00:07:29,479 --> 00:07:33,020
that make up an eye and then we match it

00:07:31,070 --> 00:07:35,270
up against all of the other eyes that we

00:07:33,020 --> 00:07:38,330
know of and then we match that against

00:07:35,270 --> 00:07:40,789
the likelihood of snouts and in fact our

00:07:38,330 --> 00:07:43,520
own classification system genus kingdom

00:07:40,789 --> 00:07:45,830
phylum is a really human comprehensible

00:07:43,520 --> 00:07:49,280
version composed of this kind of set of

00:07:45,830 --> 00:07:52,039
rules so the part I want you to remember

00:07:49,280 --> 00:07:55,280
is this moving forward the point where

00:07:52,039 --> 00:07:57,650
we check you can see how that's a very

00:07:55,280 --> 00:07:59,510
important part of this we have to check

00:07:57,650 --> 00:08:02,110
if we want to get the data right and we

00:07:59,510 --> 00:08:05,030
must do so and keep on checking and

00:08:02,110 --> 00:08:08,419
another really important part here is if

00:08:05,030 --> 00:08:09,860
I only gave these pug pictures that it

00:08:08,419 --> 00:08:13,910
was fed on I mean that would be correct

00:08:09,860 --> 00:08:15,560
right these are all pugs but what would

00:08:13,910 --> 00:08:18,080
what might happen is it might fail when

00:08:15,560 --> 00:08:20,479
it's trying to find our one true pug so

00:08:18,080 --> 00:08:22,940
the type of data that we train on is

00:08:20,479 --> 00:08:25,970
really vital to whether or not we have

00:08:22,940 --> 00:08:27,949
an accurate model okay

00:08:25,970 --> 00:08:29,539
so let's explore some ways that machine

00:08:27,949 --> 00:08:31,460
learning has failed but I'm gonna call

00:08:29,539 --> 00:08:33,650
this section the road to hell is paved

00:08:31,460 --> 00:08:35,750
with good intentions because none of

00:08:33,650 --> 00:08:38,270
what I'm showing you here today is made

00:08:35,750 --> 00:08:39,860
with the idea of corrupting society

00:08:38,270 --> 00:08:42,620
quite the opposite all of these are

00:08:39,860 --> 00:08:44,089
really and well intentioned examples but

00:08:42,620 --> 00:08:45,890
their failure is really good for us to

00:08:44,089 --> 00:08:48,920
explore so that we don't fail in the

00:08:45,890 --> 00:08:50,900
future so let's say you need to patrol a

00:08:48,920 --> 00:08:53,600
city but you don't have enough patrol

00:08:50,900 --> 00:08:55,520
units to completely cover the city so it

00:08:53,600 --> 00:08:57,890
might make sense to figure out where all

00:08:55,520 --> 00:08:59,839
the crime is happening and just have

00:08:57,890 --> 00:09:02,150
some patrol units that focus on the

00:08:59,839 --> 00:09:05,000
areas of most activity but the problem

00:09:02,150 --> 00:09:07,820
is not all crime is equal to get enough

00:09:05,000 --> 00:09:09,470
data you have to include nuisance crimes

00:09:07,820 --> 00:09:11,750
in which no one's hurt like homelessness

00:09:09,470 --> 00:09:14,209
or panhandling with the violent crimes

00:09:11,750 --> 00:09:16,160
like rape and murder but rape and murder

00:09:14,209 --> 00:09:18,620
don't tend to be clustered in this way

00:09:16,160 --> 00:09:20,900
it takes a lot more data to build an

00:09:18,620 --> 00:09:23,810
area which is why the nuisance data is

00:09:20,900 --> 00:09:26,540
included so on another issue is that

00:09:23,810 --> 00:09:28,820
those nuisance data areas those tend to

00:09:26,540 --> 00:09:31,160
be the poorer areas so what ends up

00:09:28,820 --> 00:09:33,079
happening is you get more patrol areas

00:09:31,160 --> 00:09:33,260
in the poorer areas and then we have

00:09:33,079 --> 00:09:35,990
more

00:09:33,260 --> 00:09:38,120
data on those patrol areas and we get

00:09:35,990 --> 00:09:40,160
into this weird feedback loop where

00:09:38,120 --> 00:09:41,900
we're just patrolling those and then the

00:09:40,160 --> 00:09:44,570
rapes and murders that happen in the

00:09:41,900 --> 00:09:46,580
affluent areas start to not be reported

00:09:44,570 --> 00:09:47,900
and this is what saw the software pred

00:09:46,580 --> 00:09:50,420
pull does that we use in the United

00:09:47,900 --> 00:09:54,530
States and it's an example of sample

00:09:50,420 --> 00:09:56,150
bias and machine learning so another

00:09:54,530 --> 00:09:57,740
issue with this that the US Department

00:09:56,150 --> 00:09:58,580
of Education has found is that once

00:09:57,740 --> 00:10:01,130
you're a criminal

00:09:58,580 --> 00:10:03,260
sorry once you're a criminal you're

00:10:01,130 --> 00:10:05,630
pretty likely to become a criminal again

00:10:03,260 --> 00:10:08,000
a lot of people keep reoffending and

00:10:05,630 --> 00:10:10,160
what they found was the reason why

00:10:08,000 --> 00:10:11,660
people do this is that they report that

00:10:10,160 --> 00:10:13,790
they find it really hard to fit back

00:10:11,660 --> 00:10:16,190
into society once that once they're in

00:10:13,790 --> 00:10:17,900
jail another thing is that employment

00:10:16,190 --> 00:10:20,150
becomes more difficult because you have

00:10:17,900 --> 00:10:21,860
to disclose that you were once in jail

00:10:20,150 --> 00:10:24,710
on your employment forms and then people

00:10:21,860 --> 00:10:27,980
are less likely to employ you but here's

00:10:24,710 --> 00:10:29,750
an important part employment tends to be

00:10:27,980 --> 00:10:32,020
the number one indicator of whether

00:10:29,750 --> 00:10:34,880
you're gonna end up back in prison and

00:10:32,020 --> 00:10:37,190
even the analysis of who ends up in the

00:10:34,880 --> 00:10:39,290
system uses machine learning so when

00:10:37,190 --> 00:10:41,210
most defendants are booked in jail in

00:10:39,290 --> 00:10:42,620
jail they respond to a coppiced

00:10:41,210 --> 00:10:45,710
questionnaire and this is fed into a

00:10:42,620 --> 00:10:47,420
system that defines the risk level for

00:10:45,710 --> 00:10:49,010
that's used in the length of their

00:10:47,420 --> 00:10:50,840
sentencing so whether you're in for a

00:10:49,010 --> 00:10:53,420
short amount of time or a long amount of

00:10:50,840 --> 00:10:56,390
time in an evaluation of more than

00:10:53,420 --> 00:10:58,550
10,000 of these people found that the

00:10:56,390 --> 00:11:00,890
risk scores showed a startling

00:10:58,550 --> 00:11:03,020
difference black defendants had scores

00:11:00,890 --> 00:11:05,810
pretty evenly matched across the board

00:11:03,020 --> 00:11:08,240
one meaning no risk 10 would have have a

00:11:05,810 --> 00:11:10,100
very long sentence whereas the white

00:11:08,240 --> 00:11:12,920
risk assessments look quite different

00:11:10,100 --> 00:11:16,280
many are assessed at one steadily degree

00:11:12,920 --> 00:11:17,930
decreasing - very few at ten now you

00:11:16,280 --> 00:11:21,260
might say well what's what if that's

00:11:17,930 --> 00:11:23,210
just valid data except upon analysis

00:11:21,260 --> 00:11:24,950
it's not really the study showed that

00:11:23,210 --> 00:11:27,890
blacks were twice as likely to be

00:11:24,950 --> 00:11:29,960
labeled higher risk and not reoffended

00:11:27,890 --> 00:11:32,600
white counterparts were labeled not as

00:11:29,960 --> 00:11:34,640
risky as they were so if we look at this

00:11:32,600 --> 00:11:36,800
in human terms we see James reveille

00:11:34,640 --> 00:11:38,600
with low risk at three Robert cannon

00:11:36,800 --> 00:11:41,090
with medium risk at six

00:11:38,600 --> 00:11:43,370
James reveille had a domestic violence

00:11:41,090 --> 00:11:45,350
aggravated assault grand theft petty

00:11:43,370 --> 00:11:46,880
theft drug trafficking and he

00:11:45,350 --> 00:11:49,540
subsequently did

00:11:46,880 --> 00:11:53,210
and Robert cannon had one petty theft

00:11:49,540 --> 00:11:55,790
there are more - Gregory Lugo low risk

00:11:53,210 --> 00:11:56,360
at one Mallory Williams medium risk at

00:11:55,790 --> 00:11:59,600
six

00:11:56,360 --> 00:12:02,750
Gregory Lugo had three duis one battery

00:11:59,600 --> 00:12:04,700
and subsequently reoffended Mallory

00:12:02,750 --> 00:12:06,770
Williams had two misdemeanors and never

00:12:04,700 --> 00:12:10,040
reoffended again and these are just a

00:12:06,770 --> 00:12:11,900
couple of them there's a lot these kind

00:12:10,040 --> 00:12:14,120
of algorithms don't just affect black

00:12:11,900 --> 00:12:16,040
people either Zilly is a construction

00:12:14,120 --> 00:12:17,900
worker who stole a piece of equipment

00:12:16,040 --> 00:12:20,120
and sold it for parts because he was low

00:12:17,900 --> 00:12:22,820
on cash since then he's been going to

00:12:20,120 --> 00:12:24,950
church he's been volunteering but he

00:12:22,820 --> 00:12:26,930
says that his risk score doesn't reflect

00:12:24,950 --> 00:12:29,390
any of the ways that he's bettering

00:12:26,930 --> 00:12:31,490
himself as a person therefore not really

00:12:29,390 --> 00:12:34,580
a good indicator of whether or not he's

00:12:31,490 --> 00:12:36,290
going to reoffending n so what makes

00:12:34,580 --> 00:12:39,110
coppice fail as a machine learning

00:12:36,290 --> 00:12:41,480
algorithm one thing is that it has no

00:12:39,110 --> 00:12:43,970
transparency you basically make the

00:12:41,480 --> 00:12:46,490
algorithm and then they are sending it

00:12:43,970 --> 00:12:48,800
out so basically the people who are

00:12:46,490 --> 00:12:51,290
using it don't know how it works

00:12:48,800 --> 00:12:52,820
another thing is because of this because

00:12:51,290 --> 00:12:54,710
of that relationship it doesn't adapt

00:12:52,820 --> 00:12:57,550
over time remember we talked about how

00:12:54,710 --> 00:12:59,870
important that step of adapting is and

00:12:57,550 --> 00:13:02,660
related to these things people use it

00:12:59,870 --> 00:13:05,120
without any questions another question

00:13:02,660 --> 00:13:07,220
that might be asked at this time is

00:13:05,120 --> 00:13:08,570
whether this is a good place for machine

00:13:07,220 --> 00:13:10,700
learning algorithms machine learning

00:13:08,570 --> 00:13:12,800
algorithms are super powerful and really

00:13:10,700 --> 00:13:15,080
wonderful but sometimes this should be

00:13:12,800 --> 00:13:16,370
in the hands of a judge and here's the

00:13:15,080 --> 00:13:18,950
main thing about machine learning

00:13:16,370 --> 00:13:20,750
algorithms we try to be predictive of

00:13:18,950 --> 00:13:23,210
the future but machine learning

00:13:20,750 --> 00:13:27,050
algorithms can really only evaluate the

00:13:23,210 --> 00:13:29,510
past so Brennan one of the co-creators

00:13:27,050 --> 00:13:31,280
of this technology didn't actually build

00:13:29,510 --> 00:13:33,170
the technology with the idea of the use

00:13:31,280 --> 00:13:35,060
for sentencing but that's the strange

00:13:33,170 --> 00:13:36,980
thing about some of this tech once you

00:13:35,060 --> 00:13:38,960
put it out in the world adoption can be

00:13:36,980 --> 00:13:41,270
beyond us anyone who maintains an

00:13:38,960 --> 00:13:42,560
open-source project will say that

00:13:41,270 --> 00:13:44,570
sometimes you put something out in the

00:13:42,560 --> 00:13:46,250
world and the way people use it is

00:13:44,570 --> 00:13:47,900
totally different than the way that you

00:13:46,250 --> 00:13:50,600
intended or the way that you had

00:13:47,900 --> 00:13:52,610
foreseen this is why it's critical for

00:13:50,600 --> 00:13:54,890
these types of technologies to be

00:13:52,610 --> 00:13:57,830
transparent in order to not be abused

00:13:54,890 --> 00:14:00,000
they can't be magical people will say

00:13:57,830 --> 00:14:03,420
algorithms can't be biased

00:14:00,000 --> 00:14:04,920
but algorithms are written by people so

00:14:03,420 --> 00:14:06,870
this doesn't always happen to criminal

00:14:04,920 --> 00:14:07,920
only happen to criminals it happens to

00:14:06,870 --> 00:14:09,270
us too

00:14:07,920 --> 00:14:10,980
machine learning is now incorporated

00:14:09,270 --> 00:14:13,530
into a lot of the tools that we use

00:14:10,980 --> 00:14:15,810
every day and a lot of our Internet

00:14:13,530 --> 00:14:18,240
experience is built off of machine

00:14:15,810 --> 00:14:21,930
learning algorithms so axiom a data

00:14:18,240 --> 00:14:24,390
brokerage had an average of 1,500 data

00:14:21,930 --> 00:14:28,350
points for 500 million consumers

00:14:24,390 --> 00:14:32,370
including the entire adult population of

00:14:28,350 --> 00:14:37,170
the United States that was in 2012 and

00:14:32,370 --> 00:14:39,060
that's just one company so using a

00:14:37,170 --> 00:14:40,920
Chrome extension called what Facebook

00:14:39,060 --> 00:14:43,200
thinks you like I could find some of the

00:14:40,920 --> 00:14:44,670
ways that the systems had tagged me so

00:14:43,200 --> 00:14:46,020
some of these are pretty obvious if you

00:14:44,670 --> 00:14:47,370
follow me on Twitter you probably know

00:14:46,020 --> 00:14:48,960
some of these things like software

00:14:47,370 --> 00:14:54,240
developer I'm into cloud computing I

00:14:48,960 --> 00:14:57,210
like dogs like wine to strange and like

00:14:54,240 --> 00:14:59,430
kind of false so mermaid I don't know

00:14:57,210 --> 00:15:00,480
trenchcoats I've never read a James

00:14:59,430 --> 00:15:02,310
Patterson book

00:15:00,480 --> 00:15:06,420
I don't knit and it even got my

00:15:02,310 --> 00:15:09,990
political proclivities wrong so the

00:15:06,420 --> 00:15:12,720
issue with this is Facebook uses these

00:15:09,990 --> 00:15:14,970
types of tags to drive directed ads and

00:15:12,720 --> 00:15:17,190
you can see you that you can use these

00:15:14,970 --> 00:15:19,800
ads to target on different demographics

00:15:17,190 --> 00:15:22,140
including race and a couple years ago

00:15:19,800 --> 00:15:23,850
people could show you different houses

00:15:22,140 --> 00:15:26,670
that you could buy depending on these

00:15:23,850 --> 00:15:28,620
demographics depending on race this is

00:15:26,670 --> 00:15:31,500
illegal under the Fair Housing Act of

00:15:28,620 --> 00:15:33,750
1968 but here's the thing

00:15:31,500 --> 00:15:37,170
once that was pointed out to Facebook

00:15:33,750 --> 00:15:39,900
they fixed it and this is a major theme

00:15:37,170 --> 00:15:42,420
here it's not to make software that

00:15:39,900 --> 00:15:44,430
never fails that's impossible every

00:15:42,420 --> 00:15:47,790
machine learning algorithm can have a

00:15:44,430 --> 00:15:49,170
failure but once it does fail you have

00:15:47,790 --> 00:15:51,660
to course-correct

00:15:49,170 --> 00:15:53,010
and change things and another problem

00:15:51,660 --> 00:15:55,080
with this that you might have already

00:15:53,010 --> 00:15:57,450
thought about is that you don't tell

00:15:55,080 --> 00:16:00,450
Facebook your race their targeting based

00:15:57,450 --> 00:16:02,190
on ethnic affinity which is used by the

00:16:00,450 --> 00:16:04,980
same algorithm that thinks I'm into

00:16:02,190 --> 00:16:07,110
mermaids so I'm not coming down on

00:16:04,980 --> 00:16:09,000
Facebook in particular this is a problem

00:16:07,110 --> 00:16:13,140
with all machine learning algorithms

00:16:09,000 --> 00:16:13,800
used by every company so this the issue

00:16:13,140 --> 00:16:15,390
with this

00:16:13,800 --> 00:16:16,680
and what you might have guessed is that

00:16:15,390 --> 00:16:19,529
everything that you're seeing is

00:16:16,680 --> 00:16:22,170
filtered and that filtration system can

00:16:19,529 --> 00:16:24,540
be fallible at best and illegal at worse

00:16:22,170 --> 00:16:26,490
it might not mean houses it can also

00:16:24,540 --> 00:16:28,230
mean that the things and the studies

00:16:26,490 --> 00:16:30,240
that you're seeing and educational

00:16:28,230 --> 00:16:32,399
material that you're seeing is based on

00:16:30,240 --> 00:16:34,290
what you already believe which like that

00:16:32,399 --> 00:16:37,829
patrolling system that we saw earlier

00:16:34,290 --> 00:16:39,870
becomes a self-reinforcing prophecy and

00:16:37,829 --> 00:16:41,490
I've also personally seen this I worked

00:16:39,870 --> 00:16:42,839
at one point I work for a company that

00:16:41,490 --> 00:16:45,600
was starting to use machine learning

00:16:42,839 --> 00:16:48,450
they proposed an idea to surface content

00:16:45,600 --> 00:16:50,640
based on demographics I had just

00:16:48,450 --> 00:16:53,040
happened to read up on that issue this

00:16:50,640 --> 00:16:54,779
that weekend mainly that it wasn't legal

00:16:53,040 --> 00:16:56,399
and I brought it up to people and they

00:16:54,779 --> 00:16:58,680
said oh yeah that you're right this

00:16:56,399 --> 00:17:00,810
could probably and introduce bias so we

00:16:58,680 --> 00:17:03,000
didn't ship that feature but here's an

00:17:00,810 --> 00:17:05,280
important part I was just a regular

00:17:03,000 --> 00:17:07,290
engineer on that project I wasn't a data

00:17:05,280 --> 00:17:09,270
scientist and I wasn't a PM I wasn't the

00:17:07,290 --> 00:17:11,699
person shaping that project I was just

00:17:09,270 --> 00:17:13,439
carrying things out but without raising

00:17:11,699 --> 00:17:16,640
my hand we might have shipped that

00:17:13,439 --> 00:17:19,319
feature to some really bad consequences

00:17:16,640 --> 00:17:21,540
it's not that digitizing the world is

00:17:19,319 --> 00:17:23,910
inherently bad but the more technology

00:17:21,540 --> 00:17:26,819
becomes embedded in all aspects of life

00:17:23,910 --> 00:17:30,419
it matters whether the technology is

00:17:26,819 --> 00:17:31,919
biased alienating or harmful and these

00:17:30,419 --> 00:17:33,600
tales are pretty dark but machine

00:17:31,919 --> 00:17:35,700
learning offers us a lot of amazing

00:17:33,600 --> 00:17:38,010
benefits as well as my brilliant

00:17:35,700 --> 00:17:39,780
co-worker Paige Bailey says it's like

00:17:38,010 --> 00:17:41,429
the risk that we take every time we get

00:17:39,780 --> 00:17:43,710
into cars or airplanes they've

00:17:41,429 --> 00:17:45,600
completely transformed every aspect of

00:17:43,710 --> 00:17:47,250
life and made things possible that

00:17:45,600 --> 00:17:49,530
wouldn't be possible before they're

00:17:47,250 --> 00:17:51,929
inherently dangerous and they can be

00:17:49,530 --> 00:17:53,429
seriously misused so think about it I

00:17:51,929 --> 00:17:54,750
mean I wouldn't be on this stage if it

00:17:53,429 --> 00:17:56,760
wasn't for airplanes and some of you

00:17:54,750 --> 00:17:58,200
wouldn't be here right all in the same

00:17:56,760 --> 00:18:00,210
room that's pretty amazing

00:17:58,200 --> 00:18:02,250
but people have used airplanes as

00:18:00,210 --> 00:18:05,010
weapons and so we have a certain respect

00:18:02,250 --> 00:18:09,000
for them okay now that I've bummed

00:18:05,010 --> 00:18:10,679
everyone out let's explore some of the

00:18:09,000 --> 00:18:12,950
wonderful applications for machine

00:18:10,679 --> 00:18:15,480
learning and what we can do with it

00:18:12,950 --> 00:18:17,250
okay as I mentioned I work for Microsoft

00:18:15,480 --> 00:18:19,440
and an azure we have a thing called

00:18:17,250 --> 00:18:22,740
cognitive services there's a specific

00:18:19,440 --> 00:18:24,900
API called the mote emotion API and what

00:18:22,740 --> 00:18:27,300
it allow us to do is analyze things like

00:18:24,900 --> 00:18:27,539
pictures and videos and see what emotion

00:18:27,300 --> 00:18:29,759
it

00:18:27,539 --> 00:18:31,710
Tech's from what the computer sees it

00:18:29,759 --> 00:18:35,369
can use to be used to make things like

00:18:31,710 --> 00:18:37,879
this which I built in three Jas which

00:18:35,369 --> 00:18:39,419
has no practical application except

00:18:37,879 --> 00:18:50,039
entertaining my four-year-old

00:18:39,419 --> 00:18:52,580
stepdaughter Wow yeah there's stuff up

00:18:50,039 --> 00:18:52,580
there huh

00:18:54,019 --> 00:19:00,479
okay which can also be pretty magical to

00:18:58,559 --> 00:19:02,759
tell you the truth but what if we put

00:19:00,479 --> 00:19:04,229
this to more useful use okay so I was

00:19:02,759 --> 00:19:06,509
mentoring a blind woman and she

00:19:04,229 --> 00:19:07,950
mentioned to me that she felt left out

00:19:06,509 --> 00:19:09,570
of a lot of conversations on Twitter

00:19:07,950 --> 00:19:12,720
because a lot of the photos didn't have

00:19:09,570 --> 00:19:15,330
alt text and this is true for her even

00:19:12,720 --> 00:19:17,340
on things like news sites so I made I

00:19:15,330 --> 00:19:19,679
used another cognitive services offering

00:19:17,340 --> 00:19:22,619
the computer vision API which will not

00:19:19,679 --> 00:19:24,929
only analyze photo but also its contents

00:19:22,619 --> 00:19:27,239
like words and just to show how some of

00:19:24,929 --> 00:19:30,389
this works we're gonna dive briefly into

00:19:27,239 --> 00:19:31,950
some of the code so we'll get the image

00:19:30,389 --> 00:19:33,720
from the user I'm using view jeaious

00:19:31,950 --> 00:19:36,119
here these are methods and we're gonna

00:19:33,720 --> 00:19:38,279
store it then you can see here we're

00:19:36,119 --> 00:19:40,200
using Axios for a simple call to the

00:19:38,279 --> 00:19:41,279
computer vision API and then we're

00:19:40,200 --> 00:19:44,729
you're gonna use address cognitive

00:19:41,279 --> 00:19:46,979
services to analyze that image and we'll

00:19:44,729 --> 00:19:49,200
do so in a couple of different ways one

00:19:46,979 --> 00:19:51,419
to see what the image contains and one

00:19:49,200 --> 00:19:53,429
to check for text within the image then

00:19:51,419 --> 00:19:55,649
we'll dynamically add the alt text to

00:19:53,429 --> 00:19:57,840
the image and now that this is built I'm

00:19:55,649 --> 00:19:59,519
building a Chrome extension so that

00:19:57,840 --> 00:20:02,669
blind people can use it on any website

00:19:59,519 --> 00:20:04,289
that they can visit visit so this

00:20:02,669 --> 00:20:06,119
application of machine learning can help

00:20:04,289 --> 00:20:09,149
bring more people into the conversation

00:20:06,119 --> 00:20:10,409
and broaden our reach it's not perfect

00:20:09,149 --> 00:20:12,059
but it's headed in a really good

00:20:10,409 --> 00:20:14,429
direction where all of a sudden people

00:20:12,059 --> 00:20:15,869
who are left out of the story can now be

00:20:14,429 --> 00:20:18,809
part of it and that's the amazing thing

00:20:15,869 --> 00:20:20,999
that by being inclusive you're actually

00:20:18,809 --> 00:20:22,470
meeting business goals it doesn't mean

00:20:20,999 --> 00:20:24,629
you're excluding the people in the

00:20:22,470 --> 00:20:27,720
center it means that you're reaching a

00:20:24,629 --> 00:20:30,149
larger audience there are all sorts of

00:20:27,720 --> 00:20:32,549
ways that other people can use machine

00:20:30,149 --> 00:20:34,590
learning in medicine too so one of these

00:20:32,549 --> 00:20:37,379
is Sophia genetics is training

00:20:34,590 --> 00:20:40,470
algorithms to diagnose diseases through

00:20:37,379 --> 00:20:41,400
DNA analysis another one that's kind of

00:20:40,470 --> 00:20:43,860
dear to my heart

00:20:41,400 --> 00:20:46,020
is that all for cure is a social network

00:20:43,860 --> 00:20:47,850
for cancer patients where patients can

00:20:46,020 --> 00:20:50,040
enter diagnosis and treatment so that

00:20:47,850 --> 00:20:53,160
can be analyzed this is really important

00:20:50,040 --> 00:20:55,170
because doctors can't just report every

00:20:53,160 --> 00:20:57,390
single patient that they have and they

00:20:55,170 --> 00:20:58,680
can't read every single study either so

00:20:57,390 --> 00:21:00,870
if you have all of these people

00:20:58,680 --> 00:21:02,880
reporting what treatments they were used

00:21:00,870 --> 00:21:04,440
whether or not they're surviving well

00:21:02,880 --> 00:21:06,390
then all of a sudden this machine

00:21:04,440 --> 00:21:08,610
learning can pick up all of that data

00:21:06,390 --> 00:21:11,910
and we we can become much smarter in

00:21:08,610 --> 00:21:13,830
finding cures for cancer health map

00:21:11,910 --> 00:21:15,750
algorithm spotted in an Ebola outbreak

00:21:13,830 --> 00:21:18,000
nine days before the World Health

00:21:15,750 --> 00:21:19,530
Organization and this is a really

00:21:18,000 --> 00:21:22,410
important one so I'm just gonna focus on

00:21:19,530 --> 00:21:24,630
for just a second nine days is an

00:21:22,410 --> 00:21:27,510
incredible amount of lead time for on an

00:21:24,630 --> 00:21:29,670
outbreak outbreaks grow exponentially so

00:21:27,510 --> 00:21:31,740
by using machine learning we adjusted

00:21:29,670 --> 00:21:36,690
the amount of people infected from here

00:21:31,740 --> 00:21:38,760
to here that's amazing so you might be

00:21:36,690 --> 00:21:40,650
saying to yourself this is all well and

00:21:38,760 --> 00:21:42,450
good but I'm not a data scientist and I

00:21:40,650 --> 00:21:45,000
don't work with machine learning but

00:21:42,450 --> 00:21:47,730
that's okay this subject that about how

00:21:45,000 --> 00:21:50,130
technology can shape our lives is really

00:21:47,730 --> 00:21:52,200
broad and it doesn't necessarily have to

00:21:50,130 --> 00:21:53,640
be machine learning so let's talk about

00:21:52,200 --> 00:21:56,160
some of the things that we as web

00:21:53,640 --> 00:21:58,980
developers can do now today with what we

00:21:56,160 --> 00:22:00,600
already know we work every day on

00:21:58,980 --> 00:22:02,790
building applications for so many

00:22:00,600 --> 00:22:04,800
different use cases so what if we take

00:22:02,790 --> 00:22:07,320
some of the more generic tools that we

00:22:04,800 --> 00:22:10,380
have and put them to tasks where we can

00:22:07,320 --> 00:22:11,820
maybe help save lives so there's a

00:22:10,380 --> 00:22:13,650
firehouse near where I live and they

00:22:11,820 --> 00:22:15,030
have some sufficiently complex systems

00:22:13,650 --> 00:22:17,040
to make sure that everything is online

00:22:15,030 --> 00:22:18,690
and functioning I went out to their

00:22:17,040 --> 00:22:21,210
station and I asked them a few questions

00:22:18,690 --> 00:22:22,650
about how everything works and to see if

00:22:21,210 --> 00:22:24,930
there was anything that I could do that

00:22:22,650 --> 00:22:27,420
I might help that might help and here's

00:22:24,930 --> 00:22:29,370
what I learned when the truck goes out

00:22:27,420 --> 00:22:31,350
it has to communicate with a dispatcher

00:22:29,370 --> 00:22:34,530
which will help facilitate if they need

00:22:31,350 --> 00:22:36,060
more water supplies or backup and the

00:22:34,530 --> 00:22:37,500
app is really simple kind of

00:22:36,060 --> 00:22:39,540
purposefully so because they want

00:22:37,500 --> 00:22:41,550
everybody able to be able to use it

00:22:39,540 --> 00:22:43,050
quickly at a glance and there aren't

00:22:41,550 --> 00:22:44,730
that many things that they might need so

00:22:43,050 --> 00:22:47,130
for their use case what they created is

00:22:44,730 --> 00:22:49,740
really perfect but I asked a few

00:22:47,130 --> 00:22:51,750
questions including what happens if

00:22:49,740 --> 00:22:54,270
they're all of a sudden offline what if

00:22:51,750 --> 00:22:55,530
they can can't communicate well right

00:22:54,270 --> 00:23:00,000
now there

00:22:55,530 --> 00:23:03,510
get it host fireman okay anyway terrible

00:23:00,000 --> 00:23:04,800
joke okay so here's our opportunity what

00:23:03,510 --> 00:23:06,750
I created was a really simplified

00:23:04,800 --> 00:23:09,030
version of their app preserving the

00:23:06,750 --> 00:23:11,070
basic layout but I made use of service

00:23:09,030 --> 00:23:13,080
workers and background sync to alert

00:23:11,070 --> 00:23:15,570
them when they're offline when they're

00:23:13,080 --> 00:23:17,640
offline cue the messages and when it's

00:23:15,570 --> 00:23:20,090
backup send them out even if the browser

00:23:17,640 --> 00:23:22,020
tab is closed even if something happens

00:23:20,090 --> 00:23:25,590
so here's what the application looks

00:23:22,020 --> 00:23:27,240
like if we click on water it register if

00:23:25,590 --> 00:23:30,960
the sink is registered and it sends it

00:23:27,240 --> 00:23:33,929
out but if we go off Wi-Fi then it'll

00:23:30,960 --> 00:23:35,910
tell them that they're off offline and

00:23:33,929 --> 00:23:39,870
then I could even close out that browser

00:23:35,910 --> 00:23:44,640
tab go back and then we can turn the

00:23:39,870 --> 00:23:46,710
Wi-Fi back on and then the sync is

00:23:44,640 --> 00:23:49,860
registered and what we requested before

00:23:46,710 --> 00:23:51,240
is then sent out so the first thing that

00:23:49,860 --> 00:23:53,070
we're going to do is alert them one that

00:23:51,240 --> 00:23:55,980
when they're offline we're gonna do that

00:23:53,070 --> 00:23:57,390
with navigator dot online and we're

00:23:55,980 --> 00:23:59,070
ready we're gonna register the service

00:23:57,390 --> 00:24:00,570
worker when a request is made the

00:23:59,070 --> 00:24:02,550
service worker isn't just a normal

00:24:00,570 --> 00:24:04,380
function if you can't just add it as a

00:24:02,550 --> 00:24:05,820
method in your app it has to be in a

00:24:04,380 --> 00:24:07,260
different JavaScript file and this

00:24:05,820 --> 00:24:10,080
details really important if you want to

00:24:07,260 --> 00:24:12,059
integrate it so one thing about this is

00:24:10,080 --> 00:24:14,190
that you're kind of throwing it over the

00:24:12,059 --> 00:24:16,200
wall it's almost like communicating with

00:24:14,190 --> 00:24:18,390
a server so you're throwing things at it

00:24:16,200 --> 00:24:20,220
and it's throwing things back so we're

00:24:18,390 --> 00:24:22,440
gonna pass in the type of event that was

00:24:20,220 --> 00:24:24,630
triggered so we can submit or queue what

00:24:22,440 --> 00:24:27,690
the user requested not shown here we'll

00:24:24,630 --> 00:24:29,460
do some error handling and then after

00:24:27,690 --> 00:24:30,870
we've activated and installed the

00:24:29,460 --> 00:24:32,880
Service Worker we're gonna strip out the

00:24:30,870 --> 00:24:34,650
type from the event tag and pass it into

00:24:32,880 --> 00:24:37,440
a function that will process a fetch

00:24:34,650 --> 00:24:39,750
request when it gets connectivity in

00:24:37,440 --> 00:24:41,760
that function will fetch our URL which

00:24:39,750 --> 00:24:44,100
is a placeholder server in this case and

00:24:41,760 --> 00:24:46,320
we'll check if the response has happened

00:24:44,100 --> 00:24:48,390
and once it has we'll send a message

00:24:46,320 --> 00:24:50,580
back to the client with post message so

00:24:48,390 --> 00:24:51,840
we'll send the we'll send the response

00:24:50,580 --> 00:24:54,870
and the type of resource that was

00:24:51,840 --> 00:24:58,590
requested and after that we can alert

00:24:54,870 --> 00:25:00,420
the user when it's finally sent but the

00:24:58,590 --> 00:25:01,980
most important part of the demo isn't

00:25:00,420 --> 00:25:04,440
the service workers it's where we

00:25:01,980 --> 00:25:06,150
started we started by asking questions I

00:25:04,440 --> 00:25:07,250
went to the firehouse and I asked them

00:25:06,150 --> 00:25:09,140
questions

00:25:07,250 --> 00:25:10,730
and there are a lot of times where I

00:25:09,140 --> 00:25:13,039
wasn't even sure if I could really help

00:25:10,730 --> 00:25:15,530
them but by asking questions about what

00:25:13,039 --> 00:25:17,990
happened in any kind of use case in any

00:25:15,530 --> 00:25:19,340
kind of scenario I can't I finally found

00:25:17,990 --> 00:25:23,179
something that they could use the help

00:25:19,340 --> 00:25:24,830
for we have to ask questions we have to

00:25:23,179 --> 00:25:27,230
ask questions in the beginning like the

00:25:24,830 --> 00:25:29,150
fire fire house demo and we have to ask

00:25:27,230 --> 00:25:31,370
questions like when it's deployed like

00:25:29,150 --> 00:25:33,679
the criminal scores we have to keep

00:25:31,370 --> 00:25:35,179
evaluating these systems and where

00:25:33,679 --> 00:25:36,620
they're going and make purposeful

00:25:35,179 --> 00:25:39,890
decisions to help people in

00:25:36,620 --> 00:25:41,600
course-correct when we don't I see a lot

00:25:39,890 --> 00:25:42,559
of tech that ask questions about how we

00:25:41,600 --> 00:25:45,380
can make our lives more comfortable

00:25:42,559 --> 00:25:47,390
which I appreciate I make apps on the

00:25:45,380 --> 00:25:51,159
weekends this is a cocktail finder I

00:25:47,390 --> 00:25:53,630
built that just finds me cocktails okay

00:25:51,159 --> 00:25:55,880
but it seems to me that a

00:25:53,630 --> 00:25:58,490
disproportionate amount of projects

00:25:55,880 --> 00:26:00,350
focus inward rather than outward focus

00:25:58,490 --> 00:26:02,720
on serving people with money instead of

00:26:00,350 --> 00:26:04,610
asking people if they need help there

00:26:02,720 --> 00:26:06,110
are a lot of opportunities out there to

00:26:04,610 --> 00:26:08,450
help and you might already have the

00:26:06,110 --> 00:26:10,460
skills to do so it just takes a little

00:26:08,450 --> 00:26:13,669
bit of imagination and exploration and

00:26:10,460 --> 00:26:16,070
it can really pay off so what I want you

00:26:13,669 --> 00:26:17,539
to think about going forward is this but

00:26:16,070 --> 00:26:20,299
I truly believe that you're some of the

00:26:17,539 --> 00:26:21,950
brightest minds of our generation as you

00:26:20,299 --> 00:26:24,500
move forward building applications for

00:26:21,950 --> 00:26:26,480
the webs the web you're building the

00:26:24,500 --> 00:26:28,820
pillars and blocks that our society

00:26:26,480 --> 00:26:30,230
rests on you're crafting the shape of

00:26:28,820 --> 00:26:32,450
our digital lives

00:26:30,230 --> 00:26:33,740
this could mean building new features

00:26:32,450 --> 00:26:35,000
this could mean advocating for

00:26:33,740 --> 00:26:36,350
accessibility and this could be as

00:26:35,000 --> 00:26:38,510
simple as raising your hand when you

00:26:36,350 --> 00:26:40,669
think something isn't right it's not

00:26:38,510 --> 00:26:42,860
just your job to create these things and

00:26:40,669 --> 00:26:45,260
do as you're told it's also your job to

00:26:42,860 --> 00:26:47,659
ask are we building things correctly are

00:26:45,260 --> 00:26:49,730
we building the right things and are we

00:26:47,659 --> 00:26:54,309
building them well for all of the

00:26:49,730 --> 00:26:54,309
digital lives that we touch thank you

00:26:56,770 --> 00:26:58,830

YouTube URL: https://www.youtube.com/watch?v=i2iCyulbnus


