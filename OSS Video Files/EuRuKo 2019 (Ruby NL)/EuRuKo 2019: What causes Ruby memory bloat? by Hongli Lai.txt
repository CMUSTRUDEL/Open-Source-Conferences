Title: EuRuKo 2019: What causes Ruby memory bloat? by Hongli Lai
Publication date: 2021-01-11
Playlist: EuRuKo 2019
Description: 
	What causes Ruby memory bloat?

Ruby apps can use a lot of memory. But why? I set out on a journey of discovery, and not only found evidence that defies common wisdom, but also a simple way to reduce memory usage by 70%.

Hongli Lai - https://twitter.com/honglilai
EuRuKo 2019
Captions: 
	00:00:05,520 --> 00:00:09,840
we'd like to take

00:00:06,480 --> 00:00:14,639
introduce you to hong li yes

00:00:09,840 --> 00:00:16,000
so um hongli has been uh well

00:00:14,639 --> 00:00:17,840
he's a creator of passenger you

00:00:16,000 --> 00:00:20,400
potentially know that

00:00:17,840 --> 00:00:22,320
i think i think that i think it's nice

00:00:20,400 --> 00:00:24,160
it's uh

00:00:22,320 --> 00:00:26,160
it made a lot of things possible for me

00:00:24,160 --> 00:00:28,880
as a as a ruby developer so i'm

00:00:26,160 --> 00:00:30,720
quite thankful for that but also nice he

00:00:28,880 --> 00:00:31,439
recently asked the community on some

00:00:30,720 --> 00:00:34,640
input

00:00:31,439 --> 00:00:37,120
on a ruby optimized sorry a memory

00:00:34,640 --> 00:00:40,160
optimized ruby distribution a few

00:00:37,120 --> 00:00:41,760
weeks ago and then people i think it was

00:00:40,160 --> 00:00:43,920
hacker news responded to it well

00:00:41,760 --> 00:00:46,079
you should take a look at ruby

00:00:43,920 --> 00:00:49,920
enterprise edition

00:00:46,079 --> 00:00:50,800
so the last version was for ruby187 so

00:00:49,920 --> 00:00:53,600
that's

00:00:50,800 --> 00:00:55,280
history and he had to remind the

00:00:53,600 --> 00:00:56,079
audience that he was the creator of the

00:00:55,280 --> 00:00:58,399
thing so

00:00:56,079 --> 00:01:00,719
he cares about memory optimization

00:00:58,399 --> 00:01:03,039
already for quite some time

00:01:00,719 --> 00:01:04,960
yeah he's oh sorry yeah no go ahead oh

00:01:03,039 --> 00:01:06,400
he also knows a little bit of japanese

00:01:04,960 --> 00:01:07,760
that he learned from anime

00:01:06,400 --> 00:01:09,520
so i guess when it comes to pirates and

00:01:07,760 --> 00:01:10,799
ninjas you fall more on the ninja side

00:01:09,520 --> 00:01:13,840
of things

00:01:10,799 --> 00:01:14,400
is that right i don't know maybe i can

00:01:13,840 --> 00:01:17,119
speak

00:01:14,400 --> 00:01:18,960
uh with uh maths a little bit in

00:01:17,119 --> 00:01:27,840
japanese if i'm lucky

00:01:18,960 --> 00:01:27,840
there you go so well then take it away

00:01:29,439 --> 00:01:32,799
thank you it's a great pleasure to be

00:01:31,520 --> 00:01:36,640
here yeah

00:01:32,799 --> 00:01:39,680
okay what calls us memory bloat

00:01:36,640 --> 00:01:42,159
as the author of passenger i've had to

00:01:39,680 --> 00:01:45,439
deal with this issue

00:01:42,159 --> 00:01:48,799
for quite some time but first

00:01:45,439 --> 00:01:52,840
let me say something and that is that

00:01:48,799 --> 00:01:55,840
this talk will be a little bit different

00:01:52,840 --> 00:01:55,840
um

00:01:55,920 --> 00:01:59,920
so some people have may have already

00:01:58,799 --> 00:02:02,479
seen my talk

00:01:59,920 --> 00:02:04,399
or read my blog about the subject and is

00:02:02,479 --> 00:02:06,799
this talk just a reiteration

00:02:04,399 --> 00:02:09,360
no i have made some surprising new

00:02:06,799 --> 00:02:12,640
discoveries lately which i will

00:02:09,360 --> 00:02:14,800
talk about during this presentation

00:02:12,640 --> 00:02:17,280
and some new discoveries were made as

00:02:14,800 --> 00:02:18,959
late as monday on 1800

00:02:17,280 --> 00:02:21,440
well i also had a deadline for

00:02:18,959 --> 00:02:22,080
submitting this talk to euruco so that

00:02:21,440 --> 00:02:24,800
they can live

00:02:22,080 --> 00:02:25,440
caption it so i was working like crazy

00:02:24,800 --> 00:02:28,480
um

00:02:25,440 --> 00:02:30,879
for the past week to to rush well you

00:02:28,480 --> 00:02:32,480
can imagine how busy i must have been

00:02:30,879 --> 00:02:33,920
and at the end of the talk i will also

00:02:32,480 --> 00:02:35,840
have a nice announcement

00:02:33,920 --> 00:02:38,000
well it's not a surprise anymore for

00:02:35,840 --> 00:02:41,040
some people but uh still

00:02:38,000 --> 00:02:44,400
worth talking about so stay tuned

00:02:41,040 --> 00:02:46,480
so at fusion we have a proxy server

00:02:44,400 --> 00:02:47,840
written in ruby and it serves debian and

00:02:46,480 --> 00:02:49,280
rpm packages

00:02:47,840 --> 00:02:50,879
the server doesn't do much it's

00:02:49,280 --> 00:02:52,720
literally just a proxy

00:02:50,879 --> 00:02:54,319
makes a call to another server and then

00:02:52,720 --> 00:02:56,560
returns the result

00:02:54,319 --> 00:02:57,599
um with some slight modifications in the

00:02:56,560 --> 00:02:58,879
headers

00:02:57,599 --> 00:03:00,879
and then i noticed that the

00:02:58,879 --> 00:03:04,000
application's memory usage would

00:03:00,879 --> 00:03:05,120
balloon to 1.3 gigabyte over time and i

00:03:04,000 --> 00:03:08,239
thought well this is

00:03:05,120 --> 00:03:09,120
crazy it's just a very simple uh ruby

00:03:08,239 --> 00:03:12,000
application

00:03:09,120 --> 00:03:12,800
in less than a thousand lines of code

00:03:12,000 --> 00:03:14,319
and there's no

00:03:12,800 --> 00:03:16,000
reason why it should use that much

00:03:14,319 --> 00:03:16,560
memory it couldn't also couldn't

00:03:16,000 --> 00:03:18,560
possibly

00:03:16,560 --> 00:03:21,200
have a memory leak so why is the memory

00:03:18,560 --> 00:03:24,720
usage growing that much

00:03:21,200 --> 00:03:27,680
i have heard rumors before about

00:03:24,720 --> 00:03:28,400
ruby memory bloating and their causes in

00:03:27,680 --> 00:03:30,640
fact

00:03:28,400 --> 00:03:31,440
ruby's speed consultant and muppuma

00:03:30,640 --> 00:03:34,560
maintainer

00:03:31,440 --> 00:03:35,200
nate bergelbeck as well as hiroko they

00:03:34,560 --> 00:03:38,159
both have

00:03:35,200 --> 00:03:39,200
write-ups about this issue they

00:03:38,159 --> 00:03:42,480
basically state

00:03:39,200 --> 00:03:46,080
that memory bloating

00:03:42,480 --> 00:03:48,319
is caused by memory fragmentation

00:03:46,080 --> 00:03:50,799
and there are two proposed solutions by

00:03:48,319 --> 00:03:54,000
them you either set a magical

00:03:50,799 --> 00:03:56,640
environment variable

00:03:54,000 --> 00:03:58,560
or you override the operating systems

00:03:56,640 --> 00:04:01,360
memory allocator and use a custom one

00:03:58,560 --> 00:04:04,319
called je mallock

00:04:01,360 --> 00:04:06,319
so i implemented what they said and then

00:04:04,319 --> 00:04:10,000
my memory usage was reduced by more than

00:04:06,319 --> 00:04:10,000
half the ends any questions

00:04:11,519 --> 00:04:16,879
yeah yeah if only it was that simple

00:04:16,959 --> 00:04:22,479
their solutions indeed work but i um

00:04:20,079 --> 00:04:23,840
i read the explanation of why it happens

00:04:22,479 --> 00:04:27,440
and then something just

00:04:23,840 --> 00:04:28,320
fell off based on my understanding of

00:04:27,440 --> 00:04:31,520
how memory

00:04:28,320 --> 00:04:32,880
works their explanation seemed a bit

00:04:31,520 --> 00:04:35,440
incomplete

00:04:32,880 --> 00:04:36,720
and so i started researching and in this

00:04:35,440 --> 00:04:37,440
talk i will be talking about three

00:04:36,720 --> 00:04:40,160
things

00:04:37,440 --> 00:04:41,280
first is how does memory allocation work

00:04:40,160 --> 00:04:44,000
uh the very basic

00:04:41,280 --> 00:04:45,600
concepts second one is why does this

00:04:44,000 --> 00:04:45,919
bloating happen and then the third one

00:04:45,600 --> 00:04:49,440
is

00:04:45,919 --> 00:04:49,440
what can we do about it

00:04:49,520 --> 00:04:52,639
and one important thing to mention is

00:04:51,600 --> 00:04:54,639
that this

00:04:52,639 --> 00:04:56,880
problem that i'm describing it only

00:04:54,639 --> 00:04:58,400
manifests itself in multi-threaded ruby

00:04:56,880 --> 00:05:00,000
apps and only on linux

00:04:58,400 --> 00:05:01,440
so it doesn't happen on mac it doesn't

00:05:00,000 --> 00:05:03,120
happen on freebsd

00:05:01,440 --> 00:05:05,600
and if you do run on linux if you don't

00:05:03,120 --> 00:05:06,479
use threads then you also don't see this

00:05:05,600 --> 00:05:08,160
problem

00:05:06,479 --> 00:05:10,240
but it's kind of ironic because

00:05:08,160 --> 00:05:12,720
multi-threading is supposed to give

00:05:10,240 --> 00:05:14,960
us more concurrency with little added

00:05:12,720 --> 00:05:18,560
memory usage

00:05:14,960 --> 00:05:19,520
and i should also note that ruby is not

00:05:18,560 --> 00:05:21,759
the only

00:05:19,520 --> 00:05:23,680
issue the only application that suffers

00:05:21,759 --> 00:05:26,000
from this issue for example

00:05:23,680 --> 00:05:26,880
redis also had this issue other kinds of

00:05:26,000 --> 00:05:30,400
servers also

00:05:26,880 --> 00:05:33,759
had this issue so it's not

00:05:30,400 --> 00:05:37,120
entirely ruby specific

00:05:33,759 --> 00:05:37,120
memory allocation 101

00:05:37,680 --> 00:05:41,280
let's begin with the basics the

00:05:39,440 --> 00:05:43,840
operating system comes

00:05:41,280 --> 00:05:45,680
with an uh with a library called the

00:05:43,840 --> 00:05:47,759
memory allocator

00:05:45,680 --> 00:05:49,440
and this component is what keeps track

00:05:47,759 --> 00:05:51,919
of what memory is in use

00:05:49,440 --> 00:05:53,520
and what is available it has a very

00:05:51,919 --> 00:05:56,800
simple api

00:05:53,520 --> 00:05:57,360
the malloc function allocates a key

00:05:56,800 --> 00:06:00,160
piece of

00:05:57,360 --> 00:06:00,960
memory of the requested size and then it

00:06:00,160 --> 00:06:03,440
returns

00:06:00,960 --> 00:06:04,240
the memory address of that allocation or

00:06:03,440 --> 00:06:07,520
it returns

00:06:04,240 --> 00:06:08,560
an error the free function frees the

00:06:07,520 --> 00:06:10,639
memory piece

00:06:08,560 --> 00:06:13,120
that that was allocated at the given

00:06:10,639 --> 00:06:13,120
address

00:06:13,600 --> 00:06:18,000
but the memory allocator actually

00:06:16,080 --> 00:06:21,039
consists of two parts

00:06:18,000 --> 00:06:24,160
that work very differently one

00:06:21,039 --> 00:06:27,680
is the the application side called

00:06:24,160 --> 00:06:30,880
user space and the other one is the os

00:06:27,680 --> 00:06:33,280
kernel so the linux kernel in in

00:06:30,880 --> 00:06:34,000
in this case the linux kernel is the

00:06:33,280 --> 00:06:36,800
closest to

00:06:34,000 --> 00:06:38,479
the hardware the memory allocator is a

00:06:36,800 --> 00:06:41,280
library in user space

00:06:38,479 --> 00:06:42,080
and it calls a kernel api to actually

00:06:41,280 --> 00:06:45,280
allocate

00:06:42,080 --> 00:06:46,960
memory from the hardware

00:06:45,280 --> 00:06:49,280
the kernel can only allocate memory in

00:06:46,960 --> 00:06:52,000
blocks of 4 kilobytes

00:06:49,280 --> 00:06:53,039
and such a block is called a page there

00:06:52,000 --> 00:06:55,840
are also other

00:06:53,039 --> 00:06:58,080
unrelated things that are called pages

00:06:55,840 --> 00:07:01,120
so for clarity i will use the term

00:06:58,080 --> 00:07:01,120
os page

00:07:01,199 --> 00:07:05,520
dos can only allocate or free uh memory

00:07:04,639 --> 00:07:09,440
in units of

00:07:05,520 --> 00:07:11,360
entire os pages and the reason for this

00:07:09,440 --> 00:07:12,400
is complicated so i won't go into the

00:07:11,360 --> 00:07:14,800
details

00:07:12,400 --> 00:07:16,240
but suffice to say that this is a

00:07:14,800 --> 00:07:20,000
fundamental property

00:07:16,240 --> 00:07:20,000
of all modern kernels

00:07:20,960 --> 00:07:26,160
the user space memory allocator can be

00:07:23,440 --> 00:07:28,800
used to allocate

00:07:26,160 --> 00:07:31,039
memory pieces of any size so how does

00:07:28,800 --> 00:07:32,639
that fit with the constraints that

00:07:31,039 --> 00:07:34,160
the kernel can only allocate in four

00:07:32,639 --> 00:07:37,280
kilobyte blocks

00:07:34,160 --> 00:07:37,280
well um

00:07:37,840 --> 00:07:41,520
it does something smart with that and

00:07:40,000 --> 00:07:43,199
and there's also the thing that asking

00:07:41,520 --> 00:07:44,720
the kernels to allocate memory it's an

00:07:43,199 --> 00:07:47,120
expensive operation

00:07:44,720 --> 00:07:48,879
so you don't want to do it too often the

00:07:47,120 --> 00:07:51,039
memory allocator allocates

00:07:48,879 --> 00:07:52,720
large pieces of memory from the kernel

00:07:51,039 --> 00:07:55,199
and then defies those pieces

00:07:52,720 --> 00:07:56,639
in in smaller ones to serve its own

00:07:55,199 --> 00:07:59,120
colors

00:07:56,639 --> 00:08:00,240
so here's an example interaction an

00:07:59,120 --> 00:08:03,680
application

00:08:00,240 --> 00:08:05,840
for example ruby asks the memory

00:08:03,680 --> 00:08:08,800
allocator to allocate something

00:08:05,840 --> 00:08:10,479
and then the memory allocator asks the

00:08:08,800 --> 00:08:10,960
kernel to allocate something that is

00:08:10,479 --> 00:08:13,840
usually

00:08:10,960 --> 00:08:15,120
much bigger so the kernel gives back um

00:08:13,840 --> 00:08:18,960
a large bag of

00:08:15,120 --> 00:08:22,240
stuff and then the memory allocator will

00:08:18,960 --> 00:08:24,080
divide parts of that for its own colors

00:08:22,240 --> 00:08:26,319
what is allocated from the kernel is

00:08:24,080 --> 00:08:29,440
called an it's called a heap

00:08:26,319 --> 00:08:31,199
and likewise there are also unrelated

00:08:29,440 --> 00:08:32,240
things are also called heaps so i will

00:08:31,199 --> 00:08:35,519
use the term os

00:08:32,240 --> 00:08:37,919
heap for clarity so the memory allocator

00:08:35,519 --> 00:08:39,760
carves out a piece of the requested size

00:08:37,919 --> 00:08:41,599
from what is allocated from the kernel

00:08:39,760 --> 00:08:44,800
and then returns its address

00:08:41,599 --> 00:08:47,360
to the caller and as long as there is

00:08:44,800 --> 00:08:48,560
still space left in an os heap the

00:08:47,360 --> 00:08:51,120
memory allocator will

00:08:48,560 --> 00:08:53,519
continue to carve out free spaces from

00:08:51,120 --> 00:08:56,160
it and only when it is full

00:08:53,519 --> 00:08:57,760
and and no new allocation requests can

00:08:56,160 --> 00:09:00,080
be satisfied with an osce

00:08:57,760 --> 00:09:02,399
would allocate a new os heap from the

00:09:00,080 --> 00:09:04,959
kernel

00:09:02,399 --> 00:09:05,680
so that's it for memory allocation 101

00:09:04,959 --> 00:09:07,279
but

00:09:05,680 --> 00:09:09,600
things are actually more complicated

00:09:07,279 --> 00:09:12,880
because on the ruby side

00:09:09,600 --> 00:09:15,839
ruby manages its memory in

00:09:12,880 --> 00:09:15,839
its own manner

00:09:16,080 --> 00:09:21,760
so there are ruby objects and

00:09:19,279 --> 00:09:22,959
ruby does not allocate each object

00:09:21,760 --> 00:09:25,120
separately

00:09:22,959 --> 00:09:27,760
with an object i mean a string an array

00:09:25,120 --> 00:09:30,959
or a cl even a class or a reg x

00:09:27,760 --> 00:09:31,839
those are all objects and it is very

00:09:30,959 --> 00:09:34,880
expensive

00:09:31,839 --> 00:09:37,600
if you call malloc on each object

00:09:34,880 --> 00:09:38,640
separately that also has that has a

00:09:37,600 --> 00:09:42,080
large

00:09:38,640 --> 00:09:43,440
space overhead and a large time overhead

00:09:42,080 --> 00:09:47,360
so you don't want to do that

00:09:43,440 --> 00:09:48,320
and instead ruby ruby uses a strategy

00:09:47,360 --> 00:09:50,800
similar to

00:09:48,320 --> 00:09:52,560
what the memory allocator also does and

00:09:50,800 --> 00:09:54,959
that is it will request memory

00:09:52,560 --> 00:09:56,000
from the memory allocator in large

00:09:54,959 --> 00:09:58,800
groups

00:09:56,000 --> 00:10:00,080
and in in this case a group that is

00:09:58,800 --> 00:10:03,279
allocated by ruby

00:10:00,080 --> 00:10:04,000
is called a ruby heap page which by the

00:10:03,279 --> 00:10:06,640
way have got

00:10:04,000 --> 00:10:09,440
nothing to do with os heap or no or with

00:10:06,640 --> 00:10:09,440
os pages

00:10:10,399 --> 00:10:14,880
a ruby heap page is split into equal

00:10:12,480 --> 00:10:18,480
size slots

00:10:14,880 --> 00:10:21,920
and one ruby object occupies a one slot

00:10:18,480 --> 00:10:24,320
whether it is a string an array

00:10:21,920 --> 00:10:25,279
a class or any other objects it occupies

00:10:24,320 --> 00:10:28,640
one slot

00:10:25,279 --> 00:10:30,839
some slots could be as of yet unoccupied

00:10:28,640 --> 00:10:32,160
and rupee stores any new objects in

00:10:30,839 --> 00:10:35,760
there

00:10:32,160 --> 00:10:38,959
a slot is about 40 kilobytes

00:10:35,760 --> 00:10:39,360
and most ruby objects fit in there but

00:10:38,959 --> 00:10:41,519
not

00:10:39,360 --> 00:10:43,120
all of them do for example if you have

00:10:41,519 --> 00:10:44,640
long strings or long arrays

00:10:43,120 --> 00:10:46,399
what do you do if that string is larger

00:10:44,640 --> 00:10:48,800
than 40 kilobytes well

00:10:46,399 --> 00:10:50,720
it distorts separately it is allocated

00:10:48,800 --> 00:10:52,959
separately from the memory allocator

00:10:50,720 --> 00:10:54,399
and then in the object slot there will

00:10:52,959 --> 00:10:55,279
be a pointer to that separately

00:10:54,399 --> 00:10:58,480
allocated

00:10:55,279 --> 00:11:00,480
data that is larger and

00:10:58,480 --> 00:11:01,839
if the object is destroyed by the

00:11:00,480 --> 00:11:04,000
garbage collector

00:11:01,839 --> 00:11:06,560
then not only is that slot marked free

00:11:04,000 --> 00:11:08,800
but also the the any external data

00:11:06,560 --> 00:11:11,760
that was pointed to by that slot will

00:11:08,800 --> 00:11:11,760
also be freed

00:11:12,000 --> 00:11:15,600
so to sum up there are roughly two types

00:11:14,480 --> 00:11:18,959
of memory

00:11:15,600 --> 00:11:20,880
uh in a ruby process first there are

00:11:18,959 --> 00:11:22,399
ruby heap pages with their slots and

00:11:20,880 --> 00:11:25,760
ruby objects

00:11:22,399 --> 00:11:29,760
and second is memory stored outside of

00:11:25,760 --> 00:11:31,760
ruby heap pages

00:11:29,760 --> 00:11:33,920
and all that memory was allocated from

00:11:31,760 --> 00:11:36,160
the memory allocator library

00:11:33,920 --> 00:11:37,760
so here i have a question for the

00:11:36,160 --> 00:11:39,680
audience

00:11:37,760 --> 00:11:41,680
i have a memory benchmarking program

00:11:39,680 --> 00:11:42,720
that does nothing but allocating memory

00:11:41,680 --> 00:11:45,560
in a loop

00:11:42,720 --> 00:11:47,040
after a while memory usage balloons to

00:11:45,560 --> 00:11:49,920
00:11:47,040 --> 00:11:51,839
megabytes and yeah the benchmarking app

00:11:49,920 --> 00:11:54,560
is multi-threaded

00:11:51,839 --> 00:11:55,600
so here's a question how many percent of

00:11:54,560 --> 00:11:58,160
the

00:11:55,600 --> 00:11:59,200
megabytes belongs to the left side and

00:11:58,160 --> 00:12:01,760
how many percent

00:11:59,200 --> 00:12:03,360
belongs to the right side this is a

00:12:01,760 --> 00:12:07,200
multiple choice question

00:12:03,360 --> 00:12:09,200
hooding who think it looks like scenario

00:12:07,200 --> 00:12:11,200
a where most of the memory memories

00:12:09,200 --> 00:12:13,440
occupied by the left side put up your

00:12:11,200 --> 00:12:13,440
hand

00:12:13,760 --> 00:12:19,920
who thinks it's like 50 50.

00:12:17,519 --> 00:12:21,120
grab your hand and who thinks it's like

00:12:19,920 --> 00:12:24,639
um

00:12:21,120 --> 00:12:25,920
it's like c okay the majority of people

00:12:24,639 --> 00:12:29,440
think it's like c

00:12:25,920 --> 00:12:32,880
are you confident that was a trick

00:12:29,440 --> 00:12:32,880
question you have all been fooled

00:12:33,200 --> 00:12:37,120
there is a third category

00:12:37,279 --> 00:12:41,839
that um turns out to be the maturity of

00:12:40,639 --> 00:12:45,839
the memory usage

00:12:41,839 --> 00:12:45,839
in my benchmark application

00:12:48,160 --> 00:12:55,519
okay let's sink in for a bit my bench

00:12:52,480 --> 00:12:56,720
my my benchmarking application uses 230

00:12:55,519 --> 00:13:00,160
megabytes

00:12:56,720 --> 00:13:01,680
and ruby knows about seven megabytes so

00:13:00,160 --> 00:13:05,519
the seven megabytes is

00:13:01,680 --> 00:13:09,360
uh all the ruby objects

00:13:05,519 --> 00:13:11,920
of the ruby heap pages as well as any

00:13:09,360 --> 00:13:13,839
external data that each object points to

00:13:11,920 --> 00:13:16,399
so including string contents

00:13:13,839 --> 00:13:17,519
all of that is only seven megabytes give

00:13:16,399 --> 00:13:19,920
or take

00:13:17,519 --> 00:13:21,920
and yet the operating system memory

00:13:19,920 --> 00:13:25,120
measurement still tell me that

00:13:21,920 --> 00:13:27,440
that the process uses 230 megabytes

00:13:25,120 --> 00:13:28,560
so that red category in the previous

00:13:27,440 --> 00:13:31,760
slide

00:13:28,560 --> 00:13:34,800
it's very very big what is this

00:13:31,760 --> 00:13:36,240
it's insane it's something

00:13:34,800 --> 00:13:39,120
it's something that ruby does not

00:13:36,240 --> 00:13:40,880
control okay so people in the past

00:13:39,120 --> 00:13:42,880
tended to blame ruby for being memory

00:13:40,880 --> 00:13:43,600
inefficient or that it doesn't compact

00:13:42,880 --> 00:13:46,639
memory

00:13:43,600 --> 00:13:47,680
well some of them might there's some

00:13:46,639 --> 00:13:51,040
truth in that but

00:13:47,680 --> 00:13:52,639
it looks like for for quite a large part

00:13:51,040 --> 00:13:56,480
it's not ruby's fault

00:13:52,639 --> 00:13:56,480
so it can go free i hope

00:13:57,839 --> 00:14:03,199
but what is going on where does the

00:14:00,399 --> 00:14:05,120
memory bloat actually come from

00:14:03,199 --> 00:14:07,279
let's go back to the magical solutions

00:14:05,120 --> 00:14:10,160
that nate and heroku suggested

00:14:07,279 --> 00:14:12,240
why do they work this environment

00:14:10,160 --> 00:14:14,639
variable for example reduces memory

00:14:12,240 --> 00:14:17,120
usage significantly

00:14:14,639 --> 00:14:19,600
what does this variable do and why does

00:14:17,120 --> 00:14:19,600
that help

00:14:19,760 --> 00:14:24,160
well i dived into the documentation of

00:14:22,160 --> 00:14:25,680
the memory allocator but also the source

00:14:24,160 --> 00:14:28,399
code to figure out what it does

00:14:25,680 --> 00:14:30,320
and here's a summary and it has got to

00:14:28,399 --> 00:14:32,079
do with multi-threading

00:14:30,320 --> 00:14:33,920
when multiple threads try to allocate

00:14:32,079 --> 00:14:34,959
memory from the same os heap at the same

00:14:33,920 --> 00:14:38,720
time

00:14:34,959 --> 00:14:40,720
they content for access only one thread

00:14:38,720 --> 00:14:43,199
can perform an allocation

00:14:40,720 --> 00:14:44,720
at a time because otherwise it will not

00:14:43,199 --> 00:14:47,519
be thread safe

00:14:44,720 --> 00:14:49,040
but if you do that then you also reduce

00:14:47,519 --> 00:14:51,040
multi-threaded memory allocation

00:14:49,040 --> 00:14:53,600
performance

00:14:51,040 --> 00:14:55,440
so the writers of the memory allocator

00:14:53,600 --> 00:14:56,240
they come up with a solution for that in

00:14:55,440 --> 00:15:00,240
order to

00:14:56,240 --> 00:15:02,240
improve multi-performance

00:15:00,240 --> 00:15:03,440
the memory allocator tries to create

00:15:02,240 --> 00:15:05,600
multiple os

00:15:03,440 --> 00:15:07,040
heaps and tries to assign different

00:15:05,600 --> 00:15:10,240
threats to its own

00:15:07,040 --> 00:15:12,240
os heap and most of the time a threat

00:15:10,240 --> 00:15:13,199
only needs to work with that particular

00:15:12,240 --> 00:15:15,760
os heap

00:15:13,199 --> 00:15:18,320
therefore thereby avoiding contention

00:15:15,760 --> 00:15:21,360
with other threats

00:15:18,320 --> 00:15:23,600
in fact the maximum number of os heaps

00:15:21,360 --> 00:15:24,959
allocated in such a fashion is by

00:15:23,600 --> 00:15:26,880
default

00:15:24,959 --> 00:15:28,160
equal to eight times the number of

00:15:26,880 --> 00:15:31,839
virtual cpus

00:15:28,160 --> 00:15:34,399
so suppose i have my humble macbook with

00:15:31,839 --> 00:15:35,040
two uh two cores and two hybrid threads

00:15:34,399 --> 00:15:38,079
each

00:15:35,040 --> 00:15:41,519
that's 32 os heaps

00:15:38,079 --> 00:15:43,519
if i were to run linux on that laptop

00:15:41,519 --> 00:15:44,639
okay so there are lots of os heaps so

00:15:43,519 --> 00:15:48,320
what why does that

00:15:44,639 --> 00:15:51,040
necessarily lead to more memory usage

00:15:48,320 --> 00:15:53,360
one would expect that with more os heaps

00:15:51,040 --> 00:15:55,680
each os heap is less heavily utilized

00:15:53,360 --> 00:15:56,800
so there should not be that much uh

00:15:55,680 --> 00:15:58,800
difference in

00:15:56,800 --> 00:15:59,839
total memory usage it's like you have a

00:15:58,800 --> 00:16:02,079
bucket of water

00:15:59,839 --> 00:16:03,519
and you divide that water in multiple

00:16:02,079 --> 00:16:05,120
buckets but it's still the same amount

00:16:03,519 --> 00:16:06,959
of water

00:16:05,120 --> 00:16:09,920
well to further research this i had to

00:16:06,959 --> 00:16:12,959
gain a better understanding of how

00:16:09,920 --> 00:16:14,880
the os heaps look like maybe it's in the

00:16:12,959 --> 00:16:16,959
way that they are internally organized

00:16:14,880 --> 00:16:19,279
that causes memory bloat

00:16:16,959 --> 00:16:21,600
unfortunately there are no tools that

00:16:19,279 --> 00:16:26,800
allow me to inspect os heaps in

00:16:21,600 --> 00:16:26,800
such a manner so i wrote my own

00:16:27,279 --> 00:16:31,040
first i had to dump the layout of the os

00:16:29,199 --> 00:16:33,440
heap somehow

00:16:31,040 --> 00:16:34,800
and i was kind of forced to dive into

00:16:33,440 --> 00:16:36,880
the memory allocator

00:16:34,800 --> 00:16:39,600
source code to figure out okay what does

00:16:36,880 --> 00:16:42,720
it do how does it lay out memory

00:16:39,600 --> 00:16:45,279
and then next i wrote a library

00:16:42,720 --> 00:16:48,240
that traverses those data structures and

00:16:45,279 --> 00:16:51,279
writes the layout to a file

00:16:48,240 --> 00:16:54,320
and then finally i wrote a tool

00:16:51,279 --> 00:16:56,639
that takes such a file as input

00:16:54,320 --> 00:16:57,360
and then compiles a visualization in the

00:16:56,639 --> 00:17:01,120
form of

00:16:57,360 --> 00:17:04,559
html and png images

00:17:01,120 --> 00:17:06,319
and then it looks like this

00:17:04,559 --> 00:17:08,400
here is a visualization of one

00:17:06,319 --> 00:17:11,520
particular os heaps and there are

00:17:08,400 --> 00:17:12,640
multiple oil heaps you see that

00:17:11,520 --> 00:17:15,199
you see small blocks in this

00:17:12,640 --> 00:17:16,480
visualization those represent the os

00:17:15,199 --> 00:17:18,799
pages

00:17:16,480 --> 00:17:21,120
red areas are memory locations that are

00:17:18,799 --> 00:17:24,240
in use by the memory allocator

00:17:21,120 --> 00:17:27,520
gray areas are free locations but not

00:17:24,240 --> 00:17:31,440
free not released back to the kernel so

00:17:27,520 --> 00:17:35,120
those are locations that are part of um

00:17:31,440 --> 00:17:37,600
oh uh os heaps that have not

00:17:35,120 --> 00:17:39,520
been given back and you don't see any

00:17:37,600 --> 00:17:40,240
white blocks here but uh white blocks do

00:17:39,520 --> 00:17:42,320
exist

00:17:40,240 --> 00:17:44,160
white means that that location is not

00:17:42,320 --> 00:17:48,160
allocated by the kernel so it is

00:17:44,160 --> 00:17:48,799
fully free and well what does this tell

00:17:48,160 --> 00:17:51,200
us

00:17:48,799 --> 00:17:52,400
first there is fragmentation you see

00:17:51,200 --> 00:17:56,559
that the red spots

00:17:52,400 --> 00:17:58,480
are scattered uh and some os pages are

00:17:56,559 --> 00:18:00,559
only half red so for for example this

00:17:58,480 --> 00:18:06,559
one only tiny bit of red but

00:18:00,559 --> 00:18:06,559
the rest is gray that is fragmentation

00:18:07,679 --> 00:18:10,799
an os page that is only partially

00:18:09,840 --> 00:18:13,280
occupied

00:18:10,799 --> 00:18:14,320
still uses four kilobytes so there's

00:18:13,280 --> 00:18:16,880
some memory waste

00:18:14,320 --> 00:18:19,840
there and yeah some people blame

00:18:16,880 --> 00:18:23,760
fragmentation as a cause of memory bloat

00:18:19,840 --> 00:18:26,080
and second most os heaps

00:18:23,760 --> 00:18:29,679
look like this there's a significant

00:18:26,080 --> 00:18:32,720
number of entirely gray

00:18:29,679 --> 00:18:36,320
os pages without any red inside

00:18:32,720 --> 00:18:39,200
in fact i would say that the

00:18:36,320 --> 00:18:42,400
entirely gray blocks are the form the

00:18:39,200 --> 00:18:44,160
majority of this visualization

00:18:42,400 --> 00:18:46,320
and i also compared the visualizations

00:18:44,160 --> 00:18:49,600
between a normal run and a run with

00:18:46,320 --> 00:18:51,360
malloc arena maxis2 that magical

00:18:49,600 --> 00:18:53,679
environment variable that would reduce

00:18:51,360 --> 00:18:56,000
memory usage what is the difference

00:18:53,679 --> 00:18:56,720
well the visualization of the latter

00:18:56,000 --> 00:18:59,200
reveals

00:18:56,720 --> 00:19:00,799
a lot less gray almost everything is red

00:18:59,200 --> 00:19:01,679
but why is that why is there such a big

00:19:00,799 --> 00:19:04,799
difference between

00:19:01,679 --> 00:19:04,799
the two scenarios

00:19:05,280 --> 00:19:08,720
well it's like this the gray os pages

00:19:07,280 --> 00:19:09,440
are not released back to the kernel

00:19:08,720 --> 00:19:12,160
because they are

00:19:09,440 --> 00:19:12,799
reused by the memory allocator to

00:19:12,160 --> 00:19:15,280
satisfy

00:19:12,799 --> 00:19:16,240
future allocation requests i talked

00:19:15,280 --> 00:19:19,200
about how that

00:19:16,240 --> 00:19:19,919
it is expensive to allocate memory from

00:19:19,200 --> 00:19:22,960
the kernel

00:19:19,919 --> 00:19:24,640
and so the memory allocator will try to

00:19:22,960 --> 00:19:27,600
cache these things and keep them around

00:19:24,640 --> 00:19:27,600
for later usage

00:19:31,039 --> 00:19:36,840
if you set malloc arena maxis2

00:19:34,400 --> 00:19:38,000
then a lot of the gray os pages they get

00:19:36,840 --> 00:19:40,880
reused

00:19:38,000 --> 00:19:41,919
and so you don't get a lot of waste with

00:19:40,880 --> 00:19:45,280
a normal run

00:19:41,919 --> 00:19:48,799
because there are so many os heaps

00:19:45,280 --> 00:19:50,160
the chance that an os page gets reused

00:19:48,799 --> 00:19:55,919
becomes lower

00:19:50,160 --> 00:19:55,919
and so um and so there is more waste

00:19:57,120 --> 00:20:02,240
but what can we do about this i um i

00:20:00,400 --> 00:20:05,200
researched the memory allocator's

00:20:02,240 --> 00:20:08,159
source code a bit more and it turns out

00:20:05,200 --> 00:20:08,159
that um

00:20:08,240 --> 00:20:12,799
by default the memory allocator only

00:20:10,159 --> 00:20:15,360
releases os pages back to the kernel

00:20:12,799 --> 00:20:16,880
at the end of an os heap and even that

00:20:15,360 --> 00:20:18,559
is done occasionally

00:20:16,880 --> 00:20:20,559
but that's not good because as we saw in

00:20:18,559 --> 00:20:22,799
the visualizations lots of those

00:20:20,559 --> 00:20:26,400
gray blocks there somewhere in the

00:20:22,799 --> 00:20:26,400
middle definitely not at the end

00:20:26,799 --> 00:20:30,720
but i found a magic trick and there is

00:20:29,280 --> 00:20:33,679
an api

00:20:30,720 --> 00:20:35,120
in the linux memory allocator called

00:20:33,679 --> 00:20:38,000
malloc trim

00:20:35,120 --> 00:20:40,880
that forces it to release grey blocks

00:20:38,000 --> 00:20:40,880
back to the kernel

00:20:41,120 --> 00:20:46,400
what happens if we modify ruby to call

00:20:43,200 --> 00:20:49,520
this function during garbage collection

00:20:46,400 --> 00:20:51,039
well the result is profound memory usage

00:20:49,520 --> 00:20:55,360
becomes almost as low

00:20:51,039 --> 00:20:57,919
as setting malloc arena max s2

00:20:55,360 --> 00:21:00,320
and here's a visualization you can see

00:20:57,919 --> 00:21:03,280
that a lot of white holes have appeared

00:21:00,320 --> 00:21:03,840
where previously they were completely

00:21:03,280 --> 00:21:06,720
gray

00:21:03,840 --> 00:21:09,520
and that is the source of the memory

00:21:06,720 --> 00:21:12,880
usage reduction

00:21:09,520 --> 00:21:12,880
so in conclusion

00:21:13,280 --> 00:21:18,720
fragmentation turns out to be

00:21:16,320 --> 00:21:20,480
at least for a part a red herring it is

00:21:18,720 --> 00:21:22,559
an issue there is still something

00:21:20,480 --> 00:21:25,440
to be gained by reducing fragmentation

00:21:22,559 --> 00:21:28,480
but it looks like most of the memory

00:21:25,440 --> 00:21:30,080
is actually just wasted

00:21:28,480 --> 00:21:32,799
because the memory allocator doesn't

00:21:30,080 --> 00:21:34,400
like the freedoms back to the kernel

00:21:32,799 --> 00:21:36,000
the solution turns out to be super

00:21:34,400 --> 00:21:38,640
simple because it is literally

00:21:36,000 --> 00:21:40,559
one api call from the ruby garbage

00:21:38,640 --> 00:21:43,360
collector it's a one-line patch

00:21:40,559 --> 00:21:44,960
but yeah just finding the solution it

00:21:43,360 --> 00:21:47,760
took a lot a lot of

00:21:44,960 --> 00:21:47,760
research time

00:21:48,720 --> 00:21:52,400
by now a number of third parties have

00:21:50,640 --> 00:21:54,400
already tested my finding in production

00:21:52,400 --> 00:21:57,520
like environments

00:21:54,400 --> 00:21:58,400
they have observed 17 to 20 less memory

00:21:57,520 --> 00:22:01,760
but also

00:21:58,400 --> 00:22:03,280
a 7 to 20 percent performance drop so

00:22:01,760 --> 00:22:06,320
that call is definitely

00:22:03,280 --> 00:22:08,720
not free your mileage may vary it

00:22:06,320 --> 00:22:11,120
depends on your workload

00:22:08,720 --> 00:22:14,080
and there's still room for optimizations

00:22:11,120 --> 00:22:16,960
for example we can choose to call

00:22:14,080 --> 00:22:19,200
that api call less often or maybe over

00:22:16,960 --> 00:22:22,320
time or something

00:22:19,200 --> 00:22:22,720
like that so there are ways to amortize

00:22:22,320 --> 00:22:26,480
the

00:22:22,720 --> 00:22:28,799
performance impact of this call

00:22:26,480 --> 00:22:30,880
and third parties have also observed

00:22:28,799 --> 00:22:33,520
that using the je malloc

00:22:30,880 --> 00:22:34,799
memory allocator library is still better

00:22:33,520 --> 00:22:38,400
je mallock uses

00:22:34,799 --> 00:22:41,280
a more complex algorithm than

00:22:38,400 --> 00:22:42,080
the then the built-in linux memory

00:22:41,280 --> 00:22:44,080
allocator

00:22:42,080 --> 00:22:45,840
and it lays out memory in different way

00:22:44,080 --> 00:22:47,840
in a way that is uh

00:22:45,840 --> 00:22:49,039
that results in less fragmentation but

00:22:47,840 --> 00:22:53,200
also less maximum

00:22:49,039 --> 00:22:55,039
memory usage and less waste

00:22:53,200 --> 00:22:57,039
uh so if you use je matlock you can

00:22:55,039 --> 00:23:00,080
potentially get less memory usage

00:22:57,039 --> 00:23:02,159
and a performance win

00:23:00,080 --> 00:23:05,440
but there are also problems with that

00:23:02,159 --> 00:23:07,360
because it's a much larger dependency

00:23:05,440 --> 00:23:09,440
you need to be wary of which versions

00:23:07,360 --> 00:23:11,280
you use

00:23:09,440 --> 00:23:13,440
and and also the way the way you

00:23:11,280 --> 00:23:16,640
activate it will

00:23:13,440 --> 00:23:17,919
will will affect

00:23:16,640 --> 00:23:22,880
how much effect it has on your

00:23:17,919 --> 00:23:25,039
application and a bunch of other issues

00:23:22,880 --> 00:23:27,200
and on and well we're still not done

00:23:25,039 --> 00:23:30,480
researching because earlier this week

00:23:27,200 --> 00:23:33,760
um i stumbled upon a nice surprise

00:23:30,480 --> 00:23:35,120
i started i i was researching je maloc

00:23:33,760 --> 00:23:36,880
and then i discovered that i can

00:23:35,120 --> 00:23:39,280
actually combine ge malloc

00:23:36,880 --> 00:23:41,200
with malloc trim somehow that had some

00:23:39,280 --> 00:23:42,080
effect and it didn't make any sense for

00:23:41,200 --> 00:23:45,360
me because

00:23:42,080 --> 00:23:49,440
if if je mello gets in use why would

00:23:45,360 --> 00:23:52,400
why would this still have any effect

00:23:49,440 --> 00:23:53,760
uh well the reason behind this was

00:23:52,400 --> 00:23:57,120
because i was compiling

00:23:53,760 --> 00:23:59,200
ruby with a minus minus with je malloc

00:23:57,120 --> 00:24:02,840
built flag

00:23:59,200 --> 00:24:06,880
but that only activated je mallock

00:24:02,840 --> 00:24:08,960
um it was only half effective in

00:24:06,880 --> 00:24:10,880
activating je malok so the end result

00:24:08,960 --> 00:24:12,480
was a ruby installation that used je

00:24:10,880 --> 00:24:15,360
malok partially and the system

00:24:12,480 --> 00:24:17,679
memory allocator partially it is

00:24:15,360 --> 00:24:19,200
something that

00:24:17,679 --> 00:24:20,799
i will need to think about how to solve

00:24:19,200 --> 00:24:23,279
this and it looks like i will have to

00:24:20,799 --> 00:24:26,320
use the ld preload mechanism but that

00:24:23,279 --> 00:24:26,320
has its own problems

00:24:26,480 --> 00:24:32,000
well researching is fine but

00:24:29,760 --> 00:24:32,799
i also want to bring value to the ruby

00:24:32,000 --> 00:24:35,360
community

00:24:32,799 --> 00:24:36,159
like i'm also a very pragmatic guy and

00:24:35,360 --> 00:24:38,159
if i see

00:24:36,159 --> 00:24:40,000
opportunities for optimization i want

00:24:38,159 --> 00:24:43,039
people to be able to use this

00:24:40,000 --> 00:24:44,720
ge malloc and and

00:24:43,039 --> 00:24:46,159
other kinds of optimizations they have

00:24:44,720 --> 00:24:49,120
been around forever

00:24:46,159 --> 00:24:50,320
but they are not officially incorporated

00:24:49,120 --> 00:24:52,559
in ruby

00:24:50,320 --> 00:24:53,520
uh for good reason because there are

00:24:52,559 --> 00:24:56,640
risks

00:24:53,520 --> 00:24:58,799
risks attached to it but i think that

00:24:56,640 --> 00:25:00,080
for some people maybe they are willing

00:24:58,799 --> 00:25:03,840
to take some risks

00:25:00,080 --> 00:25:05,600
in order to get a large benefit so

00:25:03,840 --> 00:25:07,919
i think a lot of people are thinking

00:25:05,600 --> 00:25:10,720
okay when can i make use of this

00:25:07,919 --> 00:25:14,240
um of these optimizations and what is

00:25:10,720 --> 00:25:15,840
the easiest way to make use of this

00:25:14,240 --> 00:25:17,600
because right now you have to compile

00:25:15,840 --> 00:25:18,000
ruby from source in order to make use of

00:25:17,600 --> 00:25:20,240
this

00:25:18,000 --> 00:25:23,840
and that is a big hassle nobody wants to

00:25:20,240 --> 00:25:23,840
do that i don't want to do it either

00:25:24,080 --> 00:25:26,320
well

00:25:27,279 --> 00:25:33,760
well uh this is what i just talked about

00:25:30,320 --> 00:25:34,799
so here's the announcement together with

00:25:33,760 --> 00:25:37,520
full stack

00:25:34,799 --> 00:25:38,320
i am working on a new ruby distribution

00:25:37,520 --> 00:25:40,799
whose goal

00:25:38,320 --> 00:25:42,000
is to package all this memory reduction

00:25:40,799 --> 00:25:45,520
work in a form that

00:25:42,000 --> 00:25:48,080
is easily usable by users

00:25:45,520 --> 00:25:50,240
and we call this distribution full stack

00:25:48,080 --> 00:25:52,880
ruby

00:25:50,240 --> 00:25:55,200
so it'll be faster use less memory and

00:25:52,880 --> 00:25:57,600
we do this by incorporating je malloc

00:25:55,200 --> 00:26:00,720
and malloc trim

00:25:57,600 --> 00:26:02,400
we distribute binaries not source and we

00:26:00,720 --> 00:26:04,880
distribute them in the form of debian

00:26:02,400 --> 00:26:06,720
packages and rpm packages

00:26:04,880 --> 00:26:08,799
so that you don't have to compile and we

00:26:06,720 --> 00:26:10,880
integrate with os package manager

00:26:08,799 --> 00:26:12,159
so that you can also get security

00:26:10,880 --> 00:26:14,960
updates that way

00:26:12,159 --> 00:26:17,039
and i have understood that just keeping

00:26:14,960 --> 00:26:17,840
ruby security patch if you compile it

00:26:17,039 --> 00:26:19,919
from source

00:26:17,840 --> 00:26:24,000
with rb and if install of rpm install

00:26:19,919 --> 00:26:27,039
that's also a huge hassle

00:26:24,000 --> 00:26:30,640
now this is still in beta uh

00:26:27,039 --> 00:26:31,840
we got an mvp out yesterday so

00:26:30,640 --> 00:26:34,080
there's still a lot of room for

00:26:31,840 --> 00:26:35,440
improvements in the future we will even

00:26:34,080 --> 00:26:38,400
supply docker images

00:26:35,440 --> 00:26:39,200
and provide heroku support and you will

00:26:38,400 --> 00:26:41,600
be able to

00:26:39,200 --> 00:26:42,240
um so the amp and jump repository isn't

00:26:41,600 --> 00:26:45,520
there are

00:26:42,240 --> 00:26:48,320
isn't there yet but it will be right now

00:26:45,520 --> 00:26:50,480
it's a bunch of debian and rpm packages

00:26:48,320 --> 00:26:52,159
files that you can download from github

00:26:50,480 --> 00:26:55,600
but we are working on

00:26:52,159 --> 00:26:57,279
apps and yum with high priority

00:26:55,600 --> 00:26:58,960
if you are interested please take a look

00:26:57,279 --> 00:27:02,320
at our roadmap on github

00:26:58,960 --> 00:27:04,960
to see what we have in store it's

00:27:02,320 --> 00:27:06,240
100 open source and we welcome

00:27:04,960 --> 00:27:08,000
contributors

00:27:06,240 --> 00:27:09,679
so we're not going to make money out of

00:27:08,000 --> 00:27:12,960
this there's not going to be

00:27:09,679 --> 00:27:15,520
a paid version this is a community

00:27:12,960 --> 00:27:15,520
project

00:27:16,640 --> 00:27:25,039
so check out its website

00:27:21,760 --> 00:27:27,760
um i'm and there's an faq

00:27:25,039 --> 00:27:28,799
section so if you have any questions

00:27:27,760 --> 00:27:30,240
about hey

00:27:28,799 --> 00:27:31,550
what about fender log in that is

00:27:30,240 --> 00:27:33,279
probably the biggest

00:27:31,550 --> 00:27:35,360
[Music]

00:27:33,279 --> 00:27:36,960
concern i have heard from people check

00:27:35,360 --> 00:27:40,320
out the faq i have some

00:27:36,960 --> 00:27:41,919
answers written there fullstack is a

00:27:40,320 --> 00:27:43,600
technology partner company in the

00:27:41,919 --> 00:27:45,120
netherlands and they are chock full of

00:27:43,600 --> 00:27:47,520
experts like this guy

00:27:45,120 --> 00:27:48,240
this is peter langer who is also in this

00:27:47,520 --> 00:27:51,919
building

00:27:48,240 --> 00:27:53,679
and he's probably the best kubernetes

00:27:51,919 --> 00:27:55,840
and devops expert in the netherlands

00:27:53,679 --> 00:27:57,200
it's it's just amazing how much he knows

00:27:55,840 --> 00:27:58,880
he's helping me so much with the

00:27:57,200 --> 00:28:01,039
infrastructure for this project

00:27:58,880 --> 00:28:02,000
which is not trivial because we need a

00:28:01,039 --> 00:28:04,320
high performance ci

00:28:02,000 --> 00:28:05,600
pipeline to generate something like 70

00:28:04,320 --> 00:28:07,919
to 100

00:28:05,600 --> 00:28:09,200
rpm and debian packages and docker files

00:28:07,919 --> 00:28:10,640
for each release from multiple

00:28:09,200 --> 00:28:12,559
distributions

00:28:10,640 --> 00:28:13,760
and then and they're supplying bandwidth

00:28:12,559 --> 00:28:16,399
and that sort of thing

00:28:13,760 --> 00:28:17,840
so he if you see him please shake his

00:28:16,399 --> 00:28:19,600
hands he's awesome

00:28:17,840 --> 00:28:21,520
oh yeah and this goes especially to

00:28:19,600 --> 00:28:23,760
charity mayors

00:28:21,520 --> 00:28:24,640
because peter is a huge fan of you

00:28:23,760 --> 00:28:26,080
charity

00:28:24,640 --> 00:28:28,399
if you see him please give him your

00:28:26,080 --> 00:28:31,279
autograph so thank you

00:28:28,399 --> 00:28:31,919
and please follow my blog for updates or

00:28:31,279 --> 00:28:44,159
check out

00:28:31,919 --> 00:28:46,640
full stack ruby

00:28:44,159 --> 00:28:48,559
thank you hungley that was wonderful you

00:28:46,640 --> 00:28:50,640
could say it brought me good memories of

00:28:48,559 --> 00:28:52,420
the sea

00:28:50,640 --> 00:28:56,720
oh thank you

00:28:52,420 --> 00:28:58,320
[Applause]

00:28:56,720 --> 00:29:00,720
no that was great thank you so much

00:28:58,320 --> 00:29:00,720

YouTube URL: https://www.youtube.com/watch?v=JBIN-Hh8wTA


