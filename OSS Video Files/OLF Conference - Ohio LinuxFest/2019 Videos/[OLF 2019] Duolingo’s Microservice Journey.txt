Title: [OLF 2019] Duolingoâ€™s Microservice Journey
Publication date: 2020-01-06
Playlist: 2019 Videos
Description: 
	Speaker: Max Blaze
Slides: https://ohiolinux.org/wp-content/uploads/2019/11/2019-Duolingo_Microservice_Journey.pdf
Videographer: Ernest Parke

Applications built on a microservices-based architecture and packaged as containers bring several benefits, but also some pitfalls. In this session, Duolingo, a popular language-learning platform, describes its journey from a monolith to a microservices architecture and its experiences building a global product in the cloud.
Captions: 
	00:00:02,260 --> 00:00:08,780
so how many of you have heard of

00:00:04,580 --> 00:00:11,020
duolingo oh geez everyone I think a one

00:00:08,780 --> 00:00:15,590
or two people how many of you use it

00:00:11,020 --> 00:00:17,420
okay so I have that so for those of you

00:00:15,590 --> 00:00:19,160
who don't know this is our mission

00:00:17,420 --> 00:00:22,580
statement free and accessible language

00:00:19,160 --> 00:00:27,230
education for all and this kind of

00:00:22,580 --> 00:00:29,510
defines everything that we do you have

00:00:27,230 --> 00:00:32,960
the most downloaded education app in the

00:00:29,510 --> 00:00:37,910
world bait both on iOS and Android we're

00:00:32,960 --> 00:00:39,980
in every major country in the world we

00:00:37,910 --> 00:00:43,730
have 30 languages that you can learn

00:00:39,980 --> 00:00:45,350
across 80 different courses anyone want

00:00:43,730 --> 00:00:52,220
to take a guess on what our most popular

00:00:45,350 --> 00:00:55,670
language to learn is English yes there

00:00:52,220 --> 00:00:58,640
is an enormous demand to learn English

00:00:55,670 --> 00:01:00,950
especially in developing countries it

00:00:58,640 --> 00:01:03,770
allows for great upward mobility from

00:01:00,950 --> 00:01:05,239
poverty and we you know we really think

00:01:03,770 --> 00:01:11,600
about this this is this was one of the

00:01:05,239 --> 00:01:14,450
main things that we do so we found that

00:01:11,600 --> 00:01:16,729
duolingo is actually not only fun but

00:01:14,450 --> 00:01:18,709
fairly effective so we did an

00:01:16,729 --> 00:01:22,159
independent study a few years ago with a

00:01:18,709 --> 00:01:24,920
major university which found that 34

00:01:22,159 --> 00:01:27,889
hours on duolingo is roughly equal to

00:01:24,920 --> 00:01:30,969
one University semester of Spanish so

00:01:27,889 --> 00:01:30,969
that's quite effective

00:01:31,670 --> 00:01:36,020
right now we have over 300 million users

00:01:34,279 --> 00:01:39,529
this is actually about a year out of

00:01:36,020 --> 00:01:41,359
date so we have more than that now and

00:01:39,529 --> 00:01:43,880
we support that many users with only

00:01:41,359 --> 00:01:46,909
about 180 employees this is also a low

00:01:43,880 --> 00:01:53,299
out date we're closer to 200 now less of

00:01:46,909 --> 00:01:55,119
less than half of which are engineers so

00:01:53,299 --> 00:01:57,380
this is a graph of our user growth

00:01:55,119 --> 00:02:00,409
compared to the number of employees so

00:01:57,380 --> 00:02:02,779
this is a little crazy from even mean to

00:02:00,409 --> 00:02:04,130
think about since I've been with the

00:02:02,779 --> 00:02:07,310
company for about five and a half years

00:02:04,130 --> 00:02:08,810
I just constant constant growth like it

00:02:07,310 --> 00:02:11,930
never slows down there's always new

00:02:08,810 --> 00:02:14,690
challenges so we have you know over a

00:02:11,930 --> 00:02:17,260
million users per employee it's a little

00:02:14,690 --> 00:02:17,260
crazy to think about

00:02:17,440 --> 00:02:25,130
so a brief history of our infrastructure

00:02:20,890 --> 00:02:28,300
so we launched in late 2012 we were born

00:02:25,130 --> 00:02:29,900
on AWS or Amazon Web Services which is

00:02:28,300 --> 00:02:32,540
infrastructure-as-a-service

00:02:29,900 --> 00:02:36,100
this is our original logo she's still

00:02:32,540 --> 00:02:36,100
kind of like it it's kind of cool

00:02:36,170 --> 00:02:42,470
it was a Python to stack and all of the

00:02:40,130 --> 00:02:45,590
infrastructure was manually built we had

00:02:42,470 --> 00:02:47,510
some very early bash scripts a little

00:02:45,590 --> 00:02:49,670
bit of chef but most of it was

00:02:47,510 --> 00:02:51,860
completely hand-built there weren't too

00:02:49,670 --> 00:02:57,280
many AWS services back then there was

00:02:51,860 --> 00:03:00,020
like s3 ec2 so s3 is lean object store

00:02:57,280 --> 00:03:04,940
ec2 you could think of it as a virtual

00:03:00,020 --> 00:03:07,550
machine service and RDS which is a

00:03:04,940 --> 00:03:09,220
database service so my sequel was pretty

00:03:07,550 --> 00:03:12,800
much the only option I believe back then

00:03:09,220 --> 00:03:15,650
no sequel databases like DynamoDB did

00:03:12,800 --> 00:03:18,260
not yet exist so this was just Python

00:03:15,650 --> 00:03:20,390
with an RDS back-end I think we might

00:03:18,260 --> 00:03:22,130
have had a cache or two I want this is

00:03:20,390 --> 00:03:26,360
before my time

00:03:22,130 --> 00:03:32,110
but yet monolithic manual deployments

00:03:26,360 --> 00:03:35,390
manual everything so I joined around

00:03:32,110 --> 00:03:38,900
2014 one of my first tasks was to get

00:03:35,390 --> 00:03:42,290
config management into the company we

00:03:38,900 --> 00:03:45,170
ended up deciding on ansible I was

00:03:42,290 --> 00:03:48,590
familiar with puppet but ansible was

00:03:45,170 --> 00:03:50,630
definitely coming into the fray I don't

00:03:48,590 --> 00:03:51,920
really glad that we we use ansible

00:03:50,630 --> 00:03:54,680
because that is this one should become

00:03:51,920 --> 00:03:57,650
the de-facto standard now sorry if you

00:03:54,680 --> 00:04:01,310
don't if you use puppet but I don't know

00:03:57,650 --> 00:04:04,130
I feel like hands was kind of one so at

00:04:01,310 --> 00:04:06,890
this point everything was in ansible and

00:04:04,130 --> 00:04:09,410
we're actually able to have an automated

00:04:06,890 --> 00:04:11,750
process to build machine images which

00:04:09,410 --> 00:04:16,010
was you know a great starting point in

00:04:11,750 --> 00:04:19,190
terms of automation so another year or

00:04:16,010 --> 00:04:23,090
so passed and again we had this

00:04:19,190 --> 00:04:25,280
monolithic Python application and I was

00:04:23,090 --> 00:04:27,440
responsible for you know dozens of

00:04:25,280 --> 00:04:28,570
deployments per day we were very quickly

00:04:27,440 --> 00:04:30,880
moving company

00:04:28,570 --> 00:04:34,570
at the time which I'll discuss a little

00:04:30,880 --> 00:04:37,330
bit more weren't auto-scaling so we just

00:04:34,570 --> 00:04:38,650
add machines when we needed them and

00:04:37,330 --> 00:04:39,760
around this time we started auto-scaling

00:04:38,650 --> 00:04:42,160
which was actually a pretty big

00:04:39,760 --> 00:04:44,800
challenge back then it seems like old

00:04:42,160 --> 00:04:47,020
hat now but it was a pretty big deal and

00:04:44,800 --> 00:04:50,350
we actually say it was a significant

00:04:47,020 --> 00:04:52,300
amount of money by doing so and then we

00:04:50,350 --> 00:04:54,820
also started to think about micro

00:04:52,300 --> 00:04:58,450
services so we had a newly formed

00:04:54,820 --> 00:05:02,080
architecture team that try to come up

00:04:58,450 --> 00:05:03,220
with service templates for our engineers

00:05:02,080 --> 00:05:05,860
so that they could start with this

00:05:03,220 --> 00:05:08,950
baseline and then build from there these

00:05:05,860 --> 00:05:11,170
new micro services and we originally ran

00:05:08,950 --> 00:05:13,930
them on Beanstalk and the reason for

00:05:11,170 --> 00:05:16,920
that is just so that a team could fully

00:05:13,930 --> 00:05:18,670
own the stack it was fairly easy

00:05:16,920 --> 00:05:20,860
eventually got rid of it which I'll

00:05:18,670 --> 00:05:24,820
discuss but at the time it was it was

00:05:20,860 --> 00:05:27,250
the correct decision and then we started

00:05:24,820 --> 00:05:29,650
to centralize a lot of the view into the

00:05:27,250 --> 00:05:32,590
infrastructure and also handing things

00:05:29,650 --> 00:05:34,180
off to the teams so we put a knelt stack

00:05:32,590 --> 00:05:36,820
in place you could view all the logs and

00:05:34,180 --> 00:05:38,350
search them so the teams can manage

00:05:36,820 --> 00:05:40,000
their own services and keep track of

00:05:38,350 --> 00:05:42,820
them so it's not just you know me and

00:05:40,000 --> 00:05:44,350
one other person on the ops team we also

00:05:42,820 --> 00:05:46,900
started to create standardized

00:05:44,350 --> 00:05:48,580
dashboards and allow teams to create

00:05:46,900 --> 00:05:50,170
their own dashboards their own views of

00:05:48,580 --> 00:05:53,020
the things they care about and their

00:05:50,170 --> 00:05:55,360
service Gravano

00:05:53,020 --> 00:05:57,280
was one of the life-changing things so

00:05:55,360 --> 00:06:00,220
if you haven't checked out her phone I

00:05:57,280 --> 00:06:01,390
definitely do that and then shortly

00:06:00,220 --> 00:06:04,750
after that we started to look at

00:06:01,390 --> 00:06:06,610
terraform so I didn't intern and his job

00:06:04,750 --> 00:06:09,160
was to import all of our existing

00:06:06,610 --> 00:06:10,810
manually built infrastructure into

00:06:09,160 --> 00:06:12,250
terraform and then at least we had a

00:06:10,810 --> 00:06:14,710
central repository where he can make

00:06:12,250 --> 00:06:19,810
changes again that was definitely

00:06:14,710 --> 00:06:21,040
life-changing as well and then we

00:06:19,810 --> 00:06:25,240
started to get really serious about

00:06:21,040 --> 00:06:27,610
micro services bean suck was just not

00:06:25,240 --> 00:06:30,340
scaling if you know anything about

00:06:27,610 --> 00:06:33,120
Beanstalk when you use it with terraform

00:06:30,340 --> 00:06:35,710
it doesn't really jive very well

00:06:33,120 --> 00:06:37,390
beanstalk is very monolithic and you end

00:06:35,710 --> 00:06:39,160
up with you know thousands upon

00:06:37,390 --> 00:06:41,350
thousands of configuration parameters

00:06:39,160 --> 00:06:43,930
and it's just it comes in a huge mess

00:06:41,350 --> 00:06:47,080
so what we ended up doing was building

00:06:43,930 --> 00:06:50,430
entire new infrastructure platform based

00:06:47,080 --> 00:06:53,530
on ECS which is AWS is proprietary

00:06:50,430 --> 00:06:56,920
elastic container service kind of

00:06:53,530 --> 00:06:58,540
similar to Cooper Nettie's but a lot

00:06:56,920 --> 00:07:02,110
less complex and they managed the whole

00:06:58,540 --> 00:07:04,180
thing so let's jump back

00:07:02,110 --> 00:07:08,880
so why moved to micro service at all I

00:07:04,180 --> 00:07:14,200
mean we have monolith it was it was okay

00:07:08,880 --> 00:07:15,460
it was running but the thing is at a

00:07:14,200 --> 00:07:17,620
certain point when you have enough

00:07:15,460 --> 00:07:19,720
enough people again you're gonna run out

00:07:17,620 --> 00:07:21,550
of time in which to do deployments for

00:07:19,720 --> 00:07:22,930
all the teams on a given day or a given

00:07:21,550 --> 00:07:26,410
week depending on your deployment

00:07:22,930 --> 00:07:29,350
strategy so we not only wanted to scale

00:07:26,410 --> 00:07:32,110
the application horizontally we also

00:07:29,350 --> 00:07:33,940
wanted to scale the team so that the

00:07:32,110 --> 00:07:36,940
teams themselves so we didn't want to

00:07:33,940 --> 00:07:39,730
hire you know one or two ops people or

00:07:36,940 --> 00:07:42,190
one or two engineers for every single

00:07:39,730 --> 00:07:44,080
service that we had right so we're gonna

00:07:42,190 --> 00:07:46,570
make these applications small so that

00:07:44,080 --> 00:07:51,220
maybe one person can manage two three

00:07:46,570 --> 00:07:53,800
four different services velocity again

00:07:51,220 --> 00:07:55,750
most of this relates to deployment so

00:07:53,800 --> 00:07:58,000
when your micro services all the teams

00:07:55,750 --> 00:08:00,180
can independently deploy you're not

00:07:58,000 --> 00:08:03,220
stuck with a centralized team

00:08:00,180 --> 00:08:06,690
flexibility we started on Python 2 but

00:08:03,220 --> 00:08:10,650
engineers wanted to run you know nodejs

00:08:06,690 --> 00:08:12,810
Python 3 was coming into play Java

00:08:10,650 --> 00:08:17,170
people wanted a more enterprise e

00:08:12,810 --> 00:08:18,670
language Scala was a big deal at the

00:08:17,170 --> 00:08:20,980
time

00:08:18,670 --> 00:08:23,230
and then reliability up to this point we

00:08:20,980 --> 00:08:25,990
had never hit four nines and a quarter

00:08:23,230 --> 00:08:29,650
and that was becoming a really big goal

00:08:25,990 --> 00:08:31,240
for us and with a micro service

00:08:29,650 --> 00:08:33,940
architecture you could put in things

00:08:31,240 --> 00:08:36,820
like circuit breakers which will cut off

00:08:33,940 --> 00:08:38,289
access to poorly performing or services

00:08:36,820 --> 00:08:41,580
that are down and keep the rest of the

00:08:38,289 --> 00:08:44,050
infrastructure up and also cost savings

00:08:41,580 --> 00:08:47,410
you know a few years ago our costs are

00:08:44,050 --> 00:08:49,480
aw scoffs were steadily increasing with

00:08:47,410 --> 00:08:53,620
the number of users and it was becoming

00:08:49,480 --> 00:08:54,910
really significant any of you who are on

00:08:53,620 --> 00:08:56,290
AWS know that

00:08:54,910 --> 00:09:00,600
there are lots of hidden costs and

00:08:56,290 --> 00:09:00,600
things to worry about on those platforms

00:09:01,080 --> 00:09:06,910
so we had to decide what to carve out of

00:09:04,180 --> 00:09:08,710
the monolith so this is what I would

00:09:06,910 --> 00:09:09,580
recommend so start with something small

00:09:08,710 --> 00:09:11,160
bit impactful

00:09:09,580 --> 00:09:13,110
don't try to carve out the largest piece

00:09:11,160 --> 00:09:16,410
because it will probably never ever

00:09:13,110 --> 00:09:19,090
finish and it just become a nightmare

00:09:16,410 --> 00:09:21,970
and then move up in size complexity and

00:09:19,090 --> 00:09:23,610
risk so what we found with every single

00:09:21,970 --> 00:09:25,570
piece that we carved out or

00:09:23,610 --> 00:09:28,260
microservices that we had moved from

00:09:25,570 --> 00:09:31,060
beanstalks of the new ECS framework

00:09:28,260 --> 00:09:33,010
there was a problem every single time it

00:09:31,060 --> 00:09:35,050
was amazing I thought this one's gonna

00:09:33,010 --> 00:09:37,050
go smooth this time and every single

00:09:35,050 --> 00:09:40,570
time we hit you know this huge roadblock

00:09:37,050 --> 00:09:44,290
brand new problem it was quite the

00:09:40,570 --> 00:09:46,030
process but by the very end things were

00:09:44,290 --> 00:09:48,000
really smooth and now we can onboard

00:09:46,030 --> 00:09:53,680
micro services you know in a few hours

00:09:48,000 --> 00:09:55,420
where it used to take weeks so the first

00:09:53,680 --> 00:09:58,090
thing we carved out was a reminder

00:09:55,420 --> 00:10:00,040
service this is just a service that

00:09:58,090 --> 00:10:02,520
reminds you to do your daily lesson just

00:10:00,040 --> 00:10:05,880
sends out an email or or a text message

00:10:02,520 --> 00:10:08,350
and that was actually really impactful

00:10:05,880 --> 00:10:12,430
our daily usage is actually highly

00:10:08,350 --> 00:10:15,370
impacted by these providers and you also

00:10:12,430 --> 00:10:18,460
need to consider dependencies so let's

00:10:15,370 --> 00:10:20,910
just look at some raw statistics let's

00:10:18,460 --> 00:10:24,250
say your monolith uptime is two nines

00:10:20,910 --> 00:10:26,200
not great not terrible right

00:10:24,250 --> 00:10:28,540
but you split that into three different

00:10:26,200 --> 00:10:30,600
services that have the same uptime and

00:10:28,540 --> 00:10:33,790
suddenly you're at point nine seven

00:10:30,600 --> 00:10:35,380
which is worse because if they're all

00:10:33,790 --> 00:10:38,620
tied together they're all gonna go down

00:10:35,380 --> 00:10:40,510
if one of them goes down but if you make

00:10:38,620 --> 00:10:42,900
them independent you're now at six nines

00:10:40,510 --> 00:10:46,330
of uptime so this is the ultimate goal

00:10:42,900 --> 00:10:47,710
and you can achieve that with circuit

00:10:46,330 --> 00:10:51,310
breaking libraries there's a few out

00:10:47,710 --> 00:10:52,690
there Netflix has one we ended up

00:10:51,310 --> 00:10:57,670
building our own circuit breaking

00:10:52,690 --> 00:10:59,830
library for Python internally but this

00:10:57,670 --> 00:11:01,620
is where you want to get you don't want

00:10:59,830 --> 00:11:04,420
to chain things forever and especially

00:11:01,620 --> 00:11:05,740
you don't want to have all the

00:11:04,420 --> 00:11:08,100
dependencies still in the monolith while

00:11:05,740 --> 00:11:09,720
you have micro services or worse

00:11:08,100 --> 00:11:11,970
have a circular dependency between the

00:11:09,720 --> 00:11:13,500
monolith and a microservice you just

00:11:11,970 --> 00:11:16,980
made everything much much worse in terms

00:11:13,500 --> 00:11:25,560
of all-time and Netflix this tool is

00:11:16,980 --> 00:11:27,240
called history's so why use docker so

00:11:25,560 --> 00:11:29,190
I'm not the biggest fan of docker it

00:11:27,240 --> 00:11:32,730
took me a long time to get used to it

00:11:29,190 --> 00:11:34,550
and actually use it but it's one you

00:11:32,730 --> 00:11:39,209
kind of have to use it if you want to do

00:11:34,550 --> 00:11:41,220
containers the main advantage is that it

00:11:39,209 --> 00:11:43,829
standardizes the build process and

00:11:41,220 --> 00:11:45,810
encapsulate dependencies so you move

00:11:43,829 --> 00:11:47,759
from these weird scripts for every

00:11:45,810 --> 00:11:51,870
service our team to this nice

00:11:47,759 --> 00:11:53,940
standardized docker file and it allows

00:11:51,870 --> 00:11:56,040
for very quick deployments and rollback

00:11:53,940 --> 00:11:59,579
so rather than it taking you know

00:11:56,040 --> 00:12:01,949
minutes many minutes if you're on like a

00:11:59,579 --> 00:12:04,769
physical server to boot up and start the

00:12:01,949 --> 00:12:06,750
application it might take seconds so if

00:12:04,769 --> 00:12:09,449
you have a failure and your service

00:12:06,750 --> 00:12:12,480
crashes that can very quickly come back

00:12:09,449 --> 00:12:14,970
up it also allows for flexible resource

00:12:12,480 --> 00:12:16,649
allocations so when you're on these

00:12:14,970 --> 00:12:20,279
cloud platforms generally you have a

00:12:16,649 --> 00:12:22,290
fixed size of CPU and memory but with

00:12:20,279 --> 00:12:24,300
docker you can kind of have an infinite

00:12:22,290 --> 00:12:26,339
size and scale so you could actually

00:12:24,300 --> 00:12:27,860
scale the memory and CPU to the

00:12:26,339 --> 00:12:33,209
application and not the other way around

00:12:27,860 --> 00:12:35,160
which is quite nice so in terms of

00:12:33,209 --> 00:12:37,410
simplifying love development this was

00:12:35,160 --> 00:12:40,380
how we used to do things this was you

00:12:37,410 --> 00:12:41,610
know like a long step of manual tasks

00:12:40,380 --> 00:12:43,769
you had to run in order to get this

00:12:41,610 --> 00:12:46,680
service to work locally

00:12:43,769 --> 00:12:50,430
please don't screenshot that this it's

00:12:46,680 --> 00:12:53,220
not relevant at all so we went from this

00:12:50,430 --> 00:12:55,560
to this and actually we've gone a step

00:12:53,220 --> 00:12:59,750
further now we have a script that does

00:12:55,560 --> 00:12:59,750
both of these for you it's like run SH

00:13:01,040 --> 00:13:07,560
so that's great so every new employee

00:13:03,420 --> 00:13:09,029
that comes in every service is coded in

00:13:07,560 --> 00:13:10,920
such a way that you could just you build

00:13:09,029 --> 00:13:12,630
an up and it'll start in a local machine

00:13:10,920 --> 00:13:17,970
you could get started very very quickly

00:13:12,630 --> 00:13:20,850
so let's go back to UCS why did we

00:13:17,970 --> 00:13:21,930
choose these yes so this was almost a

00:13:20,850 --> 00:13:24,420
little over three years ago

00:13:21,930 --> 00:13:27,300
we were making this decision Cooper

00:13:24,420 --> 00:13:29,610
Nettie's was out but it still seemed

00:13:27,300 --> 00:13:32,790
extremely scary to me there was a

00:13:29,610 --> 00:13:35,280
thousand node limit and we were nearing

00:13:32,790 --> 00:13:36,840
that on ec2 so I was like I don't know

00:13:35,280 --> 00:13:39,450
if that's gonna work

00:13:36,840 --> 00:13:40,770
also ingress was was a problem back then

00:13:39,450 --> 00:13:43,500
I think most of these things had been

00:13:40,770 --> 00:13:46,530
solved and there was also hash of Court

00:13:43,500 --> 00:13:49,560
nomads which I liked a lot it fits very

00:13:46,530 --> 00:13:52,110
well into the terraform system but we

00:13:49,560 --> 00:13:54,540
went with ECS so it provided a lot of

00:13:52,110 --> 00:13:57,060
really nice features that the others

00:13:54,540 --> 00:14:00,960
didn't so we had tasks based auto

00:13:57,060 --> 00:14:02,520
scaling a task and this in this instance

00:14:00,960 --> 00:14:03,660
is just a group of containers that kind

00:14:02,520 --> 00:14:06,120
of work together so you might have a

00:14:03,660 --> 00:14:07,710
service and then a logging daemon and

00:14:06,120 --> 00:14:10,140
they're grouped is a task you might have

00:14:07,710 --> 00:14:11,790
lots of different what are called

00:14:10,140 --> 00:14:13,470
sidecar containers where it's not the

00:14:11,790 --> 00:14:16,110
actual application it's just something

00:14:13,470 --> 00:14:19,440
that kind of helps the service along or

00:14:16,110 --> 00:14:21,180
monitors the service we had auto scaling

00:14:19,440 --> 00:14:23,040
for this so we didn't have to have a

00:14:21,180 --> 00:14:25,320
fixed number of containers they would

00:14:23,040 --> 00:14:29,160
just scale with the amount of CPU or

00:14:25,320 --> 00:14:31,320
less very easy and also something that

00:14:29,160 --> 00:14:36,410
we were used to at the time we also had

00:14:31,320 --> 00:14:38,970
task level I am so I am is the way to

00:14:36,410 --> 00:14:42,110
give permission to the service so they

00:14:38,970 --> 00:14:44,460
can access some other AWS resource and

00:14:42,110 --> 00:14:46,170
with this we could actually do it at per

00:14:44,460 --> 00:14:47,700
service level even if there were

00:14:46,170 --> 00:14:49,230
different services running on the same

00:14:47,700 --> 00:14:51,270
machine they would all at different

00:14:49,230 --> 00:14:53,580
permissions see so that you would have

00:14:51,270 --> 00:14:54,930
some really nice isolation there you

00:14:53,580 --> 00:14:56,820
wouldn't have to sign all the

00:14:54,930 --> 00:14:59,640
permissions to all the machines in the

00:14:56,820 --> 00:15:01,320
cluster is very nice and also teams

00:14:59,640 --> 00:15:04,470
could then manage their own permissions

00:15:01,320 --> 00:15:06,900
which was the side of side benefit we

00:15:04,470 --> 00:15:09,240
use cloud watch so cloud watch is

00:15:06,900 --> 00:15:11,070
Amazon's monitoring service all the

00:15:09,240 --> 00:15:12,570
metrics coming from all the other AWS

00:15:11,070 --> 00:15:16,320
services

00:15:12,570 --> 00:15:19,070
ECS fits very nicely into that and then

00:15:16,320 --> 00:15:21,630
the load balancer so alb stands for

00:15:19,070 --> 00:15:26,040
application load balancer so it's 11

00:15:21,630 --> 00:15:27,150
layer 7 load balancer and if you've ever

00:15:26,040 --> 00:15:28,800
worked with docker you know you're

00:15:27,150 --> 00:15:33,120
normally dealing with ephemeral ports

00:15:28,800 --> 00:15:35,009
and this nicely maps to some external

00:15:33,120 --> 00:15:38,879
port for you automatically

00:15:35,009 --> 00:15:42,449
and then manageability at the time we

00:15:38,879 --> 00:15:44,910
had an op ops team the size of two we're

00:15:42,449 --> 00:15:48,089
now five and we just didn't want to

00:15:44,910 --> 00:15:50,279
manage you know a cooper Nettie's

00:15:48,089 --> 00:15:52,850
cluster or a nomad cluster we just

00:15:50,279 --> 00:15:52,850
wanted to work

00:15:54,949 --> 00:16:01,379
so using terraform we created a bunch of

00:15:59,040 --> 00:16:03,749
abstractions for our teams using

00:16:01,379 --> 00:16:08,069
terraform modules and you could think of

00:16:03,749 --> 00:16:11,009
a module is a library for terraform code

00:16:08,069 --> 00:16:14,489
so we created two services a web service

00:16:11,009 --> 00:16:16,949
might be internal or external to the

00:16:14,489 --> 00:16:21,179
world or a worker service which could be

00:16:16,949 --> 00:16:23,639
demonized and feed in messages from a

00:16:21,179 --> 00:16:27,809
queue or it could be cron based a

00:16:23,639 --> 00:16:30,290
time-based tasks we also had a bunch of

00:16:27,809 --> 00:16:33,299
different data stores so we were using

00:16:30,290 --> 00:16:36,449
our relational database service so

00:16:33,299 --> 00:16:39,720
Postgres my sequel kms allowed us to

00:16:36,449 --> 00:16:44,040
encrypt passwords and keys and then we

00:16:39,720 --> 00:16:47,790
had resum and memcache caches and also

00:16:44,040 --> 00:16:50,339
no sequel DynamoDB which we were one of

00:16:47,790 --> 00:16:54,299
the largest users a few years ago i'm

00:16:50,339 --> 00:16:56,009
sure someone has surpassed us but we

00:16:54,299 --> 00:16:59,129
also had monitoring so all these things

00:16:56,009 --> 00:17:00,720
were now starting to tie together so we

00:16:59,129 --> 00:17:03,989
didn't really get rid of the old

00:17:00,720 --> 00:17:06,539
infrastructure we just built upon it and

00:17:03,989 --> 00:17:08,819
we have now cloud watch talk to him

00:17:06,539 --> 00:17:10,679
graph on ax and the elk stack you could

00:17:08,819 --> 00:17:12,389
view logs within graph Ronna and then

00:17:10,679 --> 00:17:15,299
this was all tied into page of duty so

00:17:12,389 --> 00:17:17,909
you can get alerts if something went

00:17:15,299 --> 00:17:20,100
above a certain threshold and then we

00:17:17,909 --> 00:17:24,329
were also feeding all of our alerts to

00:17:20,100 --> 00:17:27,149
slack we're huge users of slack I swear

00:17:24,329 --> 00:17:29,669
I get like two internal emails a couple

00:17:27,149 --> 00:17:33,779
every couple days everything is on slack

00:17:29,669 --> 00:17:36,120
for better for worse and for the web

00:17:33,779 --> 00:17:38,340
service you could also put some other

00:17:36,120 --> 00:17:40,950
things in front of the load balancer

00:17:38,340 --> 00:17:46,250
like a Web Application Firewall or a

00:17:40,950 --> 00:17:49,179
laughs or even a CDN like cloud front

00:17:46,250 --> 00:17:51,919
it's very modular

00:17:49,179 --> 00:17:54,289
so this is the engineers view into their

00:17:51,919 --> 00:17:55,610
service at a high level so these are the

00:17:54,289 --> 00:17:58,340
only parameters they have to fill in

00:17:55,610 --> 00:18:00,799
everything else is done for them with

00:17:58,340 --> 00:18:03,440
terraform so let's just quickly go

00:18:00,799 --> 00:18:05,230
through here like I said billing is very

00:18:03,440 --> 00:18:09,320
important when you're at a certain scale

00:18:05,230 --> 00:18:10,370
so these are all mandatory tags so what

00:18:09,320 --> 00:18:15,139
sort of environment are you any of

00:18:10,370 --> 00:18:17,330
production staging dev product so that

00:18:15,139 --> 00:18:20,029
this for instance is our main duolingo

00:18:17,330 --> 00:18:23,149
API which the app of the web service

00:18:20,029 --> 00:18:25,100
interfaces with and then in an owner so

00:18:23,149 --> 00:18:26,779
you have to know who to talk to if you

00:18:25,100 --> 00:18:27,350
see a bill going completely out of

00:18:26,779 --> 00:18:30,860
control

00:18:27,350 --> 00:18:32,690
for a particular service and then we

00:18:30,860 --> 00:18:35,330
have the min and Max count so this is

00:18:32,690 --> 00:18:38,720
how we tie in the auto scaling this will

00:18:35,330 --> 00:18:45,259
scale based on CPU the amount of CPU and

00:18:38,720 --> 00:18:47,840
memory allocated to each task and here

00:18:45,259 --> 00:18:50,450
is a database so this is Aurora this is

00:18:47,840 --> 00:18:52,940
kind of Amazon's equivalent of Oracle

00:18:50,450 --> 00:18:56,869
our competitor now this is a Postgres

00:18:52,940 --> 00:19:01,100
database again billing tags you can see

00:18:56,869 --> 00:19:01,730
that we now have a sub service of DB as

00:19:01,100 --> 00:19:06,110
opposed

00:19:01,730 --> 00:19:11,330
API or a Postgres is the database engine

00:19:06,110 --> 00:19:12,740
we also allow my sequel and then this is

00:19:11,330 --> 00:19:14,690
the instance class so if you're not

00:19:12,740 --> 00:19:17,029
familiar with cloud services I'm gonna

00:19:14,690 --> 00:19:18,289
be using this a lot throughout the rest

00:19:17,029 --> 00:19:21,470
of the talk so I guess I'll kind of

00:19:18,289 --> 00:19:23,269
explain it so our four is a family so

00:19:21,470 --> 00:19:27,590
it's a family of a type type of machine

00:19:23,269 --> 00:19:29,809
and this is a memory based or based

00:19:27,590 --> 00:19:32,299
instance so it has a lot more memory

00:19:29,809 --> 00:19:34,369
that does CPU and then the size so

00:19:32,299 --> 00:19:38,929
that's basically amount of CPU memory it

00:19:34,369 --> 00:19:42,980
has so this is our continuous

00:19:38,929 --> 00:19:45,080
integration deployment cycle CI CD we

00:19:42,980 --> 00:19:50,119
use Jenkins for our builds and

00:19:45,080 --> 00:19:51,950
deployments we build a docker file after

00:19:50,119 --> 00:19:55,549
doing a whole code review process and

00:19:51,950 --> 00:19:57,889
github and then we push it to elastic

00:19:55,549 --> 00:19:59,869
container repository so that's kind of a

00:19:57,889 --> 00:20:02,480
docker hub thing that Amazon manages for

00:19:59,869 --> 00:20:04,670
us and then we do a plan

00:20:02,480 --> 00:20:06,050
and apply what terraform to actually do

00:20:04,670 --> 00:20:08,240
the deployment so we feed in a version

00:20:06,050 --> 00:20:12,170
number and that force is a brand new

00:20:08,240 --> 00:20:15,530
version on dcs and i should mention that

00:20:12,170 --> 00:20:17,390
ECS functions is a state machine so if

00:20:15,530 --> 00:20:19,880
you feed it all the same information in

00:20:17,390 --> 00:20:21,980
it's not gonna change anything you have

00:20:19,880 --> 00:20:27,890
to keep getting a new version number in

00:20:21,980 --> 00:20:29,510
if you want things to redeploy okay oh I

00:20:27,890 --> 00:20:30,260
should also mention I don't have it up

00:20:29,510 --> 00:20:33,020
here

00:20:30,260 --> 00:20:36,010
use this great tool called Atlantis to

00:20:33,020 --> 00:20:38,690
actually code review our terraform code

00:20:36,010 --> 00:20:42,140
what that does is it runs a terraform

00:20:38,690 --> 00:20:46,040
plan and puts the output on the github

00:20:42,140 --> 00:20:47,600
PR and then you can run your plan apply

00:20:46,040 --> 00:20:52,520
actually through github it's really

00:20:47,600 --> 00:20:55,280
fantastic let's talk about load

00:20:52,520 --> 00:20:56,960
balancing a little bit so when we were

00:20:55,280 --> 00:21:00,020
doing this transition from ec2 and

00:20:56,960 --> 00:21:03,440
Beanstalk to this new containerized

00:21:00,020 --> 00:21:05,690
environment we were using a elby's order

00:21:03,440 --> 00:21:08,200
called elastic load balancers it was the

00:21:05,690 --> 00:21:11,690
first iteration of load bouncer on AWS

00:21:08,200 --> 00:21:15,800
they have now confusingly renamed them

00:21:11,690 --> 00:21:18,110
to CL B's or classic load bouncer so

00:21:15,800 --> 00:21:20,540
I'll just refer them as CL B's now but

00:21:18,110 --> 00:21:23,060
ELB is still spread throughout all the

00:21:20,540 --> 00:21:26,120
API it's it's it's a horrible mess it's

00:21:23,060 --> 00:21:29,690
very confusing but yeah we were moving

00:21:26,120 --> 00:21:31,700
to CL b / e lb - al b which is an

00:21:29,690 --> 00:21:35,200
application load balancer it actually

00:21:31,700 --> 00:21:40,040
works with docker and ECS really well

00:21:35,200 --> 00:21:42,020
the problem is the e lb was a layer four

00:21:40,040 --> 00:21:44,390
of the bouncers so if you had TCP

00:21:42,020 --> 00:21:49,160
applications you could not port those to

00:21:44,390 --> 00:21:52,190
the alb with the LB being layer 7 and LB

00:21:49,160 --> 00:21:53,780
also suddenly started to support HTTP -

00:21:52,190 --> 00:21:55,130
and if you've ever worked with that

00:21:53,780 --> 00:21:56,480
protocol you know that there are

00:21:55,130 --> 00:21:59,960
actually quite a few differences between

00:21:56,480 --> 00:22:01,730
that and HTTP uh-huh one thing we ran

00:21:59,960 --> 00:22:03,800
into very quickly was that headers are

00:22:01,730 --> 00:22:06,860
always passive lower case we actually

00:22:03,800 --> 00:22:08,750
had some services that assumed they were

00:22:06,860 --> 00:22:13,610
always upper case which was kind of

00:22:08,750 --> 00:22:16,400
funny and then L B's are also a lot more

00:22:13,610 --> 00:22:19,160
strict they're dealing with a higher

00:22:16,400 --> 00:22:22,340
err so they were not taking the

00:22:19,160 --> 00:22:24,620
malformed HTTP requests we're getting

00:22:22,340 --> 00:22:26,990
all sorts of weird error messages it was

00:22:24,620 --> 00:22:29,600
really really hard to debug and this was

00:22:26,990 --> 00:22:33,100
because our applications were creating

00:22:29,600 --> 00:22:35,840
custom requests to the load balancers

00:22:33,100 --> 00:22:38,090
like I said every single new service ran

00:22:35,840 --> 00:22:41,030
in something strange that we hadn't

00:22:38,090 --> 00:22:45,110
before there are also differences in how

00:22:41,030 --> 00:22:48,260
Amazon presented the metrics so the ELB

00:22:45,110 --> 00:22:51,800
provided very nice smooth connected dots

00:22:48,260 --> 00:22:54,200
and suddenly with a lbs probably to save

00:22:51,800 --> 00:22:55,940
space on their back-end if there was a

00:22:54,200 --> 00:22:57,710
request they just like throw a dot down

00:22:55,940 --> 00:22:59,420
and if there were two requests within

00:22:57,710 --> 00:23:01,310
like one minute of each other they would

00:22:59,420 --> 00:23:03,200
connect them so we'd have these like

00:23:01,310 --> 00:23:05,330
weird things so we had to figure out how

00:23:03,200 --> 00:23:06,800
to actually connect the dots in the

00:23:05,330 --> 00:23:10,970
graph so they one would look really

00:23:06,800 --> 00:23:13,340
strange yeah let's go back to task

00:23:10,970 --> 00:23:15,620
global I am role permissions because I I

00:23:13,340 --> 00:23:17,330
find this this is really important if

00:23:15,620 --> 00:23:20,840
you care about security and isolation

00:23:17,330 --> 00:23:24,170
between your services again they apply

00:23:20,840 --> 00:23:28,790
service commissions at the tasks level

00:23:24,170 --> 00:23:30,950
or our cases service level really do not

00:23:28,790 --> 00:23:33,140
share permissions and policies across

00:23:30,950 --> 00:23:36,250
services because what can happen with

00:23:33,140 --> 00:23:38,420
care for all sorts of badness you might

00:23:36,250 --> 00:23:39,950
you know write some terraform code that

00:23:38,420 --> 00:23:41,930
might blow away a completely different

00:23:39,950 --> 00:23:43,250
service if you're not careful if they

00:23:41,930 --> 00:23:46,670
have the same state file or you

00:23:43,250 --> 00:23:48,500
accidentally you know forget to change

00:23:46,670 --> 00:23:51,050
the state file path you might blow away

00:23:48,500 --> 00:23:53,030
a service so we try to keep all the

00:23:51,050 --> 00:23:54,860
policies completely independent we try

00:23:53,030 --> 00:23:57,410
to keep all the resources for every

00:23:54,860 --> 00:24:00,080
service independent so they don't share

00:23:57,410 --> 00:24:02,690
resources just in case you know

00:24:00,080 --> 00:24:05,480
something bad like that would happen the

00:24:02,690 --> 00:24:06,860
catch is you they need to be task little

00:24:05,480 --> 00:24:09,740
permissions you need to be supported by

00:24:06,860 --> 00:24:12,710
the client library that you're using so

00:24:09,740 --> 00:24:15,500
there are two client libraries for

00:24:12,710 --> 00:24:18,080
Python one is boto two which is the

00:24:15,500 --> 00:24:20,180
older version which doesn't support

00:24:18,080 --> 00:24:21,830
tasks global I am old permissions we're

00:24:20,180 --> 00:24:24,200
actually still phasing this out for

00:24:21,830 --> 00:24:26,630
summer services but boda 3 does and

00:24:24,200 --> 00:24:27,770
that's the one that you want to use from

00:24:26,630 --> 00:24:30,380
here on out because they're actually

00:24:27,770 --> 00:24:34,280
making changes to that service now

00:24:30,380 --> 00:24:37,070
to that client library the other

00:24:34,280 --> 00:24:39,050
important thing is standardizing the

00:24:37,070 --> 00:24:40,120
microservices how you create them how

00:24:39,050 --> 00:24:43,490
you name them

00:24:40,120 --> 00:24:45,500
you should be establishing this day 1 I

00:24:43,490 --> 00:24:46,880
can't even imagine running half a dozen

00:24:45,500 --> 00:24:50,030
services without some sort of

00:24:46,880 --> 00:24:52,580
standardization so again develop a

00:24:50,030 --> 00:24:54,350
common naming scheme Auto generate as

00:24:52,580 --> 00:24:56,540
much of the initial code as you possibly

00:24:54,350 --> 00:24:59,540
can just to make it easy to onboard new

00:24:56,540 --> 00:25:01,100
services move core functionality to

00:24:59,540 --> 00:25:03,830
shared libraries especially any

00:25:01,100 --> 00:25:06,680
interactions with AWS because you're

00:25:03,830 --> 00:25:09,920
likely going to be doing that a lot you

00:25:06,680 --> 00:25:13,130
want to keep that functionality very

00:25:09,920 --> 00:25:15,140
clean and share it so that not every

00:25:13,130 --> 00:25:19,400
service has to re-implement that sort of

00:25:15,140 --> 00:25:21,890
functionality also we're very big into

00:25:19,400 --> 00:25:25,700
providing standard alarms and dashboards

00:25:21,890 --> 00:25:27,800
so as of probably little over six months

00:25:25,700 --> 00:25:31,580
ago now every single engineer in the

00:25:27,800 --> 00:25:32,930
company is on call so what does that

00:25:31,580 --> 00:25:34,670
mean you're going to be on call for

00:25:32,930 --> 00:25:37,280
services you may not know very much

00:25:34,670 --> 00:25:39,920
about so it it helps to have the same

00:25:37,280 --> 00:25:42,550
dashboards for every service we allow

00:25:39,920 --> 00:25:45,830
teams to create their own like front-end

00:25:42,550 --> 00:25:47,390
custom dashboards if they want but they

00:25:45,830 --> 00:25:49,970
also have these manually or if sorry

00:25:47,390 --> 00:25:52,370
automatically created dashboards behind

00:25:49,970 --> 00:25:54,740
that just write a nice stable interface

00:25:52,370 --> 00:25:57,280
we also periodically review micro

00:25:54,740 --> 00:25:59,600
services for consistency in quality so

00:25:57,280 --> 00:26:02,600
people might you know rotate in and out

00:25:59,600 --> 00:26:03,890
of teams they may have a varying

00:26:02,600 --> 00:26:05,420
understanding of how the service

00:26:03,890 --> 00:26:07,970
actually works when they're making

00:26:05,420 --> 00:26:10,760
changes so the code can get kind of

00:26:07,970 --> 00:26:15,260
crufty so you have to kind of reel the

00:26:10,760 --> 00:26:17,710
services back every once in a while so

00:26:15,260 --> 00:26:19,880
this is what one of our standard

00:26:17,710 --> 00:26:23,540
dashboards for a web service looks like

00:26:19,880 --> 00:26:27,410
so important thing is we have both local

00:26:23,540 --> 00:26:29,390
time and UTC at the top for whatever

00:26:27,410 --> 00:26:34,730
reason when you're looking at the Amazon

00:26:29,390 --> 00:26:36,830
web interface some metrics and screens

00:26:34,730 --> 00:26:39,170
are in local time somewhere in your TC

00:26:36,830 --> 00:26:42,950
it drives me insane it just kind of

00:26:39,170 --> 00:26:43,970
demonstrates the silos within Amazon so

00:26:42,950 --> 00:26:46,039
sometimes you need both

00:26:43,970 --> 00:26:49,190
I mean all of our internal timestamps

00:26:46,039 --> 00:26:50,840
are UTC but for whatever reason there

00:26:49,190 --> 00:26:53,629
might be something we're they're looking

00:26:50,840 --> 00:26:56,320
at where it's not so I think it's really

00:26:53,629 --> 00:26:59,929
important to have that we also show

00:26:56,320 --> 00:27:01,340
healthy unhealthy containers and the

00:26:59,929 --> 00:27:05,889
number of running tasks with each

00:27:01,340 --> 00:27:08,960
service lien sees number requests CPU

00:27:05,889 --> 00:27:11,980
and another important thing is to split

00:27:08,960 --> 00:27:14,330
all the errors by both load balancer

00:27:11,980 --> 00:27:16,220
which will show all the errors in

00:27:14,330 --> 00:27:18,379
frontal-lobe balancer the load balancer

00:27:16,220 --> 00:27:20,480
might be handling that may never get to

00:27:18,379 --> 00:27:23,419
the service and then also show the

00:27:20,480 --> 00:27:24,950
service errors and we also split this by

00:27:23,419 --> 00:27:27,440
a Z so if you're not familiar with the

00:27:24,950 --> 00:27:29,720
terminology that's an availability zone

00:27:27,440 --> 00:27:33,259
which is kind of one or more physical

00:27:29,720 --> 00:27:35,480
buildings within an Amazon region so you

00:27:33,259 --> 00:27:39,230
could just kind of think of that as one

00:27:35,480 --> 00:27:40,759
or more data centers and they're

00:27:39,230 --> 00:27:43,700
supposed to be completely isolated

00:27:40,759 --> 00:27:45,679
that's not always true but it's still a

00:27:43,700 --> 00:27:49,639
good thing to see because one of those

00:27:45,679 --> 00:27:51,980
buildings might completely lose power so

00:27:49,639 --> 00:27:54,529
if you see one AZ across all your

00:27:51,980 --> 00:27:56,600
services it's throwing errors you might

00:27:54,529 --> 00:28:02,690
just knock that AZ out and scale up

00:27:56,600 --> 00:28:05,559
across other Razi's we have a worker

00:28:02,690 --> 00:28:07,850
dashboard here which is very similar

00:28:05,559 --> 00:28:09,679
it's much more simple obviously because

00:28:07,850 --> 00:28:10,629
you don't have requests and those sorts

00:28:09,679 --> 00:28:13,700
of things

00:28:10,629 --> 00:28:15,860
CPU and memory we also have a queue

00:28:13,700 --> 00:28:17,750
which shows the number of messages

00:28:15,860 --> 00:28:19,970
available and then never imagine the

00:28:17,750 --> 00:28:22,519
number of messages deleted so if the

00:28:19,970 --> 00:28:24,080
number of visible or available messages

00:28:22,519 --> 00:28:27,129
keeps going up and up and up you know

00:28:24,080 --> 00:28:30,259
that your work or service is struggling

00:28:27,129 --> 00:28:34,730
and we also integrate this all with

00:28:30,259 --> 00:28:38,210
pager duty and define all of our alarms

00:28:34,730 --> 00:28:40,460
within terraform itself so we have a

00:28:38,210 --> 00:28:43,629
bunch of default alarms but the teams

00:28:40,460 --> 00:28:47,529
are very welcome to adjust those as

00:28:43,629 --> 00:28:51,080
needed we also have high latency an

00:28:47,529 --> 00:28:54,070
emergency alarm which will actually page

00:28:51,080 --> 00:28:57,070
and then an example of a you know

00:28:54,070 --> 00:28:59,890
low alarm would be you know memory usage

00:28:57,070 --> 00:29:02,230
if we get to 100% and TAS start you know

00:28:59,890 --> 00:29:04,180
shutting down that's a problem but if we

00:29:02,230 --> 00:29:05,860
get to like a 70 or 80% memory it's

00:29:04,180 --> 00:29:09,280
normally not a big deal it can wait to

00:29:05,860 --> 00:29:11,920
the next morning and another important

00:29:09,280 --> 00:29:13,510
thing is to have play books again if a

00:29:11,920 --> 00:29:15,040
lot of your engineers are on call with

00:29:13,510 --> 00:29:17,380
services they're you know they're not

00:29:15,040 --> 00:29:18,660
that familiar with include links on the

00:29:17,380 --> 00:29:24,250
alarm so they could just click through

00:29:18,660 --> 00:29:26,500
nice and easy I talked about grading of

00:29:24,250 --> 00:29:28,750
micro-services a little bit so we game

00:29:26,500 --> 00:29:31,150
if I learning we try to gamify this

00:29:28,750 --> 00:29:32,520
process to kind of make it fun so we

00:29:31,150 --> 00:29:36,100
have four different levels

00:29:32,520 --> 00:29:37,810
it's a bronze silver gold and platinum

00:29:36,100 --> 00:29:40,000
so if you have a service has all

00:29:37,810 --> 00:29:42,670
platinum shields that's like top at the

00:29:40,000 --> 00:29:44,860
top service you want to get there and we

00:29:42,670 --> 00:29:47,320
have all these different categories this

00:29:44,860 --> 00:29:50,770
is what one of our grading rubrics looks

00:29:47,320 --> 00:29:53,380
like so the the green checks are

00:29:50,770 --> 00:29:56,470
actually automated so we can do a lot of

00:29:53,380 --> 00:29:58,990
checks on each repo automatically I

00:29:56,470 --> 00:30:02,440
should mention every micro service has

00:29:58,990 --> 00:30:05,050
its own repo and within each one of

00:30:02,440 --> 00:30:06,850
those repos we have different terraform

00:30:05,050 --> 00:30:09,700
environments so all the terraform code

00:30:06,850 --> 00:30:15,040
for each service lives in its services

00:30:09,700 --> 00:30:17,050
repo that's really important I feel so

00:30:15,040 --> 00:30:19,150
we have some automated checks like is

00:30:17,050 --> 00:30:20,560
there a readme file and then we have

00:30:19,150 --> 00:30:24,760
some things that you can't really check

00:30:20,560 --> 00:30:27,610
automatically like is this good toad you

00:30:24,760 --> 00:30:29,920
know what we do is we kind of sits in a

00:30:27,610 --> 00:30:31,750
conference room and just you know take a

00:30:29,920 --> 00:30:34,870
look and see oh that that doesn't look

00:30:31,750 --> 00:30:37,150
so great I don't know that's a bronze

00:30:34,870 --> 00:30:38,470
something like that now if you have a

00:30:37,150 --> 00:30:40,960
way to automate that please let me know

00:30:38,470 --> 00:30:45,700
I really like you know automated code

00:30:40,960 --> 00:30:46,990
quality checking ok so now we're into

00:30:45,700 --> 00:30:49,780
talking about cost reduction

00:30:46,990 --> 00:30:52,570
I love reducing cost in infrastructure

00:30:49,780 --> 00:30:56,230
it's really it's really fun for me at

00:30:52,570 --> 00:30:58,120
least so when you're in a docker eyes

00:30:56,230 --> 00:31:00,280
environment there's normally two ways

00:30:58,120 --> 00:31:03,160
you can reduce costs so you can reduce

00:31:00,280 --> 00:31:05,740
the cluster so this is the large group

00:31:03,160 --> 00:31:07,130
of machines where your containers

00:31:05,740 --> 00:31:09,470
actually run

00:31:07,130 --> 00:31:12,100
so you could have different instance

00:31:09,470 --> 00:31:14,690
types so these are the sizes of VMS

00:31:12,100 --> 00:31:16,970
available from amazon you get a

00:31:14,690 --> 00:31:18,799
different pricing options boy there's so

00:31:16,970 --> 00:31:22,160
so many options in the Amazon you can

00:31:18,799 --> 00:31:24,740
have you can pay for machines up front

00:31:22,160 --> 00:31:27,620
which gives you about 40% cost reduction

00:31:24,740 --> 00:31:30,140
you can do them on demand or you have a

00:31:27,620 --> 00:31:33,530
hundred percent of the cost you pay

00:31:30,140 --> 00:31:36,340
incrementally per hour there are also

00:31:33,530 --> 00:31:40,039
things called spot where you actually

00:31:36,340 --> 00:31:43,340
throw in bids where you can get up to

00:31:40,039 --> 00:31:44,720
90% cost reduction which is nice but the

00:31:43,340 --> 00:31:46,940
catch there is the machines can

00:31:44,720 --> 00:31:51,620
disappear within two minutes at any

00:31:46,940 --> 00:31:53,030
point in time and I'll get into that you

00:31:51,620 --> 00:31:54,500
could auto scale so if you're just

00:31:53,030 --> 00:31:57,919
running you know the same number of

00:31:54,500 --> 00:32:00,440
machines at all times and your traffic

00:31:57,919 --> 00:32:02,510
looks like a sine wave well you're

00:32:00,440 --> 00:32:03,740
probably wasting a lot of money so

00:32:02,510 --> 00:32:05,870
you're not on a scale but there's also

00:32:03,740 --> 00:32:09,110
you know dangers with that too so you

00:32:05,870 --> 00:32:11,299
have two-way stability with cost and you

00:32:09,110 --> 00:32:13,100
could also add or remove a Z's and

00:32:11,299 --> 00:32:15,380
you're probably like this is weird so

00:32:13,100 --> 00:32:17,330
like what does it matter whether we add

00:32:15,380 --> 00:32:20,030
more data center buildings or not well

00:32:17,330 --> 00:32:23,440
the reality is not all the data centers

00:32:20,030 --> 00:32:26,600
have the same physical machine types or

00:32:23,440 --> 00:32:28,640
one data center might have really old

00:32:26,600 --> 00:32:31,250
servers and one might have fully new

00:32:28,640 --> 00:32:33,110
servers they're much faster and Amazon

00:32:31,250 --> 00:32:33,890
doesn't really tell you this but it's

00:32:33,110 --> 00:32:36,710
totally true

00:32:33,890 --> 00:32:39,130
all the data centers in each region are

00:32:36,710 --> 00:32:41,210
not equal by any stretch of imagination

00:32:39,130 --> 00:32:43,070
because they'll fill up like an entire

00:32:41,210 --> 00:32:44,330
building and then build a whole new

00:32:43,070 --> 00:32:49,250
building and those will have all new

00:32:44,330 --> 00:32:51,409
service I mean it totally makes sense so

00:32:49,250 --> 00:32:53,780
the other way to save cost is by

00:32:51,409 --> 00:32:56,809
actually tweaking the tasks of the

00:32:53,780 --> 00:33:00,710
services themselves so you know not

00:32:56,809 --> 00:33:03,230
using too much CPU memory and you could

00:33:00,710 --> 00:33:04,760
also Auto scale of tasks so the

00:33:03,230 --> 00:33:09,470
containers within the machines

00:33:04,760 --> 00:33:11,960
themselves so back when we first started

00:33:09,470 --> 00:33:16,940
and launched this in production this is

00:33:11,960 --> 00:33:18,799
now three years ago we started with this

00:33:16,940 --> 00:33:20,179
fixed machine type we're actually using

00:33:18,799 --> 00:33:20,720
this for almost everything at the time

00:33:20,179 --> 00:33:24,460
it's probably

00:33:20,720 --> 00:33:27,230
70% of our infrastructure so ac3 family

00:33:24,460 --> 00:33:30,770
so it's compute optimized rather memory

00:33:27,230 --> 00:33:33,409
optimized with a size of 2x large I

00:33:30,770 --> 00:33:35,510
believe that was 16 gigabytes a memory

00:33:33,409 --> 00:33:38,809
thing please correct me if I'm wrong

00:33:35,510 --> 00:33:41,330
right I think that's what it was and we

00:33:38,809 --> 00:33:43,400
bought some reserves so that means that

00:33:41,330 --> 00:33:46,970
we paid upfront for a lot of our

00:33:43,400 --> 00:33:50,870
machines and then the rest was on demand

00:33:46,970 --> 00:33:53,360
so we got roughly a four percent cost

00:33:50,870 --> 00:33:56,750
savings on maybe 40 to 50 percent of our

00:33:53,360 --> 00:34:02,510
infrastructure at the time so fast

00:33:56,750 --> 00:34:04,669
forward a year or two the c4 large came

00:34:02,510 --> 00:34:06,710
out the problem there is it didn't have

00:34:04,669 --> 00:34:09,440
a local disk it just had a network-based

00:34:06,710 --> 00:34:11,659
disk option and we were doing a ton of

00:34:09,440 --> 00:34:17,320
logging and that would have cost a ton

00:34:11,659 --> 00:34:19,520
of money so I should skip that and then

00:34:17,320 --> 00:34:23,960
one of the best days of my career was

00:34:19,520 --> 00:34:27,230
c5v came out which local nvme storage

00:34:23,960 --> 00:34:29,839
that was really great

00:34:27,230 --> 00:34:33,260
which was also 25% faster than the

00:34:29,839 --> 00:34:36,980
previous generation and also cheaper so

00:34:33,260 --> 00:34:39,200
it was 9 cents per hour and if you do

00:34:36,980 --> 00:34:42,589
the math there the sea fire was also 50%

00:34:39,200 --> 00:34:49,250
faster than what we were using so

00:34:42,589 --> 00:34:52,070
win-win really great the problem is that

00:34:49,250 --> 00:34:53,990
keep going is how ECS was actually

00:34:52,070 --> 00:34:56,629
scaling the tasks which was really weird

00:34:53,990 --> 00:35:00,890
if you think about it so they had the

00:34:56,629 --> 00:35:04,880
cpu unit which just counts the fractions

00:35:00,890 --> 00:35:07,160
of a core and if you think about it like

00:35:04,880 --> 00:35:10,220
cores are not equal across generations

00:35:07,160 --> 00:35:12,050
of machines so we were actually hitting

00:35:10,220 --> 00:35:15,320
scaling issues where it wasn't scaling

00:35:12,050 --> 00:35:17,420
up when I was supposed to so rather than

00:35:15,320 --> 00:35:19,070
being you know 60% CPU when I was

00:35:17,420 --> 00:35:22,160
supposed to scale it was it like 30 or

00:35:19,070 --> 00:35:23,810
40 percent CPU and we actually started

00:35:22,160 --> 00:35:28,369
to run out of threads on some of their

00:35:23,810 --> 00:35:30,410
early clusters which is a huge problem

00:35:28,369 --> 00:35:31,640
in in Amazon never really like talks

00:35:30,410 --> 00:35:33,349
about this at all sorry I thought I

00:35:31,640 --> 00:35:34,910
would mention it because you will hit it

00:35:33,349 --> 00:35:39,590
if you upgrade

00:35:34,910 --> 00:35:40,790
generations with an ax cluster and I'll

00:35:39,590 --> 00:35:44,540
show actually how you do that

00:35:40,790 --> 00:35:46,760
calculation so we started with this

00:35:44,540 --> 00:35:49,040
fixed number of instances and we wanted

00:35:46,760 --> 00:35:50,990
to get two here we wanted auto-scale at

00:35:49,040 --> 00:35:53,690
the container level and at the machine

00:35:50,990 --> 00:35:56,270
level very dynamic environment fully

00:35:53,690 --> 00:35:58,820
replaceable and we probably replace 80

00:35:56,270 --> 00:36:02,450
percent of our machines I don't know

00:35:58,820 --> 00:36:05,480
twice a month I mean early on I was

00:36:02,450 --> 00:36:07,340
replacing entire clusters four or five

00:36:05,480 --> 00:36:09,650
times a day just the stress tested and

00:36:07,340 --> 00:36:11,510
it worked pretty well so we actually

00:36:09,650 --> 00:36:14,000
wanted to start using all sizes of

00:36:11,510 --> 00:36:16,430
machines available to us from Amazon so

00:36:14,000 --> 00:36:17,630
we're using the compute ones the c-5s we

00:36:16,430 --> 00:36:20,990
also want to bring in the memory

00:36:17,630 --> 00:36:23,119
intensive one so the m5 because some of

00:36:20,990 --> 00:36:25,040
our applications needed more memory some

00:36:23,119 --> 00:36:27,350
of them required more CPU so we wanted

00:36:25,040 --> 00:36:29,720
to get all those options and we also

00:36:27,350 --> 00:36:31,970
wanted to bring in the bidding market

00:36:29,720 --> 00:36:35,030
with the spot and get you know up to 90%

00:36:31,970 --> 00:36:38,960
cost reduction to kind of reduce our

00:36:35,030 --> 00:36:40,220
costs there Amazon now has options for

00:36:38,960 --> 00:36:42,650
this at the time

00:36:40,220 --> 00:36:44,740
it wasn't so great so we ended up going

00:36:42,650 --> 00:36:48,830
with this third party called spot inst

00:36:44,740 --> 00:36:50,359
highly recommend them even today it gave

00:36:48,830 --> 00:36:53,030
us a lot of different features so we're

00:36:50,359 --> 00:36:55,460
able to mix the Machine sizes and

00:36:53,030 --> 00:36:58,550
families we're able to use all of our

00:36:55,460 --> 00:37:02,210
eyes which you had purchased up front so

00:36:58,550 --> 00:37:05,230
any any there we don't use is kind of a

00:37:02,210 --> 00:37:07,160
waste of money they also have a lot of

00:37:05,230 --> 00:37:09,170
machine learning algorithms that

00:37:07,160 --> 00:37:11,570
actually predict when machines are gonna

00:37:09,170 --> 00:37:13,310
shut down within a 15-minute window

00:37:11,570 --> 00:37:16,580
rather than two minutes which is what

00:37:13,310 --> 00:37:18,080
Amazon gave us and what's really neat to

00:37:16,580 --> 00:37:21,590
me is that they actually fit the

00:37:18,080 --> 00:37:23,750
capacity of the VMS to the service

00:37:21,590 --> 00:37:28,100
themselves so it's kind of like a

00:37:23,750 --> 00:37:31,790
reverse bin packing and they also gave

00:37:28,100 --> 00:37:33,619
us these AZ capacity heat maps so I

00:37:31,790 --> 00:37:35,810
don't know if any of you can see this

00:37:33,619 --> 00:37:38,300
but on the left hand side are the

00:37:35,810 --> 00:37:42,020
different AZ's and then at the bottom

00:37:38,300 --> 00:37:45,590
are the types and sizes of machines and

00:37:42,020 --> 00:37:46,970
you can see that US East 1e basically

00:37:45,590 --> 00:37:47,890
does not have any of the machine types

00:37:46,970 --> 00:37:49,710
that we need

00:37:47,890 --> 00:37:51,400
so we basically just don't use that

00:37:49,710 --> 00:37:53,710
Amazon doesn't provide you with

00:37:51,400 --> 00:37:55,839
hannelius information I think it's at

00:37:53,710 --> 00:37:59,589
least in this format it's a really great

00:37:55,839 --> 00:38:03,099
feature some other things it does for us

00:37:59,589 --> 00:38:04,960
it automatically drains tasks so that

00:38:03,099 --> 00:38:09,369
you're not throwing you know five

00:38:04,960 --> 00:38:11,319
hundreds or four or fours when a task is

00:38:09,369 --> 00:38:14,440
shutting down gives you a little bit of

00:38:11,319 --> 00:38:17,769
headroom so if we have a sudden traffic

00:38:14,440 --> 00:38:20,049
spike if you know somebody posts review

00:38:17,769 --> 00:38:23,589
on YouTube of duolingo and suddenly we

00:38:20,049 --> 00:38:24,549
have 10,000 requests per second more

00:38:23,589 --> 00:38:28,660
than we were expecting

00:38:24,549 --> 00:38:31,480
we have nice capacity Headroom there it

00:38:28,660 --> 00:38:33,190
also spreads capacity across all the

00:38:31,480 --> 00:38:36,910
AZ's and all the data centers so that

00:38:33,190 --> 00:38:38,619
gives you a lot of reliability it

00:38:36,910 --> 00:38:40,599
guarantees in case one of the data

00:38:38,619 --> 00:38:44,920
center starts acting a little bit wonky

00:38:40,599 --> 00:38:47,739
and the best of the best is that they

00:38:44,920 --> 00:38:50,430
bill based on how much you save so

00:38:47,739 --> 00:38:53,529
they're incentivized to save you money a

00:38:50,430 --> 00:38:56,680
lot of the actually I'd say almost all

00:38:53,529 --> 00:38:58,690
of the cloud vendors that say they're

00:38:56,680 --> 00:38:59,890
gonna save you a ton of money they bill

00:38:58,690 --> 00:39:03,700
you based on how much you're spending

00:38:59,890 --> 00:39:05,559
which is complete opposite okay why

00:39:03,700 --> 00:39:09,640
would you want that you're gonna save me

00:39:05,559 --> 00:39:11,559
any money and we all today you know and

00:39:09,640 --> 00:39:14,230
also supported terraform Watchers which

00:39:11,559 --> 00:39:16,480
was great so this is what our Auto

00:39:14,230 --> 00:39:18,539
scaled environment looks like these are

00:39:16,480 --> 00:39:21,880
the different billing options so

00:39:18,539 --> 00:39:25,690
reserved as the purple so that's what we

00:39:21,880 --> 00:39:27,700
paid up front for the green is the spot

00:39:25,690 --> 00:39:29,890
market so those are the things that can

00:39:27,700 --> 00:39:32,950
disappear in any moment so we try to run

00:39:29,890 --> 00:39:37,450
about 50/50 we don't want to go too

00:39:32,950 --> 00:39:38,650
crazy there there are some large large

00:39:37,450 --> 00:39:39,970
companies that actually run almost a

00:39:38,650 --> 00:39:43,720
hundred percent spot I think that's a

00:39:39,970 --> 00:39:46,809
little bit too much and then there's a

00:39:43,720 --> 00:39:48,759
tiny tiny sliver of blue which is on

00:39:46,809 --> 00:39:51,609
demand so that's the most expensive and

00:39:48,759 --> 00:39:55,599
we barely use any of that anymore

00:39:51,609 --> 00:39:59,140
very quickly per micro service costs we

00:39:55,599 --> 00:40:01,660
try to get services to about 60% CPU and

00:39:59,140 --> 00:40:03,579
sixty percent sixty eighty percent of

00:40:01,660 --> 00:40:06,849
usage that gives you a nice little

00:40:03,579 --> 00:40:09,430
buffer obviously if your service comes

00:40:06,849 --> 00:40:11,589
online and the memory usage is going up

00:40:09,430 --> 00:40:15,760
until the right you probably have a

00:40:11,589 --> 00:40:19,839
memory leak should fix it

00:40:15,760 --> 00:40:21,910
and another important thing here is you

00:40:19,839 --> 00:40:24,039
should always have your docker tasks

00:40:21,910 --> 00:40:27,369
exit you shouldn't over allocate memory

00:40:24,039 --> 00:40:29,319
you might have tasks starting to crash

00:40:27,369 --> 00:40:30,819
other tasks on the same machine and you

00:40:29,319 --> 00:40:35,680
never want that to happen

00:40:30,819 --> 00:40:38,770
I mentioned the adjusting CPU for

00:40:35,680 --> 00:40:41,430
scaling this is how you calculate it you

00:40:38,770 --> 00:40:43,299
could kind of eyeball if you want to but

00:40:41,430 --> 00:40:47,140
if you have this problem we should

00:40:43,299 --> 00:40:49,569
probably screenshot this slide you

00:40:47,140 --> 00:40:51,309
essentially multiply allocated CPU times

00:40:49,569 --> 00:40:53,349
the current utilization to give you the

00:40:51,309 --> 00:40:56,500
CPU you're actually using and then

00:40:53,349 --> 00:40:57,670
divide the actual CPU divided by what

00:40:56,500 --> 00:41:01,569
you actually want in terms of

00:40:57,670 --> 00:41:05,049
utilization so there's an example there

00:41:01,569 --> 00:41:07,539
I should mention that 1024 units is just

00:41:05,049 --> 00:41:14,799
one CPU core on whatever machine type

00:41:07,539 --> 00:41:17,319
you're using I think I mentioned most of

00:41:14,799 --> 00:41:20,309
this yeah that's just in bullet form

00:41:17,319 --> 00:41:20,309
skip that

00:41:20,950 --> 00:41:27,789
let's just talk briefly about API costs

00:41:24,099 --> 00:41:29,589
in general on AWS I think I mentioned

00:41:27,789 --> 00:41:32,140
this at the beginning but there are a

00:41:29,589 --> 00:41:34,240
lot of different hidden costs that are

00:41:32,140 --> 00:41:37,410
really difficult to track down so this

00:41:34,240 --> 00:41:40,150
is our s3 bucket costs a few years ago

00:41:37,410 --> 00:41:41,770
so it's an object store you would think

00:41:40,150 --> 00:41:43,180
that you'd be paying for actual storage

00:41:41,770 --> 00:41:45,490
that would be the thing you're worried

00:41:43,180 --> 00:41:48,279
about but that's not always the case

00:41:45,490 --> 00:41:50,079
so it turned out that our bill just kept

00:41:48,279 --> 00:41:53,200
increasing month over month over month

00:41:50,079 --> 00:41:54,730
and our storage was actually going on

00:41:53,200 --> 00:41:56,710
I'm like why are we paying so much more

00:41:54,730 --> 00:41:59,289
money so I actually divided the bill

00:41:56,710 --> 00:42:02,020
into API Kyle calls and you can actually

00:41:59,289 --> 00:42:05,400
see that list buckets plus get object

00:42:02,020 --> 00:42:07,390
was over 50% of our costs on s3 which is

00:42:05,400 --> 00:42:10,150
staggering to me and it turned out we

00:42:07,390 --> 00:42:12,730
just had a service that had done their

00:42:10,150 --> 00:42:15,510
own s3 implementation that was just like

00:42:12,730 --> 00:42:16,860
going bonkers on s3 like every

00:42:15,510 --> 00:42:20,340
request was like doing a list on my

00:42:16,860 --> 00:42:23,210
buckets call so that that was actually

00:42:20,340 --> 00:42:26,670
fairly easy to to fix means we found it

00:42:23,210 --> 00:42:30,510
there are also lots of gotcha limits on

00:42:26,670 --> 00:42:31,680
AWS and I swear I find a new one every

00:42:30,510 --> 00:42:33,920
couple weeks even though I've been using

00:42:31,680 --> 00:42:37,770
the service for five and a half years

00:42:33,920 --> 00:42:45,200
this was a really crazy one so a little

00:42:37,770 --> 00:42:47,910
bit of history ec2 while still does runs

00:42:45,200 --> 00:42:50,100
what did it used to run on oh it used to

00:42:47,910 --> 00:42:52,500
run on Zen Allen still does the old

00:42:50,100 --> 00:42:55,400
instance types and they switched to a

00:42:52,500 --> 00:42:58,700
cam what is it

00:42:55,400 --> 00:43:03,720
yeah nitro but what's it based off of

00:42:58,700 --> 00:43:05,160
yeah KBM thank you so they have this

00:43:03,720 --> 00:43:07,530
brand new system a lot of it is

00:43:05,160 --> 00:43:10,380
implemented hardware it's all nice and

00:43:07,530 --> 00:43:12,480
good it turns out they stop caching DNS

00:43:10,380 --> 00:43:16,470
requests on the machine the actual

00:43:12,480 --> 00:43:19,260
physical machine and didn't really tell

00:43:16,470 --> 00:43:22,890
it Tony one I actually found this in a

00:43:19,260 --> 00:43:24,630
reddit post because we had clusters that

00:43:22,890 --> 00:43:25,860
were throwing these random errors every

00:43:24,630 --> 00:43:27,660
once in a while like they couldn't

00:43:25,860 --> 00:43:31,440
connect to you know a different micro

00:43:27,660 --> 00:43:34,590
service and we found this DNS request

00:43:31,440 --> 00:43:36,390
limit of 1024 per second and if you have

00:43:34,590 --> 00:43:37,380
a really large machine with hundreds of

00:43:36,390 --> 00:43:40,140
tasks running on it

00:43:37,380 --> 00:43:41,760
you're gonna hit this pretty quickly so

00:43:40,140 --> 00:43:43,830
we actually just have to put a caching

00:43:41,760 --> 00:43:46,800
daemon on the machine which is kind of

00:43:43,830 --> 00:43:48,210
annoying I don't know yeah if you're

00:43:46,800 --> 00:43:51,180
using the cloud you're gonna hate all

00:43:48,210 --> 00:43:55,200
sorts of crazy limits like this so just

00:43:51,180 --> 00:43:58,080
in a summary we reduced our compute cost

00:43:55,200 --> 00:44:00,390
by 60% through this project you can

00:43:58,080 --> 00:44:04,170
actually see it on the right there from

00:44:00,390 --> 00:44:06,930
May to October which also reduce our

00:44:04,170 --> 00:44:10,170
cost per monthly active user on duolingo

00:44:06,930 --> 00:44:15,180
by 30% and our overall AWS bill went

00:44:10,170 --> 00:44:17,720
down by 25% we also had some key results

00:44:15,180 --> 00:44:20,310
here so right now we manage about

00:44:17,720 --> 00:44:24,360
hundred micro service this is probably

00:44:20,310 --> 00:44:27,240
around 150 nearing 200 now this is about

00:44:24,360 --> 00:44:29,170
a year out of date all of our teams now

00:44:27,240 --> 00:44:32,109
deploy their own micro services

00:44:29,170 --> 00:44:33,819
most I would say 98% of them I deploy

00:44:32,109 --> 00:44:36,549
and merge so there you have to worry

00:44:33,819 --> 00:44:37,960
about deployment we now officially

00:44:36,549 --> 00:44:41,380
support three different programming

00:44:37,960 --> 00:44:43,960
languages including things like Java and

00:44:41,380 --> 00:44:46,089
Scala which were completely new runtimes

00:44:43,960 --> 00:44:48,190
for us and we actually hit the quarter

00:44:46,089 --> 00:44:49,960
after some limitation we hit four nines

00:44:48,190 --> 00:44:52,720
for the first time ever which was

00:44:49,960 --> 00:44:56,890
fantastic and then again a 60% reduction

00:44:52,720 --> 00:44:59,380
in compute costs so just a quick pitch

00:44:56,890 --> 00:45:02,529
if he likes like this sort of stuff

00:44:59,380 --> 00:45:04,269
we're always hiring our main offices in

00:45:02,529 --> 00:45:06,700
Pittsburgh Pennsylvania most of our

00:45:04,269 --> 00:45:09,640
people were there but we also have

00:45:06,700 --> 00:45:13,150
offices in New York City Beijing and

00:45:09,640 --> 00:45:16,599
Seattle so check it out we're talking

00:45:13,150 --> 00:45:18,369
afterwards all right here's the final

00:45:16,599 --> 00:45:21,099
slide and the one for your phone so

00:45:18,369 --> 00:45:23,200
these are all the different tools and

00:45:21,099 --> 00:45:25,630
services that we use and there's some

00:45:23,200 --> 00:45:27,220
really good references there if you're

00:45:25,630 --> 00:45:28,630
interested in micro services and how to

00:45:27,220 --> 00:45:31,059
actually split up your model with

00:45:28,630 --> 00:45:33,490
effectively builder micro services

00:45:31,059 --> 00:45:35,829
designing fine-grained systems by san

00:45:33,490 --> 00:45:37,420
Neumann is kind of de facto micro

00:45:35,829 --> 00:45:39,970
services book

00:45:37,420 --> 00:45:42,160
Susan Fowler's book is also really good

00:45:39,970 --> 00:45:45,250
that actually shows you how to run the

00:45:42,160 --> 00:45:46,779
stuff in production which is kind of an

00:45:45,250 --> 00:45:49,990
operational perspective rather than a

00:45:46,779 --> 00:45:53,410
developer's perspective and there are

00:45:49,990 --> 00:45:56,410
also some really good clinical open

00:45:53,410 --> 00:45:59,529
source references I find the open guides

00:45:56,410 --> 00:46:01,329
in github sometimes a lot better than

00:45:59,529 --> 00:46:05,230
Amazon's own documentation this is kind

00:46:01,329 --> 00:46:07,450
of community driven documentation they

00:46:05,230 --> 00:46:12,039
point out a lot of gotchas that I

00:46:07,450 --> 00:46:14,549
mentioned actually there okay quickly

00:46:12,039 --> 00:46:14,549
questions

00:46:38,690 --> 00:46:47,789
if you're familiar with an auto scaling

00:46:40,829 --> 00:46:50,759
group okay so what they do they allow

00:46:47,789 --> 00:46:52,769
you to scale based on some metric so we

00:46:50,759 --> 00:46:54,989
do it based off of memory for our

00:46:52,769 --> 00:46:57,930
clusters so if the memory increase

00:46:54,989 --> 00:47:00,829
increases they'll automatically add one

00:46:57,930 --> 00:47:04,140
or more machines to the cluster and

00:47:00,829 --> 00:47:06,720
they'll fit it based on the tasks

00:47:04,140 --> 00:47:07,890
running on that cluster so we don't have

00:47:06,720 --> 00:47:10,259
to worry about what size they'll pick

00:47:07,890 --> 00:47:13,349
the right size and they'll also adjust

00:47:10,259 --> 00:47:15,989
the pricing options to ours 50/50 ratio

00:47:13,349 --> 00:47:29,849
at least as much as possible so they

00:47:15,989 --> 00:47:31,589
manage the cluster state for us so if

00:47:29,849 --> 00:47:33,749
you run any sort of application with

00:47:31,589 --> 00:47:35,670
like a load balancer in front you might

00:47:33,749 --> 00:47:39,630
have a period of time where the service

00:47:35,670 --> 00:47:41,640
still looks healthy you know internally

00:47:39,630 --> 00:47:43,769
but it really isn't you might get errors

00:47:41,640 --> 00:47:47,279
if you hit it just like you do you have

00:47:43,769 --> 00:47:48,690
a start up time before it goes live so

00:47:47,279 --> 00:47:50,400
it just makes sure that it's completely

00:47:48,690 --> 00:47:53,940
drained rather than doing like a kill an

00:47:50,400 --> 00:47:58,099
eye on the process so it gives another

00:47:53,940 --> 00:47:58,099
cast some time to start up to replace it

00:48:05,029 --> 00:48:11,819
we do both so we have a what we call

00:48:08,730 --> 00:48:15,119
infrastructure core repo which is kind

00:48:11,819 --> 00:48:18,599
of the extreme base level stuff but all

00:48:15,119 --> 00:48:20,279
the application level codes so caches

00:48:18,599 --> 00:48:23,759
databases those modules that I showed

00:48:20,279 --> 00:48:26,069
you live with each service so the teams

00:48:23,759 --> 00:48:27,720
can manage their own things so if we

00:48:26,069 --> 00:48:29,759
want to add a new feature to like a web

00:48:27,720 --> 00:48:32,609
service or a worker service we do it at

00:48:29,759 --> 00:48:35,069
the module level we maintain those

00:48:32,609 --> 00:48:36,420
modules in its own repository and then

00:48:35,069 --> 00:48:39,360
on the next deployment they'll get all

00:48:36,420 --> 00:48:48,270
the updates yeah

00:48:39,360 --> 00:48:52,380
ah yeah we use the kms service okay I

00:48:48,270 --> 00:48:55,320
actually have an example of this it's

00:48:52,380 --> 00:48:59,400
key management service and it works

00:48:55,320 --> 00:49:02,490
nicely with terraform so here there's

00:48:59,400 --> 00:49:05,790
actually a variable for where a secret

00:49:02,490 --> 00:49:08,940
will go so that the secret is actually

00:49:05,790 --> 00:49:10,380
not in the repo it's out on the kms

00:49:08,940 --> 00:49:12,060
service and it'll be updated when

00:49:10,380 --> 00:49:29,280
whenever we run the terraform apply

00:49:12,060 --> 00:49:31,830
it'll it'll pull the key in yes we we do

00:49:29,280 --> 00:49:34,110
do a lot of static code analysis on at

00:49:31,830 --> 00:49:37,140
Build time as part of the github review

00:49:34,110 --> 00:49:38,670
process so before it's actually reviewed

00:49:37,140 --> 00:49:42,750
by a human we go through a bunch of

00:49:38,670 --> 00:50:06,230
static checks and also we factor in code

00:49:42,750 --> 00:50:08,580
coverage and things like that actually

00:50:06,230 --> 00:50:10,740
the one that's probably the most active

00:50:08,580 --> 00:50:13,200
is the one I'm in which includes what's

00:50:10,740 --> 00:50:16,470
left of our monolith just because it's

00:50:13,200 --> 00:50:18,360
the it's the largest service and we're

00:50:16,470 --> 00:50:20,580
still it's probably gonna take us a few

00:50:18,360 --> 00:50:21,540
more years at least the finish breaking

00:50:20,580 --> 00:50:25,320
apart the monolith another

00:50:21,540 --> 00:50:27,720
micro-services yeah yeah we we're trying

00:50:25,320 --> 00:50:30,540
to make a really big effort to smooth

00:50:27,720 --> 00:50:36,000
out the number of pages across the

00:50:30,540 --> 00:50:44,220
groups yeah I think I do want one more

00:50:36,000 --> 00:51:01,560
okay now we all we all use it internally

00:50:44,220 --> 00:51:04,500
yeah no sorry quickly yeah I'm still all

00:51:01,560 --> 00:51:07,610
about ECS I would honestly look at Nomad

00:51:04,500 --> 00:51:07,610
before queue bananas

00:51:10,010 --> 00:51:17,660
yes it's not it's not quite there yet

00:51:12,720 --> 00:51:17,660
in my opinion ECS has never gone down

00:51:18,440 --> 00:51:47,990
and it's it's really nice and simple in

00:51:21,570 --> 00:51:47,990
comparison it just kind of works yeah

00:51:49,340 --> 00:51:56,490
yeah Oh answer as quickly as I can so if

00:51:54,330 --> 00:51:59,040
you if you work with containers a lot

00:51:56,490 --> 00:52:01,350
right now you'll find that you have like

00:51:59,040 --> 00:52:05,460
endless sidecars which drives me insane

00:52:01,350 --> 00:52:10,140
I think a lot of that will fold into

00:52:05,460 --> 00:52:14,070
these different schedulers I hope that's

00:52:10,140 --> 00:52:16,010
my prediction at least it all right I

00:52:14,070 --> 00:52:21,150
think that's it thank you

00:52:16,010 --> 00:52:21,150

YouTube URL: https://www.youtube.com/watch?v=1EEe-mxHrT0


