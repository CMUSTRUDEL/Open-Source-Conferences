Title: Michael W Lucas   Introducing ZFS at Ohio Linuxfest 2016
Publication date: 2016-10-24
Playlist: 2016 Videos
Description: 
	The famous BSD author came to Ohio Linuxfest 2016 to present a talk about ZFS. This filesystem is what should be held as the gold standard of enterprise grade filesystems should be. 

Table of Contents

0:00 - Clock says 3PM so let's get this started!
0:55 - What is ZFS?
2:40 - What makes ZFS special?
3:10 - Mr. Zack helps out.
7:35 - ZFS Hardware.
10:03 - ZFS Terminology
11:56 - Virtual Devices
13:17 - VDEV's and Pools
14:15 - Stripe VDEV/Pool
14:54 - Mirror VDEV/Pool
15:45 - RAIDZ VDEV/Pool
18:20 - RAIDZ Types
20:09 - RAIDZ Rule of 2s is Bogus
21:45 - How Many Disks and Pools
23:42 - RAIDZ vs Traditional RAID
24:29 - Create Striped Pools
     example: zpool create compost gpt/zfs0 gpt/zfs1 \ gpt/zfs2 gpt/zfs3 gpt/zfs4
25:30 - Making a Mirror is Very Very Similar.
     example: zpool create reflect mirror gpt/zfs0 gpt/zfs1
To view or verify the stripe or mirror use
                       zpool status
25:55 - Creating RAIDZ Pools
     example: zpool create bucket raidz1 gpt/zfs0 \ gpt/zfs1 gpt/zfs2
26:20 - Multi-VDEV RAIDZ
27:35 - Malformed Pools
29:10 - Reusing Providers
29:42 - Viewing Pools
     zpool list
30:10 - Pool Integrity
31:35 - Scrub vs fsck
32:45 - Pool Properties
33:40 - Changing Pool Properties
33:50 - Pool History
34:53 - Zpool Feature Flags and Distroying Pools
37:00 - Datasets
39:10 - Parent-Child Relationships
39:55 - Pool Repair and Maintenance
40:54 - Add VDEV to Pool
41:17 - Errors Through the ZFS Stack
41:41 - Caching, Compression, Deduplication, Snapshots and Clones
45:40 - Boot Environments
46:32 - ZFS Send and Receive
47:16 - Q&A

#ohiolinux
Check out his website over at https://www.michaelwlucas.com/
Captions: 
	00:00:00,030 --> 00:00:04,259
Paul said clop says three on one so

00:00:02,310 --> 00:00:08,840
let's get this started

00:00:04,259 --> 00:00:15,630
can you hear me in the back okay

00:00:08,840 --> 00:00:20,789
hi best talk at Ohio Linux fest my name

00:00:15,630 --> 00:00:23,490
is Michael Lucas I spent 20 years as a

00:00:20,789 --> 00:00:26,900
professional assistant men I've been on

00:00:23,490 --> 00:00:30,920
UNIX since some time in the late 1980s

00:00:26,900 --> 00:00:33,809
you could to be my age and they kind of

00:00:30,920 --> 00:00:38,700
mixed together whatever year that was

00:00:33,809 --> 00:00:40,649
maybe 589 something I'm finally

00:00:38,700 --> 00:00:43,800
remembering the southeast Michigan VSD

00:00:40,649 --> 00:00:48,510
user group I go to a lot of places to

00:00:43,800 --> 00:00:51,210
talk about VSD the Linux pulse were very

00:00:48,510 --> 00:00:53,670
welcoming though and other days I'm

00:00:51,210 --> 00:00:58,320
making my living writing technical books

00:00:53,670 --> 00:01:04,400
and I'm here because of the ZFS books I

00:00:58,320 --> 00:01:08,640
grow up with Alan Jude so what is ZFS

00:01:04,400 --> 00:01:10,830
Zia s-saw is in modern highly featured

00:01:08,640 --> 00:01:15,119
file system it has built-in data

00:01:10,830 --> 00:01:22,650
integrity checking comes with automatic

00:01:15,119 --> 00:01:27,829
snapshots ZFS was created by Sun once

00:01:22,650 --> 00:01:33,060
Oracle bought Sun CFS was forked into

00:01:27,829 --> 00:01:37,790
open ZFS these days the the Oracle UNIX

00:01:33,060 --> 00:01:40,590
/ the large Solaris version of CMS is

00:01:37,790 --> 00:01:43,890
less fully featured than the open source

00:01:40,590 --> 00:01:47,729
version the opens remain open source

00:01:43,890 --> 00:01:51,590
version is called open ZFS and you can

00:01:47,729 --> 00:01:56,689
find it mainly in opensolaris

00:01:51,590 --> 00:02:00,030
derivatives like the Lumos FreeBSD has

00:01:56,689 --> 00:02:02,570
perhaps the largest open source ZFS

00:02:00,030 --> 00:02:05,510
deployment and canonical reason

00:02:02,570 --> 00:02:11,570
we declared that CFS was their

00:02:05,510 --> 00:02:13,940
filesystem more containers have known

00:02:11,570 --> 00:02:15,470
canonical was going to do that I would

00:02:13,940 --> 00:02:19,400
have written quite ZFS books a little

00:02:15,470 --> 00:02:23,960
differently but that that's what the

00:02:19,400 --> 00:02:30,320
industry does to us Tech offers some ZFS

00:02:23,960 --> 00:02:32,750
features that I'll mention will see in

00:02:30,320 --> 00:02:34,310
proprietary raid controllers and some of

00:02:32,750 --> 00:02:37,880
these features have been added on

00:02:34,310 --> 00:02:42,230
elsewhere but in CFS there are full

00:02:37,880 --> 00:02:45,770
integrated whole and what makes ZFS

00:02:42,230 --> 00:02:48,530
special well there are a lot of good

00:02:45,770 --> 00:02:49,400
ideas that we'd like to see in file

00:02:48,530 --> 00:02:51,470
systems

00:02:49,400 --> 00:02:55,390
it's like check something and

00:02:51,470 --> 00:02:58,340
copy-on-write proper metadata management

00:02:55,390 --> 00:03:01,300
all of these very simple and well

00:02:58,340 --> 00:03:04,970
understood components can be combined

00:03:01,300 --> 00:03:09,190
into something extremely powerful and

00:03:04,970 --> 00:03:09,190
and to demonstrate how that works

00:03:09,519 --> 00:03:16,010
mr. Zedeck here has volunteered to help

00:03:12,380 --> 00:03:18,290
me um how are you today that I'm great

00:03:16,010 --> 00:03:23,380
uh just to be sure to have health

00:03:18,290 --> 00:03:28,010
insurance I do okay so a very simple

00:03:23,380 --> 00:03:30,950
simple things here can be combined and

00:03:28,010 --> 00:03:38,330
you can basically make the file system

00:03:30,950 --> 00:03:51,049
anything you want just very very simple

00:03:38,330 --> 00:03:58,190
thank you sir see how this basically

00:03:51,049 --> 00:04:01,959
turns boring computer stuff into a very

00:03:58,190 --> 00:04:05,239
very powerful way to get a lot

00:04:01,959 --> 00:04:09,920
however it requires changing some things

00:04:05,239 --> 00:04:17,989
in how you look at file systems the ZFS

00:04:09,920 --> 00:04:22,400
is not ext it is not you FS and it is

00:04:17,989 --> 00:04:24,850
not fat but the assumptions that you

00:04:22,400 --> 00:04:32,300
have when running those file systems

00:04:24,850 --> 00:04:38,229
will turn around and bite you hard so if

00:04:32,300 --> 00:04:44,229
I say s 51 K how many of you get that

00:04:38,229 --> 00:04:47,240
little bit of nausea and and feel unwell

00:04:44,229 --> 00:04:53,449
there was a generation of file systems

00:04:47,240 --> 00:04:54,919
before EXT before you FS and s 51 k is

00:04:53,449 --> 00:04:59,479
the only one of them I could remember

00:04:54,919 --> 00:05:02,530
the original UNIX file system are the

00:04:59,479 --> 00:05:06,710
UNIX file system that predated you FS

00:05:02,530 --> 00:05:08,530
and you of us today seems bare-bones

00:05:06,710 --> 00:05:11,090
stripped-down

00:05:08,530 --> 00:05:14,360
but at the time it came out people

00:05:11,090 --> 00:05:16,460
argued against it because it broke the

00:05:14,360 --> 00:05:19,610
rules of file systems and you have to

00:05:16,460 --> 00:05:21,139
have a built-in integrity check both

00:05:19,610 --> 00:05:24,320
dismisses this what do you need all

00:05:21,139 --> 00:05:28,280
these features were are you mad well

00:05:24,320 --> 00:05:35,030
today you hear a bunch of the same sort

00:05:28,280 --> 00:05:38,199
of thing around ZFS so if you have to

00:05:35,030 --> 00:05:42,169
readjust some of your fingers

00:05:38,199 --> 00:05:46,099
one key ZFS feature is tapi on writing

00:05:42,169 --> 00:05:48,260
in a traditional file system what

00:05:46,099 --> 00:05:50,930
happens when you want to change a file

00:05:48,260 --> 00:05:55,880
as the file system identifies the blocks

00:05:50,930 --> 00:05:58,970
on disk that have to have to be edited

00:05:55,880 --> 00:06:03,770
pick that lock up make the changes and

00:05:58,970 --> 00:06:06,169
write it back in the same spot if

00:06:03,770 --> 00:06:08,870
something goes wrong and your server

00:06:06,169 --> 00:06:12,969
goes belly-up when that block is half

00:06:08,870 --> 00:06:12,969
written you have data corruption

00:06:14,190 --> 00:06:21,760
this is commonly called the rape five

00:06:17,950 --> 00:06:25,330
right holes although it can happen with

00:06:21,760 --> 00:06:27,250
pretty much anything so if you've ever

00:06:25,330 --> 00:06:31,530
suffered from this problem and have a

00:06:27,250 --> 00:06:35,770
file turn out to be corrupt months or

00:06:31,530 --> 00:06:40,919
years after the failure and you find

00:06:35,770 --> 00:06:45,850
that it is so corrupted for so long that

00:06:40,919 --> 00:06:48,669
your backups have rotated away I've seen

00:06:45,850 --> 00:06:51,820
this happen more than once the nice

00:06:48,669 --> 00:06:54,160
thing about happy and money the file

00:06:51,820 --> 00:06:55,650
system identifies a block that needs to

00:06:54,160 --> 00:06:58,060
be changed

00:06:55,650 --> 00:07:01,930
copies it in the memory makes the

00:06:58,060 --> 00:07:08,020
changes and allocates a new sector on

00:07:01,930 --> 00:07:10,870
disk so if something happens yes you you

00:07:08,020 --> 00:07:14,169
lose what is not written to disk nobody

00:07:10,870 --> 00:07:16,539
can do anything about that yet but your

00:07:14,169 --> 00:07:20,820
old block is still there and the file is

00:07:16,539 --> 00:07:25,030
not corrupted so the data on the disk is

00:07:20,820 --> 00:07:27,820
always coherent an interesting side

00:07:25,030 --> 00:07:30,250
effect here is you get free file system

00:07:27,820 --> 00:07:33,520
snapshots it's just a matter of

00:07:30,250 --> 00:07:40,120
accounting what blocks are in use what

00:07:33,520 --> 00:07:44,320
were replaced and so on now ZFS expects

00:07:40,120 --> 00:07:48,220
access to the disk it has internal

00:07:44,320 --> 00:07:51,880
routines to check for disk errors great

00:07:48,220 --> 00:07:57,940
controllers conceal these errors that

00:07:51,880 --> 00:08:00,580
ZFS is looking for so if you're running

00:07:57,940 --> 00:08:03,490
is the FS on top of a RAID controller or

00:08:00,580 --> 00:08:07,120
on top of some sort of disk abstraction

00:08:03,490 --> 00:08:09,650
layer you will lose some of the into

00:08:07,120 --> 00:08:13,430
some not all but some of the intent

00:08:09,650 --> 00:08:16,449
each other you can lose a rate

00:08:13,430 --> 00:08:18,740
controller in just a bunch of discommode

00:08:16,449 --> 00:08:22,490
but make sure it's actually giving

00:08:18,740 --> 00:08:26,840
access to the disk and not creating a

00:08:22,490 --> 00:08:31,759
one disk raid zero and you know

00:08:26,840 --> 00:08:36,800
inserting itself in the middle there now

00:08:31,759 --> 00:08:40,130
people will talk about PCC memory ECC is

00:08:36,800 --> 00:08:42,070
a wonderful thing I highly encourage you

00:08:40,130 --> 00:08:45,740
to have it

00:08:42,070 --> 00:08:50,050
how many of you use ECC memory in your

00:08:45,740 --> 00:08:54,110
servers okay

00:08:50,050 --> 00:08:57,279
some of ZFS is error checking can

00:08:54,110 --> 00:09:05,600
duplicate what's in ECC Ram

00:08:57,279 --> 00:09:10,790
so ZFS according to ZFS developers the

00:09:05,600 --> 00:09:15,589
ZFS on a non-ecc system is no worse than

00:09:10,790 --> 00:09:19,310
a regular file system with BCC I would

00:09:15,589 --> 00:09:31,339
still encourage you to use ECC and ZFS

00:09:19,310 --> 00:09:35,390
together but it's it is viable now disk

00:09:31,339 --> 00:09:38,230
redundancy a lot of this error checking

00:09:35,390 --> 00:09:41,600
and such requires some sort of metadata

00:09:38,230 --> 00:09:43,720
some sort of parity data and depending

00:09:41,600 --> 00:09:47,620
on your physical disks you may have to

00:09:43,720 --> 00:09:51,140
adjust how you store data for example

00:09:47,620 --> 00:09:55,790
it's very hard on the one disk in your

00:09:51,140 --> 00:09:58,730
laptop to have a raid array or a ZFS

00:09:55,790 --> 00:10:01,400
raid array you know you just have to be

00:09:58,730 --> 00:10:06,770
aware of that and adjust your

00:10:01,400 --> 00:10:11,000
installation accordingly but let's talk

00:10:06,770 --> 00:10:14,560
about some ZFS terms the the big part of

00:10:11,000 --> 00:10:19,480
ZFS is the v depth the virtual device

00:10:14,560 --> 00:10:19,480
which is a group of storage providers

00:10:20,230 --> 00:10:29,380
disks you have a video of three

00:10:23,440 --> 00:10:33,100
five disks or our one disk a pool is a

00:10:29,380 --> 00:10:36,520
group of identical virtual devices if

00:10:33,100 --> 00:10:39,640
you build pay if you start off with a

00:10:36,520 --> 00:10:43,930
virtual device of five disks in a ZFS

00:10:39,640 --> 00:10:46,660
raid and and that is a storage pool for

00:10:43,930 --> 00:10:48,820
you you can act you can increase the

00:10:46,660 --> 00:10:53,440
size of that pool by adding a second

00:10:48,820 --> 00:10:55,690
group of five disks but the virtual

00:10:53,440 --> 00:11:01,570
devices are all identical with a nickel

00:10:55,690 --> 00:11:05,200
a data set is a named chunk of data on

00:11:01,570 --> 00:11:12,820
in a pool that can be something like

00:11:05,200 --> 00:11:15,040
slash bar or root or a lock device that

00:11:12,820 --> 00:11:18,670
you're handing off to a VMware machine

00:11:15,040 --> 00:11:21,370
for storage and you can arrange these

00:11:18,670 --> 00:11:25,210
data sets any way you want it is

00:11:21,370 --> 00:11:30,070
incredibly flexible one thing to

00:11:25,210 --> 00:11:34,690
remember on ZFS is the minus F flag to

00:11:30,070 --> 00:11:38,740
any command it's important - f means

00:11:34,690 --> 00:11:41,200
force this is the ZFS developers way of

00:11:38,740 --> 00:11:45,730
saying you are about to do something

00:11:41,200 --> 00:11:52,450
really stupid are you sure that you want

00:11:45,730 --> 00:11:55,030
to do this and there is a case where you

00:11:52,450 --> 00:12:01,630
want to do something that might be

00:11:55,030 --> 00:12:05,380
stupid okay virtual devices the basic

00:12:01,630 --> 00:12:07,570
unit of storage and ZFS all ZFS

00:12:05,380 --> 00:12:12,190
redundancy comes at the virtual device

00:12:07,570 --> 00:12:15,100
level so if your virtual devices are

00:12:12,190 --> 00:12:18,160
that group of five disks redundancy is

00:12:15,100 --> 00:12:21,700
within that group of five if your

00:12:18,160 --> 00:12:27,460
storage pool has 300 groups of five

00:12:21,700 --> 00:12:31,770
disks in it the redundancy you have 300

00:12:27,460 --> 00:12:31,770
separate internally redundant units

00:12:32,550 --> 00:12:42,730
and you can put a ZFS pool on top of

00:12:36,129 --> 00:12:46,889
anything that is a storage provider CFX

00:12:42,730 --> 00:12:50,410
CMS expects that it will be a raw disk

00:12:46,889 --> 00:12:55,720
however you can use a virtual machine

00:12:50,410 --> 00:12:58,569
disk image you can use a freebsd

00:12:55,720 --> 00:13:02,589
stackable storage device you can use an

00:12:58,569 --> 00:13:04,870
LLVM device all of these other things

00:13:02,589 --> 00:13:09,339
they are not wrong disks you'll want to

00:13:04,870 --> 00:13:11,769
make sure you add more error checking

00:13:09,339 --> 00:13:14,709
and I'm going to call what's at the

00:13:11,769 --> 00:13:18,629
bottom a disk even though it might not

00:13:14,709 --> 00:13:18,629
be because it really should

00:13:36,560 --> 00:13:42,600
virtual devices and pools a pool a

00:13:39,630 --> 00:13:44,250
storage pool of data contains only one

00:13:42,600 --> 00:13:47,850
type of Edith

00:13:44,250 --> 00:13:49,740
you'll hear the terms this type of Vida

00:13:47,850 --> 00:13:51,540
and this type of pool that used

00:13:49,740 --> 00:13:56,639
interchangeably even though they're

00:13:51,540 --> 00:14:00,269
really not you add virtual devices to

00:13:56,639 --> 00:14:04,490
pools if you have your five disk Vida

00:14:00,269 --> 00:14:08,190
you have multiples of that you don't add

00:14:04,490 --> 00:14:11,430
discs to Venus if you have a group of

00:14:08,190 --> 00:14:18,769
five disks as one virtual device you

00:14:11,430 --> 00:14:21,560
don't add a sixth to that device and

00:14:18,769 --> 00:14:25,940
there's a few different types of B dev

00:14:21,560 --> 00:14:29,279
slash pool the simplest is the stripe

00:14:25,940 --> 00:14:31,680
where each debtor each disk is its own

00:14:29,279 --> 00:14:36,350
virtual device and data is just straight

00:14:31,680 --> 00:14:39,750
across them much like a rate one this is

00:14:36,350 --> 00:14:42,779
great for scratched space it has no

00:14:39,750 --> 00:14:46,829
redundancy you lose one disk the whole

00:14:42,779 --> 00:14:50,959
thing goes away and for some computing

00:14:46,829 --> 00:14:50,959
applications this makes perfect sense

00:14:55,490 --> 00:15:04,019
this should also look fairly familiar

00:14:58,399 --> 00:15:09,329
the mirror multiple disks that copied

00:15:04,019 --> 00:15:11,640
each other and each this replicates the

00:15:09,329 --> 00:15:15,569
content of all the other disks in the

00:15:11,640 --> 00:15:21,930
pool or all the other disks in the V dev

00:15:15,569 --> 00:15:26,790
a pool that has multiple mirrors in it

00:15:21,930 --> 00:15:31,440
is a lot like raid 10 a strength of

00:15:26,790 --> 00:15:32,960
mirrors this is my favorite for

00:15:31,440 --> 00:15:35,450
high-performance

00:15:32,960 --> 00:15:38,330
it's also not terribly efficient with

00:15:35,450 --> 00:15:42,940
disk space because you're using half of

00:15:38,330 --> 00:15:42,940
your disk space for redundancy

00:15:45,760 --> 00:15:56,300
rixy is where things get a little fun

00:15:50,620 --> 00:15:58,640
raid Z is like they call it great so

00:15:56,300 --> 00:16:01,430
that you get an idea of sort of what it

00:15:58,640 --> 00:16:03,310
does and they add the Z to tell you this

00:16:01,430 --> 00:16:07,670
is a completely different thing that

00:16:03,310 --> 00:16:11,570
happens to be sort of like that so each

00:16:07,670 --> 00:16:14,589
video contains multiple disks the disk

00:16:11,570 --> 00:16:17,690
integrity is maintained via parity you

00:16:14,589 --> 00:16:21,850
can lose one or more discs depending on

00:16:17,690 --> 00:16:25,550
the type of raid Z and you lose no data

00:16:21,850 --> 00:16:29,480
rate C can self-heal be because of

00:16:25,550 --> 00:16:32,630
redundant checksums every block of data

00:16:29,480 --> 00:16:35,720
is check summed and the checksum is

00:16:32,630 --> 00:16:37,880
stored in the parent block all the way

00:16:35,720 --> 00:16:42,920
to up to the root of the of the storage

00:16:37,880 --> 00:16:46,640
tree so every time you access them

00:16:42,920 --> 00:16:49,820
Ziya us checks the checksum and if

00:16:46,640 --> 00:16:52,839
there's an error it goes and

00:16:49,820 --> 00:16:56,330
reconstructs that block from parity data

00:16:52,839 --> 00:16:59,510
so you never even know except you start

00:16:56,330 --> 00:17:03,010
to get error messages in the law that

00:16:59,510 --> 00:17:06,170
say this was a problem and I fixed it

00:17:03,010 --> 00:17:07,630
you do all read your blog messages every

00:17:06,170 --> 00:17:12,679
day don't you

00:17:07,630 --> 00:17:18,650
okay here you are all good decent people

00:17:12,679 --> 00:17:20,839
so I'm sure you do so one thing that's a

00:17:18,650 --> 00:17:25,130
common misconception is you cannot add

00:17:20,839 --> 00:17:26,980
discs to a raid Z or or any vida you add

00:17:25,130 --> 00:17:31,070
a new video

00:17:26,980 --> 00:17:32,690
I've more than once seen someone saying

00:17:31,070 --> 00:17:36,920
that they have trouble expanding their

00:17:32,690 --> 00:17:38,110
ZFS pool and I came in to find they have

00:17:36,920 --> 00:17:42,230
a 10 misc

00:17:38,110 --> 00:17:45,730
array which is fine and they started off

00:17:42,230 --> 00:17:49,190
with a six-disc V dev

00:17:45,730 --> 00:17:54,919
they can't add another unit of six discs

00:17:49,190 --> 00:17:56,720
there are only four spaces left at that

00:17:54,919 --> 00:17:58,700
point you might have to add a new disk

00:17:56,720 --> 00:18:02,659
arrays you can get those other two disks

00:17:58,700 --> 00:18:05,570
in or throw it out and start with five

00:18:02,659 --> 00:18:08,380
disk units but if you have a storage

00:18:05,570 --> 00:18:13,250
shelf and you want to use it for ZFS

00:18:08,380 --> 00:18:16,659
take the number of slots you have find a

00:18:13,250 --> 00:18:22,850
number it divides nicely by and use

00:18:16,659 --> 00:18:26,409
multiples of that for your storage and

00:18:22,850 --> 00:18:30,679
there are multiple raid-z types

00:18:26,409 --> 00:18:33,980
raid Z one requires three or more disks

00:18:30,679 --> 00:18:41,120
and you can lose one disk

00:18:33,980 --> 00:18:44,750
perviness ritzy to HIV dev must be four

00:18:41,120 --> 00:18:47,870
or more disks and even lose two disks of

00:18:44,750 --> 00:18:49,490
an event so half of your disks could

00:18:47,870 --> 00:18:54,889
have failed and your data will still be

00:18:49,490 --> 00:19:00,620
served at rates III three disks for B

00:18:54,889 --> 00:19:03,919
dev rate C three is increasingly

00:19:00,620 --> 00:19:07,370
necessary simply because the size of the

00:19:03,919 --> 00:19:12,080
disk is quickly outstripping our

00:19:07,370 --> 00:19:12,980
throughput to the disk six megabit a

00:19:12,080 --> 00:19:16,669
second

00:19:12,980 --> 00:19:20,960
sounds really fast and it was great for

00:19:16,669 --> 00:19:24,080
a two gig disk now we have ten gig disks

00:19:20,960 --> 00:19:30,049
and by the end of the week probably 30

00:19:24,080 --> 00:19:36,710
gig I don't know if they grow faster

00:19:30,049 --> 00:19:39,909
than I can watch so it is possible with

00:19:36,710 --> 00:19:44,720
both rate Z and some traditional raids

00:19:39,909 --> 00:19:48,289
that you can shuffle data for so long

00:19:44,720 --> 00:19:51,470
that as one disk fails are sorry as you

00:19:48,289 --> 00:19:53,770
are replacing one failed disk another

00:19:51,470 --> 00:19:57,660
disk can fail

00:19:53,770 --> 00:20:00,340
how many of you have had that happened

00:19:57,660 --> 00:20:04,000
it's it's look around it it's not

00:20:00,340 --> 00:20:11,890
uncommon it sounds ridiculous it is

00:20:04,000 --> 00:20:14,680
ridiculous but it's really sorry common

00:20:11,890 --> 00:20:17,350
cause often you have a batch of disks

00:20:14,680 --> 00:20:25,360
from the same manufacturer at the same

00:20:17,350 --> 00:20:28,270
time terrible things happened I have a

00:20:25,360 --> 00:20:31,150
lovely story from a manufacturer on how

00:20:28,270 --> 00:20:34,960
they traced a rash of disk failures down

00:20:31,150 --> 00:20:41,760
to one guy in China had a colon and

00:20:34,960 --> 00:20:44,200
everything he touched that day fate so

00:20:41,760 --> 00:20:46,180
there are people around here who run

00:20:44,200 --> 00:20:50,490
cash storage arrays to talk to them

00:20:46,180 --> 00:20:53,050
about hardware and how not to do it so

00:20:50,490 --> 00:20:55,450
if you've been around CFS you've

00:20:53,050 --> 00:20:58,480
probably heard this rule of tubes

00:20:55,450 --> 00:21:00,130
it is commonly repeated and I'm taking

00:20:58,480 --> 00:21:05,440
the time to mention it because it is

00:21:00,130 --> 00:21:08,640
bogus for most of us you will hear that

00:21:05,440 --> 00:21:12,550
a raid-z should have a number of disks

00:21:08,640 --> 00:21:17,170
equal to a multiple of 2 plus the parity

00:21:12,550 --> 00:21:22,270
level so now a raid Z should have 2n

00:21:17,170 --> 00:21:25,600
plus 3 disks in it this is true if and

00:21:22,270 --> 00:21:30,370
only if your data is consistently small

00:21:25,600 --> 00:21:33,460
box small blocks with a size that is a

00:21:30,370 --> 00:21:38,860
multiple of 2 and most of us do not have

00:21:33,460 --> 00:21:42,610
this data if you have a 10 discovered

00:21:38,860 --> 00:21:48,970
shelf and you want to use 5 disks use 5

00:21:42,610 --> 00:21:54,810
disks so all of this flexibility kind of

00:21:48,970 --> 00:21:57,810
makes you wonder well what should you do

00:21:54,810 --> 00:21:57,810
well

00:21:59,320 --> 00:22:06,140
the argument of how you should configure

00:22:02,060 --> 00:22:07,760
your storage has been going on since the

00:22:06,140 --> 00:22:13,010
beginning of storage and it will

00:22:07,760 --> 00:22:16,700
continue long after we're gone but here

00:22:13,010 --> 00:22:19,370
here are some general guidelines don't

00:22:16,700 --> 00:22:26,210
put more than nine to twelve disks in a

00:22:19,370 --> 00:22:29,270
single V - if you have an ace and shelf

00:22:26,210 --> 00:22:32,890
with a hundred and forty-four disk slots

00:22:29,270 --> 00:22:40,040
in it and you do it all this one giant

00:22:32,890 --> 00:22:42,080
grade z3 a three disks out of that fail

00:22:40,040 --> 00:22:43,820
and and suddenly you're out of

00:22:42,080 --> 00:22:45,650
redundancy and you better hope nothing

00:22:43,820 --> 00:22:50,260
else fails while you're trying to repair

00:22:45,650 --> 00:22:50,260
those disks so ah

00:22:50,380 --> 00:22:57,380
9 to 12 disks per v-dub is a good

00:22:53,540 --> 00:23:00,050
general purpose rule of thumb if you are

00:22:57,380 --> 00:23:01,910
special and you need to do it different

00:23:00,050 --> 00:23:05,540
and you need to have much larger v-dubs

00:23:01,910 --> 00:23:07,430
I will not argue with you I'd be

00:23:05,540 --> 00:23:11,410
interested in talking to you in a few

00:23:07,430 --> 00:23:11,410
years to see how that worked out for you

00:23:11,800 --> 00:23:17,330
some people put the whole OS in one

00:23:14,480 --> 00:23:19,850
monster pool I personally like to have

00:23:17,330 --> 00:23:23,570
the operating system in a two disc or

00:23:19,850 --> 00:23:26,900
three disk mirror and then have the data

00:23:23,570 --> 00:23:29,120
separate in its own pools I know people

00:23:26,900 --> 00:23:32,020
who are running huge databases and they

00:23:29,120 --> 00:23:35,030
will even have a pool purge table

00:23:32,020 --> 00:23:40,670
however they have entire rooms dedicated

00:23:35,030 --> 00:23:43,240
to storage so what works for your

00:23:40,670 --> 00:23:43,240
environment

00:23:44,700 --> 00:23:54,070
so what makes rate Z different from

00:23:50,440 --> 00:23:57,429
traditional rave CFS combines a volume

00:23:54,070 --> 00:24:00,610
manager and a file system this means

00:23:57,429 --> 00:24:03,190
when it's rebuilding a failed disk it

00:24:00,610 --> 00:24:06,610
only rebuilds the part that actually has

00:24:03,190 --> 00:24:07,559
data instead of some implementations

00:24:06,610 --> 00:24:11,200
that right

00:24:07,559 --> 00:24:12,970
you know the entire six terabytes to

00:24:11,200 --> 00:24:17,830
your six terabyte disk even though

00:24:12,970 --> 00:24:22,570
there's 500 mega not that I ever set

00:24:17,830 --> 00:24:26,289
through that screaming with rage captain

00:24:22,570 --> 00:24:30,429
right is another thing that very much

00:24:26,289 --> 00:24:34,419
makes raid-z different than raid so how

00:24:30,429 --> 00:24:37,360
do you actually do this the command

00:24:34,419 --> 00:24:40,090
lines show that they are havoc that they

00:24:37,360 --> 00:24:44,679
have a son heritage they get very long

00:24:40,090 --> 00:24:48,340
very quickly but let's see start by

00:24:44,679 --> 00:24:54,370
creating a pool this pool is called

00:24:48,340 --> 00:24:58,419
compost and it has these six one two

00:24:54,370 --> 00:25:01,330
five disk devices in it these examples

00:24:58,419 --> 00:25:04,720
aren't from FreeBSD you would wind up

00:25:01,330 --> 00:25:07,030
putting in whatever the operating system

00:25:04,720 --> 00:25:11,400
you're on what the device nodes are for

00:25:07,030 --> 00:25:16,390
your disk I use GPT labels on my disks

00:25:11,400 --> 00:25:22,450
because I'm easily confused so this has

00:25:16,390 --> 00:25:25,360
created a striped pull it's called

00:25:22,450 --> 00:25:31,110
compost that has these five entries and

00:25:25,360 --> 00:25:35,580
they're all there yeah what you'd be

00:25:31,110 --> 00:25:38,679
making the mirror is very very similar

00:25:35,580 --> 00:25:42,340
zpool create this mirror is called

00:25:38,679 --> 00:25:46,750
reflect its a mirror device and it has

00:25:42,340 --> 00:25:51,610
these two members and they the entry for

00:25:46,750 --> 00:25:52,480
this is very similar cooling the Deaf

00:25:51,610 --> 00:26:01,380
name

00:25:52,480 --> 00:26:05,290
and then members of the be dev rate C is

00:26:01,380 --> 00:26:12,690
boringly similar who'll create name type

00:26:05,290 --> 00:26:15,669
of rate Z members and it ZFS is

00:26:12,690 --> 00:26:18,040
meticulously consistent in how it

00:26:15,669 --> 00:26:21,809
displays information which of your

00:26:18,040 --> 00:26:32,490
writing scripts and such is really nice

00:26:21,809 --> 00:26:36,130
so there's no standard for raid / raid

00:26:32,490 --> 00:26:39,090
in an arbitrary way some proprietary

00:26:36,130 --> 00:26:44,020
raid controllers let you do that but

00:26:39,090 --> 00:26:47,830
here I'm creating a pool it's called

00:26:44,020 --> 00:26:50,710
that it's raid z1 and it has these three

00:26:47,830 --> 00:26:55,440
members and here is the start of the

00:26:50,710 --> 00:26:55,440
second v10 with these three members

00:26:58,080 --> 00:27:01,080
sorry

00:27:04,049 --> 00:27:14,139
well this is the main this is the type

00:27:08,950 --> 00:27:23,350
of Vida and yes any pool only has one

00:27:14,139 --> 00:27:26,609
type of virtual device in it and here we

00:27:23,350 --> 00:27:29,619
see where the Vita game comes in handy

00:27:26,609 --> 00:27:36,820
here's the beat out with these members

00:27:29,619 --> 00:27:40,830
other V deaf with those members so this

00:27:36,820 --> 00:27:45,220
happens a lot this is the most common

00:27:40,830 --> 00:27:52,059
error that I see with ZFS it's some

00:27:45,220 --> 00:27:55,629
variant on this we've created a pool it

00:27:52,059 --> 00:28:00,849
has a red ZB depth and it has a mirrored

00:27:55,629 --> 00:28:02,830
V depth and not that I've been really

00:28:00,849 --> 00:28:08,349
tired late at night trying to get the

00:28:02,830 --> 00:28:12,849
install done and had this happen and CMS

00:28:08,349 --> 00:28:17,200
will ask you it will tell you that this

00:28:12,849 --> 00:28:23,499
is invalid use - f if you really mean

00:28:17,200 --> 00:28:28,299
this once you do this there is no going

00:28:23,499 --> 00:28:32,109
that you have to throw the pool up and

00:28:28,299 --> 00:28:36,489
start over once a V dot goes in a pool

00:28:32,109 --> 00:28:40,899
it's there it does not come out open CFS

00:28:36,489 --> 00:28:44,859
is working on that but the process of

00:28:40,899 --> 00:28:47,830
emptying the data from a pool moving it

00:28:44,859 --> 00:28:53,889
to this that will remain and freeing up

00:28:47,830 --> 00:28:56,379
that V tub for removal is not let's just

00:28:53,889 --> 00:29:03,029
say it's not as straightforward as we

00:28:56,379 --> 00:29:03,029
might hold so listen to the - f

00:29:05,440 --> 00:29:14,510
one time you will want to use - f is

00:29:10,700 --> 00:29:18,290
when you're reusing a disk I had a bunch

00:29:14,510 --> 00:29:22,850
of disks in a pool I have my disk my

00:29:18,290 --> 00:29:25,130
desktop has 10 hard drives in it because

00:29:22,850 --> 00:29:30,670
I was writing a book on ZFS and wanted

00:29:25,130 --> 00:29:33,860
enough storage to do stupid things and I

00:29:30,670 --> 00:29:36,290
would have to use - F to tell it yes I

00:29:33,860 --> 00:29:38,660
really mean to use this disk that is

00:29:36,290 --> 00:29:44,750
marked as part of this pool because I'm

00:29:38,660 --> 00:29:47,780
done with that other pool so you can

00:29:44,750 --> 00:29:52,670
look at a pool with zpool list shows all

00:29:47,780 --> 00:29:55,580
of the pools here I have this Z root

00:29:52,670 --> 00:30:00,320
pool is my operating system the DB pool

00:29:55,580 --> 00:30:03,140
is my database and we can get some

00:30:00,320 --> 00:30:06,110
general statistics from it see pool the

00:30:03,140 --> 00:30:12,200
list - ve lists all of the individual

00:30:06,110 --> 00:30:15,850
providers in the pool let's talk about

00:30:12,200 --> 00:30:15,850
CFS integrity

00:30:18,570 --> 00:30:28,440
you're 30 okay I need to talk a little

00:30:22,650 --> 00:30:30,890
more quickly ZFS uses checksums

00:30:28,440 --> 00:30:33,390
and parity data to heal itself

00:30:30,890 --> 00:30:35,100
everything is hashed things that you

00:30:33,390 --> 00:30:37,740
think shouldn't be hashed are hash

00:30:35,100 --> 00:30:43,470
because someone discovered that Oh crack

00:30:37,740 --> 00:30:46,110
we should have hacked them CFS has this

00:30:43,470 --> 00:30:48,810
process called the scrub which has

00:30:46,110 --> 00:30:54,360
walked the data tree and check every

00:30:48,810 --> 00:30:57,680
jetsam if you have no redundancy in a V

00:30:54,360 --> 00:31:00,870
dev if you're running CMS on your laptop

00:30:57,680 --> 00:31:03,830
you can use the pull was already the

00:31:00,870 --> 00:31:06,750
data sent copies property to let you

00:31:03,830 --> 00:31:11,640
duplicate data checksum everything and

00:31:06,750 --> 00:31:14,130
have a good version to restore from yes

00:31:11,640 --> 00:31:17,970
that cuts your storage and a half if you

00:31:14,130 --> 00:31:20,790
keep two copies of everything if you

00:31:17,970 --> 00:31:24,060
have a spinning hard disk you probably

00:31:20,790 --> 00:31:30,180
have like what 15 terabytes of storage

00:31:24,060 --> 00:31:35,360
in your laptop whatever it is now you

00:31:30,180 --> 00:31:39,390
can I found I could spend in space now

00:31:35,360 --> 00:31:43,790
how to strum compared to F sick how many

00:31:39,390 --> 00:31:43,790
of us know when glove and low left sick

00:31:43,850 --> 00:31:52,530
how many of us have had to play with

00:31:46,760 --> 00:31:56,930
FSDB and CL RI you will play with them

00:31:52,530 --> 00:31:56,930
you will adore F sick

00:31:57,200 --> 00:32:05,910
so one common criticism of CFS is that

00:32:01,830 --> 00:32:07,310
there's no offline integrity check well

00:32:05,910 --> 00:32:10,800
that's true

00:32:07,310 --> 00:32:14,640
Isaiah has struct us everything else it

00:32:10,800 --> 00:32:18,510
does and more you can choose to take

00:32:14,640 --> 00:32:20,850
your pool offline to scrub it it won't

00:32:18,510 --> 00:32:24,180
really give you any advantages other

00:32:20,850 --> 00:32:25,830
than maybe disk IO if there's a you know

00:32:24,180 --> 00:32:29,240
it just as if you were shutting down the

00:32:25,830 --> 00:32:29,240
app but yeah

00:32:29,360 --> 00:32:34,980
many of the complaints about strummed

00:32:31,830 --> 00:32:36,960
not checking this or checking that there

00:32:34,980 --> 00:32:40,140
are some valid things with scrub that

00:32:36,960 --> 00:32:43,200
they are still developing however the

00:32:40,140 --> 00:32:49,620
same complaints and many more apply to F

00:32:43,200 --> 00:32:55,080
sync now CMS pools have properties these

00:32:49,620 --> 00:33:00,300
are tunable will use Z pool network ZFS

00:32:55,080 --> 00:33:02,520
get and set to view and change them some

00:33:00,300 --> 00:33:05,400
are read-only some are calculated by the

00:33:02,520 --> 00:33:09,690
pool characteristic time for example

00:33:05,400 --> 00:33:10,710
here's the property on a pool on the

00:33:09,690 --> 00:33:15,360
zebra pool

00:33:10,710 --> 00:33:17,730
the size is 920 gig you cannot increase

00:33:15,360 --> 00:33:22,790
the size of your disk by changing the

00:33:17,730 --> 00:33:24,990
tunable on the other hand character

00:33:22,790 --> 00:33:29,160
characteristics and properties like food

00:33:24,990 --> 00:33:31,370
FS what data sent do we point out for

00:33:29,160 --> 00:33:33,750
the root filesystem you can change that

00:33:31,370 --> 00:33:36,810
which means you can have multiple routes

00:33:33,750 --> 00:33:43,830
on the same disk and move to a different

00:33:36,810 --> 00:33:47,580
one so you can set commentary on of your

00:33:43,830 --> 00:33:53,370
pool you can set properties what have

00:33:47,580 --> 00:33:57,150
you with set command last nifty feature

00:33:53,370 --> 00:34:00,630
on cool pools retain a history of

00:33:57,150 --> 00:34:01,690
everything that was done to them every

00:34:00,630 --> 00:34:08,079
z4

00:34:01,690 --> 00:34:11,379
and that changes the pool so when you're

00:34:08,079 --> 00:34:17,099
the lead tech and you come in in the

00:34:11,379 --> 00:34:20,649
morning and the pool is destroyed and

00:34:17,099 --> 00:34:25,329
you're your junior sis admin says I

00:34:20,649 --> 00:34:28,599
don't know what happened you'd go if you

00:34:25,329 --> 00:34:32,639
import the pool you look at the history

00:34:28,599 --> 00:34:36,510
and it says oh what is this Z pool

00:34:32,639 --> 00:34:41,379
destroy command that was run you know at

00:34:36,510 --> 00:34:42,040
7:53 a.m. when you've been here for 20

00:34:41,379 --> 00:34:45,060
minutes

00:34:42,040 --> 00:34:45,060
how did that happen

00:34:45,250 --> 00:34:53,700
there are times I would have committed

00:34:48,909 --> 00:34:58,180
murder for this feature on you FS or ext

00:34:53,700 --> 00:35:04,240
speaking of destroying the pole the pole

00:34:58,180 --> 00:35:07,950
destroy now CFS evolves in particular

00:35:04,240 --> 00:35:11,819
open CFS evolves quickly and their

00:35:07,950 --> 00:35:16,540
development is very widely distributed

00:35:11,819 --> 00:35:19,720
so original son Z of us had version

00:35:16,540 --> 00:35:21,730
numbers at version 26 you grew this

00:35:19,720 --> 00:35:24,460
feature at version 28 you proved that

00:35:21,730 --> 00:35:29,589
future um

00:35:24,460 --> 00:35:34,619
the final Oracle ZFS version sorry the

00:35:29,589 --> 00:35:37,480
final son CFS version was version 28

00:35:34,619 --> 00:35:42,220
Oracle isn't doing much for ZFS

00:35:37,480 --> 00:35:46,630
development now so open ZFS has set the

00:35:42,220 --> 00:35:48,099
version number to 5000 giving Oracle

00:35:46,630 --> 00:35:49,990
room to do some of their own

00:35:48,099 --> 00:35:57,220
improvements and then they've added this

00:35:49,990 --> 00:35:58,690
thing called feature flags where you get

00:35:57,220 --> 00:36:02,050
all the properties from your pool and

00:35:58,690 --> 00:36:04,780
rep for feature feature names all start

00:36:02,050 --> 00:36:09,270
the feature and then whenever the

00:36:04,780 --> 00:36:09,270
feature is LZ for compression

00:36:09,329 --> 00:36:19,859
transaction groups etc and so if you are

00:36:15,319 --> 00:36:23,430
I'm a I'm a BST gun if I give you a

00:36:19,859 --> 00:36:27,959
bunch of disks from my BST storage array

00:36:23,430 --> 00:36:30,989
on here put these in the Linux box you

00:36:27,959 --> 00:36:33,150
can actually have your CFS on Linux

00:36:30,989 --> 00:36:36,660
installed check for feature flags and

00:36:33,150 --> 00:36:40,709
see if there's anything that these polls

00:36:36,660 --> 00:36:42,719
have enabled that your ZFS can't handle

00:36:40,709 --> 00:36:45,809
or vice versa

00:36:42,719 --> 00:36:47,640
you can pick up ZFS disks and move them

00:36:45,809 --> 00:36:51,779
to son offs sorry

00:36:47,640 --> 00:36:56,009
move them to Solaris son us would be a

00:36:51,779 --> 00:36:57,390
good trick you can move them to lettuce

00:36:56,009 --> 00:37:05,249
you can move them to anything that

00:36:57,390 --> 00:37:08,130
speaks ZFS now a data center lets go

00:37:05,249 --> 00:37:11,459
from the great big storage to the

00:37:08,130 --> 00:37:14,309
smaller unit a data set is a main chunk

00:37:11,459 --> 00:37:17,849
of opinion it's a filesystem it's a

00:37:14,309 --> 00:37:19,799
volume which is a blob of data that you

00:37:17,849 --> 00:37:23,069
use as a storage back-end it's a block

00:37:19,799 --> 00:37:27,959
device it's a snapshot it's a clone it's

00:37:23,069 --> 00:37:31,769
a bookmark the rule with data sets is

00:37:27,959 --> 00:37:33,930
using lots of all of those properties

00:37:31,769 --> 00:37:37,859
almost suitable you can set on a per

00:37:33,930 --> 00:37:41,969
dataset basis so here's the beginning of

00:37:37,859 --> 00:37:45,539
the plot of a list of ZFS data sets from

00:37:41,969 --> 00:37:48,869
one of my three BSD machines I have five

00:37:45,539 --> 00:37:50,999
different root partitions user is one

00:37:48,869 --> 00:37:55,069
user home as another user ports as

00:37:50,999 --> 00:38:01,199
another attempt is another bar var log

00:37:55,069 --> 00:38:03,269
bar DB everything is a data set you can

00:38:01,199 --> 00:38:07,949
also look at a smaller portion of the

00:38:03,269 --> 00:38:13,499
data set tree creating data sets is very

00:38:07,949 --> 00:38:15,370
simple to add a new one ZFS create give

00:38:13,499 --> 00:38:18,880
the the pool Lane and

00:38:15,370 --> 00:38:27,480
after the data set I've created bar

00:38:18,880 --> 00:38:27,480
MySQL the - V lets you create a volume

00:38:27,810 --> 00:38:35,260
you can move data sense is the MS rename

00:38:31,800 --> 00:38:36,910
you can destroy data sets and you

00:38:35,260 --> 00:38:41,050
destroy data sets there are two

00:38:36,910 --> 00:38:44,770
important ZFS flags that are consistent

00:38:41,050 --> 00:38:48,460
across the command - V is verbose mode -

00:38:44,770 --> 00:38:51,520
n is the know Bob flag combined they

00:38:48,460 --> 00:38:56,260
they say tell me what you would do if I

00:38:51,520 --> 00:38:57,910
took these flags away and that lets you

00:38:56,260 --> 00:39:05,490
make sure that you didn't accidentally

00:38:57,910 --> 00:39:08,530
blow away your user or your route and

00:39:05,490 --> 00:39:12,820
data sets can have properties just like

00:39:08,530 --> 00:39:18,610
the pools those properties aren't

00:39:12,820 --> 00:39:20,950
inheritable you can set a property on

00:39:18,610 --> 00:39:26,440
the route data set and it will propagate

00:39:20,950 --> 00:39:28,870
down to everything beneath it to mount

00:39:26,440 --> 00:39:31,540
and unmount data sets you use ZFS

00:39:28,870 --> 00:39:34,030
mountains in FSC unmount there is a

00:39:31,540 --> 00:39:36,660
mount point property that lets you

00:39:34,030 --> 00:39:40,810
change where something gets mounted I

00:39:36,660 --> 00:39:46,480
have my old MySQL database I want to

00:39:40,810 --> 00:39:48,880
mount it on slash note now some things

00:39:46,480 --> 00:39:54,280
you have to do to take care of pools

00:39:48,880 --> 00:39:59,890
because 41 they need to speak faster

00:39:54,280 --> 00:40:05,080
sorry Oh resilvered is when you rebuild

00:39:59,890 --> 00:40:07,750
from parity resilvered is throttled by

00:40:05,080 --> 00:40:11,290
your disk i/o as you would expect if you

00:40:07,750 --> 00:40:14,680
have no redundancy if you have no parity

00:40:11,290 --> 00:40:16,750
there is no Reese over ZFS will tell you

00:40:14,680 --> 00:40:20,230
these check sums are bad this file is

00:40:16,750 --> 00:40:23,580
bogus sorry but at least you know the

00:40:20,230 --> 00:40:23,580
file is broken

00:40:23,990 --> 00:40:31,500
so and if you have to replace a disk

00:40:28,280 --> 00:40:33,780
remember not all disks are the second

00:40:31,500 --> 00:40:38,910
not all disks with the same label on the

00:40:33,780 --> 00:40:42,270
box are the same size you have that two

00:40:38,910 --> 00:40:48,089
terabyte disk but the new disk is three

00:40:42,270 --> 00:40:50,460
sectors smaller than the old disk you

00:40:48,089 --> 00:40:55,730
have a problem that replacement disks

00:40:50,460 --> 00:40:59,369
have to be the same size or larger and

00:40:55,730 --> 00:41:02,880
adding a Vita to a pool looks very much

00:40:59,369 --> 00:41:06,359
like creating the pool in the first

00:41:02,880 --> 00:41:10,680
place it's just Z pole ad instead of Z

00:41:06,359 --> 00:41:13,050
coil created see how this has a bunch of

00:41:10,680 --> 00:41:15,030
hardware states and I'm not going to go

00:41:13,050 --> 00:41:18,839
through them but they tell you pretty

00:41:15,030 --> 00:41:23,810
specifically what went wrong and you can

00:41:18,839 --> 00:41:26,640
see these errors propagate up the stack

00:41:23,810 --> 00:41:31,099
here's this pool it's in a state of

00:41:26,640 --> 00:41:34,380
degraded which if you look down these

00:41:31,099 --> 00:41:37,710
these this devices don't look right it

00:41:34,380 --> 00:41:41,040
looks almost like some jerks yanked hard

00:41:37,710 --> 00:41:50,849
disks out of his PC just to generate an

00:41:41,040 --> 00:41:54,000
error s also has caches new types of

00:41:50,849 --> 00:41:57,329
caches do you have a write intensive

00:41:54,000 --> 00:42:00,619
pool maybe you need a really fast disk

00:41:57,329 --> 00:42:04,460
to cache your right zone so that you can

00:42:00,619 --> 00:42:08,849
spoil them off to a lower speed storage

00:42:04,460 --> 00:42:10,950
maybe you have a really breed intensive

00:42:08,849 --> 00:42:15,200
disk and you want a really fast small

00:42:10,950 --> 00:42:20,030
disk to cache the most heavily read data

00:42:15,200 --> 00:42:25,400
what you find your bottleneck and use it

00:42:20,030 --> 00:42:31,020
or and treat it compression compression

00:42:25,400 --> 00:42:33,810
exchanges CPU time for disk i/o on most

00:42:31,020 --> 00:42:35,240
of our systems disk i/o it is the

00:42:33,810 --> 00:42:39,080
limiting factor but

00:42:35,240 --> 00:42:42,890
gobs of CPU some of you probably have

00:42:39,080 --> 00:42:46,130
poor laptops right now that are mostly

00:42:42,890 --> 00:42:51,440
item you put those two were compressing

00:42:46,130 --> 00:42:55,340
the data as it goes to disk the default

00:42:51,440 --> 00:42:59,060
compression algorithm is lz4 for certain

00:42:55,340 --> 00:43:04,010
types of data look at using gzip nine I

00:42:59,060 --> 00:43:08,380
have a system with 15 years of telecom

00:43:04,010 --> 00:43:14,480
call records that were plain text and

00:43:08,380 --> 00:43:18,680
gzip 9 was much better CMS also does

00:43:14,480 --> 00:43:21,590
deduplication um I'm not going to go

00:43:18,680 --> 00:43:24,680
into details again because there are

00:43:21,590 --> 00:43:27,200
more interesting things but you probably

00:43:24,680 --> 00:43:29,060
don't need it and you probably don't

00:43:27,200 --> 00:43:31,220
want it there are some people that works

00:43:29,060 --> 00:43:39,410
for I have never actually logged into

00:43:31,220 --> 00:43:42,140
one of those systems snapshots CFS keeps

00:43:39,410 --> 00:43:44,840
a record of blocks that used to be in

00:43:42,140 --> 00:43:46,670
use but that you still want so you can

00:43:44,840 --> 00:43:51,770
snapshot the filesystem

00:43:46,670 --> 00:43:54,890
right now and as you write and make

00:43:51,770 --> 00:43:57,800
changes in those internal accounting to

00:43:54,890 --> 00:43:59,690
say at the time of a snapshot this block

00:43:57,800 --> 00:44:03,920
was the one in use not that one over

00:43:59,690 --> 00:44:07,850
here and so through simple accounting it

00:44:03,920 --> 00:44:12,220
tracks what the file system state was at

00:44:07,850 --> 00:44:15,290
this time you can go into that you can

00:44:12,220 --> 00:44:18,560
roll the file system back to the

00:44:15,290 --> 00:44:25,160
snapshot say oh this upgrade was a bad

00:44:18,560 --> 00:44:27,050
idea let's retreat I've had nights where

00:44:25,160 --> 00:44:32,000
again I would have committed murder for

00:44:27,050 --> 00:44:34,220
that ability so the problem with

00:44:32,000 --> 00:44:37,520
snapshots your disk is running out of

00:44:34,220 --> 00:44:41,020
space you go in your home directory and

00:44:37,520 --> 00:44:45,230
you delete all those downloaded is OS

00:44:41,020 --> 00:44:46,460
they're still in use on the snapshots so

00:44:45,230 --> 00:44:51,680
it doesn't actually free

00:44:46,460 --> 00:44:56,390
space so sectors are only free from disk

00:44:51,680 --> 00:45:00,080
once no snapshots are using them and you

00:44:56,390 --> 00:45:02,109
can roll back the snapshot the Norfolk

00:45:00,080 --> 00:45:06,680
data will be destroyed

00:45:02,109 --> 00:45:09,950
you can also clone snapshots which is a

00:45:06,680 --> 00:45:13,599
way of saying I want to rewrite copy of

00:45:09,950 --> 00:45:17,119
this snapshot you snapshot your database

00:45:13,599 --> 00:45:20,869
and create a clone of the snapshot you

00:45:17,119 --> 00:45:23,089
run the upgrade on the clone the only

00:45:20,869 --> 00:45:27,020
disk space it uses is the difference

00:45:23,089 --> 00:45:29,900
between the new and old data so you know

00:45:27,020 --> 00:45:31,820
if you have massive data savings and you

00:45:29,900 --> 00:45:38,300
get to test and see if the application

00:45:31,820 --> 00:45:42,470
works after an upgrade and when you're

00:45:38,300 --> 00:45:44,839
done with you throw it away food

00:45:42,470 --> 00:45:47,839
environments are built on clothes and

00:45:44,839 --> 00:45:51,070
snapshots your snapshot the root file

00:45:47,839 --> 00:45:54,830
system before the upgrade you comb it

00:45:51,070 --> 00:45:59,690
you upgrade the clone you boot the clone

00:45:54,830 --> 00:46:05,619
if it fails you boot the original

00:45:59,690 --> 00:46:08,869
snapshot and this takes a huge amount of

00:46:05,619 --> 00:46:12,070
upgrades can still go bad but the

00:46:08,869 --> 00:46:16,190
recovery from the upgrade is so much

00:46:12,070 --> 00:46:19,400
easier I forget how many systems I have

00:46:16,190 --> 00:46:22,760
restored from tape because some

00:46:19,400 --> 00:46:27,160
commercial UNIX vendor that showed that

00:46:22,760 --> 00:46:27,160
shall remain nameless AIX

00:46:27,910 --> 00:46:35,720
did something horrible in the upgrade

00:46:30,290 --> 00:46:38,420
procedure CFS send and receive remember

00:46:35,720 --> 00:46:41,900
the ZFS tracks the blocks that are in

00:46:38,420 --> 00:46:45,410
use you can pick up the filesystem and

00:46:41,900 --> 00:46:47,780
send the whole thing to another host you

00:46:45,410 --> 00:46:51,500
can pick up the difference between the

00:46:47,780 --> 00:46:54,000
live filesystem and the last snapshot

00:46:51,500 --> 00:46:58,050
and send only those

00:46:54,000 --> 00:47:02,130
locks to the other post if you have

00:46:58,050 --> 00:47:05,790
large data replication means this beats

00:47:02,130 --> 00:47:10,740
arcing so badly that our sings mom needs

00:47:05,790 --> 00:47:14,930
urgent medical attention definitely look

00:47:10,740 --> 00:47:14,930
at CMS Senate received for replication

00:47:20,450 --> 00:47:29,640
okay 348 and have a couple minutes

00:47:25,310 --> 00:47:32,700
questions please ask them with a couple

00:47:29,640 --> 00:47:36,260
caveats don't ask me about the legality

00:47:32,700 --> 00:47:41,730
of cv d LM GPL I am NOT a lawyer and

00:47:36,260 --> 00:47:43,860
I've given up explaining Oracle 90s come

00:47:41,730 --> 00:47:47,460
talk to me about any number of random

00:47:43,860 --> 00:47:49,860
topics if you're interested my my on

00:47:47,460 --> 00:47:51,770
election night I'm giving a talk on cam

00:47:49,860 --> 00:47:56,280
is unamerican

00:47:51,770 --> 00:48:00,210
and I have a certain legality I need to

00:47:56,280 --> 00:48:04,700
comply with here so do you dear IRS bug

00:48:00,210 --> 00:48:07,110
off I'm deducting this whole trip so and

00:48:04,700 --> 00:48:08,910
if you want I'll have to works outside

00:48:07,110 --> 00:48:11,670
in the hall and I'm also giving away

00:48:08,910 --> 00:48:13,860
nifty little puzzle pens with two balls

00:48:11,670 --> 00:48:18,750
in them so you have something to do

00:48:13,860 --> 00:48:25,170
during that kind of all time so

00:48:18,750 --> 00:48:26,400
questions you let me throw you over but

00:48:25,170 --> 00:48:29,130
you never get to go first

00:48:26,400 --> 00:48:32,100
is there a helper utility you prefer to

00:48:29,130 --> 00:48:36,090
use to manage your snapshots or

00:48:32,100 --> 00:48:39,920
different general ezx firm is what I use

00:48:36,090 --> 00:48:46,180
to help with replication of snapshots if

00:48:39,920 --> 00:48:48,700
just local snapshots scripts yes sir

00:48:46,180 --> 00:48:59,710
so like your earlier slides you were

00:48:48,700 --> 00:49:02,410
talking about the strength yes but a

00:48:59,710 --> 00:49:15,250
pool that is purely striped has no

00:49:02,410 --> 00:49:18,069
redundancy yes that gives you an

00:49:15,250 --> 00:49:21,069
additional copy of the data it writes

00:49:18,069 --> 00:49:24,010
the data twice so that if it discovers a

00:49:21,069 --> 00:49:33,819
checksum error it can restore the data

00:49:24,010 --> 00:49:34,510
from a good backup yes if well sorry yes

00:49:33,819 --> 00:49:36,220
it does

00:49:34,510 --> 00:49:38,230
distribute among different PDFs if

00:49:36,220 --> 00:49:41,230
you're using something like disk images

00:49:38,230 --> 00:49:44,440
though that major book they may turn out

00:49:41,230 --> 00:49:46,180
to be on the same underlying disk so ZFS

00:49:44,440 --> 00:49:52,660
does its best to distribute data

00:49:46,180 --> 00:50:00,460
intelligently if you trick ZFS and it's

00:49:52,660 --> 00:50:02,829
pretty much sucks to be you then I would

00:50:00,460 --> 00:50:05,529
do raid z1 or mirroring for most

00:50:02,829 --> 00:50:08,559
operations because a striped pool if you

00:50:05,529 --> 00:50:13,869
lose one disk the pool is destroyed you

00:50:08,559 --> 00:50:16,450
can't recover from that it's what John

00:50:13,869 --> 00:50:18,700
cop is equal to human will recover from

00:50:16,450 --> 00:50:21,010
the straight cosmic ray or electric

00:50:18,700 --> 00:50:21,520
spark that flipped a couple bits on the

00:50:21,010 --> 00:50:23,920
disk

00:50:21,520 --> 00:50:32,670
you won't recover from the disk

00:50:23,920 --> 00:50:32,670
releasing the magic smoke sir

00:50:36,869 --> 00:50:42,450
ZFS is working on internal encryption

00:50:45,630 --> 00:50:54,190
people do things like loop if s it will

00:50:50,289 --> 00:50:58,349
obscure some errors however it will work

00:50:54,190 --> 00:51:04,869
better than anything else

00:50:58,349 --> 00:51:08,739
Luke offense I'm not familiar with it so

00:51:04,869 --> 00:51:13,269
I'm going to abstain from answer people

00:51:08,739 --> 00:51:15,489
do put encryption layers under ZFS in

00:51:13,269 --> 00:51:17,819
the back you had a question or you just

00:51:15,489 --> 00:51:17,819
stretch

00:51:43,010 --> 00:51:51,360
um yes it would be suited for that um I

00:51:48,000 --> 00:51:54,120
would not put discs in different

00:51:51,360 --> 00:51:57,420
buildings on the same Vida or in the

00:51:54,120 --> 00:52:01,650
same hole I do know people who replicate

00:51:57,420 --> 00:52:04,980
their data every 60 seconds across the

00:52:01,650 --> 00:52:09,660
web and depending on how they do the

00:52:04,980 --> 00:52:12,330
data that may work well anything with

00:52:09,660 --> 00:52:16,140
replication and distribution gets very

00:52:12,330 --> 00:52:20,010
complicated and very individualized very

00:52:16,140 --> 00:52:34,280
quickly but people do replicate every 60

00:52:20,010 --> 00:52:36,990
seconds and get away with it yes sir now

00:52:34,280 --> 00:52:39,900
there are things like butter requests

00:52:36,990 --> 00:52:43,080
recently had the raid 5 raid 6 blog of

00:52:39,900 --> 00:52:45,390
data corruption and it would be really

00:52:43,080 --> 00:52:50,900
easy for me to go and either neener

00:52:45,390 --> 00:52:52,230
there's the FSB but it can happen to

00:52:50,900 --> 00:52:54,660
anymore

00:52:52,230 --> 00:53:00,450
I don't care what your favorite

00:52:54,660 --> 00:53:02,610
operating system is at some time there

00:53:00,450 --> 00:53:05,040
has been some error that has made you go

00:53:02,610 --> 00:53:09,930
oh my god how did that happen I am

00:53:05,040 --> 00:53:13,470
ashamed of my people I would yeah what

00:53:09,930 --> 00:53:15,900
RFS is not there yet it may be I want

00:53:13,470 --> 00:53:20,000
them to exist for the same reason that

00:53:15,900 --> 00:53:24,740
as a BSD guy I want Linux to exist

00:53:20,000 --> 00:53:24,740
competition drives people forward

00:53:27,010 --> 00:53:35,410
I really want another widely-used SSH

00:53:31,330 --> 00:53:38,590
server because OpenSSH owns that market

00:53:35,410 --> 00:53:45,210
yes there's like 3% of other servers out

00:53:38,590 --> 00:53:49,990
there but no we need we need capital and

00:53:45,210 --> 00:53:52,390
I really want to give the person who

00:53:49,990 --> 00:53:54,700
follows me and honest five minutes to

00:53:52,390 --> 00:53:57,120
like plug in their laptop and let their

00:53:54,700 --> 00:53:57,120
imagine

00:53:57,420 --> 00:54:07,030
the backdoor please talk to me about

00:54:03,900 --> 00:54:08,350
what's on that list or anything else I'm

00:54:07,030 --> 00:54:11,490
under all weekend

00:54:08,350 --> 00:54:11,490

YouTube URL: https://www.youtube.com/watch?v=pfBI47UwKL4


