Title: LinuxFest - PostgreSQL + Kafka: The Delight of Change Data Capture
Publication date: 2017-10-18
Playlist: 2017 Videos
Description: 
	This talk is part of the Ohio LinuxFest sysadmin track. It was held in Franklin C at 1PM. The speaker is Jeff Klukas.

The slide deck may be found here: https://www.slideshare.net/JeffKlukas/postgresql-kafka-the-delight-of-change-data-capture
Captions: 
	00:00:02,030 --> 00:00:06,930
change into capture is a topic that I am

00:00:04,890 --> 00:00:10,860
passionate about I hope to get you

00:00:06,930 --> 00:00:12,809
passionate about it to Postgres it's a

00:00:10,860 --> 00:00:15,750
relational database Kafka

00:00:12,809 --> 00:00:17,369
it's a datastore thing and we'll learn

00:00:15,750 --> 00:00:19,730
about it today so if you know nothing

00:00:17,369 --> 00:00:21,630
about Kafka this will be fun

00:00:19,730 --> 00:00:24,029
also if you know nothing about change

00:00:21,630 --> 00:00:25,830
data capture this will be fun there's

00:00:24,029 --> 00:00:30,060
gonna be a live demo involved it's gonna

00:00:25,830 --> 00:00:31,920
be glorious I am Jeff cookus I'm a data

00:00:30,060 --> 00:00:33,960
engineer it's simple if you've never

00:00:31,920 --> 00:00:36,719
heard of simple there's a marketing

00:00:33,960 --> 00:00:38,370
spiel up here on this slide it's the

00:00:36,719 --> 00:00:40,020
whole idea of banking remade with lovely

00:00:38,370 --> 00:00:41,489
design equally lovely tools to help you

00:00:40,020 --> 00:00:44,430
save right inside your account and

00:00:41,489 --> 00:00:46,620
genuine human goodness it's a pretty

00:00:44,430 --> 00:00:49,260
cool product from a technological

00:00:46,620 --> 00:00:51,090
standpoint you can think of us as were

00:00:49,260 --> 00:00:54,360
the first bank born in the cloud we were

00:00:51,090 --> 00:00:57,239
founded in 2009 and we have been Amazon

00:00:54,360 --> 00:01:00,390
Web Services infrastructure since the

00:00:57,239 --> 00:01:05,280
beginning and still are so I have a bias

00:01:00,390 --> 00:01:07,770
towards towards the cloud my particular

00:01:05,280 --> 00:01:09,900
role as I said I'm a data engineer

00:01:07,770 --> 00:01:12,869
I'm also the technical lead for our data

00:01:09,900 --> 00:01:15,360
platform in general so that team is

00:01:12,869 --> 00:01:18,330
responsible for operating Postgres and

00:01:15,360 --> 00:01:21,509
Kafka we're also responsible for data

00:01:18,330 --> 00:01:23,759
pipelines and serving your business

00:01:21,509 --> 00:01:25,500
intelligence users getting it into our

00:01:23,759 --> 00:01:28,170
warehouse I'll talk a little bit about

00:01:25,500 --> 00:01:31,439
that the pipeline I'm going to talk

00:01:28,170 --> 00:01:33,420
about today is it's become a super

00:01:31,439 --> 00:01:35,909
important part of our infrastructure

00:01:33,420 --> 00:01:39,570
both for how our services work and for

00:01:35,909 --> 00:01:41,640
how we do analytics so what are we gonna

00:01:39,570 --> 00:01:44,610
talk about we're gonna talk about this

00:01:41,640 --> 00:01:47,820
data structure idea called a commit log

00:01:44,610 --> 00:01:50,100
I'll explain what that is and it's a

00:01:47,820 --> 00:01:51,930
pattern that shows up in a lot of

00:01:50,100 --> 00:01:55,200
different data stores and we'll see how

00:01:51,930 --> 00:01:56,880
commit logs relate to Postgres and we'll

00:01:55,200 --> 00:01:58,560
see how commit logs are really the

00:01:56,880 --> 00:02:02,490
foundation of Kafka this other data

00:01:58,560 --> 00:02:04,590
store we will do a live demo of bottled

00:02:02,490 --> 00:02:06,689
water which is an open source change

00:02:04,590 --> 00:02:10,530
data capture pipeline from Postgres to

00:02:06,689 --> 00:02:11,830
Kafka and then we will look at some of

00:02:10,530 --> 00:02:14,830
the details of how

00:02:11,830 --> 00:02:15,850
simple uses this whole thing we're gonna

00:02:14,830 --> 00:02:17,860
be a little bit light on that I really

00:02:15,850 --> 00:02:20,350
want to focus on you know the concepts

00:02:17,860 --> 00:02:22,600
here rather than the details of what

00:02:20,350 --> 00:02:24,520
we're doing but I want to motivate like

00:02:22,600 --> 00:02:26,920
it's a pretty powerful pipeline and

00:02:24,520 --> 00:02:29,730
you'll see kind of how it it's just

00:02:26,920 --> 00:02:32,260
pervasive for us at this point

00:02:29,730 --> 00:02:34,540
so you're ready to learn about commit

00:02:32,260 --> 00:02:38,560
logs let's bring it on

00:02:34,540 --> 00:02:40,680
so commit log is it's an abstract you

00:02:38,560 --> 00:02:42,880
know kind of concept of a data structure

00:02:40,680 --> 00:02:45,700
you can imagine it's an ordered sequence

00:02:42,880 --> 00:02:48,400
of records you always append things to

00:02:45,700 --> 00:02:50,500
the end of it it has this word log in

00:02:48,400 --> 00:02:53,470
the name when you think of logs you

00:02:50,500 --> 00:02:55,240
probably think of like system logs you

00:02:53,470 --> 00:02:57,760
know those files that exist on your

00:02:55,240 --> 00:02:59,350
server and your application writes a

00:02:57,760 --> 00:03:01,420
bunch of information about what on earth

00:02:59,350 --> 00:03:02,530
is it doing right now and if you think

00:03:01,420 --> 00:03:04,390
about some of the properties of that log

00:03:02,530 --> 00:03:07,200
you know really if you take this thing

00:03:04,390 --> 00:03:09,850
that's on the screen and you rotate it

00:03:07,200 --> 00:03:11,470
it looks kind of like a log file where

00:03:09,850 --> 00:03:13,450
you're always appending things to the

00:03:11,470 --> 00:03:16,900
end of the file it's scrolling up your

00:03:13,450 --> 00:03:18,400
screen things are freaking out and

00:03:16,900 --> 00:03:23,950
you're trying to figure out what's going

00:03:18,400 --> 00:03:27,130
on but there's some properties of of

00:03:23,950 --> 00:03:29,910
that server log and the way that data is

00:03:27,130 --> 00:03:32,830
going into it first of all it's ordered

00:03:29,910 --> 00:03:34,360
so if you have a single thread of a

00:03:32,830 --> 00:03:35,320
single application that's right into the

00:03:34,360 --> 00:03:37,840
single log file

00:03:35,320 --> 00:03:40,000
you know those messages are going to end

00:03:37,840 --> 00:03:41,290
up in the log in the order in which the

00:03:40,000 --> 00:03:42,730
application wrote them

00:03:41,290 --> 00:03:43,810
there are probably time stamps attached

00:03:42,730 --> 00:03:46,239
and you can see those time stamps

00:03:43,810 --> 00:03:48,610
increasing as it goes on it's immutable

00:03:46,239 --> 00:03:50,170
there's no real concept of going back

00:03:48,610 --> 00:03:52,450
and changing what happened in the server

00:03:50,170 --> 00:03:54,940
to log you're just saying hey fire it

00:03:52,450 --> 00:03:55,959
this event happened it's in the log and

00:03:54,940 --> 00:03:59,019
that's something you go back and

00:03:55,959 --> 00:04:00,160
reference later and it's durable it's

00:03:59,019 --> 00:04:03,250
something that's actually ending up on

00:04:00,160 --> 00:04:05,140
disk if the server goes down you know it

00:04:03,250 --> 00:04:08,799
comes back up and you're able to read

00:04:05,140 --> 00:04:11,860
that file those are the same properties

00:04:08,799 --> 00:04:13,450
of you know commit logs in general and

00:04:11,860 --> 00:04:15,489
as I'm talking about them in in other

00:04:13,450 --> 00:04:17,140
systems as well you can kind of think of

00:04:15,489 --> 00:04:20,230
those same properties of what happens

00:04:17,140 --> 00:04:22,120
with with a server log

00:04:20,230 --> 00:04:25,380
so I said this is immutable we're always

00:04:22,120 --> 00:04:28,900
appending to it it kind of grows forever

00:04:25,380 --> 00:04:31,870
that might be a problem you can't have a

00:04:28,900 --> 00:04:33,940
crow forever so in practice you know

00:04:31,870 --> 00:04:36,100
what do you do with server logs you

00:04:33,940 --> 00:04:38,350
probably you roll over to a new log file

00:04:36,100 --> 00:04:40,210
every day or on some time period or when

00:04:38,350 --> 00:04:41,800
it hits a gigabyte in size you might

00:04:40,210 --> 00:04:43,870
compress it and send it off to some

00:04:41,800 --> 00:04:47,020
other system or delete it off of the

00:04:43,870 --> 00:04:48,460
server and in practice this is a concept

00:04:47,020 --> 00:04:49,960
that happens in other systems that are

00:04:48,460 --> 00:04:52,630
using commit logs too so it kind of

00:04:49,960 --> 00:04:54,760
logically it's this whole history from

00:04:52,630 --> 00:04:57,340
the beginning of time and once you write

00:04:54,760 --> 00:04:58,870
records they stick around forever but in

00:04:57,340 --> 00:05:00,400
practice you're eventually going to

00:04:58,870 --> 00:05:02,020
clean up some of those old records and

00:05:00,400 --> 00:05:05,590
they're not gonna be available on the

00:05:02,020 --> 00:05:08,440
system anymore so let's talk about our

00:05:05,590 --> 00:05:10,480
friend Postgres Postgres it's a

00:05:08,440 --> 00:05:13,600
relational database it has these tables

00:05:10,480 --> 00:05:17,110
you insert records into tables you read

00:05:13,600 --> 00:05:19,120
information out of tables so it doesn't

00:05:17,110 --> 00:05:22,690
look like it's a commit log but there is

00:05:19,120 --> 00:05:24,250
a very important commit log lying behind

00:05:22,690 --> 00:05:26,440
the scenes when you're writing dated a

00:05:24,250 --> 00:05:27,520
Postgres if you're a developer and you

00:05:26,440 --> 00:05:29,290
work with Postgres you may have never

00:05:27,520 --> 00:05:31,510
heard of right ahead logging if you're

00:05:29,290 --> 00:05:32,860
administrator you probably have heard

00:05:31,510 --> 00:05:36,370
and think deeply about write ahead

00:05:32,860 --> 00:05:39,220
logging so what is this I'm going to

00:05:36,370 --> 00:05:43,120
give you a blurb from the Postgres Docs

00:05:39,220 --> 00:05:44,860
because they say it well so while

00:05:43,120 --> 00:05:46,810
central concept is that changes to data

00:05:44,860 --> 00:05:49,330
files who are tables and indexes reside

00:05:46,810 --> 00:05:52,240
must be written only after those changes

00:05:49,330 --> 00:05:53,680
have been logged that is after log

00:05:52,240 --> 00:05:56,680
records describing the changes have been

00:05:53,680 --> 00:05:58,500
flushed to permanent storage so you

00:05:56,680 --> 00:06:00,880
didn't know it but behind the scenes

00:05:58,500 --> 00:06:03,820
Postgres is writing to this right ahead

00:06:00,880 --> 00:06:05,230
log which is a commit log before it ever

00:06:03,820 --> 00:06:06,580
changes those tables that you're

00:06:05,230 --> 00:06:08,230
actually accessing and this is a really

00:06:06,580 --> 00:06:10,960
important part of how Postgres maintains

00:06:08,230 --> 00:06:13,030
durability if if you think about like

00:06:10,960 --> 00:06:15,010
pro scripts has transactions a lot of

00:06:13,030 --> 00:06:19,120
relational databases have transactions

00:06:15,010 --> 00:06:23,320
and within a transaction you open it you

00:06:19,120 --> 00:06:25,630
might write 12 different records to 12

00:06:23,320 --> 00:06:28,240
different tables and then you commit the

00:06:25,630 --> 00:06:30,310
transaction and you expect either the

00:06:28,240 --> 00:06:32,080
entire transaction is going to happen or

00:06:30,310 --> 00:06:33,490
it's going to get aborted and it says

00:06:32,080 --> 00:06:36,069
sorry that didn't work

00:06:33,490 --> 00:06:38,289
and none of those changes happens but

00:06:36,069 --> 00:06:39,789
you're hitting 12 different tables those

00:06:38,289 --> 00:06:42,190
all you know are happening in different

00:06:39,789 --> 00:06:42,789
files actually sitting on the file

00:06:42,190 --> 00:06:44,979
system

00:06:42,789 --> 00:06:46,599
so if Postgres starts into that update

00:06:44,979 --> 00:06:48,460
you know makes changes to two of those

00:06:46,599 --> 00:06:51,639
tables and then somebody pulls the plug

00:06:48,460 --> 00:06:54,460
on the server it goes down when it comes

00:06:51,639 --> 00:06:55,780
back up it's now in a corrupt state some

00:06:54,460 --> 00:06:57,759
of those changes have happened some of

00:06:55,780 --> 00:06:59,319
them haven't and the way that Postgres

00:06:57,759 --> 00:07:01,060
protects against this is that it

00:06:59,319 --> 00:07:03,550
realizes hey i'm in a corrupt state

00:07:01,060 --> 00:07:06,250
I'm going to go replay what I've already

00:07:03,550 --> 00:07:08,770
written into the wall to make sure that

00:07:06,250 --> 00:07:12,599
I get back up to a consistent state and

00:07:08,770 --> 00:07:14,680
yeah your logic is all is all happy so

00:07:12,599 --> 00:07:17,349
so it's really kind of the wall is this

00:07:14,680 --> 00:07:20,110
data store behind the data store of

00:07:17,349 --> 00:07:21,789
Postgres that's recording all these

00:07:20,110 --> 00:07:24,430
changes so every insert every update

00:07:21,789 --> 00:07:27,520
every delete has this separate record

00:07:24,430 --> 00:07:29,259
inside of the wall and it's this whole

00:07:27,520 --> 00:07:32,229
event history that's being kept by

00:07:29,259 --> 00:07:33,789
Postgres so this is exactly what we want

00:07:32,229 --> 00:07:35,740
what I'm talking about change data

00:07:33,789 --> 00:07:38,949
capture change data capture is the

00:07:35,740 --> 00:07:41,889
concept of getting a full stream of

00:07:38,949 --> 00:07:45,280
events out of a database like that so

00:07:41,889 --> 00:07:47,740
the wall is already what we want except

00:07:45,280 --> 00:07:49,270
it's in this format that exists for

00:07:47,740 --> 00:07:50,860
Postgres to be able to do stuff with it

00:07:49,270 --> 00:07:52,599
it's in this you know format that's

00:07:50,860 --> 00:07:54,729
talking about where are things on disk

00:07:52,599 --> 00:07:56,159
it's not something that we can interpret

00:07:54,729 --> 00:08:01,060
for other uses

00:07:56,159 --> 00:08:02,949
so back in 2015 Postgres 29.4 introduced

00:08:01,060 --> 00:08:05,889
a new feature called logical decoding

00:08:02,949 --> 00:08:08,409
and this is what the Postgres Doc's say

00:08:05,889 --> 00:08:10,719
about logical decoding it's the process

00:08:08,409 --> 00:08:12,330
of extracting all persistent changes to

00:08:10,719 --> 00:08:15,190
a databases tables into a coherent

00:08:12,330 --> 00:08:16,719
easy-to-understand format which can be

00:08:15,190 --> 00:08:20,110
interpreted without detailed knowledge

00:08:16,719 --> 00:08:21,520
of the databases internal state so the

00:08:20,110 --> 00:08:24,280
pipeline I'm talking about today is

00:08:21,520 --> 00:08:26,620
using this logical decoding feature it's

00:08:24,280 --> 00:08:29,319
basically as as those changes being

00:08:26,620 --> 00:08:31,180
written to log you can also have an

00:08:29,319 --> 00:08:34,000
output plug-in that's installed in

00:08:31,180 --> 00:08:36,550
Postgres that is additionally writing

00:08:34,000 --> 00:08:37,899
those messages in some format that you

00:08:36,550 --> 00:08:41,409
actually want to extract to another

00:08:37,899 --> 00:08:43,779
system in our for our purposes here it's

00:08:41,409 --> 00:08:47,050
going to be a JSON format so there's an

00:08:43,779 --> 00:08:49,149
output plug-in it's some C code that you

00:08:47,050 --> 00:08:51,820
that you add is an extension to Postgres

00:08:49,149 --> 00:08:54,910
it lets it know how to translate records

00:08:51,820 --> 00:08:56,820
into the format that you want and then

00:08:54,910 --> 00:09:00,970
you end up having a client application

00:08:56,820 --> 00:09:03,130
that connects to Postgres on a what's

00:09:00,970 --> 00:09:05,440
called a replication slot and then that

00:09:03,130 --> 00:09:07,300
replication slot Postgres is you know

00:09:05,440 --> 00:09:08,800
storing some state about where are you

00:09:07,300 --> 00:09:10,870
in the changes that have happened to the

00:09:08,800 --> 00:09:12,550
database and post-course is able to ship

00:09:10,870 --> 00:09:18,450
those changes in the format that you

00:09:12,550 --> 00:09:20,769
want to your application so that is a

00:09:18,450 --> 00:09:23,079
commit log that's lying underneath

00:09:20,769 --> 00:09:25,899
Postgres is that you know that right

00:09:23,079 --> 00:09:28,839
ahead log that Postgres uses for its its

00:09:25,899 --> 00:09:31,529
own uses for recovering to a consensus

00:09:28,839 --> 00:09:34,779
of the state it also uses that for

00:09:31,529 --> 00:09:36,130
replicas other Postgres instances that

00:09:34,779 --> 00:09:38,410
are mirroring the changes in the mean

00:09:36,130 --> 00:09:40,240
database and logical decoding is this

00:09:38,410 --> 00:09:41,560
other way of extracting changes and

00:09:40,240 --> 00:09:42,250
getting them into a format we can use

00:09:41,560 --> 00:09:45,100
somewhere else

00:09:42,250 --> 00:09:49,089
so in our case that other place we want

00:09:45,100 --> 00:09:51,970
things to go is Kafka so what on earth

00:09:49,089 --> 00:09:55,540
is this Kafka thing I'd do any of you

00:09:51,970 --> 00:09:58,000
use Kafka all right

00:09:55,540 --> 00:10:00,579
have others of you heard of Kafka you

00:09:58,000 --> 00:10:03,790
have no idea what it is good

00:10:00,579 --> 00:10:06,190
okay so Kafka it is a data store it

00:10:03,790 --> 00:10:09,250
builds itself as a distributed streaming

00:10:06,190 --> 00:10:11,740
platform but this is kind of a recent

00:10:09,250 --> 00:10:15,640
rebranding if you looked at the Kafka

00:10:11,740 --> 00:10:19,810
Doc's about a year ago it said Apache

00:10:15,640 --> 00:10:21,940
Kafka a distributed commit log so big

00:10:19,810 --> 00:10:24,010
surprise commit logs are involved here

00:10:21,940 --> 00:10:25,390
and really is like cough gives a data

00:10:24,010 --> 00:10:27,339
store that takes this concept of a

00:10:25,390 --> 00:10:28,720
commit log and kind of puts it front and

00:10:27,339 --> 00:10:29,260
center and it's the thing that you

00:10:28,720 --> 00:10:32,230
interact with

00:10:29,260 --> 00:10:36,579
so in Kafka terminology a single commit

00:10:32,230 --> 00:10:39,190
log is called a topic partition these

00:10:36,579 --> 00:10:41,709
are groups together into logical topics

00:10:39,190 --> 00:10:45,459
hence the name but the topic partition

00:10:41,709 --> 00:10:47,199
is the fundamental unit of of data

00:10:45,459 --> 00:10:48,660
storage and it's the thing that under

00:10:47,199 --> 00:10:50,800
the hood everything is interacting with

00:10:48,660 --> 00:10:53,500
so what is the talk what partition look

00:10:50,800 --> 00:10:55,660
like again it looks like this commit log

00:10:53,500 --> 00:10:57,190
that we've seen in other images and in

00:10:55,660 --> 00:10:58,700
this case these numbers on them are

00:10:57,190 --> 00:11:02,210
actually significant

00:10:58,700 --> 00:11:04,460
Kafka calls those offsets so when you

00:11:02,210 --> 00:11:05,840
create a new topic partition the very

00:11:04,460 --> 00:11:07,760
first message is going to be assigned to

00:11:05,840 --> 00:11:10,910
offset zero the next message will be

00:11:07,760 --> 00:11:13,280
offset one then offset two and on and on

00:11:10,910 --> 00:11:17,420
that's how Kafka knows about indexing

00:11:13,280 --> 00:11:19,580
you where it is in a log so there are

00:11:17,420 --> 00:11:23,090
clients that connect to this producers

00:11:19,580 --> 00:11:25,130
and consumers and producers they're

00:11:23,090 --> 00:11:27,290
basically they're talking to Kafka they

00:11:25,130 --> 00:11:28,550
are saying hey I have this bucket of

00:11:27,290 --> 00:11:30,920
bytes that I wants you to write to this

00:11:28,550 --> 00:11:32,570
particular topic partition Kafka accepts

00:11:30,920 --> 00:11:34,250
those rights and appends them to the end

00:11:32,570 --> 00:11:37,760
of the log it will you know whatever

00:11:34,250 --> 00:11:39,650
order it receives and accepts those

00:11:37,760 --> 00:11:42,230
rights in is the order in which it's

00:11:39,650 --> 00:11:43,190
gonna write them into the log at the

00:11:42,230 --> 00:11:45,710
same time you can have multiple

00:11:43,190 --> 00:11:48,680
consumers consuming from that topic

00:11:45,710 --> 00:11:51,980
partition and when a consumer connects

00:11:48,680 --> 00:11:53,720
to a topic partition basically all it

00:11:51,980 --> 00:11:55,580
says is I want you I want this

00:11:53,720 --> 00:11:57,740
particular topic I want this particular

00:11:55,580 --> 00:12:00,650
partition number and I want to start at

00:11:57,740 --> 00:12:02,810
this particular offset so again the

00:12:00,650 --> 00:12:05,890
offset is that number of the record in

00:12:02,810 --> 00:12:08,870
the log and when you attach to an offset

00:12:05,890 --> 00:12:10,910
Kafka's gonna start just sending you big

00:12:08,870 --> 00:12:13,310
batches of records until you get up all

00:12:10,910 --> 00:12:15,410
the way to the end of the log and the

00:12:13,310 --> 00:12:17,030
consumer has a pole loop where it's

00:12:15,410 --> 00:12:18,830
continually asking hey do you have new

00:12:17,030 --> 00:12:20,770
information and Kafka you know once you

00:12:18,830 --> 00:12:23,600
get to the end of the of the partition

00:12:20,770 --> 00:12:26,510
Kafka will send you you know new records

00:12:23,600 --> 00:12:28,460
as it has them available so different

00:12:26,510 --> 00:12:31,100
consumers can attach at different

00:12:28,460 --> 00:12:33,080
offsets and that's how you interact with

00:12:31,100 --> 00:12:35,720
it producers and consumers if you're

00:12:33,080 --> 00:12:37,730
familiar with message queues things like

00:12:35,720 --> 00:12:41,570
rabbitmq other things that are using

00:12:37,730 --> 00:12:44,360
AMQP protocol this looks a lot like it a

00:12:41,570 --> 00:12:46,780
lot like a message queue a fundamental

00:12:44,360 --> 00:12:49,760
difference here though is this idea of

00:12:46,780 --> 00:12:52,550
attaching it offsets and that's really

00:12:49,760 --> 00:12:54,890
all of the tracking that happens so in a

00:12:52,550 --> 00:12:56,600
message queue like rabbitmq the broker

00:12:54,890 --> 00:12:58,370
has to keep track of which messages have

00:12:56,600 --> 00:13:00,260
I sent and it's really doing it message

00:12:58,370 --> 00:13:01,850
by message and your consumer has to send

00:13:00,260 --> 00:13:03,890
back acknowledgement saying like yes I

00:13:01,850 --> 00:13:04,460
got this message I processed it you're

00:13:03,890 --> 00:13:07,130
good to go

00:13:04,460 --> 00:13:08,059
and you know it'll be removing messages

00:13:07,130 --> 00:13:11,839
as

00:13:08,059 --> 00:13:13,399
as they get consumed in Kafka like Kafka

00:13:11,839 --> 00:13:15,499
has a whole lot less responsibility on

00:13:13,399 --> 00:13:17,149
the burger side just when you attach as

00:13:15,499 --> 00:13:18,559
a consumer you say I want this offset

00:13:17,149 --> 00:13:21,170
and then Kafka is gonna give you

00:13:18,559 --> 00:13:23,809
everything after that and it's kind of

00:13:21,170 --> 00:13:24,920
up to you as the consumer to make sure

00:13:23,809 --> 00:13:26,720
that you've processed all of those

00:13:24,920 --> 00:13:31,639
records and that you're keeping track of

00:13:26,720 --> 00:13:34,279
where you are I'm in the log so we're

00:13:31,639 --> 00:13:37,790
gonna zoom out a little bit this is an

00:13:34,279 --> 00:13:38,990
individual topic partition now talk

00:13:37,790 --> 00:13:42,559
about partitions can be grouped into

00:13:38,990 --> 00:13:44,899
topics and this also gets to the

00:13:42,559 --> 00:13:47,629
distributed nature of Kafka so Kafka is

00:13:44,899 --> 00:13:51,740
designed to have many brokers in a

00:13:47,629 --> 00:13:55,730
cluster so a broker is a server in the

00:13:51,740 --> 00:13:57,259
Kafka cluster and if you create a topic

00:13:55,730 --> 00:14:00,679
so when you create a topic you can say

00:13:57,259 --> 00:14:02,749
hey I want to have a topic named foo and

00:14:00,679 --> 00:14:04,819
I want it to have three partitions and

00:14:02,749 --> 00:14:07,249
it's going to create those partitions

00:14:04,819 --> 00:14:09,949
across different brokers in the cluster

00:14:07,249 --> 00:14:11,509
so this particular topic here has three

00:14:09,949 --> 00:14:14,449
partitions each one of those is on a

00:14:11,509 --> 00:14:17,170
separate broker in simplest case we have

00:14:14,449 --> 00:14:19,490
a pretty small cluster of four brokers

00:14:17,170 --> 00:14:21,230
so this is realistic for us that you'd

00:14:19,490 --> 00:14:26,089
have a simple topic that's spread across

00:14:21,230 --> 00:14:28,910
multiple brokers rights come in and they

00:14:26,089 --> 00:14:31,249
can be round robins to different topics

00:14:28,910 --> 00:14:33,619
but usually what you want is that you

00:14:31,249 --> 00:14:35,990
want to set a key on the message when

00:14:33,619 --> 00:14:37,579
you write it and that key is used for

00:14:35,990 --> 00:14:40,459
routing it to a particular topic so

00:14:37,579 --> 00:14:43,519
let's imagine that we set the user ID as

00:14:40,459 --> 00:14:45,829
the key of the message what that means

00:14:43,519 --> 00:14:48,549
is that you know the user ID for Alice

00:14:45,829 --> 00:14:51,139
every time a message for Alice comes in

00:14:48,549 --> 00:14:53,660
we are ensured that we'll always go to

00:14:51,139 --> 00:14:55,369
the same topic partition so if Alice

00:14:53,660 --> 00:14:57,439
goes to the to partition zero the first

00:14:55,369 --> 00:14:59,089
time record is written for for her all

00:14:57,439 --> 00:15:01,249
of the other records related to Alice

00:14:59,089 --> 00:15:03,079
will be written to partition 0 this

00:15:01,249 --> 00:15:05,420
becomes super helpful when you have a

00:15:03,079 --> 00:15:07,129
distributed application that's consuming

00:15:05,420 --> 00:15:09,199
and consumers also know about this

00:15:07,129 --> 00:15:10,819
multiple partition thing going on

00:15:09,199 --> 00:15:12,860
there's a concept called consumer groups

00:15:10,819 --> 00:15:14,540
and you can have you know multiple

00:15:12,860 --> 00:15:16,399
instances of your application all

00:15:14,540 --> 00:15:17,720
connecting to Kafka and Kafka will

00:15:16,399 --> 00:15:20,160
figure out ok I'm going to distribute

00:15:17,720 --> 00:15:21,750
these partitions amongst you and maybe

00:15:20,160 --> 00:15:23,899
one instance of your application is only

00:15:21,750 --> 00:15:26,250
getting records from partition zero and

00:15:23,899 --> 00:15:29,160
you each of your instances is then

00:15:26,250 --> 00:15:31,620
effectively you servicing some subset of

00:15:29,160 --> 00:15:33,839
your customers because they're all your

00:15:31,620 --> 00:15:38,100
user ideas are determining where they go

00:15:33,839 --> 00:15:42,569
in the partitions so cool that is that

00:15:38,100 --> 00:15:44,220
is the topic and we talked about with

00:15:42,569 --> 00:15:45,180
commit logs in general they just grow

00:15:44,220 --> 00:15:46,529
and grow and grow

00:15:45,180 --> 00:15:50,730
you're always depending records to the

00:15:46,529 --> 00:15:53,100
end in reality you have to get those off

00:15:50,730 --> 00:15:55,430
of disk sometime so the default behavior

00:15:53,100 --> 00:15:57,810
in Kafka is if the time-based retention

00:15:55,430 --> 00:15:58,949
so this is different from message queues

00:15:57,810 --> 00:16:00,389
message queues are keeping track of

00:15:58,949 --> 00:16:01,860
where are all my consumers and then I'll

00:16:00,389 --> 00:16:03,240
get rid of messages as fast as it can

00:16:01,860 --> 00:16:06,689
once all the consumers have consumed

00:16:03,240 --> 00:16:08,519
them in Kafka by default records stick

00:16:06,689 --> 00:16:10,939
around for seven days so that's an

00:16:08,519 --> 00:16:13,019
interesting a piece of durability and

00:16:10,939 --> 00:16:14,970
flexibility and that you can have some

00:16:13,019 --> 00:16:16,829
consumer three days down the road that

00:16:14,970 --> 00:16:19,050
then is able to see a bunch of history

00:16:16,829 --> 00:16:20,850
of what's happened without knowing about

00:16:19,050 --> 00:16:24,120
it beforehand but that's the default

00:16:20,850 --> 00:16:27,000
behavior hmm there is another type of

00:16:24,120 --> 00:16:30,839
behavior called log compaction that you

00:16:27,000 --> 00:16:34,170
can choose on a per topic basis so what

00:16:30,839 --> 00:16:37,589
does a compacted topic look like instead

00:16:34,170 --> 00:16:40,889
of time-based retention it will retain

00:16:37,589 --> 00:16:43,259
messages based on key so when a new

00:16:40,889 --> 00:16:46,470
message comes in with the same key as an

00:16:43,259 --> 00:16:49,110
old message those old messages with that

00:16:46,470 --> 00:16:53,220
same key are eligible for your being

00:16:49,110 --> 00:16:56,100
ejected and removed from disk so at the

00:16:53,220 --> 00:16:58,259
top here we have this is probably too

00:16:56,100 --> 00:17:01,620
small to read so I'll go into some

00:16:58,259 --> 00:17:04,110
detail here we have what a log looks

00:17:01,620 --> 00:17:05,669
like before compaction there are values

00:17:04,110 --> 00:17:08,730
the values in each of these cases are

00:17:05,669 --> 00:17:10,909
different v1 through the 11 but the keys

00:17:08,730 --> 00:17:15,990
are sometimes repeating so if we look at

00:17:10,909 --> 00:17:18,059
key k1 that appears at offset zero it

00:17:15,990 --> 00:17:23,010
appears it offset two and it appears at

00:17:18,059 --> 00:17:26,069
offset three so if this topic is is

00:17:23,010 --> 00:17:28,679
configured for compaction Kafka is only

00:17:26,069 --> 00:17:31,710
required to keep that most recent value

00:17:28,679 --> 00:17:33,450
at offset three and after compaction

00:17:31,710 --> 00:17:37,679
happens as a periodic process

00:17:33,450 --> 00:17:41,370
that's running you'll end up with those

00:17:37,679 --> 00:17:43,500
initial values for K one at offset zero

00:17:41,370 --> 00:17:48,110
and offset two they're removed from the

00:17:43,500 --> 00:17:50,460
log and you only have offset three left

00:17:48,110 --> 00:17:52,620
so what this looks like afterwards is

00:17:50,460 --> 00:17:55,169
that you only have the most recent key

00:17:52,620 --> 00:17:59,090
or the most recent message for each key

00:17:55,169 --> 00:18:03,870
and this looks a whole lot like a

00:17:59,090 --> 00:18:06,630
database table with a primary key on it

00:18:03,870 --> 00:18:08,940
so logically this is exactly the same

00:18:06,630 --> 00:18:12,000
thing that Postgres is doing with its

00:18:08,940 --> 00:18:14,639
with its wall so if you insert you open

00:18:12,000 --> 00:18:16,740
a transaction you insert a record you

00:18:14,639 --> 00:18:18,809
update it you delete it you insert it

00:18:16,740 --> 00:18:21,029
again you update it again those are each

00:18:18,809 --> 00:18:24,360
individual messages in the right ahead

00:18:21,029 --> 00:18:25,980
log but what ends up in your table at

00:18:24,360 --> 00:18:27,750
the end of the day when you make a query

00:18:25,980 --> 00:18:30,029
is you're only seeing the most recent

00:18:27,750 --> 00:18:34,289
version of that row you know for that

00:18:30,029 --> 00:18:36,510
primary key coming back at you so this

00:18:34,289 --> 00:18:40,049
this is a perfect way to model a

00:18:36,510 --> 00:18:45,330
database table in Kafka in a log

00:18:40,049 --> 00:18:47,010
compacted topic so that's a bunch of it

00:18:45,330 --> 00:18:48,600
and also if there are questions I'm

00:18:47,010 --> 00:18:51,690
totally ok with taking questions during

00:18:48,600 --> 00:18:53,519
the talk the next step of this is that

00:18:51,690 --> 00:18:56,070
we're gonna go into the live demo so if

00:18:53,519 --> 00:18:58,769
there any questions you have at the

00:18:56,070 --> 00:19:01,110
moment before we see the world fall

00:18:58,769 --> 00:19:09,630
apart in a terminal this is your chance

00:19:01,110 --> 00:19:11,700
yes yet so the question is so we're

00:19:09,630 --> 00:19:14,940
pointing consumers at Kafka rather than

00:19:11,700 --> 00:19:16,679
the SQL Server yeah so in our case we

00:19:14,940 --> 00:19:18,870
have a market micro service architecture

00:19:16,679 --> 00:19:20,460
it's only near the service that writes

00:19:18,870 --> 00:19:21,960
to the database is the only thing that's

00:19:20,460 --> 00:19:23,880
able to read from the database and if

00:19:21,960 --> 00:19:26,340
anything else outside of that micro

00:19:23,880 --> 00:19:28,980
service wants to find out about what's

00:19:26,340 --> 00:19:30,570
happening then they go to Kafka to hear

00:19:28,980 --> 00:19:31,270
about hey this service is doing this

00:19:30,570 --> 00:19:59,320
other thing

00:19:31,270 --> 00:20:01,960
question over here but so so the

00:19:59,320 --> 00:20:04,090
question is about yeah is about rights

00:20:01,960 --> 00:20:06,700
coming from multiple sources and what

00:20:04,090 --> 00:20:09,970
guarantees are there around that I'm not

00:20:06,700 --> 00:20:12,430
going to get into that in this talk but

00:20:09,970 --> 00:20:14,290
we can talk about it afterwards so in

00:20:12,430 --> 00:20:17,200
general if you have multiple producers

00:20:14,290 --> 00:20:19,930
going to a topic those might come in in

00:20:17,200 --> 00:20:22,120
whatever order there are tricks you can

00:20:19,930 --> 00:20:25,450
do and there's a new feature recently of

00:20:22,120 --> 00:20:29,250
a concept of transactions with Kafka it

00:20:25,450 --> 00:20:31,330
gets crazy it's cool it's also very new

00:20:29,250 --> 00:20:44,440
that's that one more question and then

00:20:31,330 --> 00:20:46,210
we'll move on so things to watch out for

00:20:44,440 --> 00:20:50,100
for consumers with persistent

00:20:46,210 --> 00:20:50,100
connections tell me more

00:20:54,640 --> 00:21:02,810
sure yeah yeah so sure so the you know

00:21:00,170 --> 00:21:05,660
the main library that is included with

00:21:02,810 --> 00:21:08,680
you know Kafka is there Java library

00:21:05,660 --> 00:21:10,820
that's kind of like the you know the

00:21:08,680 --> 00:21:12,290
default consumer and there are other

00:21:10,820 --> 00:21:14,330
libraries that are kind of mimicking the

00:21:12,290 --> 00:21:17,600
behavior but what's implemented there

00:21:14,330 --> 00:21:20,120
for consumer behavior is so so I said

00:21:17,600 --> 00:21:21,860
it's kind of applications responsibility

00:21:20,120 --> 00:21:26,840
to keep track of where it is like what

00:21:21,860 --> 00:21:29,570
offset it is it is consumed Kafka does

00:21:26,840 --> 00:21:32,510
provide actually a nice mechanism around

00:21:29,570 --> 00:21:34,970
tracking offsets now of like within that

00:21:32,510 --> 00:21:36,950
consumer code you can have it commit

00:21:34,970 --> 00:21:39,410
offsets to a special topic in Kafka and

00:21:36,950 --> 00:21:41,630
it kind of hides all that from you but

00:21:39,410 --> 00:21:44,270
it is very important to understand the

00:21:41,630 --> 00:21:46,970
details of that of how the consumer loop

00:21:44,270 --> 00:21:48,950
works when you're committing offsets and

00:21:46,970 --> 00:21:50,840
there is there's a heartbeat mechanism

00:21:48,950 --> 00:21:52,820
to with the clients so that the broker

00:21:50,840 --> 00:21:55,100
will eventually like if you if it

00:21:52,820 --> 00:21:56,780
doesn't hear from a client for by

00:21:55,100 --> 00:21:59,750
default like a minute something like

00:21:56,780 --> 00:22:02,210
that it will consider that consumer dead

00:21:59,750 --> 00:22:03,530
it will you know it will allow those

00:22:02,210 --> 00:22:04,940
partitions that were assigned to that

00:22:03,530 --> 00:22:07,490
consumer to be read by some other

00:22:04,940 --> 00:22:09,110
instance in that consumer group but this

00:22:07,490 --> 00:22:11,450
is getting into details that are kind of

00:22:09,110 --> 00:22:14,630
beyond the scope of what I'm playing it

00:22:11,450 --> 00:22:18,050
to talk about in here so I'm gonna forge

00:22:14,630 --> 00:22:19,730
ahead with with the demo and we can we

00:22:18,050 --> 00:22:21,170
can get back to these things at the end

00:22:19,730 --> 00:22:25,460
I'll leave some time for questions at

00:22:21,170 --> 00:22:28,780
the end so bottled water created by this

00:22:25,460 --> 00:22:31,790
gentleman Martin Collette min I love him

00:22:28,780 --> 00:22:33,260
he he has some writings which are

00:22:31,790 --> 00:22:37,130
fantastic

00:22:33,260 --> 00:22:42,920
he is a a researcher in distributed

00:22:37,130 --> 00:22:44,510
systems at Cambridge in the UK and

00:22:42,920 --> 00:22:47,570
you'll see him at conferences on

00:22:44,510 --> 00:22:50,660
distributed systems all the time but he

00:22:47,570 --> 00:22:53,300
was so he also so he's an academic but

00:22:50,660 --> 00:22:55,970
he also has some history in the startup

00:22:53,300 --> 00:22:58,940
world I believe he spent some time at

00:22:55,970 --> 00:23:00,440
LinkedIn Kafka came out of LinkedIn it

00:22:58,940 --> 00:23:02,930
was developed at LinkedIn it was open

00:23:00,440 --> 00:23:05,870
sourced than you know the engineers that

00:23:02,930 --> 00:23:08,180
made Kafka left LinkedIn and

00:23:05,870 --> 00:23:10,730
have a company called confluent now that

00:23:08,180 --> 00:23:13,790
does that they're the main maintainer of

00:23:10,730 --> 00:23:15,620
Kafka my name is Martin Clubman knows

00:23:13,790 --> 00:23:19,820
those folks he was supported to write

00:23:15,620 --> 00:23:22,280
this kind of proof of concept of a

00:23:19,820 --> 00:23:24,800
change data capture pipeline from from

00:23:22,280 --> 00:23:26,270
Postgres to Kafka and that is what we're

00:23:24,800 --> 00:23:29,690
gonna show here so I'm going to demo

00:23:26,270 --> 00:23:33,110
that we are going to mirror the display

00:23:29,690 --> 00:23:35,090
everything is going to work perfectly we

00:23:33,110 --> 00:23:38,300
have a terminal all right I'm gonna

00:23:35,090 --> 00:23:41,090
clear this dismiss tip I'm gonna clear

00:23:38,300 --> 00:23:43,970
this all right so we're gonna go over to

00:23:41,090 --> 00:23:47,540
Chrome here so bottled water PG this

00:23:43,970 --> 00:23:49,820
exists on on github I I forked it for

00:23:47,540 --> 00:23:51,770
the purposes of this talk I changed some

00:23:49,820 --> 00:23:55,670
of the the tutorial

00:23:51,770 --> 00:23:58,070
yo details to match what I want to show

00:23:55,670 --> 00:24:01,460
you today but most of the content here

00:23:58,070 --> 00:24:04,580
in this readme is is from the main

00:24:01,460 --> 00:24:06,710
confluent Inc bottled water project most

00:24:04,580 --> 00:24:08,330
important thing to note is I am so

00:24:06,710 --> 00:24:09,740
thankful that they actually put this

00:24:08,330 --> 00:24:11,030
note here now because it was in limbo

00:24:09,740 --> 00:24:12,740
for a long time it seems like this was

00:24:11,030 --> 00:24:16,550
gonna evolve into like this is gonna be

00:24:12,740 --> 00:24:18,110
the Postgres to kafka pipeline but

00:24:16,550 --> 00:24:20,780
nobody was working on it and they

00:24:18,110 --> 00:24:24,650
finally declared it dead so don't use

00:24:20,780 --> 00:24:27,350
this in production but they do helpfully

00:24:24,650 --> 00:24:29,210
link you to Debbie's iam I will mention

00:24:27,350 --> 00:24:31,160
that at the end the B's iam is really

00:24:29,210 --> 00:24:33,860
the open source solution that exists

00:24:31,160 --> 00:24:35,179
today that will let you do what we're

00:24:33,860 --> 00:24:36,890
talking about here it's a much more

00:24:35,179 --> 00:24:39,410
complicated project though than bottled

00:24:36,890 --> 00:24:43,190
water which is why I am demoing this

00:24:39,410 --> 00:24:46,250
today so we're gonna jump to it they

00:24:43,190 --> 00:24:48,920
have this whole docker compose set up I

00:24:46,250 --> 00:24:50,540
don't know a whole lot about docker but

00:24:48,920 --> 00:24:52,490
I can run the commands that are in this

00:24:50,540 --> 00:24:55,040
readme and that's what we're gonna do

00:24:52,490 --> 00:24:57,410
today so I'm gonna start copying pasting

00:24:55,040 --> 00:25:00,530
stuff so first this is just setting a

00:24:57,410 --> 00:25:04,240
bunch of parameters that are going to be

00:25:00,530 --> 00:25:07,600
sent to Kafka when we start that up and

00:25:04,240 --> 00:25:14,679
then we're gonna start up some services

00:25:07,600 --> 00:25:15,160
so if you've never seen docker it's a

00:25:14,679 --> 00:25:17,020
thing

00:25:15,160 --> 00:25:19,960
there are containers it runs a bunch of

00:25:17,020 --> 00:25:22,120
stuff on your machine docker compose is

00:25:19,960 --> 00:25:23,710
a nice wrapper on top of docker where

00:25:22,120 --> 00:25:26,230
you can have you basically have a yeah

00:25:23,710 --> 00:25:28,179
mol file and you define I want all of

00:25:26,230 --> 00:25:31,150
these containers to work together and

00:25:28,179 --> 00:25:32,500
it's a way for orchestrating a couple of

00:25:31,150 --> 00:25:34,960
containers to all run on the same

00:25:32,500 --> 00:25:37,660
machine so in this case we just told

00:25:34,960 --> 00:25:41,260
docker compose look at your

00:25:37,660 --> 00:25:43,630
configuration file and spin up Kafka the

00:25:41,260 --> 00:25:44,799
confluence schema registry don't worry

00:25:43,630 --> 00:25:47,620
about that part we're not gonna talk

00:25:44,799 --> 00:25:50,740
about it it's a detail here and Postgres

00:25:47,620 --> 00:25:53,230
so we got Kafka and Postgres running now

00:25:50,740 --> 00:25:55,330
we are going to verify that those are

00:25:53,230 --> 00:25:59,200
indeed running by running docker compose

00:25:55,330 --> 00:26:03,270
PS and tells us hey we've got Kafka

00:25:59,200 --> 00:26:08,470
we've got scheme and registry we've got

00:26:03,270 --> 00:26:12,640
zookeeper going on here and we'll see if

00:26:08,470 --> 00:26:15,690
this works and then what I'm going to do

00:26:12,640 --> 00:26:20,950
next here is I'm going to create a topic

00:26:15,690 --> 00:26:22,600
so this is a wrapper around there's a

00:26:20,950 --> 00:26:25,659
command-line tool that's included with

00:26:22,600 --> 00:26:27,220
Kafka called Kafka topics and this is a

00:26:25,659 --> 00:26:29,640
way that you can create a topic from the

00:26:27,220 --> 00:26:32,020
command line there are some

00:26:29,640 --> 00:26:34,090
configuration options in here related to

00:26:32,020 --> 00:26:36,309
compaction just to make this easier for

00:26:34,090 --> 00:26:38,470
this talk but I'm telling it I want one

00:26:36,309 --> 00:26:41,260
partition for this topic and I wanted to

00:26:38,470 --> 00:26:42,909
have a replication factor of one Kafka's

00:26:41,260 --> 00:26:44,970
got a cool replication mechanism built

00:26:42,909 --> 00:26:47,590
in so you can have a partition

00:26:44,970 --> 00:26:52,419
replicated to multiple brokers that's

00:26:47,590 --> 00:26:55,419
important if you want consistency in the

00:26:52,419 --> 00:26:57,370
case of various failures so we ran that

00:26:55,419 --> 00:27:03,010
we created this topic test it told us

00:26:57,370 --> 00:27:05,230
great things look good now we are going

00:27:03,010 --> 00:27:08,080
to actually log into our Postgres

00:27:05,230 --> 00:27:13,299
instance using the lovely piece equal

00:27:08,080 --> 00:27:15,669
tool I'm gonna clear the screen first so

00:27:13,299 --> 00:27:17,620
this should get us to a P P SQL prompt

00:27:15,669 --> 00:27:18,820
great we have this nice pristine

00:27:17,620 --> 00:27:20,889
Postgres instance

00:27:18,820 --> 00:27:24,809
no relations found there's nothing in

00:27:20,889 --> 00:27:24,809
here right now so let's create something

00:27:25,289 --> 00:27:29,230
we're gonna run these commands and

00:27:27,519 --> 00:27:32,039
Postgres so what did we just do there

00:27:29,230 --> 00:27:34,960
first we create extension bottled-water

00:27:32,039 --> 00:27:37,750
this instance of Postgres has the

00:27:34,960 --> 00:27:39,759
bottled water output plug-in available

00:27:37,750 --> 00:27:43,179
to it and this is something you do in

00:27:39,759 --> 00:27:44,710
Postgres to enable an extension so this

00:27:43,179 --> 00:27:47,440
is enabling the bottled water output

00:27:44,710 --> 00:27:49,539
plug-in and then we are creating a table

00:27:47,440 --> 00:27:56,110
called test and it's just got these

00:27:49,539 --> 00:27:58,629
these two these two columns ID which

00:27:56,110 --> 00:28:00,639
it's automatically incrementing that

00:27:58,629 --> 00:28:02,830
every time we inserted a record and a

00:28:00,639 --> 00:28:07,210
value called text so let's select star

00:28:02,830 --> 00:28:12,759
from test and this is what our table

00:28:07,210 --> 00:28:16,059
looks like right now now we're gonna

00:28:12,759 --> 00:28:23,169
step back and we are going to start up

00:28:16,059 --> 00:28:25,269
the bottled water client program so this

00:28:23,169 --> 00:28:28,299
change in to capture pipeline one side

00:28:25,269 --> 00:28:30,759
of it is on the Postgres side you set up

00:28:28,299 --> 00:28:32,740
an output plug-in it has you know a

00:28:30,759 --> 00:28:35,049
logical replication slot on the Postgres

00:28:32,740 --> 00:28:36,730
side but then you also need some client

00:28:35,049 --> 00:28:39,399
application that's connecting to that

00:28:36,730 --> 00:28:40,950
and pulling this data out so that's

00:28:39,399 --> 00:28:43,990
responsibility of this bottled water

00:28:40,950 --> 00:28:46,330
client application it's connecting the

00:28:43,990 --> 00:28:48,070
Postgres pulling those records out of

00:28:46,330 --> 00:28:50,620
the change stream and then it is

00:28:48,070 --> 00:28:52,960
producing two kafka so this is a client

00:28:50,620 --> 00:28:55,899
that's pulling things out of pro stress

00:28:52,960 --> 00:28:59,019
producing them to Kafka and we will

00:28:55,899 --> 00:29:01,990
start this up and we'll just let this

00:28:59,019 --> 00:29:03,759
sit there and do its thing it's it's

00:29:01,990 --> 00:29:06,009
listening for changes in Postgres as

00:29:03,759 --> 00:29:10,799
they occur we're getting just check the

00:29:06,009 --> 00:29:13,210
logs see if it looks like that works

00:29:10,799 --> 00:29:16,509
alright snapshot complete streaming

00:29:13,210 --> 00:29:19,559
changes that's good this is going better

00:29:16,509 --> 00:29:19,559
than my tests have gone

00:29:19,859 --> 00:29:27,330
all right hmm next what we're gonna do

00:29:23,470 --> 00:29:29,139
is so I said the the Kafka topics

00:29:27,330 --> 00:29:30,940
executable is something that's included

00:29:29,139 --> 00:29:33,239
with the Kafka distribution they also

00:29:30,940 --> 00:29:36,580
include this Kafka console consumer

00:29:33,239 --> 00:29:41,379
executable that you can use to consume

00:29:36,580 --> 00:29:44,080
Kafka messages on on the terminal so we

00:29:41,379 --> 00:29:45,159
are going to run this now so we're gonna

00:29:44,080 --> 00:29:47,379
clear over here

00:29:45,159 --> 00:29:49,840
I'm gonna paste that in so we're telling

00:29:47,379 --> 00:29:52,389
it I want to consume from Kafka I want

00:29:49,840 --> 00:29:56,649
to do it from the very beginning of the

00:29:52,389 --> 00:30:02,409
topic and I want to show the keys along

00:29:56,649 --> 00:30:04,599
with the messages the question yeah in

00:30:02,409 --> 00:30:08,590
essence this is a tail this is a tail

00:30:04,599 --> 00:30:10,359
minus F on on the topic except we're

00:30:08,590 --> 00:30:12,519
also making sure we're going all the way

00:30:10,359 --> 00:30:13,840
at the beginning of the file as well or

00:30:12,519 --> 00:30:15,249
the beginning of the topic in this in

00:30:13,840 --> 00:30:19,769
this case and I'm telling it that I want

00:30:15,249 --> 00:30:19,769
to connect to topic test here

00:30:23,460 --> 00:30:28,620
so you look at that so there's something

00:30:26,340 --> 00:30:31,200
on there so we did things right

00:30:28,620 --> 00:30:32,760
this was actually connected so this is

00:30:31,200 --> 00:30:37,350
reflecting this one value that's in the

00:30:32,760 --> 00:30:40,260
table so far the the format here is that

00:30:37,350 --> 00:30:42,000
this is a tab this this highlighted

00:30:40,260 --> 00:30:43,890
space right here and the format is it's

00:30:42,000 --> 00:30:46,260
showing you the key of the message then

00:30:43,890 --> 00:30:51,630
a tab and then the value of the message

00:30:46,260 --> 00:30:54,680
so you can see here the key that is put

00:30:51,630 --> 00:30:59,550
onto these kafka messages is a JSON

00:30:54,680 --> 00:31:01,890
formatted you know blob that is just

00:30:59,550 --> 00:31:04,680
representing the primary key the value

00:31:01,890 --> 00:31:07,230
of the primary key for that row so in

00:31:04,680 --> 00:31:10,070
this case ID is the primary key it's

00:31:07,230 --> 00:31:12,870
telling us the ID has this value of one

00:31:10,070 --> 00:31:14,790
this is a little bit of a weird JSON

00:31:12,870 --> 00:31:17,580
format why is there this like map inside

00:31:14,790 --> 00:31:20,040
wasn't just saying ID : one why does it

00:31:17,580 --> 00:31:22,050
also have this engine here this is

00:31:20,040 --> 00:31:25,680
because bottled water is actually using

00:31:22,050 --> 00:31:27,840
Avro which is this cool wire format that

00:31:25,680 --> 00:31:30,720
like yeah it includes a wire format you

00:31:27,840 --> 00:31:33,420
can define schemas with it but it also

00:31:30,720 --> 00:31:35,520
has a JSON representation so this is the

00:31:33,420 --> 00:31:37,260
JSON representation of an Avro record

00:31:35,520 --> 00:31:38,850
and this is the way that they decide to

00:31:37,260 --> 00:31:41,070
do that because they want to include the

00:31:38,850 --> 00:31:43,250
type information along with the value

00:31:41,070 --> 00:31:46,380
this is how they've decided to do that

00:31:43,250 --> 00:31:46,800
um the value looks very similar to the

00:31:46,380 --> 00:31:49,890
key

00:31:46,800 --> 00:31:51,960
it's that same Avro JSON format but in

00:31:49,890 --> 00:31:53,700
this case it also contains it contains

00:31:51,960 --> 00:32:00,350
all of the columns not just the primary

00:31:53,700 --> 00:32:05,520
key so let's let's write into this

00:32:00,350 --> 00:32:13,880
insert some more rows let's update a row

00:32:05,520 --> 00:32:24,540
update test set value equals modified

00:32:13,880 --> 00:32:28,710
where ID equals 1 so you see another

00:32:24,540 --> 00:32:30,600
message appeared in Kafka again it's

00:32:28,710 --> 00:32:33,330
telling us the primary key is

00:32:30,600 --> 00:32:36,929
this value one and we see this new

00:32:33,330 --> 00:32:39,929
modified value here so every insert

00:32:36,929 --> 00:32:42,840
every update every delete will show up

00:32:39,929 --> 00:32:45,990
in that I'm gonna insert one more new

00:32:42,840 --> 00:32:50,460
record in here and then I am going to

00:32:45,990 --> 00:32:53,760
close down this consumer I'm gonna start

00:32:50,460 --> 00:32:55,289
consuming from this topic again and I

00:32:53,760 --> 00:32:57,990
want you to ask yourself what do you

00:32:55,289 --> 00:32:59,610
expect to see when we connect again so

00:32:57,990 --> 00:33:02,070
I'm telling it I want to consume from

00:32:59,610 --> 00:33:03,419
the beginning of the topic we're doing

00:33:02,070 --> 00:33:05,850
all the same things the exact same

00:33:03,419 --> 00:33:07,590
command that we ran before so when we

00:33:05,850 --> 00:33:13,860
connect to the being of the topic what

00:33:07,590 --> 00:33:19,200
do you expect to see down here let's see

00:33:13,860 --> 00:33:21,809
what happens log compaction is turned on

00:33:19,200 --> 00:33:23,130
for this topic so yeah

00:33:21,809 --> 00:33:24,600
so that that's the important question I

00:33:23,130 --> 00:33:27,080
wanted to demonstrate here at log

00:33:24,600 --> 00:33:30,360
compaction at work you'll notice that

00:33:27,080 --> 00:33:33,330
this record with primary key one we see

00:33:30,360 --> 00:33:35,070
this modified value but that very first

00:33:33,330 --> 00:33:37,110
record that we wrote in it's now been

00:33:35,070 --> 00:33:39,860
cleaned up by compaction because we got

00:33:37,110 --> 00:33:42,360
this update in it had the same key as

00:33:39,860 --> 00:33:43,919
that first message that we wrote in so

00:33:42,360 --> 00:33:45,630
it became eligible for compaction and

00:33:43,919 --> 00:33:50,610
compaction ran and cleaned it up that

00:33:45,630 --> 00:33:52,020
question that is the very next thing

00:33:50,610 --> 00:33:55,530
that I'm gonna show you is what deletes

00:33:52,020 --> 00:33:57,360
look like so let's decide we really

00:33:55,530 --> 00:34:03,419
don't like this table anymore and we're

00:33:57,360 --> 00:34:06,929
just gonna delete from test so you can

00:34:03,419 --> 00:34:08,879
see there like in in Java in Java this

00:34:06,929 --> 00:34:12,090
is literally like you produce a null as

00:34:08,879 --> 00:34:14,340
the as the value of your message with a

00:34:12,090 --> 00:34:17,580
given primary key and that's used as a

00:34:14,340 --> 00:34:23,460
representation of a delete for this

00:34:17,580 --> 00:34:26,030
concept of log compaction there's a

00:34:23,460 --> 00:34:26,030
question back there

00:34:35,379 --> 00:34:40,639
so the question is is is this auditable

00:34:38,299 --> 00:34:43,069
this has been cleaned up off of Kafka at

00:34:40,639 --> 00:34:44,509
this point so if you're relying on Kafka

00:34:43,069 --> 00:34:45,049
that record doesn't exist up there

00:34:44,509 --> 00:34:47,749
anymore

00:34:45,049 --> 00:34:50,389
I will show you in a bit like in our

00:34:47,749 --> 00:34:53,089
case we have another service that is

00:34:50,389 --> 00:34:55,789
reading from from these things and it's

00:34:53,089 --> 00:34:57,259
archiving topics to s3 so we do have

00:34:55,789 --> 00:34:59,539
like a full audit history of what's

00:34:57,259 --> 00:35:01,819
happening in these topics being

00:34:59,539 --> 00:35:04,130
persistent s3 all right I'm Rios I'm

00:35:01,819 --> 00:35:05,839
running out of time here so we just did

00:35:04,130 --> 00:35:08,630
a bunch of deletes I'm gonna do one more

00:35:05,839 --> 00:35:09,859
insert I'm gonna ask you the question

00:35:08,630 --> 00:35:12,890
again we're gonna shut down this

00:35:09,859 --> 00:35:17,630
consumer and now what do you expect to

00:35:12,890 --> 00:35:20,140
see when we connect again I'm hearing

00:35:17,630 --> 00:35:24,049
somebody say they expect just the seven

00:35:20,140 --> 00:35:27,979
you are you are so right but you're so

00:35:24,049 --> 00:35:28,430
very wrong this is just another detail

00:35:27,979 --> 00:35:30,349
here

00:35:28,430 --> 00:35:33,829
those nulls all stick around this is a

00:35:30,349 --> 00:35:36,380
detail of those nodes can be considered

00:35:33,829 --> 00:35:38,269
a tombstone record and there's some

00:35:36,380 --> 00:35:40,699
details of if you have a consumer that's

00:35:38,269 --> 00:35:43,069
falling a little bit behind and a delete

00:35:40,699 --> 00:35:44,989
comes in and that delete gets cleaned up

00:35:43,069 --> 00:35:46,579
before your consumer has read the delete

00:35:44,989 --> 00:35:47,989
then it's never gonna know that that

00:35:46,579 --> 00:35:49,429
record was deleted and you can end up

00:35:47,989 --> 00:35:52,880
with a corrupted cache so now they're

00:35:49,429 --> 00:35:56,749
handed handled in a special way they are

00:35:52,880 --> 00:35:58,369
kept for seven days even if they're you

00:35:56,749 --> 00:36:01,009
know would otherwise be eligible for a

00:35:58,369 --> 00:36:02,900
compaction so that's the live demo that

00:36:01,009 --> 00:36:05,499
worked really well I'm happy thank you

00:36:02,900 --> 00:36:05,499
for being engaged

00:36:05,900 --> 00:36:15,690
always exciting all right it is it is

00:36:09,740 --> 00:36:17,100
140 so I I want to be cognizant of time

00:36:15,690 --> 00:36:19,530
I want to leave some time for questions

00:36:17,100 --> 00:36:20,670
at the end so I'm gonna try to finish up

00:36:19,530 --> 00:36:22,110
here the last thing that I want to talk

00:36:20,670 --> 00:36:25,830
about here yeah I meant to skip over

00:36:22,110 --> 00:36:28,980
these this was just in case the the demo

00:36:25,830 --> 00:36:31,220
didn't work I had some backup slides so

00:36:28,980 --> 00:36:34,380
use cases what is simple actually do

00:36:31,220 --> 00:36:36,660
with change data capture so and we we do

00:36:34,380 --> 00:36:38,370
not use bottled water we would be in

00:36:36,660 --> 00:36:40,710
kind of bad position if we had committed

00:36:38,370 --> 00:36:42,570
to bottled water at this point we we

00:36:40,710 --> 00:36:45,090
ended up writing our own Postgres to

00:36:42,570 --> 00:36:47,520
kafka change to the capture pipeline

00:36:45,090 --> 00:36:51,030
it's similar to bottled water in a lot

00:36:47,520 --> 00:36:53,190
of ways we are looking to maybe open

00:36:51,030 --> 00:36:54,630
source it at some point but we also have

00:36:53,190 --> 00:36:56,850
a blog post that goes into a lot of

00:36:54,630 --> 00:36:58,080
detail about design choices we may do

00:36:56,850 --> 00:36:59,970
they're different from from bottled

00:36:58,080 --> 00:37:02,340
water I will link you to that blog post

00:36:59,970 --> 00:37:04,050
later in here um but I want to give you

00:37:02,340 --> 00:37:05,580
a sense of the overall flow of what

00:37:04,050 --> 00:37:08,340
happens in our system and how powerful

00:37:05,580 --> 00:37:09,390
this is so you probably won't be able to

00:37:08,340 --> 00:37:10,830
read the text on here they're like

00:37:09,390 --> 00:37:13,080
really tiny labels I will tell you

00:37:10,830 --> 00:37:15,150
what's going on we have a micro services

00:37:13,080 --> 00:37:16,350
architecture so the fundamental unit

00:37:15,150 --> 00:37:18,720
looks kind of like that thing that you

00:37:16,350 --> 00:37:21,120
see there where the orange part is a

00:37:18,720 --> 00:37:22,760
service that could be multiple instances

00:37:21,120 --> 00:37:25,740
all running the same application and

00:37:22,760 --> 00:37:28,550
they're all talking to a single Postgres

00:37:25,740 --> 00:37:30,900
database that is just for that service

00:37:28,550 --> 00:37:35,220
in this case this is our transaction

00:37:30,900 --> 00:37:37,050
service we have we have a backing bank

00:37:35,220 --> 00:37:39,150
that actually holds accounts when

00:37:37,050 --> 00:37:40,350
transactions happen they get sent to us

00:37:39,150 --> 00:37:42,170
over a message queue and this

00:37:40,350 --> 00:37:44,340
transaction services what does our

00:37:42,170 --> 00:37:48,570
bookkeeping about what's going on with

00:37:44,340 --> 00:37:52,050
transactions that Postgres instance has

00:37:48,570 --> 00:37:54,210
a kind of Co service sitting next to it

00:37:52,050 --> 00:37:58,200
on another instance again this is all in

00:37:54,210 --> 00:38:00,420
AWS we call it PG Kafka as the name of

00:37:58,200 --> 00:38:03,090
our change data capture pipeline rather

00:38:00,420 --> 00:38:04,920
than bottled water so that Postgres

00:38:03,090 --> 00:38:07,710
instance has an instance of PG Kafka

00:38:04,920 --> 00:38:09,690
that is subscribed to a logical

00:38:07,710 --> 00:38:12,720
replication slot it's sending all of

00:38:09,690 --> 00:38:14,250
those records to to Kafka one of the

00:38:12,720 --> 00:38:17,010
most important design decisions that's

00:38:14,250 --> 00:38:18,660
different from bottled water is that so

00:38:17,010 --> 00:38:19,510
if you think about the the right ahead

00:38:18,660 --> 00:38:21,610
log

00:38:19,510 --> 00:38:24,220
Postgres there's just one right ahead

00:38:21,610 --> 00:38:26,440
log for the whole instance you know it

00:38:24,220 --> 00:38:29,800
each record would be like I'm inserting

00:38:26,440 --> 00:38:31,060
a record into table X with value Y the

00:38:29,800 --> 00:38:35,830
next record could be I'm inserting into

00:38:31,060 --> 00:38:38,170
table Z with value 92 and we've decided

00:38:35,830 --> 00:38:40,060
to model that directly in Kafka so we

00:38:38,170 --> 00:38:42,430
just write to one topic for the entire

00:38:40,060 --> 00:38:45,880
instance that's multiplexed all of those

00:38:42,430 --> 00:38:47,380
different tables together which has some

00:38:45,880 --> 00:38:49,000
advantages it also has some

00:38:47,380 --> 00:38:51,220
disadvantages sometimes you really only

00:38:49,000 --> 00:38:52,780
want to know about that one table so one

00:38:51,220 --> 00:38:54,960
thing that we've done is we have a

00:38:52,780 --> 00:38:57,910
service that we call D MUX our

00:38:54,960 --> 00:39:01,630
demultiplexer that consumes from that

00:38:57,910 --> 00:39:03,670
topic and writes out records per table

00:39:01,630 --> 00:39:06,400
to a bunch of other topics so this takes

00:39:03,670 --> 00:39:08,230
in the transactions PG Kafka topic it

00:39:06,400 --> 00:39:11,410
writes out just the transactions table

00:39:08,230 --> 00:39:13,690
to one topic and the customers table to

00:39:11,410 --> 00:39:16,690
another topic then we have another

00:39:13,690 --> 00:39:17,650
service our user activity service that

00:39:16,690 --> 00:39:18,940
cares about knowing about these

00:39:17,650 --> 00:39:21,070
transactions coming in so when you open

00:39:18,940 --> 00:39:22,600
up the simple app the main page that you

00:39:21,070 --> 00:39:24,490
see is these are all your recent

00:39:22,600 --> 00:39:26,890
transactions the waiter that finds out

00:39:24,490 --> 00:39:30,820
about that is by consuming from this

00:39:26,890 --> 00:39:33,010
transactions table topic in Kafka it in

00:39:30,820 --> 00:39:36,010
just those messages it writes those to

00:39:33,010 --> 00:39:37,930
its own database that database also has

00:39:36,010 --> 00:39:41,170
an instance of PG Kafka and those

00:39:37,930 --> 00:39:43,720
changes are getting sent to to Kafka in

00:39:41,170 --> 00:39:46,000
a separate topic in a different format

00:39:43,720 --> 00:39:48,280
that's appropriate for that service and

00:39:46,000 --> 00:39:50,020
while this is all going on and we have a

00:39:48,280 --> 00:39:52,660
whole analytics infrastructure that is

00:39:50,020 --> 00:39:55,960
reading from all of these PG Kafka

00:39:52,660 --> 00:39:58,720
produced two topics its archiving them

00:39:55,960 --> 00:40:01,270
to Amazon s3 so we have a kind of source

00:39:58,720 --> 00:40:03,070
of truth of what happened to go back to

00:40:01,270 --> 00:40:04,210
we're also putting those into Amazon

00:40:03,070 --> 00:40:08,320
redshift which is our data warehouse

00:40:04,210 --> 00:40:11,440
that we use and that's the basis of like

00:40:08,320 --> 00:40:15,520
75% of what our business intelligence

00:40:11,440 --> 00:40:16,810
and data scientists users you know

00:40:15,520 --> 00:40:20,140
they're using this data that is derived

00:40:16,810 --> 00:40:23,530
from this whole pipeline from from

00:40:20,140 --> 00:40:25,150
Postgres so just high-level what are the

00:40:23,530 --> 00:40:26,920
pieces here there's that core change

00:40:25,150 --> 00:40:29,500
data capture pipeline that part that's

00:40:26,920 --> 00:40:32,250
actually you know an application that is

00:40:29,500 --> 00:40:36,260
reading from Postgres on this lot

00:40:32,250 --> 00:40:40,380
decoding slot and is pushing the

00:40:36,260 --> 00:40:42,359
producing the messages to Kafka this is

00:40:40,380 --> 00:40:44,810
a form of messaging between services if

00:40:42,359 --> 00:40:47,430
you'll notice the transaction service

00:40:44,810 --> 00:40:49,380
doesn't know anything about Kafka at all

00:40:47,430 --> 00:40:52,500
it's just writing things to its own

00:40:49,380 --> 00:40:54,359
database for its own purposes and it's a

00:40:52,500 --> 00:40:56,369
separate process that is consuming those

00:40:54,359 --> 00:40:57,990
changes in writing it - Kafka so it's

00:40:56,369 --> 00:41:00,510
pretty cool that this user activity

00:40:57,990 --> 00:41:01,859
service can consume from that without

00:41:00,510 --> 00:41:03,690
the transaction service ever having to

00:41:01,859 --> 00:41:05,490
worry about Kafka and this is in

00:41:03,690 --> 00:41:07,500
practice this is how services do

00:41:05,490 --> 00:41:09,060
asynchronous messages messaging in our

00:41:07,500 --> 00:41:10,800
infrastructure this is how you know

00:41:09,060 --> 00:41:13,830
about what some of the services doing is

00:41:10,800 --> 00:41:15,960
you subscribe to its log of changes in

00:41:13,830 --> 00:41:18,410
the database and then finally our

00:41:15,960 --> 00:41:21,330
analytics is is powered by this

00:41:18,410 --> 00:41:22,740
so to recap commit logs they show up all

00:41:21,330 --> 00:41:25,940
over the place they are really central

00:41:22,740 --> 00:41:29,220
to how Postgres and other databases

00:41:25,940 --> 00:41:32,339
maintain durability MySQL has something

00:41:29,220 --> 00:41:34,740
called the bin log which is analogous to

00:41:32,339 --> 00:41:37,109
the wall and the bin log is also

00:41:34,740 --> 00:41:39,660
available for doing the sort of logical

00:41:37,109 --> 00:41:42,839
decoding thing and there are solutions

00:41:39,660 --> 00:41:44,760
for MySQL as well Kafka

00:41:42,839 --> 00:41:47,220
it's cool it's really commit logs as the

00:41:44,760 --> 00:41:49,320
data store distributed and it can handle

00:41:47,220 --> 00:41:52,080
huge amounts of data if you have a need

00:41:49,320 --> 00:41:53,760
for that we saw bottled water and we

00:41:52,080 --> 00:41:56,609
talked a bit about how simple uses this

00:41:53,760 --> 00:42:00,210
so I want to leave you with some

00:41:56,609 --> 00:42:01,230
resources if you actually want to do

00:42:00,210 --> 00:42:04,680
something with this or you want to learn

00:42:01,230 --> 00:42:07,500
more about the concept again the other

00:42:04,680 --> 00:42:09,109
hairy details about you know how simple

00:42:07,500 --> 00:42:11,730
has decided to implement this pipeline

00:42:09,109 --> 00:42:13,349
comparing that to bottled water is in a

00:42:11,730 --> 00:42:17,160
blog post on simple accomplished

00:42:13,349 --> 00:42:20,310
engineering bottled water itself looking

00:42:17,160 --> 00:42:22,890
at the repo and there's a blog post that

00:42:20,310 --> 00:42:26,609
Martin clapman wrote for confluent which

00:42:22,890 --> 00:42:29,430
is it's really fabulous kind of overview

00:42:26,609 --> 00:42:31,140
of the of the concepts involved if you

00:42:29,430 --> 00:42:33,900
actually want to implement this division

00:42:31,140 --> 00:42:36,660
as the project to look at it is a change

00:42:33,900 --> 00:42:38,640
data capture pipeline to Kafka from

00:42:36,660 --> 00:42:41,170
multiple sources so it has connectors

00:42:38,640 --> 00:42:43,780
for Postgres MySQL and

00:42:41,170 --> 00:42:46,599
and it's something that there are actual

00:42:43,780 --> 00:42:48,700
people that use in this world we pay in

00:42:46,599 --> 00:42:51,430
particular wrote a blog post about their

00:42:48,700 --> 00:42:55,150
MySQL - Kafka pipeline using - Museum

00:42:51,430 --> 00:42:57,580
and they also gave a talk at the Kafka

00:42:55,150 --> 00:42:59,440
Summit this summer and those are great

00:42:57,580 --> 00:43:02,230
resources as well

00:42:59,440 --> 00:43:04,869
martin Clubman i mentioned him earlier

00:43:02,230 --> 00:43:07,210
he's the bottled water author everything

00:43:04,869 --> 00:43:08,830
he writes I love but the making sense of

00:43:07,210 --> 00:43:12,190
stream processing he book is available

00:43:08,830 --> 00:43:13,570
for free and goes into a lot of kind of

00:43:12,190 --> 00:43:20,470
theoretical detail and all of these

00:43:13,570 --> 00:43:23,220
concepts as well so we can go ahead and

00:43:20,470 --> 00:43:27,160
do some final questions before we close

00:43:23,220 --> 00:43:42,250
all right I am okay I am going to stay

00:43:27,160 --> 00:43:43,450
in the front yes yes okay so the

00:43:42,250 --> 00:43:45,220
question is it seems a little bit

00:43:43,450 --> 00:43:47,530
strange that we're taking records from a

00:43:45,220 --> 00:43:49,119
database they're going through Kafka

00:43:47,530 --> 00:43:49,869
they're ending up in another service and

00:43:49,119 --> 00:43:51,220
they're getting written to the database

00:43:49,869 --> 00:43:55,150
again

00:43:51,220 --> 00:43:57,130
so load wise like we're actually we

00:43:55,150 --> 00:43:59,589
don't have very high throughput at least

00:43:57,130 --> 00:44:01,599
in Kafka terms our throughput is is not

00:43:59,589 --> 00:44:05,890
really problem and we've been easily

00:44:01,599 --> 00:44:07,900
able to handle that and the you know the

00:44:05,890 --> 00:44:11,020
the decoupling that that allows between

00:44:07,900 --> 00:44:13,530
the different services has been has been

00:44:11,020 --> 00:44:17,859
pretty nice keeping track of schemas is

00:44:13,530 --> 00:44:19,839
not a so easy JSON doesn't give you a

00:44:17,859 --> 00:44:22,060
whole lot of help in that so like our

00:44:19,839 --> 00:44:25,480
pipeline that we use internally it is it

00:44:22,060 --> 00:44:28,060
is JSON based schemas are kind of inside

00:44:25,480 --> 00:44:29,470
the message but there's no central

00:44:28,060 --> 00:44:30,640
registry and that is that's another

00:44:29,470 --> 00:44:32,109
piece of bottled water they didn't talk

00:44:30,640 --> 00:44:34,839
about there is this schema registry that

00:44:32,109 --> 00:44:36,609
confluent provides Avro has this whole

00:44:34,839 --> 00:44:39,130
concept of you write your schema to the

00:44:36,609 --> 00:44:40,390
schema registry the schema registry will

00:44:39,130 --> 00:44:42,580
yell at you if you tried to write

00:44:40,390 --> 00:44:44,950
records that don't fit that schema and

00:44:42,580 --> 00:44:47,380
schema management is is a problem that

00:44:44,950 --> 00:44:49,150
we grapple with sometimes and we don't

00:44:47,380 --> 00:44:50,349
we haven't totally figured it out but I

00:44:49,150 --> 00:44:52,510
would love to get to a point where we're

00:44:50,349 --> 00:44:53,560
using something like the schema registry

00:44:52,510 --> 00:44:55,270
for

00:44:53,560 --> 00:44:56,950
for doing one event management of what's

00:44:55,270 --> 00:45:11,530
happening in different services and and

00:44:56,950 --> 00:45:13,630
documenting it hopefully yeah so what

00:45:11,530 --> 00:45:15,760
adventures do we have doing this thing

00:45:13,630 --> 00:45:18,010
rather than going you're directly from

00:45:15,760 --> 00:45:21,670
one Postgres to another Postgres using

00:45:18,010 --> 00:45:23,200
wall so one thing okay so transmitting

00:45:21,670 --> 00:45:24,880
wall from one Postgres server to another

00:45:23,200 --> 00:45:27,490
Postgres server is known as physical

00:45:24,880 --> 00:45:30,340
replication that is something that's an

00:45:27,490 --> 00:45:31,540
all-or-nothing concept of like you can

00:45:30,340 --> 00:45:33,490
use that to create a replica of the

00:45:31,540 --> 00:45:34,900
database but you have no control over if

00:45:33,490 --> 00:45:36,940
you only want this one table to appear

00:45:34,900 --> 00:45:38,860
somewhere else you can't do that with

00:45:36,940 --> 00:45:40,690
physical decoding one of the new

00:45:38,860 --> 00:45:42,010
features in Postgres 10 that either just

00:45:40,690 --> 00:45:44,680
got released or is getting released in

00:45:42,010 --> 00:45:48,010
like the next couple of days is called

00:45:44,680 --> 00:45:50,230
logical replication where you can where

00:45:48,010 --> 00:45:51,610
it's using this logical decoding new

00:45:50,230 --> 00:45:52,960
thing of putting things to a logical

00:45:51,610 --> 00:45:55,180
format you can say I just want this one

00:45:52,960 --> 00:45:57,310
table to be sent over the wire to this

00:45:55,180 --> 00:45:59,440
other Postgres instance and yo and have

00:45:57,310 --> 00:46:02,710
the table appear there so that that's an

00:45:59,440 --> 00:46:04,390
option that we could explore I think

00:46:02,710 --> 00:46:06,610
like the flexibility of just sending

00:46:04,390 --> 00:46:08,470
everything to Kafka not worrying about

00:46:06,610 --> 00:46:11,980
the schema on the other side at the time

00:46:08,470 --> 00:46:14,820
that we're writing it has been has been

00:46:11,980 --> 00:46:22,060
pretty flexible and beneficial for us I

00:46:14,820 --> 00:46:25,200
have a question in the front here sure

00:46:22,060 --> 00:46:25,200
we're gonna grow

00:46:37,350 --> 00:46:43,240
okay so yeah the question is as you grow

00:46:40,500 --> 00:46:45,640
what what happens like you've decided to

00:46:43,240 --> 00:46:47,710
lay out your data the way it is how do

00:46:45,640 --> 00:46:48,160
you manage adding additional brokers to

00:46:47,710 --> 00:46:51,820
your cluster

00:46:48,160 --> 00:46:53,770
so in our case we actually we're doing

00:46:51,820 --> 00:46:56,500
like a super simple thing of like each

00:46:53,770 --> 00:46:59,020
of our almost all of our topics are just

00:46:56,500 --> 00:47:00,610
one partition and we don't do any like

00:46:59,020 --> 00:47:05,170
partitioning between different brokers

00:47:00,610 --> 00:47:09,190
that obviously won't last forever but if

00:47:05,170 --> 00:47:10,990
you do it's really hard to increase the

00:47:09,190 --> 00:47:12,850
number of partitions on a topic in fact

00:47:10,990 --> 00:47:13,960
with Kafka it's impossible to increase

00:47:12,850 --> 00:47:15,880
the number of partitions that are

00:47:13,960 --> 00:47:17,350
assigned to a topic so if you get to the

00:47:15,880 --> 00:47:19,120
point of like I need more partitions you

00:47:17,350 --> 00:47:19,600
have to you have to just create a new

00:47:19,120 --> 00:47:21,910
topic

00:47:19,600 --> 00:47:23,320
so that's best practices is if you do

00:47:21,910 --> 00:47:25,840
want to do this thing of having multiple

00:47:23,320 --> 00:47:26,920
partitions in a topic create way more of

00:47:25,840 --> 00:47:29,500
them than you ever think you're gonna

00:47:26,920 --> 00:47:32,200
need and then it is really easy as you

00:47:29,500 --> 00:47:35,050
add new brokers to your cluster it's

00:47:32,200 --> 00:47:38,380
pretty easy to rebalance the partitions

00:47:35,050 --> 00:47:39,850
across more brokers as long as you had

00:47:38,380 --> 00:47:43,060
it split out to a bunch of partitions in

00:47:39,850 --> 00:47:45,280
the first place and Kafka liked topics

00:47:43,060 --> 00:47:47,410
and partitions are super cheap for Kafka

00:47:45,280 --> 00:47:49,570
so it's totally feasible to have many

00:47:47,410 --> 00:47:51,520
hundreds or thousands or tens of

00:47:49,570 --> 00:47:52,840
thousands of partitions so you can

00:47:51,520 --> 00:47:56,200
create a topic with one hundred

00:47:52,840 --> 00:47:57,400
partitions and you'll really only care

00:47:56,200 --> 00:47:59,530
about like you only have two instances

00:47:57,400 --> 00:48:01,360
of your app so they're each consuming 50

00:47:59,530 --> 00:48:02,500
partitions it seems a little silly but

00:48:01,360 --> 00:48:05,980
there's nothing there's really no

00:48:02,500 --> 00:48:06,880
overhead to doing that and Sam so best

00:48:05,980 --> 00:48:08,020
practice is to create

00:48:06,880 --> 00:48:10,840
way more partitions than you think

00:48:08,020 --> 00:48:12,400
you're going to need I will take one

00:48:10,840 --> 00:48:14,580
more question if there is one in the

00:48:12,400 --> 00:48:14,580
back

00:48:20,970 --> 00:48:26,470
yeah the question is is compaction

00:48:23,560 --> 00:48:28,780
guaranteed like for the next read that

00:48:26,470 --> 00:48:30,210
you won't see those old messages no it's

00:48:28,780 --> 00:48:34,420
something that happens in the background

00:48:30,210 --> 00:48:36,580
periodically and you know if they're a

00:48:34,420 --> 00:48:42,220
bunch of tuna bulls as far as like how

00:48:36,580 --> 00:48:43,870
often it runs yeah I did so when I

00:48:42,220 --> 00:48:46,420
created the topic I set a bunch of

00:48:43,870 --> 00:48:49,000
parameters on there that said like hey

00:48:46,420 --> 00:48:51,310
run the like items are eligible for

00:48:49,000 --> 00:48:52,810
compaction within 100 milliseconds or

00:48:51,310 --> 00:48:53,830
whatever and the default for that is

00:48:52,810 --> 00:48:56,080
that I think that they're not eligible

00:48:53,830 --> 00:48:58,990
for compaction for like 8 hours or

00:48:56,080 --> 00:49:00,790
something on that burger so if ya if you

00:48:58,990 --> 00:49:03,610
have consumers that are within a couple

00:49:00,790 --> 00:49:05,380
of hours of real time then they

00:49:03,610 --> 00:49:06,820
generally will never see the effects of

00:49:05,380 --> 00:49:08,980
compaction and they're gonna see the

00:49:06,820 --> 00:49:11,030
whole list of changes as they come

00:49:08,980 --> 00:49:21,429
through all right thank you so much

00:49:11,030 --> 00:49:21,429

YouTube URL: https://www.youtube.com/watch?v=ehAfosAyHsw


