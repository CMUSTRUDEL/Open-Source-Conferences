Title: Zac Burns - Everything is serialization â€” RustFest Global 2020
Publication date: 2020-12-19
Playlist: RustFest Global 2020
Description: 
	Let's peek under the hood of serialization formats and see how properties inherent to the data representations themselves will either help or hinder us for a given problem.

Learn what to consider when writing your own formats by looking inside some of the best.

More at https://rustfest.global/session/9-everything-is-serialization/
Captions: 
	00:00:02,879 --> 00:00:07,359
zach burns

00:00:03,840 --> 00:00:09,120
wants to serialize some ideas that we

00:00:07,359 --> 00:00:12,240
all won't despise

00:00:09,120 --> 00:00:12,960
into one talk to make us see what it

00:00:12,240 --> 00:00:17,920
will take

00:00:12,960 --> 00:00:17,920
to make code easier to realize

00:00:18,160 --> 00:00:25,039
your computer by itself doesn't do

00:00:21,520 --> 00:00:27,760
anything of value here's a picture of

00:00:25,039 --> 00:00:30,080
the inside of a computer

00:00:27,760 --> 00:00:31,599
you can't tell from the picture what the

00:00:30,080 --> 00:00:34,719
computer is doing

00:00:31,599 --> 00:00:38,000
if it is doing anything at all

00:00:34,719 --> 00:00:42,480
for the computer to be useful it must be

00:00:38,000 --> 00:00:44,480
a component connected to a larger system

00:00:42,480 --> 00:00:45,520
the system that the computer is a part

00:00:44,480 --> 00:00:47,360
of includes

00:00:45,520 --> 00:00:48,559
other components attached to the

00:00:47,360 --> 00:00:52,079
computer

00:00:48,559 --> 00:00:55,680
components like mice keyboards speakers

00:00:52,079 --> 00:00:58,480
screens and network cards send data

00:00:55,680 --> 00:01:01,440
to and from the computer through wires

00:00:58,480 --> 00:01:04,320
like in this picture

00:01:01,440 --> 00:01:05,119
because of these wires and the physical

00:01:04,320 --> 00:01:07,840
separation

00:01:05,119 --> 00:01:08,720
of the system's components the data

00:01:07,840 --> 00:01:12,159
which drives

00:01:08,720 --> 00:01:15,600
each component must be in well-specified

00:01:12,159 --> 00:01:17,280
agreed upon formats at this level of

00:01:15,600 --> 00:01:19,759
abstraction of the system

00:01:17,280 --> 00:01:21,680
we usually think of the data in terms of

00:01:19,759 --> 00:01:24,080
serialization

00:01:21,680 --> 00:01:26,000
serialization at this level includes

00:01:24,080 --> 00:01:29,280
many well-known formats

00:01:26,000 --> 00:01:32,159
mp3 json http

00:01:29,280 --> 00:01:34,240
among others if we break down this

00:01:32,159 --> 00:01:36,880
high-level problem further and look

00:01:34,240 --> 00:01:38,479
inside the computer we see the same

00:01:36,880 --> 00:01:41,439
setup on the inside

00:01:38,479 --> 00:01:42,320
that we saw on the outside here is a

00:01:41,439 --> 00:01:44,799
picture of the

00:01:42,320 --> 00:01:46,880
inside of a computer subsystem which

00:01:44,799 --> 00:01:50,159
comprises several components

00:01:46,880 --> 00:01:52,840
each component driven by data sent

00:01:50,159 --> 00:01:54,000
over the wires that connect the

00:01:52,840 --> 00:01:57,280
components

00:01:54,000 --> 00:01:59,840
the cpu gpu ram and hard drive are

00:01:57,280 --> 00:02:00,719
all data driven components and

00:01:59,840 --> 00:02:03,439
subsystems

00:02:00,719 --> 00:02:04,719
unto themselves we don't always think of

00:02:03,439 --> 00:02:07,439
the things happening

00:02:04,719 --> 00:02:08,800
at this level of abstraction in terms of

00:02:07,439 --> 00:02:12,000
serialization

00:02:08,800 --> 00:02:14,959
but it is serialization just the same

00:02:12,000 --> 00:02:16,000
here too the physical separation of each

00:02:14,959 --> 00:02:18,400
component

00:02:16,000 --> 00:02:20,640
aside from the wires connecting them

00:02:18,400 --> 00:02:23,280
necessitates that the data which

00:02:20,640 --> 00:02:27,200
drives each component to be serialized

00:02:23,280 --> 00:02:29,760
in well-specified agreed upon formats

00:02:27,200 --> 00:02:30,879
file system formats like ntfs are

00:02:29,760 --> 00:02:33,360
serialized

00:02:30,879 --> 00:02:34,879
by the cpu and sent to the hard drive

00:02:33,360 --> 00:02:37,280
for safekeeping

00:02:34,879 --> 00:02:39,760
data is serialized in vertex buffers to

00:02:37,280 --> 00:02:42,640
be sent to the gpu for drawing

00:02:39,760 --> 00:02:43,440
when data is fetched from ram that fetch

00:02:42,640 --> 00:02:46,720
instruction

00:02:43,440 --> 00:02:51,280
is bytes in a serialization format sent

00:02:46,720 --> 00:02:53,440
over wires between the cpu and ram

00:02:51,280 --> 00:02:55,360
instructions sent to the cpu are

00:02:53,440 --> 00:02:59,200
serialized machine code

00:02:55,360 --> 00:03:02,400
which came from serialized x64 assembly

00:02:59,200 --> 00:03:06,159
which came from serialized miri which

00:03:02,400 --> 00:03:08,800
came from serialized rust source files

00:03:06,159 --> 00:03:09,840
which came from serialized keyboard

00:03:08,800 --> 00:03:12,800
presses

00:03:09,840 --> 00:03:13,920
and so on if you look into any of these

00:03:12,800 --> 00:03:17,519
subsystems

00:03:13,920 --> 00:03:20,800
ram cpu gpu network card

00:03:17,519 --> 00:03:21,920
you'll find the exact same setup data

00:03:20,800 --> 00:03:25,200
driven components

00:03:21,920 --> 00:03:26,959
connected by wires we can take the cpu

00:03:25,200 --> 00:03:28,640
for example and see what it looks like

00:03:26,959 --> 00:03:30,480
on the inside

00:03:28,640 --> 00:03:32,799
here's a picture of a cpu with

00:03:30,480 --> 00:03:35,840
components for instruction decoding

00:03:32,799 --> 00:03:37,040
branch prediction caches schedulers and

00:03:35,840 --> 00:03:39,440
so on

00:03:37,040 --> 00:03:40,400
each component in this system is data

00:03:39,440 --> 00:03:43,200
driven

00:03:40,400 --> 00:03:44,799
and is connected by wires which transfer

00:03:43,200 --> 00:03:47,760
data to other components

00:03:44,799 --> 00:03:48,720
and well specified predefined

00:03:47,760 --> 00:03:52,000
purpose-built

00:03:48,720 --> 00:03:53,519
serialization formats at this level

00:03:52,000 --> 00:03:55,360
we're thinking about smaller

00:03:53,519 --> 00:03:57,360
serialization formats like the little

00:03:55,360 --> 00:03:59,680
endian format for integers

00:03:57,360 --> 00:04:00,720
instruction sets addresses floating

00:03:59,680 --> 00:04:04,000
point opcodes

00:04:00,720 --> 00:04:06,080
and many others at each

00:04:04,000 --> 00:04:07,200
level of abstraction of the computer

00:04:06,080 --> 00:04:10,239
system you will find

00:04:07,200 --> 00:04:11,120
components driven by data sent over

00:04:10,239 --> 00:04:14,319
wires

00:04:11,120 --> 00:04:15,120
in serialization formats unsurprisingly

00:04:14,319 --> 00:04:17,600
the design

00:04:15,120 --> 00:04:18,320
of the serialization formats which is

00:04:17,600 --> 00:04:20,880
the design

00:04:18,320 --> 00:04:21,919
of how the components interact has a

00:04:20,880 --> 00:04:25,680
large effect

00:04:21,919 --> 00:04:28,400
on the system as a whole maybe you feel

00:04:25,680 --> 00:04:30,560
this characterization of the computer as

00:04:28,400 --> 00:04:32,080
driven by serialization to be

00:04:30,560 --> 00:04:33,680
reductionist

00:04:32,080 --> 00:04:35,440
maybe you prefer to think in

00:04:33,680 --> 00:04:37,840
abstractions

00:04:35,440 --> 00:04:39,440
i'd like to point out that abstractions

00:04:37,840 --> 00:04:42,880
cannot be implemented

00:04:39,440 --> 00:04:44,560
without the use of serialization perhaps

00:04:42,880 --> 00:04:47,520
the greatest abstraction of

00:04:44,560 --> 00:04:50,000
all time is the function call what

00:04:47,520 --> 00:04:51,759
happens when you call a function

00:04:50,000 --> 00:04:53,199
the first thing that happens is the

00:04:51,759 --> 00:04:56,080
arguments to the function

00:04:53,199 --> 00:04:57,840
are serialized to the stack the order

00:04:56,080 --> 00:05:00,479
and layout of the arguments or

00:04:57,840 --> 00:05:03,039
the file format if you will is called

00:05:00,479 --> 00:05:04,800
the calling convention

00:05:03,039 --> 00:05:07,199
maybe you don't like the things in terms

00:05:04,800 --> 00:05:09,520
of implementing abstractions

00:05:07,199 --> 00:05:13,360
perhaps you think in high-level tasks

00:05:09,520 --> 00:05:16,000
like serving an http request

00:05:13,360 --> 00:05:16,560
high-level tasks are all described in

00:05:16,000 --> 00:05:19,759
terms

00:05:16,560 --> 00:05:22,400
first of parsing data in this case a url

00:05:19,759 --> 00:05:23,440
which is a standard serialization format

00:05:22,400 --> 00:05:25,759
having a host

00:05:23,440 --> 00:05:26,960
a path and other data followed by a

00:05:25,759 --> 00:05:29,759
transform

00:05:26,960 --> 00:05:32,400
and lastly a serialization in this case

00:05:29,759 --> 00:05:35,199
an http response

00:05:32,400 --> 00:05:36,479
there are two serialization steps parse

00:05:35,199 --> 00:05:39,840
and serialize

00:05:36,479 --> 00:05:41,440
book ending every transform step it may

00:05:39,840 --> 00:05:42,800
seem that i'm overlooking the most

00:05:41,440 --> 00:05:44,960
significant step

00:05:42,800 --> 00:05:47,840
the transform step but if we were to

00:05:44,960 --> 00:05:50,160
look at what the transform step entails

00:05:47,840 --> 00:05:52,240
we would see that it breaks down further

00:05:50,160 --> 00:05:55,120
into a series of parse

00:05:52,240 --> 00:05:56,479
transform and serialized steps it's

00:05:55,120 --> 00:05:59,600
serialization

00:05:56,479 --> 00:06:02,080
all the way down your database is just a

00:05:59,600 --> 00:06:04,560
giant serialized file after all

00:06:02,080 --> 00:06:05,520
as are the requests for the data in the

00:06:04,560 --> 00:06:08,800
database

00:06:05,520 --> 00:06:10,000
in fact all of the state of your entire

00:06:08,800 --> 00:06:13,039
running program

00:06:10,000 --> 00:06:16,479
is stored in memory in a complex

00:06:13,039 --> 00:06:20,880
serialization format comprised of other

00:06:16,479 --> 00:06:22,400
nested simpler serialization formats

00:06:20,880 --> 00:06:24,000
the first point that i'm trying to make

00:06:22,400 --> 00:06:26,800
here today is that at

00:06:24,000 --> 00:06:27,440
every level serialization doesn't just

00:06:26,800 --> 00:06:30,479
underlie

00:06:27,440 --> 00:06:32,960
everything we do but is in some sense

00:06:30,479 --> 00:06:34,240
both the means and the ends of all

00:06:32,960 --> 00:06:36,800
programs

00:06:34,240 --> 00:06:39,280
as such serialization should be at the

00:06:36,800 --> 00:06:42,000
front of our minds when engineering

00:06:39,280 --> 00:06:43,360
and yet despite the centrality of

00:06:42,000 --> 00:06:46,400
serialization

00:06:43,360 --> 00:06:49,759
how to represent data as bytes is not a

00:06:46,400 --> 00:06:52,240
hot topic in programming circles

00:06:49,759 --> 00:06:53,919
there is the occasional flame war about

00:06:52,240 --> 00:06:55,120
whether it's better to have a human

00:06:53,919 --> 00:06:58,240
readable format

00:06:55,120 --> 00:07:01,280
like json or a high performance format

00:06:58,240 --> 00:07:03,440
with a schema like protobuf

00:07:01,280 --> 00:07:04,479
but the trade-off space goes much deeper

00:07:03,440 --> 00:07:06,720
than that

00:07:04,479 --> 00:07:09,039
my experience has been that the choice

00:07:06,720 --> 00:07:11,280
of representation of the data

00:07:09,039 --> 00:07:13,039
is an essential factor in determining

00:07:11,280 --> 00:07:14,720
the system's performance

00:07:13,039 --> 00:07:16,319
the engineering effort required to

00:07:14,720 --> 00:07:19,520
produce the system

00:07:16,319 --> 00:07:20,960
and the system's capabilities as a whole

00:07:19,520 --> 00:07:24,479
the reasons for this are easy to

00:07:20,960 --> 00:07:26,639
overlook so i'll go over each

00:07:24,479 --> 00:07:27,599
the first statement was that the data

00:07:26,639 --> 00:07:29,680
representation

00:07:27,599 --> 00:07:31,840
is an essential factor in determining

00:07:29,680 --> 00:07:33,840
the system's performance

00:07:31,840 --> 00:07:36,080
this comes down to the limits of each

00:07:33,840 --> 00:07:38,800
component in the system to produce

00:07:36,080 --> 00:07:41,840
and consume data and the limits of the

00:07:38,800 --> 00:07:43,919
wires connecting these components

00:07:41,840 --> 00:07:44,879
if the serialization format has low

00:07:43,919 --> 00:07:47,120
entropy

00:07:44,879 --> 00:07:48,000
the throughput of data flowing through

00:07:47,120 --> 00:07:50,080
the system

00:07:48,000 --> 00:07:51,280
is limited by the wires connecting

00:07:50,080 --> 00:07:53,360
components

00:07:51,280 --> 00:07:55,199
put another way bloat in the

00:07:53,360 --> 00:07:59,280
representation of the data

00:07:55,199 --> 00:08:01,440
throttles the throughput of information

00:07:59,280 --> 00:08:02,720
also data dependencies in the

00:08:01,440 --> 00:08:04,560
serialization format

00:08:02,720 --> 00:08:07,120
pause the flow of data through the

00:08:04,560 --> 00:08:10,560
system incurring the latency cost

00:08:07,120 --> 00:08:12,960
of the wires the system operates at

00:08:10,560 --> 00:08:14,240
peak efficiency only when the

00:08:12,960 --> 00:08:17,440
communication between

00:08:14,240 --> 00:08:20,479
components is saturated with efficiently

00:08:17,440 --> 00:08:20,479
represented data

00:08:20,800 --> 00:08:24,080
the second point is that data

00:08:23,120 --> 00:08:26,400
representation

00:08:24,080 --> 00:08:27,440
is an essential factor in determining

00:08:26,400 --> 00:08:30,639
the engineering

00:08:27,440 --> 00:08:32,399
effort required to produce the system

00:08:30,639 --> 00:08:34,800
given input and output data

00:08:32,399 --> 00:08:36,000
representations in an algorithm sitting

00:08:34,800 --> 00:08:38,279
between

00:08:36,000 --> 00:08:39,519
a change to either representation

00:08:38,279 --> 00:08:42,399
necessitates

00:08:39,519 --> 00:08:45,760
a corresponding change to the algorithm

00:08:42,399 --> 00:08:48,160
note that the inverse is not always true

00:08:45,760 --> 00:08:49,760
a change in the algorithm does not

00:08:48,160 --> 00:08:52,720
necessarily require

00:08:49,760 --> 00:08:55,440
a change to the data the algorithms

00:08:52,720 --> 00:08:57,200
available to us and their limitations

00:08:55,440 --> 00:08:58,880
including the algorithm's minimum

00:08:57,200 --> 00:09:01,200
possible complexity

00:08:58,880 --> 00:09:02,880
are determined by the characteristics

00:09:01,200 --> 00:09:06,320
and representations

00:09:02,880 --> 00:09:08,399
of the data the third point

00:09:06,320 --> 00:09:09,760
was that data representation is an

00:09:08,399 --> 00:09:12,000
essential factor

00:09:09,760 --> 00:09:13,600
in determining the system's capabilities

00:09:12,000 --> 00:09:16,160
as a whole

00:09:13,600 --> 00:09:18,880
given a representation and time limit

00:09:16,160 --> 00:09:22,080
the set of inputs and calculated outputs

00:09:18,880 --> 00:09:24,800
expressed within those bounds is finite

00:09:22,080 --> 00:09:25,760
that finite set of inputs and outputs is

00:09:24,800 --> 00:09:28,880
the total

00:09:25,760 --> 00:09:30,880
of the capabilities of the system there

00:09:28,880 --> 00:09:33,440
is always a size limit

00:09:30,880 --> 00:09:34,399
even when that limit is bound primarily

00:09:33,440 --> 00:09:37,360
by throughput

00:09:34,399 --> 00:09:38,000
and time i'd like to drive these points

00:09:37,360 --> 00:09:40,560
home

00:09:38,000 --> 00:09:42,560
with a series of case studies we will

00:09:40,560 --> 00:09:45,200
look at some properties inherent to the

00:09:42,560 --> 00:09:47,600
data representations used by specific

00:09:45,200 --> 00:09:49,200
serialization formats and see how the

00:09:47,600 --> 00:09:51,600
formats themselves

00:09:49,200 --> 00:09:52,560
either help us solve a problem or get in

00:09:51,600 --> 00:09:54,720
the way

00:09:52,560 --> 00:09:57,040
in each example we will also get to see

00:09:54,720 --> 00:09:59,040
how rust gives you best in class tools

00:09:57,040 --> 00:10:02,640
for manipulating data across

00:09:59,040 --> 00:10:05,519
any representation the first example

00:10:02,640 --> 00:10:08,000
will be in parsing graphql the second in

00:10:05,519 --> 00:10:09,680
writing vertex buffers for the gpu

00:10:08,000 --> 00:10:13,200
and the third will be the use of

00:10:09,680 --> 00:10:16,800
compression in the tree buff format

00:10:13,200 --> 00:10:19,360
first graphql serialization formats

00:10:16,800 --> 00:10:21,519
tend to reflect the architecture of the

00:10:19,360 --> 00:10:23,600
systems that use them

00:10:21,519 --> 00:10:24,800
our computer systems are constructed of

00:10:23,600 --> 00:10:28,079
many components

00:10:24,800 --> 00:10:30,320
nesting into other subsystems comprised

00:10:28,079 --> 00:10:32,560
of more components

00:10:30,320 --> 00:10:34,160
serialization formats nest in a way that

00:10:32,560 --> 00:10:37,360
reflects that

00:10:34,160 --> 00:10:40,480
for example inside a tcp packet

00:10:37,360 --> 00:10:42,640
a serialization format you may find part

00:10:40,480 --> 00:10:45,279
of an http request

00:10:42,640 --> 00:10:46,800
the http request comprises multiple

00:10:45,279 --> 00:10:49,279
serialization formats

00:10:46,800 --> 00:10:49,839
the http headers for example which

00:10:49,279 --> 00:10:51,920
itself

00:10:49,839 --> 00:10:53,440
nests even further and other

00:10:51,920 --> 00:10:56,240
serialization formats

00:10:53,440 --> 00:10:59,440
being the payload in a format which is

00:10:56,240 --> 00:11:02,240
specified by the headers

00:10:59,440 --> 00:11:04,480
the payload may nest to unicode which

00:11:02,240 --> 00:11:07,200
may nest to graphql

00:11:04,480 --> 00:11:08,160
which itself nests to many different

00:11:07,200 --> 00:11:10,560
subformats

00:11:08,160 --> 00:11:12,079
as defined by the spec and if you find a

00:11:10,560 --> 00:11:14,240
string in the graphql

00:11:12,079 --> 00:11:15,920
that string may nest further if it

00:11:14,240 --> 00:11:19,040
contains binary data

00:11:15,920 --> 00:11:20,800
as perhaps a base64 encoded string which

00:11:19,040 --> 00:11:22,800
may also nest

00:11:20,800 --> 00:11:24,000
the nesting reflects the system's

00:11:22,800 --> 00:11:27,120
architecture because

00:11:24,000 --> 00:11:30,079
many layers exist to address concerns

00:11:27,120 --> 00:11:31,040
that manifest at that particular layer

00:11:30,079 --> 00:11:34,480
of abstraction

00:11:31,040 --> 00:11:37,600
of the computer because nesting

00:11:34,480 --> 00:11:40,160
is the natural tendency of serialization

00:11:37,600 --> 00:11:40,839
we need formats that allow us to nest

00:11:40,160 --> 00:11:43,279
data

00:11:40,839 --> 00:11:45,040
efficiently we also need tools for

00:11:43,279 --> 00:11:48,160
parsing and manipulating

00:11:45,040 --> 00:11:49,360
nested data rust and many libraries

00:11:48,160 --> 00:11:52,560
written in rust

00:11:49,360 --> 00:11:54,880
give you these tools in abundance

00:11:52,560 --> 00:11:57,200
some of the baseline things you need are

00:11:54,880 --> 00:11:58,800
to be able to view slices of strings and

00:11:57,200 --> 00:12:02,160
byte arrays safely

00:11:58,800 --> 00:12:03,360
and without copying interpreting bytes

00:12:02,160 --> 00:12:05,680
as another type

00:12:03,360 --> 00:12:07,040
like a string or an integer is also

00:12:05,680 --> 00:12:10,079
important

00:12:07,040 --> 00:12:11,519
safe mutable appendable strings or

00:12:10,079 --> 00:12:13,760
binary types

00:12:11,519 --> 00:12:15,120
allow us to progressively push

00:12:13,760 --> 00:12:18,079
serialized data

00:12:15,120 --> 00:12:19,680
from each format into the same buffer

00:12:18,079 --> 00:12:22,480
rather than serializing

00:12:19,680 --> 00:12:24,079
into separate buffers independently and

00:12:22,480 --> 00:12:27,120
then copying each buffer

00:12:24,079 --> 00:12:29,839
into the nesting format above

00:12:27,120 --> 00:12:31,680
moving control over memory to the caller

00:12:29,839 --> 00:12:34,240
and safely passing mutable

00:12:31,680 --> 00:12:35,279
or immutable data is the name of the

00:12:34,240 --> 00:12:37,360
game

00:12:35,279 --> 00:12:39,200
these capabilities are all necessary

00:12:37,360 --> 00:12:42,079
when parsing and writing

00:12:39,200 --> 00:12:43,920
nesting serialization formats i wish i

00:12:42,079 --> 00:12:44,560
could say that these basic features were

00:12:43,920 --> 00:12:47,279
not rest

00:12:44,560 --> 00:12:49,519
differentiators but a surprising number

00:12:47,279 --> 00:12:51,040
of languages do not meet these minimum

00:12:49,519 --> 00:12:54,160
requirements

00:12:51,040 --> 00:12:56,880
rust is the only memory safe language

00:12:54,160 --> 00:12:57,920
that i'm aware of that does but what

00:12:56,880 --> 00:13:01,760
rust gives you

00:12:57,920 --> 00:13:04,839
is much more here's a type

00:13:01,760 --> 00:13:06,079
from the graphql parser crate by paul

00:13:04,839 --> 00:13:08,800
colomies

00:13:06,079 --> 00:13:10,399
it is an enum named value containing all

00:13:08,800 --> 00:13:11,920
the different kinds of values in a

00:13:10,399 --> 00:13:16,000
graphql query

00:13:11,920 --> 00:13:18,160
like numbers objects and so on

00:13:16,000 --> 00:13:21,200
what's great about this is that value is

00:13:18,160 --> 00:13:24,480
generic over the kind of text to parse

00:13:21,200 --> 00:13:25,200
into one type that implements the text

00:13:24,480 --> 00:13:28,880
trait

00:13:25,200 --> 00:13:30,959
is string so you can parse a graphql

00:13:28,880 --> 00:13:33,600
query into string as the text

00:13:30,959 --> 00:13:34,399
type and because value then will own its

00:13:33,600 --> 00:13:36,079
data

00:13:34,399 --> 00:13:38,480
this allows you to manipulate the

00:13:36,079 --> 00:13:41,199
graphql and write it back out

00:13:38,480 --> 00:13:42,639
that capability comes with a trade-off

00:13:41,199 --> 00:13:45,360
the performance will be

00:13:42,639 --> 00:13:46,959
about as bad as those other garbage

00:13:45,360 --> 00:13:49,839
collected languages

00:13:46,959 --> 00:13:50,560
because of all the extra allocating and

00:13:49,839 --> 00:13:53,680
copying

00:13:50,560 --> 00:13:57,279
that necessarily entails

00:13:53,680 --> 00:14:00,240
reference to stir also implements text

00:13:57,279 --> 00:14:01,120
so you could parse the graphql in a read

00:14:00,240 --> 00:14:03,519
only mode

00:14:01,120 --> 00:14:05,760
that references the underlying text that

00:14:03,519 --> 00:14:08,320
the graphql was parsed from

00:14:05,760 --> 00:14:09,040
with that you get the best performance

00:14:08,320 --> 00:14:12,240
possible

00:14:09,040 --> 00:14:13,360
by avoiding allocations and copies but

00:14:12,240 --> 00:14:16,639
you lose out

00:14:13,360 --> 00:14:19,760
on the ability to manipulate the data

00:14:16,639 --> 00:14:21,920
in some cases that's okay rust

00:14:19,760 --> 00:14:24,240
takes this up a notch because there is a

00:14:21,920 --> 00:14:27,920
third type from the standard library

00:14:24,240 --> 00:14:29,040
that implements text this type is a cow

00:14:27,920 --> 00:14:32,240
of string

00:14:29,040 --> 00:14:33,279
a clone on right string with this safe

00:14:32,240 --> 00:14:35,680
and convenient type

00:14:33,279 --> 00:14:36,959
enabled by our friend and ally the

00:14:35,680 --> 00:14:39,760
borrow checker

00:14:36,959 --> 00:14:41,120
we can parse the graphql in such a way

00:14:39,760 --> 00:14:43,920
that all of the text

00:14:41,120 --> 00:14:46,240
efficiently refers to the source except

00:14:43,920 --> 00:14:49,199
just the parts that you manipulate

00:14:46,240 --> 00:14:51,040
and it's all specified at the call site

00:14:49,199 --> 00:14:52,959
this is the kind of pleasantry

00:14:51,040 --> 00:14:54,480
that i've come to expect from rust

00:14:52,959 --> 00:14:55,920
dependencies

00:14:54,480 --> 00:14:58,079
if you want to change some of the

00:14:55,920 --> 00:15:00,240
graphql text you can

00:14:58,079 --> 00:15:01,440
safely and efficiently do so using this

00:15:00,240 --> 00:15:04,800
type

00:15:01,440 --> 00:15:06,399
almost i say almost

00:15:04,800 --> 00:15:08,880
because there is a fundamental

00:15:06,399 --> 00:15:10,000
limitation of the graphql query format

00:15:08,880 --> 00:15:12,240
itself

00:15:10,000 --> 00:15:14,079
that no amount of rest features or

00:15:12,240 --> 00:15:15,839
excellence in library apis could

00:15:14,079 --> 00:15:17,440
overcome

00:15:15,839 --> 00:15:19,839
looking at the list of different values

00:15:17,440 --> 00:15:22,720
here we see that the entries for

00:15:19,839 --> 00:15:23,920
variable enum and object are generic

00:15:22,720 --> 00:15:27,519
over text

00:15:23,920 --> 00:15:29,759
ironically the string variant is not

00:15:27,519 --> 00:15:30,959
the string variant just contains the

00:15:29,759 --> 00:15:34,240
string type

00:15:30,959 --> 00:15:36,000
requiring allocating and copying what's

00:15:34,240 --> 00:15:38,480
going on here

00:15:36,000 --> 00:15:40,880
the issue is in the way that graphql

00:15:38,480 --> 00:15:43,839
nests its serialization formats

00:15:40,880 --> 00:15:46,160
the graphql string value is unicode but

00:15:43,839 --> 00:15:48,560
the way that graphql embeds strings

00:15:46,160 --> 00:15:50,000
is by putting quotes around them with

00:15:48,560 --> 00:15:52,880
this design choice

00:15:50,000 --> 00:15:53,680
any quotes in the string must be escaped

00:15:52,880 --> 00:15:56,000
which inserts

00:15:53,680 --> 00:15:57,199
new data interspersed with the original

00:15:56,000 --> 00:16:00,320
data

00:15:57,199 --> 00:16:02,480
this comes with consequences one when

00:16:00,320 --> 00:16:04,959
encoding a graphql string value

00:16:02,480 --> 00:16:06,000
the length of the value is not known up

00:16:04,959 --> 00:16:07,839
front

00:16:06,000 --> 00:16:09,839
the length may increase from the

00:16:07,839 --> 00:16:11,440
re-encoding process

00:16:09,839 --> 00:16:13,440
that means that you can't rely on

00:16:11,440 --> 00:16:15,440
resizing the buffer you are encoding to

00:16:13,440 --> 00:16:18,079
upfront before copying

00:16:15,440 --> 00:16:19,199
but instead must continually check this

00:16:18,079 --> 00:16:21,519
buffer size

00:16:19,199 --> 00:16:22,720
when encoding this value or over

00:16:21,519 --> 00:16:25,759
allocate by twice

00:16:22,720 --> 00:16:27,759
as much when reading graphql it is

00:16:25,759 --> 00:16:29,600
impossible to refer to data

00:16:27,759 --> 00:16:32,399
because it needs to go through a parse

00:16:29,600 --> 00:16:34,880
step to remove the escape character

00:16:32,399 --> 00:16:36,320
this problem compounds if you want to

00:16:34,880 --> 00:16:38,639
nest a byte array

00:16:36,320 --> 00:16:40,240
containing another serialization format

00:16:38,639 --> 00:16:42,560
in graphql

00:16:40,240 --> 00:16:44,160
there is no support for directly storing

00:16:42,560 --> 00:16:46,800
bytes in graphql

00:16:44,160 --> 00:16:50,320
so bytes must be encoded into a string

00:16:46,800 --> 00:16:53,279
using base 16 or base64 or similar

00:16:50,320 --> 00:16:54,079
that means three three encode steps are

00:16:53,279 --> 00:16:57,040
necessary

00:16:54,079 --> 00:16:58,560
to nest another format there is encoding

00:16:57,040 --> 00:17:01,839
the data as bytes

00:16:58,560 --> 00:17:05,039
encoding that as a string and finally

00:17:01,839 --> 00:17:07,280
re-encoding the escaped string

00:17:05,039 --> 00:17:10,319
that may compound even further if you

00:17:07,280 --> 00:17:12,559
want to store graphql in another format

00:17:10,319 --> 00:17:14,880
it is common to store graphql query and

00:17:12,559 --> 00:17:18,400
as a string embedded in json

00:17:14,880 --> 00:17:21,600
alongside the graphql variables

00:17:18,400 --> 00:17:23,679
json strings are also quoted strings

00:17:21,600 --> 00:17:26,880
meaning the same data goes through

00:17:23,679 --> 00:17:29,760
another allocation and decode step

00:17:26,880 --> 00:17:30,559
it is common to log the json another

00:17:29,760 --> 00:17:33,360
layer

00:17:30,559 --> 00:17:33,919
another encode step so now if we want to

00:17:33,360 --> 00:17:36,880
get

00:17:33,919 --> 00:17:37,760
that binary data from the logs it's just

00:17:36,880 --> 00:17:41,440
allocating

00:17:37,760 --> 00:17:44,559
and decoding the same data over and over

00:17:41,440 --> 00:17:46,880
up through each layer for every field

00:17:44,559 --> 00:17:49,200
it doesn't have to be this way one

00:17:46,880 --> 00:17:51,600
alternate method of storing a string

00:17:49,200 --> 00:17:54,320
and other variable length data is to

00:17:51,600 --> 00:17:56,799
prefix the data with its length

00:17:54,320 --> 00:17:58,400
not doing this is a familiar mistake

00:17:56,799 --> 00:18:01,520
that was made way back since

00:17:58,400 --> 00:18:02,960
null terminated strings in c the

00:18:01,520 --> 00:18:04,720
difference between the two

00:18:02,960 --> 00:18:07,120
can be the difference between having

00:18:04,720 --> 00:18:10,080
decoding be a major bottleneck

00:18:07,120 --> 00:18:10,480
or instant no amount of engineering

00:18:10,080 --> 00:18:12,640
effort

00:18:10,480 --> 00:18:14,640
spent on optimizing the pipeline that

00:18:12,640 --> 00:18:15,840
consumes the data can improve the

00:18:14,640 --> 00:18:17,919
situation

00:18:15,840 --> 00:18:18,960
because the cost is intrinsic to the

00:18:17,919 --> 00:18:21,679
representation

00:18:18,960 --> 00:18:22,640
of the data you have to design the

00:18:21,679 --> 00:18:25,919
representation

00:18:22,640 --> 00:18:27,679
differently to overcome this i'm not

00:18:25,919 --> 00:18:30,000
saying avoid graphql

00:18:27,679 --> 00:18:30,880
the concepts at play in graphql are

00:18:30,000 --> 00:18:33,840
great

00:18:30,880 --> 00:18:35,280
i use graphql we're all on the same team

00:18:33,840 --> 00:18:37,520
here

00:18:35,280 --> 00:18:39,120
i mentioned graphql in this example

00:18:37,520 --> 00:18:41,360
because using a standard format to

00:18:39,120 --> 00:18:42,799
illustrate this problem is easier for me

00:18:41,360 --> 00:18:44,640
than just inventing one for this

00:18:42,799 --> 00:18:46,000
cautionary tale

00:18:44,640 --> 00:18:47,840
when you go out and design your

00:18:46,000 --> 00:18:50,000
civilization formats

00:18:47,840 --> 00:18:52,080
consider designing with efficient

00:18:50,000 --> 00:18:54,000
nesting in mind

00:18:52,080 --> 00:18:55,440
let's look at one more example of how we

00:18:54,000 --> 00:18:58,400
can build capabilities

00:18:55,440 --> 00:18:59,039
into a serialization format and how rust

00:18:58,400 --> 00:19:00,559
works with

00:18:59,039 --> 00:19:02,160
us to take advantage of those

00:19:00,559 --> 00:19:04,000
capabilities

00:19:02,160 --> 00:19:06,640
for this case study we're going to be

00:19:04,000 --> 00:19:09,440
sending some data to the gpu

00:19:06,640 --> 00:19:11,520
a gpu is driven by data sent to it in a

00:19:09,440 --> 00:19:12,240
serialization format we'll call vertex

00:19:11,520 --> 00:19:14,400
buffers

00:19:12,240 --> 00:19:16,240
vertex buffers contain data like the

00:19:14,400 --> 00:19:17,360
positions of the points that make up

00:19:16,240 --> 00:19:19,520
polygons

00:19:17,360 --> 00:19:21,520
colors material properties and other

00:19:19,520 --> 00:19:24,480
data needed for rendering

00:19:21,520 --> 00:19:24,880
this data comes in two parts the first

00:19:24,480 --> 00:19:27,360
part

00:19:24,880 --> 00:19:28,240
describes a struct's format and the

00:19:27,360 --> 00:19:30,720
second part

00:19:28,240 --> 00:19:32,160
is a contiguous region of memory

00:19:30,720 --> 00:19:35,280
containing those structs

00:19:32,160 --> 00:19:38,960
evenly spaced in an array

00:19:35,280 --> 00:19:41,440
this diagram depicts a vertex buffer

00:19:38,960 --> 00:19:42,559
the top portion is the description

00:19:41,440 --> 00:19:46,160
including the names

00:19:42,559 --> 00:19:47,360
pause x y and z for a vertex position

00:19:46,160 --> 00:19:50,960
and a mesh

00:19:47,360 --> 00:19:53,039
and r b and g color channels

00:19:50,960 --> 00:19:55,120
the bottom part depicts the data with

00:19:53,039 --> 00:19:57,520
three f64 slots

00:19:55,120 --> 00:19:58,480
one for each position coordinate three

00:19:57,520 --> 00:20:00,960
uh slots

00:19:58,480 --> 00:20:02,159
one for each color channel and a blank

00:20:00,960 --> 00:20:04,080
slot for padding

00:20:02,159 --> 00:20:05,679
which just makes everything line up

00:20:04,080 --> 00:20:08,559
nicely

00:20:05,679 --> 00:20:10,559
these slots repeat over and over again

00:20:08,559 --> 00:20:13,679
taking up the same amount of space

00:20:10,559 --> 00:20:16,640
each time there's a good reason

00:20:13,679 --> 00:20:18,320
that the gpu receives data in fixed size

00:20:16,640 --> 00:20:21,679
structs evenly spaced

00:20:18,320 --> 00:20:23,840
in contiguous arrays the gpu is a

00:20:21,679 --> 00:20:26,159
massively parallel device

00:20:23,840 --> 00:20:27,039
the latest nvidia rtx cards have a

00:20:26,159 --> 00:20:30,880
staggering

00:20:27,039 --> 00:20:32,480
10 496 cuda cores

00:20:30,880 --> 00:20:34,880
and that's not even counting tensor

00:20:32,480 --> 00:20:39,520
cores and ray tracing course

00:20:34,880 --> 00:20:42,240
here's a picture of 10 496 boxes

00:20:39,520 --> 00:20:43,919
it's a lot i'm even in the way of some

00:20:42,240 --> 00:20:45,440
of these

00:20:43,919 --> 00:20:47,679
if you want to break up data into

00:20:45,440 --> 00:20:49,760
batches for parallelism

00:20:47,679 --> 00:20:51,600
the most straightforward way you can do

00:20:49,760 --> 00:20:55,200
that is to have fixed

00:20:51,600 --> 00:20:57,760
size structs stored in contiguous arrays

00:20:55,200 --> 00:20:58,799
with that choice of sterilization format

00:20:57,760 --> 00:21:01,440
you can know where

00:20:58,799 --> 00:21:02,480
any arbitrary slice of data lives and

00:21:01,440 --> 00:21:04,720
therefore

00:21:02,480 --> 00:21:05,840
breaks the data up into batches of any

00:21:04,720 --> 00:21:08,880
desired size

00:21:05,840 --> 00:21:11,440
in constant time the serialization

00:21:08,880 --> 00:21:13,360
format reflects the architecture of the

00:21:11,440 --> 00:21:15,360
system

00:21:13,360 --> 00:21:16,640
contrast that to sending the data to the

00:21:15,360 --> 00:21:20,000
gpu in

00:21:16,640 --> 00:21:22,559
say json with json

00:21:20,000 --> 00:21:23,600
the interpretation of every single byte

00:21:22,559 --> 00:21:26,720
in the data

00:21:23,600 --> 00:21:29,039
depends on every preceding byte the

00:21:26,720 --> 00:21:31,520
current element's length is unknown

00:21:29,039 --> 00:21:33,840
until you search for and find a token

00:21:31,520 --> 00:21:37,280
indicating the end of that item

00:21:33,840 --> 00:21:38,480
often a comma or a closed bracket

00:21:37,280 --> 00:21:40,720
if we were to graph the data

00:21:38,480 --> 00:21:43,039
dependencies of a json document

00:21:40,720 --> 00:21:44,080
it would form a continuous chain

00:21:43,039 --> 00:21:46,559
starting with the first

00:21:44,080 --> 00:21:47,600
byte the second byte depending on the

00:21:46,559 --> 00:21:49,600
first

00:21:47,600 --> 00:21:51,200
the third byte depending on the previous

00:21:49,600 --> 00:21:53,280
two continuing

00:21:51,200 --> 00:21:54,640
until the very last byte of that

00:21:53,280 --> 00:21:58,080
document

00:21:54,640 --> 00:22:01,280
consider a string in json is there a key

00:21:58,080 --> 00:22:03,919
or a value that depends at least on

00:22:01,280 --> 00:22:06,400
whether it is inside an object

00:22:03,919 --> 00:22:07,840
if i hid the values of any preceding

00:22:06,400 --> 00:22:10,640
bytes in the document

00:22:07,840 --> 00:22:12,559
it would be impossible to tell the

00:22:10,640 --> 00:22:13,440
problem with that is that data

00:22:12,559 --> 00:22:16,799
dependencies

00:22:13,440 --> 00:22:19,840
limit parallelism a json document

00:22:16,799 --> 00:22:20,880
must be processed sequentially because

00:22:19,840 --> 00:22:24,240
that is a property

00:22:20,880 --> 00:22:26,400
intrinsic to the format making json a

00:22:24,240 --> 00:22:29,840
non-starter for a gpu

00:22:26,400 --> 00:22:31,919
with over 10 000 cores the data

00:22:29,840 --> 00:22:34,080
dependencies limit parallelism

00:22:31,919 --> 00:22:37,360
and add complexity to the engineering

00:22:34,080 --> 00:22:40,080
that goes into writing a parser

00:22:37,360 --> 00:22:42,400
arguably it's the data dependencies that

00:22:40,080 --> 00:22:43,120
make writing a correct json parser a

00:22:42,400 --> 00:22:46,080
challenging

00:22:43,120 --> 00:22:48,400
engineering problem in the first place

00:22:46,080 --> 00:22:49,919
returning to the vertex buffer format

00:22:48,400 --> 00:22:51,520
if we were to graph its data

00:22:49,919 --> 00:22:53,919
dependencies the

00:22:51,520 --> 00:22:55,120
interpretation of each byte in the data

00:22:53,919 --> 00:22:56,880
is only dependent

00:22:55,120 --> 00:22:59,679
on the first few bytes in the

00:22:56,880 --> 00:23:01,760
description of the buffer

00:22:59,679 --> 00:23:03,280
aside from that all bytes are

00:23:01,760 --> 00:23:05,600
independent

00:23:03,280 --> 00:23:07,200
by representing data in a contiguous

00:23:05,600 --> 00:23:09,600
array of fixed size

00:23:07,200 --> 00:23:10,799
elements we can process data

00:23:09,600 --> 00:23:14,240
independently

00:23:10,799 --> 00:23:16,240
and therefore parallelize it's not all

00:23:14,240 --> 00:23:18,480
unicorns and rainbows though

00:23:16,240 --> 00:23:20,080
there are downsides to arrays of fixed

00:23:18,480 --> 00:23:23,280
width elements

00:23:20,080 --> 00:23:25,360
while we gain data independence we

00:23:23,280 --> 00:23:27,200
lose the ability to use compression

00:23:25,360 --> 00:23:30,400
techniques that would rely

00:23:27,200 --> 00:23:31,280
on variable length encoding this means

00:23:30,400 --> 00:23:33,600
that you can use

00:23:31,280 --> 00:23:35,360
some kinds of lossy compression within a

00:23:33,600 --> 00:23:38,559
vertex buffer

00:23:35,360 --> 00:23:40,480
but you cannot use lossless compression

00:23:38,559 --> 00:23:42,240
the trade-off is inherent to the

00:23:40,480 --> 00:23:45,440
representation

00:23:42,240 --> 00:23:46,320
json can utilize both in json for

00:23:45,440 --> 00:23:48,400
example

00:23:46,320 --> 00:23:51,360
a smaller number will take fewer bytes

00:23:48,400 --> 00:23:54,320
to represent than the larger number

00:23:51,360 --> 00:23:55,520
integers between 0 and 9 take 1 byte

00:23:54,320 --> 00:23:58,400
because they only need

00:23:55,520 --> 00:23:59,919
a single character numbers between 10

00:23:58,400 --> 00:24:04,320
and 9 take 2 bytes

00:23:59,919 --> 00:24:06,960
and so on here's a depiction of that

00:24:04,320 --> 00:24:07,520
it shows for a fixed amount of bytes

00:24:06,960 --> 00:24:11,520
storing

00:24:07,520 --> 00:24:14,000
a few or many numbers using json

00:24:11,520 --> 00:24:14,880
i wouldn't ever call json a compression

00:24:14,000 --> 00:24:17,200
format

00:24:14,880 --> 00:24:18,720
but in principle the building blocks of

00:24:17,200 --> 00:24:21,200
lossless compression are

00:24:18,720 --> 00:24:22,559
there in variable length encoding for

00:24:21,200 --> 00:24:24,320
integers

00:24:22,559 --> 00:24:26,640
there are better ways to do this though

00:24:24,320 --> 00:24:28,159
which we'll return to later

00:24:26,640 --> 00:24:30,400
the building blocks for lossy

00:24:28,159 --> 00:24:33,440
compression are present in json 2

00:24:30,400 --> 00:24:35,760
in the form of truncating floats here's

00:24:33,440 --> 00:24:36,880
a depiction of the lossy compression of

00:24:35,760 --> 00:24:39,400
pi

00:24:36,880 --> 00:24:40,799
the first rendition of pi contains only

00:24:39,400 --> 00:24:42,559
3.14

00:24:40,799 --> 00:24:44,159
which is lossier than the second

00:24:42,559 --> 00:24:48,000
rendition having pi

00:24:44,159 --> 00:24:50,240
to more than 10 digits to recap

00:24:48,000 --> 00:24:52,480
that the format used by vertex buffers

00:24:50,240 --> 00:24:53,440
has a different set of capabilities than

00:24:52,480 --> 00:24:55,279
json

00:24:53,440 --> 00:24:57,520
is not something that can be worked

00:24:55,279 --> 00:24:58,000
around with any amount of engineering

00:24:57,520 --> 00:25:00,320
effort

00:24:58,000 --> 00:25:01,200
when consuming the data those

00:25:00,320 --> 00:25:03,200
capabilities

00:25:01,200 --> 00:25:04,400
are inherent to the representations

00:25:03,200 --> 00:25:06,400
themselves

00:25:04,400 --> 00:25:09,679
and if you want different capabilities

00:25:06,400 --> 00:25:13,039
you need to change the representation

00:25:09,679 --> 00:25:13,600
okay having established that writing the

00:25:13,039 --> 00:25:15,919
data

00:25:13,600 --> 00:25:17,120
is the problem we are trying to solve

00:25:15,919 --> 00:25:18,480
and the characteristics the

00:25:17,120 --> 00:25:20,880
serialization format

00:25:18,480 --> 00:25:22,000
must have because of the gpu's

00:25:20,880 --> 00:25:23,679
architecture

00:25:22,000 --> 00:25:25,279
let's write a program to serialize the

00:25:23,679 --> 00:25:26,799
data

00:25:25,279 --> 00:25:29,200
we'll write this program in two

00:25:26,799 --> 00:25:30,320
languages first in typescript and then

00:25:29,200 --> 00:25:32,880
in rust

00:25:30,320 --> 00:25:33,440
i don't do this to disparage typescript

00:25:32,880 --> 00:25:36,080
actually

00:25:33,440 --> 00:25:36,880
parts of typescript are pretty neat but

00:25:36,080 --> 00:25:38,960
rather

00:25:36,880 --> 00:25:40,080
show you the complexity that a memory

00:25:38,960 --> 00:25:42,240
managed program

00:25:40,080 --> 00:25:43,360
adds to the problem that wasn't there to

00:25:42,240 --> 00:25:45,360
start

00:25:43,360 --> 00:25:47,200
without seeing the difference it's hard

00:25:45,360 --> 00:25:50,640
to appreciate the power

00:25:47,200 --> 00:25:52,480
that rust has over data

00:25:50,640 --> 00:25:54,400
the function we will write is a very

00:25:52,480 --> 00:25:56,720
stripped down version of what you might

00:25:54,400 --> 00:25:58,880
need to write a single vertex to a

00:25:56,720 --> 00:26:01,039
vertex buffer for a game

00:25:58,880 --> 00:26:02,000
our vertex will consist of only a

00:26:01,039 --> 00:26:05,360
position

00:26:02,000 --> 00:26:08,720
with three 32-bit float coordinates

00:26:05,360 --> 00:26:10,720
and a color having three u8 channels

00:26:08,720 --> 00:26:11,840
there are likely significantly more

00:26:10,720 --> 00:26:14,880
fields you would want

00:26:11,840 --> 00:26:17,440
to pack into a vertex in a real game

00:26:14,880 --> 00:26:20,320
but this is good for illustration let's

00:26:17,440 --> 00:26:22,720
start with the typescript code

00:26:20,320 --> 00:26:23,440
here is the typescript code if you're

00:26:22,720 --> 00:26:26,159
thinking

00:26:23,440 --> 00:26:27,200
whoa that is too much code to put on a

00:26:26,159 --> 00:26:29,520
slide

00:26:27,200 --> 00:26:31,360
that's the right reaction and it's also

00:26:29,520 --> 00:26:33,679
the point i'm trying to make

00:26:31,360 --> 00:26:36,000
i'm going to describe the code but don't

00:26:33,679 --> 00:26:37,679
worry about following too much

00:26:36,000 --> 00:26:39,840
there's not going to be a quiz and this

00:26:37,679 --> 00:26:41,919
is not a talk about typescript

00:26:39,840 --> 00:26:44,480
just listen enough to get a high level

00:26:41,919 --> 00:26:47,360
feel for the concerns the code addresses

00:26:44,480 --> 00:26:49,919
and don't worry about the details the

00:26:47,360 --> 00:26:52,559
first section defines our interfaces

00:26:49,919 --> 00:26:54,159
vertex position and color are

00:26:52,559 --> 00:26:56,640
unsurprising

00:26:54,159 --> 00:26:57,919
we have this other interface buffer

00:26:56,640 --> 00:27:00,080
which has a byte array

00:26:57,919 --> 00:27:02,400
and a count of how many items are

00:27:00,080 --> 00:27:04,320
written in the array

00:27:02,400 --> 00:27:06,640
the next section is all about

00:27:04,320 --> 00:27:08,480
calculating offsets of where the data

00:27:06,640 --> 00:27:10,480
lives in the buffer

00:27:08,480 --> 00:27:12,320
you could hard code these but the

00:27:10,480 --> 00:27:13,360
comment explaining what the magic

00:27:12,320 --> 00:27:16,000
numbers were

00:27:13,360 --> 00:27:18,080
would be just as long as the code anyway

00:27:16,000 --> 00:27:20,159
so it might as well be code since that

00:27:18,080 --> 00:27:21,600
makes it more likely to be correct

00:27:20,159 --> 00:27:23,600
and in sync with the rest of the

00:27:21,600 --> 00:27:25,919
function

00:27:23,600 --> 00:27:28,080
particularly cumbersome is the line that

00:27:25,919 --> 00:27:28,880
calculates the offset of the r color

00:27:28,080 --> 00:27:31,360
field

00:27:28,880 --> 00:27:33,279
the value for which is a byte but the

00:27:31,360 --> 00:27:37,120
offset is the offset of the

00:27:33,279 --> 00:27:38,080
previous field plus 1 times the size of

00:27:37,120 --> 00:27:41,200
an f32

00:27:38,080 --> 00:27:44,480
in bytes that mixing of types accounts

00:27:41,200 --> 00:27:46,799
for a discontinuity our offsets describe

00:27:44,480 --> 00:27:49,440
because later we're going to use two

00:27:46,799 --> 00:27:52,720
different views over the same allocation

00:27:49,440 --> 00:27:54,880
which is profoundly unsettling we also

00:27:52,720 --> 00:27:57,120
have to calculate each element size

00:27:54,880 --> 00:27:59,200
both in units of bytes and floats for

00:27:57,120 --> 00:28:01,520
similar reasons

00:27:59,200 --> 00:28:03,679
the next thing we are going to do is to

00:28:01,520 --> 00:28:05,520
possibly resize the buffer

00:28:03,679 --> 00:28:07,440
this part is not interesting but the

00:28:05,520 --> 00:28:10,880
code has to be there or the program will

00:28:07,440 --> 00:28:13,200
crash when the buffer runs out of space

00:28:10,880 --> 00:28:14,320
next we set up the views and calculate

00:28:13,200 --> 00:28:16,559
the beginning position

00:28:14,320 --> 00:28:17,360
of the data we want to write within each

00:28:16,559 --> 00:28:20,880
view

00:28:17,360 --> 00:28:23,760
relative to the data size in each view

00:28:20,880 --> 00:28:27,039
these offsets are different even though

00:28:23,760 --> 00:28:30,240
they point to the same place

00:28:27,039 --> 00:28:32,159
lastly we can finally copy the data from

00:28:30,240 --> 00:28:34,240
our vertex into the buffer

00:28:32,159 --> 00:28:36,159
assuming all the previous code is

00:28:34,240 --> 00:28:39,760
correct

00:28:36,159 --> 00:28:42,080
phew now let's take a look at the

00:28:39,760 --> 00:28:44,559
equivalent rust program

00:28:42,080 --> 00:28:46,480
first we define our structs much like we

00:28:44,559 --> 00:28:47,520
did our interfaces in the typescript

00:28:46,480 --> 00:28:49,600
program

00:28:47,520 --> 00:28:51,440
we leave out the interface for buffer

00:28:49,600 --> 00:28:54,000
holding the byte array in count

00:28:51,440 --> 00:28:56,799
we aren't going to need that now let's

00:28:54,000 --> 00:28:59,840
look at the function to write the vertex

00:28:56,799 --> 00:29:03,120
buffer dot push vertex

00:28:59,840 --> 00:29:05,279
that's it rust isn't hiding the fact

00:29:03,120 --> 00:29:06,480
that our data is represented as bytes

00:29:05,279 --> 00:29:08,320
underneath the hood

00:29:06,480 --> 00:29:09,760
and has given us control of the

00:29:08,320 --> 00:29:11,600
representation

00:29:09,760 --> 00:29:12,799
we needed to annotate the structs on the

00:29:11,600 --> 00:29:16,159
previous slide with

00:29:12,799 --> 00:29:19,600
wrapper c moving all error-prone work

00:29:16,159 --> 00:29:20,320
into the compiler between javascript and

00:29:19,600 --> 00:29:21,600
rust

00:29:20,320 --> 00:29:23,600
which do you think would have better

00:29:21,600 --> 00:29:25,679
performance

00:29:23,600 --> 00:29:26,720
the difference is starker than you might

00:29:25,679 --> 00:29:28,559
think

00:29:26,720 --> 00:29:30,320
not just because of all the extra

00:29:28,559 --> 00:29:33,440
boilerplate code

00:29:30,320 --> 00:29:35,679
or it being javascript or

00:29:33,440 --> 00:29:36,720
the cast from float to end for typed

00:29:35,679 --> 00:29:39,440
arrays

00:29:36,720 --> 00:29:40,960
but mostly because of again data

00:29:39,440 --> 00:29:43,600
dependencies

00:29:40,960 --> 00:29:45,440
this time in the form of pointer chases

00:29:43,600 --> 00:29:49,799
when accessing the properties of

00:29:45,440 --> 00:29:51,600
objects in typescript for example

00:29:49,799 --> 00:29:53,919
element.position.x

00:29:51,600 --> 00:29:56,480
it's slow because the serialization

00:29:53,919 --> 00:29:59,360
format used by the javascript runtime

00:29:56,480 --> 00:30:01,279
to represent objects introduced data

00:29:59,360 --> 00:30:03,360
dependencies

00:30:01,279 --> 00:30:04,559
one thing we mean by zero cost

00:30:03,360 --> 00:30:07,360
abstractions

00:30:04,559 --> 00:30:08,320
are abstractions that don't introduce

00:30:07,360 --> 00:30:11,440
unnecessary

00:30:08,320 --> 00:30:13,279
serialization formats

00:30:11,440 --> 00:30:14,720
remember that because the choice of

00:30:13,279 --> 00:30:17,039
serialization format

00:30:14,720 --> 00:30:18,720
is a deciding factor and how you can

00:30:17,039 --> 00:30:20,640
approach the problem

00:30:18,720 --> 00:30:22,000
that the advantage rest gives us of

00:30:20,640 --> 00:30:25,039
being able to choose how

00:30:22,000 --> 00:30:27,679
data is represented carries forward into

00:30:25,039 --> 00:30:29,360
every problem not just writing vertex

00:30:27,679 --> 00:30:31,520
buffers

00:30:29,360 --> 00:30:33,919
for the final case study i'd like to

00:30:31,520 --> 00:30:36,080
take some time to go into how a new

00:30:33,919 --> 00:30:36,960
experimental serialization format called

00:30:36,080 --> 00:30:38,799
tree buff

00:30:36,960 --> 00:30:40,080
represents data in a way that is

00:30:38,799 --> 00:30:43,279
amenable to

00:30:40,080 --> 00:30:43,919
fast compression before we talk about

00:30:43,279 --> 00:30:46,480
the format

00:30:43,919 --> 00:30:47,600
itself we need to talk about the nature

00:30:46,480 --> 00:30:49,760
of data sets

00:30:47,600 --> 00:30:51,120
and we'll use the game of go as an

00:30:49,760 --> 00:30:53,440
example

00:30:51,120 --> 00:30:56,080
we'll also hand roll a custom

00:30:53,440 --> 00:30:58,799
compression format for the game of go

00:30:56,080 --> 00:31:00,880
to use as a baseline to compare against

00:30:58,799 --> 00:31:04,080
tree buff

00:31:00,880 --> 00:31:06,720
this movie depicts a game of go

00:31:04,080 --> 00:31:07,679
i haven't told you anything about how go

00:31:06,720 --> 00:31:09,919
works

00:31:07,679 --> 00:31:13,200
but by watching this movie you might

00:31:09,919 --> 00:31:15,840
pick up on some patterns in the data

00:31:13,200 --> 00:31:18,000
the first pattern we might pick up on is

00:31:15,840 --> 00:31:20,960
that most of the moves are being made

00:31:18,000 --> 00:31:23,039
in certain areas of the board many are

00:31:20,960 --> 00:31:24,799
on the sides and corners

00:31:23,039 --> 00:31:26,240
there's very little going on in the

00:31:24,799 --> 00:31:28,559
center

00:31:26,240 --> 00:31:30,240
another thing you might pick up on is

00:31:28,559 --> 00:31:33,200
that a lot of the time

00:31:30,240 --> 00:31:35,519
a move is adjacent to or very near the

00:31:33,200 --> 00:31:37,600
previous move

00:31:35,519 --> 00:31:38,640
the observation that local data are

00:31:37,600 --> 00:31:40,960
related

00:31:38,640 --> 00:31:43,200
and that not all of the state space of a

00:31:40,960 --> 00:31:46,240
type is likely to be used

00:31:43,200 --> 00:31:46,640
is not specific to the game of go if you

00:31:46,240 --> 00:31:49,679
have an

00:31:46,640 --> 00:31:51,519
image adjacent pixels are likely to be

00:31:49,679 --> 00:31:53,519
of similar colors

00:31:51,519 --> 00:31:55,840
most of the colors of an image may not

00:31:53,519 --> 00:31:56,640
be far off from the image's global color

00:31:55,840 --> 00:31:59,440
palette

00:31:56,640 --> 00:32:01,039
with large swaths of color space being

00:31:59,440 --> 00:32:03,200
unused

00:32:01,039 --> 00:32:04,880
we can extend these observations to a

00:32:03,200 --> 00:32:08,000
complex 2d polygon

00:32:04,880 --> 00:32:09,919
described by a series of points any

00:32:08,000 --> 00:32:12,399
given point in the polygon

00:32:09,919 --> 00:32:14,640
is not likely to be randomly selected

00:32:12,399 --> 00:32:15,760
from all possible points with an even

00:32:14,640 --> 00:32:19,440
probability

00:32:15,760 --> 00:32:22,399
no each point is very likely to be near

00:32:19,440 --> 00:32:23,120
the previous and there are vast vast

00:32:22,399 --> 00:32:25,120
regions

00:32:23,120 --> 00:32:27,760
of the possibility space that will not

00:32:25,120 --> 00:32:31,200
be selected at all

00:32:27,760 --> 00:32:31,840
and so what we observe is that data sets

00:32:31,200 --> 00:32:35,120
containing

00:32:31,840 --> 00:32:37,760
arrays are often predictable

00:32:35,120 --> 00:32:38,880
all lossless compression works by making

00:32:37,760 --> 00:32:40,960
predictions

00:32:38,880 --> 00:32:42,799
first predict what the next data in the

00:32:40,960 --> 00:32:45,679
series is going to be

00:32:42,799 --> 00:32:48,480
then assign variable length binary

00:32:45,679 --> 00:32:50,799
representations to each possible value

00:32:48,480 --> 00:32:52,080
so that if the prediction is accurate

00:32:50,799 --> 00:32:55,279
very few bits can be

00:32:52,080 --> 00:32:56,640
used to represent the value but if the

00:32:55,279 --> 00:32:59,600
prediction is wrong

00:32:56,640 --> 00:33:01,679
you have to pay more bits the quality of

00:32:59,600 --> 00:33:04,080
the prediction is the main factor

00:33:01,679 --> 00:33:05,600
in determining how well the compression

00:33:04,080 --> 00:33:07,760
works

00:33:05,600 --> 00:33:09,679
taken to the extreme if you could

00:33:07,760 --> 00:33:12,399
accurately predict the contents

00:33:09,679 --> 00:33:13,519
of every byte in a file you could

00:33:12,399 --> 00:33:16,880
compress that file

00:33:13,519 --> 00:33:20,320
to zero bytes no such prediction method

00:33:16,880 --> 00:33:22,799
exists we have a data set

00:33:20,320 --> 00:33:24,159
a game of go what we want is an

00:33:22,799 --> 00:33:27,200
algorithm to predict

00:33:24,159 --> 00:33:29,120
the next move in the game to help us

00:33:27,200 --> 00:33:31,120
we're going to visualize the raw data

00:33:29,120 --> 00:33:33,679
from the data set

00:33:31,120 --> 00:33:34,559
this scatter plot is a visual

00:33:33,679 --> 00:33:37,440
representation

00:33:34,559 --> 00:33:38,960
of the actual bytes of a go game as you

00:33:37,440 --> 00:33:41,840
read from left to right

00:33:38,960 --> 00:33:42,960
there is a dot for each byte in the file

00:33:41,840 --> 00:33:46,320
with the dot's height

00:33:42,960 --> 00:33:48,559
corresponding to the value of that byte

00:33:46,320 --> 00:33:49,519
if the game starts with a move at x

00:33:48,559 --> 00:33:52,320
coordinate 4

00:33:49,519 --> 00:33:52,960
and y coordinate 3 there would be a dot

00:33:52,320 --> 00:33:56,080
with height

00:33:52,960 --> 00:33:59,840
4 followed by a dot with height 3

00:33:56,080 --> 00:34:01,039
and so on our eyes can kind of pick up

00:33:59,840 --> 00:34:04,159
on some kind of

00:34:01,039 --> 00:34:05,440
clustering of the dots they don't appear

00:34:04,159 --> 00:34:08,079
random

00:34:05,440 --> 00:34:09,599
that the data does not appear random is

00:34:08,079 --> 00:34:11,599
a good indication

00:34:09,599 --> 00:34:12,960
that some sort of compression is

00:34:11,599 --> 00:34:14,800
possible

00:34:12,960 --> 00:34:16,320
coming up with an algorithm to predict

00:34:14,800 --> 00:34:18,560
the value of a dot

00:34:16,320 --> 00:34:20,320
may not be apparent by just looking at a

00:34:18,560 --> 00:34:22,000
scatter plot

00:34:20,320 --> 00:34:23,679
we can see that there's probably

00:34:22,000 --> 00:34:26,800
something there

00:34:23,679 --> 00:34:28,480
we just don't know yet what it is

00:34:26,800 --> 00:34:30,720
it's worth taking a moment to consider

00:34:28,480 --> 00:34:33,280
how a general purpose algorithm

00:34:30,720 --> 00:34:34,320
like deflate which is the algorithm used

00:34:33,280 --> 00:34:37,359
by gzip

00:34:34,320 --> 00:34:41,119
would approach this gzip works

00:34:37,359 --> 00:34:43,679
by searching for redundancy in the data

00:34:41,119 --> 00:34:45,679
the basic prediction of this method is

00:34:43,679 --> 00:34:46,560
that if you have seen some sequence of

00:34:45,679 --> 00:34:48,879
bytes

00:34:46,560 --> 00:34:51,520
you're likely to see the same sequence

00:34:48,879 --> 00:34:54,800
repeated later

00:34:51,520 --> 00:34:57,040
so gzip scans back in the file to find

00:34:54,800 --> 00:34:58,560
previous occurrences of the data to

00:34:57,040 --> 00:35:01,760
reference

00:34:58,560 --> 00:35:04,640
gzip's prediction works great for text

00:35:01,760 --> 00:35:06,240
where words are often repeated at least

00:35:04,640 --> 00:35:08,560
in the english language

00:35:06,240 --> 00:35:10,720
words are constructed from syllables so

00:35:08,560 --> 00:35:11,359
it's even possible to find repetition in

00:35:10,720 --> 00:35:15,119
a text

00:35:11,359 --> 00:35:18,320
even in the absence of repeated words

00:35:15,119 --> 00:35:20,400
the problem is that in our go game

00:35:18,320 --> 00:35:22,000
the same coordinate on the board is

00:35:20,400 --> 00:35:24,079
seldom repeated

00:35:22,000 --> 00:35:26,079
you can't place a stone on top of a

00:35:24,079 --> 00:35:28,880
previously played stone

00:35:26,079 --> 00:35:30,720
barring a few exceptions then each two

00:35:28,880 --> 00:35:34,079
byte sequence in this file

00:35:30,720 --> 00:35:35,200
is unique a redundancy-based solution

00:35:34,079 --> 00:35:37,520
like gzip

00:35:35,200 --> 00:35:38,960
will produce a compressed file that is

00:35:37,520 --> 00:35:41,440
far from optimal

00:35:38,960 --> 00:35:43,680
because the underlying prediction that

00:35:41,440 --> 00:35:47,040
sequences of bytes would repeat

00:35:43,680 --> 00:35:48,400
has not helped this observation

00:35:47,040 --> 00:35:52,079
generalizes to many

00:35:48,400 --> 00:35:52,079
other kinds of data as well

00:35:52,160 --> 00:35:56,240
recall that we stated that each move is

00:35:55,040 --> 00:35:58,640
likely to be near

00:35:56,240 --> 00:35:59,440
the previous move we could try

00:35:58,640 --> 00:36:01,599
subtracting

00:35:59,440 --> 00:36:04,000
each byte from the last so that instead

00:36:01,599 --> 00:36:06,960
of seeing moves in absolute coordinates

00:36:04,000 --> 00:36:10,160
we'll see them in relative coordinates

00:36:06,960 --> 00:36:12,800
here is a visual representation of that

00:36:10,160 --> 00:36:13,680
this is garbage there are points

00:36:12,800 --> 00:36:15,920
everywhere

00:36:13,680 --> 00:36:16,880
and there seems to be no visual pattern

00:36:15,920 --> 00:36:19,359
at all

00:36:16,880 --> 00:36:20,960
it looks random indicating that the data

00:36:19,359 --> 00:36:24,720
is difficult to predict

00:36:20,960 --> 00:36:26,960
and therefore difficult to compress

00:36:24,720 --> 00:36:27,920
the problem with subtracting is that the

00:36:26,960 --> 00:36:30,160
x and y

00:36:27,920 --> 00:36:31,280
coordinates from the data are logically

00:36:30,160 --> 00:36:34,320
independent

00:36:31,280 --> 00:36:36,240
but interleaved in the data so when we

00:36:34,320 --> 00:36:38,480
subtracted adjacent bytes

00:36:36,240 --> 00:36:40,000
x values were subtracted from y values

00:36:38,480 --> 00:36:42,560
and vice versa

00:36:40,000 --> 00:36:44,560
let's go back here's the same image as

00:36:42,560 --> 00:36:47,359
before

00:36:44,560 --> 00:36:47,680
we first need to separate the data so

00:36:47,359 --> 00:36:50,000
that

00:36:47,680 --> 00:36:51,200
logically related data are stored

00:36:50,000 --> 00:36:54,079
locally

00:36:51,200 --> 00:36:57,200
instead of writing an x followed by a y

00:36:54,079 --> 00:36:59,359
like most serialization formats would do

00:36:57,200 --> 00:37:01,440
let's write out all the x's first and

00:36:59,359 --> 00:37:04,160
then all the y's

00:37:01,440 --> 00:37:05,440
here's a visual representation of that

00:37:04,160 --> 00:37:08,560
it looks maybe

00:37:05,440 --> 00:37:09,520
tighter than before this indicates that

00:37:08,560 --> 00:37:13,359
our data is

00:37:09,520 --> 00:37:16,160
less random now let's try subtracting

00:37:13,359 --> 00:37:18,240
here's a visual representation of that

00:37:16,160 --> 00:37:20,240
now we're making progress

00:37:18,240 --> 00:37:21,599
what i want you to notice is three

00:37:20,240 --> 00:37:24,640
horizontal lines

00:37:21,599 --> 00:37:28,079
right near the center most of the points

00:37:24,640 --> 00:37:30,240
about two-thirds lie on these lines

00:37:28,079 --> 00:37:31,119
these lines correspond to the values

00:37:30,240 --> 00:37:34,400
zero

00:37:31,119 --> 00:37:34,960
negative one and one if we wanted to

00:37:34,400 --> 00:37:37,119
write an

00:37:34,960 --> 00:37:38,800
algorithm to predict what would come

00:37:37,119 --> 00:37:41,839
next in the sequence

00:37:38,800 --> 00:37:46,079
the algorithm could be minimal it's just

00:37:41,839 --> 00:37:48,480
the value is probably 0 negative 1 or 1.

00:37:46,079 --> 00:37:51,119
we can simplify this table further and

00:37:48,480 --> 00:37:52,720
say that the number is likely to be near

00:37:51,119 --> 00:37:55,760
zero

00:37:52,720 --> 00:37:57,359
a small number which sounds familiar

00:37:55,760 --> 00:38:00,320
from when we looked at the variable

00:37:57,359 --> 00:38:02,800
length encoding used in json

00:38:00,320 --> 00:38:04,000
that's going to be our prediction with

00:38:02,800 --> 00:38:05,920
our prediction algorithm

00:38:04,000 --> 00:38:07,440
in hand next we need to come up with a

00:38:05,920 --> 00:38:09,839
representation

00:38:07,440 --> 00:38:12,160
we're going to write a variable length

00:38:09,839 --> 00:38:14,400
encoding

00:38:12,160 --> 00:38:15,200
in this graphic we have three rows of

00:38:14,400 --> 00:38:17,040
boxes

00:38:15,200 --> 00:38:18,720
where we will describe the variable

00:38:17,040 --> 00:38:21,520
length encoding

00:38:18,720 --> 00:38:23,440
each box holds a single bit there are

00:38:21,520 --> 00:38:26,320
three boxes on the top row

00:38:23,440 --> 00:38:28,560
the first box contains zero the next two

00:38:26,320 --> 00:38:31,200
boxes are blank

00:38:28,560 --> 00:38:32,000
the zero at the beginning is a tag bit

00:38:31,200 --> 00:38:33,920
it will indicate

00:38:32,000 --> 00:38:36,640
whether we are in the four smallest

00:38:33,920 --> 00:38:39,920
values in most likely cases

00:38:36,640 --> 00:38:42,640
0 1 negative 1 and 2

00:38:39,920 --> 00:38:44,160
or the unlikely value case for all the

00:38:42,640 --> 00:38:46,720
other values

00:38:44,160 --> 00:38:48,880
the first bit is taken for the tag bit

00:38:46,720 --> 00:38:50,800
leaving two bits for storing those four

00:38:48,880 --> 00:38:53,359
values

00:38:50,800 --> 00:38:54,000
on the second row we have the tag bit 1

00:38:53,359 --> 00:38:57,119
followed by

00:38:54,000 --> 00:39:00,560
4 bits allowing us to store the 16

00:38:57,119 --> 00:39:02,000
least likely values the bottom row shows

00:39:00,560 --> 00:39:04,560
8 bits for reference

00:39:02,000 --> 00:39:05,920
which is how many bits are in a byte

00:39:04,560 --> 00:39:08,640
before we were writing

00:39:05,920 --> 00:39:09,839
each coordinate in a single byte so with

00:39:08,640 --> 00:39:12,240
this encoding

00:39:09,839 --> 00:39:13,760
all moves will always save some amount

00:39:12,240 --> 00:39:16,480
of space

00:39:13,760 --> 00:39:17,440
it didn't have to work out that way but

00:39:16,480 --> 00:39:20,240
we can do this

00:39:17,440 --> 00:39:20,640
because a go board has only 19 points

00:39:20,240 --> 00:39:23,040
along

00:39:20,640 --> 00:39:25,920
each axis which means that we're not

00:39:23,040 --> 00:39:27,920
using the full range of a byte

00:39:25,920 --> 00:39:29,280
if we did use the full range the

00:39:27,920 --> 00:39:32,720
encoding would have to have

00:39:29,280 --> 00:39:36,000
some values extend beyond 8 bits

00:39:32,720 --> 00:39:38,400
but indeed most data sets do not use the

00:39:36,000 --> 00:39:40,160
full range of the underlying types used

00:39:38,400 --> 00:39:42,400
in the representation

00:39:40,160 --> 00:39:44,720
so this generalizes as well to other

00:39:42,400 --> 00:39:46,480
data sets

00:39:44,720 --> 00:39:48,560
the result is that our go game

00:39:46,480 --> 00:39:51,119
compresses to less than half

00:39:48,560 --> 00:39:51,599
of the size of writing the data out

00:39:51,119 --> 00:39:53,839
using

00:39:51,599 --> 00:39:56,079
one byte per coordinate which was

00:39:53,839 --> 00:39:59,119
already pretty efficient

00:39:56,079 --> 00:40:01,119
this result is decent the size is

00:39:59,119 --> 00:40:02,320
smaller than what would be produced by

00:40:01,119 --> 00:40:04,800
gzip

00:40:02,320 --> 00:40:07,200
but the file can be written faster than

00:40:04,800 --> 00:40:09,280
it could be compressed by gzip

00:40:07,200 --> 00:40:10,400
this is because the prediction is more

00:40:09,280 --> 00:40:12,720
accurate

00:40:10,400 --> 00:40:14,000
while being computationally easier to

00:40:12,720 --> 00:40:16,480
produce

00:40:14,000 --> 00:40:18,480
it requires less work to subtract the

00:40:16,480 --> 00:40:20,560
previous value in a sequence

00:40:18,480 --> 00:40:23,760
than to search for redundancy by

00:40:20,560 --> 00:40:25,359
scanning many values in the sequence

00:40:23,760 --> 00:40:27,760
note that this is not the best

00:40:25,359 --> 00:40:29,440
prediction algorithm possible

00:40:27,760 --> 00:40:31,920
if you wanted to get serious about

00:40:29,440 --> 00:40:33,119
compression and squeeze the file down

00:40:31,920 --> 00:40:35,520
further

00:40:33,119 --> 00:40:36,880
you could make an even better prediction

00:40:35,520 --> 00:40:39,839
algorithm

00:40:36,880 --> 00:40:41,760
you could write a deterministic go ai

00:40:39,839 --> 00:40:42,240
and have it sort moves from best to

00:40:41,760 --> 00:40:44,240
worst

00:40:42,240 --> 00:40:45,920
and predict that it is more likely for

00:40:44,240 --> 00:40:48,319
the player to make a good move

00:40:45,920 --> 00:40:49,200
than a bad one this could give us

00:40:48,319 --> 00:40:51,280
perhaps

00:40:49,200 --> 00:40:52,560
twice as good a result as our delta

00:40:51,280 --> 00:40:55,359
compression algorithm

00:40:52,560 --> 00:40:56,160
on a professional go game but the

00:40:55,359 --> 00:40:58,400
trade-off

00:40:56,160 --> 00:40:59,680
is that the ai would be computationally

00:40:58,400 --> 00:41:02,800
expensive

00:40:59,680 --> 00:41:05,520
require a lot of engineering effort

00:41:02,800 --> 00:41:06,000
and once completed would only be able to

00:41:05,520 --> 00:41:09,040
compress

00:41:06,000 --> 00:41:10,400
the game of go whereas the delta

00:41:09,040 --> 00:41:12,400
compression method

00:41:10,400 --> 00:41:14,480
sounds like it might be useful for more

00:41:12,400 --> 00:41:16,560
than just go

00:41:14,480 --> 00:41:18,000
let's review by comparing these methods

00:41:16,560 --> 00:41:19,760
in a matrix

00:41:18,000 --> 00:41:21,680
this chart shows each of the three

00:41:19,760 --> 00:41:22,640
methods we considered written across the

00:41:21,680 --> 00:41:25,359
top

00:41:22,640 --> 00:41:26,400
gzip delta compression and ai

00:41:25,359 --> 00:41:27,760
compression

00:41:26,400 --> 00:41:30,319
written on the side we have three

00:41:27,760 --> 00:41:33,040
categories the compression ratio

00:41:30,319 --> 00:41:35,520
is how small the file is performance is

00:41:33,040 --> 00:41:37,680
how fast we can read and write the file

00:41:35,520 --> 00:41:39,680
and the difficulty is the engineering

00:41:37,680 --> 00:41:42,079
effort required to produce

00:41:39,680 --> 00:41:43,920
and maintain the code that implements

00:41:42,079 --> 00:41:46,800
the compression method

00:41:43,920 --> 00:41:48,319
a check mark goes to the best compressor

00:41:46,800 --> 00:41:51,359
in each category

00:41:48,319 --> 00:41:52,000
an x to the worst and no mark for the

00:41:51,359 --> 00:41:55,280
compressor

00:41:52,000 --> 00:41:55,760
in between each of the methods is the

00:41:55,280 --> 00:41:58,240
best

00:41:55,760 --> 00:42:00,720
at something the delta compression

00:41:58,240 --> 00:42:03,760
method sits in a sweet spot however

00:42:00,720 --> 00:42:06,400
it's not the worst at any category

00:42:03,760 --> 00:42:08,640
so if we were to assign a score of plus

00:42:06,400 --> 00:42:11,680
one for being the best at something

00:42:08,640 --> 00:42:12,560
and minus one for being the worst delta

00:42:11,680 --> 00:42:14,839
compression

00:42:12,560 --> 00:42:17,359
would come out on top with a score of

00:42:14,839 --> 00:42:18,880
one gzip would come in second with a

00:42:17,359 --> 00:42:21,440
score of zero

00:42:18,880 --> 00:42:23,119
and a i would come last with a score of

00:42:21,440 --> 00:42:25,839
negative one

00:42:23,119 --> 00:42:27,440
the overall score hardly matters though

00:42:25,839 --> 00:42:30,400
because where gzip wins

00:42:27,440 --> 00:42:32,800
is in the difficulty category it doesn't

00:42:30,400 --> 00:42:35,440
take a lot of engineering effort to grab

00:42:32,800 --> 00:42:37,440
an existing crate from crates.i o and

00:42:35,440 --> 00:42:39,920
run gzip on your data

00:42:37,440 --> 00:42:42,240
you get a lot with minimal effort using

00:42:39,920 --> 00:42:44,319
something like gzip

00:42:42,240 --> 00:42:46,960
effort is important for working

00:42:44,319 --> 00:42:49,200
professionals under tight deadlines

00:42:46,960 --> 00:42:50,960
i'd go so far as to say that many of us

00:42:49,200 --> 00:42:52,960
even code in a culture

00:42:50,960 --> 00:42:54,480
that is hostile to high performance

00:42:52,960 --> 00:42:56,880
programming methods

00:42:54,480 --> 00:42:58,000
this is especially true when those

00:42:56,880 --> 00:43:00,880
performance gains

00:42:58,000 --> 00:43:02,560
come with any engineering cost you're

00:43:00,880 --> 00:43:04,400
not likely to be criticized by your

00:43:02,560 --> 00:43:06,880
peers for using gzip

00:43:04,400 --> 00:43:08,240
whereas the delta compression method

00:43:06,880 --> 00:43:11,359
required a fair bit

00:43:08,240 --> 00:43:12,480
of custom code but what if we could move

00:43:11,359 --> 00:43:14,880
that check mark

00:43:12,480 --> 00:43:16,480
for the lowest difficulty in engineering

00:43:14,880 --> 00:43:19,119
effort from gzip

00:43:16,480 --> 00:43:20,160
to the delta compression method if we

00:43:19,119 --> 00:43:21,920
could do that

00:43:20,160 --> 00:43:23,359
then the delta compression method would

00:43:21,920 --> 00:43:26,160
dominate gzip in

00:43:23,359 --> 00:43:27,119
every category and that is the

00:43:26,160 --> 00:43:30,319
aspiration

00:43:27,119 --> 00:43:32,079
of tree buff if you followed so far

00:43:30,319 --> 00:43:33,760
in understanding how the delta

00:43:32,079 --> 00:43:35,280
compression method works

00:43:33,760 --> 00:43:37,520
you're already almost there in

00:43:35,280 --> 00:43:40,160
understanding tree buff

00:43:37,520 --> 00:43:41,760
if we forget about the details and look

00:43:40,160 --> 00:43:43,680
at the delta compression methods

00:43:41,760 --> 00:43:46,800
underlying principles

00:43:43,680 --> 00:43:49,520
we find the essence of tree buff

00:43:46,800 --> 00:43:50,480
let's review the process the first thing

00:43:49,520 --> 00:43:52,960
that we did

00:43:50,480 --> 00:43:54,319
when applying our custom designed delta

00:43:52,960 --> 00:43:57,040
compression method

00:43:54,319 --> 00:43:58,640
was to separate the x and y coordinates

00:43:57,040 --> 00:44:01,040
storage

00:43:58,640 --> 00:44:03,040
treebuff generalizes this separation to

00:44:01,040 --> 00:44:04,640
the entire schema

00:44:03,040 --> 00:44:06,960
if we were going to extend this from

00:44:04,640 --> 00:44:08,079
just the x and y coordinates of a single

00:44:06,960 --> 00:44:10,000
game of go

00:44:08,079 --> 00:44:13,440
to all the data for a whole go

00:44:10,000 --> 00:44:16,079
tournament it might look like this

00:44:13,440 --> 00:44:17,520
here we have all the data for a go

00:44:16,079 --> 00:44:19,920
tournament

00:44:17,520 --> 00:44:21,040
at the top we have the root element

00:44:19,920 --> 00:44:23,839
tournament

00:44:21,040 --> 00:44:24,960
which is a type struct the struct has

00:44:23,839 --> 00:44:27,440
three properties

00:44:24,960 --> 00:44:28,160
champion a string on the left in the

00:44:27,440 --> 00:44:31,440
middle

00:44:28,160 --> 00:44:33,359
games of vec and if you follow that

00:44:31,440 --> 00:44:35,200
through all of the moves of the games

00:44:33,359 --> 00:44:36,000
and their coordinates down to the bottom

00:44:35,200 --> 00:44:38,720
row

00:44:36,000 --> 00:44:40,000
there's an x property and a y property

00:44:38,720 --> 00:44:42,400
which are buffers

00:44:40,000 --> 00:44:44,960
holding all of the x coordinates of all

00:44:42,400 --> 00:44:47,280
games in the tournament in one buffer

00:44:44,960 --> 00:44:48,400
and another buffer containing all of the

00:44:47,280 --> 00:44:51,440
y coordinates of

00:44:48,400 --> 00:44:55,040
all the games in the tournament this

00:44:51,440 --> 00:44:58,800
is a tree of buffers hence the name

00:44:55,040 --> 00:44:59,680
tree buff this structure brings locality

00:44:58,800 --> 00:45:02,079
to data

00:44:59,680 --> 00:45:03,440
that is semantically related and of the

00:45:02,079 --> 00:45:06,000
same type

00:45:03,440 --> 00:45:07,280
this transformation is only possible if

00:45:06,000 --> 00:45:10,079
you know the schema

00:45:07,280 --> 00:45:11,839
of the data being written the next thing

00:45:10,079 --> 00:45:14,480
we did with the delta compression

00:45:11,839 --> 00:45:15,839
was that we applied a type aware

00:45:14,480 --> 00:45:17,680
compression method

00:45:15,839 --> 00:45:20,640
after having arranged the data to

00:45:17,680 --> 00:45:22,960
maximize the locality of related data

00:45:20,640 --> 00:45:23,760
subtracting ins and writing the deltas

00:45:22,960 --> 00:45:26,319
was only

00:45:23,760 --> 00:45:27,200
possible because we knew that the bytes

00:45:26,319 --> 00:45:30,319
were uhs

00:45:27,200 --> 00:45:32,480
and not say strings where subtracting

00:45:30,319 --> 00:45:34,480
adjacent characters would produce

00:45:32,480 --> 00:45:36,560
nonsense

00:45:34,480 --> 00:45:38,480
tree buff again generalizes this

00:45:36,560 --> 00:45:40,960
principle and uses different

00:45:38,480 --> 00:45:43,040
high performance type aware compression

00:45:40,960 --> 00:45:44,640
methods for the different kinds of data

00:45:43,040 --> 00:45:46,800
in the tree

00:45:44,640 --> 00:45:47,760
since no compression method is one size

00:45:46,800 --> 00:45:50,319
fits all

00:45:47,760 --> 00:45:52,160
it even spends some performance trying a

00:45:50,319 --> 00:45:55,760
few different compression techniques

00:45:52,160 --> 00:45:58,480
on a sample of the data from each buffer

00:45:55,760 --> 00:45:59,440
the result approximates a hand-rolled

00:45:58,480 --> 00:46:02,960
file format

00:45:59,440 --> 00:46:06,079
that genuinely understands your data

00:46:02,960 --> 00:46:08,800
so what we have is fantastic performance

00:46:06,079 --> 00:46:10,560
and compression what about ease of use

00:46:08,800 --> 00:46:12,800
and engineering effort

00:46:10,560 --> 00:46:14,720
can i claim that it's easier to use tree

00:46:12,800 --> 00:46:18,480
treebuff than gzip

00:46:14,720 --> 00:46:19,599
yes the trick here is that gzip is not

00:46:18,480 --> 00:46:22,480
by itself

00:46:19,599 --> 00:46:24,319
a serialization format using gzip

00:46:22,480 --> 00:46:27,119
assumes that you already have

00:46:24,319 --> 00:46:28,079
some method for writing structured data

00:46:27,119 --> 00:46:32,000
like protobuf

00:46:28,079 --> 00:46:35,040
or csv or message pack or whatever

00:46:32,000 --> 00:46:36,880
using gzip always entails introducing a

00:46:35,040 --> 00:46:39,920
second step

00:46:36,880 --> 00:46:42,800
writing a tree buff file is one step

00:46:39,920 --> 00:46:44,720
the rust implementation has an api very

00:46:42,800 --> 00:46:46,400
much like cerade

00:46:44,720 --> 00:46:48,160
you just put in code or decode

00:46:46,400 --> 00:46:51,040
attributes on your structs

00:46:48,160 --> 00:46:52,480
call the encode method and you're done

00:46:51,040 --> 00:46:55,200
there is no second pass

00:46:52,480 --> 00:46:56,800
over the data using another dependency

00:46:55,200 --> 00:46:58,880
in another format

00:46:56,800 --> 00:47:00,640
it's just the same amount of work as it

00:46:58,880 --> 00:47:04,720
would take to use any other

00:47:00,640 --> 00:47:07,359
serialization format before compression

00:47:04,720 --> 00:47:08,480
so tree buff is easy to use as easy as

00:47:07,359 --> 00:47:10,319
certain

00:47:08,480 --> 00:47:11,839
how does it do on compression and

00:47:10,319 --> 00:47:14,640
performance

00:47:11,839 --> 00:47:16,880
it's time to look at benchmarks this

00:47:14,640 --> 00:47:17,680
benchmark will use real-world production

00:47:16,880 --> 00:47:20,319
data

00:47:17,680 --> 00:47:21,040
served by the graph a decentralized

00:47:20,319 --> 00:47:24,240
indexing

00:47:21,040 --> 00:47:27,040
and query protocol for blockchain data

00:47:24,240 --> 00:47:28,720
for this a graphql query was made to an

00:47:27,040 --> 00:47:32,800
indexer from the graph

00:47:28,720 --> 00:47:35,440
for 1000 recent wearable entity auctions

00:47:32,800 --> 00:47:36,240
in decentraland each entity in the

00:47:35,440 --> 00:47:39,520
response

00:47:36,240 --> 00:47:41,920
looks something like this there are many

00:47:39,520 --> 00:47:43,280
properties of different types there are

00:47:41,920 --> 00:47:46,400
nested objects

00:47:43,280 --> 00:47:47,359
arrays and thousands other entities like

00:47:46,400 --> 00:47:49,599
this one

00:47:47,359 --> 00:47:51,200
but with a cardinality in the data that

00:47:49,599 --> 00:47:54,000
reflects a real-world

00:47:51,200 --> 00:47:54,640
distribution of values what will be

00:47:54,000 --> 00:47:57,200
measured

00:47:54,640 --> 00:48:00,319
is relative cpu time to round-trip the

00:47:57,200 --> 00:48:03,280
data through serialize and deserialize

00:48:00,319 --> 00:48:04,640
and the relative file size the format

00:48:03,280 --> 00:48:07,480
we'll be comparing two

00:48:04,640 --> 00:48:09,040
is message pack which as described by

00:48:07,480 --> 00:48:13,119
messagepack.org

00:48:09,040 --> 00:48:15,200
is like json but fast and small

00:48:13,119 --> 00:48:17,920
i've chosen this format because message

00:48:15,200 --> 00:48:20,800
pack is smaller and faster than json

00:48:17,920 --> 00:48:22,960
and like json is self-describing which

00:48:20,800 --> 00:48:25,520
works well for graphql

00:48:22,960 --> 00:48:27,520
treebuff is also self-describing which

00:48:25,520 --> 00:48:30,000
means that you could open up and read

00:48:27,520 --> 00:48:32,640
any treebuff file without requiring a

00:48:30,000 --> 00:48:36,000
separate schema to interpret the data

00:48:32,640 --> 00:48:38,240
also making it a good fit for graphql

00:48:36,000 --> 00:48:40,079
the feature sets are similar enough that

00:48:38,240 --> 00:48:40,640
we can't attribute a difference in the

00:48:40,079 --> 00:48:42,640
results

00:48:40,640 --> 00:48:45,040
to difference in capabilities

00:48:42,640 --> 00:48:46,880
sidestepping the typical argument

00:48:45,040 --> 00:48:49,200
that schemas are necessary for

00:48:46,880 --> 00:48:52,480
performance

00:48:49,200 --> 00:48:52,880
here are the results the big green box's

00:48:52,480 --> 00:48:55,280
height

00:48:52,880 --> 00:48:57,119
is how long it takes the cpu to round

00:48:55,280 --> 00:48:59,359
trip the message pack file

00:48:57,119 --> 00:49:00,800
and its width is the size of the file

00:48:59,359 --> 00:49:03,920
and bytes

00:49:00,800 --> 00:49:06,960
the message pack file is more than 17

00:49:03,920 --> 00:49:09,839
times as large as the tree buff file

00:49:06,960 --> 00:49:10,640
and it takes more than twice as long to

00:49:09,839 --> 00:49:13,680
serialize

00:49:10,640 --> 00:49:15,200
and deserialize the improvements are

00:49:13,680 --> 00:49:16,800
significant

00:49:15,200 --> 00:49:18,559
considering that the first thing that

00:49:16,800 --> 00:49:21,599
treebuff has to do

00:49:18,559 --> 00:49:22,640
is to reorganize your data into a tree

00:49:21,599 --> 00:49:25,119
of buffers

00:49:22,640 --> 00:49:27,200
before starting to write and then

00:49:25,119 --> 00:49:28,319
reverse that transformation when reading

00:49:27,200 --> 00:49:31,680
the data

00:49:28,319 --> 00:49:33,040
it has no right to even match the speed

00:49:31,680 --> 00:49:36,160
of message pack

00:49:33,040 --> 00:49:38,640
much less significantly outperform it if

00:49:36,160 --> 00:49:41,119
you wonder how this can be real

00:49:38,640 --> 00:49:42,400
the answers have everything to do with

00:49:41,119 --> 00:49:44,480
data dependencies

00:49:42,400 --> 00:49:46,000
and choices made in representing the

00:49:44,480 --> 00:49:49,040
data as bytes

00:49:46,000 --> 00:49:51,280
everything we just covered let's take a

00:49:49,040 --> 00:49:53,680
look at a different data set

00:49:51,280 --> 00:49:54,800
for this benchmark we will consider

00:49:53,680 --> 00:49:56,880
geojson

00:49:54,800 --> 00:49:58,800
for serializing a list of all the

00:49:56,880 --> 00:50:00,800
countries in the world

00:49:58,800 --> 00:50:02,079
the data set includes things like the

00:50:00,800 --> 00:50:04,079
country's names

00:50:02,079 --> 00:50:07,760
but the bulk of the data is in the

00:50:04,079 --> 00:50:10,720
polygons that describe their borders

00:50:07,760 --> 00:50:12,800
geojson is a relatively compact format

00:50:10,720 --> 00:50:15,440
as far as json goes

00:50:12,800 --> 00:50:16,960
because geojson doesn't describe each

00:50:15,440 --> 00:50:20,240
point with redundant

00:50:16,960 --> 00:50:23,200
tags like latitude and longitude

00:50:20,240 --> 00:50:24,319
repeated over and over as most json

00:50:23,200 --> 00:50:27,280
formats would

00:50:24,319 --> 00:50:27,839
but instead opt to store that data in a

00:50:27,280 --> 00:50:32,160
giant

00:50:27,839 --> 00:50:34,960
nested arrays to minimize overhead

00:50:32,160 --> 00:50:36,319
here are the results the green box is

00:50:34,960 --> 00:50:38,160
geojson

00:50:36,319 --> 00:50:40,319
the blue box which is partly

00:50:38,160 --> 00:50:41,520
overshadowed by a red box that i'll get

00:50:40,319 --> 00:50:44,960
to in a second

00:50:41,520 --> 00:50:48,559
is tree buff tree buff is more than

00:50:44,960 --> 00:50:52,400
10 times as fast as geojson

00:50:48,559 --> 00:50:54,400
and it produces a file that is less than

00:50:52,400 --> 00:50:56,640
one-third the size

00:50:54,400 --> 00:50:57,760
the red box is what we get if we opt

00:50:56,640 --> 00:51:00,400
into treebuff's

00:50:57,760 --> 00:51:01,440
lossy float compression which allows us

00:51:00,400 --> 00:51:03,920
to specify

00:51:01,440 --> 00:51:05,280
how much precision is necessary to

00:51:03,920 --> 00:51:08,240
represent the data set

00:51:05,280 --> 00:51:10,000
accurately suppose we instruct treblef

00:51:08,240 --> 00:51:11,440
to encode the floats to the precision

00:51:10,000 --> 00:51:14,640
required to have

00:51:11,440 --> 00:51:15,280
better than one meter accuracy which is

00:51:14,640 --> 00:51:17,040
pretty good

00:51:15,280 --> 00:51:18,480
on world scale data like country's

00:51:17,040 --> 00:51:20,559
borders

00:51:18,480 --> 00:51:22,000
in that case the resulting file

00:51:20,559 --> 00:51:25,119
compresses down to less

00:51:22,000 --> 00:51:28,480
than one tenth the size of geojson

00:51:25,119 --> 00:51:30,400
without sacrificing speed

00:51:28,480 --> 00:51:32,079
we've seen how the choices in the

00:51:30,400 --> 00:51:34,160
representation of data

00:51:32,079 --> 00:51:35,119
can have a significant impact on the

00:51:34,160 --> 00:51:38,960
speed

00:51:35,119 --> 00:51:40,800
size engineering effort and capabilities

00:51:38,960 --> 00:51:42,240
these impacts are not restricted to the

00:51:40,800 --> 00:51:44,480
cases we have studied

00:51:42,240 --> 00:51:46,000
but affect every software engineering

00:51:44,480 --> 00:51:48,000
problem

00:51:46,000 --> 00:51:50,000
there are many capabilities that you can

00:51:48,000 --> 00:51:52,480
design into representations

00:51:50,000 --> 00:51:53,839
that we did not explore today if you

00:51:52,480 --> 00:51:56,079
consider serialization

00:51:53,839 --> 00:51:58,240
and representation as first class

00:51:56,079 --> 00:51:59,359
citizens next to algorithms and code

00:51:58,240 --> 00:52:01,680
structure

00:51:59,359 --> 00:52:03,280
and if you use the proper tools to parse

00:52:01,680 --> 00:52:06,400
and manipulate data

00:52:03,280 --> 00:52:08,559
you'll be surprised by the impact thank

00:52:06,400 --> 00:52:08,559
you

00:52:15,200 --> 00:52:21,280
yes thank you zach um

00:52:18,400 --> 00:52:23,920
actually uh to be honest um um your

00:52:21,280 --> 00:52:27,520
civilization is much much much deeper

00:52:23,920 --> 00:52:30,800
than i understand now

00:52:27,520 --> 00:52:30,800
and um yeah

00:52:31,359 --> 00:52:39,520
yeah thank you for the such deep

00:52:35,040 --> 00:52:39,520
presentation and uh

00:52:40,240 --> 00:52:46,400
we have we have two

00:52:43,760 --> 00:52:46,400
three minutes

00:52:48,800 --> 00:52:52,240
no questions by now that do you have

00:52:51,280 --> 00:52:55,359
anything to

00:52:52,240 --> 00:52:58,400
add your presentation or comment or

00:52:55,359 --> 00:53:00,400
additional message yeah well i'm sorry

00:52:58,400 --> 00:53:01,520
that i didn't make it as accessible as i

00:53:00,400 --> 00:53:03,760
planned to be

00:53:01,520 --> 00:53:06,000
it was uh i don't know it was i did find

00:53:03,760 --> 00:53:09,280
it a bit of a struggle to kind of uh

00:53:06,000 --> 00:53:11,040
kind of get the ideas down into

00:53:09,280 --> 00:53:12,319
something like into a small package and

00:53:11,040 --> 00:53:14,240
really present them

00:53:12,319 --> 00:53:15,920
it was a struggle so i'm i'm sorry that

00:53:14,240 --> 00:53:17,760
it didn't uh

00:53:15,920 --> 00:53:20,559
it wasn't as easy to follow as i had

00:53:17,760 --> 00:53:24,559
hoped when i planned for the talk

00:53:20,559 --> 00:53:28,160
but yeah no um no worries about

00:53:24,559 --> 00:53:31,839
uh it has a lot of um i say case studies

00:53:28,160 --> 00:53:34,160
so it should be um to be honest i need

00:53:31,839 --> 00:53:37,599
some more time to digest your what you

00:53:34,160 --> 00:53:40,880
uh representation right now but um

00:53:37,599 --> 00:53:43,520
well i understand that how important

00:53:40,880 --> 00:53:44,240
actually yeah that should be our first

00:53:43,520 --> 00:53:47,280
season

00:53:44,240 --> 00:53:50,319
in the programming that is a quite

00:53:47,280 --> 00:53:51,760
important message i believe

00:53:50,319 --> 00:53:53,359
yeah that really is the focus of the

00:53:51,760 --> 00:53:55,440
talk i mean if if you want

00:53:53,359 --> 00:53:56,960
uh to to bring your programming to the

00:53:55,440 --> 00:53:58,720
next level

00:53:56,960 --> 00:54:00,720
i think that the best way to do that is

00:53:58,720 --> 00:54:01,599
to just go back to the basics of the

00:54:00,720 --> 00:54:03,599
problem

00:54:01,599 --> 00:54:04,640
every problem really is a problem about

00:54:03,599 --> 00:54:07,520
data

00:54:04,640 --> 00:54:09,359
and um transforming data and then in the

00:54:07,520 --> 00:54:12,240
end serializing data so just like

00:54:09,359 --> 00:54:14,640
keeping that in mind instead of adding

00:54:12,240 --> 00:54:15,359
a lot of layers of complexity on top of

00:54:14,640 --> 00:54:17,520
that

00:54:15,359 --> 00:54:19,359
and really focusing on that problem i

00:54:17,520 --> 00:54:21,280
think can

00:54:19,359 --> 00:54:22,640
can help a lot so if people are are

00:54:21,280 --> 00:54:23,280
interested in that kind of thing there's

00:54:22,640 --> 00:54:25,839
some other

00:54:23,280 --> 00:54:25,839
interesting

00:54:26,319 --> 00:54:29,680
presentations that you could watch i'd

00:54:28,160 --> 00:54:33,040
recommend for example

00:54:29,680 --> 00:54:36,160
watching data oriented programming by

00:54:33,040 --> 00:54:37,680
mike acton is he he

00:54:36,160 --> 00:54:39,599
talks about a lot of things in the same

00:54:37,680 --> 00:54:41,599
terms so that's interesting

00:54:39,599 --> 00:54:43,119
um yeah definitely start there and then

00:54:41,599 --> 00:54:46,000
just follow the line with uh

00:54:43,119 --> 00:54:47,599
data oriented programming it's a lot to

00:54:46,000 --> 00:54:49,920
learn in that field

00:54:47,599 --> 00:54:51,280
yeah thanks thanks for suggestion and uh

00:54:49,920 --> 00:54:54,559
we have one

00:54:51,280 --> 00:54:55,520
question that is um is there anything

00:54:54,559 --> 00:55:00,640
tribal

00:54:55,520 --> 00:55:00,640
is bad for sure um so

00:55:01,040 --> 00:55:05,440
tree buff is is taking advantage of

00:55:03,920 --> 00:55:08,240
being able to

00:55:05,440 --> 00:55:09,359
like find predictability in data with

00:55:08,240 --> 00:55:12,000
arrays

00:55:09,359 --> 00:55:12,640
so if you are if you want to do say

00:55:12,000 --> 00:55:15,040
messages

00:55:12,640 --> 00:55:16,640
like server to server communication for

00:55:15,040 --> 00:55:18,079
things which do not contain

00:55:16,640 --> 00:55:20,720
a race then maybe something like

00:55:18,079 --> 00:55:24,720
protobuf would be better for that

00:55:20,720 --> 00:55:26,240
it tries hard not to be bad

00:55:24,720 --> 00:55:28,720
and that kind of a case where there are

00:55:26,240 --> 00:55:31,680
no arrays but there are some

00:55:28,720 --> 00:55:33,599
fundamental tradeoffs that wherever

00:55:31,680 --> 00:55:36,000
prebuff can optimize for the case

00:55:33,599 --> 00:55:37,839
with arrays it will do so because the

00:55:36,000 --> 00:55:39,839
gains there can be significant

00:55:37,839 --> 00:55:41,040
where there isn't that much to add to

00:55:39,839 --> 00:55:42,799
that case where

00:55:41,040 --> 00:55:45,040
arrays are not being used we're doing

00:55:42,799 --> 00:55:48,160
pretty well in that case already with

00:55:45,040 --> 00:55:51,280
other serialization formats

00:55:48,160 --> 00:55:54,640
okay thanks thanks for answering and

00:55:51,280 --> 00:55:56,000
we are learning all the time okay um

00:55:54,640 --> 00:55:57,280
i'm gonna stick around in the chat too

00:55:56,000 --> 00:55:58,319
for a little bit so if anyone has the

00:55:57,280 --> 00:56:01,520
questions there

00:55:58,319 --> 00:56:06,400
i'll answer them oh it's thanks again

00:56:01,520 --> 00:56:06,400
zach for the great presentation thank

00:56:11,319 --> 00:56:14,319

YouTube URL: https://www.youtube.com/watch?v=vHmsugjljn8


