Title: tkat0 - エッジとクラウドでRustを使いこなす ～AI IoTでの事例～ — RustFest Global 2020
Publication date: 2020-12-19
Playlist: RustFest Global 2020
Description: 
	English title: Full use of Rust on edge and cloud: AI and IoT use cases

この発表では、私たちのプロダクトの開発事例を通して、エッジデバイスやクラウド側でのRustを使った開発をする上で、実践的な設計実装のテクニックを紹介します。


More at https://rustfest.global/session/10-%E3%82%A8%E3%83%83%E3%82%B8%E3%81%A8%E3%82%AF%E3%83%A9%E3%82%A6%E3%83%89%E3%81%A7rust%E3%82%92%E4%BD%BF%E3%81%84%E3%81%93%E3%81%AA%E3%81%99-%EF%BD%9Eai-iot%E3%81%A7%E3%81%AE%E4%BA%8B%E4%BE%8B%EF%BD%9E/
Captions: 
	00:00:29,000 --> 00:00:35,291
This presentation is the ‘Full use of Rust on Edge and Cloud: AI and IoT Use Cases’ by tkato.

00:00:36,625 --> 00:00:40,166
Hello. I’m Tomohiro KATO.

00:00:40,708 --> 00:00:45,500
I work online with the ID, tkato.

00:00:46,250 --> 00:00:51,958
I’m a software engineer at the Japanese Company, Mobility Technologies.

00:00:52,041 --> 00:00:58,916
At my job I use Rust to develop a machine learning system with an Edge device.

00:00:59,000 --> 00:01:01,000
I hope you enjoy this talk.

00:01:03,833 --> 00:01:12,000
In this presentation, I will go through product development examples and introduce design techniques

00:01:12,000 --> 00:01:16,166
developed with Rust on an Edge device or Cloud.

00:01:16,791 --> 00:01:18,791
This is my agenda.

00:01:19,666 --> 00:01:27,708
I’ll first introduce Drive Chart, our product, and then developing Rust on Edge and Cloud.

00:01:30,541 --> 00:01:32,541
First is an introduction to our product.

00:01:33,583 --> 00:01:37,291
We are developing a product called Drive Chart.

00:01:38,250 --> 00:01:46,500
AI Drive Chart is a service to help reduce traffic accidents using an AI for taxis and commercial vehicles.

00:01:48,458 --> 00:01:56,833
It uses the drive recorder and Cloud to move the AI algorithm in relation to camera image and sensor data

00:01:56,833 --> 00:02:00,750
and creates a safe driving report.

00:02:02,875 --> 00:02:07,208
I’ll introduce the part of Drive Chart developed with Rust.

00:02:08,583 --> 00:02:17,958
This is computer vision processing with production on Edge and an evaluation simulation with development on Cloud.

00:02:18,958 --> 00:02:27,166
We implement a computer vision algorithm with real-time deep learning on the drive recorder using Edge.

00:02:28,333 --> 00:02:33,958
We slightly alter the Edge implementation on Cloud and operate it on Kubernetes.

00:02:37,041 --> 00:02:41,625
Next, I’ll explain how this development progressed by adopting Rust.

00:02:42,750 --> 00:02:52,791
We initially launched using C++. We considered switching to Rust due to the development time and quality issues.

00:02:53,666 --> 00:03:01,750
However, I’m not saying that C++ has issues. Just in our case with limited time and manpower to develop

00:03:01,750 --> 00:03:06,000
Rust was more suitable.

00:03:07,041 --> 00:03:11,166
The first issue is the build system and test framework.

00:03:11,625 --> 00:03:20,291
These aren’t built into C++, so we had to select our own technology and write most of the script for it.

00:03:20,708 --> 00:03:27,750
We wanted to simplify the non-code script to focus on substantial system design and coding.

00:03:29,625 --> 00:03:37,750
Rust prepares the build, test, and documentation from the project template via Cargo.

00:03:37,750 --> 00:03:40,541
That solved this issue.

00:03:41,166 --> 00:03:43,166
The second is safety.

00:03:43,875 --> 00:03:51,000
The drive recorder requires high-quality software so it won’t crash even if there are issues during operation.

00:03:52,333 --> 00:03:59,125
Rust mostly removed the the memory operation bugs that we had during the compilation time.

00:04:00,250 --> 00:04:08,250
Since we switched to Rust, no bugs causing it to crash have been imbedded even during its development.

00:04:09,291 --> 00:04:14,333
Even Rust can’t prevent the logic bug, so be careful.

00:04:16,000 --> 00:04:23,250
Because it had these advantages and we expected the same execution speed and memory as on C++

00:04:23,250 --> 00:04:25,875
we actively switched to Rust.

00:04:27,791 --> 00:04:31,958
In about two weeks, we created a prototype to make this switch.

00:04:33,000 --> 00:04:39,750
The core was essentially the same as on C++. We implemented the application on Rust

00:04:39,750 --> 00:04:43,625
and checked the drive recorder’s operational performance.

00:04:44,833 --> 00:04:53,083
We didn’t rewrite everything all at once. We slowly switched what worked from C++ to Rust.

00:04:53,083 --> 00:04:56,666
Only then did we decide to develop a product on Rust.

00:04:57,666 --> 00:05:03,958
Afterward, we progressed while improving our knowledge of Rust and launched without any issues.

00:05:04,458 --> 00:05:09,250
Part of this is still on C++, but it was mostly implemented on Rust.

00:05:12,291 --> 00:05:15,291
Next, I’ll introduce the development flow.

00:05:16,666 --> 00:05:25,875
We develop the AI algorithm on Python, reimplement it on Rust, and operate it in the production environment.

00:05:27,250 --> 00:05:35,125
Finally, an AI function was supplied as an AI library and incorporated into the drive recorder application.

00:05:35,958 --> 00:05:39,791
This is an example of how we add a new function.

00:05:42,458 --> 00:05:47,208
First, we do R&D on Python and verify the algorithm.

00:05:48,000 --> 00:05:52,708
This is because Python is the main ecosystem to develop deep learning.

00:05:53,625 --> 00:05:59,041
Next, we verify this while integrating the Python module and Edge AI library.

00:05:59,666 --> 00:06:06,958
Once the deep learning module is finished, we can verify it by adding it on the drive recorder’s production pipeline.

00:06:08,000 --> 00:06:14,041
In this development on Cloud, we use massive test data including videos and sensor data.

00:06:14,041 --> 00:06:18,000
We evaluate the overall algorithm and machine learning system.

00:06:19,625 --> 00:06:24,583
Next, we reimplement and verify Python on the AI library with Rust.

00:06:25,708 --> 00:06:31,250
Finally, we incorporate it into the drive recorder, repeatedly verify the recording, and go live to release it.

00:06:32,458 --> 00:06:40,291
We can efficiently develop new functions by seamlessly linking R&D with production.

00:06:41,833 --> 00:06:46,458
We brainstorm software designs to facilitate this.

00:06:49,166 --> 00:06:53,083
Let’s get into the design and implementation.

00:06:53,916 --> 00:06:57,875
First, I’ll explain how to handle deep learning and the computer vision processing

00:06:57,875 --> 00:07:01,208
on the drive recorder with Rust.

00:07:02,125 --> 00:07:08,333
We write using a standard library on Rust and an image processing crate.

00:07:08,333 --> 00:07:13,458
We wrap the C++ library with Rust as needed for performance.

00:07:13,958 --> 00:07:22,416
We use our successful C++ library for deep learning processing, but I won’t go into too much detail.

00:07:23,000 --> 00:07:31,708
We create our own API on C for the C++ library and wrap it with Bindgen.

00:07:31,708 --> 00:07:38,000
But we use CPP when we want to quickly call C++ from Rust.

00:07:39,500 --> 00:07:44,083
For reference, we shared the information on the Rust machine learning ecosystem.

00:07:46,000 --> 00:07:51,958
There is already Rust binding for major frameworks like TensorFlow or Pitorch.

00:07:52,458 --> 00:07:58,708
Also, this runs on the hardware of the common Rasberry Pi and Smartphone,

00:07:58,708 --> 00:08:03,375
so TensorFlow and TVM can be used on Rust.

00:08:03,958 --> 00:08:12,041
In addition to this, I recommend binding the C or C++ library on Rust like we did.

00:08:15,166 --> 00:08:17,166
Let’s go onto the unit test.

00:08:18,416 --> 00:08:26,375
Rust is particularly reliant on third-party crates, and they sometimes begin to degrade because of this.

00:08:27,375 --> 00:08:30,875
I’ll introduce several measures to prevent this.

00:08:31,250 --> 00:08:37,291
Firstly, I recommend a snapshot testing method using a crate called Intsa.

00:08:37,875 --> 00:08:46,333
This crate stores the execution results of the test and can easily compare results while running the test.

00:08:47,083 --> 00:08:54,791
We use this to check that the system detection for videos is consistent.

00:08:57,250 --> 00:09:03,041
The automatic test runs on GitHub Actions and includes snapshot testing.

00:09:03,791 --> 00:09:10,250
This is useful, because there are many useable commands on GitHub Actions in Actions RS.

00:09:10,750 --> 00:09:17,083
The Dependency version recorded on Cargo.toml goes up to the patched version.

00:09:17,083 --> 00:09:21,583
The GitHub dependabot will take care of updates.

00:09:22,875 --> 00:09:28,875
The dependabot creates a pull request for any crate updates, automatically test it, and verify if it’s degraded.

00:09:28,875 --> 00:09:34,291
This makes operation a breeze.

00:09:37,500 --> 00:09:40,250
Let’s continue onto development on Cloud.

00:09:41,041 --> 00:09:46,791
We are constructing a simulator to evaluate Edge implementation on Cloud.

00:09:47,500 --> 00:09:55,250
As shown in the diagram, we join multiple crates in the application on Edge as a monolithic library.

00:09:55,250 --> 00:10:01,666
On Cloud, we deploy crates as containers and structure it as a distribution system.

00:10:02,833 --> 00:10:04,833
There are two reasons for this.

00:10:05,000 --> 00:10:08,166
The first is it accelerates building.

00:10:08,791 --> 00:10:17,791
It splits containers to hasten updates and deployment per function with differing development cycles.

00:10:18,791 --> 00:10:23,000
The second is it integrates R&D and production.

00:10:23,833 --> 00:10:27,875
In one use case, we wanted to evaluate the deep learning module 

00:10:27,875 --> 00:10:33,583
within the current pipeline with Python implementation during R&D.

00:10:33,875 --> 00:10:40,958
In brief, we selected an architecture to create a mixed system with Rust and Python.

00:10:42,125 --> 00:10:52,500
You can consider other methods to wrap Rust as a Python library and structure the overall system on Python.

00:10:52,500 --> 00:10:58,541
We selected this structure for its loosely-coupled, simple structure.

00:11:01,166 --> 00:11:07,875
We collaborate on shared code for Edge and Cloud within the crate

00:11:07,875 --> 00:11:15,000
and give it a design with hexagonal architecture to continue developing it on Edge and Cloud.

00:11:17,333 --> 00:11:20,916
Edge and Cloud have a common core logic.

00:11:21,666 --> 00:11:29,541
But the implementation selection depends on the application entry point, the hardware, and the use case.

00:11:29,541 --> 00:11:36,875
For example, during Edge production, we incorporate Rust implementation in the application as C API.

00:11:38,625 --> 00:11:45,583
When we set a device benchmark from a PC or call from other applications on Kubernetes,

00:11:45,583 --> 00:11:49,666
we use the gRPC or CLI interface.

00:11:52,250 --> 00:12:01,583
The inference function allows us to select implementation to call the R&D Python model with gRPC,

00:12:01,583 --> 00:12:07,416
a deep learning framework for the device, and a mock test framework for X86.

00:12:07,833 --> 00:12:14,583
This seamlessly links from the R&D on Python to the production development on Rust.

00:12:14,958 --> 00:12:20,208
Switching over functions is implemented with Rust traits as needed for each use case.

00:12:22,291 --> 00:12:24,958
This slide shows implementation examples.

00:12:25,375 --> 00:12:31,250
The code is abridged but sufficient to get the gist.

00:12:31,583 --> 00:12:40,583
This allows the module inference to be switched to three different types: gRPC, Edge, or mock.

00:12:41,083 --> 00:12:44,583
First, it implements the trait module service.

00:12:45,250 --> 00:12:51,000
Consider the input image and inference results for the execute method.

00:12:51,458 --> 00:12:54,666
Next, we implement each trait.

00:12:55,416 --> 00:13:03,541
The gPRC module service uses the gRPC client, like Tonic to access the external API.

00:13:04,708 --> 00:13:12,708
The code for the Edge module is abridged. This is implemented through a deep learning framework for Edge.

00:13:13,166 --> 00:13:20,750
The mock module service is a unit test implementation that responds when there’s an instance.

00:13:21,250 --> 00:13:26,458
This is how you create multiple implementations from one trait.

00:13:28,083 --> 00:13:32,958
I’ll explain the part of the code used in the application.

00:13:33,208 --> 00:13:38,916
This helps to switch the implementation when starting the application.

00:13:38,916 --> 00:13:48,125
This uses the trait object to reset different implementations based on the given module type variation.

00:13:50,416 --> 00:13:54,791
You just input Model.Execute to call it.

00:13:55,708 --> 00:14:00,291
This abstracts the implementation and lets it switch depending on the use case.

00:14:01,000 --> 00:14:09,833
We used these techniques to facilitate an improved testability and integration with systems other than Rust.

00:14:12,708 --> 00:14:16,375
Our final technical topic is logging.

00:14:16,791 --> 00:14:22,833
We often use log to debug and profile at the application level.

00:14:23,500 --> 00:14:29,250
Naturally, we use a benchmark like Cargo bench during development at the crate level.

00:14:29,791 --> 00:14:39,041
We often use JSON structure logging, then parse and visualize it with a tool like Chrome tracing.

00:14:40,166 --> 00:14:45,208
This lets you see processing with multiple processes and threads.

00:14:45,208 --> 00:14:51,875
This is a reasonable method to get an initial indication of bottlenecks.

00:14:52,625 --> 00:15:01,458
The log base doesn’t rely on language as it’s on a system constructed with multiple languages, like Rust and Python.

00:15:03,208 --> 00:15:08,291
You use a s-log crate on Rust to do structured logging.

00:15:09,125 --> 00:15:14,500
You can output a JSON log with a macro, like in the diagram.

00:15:14,958 --> 00:15:20,541
This shows each inference begin and end event in the log.

00:15:22,125 --> 00:15:27,666
Let’s discuss system changes within half a year of development.

00:15:28,708 --> 00:15:36,000
In a few words, the system changed from a monolithic crate to a micro crate.

00:15:37,291 --> 00:15:46,833
In our initial development with Rust, we often stuffed multiple functions into a crate, but overtime split those up.

00:15:47,041 --> 00:15:49,041
There are several reasons for this.

00:15:49,583 --> 00:15:59,583
First is the build time. Differentiating crates accelerates this since we don’t build all functions at once during development.

00:16:00,125 --> 00:16:07,958
One advantage is how easily it combines with other languages once you can make a container per function and crate.

00:16:08,791 --> 00:16:14,083
In our use case, we wanted to make a system that mixed Python and Rust.

00:16:14,083 --> 00:16:21,625
One benefit of employing a microservice is how flexibly it selects technology per function.

00:16:22,291 --> 00:16:29,250
I recommend the option that doesn’t force you to use Rust for several lines written on a shell script.

00:16:30,666 --> 00:16:36,083
Our goal for this microservice was to improve maintenance.

00:16:36,958 --> 00:16:45,250
Currently our team has 4 people writing Rust, and is a small part of the overall company.

00:16:45,625 --> 00:16:52,750
We value making it simple so that any new participating members can quickly catch up.

00:16:55,041 --> 00:17:02,291
Lastly, I’ll give my impression of using Rust in product development for half a year.

00:17:03,500 --> 00:17:11,250
I felt Rust was a good choice, because its team development is good and production would develop quickly.

00:17:12,750 --> 00:17:19,875
Since I proposed switching to Rust, the team has grown to its current size of 4 people.

00:17:19,875 --> 00:17:25,416
Rust was the first programming language any of us experienced.

00:17:28,166 --> 00:17:36,208
Familiarity is the key. We got used to Rust through the compile error messages and the Clippy messages.

00:17:39,708 --> 00:17:46,333
Earlier I spoke about system transitions. We were able to work on stress-free incremental development

00:17:46,333 --> 00:17:54,250
because even when we conducted major refactoring to differentiate crates, we could check for slipups while compiling.

00:17:56,666 --> 00:18:04,250
I spoke about our initial motivation to use Rust. I think it has an enhanced ecosystem.

00:18:04,666 --> 00:18:13,208
In product development, our team focused on the design and coding without overwhelming the repository

00:18:13,208 --> 00:18:16,458
with script on build and test documentation.

00:18:17,666 --> 00:18:23,083
Another positive is that the role of our team expanded.

00:18:24,458 --> 00:18:31,125
Rust can be developed on a variety of platforms and domains. We mainly worked on the development on devices.

00:18:31,125 --> 00:18:34,333
But we could sufficiently develop on Cloud too.

00:18:37,416 --> 00:18:38,875
To conclude

00:18:39,875 --> 00:18:48,916
Today, I went through product development examples and how we used Rust on Edge and Cloud.

00:18:49,250 --> 00:18:55,000
We devised a software architecture on Edge and Cloud to integrate R&D on Python

00:18:55,000 --> 00:18:59,166
to the production on Rust.

00:18:59,750 --> 00:19:08,333
This approach and implementation were based on our use case and may not be applicable to everyone.

00:19:08,833 --> 00:19:12,541
Still I’m please so many joined this session.

00:19:13,041 --> 00:19:17,708
Let’s all try out product development on Rust.

00:19:17,958 --> 00:19:19,000
That’s it.

00:19:19,208 --> 00:19:22,416

YouTube URL: https://www.youtube.com/watch?v=jUvLk_U6ymk


