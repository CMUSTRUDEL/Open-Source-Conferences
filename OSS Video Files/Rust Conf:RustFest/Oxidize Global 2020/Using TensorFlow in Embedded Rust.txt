Title: Using TensorFlow in Embedded Rust
Publication date: 2020-11-06
Playlist: Oxidize Global 2020
Description: 
	Some microcontrollers are powerful enough to usefully run Machine Learning (ML) models on audio, motion or even image data. Even though the practical applications of this are not yet clear, there is considerable interest in Edge ML.

TensorFlow is a popular framework for training and running ML models, and there are good resources for training and miniaturising TensorFlow models. This talk is an introduction to using these models in an Embedded Rust project, and briefly walks through the process of instantiating a model, setting input tensors, running inference and reading outputs. We will use a Rust crate that wraps the C API, and this talk will outline how this crate works and how it catches many common errors at compile time.
Captions: 
	00:00:01,770 --> 00:00:34,559
[Music]

00:00:32,719 --> 00:00:35,680
there we go from machine learning to

00:00:34,559 --> 00:00:38,559
machine learning

00:00:35,680 --> 00:00:39,280
from a generative art tracks to richard

00:00:38,559 --> 00:00:42,160
meadows

00:00:39,280 --> 00:00:43,520
talking about integrating tensorflow or

00:00:42,160 --> 00:00:45,680
using tensorflow

00:00:43,520 --> 00:00:46,960
on embedded rust which i also find it

00:00:45,680 --> 00:00:48,640
very interesting subject because we've

00:00:46,960 --> 00:00:49,840
been talking so much about integrating

00:00:48,640 --> 00:00:51,360
rust with other stuff

00:00:49,840 --> 00:00:53,520
over the whole morning in the chat and

00:00:51,360 --> 00:00:56,000
during the talks so

00:00:53,520 --> 00:00:58,800
here's the talk that applies it give it

00:00:56,000 --> 00:01:02,399
a go richard

00:00:58,800 --> 00:01:03,840
okay so my name is richard meadows

00:01:02,399 --> 00:01:05,760
and i'm going to be talking about using

00:01:03,840 --> 00:01:07,520
the tensorflow framework in embedded

00:01:05,760 --> 00:01:10,080
rust

00:01:07,520 --> 00:01:11,520
so tensorflow is a machine learning

00:01:10,080 --> 00:01:14,720
framework

00:01:11,520 --> 00:01:16,880
so what is machine learning so firstly

00:01:14,720 --> 00:01:18,880
machine learning is about transferring

00:01:16,880 --> 00:01:20,960
transforming some input data into some

00:01:18,880 --> 00:01:22,320
useful outputs

00:01:20,960 --> 00:01:24,320
this is something we do all the time

00:01:22,320 --> 00:01:26,960
when we're programming conditional

00:01:24,320 --> 00:01:28,479
statements if statements

00:01:26,960 --> 00:01:30,240
but in this case we're going to use a

00:01:28,479 --> 00:01:33,600
model to make the transformation rather

00:01:30,240 --> 00:01:33,600
than handcrafted code

00:01:34,000 --> 00:01:37,840
so what's more this model is created

00:01:36,400 --> 00:01:40,640
using an algorithm

00:01:37,840 --> 00:01:43,040
usually based on training data because

00:01:40,640 --> 00:01:44,560
it's generated computationally

00:01:43,040 --> 00:01:46,000
the model can be much more complex than

00:01:44,560 --> 00:01:47,040
anything we could practically create by

00:01:46,000 --> 00:01:51,040
hand

00:01:47,040 --> 00:01:51,040
and this enables new applications

00:01:51,920 --> 00:01:57,520
so a little bit about tensorflow

00:01:55,520 --> 00:01:59,119
you can think of tensorflow as a toolbox

00:01:57,520 --> 00:02:01,119
machine learning it gives you many of

00:01:59,119 --> 00:02:03,360
the tools you need

00:02:01,119 --> 00:02:05,520
there are other frameworks but

00:02:03,360 --> 00:02:07,040
tensorflow is one of the most popular

00:02:05,520 --> 00:02:09,039
and that gives it the advantage of good

00:02:07,040 --> 00:02:12,480
documentation and

00:02:09,039 --> 00:02:13,760
a big community it also has support for

00:02:12,480 --> 00:02:16,800
embedded platforms

00:02:13,760 --> 00:02:16,800
which is important for us

00:02:18,400 --> 00:02:23,360
okay so this slide is the simplest

00:02:21,840 --> 00:02:25,760
drawing of machine learning

00:02:23,360 --> 00:02:27,120
i could come up with so at the top

00:02:25,760 --> 00:02:29,840
there's training

00:02:27,120 --> 00:02:31,680
which is where we create the model the

00:02:29,840 --> 00:02:34,800
model then goes down the arrow

00:02:31,680 --> 00:02:36,720
and the model is used to influence so

00:02:34,800 --> 00:02:38,080
influence is just a name for the process

00:02:36,720 --> 00:02:41,680
of creating

00:02:38,080 --> 00:02:41,680
outputs from the input data

00:02:42,000 --> 00:02:45,200
so this drawing doesn't quite cover all

00:02:43,840 --> 00:02:47,280
possible forms of machine learning

00:02:45,200 --> 00:02:49,760
there's also reinforcement learning and

00:02:47,280 --> 00:02:52,560
other schemes with feedback but this is

00:02:49,760 --> 00:02:52,560
good enough for today

00:02:53,040 --> 00:02:57,120
and something else about this talk so

00:02:54,400 --> 00:02:58,159
we're only going to cover inference

00:02:57,120 --> 00:03:00,239
training is something that would

00:02:58,159 --> 00:03:03,040
typically happen on a

00:03:00,239 --> 00:03:05,920
on a pc or a server with lots of compute

00:03:03,040 --> 00:03:07,920
resources maybe also a gpu

00:03:05,920 --> 00:03:09,120
and you can do this in python java c

00:03:07,920 --> 00:03:11,760
plus

00:03:09,120 --> 00:03:14,959
rust of course but either way at the end

00:03:11,760 --> 00:03:14,959
of training you get the model

00:03:15,680 --> 00:03:19,519
there is one really important part of

00:03:17,519 --> 00:03:21,120
training called quantization

00:03:19,519 --> 00:03:23,840
and we'll make a small exception to talk

00:03:21,120 --> 00:03:23,840
about that later

00:03:24,080 --> 00:03:28,480
it's also worth at this point defining

00:03:25,760 --> 00:03:31,840
the term edge machine learning

00:03:28,480 --> 00:03:33,440
you see as the acronym edge ml as this

00:03:31,840 --> 00:03:35,040
is where one or other of these steps

00:03:33,440 --> 00:03:36,720
happens physically close to the input

00:03:35,040 --> 00:03:39,599
data

00:03:36,720 --> 00:03:40,319
so that's usually a sensor or microphone

00:03:39,599 --> 00:03:43,840
and

00:03:40,319 --> 00:03:43,840
that means it's happening on an embedded

00:03:44,840 --> 00:03:47,840
platform

00:03:48,799 --> 00:03:53,920
okay so more details about using

00:03:51,360 --> 00:03:56,400
tensorflow and microcontrollers

00:03:53,920 --> 00:03:58,159
so this is a subset of the overall

00:03:56,400 --> 00:04:00,879
tensorflow project

00:03:58,159 --> 00:04:03,280
and the official name is tensorflow lite

00:04:00,879 --> 00:04:05,519
for microcontrollers

00:04:03,280 --> 00:04:07,760
that's a bit of a handful so i'm gonna

00:04:05,519 --> 00:04:09,680
shorten it to tensorflow micro

00:04:07,760 --> 00:04:11,760
uh for the rest of this talk you may

00:04:09,680 --> 00:04:14,000
also see some of the other abbreviations

00:04:11,760 --> 00:04:14,000
there

00:04:14,480 --> 00:04:17,680
one of the good resources to learn in

00:04:16,639 --> 00:04:20,400
this area is

00:04:17,680 --> 00:04:23,120
a book so that's written by two of the

00:04:20,400 --> 00:04:23,120
contributors

00:04:23,600 --> 00:04:26,639
there's also some screencasts online

00:04:25,440 --> 00:04:28,479
relating to this book

00:04:26,639 --> 00:04:29,759
and all this material is a great place

00:04:28,479 --> 00:04:30,800
to learn more about training and

00:04:29,759 --> 00:04:32,000
quantization

00:04:30,800 --> 00:04:34,479
because we're not going to cover those

00:04:32,000 --> 00:04:34,479
in detail

00:04:35,600 --> 00:04:38,960
okay so so far we know tensorflow micro

00:04:38,080 --> 00:04:42,560
is written in c

00:04:38,960 --> 00:04:44,400
plus and it runs on microcontrollers

00:04:42,560 --> 00:04:47,280
so how do we go about making use of that

00:04:44,400 --> 00:04:47,280
in a rust project

00:04:48,960 --> 00:04:52,960
so one of the features of rust is that

00:04:50,800 --> 00:04:56,320
it can be application binary interface

00:04:52,960 --> 00:04:58,240
api compatible with c

00:04:56,320 --> 00:04:59,919
so in simple terms that means that we

00:04:58,240 --> 00:05:02,880
can configure things so we can call c

00:04:59,919 --> 00:05:04,080
functions from within our rust and it

00:05:02,880 --> 00:05:04,960
even works the other way around so we

00:05:04,080 --> 00:05:08,720
can call

00:05:04,960 --> 00:05:08,720
loss functions from within our c

00:05:08,840 --> 00:05:14,400
programs

00:05:11,280 --> 00:05:17,680
so so once we have that power

00:05:14,400 --> 00:05:20,960
to call c functions from our loss code

00:05:17,680 --> 00:05:23,680
we can write what's called bindings so

00:05:20,960 --> 00:05:24,800
these are sets of rust functions the

00:05:23,680 --> 00:05:28,080
core corresponding c

00:05:24,800 --> 00:05:29,680
functions and we can group and structure

00:05:28,080 --> 00:05:32,880
those rust functions together

00:05:29,680 --> 00:05:35,280
so it looks like a regular rust api and

00:05:32,880 --> 00:05:36,800
that gives it a form and structure we

00:05:35,280 --> 00:05:39,840
expect when we're using

00:05:36,800 --> 00:05:39,840
a rust api

00:05:40,880 --> 00:05:44,560
and once this abstraction is built and

00:05:42,479 --> 00:05:45,919
tested then we can now we can use it

00:05:44,560 --> 00:05:48,000
without really worrying that there's

00:05:45,919 --> 00:05:50,320
actually c under there

00:05:48,000 --> 00:05:51,840
so it's important not to forget entirely

00:05:50,320 --> 00:05:54,960
but this is a good enough abstraction

00:05:51,840 --> 00:05:54,960
and it can take us a long way

00:05:58,960 --> 00:06:04,960
okay so to support different platforms

00:06:01,919 --> 00:06:07,600
tensorflow comes in several flavors

00:06:04,960 --> 00:06:09,199
so there's three i've got us on this

00:06:07,600 --> 00:06:10,960
slide

00:06:09,199 --> 00:06:12,240
and this is about the code that's

00:06:10,960 --> 00:06:15,919
running our influence

00:06:12,240 --> 00:06:18,720
step and um

00:06:15,919 --> 00:06:21,360
the three types differ on the platform

00:06:18,720 --> 00:06:24,400
that they're intended to run on

00:06:21,360 --> 00:06:25,039
and for each one there's a set of russ

00:06:24,400 --> 00:06:26,319
bindings

00:06:25,039 --> 00:06:28,319
these are the rust functions that have

00:06:26,319 --> 00:06:29,840
been written to the call

00:06:28,319 --> 00:06:31,600
the c functions in the tensorflow

00:06:29,840 --> 00:06:34,319
project

00:06:31,600 --> 00:06:35,680
and on the right turn column there

00:06:34,319 --> 00:06:39,919
there's the list of links to

00:06:35,680 --> 00:06:42,960
the relevant crates and crate style here

00:06:39,919 --> 00:06:44,000
and um for running on microcontrollers

00:06:42,960 --> 00:06:46,319
particularly

00:06:44,000 --> 00:06:48,560
um we're looking for crates that doesn't

00:06:46,319 --> 00:06:50,880
use the rough standard library

00:06:48,560 --> 00:06:52,160
and so only the bottom line on this

00:06:50,880 --> 00:06:54,479
chart

00:06:52,160 --> 00:06:56,319
has that property so this is useful for

00:06:54,479 --> 00:06:56,880
microcontrollers and it's also useful

00:06:56,319 --> 00:06:58,800
for

00:06:56,880 --> 00:07:00,319
other platforms that traditionally

00:06:58,800 --> 00:07:02,240
haven't had the standard library or

00:07:00,319 --> 00:07:03,759
where it's useful to save the space

00:07:02,240 --> 00:07:06,000
of not having the standard library in

00:07:03,759 --> 00:07:10,000
your binary

00:07:06,000 --> 00:07:10,000
so wasm is a good example of that

00:07:10,560 --> 00:07:14,080
there is a rust special interest group

00:07:12,240 --> 00:07:16,880
within the tensorflow projects

00:07:14,080 --> 00:07:17,840
and that's primarily concerns around the

00:07:16,880 --> 00:07:20,960
top line on the

00:07:17,840 --> 00:07:22,960
on the chart there um and the rest

00:07:20,960 --> 00:07:26,080
bindings there are sort of semi-official

00:07:22,960 --> 00:07:28,319
um google doesn't give them full support

00:07:26,080 --> 00:07:31,840
but but they are developed particularly

00:07:28,319 --> 00:07:31,840
by some google employees

00:07:32,479 --> 00:07:35,520
now the other interesting thing to note

00:07:34,000 --> 00:07:37,280
here is that tensorflow lite

00:07:35,520 --> 00:07:39,280
and below it tends to flow light for

00:07:37,280 --> 00:07:42,400
microcontrollers

00:07:39,280 --> 00:07:46,080
these have their own model format

00:07:42,400 --> 00:07:49,039
so this is a way of writing the model

00:07:46,080 --> 00:07:52,560
as a as a flat file format and we're

00:07:49,039 --> 00:07:52,560
going to encounter that in a few minutes

00:07:54,479 --> 00:08:00,000
okay so what have we seen so far

00:07:58,560 --> 00:08:01,680
so the last few slides have been about

00:08:00,000 --> 00:08:04,319
machine learning

00:08:01,680 --> 00:08:07,280
about tensorflow and how we can run

00:08:04,319 --> 00:08:09,680
inference on microcontrollers

00:08:07,280 --> 00:08:11,440
so the next few slides are going to be

00:08:09,680 --> 00:08:14,080
about using all of this in a real life

00:08:11,440 --> 00:08:14,080
rust project

00:08:14,319 --> 00:08:20,000
so we're going to go for typical rust

00:08:17,360 --> 00:08:23,440
projects where we use cargo

00:08:20,000 --> 00:08:26,560
so cargo is our build system and

00:08:23,440 --> 00:08:27,919
we add add the bindings crate to cargo

00:08:26,560 --> 00:08:29,039
in the usual way

00:08:27,919 --> 00:08:31,599
which is by adding it to our

00:08:29,039 --> 00:08:34,000
cargo.tomorrow file

00:08:31,599 --> 00:08:37,120
and you can find some details about the

00:08:34,000 --> 00:08:37,120
crate on crates that i owe

00:08:38,560 --> 00:08:41,039
and the really neat thing here is that

00:08:40,000 --> 00:08:42,800
this works out when we're

00:08:41,039 --> 00:08:45,279
cross-compiling as well

00:08:42,800 --> 00:08:46,800
uh particularly um cortex is probably

00:08:45,279 --> 00:08:50,080
the most commonly uh

00:08:46,800 --> 00:08:52,399
commonly used cross-compiling targets

00:08:50,080 --> 00:08:54,320
and the only difference to a normal

00:08:52,399 --> 00:08:56,000
busquet is that we need to have

00:08:54,320 --> 00:08:58,720
in this case a c-plus plus compiler

00:08:56,000 --> 00:08:58,720
behind the scenes

00:08:58,800 --> 00:09:04,080
um so yeah the rust

00:09:02,720 --> 00:09:06,560
ecosystem here is actually doing some

00:09:04,080 --> 00:09:08,399
great things

00:09:06,560 --> 00:09:09,680
there is one open issue i think that's

00:09:08,399 --> 00:09:11,200
been actively worked on

00:09:09,680 --> 00:09:12,800
um but for the moment it does need a

00:09:11,200 --> 00:09:14,399
nightly build flag

00:09:12,800 --> 00:09:16,880
um because we use some of the same

00:09:14,399 --> 00:09:17,600
crates on both on the host side of the

00:09:16,880 --> 00:09:20,399
build system

00:09:17,600 --> 00:09:23,839
and on the targets which card which

00:09:20,399 --> 00:09:23,839
cargo doesn't support by default

00:09:25,200 --> 00:09:29,600
okay so now we're going to get onto the

00:09:27,279 --> 00:09:29,600
code

00:09:29,839 --> 00:09:35,040
so in this example we're going to work

00:09:31,680 --> 00:09:37,440
towards performing inference

00:09:35,040 --> 00:09:39,440
so we finished the training step by

00:09:37,440 --> 00:09:42,360
exporting the model into a flat binary

00:09:39,440 --> 00:09:45,120
file and this is the file here

00:09:42,360 --> 00:09:46,880
microspeech.tflight

00:09:45,120 --> 00:09:48,240
to escape the training step we can use

00:09:46,880 --> 00:09:49,760
one of the example models from the

00:09:48,240 --> 00:09:52,959
tensorflow projects and

00:09:49,760 --> 00:09:52,959
in fact this is one of those

00:09:55,600 --> 00:09:59,279
so we want to get the bytes from our

00:09:58,560 --> 00:10:01,600
model file

00:09:59,279 --> 00:10:03,120
obviously into our program and the

00:10:01,600 --> 00:10:06,079
include bytes macro

00:10:03,120 --> 00:10:06,079
does exactly that

00:10:08,480 --> 00:10:12,800
on the next step tensorflow has some

00:10:11,360 --> 00:10:16,839
model validation and

00:10:12,800 --> 00:10:19,839
passing functions and we run them like

00:10:16,839 --> 00:10:19,839
this

00:10:21,680 --> 00:10:24,959
so there's two other things that we need

00:10:23,279 --> 00:10:27,200
to start doing inference

00:10:24,959 --> 00:10:28,480
and i'm going to skip over them here but

00:10:27,200 --> 00:10:30,640
i promise they're just one or two lines

00:10:28,480 --> 00:10:32,160
of code

00:10:30,640 --> 00:10:34,320
so we need something for looking up the

00:10:32,160 --> 00:10:35,360
dsp operations that are used as part of

00:10:34,320 --> 00:10:39,200
the

00:10:35,360 --> 00:10:41,839
as part of the influence process

00:10:39,200 --> 00:10:43,760
the default option is to use all the

00:10:41,839 --> 00:10:45,040
possible dsp operations supported by

00:10:43,760 --> 00:10:46,320
tensorflow

00:10:45,040 --> 00:10:48,560
and but the downside of that for

00:10:46,320 --> 00:10:50,480
microcontrollers is that it uses lots of

00:10:48,560 --> 00:10:52,720
flash memory

00:10:50,480 --> 00:10:54,160
so we can get around that by specifying

00:10:52,720 --> 00:10:56,480
only the operations we need at this

00:10:54,160 --> 00:10:56,480
stage

00:10:58,000 --> 00:11:04,880
the second thing we need here is

00:11:02,079 --> 00:11:05,519
tensorflow internally uses has its own

00:11:04,880 --> 00:11:07,760
stack

00:11:05,519 --> 00:11:11,040
or stack or heap memory that it uses and

00:11:07,760 --> 00:11:14,160
it allocates to its own memory allocator

00:11:11,040 --> 00:11:16,399
so we need to provide some memory for

00:11:14,160 --> 00:11:16,399
that

00:11:18,560 --> 00:11:22,560
okay so once we've got all those three

00:11:21,760 --> 00:11:25,279
things the model

00:11:22,560 --> 00:11:26,839
the opera solver and the tensor arena we

00:11:25,279 --> 00:11:29,839
can put them together to make an

00:11:26,839 --> 00:11:29,839
interpreter

00:11:33,839 --> 00:11:36,079
okay

00:11:36,959 --> 00:11:43,920
so at this point we actually need to

00:11:38,160 --> 00:11:46,000
come back to what our tensor is so

00:11:43,920 --> 00:11:47,120
in mathematics tensor tensor is used for

00:11:46,000 --> 00:11:49,360
various things

00:11:47,120 --> 00:11:50,959
i'm not going to explain that here but

00:11:49,360 --> 00:11:54,959
here we can think of the tensor as just

00:11:50,959 --> 00:11:57,760
being an n-dimensional array of data

00:11:54,959 --> 00:11:59,680
um and for putting it into the model

00:11:57,760 --> 00:12:02,560
actually we needs to be a flat

00:11:59,680 --> 00:12:04,079
flat away and there are some other

00:12:02,560 --> 00:12:05,360
methods you can call on the interpreter

00:12:04,079 --> 00:12:07,920
to find out what the

00:12:05,360 --> 00:12:08,399
what the dimensionality should be which

00:12:07,920 --> 00:12:11,519
is

00:12:08,399 --> 00:12:13,600
fixed by the model

00:12:11,519 --> 00:12:17,120
but in this case we has to be flattened

00:12:13,600 --> 00:12:17,120
down and we pass this as a slice

00:12:17,440 --> 00:12:25,120
and once that's done we're almost there

00:12:22,160 --> 00:12:25,519
so the invoke method actually runs the

00:12:25,120 --> 00:12:27,040
model

00:12:25,519 --> 00:12:30,320
and this is what takes the time normally

00:12:27,040 --> 00:12:32,160
so this is quite processor intensive

00:12:30,320 --> 00:12:34,880
and then after that we can get the

00:12:32,160 --> 00:12:34,880
output tensor

00:12:35,839 --> 00:12:41,440
so that's pretty much all there is to it

00:12:37,519 --> 00:12:44,800
um if you want to see all this

00:12:41,440 --> 00:12:46,079
this code in an example then in the tf

00:12:44,800 --> 00:12:49,040
micro crates

00:12:46,079 --> 00:12:50,000
it's uh it's one of the tests so you can

00:12:49,040 --> 00:12:53,600
see all this code

00:12:50,000 --> 00:12:57,120
making your full program

00:12:53,600 --> 00:13:00,720
okay so now it's time for a quick demo

00:12:57,120 --> 00:13:04,240
um so

00:13:00,720 --> 00:13:07,760
so hopefully what you can see is a

00:13:04,240 --> 00:13:10,959
stm32 development board and

00:13:07,760 --> 00:13:13,920
this is running one of the examples

00:13:10,959 --> 00:13:15,519
and the the output from the model the

00:13:13,920 --> 00:13:17,920
output tensor

00:13:15,519 --> 00:13:19,440
in this case the dimensionality is one

00:13:17,920 --> 00:13:20,959
by four

00:13:19,440 --> 00:13:23,120
and so there's four output values and

00:13:20,959 --> 00:13:27,040
these are being plotted on the bar graph

00:13:23,120 --> 00:13:28,800
on the screen so

00:13:27,040 --> 00:13:30,560
you may also get audio from this so

00:13:28,800 --> 00:13:31,360
hopefully it's someone saying the word

00:13:30,560 --> 00:13:34,639
no

00:13:31,360 --> 00:13:36,800
a couple of times and

00:13:34,639 --> 00:13:39,360
if the model detects that word as a

00:13:36,800 --> 00:13:41,440
keyword detection

00:13:39,360 --> 00:13:43,279
then the bar will go over a threshold i

00:13:41,440 --> 00:13:45,839
think 220

00:13:43,279 --> 00:13:48,160
and the bar changes color to signify the

00:13:45,839 --> 00:13:51,519
phrase that's being detected

00:13:48,160 --> 00:13:51,519
so i'm just going to play the video

00:13:59,600 --> 00:14:08,000
and just once more

00:14:03,279 --> 00:14:08,000
no no

00:14:09,920 --> 00:14:16,240
okay um so

00:14:14,240 --> 00:14:17,279
this model probably isn't the um you

00:14:16,240 --> 00:14:17,760
know it's probably not the greatest

00:14:17,279 --> 00:14:19,199
model

00:14:17,760 --> 00:14:22,480
yeah it's probably not the most robust

00:14:19,199 --> 00:14:25,680
but it does work as a superficial level

00:14:22,480 --> 00:14:29,120
and um so there we go

00:14:25,680 --> 00:14:31,760
um one interesting thing to note here

00:14:29,120 --> 00:14:32,800
is that the output data is is eight bit

00:14:31,760 --> 00:14:35,040
integers so

00:14:32,800 --> 00:14:36,880
it's over and unsigned in so the

00:14:35,040 --> 00:14:38,399
possible values are between zero and two

00:14:36,880 --> 00:14:40,800
five five

00:14:38,399 --> 00:14:42,720
um so this has become because the model

00:14:40,800 --> 00:14:44,320
has been quantized

00:14:42,720 --> 00:14:45,920
so normally during training the model

00:14:44,320 --> 00:14:49,040
weights and the inputs and outputs

00:14:45,920 --> 00:14:51,279
are normally floating point values um

00:14:49,040 --> 00:14:53,120
but at the end of the training step you

00:14:51,279 --> 00:14:56,240
can use quantization to reduce this to

00:14:53,120 --> 00:14:57,839
eight bits operations

00:14:56,240 --> 00:14:59,920
and at the same time you keep track to

00:14:57,839 --> 00:15:00,880
make sure that the accuracy of the model

00:14:59,920 --> 00:15:04,240
isn't being degraded

00:15:00,880 --> 00:15:05,920
too much so when generating models from

00:15:04,240 --> 00:15:07,920
microcontrollers

00:15:05,920 --> 00:15:10,839
this is really important because it

00:15:07,920 --> 00:15:12,639
significantly increases the execution

00:15:10,839 --> 00:15:15,279
speed

00:15:12,639 --> 00:15:15,839
um so you should probably you should do

00:15:15,279 --> 00:15:16,720
this

00:15:15,839 --> 00:15:18,079
when creating a model for

00:15:16,720 --> 00:15:21,440
microcontrollers unless you're

00:15:18,079 --> 00:15:21,440
specifically trying something else

00:15:22,079 --> 00:15:30,560
okay back to the slides

00:15:27,199 --> 00:15:32,880
okay so let's step back a bit um

00:15:30,560 --> 00:15:35,120
and talk about the crates that we've

00:15:32,880 --> 00:15:37,360
used to do this

00:15:35,120 --> 00:15:41,040
so this is an open source crate written

00:15:37,360 --> 00:15:44,320
by myself and kevin hill and

00:15:41,040 --> 00:15:48,160
it's on craits.io it's documented

00:15:44,320 --> 00:15:49,759
it's got tests it's got ci um

00:15:48,160 --> 00:15:51,680
it's partially checked against the rust

00:15:49,759 --> 00:15:53,279
api guidelines

00:15:51,680 --> 00:15:54,720
and probably could do with a little more

00:15:53,279 --> 00:15:56,480
work there but

00:15:54,720 --> 00:16:00,160
it's pretty usable and it follows the

00:15:56,480 --> 00:16:00,160
usual rust patterns that we expect

00:16:04,160 --> 00:16:09,759
okay so challenges around maintaining

00:16:07,759 --> 00:16:13,120
equate like this

00:16:09,759 --> 00:16:16,320
um so the tensorflow projects as a whole

00:16:13,120 --> 00:16:17,120
gets about 500 commits a week so that's

00:16:16,320 --> 00:16:20,959
a pretty big

00:16:17,120 --> 00:16:22,000
velocity of change the tensorflow micro

00:16:20,959 --> 00:16:25,120
subsets

00:16:22,000 --> 00:16:26,720
is not so bad but still there's several

00:16:25,120 --> 00:16:29,519
people working full time on this

00:16:26,720 --> 00:16:32,079
and the c api does change fairly

00:16:29,519 --> 00:16:32,079
regularly

00:16:33,040 --> 00:16:35,519
however the good thing about ross

00:16:34,320 --> 00:16:37,199
bindings is they give us some

00:16:35,519 --> 00:16:39,040
flexibility

00:16:37,199 --> 00:16:42,160
and actually for some of the recent

00:16:39,040 --> 00:16:42,160
changes we've been able to

00:16:42,240 --> 00:16:45,360
refactor for the changes in the c plus

00:16:43,839 --> 00:16:51,839
plus api

00:16:45,360 --> 00:16:51,839
but the rest api remains unchanged

00:16:56,560 --> 00:17:01,839
okay um so why do this invest

00:17:03,120 --> 00:17:07,280
so um some of the benefits of bust here

00:17:05,760 --> 00:17:09,439
is really the tooling

00:17:07,280 --> 00:17:11,839
so not the language but actually the

00:17:09,439 --> 00:17:14,959
ecosystems of tools like cargo

00:17:11,839 --> 00:17:16,799
the cc crates and bind gen um that

00:17:14,959 --> 00:17:19,600
really help us out

00:17:16,799 --> 00:17:21,360
so without cargo integrating build

00:17:19,600 --> 00:17:23,199
systems from a microcontroller vendor

00:17:21,360 --> 00:17:26,400
with tensorflow's own

00:17:23,199 --> 00:17:27,679
can be quite a task and

00:17:26,400 --> 00:17:29,760
there's actually various development

00:17:27,679 --> 00:17:32,400
boards that advertise having

00:17:29,760 --> 00:17:34,320
a tensorflow build system and hardwares

00:17:32,400 --> 00:17:35,760
and hardware examples

00:17:34,320 --> 00:17:37,760
and they advertise this as a feature

00:17:35,760 --> 00:17:39,600
when selling real hardware

00:17:37,760 --> 00:17:41,520
and of course if you're trying to run

00:17:39,600 --> 00:17:44,559
tensorflow this this is worth

00:17:41,520 --> 00:17:47,600
paying a small premium for

00:17:44,559 --> 00:17:48,799
however with cargo um this is just a

00:17:47,600 --> 00:17:50,480
standard

00:17:48,799 --> 00:17:53,679
uh here we have a standard build system

00:17:50,480 --> 00:17:53,679
and this all comes for free

00:17:54,720 --> 00:17:58,320
so the rust system the rust ecosystem

00:17:56,960 --> 00:18:01,039
gives us this great build

00:17:58,320 --> 00:18:03,440
tooling and we can easily add tensorflow

00:18:01,039 --> 00:18:05,280
to an existing project

00:18:03,440 --> 00:18:07,039
it also allows us very easily to build

00:18:05,280 --> 00:18:09,120
for multiple targets

00:18:07,039 --> 00:18:10,400
so we can use the same crate for testing

00:18:09,120 --> 00:18:13,039
on

00:18:10,400 --> 00:18:14,559
a desktop pc and then exactly the same

00:18:13,039 --> 00:18:17,200
code then gets cross-compiled and runs

00:18:14,559 --> 00:18:20,080
on a microcontroller

00:18:17,200 --> 00:18:21,520
um however to achieve this kind of

00:18:20,080 --> 00:18:24,160
composability

00:18:21,520 --> 00:18:25,780
of the library the language also plays a

00:18:24,160 --> 00:18:26,960
role

00:18:25,780 --> 00:18:28,960
[Music]

00:18:26,960 --> 00:18:30,000
so if the language allows memory a

00:18:28,960 --> 00:18:31,360
listing

00:18:30,000 --> 00:18:33,360
then we actually have to manually

00:18:31,360 --> 00:18:35,600
evaluate evaluate the memory usage

00:18:33,360 --> 00:18:37,760
all the different component parts in

00:18:35,600 --> 00:18:40,160
order to make sure we're building a free

00:18:37,760 --> 00:18:40,160
program

00:18:41,120 --> 00:18:45,440
however with a memory safe language like

00:18:43,200 --> 00:18:46,720
rust we can delegate this task to the

00:18:45,440 --> 00:18:48,240
compiler

00:18:46,720 --> 00:18:50,640
i'm trusting it to error if there's a

00:18:48,240 --> 00:18:50,640
mistake

00:18:51,760 --> 00:18:57,039
okay so time for some codes

00:18:55,200 --> 00:18:58,720
okay so if you remember that function a

00:18:57,039 --> 00:18:59,280
few slides ago where we created the

00:18:58,720 --> 00:19:01,679
micro

00:18:59,280 --> 00:19:03,440
interpreter out of the three component

00:19:01,679 --> 00:19:07,360
parts uh so the model

00:19:03,440 --> 00:19:07,360
the op resolver and the tensor arena

00:19:08,640 --> 00:19:14,000
so here it is as a c function definition

00:19:11,039 --> 00:19:17,360
uh taken out of the tensorflow project

00:19:14,000 --> 00:19:19,120
and um okay that's

00:19:17,360 --> 00:19:20,400
pretty big comments i'm just going to

00:19:19,120 --> 00:19:22,960
summarize it so

00:19:20,400 --> 00:19:24,480
it says that the lifetime of all these

00:19:22,960 --> 00:19:28,160
things

00:19:24,480 --> 00:19:31,440
must be at least as long as the lifetime

00:19:28,160 --> 00:19:32,720
of the interpreter object

00:19:31,440 --> 00:19:36,080
and then it gives you some advice for

00:19:32,720 --> 00:19:39,440
doing this in a typical project

00:19:36,080 --> 00:19:41,520
so um this

00:19:39,440 --> 00:19:44,000
if you've been programming in a language

00:19:41,520 --> 00:19:46,080
of manual memory management for a while

00:19:44,000 --> 00:19:47,039
then you can understand this and you can

00:19:46,080 --> 00:19:50,480
do this

00:19:47,039 --> 00:19:51,679
um but um

00:19:50,480 --> 00:19:53,520
so we want to make things more

00:19:51,679 --> 00:19:55,760
accessible and

00:19:53,520 --> 00:19:57,760
and also in the real world things change

00:19:55,760 --> 00:19:59,679
and um

00:19:57,760 --> 00:20:02,400
you need to keep you would need to keep

00:19:59,679 --> 00:20:05,120
uh keep checking this invariant

00:20:02,400 --> 00:20:07,840
for all the time your project changes

00:20:05,120 --> 00:20:07,840
and that's not so great

00:20:10,720 --> 00:20:16,960
what lost allows us to do is is model

00:20:15,360 --> 00:20:21,679
model this constraint actually in the

00:20:16,960 --> 00:20:25,039
language as lifetime annotations

00:20:21,679 --> 00:20:26,640
so here's some must so this is a pretty

00:20:25,039 --> 00:20:28,880
big function definition

00:20:26,640 --> 00:20:30,559
um some moderately advanced to us going

00:20:28,880 --> 00:20:32,080
on here

00:20:30,559 --> 00:20:35,840
i'm just going to give you a few seconds

00:20:32,080 --> 00:20:35,840
to read it

00:20:41,200 --> 00:20:47,200
okay so

00:20:44,640 --> 00:20:48,080
this function says something like so

00:20:47,200 --> 00:20:50,880
we're going to implement

00:20:48,080 --> 00:20:53,039
the micro interpreter with its lifetime

00:20:50,880 --> 00:20:55,360
a

00:20:53,039 --> 00:20:56,400
and then the function is public function

00:20:55,360 --> 00:20:58,640
new

00:20:56,400 --> 00:21:00,080
and it has the lifetime m which exceeds

00:20:58,640 --> 00:21:02,320
lifetime a

00:21:00,080 --> 00:21:03,760
and the lifetime t which exceeds

00:21:02,320 --> 00:21:07,280
lifetime a

00:21:03,760 --> 00:21:09,919
and some generic types

00:21:07,280 --> 00:21:11,039
then when we pass the model into the

00:21:09,919 --> 00:21:12,640
function

00:21:11,039 --> 00:21:14,080
we tell the rust compiler that we're

00:21:12,640 --> 00:21:16,400
borrowing the model

00:21:14,080 --> 00:21:18,000
for the lifetime m which from the

00:21:16,400 --> 00:21:18,960
previous line we know exceeds the

00:21:18,000 --> 00:21:21,600
lifetime

00:21:18,960 --> 00:21:22,240
of the micro interpreter so actually

00:21:21,600 --> 00:21:25,440
we've now

00:21:22,240 --> 00:21:27,520
enforced that constraint in software

00:21:25,440 --> 00:21:30,640
and that means that when we do a ci

00:21:27,520 --> 00:21:33,760
build and this constraint gets checked

00:21:30,640 --> 00:21:33,760
and that's really useful

00:21:34,720 --> 00:21:39,200
um there's an extra some

00:21:38,000 --> 00:21:40,960
more advanced rust going on at the

00:21:39,200 --> 00:21:43,039
bottom here where the tensor arena can

00:21:40,960 --> 00:21:46,640
either be immutably borrowed

00:21:43,039 --> 00:21:47,679
from the stack or come from the heap so

00:21:46,640 --> 00:21:49,760
if you're running a larger

00:21:47,679 --> 00:21:51,679
microcontroller projects you might have

00:21:49,760 --> 00:21:54,159
an allocator for your heap

00:21:51,679 --> 00:21:56,400
and in this case you can pass either

00:21:54,159 --> 00:21:56,400
here

00:21:57,200 --> 00:21:59,840
okay

00:22:03,600 --> 00:22:10,960
so quick summary

00:22:07,840 --> 00:22:12,320
so we can make use of tensorflow i like

00:22:10,960 --> 00:22:15,919
for microcontrollers

00:22:12,320 --> 00:22:17,360
in rust through building and sharing

00:22:15,919 --> 00:22:19,200
rust binaries we can make use of

00:22:17,360 --> 00:22:22,400
tensorflow and many other

00:22:19,200 --> 00:22:26,799
big c and c plus code bases that are

00:22:22,400 --> 00:22:28,640
useful for embedded projects

00:22:26,799 --> 00:22:30,240
rust gives us some great advantages with

00:22:28,640 --> 00:22:32,960
consistent build tooling

00:22:30,240 --> 00:22:34,559
combined with a memory safe language and

00:22:32,960 --> 00:22:36,480
we can easily compose

00:22:34,559 --> 00:22:37,840
a program with multiple complex

00:22:36,480 --> 00:22:41,840
libraries without

00:22:37,840 --> 00:22:41,840
introducing new bugs

00:22:44,320 --> 00:22:48,080
the rust language also helps maintenance

00:22:46,559 --> 00:22:50,480
because we can make sure we're keeping

00:22:48,080 --> 00:22:52,000
up these invariants

00:22:50,480 --> 00:22:55,840
with the compiler and the compiler can

00:22:52,000 --> 00:22:55,840
check this in continuous integration

00:22:59,200 --> 00:23:04,640
okay great so that's the end of this

00:23:01,039 --> 00:23:09,840
slide i'm happy to take a few questions

00:23:04,640 --> 00:23:09,840
but otherwise thank you for listening

00:23:10,880 --> 00:23:15,440
yeah thank you for the talk and

00:23:15,600 --> 00:23:20,159
i'm looking this talk also sparked a lot

00:23:19,120 --> 00:23:24,080
of conversation

00:23:20,159 --> 00:23:25,840
in the speaker's room and uh

00:23:24,080 --> 00:23:27,600
i'll have to see how many of those

00:23:25,840 --> 00:23:30,559
questions are actually not asking other

00:23:27,600 --> 00:23:30,559
people questions

00:23:34,840 --> 00:23:40,880
um

00:23:37,840 --> 00:23:42,880
ah here question for richard meadows

00:23:40,880 --> 00:23:45,120
what kind of data applications do you

00:23:42,880 --> 00:23:46,559
recommend using ml on microcontrollers

00:23:45,120 --> 00:23:50,400
for

00:23:46,559 --> 00:23:54,159
so um so the models that we

00:23:50,400 --> 00:23:55,919
that i've demoed here are all um

00:23:54,159 --> 00:23:57,919
so they're all kind of linear models so

00:23:55,919 --> 00:24:00,000
what that means is that

00:23:57,919 --> 00:24:01,039
the the input data goes through a series

00:24:00,000 --> 00:24:04,159
of transformations

00:24:01,039 --> 00:24:08,240
a series of dsp operations

00:24:04,159 --> 00:24:08,799
um and they they form a kind of linear

00:24:08,240 --> 00:24:11,840
graph

00:24:08,799 --> 00:24:13,520
to get to the outputs um

00:24:11,840 --> 00:24:15,200
so this is good for things like uh

00:24:13,520 --> 00:24:18,480
classifiers so

00:24:15,200 --> 00:24:21,679
um we demo here with speech classifier

00:24:18,480 --> 00:24:25,200
um it can also be used uh some very

00:24:21,679 --> 00:24:28,960
basic image classification is possible

00:24:25,200 --> 00:24:31,039
um and also things like classifying

00:24:28,960 --> 00:24:32,640
accelerometer movements so gesture

00:24:31,039 --> 00:24:36,320
detection

00:24:32,640 --> 00:24:37,440
um i think um there are other more

00:24:36,320 --> 00:24:42,080
advanced forms

00:24:37,440 --> 00:24:46,640
of um of machine learning here we

00:24:42,080 --> 00:24:48,799
aren't supported so there's uh

00:24:46,640 --> 00:24:50,159
there's long long-term short-term memory

00:24:48,799 --> 00:24:51,679
and other kind of recursive structures

00:24:50,159 --> 00:24:52,640
that tensorflow micro doesn't yet

00:24:51,679 --> 00:24:55,120
support

00:24:52,640 --> 00:24:55,919
um so we can't do everything um so i

00:24:55,120 --> 00:24:57,760
think

00:24:55,919 --> 00:24:59,279
yeah in short we can do classification

00:24:57,760 --> 00:25:01,840
um but i think big

00:24:59,279 --> 00:25:03,200
sort of large data processing is is not

00:25:01,840 --> 00:25:06,400
yet

00:25:03,200 --> 00:25:06,400
ready for micro controllers

00:25:07,440 --> 00:25:12,960
thank you i

00:25:10,799 --> 00:25:14,320
unless someone quickly posts a question

00:25:12,960 --> 00:25:16,880
i have seen

00:25:14,320 --> 00:25:18,400
no other questions to you but as i said

00:25:16,880 --> 00:25:19,440
a lot of discussion that you can hop

00:25:18,400 --> 00:25:22,320
into

00:25:19,440 --> 00:25:24,320
and thank you for that talk oh here's

00:25:22,320 --> 00:25:28,320
here's one question

00:25:24,320 --> 00:25:31,039
um how limited

00:25:28,320 --> 00:25:33,039
is a tf light for microcontrollers

00:25:31,039 --> 00:25:34,880
feature wise compared to the regular tf

00:25:33,039 --> 00:25:37,679
light which is already limited compared

00:25:34,880 --> 00:25:40,000
to the regular tensorflow

00:25:37,679 --> 00:25:41,039
okay yes so i think part of that is what

00:25:40,000 --> 00:25:44,480
we're just talking about

00:25:41,039 --> 00:25:45,200
that that's things like the recursive

00:25:44,480 --> 00:25:46,960
networks

00:25:45,200 --> 00:25:48,400
and long-term short-term memory isn't

00:25:46,960 --> 00:25:52,080
yet supported in

00:25:48,400 --> 00:25:53,760
tensorflow lite um my understanding is

00:25:52,080 --> 00:25:56,320
that tensorflow lite for microcontroller

00:25:53,760 --> 00:25:57,520
supports many of the many of the

00:25:56,320 --> 00:26:00,720
operations

00:25:57,520 --> 00:26:02,240
that are supported in lights so i think

00:26:00,720 --> 00:26:04,400
it's definitely a subset of the overall

00:26:02,240 --> 00:26:05,679
tensorflow projects

00:26:04,400 --> 00:26:08,400
but many of the features are now

00:26:05,679 --> 00:26:08,400
trickling down

00:26:09,200 --> 00:26:14,559
thank you okay then

00:26:12,799 --> 00:26:15,840
with that i'd leave you into the

00:26:14,559 --> 00:26:25,840
discussion

00:26:15,840 --> 00:26:25,840
and we'll see us after the break

00:26:27,360 --> 00:26:29,440

YouTube URL: https://www.youtube.com/watch?v=DUVE86yTfKU


