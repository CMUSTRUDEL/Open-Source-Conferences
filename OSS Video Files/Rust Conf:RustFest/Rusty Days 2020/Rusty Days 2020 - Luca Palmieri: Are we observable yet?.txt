Title: Rusty Days 2020 - Luca Palmieri: Are we observable yet?
Publication date: 2020-08-02
Playlist: Rusty Days 2020
Description: 
	Agenda ► https://rusty-days.org/agenda
Slides ►https://rusty-days.org/assets/slides/07-are-we-observable-yet.pdf
Playlist with all talks ► https://www.youtube.com/playlist?list=PLf3u8NhoEikhTC5radGrmmqdkOK-xMDoZ

Follow ►
Facebook: https://rusty-days.org/facebook
Twitch: https://rusty-days.org/twitch
Twitter: https://rusty-days.org/twitter

This video ►
Is Rust ready for mainstream usage in backend development?

There is a lot of buzz around web frameworks while many other (critical!) Day 2 concerns do not get nearly as much attention.

We will discuss observability: do the tools currently available in the Rust ecosystem cover most of your telemetry needs?

I will walk you through our journey here at TrueLayer when we built our first production backend system in Rust, Donate Direct.

We will be touching on the state of Rust tooling for logging, metrics and distributed tracing.
Captions: 
	00:00:04,960 --> 00:00:08,559
amazing

00:00:06,480 --> 00:00:10,480
good evening everybody uh thanks for

00:00:08,559 --> 00:00:13,440
joining for the last day of

00:00:10,480 --> 00:00:14,639
rusty days we're gonna chat for the next

00:00:13,440 --> 00:00:17,279
30 minutes or so

00:00:14,639 --> 00:00:18,320
about observability in particular we're

00:00:17,279 --> 00:00:20,960
going to discuss

00:00:18,320 --> 00:00:21,600
if the rust ecosystem at this point in

00:00:20,960 --> 00:00:23,039
time

00:00:21,600 --> 00:00:25,119
provides enough tooling to provide

00:00:23,039 --> 00:00:26,880
observable apis and we're going to go

00:00:25,119 --> 00:00:29,199
through the journey of writing one and

00:00:26,880 --> 00:00:31,920
see how that came along

00:00:29,199 --> 00:00:33,600
my name is luca palmieri i work as a

00:00:31,920 --> 00:00:34,480
leave engineer through layer we're going

00:00:33,600 --> 00:00:37,040
to spend some

00:00:34,480 --> 00:00:37,600
words about that in a second in the

00:00:37,040 --> 00:00:40,079
rustic

00:00:37,600 --> 00:00:40,719
system i contribute to the rust longer

00:00:40,079 --> 00:00:43,760
user group

00:00:40,719 --> 00:00:46,000
where i curate the code dojo

00:00:43,760 --> 00:00:47,120
i've been contributor and maintainer of

00:00:46,000 --> 00:00:49,280
variety of crates

00:00:47,120 --> 00:00:51,520
in the open source ecosystem lympha

00:00:49,280 --> 00:00:53,039
wiremock and some others

00:00:51,520 --> 00:00:54,719
and i'm currently writing zero to

00:00:53,039 --> 00:00:56,879
production which is a book

00:00:54,719 --> 00:00:58,000
on brass became development which i

00:00:56,879 --> 00:01:00,239
publish chapter

00:00:58,000 --> 00:01:01,039
by chapter on my blog which you can see

00:01:00,239 --> 00:01:04,799
linked

00:01:01,039 --> 00:01:07,600
down there so uh let's get to the meet

00:01:04,799 --> 00:01:09,040
what we're gonna discuss tonight this is

00:01:07,600 --> 00:01:11,520
a little bit our agenda

00:01:09,040 --> 00:01:13,200
so we're gonna see what donate direct is

00:01:11,520 --> 00:01:15,680
trying to write is an application

00:01:13,200 --> 00:01:17,520
and it's going to drive a role journey

00:01:15,680 --> 00:01:19,920
we're going to see what it entailed

00:01:17,520 --> 00:01:21,520
to bring that application to production

00:01:19,920 --> 00:01:23,600
and then we're going to zoom in

00:01:21,520 --> 00:01:24,880
on three types of telemetry data which

00:01:23,600 --> 00:01:26,880
are often collected

00:01:24,880 --> 00:01:28,560
to observe the behavior of applications

00:01:26,880 --> 00:01:32,640
in production environments

00:01:28,560 --> 00:01:34,000
metrics logging and distributed traces

00:01:32,640 --> 00:01:35,360
if you don't know what they are or you

00:01:34,000 --> 00:01:36,880
have an experience working with them

00:01:35,360 --> 00:01:38,799
before that's not a problem

00:01:36,880 --> 00:01:40,159
we're gonna give all the details and

00:01:38,799 --> 00:01:41,759
i'll walk you through

00:01:40,159 --> 00:01:43,360
why they're useful and how we collect

00:01:41,759 --> 00:01:45,439
them so

00:01:43,360 --> 00:01:47,119
let's start from the very basics what is

00:01:45,439 --> 00:01:50,079
donate direct

00:01:47,119 --> 00:01:50,399
well before that let's get two words on

00:01:50,079 --> 00:01:52,840
what

00:01:50,399 --> 00:01:55,119
layer does which is gonna frame the

00:01:52,840 --> 00:01:56,560
conversation now truly is a company

00:01:55,119 --> 00:01:57,920
which are praised in the financial

00:01:56,560 --> 00:02:00,880
technology space

00:01:57,920 --> 00:02:02,240
in particular we provide apis for people

00:02:00,880 --> 00:02:05,200
to consume

00:02:02,240 --> 00:02:06,880
we mainly provide two types of apis one

00:02:05,200 --> 00:02:08,800
for accessing banking data

00:02:06,880 --> 00:02:10,399
on behalf of a user and then we want to

00:02:08,800 --> 00:02:12,560
initiate a banking transfer

00:02:10,399 --> 00:02:14,560
once again on the alphabet user so you

00:02:12,560 --> 00:02:16,560
pay using your own bank account without

00:02:14,560 --> 00:02:18,080
credit cards without intermediaries of

00:02:16,560 --> 00:02:20,319
other type

00:02:18,080 --> 00:02:22,480
during the kovit pandemic as many of the

00:02:20,319 --> 00:02:24,800
people we kind of tried to

00:02:22,480 --> 00:02:27,120
think what we could do in any way to

00:02:24,800 --> 00:02:30,319
help relieve pressure or contribute

00:02:27,120 --> 00:02:31,680
to what was happening so myself with a

00:02:30,319 --> 00:02:33,440
group of other colleagues

00:02:31,680 --> 00:02:34,720
put together an application called

00:02:33,440 --> 00:02:36,480
donate direct

00:02:34,720 --> 00:02:38,560
which lets you use our payment

00:02:36,480 --> 00:02:39,599
initiation technology to donate money to

00:02:38,560 --> 00:02:42,319
charities

00:02:39,599 --> 00:02:43,599
so as you can see in the gif on the left

00:02:42,319 --> 00:02:46,720
the flow is very simple

00:02:43,599 --> 00:02:49,680
so you select a charity from a list

00:02:46,720 --> 00:02:51,040
you specify how much amount you want to

00:02:49,680 --> 00:02:53,120
donate

00:02:51,040 --> 00:02:55,519
then you fill in some tax stuff and you

00:02:53,120 --> 00:02:57,760
get redirected to the flow of your bank

00:02:55,519 --> 00:02:59,920
and the money goes through your bank

00:02:57,760 --> 00:03:01,120
account to the charity without any fee

00:02:59,920 --> 00:03:05,120
so today you did this

00:03:01,120 --> 00:03:07,280
completely free of charge some donations

00:03:05,120 --> 00:03:09,599
now as it happens when you do site

00:03:07,280 --> 00:03:11,920
projects of different kinds so

00:03:09,599 --> 00:03:13,760
things that are a little bit outside the

00:03:11,920 --> 00:03:15,360
main product line

00:03:13,760 --> 00:03:17,360
you have a chance to experiment with

00:03:15,360 --> 00:03:18,000
technologies which will be considered a

00:03:17,360 --> 00:03:21,120
little bit too

00:03:18,000 --> 00:03:22,959
edgy to be used in the core product

00:03:21,120 --> 00:03:24,159
and as you might imagine considering

00:03:22,959 --> 00:03:26,640
that this is a rust talk

00:03:24,159 --> 00:03:27,680
in advance conference uh the network

00:03:26,640 --> 00:03:30,799
back-end api

00:03:27,680 --> 00:03:34,000
is very robust uh now

00:03:30,799 --> 00:03:35,040
it's not our first round with thrust but

00:03:34,000 --> 00:03:37,040
it was our first

00:03:35,040 --> 00:03:38,959
rust api in production here through

00:03:37,040 --> 00:03:40,480
layer it was the first time we were

00:03:38,959 --> 00:03:42,319
actually shipping code that was

00:03:40,480 --> 00:03:45,519
responding interactively

00:03:42,319 --> 00:03:47,360
to users coming from the wild web

00:03:45,519 --> 00:03:49,040
so i want to see lots of emojis when i

00:03:47,360 --> 00:03:50,239
re-watch the stream at this specific

00:03:49,040 --> 00:03:52,319
slide

00:03:50,239 --> 00:03:54,239
now as i said we experimented with it

00:03:52,319 --> 00:03:56,640
before so we were doing build tooling we

00:03:54,239 --> 00:03:58,560
were doing clies we were doing some we

00:03:56,640 --> 00:04:00,319
had kevin these controllers

00:03:58,560 --> 00:04:02,480
for non-critical stuff and so on and so

00:04:00,319 --> 00:04:04,640
forth but once you actually

00:04:02,480 --> 00:04:07,120
put an api in front of a user then the

00:04:04,640 --> 00:04:10,560
bar of our that api needs to be

00:04:07,120 --> 00:04:15,439
raised significantly which brings us

00:04:10,560 --> 00:04:18,479
to our journey to production

00:04:15,439 --> 00:04:19,759
now to use the words from someone that's

00:04:18,479 --> 00:04:22,479
wiser than myself

00:04:19,759 --> 00:04:24,240
one does not simply work into production

00:04:22,479 --> 00:04:26,320
for a variety of reason

00:04:24,240 --> 00:04:28,320
and reason number one is that generally

00:04:26,320 --> 00:04:30,080
speaking production environments are

00:04:28,320 --> 00:04:33,199
very complex

00:04:30,080 --> 00:04:34,400
so if we look at this diagram this

00:04:33,199 --> 00:04:37,520
depicts

00:04:34,400 --> 00:04:38,639
monza's production environment so each

00:04:37,520 --> 00:04:41,360
of the blue dots

00:04:38,639 --> 00:04:43,040
is a microservice in monsters cluster

00:04:41,360 --> 00:04:43,759
and each of the lines connecting two

00:04:43,040 --> 00:04:45,680
dots

00:04:43,759 --> 00:04:47,040
are microservices talking to each other

00:04:45,680 --> 00:04:49,199
over the network

00:04:47,040 --> 00:04:51,440
now with layer is not monster so it

00:04:49,199 --> 00:04:52,400
doesn't have a half thousand six hundred

00:04:51,440 --> 00:04:55,360
microservices

00:04:52,400 --> 00:04:56,080
interacting in production but you might

00:04:55,360 --> 00:04:58,400
imagine the

00:04:56,080 --> 00:05:01,199
production environment is equally

00:04:58,400 --> 00:05:03,520
complex in many subtle ways

00:05:01,199 --> 00:05:04,800
and what generally you try to plan for

00:05:03,520 --> 00:05:07,759
in a production environment

00:05:04,800 --> 00:05:09,600
is not even really the epicase so is

00:05:07,759 --> 00:05:10,320
stuff actually working but you try to

00:05:09,600 --> 00:05:13,680
predict

00:05:10,320 --> 00:05:15,600
or to mitigate the way stuff can fail

00:05:13,680 --> 00:05:17,919
so what happens if one of those blue

00:05:15,600 --> 00:05:18,800
dots for example in the monster cluster

00:05:17,919 --> 00:05:20,560
goes down

00:05:18,800 --> 00:05:23,039
what happens in one of those blue dots

00:05:20,560 --> 00:05:25,360
start responding more slowly than

00:05:23,039 --> 00:05:27,039
it generally does or is supposed to do

00:05:25,360 --> 00:05:29,680
or it doesn't elastically

00:05:27,039 --> 00:05:30,560
react to certain traffic all these kind

00:05:29,680 --> 00:05:33,440
of behaviors

00:05:30,560 --> 00:05:34,880
in a very connected graph like that can

00:05:33,440 --> 00:05:37,440
cause cascading figures

00:05:34,880 --> 00:05:38,880
and it becomes very very difficult once

00:05:37,440 --> 00:05:40,080
something like that is happening to

00:05:38,880 --> 00:05:43,919
troubleshoot y

00:05:40,080 --> 00:05:47,120
and fix it if possible now what does

00:05:43,919 --> 00:05:49,680
each of those blue dots actually is

00:05:47,120 --> 00:05:51,120
now into layer's case we run a

00:05:49,680 --> 00:05:52,800
kubernetes cluster so

00:05:51,120 --> 00:05:54,960
all our production applications are

00:05:52,800 --> 00:05:56,800
deployed on top of kubernetes

00:05:54,960 --> 00:05:58,080
which means those blue dots are

00:05:56,800 --> 00:06:00,800
kubernetes deployment

00:05:58,080 --> 00:06:02,319
cubenase deployment is just a service

00:06:00,800 --> 00:06:04,240
definition

00:06:02,319 --> 00:06:06,160
which is going to orchestrate a bunch of

00:06:04,240 --> 00:06:09,039
copies of the application

00:06:06,160 --> 00:06:11,039
each of those copies is called a pod and

00:06:09,039 --> 00:06:13,840
the pod may be composed on one or more

00:06:11,039 --> 00:06:15,520
docker containers

00:06:13,840 --> 00:06:17,520
the pods are identical to one another

00:06:15,520 --> 00:06:20,560
and so they can be dynamically scaled

00:06:17,520 --> 00:06:22,160
to match traffic increasing and they can

00:06:20,560 --> 00:06:23,520
also be on different machines in order

00:06:22,160 --> 00:06:26,319
to give us redundancy

00:06:23,520 --> 00:06:28,800
if one of those machines ends up going

00:06:26,319 --> 00:06:30,800
down for whatever reason

00:06:28,800 --> 00:06:32,800
now what does it mean to release

00:06:30,800 --> 00:06:35,120
something to production

00:06:32,800 --> 00:06:36,479
error through layer um especially when

00:06:35,120 --> 00:06:37,600
you look at things from an operational

00:06:36,479 --> 00:06:39,600
perspective

00:06:37,600 --> 00:06:40,639
you want to have a certain set of

00:06:39,600 --> 00:06:44,080
guarantees

00:06:40,639 --> 00:06:46,080
about what each of those applications

00:06:44,080 --> 00:06:49,039
provides from an operational point of

00:06:46,080 --> 00:06:51,199
view this means is you want to be sure

00:06:49,039 --> 00:06:53,919
that the set of best practices

00:06:51,199 --> 00:06:56,080
uh is being followed consistently all

00:06:53,919 --> 00:06:58,560
those best practices are collected

00:06:56,080 --> 00:07:00,720
in a huge checklist called the

00:06:58,560 --> 00:07:03,039
pre-production checklist

00:07:00,720 --> 00:07:04,639
now if you are an on-call engineer the

00:07:03,039 --> 00:07:05,360
pre-production checks list is in many

00:07:04,639 --> 00:07:07,520
ways

00:07:05,360 --> 00:07:09,199
a very nice thing in the sense that it

00:07:07,520 --> 00:07:11,199
gives you a baseline

00:07:09,199 --> 00:07:13,680
uh level of quality especially on the

00:07:11,199 --> 00:07:15,599
observability side as we're gonna see

00:07:13,680 --> 00:07:17,680
and you can be sure that those metrics

00:07:15,599 --> 00:07:19,680
and those logs are going to be there

00:07:17,680 --> 00:07:21,599
now if you're a developer who's trying

00:07:19,680 --> 00:07:23,599
to deploy a new application the

00:07:21,599 --> 00:07:25,120
pre-production checklist

00:07:23,599 --> 00:07:26,639
can be a significant and hard goal

00:07:25,120 --> 00:07:26,960
because a lot of things that you need to

00:07:26,639 --> 00:07:28,240
do

00:07:26,960 --> 00:07:30,800
in order to actually see your

00:07:28,240 --> 00:07:33,039
application out there and so

00:07:30,800 --> 00:07:33,840
keeping along with the lord of the rings

00:07:33,039 --> 00:07:35,280
metaphor

00:07:33,840 --> 00:07:36,880
they might look a little bit like the

00:07:35,280 --> 00:07:38,479
ghost of the rings and they look quite

00:07:36,880 --> 00:07:40,000
scary at this point in time this is like

00:07:38,479 --> 00:07:43,199
the first movie

00:07:40,000 --> 00:07:45,120
so what's the dilemma on the left

00:07:43,199 --> 00:07:46,479
application developer or in general like

00:07:45,120 --> 00:07:48,319
you're building something that looks

00:07:46,479 --> 00:07:50,400
really cool you want to ship it

00:07:48,319 --> 00:07:51,520
and when you are at the beginning of

00:07:50,400 --> 00:07:54,479
your startup journey

00:07:51,520 --> 00:07:55,680
so when you are askanti group doing a

00:07:54,479 --> 00:07:57,759
skanky up

00:07:55,680 --> 00:07:59,199
that is only built by a bunch of people

00:07:57,759 --> 00:08:01,280
we increasingly know

00:07:59,199 --> 00:08:03,759
that what you're doing is particularly

00:08:01,280 --> 00:08:04,720
risky because a new product is a new

00:08:03,759 --> 00:08:07,039
company

00:08:04,720 --> 00:08:08,720
you just did the rating fast it's fine

00:08:07,039 --> 00:08:09,360
to just ship it so you put your kaboy

00:08:08,720 --> 00:08:12,160
hat on

00:08:09,360 --> 00:08:14,000
and you just deploy to production now as

00:08:12,160 --> 00:08:16,400
you mature along your journey

00:08:14,000 --> 00:08:18,160
you start to get bigger bigger customers

00:08:16,400 --> 00:08:19,039
and those customers will have enterprise

00:08:18,160 --> 00:08:21,120
expectations

00:08:19,039 --> 00:08:22,879
so they want your service to be up you

00:08:21,120 --> 00:08:25,919
will have a slash with them

00:08:22,879 --> 00:08:27,759
and in general just your reputation will

00:08:25,919 --> 00:08:29,120
demand of you higher level of

00:08:27,759 --> 00:08:30,960
reliability

00:08:29,120 --> 00:08:32,959
now if you are too lazy and you work in

00:08:30,960 --> 00:08:36,080
financial technology that is even

00:08:32,959 --> 00:08:38,719
truer so to speak or for example a

00:08:36,080 --> 00:08:40,320
random consumer app you don't expect

00:08:38,719 --> 00:08:41,839
your payments to stop working

00:08:40,320 --> 00:08:43,680
they should always be working and if

00:08:41,839 --> 00:08:44,720
they don't that can cause some serious

00:08:43,680 --> 00:08:46,560
disruption

00:08:44,720 --> 00:08:48,720
so software has to be treated as we

00:08:46,560 --> 00:08:50,560
should critical as much as possible

00:08:48,720 --> 00:08:52,320
and to be reliable there's a lot of bets

00:08:50,560 --> 00:08:53,440
and wizards that you need to touch to

00:08:52,320 --> 00:08:56,000
your application

00:08:53,440 --> 00:08:57,200
so metrics tracing logs horizontal

00:08:56,000 --> 00:08:59,040
protocol scaling

00:08:57,200 --> 00:09:01,600
alerts to know when something goes wrong

00:08:59,040 --> 00:09:03,760
network policies to prevent escalations

00:09:01,600 --> 00:09:05,600
liveness and radium probes uh to drive

00:09:03,760 --> 00:09:06,720
cuban needs when to restart something

00:09:05,600 --> 00:09:09,839
and so on and so forth

00:09:06,720 --> 00:09:12,640
at the least can get very long

00:09:09,839 --> 00:09:13,040
and that is troublesome because in the

00:09:12,640 --> 00:09:16,399
end

00:09:13,040 --> 00:09:17,040
that's my personal um personal model i

00:09:16,399 --> 00:09:19,519
would say

00:09:17,040 --> 00:09:20,240
is convenience beats correctness what

00:09:19,519 --> 00:09:23,360
this means

00:09:20,240 --> 00:09:25,440
is that if doing the right thing is

00:09:23,360 --> 00:09:26,399
in any way shape or form more

00:09:25,440 --> 00:09:29,200
complicated

00:09:26,399 --> 00:09:31,040
than doing the wrong thing then someone

00:09:29,200 --> 00:09:33,279
at a certain point in time will find the

00:09:31,040 --> 00:09:36,160
reason not to do the right thing

00:09:33,279 --> 00:09:37,519
the deadline next monday and really need

00:09:36,160 --> 00:09:40,160
to ship this application

00:09:37,519 --> 00:09:41,680
or dating is too complex and actually

00:09:40,160 --> 00:09:42,399
they don't need all the stuff like this

00:09:41,680 --> 00:09:43,680
is a small

00:09:42,399 --> 00:09:45,279
small thing it's going to run in the

00:09:43,680 --> 00:09:46,959
cluster it's not going to get big then

00:09:45,279 --> 00:09:48,160
it gets big then it fails and then your

00:09:46,959 --> 00:09:50,000
problems

00:09:48,160 --> 00:09:51,519
so you won't be able to fall into the

00:09:50,000 --> 00:09:53,839
so-called peter success

00:09:51,519 --> 00:09:55,360
that you naturally converge to doing the

00:09:53,839 --> 00:09:55,680
right thing because doing the right

00:09:55,360 --> 00:09:58,560
thing

00:09:55,680 --> 00:09:59,519
is the easiest thing to do now i'm not

00:09:58,560 --> 00:10:01,200
going to cover

00:09:59,519 --> 00:10:02,800
uh all the possible things that we

00:10:01,200 --> 00:10:03,279
require application to do because that

00:10:02,800 --> 00:10:05,839
would be

00:10:03,279 --> 00:10:06,800
long and potentially quite boring we're

00:10:05,839 --> 00:10:10,160
still going to focus

00:10:06,800 --> 00:10:10,880
on telemetry data so i think in topic

00:10:10,160 --> 00:10:13,600
with the talk

00:10:10,880 --> 00:10:14,640
are we observable yet what kind of

00:10:13,600 --> 00:10:16,880
telemetry data

00:10:14,640 --> 00:10:18,240
so we said logs that we ship into

00:10:16,880 --> 00:10:20,320
elasticsearch

00:10:18,240 --> 00:10:22,480
metrics which are scraped from

00:10:20,320 --> 00:10:24,560
prometheus from our applications

00:10:22,480 --> 00:10:26,000
and traces that we push into jaeger for

00:10:24,560 --> 00:10:28,160
distributed tracing

00:10:26,000 --> 00:10:30,000
so we're going to go one by one uh look

00:10:28,160 --> 00:10:31,440
at what they are why they're useful

00:10:30,000 --> 00:10:33,839
and now you collect them in a rust

00:10:31,440 --> 00:10:33,839
application

00:10:33,920 --> 00:10:38,160
so let's start from metrics

00:10:40,959 --> 00:10:44,399
why do you want to collect metrics well

00:10:43,360 --> 00:10:46,320
generally speaking

00:10:44,399 --> 00:10:48,160
you want to collect metrics because you

00:10:46,320 --> 00:10:48,560
want to be able to produce plots that

00:10:48,160 --> 00:10:50,959
look

00:10:48,560 --> 00:10:53,120
exactly like this so you want to be able

00:10:50,959 --> 00:10:55,360
to see well what's the latency of this

00:10:53,120 --> 00:10:57,760
application in the last 30 minutes

00:10:55,360 --> 00:10:58,640
and potentially break it down by

00:10:57,760 --> 00:11:00,880
percentile

00:10:58,640 --> 00:11:03,279
so the 50th percentile the 70th

00:11:00,880 --> 00:11:04,720
percentile the 90th and the 99th

00:11:03,279 --> 00:11:06,480
depending on the type of application

00:11:04,720 --> 00:11:08,000
your performance profile

00:11:06,480 --> 00:11:10,720
or you might want to know what's the

00:11:08,000 --> 00:11:11,760
response breakdown so how many 200s only

00:11:10,720 --> 00:11:14,880
500s

00:11:11,760 --> 00:11:17,360
400s and so on and so forth metrics

00:11:14,880 --> 00:11:19,760
generally speaking are there to give us

00:11:17,360 --> 00:11:21,360
an aggregate picture of the system state

00:11:19,760 --> 00:11:23,360
so they're there to answer boolean

00:11:21,360 --> 00:11:25,519
questions very often

00:11:23,360 --> 00:11:28,480
about how the system is doing is our

00:11:25,519 --> 00:11:31,760
error rate above or below 10 percent

00:11:28,480 --> 00:11:33,760
uh is the error rate for requests uh

00:11:31,760 --> 00:11:35,600
that come to this specific api on this

00:11:33,760 --> 00:11:36,720
endpoint above or below a certain

00:11:35,600 --> 00:11:39,839
threshold

00:11:36,720 --> 00:11:41,680
uh are we breaking our slas on latency

00:11:39,839 --> 00:11:43,279
and metrics are supposed to be as real

00:11:41,680 --> 00:11:44,399
time as possible so they tell you what

00:11:43,279 --> 00:11:48,160
the system state

00:11:44,399 --> 00:11:49,839
is now in this very very moment

00:11:48,160 --> 00:11:51,920
what do metrics look like so how do you

00:11:49,839 --> 00:11:53,120
actually get those plots that we just

00:11:51,920 --> 00:11:55,279
saw

00:11:53,120 --> 00:11:56,160
metrics are generally looks somewhat

00:11:55,279 --> 00:11:58,800
like this

00:11:56,160 --> 00:12:00,959
um so you have a metric name which in

00:11:58,800 --> 00:12:03,200
this case is http requests

00:12:00,959 --> 00:12:04,880
duration seconds bucket it's a bit more

00:12:03,200 --> 00:12:06,560
full but it's very precise

00:12:04,880 --> 00:12:08,160
so we're talking about the duration of

00:12:06,560 --> 00:12:10,160
http requests

00:12:08,160 --> 00:12:12,000
and we're looking at the histogram uh so

00:12:10,160 --> 00:12:13,519
we're looking at buckets of requests

00:12:12,000 --> 00:12:15,440
at different type at different

00:12:13,519 --> 00:12:17,680
thresholds of latencies

00:12:15,440 --> 00:12:19,519
on this metric we have a set of labels

00:12:17,680 --> 00:12:22,160
that we can use to slice

00:12:19,519 --> 00:12:22,560
uh the matrix value so we have endpoint

00:12:22,160 --> 00:12:26,240
so

00:12:22,560 --> 00:12:27,680
bottom point http method get post boot

00:12:26,240 --> 00:12:29,519
match whatever

00:12:27,680 --> 00:12:30,880
the status code we returned so in this

00:12:29,519 --> 00:12:32,560
case 404

00:12:30,880 --> 00:12:34,800
and then you have the bucket that we're

00:12:32,560 --> 00:12:36,959
looking at so 5 milliseconds

00:12:34,800 --> 00:12:39,200
10 milliseconds 25 and so on and so

00:12:36,959 --> 00:12:40,639
forth and then the number of requests

00:12:39,200 --> 00:12:42,959
falling inside that bucket

00:12:40,639 --> 00:12:43,839
now this was a super fast photo four so

00:12:42,959 --> 00:12:46,560
all

00:12:43,839 --> 00:12:46,880
thousand six hundred or one uh fell we

00:12:46,560 --> 00:12:48,720
know

00:12:46,880 --> 00:12:50,399
beneath the five milliseconds that

00:12:48,720 --> 00:12:52,079
generally it's gonna be a little bit

00:12:50,399 --> 00:12:54,959
more varied

00:12:52,079 --> 00:12:56,720
uh this is basically a time series a

00:12:54,959 --> 00:12:57,440
time series with a variety of values you

00:12:56,720 --> 00:13:00,720
can slice

00:12:57,440 --> 00:13:02,959
and dice from these time series are

00:13:00,720 --> 00:13:03,920
produced by the application and then are

00:13:02,959 --> 00:13:06,160
aggregated

00:13:03,920 --> 00:13:07,519
uh by prometheus at least in our

00:13:06,160 --> 00:13:09,839
specific setup

00:13:07,519 --> 00:13:11,680
so chromidius hits the slash metrics and

00:13:09,839 --> 00:13:12,480
point on all the copies of an

00:13:11,680 --> 00:13:14,240
application

00:13:12,480 --> 00:13:15,760
it could be another endpoint but that's

00:13:14,240 --> 00:13:18,079
generally the default

00:13:15,760 --> 00:13:18,800
aggregates all these metrics indexes

00:13:18,079 --> 00:13:20,720
them

00:13:18,800 --> 00:13:21,839
and then allows you to perform queries

00:13:20,720 --> 00:13:23,839
against them

00:13:21,839 --> 00:13:25,680
one way to perform queries is to do

00:13:23,839 --> 00:13:27,760
alerts so alert manager

00:13:25,680 --> 00:13:28,720
you define a variety of queries which

00:13:27,760 --> 00:13:31,440
i'll wait for boolean

00:13:28,720 --> 00:13:33,120
so as we said before is the error rate

00:13:31,440 --> 00:13:36,480
so the number of 500s

00:13:33,120 --> 00:13:39,519
above or below 10 for 15 minutes

00:13:36,480 --> 00:13:40,240
if yes uh then through pagerduty get an

00:13:39,519 --> 00:13:42,000
uncooler

00:13:40,240 --> 00:13:44,399
so i'm going to call engineer to look at

00:13:42,000 --> 00:13:46,639
the system because something is wrong

00:13:44,399 --> 00:13:48,480
uh otherwise you can use grafana if you

00:13:46,639 --> 00:13:50,560
just want to do some pd visualization so

00:13:48,480 --> 00:13:51,519
if we go back to the slide we saw before

00:13:50,560 --> 00:13:54,000
which is this one

00:13:51,519 --> 00:13:56,399
this is grafana so we're looking just

00:13:54,000 --> 00:13:58,959
that premiersquare is visualized

00:13:56,399 --> 00:14:00,480
this is very very useful for an uncool

00:13:58,959 --> 00:14:03,600
team or operation team

00:14:00,480 --> 00:14:06,079
to actually understand what's going on

00:14:03,600 --> 00:14:06,880
now how do you actually get metrics so

00:14:06,079 --> 00:14:11,120
how do you get

00:14:06,880 --> 00:14:13,440
your api to produce metrics uh redirect

00:14:11,120 --> 00:14:14,480
was developed using optics web for a

00:14:13,440 --> 00:14:16,639
variety of reasons

00:14:14,480 --> 00:14:18,880
brought a piece about that a couple of

00:14:16,639 --> 00:14:21,199
weeks ago feeble curtis

00:14:18,880 --> 00:14:23,600
uh it's very very easy so there's a

00:14:21,199 --> 00:14:26,240
package on craigslist called arctic's

00:14:23,600 --> 00:14:28,320
web prom so active swept prometheus

00:14:26,240 --> 00:14:30,399
and you just plug the middleware inside

00:14:28,320 --> 00:14:32,800
your application is that dot drop

00:14:30,399 --> 00:14:34,480
from dot clone line the middleware takes

00:14:32,800 --> 00:14:36,880
some very very basic

00:14:34,480 --> 00:14:37,839
configuration parameters so a prefix for

00:14:36,880 --> 00:14:39,760
the metrics

00:14:37,839 --> 00:14:41,120
and the point you want to use and then

00:14:39,760 --> 00:14:42,800
you're set up now you're just going to

00:14:41,120 --> 00:14:45,120
experience slash metrics

00:14:42,800 --> 00:14:47,040
now you might want to customize it uh

00:14:45,120 --> 00:14:48,720
for your specific application because

00:14:47,040 --> 00:14:49,839
you might need to collect metrics which

00:14:48,720 --> 00:14:51,839
are known standards

00:14:49,839 --> 00:14:53,680
uh you might have specific naming

00:14:51,839 --> 00:14:56,399
conventions and so on and so forth

00:14:53,680 --> 00:14:57,440
lexus web prompt is like a single file

00:14:56,399 --> 00:14:59,360
type of crate

00:14:57,440 --> 00:15:01,040
so you can go there use it as some kind

00:14:59,360 --> 00:15:03,920
of a blueprint and adopt it

00:15:01,040 --> 00:15:04,480
to do whatever you need to do so metrics

00:15:03,920 --> 00:15:07,040
useful

00:15:04,480 --> 00:15:10,000
very easy to collect just plug and play

00:15:07,040 --> 00:15:10,000
if you're using optics

00:15:10,839 --> 00:15:13,839
logging

00:15:14,240 --> 00:15:18,399
as we saw metrics are about what is

00:15:17,040 --> 00:15:20,160
happening in the system

00:15:18,399 --> 00:15:22,240
in the aggregate at this very very

00:15:20,160 --> 00:15:26,000
moment so low latency

00:15:22,240 --> 00:15:27,600
fairly aggregated type of data logs

00:15:26,000 --> 00:15:29,759
are instead useful to answer the

00:15:27,600 --> 00:15:31,360
question what is happening to this

00:15:29,759 --> 00:15:34,240
specific request

00:15:31,360 --> 00:15:35,199
such as what happened to users who tried

00:15:34,240 --> 00:15:38,320
to do a payment

00:15:35,199 --> 00:15:38,880
from let's say hsbc to barclays in the

00:15:38,320 --> 00:15:41,680
uk

00:15:38,880 --> 00:15:42,800
between 5 pm and 6 pm on the 27th of

00:15:41,680 --> 00:15:45,680
july

00:15:42,800 --> 00:15:47,199
there's no way unless i'm very lucky and

00:15:45,680 --> 00:15:48,079
the labels on the metrics are exactly

00:15:47,199 --> 00:15:50,079
the ones i need

00:15:48,079 --> 00:15:51,920
but generally they aren't because labels

00:15:50,079 --> 00:15:52,800
are supposed to be locationality or

00:15:51,920 --> 00:15:54,560
metrics

00:15:52,800 --> 00:15:55,920
there's no way i can generally answer

00:15:54,560 --> 00:15:57,600
this type of question

00:15:55,920 --> 00:15:59,920
absolutely i cannot answer it to the

00:15:57,600 --> 00:16:00,800
single request type of granularity

00:15:59,920 --> 00:16:03,839
because

00:16:00,800 --> 00:16:06,240
those are all aggregated in metrics logs

00:16:03,839 --> 00:16:07,040
instead can provide us to that level of

00:16:06,240 --> 00:16:09,360
drill down

00:16:07,040 --> 00:16:10,720
that can allow us to slice and dice to

00:16:09,360 --> 00:16:13,440
get that precise

00:16:10,720 --> 00:16:14,240
level of information that is key to

00:16:13,440 --> 00:16:16,240
actually

00:16:14,240 --> 00:16:18,800
debug what is going on in a distributed

00:16:16,240 --> 00:16:20,160
system especially when things go wrong

00:16:18,800 --> 00:16:22,160
in a way which you haven't actually

00:16:20,160 --> 00:16:23,920
accounted for the so-called unknown

00:16:22,160 --> 00:16:26,240
unknowns or emergent behavior in

00:16:23,920 --> 00:16:28,399
distributed systems

00:16:26,240 --> 00:16:30,800
so let's look at what it looks like to

00:16:28,399 --> 00:16:34,000
log in rust

00:16:30,800 --> 00:16:37,040
so classic approach rust 1.0 approach

00:16:34,000 --> 00:16:37,519
logging easy user log crate the log

00:16:37,040 --> 00:16:40,639
crate

00:16:37,519 --> 00:16:42,399
is built using a facade pattern so the

00:16:40,639 --> 00:16:45,839
lock crate provides you steady macros

00:16:42,399 --> 00:16:48,079
debug trays info worn and error

00:16:45,839 --> 00:16:50,000
to actually instrument your application

00:16:48,079 --> 00:16:52,320
this is an example taken straight from

00:16:50,000 --> 00:16:53,600
the log crate documentation or a place i

00:16:52,320 --> 00:16:55,440
think it was

00:16:53,600 --> 00:16:57,360
uh you enter into the shape the yak

00:16:55,440 --> 00:16:59,920
function takes a yak

00:16:57,360 --> 00:17:01,279
in a mutable reference to yak you meet a

00:16:59,920 --> 00:17:03,279
trace level statement

00:17:01,279 --> 00:17:04,959
so you announce to the world we are

00:17:03,279 --> 00:17:08,000
commencing the shaving

00:17:04,959 --> 00:17:09,360
is trace level so it's at a very very

00:17:08,000 --> 00:17:10,720
bose logging level

00:17:09,360 --> 00:17:12,400
in most cases it's going to be filtered

00:17:10,720 --> 00:17:14,480
out then you loop

00:17:12,400 --> 00:17:16,959
and try to require a razor if you get a

00:17:14,480 --> 00:17:18,240
razor info level lock statement presser

00:17:16,959 --> 00:17:20,799
located

00:17:18,240 --> 00:17:22,400
display implementation of the razor you

00:17:20,799 --> 00:17:23,039
shape the yak and you break from the

00:17:22,400 --> 00:17:25,280
loop

00:17:23,039 --> 00:17:27,520
and then you exit the function if

00:17:25,280 --> 00:17:29,679
instead you fail to find the razor

00:17:27,520 --> 00:17:31,840
then you need a warning saying i was

00:17:29,679 --> 00:17:34,640
unable to look at the razor

00:17:31,840 --> 00:17:34,640
you're going to retry

00:17:36,799 --> 00:17:42,240
now facade means that

00:17:40,480 --> 00:17:44,559
you have no idea what is actually going

00:17:42,240 --> 00:17:46,640
to consume these log statements

00:17:44,559 --> 00:17:48,080
just instrument your code and then

00:17:46,640 --> 00:17:48,960
generally at the entry point of your

00:17:48,080 --> 00:17:51,039
binary

00:17:48,960 --> 00:17:52,480
you're gonna introduce a local

00:17:51,039 --> 00:17:54,799
implementation so an actual

00:17:52,480 --> 00:17:56,559
implementation that takes this log data

00:17:54,799 --> 00:17:58,080
and then does something with them where

00:17:56,559 --> 00:17:59,520
something is generally shipping them

00:17:58,080 --> 00:18:01,679
some places

00:17:59,520 --> 00:18:03,600
if you use the simplest possible logger

00:18:01,679 --> 00:18:05,280
which is generally a blocker

00:18:03,600 --> 00:18:08,480
you're gonna see something like this so

00:18:05,280 --> 00:18:10,640
you log into the console standard out

00:18:08,480 --> 00:18:12,720
in this specific execution which i made

00:18:10,640 --> 00:18:14,799
you get unable to locate the razer for

00:18:12,720 --> 00:18:16,640
three times so we loop in three times

00:18:14,799 --> 00:18:18,480
and then you actually locate the razor

00:18:16,640 --> 00:18:21,760
so you have the log message

00:18:18,480 --> 00:18:22,240
uh the name of the function and then you

00:18:21,760 --> 00:18:26,000
have

00:18:22,240 --> 00:18:28,240
a timestamp and the log level now

00:18:26,000 --> 00:18:29,520
this may work if you're doing common

00:18:28,240 --> 00:18:32,880
line applications

00:18:29,520 --> 00:18:34,720
uh so if it's a single um

00:18:32,880 --> 00:18:36,480
main function running and you have a

00:18:34,720 --> 00:18:37,840
user looking at logs to understand what

00:18:36,480 --> 00:18:40,799
is going on

00:18:37,840 --> 00:18:42,320
in a back-end system uh especially in a

00:18:40,799 --> 00:18:43,840
distributed backend system

00:18:42,320 --> 00:18:46,240
uh you have applications running on

00:18:43,840 --> 00:18:46,799
multiple machines these applications are

00:18:46,240 --> 00:18:48,960
generally

00:18:46,799 --> 00:18:50,000
some kind of server either a web server

00:18:48,960 --> 00:18:51,919
or a new consumer

00:18:50,000 --> 00:18:53,679
or something like that and they're

00:18:51,919 --> 00:18:56,559
executing many many requests

00:18:53,679 --> 00:18:58,080
concurrently and you want to be able at

00:18:56,559 --> 00:18:59,120
a certain point generally later so

00:18:58,080 --> 00:19:01,679
you're not really there

00:18:59,120 --> 00:19:02,559
tailing the logs to say what happened to

00:19:01,679 --> 00:19:05,440
request

00:19:02,559 --> 00:19:07,200
xyz which was about this type of users

00:19:05,440 --> 00:19:09,200
as we discussed before

00:19:07,200 --> 00:19:12,000
and the only way you can do that in

00:19:09,200 --> 00:19:15,360
plain logging is using text search

00:19:12,000 --> 00:19:17,679
but text search is not easy to search

00:19:15,360 --> 00:19:18,559
first of all is expensive it cannot be

00:19:17,679 --> 00:19:20,640
indexed

00:19:18,559 --> 00:19:22,559
and requires also a lot of knowledge

00:19:20,640 --> 00:19:24,720
about how the logs are structured

00:19:22,559 --> 00:19:26,720
so you end up if you want to do anything

00:19:24,720 --> 00:19:27,600
that is non-trivial so anything which is

00:19:26,720 --> 00:19:30,240
not

00:19:27,600 --> 00:19:31,760
tell me if this substring is in the log

00:19:30,240 --> 00:19:32,880
you end up writing pregnancies and

00:19:31,760 --> 00:19:34,559
running reg exes

00:19:32,880 --> 00:19:36,240
means that you are coupled to the

00:19:34,559 --> 00:19:38,559
implementation of the login

00:19:36,240 --> 00:19:40,240
inside the application which makes it

00:19:38,559 --> 00:19:42,400
very very complicated

00:19:40,240 --> 00:19:44,080
for operators and support people to

00:19:42,400 --> 00:19:45,600
actually go and use these logs

00:19:44,080 --> 00:19:47,200
so all the pressure of operating the

00:19:45,600 --> 00:19:49,679
software ends up

00:19:47,200 --> 00:19:50,880
on the shoulders of the developers which

00:19:49,679 --> 00:19:52,240
we want them to be there

00:19:50,880 --> 00:19:53,919
but we don't want them to be the only

00:19:52,240 --> 00:19:55,840
ones who can answer questions about the

00:19:53,919 --> 00:19:58,000
system

00:19:55,840 --> 00:19:59,039
so a much better way is to have

00:19:58,000 --> 00:20:00,960
structure logs

00:19:59,039 --> 00:20:02,159
structured logs in the sense that to

00:20:00,960 --> 00:20:04,720
each lock line

00:20:02,159 --> 00:20:05,840
we associate a context and that context

00:20:04,720 --> 00:20:08,880
needs to be searchable

00:20:05,840 --> 00:20:11,280
which in a very informal terms means

00:20:08,880 --> 00:20:12,880
that the context is in some machine

00:20:11,280 --> 00:20:15,600
readable format

00:20:12,880 --> 00:20:17,440
that somebody can parse and index

00:20:15,600 --> 00:20:18,720
allowing people to filter native perform

00:20:17,440 --> 00:20:20,559
queries

00:20:18,720 --> 00:20:22,320
so let's have a look and how we could do

00:20:20,559 --> 00:20:25,440
structure logging so

00:20:22,320 --> 00:20:27,760
similar example not fully identical

00:20:25,440 --> 00:20:29,120
this time the debug macro is coming from

00:20:27,760 --> 00:20:31,919
this log crate

00:20:29,120 --> 00:20:32,799
it's log standing for structure logging

00:20:31,919 --> 00:20:34,960
slot create

00:20:32,799 --> 00:20:36,559
this one as well but established been

00:20:34,960 --> 00:20:40,480
there for quite some time

00:20:36,559 --> 00:20:42,240
it allows you to specify the log message

00:20:40,480 --> 00:20:44,640
so very similarly to what we were doing

00:20:42,240 --> 00:20:47,760
before and that allows you to specify

00:20:44,640 --> 00:20:48,240
using the old macro uh sum key value

00:20:47,760 --> 00:20:51,760
pairs

00:20:48,240 --> 00:20:53,600
to be attached to your locks now slog

00:20:51,760 --> 00:20:56,000
has been for a very long time the only

00:20:53,600 --> 00:20:58,320
way to do structure logging in rust

00:20:56,000 --> 00:20:59,600
uh recently if i'm not mistaken the lock

00:20:58,320 --> 00:21:03,200
crate has update

00:20:59,600 --> 00:21:05,520
a feature to add key value

00:21:03,200 --> 00:21:07,600
key value pairs to log statements but

00:21:05,520 --> 00:21:09,919
once again as far as i've seen at least

00:21:07,600 --> 00:21:11,360
after a month ago almost none of the

00:21:09,919 --> 00:21:12,159
logger implementations actually

00:21:11,360 --> 00:21:14,400
supported

00:21:12,159 --> 00:21:15,520
key value pair logging so you once again

00:21:14,400 --> 00:21:17,840
down to zlog

00:21:15,520 --> 00:21:19,360
for doing structure logging so what are

00:21:17,840 --> 00:21:20,000
we trying to do here we're trying to do

00:21:19,360 --> 00:21:21,440
here is

00:21:20,000 --> 00:21:23,120
what we generally want to do in

00:21:21,440 --> 00:21:24,720
distributed applications so i want to

00:21:23,120 --> 00:21:27,039
know when something is beginning

00:21:24,720 --> 00:21:29,120
i want to do some stuff which might be

00:21:27,039 --> 00:21:31,679
composed of some subroutines

00:21:29,120 --> 00:21:33,280
so this sub you need the work function

00:21:31,679 --> 00:21:35,280
that might have made their own logs so

00:21:33,280 --> 00:21:36,559
this event log so cool

00:21:35,280 --> 00:21:38,480
and then you want to know when that

00:21:36,559 --> 00:21:40,000
thing has ended

00:21:38,480 --> 00:21:41,679
and then given that we're shaving the

00:21:40,000 --> 00:21:43,039
yacht on behalf of somebody else so

00:21:41,679 --> 00:21:45,039
we're taking this user id

00:21:43,039 --> 00:21:46,720
i want the user id to be associated to

00:21:45,039 --> 00:21:48,640
every lock line and i also want to

00:21:46,720 --> 00:21:50,080
capture along the whole application took

00:21:48,640 --> 00:21:52,559
so i want to capture that elapsed

00:21:50,080 --> 00:21:55,919
milliseconds at the bottom

00:21:52,559 --> 00:21:57,919
if once again we plug into it the most

00:21:55,919 --> 00:21:59,200
basic type of a matter so in this case

00:21:57,919 --> 00:22:01,760
it's a boolean formatter

00:21:59,200 --> 00:22:02,480
login to standard out we get exactly

00:22:01,760 --> 00:22:04,240
this

00:22:02,480 --> 00:22:06,000
so you see all the log statements you

00:22:04,240 --> 00:22:08,799
see all the bunion metadata

00:22:06,000 --> 00:22:11,280
and everything is adjacent so that means

00:22:08,799 --> 00:22:12,000
that i can parse this as jsons and i can

00:22:11,280 --> 00:22:15,200
filter user

00:22:12,000 --> 00:22:16,960
id very very fast very very easily or

00:22:15,200 --> 00:22:18,720
i can push all these things somewhere

00:22:16,960 --> 00:22:19,440
else which is going to index them and

00:22:18,720 --> 00:22:21,520
search them

00:22:19,440 --> 00:22:23,919
and we're going to see that in a few

00:22:21,520 --> 00:22:23,919
seconds

00:22:25,760 --> 00:22:31,440
now let's go back to the code

00:22:29,520 --> 00:22:32,960
you may agree with me that this is very

00:22:31,440 --> 00:22:35,440
verbose

00:22:32,960 --> 00:22:37,440
uh it's very very noisy like you have a

00:22:35,440 --> 00:22:39,200
lot of lock statements which are

00:22:37,440 --> 00:22:40,559
interleaved with the application code

00:22:39,200 --> 00:22:42,400
you don't even see the application code

00:22:40,559 --> 00:22:44,080
here but this function is really looking

00:22:42,400 --> 00:22:47,039
a little bit hairy

00:22:44,080 --> 00:22:47,760
and this is because generally speaking

00:22:47,039 --> 00:22:50,559
for most

00:22:47,760 --> 00:22:52,480
use cases at least the ones the one i

00:22:50,559 --> 00:22:54,640
encountered in the world

00:22:52,480 --> 00:22:55,760
having or fun log events is generally

00:22:54,640 --> 00:22:58,400
the wrong abstraction

00:22:55,760 --> 00:23:00,159
you reason about tasks and tasks have a

00:22:58,400 --> 00:23:02,799
start time they do something

00:23:00,159 --> 00:23:03,600
and then they end so what you really

00:23:02,799 --> 00:23:05,760
want to use

00:23:03,600 --> 00:23:07,760
is your primary building block uh when

00:23:05,760 --> 00:23:10,480
you're doing some kind

00:23:07,760 --> 00:23:11,840
of instrumentation for structure logging

00:23:10,480 --> 00:23:14,640
is a spawn

00:23:11,840 --> 00:23:16,799
and a span represents exactly a unit of

00:23:14,640 --> 00:23:18,880
work done in the system

00:23:16,799 --> 00:23:20,640
so let's look at the same function using

00:23:18,880 --> 00:23:22,720
spawns

00:23:20,640 --> 00:23:24,080
we are moving away from slog so we'll

00:23:22,720 --> 00:23:25,919
leave this log

00:23:24,080 --> 00:23:27,840
behind for the time being and we're

00:23:25,919 --> 00:23:28,400
moving on to the tracing crate tracing

00:23:27,840 --> 00:23:32,080
crate

00:23:28,400 --> 00:23:34,480
is part of the tokyo project and

00:23:32,080 --> 00:23:36,559
i think it's not an overstatement it's

00:23:34,480 --> 00:23:38,159
one of the most impactful crate at least

00:23:36,559 --> 00:23:39,760
on what they do on a daily basis

00:23:38,159 --> 00:23:42,080
which has been released in the past year

00:23:39,760 --> 00:23:44,000
or so provides extremely quality

00:23:42,080 --> 00:23:45,120
implementation and we're gonna see it

00:23:44,000 --> 00:23:48,480
suits our needs

00:23:45,120 --> 00:23:50,080
perfectly so aspar we

00:23:48,480 --> 00:23:51,679
enter into the function and we create a

00:23:50,080 --> 00:23:53,919
spawn the bug level

00:23:51,679 --> 00:23:55,200
so we set the level as if we were doing

00:23:53,919 --> 00:23:57,120
logging

00:23:55,200 --> 00:23:58,320
we tell what's the name of the spawn

00:23:57,120 --> 00:23:59,840
yacht shave

00:23:58,320 --> 00:24:01,919
and we associate with the spawn the user

00:23:59,840 --> 00:24:04,320
id now

00:24:01,919 --> 00:24:05,600
the threshing crate uses a guard button

00:24:04,320 --> 00:24:07,679
so when you press

00:24:05,600 --> 00:24:09,120
when you press when you call the dot

00:24:07,679 --> 00:24:11,120
enter function

00:24:09,120 --> 00:24:13,360
uh then you're gonna enter inside the

00:24:11,120 --> 00:24:16,720
spawn everything that happens between

00:24:13,360 --> 00:24:19,360
the enter function method invocation and

00:24:16,720 --> 00:24:20,559
the dropping point of the underscore

00:24:19,360 --> 00:24:22,159
enter guard

00:24:20,559 --> 00:24:24,080
is going to happen in the context of the

00:24:22,159 --> 00:24:26,960
same spawn which means

00:24:24,080 --> 00:24:28,080
there's no need for us to add once again

00:24:26,960 --> 00:24:30,720
the user id

00:24:28,080 --> 00:24:32,960
to debug there's also no need for us to

00:24:30,720 --> 00:24:34,000
do anything weird about sub-unity work

00:24:32,960 --> 00:24:36,080
so being the work

00:24:34,000 --> 00:24:37,360
can ignore the fact that it's part of

00:24:36,080 --> 00:24:39,760
the yak-shay function

00:24:37,360 --> 00:24:41,440
it can just go on to do its thing and

00:24:39,760 --> 00:24:42,240
they will be able to emit lock

00:24:41,440 --> 00:24:43,760
statements

00:24:42,240 --> 00:24:45,600
and if that was log statements that

00:24:43,760 --> 00:24:47,600
touch context then we can also capture

00:24:45,600 --> 00:24:49,520
the context from the pattern function

00:24:47,600 --> 00:24:51,919
and all of these effects happens pretty

00:24:49,520 --> 00:24:54,320
much transparently

00:24:51,919 --> 00:24:55,520
what this means is that if we really

00:24:54,320 --> 00:24:57,200
want to shrink it

00:24:55,520 --> 00:24:59,360
so if we really want to go to the

00:24:57,200 --> 00:25:00,799
essential of it we can also remove those

00:24:59,360 --> 00:25:02,960
two lines of boilerplate

00:25:00,799 --> 00:25:05,120
so that span equal and then the enter

00:25:02,960 --> 00:25:06,159
function just use the tracing instrument

00:25:05,120 --> 00:25:08,080
proc macro

00:25:06,159 --> 00:25:09,919
which is gonna basically the sugar

00:25:08,080 --> 00:25:12,400
exactly to the same thing

00:25:09,919 --> 00:25:14,320
and leaves us with this function now

00:25:12,400 --> 00:25:14,720
what's that that's like one two three

00:25:14,320 --> 00:25:17,440
four

00:25:14,720 --> 00:25:18,559
five lines uh considering there's a

00:25:17,440 --> 00:25:20,080
closing bracket so

00:25:18,559 --> 00:25:21,679
four or five depending on how you count

00:25:20,080 --> 00:25:23,679
it if you go

00:25:21,679 --> 00:25:25,600
and compare that to our log version of

00:25:23,679 --> 00:25:28,880
this you can clearly see

00:25:25,600 --> 00:25:31,840
how diagnostic implementation is now

00:25:28,880 --> 00:25:32,559
much less intrusive it's as we were

00:25:31,840 --> 00:25:34,880
saying before

00:25:32,559 --> 00:25:36,159
much more convenient it's much easier

00:25:34,880 --> 00:25:40,320
for developers

00:25:36,159 --> 00:25:42,640
to slap slash hush tracing instrument

00:25:40,320 --> 00:25:43,679
on top of a function and so allow them

00:25:42,640 --> 00:25:47,679
to build

00:25:43,679 --> 00:25:49,760
very very domain oriented uh trace bonds

00:25:47,679 --> 00:25:51,840
and do that consistently if that does

00:25:49,760 --> 00:25:52,799
not involve finding a load of code this

00:25:51,840 --> 00:25:54,880
does not involve

00:25:52,799 --> 00:25:56,960
polluting the function code and it's

00:25:54,880 --> 00:26:00,000
generally transparent

00:25:56,960 --> 00:26:02,559
to the application now tracing

00:26:00,000 --> 00:26:04,240
just like log and just like slog is a

00:26:02,559 --> 00:26:05,600
facade pattern

00:26:04,240 --> 00:26:07,840
so what you do you instrument your

00:26:05,600 --> 00:26:09,279
application using those macros

00:26:07,840 --> 00:26:10,960
and then you have subscribers

00:26:09,279 --> 00:26:12,799
subscribers are the ones that actually

00:26:10,960 --> 00:26:14,400
receive these tracing data and can do

00:26:12,799 --> 00:26:16,159
something with it

00:26:14,400 --> 00:26:18,000
so tracing can be used for structure

00:26:16,159 --> 00:26:18,880
logging i think at this point in time is

00:26:18,000 --> 00:26:20,880
the best crate

00:26:18,880 --> 00:26:22,240
if you really want to do structure logic

00:26:20,880 --> 00:26:24,480
so you can log

00:26:22,240 --> 00:26:26,559
all those funds to standard out or to a

00:26:24,480 --> 00:26:27,360
file or whatever you think is useful to

00:26:26,559 --> 00:26:29,840
you

00:26:27,360 --> 00:26:31,279
at the same time using spawns and spawns

00:26:29,840 --> 00:26:32,960
are exactly the concept used by

00:26:31,279 --> 00:26:33,919
distributed tracing as we'll see in a

00:26:32,960 --> 00:26:37,279
second

00:26:33,919 --> 00:26:39,600
so one type of instrumentation tracing

00:26:37,279 --> 00:26:41,120
and you're able to get at the same time

00:26:39,600 --> 00:26:44,080
writing no extra code

00:26:41,120 --> 00:26:46,080
both structure locking and dcbd tracing

00:26:44,080 --> 00:26:47,520
and this is extremely powerful and also

00:26:46,080 --> 00:26:49,200
extremely consistent because you're

00:26:47,520 --> 00:26:49,840
going to get the same spans across the

00:26:49,200 --> 00:26:53,360
two

00:26:49,840 --> 00:26:55,360
type of telemetry data so timely data

00:26:53,360 --> 00:26:58,400
how do we actually process logs

00:26:55,360 --> 00:27:01,760
and we actually process traces so

00:26:58,400 --> 00:27:02,799
logs we take tracing then we have a

00:27:01,760 --> 00:27:05,360
subscriber

00:27:02,799 --> 00:27:06,640
that prints logs to standard out in

00:27:05,360 --> 00:27:08,799
boone format

00:27:06,640 --> 00:27:09,679
is the tracing bunion log for matter

00:27:08,799 --> 00:27:11,919
which i wrote

00:27:09,679 --> 00:27:14,000
for donate direct and some crates of io

00:27:11,919 --> 00:27:17,840
if you want to use it

00:27:14,000 --> 00:27:21,120
then standard out is tailed by vector

00:27:17,840 --> 00:27:23,440
vector is another rust lock corrector

00:27:21,120 --> 00:27:24,240
that we use to get logs from standard

00:27:23,440 --> 00:27:26,960
out

00:27:24,240 --> 00:27:28,080
to elaborate kinesis which is then going

00:27:26,960 --> 00:27:30,159
into elastic search

00:27:28,080 --> 00:27:31,679
which we then search using kibana so

00:27:30,159 --> 00:27:34,159
there's a bunch of hops

00:27:31,679 --> 00:27:35,760
bring the engine up in hibana and kibana

00:27:34,159 --> 00:27:37,039
is fairly good to search logs and you

00:27:35,760 --> 00:27:38,000
don't need to be a developer to search

00:27:37,039 --> 00:27:39,600
logs in kibana

00:27:38,000 --> 00:27:42,000
so you go there you have all the

00:27:39,600 --> 00:27:44,799
possible fields of your logs on the left

00:27:42,000 --> 00:27:46,960
and you can filter either existence

00:27:44,799 --> 00:27:49,760
non-existence on the specific value

00:27:46,960 --> 00:27:50,000
doing regulations if you need to it can

00:27:49,760 --> 00:27:52,320
be

00:27:50,000 --> 00:27:54,240
views and graphs and in general it's

00:27:52,320 --> 00:27:56,559
very very friendly

00:27:54,240 --> 00:27:57,360
and we use kibana at all levels inside

00:27:56,559 --> 00:27:59,200
the company

00:27:57,360 --> 00:28:00,480
so from the application developers to

00:27:59,200 --> 00:28:03,039
the product managers

00:28:00,480 --> 00:28:05,039
to the support engineers to the first

00:28:03,039 --> 00:28:06,240
level of support to customer success

00:28:05,039 --> 00:28:08,880
managers

00:28:06,240 --> 00:28:10,559
that's what allows us to earn in a

00:28:08,880 --> 00:28:13,520
disability fashion

00:28:10,559 --> 00:28:14,399
the operation of a product disability

00:28:13,520 --> 00:28:17,440
raising

00:28:14,399 --> 00:28:19,279
is more or less the same thing just from

00:28:17,440 --> 00:28:20,880
a different perspective

00:28:19,279 --> 00:28:23,440
so when you talk about logs it's

00:28:20,880 --> 00:28:24,960
generally about a single application

00:28:23,440 --> 00:28:26,080
so you have this application that is

00:28:24,960 --> 00:28:27,360
there and it's doing stuff and it's

00:28:26,080 --> 00:28:29,760
emitting blocks

00:28:27,360 --> 00:28:30,960
now in a micro service architecture as

00:28:29,760 --> 00:28:33,679
the one we have here

00:28:30,960 --> 00:28:35,679
later and in many places at this point

00:28:33,679 --> 00:28:37,440
to serve a single request

00:28:35,679 --> 00:28:40,080
which is hitting the edge of your

00:28:37,440 --> 00:28:42,399
cluster that request generally

00:28:40,080 --> 00:28:44,240
flows through one two three four five

00:28:42,399 --> 00:28:47,679
six different microservices

00:28:44,240 --> 00:28:50,080
which cooperate to fulfill the job now

00:28:47,679 --> 00:28:51,440
when a customer comes to you saying i

00:28:50,080 --> 00:28:53,360
tried to do x

00:28:51,440 --> 00:28:54,799
and it didn't work you need to

00:28:53,360 --> 00:28:56,799
understand where

00:28:54,799 --> 00:28:58,720
exactly the problem is so you need to be

00:28:56,799 --> 00:29:00,559
able to trace that request

00:28:58,720 --> 00:29:02,320
across the different microservices and

00:29:00,559 --> 00:29:04,080
it should be easy to do so

00:29:02,320 --> 00:29:06,720
the way you do this or one of the

00:29:04,080 --> 00:29:10,000
possible ways is by adhering to the

00:29:06,720 --> 00:29:12,000
um jaeger tracing format

00:29:10,000 --> 00:29:13,440
which is now being evolved by the open

00:29:12,000 --> 00:29:15,600
tracing tracing format

00:29:13,440 --> 00:29:17,200
which is now being merged into the open

00:29:15,600 --> 00:29:20,240
telemetry format

00:29:17,200 --> 00:29:22,080
so on the tracing crate

00:29:20,240 --> 00:29:23,840
you have a tracing open telemetry

00:29:22,080 --> 00:29:26,399
subscriber which is maintained

00:29:23,840 --> 00:29:27,039
in the same repository you can use that

00:29:26,399 --> 00:29:29,200
and we do

00:29:27,039 --> 00:29:30,240
to ship logs into jager to ship traces

00:29:29,200 --> 00:29:32,159
into jager

00:29:30,240 --> 00:29:33,679
jager is once again backed by

00:29:32,159 --> 00:29:35,279
elasticsearch so it's more or less the

00:29:33,679 --> 00:29:38,080
same infrastructure

00:29:35,279 --> 00:29:38,399
and allows you to have this kind of view

00:29:38,080 --> 00:29:40,320
so

00:29:38,399 --> 00:29:43,200
each of the units of work appear as a

00:29:40,320 --> 00:29:44,559
bar you track along each of those stage

00:29:43,200 --> 00:29:47,120
and you can see all the different

00:29:44,559 --> 00:29:49,760
services uh that a single request

00:29:47,120 --> 00:29:51,760
coming from the outside flows through

00:29:49,760 --> 00:29:53,039
that is very very powerful to understand

00:29:51,760 --> 00:29:55,200
when something went wrong

00:29:53,039 --> 00:29:56,799
so you're able to correlate a request

00:29:55,200 --> 00:29:59,200
across everything that is happening

00:29:56,799 --> 00:30:02,799
inside the cluster

00:29:59,200 --> 00:30:02,799
so one final recap

00:30:05,760 --> 00:30:10,720
as we said production environments are

00:30:08,799 --> 00:30:12,640
extremely complex

00:30:10,720 --> 00:30:14,240
and if you don't have any way to observe

00:30:12,640 --> 00:30:16,720
what is happening and that generally

00:30:14,240 --> 00:30:18,080
means having some kind of telemetry data

00:30:16,720 --> 00:30:20,399
then your production environment is a

00:30:18,080 --> 00:30:21,919
ticking pump it might be in life today

00:30:20,399 --> 00:30:23,600
but it's going to go off at a certain

00:30:21,919 --> 00:30:24,960
point in the future and you're not gonna

00:30:23,600 --> 00:30:26,559
like it

00:30:24,960 --> 00:30:28,399
in order to know what is going on you

00:30:26,559 --> 00:30:30,880
need to add diagnostic instrumentation

00:30:28,399 --> 00:30:31,919
but for that to be there consistently it

00:30:30,880 --> 00:30:34,799
is to be easy

00:30:31,919 --> 00:30:36,880
to add that instrumentation and make it

00:30:34,799 --> 00:30:37,840
easy and convenient is your number one

00:30:36,880 --> 00:30:39,440
priority

00:30:37,840 --> 00:30:41,440
as an operator in general as an

00:30:39,440 --> 00:30:43,679
architect of a platform

00:30:41,440 --> 00:30:45,440
now different type of telemetry data

00:30:43,679 --> 00:30:47,440
gives us different type of information

00:30:45,440 --> 00:30:49,440
so metrics are great to alert and

00:30:47,440 --> 00:30:52,320
monitor system state

00:30:49,440 --> 00:30:54,159
while logs especially structure logging

00:30:52,320 --> 00:30:56,559
with icard in rd context

00:30:54,159 --> 00:30:58,159
is amazing to try to detect and triage

00:30:56,559 --> 00:31:00,000
failure modes that you might not

00:30:58,159 --> 00:31:02,080
have prevented when you design the

00:31:00,000 --> 00:31:03,200
system to get very high quality

00:31:02,080 --> 00:31:04,960
structure logs

00:31:03,200 --> 00:31:07,200
spawn is generally the type of

00:31:04,960 --> 00:31:10,000
abstraction that you want to use

00:31:07,200 --> 00:31:11,519
and no matter how good your logging is

00:31:10,000 --> 00:31:13,360
at a single service level

00:31:11,519 --> 00:31:15,279
you need to be able to trace our

00:31:13,360 --> 00:31:16,720
requests across the different services

00:31:15,279 --> 00:31:18,720
either you do that with disability

00:31:16,720 --> 00:31:19,679
tracing or just a correlation id that

00:31:18,720 --> 00:31:22,480
flows through

00:31:19,679 --> 00:31:23,279
you need to have that somewhere and

00:31:22,480 --> 00:31:25,440
overall

00:31:23,279 --> 00:31:27,440
i guess the lesson learned is we were

00:31:25,440 --> 00:31:29,919
able to get a rust application

00:31:27,440 --> 00:31:30,640
in production in less than a couple of

00:31:29,919 --> 00:31:32,720
weeks

00:31:30,640 --> 00:31:33,840
with top-notch observability and

00:31:32,720 --> 00:31:35,840
telemetry data

00:31:33,840 --> 00:31:37,679
and that generally means that the answer

00:31:35,840 --> 00:31:39,279
to the talk which generally is

00:31:37,679 --> 00:31:41,360
if you're doing a talk with a question

00:31:39,279 --> 00:31:43,360
as a title the answer is no

00:31:41,360 --> 00:31:44,799
like steve for the first day the answer

00:31:43,360 --> 00:31:47,440
in this case is yes

00:31:44,799 --> 00:31:48,480
so are we observable yet absolutely like

00:31:47,440 --> 00:31:51,600
tracing has been

00:31:48,480 --> 00:31:53,120
a step step change improvement into the

00:31:51,600 --> 00:31:56,320
quality of the rust lock

00:31:53,120 --> 00:31:57,919
ecosystem when it comes to telemetry and

00:31:56,320 --> 00:31:58,799
you can definitely ship high quality

00:31:57,919 --> 00:32:02,559
applications

00:31:58,799 --> 00:32:05,440
with very very granular telemetry data

00:32:02,559 --> 00:32:06,480
now the netdirect was an experiment in

00:32:05,440 --> 00:32:08,399
using rust

00:32:06,480 --> 00:32:11,200
in a live production application and we

00:32:08,399 --> 00:32:13,440
liked it so in one way or another

00:32:11,200 --> 00:32:14,720
probably the cto was not fully sovereign

00:32:13,440 --> 00:32:17,840
when he said that

00:32:14,720 --> 00:32:18,880
but we chose to bet on rust to do some

00:32:17,840 --> 00:32:20,880
new core projects

00:32:18,880 --> 00:32:21,919
in particular providing a core banking

00:32:20,880 --> 00:32:24,480
application

00:32:21,919 --> 00:32:26,320
which in a nutshell means cleaning

00:32:24,480 --> 00:32:27,120
accounts programmatically moving money

00:32:26,320 --> 00:32:29,519
in and out

00:32:27,120 --> 00:32:31,120
uh programmatically once again we're

00:32:29,519 --> 00:32:32,720
assembling a team we hide

00:32:31,120 --> 00:32:34,640
writing bunch we're still looking for

00:32:32,720 --> 00:32:37,279
one raspberry can engineer

00:32:34,640 --> 00:32:38,720
so if whatever we covered here sounds

00:32:37,279 --> 00:32:40,559
interesting to you

00:32:38,720 --> 00:32:42,320
just reach out that's the opening there

00:32:40,559 --> 00:32:42,960
that's my twitter handle like in many

00:32:42,320 --> 00:32:46,399
ways

00:32:42,960 --> 00:32:48,159
to get in touch and with that i think

00:32:46,399 --> 00:32:49,840
this is the end of the talk

00:32:48,159 --> 00:32:57,840
and i've been more than happy to take

00:32:49,840 --> 00:32:57,840
some questions

00:33:36,480 --> 00:33:42,880
okay we have one so let's waffle

00:33:39,919 --> 00:33:43,919
from twitch is asking does this

00:33:42,880 --> 00:33:46,559
telemetry setup

00:33:43,919 --> 00:33:49,039
integrate well with distributed non-rust

00:33:46,559 --> 00:33:52,480
applications

00:33:49,039 --> 00:33:55,200
uh well it depends on how we uh

00:33:52,480 --> 00:33:56,720
what do we mean by integrating well in

00:33:55,200 --> 00:33:59,840
our specific use case

00:33:56,720 --> 00:34:00,480
uh we do have some structures that we

00:33:59,840 --> 00:34:02,559
expect

00:34:00,480 --> 00:34:05,039
application to follow in the type of

00:34:02,559 --> 00:34:08,000
telemetry data that they produce

00:34:05,039 --> 00:34:10,320
so for expect for example we expect

00:34:08,000 --> 00:34:10,960
metrics exposed by apis but a certain

00:34:10,320 --> 00:34:13,520
format

00:34:10,960 --> 00:34:14,480
on a certain naming convention uh we

00:34:13,520 --> 00:34:17,839
expect

00:34:14,480 --> 00:34:19,440
our logs to follow the canonical lock

00:34:17,839 --> 00:34:21,520
button so

00:34:19,440 --> 00:34:23,599
generally meet one lock line with a lot

00:34:21,520 --> 00:34:25,200
of item united data that we then use to

00:34:23,599 --> 00:34:27,520
do various things

00:34:25,200 --> 00:34:29,040
so generally speaking there needs to be

00:34:27,520 --> 00:34:31,040
a little bit of coordination because of

00:34:29,040 --> 00:34:34,000
course if somebody goes with the

00:34:31,040 --> 00:34:35,599
net core defaults format and i go with

00:34:34,000 --> 00:34:37,040
the ras the full format

00:34:35,599 --> 00:34:38,960
and you go with the python default

00:34:37,040 --> 00:34:40,159
format it's very unlikely that they're

00:34:38,960 --> 00:34:42,240
going to really match up

00:34:40,159 --> 00:34:43,200
really nicely but you can use

00:34:42,240 --> 00:34:45,119
architectural

00:34:43,200 --> 00:34:46,839
decision records to just say these are

00:34:45,119 --> 00:34:49,679
we do logs

00:34:46,839 --> 00:34:50,960
and then everybody implements in such a

00:34:49,679 --> 00:35:03,839
way that they can interoperate

00:34:50,960 --> 00:35:03,839
so it needs a little bit of coordination

00:35:14,839 --> 00:35:17,839
foreign

00:35:30,720 --> 00:35:36,960
okay so another one from twitch

00:35:34,160 --> 00:35:38,320
that chris is asking the trace crate

00:35:36,960 --> 00:35:40,400
looks very powerful

00:35:38,320 --> 00:35:42,400
are there any features that you wish it

00:35:40,400 --> 00:35:44,079
had they don't have to be easy features

00:35:42,400 --> 00:35:45,599
i just like to hear your thoughts on the

00:35:44,079 --> 00:35:46,800
design space more

00:35:45,599 --> 00:35:48,640
well yeah the trading credit is

00:35:46,800 --> 00:35:51,359
extremely powerful uh

00:35:48,640 --> 00:35:52,079
i did raise some issues for some of the

00:35:51,359 --> 00:35:55,599
things that

00:35:52,079 --> 00:35:56,960
uh kind of surprised me uh so those some

00:35:55,599 --> 00:35:59,520
of those have made their way

00:35:56,960 --> 00:36:00,160
into the tracing crate itself also bug

00:35:59,520 --> 00:36:03,359
fix

00:36:00,160 --> 00:36:06,000
on a core dump that was nasty uh but

00:36:03,359 --> 00:36:08,320
generally speaking has been amazing uh

00:36:06,000 --> 00:36:10,480
things that i wish would be different

00:36:08,320 --> 00:36:11,440
so at the moment the tracing trade has a

00:36:10,480 --> 00:36:14,880
lot of focus

00:36:11,440 --> 00:36:17,839
on making telemetry uh

00:36:14,880 --> 00:36:20,720
fast or in general reducing the overhead

00:36:17,839 --> 00:36:24,400
of doing certain types of operations

00:36:20,720 --> 00:36:26,720
for example one thing is traces

00:36:24,400 --> 00:36:28,079
so the the metadata you collect about

00:36:26,720 --> 00:36:30,480
the spawn

00:36:28,079 --> 00:36:32,880
is statically determined at the moment

00:36:30,480 --> 00:36:34,800
of spawn creation

00:36:32,880 --> 00:36:36,960
and that is great uh because then

00:36:34,800 --> 00:36:38,000
everything is much faster it consumes

00:36:36,960 --> 00:36:40,079
less memory

00:36:38,000 --> 00:36:41,599
but sometimes for the way certain

00:36:40,079 --> 00:36:44,400
applications are architected

00:36:41,599 --> 00:36:45,599
uh you would like to be able to add

00:36:44,400 --> 00:36:47,440
additional metadata

00:36:45,599 --> 00:36:49,680
dynamically even if that means

00:36:47,440 --> 00:36:51,040
allocating or doing stuff that

00:36:49,680 --> 00:36:53,359
might not be what you want to do in an

00:36:51,040 --> 00:36:54,800
off loop maybe for that application and

00:36:53,359 --> 00:36:57,680
its performance profile

00:36:54,800 --> 00:36:58,800
works fairly well another thing that we

00:36:57,680 --> 00:37:00,800
found

00:36:58,800 --> 00:37:02,320
was a little bit of a slippery slope was

00:37:00,800 --> 00:37:04,720
the instrument macro

00:37:02,320 --> 00:37:06,160
which is very very convenient because it

00:37:04,720 --> 00:37:07,920
captures the name of the function

00:37:06,160 --> 00:37:09,599
body captures by default all the

00:37:07,920 --> 00:37:12,320
arguments of the function

00:37:09,599 --> 00:37:13,520
and that can somewhat be tricky if

00:37:12,320 --> 00:37:15,200
you're managing secrets

00:37:13,520 --> 00:37:17,359
uh so if you're managing things that you

00:37:15,200 --> 00:37:18,880
don't want to log and so it's very easy

00:37:17,359 --> 00:37:21,119
to write a function today

00:37:18,880 --> 00:37:22,880
uh put the instrument macro up there

00:37:21,119 --> 00:37:23,440
then somebody else comes two weeks from

00:37:22,880 --> 00:37:25,520
now

00:37:23,440 --> 00:37:28,000
uh adds another argument which is a gwd

00:37:25,520 --> 00:37:29,200
token and then the gwd token ends up in

00:37:28,000 --> 00:37:31,359
kibana

00:37:29,200 --> 00:37:32,400
um so it would be nice to have the

00:37:31,359 --> 00:37:35,440
possibility

00:37:32,400 --> 00:37:39,040
or different macros or whatever to have

00:37:35,440 --> 00:37:41,599
a deny all approach so that i need to

00:37:39,040 --> 00:37:42,560
explicitly allow certain fields to be

00:37:41,599 --> 00:37:44,079
locked

00:37:42,560 --> 00:37:47,359
which for the type of application that

00:37:44,079 --> 00:37:48,960
we do will make us sleep better

00:37:47,359 --> 00:37:50,160
but generally speaking getting is great

00:37:48,960 --> 00:37:52,079
and i think it's going to get more and

00:37:50,160 --> 00:37:52,960
more useful as different subscribers

00:37:52,079 --> 00:37:55,920
implementation

00:37:52,960 --> 00:37:57,359
uh come into play so not much to say

00:37:55,920 --> 00:37:59,040
there

00:37:57,359 --> 00:38:00,960
okay there's another question coming

00:37:59,040 --> 00:38:03,680
from youtube so jeff

00:38:00,960 --> 00:38:05,520
barzeski i hope i pronounced that even

00:38:03,680 --> 00:38:07,520
remotely correctly

00:38:05,520 --> 00:38:09,119
how do you configure tracing to send its

00:38:07,520 --> 00:38:11,200
data to the various back ends

00:38:09,119 --> 00:38:12,960
are there docs that is also support

00:38:11,200 --> 00:38:16,839
cloud distributed tracing backends

00:38:12,960 --> 00:38:19,760
like aws x-ray so there are docs

00:38:16,839 --> 00:38:21,280
absolutely so if you go on the tracing

00:38:19,760 --> 00:38:24,320
subscriber

00:38:21,280 --> 00:38:25,440
crate there are very little docks on how

00:38:24,320 --> 00:38:28,960
to add different

00:38:25,440 --> 00:38:30,880
subscribers the tracing pipeline

00:38:28,960 --> 00:38:32,000
at the moment of course uh there are

00:38:30,880 --> 00:38:34,960
some type of

00:38:32,000 --> 00:38:36,960
trading subscribers implemented but i

00:38:34,960 --> 00:38:39,680
doubt there are trading subscribers for

00:38:36,960 --> 00:38:42,000
all the possible things uh if in

00:38:39,680 --> 00:38:45,599
specifically you wanna

00:38:42,000 --> 00:38:46,800
ship tracing data to x-ray i think

00:38:45,599 --> 00:38:48,720
the work that has been done in open

00:38:46,800 --> 00:38:50,880
telemetry for us means that you probably

00:38:48,720 --> 00:38:52,240
have an implementation of the standard

00:38:50,880 --> 00:38:54,240
and you might have to drive your own

00:38:52,240 --> 00:38:55,839
subscriber uh probably using resolver

00:38:54,240 --> 00:38:59,040
that should be not too complicated

00:38:55,839 --> 00:39:00,400
to actually ship it to x-ray but

00:38:59,040 --> 00:39:15,839
i haven't used it personally so i don't

00:39:00,400 --> 00:39:15,839
know if it's out there already

00:39:26,839 --> 00:39:29,839
so

00:40:04,000 --> 00:40:09,839
okay uh once again from solace waffle do

00:40:07,920 --> 00:40:10,319
you have an approach to avoid handling

00:40:09,839 --> 00:40:12,079
fields

00:40:10,319 --> 00:40:14,800
that contain personal identifiable

00:40:12,079 --> 00:40:16,240
information in telemetry data

00:40:14,800 --> 00:40:17,839
well the approach at this point is

00:40:16,240 --> 00:40:20,480
trying not to put up that

00:40:17,839 --> 00:40:22,079
which as i said before so responding to

00:40:20,480 --> 00:40:22,960
the other question can sometimes be

00:40:22,079 --> 00:40:26,319
tricky

00:40:22,960 --> 00:40:28,480
because of the way instrument works um

00:40:26,319 --> 00:40:30,560
so generally we do have a detection

00:40:28,480 --> 00:40:33,760
system here two layers so what we do is

00:40:30,560 --> 00:40:36,240
we continuously scan the logs uh create

00:40:33,760 --> 00:40:37,680
semantic passes that look for certain

00:40:36,240 --> 00:40:40,720
types of secrets that we know

00:40:37,680 --> 00:40:41,440
might possibly end up in logs like gmt

00:40:40,720 --> 00:40:44,560
tokens

00:40:41,440 --> 00:40:47,040
uh aws credentials and other types

00:40:44,560 --> 00:40:47,680
of secrets that we don't really want

00:40:47,040 --> 00:40:50,160
people to

00:40:47,680 --> 00:40:51,040
to have but at the application level of

00:40:50,160 --> 00:40:53,520
our switching

00:40:51,040 --> 00:40:54,800
instrument for being allowed all to be

00:40:53,520 --> 00:40:56,880
in denial

00:40:54,800 --> 00:40:58,720
we don't have necessarily any specific

00:40:56,880 --> 00:41:00,160
approach

00:40:58,720 --> 00:41:02,800
okay that's another question once again

00:41:00,160 --> 00:41:04,560
on youtube uh from jeff

00:41:02,800 --> 00:41:06,000
general question what is your preferred

00:41:04,560 --> 00:41:09,520
strategy for dealing with

00:41:06,000 --> 00:41:11,520
aeroband link okay interesting um

00:41:09,520 --> 00:41:13,119
in most of the applications we're

00:41:11,520 --> 00:41:15,119
driving at the moment

00:41:13,119 --> 00:41:16,560
we use a combination of this error and

00:41:15,119 --> 00:41:19,520
anyhow

00:41:16,560 --> 00:41:20,319
so we use this error for all the places

00:41:19,520 --> 00:41:23,440
but we need to

00:41:20,319 --> 00:41:25,839
end the letters so it's very nice to get

00:41:23,440 --> 00:41:27,280
uh structured names and you can match on

00:41:25,839 --> 00:41:28,720
and then do different things depending

00:41:27,280 --> 00:41:30,880
on the variant

00:41:28,720 --> 00:41:32,720
and then when we just want to report

00:41:30,880 --> 00:41:33,680
errors so we just want to have something

00:41:32,720 --> 00:41:36,319
that we log

00:41:33,680 --> 00:41:37,520
on with the time to people as a response

00:41:36,319 --> 00:41:39,280
then we use any out

00:41:37,520 --> 00:41:40,960
and we generally use them in conjunction

00:41:39,280 --> 00:41:43,440
so you might have

00:41:40,960 --> 00:41:45,040
an aerodynami which is using this error

00:41:43,440 --> 00:41:46,560
to get the error implementation

00:41:45,040 --> 00:41:49,040
and then the different variants are

00:41:46,560 --> 00:41:50,560
actually dropping in any hour

00:41:49,040 --> 00:41:52,720
and what we're starting to do recently

00:41:50,560 --> 00:41:54,560
once again leveraging the tracing crate

00:41:52,720 --> 00:41:57,760
is using tracing error

00:41:54,560 --> 00:42:00,400
so capturing spun traces uh in our

00:41:57,760 --> 00:42:01,920
errors so that when we get logs uh

00:42:00,400 --> 00:42:02,640
they're actually very detailed about

00:42:01,920 --> 00:42:17,839
what that pinch

00:42:02,640 --> 00:42:17,839
and that allows us to debug faster

00:43:36,079 --> 00:43:40,160
okay i guess that means it's all for

00:43:39,200 --> 00:43:43,839
today

00:43:40,160 --> 00:43:44,560
in terms of questions i've been asked by

00:43:43,839 --> 00:43:47,680
the friends

00:43:44,560 --> 00:43:51,760
of rusty bay's uh tweeker winner

00:43:47,680 --> 00:43:53,359
for a manning book promo code i assume

00:43:51,760 --> 00:43:55,760
uh in terms of best questions that's

00:43:53,359 --> 00:43:58,880
going to be solace waffle from twitch

00:43:55,760 --> 00:44:01,440
so i think you need to stay

00:43:58,880 --> 00:44:03,680
online for them to reach out to you it

00:44:01,440 --> 00:44:05,280
seems there's one more question though

00:44:03,680 --> 00:44:07,119
once again by you so that doesn't change

00:44:05,280 --> 00:44:08,400
the winner anyway

00:44:07,119 --> 00:44:10,240
what was your thought process for

00:44:08,400 --> 00:44:12,319
deciding to build this project in brust

00:44:10,240 --> 00:44:13,520
were there any attributes that made this

00:44:12,319 --> 00:44:14,800
project a good fit

00:44:13,520 --> 00:44:17,040
for the first production grass

00:44:14,800 --> 00:44:19,599
application at your company

00:44:17,040 --> 00:44:20,839
uh in terms of the application itself uh

00:44:19,599 --> 00:44:24,160
nothing specifically

00:44:20,839 --> 00:44:26,079
like we're talking of basically a client

00:44:24,160 --> 00:44:27,920
of an api that we suppose publicly is

00:44:26,079 --> 00:44:30,800
going to power a ui

00:44:27,920 --> 00:44:32,560
um so it's not necessarily doesn't need

00:44:30,800 --> 00:44:35,440
necessarily to be the fastest

00:44:32,560 --> 00:44:36,319
uh doesn't mean necessarily all the

00:44:35,440 --> 00:44:38,480
guarantees that

00:44:36,319 --> 00:44:39,680
gives you we could have done that in any

00:44:38,480 --> 00:44:42,240
language

00:44:39,680 --> 00:44:43,680
but we were looking to use rust for

00:44:42,240 --> 00:44:45,839
other types of projects

00:44:43,680 --> 00:44:46,720
so for mission critical projects in

00:44:45,839 --> 00:44:50,800
particular

00:44:46,720 --> 00:44:52,880
to leverage rus very strong type system

00:44:50,800 --> 00:44:54,960
as combined with this very predictable

00:44:52,880 --> 00:44:58,160
performance profile

00:44:54,960 --> 00:45:01,119
but it's somewhat of a big leap to

00:44:58,160 --> 00:45:03,040
adopt a new language when driving a new

00:45:01,119 --> 00:45:05,200
mission critical project only to find

00:45:03,040 --> 00:45:06,480
out when you actually release it

00:45:05,200 --> 00:45:09,440
uh that you might have with it all the

00:45:06,480 --> 00:45:10,000
time so this was a very nice incremental

00:45:09,440 --> 00:45:13,040
step

00:45:10,000 --> 00:45:14,480
to de-risk the technology so for example

00:45:13,040 --> 00:45:16,800
look at all the observability

00:45:14,480 --> 00:45:18,240
situations say is this actually ready

00:45:16,800 --> 00:45:20,319
for what we need to do

00:45:18,240 --> 00:45:21,359
and look at all the things that we need

00:45:20,319 --> 00:45:23,359
in an api

00:45:21,359 --> 00:45:25,359
and can we actually provide apis with

00:45:23,359 --> 00:45:28,319
this and so on and so forth

00:45:25,359 --> 00:45:29,680
so it was very much the risk operation

00:45:28,319 --> 00:45:31,839
and as with the risk all of these

00:45:29,680 --> 00:45:33,200
aspects then it became possible for us

00:45:31,839 --> 00:45:35,599
to say okay

00:45:33,200 --> 00:45:37,119
now we can confidently bet on it for

00:45:35,599 --> 00:45:38,800
this other new product that we want to

00:45:37,119 --> 00:45:41,839
do and there's a huge project

00:45:38,800 --> 00:45:42,720
and that fits ras profile for a variety

00:45:41,839 --> 00:45:44,000
of reasons

00:45:42,720 --> 00:45:46,400
and now we know we're not risking too

00:45:44,000 --> 00:45:48,880
much we're still taking some risk

00:45:46,400 --> 00:45:50,079
but it's not as big of a risk of passing

00:45:48,880 --> 00:46:01,839
from small cli

00:45:50,079 --> 00:46:01,839
to mission critical product as they do

00:46:32,640 --> 00:46:36,240
okay it's goodbye time so thanks a lot

00:46:35,359 --> 00:46:39,520
for

00:46:36,240 --> 00:46:42,640
tuning in for rusty days and stay

00:46:39,520 --> 00:46:43,760
for the next talk from dick mcmannara on

00:46:42,640 --> 00:46:47,520
unsafe goods

00:46:43,760 --> 00:46:47,520

YouTube URL: https://www.youtube.com/watch?v=HtKnLiFwHJM


