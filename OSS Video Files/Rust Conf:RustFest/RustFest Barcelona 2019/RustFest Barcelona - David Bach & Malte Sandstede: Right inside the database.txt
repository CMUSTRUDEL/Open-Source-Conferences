Title: RustFest Barcelona - David Bach & Malte Sandstede: Right inside the database
Publication date: 2019-11-27
Playlist: RustFest Barcelona 2019
Description: 
	Modern, stateful web applications are increasingly dependent on high-frequency updates from multiple database systems. Each application maintains its own dynamic view of shared and private data. What if querying and synchronizing this state consistently was as simple as writing a Datalog query?

Using a reactive query engine powered by differential computation built in Rust, we explore this question. We present a novel architecture that selectively and incrementally replicates state between server and application and thus allows you to develop as if you were sitting right inside of the database.

https://barcelona.rustfest.eu/sessions/right-inside-the-database
Captions: 
	00:00:07,609 --> 00:00:11,099
MALTE: Hi.

00:00:11,099 --> 00:00:19,099
I want you to imagine a world where obtaining backend data is as simple as writing a query,

00:00:19,099 --> 00:00:26,660
where your apps are always up-to-date, where offline-first is only a caching decision about

00:00:26,660 --> 00:00:32,180
what kind of your backend you replicate in your frontend, and where object relational

00:00:32,180 --> 00:00:39,840
matters or manual graph, cure resolvers or mappers in general don't even exist.

00:00:39,840 --> 00:00:47,530
We want you to imagine a world where you are right inside the database, and with that,

00:00:47,530 --> 00:00:48,530
welcome.

00:00:48,530 --> 00:00:49,530
This is David.

00:00:49,530 --> 00:00:50,530
My name is Malte.

00:00:50,530 --> 00:00:56,480
We work at our own small software consultancy where we consult people on the stuff we are

00:00:56,480 --> 00:01:04,699
talking about today and a lot of the stuff originated from the ETH Zurich systems group

00:01:04,699 --> 00:01:10,260
and also Frank McSherry, who is the creator of some of the systems we are going to introduce

00:01:10,260 --> 00:01:13,960
to you today.

00:01:13,960 --> 00:01:23,500
So what better way to start a Rust talk than to show some JavaScript, so of course if we

00:01:23,500 --> 00:01:28,720
start at the frontend and want to make this utopia come true of writing them in a declarative

00:01:28,720 --> 00:01:34,570
manner, we could just imagine what this should look like.

00:01:34,570 --> 00:01:40,460
At the bottom half of the slide you can see the frontend function, so it's just a component,

00:01:40,460 --> 00:01:42,830
and we just describe what we want to display.

00:01:42,830 --> 00:01:44,610
Some senders.

00:01:44,610 --> 00:01:47,690
And at the top we see a declarative query.

00:01:47,690 --> 00:01:55,470
This is written in Datalog but it is the names of the senders of the messages that the current

00:01:55,470 --> 00:02:03,260
user has received, so you can see on the message ID slide there, message ID entity there is

00:02:03,260 --> 00:02:08,069
the recipient and then it matches all the sender IDs and gets the names from them, so

00:02:08,069 --> 00:02:12,400
you could also write SQL if you are more inclined to do so.

00:02:12,400 --> 00:02:18,760
This gives us a very declarative manner of specifying our data needs in the frontend,

00:02:18,760 --> 00:02:24,890
and since we register this query, as you can see down below, what we want to say with that

00:02:24,890 --> 00:02:30,860
is that you also don't have to worry about at what time this data arrives in our system.

00:02:30,860 --> 00:02:37,180
We always think that our data just will flow into the system and changes will propagate

00:02:37,180 --> 00:02:39,810
through the system as soon as they arrive.

00:02:39,810 --> 00:02:45,130
We in the frontend don't have to care about this at all.

00:02:45,130 --> 00:02:49,750
So declarative and reactive frontends are something you can kind of have today if you

00:02:49,750 --> 00:02:54,770
look at libraries such as React or think about component queries, but now if we look at the

00:02:54,770 --> 00:03:01,570
whole Full Stack side of things, things look very differently so we are not really reactive

00:03:01,570 --> 00:03:04,599
anymore since we mostly resort to polling.

00:03:04,599 --> 00:03:09,660
We ask time and again our backend: is there something new, is there something new, without

00:03:09,660 --> 00:03:12,510
knowing if there is something new.

00:03:12,510 --> 00:03:17,060
There is also a translation step between the application server and the source of truth

00:03:17,060 --> 00:03:25,360
since, for example, if we use an object relational matter or resolvers we always have to speak

00:03:25,360 --> 00:03:31,450
to our source of truth and translate the queries we formulate in the frontend so that our applications,

00:03:31,450 --> 00:03:37,500
we can actually understand them and the backend can understand them.

00:03:37,500 --> 00:03:42,330
What we would rather like to see is something that looks like this and is slightly stolen

00:03:42,330 --> 00:03:53,099
from a great blog post, "The source of truth of tomorrow", and the first we can use is

00:03:53,099 --> 00:04:04,000
by using a declarative query, what type should be accessible, so this is in the server.

00:04:04,000 --> 00:04:09,310
We also want to formulate what of the data is currently relevant.

00:04:09,310 --> 00:04:14,270
This is the query you just saw in the frontend JavaScript code thing where we specified what

00:04:14,270 --> 00:04:20,220
data we are actually interested in, and lastly we want to push this data into the frontend

00:04:20,220 --> 00:04:24,999
instead of asking for it and pulling it, to make things reactive.

00:04:24,999 --> 00:04:30,589
So if we want to re-evaluate how we build this Full Stack architecture, the first thing

00:04:30,589 --> 00:04:39,020
we have to look for is a push-based programming model, to avoid, or to become reactive.

00:04:39,020 --> 00:04:45,080
If we look somewhere we might stumble upon the dataflow programming model and this is

00:04:45,080 --> 00:04:50,629
really a nice way to model these kind of things because in the dataflow programming model

00:04:50,629 --> 00:04:53,779
you do it as graphs.

00:04:53,779 --> 00:05:03,439
The nodes are operators and they do some work, for example if you have a map operator it

00:05:03,439 --> 00:05:09,180
takes some data and it will map over it and produce a result based on it.

00:05:09,180 --> 00:05:14,569
The edges specify from which operator to which operator data can flow.

00:05:14,569 --> 00:05:19,649
For example, if you have a join operator it probably has to input edges and produces a

00:05:19,649 --> 00:05:22,680
single output edge.

00:05:22,680 --> 00:05:25,759
This offers some nice characteristics.

00:05:25,759 --> 00:05:29,449
For example, there is no central co-ordination needed.

00:05:29,449 --> 00:05:33,409
Data co-ordinates the execution of the data flow.

00:05:33,409 --> 00:05:37,979
Secondly, and this plays nicely with Rust or if you want to implement this in Rust,

00:05:37,979 --> 00:05:44,889
we don't really need to fight the borrow checker since every operator owns its own part of

00:05:44,889 --> 00:05:51,129
the data so there are never two separate entities owning the same part of data and fighting

00:05:51,129 --> 00:05:52,750
over it.

00:05:52,750 --> 00:05:59,430
Lastly, it's quite easy to distribute since we can just clone operators and chart the

00:05:59,430 --> 00:06:05,720
data we input into our system and then just feed one operator, one half of the data and

00:06:05,720 --> 00:06:11,589
another one the other, and this way parallel with ease.

00:06:11,589 --> 00:06:16,229
And an implementation in Rust based on this dataflow model is called timely data flow

00:06:16,229 --> 00:06:24,319
and it offers those that can even be cyclic and this can be something like this so this

00:06:24,319 --> 00:06:33,449
is a Rust program using Timely Dataflow and the first thing is you always write it from

00:06:33,449 --> 00:06:38,740
the perspective of a single worker so while constructing my dataflow I don't have to worry

00:06:38,740 --> 00:06:40,969
about distribution or parallelisation.

00:06:40,969 --> 00:06:45,499
I can do all of that stuff later on.

00:06:45,499 --> 00:06:52,199
Then comes the first stage of dataflow programming, the creation of the actual dataflow so this

00:06:52,199 --> 00:06:58,959
is how we construct the dataflow from small operators that are chained together and to

00:06:58,959 --> 00:07:03,879
the right here you can see a visualisation of that, so we are ingesting some data, exchanging

00:07:03,879 --> 00:07:07,879
it to possibly other workers and then inspecting it.

00:07:07,879 --> 00:07:13,150
Then, as the second stage of the dataflow programming we are actually running it.

00:07:13,150 --> 00:07:17,539
So now we are feeding - after we have created the structure, we are now feeding input into

00:07:17,539 --> 00:07:22,440
the dataflow to compute something.

00:07:22,440 --> 00:07:27,969
So now that we know what dataflows are all about, we can re-examine our introductory

00:07:27,969 --> 00:07:33,999
problem of thinking about how we can build a Full Stack architecture using that, and

00:07:33,999 --> 00:07:41,139
if we squint a little it could look something like this, where we have the ingesting operators

00:07:41,139 --> 00:07:47,509
in our dataflow being the source of truth, then we have the application server level

00:07:47,509 --> 00:07:52,330
which is the actual dataflow computation and which will narrow our data down to relevant

00:07:52,330 --> 00:08:01,059
data and apply access policies and lastly the exit nodes are our frontend, and of course

00:08:01,059 --> 00:08:06,189
we won't model our frontend as a dataflow operator, so this is more of a conceptual

00:08:06,189 --> 00:08:12,849
dataflow sync but we can actually model all of the backend stuff as a dataflow, as a real

00:08:12,849 --> 00:08:16,659
dataflow, or in this case a timely dataflow.

00:08:16,659 --> 00:08:21,210
So what does this look like?

00:08:21,210 --> 00:08:26,110
The first thing we have to worry about is how we now model our data because we are not

00:08:26,110 --> 00:08:31,610
anymore in a static mindset where we have just a database sitting somewhere and we query

00:08:31,610 --> 00:08:37,310
it but we are talking about dataflows and as the name suggests data is streamed through

00:08:37,310 --> 00:08:38,770
this system.

00:08:38,770 --> 00:08:44,110
What we propose is using a fully normalised attribute-oriented data model, so something

00:08:44,110 --> 00:08:50,760
like the six normal form and in that data model the fundamental unit is the fact, a

00:08:50,760 --> 00:08:53,220
triple of entity, attribute and value.

00:08:53,220 --> 00:08:58,500
For example, if you have an entity one it might have an attribute name and the value

00:08:58,500 --> 00:09:01,300
of that attribute is Peter.

00:09:01,300 --> 00:09:08,960
This allows us to freely compose very small units of data into higher level concepts,

00:09:08,960 --> 00:09:14,520
so for example if we want to model a person and her residence, she might have a name,

00:09:14,520 --> 00:09:19,430
she might have an age and a residence, and the residence then again links to the entity

00:09:19,430 --> 00:09:26,830
ID of the resident facts we define below and this composes, as is quite straightforward

00:09:26,830 --> 00:09:34,459
to see, to a higher level concept such as a person or a resident.

00:09:34,459 --> 00:09:38,920
If we model this in a database setting we would have a separate table, since we are

00:09:38,920 --> 00:09:41,180
completely normalised, for every attribute.

00:09:41,180 --> 00:09:46,690
So, for example, the pers/name has an entity column and a value column.

00:09:46,690 --> 00:09:51,810
If we want to transfer this to the dataflow system where we think about streams we can

00:09:51,810 --> 00:09:58,060
think about entity and value streams so we introduce the notion of time because in a

00:09:58,060 --> 00:10:04,779
streaming world we have to somehow define at what point this data arrived, so Alice,

00:10:04,779 --> 00:10:11,800
for example, the name Alice was introduced at t0 and we also want to make multiplicities

00:10:11,800 --> 00:10:16,139
or differences explicit because data can change in this stream.

00:10:16,139 --> 00:10:23,370
Alice's name as of t0 is Alice but later on she changed her name to Bob, so in this case

00:10:23,370 --> 00:10:29,170
by using these multiplicities and additions and retractions, we can say at t2 we retract

00:10:29,170 --> 00:10:35,740
the fact Alice, or the first name is Alice for entity 1, and we add the fact Bob for

00:10:35,740 --> 00:10:38,139
entity 1.

00:10:38,139 --> 00:10:43,670
This also allows us to become very reactive since now we can observe or make explicit

00:10:43,670 --> 00:10:47,660
every single data change in our system.

00:10:47,660 --> 00:10:52,089
If we now want to arrive at this higher level concept of a whole person, what we are going

00:10:52,089 --> 00:10:56,899
to do is we consolidate the streams, that is we add together all these differences and

00:10:56,899 --> 00:11:04,120
subtract the ones that are negative so that we obtain a consistent view as of a common

00:11:04,120 --> 00:11:10,319
t*, so for example if you have a common pers/name stream, age which will probably update more

00:11:10,319 --> 00:11:18,060
frequently and residents, we just sum up all these facts to obtain the person table as

00:11:18,060 --> 00:11:22,189
of some consistent time, t*.

00:11:22,189 --> 00:11:23,670
Cool.

00:11:23,670 --> 00:11:29,519
Now we have a data model that can express everything we want from our Full Stack application

00:11:29,519 --> 00:11:34,940
in a dataflow setting but what is still missing is making things declarative and formulating

00:11:34,940 --> 00:11:41,439
queries on top of them, so the first thing we want to do is register a query, again so

00:11:41,439 --> 00:11:46,819
that every time something changes updates are pushed, for example via a websocket connection

00:11:46,819 --> 00:11:52,380
to our frontend, and we also want to make sure that we can even formulate very complex

00:11:52,380 --> 00:11:56,180
queries such as access policies using that model.

00:11:56,180 --> 00:12:01,689
So here, this is a reachability query, so for whatever reason we might be interested

00:12:01,689 --> 00:12:06,529
in finding out all of Alice's friends and friends of friends, and again it's Datalog

00:12:06,529 --> 00:12:11,610
but basically what it does is it takes the first clause and matches it, so this would

00:12:11,610 --> 00:12:17,519
be direct friends, or it matches the second clause which is a direct friend of one of

00:12:17,519 --> 00:12:22,149
the friends or friends of friends we have already discovered.

00:12:22,149 --> 00:12:29,209
Even if you don't read Datalog all day, what you can see here just from syntax is that

00:12:29,209 --> 00:12:34,259
we are still using these facts we defined before, even now that we are in our query

00:12:34,259 --> 00:12:35,259
world.

00:12:35,259 --> 00:12:40,630
What is not so straightforward is that to match these clauses we have to introduce additional

00:12:40,630 --> 00:12:45,709
operators, so for example here matching on the hop is nothing more than a relational

00:12:45,709 --> 00:12:52,540
join, so we have to have some kind of means to express this in a dataflow setting and

00:12:52,540 --> 00:12:58,490
iteratively calling the reachability query means we need some kind of iterate clause.

00:12:58,490 --> 00:13:03,889
Especially if you know stream processing and dataflows this is often very hard to do and

00:13:03,889 --> 00:13:08,870
fortunately again there is already a system, again written in Rust, that solves this.

00:13:08,870 --> 00:13:12,889
It's called differential dataflow and it provides complex operators.

00:13:12,889 --> 00:13:18,509
So to the left you can see a differential dataflow having these iterate operators and

00:13:18,509 --> 00:13:24,459
this join rate and it looks like any other Rust programme and the cool thing is that

00:13:24,459 --> 00:13:31,459
also it can incrementalise these operators, so if we at some later point in time change

00:13:31,459 --> 00:13:36,250
a fact, we don't have to recompute all of the friends of friends from scratch, but can

00:13:36,250 --> 00:13:41,269
just do work in the order of change.

00:13:41,269 --> 00:13:47,480
So now that we've covered how we can actually model these complex query matching clauses

00:13:47,480 --> 00:13:52,740
in our dataflow world, what is still missing is kind of obvious: how do we get from the

00:13:52,740 --> 00:13:56,019
query representation to the Rust representation?

00:13:56,019 --> 00:13:58,380
This is what David is going to talk about.

00:13:58,380 --> 00:14:00,500
DAVID: Okay, cool.

00:14:00,500 --> 00:14:02,630
All right, yes.

00:14:02,630 --> 00:14:08,370
So Malte just sort of explained the way we end up with these timing programmes written

00:14:08,370 --> 00:14:14,639
in Rust and we know how Rust works, we write this programme, we compile it and we run it,

00:14:14,639 --> 00:14:19,370
and the idea of these dataflow programmes that you start to run them and they run indefinitely

00:14:19,370 --> 00:14:21,819
because we never know if we are done with the input.

00:14:21,819 --> 00:14:26,999
There could be potentially unbounded new input coming in and we always want results given

00:14:26,999 --> 00:14:32,490
these new inputs but as a client talking to some sort of database we always have this

00:14:32,490 --> 00:14:37,879
interaction role that a client comes at some arbitrary point in time, asks the database

00:14:37,879 --> 00:14:42,910
to compute some query, the database computes the query, the client takes the result and

00:14:42,910 --> 00:14:44,399
minds its own business.

00:14:44,399 --> 00:14:48,810
So this is what this match between aesthetically compiled Rust programme on the right-hand

00:14:48,810 --> 00:14:56,879
side doing this dataflow business and a dynamic query from client - and what we build is something

00:14:56,879 --> 00:15:05,459
called 3DF, which stands for declarative dataflow and what you can see here is an excerpt of

00:15:05,459 --> 00:15:06,860
the plan extraction.

00:15:06,860 --> 00:15:10,939
We have this Rust data structure.

00:15:10,939 --> 00:15:19,749
That is the idea that it provides the server with a plan of how to implement these operators.

00:15:19,749 --> 00:15:26,480
So the idea is that we enumerate all possible operations that clients could ask us for,

00:15:26,480 --> 00:15:31,709
for example the join operation, and when we give the server or someone implementing this

00:15:31,709 --> 00:15:36,589
library this plan struct he will just walk through it and implement all these plans.

00:15:36,589 --> 00:15:42,870
What he will do when he implements this plan is we basically take all these operators we

00:15:42,870 --> 00:15:50,059
saw earlier from Malte, we stick them together and for example a website with a stream of

00:15:50,059 --> 00:15:53,790
the resulting data.

00:15:53,790 --> 00:16:02,459
Perhaps time for a recap.

00:16:02,459 --> 00:16:09,309
The first thing was basically having a run time to run these in.

00:16:09,309 --> 00:16:14,660
Then we talked how do we model the data such that it makes sense in a streaming setting,

00:16:14,660 --> 00:16:19,089
having all these normalised streams coming into our database or into our system.

00:16:19,089 --> 00:16:28,070
I just told you how we can make this all dynamic by using this layer that in a running Rust

00:16:28,070 --> 00:16:35,510
program constructs these dataflows and hands us back the resulting data or the results.

00:16:35,510 --> 00:16:40,980
Using this declarative layer we can actually now query this system and the system will

00:16:40,980 --> 00:16:46,390
produce these dataflows and keeps us up-to-date with all the data needs we might have as a

00:16:46,390 --> 00:16:47,390
client.

00:16:47,390 --> 00:16:52,319
But of course, we need to now think of how this now fits into the frontend story, so

00:16:52,319 --> 00:16:57,170
we are going to do this now.

00:16:57,170 --> 00:17:02,529
So what we are supposed to do here with this talk is giving you the idea that a client

00:17:02,529 --> 00:17:10,040
is actually also just a peer through the database, and in another sense the client basically

00:17:10,040 --> 00:17:14,990
has the illusion that he sits just next to the database, or inside the database, so he

00:17:14,990 --> 00:17:20,319
has all the query power that a normal relational database has and the whole rest of this data

00:17:20,319 --> 00:17:22,490
is extracted away.

00:17:22,490 --> 00:17:28,559
When we start thinking of this, just as a quick reminder, maybe relational databases

00:17:28,559 --> 00:17:33,690
are also created at modelling states for the frontend themselves, so if you have some frontend

00:17:33,690 --> 00:17:39,559
application that some web page, different things, you can think of this state of this

00:17:39,559 --> 00:17:46,360
web page as being the database and if different parts need different data it will just query

00:17:46,360 --> 00:17:48,580
the database and ask for some data.

00:17:48,580 --> 00:17:52,510
If it wants to do some transitions, for example someone puts in data into the text fields,

00:17:52,510 --> 00:17:58,960
it will transact this into the database so there is already a good fit as using a relational

00:17:58,960 --> 00:18:05,740
database as a means of modelling frontend state.

00:18:05,740 --> 00:18:10,450
When we talk about databases usually we say we have a query and the query will be evaluated

00:18:10,450 --> 00:18:15,580
and we get a result set, but the whole story we are telling here is we want to replicate

00:18:15,580 --> 00:18:21,320
the query for the client so the client feels he is right inside the database, so what we

00:18:21,320 --> 00:18:26,610
want is this function f that takes the database and gives us back this selective database

00:18:26,610 --> 00:18:31,490
that only contains the data that is accessible, relevant and important for the application

00:18:31,490 --> 00:18:34,740
itself.

00:18:34,740 --> 00:18:41,870
So how do we go from queries to these replication queries, what is this function f that makes

00:18:41,870 --> 00:18:48,190
it possible for the database to be replicated in this sense?

00:18:48,190 --> 00:18:53,480
The first thing to notice is that, if we use the same data model it will make it easier

00:18:53,480 --> 00:18:57,760
so we will use this same thing that Malte told us earlier, that our application database

00:18:57,760 --> 00:19:06,159
is also this database of facts, making up our local database, and there is one implementation

00:19:06,159 --> 00:19:11,039
in JavaScript called data script that will do that, gives us a Datalog interface over

00:19:11,039 --> 00:19:13,950
this local application database.

00:19:13,950 --> 00:19:21,260
Our job is now how do we populate this database with all the facts that this client is interested

00:19:21,260 --> 00:19:23,220
in seeing?

00:19:23,220 --> 00:19:29,460
The idea is to just query for all the attributes that the client is interested in.

00:19:29,460 --> 00:19:35,159
These attributes are in itself binary relations, so that when we query it we get back some

00:19:35,159 --> 00:19:42,330
triplets and attribute value that the client can put into its local database.

00:19:42,330 --> 00:19:46,490
To make this a bit clearer I have come up with a simple example.

00:19:46,490 --> 00:19:52,620
On the right-hand side again we see this query struct or the query in Datalog where we have

00:19:52,620 --> 00:19:57,020
some message sender query we ask, and the query body, starting with the find and having

00:19:57,020 --> 00:20:01,731
all these clauses, and if we look at what the find says, it says we want the message

00:20:01,731 --> 00:20:06,220
ID which corresponds to the entity we are wanting to find.

00:20:06,220 --> 00:20:11,490
We have a value which is value of the thing, and of course we give it a name and this is

00:20:11,490 --> 00:20:16,340
sort of the fact, so the right-hand side query, message sender query, we are resolving a stream

00:20:16,340 --> 00:20:24,140
of resolved entity attributes that we can put into our local database, thus replicating

00:20:24,140 --> 00:20:27,600
it at our local database.

00:20:27,600 --> 00:20:32,240
The cool thing is we don't need to necessarily only replicate data; we can potentially use

00:20:32,240 --> 00:20:38,159
the full power of our Datalog or SQL query language to do some aggregations as Malte

00:20:38,159 --> 00:20:40,220
showed us earlier.

00:20:40,220 --> 00:20:47,779
We have to describe all the data needs that we would like to have replicated in our local

00:20:47,779 --> 00:20:51,620
database.

00:20:51,620 --> 00:20:57,630
Some of these things may seem a bit effortful because we need to take all these different

00:20:57,630 --> 00:21:02,230
attributes maybe, corresponding for example to the same entity, but there is something

00:21:02,230 --> 00:21:07,190
called a pull syntax where we can see I want all the attributes corresponding to some one

00:21:07,190 --> 00:21:08,269
entity.

00:21:08,269 --> 00:21:09,700
It just makes a few things easier.

00:21:09,700 --> 00:21:17,630
All right, now let's step back again and have a look at what we've covered so far.

00:21:17,630 --> 00:21:23,470
From the beginning we started with this system that allows us to write these dataflow programs,

00:21:23,470 --> 00:21:29,019
distribute dataflow programmes and gives us this push access model.

00:21:29,019 --> 00:21:37,549
We showed you 3DF which allows us to write these queries and the queries are our means

00:21:37,549 --> 00:21:42,010
of describing the data interest we want and also describes all the access policy that

00:21:42,010 --> 00:21:46,399
is potentially necessary to enforce privacy.

00:21:46,399 --> 00:21:51,299
Then now we covered, when these streams come in, or these entity attribute value streams

00:21:51,299 --> 00:21:56,300
come in, we put them into our local database and the rest of the system of the frontend

00:21:56,300 --> 00:22:02,750
can talk to this replicated database right now and render its messages, for example,

00:22:02,750 --> 00:22:07,059
from this local database, and does not have to care about all the rest of the stack being

00:22:07,059 --> 00:22:12,040
kept in sync, so to say.

00:22:12,040 --> 00:22:15,610
The cool thing about this is that all these errors we were showing earlier in all the

00:22:15,610 --> 00:22:20,899
thing, it's just one declarative query that describes this one attribute message sender

00:22:20,899 --> 00:22:26,240
and the rest is basically taken care of, so one declarative query pulled them all, so

00:22:26,240 --> 00:22:30,470
to say.

00:22:30,470 --> 00:22:36,820
We are nearing the end of the talk so we thought we would include a bit of a - what sort of

00:22:36,820 --> 00:22:38,669
open things are there currently occurring?

00:22:38,669 --> 00:22:44,149
The first is we are using this data script thing for our frontend application, but 3DF

00:22:44,149 --> 00:22:56,309
is written in Rust and we can use literally the same code as is used in the big end machines

00:22:56,309 --> 00:23:00,620
and this is something we have been playing in a bit and there is a point if you are interested

00:23:00,620 --> 00:23:02,080
in this.

00:23:02,080 --> 00:23:08,960
Of course, we love Datalog but other people might not, so this Plan struct I showed earlier

00:23:08,960 --> 00:23:18,290
is an easy target for SQL or GraphQL, other languages, so we could simply write a GraphQL

00:23:18,290 --> 00:23:25,570
path which then gives it to our database or to the system and it will work just fine.

00:23:25,570 --> 00:23:29,320
All right, let's see what we did here.

00:23:29,320 --> 00:23:35,212
So we started off with the way it used to be, arrows pointing up because people have

00:23:35,212 --> 00:23:40,760
to pull down all the data they are interested in, transform it and potentially in different

00:23:40,760 --> 00:23:46,330
languages, and the first thing we did was introduced a dataflow and push-based model

00:23:46,330 --> 00:23:54,380
where all these arrows are push-based but we still query languages up on the top to

00:23:54,380 --> 00:23:57,960
talk about in database languages.

00:23:57,960 --> 00:24:04,340
I hope what we've sort of conveyed now is that we pushed relational queries all the

00:24:04,340 --> 00:24:10,490
way down to the frontend itself so it's all written as a single Datalog query and the

00:24:10,490 --> 00:24:16,779
frontend is also in a relational query so in a sense we put the frontend right inside

00:24:16,779 --> 00:24:19,970
the database.

00:24:19,970 --> 00:24:25,809
Thank you.

00:24:25,809 --> 00:24:30,020
[Applause] >> Thank you for that great talk.

00:24:30,020 --> 00:24:31,960
Are there any questions in the room?

00:24:31,960 --> 00:24:34,229
Please raise your hand.

00:24:34,229 --> 00:24:35,919
>> Hi.

00:24:35,919 --> 00:24:39,230
A great talk.

00:24:39,230 --> 00:24:45,440
So an association I immediately have when I see attribute value is RDF where you have

00:24:45,440 --> 00:24:55,320
subject, predicate, object, and I was wondering the semantic web which is based also on this

00:24:55,320 --> 00:25:00,279
would benefit tremendously from having a client-side database.

00:25:00,279 --> 00:25:09,610
Projects are already working on that but do you think this might be an alternative to

00:25:09,610 --> 00:25:13,750
work, for example, in the [inaudible]?

00:25:13,750 --> 00:25:19,860
DAVID: I would say, if you are interested in these sort of incrementalising these updates,

00:25:19,860 --> 00:25:26,200
so if your database is really interested in getting push-based access to all the rest

00:25:26,200 --> 00:25:31,580
of the data that is potentially needed, then of course, the stuff I showed you is simply

00:25:31,580 --> 00:25:32,580
super early.

00:25:32,580 --> 00:25:39,789
So if there are some funny cursor movements and colours.

00:25:39,789 --> 00:25:42,600
I wouldn't at all suggest you use this right now.

00:25:42,600 --> 00:25:50,539
But definitely, in the future, yes, I would think so, yes.

00:25:50,539 --> 00:25:52,200
>> Next question.

00:25:52,200 --> 00:25:57,250
>> Thank you very much.

00:25:57,250 --> 00:25:59,780
Great talk.

00:25:59,780 --> 00:26:13,730
I am totally on board with the idea of moving relational queries down to the client, however

00:26:13,730 --> 00:26:20,440
in many contexts where you have this version, the clients are - it's running on someone

00:26:20,440 --> 00:26:28,679
else's machine and now that it's providing them with a lot of additional power they might

00:26:28,679 --> 00:26:33,360
decide to run a query that is just going to make your backend slow down to a crawl.

00:26:33,360 --> 00:26:35,440
Have you thought about how to deal with this?

00:26:35,440 --> 00:26:36,440
DAVID: Yes, we did.

00:26:36,440 --> 00:26:38,799
We had a slide but we thought maybe that would be too much.

00:26:38,799 --> 00:26:40,500
Totally fair.

00:26:40,500 --> 00:26:44,370
Usually you wouldn't trust any client to just execute any query, of course.

00:26:44,370 --> 00:26:47,770
There are two things to this story, basically.

00:26:47,770 --> 00:26:54,780
One is worst case, which is like a research topic where you can do all these join operations

00:26:54,780 --> 00:27:01,049
in that even if clients ask for weird join queries we won't throw up the whole server

00:27:01,049 --> 00:27:04,909
because we are smart in these worst case cases.

00:27:04,909 --> 00:27:17,170
If we restrict the aggregations to only count - as long as we don't allow user-defined aggregations,

00:27:17,170 --> 00:27:20,169
it couldn't blow up too much in a sense.

00:27:20,169 --> 00:27:23,990
Last part is basically access policy, so the access policy must be enforced not by the

00:27:23,990 --> 00:27:31,299
client itself but of course by our system, but yes, great question.

00:27:31,299 --> 00:27:34,980
>> The last question?

00:27:34,980 --> 00:27:36,690
Anyone?

00:27:36,690 --> 00:27:43,529
>> Thank you, great talk.

00:27:43,529 --> 00:27:48,419
Some time ago I wrote down an application with Kafka Streams for interactive queries.

00:27:48,419 --> 00:27:54,970
I didn't do it properly as they suggested and I was moving all the state to all the

00:27:54,970 --> 00:28:01,580
nodes in the front which gave us a lot of problems, especially when a node goes down

00:28:01,580 --> 00:28:02,580
and needs to recover.

00:28:02,580 --> 00:28:07,490
There is a lot of overhead of gigabytes of data being moved.

00:28:07,490 --> 00:28:12,640
How do you deal with boot strapping and fault re-runs?

00:28:12,640 --> 00:28:19,710
MALTE: Yes, so also a very good question and I think there are multiple projects that are

00:28:19,710 --> 00:28:25,159
trying to replicate basically the whole database on the frontend side.

00:28:25,159 --> 00:28:31,669
One thing to tackle this is, of course, because we can write the specific queries we already

00:28:31,669 --> 00:28:36,330
narrow down the data to the data that is actually relevant to the query.

00:28:36,330 --> 00:28:41,980
The other thing is, for example, if you want to recover from a node breaking down, that

00:28:41,980 --> 00:28:47,980
you don't have to send all of this data over as data, so not only the differences but that

00:28:47,980 --> 00:28:53,960
you can, of course, think about creating snapshots, similar to what we showed about the table

00:28:53,960 --> 00:29:00,960
as of t* so that you can send over an entity just as a single serialised entity and not

00:29:00,960 --> 00:29:05,770
have to replay all the work you've done before.

00:29:05,770 --> 00:29:07,830
Yes.

00:29:07,830 --> 00:29:12,600
But of course it's a real concern, especially in the frontend part.

00:29:12,600 --> 00:29:13,600
Yes.

00:29:13,600 --> 00:29:14,709
>> Thank you.

00:29:14,709 --> 00:29:15,709
>> Thanks.

00:29:15,709 --> 00:29:16,909
Give a great hand to David and Malte.

00:29:16,909 --> 00:29:16,989

YouTube URL: https://www.youtube.com/watch?v=6aTXW5mOlj4


