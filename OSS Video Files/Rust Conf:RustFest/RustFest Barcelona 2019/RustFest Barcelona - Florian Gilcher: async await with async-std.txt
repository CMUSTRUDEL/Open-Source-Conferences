Title: RustFest Barcelona - Florian Gilcher: async await with async-std
Publication date: 2019-11-28
Playlist: RustFest Barcelona 2019
Description: 
	async/await will be stabilised around RustFest Barcelona. This talk gives an overview of the existing ecosystem by then as well as practical guidance on how to use the feature. It puts a focus on modern async programming instead of comparisons with the past. For exemplary purposes, I will use async-std, a library built from the ground up around std::futures::Future and the modern async model.

https://barcelona.rustfest.eu/sessions/async-await-with-async-std
Captions: 
	00:00:07,669 --> 00:00:10,849
FLORIAN: Okay, here we go.

00:00:10,849 --> 00:00:13,559
Yes, so hello and welcome.

00:00:13,559 --> 00:00:18,470
This is a very special talk to me because that means I'm so out of organising this conference

00:00:18,470 --> 00:00:23,699
that they are letting me give talks, so just as an illustration there.

00:00:23,699 --> 00:00:25,080
So who am I?

00:00:25,080 --> 00:00:27,190
I'm Florian, you can find me on Twitter and GitHub.

00:00:27,190 --> 00:00:30,980
I own two companies, one that has nothing to do with programming languages and the other

00:00:30,980 --> 00:00:38,789
one that does just Rust many I do programme Rust since 2013, I do train it since 2015,

00:00:38,789 --> 00:00:40,420
and since then I am a team member.

00:00:40,420 --> 00:00:43,059
I also formed a couple of community things.

00:00:43,059 --> 00:00:46,999
I'm part of the community team, I founded this conference, I founded another conference

00:00:46,999 --> 00:00:54,079
based on embedded subjects, so I have been active.

00:00:54,079 --> 00:01:04,220
Today I'm going to present the async-rs/async-std project and we decided

00:01:04,220 --> 00:01:07,170
to call it async-std because that's what it is.

00:01:07,170 --> 00:01:11,240
It is a port of the Rust standard library into the async world.

00:01:11,240 --> 00:01:19,700
It takes the whole interface of libstd or anything that might do thread blocking, then

00:01:19,700 --> 00:01:25,689
puts it on top of an executor and the rest of the interface is based on the futures-rs

00:01:25,689 --> 00:01:31,020
library, which is the abstract interface for working with futures.

00:01:31,020 --> 00:01:37,659
The futures trade that was stabilised within Rust is a very, very minimal one, to allow

00:01:37,659 --> 00:01:40,290
libraries on top of it, write extensions to it.

00:01:40,290 --> 00:01:45,329
It really has the bare minimum interface of this is what a future is and all the operations

00:01:45,329 --> 00:01:50,890
that you can do on top of them are provided by the futures-rs library.

00:01:50,890 --> 00:01:53,539
Async-std is not new.

00:01:53,539 --> 00:02:01,149
The people who kicked it off were first of all Stjepan Glavina who is also known as the

00:02:01,149 --> 00:02:09,280
maintainer of Crossbeam and used to work on Tokio, and Yoshua who works on tide and surf.

00:02:09,280 --> 00:02:16,220
It's a community project with a lot of the development nowadays being done by community

00:02:16,220 --> 00:02:17,700
contributors.

00:02:17,700 --> 00:02:19,530
So why are we building async-std?

00:02:19,530 --> 00:02:21,970
First of all, stability.

00:02:21,970 --> 00:02:28,300
We believe that the Rust async ecosystem has been in flux for far too long and we don't

00:02:28,300 --> 00:02:29,300
want to reiterate further.

00:02:29,300 --> 00:02:33,549
We want to collect the knowledge of the last few years and put it in a library in a stable

00:02:33,549 --> 00:02:35,640
fashion that you can rely on.

00:02:35,640 --> 00:02:41,930
Ergonomics: it should be easy and consistent to use and we figured out that the standard

00:02:41,930 --> 00:02:44,099
library interfaces aren't actually bad.

00:02:44,099 --> 00:02:48,989
There is a lot of work that has already been done around ergonomics so we have adopted

00:02:48,989 --> 00:02:53,970
them instead of coming up with our own again, again not doing research.

00:02:53,970 --> 00:03:00,510
It should be accessible, it comes with a book and full API docs so we have extensive API

00:03:00,510 --> 00:03:01,860
documentation.

00:03:01,860 --> 00:03:07,510
It fully integrates with the wider Rust ecosystem, mostly with futures-rs which it uses as its

00:03:07,510 --> 00:03:09,829
interface, and speed.

00:03:09,829 --> 00:03:16,931
Speed should come out of the box and we believe it's the best library to get started with

00:03:16,931 --> 00:03:17,931
async/await.

00:03:17,931 --> 00:03:20,980
I want to give a personal motivation to the first point.

00:03:20,980 --> 00:03:25,580
As a trainer I found it quite interesting that over the last two years I have seen more

00:03:25,580 --> 00:03:30,769
and more people not be interested in async Rust training, particularly to the point where

00:03:30,769 --> 00:03:33,440
last year people have said: can you just skip the async part?

00:03:33,440 --> 00:03:35,490
We are not going to use this for the next two years.

00:03:35,490 --> 00:03:36,719
This is too much in flux.

00:03:36,719 --> 00:03:41,200
So this is the state we are in, just to be clear.

00:03:41,200 --> 00:03:42,730
It has a couple of additional properties.

00:03:42,730 --> 00:03:44,659
It has a very small dependency tree.

00:03:44,659 --> 00:03:51,879
It is not overly generic which means it compiles fast and does not expose you to an end application,

00:03:51,879 --> 00:03:59,230
or does not contribute to an end application that compiles from it.

00:03:59,230 --> 00:04:01,099
Let's talk about async.

00:04:01,099 --> 00:04:05,500
So this is a synchronous function.

00:04:05,500 --> 00:04:14,830
It takes a path, returns an io result, takes a file and by opening and reading it through

00:04:14,830 --> 00:04:25,470
a buffer, we provide the capability of doing this, basically adding async-std, but you

00:04:25,470 --> 00:04:36,130
have to add the prelude and then you put async in front of the function and every operation

00:04:36,130 --> 00:04:43,110
here becomes an awaitable future operation, so file open will notify you once it's done

00:04:43,110 --> 00:04:47,200
and then you can continue and the same goes for reading.

00:04:47,200 --> 00:04:54,250
Yes, and yesterday someone, particularly Pascal, met me here and said: yes, we used it internally

00:04:54,250 --> 00:04:55,940
and it really works that way.

00:04:55,940 --> 00:05:03,160
We took our code, put async on top of every sync port and it worked.

00:05:03,160 --> 00:05:09,790
It exports all types necessary for async programming including some types of standard library type

00:05:09,790 --> 00:05:14,360
so if you are talking about io result here that's the same result as the standard library

00:05:14,360 --> 00:05:16,390
has.

00:05:16,390 --> 00:05:17,710
Why are we doing this?

00:05:17,710 --> 00:05:23,880
The standard library has the habit of at some points actually being subtly blocking, for

00:05:23,880 --> 00:05:31,810
example standard path: path has blocking function, particularly that exists for metadata and

00:05:31,810 --> 00:05:41,910
all of these functions, so if ever there's a type within async-std that is also in std,

00:05:41,910 --> 00:05:45,530
you should use the async.

00:05:45,530 --> 00:05:53,440
But talking more about what async functions represent, so this is a manual desugaring

00:05:53,440 --> 00:05:54,980
of what the async keyword does.

00:05:54,980 --> 00:05:58,410
It does a little more.

00:05:58,410 --> 00:06:02,150
For example, it activates the compiler mode in which again you are allowed to use await

00:06:02,150 --> 00:06:07,710
within a function block but most importantly it returns the return type of a function into

00:06:07,710 --> 00:06:15,210
impl future item and the result that we have stated there.

00:06:15,210 --> 00:06:19,280
That also gives an interesting insight in what those functions do.

00:06:19,280 --> 00:06:20,570
They generate futures.

00:06:20,570 --> 00:06:22,030
They don't actually do work.

00:06:22,030 --> 00:06:25,240
We will come back to what that means in a second.

00:06:25,240 --> 00:06:29,190
But what does the await mean?

00:06:29,190 --> 00:06:36,190
The await means that at all of these points there's an operation where this task has to

00:06:36,190 --> 00:06:46,340
wait for something to complete, so it will indicate actively: I register interest in

00:06:46,340 --> 00:06:54,330
a certain event, in the first case I want to open a file, please tell me when you are

00:06:54,330 --> 00:06:55,330
done.

00:06:55,330 --> 00:07:00,020
This is what await - the active indication that you are waiting for.

00:07:00,020 --> 00:07:04,510
It's also called completion base, so await is the moment where you wait for something.

00:07:04,510 --> 00:07:13,680
If you call this function, we get a nice compiler warning which is: futures do nothing unless

00:07:13,680 --> 00:07:15,600
you await or poll them.

00:07:15,600 --> 00:07:20,670
That's what I mean by async functions are generating futures.

00:07:20,670 --> 00:07:24,710
The return type of this is this is a future that will open this file.

00:07:24,710 --> 00:07:30,390
It kind of moves the thinking in the future, that's where the name comes from.

00:07:30,390 --> 00:07:40,060
You are asking for: please set up my program so that it can now open this file and tell

00:07:40,060 --> 00:07:44,040
me when you are done.

00:07:44,040 --> 00:07:45,790
And what do you need to run them?

00:07:45,790 --> 00:07:49,590
Futures run using a task, and that one is important.

00:07:49,590 --> 00:07:53,900
There's multiple ways to get to a task.

00:07:53,900 --> 00:08:00,500
Those mostly differentiate in blocking, non-blocking, blocking in the background - I come to that

00:08:00,500 --> 00:08:01,670
in a second.

00:08:01,670 --> 00:08:07,890
The important thing is multiple futures can run in the same task currently.

00:08:07,890 --> 00:08:11,650
Tasks themselves may run in parallel.

00:08:11,650 --> 00:08:13,600
This is depending on the execution strategy.

00:08:13,600 --> 00:08:19,370
I will come back to that in a couple of slides.

00:08:19,370 --> 00:08:26,020
As a differential for this talk, this is a really, really rough comparison but concurrent

00:08:26,020 --> 00:08:32,110
means multiple processes run in a group, yielding to each other when they need to wait, usually

00:08:32,110 --> 00:08:38,279
called corporate scheduling, so any of those operations will go until they need to wait

00:08:38,279 --> 00:08:42,839
for something again and then actively indicate: I'm waiting, someone else can run.

00:08:42,839 --> 00:08:48,560
While parallel means: multiple of these things run next to each other, usually on multiple

00:08:48,560 --> 00:08:49,650
threads.

00:08:49,650 --> 00:08:56,680
When we talk about blocking, in this case blocking, sadly, as many IT terms, is not

00:08:56,680 --> 00:09:02,480
sharply defined but for this presentation when I say something blocks, it is blocking

00:09:02,480 --> 00:09:07,710
the thread that it's currently on, which means, if you have additional concurrent tasks on

00:09:07,710 --> 00:09:13,220
this thread, they will be blocked as well, and that is a huge problem and this is something

00:09:13,220 --> 00:09:14,510
to avoid.

00:09:14,510 --> 00:09:21,189
This is why we choose how to run our futures task.

00:09:21,189 --> 00:09:26,490
This is the task block on interface in async-std.

00:09:26,490 --> 00:09:35,189
You will pretty often see that in examples for the reason - yes - for the reason that

00:09:35,189 --> 00:09:41,280
you need to block the main thread as the same goes for any kind of example that does threading,

00:09:41,280 --> 00:09:44,420
you need to block the main thread to make sure that your program doesn't exit while

00:09:44,420 --> 00:09:49,089
something runs in the background, so all examples pretty often use task block on because the

00:09:49,089 --> 00:09:54,829
definition of task block on is it blocks the thread you are currently on and instead runs

00:09:54,829 --> 00:10:00,110
the task that you are passing, so in this case for example reading the file.

00:10:00,110 --> 00:10:04,649
The nice thing about block on is it actually gives you the results back like if you had

00:10:04,649 --> 00:10:06,060
called a normal function.

00:10:06,060 --> 00:10:11,660
Because you can do that, you are completely blocking the thread until it's done and then

00:10:11,660 --> 00:10:17,769
you can collect the result and immediately put it into a variable.

00:10:17,769 --> 00:10:20,360
The second interface is spawn.

00:10:20,360 --> 00:10:24,710
Spawn runs a background task concurrently and waits for its completion.

00:10:24,710 --> 00:10:28,999
Sorry, no, it doesn't wait for its completion.

00:10:28,999 --> 00:10:32,560
It gives you a JoinHandle back and block on.

00:10:32,560 --> 00:10:37,080
You can then call block on, on the JoinHandle, blocking the main thread.

00:10:37,080 --> 00:10:41,810
The semantics here are: spawn spawns the future that does the operation and it gives you a

00:10:41,810 --> 00:10:46,240
JoinHandle back, exactly like the threading API in the normal standard library.

00:10:46,240 --> 00:10:51,269
It gives you a JoinHandle back that is in itself a future and the operation that this

00:10:51,269 --> 00:10:56,040
future represents is: wait for the background task and then take the result back.

00:10:56,040 --> 00:10:58,120
But these are two different kinds of operations.

00:10:58,120 --> 00:11:03,750
It's always important to understand what is the thing that I'm actually waiting for?

00:11:03,750 --> 00:11:07,060
In this case, first of all I'm waiting for the file to be read.

00:11:07,060 --> 00:11:12,040
Second, I'm waiting for the background task to complete.

00:11:12,040 --> 00:11:20,579
JoinHandles, much like the JoinHandles in the standard library, will give you the result

00:11:20,579 --> 00:11:24,720
back, the past result.

00:11:24,720 --> 00:11:26,730
JoinHandles are similar to standard thread JoinHandles.

00:11:26,730 --> 00:11:30,639
They are allocated together with the task that they spawn in one go.

00:11:30,639 --> 00:11:37,120
This is a single allocation and they also provide you a back channel to the spawner

00:11:37,120 --> 00:11:39,339
and the JoinHandle resource.

00:11:39,339 --> 00:11:45,810
There's a second function and I am mixing up my slides.

00:11:45,810 --> 00:11:48,579
This should be spawn blocking up there.

00:11:48,579 --> 00:11:52,860
It's a second function called spawn blocking.

00:11:52,860 --> 00:11:54,009
The body is correct.

00:11:54,009 --> 00:12:01,660
This is the old standard lib API so this is opening a file and the way the standard library

00:12:01,660 --> 00:12:07,610
works is it will block the current thread until the file is opened, until the kernel

00:12:07,610 --> 00:12:12,790
wakes up that thread again, but spawn blocking indicates through the runtime: by the way

00:12:12,790 --> 00:12:19,230
I am aware that there's blocking things in this task, which will make the runtime put

00:12:19,230 --> 00:12:26,880
the task on a single thread on its own thread, to avoid blocking other concurrent processes.

00:12:26,880 --> 00:12:28,709
It behaves exactly the same.

00:12:28,709 --> 00:12:33,839
It gives you a JoinHandle back, so what is possible, for example, is having blocking

00:12:33,839 --> 00:12:39,019
and non-blocking tasks resulting in the same type of result, mixing them, for example putting

00:12:39,019 --> 00:12:44,329
them all in a vector and waiting for all of them so our model unifies these things on

00:12:44,329 --> 00:12:48,259
the level of the spawn.

00:12:48,259 --> 00:12:54,810
There's two patterns on futures that I would like to introduce you, the first being racing,

00:12:54,810 --> 00:12:58,509
the second being joining.

00:12:58,509 --> 00:13:00,999
What does racing mean?

00:13:00,999 --> 00:13:05,579
Racing means I take two operations and await for the first to complete.

00:13:05,579 --> 00:13:15,059
As an example here, I use the surf HTTP client and I am trying to get the index of two mirrors,

00:13:15,059 --> 00:13:18,970
for example, and trying to work out which one answers fastest.

00:13:18,970 --> 00:13:23,589
What I can do then is say take the first future, race, with the second, await.

00:13:23,589 --> 00:13:29,300
We will get the first back and the second will be dropped and removed and potentially

00:13:29,300 --> 00:13:31,350
the request aborted.

00:13:31,350 --> 00:13:35,029
So this is racing.

00:13:35,029 --> 00:13:43,029
Sometimes in the futures crate there is also a macro that will have similar behaviour but

00:13:43,029 --> 00:13:47,950
not quite, but you may have seen that one.

00:13:47,950 --> 00:13:52,519
What's important here is, when we are talking about concurrency where there's parallelism,

00:13:52,519 --> 00:13:59,230
these two are concurrent because I'm taking two of these, putting them on a concurrent

00:13:59,230 --> 00:14:08,389
- merging them into the same future and then I'm run them both on the same task.

00:14:08,389 --> 00:14:15,319
What I can also do is I can spawn both of them, which may run them in parallel because

00:14:15,319 --> 00:14:20,980
now the runtime is free to do so, and then race for the completion of the task.

00:14:20,980 --> 00:14:28,550
So this means I will - the spawner will say: let's join the JoinHandles together and wait

00:14:28,550 --> 00:14:33,970
for one of them to return, so this is a similar kind of execution behaviour.

00:14:33,970 --> 00:14:39,610
A different kind of execution behaviour.

00:14:39,610 --> 00:14:42,040
The next that I would like to present is joining.

00:14:42,040 --> 00:14:46,079
Joining is important in the sense of: I have two operations and I want to execute them

00:14:46,079 --> 00:14:50,350
both and I don't want to continue until they are both complete.

00:14:50,350 --> 00:14:56,529
In this case, I want to make multiple requests to a backend and I want to wait until the

00:14:56,529 --> 00:14:58,879
outcome.

00:14:58,879 --> 00:15:02,209
The next concept I need to talk about is streams.

00:15:02,209 --> 00:15:09,149
Streams are a fundamental abstraction around items arriving concurrently into any kind

00:15:09,149 --> 00:15:11,550
of part of my programme.

00:15:11,550 --> 00:15:18,059
In async-std they take the place of the iterator trait so basically standard iterator is replaced

00:15:18,059 --> 00:15:25,949
by async-std stream so they can be split, merged, iterated over, all these kind of things.

00:15:25,949 --> 00:15:32,550
The classic example that you will probably have seen if you have ever had a look at async

00:15:32,550 --> 00:15:43,329
code is the TCPListener so new connections are arriving to my program.

00:15:43,329 --> 00:15:48,790
I don't know when, I don't know how many and I don't know how many at once and streams

00:15:48,790 --> 00:15:56,089
give you the ability to work on this one-by-one, so the TCPListener on the standard lib has

00:15:56,089 --> 00:16:05,800
a sync liberator, we have a sync incoming stream and in now Rust 1.39.0 the recommended

00:16:05,800 --> 00:16:12,399
way of iterating over that is by calling incoming next, stream.next.

00:16:12,399 --> 00:16:16,279
This is the future because we don't know approximate there's something there yet or if it might

00:16:16,279 --> 00:16:22,339
come in the future so this will lay the task sleeping until something is coming in and

00:16:22,339 --> 00:16:26,519
we are awaiting on it.

00:16:26,519 --> 00:16:35,029
There's no integration into four loops currently in the language.

00:16:35,029 --> 00:16:42,480
Then we spawn for each of those connections, we spawn a concurrent task that works on them.

00:16:42,480 --> 00:16:48,990
Process is some function that processes this.

00:16:48,990 --> 00:16:52,339
One important operation on streams is merging them.

00:16:52,339 --> 00:16:57,040
There's a couple of other operations, but I could give a whole talk on streams.

00:16:57,040 --> 00:17:07,000
Here is an example that listens both on an ipv4 and an ipv6 interface so I can use two

00:17:07,000 --> 00:17:14,960
TCPListeners, bind them, get the incoming stream for both and then I say ipv4 merge

00:17:14,960 --> 00:17:20,560
to ipv6 and then just continue because they are returning exactly the same things.

00:17:20,560 --> 00:17:24,880
They are giving me a TCP socket with a TCP connection so I can merge them and start iterating

00:17:24,880 --> 00:17:27,970
over them.

00:17:27,970 --> 00:17:32,320
We have a sync module exactly like standard sync.

00:17:32,320 --> 00:17:36,710
It comes with async/await ready versions of standard lib structures.

00:17:36,710 --> 00:17:43,910
It is important to understand that a lot of standard lib structures, if they put you waiting,

00:17:43,910 --> 00:17:48,440
like for example you are trying to lock a Mutex, they will lock the current thread.

00:17:48,440 --> 00:17:54,930
What it gives you is they will lay the current task waiting.

00:17:54,930 --> 00:18:01,470
So it comes with Mutex, Barrier, read write locks, all that.

00:18:01,470 --> 00:18:08,720
This is what the code looks like, almost exactly like the standard Mutex example that you already

00:18:08,720 --> 00:18:09,940
have.

00:18:09,940 --> 00:18:21,910
The example here ...

00:18:21,910 --> 00:18:26,030
wake me up when it's my go.

00:18:26,030 --> 00:18:32,230
So it is important to use future aware data structures in future code.

00:18:32,230 --> 00:18:38,450
One interesting thing is we have the Arc here, this is just an export from the standard library

00:18:38,450 --> 00:18:46,810
to make sure that you don't have a mix of std and async-std types.

00:18:46,810 --> 00:18:53,210
Almost all types that async-std exports are either exported because they have a relationship

00:18:53,210 --> 00:18:58,540
to another one, or because we need to make changes.

00:18:58,540 --> 00:19:05,650
Arc is the small exception and we don't want people to be too confused by having these

00:19:05,650 --> 00:19:13,880
small exceptions of maybe 1%, 2% of the interface.

00:19:13,880 --> 00:19:18,880
We have a channel implementation, async-std channels are based on crossbeam channels but

00:19:18,880 --> 00:19:20,130
in an async version.

00:19:20,130 --> 00:19:22,510
They are multiple producer, multiple consumer channels.

00:19:22,510 --> 00:19:24,630
They are always bounded.

00:19:24,630 --> 00:19:27,240
We think that unbounded channels are an anti-pattern.

00:19:27,240 --> 00:19:32,430
You will run into problems with bounded anyways.

00:19:32,430 --> 00:19:40,330
What that means is they can - bounder channels can only hold a certain number of items at

00:19:40,330 --> 00:19:48,470
once otherwise trying to write into them will lay your task sleeping, and they are very

00:19:48,470 --> 00:19:49,470
fast.

00:19:49,470 --> 00:19:51,000
They are faster than crossbeam channels.

00:19:51,000 --> 00:19:56,240
These are the ones used in Servo in the browser.

00:19:56,240 --> 00:19:59,560
Because they are multiple producer multiple consumer channels, they should cover all your

00:19:59,560 --> 00:20:05,640
generic use cases, so that also means they cover one-to-one, they cover one-to-multiple,

00:20:05,640 --> 00:20:12,760
they cover multiple-to-one, just by virtue of covering end to end.

00:20:12,760 --> 00:20:13,792
Channels are currently unstable for API discussions.

00:20:13,792 --> 00:20:15,490
A little.

00:20:15,490 --> 00:20:19,730
We want to fix that in the next a couple of weeks so you need to activate the unstable

00:20:19,730 --> 00:20:22,960
feature flag if you want to play around with them.

00:20:22,960 --> 00:20:28,320
As a short example, they work much like the stuff that I introduced.

00:20:28,320 --> 00:20:33,540
Specifically, the receiving end of a channel is a stream, so it can generically be seen

00:20:33,540 --> 00:20:37,880
as a stream, so the while-at-some pattern applies.

00:20:37,880 --> 00:20:45,880
If you send, sending becomes a future operation because if the channel is full you will have

00:20:45,880 --> 00:20:51,680
to wait until the channel has capacity.

00:20:51,680 --> 00:20:56,510
This is a ping pong example of two tasks or two futures that will ping pong each other

00:20:56,510 --> 00:21:00,960
through a channel, or through two channels, and you spawn both of them and they will stop

00:21:00,960 --> 00:21:07,590
playing ping pong until kingdom come.

00:21:07,590 --> 00:21:12,130
Understanding tasks and streams in my opinion is far more important than understanding futures

00:21:12,130 --> 00:21:17,780
because streams, channels and tasks are the things - tasks are the things that communicate.

00:21:17,780 --> 00:21:23,180
Streams and channels, streams being the abstract version of channels, not everything - almost

00:21:23,180 --> 00:21:25,130
all channels are streams.

00:21:25,130 --> 00:21:27,520
Not every stream is a channel.

00:21:27,520 --> 00:21:29,890
Those are the things, how tasks communicate.

00:21:29,890 --> 00:21:33,940
This is, if you have a problem, how do you get some piece of data from A to B between

00:21:33,940 --> 00:21:34,940
two concurrent tasks?

00:21:34,940 --> 00:21:39,490
The answer is almost always: use a channel.

00:21:39,490 --> 00:21:43,380
So even if you are talking about I want to work on a futures-based system, it is important

00:21:43,380 --> 00:21:46,480
that you have that in mind.

00:21:46,480 --> 00:21:52,000
So as a summary, async-std provides a known and familiar interface of the Rust standard

00:21:52,000 --> 00:21:54,500
library with appropriate changes for async.

00:21:54,500 --> 00:22:04,500
It avoids pitfalls by providing a full API surface around all async-critical modules.

00:22:04,500 --> 00:22:09,440
It is fully based on futures-rs so it integrates through the ecosystem very well.

00:22:09,440 --> 00:22:15,100
We fully embrace the futures-rs library so if you want to use async-std in an abstract

00:22:15,100 --> 00:22:22,250
fashion without relying on async-std types, you can take a step up and rely on the futures-rs

00:22:22,250 --> 00:22:24,690
interface traits.

00:22:24,690 --> 00:22:31,620
Because all types expose the relevant interfaces from futures-rs, so futures, obviously futures,

00:22:31,620 --> 00:22:39,110
but async-std streams are always also futures streams and everything that is read write,

00:22:39,110 --> 00:22:45,880
async read write, is also futures async read write.

00:22:45,880 --> 00:22:53,720
If you want to have the full interface of futures-rs then you can also on top of that

00:22:53,720 --> 00:22:56,480
load a futures-rs library.

00:22:56,480 --> 00:23:02,110
One note about the futures-rs library, it has recently been rearchitected to a core

00:23:02,110 --> 00:23:07,240
and an util and a couple of additions and from core to util to some of the remote additions

00:23:07,240 --> 00:23:13,460
it becomes more experimental, so when I say we export the futures-rs library, specifically

00:23:13,460 --> 00:23:19,650
we rely on the core types where we know that they are going to be stable and probably not

00:23:19,650 --> 00:23:23,490
subject to churn.

00:23:23,490 --> 00:23:28,370
But if you want to opt into any of the more experimental types, you can totally do so

00:23:28,370 --> 00:23:35,270
by just loading the futures-rs library on top and using it.

00:23:35,270 --> 00:23:38,970
Particularly of interest are the async read, async write, async seek traits.

00:23:38,970 --> 00:23:43,600
A lot of the criticism that we got is you are not compatible with the ecosystem, particularly

00:23:43,600 --> 00:23:44,600
like Tokio.

00:23:44,600 --> 00:23:49,290
The problem there is a lot of libraries are written on top of Tokio which don't implement

00:23:49,290 --> 00:23:58,100
the futures async read, write, seek traits unless you use - they use Tokio, which are

00:23:58,100 --> 00:24:05,920
different, and also subject to change, making them incompatible with the rest of the ecosystem.

00:24:05,920 --> 00:24:09,470
I think it's okay.

00:24:09,470 --> 00:24:16,830
Their view on this is we still want to experiment on this and we would like to iterate on that

00:24:16,830 --> 00:24:22,790
interface, but that very much collides with our view that we should get stuff stabilised

00:24:22,790 --> 00:24:28,260
and that we have researched those traits for three years, so addressing the common question

00:24:28,260 --> 00:24:34,280
why async stood, it's pretty simple: fundamental mission goal.

00:24:34,280 --> 00:24:37,760
That's as deep as it is.

00:24:37,760 --> 00:24:47,330
Our view and vision for using std is applications should use async-std directly because application

00:24:47,330 --> 00:24:50,630
development is far less abstract.

00:24:50,630 --> 00:25:00,390
You want to pick a couple of libraries solving your needs, use them and then the abstraction

00:25:00,390 --> 00:25:03,030
level is usually pretty low.

00:25:03,030 --> 00:25:06,630
You want to get stuff done.

00:25:06,630 --> 00:25:10,960
Libraries should use futures-rs as their interface.

00:25:10,960 --> 00:25:18,660
As an example, we have a TLS library under our own organisation, and this one operates

00:25:18,660 --> 00:25:30,390
completely on top of async read and write, but you can use it on all async-std TCP socket

00:25:30,390 --> 00:25:36,381
and so on types that expose read and write, so to make this more concrete, if you are

00:25:36,381 --> 00:25:40,990
in an application it is okay to take an async-std net TCP socket directly.

00:25:40,990 --> 00:25:47,900
If you are in a library I would highly recommend you to instead use futures io async read so

00:25:47,900 --> 00:25:57,710
that doesn't expose users to your choice of base runtime.

00:25:57,710 --> 00:26:02,330
Because there are other runtimes around, and this is something - there's currently a binary

00:26:02,330 --> 00:26:06,140
discussion happening around async-std versus Tokio.

00:26:06,140 --> 00:26:11,590
I don't want to have this discussion because there's a number of executables in our ecosystem.

00:26:11,590 --> 00:26:15,840
Particularly Google Fuchsia is a big one.

00:26:15,840 --> 00:26:27,000
There's bass I do not know.rs which is: this thing never crashes, under no conditions.

00:26:27,000 --> 00:26:35,940
There's wasm-bindgen-futures which is a particularly built executor for WASM, and there's a number

00:26:35,940 --> 00:26:41,790
of company internal ones, that those companies sometimes speak about but pretty rarely speak

00:26:41,790 --> 00:26:43,940
about in public.

00:26:43,940 --> 00:26:50,730
And async-std is meant for writing compatible libraries, so if you are writing a library

00:26:50,730 --> 00:26:54,440
you can use async-std as your test bed.

00:26:54,440 --> 00:27:01,360
Probably you should avoid binding to it directly, which brings me to speed.

00:27:01,360 --> 00:27:04,790
It's something we have been asked about a lot.

00:27:04,790 --> 00:27:05,790
There's a problem there.

00:27:05,790 --> 00:27:10,110
We also believe there's a hyperfocus on benchmarks in the Rust community at the cost of ergonomics

00:27:10,110 --> 00:27:11,610
and barring stabilisation.

00:27:11,610 --> 00:27:17,260
The biggest problem with benchmarks is they are often changing and we don't really want

00:27:17,260 --> 00:27:23,650
to be a part of the benchmark race so probably I will present these benchmarks and in two

00:27:23,650 --> 00:27:30,920
weeks they will look very, very different and I don't believe you should choose software

00:27:30,920 --> 00:27:35,910
by benchmarks alone which also doesn't mean you shouldn't factor it in.

00:27:35,910 --> 00:27:44,440
On reading async-std is 1.56 faster than Tokio.

00:27:44,440 --> 00:27:49,030
You can look at that repository.

00:27:49,030 --> 00:27:54,020
That's only on reading a 256K file.

00:27:54,020 --> 00:28:00,050
If your application has a different read write behaviour your benchmarks might be different.

00:28:00,050 --> 00:28:02,180
There's a fair warning.

00:28:02,180 --> 00:28:11,310
Our Mutex is fast, up to nine times faster.

00:28:11,310 --> 00:28:18,990
There is futures intrusive.

00:28:18,990 --> 00:28:26,520
We are nine times as fast as the futures implementation at that.

00:28:26,520 --> 00:28:32,220
This also gives an interesting insight of should I use futures lock Mutex?

00:28:32,220 --> 00:28:38,190
These types are mostly implemented for making sure people can use them and run with them

00:28:38,190 --> 00:28:44,289
so these were for the implementation phase of futures and they are still shipped, so

00:28:44,289 --> 00:28:46,690
that people that rely on them can still use them.

00:28:46,690 --> 00:28:48,380
They are good.

00:28:48,380 --> 00:28:54,710
They are just not built towards they should be fast.

00:28:54,710 --> 00:29:02,390
So this is Mutexs on the contention so actually Mutexs, there's frequent locking.

00:29:02,390 --> 00:29:08,350
There we are up to two times faster than the fastest one and we are the fastest one under

00:29:08,350 --> 00:29:09,350
contention.

00:29:09,350 --> 00:29:15,460
Mutexs without contention, and that is interesting, also have a speed task so Mutexs that are

00:29:15,460 --> 00:29:23,320
rarely actually a challenge, and there we are around roughly the same speed.

00:29:23,320 --> 00:29:26,150
Why do I say the same speed?

00:29:26,150 --> 00:29:33,230
When we run those benchmarks, they are highly variant in the sense the results will remain

00:29:33,230 --> 00:29:34,580
clear.

00:29:34,580 --> 00:29:42,560
The implementations don't change spot but on the other hand some of those numbers change

00:29:42,560 --> 00:29:43,960
up and down.

00:29:43,960 --> 00:29:47,550
Finally, how fast is task spawning?

00:29:47,550 --> 00:29:49,470
These are the Tokio ten times benchmarks.

00:29:49,470 --> 00:29:52,330
We are as fast.

00:29:52,330 --> 00:30:02,030
There is another benchmark where we are slower, the channel ring benchmark that tests single

00:30:02,030 --> 00:30:08,110
threaded executors where you put up a number of nodes and connect them in a ring and then

00:30:08,110 --> 00:30:14,280
send a message around as fast as possible multiple times, and there we are 0.9 times

00:30:14,280 --> 00:30:19,870
slower than Tokio and roughly three times faster than actix.

00:30:19,870 --> 00:30:35,420
Yes, notice, for risks and side effects of synthetic benchmark please consult your active

00:30:35,420 --> 00:30:37,550
keynoter.

00:30:37,550 --> 00:30:38,980
It is an innovative space.

00:30:38,980 --> 00:30:41,460
I want to mention that again.

00:30:41,460 --> 00:30:44,690
JoinHandles were built by async-std and already adopted by others.

00:30:44,690 --> 00:30:50,620
Single allocation tasks were invented in async-std and adopted by others.

00:30:50,620 --> 00:30:54,290
You can both innovate and commit to stability.

00:30:54,290 --> 00:30:58,560
Be aware what you - which thread should you use.

00:30:58,560 --> 00:31:04,160
Our roadmap 1.0 will be released on Monday, so tomorrow, which will be a stable release

00:31:04,160 --> 00:31:06,840
with all base functionality and runtime concerns.

00:31:06,840 --> 00:31:12,260
Ongoing, stabilisation of currently unstable library API.

00:31:12,260 --> 00:31:14,900
Most importantly channels.

00:31:14,900 --> 00:31:20,360
Also designing features that make async-std usable without the runtime.

00:31:20,360 --> 00:31:21,850
I think this is particularly interesting.

00:31:21,850 --> 00:31:25,880
What if I want all those features but actually don't want the runtime?

00:31:25,880 --> 00:31:31,020
We want to provide additional libraries with similar guarantees, access compatibility,

00:31:31,020 --> 00:31:32,020
one onus.

00:31:32,020 --> 00:31:42,530
And just to be clear, it is clear that we will have 2.0 at some point because there's

00:31:42,530 --> 00:31:49,310
new language features arriving and some of those might lead to breaking interface changes,

00:31:49,310 --> 00:31:55,010
not only in base grades but for example there is an asynchronous four loop coming and that

00:31:55,010 --> 00:31:59,450
might need some changes to integrate well with that and when these changes are coming

00:31:59,450 --> 00:32:06,360
down the line we will release 2.0 plus update guides on how to do that.

00:32:06,360 --> 00:32:11,710
We would like you to hack with us in the next few days, so getting started writing more

00:32:11,710 --> 00:32:16,590
libraries on top or porting libraries to work with multiple executors.

00:32:16,590 --> 00:32:20,830
We want you to challenge our benchmarks because all benchmarks are wrong, and let's figure

00:32:20,830 --> 00:32:23,280
out how.

00:32:23,280 --> 00:32:27,130
If you want, we can get started writing an application and we would like to get a lot

00:32:27,130 --> 00:32:32,460
of feedback on our unstable API because we want to stabilise, and that means we need

00:32:32,460 --> 00:32:34,130
a couple of ibots.

00:32:34,130 --> 00:32:42,040
Finally, async-std is currently completely funded by Ferrous Systems and we have started

00:32:42,040 --> 00:32:43,860
offsetting for all those projects.

00:32:43,860 --> 00:32:49,150
We are also hosting Rust analyser and open collective so if you agree with all those

00:32:49,150 --> 00:33:00,730
goals and would like to give us some more time to work on it, thank you.

00:33:00,730 --> 00:33:11,650
[Applause] >> Thank you, Florian.

00:33:11,650 --> 00:33:14,140
We have time for two questions.

00:33:14,140 --> 00:33:16,140
Hands, please?

00:33:16,140 --> 00:33:23,140
>> Hello, thank you for the great talk.

00:33:23,140 --> 00:33:33,470
Do you think there are ever any use cases in which Tokio should be preferred to async

00:33:33,470 --> 00:33:34,470
standard?

00:33:34,470 --> 00:33:35,470
FLORIAN: Sure.

00:33:35,470 --> 00:33:42,470
No, seriously, there's at least six executors that I know of around, all of them with different

00:33:42,470 --> 00:33:50,970
tuning settings, different kinds of APIs, so yes, there's reasons for all of them.

00:33:50,970 --> 00:33:56,220
Tokio currently has a better API for fine grain control of placing of a runtime which

00:33:56,220 --> 00:34:04,830
is a very, very detailed thing to do and needs a lot of knowledge on how to actually place

00:34:04,830 --> 00:34:10,210
runtimes correctly and this is something that we currently don't provide at all.

00:34:10,210 --> 00:34:16,179
So, yes, definitely.

00:34:16,179 --> 00:34:20,759
>> You mentioned that racing and selecting aren't quite the same thing.

00:34:20,759 --> 00:34:23,000
Could you elaborate on that?

00:34:23,000 --> 00:34:30,079
FLORIAN: [Laughing] So the select macro of the futures crate is a pretty, pretty big

00:34:30,079 --> 00:34:38,119
beast that works on top of a library called proc-macro-hack which implements by itself

00:34:38,119 --> 00:34:44,999
a way of doing - basically implements its own match statement along with two different

00:34:44,999 --> 00:34:56,649
kinds of operations, how to react to an event happening on each of the branches.

00:34:56,649 --> 00:35:08,220
So yes, what I was hinting at is this select in Rust isn't a very well-defined term, while

00:35:08,220 --> 00:35:13,950
racing is pretty clearly: there's two futures giving you the same results and that's also

00:35:13,950 --> 00:35:14,950
the other thing.

00:35:14,950 --> 00:35:19,190
The select macro in the futures crate allows you to use two futures that return different

00:35:19,190 --> 00:35:22,490
kinds of results and react differently on whether the one resolves or the other one

00:35:22,490 --> 00:35:26,920
resolves where racing is: I expect the same back and I wait for the first.

00:35:26,920 --> 00:35:30,160
So it's a vastly simpler operation.

00:35:30,160 --> 00:35:32,210
>> Thank you, Florian.

00:35:32,210 --> 00:35:32,900

YouTube URL: https://www.youtube.com/watch?v=L7X0vpAU-sU


