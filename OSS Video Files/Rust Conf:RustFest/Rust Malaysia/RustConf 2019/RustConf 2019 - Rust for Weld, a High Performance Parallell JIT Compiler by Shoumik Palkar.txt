Title: RustConf 2019 - Rust for Weld, a High Performance Parallell JIT Compiler by Shoumik Palkar
Publication date: 2019-09-19
Playlist: RustConf 2019
Description: 
	RustConf 2019 - Rust for Weld, a High Performance Parallell JIT Compiler by Shoumik Palkar

Weld is an open source Rust project that accelerates data-intensive libraries and frameworks by as much as 100x. It does so by JIT-compiling a custom parallel intermediate representation and optimizing across functions within a single library as well as across different libraries, so developers can write modular code and still get close to bare metal performance without incurring expensive data movement costs. We first describe our use of Rust to build an extensible, performance sensitive compiler for Weld. In particular, we discuss challenges in writing a native code generator with LLVM in Rust, challenges in reducing JIT compilation times within our compiler, and architecting the compiler for both extensibility using Rust's powerful trait system, but also portability of our library across other languages, such as Python and C. Finally, we discuss our experiences with building a parallel runtime in Rust, and the unique challenges associated with calling unsafe Rust from JIT'd code.
Captions: 
	                              [Music]                               all right thanks yeah so my name is Cho                               Makai man anthea graduate student now at                               Stanford and I'm gonna talk about using                               rust for weld which is a                               high-performance parallel JIT compiler                               that we've been developing over the last                               few years so just to provide an agenda                               for the talk I'll first talk a little                                bit about what weld is and what its                                motivations are followed by the path                                that we kind of took that you know                                landed us at rust and then finally I'll                                talk about some of the features of rust                                that we use in weld and also where we                                hope to take the project from here so                                jumping right in the motivation for the                                weld project is that modern data                                analytics applications combine many                                disjoint processing libraries and                                functions so taking the Panda or say the                                Python ecosystem as an example a typical                                Python data science application may you                                know load some data and pandas and do                                some pre-processing maybe train a                                machine learning model and scikit-learn                                and then maybe do some statistical                                analysis in numpy so this is good                                because on one hand you get great                                results kind of leveraging the work of                                thousands of authors who are                                implementing the best algorithms in each                                domain but on the other hand in these                                kinds of libraries you generally don't                                get any optimization across individually                                written and optimized functions so how                                bad is this problem that we don't get                                any cross function optimization it turns                                out that the growing gap between memory                                speeds on newer hardware and processing                                speed on new hardware it's kind of being                                this rigid function call interface to                                these kinds of libraries worse and worse                                over time so as an example of there so                                here's a pretty simple Python program                                and so it's using pandas to parse the                                CSV and then drop some null values and                                then it's using numpy to compute a mean                                and realize for these like Python and                                kind of data science tile libraries                                there's nothing like straight iterator                                right the interface to these libraries                                is you have some functions and you throw                                some data at them and they you know                                crunch some numbers and give you back a                                result and because of this when you                                actually look at the execution of these                                like typical data science type workflows                                it looks something like this                                you have some memory sitting around and                                you call a function and basically what                                that entails is that you give it some                                data it processes something and then it                                gives you back a result when you call                                another function you basically do                                another entire scan over your data set                                you know go back to main memory you load                                those values back into the CPU cache                                into the registers compute some more                                values and then                                again you know a store the values back                                and if you have these large data sets                                what ends up happening because of this                                gap between memory access speeds and                                processing speeds is that you actually                                spend most of your time just accessing                                memory instead of actually doing                                meaningful work on the CPU and for these                                kinds of workloads we found up to                                    slowdowns in popular libraries such as                                numpy panda as tensorflow all these kind                                of data analytics libraries that you                                hear about compared to an optimized just                                c or washed implementation so if I were                                to take the same application and write                                it and see there would be this huge kind                                of gap between what people actually use                                to this kind of stuff today and what the                                hardware is actually capable of so well                                this is a common runtime for data                                analytics libraries that kind of                                attempts to tackle this problem and the                                approach that well takes is something                                like this there's these libraries for                                different domains just sequel or machine                                learning and they effectively target a                                common parallel runtime and instead of                                executing each function individually                                this runtime will act like a you know                                 like a standard compiler it'll look at                                 all your code just at runtime instead of                                 at compile time and it'll basically run                                 an optimizer over it and JIT code to                                 execute during the execution of your                                 application and you can get code for                                 these different kind of parallel                                 platforms such as CPUs or GPUs or                                 something else so internally well it has                                 a bunch of components the first one is                                 this runtime api that developers of                                 libraries use to actually submit work                                 into wealth the second component is this                                 world IRS this is basically like a                                 parallel kind of functional small                                 programming language that developers of                                 libraries use to submit work into the                                 runtime and I'll talk a bit more about                                 that in a in a bit and the final                                 component is this optimizer which is                                 basically like a standard compiler                                 optimizer which takes the IR and kind of                                 just transforms it into a basically more                                 efficient version of the same IR and                                 then that's sent over to the back ends                                 and the back ends use something like                                 LLVM to you know do code generation so                                 if we look at kind of the thousand foot                                 view of how a program using world                                 executes it starts with some user                                 application that's unmodified so the                                 idea here is that for users Weld is kind                                 of invisible but for library developers                                 you're you know replacing the                                 implementation of your library functions                                 to integrate with well so let's say we                                 have a function with some Weld enabled                                 when a user calls a function instead of                                 evaluating those functions internally                                 those function calls will kind of submit                                 these fragments of IR code into the                                 world runtime when you actually want to                                 evaluate this so this might be when you                                 say I want to print something to the                                 screen or send it over the network or                                 whatever the the runtime will basically                                 figure out that there's these data                                 dependencies between these lazy                                 computations and it'll combine them into                                 a single program and then it'll fire up                                 the optimizer optimize that program and                                 then JIT some code and that's what the                                 data in your application actually                                 interacts with and then there's also                                 this kind of managed parallel runtime                                 that hooks into this thing because we're                                 actually generating code that's meant to                                 be run in parallel so there needs to be                                 a runtime to do things like scheduling                                 and task management and memory                                 management and all that good stuff so                                 when you started with Weld it was kind                                 of meant to target these cross library                                 optimization based workloads but over                                 time we've kind of been we've kind of                                 seen that it's been useful for other                                 things as well                                 so as a few examples we've seen people                                 use well to build jits or new physical                                 execution engines for databases or                                 analytics frameworks we've seen it being                                 used just within individual libraries so                                 if you just want better kind of native                                 code execution and a Python library                                 using a JIT compiler is a great way to                                 do that so we've seen it being used for                                 that and we've also been seen it being                                 used to target new kinds of parallel                                 hardware and the reason that weld is                                 especially nice for that is because the                                 ir is just structured in a way that it                                 can assume that a bunch of things are                                 inherently parallel so if you have                                 hardware such as GPUs or vector                                 accelerators the IR is kind of an                                 attractive way to kind of bridge                                 high-level code with the kind of machine                                 specific code so just quickly to provide                                 some context and the kinds of speed ups                                 where else could provide these are                                 obviously cherry-picked benchmarks to                                 make us look better to take them with a                                 grain of salt but in this like Tagle                                 type competition where we basically do                                 some data cleaning and linear algebra                                 and numpy and pandas we saw                                             up here so this is a because we                                 parallelized the single threaded                                 libraries numpy and pandas but it's also                                 because we can do optimizations such as                                 loop fusion that you know like iterators                                 and rush can do for example but it's                                 doing them over these kind of native pi                                 libraries and I want to point out that                                 pan doesn't numpy are written and see                                 internally so this isn't like comparing                                 against like some weird Python loop or                                 something as another example we were                                 able to optimize this image whitening                                 plus linear regression workflow in                                 tensorflow and numpy and this is                                 comparing against tensor flows own kind                                 of excel a compiler so cancer for all                                 journey has this compiler that it uses                                 to do things like operator fusion but                                 because we can actually look across                                 library boundaries we get we still get a                                 pretty nice speed up here and finally we                                 can also use well to kind of just                                 optimize the execution of things like                                 data analytics frameworks so here we                                 integrated it with spark sequel and we                                 can basically get some native code                                 execution and spark so instead of                                 relying on the JVM we're executing you                                 know like vectorized native code so we                                 get some nice speed ups there okay so                                 just to provide some more context for                                 the rest of the talk and to show how the                                 optimizer kind of functions and I'm                                 gonna do a quick demo where we compile a                                 weld program using this rebel that we've                                 built so I'll switch over to that now                                 all right hopefully that's readable so I                                 have to look at this alright so here's                                 the repple so basically you can just                                 type in weld programs here so here's a                                 simple one that just adds one two input                                 and you can see that it basically just                                 dumps out some information on things                                 that it's doing so here it did some                                 macro substitution followed by type                                 inference so this isn't gonna look super                                 interesting because this program is                                 pretty simple you can hit enter and kind                                 of walk through the different passes                                 that the compiler applies here it's not                                 really going to do much because it's                                 kind of hard to optimize adding one to a                                 number but you can also get some stats                                 about how the compiler does there so if                                 you look at the right side here now                                 there's a slightly more complicated                                 program where we're computing the sum of                                 squares so that first line with the map                                 is basically squaring each integer that                                 we pass in and the second line is                                 summing the squared integers and you                                 might be wondering why we've actually                                 written this program in this way right                                 why didn't we just write a single loop                                 that does this right off the bat with a                                 so remember that weld is meant to be in                                 degree                                 to these kind of higher-level libraries                                 so this is kind of how you can expect a                                 typical world program to look so for                                 example a numpy program that call or                                 sorry an umpire function that squares                                 things might generate the first line of                                 this program and a numpy function that                                 you know computes as some might generate                                 the second line and it's up to well to                                 kind of look at these individual                                 fragments and come up with something                                 more reasonable so we'll go ahead and                                 load this into the repple so I'll just                                 hit load demo dot weld here and right                                 off the bat you can see that this                                 program looks fairly different from what                                 we have on the right side so here we've                                 done macro substitution and type                                 inference and the reason this program                                 looks so different is because map and                                 summon well they're actually macros that                                 expand into these parallel for loop                                 operators on so you can see there's that                                 for operator there and that just means                                 that we're iterating over the input that                                 we give it in parallel so that first for                                 loop is iterating over a in parallel and                                 it's using this thing called an appender                                 to basically append items to a list so                                 that's our kind of map that constructs                                 the squares and the second floor loop is                                 actually doing the sum so that merger                                 with the plus in it that's just a way to                                 represent a parallel aggregation in well                                 so these are just details about the ir                                 that i'm not gonna spend too much time                                 on but that's that's basically how the                                 program looks like and it's kind of                                 unfurnished for so well now step through                                 the optimizer and look at what it does                                 so this in line let pass as an example                                 got rid of the assignments and kind of                                 inline those loops but we still have                                 those two passes over the data stepping                                 through it a couple more times you can                                 see this loop fusion pass which is                                 probably the most interesting one in                                 this example actually got rid of one of                                 the loops and it inlined the squaring of                                 the input into that aggregation so now                                 we're just doing one pass over the data                                 so we'll step through it a little bit                                 more and you can see that it also                                 vectorized this thing by basically                                 introducing these sim da tur things that                                 allow the backend to generate sim decode                                 and finally we get some statistics about                                 how the compiler did okay so that's kind                                 of a thousand foot view of what well                                 there's and outworks alright so                                 switching a little bit to the                                 implementation side of things                                 the first kind of Weld compiler                                 implementation was in scala so I'll                                 briefly talk about what was good about                                 Scala                                 and then I'll talk about what was bad                                 and why we ended up with rust so the                                 good things about Scala the main reason                                 that we chose Scala to begin with was                                 because it had these algebraic types and                                 this really nice pattern matching                                 mechanism so all those rules that I                                 showed you in the demo those are                                 basically just you know pattern matching                                 rules where you look at some subtree of                                 a program and you substitute it with                                 another tree and that out of the tree                                 just happens to be more efficient but it                                 computes the kind of same result that                                 the first tree computes as well other                                 kind of auxilary reasons it already had                                 like a large ecosystem so with Scala you                                 can use all the things that Java has and                                 it also also had things like collections                                 and everything so that just makes you                                 know building stuff easier and also my                                 adviser liked it so that obviously                                 impacted things as well so going back to                                 this pattern matching example here's an                                 example of converting a boolean                                 expression into a short-circuited                                 version of that boolean expression so as                                 an example this end expression it                                 computes the computes left and then                                 computes right and then it takes the N                                 and then returns either true and false                                 so if we want to convert this to a                                 short-circuited expression we can                                 convert it to an if statement where it                                 evaluates left if left is true we                                 evaluate right and if left is false you                                 just return false okay and you can see                                 that writing these kinds of pattern                                 matching rules for an optimizer or you                                 know these kind of correctness rules as                                 well it's super natural both to read and                                 to write and this was kind of the main                                 reason that we that we chose Scala to                                 begin with and when I talk about the                                 rest part you'll see that it also                                 influenced the design of how we decide                                 to structure the compiler and rust as                                 well okay this is a talk about rust and                                 so naturally there are some bad things                                 that we ran into that led us to move on                                 and kind of the main bad thing about                                 Scala was that it was very hard to embed                                 into other things and you know hindsight                                 is                                                                     but it's not great if you have some                                 Python program running and you want to                                 compile something and you have to fire                                 up like a you know                                                  process to do that so that was probably                                 the biggest reason why we decided to                                 move away from this kind of management                                 world auxilary reasons JIT compilation                                 times we also found were pretty slow for                                 larger programs for smaller programs it                                 didn't really matter and this was again                                 just due to the fact that we were                                 actually interpreting code and usually                                 we weren't running things enough for the                                 JVM to you know compile things to native                                 code we had to manage runtime this feeds                                 back into the first reason but it also                                 makes things like writing FF I is very                                 difficult it had a pretty clunky build                                 system I don't know if anyone's ever                                 tried to use SBT here but I'm not gonna                                 say more about it finally the runtime                                 also had to be in a different language                                 right so we're ginning code and this                                 jittered code is just some arbitrary                                 machine code that's expected to be                                 scheduled by you know some kind of                                 threading library and you know there was                                 no way that we were going to use like                                 Java threads for that because that's                                 that's just not funny so effectively we                                 wanted to redesign the JIT compiler the                                 core API and runtime from the gap from                                 the ground up because of these kind of                                 shortcomings of the language that we                                 were using but if you think about it                                 these three things have some fairly                                 different requirements the JIT compiler                                 needs things like pattern matching and                                 algebraic data type so it's nice to                                 express these kind of optimization rules                                 the core API because we want to                                 integrate it into other things needs a                                 nicely compatible FFI and we also need                                 strong support for parallelism and also                                 the ability to control kind of low-level                                 things like memory layout okay so I                                 think everyone knows where I'm going                                 with this so this was kind of what led                                 us to rust just kind of really quickly                                 reiterating the requirements we had                                 these are in no specific order we needed                                 speed because compilation happens at                                 runtime so this isn't some offline                                 compiler right the compilation time                                 actually counts toward what the user                                 sees we needed safety because we were                                 embedding things into other libraries we                                 needed a managed we did not want to                                 manage run time again because we were                                 embedding this into other runtimes we                                 wanted a rich standard library                                 functional paradigms for things like                                 pattern matching and also a good bandage                                 good managed build system because of you                                 know the kind of struggles that we had                                 with SPG so we started our search for a                                 new language and these are kind of the                                 languages that we considered up off the                                 bat but going through our requirements                                 it was pretty easy to kind of knock                                 these off right so we wanted things to                                 be fast                                 to back out of Python we wanted it to be                                 safe so we said goodbye to C++ we didn't                                 want to manage runtime so there went                                 oops there went going and Java and we                                 were basically left with Swift and rust                                 and recall that this is back in like                                                                                                        different landscape back then we wanted                                 rich standard library we wanted these                                 functional paradigms for things like                                 pattern matching so both of them kind of                                 had that back then and we also wanted a                                 good package manager and this isn't on                                 the slides but after we started using                                 both of these and evaluating them we                                 realized that we also wanted a good                                 community and that's kind of a rust                                 really one out the ability to have                                 crates dot IO and these super high                                 quality packages and also you know the                                 rust forums and everything to have                                 people help us with stuff and also the                                 ability to ask questions that weren't                                 related to iOS development um all that                                 stuff was pretty nice and it's kind of                                 what sold rust for us at the end ok so                                 i'll dive a little bit into how we                                 actually use rust for building weld them                                 so the first iteration of using of weld                                 and rust was basically just looking at                                 the compiler so we took all the scala                                 bits of our code and we ported them over                                 to rust so this involves the core api                                 the optimizer and also the compiler                                 backends                                 so to begin with just because we were                                 trying things out we decided to keep the                                 C++ runtime to do things like thread                                 management and memory and we basically                                 just had some like thin bindings between                                 the main kind of compilation part and                                 the actual runtime part to build                                 bindings that we had from before so we                                 had bindings for like Java and Python we                                 basically built a C API and I'll talk a                                 bit more about how we did this in a few                                 slides and then the C API bindings                                 basically just mapped into our existing                                 bindings for languages like Python and                                 Java okay so go anything to a little bit                                 more detail now the world IR itself is                                 implemented basically as a tree with all                                 the operators represented using an enum                                 so the main kind of construct in the IR                                 is this thing called an expression or                                 exper that has a kind that's the kind of                                 expression it is and a type such as you                                 know integer float or whatever and                                 expert kind itself is basically this                                 giant in                                 that defines all the different kinds of                                 operators that we support so you might                                 wonder why we actually did did it this                                 way another you know kind of strongman                                 way of doing it is to you something like                                 trade objects which is arguably more                                 extensible so if you want to add more                                 things you don't have to go and update                                 this enum every way and this kind of                                 goes back to the pattern matching thing                                 I was talking about a while back these                                 genomes make pattern matching super-easy                                 and basically because so much of the                                 compiler has to do with the optimizer                                 rules this was kind of a first class                                 concern for us so diving into that a                                 little bit more we were pleasantly                                 surprised that the pattern matching                                 rules were fairly similar to Scala                                 despite having to you know get around                                 the borrowed checker and everything so                                 in most cases this is the same rule that                                 we had from before it's basically                                 converting binary operations into                                 short-circuited operations all you have                                 to do is match on target pattern create                                 a substitution and then replace the                                 expression in place so there was one                                 kind of quirky thing that we had to deal                                 with in the beginning while we were                                 learning rust and that was learning to                                 live without clones and we found that                                 this was especially difficult with these                                 kind of tree and graph data structures                                 just because it's an easy escape hatch                                 so our old code for this transform looks                                 something like this and if you squint up                                 this code you can see that what it's                                 basically doing is that it's taking your                                 entire sub program and copying it every                                 time this pattern matches which isn't                                 great but there's kind of two things                                 wrong with this the first is that it's                                 kind of tricky to avoid it at least for                                 us when we were newcomers because these                                 data structures are kind of pointer                                 based and you have to deal with the                                 borrowed gender so we knew in the back                                 of our minds that there were some                                 combination of ref and ref mud and and                                 mud and whatever to you know fix this                                 but it was easier just to drop in some                                 clones and some well-chosen spaces and                                 get the bars I gotta shut up for a                                 couple weeks but unfortunately the other                                 bad thing is that this is especially                                 fatal for performance because we're                                 fundamentally dealing with recursive                                 data structures here so this ends up                                 becoming a recursive clone so at some                                 point we did have to fix these issues so                                 our solution to this was fairly simple                                 we have this cake call now and basically                                 all it does                                 you create a placeholder expression and                                 then you use this mem swap to you know                                 swap it out and he returned the old                                 thing so you can basically drop in take                                 in most places where use clone and as                                 long as you're replacing that old                                 expression that you're substituting this                                 is basically like a pointer slot and                                 this obviously gives us some pretty                                 substantial and performance benefits it                                 was just you know as newcomers to the                                 language it was hard for us to kind of                                 find these kinds of things I think                                 Steven is key note even said that mem                                 swap used to be part of the syntax you                                 know few years ago and I can totally see                                 why that's the case                                 right it took us like probably a year to                                 find that this mem swap utility even                                 existed and it's our fault for not                                 asking but now that we have it you know                                 performance is great                                 and our rules are also be easy to write                                 so we're pretty happy other things we                                 use LLVM C API for code generation and                                 it's pleasantly easy to interface with a                                 few libraries and again this is in large                                 part thanks to cargo so cargo has these                                 - sis crates it's just a paradigm in the                                 cargo where - this crate is basically                                 just definition rust definitions of ACC                                 library so here's some code that you                                 don't have to read but it's basically                                 taking a struct and generating code for                                 creating that struct and embedded in                                 there are two kind of llbmc api calls                                 and it was very natural just to you know                                 call see from rust and also the other                                 way around which i'll talk about and                                 that kind of made it easy to kind of                                 take the huge like ecosystem of c                                 libraries that are out there and use                                 them in cases where you know the same                                 thing in rust didn't exist                                 FF eyes are also very easy to build                                 compared to Scala where we needed things                                 like these weird wrapper objects we                                 needed to you know tell the GC not to                                 throw our stuff away and so forth in                                 rust we basically define some type you                                 can you know easily specify how long you                                 want that type to be you know we made an                                 alias for it and then you basically                                 create a function that you know                                 allocates the memory on the heap and and                                 forgets about it so for a rest developer                                 this is evil but you know for people who                                 you see this is very natural right you                                 call a function it allocates the memory                                 on the heap and you're responsible to                                 write it down                                 and free it somewhere else so it's a                                 very thin layer of kind of connecting                                 the sea world we're giving an interface                                 to see developers that they're                                 comfortable with and also kind of the                                 rest world where everything above this                                 is just pure rust and you don't have to                                 worry about this stuff anymore                                 and I also want to say that you can                                 almost certainly automate this kind of                                 stuff with procedural macros that we                                 haven't tried so if anyone knows of any                                 crates that does stuff like this please                                 let me know ok finally we use cargo to                                 manage literally everything about our                                 project so the FFI that was generated on                                 the previous slide                                 that's a header for that is generated                                 using a build RS file we use the                                 workspaces to build tools automatically                                 so you know literally all you have to do                                 to get the tooling and the compiler and                                 the runtime is you get clone and carve                                 go build release and everything is just                                 there sitting waiting to be run so                                 that's really nice                                 Docs and testing all that standard stuff                                 we also use cargo for yeah I love cargo                                 I think it's probably one of the most                                 underrated features of rust just because                                 it's so easy to use and get started with                                 so you know people often say that rust                                 is one of the harder programming                                 languages to learn today I've been                                 writing C for                                                       still don't know how to write a proper                                 make file so yeah if you if you think                                 that learning rust is difficult yeah try                                 to write a make file or auto comm for C                                 make or whatever those other things are                                 it's not that easy                                 so overall life was pretty good but we                                 still had that pesky C++ runtime and                                 that led to the usual host of issues so                                 we had concurrency bugs that were                                 unrelated to the generated code we had                                 two code bases which wasn't fun we had a                                 more complex build system so we still                                 had you know some nested makes left and                                 there somewhere                                 we had to logging and debugging systems                                 so just like small usability things like                                 that so our second cut of your well Dan                                 rust actually ported the entire runtime                                 to use rust as well so the fundamental                                 kind of architecture picture looks the                                 same it's just that we've replaced the                                 chunk that was in C++ and we find                                 move that into West and this gave us                                 some advantages it's safer than see                                 there's really no guarantees when you're                                 interacting with this arbitrary jittered                                 code but it's definitely things like                                 scheduling and task management are                                 definitely safer than they were before                                 we also have a nice single logging and                                 debugging API so you can set a log level                                 in the rust compiler and then you'll get                                 the kind of same logging output for when                                 you're running generated code which is                                 nice and it's also easier to pass data                                 from the runtime to the compiler also um                                 so this was easy enough before but this                                 lets us do things like asserting the                                 size of some generated structure or                                 compiler matches the size that we expect                                 in like a unit test or something it's                                 like kind of clean key stuff like that                                 that just ensures that stuff is clicking                                 so a little bit of detail on this so the                                 way that this runtime works is as                                 follows so you basically jit some                                 machine code and then you basically call                                 into rust using these FFI style                                 functions so on the rest side you                                 basically have this type definition and                                 that's all there's this JIT func type                                 definition and this indicates this                                 basically refers to a function pointer                                 that is generated somewhere by the                                 actual JIT compiler so there's some                                 function somewhere that was generated                                 because you took some Weld ir and you                                 compiled it into the memory of the                                 existing process and then you have this                                 run task function which is a standard                                 everyday rust function except it has                                 this C wrapper as well and by wrapping                                 it as like a FFI function you can                                 basically call that function from the                                 generated code so this is what that                                 looks like this is kind of scary looking                                 but it's not that bad you basically have                                 this function f                                                       the JIT and then you have some code that                                 does someone packing and stuff and then                                 you have the writing task function so                                 run task basically takes f                                             function that we generated as a input                                 this is just a pointer and run task is                                 just written in rust and how we can do                                 our standard rust stuff here so we can                                 use the standard watch library to spawn                                 threads we can you know time tasks do                                 load balancing view stuff from Quetta or                                 whatever the only funky thing is that we                                 have this call to f                                                      from rest perspective is just this kind                                 of block                                 function that it's calling and it really                                 just ends up calling into some code that                                 we generated so this obviously needs to                                 be wrapped in unsafe but you know every                                 it's it's pretty nice just because you                                 get to kind of unify the compilation                                 part and the runtime part of this stuff                                 okay so if you're interested in this                                 stuff we'd love contributors we have you                                 know around thirty contributors today                                 over a thousand github stars so the                                 project is definitely still growing                                 there's many many things to do ranging                                 from compiler optimizations to fleshing                                 out things like GPU support and we also                                 have contributions by many other groups                                 in academia and Industry so I think we                                 can find something to do no matter where                                 you are I also just want to give a quick                                 shout out to the other people at                                 Stanford who contribute to this project                                 obviously none of this would exist                                 without them and now just to conclude                                 rust we thought was a fantastic fit for                                 building kind of a modern                                 high-performance JIT compiler and                                 runtime has these nice functional                                 semantics for building a compiler it                                 also has the native execution speed                                 which is important for the runtime but                                 also for the JIT compilation part and it                                 also gives you the kind of low-level                                 control you need to build these kinds of                                 things and finally it interoperates                                 fairly seamlessly it's C which is                                 important for hooking into other                                 languages I'm so the link to the code                                 and the website and stuff is up there my                                 email is also there so if you have any                                 questions feel free to reach out Thanks                                 [Music]
YouTube URL: https://www.youtube.com/watch?v=gr11KYrB78E


