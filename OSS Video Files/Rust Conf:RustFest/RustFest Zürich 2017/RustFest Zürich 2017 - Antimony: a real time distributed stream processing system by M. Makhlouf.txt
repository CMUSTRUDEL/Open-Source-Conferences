Title: RustFest Zürich 2017 - Antimony: a real time distributed stream processing system by M. Makhlouf
Publication date: 2017-10-21
Playlist: RustFest Zürich 2017
Description: 
	A real-time stream processing and distributed computation system written completely in Rust designed to out-storm storm and is heavily influenced by the Twitter Heron paper. Antimony addresses all the architectural and efficiency issues of storm just like Heron does however implemented in a much safer language and is not bound to be backward compatible with storm. we will explore the implementation of a cross-platform high performance / lightweight stream processing and distributed computation system in Rust backed by the full power of Tokio.rs

About Mohammed Makhlouf:
Mohammed Makhlouf is a experienced Software / Security Engineer, with a passionate interest in machine learning, security and large scale distributed systems. He is the Threat Intelligence Manager at the National CERT of Qatar and is based in Qatar. He infrequently blogs at: http://blog.mak.my
Captions: 
	00:00:12,780 --> 00:00:17,380
hello i'm mohammad mahal and i work at

00:00:15,810 --> 00:00:19,720
the national search

00:00:17,380 --> 00:00:22,570
today I wish to share with you a journey

00:00:19,720 --> 00:00:25,450
that we took in as a team in learning

00:00:22,570 --> 00:00:28,360
rust and rust defying our data

00:00:25,450 --> 00:00:32,500
processing pipeline so this is a rust

00:00:28,360 --> 00:00:35,590
Asians approach to a real-time stream

00:00:32,500 --> 00:00:37,210
processing and this work is done in

00:00:35,590 --> 00:00:39,940
collaboration with one of my colleagues

00:00:37,210 --> 00:00:43,510
also another Mohammed Mohammed Samira

00:00:39,940 --> 00:00:45,550
desert and before we dive into the

00:00:43,510 --> 00:00:50,230
details of this journey of how we

00:00:45,550 --> 00:00:53,769
learned to Rasta for our data processing

00:00:50,230 --> 00:00:57,550
pipeline I want to give some background

00:00:53,769 --> 00:01:00,790
and what we use the real-time stream

00:00:57,550 --> 00:01:05,560
processing for and to give some history

00:01:00,790 --> 00:01:07,360
and context about why we use that so at

00:01:05,560 --> 00:01:09,729
first I will be rambling about some

00:01:07,360 --> 00:01:11,290
relevant and irrelevant history to why

00:01:09,729 --> 00:01:13,180
we use a real-time streaming

00:01:11,290 --> 00:01:16,840
why do we need real-time streaming and

00:01:13,180 --> 00:01:20,130
our first attempt at doing real-time

00:01:16,840 --> 00:01:25,390
stream processing using Apache storm and

00:01:20,130 --> 00:01:29,079
why we came to write antimony and I will

00:01:25,390 --> 00:01:36,969
give some code example and in the end I

00:01:29,079 --> 00:01:41,770
will I will explain why the name okay so

00:01:36,969 --> 00:01:44,109
we do real-time DNS log analysis at for

00:01:41,770 --> 00:01:47,429
multiple government organizations for

00:01:44,109 --> 00:01:50,289
over a hundred government entities that

00:01:47,429 --> 00:01:52,869
vary greatly in their maturity level

00:01:50,289 --> 00:01:55,299
some of them are entities that are small

00:01:52,869 --> 00:01:58,359
offices was a handful of nodes others

00:01:55,299 --> 00:02:01,719
are mega corporations that have 10,000

00:01:58,359 --> 00:02:05,189
nodes and above and we process around 10

00:02:01,719 --> 00:02:09,610
terabytes of data of DNS logs every day

00:02:05,189 --> 00:02:11,920
and we we are mostly interested in DNS

00:02:09,610 --> 00:02:16,000
logs not the entire network stream

00:02:11,920 --> 00:02:18,490
because the DNS log itself contains a

00:02:16,000 --> 00:02:21,610
lot of interesting information that you

00:02:18,490 --> 00:02:23,950
can use to infer a lot of activity that

00:02:21,610 --> 00:02:27,220
is going on into the network so if you

00:02:23,950 --> 00:02:28,840
take a look at a sample DNS log entry it

00:02:27,220 --> 00:02:30,850
contains a lot of interesting stuff that

00:02:28,840 --> 00:02:33,550
you could use to infer what is going

00:02:30,850 --> 00:02:35,740
on inside a particular network so it has

00:02:33,550 --> 00:02:38,380
stuff like the timestamp a client the

00:02:35,740 --> 00:02:40,120
status of the request and it also have

00:02:38,380 --> 00:02:43,330
the fully qualified domain name that

00:02:40,120 --> 00:02:45,400
someone was trying to resolve in DNS and

00:02:43,330 --> 00:02:47,980
I will not dive into all the patterns

00:02:45,400 --> 00:02:50,050
that you can infer from such data but

00:02:47,980 --> 00:02:53,230
that will focus on me and one simple

00:02:50,050 --> 00:02:58,390
example that relies solely on the fqdn

00:02:53,230 --> 00:03:00,340
only so if we consider just if you look

00:02:58,390 --> 00:03:03,400
at the this logs and you only look at

00:03:00,340 --> 00:03:05,970
the fqdn and you just consider that you

00:03:03,400 --> 00:03:09,340
could you have a way to separate certain

00:03:05,970 --> 00:03:11,680
fq dns based on some rules you could

00:03:09,340 --> 00:03:14,740
find very interesting stuff in the logs

00:03:11,680 --> 00:03:19,150
so for instance if you you can weed out

00:03:14,740 --> 00:03:21,720
or separate fqd ends that contain

00:03:19,150 --> 00:03:25,270
certain keywords you could catch and

00:03:21,720 --> 00:03:27,400
detect very advanced malware stuff that

00:03:25,270 --> 00:03:30,010
could be that was used by the carbon act

00:03:27,400 --> 00:03:32,560
cyber gang who allegedly stole around

00:03:30,010 --> 00:03:35,320
500 million to 1 billion dollars and

00:03:32,560 --> 00:03:37,660
their apt malware was simply

00:03:35,320 --> 00:03:39,460
communicating with domains like this

00:03:37,660 --> 00:03:43,030
that contains certain keywords like

00:03:39,460 --> 00:03:46,510
update - Java and Adobe AIR update and

00:03:43,030 --> 00:03:49,560
system has me so I wouldn't advise you

00:03:46,510 --> 00:03:49,560
visit these websites

00:03:50,060 --> 00:03:55,800
but if you can look at the logs and you

00:03:53,220 --> 00:03:57,570
detect these certain keywords and you

00:03:55,800 --> 00:04:00,720
separate it and have an analyst look at

00:03:57,570 --> 00:04:03,300
it you could stop really big damage that

00:04:00,720 --> 00:04:06,180
could be in certain networks from

00:04:03,300 --> 00:04:08,460
targeted attacks others of that you can

00:04:06,180 --> 00:04:10,740
look about is fqd ends that are

00:04:08,460 --> 00:04:13,860
ridiculously long for human beings to

00:04:10,740 --> 00:04:16,860
remember so if you set a rule to find a

00:04:13,860 --> 00:04:19,709
few dents that are quite long you can

00:04:16,860 --> 00:04:21,630
find a lot of domain generated F

00:04:19,709 --> 00:04:23,610
gideon's that are generated by a domain

00:04:21,630 --> 00:04:28,020
generation algorithm and that's commonly

00:04:23,610 --> 00:04:31,110
used by malware writers and there's this

00:04:28,020 --> 00:04:33,900
paper that did a large scale malware

00:04:31,110 --> 00:04:37,440
analysis of a huge amount of malware and

00:04:33,900 --> 00:04:40,380
DNS is one of the favorite channels that

00:04:37,440 --> 00:04:42,240
malware writers used to do command and

00:04:40,380 --> 00:04:45,930
control and also to exfiltrate data

00:04:42,240 --> 00:04:47,700
through the DNS so as it's a great tool

00:04:45,930 --> 00:04:50,580
for the attackers it's also a great tool

00:04:47,700 --> 00:04:53,550
for defenders if you want to have a kill

00:04:50,580 --> 00:04:56,700
zone to stop this traffic if you do the

00:04:53,550 --> 00:04:58,140
DNS log analysis you can infer a lot of

00:04:56,700 --> 00:05:01,080
things that's going on in a network and

00:04:58,140 --> 00:05:04,550
you could stop part of bad malicious

00:05:01,080 --> 00:05:06,900
activities within the network so we

00:05:04,550 --> 00:05:09,990
decided as a team we were two

00:05:06,900 --> 00:05:16,020
programmers and three or four analysts

00:05:09,990 --> 00:05:18,780
simply started to write a real-time

00:05:16,020 --> 00:05:21,210
stream processing data pipeline to

00:05:18,780 --> 00:05:23,340
ingest all this data that comes at

00:05:21,210 --> 00:05:26,340
various speeds and do some analysis and

00:05:23,340 --> 00:05:28,290
we we we were coming from different

00:05:26,340 --> 00:05:30,270
programming backgrounds and we all

00:05:28,290 --> 00:05:32,160
agreed that Python is a good middle

00:05:30,270 --> 00:05:34,680
ground for everyone for the programmers

00:05:32,160 --> 00:05:36,510
at the analyst and we wanted to use

00:05:34,680 --> 00:05:38,580
Apache storm because we thought that's a

00:05:36,510 --> 00:05:41,190
good fit and we came across something

00:05:38,580 --> 00:05:44,550
called pilots from Yelp that allows you

00:05:41,190 --> 00:05:47,130
to write stream processing real-time

00:05:44,550 --> 00:05:50,040
jobs in in Python and you don't have to

00:05:47,130 --> 00:05:54,420
fiddle around with anything related to

00:05:50,040 --> 00:05:54,930
JVM and so that was a good choice for

00:05:54,420 --> 00:05:58,740
everyone

00:05:54,930 --> 00:06:00,990
programmers and the analysts so storm

00:05:58,740 --> 00:06:02,540
was open sourced by Twitter and written

00:06:00,990 --> 00:06:06,110
by Nathan Mars in 2000

00:06:02,540 --> 00:06:09,080
eleven and we built our entire DNS log

00:06:06,110 --> 00:06:14,750
analysis pipeline on top of Apache stone

00:06:09,080 --> 00:06:17,780
using a Python and a what is the essence

00:06:14,750 --> 00:06:20,690
of a real real time streaming job it's

00:06:17,780 --> 00:06:23,750
basically a MapReduce like of a job but

00:06:20,690 --> 00:06:26,540
where the input is a continuously

00:06:23,750 --> 00:06:30,340
incoming stream of data tuples that

00:06:26,540 --> 00:06:32,180
never answers an ever going process that

00:06:30,340 --> 00:06:34,580
continually running and you can

00:06:32,180 --> 00:06:38,360
represent this computation in the form

00:06:34,580 --> 00:06:40,340
of a directed acyclic graph that the

00:06:38,360 --> 00:06:43,510
stream of data is incoming and you want

00:06:40,340 --> 00:06:45,590
to direct the stream into a series of

00:06:43,510 --> 00:06:48,080
computations without having to worry

00:06:45,590 --> 00:06:52,780
about how to do the whole distribution

00:06:48,080 --> 00:06:56,570
of this you don't have to write complex

00:06:52,780 --> 00:07:00,950
networking layer stuff so storm is great

00:06:56,570 --> 00:07:02,870
for that and the this directly the

00:07:00,950 --> 00:07:05,780
cyclic graph in the terminology of

00:07:02,870 --> 00:07:09,470
apache storm it's called a topology and

00:07:05,780 --> 00:07:11,750
it contains nodes of two kinds one kind

00:07:09,470 --> 00:07:14,840
is called the spout and the spout is a

00:07:11,750 --> 00:07:17,270
source of data it's a a node that is the

00:07:14,840 --> 00:07:19,550
reading data in a continuous fashion so

00:07:17,270 --> 00:07:22,040
it could be reading data from a queue

00:07:19,550 --> 00:07:24,590
like Kafka or Anne Askew or even

00:07:22,040 --> 00:07:27,860
grabbing data from network or from a

00:07:24,590 --> 00:07:30,830
MySQL database and we have a bolt both

00:07:27,860 --> 00:07:32,780
is a the node that is actually doing the

00:07:30,830 --> 00:07:34,790
computation so if you are computing some

00:07:32,780 --> 00:07:37,070
value you're performing some arbitrary

00:07:34,790 --> 00:07:39,530
function or joins or these type of

00:07:37,070 --> 00:07:44,510
operations you write this in the form of

00:07:39,530 --> 00:07:47,300
a bolt and so if we consider a very

00:07:44,510 --> 00:07:50,000
simple topology linear topology like

00:07:47,300 --> 00:07:52,010
that that have a spout that is a low

00:07:50,000 --> 00:07:53,990
entry spout that is grabbing the log

00:07:52,010 --> 00:07:56,080
entries from somewhere from a queue or

00:07:53,990 --> 00:08:00,440
from a network service and it is

00:07:56,080 --> 00:08:03,800
forwarding this incoming data tupple to

00:08:00,440 --> 00:08:07,160
the extract fqdn bolt to extract the

00:08:03,800 --> 00:08:10,040
fqdn out of the log entry and then it

00:08:07,160 --> 00:08:14,419
passes on to count how many occurrences

00:08:10,040 --> 00:08:16,139
of this fqdn has appeared in the entire

00:08:14,419 --> 00:08:22,499
stream over a certain period

00:08:16,139 --> 00:08:25,110
time or in general and this is a that

00:08:22,499 --> 00:08:29,490
directed acyclic graph that it's called

00:08:25,110 --> 00:08:32,159
a topology and storms terminology and

00:08:29,490 --> 00:08:34,409
it's also known as the logical plan you

00:08:32,159 --> 00:08:36,269
you simply represent your computation in

00:08:34,409 --> 00:08:38,579
the form of a topology and you submit it

00:08:36,269 --> 00:08:41,760
to storm and storm or handle how to

00:08:38,579 --> 00:08:45,300
distribute that over a set up of of

00:08:41,760 --> 00:08:48,769
nodes cluster of nodes and four so in

00:08:45,300 --> 00:08:52,529
this example if you have this topology

00:08:48,769 --> 00:08:54,529
you you want the look to come in and you

00:08:52,529 --> 00:08:57,630
want to parse so you don't care which

00:08:54,529 --> 00:08:59,399
particular bolt is going to parse

00:08:57,630 --> 00:09:01,829
they're all equal you're just extracting

00:08:59,399 --> 00:09:03,750
the fqdn so you can shuffle group any

00:09:01,829 --> 00:09:06,390
you have multiple instances of the

00:09:03,750 --> 00:09:10,560
spouts and it pushes this couple of data

00:09:06,390 --> 00:09:13,410
into a extraction bolt that will extract

00:09:10,560 --> 00:09:15,390
the fqdn out of the log and then when

00:09:13,410 --> 00:09:17,940
you are passing it on to the count fqdn

00:09:15,390 --> 00:09:21,029
you care where this exact that QD n go

00:09:17,940 --> 00:09:23,550
to which exact bolt to maintain the camp

00:09:21,029 --> 00:09:30,600
so you can filter by the field of

00:09:23,550 --> 00:09:34,890
outgoing stream so what is the

00:09:30,600 --> 00:09:35,970
motivation to rewrite the entire data

00:09:34,890 --> 00:09:37,980
stream processing

00:09:35,970 --> 00:09:43,019
well first we needed an excuse to write

00:09:37,980 --> 00:09:45,209
more rust so and we thought that the way

00:09:43,019 --> 00:09:48,029
we were running storm was in the most

00:09:45,209 --> 00:09:50,070
efficient way because we are using PI

00:09:48,029 --> 00:09:52,380
layers which takes the entire Python

00:09:50,070 --> 00:09:54,779
interpreter put it inside of a jar and

00:09:52,380 --> 00:09:57,269
then take this jar and distribute all

00:09:54,779 --> 00:09:59,940
that onto the notes so there is in very

00:09:57,269 --> 00:10:02,970
efficient way to do it and we thought

00:09:59,940 --> 00:10:06,890
that if we rewrite the the streaming

00:10:02,970 --> 00:10:11,130
engine in in rust we might gain some

00:10:06,890 --> 00:10:15,209
performance out of that so this is how

00:10:11,130 --> 00:10:18,540
storm does this distribution you submit

00:10:15,209 --> 00:10:20,910
a topology in the form of a jar to a

00:10:18,540 --> 00:10:23,100
master node called Nimbus and the Nimbus

00:10:20,910 --> 00:10:25,140
is responsible for transforming your

00:10:23,100 --> 00:10:27,060
logical topology plane into a physical

00:10:25,140 --> 00:10:27,780
plan and how to distribute that and a

00:10:27,060 --> 00:10:30,800
bunch of

00:10:27,780 --> 00:10:33,270
cluster notes a cluster of notes and

00:10:30,800 --> 00:10:36,120
there are slave nodes that contain a

00:10:33,270 --> 00:10:38,640
process called a supervisor and has a

00:10:36,120 --> 00:10:42,360
bunch of storm workers that will

00:10:38,640 --> 00:10:46,950
actually do the job of your of your

00:10:42,360 --> 00:10:53,460
topology and this had some serious

00:10:46,950 --> 00:10:56,820
problems was it first the Nimbus as the

00:10:53,460 --> 00:11:00,020
master node is very overloaded is a it's

00:10:56,820 --> 00:11:02,340
responsible for scheduling for

00:11:00,020 --> 00:11:04,410
coordination and monitoring and it is

00:11:02,340 --> 00:11:07,890
not resource aware so if you ask the

00:11:04,410 --> 00:11:09,570
Nimbus to make four hundred instances of

00:11:07,890 --> 00:11:11,850
a certain process it will just do that

00:11:09,570 --> 00:11:13,860
regardless of the resources that are

00:11:11,850 --> 00:11:19,170
available in your cluster so it's not

00:11:13,860 --> 00:11:22,140
resource aware and it is also a single

00:11:19,170 --> 00:11:25,110
point of failure so if you if the Nimbus

00:11:22,140 --> 00:11:26,490
node fails your entire cluster of storm

00:11:25,110 --> 00:11:30,410
all the topologies that were running

00:11:26,490 --> 00:11:33,420
there are gone another thing was the

00:11:30,410 --> 00:11:38,460
with storm is that it has a quite of a

00:11:33,420 --> 00:11:41,180
complex execution model where it the

00:11:38,460 --> 00:11:45,060
storm worker itself is just a JVM

00:11:41,180 --> 00:11:46,980
process and it inside the JVM process it

00:11:45,060 --> 00:11:50,160
have multiple executors which are

00:11:46,980 --> 00:11:51,780
essentially a pair of threads and this

00:11:50,160 --> 00:11:54,360
pair of threads are responsible for

00:11:51,780 --> 00:11:56,460
taking your task your code logic that

00:11:54,360 --> 00:11:59,510
that you want to execute to execute them

00:11:56,460 --> 00:12:01,980
within the JVM process so there is a

00:11:59,510 --> 00:12:03,990
multiple you are doing scheduling at

00:12:01,980 --> 00:12:06,750
multiple layers and the multiplexing

00:12:03,990 --> 00:12:08,880
multiplexing of this scheduling is quite

00:12:06,750 --> 00:12:12,810
difficult to reason about and very hard

00:12:08,880 --> 00:12:14,700
to debug a task that is there is

00:12:12,810 --> 00:12:20,520
scheduled with this multiple layers of

00:12:14,700 --> 00:12:24,839
multiplexing so inside of a storm storm

00:12:20,520 --> 00:12:27,690
worker the this is a JVM process and it

00:12:24,839 --> 00:12:30,000
has two main two threads per worker one

00:12:27,690 --> 00:12:32,190
is called the work to receive thread and

00:12:30,000 --> 00:12:35,339
the other is called the workers and

00:12:32,190 --> 00:12:37,569
thread and they pick up data data tuples

00:12:35,339 --> 00:12:41,470
from the from the network and

00:12:37,569 --> 00:12:44,199
pass it on to the to the executor that

00:12:41,470 --> 00:12:47,709
is containing your code logic and within

00:12:44,199 --> 00:12:51,670
this executor you have two two threads

00:12:47,709 --> 00:12:53,199
one is called the user logic thread that

00:12:51,670 --> 00:12:55,600
contains the code that you want to

00:12:53,199 --> 00:12:58,059
perform and the other is the same thread

00:12:55,600 --> 00:12:59,850
that is responsible for sending out the

00:12:58,059 --> 00:13:04,179
data Topol after it's being processed

00:12:59,850 --> 00:13:06,429
and it uses something called the Lmax

00:13:04,179 --> 00:13:09,249
disruptor queues inside of this process

00:13:06,429 --> 00:13:11,889
to queue this data tuples and this is

00:13:09,249 --> 00:13:13,569
quite a little bit too much - if you

00:13:11,889 --> 00:13:16,600
want to debug something that is running

00:13:13,569 --> 00:13:19,209
in this form and you also wrote that in

00:13:16,600 --> 00:13:21,639
and in the form of a python interpreter

00:13:19,209 --> 00:13:24,519
and a python script inside of a jar it's

00:13:21,639 --> 00:13:28,449
kind of messy to debug if you when you

00:13:24,519 --> 00:13:30,249
have a strong cluster and it fails but

00:13:28,449 --> 00:13:33,759
it's very difficult to know where to

00:13:30,249 --> 00:13:37,059
even start with that and another problem

00:13:33,759 --> 00:13:42,819
that we started to to face also is the

00:13:37,059 --> 00:13:45,249
larger the the the storm cluster we have

00:13:42,819 --> 00:13:49,029
the more overloaded zookeeper started to

00:13:45,249 --> 00:13:51,759
become and it becomes very difficult to

00:13:49,029 --> 00:13:55,179
debug because storm uses zookeeper for

00:13:51,759 --> 00:14:01,089
heartbeats to maintain the the state of

00:13:55,179 --> 00:14:03,369
the of the of the stream and and that

00:14:01,089 --> 00:14:05,799
led to the zookeeper to becoming

00:14:03,369 --> 00:14:07,959
overloaded and if you are using a single

00:14:05,799 --> 00:14:11,110
zookeeper cluster for other services as

00:14:07,959 --> 00:14:14,709
well it becomes a big mess to to debug

00:14:11,110 --> 00:14:17,410
that so some homegrown wisdom that

00:14:14,709 --> 00:14:18,999
started to occur in our team is that if

00:14:17,410 --> 00:14:22,269
you want to write the topology just

00:14:18,999 --> 00:14:25,029
simply separate write deploy a cluster

00:14:22,269 --> 00:14:28,989
of storm workers and keep each topology

00:14:25,029 --> 00:14:33,879
at a separate cluster of storm which is

00:14:28,989 --> 00:14:36,720
terrible very terrible idea and you end

00:14:33,879 --> 00:14:39,369
up over provisioning for everything and

00:14:36,720 --> 00:14:42,879
then two things happened we thought

00:14:39,369 --> 00:14:45,069
maybe we can rewrite apache storm or

00:14:42,879 --> 00:14:47,059
find a better alternative and at that

00:14:45,069 --> 00:14:49,279
time twitter published a

00:14:47,059 --> 00:14:54,559
a research paper called Twitter heron

00:14:49,279 --> 00:14:55,969
and we saw that from the error messages

00:14:54,559 --> 00:14:58,999
that we were getting from stone we

00:14:55,969 --> 00:15:01,249
learned that strong is using Metis for I

00:14:58,999 --> 00:15:03,769
think networking for a synchronous

00:15:01,249 --> 00:15:06,979
networking and at the time we watched

00:15:03,769 --> 00:15:09,559
the mile talk in rust camp the first row

00:15:06,979 --> 00:15:12,139
scam who said well if rust have a

00:15:09,559 --> 00:15:14,929
synchronous networking maybe we could

00:15:12,139 --> 00:15:19,309
rewrite a stream processing engine and

00:15:14,929 --> 00:15:21,649
rust so this is the paper that Twitter

00:15:19,309 --> 00:15:24,109
published Heron which addresses all the

00:15:21,649 --> 00:15:25,909
problems that storm has and we thought

00:15:24,109 --> 00:15:29,149
we will use this as a guiding principle

00:15:25,909 --> 00:15:32,209
into designing our system so first of

00:15:29,149 --> 00:15:34,579
all Heron does not have a master node it

00:15:32,209 --> 00:15:36,109
doesn't use Nimbus doesn't have a master

00:15:34,579 --> 00:15:39,259
node that is responsible for scheduling

00:15:36,109 --> 00:15:44,019
it uses existing schedulers stuff like

00:15:39,259 --> 00:15:46,969
Apache misses or Marathon or aurora and

00:15:44,019 --> 00:15:50,379
each you submit your topology to the

00:15:46,969 --> 00:15:56,779
scheduler and each topology have its own

00:15:50,379 --> 00:15:59,239
topology master so the topology master

00:15:56,779 --> 00:16:01,939
is a component inherent that is

00:15:59,239 --> 00:16:05,329
responsible for translating your logical

00:16:01,939 --> 00:16:07,339
topology plane into your physical plane

00:16:05,329 --> 00:16:11,229
and also is responsible for maintaining

00:16:07,339 --> 00:16:14,209
the state and is not it does not handle

00:16:11,229 --> 00:16:16,039
heartbeats or maintaining the the flow

00:16:14,209 --> 00:16:18,289
of the data there's a separate process

00:16:16,039 --> 00:16:21,919
for that called the stream manager and

00:16:18,289 --> 00:16:24,919
the unit of abstraction for execution

00:16:21,919 --> 00:16:27,619
here is is a container not a JVM process

00:16:24,919 --> 00:16:34,129
so it's much easier to deploy on on a

00:16:27,619 --> 00:16:36,589
larger scale network so zookeeper is the

00:16:34,129 --> 00:16:39,439
one that sorry the topology master is

00:16:36,589 --> 00:16:42,229
the product that is responsible for

00:16:39,439 --> 00:16:44,539
communicating with the existing

00:16:42,229 --> 00:16:49,729
scheduler that is a resource aware so it

00:16:44,539 --> 00:16:53,689
you realize on Aurora or marathon to to

00:16:49,729 --> 00:16:55,909
allocate resources to your topology and

00:16:53,689 --> 00:16:58,399
the stream manager is the process that

00:16:55,909 --> 00:17:00,149
is responsible for the routing of the

00:16:58,399 --> 00:17:04,860
tuples and also up

00:17:00,149 --> 00:17:08,160
backpressure so if we take an example

00:17:04,860 --> 00:17:12,689
here again our same topology does the

00:17:08,160 --> 00:17:16,949
extraction and when you deploy that it

00:17:12,689 --> 00:17:20,030
becomes the physical plan when you

00:17:16,949 --> 00:17:23,730
deploy the topology the linear topology

00:17:20,030 --> 00:17:26,189
it turns into the physical plan that

00:17:23,730 --> 00:17:31,740
actually distributes your code on

00:17:26,189 --> 00:17:33,210
multiple containers and each container

00:17:31,740 --> 00:17:35,100
will contain something called the stream

00:17:33,210 --> 00:17:36,809
manager and the stream managers of all

00:17:35,100 --> 00:17:39,540
the containers in your topology will

00:17:36,809 --> 00:17:44,190
form a fully connected Network so it

00:17:39,540 --> 00:17:48,510
will connect all all of your containers

00:17:44,190 --> 00:17:50,520
with if the routing requires to go

00:17:48,510 --> 00:17:53,179
across containers there's a TCP stream

00:17:50,520 --> 00:17:56,160
between the two containers and if it's

00:17:53,179 --> 00:17:58,799
within communication needs to happen

00:17:56,160 --> 00:18:01,169
between two instances or two processes

00:17:58,799 --> 00:18:06,900
inside of the same container it was the

00:18:01,169 --> 00:18:09,179
result to UNIX domain sockets and so we

00:18:06,900 --> 00:18:11,240
the whole purpose of writing this was

00:18:09,179 --> 00:18:15,960
that we wanted to move away from our

00:18:11,240 --> 00:18:18,780
inefficient Python implementation on top

00:18:15,960 --> 00:18:19,710
of storm and we wanted to write that in

00:18:18,780 --> 00:18:24,720
in rust

00:18:19,710 --> 00:18:26,640
so in antimony you can define a topology

00:18:24,720 --> 00:18:29,940
in the form of a JSON file that explains

00:18:26,640 --> 00:18:31,650
the the components and how it flows how

00:18:29,940 --> 00:18:33,900
the data flows from one component the

00:18:31,650 --> 00:18:36,900
other in the form of a JSON file and you

00:18:33,900 --> 00:18:38,730
implement your code logic of the spouts

00:18:36,900 --> 00:18:42,470
and the bolt that you want in the

00:18:38,730 --> 00:18:46,559
following folder structure like that and

00:18:42,470 --> 00:18:48,720
you simply implement the spout on the

00:18:46,559 --> 00:18:53,040
components and you can write the code in

00:18:48,720 --> 00:18:56,250
rust so you can have as spouts and you

00:18:53,040 --> 00:19:00,510
simply this is a sample spout that just

00:18:56,250 --> 00:19:06,029
emits continuous emission of log entries

00:19:00,510 --> 00:19:11,249
and the bolt would do the actual

00:19:06,029 --> 00:19:11,929
and computation on the emitted and the

00:19:11,249 --> 00:19:16,529
emitted

00:19:11,929 --> 00:19:20,159
Topo and then there is a CLI tool which

00:19:16,529 --> 00:19:22,229
you run inside of your topology that

00:19:20,159 --> 00:19:26,099
would submit this to the topology master

00:19:22,229 --> 00:19:29,489
and would communicate with the scheduler

00:19:26,099 --> 00:19:33,229
to schedule to allocate resources for

00:19:29,489 --> 00:19:33,229
your at the components of your topology

00:19:34,369 --> 00:19:46,679
and so regarding the the name we wanted

00:19:41,820 --> 00:19:48,899
to name the project some metallic kind

00:19:46,679 --> 00:19:50,849
of name because it seems that if you go

00:19:48,899 --> 00:19:53,159
to crates dot IO and you've tried to

00:19:50,849 --> 00:19:54,690
find the rust crates you'll find that

00:19:53,159 --> 00:19:56,580
most of them are metals and you've

00:19:54,690 --> 00:19:58,589
noticed that all the metals and the

00:19:56,580 --> 00:20:04,289
periodic table are already taking on

00:19:58,589 --> 00:20:06,950
entrees at i/o so we I mean there is

00:20:04,289 --> 00:20:10,080
iron cobalt nickel titanium aluminium

00:20:06,950 --> 00:20:13,820
even stuff that cannot really rust like

00:20:10,080 --> 00:20:17,460
cobalt doesn't realize a super alloy

00:20:13,820 --> 00:20:19,469
nickel also doesn't rust so anyways we

00:20:17,460 --> 00:20:22,049
we couldn't we had to settle on

00:20:19,469 --> 00:20:26,599
something that is close enough so we

00:20:22,049 --> 00:20:29,700
picked metalloid and antimony is kind of

00:20:26,599 --> 00:20:32,369
sort of a semi metal type of thing and

00:20:29,700 --> 00:20:35,749
so we thought okay not close enough and

00:20:32,369 --> 00:20:38,909
that's how it looks like in reality and

00:20:35,749 --> 00:20:45,769
the other fact is that ancient Egyptians

00:20:38,909 --> 00:20:45,769
used to use this as mascara so thank you

00:20:46,750 --> 00:21:00,049
[Applause]

00:20:53,490 --> 00:21:13,950
thank you very much questions we have

00:21:00,049 --> 00:21:17,130
about the stream the the the throughput

00:21:13,950 --> 00:21:20,580
is about two to three times better than

00:21:17,130 --> 00:21:22,890
our apache storm but what we really made

00:21:20,580 --> 00:21:26,010
progress on as the resource allocation

00:21:22,890 --> 00:21:29,100
because our way of running storm was to

00:21:26,010 --> 00:21:31,890
do KVM virtualization for building the

00:21:29,100 --> 00:21:35,159
clusters and to move away to something

00:21:31,890 --> 00:21:39,929
that is using messes we saved over 10x

00:21:35,159 --> 00:21:42,120
in resource usage so because because we

00:21:39,929 --> 00:21:53,990
started with a not a very wise way of

00:21:42,120 --> 00:21:53,990
deploying storm yes

00:21:56,110 --> 00:22:03,030
[Applause]

00:21:59,790 --> 00:22:03,030

YouTube URL: https://www.youtube.com/watch?v=gBRwLAyZHlU


