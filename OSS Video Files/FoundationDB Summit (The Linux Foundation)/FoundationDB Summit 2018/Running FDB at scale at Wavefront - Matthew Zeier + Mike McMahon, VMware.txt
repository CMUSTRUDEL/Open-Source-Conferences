Title: Running FDB at scale at Wavefront - Matthew Zeier + Mike McMahon, VMware
Publication date: 2018-12-14
Playlist: FoundationDB Summit 2018
Description: 
	Wavefront has been running FoundationDB in production for years. Our fleet consists of 500 instances of FoundationDB spanning 35 Wavefront clusters. About 400 of those are running FoundationDB as both memory & ssd systems.

Wavefront Ops will show how we manage FoundationDB at scale, including automatically provisioning new clusters, expanding/contracting existing clusters and how we replace an entire FoundationDB fleet. 

Weâ€™ll include a live demo showing how we use Terraform to quickly add capacity and how we use Ansible to manage exclusions (or replace an entire running fleet).

The discussion will also cover the architecture design of a single Fdb instance, including disk layouts and instance store caching that enable us to achieve memory write rates of 800KHz and ssd write rates of 70KHz.
Captions: 
	00:00:04,549 --> 00:00:10,139
okay good morning everyone my Matthew

00:00:08,040 --> 00:00:12,150
this is Mike Mike needs an introduction

00:00:10,139 --> 00:00:13,980
we're both wavefront we've been away

00:00:12,150 --> 00:00:15,780
from for about two years

00:00:13,980 --> 00:00:16,740
and we're both on the team that run the

00:00:15,780 --> 00:00:20,810
service called wavefront

00:00:16,740 --> 00:00:23,880
and found HDB and this is our story

00:00:20,810 --> 00:00:24,960
before I talk about FBB I'll tell you a

00:00:23,880 --> 00:00:27,210
little bit about wavefront

00:00:24,960 --> 00:00:29,820
we're Silicon Valley's best-kept secret

00:00:27,210 --> 00:00:31,859
we're a SAS time-series metrics tracing

00:00:29,820 --> 00:00:34,260
monitoring platform and one a few places

00:00:31,859 --> 00:00:35,460
running FTB at scale and so because of

00:00:34,260 --> 00:00:37,370
that we've learned a lot of things about

00:00:35,460 --> 00:00:40,680
running it or the last couple years and

00:00:37,370 --> 00:00:45,090
people like us we have startups and

00:00:40,680 --> 00:00:47,850
large SAS companies as well so we talked

00:00:45,090 --> 00:00:50,910
about scale to the earlier we run about

00:00:47,850 --> 00:00:52,110
470 instances across 80 clusters some of

00:00:50,910 --> 00:00:56,670
these clusters actually span multiple

00:00:52,110 --> 00:01:01,770
AWS regions and that for us is about 330

00:00:56,670 --> 00:01:03,390
or sorry 3300 FTB processes and our

00:01:01,770 --> 00:01:04,979
largest cluster is often exceed about a

00:01:03,390 --> 00:01:09,150
million rights a second on the memory

00:01:04,979 --> 00:01:11,909
engine since we're all showing up

00:01:09,150 --> 00:01:14,310
architecture I'll show you ours so this

00:01:11,909 --> 00:01:16,369
is a real simplified view of a wave

00:01:14,310 --> 00:01:18,900
front stack there's three tiers and

00:01:16,369 --> 00:01:20,610
applications I'm sorry web web tier and

00:01:18,900 --> 00:01:23,759
application tier then a database channel

00:01:20,610 --> 00:01:26,040
I'll go into there the web servers here

00:01:23,759 --> 00:01:27,720
run a small three node key value store

00:01:26,040 --> 00:01:30,000
running FDB we share a content that's

00:01:27,720 --> 00:01:31,409
common to both with wavefront runs two

00:01:30,000 --> 00:01:33,930
mirrors of a primary secondary mirror

00:01:31,409 --> 00:01:35,490
and this this key value store shares

00:01:33,930 --> 00:01:36,960
stores content that's common to both

00:01:35,490 --> 00:01:38,750
sides of the mirror so things like

00:01:36,960 --> 00:01:42,420
alerts or dashboards or user

00:01:38,750 --> 00:01:44,729
usernames on the app tiers where all the

00:01:42,420 --> 00:01:46,350
magic happens we take in telemetry we do

00:01:44,729 --> 00:01:47,970
stuff with telemetry we do queries we do

00:01:46,350 --> 00:01:51,899
alerting and we store it into the

00:01:47,970 --> 00:01:54,060
database which is FTB on the database

00:01:51,899 --> 00:01:56,070
side we run as I mentioned we memory

00:01:54,060 --> 00:01:58,619
engine and the SSD engine and a

00:01:56,070 --> 00:02:02,329
coprocessor and I'll let Mike talk about

00:01:58,619 --> 00:02:05,790
the rest there yeah so this is a very

00:02:02,329 --> 00:02:07,649
high-level view of how we architect our

00:02:05,790 --> 00:02:10,979
cluster every wavefront clusters he said

00:02:07,649 --> 00:02:12,210
is actually two databases there we run

00:02:10,979 --> 00:02:14,370
the memory and the S

00:02:12,210 --> 00:02:18,390
the engine actually the easier if I come

00:02:14,370 --> 00:02:22,080
out here on the SSD side we put a stripe

00:02:18,390 --> 00:02:24,690
across a set of EBS volumes just basic

00:02:22,080 --> 00:02:26,880
GP two volumes and we shim in in front

00:02:24,690 --> 00:02:29,280
of them using enhanced i/o cache volumes

00:02:26,880 --> 00:02:32,250
those cache volumes we use to cache our

00:02:29,280 --> 00:02:33,960
reads this is important for AWS and

00:02:32,250 --> 00:02:35,370
we'll talk about that later and the

00:02:33,960 --> 00:02:36,630
rights are it's just a write through

00:02:35,370 --> 00:02:38,510
cache so the rights actually fall

00:02:36,630 --> 00:02:41,730
straight through into the EBS volumes

00:02:38,510 --> 00:02:44,160
the memory engine is not using a stripe

00:02:41,730 --> 00:02:45,570
the memory engine has we can kind of

00:02:44,160 --> 00:02:47,280
take advantage of being a time-series

00:02:45,570 --> 00:02:48,990
database we know we're always writing to

00:02:47,280 --> 00:02:51,930
the tip of the database so we use write

00:02:48,990 --> 00:02:53,430
throughput optimized st-1 disks are

00:02:51,930 --> 00:02:56,070
actually magnetic volumes but they're

00:02:53,430 --> 00:02:58,590
highly tuned for doing sequential reads

00:02:56,070 --> 00:03:01,110
and writes never at the same time but

00:02:58,590 --> 00:03:03,810
the FTB memory engine is only ever

00:03:01,110 --> 00:03:05,460
writing or reading and it's doing reads

00:03:03,810 --> 00:03:07,080
during recovery and it's doing writes

00:03:05,460 --> 00:03:10,290
during normal operations because your

00:03:07,080 --> 00:03:12,900
reads come from memory and in between

00:03:10,290 --> 00:03:16,860
the two databases we have on each node

00:03:12,900 --> 00:03:18,840
what is an f DB coprocessor let's

00:03:16,860 --> 00:03:21,200
actually see if I can have to click over

00:03:18,840 --> 00:03:21,200
here to

00:03:24,250 --> 00:03:34,430
there we go

00:03:26,890 --> 00:03:36,500
the FDB coprocessor lives between the or

00:03:34,430 --> 00:03:38,990
the FDB coprocessor lives between our

00:03:36,500 --> 00:03:40,520
SSD and our memory engine and because

00:03:38,990 --> 00:03:42,860
this isn't like a Cassandra database

00:03:40,520 --> 00:03:44,810
where it's consistently hashed it's an

00:03:42,860 --> 00:03:47,450
ordered key value store it's performing

00:03:44,810 --> 00:03:49,310
compression sorting optimizations

00:03:47,450 --> 00:03:51,170
aligning the data and the boundaries in

00:03:49,310 --> 00:03:54,830
the database so that we get the maximum

00:03:51,170 --> 00:03:56,450
reads and writes out and it's aware of

00:03:54,830 --> 00:03:58,220
the operating space available to the

00:03:56,450 --> 00:03:59,780
memory tier the memory tier is very

00:03:58,220 --> 00:04:03,920
sensitive to how much space is available

00:03:59,780 --> 00:04:06,590
to be written into it so we have our F

00:04:03,920 --> 00:04:09,200
DB coprocessor that basically can shut

00:04:06,590 --> 00:04:11,690
down writes to the database shoveled

00:04:09,200 --> 00:04:13,280
data out and then resume workloads and

00:04:11,690 --> 00:04:15,260
we queue all the workloads up keep

00:04:13,280 --> 00:04:19,640
everything durable so we don't lose

00:04:15,260 --> 00:04:22,790
anything we run our databases with a one

00:04:19,640 --> 00:04:24,080
to one processor count so earlier we

00:04:22,790 --> 00:04:26,720
talked about like there were different

00:04:24,080 --> 00:04:29,870
roles like proxies resolvers and logs

00:04:26,720 --> 00:04:32,180
and storage there's these stateful and

00:04:29,870 --> 00:04:33,979
stateless transaction processes those

00:04:32,180 --> 00:04:37,280
actually aren't like specific things

00:04:33,979 --> 00:04:39,740
like you have to run their a just a

00:04:37,280 --> 00:04:41,990
denote on a process that this is a

00:04:39,740 --> 00:04:44,450
transaction process and it can be any

00:04:41,990 --> 00:04:46,490
one of these roles so we make sure that

00:04:44,450 --> 00:04:50,240
we have one two one four storage engines

00:04:46,490 --> 00:04:52,970
and transactions are a two to one so we

00:04:50,240 --> 00:04:54,830
run two storage processes for every one

00:04:52,970 --> 00:04:56,390
transaction process on the memory tier

00:04:54,830 --> 00:04:58,760
because we're doing such a high volume

00:04:56,390 --> 00:05:01,370
of writes we need lots of proxies and

00:04:58,760 --> 00:05:02,600
resolvers to push into the transaction

00:05:01,370 --> 00:05:07,580
locks which are carrying those

00:05:02,600 --> 00:05:09,710
permutations now pass it back right we

00:05:07,580 --> 00:05:11,870
run Linux and we've become experts at

00:05:09,710 --> 00:05:13,640
running on Linux and experts at running

00:05:11,870 --> 00:05:18,410
in a somewhat hostile virtual

00:05:13,640 --> 00:05:20,419
environment so so we've learned a lot of

00:05:18,410 --> 00:05:22,130
things that has how to tune Linux or the

00:05:20,419 --> 00:05:25,300
machine we're running on around disk i/o

00:05:22,130 --> 00:05:28,160
around CPU and memory and kernel tuning

00:05:25,300 --> 00:05:30,320
so Numa it's a thing that no one

00:05:28,160 --> 00:05:34,550
remembers until snowflake told us to

00:05:30,320 --> 00:05:37,070
think about it so on multi cpu instances

00:05:34,550 --> 00:05:39,500
like the i3 16x or

00:05:37,070 --> 00:05:41,990
we are pretty prescriptive we bind a

00:05:39,500 --> 00:05:44,510
process to a sepia dove memory so we

00:05:41,990 --> 00:05:47,530
don't have the CPU thrashing across from

00:05:44,510 --> 00:05:47,530
memory access

00:05:47,690 --> 00:05:51,140
you know Mike touched on our disk layout

00:05:49,460 --> 00:05:53,510
a little earlier we're heavy users of

00:05:51,140 --> 00:05:55,730
disk caching called enhance i/o we focus

00:05:53,510 --> 00:05:57,590
mostly on read caching and so we have a

00:05:55,730 --> 00:06:00,320
1:1 mapping between the EBS volume and

00:05:57,590 --> 00:06:01,940
the instance store itself mostly because

00:06:00,320 --> 00:06:04,850
Amazon doesn't discriminate between I

00:06:01,940 --> 00:06:06,950
ops and so we sort of cheap we get free

00:06:04,850 --> 00:06:09,890
read caching i/o we then have a bunch of

00:06:06,950 --> 00:06:11,210
extra i/o for our rights and what this

00:06:09,890 --> 00:06:13,130
means in practice is you can see the

00:06:11,210 --> 00:06:15,440
blue line is our read cash rate across

00:06:13,130 --> 00:06:16,760
one of the figure out one of the larger

00:06:15,440 --> 00:06:18,830
clusters we have and then the green is

00:06:16,760 --> 00:06:21,980
the writes which we don't really cache

00:06:18,830 --> 00:06:25,100
at all yeah so we get about a hundred

00:06:21,980 --> 00:06:27,530
percent read caching because we shim and

00:06:25,100 --> 00:06:29,510
nvme in front of each one so the only

00:06:27,530 --> 00:06:31,190
overhead really on reads is just what it

00:06:29,510 --> 00:06:34,250
costs in foundation DB because it's

00:06:31,190 --> 00:06:35,720
lightning fast the only cost is also the

00:06:34,250 --> 00:06:38,380
time it takes to build the cache which

00:06:35,720 --> 00:06:40,760
is not very long about a day or so or

00:06:38,380 --> 00:06:42,740
also experts between the kernel or we've

00:06:40,760 --> 00:06:45,130
become experts these values work for us

00:06:42,740 --> 00:06:48,050
in our workload you should experiment

00:06:45,130 --> 00:06:51,290
but this is something that we have done

00:06:48,050 --> 00:06:53,930
yeah it was really important for us to

00:06:51,290 --> 00:06:55,610
tune the networking layer on the kernel

00:06:53,930 --> 00:06:58,340
especially for the cluster controller

00:06:55,610 --> 00:06:59,990
that's where latency can kill your

00:06:58,340 --> 00:07:02,510
cluster it's doing the health checks

00:06:59,990 --> 00:07:04,160
it's deciding who's gonna take what role

00:07:02,510 --> 00:07:07,250
and what you know what roles are going

00:07:04,160 --> 00:07:10,370
to be assigned out if your kernel isn't

00:07:07,250 --> 00:07:13,070
tuned well you can actually flood and

00:07:10,370 --> 00:07:14,390
DDoS your cluster controller and can

00:07:13,070 --> 00:07:15,890
take the cluster down and we're not

00:07:14,390 --> 00:07:20,540
saying it so what happened to us no well

00:07:15,890 --> 00:07:22,100
yeah not on production but so it was

00:07:20,540 --> 00:07:25,100
important to make some of these changes

00:07:22,100 --> 00:07:27,740
so that we could go from being able to

00:07:25,100 --> 00:07:32,110
run three four hundred process clusters

00:07:27,740 --> 00:07:36,590
up to much higher now were almost 500

00:07:32,110 --> 00:07:39,980
600 on a single cluster just out of

00:07:36,590 --> 00:07:42,790
tourney tuning the kernel so that we get

00:07:39,980 --> 00:07:42,790
the best performance

00:07:42,840 --> 00:07:49,120
that's all you alright so one of the

00:07:47,380 --> 00:07:50,860
things that's also important if you're

00:07:49,120 --> 00:07:53,890
gonna operate FTP at scale is the

00:07:50,860 --> 00:07:56,650
instance lifecycle it's hugely important

00:07:53,890 --> 00:07:59,850
because we have some clusters that are

00:07:56,650 --> 00:08:03,520
operating at up to 20 nodes in a cluster

00:07:59,850 --> 00:08:06,040
so being able to ensure that these

00:08:03,520 --> 00:08:10,450
instances come online and they're ready

00:08:06,040 --> 00:08:14,110
to go that they are 100% operational

00:08:10,450 --> 00:08:17,650
ready is important so we use terraform

00:08:14,110 --> 00:08:19,240
we have a system called landing party

00:08:17,650 --> 00:08:22,480
that is going that configures these

00:08:19,240 --> 00:08:24,640
instances on first boot and we also use

00:08:22,480 --> 00:08:27,810
make heavy use of ansible so that we can

00:08:24,640 --> 00:08:30,640
do entire fleet replacements at once

00:08:27,810 --> 00:08:32,650
which is we want to be able to quickly

00:08:30,640 --> 00:08:34,780
and easily change out all of the

00:08:32,650 --> 00:08:36,909
infrastructure so our database

00:08:34,780 --> 00:08:39,940
configuration and terraform is is dead

00:08:36,909 --> 00:08:42,610
simple it's how many nodes you know of

00:08:39,940 --> 00:08:44,740
what type how much storage do you want

00:08:42,610 --> 00:08:47,890
on it what version of ubuntu are you

00:08:44,740 --> 00:08:50,320
running we were locked to 1604 a bit

00:08:47,890 --> 00:08:53,200
we've just been unlocked because we had

00:08:50,320 --> 00:08:55,450
to rewrite some of the enhance io cash

00:08:53,200 --> 00:09:01,000
to support later kernels to get us on to

00:08:55,450 --> 00:09:02,650
AWS kernels as well that enables us with

00:09:01,000 --> 00:09:04,690
just some very simple helper scripts

00:09:02,650 --> 00:09:07,810
that will go out there and deploy these

00:09:04,690 --> 00:09:10,240
databases and of course because we're

00:09:07,810 --> 00:09:12,430
observability it's important for us to

00:09:10,240 --> 00:09:14,230
be able to immediately see when these

00:09:12,430 --> 00:09:17,080
nodes come online and how they actually

00:09:14,230 --> 00:09:19,810
perform and what it means you can see

00:09:17,080 --> 00:09:23,500
new memory storage nodes coming online

00:09:19,810 --> 00:09:25,330
how much CPU you can see a spike in the

00:09:23,500 --> 00:09:27,490
data that was being processed

00:09:25,330 --> 00:09:31,930
it's because there was likely a slight

00:09:27,490 --> 00:09:33,280
backup as a node joy joined so honestly

00:09:31,930 --> 00:09:35,860
we want to let computers do the hard

00:09:33,280 --> 00:09:37,839
work for us so that's what landing-party

00:09:35,860 --> 00:09:39,790
does we have landing-party and post boot

00:09:37,839 --> 00:09:41,680
systems they actually when a system

00:09:39,790 --> 00:09:43,570
comes online they're gonna go in there

00:09:41,680 --> 00:09:45,940
if it's a brand new cluster it's going

00:09:43,570 --> 00:09:47,860
to configure the memory and the SSD tier

00:09:45,940 --> 00:09:50,140
for us automatically we don't have to

00:09:47,860 --> 00:09:53,620
touch it it just configures new double

00:09:50,140 --> 00:09:55,050
redundant memory or SSD engines it's

00:09:53,620 --> 00:09:57,810
going to prepare the

00:09:55,050 --> 00:10:00,180
Foundation DB configuration files like

00:09:57,810 --> 00:10:02,820
our Numa settings it detects are we on

00:10:00,180 --> 00:10:05,760
an i3 is this a 16 X or an 8 X do I need

00:10:02,820 --> 00:10:07,830
to push out the Numa configurations it's

00:10:05,760 --> 00:10:09,990
gonna get everything ready for us so

00:10:07,830 --> 00:10:12,149
that all we have to do is turn FTB on

00:10:09,990 --> 00:10:13,980
that's the one thing we don't do we

00:10:12,149 --> 00:10:16,170
don't let it turn FTB on automatically

00:10:13,980 --> 00:10:17,700
the instances join they're prepared

00:10:16,170 --> 00:10:20,550
they're ready to roll

00:10:17,700 --> 00:10:22,890
we turn them on with intent allows us to

00:10:20,550 --> 00:10:24,959
stage work it allows us to prepare a

00:10:22,890 --> 00:10:28,529
customer to grow their infrastructure

00:10:24,959 --> 00:10:29,910
and then turn it on under controlled in

00:10:28,529 --> 00:10:31,680
the background there's a process that's

00:10:29,910 --> 00:10:33,720
taken to cluster it config and pushing

00:10:31,680 --> 00:10:35,040
it in s3 so I'll launch we can pull it

00:10:33,720 --> 00:10:38,160
back out so the machines kind of come up

00:10:35,040 --> 00:10:39,810
ready to go yeah so we can yeah exactly

00:10:38,160 --> 00:10:41,570
so the landing party also pulls down

00:10:39,810 --> 00:10:44,070
that cluster files so that's

00:10:41,570 --> 00:10:45,930
synchronized across all of the machines

00:10:44,070 --> 00:10:47,250
all of the nodes so that we don't ever

00:10:45,930 --> 00:10:52,620
have to manage that either because that

00:10:47,250 --> 00:10:54,360
that can be rather error-prone so fleet

00:10:52,620 --> 00:10:57,120
replacements really let computers do the

00:10:54,360 --> 00:10:59,910
hard work for you the thing with the

00:10:57,120 --> 00:11:01,800
fleet replacements is its we need to be

00:10:59,910 --> 00:11:05,130
able to change the tires on the car

00:11:01,800 --> 00:11:10,579
weights to the left right yep while the

00:11:05,130 --> 00:11:14,250
car is moving we don't want to have to

00:11:10,579 --> 00:11:16,010
deal with it so we have written tooling

00:11:14,250 --> 00:11:18,390
that actually will go in there and

00:11:16,010 --> 00:11:20,010
completely replace an entire cluster

00:11:18,390 --> 00:11:20,640
without any human interaction besides

00:11:20,010 --> 00:11:23,700
starting it

00:11:20,640 --> 00:11:26,310
it goes through identify as which nodes

00:11:23,700 --> 00:11:29,370
are to be removed excludes them re--

00:11:26,310 --> 00:11:31,740
coordinates to cluster checks to make

00:11:29,370 --> 00:11:33,300
sure the excludes have completed checks

00:11:31,740 --> 00:11:35,640
to make sure the coordination states

00:11:33,300 --> 00:11:37,949
have changed and let's see if I can

00:11:35,640 --> 00:11:40,370
create my head enough to speed this up

00:11:37,949 --> 00:11:40,370
for you guys

00:11:44,819 --> 00:11:48,129
so that's that's actually what it's

00:11:46,839 --> 00:11:50,769
going through and doing right now this

00:11:48,129 --> 00:11:52,720
is just an ansible job we wrote we had

00:11:50,769 --> 00:11:54,399
been challenged when I had first started

00:11:52,720 --> 00:11:55,600
at wafer and I was challenged by one of

00:11:54,399 --> 00:11:57,519
the founders who said it wasn't possible

00:11:55,600 --> 00:11:59,559
to do this he said you're not going to

00:11:57,519 --> 00:12:01,539
be able to automate excluding and

00:11:59,559 --> 00:12:07,629
replacing the entire cluster all at once

00:12:01,539 --> 00:12:09,939
is it all right so we did so this is a

00:12:07,629 --> 00:12:12,279
lot to do in flight OS upgrades without

00:12:09,939 --> 00:12:13,479
basically hitless upgrades I mean just

00:12:12,279 --> 00:12:15,850
drop a whole new set of infrastructure

00:12:13,479 --> 00:12:17,649
the tooling understands what was new it

00:12:15,850 --> 00:12:19,239
was old and goes through the exclude

00:12:17,649 --> 00:12:21,339
process without the human interaction

00:12:19,239 --> 00:12:23,679
piece of it and at the very end it

00:12:21,339 --> 00:12:26,229
disables term protection turns off the

00:12:23,679 --> 00:12:29,499
instances for us yeah so yeah that is

00:12:26,229 --> 00:12:31,919
yeah the last thing it does is before it

00:12:29,499 --> 00:12:34,269
does that is it does a sanity check

00:12:31,919 --> 00:12:36,729
coordinators move for both tiers or

00:12:34,269 --> 00:12:39,819
excludes finish did we actually exclude

00:12:36,729 --> 00:12:41,949
the right number of nodes and if

00:12:39,819 --> 00:12:44,739
everything passes disables termination

00:12:41,949 --> 00:12:47,409
protection destroys the instances and

00:12:44,739 --> 00:12:48,699
then Rhian cludes the excluded IP so

00:12:47,409 --> 00:12:52,539
that you don't have wasted exclude

00:12:48,699 --> 00:12:53,949
sitting out there a lot of motivation

00:12:52,539 --> 00:12:55,269
for the tooling was we learn the hard

00:12:53,949 --> 00:12:59,019
way what happens if you terminate a

00:12:55,269 --> 00:13:02,049
cluster coordinator without having a

00:12:59,019 --> 00:13:03,369
replacement for it yeah which is it

00:13:02,049 --> 00:13:05,789
doesn't work out yeah it doesn't work

00:13:03,369 --> 00:13:05,789
after that

00:13:09,140 --> 00:13:15,529
so this I believe yeah so it's going

00:13:11,630 --> 00:13:18,290
through the cleanup phase now so all of

00:13:15,529 --> 00:13:19,730
this is really like prime candidates we

00:13:18,290 --> 00:13:21,410
have these in Jenkins they're scheduled

00:13:19,730 --> 00:13:24,140
little jobs we don't want operators to

00:13:21,410 --> 00:13:25,940
have to remember syntax and have to go

00:13:24,140 --> 00:13:28,820
in there and do this this is just a good

00:13:25,940 --> 00:13:32,079
show for how we you know what it looks

00:13:28,820 --> 00:13:32,079
like how we do it

00:13:44,350 --> 00:13:50,180
like yeah it's only three minutes it

00:13:46,760 --> 00:13:52,880
feels like you know the longest three

00:13:50,180 --> 00:13:54,500
minutes in your life man no really

00:13:52,880 --> 00:13:56,510
though the importance here was we did as

00:13:54,500 --> 00:13:58,580
MRC said we did find out the hard way

00:13:56,510 --> 00:14:00,440
what happens if you destroy your

00:13:58,580 --> 00:14:03,320
coordinators before you've recordin ated

00:14:00,440 --> 00:14:06,950
the cluster is gone it's dead you know

00:14:03,320 --> 00:14:08,780
what happens if you terminate your node

00:14:06,950 --> 00:14:10,520
before the excludes finished well if

00:14:08,780 --> 00:14:12,380
it's only one it'll heal it'll rear

00:14:10,520 --> 00:14:14,900
eppley kate if you don't have enough

00:14:12,380 --> 00:14:18,530
replication factor or it's too many you

00:14:14,900 --> 00:14:21,590
have data loss so these tools were

00:14:18,530 --> 00:14:24,650
specifically built to remove that human

00:14:21,590 --> 00:14:26,260
error get rid of that element of it's

00:14:24,650 --> 00:14:28,730
possible that I could you know

00:14:26,260 --> 00:14:30,680
fat-fingers something forget to exclude

00:14:28,730 --> 00:14:33,980
something forget a record ination step

00:14:30,680 --> 00:14:36,110
the goal was to enable future engineers

00:14:33,980 --> 00:14:37,790
as they joined the team to not have to

00:14:36,110 --> 00:14:41,450
worry about those things and just join

00:14:37,790 --> 00:14:43,100
without there being any concerns they

00:14:41,450 --> 00:14:52,880
could focus on operating and not

00:14:43,100 --> 00:14:58,130
learning the scary parts of FDB oh there

00:14:52,880 --> 00:15:00,850
we go so I don't know how many of you

00:14:58,130 --> 00:15:04,670
are familiar with the f DB trace logs

00:15:00,850 --> 00:15:10,240
this is what the trace logs look like if

00:15:04,670 --> 00:15:13,040
you enjoy reading XML I don't they're

00:15:10,240 --> 00:15:14,780
incredibly Spartan and I actually found

00:15:13,040 --> 00:15:17,450
reading the source code for foundation

00:15:14,780 --> 00:15:21,050
dB lighter reading than trying to crack

00:15:17,450 --> 00:15:23,030
these so we have a tool internally it's

00:15:21,050 --> 00:15:27,230
called wavefront f DB tailor it

00:15:23,030 --> 00:15:30,100
specifically takes this eyesore and it

00:15:27,230 --> 00:15:33,350
turns it into this so that we can see

00:15:30,100 --> 00:15:36,110
this is on one of our larger clusters

00:15:33,350 --> 00:15:38,020
you can it takes and it shows you where

00:15:36,110 --> 00:15:41,630
the roles are assigned what machines

00:15:38,020 --> 00:15:43,880
what process import they're on how much

00:15:41,630 --> 00:15:46,700
data is flowing through overall on each

00:15:43,880 --> 00:15:47,930
node and then how much per process so

00:15:46,700 --> 00:15:50,270
that we can find when there's data

00:15:47,930 --> 00:15:52,400
distribution issues sometimes data is

00:15:50,270 --> 00:15:54,800
not properly distributed sometimes a CPU

00:15:52,400 --> 00:15:56,310
is hot somewhere and your tea logs with

00:15:54,800 --> 00:15:58,650
your CPU bound are going to have some

00:15:56,310 --> 00:16:01,529
shoes it shows us how much storage

00:15:58,650 --> 00:16:03,930
memory is being used how much if there's

00:16:01,529 --> 00:16:05,730
any transaction processes how much is

00:16:03,930 --> 00:16:08,340
being used this this is everything that

00:16:05,730 --> 00:16:12,029
comes out of the trace logs but it

00:16:08,340 --> 00:16:15,240
readable so which leads to the next

00:16:12,029 --> 00:16:17,700
point of monitoring which is basically

00:16:15,240 --> 00:16:19,589
the bread and butter of what we do so we

00:16:17,700 --> 00:16:23,310
you know we dog food we use our own

00:16:19,589 --> 00:16:25,830
platform to monitor and internally we

00:16:23,310 --> 00:16:28,230
have some tools the the wavefront fdv

00:16:25,830 --> 00:16:31,140
tailor and we also have some Python

00:16:28,230 --> 00:16:33,839
scripts it's pull data out and push it

00:16:31,140 --> 00:16:35,700
up into our monitoring platform but

00:16:33,839 --> 00:16:37,200
instead of really just showing you more

00:16:35,700 --> 00:16:39,600
pictures of dashboards I think what I

00:16:37,200 --> 00:16:41,190
would try and do is just show you like

00:16:39,600 --> 00:16:44,400
live what it looks like on our platform

00:16:41,190 --> 00:16:46,290
and how it works so while we figure out

00:16:44,400 --> 00:16:50,730
the technical aspects if this is just

00:16:46,290 --> 00:16:52,920
gonna work give us a second it's Mike

00:16:50,730 --> 00:16:56,990
Mitch and the wafer FDB tailor BN will

00:16:52,920 --> 00:16:56,990
have a link that was open sourced today

00:16:57,860 --> 00:17:02,910
yeah that's a big shout out to Devon

00:17:01,080 --> 00:17:04,500
sitting over there and Jay Bell right

00:17:02,910 --> 00:17:09,569
behind him who put in a lot of work to

00:17:04,500 --> 00:17:13,339
make that possible because we weren't

00:17:09,569 --> 00:17:13,339
sure if that was gonna happen by today

00:17:14,630 --> 00:17:25,679
yeah so the big thing in our platform is

00:17:22,459 --> 00:17:29,610
you know we had time series monitoring

00:17:25,679 --> 00:17:32,130
alerts events tracing but I think that

00:17:29,610 --> 00:17:33,990
really what sells me on it and why I

00:17:32,130 --> 00:17:38,640
love it so much is just how easy it is

00:17:33,990 --> 00:17:40,470
to see what's going on now this just

00:17:38,640 --> 00:17:42,720
looking at it doesn't look like an

00:17:40,470 --> 00:17:46,230
incredible amount but this is actually

00:17:42,720 --> 00:17:48,570
running a derivative of the roles that

00:17:46,230 --> 00:17:52,679
are assigned and we can see immediately

00:17:48,570 --> 00:17:54,510
that there are changes in the rate so if

00:17:52,679 --> 00:17:56,700
the role is a log the value is always

00:17:54,510 --> 00:17:58,650
some amount and if that changes you're

00:17:56,700 --> 00:18:00,510
gonna see a flick on the value and we

00:17:58,650 --> 00:18:03,030
can see a big red box that's an alert

00:18:00,510 --> 00:18:07,320
that we have cache errors there's

00:18:03,030 --> 00:18:10,200
something just broke so this is how we

00:18:07,320 --> 00:18:11,640
monitor a FDB we have to know

00:18:10,200 --> 00:18:16,230
every piece from top to bottom what's

00:18:11,640 --> 00:18:18,720
working what's not working yeah there we

00:18:16,230 --> 00:18:23,960
go and hand say okay shares we can see

00:18:18,720 --> 00:18:26,880
which host is dead which cash is dead

00:18:23,960 --> 00:18:28,170
and so this was a start of an event

00:18:26,880 --> 00:18:30,780
where we actually lost an instance door

00:18:28,170 --> 00:18:33,360
yeah and so we caught it right here so

00:18:30,780 --> 00:18:35,970
one of the fun things that AWS will

00:18:33,360 --> 00:18:37,980
never admit to is that things do break

00:18:35,970 --> 00:18:44,490
all the time in their environment it is

00:18:37,980 --> 00:18:46,590
a very ephemeral cloud things come and

00:18:44,490 --> 00:18:51,450
go all the time instant stores will die

00:18:46,590 --> 00:18:55,620
EBS volumes will go corrupt you have to

00:18:51,450 --> 00:18:57,780
architect to deal with that so like we

00:18:55,620 --> 00:19:00,960
put an nvme in front of each of the EBS

00:18:57,780 --> 00:19:02,760
volumes when one of them goes we have to

00:19:00,960 --> 00:19:04,880
start stock the system to get an instant

00:19:02,760 --> 00:19:07,110
store back we have to rebuild our caches

00:19:04,880 --> 00:19:09,990
but we have the alerts that show us s

00:19:07,110 --> 00:19:11,760
and we can actually see the FTB data

00:19:09,990 --> 00:19:15,060
that tells us that hey something isn't

00:19:11,760 --> 00:19:17,070
performing right and then here near the

00:19:15,060 --> 00:19:18,660
end you can see a lot more role changes

00:19:17,070 --> 00:19:22,160
as the cluster is starting to restore

00:19:18,660 --> 00:19:22,160
order and come back to life

00:19:28,020 --> 00:19:41,830
where did it go don't just leave it in

00:19:35,440 --> 00:19:44,880
this field so this is more of what comes

00:19:41,830 --> 00:19:50,080
out of your trace you can see around

00:19:44,880 --> 00:19:52,330
10:45 that the latency in f DB starts

00:19:50,080 --> 00:19:53,919
going awry we're missing data its

00:19:52,330 --> 00:19:57,490
spiking up and down the cluster is

00:19:53,919 --> 00:20:00,280
actually struggling a bit the storage

00:19:57,490 --> 00:20:03,130
and log queues which we had talked a bit

00:20:00,280 --> 00:20:05,230
about earlier your log queues writing

00:20:03,130 --> 00:20:06,669
the permutations of what's going to

00:20:05,230 --> 00:20:09,760
eventually make it into the storage

00:20:06,669 --> 00:20:11,230
queues typically should never reach

00:20:09,760 --> 00:20:13,830
above a threshold and for our

00:20:11,230 --> 00:20:17,350
configuration it's about one gig of data

00:20:13,830 --> 00:20:19,270
we can see where we begin to lose out

00:20:17,350 --> 00:20:20,710
how much operating space we can see it

00:20:19,270 --> 00:20:23,380
falling off this is all stuff that we

00:20:20,710 --> 00:20:27,190
get out of it those the trace logs from

00:20:23,380 --> 00:20:28,690
the tailing system we pull it in we can

00:20:27,190 --> 00:20:31,720
see where the SSDs here which is

00:20:28,690 --> 00:20:34,360
actually this area right down here the

00:20:31,720 --> 00:20:37,270
key value store drops out a couple times

00:20:34,360 --> 00:20:41,130
and that is from the initial we lost a

00:20:37,270 --> 00:20:41,130
drive to we had to restart

00:20:46,659 --> 00:20:50,950
you see if I couldn't one of the other

00:20:49,239 --> 00:20:55,059
nice things we get is you can see which

00:20:50,950 --> 00:20:57,820
processes are dying you have immediate

00:20:55,059 --> 00:21:00,759
observability in two processes are dying

00:20:57,820 --> 00:21:03,220
they're restarting those are just memory

00:21:00,759 --> 00:21:06,489
storage notes that went down with the

00:21:03,220 --> 00:21:08,950
event and there was they begin the

00:21:06,489 --> 00:21:10,979
process of self recovery this is a very

00:21:08,950 --> 00:21:14,919
very durable database it's very

00:21:10,979 --> 00:21:17,889
operationally it's a challenge it's a

00:21:14,919 --> 00:21:20,619
challenge but it's not a challenge

00:21:17,889 --> 00:21:22,419
because you're trying to keep it from

00:21:20,619 --> 00:21:25,659
corrupting your data like my sequel or

00:21:22,419 --> 00:21:27,669
 it's not a challenge because it

00:21:25,659 --> 00:21:30,009
has insane defaults it's a challenge

00:21:27,669 --> 00:21:33,039
because it's so durable that it favors

00:21:30,009 --> 00:21:34,840
keeping your things together which means

00:21:33,039 --> 00:21:37,210
it's going to make some hard decisions

00:21:34,840 --> 00:21:39,460
for you which is it'll stop the world

00:21:37,210 --> 00:21:43,119
and recover itself if it thinks it needs

00:21:39,460 --> 00:21:44,679
to but the alternative is losing

00:21:43,119 --> 00:21:50,190
customer data and you don't want to

00:21:44,679 --> 00:21:50,190
can't really afford that this is

00:21:50,609 --> 00:21:54,580
actually great this is one of

00:21:52,509 --> 00:21:57,249
Emergencies favorite charts this

00:21:54,580 --> 00:21:59,830
monitors the file sizes on disk for the

00:21:57,249 --> 00:22:01,479
transaction processes it monitors the

00:21:59,830 --> 00:22:03,970
actual transaction log itself

00:22:01,479 --> 00:22:06,820
and it monitors the rate of change in

00:22:03,970 --> 00:22:10,509
the transaction logs our transaction

00:22:06,820 --> 00:22:12,789
processes as they are as we are

00:22:10,509 --> 00:22:14,499
recovering and it's pulling data in and

00:22:12,789 --> 00:22:16,899
it's reading those files back into

00:22:14,499 --> 00:22:21,460
memory the processes are growing in size

00:22:16,899 --> 00:22:23,200
they're changing they're recovering and

00:22:21,460 --> 00:22:25,239
we need to be able to see that in that

00:22:23,200 --> 00:22:27,460
black line right there which is his

00:22:25,239 --> 00:22:28,809
favorite line it tells you whether or

00:22:27,460 --> 00:22:31,029
not the transaction logs are still

00:22:28,809 --> 00:22:34,509
reporting out of FDB if those stopped

00:22:31,029 --> 00:22:36,549
reporting FDB is hard down you you you

00:22:34,509 --> 00:22:39,190
have to then go in and do a little bit

00:22:36,549 --> 00:22:41,649
deeper surgery on fdv but this is how we

00:22:39,190 --> 00:22:44,019
can quickly and easily identify and show

00:22:41,649 --> 00:22:46,330
people hey FTP is recovering fine it'll

00:22:44,019 --> 00:22:49,090
come back on its own don't worry it's

00:22:46,330 --> 00:22:52,259
you know rereading in T log files from

00:22:49,090 --> 00:22:52,259
an earlier outage

00:23:00,640 --> 00:23:06,160
and some of the other items around this

00:23:03,340 --> 00:23:09,940
event you can see we we track all the

00:23:06,160 --> 00:23:12,850
metrics because they matter to us let's

00:23:09,940 --> 00:23:15,520
see if I can so you can see in the

00:23:12,850 --> 00:23:19,059
window when it first went out our oq

00:23:15,520 --> 00:23:20,980
depth if you're very familiar with IO it

00:23:19,059 --> 00:23:23,470
was not suppressing which means IO is

00:23:20,980 --> 00:23:26,620
not being read or written nothing was

00:23:23,470 --> 00:23:29,110
happening you can see where we lost it

00:23:26,620 --> 00:23:31,440
and it came back at that particular node

00:23:29,110 --> 00:23:34,950
so this is all what goes into

00:23:31,440 --> 00:23:38,260
observability for foundation DB for us

00:23:34,950 --> 00:23:41,260
and without it I don't know how we would

00:23:38,260 --> 00:23:43,570
survive to be honest and we build alerts

00:23:41,260 --> 00:23:45,580
on us yeah we do we actually build a lot

00:23:43,570 --> 00:23:47,740
of alerts on this so one of the charts

00:23:45,580 --> 00:23:50,350
one of the charts I have scrolled past

00:23:47,740 --> 00:23:54,040
quite a few times is this pegged CPU

00:23:50,350 --> 00:23:57,490
process so what's fun about this chart

00:23:54,040 --> 00:24:00,220
is so those trace logs are constantly

00:23:57,490 --> 00:24:03,610
emitting values and a very important one

00:24:00,220 --> 00:24:07,559
is the CPU seconds how much time

00:24:03,610 --> 00:24:10,720
CPU time is this process consuming if a

00:24:07,559 --> 00:24:13,870
particular process dies and it becomes

00:24:10,720 --> 00:24:17,110
unreachable by the cluster it stops

00:24:13,870 --> 00:24:21,910
emitting that value well the FTB log

00:24:17,110 --> 00:24:24,700
tailor actually continues to emit the

00:24:21,910 --> 00:24:29,370
last value it saw so we can abuse this

00:24:24,700 --> 00:24:31,960
knowledge and write alerts that look

00:24:29,370 --> 00:24:34,570
specifically for processes that are no

00:24:31,960 --> 00:24:36,610
longer in mitting this data and we can

00:24:34,570 --> 00:24:38,620
find processes that have fallen out of

00:24:36,610 --> 00:24:41,320
the cluster and they have fallen out of

00:24:38,620 --> 00:24:43,750
the cluster for any number of reasons

00:24:41,320 --> 00:24:45,340
but we can go identify those processes

00:24:43,750 --> 00:24:49,740
because the rate of change in that

00:24:45,340 --> 00:24:52,510
process is now zero and it allows us to

00:24:49,740 --> 00:24:54,010
nullify all other processes that are

00:24:52,510 --> 00:24:56,260
showing rates of change and highlight

00:24:54,010 --> 00:24:58,780
only the ones that are no longer

00:24:56,260 --> 00:25:01,120
reporting their CPU time and we use this

00:24:58,780 --> 00:25:03,340
find the process kill the process and

00:25:01,120 --> 00:25:05,140
restart it FDB monitor goes in restarts

00:25:03,340 --> 00:25:07,600
the process cluster goes back to normal

00:25:05,140 --> 00:25:10,059
operations so these are just some of the

00:25:07,600 --> 00:25:13,179
operational challenges for us but having

00:25:10,059 --> 00:25:14,110
those trace locks turned into telemetry

00:25:13,179 --> 00:25:16,090
enable

00:25:14,110 --> 00:25:18,100
us to really dig in and find these kinds

00:25:16,090 --> 00:25:20,200
of issues where we would typically

00:25:18,100 --> 00:25:21,880
before be banging our heads against the

00:25:20,200 --> 00:25:24,030
wall like why isn't this recovering why

00:25:21,880 --> 00:25:26,740
isn't this coming back

00:25:24,030 --> 00:25:28,690
we built dashboards we built alerts and

00:25:26,740 --> 00:25:33,250
now computers do all the heavy lifting

00:25:28,690 --> 00:25:34,929
and work for us and that's all I have

00:25:33,250 --> 00:25:46,780
nothing is it don't you guys have any

00:25:34,929 --> 00:25:48,190
questions about how we do what we do so

00:25:46,780 --> 00:25:51,970
you try to find the last slide here I'll

00:25:48,190 --> 00:25:55,450
have the link to the github repo or

00:25:51,970 --> 00:25:57,730
they're posted to the dashboard that

00:25:55,450 --> 00:25:59,530
Mike showed you we've explored the JSON

00:25:57,730 --> 00:26:00,520
it'll be up on the same repo to you if

00:25:59,530 --> 00:26:02,890
you were to try it out you can

00:26:00,520 --> 00:26:07,190
experiment with the same sort - whether

00:26:02,890 --> 00:26:15,589
we use in production that's what we got

00:26:07,190 --> 00:26:15,589

YouTube URL: https://www.youtube.com/watch?v=M438R4SlTFE


