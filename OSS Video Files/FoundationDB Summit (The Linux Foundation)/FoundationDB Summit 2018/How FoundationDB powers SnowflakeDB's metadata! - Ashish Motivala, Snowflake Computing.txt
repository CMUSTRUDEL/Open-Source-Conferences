Title: How FoundationDB powers SnowflakeDB's metadata! - Ashish Motivala, Snowflake Computing
Publication date: 2018-12-14
Playlist: FoundationDB Summit 2018
Description: 
	FoundationDB is an integral part of the architecture of SnowflakeDB, a cloud SQL analytics database. FDB is effectively their OLTP engine and has allowed them to build some truly amazing and differentiating features. 

Snowflake have been operating it for 4+ years, and as they have grown, FDB has continued to scale from tens of servers to thousands of servers at present.

In this talk Ashish will present why FDB was their preferred choice, what purposes it serves, and the reliability and scale of their FDB clusters.
Captions: 
	00:00:00,659 --> 00:00:06,060
hi folks I'm Ashish motivo Allah from

00:00:03,419 --> 00:00:07,500
stop lake computing I've been at

00:00:06,060 --> 00:00:09,750
snowflake computing for a long time one

00:00:07,500 --> 00:00:12,210
of the early engineers and I currently

00:00:09,750 --> 00:00:17,910
manage the foundation DB team at

00:00:12,210 --> 00:00:20,820
snowflake so Ben started this with a

00:00:17,910 --> 00:00:26,430
story and I am here to tell you another

00:00:20,820 --> 00:00:28,500
story and the story kind of goes with

00:00:26,430 --> 00:00:31,050
how snowflake started using foundation

00:00:28,500 --> 00:00:33,090
DB and what it's done with it but to

00:00:31,050 --> 00:00:35,520
tell you the story I need to tell you a

00:00:33,090 --> 00:00:39,020
little bit about scoff Lake and why we

00:00:35,520 --> 00:00:44,129
needed a product like foundation day B

00:00:39,020 --> 00:00:48,050
so and had some fantastic motivations

00:00:44,129 --> 00:00:50,820
about why they started foundation DB and

00:00:48,050 --> 00:00:54,000
it turns out that snowflake had very

00:00:50,820 --> 00:00:55,949
similar motivations right but we came

00:00:54,000 --> 00:01:00,989
out with a completely different set of

00:00:55,949 --> 00:01:02,879
products and a different product so Ben

00:01:00,989 --> 00:01:05,010
mentioned that sequel date databases

00:01:02,879 --> 00:01:09,180
were hard to manage they were hard to

00:01:05,010 --> 00:01:12,270
scale and this is true and snowflakes

00:01:09,180 --> 00:01:16,140
motivation was to build a cloud sequel

00:01:12,270 --> 00:01:19,590
analytics database which was extremely

00:01:16,140 --> 00:01:24,150
easy to manage had full six equal

00:01:19,590 --> 00:01:26,040
capability has zero tuning knobs and it

00:01:24,150 --> 00:01:29,869
supported all data models just like

00:01:26,040 --> 00:01:32,579
foundation DB layers can which include

00:01:29,869 --> 00:01:34,770
structured and semi-structured data it

00:01:32,579 --> 00:01:39,180
can support it can scale pretty easily

00:01:34,770 --> 00:01:42,420
to multiple different users multiple

00:01:39,180 --> 00:01:44,130
concurrent users and it would be a cloud

00:01:42,420 --> 00:01:49,439
database which means you would only pay

00:01:44,130 --> 00:01:52,110
for what you use so you know you'll see

00:01:49,439 --> 00:01:53,759
this as a theme whereas you see all the

00:01:52,110 --> 00:01:56,880
folks who work at Apple they've got

00:01:53,759 --> 00:01:59,219
simple beautiful elegant slides and you

00:01:56,880 --> 00:02:03,149
see all the folks from snowflake have

00:01:59,219 --> 00:02:04,829
complex and you know detail architecture

00:02:03,149 --> 00:02:06,869
slides but you know come on we're

00:02:04,829 --> 00:02:11,940
engineers you know we like architectural

00:02:06,869 --> 00:02:13,060
slides so I I have to toss you into this

00:02:11,940 --> 00:02:15,370
one but this is

00:02:13,060 --> 00:02:20,140
in some sense the architecture off

00:02:15,370 --> 00:02:21,790
snowflake and to sort of achieve all

00:02:20,140 --> 00:02:25,750
those goals that I described right

00:02:21,790 --> 00:02:28,030
before this snowflake had to do one

00:02:25,750 --> 00:02:30,970
crucial thing to begin with which was

00:02:28,030 --> 00:02:33,280
the separate compute and storage and it

00:02:30,970 --> 00:02:35,290
did it did this by saying you know we'll

00:02:33,280 --> 00:02:36,970
move to the cloud Hadoop systems are the

00:02:35,290 --> 00:02:39,760
rage already they they separated

00:02:36,970 --> 00:02:42,010
computing storage we can do the same and

00:02:39,760 --> 00:02:44,920
so we said we will put all our storage

00:02:42,010 --> 00:02:49,030
in blob storage available in the cloud

00:02:44,920 --> 00:02:53,050
cheap free abundant infinite and we can

00:02:49,030 --> 00:02:55,239
provision clusters and compute you know

00:02:53,050 --> 00:02:57,220
very easily you provision them you run

00:02:55,239 --> 00:03:00,130
your queries that and you return them

00:02:57,220 --> 00:03:02,739
when you're done right but that wasn't

00:03:00,130 --> 00:03:05,530
enough right so this this is good enough

00:03:02,739 --> 00:03:07,810
to start off building a database but to

00:03:05,530 --> 00:03:10,060
build a full cloud database you sort of

00:03:07,810 --> 00:03:12,790
had to do you know the kind of things

00:03:10,060 --> 00:03:13,989
that other database the databases didn't

00:03:12,790 --> 00:03:15,910
have to do you think you didn't have you

00:03:13,989 --> 00:03:17,290
need to have math manage billing you

00:03:15,910 --> 00:03:20,769
need to manage accounts you need to

00:03:17,290 --> 00:03:22,570
manage users you need to build all these

00:03:20,769 --> 00:03:26,290
cloud services which sort of need to

00:03:22,570 --> 00:03:28,709
talk to each other and so very early on

00:03:26,290 --> 00:03:31,660
we realized that we needed a separate

00:03:28,709 --> 00:03:35,350
metadata from storage and compute as

00:03:31,660 --> 00:03:37,120
well and that's where we started and

00:03:35,350 --> 00:03:40,060
that's what our story really starts with

00:03:37,120 --> 00:03:42,850
using foundation DB or you know a

00:03:40,060 --> 00:03:44,590
metadata stored per se all these

00:03:42,850 --> 00:03:46,150
services that you see up top you know

00:03:44,590 --> 00:03:47,769
the infrastructure manager you couldn't

00:03:46,150 --> 00:03:50,620
really think of it as our kubernetes

00:03:47,769 --> 00:03:53,739
version which existed I mean kubernetes

00:03:50,620 --> 00:03:56,500
is around here it came after a snowflake

00:03:53,739 --> 00:03:59,019
did our query optimizer our our

00:03:56,500 --> 00:04:01,030
transaction manager security and their

00:03:59,019 --> 00:04:05,320
ton of other services all of these

00:04:01,030 --> 00:04:08,350
services effectively rely on a layer of

00:04:05,320 --> 00:04:13,019
metadata services to talk to each other

00:04:08,350 --> 00:04:15,850
to get data from this meta story and

00:04:13,019 --> 00:04:17,979
these services are completely stateless

00:04:15,850 --> 00:04:19,570
all the hard state between them is

00:04:17,979 --> 00:04:25,300
stored in the middle in the metadata

00:04:19,570 --> 00:04:26,530
story so when we started out we were not

00:04:25,300 --> 00:04:28,720
using foundation

00:04:26,530 --> 00:04:31,600
we started out by using my sequel and

00:04:28,720 --> 00:04:34,180
you know we were most a lot of us came

00:04:31,600 --> 00:04:36,040
from other database companies sequel

00:04:34,180 --> 00:04:38,830
database companies and we said yeah we

00:04:36,040 --> 00:04:41,800
understand sequel we understand sequel

00:04:38,830 --> 00:04:44,110
databases we can use another sequel

00:04:41,800 --> 00:04:46,480
database as a meta story and build an

00:04:44,110 --> 00:04:52,360
application for our services on top of

00:04:46,480 --> 00:04:54,850
this but it turns out that using my

00:04:52,360 --> 00:04:59,410
sequel as a meta story is not easy and

00:04:54,850 --> 00:05:01,840
and again all the reasons that you know

00:04:59,410 --> 00:05:03,760
we we all the reasons that been

00:05:01,840 --> 00:05:06,940
mentioned kind of existed as problems

00:05:03,760 --> 00:05:08,950
were for in my sequel and possibly

00:05:06,940 --> 00:05:11,020
others as well and I'm not saying that

00:05:08,950 --> 00:05:14,140
it can't be made to work it's just not

00:05:11,020 --> 00:05:16,810
easy right so we were we were a young

00:05:14,140 --> 00:05:18,910
company we were sort of making changes

00:05:16,810 --> 00:05:22,810
you know at Lightspeed we were adding

00:05:18,910 --> 00:05:25,960
new entities for services to interact

00:05:22,810 --> 00:05:30,100
with every day and we were updating

00:05:25,960 --> 00:05:32,229
existing entities every hour so as a

00:05:30,100 --> 00:05:35,229
result the schema for the my sequel

00:05:32,229 --> 00:05:37,780
database is like it's going up it's

00:05:35,229 --> 00:05:40,210
going down you go through an upgrade and

00:05:37,780 --> 00:05:42,550
now it takes a little while for my

00:05:40,210 --> 00:05:45,310
sequel to upgrade their solutions to

00:05:42,550 --> 00:05:46,840
work with this but and even you know

00:05:45,310 --> 00:05:49,750
sort of contracted with a couple of

00:05:46,840 --> 00:05:52,960
people to say how how easy is it to do

00:05:49,750 --> 00:05:54,370
an online upgrade and then every now and

00:05:52,960 --> 00:05:56,020
then you know there was there was a bug

00:05:54,370 --> 00:05:58,450
and you made a mistake and and he said

00:05:56,020 --> 00:06:01,450
oh god I actually need to downgrade this

00:05:58,450 --> 00:06:07,120
game and change it it was it was

00:06:01,450 --> 00:06:08,650
extremely painful and of course you know

00:06:07,120 --> 00:06:10,840
early on we were running a single

00:06:08,650 --> 00:06:12,669
instance of my sequel but we knew at

00:06:10,840 --> 00:06:14,260
some point we would have to tackle scale

00:06:12,669 --> 00:06:16,419
and fault-tolerance

00:06:14,260 --> 00:06:20,860
and none of these come out of the box

00:06:16,419 --> 00:06:22,870
with my sequel you had to sort of build

00:06:20,860 --> 00:06:25,030
application level sharding is the norm

00:06:22,870 --> 00:06:29,500
anybody who runs you know these at scale

00:06:25,030 --> 00:06:31,450
those kind of knows this and the same

00:06:29,500 --> 00:06:33,490
applies to performance tuning you have

00:06:31,450 --> 00:06:36,920
to manage indexes you have to tune them

00:06:33,490 --> 00:06:39,530
somebody's full-time job if not

00:06:36,920 --> 00:06:42,230
more than one person they just end up

00:06:39,530 --> 00:06:44,600
doing this and eventually it turns out

00:06:42,230 --> 00:06:46,520
that you know database kernel developers

00:06:44,600 --> 00:06:48,710
are not the best application developers

00:06:46,520 --> 00:06:51,140
they really don't know how to build

00:06:48,710 --> 00:06:54,230
database applications too well so

00:06:51,140 --> 00:07:00,200
instead we said why don't we go look out

00:06:54,230 --> 00:07:02,720
for a no sequel solution and one that

00:07:00,200 --> 00:07:04,790
has asset transactions and the only one

00:07:02,720 --> 00:07:09,490
that we kind of found that we really

00:07:04,790 --> 00:07:13,880
considered from amongst that list was

00:07:09,490 --> 00:07:17,180
foundation DB but you know as I

00:07:13,880 --> 00:07:18,560
mentioned already this my Siebel had all

00:07:17,180 --> 00:07:26,150
the problems that we were trying to

00:07:18,560 --> 00:07:28,550
solve for snowflake itself right and one

00:07:26,150 --> 00:07:30,230
question that I get often from people is

00:07:28,550 --> 00:07:32,600
why don't you use snowflake itself as

00:07:30,230 --> 00:07:36,800
your meta store right most databases do

00:07:32,600 --> 00:07:38,180
this and why not look like you know I

00:07:36,800 --> 00:07:41,030
mean we sort of build transactions

00:07:38,180 --> 00:07:45,220
ourselves for our database so it turns

00:07:41,030 --> 00:07:48,800
out that the read and write patterns of

00:07:45,220 --> 00:07:51,560
a database like snowflake are very

00:07:48,800 --> 00:07:53,870
different from an OLTP data base all our

00:07:51,560 --> 00:07:56,000
algorithms are built for really high

00:07:53,870 --> 00:07:58,130
throughput really high read performance

00:07:56,000 --> 00:08:01,750
like you know scanning gigabytes and

00:07:58,130 --> 00:08:04,930
gigabytes of data really quickly and

00:08:01,750 --> 00:08:08,630
they're not built for low latency OLTP

00:08:04,930 --> 00:08:11,180
and also if you sort of go back to my

00:08:08,630 --> 00:08:12,770
architecture slide and you you'll

00:08:11,180 --> 00:08:15,110
remember that snowflake is just a set of

00:08:12,770 --> 00:08:18,050
services we're just running on top of a

00:08:15,110 --> 00:08:20,920
bunch of other cloud services and we're

00:08:18,050 --> 00:08:24,410
leveraging this to build a full database

00:08:20,920 --> 00:08:26,450
so foundation to be fit that bill

00:08:24,410 --> 00:08:28,370
perfectly it was simple it was a key

00:08:26,450 --> 00:08:31,190
value store it's supported transactions

00:08:28,370 --> 00:08:34,970
it was extremely reliable knock on wood

00:08:31,190 --> 00:08:43,820
we've not lost data yet ever as far as

00:08:34,970 --> 00:08:45,410
we know there's a pretty good chance you

00:08:43,820 --> 00:08:48,560
would know if you lost data so it was

00:08:45,410 --> 00:08:51,290
just very clarify that - there's

00:08:48,560 --> 00:08:54,380
sub-millisecond performance for for

00:08:51,290 --> 00:08:57,100
small reads and writes were pretty close

00:08:54,380 --> 00:08:59,420
to a millisecond performance as well and

00:08:57,100 --> 00:09:01,070
range read performance was reasonable I

00:08:59,420 --> 00:09:02,900
had to sort of put that in because it

00:09:01,070 --> 00:09:04,760
was stuff like really cares about to

00:09:02,900 --> 00:09:08,300
create performance in some sense for its

00:09:04,760 --> 00:09:10,010
own database so they were three replicas

00:09:08,300 --> 00:09:12,620
of data out of the box you didn't have

00:09:10,010 --> 00:09:14,780
to tea on this you didn't have to do any

00:09:12,620 --> 00:09:18,470
additional work for rebalancing or

00:09:14,780 --> 00:09:20,090
repartition of data and it turns out

00:09:18,470 --> 00:09:22,250
that this five-second rule that you know

00:09:20,090 --> 00:09:24,560
people you know either

00:09:22,250 --> 00:09:26,120
you don't really think is such a good

00:09:24,560 --> 00:09:28,850
thing it's really a blessing in disguise

00:09:26,120 --> 00:09:32,150
it really changes the way you architect

00:09:28,850 --> 00:09:35,330
your applications and you push a lot of

00:09:32,150 --> 00:09:37,070
logic and a lot of thinking into how you

00:09:35,330 --> 00:09:38,930
should write your transactions to make

00:09:37,070 --> 00:09:40,940
sure that they don't conflict with each

00:09:38,930 --> 00:09:44,590
other and you're sort of doing things

00:09:40,940 --> 00:09:47,030
that you know separately some sense and

00:09:44,590 --> 00:09:50,570
there's one thing there's also a feature

00:09:47,030 --> 00:09:52,220
called watches in foundation DB which is

00:09:50,570 --> 00:09:56,990
totally awesome I'll talk a little more

00:09:52,220 --> 00:09:59,540
about it later on so all of this was

00:09:56,990 --> 00:10:02,810
nice what did we lose we lost the

00:09:59,540 --> 00:10:05,000
ability to query our data directly we

00:10:02,810 --> 00:10:07,670
also lost the ability to make like easy

00:10:05,000 --> 00:10:09,260
changes using you know a functional

00:10:07,670 --> 00:10:10,850
programming language like sequel or you

00:10:09,260 --> 00:10:11,360
say oh my gosh there's a bug in the

00:10:10,850 --> 00:10:12,500
metadata

00:10:11,360 --> 00:10:15,050
why didn't I type a little sequel

00:10:12,500 --> 00:10:20,660
command and fix it sorry you can't do

00:10:15,050 --> 00:10:22,280
that so there are ways that we worked

00:10:20,660 --> 00:10:24,890
around it and I'll tell you what we did

00:10:22,280 --> 00:10:27,980
for that so back in the day when we

00:10:24,890 --> 00:10:30,980
started using foundation they'd be the

00:10:27,980 --> 00:10:33,380
entity layers that were there were not

00:10:30,980 --> 00:10:35,240
quite you know the kind that we needed

00:10:33,380 --> 00:10:36,830
right so there was some entity layers I

00:10:35,240 --> 00:10:38,030
think there's a lot more in terms of

00:10:36,830 --> 00:10:41,000
entity layers out there today

00:10:38,030 --> 00:10:46,580
and and we really needed to build our

00:10:41,000 --> 00:10:48,770
own entity layer and so we started off

00:10:46,580 --> 00:10:51,620
by saying we will have a POJO like

00:10:48,770 --> 00:10:53,150
object we call it DP or data persistence

00:10:51,620 --> 00:10:57,830
object

00:10:53,150 --> 00:10:59,780
and if you can think of it as as a table

00:10:57,830 --> 00:11:02,240
or you can think of it as an index or

00:10:59,780 --> 00:11:06,860
you can think of it as maybe a vertical

00:11:02,240 --> 00:11:10,670
partition of a table and it basically

00:11:06,860 --> 00:11:12,580
maps are objects that you get in the

00:11:10,670 --> 00:11:15,560
channel and our services are in in Java

00:11:12,580 --> 00:11:17,390
down to a key value back in in

00:11:15,560 --> 00:11:21,290
foundation dB

00:11:17,390 --> 00:11:22,700
it does versioning which means that when

00:11:21,290 --> 00:11:25,310
we change our code and we add a new

00:11:22,700 --> 00:11:27,350
property to this Java object and we

00:11:25,310 --> 00:11:28,880
release that code you know a call back

00:11:27,350 --> 00:11:30,440
comes and tells us ah hot this was the

00:11:28,880 --> 00:11:34,550
old version now here's the new version

00:11:30,440 --> 00:11:36,410
how do you want to handle this right and

00:11:34,550 --> 00:11:40,360
we were able to build really cool

00:11:36,410 --> 00:11:42,880
features on top of this like IDs our

00:11:40,360 --> 00:11:46,910
Lamport clock is built on top of this

00:11:42,880 --> 00:11:48,830
message passing and queues in fact the

00:11:46,910 --> 00:11:52,340
message passing in queues use the watch

00:11:48,830 --> 00:11:54,620
feature which I just mentioned before

00:11:52,340 --> 00:11:56,540
this and we build the kind of

00:11:54,620 --> 00:12:00,890
transaction retry logic that you know

00:11:56,540 --> 00:12:02,480
Evan said that you need in case you have

00:12:00,890 --> 00:12:04,670
a five-second timeout or there's a

00:12:02,480 --> 00:12:07,850
conflict in your transaction or there's

00:12:04,670 --> 00:12:10,820
a small hiccup in FDB we built startling

00:12:07,850 --> 00:12:12,530
in this layer we build statistics and we

00:12:10,820 --> 00:12:14,000
also got a query ability back you know

00:12:12,530 --> 00:12:15,350
of course we were finally sequel people

00:12:14,000 --> 00:12:19,490
so he said you know you can take this

00:12:15,350 --> 00:12:21,980
away from us so we we little CLI which

00:12:19,490 --> 00:12:25,280
can query these entities we build a

00:12:21,980 --> 00:12:27,170
groovy scripting tool which can so you

00:12:25,280 --> 00:12:29,770
can write tiny scripts that can go and

00:12:27,170 --> 00:12:33,350
create these entities and of course we

00:12:29,770 --> 00:12:35,510
turned our sequel engine and attached it

00:12:33,350 --> 00:12:37,250
back to these entities so you can scan

00:12:35,510 --> 00:12:39,170
these entities from foundation DB push

00:12:37,250 --> 00:12:41,000
it to our execution engine and run

00:12:39,170 --> 00:12:47,840
complex queries which can do all kinds

00:12:41,000 --> 00:12:49,760
of sequel things on top of this so I put

00:12:47,840 --> 00:12:52,640
this in just to give you a rough sense

00:12:49,760 --> 00:12:55,070
of what you know an entity specification

00:12:52,640 --> 00:12:57,380
looks like but it's really simple it's

00:12:55,070 --> 00:12:59,720
it this is this is really the set of

00:12:57,380 --> 00:13:02,150
properties that exist in an entity like

00:12:59,720 --> 00:13:04,610
in this case it's it's one key it's it's

00:13:02,150 --> 00:13:04,970
three keys and all the values and then

00:13:04,610 --> 00:13:09,079
you can

00:13:04,970 --> 00:13:13,129
specify another what we call a slice

00:13:09,079 --> 00:13:15,319
which is the same object but a partition

00:13:13,129 --> 00:13:20,029
here is a vertical partition of that

00:13:15,319 --> 00:13:22,850
object and that gives you an index and

00:13:20,029 --> 00:13:28,100
and by building all of these we were

00:13:22,850 --> 00:13:30,800
able to cobble up a ton of different

00:13:28,100 --> 00:13:33,379
entities pretty much everything in

00:13:30,800 --> 00:13:36,139
snowflake that is exposed to customers

00:13:33,379 --> 00:13:38,329
that's internal is effectively an entity

00:13:36,139 --> 00:13:41,629
in foundation DB other than the

00:13:38,329 --> 00:13:44,029
customers data itself so the last I

00:13:41,629 --> 00:13:44,360
checked this was just a couple of days

00:13:44,029 --> 00:13:47,750
ago

00:13:44,360 --> 00:13:50,029
we had close to 200 entities and here's

00:13:47,750 --> 00:13:54,050
a partial list of the kind of entities

00:13:50,029 --> 00:13:56,899
we have there a database that's an

00:13:54,050 --> 00:14:00,589
entity it foundation DB tables is just

00:13:56,899 --> 00:14:05,180
an entity with a set of pointers to blob

00:14:00,589 --> 00:14:07,279
storage partitions users accounts I

00:14:05,180 --> 00:14:09,170
mentioned all of these even our services

00:14:07,279 --> 00:14:13,129
started putting more and more data into

00:14:09,170 --> 00:14:16,100
foundation DB every single Quay that

00:14:13,129 --> 00:14:18,800
runs in snowflake turns out turns into

00:14:16,100 --> 00:14:20,930
an entry in foundation to be in fact

00:14:18,800 --> 00:14:24,800
multiple entries in foundation DB where

00:14:20,930 --> 00:14:26,990
we store stats as well for these our own

00:14:24,800 --> 00:14:30,379
transactions and locks are built on top

00:14:26,990 --> 00:14:33,500
of foundation DB and you know the you

00:14:30,379 --> 00:14:35,600
know the when when a certain job decides

00:14:33,500 --> 00:14:37,519
to wait and say aha you know I can't run

00:14:35,600 --> 00:14:39,740
because another job is holding a lock

00:14:37,519 --> 00:14:42,470
it's using that watch feature to figure

00:14:39,740 --> 00:14:43,160
out when the other job is done and it

00:14:42,470 --> 00:14:50,800
can continue

00:14:43,160 --> 00:14:53,899
so anyways going further down the path

00:14:50,800 --> 00:14:56,420
snowflake o NGA and foundation maybe got

00:14:53,899 --> 00:14:59,329
acquired roughly at the same time and

00:14:56,420 --> 00:15:01,759
we're like you know I could use a couple

00:14:59,329 --> 00:15:05,029
of expletives here but I was like like

00:15:01,759 --> 00:15:07,670
that was fine we were too busy focusing

00:15:05,029 --> 00:15:10,639
on our GA and and the product worked

00:15:07,670 --> 00:15:12,199
really well for us so we we were we were

00:15:10,639 --> 00:15:14,509
happy to continue using it at the same

00:15:12,199 --> 00:15:15,980
time what happened was as part of our

00:15:14,509 --> 00:15:17,050
contract we managed to get the source

00:15:15,980 --> 00:15:19,630
code or foundation

00:15:17,050 --> 00:15:20,890
and he said you know what we're database

00:15:19,630 --> 00:15:22,540
engineers we know a thing or two about

00:15:20,890 --> 00:15:25,990
databases we should be able to figure

00:15:22,540 --> 00:15:27,460
this out and and and so we continue down

00:15:25,990 --> 00:15:32,320
that path and I'll tell you a little bit

00:15:27,460 --> 00:15:33,940
a little bit more about that later so we

00:15:32,320 --> 00:15:37,800
built a bunch of metadata features which

00:15:33,940 --> 00:15:40,060
are purely run by FTP you know these

00:15:37,800 --> 00:15:42,100
cloning of data and time-travel of data

00:15:40,060 --> 00:15:45,520
where a table is really just pointers as

00:15:42,100 --> 00:15:47,560
to blob storage with metadata enables a

00:15:45,520 --> 00:15:50,700
ton of metadata operations which are so

00:15:47,560 --> 00:15:53,830
easy and and really and really

00:15:50,700 --> 00:15:55,150
differentiate itself like time travel

00:15:53,830 --> 00:15:56,560
which allows us to create data in the

00:15:55,150 --> 00:16:00,540
past it's purely just a metadata

00:15:56,560 --> 00:16:04,780
operation access control and security

00:16:00,540 --> 00:16:06,190
again purely metadata operations allows

00:16:04,780 --> 00:16:08,050
us to share data with other customers

00:16:06,190 --> 00:16:13,210
which is really hard to do for other

00:16:08,050 --> 00:16:15,790
databases purely metadata but as we

00:16:13,210 --> 00:16:16,780
started growing further we you know we

00:16:15,790 --> 00:16:19,110
started building a team around

00:16:16,780 --> 00:16:22,270
foundation DB in the court that we had

00:16:19,110 --> 00:16:25,300
we started running with more operational

00:16:22,270 --> 00:16:26,410
challenges so I'll start by talking a

00:16:25,300 --> 00:16:29,020
little bit about where that production

00:16:26,410 --> 00:16:32,830
deployment looks like today we run in

00:16:29,020 --> 00:16:34,660
Amazon in Azure and there's a large

00:16:32,830 --> 00:16:37,210
number of cross a large number of hosts

00:16:34,660 --> 00:16:39,940
that we run on and we split these across

00:16:37,210 --> 00:16:42,880
availability zones with with with with

00:16:39,940 --> 00:16:44,620
with the thought process that if a

00:16:42,880 --> 00:16:46,410
single fault tolerance zone or

00:16:44,620 --> 00:16:48,400
availability zone goes down then

00:16:46,410 --> 00:16:51,400
condition D begins still continue to

00:16:48,400 --> 00:16:53,110
operate we use triple replication and

00:16:51,400 --> 00:16:57,610
the SSE engine only we don't use the

00:16:53,110 --> 00:16:59,590
in-memory engine and with this current

00:16:57,610 --> 00:17:01,540
model of availability zones we get two

00:16:59,590 --> 00:17:06,550
copies of data in at least two different

00:17:01,540 --> 00:17:10,510
reasons RT logs and storage are on block

00:17:06,550 --> 00:17:14,500
storage EBS on Amazon and managed disks

00:17:10,510 --> 00:17:19,050
on AWS and be monitored stats logs

00:17:14,500 --> 00:17:19,050
errors in foundation DB this is kind of

00:17:19,620 --> 00:17:26,709
and this is what a file i today we have

00:17:24,490 --> 00:17:29,560
thirty three clusters thousand processes

00:17:26,709 --> 00:17:30,300
largest cluster it does about 300k reads

00:17:29,560 --> 00:17:33,030
00:17:30,300 --> 00:17:35,760
kay writes per second these are not huge

00:17:33,030 --> 00:17:37,080
numbers but you're gonna realize that

00:17:35,760 --> 00:17:42,690
we're running on block storage where the

00:17:37,080 --> 00:17:44,160
latencies are quite atrocious so what

00:17:42,690 --> 00:17:46,710
happens when you run in that kind of a

00:17:44,160 --> 00:17:49,110
mode right you run into challenges you

00:17:46,710 --> 00:17:51,270
you get not enough I ops and of course

00:17:49,110 --> 00:17:54,120
your latency so terrible so the death

00:17:51,270 --> 00:17:55,950
spiral problem that you know Evan

00:17:54,120 --> 00:17:58,560
mentioned not that long ago we run into

00:17:55,950 --> 00:18:01,560
this often enough and actually that was

00:17:58,560 --> 00:18:02,970
our I think we had like two outages that

00:18:01,560 --> 00:18:05,040
were long enough for at least a couple

00:18:02,970 --> 00:18:08,220
of hours long and we said no we need to

00:18:05,040 --> 00:18:09,810
do something about this and so what we

00:18:08,220 --> 00:18:11,850
also realizes the foundation to be

00:18:09,810 --> 00:18:14,670
prioritizes reads or writes at the

00:18:11,850 --> 00:18:17,520
storage node and so as soon as you know

00:18:14,670 --> 00:18:20,070
you find that you you sort of try to

00:18:17,520 --> 00:18:21,600
preempt this by saying oh if I think

00:18:20,070 --> 00:18:23,070
even if I think there's even a small

00:18:21,600 --> 00:18:25,200
probability of me going in their debts

00:18:23,070 --> 00:18:27,960
vital I will change the priorities of my

00:18:25,200 --> 00:18:34,200
rights and up them and start writing and

00:18:27,960 --> 00:18:36,240
started before reading and so if you

00:18:34,200 --> 00:18:38,370
lose the other thing that happens is

00:18:36,240 --> 00:18:39,930
since you don't have enough I ops and

00:18:38,370 --> 00:18:41,190
the data movement engineering

00:18:39,930 --> 00:18:44,970
application that needs to happen in

00:18:41,190 --> 00:18:48,300
foundation DB takes up some of your i/o

00:18:44,970 --> 00:18:49,950
bandwidth right and to sort of

00:18:48,300 --> 00:18:53,340
compensate for that you have to slow

00:18:49,950 --> 00:18:55,530
down that movement and say you know

00:18:53,340 --> 00:18:57,780
don't move data too fast what this means

00:18:55,530 --> 00:19:00,210
is you now have to over provision your

00:18:57,780 --> 00:19:02,250
storage processes right so you say I

00:19:00,210 --> 00:19:04,440
need more storage processes with smaller

00:19:02,250 --> 00:19:07,020
you know amounts of data and each one of

00:19:04,440 --> 00:19:13,170
them any provision T Logs with large

00:19:07,020 --> 00:19:15,570
amounts of memory as a multi-tenant you

00:19:13,170 --> 00:19:18,750
know architecture and a multi-channel

00:19:15,570 --> 00:19:20,640
solution snowflake has a lot of

00:19:18,750 --> 00:19:22,740
unpredictability in its Lord all of a

00:19:20,640 --> 00:19:24,990
sudden you can get a Quay that's 25

00:19:22,740 --> 00:19:28,050
megabytes long or equity that's just or

00:19:24,990 --> 00:19:30,420
you know 10 queries or baby 100 plays

00:19:28,050 --> 00:19:33,000
that are megabyte long and and you sort

00:19:30,420 --> 00:19:35,940
of have to deal with all of these and

00:19:33,000 --> 00:19:38,400
all of these will hit foundation DB as

00:19:35,940 --> 00:19:41,970
well in some sense in English in subject

00:19:38,400 --> 00:19:43,950
for the earth and so application level

00:19:41,970 --> 00:19:46,380
monitoring and throttling was a

00:19:43,950 --> 00:19:50,639
mas for us and it's really saved us a

00:19:46,380 --> 00:19:54,210
number of times the other problems that

00:19:50,639 --> 00:19:56,970
we ran into are that we turn a lot of

00:19:54,210 --> 00:20:00,029
data we try and print more than 50% of

00:19:56,970 --> 00:20:03,510
her data in foundation DB in less than

00:20:00,029 --> 00:20:06,690
two weeks and so we fragment that B tree

00:20:03,510 --> 00:20:09,480
like crazy so every now and then we had

00:20:06,690 --> 00:20:11,220
a wiggle over from the old B tree and so

00:20:09,480 --> 00:20:15,000
here we release a bunch of those

00:20:11,220 --> 00:20:17,010
processes more to new processes similar

00:20:15,000 --> 00:20:19,620
problems with back up right back up and

00:20:17,010 --> 00:20:22,620
generate a lot of data and we need to

00:20:19,620 --> 00:20:25,559
make sure that we can we can actually

00:20:22,620 --> 00:20:28,380
succeed with backhoe and takes good

00:20:25,559 --> 00:20:30,720
backup so you'll hear a presentation by

00:20:28,380 --> 00:20:32,490
one of another stall a colleague will

00:20:30,720 --> 00:20:35,760
talk about snapshot backups that were

00:20:32,490 --> 00:20:38,309
working on and three replicas are

00:20:35,760 --> 00:20:40,139
necessary but not sufficient both

00:20:38,309 --> 00:20:42,000
businesses need D our solutions and

00:20:40,139 --> 00:20:47,690
we'll hear a lot more about these as

00:20:42,000 --> 00:20:50,039
well so key takeaways from this talk

00:20:47,690 --> 00:20:51,570
that building a team around fdbs

00:20:50,039 --> 00:20:54,539
snowflake was one of the best things

00:20:51,570 --> 00:20:57,480
that we ever did if you're going to run

00:20:54,539 --> 00:21:00,269
in the cloud and you're gonna run on

00:20:57,480 --> 00:21:02,990
managed disks or a block storage make

00:21:00,269 --> 00:21:05,940
sure you're not stingy on eye ops

00:21:02,990 --> 00:21:08,519
SDB as a product is extremely robust

00:21:05,940 --> 00:21:10,139
very reliable but you need to monitor it

00:21:08,519 --> 00:21:11,429
and make sure you can react to it

00:21:10,139 --> 00:21:14,970
especially if you're running in a

00:21:11,429 --> 00:21:17,730
multi-tenant environment and the ftp

00:21:14,970 --> 00:21:19,620
layers are a huge enabler there is not a

00:21:17,730 --> 00:21:21,870
single person in the engineering

00:21:19,620 --> 00:21:24,059
organization who doesn't touch this

00:21:21,870 --> 00:21:26,490
today at snowflake and isn't familiar

00:21:24,059 --> 00:21:28,139
with it I mean the word FTB see though

00:21:26,490 --> 00:21:35,330
it's probably as popular as the word

00:21:28,139 --> 00:21:37,260
Steve Jobs at Apple so and with that

00:21:35,330 --> 00:21:38,610
we're really looking forward to

00:21:37,260 --> 00:21:39,360
contributing to the open-source

00:21:38,610 --> 00:21:41,230
community

00:21:39,360 --> 00:21:46,059
thank you

00:21:41,230 --> 00:21:46,059

YouTube URL: https://www.youtube.com/watch?v=KkeyjFMmIf8


