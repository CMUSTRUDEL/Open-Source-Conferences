Title: Future of FoundationDB Storage Engines - Steve Atherton, Apple
Publication date: 2018-12-14
Playlist: FoundationDB Summit 2018
Description: 
	This presentation will explain the role of FDB's storage engine component, how the storage engine's requirements are influenced by FDB's distributed architecture, and it will give a brief overview of the upcoming first storage engine to be designed with FDB's specific read and commit paths in mind.  In addition to higher write throughput, this new design will have a smaller space footprint via key prefix compression and a configurable ability to enable long running read transactions by trading off a reasonable amount of disk space to persist old data versions on disk.
Captions: 
	00:00:00,000 --> 00:00:04,799
this is about the future of FTB storage

00:00:02,129 --> 00:00:07,500
engines my name is Steve Atherton I've

00:00:04,799 --> 00:00:09,420
been let me use this thing I've been

00:00:07,500 --> 00:00:11,219
working on foundation DB for about four

00:00:09,420 --> 00:00:12,960
years I've done a bunch of different

00:00:11,219 --> 00:00:15,690
things but the thing I'm here to talk

00:00:12,960 --> 00:00:17,070
about is storage engines in general and

00:00:15,690 --> 00:00:22,380
the Redwood storage engine which I'm

00:00:17,070 --> 00:00:24,539
very excited to talk about today so this

00:00:22,380 --> 00:00:27,420
presentation is gonna hopefully have

00:00:24,539 --> 00:00:30,779
about four parts and minimal pauses and

00:00:27,420 --> 00:00:32,130
mistakes made by the speaker so first

00:00:30,779 --> 00:00:35,370
I'm gonna talk about storage

00:00:32,130 --> 00:00:36,989
architecture of fundation DB and then

00:00:35,370 --> 00:00:38,670
review current storage engine options

00:00:36,989 --> 00:00:40,530
talk about what we'd like to see in

00:00:38,670 --> 00:00:42,360
future storage engines and then

00:00:40,530 --> 00:00:45,809
introduce the Redwood storage engine and

00:00:42,360 --> 00:00:47,850
some of its technical highlights so

00:00:45,809 --> 00:00:50,100
what's an f DB storage engine so its

00:00:47,850 --> 00:00:52,980
main purpose is to persist keys and

00:00:50,100 --> 00:00:55,020
values to disk so it's not distributed

00:00:52,980 --> 00:00:58,620
it lives and it's used by a single

00:00:55,020 --> 00:01:00,329
process from a single thread which makes

00:00:58,620 --> 00:01:02,309
it the most exciting part of this

00:01:00,329 --> 00:01:04,409
distributed database okay

00:01:02,309 --> 00:01:07,799
maybe not but it's really important

00:01:04,409 --> 00:01:10,530
because about 90% of FTB's of an f DB

00:01:07,799 --> 00:01:13,409
clusters processes are half storage

00:01:10,530 --> 00:01:15,509
engines and the other 10% are designed

00:01:13,409 --> 00:01:20,610
to funnel data to those storage engines

00:01:15,509 --> 00:01:23,060
as fast as possible so just a little

00:01:20,610 --> 00:01:25,320
block diagram here we have to have one

00:01:23,060 --> 00:01:27,420
every day you know there has to be one

00:01:25,320 --> 00:01:31,920
system diagram and every presentation I

00:01:27,420 --> 00:01:34,200
think is the rule so we so this is

00:01:31,920 --> 00:01:37,650
showing basically where the storage

00:01:34,200 --> 00:01:39,299
engine fits in to FTB's architecture so

00:01:37,650 --> 00:01:41,490
we have this distributed log system and

00:01:39,299 --> 00:01:43,049
we have a storage server role the

00:01:41,490 --> 00:01:45,090
storage engine lives inside the storage

00:01:43,049 --> 00:01:48,509
server every storage server has exactly

00:01:45,090 --> 00:01:51,030
one storage engine instance the storage

00:01:48,509 --> 00:01:53,130
server receives mutations in version

00:01:51,030 --> 00:01:55,350
order version order from the distributed

00:01:53,130 --> 00:01:57,030
log system and it initially writes them

00:01:55,350 --> 00:01:59,820
to two places in memory two different

00:01:57,030 --> 00:02:02,009
data structures one is a tree like thing

00:01:59,820 --> 00:02:05,399
that lets you efficiently point read and

00:02:02,009 --> 00:02:09,300
range scan values of keys by their

00:02:05,399 --> 00:02:12,920
version their sorry values of sorry you

00:02:09,300 --> 00:02:15,680
can read keys and ranges

00:02:12,920 --> 00:02:17,599
a very specific consistent version and

00:02:15,680 --> 00:02:19,940
get of course their values which is why

00:02:17,599 --> 00:02:22,370
you would never mind that was a blender

00:02:19,940 --> 00:02:26,750
okay so the other place that the

00:02:22,370 --> 00:02:29,090
mutations go is a structure that stores

00:02:26,750 --> 00:02:32,540
blocks of mutations ordered by version

00:02:29,090 --> 00:02:35,180
and then on a delay those versions are

00:02:32,540 --> 00:02:39,050
about those mutations are applied in

00:02:35,180 --> 00:02:42,170
version order on the storage engine so I

00:02:39,050 --> 00:02:44,269
say on a delay because our storage

00:02:42,170 --> 00:02:45,650
engines currently are single version so

00:02:44,269 --> 00:02:47,420
once you've written something to the

00:02:45,650 --> 00:02:49,370
storage engine you can't read the value

00:02:47,420 --> 00:02:51,860
that was there before and so in order to

00:02:49,370 --> 00:02:54,830
support reads during the five-second

00:02:51,860 --> 00:02:57,920
window that that our transactions live

00:02:54,830 --> 00:02:59,299
for we have we need to keep that data in

00:02:57,920 --> 00:03:02,390
memory which is that little tree

00:02:59,299 --> 00:03:04,459
structure on the right and not push

00:03:02,390 --> 00:03:06,890
mutations to the storage engine until

00:03:04,459 --> 00:03:09,260
they've left that buffer then

00:03:06,890 --> 00:03:11,390
periodically and in practice it's

00:03:09,260 --> 00:03:14,540
whenever once every one to two seconds

00:03:11,390 --> 00:03:17,060
we commit on the storage engine and then

00:03:14,540 --> 00:03:20,690
we send a message to the distribute log

00:03:17,060 --> 00:03:22,130
system telling it it can forget the the

00:03:20,690 --> 00:03:24,850
log it was talking to can forget

00:03:22,130 --> 00:03:27,650
versions up up to that committed point

00:03:24,850 --> 00:03:32,180
so commits always happen on a clean

00:03:27,650 --> 00:03:33,829
version boundary so our current storage

00:03:32,180 --> 00:03:37,370
engines there's two of them

00:03:33,829 --> 00:03:43,160
SSD and memory so on the Left we have

00:03:37,370 --> 00:03:45,950
our SSD engine is based on sequel light

00:03:43,160 --> 00:03:47,420
so it's a b-tree on disk which has a

00:03:45,950 --> 00:03:51,109
nice property of giving you instant

00:03:47,420 --> 00:03:54,530
recovery for on on a cold start it and

00:03:51,109 --> 00:03:56,540
it's as its name implies you're supposed

00:03:54,530 --> 00:03:58,640
to use the storage engine only on SSDs

00:03:56,540 --> 00:04:01,280
on the right we have the memory storage

00:03:58,640 --> 00:04:05,000
engine which despite its name does

00:04:01,280 --> 00:04:09,489
persist data to disk and exists as

00:04:05,000 --> 00:04:13,130
essentially a binary tree in memory and

00:04:09,489 --> 00:04:15,380
it has it's on this structure is a

00:04:13,130 --> 00:04:18,350
rolling log of mutations and snapshots

00:04:15,380 --> 00:04:21,830
of keys and values from the in-memory

00:04:18,350 --> 00:04:22,990
structure so so as a result it has a

00:04:21,830 --> 00:04:27,160
slow recovery

00:04:22,990 --> 00:04:28,660
from disk it we recommend using SSDs for

00:04:27,160 --> 00:04:30,069
the memory storage engine but you could

00:04:28,660 --> 00:04:33,580
probably get away with using spinning

00:04:30,069 --> 00:04:35,590
disks so that we have these two storage

00:04:33,580 --> 00:04:37,060
engines and they're pretty great so what

00:04:35,590 --> 00:04:40,060
else could we possibly want

00:04:37,060 --> 00:04:41,830
well three main things first is read

00:04:40,060 --> 00:04:45,340
only transaction lifetimes that are

00:04:41,830 --> 00:04:47,080
longer than five seconds so note that

00:04:45,340 --> 00:04:49,690
this is not going to increase the right

00:04:47,080 --> 00:04:52,449
transaction length that's desert

00:04:49,690 --> 00:04:55,389
determined by the version arable held in

00:04:52,449 --> 00:04:57,009
the resolvers and fundamentally you

00:04:55,389 --> 00:04:58,660
probably don't want to keep on

00:04:57,009 --> 00:05:00,699
increasing that number in an optimistic

00:04:58,660 --> 00:05:04,000
concurrency system because you're going

00:05:00,699 --> 00:05:06,370
to increase the the longer you let your

00:05:04,000 --> 00:05:12,250
transactions run the more likely you are

00:05:06,370 --> 00:05:15,009
to have conflicts but to get see sorry I

00:05:12,250 --> 00:05:18,099
lost my place another thing we want is

00:05:15,009 --> 00:05:20,560
prefix compression because as you've

00:05:18,099 --> 00:05:22,979
seen earlier today with like the

00:05:20,560 --> 00:05:25,930
document model and the graph model

00:05:22,979 --> 00:05:29,680
modeling data models on top of FDB tend

00:05:25,930 --> 00:05:34,030
to use a lot of common prefix bytes in

00:05:29,680 --> 00:05:37,000
their keys we'd also like better

00:05:34,030 --> 00:05:39,880
performance particularly fewer disk

00:05:37,000 --> 00:05:42,520
reads per key and more write parallelism

00:05:39,880 --> 00:05:49,990
which are which I'll talk a little bit

00:05:42,520 --> 00:05:54,940
more about later so regarding that that

00:05:49,990 --> 00:05:57,639
five second transaction lifetimes so one

00:05:54,940 --> 00:05:59,889
thing we could do which could be done on

00:05:57,639 --> 00:06:03,159
top of foundation DB or it could be done

00:05:59,889 --> 00:06:04,810
on top of a storage engine by a proxying

00:06:03,159 --> 00:06:06,190
storage engine that basically turns a

00:06:04,810 --> 00:06:07,419
single version storage engine like

00:06:06,190 --> 00:06:09,070
signal light into a multi version

00:06:07,419 --> 00:06:13,449
storage engine with being a multi

00:06:09,070 --> 00:06:16,599
version layer so basically you'd store

00:06:13,449 --> 00:06:18,849
keys as tuples of key inversion here's a

00:06:16,599 --> 00:06:20,469
table showing some examples so we have

00:06:18,849 --> 00:06:23,830
three keys that were set at different

00:06:20,469 --> 00:06:29,650
versions and two of them were cleared so

00:06:23,830 --> 00:06:31,599
in this model reading a key becomes a in

00:06:29,650 --> 00:06:35,590
this model reading a key becomes a

00:06:31,599 --> 00:06:37,210
reverse range read from well with

00:06:35,590 --> 00:06:40,780
basically from the version you want to

00:06:37,210 --> 00:06:43,630
read plus one the way our ranges work

00:06:40,780 --> 00:06:44,860
down to the down to version zero for

00:06:43,630 --> 00:06:46,180
that key with a limit of one because you

00:06:44,860 --> 00:06:48,670
so you're doing a range me but you

00:06:46,180 --> 00:06:50,230
really just want the first result back

00:06:48,670 --> 00:06:51,670
because you you say you want to read it

00:06:50,230 --> 00:06:53,920
a at fifty and you don't actually know

00:06:51,670 --> 00:06:58,810
where a was last read or cleared our

00:06:53,920 --> 00:07:00,700
sorry said are cleared at but range read

00:06:58,810 --> 00:07:03,190
performance suffers as you accumulate

00:07:00,700 --> 00:07:05,170
old versions so in this example if I

00:07:03,190 --> 00:07:07,570
were to range read from A to D at

00:07:05,170 --> 00:07:11,530
version 30 I would read over six key

00:07:07,570 --> 00:07:13,150
value pairs and ratone only one so and

00:07:11,530 --> 00:07:14,950
you also have to scan your entire keys

00:07:13,150 --> 00:07:19,450
base to remove expired versions at some

00:07:14,950 --> 00:07:22,240
point so we don't really what so we

00:07:19,450 --> 00:07:24,310
don't want to do that we'd like to push

00:07:22,240 --> 00:07:25,780
multi version support into the storage

00:07:24,310 --> 00:07:29,230
engine and do something more efficient

00:07:25,780 --> 00:07:32,320
so in general FTB storage engine

00:07:29,230 --> 00:07:33,700
requirements and what i'd like to review

00:07:32,320 --> 00:07:35,680
this now so it has to be an ordered key

00:07:33,700 --> 00:07:39,130
value store you of course have to be

00:07:35,680 --> 00:07:41,350
able to read and write keys you need to

00:07:39,130 --> 00:07:43,840
support range read in forward and

00:07:41,350 --> 00:07:46,419
reverse order some certain encodings

00:07:43,840 --> 00:07:48,550
could make reverse odd you know awkward

00:07:46,419 --> 00:07:49,860
but that's the only reason why i bring

00:07:48,550 --> 00:07:52,030
that up

00:07:49,860 --> 00:07:55,570
here's an important one you need to have

00:07:52,030 --> 00:07:58,510
fast range clears so that is to say that

00:07:55,570 --> 00:08:01,090
your range clear operation has to take

00:07:58,510 --> 00:08:03,370
immediate effect and not significantly

00:08:01,090 --> 00:08:06,340
harm subsequent reader what or right

00:08:03,370 --> 00:08:09,700
latency it can have background work that

00:08:06,340 --> 00:08:11,620
that happens later and for a long time

00:08:09,700 --> 00:08:15,729
as is the case in our current storage

00:08:11,620 --> 00:08:19,840
engine but but the the clear age can't

00:08:15,729 --> 00:08:22,410
stop or stall the speed at which you can

00:08:19,840 --> 00:08:24,880
apply mutations to the storage engine

00:08:22,410 --> 00:08:28,120
you also need to be to read data at a

00:08:24,880 --> 00:08:30,610
committed version what we have today you

00:08:28,120 --> 00:08:32,740
can only read the latest version commit

00:08:30,610 --> 00:08:34,990
on the storage engine but what we want

00:08:32,740 --> 00:08:36,820
for future storage engines is to be able

00:08:34,990 --> 00:08:41,680
to read any committed version within

00:08:36,820 --> 00:08:43,030
some defined some configured interval so

00:08:41,680 --> 00:08:44,770
notably missing from this list of

00:08:43,030 --> 00:08:46,570
requirements are low commit latency

00:08:44,770 --> 00:08:49,480
because our distributive log system

00:08:46,570 --> 00:08:50,769
provides papaya is what

00:08:49,480 --> 00:08:53,320
determines our commit latency and

00:08:50,769 --> 00:08:55,420
provides durability for FTB transactions

00:08:53,320 --> 00:08:59,170
when you commit them so the storage

00:08:55,420 --> 00:09:02,019
engine isn't involved until later so we

00:08:59,170 --> 00:09:04,329
so we can buffer up rights and commit

00:09:02,019 --> 00:09:06,370
them like you know periodically every

00:09:04,329 --> 00:09:07,779
couple seconds for example we also don't

00:09:06,370 --> 00:09:09,190
need concurrent writers because the

00:09:07,779 --> 00:09:12,310
storage server is going to apply

00:09:09,190 --> 00:09:13,750
mutations serially so there's no need to

00:09:12,310 --> 00:09:16,779
worry about different threads or

00:09:13,750 --> 00:09:19,779
different processes accessing does the

00:09:16,779 --> 00:09:21,279
storage engine so I'd like to talk a

00:09:19,779 --> 00:09:23,889
little bit more about our current SSD

00:09:21,279 --> 00:09:25,810
storage engine which is based on the

00:09:23,889 --> 00:09:29,410
sequel Lite b-tree which notably is not

00:09:25,810 --> 00:09:31,420
a B+ tree and so quick review B B trees

00:09:29,410 --> 00:09:35,019
have values inside their internal nodes

00:09:31,420 --> 00:09:38,079
and B plus trees do not the so as a

00:09:35,019 --> 00:09:41,110
result B trees tend to have worse

00:09:38,079 --> 00:09:46,060
branching factor a branching factors

00:09:41,110 --> 00:09:50,860
than B plus trees and so we which in

00:09:46,060 --> 00:09:54,069
turn will basically sorry I lost my

00:09:50,860 --> 00:09:56,110
train of thought there with a high

00:09:54,069 --> 00:09:58,420
branching factor you could hopefully

00:09:56,110 --> 00:10:00,910
reach a point where you only have one

00:09:58,420 --> 00:10:02,440
out of cache read per per look up for

00:10:00,910 --> 00:10:05,829
point lookup which is a great property

00:10:02,440 --> 00:10:08,290
to have so sequel light is not optimized

00:10:05,829 --> 00:10:09,940
for single write or throughput every set

00:10:08,290 --> 00:10:12,699
and clear operation must traverse the

00:10:09,940 --> 00:10:15,550
tree serially to its target page and

00:10:12,699 --> 00:10:18,370
then modify it so as a result our writer

00:10:15,550 --> 00:10:20,440
thread it's really an actor but I'm

00:10:18,370 --> 00:10:22,329
calling it a thread in FDB only has

00:10:20,440 --> 00:10:26,680
about one outstanding IAP at any given

00:10:22,329 --> 00:10:29,949
time and sequel eight is also not as

00:10:26,680 --> 00:10:32,019
optimized for large key value pairs and

00:10:29,949 --> 00:10:33,579
it's not designed to be used in an async

00:10:32,019 --> 00:10:37,269
framework it's not written for an async

00:10:33,579 --> 00:10:39,910
framework so we've adapted it using Lib

00:10:37,269 --> 00:10:44,380
core o which is a library for stack

00:10:39,910 --> 00:10:46,899
folker routines and it's kind of it's a

00:10:44,380 --> 00:10:48,579
lot of complexity and we'd prefer to

00:10:46,899 --> 00:10:49,990
have a storage engine that was written

00:10:48,579 --> 00:10:52,630
in flow that's not to say that we

00:10:49,990 --> 00:10:54,130
wouldn't do the same tune a stiix again

00:10:52,630 --> 00:10:58,010
to AB dab some other great storage

00:10:54,130 --> 00:11:02,790
engine it's just that you know

00:10:58,010 --> 00:11:04,350
right now that sorry I totally it's just

00:11:02,790 --> 00:11:07,470
we're not doing that right now we we

00:11:04,350 --> 00:11:09,690
have some ideas in mind for we want our

00:11:07,470 --> 00:11:12,300
storage engine to do and nothing else

00:11:09,690 --> 00:11:13,590
does it exactly so we're writing it from

00:11:12,300 --> 00:11:16,050
scratch

00:11:13,590 --> 00:11:18,690
so the first decision to make is do we

00:11:16,050 --> 00:11:21,360
want a b-tree or in the you know B+ tree

00:11:18,690 --> 00:11:23,460
of course or an LS M so a B+ tree

00:11:21,360 --> 00:11:24,900
optimizes for read performance which is

00:11:23,460 --> 00:11:28,020
in line with the rest of FTB's

00:11:24,900 --> 00:11:30,240
architecture Ellison's do usually have

00:11:28,020 --> 00:11:32,250
fast point lookup using probabilistic

00:11:30,240 --> 00:11:36,390
hints like bloom filters or cuckoo

00:11:32,250 --> 00:11:38,550
hashes but range reeds are very common

00:11:36,390 --> 00:11:41,250
in FDB applications and probabilistic

00:11:38,550 --> 00:11:42,690
ants are less useful there but I

00:11:41,250 --> 00:11:46,080
understand there is research being done

00:11:42,690 --> 00:11:48,810
in that area a good example of this is

00:11:46,080 --> 00:11:50,850
non-unique indexes you'll have some

00:11:48,810 --> 00:11:52,800
index identifiers some value and then

00:11:50,850 --> 00:11:54,450
your primary keys are the last part of

00:11:52,800 --> 00:11:56,580
that key so you need to range scan your

00:11:54,450 --> 00:11:59,070
index name and value to do a lookup and

00:11:56,580 --> 00:12:02,370
that index and get all the primary keys

00:11:59,070 --> 00:12:05,400
of the relevant records so without

00:12:02,370 --> 00:12:07,620
native versioning like for example

00:12:05,400 --> 00:12:09,300
people often ask but rocks DB without

00:12:07,620 --> 00:12:11,040
native versioning support we would need

00:12:09,300 --> 00:12:14,460
to use something like the multi version

00:12:11,040 --> 00:12:18,150
layer on top of that which has the

00:12:14,460 --> 00:12:19,860
pitfalls discussed earlier so this isn't

00:12:18,150 --> 00:12:22,650
to say that Ellison storage engine is a

00:12:19,860 --> 00:12:24,810
bad idea it's just it's certainly not

00:12:22,650 --> 00:12:26,580
and it's a great idea for some workloads

00:12:24,810 --> 00:12:33,120
it's just not our focus right now based

00:12:26,580 --> 00:12:34,770
on what what our needs are so so this

00:12:33,120 --> 00:12:36,990
brings me to the Redwood storage engine

00:12:34,770 --> 00:12:39,660
so it's a version to be plus tree on top

00:12:36,990 --> 00:12:41,370
of a version of pager it persists

00:12:39,660 --> 00:12:43,440
version history but it mitigates the

00:12:41,370 --> 00:12:45,630
inefficiencies of the multi version

00:12:43,440 --> 00:12:46,800
layer design I'll talk more about this

00:12:45,630 --> 00:12:49,110
later so at this point I'm just kind of

00:12:46,800 --> 00:12:52,170
reading bullets to you and of course it

00:12:49,110 --> 00:12:53,970
has a key prefix compression which I

00:12:52,170 --> 00:12:58,260
mentioned earlier and I'm gonna talk

00:12:53,970 --> 00:13:01,320
about that in more detail later so a

00:12:58,260 --> 00:13:05,070
quick review of what a copy-on-write B+

00:13:01,320 --> 00:13:07,650
tree is so here we have a b-tree root

00:13:05,070 --> 00:13:09,400
node and and we're gonna show the child

00:13:07,650 --> 00:13:11,770
length of H

00:13:09,400 --> 00:13:13,810
points to page 7 so there's there's that

00:13:11,770 --> 00:13:15,910
child page and it's in our child page

00:13:13,810 --> 00:13:19,900
and so when you want to modify this

00:13:15,910 --> 00:13:23,050
structure you the the sequence is you

00:13:19,900 --> 00:13:26,350
first copy the page then you modify it

00:13:23,050 --> 00:13:30,340
so here we've we've added hi equals Z at

00:13:26,350 --> 00:13:31,690
the leaf level and we've copied page 11

00:13:30,340 --> 00:13:35,080
to page 25 before we made that

00:13:31,690 --> 00:13:38,950
modification so we've done this and now

00:13:35,080 --> 00:13:40,810
we need to make page 7.2 page 25 so we

00:13:38,950 --> 00:13:43,450
have to update the parents pointer which

00:13:40,810 --> 00:13:45,220
means we have to copy page 7 and and

00:13:43,450 --> 00:13:46,510
make the appropriate change and then we

00:13:45,220 --> 00:13:48,850
have to do it again all the way to the

00:13:46,510 --> 00:13:50,290
roots and we have a new root and the

00:13:48,850 --> 00:13:52,090
nice thing about this is we don't have

00:13:50,290 --> 00:13:53,980
to have a right ahead log because we not

00:13:52,090 --> 00:13:56,110
left our data structure in an

00:13:53,980 --> 00:13:59,740
inconsistent State on disk at any point

00:13:56,110 --> 00:14:02,200
the atomic like the point in time at

00:13:59,740 --> 00:14:04,030
which all of the new data is visible

00:14:02,200 --> 00:14:07,540
from the tree is that last step where we

00:14:04,030 --> 00:14:10,930
updated the root and so this is

00:14:07,540 --> 00:14:13,090
expensive for random writes because so

00:14:10,930 --> 00:14:15,610
if you have a branching factor of 200 to

00:14:13,090 --> 00:14:19,570
1 and you have 4 levels in your tree if

00:14:15,610 --> 00:14:22,060
you touch 200 random leaf pages you're

00:14:19,570 --> 00:14:24,040
likely going to have to touch 200 random

00:14:22,060 --> 00:14:26,290
parent of leaf pages to update those

00:14:24,040 --> 00:14:29,830
pointers because your third level of the

00:14:26,290 --> 00:14:32,410
tree is also larger than 200 pages and

00:14:29,830 --> 00:14:33,940
then probably also most of your second

00:14:32,410 --> 00:14:36,190
level will change to so it gets the

00:14:33,940 --> 00:14:39,820
problem so you get a lot of write

00:14:36,190 --> 00:14:41,550
amplification basically so we can limit

00:14:39,820 --> 00:14:45,100
the copy-on-write

00:14:41,550 --> 00:14:47,530
cost using indirection so here I have

00:14:45,100 --> 00:14:50,530
the same three nodes the same setup as

00:14:47,530 --> 00:14:53,620
before and on the right I'm going to

00:14:50,530 --> 00:14:56,080
show the same sequence using indirection

00:14:53,620 --> 00:14:59,590
with a page table so this page table

00:14:56,080 --> 00:15:03,220
Maps logical pages to physical pages and

00:14:59,590 --> 00:15:06,790
so now the same 7 11 and page numbers

00:15:03,220 --> 00:15:08,740
that are in my be tree nodes those are

00:15:06,790 --> 00:15:11,890
now logical pages instead of physical

00:15:08,740 --> 00:15:14,290
pages so whereas on the Left we copied

00:15:11,890 --> 00:15:17,560
page 11 to page 25 and modified it on

00:15:14,290 --> 00:15:20,290
the right we keep page 11 as page 11 we

00:15:17,560 --> 00:15:21,920
write it to a new place and we update

00:15:20,290 --> 00:15:24,319
the page table to say

00:15:21,920 --> 00:15:27,230
the new place with the new slot where

00:15:24,319 --> 00:15:28,790
page 11 is physically located whereas on

00:15:27,230 --> 00:15:32,239
the Left we still have to do the copy on

00:15:28,790 --> 00:15:35,540
a copy upwards to the root so redwood

00:15:32,239 --> 00:15:38,239
has a version 2 pager which is like that

00:15:35,540 --> 00:15:41,209
page table we just saw but it also has a

00:15:38,239 --> 00:15:44,119
version dimension all memory all entries

00:15:41,209 --> 00:15:45,649
of the of the page table are kept in

00:15:44,119 --> 00:15:48,589
memory at all times and the on disk

00:15:45,649 --> 00:15:50,899
format is is very much like in fact the

00:15:48,589 --> 00:15:52,970
prototype it's literally exactly the

00:15:50,899 --> 00:15:55,609
same thing as the FTB memory storage

00:15:52,970 --> 00:16:00,199
engine as a result recovery from disk is

00:15:55,609 --> 00:16:02,480
slow and so to avoid that the in memory

00:16:00,199 --> 00:16:06,319
state will be stored in a shared memory

00:16:02,480 --> 00:16:08,720
buffer that lives that that can survive

00:16:06,319 --> 00:16:10,189
graceful process exits and restarts and

00:16:08,720 --> 00:16:12,980
the new processors are just attached to

00:16:10,189 --> 00:16:16,040
it and use that structure but of course

00:16:12,980 --> 00:16:19,189
if you you know powered out and power

00:16:16,040 --> 00:16:20,749
off the machine rebooted etc you're

00:16:19,189 --> 00:16:26,299
gonna have to do a slow recovery from

00:16:20,749 --> 00:16:32,419
disk so the the sort of version B+ tree

00:16:26,299 --> 00:16:35,569
on top of this pager basically it

00:16:32,419 --> 00:16:37,429
consists of logical pages that are all

00:16:35,569 --> 00:16:39,319
red at the same version so there's many

00:16:37,429 --> 00:16:41,869
in a sense there's many versions of the

00:16:39,319 --> 00:16:43,519
B+ tree because if you start at the root

00:16:41,869 --> 00:16:45,169
and read it at some version and then

00:16:43,519 --> 00:16:48,919
read every page below it at the same

00:16:45,169 --> 00:16:53,629
version you essentially have a like

00:16:48,919 --> 00:16:56,629
unchanging version of the B+ tree like a

00:16:53,629 --> 00:16:58,790
snapshot of the entire B+ tree so within

00:16:56,629 --> 00:17:00,709
a version within a page you can have

00:16:58,790 --> 00:17:03,769
multiple versions of the same key but we

00:17:00,709 --> 00:17:07,519
control the amount of history to avoid

00:17:03,769 --> 00:17:08,720
that's that slow range read effect that

00:17:07,519 --> 00:17:10,339
we talked about earlier where as you

00:17:08,720 --> 00:17:13,370
accumulate older data you're getting

00:17:10,339 --> 00:17:16,089
less your weeds are less potent in terms

00:17:13,370 --> 00:17:18,829
of actual useful results you can return

00:17:16,089 --> 00:17:20,449
also it's notable that prefix

00:17:18,829 --> 00:17:23,419
compression makes this cheaper because

00:17:20,449 --> 00:17:26,750
if you have you know a at 5 a at an 8

00:17:23,419 --> 00:17:28,220
you know 15 like the a part you know it

00:17:26,750 --> 00:17:30,019
could be some long key it cancels out

00:17:28,220 --> 00:17:32,840
and even the first couple bites of the

00:17:30,019 --> 00:17:35,310
version could actually sorry not cancel

00:17:32,840 --> 00:17:39,000
out but prefix out

00:17:35,310 --> 00:17:40,140
so even our versions are as I mentioned

00:17:39,000 --> 00:17:44,730
earlier versions are actually pretty

00:17:40,140 --> 00:17:46,980
long there are these versions for the

00:17:44,730 --> 00:17:50,900
well for the purposes of this interface

00:17:46,980 --> 00:17:55,080
they're going to be 8 byte versions and

00:17:50,900 --> 00:17:57,450
it basically if you write a three times

00:17:55,080 --> 00:17:59,430
rapidly the first few bites of that

00:17:57,450 --> 00:18:02,640
version of probably unchanging and can

00:17:59,430 --> 00:18:03,810
be borrowed from the earlier node well

00:18:02,640 --> 00:18:07,500
I'll get an actually I'll get into

00:18:03,810 --> 00:18:11,610
prefix compression more later getting

00:18:07,500 --> 00:18:13,260
close to the end ok so another property

00:18:11,610 --> 00:18:16,380
of Redwood is a high branching factor

00:18:13,260 --> 00:18:18,300
which is which comes it was as a result

00:18:16,380 --> 00:18:24,090
of minimal using minimal boundary keys

00:18:18,300 --> 00:18:25,710
and having key prefix compression thanks

00:18:24,090 --> 00:18:29,370
so the last thing I'd like to talk about

00:18:25,710 --> 00:18:32,160
is prefix compression and a little bit

00:18:29,370 --> 00:18:34,500
about how Redwood does it so data models

00:18:32,160 --> 00:18:37,110
on FTB tend to use repeat prefixes I

00:18:34,500 --> 00:18:40,380
think I've said that before in last few

00:18:37,110 --> 00:18:42,300
minutes and so string and turning can

00:18:40,380 --> 00:18:44,220
reduce repeats but at a cost of

00:18:42,300 --> 00:18:46,530
additional reads and additional

00:18:44,220 --> 00:18:51,180
complexity in your application

00:18:46,530 --> 00:18:53,850
I'd like to so ideally you would like to

00:18:51,180 --> 00:18:55,770
just keep you it would be better to not

00:18:53,850 --> 00:18:58,250
have to do any any string replacements

00:18:55,770 --> 00:19:00,870
or enumerations and just you know

00:18:58,250 --> 00:19:05,700
construct your keys in a way that that

00:19:00,870 --> 00:19:07,890
is natural for your data so here we have

00:19:05,700 --> 00:19:10,290
one option with the JSON model if you

00:19:07,890 --> 00:19:14,940
can have some key equal to a value of an

00:19:10,290 --> 00:19:16,350
entire document or you can have what the

00:19:14,940 --> 00:19:19,020
document layer actually does which is a

00:19:16,350 --> 00:19:20,880
bunch of separate keys and values want

00:19:19,020 --> 00:19:22,260
one key value pair poor value in the

00:19:20,880 --> 00:19:28,400
document so you can see if there's a lot

00:19:22,260 --> 00:19:28,400
of repeat sequences there in those keys

00:19:32,180 --> 00:19:39,660
sorry ok so one way to to do prefix

00:19:38,190 --> 00:19:42,360
compression is linear so you basically

00:19:39,660 --> 00:19:44,720
serialize the sorted set of keys as a

00:19:42,360 --> 00:19:48,120
prefix length and then a suffix string

00:19:44,720 --> 00:19:49,020
and that gives you the minimal possible

00:19:48,120 --> 00:19:53,520
footprint

00:19:49,020 --> 00:19:58,400
um and so here on the right I'm gonna

00:19:53,520 --> 00:20:02,280
add some links that show basically each

00:19:58,400 --> 00:20:03,900
where each record borrows its prefix

00:20:02,280 --> 00:20:07,650
bytes from so as you can see they're all

00:20:03,900 --> 00:20:09,210
just everything every record borrows

00:20:07,650 --> 00:20:13,310
from that's from the previous record and

00:20:09,210 --> 00:20:15,930
therefore to search this structure

00:20:13,310 --> 00:20:16,980
it's Lin it's a it's a linear search

00:20:15,930 --> 00:20:18,930
because you have to start at the first

00:20:16,980 --> 00:20:20,340
key and read you know all of them to get

00:20:18,930 --> 00:20:26,040
to the last one actually be able decode

00:20:20,340 --> 00:20:28,430
it so is there a better set of prefix or

00:20:26,040 --> 00:20:31,110
slinks could we could we could we change

00:20:28,430 --> 00:20:33,000
these borrowing these prefix borrowings

00:20:31,110 --> 00:20:35,070
borrowing relationships to get a better

00:20:33,000 --> 00:20:38,580
search time well it turns out that we

00:20:35,070 --> 00:20:40,760
start at the middle and work out there

00:20:38,580 --> 00:20:43,260
we go

00:20:40,760 --> 00:20:45,090
have some nodes borrow from the middle

00:20:43,260 --> 00:20:48,240
and then some other nodes borrow from

00:20:45,090 --> 00:20:51,840
those nodes we get a set of links that

00:20:48,240 --> 00:20:54,810
look like this and then if we redraw

00:20:51,840 --> 00:20:56,970
those in a different way this is the

00:20:54,810 --> 00:20:59,280
same links here so the dotted lines are

00:20:56,970 --> 00:21:00,900
showing the prefix source the prefix

00:20:59,280 --> 00:21:02,190
borrowing source and the straight lines

00:21:00,900 --> 00:21:05,670
are the solid lines of the child

00:21:02,190 --> 00:21:07,110
pointers we get a binary tree notably

00:21:05,670 --> 00:21:09,690
everything I've added to the binary tree

00:21:07,110 --> 00:21:12,300
so far borrows from its immediate parent

00:21:09,690 --> 00:21:14,120
this last one actually borrows from the

00:21:12,300 --> 00:21:17,550
root it skips over its immediate parent

00:21:14,120 --> 00:21:21,600
it turns out and I don't have time to go

00:21:17,550 --> 00:21:24,270
into deriving this or but it turns out

00:21:21,600 --> 00:21:28,740
that for any Bani binary tree whether

00:21:24,270 --> 00:21:31,440
it's balanced or not there is one ideal

00:21:28,740 --> 00:21:33,180
prefix source in your ancestry and you

00:21:31,440 --> 00:21:34,860
can describe it with just one bit and

00:21:33,180 --> 00:21:36,420
that one bit is telling you whether

00:21:34,860 --> 00:21:38,160
you're whether you're borrowing from

00:21:36,420 --> 00:21:41,940
your previous ancestor or your next

00:21:38,160 --> 00:21:44,580
ancestor previous means the ancestor on

00:21:41,940 --> 00:21:46,260
your left that is greatest and next

00:21:44,580 --> 00:21:50,850
means the ancestor on your right that is

00:21:46,260 --> 00:21:53,490
least this results in perfect prefix

00:21:50,850 --> 00:21:56,790
compression and Lanier's and log and

00:21:53,490 --> 00:21:58,380
search time without they'll be the old

00:21:56,790 --> 00:22:00,330
additional overhead and this be hot

00:21:58,380 --> 00:22:02,010
besides a binary tree is your prefix

00:22:00,330 --> 00:22:04,830
length and that extra bit

00:22:02,010 --> 00:22:08,520
so why does this matter it's not because

00:22:04,830 --> 00:22:10,919
of space non-perfect prefix compression

00:22:08,520 --> 00:22:14,549
gives you pretty good it gives you a

00:22:10,919 --> 00:22:17,250
pretty good space-saving it's about

00:22:14,549 --> 00:22:19,980
predictability because now we can we can

00:22:17,250 --> 00:22:21,990
very quickly answer the question will

00:22:19,980 --> 00:22:26,280
this set of keys that I want to add to

00:22:21,990 --> 00:22:28,830
this compress page fit once I you know

00:22:26,280 --> 00:22:30,780
fit fit in the page with it which has a

00:22:28,830 --> 00:22:35,100
fixed size you know limit of say four K

00:22:30,780 --> 00:22:37,679
at once they are compressed so that ends

00:22:35,100 --> 00:22:39,240
up being really useful for meet if we're

00:22:37,679 --> 00:22:40,919
adding to a page it's also really useful

00:22:39,240 --> 00:22:42,540
when you have a bunch of sort of data

00:22:40,919 --> 00:22:44,400
you want to bulk build pages you can

00:22:42,540 --> 00:22:48,120
scan through it linearly and you know

00:22:44,400 --> 00:22:51,690
exactly where based on just comparing

00:22:48,120 --> 00:22:55,860
adjacent prefixes you know exactly where

00:22:51,690 --> 00:22:59,790
you can stop your scan and then build a

00:22:55,860 --> 00:23:01,760
binary tree because you know exactly the

00:22:59,790 --> 00:23:06,419
point where the the compressed form

00:23:01,760 --> 00:23:08,460
would overfill your page it turns out

00:23:06,419 --> 00:23:12,960
that this also works at the B+ tree

00:23:08,460 --> 00:23:15,960
level so here I have two pages and two

00:23:12,960 --> 00:23:18,480
beats to be plus three pages each with a

00:23:15,960 --> 00:23:20,130
small binary tree inside it and if

00:23:18,480 --> 00:23:23,610
you'll notice in page seven at the

00:23:20,130 --> 00:23:25,320
bottom the left side and the right side

00:23:23,610 --> 00:23:27,390
and the root of the binary tree don't

00:23:25,320 --> 00:23:30,390
actually have a previous ancestor or a

00:23:27,390 --> 00:23:33,090
next ancestor in the binary tree but it

00:23:30,390 --> 00:23:36,590
turns out that if you substitute the the

00:23:33,090 --> 00:23:39,150
previous key boundary from the parent

00:23:36,590 --> 00:23:43,140
for the parent page or the next key

00:23:39,150 --> 00:23:45,330
boundary it it actually it well

00:23:43,140 --> 00:23:47,400
basically you get you get exactly the

00:23:45,330 --> 00:23:49,500
same effect so if you take a bunch of

00:23:47,400 --> 00:23:52,770
data that's sorted and you build a whole

00:23:49,500 --> 00:23:57,179
bunch of B tree pages out of it using

00:23:52,770 --> 00:23:59,640
this this this borrowing bit you know

00:23:57,179 --> 00:24:02,630
you using the same single bit precision

00:23:59,640 --> 00:24:06,500
you know borrowing sorry prefix or

00:24:02,630 --> 00:24:08,760
specifier that was terrible then you get

00:24:06,500 --> 00:24:11,580
perfect prefix compression for that tree

00:24:08,760 --> 00:24:13,559
now note that the the tree as a whole is

00:24:11,580 --> 00:24:15,429
not always going to have perfect prefix

00:24:13,559 --> 00:24:20,889
compression because

00:24:15,429 --> 00:24:21,730
as you know sorry I thought I put that

00:24:20,889 --> 00:24:24,700
here

00:24:21,730 --> 00:24:27,460
so deletions can cause parent pages that

00:24:24,700 --> 00:24:30,029
originally had ideal ideal boundaries

00:24:27,460 --> 00:24:33,460
that the child notes were borrowing from

00:24:30,029 --> 00:24:35,470
to basically have bytes that the child

00:24:33,460 --> 00:24:38,350
pages are no longer borrowing because

00:24:35,470 --> 00:24:40,840
the because the items that were that

00:24:38,350 --> 00:24:43,299
we're borrowing the parents bytes have

00:24:40,840 --> 00:24:45,940
been deleted so it'll drift over time

00:24:43,299 --> 00:24:47,350
but it's again that's not that main

00:24:45,940 --> 00:24:51,009
purpose but it's still pretty good

00:24:47,350 --> 00:24:52,240
compression so that's that's all I had

00:24:51,009 --> 00:24:54,999
I'm sorry I did run a little bit over

00:24:52,240 --> 00:24:58,240
I'm in conclusion redwood is gonna bring

00:24:54,999 --> 00:24:59,799
longer read-only transactions it'll be

00:24:58,240 --> 00:25:02,740
faster for reading and writing and it'll

00:24:59,799 --> 00:25:04,210
be smaller on disk it is on master bran

00:25:02,740 --> 00:25:07,389
the master branch right now it's a work

00:25:04,210 --> 00:25:09,059
in progress it's far from finished a lot

00:25:07,389 --> 00:25:11,590
of the work done so far has been

00:25:09,059 --> 00:25:13,929
experimenting it has been very

00:25:11,590 --> 00:25:16,149
experimental and just kind of exploring

00:25:13,929 --> 00:25:18,429
design choices there's a lot of check

00:25:16,149 --> 00:25:19,899
ins that completely replace the results

00:25:18,429 --> 00:25:24,730
of whether it's the content of other

00:25:19,899 --> 00:25:26,460
check-ins but it's work work is in

00:25:24,730 --> 00:25:29,140
progress it's coming along pretty well

00:25:26,460 --> 00:25:35,049
that's that's all I had

00:25:29,140 --> 00:25:35,049

YouTube URL: https://www.youtube.com/watch?v=nlus1Z7TVTI


