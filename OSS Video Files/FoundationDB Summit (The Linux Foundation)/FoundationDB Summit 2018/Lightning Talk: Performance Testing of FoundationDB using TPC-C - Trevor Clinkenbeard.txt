Title: Lightning Talk: Performance Testing of FoundationDB using TPC-C - Trevor Clinkenbeard
Publication date: 2018-12-14
Playlist: FoundationDB Summit 2018
Description: 
	Lightning Talk: Performance Testing of FoundationDB using TPC-C - Trevor Clinkenbeard, Snowflake ComputingÂ 

This talk discusses the performance testing of FoundationDB done at Snowflake Computing. This includes discussion of a TPC-C workload that has been used to discover performance regressions and to test the performance effects of new features, rolling upgrades, and various cluster configurations.
Captions: 
	00:00:00,030 --> 00:00:02,939
we're from snowflake computing and today

00:00:01,829 --> 00:00:04,560
I'm going to be talking about some of

00:00:02,939 --> 00:00:06,150
our performance testing infrastructure

00:00:04,560 --> 00:00:09,330
at snowflake with an emphasis on our use

00:00:06,150 --> 00:00:12,389
of the TPCC workload so what is TPCC

00:00:09,330 --> 00:00:14,219
TPCC is an industry standard ltp

00:00:12,389 --> 00:00:16,710
benchmark used to simulate a realistic

00:00:14,219 --> 00:00:18,630
workload of a retail business it

00:00:16,710 --> 00:00:21,150
consists of 5 diverse transaction types

00:00:18,630 --> 00:00:22,560
that have somewhat read only others are

00:00:21,150 --> 00:00:24,510
readwrite they also vary in terms of

00:00:22,560 --> 00:00:25,920
complexity and duration there's also an

00:00:24,510 --> 00:00:27,599
approximately two to one read/write

00:00:25,920 --> 00:00:29,730
ratio which is a similar to what we

00:00:27,599 --> 00:00:31,199
experienced at snowflake so that's why

00:00:29,730 --> 00:00:33,930
we've relied so heavily or part of the

00:00:31,199 --> 00:00:36,059
reason we've relied so heavily on this

00:00:33,930 --> 00:00:39,360
workload for our performance testing as

00:00:36,059 --> 00:00:41,309
you can see from the diagram a TPCC

00:00:39,360 --> 00:00:43,559
database consists of a configurable

00:00:41,309 --> 00:00:45,300
number of warehouses and both the size

00:00:43,559 --> 00:00:47,340
of the database and the number of

00:00:45,300 --> 00:00:49,980
concurrent clients scale in proportion

00:00:47,340 --> 00:00:51,690
to the number of warehouses chosen so

00:00:49,980 --> 00:00:53,399
how do we use TPCC

00:00:51,690 --> 00:00:54,780
we use it to analyze performance in a

00:00:53,399 --> 00:00:57,210
number of different ways we have

00:00:54,780 --> 00:00:58,770
periodic jenkins not just runs to search

00:00:57,210 --> 00:01:01,320
for unexpected performance regressions

00:00:58,770 --> 00:01:03,870
in our master branch we also test the

00:01:01,320 --> 00:01:05,460
performance effects of enabling new

00:01:03,870 --> 00:01:07,170
features such as our new serialization

00:01:05,460 --> 00:01:09,150
protocol which andrew will be discussing

00:01:07,170 --> 00:01:10,950
next and also changing different

00:01:09,150 --> 00:01:13,020
parameters for example using different

00:01:10,950 --> 00:01:15,119
asynchronous i/o libraries comparing

00:01:13,020 --> 00:01:17,100
different page cache sizes we also use

00:01:15,119 --> 00:01:19,619
it as a baseline workload to ensure that

00:01:17,100 --> 00:01:21,360
foundation DB remains available in

00:01:19,619 --> 00:01:24,150
performance while we perform snapshot

00:01:21,360 --> 00:01:26,820
backups and rolling upgrades and we test

00:01:24,150 --> 00:01:28,110
a variety of cluster configurations with

00:01:26,820 --> 00:01:30,750
different hardware and different

00:01:28,110 --> 00:01:33,090
topologies so what does a typical test

00:01:30,750 --> 00:01:36,780
setup look like for us most of our tests

00:01:33,090 --> 00:01:39,000
run on a 5 node cluster of AWS 360 next

00:01:36,780 --> 00:01:41,100
large machines we always ensure at least

00:01:39,000 --> 00:01:42,990
one virtual CPU per process and

00:01:41,100 --> 00:01:46,439
typically run with triple triple

00:01:42,990 --> 00:01:48,840
replication and an SSD engine and too

00:01:46,439 --> 00:01:50,700
close to resemble what snowflake uses we

00:01:48,840 --> 00:01:53,250
run most tests with three resolvers

00:01:50,700 --> 00:01:55,470
three proxies and five transaction logs

00:01:53,250 --> 00:01:57,000
and a large tester pool on separate

00:01:55,470 --> 00:01:59,939
machines to ensure we're not constrained

00:01:57,000 --> 00:02:01,649
by testing resources so now I want to

00:01:59,939 --> 00:02:03,270
just briefly talk about one example of

00:02:01,649 --> 00:02:06,450
an interesting finding that we had well

00:02:03,270 --> 00:02:07,729
running these tests so we used to

00:02:06,450 --> 00:02:09,629
operate under the assumption that

00:02:07,729 --> 00:02:10,950
scaling the number of storage servers

00:02:09,629 --> 00:02:12,720
would always result in increased

00:02:10,950 --> 00:02:14,040
performance but as you can see that's

00:02:12,720 --> 00:02:16,920
not always been the case

00:02:14,040 --> 00:02:20,120
in TPCC throughput is measured in terms

00:02:16,920 --> 00:02:22,860
of transactions permitted committed and

00:02:20,120 --> 00:02:24,510
the number of clients scales linearly

00:02:22,860 --> 00:02:26,400
with the number of warehouses used

00:02:24,510 --> 00:02:29,459
so ideally one would expect linear

00:02:26,400 --> 00:02:32,099
scaling of throughput and that's what we

00:02:29,459 --> 00:02:33,450
saw here with 32 storage servers or 64

00:02:32,099 --> 00:02:36,330
but in this particular configuration

00:02:33,450 --> 00:02:38,370
when we went to 128 we started seeing

00:02:36,330 --> 00:02:39,599
decreased throughput throughput and that

00:02:38,370 --> 00:02:41,910
was really surprising even though we

00:02:39,599 --> 00:02:44,129
replicated this many times and what we

00:02:41,910 --> 00:02:45,390
found is for reasons that we still don't

00:02:44,129 --> 00:02:47,879
entirely understand and are still

00:02:45,390 --> 00:02:49,500
investigating we saw an increase in real

00:02:47,879 --> 00:02:53,010
latency as we scale about the number of

00:02:49,500 --> 00:02:54,690
warehouses and the number of storage

00:02:53,010 --> 00:02:56,220
servers and this causes longer

00:02:54,690 --> 00:02:57,540
transactions and with the way foundation

00:02:56,220 --> 00:03:00,450
to be handles optimistic concurrency

00:02:57,540 --> 00:03:03,180
control larger transactions result in a

00:03:00,450 --> 00:03:05,489
higher conflict probability and this

00:03:03,180 --> 00:03:07,440
higher conflict rate resulted in the

00:03:05,489 --> 00:03:09,030
decrease in throughput so this is by no

00:03:07,440 --> 00:03:10,980
means an official benchmark or anything

00:03:09,030 --> 00:03:12,209
we can scale beyond this but it's just

00:03:10,980 --> 00:03:13,769
an example of some problems that we

00:03:12,209 --> 00:03:16,650
identified with one particular cluster

00:03:13,769 --> 00:03:18,269
configuration and we were lucky to not

00:03:16,650 --> 00:03:20,250
be surprised by production workload and

00:03:18,269 --> 00:03:23,459
be able to debug this in our test

00:03:20,250 --> 00:03:25,859
environment in addition to our work with

00:03:23,459 --> 00:03:29,389
DP CC we've also done a lot of work

00:03:25,859 --> 00:03:31,739
scaling out how many processes can we

00:03:29,389 --> 00:03:34,230
can we handle in a single cluster and

00:03:31,739 --> 00:03:36,750
often we found that the bottleneck there

00:03:34,230 --> 00:03:38,430
was the cluster controller CPU so a

00:03:36,750 --> 00:03:40,230
quick refresher the cluster controller

00:03:38,430 --> 00:03:42,359
is a process whose responsibilities

00:03:40,230 --> 00:03:44,160
include handling client connections

00:03:42,359 --> 00:03:45,900
running a failure monitor and handling

00:03:44,160 --> 00:03:48,599
status requests and this is largely

00:03:45,900 --> 00:03:50,130
workload independent also cluster

00:03:48,599 --> 00:03:51,389
controller CPU will increase with the

00:03:50,130 --> 00:03:53,940
total number of connections and the

00:03:51,389 --> 00:03:55,500
frequency of status requests and we see

00:03:53,940 --> 00:03:57,540
here that even in the absence of any

00:03:55,500 --> 00:04:00,180
workload as you scale out the number of

00:03:57,540 --> 00:04:03,180
clients and the number of servers used

00:04:00,180 --> 00:04:04,560
you see a steady increase in CPU and

00:04:03,180 --> 00:04:08,329
this can be a problem because the

00:04:04,560 --> 00:04:11,099
cluster controller is single threaded so

00:04:08,329 --> 00:04:13,440
these tests and observations led to a

00:04:11,099 --> 00:04:15,329
series of optimizations to help out with

00:04:13,440 --> 00:04:17,609
making the cluster controller more

00:04:15,329 --> 00:04:19,440
efficient with these optimizations we

00:04:17,609 --> 00:04:22,049
were handle able to handle an additional

00:04:19,440 --> 00:04:25,080
150 percent more status requests per

00:04:22,049 --> 00:04:27,090
second these optimizations included pre

00:04:25,080 --> 00:04:27,720
serialization of status of status

00:04:27,090 --> 00:04:29,550
objects

00:04:27,720 --> 00:04:34,050
and some low-level optimizations to the

00:04:29,550 --> 00:04:35,430
float sq we also cherry-pick to change

00:04:34,050 --> 00:04:37,140
from the open-source version to allow

00:04:35,430 --> 00:04:38,940
processes to run with a cluster

00:04:37,140 --> 00:04:40,650
controller class and when you run a

00:04:38,940 --> 00:04:42,510
process with the cluster controller

00:04:40,650 --> 00:04:46,050
class then you're ensuring that under

00:04:42,510 --> 00:04:47,460
ideal circumstances that process will be

00:04:46,050 --> 00:04:49,470
elected cluster controller but won't

00:04:47,460 --> 00:04:51,990
have any other responsibilities so if

00:04:49,470 --> 00:04:53,520
you run without this class you run the

00:04:51,990 --> 00:04:55,140
risk that for example a hot storage

00:04:53,520 --> 00:04:57,000
server could also be elected cluster

00:04:55,140 --> 00:04:58,680
controller and that's be overwhelmed

00:04:57,000 --> 00:05:00,500
start missing heartbeats and eventually

00:04:58,680 --> 00:05:03,090
that can cause reelection zin downtime

00:05:00,500 --> 00:05:04,590
we also improved the way we do

00:05:03,090 --> 00:05:06,420
throttling so we don't have to make as

00:05:04,590 --> 00:05:09,240
many status requests to the cluster

00:05:06,420 --> 00:05:10,680
controller so some of the metrics that

00:05:09,240 --> 00:05:13,230
Ashish was talking about you can get

00:05:10,680 --> 00:05:15,240
those with a status request but if your

00:05:13,230 --> 00:05:17,130
cluster controller is under duress you

00:05:15,240 --> 00:05:18,840
might not want to do that so we pushed a

00:05:17,130 --> 00:05:20,880
lot of those metrics to the proxies and

00:05:18,840 --> 00:05:24,150
that we can get those cheaply for

00:05:20,880 --> 00:05:26,280
efficient client-side throttling so

00:05:24,150 --> 00:05:29,310
where do we plan to go from here we like

00:05:26,280 --> 00:05:31,410
to continue scaling out our TPCC tests

00:05:29,310 --> 00:05:33,600
to more closely resemble a production

00:05:31,410 --> 00:05:35,400
workload we also hope that now that our

00:05:33,600 --> 00:05:37,980
implementation has been open sourced and

00:05:35,400 --> 00:05:41,000
I actually had the link on a previous

00:05:37,980 --> 00:05:43,710
slide where we open sourced our

00:05:41,000 --> 00:05:45,720
implementation written in flow and we

00:05:43,710 --> 00:05:48,780
hope that other companies can run

00:05:45,720 --> 00:05:49,890
similar tests on their own environments

00:05:48,780 --> 00:05:51,840
because there's a lot of different

00:05:49,890 --> 00:05:53,520
parameters you can change we also didn't

00:05:51,840 --> 00:05:54,690
do any official benchmarking but we hope

00:05:53,520 --> 00:05:56,430
that this robust open-source

00:05:54,690 --> 00:05:58,530
implementation can lead the way for

00:05:56,430 --> 00:06:01,490
doing that in the future also another

00:05:58,530 --> 00:06:05,940
thing to consider we didn't do TPC II

00:06:01,490 --> 00:06:07,979
make a TPC II workload but for other if

00:06:05,940 --> 00:06:10,050
you have for example a higher readwrite

00:06:07,979 --> 00:06:13,260
ratio or a more random workload that's

00:06:10,050 --> 00:06:14,940
another industry standard benchmark that

00:06:13,260 --> 00:06:17,370
could be used for Simmons similar

00:06:14,940 --> 00:06:23,089
purposes thank you

00:06:17,370 --> 00:06:23,089

YouTube URL: https://www.youtube.com/watch?v=J5dvTdEBuTg


