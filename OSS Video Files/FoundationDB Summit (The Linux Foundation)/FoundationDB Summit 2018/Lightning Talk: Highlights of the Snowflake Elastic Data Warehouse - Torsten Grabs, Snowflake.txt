Title: Lightning Talk: Highlights of the Snowflake Elastic Data Warehouse - Torsten Grabs, Snowflake
Publication date: 2018-12-14
Playlist: FoundationDB Summit 2018
Description: 
	Snowflake is an analytics database provided as a cloud service. The Snowflake data warehouse uses a new SQL database engine that built from scratch and was designed for the cloud. In this talk Sameet will describe the design of Snowflake's novel multi-cluster, shared-data cloud architecture. The talk will highlight some of the key features of Snowflakelike extreme and instant elasticity, support for semi-structured and schema-less data, and data sharing. The talk will also present upcoming features.
Captions: 
	00:00:00,030 --> 00:00:06,150
well my name is Torsten I'm one of the

00:00:01,410 --> 00:00:08,160
product managers at snowflake and so

00:00:06,150 --> 00:00:10,290
this is a sponsor talk so I'm by no

00:00:08,160 --> 00:00:12,090
means an FTP expert but I do want to

00:00:10,290 --> 00:00:13,410
walk you through some of the highlights

00:00:12,090 --> 00:00:17,340
of snowflake which as you know is

00:00:13,410 --> 00:00:19,560
heavily using FDB and you might have

00:00:17,340 --> 00:00:21,150
heard this before during the day so one

00:00:19,560 --> 00:00:23,100
of the foundations for snowflake is what

00:00:21,150 --> 00:00:25,949
we call a multi cluster shared data

00:00:23,100 --> 00:00:27,449
architecture what it means is that we're

00:00:25,949 --> 00:00:29,730
very disciplined about separating

00:00:27,449 --> 00:00:31,470
compute from storage and that gives us a

00:00:29,730 --> 00:00:33,540
number of interesting capabilities so

00:00:31,470 --> 00:00:35,940
the first one is everybody that's

00:00:33,540 --> 00:00:38,340
working on the same snowflake account

00:00:35,940 --> 00:00:40,530
it's working on the same data set so

00:00:38,340 --> 00:00:42,899
they're all seeing the same

00:00:40,530 --> 00:00:45,809
transactionally consistent view of the

00:00:42,899 --> 00:00:48,270
data no matter which team or which part

00:00:45,809 --> 00:00:52,260
of the company is making changes to the

00:00:48,270 --> 00:00:54,420
to the data set in addition to that you

00:00:52,260 --> 00:00:56,430
if you have different teams or different

00:00:54,420 --> 00:00:58,890
departments in your company you can

00:00:56,430 --> 00:01:01,559
define and create independent compute

00:00:58,890 --> 00:01:03,539
resources that are powering your team

00:01:01,559 --> 00:01:05,689
and they then get access to the shared

00:01:03,539 --> 00:01:08,640
data set and they're independent of

00:01:05,689 --> 00:01:10,710
whatever other teams are doing over the

00:01:08,640 --> 00:01:12,479
same data this has a great benefit that

00:01:10,710 --> 00:01:13,890
you can do workload separation so for

00:01:12,479 --> 00:01:16,229
instance if you look into this busy

00:01:13,890 --> 00:01:18,150
picture here at the top you can see

00:01:16,229 --> 00:01:20,070
there is data loading going on ETL

00:01:18,150 --> 00:01:21,060
workloads are happening and in the

00:01:20,070 --> 00:01:23,880
bottom you have for instance

00:01:21,060 --> 00:01:26,700
dashboarding power users they're using

00:01:23,880 --> 00:01:28,979
for instance to blow to do bi whatever

00:01:26,700 --> 00:01:32,250
happens on the data loading side gets

00:01:28,979 --> 00:01:34,079
its own dedicated compute resources you

00:01:32,250 --> 00:01:36,060
see this little sequel cluster that's

00:01:34,079 --> 00:01:38,100
sitting there so that's the independent

00:01:36,060 --> 00:01:41,100
compute for data loading and then at the

00:01:38,100 --> 00:01:43,020
bottom you have different compute

00:01:41,100 --> 00:01:45,079
clusters that are powering the BI

00:01:43,020 --> 00:01:47,790
experience for users that are using

00:01:45,079 --> 00:01:50,220
tableau so you have this workload

00:01:47,790 --> 00:01:51,829
separation the other aspect is you can

00:01:50,220 --> 00:01:54,060
also scale these compute resources

00:01:51,829 --> 00:01:56,250
independently of each other so for

00:01:54,060 --> 00:01:59,310
instance if your data science team needs

00:01:56,250 --> 00:02:00,750
massive scale out compute capacity you

00:01:59,310 --> 00:02:03,180
can have that for them in a dedicated

00:02:00,750 --> 00:02:05,070
computer cluster while other teams are

00:02:03,180 --> 00:02:07,429
working with much smaller clusters and

00:02:05,070 --> 00:02:09,989
then last point what I mentioned here is

00:02:07,429 --> 00:02:13,410
you can also give the same team multiple

00:02:09,989 --> 00:02:15,380
clusters and that essentially gives you

00:02:13,410 --> 00:02:17,550
the ability to get virtually unlimited

00:02:15,380 --> 00:02:19,260
concurrency so unlimited numbers of

00:02:17,550 --> 00:02:23,100
users or queries and running at the same

00:02:19,260 --> 00:02:24,900
time over the same data set so let's

00:02:23,100 --> 00:02:26,670
drill a little bit into how we are doing

00:02:24,900 --> 00:02:29,460
this so this is an architectural

00:02:26,670 --> 00:02:32,760
blueprint of the different layers that a

00:02:29,460 --> 00:02:35,370
snowflake is is using at the top you see

00:02:32,760 --> 00:02:37,110
what we call the cloud services and

00:02:35,370 --> 00:02:38,550
essentially they are first of all making

00:02:37,110 --> 00:02:40,920
sure that when a connection comes in

00:02:38,550 --> 00:02:43,800
that you're authorized and authenticated

00:02:40,920 --> 00:02:45,120
to work with snowflake and then as the

00:02:43,800 --> 00:02:48,270
query goes through the different layers

00:02:45,120 --> 00:02:49,940
we are compiling a query plan we're

00:02:48,270 --> 00:02:52,560
taking care of transaction management

00:02:49,940 --> 00:02:54,840
and all of that is backed by metadata

00:02:52,560 --> 00:02:58,380
that is stored for all intents and

00:02:54,840 --> 00:03:00,360
purposes in an F dB so once a query gets

00:02:58,380 --> 00:03:02,790
submitted for execution then it goes

00:03:00,360 --> 00:03:04,980
into the second layer that you see here

00:03:02,790 --> 00:03:07,230
this is where we have our so-called

00:03:04,980 --> 00:03:09,360
virtual warehouses those are these are

00:03:07,230 --> 00:03:11,550
independent compute clusters that you

00:03:09,360 --> 00:03:13,080
saw on the previous slide and they're

00:03:11,550 --> 00:03:15,690
connecting to the third layer at the

00:03:13,080 --> 00:03:18,450
bottom which is the data storage where

00:03:15,690 --> 00:03:20,160
snowflake databases live what's nice

00:03:18,450 --> 00:03:22,410
about this is the tiered storage

00:03:20,160 --> 00:03:25,620
architecture architecture between level

00:03:22,410 --> 00:03:28,020
2 and level 3 so we're using the local

00:03:25,620 --> 00:03:29,610
memory and the local disks on the VMs

00:03:28,020 --> 00:03:32,010
that are powering this this middle layer

00:03:29,610 --> 00:03:34,170
the virtual warehouses we're using that

00:03:32,010 --> 00:03:37,070
to cache any data that you are

00:03:34,170 --> 00:03:39,840
frequently used for for for your queries

00:03:37,070 --> 00:03:42,750
and that that saves a lot of round trips

00:03:39,840 --> 00:03:46,440
into remote storage for for the third

00:03:42,750 --> 00:03:48,570
layer alright looking at this storage

00:03:46,440 --> 00:03:51,000
this might be a term that you also have

00:03:48,570 --> 00:03:53,400
heard earlier today the way that

00:03:51,000 --> 00:03:56,880
snowflake organized it's it's it's data

00:03:53,400 --> 00:03:58,290
internally is into so-called micro

00:03:56,880 --> 00:04:00,060
partitions and this is done

00:03:58,290 --> 00:04:02,700
automatically as the data is being

00:04:00,060 --> 00:04:05,130
loaded into snowflake we're taking the

00:04:02,700 --> 00:04:06,300
data and we're cutting it into micro

00:04:05,130 --> 00:04:08,430
partitions that are a couple of

00:04:06,300 --> 00:04:10,950
megabytes maybe a few tens of megabytes

00:04:08,430 --> 00:04:12,990
in size what we're doing as part of that

00:04:10,950 --> 00:04:15,420
is we're transforming the incoming data

00:04:12,990 --> 00:04:17,220
into a columnar representation and we're

00:04:15,420 --> 00:04:18,840
also building up metadata structures

00:04:17,220 --> 00:04:21,060
that allow us to reason about what's

00:04:18,840 --> 00:04:23,340
contained in what partition this gives

00:04:21,060 --> 00:04:25,530
us the ability to do pruning during

00:04:23,340 --> 00:04:27,090
query processing at various levels so

00:04:25,530 --> 00:04:30,570
first of all because of the columnar

00:04:27,090 --> 00:04:32,220
presentation we can discard any any any

00:04:30,570 --> 00:04:33,600
columns that are not participating in

00:04:32,220 --> 00:04:35,580
the query we don't need to touch those

00:04:33,600 --> 00:04:37,229
and then in addition to that the

00:04:35,580 --> 00:04:39,780
metadata gives us the ability to reason

00:04:37,229 --> 00:04:41,669
about which which micro partitions may

00:04:39,780 --> 00:04:44,310
actually contribute results to the query

00:04:41,669 --> 00:04:45,540
that you've just submitted and for those

00:04:44,310 --> 00:04:47,070
where we are sure that they're not

00:04:45,540 --> 00:04:50,490
contributing we don't need to touch

00:04:47,070 --> 00:04:53,940
those partitions either so that gives

00:04:50,490 --> 00:04:55,889
you great performance benefits obviously

00:04:53,940 --> 00:04:58,620
and these partitions are also the unit

00:04:55,889 --> 00:05:00,630
for our DML operations so when you're

00:04:58,620 --> 00:05:02,700
running insert operations we are not

00:05:00,630 --> 00:05:04,080
changing existing micro partitions who

00:05:02,700 --> 00:05:08,790
are typically just adding new micro

00:05:04,080 --> 00:05:10,940
partitions to the end now the one of the

00:05:08,790 --> 00:05:13,050
cool pieces here is that's true for both

00:05:10,940 --> 00:05:15,600
structure CH data as well as

00:05:13,050 --> 00:05:17,669
semi-structured data that you may may

00:05:15,600 --> 00:05:20,039
load into a snowflake no matter what

00:05:17,669 --> 00:05:22,530
you're using as the incoming data format

00:05:20,039 --> 00:05:24,870
we are converting the add which is

00:05:22,530 --> 00:05:27,630
forming that into this optimized storage

00:05:24,870 --> 00:05:30,449
that's based on micro partitions and

00:05:27,630 --> 00:05:32,850
there's there's no schema or no hints

00:05:30,449 --> 00:05:34,860
required to do that and as soon as you

00:05:32,850 --> 00:05:37,500
submit a query that will automatically

00:05:34,860 --> 00:05:38,940
benefit from from the optimized data

00:05:37,500 --> 00:05:40,530
structure so you're getting the pruning

00:05:38,940 --> 00:05:44,250
and the filtering that we just talked

00:05:40,530 --> 00:05:47,910
about so that's a that's a bigger topic

00:05:44,250 --> 00:05:51,539
for us so we'd like to make things as

00:05:47,910 --> 00:05:54,030
automatic as possible for our users

00:05:51,539 --> 00:05:56,010
we talked about micro partitions there

00:05:54,030 --> 00:05:58,560
is no distribution key that you have to

00:05:56,010 --> 00:05:58,889
define we are doing it automatically for

00:05:58,560 --> 00:06:00,419
you

00:05:58,889 --> 00:06:02,460
you don't have to worry about like a

00:06:00,419 --> 00:06:04,440
high availability configuration failover

00:06:02,460 --> 00:06:06,450
between different virtual machines all

00:06:04,440 --> 00:06:08,310
that comes automatically out-of-the-box

00:06:06,450 --> 00:06:10,770
there is no vacuum you don't have to

00:06:08,310 --> 00:06:12,960
define specific statistics to make sure

00:06:10,770 --> 00:06:14,760
that your queries run fast so although

00:06:12,960 --> 00:06:18,990
it comes out-of-the-box virtually no

00:06:14,760 --> 00:06:22,320
knobs and the time to choose the great

00:06:18,990 --> 00:06:25,740
performance for your workload is super

00:06:22,320 --> 00:06:27,810
small so let me jump over into quick

00:06:25,740 --> 00:06:32,250
demo so that we get a brief look of

00:06:27,810 --> 00:06:34,650
snowflake in action and what I would

00:06:32,250 --> 00:06:38,580
like to do in the demo is just show you

00:06:34,650 --> 00:06:40,770
one particular aspect where snowflake

00:06:38,580 --> 00:06:42,840
integrates into the spark ecosystem

00:06:40,770 --> 00:06:45,150
so we have a spark connector that you

00:06:42,840 --> 00:06:48,660
can deploy into your spark application

00:06:45,150 --> 00:06:51,509
that injects itself into the query a

00:06:48,660 --> 00:06:53,639
planned generation process of spark to

00:06:51,509 --> 00:06:55,289
give users a highly optimized experience

00:06:53,639 --> 00:06:57,180
when they're storing their data in

00:06:55,289 --> 00:06:59,400
snowflake so what I've done here

00:06:57,180 --> 00:07:01,340
essentially I've taken about two

00:06:59,400 --> 00:07:04,139
gigabytes worth of tweets from Twitter

00:07:01,340 --> 00:07:09,289
stored them into s3 and you can see

00:07:04,139 --> 00:07:17,699
those files here and let me jump over

00:07:09,289 --> 00:07:19,830
here I have browser window with some

00:07:17,699 --> 00:07:22,650
queries over that data in data bricks

00:07:19,830 --> 00:07:25,590
you can see here I'm populating a data

00:07:22,650 --> 00:07:28,650
frame with the data that sits in the

00:07:25,590 --> 00:07:30,150
bucket in s3 over on the left and you

00:07:28,650 --> 00:07:33,060
can see this is the schema that we're

00:07:30,150 --> 00:07:35,940
getting it's all JSON lots of properties

00:07:33,060 --> 00:07:38,130
in here if we are doing account you can

00:07:35,940 --> 00:07:39,780
see that it's about 3.2 million

00:07:38,130 --> 00:07:41,400
different tweets that are contained in

00:07:39,780 --> 00:07:43,349
the data set that I have in the bucket

00:07:41,400 --> 00:07:45,630
and if you squint you can see that's

00:07:43,349 --> 00:07:48,030
down there it takes about half a minute

00:07:45,630 --> 00:07:50,069
to run the count operation and running

00:07:48,030 --> 00:07:52,199
this on a two notes bar cluster so to

00:07:50,069 --> 00:07:54,240
compute notes one master note pretty

00:07:52,199 --> 00:07:56,219
standard configuration are just clicking

00:07:54,240 --> 00:07:58,020
through the cluster setup and here's a

00:07:56,219 --> 00:08:00,300
little bit more involved query so over

00:07:58,020 --> 00:08:02,370
these these tweets I'm doing a group by

00:08:00,300 --> 00:08:04,279
the language property and then just

00:08:02,370 --> 00:08:06,840
counting to get the most popular

00:08:04,279 --> 00:08:08,819
languages for the different tweets and

00:08:06,840 --> 00:08:12,000
you can see English and Japanese are

00:08:08,819 --> 00:08:13,770
awfully popular here and again if you

00:08:12,000 --> 00:08:17,490
squint you see that takes about half a

00:08:13,770 --> 00:08:18,779
minute to to run so that's kind of

00:08:17,490 --> 00:08:21,360
interesting now let's take that same

00:08:18,779 --> 00:08:23,880
data set and bring it over into into

00:08:21,360 --> 00:08:25,620
snowflake and I've done that over here

00:08:23,880 --> 00:08:28,050
you see a tweets table that lives in a

00:08:25,620 --> 00:08:30,389
snowflake database if I click on this

00:08:28,050 --> 00:08:33,029
you can see that we are storing the JSON

00:08:30,389 --> 00:08:35,729
in a variant column so this is our data

00:08:33,029 --> 00:08:37,709
type for this semi structured data like

00:08:35,729 --> 00:08:39,659
JSON and Park a that I talked about that

00:08:37,709 --> 00:08:43,320
automatically does the translation into

00:08:39,659 --> 00:08:47,100
the internal storage representation and

00:08:43,320 --> 00:08:48,930
now let's jump back over here into a

00:08:47,100 --> 00:08:51,390
different browser window where I'm

00:08:48,930 --> 00:08:53,459
connected to a somewhat smaller cluster

00:08:51,390 --> 00:08:53,990
just one compute node instead of two and

00:08:53,459 --> 00:08:56,330
I'm

00:08:53,990 --> 00:08:57,920
- snowflake and let me run a couple of

00:08:56,330 --> 00:08:59,779
queries here against snowflakes so this

00:08:57,920 --> 00:09:02,839
is the first one so you're just checking

00:08:59,779 --> 00:09:05,149
if we can get a connection to my

00:09:02,839 --> 00:09:07,130
snowflake warehouse and that should

00:09:05,149 --> 00:09:09,110
hopefully just finish in a couple of

00:09:07,130 --> 00:09:12,589
seconds and spit out the current date

00:09:09,110 --> 00:09:16,339
and you can see today is December the

00:09:12,589 --> 00:09:17,930
11th and you can also see that here we

00:09:16,339 --> 00:09:20,450
are essentially telling the spark read

00:09:17,930 --> 00:09:23,899
operation to run a query with current

00:09:20,450 --> 00:09:26,930
aid against snowflake so let's do

00:09:23,899 --> 00:09:28,730
something more interesting here let's

00:09:26,930 --> 00:09:31,670
populate a data frame this data frame

00:09:28,730 --> 00:09:33,470
with all the rows in this table the

00:09:31,670 --> 00:09:38,060
tweets table that we just looked at over

00:09:33,470 --> 00:09:40,130
on the Left listener on this and then do

00:09:38,060 --> 00:09:43,490
the same count operation on it and you

00:09:40,130 --> 00:09:46,149
can see here the JSON is coming back for

00:09:43,490 --> 00:09:49,550
the first couple of rows

00:09:46,149 --> 00:09:53,180
here's a count operation in about two

00:09:49,550 --> 00:09:56,990
seconds the same 3.2 million JSON

00:09:53,180 --> 00:09:59,089
documents now here I'm running an actual

00:09:56,990 --> 00:10:01,670
snowflake query to populate another data

00:09:59,089 --> 00:10:04,220
frame and you can see here in the syntax

00:10:01,670 --> 00:10:06,320
I'm using some of the built in snowflake

00:10:04,220 --> 00:10:09,649
sequel constructs to work with semi

00:10:06,320 --> 00:10:14,540
structured data in snowflake let's send

00:10:09,649 --> 00:10:16,399
this off and there you can see we're

00:10:14,540 --> 00:10:19,790
getting languages and some texts back

00:10:16,399 --> 00:10:22,070
and now let's run this query here which

00:10:19,790 --> 00:10:26,149
does again the same grouping by language

00:10:22,070 --> 00:10:28,730
and counting the tweets per language and

00:10:26,149 --> 00:10:31,910
you can see that same result in about

00:10:28,730 --> 00:10:33,920
three and a half seconds coming back now

00:10:31,910 --> 00:10:36,470
the nice thing again here is we're

00:10:33,920 --> 00:10:38,839
running a much smaller cluster backed by

00:10:36,470 --> 00:10:40,339
a snowflake warehouse and you can see

00:10:38,839 --> 00:10:42,980
some of the performance benefits here

00:10:40,339 --> 00:10:44,630
which which are due to the to the

00:10:42,980 --> 00:10:47,690
internal optimizations that snowflake

00:10:44,630 --> 00:10:49,610
does behind the scenes and know that you

00:10:47,690 --> 00:10:52,520
literally have don't have to do any

00:10:49,610 --> 00:10:54,860
changes to your spark application code

00:10:52,520 --> 00:10:56,810
so the connector injects itself

00:10:54,860 --> 00:10:58,850
transparently into the plan generation

00:10:56,810 --> 00:11:00,860
process whatever transformations you do

00:10:58,850 --> 00:11:02,810
over your data frames will look at them

00:11:00,860 --> 00:11:04,760
and figure out which ones are relational

00:11:02,810 --> 00:11:06,529
in nature and backed by data that sits

00:11:04,760 --> 00:11:07,820
in snowflake and will automatically

00:11:06,529 --> 00:11:09,380
translate that

00:11:07,820 --> 00:11:11,870
into sequel queries and run over slope

00:11:09,380 --> 00:11:15,410
in snowflake we can take a quick look at

00:11:11,870 --> 00:11:19,430
this here so here is the sequel query

00:11:15,410 --> 00:11:21,650
for this the does the counting and down

00:11:19,430 --> 00:11:23,000
here you see the group by language so

00:11:21,650 --> 00:11:25,040
this is the sequel query that was

00:11:23,000 --> 00:11:28,070
automatically generated by the spark

00:11:25,040 --> 00:11:33,250
connector when we when we submitted that

00:11:28,070 --> 00:11:40,270
that last group by statement in in spark

00:11:33,250 --> 00:11:40,270
so with that let me jump back in here

00:11:41,950 --> 00:11:50,600
and the great software only exists

00:11:47,390 --> 00:11:52,010
because of great engineers so obviously

00:11:50,600 --> 00:11:55,400
being a sponsor talking wanted to show

00:11:52,010 --> 00:11:58,160
this for a minute we are we are hiring

00:11:55,400 --> 00:11:59,840
in into our development offices one

00:11:58,160 --> 00:12:01,970
local here to the Seattle area down in

00:11:59,840 --> 00:12:03,980
San Mateo as well as in Berlin Germany

00:12:01,970 --> 00:12:06,500
things that we are working on are

00:12:03,980 --> 00:12:08,020
obviously performance one of the most

00:12:06,500 --> 00:12:11,420
recent things in that space was

00:12:08,020 --> 00:12:14,150
materialized views which where we were

00:12:11,420 --> 00:12:15,830
taking a fairly fresh view on how to do

00:12:14,150 --> 00:12:17,870
materialized views for relational

00:12:15,830 --> 00:12:19,760
databases on the areas that are of

00:12:17,870 --> 00:12:21,500
interest to us is the whole data science

00:12:19,760 --> 00:12:23,120
machine learning space and how do we

00:12:21,500 --> 00:12:26,360
best support these workloads from a

00:12:23,120 --> 00:12:27,650
snowflake perspective modern enterprise

00:12:26,360 --> 00:12:29,360
data architectures like how do we

00:12:27,650 --> 00:12:31,190
integrate into application stacks like

00:12:29,360 --> 00:12:33,740
spark which we spend some time on today

00:12:31,190 --> 00:12:36,290
and then obviously performance also for

00:12:33,740 --> 00:12:39,650
our metadata layers in our cloud

00:12:36,290 --> 00:12:44,030
services particular F DB for metadata

00:12:39,650 --> 00:12:46,160
scale and then also increasing our

00:12:44,030 --> 00:12:49,570
footprint growing into additional cloud

00:12:46,160 --> 00:12:52,640
regions both in AWS as well as an azure

00:12:49,570 --> 00:12:53,760
that's all I have for today thanks very

00:12:52,640 --> 00:12:57,919
much

00:12:53,760 --> 00:12:57,919

YouTube URL: https://www.youtube.com/watch?v=Xf6b-PAdIFc


