Title: Lightning Talk: Hotspot detection and handling in FoundationDB - Ashish Motivala
Publication date: 2018-12-14
Playlist: FoundationDB Summit 2018
Description: 
	Hotspot, are a source of problems for all databases. On FDB they can result in a system-wide performance degradation. This talk will demonstrate how SnowflakeDB, which uses FDB as its OLTP database, keeps track of FDB transactions and maps data to partitions on FDB storage nodes. The enables detection of hotspots, and discusses ways to handle them.
Captions: 
	00:00:00,050 --> 00:00:05,130
what is the hot spot in foundation to be

00:00:02,460 --> 00:00:07,470
a hot spot in any database for that

00:00:05,130 --> 00:00:12,210
matter high volume of writes or reads to

00:00:07,470 --> 00:00:14,549
a small set of blocks a small number of

00:00:12,210 --> 00:00:18,260
partitions in this case on a storage

00:00:14,549 --> 00:00:22,680
node or a set of storage nodes team as

00:00:18,260 --> 00:00:25,980
depends on how much as application a

00:00:22,680 --> 00:00:28,590
couple of matrix just to sort of give

00:00:25,980 --> 00:00:31,590
you context everybody's mentioned Steve

00:00:28,590 --> 00:00:36,300
mentioned the storage node keeps five

00:00:31,590 --> 00:00:39,480
seconds of data in a version tree that

00:00:36,300 --> 00:00:42,360
are non-durable this can grow and shrink

00:00:39,480 --> 00:00:45,780
in size the storage queue is the size of

00:00:42,360 --> 00:00:48,149
these mutations the the and another

00:00:45,780 --> 00:00:50,700
metric called non-durable versions is

00:00:48,149 --> 00:00:54,469
the number of versions that have not

00:00:50,700 --> 00:00:56,550
been persisted to the sequel light and

00:00:54,469 --> 00:00:58,469
the only thing you really need to know

00:00:56,550 --> 00:01:01,170
is that when storage queue goes up rate

00:00:58,469 --> 00:01:06,049
keeper goes down when NDB goes up bad

00:01:01,170 --> 00:01:08,430
things happen when it goes too high so

00:01:06,049 --> 00:01:10,619
there's this is a simple talk there's

00:01:08,430 --> 00:01:15,180
just a couple of takeaways and maybe I

00:01:10,619 --> 00:01:16,619
can give some more time back if you want

00:01:15,180 --> 00:01:18,570
to detect two kinds of things if you

00:01:16,619 --> 00:01:21,409
have a right hotspot you will find that

00:01:18,570 --> 00:01:26,100
both the storage Q and the NDB go up

00:01:21,409 --> 00:01:28,320
simultaneously on a specific process if

00:01:26,100 --> 00:01:30,780
you have a read hotspot you'll find that

00:01:28,320 --> 00:01:34,799
only the story only the NDB goes up and

00:01:30,780 --> 00:01:36,810
not so much the storage queue and the

00:01:34,799 --> 00:01:40,409
reason for this is foundation DB tries

00:01:36,810 --> 00:01:44,579
to prioritize reads or writes if you

00:01:40,409 --> 00:01:47,820
have a large amount of writes then they

00:01:44,579 --> 00:01:49,740
take a deprioritized and the storage

00:01:47,820 --> 00:01:51,659
cube I mean the storage node is busy

00:01:49,740 --> 00:01:53,790
supplying all the reads if there's a

00:01:51,659 --> 00:01:55,439
large number of reads then the storage

00:01:53,790 --> 00:01:59,549
mountable versions will go up as a

00:01:55,439 --> 00:02:02,009
result so how do we solve these problems

00:01:59,549 --> 00:02:04,049
we made a code change which changes the

00:02:02,009 --> 00:02:06,540
priority of the right tasks you will see

00:02:04,049 --> 00:02:09,750
this in the previous slide that solid

00:02:06,540 --> 00:02:12,390
line up top is in in the lower chart is

00:02:09,750 --> 00:02:13,750
we said rights are not getting pushed at

00:02:12,390 --> 00:02:17,800
all let's increase the priority

00:02:13,750 --> 00:02:20,050
try to read that helps us a lot

00:02:17,800 --> 00:02:23,350
we applied application level throttling

00:02:20,050 --> 00:02:26,910
that Cantrell reads writes final grade

00:02:23,350 --> 00:02:29,350
in throttling for oh these are certain

00:02:26,910 --> 00:02:31,630
applications that are doing worse but

00:02:29,350 --> 00:02:33,430
most importantly this is the second

00:02:31,630 --> 00:02:35,920
takeaway from this talk is you need to

00:02:33,430 --> 00:02:38,260
understand your data patterns and and

00:02:35,920 --> 00:02:40,000
this is something that being a database

00:02:38,260 --> 00:02:41,770
company we should be really good at but

00:02:40,000 --> 00:02:43,870
it turns out we weren't as good as we

00:02:41,770 --> 00:02:46,120
should be and we've been sort of you

00:02:43,870 --> 00:02:47,590
know not on the head by various people

00:02:46,120 --> 00:02:52,920
including so I'm sitting in this room

00:02:47,590 --> 00:02:55,630
about that so how would you do this

00:02:52,920 --> 00:02:57,430
foundation DB keeps track of partition

00:02:55,630 --> 00:03:00,220
stats but doesn't have any application

00:02:57,430 --> 00:03:03,220
context the application of the layers

00:03:00,220 --> 00:03:06,100
itself have both application context and

00:03:03,220 --> 00:03:08,350
API that they can use to get partition

00:03:06,100 --> 00:03:10,360
stats from foundation dB I mean we may

00:03:08,350 --> 00:03:12,250
have made some changes to it to give you

00:03:10,360 --> 00:03:14,920
both the storage and the port

00:03:12,250 --> 00:03:16,540
information but it's a trivial change

00:03:14,920 --> 00:03:22,390
that can be ported to the open source

00:03:16,540 --> 00:03:25,060
quite quickly and in and and so now if

00:03:22,390 --> 00:03:27,610
you take your application context and

00:03:25,060 --> 00:03:30,130
merge it along with the partition stats

00:03:27,610 --> 00:03:31,989
then you kind of know where your data

00:03:30,130 --> 00:03:33,459
accesses is are going which storage

00:03:31,989 --> 00:03:37,299
nodes are they going very right it's

00:03:33,459 --> 00:03:39,850
really flowing - and then we keep stats

00:03:37,299 --> 00:03:42,160
for every single transaction we we just

00:03:39,850 --> 00:03:44,019
buffer this up in memory and when it's

00:03:42,160 --> 00:03:45,610
time we flush it out to disk if you're

00:03:44,019 --> 00:03:48,579
running very high throughput you can

00:03:45,610 --> 00:03:52,209
switch over to sampling this is a sample

00:03:48,579 --> 00:03:54,610
stat all I really want to show is that

00:03:52,209 --> 00:03:56,920
we have partition ranking from we call

00:03:54,610 --> 00:04:02,980
it partition rank but it's really a

00:03:56,920 --> 00:04:04,600
shard in FTB terms and we see AHA here

00:04:02,980 --> 00:04:09,250
oh we fit so many bytes from here we

00:04:04,600 --> 00:04:11,110
said so many bytes there and then you

00:04:09,250 --> 00:04:12,850
take this information in our case we

00:04:11,110 --> 00:04:14,079
push it back into snowflake because we

00:04:12,850 --> 00:04:17,260
can create easily you can push it

00:04:14,079 --> 00:04:18,790
anywhere else you want to be I've heard

00:04:17,260 --> 00:04:20,950
the FT B team is considered pushing it

00:04:18,790 --> 00:04:23,950
back into FDB itself and running queries

00:04:20,950 --> 00:04:25,570
on top of it but you know it's very

00:04:23,950 --> 00:04:26,680
common to see different distributions

00:04:25,570 --> 00:04:31,270
for your data where is it

00:04:26,680 --> 00:04:33,550
a large amount of of certain kinds of

00:04:31,270 --> 00:04:35,050
data and very little of the others this

00:04:33,550 --> 00:04:37,780
in itself is not a problem

00:04:35,050 --> 00:04:41,590
however skews in data access is a

00:04:37,780 --> 00:04:43,810
problem and here what we've done is

00:04:41,590 --> 00:04:46,720
we've tried to figure out how much data

00:04:43,810 --> 00:04:53,289
is read or written per minute for like

00:04:46,720 --> 00:04:55,690
top partitions and the and that's the

00:04:53,289 --> 00:04:58,210
graph on the right by the way oh sorry

00:04:55,690 --> 00:04:59,979
that's the graph on the left which is PI

00:04:58,210 --> 00:05:02,139
partition what are the reads and then

00:04:59,979 --> 00:05:04,419
the graph on the right is by the entity

00:05:02,139 --> 00:05:06,490
so this is the graph on the left is the

00:05:04,419 --> 00:05:07,900
view from the FGB side the graph on the

00:05:06,490 --> 00:05:09,310
right is the view from the application

00:05:07,900 --> 00:05:12,250
side but they've both been sort of

00:05:09,310 --> 00:05:15,610
merged and they're sort of looking at

00:05:12,250 --> 00:05:18,039
them together so what do you do you

00:05:15,610 --> 00:05:22,090
address this in various ways like the

00:05:18,039 --> 00:05:23,860
first two or three sort of deal access

00:05:22,090 --> 00:05:27,610
patterns here are clearly egregious and

00:05:23,860 --> 00:05:29,740
need fixing you can cache you can fix

00:05:27,610 --> 00:05:32,349
your accesses but you know if you're

00:05:29,740 --> 00:05:34,690
going to access close to a gigabyte a

00:05:32,349 --> 00:05:36,010
second or so you gigabyte a minute and

00:05:34,690 --> 00:05:41,860
at least on our system you will cause

00:05:36,010 --> 00:05:45,660
problems and that's it thank you

00:05:41,860 --> 00:05:45,660

YouTube URL: https://www.youtube.com/watch?v=0xEjlPK3je4


