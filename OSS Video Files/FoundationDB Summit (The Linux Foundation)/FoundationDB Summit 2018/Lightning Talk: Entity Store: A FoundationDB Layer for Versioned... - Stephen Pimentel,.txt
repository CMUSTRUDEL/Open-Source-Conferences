Title: Lightning Talk: Entity Store: A FoundationDB Layer for Versioned... - Stephen Pimentel,
Publication date: 2018-12-14
Playlist: FoundationDB Summit 2018
Description: 
	Lightning Talk: Entity Store: A FoundationDB Layer for Versioned Entities with Fine Grained Authorization and Lineage - Stephen Pimentel, Apple
Captions: 
	00:00:00,000 --> 00:00:06,330
all right so this is the last talk of

00:00:03,419 --> 00:00:10,500
the day my mandate is to be quick I

00:00:06,330 --> 00:00:13,380
promise I will be so metadata management

00:00:10,500 --> 00:00:15,509
for machine learning pipelines my name

00:00:13,380 --> 00:00:18,090
is Steven Pimentel and I work at Apple I

00:00:15,509 --> 00:00:19,920
would normally give a 40 minute version

00:00:18,090 --> 00:00:21,810
of this presentation this is my first

00:00:19,920 --> 00:00:24,420
attempt at compressing it to five

00:00:21,810 --> 00:00:27,060
minutes I'm calling this the haiku

00:00:24,420 --> 00:00:30,000
version and I hope it isn't more like a

00:00:27,060 --> 00:00:32,189
cohan I'm going to talk about a

00:00:30,000 --> 00:00:33,960
foundation DB layer called entities

00:00:32,189 --> 00:00:36,750
store that I wrote for an internal

00:00:33,960 --> 00:00:38,190
system at Apple we use it to store and

00:00:36,750 --> 00:00:41,670
manage metadata for machine learning

00:00:38,190 --> 00:00:43,530
pipelines entity store exposes a data

00:00:41,670 --> 00:00:46,050
model for versioned entities with

00:00:43,530 --> 00:00:48,960
fine-grained authorization and lineage

00:00:46,050 --> 00:00:53,910
that summary on the slide isn't very

00:00:48,960 --> 00:00:57,360
complete but it is a haiku so why does

00:00:53,910 --> 00:00:58,530
metadata matter for machine learning say

00:00:57,360 --> 00:01:00,660
you're running machine learning

00:00:58,530 --> 00:01:03,390
pipelines on a cloud platform for lots

00:01:00,660 --> 00:01:05,519
of users the number of users is large

00:01:03,390 --> 00:01:09,390
and they span a diverse range of use

00:01:05,519 --> 00:01:12,600
cases and teams pipelines start with

00:01:09,390 --> 00:01:15,540
data sets the data sets are many varied

00:01:12,600 --> 00:01:17,280
and usually raw there are often

00:01:15,540 --> 00:01:21,150
requirements around who is allowed to

00:01:17,280 --> 00:01:24,150
see which parts of which data sets raw

00:01:21,150 --> 00:01:27,090
data sets are transformed to produce

00:01:24,150 --> 00:01:29,520
datasets suitable for training and then

00:01:27,090 --> 00:01:31,860
we must track what raw data sets were

00:01:29,520 --> 00:01:34,380
used two or more can be joined or

00:01:31,860 --> 00:01:37,049
otherwise used in combination what

00:01:34,380 --> 00:01:39,840
transforms were applied using what code

00:01:37,049 --> 00:01:44,220
in what languages with what versions of

00:01:39,840 --> 00:01:47,340
what libraries training produces models

00:01:44,220 --> 00:01:49,590
we must now track what architectures

00:01:47,340 --> 00:01:50,970
were used what learning algorithms what

00:01:49,590 --> 00:01:53,939
frameworks tensorflow

00:01:50,970 --> 00:01:56,070
PI torch with what libraries and what

00:01:53,939 --> 00:01:58,369
versions of these libraries and this is

00:01:56,070 --> 00:02:01,200
before we even get to hyper parameters

00:01:58,369 --> 00:02:03,930
we can also use one or more pre-trained

00:02:01,200 --> 00:02:07,829
models in case of transfer learning or

00:02:03,930 --> 00:02:09,869
fine tuning in an experimentation

00:02:07,829 --> 00:02:12,360
framework you often iterate through a

00:02:09,869 --> 00:02:13,140
number of models varying architectures

00:02:12,360 --> 00:02:15,870
and algorithm

00:02:13,140 --> 00:02:19,380
but all trained for the same task or use

00:02:15,870 --> 00:02:22,440
case now metadata must be tracked per

00:02:19,380 --> 00:02:24,420
run in addition to the previous types of

00:02:22,440 --> 00:02:28,620
metadata we have additional run specific

00:02:24,420 --> 00:02:31,200
types finally we may want to serve a

00:02:28,620 --> 00:02:32,760
model in production but as with any

00:02:31,200 --> 00:02:34,860
production system a model will go

00:02:32,760 --> 00:02:37,980
through versions as it is updated and

00:02:34,860 --> 00:02:40,530
improved you may have Canary deployments

00:02:37,980 --> 00:02:44,780
or roll back to a previous version or

00:02:40,530 --> 00:02:47,040
deploy multiple versions for a/b testing

00:02:44,780 --> 00:02:49,470
we want to store and manage this

00:02:47,040 --> 00:02:51,810
metadata in a unified framework that

00:02:49,470 --> 00:02:54,870
tracks provenance for each version of

00:02:51,810 --> 00:02:58,500
each entity this is what entities store

00:02:54,870 --> 00:03:01,020
does so entity store is a foundation DB

00:02:58,500 --> 00:03:03,470
layer that implements a data model for

00:03:01,020 --> 00:03:06,180
versioned entities with fine-grained

00:03:03,470 --> 00:03:08,459
authorization and lineage it's

00:03:06,180 --> 00:03:10,980
implemented as a python library exposing

00:03:08,459 --> 00:03:15,350
its own API above the foundation DB

00:03:10,980 --> 00:03:17,130
Python bindings read and write

00:03:15,350 --> 00:03:20,310
authorizations are separately recorded

00:03:17,130 --> 00:03:24,750
at the level of individual fields also

00:03:20,310 --> 00:03:26,730
known as cell based security versioning

00:03:24,750 --> 00:03:28,800
of entities is automatic with

00:03:26,730 --> 00:03:31,860
modifications to immutable fields

00:03:28,800 --> 00:03:34,739
resulting in a new version rather than a

00:03:31,860 --> 00:03:37,739
mutation each version has a unique ID

00:03:34,739 --> 00:03:40,739
like a get commit ID formed from the

00:03:37,739 --> 00:03:43,950
sha-1 hash of its primary key in mutable

00:03:40,739 --> 00:03:47,370
fields parents version and authorization

00:03:43,950 --> 00:03:49,709
groups versions form a parentage tree

00:03:47,370 --> 00:03:54,540
and can be explicitly selected for use

00:03:49,709 --> 00:03:56,790
also like get each entity is modeled as

00:03:54,540 --> 00:03:59,640
a collection of objects that represent

00:03:56,790 --> 00:04:02,459
its distinctive versions along with a

00:03:59,640 --> 00:04:08,430
core object for mutable non versioned

00:04:02,459 --> 00:04:11,010
fields provenance or lineage is a record

00:04:08,430 --> 00:04:13,560
of datas origin and the transforms that

00:04:11,010 --> 00:04:17,700
have been applied to it entity store

00:04:13,560 --> 00:04:21,000
stores records records lineage via label

00:04:17,700 --> 00:04:23,400
directed multigraphs the graph for

00:04:21,000 --> 00:04:25,919
version parentage is constructed by the

00:04:23,400 --> 00:04:26,760
versioning mechanism other forms of

00:04:25,919 --> 00:04:28,860
provenance such

00:04:26,760 --> 00:04:31,140
derivation of training data sets from

00:04:28,860 --> 00:04:36,450
raw data sets are represented by

00:04:31,140 --> 00:04:38,430
different labels for graph edges objects

00:04:36,450 --> 00:04:41,490
are schema lists and consist of any

00:04:38,430 --> 00:04:43,380
number of fields with values fields can

00:04:41,490 --> 00:04:45,420
be single valued or set valued and

00:04:43,380 --> 00:04:47,520
either case can optionally be indexed

00:04:45,420 --> 00:04:52,800
and fields can also have large blob

00:04:47,520 --> 00:04:55,250
values this rich data model is mapped to

00:04:52,800 --> 00:04:57,360
the key value store versioning

00:04:55,250 --> 00:04:59,280
authorization and lineage are

00:04:57,360 --> 00:05:02,390
incorporated directly into the data

00:04:59,280 --> 00:05:04,740
model layer so clients get them for free

00:05:02,390 --> 00:05:06,510
transactions allow multiple clients to

00:05:04,740 --> 00:05:08,640
concurrently access entities without

00:05:06,510 --> 00:05:12,910
fear of inconsistent results or data

00:05:08,640 --> 00:05:17,049
corruption and thank you very much

00:05:12,910 --> 00:05:17,049

YouTube URL: https://www.youtube.com/watch?v=16uU_Aaxp9Y


