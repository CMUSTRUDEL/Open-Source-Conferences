Title: Technical Overview of FoundationDB - Evan Tschannen, Apple
Publication date: 2018-12-14
Playlist: FoundationDB Summit 2018
Description: 
	This presentation will walk through what is happening in the internals of the FoundationDB in response to user operations. The talk will focus on explaining how FoundationDB provides external consistency, and how FoundationDB scales as more processes join a cluster.
Captions: 
	00:00:00,000 --> 00:00:05,640
i'm evan shannon so i managed the

00:00:03,840 --> 00:00:08,790
development of the foundation DB key

00:00:05,640 --> 00:00:10,290
value store at apple and i'm gonna spend

00:00:08,790 --> 00:00:13,259
the next 40 minutes taking you through

00:00:10,290 --> 00:00:14,630
the architecture of foundation DB i was

00:00:13,259 --> 00:00:17,760
first taken through the architecture

00:00:14,630 --> 00:00:20,550
eight years ago however back then it

00:00:17,760 --> 00:00:23,090
existed as some hand-drawn diagrams on a

00:00:20,550 --> 00:00:23,090
piece of graph paper

00:00:23,130 --> 00:00:27,449
what's pretty remarkable though is that

00:00:25,170 --> 00:00:31,260
it took around three years of

00:00:27,449 --> 00:00:33,300
development before we actually before we

00:00:31,260 --> 00:00:35,370
actually made any significant changes to

00:00:33,300 --> 00:00:38,879
the database that like weren't on that

00:00:35,370 --> 00:00:41,670
original sketch and a lot of what i'm

00:00:38,879 --> 00:00:45,090
gonna talk about today if you found that

00:00:41,670 --> 00:00:49,590
graph paper now which would be there so

00:00:45,090 --> 00:00:51,360
it's pretty pretty remarkable um so to

00:00:49,590 --> 00:00:53,879
focus this presentation a little bit

00:00:51,360 --> 00:00:55,739
before i get into it i wanted to

00:00:53,879 --> 00:00:58,230
highlight some of the foundation DB

00:00:55,739 --> 00:01:00,629
strengths so as i'm going to later as

00:00:58,230 --> 00:01:02,879
I'm going through the architecture I can

00:01:00,629 --> 00:01:06,210
kind of explain how foundation DB is

00:01:02,879 --> 00:01:09,930
providing these properties so one of the

00:01:06,210 --> 00:01:12,930
things is that foundation DB is is

00:01:09,930 --> 00:01:15,000
operationally very easy to use and

00:01:12,930 --> 00:01:17,610
specifically it does a lot for you in

00:01:15,000 --> 00:01:20,189
terms of load balancing traffic across

00:01:17,610 --> 00:01:23,250
the different machines and a cluster and

00:01:20,189 --> 00:01:25,380
it also does a lot for self-healing in

00:01:23,250 --> 00:01:26,610
response to failures so I'll come back

00:01:25,380 --> 00:01:27,810
to both of those points as I'm going

00:01:26,610 --> 00:01:30,750
through the architecture have for how we

00:01:27,810 --> 00:01:32,640
do it um the other point is comes back

00:01:30,750 --> 00:01:35,579
to something Ben really mentioned which

00:01:32,640 --> 00:01:37,700
is that we provide scalability without

00:01:35,579 --> 00:01:40,890
sacrificing performance or consistency

00:01:37,700 --> 00:01:43,439
it's really easy to be skeptical about a

00:01:40,890 --> 00:01:45,869
claim like that because it sounds too

00:01:43,439 --> 00:01:47,790
good to be true so I want to be really

00:01:45,869 --> 00:01:50,100
specific about what we mean about this

00:01:47,790 --> 00:01:55,380
and so hopefully I can convince you all

00:01:50,100 --> 00:01:57,479
that we actually do it so it's easy it's

00:01:55,380 --> 00:02:00,360
a little easier to talk about not giving

00:01:57,479 --> 00:02:02,100
up consistency and foundation DB for

00:02:00,360 --> 00:02:04,710
scalability you know foundation DB

00:02:02,100 --> 00:02:07,380
provides gives you acid transactions

00:02:04,710 --> 00:02:08,759
there's a single global key space inside

00:02:07,380 --> 00:02:11,930
of these transactions you can read or

00:02:08,759 --> 00:02:13,350
write any key across the whole database

00:02:11,930 --> 00:02:15,150
foundation you begin

00:02:13,350 --> 00:02:17,730
you strict serializability and external

00:02:15,150 --> 00:02:20,550
consistency which I'll get into a little

00:02:17,730 --> 00:02:24,240
bit later it's a little bit harder to

00:02:20,550 --> 00:02:27,750
quantify what it means to not sacrifice

00:02:24,240 --> 00:02:28,830
performance for poor scalability because

00:02:27,750 --> 00:02:32,190
there's a lot of dimensions to

00:02:28,830 --> 00:02:35,550
performance so my approach is to think

00:02:32,190 --> 00:02:38,460
about Foundation DB as organizing a lot

00:02:35,550 --> 00:02:41,510
of individual like non-scalable key

00:02:38,460 --> 00:02:44,390
value stores into a single cohesive unit

00:02:41,510 --> 00:02:46,830
so we can talk about the like

00:02:44,390 --> 00:02:48,510
theoretical perfect performance of the

00:02:46,830 --> 00:02:50,310
system is if you dedicated all of your

00:02:48,510 --> 00:02:52,350
hardware to these individual key value

00:02:50,310 --> 00:02:54,120
stores what would be the aggregate

00:02:52,350 --> 00:02:56,820
throughput of all of these things put

00:02:54,120 --> 00:02:59,970
together what would be and so when we

00:02:56,820 --> 00:03:03,150
compare Foundation DB against that model

00:02:59,970 --> 00:03:08,810
we achieve roughly 90% of that

00:03:03,150 --> 00:03:10,770
theoretical best perfect scalability so

00:03:08,810 --> 00:03:12,300
and hopefully I'll be able to convince

00:03:10,770 --> 00:03:15,060
you of that as we're as we're going

00:03:12,300 --> 00:03:16,680
through it in terms of latency czar read

00:03:15,060 --> 00:03:18,720
latency Zoar basically as good as you

00:03:16,680 --> 00:03:20,370
could possibly get there a single hop

00:03:18,720 --> 00:03:22,650
from a client directly to a server

00:03:20,370 --> 00:03:25,200
that's going to serve the response for

00:03:22,650 --> 00:03:27,750
that read writes are a little bit worse

00:03:25,200 --> 00:03:30,120
to get a write successfully committed to

00:03:27,750 --> 00:03:33,120
the database it's about four hops

00:03:30,120 --> 00:03:34,380
through the system in practice this is

00:03:33,120 --> 00:03:37,770
going to mean about four or five

00:03:34,380 --> 00:03:41,580
milliseconds of latency on commits so

00:03:37,770 --> 00:03:43,410
it's important to note that all of this

00:03:41,580 --> 00:03:44,910
argument about performance is related to

00:03:43,410 --> 00:03:46,740
the architecture which is what I'm going

00:03:44,910 --> 00:03:48,930
through but it says nothing about our

00:03:46,740 --> 00:03:50,310
actual performance and in fact a lot of

00:03:48,930 --> 00:03:51,840
the projects were working on today are

00:03:50,310 --> 00:03:52,980
gonna significantly improve the

00:03:51,840 --> 00:03:56,520
performance of our system as a whole

00:03:52,980 --> 00:03:58,140
like the Redwood storage engine so but

00:03:56,520 --> 00:04:00,020
this is basically showing us with a

00:03:58,140 --> 00:04:03,740
perfect implementation of foundation DB

00:04:00,020 --> 00:04:03,740
this the sky's the limit

00:04:03,950 --> 00:04:09,090
so now I'm finally ready to get into it

00:04:06,930 --> 00:04:11,540
and I'm gonna do that by starting with a

00:04:09,090 --> 00:04:15,780
little bit of distributed databases 101

00:04:11,540 --> 00:04:17,730
so in this diagram we have three servers

00:04:15,780 --> 00:04:19,920
and a single writer which is trying to

00:04:17,730 --> 00:04:21,630
push data into the system and a single

00:04:19,920 --> 00:04:25,480
reader which is trying to get data out

00:04:21,630 --> 00:04:27,370
of the system and in this system we

00:04:25,480 --> 00:04:30,090
you have the criteria that we need to be

00:04:27,370 --> 00:04:33,310
resilient to a failure of a machine um

00:04:30,090 --> 00:04:36,430
so the traditional way to do this is

00:04:33,310 --> 00:04:38,620
using corn logic so if the writer when

00:04:36,430 --> 00:04:41,590
it's writing data make sure the data

00:04:38,620 --> 00:04:44,200
gets to two of the three servers um

00:04:41,590 --> 00:04:47,770
and the reader may like reads from all

00:04:44,200 --> 00:04:50,370
of the locations and elite is sorry

00:04:47,770 --> 00:04:52,480
about that is only successfully reads

00:04:50,370 --> 00:04:53,980
when it can talk to two of the three

00:04:52,480 --> 00:04:56,740
servers we'll make sure every piece of

00:04:53,980 --> 00:04:58,660
data gets to the system and then if one

00:04:56,740 --> 00:04:59,980
of the server's fails well the system

00:04:58,660 --> 00:05:01,360
will just keep going on naturally

00:04:59,980 --> 00:05:03,070
because we only needed two out of three

00:05:01,360 --> 00:05:05,170
responses from both places basically

00:05:03,070 --> 00:05:08,610
this system has implicit failure

00:05:05,170 --> 00:05:12,280
handling it just will naturally increase

00:05:08,610 --> 00:05:15,490
so kind of the I guess I should have

00:05:12,280 --> 00:05:20,830
brought some water up here so sort of

00:05:15,490 --> 00:05:24,280
the core principle behind foundation DB

00:05:20,830 --> 00:05:26,560
is if we don't actually need this

00:05:24,280 --> 00:05:29,290
implicit failure handling that I just

00:05:26,560 --> 00:05:31,120
described um there's actually a lot of

00:05:29,290 --> 00:05:33,930
benefit we can gain by doing things

00:05:31,120 --> 00:05:36,100
slightly differently so in this case

00:05:33,930 --> 00:05:38,440
since we don't have to handle failures

00:05:36,100 --> 00:05:42,100
our writer can write to all three of the

00:05:38,440 --> 00:05:43,990
servers um because we now have no well

00:05:42,100 --> 00:05:56,410
thank you van you're rescuing me from my

00:05:43,990 --> 00:05:58,930
cough okay so if the writer writes to

00:05:56,410 --> 00:06:01,780
all three of the locations this actually

00:05:58,930 --> 00:06:03,880
gives us two huge performance wins one

00:06:01,780 --> 00:06:06,700
of them is obvious in the lettle the

00:06:03,880 --> 00:06:08,530
other one is a little more subtle so the

00:06:06,700 --> 00:06:10,660
obvious one is because the data is now

00:06:08,530 --> 00:06:12,430
at all of the server's the reader no

00:06:10,660 --> 00:06:14,110
longer has to talk to all the servers to

00:06:12,430 --> 00:06:16,750
do a read it can actually go to just one

00:06:14,110 --> 00:06:18,910
of the servers to get a response because

00:06:16,750 --> 00:06:20,980
they all have all of the data there's no

00:06:18,910 --> 00:06:22,540
like quorum to combine them so basically

00:06:20,980 --> 00:06:23,710
our reader is now three times more

00:06:22,540 --> 00:06:26,650
efficient than it was in the previous

00:06:23,710 --> 00:06:29,680
diagram the or the other more subtle

00:06:26,650 --> 00:06:31,360
point is that when we can handle

00:06:29,680 --> 00:06:33,850
failures in this system we're actually

00:06:31,360 --> 00:06:36,220
going to do it do a lot better job so

00:06:33,850 --> 00:06:38,230
the quorum logic with three servers gave

00:06:36,220 --> 00:06:38,889
us a resilience of a loss of us one

00:06:38,230 --> 00:06:42,430
single

00:06:38,889 --> 00:06:44,710
gene in this system because the data is

00:06:42,430 --> 00:06:46,449
making it to all three servers we

00:06:44,710 --> 00:06:48,789
actually can lose two of our three

00:06:46,449 --> 00:06:50,349
servers and not lose any data so that so

00:06:48,789 --> 00:06:53,319
we're going to be much more resilient to

00:06:50,349 --> 00:06:55,659
failures so then the question obviously

00:06:53,319 --> 00:06:58,240
becomes well how do you handle failures

00:06:55,659 --> 00:06:59,919
and so the foundation DB answer to this

00:06:58,240 --> 00:07:03,939
and kind of one of the key innovations

00:06:59,919 --> 00:07:05,889
behind the design is that we are going

00:07:03,939 --> 00:07:07,779
to have an entirely different database

00:07:05,889 --> 00:07:10,569
that it's going to store metadata about

00:07:07,779 --> 00:07:12,879
this other database about our primary

00:07:10,569 --> 00:07:14,379
database and so basically this other

00:07:12,879 --> 00:07:17,020
metadata database is going to hold

00:07:14,379 --> 00:07:20,529
membership of the servers that are in in

00:07:17,020 --> 00:07:22,569
our primary database so in this case the

00:07:20,529 --> 00:07:24,129
servers a B and C we're holding we're

00:07:22,569 --> 00:07:25,659
responsible for committing data at

00:07:24,129 --> 00:07:27,789
version zero when we started up the

00:07:25,659 --> 00:07:31,270
database and after at some point in time

00:07:27,789 --> 00:07:34,029
you know server a died and then we

00:07:31,270 --> 00:07:36,129
replace server a with server D and so we

00:07:34,029 --> 00:07:39,699
wrote to this other database now we're

00:07:36,129 --> 00:07:42,069
doing our commits to B C and D this

00:07:39,699 --> 00:07:43,389
other database can use the quorum logic

00:07:42,069 --> 00:07:45,129
and implicit failure handling that we

00:07:43,389 --> 00:07:46,810
talked about before and because it's

00:07:45,129 --> 00:07:48,550
very low traffic database it's only

00:07:46,810 --> 00:07:51,069
gonna you're only going to ever write to

00:07:48,550 --> 00:07:53,550
this other database when there are

00:07:51,069 --> 00:07:56,889
failures in our in our primary database

00:07:53,550 --> 00:07:59,409
so Foundation B take took this basic

00:07:56,889 --> 00:08:04,020
idea and turn it into a system and so

00:07:59,409 --> 00:08:07,839
now I'll go in I'll start adding boxes

00:08:04,020 --> 00:08:10,569
so this diagram here shows all of the

00:08:07,839 --> 00:08:14,110
stateful components of foundation dB so

00:08:10,569 --> 00:08:15,789
the coordinators are this other database

00:08:14,110 --> 00:08:18,279
I talked about this metadata database

00:08:15,789 --> 00:08:22,000
that uses you know Paxos and quorum

00:08:18,279 --> 00:08:24,039
logic to do to do its writes in a fun

00:08:22,000 --> 00:08:25,330
trivia fact the first version of

00:08:24,039 --> 00:08:26,979
foundation DB this literally was a

00:08:25,330 --> 00:08:29,710
separate database we used a patchy

00:08:26,979 --> 00:08:31,750
zookeeper to hold this metadata and

00:08:29,710 --> 00:08:34,750
which was quickly replaced with our own

00:08:31,750 --> 00:08:38,800
implication you know a few like a year

00:08:34,750 --> 00:08:41,560
later so for the other stateful systems

00:08:38,800 --> 00:08:43,329
we have the transaction logs and this is

00:08:41,560 --> 00:08:45,670
a distributed right ahead log that's

00:08:43,329 --> 00:08:48,190
responsible basically for accepting

00:08:45,670 --> 00:08:50,470
commits and writes into the system it's

00:08:48,190 --> 00:08:52,480
job is basically to get sub durable on

00:08:50,470 --> 00:08:55,529
disk as fast as possible so that we can

00:08:52,480 --> 00:08:57,790
turn commit successfully to the client

00:08:55,529 --> 00:09:00,639
basically it's a write once read never

00:08:57,790 --> 00:09:02,800
data structure mutations are only being

00:09:00,639 --> 00:09:04,420
held there transiently they're coming in

00:09:02,800 --> 00:09:07,449
we're making them like appending them to

00:09:04,420 --> 00:09:09,250
the end of the file and then once the

00:09:07,449 --> 00:09:11,170
storage servers have the data we'll

00:09:09,250 --> 00:09:12,730
quickly get rid of it so generally they

00:09:11,170 --> 00:09:16,839
have very little they're using very

00:09:12,730 --> 00:09:18,190
little storage there they're also super

00:09:16,839 --> 00:09:22,269
efficient because they have such a

00:09:18,190 --> 00:09:23,620
simple job these storage servers go back

00:09:22,269 --> 00:09:24,699
to the point I was making when I was

00:09:23,620 --> 00:09:27,459
talking about performance and

00:09:24,699 --> 00:09:29,529
scalability so they are 90 percent of

00:09:27,459 --> 00:09:31,630
our system and they are all basically

00:09:29,529 --> 00:09:34,480
individual key value stores that were a

00:09:31,630 --> 00:09:36,220
group that we're kind of allowing to

00:09:34,480 --> 00:09:38,949
cooperate together to act as one single

00:09:36,220 --> 00:09:42,430
big key value store um

00:09:38,949 --> 00:09:44,589
so each one of them you know has a is

00:09:42,430 --> 00:09:45,910
basically holding data for long-term

00:09:44,589 --> 00:09:47,949
storage that is getting from the

00:09:45,910 --> 00:09:49,570
transaction logs and it's serving reads

00:09:47,949 --> 00:09:54,250
re requests that are coming in from the

00:09:49,570 --> 00:09:56,920
users basically the entire system is

00:09:54,250 --> 00:09:59,680
designed around making these guys job as

00:09:56,920 --> 00:10:00,940
easy as possible and to simplify that so

00:09:59,680 --> 00:10:03,910
because they're the bulk of the system

00:10:00,940 --> 00:10:06,010
the transaction logs basically are set

00:10:03,910 --> 00:10:07,329
up in such a way to make the the storage

00:10:06,010 --> 00:10:11,230
servers have to do as little as possible

00:10:07,329 --> 00:10:12,490
when they're ingesting writes and like

00:10:11,230 --> 00:10:14,260
the clients and how we're doing our

00:10:12,490 --> 00:10:18,819
reads are also trying to do as few reads

00:10:14,260 --> 00:10:22,329
as possible and download for them so to

00:10:18,819 --> 00:10:24,069
that end when were the transaction logs

00:10:22,329 --> 00:10:26,230
are sending data to the storage servers

00:10:24,069 --> 00:10:30,310
the way we do this is we have every

00:10:26,230 --> 00:10:33,430
storage server has a buddy transaction

00:10:30,310 --> 00:10:35,260
log and that storage server is gonna get

00:10:33,430 --> 00:10:38,410
all the data that it's responsible for

00:10:35,260 --> 00:10:40,480
from that one location so basically it's

00:10:38,410 --> 00:10:43,000
getting an exact stream of writes that

00:10:40,480 --> 00:10:46,029
are specifically designated to to this

00:10:43,000 --> 00:10:47,680
one specific location and so we'll we'll

00:10:46,029 --> 00:10:49,300
talk a little bit about how that happens

00:10:47,680 --> 00:10:50,470
but we're doing it for efficiency sake

00:10:49,300 --> 00:10:52,360
because these storage servers are only

00:10:50,470 --> 00:10:56,290
talking to one other server it's super

00:10:52,360 --> 00:10:58,120
efficient so now we can start from here

00:10:56,290 --> 00:10:59,939
and start building adding on the

00:10:58,120 --> 00:11:02,079
stateless components onto the system and

00:10:59,939 --> 00:11:04,149
the first one I'll talk about is the

00:11:02,079 --> 00:11:06,190
cluster controller so the cluster

00:11:04,149 --> 00:11:09,460
controller is a leader that

00:11:06,190 --> 00:11:12,490
selected by the coordinators and its job

00:11:09,460 --> 00:11:15,460
is basically to organize the all of the

00:11:12,490 --> 00:11:18,880
processes in the cluster into the full

00:11:15,460 --> 00:11:20,290
system so basically when every process

00:11:18,880 --> 00:11:21,490
starts up it's going to talk to the

00:11:20,290 --> 00:11:23,110
coordinators to figure out who the

00:11:21,490 --> 00:11:24,790
cluster controller is and then it's

00:11:23,110 --> 00:11:27,430
going to register itself and kind of

00:11:24,790 --> 00:11:29,380
whatever information it knows about the

00:11:27,430 --> 00:11:32,050
process to that cluster controller so

00:11:29,380 --> 00:11:34,900
it's going to say I have a disk I prefer

00:11:32,050 --> 00:11:36,790
to be a storage server you know and this

00:11:34,900 --> 00:11:38,380
is my information so the cluster

00:11:36,790 --> 00:11:39,880
controller is then going to take all of

00:11:38,380 --> 00:11:41,290
the workers that have registered with

00:11:39,880 --> 00:11:42,370
the system and it's going to start

00:11:41,290 --> 00:11:44,080
assigning them to do these different

00:11:42,370 --> 00:11:47,200
roles well you become a transaction log

00:11:44,080 --> 00:11:48,610
you become a storage server the cluster

00:11:47,200 --> 00:11:50,200
controller in addition to like giving

00:11:48,610 --> 00:11:53,350
out roles is also doing failure

00:11:50,200 --> 00:11:55,450
monitoring so it's going to track it's

00:11:53,350 --> 00:11:57,520
going to be basically talking to these

00:11:55,450 --> 00:11:59,770
processes continually to determine if

00:11:57,520 --> 00:12:05,050
they fail and we'll get into failures a

00:11:59,770 --> 00:12:06,730
little bit later so before I can add

00:12:05,050 --> 00:12:11,290
more boxes and trust me there's more

00:12:06,730 --> 00:12:13,210
boxes we have to take a little break to

00:12:11,290 --> 00:12:16,930
talk a little bit about how foundation

00:12:13,210 --> 00:12:20,160
DB provides consistency so I mentioned

00:12:16,930 --> 00:12:23,560
strict serializability before um and so

00:12:20,160 --> 00:12:26,230
to provide that we first need serialize

00:12:23,560 --> 00:12:28,600
ability and in foundation DB serialize

00:12:26,230 --> 00:12:31,150
ill serialize ability is explicit um

00:12:28,600 --> 00:12:32,890
every single commit you do is given a

00:12:31,150 --> 00:12:34,810
version number and you can just observe

00:12:32,890 --> 00:12:36,520
the serialize ability by comparing

00:12:34,810 --> 00:12:37,960
version numbers you know this commit

00:12:36,520 --> 00:12:39,730
committed after the other one or

00:12:37,960 --> 00:12:42,520
happened after another one if it's

00:12:39,730 --> 00:12:45,460
committed was higher so it's right there

00:12:42,520 --> 00:12:46,780
for you to see um it gets a little bit

00:12:45,460 --> 00:12:48,520
more complicated when you want strict

00:12:46,780 --> 00:12:52,270
serialize ability and basically what we

00:12:48,520 --> 00:12:54,010
need to do here is make it so that when

00:12:52,270 --> 00:12:55,300
we commit it's like our whole

00:12:54,010 --> 00:12:57,490
transaction happened at the

00:12:55,300 --> 00:12:59,650
instantaneous moment in time that the

00:12:57,490 --> 00:13:01,780
transaction was committed and the

00:12:59,650 --> 00:13:04,300
foundation DB approach here is basically

00:13:01,780 --> 00:13:07,150
when you start a transaction you get a

00:13:04,300 --> 00:13:09,670
read version which is basically the

00:13:07,150 --> 00:13:12,700
latest commit version of the the system

00:13:09,670 --> 00:13:14,500
has committed previously once you have

00:13:12,700 --> 00:13:16,540
that version for the rest of your

00:13:14,500 --> 00:13:18,040
transaction all your reads are stuck at

00:13:16,540 --> 00:13:19,269
that moment in time in

00:13:18,040 --> 00:13:21,040
the pass for when you started your

00:13:19,269 --> 00:13:22,540
transaction and so the data you're

00:13:21,040 --> 00:13:24,310
reading from the database it's gonna be

00:13:22,540 --> 00:13:25,810
you know a little bit old it's not gonna

00:13:24,310 --> 00:13:28,209
see the newer stuff that's gonna come in

00:13:25,810 --> 00:13:32,019
it's just it's just stuck at that

00:13:28,209 --> 00:13:34,750
consistent point in time um and then

00:13:32,019 --> 00:13:37,779
when you finally go to write the whole

00:13:34,750 --> 00:13:39,819
concept here is that if none of the keys

00:13:37,779 --> 00:13:41,680
that you read changed in this time

00:13:39,819 --> 00:13:42,970
interval before between when you started

00:13:41,680 --> 00:13:47,740
your transaction and when you eventually

00:13:42,970 --> 00:13:52,060
committed then it's actually like you

00:13:47,740 --> 00:13:53,889
did your it's then it's actually like

00:13:52,060 --> 00:13:55,209
you did all your reads at the final

00:13:53,889 --> 00:13:56,800
commit Bergeon it's like you're at an

00:13:55,209 --> 00:13:58,600
instantaneous point in time so that's

00:13:56,800 --> 00:14:02,350
the optimistic concurrency model at

00:13:58,600 --> 00:14:04,209
foundation DB and as a consequence if

00:14:02,350 --> 00:14:05,589
someone changes one of the keys you read

00:14:04,209 --> 00:14:07,120
in this time interval in this short

00:14:05,589 --> 00:14:09,220
window between your read version and

00:14:07,120 --> 00:14:14,050
your commit Bergeon um we fail your

00:14:09,220 --> 00:14:17,019
transaction as a client you retry so the

00:14:14,050 --> 00:14:18,490
layers team you know is kind of having

00:14:17,019 --> 00:14:21,240
to deal a lot and you're gonna hear a

00:14:18,490 --> 00:14:23,230
talk later by Alec that like this is

00:14:21,240 --> 00:14:25,000
something that you have to be very

00:14:23,230 --> 00:14:28,529
conscious of that happens in foundation

00:14:25,000 --> 00:14:28,529
DBA and you have to work around it okay

00:14:29,010 --> 00:14:37,720
back to the boxes we now have an API and

00:14:34,949 --> 00:14:39,850
you can see that there's now a master

00:14:37,720 --> 00:14:42,100
and proxies and resolvers added to the

00:14:39,850 --> 00:14:43,930
diagram and these components are all

00:14:42,100 --> 00:14:47,319
basically the stateless components that

00:14:43,930 --> 00:14:49,470
are here to implement the the basically

00:14:47,319 --> 00:14:51,750
the consistency model I just described

00:14:49,470 --> 00:14:53,800
so let's go through the different

00:14:51,750 --> 00:14:56,019
operations you can do on foundation DB

00:14:53,800 --> 00:15:00,639
and see what happens and see what

00:14:56,019 --> 00:15:04,300
happens so we'll start in with reads as

00:15:00,639 --> 00:15:06,040
mentioned reads are going directly to

00:15:04,300 --> 00:15:08,290
the store servers that are responsible

00:15:06,040 --> 00:15:10,720
for those range of keys so if you read

00:15:08,290 --> 00:15:12,010
key a there's going to be some set if

00:15:10,720 --> 00:15:14,410
your triple replicated there's going to

00:15:12,010 --> 00:15:17,350
be some set of three servers in the

00:15:14,410 --> 00:15:18,790
system that are that have that data and

00:15:17,350 --> 00:15:20,380
you're going to talk directly to one of

00:15:18,790 --> 00:15:22,209
those servers and the storage server is

00:15:20,380 --> 00:15:23,800
just going to give you an answer if you

00:15:22,209 --> 00:15:25,209
remember from the previous slide your

00:15:23,800 --> 00:15:27,069
reads are versioned right so you're

00:15:25,209 --> 00:15:28,569
passing in a read version with your

00:15:27,069 --> 00:15:30,880
thing and the storage server has to give

00:15:28,569 --> 00:15:32,529
you the value of that key at that moment

00:15:30,880 --> 00:15:35,109
in time so we're reading

00:15:32,529 --> 00:15:37,029
at version 200 and the storage server

00:15:35,109 --> 00:15:39,789
basically is going to keep some history

00:15:37,029 --> 00:15:44,319
in memory of recent commits and it'll

00:15:39,789 --> 00:15:46,449
give us a at that version uh D this

00:15:44,319 --> 00:15:48,099
comes back to one of kind of the

00:15:46,449 --> 00:15:49,749
constraints of foundation DB that you

00:15:48,099 --> 00:15:52,779
probably already all know about which is

00:15:49,749 --> 00:15:54,729
the five-second transaction limit the

00:15:52,779 --> 00:15:56,229
there's there's two places I'll mention

00:15:54,729 --> 00:15:59,079
the five-second limit one of them is

00:15:56,229 --> 00:16:01,569
right here because it's currently the

00:15:59,079 --> 00:16:03,999
storage server is keeping the recent

00:16:01,569 --> 00:16:05,229
history in memory and so to limit the

00:16:03,999 --> 00:16:07,779
amount of memory used by the storage

00:16:05,229 --> 00:16:09,459
servers we restrict like how much

00:16:07,779 --> 00:16:14,049
history we keep so we only keep five

00:16:09,459 --> 00:16:18,699
seconds so the other thing to mention

00:16:14,049 --> 00:16:21,609
here is this metadata related to which

00:16:18,699 --> 00:16:24,069
storage servers have which keys because

00:16:21,609 --> 00:16:25,689
as a client you need to know who to talk

00:16:24,069 --> 00:16:28,449
to you to give you any answer to any

00:16:25,689 --> 00:16:30,429
video individual query um so this this

00:16:28,449 --> 00:16:31,809
like mapping between keys and the

00:16:30,429 --> 00:16:34,059
storage server is responsible for them

00:16:31,809 --> 00:16:37,359
is state that's actually held and a lot

00:16:34,059 --> 00:16:40,059
of components in the system the client

00:16:37,359 --> 00:16:41,889
keeps a cache of these locations and if

00:16:40,059 --> 00:16:43,720
it doesn't have an answer if it doesn't

00:16:41,889 --> 00:16:46,389
know at any given moment whose response

00:16:43,720 --> 00:16:48,789
for keys it's going to ask the proxy for

00:16:46,389 --> 00:16:50,979
for that information so the proxy keeps

00:16:48,789 --> 00:16:54,099
the entire map and the client will send

00:16:50,979 --> 00:17:01,149
a request over the proxy saying who's

00:16:54,099 --> 00:17:03,369
responsible for key a the if the if we

00:17:01,149 --> 00:17:05,649
move if we shift responsibility of a

00:17:03,369 --> 00:17:08,049
shard the clients cache could be

00:17:05,649 --> 00:17:09,429
invalidated because the the person it

00:17:08,049 --> 00:17:13,779
previously thought was responsible the

00:17:09,429 --> 00:17:15,100
keys is now shifted and in that case the

00:17:13,779 --> 00:17:17,439
storage server itself will tell the

00:17:15,100 --> 00:17:19,329
client that the data's have the data is

00:17:17,439 --> 00:17:21,759
moved the client I'll invalidate its

00:17:19,329 --> 00:17:24,899
cache and it'll regrab the answer from

00:17:21,759 --> 00:17:27,369
the proxy to get the correct location um

00:17:24,899 --> 00:17:30,419
this state this mapping is actually

00:17:27,369 --> 00:17:32,590
stored in the database itself um and

00:17:30,419 --> 00:17:34,269
what's kind of a really cool and

00:17:32,590 --> 00:17:36,970
interesting design so it's the

00:17:34,269 --> 00:17:39,639
foundation to be the byte FF is the

00:17:36,970 --> 00:17:43,090
system key prefix and it stores system

00:17:39,639 --> 00:17:45,220
metadata in the database there um and to

00:17:43,090 --> 00:17:46,280
change shard responsibility to give

00:17:45,220 --> 00:17:48,230
ownership of a key

00:17:46,280 --> 00:17:49,340
from one set of servers to another we

00:17:48,230 --> 00:17:51,230
actually accomplish this by just

00:17:49,340 --> 00:17:53,780
committing transactions to the database

00:17:51,230 --> 00:17:55,670
itself um so you can it's a two phase

00:17:53,780 --> 00:17:57,800
protocol where you say like I'm

00:17:55,670 --> 00:17:59,630
intending to move this range to some

00:17:57,800 --> 00:18:01,040
other location and then once that

00:17:59,630 --> 00:18:02,360
location has copied all the data then

00:18:01,040 --> 00:18:03,620
you do another one saying this these

00:18:02,360 --> 00:18:06,350
these servers are taking over

00:18:03,620 --> 00:18:08,900
responsibility um so this data stryn

00:18:06,350 --> 00:18:10,880
algorithm is is you know responsible for

00:18:08,900 --> 00:18:12,710
a lot of the load balancing that I was

00:18:10,880 --> 00:18:15,260
mentioning earlier that we get out of

00:18:12,710 --> 00:18:16,940
foundation DB so basically this we're

00:18:15,260 --> 00:18:18,500
constantly kind of monitoring how much

00:18:16,940 --> 00:18:21,140
work each of the very sort servers are

00:18:18,500 --> 00:18:22,640
doing and shifting responsibility around

00:18:21,140 --> 00:18:25,070
by executing transactions on the

00:18:22,640 --> 00:18:27,620
database kind of giving ownership of

00:18:25,070 --> 00:18:29,240
keys so like if you added a completely

00:18:27,620 --> 00:18:30,980
empty new process to a foundation you'd

00:18:29,240 --> 00:18:32,450
be cluster what's gonna happen is the

00:18:30,980 --> 00:18:34,280
data distribution algorithm is gonna

00:18:32,450 --> 00:18:36,710
notice there's now a new process in the

00:18:34,280 --> 00:18:38,960
system and it'll just start giving it

00:18:36,710 --> 00:18:43,450
key ranges and key ranges until it's

00:18:38,960 --> 00:18:43,450
part it's like taking a some traffic

00:18:44,080 --> 00:18:52,370
okay although a lot on reads or I want

00:18:49,010 --> 00:18:54,140
to commits so rights and foundation DB

00:18:52,370 --> 00:18:55,580
are actually just cashed up on clients

00:18:54,140 --> 00:18:59,060
so there's nothing really to talk about

00:18:55,580 --> 00:19:00,320
there and commits are basically going to

00:18:59,060 --> 00:19:02,870
take everything you did in your

00:19:00,320 --> 00:19:04,790
transaction bundle them up together and

00:19:02,870 --> 00:19:07,430
send them as a single unit - one of the

00:19:04,790 --> 00:19:09,110
proxies so in this case you're gonna

00:19:07,430 --> 00:19:11,090
you're going to include the the version

00:19:09,110 --> 00:19:13,250
that you did you read that every key

00:19:11,090 --> 00:19:14,990
that you read and then every mutation

00:19:13,250 --> 00:19:16,970
you're attempting to write and package

00:19:14,990 --> 00:19:18,410
that up together and the very first

00:19:16,970 --> 00:19:20,510
thing the proxy is going to do with that

00:19:18,410 --> 00:19:22,910
information is a sign that that

00:19:20,510 --> 00:19:25,640
transaction a commit version so in this

00:19:22,910 --> 00:19:27,080
case we did our reads at 200 and the

00:19:25,640 --> 00:19:29,990
master is going to tell us that we're

00:19:27,080 --> 00:19:31,850
committing at 400 so the masters job

00:19:29,990 --> 00:19:33,950
it's the only singleton in the pipeline

00:19:31,850 --> 00:19:37,010
that's that's not scaled out it are only

00:19:33,950 --> 00:19:38,990
a single box here and it's entire job is

00:19:37,010 --> 00:19:43,730
just to give larger and larger commit

00:19:38,990 --> 00:19:45,320
versions back to the user so I it even

00:19:43,730 --> 00:19:48,040
though it's a singleton it'll never be a

00:19:45,320 --> 00:19:51,530
scalability limit to a cluster um

00:19:48,040 --> 00:19:53,210
because it has such a simple job and as

00:19:51,530 --> 00:19:55,040
you scale up the system we actually

00:19:53,210 --> 00:19:57,380
combine different transactions together

00:19:55,040 --> 00:19:58,790
into batches and the master is only

00:19:57,380 --> 00:19:59,720
giving a single version number to an

00:19:58,790 --> 00:20:01,490
entire batch

00:19:59,720 --> 00:20:03,919
actions so basically even if you hammer

00:20:01,490 --> 00:20:05,330
the database super hard the Masters is

00:20:03,919 --> 00:20:08,690
not really going to sweat giving out

00:20:05,330 --> 00:20:10,789
these version numbers so once you have a

00:20:08,690 --> 00:20:12,549
commit version you're now ready to do

00:20:10,789 --> 00:20:15,470
what I was saying on the previous slide

00:20:12,549 --> 00:20:17,270
and detect to see if anything you read

00:20:15,470 --> 00:20:18,740
in this transaction changed between your

00:20:17,270 --> 00:20:21,559
read version and your commit version and

00:20:18,740 --> 00:20:24,830
there's that is being done on the

00:20:21,559 --> 00:20:27,260
resolvers so basically the this is this

00:20:24,830 --> 00:20:28,850
is another location where we get back to

00:20:27,260 --> 00:20:31,400
the five second transaction limit

00:20:28,850 --> 00:20:34,100
because the resolver a stateless role

00:20:31,400 --> 00:20:36,110
that are that are storing the previous

00:20:34,100 --> 00:20:39,289
five seconds of history of reads and

00:20:36,110 --> 00:20:40,760
commits a reads and writes and it will

00:20:39,289 --> 00:20:43,309
basically be able to tell you for any

00:20:40,760 --> 00:20:47,120
given read if it's changed in this in a

00:20:43,309 --> 00:20:49,789
time range so one important thing to

00:20:47,120 --> 00:20:51,230
note about resolvers is they're sharded

00:20:49,789 --> 00:20:53,900
bike when you add multiple of them

00:20:51,230 --> 00:20:55,880
they're sharded by keys key range and so

00:20:53,900 --> 00:20:58,220
you're going to give your gonna split up

00:20:55,880 --> 00:21:00,020
your your reads and writes to according

00:20:58,220 --> 00:21:01,669
to the key ranges of those resolvers and

00:21:00,020 --> 00:21:03,500
send the just the specific data for

00:21:01,669 --> 00:21:06,620
those that that resolver is responsible

00:21:03,500 --> 00:21:08,480
for to those locations what this means

00:21:06,620 --> 00:21:10,490
though is that as you add more respond

00:21:08,480 --> 00:21:12,830
more and more resolvers to a system

00:21:10,490 --> 00:21:16,360
you're doing a little bit of conflict

00:21:12,830 --> 00:21:19,159
implicit amplify amplify Haitian

00:21:16,360 --> 00:21:20,419
basically the two resolvers don't know

00:21:19,159 --> 00:21:21,590
about the decisions the other one the

00:21:20,419 --> 00:21:23,570
resolver z' don't know about the

00:21:21,590 --> 00:21:25,490
decisions other solvers are making so if

00:21:23,570 --> 00:21:27,470
one of the resolvers fails a transaction

00:21:25,490 --> 00:21:29,990
and the other thinks it succeeds well

00:21:27,470 --> 00:21:31,460
the what the transaction was failed but

00:21:29,990 --> 00:21:33,710
the one that thought it was successful

00:21:31,460 --> 00:21:35,150
is gonna fail future transactions based

00:21:33,710 --> 00:21:36,710
on the rights that happened in that one

00:21:35,150 --> 00:21:39,890
even though they didn't actually apply

00:21:36,710 --> 00:21:43,669
to the database in general this is not a

00:21:39,890 --> 00:21:45,440
huge issue and foundation DB as you as

00:21:43,669 --> 00:21:47,360
I've mentioned already you already have

00:21:45,440 --> 00:21:48,559
to design your clients to avoid high

00:21:47,360 --> 00:21:52,100
contention workloads that are doing a

00:21:48,559 --> 00:21:53,570
lot of conflicts anyways but in general

00:21:52,100 --> 00:21:55,010
it means that you don't really want to

00:21:53,570 --> 00:21:56,510
scale out resolver you don't want to

00:21:55,010 --> 00:21:59,390
just configure 100 of them right off the

00:21:56,510 --> 00:22:02,480
bat start slow and scale up to just as

00:21:59,390 --> 00:22:04,100
many as you need so assuming the

00:22:02,480 --> 00:22:06,590
resolvers haven't found any problems

00:22:04,100 --> 00:22:09,350
it's now we can now finally make the

00:22:06,590 --> 00:22:11,539
transaction durable so the proxy is then

00:22:09,350 --> 00:22:12,060
is now going to ship the mutations the

00:22:11,539 --> 00:22:13,590
right

00:22:12,060 --> 00:22:17,780
from this transaction to the transaction

00:22:13,590 --> 00:22:20,970
logs and so as mentioned previously

00:22:17,780 --> 00:22:22,890
every transaction log or every storage

00:22:20,970 --> 00:22:24,510
server has exactly one transaction log

00:22:22,890 --> 00:22:27,330
that it's going to get all this data

00:22:24,510 --> 00:22:29,850
from so if we're writing key B here

00:22:27,330 --> 00:22:31,740
there's some team of storage servers at

00:22:29,850 --> 00:22:34,920
the end of this pipeline that need that

00:22:31,740 --> 00:22:37,350
that mutation that changes key B and so

00:22:34,920 --> 00:22:39,870
those servers are mapped to some

00:22:37,350 --> 00:22:41,640
transaction logs and so the proxy

00:22:39,870 --> 00:22:43,800
basically knows all these mappings and

00:22:41,640 --> 00:22:45,510
it's going to send the change to key B

00:22:43,800 --> 00:22:48,090
to the set of transaction logs that'll

00:22:45,510 --> 00:22:50,270
eventually get the data over to the the

00:22:48,090 --> 00:22:52,650
sort servers responsible for it um

00:22:50,270 --> 00:22:55,910
because there's way less transaction

00:22:52,650 --> 00:22:58,560
logs and there are storage servers the

00:22:55,910 --> 00:23:02,790
you could just happen to get lucky or

00:22:58,560 --> 00:23:04,560
unlucky that all the locations that want

00:23:02,790 --> 00:23:05,550
key B are just one transaction log

00:23:04,560 --> 00:23:07,470
that's all you know

00:23:05,550 --> 00:23:09,570
and in that case we actually have to

00:23:07,470 --> 00:23:11,370
additionally replicate key B onto some

00:23:09,570 --> 00:23:13,080
other transaction logs so that we're

00:23:11,370 --> 00:23:15,600
safe against failures in the transaction

00:23:13,080 --> 00:23:17,280
logging subsystem so there's a little

00:23:15,600 --> 00:23:18,480
bit of complicated logic the proxy does

00:23:17,280 --> 00:23:20,720
to figure out where to store the

00:23:18,480 --> 00:23:23,310
mutations in the transaction subsystem

00:23:20,720 --> 00:23:24,990
once the data has been F synced on to

00:23:23,310 --> 00:23:28,020
the on those transaction logs and come

00:23:24,990 --> 00:23:29,910
in durable made durable there we finally

00:23:28,020 --> 00:23:31,980
can return success out to the back to

00:23:29,910 --> 00:23:33,300
the user I mean behind the scenes the

00:23:31,980 --> 00:23:37,530
data is going to be replicated over to

00:23:33,300 --> 00:23:39,510
the storage servers so finally I can

00:23:37,530 --> 00:23:42,600
wrap all the way back around to a get

00:23:39,510 --> 00:23:44,490
read version request and get reversion

00:23:42,600 --> 00:23:46,950
requests are responsible for giving us

00:23:44,490 --> 00:23:50,630
how we provide them or responsible for

00:23:46,950 --> 00:23:53,580
how we provide external consistency so

00:23:50,630 --> 00:23:56,160
the concept here is that when you start

00:23:53,580 --> 00:23:58,230
a transaction it's a very nice property

00:23:56,160 --> 00:24:00,660
that you'll see every commit that's ever

00:23:58,230 --> 00:24:03,270
previously happened on the system so in

00:24:00,660 --> 00:24:05,730
in this specific example like we

00:24:03,270 --> 00:24:07,740
previously committed to one of the

00:24:05,730 --> 00:24:09,720
proxies and now we if we immediately

00:24:07,740 --> 00:24:11,520
start another transaction that talks to

00:24:09,720 --> 00:24:13,260
a different proxy we want to make sure

00:24:11,520 --> 00:24:15,150
that this new transaction sees the

00:24:13,260 --> 00:24:17,610
result of the previous one we just did

00:24:15,150 --> 00:24:19,200
so the way that we do this is that when

00:24:17,610 --> 00:24:21,180
we talk to one of the proxies for a read

00:24:19,200 --> 00:24:22,710
version that proxy will send a message

00:24:21,180 --> 00:24:24,450
to the other proxies asking if they've

00:24:22,710 --> 00:24:25,510
seen any versions that are higher than

00:24:24,450 --> 00:24:27,640
the one

00:24:25,510 --> 00:24:29,830
we have locally and we'll just take a

00:24:27,640 --> 00:24:32,530
max of all those responses and send it

00:24:29,830 --> 00:24:35,320
back to the client this may at first

00:24:32,530 --> 00:24:37,630
glance seem like a scalability problem

00:24:35,320 --> 00:24:39,340
because every proxy is basically talking

00:24:37,630 --> 00:24:41,920
to every other proxy exchanging these

00:24:39,340 --> 00:24:45,130
versions however we're saved here by

00:24:41,920 --> 00:24:47,110
batching we basically can we don't have

00:24:45,130 --> 00:24:48,880
to do these reversion requests for every

00:24:47,110 --> 00:24:50,650
single transaction we can group up a lot

00:24:48,880 --> 00:24:52,990
of different requests together and for a

00:24:50,650 --> 00:24:54,700
whole bunch of them we can send it send

00:24:52,990 --> 00:24:56,260
these requests to the other proxies so

00:24:54,700 --> 00:24:58,000
really we're only sending these messages

00:24:56,260 --> 00:25:05,710
between proxies every 18 a millisecond

00:24:58,000 --> 00:25:08,200
or so okay so uh that's kind of takes

00:25:05,710 --> 00:25:11,050
you takes that's the that's the first

00:25:08,200 --> 00:25:14,080
part we've gotten through how you how

00:25:11,050 --> 00:25:16,350
you do commit how Trent like how

00:25:14,080 --> 00:25:19,000
transactions flow in foundation DV um

00:25:16,350 --> 00:25:21,130
however if we wrap all the way back to

00:25:19,000 --> 00:25:24,490
what I originally said a huge part of

00:25:21,130 --> 00:25:25,900
this design that that we've a trade-off

00:25:24,490 --> 00:25:27,280
that we've made is in relation to

00:25:25,900 --> 00:25:30,340
failures

00:25:27,280 --> 00:25:32,440
so basically we're having to explicitly

00:25:30,340 --> 00:25:33,610
handle like failures and foundation DV

00:25:32,440 --> 00:25:35,830
because we don't have the implicit

00:25:33,610 --> 00:25:38,890
failure handling that a chorim's give

00:25:35,830 --> 00:25:41,560
you so now I'm gonna take you a little

00:25:38,890 --> 00:25:45,340
bit through through what happens when a

00:25:41,560 --> 00:25:46,540
process fails in foundation DB so I'm

00:25:45,340 --> 00:25:49,570
gonna start with the target with the

00:25:46,540 --> 00:25:52,690
hardest one in this case in which case a

00:25:49,570 --> 00:25:54,820
transaction log dies and so this is

00:25:52,690 --> 00:25:56,500
gonna kick off a recovery process so the

00:25:54,820 --> 00:25:58,720
cluster controller is going to detect

00:25:56,500 --> 00:26:00,040
that this server has gone down and the

00:25:58,720 --> 00:26:02,740
first thing it's going to do is

00:26:00,040 --> 00:26:04,960
basically attempt to recruit an entire

00:26:02,740 --> 00:26:07,090
replacement for the entire transaction

00:26:04,960 --> 00:26:10,140
subsystem that's going to basically take

00:26:07,090 --> 00:26:14,020
over from the previous generation

00:26:10,140 --> 00:26:16,570
basically like storing the data in this

00:26:14,020 --> 00:26:19,330
other coordination database the court in

00:26:16,570 --> 00:26:21,310
ethic on the coordinators so this is

00:26:19,330 --> 00:26:23,200
using this this right is going to use

00:26:21,310 --> 00:26:24,760
Paxos and we're doing this because it

00:26:23,200 --> 00:26:26,260
would be an absolute disaster if we

00:26:24,760 --> 00:26:29,320
could have two different masters

00:26:26,260 --> 00:26:30,700
takeover simultaneously like any

00:26:29,320 --> 00:26:34,480
consistent system is going to need a

00:26:30,700 --> 00:26:36,550
two-phase commit somewhere so the first

00:26:34,480 --> 00:26:37,990
step is that you know the the first

00:26:36,550 --> 00:26:38,980
phase of our two-phase commit where

00:26:37,990 --> 00:26:40,280
we're going to do a read from the

00:26:38,980 --> 00:26:43,429
coordinator

00:26:40,280 --> 00:26:45,020
to find out who exactly was who were

00:26:43,429 --> 00:26:47,840
exactly were the transaction logs of the

00:26:45,020 --> 00:26:49,610
previous system this new master is then

00:26:47,840 --> 00:26:51,860
going to talk to those old transaction

00:26:49,610 --> 00:26:55,940
logs and find out the final version that

00:26:51,860 --> 00:26:59,630
was committed to them in this case were

00:26:55,940 --> 00:27:01,100
the the one that died had version 400

00:26:59,630 --> 00:27:02,900
and the other ones that were alive

00:27:01,100 --> 00:27:05,360
actually got another commit after that

00:27:02,900 --> 00:27:09,380
there's 410 and 420 so they have even

00:27:05,360 --> 00:27:11,630
more recent data for because the commits

00:27:09,380 --> 00:27:13,340
had to go to every single log the only

00:27:11,630 --> 00:27:15,440
version here that actually was returned

00:27:13,340 --> 00:27:19,700
commit success back to a client would

00:27:15,440 --> 00:27:23,179
have only been version 400 and so but

00:27:19,700 --> 00:27:27,200
we're allowed to take any version after

00:27:23,179 --> 00:27:29,059
that so 410 or 420 is also fine it's

00:27:27,200 --> 00:27:32,990
sort of like a client it doesn't know

00:27:29,059 --> 00:27:34,940
the result of those transactions um so

00:27:32,990 --> 00:27:36,740
the master neat so when it's asking for

00:27:34,940 --> 00:27:38,210
these versions the master is also

00:27:36,740 --> 00:27:39,620
locking these transaction locks to

00:27:38,210 --> 00:27:40,909
prevent them from accepting any

00:27:39,620 --> 00:27:43,669
additional rights in the future

00:27:40,909 --> 00:27:44,990
basically preventing us from recruiting

00:27:43,669 --> 00:27:48,169
new logs and having the old logs

00:27:44,990 --> 00:27:50,330
continue accepting rights um and the

00:27:48,169 --> 00:27:51,710
next so them so the master is then going

00:27:50,330 --> 00:27:53,120
to pick kind of the lowest of the

00:27:51,710 --> 00:27:56,179
responses it gets that from that from

00:27:53,120 --> 00:27:58,640
these guys and command take that and

00:27:56,179 --> 00:28:00,320
basically tell the next generation that

00:27:58,640 --> 00:28:04,789
they're taking ownership of the days

00:28:00,320 --> 00:28:07,190
starting at that moment so the master is

00:28:04,789 --> 00:28:09,590
then gonna basically start up all of the

00:28:07,190 --> 00:28:11,539
other systems at that moment in time so

00:28:09,590 --> 00:28:13,820
the proxies were going to recover that

00:28:11,539 --> 00:28:16,159
key to server look mapping as of that

00:28:13,820 --> 00:28:17,539
version and the transaction logs

00:28:16,159 --> 00:28:20,360
actually have to do a little bit of

00:28:17,539 --> 00:28:23,419
copying as well and this the reason for

00:28:20,360 --> 00:28:25,580
that is actually really subtle so back

00:28:23,419 --> 00:28:27,260
on the previous step I said that we see

00:28:25,580 --> 00:28:30,020
that this transaction log with version

00:28:27,260 --> 00:28:31,580
400 is dead it's gone right but that

00:28:30,020 --> 00:28:32,840
might actually just be a temporary

00:28:31,580 --> 00:28:34,640
failure it could just be a little

00:28:32,840 --> 00:28:37,010
network hiccup that triggered this

00:28:34,640 --> 00:28:38,929
recovery um and if we have three servers

00:28:37,010 --> 00:28:41,450
and we're triple replicated we actually

00:28:38,929 --> 00:28:44,480
want to be safe against two failures so

00:28:41,450 --> 00:28:46,370
what can happen here is that after we've

00:28:44,480 --> 00:28:49,159
committed to a recovery version of 410

00:28:46,370 --> 00:28:50,870
the two remaining guys who are alive can

00:28:49,159 --> 00:28:52,440
permanently fail and this one guy who

00:28:50,870 --> 00:28:54,990
was temporarily that at the store

00:28:52,440 --> 00:28:58,290
can come back alive and he only had and

00:28:54,990 --> 00:29:00,600
this this guy only has version 400 so we

00:28:58,290 --> 00:29:02,310
need to copy just a little bit of data

00:29:00,600 --> 00:29:04,020
from the previous generation to make

00:29:02,310 --> 00:29:05,820
sure that even if there's this crazy

00:29:04,020 --> 00:29:07,860
scenario of things dying and coming back

00:29:05,820 --> 00:29:09,360
alive we're still able to have all of

00:29:07,860 --> 00:29:12,450
the data up to the version that we

00:29:09,360 --> 00:29:14,340
committed to be our recovery version in

00:29:12,450 --> 00:29:16,170
any case once the transaction logs got

00:29:14,340 --> 00:29:18,300
this little bit of data we're finally

00:29:16,170 --> 00:29:19,920
ready to go with this next generation so

00:29:18,300 --> 00:29:21,360
the master is then gonna do the next

00:29:19,920 --> 00:29:23,400
phase of the two-phase commit to the

00:29:21,360 --> 00:29:25,350
coordinators kind of finally writing

00:29:23,400 --> 00:29:27,480
these new sets of transaction logs into

00:29:25,350 --> 00:29:29,210
the database saying that they're taking

00:29:27,480 --> 00:29:31,590
ownership from the previous generation

00:29:29,210 --> 00:29:32,670
after all of that happened we can

00:29:31,590 --> 00:29:34,200
finally tell the cluster controller

00:29:32,670 --> 00:29:36,090
we've taken over which can then

00:29:34,200 --> 00:29:42,390
basically open the floodgates of traffic

00:29:36,090 --> 00:29:45,660
onto the system again so that was the

00:29:42,390 --> 00:29:48,450
hard one if that same recovery process

00:29:45,660 --> 00:29:51,810
that I just went through is gonna happen

00:29:48,450 --> 00:29:56,790
even if a master resolve or a proxy dies

00:29:51,810 --> 00:29:58,350
the philosophy here is basically we know

00:29:56,790 --> 00:30:01,560
we have to make this one recovery

00:29:58,350 --> 00:30:03,810
algorithm really fast and so if we do

00:30:01,560 --> 00:30:05,910
the card case really well we can just

00:30:03,810 --> 00:30:07,320
use that hard case for any to handle any

00:30:05,910 --> 00:30:09,690
failures even if they were potentially a

00:30:07,320 --> 00:30:10,920
little easier um so eventually we may

00:30:09,690 --> 00:30:12,390
get around to writing specialized

00:30:10,920 --> 00:30:14,130
recovery logic for replacing some of

00:30:12,390 --> 00:30:15,450
these stateless roles but for now

00:30:14,130 --> 00:30:17,690
there's just one thing that the one

00:30:15,450 --> 00:30:20,520
process that happens if any of them die

00:30:17,690 --> 00:30:22,980
um if the cluster controller dies well

00:30:20,520 --> 00:30:25,560
that's really simple it's a leader

00:30:22,980 --> 00:30:27,420
elected by the coordinators and so it's

00:30:25,560 --> 00:30:29,730
heart beating constantly to those

00:30:27,420 --> 00:30:31,260
coordinators and so when it dies it'll

00:30:29,730 --> 00:30:32,880
stop heart beating and the coordinators

00:30:31,260 --> 00:30:36,630
will detect that and just to reflect a

00:30:32,880 --> 00:30:37,290
little electro replacement and even

00:30:36,630 --> 00:30:39,420
simpler than that

00:30:37,290 --> 00:30:41,160
if a coordinator dies well it's using

00:30:39,420 --> 00:30:42,660
quorum logic it has implicit failure

00:30:41,160 --> 00:30:44,850
handling nothing happens when it dies

00:30:42,660 --> 00:30:47,400
the user will probably want to replace

00:30:44,850 --> 00:30:49,350
it just to restore cual tolerance but

00:30:47,400 --> 00:30:52,770
it's there's no emergency

00:30:49,350 --> 00:30:54,240
and finally we wrap down back over to

00:30:52,770 --> 00:30:56,580
the storage servers which as you can

00:30:54,240 --> 00:30:58,650
recall or 90% of our processes in the

00:30:56,580 --> 00:31:00,810
system and one of one of these things

00:30:58,650 --> 00:31:03,630
dies there's basically no effect to the

00:31:00,810 --> 00:31:05,280
user clients will just automatically

00:31:03,630 --> 00:31:05,850
start sending requests to the other

00:31:05,280 --> 00:31:07,919
remaining

00:31:05,850 --> 00:31:10,200
responsible for the keys at that server

00:31:07,919 --> 00:31:12,570
had and that day distribution algorithm

00:31:10,200 --> 00:31:14,220
I mentioned earlier is going to start

00:31:12,570 --> 00:31:16,350
shifting a responsibility of all those

00:31:14,220 --> 00:31:19,260
key ranges on the failed note two other

00:31:16,350 --> 00:31:20,549
sets of a live servers so the system

00:31:19,260 --> 00:31:25,160
will just naturally heal from this

00:31:20,549 --> 00:31:25,160
problem note like with no problem

00:31:27,530 --> 00:31:34,020
so that's kind of the the gist of it I

00:31:31,860 --> 00:31:38,700
just have a few other points to talk

00:31:34,020 --> 00:31:40,980
about one of them is kind of a potala

00:31:38,700 --> 00:31:44,640
form of pathology that's specific just

00:31:40,980 --> 00:31:46,289
to foundation DB really um so as I've

00:31:44,640 --> 00:31:47,820
been talking this whole time we've

00:31:46,289 --> 00:31:51,840
mentioned a number of times this five

00:31:47,820 --> 00:31:56,130
second transaction limit and normally

00:31:51,840 --> 00:31:59,580
databases handle saturation performance

00:31:56,130 --> 00:32:02,100
by basically relying on back pressure to

00:31:59,580 --> 00:32:04,230
a so as as more and more people overload

00:32:02,100 --> 00:32:08,580
a server it'll just naturally slow down

00:32:04,230 --> 00:32:10,409
responses to those requests however in

00:32:08,580 --> 00:32:12,590
foundation DB because of a five-second

00:32:10,409 --> 00:32:15,419
limit that can lead to a death spiral

00:32:12,590 --> 00:32:17,190
the concept is like if every Reed you're

00:32:15,419 --> 00:32:18,450
doing if you're doing like five or 10

00:32:17,190 --> 00:32:21,419
serial reads in a row and your

00:32:18,450 --> 00:32:23,190
transaction if every reads starts taking

00:32:21,419 --> 00:32:24,570
a second well all of a sudden by the

00:32:23,190 --> 00:32:26,700
time you get to your last read you've

00:32:24,570 --> 00:32:28,200
passed the five-second limit so then

00:32:26,700 --> 00:32:29,640
you've done all of this work and all of

00:32:28,200 --> 00:32:31,230
these reads and you get to the end and

00:32:29,640 --> 00:32:33,750
you try and commit and you just fail and

00:32:31,230 --> 00:32:35,429
if every client starts doing this then

00:32:33,750 --> 00:32:37,770
basically no work is being done in the

00:32:35,429 --> 00:32:39,299
cluster it's just a disaster your index

00:32:37,770 --> 00:32:40,919
file everyone's hammering the server

00:32:39,299 --> 00:32:44,280
hammering the server's and no work is

00:32:40,919 --> 00:32:47,370
getting done so we saw this problem and

00:32:44,280 --> 00:32:49,620
our solution to it was this concept of

00:32:47,370 --> 00:32:52,140
rate keeper the idea is that when we're

00:32:49,620 --> 00:32:54,150
in saturation we're gonna build up all

00:32:52,140 --> 00:32:56,100
of the latency before we get the

00:32:54,150 --> 00:33:01,590
original read version to our request

00:32:56,100 --> 00:33:02,880
so basically back to the diagram when

00:33:01,590 --> 00:33:04,679
you start a transaction you're

00:33:02,880 --> 00:33:06,000
five-second limit really only starts

00:33:04,679 --> 00:33:07,500
from the moment you get a read version

00:33:06,000 --> 00:33:09,179
basically you have two commits within

00:33:07,500 --> 00:33:11,130
five seconds of getting that version so

00:33:09,179 --> 00:33:12,360
if we can basically slow down the rate

00:33:11,130 --> 00:33:15,059
at which you the clients are getting

00:33:12,360 --> 00:33:16,290
versions we can make it so that one even

00:33:15,059 --> 00:33:17,669
though it might take a few seconds to

00:33:16,290 --> 00:33:18,870
get a read version once I have one

00:33:17,669 --> 00:33:19,370
they'll be able to do all the rest of

00:33:18,870 --> 00:33:23,400
their

00:33:19,370 --> 00:33:25,020
operations with very low latency so that

00:33:23,400 --> 00:33:28,710
rate fear keeper components another

00:33:25,020 --> 00:33:30,900
singleton that lives on the master so

00:33:28,710 --> 00:33:32,520
the final thing to mention is sort of a

00:33:30,900 --> 00:33:36,870
theme of foundation DB and been

00:33:32,520 --> 00:33:38,850
highlighted it a little bit but uh you

00:33:36,870 --> 00:33:40,410
know obviously this design is really

00:33:38,850 --> 00:33:42,390
complicated there's a lot of little

00:33:40,410 --> 00:33:44,100
moving pieces there and there's a lot of

00:33:42,390 --> 00:33:48,090
edge cases I didn't even really get into

00:33:44,100 --> 00:33:49,620
in this architecture discussion so the

00:33:48,090 --> 00:33:51,270
question of the day comes back to well

00:33:49,620 --> 00:33:53,970
how can we trust or how can you guys

00:33:51,270 --> 00:33:55,500
trust that we actually don't have any

00:33:53,970 --> 00:33:57,270
bugs that just throw out this entire

00:33:55,500 --> 00:33:59,700
design like do we act you don't actually

00:33:57,270 --> 00:34:00,930
have acid guarantees if there's some bug

00:33:59,700 --> 00:34:04,560
that corrupts your data in some weird

00:34:00,930 --> 00:34:05,880
case um so basically the the founders

00:34:04,560 --> 00:34:08,130
recognized this basically from the

00:34:05,880 --> 00:34:10,560
get-go and that's why they started with

00:34:08,130 --> 00:34:12,960
simulation right because the the idea is

00:34:10,560 --> 00:34:14,460
this thing needs to be tested really

00:34:12,960 --> 00:34:16,560
severely so that we can actually trust

00:34:14,460 --> 00:34:18,720
that our inflammation our implementation

00:34:16,560 --> 00:34:22,100
of this design is meeting the guarantees

00:34:18,720 --> 00:34:24,419
we say it does so the way this works is

00:34:22,100 --> 00:34:25,980
kind of when I was going through this

00:34:24,419 --> 00:34:27,120
whole architecture right I was talking

00:34:25,980 --> 00:34:29,730
about all of these different processes

00:34:27,120 --> 00:34:31,110
in these boxes sort of in my mind and

00:34:29,730 --> 00:34:32,490
maybe in your mind you're thinking about

00:34:31,110 --> 00:34:34,470
them is happening on different machines

00:34:32,490 --> 00:34:36,810
or different processes in those machines

00:34:34,470 --> 00:34:39,000
but in actuality there's an indirection

00:34:36,810 --> 00:34:40,740
between the roles in the work that's

00:34:39,000 --> 00:34:43,200
happening and where it's happening in

00:34:40,740 --> 00:34:44,730
the cluster so it's actually you can

00:34:43,200 --> 00:34:47,220
actually start up a foundation to be

00:34:44,730 --> 00:34:49,679
like cluster on your laptop in a single

00:34:47,220 --> 00:34:51,389
process and that one process will do all

00:34:49,679 --> 00:34:53,610
of the work of all these roles right

00:34:51,389 --> 00:34:56,220
there locally in that one process what

00:34:53,610 --> 00:34:58,550
this allows us to do is if when we're

00:34:56,220 --> 00:35:01,740
running the whole system in one process

00:34:58,550 --> 00:35:04,140
we can actually have that single process

00:35:01,740 --> 00:35:06,570
pretend like it's an entire network so

00:35:04,140 --> 00:35:08,520
it could because it's all one process

00:35:06,570 --> 00:35:09,900
just instantly send messages you know

00:35:08,520 --> 00:35:11,670
from the proxies the resolver from the

00:35:09,900 --> 00:35:14,220
revolvers you know back to the proxy

00:35:11,670 --> 00:35:17,280
from the proxies of transaction log but

00:35:14,220 --> 00:35:18,810
instead of doing this instantaneously we

00:35:17,280 --> 00:35:20,730
actually pretend like there's latency

00:35:18,810 --> 00:35:23,010
between these components we drop some

00:35:20,730 --> 00:35:25,110
packets randomly we reorder things we

00:35:23,010 --> 00:35:27,420
pretend like whole system dies we

00:35:25,110 --> 00:35:29,280
pretend like there's corruption on disk

00:35:27,420 --> 00:35:31,020
basically every bad thing you could

00:35:29,280 --> 00:35:32,760
possibly think of we we do in this

00:35:31,020 --> 00:35:35,160
system

00:35:32,760 --> 00:35:37,140
and because it's an because it's a like

00:35:35,160 --> 00:35:39,930
single process that's doing this we have

00:35:37,140 --> 00:35:41,490
determinism so if we can run you know

00:35:39,930 --> 00:35:43,440
hunt we run hundreds of thousands of

00:35:41,490 --> 00:35:45,090
these things of these randomized tests

00:35:43,440 --> 00:35:48,180
every night pairing these random

00:35:45,090 --> 00:35:49,290
failures with some workload that's going

00:35:48,180 --> 00:35:51,840
to ship check some property of the

00:35:49,290 --> 00:35:54,300
database and when it comes back with an

00:35:51,840 --> 00:35:56,550
error we can in the next morning like

00:35:54,300 --> 00:35:58,920
replay the exact really rare series of

00:35:56,550 --> 00:36:01,110
events that caused this and kind of

00:35:58,920 --> 00:36:02,490
figure out what happened this is really

00:36:01,110 --> 00:36:04,500
powerful

00:36:02,490 --> 00:36:06,570
oh my eight years working on this

00:36:04,500 --> 00:36:08,430
database probably six of them has been

00:36:06,570 --> 00:36:13,610
sent tracking our problems down by this

00:36:08,430 --> 00:36:16,560
thing so it's it's really kind of the

00:36:13,610 --> 00:36:21,090
one of the big secrets sauces that makes

00:36:16,560 --> 00:36:24,540
foundation to be possible um so that's

00:36:21,090 --> 00:36:26,190
all I have for you guys before I step

00:36:24,540 --> 00:36:29,910
down I just want to say thank you to

00:36:26,190 --> 00:36:31,920
Dave Rosenthal and Dave shear they made

00:36:29,910 --> 00:36:33,690
a lot of hard decisions early on with

00:36:31,920 --> 00:36:35,460
like developing the simulator developing

00:36:33,690 --> 00:36:37,200
flow even through the developing this

00:36:35,460 --> 00:36:38,370
whole architecture basically at every

00:36:37,200 --> 00:36:40,110
moment they were making technical

00:36:38,370 --> 00:36:42,630
decisions early on they were focused on

00:36:40,110 --> 00:36:45,600
kind of the long-term sustainability or

00:36:42,630 --> 00:36:47,400
long-term success of this database and

00:36:45,600 --> 00:36:49,440
so now I'm reaping the benefits of the

00:36:47,400 --> 00:36:50,860
work they've laid out and early on so

00:36:49,440 --> 00:36:54,649
thank you guys

00:36:50,860 --> 00:36:54,649

YouTube URL: https://www.youtube.com/watch?v=EMwhsGsxfPU


