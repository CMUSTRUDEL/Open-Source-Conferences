Title: Managing FoundationDB at Scale - John Brownlee, Apple
Publication date: 2020-05-07
Playlist: FoundationDB Summit 2019
Description: 
	Managing FoundationDB at Scale - John Brownlee, Apple 

Apple runs hundreds of FoundationDB clusters with high availability requirements. Running FoundationDB at this scale requires careful design and specialized tooling. In this presentation, we will explore the core workflows we use to manage cluster lifecycle, the high-level design considerations in managing FoundationDB, and how we handle common operational problems.
Captions: 
	00:00:00,030 --> 00:00:04,140
all right good afternoon I'm John

00:00:02,129 --> 00:00:05,879
Brownlee I'm on the foundation DB SRE

00:00:04,140 --> 00:00:07,859
team at Apple and I'm here to talk about

00:00:05,879 --> 00:00:10,830
how we manage foundation DB at scale at

00:00:07,859 --> 00:00:12,269
Apple give you a little bit of context

00:00:10,830 --> 00:00:14,130
we've been running FTP and production

00:00:12,269 --> 00:00:15,839
for a little over three years now we

00:00:14,130 --> 00:00:18,859
have hundreds of clusters with over a

00:00:15,839 --> 00:00:20,970
hundred thousand total f2b processes and

00:00:18,859 --> 00:00:22,500
that footprint has been growing steadily

00:00:20,970 --> 00:00:24,150
over the years and we know it's gonna

00:00:22,500 --> 00:00:26,279
keep growing in the future and so one of

00:00:24,150 --> 00:00:28,080
our core concerns is how we can manage a

00:00:26,279 --> 00:00:33,300
deployment of that size without scaling

00:00:28,080 --> 00:00:34,829
our operational staffing accordingly and

00:00:33,300 --> 00:00:36,510
so to do that we have extensive

00:00:34,829 --> 00:00:37,800
automation and operational tooling

00:00:36,510 --> 00:00:41,160
designed to make it as easy as possible

00:00:37,800 --> 00:00:43,879
to rob and today I'm gonna go over how

00:00:41,160 --> 00:00:46,289
we run DB on kubernetes as an example of

00:00:43,879 --> 00:00:48,239
the kinds of challenges people will face

00:00:46,289 --> 00:00:51,270
running foundation DB in real-world

00:00:48,239 --> 00:00:54,480
circumstances so first some overall

00:00:51,270 --> 00:00:56,460
design principles we run all of our FTB

00:00:54,480 --> 00:00:59,520
processes through FTP monitor which is a

00:00:56,460 --> 00:01:01,890
process launcher that ships with F DB we

00:00:59,520 --> 00:01:04,680
run one FTP server process for each f2b

00:01:01,890 --> 00:01:06,150
monitor process and we use config maps

00:01:04,680 --> 00:01:08,280
and kubernetes to provide the monitor

00:01:06,150 --> 00:01:09,479
comp in the cluster file config maps are

00:01:08,280 --> 00:01:11,400
a really great tool for this because

00:01:09,479 --> 00:01:13,500
they allow us to update these files

00:01:11,400 --> 00:01:16,860
dynamically without having to bounce the

00:01:13,500 --> 00:01:18,509
pod and we also use anti affinity to get

00:01:16,860 --> 00:01:20,490
kubernetes to spread processes across

00:01:18,509 --> 00:01:21,840
hosts we have pretty simple anti

00:01:20,490 --> 00:01:24,210
affinity constraints and we trust in

00:01:21,840 --> 00:01:27,420
kubernetes to do the scheduling properly

00:01:24,210 --> 00:01:29,610
and give us the diversity we need we

00:01:27,420 --> 00:01:31,740
have also a custom sidecar process and

00:01:29,610 --> 00:01:33,630
the reason we have that is it allows us

00:01:31,740 --> 00:01:35,729
to provide binaries dynamically and

00:01:33,630 --> 00:01:37,170
configuration dynamically again to give

00:01:35,729 --> 00:01:39,810
us the properties we need be able to

00:01:37,170 --> 00:01:41,850
update foundation DB's configuration and

00:01:39,810 --> 00:01:45,079
even at the code without bouncing the

00:01:41,850 --> 00:01:47,100
entire pod which is very costly for us I

00:01:45,079 --> 00:01:48,600
also wanted to go over some general

00:01:47,100 --> 00:01:51,149
design considerations about running

00:01:48,600 --> 00:01:53,460
foundation DB based on common questions

00:01:51,149 --> 00:01:55,409
we get one thing that you'll want to do

00:01:53,460 --> 00:01:58,380
is to use different process classes to

00:01:55,409 --> 00:02:00,659
help isolate roles this is important not

00:01:58,380 --> 00:02:02,759
just to first to distinguish between

00:02:00,659 --> 00:02:04,290
staple and stateless processes which on

00:02:02,759 --> 00:02:06,180
a platform like kubernetes are going to

00:02:04,290 --> 00:02:08,009
have pretty different requirements it

00:02:06,180 --> 00:02:09,810
also allows you to customize memory

00:02:08,009 --> 00:02:12,690
allocations and knobs for these

00:02:09,810 --> 00:02:13,650
different roles for the log and

00:02:12,690 --> 00:02:15,090
stateless process

00:02:13,650 --> 00:02:17,099
you're gonna want to provision those

00:02:15,090 --> 00:02:18,840
based on how many of those processes you

00:02:17,099 --> 00:02:21,569
want to recruit how many logs you need

00:02:18,840 --> 00:02:22,830
how many resolvers and proxies and you

00:02:21,569 --> 00:02:25,709
also want to throw in a few spares

00:02:22,830 --> 00:02:27,480
because there are some failure modes

00:02:25,709 --> 00:02:29,489
that you're not going to hit until it's

00:02:27,480 --> 00:02:32,700
way too late if you try and run these

00:02:29,489 --> 00:02:35,489
things too tight for instance if you

00:02:32,700 --> 00:02:36,870
were one of your proxies dies then that

00:02:35,489 --> 00:02:38,370
will initiate a recovery which is going

00:02:36,870 --> 00:02:40,829
to cause a few seconds of unavailability

00:02:38,370 --> 00:02:44,280
and if you're running exactly the number

00:02:40,829 --> 00:02:46,200
of proxies that your database is

00:02:44,280 --> 00:02:48,090
configured to accept then when that

00:02:46,200 --> 00:02:50,940
proxy comes back it's going to recover

00:02:48,090 --> 00:02:52,290
again and if it's flapping going up and

00:02:50,940 --> 00:02:54,329
down constantly you're just going to be

00:02:52,290 --> 00:02:56,160
unavailable until you can get it back up

00:02:54,329 --> 00:02:57,930
to a stable State so in order to help

00:02:56,160 --> 00:02:59,760
FTP you recover onto a stable

00:02:57,930 --> 00:03:01,049
configuration in these circumstances we

00:02:59,760 --> 00:03:02,400
just throw in a few spares which are

00:03:01,049 --> 00:03:06,690
pretty cheap for these kind of process

00:03:02,400 --> 00:03:08,069
types an overall property of FTP that is

00:03:06,690 --> 00:03:09,720
going to be surprising to people that

00:03:08,069 --> 00:03:11,849
you'll need to prepare for is how to

00:03:09,720 --> 00:03:13,709
handle recoveries these happen lots of

00:03:11,849 --> 00:03:16,310
different circumstances they can happen

00:03:13,709 --> 00:03:18,989
due to process failures or exclusions or

00:03:16,310 --> 00:03:21,269
reconfiguration or bounces and they're a

00:03:18,989 --> 00:03:22,739
necessary part of running foundation DB

00:03:21,269 --> 00:03:24,840
and something that clients need to be

00:03:22,739 --> 00:03:26,790
able to accept but if you have multiple

00:03:24,840 --> 00:03:28,440
recoveries in close succession it can

00:03:26,790 --> 00:03:30,209
effectively extend that window beyond

00:03:28,440 --> 00:03:31,440
what clients can handle so you'll need

00:03:30,209 --> 00:03:33,900
to make sure that you're careful to

00:03:31,440 --> 00:03:35,160
avoid this and control how you're taking

00:03:33,900 --> 00:03:38,639
deliberate actions that you know are

00:03:35,160 --> 00:03:39,959
going to cause recoveries next up I

00:03:38,639 --> 00:03:43,109
wanted to run through our core

00:03:39,959 --> 00:03:44,940
operational loop these are the ten basic

00:03:43,109 --> 00:03:48,299
steps that we run for managing the FTP

00:03:44,940 --> 00:03:50,190
cluster lifecycle and these steps can

00:03:48,299 --> 00:03:53,370
encompass almost everything you need to

00:03:50,190 --> 00:03:55,560
do to run an FTP cluster when we make we

00:03:53,370 --> 00:03:57,599
design our configurations based on a

00:03:55,560 --> 00:03:59,579
cluster spec which specifies the desired

00:03:57,599 --> 00:04:00,959
state of the cluster and then we have

00:03:59,579 --> 00:04:03,060
this operation loop capable of

00:04:00,959 --> 00:04:04,769
reconciling it by taking a variety of

00:04:03,060 --> 00:04:08,310
different actions as necessary in order

00:04:04,769 --> 00:04:10,019
to make sure that the the current state

00:04:08,310 --> 00:04:12,329
of the N state of the cluster is what

00:04:10,019 --> 00:04:14,129
has been configured and depending on

00:04:12,329 --> 00:04:15,900
what's changing we'll only run a subset

00:04:14,129 --> 00:04:18,299
of these but this is the overall

00:04:15,900 --> 00:04:19,829
framework of what we're doing so first

00:04:18,299 --> 00:04:22,169
off I wanted to dig in a little deeper

00:04:19,829 --> 00:04:25,740
into the binaries and configuration file

00:04:22,169 --> 00:04:27,540
management in general you're going to

00:04:25,740 --> 00:04:29,220
need some kind of dynamic

00:04:27,540 --> 00:04:31,260
a process to be able to install the new

00:04:29,220 --> 00:04:34,110
monitor cough when you're changing the

00:04:31,260 --> 00:04:36,240
configuration for the processes and

00:04:34,110 --> 00:04:38,040
similarly even though when you change

00:04:36,240 --> 00:04:40,140
the cluster file you can generally get

00:04:38,040 --> 00:04:42,090
that propagated live to clients without

00:04:40,140 --> 00:04:44,490
any explicit action in order to make

00:04:42,090 --> 00:04:45,720
sure that things are being kept in a

00:04:44,490 --> 00:04:47,850
consistent state you'll need to update

00:04:45,720 --> 00:04:49,020
that through a narrow band process as

00:04:47,850 --> 00:04:51,210
well just to make sure that when the

00:04:49,020 --> 00:04:52,710
process is if the process is getting

00:04:51,210 --> 00:04:54,270
restarted in a way that they lose their

00:04:52,710 --> 00:04:58,680
container context you're gonna get the

00:04:54,270 --> 00:05:00,060
same configuration back the most complex

00:04:58,680 --> 00:05:01,980
part is that when you're doing upgrades

00:05:00,060 --> 00:05:04,650
we have to install the new binaries

00:05:01,980 --> 00:05:06,960
inside the live container and this is

00:05:04,650 --> 00:05:09,150
what the sidecar process is for there's

00:05:06,960 --> 00:05:10,890
a neat feature in kubernetes where if

00:05:09,150 --> 00:05:13,290
you have a pod that contains multiple

00:05:10,890 --> 00:05:15,060
containers you can upgrade one of those

00:05:13,290 --> 00:05:16,800
containers by changing its image without

00:05:15,060 --> 00:05:19,170
bouncing or affecting the other one at

00:05:16,800 --> 00:05:21,500
all so we use this as a beautiful little

00:05:19,170 --> 00:05:24,180
hack to be able to upgrade the sidecar

00:05:21,500 --> 00:05:26,130
just restart that process and copy the

00:05:24,180 --> 00:05:27,780
binary in live and then update the

00:05:26,130 --> 00:05:29,310
monitor cough and bounce the processes

00:05:27,780 --> 00:05:32,220
on the new binaries that were never in

00:05:29,310 --> 00:05:34,890
that image to begin with this is we

00:05:32,220 --> 00:05:39,090
think the safest way to dynamically

00:05:34,890 --> 00:05:40,320
inject code into a running process next

00:05:39,090 --> 00:05:43,950
up we want to talk a little bit about

00:05:40,320 --> 00:05:45,870
adding new processes one of the great

00:05:43,950 --> 00:05:48,000
things about expanding on FTP is that

00:05:45,870 --> 00:05:49,740
unlike a lot of databases new processes

00:05:48,000 --> 00:05:51,300
can be bootstrapped and join the cluster

00:05:49,740 --> 00:05:53,100
just by providing that cluster file

00:05:51,300 --> 00:05:54,840
they'll need you know your basic start

00:05:53,100 --> 00:05:56,610
parameters around the public addresses

00:05:54,840 --> 00:05:58,080
and that kind of stuff but you don't

00:05:56,610 --> 00:06:00,570
need to configure anything in the

00:05:58,080 --> 00:06:02,010
database to accept a new process just by

00:06:00,570 --> 00:06:03,480
having the process connect it'll be able

00:06:02,010 --> 00:06:05,580
to join the cluster and start taking on

00:06:03,480 --> 00:06:07,290
work however when you're creating a

00:06:05,580 --> 00:06:08,730
cluster from scratch

00:06:07,290 --> 00:06:10,230
you still need to seed it with the

00:06:08,730 --> 00:06:11,760
initial cluster file in order to get

00:06:10,230 --> 00:06:14,520
those seed processes to talk to each

00:06:11,760 --> 00:06:17,130
other one option is to bootstrap with a

00:06:14,520 --> 00:06:18,750
dummy cluster file and I should say the

00:06:17,130 --> 00:06:20,700
challenge in this is the cluster file

00:06:18,750 --> 00:06:22,470
needs to contain IP addresses and on a

00:06:20,700 --> 00:06:23,610
platform like kubernetes you don't know

00:06:22,470 --> 00:06:25,470
what the IP addresses are going to be

00:06:23,610 --> 00:06:27,120
until you've launched the process so

00:06:25,470 --> 00:06:28,950
you're in something of a catch-22 and

00:06:27,120 --> 00:06:30,660
one approach to that is to launch it

00:06:28,950 --> 00:06:32,580
with a dummy cluster file that won't

00:06:30,660 --> 00:06:33,900
enable it to form a cluster and then

00:06:32,580 --> 00:06:36,690
update it once you have the real IP

00:06:33,900 --> 00:06:38,340
addresses another option is to bootstrap

00:06:36,690 --> 00:06:39,630
it with an empty monitor comp file in

00:06:38,340 --> 00:06:41,949
which case it just won't run any

00:06:39,630 --> 00:06:44,150
processes at all

00:06:41,949 --> 00:06:46,939
similarly if you're replicating across

00:06:44,150 --> 00:06:48,650
say visible hosts you're going to need

00:06:46,939 --> 00:06:50,960
some way to get that machine information

00:06:48,650 --> 00:06:52,729
and dynamically see the monitor comp

00:06:50,960 --> 00:06:56,360
file is just a flat text file that's

00:06:52,729 --> 00:06:58,009
gonna give the start args and so if you

00:06:56,360 --> 00:06:59,180
have something like the machine name

00:06:58,009 --> 00:07:01,789
which needs to come from the environment

00:06:59,180 --> 00:07:05,780
dynamically you will need to run some

00:07:01,789 --> 00:07:07,550
additional cleverness to get that supply

00:07:05,780 --> 00:07:09,259
dynamically and this is another feature

00:07:07,550 --> 00:07:10,819
we've built into this sidecar since it's

00:07:09,259 --> 00:07:12,740
already providing the monitor comp file

00:07:10,819 --> 00:07:14,900
from the config map and copying it into

00:07:12,740 --> 00:07:16,729
a volume where the monitor process can

00:07:14,900 --> 00:07:18,770
access it we also have it doing some

00:07:16,729 --> 00:07:20,060
basic template substitution it's not

00:07:18,770 --> 00:07:21,740
really that hard of a thing to write for

00:07:20,060 --> 00:07:23,840
the limited scope of what you need to

00:07:21,740 --> 00:07:25,819
substitute into a monitor count file but

00:07:23,840 --> 00:07:27,229
it may also be something that given that

00:07:25,819 --> 00:07:30,250
this is a common need we may want to

00:07:27,229 --> 00:07:33,469
move into the core as a core feature

00:07:30,250 --> 00:07:36,259
another tricky bit here that that people

00:07:33,469 --> 00:07:37,939
often run into is even though when you

00:07:36,259 --> 00:07:42,439
change coordinators the cluster file

00:07:37,939 --> 00:07:43,370
gets updated automatically you need and

00:07:42,439 --> 00:07:46,129
even though you can update it

00:07:43,370 --> 00:07:47,569
out-of-band as well if a process client

00:07:46,129 --> 00:07:49,159
process can't write its cluster file

00:07:47,569 --> 00:07:50,719
it'll report itself as being in a

00:07:49,159 --> 00:07:52,490
degraded State which will leave the

00:07:50,719 --> 00:07:54,529
whole cluster marked as unhealthy until

00:07:52,490 --> 00:07:56,180
you remediate that problem and so if

00:07:54,529 --> 00:07:57,860
you're using something like a config map

00:07:56,180 --> 00:07:59,060
you may run into volume permissions that

00:07:57,860 --> 00:08:01,430
prevent this from working and you may

00:07:59,060 --> 00:08:02,419
need to do add an indirection layer to

00:08:01,430 --> 00:08:05,750
make sure that you don't get hung up

00:08:02,419 --> 00:08:07,550
there next up I want to talk a little

00:08:05,750 --> 00:08:10,129
bit about how we read reconfigure the

00:08:07,550 --> 00:08:11,839
database this is really one of the

00:08:10,129 --> 00:08:13,699
simpler actions because we just need to

00:08:11,839 --> 00:08:15,949
run a CLI with a configure command and

00:08:13,699 --> 00:08:17,629
the new desired configuration and then

00:08:15,949 --> 00:08:19,189
wait for the cluster to be healthy so we

00:08:17,629 --> 00:08:20,509
know that it's you know got the

00:08:19,189 --> 00:08:23,300
appropriate fault tolerance and that

00:08:20,509 --> 00:08:25,939
it's rolled out the new and you know any

00:08:23,300 --> 00:08:27,680
new parts of the replication in general

00:08:25,939 --> 00:08:29,509
we think it's best to do this between

00:08:27,680 --> 00:08:32,110
adding the new processes and removing

00:08:29,509 --> 00:08:35,120
the old processes because this gives you

00:08:32,110 --> 00:08:36,620
an assurance that whether you're you

00:08:35,120 --> 00:08:38,089
know going to a higher replication mode

00:08:36,620 --> 00:08:40,190
or a lower replication mode you're

00:08:38,089 --> 00:08:41,419
always going to have the safest number

00:08:40,190 --> 00:08:43,669
of processes at the time you do this

00:08:41,419 --> 00:08:45,620
configuration another aspect that's

00:08:43,669 --> 00:08:47,149
really complicated and unfortunately way

00:08:45,620 --> 00:08:49,610
too complicated for me to get into in

00:08:47,149 --> 00:08:51,709
this format is changing between data

00:08:49,610 --> 00:08:53,329
centers we manage our data center our

00:08:51,709 --> 00:08:54,279
multi data center configs through FTB's

00:08:53,329 --> 00:08:56,740
region configuration

00:08:54,279 --> 00:08:58,480
but the regen configuration is one of

00:08:56,740 --> 00:08:59,499
the areas where you can't change the

00:08:58,480 --> 00:09:01,480
whole thing at once

00:08:59,499 --> 00:09:03,189
if you're mixing changes of like adding

00:09:01,480 --> 00:09:05,050
regions and removing regions and

00:09:03,189 --> 00:09:06,610
changing the usable regions each of

00:09:05,050 --> 00:09:08,139
those may need to be its own atomic step

00:09:06,610 --> 00:09:10,660
this is something that we're gonna need

00:09:08,139 --> 00:09:12,100
to add more documentation about and some

00:09:10,660 --> 00:09:13,389
guidelines for how to build a safe

00:09:12,100 --> 00:09:16,420
process to do this in an iterative

00:09:13,389 --> 00:09:17,680
fashion but it's only really a problem

00:09:16,420 --> 00:09:19,600
if you need to run in that kind of multi

00:09:17,680 --> 00:09:23,499
region config most other changes you can

00:09:19,600 --> 00:09:25,959
just run a single configure command next

00:09:23,499 --> 00:09:28,300
up I wanted to go into a few different

00:09:25,959 --> 00:09:31,240
stages that all are around the area of

00:09:28,300 --> 00:09:32,350
how you remove processes one of the

00:09:31,240 --> 00:09:35,379
things about this overall architecture

00:09:32,350 --> 00:09:37,149
is that the process of replacing a host

00:09:35,379 --> 00:09:39,069
is effectively identical to doing a

00:09:37,149 --> 00:09:40,449
Grove followed by a shrink and this

00:09:39,069 --> 00:09:43,629
allows us to reuse a lot of that logic

00:09:40,449 --> 00:09:46,689
and create a much smoother flow so the

00:09:43,629 --> 00:09:48,009
you know you will need some logic for

00:09:46,689 --> 00:09:49,959
determining what processes you want to

00:09:48,009 --> 00:09:51,970
remove either by having whoever is

00:09:49,959 --> 00:09:54,459
operating it provide a list or doing a

00:09:51,970 --> 00:09:56,470
shrink operation that has logic for

00:09:54,459 --> 00:10:00,189
doing a diverse selection of what

00:09:56,470 --> 00:10:02,259
processes to keep around and then once

00:10:00,189 --> 00:10:04,480
you've done that you you just need to do

00:10:02,259 --> 00:10:06,850
an exclusion which tells FTB to evacuate

00:10:04,480 --> 00:10:08,259
the data from those processes and also

00:10:06,850 --> 00:10:10,209
tell them not to serve any other roles

00:10:08,259 --> 00:10:12,000
like make sure that they're not serving

00:10:10,209 --> 00:10:14,740
as the master or something like that

00:10:12,000 --> 00:10:16,329
that operation is going to block until

00:10:14,740 --> 00:10:18,160
the database can confirm that it's safe

00:10:16,329 --> 00:10:20,110
to remove the processes and once you get

00:10:18,160 --> 00:10:22,089
that confirmation you can just shut them

00:10:20,110 --> 00:10:23,769
down through the control plane you know

00:10:22,089 --> 00:10:26,170
it for instance deleting the pods in

00:10:23,769 --> 00:10:27,699
kubernetes and then check the closure

00:10:26,170 --> 00:10:29,439
status to make sure those processes

00:10:27,699 --> 00:10:31,240
aren't removing because one thing that

00:10:29,439 --> 00:10:33,100
can happen is your control plane can

00:10:31,240 --> 00:10:34,870
just lie to you and say it shut things

00:10:33,100 --> 00:10:36,279
down even though it couldn't confirm

00:10:34,870 --> 00:10:38,769
whether it did or it didn't so it's

00:10:36,279 --> 00:10:40,089
always best to double-check in both the

00:10:38,769 --> 00:10:41,679
control planes belief as to what's

00:10:40,089 --> 00:10:44,500
configured to run and the databases

00:10:41,679 --> 00:10:46,209
believe as to what is running after

00:10:44,500 --> 00:10:48,819
you've confirmed that you can use the

00:10:46,209 --> 00:10:50,620
inclusion command to read the processes

00:10:48,819 --> 00:10:51,759
which just cleans up the exclusion state

00:10:50,620 --> 00:10:53,620
and make sure you're not keeping around

00:10:51,759 --> 00:10:57,910
dead configuration about processes that

00:10:53,620 --> 00:10:59,410
don't exist now in between that flow is

00:10:57,910 --> 00:11:01,029
where you'll probably need to change

00:10:59,410 --> 00:11:02,679
coordinators for instance if you're

00:11:01,029 --> 00:11:05,769
replacing a bad host and that host has a

00:11:02,679 --> 00:11:07,209
coordinator on it you'll need to remove

00:11:05,769 --> 00:11:08,020
it from the coordinator list by

00:11:07,209 --> 00:11:09,700
selecting a new

00:11:08,020 --> 00:11:11,890
and you'll probably want to do that

00:11:09,700 --> 00:11:13,330
during the exclusion phase because that

00:11:11,890 --> 00:11:14,830
way the database also has knowledge

00:11:13,330 --> 00:11:19,510
about the fact that this is an undesired

00:11:14,830 --> 00:11:20,740
coordinator now when you're starting a

00:11:19,510 --> 00:11:22,540
new cluster as I mentioned before you

00:11:20,740 --> 00:11:25,480
need to generate an initial cluster file

00:11:22,540 --> 00:11:27,160
from scratch after that's done you just

00:11:25,480 --> 00:11:29,080
need to recruit new coordinators by

00:11:27,160 --> 00:11:31,000
running a coordinators change command

00:11:29,080 --> 00:11:34,240
whenever a coordinator is unreachable or

00:11:31,000 --> 00:11:35,440
when it's being removed another thing to

00:11:34,240 --> 00:11:37,450
note that's very challenging on

00:11:35,440 --> 00:11:39,130
kubernetes is that when if a coordinator

00:11:37,450 --> 00:11:41,440
changes its IP it's going to be

00:11:39,130 --> 00:11:43,180
unreachable as a coordinator no other

00:11:41,440 --> 00:11:45,010
part of FTB has this constraint it's

00:11:43,180 --> 00:11:46,990
just an ordinary Storage Server comes up

00:11:45,010 --> 00:11:48,910
with a new IP it's fine it gets to keep

00:11:46,990 --> 00:11:50,589
serving its own roles doesn't even need

00:11:48,910 --> 00:11:52,330
to redistribute data but if a

00:11:50,589 --> 00:11:53,740
coordinator changes its IP it's a

00:11:52,330 --> 00:11:56,230
totally different process from the

00:11:53,740 --> 00:11:58,540
databases standpoint now one way you can

00:11:56,230 --> 00:12:00,520
you can mitigate this more or less in

00:11:58,540 --> 00:12:02,380
practice as long as you can control the

00:12:00,520 --> 00:12:03,730
rate at which this is happening but

00:12:02,380 --> 00:12:04,990
that's not necessarily tenable in the

00:12:03,730 --> 00:12:06,490
long term which is why we have a project

00:12:04,990 --> 00:12:09,040
to make sure that this constraint goes

00:12:06,490 --> 00:12:11,170
away and that you can address things in

00:12:09,040 --> 00:12:13,540
the cluster file by some other things by

00:12:11,170 --> 00:12:15,250
a DNS name rather than an IP that I

00:12:13,540 --> 00:12:16,600
believe is being slated for seven oh and

00:12:15,250 --> 00:12:19,410
we think it's gonna be a key piece of

00:12:16,600 --> 00:12:21,730
making FTB work reliably on kubernetes

00:12:19,410 --> 00:12:22,990
after you change the coordinators you

00:12:21,730 --> 00:12:24,760
can update the cluster file lazily

00:12:22,990 --> 00:12:26,350
because any process that was connected

00:12:24,760 --> 00:12:28,000
to the database would it get that new

00:12:26,350 --> 00:12:31,360
coordinator list automatically and right

00:12:28,000 --> 00:12:34,230
into its own cluster file last step I

00:12:31,360 --> 00:12:36,459
wanted to go into is bouncing instances

00:12:34,230 --> 00:12:38,529
so once you've installed the new monitor

00:12:36,459 --> 00:12:40,839
comp and validated that it's been

00:12:38,529 --> 00:12:42,490
properly installed on the machines we

00:12:40,839 --> 00:12:45,339
bounce all of the new processes through

00:12:42,490 --> 00:12:46,899
the CLI after you've done the bounce you

00:12:45,339 --> 00:12:48,160
can then check in the cluster status to

00:12:46,899 --> 00:12:50,709
make sure all the new processes have

00:12:48,160 --> 00:12:53,500
come back up you know for instance that

00:12:50,709 --> 00:12:55,120
there wasn't some kind of problem on the

00:12:53,500 --> 00:12:57,579
host that prevented it from relaunching

00:12:55,120 --> 00:12:59,610
it one thing to note is that when you're

00:12:57,579 --> 00:13:01,600
bouncing is part of an upgrade you

00:12:59,610 --> 00:13:03,850
probably want to confirm that the

00:13:01,600 --> 00:13:05,620
clients all have compatible client

00:13:03,850 --> 00:13:07,089
libraries and you can do that through

00:13:05,620 --> 00:13:09,790
the cluster status by checking this

00:13:07,089 --> 00:13:11,110
connected clients key or if you're

00:13:09,790 --> 00:13:13,360
willing to be a little patient this

00:13:11,110 --> 00:13:16,600
requirement should go away totally once

00:13:13,360 --> 00:13:18,310
we have the new RPC layer that Evan

00:13:16,600 --> 00:13:19,750
talked about earlier today I think

00:13:18,310 --> 00:13:21,630
that's really going to be a key piece of

00:13:19,750 --> 00:13:23,800
making the upgrade story

00:13:21,630 --> 00:13:26,470
sustainable for four people running

00:13:23,800 --> 00:13:30,820
foundation DB outside of very narrow

00:13:26,470 --> 00:13:32,470
integrated use cases I wanted to spend

00:13:30,820 --> 00:13:34,330
some more time now talking about our

00:13:32,470 --> 00:13:37,270
bounce strategy in more detail because

00:13:34,330 --> 00:13:40,209
this is one of the most controversial

00:13:37,270 --> 00:13:41,709
parts of the way we do our operations so

00:13:40,209 --> 00:13:43,630
when we bounce a cluster whether it's

00:13:41,709 --> 00:13:46,300
part of an upgrade or a knob change any

00:13:43,630 --> 00:13:47,800
reason whatsoever we bounce everything

00:13:46,300 --> 00:13:49,570
at once

00:13:47,800 --> 00:13:51,100
and because we're running through after

00:13:49,570 --> 00:13:53,320
we monitor which is designed for fast

00:13:51,100 --> 00:13:56,290
restarts the processes come back up in a

00:13:53,320 --> 00:13:58,300
matter of seconds and to our to the

00:13:56,290 --> 00:14:00,370
clients of the cluster this is totally

00:13:58,300 --> 00:14:02,709
transparent they'll just get a latency

00:14:00,370 --> 00:14:06,430
spike that should be within the

00:14:02,709 --> 00:14:08,050
tolerances and the client the

00:14:06,430 --> 00:14:09,490
transaction retry logic will make sure

00:14:08,050 --> 00:14:11,860
that any in-flight operations will

00:14:09,490 --> 00:14:13,029
eventually get completed now the reason

00:14:11,860 --> 00:14:15,220
we do this there's a few different

00:14:13,029 --> 00:14:17,709
reasons we do this one of them is that

00:14:15,220 --> 00:14:20,709
when you bounce a log or a stateless

00:14:17,709 --> 00:14:22,480
process it initiates a recovery but if

00:14:20,709 --> 00:14:24,520
you bounce all of them at once it still

00:14:22,480 --> 00:14:25,930
just initiates a single recovery so this

00:14:24,520 --> 00:14:28,390
means that your clients are getting a

00:14:25,930 --> 00:14:30,339
more compacted window in which things

00:14:28,390 --> 00:14:31,720
are a little weird rather than having it

00:14:30,339 --> 00:14:34,420
stretched out over a longer period of

00:14:31,720 --> 00:14:36,040
time it also means that you don't have

00:14:34,420 --> 00:14:38,430
multiple you don't have different

00:14:36,040 --> 00:14:41,380
processes on different configurations

00:14:38,430 --> 00:14:43,000
this isn't necessarily bad to have them

00:14:41,380 --> 00:14:44,980
on different configurations but it is

00:14:43,000 --> 00:14:46,870
less well tested and one of the

00:14:44,980 --> 00:14:48,640
principles that animates the FTB team is

00:14:46,870 --> 00:14:51,220
that the safest thing is the thing that

00:14:48,640 --> 00:14:53,589
we can test the most and even for

00:14:51,220 --> 00:14:55,270
upgrades we have simulation tests that

00:14:53,589 --> 00:14:57,250
can simulate this upgrade process of

00:14:55,270 --> 00:14:58,779
just running a cluster bringing up

00:14:57,250 --> 00:15:01,810
killing it bringing up at a new version

00:14:58,779 --> 00:15:03,250
but well any kind of rolling change it's

00:15:01,810 --> 00:15:04,510
much harder with our simulation

00:15:03,250 --> 00:15:06,190
technology so this gives us some

00:15:04,510 --> 00:15:08,950
additional confidence that the changes

00:15:06,190 --> 00:15:10,420
are going to behave as we expect now

00:15:08,950 --> 00:15:12,130
there is one time where we do a rolling

00:15:10,420 --> 00:15:14,320
bounce and that's where we need when we

00:15:12,130 --> 00:15:15,940
upgrade after you monitor for the most

00:15:14,320 --> 00:15:17,380
part you have TV releases don't really

00:15:15,940 --> 00:15:19,240
require upgrading after you monitor

00:15:17,380 --> 00:15:21,610
because it's not doing much there are

00:15:19,240 --> 00:15:23,140
rare occasions where it's necessary but

00:15:21,610 --> 00:15:23,980
going forward on kubernetes we're going

00:15:23,140 --> 00:15:25,690
to try and make this some more

00:15:23,980 --> 00:15:27,820
consistent and just for safety sake

00:15:25,690 --> 00:15:30,600
always upgrade after you monitor and the

00:15:27,820 --> 00:15:33,579
main FTB container to make sure that

00:15:30,600 --> 00:15:34,840
that things are converged on the

00:15:33,579 --> 00:15:35,950
configuration that you would get

00:15:34,840 --> 00:15:40,060
if you were creating a cluster from

00:15:35,950 --> 00:15:42,670
scratch now you may still be skeptical

00:15:40,060 --> 00:15:44,050
of this you may say this goes against

00:15:42,670 --> 00:15:47,710
everything I know about distributed

00:15:44,050 --> 00:15:49,360
systems and you're not wrong but what I

00:15:47,710 --> 00:15:51,550
will say is that this has proven to be

00:15:49,360 --> 00:15:54,250
an extremely effective strategy at Apple

00:15:51,550 --> 00:15:56,890
we have challenges running FTB at Apple

00:15:54,250 --> 00:15:59,740
we're not perfect but this part of the

00:15:56,890 --> 00:16:03,330
process works very very well and it has

00:15:59,740 --> 00:16:05,080
no negative impact on our uptime or RSOs

00:16:03,330 --> 00:16:07,270
furthermore we're in a relatively good

00:16:05,080 --> 00:16:09,760
position here because we can build

00:16:07,270 --> 00:16:11,470
confidence not by doing a rolling doubts

00:16:09,760 --> 00:16:13,540
within a cluster but rather by moving

00:16:11,470 --> 00:16:16,210
new configurations in new versions of

00:16:13,540 --> 00:16:18,730
FTB through a QA to prod pipeline and

00:16:16,210 --> 00:16:19,990
the benefit of this is that if you're

00:16:18,730 --> 00:16:22,210
trying to do something with canary

00:16:19,990 --> 00:16:23,530
processes in a cluster it's hard to be

00:16:22,210 --> 00:16:25,720
confident that you've found all the

00:16:23,530 --> 00:16:28,030
potential problems FTB is a very

00:16:25,720 --> 00:16:29,320
heterogeneous system there's lots of

00:16:28,030 --> 00:16:32,560
different processes serving lots of

00:16:29,320 --> 00:16:34,690
different roles and if you do something

00:16:32,560 --> 00:16:36,340
screwy with one process FTB is pretty

00:16:34,690 --> 00:16:38,320
good at making sure that it's not going

00:16:36,340 --> 00:16:41,110
to harm you too much so you may

00:16:38,320 --> 00:16:42,130
accidentally just get into a position

00:16:41,110 --> 00:16:44,290
where you're testing didn't discover

00:16:42,130 --> 00:16:46,000
anything so instead we think it's a lot

00:16:44,290 --> 00:16:47,860
better to do this through a full QA

00:16:46,000 --> 00:16:51,160
pipeline where you can do a rigorous set

00:16:47,860 --> 00:16:53,260
of tests on the new configuration on

00:16:51,160 --> 00:16:55,060
that note there's also changes which do

00:16:53,260 --> 00:16:57,100
to FTB's architecture are fundamentally

00:16:55,060 --> 00:16:58,450
impossible to canary for instance if

00:16:57,100 --> 00:17:01,030
you're changing to a new replication

00:16:58,450 --> 00:17:03,550
mode that affects the whole database and

00:17:01,030 --> 00:17:06,610
there's no way around that right now so

00:17:03,550 --> 00:17:08,230
you need a an effective QA pipeline to

00:17:06,610 --> 00:17:09,670
be able to test that and once you have

00:17:08,230 --> 00:17:11,380
that QA pipeline you're better off just

00:17:09,670 --> 00:17:12,730
using that for everything because then

00:17:11,380 --> 00:17:14,350
you have one test strategy that you're

00:17:12,730 --> 00:17:17,800
using for a wide variety of potential

00:17:14,350 --> 00:17:19,300
problems however I will also say that

00:17:17,800 --> 00:17:22,630
part of what influences this is that

00:17:19,300 --> 00:17:23,950
since the we also develop do a huge

00:17:22,630 --> 00:17:26,380
amount of development on FTB and we've

00:17:23,950 --> 00:17:28,390
been running it for years we have a lot

00:17:26,380 --> 00:17:29,590
of confidence in our testing

00:17:28,390 --> 00:17:31,750
infrastructure and I can totally

00:17:29,590 --> 00:17:33,730
understand if people outside of Apple

00:17:31,750 --> 00:17:35,410
don't you know it's pretty extraordinary

00:17:33,730 --> 00:17:37,300
claim so I did want to go into some

00:17:35,410 --> 00:17:39,430
potential alternatives to this bount

00:17:37,300 --> 00:17:42,370
strategy and what kind of pitfalls you'd

00:17:39,430 --> 00:17:44,320
run into first I want to talk about the

00:17:42,370 --> 00:17:46,120
problems with rolling bounces and the

00:17:44,320 --> 00:17:48,400
advantages of rolling bounces the

00:17:46,120 --> 00:17:48,790
biggest one is that if you're doing a

00:17:48,400 --> 00:17:50,530
minor

00:17:48,790 --> 00:17:52,000
or major version upgrade the new

00:17:50,530 --> 00:17:53,650
processes are going to be protocol and

00:17:52,000 --> 00:17:55,330
compatible with the old ones which means

00:17:53,650 --> 00:17:58,150
there is absolutely no way to do a

00:17:55,330 --> 00:17:59,860
rolling balance and I'll also say that

00:17:58,150 --> 00:18:01,840
the work that's being done around a

00:17:59,860 --> 00:18:03,610
stable wire protocol only affects the

00:18:01,840 --> 00:18:05,560
protocol between clients and the

00:18:03,610 --> 00:18:06,700
server's so the server processes are

00:18:05,560 --> 00:18:08,800
still going to be protocol and

00:18:06,700 --> 00:18:09,940
compatible with each other and you still

00:18:08,800 --> 00:18:12,280
won't be able to do a rolling bounce

00:18:09,940 --> 00:18:13,570
strategy you also need to handle the

00:18:12,280 --> 00:18:15,580
number of recoveries for the login

00:18:13,570 --> 00:18:17,140
stateless processes which you can kind

00:18:15,580 --> 00:18:18,730
of mitigate by just waiting in between

00:18:17,140 --> 00:18:20,050
doing those bounces and making sure that

00:18:18,730 --> 00:18:23,620
things get stable before you do the next

00:18:20,050 --> 00:18:25,420
set or another way you could approach

00:18:23,620 --> 00:18:27,400
that is and this is especially relevant

00:18:25,420 --> 00:18:29,260
on something like kubernetes is you just

00:18:27,400 --> 00:18:31,780
bring up a brand new set of login

00:18:29,260 --> 00:18:33,730
stateless processes and then exclude the

00:18:31,780 --> 00:18:36,040
old ones to force all of those roles

00:18:33,730 --> 00:18:37,690
under the new processes and that can

00:18:36,040 --> 00:18:41,190
also give you a pretty widespread canary

00:18:37,690 --> 00:18:44,740
that's relatively easy to roll back from

00:18:41,190 --> 00:18:46,600
the other thing to be wary of is that if

00:18:44,740 --> 00:18:47,680
you're rolling out new knobs setting

00:18:46,600 --> 00:18:49,510
those in a way that's heterogeneous

00:18:47,680 --> 00:18:51,790
across the cluster isn't necessarily

00:18:49,510 --> 00:18:53,350
safe there's some knobs where we know

00:18:51,790 --> 00:18:55,990
it's totally safe there's some knobs

00:18:53,350 --> 00:18:58,060
where we know it's totally unsafe and a

00:18:55,990 --> 00:18:58,780
large set that we just haven't tested so

00:18:58,060 --> 00:19:02,320
that's something you'd want to be

00:18:58,780 --> 00:19:04,750
careful of another strategy is doing a

00:19:02,320 --> 00:19:06,100
dr cutover we have a multi clustered ER

00:19:04,750 --> 00:19:07,810
solution that allows you to have a

00:19:06,100 --> 00:19:09,940
secondary cluster that's kept up to date

00:19:07,810 --> 00:19:12,040
within a few seconds with the primary

00:19:09,940 --> 00:19:14,100
cluster so one thing you could do is

00:19:12,040 --> 00:19:17,050
bring up a new cluster on the new

00:19:14,100 --> 00:19:19,300
configuration set up a dr relationship

00:19:17,050 --> 00:19:21,040
do a dr switchover and then make that

00:19:19,300 --> 00:19:24,100
your new cluster the air clients are

00:19:21,040 --> 00:19:25,420
connecting to now this also doesn't work

00:19:24,100 --> 00:19:27,040
across versions right now

00:19:25,420 --> 00:19:28,300
but it's a smaller problem space that's

00:19:27,040 --> 00:19:30,490
likely going to be tackled as part of

00:19:28,300 --> 00:19:32,080
that RPC layer work it's also very

00:19:30,490 --> 00:19:33,880
expensive because you have to copy all

00:19:32,080 --> 00:19:35,230
of your data to a new cluster which

00:19:33,880 --> 00:19:37,990
means it's probably not practical for

00:19:35,230 --> 00:19:41,080
simple things but it could be a good way

00:19:37,990 --> 00:19:42,670
to do version upgrades in the future and

00:19:41,080 --> 00:19:44,230
in concert with the rolling bounce

00:19:42,670 --> 00:19:45,460
strategy could provide a full

00:19:44,230 --> 00:19:51,850
alternative to doing the kind of

00:19:45,460 --> 00:19:54,160
instable as we do oh so in conclusion

00:19:51,850 --> 00:19:56,410
what I want to focus on is that having a

00:19:54,160 --> 00:19:58,570
strong story around operations is key in

00:19:56,410 --> 00:20:00,340
our minds to FTB's adoption and success

00:19:58,570 --> 00:20:01,659
and it's an area where we know there's

00:20:00,340 --> 00:20:03,700
some weakness in our in our

00:20:01,659 --> 00:20:05,049
documentation and our overall engagement

00:20:03,700 --> 00:20:06,880
with the community and that's something

00:20:05,049 --> 00:20:08,919
that we want to make a huge effort on in

00:20:06,880 --> 00:20:10,389
the coming years and we're also always

00:20:08,919 --> 00:20:12,159
looking for more opportunities to share

00:20:10,389 --> 00:20:13,779
internal tooling and things of that

00:20:12,159 --> 00:20:16,929
nature with the community and build a

00:20:13,779 --> 00:20:18,730
stronger operational ecosystem and light

00:20:16,929 --> 00:20:21,460
of that I'm very excited to announce

00:20:18,730 --> 00:20:23,139
that for the last year or so we've been

00:20:21,460 --> 00:20:25,600
working on a crew Bernays operator for

00:20:23,139 --> 00:20:27,490
FTB and in fact that poor operation loop

00:20:25,600 --> 00:20:29,289
is the operation loop of our operator

00:20:27,490 --> 00:20:30,549
and later today

00:20:29,289 --> 00:20:32,919
we're going to be open sourcing that

00:20:30,549 --> 00:20:34,059
operator I'm so excited to get to share

00:20:32,919 --> 00:20:35,620
this with the community because it's the

00:20:34,059 --> 00:20:37,450
first time we've really been able to get

00:20:35,620 --> 00:20:39,669
our day-to-day operational tooling in a

00:20:37,450 --> 00:20:41,950
forum where we can share it and get

00:20:39,669 --> 00:20:44,169
community feedback and I can't wait to

00:20:41,950 --> 00:20:48,059
see how you all can help shape our

00:20:44,169 --> 00:20:48,059
kubernetes solution thank you

00:20:48,620 --> 00:20:52,779
[Music]

00:20:49,400 --> 00:20:52,779

YouTube URL: https://www.youtube.com/watch?v=A3U8M8pt3Ks


