Title: Restoring FDB after Catastrophic Accidents - Brandon Burton & Matthew Zeier, Wavefront
Publication date: 2019-11-25
Playlist: FoundationDB Summit 2019
Description: 
	Restoring FDB after Catastrophic Accidents - Brandon Burton & Matthew Zeier, Wavefront

How do you backup (and restore) a multi-petabyte FoundationDB cluster? How do you do it when you've suffered a catastrophic event?




We'll cover three real-world examples of failures and how we recovered from them, including tips we wish we knew. Some will involve FDBDR and others will involve Linux filesystem snapshots and as an added bonus some tricks too, in near real-time, duplicate a running production cluster elsewhere.
Captions: 
	00:00:00,000 --> 00:00:06,629
they're all right someone talked about

00:00:03,799 --> 00:00:08,639
when FTB or the when there's a

00:00:06,629 --> 00:00:10,110
catastrophe have to be a little

00:00:08,639 --> 00:00:12,750
introduction Brian and I are both with

00:00:10,110 --> 00:00:16,170
wavefront we're both in the operations

00:00:12,750 --> 00:00:17,910
team and I've been at waivered for more

00:00:16,170 --> 00:00:19,830
than three years we've been running

00:00:17,910 --> 00:00:22,800
foundation DB as the core data platform

00:00:19,830 --> 00:00:25,949
for this service for more than four or

00:00:22,800 --> 00:00:28,140
five years so we'll go over a little bit

00:00:25,949 --> 00:00:32,040
about a background on wavefront and then

00:00:28,140 --> 00:00:36,540
two case studies so this talk is titled

00:00:32,040 --> 00:00:38,100
restoring FTB after catastrophe but it's

00:00:36,540 --> 00:00:40,340
really about managing FTB when your

00:00:38,100 --> 00:00:42,480
infrastructure decides to break on you

00:00:40,340 --> 00:00:44,190
and that's important because over the

00:00:42,480 --> 00:00:46,530
time we've been using FTP we haven't had

00:00:44,190 --> 00:00:48,450
issues with FTP itself it's always been

00:00:46,530 --> 00:00:50,010
something related to infrastructure and

00:00:48,450 --> 00:00:53,960
as Evan said the database runs on

00:00:50,010 --> 00:00:56,100
computers and disks those tend to break

00:00:53,960 --> 00:00:56,940
so a little bit of background wavefront

00:00:56,100 --> 00:00:59,340
for those of you who aren't aware

00:00:56,940 --> 00:01:01,260
wavefronts is cloud native monitoring

00:00:59,340 --> 00:01:03,329
platform it's a time series observer

00:01:01,260 --> 00:01:06,049
ability platform and as I mentioned we

00:01:03,329 --> 00:01:08,790
use FTP as the primary data store

00:01:06,049 --> 00:01:11,220
so we've taken all this data and we make

00:01:08,790 --> 00:01:14,840
sense of it in either alerts or charts

00:01:11,220 --> 00:01:17,460
but but most importantly mostly charts

00:01:14,840 --> 00:01:25,920
because sometimes your data looks like

00:01:17,460 --> 00:01:27,810
this or like this or even like this and

00:01:25,920 --> 00:01:29,520
since we're arguably in the phish tahko

00:01:27,810 --> 00:01:32,250
capital of the world

00:01:29,520 --> 00:01:36,360
sometimes your data looks like this and

00:01:32,250 --> 00:01:37,950
that's an actual wavefront chart so I'll

00:01:36,360 --> 00:01:41,280
talk a little bit about how wait for it

00:01:37,950 --> 00:01:45,509
uses FDB to sort of set some context we

00:01:41,280 --> 00:01:50,759
run a lot of FTB when I put this

00:01:45,509 --> 00:01:52,110
together we had 90 some clusters so

00:01:50,759 --> 00:01:54,360
that's about I think it was a seven or

00:01:52,110 --> 00:01:57,240
two instances you know just under 19,000

00:01:54,360 --> 00:02:01,380
processes so not a small not a small

00:01:57,240 --> 00:02:02,969
deployment and since Evan used last year

00:02:01,380 --> 00:02:04,469
slides I'm going to also use some of the

00:02:02,969 --> 00:02:06,420
last year slides to kind of show you

00:02:04,469 --> 00:02:09,500
what wafer it looks like to help you

00:02:06,420 --> 00:02:11,340
understand how it can break for us so

00:02:09,500 --> 00:02:12,850
super-high level this is a very

00:02:11,340 --> 00:02:14,530
simplified view of a

00:02:12,850 --> 00:02:17,740
front cluster there are three tiers

00:02:14,530 --> 00:02:20,620
there's this web server tier an

00:02:17,740 --> 00:02:22,330
application tier and then a database

00:02:20,620 --> 00:02:24,160
tier and importantly wafer and runs

00:02:22,330 --> 00:02:26,080
active active so what you see here is

00:02:24,160 --> 00:02:28,800
duplicated on that little blue square

00:02:26,080 --> 00:02:32,980
that's not filled out and if I take a

00:02:28,800 --> 00:02:35,230
30,000 square-foot 30,000 foot view this

00:02:32,980 --> 00:02:36,850
is what it looks like so we have this

00:02:35,230 --> 00:02:39,400
front-end or the web server as I showed

00:02:36,850 --> 00:02:41,230
you and then we have the duplicate

00:02:39,400 --> 00:02:43,930
services either in two different AZ's or

00:02:41,230 --> 00:02:49,030
two dan regions and the bottom there is

00:02:43,930 --> 00:02:50,530
of course f DB so I mentioned sometimes

00:02:49,030 --> 00:02:53,350
the infrastructure decides he doesn't

00:02:50,530 --> 00:02:55,690
want to cooperate so I'm gonna break let

00:02:53,350 --> 00:02:58,290
Brandon talk about in the case of the

00:02:55,690 --> 00:02:58,290
missing kernels

00:03:06,930 --> 00:03:12,550
well good okay cool

00:03:10,630 --> 00:03:15,400
so all right yeah I'm gonna talk about

00:03:12,550 --> 00:03:17,709
the case the missing kernels so a little

00:03:15,400 --> 00:03:20,860
bit of background so something happened

00:03:17,709 --> 00:03:23,220
earlier this year we were our database

00:03:20,860 --> 00:03:25,330
tier was still pretty much on trusty

00:03:23,220 --> 00:03:27,970
which is you know no version of Ubuntu

00:03:25,330 --> 00:03:31,030
Linux are for data storage we were

00:03:27,970 --> 00:03:33,190
currently using GPS for all of the f2b

00:03:31,030 --> 00:03:35,470
storage and at the time we were mostly

00:03:33,190 --> 00:03:37,569
on the 3-3 kernel which is it was kind

00:03:35,470 --> 00:03:40,330
of a stock kernel at the time but based

00:03:37,569 --> 00:03:42,340
on some recommendations from Amazon

00:03:40,330 --> 00:03:44,799
they'd recommend updating to a four for

00:03:42,340 --> 00:03:47,230
kernel I was supposed to improve some

00:03:44,799 --> 00:03:49,360
things and that's the time also we had a

00:03:47,230 --> 00:03:51,430
few clusters that we had running on

00:03:49,360 --> 00:03:53,950
xenial with a 4/4 kernel where we were

00:03:51,430 --> 00:04:01,890
testing moving to using the local nvme

00:03:53,950 --> 00:04:04,720
instant storage so this is a graph of

00:04:01,890 --> 00:04:06,610
TCP retransmits this is on three three

00:04:04,720 --> 00:04:07,569
okay before we start putting out the 4

00:04:06,610 --> 00:04:09,519
volt kernel

00:04:07,569 --> 00:04:11,109
we've got the fourth floor kernel you

00:04:09,519 --> 00:04:14,890
can kind of see you know scope wise it's

00:04:11,109 --> 00:04:18,070
under one we switched the for for kernel

00:04:14,890 --> 00:04:19,329
on some clusters and we start to see

00:04:18,070 --> 00:04:21,430
pretty bad

00:04:19,329 --> 00:04:24,340
TCP retransmits a lot of other things

00:04:21,430 --> 00:04:26,020
look fine in fact you know it didn't

00:04:24,340 --> 00:04:28,419
kind of manifest itself as a big problem

00:04:26,020 --> 00:04:29,590
for a little while and the kernels had

00:04:28,419 --> 00:04:35,410
started trickling up some of our

00:04:29,590 --> 00:04:37,150
clusters so right big jump so we say

00:04:35,410 --> 00:04:38,560
okay this isn't working

00:04:37,150 --> 00:04:40,330
we're kind of able you know looking

00:04:38,560 --> 00:04:42,550
graphs look at times realize okay it

00:04:40,330 --> 00:04:44,350
definitely is the new kernel in fact we

00:04:42,550 --> 00:04:46,210
do some verification some testing to

00:04:44,350 --> 00:04:48,360
verify that switching between kernel

00:04:46,210 --> 00:04:50,919
versions and some of our test clusters

00:04:48,360 --> 00:04:54,690
let me go back as you can see things

00:04:50,919 --> 00:04:58,990
look much better afterward so okay so

00:04:54,690 --> 00:05:00,190
now at this point right we have this

00:04:58,990 --> 00:05:03,070
mixed kernel version wheeler doing

00:05:00,190 --> 00:05:04,840
everything back two three three the same

00:05:03,070 --> 00:05:07,050
time we're also doing in place f2b

00:05:04,840 --> 00:05:08,699
upgrades which for us

00:05:07,050 --> 00:05:10,710
is because we have this active active

00:05:08,699 --> 00:05:12,990
model we can be serving on one Marin

00:05:10,710 --> 00:05:15,449
will actually stop FTB do an in-place

00:05:12,990 --> 00:05:16,560
upgrade and since we're kind of pausing

00:05:15,449 --> 00:05:18,389
the world on that one mirror what we do

00:05:16,560 --> 00:05:19,919
that we thought okay well we'll go ahead

00:05:18,389 --> 00:05:21,060
and get these kernel versions consistent

00:05:19,919 --> 00:05:23,699
with a reboot because that's relatively

00:05:21,060 --> 00:05:25,590
quick so we're working through you know

00:05:23,699 --> 00:05:27,690
doing these upgrades in general what's

00:05:25,590 --> 00:05:33,150
going well as I mentioned there's a

00:05:27,690 --> 00:05:38,280
little bit of a wrinkle where some of

00:05:33,150 --> 00:05:39,870
the clusters are xenial with 44 so right

00:05:38,280 --> 00:05:42,000
multiple multiple clusters are being

00:05:39,870 --> 00:05:43,379
upgraded at the same time things seem

00:05:42,000 --> 00:05:46,469
like they're going well you get in the

00:05:43,379 --> 00:05:48,240
groove one of those email clusters we're

00:05:46,469 --> 00:05:51,120
on the trusty steps which include

00:05:48,240 --> 00:05:52,710
purging all the four floor kernels you

00:05:51,120 --> 00:05:53,759
know and there's some sanity checks

00:05:52,710 --> 00:05:57,389
there this is all kind of manual

00:05:53,759 --> 00:05:59,729
PlayBook with ansible and so you know

00:05:57,389 --> 00:06:01,620
working on we're lustre go to the reboot

00:05:59,729 --> 00:06:04,229
they're not coming back up

00:06:01,620 --> 00:06:07,800
okay this was xenial we have no kernels

00:06:04,229 --> 00:06:08,120
we're stuck at grub do I still have a

00:06:07,800 --> 00:06:11,729
job

00:06:08,120 --> 00:06:13,680
actual quote so you know first things

00:06:11,729 --> 00:06:15,900
first to call it OBS is there anything

00:06:13,680 --> 00:06:18,870
we can do is there any way we can

00:06:15,900 --> 00:06:20,279
somehow get at the boot volumes of these

00:06:18,870 --> 00:06:23,130
instances which is the boot volumes or

00:06:20,279 --> 00:06:25,800
EBS we can slide another kernel in there

00:06:23,130 --> 00:06:29,669
in some way otherwise basically data

00:06:25,800 --> 00:06:31,500
gone basically they say no various you

00:06:29,669 --> 00:06:33,210
know technical constraints and a lot of

00:06:31,500 --> 00:06:34,770
them around you know not me and Lee had

00:06:33,210 --> 00:06:38,190
a customer data encryption things

00:06:34,770 --> 00:06:42,479
basically no so that's gone 20 terabytes

00:06:38,190 --> 00:06:46,020
of data gone okay so how do we get out

00:06:42,479 --> 00:06:48,750
of this have to be dr at this point we

00:06:46,020 --> 00:06:51,840
had upgraded part of this cluster 2ft

00:06:48,750 --> 00:06:54,539
VTR to a version of had it available you

00:06:51,840 --> 00:06:56,879
know so we knew it existed but it wasn't

00:06:54,539 --> 00:06:58,919
something we'd actually use yet okay so

00:06:56,879 --> 00:07:00,930
what do we do well will you start doing

00:06:58,919 --> 00:07:03,779
some research as I mentioned earlier we

00:07:00,930 --> 00:07:05,550
have test clusters this is actually a a

00:07:03,779 --> 00:07:07,400
very cool thing that we're able to do

00:07:05,550 --> 00:07:09,110
where we can build

00:07:07,400 --> 00:07:12,380
a full way front cluster and we've the

00:07:09,110 --> 00:07:15,020
ability to replay data out of our

00:07:12,380 --> 00:07:16,310
production monitoring into test clusters

00:07:15,020 --> 00:07:18,440
so we can give them a good volume in a

00:07:16,310 --> 00:07:20,810
good data shape so we take one of those

00:07:18,440 --> 00:07:23,690
clusters we built through the run book

00:07:20,810 --> 00:07:25,340
we basically you know figure out what we

00:07:23,690 --> 00:07:28,220
think the process will be to drop your

00:07:25,340 --> 00:07:31,340
notes you know so backup agent or the

00:07:28,220 --> 00:07:33,800
our age and all that works okay

00:07:31,340 --> 00:07:35,210
so testing a production sort of now I

00:07:33,800 --> 00:07:36,919
say testing abruption sort of because

00:07:35,210 --> 00:07:38,870
like I mentioned you know we we went

00:07:36,919 --> 00:07:42,470
through the whole process once on one of

00:07:38,870 --> 00:07:44,930
our test clusters to verify it works you

00:07:42,470 --> 00:07:46,430
know get everything out of the way so

00:07:44,930 --> 00:07:49,580
then with our detailed run book in hand

00:07:46,430 --> 00:07:51,910
we go through setting it up

00:07:49,580 --> 00:07:54,440
why are everything up data starts moving

00:07:51,910 --> 00:07:57,860
20 terabytes many many hours of data

00:07:54,440 --> 00:08:01,610
later it works it's done everything's

00:07:57,860 --> 00:08:03,830
caught up you know and so we disabled

00:08:01,610 --> 00:08:06,289
the F DVD or the link basically we break

00:08:03,830 --> 00:08:09,800
that so now this other cluster is

00:08:06,289 --> 00:08:12,889
independent do a bunch of tests for

00:08:09,800 --> 00:08:14,630
functionality and correctness clusters

00:08:12,889 --> 00:08:19,460
high availability again we're back in

00:08:14,630 --> 00:08:23,360
action okay so what do we learn

00:08:19,460 --> 00:08:27,020
first thing okay now we have a pretty

00:08:23,360 --> 00:08:30,919
thorough pretty good FTP DRM book built

00:08:27,020 --> 00:08:32,270
under s but it works another thing that

00:08:30,919 --> 00:08:35,360
kind of came out of this part of this

00:08:32,270 --> 00:08:37,490
process was okay realizing hum now that

00:08:35,360 --> 00:08:39,500
we're you know gonna shift more and more

00:08:37,490 --> 00:08:41,570
to using enemy only instances because

00:08:39,500 --> 00:08:43,880
there's a significant improvement in you

00:08:41,570 --> 00:08:46,430
know available disk i/o we actually

00:08:43,880 --> 00:08:49,400
started making some I M policies alright

00:08:46,430 --> 00:08:51,020
we run an AWS around how these instances

00:08:49,400 --> 00:08:53,600
potentially be stopped and started

00:08:51,020 --> 00:08:57,160
that's the main drawback is the instant

00:08:53,600 --> 00:09:00,480
stopped those local disks are gone so

00:08:57,160 --> 00:09:03,190
groups of things there automate checks

00:09:00,480 --> 00:09:05,590
automate more of the process right way

00:09:03,190 --> 00:09:08,590
said we had a few things in ansible but

00:09:05,590 --> 00:09:10,000
you know some of it was copy and like I

00:09:08,590 --> 00:09:12,700
run but right you copy this command you

00:09:10,000 --> 00:09:15,100
copy that command okay autumn in more of

00:09:12,700 --> 00:09:18,340
that add more automated health checks

00:09:15,100 --> 00:09:21,370
and that's also a thing that we took to

00:09:18,340 --> 00:09:24,880
you know other routine procedures in

00:09:21,370 --> 00:09:27,400
other places and then finally fleet

00:09:24,880 --> 00:09:31,570
replace so clear place is a term that we

00:09:27,400 --> 00:09:33,670
use to you know you have an entire F to

00:09:31,570 --> 00:09:35,020
be cluster and what we what we will do

00:09:33,670 --> 00:09:36,370
is we actually launch like a whole new

00:09:35,020 --> 00:09:38,740
set of instances and we froze a fleet

00:09:36,370 --> 00:09:42,010
replace using the native exclude hints

00:09:38,740 --> 00:09:45,160
include of capabilities of foundation

00:09:42,010 --> 00:09:47,650
Navy so this is actually just for in

00:09:45,160 --> 00:09:49,090
general operations uh something that we

00:09:47,650 --> 00:09:51,490
have had a lot of success with them and

00:09:49,090 --> 00:09:53,920
very much rely on them right as we scale

00:09:51,490 --> 00:09:57,070
away from clusters up and down based on

00:09:53,920 --> 00:09:58,960
point rate and demand we have a process

00:09:57,070 --> 00:10:01,240
of deciding how many instances we use in

00:09:58,960 --> 00:10:04,150
what size and so you know in the case

00:10:01,240 --> 00:10:06,310
we're going up or down we will launch

00:10:04,150 --> 00:10:07,960
those new instances bring them in they

00:10:06,310 --> 00:10:10,060
have a whole ansible based process that

00:10:07,960 --> 00:10:13,030
goes through and right make sure

00:10:10,060 --> 00:10:15,100
coordinators are moved starts the

00:10:13,030 --> 00:10:17,230
excludes you know depending on whether

00:10:15,100 --> 00:10:19,300
it's SSD or memory base we have a

00:10:17,230 --> 00:10:20,890
different process for that and at the

00:10:19,300 --> 00:10:23,470
end does a bunch of health checks right

00:10:20,890 --> 00:10:24,580
is it healthy are there any an original

00:10:23,470 --> 00:10:28,150
process it's kind of going through all

00:10:24,580 --> 00:10:29,470
these things and so having that process

00:10:28,150 --> 00:10:31,660
in place and in fact in some ways maybe

00:10:29,470 --> 00:10:33,850
I'm you know learning from this

00:10:31,660 --> 00:10:35,380
experience in you know adding more

00:10:33,850 --> 00:10:37,750
robust health checks looking from our

00:10:35,380 --> 00:10:39,430
corner cases automatically we also

00:10:37,750 --> 00:10:41,080
decided ok things like Colonel upgrades

00:10:39,430 --> 00:10:43,480
we can do in the fleet replace because

00:10:41,080 --> 00:10:45,010
what we found is that FDB as long as

00:10:43,480 --> 00:10:46,720
it's running it doesn't care right

00:10:45,010 --> 00:10:48,910
you're currently on trusty instances and

00:10:46,720 --> 00:10:50,290
you drop Bionic that's fine in fact we

00:10:48,910 --> 00:10:52,180
went through that process with all of

00:10:50,290 --> 00:10:54,670
our databases during the middle of this

00:10:52,180 --> 00:10:57,220
year completely moved all the Bionic and

00:10:54,670 --> 00:10:59,580
you know 4.15 or whatever the current

00:10:57,220 --> 00:11:04,150
colonel is which is also been good

00:10:59,580 --> 00:11:05,860
yeah so this is where I'm gonna head it

00:11:04,150 --> 00:11:10,990
back to Mr Z to talk about some other

00:11:05,860 --> 00:11:13,570
things you learn right we are running on

00:11:10,990 --> 00:11:15,370
EVs where he were in an EVs world we had

00:11:13,570 --> 00:11:17,140
a couple problems we'd run into either

00:11:15,370 --> 00:11:19,120
EVs would have a castro excel yer and

00:11:17,140 --> 00:11:21,280
since we had a duplicate copy for data

00:11:19,120 --> 00:11:23,500
in another mirror we could copy it and

00:11:21,280 --> 00:11:26,050
the only other use case for us to take

00:11:23,500 --> 00:11:26,650
an existing data set and just copy it

00:11:26,050 --> 00:11:29,140
someplace else

00:11:26,650 --> 00:11:31,870
so I'll kind of walk you through what we

00:11:29,140 --> 00:11:33,130
did from a process standpoint so very

00:11:31,870 --> 00:11:36,240
very important you should back up your

00:11:33,130 --> 00:11:38,830
coordinators we tracked that by using

00:11:36,240 --> 00:11:40,270
instance tags or I'm sorry volume tags

00:11:38,830 --> 00:11:41,710
so we could kind of relate the volumes

00:11:40,270 --> 00:11:44,500
back to where the coordinators were and

00:11:41,710 --> 00:11:46,720
then we stopped the cluster and we did

00:11:44,500 --> 00:11:47,980
an EBS snapshot so those are the simple

00:11:46,720 --> 00:11:50,070
steps but the important thing is make

00:11:47,980 --> 00:11:53,020
sure you keep track to your coordinators

00:11:50,070 --> 00:11:55,600
and then the lengthy piece was to

00:11:53,020 --> 00:11:57,820
resurrect those snapshots on other

00:11:55,600 --> 00:12:00,100
machines we were using LVM so part of

00:11:57,820 --> 00:12:03,220
was to also make sure LVM came back up

00:12:00,100 --> 00:12:05,020
and then fixed two coordinators and

00:12:03,220 --> 00:12:06,190
redistribute the cluster config to all

00:12:05,020 --> 00:12:09,580
the members and then restart the

00:12:06,190 --> 00:12:12,130
database pray and hope and wait and so

00:12:09,580 --> 00:12:13,750
with that what that look like on the

00:12:12,130 --> 00:12:15,100
file is we would have a set of machines

00:12:13,750 --> 00:12:17,080
that were the origin machines and we

00:12:15,100 --> 00:12:18,430
would copy the cluster elsewhere we just

00:12:17,080 --> 00:12:19,780
simply handed it to the file even though

00:12:18,430 --> 00:12:22,720
it says do not hand at it we actually

00:12:19,780 --> 00:12:25,480
did that and then would I think we're

00:12:22,720 --> 00:12:30,100
lazy we just Essie peed it out and then

00:12:25,480 --> 00:12:31,510
we started back up so these were the six

00:12:30,100 --> 00:12:32,710
tests we had but I think the important

00:12:31,510 --> 00:12:34,690
thing was we had always thought - this

00:12:32,710 --> 00:12:35,920
could work and then we were in situation

00:12:34,690 --> 00:12:38,200
we had to figure out doesn't actually

00:12:35,920 --> 00:12:39,430
work and I'm just gonna read from my

00:12:38,200 --> 00:12:41,590
notes here so this actually worked

00:12:39,430 --> 00:12:43,780
because the coordinators keep track of

00:12:41,590 --> 00:12:45,430
the topology of the cluster and so as

00:12:43,780 --> 00:12:47,650
everything came back online they checked

00:12:45,430 --> 00:12:50,320
in with the existing coordinators and

00:12:47,650 --> 00:12:51,640
then update all the essentially update

00:12:50,320 --> 00:12:54,010
and all the shards will be accounted for

00:12:51,640 --> 00:12:55,540
and behind the scenes and the bytes on

00:12:54,010 --> 00:12:58,720
disk must be Rhema T realized from the

00:12:55,540 --> 00:13:01,030
snapshot and I think this took well a

00:12:58,720 --> 00:13:03,460
matter of hours it was not fast but

00:13:01,030 --> 00:13:05,740
we've done this I think at least once

00:13:03,460 --> 00:13:07,270
maybe twice now it's a great way to copy

00:13:05,740 --> 00:13:08,800
an existing production instance up by

00:13:07,270 --> 00:13:11,389
cells if you can afford the downtime to

00:13:08,800 --> 00:13:13,560
do the snapshots

00:13:11,389 --> 00:13:15,180
yeah I think that's that's all we had

00:13:13,560 --> 00:13:17,249
don't know if we a little we have four

00:13:15,180 --> 00:13:21,660
minutes for questions if there's any

00:13:17,249 --> 00:13:23,699
questions I'll say as an aside the TCP

00:13:21,660 --> 00:13:25,639
segment retransmission we saw was not

00:13:23,699 --> 00:13:27,420
catastrophic until it was because

00:13:25,639 --> 00:13:28,800
processes would join the cluster

00:13:27,420 --> 00:13:30,509
transfer be transmitted so much they

00:13:28,800 --> 00:13:32,939
fall back out and so the cluster was

00:13:30,509 --> 00:13:35,430
thrashing so on larger clusters

00:13:32,939 --> 00:13:36,689
it was never stable and then we looked

00:13:35,430 --> 00:13:42,319
at all the charts that we saw this piece

00:13:36,689 --> 00:13:42,319
that was on the if you care is on a 415

00:13:42,499 --> 00:13:54,149
1037 Colonel don't use that one this is

00:13:52,410 --> 00:13:58,490
not using that yet this was just using

00:13:54,149 --> 00:13:58,490
GPS snapshots I have not looked at that

00:13:59,720 --> 00:14:04,529
I'm not sure anymore because we're not

00:14:01,889 --> 00:14:08,970
using GPS anymore we've moved over

00:14:04,529 --> 00:14:11,009
entirely to nvme because I have infinite

00:14:08,970 --> 00:14:13,829
IO which seemed to be one of the gating

00:14:11,009 --> 00:14:16,170
factors to running high performers

00:14:13,829 --> 00:14:17,879
database with the caveat that you have

00:14:16,170 --> 00:14:19,889
to put a lot of guardrails into making

00:14:17,879 --> 00:14:21,420
sure humans don't destroy your fleet you

00:14:19,889 --> 00:14:23,370
have to build a monitor amazon's

00:14:21,420 --> 00:14:25,079
infrastructure because they don't they

00:14:23,370 --> 00:14:26,160
don't tell you about smart you don't get

00:14:25,079 --> 00:14:28,529
to look at that stuff so you have to

00:14:26,160 --> 00:14:29,670
infer disk failures or pending disk

00:14:28,529 --> 00:14:33,180
failures by looking at that we're

00:14:29,670 --> 00:14:35,399
looking at the disk i/o right queue

00:14:33,180 --> 00:14:38,160
length as an indicator when it starts to

00:14:35,399 --> 00:14:40,050
go not straight it's an indicator that

00:14:38,160 --> 00:14:42,600
it may be time to just shut down F to be

00:14:40,050 --> 00:14:44,220
on that node and replace it I think the

00:14:42,600 --> 00:14:46,139
other thing we did too is we put only a

00:14:44,220 --> 00:14:48,870
few processes on each actual physical

00:14:46,139 --> 00:14:50,579
disk so that if one just dies the whole

00:14:48,870 --> 00:14:52,790
machine's not down just the processes on

00:14:50,579 --> 00:14:52,790
those

00:14:55,990 --> 00:15:03,519
all right thanks

00:14:58,250 --> 00:15:03,519

YouTube URL: https://www.youtube.com/watch?v=srFIuPaK6ZM


