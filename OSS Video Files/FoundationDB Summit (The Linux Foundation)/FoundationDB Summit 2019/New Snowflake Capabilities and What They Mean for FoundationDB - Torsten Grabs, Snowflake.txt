Title: New Snowflake Capabilities and What They Mean for FoundationDB - Torsten Grabs, Snowflake
Publication date: 2019-11-25
Playlist: FoundationDB Summit 2019
Description: 
	New Snowflake Capabilities and What They Mean for FoundationDB - Torsten Grabs, Snowflake 

Snowflake as a cloud-built data warehouse heavily relies on FoundationDB for metadata management. In this talk, we will explore some of Snowflake's newest capabilities such as database replication, data sharing, data pipelines and data lake integration. Their implementation presents us with interesting challenges, both for metadata management in FoundationDB and for elastic and scalable cloud data processing in general. We will discuss how we have tackled these challenges in Snowflake and where we see additional work in FoundationDB and beyond.
Captions: 
	00:00:00,030 --> 00:00:03,780
all righty good afternoon everyone my

00:00:01,979 --> 00:00:06,509
name is horseman and I'm a product

00:00:03,780 --> 00:00:08,610
manager at snowflake in my talk today I

00:00:06,509 --> 00:00:10,830
want to walk you through a few new

00:00:08,610 --> 00:00:12,450
capabilities in snowflake and explain a

00:00:10,830 --> 00:00:15,059
little bit more how they rely on

00:00:12,450 --> 00:00:17,220
foundation maybe given the limited time

00:00:15,059 --> 00:00:19,320
I had to be quite selective so I just

00:00:17,220 --> 00:00:21,779
picked my most favorite new capability

00:00:19,320 --> 00:00:22,859
in snowflake and I'll walk you through

00:00:21,779 --> 00:00:25,920
that

00:00:22,859 --> 00:00:28,109
a quick architecture slide as a lead-in

00:00:25,920 --> 00:00:30,179
for those that are not familiar with

00:00:28,109 --> 00:00:32,250
snowflake this is from a 30,000 foot

00:00:30,179 --> 00:00:34,950
perspective the architecture that

00:00:32,250 --> 00:00:37,739
snowflake uses you can see the clear

00:00:34,950 --> 00:00:41,550
separation between storage at the bottom

00:00:37,739 --> 00:00:43,320
and then compute at the top and customer

00:00:41,550 --> 00:00:45,270
query processing data processing all

00:00:43,320 --> 00:00:47,760
happens in the virtual warehouses that

00:00:45,270 --> 00:00:50,039
you see in the middle and the storage

00:00:47,760 --> 00:00:52,050
and the compute they scale completely

00:00:50,039 --> 00:00:54,210
independent of one another so you can

00:00:52,050 --> 00:00:56,370
have a very very huge data data set in

00:00:54,210 --> 00:00:59,370
storage you only pay for the storage and

00:00:56,370 --> 00:01:01,469
I think then you can put as much compute

00:00:59,370 --> 00:01:03,030
on top of it as you need at any given

00:01:01,469 --> 00:01:04,500
point in time that can be very small for

00:01:03,030 --> 00:01:07,200
instance during the weekends but it can

00:01:04,500 --> 00:01:08,430
be massive during a regular work a work

00:01:07,200 --> 00:01:10,770
week where you're running for instance

00:01:08,430 --> 00:01:13,229
your D designs were closed at the top of

00:01:10,770 --> 00:01:16,409
that sits let's say the brain or that

00:01:13,229 --> 00:01:18,869
that stitches all of this together this

00:01:16,409 --> 00:01:22,890
is where metadata management transaction

00:01:18,869 --> 00:01:24,869
management security and query

00:01:22,890 --> 00:01:27,810
optimization all happen

00:01:24,869 --> 00:01:30,150
most of these tasks rely heavily on FDB

00:01:27,810 --> 00:01:33,570
because the complete system stay for

00:01:30,150 --> 00:01:36,140
snowflake all of our metadata is is

00:01:33,570 --> 00:01:39,750
stored in NF dB

00:01:36,140 --> 00:01:42,079
so let's zoom into one of the features

00:01:39,750 --> 00:01:45,030
and try to understand a little bit more

00:01:42,079 --> 00:01:47,329
what it means to to add a feature to

00:01:45,030 --> 00:01:49,920
snowflake and what's the impact for f DB

00:01:47,329 --> 00:01:51,990
the one that I picked is what we

00:01:49,920 --> 00:01:54,630
launched earlier this year at our summit

00:01:51,990 --> 00:01:55,320
conference another umbrella of data

00:01:54,630 --> 00:01:58,710
pipelines

00:01:55,320 --> 00:02:01,490
then the scenario here is particularly

00:01:58,710 --> 00:02:04,290
with IOT scenarios mobile applications

00:02:01,490 --> 00:02:07,489
sensor applications you have data that's

00:02:04,290 --> 00:02:10,830
constantly born and this can be websites

00:02:07,489 --> 00:02:11,760
devices mobile phones the data either

00:02:10,830 --> 00:02:14,220
comes to

00:02:11,760 --> 00:02:16,200
messaging systems such as Kafka or it's

00:02:14,220 --> 00:02:18,629
stored somewhere in in the public cloud

00:02:16,200 --> 00:02:21,120
and from there what customers usually do

00:02:18,629 --> 00:02:22,680
is they load that either in in batch or

00:02:21,120 --> 00:02:24,750
through some of our continuous load

00:02:22,680 --> 00:02:26,940
functionality into what we call a

00:02:24,750 --> 00:02:29,129
staging table the staging table is

00:02:26,940 --> 00:02:30,780
essentially an append-only table that

00:02:29,129 --> 00:02:33,690
keeps track of all the changes that are

00:02:30,780 --> 00:02:35,099
happening at the source systems and then

00:02:33,690 --> 00:02:37,230
what customers want to do is they want

00:02:35,099 --> 00:02:39,510
to inspect the net new changes that came

00:02:37,230 --> 00:02:41,310
into the staging table and then figure

00:02:39,510 --> 00:02:43,799
out what are the changes that they need

00:02:41,310 --> 00:02:45,959
to apply to the actual business tables

00:02:43,799 --> 00:02:48,150
that are being used by analysts or

00:02:45,959 --> 00:02:49,590
business users and those are these

00:02:48,150 --> 00:02:50,340
transformations that you see in the

00:02:49,590 --> 00:02:52,590
middle here

00:02:50,340 --> 00:02:54,359
those can be simple merge statements in

00:02:52,590 --> 00:02:56,629
snowflakes sequel but they can also be

00:02:54,359 --> 00:02:59,459
more complex stored procedures that have

00:02:56,629 --> 00:03:02,040
really rich business logic in them the

00:02:59,459 --> 00:03:04,379
challenge here is you want to run these

00:03:02,040 --> 00:03:06,629
transformations automatically ideally

00:03:04,379 --> 00:03:08,670
every time when new changes are landing

00:03:06,629 --> 00:03:10,349
in your staging table you want to

00:03:08,670 --> 00:03:12,180
reliably figure out what those changes

00:03:10,349 --> 00:03:14,190
are send them through your business

00:03:12,180 --> 00:03:16,799
logic through your transformations and

00:03:14,190 --> 00:03:18,239
then apply the resulting changes to the

00:03:16,799 --> 00:03:20,549
target tables that you see on the

00:03:18,239 --> 00:03:22,380
right-hand side in this picture here in

00:03:20,549 --> 00:03:24,599
order to support this use case we

00:03:22,380 --> 00:03:26,549
introduced two new concepts - snowflake

00:03:24,599 --> 00:03:28,829
one is what we call a table change

00:03:26,549 --> 00:03:31,620
stream it gives you the ability to run a

00:03:28,829 --> 00:03:34,560
sequel statement against the change

00:03:31,620 --> 00:03:36,540
stream to figure out what changes what

00:03:34,560 --> 00:03:38,639
net new changes have happened to the

00:03:36,540 --> 00:03:40,200
underlying staging table since the last

00:03:38,639 --> 00:03:42,780
time that around this this statement

00:03:40,200 --> 00:03:45,269
this is typically used in what we call a

00:03:42,780 --> 00:03:47,459
task a task is the ability to schedule a

00:03:45,269 --> 00:03:49,049
piece of sequel that could be that merge

00:03:47,459 --> 00:03:51,630
statement or a call to a stored

00:03:49,049 --> 00:03:54,120
procedure and the sequel code in these

00:03:51,630 --> 00:03:56,310
tasks usually then refers to a stream to

00:03:54,120 --> 00:03:58,260
figure out what are the changes that

00:03:56,310 --> 00:03:59,940
have happened to the staging table then

00:03:58,260 --> 00:04:02,879
apply the business logic to it and then

00:03:59,940 --> 00:04:05,579
persist the results of that into the

00:04:02,879 --> 00:04:07,410
target tables now all this scheduling

00:04:05,579 --> 00:04:10,440
all this state management for these

00:04:07,410 --> 00:04:13,260
tasks is happening through FTP so we

00:04:10,440 --> 00:04:16,470
keep the metadata obviously for the the

00:04:13,260 --> 00:04:18,120
task definitions in FTB but also the

00:04:16,470 --> 00:04:19,769
queue management for what's the next

00:04:18,120 --> 00:04:22,830
task that needs to run the scheduling

00:04:19,769 --> 00:04:24,160
all of this is done through FTP that you

00:04:22,830 --> 00:04:26,020
see at the bottom here

00:04:24,160 --> 00:04:30,190
so I want to walk you through a quick

00:04:26,020 --> 00:04:33,970
demo of this so here we're connected to

00:04:30,190 --> 00:04:36,760
a snowflake account let's start by

00:04:33,970 --> 00:04:39,190
creating a staging table for a customer

00:04:36,760 --> 00:04:41,830
scenario here is my staging table for

00:04:39,190 --> 00:04:44,590
the customer customer ID name date and a

00:04:41,830 --> 00:04:48,760
zip code now let's create the target

00:04:44,590 --> 00:04:52,180
table here again called customer ID name

00:04:48,760 --> 00:04:54,010
date and the zip code now let's pretend

00:04:52,180 --> 00:04:57,310
that a change is happening to the

00:04:54,010 --> 00:05:02,170
staging table and let's insert a new row

00:04:57,310 --> 00:05:08,370
to the staging table here and let's see

00:05:02,170 --> 00:05:12,460
when that finishes not sure why not

00:05:08,370 --> 00:05:16,150
there we go all right so let's take a

00:05:12,460 --> 00:05:19,510
look add the staging table let's move

00:05:16,150 --> 00:05:21,640
this up a little bit so here you can see

00:05:19,510 --> 00:05:24,160
this is our staging table we have a new

00:05:21,640 --> 00:05:28,630
event that snowflake moved in ads if

00:05:24,160 --> 00:05:30,430
code 401 but if we look at the actual

00:05:28,630 --> 00:05:32,320
customer table it's still empty so that

00:05:30,430 --> 00:05:34,000
change record has not been picked up

00:05:32,320 --> 00:05:36,790
from the staging table yet it has not

00:05:34,000 --> 00:05:40,060
been processed yet so here comes a merge

00:05:36,790 --> 00:05:41,980
statement that implements the that

00:05:40,060 --> 00:05:43,960
changed processing and it does two

00:05:41,980 --> 00:05:46,000
things so first it checks whether there

00:05:43,960 --> 00:05:48,370
is ordering already a customer record in

00:05:46,000 --> 00:05:50,200
the target table if so then it will just

00:05:48,370 --> 00:05:52,390
apply an update with the new values to

00:05:50,200 --> 00:05:54,160
that table if there is no record for

00:05:52,390 --> 00:05:55,720
this customer in the target table then

00:05:54,160 --> 00:05:58,330
it will run an insertion you can see

00:05:55,720 --> 00:06:00,040
that here in the when matched Clause we

00:05:58,330 --> 00:06:02,320
are doing the update and in a not match

00:06:00,040 --> 00:06:05,340
clause we are running the insert so

00:06:02,320 --> 00:06:08,440
let's run this merge statement over the

00:06:05,340 --> 00:06:10,960
the change that we have in the staging

00:06:08,440 --> 00:06:16,060
table and now you can see there is

00:06:10,960 --> 00:06:19,480
snowflake add zip code 401 as it is in

00:06:16,060 --> 00:06:21,160
our staging table now I ran this merge

00:06:19,480 --> 00:06:22,810
statement manually just to illustrate

00:06:21,160 --> 00:06:24,640
the processing that we wanted to happen

00:06:22,810 --> 00:06:27,280
now let's automate this using streams

00:06:24,640 --> 00:06:30,850
and tasks let me first clean up my

00:06:27,280 --> 00:06:33,520
staging table here and then the first

00:06:30,850 --> 00:06:36,010
step typically is to create a stream

00:06:33,520 --> 00:06:37,779
over the staging table that gives me the

00:06:36,010 --> 00:06:40,389
ability to incrementally

00:06:37,779 --> 00:06:43,719
for changes that are happening to our

00:06:40,389 --> 00:06:46,209
staging table so now let's add a change

00:06:43,719 --> 00:06:48,219
record into this gauging table let's say

00:06:46,209 --> 00:06:53,740
that snowflake moves from zip code 401

00:06:48,219 --> 00:06:56,319
to 402 which we actually did and you can

00:06:53,740 --> 00:06:59,709
see that change here in the staging

00:06:56,319 --> 00:07:01,569
table but it's not yet processed for the

00:06:59,709 --> 00:07:03,580
actual targets table so it's still

00:07:01,569 --> 00:07:05,919
sitting there and remember we haven't

00:07:03,580 --> 00:07:08,169
defined any tasks to do that but let's

00:07:05,919 --> 00:07:09,909
take a look at the stream so here is the

00:07:08,169 --> 00:07:11,499
stream object it tells us there is a

00:07:09,909 --> 00:07:13,539
record sitting in the staging table

00:07:11,499 --> 00:07:15,159
ready to be picked up and it's due to an

00:07:13,539 --> 00:07:18,729
insertion that happened to this staging

00:07:15,159 --> 00:07:22,300
table so now let's define a task that

00:07:18,729 --> 00:07:24,969
runs every minute in a given snowflake

00:07:22,300 --> 00:07:26,740
warehouse and the body of the task is

00:07:24,969 --> 00:07:29,110
the merge statement that we looked at

00:07:26,740 --> 00:07:31,959
earlier so this essentially means this

00:07:29,110 --> 00:07:34,719
task wakes up once every minute and is

00:07:31,959 --> 00:07:36,729
looking for changes to the station table

00:07:34,719 --> 00:07:39,519
through the lens of this stream and then

00:07:36,729 --> 00:07:43,029
running the merge logic on top of it so

00:07:39,519 --> 00:07:48,240
let's get this task created and let's

00:07:43,029 --> 00:07:52,300
kick it off with a task resume statement

00:07:48,240 --> 00:07:55,089
here is the task and we can look at the

00:07:52,300 --> 00:07:59,469
task history and the task history is

00:07:55,089 --> 00:08:02,559
essentially a look into into FDB

00:07:59,469 --> 00:08:04,959
metadata which gives me visibility into

00:08:02,559 --> 00:08:06,729
the queue which changes or which tasks

00:08:04,959 --> 00:08:08,800
executions have been scheduled in the

00:08:06,729 --> 00:08:10,389
past were they successful or not what

00:08:08,800 --> 00:08:12,909
where the error codes that we have hid

00:08:10,389 --> 00:08:16,329
along the way and what are the scheduled

00:08:12,909 --> 00:08:18,369
tasks that are subject to execution next

00:08:16,329 --> 00:08:20,919
when you can see here that in about half

00:08:18,369 --> 00:08:24,069
a minute the next incarnation of that

00:08:20,919 --> 00:08:26,499
task is supposed to run and until that

00:08:24,069 --> 00:08:30,699
point in time we should see the old zip

00:08:26,499 --> 00:08:33,579
code int in our customer table and we

00:08:30,699 --> 00:08:37,779
should still see the change record in

00:08:33,579 --> 00:08:43,439
our stream so let's wait for a few more

00:08:37,779 --> 00:08:46,420
seconds and look at some of our adoption

00:08:43,439 --> 00:08:48,490
data before I jump back

00:08:46,420 --> 00:08:50,830
so here you can see the number of tasks

00:08:48,490 --> 00:08:52,810
executions on a daily basis plotted over

00:08:50,830 --> 00:08:55,000
a time line and you can see that we are

00:08:52,810 --> 00:08:58,690
now well above one and a half million

00:08:55,000 --> 00:09:00,880
tasks executions every day imagine that

00:08:58,690 --> 00:09:03,490
all of those task executions trigger in

00:09:00,880 --> 00:09:06,400
the order of ten maybe a few dozens FTP

00:09:03,490 --> 00:09:08,290
transactions each of them so there is a

00:09:06,400 --> 00:09:09,430
substan substantial additional load to

00:09:08,290 --> 00:09:11,230
it and you can see that one of our

00:09:09,430 --> 00:09:14,290
customers went super crazy creating

00:09:11,230 --> 00:09:16,840
tasks we obviously found that out and we

00:09:14,290 --> 00:09:18,640
were talking to to them and we found an

00:09:16,840 --> 00:09:20,620
easier way to implement their particular

00:09:18,640 --> 00:09:23,350
use case so that's why you're seeing us

00:09:20,620 --> 00:09:26,080
coming back to that regular hockey-stick

00:09:23,350 --> 00:09:28,090
trajectory that we are that we're on

00:09:26,080 --> 00:09:31,450
just wanted to show you that real quick

00:09:28,090 --> 00:09:33,850
now let's jump back into the demo here

00:09:31,450 --> 00:09:36,580
and let's take a look at our target

00:09:33,850 --> 00:09:39,910
table and you can see now the zip code

00:09:36,580 --> 00:09:42,130
has changed to 402 so our task has

00:09:39,910 --> 00:09:45,010
probably run let's also take a look at

00:09:42,130 --> 00:09:47,110
our stream and here you see the stream

00:09:45,010 --> 00:09:49,720
is empty so we have picked up the change

00:09:47,110 --> 00:09:52,630
from this stream and followed it forward

00:09:49,720 --> 00:09:57,370
into the target table and let me also

00:09:52,630 --> 00:09:59,410
run the history statement here again and

00:09:57,370 --> 00:10:03,160
if we scroll down you can see our

00:09:59,410 --> 00:10:05,170
successful execution down here and you

00:10:03,160 --> 00:10:12,460
can see this the next one is scheduled

00:10:05,170 --> 00:10:15,340
for about a minute out from now with

00:10:12,460 --> 00:10:20,560
that let me jump to my last slide here

00:10:15,340 --> 00:10:22,750
and showing my shameless plug so as

00:10:20,560 --> 00:10:25,330
always we're looking for great engineers

00:10:22,750 --> 00:10:28,270
we have offices in San Mateo California

00:10:25,330 --> 00:10:30,790
in Bellevue Washington and in Berlin

00:10:28,270 --> 00:10:32,740
Germany if any of these items that you

00:10:30,790 --> 00:10:34,720
see up here sound remotely interesting

00:10:32,740 --> 00:10:36,100
to you than we'd love to chat so we're

00:10:34,720 --> 00:10:38,260
working on all sorts of interesting

00:10:36,100 --> 00:10:40,330
spaces from core data warehousing to

00:10:38,260 --> 00:10:44,140
data lakes and data pipelines as you

00:10:40,330 --> 00:10:45,550
just saw and last but not least FTP with

00:10:44,140 --> 00:10:47,880
that thank you very much and have a

00:10:45,550 --> 00:10:47,880

YouTube URL: https://www.youtube.com/watch?v=j0suBhYKav4


