Title: NuGraph: GraphDB as a Cloud Service Built Upon JanusGraph and FoundationDB - Jun Li & Hieu Nguyen
Publication date: 2019-11-25
Playlist: FoundationDB Summit 2019
Description: 
	NuGraph: GraphDB as a Cloud Service Built Upon JanusGraph and FoundationDB - Jun Li & Hieu Nguyen, eBay 

In eBay, we have many applications that require Graph Databases to capture and update business insights from diverse data sources and have insights consumed at real-time. Often such data is sensitive and requires secure protection. Some of these applications can have billions of nodes and billions of edges.  We have built a GraphDB cloud service called NuGraph, which is based on JanusGraph. FoundationDB is chosen as the JanusGraph’s storage plugin, because of its high-performance and distributed transaction support. We will present our GraphDB architecture, and focus on how we deploy and manage FoundationDB in Kubernetes, how we improve JansGraph query performance in a cross-data center environment, how we bulk load the graph into FoundationDB with its transactional support, and how we secure the 3-tier cloud service with limited security support from FoundationDB.
Captions: 
	00:00:01,350 --> 00:00:05,250
captain moon so - June and this is my

00:00:04,920 --> 00:00:08,610
career

00:00:05,250 --> 00:00:10,040
Hugh we both from the eBay the team is

00:00:08,610 --> 00:00:12,300
called new data team which is

00:00:10,040 --> 00:00:13,830
responsible or developing and

00:00:12,300 --> 00:00:15,599
maintaining adequate data infrastructure

00:00:13,830 --> 00:00:17,960
in eBay so today we like to present to

00:00:15,599 --> 00:00:20,250
you the new graph which is a co-op DB

00:00:17,960 --> 00:00:24,330
services step you upon Jenny Scott and

00:00:20,250 --> 00:00:26,369
foundation DB well just to give you a

00:00:24,330 --> 00:00:28,560
very brief introduction about the crafty

00:00:26,369 --> 00:00:31,289
be so a crop is consist of vertices and

00:00:28,560 --> 00:00:32,550
edges item on the right hand side on the

00:00:31,289 --> 00:00:34,739
right hand side diagram you can see the

00:00:32,550 --> 00:00:36,480
airport is a suitable test kinetic

00:00:34,739 --> 00:00:38,879
between these Airport it's a circle

00:00:36,480 --> 00:00:40,890
route which is the edge and then you had

00:00:38,879 --> 00:00:42,840
the property a distance attached total

00:00:40,890 --> 00:00:44,760
eight right route and also the property

00:00:42,840 --> 00:00:47,579
attached to the name which is the belong

00:00:44,760 --> 00:00:49,800
to the Buddhist Airport so the graph

00:00:47,579 --> 00:00:51,449
traversal query is actually to start

00:00:49,800 --> 00:00:53,399
with a set of vertices and then you

00:00:51,449 --> 00:00:55,500
following the incoming outgoing edge and

00:00:53,399 --> 00:00:56,989
move to the next set on the boat s and

00:00:55,500 --> 00:00:59,399
then you start the enzyme e Nathan again

00:00:56,989 --> 00:01:01,199
so then we also have call one hot query

00:00:59,399 --> 00:01:03,960
which is a start with the one good test

00:01:01,199 --> 00:01:07,110
and then and they move to the kinetic

00:01:03,960 --> 00:01:09,720
blue test the next immediate neighbor

00:01:07,110 --> 00:01:12,680
and so this one hub and similar you can

00:01:09,720 --> 00:01:16,290
have a two hops three hop in hop query

00:01:12,680 --> 00:01:17,880
so anyway we had many application that

00:01:16,290 --> 00:01:20,130
like to have crafty be and this

00:01:17,880 --> 00:01:21,960
application in karate lesson knowledge

00:01:20,130 --> 00:01:27,270
graph IT infrastructure management and

00:01:21,960 --> 00:01:28,619
product recommendation so this graph DBS

00:01:27,270 --> 00:01:31,799
required a real-time query

00:01:28,619 --> 00:01:33,689
near Luton update page update and also

00:01:31,799 --> 00:01:36,420
had a pot loading at an initial time

00:01:33,689 --> 00:01:38,880
when the quality B is being constructed

00:01:36,420 --> 00:01:40,979
so most of this application is cooling

00:01:38,880 --> 00:01:42,840
the first initial loading phase actually

00:01:40,979 --> 00:01:47,430
a GB the traffic pattern that's read

00:01:42,840 --> 00:01:49,170
heavy so for the rest of presentation we

00:01:47,430 --> 00:01:51,360
will go over this new graph architecture

00:01:49,170 --> 00:01:52,680
and then I detailed the important

00:01:51,360 --> 00:01:54,360
feature that we developed for the

00:01:52,680 --> 00:01:56,939
foundation to be storage packing and

00:01:54,360 --> 00:01:58,649
then we also try to identify some

00:01:56,939 --> 00:02:00,390
feature and improvement that we like to

00:01:58,649 --> 00:02:03,899
have fun foundation DB and finally the

00:02:00,390 --> 00:02:06,930
conclusion so this is a sweetie

00:02:03,899 --> 00:02:08,369
architecture of a new graph this you

00:02:06,930 --> 00:02:09,629
start with the graph application on the

00:02:08,369 --> 00:02:11,459
left hand side and then you leave the

00:02:09,629 --> 00:02:13,349
application leave with our new Cochrane

00:02:11,459 --> 00:02:15,359
Library which is divided by RT

00:02:13,349 --> 00:02:17,400
and then the request will rouse with G

00:02:15,359 --> 00:02:19,579
RPC and enter to the canoe craft service

00:02:17,400 --> 00:02:22,530
tier and then the quest actually will be

00:02:19,579 --> 00:02:24,900
landed at the particular no and the

00:02:22,530 --> 00:02:27,209
inside is no the request is a go to the

00:02:24,900 --> 00:02:28,650
Janus graph and then it goes down to

00:02:27,209 --> 00:02:31,079
this Janus to a foundation TB storage

00:02:28,650 --> 00:02:33,299
plug-in and user this storage Park in

00:02:31,079 --> 00:02:35,069
that translate the graph period into the

00:02:33,299 --> 00:02:37,889
foundation to be specific key reduce

00:02:35,069 --> 00:02:39,959
table based query and then query enrich

00:02:37,889 --> 00:02:42,329
this stuff unnecessarily Beck install

00:02:39,959 --> 00:02:44,159
and this store is a store both the data

00:02:42,329 --> 00:02:46,469
table and the index table for the graph

00:02:44,159 --> 00:02:48,510
DB and separately we also have the

00:02:46,469 --> 00:02:50,519
management train to manage the cluster

00:02:48,510 --> 00:02:52,769
and managers schema for both the service

00:02:50,519 --> 00:02:57,030
tier and therefore this is VPP custard

00:02:52,769 --> 00:02:59,180
here so Jenna scrub it's inherently is a

00:02:57,030 --> 00:03:01,919
support to change transaction however

00:02:59,180 --> 00:03:03,120
it's up to the storage plug-in to decide

00:03:01,919 --> 00:03:06,239
whether they're going to implement the

00:03:03,120 --> 00:03:07,919
changes in logit or not so today all of

00:03:06,239 --> 00:03:09,650
the storage plug-in download that you

00:03:07,919 --> 00:03:14,310
can get from the Janus crop

00:03:09,650 --> 00:03:17,400
probabilities and not do not have a

00:03:14,310 --> 00:03:18,810
transaction the transaction so Berkeley

00:03:17,400 --> 00:03:21,659
DB is the one day support condition but

00:03:18,810 --> 00:03:22,829
it's not horizontally scalable so but

00:03:21,659 --> 00:03:24,629
the chip generation is the one that

00:03:22,829 --> 00:03:26,370
provided by foundation DB is key to

00:03:24,629 --> 00:03:28,229
address the data inconsistency that we

00:03:26,370 --> 00:03:29,790
encounter when we develop this graph DB

00:03:28,229 --> 00:03:32,519
so I'm going to spend the next three

00:03:29,790 --> 00:03:34,019
slide to identify the key data

00:03:32,519 --> 00:03:36,930
inconsistent that we encounter and to

00:03:34,019 --> 00:03:39,090
see that and show you that did the DTH

00:03:36,930 --> 00:03:42,540
ingestion is the way to tackle all this

00:03:39,090 --> 00:03:45,180
issue in a very straightforward way so

00:03:42,540 --> 00:03:47,519
the first one is a is you have two

00:03:45,180 --> 00:03:51,419
vertices you add H this is a very sing

00:03:47,519 --> 00:03:53,759
simple graph DB operation and because

00:03:51,419 --> 00:03:56,549
the graph query usually required a

00:03:53,759 --> 00:03:58,259
forward to whistle from the v1 to v2 on

00:03:56,549 --> 00:04:00,900
this diagram and also he supported

00:03:58,259 --> 00:04:02,549
backward traversal from v2 to the v1 so

00:04:00,900 --> 00:04:04,560
as a result the edge information had to

00:04:02,549 --> 00:04:06,629
store in two pressures the first role

00:04:04,560 --> 00:04:08,280
belong to the root test one the second

00:04:06,629 --> 00:04:10,079
rope you known to the boot s2 but

00:04:08,280 --> 00:04:12,269
because these two vertices car is

00:04:10,079 --> 00:04:13,889
actually shot in different shara and

00:04:12,269 --> 00:04:16,590
therefore this edge information of the

00:04:13,889 --> 00:04:19,109
become a de tribute kosher update right

00:04:16,590 --> 00:04:21,570
in the truth is no eventual consistency

00:04:19,109 --> 00:04:23,490
Stern supports now the first row updates

00:04:21,570 --> 00:04:25,590
the seed but not the second one so as a

00:04:23,490 --> 00:04:27,150
result the forward traversal query come

00:04:25,590 --> 00:04:28,770
from we went to v2 we see

00:04:27,150 --> 00:04:30,930
but we want back what reversal to be

00:04:28,770 --> 00:04:33,750
when we fail so if you had the teacher

00:04:30,930 --> 00:04:35,250
congestion that both update of the row

00:04:33,750 --> 00:04:36,710
up there HDPE alone to the single

00:04:35,250 --> 00:04:39,000
generation then either all of this

00:04:36,710 --> 00:04:42,650
updates the seat and therefore you both

00:04:39,000 --> 00:04:45,810
queries to see or both actually are or

00:04:42,650 --> 00:04:47,460
both will not fail and will not become

00:04:45,810 --> 00:04:51,020
visible to external and therefore both

00:04:47,460 --> 00:04:54,960
query with so nothing so there is still

00:04:51,020 --> 00:04:58,410
consistent so second wiser theory with

00:04:54,960 --> 00:05:00,660
these are many of these operation

00:04:58,410 --> 00:05:02,850
related to this a crafty B we had Newton

00:05:00,660 --> 00:05:04,020
query up mutant query near Luton update

00:05:02,850 --> 00:05:05,729
match update

00:05:04,020 --> 00:05:09,030
so all these go to a single data be

00:05:05,729 --> 00:05:11,100
database right with the church Anderson

00:05:09,030 --> 00:05:12,810
all these critical simultaneous come and

00:05:11,100 --> 00:05:14,250
they all get the consistent result and

00:05:12,810 --> 00:05:16,650
this is very different from the

00:05:14,250 --> 00:05:18,690
traditional lambda architecture so if

00:05:16,650 --> 00:05:20,820
follow the unwound architecture it

00:05:18,690 --> 00:05:22,320
propose that you have temporary db2 host

00:05:20,820 --> 00:05:25,110
this recent update and you also have

00:05:22,320 --> 00:05:27,270
master DB and you merge the master data

00:05:25,110 --> 00:05:29,220
periodically from the temporary DB back

00:05:27,270 --> 00:05:31,169
to the master DB what that's introduced

00:05:29,220 --> 00:05:32,910
at least two challenges the first one is

00:05:31,169 --> 00:05:36,389
that well the query has to spend across

00:05:32,910 --> 00:05:37,979
two database or a wise appear once the

00:05:36,389 --> 00:05:39,479
temporary DB when's the master DB and

00:05:37,979 --> 00:05:41,669
you had emerged at that data back by the

00:05:39,479 --> 00:05:44,099
application well therefore the complex

00:05:41,669 --> 00:05:46,250
query is very challenging the second one

00:05:44,099 --> 00:05:48,330
is actually introduced a beg and

00:05:46,250 --> 00:05:50,310
compress it in terms of data management

00:05:48,330 --> 00:05:52,800
because now you had a temporary DB

00:05:50,310 --> 00:05:55,200
back and DB both serving online Cioffi

00:05:52,800 --> 00:05:56,699
in particular you have all of these two

00:05:55,200 --> 00:05:58,260
can serve in the right traffic and now

00:05:56,699 --> 00:05:59,460
how are you going to merge these two in

00:05:58,260 --> 00:06:01,729
a consistent way well that's very

00:05:59,460 --> 00:06:05,130
challenging as well

00:06:01,729 --> 00:06:06,930
so this slide is to show you the ferry

00:06:05,130 --> 00:06:08,910
handling of Janus graph right suppose

00:06:06,930 --> 00:06:10,289
you have crying you have the node you're

00:06:08,910 --> 00:06:12,840
going to update a note of journey squad

00:06:10,289 --> 00:06:15,720
so updated no actually introduce you is

00:06:12,840 --> 00:06:19,500
a required just change card will give

00:06:15,720 --> 00:06:22,949
you the new butas ID for each new no

00:06:19,500 --> 00:06:24,810
creation so there's a set of mutation

00:06:22,949 --> 00:06:27,479
each mutation corresponding or key value

00:06:24,810 --> 00:06:30,330
pair update and that those keys derive

00:06:27,479 --> 00:06:32,039
from this vertex ID right suppose now in

00:06:30,330 --> 00:06:34,020
this eventual consistent system some

00:06:32,039 --> 00:06:36,300
update or some nutrition fail some

00:06:34,020 --> 00:06:38,760
limitations to seed the mutation succeed

00:06:36,300 --> 00:06:40,590
why we persist to the begin and then the

00:06:38,760 --> 00:06:41,129
you also saw the assertion because the

00:06:40,590 --> 00:06:43,319
some of these

00:06:41,129 --> 00:06:45,029
it isn't fair so now when this crime

00:06:43,319 --> 00:06:47,339
received exception and mega secondly try

00:06:45,029 --> 00:06:49,439
and then this would lead to the

00:06:47,339 --> 00:06:52,110
Neo British ID and this lead to the new

00:06:49,439 --> 00:06:53,459
mutation stare and listenin suppose now

00:06:52,110 --> 00:06:56,099
these mutations has succeed and

00:06:53,459 --> 00:06:58,080
therefore you have this duplicate a

00:06:56,099 --> 00:07:00,089
mutation right in this case M 1 and M 2

00:06:58,080 --> 00:07:01,589
and 1 prime name to pawn in this pagan

00:07:00,089 --> 00:07:04,139
database and therefore you get

00:07:01,589 --> 00:07:05,759
potentially duplicated vertices right so

00:07:04,139 --> 00:07:07,949
this inherently the problem that you

00:07:05,759 --> 00:07:09,779
have in jail squad if you do not have a

00:07:07,949 --> 00:07:10,949
teacher witch and Jason began well you

00:07:09,779 --> 00:07:12,989
will have the to which and this impact

00:07:10,949 --> 00:07:15,059
and that this fairy Hendrie is very

00:07:12,989 --> 00:07:16,919
simple you just have a row back and this

00:07:15,059 --> 00:07:21,539
load back we take care this a puzzle

00:07:16,919 --> 00:07:23,849
update failure for you so foundation DB

00:07:21,539 --> 00:07:26,459
which we choose choose it as a new graph

00:07:23,849 --> 00:07:29,189
back end because it is T to be coaster

00:07:26,459 --> 00:07:31,379
injection and in addition it has the

00:07:29,189 --> 00:07:33,779
horizontal scalability cause reasoned

00:07:31,379 --> 00:07:35,610
hyper ability low latency key value

00:07:33,779 --> 00:07:38,490
store assess and also have intelligent

00:07:35,610 --> 00:07:40,709
self management so the this diagram just

00:07:38,490 --> 00:07:42,300
show you that how we deploy our custody

00:07:40,709 --> 00:07:45,839
in the kubernetes environment we have

00:07:42,300 --> 00:07:48,119
three cluster pc-1 pc2 forms the active

00:07:45,839 --> 00:07:52,979
region and then tc3 from the stem by

00:07:48,119 --> 00:07:56,249
reason so a fan is me storage hugging is

00:07:52,979 --> 00:07:59,939
best the one that that provided by tech

00:07:56,249 --> 00:08:02,399
we miss so who presented is a gen scrub

00:07:59,939 --> 00:08:03,719
foundation to be plugged in pre and

00:08:02,399 --> 00:08:05,279
hasten last year in this in this

00:08:03,719 --> 00:08:08,490
foundation tree summit as well

00:08:05,279 --> 00:08:10,439
so we take these why he prove has and

00:08:08,490 --> 00:08:12,719
then fix the pox and we also add the new

00:08:10,439 --> 00:08:14,579
features including AC iterator

00:08:12,719 --> 00:08:16,679
we only knew craft services re only

00:08:14,579 --> 00:08:21,059
query optimization across the betas and

00:08:16,679 --> 00:08:22,769
data center and among others so here is

00:08:21,059 --> 00:08:24,389
an S is going to present to you this

00:08:22,769 --> 00:08:30,829
important feature that we develop in the

00:08:24,389 --> 00:08:30,829
Fondation be storage pocket hello

00:08:33,729 --> 00:08:42,289
hello yeah drew just introduce to you

00:08:38,960 --> 00:08:44,540
the new grab architecture and that we

00:08:42,289 --> 00:08:46,310
use for agenda B as our storage began in

00:08:44,540 --> 00:08:48,950
the next slides I will introduce some

00:08:46,310 --> 00:08:50,930
important features of the storage

00:08:48,950 --> 00:08:54,170
bondage of the furniture in DB storage

00:08:50,930 --> 00:08:56,210
plugin of janice graph so first of all

00:08:54,170 --> 00:08:59,330
is a queries on fetching using iterator

00:08:56,210 --> 00:09:01,310
so as you know furniture in DB can query

00:08:59,330 --> 00:09:03,500
the data using two more eyes the

00:09:01,310 --> 00:09:05,180
blocking or non blocking blocking on the

00:09:03,500 --> 00:09:08,810
reason can be returned in one blocking

00:09:05,180 --> 00:09:11,030
call or non blocking we can use a acing

00:09:08,810 --> 00:09:14,120
iterator to first original resolve one

00:09:11,030 --> 00:09:16,220
by one on demand so the high-level

00:09:14,120 --> 00:09:19,070
Chattisgarh query executor relies

00:09:16,220 --> 00:09:22,250
heavily on iterators to put the data on

00:09:19,070 --> 00:09:24,350
demand so it is very in line with the

00:09:22,250 --> 00:09:26,240
non blocking mode of origin DB so that's

00:09:24,350 --> 00:09:29,450
why we use icing iterator you know

00:09:26,240 --> 00:09:30,890
china's grab storage plugin so here just

00:09:29,450 --> 00:09:33,770
to give it an example we can have a

00:09:30,890 --> 00:09:37,900
queries for example g dot v dot iterator

00:09:33,770 --> 00:09:42,830
to get all the vertices in the graph so

00:09:37,900 --> 00:09:44,930
the jealous grab executed we would first

00:09:42,830 --> 00:09:48,020
get the iterator and then from the

00:09:44,930 --> 00:09:51,410
iterator you can look through each in a

00:09:48,020 --> 00:09:55,250
while loop to get the result and then

00:09:51,410 --> 00:09:56,630
processing it so with blocking the all

00:09:55,250 --> 00:09:58,730
the data will be fetched in the first

00:09:56,630 --> 00:10:02,000
call so that this is not good for our

00:09:58,730 --> 00:10:04,790
new graph service because it increase

00:10:02,000 --> 00:10:08,810
the memory usage and also the cpu in

00:10:04,790 --> 00:10:11,900
case the janice crowd executed terminate

00:10:08,810 --> 00:10:15,350
the iterator as soon as it miss comes

00:10:11,900 --> 00:10:18,020
some condition so with none broken the

00:10:15,350 --> 00:10:23,210
first cone is actually return only the

00:10:18,020 --> 00:10:24,980
icing iterator of furniture DB and while

00:10:23,210 --> 00:10:27,080
it fetching through the eater iterator

00:10:24,980 --> 00:10:30,140
one by one it can get the result as

00:10:27,080 --> 00:10:34,730
needed so this is a very good solution

00:10:30,140 --> 00:10:38,820
it keep our service low memory and main

00:10:34,730 --> 00:10:41,640
CPU consumption the second

00:10:38,820 --> 00:10:43,740
future is request contact propagation so

00:10:41,640 --> 00:10:45,900
a request come from the client can has

00:10:43,740 --> 00:10:48,540
some request contact that carries

00:10:45,900 --> 00:10:52,590
metadata such as request ID kind address

00:10:48,540 --> 00:10:55,440
and application ID so the service upon

00:10:52,590 --> 00:10:57,180
receiving a request can get the context

00:10:55,440 --> 00:10:59,460
and attach it to the track locker and

00:10:57,180 --> 00:11:00,660
the furnishing DB storage protein can

00:10:59,460 --> 00:11:01,290
get the request contact from the

00:11:00,660 --> 00:11:06,420
threadlocker

00:11:01,290 --> 00:11:10,860
and then and then enforcing some

00:11:06,420 --> 00:11:13,560
condition at the that layer to to handle

00:11:10,860 --> 00:11:16,710
the request differently so note that by

00:11:13,560 --> 00:11:18,480
this way we bypass the Janus grab so

00:11:16,710 --> 00:11:22,230
there is no code change at the Janus

00:11:18,480 --> 00:11:26,400
grab layer and we can get the request

00:11:22,230 --> 00:11:29,340
contact at the storage plugin so how is

00:11:26,400 --> 00:11:32,160
the contact useful so with that context

00:11:29,340 --> 00:11:36,440
first we can enable the new web service

00:11:32,160 --> 00:11:38,940
to run in read-only mode in this mode

00:11:36,440 --> 00:11:41,100
basically every write request from the

00:11:38,940 --> 00:11:43,680
client will be directed so you may think

00:11:41,100 --> 00:11:45,980
it is a simple solution just enable

00:11:43,680 --> 00:11:48,330
read-only mode at the FPV database level

00:11:45,980 --> 00:11:50,760
unfortunately it is impossible because

00:11:48,330 --> 00:11:52,890
the Janus grab underneath it requires

00:11:50,760 --> 00:11:56,550
some administrative write operations to

00:11:52,890 --> 00:11:59,940
the database and if we enable read-only

00:11:56,550 --> 00:12:01,980
mode at the MPP then on the drive from

00:11:59,940 --> 00:12:03,210
the admin app from this right on the

00:12:01,980 --> 00:12:07,010
destroy form transcript would be

00:12:03,210 --> 00:12:09,720
directed to so the solution is that we

00:12:07,010 --> 00:12:10,860
check the parish in DB storage protein

00:12:09,720 --> 00:12:13,140
for the request context

00:12:10,860 --> 00:12:14,700
it exists in meaning that this this

00:12:13,140 --> 00:12:17,580
request come from the client so that we

00:12:14,700 --> 00:12:20,550
allow we we don't allow it to go to

00:12:17,580 --> 00:12:22,050
furnish NDB cluster otherwise those

00:12:20,550 --> 00:12:26,940
requests are from Janice grub and we

00:12:22,050 --> 00:12:30,180
allow them to go to the partition DB the

00:12:26,940 --> 00:12:32,460
second use case request context is that

00:12:30,180 --> 00:12:36,030
we enable prefetching the transaction

00:12:32,460 --> 00:12:37,770
version so remember that average 14 DB

00:12:36,030 --> 00:12:40,740
transaction need to get the transaction

00:12:37,770 --> 00:12:42,360
version at the primary DC this is good

00:12:40,740 --> 00:12:45,840
it provides strong consistency

00:12:42,360 --> 00:12:48,930
however it also cost the request to

00:12:45,840 --> 00:12:51,529
incur high latency because especially

00:12:48,930 --> 00:12:54,319
when the read come from the secondary DC

00:12:51,529 --> 00:12:56,480
since it has a car species Roderick just

00:12:54,319 --> 00:12:59,029
to get the rig to get the transaction

00:12:56,480 --> 00:13:00,920
version so we come up with an

00:12:59,029 --> 00:13:04,730
optimization that we can allow the

00:13:00,920 --> 00:13:08,569
client to optionally hints that the

00:13:04,730 --> 00:13:11,059
request is a tie breed and then in this

00:13:08,569 --> 00:13:13,519
case the client can prefer low latency

00:13:11,059 --> 00:13:16,220
rather than strong consistency so the

00:13:13,519 --> 00:13:20,089
client can annotate the request with

00:13:16,220 --> 00:13:22,639
type read and then the service will put

00:13:20,089 --> 00:13:24,379
this into the bluegrass context and how

00:13:22,639 --> 00:13:26,589
didn't function at the furnish NDB

00:13:24,379 --> 00:13:29,899
storage block in you can get the

00:13:26,589 --> 00:13:31,879
annotation type reads and then is that a

00:13:29,899 --> 00:13:34,430
growing courtesy to get the transaction

00:13:31,879 --> 00:13:37,189
version it gets the transaction version

00:13:34,430 --> 00:13:38,839
locally prefetch it by a different

00:13:37,189 --> 00:13:42,199
background thread that is running the

00:13:38,839 --> 00:13:44,689
bike rack so with this solution we can

00:13:42,199 --> 00:13:47,930
have a low latency and a higher

00:13:44,689 --> 00:13:49,610
throughput especially you know use case

00:13:47,930 --> 00:13:53,749
most of the work load are very good

00:13:49,610 --> 00:13:56,899
heavy so with the example we come up

00:13:53,749 --> 00:13:59,059
with a real grap example and run some

00:13:56,899 --> 00:14:01,910
numbers this is an account linking wrap

00:13:59,059 --> 00:14:03,319
in eBay and we have multiple accounts

00:14:01,910 --> 00:14:05,629
that can be linked together with

00:14:03,319 --> 00:14:07,490
different linking batteries so in this

00:14:05,629 --> 00:14:09,500
example you can see that account one can

00:14:07,490 --> 00:14:11,300
be linked with a cow two three four here

00:14:09,500 --> 00:14:14,689
at the linking start every one or car

00:14:11,300 --> 00:14:17,000
one can be linked to a cow seven via the

00:14:14,689 --> 00:14:19,430
linking structure a tree there are two

00:14:17,000 --> 00:14:21,259
type of what TCC account and linking

00:14:19,430 --> 00:14:25,129
star tree and one type of edge which is

00:14:21,259 --> 00:14:27,829
linking note that after maybe multiple

00:14:25,129 --> 00:14:29,509
hook Tamotsu like a six or seven in this

00:14:27,829 --> 00:14:34,490
case all the accounts are linked

00:14:29,509 --> 00:14:37,250
together so in our deployment as to

00:14:34,490 --> 00:14:39,379
instead we have three DC to region

00:14:37,250 --> 00:14:43,009
deployment and the total number of parts

00:14:39,379 --> 00:14:45,170
are nearly two hundred ninety parts

00:14:43,009 --> 00:14:48,889
since you very large the database

00:14:45,170 --> 00:14:51,410
consists 1.3 billion ash and 1.8 billion

00:14:48,889 --> 00:14:54,740
at one point three billion nodes and 1.8

00:14:51,410 --> 00:14:57,770
billion edges and the trigger mode is

00:14:54,740 --> 00:14:59,090
we put the storage entry permit meaning

00:14:57,770 --> 00:15:02,000
that there are three copies of each

00:14:59,090 --> 00:15:05,600
later at each data center and that

00:15:02,000 --> 00:15:07,480
result in 16.8 tetra buy storage across

00:15:05,600 --> 00:15:10,730
on 3dc

00:15:07,480 --> 00:15:13,910
so there are three type of query that we

00:15:10,730 --> 00:15:16,880
experiment one hop three hub and one hub

00:15:13,910 --> 00:15:18,980
to hop and three hubs the major query in

00:15:16,880 --> 00:15:21,880
our application is three hub for Versa

00:15:18,980 --> 00:15:24,770
note that in this tree hub we limit the

00:15:21,880 --> 00:15:26,750
query to return only 50 reason to

00:15:24,770 --> 00:15:28,580
provide a consistent zero because there

00:15:26,750 --> 00:15:30,650
may be some like super node that has a

00:15:28,580 --> 00:15:33,920
lot of vertices that satisfy the

00:15:30,650 --> 00:15:36,320
condition so we don't the performance

00:15:33,920 --> 00:15:38,150
with from the primary DC and from both

00:15:36,320 --> 00:15:41,090
these two simultaneously so here I only

00:15:38,150 --> 00:15:45,170
focus on the three hard traversal so up

00:15:41,090 --> 00:15:49,160
to 2 DC we can get to 15,000 query per

00:15:45,170 --> 00:15:51,380
second and with that we can get the 95

00:15:49,160 --> 00:15:53,720
latency at less than 50 milliseconds so

00:15:51,380 --> 00:15:56,390
we are quite satisfied with this result

00:15:53,720 --> 00:15:59,120
note that this is we haven't put

00:15:56,390 --> 00:16:02,210
foundation diamond for agenda me to the

00:15:59,120 --> 00:16:04,610
limit in this case because we only have

00:16:02,210 --> 00:16:07,160
a 20 service node and if we increase the

00:16:04,610 --> 00:16:13,580
number of thread this service CPU would

00:16:07,160 --> 00:16:15,710
be exhausted so next we continue to talk

00:16:13,580 --> 00:16:19,580
about some feature that we want to have

00:16:15,710 --> 00:16:21,920
because well so overall the condition is

00:16:19,580 --> 00:16:24,140
great to power our God teepee bacon but

00:16:21,920 --> 00:16:26,180
we like to have some feature improvement

00:16:24,140 --> 00:16:27,650
or some features so the first one is a

00:16:26,180 --> 00:16:31,190
treat that 5 second transition Indian

00:16:27,650 --> 00:16:32,480
right so we maybe could be extended to

00:16:31,190 --> 00:16:34,370
30 second or 1 minute

00:16:32,480 --> 00:16:36,860
well the reason why is because a similar

00:16:34,370 --> 00:16:38,900
query is particular multi hoc query that

00:16:36,860 --> 00:16:40,610
also has a compass a pattern matching is

00:16:38,900 --> 00:16:42,110
spread and also have the children know

00:16:40,610 --> 00:16:44,090
they can spend thousands or children now

00:16:42,110 --> 00:16:47,300
so in that case we're often there we get

00:16:44,090 --> 00:16:49,250
the transition to or me because uh so if

00:16:47,300 --> 00:16:50,930
by having this or longer generation

00:16:49,250 --> 00:16:53,360
limit the energy you can support a much

00:16:50,930 --> 00:16:55,220
more complex query so the second way is

00:16:53,360 --> 00:16:57,110
actually we'd like to have a better

00:16:55,220 --> 00:16:58,610
storage management so this happen

00:16:57,110 --> 00:17:02,060
we found this problem when we do the

00:16:58,610 --> 00:17:04,790
one-week wrong Regency testing by you

00:17:02,060 --> 00:17:06,980
would I we ate we add in the storage nor

00:17:04,790 --> 00:17:07,970
with the Rena storage now and continuing

00:17:06,980 --> 00:17:08,839
doing there

00:17:07,970 --> 00:17:11,480
and we found out the actually the

00:17:08,839 --> 00:17:13,760
storage occupants actually stuff on

00:17:11,480 --> 00:17:15,530
there like 55% foo and then go up to

00:17:13,760 --> 00:17:17,150
like 75 percent rule and some of them

00:17:15,530 --> 00:17:19,459
even go to nineteen ninety percent full

00:17:17,150 --> 00:17:21,770
so this has introduced as storage in

00:17:19,459 --> 00:17:25,309
Paris as well and then so this is a we

00:17:21,770 --> 00:17:26,990
actually put into the forum so also for

00:17:25,309 --> 00:17:29,360
this system that we have the vessel fire

00:17:26,990 --> 00:17:33,049
it's actually OLTP oriented so that's a

00:17:29,360 --> 00:17:36,169
we had to reason then and stand by to

00:17:33,049 --> 00:17:38,659
support this over TP transaction but

00:17:36,169 --> 00:17:40,549
then we like if we want to have a graph

00:17:38,659 --> 00:17:42,530
and Anytus and that's the workload le

00:17:40,549 --> 00:17:44,900
quite the patch retrieval over the

00:17:42,530 --> 00:17:46,610
entire database sometimes then then we

00:17:44,900 --> 00:17:48,559
didn't need to have another third the

00:17:46,610 --> 00:17:51,799
reason just the hosting score and it is

00:17:48,559 --> 00:17:54,620
could walk alone so finally i touch up

00:17:51,799 --> 00:17:57,620
this this pop loading so for the example

00:17:54,620 --> 00:18:00,140
dataset that Humason actually to ask the

00:17:57,620 --> 00:18:02,750
more than two days to load that whole

00:18:00,140 --> 00:18:04,970
data set into the into the our cluster

00:18:02,750 --> 00:18:06,650
so now the question is that is that for

00:18:04,970 --> 00:18:08,330
this initial data loading is that the

00:18:06,650 --> 00:18:13,340
way that we can bypass the normal

00:18:08,330 --> 00:18:14,900
generation path so as a conclusion so we

00:18:13,340 --> 00:18:16,190
had to develop the graph TV service call

00:18:14,900 --> 00:18:18,500
new graph which is a based on the

00:18:16,190 --> 00:18:21,650
fundation DB and Janice and Jenna squat

00:18:18,500 --> 00:18:23,480
so foundation DB offered this cubic

00:18:21,650 --> 00:18:24,799
coach Anderson and this is a feature

00:18:23,480 --> 00:18:26,659
that key to adjust the data

00:18:24,799 --> 00:18:29,360
inconsistency issue that we encounter in

00:18:26,659 --> 00:18:30,980
gravity B and also with this nutrition

00:18:29,360 --> 00:18:33,380
lesson it quickly simplified the

00:18:30,980 --> 00:18:35,120
classification development so with the

00:18:33,380 --> 00:18:37,669
last data set that we have we found out

00:18:35,120 --> 00:18:39,350
we can we saw their foundation DB so the

00:18:37,669 --> 00:18:42,289
high performance in terms of the high

00:18:39,350 --> 00:18:44,860
surfer and low latency so with that we

00:18:42,289 --> 00:18:52,300
conclude our presentation

00:18:44,860 --> 00:18:52,300

YouTube URL: https://www.youtube.com/watch?v=EtB1BPG00PE


