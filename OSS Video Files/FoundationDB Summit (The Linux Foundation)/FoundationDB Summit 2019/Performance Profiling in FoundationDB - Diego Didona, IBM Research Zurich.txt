Title: Performance Profiling in FoundationDB - Diego Didona, IBM Research Zurich
Publication date: 2019-11-25
Playlist: FoundationDB Summit 2019
Description: 
	Performance Profiling in FoundationDB - Diego Didona, IBM Research Zurich

Understanding the performance of a system is paramount to identify which components and functions need to be improved to obtain better performance.  In this talk, we will present how we use the Linux performance analysis tools, e.g., perf, to analyze the performance of FoundationDB. Specifically, we will describe how we use these tools to identify performance bottlenecks and areas for potential performance improvements within FoundationDB. We will use concrete examples taken from the analysis of both the client library and components of the server, e.g., the new Redwood storage engine.
Captions: 
	00:00:00,000 --> 00:00:05,790
let's start I'm yeah buddy Donna from

00:00:02,909 --> 00:00:07,740
IBM research in Surrey I'm going to talk

00:00:05,790 --> 00:00:09,389
to you a little bit about our work on

00:00:07,740 --> 00:00:11,969
performance profiling on foundation DB

00:00:09,389 --> 00:00:15,240
and this is joint work with our

00:00:11,969 --> 00:00:20,550
colleagues on IBM Research in suruc with

00:00:15,240 --> 00:00:22,470
IBM cloud and also with snowflake so

00:00:20,550 --> 00:00:24,269
before starting with the presentation

00:00:22,470 --> 00:00:26,279
with the technical content you might be

00:00:24,269 --> 00:00:29,310
wondering what is the involvement of IBM

00:00:26,279 --> 00:00:31,050
Research with one nation DB or you have

00:00:29,310 --> 00:00:33,180
seen most likely in the first

00:00:31,050 --> 00:00:35,760
presentation of technical presentation

00:00:33,180 --> 00:00:38,219
of the day and I'm talking about IBM

00:00:35,760 --> 00:00:41,430
cloud and how IBM cloud is using

00:00:38,219 --> 00:00:43,980
foundation to be now in production to

00:00:41,430 --> 00:00:45,840
support strongly consistent distributed

00:00:43,980 --> 00:00:49,020
transactions so what we're doing there

00:00:45,840 --> 00:00:51,570
as IBM research we're helping deploying

00:00:49,020 --> 00:00:53,640
foundation to be in again cloud in an

00:00:51,570 --> 00:00:55,710
efficient way and in particular we are

00:00:53,640 --> 00:00:58,050
looking at Android performance issues

00:00:55,710 --> 00:01:00,270
and optimizations that we can do a

00:00:58,050 --> 00:01:04,619
foundation in be especially at the

00:01:00,270 --> 00:01:05,970
storage engine level so in this talk I'm

00:01:04,619 --> 00:01:08,909
going to talk to you about a little bit

00:01:05,970 --> 00:01:11,430
about how we use the Linux curve tool

00:01:08,909 --> 00:01:13,530
for our purposes to analyze improve and

00:01:11,430 --> 00:01:15,509
monitor the performance of foundation EB

00:01:13,530 --> 00:01:17,520
any particular we'll talk to you about

00:01:15,509 --> 00:01:19,640
three use cases the first one being

00:01:17,520 --> 00:01:22,320
bottleneck identification using perf

00:01:19,640 --> 00:01:24,090
second one is overhead analysis and

00:01:22,320 --> 00:01:26,790
performance improvement of the red goo

00:01:24,090 --> 00:01:29,939
storage engine about which we will hear

00:01:26,790 --> 00:01:31,350
more later and the third use case is

00:01:29,939 --> 00:01:33,630
called instrumentation for monitoring

00:01:31,350 --> 00:01:36,329
the performance of initially B and this

00:01:33,630 --> 00:01:40,650
last bullet is kindly contributed by the

00:01:36,329 --> 00:01:43,170
snowflake team just one slide of

00:01:40,650 --> 00:01:45,689
introduction and perf for whoever is not

00:01:43,170 --> 00:01:47,460
familiar with it perf is the performance

00:01:45,689 --> 00:01:49,229
on housing - it is part of the Linux

00:01:47,460 --> 00:01:50,399
kernel it's very powerful it's very

00:01:49,229 --> 00:01:53,759
versatile you can do a lot of things

00:01:50,399 --> 00:01:56,040
among which tracing performance counters

00:01:53,759 --> 00:01:57,930
setting and monitoring trace points

00:01:56,040 --> 00:01:59,729
doing profiling but in this presentation

00:01:57,930 --> 00:02:02,490
I'm just going to cover three basic

00:01:59,729 --> 00:02:04,829
features of perf this that our system

00:02:02,490 --> 00:02:06,950
code analysis CPU profiling on a per

00:02:04,829 --> 00:02:09,110
function basis and ready

00:02:06,950 --> 00:02:10,849
and monetary interest points be sure to

00:02:09,110 --> 00:02:12,319
study another use cases of perf there

00:02:10,849 --> 00:02:14,599
are plenty of resources online that you

00:02:12,319 --> 00:02:18,200
can check out it's really there is a

00:02:14,599 --> 00:02:20,900
word about it outside so let's get

00:02:18,200 --> 00:02:24,140
started with the first use case for

00:02:20,900 --> 00:02:25,640
today bottleneck identification so the

00:02:24,140 --> 00:02:27,200
first thing we did when we started our

00:02:25,640 --> 00:02:29,209
project we financially B of course we

00:02:27,200 --> 00:02:30,800
wanted to get a sense of the perform as

00:02:29,209 --> 00:02:32,600
a foundation we can achieve so we ran a

00:02:30,800 --> 00:02:34,819
very simple benchmark we deployed a

00:02:32,600 --> 00:02:36,769
server with the memory engineer the

00:02:34,819 --> 00:02:38,900
simplest one and we loaded it with some

00:02:36,769 --> 00:02:41,750
very simple uniform key value pairs and

00:02:38,900 --> 00:02:44,569
then we also run a another machine a

00:02:41,750 --> 00:02:46,400
client that injects the most simple

00:02:44,569 --> 00:02:47,870
transactions we can think of so read all

00:02:46,400 --> 00:02:50,690
the transactions from a uniform workload

00:02:47,870 --> 00:02:53,269
and we deploy several threads within

00:02:50,690 --> 00:02:55,450
this client process we want to see as we

00:02:53,269 --> 00:02:57,319
vary the number of threads how

00:02:55,450 --> 00:03:00,769
correspondingly bear is the performance

00:02:57,319 --> 00:03:03,650
on deliver body servers and the result

00:03:00,769 --> 00:03:05,180
we got is the plot in this slide on the

00:03:03,650 --> 00:03:08,420
x-axis we have the number of threads

00:03:05,180 --> 00:03:11,120
that we deploy within the client process

00:03:08,420 --> 00:03:12,640
on the y-axis we get the death report of

00:03:11,120 --> 00:03:15,170
the systems in terms of performance

00:03:12,640 --> 00:03:16,850
operations per second and what we can

00:03:15,170 --> 00:03:18,440
see is then after an initial scalability

00:03:16,850 --> 00:03:21,230
we a below it here

00:03:18,440 --> 00:03:23,090
the performance stop stopped growing and

00:03:21,230 --> 00:03:25,790
we want to understand what's going on at

00:03:23,090 --> 00:03:27,890
this point so to do this we looked at a

00:03:25,790 --> 00:03:30,680
server and a client and what we saw

00:03:27,890 --> 00:03:33,500
first is that on the client despite

00:03:30,680 --> 00:03:35,780
adding 320 threads active and then if

00:03:33,500 --> 00:03:37,940
you can see only one core is basically

00:03:35,780 --> 00:03:40,820
active at the time the other ones are

00:03:37,940 --> 00:03:44,750
mostly idle so to find out what's going

00:03:40,820 --> 00:03:47,870
on we run perfect basically we use the

00:03:44,750 --> 00:03:49,850
perf trace - s command on the binary of

00:03:47,870 --> 00:03:52,040
all the clients and this command gives

00:03:49,850 --> 00:03:55,130
us a trace of the system calls that are

00:03:52,040 --> 00:03:57,049
called by a binary on a per thread basis

00:03:55,130 --> 00:04:00,230
so what we get is for each thread of

00:03:57,049 --> 00:04:02,989
this binary we get first of all the load

00:04:00,230 --> 00:04:04,730
of the thread it's up here and then we

00:04:02,989 --> 00:04:06,980
get the system calls with are called by

00:04:04,730 --> 00:04:09,650
by by the thread with some useful

00:04:06,980 --> 00:04:11,870
statistics like aggregated CPU time and

00:04:09,650 --> 00:04:12,410
average CPU time per system called

00:04:11,870 --> 00:04:15,290
invocation

00:04:12,410 --> 00:04:17,420
so when we run this on the client binary

00:04:15,290 --> 00:04:19,010
what we get is we get three hand

00:04:17,420 --> 00:04:21,590
20 very similar traces each

00:04:19,010 --> 00:04:23,360
corresponding to the 320 clients of the

00:04:21,590 --> 00:04:24,800
application that we spawned and then we

00:04:23,360 --> 00:04:26,870
get one different trace its

00:04:24,800 --> 00:04:28,730
corresponding to their library thread it

00:04:26,870 --> 00:04:31,540
is pawned by the foundation in bill

00:04:28,730 --> 00:04:33,980
library transparently to the user

00:04:31,540 --> 00:04:36,140
looking at these traces what we found

00:04:33,980 --> 00:04:38,740
out two interesting things the first one

00:04:36,140 --> 00:04:42,290
being that this client library fred is

00:04:38,740 --> 00:04:45,500
taking up 50% almost of the cpu and this

00:04:42,290 --> 00:04:47,990
is visible so this corresponds to mostly

00:04:45,500 --> 00:04:50,810
the most of the load that we see on the

00:04:47,990 --> 00:04:52,910
on the top of this line and the second

00:04:50,810 --> 00:04:54,260
thing we see is that the CPU the system

00:04:52,910 --> 00:04:55,790
called the consumes the most CPU is the

00:04:54,260 --> 00:04:57,710
food tax system called there is a

00:04:55,790 --> 00:05:00,410
synchronization system call and this

00:04:57,710 --> 00:05:02,810
shows up both in the quiet threads as

00:05:00,410 --> 00:05:05,330
well as in the library frame so this

00:05:02,810 --> 00:05:07,250
means that most of the CPU time is spent

00:05:05,330 --> 00:05:10,630
in synchronization between the library

00:05:07,250 --> 00:05:12,650
thread and the application threads and

00:05:10,630 --> 00:05:14,480
overall this means that the bottleneck

00:05:12,650 --> 00:05:16,130
that we are hitting here is this single

00:05:14,480 --> 00:05:19,730
networking thread it is doing too much

00:05:16,130 --> 00:05:21,800
work to handle the requests of the user

00:05:19,730 --> 00:05:23,870
level threads and it's awakening them to

00:05:21,800 --> 00:05:26,450
process and send them the replies of the

00:05:23,870 --> 00:05:28,130
whole definition to be so the solution

00:05:26,450 --> 00:05:30,500
to build around this bottleneck is

00:05:28,130 --> 00:05:32,780
instead of spawning a single client

00:05:30,500 --> 00:05:36,410
process we can spawn multiple client

00:05:32,780 --> 00:05:39,920
processes each with its own library

00:05:36,410 --> 00:05:42,440
friend and we did that and indeed we we

00:05:39,920 --> 00:05:45,020
saw that performance increased a lot we

00:05:42,440 --> 00:05:48,680
have here an example of this again this

00:05:45,020 --> 00:05:50,510
is the same test that we run earlier the

00:05:48,680 --> 00:05:52,880
in in red we have the same configuration

00:05:50,510 --> 00:05:56,150
as before we have all the threads in one

00:05:52,880 --> 00:05:58,580
single process and in blue we have a the

00:05:56,150 --> 00:06:00,460
same test in which we deploy the same

00:05:58,580 --> 00:06:02,780
number of threads but across 20

00:06:00,460 --> 00:06:04,010
foundationally be client processes and

00:06:02,780 --> 00:06:05,360
we see that performance are much much

00:06:04,010 --> 00:06:08,000
better and it scalability is also much

00:06:05,360 --> 00:06:09,680
much better now whoever you is familiar

00:06:08,000 --> 00:06:12,110
with the with the with the forum you

00:06:09,680 --> 00:06:13,880
know that these very same issues issue

00:06:12,110 --> 00:06:16,280
has been reported a couple of times at

00:06:13,880 --> 00:06:18,560
least and discuss the capitalist and to

00:06:16,280 --> 00:06:20,900
answer this somebody is very versed in

00:06:18,560 --> 00:06:22,070
funda she has to tell the user at least

00:06:20,900 --> 00:06:24,350
and yes there is this problem with the

00:06:22,070 --> 00:06:26,840
formation will be client thread and this

00:06:24,350 --> 00:06:29,030
is because the guy who replies is very

00:06:26,840 --> 00:06:30,320
expert information to be we were at the

00:06:29,030 --> 00:06:31,970
beginning of our journey internationally

00:06:30,320 --> 00:06:34,100
we didn't know about this specific issue

00:06:31,970 --> 00:06:35,630
and this demonstrates how perf can give

00:06:34,100 --> 00:06:37,250
you very good insight about the

00:06:35,630 --> 00:06:39,560
performance and the issues of the code

00:06:37,250 --> 00:06:41,389
even without being an expert in the code

00:06:39,560 --> 00:06:46,220
and heavy without having really access

00:06:41,389 --> 00:06:49,430
to the little details of that code on to

00:06:46,220 --> 00:06:50,690
the second use case overhead analysis

00:06:49,430 --> 00:06:54,050
and performance improvement in the

00:06:50,690 --> 00:06:55,550
Redwood as you should know redwood is a

00:06:54,050 --> 00:06:57,530
next-generation storage engine our

00:06:55,550 --> 00:06:59,660
foundation EB is going to be it's

00:06:57,530 --> 00:07:00,680
already pre release I guess and we are

00:06:59,660 --> 00:07:03,050
working with the Apple team in

00:07:00,680 --> 00:07:05,090
particular with Steven Evan to improve

00:07:03,050 --> 00:07:08,930
this performance so what we are doing is

00:07:05,090 --> 00:07:11,479
we profile the benchmark using redwood

00:07:08,930 --> 00:07:13,580
with Perth we identify the hotspots

00:07:11,479 --> 00:07:16,130
meaning the CPU the functions that

00:07:13,580 --> 00:07:18,080
consume the most CPU we try to improve

00:07:16,130 --> 00:07:22,729
those functions and then we repeat the

00:07:18,080 --> 00:07:26,050
process so what we do is we use the perf

00:07:22,729 --> 00:07:30,020
record command this command basically

00:07:26,050 --> 00:07:31,580
profiles the binary and sees the CPU

00:07:30,020 --> 00:07:34,370
overhead corresponding to each function

00:07:31,580 --> 00:07:36,410
and then with perf report we have a very

00:07:34,370 --> 00:07:38,419
nice view of these functions and the

00:07:36,410 --> 00:07:40,610
corresponding overhead so we run these

00:07:38,419 --> 00:07:43,430
on the on a bench where it uses redwood

00:07:40,610 --> 00:07:45,260
and on the rightmost color you can see

00:07:43,430 --> 00:07:47,720
the name on the functions and on the

00:07:45,260 --> 00:07:50,090
leftmost column you can see the CPU

00:07:47,720 --> 00:07:51,710
overhead for that function in orden so

00:07:50,090 --> 00:07:54,380
the top function is the one that

00:07:51,710 --> 00:07:55,789
consumes the most view in this procedure

00:07:54,380 --> 00:07:57,770
in this case what we see is that there

00:07:55,789 --> 00:08:00,500
are two main sources of overhead CPU

00:07:57,770 --> 00:08:03,560
overhead memory comparison the first two

00:08:00,500 --> 00:08:04,910
entries and memory management memory

00:08:03,560 --> 00:08:07,789
allocation and the allocation on your

00:08:04,910 --> 00:08:09,020
mal working with free so we to improve

00:08:07,789 --> 00:08:11,090
the performance or redwood we want to

00:08:09,020 --> 00:08:12,470
address these two overheads starting

00:08:11,090 --> 00:08:14,510
with the memory comparison one because

00:08:12,470 --> 00:08:17,900
it's the one that has the most impact on

00:08:14,510 --> 00:08:19,099
CPU of course to address the overhead we

00:08:17,900 --> 00:08:21,770
first have to understand where the

00:08:19,099 --> 00:08:23,450
halberds comes from and what we find out

00:08:21,770 --> 00:08:24,320
is that Redwood uses a temporary

00:08:23,450 --> 00:08:25,760
in-memory

00:08:24,320 --> 00:08:27,919
shorter buffer and is called the

00:08:25,760 --> 00:08:30,410
mutation buffer and this buffer the

00:08:27,919 --> 00:08:32,839
update the incoming updates are sorted

00:08:30,410 --> 00:08:35,659
and kept there and then they are flushed

00:08:32,839 --> 00:08:36,800
in batch all together at the end upon

00:08:35,659 --> 00:08:38,959
committing

00:08:36,800 --> 00:08:40,819
this mutation buffer is implemented as a

00:08:38,959 --> 00:08:43,129
standard library map then oyster is

00:08:40,819 --> 00:08:44,540
implemented as a regular tree now red

00:08:43,129 --> 00:08:48,259
black tree is basically a balanced

00:08:44,540 --> 00:08:49,910
binary tree which each node is a key so

00:08:48,259 --> 00:08:51,230
whenever you want to find the key or you

00:08:49,910 --> 00:08:55,129
want to insert a key here to traverse

00:08:51,230 --> 00:08:56,689
the tree top now and at each level that

00:08:55,129 --> 00:08:58,339
you compared to determine where is the

00:08:56,689 --> 00:09:01,040
level at which we have to stop you have

00:08:58,339 --> 00:09:02,839
to compare the target key and the key in

00:09:01,040 --> 00:09:04,339
the node so this means that each

00:09:02,839 --> 00:09:06,019
comparison we're comparing the whole

00:09:04,339 --> 00:09:08,209
target key with the whole key in the

00:09:06,019 --> 00:09:12,249
node and if the two keys have besides of

00:09:08,209 --> 00:09:14,749
roughly B bits this is an O bit or B

00:09:12,249 --> 00:09:16,639
operation and the number of comparison

00:09:14,749 --> 00:09:19,160
that you had to do is asymptotically no

00:09:16,639 --> 00:09:20,749
terrific in the number of elements as

00:09:19,160 --> 00:09:23,420
you have in the tree so the total cost

00:09:20,749 --> 00:09:28,759
of finding and inserting a key is o OB

00:09:23,420 --> 00:09:30,619
log n the red black tree is it's not

00:09:28,759 --> 00:09:32,329
aware of the fact that actually in many

00:09:30,619 --> 00:09:35,029
workloads that are interesting for from

00:09:32,329 --> 00:09:38,299
enemy keys shared prefixes for in any

00:09:35,029 --> 00:09:41,559
case a common part of the key and we

00:09:38,299 --> 00:09:43,939
want to leverage this particularity of

00:09:41,559 --> 00:09:46,429
on the workload of the keys

00:09:43,939 --> 00:09:51,110
so instead of looking at red bird tree

00:09:46,429 --> 00:09:53,629
we propose to use a tri tri is a tree

00:09:51,110 --> 00:09:55,790
like structure but where each node of

00:09:53,629 --> 00:09:57,350
the tree instead of storing the whole

00:09:55,790 --> 00:09:59,629
key just towards a portion of the key

00:09:57,350 --> 00:10:01,970
that corresponds to the prefix of the

00:09:59,629 --> 00:10:04,189
keys that are in the subtree so whenever

00:10:01,970 --> 00:10:04,970
we hey we have to find a key or where to

00:10:04,189 --> 00:10:06,740
insert a key

00:10:04,970 --> 00:10:09,499
we had to descend again the tree from

00:10:06,740 --> 00:10:11,779
top to bottom front or to top-down but

00:10:09,499 --> 00:10:14,179
at each level instead of comparing the

00:10:11,779 --> 00:10:16,129
whole key we just compare the target key

00:10:14,179 --> 00:10:20,589
with a portion of the turkey with the

00:10:16,129 --> 00:10:24,069
corresponding portion in the in the node

00:10:20,589 --> 00:10:27,499
so the total cost for inserting or

00:10:24,069 --> 00:10:31,220
finding a key just OB it's a logarithmic

00:10:27,499 --> 00:10:32,899
improvement and in particular we we

00:10:31,220 --> 00:10:35,839
propose to use the adaptive verdict tree

00:10:32,899 --> 00:10:37,369
in short art is a state-of-the-art try

00:10:35,839 --> 00:10:39,529
like the infrastructure it is very

00:10:37,369 --> 00:10:40,790
compact is very cash friendly and also

00:10:39,529 --> 00:10:42,499
implements a couple of tricks that

00:10:40,790 --> 00:10:45,290
perform practice compression that

00:10:42,499 --> 00:10:47,360
further reduce the number of bytes that

00:10:45,290 --> 00:10:48,620
are compared when you are inserting or

00:10:47,360 --> 00:10:54,050
you are

00:10:48,620 --> 00:10:55,520
kinga 40 so what we did we created a

00:10:54,050 --> 00:10:58,040
version of limitation buffer that

00:10:55,520 --> 00:11:00,560
instead of using the map uses this arc

00:10:58,040 --> 00:11:02,510
data structure and we compared to

00:11:00,560 --> 00:11:04,670
various using the micro benchmark on the

00:11:02,510 --> 00:11:07,550
storage engine this micro benchmark is

00:11:04,670 --> 00:11:09,680
very simple it ingests two gigabyte

00:11:07,550 --> 00:11:11,960
worth of key value pairs in 100

00:11:09,680 --> 00:11:14,540
iterations and the workload is very

00:11:11,960 --> 00:11:16,610
simple is random keys 500 bytes values

00:11:14,540 --> 00:11:19,490
and we experiment both with monkeys and

00:11:16,610 --> 00:11:22,730
with large keys and the results are in

00:11:19,490 --> 00:11:25,550
this slide where we report the speed-up

00:11:22,730 --> 00:11:27,740
over redwood in terms of comment rate to

00:11:25,550 --> 00:11:29,360
disk on the Left we have the results for

00:11:27,740 --> 00:11:32,120
smoke's on the right we have the results

00:11:29,360 --> 00:11:34,070
for for the large keys so in blue we

00:11:32,120 --> 00:11:35,450
have the baseline it is one meaning of

00:11:34,070 --> 00:11:37,820
course there is no speed-up and respect

00:11:35,450 --> 00:11:39,170
to itself and in orange we have the

00:11:37,820 --> 00:11:41,660
speed-up that we have with our a

00:11:39,170 --> 00:11:43,520
modified version and the speed-up ranges

00:11:41,660 --> 00:11:48,560
between 11 and 15 percent for our news

00:11:43,520 --> 00:11:50,810
cases now changing from the map to art

00:11:48,560 --> 00:11:52,640
improves the memory comparison overhead

00:11:50,810 --> 00:11:54,080
but it doesn't get rid of the code the

00:11:52,640 --> 00:11:57,380
other overhead the memory allocation

00:11:54,080 --> 00:11:58,670
overhead this is because also in art we

00:11:57,380 --> 00:12:00,740
have to allocate and de-allocate the

00:11:58,670 --> 00:12:02,630
internal node of the tree so we profile

00:12:00,740 --> 00:12:04,730
again the system and we see indeed that

00:12:02,630 --> 00:12:07,250
the problem of the memory still persists

00:12:04,730 --> 00:12:10,280
so what we want to do here is instead of

00:12:07,250 --> 00:12:12,890
using malloc and free as they are now we

00:12:10,280 --> 00:12:14,360
want to use a slob a locator as it's

00:12:12,890 --> 00:12:16,940
none information be elsewhere

00:12:14,360 --> 00:12:18,800
using the arena of course instead of

00:12:16,940 --> 00:12:20,510
implementing everything and then see

00:12:18,800 --> 00:12:21,770
what is the gain that we have by

00:12:20,510 --> 00:12:23,450
employment in this lab application we

00:12:21,770 --> 00:12:25,640
first want to have an idea of what is

00:12:23,450 --> 00:12:27,530
the improvement that we can get so

00:12:25,640 --> 00:12:31,550
instead of implementing everything with

00:12:27,530 --> 00:12:33,620
the arena we just linked our binaries to

00:12:31,550 --> 00:12:35,930
TC malloc it is a swamp allocator for

00:12:33,620 --> 00:12:37,730
memory instead of malloc and then we

00:12:35,930 --> 00:12:38,990
measure the performance that we get so

00:12:37,730 --> 00:12:40,160
this would be an upper bound on a

00:12:38,990 --> 00:12:42,740
performance that we would get if we

00:12:40,160 --> 00:12:45,950
implement our own alligator for art with

00:12:42,740 --> 00:12:48,110
with the arena foundation DB and this is

00:12:45,950 --> 00:12:49,550
the results that these are tasks that we

00:12:48,110 --> 00:12:52,640
get there is an additional

00:12:49,550 --> 00:12:54,980
colony here for both small keys on the

00:12:52,640 --> 00:12:57,410
left and large keys on the right this is

00:12:54,980 --> 00:12:59,180
this - column that represents the

00:12:57,410 --> 00:13:00,830
speed-up over the best line that we have

00:12:59,180 --> 00:13:04,250
when we use both our

00:13:00,830 --> 00:13:06,140
art did the structure and this lab

00:13:04,250 --> 00:13:08,029
allocation and we can see that the

00:13:06,140 --> 00:13:11,420
improvement with respect to the base

00:13:08,029 --> 00:13:13,370
line ranges from 22 to 28 percent so by

00:13:11,420 --> 00:13:15,170
using perf as an indicator of where the

00:13:13,370 --> 00:13:17,690
over Andes we could improve the

00:13:15,170 --> 00:13:19,910
performance of this specific part of a

00:13:17,690 --> 00:13:24,860
foundation be by up to 20 percent over

00:13:19,910 --> 00:13:25,790
the baseline the last year's case I want

00:13:24,860 --> 00:13:26,990
to talk to you about this code

00:13:25,790 --> 00:13:30,529
instrumentation for monitoring

00:13:26,990 --> 00:13:34,040
performance so what the snowflake team

00:13:30,529 --> 00:13:37,399
did is it added support for us DT probes

00:13:34,040 --> 00:13:39,380
in in Perth USD probes are user level

00:13:37,399 --> 00:13:41,709
trace points that provide a hook to call

00:13:39,380 --> 00:13:44,930
arbitrary function within the code flow

00:13:41,709 --> 00:13:48,110
so what it's new now information in B

00:13:44,930 --> 00:13:50,540
there is this new macro FDB trace probe

00:13:48,110 --> 00:13:52,490
it takes the first parameter the ID of

00:13:50,540 --> 00:13:54,529
the probability that you are defining

00:13:52,490 --> 00:13:56,630
and the arguments for this hook function

00:13:54,529 --> 00:13:59,240
that you want to to use to call at a

00:13:56,630 --> 00:14:01,880
specific point in the code with perf you

00:13:59,240 --> 00:14:03,260
can enable and disable this this probes

00:14:01,880 --> 00:14:04,880
at runtime so whenever we are not

00:14:03,260 --> 00:14:06,620
interested for example in production to

00:14:04,880 --> 00:14:09,050
something you just disable this traces

00:14:06,620 --> 00:14:10,520
these probes and the corresponding

00:14:09,050 --> 00:14:11,660
tracing cost is very very low when

00:14:10,520 --> 00:14:13,010
you're interested in something you see

00:14:11,660 --> 00:14:14,630
there is a hiccup in performance you

00:14:13,010 --> 00:14:15,709
want to see what's going on you enable

00:14:14,630 --> 00:14:17,770
them and then you pay a little bit in

00:14:15,709 --> 00:14:20,959
performance but you get an idea of

00:14:17,770 --> 00:14:23,660
what's going on in your code so you can

00:14:20,959 --> 00:14:24,770
get live measurements of performance

00:14:23,660 --> 00:14:27,140
measures that are defined within these

00:14:24,770 --> 00:14:30,310
probes so let's see with a couple of use

00:14:27,140 --> 00:14:32,930
cases how this work first use case is

00:14:30,310 --> 00:14:37,640
monitoring the invocations of the

00:14:32,930 --> 00:14:40,220
profile actor enter and exit so to do

00:14:37,640 --> 00:14:42,260
this there are now two new invocations

00:14:40,220 --> 00:14:45,560
of this FDB trace probe the first one

00:14:42,260 --> 00:14:47,420
being is one actor with the name actor

00:14:45,560 --> 00:14:49,730
enter and takes as parameter D actor

00:14:47,420 --> 00:14:51,320
name and the second one is the actor

00:14:49,730 --> 00:14:54,320
axis that takes as input the actor name

00:14:51,320 --> 00:14:55,490
as well so they have these are added

00:14:54,320 --> 00:14:57,620
automatically to the code it's

00:14:55,490 --> 00:14:59,690
transparent to you because they are

00:14:57,620 --> 00:15:01,970
integrated competitor in the actor

00:14:59,690 --> 00:15:03,709
compiler code so to use these you have

00:15:01,970 --> 00:15:05,510
to activate them on the phone they

00:15:03,709 --> 00:15:07,699
should be binary using this perf command

00:15:05,510 --> 00:15:10,970
and then you just run your foundation be

00:15:07,699 --> 00:15:12,949
the server normally then whenever you're

00:15:10,970 --> 00:15:14,310
interested in looking at these perks you

00:15:12,949 --> 00:15:15,870
use again

00:15:14,310 --> 00:15:18,899
periphery core specifying with the minus

00:15:15,870 --> 00:15:20,370
e it's like what are the actors that

00:15:18,899 --> 00:15:22,709
you're interested in and then you run

00:15:20,370 --> 00:15:25,439
perfect ripped a perf script gives you

00:15:22,709 --> 00:15:28,139
for each of these probes the invocation

00:15:25,439 --> 00:15:31,050
time on the leftmost column what is the

00:15:28,139 --> 00:15:33,870
probe that is is being a triggered and

00:15:31,050 --> 00:15:36,180
the parameters as an act ready so

00:15:33,870 --> 00:15:37,709
looking at this wall of text you can

00:15:36,180 --> 00:15:39,480
post process it and what you can get for

00:15:37,709 --> 00:15:41,339
example is interesting or interesting

00:15:39,480 --> 00:15:43,529
veterans such as you can compute the

00:15:41,339 --> 00:15:45,240
actor execution time between and enter

00:15:43,529 --> 00:15:49,620
and exit just by looking at the

00:15:45,240 --> 00:15:52,649
differential within the time stamps the

00:15:49,620 --> 00:15:54,209
second use case and has been added is

00:15:52,649 --> 00:15:56,610
the monitoring on the rank you in

00:15:54,209 --> 00:16:00,230
foundation EB so foundation EB is mostly

00:15:56,610 --> 00:16:03,899
single process and there is a a queue of

00:16:00,230 --> 00:16:05,069
tasks of event where events are placed

00:16:03,899 --> 00:16:07,529
and there is a single thread that goes

00:16:05,069 --> 00:16:10,170
through this this list processes the

00:16:07,529 --> 00:16:12,300
tasks and and then it goes over and over

00:16:10,170 --> 00:16:14,519
it in a loop so maybe what we are

00:16:12,300 --> 00:16:16,889
interested in to understand the load on

00:16:14,519 --> 00:16:19,199
the specific condition observer is how

00:16:16,889 --> 00:16:21,360
many of these tasks are in this queue

00:16:19,199 --> 00:16:24,089
and how much time it takes to go through

00:16:21,360 --> 00:16:25,910
all these tasks at once so to do this

00:16:24,089 --> 00:16:28,500
was not vague added a couple of probes

00:16:25,910 --> 00:16:30,779
before the invocation before starting on

00:16:28,500 --> 00:16:33,240
this loop with this run loop tasks start

00:16:30,779 --> 00:16:36,809
the probe that takes input parameter the

00:16:33,240 --> 00:16:38,720
current queue size and another one

00:16:36,809 --> 00:16:40,980
another probe at the end of the loop

00:16:38,720 --> 00:16:43,860
these foundationally be trace program

00:16:40,980 --> 00:16:45,779
loop done and the current queue size so

00:16:43,860 --> 00:16:47,910
again you can activate these two probes

00:16:45,779 --> 00:16:49,610
on the binary and then at runtime you

00:16:47,910 --> 00:16:51,990
can probe them to add information about

00:16:49,610 --> 00:16:53,579
these two statistics so the number of

00:16:51,990 --> 00:16:57,050
elements in ranked you and how much time

00:16:53,579 --> 00:17:01,259
it takes to go through the ranking once

00:16:57,050 --> 00:17:03,000
a nice way to visualize this has been

00:17:01,259 --> 00:17:05,669
also contributed by the snowflake team

00:17:03,000 --> 00:17:08,280
with a script that uses BCC that is a

00:17:05,669 --> 00:17:09,929
framework to build profiling programs I

00:17:08,280 --> 00:17:12,449
will not go into the details of this but

00:17:09,929 --> 00:17:14,460
the code I think will be made available

00:17:12,449 --> 00:17:17,700
online for anyone to see so

00:17:14,460 --> 00:17:19,830
this script does it consumes the results

00:17:17,700 --> 00:17:21,570
the traces generated by these probes at

00:17:19,830 --> 00:17:23,910
the trunk time generates the histogram

00:17:21,570 --> 00:17:26,250
to see the distribution both of the

00:17:23,910 --> 00:17:28,800
sides of the ranked hue on the left and

00:17:26,250 --> 00:17:30,810
the time it takes to go through the rank

00:17:28,800 --> 00:17:34,220
here once so every second this gets

00:17:30,810 --> 00:17:36,330
refreshed so you can have very neatly

00:17:34,220 --> 00:17:40,140
visualization of what's going on in your

00:17:36,330 --> 00:17:41,700
foundation will be deployment I think

00:17:40,140 --> 00:17:45,360
this is the last technical slide I have

00:17:41,700 --> 00:17:47,340
so I just describe three use cases for

00:17:45,360 --> 00:17:49,680
the perf tool that we use to analyze

00:17:47,340 --> 00:17:51,600
improve and monitor the performance of

00:17:49,680 --> 00:17:53,490
foundationally B and other than using

00:17:51,600 --> 00:17:55,380
these for our own purposes we are trying

00:17:53,490 --> 00:17:57,330
to build a set of tools that we can then

00:17:55,380 --> 00:18:01,410
release to the public for you for you

00:17:57,330 --> 00:18:04,380
all to use okay I understand I have to

00:18:01,410 --> 00:18:05,790
be there and thank you if you have any

00:18:04,380 --> 00:18:08,250
questions I'd be happy to take them now

00:18:05,790 --> 00:18:15,150
or of wine

00:18:08,250 --> 00:18:15,150

YouTube URL: https://www.youtube.com/watch?v=4C3CpnKE7Ko


