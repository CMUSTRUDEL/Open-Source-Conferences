Title: Redwood Storage Engine Update - Steve Atherton, Apple
Publication date: 2020-05-07
Playlist: FoundationDB Summit 2019
Description: 
	Redwood Storage Engine Update - Steve Atherton, Apple

Redwood is FoundationDBâ€™s new B-Tree based storage engine which is aimed at offering higher I/O throughput and smaller on-disk footprints. This talk will give an overview and update on the project, covering current features and caveats to consider, performance characteristics for different workloads, and planned future optimizations.
Captions: 
	00:00:00,659 --> 00:00:05,700
okay so I'm Steve after 10 and I'm here

00:00:04,200 --> 00:00:07,470
to give an update on the Redwood storage

00:00:05,700 --> 00:00:10,920
engine which was it which was introduced

00:00:07,470 --> 00:00:12,389
at last year's summit so there's these

00:00:10,920 --> 00:00:15,480
are the topics I'll be covering first an

00:00:12,389 --> 00:00:18,150
overview then status on what's done in

00:00:15,480 --> 00:00:20,400
not done some performance numbers and

00:00:18,150 --> 00:00:22,080
then I'll talk about an important design

00:00:20,400 --> 00:00:24,199
update that was made in the past year

00:00:22,080 --> 00:00:27,660
and then I'm gonna delve into detail on

00:00:24,199 --> 00:00:29,849
IO patterns for some operations that the

00:00:27,660 --> 00:00:31,949
storage engine has to do and then lastly

00:00:29,849 --> 00:00:34,730
I'll talk about future improvements in

00:00:31,949 --> 00:00:37,890
those that are coming in the short term

00:00:34,730 --> 00:00:40,379
so a quick overview so redwood is a

00:00:37,890 --> 00:00:43,710
beach it's a B+ tree it's multi version

00:00:40,379 --> 00:00:45,329
so it can contain different values for

00:00:43,710 --> 00:00:47,820
the same keys at different versions and

00:00:45,329 --> 00:00:49,920
it employs prefix compression both

00:00:47,820 --> 00:00:54,210
across nodes and also within nodes

00:00:49,920 --> 00:00:54,780
themselves in it the B tree lives on a

00:00:54,210 --> 00:00:57,300
pager

00:00:54,780 --> 00:00:59,820
which is a stateful component it is also

00:00:57,300 --> 00:01:02,010
multi version the pager provides read

00:00:59,820 --> 00:01:06,810
write and commit access to fixed size

00:01:02,010 --> 00:01:10,200
pages by version sorry

00:01:06,810 --> 00:01:12,360
last month I thought I won't okay so a

00:01:10,200 --> 00:01:15,360
little bit on the on status so what's

00:01:12,360 --> 00:01:18,540
done so first and foremost it works and

00:01:15,360 --> 00:01:21,740
it's actually been shipping so to speak

00:01:18,540 --> 00:01:24,119
in recent patch releases of six two

00:01:21,740 --> 00:01:26,700
passes correctness testing most of the

00:01:24,119 --> 00:01:29,070
necessary functionality is done notably

00:01:26,700 --> 00:01:31,229
prefix compression is done and is in

00:01:29,070 --> 00:01:33,180
final form for the near future the

00:01:31,229 --> 00:01:35,369
foreseeable future and deferred range

00:01:33,180 --> 00:01:38,189
clears are done and also are in final

00:01:35,369 --> 00:01:43,290
form for the foreseeable future pretty

00:01:38,189 --> 00:01:46,140
much all the i/o patterns are where they

00:01:43,290 --> 00:01:47,909
need to be at this point so that all

00:01:46,140 --> 00:01:51,000
sounds great so what's not done well

00:01:47,909 --> 00:01:53,040
performance mainly because there are a

00:01:51,000 --> 00:01:56,790
lot of CPU optimizations that haven't

00:01:53,040 --> 00:01:58,530
been done yet and actually earlier Diego

00:01:56,790 --> 00:02:00,390
talked about one of them where the

00:01:58,530 --> 00:02:04,829
mutation buffer definitely has some some

00:02:00,390 --> 00:02:06,600
performance issues so also notably

00:02:04,829 --> 00:02:08,610
b-tree node merging is not yet

00:02:06,600 --> 00:02:10,619
implemented you need node merging in a

00:02:08,610 --> 00:02:12,750
b-tree to reclaim accumulated slack

00:02:10,619 --> 00:02:15,890
space that you'll get

00:02:12,750 --> 00:02:18,480
within nodes overtime by clearing keys

00:02:15,890 --> 00:02:20,400
if you clear the entire page or set of

00:02:18,480 --> 00:02:22,080
pages that's great but if because those

00:02:20,400 --> 00:02:23,610
pages will be free but if you only clear

00:02:22,080 --> 00:02:26,130
part of a page you need to eventually

00:02:23,610 --> 00:02:27,900
reclaim that space also the on disk

00:02:26,130 --> 00:02:30,990
format may change and no backward

00:02:27,900 --> 00:02:35,010
compatibility on the disk on the file

00:02:30,990 --> 00:02:36,870
format is being provided yet so next I'd

00:02:35,010 --> 00:02:42,600
like to talk about performance

00:02:36,870 --> 00:02:45,330
so specifically prove a performance with

00:02:42,600 --> 00:02:47,550
compressible prefixes this avoids most

00:02:45,330 --> 00:02:52,350
of the unoptimized parts that still

00:02:47,550 --> 00:02:55,620
exist in Redwood so I have a data set

00:02:52,350 --> 00:02:58,110
that is 40 million records each record

00:02:55,620 --> 00:03:00,930
exists under one of 100,000 unique

00:02:58,110 --> 00:03:03,900
prefixes and each key has a random four

00:03:00,930 --> 00:03:06,810
byte suffix the values are 32 bytes of

00:03:03,900 --> 00:03:12,210
random data the inserts are mostly

00:03:06,810 --> 00:03:15,270
localized under one of the prefixes what

00:03:12,210 --> 00:03:18,240
I mean by that is groups of inserts are

00:03:15,270 --> 00:03:19,170
done together under one of the unique

00:03:18,240 --> 00:03:22,709
one of the hundred thousand unique

00:03:19,170 --> 00:03:24,840
prefixes so here's a big chart and I'm

00:03:22,709 --> 00:03:28,890
gonna focus on just a column at a time

00:03:24,840 --> 00:03:31,590
so what this column is saying is here

00:03:28,890 --> 00:03:33,959
we've set our prefix lens to three so we

00:03:31,590 --> 00:03:35,880
have a hundred thousand unique prefixes

00:03:33,959 --> 00:03:39,150
that are all three bytes in length

00:03:35,880 --> 00:03:41,220
this produces a KB a key values set size

00:03:39,150 --> 00:03:43,260
as far as key and about just just the

00:03:41,220 --> 00:03:44,700
key lengths and value lengths of nine

00:03:43,260 --> 00:03:47,310
hundred twenty megabytes and we're using

00:03:44,700 --> 00:03:48,600
a two gig page cache this is very

00:03:47,310 --> 00:03:50,580
similar to what you would get if you

00:03:48,600 --> 00:03:52,020
were using the FTP directory layer and

00:03:50,580 --> 00:03:54,030
you created a hundred thousand

00:03:52,020 --> 00:03:55,590
directories your average shortened

00:03:54,030 --> 00:03:58,110
prefix if the directory layer would give

00:03:55,590 --> 00:04:00,989
you would be about three bytes so on

00:03:58,110 --> 00:04:03,660
this test you can see that both sequel

00:04:00,989 --> 00:04:06,180
Lite in Redwood are around 1.5 gigs of

00:04:03,660 --> 00:04:09,630
used space Redwood loaded it a little

00:04:06,180 --> 00:04:12,270
faster at 116 seconds and both of the

00:04:09,630 --> 00:04:16,230
deferred clear time which is the time it

00:04:12,270 --> 00:04:19,260
takes to walk the tree and and clear the

00:04:16,230 --> 00:04:22,109
entire and clear all keys from the data

00:04:19,260 --> 00:04:24,240
structure is about the same so on the

00:04:22,109 --> 00:04:26,139
next column we're going to bump up the

00:04:24,240 --> 00:04:28,210
prefix size to 16 so

00:04:26,139 --> 00:04:29,680
this is pretend instead of using the

00:04:28,210 --> 00:04:34,110
directory layer you used long

00:04:29,680 --> 00:04:37,749
descriptive names for every prefix so

00:04:34,110 --> 00:04:40,930
redwoods characteristics haven't really

00:04:37,749 --> 00:04:43,779
changed actually everything kind of gots

00:04:40,930 --> 00:04:47,710
got lower which is just a basically

00:04:43,779 --> 00:04:50,379
noise in the test but essentially a

00:04:47,710 --> 00:04:52,629
footprint and a load time and the big

00:04:50,379 --> 00:04:54,819
clear time are the same meanwhile sequel

00:04:52,629 --> 00:04:57,819
light has seen an increase in all three

00:04:54,819 --> 00:05:00,539
of those things we're still using a two

00:04:57,819 --> 00:05:02,349
gig page cache and so everything

00:05:00,539 --> 00:05:04,840
comfortably just about everything

00:05:02,349 --> 00:05:06,400
comfortably comfortably fits into memory

00:05:04,840 --> 00:05:08,979
for both storage engines at this point

00:05:06,400 --> 00:05:11,199
then we move up to 32 byte prefixes

00:05:08,979 --> 00:05:12,819
which are just more flourish a

00:05:11,199 --> 00:05:16,419
descriptive names on the same hundred

00:05:12,819 --> 00:05:18,659
thousand unique prefixes and again

00:05:16,419 --> 00:05:22,300
redwoods metrics are basically the same

00:05:18,659 --> 00:05:26,589
secret light is seen a large bump in in

00:05:22,300 --> 00:05:27,969
in this space used and also notably that

00:05:26,589 --> 00:05:29,439
twenty eight hundred fifty five

00:05:27,969 --> 00:05:31,270
megabytes it's larger than the 2gig

00:05:29,439 --> 00:05:34,029
cache and so now there's a bunch of

00:05:31,270 --> 00:05:36,250
cache misses happening so the load time

00:05:34,029 --> 00:05:40,029
has gone up substantially and the clear

00:05:36,250 --> 00:05:42,849
time is twenty times higher and then we

00:05:40,029 --> 00:05:48,689
I did one more test which is using the

00:05:42,849 --> 00:05:50,949
same exact prefix size and sorry

00:05:48,689 --> 00:05:54,460
workload shape is the previous one but

00:05:50,949 --> 00:05:58,750
we lowered the page cache to only one

00:05:54,460 --> 00:06:02,020
hundred megabytes so redwood again is

00:05:58,750 --> 00:06:05,110
basically unchanged the clear time did

00:06:02,020 --> 00:06:07,479
get did become four times longer but

00:06:05,110 --> 00:06:10,300
it's still very small load time was

00:06:07,479 --> 00:06:13,300
largely unaffected and sequel Lite has

00:06:10,300 --> 00:06:15,159
the same footprint as before but the

00:06:13,300 --> 00:06:17,979
load time has gone up and the clear time

00:06:15,159 --> 00:06:21,750
has almost doubled I'll talk more later

00:06:17,979 --> 00:06:24,279
about some of the reasons why that is

00:06:21,750 --> 00:06:26,680
but the takeaway from this slide is that

00:06:24,279 --> 00:06:31,389
common people fixes in redwood are

00:06:26,680 --> 00:06:33,120
nearly free so moving on I'd like to

00:06:31,389 --> 00:06:36,990
talk in detail about a design update

00:06:33,120 --> 00:06:39,039
that I made in the past year based on

00:06:36,990 --> 00:06:41,589
basically the leaks

00:06:39,039 --> 00:06:43,779
I had going down the path laid out in

00:06:41,589 --> 00:06:46,689
last presentation so that a quick review

00:06:43,779 --> 00:06:49,119
of the pager design so the idea is you

00:06:46,689 --> 00:06:51,580
can read or update or read a page at a

00:06:49,119 --> 00:06:54,159
version and the pager would keep a page

00:06:51,580 --> 00:06:55,839
table that map's logical pages at a

00:06:54,159 --> 00:06:58,960
version to some physical page that

00:06:55,839 --> 00:07:00,460
contains the actual data this table will

00:06:58,960 --> 00:07:02,649
be kept entirely in memory so that

00:07:00,460 --> 00:07:05,020
lookups are fast and then it also must

00:07:02,649 --> 00:07:07,719
be periodically in some way snapshotted

00:07:05,020 --> 00:07:09,999
to persisted to disk like some sort of

00:07:07,719 --> 00:07:14,589
rolling snapshot and it needs to be

00:07:09,999 --> 00:07:16,689
recovered on startup so the goal of this

00:07:14,589 --> 00:07:19,509
design was that one page update equals

00:07:16,689 --> 00:07:21,490
one right because basically when you

00:07:19,509 --> 00:07:23,349
update a page at some version you put

00:07:21,490 --> 00:07:25,899
that data somewhere and then you use the

00:07:23,349 --> 00:07:27,759
page table to route reads to that

00:07:25,899 --> 00:07:30,300
location so there's never a reason to

00:07:27,759 --> 00:07:31,719
copy that data anywhere else

00:07:30,300 --> 00:07:35,050
unfortunately there's some drawbacks

00:07:31,719 --> 00:07:37,270
with this design and one of them is once

00:07:35,050 --> 00:07:39,189
a page is updated one or more remap

00:07:37,270 --> 00:07:42,099
entries will exist for that page

00:07:39,189 --> 00:07:44,379
basically forever so here we have on the

00:07:42,099 --> 00:07:47,529
left a table showing nine different

00:07:44,379 --> 00:07:49,839
versions of logical page 9 at different

00:07:47,529 --> 00:07:52,330
physical pages once time is going gone

00:07:49,839 --> 00:07:55,659
on and we are only maintaining version

00:07:52,330 --> 00:07:58,990
44 and newer we only need one of these

00:07:55,659 --> 00:08:01,089
entries which sounds good except for we

00:07:58,990 --> 00:08:03,039
still need one of these entries and if

00:08:01,089 --> 00:08:05,769
you think about it over time almost

00:08:03,039 --> 00:08:08,199
every page in your storage space is

00:08:05,769 --> 00:08:10,629
going to be updated and so nearly every

00:08:08,199 --> 00:08:12,999
page is going to have its actual data

00:08:10,629 --> 00:08:15,610
located at a different page from where

00:08:12,999 --> 00:08:17,169
it started and so with something like 2

00:08:15,610 --> 00:08:20,830
terabytes of space you have a half a

00:08:17,169 --> 00:08:23,439
billion pages a page IDs and if nearly

00:08:20,830 --> 00:08:25,240
all of those are remap somewhere no

00:08:23,439 --> 00:08:27,009
matter how you design it you have you

00:08:25,240 --> 00:08:28,930
have gigabytes many gigabytes of memory

00:08:27,009 --> 00:08:32,500
being used for your lookup table and

00:08:28,930 --> 00:08:34,269
then also being a snapshot it persisted

00:08:32,500 --> 00:08:36,459
to disk and reload it on cold-start so

00:08:34,269 --> 00:08:40,089
this is very memory intensive and it's

00:08:36,459 --> 00:08:42,010
it's slow to start so the new design is

00:08:40,089 --> 00:08:45,790
what I call a delayed right ahead log

00:08:42,010 --> 00:08:47,800
it's for shorter retention periods and

00:08:45,790 --> 00:08:50,829
the pager is pluggable so other pages

00:08:47,800 --> 00:08:52,120
can be written the idea here is that one

00:08:50,829 --> 00:08:54,100
page update is

00:08:52,120 --> 00:08:55,510
to to rights so there's one right

00:08:54,100 --> 00:08:57,640
immediately to get your data to be

00:08:55,510 --> 00:08:59,290
durable somewhere and then there's up to

00:08:57,640 --> 00:09:01,210
there's up to an additional right later

00:08:59,290 --> 00:09:02,830
what's usually but will be wrong one and

00:09:01,210 --> 00:09:04,540
I want to go through the sequence of

00:09:02,830 --> 00:09:07,480
what of what that is actually referring

00:09:04,540 --> 00:09:10,270
to so in the updated design the page

00:09:07,480 --> 00:09:12,220
table has now has an implied identity

00:09:10,270 --> 00:09:14,260
record for each logical page which

00:09:12,220 --> 00:09:16,420
basically says that if you look up a

00:09:14,260 --> 00:09:18,310
page at a version that is less than the

00:09:16,420 --> 00:09:21,910
oldest version in the table then the

00:09:18,310 --> 00:09:23,770
answer is the same page ID that went in

00:09:21,910 --> 00:09:26,380
so the the physical page ID is the same

00:09:23,770 --> 00:09:29,230
as the logical page ID so as older page

00:09:26,380 --> 00:09:31,060
told our page table entries expire they

00:09:29,230 --> 00:09:34,839
are removed so how can we do that well

00:09:31,060 --> 00:09:37,120
so here on the second graph we our chart

00:09:34,839 --> 00:09:39,790
we've advanced the oldest retained

00:09:37,120 --> 00:09:44,820
version to vert two to twenty-two so now

00:09:39,790 --> 00:09:47,529
we no longer need to keep version oh

00:09:44,820 --> 00:09:54,430
sorry did I messed that up I think I did

00:09:47,529 --> 00:09:58,060
I'm sorry pretend that that is wait a

00:09:54,430 --> 00:09:59,620
minute this completely threw me okay the

00:09:58,060 --> 00:10:00,870
version on that might be wrong I can't

00:09:59,620 --> 00:10:04,029
think about it at the moment but

00:10:00,870 --> 00:10:07,270
basically we want to remove the nine

00:10:04,029 --> 00:10:09,520
five twelve entry and we can do that by

00:10:07,270 --> 00:10:12,339
copying the contents from page twelve

00:10:09,520 --> 00:10:15,700
onto page 9 and then we no longer need

00:10:12,339 --> 00:10:18,370
that second entry so if you read oh yeah

00:10:15,700 --> 00:10:22,209
it actually is sorry it is correct so if

00:10:18,370 --> 00:10:24,459
you read at version 22 or 23 it will be

00:10:22,209 --> 00:10:26,230
the mapping will be served by the

00:10:24,459 --> 00:10:28,600
identity record and it will give you a

00:10:26,230 --> 00:10:31,300
physical page nine and that is where

00:10:28,600 --> 00:10:32,740
that data does in fact exist so now we

00:10:31,300 --> 00:10:35,350
have only three entries left for page

00:10:32,740 --> 00:10:37,839
nine which is nice and then as time goes

00:10:35,350 --> 00:10:44,020
on now we've bumped our retained version

00:10:37,839 --> 00:10:45,760
forward to version 44 so now we do the

00:10:44,020 --> 00:10:47,440
same trick again but with an additional

00:10:45,760 --> 00:10:50,410
optimization so since we bumped the

00:10:47,440 --> 00:10:52,720
retained version forward a large amount

00:10:50,410 --> 00:10:54,310
at once we can actually skip over two of

00:10:52,720 --> 00:10:56,230
these copies because we only really only

00:10:54,310 --> 00:10:57,940
need the latest version to be copied

00:10:56,230 --> 00:11:00,640
back to physical page nine so we skipped

00:10:57,940 --> 00:11:02,680
over 23 and 40 which saves rights which

00:11:00,640 --> 00:11:04,270
is why the last ice let's the last slide

00:11:02,680 --> 00:11:08,110
set up to

00:11:04,270 --> 00:11:11,410
additional right and so now we have no

00:11:08,110 --> 00:11:15,010
entry left in our page in our page table

00:11:11,410 --> 00:11:16,420
for page ID nine so next I'm going to

00:11:15,010 --> 00:11:19,870
talk about some i/o patterns

00:11:16,420 --> 00:11:22,210
so first rights so specifically the

00:11:19,870 --> 00:11:25,120
reads to service rights so sequel Lite

00:11:22,210 --> 00:11:28,210
does one tree traversal for every

00:11:25,120 --> 00:11:30,280
mutation in order and so it incur serial

00:11:28,210 --> 00:11:33,460
cache misses so I'm gonna go through a

00:11:30,280 --> 00:11:35,560
series of six insertions here so we

00:11:33,460 --> 00:11:37,390
start at the root we tours to here we go

00:11:35,560 --> 00:11:39,730
to this page we have to wait for it to

00:11:37,390 --> 00:11:43,590
load oh well I'm using the wrong key I

00:11:39,730 --> 00:11:46,630
think so it's not something animating oh

00:11:43,590 --> 00:11:48,850
are you guys seeing the animations okay

00:11:46,630 --> 00:11:51,070
it doesn't show it to me here okay so

00:11:48,850 --> 00:11:53,040
we're on our second latency incurred we

00:11:51,070 --> 00:11:56,920
go back to the root wait for this

00:11:53,040 --> 00:11:58,480
internal node to load and then wait for

00:11:56,920 --> 00:11:59,440
number four I'm just gonna go through

00:11:58,480 --> 00:12:00,550
this kind of quickly I think you get the

00:11:59,440 --> 00:12:03,700
idea

00:12:00,550 --> 00:12:05,440
so basically each of these seeks you

00:12:03,700 --> 00:12:07,510
know I had some stops along the way to

00:12:05,440 --> 00:12:09,850
wait for pages to loads we suffered six

00:12:07,510 --> 00:12:12,640
serial read Layton sees to do these

00:12:09,850 --> 00:12:14,950
changes but in Redwood we buffer and

00:12:12,640 --> 00:12:17,590
sort mutations for an entire commit and

00:12:14,950 --> 00:12:20,800
then we do one parallel Traverse or poke

00:12:17,590 --> 00:12:23,380
traversal per commit cycle to all of the

00:12:20,800 --> 00:12:25,120
nodes affected by those changes so the

00:12:23,380 --> 00:12:26,770
turret and the traversal is not purely

00:12:25,120 --> 00:12:28,600
breadth-first or depth-first it actually

00:12:26,770 --> 00:12:30,370
depends on what's in the cache so we

00:12:28,600 --> 00:12:32,260
start at the root and we start

00:12:30,370 --> 00:12:34,630
traversing and we start loading these

00:12:32,260 --> 00:12:37,260
pages we don't have to wait for them we

00:12:34,630 --> 00:12:40,900
just continue on going as far as we can

00:12:37,260 --> 00:12:42,760
and we now that all those are loading we

00:12:40,900 --> 00:12:44,020
wait for them to finish and then once

00:12:42,760 --> 00:12:45,880
this one finishes has some follow-on

00:12:44,020 --> 00:12:47,140
work so it has to wait for these

00:12:45,880 --> 00:12:50,470
children slow because we're editing both

00:12:47,140 --> 00:12:53,080
of these pages and once they're loaded

00:12:50,470 --> 00:12:56,110
then we've written our changes and we're

00:12:53,080 --> 00:12:58,090
done I'd also like to point out that

00:12:56,110 --> 00:13:00,070
let's say we had a bunch of changes

00:12:58,090 --> 00:13:01,960
during our during our one second commit

00:13:00,070 --> 00:13:05,800
cycle a bunch of different writes came

00:13:01,960 --> 00:13:07,360
in between the keys D and E all of those

00:13:05,800 --> 00:13:08,890
insertions in that range would be

00:13:07,360 --> 00:13:10,450
handled together at once

00:13:08,890 --> 00:13:14,080
which makes for very efficient

00:13:10,450 --> 00:13:15,580
sequential insertion on redwood so now

00:13:14,080 --> 00:13:18,070
I'd like to talk about read latency so

00:13:15,580 --> 00:13:20,350
with b-trees read latency is a

00:13:18,070 --> 00:13:22,570
affected by branching factor ideally

00:13:20,350 --> 00:13:25,930
cache size is approximately the amount

00:13:22,570 --> 00:13:28,120
of space you're using / / your branching

00:13:25,930 --> 00:13:30,820
factor the idea there is that every page

00:13:28,120 --> 00:13:33,010
hopefully in your cache points to

00:13:30,820 --> 00:13:35,140
branching factor other pages so if your

00:13:33,010 --> 00:13:36,820
branch of branching factors 20 every

00:13:35,140 --> 00:13:38,380
hopefully most of the pages in your

00:13:36,820 --> 00:13:41,590
cache or internal and they point to 20

00:13:38,380 --> 00:13:45,730
of their pages and so in that case if

00:13:41,590 --> 00:13:47,440
you had 5% cache to disk ratio you would

00:13:45,730 --> 00:13:51,130
have most of the internal structure of

00:13:47,440 --> 00:13:53,710
your tree in memory so if you're dealing

00:13:51,130 --> 00:13:57,490
with you know say a 2 terabyte or 4

00:13:53,710 --> 00:14:00,850
terabyte disk having to have 5% of that

00:13:57,490 --> 00:14:02,320
as a disk cache is pretty high so the

00:14:00,850 --> 00:14:05,020
higher your branching factor is the

00:14:02,320 --> 00:14:07,540
lower your cache needs to be to ensure

00:14:05,020 --> 00:14:12,250
that most reads suffer only a single out

00:14:07,540 --> 00:14:14,860
of cache a single cache miss so in

00:14:12,250 --> 00:14:17,350
sequel light repeated key prefixes make

00:14:14,860 --> 00:14:19,540
the records inside pages bigger and so

00:14:17,350 --> 00:14:22,120
they harm branching factor non-trivial

00:14:19,540 --> 00:14:24,580
values like larger than 4 bytes also

00:14:22,120 --> 00:14:26,140
harm branching factor and redwood values

00:14:24,580 --> 00:14:29,230
aren't stored in the internal nodes and

00:14:26,140 --> 00:14:30,670
so that actually means that the keys and

00:14:29,230 --> 00:14:32,470
internal nodes don't need to be complete

00:14:30,670 --> 00:14:34,000
because they're not user Keys so they

00:14:32,470 --> 00:14:37,750
act they can actually minimal boundary

00:14:34,000 --> 00:14:39,730
keys that are just long enough to to do

00:14:37,750 --> 00:14:42,100
the continue the left-to-right decision

00:14:39,730 --> 00:14:44,530
of whether or not you can go down on

00:14:42,100 --> 00:14:47,950
once you know downward on the tree on

00:14:44,530 --> 00:14:50,920
one side or the other of that boundary

00:14:47,950 --> 00:14:52,600
also prefix compression means that

00:14:50,920 --> 00:14:54,070
there's no penalty for having repeat

00:14:52,600 --> 00:14:58,420
keeper if fixes and nodes we get more

00:14:54,070 --> 00:15:00,430
entities more entries per node so I like

00:14:58,420 --> 00:15:01,570
to talk about large get ranges so in

00:15:00,430 --> 00:15:03,340
sequel light if we want to read the

00:15:01,570 --> 00:15:07,840
range G through K we'll start at the

00:15:03,340 --> 00:15:09,730
root over here and we will traverse down

00:15:07,840 --> 00:15:11,770
to G we'll wait for that page to load

00:15:09,730 --> 00:15:13,600
will we get its contents and then we'll

00:15:11,770 --> 00:15:16,420
go to the next page so we're this is our

00:15:13,600 --> 00:15:18,460
second cache miss third cache miss so we

00:15:16,420 --> 00:15:21,760
have four real agencies to get this data

00:15:18,460 --> 00:15:23,770
in red would it supports a prefetch size

00:15:21,760 --> 00:15:26,710
so in this case we have four K pages and

00:15:23,770 --> 00:15:28,740
we're using a prefetch of 15k so during

00:15:26,710 --> 00:15:32,250
the traversal it'll recognize

00:15:28,740 --> 00:15:34,980
that 15k is not that one page is not

00:15:32,250 --> 00:15:37,950
enough to satisfy 60 15k and so it will

00:15:34,980 --> 00:15:40,920
start for adjacent siblings loading

00:15:37,950 --> 00:15:44,670
simultaneously now I'd like to talk

00:15:40,920 --> 00:15:46,709
about oversized nodes so in sequel light

00:15:44,670 --> 00:15:48,899
right it's actually records instead of

00:15:46,709 --> 00:15:50,370
pages that are oversized and they're

00:15:48,899 --> 00:15:53,310
actually oversized individually using

00:15:50,370 --> 00:15:57,990
extra page it's called overflow pages so

00:15:53,310 --> 00:16:00,240
here if we want to scan C through e we

00:15:57,990 --> 00:16:02,880
start at the root and we jump down to

00:16:00,240 --> 00:16:04,950
this leaf node and we find as we're

00:16:02,880 --> 00:16:06,810
scanning that some of these records have

00:16:04,950 --> 00:16:08,730
these overflow page chains this one is

00:16:06,810 --> 00:16:10,080
one of the flow page we read that then

00:16:08,730 --> 00:16:11,730
we go to the next one oh it has an

00:16:10,080 --> 00:16:12,870
overflow page and that and that overflow

00:16:11,730 --> 00:16:15,240
page actually points to get another

00:16:12,870 --> 00:16:17,430
overflow page so we have three read

00:16:15,240 --> 00:16:21,390
Layton sees to read essentially four to

00:16:17,430 --> 00:16:23,279
four pages worth of data and right away

00:16:21,390 --> 00:16:26,190
what we do is we just make these child

00:16:23,279 --> 00:16:30,540
pointers bigger so so no links our

00:16:26,190 --> 00:16:32,399
variable size and a child pointer has a

00:16:30,540 --> 00:16:35,490
node link which contains all of the

00:16:32,399 --> 00:16:38,370
pages that the next lower node is made

00:16:35,490 --> 00:16:41,100
of so in this case there's four of them

00:16:38,370 --> 00:16:42,870
so we we start at the root and we in in

00:16:41,100 --> 00:16:45,300
parallel load all four of these pages

00:16:42,870 --> 00:16:48,209
that constitute what I call a super page

00:16:45,300 --> 00:16:52,440
and now we can read see through E is

00:16:48,209 --> 00:16:53,790
suffering only one cache miss latency so

00:16:52,440 --> 00:16:56,220
next I'd like to talk about deferred

00:16:53,790 --> 00:16:58,079
range clear so this relates very closer

00:16:56,220 --> 00:17:01,160
to the previous slide so sequel Lite

00:16:58,079 --> 00:17:04,020
must read every page in a cleared range

00:17:01,160 --> 00:17:06,150
in order to determine if any of those

00:17:04,020 --> 00:17:09,120
pages have any of the records in those

00:17:06,150 --> 00:17:11,370
pages have any overflow page chains so

00:17:09,120 --> 00:17:13,260
it has to read not only read the pages

00:17:11,370 --> 00:17:15,059
into memory but it also has to iterate

00:17:13,260 --> 00:17:18,480
over all the records in them redwood

00:17:15,059 --> 00:17:21,870
because of its because of its multi page

00:17:18,480 --> 00:17:25,050
pointer design doesn't have to read or

00:17:21,870 --> 00:17:27,689
scan through leaf nodes when doing range

00:17:25,050 --> 00:17:29,940
clear so this results in in branching

00:17:27,689 --> 00:17:31,800
factor X less i/o which in redwood is

00:17:29,940 --> 00:17:32,360
going to be usually way more than a

00:17:31,800 --> 00:17:37,040
hundred

00:17:32,360 --> 00:17:39,440
X less IO that is the reason why that

00:17:37,040 --> 00:17:43,010
third number on the performance slide

00:17:39,440 --> 00:17:46,160
was so good for Redwood the four for

00:17:43,010 --> 00:17:49,280
clearing all of the keys in the data set

00:17:46,160 --> 00:17:52,030
it just has to do you know in that case

00:17:49,280 --> 00:17:57,049
probably 150 X less IO so that's why

00:17:52,030 --> 00:17:59,270
which is also why having a 1.5 data set

00:17:57,049 --> 00:18:01,010
on hundred megabyte cache wasn't really

00:17:59,270 --> 00:18:02,750
a problem because most of the nodes that

00:18:01,010 --> 00:18:07,549
were needed for the deferred clear were

00:18:02,750 --> 00:18:09,950
already we're still in the cache so next

00:18:07,549 --> 00:18:11,870
I want to talk about data movement so

00:18:09,950 --> 00:18:13,429
base data movement in FTB is basically

00:18:11,870 --> 00:18:14,660
when you read a shard from one storage

00:18:13,429 --> 00:18:16,190
server and write it to another storage

00:18:14,660 --> 00:18:19,370
server and then clear on the original

00:18:16,190 --> 00:18:21,440
storage server so this is not a great

00:18:19,370 --> 00:18:25,070
pattern for sequel light because the

00:18:21,440 --> 00:18:26,630
reads are of the reads of this couple

00:18:25,070 --> 00:18:28,669
hundred megabytes shard are gonna push a

00:18:26,630 --> 00:18:30,530
bunch of data out of cache to do this

00:18:28,669 --> 00:18:32,900
one-time read which likely won't be

00:18:30,530 --> 00:18:35,150
repeated and then the write happens over

00:18:32,900 --> 00:18:36,650
time and then we come back likely after

00:18:35,150 --> 00:18:38,809
a lot of those pages have been evicted

00:18:36,650 --> 00:18:41,120
from the cache and we do the same reads

00:18:38,809 --> 00:18:44,870
all over again and we pollute the cache

00:18:41,120 --> 00:18:47,090
likely again just to to clear that data

00:18:44,870 --> 00:18:49,370
so redwood supports non caching reads

00:18:47,090 --> 00:18:51,169
which are used in this case in some

00:18:49,370 --> 00:18:52,640
other situations too and as we talked

00:18:51,169 --> 00:18:54,200
about it before sequential insertions

00:18:52,640 --> 00:18:56,030
faster and deferred range clears much

00:18:54,200 --> 00:18:58,520
faster so data movement on a redwood

00:18:56,030 --> 00:19:00,620
back cluster of FTB is going to be a lot

00:18:58,520 --> 00:19:02,720
is going to have a lot less impact on

00:19:00,620 --> 00:19:05,200
the clusters performance still actually

00:19:02,720 --> 00:19:10,820
want to talk about future improvements

00:19:05,200 --> 00:19:12,559
so first on my list is to reduce

00:19:10,820 --> 00:19:14,870
duplicative right path mutation buffer

00:19:12,559 --> 00:19:17,299
lookups basically what this is saying is

00:19:14,870 --> 00:19:18,950
that there's a lot of extra lookups into

00:19:17,299 --> 00:19:24,650
the mutation buffer being done really

00:19:18,950 --> 00:19:26,360
about 50 X for some workloads that is

00:19:24,650 --> 00:19:28,250
basically because that code was written

00:19:26,360 --> 00:19:29,570
for a correct is first and not for

00:19:28,250 --> 00:19:32,000
performance and I haven't gotten to

00:19:29,570 --> 00:19:35,540
fixing that yet but additionally as

00:19:32,000 --> 00:19:37,400
Diego presented earlier his team is

00:19:35,540 --> 00:19:38,720
working on a replacement for the

00:19:37,400 --> 00:19:41,720
mutation buffer which is currently just

00:19:38,720 --> 00:19:43,250
a SP map for convenience and so the

00:19:41,720 --> 00:19:44,670
lookups themselves are also going to get

00:19:43,250 --> 00:19:46,470
a lot faster and

00:19:44,670 --> 00:19:48,330
and very notably the insertions are

00:19:46,470 --> 00:19:52,080
gonna get a lot faster as those needs to

00:19:48,330 --> 00:19:54,060
happen we're you know either way also

00:19:52,080 --> 00:19:59,100
the the prefix compress structure of the

00:19:54,060 --> 00:20:00,780
of the nodes and the the the the overall

00:19:59,100 --> 00:20:02,310
tree is not being fully leveraged

00:20:00,780 --> 00:20:03,900
there's a lot more bite skipping that

00:20:02,310 --> 00:20:07,470
can be that could be being done during

00:20:03,900 --> 00:20:09,240
comparisons nodes themselves are

00:20:07,470 --> 00:20:10,410
currently always rebuilt by taking all

00:20:09,240 --> 00:20:13,440
the old data on the new data and

00:20:10,410 --> 00:20:15,510
building a new node that also for small

00:20:13,440 --> 00:20:16,800
edits nurit nodes could be surgically

00:20:15,510 --> 00:20:19,530
updated which would be a lot faster for

00:20:16,800 --> 00:20:21,270
highly random small writes and there's

00:20:19,530 --> 00:20:22,650
also some single version optimizations

00:20:21,270 --> 00:20:27,870
that could be done for when you know you

00:20:22,650 --> 00:20:30,660
don't need any version history so the

00:20:27,870 --> 00:20:33,540
first production or release of Redwood

00:20:30,660 --> 00:20:35,340
is planned to be an FDB 7.0 it will

00:20:33,540 --> 00:20:37,410
definitely be a single version mode only

00:20:35,340 --> 00:20:38,520
no history will be exposed yet even

00:20:37,410 --> 00:20:40,590
though a lot of the internals are there

00:20:38,520 --> 00:20:42,480
it should be faster for many workloads

00:20:40,590 --> 00:20:45,000
but the worst case is going to be small

00:20:42,480 --> 00:20:47,310
key value pairs with highly random

00:20:45,000 --> 00:20:51,220
writes that are very little sequential

00:20:47,310 --> 00:20:57,319
nests and that is all I had

00:20:51,220 --> 00:20:57,319

YouTube URL: https://www.youtube.com/watch?v=5iqKu1pVDvE


