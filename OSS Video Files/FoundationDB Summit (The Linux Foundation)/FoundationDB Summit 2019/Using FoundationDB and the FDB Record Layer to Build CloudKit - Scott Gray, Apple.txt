Title: Using FoundationDB and the FDB Record Layer to Build CloudKit - Scott Gray, Apple
Publication date: 2020-05-07
Playlist: FoundationDB Summit 2019
Description: 
	Using FoundationDB and the FDB Record Layer to Build CloudKit - Scott Gray, Apple 

In this case study, we will highlight some key features of FoundationDB and the Record Layer that help us build CloudKit, Apple’s cloud storage system for structured data. We will focus on three key areas where FoundationDB and the Record Layer have unlocked large benefits for CloudKit. First, FoundationDB’s arbitrary multi-key ACID transactions have allowed us to implement advanced secondary indexing at scale, including our recently-developed transactional full-text search system. Second, the Record Layer’s version indexes have greatly increased the scalability of CloudKit’s core cross-device synchronization functionality. Finally, FoundationDB’s best-in-class reliability has simplified our operations, especially while recovering from failures. All together, FoundationDB has allowed us to provide richer APIs, greater scalability, and higher availability across billions of databases.
Captions: 
	00:00:00,359 --> 00:00:04,980
okay hi I'm Scott gray I'm an engineer

00:00:03,000 --> 00:00:09,330
with Apple thus the the dark mode

00:00:04,980 --> 00:00:12,690
presentation so I'm here to talk to you

00:00:09,330 --> 00:00:14,700
about cloud kit which our cloud kit

00:00:12,690 --> 00:00:18,930
built on foundation DB and record layer

00:00:14,700 --> 00:00:22,199
and specifically I should probably start

00:00:18,930 --> 00:00:24,720
with kind of what is cloud kit cloud kit

00:00:22,199 --> 00:00:26,310
is resilient structured cloud to storage

00:00:24,720 --> 00:00:27,330
you might otherwise know that as a

00:00:26,310 --> 00:00:29,939
database

00:00:27,330 --> 00:00:31,439
the idea behind cloud kit is we provide

00:00:29,939 --> 00:00:34,770
infrastructure for an application

00:00:31,439 --> 00:00:36,840
developer to come in design a schema for

00:00:34,770 --> 00:00:38,940
their application deploy that schema to

00:00:36,840 --> 00:00:41,280
the cloud and then provide all your

00:00:38,940 --> 00:00:44,309
standard database functionality insert

00:00:41,280 --> 00:00:45,750
update delete query and really kind of

00:00:44,309 --> 00:00:49,320
where the bread and butter of it comes

00:00:45,750 --> 00:00:51,780
in are on notification and sync between

00:00:49,320 --> 00:00:54,390
the synchronization of changes between

00:00:51,780 --> 00:00:56,610
your devices so we're not just a place

00:00:54,390 --> 00:00:58,710
to kind of rest your data in case you

00:00:56,610 --> 00:01:03,899
drop your phone in the toilet which I've

00:00:58,710 --> 00:01:05,220
done don't ask me how and but also to

00:01:03,899 --> 00:01:07,110
make sure that any change you make on

00:01:05,220 --> 00:01:16,110
one device has been synchronized to

00:01:07,110 --> 00:01:19,140
another device change so from a tenancy

00:01:16,110 --> 00:01:21,509
model cloudkit takes sort of an inverted

00:01:19,140 --> 00:01:23,880
view of tenancy from although what a lot

00:01:21,509 --> 00:01:26,729
of other database systems do and that is

00:01:23,880 --> 00:01:28,380
that for every application that you're

00:01:26,729 --> 00:01:30,119
running on your device whether it be

00:01:28,380 --> 00:01:31,970
your phone or on Mac OS every

00:01:30,119 --> 00:01:35,640
application that uses the cloud is

00:01:31,970 --> 00:01:37,829
allocated a distinct logical database so

00:01:35,640 --> 00:01:39,689
what this means is across the thousands

00:01:37,829 --> 00:01:41,729
of applications whether they be the

00:01:39,689 --> 00:01:42,899
applications that are natively available

00:01:41,729 --> 00:01:45,659
on your device or third-party

00:01:42,899 --> 00:01:47,670
applications written for the cloud each

00:01:45,659 --> 00:01:49,200
instance of an application that you're

00:01:47,670 --> 00:01:54,229
running you're allocated your own

00:01:49,200 --> 00:01:57,810
database so this has made it possible

00:01:54,229 --> 00:02:00,270
thanks to foundation DB of course or I

00:01:57,810 --> 00:02:03,270
wouldn't be here talking about it and

00:02:00,270 --> 00:02:04,950
also by record layer so record layer is

00:02:03,270 --> 00:02:08,429
a project that actually came out of

00:02:04,950 --> 00:02:11,340
cloud kit it's it was open sourced in

00:02:08,429 --> 00:02:13,800
January of this year it provides

00:02:11,340 --> 00:02:15,930
facilities that give you basic will

00:02:13,800 --> 00:02:17,670
database semantics relational ish

00:02:15,930 --> 00:02:19,290
there's a couple places where it might

00:02:17,670 --> 00:02:21,720
feel a little foreign but by-and-large

00:02:19,290 --> 00:02:23,610
relational semantics which of course

00:02:21,720 --> 00:02:25,560
includes the ability to have a schema to

00:02:23,610 --> 00:02:28,560
define record types to create indexes

00:02:25,560 --> 00:02:30,900
and maintain those indexes it has a

00:02:28,560 --> 00:02:32,670
stateless execution architecture which

00:02:30,900 --> 00:02:36,150
I'm going to talk about in a bit

00:02:32,670 --> 00:02:37,890
particularly on how we leverage that and

00:02:36,150 --> 00:02:40,440
also point out there is a more detailed

00:02:37,890 --> 00:02:45,060
talk specifically on record layer that's

00:02:40,440 --> 00:02:46,470
going to happen at 3:40 p.m. so let's

00:02:45,060 --> 00:02:49,230
start off with kind of one of the key

00:02:46,470 --> 00:02:52,410
abstractions of record layer which is

00:02:49,230 --> 00:02:56,490
this concept of a record store so a

00:02:52,410 --> 00:02:59,550
record store is a logical database it is

00:02:56,490 --> 00:03:00,990
defined by an external schema and as I

00:02:59,550 --> 00:03:03,720
said that's because if you think about

00:03:00,990 --> 00:03:06,300
it I'm developing say the photos

00:03:03,720 --> 00:03:08,280
application I have one schema for photos

00:03:06,300 --> 00:03:14,070
but I want to instantiate a billion

00:03:08,280 --> 00:03:16,290
databases you locate of record stores

00:03:14,070 --> 00:03:20,760
located at a given prefix in the f DB

00:03:16,290 --> 00:03:24,150
key space and the in the entire database

00:03:20,760 --> 00:03:26,760
is contained within a contiguous key

00:03:24,150 --> 00:03:28,620
range that means all the records all of

00:03:26,760 --> 00:03:30,660
the indexes all of the metadata about

00:03:28,620 --> 00:03:32,430
the state of the record store is all

00:03:30,660 --> 00:03:34,470
contained in a contiguous key range and

00:03:32,430 --> 00:03:38,520
that that can becomes very important in

00:03:34,470 --> 00:03:42,900
a minute so for example let's say we

00:03:38,520 --> 00:03:45,480
took we decided that our key at which we

00:03:42,900 --> 00:03:47,250
wish to locate our record store is maybe

00:03:45,480 --> 00:03:49,350
the first part of the the prefix is

00:03:47,250 --> 00:03:51,000
going to be the user ID the second part

00:03:49,350 --> 00:03:53,459
might be the application that we're

00:03:51,000 --> 00:03:55,830
talking about so what this allows us to

00:03:53,459 --> 00:03:58,080
do is have a series of databases that

00:03:55,830 --> 00:04:00,300
all that all belong to this user they

00:03:58,080 --> 00:04:01,860
all are all contiguously located next to

00:04:00,300 --> 00:04:04,140
each other and each one is of course

00:04:01,860 --> 00:04:06,120
completely self-contained and thus

00:04:04,140 --> 00:04:08,610
within our cluster we can be housing

00:04:06,120 --> 00:04:12,269
millions of users and many tens of

00:04:08,610 --> 00:04:14,250
millions of databases and of course when

00:04:12,269 --> 00:04:15,840
we want to scale out it's simply a

00:04:14,250 --> 00:04:18,810
matter of adding a directory service

00:04:15,840 --> 00:04:20,760
which basically keeps track of which

00:04:18,810 --> 00:04:21,840
cluster owns a particular users data the

00:04:20,760 --> 00:04:24,480
data

00:04:21,840 --> 00:04:26,220
and every request gets routed to the

00:04:24,480 --> 00:04:28,710
cluster we know because of how we've

00:04:26,220 --> 00:04:30,570
organized our our keys we know exactly

00:04:28,710 --> 00:04:34,560
where the database for that application

00:04:30,570 --> 00:04:36,210
lives taking out a step further of

00:04:34,560 --> 00:04:38,820
course you know scale out becomes easy

00:04:36,210 --> 00:04:40,320
every time every time we feel that we're

00:04:38,820 --> 00:04:43,700
reaching capacity you can throw another

00:04:40,320 --> 00:04:47,850
server in new accounts will become

00:04:43,700 --> 00:04:50,820
allocated to this and furthermore so we

00:04:47,850 --> 00:04:52,200
have some machinery that sits at the

00:04:50,820 --> 00:04:55,260
bottom that's physically at the bottom

00:04:52,200 --> 00:04:57,570
of everything so we have this machinery

00:04:55,260 --> 00:04:59,880
that's keeping an eye on things so how

00:04:57,570 --> 00:05:03,389
how is the workload distributed across

00:04:59,880 --> 00:05:05,040
the cluster if it sees an imbalance you

00:05:03,389 --> 00:05:06,870
know so now here's where the contiguous

00:05:05,040 --> 00:05:08,850
key range comes in you know so is if

00:05:06,870 --> 00:05:10,500
it's season imbalance all that all we

00:05:08,850 --> 00:05:13,020
need to do is to be able to pick up

00:05:10,500 --> 00:05:14,610
these ranges of keys and begin shifting

00:05:13,020 --> 00:05:17,700
and moving them around to rebalance the

00:05:14,610 --> 00:05:19,020
load across across the clusters of

00:05:17,700 --> 00:05:21,690
course in the you know in this example

00:05:19,020 --> 00:05:24,090
I've kind of lumped our our unit of work

00:05:21,690 --> 00:05:26,039
is an individual user with its

00:05:24,090 --> 00:05:27,510
contiguous set of databases but of

00:05:26,039 --> 00:05:29,400
course that's kind of arbitrary you

00:05:27,510 --> 00:05:30,840
could just as easily decided that every

00:05:29,400 --> 00:05:36,780
application is gonna live in a different

00:05:30,840 --> 00:05:38,310
set of clusters so the other major

00:05:36,780 --> 00:05:40,560
attribute that I mentioned about record

00:05:38,310 --> 00:05:41,940
layer is stateless compute so what does

00:05:40,560 --> 00:05:45,690
that really mean

00:05:41,940 --> 00:05:49,729
so what this means is that every server

00:05:45,690 --> 00:05:52,470
can handle handle any given request and

00:05:49,729 --> 00:05:55,350
a great deal of care went in to record

00:05:52,470 --> 00:05:57,300
layer to make sure that the act of

00:05:55,350 --> 00:05:58,830
physically opening a database or

00:05:57,300 --> 00:06:01,530
physically opening a record store and

00:05:58,830 --> 00:06:04,979
beginning work on it is extremely fast

00:06:01,530 --> 00:06:06,510
so it can be done in milliseconds so

00:06:04,979 --> 00:06:08,280
what this means is every incoming

00:06:06,510 --> 00:06:10,440
request these servers are just sitting

00:06:08,280 --> 00:06:12,090
out here being arbitrarily handled

00:06:10,440 --> 00:06:14,330
requests they open the database do their

00:06:12,090 --> 00:06:16,950
work close it and they're done

00:06:14,330 --> 00:06:19,889
furthermore all of the requests that we

00:06:16,950 --> 00:06:21,360
currently support are are stateless and

00:06:19,889 --> 00:06:23,070
streaming and what that means is

00:06:21,360 --> 00:06:26,010
currently none of the requests that we

00:06:23,070 --> 00:06:27,810
support will currently do any any and

00:06:26,010 --> 00:06:31,650
memory work that means no in memory

00:06:27,810 --> 00:06:34,250
sorting no in memory joins nothing that

00:06:31,650 --> 00:06:37,230
spills to disk like in memories

00:06:34,250 --> 00:06:39,270
instead we rely very very heavily on

00:06:37,230 --> 00:06:42,060
record layers extensive library of

00:06:39,270 --> 00:06:45,030
indexes to support these operations so

00:06:42,060 --> 00:06:47,820
record layer has text indexes we have

00:06:45,030 --> 00:06:51,230
indexes that support aggregates and

00:06:47,820 --> 00:06:53,760
indexes that support joinings joins and

00:06:51,230 --> 00:06:55,020
but as a result kind of part of this

00:06:53,760 --> 00:06:57,990
kind of streaming and stateless

00:06:55,020 --> 00:07:03,870
architecture is in terms of memory every

00:06:57,990 --> 00:07:06,840
request is very well bounded this goes

00:07:03,870 --> 00:07:09,830
all the way to kind of larger long you

00:07:06,840 --> 00:07:13,230
know larger high scale query operations

00:07:09,830 --> 00:07:16,470
so for example any query that's being

00:07:13,230 --> 00:07:18,840
executed on the server can be stopped at

00:07:16,470 --> 00:07:20,700
any point and what that means is it may

00:07:18,840 --> 00:07:22,680
have made some progress but may not have

00:07:20,700 --> 00:07:24,750
finished and what happens is at that

00:07:22,680 --> 00:07:27,590
point that it gets stopped we return the

00:07:24,750 --> 00:07:29,610
result set along with this opaque

00:07:27,590 --> 00:07:32,300
continuation which is really just an

00:07:29,610 --> 00:07:35,490
encapsulation of the state of the query

00:07:32,300 --> 00:07:38,900
at any point the client can then return

00:07:35,490 --> 00:07:41,460
with this with this continuation and

00:07:38,900 --> 00:07:43,560
pass it back in and it gets routed to an

00:07:41,460 --> 00:07:46,080
arbitrary server in the cluster which

00:07:43,560 --> 00:07:48,450
can continue the query proceeding

00:07:46,080 --> 00:07:53,550
handing back results back and forth

00:07:48,450 --> 00:07:55,350
until the operation is completed as you

00:07:53,550 --> 00:07:57,780
saw the continuation is valid across

00:07:55,350 --> 00:08:00,330
servers it's even valid were you to say

00:07:57,780 --> 00:08:02,700
bounce the entire system between we

00:08:00,330 --> 00:08:04,740
don't but but we're going to bounce the

00:08:02,700 --> 00:08:06,750
entire system between these

00:08:04,740 --> 00:08:08,250
continuations it wouldn't matter as long

00:08:06,750 --> 00:08:13,410
as there's a server to process that it

00:08:08,250 --> 00:08:15,330
will continue to work so we're this is

00:08:13,410 --> 00:08:17,730
where this comes into play when you pull

00:08:15,330 --> 00:08:20,400
all of this together is if you think

00:08:17,730 --> 00:08:23,040
about a request what is the cost of a

00:08:20,400 --> 00:08:24,930
request a standard you know like an

00:08:23,040 --> 00:08:27,150
insert update delete operation they're

00:08:24,930 --> 00:08:30,000
all a fairly finite fixed well-defined

00:08:27,150 --> 00:08:33,390
unit of work well what we've just done

00:08:30,000 --> 00:08:36,000
is we have now broken a query into a

00:08:33,390 --> 00:08:38,340
well-defined unit of work so what we can

00:08:36,000 --> 00:08:40,380
do is we can place limits this is

00:08:38,340 --> 00:08:43,670
another facility of record layer we can

00:08:40,380 --> 00:08:46,020
place limits on how much work a given

00:08:43,670 --> 00:08:46,690
portion of a query is allowed to consume

00:08:46,020 --> 00:08:48,160
whether

00:08:46,690 --> 00:08:50,139
beyond the number of records that have

00:08:48,160 --> 00:08:52,269
been read the number of bytes that have

00:08:50,139 --> 00:08:54,699
been read the the amount of time that's

00:08:52,269 --> 00:08:57,339
passed since the query began so now we

00:08:54,699 --> 00:08:59,139
have a well-defined discrete unit of

00:08:57,339 --> 00:09:02,199
work and so now what that allows us to

00:08:59,139 --> 00:09:04,689
do is the entire rest of the system can

00:09:02,199 --> 00:09:07,540
think about resource management in terms

00:09:04,689 --> 00:09:09,189
of just rate of requests the amount of

00:09:07,540 --> 00:09:11,860
work being done by every request is

00:09:09,189 --> 00:09:14,589
fairly fixed so now we can just simply

00:09:11,860 --> 00:09:16,839
apply rules on how fast Mia may an

00:09:14,589 --> 00:09:19,569
individual user come back with with

00:09:16,839 --> 00:09:21,579
requests how fast can an individual

00:09:19,569 --> 00:09:25,420
application process requests we can have

00:09:21,579 --> 00:09:27,490
all these rules that maintain order and

00:09:25,420 --> 00:09:30,160
maintain stability across the system

00:09:27,490 --> 00:09:33,029
simply by rejecting if we see that a

00:09:30,160 --> 00:09:37,120
resources become or if we see that a

00:09:33,029 --> 00:09:39,339
that a request is exceeding the rate for

00:09:37,120 --> 00:09:40,509
which it is loud we can go tell the we

00:09:39,339 --> 00:09:42,939
can tell the client in this case a

00:09:40,509 --> 00:09:44,379
device like a phone go away and come

00:09:42,939 --> 00:09:50,259
back in just a little bit give us some

00:09:44,379 --> 00:09:53,980
time next I'm going to dive in a little

00:09:50,259 --> 00:09:56,170
bit into how cloud kit how cloud kit

00:09:53,980 --> 00:10:00,790
uses foundation DB and record layer for

00:09:56,170 --> 00:10:04,509
indexing originally cloud kit relied on

00:10:00,790 --> 00:10:06,490
mostly non transactional indexing kind

00:10:04,509 --> 00:10:08,130
of external indexing services and I'll

00:10:06,490 --> 00:10:11,050
kind of drill into that a little bit but

00:10:08,130 --> 00:10:13,209
as most of you probably know by having

00:10:11,050 --> 00:10:15,160
an external non transactional system you

00:10:13,209 --> 00:10:17,529
get lots of strange idiosyncrasies when

00:10:15,160 --> 00:10:20,829
it comes to to handling requests against

00:10:17,529 --> 00:10:22,750
these indexes a foundation DB of course

00:10:20,829 --> 00:10:26,019
gives us transactions transactions give

00:10:22,750 --> 00:10:27,699
this fully consistent indexes and I'm

00:10:26,019 --> 00:10:29,680
going to dive into a couple of the key

00:10:27,699 --> 00:10:34,600
index that the key indexes that we

00:10:29,680 --> 00:10:37,509
utilize today one is what we call our

00:10:34,600 --> 00:10:38,949
sync index so as I said you know syncing

00:10:37,509 --> 00:10:40,180
is really kind of the bread and butter

00:10:38,949 --> 00:10:44,309
it's one of the things that makes

00:10:40,180 --> 00:10:48,970
cloudkit special and particularly useful

00:10:44,309 --> 00:10:51,279
this sink is is implemented via an index

00:10:48,970 --> 00:10:53,350
the index itself is implemented the

00:10:51,279 --> 00:10:55,360
version stamp and I know several people

00:10:53,350 --> 00:10:57,819
have discussed a version stamp previous

00:10:55,360 --> 00:10:58,700
to this but effectively what version

00:10:57,819 --> 00:11:00,740
stamp is it's

00:10:58,700 --> 00:11:02,870
it's foundation DB provides the ability

00:11:00,740 --> 00:11:04,430
to insert a value with just a

00:11:02,870 --> 00:11:07,190
placeholder and what the placeholder

00:11:04,430 --> 00:11:09,050
says is when you commit I want you to

00:11:07,190 --> 00:11:11,600
replace this placeholder with your

00:11:09,050 --> 00:11:13,820
commit version and the commit version is

00:11:11,600 --> 00:11:15,430
is really really useful because it's

00:11:13,820 --> 00:11:21,340
monotonically increasing monotonically

00:11:15,430 --> 00:11:23,900
increasing and it's unique the other

00:11:21,340 --> 00:11:25,970
feature that we leverage when building

00:11:23,900 --> 00:11:28,460
this index is you remember I mentioned

00:11:25,970 --> 00:11:31,580
that record layer is kind of relational

00:11:28,460 --> 00:11:33,200
ish one of the one of the interesting

00:11:31,580 --> 00:11:36,100
things that it's capable of doing is

00:11:33,200 --> 00:11:38,510
defining indexes that span record types

00:11:36,100 --> 00:11:41,090
for example you know theoretically you

00:11:38,510 --> 00:11:43,910
can create an index on the field first

00:11:41,090 --> 00:11:45,350
name and regardless of the record type I

00:11:43,910 --> 00:11:46,520
want to if you've you know if you've got

00:11:45,350 --> 00:11:48,620
a field called first name I'm going to

00:11:46,520 --> 00:11:50,240
index it if I say give me all the

00:11:48,620 --> 00:11:52,280
records that that have a first name of

00:11:50,240 --> 00:11:54,410
Scott I will get all the records of all

00:11:52,280 --> 00:11:56,990
the record types for which there is a

00:11:54,410 --> 00:11:59,750
first name of Scott so by leveraging

00:11:56,990 --> 00:12:02,390
this universal index feature what we can

00:11:59,750 --> 00:12:04,640
do is we can build an index that spans

00:12:02,390 --> 00:12:06,260
say like the modification time or more

00:12:04,640 --> 00:12:08,270
importantly because it spans the commit

00:12:06,260 --> 00:12:12,650
version which really is a modification

00:12:08,270 --> 00:12:14,000
time of all the records so let's let's

00:12:12,650 --> 00:12:17,030
talk about this in action but before I

00:12:14,000 --> 00:12:19,970
do I want to go into a little bit about

00:12:17,030 --> 00:12:22,310
how we used to do it and kind of maybe

00:12:19,970 --> 00:12:24,340
the naive way and actually we used to do

00:12:22,310 --> 00:12:27,800
it before version Stamp was introduced

00:12:24,340 --> 00:12:30,830
was our old index was built on this

00:12:27,800 --> 00:12:32,510
concept of just a change token so every

00:12:30,830 --> 00:12:35,030
record you insert is just we give it a

00:12:32,510 --> 00:12:37,610
unique number a unique increasing number

00:12:35,030 --> 00:12:40,180
called change token so along comes a

00:12:37,610 --> 00:12:42,530
client they need to insert their lemon

00:12:40,180 --> 00:12:45,140
because who doesn't need to insert their

00:12:42,530 --> 00:12:47,840
lemon the first thing they do is they go

00:12:45,140 --> 00:12:50,810
read the next the maximum token we have

00:12:47,840 --> 00:12:53,930
they see that it's four they incremented

00:12:50,810 --> 00:12:56,450
and voila we have we have an order and

00:12:53,930 --> 00:12:58,460
commit maybe we had another device out

00:12:56,450 --> 00:13:01,190
there that the last time they looked at

00:12:58,460 --> 00:13:04,730
their fruit they were at two so they

00:13:01,190 --> 00:13:06,230
knew all about their grapes and they're

00:13:04,730 --> 00:13:08,690
asking for you know what fruit has been

00:13:06,230 --> 00:13:09,800
added since I last looked and voila

00:13:08,690 --> 00:13:13,730
banana

00:13:09,800 --> 00:13:17,899
a strawberry and a lemon I know my fruit

00:13:13,730 --> 00:13:20,959
really well so works fine the problem is

00:13:17,899 --> 00:13:24,170
is he realises all modifications so for

00:13:20,959 --> 00:13:25,910
example let's say we got two people come

00:13:24,170 --> 00:13:27,500
in at the same time we've got your fruit

00:13:25,910 --> 00:13:30,800
they've got your lemon you've got your

00:13:27,500 --> 00:13:32,600
pear and at the same time they both read

00:13:30,800 --> 00:13:35,240
the same the max version they both get

00:13:32,600 --> 00:13:37,730
four they both rate right at five and

00:13:35,240 --> 00:13:40,300
the first one that commits wins the

00:13:37,730 --> 00:13:43,850
second one that commits gets a conflict

00:13:40,300 --> 00:13:45,740
no problem you simply retry the

00:13:43,850 --> 00:13:47,779
transaction and it works

00:13:45,740 --> 00:13:50,959
so this actually served us really really

00:13:47,779 --> 00:13:52,760
well because if you think about our use

00:13:50,959 --> 00:13:55,100
or usage model you know you have a

00:13:52,760 --> 00:13:58,130
database per application so what is the

00:13:55,100 --> 00:13:59,750
actual concurrency in that database it's

00:13:58,130 --> 00:14:00,320
fairly low it's like if you have two

00:13:59,750 --> 00:14:01,940
devices

00:14:00,320 --> 00:14:04,310
it's the odds of those two devices

00:14:01,940 --> 00:14:05,690
trying to write the same set of key or

00:14:04,310 --> 00:14:09,470
trying to write or do a modification

00:14:05,690 --> 00:14:11,180
concurrently but this cloud kit grows

00:14:09,470 --> 00:14:13,220
and evolves or handling larger and

00:14:11,180 --> 00:14:15,529
larger use cases more concurrency on

00:14:13,220 --> 00:14:20,140
devices more concurrency on on users and

00:14:15,529 --> 00:14:23,140
clients and this just doesn't scale so

00:14:20,140 --> 00:14:26,240
everything is better with version stamp

00:14:23,140 --> 00:14:28,070
so now what happens when we bring in the

00:14:26,240 --> 00:14:29,570
idea version stamp so now when our two

00:14:28,070 --> 00:14:31,540
clients come in and they decide they

00:14:29,570 --> 00:14:34,670
want to insert their respective fruits

00:14:31,540 --> 00:14:36,589
what they're insert becomes is insert a

00:14:34,670 --> 00:14:39,560
key of version stamp which like I said

00:14:36,589 --> 00:14:43,310
it's just this opaque thing and your

00:14:39,560 --> 00:14:45,620
fruit and then at the time one of the

00:14:43,310 --> 00:14:48,500
transactions is committed they are

00:14:45,620 --> 00:14:50,540
assigned the commit version that fbb

00:14:48,500 --> 00:14:52,760
assigns it's guaranteed to be unique

00:14:50,540 --> 00:14:54,649
it's guaranteed to be monotonically

00:14:52,760 --> 00:14:57,350
increasing and thus we have a

00:14:54,649 --> 00:14:59,959
consistently ordered as consistently

00:14:57,350 --> 00:15:06,920
ordered index of modifications that's

00:14:59,959 --> 00:15:09,230
also free from conflicts there are some

00:15:06,920 --> 00:15:11,089
really interesting aspects about this

00:15:09,230 --> 00:15:12,890
I'll highlight some of the the the

00:15:11,089 --> 00:15:16,190
interesting challenges but I can't

00:15:12,890 --> 00:15:18,709
really go into it in a 20 minute talk so

00:15:16,190 --> 00:15:19,910
first is how you know how do you

00:15:18,709 --> 00:15:20,279
implement deletes you know how do you

00:15:19,910 --> 00:15:21,720
have

00:15:20,279 --> 00:15:23,249
deletes and updates in this world

00:15:21,720 --> 00:15:24,839
because you don't want to remove the

00:15:23,249 --> 00:15:28,490
record you want you know you would like

00:15:24,839 --> 00:15:31,740
to maintain a forward view of changes

00:15:28,490 --> 00:15:33,779
how do you move data between clusters so

00:15:31,740 --> 00:15:36,540
that commit version is unique per

00:15:33,779 --> 00:15:38,579
cluster so if you just that that

00:15:36,540 --> 00:15:41,550
machinery I showed moving users from one

00:15:38,579 --> 00:15:44,100
cluster to another if you just move that

00:15:41,550 --> 00:15:46,199
data as is you get this interesting

00:15:44,100 --> 00:15:50,850
problem where the commit version on the

00:15:46,199 --> 00:15:53,279
target cluster might be logically before

00:15:50,850 --> 00:15:55,019
the data you just moved so if the user

00:15:53,279 --> 00:15:57,029
begins doing new inserts they will

00:15:55,019 --> 00:15:58,980
actually be placing data earlier in the

00:15:57,029 --> 00:16:01,829
index then at the tail of the index so

00:15:58,980 --> 00:16:04,439
how do you deal with that and further we

00:16:01,829 --> 00:16:05,639
have we have this old implementation and

00:16:04,439 --> 00:16:07,410
we needed to switch this new

00:16:05,639 --> 00:16:09,449
implementation without shutting

00:16:07,410 --> 00:16:11,970
everything down so how do you how do you

00:16:09,449 --> 00:16:15,089
do this change without disrupting the

00:16:11,970 --> 00:16:16,740
the client behavior so all of this is in

00:16:15,089 --> 00:16:18,389
the record layer paper I'll be talking

00:16:16,740 --> 00:16:22,860
I'll have a link to that at the end of

00:16:18,389 --> 00:16:25,740
the presentation but next I just wanted

00:16:22,860 --> 00:16:28,980
to move on to text indexes so I

00:16:25,740 --> 00:16:30,629
mentioned you know we had our indexing

00:16:28,980 --> 00:16:34,680
used to rely on kind of external

00:16:30,629 --> 00:16:37,759
indexing services like solar this has

00:16:34,680 --> 00:16:40,079
the problem that there is some lag so

00:16:37,759 --> 00:16:41,550
that you know when you when you write a

00:16:40,079 --> 00:16:43,680
record you might have a slight delay

00:16:41,550 --> 00:16:45,149
between the record when the record is

00:16:43,680 --> 00:16:47,879
available in the index so for a brief

00:16:45,149 --> 00:16:50,220
period you may not get results or even

00:16:47,879 --> 00:16:51,689
worse you might end up out of sync so if

00:16:50,220 --> 00:16:52,889
something goes wrong between the time

00:16:51,689 --> 00:16:54,569
you wrote the record and the time you

00:16:52,889 --> 00:16:58,379
indexed it you may not get an index

00:16:54,569 --> 00:17:01,350
entry at all obviously Foundation DB

00:16:58,379 --> 00:17:03,660
transactions everything's consistent but

00:17:01,350 --> 00:17:05,939
in order to do that we needed a text

00:17:03,660 --> 00:17:07,740
index so one of the features that was

00:17:05,939 --> 00:17:10,350
added to record layer over I guess the

00:17:07,740 --> 00:17:12,720
last year was was the presence of this

00:17:10,350 --> 00:17:15,750
this text index which is again fully

00:17:12,720 --> 00:17:16,949
transactional low overhead and it's also

00:17:15,750 --> 00:17:19,020
interesting to think of it as sort of

00:17:16,949 --> 00:17:20,789
like a personalized index so typically

00:17:19,020 --> 00:17:23,659
with a text indexing scheme you have a

00:17:20,789 --> 00:17:26,399
big indexing service sitting out there

00:17:23,659 --> 00:17:30,000
managing you know I managing the world

00:17:26,399 --> 00:17:31,950
in this environment again a record store

00:17:30,000 --> 00:17:32,940
is completely self-contained which means

00:17:31,950 --> 00:17:34,860
you have your text

00:17:32,940 --> 00:17:37,890
totally self-contained alongside the

00:17:34,860 --> 00:17:40,140
data so it's fairly inexpensive it moves

00:17:37,890 --> 00:17:41,490
with the data it's managed with with all

00:17:40,140 --> 00:17:46,830
the rest of the data it's super

00:17:41,490 --> 00:17:48,330
convenient and the last thing I'll touch

00:17:46,830 --> 00:17:50,600
on is just kind of like what our

00:17:48,330 --> 00:17:54,360
experience has been with foundation dB

00:17:50,600 --> 00:17:57,450
obviously you know foundation DB came

00:17:54,360 --> 00:17:59,490
that was open sourced out of apple we

00:17:57,450 --> 00:18:01,230
didn't open source it because we do

00:17:59,490 --> 00:18:03,630
because we're through with it we open

00:18:01,230 --> 00:18:08,070
sourced it because it's great and you

00:18:03,630 --> 00:18:09,750
should use it it's obviously it's it's

00:18:08,070 --> 00:18:13,020
the main development focus of it is

00:18:09,750 --> 00:18:15,179
testability and simulation because of

00:18:13,020 --> 00:18:17,100
this it tends not to break bugs tend to

00:18:15,179 --> 00:18:20,429
get caught during development and not

00:18:17,100 --> 00:18:23,640
deployment there's extensive testing of

00:18:20,429 --> 00:18:25,260
kind of cross version upgrades what

00:18:23,640 --> 00:18:28,770
happens when you're upgrading this thing

00:18:25,260 --> 00:18:30,120
live as you know with making sure that

00:18:28,770 --> 00:18:33,260
every year that the old version of the

00:18:30,120 --> 00:18:36,360
new version will fully compatible and

00:18:33,260 --> 00:18:37,919
cloudkit is built on a bunch of

00:18:36,360 --> 00:18:40,169
different technologies a bunch of

00:18:37,919 --> 00:18:43,559
different services and it is a

00:18:40,169 --> 00:18:46,890
tremendous testament to the quality of

00:18:43,559 --> 00:18:49,500
testing and foundation DB that we can

00:18:46,890 --> 00:18:51,030
take a release of foundation DB have it

00:18:49,500 --> 00:18:53,340
rolled in production rolled into

00:18:51,030 --> 00:18:55,409
production in a matter of weeks rather

00:18:53,340 --> 00:18:58,289
than a matter of years of what you know

00:18:55,409 --> 00:18:59,909
of testing and fixing problems and we

00:18:58,289 --> 00:19:08,159
you can have a great deal of confidence

00:18:59,909 --> 00:19:12,210
that it will just work record layer and

00:19:08,159 --> 00:19:13,350
foundation DB are awesome I'm not just

00:19:12,210 --> 00:19:16,830
saying that because it came out of my

00:19:13,350 --> 00:19:18,480
team but I highly encourage everybody to

00:19:16,830 --> 00:19:22,350
use it if you haven't if you aren't

00:19:18,480 --> 00:19:23,970
already we have a number of papers so

00:19:22,350 --> 00:19:25,770
actually only list one of them here but

00:19:23,970 --> 00:19:28,620
this is the one very specific about

00:19:25,770 --> 00:19:31,530
record layer as I said at 3:40 there's a

00:19:28,620 --> 00:19:34,409
talk from another member of my team

00:19:31,530 --> 00:19:37,500
Nicholas I highly encourage you to go go

00:19:34,409 --> 00:19:40,289
check that out and please get involved

00:19:37,500 --> 00:19:42,800
we get involved with foundation BB

00:19:40,289 --> 00:19:45,320
getting involved with record layer

00:19:42,800 --> 00:19:48,500
we would we would love to see see the

00:19:45,320 --> 00:19:52,870
support from the community on this and

00:19:48,500 --> 00:19:52,870

YouTube URL: https://www.youtube.com/watch?v=SvoUHHM9IKU


