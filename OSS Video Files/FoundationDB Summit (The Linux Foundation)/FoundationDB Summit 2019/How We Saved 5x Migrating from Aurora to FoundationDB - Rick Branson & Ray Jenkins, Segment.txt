Title: How We Saved 5x Migrating from Aurora to FoundationDB - Rick Branson & Ray Jenkins, Segment
Publication date: 2019-11-25
Playlist: FoundationDB Summit 2019
Description: 
	How We Saved 5x Migrating from Aurora to FoundationDB - Rick Branson & Ray Jenkins, SegmentÂ 

Segment is on a journey to help companies better understand their customers. A key piece of the Segment solution involves near real-time translation of customer interactions into models of customer Personas. This is the story of how Segment selected FoundationDB to power a critical part of this infrastructure, the identity resolution system.

In this talk, we'll discuss how and why we selected FoundationDB. We'll cover Segment's identity resolution system architecture and how FoundationDB has enabled Segment to deliver Personas modeling at a 5x cost reduction. Finally, we'll discuss the benefits of migrating away from AWS Aurora and the specific operational challenges we faced and how we overcame them.
Captions: 
	00:00:00,230 --> 00:00:05,279
no notes so we're gonna do this without

00:00:02,370 --> 00:00:07,350
presenter notes so I don't know how well

00:00:05,279 --> 00:00:09,240
this is gonna go but anyway thank you

00:00:07,350 --> 00:00:10,769
for your patience

00:00:09,240 --> 00:00:13,410
my name is Rick I'm here with my

00:00:10,769 --> 00:00:15,839
esteemed colleague ray we work on the

00:00:13,410 --> 00:00:18,060
engineering team at segment we're going

00:00:15,839 --> 00:00:21,260
to talk about how we saved a ton of

00:00:18,060 --> 00:00:24,300
money migrating from Aurora which is

00:00:21,260 --> 00:00:27,590
nice but expensive to foundation DB

00:00:24,300 --> 00:00:30,660
which is quite efficient and also nice

00:00:27,590 --> 00:00:32,160
yeah they talk about a few things first

00:00:30,660 --> 00:00:34,530
we're gonna talk about what segments

00:00:32,160 --> 00:00:36,660
does since that seems to be a question

00:00:34,530 --> 00:00:38,399
many people have we're going to talk

00:00:36,660 --> 00:00:40,410
about the use case of identity

00:00:38,399 --> 00:00:41,820
resolution we're going to talk about the

00:00:40,410 --> 00:00:45,840
data model we use on top of foundation

00:00:41,820 --> 00:00:48,840
DB then Ray is going to talk about how

00:00:45,840 --> 00:00:51,030
foundation DB saves us money how we got

00:00:48,840 --> 00:00:53,579
to production and some potential future

00:00:51,030 --> 00:00:57,090
use cases for foundation dB at segments

00:00:53,579 --> 00:00:59,609
so what is segment segment is a platform

00:00:57,090 --> 00:01:02,850
that you use to instrument your mobile

00:00:59,609 --> 00:01:04,769
apps your websites to get data from

00:01:02,850 --> 00:01:07,110
those about how your customers are using

00:01:04,769 --> 00:01:09,840
those products and forward them through

00:01:07,110 --> 00:01:11,939
segments to standardize them to make

00:01:09,840 --> 00:01:14,250
sure they're clean of PII and afford

00:01:11,939 --> 00:01:16,770
that information on to downstream

00:01:14,250 --> 00:01:18,000
analytics tools a/b testing tools mail

00:01:16,770 --> 00:01:21,600
campaign management tools

00:01:18,000 --> 00:01:23,100
lots of stuff so the idea is segment

00:01:21,600 --> 00:01:26,700
delivers these amazing customer

00:01:23,100 --> 00:01:28,619
experiences this is common we see people

00:01:26,700 --> 00:01:30,860
using tools that connect things all over

00:01:28,619 --> 00:01:34,259
the place there's a lot of inconsistency

00:01:30,860 --> 00:01:36,240
data is going to the wrong place it's

00:01:34,259 --> 00:01:37,799
it's full of stuff like PII where it

00:01:36,240 --> 00:01:39,720
doesn't need to be

00:01:37,799 --> 00:01:42,780
you may recognize some logos here I hope

00:01:39,720 --> 00:01:45,090
that this isn't offending anybody for

00:01:42,780 --> 00:01:46,530
some reason we call this and we call

00:01:45,090 --> 00:01:48,750
this CDI which is customer data

00:01:46,530 --> 00:01:49,890
infrastructure now this is kind of a you

00:01:48,750 --> 00:01:51,990
know we have a few slides that are kind

00:01:49,890 --> 00:01:53,310
of from our marketing team but they help

00:01:51,990 --> 00:01:54,710
kind of illustrate like what we do

00:01:53,310 --> 00:01:58,560
because it's kind of complicated

00:01:54,710 --> 00:02:00,000
so the idea is we provide this single

00:01:58,560 --> 00:02:02,549
point of customer understanding in

00:02:00,000 --> 00:02:04,380
action we help you synthesize your data

00:02:02,549 --> 00:02:06,719
we help you standardize it so clean it

00:02:04,380 --> 00:02:09,509
up make sure it's all in the same same

00:02:06,719 --> 00:02:12,540
format and then collect all that from

00:02:09,509 --> 00:02:13,420
various endpoints from you the Roku box

00:02:12,540 --> 00:02:16,660
sitting on top

00:02:13,420 --> 00:02:19,989
TV and then power all the same tools

00:02:16,660 --> 00:02:21,370
from that same data this kind of a slide

00:02:19,989 --> 00:02:23,590
you can kind of see this is this has

00:02:21,370 --> 00:02:26,440
nothing to do with actual reality this

00:02:23,590 --> 00:02:29,500
is just how the thing works conceptually

00:02:26,440 --> 00:02:31,350
data flows from the sources we clean and

00:02:29,500 --> 00:02:34,270
synthesize it and push it out to

00:02:31,350 --> 00:02:36,130
destinations so this is kind of a

00:02:34,270 --> 00:02:37,150
timeline this is only really inserted

00:02:36,130 --> 00:02:40,930
here to kind of give you an idea of

00:02:37,150 --> 00:02:42,670
where we are relative to front asian DBS

00:02:40,930 --> 00:02:43,750
open source release in 2018 which is

00:02:42,670 --> 00:02:46,269
where we started to really be interested

00:02:43,750 --> 00:02:47,290
in using foundation DB so around the

00:02:46,269 --> 00:02:50,739
time that we launched this product

00:02:47,290 --> 00:02:52,989
called personas you see in 2017 we built

00:02:50,739 --> 00:02:54,160
this thing on top of rora and then we

00:02:52,989 --> 00:02:56,950
started to look at foundation DB

00:02:54,160 --> 00:02:58,540
afterwards so what is identity

00:02:56,950 --> 00:03:01,690
resolution this is the use case for

00:02:58,540 --> 00:03:03,970
foundation dB at segments it's kind of

00:03:01,690 --> 00:03:06,970
in the center here what we do is we try

00:03:03,970 --> 00:03:10,360
to build profiles of users that are that

00:03:06,970 --> 00:03:11,920
are using our customers products and

00:03:10,360 --> 00:03:14,080
that often means that they have various

00:03:11,920 --> 00:03:17,920
identities across the product that we

00:03:14,080 --> 00:03:19,840
want to unify into a single picture and

00:03:17,920 --> 00:03:22,329
so what we do is we build an aggregated

00:03:19,840 --> 00:03:24,910
graph of data associated with a unique

00:03:22,329 --> 00:03:27,130
user and this helps simplify analytics

00:03:24,910 --> 00:03:30,519
and provides a unified view in your

00:03:27,130 --> 00:03:33,220
downstream tools so a common problem you

00:03:30,519 --> 00:03:36,220
might see is a user logos to a website

00:03:33,220 --> 00:03:38,560
or opens an email and then they're

00:03:36,220 --> 00:03:40,780
redirected to a website and all we know

00:03:38,560 --> 00:03:42,880
about you is some random ID we've

00:03:40,780 --> 00:03:46,299
generated and given in your cookie so

00:03:42,880 --> 00:03:49,090
you can see as I did anonymous ID you

00:03:46,299 --> 00:03:50,709
might have opened an email and through

00:03:49,090 --> 00:03:53,049
the data that we collect over time we're

00:03:50,709 --> 00:03:55,170
able to basically build a unified ID you

00:03:53,049 --> 00:03:58,060
may log in you may complete a

00:03:55,170 --> 00:04:00,100
transaction and all of those events have

00:03:58,060 --> 00:04:01,390
different identifiers associated with

00:04:00,100 --> 00:04:04,390
them and the goal is to basically

00:04:01,390 --> 00:04:06,310
provide a unique ID that connect

00:04:04,390 --> 00:04:09,819
connects all those other identifiers

00:04:06,310 --> 00:04:10,780
together so we do this using two

00:04:09,819 --> 00:04:12,670
different types of data

00:04:10,780 --> 00:04:15,670
one we call mappings and one we call

00:04:12,670 --> 00:04:17,200
merges and a mapping is what we do to

00:04:15,670 --> 00:04:19,269
connect an external like what we call an

00:04:17,200 --> 00:04:24,400
external ID like an email address or a

00:04:19,269 --> 00:04:27,250
database ID various other things to what

00:04:24,400 --> 00:04:29,500
we call a segment idea that universal

00:04:27,250 --> 00:04:31,630
so right here in the example these

00:04:29,500 --> 00:04:34,960
external IDs would be this user ID 8y

00:04:31,630 --> 00:04:37,180
etc the email is another external ID

00:04:34,960 --> 00:04:39,970
slot that segment comm and we're trying

00:04:37,180 --> 00:04:42,640
to map those all to this universal ID

00:04:39,970 --> 00:04:44,110
use one two three now this comes with a

00:04:42,640 --> 00:04:46,480
problem which is that we generate these

00:04:44,110 --> 00:04:49,330
new IDs these new quote-unquote

00:04:46,480 --> 00:04:51,070
Universal IDs all the time and we want

00:04:49,330 --> 00:04:53,200
to be able to add a future time merge

00:04:51,070 --> 00:04:55,240
those together when we realize they're

00:04:53,200 --> 00:04:57,760
actually connected via some other

00:04:55,240 --> 00:05:01,150
element of data so that once we call a

00:04:57,760 --> 00:05:02,470
merge now let's talk about what this

00:05:01,150 --> 00:05:03,480
actually looks like on top of foundation

00:05:02,470 --> 00:05:07,150
dB

00:05:03,480 --> 00:05:10,210
so we don't use any real mapping layer

00:05:07,150 --> 00:05:12,910
we decided to go directly to the Cavey

00:05:10,210 --> 00:05:14,440
store some of that is because we use all

00:05:12,910 --> 00:05:16,830
of our services are written and go and

00:05:14,440 --> 00:05:19,240
there isn't a ton of layer layer

00:05:16,830 --> 00:05:22,390
implementations for go so we directly

00:05:19,240 --> 00:05:24,370
use the Cavey interface we use substance

00:05:22,390 --> 00:05:27,130
we use subspaces to structure our keys

00:05:24,370 --> 00:05:30,010
and we have two types of Cavey pairs so

00:05:27,130 --> 00:05:34,690
we call Cavey pairs objects which are

00:05:30,010 --> 00:05:36,790
basically surrogate IDs keys to jason

00:05:34,690 --> 00:05:39,670
serialized objects and indexes which

00:05:36,790 --> 00:05:42,340
index those objects you know based on

00:05:39,670 --> 00:05:44,680
the fields that are in those objects so

00:05:42,340 --> 00:05:47,740
again we do mostly point gets on these

00:05:44,680 --> 00:05:49,060
object KB types values adjacent

00:05:47,740 --> 00:05:51,310
serialized object and when I say

00:05:49,060 --> 00:05:52,690
surrogate key I mean just a generated

00:05:51,310 --> 00:05:55,960
key we used it doesn't really mean

00:05:52,690 --> 00:05:58,450
anything outside of our system we also

00:05:55,960 --> 00:06:00,520
have these index Cavey kinds we use the

00:05:58,450 --> 00:06:03,040
sub sub spaces to segment the compound

00:06:00,520 --> 00:06:04,950
identifier we generally look up by key

00:06:03,040 --> 00:06:07,690
prefix so we use get ranges for these

00:06:04,950 --> 00:06:09,460
and then the suffix of the key is the

00:06:07,690 --> 00:06:11,470
actual target the thing that we're

00:06:09,460 --> 00:06:14,380
mapping to so these have the stuff the

00:06:11,470 --> 00:06:17,950
prefix will have multiple targets that

00:06:14,380 --> 00:06:20,680
it's identifying in the system and the

00:06:17,950 --> 00:06:22,270
value is usually empty so this is kind

00:06:20,680 --> 00:06:23,440
of what it looks like everything is

00:06:22,270 --> 00:06:26,020
prefixed by tenant because we're a

00:06:23,440 --> 00:06:28,510
multi-tenant system this is one of them

00:06:26,020 --> 00:06:30,310
like be I did identifier type to an

00:06:28,510 --> 00:06:34,000
identifier this would be a range prefix

00:06:30,310 --> 00:06:38,770
and it's connected to this mapping ID JJ

00:06:34,000 --> 00:06:40,150
- LK etc garbage so let's talk about

00:06:38,770 --> 00:06:41,920
this first type of

00:06:40,150 --> 00:06:43,480
data which is mapping data so this

00:06:41,920 --> 00:06:47,890
connects an external ID like Rick at

00:06:43,480 --> 00:06:49,360
segment comm to a unified segment ID all

00:06:47,890 --> 00:06:51,670
right so this is kind of what it looks

00:06:49,360 --> 00:06:54,340
like it's very simple we have a tenant

00:06:51,670 --> 00:06:55,840
prefix and a mapping ID connects to the

00:06:54,340 --> 00:07:00,100
JSON object with the information in it

00:06:55,840 --> 00:07:04,000
very simple sometimes we want to list

00:07:00,100 --> 00:07:05,830
all of these we run various reports and

00:07:04,000 --> 00:07:07,030
things on these so say you go to your

00:07:05,830 --> 00:07:09,580
interface you want to see all of these

00:07:07,030 --> 00:07:12,730
mappings clearly people want to do that

00:07:09,580 --> 00:07:14,310
for some reason and so you can actually

00:07:12,730 --> 00:07:18,930
we have a big index of all these

00:07:14,310 --> 00:07:22,000
mappings that that is arranged by tenant

00:07:18,930 --> 00:07:24,910
we also have an ID index for the segment

00:07:22,000 --> 00:07:26,620
ID field so you can see we can go search

00:07:24,910 --> 00:07:28,420
for specifically for a segment ID and

00:07:26,620 --> 00:07:33,220
point directly at a mapping ID there may

00:07:28,420 --> 00:07:35,410
be multiple of one or the other okay so

00:07:33,220 --> 00:07:37,630
merge data so this is what when we

00:07:35,410 --> 00:07:39,400
realize we finally realized hey there's

00:07:37,630 --> 00:07:41,650
actually multiple mappings that connect

00:07:39,400 --> 00:07:43,420
to the same ID we need to bring these

00:07:41,650 --> 00:07:45,700
together we create what's called a merge

00:07:43,420 --> 00:07:48,750
entry and those associate to

00:07:45,700 --> 00:07:51,540
quote-unquote unified segment IDs so

00:07:48,750 --> 00:07:54,490
this is what emerge object looks like

00:07:51,540 --> 00:07:57,370
similar to the mapping it's a tenant

00:07:54,490 --> 00:08:00,190
prefix with a merge ID that's generated

00:07:57,370 --> 00:08:01,330
surrogate key and generally there's some

00:08:00,190 --> 00:08:02,770
other fields in here there's some time

00:08:01,330 --> 00:08:04,330
stamps etc but the only things that

00:08:02,770 --> 00:08:06,880
really matter is it's a good graph

00:08:04,330 --> 00:08:10,360
relationship it's a prominent - and

00:08:06,880 --> 00:08:13,540
these are the segment IDs so we have an

00:08:10,360 --> 00:08:15,700
index the from index very simple it's

00:08:13,540 --> 00:08:19,240
the from field to the merge ID and a to

00:08:15,700 --> 00:08:21,250
index a to field to the merge ID very

00:08:19,240 --> 00:08:22,510
straightforward all right I'm gonna go

00:08:21,250 --> 00:08:24,790
to my friend ray and he's gonna explain

00:08:22,510 --> 00:08:32,050
the rest of the gory details of the

00:08:24,790 --> 00:08:35,170
system thank you okay cool yeah so I'm

00:08:32,050 --> 00:08:39,250
gonna talk a little bit more about sort

00:08:35,170 --> 00:08:41,440
of the path to production and a little

00:08:39,250 --> 00:08:42,850
bit more about operating the system in

00:08:41,440 --> 00:08:46,660
production and then some future use

00:08:42,850 --> 00:08:49,240
cases so these are essentially some of

00:08:46,660 --> 00:08:50,520
the needs of this this workload as you

00:08:49,240 --> 00:08:53,280
can imagine

00:08:50,520 --> 00:08:56,130
a high throughput sort of workload it's

00:08:53,280 --> 00:09:00,780
we project the system needs hundreds of

00:08:56,130 --> 00:09:02,730
terabytes of space on disk replicated

00:09:00,780 --> 00:09:04,380
it's not that large right now it's more

00:09:02,730 --> 00:09:07,080
in the order of tens of terabytes but

00:09:04,380 --> 00:09:08,900
it's rapidly growing and of course the

00:09:07,080 --> 00:09:12,450
key pieces we need serializable

00:09:08,900 --> 00:09:15,420
isolation levels it's also it's a very

00:09:12,450 --> 00:09:16,620
high volume sort of a read workload so

00:09:15,420 --> 00:09:18,030
the read volume is much higher the

00:09:16,620 --> 00:09:20,430
rights than the writes as you'll see in

00:09:18,030 --> 00:09:22,560
a moment so a little bit about how data

00:09:20,430 --> 00:09:25,050
flows through the system so the identity

00:09:22,560 --> 00:09:26,850
resolution system sort of is teed-off of

00:09:25,050 --> 00:09:30,240
a stream processing system that handles

00:09:26,850 --> 00:09:32,790
the core data ingestion and then routing

00:09:30,240 --> 00:09:34,860
that data to all the destinations that

00:09:32,790 --> 00:09:36,900
we integrate with so data comes in

00:09:34,860 --> 00:09:39,420
starting from the left-hand side of this

00:09:36,900 --> 00:09:42,150
graph data comes into our API and into

00:09:39,420 --> 00:09:43,920
our core ingestion pipeline there's a

00:09:42,150 --> 00:09:45,330
lot of systems not featured there but

00:09:43,920 --> 00:09:48,330
we're just focusing on the identity

00:09:45,330 --> 00:09:49,830
service here part so there when it first

00:09:48,330 --> 00:09:51,690
comes into the ingestion pipeline we

00:09:49,830 --> 00:09:54,570
stored in Kafka and then we have some

00:09:51,690 --> 00:09:57,420
systems basically performing validation

00:09:54,570 --> 00:10:00,240
schema enforcement things like that any

00:09:57,420 --> 00:10:01,440
customers who are using persona that

00:10:00,240 --> 00:10:04,770
data is then teed-off

00:10:01,440 --> 00:10:05,880
into the identity ingress topic and then

00:10:04,770 --> 00:10:09,540
we have a service called the identity

00:10:05,880 --> 00:10:13,140
resolver that service scales in and out

00:10:09,540 --> 00:10:15,720
horizontally as needed depending on the

00:10:13,140 --> 00:10:19,740
volume of traffic and that queries the

00:10:15,720 --> 00:10:22,280
identity service right path essentially

00:10:19,740 --> 00:10:24,420
on the top left there so that is where

00:10:22,280 --> 00:10:26,150
for every message that's coming in

00:10:24,420 --> 00:10:28,850
essentially we're going to foundation DB

00:10:26,150 --> 00:10:31,200
looking to see if we have seen this

00:10:28,850 --> 00:10:33,660
identity before this piece of data in

00:10:31,200 --> 00:10:35,610
creating the mappings and merging as

00:10:33,660 --> 00:10:38,370
necessary on the bottom half we also

00:10:35,610 --> 00:10:40,590
have a read-only path essentially so

00:10:38,370 --> 00:10:42,450
that's just read-only requests the

00:10:40,590 --> 00:10:45,420
foundation DB for other systems

00:10:42,450 --> 00:10:49,620
downstream that are looking for

00:10:45,420 --> 00:10:51,360
identities ok so as we mentioned we

00:10:49,620 --> 00:10:53,940
originally built the system with a rora

00:10:51,360 --> 00:10:56,370
and it actually worked pretty well there

00:10:53,940 --> 00:10:58,710
was there are some nuisances for the

00:10:56,370 --> 00:11:00,300
developers and foundation DB is actually

00:10:58,710 --> 00:11:01,930
a bit better like one of the problems is

00:11:00,300 --> 00:11:04,630
you might do a read from

00:11:01,930 --> 00:11:06,279
replicas and then determine oh I need to

00:11:04,630 --> 00:11:08,860
go insert a record then you go to the

00:11:06,279 --> 00:11:10,510
master and go to insert something and

00:11:08,860 --> 00:11:12,010
another message had already come in and

00:11:10,510 --> 00:11:13,480
inserted that so then you have to go

00:11:12,010 --> 00:11:15,940
back it was sort of a bit of juggling

00:11:13,480 --> 00:11:19,510
but Aurora worked but the primary

00:11:15,940 --> 00:11:22,930
problem was the cost so this is just

00:11:19,510 --> 00:11:24,700
like some baseline cost for Aurora just

00:11:22,930 --> 00:11:26,950
they like get an idea of the back of the

00:11:24,700 --> 00:11:31,120
napkin math we did when we started to

00:11:26,950 --> 00:11:33,070
consider foundation DB so basically a

00:11:31,120 --> 00:11:35,050
terabyte and a billion I Alps a month

00:11:33,070 --> 00:11:38,980
it's not too expensive it's it's about a

00:11:35,050 --> 00:11:41,649
k' with a single instance but if you can

00:11:38,980 --> 00:11:46,149
imagine even like a medium-sized work

00:11:41,649 --> 00:11:47,740
load that that basically grows to tens

00:11:46,149 --> 00:11:50,260
of thousands of dollars really quickly

00:11:47,740 --> 00:11:52,810
and this is only like 20 terabytes and a

00:11:50,260 --> 00:11:56,100
hundred billion I office a month you're

00:11:52,810 --> 00:12:00,370
looking at tens of thousands of dollars

00:11:56,100 --> 00:12:02,620
so you can essentially basically

00:12:00,370 --> 00:12:04,240
provision we determined that we were we

00:12:02,620 --> 00:12:06,880
could run essentially the same workload

00:12:04,240 --> 00:12:10,570
on foundation DB for about a fifth or a

00:12:06,880 --> 00:12:14,170
six of the cost so roughly with the

00:12:10,570 --> 00:12:16,000
resources here so 20 I three extra large

00:12:14,170 --> 00:12:17,290
instances some stateless and some

00:12:16,000 --> 00:12:20,800
transaction notes

00:12:17,290 --> 00:12:23,410
okay so a little bit about our path from

00:12:20,800 --> 00:12:25,209
prototype to production so prototype was

00:12:23,410 --> 00:12:28,060
was pretty quick we were able to stand

00:12:25,209 --> 00:12:29,790
it up pretty fast but we hit a couple

00:12:28,060 --> 00:12:32,380
like hurdles on the way to production

00:12:29,790 --> 00:12:35,080
the first one was everyone's favorite

00:12:32,380 --> 00:12:36,310
game and so we weren't exactly sure we

00:12:35,080 --> 00:12:38,350
weren't getting the sort of performance

00:12:36,310 --> 00:12:39,990
that we thought we would get based off

00:12:38,350 --> 00:12:42,130
of some of the benchmarks we saw online

00:12:39,990 --> 00:12:44,529
and we weren't sure was it something we

00:12:42,130 --> 00:12:47,050
did wrong you know where systems under

00:12:44,529 --> 00:12:49,180
provisioned it turned out was mostly us

00:12:47,050 --> 00:12:51,070
doing things wrong we weren't like we're

00:12:49,180 --> 00:12:53,529
basically not using the API correctly we

00:12:51,070 --> 00:12:55,930
weren't pipelining requests things like

00:12:53,529 --> 00:12:58,420
that so we quickly we quickly improved

00:12:55,930 --> 00:13:00,550
performance but there was some things

00:12:58,420 --> 00:13:02,589
that was confusing so for example like

00:13:00,550 --> 00:13:04,630
the default recruitment sort of confused

00:13:02,589 --> 00:13:07,720
us we found the tuning cookbook and that

00:13:04,630 --> 00:13:09,940
sort of led us down a path of like

00:13:07,720 --> 00:13:11,680
evolving our cluster configuration so

00:13:09,940 --> 00:13:15,190
like when we started we saw this this is

00:13:11,680 --> 00:13:17,110
like CPU utilization with the default

00:13:15,190 --> 00:13:18,910
basically settings with everything unset

00:13:17,110 --> 00:13:20,350
and you see CPUs sort of all over the

00:13:18,910 --> 00:13:21,990
place it was a little bit difficult to

00:13:20,350 --> 00:13:25,509
reason about what was sort of going on

00:13:21,990 --> 00:13:27,399
so we essentially came up with a

00:13:25,509 --> 00:13:28,779
configuration we went through a little

00:13:27,399 --> 00:13:30,519
bit of an evolution but we came up with

00:13:28,779 --> 00:13:32,680
a configuration and ended up with a

00:13:30,519 --> 00:13:36,040
heterogeneous three tiers sort of

00:13:32,680 --> 00:13:39,310
cluster setup so we ran we run a

00:13:36,040 --> 00:13:42,459
stateless cluster a transaction cluster

00:13:39,310 --> 00:13:44,529
in a storage cluster actually sorry ASG

00:13:42,459 --> 00:13:49,600
auto scaling group we're running all

00:13:44,529 --> 00:13:51,339
this in AWS but that allows us to run c5

00:13:49,600 --> 00:13:54,660
instances for the higher compute

00:13:51,339 --> 00:13:58,569
workloads and then we run high three

00:13:54,660 --> 00:14:00,430
instances with nvme storage for the the

00:13:58,569 --> 00:14:03,339
storage and the transaction tiers and

00:14:00,430 --> 00:14:05,399
that allows us to scale these these

00:14:03,339 --> 00:14:08,230
clusters independently essentially it's

00:14:05,399 --> 00:14:09,970
a little bit about how we get this all

00:14:08,230 --> 00:14:12,339
sort of provisioned in production so we

00:14:09,970 --> 00:14:13,990
use terraform we sort of set out to then

00:14:12,339 --> 00:14:15,430
so after we solved some of the

00:14:13,990 --> 00:14:17,319
performance issues and we were we were

00:14:15,430 --> 00:14:20,230
we were getting the results we expected

00:14:17,319 --> 00:14:23,740
we sort of started template izing the

00:14:20,230 --> 00:14:25,630
infrastructure we run as I said three

00:14:23,740 --> 00:14:27,639
auto scaling groups and then we run a

00:14:25,630 --> 00:14:30,579
container orchestration system on top of

00:14:27,639 --> 00:14:32,949
that we'll talk about that in a moment

00:14:30,579 --> 00:14:34,449
and we run a bunch of supporting tools

00:14:32,949 --> 00:14:36,189
that we wrote in-house and we hope to

00:14:34,449 --> 00:14:38,980
actually open source and share with the

00:14:36,189 --> 00:14:41,740
community community on top of the

00:14:38,980 --> 00:14:44,050
instances as well we run the FTP server

00:14:41,740 --> 00:14:47,290
process directly on the boxes I think we

00:14:44,050 --> 00:14:49,230
were uh Neph TB monitor as well just a

00:14:47,290 --> 00:14:51,639
little bit look at the the terror code

00:14:49,230 --> 00:14:53,410
like if you look at the the top of our

00:14:51,639 --> 00:14:55,750
installation we basically have

00:14:53,410 --> 00:14:58,899
everything broken down by an environment

00:14:55,750 --> 00:15:01,569
and all the foundation DB config is

00:14:58,899 --> 00:15:03,220
under there we have like an input in a

00:15:01,569 --> 00:15:05,500
main file just running through this

00:15:03,220 --> 00:15:08,410
quickly we can specify the instance type

00:15:05,500 --> 00:15:10,000
the number of processes to run any sort

00:15:08,410 --> 00:15:12,670
of tuning we want to do knobs and things

00:15:10,000 --> 00:15:14,559
like that and then we actually provision

00:15:12,670 --> 00:15:17,019
each individual tier so here we're

00:15:14,559 --> 00:15:20,290
looking at the storage tier and the

00:15:17,019 --> 00:15:21,850
stateless tier and then at the cluster

00:15:20,290 --> 00:15:23,709
level this is where we actually have our

00:15:21,850 --> 00:15:25,089
implementations so we're running for

00:15:23,709 --> 00:15:26,470
container orchestration we're not on

00:15:25,089 --> 00:15:30,490
kubernetes for an hour on

00:15:26,470 --> 00:15:31,930
ECS but this is a we have AWS

00:15:30,490 --> 00:15:33,820
implementation and then we have some of

00:15:31,930 --> 00:15:36,960
our services down there so you can

00:15:33,820 --> 00:15:42,310
basically stand up a new a new service

00:15:36,960 --> 00:15:44,520
and create a TF file in your environment

00:15:42,310 --> 00:15:47,050
and basically it will stand up

00:15:44,520 --> 00:15:49,090
auto-scaling groups it will stand up the

00:15:47,050 --> 00:15:52,060
container orchestration systems and then

00:15:49,090 --> 00:15:54,340
run these services that we wrote the

00:15:52,060 --> 00:15:58,810
supporting services on top of those

00:15:54,340 --> 00:16:00,610
systems a little bit about the sort of

00:15:58,810 --> 00:16:02,320
how we provision the boxes when they

00:16:00,610 --> 00:16:04,720
come up so we use the user data

00:16:02,320 --> 00:16:07,450
configuration so we build we build all

00:16:04,720 --> 00:16:09,640
of the of our Amy's with Packer and then

00:16:07,450 --> 00:16:11,920
we use user data configuration to

00:16:09,640 --> 00:16:15,130
basically specify what classist is and

00:16:11,920 --> 00:16:18,310
any other properties associated with

00:16:15,130 --> 00:16:20,710
like tuning so knobs or any other

00:16:18,310 --> 00:16:23,100
parameters we want to pass and then when

00:16:20,710 --> 00:16:27,010
the instance is booted up we we have a

00:16:23,100 --> 00:16:29,530
bootstrap that runs and a set of system

00:16:27,010 --> 00:16:31,480
D processes that go ahead and look at it

00:16:29,530 --> 00:16:33,670
they determine what type of what

00:16:31,480 --> 00:16:37,300
classist is it says okay it's a storage

00:16:33,670 --> 00:16:39,880
node I need to mount the nvme I need to

00:16:37,300 --> 00:16:43,180
format the device and then I need to

00:16:39,880 --> 00:16:46,750
finally build the the foundation DB

00:16:43,180 --> 00:16:50,290
kampf so we have this this script called

00:16:46,750 --> 00:16:52,300
configure f DB that gets executed and it

00:16:50,290 --> 00:16:55,200
basically sets all our parameters sets

00:16:52,300 --> 00:16:59,320
our locality zone ID for replication

00:16:55,200 --> 00:17:01,930
sets up the backup and and then we're

00:16:59,320 --> 00:17:05,770
sort of off so that's like the core of

00:17:01,930 --> 00:17:07,420
the of the the terraform configuration

00:17:05,770 --> 00:17:10,060
and then as I mentioned we have a few

00:17:07,420 --> 00:17:13,540
services that support the system that

00:17:10,060 --> 00:17:16,570
run on the ECS or adding container

00:17:13,540 --> 00:17:20,710
orchestration system so the first one we

00:17:16,570 --> 00:17:22,480
the first one is f DB discovery and i

00:17:20,710 --> 00:17:27,940
guess you all are probably familiar with

00:17:22,480 --> 00:17:29,500
this problem yeah so so basically what f

00:17:27,940 --> 00:17:32,020
DB discovery is is it's just a small

00:17:29,500 --> 00:17:34,570
service that that runs on all the

00:17:32,020 --> 00:17:37,210
clusters its fronted by a load balancer

00:17:34,570 --> 00:17:38,260
and you can curl that service and

00:17:37,210 --> 00:17:40,929
basically get back

00:17:38,260 --> 00:17:44,830
FDB dot cluster config pop it in f DB

00:17:40,929 --> 00:17:47,770
cluster and add your node to to your

00:17:44,830 --> 00:17:51,730
cluster we have a process called f DB

00:17:47,770 --> 00:17:54,520
trace and that is basically parses all

00:17:51,730 --> 00:17:59,350
the trace log files and converts them to

00:17:54,520 --> 00:18:02,230
metrics and and we feed those into data

00:17:59,350 --> 00:18:03,730
dog today so just a look at some of

00:18:02,230 --> 00:18:05,140
those for example

00:18:03,730 --> 00:18:07,750
this view right here it's showing like a

00:18:05,140 --> 00:18:10,960
chart with master recovery commits it

00:18:07,750 --> 00:18:14,830
shows you queues for storage servers and

00:18:10,960 --> 00:18:17,950
transaction nodes you can see data in

00:18:14,830 --> 00:18:22,090
flight and moving data that's actually

00:18:17,950 --> 00:18:24,690
queued you can see mean and Max bytes

00:18:22,090 --> 00:18:30,520
per commit and then we also track

00:18:24,690 --> 00:18:33,309
transaction latency p90 and Max and then

00:18:30,520 --> 00:18:34,929
probably and then the last process we

00:18:33,309 --> 00:18:36,070
actually have a couple more supporting

00:18:34,929 --> 00:18:39,370
tools but the last one we're gonna talk

00:18:36,070 --> 00:18:40,870
about today is f DB backup and that just

00:18:39,370 --> 00:18:44,530
basically kicks off the backup process

00:18:40,870 --> 00:18:46,780
so we moved when we started we were on a

00:18:44,530 --> 00:18:49,330
6 out of version and was I can't recall

00:18:46,780 --> 00:18:53,080
what it was but we moved to 18 for the

00:18:49,330 --> 00:18:54,730
blobstore backup support it was pretty

00:18:53,080 --> 00:19:00,010
smooth we ran into a couple issues with

00:18:54,730 --> 00:19:02,230
expiring data I think we were one of the

00:19:00,010 --> 00:19:03,730
first people to use it cuz I was posting

00:19:02,230 --> 00:19:05,740
on the forum asking questions and I

00:19:03,730 --> 00:19:08,440
think we were sort of early on with it

00:19:05,740 --> 00:19:12,660
we're considering moving to maybe a you

00:19:08,440 --> 00:19:14,950
know hot standby with the dr agent and

00:19:12,660 --> 00:19:16,690
kind of because of the limitations we've

00:19:14,950 --> 00:19:19,480
seen around how quickly we can recover

00:19:16,690 --> 00:19:21,460
from backup and so I think Evan talked

00:19:19,480 --> 00:19:24,309
about this earlier but this is a this is

00:19:21,460 --> 00:19:25,809
a screenshot of like a game-day test we

00:19:24,309 --> 00:19:27,790
ran where we basically rebuild the

00:19:25,809 --> 00:19:30,790
rebuilt that restored the cluster from

00:19:27,790 --> 00:19:33,970
s3 looks like we were mostly maxed out

00:19:30,790 --> 00:19:35,650
on CPU but after we pulled down after we

00:19:33,970 --> 00:19:37,600
pull down batches of data it takes a

00:19:35,650 --> 00:19:39,370
while for it to apply so this was like

00:19:37,600 --> 00:19:44,710
it was a couple terabytes on disk and it

00:19:39,370 --> 00:19:46,480
took several hours ok and then so I'm

00:19:44,710 --> 00:19:49,120
going to talk a little bit about game

00:19:46,480 --> 00:19:52,570
day and chaos testing and then we'll

00:19:49,120 --> 00:19:53,860
wrap up with some future use cases so

00:19:52,570 --> 00:19:55,480
before we put it into production we

00:19:53,860 --> 00:19:59,409
wanted to run through and try to break

00:19:55,480 --> 00:20:03,640
it is as as good as we were as well as

00:19:59,409 --> 00:20:06,159
we could we we tried to induce tons of

00:20:03,640 --> 00:20:09,250
different failures the system behaves

00:20:06,159 --> 00:20:12,340
just as advertised it was great

00:20:09,250 --> 00:20:14,140
so we were able to kill pretty much

00:20:12,340 --> 00:20:15,880
anything in the cluster and it behaved

00:20:14,140 --> 00:20:19,210
well we did notice that partitioning

00:20:15,880 --> 00:20:20,770
storage nodes resulted in high CPU

00:20:19,210 --> 00:20:23,620
utilization for example as the cluster

00:20:20,770 --> 00:20:25,809
healed we also created a scenario where

00:20:23,620 --> 00:20:27,940
the performance dropped by basically

00:20:25,809 --> 00:20:30,429
exhausting a disk but we did that sort

00:20:27,940 --> 00:20:32,020
of not through the foundation DB path we

00:20:30,429 --> 00:20:34,870
just basically allocated a bunch of

00:20:32,020 --> 00:20:36,040
space on a drive and so we spoke with

00:20:34,870 --> 00:20:39,520
engineering and they're like yeah don't

00:20:36,040 --> 00:20:41,230
do that so so this is an example for

00:20:39,520 --> 00:20:42,730
example of like losing at losing a

00:20:41,230 --> 00:20:44,559
storage node during game day and we're

00:20:42,730 --> 00:20:45,340
like okay well the clusters healing feel

00:20:44,559 --> 00:20:48,280
pretty quick

00:20:45,340 --> 00:20:51,280
we saw increase in latency but our

00:20:48,280 --> 00:20:52,600
workload is the the data is in Kafka so

00:20:51,280 --> 00:20:54,159
we can handle a little bit of latency

00:20:52,600 --> 00:20:55,929
and then we just burn it down quickly

00:20:54,159 --> 00:20:59,169
after and this was the case where we

00:20:55,929 --> 00:21:01,270
essentially exhausted storage on a node

00:20:59,169 --> 00:21:04,540
and then once the the node basically

00:21:01,270 --> 00:21:07,570
handed off data the system recovered so

00:21:04,540 --> 00:21:11,980
other than that it's been you know it's

00:21:07,570 --> 00:21:15,030
been great it's fantastic it's been

00:21:11,980 --> 00:21:17,710
really sort of hands-off as far as a

00:21:15,030 --> 00:21:19,929
tional stuff it's been really good some

00:21:17,710 --> 00:21:21,280
some ongoing issues we have then I think

00:21:19,929 --> 00:21:24,730
these were mentioned today one of the

00:21:21,280 --> 00:21:26,740
data distribution unpredictability I

00:21:24,730 --> 00:21:29,590
think it's probably predictable but I'm

00:21:26,740 --> 00:21:31,540
not exactly sure how it's behaving so

00:21:29,590 --> 00:21:33,400
maybe just some more you know

00:21:31,540 --> 00:21:35,290
observability or some metrics associated

00:21:33,400 --> 00:21:37,480
where that would be great

00:21:35,290 --> 00:21:39,159
we need to internally consider three

00:21:37,480 --> 00:21:41,650
data hall we're running across three

00:21:39,159 --> 00:21:43,780
AZ's and we're doing triple replication

00:21:41,650 --> 00:21:46,659
now and I think that's really ideal if

00:21:43,780 --> 00:21:48,130
you have five localities zones so I

00:21:46,659 --> 00:21:49,630
think three data hall would be better

00:21:48,130 --> 00:21:52,299
for us when we're running with three

00:21:49,630 --> 00:21:54,850
AZ's and then I'd like to see maybe some

00:21:52,299 --> 00:21:56,350
some ability to throttle things like

00:21:54,850 --> 00:22:00,490
healing or things like that if I want to

00:21:56,350 --> 00:22:02,560
trade-off latency for recovery time for

00:22:00,490 --> 00:22:05,230
healing time worth

00:22:02,560 --> 00:22:06,700
finally let's talk about some potential

00:22:05,230 --> 00:22:08,710
future use cases so we're trying to get

00:22:06,700 --> 00:22:11,740
to the point where this is sort of a

00:22:08,710 --> 00:22:14,500
primitive inside of segment and new

00:22:11,740 --> 00:22:16,600
product teams can come start up new

00:22:14,500 --> 00:22:18,400
foundation DB instances get off and get

00:22:16,600 --> 00:22:21,370
running and just build their stuff on

00:22:18,400 --> 00:22:23,680
top of this one of the use cases that

00:22:21,370 --> 00:22:26,050
we're considering is for a system we

00:22:23,680 --> 00:22:27,790
have internally called D dupe basically

00:22:26,050 --> 00:22:29,730
a lot of the messages we get come from

00:22:27,790 --> 00:22:32,020
mobile devices and mobile devices are on

00:22:29,730 --> 00:22:33,610
cellular networks and those are not

00:22:32,020 --> 00:22:36,040
necessarily reliable people are going

00:22:33,610 --> 00:22:37,690
under bridges going indoors losing

00:22:36,040 --> 00:22:39,370
signal all the time so we get a lot of

00:22:37,690 --> 00:22:40,420
duplicate messages so we have a system

00:22:39,370 --> 00:22:41,920
called D dupe

00:22:40,420 --> 00:22:43,870
that's it's sort of in the towards the

00:22:41,920 --> 00:22:45,880
end of the pipeline and essentially the

00:22:43,870 --> 00:22:48,820
way it works is we hash messages by

00:22:45,880 --> 00:22:51,820
message ID to a kafka topic and then

00:22:48,820 --> 00:22:53,890
even we have a worker per topic in topic

00:22:51,820 --> 00:22:55,870
and partition that consumes that and

00:22:53,890 --> 00:22:58,960
says it looks in the local database it's

00:22:55,870 --> 00:23:01,330
rocks DB today and says have I seen this

00:22:58,960 --> 00:23:03,070
message before if it has seen it it

00:23:01,330 --> 00:23:05,380
would suppress it from going downstream

00:23:03,070 --> 00:23:08,740
otherwise if it's the first time it sees

00:23:05,380 --> 00:23:11,470
it it writes to Kafka and it commits to

00:23:08,740 --> 00:23:13,060
rocks DB so and then we jump through a

00:23:11,470 --> 00:23:15,700
bunch of Hoops to make that actually

00:23:13,060 --> 00:23:18,430
work because it's not atomic to write to

00:23:15,700 --> 00:23:20,490
to Kafka and write to rocks but anyway

00:23:18,430 --> 00:23:23,470
when we restart we we read our rights

00:23:20,490 --> 00:23:26,740
but this system is it's pretty good

00:23:23,470 --> 00:23:28,030
because it's fast and basically you

00:23:26,740 --> 00:23:30,370
don't have to deal with any issues with

00:23:28,030 --> 00:23:33,520
network partitioning it's an embedded

00:23:30,370 --> 00:23:36,220
database but the problem is is it's it's

00:23:33,520 --> 00:23:38,230
difficult to scale because it's tied to

00:23:36,220 --> 00:23:41,050
Kafka so if you want to increase your

00:23:38,230 --> 00:23:43,690
Kafka partition count if you need to

00:23:41,050 --> 00:23:46,000
increase your count to add more

00:23:43,690 --> 00:23:47,590
processing power to the system we have

00:23:46,000 --> 00:23:51,970
to jump through a bunch of Hoops to

00:23:47,590 --> 00:23:53,650
basically shuffle all this data so it

00:23:51,970 --> 00:23:55,210
would be nice to move this workload to

00:23:53,650 --> 00:23:58,180
Foundation DB so we could have an

00:23:55,210 --> 00:23:59,470
elastic tier depending on how much how

00:23:58,180 --> 00:24:01,800
many messages are coming in what the

00:23:59,470 --> 00:24:04,870
volume is those could scale up and down

00:24:01,800 --> 00:24:07,260
query foundation DB to determine if

00:24:04,870 --> 00:24:09,420
we've seen these messages before and

00:24:07,260 --> 00:24:15,290
and suppress if needed otherwise for

00:24:09,420 --> 00:24:15,290
down the rest of the pipe and that is it

00:24:15,330 --> 00:24:20,550

YouTube URL: https://www.youtube.com/watch?v=qHn2BIrp0B4


