Title: Retrofitting a Foundation for CouchDB - Adam Kocoloski, IBM
Publication date: 2019-11-25
Playlist: FoundationDB Summit 2019
Description: 
	Retrofitting a Foundation for CouchDB - Adam Kocoloski, IBM

FoundationDB tackles some wickedly hard problems in distributed systems and offers developers a reliable toolkit for building modern data services. The resulting layered approach to database development is an attractive architecture for “new construction”, but how well does it work in a renovation?

The Apache CouchDB project is adopting FoundationDB as its core persistence layer. CouchDB is a JSON document store that implements many standard DB features (primary keys, indexes, materialized views) as well as some more exotic ones, including active/active replication across any user-defined set of databases. We’ll discuss CouchDB’s rationale for adopting FoundationDB and report our progress on this journey. We’ll provide comparisons to extant layers like RecordLayer and DocumentLayer, and share some of our experiences running an open source database project over the past decade.
Captions: 
	00:00:00,030 --> 00:00:07,230
okay now what's on much better you ready

00:00:04,970 --> 00:00:09,630
okay all right

00:00:07,230 --> 00:00:11,580
so I'm delighted to be here with all of

00:00:09,630 --> 00:00:13,759
you today to talk to you about a project

00:00:11,580 --> 00:00:17,369
that we've had underway for about a year

00:00:13,759 --> 00:00:19,439
to take an existing open source document

00:00:17,369 --> 00:00:22,980
database and retrofit it so that it uses

00:00:19,439 --> 00:00:24,300
foundation DB under the covers right and

00:00:22,980 --> 00:00:26,160
so you know those of you who've spent

00:00:24,300 --> 00:00:28,140
some time around foundation DB I mean I

00:00:26,160 --> 00:00:30,179
think we recognize its potential for

00:00:28,140 --> 00:00:33,360
building these you know elegant pieces

00:00:30,179 --> 00:00:35,399
of engineering right that you know are

00:00:33,360 --> 00:00:39,000
just architected and designed and

00:00:35,399 --> 00:00:41,070
implemented to the tee right my projects

00:00:39,000 --> 00:00:42,750
a little bit different right this is not

00:00:41,070 --> 00:00:43,860
greenfield development this has taken

00:00:42,750 --> 00:00:45,239
something that we're very proud of

00:00:43,860 --> 00:00:47,340
that's got all kinds of bells and

00:00:45,239 --> 00:00:51,449
whistles and figuring out how to put it

00:00:47,340 --> 00:00:52,649
on a more solid footing going forward so

00:00:51,449 --> 00:00:54,690
I need to tell you a little bit first

00:00:52,649 --> 00:00:55,680
about couchdb itself know by the way

00:00:54,690 --> 00:00:57,539
that turns out this is something people

00:00:55,680 --> 00:00:59,250
actually do with some regularity it's

00:00:57,539 --> 00:01:03,629
kind of crazy that they pick houses up

00:00:59,250 --> 00:01:06,330
and move them around I didn't realize so

00:01:03,629 --> 00:01:07,770
CouchDB sort of at a glance right it is

00:01:06,330 --> 00:01:10,049
a database that's delivered as a web

00:01:07,770 --> 00:01:12,950
service you communicate it communicate

00:01:10,049 --> 00:01:16,020
with it using HTTP and JSON payloads

00:01:12,950 --> 00:01:18,780
these JSON documents you know that is

00:01:16,020 --> 00:01:20,189
the atomic boundary of updates to the

00:01:18,780 --> 00:01:22,590
database is the boundary of a document

00:01:20,189 --> 00:01:24,750
these documents are primary keys you can

00:01:22,590 --> 00:01:27,770
create secondary indexes oftentimes

00:01:24,750 --> 00:01:31,140
using server-side JavaScript to do so

00:01:27,770 --> 00:01:33,360
the project kind of pioneered the use of

00:01:31,140 --> 00:01:35,280
these internet accessible change capture

00:01:33,360 --> 00:01:36,720
feeds that have turned out to be a

00:01:35,280 --> 00:01:37,799
really popular way for interacting with

00:01:36,720 --> 00:01:40,290
the database and building kind of

00:01:37,799 --> 00:01:42,479
event-driven applications and one of the

00:01:40,290 --> 00:01:44,040
main use cases for that is active active

00:01:42,479 --> 00:01:46,890
replication we have lots of people who

00:01:44,040 --> 00:01:48,180
run couchdb in multiple cloud regions in

00:01:46,890 --> 00:01:50,009
their on-premises data center and the

00:01:48,180 --> 00:01:51,750
cloud region and synchronize in both

00:01:50,009 --> 00:01:53,939
directions and the database has all the

00:01:51,750 --> 00:01:55,710
metadata necessary to ultimately

00:01:53,939 --> 00:01:59,969
converge the state on each sides of that

00:01:55,710 --> 00:02:00,960
or every corner of that topology and

00:01:59,969 --> 00:02:03,030
really all those features have been

00:02:00,960 --> 00:02:05,040
around for almost a decade right one

00:02:03,030 --> 00:02:08,340
defeat included everything you see on

00:02:05,040 --> 00:02:10,050
that list of features in tirado we

00:02:08,340 --> 00:02:11,129
introduced support for clustering which

00:02:10,050 --> 00:02:12,620
is going to be a big topic of the

00:02:11,129 --> 00:02:14,659
conversation today and

00:02:12,620 --> 00:02:20,140
it's gonna evolve in this brave new

00:02:14,659 --> 00:02:23,510
world that we are entering 3.0 which is

00:02:20,140 --> 00:02:25,280
Eddy day now is intended to be our best

00:02:23,510 --> 00:02:26,480
attempt at the classic CouchDB

00:02:25,280 --> 00:02:28,489
architecture so we've added a few new

00:02:26,480 --> 00:02:30,019
features but really it's our attempt to

00:02:28,489 --> 00:02:31,489
plate a stake in the ground and say

00:02:30,019 --> 00:02:33,440
great that's kind of the end of that

00:02:31,489 --> 00:02:35,209
architectural line because 4.0 we've

00:02:33,440 --> 00:02:37,370
committed as a project is gonna be using

00:02:35,209 --> 00:02:39,799
foundation to be under the hood it's a

00:02:37,370 --> 00:02:41,030
relatively compact codebase and one of

00:02:39,799 --> 00:02:44,510
the reasons for that is because it's

00:02:41,030 --> 00:02:45,769
implemented largely in Erlang which has

00:02:44,510 --> 00:02:46,940
been a pretty expressive high-level

00:02:45,769 --> 00:02:49,760
language and a good language for

00:02:46,940 --> 00:02:51,859
building highly available concurrent web

00:02:49,760 --> 00:02:55,459
services not such a great language for

00:02:51,859 --> 00:02:57,260
high-throughput handling of data so it's

00:02:55,459 --> 00:02:59,690
you know we worked around some of those

00:02:57,260 --> 00:03:02,810
issues but overall something that we're

00:02:59,690 --> 00:03:04,340
pretty happy with and you know so step

00:03:02,810 --> 00:03:06,049
one of adopting foundation DB was us

00:03:04,340 --> 00:03:07,190
implementing our line bindings and

00:03:06,049 --> 00:03:08,959
putting those out in the community so

00:03:07,190 --> 00:03:13,010
that was one of the ones on Alex's list

00:03:08,959 --> 00:03:14,870
there so let's talk for a minute about

00:03:13,010 --> 00:03:17,359
how that clustering that we introduced

00:03:14,870 --> 00:03:18,620
into dotto it works just so you can kind

00:03:17,359 --> 00:03:20,120
of understand you know our motivation

00:03:18,620 --> 00:03:22,549
and a rationale for heading down this

00:03:20,120 --> 00:03:25,849
path every database and apache couchdb

00:03:22,549 --> 00:03:27,970
is split in two shards those shards are

00:03:25,849 --> 00:03:31,430
replicated across a series of nodes and

00:03:27,970 --> 00:03:32,870
each of the documents is mapped to a

00:03:31,430 --> 00:03:34,669
specific shard in the database using

00:03:32,870 --> 00:03:36,319
consistent hashing we also have some

00:03:34,669 --> 00:03:37,910
support for compound primary keys so

00:03:36,319 --> 00:03:39,440
instead of the entire primary key of the

00:03:37,910 --> 00:03:40,730
document determining the routing you can

00:03:39,440 --> 00:03:42,290
say hey I want to co-locate these

00:03:40,730 --> 00:03:46,750
documents that have a partition key

00:03:42,290 --> 00:03:49,760
that's very common so we do that and

00:03:46,750 --> 00:03:52,040
crucially every one of those replicas is

00:03:49,760 --> 00:03:54,590
able to independently decide whether to

00:03:52,040 --> 00:03:56,239
accept a particular update updates are

00:03:54,590 --> 00:03:58,010
supposed to be applied against a base

00:03:56,239 --> 00:03:59,989
version of the document and so the most

00:03:58,010 --> 00:04:02,060
common in a reason for an update to be

00:03:59,989 --> 00:04:04,459
rejected is because the bit you know the

00:04:02,060 --> 00:04:05,720
the snapshot of the database has changed

00:04:04,459 --> 00:04:08,239
right the document has been updated

00:04:05,720 --> 00:04:09,829
underneath you and now it is gonna

00:04:08,239 --> 00:04:13,010
reject an update that is applied against

00:04:09,829 --> 00:04:14,930
an earlier based revision but crucially

00:04:13,010 --> 00:04:16,760
every replica of a shard does that

00:04:14,930 --> 00:04:20,169
independently there's no consensus going

00:04:16,760 --> 00:04:22,400
on the shards do maintain enough

00:04:20,169 --> 00:04:24,110
metadata to synchronize after the fact

00:04:22,400 --> 00:04:25,180
and to ultimately get to the same view

00:04:24,110 --> 00:04:28,449
of the world but that can be

00:04:25,180 --> 00:04:29,919
multi-version view if you know replica

00:04:28,449 --> 00:04:31,539
one accepts one update and replica two

00:04:29,919 --> 00:04:34,870
accepts another update eventually

00:04:31,539 --> 00:04:37,600
they'll both see both updates and then

00:04:34,870 --> 00:04:40,150
from a indexing perspective each of

00:04:37,600 --> 00:04:41,410
those shards builds a local index which

00:04:40,150 --> 00:04:42,699
is great for scaling the indexing

00:04:41,410 --> 00:04:44,800
throughput it's nice and easy and simple

00:04:42,699 --> 00:04:47,440
but it does mean that when we want a

00:04:44,800 --> 00:04:49,389
query secondary indexes it's a full

00:04:47,440 --> 00:04:52,600
scatter gather operation right because

00:04:49,389 --> 00:04:54,520
we don't know a priori which nodes are

00:04:52,600 --> 00:04:56,410
actually hosting the portions of the

00:04:54,520 --> 00:05:00,430
secondary index that are relevant for

00:04:56,410 --> 00:05:02,770
the query that the user executed so this

00:05:00,430 --> 00:05:05,620
is a simple I would argue clustering

00:05:02,770 --> 00:05:07,509
design it is operationally simple in the

00:05:05,620 --> 00:05:10,270
sense that the cluster is basically

00:05:07,509 --> 00:05:11,919
homogeneous it's simple in that the

00:05:10,270 --> 00:05:14,289
system ultimately gets back to a good

00:05:11,919 --> 00:05:16,750
state on its own and you know 99 times

00:05:14,289 --> 00:05:19,210
out of 100 and it's served us well in

00:05:16,750 --> 00:05:22,150
production for quite a long time you

00:05:19,210 --> 00:05:24,759
know my employer at IBM cloud we use

00:05:22,150 --> 00:05:26,199
Cloudant which is based on CouchDB both

00:05:24,759 --> 00:05:28,509
as you know kind of our answer to

00:05:26,199 --> 00:05:29,680
DynamoDB on the amazon side and also as

00:05:28,509 --> 00:05:31,030
a core piece of critical internal

00:05:29,680 --> 00:05:35,800
infrastructure lots of the IBM cloud

00:05:31,030 --> 00:05:37,000
runs on the cloud service but those of

00:05:35,800 --> 00:05:38,949
you who you know know your way around

00:05:37,000 --> 00:05:40,630
distributed systems can foresee some of

00:05:38,949 --> 00:05:43,570
the pathologies that can occur with a

00:05:40,630 --> 00:05:45,789
system like this and so let's just kind

00:05:43,570 --> 00:05:46,930
of enumerate them a little bit here one

00:05:45,789 --> 00:05:49,150
of the challenges this is of course the

00:05:46,930 --> 00:05:50,500
scaling of those queries right this is

00:05:49,150 --> 00:05:52,000
there's a there's a top end to the

00:05:50,500 --> 00:05:53,560
amount of throughput we can deliver for

00:05:52,000 --> 00:05:54,669
queries against secondary indexes in

00:05:53,560 --> 00:05:57,630
this design because you're always

00:05:54,669 --> 00:06:02,669
hitting every shard no matter what

00:05:57,630 --> 00:06:02,669
another one yeah

00:06:02,759 --> 00:06:07,659
guaranteeing that time moves forward

00:06:05,070 --> 00:06:10,000
when reading these secondary indexes is

00:06:07,659 --> 00:06:11,740
a challenge when I'm reading primary

00:06:10,000 --> 00:06:13,870
documents in an apache couchdb cluster I

00:06:11,740 --> 00:06:16,479
got quorum operations you know I submit

00:06:13,870 --> 00:06:18,940
a document and it gets acknowledged when

00:06:16,479 --> 00:06:20,710
majority of the copies committed and I

00:06:18,940 --> 00:06:22,060
read and I also do a quorum operation

00:06:20,710 --> 00:06:23,409
against a majority that copies to

00:06:22,060 --> 00:06:26,020
actually return the response to the

00:06:23,409 --> 00:06:28,509
client and so that paper is over most of

00:06:26,020 --> 00:06:30,460
the kind of you know issues that might

00:06:28,509 --> 00:06:32,259
be encountered in in this eventually

00:06:30,460 --> 00:06:34,120
consistent system when I'm reading the

00:06:32,259 --> 00:06:35,740
secondary indexes though there's none of

00:06:34,120 --> 00:06:37,210
that quorum going on or if you like it's

00:06:35,740 --> 00:06:38,770
implicitly in our equals one quorum

00:06:37,210 --> 00:06:41,650
against the index read which

00:06:38,770 --> 00:06:43,180
means that if I let the cluster kind of

00:06:41,650 --> 00:06:45,610
bounce back and forth in terms of which

00:06:43,180 --> 00:06:48,009
replica ends up being used it's entirely

00:06:45,610 --> 00:06:49,990
plausible that I might put a document in

00:06:48,009 --> 00:06:52,090
get you know my two-thirds majority

00:06:49,990 --> 00:06:53,770
quorum to accept it and then right away

00:06:52,090 --> 00:06:56,979
trigger an index read and hit the third

00:06:53,770 --> 00:06:58,810
copy and I don't see it right or if I've

00:06:56,979 --> 00:07:00,490
got some sort of you know recovery

00:06:58,810 --> 00:07:01,750
situation where the node was down for a

00:07:00,490 --> 00:07:02,979
while and it's kind of replicating in

00:07:01,750 --> 00:07:05,530
its changes and catching back up on the

00:07:02,979 --> 00:07:08,349
index I can see a very divergent you

00:07:05,530 --> 00:07:09,759
know mixed-up notion of time and we do

00:07:08,349 --> 00:07:11,530
stuff to try to protect against that but

00:07:09,759 --> 00:07:14,560
it's you know it's stuff we have to do

00:07:11,530 --> 00:07:16,090
to try to protect against that similarly

00:07:14,560 --> 00:07:18,849
those change capture feeds I talked

00:07:16,090 --> 00:07:22,270
about they end up with another sort of

00:07:18,849 --> 00:07:24,130
interesting pathology we may can at

00:07:22,270 --> 00:07:25,150
least once guarantee on the semantics of

00:07:24,130 --> 00:07:26,860
the changes feed we guarantee you'll

00:07:25,150 --> 00:07:28,120
never miss an update to the database

00:07:26,860 --> 00:07:29,289
that occurred since the last time you

00:07:28,120 --> 00:07:32,020
checked in on the change capture feed

00:07:29,289 --> 00:07:34,030
but that means if we have to fail over

00:07:32,020 --> 00:07:36,520
to another replica of a shard in order

00:07:34,030 --> 00:07:38,289
to hand over that changes feed we don't

00:07:36,520 --> 00:07:40,150
always know exactly which updates have

00:07:38,289 --> 00:07:42,310
been observed right like the the

00:07:40,150 --> 00:07:45,280
question I'm trying to answer is you

00:07:42,310 --> 00:07:47,560
know what is the largest possible update

00:07:45,280 --> 00:07:49,509
sequence of this shard that guarantees

00:07:47,560 --> 00:07:51,430
that I don't miss anything that was

00:07:49,509 --> 00:07:53,949
written over on the other replica of the

00:07:51,430 --> 00:07:55,300
shard and we try to do a bit of

00:07:53,949 --> 00:07:58,479
bookkeeping to make sure that we have a

00:07:55,300 --> 00:08:00,580
very you know good bound on how much of

00:07:58,479 --> 00:08:01,750
the feed gets replayed but it is

00:08:00,580 --> 00:08:03,099
annoying that these feeds do get

00:08:01,750 --> 00:08:04,630
replayed from time to time and fail over

00:08:03,099 --> 00:08:06,190
situations and people see you know

00:08:04,630 --> 00:08:07,990
updates that they had already previously

00:08:06,190 --> 00:08:11,319
consumed in their change capture feed

00:08:07,990 --> 00:08:12,940
and finally the really big one for me is

00:08:11,319 --> 00:08:15,789
the unavoidability of edit conflicts

00:08:12,940 --> 00:08:17,770
like it's really almost impossible for a

00:08:15,789 --> 00:08:19,719
developer against this system to build

00:08:17,770 --> 00:08:21,250
an application that never gets into an

00:08:19,719 --> 00:08:23,289
edit conflict situation even something

00:08:21,250 --> 00:08:24,940
as simple as a retry loop with some sort

00:08:23,289 --> 00:08:26,319
of mutable bit of data that's going in a

00:08:24,940 --> 00:08:28,029
timestamp or something in your document

00:08:26,319 --> 00:08:30,250
can easily end up in a situation where

00:08:28,029 --> 00:08:31,509
you end up with concurrent edits getting

00:08:30,250 --> 00:08:34,029
accepted by different replicas of a

00:08:31,509 --> 00:08:37,300
shard simultaneously and now the system

00:08:34,029 --> 00:08:39,520
has a multi versioned you know story for

00:08:37,300 --> 00:08:40,719
that one particular document and I think

00:08:39,520 --> 00:08:42,399
if there's one thing we've learned in

00:08:40,719 --> 00:08:45,040
the past ten years of no sequel it's

00:08:42,399 --> 00:08:46,660
that managing those edit conflicts

00:08:45,040 --> 00:08:48,279
correctly handling them in the

00:08:46,660 --> 00:08:51,399
application layer is a really freaking

00:08:48,279 --> 00:08:52,310
hard problem so it kind of feels like

00:08:51,399 --> 00:08:53,690
we're here like we've got

00:08:52,310 --> 00:08:55,070
this house and we're kind of like we see

00:08:53,690 --> 00:08:57,470
this cliff looming the cliff is getting

00:08:55,070 --> 00:09:00,160
closer and so we said you know we really

00:08:57,470 --> 00:09:02,210
need to sit down and work hard to

00:09:00,160 --> 00:09:04,790
address this stuff in the core of the

00:09:02,210 --> 00:09:06,860
database and we kind of did the sizing

00:09:04,790 --> 00:09:09,830
exercise what it would take to introduce

00:09:06,860 --> 00:09:12,230
you know consensus overtop of the

00:09:09,830 --> 00:09:13,700
individual replicas of the shards you

00:09:12,230 --> 00:09:15,589
know do the work to ensure that there is

00:09:13,700 --> 00:09:17,600
one total ordering of rights for each of

00:09:15,589 --> 00:09:19,790
those individual shards do the work to

00:09:17,600 --> 00:09:20,630
reorganize secondary indexes based on

00:09:19,790 --> 00:09:22,790
that maybe something like RAM

00:09:20,630 --> 00:09:25,190
transactions things like that that would

00:09:22,790 --> 00:09:26,510
give us scalability for the queries but

00:09:25,190 --> 00:09:27,440
we also said all right let's do our

00:09:26,510 --> 00:09:29,900
homework and see if there are other

00:09:27,440 --> 00:09:31,070
things that could help us accelerate you

00:09:29,900 --> 00:09:32,270
know addressing these different gaps

00:09:31,070 --> 00:09:34,910
that we felt we had in our clustering

00:09:32,270 --> 00:09:36,170
technology and so we said we need

00:09:34,910 --> 00:09:37,460
something that preserves our existing

00:09:36,170 --> 00:09:39,080
API we've got tons of users in

00:09:37,460 --> 00:09:40,310
production we've got lots of people who

00:09:39,080 --> 00:09:42,260
are happy with the semantics that we

00:09:40,310 --> 00:09:43,700
provide sure there are warts but we

00:09:42,260 --> 00:09:45,380
can't do a wholesale change and just you

00:09:43,700 --> 00:09:46,790
like throw the you know throw everything

00:09:45,380 --> 00:09:49,670
up in the air and expect people to to

00:09:46,790 --> 00:09:50,839
join us on that journey we wanted

00:09:49,670 --> 00:09:52,580
something that we could be competent

00:09:50,839 --> 00:09:53,990
about like if there's one thing we've

00:09:52,580 --> 00:09:55,190
done as a project over the past decade

00:09:53,990 --> 00:09:58,430
it's earned a reputation for reliability

00:09:55,190 --> 00:10:01,310
and durability and we couldn't afford to

00:09:58,430 --> 00:10:03,170
kind of go backwards in our you know

00:10:01,310 --> 00:10:05,390
posture on that front like we really

00:10:03,170 --> 00:10:07,370
needed something we could count on we

00:10:05,390 --> 00:10:09,050
needed to scale up certainly in my

00:10:07,370 --> 00:10:09,620
environment we run lots of large at

00:10:09,050 --> 00:10:11,390
scale

00:10:09,620 --> 00:10:12,860
you know clusters but we also have a

00:10:11,390 --> 00:10:14,630
broad-based user community that just

00:10:12,860 --> 00:10:16,400
downloads CouchDB and uses them in very

00:10:14,630 --> 00:10:17,900
lightweight scenarios with you know

00:10:16,400 --> 00:10:19,160
little web applications in diverse

00:10:17,900 --> 00:10:21,260
environments and so we needed something

00:10:19,160 --> 00:10:23,240
that could scale down even if that

00:10:21,260 --> 00:10:24,530
wasn't necessarily its primary goal we

00:10:23,240 --> 00:10:26,360
couldn't have something that had like a

00:10:24,530 --> 00:10:29,650
minimum footprint of you know seven

00:10:26,360 --> 00:10:33,320
servers and like hundreds of gigs of ram

00:10:29,650 --> 00:10:34,610
and we wanted something that just kind

00:10:33,320 --> 00:10:36,080
of had an impedance match and this is

00:10:34,610 --> 00:10:38,900
like a you know you know it when you see

00:10:36,080 --> 00:10:41,360
it kind of thing but we needed to feel

00:10:38,900 --> 00:10:43,100
right about the sort of layering so to

00:10:41,360 --> 00:10:44,720
speak of what we might be putting in

00:10:43,100 --> 00:10:47,089
underneath you know what we wanted to do

00:10:44,720 --> 00:10:48,470
here is CouchDB and so around this time

00:10:47,089 --> 00:10:49,790
last year that's when we sort of take it

00:10:48,470 --> 00:10:51,560
a started taking a closer and closer

00:10:49,790 --> 00:10:53,390
look at foundation DB you know a few of

00:10:51,560 --> 00:10:55,100
us came to the summit last year heard

00:10:53,390 --> 00:10:56,779
lots of great stories about how people

00:10:55,100 --> 00:10:58,370
were using it and you know the way the

00:10:56,779 --> 00:11:02,480
internals were working and and really

00:10:58,370 --> 00:11:03,670
had confidence in our ability to depend

00:11:02,480 --> 00:11:06,850
on this as

00:11:03,670 --> 00:11:08,680
part of our go forward architecture so

00:11:06,850 --> 00:11:09,610
what does that do for us well it does a

00:11:08,680 --> 00:11:11,710
few things

00:11:09,610 --> 00:11:13,750
it absolutely eliminates those edit

00:11:11,710 --> 00:11:15,340
conflicts when apps are targeting a

00:11:13,750 --> 00:11:18,610
single cloud region or a single you know

00:11:15,340 --> 00:11:20,410
deployment of apache couchdb it lets us

00:11:18,610 --> 00:11:22,510
really refocus our efforts on that

00:11:20,410 --> 00:11:23,950
active active multi-region replication

00:11:22,510 --> 00:11:25,630
which I think continues to be one of the

00:11:23,950 --> 00:11:27,250
main differentiating capabilities of the

00:11:25,630 --> 00:11:29,580
project and so rather than kind of

00:11:27,250 --> 00:11:33,160
having this replication system kind of

00:11:29,580 --> 00:11:34,780
serve multiple purposes of synchronizing

00:11:33,160 --> 00:11:36,520
stuff within a cloud region you know

00:11:34,780 --> 00:11:38,260
across availability zones and between

00:11:36,520 --> 00:11:40,090
regions now we've got a nice separation

00:11:38,260 --> 00:11:41,560
of concerns we can optimize our

00:11:40,090 --> 00:11:43,090
replication system for that and let

00:11:41,560 --> 00:11:46,450
Foundation DB handle all the stuff Ian

00:11:43,090 --> 00:11:48,850
region we can redo our secondary indexes

00:11:46,450 --> 00:11:49,690
in a much more scalable way and you know

00:11:48,850 --> 00:11:51,460
included as part of the right

00:11:49,690 --> 00:11:52,480
transaction do it the way we really

00:11:51,460 --> 00:11:54,700
deserves to have been done in the first

00:11:52,480 --> 00:11:56,800
place and we get that totally ordered

00:11:54,700 --> 00:11:58,660
sort of a list of changes from the

00:11:56,800 --> 00:12:00,610
change capture feed which again is you

00:11:58,660 --> 00:12:01,870
know a nice upgrade for a feature that a

00:12:00,610 --> 00:12:05,290
lot of our users find pretty attractive

00:12:01,870 --> 00:12:06,610
I don't have time to go through all the

00:12:05,290 --> 00:12:08,260
data modeling but I can give you a

00:12:06,610 --> 00:12:09,910
little bit of a sense if you've perused

00:12:08,260 --> 00:12:11,560
the foundation DB site and gone through

00:12:09,910 --> 00:12:14,200
the design recipes for the document

00:12:11,560 --> 00:12:16,300
model and for simple indexes you get the

00:12:14,200 --> 00:12:18,060
general idea it looks a lot like the way

00:12:16,300 --> 00:12:20,230
that works

00:12:18,060 --> 00:12:21,760
version steams for those of you who know

00:12:20,230 --> 00:12:23,560
about them know that they are a great

00:12:21,760 --> 00:12:26,560
you know way to build this change

00:12:23,560 --> 00:12:29,410
capture feed for those of you don't this

00:12:26,560 --> 00:12:32,050
is a way for you to tell foundation DB

00:12:29,410 --> 00:12:33,850
to insert you know a version of the

00:12:32,050 --> 00:12:34,990
database as part of the commit so it

00:12:33,850 --> 00:12:37,630
doesn't you know you don't have to know

00:12:34,990 --> 00:12:39,190
as a client what that version is going

00:12:37,630 --> 00:12:40,720
to be you just tell it hey this sequence

00:12:39,190 --> 00:12:42,520
of bytes in the key or in the value

00:12:40,720 --> 00:12:44,710
replace this with the version at the

00:12:42,520 --> 00:12:47,410
commit time all right and so by using

00:12:44,710 --> 00:12:48,430
the version stamp as a key you just get

00:12:47,410 --> 00:12:49,930
that change of speed just kind of

00:12:48,430 --> 00:12:52,060
falling out you know almost for free

00:12:49,930 --> 00:12:53,410
it's really quite nice you do have to be

00:12:52,060 --> 00:12:54,880
a little careful because it's not you

00:12:53,410 --> 00:12:56,410
know an idempotent thing at that point

00:12:54,880 --> 00:12:57,850
and so we actually write a separate

00:12:56,410 --> 00:12:59,670
little transaction ID that allows us in

00:12:57,850 --> 00:13:01,990
a retry scenario to see whether that's

00:12:59,670 --> 00:13:03,160
you know or whether the database has

00:13:01,990 --> 00:13:05,530
already accepted it because he get these

00:13:03,160 --> 00:13:07,330
situations sometimes where Foundation DB

00:13:05,530 --> 00:13:08,800
just doesn't respond to you you know if

00:13:07,330 --> 00:13:10,540
it's like not healthy and then you got

00:13:08,800 --> 00:13:13,020
to go figure out whether it did or did

00:13:10,540 --> 00:13:16,730
not ultimately commit the update but

00:13:13,020 --> 00:13:19,590
it works for as well our data model is

00:13:16,730 --> 00:13:21,300
organized so that all our transactions

00:13:19,590 --> 00:13:23,280
end up being self conflicting that whole

00:13:21,300 --> 00:13:24,750
bit about how couchdb expects a base

00:13:23,280 --> 00:13:26,610
revision of the update against which

00:13:24,750 --> 00:13:27,720
you're trying to of the document against

00:13:26,610 --> 00:13:30,990
which you're trying to apply the update

00:13:27,720 --> 00:13:32,190
that just means that we end up with self

00:13:30,990 --> 00:13:33,990
conflicting transactions which is good

00:13:32,190 --> 00:13:35,850
thing right we don't have to do extra

00:13:33,990 --> 00:13:39,420
gymnastics that kind of just falls out

00:13:35,850 --> 00:13:43,770
of the data model and you know I think

00:13:39,420 --> 00:13:45,540
is a nice example of how the MVCC views

00:13:43,770 --> 00:13:47,540
of the world at the CouchDB level and

00:13:45,540 --> 00:13:49,800
the foundation DB level are simpatico

00:13:47,540 --> 00:13:52,290
they kind of hang together nicely in

00:13:49,800 --> 00:13:54,210
that respect we do use the atomic

00:13:52,290 --> 00:13:55,350
operations inside foundation DB for

00:13:54,210 --> 00:13:57,120
maintaining database statistics

00:13:55,350 --> 00:13:58,470
otherwise this would be a fairly high

00:13:57,120 --> 00:14:00,690
contention operation and we'd have to do

00:13:58,470 --> 00:14:02,700
some gymnastics to try to avoid lots of

00:14:00,690 --> 00:14:04,680
conflicts there so that's like a nice

00:14:02,700 --> 00:14:07,200
little feature inside foundation DB that

00:14:04,680 --> 00:14:08,160
we leverage and a new piece that you

00:14:07,200 --> 00:14:09,450
know if you've been paying attention to

00:14:08,160 --> 00:14:12,480
the forum's has shown up recently and I

00:14:09,450 --> 00:14:15,330
think 6.1 was the metadata version

00:14:12,480 --> 00:14:18,660
caching we use that as well so this

00:14:15,330 --> 00:14:19,950
actually lets foundation DB includes a

00:14:18,660 --> 00:14:21,990
bit of information with every one of

00:14:19,950 --> 00:14:26,580
your transactions that says here's this

00:14:21,990 --> 00:14:28,890
version value right and the way you can

00:14:26,580 --> 00:14:30,300
use that is you bump it if the

00:14:28,890 --> 00:14:31,830
information that you would like to cache

00:14:30,300 --> 00:14:34,020
has changed otherwise you can assume

00:14:31,830 --> 00:14:36,600
that these you know sorry let me step

00:14:34,020 --> 00:14:38,340
back the metadata version ships with

00:14:36,600 --> 00:14:39,810
every transaction and you can check to

00:14:38,340 --> 00:14:41,580
see whether it's been updated for free

00:14:39,810 --> 00:14:42,990
without issuing another read and you

00:14:41,580 --> 00:14:45,450
know have ending up the hotspot in your

00:14:42,990 --> 00:14:47,940
foundation DB environment right one nice

00:14:45,450 --> 00:14:49,740
use case for this is to enable you to

00:14:47,940 --> 00:14:51,900
cache certain portions of the key space

00:14:49,740 --> 00:14:53,430
in your client so for us we use it for

00:14:51,900 --> 00:14:55,740
database metadata schema information

00:14:53,430 --> 00:14:58,170
access control lists design documents

00:14:55,740 --> 00:15:00,420
index definitions that kind of stuff

00:14:58,170 --> 00:15:02,100
and we do it in a two level hierarchy so

00:15:00,420 --> 00:15:04,020
there's one global metadata version for

00:15:02,100 --> 00:15:05,280
the entire foundation DB cluster and if

00:15:04,020 --> 00:15:08,070
that hasn't changed at the beginning of

00:15:05,280 --> 00:15:09,360
our transaction great if it has changed

00:15:08,070 --> 00:15:11,910
then we have to do a second check to see

00:15:09,360 --> 00:15:13,350
well did the database metadata that I'm

00:15:11,910 --> 00:15:14,250
actually interested in changed right cuz

00:15:13,350 --> 00:15:15,600
we've got all you know a hundred

00:15:14,250 --> 00:15:17,820
thousand databases floating around in

00:15:15,600 --> 00:15:19,140
this cluster most of the time it's not

00:15:17,820 --> 00:15:21,060
gonna be your database whose metadata

00:15:19,140 --> 00:15:22,350
changed we don't want to go and reread

00:15:21,060 --> 00:15:23,700
all of that schema information and

00:15:22,350 --> 00:15:25,410
recompute all that stuff so we end up

00:15:23,700 --> 00:15:27,180
having this kind of two layer caching

00:15:25,410 --> 00:15:29,550
hierarchy where the second layer does

00:15:27,180 --> 00:15:31,470
actually trigger a read to foundation dB

00:15:29,550 --> 00:15:32,940
but the first layer doesn't and most of

00:15:31,470 --> 00:15:34,470
the time nothing's changed and we go on

00:15:32,940 --> 00:15:36,990
our merry way it's a nice little

00:15:34,470 --> 00:15:40,320
improvement for us at least so thanks

00:15:36,990 --> 00:15:41,700
for that there's more stuff on the data

00:15:40,320 --> 00:15:43,230
modeling Garen's got a talk this

00:15:41,700 --> 00:15:44,550
afternoon specifically on how we did our

00:15:43,230 --> 00:15:45,510
secondary indexes and kind of the

00:15:44,550 --> 00:15:49,350
different options we had there for

00:15:45,510 --> 00:15:51,030
search indexes and all kinds of stuff so

00:15:49,350 --> 00:15:54,840
if you're interested in that stuff

00:15:51,030 --> 00:15:56,130
definitely attend his session I spend a

00:15:54,840 --> 00:15:58,110
little bit of time on how the deployment

00:15:56,130 --> 00:16:01,140
looks I mentioned that we wanted to be

00:15:58,110 --> 00:16:04,140
able to scale up and scale down right so

00:16:01,140 --> 00:16:06,060
you know in couchdb for todo we still

00:16:04,140 --> 00:16:07,650
have the simple situation of I can run

00:16:06,060 --> 00:16:08,700
CouchDB with an embedded foundation DB

00:16:07,650 --> 00:16:10,650
everything's great

00:16:08,700 --> 00:16:12,180
I can run two of those things in two

00:16:10,650 --> 00:16:15,120
different regions I can set up

00:16:12,180 --> 00:16:16,410
replication across them still just like

00:16:15,120 --> 00:16:18,720
you were using gudge DB in the past

00:16:16,410 --> 00:16:20,250
right including all kinds of crazy

00:16:18,720 --> 00:16:21,660
topologies you know you want to set up a

00:16:20,250 --> 00:16:23,160
ring across five different regions where

00:16:21,660 --> 00:16:24,780
each one's replicating to its peer and

00:16:23,160 --> 00:16:25,260
the next one you can still do that sort

00:16:24,780 --> 00:16:27,150
of stuff

00:16:25,260 --> 00:16:28,650
but where it gets fun right is

00:16:27,150 --> 00:16:31,080
disaggregating this stuff a little bit

00:16:28,650 --> 00:16:32,280
and actually having Foundation DB do you

00:16:31,080 --> 00:16:33,810
know the stuff that it's good at in

00:16:32,280 --> 00:16:35,730
terms of giving me a strictly

00:16:33,810 --> 00:16:39,330
serializable consistent scale out

00:16:35,730 --> 00:16:40,830
underlying key value store and then the

00:16:39,330 --> 00:16:42,150
stuff that I have on top you know my

00:16:40,830 --> 00:16:44,760
layer that implements the CouchDB

00:16:42,150 --> 00:16:47,010
interface this is stateless right so I

00:16:44,760 --> 00:16:48,540
get to scale that out nicely and I get

00:16:47,010 --> 00:16:50,610
to take advantage of all the fun stuff

00:16:48,540 --> 00:16:52,350
that the folks in the kubernetes land

00:16:50,610 --> 00:16:54,870
are doing to make stateless application

00:16:52,350 --> 00:16:56,790
development in the cloud pleasant I am I

00:16:54,870 --> 00:16:57,960
think we're contractually obligated to

00:16:56,790 --> 00:17:00,290
this conference to talk about micro

00:16:57,960 --> 00:17:03,480
services so here we are micro services

00:17:00,290 --> 00:17:04,620
we've taken the steps to decompose some

00:17:03,480 --> 00:17:06,170
of the bits of the functionality that

00:17:04,620 --> 00:17:08,400
we're doing in CouchDB so now the

00:17:06,170 --> 00:17:10,650
JavaScript execution engine can go off

00:17:08,400 --> 00:17:12,930
into its own service the replicator can

00:17:10,650 --> 00:17:14,520
go off into its own service and we get

00:17:12,930 --> 00:17:15,720
to basically do all the fun stuff like I

00:17:14,520 --> 00:17:22,560
said that that goes along with

00:17:15,720 --> 00:17:24,240
kubernetes on the foundation DB side we

00:17:22,560 --> 00:17:26,880
also have a bunch of stuff we can do to

00:17:24,240 --> 00:17:30,660
take advantage of the scale of you know

00:17:26,880 --> 00:17:32,760
large compute infrastructure the way we

00:17:30,660 --> 00:17:34,140
are running this at the moment is in

00:17:32,760 --> 00:17:36,450
what's called the three data Hall mode

00:17:34,140 --> 00:17:39,960
so we map foundation DB's concept of a

00:17:36,450 --> 00:17:42,420
data hall to a cloud availability zone

00:17:39,960 --> 00:17:44,490
the way this system works is it does

00:17:42,420 --> 00:17:46,440
replicate every key value pair that you

00:17:44,490 --> 00:17:49,380
put in a foundation DB into each of

00:17:46,440 --> 00:17:51,720
those availability zones but the rights

00:17:49,380 --> 00:17:54,420
are actually replicated two times in

00:17:51,720 --> 00:17:55,440
each of two availability zones the

00:17:54,420 --> 00:17:57,060
reason it does this is because

00:17:55,440 --> 00:17:58,560
foundation DB has to have every

00:17:57,060 --> 00:17:59,820
transaction log that's configured to

00:17:58,560 --> 00:18:02,130
accept a particular key value pair

00:17:59,820 --> 00:18:04,050
except the right in order to commit and

00:18:02,130 --> 00:18:05,310
if you spread those across three

00:18:04,050 --> 00:18:06,450
availability zones now you'd be in

00:18:05,310 --> 00:18:08,730
trouble if one of those availability

00:18:06,450 --> 00:18:10,710
zones went down so instead we store two

00:18:08,730 --> 00:18:12,000
copies in each of two zones that means

00:18:10,710 --> 00:18:13,260
we can lose an AZ and if we've got

00:18:12,000 --> 00:18:15,030
sufficient compute capacity and the

00:18:13,260 --> 00:18:19,070
other AZ is we can lose a server in each

00:18:15,030 --> 00:18:21,150
of those and we can still keep on moving

00:18:19,070 --> 00:18:22,380
we're still experimenting with some of

00:18:21,150 --> 00:18:23,490
this stuff as we go through performance

00:18:22,380 --> 00:18:25,410
optimizations but one of the other

00:18:23,490 --> 00:18:29,190
things that we're starting to think may

00:18:25,410 --> 00:18:32,390
be important is nudging the stateless

00:18:29,190 --> 00:18:34,710
transaction processing processes into a

00:18:32,390 --> 00:18:35,730
availability zone because they do have

00:18:34,710 --> 00:18:37,860
to do a fair amount of communication

00:18:35,730 --> 00:18:39,750
with one another at the beginning and

00:18:37,860 --> 00:18:41,580
ends of transactions and so making that

00:18:39,750 --> 00:18:46,710
communication a fairly low latency

00:18:41,580 --> 00:18:47,790
operation seems to be important and then

00:18:46,710 --> 00:18:50,130
finally the other thing is the

00:18:47,790 --> 00:18:51,210
coordinators the coordinators I to be

00:18:50,130 --> 00:18:53,210
perfectly honest with you they they

00:18:51,210 --> 00:18:55,470
worry me a little bit they scare me like

00:18:53,210 --> 00:18:58,440
that's your state like you lose that

00:18:55,470 --> 00:19:00,360
you're toast so we are keeping those

00:18:58,440 --> 00:19:01,800
spread nicely across lots of different

00:19:00,360 --> 00:19:04,980
data centers and keeping them sort of

00:19:01,800 --> 00:19:06,930
far away from the action right you don't

00:19:04,980 --> 00:19:07,980
have to do that but I personally feel a

00:19:06,930 --> 00:19:09,630
little bit better about having those

00:19:07,980 --> 00:19:11,310
things kind of off in a quiet corner of

00:19:09,630 --> 00:19:12,630
the infrastructure and not in a place

00:19:11,310 --> 00:19:16,890
where they're in the data path for like

00:19:12,630 --> 00:19:19,260
lots of unexpected customer traffic and

00:19:16,890 --> 00:19:21,660
so that's kind of our or our design our

00:19:19,260 --> 00:19:26,220
deployment architecture where are we in

00:19:21,660 --> 00:19:27,750
this project actually well we've been at

00:19:26,220 --> 00:19:29,070
it like I said for close to a year we've

00:19:27,750 --> 00:19:30,780
implemented a bunch of the functionality

00:19:29,070 --> 00:19:32,430
of CouchDB not all of it but a lot of

00:19:30,780 --> 00:19:33,390
the big chunks and we've you know gone

00:19:32,430 --> 00:19:35,250
through a lot of the modeling gone

00:19:33,390 --> 00:19:36,900
through a lot of the implementation a

00:19:35,250 --> 00:19:38,400
lot of these things are v1 so there's

00:19:36,900 --> 00:19:41,640
some low-hanging performance fruit that

00:19:38,400 --> 00:19:42,990
we're working on addressing and to that

00:19:41,640 --> 00:19:44,640
end you know we've kind of turned our

00:19:42,990 --> 00:19:47,790
attention now to some operational

00:19:44,640 --> 00:19:50,250
hardening monitoring and so on bits of

00:19:47,790 --> 00:19:51,710
work one of the things we've started to

00:19:50,250 --> 00:19:53,380
do is because we've got like a

00:19:51,710 --> 00:19:55,360
distributed system

00:19:53,380 --> 00:19:57,820
problem here to a greater extent than we

00:19:55,360 --> 00:19:59,980
used to is we've picked up more tracing

00:19:57,820 --> 00:20:01,539
technology right used to be we were

00:19:59,980 --> 00:20:02,590
running a distributed Erlang environment

00:20:01,539 --> 00:20:04,299
and we could just use the tracing

00:20:02,590 --> 00:20:06,130
functionality inside that one VM but now

00:20:04,299 --> 00:20:07,750
we're dealing with foundation DB and

00:20:06,130 --> 00:20:10,299
some other stateless stuff in the

00:20:07,750 --> 00:20:12,549
CouchDB side and getting an end and view

00:20:10,299 --> 00:20:14,080
of how the request is progressing

00:20:12,549 --> 00:20:15,280
through is something that we're finding

00:20:14,080 --> 00:20:17,470
pretty helpful certainly in the

00:20:15,280 --> 00:20:19,360
development phase where we just you know

00:20:17,470 --> 00:20:22,120
don't always know where we're

00:20:19,360 --> 00:20:23,470
introducing you know high latency

00:20:22,120 --> 00:20:26,049
operations doing out reads that we

00:20:23,470 --> 00:20:27,400
didn't need to do that sort of thing but

00:20:26,049 --> 00:20:28,570
ultimately this is something I'd like to

00:20:27,400 --> 00:20:30,820
be able to turn on you know at a

00:20:28,570 --> 00:20:33,070
sampling fraction in production as well

00:20:30,820 --> 00:20:36,070
just to kind of keep tabs on you know

00:20:33,070 --> 00:20:38,080
the latency of the system over time

00:20:36,070 --> 00:20:40,900
well we've technically done here is

00:20:38,080 --> 00:20:42,780
actually just taken the JSON trace files

00:20:40,900 --> 00:20:44,740
that come out of Foundation DB and

00:20:42,780 --> 00:20:46,929
post-process them into an open tracing

00:20:44,740 --> 00:20:48,640
compatible format our layer has open

00:20:46,929 --> 00:20:50,350
tracing stuff built in and so we can

00:20:48,640 --> 00:20:52,870
load that all into gagger or whatever

00:20:50,350 --> 00:20:55,059
and you know visualize things as we go

00:20:52,870 --> 00:20:56,830
but we found this to be a nice tool and

00:20:55,059 --> 00:20:59,620
an area where I think we want to drive

00:20:56,830 --> 00:21:01,210
some more integration because I do think

00:20:59,620 --> 00:21:03,210
it's you know getting that end nd was

00:21:01,210 --> 00:21:08,110
important

00:21:03,210 --> 00:21:09,640
what are we learned like I said this is

00:21:08,110 --> 00:21:11,470
a brownfield exercise this code base was

00:21:09,640 --> 00:21:15,250
not designed with foundation DB in mind

00:21:11,470 --> 00:21:18,159
and we find ourselves using as a result

00:21:15,250 --> 00:21:20,230
maybe more transactions than we intend

00:21:18,159 --> 00:21:22,360
from time to time you know you're sort

00:21:20,230 --> 00:21:24,429
of running through the request path and

00:21:22,360 --> 00:21:25,780
you know the code is you're flowing from

00:21:24,429 --> 00:21:27,669
code module code module and then you

00:21:25,780 --> 00:21:30,520
realize oh I need this extra little bit

00:21:27,669 --> 00:21:31,960
of data from the store and I just use my

00:21:30,520 --> 00:21:34,919
closure I wrap it in a transaction off I

00:21:31,960 --> 00:21:36,640
go and like not only is that slow

00:21:34,919 --> 00:21:37,929
relatively speaking it's also not

00:21:36,640 --> 00:21:39,610
correct you know to get a different

00:21:37,929 --> 00:21:41,919
version of the data store in the course

00:21:39,610 --> 00:21:44,440
of responding to one request from a

00:21:41,919 --> 00:21:48,669
CouchDB user but that's the beauty of it

00:21:44,440 --> 00:21:50,919
right is that I can you know very easily

00:21:48,669 --> 00:21:53,020
advocate for improving that situation

00:21:50,919 --> 00:21:54,520
because the correct way of working with

00:21:53,020 --> 00:21:56,440
foundation DB also happens to be the

00:21:54,520 --> 00:21:59,710
most performant way so that's very nice

00:21:56,440 --> 00:22:01,630
little you know detail of things it it's

00:21:59,710 --> 00:22:03,429
so often isn't the case right instead of

00:22:01,630 --> 00:22:05,260
making shortcuts for performance it's

00:22:03,429 --> 00:22:06,880
actually like the most performant way to

00:22:05,260 --> 00:22:10,149
use the store is to use it

00:22:06,880 --> 00:22:12,190
that way the other thing that's

00:22:10,149 --> 00:22:14,440
happening I think the one that that that

00:22:12,190 --> 00:22:15,850
if there's any aspect of this project

00:22:14,440 --> 00:22:18,070
that causes a little consternation

00:22:15,850 --> 00:22:19,630
amongst our community of users it's that

00:22:18,070 --> 00:22:21,940
we're getting more restrictive in the

00:22:19,630 --> 00:22:26,259
sort of data sizes and volumes that we

00:22:21,940 --> 00:22:28,870
allow to be stored and frankly this is a

00:22:26,259 --> 00:22:31,419
case of us taking our medicine we should

00:22:28,870 --> 00:22:32,889
have put these limits in years ago we

00:22:31,419 --> 00:22:34,299
didn't and we've been living with the

00:22:32,889 --> 00:22:35,860
fact that they're not really documented

00:22:34,299 --> 00:22:37,299
but the database won't really work very

00:22:35,860 --> 00:22:39,669
well if you store that much data in this

00:22:37,299 --> 00:22:42,129
place or index that many fields in that

00:22:39,669 --> 00:22:43,840
document and so now we're sort of using

00:22:42,129 --> 00:22:45,460
this frankly as a place to say look hey

00:22:43,840 --> 00:22:46,870
foundation VB has these limits on keys

00:22:45,460 --> 00:22:47,139
and values we're just you know our hands

00:22:46,870 --> 00:22:49,330
are tied

00:22:47,139 --> 00:22:51,519
they're not really if we wanted to work

00:22:49,330 --> 00:22:54,580
around them we could but we're using you

00:22:51,519 --> 00:23:00,639
know foundation D B's rigor in this

00:22:54,580 --> 00:23:01,779
space to good effect my slides are still

00:23:00,639 --> 00:23:04,139
showing on my laptop I don't know what's

00:23:01,779 --> 00:23:04,139
going on here

00:23:05,549 --> 00:23:13,000
anyway I'm essentially down to the end

00:23:08,919 --> 00:23:15,879
of my talks so we can go my closing

00:23:13,000 --> 00:23:18,039
thoughts you know they say in religion

00:23:15,879 --> 00:23:19,450
that like the faith of a convert is the

00:23:18,039 --> 00:23:21,370
strongest faith and after running

00:23:19,450 --> 00:23:23,919
eventually consistent systems for a

00:23:21,370 --> 00:23:29,620
decade like hallelujah transactions are

00:23:23,919 --> 00:23:30,909
awesome right the combination of the key

00:23:29,620 --> 00:23:33,100
value interface and transactions in

00:23:30,909 --> 00:23:34,899
foundation DB is simultaneously flexible

00:23:33,100 --> 00:23:37,330
enough for us to undertake a project

00:23:34,899 --> 00:23:41,559
like this and powerful enough to make it

00:23:37,330 --> 00:23:43,000
worthwhile to do so right all in the

00:23:41,559 --> 00:23:44,409
community side I'm hearing Alex it was

00:23:43,000 --> 00:23:45,879
great that we had the community update

00:23:44,409 --> 00:23:47,710
from Alex I think you know the thing

00:23:45,879 --> 00:23:49,620
that I've recognized running my own

00:23:47,710 --> 00:23:51,340
open-source project for a decade is that

00:23:49,620 --> 00:23:53,230
contributions come in all shapes and

00:23:51,340 --> 00:23:55,090
sizes we don't all have to go like learn

00:23:53,230 --> 00:23:56,769
flow from Marcus and dig into simulation

00:23:55,090 --> 00:23:59,080
and Joshua and so on in order to make

00:23:56,769 --> 00:24:00,340
meaningful contributions to foundation

00:23:59,080 --> 00:24:02,559
to me there's a ton we can do to support

00:24:00,340 --> 00:24:03,789
the project didn't you know sort of the

00:24:02,559 --> 00:24:05,559
periphery and a lot of that is happening

00:24:03,789 --> 00:24:08,860
but certainly something that I would

00:24:05,559 --> 00:24:10,600
echo is is is super useful and then

00:24:08,860 --> 00:24:12,480
finally I guess just you know enjoy the

00:24:10,600 --> 00:24:14,710
some and have a fun time here and

00:24:12,480 --> 00:24:15,909
absolutely hit me up if you have further

00:24:14,710 --> 00:24:18,430
questions about the work that we're

00:24:15,909 --> 00:24:26,449
doing I'd love to meet you all Thanks

00:24:18,430 --> 00:24:26,449

YouTube URL: https://www.youtube.com/watch?v=SjXyVZZFkBg


