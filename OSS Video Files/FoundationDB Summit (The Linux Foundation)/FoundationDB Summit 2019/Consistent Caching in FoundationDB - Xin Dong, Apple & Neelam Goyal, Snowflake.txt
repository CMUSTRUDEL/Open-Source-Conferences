Title: Consistent Caching in FoundationDB - Xin Dong, Apple & Neelam Goyal, Snowflake
Publication date: 2020-05-07
Playlist: FoundationDB Summit 2019
Description: 
	Consistent Caching in FoundationDB - Xin Dong, Apple & Neelam Goyal, Snowflake 

We’re presenting a new FoundationDB feature that is currently worked on as a collaboration project between Apple and Snowflake. If a key-range (or a single key) is very read-hot, the storage processes will start to run out of resources. Currently, the only mitigation for this is to either change the access pattern or implement a caching functionality on top of FoundationDB. Both of these are difficult to implement and often need special implementations depending on the consistency requirement. We’re currently working on a solution for hot read ranges that attacks this problem from two angles: (1) allow for some key-ranges to be held in memory and (2) increase the replication factor for read-hot ranges. (1) will take rad-load off disks while (2) will allow using more CPUs to handle the load. This cache will provide the same consistency guarantees as the rest of FoundationDB. The administrator will have the option to define ranges and replication factors for certain ranges and additionally, FoundationDB will detect ranges that are read-hot and not write-hot and will start to cache them automatically
Captions: 
	00:00:00,030 --> 00:00:06,359
I am going to get started so that we

00:00:04,680 --> 00:00:10,320
have more time for the other layer their

00:00:06,359 --> 00:00:12,540
stocks i'm shin dong from Apple I'm from

00:00:10,320 --> 00:00:14,040
the foundation DB their knee alumni are

00:00:12,540 --> 00:00:17,160
going to talk about this awesome

00:00:14,040 --> 00:00:19,289
collaborated project between f4 spoke on

00:00:17,160 --> 00:00:21,480
the topic of native consistent catching

00:00:19,289 --> 00:00:23,310
layering foundation DB let's get started

00:00:21,480 --> 00:00:25,650
with some motivation behind it so

00:00:23,310 --> 00:00:27,390
financially just like other systems

00:00:25,650 --> 00:00:30,210
foundation DB can suffer from the

00:00:27,390 --> 00:00:31,920
red-hot traffic to understand that

00:00:30,210 --> 00:00:35,250
problem let's imagine that we have this

00:00:31,920 --> 00:00:38,010
simple cluster we have a bunch of stars

00:00:35,250 --> 00:00:39,600
happily serving requests and let's

00:00:38,010 --> 00:00:43,050
imagine that there comes a bunch of

00:00:39,600 --> 00:00:46,170
clients who all are trying to access the

00:00:43,050 --> 00:00:49,950
same smoke here Inge that is hosted by

00:00:46,170 --> 00:00:52,649
this star server team and at the first

00:00:49,950 --> 00:00:55,050
when the traffic is still moderate the

00:00:52,649 --> 00:00:57,270
low request can be load balanced between

00:00:55,050 --> 00:00:59,640
other servers in that in that team quite

00:00:57,270 --> 00:01:02,039
nicely but as we increase the traffic

00:00:59,640 --> 00:01:05,129
that set of servers will be eventually

00:01:02,039 --> 00:01:09,000
overloaded and finally saturated because

00:01:05,129 --> 00:01:12,299
of the retouch art and a rehash are can

00:01:09,000 --> 00:01:14,400
be particularly a problem problematic in

00:01:12,299 --> 00:01:16,830
foundation TV because a red-hot start

00:01:14,400 --> 00:01:18,420
server not only slows down the read it

00:01:16,830 --> 00:01:19,770
also slows down the writes and

00:01:18,420 --> 00:01:21,810
eventually it will affect the overall

00:01:19,770 --> 00:01:28,740
performance the book of the whole

00:01:21,810 --> 00:01:30,720
cluster and because and also due to the

00:01:28,740 --> 00:01:34,829
fact that the read how traffic tend to

00:01:30,720 --> 00:01:36,390
be temporary and to help in at the the

00:01:34,829 --> 00:01:38,310
point of the situation there is

00:01:36,390 --> 00:01:40,710
virtually no possible way to keep up

00:01:38,310 --> 00:01:42,659
with the traffic without somehow we

00:01:40,710 --> 00:01:46,079
increase the replication number for that

00:01:42,659 --> 00:01:49,829
for that piece of the data then like

00:01:46,079 --> 00:01:52,140
that that's when the the end because of

00:01:49,829 --> 00:01:55,049
the nature that the rig traffic tend to

00:01:52,140 --> 00:01:58,439
temporary we do not necessarily need a

00:01:55,049 --> 00:02:02,450
durable wrap roughly replica for the for

00:01:58,439 --> 00:02:05,100
that piece of data in memory temporary

00:02:02,450 --> 00:02:07,890
replicated data can help and that's when

00:02:05,100 --> 00:02:10,020
the cache comes into play now let me

00:02:07,890 --> 00:02:12,290
have to hand over to Neelam to tell you

00:02:10,020 --> 00:02:13,760
more about that

00:02:12,290 --> 00:02:16,010
Thank You shin good afternoon everyone

00:02:13,760 --> 00:02:17,720
I'm Neelam from snowflake and we're

00:02:16,010 --> 00:02:19,970
excited to talk about the caching here

00:02:17,720 --> 00:02:21,530
today so now that Sheena has can we

00:02:19,970 --> 00:02:23,480
ensure that we do need a caching

00:02:21,530 --> 00:02:24,950
solution for foundation levy I'm here to

00:02:23,480 --> 00:02:26,989
talk walk you through some of the

00:02:24,950 --> 00:02:28,760
solutions so before we actually talk

00:02:26,989 --> 00:02:30,650
about the native consitent caching

00:02:28,760 --> 00:02:32,540
within foundation maybe you might ask

00:02:30,650 --> 00:02:35,030
like why not use an application manage

00:02:32,540 --> 00:02:37,400
cash like memcache tea order Aries

00:02:35,030 --> 00:02:39,230
radish slabs between application and a

00:02:37,400 --> 00:02:40,760
foundation DB why not just use that in

00:02:39,230 --> 00:02:43,220
sort of implementing caching within

00:02:40,760 --> 00:02:44,780
foundation DB so while that is a

00:02:43,220 --> 00:02:46,940
possible solution is strong not really a

00:02:44,780 --> 00:02:48,560
good one because the entire burden of

00:02:46,940 --> 00:02:50,389
managing that cache is actually on the

00:02:48,560 --> 00:02:52,040
application and that's actually a very

00:02:50,389 --> 00:02:53,599
complex problem because the application

00:02:52,040 --> 00:02:55,010
needs to worry about the consistency and

00:02:53,599 --> 00:02:57,230
the coherency of that cache and

00:02:55,010 --> 00:02:59,599
availability and complexity in general

00:02:57,230 --> 00:03:00,920
becomes like really questionable so even

00:02:59,599 --> 00:03:02,180
though it's a viable solution it's not

00:03:00,920 --> 00:03:05,659
really a good one

00:03:02,180 --> 00:03:07,639
with that we will move on to what we are

00:03:05,659 --> 00:03:10,400
proposing and like that's a much better

00:03:07,639 --> 00:03:13,129
solution than just adding a side cache

00:03:10,400 --> 00:03:14,989
so which is to actually implement the

00:03:13,129 --> 00:03:18,919
caching functionality within foundation

00:03:14,989 --> 00:03:20,959
BB so as we know that the reason from

00:03:18,919 --> 00:03:22,819
storage servers so this cache will

00:03:20,959 --> 00:03:25,849
actually said in front of towards those

00:03:22,819 --> 00:03:27,620
storage servers and consistent caching

00:03:25,849 --> 00:03:30,560
actually attacks a problem from really

00:03:27,620 --> 00:03:32,090
two angles one it allows certain hotkey

00:03:30,560 --> 00:03:34,430
ranges to be held in memory like she

00:03:32,090 --> 00:03:36,169
pointed out and it also allows us to

00:03:34,430 --> 00:03:39,859
increase our application factor for

00:03:36,169 --> 00:03:41,750
those who read hot ranges so one the

00:03:39,859 --> 00:03:43,699
Lion get to be held in memory it

00:03:41,750 --> 00:03:46,190
basically takes and takes a load off of

00:03:43,699 --> 00:03:48,590
disks so it makes I mean it also reduces

00:03:46,190 --> 00:03:49,819
the real a Nancy and second the increase

00:03:48,590 --> 00:03:51,530
in your application factor basically

00:03:49,819 --> 00:03:53,629
allows us to throw more CPUs at a

00:03:51,530 --> 00:03:55,370
problem so the load can be basically

00:03:53,629 --> 00:03:57,980
managed by more CPUs so both of these

00:03:55,370 --> 00:04:00,340
factors combined together also leads to

00:03:57,980 --> 00:04:02,900
low latency which is also a good thing

00:04:00,340 --> 00:04:04,639
so now since this cache is completely

00:04:02,900 --> 00:04:07,069
implemented within foundation maybe it

00:04:04,639 --> 00:04:10,310
has the same consistency guarantees as

00:04:07,069 --> 00:04:12,680
rest of the foundation dB so now let's

00:04:10,310 --> 00:04:14,870
look at how all this might look within

00:04:12,680 --> 00:04:17,419
foundation do you be like you know block

00:04:14,870 --> 00:04:21,289
diagram so we have we have the

00:04:17,419 --> 00:04:22,880
foundation BB client and the tea logs so

00:04:21,289 --> 00:04:25,159
the foundation DB client will continue

00:04:22,880 --> 00:04:25,970
writing into the tea logs so I mean even

00:04:25,159 --> 00:04:27,710
though there

00:04:25,970 --> 00:04:29,510
like caching layer somewhere within

00:04:27,710 --> 00:04:32,570
foundation DB the rights continue to go

00:04:29,510 --> 00:04:34,700
to the T logs we have I mean like today

00:04:32,570 --> 00:04:36,440
we have the storage servers but now in

00:04:34,700 --> 00:04:38,750
addition to the storage servers we might

00:04:36,440 --> 00:04:40,040
we will also have storage cache servers

00:04:38,750 --> 00:04:42,710
which basically implement a caching

00:04:40,040 --> 00:04:44,420
functionality and they both will now

00:04:42,710 --> 00:04:46,100
start pulling mutations from the T logs

00:04:44,420 --> 00:04:47,540
so along with the storage servers we

00:04:46,100 --> 00:04:50,150
have the storage cache servers pulling

00:04:47,540 --> 00:04:52,810
mutations from the T logs and the FTP

00:04:50,150 --> 00:04:55,160
client can issue reads from both the

00:04:52,810 --> 00:04:56,750
storage cache servers or the storage

00:04:55,160 --> 00:05:00,770
servers depending on whether a key range

00:04:56,750 --> 00:05:02,630
is been cache or not so just mind you

00:05:00,770 --> 00:05:04,160
just reminding you that make it making

00:05:02,630 --> 00:05:05,930
it clear that it's all happening within

00:05:04,160 --> 00:05:08,510
foundation DB with basically nothing

00:05:05,930 --> 00:05:11,420
visible to the outside application other

00:05:08,510 --> 00:05:13,070
than the benefit that is gonna see so

00:05:11,420 --> 00:05:14,870
now I'm like from this I would like to

00:05:13,070 --> 00:05:16,130
zoom in a little bit on two of the main

00:05:14,870 --> 00:05:18,919
components in the next couple of slides

00:05:16,130 --> 00:05:21,320
which is the storage cache cache servers

00:05:18,919 --> 00:05:23,890
which are new and the changes that we

00:05:21,320 --> 00:05:26,570
need to make in the foundation DB client

00:05:23,890 --> 00:05:28,940
so first a storage cash flow so we are

00:05:26,570 --> 00:05:30,380
bringing up a new storage cache hole

00:05:28,940 --> 00:05:32,419
which is going to be stateless and

00:05:30,380 --> 00:05:33,950
ephemeral so what it really means is

00:05:32,419 --> 00:05:36,320
that it's not going to remember any

00:05:33,950 --> 00:05:38,540
stage persistently and it's not going to

00:05:36,320 --> 00:05:40,310
do replies any data so if your storage

00:05:38,540 --> 00:05:42,530
cache server dies it's going to come

00:05:40,310 --> 00:05:44,180
back as a brand new process and in that

00:05:42,530 --> 00:05:46,550
process it might even be responsible for

00:05:44,180 --> 00:05:48,979
a completely different key range so it's

00:05:46,550 --> 00:05:50,660
completely ephemeral and stateless so at

00:05:48,979 --> 00:05:52,340
a very high level what what is the

00:05:50,660 --> 00:05:54,140
storage that we're gonna hold you so it

00:05:52,340 --> 00:05:57,110
basically will establish a key range

00:05:54,140 --> 00:05:58,640
that it is responsible for it will pull

00:05:57,110 --> 00:06:01,280
mutations from the Qi logs it will

00:05:58,640 --> 00:06:02,720
filter out irrelevant mutations applies

00:06:01,280 --> 00:06:06,020
those mutations and serve the ket

00:06:02,720 --> 00:06:07,490
request so you might ask but isn't it

00:06:06,020 --> 00:06:10,790
very similar to a storage server role

00:06:07,490 --> 00:06:12,650
actually it is that's exactly the case

00:06:10,790 --> 00:06:14,960
but it is far simpler because we are not

00:06:12,650 --> 00:06:18,020
utilizing any data and that basically

00:06:14,960 --> 00:06:19,940
makes life a whole lot simpler so now

00:06:18,020 --> 00:06:21,380
without delving into too much detail I

00:06:19,940 --> 00:06:23,600
would like to zoom in into the second

00:06:21,380 --> 00:06:26,900
piece which is the foundation DB client

00:06:23,600 --> 00:06:29,390
side changes so we'll have the client

00:06:26,900 --> 00:06:31,520
API to configure the storage cache which

00:06:29,390 --> 00:06:33,320
is basically you can add or remove or

00:06:31,520 --> 00:06:35,419
certain key ranges to the cache and

00:06:33,320 --> 00:06:39,440
we'll have the ability to specify the

00:06:35,419 --> 00:06:42,050
application factor and in addition

00:06:39,440 --> 00:06:43,850
there is so the client basically cashes

00:06:42,050 --> 00:06:45,440
certain metadata about storage servers

00:06:43,850 --> 00:06:47,480
today so that it knows which storage

00:06:45,440 --> 00:06:50,480
server to go to whenever a read query

00:06:47,480 --> 00:06:52,490
comes in so now with additional storage

00:06:50,480 --> 00:06:54,980
cache servers it also is going to

00:06:52,490 --> 00:06:56,390
maintain metadata information about

00:06:54,980 --> 00:06:58,430
those cache server similar to the

00:06:56,390 --> 00:07:00,440
storage servers so that based on that

00:06:58,430 --> 00:07:02,360
metadata information it can direct

00:07:00,440 --> 00:07:05,060
queries either to the storage servers or

00:07:02,360 --> 00:07:06,950
the cache servers and now as you can

00:07:05,060 --> 00:07:09,770
imagine this metadata cash could be

00:07:06,950 --> 00:07:12,410
stale so for instance just let's just

00:07:09,770 --> 00:07:14,750
assume that a new cache role has come up

00:07:12,410 --> 00:07:17,810
and the proxy or the client doesn't even

00:07:14,750 --> 00:07:19,610
know about it so it might just continue

00:07:17,810 --> 00:07:22,130
sending his cache request to the storage

00:07:19,610 --> 00:07:23,510
servers but in this case with addition

00:07:22,130 --> 00:07:25,520
of cashews the storage server is gonna

00:07:23,510 --> 00:07:27,020
recognize that this key range is being

00:07:25,520 --> 00:07:29,330
cared so it's gonna serve the query but

00:07:27,020 --> 00:07:31,130
let the clients know that there is a

00:07:29,330 --> 00:07:33,050
cache you know for this particular key

00:07:31,130 --> 00:07:34,910
range so the client will then update

00:07:33,050 --> 00:07:37,010
system metadata cache and then it will

00:07:34,910 --> 00:07:38,990
start sending those queries for those

00:07:37,010 --> 00:07:41,060
key ranges to the cache server instead

00:07:38,990 --> 00:07:43,340
of the storage server so this is like a

00:07:41,060 --> 00:07:46,520
very very high level the changes that

00:07:43,340 --> 00:07:48,110
are being made to the client side so

00:07:46,520 --> 00:07:50,300
what are the implications of all is for

00:07:48,110 --> 00:07:51,620
the applications basically none and that

00:07:50,300 --> 00:07:53,690
is the whole point of this project that

00:07:51,620 --> 00:07:55,190
we do not want to burden the application

00:07:53,690 --> 00:07:58,130
they should just be able to benefit from

00:07:55,190 --> 00:07:59,900
this without any serious consequences

00:07:58,130 --> 00:08:02,090
the cache has been completely managed

00:07:59,900 --> 00:08:03,770
from within foundation DB and it's going

00:08:02,090 --> 00:08:05,600
to be completely transparent except the

00:08:03,770 --> 00:08:07,130
accept that is going to have more to

00:08:05,600 --> 00:08:10,400
configure the cache and it's gonna see

00:08:07,130 --> 00:08:13,070
the added benefit for the reads now

00:08:10,400 --> 00:08:15,260
moving over the last piece for my part

00:08:13,070 --> 00:08:17,150
which is the cache configuration modes

00:08:15,260 --> 00:08:19,130
so first we'll have the manual mode

00:08:17,150 --> 00:08:20,750
where we will have where the application

00:08:19,130 --> 00:08:23,060
will have the option to define the key

00:08:20,750 --> 00:08:24,680
ranges that must be cached and a

00:08:23,060 --> 00:08:26,750
corresponding application factor and

00:08:24,680 --> 00:08:28,730
also in addition to the manual mode

00:08:26,750 --> 00:08:30,740
we'll have an automatic mode where

00:08:28,730 --> 00:08:32,780
foundation DB will detect the hot key

00:08:30,740 --> 00:08:34,940
ranges automatically and then it will

00:08:32,780 --> 00:08:38,000
start to cache them as well so you might

00:08:34,940 --> 00:08:39,860
ask if we have the automatic detection

00:08:38,000 --> 00:08:41,780
and caching why do we even care about

00:08:39,860 --> 00:08:43,340
the manual mode so the manual mode

00:08:41,780 --> 00:08:44,810
actually gives us a lot of flexibility

00:08:43,340 --> 00:08:46,880
for instance there might be some key

00:08:44,810 --> 00:08:49,400
ranges that you just want low latency

00:08:46,880 --> 00:08:50,860
for so they might not be hot key ranges

00:08:49,400 --> 00:08:52,360
to begin with so if they are not hot

00:08:50,860 --> 00:08:54,250
automatic detection

00:08:52,360 --> 00:08:55,930
is not gonna detect those killings as

00:08:54,250 --> 00:08:58,300
being hard and they are not going to be

00:08:55,930 --> 00:09:01,420
cached automatically so we want to give

00:08:58,300 --> 00:09:03,430
applications an option to be able to

00:09:01,420 --> 00:09:04,950
cache any key range they want manually

00:09:03,430 --> 00:09:08,079
as well so that is what a manual mode

00:09:04,950 --> 00:09:09,640
gives us so from this I will likely hand

00:09:08,079 --> 00:09:10,920
over to Shane to talk about the

00:09:09,640 --> 00:09:15,670
automatic detection and cash management

00:09:10,920 --> 00:09:17,529
included on thank you thanks you so that

00:09:15,670 --> 00:09:19,329
now that we learned how the cache works

00:09:17,529 --> 00:09:21,010
and how we can manually spin them up

00:09:19,329 --> 00:09:23,500
let's talk about automatic detection and

00:09:21,010 --> 00:09:26,649
the catch management for this part we're

00:09:23,500 --> 00:09:29,019
going to reuse some of the existing

00:09:26,649 --> 00:09:31,959
metric reporting framework in in the in

00:09:29,019 --> 00:09:34,000
a cluster and the oddmund go of that is

00:09:31,959 --> 00:09:36,940
to if they should find an efficient way

00:09:34,000 --> 00:09:40,390
to marry the candidate key range and

00:09:36,940 --> 00:09:42,370
available catch together we can further

00:09:40,390 --> 00:09:43,899
divide that into two parts the first the

00:09:42,370 --> 00:09:45,550
first part is the automatic detection

00:09:43,899 --> 00:09:49,180
the second part is the man the catch

00:09:45,550 --> 00:09:51,310
management for the first part right now

00:09:49,180 --> 00:09:53,529
as I said like each star server already

00:09:51,310 --> 00:09:55,180
keeps some simple two statistics about

00:09:53,529 --> 00:09:57,550
all of requests it served during the

00:09:55,180 --> 00:09:59,980
period of time and whenever star Server

00:09:57,550 --> 00:10:02,170
finds a char becomes read hot it'll

00:09:59,980 --> 00:10:04,839
notify a single in turn process in the

00:10:02,170 --> 00:10:07,870
cluster about that about this and that

00:10:04,839 --> 00:10:09,370
cluster I mean single engine processing

00:10:07,870 --> 00:10:12,970
in cluster and that process will then

00:10:09,370 --> 00:10:15,130
contact the start server to figure out

00:10:12,970 --> 00:10:17,829
in that read hot chart which Kieran has

00:10:15,130 --> 00:10:19,390
the highest rate density so it all try

00:10:17,829 --> 00:10:22,660
to do the catch in a finer granularity

00:10:19,390 --> 00:10:24,519
than catching a hot shot and then for

00:10:22,660 --> 00:10:26,860
the cache management this same process

00:10:24,519 --> 00:10:28,540
will not only tracks all star service it

00:10:26,860 --> 00:10:30,850
also tracks all the cash flows in the

00:10:28,540 --> 00:10:33,040
cluster for further resource consumption

00:10:30,850 --> 00:10:36,310
and as it all has the knowledge to know

00:10:33,040 --> 00:10:38,920
how to like when and where to put the

00:10:36,310 --> 00:10:41,250
key range in to catch and also manage

00:10:38,920 --> 00:10:43,870
the lifetime of the data in the catch

00:10:41,250 --> 00:10:45,370
now with all that we really believe that

00:10:43,870 --> 00:10:47,260
this feature will be one of the most

00:10:45,370 --> 00:10:49,959
most exciting features in the upcoming

00:10:47,260 --> 00:10:51,670
releases of Spanish Navy because it adds

00:10:49,959 --> 00:10:53,949
a whole new dimension to the broad

00:10:51,670 --> 00:10:56,600
product and thus will allow for new use

00:10:53,949 --> 00:11:01,249
cases and thank you that's it

00:10:56,600 --> 00:11:01,249

YouTube URL: https://www.youtube.com/watch?v=LaoYkonjwrM


