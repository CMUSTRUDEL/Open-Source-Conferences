Title: FoundationDB Record Layer: Open Source Structured Storage on FoundationDB - Nicholas Schiefer, Apple
Publication date: 2020-05-07
Playlist: FoundationDB Summit 2019
Description: 
	FoundationDB Record Layer: Open Source Structured Storage on FoundationDB - Nicholas Schiefer, Apple 

The FoundationDB Record Layer is an Apache-licensed open source library for managing record-oriented data stores with semantics similar to a relational database on top of the FoundationDB key-value store. It provides a lightweight, highly extensible way to store structured data, including tools for schema management and a rich set of query and indexing features. The Record Layer is a stateless library built for massively multi-tenant cloud environments with a number of features that make it uniquely suited to hosting a large number of databases. As the most mature FoundationDB layer, the Record Layer is a core component of CloudKit, Apple's cloud structured storage system. The Record layer is actively developed by a team at Apple and has a variety of ongoing projects with opportunities for community members to contribute
Captions: 
	00:00:00,030 --> 00:00:07,049
all right great so to kick off the last

00:00:04,380 --> 00:00:09,300
session I'm Nicolas from the record

00:00:07,049 --> 00:00:11,010
layer team at Apple and today I'm

00:00:09,300 --> 00:00:12,990
delighted to be able to tell you a bit

00:00:11,010 --> 00:00:14,340
about the foundation to be record layer

00:00:12,990 --> 00:00:18,060
how it works and why you might want to

00:00:14,340 --> 00:00:21,840
use it so the foundation DP record layer

00:00:18,060 --> 00:00:24,359
is succinctly a foundation DB layer with

00:00:21,840 --> 00:00:25,560
a relational style data model that means

00:00:24,359 --> 00:00:28,650
that it's stores and retrieves

00:00:25,560 --> 00:00:31,109
structured records much like tuples in a

00:00:28,650 --> 00:00:33,270
sequel database or maybe documents in a

00:00:31,109 --> 00:00:35,660
document store and those records have a

00:00:33,270 --> 00:00:38,309
schema which you define ahead of time

00:00:35,660 --> 00:00:40,350
with that schema facility it includes

00:00:38,309 --> 00:00:42,660
extensive support for schema migrations

00:00:40,350 --> 00:00:45,360
for you to add modify the schema build

00:00:42,660 --> 00:00:47,969
new indexes things like that and as I

00:00:45,360 --> 00:00:49,379
alluded to there's a rich set of indexes

00:00:47,969 --> 00:00:51,390
all of which are transactionally

00:00:49,379 --> 00:00:54,750
maintained with writes and updates so

00:00:51,390 --> 00:00:57,480
you never read stale data it also comes

00:00:54,750 --> 00:00:58,859
with a query planner and a pretty pretty

00:00:57,480 --> 00:01:00,510
well tuned to asynchronous query

00:00:58,859 --> 00:01:01,649
execution engine which I'll get a chance

00:01:00,510 --> 00:01:04,769
to talk a little bit more about later

00:01:01,649 --> 00:01:06,510
and we were delighted in January to

00:01:04,769 --> 00:01:08,490
release it under an Apache to open

00:01:06,510 --> 00:01:10,350
source license we develop it in the open

00:01:08,490 --> 00:01:13,799
on github calm and through the forum so

00:01:10,350 --> 00:01:15,990
you can join us there and it's developed

00:01:13,799 --> 00:01:18,330
at Apple in close collaboration with the

00:01:15,990 --> 00:01:20,460
core key-value team and so we have a

00:01:18,330 --> 00:01:22,920
opportunity to do a lot of interesting

00:01:20,460 --> 00:01:24,890
work that sort of spans the - which for

00:01:22,920 --> 00:01:27,240
example Alex highlighted earlier today

00:01:24,890 --> 00:01:30,150
to make it a little bit more concrete

00:01:27,240 --> 00:01:32,460
the record layer defines its schema and

00:01:30,150 --> 00:01:34,650
stores the data using protocol buffers

00:01:32,460 --> 00:01:36,720
defined by Google so on the right I have

00:01:34,650 --> 00:01:40,470
an example of maybe a state and a city

00:01:36,720 --> 00:01:41,670
inside and in general we're designed to

00:01:40,470 --> 00:01:43,259
work with protocol buffers so for

00:01:41,670 --> 00:01:45,270
example schema evolution follows

00:01:43,259 --> 00:01:47,040
protocol buffer guidelines that is you

00:01:45,270 --> 00:01:48,869
deprecated rather than removing fields

00:01:47,040 --> 00:01:50,490
and you never reuse field numbers and

00:01:48,869 --> 00:01:53,340
this provides a really seamless way to

00:01:50,490 --> 00:01:55,670
move from one schema to the other all in

00:01:53,340 --> 00:01:57,990
a sort of a well-defined way

00:01:55,670 --> 00:02:00,390
each of these messages defines what we

00:01:57,990 --> 00:02:02,670
call a record type so here state and

00:02:00,390 --> 00:02:05,159
city are both record types and the

00:02:02,670 --> 00:02:07,380
schema plus the any indexes you've

00:02:05,159 --> 00:02:09,690
defined on the schema form will be

00:02:07,380 --> 00:02:10,720
called the record metadata I'll refer

00:02:09,690 --> 00:02:12,580
back to that later

00:02:10,720 --> 00:02:14,170
so I mentioned this rich set of

00:02:12,580 --> 00:02:15,280
transactional indexes and these are

00:02:14,170 --> 00:02:17,740
really at the core of what the record

00:02:15,280 --> 00:02:19,420
layer does really well so we have these

00:02:17,740 --> 00:02:21,070
indexes that are maintained in the same

00:02:19,420 --> 00:02:22,840
transactions as record writes and

00:02:21,070 --> 00:02:26,800
updates so you never see out of date

00:02:22,840 --> 00:02:28,240
meted out of date data and we have like

00:02:26,800 --> 00:02:30,070
I say a pretty rich set of these so we

00:02:28,240 --> 00:02:31,630
have like pretty simple value indexes we

00:02:30,070 --> 00:02:33,940
have aggregate indexes lon

00:02:31,630 --> 00:02:37,210
things like sums we have ordinal rank

00:02:33,940 --> 00:02:39,340
indexes with full-text indexes indexes

00:02:37,210 --> 00:02:41,170
on the version stamp the commit version

00:02:39,340 --> 00:02:42,880
and so on and we have pretty

00:02:41,170 --> 00:02:45,070
sophisticated tools for combining these

00:02:42,880 --> 00:02:47,560
indexes including general records

00:02:45,070 --> 00:02:48,640
functions on records and more and so

00:02:47,560 --> 00:02:50,560
this is really the bread and butter of

00:02:48,640 --> 00:02:52,570
what the record player does and all of

00:02:50,560 --> 00:02:54,550
this is designed with efficiency in mind

00:02:52,570 --> 00:02:55,960
so we built this for our use case of

00:02:54,550 --> 00:02:57,910
Apple which includes supporting things

00:02:55,960 --> 00:03:00,700
like high concurrency workloads so

00:02:57,910 --> 00:03:03,220
everything's sort of designed in concert

00:03:00,700 --> 00:03:06,510
with F DB in order to support operating

00:03:03,220 --> 00:03:09,790
these at scale we have a pretty simple

00:03:06,510 --> 00:03:11,710
query planner we have a query language

00:03:09,790 --> 00:03:14,200
that supports defining things like

00:03:11,710 --> 00:03:16,420
predicate since sort order our planner

00:03:14,200 --> 00:03:18,280
sort of specializes in selecting the

00:03:16,420 --> 00:03:22,150
right index with an I'm sort of aimed at

00:03:18,280 --> 00:03:24,190
OLTP type workloads and this query

00:03:22,150 --> 00:03:26,080
execution engine I mentioned is really

00:03:24,190 --> 00:03:28,900
sort of the key part of getting good

00:03:26,080 --> 00:03:30,310
performance at a foundation DB mostly

00:03:28,900 --> 00:03:32,220
because foundation DB is really good at

00:03:30,310 --> 00:03:34,959
supporting high concurrency workloads

00:03:32,220 --> 00:03:38,110
but it's not necessarily lower latency

00:03:34,959 --> 00:03:39,670
per operation so since we released the

00:03:38,110 --> 00:03:41,800
record layer the number one question

00:03:39,670 --> 00:03:43,600
we've received on Hacker News and the

00:03:41,800 --> 00:03:45,930
forum's sort of everywhere right is is

00:03:43,600 --> 00:03:50,320
the record layer a sequel layer and

00:03:45,930 --> 00:03:52,360
unfortunately the answer is no and you

00:03:50,320 --> 00:03:55,000
know there are a couple of ways it's not

00:03:52,360 --> 00:03:57,370
a sequel layer right so one is that we

00:03:55,000 --> 00:03:59,290
don't have sequel as a query language we

00:03:57,370 --> 00:04:01,060
don't support the sequel data

00:03:59,290 --> 00:04:03,489
declaration and modification languages

00:04:01,060 --> 00:04:05,860
we have a somewhat different data model

00:04:03,489 --> 00:04:07,480
for example we have native support for

00:04:05,860 --> 00:04:09,310
like high-level types like nested

00:04:07,480 --> 00:04:13,390
records and you know things like maps

00:04:09,310 --> 00:04:15,070
and lists and we have it's been designed

00:04:13,390 --> 00:04:17,169
with a very particular workload in mind

00:04:15,070 --> 00:04:18,850
so we place pretty strict limits on like

00:04:17,169 --> 00:04:20,650
how much work you can do in each query

00:04:18,850 --> 00:04:22,220
request and this and that and all of

00:04:20,650 --> 00:04:24,080
that is designed to serve a soar

00:04:22,220 --> 00:04:26,900
really large-scale cloud workload which

00:04:24,080 --> 00:04:28,100
I'll get into later and in general think

00:04:26,900 --> 00:04:29,240
things are just different right like the

00:04:28,100 --> 00:04:31,370
indexes are different the periphery

00:04:29,240 --> 00:04:33,650
primitives are different like it's setup

00:04:31,370 --> 00:04:35,030
by default to do different things and so

00:04:33,650 --> 00:04:36,230
to sort of tell you about why I think

00:04:35,030 --> 00:04:38,000
the record layer is a really cool

00:04:36,230 --> 00:04:41,120
project for the foundation GB community

00:04:38,000 --> 00:04:43,010
I'd like you to imagine and be not too

00:04:41,120 --> 00:04:44,630
hard for many of you developing layers

00:04:43,010 --> 00:04:45,860
that you're trying to build some type of

00:04:44,630 --> 00:04:47,720
database and you can almost pick your

00:04:45,860 --> 00:04:50,300
favorite data model because it's not

00:04:47,720 --> 00:04:51,740
going to matter very much and so if you

00:04:50,300 --> 00:04:55,070
try to do this you know that building a

00:04:51,740 --> 00:04:56,150
database is hard so for example if

00:04:55,070 --> 00:04:57,620
you're trying to build a distributed

00:04:56,150 --> 00:04:59,120
database which you might you probably

00:04:57,620 --> 00:05:00,830
are these days you have to solve all

00:04:59,120 --> 00:05:02,420
kinds of nasty distributed systems

00:05:00,830 --> 00:05:03,680
problems right how are you gonna shard

00:05:02,420 --> 00:05:05,780
the data how are you going to make the

00:05:03,680 --> 00:05:07,190
whole thing fault tolerant how are you

00:05:05,780 --> 00:05:09,170
gonna deal with the consensus problems

00:05:07,190 --> 00:05:10,700
that arise right and of course you won't

00:05:09,170 --> 00:05:12,290
have to manage like highly concurrent

00:05:10,700 --> 00:05:13,430
workloads it's we're like long past the

00:05:12,290 --> 00:05:15,970
days where you can just lock the whole

00:05:13,430 --> 00:05:17,390
database right and so you know they're

00:05:15,970 --> 00:05:19,760
implementing multi-version concurrency

00:05:17,390 --> 00:05:21,620
control as hard transaction processing

00:05:19,760 --> 00:05:24,460
is hard getting this all to scale is

00:05:21,620 --> 00:05:27,560
also hard you know mind blowing lis hard

00:05:24,460 --> 00:05:28,820
and since you're here and you've heard a

00:05:27,560 --> 00:05:31,100
whole day of talks I can tell you're

00:05:28,820 --> 00:05:33,050
fully indoctrinated and so foundation DV

00:05:31,100 --> 00:05:34,880
right can help and in fact these are

00:05:33,050 --> 00:05:40,160
precisely the problems that foundation

00:05:34,880 --> 00:05:42,350
DB solves really really well sadly there

00:05:40,160 --> 00:05:43,729
are other problems that arise when you

00:05:42,350 --> 00:05:46,729
try and build a database right and

00:05:43,729 --> 00:05:48,140
Foundation DB solves the first two of

00:05:46,729 --> 00:05:50,479
these categories are really really well

00:05:48,140 --> 00:05:51,919
but that still leaves a large number of

00:05:50,479 --> 00:05:54,260
other problems which you have to solve

00:05:51,919 --> 00:05:55,970
one way or another right then so the

00:05:54,260 --> 00:05:57,169
question is like how are you still going

00:05:55,970 --> 00:05:59,780
to solve the rest of these hard problems

00:05:57,169 --> 00:06:01,790
and foundation D B's answer to that is

00:05:59,780 --> 00:06:03,500
layers right so layers are these

00:06:01,790 --> 00:06:05,720
higher-level data models they're

00:06:03,500 --> 00:06:07,790
libraries or services they're in an

00:06:05,720 --> 00:06:09,350
ideal world stateless or nearly so and

00:06:07,790 --> 00:06:12,169
they're easy to scale and we've seen

00:06:09,350 --> 00:06:13,880
this sort of explosion in interest in

00:06:12,169 --> 00:06:15,470
layers like you know from Apple develop

00:06:13,880 --> 00:06:17,660
layers layers developed in the community

00:06:15,470 --> 00:06:21,290
even really big layers like what we've

00:06:17,660 --> 00:06:22,580
seen with couch TV right and now I'm

00:06:21,290 --> 00:06:24,440
talking to the layer developers in the

00:06:22,580 --> 00:06:26,990
crowd who know that layer engineering is

00:06:24,440 --> 00:06:28,460
still really hard right so here are some

00:06:26,990 --> 00:06:30,260
some of those problems that I had listed

00:06:28,460 --> 00:06:31,340
there and it'll slightly larger font

00:06:30,260 --> 00:06:32,599
right so you still have to deal with

00:06:31,340 --> 00:06:34,520
things like schema maintenance

00:06:32,599 --> 00:06:35,780
enforcement you still have to deal with

00:06:34,520 --> 00:06:37,880
building indexes when you

00:06:35,780 --> 00:06:40,970
them you still have to deal with

00:06:37,880 --> 00:06:42,950
indexing including pretty complicated

00:06:40,970 --> 00:06:45,980
ones like we've had several people today

00:06:42,950 --> 00:06:47,510
talk about full-text indexing you still

00:06:45,980 --> 00:06:48,860
have to have a query optimizer of some

00:06:47,510 --> 00:06:50,780
sort and you have to try and get good

00:06:48,860 --> 00:06:53,540
performance out of this and we know

00:06:50,780 --> 00:06:54,950
that's hard and increasingly many of us

00:06:53,540 --> 00:06:57,800
have to solve what I would call called

00:06:54,950 --> 00:06:59,900
Nate multi-tenant cloud native using the

00:06:57,800 --> 00:07:01,070
name of our coast conference workloads

00:06:59,900 --> 00:07:02,840
right so this is where you have a whole

00:07:01,070 --> 00:07:05,090
bunch of different users with often very

00:07:02,840 --> 00:07:06,470
different requirements all trying to

00:07:05,090 --> 00:07:08,180
like share space without stepping on

00:07:06,470 --> 00:07:10,640
each other's toes right and you want to

00:07:08,180 --> 00:07:12,830
scale quickly and these are problems

00:07:10,640 --> 00:07:14,840
that are shared by like maybe not every

00:07:12,830 --> 00:07:16,070
layer but like at least majority of

00:07:14,840 --> 00:07:19,310
layers probably even the vast majority

00:07:16,070 --> 00:07:22,130
of layers and you might see where this

00:07:19,310 --> 00:07:24,440
is going this is a rough feature list

00:07:22,130 --> 00:07:25,850
for the record layer so this gives us

00:07:24,440 --> 00:07:29,090
sort of two ways of looking at the

00:07:25,850 --> 00:07:31,070
record layer one is to say that it's a

00:07:29,090 --> 00:07:34,040
foundation DB layer with a relational

00:07:31,070 --> 00:07:35,750
like data model usable on its own but

00:07:34,040 --> 00:07:38,120
I'll warn you it's a little rough it's

00:07:35,750 --> 00:07:39,890
not a full service database the other

00:07:38,120 --> 00:07:42,080
perspective is to view the record layer

00:07:39,890 --> 00:07:43,400
as sort of this middle layer with common

00:07:42,080 --> 00:07:45,890
primitives for building structured

00:07:43,400 --> 00:07:48,290
databases of many types with foundation

00:07:45,890 --> 00:07:50,930
DB and so I'll try and walk you through

00:07:48,290 --> 00:07:53,510
a couple of things and I'm going to

00:07:50,930 --> 00:07:57,229
focus in on three particular particular

00:07:53,510 --> 00:07:59,300
areas index maintenance high performance

00:07:57,229 --> 00:08:02,330
query execution on foundation DB and

00:07:59,300 --> 00:08:03,950
these multi me ten int workloads to try

00:08:02,330 --> 00:08:05,150
and give you a perspective on like how

00:08:03,950 --> 00:08:10,040
the record layer helps to solve these

00:08:05,150 --> 00:08:11,240
problems so building indexes this is a

00:08:10,040 --> 00:08:13,490
sort of the bread and butter of a

00:08:11,240 --> 00:08:14,900
database so here I suppose we have the a

00:08:13,490 --> 00:08:17,479
whole bunch of you know city records

00:08:14,900 --> 00:08:20,000
with a state name and the year that they

00:08:17,479 --> 00:08:21,500
were established and we're going to try

00:08:20,000 --> 00:08:23,300
and add we already have all this data

00:08:21,500 --> 00:08:25,250
and we're trying to add an index on the

00:08:23,300 --> 00:08:27,020
established year and then of course we

00:08:25,250 --> 00:08:28,400
want the ID included in the index so

00:08:27,020 --> 00:08:32,030
that we can like look up the primary

00:08:28,400 --> 00:08:33,770
record data so the way this is modeled

00:08:32,030 --> 00:08:34,640
in foundation DB is basically straight

00:08:33,770 --> 00:08:36,740
out of the cookbook and the

00:08:34,640 --> 00:08:38,570
documentation right so we're gonna store

00:08:36,740 --> 00:08:40,760
in the key we're gonna store the

00:08:38,570 --> 00:08:42,800
established year and ID as a tiebreaker

00:08:40,760 --> 00:08:43,729
so that we don't get conflicts and

00:08:42,800 --> 00:08:46,940
actually we're not going to put anything

00:08:43,729 --> 00:08:49,290
in the value right so the natural way to

00:08:46,940 --> 00:08:51,630
build those index is to just do a scan

00:08:49,290 --> 00:08:54,750
over the full range of the V of all the

00:08:51,630 --> 00:08:56,699
records right and then add one one of

00:08:54,750 --> 00:08:59,610
these keys as we go right so this seems

00:08:56,699 --> 00:09:01,290
like almost dead simple right and the

00:08:59,610 --> 00:09:02,699
fact that I'm talking about it tells you

00:09:01,290 --> 00:09:04,320
that it's not and so there all kinds of

00:09:02,699 --> 00:09:06,149
surprising challenges that you hit if

00:09:04,320 --> 00:09:08,399
you try and do this in practice

00:09:06,149 --> 00:09:10,649
so the first you'll hit and get

00:09:08,399 --> 00:09:12,569
frustrated by is that a lot of your

00:09:10,649 --> 00:09:14,310
datasets are larger than the FDB

00:09:12,569 --> 00:09:16,589
transaction size limit or maybe the

00:09:14,310 --> 00:09:18,899
indexing outruns the five seconds which

00:09:16,589 --> 00:09:21,029
is likely and so yeah you can break you

00:09:18,899 --> 00:09:22,790
it you'll break into smaller batches and

00:09:21,029 --> 00:09:25,949
that only sort of raises more questions

00:09:22,790 --> 00:09:27,240
how big should the batches be ideally

00:09:25,949 --> 00:09:29,250
you sort of determine this like

00:09:27,240 --> 00:09:31,649
dynamically somehow based on like what

00:09:29,250 --> 00:09:33,480
limits you're hitting and then there's

00:09:31,649 --> 00:09:35,339
the problem of sort of dealing with the

00:09:33,480 --> 00:09:37,199
fact that all of this is happening sort

00:09:35,339 --> 00:09:39,839
of while the circus continues around you

00:09:37,199 --> 00:09:45,779
so for example suppose that you're

00:09:39,839 --> 00:09:47,040
trying to there's a simultaneous so sort

00:09:45,779 --> 00:09:48,720
of imagine that that range of

00:09:47,040 --> 00:09:51,389
highlighted is the first batch that we

00:09:48,720 --> 00:09:53,819
decided to build and so we inserted

00:09:51,389 --> 00:09:55,949
we're inserting the various keys and in

00:09:53,819 --> 00:09:57,449
particular we read that and then at the

00:09:55,949 --> 00:09:59,220
same time you know sort of before we

00:09:57,449 --> 00:10:01,920
could finish reading somebody inserted

00:09:59,220 --> 00:10:03,779
Austin into the tape and into the into

00:10:01,920 --> 00:10:06,000
the database right and so this creates a

00:10:03,779 --> 00:10:07,110
transaction conflict where you know we

00:10:06,000 --> 00:10:08,459
wrote and since is something we were

00:10:07,110 --> 00:10:10,589
reading we know we need to be very

00:10:08,459 --> 00:10:12,360
careful about that and this is just sort

00:10:10,589 --> 00:10:14,760
of the first of all the different types

00:10:12,360 --> 00:10:16,500
of problems you can hit and in fact like

00:10:14,760 --> 00:10:18,420
this problem turns out to be like the

00:10:16,500 --> 00:10:21,449
problem that keeps on giving problems

00:10:18,420 --> 00:10:23,069
and so like our code to do this is

00:10:21,449 --> 00:10:24,720
actually like more than 2,000 lines with

00:10:23,069 --> 00:10:27,089
like thousands and thousands of lines of

00:10:24,720 --> 00:10:31,139
tests this is just one file like it's

00:10:27,089 --> 00:10:33,269
it's a lot of work and a surprisingly

00:10:31,139 --> 00:10:37,079
subtle and difficult problem even though

00:10:33,269 --> 00:10:39,930
it sounds really simple a synchronous

00:10:37,079 --> 00:10:42,660
query execution is really important in

00:10:39,930 --> 00:10:43,980
foundation TP because you want to get

00:10:42,660 --> 00:10:45,449
good performance out of it even though

00:10:43,980 --> 00:10:47,310
it's not optimized for super low

00:10:45,449 --> 00:10:49,110
individual agencies right so like the

00:10:47,310 --> 00:10:50,490
thing you want to avoid is suppose you

00:10:49,110 --> 00:10:54,149
know we're trying you're trying to do a

00:10:50,490 --> 00:10:55,980
query for all cities like I had that

00:10:54,149 --> 00:10:57,449
were established after 1600 right so you

00:10:55,980 --> 00:10:59,220
scan an index and then you get like a

00:10:57,449 --> 00:11:01,050
list of primary keys and you're just

00:10:59,220 --> 00:11:03,000
trying to like look those up and they

00:11:01,050 --> 00:11:04,710
like default way to do this would

00:11:03,000 --> 00:11:07,710
you just you know sort of issue a

00:11:04,710 --> 00:11:10,850
Request and get back a record do it

00:11:07,710 --> 00:11:13,620
again do it again and do it again right

00:11:10,850 --> 00:11:15,270
and the problem with this is that the

00:11:13,620 --> 00:11:18,090
total latency is going to be at least

00:11:15,270 --> 00:11:19,620
the sum of your individual agencies and

00:11:18,090 --> 00:11:21,030
that can be quite substantial if you

00:11:19,620 --> 00:11:24,600
know each one is millisecond or

00:11:21,030 --> 00:11:26,850
something like that so the way the

00:11:24,600 --> 00:11:28,410
record layer gets around this is that

00:11:26,850 --> 00:11:31,470
all the query operations are

00:11:28,410 --> 00:11:32,730
asynchronous and pipelined so we get the

00:11:31,470 --> 00:11:34,530
sort of list of primary keys from the

00:11:32,730 --> 00:11:36,150
record and then we issue a whole bunch

00:11:34,530 --> 00:11:37,410
of requests all to the foundation DB

00:11:36,150 --> 00:11:39,480
server at once and they can sort of come

00:11:37,410 --> 00:11:40,620
back in any order and then we'll start

00:11:39,480 --> 00:11:43,350
serving them as soon as they arrive on

00:11:40,620 --> 00:11:45,210
the client and so this model you know

00:11:43,350 --> 00:11:47,910
the total latency can be much less than

00:11:45,210 --> 00:11:49,350
some of the individual agencies but even

00:11:47,910 --> 00:11:51,090
you know again look simple in the

00:11:49,350 --> 00:11:53,790
diagram but there are some surprising

00:11:51,090 --> 00:11:55,440
challenges you hit most of all you hit

00:11:53,790 --> 00:11:58,050
the fact that asynchronous code is

00:11:55,440 --> 00:11:59,160
constitutionally evil and there's a

00:11:58,050 --> 00:12:00,270
sense in which all of you already

00:11:59,160 --> 00:12:01,860
believe this because you're here in

00:12:00,270 --> 00:12:03,450
Foundation TV conferences so you know

00:12:01,860 --> 00:12:06,600
that anything concurrent or distributed

00:12:03,450 --> 00:12:09,150
or anything like that is awful right so

00:12:06,600 --> 00:12:10,650
for example you know canceling work that

00:12:09,150 --> 00:12:12,990
you've like started in the pipeline is

00:12:10,650 --> 00:12:15,780
important and also really hard to do

00:12:12,990 --> 00:12:17,820
correctly it turns out you need like a

00:12:15,780 --> 00:12:19,650
rich suite of like asynchronous cursor

00:12:17,820 --> 00:12:21,450
implementations which are like terrible

00:12:19,650 --> 00:12:22,860
to implement and in particular they're

00:12:21,450 --> 00:12:24,180
terrible because they all need to be

00:12:22,860 --> 00:12:25,440
sort of minimally blocking right you

00:12:24,180 --> 00:12:27,960
want to do the minimum amount of work

00:12:25,440 --> 00:12:30,450
until you hit you know sort of whatever

00:12:27,960 --> 00:12:32,640
it was is that you were waiting for and

00:12:30,450 --> 00:12:34,380
then you want to yield immediately and

00:12:32,640 --> 00:12:37,470
doing this right turns out to be like

00:12:34,380 --> 00:12:39,720
like a really tough nut to crack and the

00:12:37,470 --> 00:12:41,310
record layer does it for you

00:12:39,720 --> 00:12:43,410
so the last thing I'll highlight is the

00:12:41,310 --> 00:12:45,120
sort of multi-tenant workloads which are

00:12:43,410 --> 00:12:48,089
sort of our bread and butter and how we

00:12:45,120 --> 00:12:49,200
use the record layer at Apple if you

00:12:48,089 --> 00:12:51,210
heard that Scott Ray's talked earlier

00:12:49,200 --> 00:12:52,920
today he talked a lot about records

00:12:51,210 --> 00:12:55,500
stores which are sort of the fundamental

00:12:52,920 --> 00:12:56,790
database unit in the record layer so the

00:12:55,500 --> 00:12:58,860
key thing about a record store is that

00:12:56,790 --> 00:13:00,480
it has all of the records but it also

00:12:58,860 --> 00:13:02,040
has all the indexes and even the

00:13:00,480 --> 00:13:04,320
metadata needed to maintain the database

00:13:02,040 --> 00:13:07,170
and so all of this lives in like one

00:13:04,320 --> 00:13:10,050
convenient bundle literally a key space

00:13:07,170 --> 00:13:12,660
in FDB and so you can sort of place it

00:13:10,050 --> 00:13:14,490
on an FTP cluster but then you can have

00:13:12,660 --> 00:13:15,010
many of these all up sharing the same

00:13:14,490 --> 00:13:17,860
foundation

00:13:15,010 --> 00:13:19,600
TV cluster and so this sort of creates

00:13:17,860 --> 00:13:22,210
this really nice picture for how you can

00:13:19,600 --> 00:13:23,710
build like cloud scale apps so you have

00:13:22,210 --> 00:13:25,990
foundation dpe with all your record

00:13:23,710 --> 00:13:27,760
stores and it takes care of like all the

00:13:25,990 --> 00:13:30,760
like hard distributed systems and

00:13:27,760 --> 00:13:32,260
concurrency control problems right and

00:13:30,760 --> 00:13:34,960
you don't have to take care of those and

00:13:32,260 --> 00:13:36,760
then at a higher level you have the

00:13:34,960 --> 00:13:38,710
record layer probably embedded in your

00:13:36,760 --> 00:13:40,510
application and it provides the like

00:13:38,710 --> 00:13:42,250
relational database features you know

00:13:40,510 --> 00:13:44,950
query planning and indexes and all that

00:13:42,250 --> 00:13:47,320
sort of stuff and you have this really

00:13:44,950 --> 00:13:49,120
nice divide where on the bottom you have

00:13:47,320 --> 00:13:50,830
your stateful storage and on the top you

00:13:49,120 --> 00:13:52,150
have your stateless compute and you can

00:13:50,830 --> 00:13:53,950
scale these independently of each other

00:13:52,150 --> 00:13:58,000
and there's this like really clean

00:13:53,950 --> 00:13:59,080
separation of concerns and so and and

00:13:58,000 --> 00:14:01,480
one of the nicest things you can do here

00:13:59,080 --> 00:14:03,190
is that any record layer instance can

00:14:01,480 --> 00:14:05,380
serve data from any of the record stores

00:14:03,190 --> 00:14:07,150
behind me in the cluster so for example

00:14:05,380 --> 00:14:08,560
you know the red the red person can

00:14:07,150 --> 00:14:09,760
issue two requests those can be load

00:14:08,560 --> 00:14:11,470
balanced different record layer

00:14:09,760 --> 00:14:15,730
instances but then they can still access

00:14:11,470 --> 00:14:17,650
the same data and at Apple we've used

00:14:15,730 --> 00:14:19,300
the record layer to build a service

00:14:17,650 --> 00:14:21,790
called cloud kit which is sort of our

00:14:19,300 --> 00:14:25,090
cloud cloud database for application

00:14:21,790 --> 00:14:26,980
developers and we've taken it in pretty

00:14:25,090 --> 00:14:28,600
far so for cloud kit we actually

00:14:26,980 --> 00:14:30,850
maintain a separate little record store

00:14:28,600 --> 00:14:33,550
somewhere on an FTP cluster for each

00:14:30,850 --> 00:14:35,440
user of each application and that has

00:14:33,550 --> 00:14:38,350
all those all the you know primary data

00:14:35,440 --> 00:14:40,110
all the like index data all the metadata

00:14:38,350 --> 00:14:42,610
needed to maintain that little database

00:14:40,110 --> 00:14:44,230
and we can sort of do all kinds of fun

00:14:42,610 --> 00:14:46,180
things like if we need to rebalance load

00:14:44,230 --> 00:14:47,500
between our clusters we can just pack up

00:14:46,180 --> 00:14:49,690
a record store and ship it somewhere

00:14:47,500 --> 00:14:50,920
else this has turned out to be like a

00:14:49,690 --> 00:14:52,540
really core part of our cloud kit

00:14:50,920 --> 00:14:54,400
architecture and it's worked really well

00:14:52,540 --> 00:14:55,810
for us and it's also give us a given us

00:14:54,400 --> 00:14:58,230
an interesting opportunity to see how

00:14:55,810 --> 00:15:00,850
the record layer scales we operate

00:14:58,230 --> 00:15:03,760
literally billions of little record

00:15:00,850 --> 00:15:05,740
stores across a very large number of

00:15:03,760 --> 00:15:07,620
clusters and some of these records

00:15:05,740 --> 00:15:09,850
stores can be you know terabytes in size

00:15:07,620 --> 00:15:10,960
and so it's been really fun to see how

00:15:09,850 --> 00:15:14,410
you can scale the record layer in

00:15:10,960 --> 00:15:16,210
different ways cloud kit of course is

00:15:14,410 --> 00:15:18,070
like a much more comprehensive it it

00:15:16,210 --> 00:15:19,630
really is a full-service database it'll

00:15:18,070 --> 00:15:22,060
do a lot more all the database things

00:15:19,630 --> 00:15:23,410
and more including managing sharing and

00:15:22,060 --> 00:15:25,450
cryptography and all kinds of other

00:15:23,410 --> 00:15:27,100
things for our users and so it takes

00:15:25,450 --> 00:15:27,610
advantage of the record layers really

00:15:27,100 --> 00:15:30,399
deep

00:15:27,610 --> 00:15:32,050
extensibility for building sort of I

00:15:30,399 --> 00:15:33,640
would call them like higher layers right

00:15:32,050 --> 00:15:35,620
so basically our architecture is we have

00:15:33,640 --> 00:15:37,089
foundation DB foundation DB doing the

00:15:35,620 --> 00:15:39,040
distributed systems and concurrency

00:15:37,089 --> 00:15:40,839
control the record layer is sort of

00:15:39,040 --> 00:15:42,880
acting like a relational database and

00:15:40,839 --> 00:15:45,610
then cloudkit flushes it out with all

00:15:42,880 --> 00:15:48,100
kinds of like fancy features needed by

00:15:45,610 --> 00:15:49,750
application developers and i think that

00:15:48,100 --> 00:15:52,570
this is an architecture that like has a

00:15:49,750 --> 00:15:54,149
lot of potential so I mentioned we at

00:15:52,570 --> 00:15:56,200
Apple we've used it to build cloud kit

00:15:54,149 --> 00:15:57,730
there are other systems at Apple that

00:15:56,200 --> 00:15:58,959
have done a similar thing building on

00:15:57,730 --> 00:16:02,110
top of the record layer to provide a

00:15:58,959 --> 00:16:05,410
more full service database the sort of

00:16:02,110 --> 00:16:07,750
mythical sequel layer is a really hard

00:16:05,410 --> 00:16:09,040
problem but it becomes somewhat easier

00:16:07,750 --> 00:16:10,660
if you have the record layer as a

00:16:09,040 --> 00:16:12,430
starting point and so if I were to start

00:16:10,660 --> 00:16:13,930
building a sequel layer today I would

00:16:12,430 --> 00:16:16,660
definitely build it on top of the record

00:16:13,930 --> 00:16:18,010
layer but it sort of doesn't stop there

00:16:16,660 --> 00:16:20,980
like you can imagine building other

00:16:18,010 --> 00:16:22,240
layers maybe for a document store or

00:16:20,980 --> 00:16:23,950
graph database or something anything

00:16:22,240 --> 00:16:25,540
that needs like some kind of structured

00:16:23,950 --> 00:16:28,570
storage anything that needs indexes

00:16:25,540 --> 00:16:30,730
right and so you know like your system

00:16:28,570 --> 00:16:33,750
would be a fine thing to build on top of

00:16:30,730 --> 00:16:35,980
the record layer like I mentioned before

00:16:33,750 --> 00:16:38,860
we're really excited about this as a

00:16:35,980 --> 00:16:40,570
community oriented project so we try

00:16:38,860 --> 00:16:42,459
really hard to develop you know develop

00:16:40,570 --> 00:16:45,070
it in the open on github and talk about

00:16:42,459 --> 00:16:46,660
it on the forums we're actively looking

00:16:45,070 --> 00:16:47,920
for new adopters we're especially

00:16:46,660 --> 00:16:49,540
interested in talking to layer

00:16:47,920 --> 00:16:58,320
developers about how this can be sort of

00:16:49,540 --> 00:17:00,940
a layer building toolkit and there we go

00:16:58,320 --> 00:17:03,250
and we're really excited about

00:17:00,940 --> 00:17:04,600
soliciting contributions and there's

00:17:03,250 --> 00:17:05,650
actually you know it's a young project

00:17:04,600 --> 00:17:08,199
there are a lot of low hanging fruit to

00:17:05,650 --> 00:17:11,079
work on so to give you a taste of some

00:17:08,199 --> 00:17:12,669
of those we have a whole bunch of active

00:17:11,079 --> 00:17:14,650
projects that are you know you can see

00:17:12,669 --> 00:17:18,069
the pull requests flying on github right

00:17:14,650 --> 00:17:19,750
now so there's a new query optimizer and

00:17:18,069 --> 00:17:22,120
sort of the cascade style that I've been

00:17:19,750 --> 00:17:24,280
working on we're working on improving

00:17:22,120 --> 00:17:27,280
the scalability and parallelization of

00:17:24,280 --> 00:17:29,890
index building we have we're working on

00:17:27,280 --> 00:17:32,290
some improved schema evolution tools and

00:17:29,890 --> 00:17:33,970
we're also working on we call them

00:17:32,290 --> 00:17:36,540
stable continuations which basically

00:17:33,970 --> 00:17:36,540
allow you to

00:17:36,940 --> 00:17:44,440
beta software you can imagine that allow

00:17:42,010 --> 00:17:46,840
you to sort of run queries and keep

00:17:44,440 --> 00:17:48,220
running them even as even during while

00:17:46,840 --> 00:17:49,900
you're bouncing some instances during an

00:17:48,220 --> 00:17:52,090
upgrade process we're very excited about

00:17:49,900 --> 00:17:54,160
that so you know this is the call to

00:17:52,090 --> 00:17:55,450
action we hope you come join us we're

00:17:54,160 --> 00:17:57,730
looking for open source contributors

00:17:55,450 --> 00:18:00,340
application developers layer developers

00:17:57,730 --> 00:18:03,520
you there are a whole bunch more details

00:18:00,340 --> 00:18:06,250
we had a paper in Sigma industry this

00:18:03,520 --> 00:18:10,570
year so you can find that anywhere you

00:18:06,250 --> 00:18:16,450
find papers thank you for listening

00:18:10,570 --> 00:18:16,450

YouTube URL: https://www.youtube.com/watch?v=HLE8chgw6LI


