Title: Time Series and FoundationDB: Millions of Writes s and 10x Compression in 2000 Lin... Richard Artoul
Publication date: 2019-11-25
Playlist: FoundationDB Summit 2019
Description: 
	Time Series and FoundationDB: Millions of Writes/s and 10x Compression in 2000 Lines of Go - Richard Artoul, Uber 

Richard spent the last two years developing a time series database called M3DB that was designed from the ground up (including a custom storage engine) to address the issue of ingesting and compressing million of time series values per second.

In this talk, Richard will revisit the original problem that M3DB was designed for, but from a new perspective. Could M3DB have been built on top of FoundationDB instead?

Over the course of the talk, Richard will walk participants through the process of designing a distributed system on top of FoundationDB that ingests millions of data points per second and achieves industry-standard levels of compression, all in less than 2,000 lines of Go.

The talk will include detailed implementation details such as code samples, architecture diagrams, and an overview of how the FoundationDB key/values were structured.
Captions: 
	00:00:00,920 --> 00:00:08,550
cool everyone I'm Richie I work at uber

00:00:05,400 --> 00:00:10,290
and I spent the last two years working

00:00:08,550 --> 00:00:14,099
on M 3 DB which is ubers

00:00:10,290 --> 00:00:16,440
distributed time series database and my

00:00:14,099 --> 00:00:22,500
talk today is about seeing if I could

00:00:16,440 --> 00:00:24,269
reimplementation DB as the core storage

00:00:22,500 --> 00:00:27,150
layer instead of writing a storage

00:00:24,269 --> 00:00:28,140
system from scratch like we did and the

00:00:27,150 --> 00:00:29,910
thing I built there's only a prototype

00:00:28,140 --> 00:00:32,009
I'm not like actively developing it or

00:00:29,910 --> 00:00:33,480
anything like that but it's kind of a

00:00:32,009 --> 00:00:35,610
proof of concept that you can actually

00:00:33,480 --> 00:00:38,700
build really high right throughput

00:00:35,610 --> 00:00:39,989
systems on top of foundation DB so in

00:00:38,700 --> 00:00:42,629
this case you know like millions of

00:00:39,989 --> 00:00:44,040
writes per second and panics compression

00:00:42,629 --> 00:00:47,820
kind of time-series compression and

00:00:44,040 --> 00:00:51,180
2,000 lines ago and just a quick

00:00:47,820 --> 00:00:53,520
disclaimer do not use any of this code

00:00:51,180 --> 00:00:54,780
anywhere for anything important it was

00:00:53,520 --> 00:00:56,610
just a prototype to do some benchmarking

00:00:54,780 --> 00:00:58,410
and figure out different architectures

00:00:56,610 --> 00:01:00,410
for foundation DB so a lot of cut

00:00:58,410 --> 00:01:02,430
corners and to do's and missing features

00:01:00,410 --> 00:01:06,240
but I think it does kind of prove the

00:01:02,430 --> 00:01:08,040
point so very specifically what I was

00:01:06,240 --> 00:01:10,650
trying to do was to see if I could build

00:01:08,040 --> 00:01:12,689
a system with the same API compression

00:01:10,650 --> 00:01:15,509
and performance characteristics as M 3

00:01:12,689 --> 00:01:18,090
DB but as a more or less thin layer over

00:01:15,509 --> 00:01:20,189
Foundation DB and when I say kind of

00:01:18,090 --> 00:01:22,470
high or high write throughput time

00:01:20,189 --> 00:01:24,509
series data this is kind of the

00:01:22,470 --> 00:01:25,650
interface I'm talking about where you

00:01:24,509 --> 00:01:27,900
know when you're writing you're saying

00:01:25,650 --> 00:01:29,520
this is my time series ID and here's my

00:01:27,900 --> 00:01:31,770
value which has like a timestamp and a

00:01:29,520 --> 00:01:32,850
float and when you're reading I think

00:01:31,770 --> 00:01:34,560
there's actually a mistake in this

00:01:32,850 --> 00:01:36,329
method you provide the series ID and

00:01:34,560 --> 00:01:38,400
then the minimum time stamp in the

00:01:36,329 --> 00:01:42,420
maximum time stamp in it rips out all

00:01:38,400 --> 00:01:44,790
those data points right so this was kind

00:01:42,420 --> 00:01:46,079
of my first attempt which is you know

00:01:44,790 --> 00:01:47,460
what the foundation DB documentation

00:01:46,079 --> 00:01:50,189
says to do for time series data

00:01:47,460 --> 00:01:51,930
we're basically you know you you get a

00:01:50,189 --> 00:01:54,119
bunch of writes you batch them up into

00:01:51,930 --> 00:01:56,670
one transaction and then your key is

00:01:54,119 --> 00:01:58,469
basically the time series ID plus the

00:01:56,670 --> 00:02:01,009
timestamp to make it unique and then you

00:01:58,469 --> 00:02:03,649
you pack the value into into the float

00:02:01,009 --> 00:02:05,670
or you pack the float into the value

00:02:03,649 --> 00:02:07,229
this works but you get terrible

00:02:05,670 --> 00:02:09,390
compression right you're repeating the

00:02:07,229 --> 00:02:11,720
time series ID each time and you don't

00:02:09,390 --> 00:02:13,800
get prefix compression and Foundation DB

00:02:11,720 --> 00:02:16,530
and on top of that you're not

00:02:13,800 --> 00:02:18,420
the values or the floats at all it's

00:02:16,530 --> 00:02:19,800
also like the right throughputs just not

00:02:18,420 --> 00:02:21,600
that great compared to like what modern

00:02:19,800 --> 00:02:23,130
time series databases can do just

00:02:21,600 --> 00:02:26,730
because you're you know are generating a

00:02:23,130 --> 00:02:28,320
lot of transactions a second thing I

00:02:26,730 --> 00:02:30,900
tried to do which is not a good idea and

00:02:28,320 --> 00:02:32,460
I don't recommend it but I just wanted

00:02:30,900 --> 00:02:34,410
to see if I could just do guerrilla

00:02:32,460 --> 00:02:36,090
compression directly in Foundation dB I

00:02:34,410 --> 00:02:38,520
kind of knew it would be slow but I just

00:02:36,090 --> 00:02:39,660
want to see if it was possible and if

00:02:38,520 --> 00:02:41,100
you're not familiar guerrilla

00:02:39,660 --> 00:02:43,140
compression is kind of like one of the

00:02:41,100 --> 00:02:44,760
most common forms of streaming time

00:02:43,140 --> 00:02:47,010
series compression that people use for

00:02:44,760 --> 00:02:49,290
like time series data specifically in

00:02:47,010 --> 00:02:50,160
the observability space and the

00:02:49,290 --> 00:02:51,600
interesting thing about guerrilla

00:02:50,160 --> 00:02:53,880
compression is that it operates on

00:02:51,600 --> 00:02:55,350
individual bits so if you have a value

00:02:53,880 --> 00:02:57,150
that hasn't changed since the last time

00:02:55,350 --> 00:02:59,580
you encoded it you can encode that and

00:02:57,150 --> 00:03:01,440
Grille with just a single bit so that's

00:02:59,580 --> 00:03:04,230
pretty hard to map directly to the

00:03:01,440 --> 00:03:06,810
foundation DB interface what I ended up

00:03:04,230 --> 00:03:08,990
doing was basically taking the entire

00:03:06,810 --> 00:03:12,090
state of like the streaming encoder and

00:03:08,990 --> 00:03:13,410
serializing that to Foundation DB and

00:03:12,090 --> 00:03:15,540
then every time I wanted to read I would

00:03:13,410 --> 00:03:17,940
like rip out that state kind of hydrate

00:03:15,540 --> 00:03:19,290
this in-memory object some of you were

00:03:17,940 --> 00:03:21,900
shaking your heads yes it was not a good

00:03:19,290 --> 00:03:23,310
idea you know right the next value into

00:03:21,900 --> 00:03:25,709
the encoder and then serialize the state

00:03:23,310 --> 00:03:27,270
back if you know anything about how like

00:03:25,709 --> 00:03:28,860
storage system and B trees work you'll

00:03:27,270 --> 00:03:31,380
realize that this will not have good

00:03:28,860 --> 00:03:34,200
performance which it did not but you do

00:03:31,380 --> 00:03:35,760
get really good compression so after

00:03:34,200 --> 00:03:38,280
kind of trying the two like really naive

00:03:35,760 --> 00:03:40,290
things I figured that if I was going to

00:03:38,280 --> 00:03:42,980
do this the architecture would probably

00:03:40,290 --> 00:03:45,150
have to look a lot more similar to m3 DB

00:03:42,980 --> 00:03:48,239
but kind of replacing some of the lower

00:03:45,150 --> 00:03:50,910
pieces of the stack with foundation DB

00:03:48,239 --> 00:03:53,340
instead of our custom storage system so

00:03:50,910 --> 00:03:56,040
if you just to briefly go over in 3ds

00:03:53,340 --> 00:03:58,800
architecture the way things work is

00:03:56,040 --> 00:04:00,989
rights are coming in constantly and once

00:03:58,800 --> 00:04:03,630
they arrive at an individual m 3 DB no

00:04:00,989 --> 00:04:06,060
two things happen in parallel they get

00:04:03,630 --> 00:04:07,950
gorilla compressed in memory and kind of

00:04:06,060 --> 00:04:09,570
basically buffered and at the same time

00:04:07,950 --> 00:04:12,090
they're written in uncompressed form to

00:04:09,570 --> 00:04:13,560
the commit log so that if a node dies

00:04:12,090 --> 00:04:15,209
and comes back up it can just read its

00:04:13,560 --> 00:04:17,489
commit log restore the in-memory

00:04:15,209 --> 00:04:19,620
compress state and kind of keep chugging

00:04:17,489 --> 00:04:20,790
and then kind of as an asynchronous

00:04:19,620 --> 00:04:21,359
process that's happening in the

00:04:20,790 --> 00:04:23,220
background

00:04:21,359 --> 00:04:24,930
these compressed blocks that you've

00:04:23,220 --> 00:04:26,820
already buffered and compressed in

00:04:24,930 --> 00:04:27,900
memory are written to disk as immutable

00:04:26,820 --> 00:04:29,340
files

00:04:27,900 --> 00:04:30,660
so if you're familiar with systems like

00:04:29,340 --> 00:04:32,220
Cassandra you know you can imagine the

00:04:30,660 --> 00:04:34,139
in-memory part it's like the mem tables

00:04:32,220 --> 00:04:37,889
and these immutable files on disk as the

00:04:34,139 --> 00:04:39,270
SS tables pretty straightforward and if

00:04:37,889 --> 00:04:41,160
you think about where your data lives at

00:04:39,270 --> 00:04:43,260
any point in time you're kind of most

00:04:41,160 --> 00:04:44,910
recent time series writes live in two

00:04:43,260 --> 00:04:47,160
places they live in your commit log

00:04:44,910 --> 00:04:49,320
files and in your in-memory mutable

00:04:47,160 --> 00:04:51,449
encoders and those kind of can cover the

00:04:49,320 --> 00:04:52,590
same amount of data and then data that's

00:04:51,449 --> 00:04:56,280
kind of a little bit older lives in

00:04:52,590 --> 00:04:58,380
these immutable files that files so kind

00:04:56,280 --> 00:05:01,229
of mapping this over to foundation DB

00:04:58,380 --> 00:05:03,210
world I decided to take kind of a

00:05:01,229 --> 00:05:06,419
similar approach and put a semi stateful

00:05:03,210 --> 00:05:08,310
layer in front of foundation DB so I

00:05:06,419 --> 00:05:09,930
wrote basically a go program that that

00:05:08,310 --> 00:05:11,160
looks a little bit like this

00:05:09,930 --> 00:05:12,630
and it realize there's a lot going on in

00:05:11,160 --> 00:05:14,790
this picture but the idea is basically

00:05:12,630 --> 00:05:16,830
the same as what we do with M 3 DB where

00:05:14,790 --> 00:05:19,020
rights are coming in they get buffered

00:05:16,830 --> 00:05:21,180
in memory and I'm doing kind of gorilla

00:05:19,020 --> 00:05:23,130
compression in a synchronous fashion

00:05:21,180 --> 00:05:24,720
there and in the same time they're

00:05:23,130 --> 00:05:27,180
getting kind of batch together and

00:05:24,720 --> 00:05:29,070
written in uncompressed form into commit

00:05:27,180 --> 00:05:31,289
logs but the commit logs live in

00:05:29,070 --> 00:05:33,570
foundation DB so they don't have to be

00:05:31,289 --> 00:05:34,650
if a node restarts I don't have to feel

00:05:33,570 --> 00:05:36,030
as running this on kubernetes for

00:05:34,650 --> 00:05:37,919
example if a node dies and comes back

00:05:36,030 --> 00:05:39,570
it's completely stateless it has a node

00:05:37,919 --> 00:05:41,070
identifier it goes and finds its commit

00:05:39,570 --> 00:05:41,940
logs from foundation dB and you don't

00:05:41,070 --> 00:05:43,310
have to worry about any of that

00:05:41,940 --> 00:05:45,630
statefulness

00:05:43,310 --> 00:05:47,010
and then kind of like I said before

00:05:45,630 --> 00:05:48,930
where you have this asynchronous process

00:05:47,010 --> 00:05:50,550
that's constantly reading data out of

00:05:48,930 --> 00:05:52,470
the buffers and writing the compressed

00:05:50,550 --> 00:05:54,320
chunks to disk I'm doing the same thing

00:05:52,470 --> 00:05:57,300
except I'm writing them to Foundation DB

00:05:54,320 --> 00:05:58,830
and you for each time series you kind of

00:05:57,300 --> 00:06:00,870
have this like metadata entry that tells

00:05:58,830 --> 00:06:03,150
you you know for this time series here's

00:06:00,870 --> 00:06:04,560
all the chunks that I have for it here's

00:06:03,150 --> 00:06:05,910
how big they are and you kind of have

00:06:04,560 --> 00:06:07,979
these simple zone maps that tell you

00:06:05,910 --> 00:06:09,870
what is the first and last time stamp in

00:06:07,979 --> 00:06:11,130
this compressed chunks which helps you

00:06:09,870 --> 00:06:14,910
figure out what chunks to read when you

00:06:11,130 --> 00:06:16,680
need to serve queries so implementing

00:06:14,910 --> 00:06:18,990
the kind of what I call semi stateful

00:06:16,680 --> 00:06:20,820
commit logs is pretty straightforward

00:06:18,990 --> 00:06:22,770
you basically accumulate rights into

00:06:20,820 --> 00:06:24,900
batches and then dispatch them the

00:06:22,770 --> 00:06:26,370
foundation DB is one large chunk so you

00:06:24,900 --> 00:06:28,979
may have like a thousand vials a

00:06:26,370 --> 00:06:31,430
thousand writes that translate into one

00:06:28,979 --> 00:06:33,750
large chunk that goes into Foundation DB

00:06:31,430 --> 00:06:35,370
the commit log chunking format is

00:06:33,750 --> 00:06:37,590
optimized to be read from start to

00:06:35,370 --> 00:06:40,490
finish quickly for recovery you can't go

00:06:37,590 --> 00:06:42,050
in and jump around kind of randomly

00:06:40,490 --> 00:06:44,150
and the two operations that you need to

00:06:42,050 --> 00:06:45,980
support for commit logs are you need to

00:06:44,150 --> 00:06:47,150
be able to fetch all the undeleted

00:06:45,980 --> 00:06:50,000
commit locks when you're doing a

00:06:47,150 --> 00:06:51,230
recovery and so you can replay them and

00:06:50,000 --> 00:06:52,880
then you need to be able to delete all

00:06:51,230 --> 00:06:55,070
commit log chunks before a certain time

00:06:52,880 --> 00:06:56,930
so as you start flushing your compressed

00:06:55,070 --> 00:06:58,010
files to foundation maybe you can start

00:06:56,930 --> 00:06:59,180
cleaning up your commit log files

00:06:58,010 --> 00:07:02,210
otherwise they kind of accumulate

00:06:59,180 --> 00:07:03,380
forever and this is the the key format

00:07:02,210 --> 00:07:05,840
that I picked for my commit logs it's

00:07:03,380 --> 00:07:07,190
very simple if your once you actually

00:07:05,840 --> 00:07:08,510
kind of turn this into a more

00:07:07,190 --> 00:07:10,370
distributed system you'd have a host

00:07:08,510 --> 00:07:11,630
identifier in there too but to commit

00:07:10,370 --> 00:07:15,440
log and then kind of a monotonically

00:07:11,630 --> 00:07:16,610
increasing commit log index and if you

00:07:15,440 --> 00:07:18,470
think about how this actually works in

00:07:16,610 --> 00:07:19,760
terms of commit log cleanup you

00:07:18,470 --> 00:07:21,920
basically have this background process

00:07:19,760 --> 00:07:23,660
that's looping infinitely and it waits

00:07:21,920 --> 00:07:26,300
for a new commit log chunk to be written

00:07:23,660 --> 00:07:28,100
with a specific index so looking at this

00:07:26,300 --> 00:07:30,140
diagram for example we would wait for

00:07:28,100 --> 00:07:32,360
commit log chunk number four to be

00:07:30,140 --> 00:07:34,610
written once that happens we would kick

00:07:32,360 --> 00:07:36,740
off the flush process which basically

00:07:34,610 --> 00:07:38,090
goes and loops through all the

00:07:36,740 --> 00:07:40,310
compressed chunks that live in the

00:07:38,090 --> 00:07:42,020
buffer and flushes those two foundation

00:07:40,310 --> 00:07:44,150
DB and I'll go into the details of that

00:07:42,020 --> 00:07:47,090
process in a minute and then once you've

00:07:44,150 --> 00:07:49,490
done that you know that all of the data

00:07:47,090 --> 00:07:52,040
that lives in commit log chunks 1 2 & 3

00:07:49,490 --> 00:07:54,110
now lives in foundation DB in like a

00:07:52,040 --> 00:07:56,180
separate readable form so you can just

00:07:54,110 --> 00:07:58,750
delete commit log chunks 1 2 & 3 and the

00:07:56,180 --> 00:08:01,450
system is constantly doing this process

00:07:58,750 --> 00:08:03,410
this is what it looks like in code I

00:08:01,450 --> 00:08:05,060
don't need to go into the details but

00:08:03,410 --> 00:08:05,810
basically it waits for a commit log

00:08:05,060 --> 00:08:07,730
rotation

00:08:05,810 --> 00:08:09,050
it calls flush on the buffer and then it

00:08:07,730 --> 00:08:12,250
truncates the commit log using some

00:08:09,050 --> 00:08:15,140
token that came back from the rotation

00:08:12,250 --> 00:08:16,730
so are we a database yet not really

00:08:15,140 --> 00:08:19,550
right we can write commit logs clean

00:08:16,730 --> 00:08:20,980
cumulative logs and read commit logs but

00:08:19,550 --> 00:08:23,390
that's not actually useful to anyone

00:08:20,980 --> 00:08:26,240
people want to be able to read their

00:08:23,390 --> 00:08:28,130
rights basically so this is where the

00:08:26,240 --> 00:08:30,050
buffering system kind of comes in like I

00:08:28,130 --> 00:08:31,910
said before it's very similar to

00:08:30,050 --> 00:08:33,919
Cassandra and similar systems mem table

00:08:31,910 --> 00:08:35,870
we're buffering compressing time-series

00:08:33,919 --> 00:08:37,430
data in memory before flushing it to

00:08:35,870 --> 00:08:39,169
Foundation dB

00:08:37,430 --> 00:08:40,280
and when you write to foundation DB you

00:08:39,169 --> 00:08:42,140
have the option because of the

00:08:40,280 --> 00:08:43,729
transactional nature to either merge

00:08:42,140 --> 00:08:45,410
with an existing chunk or to create a

00:08:43,729 --> 00:08:48,170
new one and then update your metadata at

00:08:45,410 --> 00:08:49,700
the same time and all writes and reads

00:08:48,170 --> 00:08:51,589
kind of have to flow through this buffer

00:08:49,700 --> 00:08:53,770
system to get a consistent view of the

00:08:51,589 --> 00:08:53,770
data

00:08:53,779 --> 00:08:57,920
this is more or less what the buffer

00:08:55,580 --> 00:08:59,810
system looks like in my in my prototype

00:08:57,920 --> 00:09:02,570
it's basically a gigantic synchronized

00:08:59,810 --> 00:09:05,450
hash map where the entries are the time

00:09:02,570 --> 00:09:07,970
series ID and the values are an array of

00:09:05,450 --> 00:09:10,010
gorilla encoders and I'll explain why

00:09:07,970 --> 00:09:12,140
that's an array in a second

00:09:10,010 --> 00:09:13,310
let's we'll start by looking at the

00:09:12,140 --> 00:09:16,640
right pathway which is actually them

00:09:13,310 --> 00:09:20,000
very simple a right comes in you look up

00:09:16,640 --> 00:09:22,070
the time series ID in your hash map and

00:09:20,000 --> 00:09:23,750
then if there are no encoders there you

00:09:22,070 --> 00:09:27,500
just create a new one and code the value

00:09:23,750 --> 00:09:29,839
and you're done if there are a encoders

00:09:27,500 --> 00:09:32,180
in there then only the last one is

00:09:29,839 --> 00:09:33,709
considered mutable so you grab the last

00:09:32,180 --> 00:09:36,110
one and you encode the value in there

00:09:33,709 --> 00:09:37,700
and then you're basically done this

00:09:36,110 --> 00:09:39,769
system doesn't account for out of order

00:09:37,700 --> 00:09:41,450
rights so for this specific

00:09:39,769 --> 00:09:44,329
implementation you always have to be

00:09:41,450 --> 00:09:46,220
writing values with time stamps higher

00:09:44,329 --> 00:09:47,690
than the previous ones there's no reason

00:09:46,220 --> 00:09:48,950
it couldn't support out of order rights

00:09:47,690 --> 00:09:50,570
it just makes the implementation a

00:09:48,950 --> 00:09:53,200
little bit more complicated so I didn't

00:09:50,570 --> 00:09:55,550
do it for demo purposes but you could

00:09:53,200 --> 00:09:56,600
and then this is where things get a

00:09:55,550 --> 00:09:58,610
little bit interesting which is this

00:09:56,600 --> 00:10:00,170
flush API and this is what's actually

00:09:58,610 --> 00:10:03,500
getting all of your data out of this

00:10:00,170 --> 00:10:04,910
buffer and into foundation DB and

00:10:03,500 --> 00:10:06,620
there's a very specific contract that

00:10:04,910 --> 00:10:08,899
the flush API has to implement in order

00:10:06,620 --> 00:10:11,360
for everything to work and the contract

00:10:08,899 --> 00:10:14,209
is that once the flush method completes

00:10:11,360 --> 00:10:16,040
and returns successfully all rights that

00:10:14,209 --> 00:10:18,380
were in memory when the flush function

00:10:16,040 --> 00:10:21,140
started have to be persisted to the

00:10:18,380 --> 00:10:22,490
foundation DB in like a durable way and

00:10:21,140 --> 00:10:24,350
the way it works is it basically loops

00:10:22,490 --> 00:10:26,149
through each series in memory and then

00:10:24,350 --> 00:10:27,950
for each one and marks the current

00:10:26,149 --> 00:10:30,740
encoder as immutable so it basically

00:10:27,950 --> 00:10:32,570
seals the current compressed chunk and

00:10:30,740 --> 00:10:34,790
then flushes all the immutable encoders

00:10:32,570 --> 00:10:36,770
to memory and then once it's flushed all

00:10:34,790 --> 00:10:39,290
the immutable encoders it can evict them

00:10:36,770 --> 00:10:42,740
from memory to clear up memory for

00:10:39,290 --> 00:10:44,180
basically more rights and when I say

00:10:42,740 --> 00:10:45,860
flushing to Foundation DB there's two

00:10:44,180 --> 00:10:47,600
things that have to happen right one you

00:10:45,860 --> 00:10:49,730
have to actually store the compress data

00:10:47,600 --> 00:10:50,959
chunk but you also have to have an index

00:10:49,730 --> 00:10:52,910
over them you have to be able to say

00:10:50,959 --> 00:10:56,630
what chunks do I have and what time

00:10:52,910 --> 00:10:57,860
ranges do they cover so in a production

00:10:56,630 --> 00:11:00,529
system you probably have a slightly more

00:10:57,860 --> 00:11:02,390
sophisticated zone map than this but my

00:11:00,529 --> 00:11:04,550
implementation I just have a slice of

00:11:02,390 --> 00:11:06,500
metadata it contains the first time

00:11:04,550 --> 00:11:07,700
stamp in the last time stamp

00:11:06,500 --> 00:11:09,589
that's really useful for satisfying

00:11:07,700 --> 00:11:12,050
queries and also deciding which chunks

00:11:09,589 --> 00:11:13,399
makes sense to compact together and then

00:11:12,050 --> 00:11:14,930
you also have the size right because you

00:11:13,399 --> 00:11:16,610
don't want you probably wanted to set

00:11:14,930 --> 00:11:18,589
you don't want any individual chunk to

00:11:16,610 --> 00:11:20,480
be too large or too small so you can use

00:11:18,589 --> 00:11:24,019
the size to decide which chunks make

00:11:20,480 --> 00:11:25,220
sense to merge together awesome and then

00:11:24,019 --> 00:11:27,589
this is where I think foundation newbie

00:11:25,220 --> 00:11:28,579
really starts to shine which is when

00:11:27,589 --> 00:11:30,829
you're doing the flushes and you're

00:11:28,579 --> 00:11:33,110
doing them transactionally so when I

00:11:30,829 --> 00:11:35,329
flush a time series compressed block to

00:11:33,110 --> 00:11:37,010
Foundation maybe the first thing I do is

00:11:35,329 --> 00:11:39,800
I read the existing metadata for the

00:11:37,010 --> 00:11:41,540
series being flushed then I use the

00:11:39,800 --> 00:11:42,980
series metadata to decide if the data

00:11:41,540 --> 00:11:45,140
being flushed should be merged with the

00:11:42,980 --> 00:11:48,350
existing chunk or written out as a new

00:11:45,140 --> 00:11:49,579
independent chunk then you read the

00:11:48,350 --> 00:11:51,620
existing chunk that we need to merge

00:11:49,579 --> 00:11:53,959
with if we're deciding to do a merge if

00:11:51,620 --> 00:11:56,360
not you don't have to do that and you

00:11:53,959 --> 00:11:59,000
write the new merged chunk or just the

00:11:56,360 --> 00:12:00,709
new chunk to Foundation dB

00:11:59,000 --> 00:12:02,540
and then you update the series metadata

00:12:00,709 --> 00:12:03,920
with the new chunk information which is

00:12:02,540 --> 00:12:05,450
either a new merged chunk or a

00:12:03,920 --> 00:12:08,240
completely new chunk that you just wrote

00:12:05,450 --> 00:12:10,160
out and you can do all this as a single

00:12:08,240 --> 00:12:12,829
asset transaction performed with strict

00:12:10,160 --> 00:12:14,390
serializability I think this is kind of

00:12:12,829 --> 00:12:16,550
like the big aha moment for me when I

00:12:14,390 --> 00:12:18,170
was using foundation DB because other

00:12:16,550 --> 00:12:19,220
people have built systems that kind of

00:12:18,170 --> 00:12:21,589
look like this and they're built on top

00:12:19,220 --> 00:12:23,000
of Cassandra or other things and I you

00:12:21,589 --> 00:12:24,500
know I've implemented basically this

00:12:23,000 --> 00:12:26,540
entire system but on a custom storage

00:12:24,500 --> 00:12:28,820
engine and it's just like you just can't

00:12:26,540 --> 00:12:30,740
do this it's like really hard to get all

00:12:28,820 --> 00:12:32,209
this stuff right and it's like really

00:12:30,740 --> 00:12:34,339
easy with Foundation TV it's almost like

00:12:32,209 --> 00:12:36,199
you're like programming against an

00:12:34,339 --> 00:12:37,910
in-memory lock system it's just really

00:12:36,199 --> 00:12:39,589
easy to get it right and know that it's

00:12:37,910 --> 00:12:42,220
going to do the right thing so to me

00:12:39,589 --> 00:12:43,880
this was like really really powerful

00:12:42,220 --> 00:12:45,620
finally just to walk through how the

00:12:43,880 --> 00:12:47,839
reads work you've probably figured this

00:12:45,620 --> 00:12:50,410
out by now but we for the time series we

00:12:47,839 --> 00:12:52,579
go and look up the time series metadata

00:12:50,410 --> 00:12:54,019
and read the latest version of the

00:12:52,579 --> 00:12:56,120
series metadata at a foundation d beam

00:12:54,019 --> 00:12:58,910
then we use the metadata to determine

00:12:56,120 --> 00:13:00,230
which chunks contain data for the

00:12:58,910 --> 00:13:02,060
specific time range so you're basically

00:13:00,230 --> 00:13:03,980
taking this is the interval that the

00:13:02,060 --> 00:13:05,570
user wants to read and here's all the

00:13:03,980 --> 00:13:07,070
intervals that these chunks cover and

00:13:05,570 --> 00:13:08,870
just basically doing interval

00:13:07,070 --> 00:13:11,480
intersection to see which chunks contain

00:13:08,870 --> 00:13:13,430
data that you care about you also need

00:13:11,480 --> 00:13:14,750
to look at if you have any in memory

00:13:13,430 --> 00:13:17,029
encoders that haven't been flushed to

00:13:14,750 --> 00:13:19,470
foundation DB that contain data for the

00:13:17,029 --> 00:13:21,030
time range and then you just merge a

00:13:19,470 --> 00:13:24,090
all those compressed chunks and return

00:13:21,030 --> 00:13:25,740
to the user like a emerged stream it's

00:13:24,090 --> 00:13:27,030
basically the same algorithm is merging

00:13:25,740 --> 00:13:30,060
case sorted erase there's there's

00:13:27,030 --> 00:13:33,120
nothing complicated there it's kind of

00:13:30,060 --> 00:13:34,770
in conclusion building a Sunday staple

00:13:33,120 --> 00:13:36,630
distributed system on top Foundation B

00:13:34,770 --> 00:13:39,960
is a lot more work than building a

00:13:36,630 --> 00:13:41,310
stateless one but it's a lot less work

00:13:39,960 --> 00:13:43,470
than building a distributed system from

00:13:41,310 --> 00:13:44,910
scratch so I recommend it if you have

00:13:43,470 --> 00:13:47,250
the if you think you can find a way to

00:13:44,910 --> 00:13:48,960
model if you have a problem that doesn't

00:13:47,250 --> 00:13:51,510
seem like it would work well on

00:13:48,960 --> 00:13:53,310
foundation EB but you can kind of find a

00:13:51,510 --> 00:13:55,680
way to model it as a semi stateful

00:13:53,310 --> 00:13:57,570
system instead it's probably a better

00:13:55,680 --> 00:13:59,820
idea than writing a custom storage

00:13:57,570 --> 00:14:01,800
engine from scratch so like I said

00:13:59,820 --> 00:14:03,390
before this will do like millions of

00:14:01,800 --> 00:14:05,610
writes per second and give you like

00:14:03,390 --> 00:14:07,080
industry competitive compression in like

00:14:05,610 --> 00:14:08,280
two thousand lines of code I mean

00:14:07,080 --> 00:14:09,630
obviously you'd probably be a lot bigger

00:14:08,280 --> 00:14:11,100
if I spent like three to six months

00:14:09,630 --> 00:14:15,030
production izing it but it's it's not

00:14:11,100 --> 00:14:16,500
far off and you know finally beating may

00:14:15,030 --> 00:14:18,750
never beat a purpose-designed storage

00:14:16,500 --> 00:14:20,490
system but it's a lot easier to program

00:14:18,750 --> 00:14:23,130
against then the operating system file

00:14:20,490 --> 00:14:25,410
system network and physical hardware and

00:14:23,130 --> 00:14:26,580
it just makes things really easy and you

00:14:25,410 --> 00:14:28,380
can spend a lot more time kind of

00:14:26,580 --> 00:14:30,810
focusing on the things that are core to

00:14:28,380 --> 00:14:32,640
your system and less time dealing with a

00:14:30,810 --> 00:14:35,940
lot of problems that foundation any

00:14:32,640 --> 00:14:38,280
beacon has already solved for you that's

00:14:35,940 --> 00:14:41,160
it I think I have a few minutes for

00:14:38,280 --> 00:14:43,670
questions if anyone wants to ask but

00:14:41,160 --> 00:14:43,670
that's all my slides

00:14:46,840 --> 00:14:52,019

YouTube URL: https://www.youtube.com/watch?v=W6yQ9Pwgb1A


