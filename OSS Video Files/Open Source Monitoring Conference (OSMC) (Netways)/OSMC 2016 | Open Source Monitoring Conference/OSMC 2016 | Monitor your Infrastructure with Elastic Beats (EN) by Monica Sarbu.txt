Title: OSMC 2016 | Monitor your Infrastructure with Elastic Beats (EN) by Monica Sarbu
Publication date: 2016-12-13
Playlist: OSMC 2016 | Open Source Monitoring Conference
Description: 
	The Beats are a friendly army of lightweight agents that, installed on your servers, capture operational data and ship it to Elasticsearch for analysis. They collect the logs from your servers, get system statistics like CPU, memory, disk usage, gather metrics by interrogating periodically external systems like MySQL, Docker, Zookeeper and give you visibility into your network by sniffing the traffic exchanged between your servers.
This talk shows how to combine the Beats with Elasticsearch and Kibana in one complete open source monitoring solution that helps you monitor and troubleshoot your distributed infrastructure.
Captions: 
	00:00:09,880 --> 00:00:15,700
morning from my side um our first talk

00:00:13,030 --> 00:00:17,560
in this room is from Monica's our boo

00:00:15,700 --> 00:00:20,020
she's from elastic and she's talking

00:00:17,560 --> 00:00:30,700
about monitoring the infrastructure with

00:00:20,020 --> 00:00:32,800
elastic beads today I'll talk about how

00:00:30,700 --> 00:00:35,890
to monitor your infrastructure using the

00:00:32,800 --> 00:00:37,960
elastic bit a bit about myself so my

00:00:35,890 --> 00:00:40,390
name is Monika Sabu and I'm working for

00:00:37,960 --> 00:00:41,980
elastic the company behind elastic

00:00:40,390 --> 00:00:46,780
search keep an eye on lock stitch and

00:00:41,980 --> 00:00:48,910
I'm timid in Xela in the team so

00:00:46,780 --> 00:00:51,700
if you want to monitor your servers

00:00:48,910 --> 00:00:53,950
usually the first thing you do you want

00:00:51,700 --> 00:00:56,050
to get the logs from your servers for

00:00:53,950 --> 00:00:58,180
example if you have an Apache server

00:00:56,050 --> 00:01:00,340
then you will be interested in getting

00:00:58,180 --> 00:01:02,920
the Apache logs in order to monitor your

00:01:00,340 --> 00:01:05,619
passion' server but then you decide that

00:01:02,920 --> 00:01:07,420
you also want to monitor the processes

00:01:05,619 --> 00:01:10,420
that are running on your servers and you

00:01:07,420 --> 00:01:12,549
are interesting for example in memory

00:01:10,420 --> 00:01:15,689
usage and CPU usage for all the

00:01:12,549 --> 00:01:19,179
processes Millenia you want also to

00:01:15,689 --> 00:01:21,670
monitor the health of your services by

00:01:19,179 --> 00:01:29,499
interrogating periodical is that servers

00:01:21,670 --> 00:01:34,450
and fetch metrics from it sorry wrong

00:01:29,499 --> 00:01:36,999
button but then if this information are

00:01:34,450 --> 00:01:39,369
not enough for you then probably you and

00:01:36,999 --> 00:01:42,819
you also want to see the interaction

00:01:39,369 --> 00:01:44,740
between your servers then see the

00:01:42,819 --> 00:01:48,939
messages that are exchanged between your

00:01:44,740 --> 00:01:52,359
servers then you also maybe want to see

00:01:48,939 --> 00:01:54,340
the HTTP transaction for example to make

00:01:52,359 --> 00:01:57,369
my sequel transaction that I receive

00:01:54,340 --> 00:02:00,939
between your servers in the world of

00:01:57,369 --> 00:02:03,670
micro services and containers well you

00:02:00,939 --> 00:02:06,429
don't have to monitor a single server so

00:02:03,670 --> 00:02:09,429
you have a bunch of servers 100 servers

00:02:06,429 --> 00:02:11,200
1,000 silver and even more then you

00:02:09,429 --> 00:02:13,720
probably want to have a central point

00:02:11,200 --> 00:02:17,380
where to store and collect all this kind

00:02:13,720 --> 00:02:19,990
of information in order to be able to

00:02:17,380 --> 00:02:24,239
search across them and also do analytics

00:02:19,990 --> 00:02:26,680
on them and this point is elasticsearch

00:02:24,239 --> 00:02:28,450
and you don't have to worry that you

00:02:26,680 --> 00:02:30,430
have to scale your infrastructure

00:02:28,450 --> 00:02:34,450
because the last desert was built from

00:02:30,430 --> 00:02:37,650
the first day to be a scalable system so

00:02:34,450 --> 00:02:40,780
it grows with a wizard of your data and

00:02:37,650 --> 00:02:44,200
because it's it is scalable and

00:02:40,780 --> 00:02:47,500
distributed a search engine in this talk

00:02:44,200 --> 00:02:50,230
I will concentrate in how to collect and

00:02:47,500 --> 00:02:55,500
ship different kinds of operational data

00:02:50,230 --> 00:02:55,500
and ship them to elastic search using

00:02:55,620 --> 00:03:01,540
bit-bit is part of the elastic stack o

00:02:58,510 --> 00:03:04,930
elastic stack is a suite of open source

00:03:01,540 --> 00:03:07,480
project bits and locks - on the angel

00:03:04,930 --> 00:03:11,530
side elastic search which is a storage

00:03:07,480 --> 00:03:14,260
engine and you can do analytics on your

00:03:11,530 --> 00:03:16,060
data and Cabana that can be used

00:03:14,260 --> 00:03:20,590
it's a UI that can be used for

00:03:16,060 --> 00:03:23,530
visualizing your data the piece family

00:03:20,590 --> 00:03:26,319
consists of file bit that is used for

00:03:23,530 --> 00:03:30,299
collecting your log files metric bit

00:03:26,319 --> 00:03:32,910
which is for interrogating periodically

00:03:30,299 --> 00:03:36,910
services and fetches metrics from them

00:03:32,910 --> 00:03:39,579
then it's packet bit for listening to

00:03:36,910 --> 00:03:42,760
network data and we log bit for

00:03:39,579 --> 00:03:45,010
collecting windows event logs and you

00:03:42,760 --> 00:03:47,590
can create your own bit based on the

00:03:45,010 --> 00:03:51,430
beats platform a proof is that they're

00:03:47,590 --> 00:03:56,349
already created 40 over 40 community

00:03:51,430 --> 00:03:59,139
beats let's start with file bit so 5 it

00:03:56,349 --> 00:04:02,019
is like running the tale - f on all your

00:03:59,139 --> 00:04:04,449
servers but instead in and printing the

00:04:02,019 --> 00:04:08,500
result on the screen it ships a result

00:04:04,449 --> 00:04:11,590
over the network to elasticsearch when

00:04:08,500 --> 00:04:14,859
it comes with some extra powers for

00:04:11,590 --> 00:04:17,349
example had support for multi-line think

00:04:14,859 --> 00:04:19,840
of this Java exceptions that are all

00:04:17,349 --> 00:04:22,000
multiple lines and usually want to

00:04:19,840 --> 00:04:25,930
combine them to see the exception in

00:04:22,000 --> 00:04:29,110
only one line jason locks think of these

00:04:25,930 --> 00:04:31,289
applications they have struck they have

00:04:29,110 --> 00:04:35,260
support for structure logging and

00:04:31,289 --> 00:04:37,600
filtering for example you want to store

00:04:35,260 --> 00:04:39,820
only a subset of your localized

00:04:37,600 --> 00:04:46,380
for example you want to store only the

00:04:39,820 --> 00:04:49,630
errormsgs file bit sends the roll

00:04:46,380 --> 00:04:54,220
localized as they are so without parsing

00:04:49,630 --> 00:04:56,500
them here is an example if you want to

00:04:54,220 --> 00:04:58,900
parse your lock lines then you'll have

00:04:56,500 --> 00:05:00,730
to write your own crop patterns and

00:04:58,900 --> 00:05:02,890
there are two ways how you can write

00:05:00,730 --> 00:05:05,140
your groc patterns one option is to use

00:05:02,890 --> 00:05:07,540
the ingest node which is a plugin for

00:05:05,140 --> 00:05:10,780
elastic search and the second option is

00:05:07,540 --> 00:05:13,690
to use log stash in the second case you

00:05:10,780 --> 00:05:17,790
basically need that file bit to send the

00:05:13,690 --> 00:05:23,530
data through lock - not directly to

00:05:17,790 --> 00:05:26,860
elastic search here is an example of a

00:05:23,530 --> 00:05:29,410
grog pattern for example in this message

00:05:26,860 --> 00:05:32,710
we we can identify that the first part

00:05:29,410 --> 00:05:35,770
is a client then we have the method the

00:05:32,710 --> 00:05:38,380
get method and then we have the URL then

00:05:35,770 --> 00:05:43,330
the number of bytes and the duration of

00:05:38,380 --> 00:05:46,260
the get transaction after applying the

00:05:43,330 --> 00:05:49,930
grog pattern you will be able to have

00:05:46,260 --> 00:05:54,370
the message the look yellow log message

00:05:49,930 --> 00:06:01,870
part here is an example how the message

00:05:54,370 --> 00:06:04,600
will look like in elasticsearch an

00:06:01,870 --> 00:06:07,540
important feature that 5-bit has is that

00:06:04,600 --> 00:06:08,490
it's capable capability of handling back

00:06:07,540 --> 00:06:11,650
pressure

00:06:08,490 --> 00:06:14,080
what peg type way back pressure is

00:06:11,650 --> 00:06:16,480
important because file bit is a

00:06:14,080 --> 00:06:19,570
lightweight shipper that can send the

00:06:16,480 --> 00:06:21,880
lock lines at the maximum speed and you

00:06:19,570 --> 00:06:27,820
have to take care not to overload log

00:06:21,880 --> 00:06:31,930
stash so that's why it's important for

00:06:27,820 --> 00:06:34,810
file bit to be able to slow down when

00:06:31,930 --> 00:06:38,590
when it's needed the way the way it

00:06:34,810 --> 00:06:42,130
works is that file bit retail a bunch of

00:06:38,590 --> 00:06:45,280
localized then send them to log stash

00:06:42,130 --> 00:06:48,190
then wait for ACK from log stash and

00:06:45,280 --> 00:06:50,800
only after you receive the hdk from log

00:06:48,190 --> 00:06:53,979
stash then it Mars

00:06:50,800 --> 00:06:58,360
clines is acknowledged then right the

00:06:53,979 --> 00:07:01,539
offset to until when the local eyes are

00:06:58,360 --> 00:07:04,360
read into a registry file on disk and in

00:07:01,539 --> 00:07:07,270
order to make sure that the office it's

00:07:04,360 --> 00:07:12,220
written on disk that it performs FF

00:07:07,270 --> 00:07:20,199
thanks can I think and only after that

00:07:12,220 --> 00:07:22,990
it reads the second batch of localize so

00:07:20,199 --> 00:07:26,259
what this means is that file bit is able

00:07:22,990 --> 00:07:29,729
to adapt is speed depending on how fast

00:07:26,259 --> 00:07:32,590
the next stage can process the data and

00:07:29,729 --> 00:07:35,860
if for example the next stage is down

00:07:32,590 --> 00:07:38,379
then file bit wait and doesn't do

00:07:35,860 --> 00:07:41,080
doesn't read any lock line so this means

00:07:38,379 --> 00:07:44,139
that doesn't buffer the lock lines on on

00:07:41,080 --> 00:07:53,409
disk doesn't allocate any memory so

00:07:44,139 --> 00:07:56,919
doesn't lose any localized another

00:07:53,409 --> 00:07:59,800
important feature that file build has is

00:07:56,919 --> 00:08:04,060
that is able to guarantee that each lock

00:07:59,800 --> 00:08:07,750
line is sent at least once why I mean at

00:08:04,060 --> 00:08:10,419
least once and not exactly once well

00:08:07,750 --> 00:08:12,610
because it was theoretically proven that

00:08:10,419 --> 00:08:14,789
this is impossible to send a message

00:08:12,610 --> 00:08:19,110
exactly once

00:08:14,789 --> 00:08:23,080
as a consequence over over a lousy

00:08:19,110 --> 00:08:29,110
connection this is a consequence of the

00:08:23,080 --> 00:08:32,589
two peas and in general problem so let's

00:08:29,110 --> 00:08:34,899
take an example to show that is really

00:08:32,589 --> 00:08:37,630
impossible to send to guarantee that the

00:08:34,899 --> 00:08:40,300
log message is then exactly once and

00:08:37,630 --> 00:08:44,020
let's take an example five bit a log

00:08:40,300 --> 00:08:47,470
stash so five bit sends a bunch of log

00:08:44,020 --> 00:08:50,470
lines to log stash and in case lock - it

00:08:47,470 --> 00:08:52,810
down or it becomes in available for some

00:08:50,470 --> 00:08:56,680
reasons then five bit is not able to

00:08:52,810 --> 00:08:59,680
know if lock - processed or not the data

00:08:56,680 --> 00:09:03,670
so there are two options here one option

00:08:59,680 --> 00:09:04,270
is to drop the log lines and in this

00:09:03,670 --> 00:09:06,940
case

00:09:04,270 --> 00:09:10,920
file bit will we looking guarantee that

00:09:06,940 --> 00:09:13,240
each lock line will send at most once

00:09:10,920 --> 00:09:17,500
duty something that we don't want right

00:09:13,240 --> 00:09:21,270
and the second option is to resend the

00:09:17,500 --> 00:09:24,160
localized to a different lock - instance

00:09:21,270 --> 00:09:26,980
but in this case we might have

00:09:24,160 --> 00:09:30,610
duplicates especially in the case that

00:09:26,980 --> 00:09:37,210
the first lock - already processed the

00:09:30,610 --> 00:09:40,690
lock lines and in order to solve the

00:09:37,210 --> 00:09:43,720
duplicates we are planning to do the

00:09:40,690 --> 00:09:48,850
follow thing to assign a unique ID for

00:09:43,720 --> 00:09:50,920
each lock line and try to duplicate the

00:09:48,850 --> 00:09:55,420
lock lines at the indexing time in

00:09:50,920 --> 00:09:58,870
elasticsearch this way we kind of

00:09:55,420 --> 00:10:05,710
simulate that each lock lines is then

00:09:58,870 --> 00:10:11,140
exactly once let's see how we can use

00:10:05,710 --> 00:10:14,560
fiber to collect the lock lines this is

00:10:11,140 --> 00:10:17,290
a very delicate topic especially because

00:10:14,560 --> 00:10:20,320
as you can see there are so many options

00:10:17,290 --> 00:10:25,270
on how to get the locks from your

00:10:20,320 --> 00:10:28,360
containers and all of these options they

00:10:25,270 --> 00:10:31,150
they have issues at least at the moment

00:10:28,360 --> 00:10:33,700
so I would like to go and show you a

00:10:31,150 --> 00:10:37,720
football a few possibilities on how to

00:10:33,700 --> 00:10:41,760
collect locks so first option will be to

00:10:37,720 --> 00:10:45,370
use the gal driver together of locks -

00:10:41,760 --> 00:10:48,370
but this solution has it because its

00:10:45,370 --> 00:10:52,240
advantage that is udp-based so you

00:10:48,370 --> 00:10:54,310
cannot guarantee that the local eyes are

00:10:52,240 --> 00:10:59,380
delivered and also there is no

00:10:54,310 --> 00:11:03,010
congestion control another option is to

00:10:59,380 --> 00:11:06,220
use the json driver from docker together

00:11:03,010 --> 00:11:10,510
with it--but this is a series really

00:11:06,220 --> 00:11:13,530
simple solution set up because JSON

00:11:10,510 --> 00:11:17,260
driver is a default driver in docker and

00:11:13,530 --> 00:11:17,830
another disadvantage is that it's easy

00:11:17,260 --> 00:11:20,410
to

00:11:17,830 --> 00:11:23,019
get a container metadata like the name

00:11:20,410 --> 00:11:25,930
of the container or the label of the

00:11:23,019 --> 00:11:27,910
container another thing is that the

00:11:25,930 --> 00:11:30,730
docker locks command works in this case

00:11:27,910 --> 00:11:35,050
the disadvantage is that some people

00:11:30,730 --> 00:11:38,800
think that by enabling the adjacent

00:11:35,050 --> 00:11:43,959
driver it slows down a bit the docker

00:11:38,800 --> 00:11:46,240
container another option is to use a six

00:11:43,959 --> 00:11:49,839
load driver together with a syslog

00:11:46,240 --> 00:11:51,579
server together we file bit the

00:11:49,839 --> 00:11:54,670
advantage of the solution is that you

00:11:51,579 --> 00:11:57,220
know the path where the log line loads

00:11:54,670 --> 00:12:00,459
the log files are sort and also comes

00:11:57,220 --> 00:12:03,250
with from rotating your FAFSA you get

00:12:00,459 --> 00:12:05,709
this for free so the disadvantage is

00:12:03,250 --> 00:12:10,029
that you need to manage another server

00:12:05,709 --> 00:12:12,610
the syslog server the metadata from the

00:12:10,029 --> 00:12:15,190
container are serialized so you have to

00:12:12,610 --> 00:12:19,690
be Sarai's them and in case of multi

00:12:15,190 --> 00:12:22,270
line is very difficult to to get that

00:12:19,690 --> 00:12:28,660
because the data from different

00:12:22,270 --> 00:12:31,930
containers is mixed another option is to

00:12:28,660 --> 00:12:37,000
use the journal d driver together with a

00:12:31,930 --> 00:12:40,149
bit the advantage a similar like in with

00:12:37,000 --> 00:12:44,290
jason driver you get support for

00:12:40,149 --> 00:12:46,990
container metadata and also dr. locks

00:12:44,290 --> 00:12:50,399
works but the disadvantage is that file

00:12:46,990 --> 00:12:53,290
B doesn't have yet support for journal D

00:12:50,399 --> 00:12:57,820
but there is a general D bit from the

00:12:53,290 --> 00:13:00,910
community that has that another option

00:12:57,820 --> 00:13:04,329
is to use share volume together we file

00:13:00,910 --> 00:13:07,600
file bit so what this means is that you

00:13:04,329 --> 00:13:09,940
mount a directory from your host into

00:13:07,600 --> 00:13:12,579
the container and then you instruct the

00:13:09,940 --> 00:13:17,589
application to write the locks into that

00:13:12,579 --> 00:13:20,589
shelf of directory this solution is very

00:13:17,589 --> 00:13:24,190
easy to set up especially when your

00:13:20,589 --> 00:13:27,329
application supports rotating your the

00:13:24,190 --> 00:13:27,329
Lak line the low path

00:13:28,270 --> 00:13:37,010
so disadvantage is that is difficult to

00:13:31,310 --> 00:13:41,260
pass a container metadata this way so in

00:13:37,010 --> 00:13:45,440
conclusion all the solutions they have

00:13:41,260 --> 00:13:48,200
file bit involved have support for at

00:13:45,440 --> 00:13:50,780
least ones delivery guarantee and also

00:13:48,200 --> 00:13:55,670
handles is able to handle back pressure

00:13:50,780 --> 00:13:57,890
and the other solutions like using a

00:13:55,670 --> 00:14:00,620
girl driver together vlog stash or

00:13:57,890 --> 00:14:04,340
friendly driver together luggage comes

00:14:00,620 --> 00:14:06,770
with no guarantees and here it's up as

00:14:04,340 --> 00:14:08,420
you see there are quite many solutions

00:14:06,770 --> 00:14:11,990
right there are advantages and

00:14:08,420 --> 00:14:14,270
disadvantages for each is this so you

00:14:11,990 --> 00:14:19,130
are called to decide which one fits

00:14:14,270 --> 00:14:21,830
better for your infrastructure now I

00:14:19,130 --> 00:14:24,680
would like to talk about metric bit met

00:14:21,830 --> 00:14:27,140
ribbit is a new bit that we just

00:14:24,680 --> 00:14:30,530
released a couple of weeks ago we fight

00:14:27,140 --> 00:14:32,870
those euro release and the way metric

00:14:30,530 --> 00:14:35,720
bit works is like interrogated

00:14:32,870 --> 00:14:37,760
periodically external services fetches

00:14:35,720 --> 00:14:41,480
metric from them and then push them to

00:14:37,760 --> 00:14:45,800
elasticsearch and i have support for a

00:14:41,480 --> 00:14:49,790
few services like apache nginx MongoDB

00:14:45,800 --> 00:14:52,850
my sequel PostgreSQL readies apache

00:14:49,790 --> 00:14:55,550
zookeeper and you can also add support

00:14:52,850 --> 00:15:00,560
for a different service by creating a

00:14:55,550 --> 00:15:05,000
module in metric bit metric bit comes

00:15:00,560 --> 00:15:07,880
with a system module that fetches system

00:15:05,000 --> 00:15:11,030
statistics from your servers like CPU

00:15:07,880 --> 00:15:14,630
usage memory usage disk i/o filesystem

00:15:11,030 --> 00:15:20,360
load network per CPU cores information

00:15:14,630 --> 00:15:23,540
per process information now let's see

00:15:20,360 --> 00:15:27,350
how you can use magic bit to collect the

00:15:23,540 --> 00:15:29,120
metrics from your containers and

00:15:27,350 --> 00:15:32,660
actually there are two options how you

00:15:29,120 --> 00:15:36,410
can do that only two options one option

00:15:32,660 --> 00:15:39,590
is by querying the docker API and with

00:15:36,410 --> 00:15:41,720
this you get information about the CPU

00:15:39,590 --> 00:15:44,660
and memory usage

00:15:41,720 --> 00:15:47,300
docker container information network

00:15:44,660 --> 00:15:50,240
information in and out bytes

00:15:47,300 --> 00:15:52,819
drop packets disk IO information like

00:15:50,240 --> 00:15:55,069
read verse right and also you get the

00:15:52,819 --> 00:15:57,139
status of your containers like how many

00:15:55,069 --> 00:16:01,269
containers are stopped how many

00:15:57,139 --> 00:16:01,269
continents are running things like this

00:16:01,540 --> 00:16:08,449
there is a docker module in metric bit

00:16:04,490 --> 00:16:13,579
that gets container information by

00:16:08,449 --> 00:16:16,850
querying the docker API doc remedial is

00:16:13,579 --> 00:16:19,160
not yet released but it's already in the

00:16:16,850 --> 00:16:21,860
master branch in github so if you want

00:16:19,160 --> 00:16:23,930
to check it out so the entities that is

00:16:21,860 --> 00:16:26,839
very easy to use you don't have to set

00:16:23,930 --> 00:16:29,300
up anything and it has access to

00:16:26,839 --> 00:16:37,810
container names and labels which is very

00:16:29,300 --> 00:16:40,699
important another option is to get

00:16:37,810 --> 00:16:44,899
container information by reading the

00:16:40,699 --> 00:16:47,089
cgroups data directly from /proc and the

00:16:44,899 --> 00:16:49,220
advantage is that you are able to get

00:16:47,089 --> 00:16:52,399
this information no matter what

00:16:49,220 --> 00:16:55,459
container technology you are using nomad

00:16:52,399 --> 00:16:57,649
not only docker another advantage is

00:16:55,459 --> 00:17:01,449
that you don't have to query the docker

00:16:57,649 --> 00:17:05,750
api which and sometimes my have security

00:17:01,449 --> 00:17:08,120
concerns but the disadvantage of this

00:17:05,750 --> 00:17:10,309
solution is that you are not able to get

00:17:08,120 --> 00:17:14,799
the container name and the container

00:17:10,309 --> 00:17:14,799
labels you can get only the container ID

00:17:15,880 --> 00:17:22,549
so if you are enabling the cgroups

00:17:19,459 --> 00:17:26,089
option in the configuration file off of

00:17:22,549 --> 00:17:28,580
metro beat then automatically the

00:17:26,089 --> 00:17:35,409
process information will be enhanced

00:17:28,580 --> 00:17:38,240
with c group information no matter what

00:17:35,409 --> 00:17:40,760
option you are using to collect the

00:17:38,240 --> 00:17:44,330
container information you might want to

00:17:40,760 --> 00:17:46,880
run metric bit in a container on your

00:17:44,330 --> 00:17:48,679
host in order to monitor all the

00:17:46,880 --> 00:17:50,980
containers that are running on that

00:17:48,679 --> 00:17:50,980
house

00:17:52,520 --> 00:17:56,660
the objection that I get when I'm

00:17:54,770 --> 00:17:59,330
presenting metric bill is that

00:17:56,660 --> 00:18:03,260
elasticsearch is not meant to be a time

00:17:59,330 --> 00:18:05,929
series database because people think of

00:18:03,260 --> 00:18:08,960
elastic search is a search engine that

00:18:05,929 --> 00:18:12,530
is very good for text search but is not

00:18:08,960 --> 00:18:14,270
so efficient for story number numbers

00:18:12,530 --> 00:18:18,860
and actually this is not really true

00:18:14,270 --> 00:18:21,350
elastic search and lock - and loosing

00:18:18,860 --> 00:18:24,530
the project behind elastic search

00:18:21,350 --> 00:18:29,780
improve quite a lot in storing the

00:18:24,530 --> 00:18:33,130
numbers in recently and this actually

00:18:29,780 --> 00:18:36,410
got even better with elastic search 5.0

00:18:33,130 --> 00:18:43,010
with the introduction of a new storage

00:18:36,410 --> 00:18:46,059
engine that is based on big ad trees so

00:18:43,010 --> 00:18:48,440
let's see what PKD trees are so they're

00:18:46,059 --> 00:18:50,690
initially they were introduced behave

00:18:48,440 --> 00:18:54,440
because they have support for multiple

00:18:50,690 --> 00:18:56,720
dimensions think of geoip points where

00:18:54,440 --> 00:18:59,510
you have two dimensions one longitude

00:18:56,720 --> 00:19:03,230
and one latitude but this proved to be

00:18:59,510 --> 00:19:06,350
very efficient only also for one

00:19:03,230 --> 00:19:06,890
dimension value and these are numbers

00:19:06,350 --> 00:19:11,230
right

00:19:06,890 --> 00:19:14,330
so we biggity trees is faster to index

00:19:11,230 --> 00:19:21,050
faster to query more disk efficient and

00:19:14,330 --> 00:19:23,330
more memory efficient another

00:19:21,050 --> 00:19:27,470
disadvantage that elasticsearch had

00:19:23,330 --> 00:19:31,610
before 5.0 was that is not was not able

00:19:27,470 --> 00:19:35,390
to compress the flow values and they

00:19:31,610 --> 00:19:37,870
always they were stored on 4 bytes there

00:19:35,390 --> 00:19:41,780
are quite many compression algorithms

00:19:37,870 --> 00:19:44,690
and think of this gorilla paper from

00:19:41,780 --> 00:19:46,940
Facebook they describe the compression

00:19:44,690 --> 00:19:49,970
algorithm but unfortunately they cannot

00:19:46,940 --> 00:19:53,980
be used because of the Lucine way of

00:19:49,970 --> 00:19:58,309
storing the flow good numbers the values

00:19:53,980 --> 00:20:01,820
so we 5 toes with elasticsearch 5.0 as a

00:19:58,309 --> 00:20:04,910
workaround there were two new flow types

00:20:01,820 --> 00:20:06,470
introduced one of them is half float

00:20:04,910 --> 00:20:10,159
which is always

00:20:06,470 --> 00:20:12,820
start on two bytes and this has good

00:20:10,159 --> 00:20:15,230
precision for small numbers but the

00:20:12,820 --> 00:20:18,650
precision degree date with the bigger

00:20:15,230 --> 00:20:22,460
number it is the second type is scale

00:20:18,650 --> 00:20:25,850
float which is the idea is to store a

00:20:22,460 --> 00:20:29,090
flow numbers as an integral using a

00:20:25,850 --> 00:20:31,340
scaling factor so what this means is for

00:20:29,090 --> 00:20:36,320
example if you have a scaling factor of

00:20:31,340 --> 00:20:39,770
100 you multiply 100 by your float value

00:20:36,320 --> 00:20:42,860
and the result you store it as integral

00:20:39,770 --> 00:20:46,490
in elasticsearch and when you query

00:20:42,860 --> 00:20:51,350
elasticsearch you divide the value by

00:20:46,490 --> 00:20:54,169
100 and you get resolve AK and this is

00:20:51,350 --> 00:20:58,190
useful for example if you store

00:20:54,169 --> 00:21:05,090
percentages think of for example the CPU

00:20:58,190 --> 00:21:08,110
usage right so why elasticsearch is good

00:21:05,090 --> 00:21:11,270
for time series for storing time series

00:21:08,110 --> 00:21:14,659
so because it comes with horizontal

00:21:11,270 --> 00:21:17,929
scalability it has a mature support for

00:21:14,659 --> 00:21:20,900
clustering it comes with flexible

00:21:17,929 --> 00:21:23,929
aggregations including moving averages

00:21:20,900 --> 00:21:28,309
and hot winters that you use for anomaly

00:21:23,929 --> 00:21:32,030
detection you can use a single storage

00:21:28,309 --> 00:21:35,750
system to store locks and metrics comes

00:21:32,030 --> 00:21:40,039
with specialized UI for visualizing the

00:21:35,750 --> 00:21:42,799
data especially a matrix like a time

00:21:40,039 --> 00:21:44,929
line graph on ax and it comes with a

00:21:42,799 --> 00:21:48,770
great ecosystem in the sense that comes

00:21:44,929 --> 00:21:54,260
with alerts alerting and reporting and

00:21:48,770 --> 00:21:58,870
things like this now let's have a look

00:21:54,260 --> 00:22:02,360
to packet bit which is for getting the

00:21:58,870 --> 00:22:04,850
the net that is listening to the network

00:22:02,360 --> 00:22:08,780
traffic so the way packet bit works is

00:22:04,850 --> 00:22:11,750
like install as an agent on your server

00:22:08,780 --> 00:22:14,450
is listening to the network traffic that

00:22:11,750 --> 00:22:16,330
is exchanged between your servers then

00:22:14,450 --> 00:22:18,980
the codes upper layer protocols

00:22:16,330 --> 00:22:20,000
correlate the request is a response into

00:22:18,980 --> 00:22:25,460
transactions

00:22:20,000 --> 00:22:31,460
and send them to elasticsearch and from

00:22:25,460 --> 00:22:36,980
the decoders we can note HTTP MongoDB my

00:22:31,460 --> 00:22:40,010
sequel postcards kill DNS Redis memcache

00:22:36,980 --> 00:22:44,630
Cassandra and so on and here you can add

00:22:40,010 --> 00:22:51,620
support for another protocol by creating

00:22:44,630 --> 00:22:54,890
a module in back a bit if you have an

00:22:51,620 --> 00:22:58,880
unknown traffic meaning that there is no

00:22:54,890 --> 00:23:01,850
decoder for for that protocol or that

00:22:58,880 --> 00:23:05,180
your traffic is encrypted you can still

00:23:01,850 --> 00:23:07,010
see a few details about the traffic that

00:23:05,180 --> 00:23:10,460
is exchanged between your servers and

00:23:07,010 --> 00:23:12,920
this can be done within flows so if

00:23:10,460 --> 00:23:14,900
flows you can have a few information

00:23:12,920 --> 00:23:21,080
like the number of packets the number of

00:23:14,900 --> 00:23:23,990
bytes for transmissions and so on packet

00:23:21,080 --> 00:23:26,420
which can also be used to monitor the

00:23:23,990 --> 00:23:29,660
traffic exchange between your containers

00:23:26,420 --> 00:23:32,690
you Joey all you have to do is to

00:23:29,660 --> 00:23:35,180
install packet bit in a container on

00:23:32,690 --> 00:23:37,490
your host and you'll be able to monitor

00:23:35,180 --> 00:23:40,640
the traffic that is flowing between all

00:23:37,490 --> 00:23:46,580
the other containers that are installed

00:23:40,640 --> 00:23:48,970
on that host let me show you now quick

00:23:46,580 --> 00:23:48,970
demo

00:24:28,930 --> 00:24:32,320
one chicken

00:25:49,860 --> 00:26:00,640
is it big enough okay so now I'd like to

00:25:55,750 --> 00:26:04,140
show you how you can monitor your

00:26:00,640 --> 00:26:06,190
containers using the elastic stack and

00:26:04,140 --> 00:26:08,140
for this demo I Cree

00:26:06,190 --> 00:26:11,500
I have a virtual machine with ubuntu

00:26:08,140 --> 00:26:16,480
install on my macbook and we rinse old

00:26:11,500 --> 00:26:18,160
file big packet bit and metric bit it's

00:26:16,480 --> 00:26:21,760
very easy to install them so you just

00:26:18,160 --> 00:26:26,560
need to go to the web interface to the

00:26:21,760 --> 00:26:28,900
web page of elastic download the package

00:26:26,560 --> 00:26:31,510
that you want in my case it was a

00:26:28,900 --> 00:26:35,320
dipping package and you install it and

00:26:31,510 --> 00:26:41,440
then you have so that's why I'm skipping

00:26:35,320 --> 00:26:45,580
this step and now let's let's start on

00:26:41,440 --> 00:26:48,070
let's start container with we're ready

00:26:45,580 --> 00:26:51,810
sees install and music as you can see

00:26:48,070 --> 00:26:56,350
here I'm passing some labels in order to

00:26:51,810 --> 00:26:59,440
to see how the containers labels can be

00:26:56,350 --> 00:27:05,830
monitored with with a bit so and I am

00:26:59,440 --> 00:27:10,330
starting the container and now let's

00:27:05,830 --> 00:27:14,290
let's also let's open the configuration

00:27:10,330 --> 00:27:16,480
file or file off I'll be to show you how

00:27:14,290 --> 00:27:19,740
you configure five bit in order to get

00:27:16,480 --> 00:27:22,750
the locks from from the Redis container

00:27:19,740 --> 00:27:28,210
here so here you need to configure a

00:27:22,750 --> 00:27:32,590
prospector and with a path from where to

00:27:28,210 --> 00:27:33,790
fetch the locks as you guys know if you

00:27:32,590 --> 00:27:38,220
by default

00:27:33,790 --> 00:27:44,380
the container has a JSON driver enabled

00:27:38,220 --> 00:27:47,250
so it writes the the lock lines in a

00:27:44,380 --> 00:27:55,450
JSON format under this

00:27:47,250 --> 00:28:01,150
so basically in monitors these paths and

00:27:55,450 --> 00:28:06,460
now if I start file bit then five it

00:28:01,150 --> 00:28:08,440
will send all the lock lines to to

00:28:06,460 --> 00:28:13,810
elasticsearch and store them in a

00:28:08,440 --> 00:28:19,180
special in the index in in file bit the

00:28:13,810 --> 00:28:23,830
file bit index let me show you so if we

00:28:19,180 --> 00:28:27,520
go in the index patterns I have already

00:28:23,830 --> 00:28:36,010
created the file bit index right and now

00:28:27,520 --> 00:28:38,320
let's go to the discovery page select

00:28:36,010 --> 00:28:40,890
the file bit index and we'll be able to

00:28:38,320 --> 00:28:43,960
see all the lock lines from that

00:28:40,890 --> 00:28:47,800
container ready that I just created and

00:28:43,960 --> 00:28:53,110
as you can see let's go to one we can

00:28:47,800 --> 00:28:57,280
see here um basically one lock line that

00:28:53,110 --> 00:29:03,310
the red is um container print when it

00:28:57,280 --> 00:29:05,800
starts and as I said the locks are

00:29:03,310 --> 00:29:08,500
written in a JSON format so we'll be

00:29:05,800 --> 00:29:11,680
able to see that here in in a JSON

00:29:08,500 --> 00:29:15,430
format so it has the log stream and the

00:29:11,680 --> 00:29:20,880
time in addition five it also export the

00:29:15,430 --> 00:29:24,370
offset of the log line and from where

00:29:20,880 --> 00:29:36,040
basically those and the path in the name

00:29:24,370 --> 00:29:40,420
of the of the log file okay now let's

00:29:36,040 --> 00:29:43,090
let's see how we can get the matrix from

00:29:40,420 --> 00:29:45,460
from our Redis container and for that

00:29:43,090 --> 00:29:50,590
let me show you the configuration file

00:29:45,460 --> 00:29:53,560
of met repeat and here you can configure

00:29:50,590 --> 00:29:56,260
multiple modules for example you can we

00:29:53,560 --> 00:29:58,840
have here configured the system will use

00:29:56,260 --> 00:30:00,550
in order to get system statistics like

00:29:58,840 --> 00:30:05,890
CPU usage

00:30:00,550 --> 00:30:10,200
five system and so on another module

00:30:05,890 --> 00:30:13,990
that I enabled is Redis module that

00:30:10,200 --> 00:30:16,929
interrogate the ready servers and get a

00:30:13,990 --> 00:30:19,750
few information from the register and

00:30:16,929 --> 00:30:21,520
here you can specify what kind of

00:30:19,750 --> 00:30:24,790
information you want to get from the

00:30:21,520 --> 00:30:28,570
ready server and how often to query the

00:30:24,790 --> 00:30:32,350
Redis service and of course the host of

00:30:28,570 --> 00:30:34,840
the ready server and another module that

00:30:32,350 --> 00:30:38,820
I enable is the docker module in order

00:30:34,840 --> 00:30:43,660
to get information about our Redis

00:30:38,820 --> 00:30:47,650
container we had to enable that and here

00:30:43,660 --> 00:30:51,940
as you see we can I hope you can see

00:30:47,650 --> 00:30:54,760
because of the colors and here you can

00:30:51,940 --> 00:30:57,730
specify what kind of information you are

00:30:54,760 --> 00:31:00,670
interested from your docker container of

00:30:57,730 --> 00:31:02,700
course you can specify how often to

00:31:00,670 --> 00:31:06,309
fetch the information from the container

00:31:02,700 --> 00:31:11,230
the host and also optionally the socket

00:31:06,309 --> 00:31:16,270
that is used okay now if we are starting

00:31:11,230 --> 00:31:18,490
metric bid magic bit will send the

00:31:16,270 --> 00:31:23,340
matrix information to a special index

00:31:18,490 --> 00:31:30,160
better in in in met repeat so let's

00:31:23,340 --> 00:31:32,140
let's select the metric bit index and as

00:31:30,160 --> 00:31:37,330
you can see here I get all kinds of

00:31:32,140 --> 00:31:45,280
information so let's drill down and for

00:31:37,330 --> 00:31:47,380
example check the system statistics so

00:31:45,280 --> 00:31:50,980
for example here I get information about

00:31:47,380 --> 00:31:56,080
the file system statistics how much

00:31:50,980 --> 00:32:01,960
available how many mount points and fix

00:31:56,080 --> 00:32:04,330
like this right also we can get

00:32:01,960 --> 00:32:07,450
information about the processes for

00:32:04,330 --> 00:32:15,150
example and we get one event for each

00:32:07,450 --> 00:32:15,150
process yeah like so

00:32:17,100 --> 00:32:22,870
the state of the process the PID of the

00:32:20,080 --> 00:32:27,610
process the memory that is used by this

00:32:22,870 --> 00:32:36,059
process in the ladies right and now let

00:32:27,610 --> 00:32:36,059
me show you information about readies

00:32:38,160 --> 00:32:45,820
and you can see here I get information

00:32:41,950 --> 00:32:53,500
from the ready service like yeah things

00:32:45,820 --> 00:32:58,090
like yeah as you can see kind of a lot

00:32:53,500 --> 00:33:04,630
of information but let's go to some easy

00:32:58,090 --> 00:33:09,070
ones like yeah for some persons of

00:33:04,630 --> 00:33:15,210
operating system the server version

00:33:09,070 --> 00:33:18,480
things like this right set up time okay

00:33:15,210 --> 00:33:24,210
and now I would like to show you also

00:33:18,480 --> 00:33:28,360
the docker information and for that we

00:33:24,210 --> 00:33:30,280
we filter to the doctor module and as

00:33:28,360 --> 00:33:34,090
you can see we we get quite a lot of

00:33:30,280 --> 00:33:37,059
information for example we get

00:33:34,090 --> 00:33:39,790
information about our docker container

00:33:37,059 --> 00:33:42,820
as you can see here we get the labels

00:33:39,790 --> 00:33:46,210
that I passed when starting the Redis

00:33:42,820 --> 00:33:50,970
container and also I get the socket name

00:33:46,210 --> 00:33:54,809
and memory usage things like this right

00:33:50,970 --> 00:33:59,590
and what is interesting is that each

00:33:54,809 --> 00:34:05,320
metric has a round-trip time that is

00:33:59,590 --> 00:34:09,010
counted automatically now let's drill a

00:34:05,320 --> 00:34:15,220
bit down and show you a few information

00:34:09,010 --> 00:34:17,710
about the docker container like how many

00:34:15,220 --> 00:34:20,379
containers are running how many are

00:34:17,710 --> 00:34:24,639
stopped how they are post and things

00:34:20,379 --> 00:34:28,859
like this right and the name the idea of

00:34:24,639 --> 00:34:28,859
the container the name of the container

00:34:31,679 --> 00:34:40,329
yes the name of the container is not

00:34:33,970 --> 00:34:48,970
here another you can also get statistics

00:34:40,329 --> 00:34:51,490
about your containers like as you can

00:34:48,970 --> 00:34:54,579
see here is the container ID when the

00:34:51,490 --> 00:35:00,520
container was created just a few minutes

00:34:54,579 --> 00:35:06,150
ago the uptime of the containers it's

00:35:00,520 --> 00:35:06,150
seven minutes and things like this right

00:35:07,890 --> 00:35:18,970
now let's let see some details about the

00:35:15,910 --> 00:35:21,730
CPU usage of the container as you can

00:35:18,970 --> 00:35:25,150
see we have the CPU usage in percentage

00:35:21,730 --> 00:35:28,630
and what's important here is a container

00:35:25,150 --> 00:35:31,690
name right and which is because usually

00:35:28,630 --> 00:35:34,270
you get a container ID but it's very

00:35:31,690 --> 00:35:36,880
difficult to match which one you are

00:35:34,270 --> 00:35:39,250
interested so that's why in my opinion I

00:35:36,880 --> 00:35:45,280
think it's nice to have the name of the

00:35:39,250 --> 00:35:49,450
container so in this in this setup as

00:35:45,280 --> 00:35:51,250
you noticed we are using we are getting

00:35:49,450 --> 00:35:53,680
the information from the containers

00:35:51,250 --> 00:35:56,319
using the docker API because I didn't

00:35:53,680 --> 00:35:58,569
set anything up as you will see it's out

00:35:56,319 --> 00:36:00,819
of the box and that's why in the

00:35:58,569 --> 00:36:02,619
solution we can get with this option

00:36:00,819 --> 00:36:11,050
with solution we can get the name of the

00:36:02,619 --> 00:36:13,329
container okay now let's see how we can

00:36:11,050 --> 00:36:16,359
get the traffic that is exchanged

00:36:13,329 --> 00:36:21,819
between York our containers and for that

00:36:16,359 --> 00:36:25,170
we need to come we need to change the

00:36:21,819 --> 00:36:29,260
configuration file of packet bit and

00:36:25,170 --> 00:36:31,000
therefore basically the most important

00:36:29,260 --> 00:36:34,030
option that you need to configure in

00:36:31,000 --> 00:36:36,310
packet bit is the interface on which to

00:36:34,030 --> 00:36:40,359
listen for for the traffic and in our

00:36:36,310 --> 00:36:42,130
case is any and also for each type of

00:36:40,359 --> 00:36:44,019
protocol you can space

00:36:42,130 --> 00:36:48,640
by the port where to listen for the

00:36:44,019 --> 00:36:50,799
traffic so in our case we don't have to

00:36:48,640 --> 00:36:53,410
change anything we can use the default

00:36:50,799 --> 00:36:59,430
configuration so let me generate some

00:36:53,410 --> 00:37:03,970
know some ready strophic by generating a

00:36:59,430 --> 00:37:08,200
set command in Redis and now if we are

00:37:03,970 --> 00:37:11,170
starting picky bit we'll be able to pack

00:37:08,200 --> 00:37:14,950
admit we send all the registrants

00:37:11,170 --> 00:37:18,460
actions to elasticsearch and we'll be

00:37:14,950 --> 00:37:21,660
able to see them so let me first select

00:37:18,460 --> 00:37:21,660
the packet bit index

00:37:22,620 --> 00:37:25,989
[Music]

00:37:37,440 --> 00:37:49,269
let me generate again it there but just

00:37:42,880 --> 00:37:52,569
to be the top one yeah as you can see

00:37:49,269 --> 00:37:55,359
here the query is he's here

00:37:52,569 --> 00:38:00,099
it's the query is already so query that

00:37:55,359 --> 00:38:02,920
I already I started and you can also see

00:38:00,099 --> 00:38:04,960
a few details about that query what is

00:38:02,920 --> 00:38:09,220
also important here as I as I mentioned

00:38:04,960 --> 00:38:14,710
in the presentation you get you can also

00:38:09,220 --> 00:38:16,480
get if you are not able to to see to

00:38:14,710 --> 00:38:19,180
decode your traffic or you have

00:38:16,480 --> 00:38:22,740
encrypted traffic you can see details a

00:38:19,180 --> 00:38:28,839
few basic details of you about your

00:38:22,740 --> 00:38:40,089
traffic or by using flows and let me

00:38:28,839 --> 00:38:45,430
filter for those as you can see here I

00:38:40,089 --> 00:38:47,529
have a few details like the number of

00:38:45,430 --> 00:38:49,359
bytes and the number of packets

00:38:47,529 --> 00:38:55,670
exchanged between the source and the

00:38:49,359 --> 00:39:01,170
destination together with um

00:38:55,670 --> 00:39:03,180
yeah the IPO feature for for each so the

00:39:01,170 --> 00:39:04,500
number of bytes and packets exchange

00:39:03,180 --> 00:39:11,070
between the source and the destination

00:39:04,500 --> 00:39:16,340
and the other way around right okay I

00:39:11,070 --> 00:39:20,790
think that's it if you now if you have

00:39:16,340 --> 00:39:24,330
before going to the start if you have

00:39:20,790 --> 00:39:26,760
questions I would like to mention that

00:39:24,330 --> 00:39:30,120
on in the last day of the conference

00:39:26,760 --> 00:39:32,850
there is a hackathon about the elastic

00:39:30,120 --> 00:39:37,140
page where a colleague of mine will

00:39:32,850 --> 00:39:40,200
present on how you can create your own

00:39:37,140 --> 00:39:44,190
beat or how you can contribute to open

00:39:40,200 --> 00:39:47,910
source community by creating a module on

00:39:44,190 --> 00:39:49,890
in one of harpies so yeah thank you very

00:39:47,910 --> 00:40:01,080
much for your attention now if you have

00:39:49,890 --> 00:40:03,240
questions feel free to ask me sorry hi

00:40:01,080 --> 00:40:05,820
talking about five beat the biggest

00:40:03,240 --> 00:40:09,090
issue is always handling the file

00:40:05,820 --> 00:40:11,150
rotation what is if I've been doing

00:40:09,090 --> 00:40:16,440
about it

00:40:11,150 --> 00:40:19,880
so just to clarify your question so the

00:40:16,440 --> 00:40:22,830
application is needs to take care of

00:40:19,880 --> 00:40:25,710
rotating the low file but in case of a

00:40:22,830 --> 00:40:35,220
server tating five it is able to detect

00:40:25,710 --> 00:40:37,830
that so five it is continually tracking

00:40:35,220 --> 00:40:44,120
the lock until it stopped and then it

00:40:37,830 --> 00:40:44,120
takes the other one oh yeah okay yes

00:40:51,000 --> 00:40:56,900
do you have any performance analysis on

00:40:53,970 --> 00:40:59,580
the number of metrics that can be

00:40:56,900 --> 00:41:01,560
inserted per second and like in

00:40:59,580 --> 00:41:06,600
comparison to other time series

00:41:01,560 --> 00:41:08,730
databases as well as like queries yeah

00:41:06,600 --> 00:41:10,880
it formed that that's a really good

00:41:08,730 --> 00:41:15,120
question unfortunately I don't have a

00:41:10,880 --> 00:41:19,920
formal answer for that we are we didn't

00:41:15,120 --> 00:41:22,320
do any official benchmarking on that we

00:41:19,920 --> 00:41:29,640
only have some internal ones that yeah I

00:41:22,320 --> 00:41:34,620
don't know them by heart but to answer

00:41:29,640 --> 00:41:36,870
your question roughly it there are some

00:41:34,620 --> 00:41:39,750
comparison between elasticsearch and

00:41:36,870 --> 00:41:43,950
influx DP and of course the elastic

00:41:39,750 --> 00:41:47,390
search cannot really compete with other

00:41:43,950 --> 00:41:52,200
systems that are specializing metrics

00:41:47,390 --> 00:41:55,050
but the difference is not big and you

00:41:52,200 --> 00:41:58,530
have to think that elastic search is a

00:41:55,050 --> 00:42:02,190
generic system that gives you the chance

00:41:58,530 --> 00:42:05,220
to use a single system to collect all

00:42:02,190 --> 00:42:08,850
kinds of data not only metrics but also

00:42:05,220 --> 00:42:12,870
locks compared to other metrics vendors

00:42:08,850 --> 00:42:15,660
or specialized vendors that are

00:42:12,870 --> 00:42:22,260
specializing metrics right that can do

00:42:15,660 --> 00:42:26,310
only metrics so our goal is to to offer

00:42:22,260 --> 00:42:30,720
a system that is generic enough and we

00:42:26,310 --> 00:42:32,670
are trying to already with all the the

00:42:30,720 --> 00:42:36,720
new improvement as we did it for five to

00:42:32,670 --> 00:42:39,750
zero we are going towards competing with

00:42:36,720 --> 00:42:44,580
the others completely source for metrics

00:42:39,750 --> 00:42:47,270
that's why I I presented quite in-depth

00:42:44,580 --> 00:42:53,550
in my presentation that elastic search

00:42:47,270 --> 00:42:56,400
is an option for a time series I don't

00:42:53,550 --> 00:42:59,490
question you mentioned in case the next

00:42:56,400 --> 00:43:01,320
stage in five bits is down everything is

00:42:59,490 --> 00:43:02,640
a queued up with five bit itself it's

00:43:01,320 --> 00:43:04,140
clarissa Rican okay let me just

00:43:02,640 --> 00:43:06,059
repeating sorry

00:43:04,140 --> 00:43:07,979
in case the next stage is five meters

00:43:06,059 --> 00:43:10,380
down you mentioned that five bits is

00:43:07,979 --> 00:43:12,479
just just storing the messages by its

00:43:10,380 --> 00:43:14,459
own so means the recommendation is

00:43:12,479 --> 00:43:16,170
there's no need of an additional queuing

00:43:14,459 --> 00:43:19,440
mechanism alight why this is a Kafka

00:43:16,170 --> 00:43:20,489
yes that's true so in case the next

00:43:19,440 --> 00:43:23,099
stage is down

00:43:20,489 --> 00:43:25,559
for example log stash or elasticsearch

00:43:23,099 --> 00:43:28,619
file it doesn't have where to send the

00:43:25,559 --> 00:43:32,039
data so what it does it doesn't queue or

00:43:28,619 --> 00:43:35,489
buffer the lock lines in addition in

00:43:32,039 --> 00:43:39,329
additional place right on disk it waits

00:43:35,489 --> 00:43:41,670
until the next stage comes back again or

00:43:39,329 --> 00:43:47,549
if there is another one available

00:43:41,670 --> 00:43:53,519
it sends into the other one anymore

00:43:47,549 --> 00:43:56,459
sure hi

00:43:53,519 --> 00:43:59,940
concerning the beats in general how do

00:43:56,459 --> 00:44:02,190
they manage the connections is it do

00:43:59,940 --> 00:44:04,469
they establish the connection on demand

00:44:02,190 --> 00:44:12,119
to send off the data or the is it's a

00:44:04,469 --> 00:44:13,949
stable connection um if it's a if it is

00:44:12,119 --> 00:44:15,779
a stable connection so if one of the

00:44:13,949 --> 00:44:18,660
beats file be to a metric beat is

00:44:15,779 --> 00:44:20,430
getting supposed to send new data do

00:44:18,660 --> 00:44:21,900
they then establish the connection or

00:44:20,430 --> 00:44:24,359
they do they constantly keep a

00:44:21,900 --> 00:44:28,170
connection open yeah so idea is to to

00:44:24,359 --> 00:44:29,819
keep the connection open just yeah not

00:44:28,170 --> 00:44:32,279
to open too many connections yet and

00:44:29,819 --> 00:44:34,140
nowadays and how do they notice if you

00:44:32,279 --> 00:44:37,559
so what is your recommendation if you

00:44:34,140 --> 00:44:40,079
have led a couple of thousands of beats

00:44:37,559 --> 00:44:45,059
how do you scale then on the on the

00:44:40,079 --> 00:44:47,479
receiving part oh that's an interesting

00:44:45,059 --> 00:44:50,759
question

00:44:47,479 --> 00:44:53,670
yeah I'm not sure I can give you the

00:44:50,759 --> 00:44:57,359
right answer for that so we we did we

00:44:53,670 --> 00:45:00,329
invested quite a lot of time in in

00:44:57,359 --> 00:45:03,299
keeping the improving the connection

00:45:00,329 --> 00:45:05,430
between up the BT and log station also

00:45:03,299 --> 00:45:07,709
the last with the last research and of

00:45:05,430 --> 00:45:11,519
course we deal with all these things

00:45:07,709 --> 00:45:14,039
that you said but I don't know I don't

00:45:11,519 --> 00:45:15,900
know to to give you the technical

00:45:14,039 --> 00:45:17,970
details about this okay

00:45:15,900 --> 00:45:20,850
right but if you want to know more

00:45:17,970 --> 00:45:23,790
and you are here for the hackathon you

00:45:20,850 --> 00:45:26,580
can ask the guy that already that he

00:45:23,790 --> 00:45:28,560
implemented this part and he can give

00:45:26,580 --> 00:45:32,970
you I'm sure a lot more information

00:45:28,560 --> 00:45:36,060
about that hey a couple of questions

00:45:32,970 --> 00:45:37,950
first to figure out whether I've

00:45:36,060 --> 00:45:40,200
understood this correctly will the file

00:45:37,950 --> 00:45:42,840
beat BD officially suggested way to

00:45:40,200 --> 00:45:47,460
ingest the log data in elasticsearch in

00:45:42,840 --> 00:45:49,980
the future it depends on your

00:45:47,460 --> 00:45:52,770
infrastructure so the idea if you are

00:45:49,980 --> 00:45:55,440
asking between what's better 5-bit or

00:45:52,770 --> 00:45:57,420
log stash then the answer is that they

00:45:55,440 --> 00:46:02,550
have different use case right file bit

00:45:57,420 --> 00:46:05,550
is for getting the logs from from end

00:46:02,550 --> 00:46:09,360
edges and log stash is more considered

00:46:05,550 --> 00:46:12,630
on the on the edge on the server side

00:46:09,360 --> 00:46:17,640
and soon a lock stage will also have

00:46:12,630 --> 00:46:19,680
support for queuing so it would be its

00:46:17,640 --> 00:46:22,800
place would be at the edge point

00:46:19,680 --> 00:46:24,810
I hope this answer your question okay

00:46:22,800 --> 00:46:26,400
and I really like the first question

00:46:24,810 --> 00:46:28,260
about the log rotation and you said that

00:46:26,400 --> 00:46:30,240
it's supported but there's a lots of

00:46:28,260 --> 00:46:32,340
different scenarios how applications

00:46:30,240 --> 00:46:35,070
handle the rotation of the log file so

00:46:32,340 --> 00:46:37,500
some of them overwrite the previous file

00:46:35,070 --> 00:46:39,150
some of them write in different log file

00:46:37,500 --> 00:46:40,830
with a different file name every day or

00:46:39,150 --> 00:46:42,270
every hour then there's some

00:46:40,830 --> 00:46:43,190
applications that update the timestamp

00:46:42,270 --> 00:46:46,799
on the file

00:46:43,190 --> 00:46:50,369
have you delved into this

00:46:46,799 --> 00:46:53,689
oh yeah yeah yeah we did that

00:46:50,369 --> 00:46:56,039
fortunately yeah we spend a lot of time

00:46:53,689 --> 00:46:59,099
trying to figure out what's the best

00:46:56,039 --> 00:47:01,890
solution for for attacking all these

00:46:59,099 --> 00:47:06,119
problems and we have quite a bit of

00:47:01,890 --> 00:47:09,420
configuration options in order to have

00:47:06,119 --> 00:47:12,179
support for one case or another if you

00:47:09,420 --> 00:47:13,650
if you are interested you can check the

00:47:12,179 --> 00:47:18,650
documentation I think in our

00:47:13,650 --> 00:47:22,799
documentation it's described in details

00:47:18,650 --> 00:47:25,949
yeah all this configuration option that

00:47:22,799 --> 00:47:28,019
you can use to a yeah to solve one

00:47:25,949 --> 00:47:30,989
option or a other option yeah but that's

00:47:28,019 --> 00:47:32,130
true it yeah it was a bit of a hassle

00:47:30,989 --> 00:47:35,880
yeah thank you I'll check that

00:47:32,130 --> 00:47:40,079
definitely Thanks and yeah if you have

00:47:35,880 --> 00:47:44,039
questions feel free to send us a message

00:47:40,079 --> 00:47:47,009
on on our forum at discuss total ass to

00:47:44,039 --> 00:47:50,039
the CEO and also if you find box feel

00:47:47,009 --> 00:47:52,859
free to post them on github on our

00:47:50,039 --> 00:48:00,660
github repository anyone else more

00:47:52,859 --> 00:48:02,579
questions yeah hi I have one general

00:48:00,660 --> 00:48:05,119
question regarding bits products are

00:48:02,579 --> 00:48:09,119
going to release something some alerting

00:48:05,119 --> 00:48:14,219
model like alert bits or some slices for

00:48:09,119 --> 00:48:17,609
animal detection Samsung okay so I'm not

00:48:14,219 --> 00:48:21,390
sure I totally understand your question

00:48:17,609 --> 00:48:23,999
so you are the question is do you refer

00:48:21,390 --> 00:48:27,390
to implement the bit that is handling

00:48:23,999 --> 00:48:32,969
security cases oh I do want to refer to

00:48:27,390 --> 00:48:38,670
a half support for alerting okay so for

00:48:32,969 --> 00:48:43,739
that it's so elastic is coming it has

00:48:38,670 --> 00:48:48,420
already water which is a closed source

00:48:43,739 --> 00:48:50,400
of solution for for alerting and yeah

00:48:48,420 --> 00:48:52,709
you can use that we're together with

00:48:50,400 --> 00:48:56,489
four before but unfortunately it doesn't

00:48:52,709 --> 00:48:58,559
support like on a multi detection yeah

00:48:56,489 --> 00:49:00,420
for anomaly detection I don't know if

00:48:58,559 --> 00:49:04,980
you guys know but

00:49:00,420 --> 00:49:08,700
stick recently acquired pre Lert and we

00:49:04,980 --> 00:49:10,130
are working in in integrating a solution

00:49:08,700 --> 00:49:14,280
for that into elastics

00:49:10,130 --> 00:49:17,030
so yeah it's coming up soon thank you we

00:49:14,280 --> 00:49:17,030
are working on that

00:49:17,510 --> 00:49:22,920
okay so we are done no more questions

00:49:21,150 --> 00:49:31,310
thank you very much thank you very much

00:49:22,920 --> 00:49:31,310

YouTube URL: https://www.youtube.com/watch?v=5Z028ubqpt0


