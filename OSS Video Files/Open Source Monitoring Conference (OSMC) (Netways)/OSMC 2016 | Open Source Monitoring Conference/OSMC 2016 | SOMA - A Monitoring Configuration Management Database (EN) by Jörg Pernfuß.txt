Title: OSMC 2016 | SOMA - A Monitoring Configuration Management Database (EN) by Jörg Pernfuß
Publication date: 2016-12-14
Playlist: OSMC 2016 | Open Source Monitoring Conference
Description: 
	SOMA ist ein API-zentrischer Konfigurationsdiest zur Provisionierung beliebiger angeschlossener Monitoringsysteme. Es unterstuetzt die Stammdatenorganisation in Baumstrukturen mit Vererbung von Elementeigenschaften. Dies erlaubt die regelbasierte, automatische Provisionierung von Checkprofilen basierend auf den Eigenschaften einzelner Knoten. Der monitoringsystem-unabhaenige Kern wird an Integrationsbeispielen basierend auf Icinga2 und Puppet vorgestellt.
Captions: 
	00:00:09,800 --> 00:00:15,650
so I in hamano um let me up using two

00:00:13,910 --> 00:00:20,330
different we almost got his connect land

00:00:15,650 --> 00:00:22,460
earhart near cmdb MacLeish fuse

00:00:20,330 --> 00:00:24,970
monitoring for him and weekly twenty

00:00:22,460 --> 00:00:28,340
pensions since i spawned off which mass

00:00:24,970 --> 00:00:32,000
thank you so hi my name is Jack I'm from

00:00:28,340 --> 00:00:36,579
one and burning cars were and I work as

00:00:32,000 --> 00:00:39,140
a senior linux admin in the monitoring

00:00:36,579 --> 00:00:41,630
in the monitoring and infrastructure

00:00:39,140 --> 00:00:44,840
team on the monitoring side and we run

00:00:41,630 --> 00:00:47,090
services like the 24-7 lock service desk

00:00:44,840 --> 00:00:49,879
the incident management monitoring

00:00:47,090 --> 00:00:53,289
system as a service for other internal

00:00:49,879 --> 00:00:57,519
teams in our company and over the last

00:00:53,289 --> 00:01:00,260
year one and a half years we've re

00:00:57,519 --> 00:01:03,699
deployed and rebuild our entire

00:01:00,260 --> 00:01:06,350
monitoring stack and i'm here to present

00:01:03,699 --> 00:01:10,490
one part of the software that came out

00:01:06,350 --> 00:01:12,260
of this will go slightly over the

00:01:10,490 --> 00:01:14,690
previous system where we came from then

00:01:12,260 --> 00:01:18,980
how we set up the data model how the

00:01:14,690 --> 00:01:20,600
rollout workflow works and then the

00:01:18,980 --> 00:01:24,710
present day system that we are running

00:01:20,600 --> 00:01:28,910
right now and how we plan to expand it

00:01:24,710 --> 00:01:33,200
in the future so this was our previous

00:01:28,910 --> 00:01:37,100
system installation time frame was

00:01:33,200 --> 00:01:40,040
around 2009-2010 it was already

00:01:37,100 --> 00:01:42,950
monitoring as a service admins could

00:01:40,040 --> 00:01:45,500
access an inventory system say they want

00:01:42,950 --> 00:01:49,780
to check which then gets provisioned on

00:01:45,500 --> 00:01:54,110
in the monitoring system and our 24-7

00:01:49,780 --> 00:01:56,360
operator desks existed over the front

00:01:54,110 --> 00:01:58,670
and console and also we have a lot of

00:01:56,360 --> 00:02:02,240
monitoring systems because we have a lot

00:01:58,670 --> 00:02:07,010
of service so they also needed to access

00:02:02,240 --> 00:02:10,429
that so our on-call qds could be

00:02:07,010 --> 00:02:11,840
dispatched from a single console we

00:02:10,429 --> 00:02:15,860
provided remote monitoring local

00:02:11,840 --> 00:02:21,470
monitoring via an agent and the backbone

00:02:15,860 --> 00:02:23,680
itself was rebuilt in 2011 for t

00:02:21,470 --> 00:02:26,109
redundancy on a

00:02:23,680 --> 00:02:28,579
structured lock replication bases

00:02:26,109 --> 00:02:33,560
similar to what you nowadays find in

00:02:28,579 --> 00:02:35,659
Apache Kafka but we weren't entirely

00:02:33,560 --> 00:02:37,159
happy over the years with them there

00:02:35,659 --> 00:02:40,730
were a couple of understandable

00:02:37,159 --> 00:02:43,489
decisions that were made which didn't

00:02:40,730 --> 00:02:46,010
work out with how the industry developed

00:02:43,489 --> 00:02:47,900
so what we didn't really like so that

00:02:46,010 --> 00:02:50,680
the system was proprietary system with a

00:02:47,900 --> 00:02:54,340
proprietary configuration interface so

00:02:50,680 --> 00:02:56,689
the adaptability for us was very limited

00:02:54,340 --> 00:02:59,989
it was integrated with a single

00:02:56,689 --> 00:03:03,470
inventory system our datacenter hardware

00:02:59,989 --> 00:03:06,500
inventory system nowadays it's most DBMS

00:03:03,470 --> 00:03:08,959
or it's still a lot of Harper but it's

00:03:06,500 --> 00:03:14,349
also a lot of VMs which the hardware my

00:03:08,959 --> 00:03:17,599
inventory system doesn't register

00:03:14,349 --> 00:03:18,950
natively let's say it that way so we

00:03:17,599 --> 00:03:21,260
also had limited automation

00:03:18,950 --> 00:03:24,590
opportunities and the system could only

00:03:21,260 --> 00:03:28,810
configure the one proprietary monitoring

00:03:24,590 --> 00:03:31,010
system the system also came with a

00:03:28,810 --> 00:03:37,370
custom agent that was running on the

00:03:31,010 --> 00:03:39,019
systems and basically if you went into

00:03:37,370 --> 00:03:41,479
the inventory system and said I want to

00:03:39,019 --> 00:03:43,849
create a check that does something

00:03:41,479 --> 00:03:45,829
locally on the system then the

00:03:43,849 --> 00:03:48,220
monitoring system actually pushed the

00:03:45,829 --> 00:03:51,560
executable that would do that on to the

00:03:48,220 --> 00:03:54,620
local server and we as a monitoring team

00:03:51,560 --> 00:03:57,129
we didn't like installing bra executable

00:03:54,620 --> 00:04:01,159
software on other people's machines

00:03:57,129 --> 00:04:02,930
especially since it sidestepped every

00:04:01,159 --> 00:04:04,879
standard deployment workflow we had in

00:04:02,930 --> 00:04:06,919
the company there's no patch management

00:04:04,879 --> 00:04:10,159
no deployment or the trail what is

00:04:06,919 --> 00:04:13,729
running on what software it wasn't all

00:04:10,159 --> 00:04:16,609
so it wasn't easily carry able if at all

00:04:13,729 --> 00:04:22,580
so this was also something we didn't

00:04:16,609 --> 00:04:24,669
like we had here this alarm API which is

00:04:22,580 --> 00:04:28,970
basically a chase and post interface

00:04:24,669 --> 00:04:30,440
that creates a alerts inside this

00:04:28,970 --> 00:04:32,680
monitoring system so every other

00:04:30,440 --> 00:04:36,310
monitoring system can pretend to be a

00:04:32,680 --> 00:04:36,310
probe in this one

00:04:36,820 --> 00:04:44,560
this was a custom edition from us also

00:04:39,370 --> 00:04:46,750
around 2011-2012 but as it was a custom

00:04:44,560 --> 00:04:49,330
extension from us the capabilities of

00:04:46,750 --> 00:04:52,090
what we could do with that API was very

00:04:49,330 --> 00:04:54,190
very limited because we had of course to

00:04:52,090 --> 00:04:59,650
adhere to the data format of the vendor

00:04:54,190 --> 00:05:05,370
and lastly the front end application for

00:04:59,650 --> 00:05:12,040
our permanent shift we couldn't really

00:05:05,370 --> 00:05:14,980
really update at all or customize so if

00:05:12,040 --> 00:05:19,540
we take all those identified issues and

00:05:14,980 --> 00:05:22,030
apply them to the system really not that

00:05:19,540 --> 00:05:24,520
many parts left we were entirely happy

00:05:22,030 --> 00:05:27,130
with what worked for us was the alarm

00:05:24,520 --> 00:05:29,680
API and what really worked for us was

00:05:27,130 --> 00:05:33,340
the Geo redundancy synchronization we

00:05:29,680 --> 00:05:37,230
are replicated locks this was something

00:05:33,340 --> 00:05:40,480
we very much liked and wanted to keep so

00:05:37,230 --> 00:05:43,750
we looked at what we need we need a new

00:05:40,480 --> 00:05:46,800
local monitoring component if possible

00:05:43,750 --> 00:05:49,780
not yet another agent on the server and

00:05:46,800 --> 00:05:52,390
also we wanted local configuration

00:05:49,780 --> 00:05:54,880
rollout we are pool and the old system

00:05:52,390 --> 00:05:59,340
was push and if you try to push to

00:05:54,880 --> 00:06:03,760
40,000 servers 12 of them are not online

00:05:59,340 --> 00:06:06,490
five have a full hard disk and to have

00:06:03,760 --> 00:06:12,160
been shut down last year but you didn't

00:06:06,490 --> 00:06:17,350
get the memo so a pool based approach is

00:06:12,160 --> 00:06:21,100
much easier also pool you can you can

00:06:17,350 --> 00:06:24,730
pull every one hour and hit but you can

00:06:21,100 --> 00:06:28,840
redeploy and pull again but to push you

00:06:24,730 --> 00:06:31,690
have will only come one so this was very

00:06:28,840 --> 00:06:34,210
important for us remote monitoring well

00:06:31,690 --> 00:06:37,480
we need high availability we needed

00:06:34,210 --> 00:06:39,550
multiple data center locations we needed

00:06:37,480 --> 00:06:42,430
location-aware monitoring configuration

00:06:39,550 --> 00:06:47,260
rollout so if you have a check for a

00:06:42,430 --> 00:06:50,380
server in somewhere in middle america in

00:06:47,260 --> 00:06:52,840
the midwest you don't want to check that

00:06:50,380 --> 00:06:55,260
from southern Germany you want to run

00:06:52,840 --> 00:06:58,570
the check in America as well and

00:06:55,260 --> 00:07:00,700
something that is often problematic with

00:06:58,570 --> 00:07:03,550
a lot of monitoring systems we need

00:07:00,700 --> 00:07:06,070
support for asymmetric networking so

00:07:03,550 --> 00:07:08,760
most monitoring systems assume if one

00:07:06,070 --> 00:07:12,520
monitoring node can access that server

00:07:08,760 --> 00:07:15,850
then every monitoring node can and we

00:07:12,520 --> 00:07:18,280
have a couple of thousand VLANs a highly

00:07:15,850 --> 00:07:22,330
segmented network over all the data

00:07:18,280 --> 00:07:25,660
centers so just because one server can

00:07:22,330 --> 00:07:29,230
reach that resource and monitoring it

00:07:25,660 --> 00:07:31,210
doesn't mean that the other parts of the

00:07:29,230 --> 00:07:34,240
system actually can reach it or should

00:07:31,210 --> 00:07:37,330
be able to reach it and this is a big

00:07:34,240 --> 00:07:39,760
problem with a lot of monitoring systems

00:07:37,330 --> 00:07:41,920
if they are have a high availability

00:07:39,760 --> 00:07:45,750
options where they can migrate the

00:07:41,920 --> 00:07:50,050
checks to somewhere else they don't know

00:07:45,750 --> 00:07:52,510
what is reachable from where then of

00:07:50,050 --> 00:07:55,090
course of the next role that we needed

00:07:52,510 --> 00:07:57,640
to accommodate what's the we needed a

00:07:55,090 --> 00:08:03,180
central notification system for our

00:07:57,640 --> 00:08:05,020
uncle or on-site teams so we split the

00:08:03,180 --> 00:08:06,670
monitoring and the notification

00:08:05,020 --> 00:08:10,030
aggregation maybe in two separate

00:08:06,670 --> 00:08:13,480
systems we keep the alarm API because we

00:08:10,030 --> 00:08:15,490
liked it and by keeping the alarm API

00:08:13,480 --> 00:08:17,200
with a backward compatible format we

00:08:15,490 --> 00:08:19,120
don't actually need to migrate all the

00:08:17,200 --> 00:08:22,830
other monitoring systems but means that

00:08:19,120 --> 00:08:25,990
we migrate where the same API leads to

00:08:22,830 --> 00:08:29,350
and we of course need improved

00:08:25,990 --> 00:08:31,090
automation capabilities and we wanted to

00:08:29,350 --> 00:08:34,599
keep the monitoring as a service aspect

00:08:31,090 --> 00:08:37,780
where we provide a self-service where

00:08:34,599 --> 00:08:43,060
other applicants traders can say I want

00:08:37,780 --> 00:08:45,610
this and that monitoring from you for my

00:08:43,060 --> 00:08:47,830
servers based on system defined

00:08:45,610 --> 00:08:50,110
capabilities not every monitoring system

00:08:47,830 --> 00:08:54,400
that is it attached can do the same

00:08:50,110 --> 00:08:58,450
things and of course a user permissions

00:08:54,400 --> 00:09:02,680
so what we came up with and what was

00:08:58,450 --> 00:09:04,470
pretty clear at the time was we need a

00:09:02,680 --> 00:09:09,939
configuration service middleware

00:09:04,470 --> 00:09:11,649
we need a scriptable CLI and then we

00:09:09,939 --> 00:09:14,069
want to import from external data

00:09:11,649 --> 00:09:16,990
sources instead of referencing them so

00:09:14,069 --> 00:09:20,499
by importing all the data that we need

00:09:16,990 --> 00:09:22,389
in the disaster recovery case we know

00:09:20,499 --> 00:09:24,910
that we have all the data we need to

00:09:22,389 --> 00:09:28,179
restart the system and if one of the

00:09:24,910 --> 00:09:32,290
external sources changes a new inventory

00:09:28,179 --> 00:09:34,740
system we only need to update the

00:09:32,290 --> 00:09:36,790
importer and of course this also

00:09:34,740 --> 00:09:39,429
accidentally means that we have to

00:09:36,790 --> 00:09:43,350
provide a full API layer so everything

00:09:39,429 --> 00:09:45,329
can be modified by an external tool and

00:09:43,350 --> 00:09:48,670
importers kind of thoughts be

00:09:45,329 --> 00:09:51,369
implemented on top of the a CLI since

00:09:48,670 --> 00:09:53,249
the CLI is scriptable and we want to

00:09:51,369 --> 00:09:58,660
support an arbitrary numbers of

00:09:53,249 --> 00:10:00,579
monitoring systems not just one and for

00:09:58,660 --> 00:10:02,529
the same monitoring system also

00:10:00,579 --> 00:10:06,490
different versions if they have any

00:10:02,529 --> 00:10:09,610
incompatibilities which meant okay we

00:10:06,490 --> 00:10:11,589
will probably need to go with a generic

00:10:09,610 --> 00:10:14,410
output format instead of trying to

00:10:11,589 --> 00:10:17,939
reimplement and keep up with every

00:10:14,410 --> 00:10:22,110
monitoring system that is out there so

00:10:17,939 --> 00:10:22,110
let's build a configuration service

00:10:23,189 --> 00:10:30,449
which brings us to our data model a

00:10:26,129 --> 00:10:33,040
simple object no optional fields because

00:10:30,449 --> 00:10:35,439
you want to create a service here are

00:10:33,040 --> 00:10:37,389
the 750 attributes that you can possibly

00:10:35,439 --> 00:10:41,110
select and we think those are enough

00:10:37,389 --> 00:10:42,699
those lists are always outdated they

00:10:41,110 --> 00:10:46,600
never survive half a year or something

00:10:42,699 --> 00:10:50,079
like that our previous system allowed a

00:10:46,600 --> 00:10:52,029
tree based organization which a lot of

00:10:50,079 --> 00:10:54,730
people liked until they realize there

00:10:52,029 --> 00:10:58,389
was no inheritance so it only looked

00:10:54,730 --> 00:11:00,639
fancy but it wasn't so we wanted to

00:10:58,389 --> 00:11:05,079
provide inheritance but not mandatory

00:11:00,639 --> 00:11:11,379
but optional feature if you want to use

00:11:05,079 --> 00:11:15,040
it the our basic object types are the

00:11:11,379 --> 00:11:16,440
configuration repositories buckets and

00:11:15,040 --> 00:11:20,970
loads

00:11:16,440 --> 00:11:22,950
and these are also the global objects so

00:11:20,970 --> 00:11:25,440
the repository is the root of a

00:11:22,950 --> 00:11:28,830
configuration tree it's also a

00:11:25,440 --> 00:11:34,260
concurrency border for in terms of job

00:11:28,830 --> 00:11:36,690
execution serialization and if a team of

00:11:34,260 --> 00:11:39,140
administrators has a repository it can

00:11:36,690 --> 00:11:42,990
partition it further into packets and

00:11:39,140 --> 00:11:48,060
then we have the notes which is in our

00:11:42,990 --> 00:11:50,880
case simply a generic execution

00:11:48,060 --> 00:11:56,610
environment whether it is Hardware vm

00:11:50,880 --> 00:11:59,970
jail container zone we don't track how

00:11:56,610 --> 00:12:02,760
the execution environment is actually

00:11:59,970 --> 00:12:04,860
running on the physical hardware if

00:12:02,760 --> 00:12:09,750
there's one layer two layer three layers

00:12:04,860 --> 00:12:11,790
in between we don't track that we only

00:12:09,750 --> 00:12:14,100
track on which physical Hardware it is

00:12:11,790 --> 00:12:17,760
so we can get the data center location

00:12:14,100 --> 00:12:20,790
back from it and these are of course a

00:12:17,760 --> 00:12:25,410
global objects because we import them

00:12:20,790 --> 00:12:28,530
from other sources so we can't have to

00:12:25,410 --> 00:12:30,810
multiple times we also have advanced

00:12:28,530 --> 00:12:33,210
object types which is a cluster which

00:12:30,810 --> 00:12:36,180
allows to group loads and these are

00:12:33,210 --> 00:12:39,870
actually unique for bucket and we have

00:12:36,180 --> 00:12:43,980
groups and groups allow to group

00:12:39,870 --> 00:12:47,550
basically everything except itself and

00:12:43,980 --> 00:12:51,150
the main difference for this is that for

00:12:47,550 --> 00:12:54,930
clusters so classes can do nothing that

00:12:51,150 --> 00:12:57,930
a group can also do but a cluster gift

00:12:54,930 --> 00:13:01,500
gives us the assurance that it only only

00:12:57,930 --> 00:13:08,400
contains leaf nodes and no arbitrary

00:13:01,500 --> 00:13:11,190
deep additional trees and here is one

00:13:08,400 --> 00:13:14,070
example how such a tree could look we

00:13:11,190 --> 00:13:17,040
have a repository a packet then three

00:13:14,070 --> 00:13:18,960
notes directly attached to the packet we

00:13:17,040 --> 00:13:21,510
also have a cluster of nodes and we have

00:13:18,960 --> 00:13:25,170
a group that contains two classes of

00:13:21,510 --> 00:13:27,660
nodes all very nice but as we said all

00:13:25,170 --> 00:13:29,880
the objects are very simple and have

00:13:27,660 --> 00:13:31,950
only the minimum

00:13:29,880 --> 00:13:34,740
amount of keys that we really couldn't

00:13:31,950 --> 00:13:39,060
do without so this is actually not that

00:13:34,740 --> 00:13:42,900
expressive and for that we introduce

00:13:39,060 --> 00:13:45,780
properties if you are aware familiar

00:13:42,900 --> 00:13:49,520
with set up as user properties this is

00:13:45,780 --> 00:13:52,110
basically stolen from them as a concept

00:13:49,520 --> 00:13:54,330
they are run time defined we are the CLI

00:13:52,110 --> 00:13:58,710
the moment you define them they're ready

00:13:54,330 --> 00:14:01,140
to use they support inheritance and they

00:13:58,710 --> 00:14:05,700
are assigned to an object in a specific

00:14:01,140 --> 00:14:08,850
view so you can say this node has this

00:14:05,700 --> 00:14:11,640
property but only if it's checked via

00:14:08,850 --> 00:14:16,470
the external interface or the internal

00:14:11,640 --> 00:14:20,880
interface or if you reference it from

00:14:16,470 --> 00:14:25,320
the management interface I drag all that

00:14:20,880 --> 00:14:28,380
stuff and then we have the property

00:14:25,320 --> 00:14:31,770
types we have system properties custom

00:14:28,380 --> 00:14:35,550
properties or I'll explain all of them

00:14:31,770 --> 00:14:38,430
in the next slide so a system property

00:14:35,550 --> 00:14:40,350
is a simple key value pair it's globally

00:14:38,430 --> 00:14:44,900
available to everybody so it's pre

00:14:40,350 --> 00:14:48,780
defined by us in terms of keys but the

00:14:44,900 --> 00:14:50,880
values of course assignable and the

00:14:48,780 --> 00:14:54,390
system property has the advantage that

00:14:50,880 --> 00:14:56,880
we can restrict which objects can have

00:14:54,390 --> 00:14:59,550
that property so we can say this

00:14:56,880 --> 00:15:02,640
property can be only assigned to a

00:14:59,550 --> 00:15:05,130
cluster and a node can have it but only

00:15:02,640 --> 00:15:07,350
if the node has inherited it from a

00:15:05,130 --> 00:15:14,280
cluster but it cannot be assigned to a

00:15:07,350 --> 00:15:18,690
group we have custom properties those

00:15:14,280 --> 00:15:21,720
are the same roughly but they are pair

00:15:18,690 --> 00:15:24,300
repository and everybody can freely

00:15:21,720 --> 00:15:26,820
define them even the keys so they are

00:15:24,300 --> 00:15:30,320
the closest to true telophase user

00:15:26,820 --> 00:15:33,180
properties we have native properties

00:15:30,320 --> 00:15:35,280
this though simply allow for three

00:15:33,180 --> 00:15:38,190
introspection which object type is this

00:15:35,280 --> 00:15:40,730
is it a part of a cluster is a part of a

00:15:38,190 --> 00:15:40,730
group and

00:15:40,960 --> 00:15:47,530
one called properties are allowed to

00:15:44,150 --> 00:15:50,720
assign an unco duty they have been

00:15:47,530 --> 00:15:53,330
modeled as a property so that the

00:15:50,720 --> 00:15:55,970
internal and the external view can have

00:15:53,330 --> 00:16:00,890
different uncle teams that will handle

00:15:55,970 --> 00:16:04,670
the problem and lastly we have service

00:16:00,890 --> 00:16:07,370
properties and service properties are

00:16:04,670 --> 00:16:09,410
dictionaries and not single key value

00:16:07,370 --> 00:16:11,290
pairs but attributes can be specified

00:16:09,410 --> 00:16:14,300
multiple times so it's actually a

00:16:11,290 --> 00:16:20,410
dictionary of key array of value pairs

00:16:14,300 --> 00:16:23,420
um you're available attributes are

00:16:20,410 --> 00:16:25,490
global as well and they can be defined

00:16:23,420 --> 00:16:28,340
per team so if a team has multiple

00:16:25,490 --> 00:16:30,680
repositories they have the same services

00:16:28,340 --> 00:16:33,470
available in all of them but every team

00:16:30,680 --> 00:16:38,390
can have a ftp service that is different

00:16:33,470 --> 00:16:42,410
for every team and those you can attach

00:16:38,390 --> 00:16:48,380
to the tree and the white one is the one

00:16:42,410 --> 00:16:51,800
that is user set and then the tree has

00:16:48,380 --> 00:16:55,570
every object in in it hasit you can add

00:16:51,800 --> 00:16:59,990
a second pink property that is then only

00:16:55,570 --> 00:17:03,530
visible in this part of the tree and you

00:16:59,990 --> 00:17:05,750
can also here is the gray one and this

00:17:03,530 --> 00:17:08,060
one is actually overriding the green one

00:17:05,750 --> 00:17:10,010
so in this part of the tree the green

00:17:08,060 --> 00:17:13,160
property doesn't exist but instead you

00:17:10,010 --> 00:17:14,630
have the gray property which is the same

00:17:13,160 --> 00:17:21,040
key as the green property but a

00:17:14,630 --> 00:17:21,040
different value and this allows

00:17:21,100 --> 00:17:27,920
arbitrary modeling of what you need in

00:17:24,230 --> 00:17:36,890
terms of information at every point of

00:17:27,920 --> 00:17:39,410
the of the tree then we can come to

00:17:36,890 --> 00:17:41,360
capabilities so we want to support

00:17:39,410 --> 00:17:44,180
arbitrary numbers of monitoring systems

00:17:41,360 --> 00:17:47,210
but monitoring system stoned all the

00:17:44,180 --> 00:17:50,900
port the same things there's a huge

00:17:47,210 --> 00:17:53,410
overlap but some edge cases are

00:17:50,900 --> 00:17:55,450
different so we say

00:17:53,410 --> 00:17:58,120
every attached monitoring system has to

00:17:55,450 --> 00:18:00,130
declare its capabilities those r /

00:17:58,120 --> 00:18:03,760
monitoring system purview and / metrics

00:18:00,130 --> 00:18:07,630
or one monitoring system can say i can

00:18:03,760 --> 00:18:09,400
monitor ipmi is something over the

00:18:07,630 --> 00:18:12,400
management network another monitoring

00:18:09,400 --> 00:18:15,220
system the system can say i can monitor

00:18:12,400 --> 00:18:16,870
or ping latency or what internal

00:18:15,220 --> 00:18:20,650
production network and another one can

00:18:16,870 --> 00:18:24,040
say i can monitor or ssl certificate

00:18:20,650 --> 00:18:28,050
validity over the production external

00:18:24,040 --> 00:18:32,830
network which views and which metrics

00:18:28,050 --> 00:18:34,510
exist is again global so that you know

00:18:32,830 --> 00:18:37,330
that different monitoring systems

00:18:34,510 --> 00:18:39,480
actually monitor the same thing there is

00:18:37,330 --> 00:18:43,120
no required overlap between systems and

00:18:39,480 --> 00:18:47,050
every system can also at this step save

00:18:43,120 --> 00:18:50,560
how many thresholds it supports for that

00:18:47,050 --> 00:18:53,790
specific metric or capability so one

00:18:50,560 --> 00:18:57,130
system can say I have eight or nine

00:18:53,790 --> 00:19:02,650
possible thresholds another one says I

00:18:57,130 --> 00:19:05,440
have two can both be modeled and of

00:19:02,650 --> 00:19:06,940
course we need checks because otherwise

00:19:05,440 --> 00:19:10,780
the whole thing would be pretty

00:19:06,940 --> 00:19:14,830
pointless checks are assigned to object

00:19:10,780 --> 00:19:17,350
similar to properties they also are

00:19:14,830 --> 00:19:20,500
propagated throughout the three via

00:19:17,350 --> 00:19:24,040
inheritance they then define how many

00:19:20,500 --> 00:19:26,470
thresholds you use which alarm levels

00:19:24,040 --> 00:19:31,510
you want at which execution in the wall

00:19:26,470 --> 00:19:33,760
and by defining them on a capability you

00:19:31,510 --> 00:19:37,270
actually say what to monitor we are the

00:19:33,760 --> 00:19:39,160
metric from where to monitoring to

00:19:37,270 --> 00:19:43,000
monitor it we had a few and on which

00:19:39,160 --> 00:19:46,120
monitoring system to run it and this

00:19:43,000 --> 00:19:49,780
then looks like this with the check

00:19:46,120 --> 00:19:54,010
being here the red one and then it

00:19:49,780 --> 00:19:56,160
propagates through out the tree but now

00:19:54,010 --> 00:20:00,640
you have a problem if you want to have

00:19:56,160 --> 00:20:02,290
the same check on all the nodes but it

00:20:00,640 --> 00:20:05,860
don't actually want it on the group and

00:20:02,290 --> 00:20:08,860
on the two clusters and for that we

00:20:05,860 --> 00:20:11,770
check constraints that are part of the

00:20:08,860 --> 00:20:14,980
definition and they place conditions on

00:20:11,770 --> 00:20:19,900
the checks to become active for example

00:20:14,980 --> 00:20:21,549
constrained property type native and the

00:20:19,900 --> 00:20:26,440
native property is object type and it

00:20:21,549 --> 00:20:29,980
snowed and then only the yellow one are

00:20:26,440 --> 00:20:31,900
actually a check instance that is

00:20:29,980 --> 00:20:36,100
propagated to the monitoring system and

00:20:31,900 --> 00:20:41,919
these two here are dormant and don't do

00:20:36,100 --> 00:20:44,549
anything and of course multiple

00:20:41,919 --> 00:20:48,000
constraints can be combined here we say

00:20:44,549 --> 00:20:51,820
system property cluster state active and

00:20:48,000 --> 00:20:55,750
now we say here we say this one here is

00:20:51,820 --> 00:20:57,669
the active one that s the pink active

00:20:55,750 --> 00:21:01,780
property that I've relocated from the

00:20:57,669 --> 00:21:04,030
group and now only these checks become

00:21:01,780 --> 00:21:06,250
active and if that property is removed

00:21:04,030 --> 00:21:11,620
they become dormant if it's added here

00:21:06,250 --> 00:21:13,840
then these ones become active which of

00:21:11,620 --> 00:21:17,910
course also means we can add a second

00:21:13,840 --> 00:21:20,679
property a second check that runs on

00:21:17,910 --> 00:21:22,929
cluster state inactive but with

00:21:20,679 --> 00:21:25,240
different thresholds and different alarm

00:21:22,929 --> 00:21:29,500
levels so we also know that the inactive

00:21:25,240 --> 00:21:34,080
backup clusters actually working the

00:21:29,500 --> 00:21:37,570
ones we have here the active

00:21:34,080 --> 00:21:41,770
configuration we need to roll out it we

00:21:37,570 --> 00:21:45,790
need to roll it out for this active

00:21:41,770 --> 00:21:48,130
checks create active check instances if

00:21:45,790 --> 00:21:50,830
there are no service constraints then

00:21:48,130 --> 00:21:54,940
it's just one instance if they are

00:21:50,830 --> 00:21:58,000
service attributes then the system will

00:21:54,940 --> 00:22:01,660
actually build from so a service

00:21:58,000 --> 00:22:05,559
property as we said can have multiple

00:22:01,660 --> 00:22:08,440
values for the same key and the system

00:22:05,559 --> 00:22:10,600
will then normalize it so every team has

00:22:08,440 --> 00:22:14,130
only one value and it will construct all

00:22:10,600 --> 00:22:16,540
possible combinations and issue one

00:22:14,130 --> 00:22:18,890
check for every combinations or if you

00:22:16,540 --> 00:22:22,340
have a service before ports and you

00:22:18,890 --> 00:22:25,690
a monitor disport then it will create

00:22:22,340 --> 00:22:28,550
three checks for each of the pot and

00:22:25,690 --> 00:22:31,850
these check instances are sent to the

00:22:28,550 --> 00:22:33,740
monitoring systems check instances are

00:22:31,850 --> 00:22:38,000
versioned every time something in the

00:22:33,740 --> 00:22:41,240
tree changes it looks for the it creates

00:22:38,000 --> 00:22:42,920
a new version and then compares the

00:22:41,240 --> 00:22:45,740
current version with the previous

00:22:42,920 --> 00:22:48,400
version and if there was no change then

00:22:45,740 --> 00:22:52,450
nothing is sent to the monitoring system

00:22:48,400 --> 00:22:52,450
we don't do update rollouts

00:22:52,640 --> 00:22:58,559
if there is a change and as a new

00:22:55,620 --> 00:23:01,170
version then we actually send the old

00:22:58,559 --> 00:23:03,750
version to the downstream system for D

00:23:01,170 --> 00:23:05,880
provisioning and then the rollout of the

00:23:03,750 --> 00:23:08,610
new version which means the downstream

00:23:05,880 --> 00:23:10,470
system can be completely stateless and

00:23:08,610 --> 00:23:18,480
doesn't have to implement update

00:23:10,470 --> 00:23:20,520
statements and yeah it also means we

00:23:18,480 --> 00:23:24,030
implement no monitoring system specifics

00:23:20,520 --> 00:23:28,110
and every attached system requires a

00:23:24,030 --> 00:23:31,860
slim translation layer which in turn

00:23:28,110 --> 00:23:35,250
means the core can be independent and we

00:23:31,860 --> 00:23:37,950
can interact with multiple versions of

00:23:35,250 --> 00:23:40,320
the same monitoring system by slightly

00:23:37,950 --> 00:23:45,330
adjusting the templating layup it in

00:23:40,320 --> 00:23:49,230
between yeah can be a simple as a

00:23:45,330 --> 00:23:51,000
template system of course a native

00:23:49,230 --> 00:23:54,059
support could be implemented but current

00:23:51,000 --> 00:23:56,309
leaders of course no system apart from

00:23:54,059 --> 00:24:00,929
ours that implements native support

00:23:56,309 --> 00:24:03,360
would monitoring systems can use a pool

00:24:00,929 --> 00:24:06,210
interface to receive rollout they can

00:24:03,360 --> 00:24:08,870
also register a callback address and

00:24:06,210 --> 00:24:12,570
then they get push notifications and

00:24:08,870 --> 00:24:14,940
they can also combine both and on

00:24:12,570 --> 00:24:18,240
startup U spool to synchronize what they

00:24:14,940 --> 00:24:21,330
have and then switch to pull afterwards

00:24:18,240 --> 00:24:24,360
and periodically switch to push

00:24:21,330 --> 00:24:27,799
afterwards and then periodically run a

00:24:24,360 --> 00:24:33,299
pool or not and synchronize that way

00:24:27,799 --> 00:24:38,940
also notes itself can query they're

00:24:33,299 --> 00:24:41,630
monitoring configuration so one of the

00:24:38,940 --> 00:24:46,070
possible ways and the most convenient is

00:24:41,630 --> 00:24:48,809
the nodes ends the configuration service

00:24:46,070 --> 00:24:51,900
which monitoring profiles it currently

00:24:48,809 --> 00:24:55,200
halves has and the reply is what to keep

00:24:51,900 --> 00:24:59,280
what to remove and what is missing man

00:24:55,200 --> 00:25:02,370
should be fetched and this one is

00:24:59,280 --> 00:25:04,940
actually implemented as a puppet effect

00:25:02,370 --> 00:25:08,860
so

00:25:04,940 --> 00:25:11,510
you spoiler ahead and the person this

00:25:08,860 --> 00:25:13,340
that wrote that is still not anymore

00:25:11,510 --> 00:25:22,880
with the company but is he is in the

00:25:13,340 --> 00:25:27,200
audience wave yeah so as a recap this is

00:25:22,880 --> 00:25:30,890
how the old system looked and this one

00:25:27,200 --> 00:25:33,650
is the new one slightly different but

00:25:30,890 --> 00:25:37,730
you can we have the alarm API here which

00:25:33,650 --> 00:25:40,100
is still available here we had the front

00:25:37,730 --> 00:25:43,400
end console which is now a standalone

00:25:40,100 --> 00:25:46,670
application called global notification

00:25:43,400 --> 00:25:48,980
system here we have multiple inventory

00:25:46,670 --> 00:25:52,010
systems that feed into the configuration

00:25:48,980 --> 00:25:55,910
system the configuration system internal

00:25:52,010 --> 00:25:58,370
in turn configures three I singer too

00:25:55,910 --> 00:26:01,370
high availability setups in different

00:25:58,370 --> 00:26:05,120
data centers we also have other

00:26:01,370 --> 00:26:08,000
monitoring systems so around 40 odd of

00:26:05,120 --> 00:26:15,020
them that also feed in the API here and

00:26:08,000 --> 00:26:18,350
also use this service and for local

00:26:15,020 --> 00:26:22,670
monitoring to reconfigure the metric

00:26:18,350 --> 00:26:24,920
monitoring system and puppet queries to

00:26:22,670 --> 00:26:28,300
service and configures the metrics

00:26:24,920 --> 00:26:30,440
client based on that to ensure that

00:26:28,300 --> 00:26:32,810
metrics that are to be monitored

00:26:30,440 --> 00:26:36,830
actually show up in the right metric

00:26:32,810 --> 00:26:42,250
monitoring and up here we have a

00:26:36,830 --> 00:26:45,770
sysadmin with the CLI and d so these

00:26:42,250 --> 00:26:49,730
parts here are actually if you've been

00:26:45,770 --> 00:26:53,480
to the mollusca velasca talk this

00:26:49,730 --> 00:26:55,580
morning period it will eerily similar

00:26:53,480 --> 00:27:00,650
and what they came up with to what we

00:26:55,580 --> 00:27:05,480
did so the local monitoring system

00:27:00,650 --> 00:27:07,430
reuses existing statistics clients we

00:27:05,480 --> 00:27:10,040
have automatic provisioning of local

00:27:07,430 --> 00:27:13,640
checks we are puppet the puppet module

00:27:10,040 --> 00:27:17,000
fetches and then configure statistics

00:27:13,640 --> 00:27:18,220
client and actually sets a correlation

00:27:17,000 --> 00:27:20,159
id Tech

00:27:18,220 --> 00:27:24,010
so the monitoring system knows which

00:27:20,159 --> 00:27:26,830
monitoring profile to apply so this is a

00:27:24,010 --> 00:27:29,169
difference to mollusca we don't do

00:27:26,830 --> 00:27:31,960
pattern matching so it's a difference

00:27:29,169 --> 00:27:36,549
between do we monitor what comes in or

00:27:31,960 --> 00:27:37,720
do we monitor what should come in we

00:27:36,549 --> 00:27:42,179
import inventory data from multiple

00:27:37,720 --> 00:27:46,510
sources we have two scriptable CLI and

00:27:42,179 --> 00:27:48,970
the system already configures three

00:27:46,510 --> 00:27:51,880
different systems in three different

00:27:48,970 --> 00:27:53,500
ways so here we have a template for

00:27:51,880 --> 00:27:56,770
installation player the metric

00:27:53,500 --> 00:27:59,860
monitoring natively speaks the former

00:27:56,770 --> 00:28:03,850
from Zuma and then we also have the

00:27:59,860 --> 00:28:07,750
puppet fact that does the metrics client

00:28:03,850 --> 00:28:11,230
configuration so from our current point

00:28:07,750 --> 00:28:18,190
of view we're pretty good on delivering

00:28:11,230 --> 00:28:20,799
on what we intended to do the future

00:28:18,190 --> 00:28:23,110
outlook so in my opinion classic

00:28:20,799 --> 00:28:25,350
hardware-related errors on ettore will

00:28:23,110 --> 00:28:29,380
become the exception instead of bike

00:28:25,350 --> 00:28:30,820
because they will all there will be more

00:28:29,380 --> 00:28:33,039
in the cloud there will be more

00:28:30,820 --> 00:28:37,570
virtualization on cloud on-premise cloud

00:28:33,039 --> 00:28:39,669
how you want to call it the simple fault

00:28:37,570 --> 00:28:42,100
conditions that were the bread and

00:28:39,669 --> 00:28:46,720
butter of many monitoring systems is

00:28:42,100 --> 00:28:49,860
this process running system d brought

00:28:46,720 --> 00:28:54,070
the demons tools service supervision

00:28:49,860 --> 00:28:55,630
with with a vengeance so you actually

00:28:54,070 --> 00:28:58,150
don't want to see if the process is

00:28:55,630 --> 00:29:01,000
running but you can simply ask is it

00:28:58,150 --> 00:29:05,380
running or the proper question would be

00:29:01,000 --> 00:29:08,110
why aren't you restarting it ping checks

00:29:05,380 --> 00:29:11,110
were always used as is this whole

00:29:08,110 --> 00:29:13,840
available is the source down you can

00:29:11,110 --> 00:29:16,480
just query the hypervisor because we've

00:29:13,840 --> 00:29:18,340
Software Defined Networking a ping check

00:29:16,480 --> 00:29:20,860
isn't the bottom of the stack anymore

00:29:18,340 --> 00:29:25,270
for the pink check to work there has a

00:29:20,860 --> 00:29:30,159
lot of things to work so human

00:29:25,270 --> 00:29:31,670
notification will in my opinion in the

00:29:30,159 --> 00:29:34,460
future only happen for more

00:29:31,670 --> 00:29:39,260
complex fault conditions which of course

00:29:34,460 --> 00:29:43,670
means you need more input data to detect

00:29:39,260 --> 00:29:45,500
the fault vision a single latency

00:29:43,670 --> 00:29:49,960
measurement won't give you enough data

00:29:45,500 --> 00:29:54,620
to say this complex fault condition has

00:29:49,960 --> 00:29:57,650
occurred so the decision the decision

00:29:54,620 --> 00:30:01,150
stage will need access to all data

00:29:57,650 --> 00:30:04,090
sources so you need to decouple

00:30:01,150 --> 00:30:06,740
observation from decision making and

00:30:04,090 --> 00:30:09,320
configuring a check might happen on

00:30:06,740 --> 00:30:11,540
multiple system so you want one check

00:30:09,320 --> 00:30:15,140
but you have to configure 12 data

00:30:11,540 --> 00:30:19,070
sources and this is then the part here

00:30:15,140 --> 00:30:21,260
where we already for one check right now

00:30:19,070 --> 00:30:22,760
interact with two data sources but

00:30:21,260 --> 00:30:25,960
having a single system that is capable

00:30:22,760 --> 00:30:29,830
of configuring multiple data sources and

00:30:25,960 --> 00:30:34,400
then relocating in the future maybe

00:30:29,830 --> 00:30:37,610
these here to basically remote metrics

00:30:34,400 --> 00:30:39,950
providers that feed into the metric API

00:30:37,610 --> 00:30:44,030
and have all the threshold and

00:30:39,950 --> 00:30:46,850
conditional evaluation happening here is

00:30:44,030 --> 00:30:56,840
to plan and at that point we are really

00:30:46,850 --> 00:30:59,720
looking like Monica yeah the original

00:30:56,840 --> 00:31:02,060
plan was to publish the tool as part of

00:30:59,720 --> 00:31:03,830
this presentation but I had all the

00:31:02,060 --> 00:31:08,030
required a case a couple of months ago

00:31:03,830 --> 00:31:10,310
so it is already available on github it

00:31:08,030 --> 00:31:12,770
is not a Cottam development happens

00:31:10,310 --> 00:31:16,730
there and I've closed all internal

00:31:12,770 --> 00:31:20,300
repositories it's licensed as too close

00:31:16,730 --> 00:31:22,910
bsd included third party libraries in

00:31:20,300 --> 00:31:28,130
flashbender are licensed according to

00:31:22,910 --> 00:31:30,370
their license clauses of course but they

00:31:28,130 --> 00:31:34,100
are all permissive licensed as well and

00:31:30,370 --> 00:31:38,750
it's currently 100% go and won't make

00:31:34,100 --> 00:31:42,080
fine um but the project pass factor is a

00:31:38,750 --> 00:31:44,930
bit limited because on the core system

00:31:42,080 --> 00:31:47,780
is actually only me

00:31:44,930 --> 00:31:51,320
and now this light that I added this

00:31:47,780 --> 00:31:54,500
morning a small disclaimer the

00:31:51,320 --> 00:32:02,030
conference was too early I wanted to

00:31:54,500 --> 00:32:04,790
present the 09 so the current version is

00:32:02,030 --> 00:32:08,090
already running in production in our

00:32:04,790 --> 00:32:10,460
environment but we wrote this software

00:32:08,090 --> 00:32:12,140
and running software you wrote to

00:32:10,460 --> 00:32:14,900
running software somebody else wrote are

00:32:12,140 --> 00:32:18,320
two completely different things as every

00:32:14,900 --> 00:32:22,120
operator knows man every admin so the

00:32:18,320 --> 00:32:24,650
version originally intended here was 09

00:32:22,120 --> 00:32:29,000
eating includes a lot of usability

00:32:24,650 --> 00:32:35,350
enhancements especially for manual use

00:32:29,000 --> 00:32:35,350
of the CLI and running this software

00:32:37,000 --> 00:32:43,420
outside of a system where it's fully

00:32:39,380 --> 00:32:46,640
scripted and automated so for example

00:32:43,420 --> 00:32:49,040
depending on your personality a CLI that

00:32:46,640 --> 00:32:51,740
only speaks Jason might be exactly your

00:32:49,040 --> 00:32:58,490
thing and turns out for a lot of people

00:32:51,740 --> 00:33:04,090
it's not suma awesome has internally

00:32:58,490 --> 00:33:06,980
around 170 150 actions that attracts and

00:33:04,090 --> 00:33:10,280
in the new permission system that I'm

00:33:06,980 --> 00:33:12,440
currently writing on each action can be

00:33:10,280 --> 00:33:14,960
individually assigned to permissions

00:33:12,440 --> 00:33:18,230
that can be granted so it's a lot more

00:33:14,960 --> 00:33:22,580
flexible in terms of who is allowed to

00:33:18,230 --> 00:33:25,160
do what in the 08 version it's basically

00:33:22,580 --> 00:33:26,750
the UNIX model where you are not allowed

00:33:25,160 --> 00:33:32,830
to do anything or allowed to do

00:33:26,750 --> 00:33:35,210
everything so well also i'm currently

00:33:32,830 --> 00:33:37,220
rewriting all the documentation so they

00:33:35,210 --> 00:33:41,390
don't contain company internal

00:33:37,220 --> 00:33:44,870
references and i'm doing that with 09 in

00:33:41,390 --> 00:33:48,500
in mind which is of course not released

00:33:44,870 --> 00:33:54,260
yet and nothing that works for 09 works

00:33:48,500 --> 00:33:56,120
for 85 so this is a bit of a letdown

00:33:54,260 --> 00:33:58,150
disclaimer but

00:33:56,120 --> 00:34:00,620
the next version will be around

00:33:58,150 --> 00:34:02,690
Christmas and I'm not meaning the pro 6

00:34:00,620 --> 00:34:06,950
20 year version of Christmas but I'm

00:34:02,690 --> 00:34:09,380
actually meaning this Christmas so if

00:34:06,950 --> 00:34:12,980
this all sound sounded interesting to

00:34:09,380 --> 00:34:15,320
you a check at the beginning of next

00:34:12,980 --> 00:34:20,210
year end of end of this year around

00:34:15,320 --> 00:34:21,860
Christmas 09 should be done and then a

00:34:20,210 --> 00:34:28,810
bit of release testing regression

00:34:21,860 --> 00:34:32,270
testing and then it will be checked yeah

00:34:28,810 --> 00:34:34,040
I'm actually a lot faster I usually

00:34:32,270 --> 00:34:36,830
don't have slides and only a player a

00:34:34,040 --> 00:34:43,670
whiteboard so I can go into weird

00:34:36,830 --> 00:34:47,680
technical details and tangents you can

00:34:43,670 --> 00:34:50,270
of course we don't have a contribution

00:34:47,680 --> 00:34:52,970
license we don't have any of those fancy

00:34:50,270 --> 00:34:55,360
CLA things right now so if you want to

00:34:52,970 --> 00:35:01,220
contribute just open a pull request or

00:34:55,360 --> 00:35:04,640
send me a mail and I guess the

00:35:01,220 --> 00:35:10,300
conference stock stops 20 minutes early

00:35:04,640 --> 00:35:10,300
because I've had too few slides I

00:35:12,000 --> 00:35:18,970
45 minutes 50 slides its sound like a

00:35:15,130 --> 00:35:21,270
good average that's okay have any

00:35:18,970 --> 00:35:21,270
questions

00:35:24,850 --> 00:35:30,090

YouTube URL: https://www.youtube.com/watch?v=6rx9h9IAOUM


