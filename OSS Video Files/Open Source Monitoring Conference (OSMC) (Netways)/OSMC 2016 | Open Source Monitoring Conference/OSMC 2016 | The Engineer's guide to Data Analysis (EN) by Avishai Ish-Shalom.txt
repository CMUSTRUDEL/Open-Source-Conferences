Title: OSMC 2016 | The Engineer's guide to Data Analysis (EN) by Avishai Ish-Shalom
Publication date: 2016-12-12
Playlist: OSMC 2016 | Open Source Monitoring Conference
Description: 
	"With great power comes great confusion"
It is not enough to generate, collect and store metrics; One needs to know how to design, handle and analyse them to get the full benefit of metrics. Due to the statistical nature of metrics, there are various gotchas associated them - and the rabbit hole goes deeper when you discover how collectors, aggregators and graphing systems handle metrics.
In this talk we will explore the basics of metrics and how to use them correctly to detect problems, correlate and investigate issues. This is intended to be an interactive talk.
Captions: 
	00:00:09,880 --> 00:00:17,320
so here we go again our next speaker I

00:00:13,510 --> 00:00:18,910
wish I Shalom he's been practicing as in

00:00:17,320 --> 00:00:21,880
operations and software engineer for

00:00:18,910 --> 00:00:23,950
quite a long time now currently he is

00:00:21,880 --> 00:00:27,790
leading a team of software engineers at

00:00:23,950 --> 00:00:29,980
wix.com and his talk will be about an

00:00:27,790 --> 00:00:34,810
engineer's guide to data analysis so

00:00:29,980 --> 00:00:36,670
enjoy hi first of all you've got five

00:00:34,810 --> 00:00:39,130
minutes to tell all chill of the jokes

00:00:36,670 --> 00:00:41,710
you want about wigs I know that the name

00:00:39,130 --> 00:00:43,270
is problematic nobody's going to file

00:00:41,710 --> 00:00:48,850
the grievance with a gel if you don't

00:00:43,270 --> 00:00:50,290
like your jokes maybe I will so I'm here

00:00:48,850 --> 00:00:52,150
to talk to you about the metrics and

00:00:50,290 --> 00:00:54,610
graphs today but on a more practical

00:00:52,150 --> 00:00:56,920
perspective for example we've heard a

00:00:54,610 --> 00:00:59,880
lot of talks about tools and about you

00:00:56,920 --> 00:01:02,680
know mathematics and various use cases

00:00:59,880 --> 00:01:04,479
but not a lot of people talk about what

00:01:02,680 --> 00:01:07,060
should you be measuring what should you

00:01:04,479 --> 00:01:09,130
be looking at and this talk is more

00:01:07,060 --> 00:01:11,170
about that stuff when what kind of

00:01:09,130 --> 00:01:14,619
metrics do we want how do we design

00:01:11,170 --> 00:01:18,100
those metrics how do we see problems in

00:01:14,619 --> 00:01:20,049
our systems so first of all I'm going to

00:01:18,100 --> 00:01:21,789
tell you a little bit about twix we've

00:01:20,049 --> 00:01:24,369
got about four hundred engineers we've

00:01:21,789 --> 00:01:26,259
got about 1,400 employees lots and lots

00:01:24,369 --> 00:01:29,590
of websites about 100 million of them

00:01:26,259 --> 00:01:31,590
and about 200 million 50 microservices

00:01:29,590 --> 00:01:34,240
we've got officers in various locations

00:01:31,590 --> 00:01:36,880
those are just engineering an officer

00:01:34,240 --> 00:01:39,459
and actually it's not updated basically

00:01:36,880 --> 00:01:43,030
what I'm trying to say is that wicks is

00:01:39,459 --> 00:01:48,039
big and complicated and we do a lot of

00:01:43,030 --> 00:01:50,139
interesting stuff so yeah we're going to

00:01:48,039 --> 00:01:51,880
be using something called is and it's

00:01:50,139 --> 00:01:54,969
not infrastructure of the service it's a

00:01:51,880 --> 00:01:57,609
kind of a demo application you know the

00:01:54,969 --> 00:01:58,990
pseudo insult mode where you know Sulu

00:01:57,609 --> 00:02:01,509
insults you if you input the wrong

00:01:58,990 --> 00:02:03,819
password so it's kind of an extension

00:02:01,509 --> 00:02:06,639
for that basically give you a random

00:02:03,819 --> 00:02:08,920
insults of an API I actually looked for

00:02:06,639 --> 00:02:13,420
insult databases apparently the couple

00:02:08,920 --> 00:02:15,880
of them online I don't know why but will

00:02:13,420 --> 00:02:19,270
using will be using that as a kind of a

00:02:15,880 --> 00:02:22,120
you know a demo application or something

00:02:19,270 --> 00:02:23,530
to simulate on so the basic architecture

00:02:22,120 --> 00:02:26,860
of the application is this

00:02:23,530 --> 00:02:29,980
we have an Apache load balancer we have

00:02:26,860 --> 00:02:32,380
flask it's basically a python-based web

00:02:29,980 --> 00:02:34,810
server application server and then we

00:02:32,380 --> 00:02:38,080
have CouchDB which is an HTTP database

00:02:34,810 --> 00:02:40,510
and on top of that we have apache again

00:02:38,080 --> 00:02:42,760
and why is that because we'll be using

00:02:40,510 --> 00:02:46,390
apache both of the load balance and also

00:02:42,760 --> 00:02:49,360
as a kind of a probe to get information

00:02:46,390 --> 00:02:51,400
about you know latency and response size

00:02:49,360 --> 00:02:54,220
and that kind of stuff so basically I'm

00:02:51,400 --> 00:02:56,050
using it the telemetry collector so to

00:02:54,220 --> 00:02:57,970
speak now all the data that I'm

00:02:56,050 --> 00:02:59,500
collecting both from the application

00:02:57,970 --> 00:03:02,260
itself because application itself is

00:02:59,500 --> 00:03:04,900
instrumented and from CouchDB and from

00:03:02,260 --> 00:03:07,810
Apache and all that stuff is eventually

00:03:04,900 --> 00:03:10,680
funneled into graphite and the way I'm

00:03:07,810 --> 00:03:14,319
doing that is the application itself

00:03:10,680 --> 00:03:16,330
sends metrics to stats d and sets the

00:03:14,319 --> 00:03:19,120
sense metric to graphite also have

00:03:16,330 --> 00:03:21,250
collecti collect is a pretty common

00:03:19,120 --> 00:03:23,470
collector but a lot of people used it

00:03:21,250 --> 00:03:25,540
collects like CPU metrics memilih

00:03:23,470 --> 00:03:29,410
metrics that kind of stuff also collects

00:03:25,540 --> 00:03:31,390
CouchDB metrics and I've got logstash

00:03:29,410 --> 00:03:33,190
collecting the access logs from Apache

00:03:31,390 --> 00:03:35,709
and basically converting them into

00:03:33,190 --> 00:03:40,079
metrics which are stowed both in stats d

00:03:35,709 --> 00:03:43,390
and graphite and an elastic search so

00:03:40,079 --> 00:03:44,980
basically i'm using common tools that

00:03:43,390 --> 00:03:49,480
practically everyone knew you in the

00:03:44,980 --> 00:03:53,859
room is using it's just does anybody not

00:03:49,480 --> 00:03:55,510
know what stats there is cool okay so

00:03:53,859 --> 00:03:58,630
it's very common tools that everybody's

00:03:55,510 --> 00:04:00,070
using and that is that I don't want to

00:03:58,630 --> 00:04:04,780
surprise anyone by doing something that

00:04:00,070 --> 00:04:08,019
you can't do okay so few words on

00:04:04,780 --> 00:04:09,790
graphite because um a lot of people use

00:04:08,019 --> 00:04:13,290
graphite and I'm going to be using that

00:04:09,790 --> 00:04:15,940
isn't a prime example on how you know

00:04:13,290 --> 00:04:18,940
metric collectors and metric systems

00:04:15,940 --> 00:04:22,240
work so graphite is basically built off

00:04:18,940 --> 00:04:25,510
a bunch of parts you send metrics to

00:04:22,240 --> 00:04:27,400
carbon sorry carbon relay or carbon

00:04:25,510 --> 00:04:29,560
aggregator all current cash yes you can

00:04:27,400 --> 00:04:32,320
send to any one of them and they behave

00:04:29,560 --> 00:04:35,099
into them a bit differently the relay

00:04:32,320 --> 00:04:36,849
kind of does load balancing and shouting

00:04:35,099 --> 00:04:38,770
application

00:04:36,849 --> 00:04:40,869
karbonn aggregator aggregates netflix of

00:04:38,770 --> 00:04:43,209
course Calvin cash received the metrics

00:04:40,869 --> 00:04:45,369
souls in memory and eventually writes

00:04:43,209 --> 00:04:48,929
them to whisper whispers a kind of a

00:04:45,369 --> 00:04:51,819
round robin database for metrics now

00:04:48,929 --> 00:04:54,639
viewing the metrics is done by graphite

00:04:51,819 --> 00:04:57,939
web so graphite web presents both an API

00:04:54,639 --> 00:05:01,409
and a kind of a really lame you know

00:04:57,939 --> 00:05:04,209
user interface if you can call that and

00:05:01,409 --> 00:05:05,889
you would be using graphite web you know

00:05:04,209 --> 00:05:08,199
in order to view the metrics now

00:05:05,889 --> 00:05:11,199
graphite provides more than just few

00:05:08,199 --> 00:05:12,879
metrics it provides the Metro collect

00:05:11,199 --> 00:05:14,860
all and storage and the user interface

00:05:12,879 --> 00:05:16,929
but also mathematical functions you can

00:05:14,860 --> 00:05:19,719
use to analyze your data stuff like

00:05:16,929 --> 00:05:21,939
derivatives or time shifts or even Holt

00:05:19,719 --> 00:05:23,919
winters if if you know what it is and

00:05:21,939 --> 00:05:26,879
it's practically the standard in the

00:05:23,919 --> 00:05:30,369
monitoring world today I don't think

00:05:26,879 --> 00:05:34,740
anybody's here is not using graphite or

00:05:30,369 --> 00:05:37,689
haven't used graphite in the past so

00:05:34,740 --> 00:05:39,629
let's look at the problem that we have

00:05:37,689 --> 00:05:48,269
in our service so remember we have the

00:05:39,629 --> 00:05:48,269
fancy in ir service so one second

00:05:55,230 --> 00:06:06,240
let me show you that the console the

00:05:57,420 --> 00:06:08,640
service so this basically the service

00:06:06,240 --> 00:06:10,950
hours got an API it's got an API for

00:06:08,640 --> 00:06:12,450
random insults four categories and so on

00:06:10,950 --> 00:06:16,200
I'm just going to show you how it works

00:06:12,450 --> 00:06:18,600
I issue an API call and I get you know

00:06:16,200 --> 00:06:25,020
Jason with the pain salt I wanted that's

00:06:18,600 --> 00:06:28,860
basically it now I used a load generator

00:06:25,020 --> 00:06:31,950
and sorry and this is what I got so

00:06:28,860 --> 00:06:34,170
normally the latency of the application

00:06:31,950 --> 00:06:36,560
which is on the green line sorry one

00:06:34,170 --> 00:06:36,560
second

00:06:45,130 --> 00:06:52,610
so so that that so lonely the latency of

00:06:50,539 --> 00:06:56,210
the application is below you know around

00:06:52,610 --> 00:06:58,039
500 milliseconds and this red line shows

00:06:56,210 --> 00:07:00,889
the SLA that we're expecting for the

00:06:58,039 --> 00:07:04,370
application now around this time we had

00:07:00,889 --> 00:07:06,349
a problem obviously the latency of the

00:07:04,370 --> 00:07:09,979
application is risen significantly and

00:07:06,349 --> 00:07:13,009
now it's you know it's not no longer

00:07:09,979 --> 00:07:15,440
within the SLA so does anybody have a

00:07:13,009 --> 00:07:19,130
guess what the problem is but might be

00:07:15,440 --> 00:07:22,520
didn't what the problem might be anyone

00:07:19,130 --> 00:07:24,169
want to take a guess no this is kind of

00:07:22,520 --> 00:07:26,720
an interactive talk so if you're not

00:07:24,169 --> 00:07:31,580
going to be talking you can't proceed so

00:07:26,720 --> 00:07:36,830
you have to talk so anyone wants to take

00:07:31,580 --> 00:07:41,810
a guess a wild guess maybe okay so let's

00:07:36,830 --> 00:07:44,180
look at the throughput nope actually the

00:07:41,810 --> 00:07:49,240
throughput went down so it's probably

00:07:44,180 --> 00:07:52,610
something internal other guesses DNS no

00:07:49,240 --> 00:07:56,509
but yeah it's always the DNS isn't it so

00:07:52,610 --> 00:08:01,479
where's Lucas hmm server doing backup

00:07:56,509 --> 00:08:03,590
nope this is how much something sorry I

00:08:01,479 --> 00:08:07,250
know I can show you the memory graphs

00:08:03,590 --> 00:08:09,169
it's not bad now okay how would you go

00:08:07,250 --> 00:08:12,680
about finding the problem what graphs

00:08:09,169 --> 00:08:16,940
would you look at okay I can show the

00:08:12,680 --> 00:08:21,590
memory graph if you want let's do that

00:08:16,940 --> 00:08:24,620
then so this is grow fauna which is

00:08:21,590 --> 00:08:27,190
basically a better UI for graphite so

00:08:24,620 --> 00:08:32,890
graphite and

00:08:27,190 --> 00:08:42,080
said query select metric collect the

00:08:32,890 --> 00:08:48,160
memory which one do you want free used

00:08:42,080 --> 00:08:51,740
which one free ok let movie free and

00:08:48,160 --> 00:08:54,170
let's see now we've got about 3

00:08:51,740 --> 00:09:02,180
gigabytes of free memory no I don't

00:08:54,170 --> 00:09:03,790
think that's it anything else guys

00:09:02,180 --> 00:09:13,310
you're debugging here we're losing money

00:09:03,790 --> 00:09:21,170
do something how would you detect that

00:09:13,310 --> 00:09:22,670
actually haha it's nice but I it is your

00:09:21,170 --> 00:09:30,589
problem if you get fired for not fixing

00:09:22,670 --> 00:09:32,149
it so so suggestions ok does anybody

00:09:30,589 --> 00:09:38,320
want to take a look at lo the average

00:09:32,149 --> 00:09:43,550
CPU that kind of stuff because we can so

00:09:38,320 --> 00:09:47,089
I think it's this one nope the swap this

00:09:43,550 --> 00:09:50,890
is load average looks fine no

00:09:47,089 --> 00:09:56,360
deployments nope cpu also looks fine I

00:09:50,890 --> 00:10:02,570
oh wait we can look at that weight icpo

00:09:56,360 --> 00:10:10,610
iowait or disk disk ok let's look at the

00:10:02,570 --> 00:10:19,000
disk mm-hmm I think I'm not collecting

00:10:10,610 --> 00:10:21,260
that metric actually yeah it's the cloud

00:10:19,000 --> 00:10:23,209
there is no disconnect now i'm actually

00:10:21,260 --> 00:10:24,680
not collecting that metric but I because

00:10:23,209 --> 00:10:31,839
i simulated there I can tell you it's

00:10:24,680 --> 00:10:34,910
not the disk sorry no vendome what I

00:10:31,839 --> 00:10:36,589
know there is land embedded data from I

00:10:34,910 --> 00:10:41,589
have metrics I have the anthropometric

00:10:36,589 --> 00:10:41,589
actually so we can look at that

00:10:42,519 --> 00:10:51,079
nope we've got a grip random yes okay

00:10:49,639 --> 00:10:56,149
does anybody want to take a look at the

00:10:51,079 --> 00:10:57,649
database maybe like the fact that the

00:10:56,149 --> 00:11:03,529
instrument that the database should have

00:10:57,649 --> 00:11:06,620
been a big hint um yeah so the lines

00:11:03,529 --> 00:11:08,569
here are the average latency for for the

00:11:06,620 --> 00:11:10,490
application internally not from the

00:11:08,569 --> 00:11:12,740
world answer point but from the the

00:11:10,490 --> 00:11:14,389
Python point of view and you can see

00:11:12,740 --> 00:11:16,730
it's risen slightly but not enough to

00:11:14,389 --> 00:11:18,769
explain the problem and you can also see

00:11:16,730 --> 00:11:21,829
that this is the couch to be average

00:11:18,769 --> 00:11:24,800
latency against wiesen slightly from

00:11:21,829 --> 00:11:27,879
about let's say fifty seven milliseconds

00:11:24,800 --> 00:11:30,019
to about eighty seven milliseconds

00:11:27,879 --> 00:11:32,149
definitely not enough to explain the

00:11:30,019 --> 00:11:39,499
rise from 400 milliseconds and latency

00:11:32,149 --> 00:11:44,629
to about 900 so anybody wants take

00:11:39,499 --> 00:11:48,279
another guess what would you do at your

00:11:44,629 --> 00:11:53,290
job I mean what would you look at

00:11:48,279 --> 00:11:53,290
network traffic okay we can look at that

00:12:00,680 --> 00:12:12,450
so interface yep let's look at the

00:12:06,150 --> 00:12:17,490
package look at both nope pretty much

00:12:12,450 --> 00:12:21,090
the same sorry I've got the log files

00:12:17,490 --> 00:12:22,650
yes but actually I want to look just at

00:12:21,090 --> 00:12:26,460
metrics but we can lock them and look at

00:12:22,650 --> 00:12:32,250
the log files but can we detect the

00:12:26,460 --> 00:12:33,990
problem only by using metrics so how

00:12:32,250 --> 00:12:35,970
about if we look at the metrics the same

00:12:33,990 --> 00:12:40,980
metrics we have now just in a different

00:12:35,970 --> 00:12:44,850
way let's say that instead of looking at

00:12:40,980 --> 00:12:48,210
you know average Layton sees we'll look

00:12:44,850 --> 00:12:51,240
at let's say percentiles and see what we

00:12:48,210 --> 00:12:55,020
can see so let's look at the application

00:12:51,240 --> 00:12:58,080
percentile 99% of the application of the

00:12:55,020 --> 00:13:00,980
Python okay now that looks completely

00:12:58,080 --> 00:13:04,740
different doesn't it actually it's risen

00:13:00,980 --> 00:13:06,330
solid rise here not here even so

00:13:04,740 --> 00:13:09,120
obviously the problems started a long

00:13:06,330 --> 00:13:14,430
time ago let's look at the database

00:13:09,120 --> 00:13:19,830
latency now this here is the application

00:13:14,430 --> 00:13:22,730
latency and this is the for the database

00:13:19,830 --> 00:13:26,340
99% of latency so let's turn off all the

00:13:22,730 --> 00:13:32,760
graphs that don't let us see what's

00:13:26,340 --> 00:13:36,230
going on and okay so here you see this

00:13:32,760 --> 00:13:39,320
is a graph of the average latency of

00:13:36,230 --> 00:13:44,640
from from the application point of view

00:13:39,320 --> 00:13:46,320
and this is a 99 percentile of the

00:13:44,640 --> 00:13:53,700
database and you can see correlates

00:13:46,320 --> 00:13:55,380
pretty pretty well actually so I'm going

00:13:53,700 --> 00:14:01,410
to explain what happened here just one

00:13:55,380 --> 00:14:03,210
second to pee so this out sorry yeah

00:14:01,410 --> 00:14:05,160
this is a simulation of walking problems

00:14:03,210 --> 00:14:08,760
in database actually the the actual

00:14:05,160 --> 00:14:10,490
problem was was packet loss between the

00:14:08,760 --> 00:14:13,399
database and the

00:14:10,490 --> 00:14:15,260
and application selbo that's what I

00:14:13,399 --> 00:14:17,720
simulated but locks would behave in a

00:14:15,260 --> 00:14:19,850
very similar manner but what you can see

00:14:17,720 --> 00:14:22,220
here that the 99 percentile actually

00:14:19,850 --> 00:14:23,839
shows us the problem very well and this

00:14:22,220 --> 00:14:25,730
has been my experience with almost all

00:14:23,839 --> 00:14:28,160
database problems and a lot of different

00:14:25,730 --> 00:14:30,410
problems you can't see them in averages

00:14:28,160 --> 00:14:31,820
you just can't but you do if you look at

00:14:30,410 --> 00:14:34,370
ninety-nine percent does and sometimes

00:14:31,820 --> 00:14:36,649
even 95 percentile the problem just you

00:14:34,370 --> 00:14:39,230
know screams at you it's very very easy

00:14:36,649 --> 00:14:40,970
to observe now why is that what we're

00:14:39,230 --> 00:14:43,730
seeing basically is the effect of

00:14:40,970 --> 00:14:46,490
queuing in the system the Python server

00:14:43,730 --> 00:14:49,040
that I've used only have one thread so

00:14:46,490 --> 00:14:51,290
even a small number small fraction of

00:14:49,040 --> 00:14:53,450
slow transactions would basically hoggle

00:14:51,290 --> 00:14:55,310
the resources of that web server and we

00:14:53,450 --> 00:14:57,920
will observe queuing latency that's very

00:14:55,310 --> 00:14:59,690
very high okay now almost all our

00:14:57,920 --> 00:15:01,940
servers have some kind of limited

00:14:59,690 --> 00:15:04,580
resource maybe it's a connection pool to

00:15:01,940 --> 00:15:07,339
database maybe it's a thread pool maybe

00:15:04,580 --> 00:15:08,839
it's just kill the number of files that

00:15:07,339 --> 00:15:11,540
you can open on the number of sockets

00:15:08,839 --> 00:15:13,910
but even small number of slow

00:15:11,540 --> 00:15:16,970
transactions would hug that resource and

00:15:13,910 --> 00:15:20,180
you will observe high latency from you

00:15:16,970 --> 00:15:21,770
even average high latency so if you're

00:15:20,180 --> 00:15:24,230
looking for problems if you're debugging

00:15:21,770 --> 00:15:25,579
stuff there's no point looking at the

00:15:24,230 --> 00:15:29,660
averages you should be looking at

00:15:25,579 --> 00:15:31,640
percentiles and maximums so once we know

00:15:29,660 --> 00:15:34,910
that let's actually have a look at the

00:15:31,640 --> 00:15:38,750
latencies so first of all I'll turn off

00:15:34,910 --> 00:15:41,920
the database latency and let's see what

00:15:38,750 --> 00:15:41,920
the users actually felt

00:15:46,240 --> 00:15:52,750
so this is the application latency the

00:15:48,160 --> 00:16:03,190
average latency now let's look at the

00:15:52,750 --> 00:16:05,320
percentiles so this is the 99th

00:16:03,190 --> 00:16:09,640
percentile of the application you can

00:16:05,320 --> 00:16:12,250
see it's actually a lot higher so about

00:16:09,640 --> 00:16:13,990
1% of our users actually felt Layton

00:16:12,250 --> 00:16:16,390
sees that world's higher than two

00:16:13,990 --> 00:16:20,950
seconds higher than two seconds how much

00:16:16,390 --> 00:16:22,300
higher we don't know we don't know

00:16:20,950 --> 00:16:24,279
unless we have the maximum of the entire

00:16:22,300 --> 00:16:26,709
distribution so the problem of latency

00:16:24,279 --> 00:16:29,080
is they actually hide the worst problems

00:16:26,709 --> 00:16:31,630
in your system you have to remember that

00:16:29,080 --> 00:16:33,250
if you're showing 95% hours it means

00:16:31,630 --> 00:16:35,140
that you're throwing away the five

00:16:33,250 --> 00:16:37,360
percent of the problem of the worst

00:16:35,140 --> 00:16:41,080
problems that you have do you want to do

00:16:37,360 --> 00:16:42,760
that now with not as engineers will

00:16:41,080 --> 00:16:48,790
usually interested in the problems

00:16:42,760 --> 00:16:55,690
nothing the averages so I think it's

00:16:48,790 --> 00:16:57,160
time to go back to the slides okay so

00:16:55,690 --> 00:17:00,190
now that we know what we should be

00:16:57,160 --> 00:17:02,230
looking at maybe it's time to talk a

00:17:00,190 --> 00:17:05,290
little bit about you know matrix design

00:17:02,230 --> 00:17:08,199
how do we get that data that we need so

00:17:05,290 --> 00:17:12,429
first of all what is the metric a metric

00:17:08,199 --> 00:17:13,809
is numeric data ok often comes with a

00:17:12,429 --> 00:17:16,089
time stamp it doesn't come with this

00:17:13,809 --> 00:17:18,280
time stamp Stastny is a good example we

00:17:16,089 --> 00:17:19,720
usually slap one on to it and it's a

00:17:18,280 --> 00:17:21,339
measurement of something it's usually

00:17:19,720 --> 00:17:23,650
discreet it's not continuous because

00:17:21,339 --> 00:17:26,699
computers know the work in the discrete

00:17:23,650 --> 00:17:28,960
world and nothing is continuous and

00:17:26,699 --> 00:17:30,970
where do we get the metrics firm so

00:17:28,960 --> 00:17:33,580
usually we get them from three different

00:17:30,970 --> 00:17:36,610
sources either from events like logs and

00:17:33,580 --> 00:17:38,860
we aggregate those events or count those

00:17:36,610 --> 00:17:40,540
events in all the convent to convert

00:17:38,860 --> 00:17:42,250
them into metrics sometimes those events

00:17:40,540 --> 00:17:45,340
carry numeric data by it by themselves

00:17:42,250 --> 00:17:47,890
so we can just plot that data also we

00:17:45,340 --> 00:17:50,140
sample stuff for example with a cpu oh

00:17:47,890 --> 00:17:51,880
memory we sample it every like 10

00:17:50,140 --> 00:17:54,940
seconds or one minute and we did get

00:17:51,880 --> 00:17:58,720
that data and create a metric format now

00:17:54,940 --> 00:18:00,250
the problem with sampling is you know we

00:17:58,720 --> 00:18:02,380
have else it's a

00:18:00,250 --> 00:18:04,990
sample of something that small high

00:18:02,380 --> 00:18:07,210
resolution has higher frequency or its

00:18:04,990 --> 00:18:09,540
continuous think about temperature for

00:18:07,210 --> 00:18:12,340
example a continuous metric that with

00:18:09,540 --> 00:18:13,990
sampling discreetly and that creates

00:18:12,340 --> 00:18:15,640
problem because we can miss stuff like

00:18:13,990 --> 00:18:17,980
this pic over here and we can actually

00:18:15,640 --> 00:18:20,680
have shifts we can see the pics in the

00:18:17,980 --> 00:18:23,020
wrong place like the example here this

00:18:20,680 --> 00:18:25,000
is very common and we get a lot of out

00:18:23,020 --> 00:18:27,280
effects because of it another good

00:18:25,000 --> 00:18:29,200
example what happens when you sample a

00:18:27,280 --> 00:18:30,850
metric with low resolution and the

00:18:29,200 --> 00:18:34,060
metric itself has a very high rate of

00:18:30,850 --> 00:18:36,760
change you would miss a lot of stuff for

00:18:34,060 --> 00:18:38,350
example of the nuances here just go to

00:18:36,760 --> 00:18:40,600
waste we don't know about those Peaks

00:18:38,350 --> 00:18:43,450
here and of course we don't know that

00:18:40,600 --> 00:18:46,090
there were more than one peak okay we

00:18:43,450 --> 00:18:47,980
lose all that data and this is an

00:18:46,090 --> 00:18:51,340
artifact of sampling it's a fact of life

00:18:47,980 --> 00:18:53,290
we have to live with it now events

00:18:51,340 --> 00:18:55,180
usually come in this form I they carried

00:18:53,290 --> 00:18:57,820
some data that is numeric and data

00:18:55,180 --> 00:18:59,230
that's not numeric why because not

00:18:57,820 --> 00:19:01,780
everything in the world is a number for

00:18:59,230 --> 00:19:03,580
example status is it okay is a tener oh

00:19:01,780 --> 00:19:05,800
maybe an arrow message that's not

00:19:03,580 --> 00:19:08,020
numeric data we can convert it into

00:19:05,800 --> 00:19:13,300
numeric data by you know counting it and

00:19:08,020 --> 00:19:16,960
building pretty close I like to turn the

00:19:13,300 --> 00:19:18,970
histograms and so on now question

00:19:16,960 --> 00:19:21,070
suppose I'm collecting 10,000 events for

00:19:18,970 --> 00:19:22,720
second and that's actually not a very

00:19:21,070 --> 00:19:25,180
big number because if you think about

00:19:22,720 --> 00:19:27,220
you know the rate of stuff happening in

00:19:25,180 --> 00:19:28,870
your system for example how many I

00:19:27,220 --> 00:19:32,260
operations do you have per second

00:19:28,870 --> 00:19:34,570
thousands so let's say I'm collecting

00:19:32,260 --> 00:19:36,310
once in ten thousands events per second

00:19:34,570 --> 00:19:38,680
and each event takes about you know half

00:19:36,310 --> 00:19:44,500
a kilobyte how much data do I need to

00:19:38,680 --> 00:19:45,700
store water for one day take a guess too

00:19:44,500 --> 00:19:48,420
much is a good answer can I get a

00:19:45,700 --> 00:19:48,420
numeric answer

00:20:00,179 --> 00:20:08,669
how much 300 gigabytes megabytes okay

00:20:09,540 --> 00:20:17,710
you've got calculators you know okay

00:20:13,990 --> 00:20:20,260
it's about 400 gigabytes every day okay

00:20:17,710 --> 00:20:22,450
just our access logs the twix would take

00:20:20,260 --> 00:20:27,940
you know I think moving 10 terabytes a

00:20:22,450 --> 00:20:30,400
day probably so yeah it's it's a lot of

00:20:27,940 --> 00:20:33,040
data so it turns out that telemetry is

00:20:30,400 --> 00:20:34,929
really a big data problem it costs a lot

00:20:33,040 --> 00:20:38,620
of money and a lot of storage to store

00:20:34,929 --> 00:20:40,570
all your data so if you're going to try

00:20:38,620 --> 00:20:42,880
and you know measure everything in your

00:20:40,570 --> 00:20:44,860
system you're going to need the very

00:20:42,880 --> 00:20:46,419
large a dupe cluster and then you know

00:20:44,860 --> 00:20:48,309
and you need another cluster that's

00:20:46,419 --> 00:20:50,679
twice as big just to store the metrics

00:20:48,309 --> 00:20:55,080
for the first cluster so obviously

00:20:50,679 --> 00:20:55,080
that's not a solution so what do we do

00:20:55,470 --> 00:21:02,620
normal eyes yes all aggregate we call

00:20:59,559 --> 00:21:05,230
that aggregation aggregates aggregates

00:21:02,620 --> 00:21:08,049
or aggregation that's basically a form

00:21:05,230 --> 00:21:11,260
of lossy compression okay we stole

00:21:08,049 --> 00:21:12,820
Marshalls data but we throw away parts

00:21:11,260 --> 00:21:15,640
of that data that we think we don't need

00:21:12,820 --> 00:21:17,200
it's like mp3 for example where you know

00:21:15,640 --> 00:21:18,850
if you're not know do you feel you

00:21:17,200 --> 00:21:20,169
wouldn't hear the difference so we throw

00:21:18,850 --> 00:21:21,820
away everything everything you don't

00:21:20,169 --> 00:21:23,530
care about so it's the same idea with

00:21:21,820 --> 00:21:25,600
aggregate we just throw away a bunch of

00:21:23,530 --> 00:21:27,520
stuff that we think we won't need the

00:21:25,600 --> 00:21:30,100
problem is of course that we need to

00:21:27,520 --> 00:21:32,950
decide upfront what it is that we don't

00:21:30,100 --> 00:21:34,809
need and sometimes we find that we made

00:21:32,950 --> 00:21:36,730
a mistake we actually need that data but

00:21:34,809 --> 00:21:39,070
we can't get it anymore because we only

00:21:36,730 --> 00:21:42,580
have the aggregates so in many cases

00:21:39,070 --> 00:21:46,059
when we can we stole the raw data often

00:21:42,580 --> 00:21:48,010
in some lower priority storage but when

00:21:46,059 --> 00:21:49,990
we can't we just have the aggregates so

00:21:48,010 --> 00:21:52,450
it's very very important no one'd to

00:21:49,990 --> 00:21:55,870
pick the right aggregate otherwise we're

00:21:52,450 --> 00:21:58,210
going to have an issue so the aggregates

00:21:55,870 --> 00:22:00,910
almost all the fears stuff like a

00:21:58,210 --> 00:22:02,740
maximum minimum sound average also we've

00:22:00,910 --> 00:22:05,049
got kind of weird aggregates like last

00:22:02,740 --> 00:22:07,630
point in time window or random point in

00:22:05,049 --> 00:22:09,790
time window this is actually more common

00:22:07,630 --> 00:22:11,650
than you would think I will go and we'll

00:22:09,790 --> 00:22:12,700
be discussing that in a second also we

00:22:11,650 --> 00:22:15,660
have percentiles

00:22:12,700 --> 00:22:18,280
and histograms or reverse quantiles and

00:22:15,660 --> 00:22:21,340
each of those aggregates is suitable to

00:22:18,280 --> 00:22:23,500
different in two different things so for

00:22:21,340 --> 00:22:25,270
example i'm showing you why percentiles

00:22:23,500 --> 00:22:32,170
are critical for analyzing and finding

00:22:25,270 --> 00:22:36,370
l's so averages yes why are ever just

00:22:32,170 --> 00:22:38,560
bad especially Layton sees so if I shoot

00:22:36,370 --> 00:22:40,810
you in the head 10,000 times and I ate

00:22:38,560 --> 00:22:43,630
you only once on average you're alive

00:22:40,810 --> 00:22:46,390
but in practice you're dead and this is

00:22:43,630 --> 00:22:48,910
the same with averages your clients

00:22:46,390 --> 00:22:50,830
don't feel the average latency they feel

00:22:48,910 --> 00:22:53,200
individual engine seized and when you

00:22:50,830 --> 00:22:55,390
feel individual events you don't care

00:22:53,200 --> 00:22:57,910
about the average you care about the

00:22:55,390 --> 00:23:01,540
distribution you care about the latency

00:22:57,910 --> 00:23:04,920
that you felt okay averages would be a

00:23:01,540 --> 00:23:08,500
good fit to cases where you feel the

00:23:04,920 --> 00:23:10,750
impact of all the transactions for

00:23:08,500 --> 00:23:13,030
example money in the bank we don't care

00:23:10,750 --> 00:23:17,230
about money from individual transaction

00:23:13,030 --> 00:23:18,760
we care about you know how much money do

00:23:17,230 --> 00:23:21,370
we have in the back at the end of the

00:23:18,760 --> 00:23:23,350
year and therefore average you know

00:23:21,370 --> 00:23:26,770
money per month makes a lot more sense

00:23:23,350 --> 00:23:30,100
than you know 99 percentile of money per

00:23:26,770 --> 00:23:32,260
day okay so when you're interested in

00:23:30,100 --> 00:23:34,510
capacity planning or you know something

00:23:32,260 --> 00:23:36,160
stuff up like money then you would care

00:23:34,510 --> 00:23:39,760
about averages when you're talking about

00:23:36,160 --> 00:23:42,670
latency ello stuff that individual

00:23:39,760 --> 00:23:44,860
clients when individual clients feel

00:23:42,670 --> 00:23:46,480
individual transactions then you don't

00:23:44,860 --> 00:23:48,910
want to use averages you want to use

00:23:46,480 --> 00:23:51,990
something else and what do you want to

00:23:48,910 --> 00:23:56,950
use you usually want to use percentiles

00:23:51,990 --> 00:23:59,770
like p99 we've shown for so the

00:23:56,950 --> 00:24:01,720
definition of percentiles is the sample

00:23:59,770 --> 00:24:03,910
value sample value that was larger than

00:24:01,720 --> 00:24:07,450
other you know some number like nine

00:24:03,910 --> 00:24:10,750
this case of samples it is a number that

00:24:07,450 --> 00:24:12,340
has been that has that actually were was

00:24:10,750 --> 00:24:14,920
present in the sample something

00:24:12,340 --> 00:24:17,290
artificial number and to compute

00:24:14,920 --> 00:24:20,020
percentiles we actually need to store

00:24:17,290 --> 00:24:22,420
all the sample in memory or on disk and

00:24:20,020 --> 00:24:25,060
we have to sort through it and that

00:24:22,420 --> 00:24:26,530
means computation complexity of n log n

00:24:25,060 --> 00:24:29,860
it's expensive

00:24:26,530 --> 00:24:32,050
it's very very expensive if we try to

00:24:29,860 --> 00:24:36,250
compute that say the national percentile

00:24:32,050 --> 00:24:38,830
of of one day of transactions we would

00:24:36,250 --> 00:24:41,080
need a huge cluster because we have to

00:24:38,830 --> 00:24:42,910
store everything and we have to saw

00:24:41,080 --> 00:24:45,460
through it and that's very very

00:24:42,910 --> 00:24:48,340
expensive so instead of that we have

00:24:45,460 --> 00:24:49,480
shortcuts for example maximum is pretty

00:24:48,340 --> 00:24:51,280
easy to compute without holding

00:24:49,480 --> 00:24:52,870
everything in memory just you look at

00:24:51,280 --> 00:24:54,970
two samples you take the big one you can

00:24:52,870 --> 00:24:59,230
continue doing that all the time and

00:24:54,970 --> 00:25:02,770
that's it pretty easy oh there's no easy

00:24:59,230 --> 00:25:06,220
way to do that for for any percentile

00:25:02,770 --> 00:25:08,950
just for a few very distinct cases there

00:25:06,220 --> 00:25:12,760
are of course ways to estimate or

00:25:08,950 --> 00:25:14,790
approximate percent us and that's what

00:25:12,760 --> 00:25:19,890
we do when we have to get some kind of a

00:25:14,790 --> 00:25:23,020
number for let's say percent of a day

00:25:19,890 --> 00:25:25,720
now percentiles are not additive you

00:25:23,020 --> 00:25:27,790
cannot merge percentiles you can't take

00:25:25,720 --> 00:25:30,640
percentiles from a few hosts and say

00:25:27,790 --> 00:25:32,730
average a more you all know take a

00:25:30,640 --> 00:25:34,690
maximum of that and say this is the

00:25:32,730 --> 00:25:36,970
percentile of the cost of this is the

00:25:34,690 --> 00:25:39,970
p99 of cluster you can't do that if you

00:25:36,970 --> 00:25:43,960
need the pin 99 of that's a cluster of

00:25:39,970 --> 00:25:45,580
machines basically you need to get the

00:25:43,960 --> 00:25:48,970
data from all the machines the raw data

00:25:45,580 --> 00:25:51,970
and compute that percentile in an

00:25:48,970 --> 00:25:53,860
aggregator so you would need to send all

00:25:51,970 --> 00:25:55,990
the data to a single stats the

00:25:53,860 --> 00:25:59,680
aggregator or something like that an old

00:25:55,990 --> 00:26:02,170
computer and the p99 this means we have

00:25:59,680 --> 00:26:04,660
to design that up front if we need

00:26:02,170 --> 00:26:07,840
latency numbers for the entire cluster

00:26:04,660 --> 00:26:10,360
entire service you have to send it to a

00:26:07,840 --> 00:26:12,190
single single aggregator and if we want

00:26:10,360 --> 00:26:14,380
in addition the numbers pill host

00:26:12,190 --> 00:26:16,330
because often we would be interested in

00:26:14,380 --> 00:26:18,670
variations between the host we would

00:26:16,330 --> 00:26:20,440
have to send all the data twice to to

00:26:18,670 --> 00:26:22,540
aggregators one of the hosts and one

00:26:20,440 --> 00:26:29,050
that's doing aggregation pearl entire

00:26:22,540 --> 00:26:31,500
cluster and that's unfortunate instead

00:26:29,050 --> 00:26:34,030
we can use histograms so histograms are

00:26:31,500 --> 00:26:36,190
basically what we call reverse quantiles

00:26:34,030 --> 00:26:38,380
instead of saying give me the number for

00:26:36,190 --> 00:26:40,000
which let's say fifty percent of the

00:26:38,380 --> 00:26:42,520
sample falls below we

00:26:40,000 --> 00:26:44,890
say give me the percentile the

00:26:42,520 --> 00:26:48,340
percentage or the count of numbers

00:26:44,890 --> 00:26:51,430
between let's say 0 and 28 and then we

00:26:48,340 --> 00:26:54,760
would get a bin now normally a histogram

00:26:51,430 --> 00:26:57,070
would would have even be sitting evenly

00:26:54,760 --> 00:26:58,930
spaced bins the problem with the evenly

00:26:57,070 --> 00:27:01,870
spaced bins is that when we were talking

00:26:58,930 --> 00:27:04,300
about latency the highest latency is the

00:27:01,870 --> 00:27:06,460
timeout value which is sometimes 60

00:27:04,300 --> 00:27:09,220
seconds if you would hold you know

00:27:06,460 --> 00:27:12,220
buckets bins for every 10 milliseconds

00:27:09,220 --> 00:27:14,410
up to 60 seconds that requires a lot of

00:27:12,220 --> 00:27:15,910
memory in a lot of storage space so

00:27:14,410 --> 00:27:17,560
instead of doing that we can have

00:27:15,910 --> 00:27:20,320
something else for example

00:27:17,560 --> 00:27:23,140
logarithmically spaced bins first bin

00:27:20,320 --> 00:27:24,970
and say from 0 to 10 milliseconds second

00:27:23,140 --> 00:27:27,430
one would be from 10 milliseconds to 100

00:27:24,970 --> 00:27:30,370
milliseconds and so on that basically

00:27:27,430 --> 00:27:31,870
allows us to capture the long tail now

00:27:30,370 --> 00:27:34,030
the good thing is about this degrom is

00:27:31,870 --> 00:27:36,820
that as long as you keep the bin size is

00:27:34,030 --> 00:27:38,830
equal across the cluster across the time

00:27:36,820 --> 00:27:41,980
windows you can actually merge them very

00:27:38,830 --> 00:27:44,050
easily that you just add the account

00:27:41,980 --> 00:27:46,690
number for each bin and then you can get

00:27:44,050 --> 00:27:50,560
a histogram for cluster for an hour on

00:27:46,690 --> 00:27:53,080
and so on pretty easy so if histograms

00:27:50,560 --> 00:27:55,390
are so awesome why aren't everyone using

00:27:53,080 --> 00:27:57,640
them first of all storage because

00:27:55,390 --> 00:27:59,800
instead of storage installing one number

00:27:57,640 --> 00:28:03,790
per time window we're storing a lot of

00:27:59,800 --> 00:28:05,740
bins and that costs money and also we

00:28:03,790 --> 00:28:07,690
have to decide on beam scale some kind

00:28:05,740 --> 00:28:09,430
of bin schema are we going to use our

00:28:07,690 --> 00:28:11,740
logarithmically spaced bins so we're

00:28:09,430 --> 00:28:14,440
going to use linearly space pins what

00:28:11,740 --> 00:28:15,460
would be the bin size and so on and that

00:28:14,440 --> 00:28:17,590
means we have to know the distribution

00:28:15,460 --> 00:28:20,080
up front often we don't know the

00:28:17,590 --> 00:28:22,240
distribution up front especially when we

00:28:20,080 --> 00:28:23,950
talking about problems and the biggest

00:28:22,240 --> 00:28:25,990
problem of all not a lot of tools

00:28:23,950 --> 00:28:27,880
support histograms for example graphite

00:28:25,990 --> 00:28:32,530
doesn't support histograms doesn't

00:28:27,880 --> 00:28:33,700
visualize them doesn't store them so how

00:28:32,530 --> 00:28:35,590
do we do here we choose the right

00:28:33,700 --> 00:28:37,780
aggregate I'm first of all use

00:28:35,590 --> 00:28:41,140
histograms if you can or percentiles for

00:28:37,780 --> 00:28:44,920
latency also use min maximum size for

00:28:41,140 --> 00:28:46,930
latency and sizes because as we said 99

00:28:44,920 --> 00:28:50,110
percentile hides the one percent of your

00:28:46,930 --> 00:28:52,420
worst problems you can use histogram

00:28:50,110 --> 00:28:53,860
analysis for sizes and latency it's very

00:28:52,420 --> 00:28:55,450
very useful to understand

00:28:53,860 --> 00:28:56,830
distribution and what goes on in the

00:28:55,450 --> 00:29:00,700
system going to show an example in a

00:28:56,830 --> 00:29:02,590
second one way of doing that is storing

00:29:00,700 --> 00:29:05,230
all your transactions or at least aside

00:29:02,590 --> 00:29:07,299
labels sample of them inside the tool

00:29:05,230 --> 00:29:08,950
like elasticsearch it costs a lot of

00:29:07,299 --> 00:29:11,920
money to hold the raw data but you can

00:29:08,950 --> 00:29:14,100
do a lot more with it hold sums and

00:29:11,920 --> 00:29:17,110
averages for capacity and money and

00:29:14,100 --> 00:29:19,480
aggregate per domain the hosts per

00:29:17,110 --> 00:29:21,610
cluster the data center and so on and

00:29:19,480 --> 00:29:23,049
what you're looking at what you're

00:29:21,610 --> 00:29:26,650
looking for when you're trying to debug

00:29:23,049 --> 00:29:29,320
problems is basically deviations okay

00:29:26,650 --> 00:29:32,080
we're not looking at you know stuff

00:29:29,320 --> 00:29:33,730
that's behaving in a fairly consistent

00:29:32,080 --> 00:29:36,549
way we look at the stuff that's behaving

00:29:33,730 --> 00:29:39,010
a very inconsistent way so this is why

00:29:36,549 --> 00:29:41,530
we want the highest percentiles for

00:29:39,010 --> 00:29:45,850
example in lowest percentiles and why we

00:29:41,530 --> 00:29:53,049
care about deviations and so on so let

00:29:45,850 --> 00:29:55,960
me show an example of a histogram so

00:29:53,049 --> 00:29:57,549
this is a cabana and elastic search and

00:29:55,960 --> 00:29:59,770
basically holds the same data that we

00:29:57,549 --> 00:30:02,320
had for the problem that we simulated

00:29:59,770 --> 00:30:04,750
before same data we saw in graphite just

00:30:02,320 --> 00:30:06,640
this time in in cabana so you can

00:30:04,750 --> 00:30:08,710
actually see here this is the latency

00:30:06,640 --> 00:30:10,630
the Everett latency graph that we've

00:30:08,710 --> 00:30:13,900
seen before and this is the percentile

00:30:10,630 --> 00:30:15,970
graph that I'm creating from from the

00:30:13,900 --> 00:30:17,799
data stored from the excess log and here

00:30:15,970 --> 00:30:21,010
i can actually plot the maximum response

00:30:17,799 --> 00:30:23,530
is very easily because we stats di

00:30:21,010 --> 00:30:26,049
didn't aggregate it when I collected the

00:30:23,530 --> 00:30:28,000
data and but with elastic search I can

00:30:26,049 --> 00:30:30,429
do Alberto aggregations because I have

00:30:28,000 --> 00:30:32,020
the raw data so presenting the maximal

00:30:30,429 --> 00:30:34,750
is pretty easy and here you can see that

00:30:32,020 --> 00:30:36,790
the maximum wasn't far off from 99

00:30:34,750 --> 00:30:41,020
percent down in this case but often it

00:30:36,790 --> 00:30:43,660
is and again this is the graph for for

00:30:41,020 --> 00:30:46,120
coach ed and hearing to see that the

00:30:43,660 --> 00:30:50,020
maximum latency of CouchDB was a lot

00:30:46,120 --> 00:30:53,260
higher than the 99 percentile of the

00:30:50,020 --> 00:30:58,929
CouchDB transactions but let's look at

00:30:53,260 --> 00:31:01,660
histograms so here we can see the

00:30:58,929 --> 00:31:04,179
latency plotted on top but you can see

00:31:01,660 --> 00:31:06,070
the histograms Laden down the bottom so

00:31:04,179 --> 00:31:07,200
this histogram is for the entire time

00:31:06,070 --> 00:31:08,730
window and here

00:31:07,200 --> 00:31:11,940
to see that we have we actually have two

00:31:08,730 --> 00:31:16,200
modes we have two transactions that'll

00:31:11,940 --> 00:31:19,679
slow so here and a lot of transactions

00:31:16,200 --> 00:31:21,929
that are relatively fast it's a bimodal

00:31:19,679 --> 00:31:23,909
distribution basically and this is the

00:31:21,929 --> 00:31:26,190
response I histogram and you can see

00:31:23,909 --> 00:31:29,120
that most of the responses were roughly

00:31:26,190 --> 00:31:33,029
the same size now if i zoom into the

00:31:29,120 --> 00:31:37,919
problem window we will see the

00:31:33,029 --> 00:31:40,799
distribution changing okay and this

00:31:37,919 --> 00:31:43,049
helps to understand problems a lot

00:31:40,799 --> 00:31:45,179
better because we can see the changes in

00:31:43,049 --> 00:31:48,929
the distribution of response sizes and

00:31:45,179 --> 00:31:51,899
response response times for example now

00:31:48,929 --> 00:31:53,760
we see that if before we had a bimodal

00:31:51,899 --> 00:31:56,190
distribution now all the responses are

00:31:53,760 --> 00:31:58,769
slow most of them at least and we have a

00:31:56,190 --> 00:32:00,929
very very long tail now long tails tell

00:31:58,769 --> 00:32:02,610
us about a lot about percent us because

00:32:00,929 --> 00:32:05,100
percent I was like the hyper centers are

00:32:02,610 --> 00:32:07,019
basically the long tail so we'd see that

00:32:05,100 --> 00:32:10,500
now we have responses stretching all the

00:32:07,019 --> 00:32:12,899
way from about 500 milliseconds or even

00:32:10,500 --> 00:32:14,610
300 milliseconds all the way to about

00:32:12,899 --> 00:32:16,950
two million two milliseconds which was

00:32:14,610 --> 00:32:30,230
the maximum transaction time that we saw

00:32:16,950 --> 00:32:30,230
plotted and yes

00:32:30,450 --> 00:32:35,820
so let's talk about resolution for a

00:32:32,880 --> 00:32:38,850
second what time interval do we want to

00:32:35,820 --> 00:32:41,400
go to a graph and plot and sample so

00:32:38,850 --> 00:32:44,850
most of the tools usually have a default

00:32:41,400 --> 00:32:46,650
of 60 seconds now a human usually needs

00:32:44,850 --> 00:32:48,810
about five data points at least to

00:32:46,650 --> 00:32:51,570
actually see a trend so that means that

00:32:48,810 --> 00:32:53,370
with one minute resolution to take about

00:32:51,570 --> 00:32:55,620
five minutes before we can even observe

00:32:53,370 --> 00:32:57,360
a problem okay if you're making a change

00:32:55,620 --> 00:32:59,370
a deploy or something like that five

00:32:57,360 --> 00:33:00,840
minutes is a very long time think about

00:32:59,370 --> 00:33:03,420
your alerting system if you want to be

00:33:00,840 --> 00:33:05,400
alerted within let's say one minute you

00:33:03,420 --> 00:33:07,860
need at least five data point that means

00:33:05,400 --> 00:33:10,230
something resolution of about 10 seconds

00:33:07,860 --> 00:33:11,820
at least okay and I'm not even talking

00:33:10,230 --> 00:33:15,750
about the stuff that we missed because

00:33:11,820 --> 00:33:18,300
of sampling artifacts so although a lot

00:33:15,750 --> 00:33:20,910
of people sample with 60 second

00:33:18,300 --> 00:33:22,680
resolution I think that we probably need

00:33:20,910 --> 00:33:24,930
one second or 10 second resolution at

00:33:22,680 --> 00:33:27,930
least you don't need to hold the data

00:33:24,930 --> 00:33:29,850
for long actually you can down sample it

00:33:27,930 --> 00:33:32,010
or trampled in two different intervals

00:33:29,850 --> 00:33:34,830
and store the data the high-resolution

00:33:32,010 --> 00:33:36,140
data for say a few hours or a day just

00:33:34,830 --> 00:33:39,540
for loading and debugging purposes

00:33:36,140 --> 00:33:43,980
install the the low resolution data for

00:33:39,540 --> 00:33:45,630
you know four years so now the problem

00:33:43,980 --> 00:33:48,120
of course is the troll ups and down

00:33:45,630 --> 00:33:50,600
scaling is very very hard and I'm going

00:33:48,120 --> 00:33:54,590
to show an example of that in a second

00:33:50,600 --> 00:33:59,610
now you probably all know this quote

00:33:54,590 --> 00:34:02,700
mark Swain okay the problem with a lot

00:33:59,610 --> 00:34:04,980
of our systems is not the stuff that we

00:34:02,700 --> 00:34:06,690
see it's just stuff that the system is

00:34:04,980 --> 00:34:09,510
doing to our graphs and metrics without

00:34:06,690 --> 00:34:11,190
their snowing and this is a lot of

00:34:09,510 --> 00:34:13,170
metric systems actually do a lot of

00:34:11,190 --> 00:34:15,330
stuff that is literally bad and we just

00:34:13,170 --> 00:34:22,010
don't know about he'll give it a few

00:34:15,330 --> 00:34:22,010
examples so one second

00:34:23,230 --> 00:34:27,909
so high we actually talked about this

00:34:25,419 --> 00:34:35,619
yesterday I'm going to show another

00:34:27,909 --> 00:34:39,790
example of this anyway okay so this is a

00:34:35,619 --> 00:34:43,330
graph and Desolator was for this latency

00:34:39,790 --> 00:34:45,639
graph was 150 milliseconds and obviously

00:34:43,330 --> 00:34:49,300
there's no problem right like the system

00:34:45,639 --> 00:34:55,240
is within the SLA what happens if we

00:34:49,300 --> 00:35:00,990
zoom let's say here ouch we've exceeded

00:34:55,240 --> 00:35:00,990
the SLA now why don't we see this before

00:35:01,619 --> 00:35:11,970
sorry averages of what exactly this is

00:35:08,980 --> 00:35:14,500
known as picky as you'll spy corrosion

00:35:11,970 --> 00:35:17,560
basically when a graphing system doesn't

00:35:14,500 --> 00:35:19,420
have enough pixels to plot sake and data

00:35:17,560 --> 00:35:21,369
points it would take an average of those

00:35:19,420 --> 00:35:26,140
data points and that means that if the

00:35:21,369 --> 00:35:28,690
pic is very very very small in in terms

00:35:26,140 --> 00:35:31,330
of time if it was a very fast peak to

00:35:28,690 --> 00:35:33,790
just vanish okay so this is why you

00:35:31,330 --> 00:35:36,570
don't see those pics when you're zooming

00:35:33,790 --> 00:35:39,070
out so let me zoom out for a second and

00:35:36,570 --> 00:35:41,260
you see that the pics of their vanished

00:35:39,070 --> 00:35:44,680
now there is a way to fix that in

00:35:41,260 --> 00:35:46,450
graphite but not mean only no systems so

00:35:44,680 --> 00:35:49,590
there is a function called consolidate

00:35:46,450 --> 00:35:54,369
by and i'm going to add that function

00:35:49,590 --> 00:36:01,600
special sorry one second consolidated by

00:35:54,369 --> 00:36:02,770
and yes consolidate by maximum and again

00:36:01,600 --> 00:36:05,050
i need to know what i'm looking for

00:36:02,770 --> 00:36:06,670
because i'm looking for pigs on the

00:36:05,050 --> 00:36:10,750
upwards direction this case i will

00:36:06,670 --> 00:36:16,230
choose max and now i'm supposed to show

00:36:10,750 --> 00:36:19,410
the to see the pics right but i don't

00:36:16,230 --> 00:36:19,410
why not

00:36:20,010 --> 00:36:23,910
because it turns out that graphite and a

00:36:22,260 --> 00:36:26,850
lot of other storage engines as well

00:36:23,910 --> 00:36:29,520
it's actually doing those things in two

00:36:26,850 --> 00:36:32,460
places a in the graph system when it's

00:36:29,520 --> 00:36:36,330
presenting the data but be in storage so

00:36:32,460 --> 00:36:38,400
when you then when you some you create a

00:36:36,330 --> 00:36:41,040
metric you define how much attention you

00:36:38,400 --> 00:36:43,260
want to that matrix for example in this

00:36:41,040 --> 00:36:45,360
case I stole high-resolution data for

00:36:43,260 --> 00:36:47,310
six hours but after that they stole oh

00:36:45,360 --> 00:36:48,990
the solution data and what if graph I do

00:36:47,310 --> 00:36:51,300
when it converts you know the

00:36:48,990 --> 00:36:53,940
high-resolution data of let's say 10

00:36:51,300 --> 00:36:57,570
seconds to a one-minute resolution for

00:36:53,940 --> 00:36:59,730
long-term retention it ever jizz so I've

00:36:57,570 --> 00:37:01,290
lost the peaks and this time I can't

00:36:59,730 --> 00:37:04,590
even reconstruct the peaks I've lost

00:37:01,290 --> 00:37:07,620
them indefinitely okay because it wasn't

00:37:04,590 --> 00:37:10,620
storage so you can actually see the

00:37:07,620 --> 00:37:12,660
example of of consolidated by here when

00:37:10,620 --> 00:37:15,680
I go to the high-resolution data that's

00:37:12,660 --> 00:37:17,880
in storage now it's with consolidate by

00:37:15,680 --> 00:37:21,780
and now then I would remove that

00:37:17,880 --> 00:37:24,210
function and the sharp-eyed would see

00:37:21,780 --> 00:37:29,700
that for example this peak has become a

00:37:24,210 --> 00:37:37,620
much smaller so this is a really big

00:37:29,700 --> 00:37:40,140
problem for us because sorry this is a

00:37:37,620 --> 00:37:42,300
little big problem for us because not

00:37:40,140 --> 00:37:45,410
only a fix erased sometimes we can we

00:37:42,300 --> 00:37:49,070
can we can't recover that data okay so

00:37:45,410 --> 00:37:53,030
this means we have to design up front

00:37:49,070 --> 00:37:55,500
the retention and the sorry and

00:37:53,030 --> 00:37:57,540
aggregation roll-ups of data now

00:37:55,500 --> 00:38:01,290
graphite with graphite you can actually

00:37:57,540 --> 00:38:04,170
customize the way it was up the data so

00:38:01,290 --> 00:38:06,240
there's a file called the aggregation

00:38:04,170 --> 00:38:08,340
schema or soldier gregation if i'm not

00:38:06,240 --> 00:38:12,720
mistaken and in that file you can define

00:38:08,340 --> 00:38:14,280
for pattern of metric what kind of

00:38:12,720 --> 00:38:16,860
aggregations would use when it rolls up

00:38:14,280 --> 00:38:18,780
the data should use maximum value should

00:38:16,860 --> 00:38:21,030
use the average value or maybe the sun

00:38:18,780 --> 00:38:22,890
and so on and for counters for example

00:38:21,030 --> 00:38:25,080
you might want to take the left thumb

00:38:22,890 --> 00:38:26,640
because it keeps going up for gages you

00:38:25,080 --> 00:38:29,090
would probably take maximum if you're

00:38:26,640 --> 00:38:32,130
looking for if it's gauge that you know

00:38:29,090 --> 00:38:33,780
you're looking for problems that go up

00:38:32,130 --> 00:38:35,310
you would take the minimum

00:38:33,780 --> 00:38:37,560
looking for problems that go down for

00:38:35,310 --> 00:38:40,410
think for example of the minimum metric

00:38:37,560 --> 00:38:42,360
okay so if you design the aggregations

00:38:40,410 --> 00:38:45,150
upfront you would avoid those problems

00:38:42,360 --> 00:38:46,950
and you will be able to see the peaks

00:38:45,150 --> 00:38:54,030
that were supposed to be going to go

00:38:46,950 --> 00:38:56,640
away now I don't forget one thing that

00:38:54,030 --> 00:38:59,820
gig a tous also do this it's not only

00:38:56,640 --> 00:39:02,700
the storage what happens when you send

00:38:59,820 --> 00:39:05,850
let's say five metrics in one time

00:39:02,700 --> 00:39:08,790
window which metric with the carbon news

00:39:05,850 --> 00:39:10,650
or whatever collector you're using it

00:39:08,790 --> 00:39:12,870
can't use all of them obviously can only

00:39:10,650 --> 00:39:15,360
emit one data point to a time windows so

00:39:12,870 --> 00:39:18,480
which one will it use so carbon uses the

00:39:15,360 --> 00:39:20,160
last value okay basically it erases all

00:39:18,480 --> 00:39:23,250
the other values and get the last one

00:39:20,160 --> 00:39:24,330
with stats d it depends on you know what

00:39:23,250 --> 00:39:26,850
kind of metrics type you are using

00:39:24,330 --> 00:39:28,080
gauges would use the last one timers

00:39:26,850 --> 00:39:29,880
would actually use a bunch of

00:39:28,080 --> 00:39:33,540
obligations percentiles and so on and

00:39:29,880 --> 00:39:38,370
counters again use basically the sum

00:39:33,540 --> 00:39:39,900
over time window so this is again why

00:39:38,370 --> 00:39:42,270
you need to decide up front on the type

00:39:39,900 --> 00:39:44,220
of metric all you need to understand how

00:39:42,270 --> 00:39:48,240
your aggregator behaves it if it's

00:39:44,220 --> 00:39:53,100
configurable or not now another problem

00:39:48,240 --> 00:39:55,290
is gauges having low resolution low

00:39:53,100 --> 00:39:56,850
resolution and there's a bit hides a lot

00:39:55,290 --> 00:40:05,760
of problems so again I'll show you an

00:39:56,850 --> 00:40:08,310
example so here you think you have three

00:40:05,760 --> 00:40:11,570
lines and this is the original metric

00:40:08,310 --> 00:40:14,160
this is a high-resolution gauge okay and

00:40:11,570 --> 00:40:16,650
the first thing you'll notice is that

00:40:14,160 --> 00:40:19,740
metric behaves very differently this

00:40:16,650 --> 00:40:23,870
time window this time with your right so

00:40:19,740 --> 00:40:23,870
do i have a problem did i do a deploy

00:40:24,350 --> 00:40:33,730
everybody wants to take a guess yes

00:40:31,080 --> 00:40:35,950
so the question the answer is no I

00:40:33,730 --> 00:40:38,020
haven't done any of anything the system

00:40:35,950 --> 00:40:39,430
is actually stable the reason is seeing

00:40:38,020 --> 00:40:43,870
such a big difference between here and

00:40:39,430 --> 00:40:47,560
here is because of downscaling this is

00:40:43,870 --> 00:40:49,480
an artifact okay it's not real this is

00:40:47,560 --> 00:40:52,420
an artifact created by the downscaling

00:40:49,480 --> 00:40:54,850
behavior of the storage engine ok this

00:40:52,420 --> 00:41:00,430
is high-resolution data one second data

00:40:54,850 --> 00:41:02,230
actually and this is 10 second data ok

00:41:00,430 --> 00:41:04,600
this is how the metric actually behaved

00:41:02,230 --> 00:41:06,730
this is what you see after the

00:41:04,600 --> 00:41:08,500
downscaling and you would think that you

00:41:06,730 --> 00:41:09,850
will have a problem but actually it's

00:41:08,500 --> 00:41:11,800
only the dow scaling behavior of your

00:41:09,850 --> 00:41:13,480
system and if you're unaware of that

00:41:11,800 --> 00:41:15,220
you're going to be spending a lot of

00:41:13,480 --> 00:41:17,740
time debugging problem doesn't actually

00:41:15,220 --> 00:41:22,900
exist so let's zoom into the real data

00:41:17,740 --> 00:41:24,670
now for a second and let's see how the

00:41:22,900 --> 00:41:28,420
same gauge just sampled in lower

00:41:24,670 --> 00:41:32,310
frequency behaves ouch that looks

00:41:28,420 --> 00:41:37,660
completely different doesn't it right

00:41:32,310 --> 00:41:39,760
doesn't look even remotely the same so

00:41:37,660 --> 00:41:43,180
this is what happens what's doing this

00:41:39,760 --> 00:41:46,300
is what happens when you sample in a low

00:41:43,180 --> 00:41:49,320
frequency you get artifacts ok for

00:41:46,300 --> 00:41:52,140
example this peak here is an artifact

00:41:49,320 --> 00:41:55,240
you can see that there was no pic here

00:41:52,140 --> 00:41:58,030
so what happened basically when we do

00:41:55,240 --> 00:42:00,520
the lower solution sampling we kind of

00:41:58,030 --> 00:42:01,690
some the whole time window and I mean

00:42:00,520 --> 00:42:04,990
anything that happened in that time

00:42:01,690 --> 00:42:07,150
window we just go into one dot which is

00:42:04,990 --> 00:42:09,220
this one so depending on the grid

00:42:07,150 --> 00:42:12,640
aggregation function we use whether it

00:42:09,220 --> 00:42:14,620
was a count or last one or some we would

00:42:12,640 --> 00:42:16,150
get a different effect but it's

00:42:14,620 --> 00:42:19,450
definitely not a good representation of

00:42:16,150 --> 00:42:20,860
the metric but this one over here the

00:42:19,450 --> 00:42:22,420
blue one is actually better

00:42:20,860 --> 00:42:25,930
representation and that is the

00:42:22,420 --> 00:42:28,000
derivative of our counter so with the

00:42:25,930 --> 00:42:30,190
gauge think about memory for example we

00:42:28,000 --> 00:42:32,020
can sample memory or we can count the

00:42:30,190 --> 00:42:35,200
locations and the allocations get two

00:42:32,020 --> 00:42:37,840
counters from it and counters as opposed

00:42:35,200 --> 00:42:40,780
to two low resolution gauges they don't

00:42:37,840 --> 00:42:42,580
lose data instead the kind of smudge it

00:42:40,780 --> 00:42:44,080
over the time window so you can actually

00:42:42,580 --> 00:42:47,050
see it here pretty well

00:42:44,080 --> 00:42:51,940
um maybe the color is bad actually can

00:42:47,050 --> 00:42:53,800
you see the blue line no yes okay so you

00:42:51,940 --> 00:42:55,960
can actually see that blue line here is

00:42:53,800 --> 00:42:58,330
smudged it didn't lose all those tiny

00:42:55,960 --> 00:43:00,940
pics instead of kind of smudge them over

00:42:58,330 --> 00:43:03,250
the time window so counters are

00:43:00,940 --> 00:43:05,080
preferable to gages if possible because

00:43:03,250 --> 00:43:06,930
when you go to a low resolution you

00:43:05,080 --> 00:43:10,000
don't lose the data you just smudge it

00:43:06,930 --> 00:43:21,220
the data won't go away we'll just look a

00:43:10,000 --> 00:43:24,700
bit different sorry so the TLDR is

00:43:21,220 --> 00:43:27,250
basically use counters if you can try

00:43:24,700 --> 00:43:30,040
not to use gauges if you you know if you

00:43:27,250 --> 00:43:31,720
have low resolution gauges but you know

00:43:30,040 --> 00:43:34,270
you can't always do that one of the

00:43:31,720 --> 00:43:37,840
problems is we don't always control the

00:43:34,270 --> 00:43:39,790
metric generators okay for example jmx

00:43:37,840 --> 00:43:43,000
you know sometimes it generates counters

00:43:39,790 --> 00:43:46,750
sometimes it generates gauges we don't

00:43:43,000 --> 00:43:48,850
really have a say in that another

00:43:46,750 --> 00:43:51,370
problem that we often see is mixed modes

00:43:48,850 --> 00:43:53,740
this will happen for example when you

00:43:51,370 --> 00:43:55,180
send a metric of a transaction but you

00:43:53,740 --> 00:43:57,580
don't separate between the different

00:43:55,180 --> 00:43:59,290
modes of the transaction for example you

00:43:57,580 --> 00:44:05,460
send the elves and success at the same

00:43:59,290 --> 00:44:05,460
time okay what happens then let's see

00:44:09,290 --> 00:44:14,270
so this is a mixed timer and this is

00:44:11,810 --> 00:44:17,090
mixed gauge and here you can actually

00:44:14,270 --> 00:44:20,170
see both them both modes you can see

00:44:17,090 --> 00:44:23,990
that for example the gauge I've had

00:44:20,170 --> 00:44:25,850
successes and okay transactions they

00:44:23,990 --> 00:44:28,190
were both very very stable but if i look

00:44:25,850 --> 00:44:30,260
at the mixed metric the metric that

00:44:28,190 --> 00:44:32,600
actually includes both it looks very

00:44:30,260 --> 00:44:36,020
very unstable and because that's because

00:44:32,600 --> 00:44:38,060
aggregation is lost in every time window

00:44:36,020 --> 00:44:40,400
i would get the last value which

00:44:38,060 --> 00:44:42,350
basically is random between you know

00:44:40,400 --> 00:44:43,700
either or KL success i have no idea what

00:44:42,350 --> 00:44:46,490
I'm going to get so just flexural

00:44:43,700 --> 00:44:49,030
fluctuates randomly between no two

00:44:46,490 --> 00:44:52,610
values this metric is completely useless

00:44:49,030 --> 00:44:55,790
okay this is a badly designed metric and

00:44:52,610 --> 00:44:57,920
with timers i would get similar behavior

00:44:55,790 --> 00:45:00,230
but it depends again on the aggregation

00:44:57,920 --> 00:45:02,810
so if i use upper 99 what do you think

00:45:00,230 --> 00:45:05,030
would happen with with two transactions

00:45:02,810 --> 00:45:06,920
one that is fast their holes and once

00:45:05,030 --> 00:45:09,260
it's slow the actual successes the

00:45:06,920 --> 00:45:12,650
actual transaction excluded basically

00:45:09,260 --> 00:45:16,100
I've wiped out one metric with p99 I

00:45:12,650 --> 00:45:20,960
will only get the slow metric so you can

00:45:16,100 --> 00:45:23,960
see here that that this metric here

00:45:20,960 --> 00:45:25,760
around this is both times together

00:45:23,960 --> 00:45:29,200
ninety-nine percent of those timers

00:45:25,760 --> 00:45:32,960
together around 1.2 sorry twelve hundred

00:45:29,200 --> 00:45:36,740
milliseconds it's actually almost the

00:45:32,960 --> 00:45:38,960
same as the okay metric luckily exactly

00:45:36,740 --> 00:45:41,630
the same so I basically erased all the

00:45:38,960 --> 00:45:44,510
data i had about else if i used a

00:45:41,630 --> 00:45:48,110
different aggregation let's say median

00:45:44,510 --> 00:45:50,420
again because of the the very different

00:45:48,110 --> 00:45:53,300
values of the elves and k matrix i would

00:45:50,420 --> 00:45:55,520
just lose one of them okay so either I

00:45:53,300 --> 00:45:58,580
get a completely unusable metric or I

00:45:55,520 --> 00:46:00,410
use get a metric know without the the

00:45:58,580 --> 00:46:02,570
values of the adjutant the other type of

00:46:00,410 --> 00:46:10,240
transaction so what is the solution to

00:46:02,570 --> 00:46:10,240
this how would you solve this anyone

00:46:11,440 --> 00:46:21,080
silly exactly separate the count will

00:46:19,130 --> 00:46:23,390
separate the gauges send two metrics

00:46:21,080 --> 00:46:25,220
instead of one and this is relevant for

00:46:23,390 --> 00:46:27,200
example for web servers where we have

00:46:25,220 --> 00:46:29,570
more than one type of transaction and

00:46:27,200 --> 00:46:31,820
not all transactions have the same speed

00:46:29,570 --> 00:46:34,520
saying latency same sizes and so on if

00:46:31,820 --> 00:46:36,560
you know upfront the different modes we

00:46:34,520 --> 00:46:38,960
want to generate different metrics which

00:46:36,560 --> 00:46:41,510
type of transaction and you have to

00:46:38,960 --> 00:46:43,400
multiply that by two because for every

00:46:41,510 --> 00:46:46,040
transaction there's also a narrow mode

00:46:43,400 --> 00:46:49,040
okay and usually in el mode would be a

00:46:46,040 --> 00:46:53,360
lot faster than you know the okay mode

00:46:49,040 --> 00:46:57,130
of that transaction so the fun part

00:46:53,360 --> 00:47:00,200
because I think we're out of time is a

00:46:57,130 --> 00:47:04,130
building useful graphs how do we build

00:47:00,200 --> 00:47:07,070
useful graphs so just want to show

00:47:04,130 --> 00:47:09,980
something this is not the default view

00:47:07,070 --> 00:47:12,190
of Cabana of saurian a cabana of go

00:47:09,980 --> 00:47:12,190
fauna

00:47:13,200 --> 00:47:19,770
anybody who used griffin i knows that

00:47:14,910 --> 00:47:21,990
the default is actually black i can

00:47:19,770 --> 00:47:24,320
change it if you want to see it to be

00:47:21,990 --> 00:47:24,320
Dudley

00:47:34,240 --> 00:47:37,240
sorry

00:47:41,570 --> 00:47:53,570
I need to sign out so this is the

00:47:45,560 --> 00:47:56,900
default and obviously have personalized

00:47:53,570 --> 00:47:59,050
it anyway the default is black and the

00:47:56,900 --> 00:48:04,610
default is very very bad it it really

00:47:59,050 --> 00:48:06,710
prevents you from seeing stuff also the

00:48:04,610 --> 00:48:11,510
default by default when I create a graph

00:48:06,710 --> 00:48:14,120
so let me create a graph you can see

00:48:11,510 --> 00:48:16,790
that I have this very nicely looking

00:48:14,120 --> 00:48:20,000
area under on the bottom now this looks

00:48:16,790 --> 00:48:29,200
really good until I start adding metrics

00:48:20,000 --> 00:48:29,200
so for example I would use and let's add

00:48:39,589 --> 00:48:47,200
that said cpu follow

00:48:54,490 --> 00:49:03,369
okay this looks horrible right this is

00:48:57,369 --> 00:49:05,230
not really usable sorry about what

00:49:03,369 --> 00:49:10,180
that's not usable first of all because

00:49:05,230 --> 00:49:11,770
of the filling the just kind of overlays

00:49:10,180 --> 00:49:13,630
on each on top of each other the second

00:49:11,770 --> 00:49:17,230
reason is we basically have too many

00:49:13,630 --> 00:49:23,100
lines plotted on the same graph so never

00:49:17,230 --> 00:49:27,160
do that so basically what you want to do

00:49:23,100 --> 00:49:31,360
is first of all never put more than

00:49:27,160 --> 00:49:33,490
three serious on a graph okay if its CPU

00:49:31,360 --> 00:49:36,010
then it's okay to stack it up to one

00:49:33,490 --> 00:49:38,500
hundred percent all of them but anything

00:49:36,010 --> 00:49:41,980
else never do this it's just confusing

00:49:38,500 --> 00:49:43,840
it often I would see if people that have

00:49:41,980 --> 00:49:45,790
a cluster of ten machines they will just

00:49:43,840 --> 00:49:48,130
plot all time machines on the same graph

00:49:45,790 --> 00:49:49,930
you can't make anything out of it you

00:49:48,130 --> 00:49:52,119
have no idea which machine is behaving

00:49:49,930 --> 00:49:53,920
what in what way when you're working

00:49:52,119 --> 00:49:55,600
with the cluster you usually want just

00:49:53,920 --> 00:49:58,540
to throw the aggregates of that cluster

00:49:55,600 --> 00:50:00,100
not the Egyptian almost deviant if you

00:49:58,540 --> 00:50:03,430
if you really care about the individual

00:50:00,100 --> 00:50:05,109
individual machines now make sure that

00:50:03,430 --> 00:50:07,350
you have a reasonable time frame if

00:50:05,109 --> 00:50:10,600
you're looking for problems that

00:50:07,350 --> 00:50:12,490
happened in recently don't your graph of

00:50:10,600 --> 00:50:15,160
the entire day show a graph of one day

00:50:12,490 --> 00:50:17,710
of one hour for example if you did a

00:50:15,160 --> 00:50:19,869
reference show graph of yesterday and

00:50:17,710 --> 00:50:21,790
today but in two different graphs don't

00:50:19,869 --> 00:50:23,260
put them on their own one you know big

00:50:21,790 --> 00:50:25,359
time scale because it's impossible to

00:50:23,260 --> 00:50:31,750
understand what goes on and also you

00:50:25,359 --> 00:50:33,760
will suffer from P corrosion multiple

00:50:31,750 --> 00:50:35,619
wire scales when you're showing when

00:50:33,760 --> 00:50:37,930
you're plotting for example memory

00:50:35,619 --> 00:50:40,630
verses latency which are two very very

00:50:37,930 --> 00:50:42,730
different numbers and scale and it's

00:50:40,630 --> 00:50:44,859
very tempting to scale one graft so you

00:50:42,730 --> 00:50:47,290
can actually see the other one and use

00:50:44,859 --> 00:50:49,060
multiple y axis for that it's very very

00:50:47,290 --> 00:50:51,700
tempting try to avoid that created

00:50:49,060 --> 00:50:54,070
create two graphs instead when you're

00:50:51,700 --> 00:50:57,580
using multiple times y scales it's

00:50:54,070 --> 00:51:00,460
confusing to the eye and only put

00:50:57,580 --> 00:51:02,590
related serious on the same graph okay

00:51:00,460 --> 00:51:04,540
again if you're plotting memory and

00:51:02,590 --> 00:51:07,579
latency at the same time it's confusing

00:51:04,540 --> 00:51:11,900
you don't want to be doing that

00:51:07,579 --> 00:51:14,450
never ever ever ever mixed x scales if

00:51:11,900 --> 00:51:17,119
you're building a dashboard try not to

00:51:14,450 --> 00:51:22,249
put graphs on days and weeks at the same

00:51:17,119 --> 00:51:23,599
time it confuses everyone really you

00:51:22,249 --> 00:51:26,180
have no idea how many times I've seen

00:51:23,599 --> 00:51:27,950
people you know find bugs that weren't

00:51:26,180 --> 00:51:29,989
even in the dinner exists because they

00:51:27,950 --> 00:51:31,190
thought that we're looking at a one day

00:51:29,989 --> 00:51:32,869
time scale when they were actually

00:51:31,190 --> 00:51:35,450
looking at the one-hour time scale and

00:51:32,869 --> 00:51:37,789
so on now also it's very important to

00:51:35,450 --> 00:51:39,019
have visual references for example if

00:51:37,789 --> 00:51:41,749
you remember that I've plotted the

00:51:39,019 --> 00:51:43,579
thresholds as a line this is a visual

00:51:41,749 --> 00:51:45,380
reference it's very very important it

00:51:43,579 --> 00:51:48,170
helps you to understand whether this is

00:51:45,380 --> 00:51:52,219
an actual problem or maybe it's just a

00:51:48,170 --> 00:51:55,069
tiny fluctuation that you know is normal

00:51:52,219 --> 00:51:57,200
for that kind of metric I'm every metric

00:51:55,069 --> 00:51:59,150
everything in life has some kind of

00:51:57,200 --> 00:52:00,979
variance often that variance is

00:51:59,150 --> 00:52:03,619
insignificant how do we know if it's

00:52:00,979 --> 00:52:08,599
significant or insignificant depends on

00:52:03,619 --> 00:52:10,339
again the zoom for so for example if you

00:52:08,599 --> 00:52:12,799
if i want to go to my boss and get the

00:52:10,339 --> 00:52:16,160
know a lot of money to fix the problem

00:52:12,799 --> 00:52:20,019
that doesn't even exist i would probably

00:52:16,160 --> 00:52:23,839
show him you know step pick a graph

00:52:20,019 --> 00:52:29,739
let's pick this graph i would take this

00:52:23,839 --> 00:52:29,739
graph and I would you know go here and

00:52:29,859 --> 00:52:36,880
change the scale a bit let's say five oh

00:52:37,539 --> 00:52:42,469
my god i have a problem the system is

00:52:40,039 --> 00:52:50,509
behaving horribly i need a lot of money

00:52:42,469 --> 00:52:54,140
to fix this Oh everything is fine there

00:52:50,509 --> 00:52:56,359
is no problem the company is not losing

00:52:54,140 --> 00:52:58,309
any money this is the effect of scale

00:52:56,359 --> 00:53:00,979
and visual references this is why it's

00:52:58,309 --> 00:53:02,599
very important to tune up front the

00:53:00,979 --> 00:53:05,029
scale of your graphs especially on

00:53:02,599 --> 00:53:06,859
dashboards and often people leave it in

00:53:05,029 --> 00:53:09,199
the default mode which is auto scale

00:53:06,859 --> 00:53:11,539
then what happens without the scale if

00:53:09,199 --> 00:53:13,699
you don't have any pigs or if you have a

00:53:11,539 --> 00:53:15,979
tiny peek you would think it's a huge

00:53:13,699 --> 00:53:18,769
pig because the system auto scale to

00:53:15,979 --> 00:53:21,380
that peak okay so if you're building a

00:53:18,769 --> 00:53:25,040
dashboard make sure you decide up front

00:53:21,380 --> 00:53:28,460
what kind of value you would consider to

00:53:25,040 --> 00:53:34,540
be abnormal with 100 is it 1000 and tune

00:53:28,460 --> 00:53:44,150
the scale that graph accordingly so

00:53:34,540 --> 00:53:49,190
let's go back legend who puts legends on

00:53:44,150 --> 00:53:51,530
the graph who like 10 people put legends

00:53:49,190 --> 00:53:53,630
on your graphs for god sakes it's very

00:53:51,530 --> 00:53:56,210
important and if you put legends try to

00:53:53,630 --> 00:53:57,950
also put representative values and when

00:53:56,210 --> 00:53:59,750
I self-identity values i don't mean the

00:53:57,950 --> 00:54:02,840
average I mean the minimum and maximum

00:53:59,750 --> 00:54:06,560
values of that graph okay the average is

00:54:02,840 --> 00:54:08,360
misleading normally but the maximum

00:54:06,560 --> 00:54:10,460
minimum values without actually tell you

00:54:08,360 --> 00:54:15,460
a lot about the scale of a graph without

00:54:10,460 --> 00:54:19,300
the important so put them though now

00:54:15,460 --> 00:54:22,400
just a quick summary of metrics design

00:54:19,300 --> 00:54:23,750
the most important thing to do when

00:54:22,400 --> 00:54:25,970
you're designing a metric is to choose

00:54:23,750 --> 00:54:28,100
the aggregates wisely to choose them

00:54:25,970 --> 00:54:29,750
according to the type of metric that you

00:54:28,100 --> 00:54:31,630
have whether it's timer whether it's

00:54:29,750 --> 00:54:35,030
gauge whether it's counter and so on and

00:54:31,630 --> 00:54:38,930
to decide on the resolution properly ok

00:54:35,030 --> 00:54:41,720
the sampling rate in the time windows by

00:54:38,930 --> 00:54:43,430
the way if you have incompatible time

00:54:41,720 --> 00:54:46,040
windows for example you're sampling

00:54:43,430 --> 00:54:47,750
every one minute but your aggregate or

00:54:46,040 --> 00:54:50,960
works in let's say two minute intervals

00:54:47,750 --> 00:54:53,210
you would get weird results if your

00:54:50,960 --> 00:54:55,070
aggregator works in high resolution

00:54:53,210 --> 00:54:56,750
let's say ten minutes in ten seconds and

00:54:55,070 --> 00:54:58,940
you're sampling at one minute wait

00:54:56,750 --> 00:55:01,460
you're going to get holes in the graph

00:54:58,940 --> 00:55:03,710
and a lot of artifacts so try to make

00:55:01,460 --> 00:55:05,840
sure that you know time windows are

00:55:03,710 --> 00:55:09,110
consistent with each other explore the

00:55:05,840 --> 00:55:12,350
distribution capture some load data say

00:55:09,110 --> 00:55:13,940
an hour a day or whatever number you can

00:55:12,350 --> 00:55:15,590
store in the elastic structure whatever

00:55:13,940 --> 00:55:17,570
and explore the distribution with

00:55:15,590 --> 00:55:19,610
histograms it's going to tell you a lot

00:55:17,570 --> 00:55:22,400
about what kind of aggregates you want

00:55:19,610 --> 00:55:26,260
to use do I want 99 percentile do a one

00:55:22,400 --> 00:55:28,460
99.9 percentile maybe I want the bottom

00:55:26,260 --> 00:55:30,680
percentiles like one percent on and so

00:55:28,460 --> 00:55:34,210
on it's very hard to know that without

00:55:30,680 --> 00:55:36,980
exploring the distribution visually

00:55:34,210 --> 00:55:39,170
one thing you will also discover for the

00:55:36,980 --> 00:55:41,690
distribution how many modes do you have

00:55:39,170 --> 00:55:44,300
is my distribution by model tree model

00:55:41,690 --> 00:55:47,960
and so on and that will tell you a lot

00:55:44,300 --> 00:55:50,060
about how many metrics you need how many

00:55:47,960 --> 00:55:55,160
is what kind of separations of metrics

00:55:50,060 --> 00:55:57,560
do I need we're not going to go over

00:55:55,160 --> 00:55:59,000
that but basically when you're working

00:55:57,560 --> 00:56:00,850
on metrics and graphite you've got a

00:55:59,000 --> 00:56:02,810
bunch of functions that allow you to

00:56:00,850 --> 00:56:05,240
separate the signal from noise for

00:56:02,810 --> 00:56:07,280
example moving average is type of low

00:56:05,240 --> 00:56:11,090
pass filter that allows you to smooth

00:56:07,280 --> 00:56:14,020
just going to show that and so you'll

00:56:11,090 --> 00:56:14,020
see the power of that

00:56:36,490 --> 00:56:44,640
so this is the high resolution gauge it

00:56:38,200 --> 00:56:44,640
received four and i'm going to use

00:56:52,310 --> 00:56:55,060
yes

00:56:55,510 --> 00:57:12,260
does anybody see ya problem is no it's I

00:57:08,390 --> 00:57:13,990
don't forget to filter but maybe yeah

00:57:12,260 --> 00:57:20,270
moving average I'll write it to photo

00:57:13,990 --> 00:57:21,890
nice thanks so let's do this and turn

00:57:20,270 --> 00:57:24,980
off everything else so we actually see

00:57:21,890 --> 00:57:27,290
something okay so this is the metric and

00:57:24,980 --> 00:57:29,030
we're going to do moving average I'm

00:57:27,290 --> 00:57:37,520
going to Chris the moving average you

00:57:29,030 --> 00:57:39,740
will see the metric change okay let's

00:57:37,520 --> 00:57:42,320
say quite a big nose and zoom into that

00:57:39,740 --> 00:57:45,020
window okay so basically this moves out

00:57:42,320 --> 00:57:46,880
all the peaks okay so let's take it to

00:57:45,020 --> 00:57:52,490
low resolution this was the would you

00:57:46,880 --> 00:57:56,510
learn a trick that's smoothing and more

00:57:52,490 --> 00:57:59,660
smoothing okay it basically removes all

00:57:56,510 --> 00:58:01,460
the noise just leaves the slow changes

00:57:59,660 --> 00:58:04,150
so to speak sit remove the higher

00:58:01,460 --> 00:58:08,150
frequencies and remove and leaves the

00:58:04,150 --> 00:58:09,710
slow changes basically just one problem

00:58:08,150 --> 00:58:12,230
you need to know about we moving average

00:58:09,710 --> 00:58:16,010
it has a tendency to shift the pics

00:58:12,230 --> 00:58:17,860
slightly but it doesn't actually remove

00:58:16,010 --> 00:58:21,440
the phenomena and it does help you show

00:58:17,860 --> 00:58:24,940
doesn't help you see the slow changes in

00:58:21,440 --> 00:58:27,200
the graph okay so a very useful too I

00:58:24,940 --> 00:58:29,510
would recommend that you go and read

00:58:27,200 --> 00:58:33,550
about the graphic matrix so very useful

00:58:29,510 --> 00:58:36,050
a lot of the other metric services have

00:58:33,550 --> 00:58:42,170
no functions that you can use in order

00:58:36,050 --> 00:58:45,380
to do efficient data analysis okay so

00:58:42,170 --> 00:58:49,400
thank you very much if you've got any

00:58:45,380 --> 00:58:51,890
questions that would be awesome or not

00:58:49,400 --> 00:58:55,750
don't ask me about that picture cuz I

00:58:51,890 --> 00:58:55,750
have no idea I don't think anybody does

00:58:56,790 --> 00:59:03,820
so do you have any questions here all

00:59:02,560 --> 00:59:07,740
right then I would say thank you very

00:59:03,820 --> 00:59:07,740

YouTube URL: https://www.youtube.com/watch?v=1J82b79KHGg


