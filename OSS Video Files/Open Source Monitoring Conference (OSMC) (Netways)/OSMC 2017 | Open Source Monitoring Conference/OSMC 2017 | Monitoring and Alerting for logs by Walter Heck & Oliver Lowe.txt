Title: OSMC 2017 | Monitoring and Alerting for logs by Walter Heck & Oliver Lowe
Publication date: 2017-12-05
Playlist: OSMC 2017 | Open Source Monitoring Conference
Description: 
	Many of us are using elastic stack with logstash as a way to gather logs in a central place and parse them into understandable information. Throw on Kibana for root cause analysis and Grafana for beautiful dashboards and the picture is almost complete. But there has been one thing missing: monitoring logs for issues and taking action on them in icinga. This has recently been made possible by the logstash output for icinga (https://github.com/Icinga/logstash-output-icinga). This not only allows us to raise alerts, it also allows us to do things like schedule downtimes and add comments to hosts. In this session weâ€™ll explore the possibilities brought on by this new logstash output and show you some examples of what you can do with it.
Captions: 
	00:00:10,390 --> 00:00:16,200
hello everybody I hope everybody is wide

00:00:13,510 --> 00:00:19,150
awake in this first talk of the day I

00:00:16,200 --> 00:00:21,580
have to introduce now Walter heck from

00:00:19,150 --> 00:00:24,490
Olin datas the technical director and

00:00:21,580 --> 00:00:30,119
his sidekick Oliver low for today's

00:00:24,490 --> 00:00:33,730
first talk which is monitoring and and

00:00:30,119 --> 00:00:45,190
alerting for looks and I guess we start

00:00:33,730 --> 00:00:52,600
now so have fun good morning everyone

00:00:45,190 --> 00:00:56,379
who's drunk nobody that's very decent so

00:00:52,600 --> 00:00:58,600
good morning we are here Oliver and

00:00:56,379 --> 00:01:01,239
myself we are going to talk to you a

00:00:58,600 --> 00:01:03,519
little bit about monitoring your your

00:01:01,239 --> 00:01:04,839
logs burn show that yesterday in the

00:01:03,519 --> 00:01:07,810
road map I was in there because I was

00:01:04,839 --> 00:01:09,790
working on slides but burn showed

00:01:07,810 --> 00:01:12,700
yesterday a quick demo as far as I

00:01:09,790 --> 00:01:14,980
understood and today we'll dive a little

00:01:12,700 --> 00:01:18,700
bit more more in depth on what this

00:01:14,980 --> 00:01:20,970
feature is and how we can use it first

00:01:18,700 --> 00:01:24,100
off my name is Walter and this is Oliver

00:01:20,970 --> 00:01:27,940
and we work for a company called Olin

00:01:24,100 --> 00:01:29,770
data the stuff that we'll be talking

00:01:27,940 --> 00:01:31,960
about will will discuss a little bit

00:01:29,770 --> 00:01:35,970
about what is exactly elasticsearch log

00:01:31,960 --> 00:01:39,100
file beet and how do they fit together

00:01:35,970 --> 00:01:40,870
and then we'll dive in mostly on the on

00:01:39,100 --> 00:01:42,580
the specifically the log stash output

00:01:40,870 --> 00:01:44,320
for for icinga so we'll talk a little

00:01:42,580 --> 00:01:47,170
bit about how to how does the how do

00:01:44,320 --> 00:01:50,340
these outputs work and why would I want

00:01:47,170 --> 00:01:52,090
to hook it up to to icinga and then

00:01:50,340 --> 00:01:54,479
we'll show you a little bit of an

00:01:52,090 --> 00:01:59,979
example of the things you can do and

00:01:54,479 --> 00:02:02,680
Oliver will do a live demo so it should

00:01:59,979 --> 00:02:06,520
be should be good

00:02:02,680 --> 00:02:10,409
so first up elastic who here is not

00:02:06,520 --> 00:02:12,760
using any kind of centralized logging

00:02:10,409 --> 00:02:17,550
you should not be here you should go and

00:02:12,760 --> 00:02:22,959
set up logging all right

00:02:17,550 --> 00:02:24,220
coincidental but yeah so in a place

00:02:22,959 --> 00:02:25,660
where you're doing monitoring

00:02:24,220 --> 00:02:28,930
which should be a first-class citizen in

00:02:25,660 --> 00:02:30,670
a modern infrastructure and just as well

00:02:28,930 --> 00:02:33,700
centralized logging should also be a

00:02:30,670 --> 00:02:35,530
first-class citizen it's more important

00:02:33,700 --> 00:02:37,900
than ever to be able to see exactly what

00:02:35,530 --> 00:02:40,150
is going on and to do things like root

00:02:37,900 --> 00:02:43,420
root cause analysis and just a whole

00:02:40,150 --> 00:02:45,910
bunch of other stuff elastic search is

00:02:43,420 --> 00:02:48,430
the official product name elastic is the

00:02:45,910 --> 00:02:51,850
company and then they created this stack

00:02:48,430 --> 00:02:53,830
called the elastic stack so it's been a

00:02:51,850 --> 00:02:56,709
bit of a confusing and naming thing but

00:02:53,830 --> 00:03:00,250
elastic search is the basically it's a

00:02:56,709 --> 00:03:03,670
full-text search engine it's open source

00:03:00,250 --> 00:03:06,010
it has a enterprise backing from from

00:03:03,670 --> 00:03:08,170
elastic so if you want to buy support if

00:03:06,010 --> 00:03:12,160
that makes your bosses feel better then

00:03:08,170 --> 00:03:14,880
you can totally do that it is used for a

00:03:12,160 --> 00:03:17,590
whole bunch of things and one of them is

00:03:14,880 --> 00:03:19,480
centralized logging so actually it's

00:03:17,590 --> 00:03:24,340
it's a full-text search engine based on

00:03:19,480 --> 00:03:26,070
a technology called Lucene and yeah in

00:03:24,340 --> 00:03:29,050
the sysadmin

00:03:26,070 --> 00:03:30,700
operations field it's used usually for

00:03:29,050 --> 00:03:33,940
centralized log but you could just as

00:03:30,700 --> 00:03:40,269
well use it to power search for a for a

00:03:33,940 --> 00:03:41,650
website yeah as I said monitoring sorry

00:03:40,269 --> 00:03:43,440
a centralized logging should be a

00:03:41,650 --> 00:03:45,580
first-class citizen so you can have

00:03:43,440 --> 00:03:52,000
correlation between your logs and your

00:03:45,580 --> 00:03:54,060
monitoring data now to get dated elastic

00:03:52,000 --> 00:03:57,700
search is basically used as a storage

00:03:54,060 --> 00:04:00,250
engine so all we do is send text to it

00:03:57,700 --> 00:04:02,230
and elastic search will index it for us

00:04:00,250 --> 00:04:04,510
and store it so that we can later search

00:04:02,230 --> 00:04:08,260
through it but that's basically all that

00:04:04,510 --> 00:04:10,989
elastic elastic search is now how do we

00:04:08,260 --> 00:04:15,370
get our data into elastic search one of

00:04:10,989 --> 00:04:18,970
the ways that elastic provides us is a

00:04:15,370 --> 00:04:21,820
there's a little demon called file beat

00:04:18,970 --> 00:04:25,690
file beat runs on each server that you

00:04:21,820 --> 00:04:29,260
want to monitor logs from and basically

00:04:25,690 --> 00:04:32,630
it just it is tails a bunch of files in

00:04:29,260 --> 00:04:35,580
a fancy way and sends the

00:04:32,630 --> 00:04:38,490
events that it picks up to a defined

00:04:35,580 --> 00:04:43,669
output so here we have for instance a

00:04:38,490 --> 00:04:45,479
prospector that's that's the unit for

00:04:43,669 --> 00:04:47,820
things that you want to monitor

00:04:45,479 --> 00:04:50,190
so this prospector monitors vara log

00:04:47,820 --> 00:04:53,990
backup that log and whenever new lines

00:04:50,190 --> 00:04:57,660
appear in it it'll send those lines to

00:04:53,990 --> 00:05:00,630
to the log stash output you can do some

00:04:57,660 --> 00:05:04,560
kind of some basic processing already on

00:05:00,630 --> 00:05:08,040
the file beat end but in my personal

00:05:04,560 --> 00:05:09,840
opinion it's a it's best to do to make

00:05:08,040 --> 00:05:12,870
sure that file beat understands

00:05:09,840 --> 00:05:16,110
individual events and then send them off

00:05:12,870 --> 00:05:19,290
to a log stash for for processing and

00:05:16,110 --> 00:05:21,330
that means that most log files are just

00:05:19,290 --> 00:05:23,430
single lines but for instance if you're

00:05:21,330 --> 00:05:25,380
if you're running any Java apps you have

00:05:23,430 --> 00:05:28,860
to use beautiful stack traces that are

00:05:25,380 --> 00:05:31,020
200 lines of garbage that nobody wants

00:05:28,860 --> 00:05:35,910
to look at but those two hundred lines

00:05:31,020 --> 00:05:38,630
of stack trace they are actually one log

00:05:35,910 --> 00:05:43,410
event so to speak so foul bait also

00:05:38,630 --> 00:05:46,039
supports multi-line log formats so you

00:05:43,410 --> 00:05:48,419
can you you can do a little bit of a

00:05:46,039 --> 00:05:52,380
pre-processing on the on the server

00:05:48,419 --> 00:05:56,430
itself the majority however you want to

00:05:52,380 --> 00:05:59,669
do on the log stash end so log stash is

00:05:56,430 --> 00:06:04,889
a daemon it's quite heavy runs a in a

00:05:59,669 --> 00:06:07,229
JVM it takes events from inputs then

00:06:04,889 --> 00:06:13,729
filters them and then sends those events

00:06:07,229 --> 00:06:16,650
to outputs it can be one or more inputs

00:06:13,729 --> 00:06:21,450
the filters can be as many as you want a

00:06:16,650 --> 00:06:26,910
decent log setup will have easily dozens

00:06:21,450 --> 00:06:29,880
if not hundreds of filters present and

00:06:26,910 --> 00:06:33,990
then the number of outputs can be one or

00:06:29,880 --> 00:06:35,849
more whatever you so desire we'll see

00:06:33,990 --> 00:06:37,889
that with the icing your output the way

00:06:35,849 --> 00:06:43,580
it's currently built you'll easily end

00:06:37,889 --> 00:06:45,990
up with multiple outputs for icing alone

00:06:43,580 --> 00:06:48,600
and basically what

00:06:45,990 --> 00:06:51,330
what logstash does is it takes whatever

00:06:48,600 --> 00:06:53,180
is coming in as events then pulls them

00:06:51,330 --> 00:06:56,970
through some kind of filter that can

00:06:53,180 --> 00:06:59,940
correct fields discard some data add

00:06:56,970 --> 00:07:03,470
some data a whole bunch of different

00:06:59,940 --> 00:07:05,730
things and then send them to outputs

00:07:03,470 --> 00:07:08,640
what makes logstash

00:07:05,730 --> 00:07:10,950
so powerful is the fact that it works

00:07:08,640 --> 00:07:13,340
with plugins and they're fairly simple

00:07:10,950 --> 00:07:16,260
there are dozens and dozens available

00:07:13,340 --> 00:07:19,560
and they are both they are working on

00:07:16,260 --> 00:07:22,740
all three levels so you have input

00:07:19,560 --> 00:07:28,830
plugins filters and output plugins and

00:07:22,740 --> 00:07:31,380
that makes it really quite nice so some

00:07:28,830 --> 00:07:34,800
example input plugins are for instance

00:07:31,380 --> 00:07:38,820
file date you can read data from an AWS

00:07:34,800 --> 00:07:42,870
cloud watch if you want to very common

00:07:38,820 --> 00:07:46,020
is a TCP and a syslog input you can even

00:07:42,870 --> 00:07:47,940
do UDP there are probably about 20 or 30

00:07:46,020 --> 00:07:52,230
of them so if you want to see all of

00:07:47,940 --> 00:07:56,760
them take a look at the at the elastic

00:07:52,230 --> 00:08:00,440
website filter plugins they manipulate

00:07:56,760 --> 00:08:03,150
data so this can be anything from

00:08:00,440 --> 00:08:05,160
there's a adjacent filter which we'll

00:08:03,150 --> 00:08:07,590
see a little bit later which basically

00:08:05,160 --> 00:08:09,810
takes an event that was Jason and then

00:08:07,590 --> 00:08:11,960
splits it up into fields that are the

00:08:09,810 --> 00:08:16,920
actual fields defined in the Jason

00:08:11,960 --> 00:08:18,600
message geoip can can look up the

00:08:16,920 --> 00:08:20,910
location of an IP address and add that

00:08:18,600 --> 00:08:24,930
as a field so if you have for instance

00:08:20,910 --> 00:08:27,930
your Apache logs that have the source IP

00:08:24,930 --> 00:08:33,510
addresses of each request then geoip can

00:08:27,930 --> 00:08:38,330
add the actual geolocation data to it

00:08:33,510 --> 00:08:41,610
grok is is quite nice grok allows you to

00:08:38,330 --> 00:08:43,560
to specify a basically a regular

00:08:41,610 --> 00:08:46,530
expression it's a bit more fancy but

00:08:43,560 --> 00:08:49,920
basically a regular expression of what a

00:08:46,530 --> 00:08:51,720
message consists of and then if it

00:08:49,920 --> 00:08:54,480
matches that regular expression it will

00:08:51,720 --> 00:08:57,570
be cut up into the specific fields it's

00:08:54,480 --> 00:08:58,960
very useful for for instance syslog

00:08:57,570 --> 00:09:01,690
messages

00:08:58,960 --> 00:09:03,610
that there are many many different kinds

00:09:01,690 --> 00:09:09,670
of messages that could be sitting in a

00:09:03,610 --> 00:09:11,680
syslog that be entering a syslog but

00:09:09,670 --> 00:09:15,850
you'll usually have at least a handful

00:09:11,680 --> 00:09:17,770
of patterns that that are matching so

00:09:15,850 --> 00:09:21,220
for instance let's say that you have a

00:09:17,770 --> 00:09:25,030
Pam messages they usually have a

00:09:21,220 --> 00:09:31,060
specific form format however it is free

00:09:25,030 --> 00:09:33,430
text so it's text based out of with a

00:09:31,060 --> 00:09:36,730
specific format and then in in in a grog

00:09:33,430 --> 00:09:40,000
filter you can say okay the first word

00:09:36,730 --> 00:09:43,240
of the message is the demon that sent a

00:09:40,000 --> 00:09:45,820
sent the the message then the second

00:09:43,240 --> 00:09:50,830
part is the actual message the third

00:09:45,820 --> 00:09:56,650
part is an IP address etc etc yeah

00:09:50,830 --> 00:09:59,710
listen Jason twice there is also a ruby

00:09:56,650 --> 00:10:02,500
filter plug-in which you'll see later I

00:09:59,710 --> 00:10:04,660
think it's very very ugly but we abused

00:10:02,500 --> 00:10:05,950
it for this demo because it was quick

00:10:04,660 --> 00:10:08,520
and simple but I wouldn't actually

00:10:05,950 --> 00:10:10,750
recommend doing that in a production

00:10:08,520 --> 00:10:17,080
level setup because you have to realize

00:10:10,750 --> 00:10:19,840
that if a message hits a filter every

00:10:17,080 --> 00:10:21,820
message that hits that filter will will

00:10:19,840 --> 00:10:23,800
receive will go through the processing

00:10:21,820 --> 00:10:25,570
so you need to be a bit careful about

00:10:23,800 --> 00:10:31,600
what you're actually how you're actually

00:10:25,570 --> 00:10:34,510
determining which filter to use and an

00:10:31,600 --> 00:10:35,640
output plugins where do I send data you

00:10:34,510 --> 00:10:37,900
can send to a whole bunch of different

00:10:35,640 --> 00:10:41,740
different things cloud watch data dog

00:10:37,900 --> 00:10:44,250
elasticsearch stats d very common in the

00:10:41,740 --> 00:10:52,530
elk stack is to have a elasticsearch

00:10:44,250 --> 00:10:58,930
output and today we'll show the icing up

00:10:52,530 --> 00:11:03,490
the icing output as well so luxury

00:10:58,930 --> 00:11:05,350
single output came earlier this year I I

00:11:03,490 --> 00:11:08,200
requested it at the same time that it

00:11:05,350 --> 00:11:09,970
was just built so probably certain minds

00:11:08,200 --> 00:11:12,850
worked in the same way and thought hey

00:11:09,970 --> 00:11:16,859
wouldn't be nice if

00:11:12,850 --> 00:11:20,399
so basically what it allows us to do is

00:11:16,859 --> 00:11:23,609
do things in icing uh through the API

00:11:20,399 --> 00:11:26,609
based on messages that are coming into

00:11:23,609 --> 00:11:26,609
logstash

00:11:28,319 --> 00:11:35,160
there are a number of actions that we

00:11:30,999 --> 00:11:37,539
can we can take but before we go there

00:11:35,160 --> 00:11:40,600
so this is what it will looks like so we

00:11:37,539 --> 00:11:43,209
have each server in our network runs

00:11:40,600 --> 00:11:47,169
file beat a sing a beat whatever other

00:11:43,209 --> 00:11:51,609
ways to gather data it ships those logs

00:11:47,169 --> 00:11:55,979
to log stash log filters and processes

00:11:51,609 --> 00:11:58,720
the events and sends them based on

00:11:55,979 --> 00:12:00,999
filters it sends them to filters is the

00:11:58,720 --> 00:12:03,910
wrong word but based on on conditionals

00:12:00,999 --> 00:12:06,249
it sends them to I sing if there's for

00:12:03,910 --> 00:12:10,470
instance a problem or it sends them to

00:12:06,249 --> 00:12:13,289
elasticsearch for simple storage

00:12:10,470 --> 00:12:19,259
generally you want to send almost every

00:12:13,289 --> 00:12:22,899
message to elasticsearch and only few to

00:12:19,259 --> 00:12:24,999
two icinga so the I think I should only

00:12:22,899 --> 00:12:27,699
receive the messages that are actually

00:12:24,999 --> 00:12:30,329
relevant to it and that's done with the

00:12:27,699 --> 00:12:33,519
conditionals on the log stash level

00:12:30,329 --> 00:12:39,100
which you'll see in the demo a little

00:12:33,519 --> 00:12:41,019
bit later where could we use this one of

00:12:39,100 --> 00:12:43,239
the scenarios is for instance monitoring

00:12:41,019 --> 00:12:46,509
a backup which is what what Oliver will

00:12:43,239 --> 00:12:48,429
will show you where the moment a the

00:12:46,509 --> 00:12:50,709
message is received that a backup has

00:12:48,429 --> 00:12:53,379
started so your backup script can can

00:12:50,709 --> 00:12:55,659
output a message to syslog saying backup

00:12:53,379 --> 00:12:57,789
was started and on the log stash level

00:12:55,659 --> 00:13:00,819
you catch that message and send a

00:12:57,789 --> 00:13:04,859
downtime - - icing it to set that host

00:13:00,819 --> 00:13:11,350
to do not alert for the duration of the

00:13:04,859 --> 00:13:16,439
of the backup other use cases that we

00:13:11,350 --> 00:13:19,389
that we discussed our a big use case is

00:13:16,439 --> 00:13:21,939
devices that we don't have the ability

00:13:19,389 --> 00:13:25,209
to install monitoring on but they do

00:13:21,939 --> 00:13:26,470
have the ability to send messages to

00:13:25,209 --> 00:13:28,360
syslog

00:13:26,470 --> 00:13:29,980
for instance switches and all that kind

00:13:28,360 --> 00:13:32,380
of stuff

00:13:29,980 --> 00:13:34,120
printers they can send their messages to

00:13:32,380 --> 00:13:37,920
syslog but we cannot actually install

00:13:34,120 --> 00:13:41,860
and I saying agent on them so allowing

00:13:37,920 --> 00:13:42,310
the or picking up the events from from

00:13:41,860 --> 00:13:48,240
logstash

00:13:42,310 --> 00:13:48,240
is a is an interesting way to to do this

00:13:48,660 --> 00:13:54,220
the demo that will be giving later and

00:13:51,280 --> 00:13:56,620
this is a bit of a lost message here but

00:13:54,220 --> 00:13:56,860
I didn't really have a better way to put

00:13:56,620 --> 00:14:01,200
it

00:13:56,860 --> 00:14:05,980
there is a icing a vagrant project that

00:14:01,200 --> 00:14:11,170
has a bunch of vagrant boxes that set up

00:14:05,980 --> 00:14:15,970
simple icing scenarios for you and we

00:14:11,170 --> 00:14:19,930
used the icing at 2x elastic vagrant box

00:14:15,970 --> 00:14:22,030
for it it sets up by default already a a

00:14:19,930 --> 00:14:24,670
file beeped and an elastic search as

00:14:22,030 --> 00:14:27,430
well as an icing server and I just put

00:14:24,670 --> 00:14:30,340
in a merge request for adding log stash

00:14:27,430 --> 00:14:32,970
to that as well because I needed to do

00:14:30,340 --> 00:14:32,970
that anyway

00:14:33,160 --> 00:14:43,890
so actions we can take from the from the

00:14:39,400 --> 00:14:47,590
outputs maybe we look at a output first

00:14:43,890 --> 00:14:49,950
yeah so here is what it what it looks

00:14:47,590 --> 00:14:54,610
like a log stash configuration has a

00:14:49,950 --> 00:14:57,430
input section a filter section and an

00:14:54,610 --> 00:15:02,440
output section and as you see here our

00:14:57,430 --> 00:15:05,560
input as file beat or is the beats we're

00:15:02,440 --> 00:15:11,020
running on onboard 5044 so this makes

00:15:05,560 --> 00:15:13,930
log stash listen on port 5044 all the

00:15:11,020 --> 00:15:15,910
messages that come in will be run

00:15:13,930 --> 00:15:18,630
through these filters and this is what i

00:15:15,910 --> 00:15:21,520
said that you have to be careful that

00:15:18,630 --> 00:15:23,680
you shield off the filters that will

00:15:21,520 --> 00:15:27,910
actually be run with the correct

00:15:23,680 --> 00:15:29,980
conditionals so it means that this

00:15:27,910 --> 00:15:32,470
mutate will only run if there's a field

00:15:29,980 --> 00:15:36,190
called syslog severity and it's set to

00:15:32,470 --> 00:15:38,320
the value error in a production log

00:15:36,190 --> 00:15:40,140
stash setup you want to make sure that

00:15:38,320 --> 00:15:42,360
you think carefully about the

00:15:40,140 --> 00:15:43,890
order of in which you set your your

00:15:42,360 --> 00:15:46,980
filters because because it can make a

00:15:43,890 --> 00:15:52,440
big impact on how fast logs ash can

00:15:46,980 --> 00:15:54,870
process events we have here a sample

00:15:52,440 --> 00:15:57,690
filter or a conditional saying if the

00:15:54,870 --> 00:16:00,200
source of the of the message is actually

00:15:57,690 --> 00:16:03,830
far log my precious log Jason then

00:16:00,200 --> 00:16:07,260
filtered the message filled in in that

00:16:03,830 --> 00:16:09,510
in the event that came in with the Jason

00:16:07,260 --> 00:16:14,340
filter which may basically cuts up all

00:16:09,510 --> 00:16:17,670
of the adjacent fields into a log into

00:16:14,340 --> 00:16:20,280
fields and then the meat of course is

00:16:17,670 --> 00:16:21,690
here in the output standard out this is

00:16:20,280 --> 00:16:23,880
not something I would recommend in a

00:16:21,690 --> 00:16:26,850
production setup because you will wipe

00:16:23,880 --> 00:16:28,290
all of your messages to send it out but

00:16:26,850 --> 00:16:29,700
it's a it's really great if you're

00:16:28,290 --> 00:16:32,640
trying to figure out what the hell's

00:16:29,700 --> 00:16:35,220
happening and what is arriving in in a

00:16:32,640 --> 00:16:37,790
in log stash

00:16:35,220 --> 00:16:40,740
we have here a very simple elasticsearch

00:16:37,790 --> 00:16:44,610
output that just sends every message

00:16:40,740 --> 00:16:48,210
that comes in to do the elasticsearch

00:16:44,610 --> 00:16:50,400
daemon that's running locally and then

00:16:48,210 --> 00:16:51,690
here we have our I single output which

00:16:50,400 --> 00:16:54,200
is what we're here for

00:16:51,690 --> 00:16:57,270
we said a host a username and a password

00:16:54,200 --> 00:16:59,880
we don't care about any security of

00:16:57,270 --> 00:17:04,230
course so we just turn off SSL

00:16:59,880 --> 00:17:09,750
verification and then here we have the

00:17:04,230 --> 00:17:11,910
action to take in this case we're

00:17:09,750 --> 00:17:13,980
processing a check result and then we

00:17:11,910 --> 00:17:16,410
have one or more fields that determine

00:17:13,980 --> 00:17:19,199
the parameters to the to the action that

00:17:16,410 --> 00:17:20,520
we want to take and basically oh sorry

00:17:19,199 --> 00:17:23,610
and then here we have the host and the

00:17:20,520 --> 00:17:27,690
service that we want to work with so

00:17:23,610 --> 00:17:32,630
basically this is the basis for our API

00:17:27,690 --> 00:17:32,630
call that gets sent to this icing a host

00:17:33,020 --> 00:17:42,240
the the only thing is that here these

00:17:36,930 --> 00:17:43,460
kind of this kind of syntax determines a

00:17:42,240 --> 00:17:46,080
[Music]

00:17:43,460 --> 00:17:48,450
the field that comes from the message

00:17:46,080 --> 00:17:51,450
that that is coming into logstash so if

00:17:48,450 --> 00:17:53,400
the logstash message has a exit status

00:17:51,450 --> 00:17:55,890
field in it

00:17:53,400 --> 00:17:59,010
then that will be used as the parameter

00:17:55,890 --> 00:18:04,320
to process check result this is

00:17:59,010 --> 00:18:06,600
obviously not a production level set up

00:18:04,320 --> 00:18:08,430
neither because this will mean because

00:18:06,600 --> 00:18:10,440
we don't have an if statement like like

00:18:08,430 --> 00:18:12,690
one of these in front of the output

00:18:10,440 --> 00:18:14,100
icing output this will mean that every

00:18:12,690 --> 00:18:18,660
single message that comes into logstash

00:18:14,100 --> 00:18:22,500
will be sent to icing API which will not

00:18:18,660 --> 00:18:25,410
make anyone happy let's look a little

00:18:22,500 --> 00:18:29,280
bit about a little bit to the two

00:18:25,410 --> 00:18:32,100
different actions which is here first

00:18:29,280 --> 00:18:33,810
one is processor check result you can

00:18:32,100 --> 00:18:38,480
just manipulate the state the state of

00:18:33,810 --> 00:18:42,170
check some examples are for instance a

00:18:38,480 --> 00:18:48,000
dummy catch-all service and that just

00:18:42,170 --> 00:18:50,700
has an an output defined for it that if

00:18:48,000 --> 00:18:54,330
we see the word error in a message we

00:18:50,700 --> 00:18:57,210
send a we change the state the state of

00:18:54,330 --> 00:18:59,220
that check to critical so you can go and

00:18:57,210 --> 00:19:02,070
investigate hey I found some kind of

00:18:59,220 --> 00:19:05,130
error message in the logs and then as

00:19:02,070 --> 00:19:08,160
you go along and you identify the

00:19:05,130 --> 00:19:10,890
sources of those error messages you can

00:19:08,160 --> 00:19:13,310
exclude them in in your log stash

00:19:10,890 --> 00:19:16,020
configuration from actually raising this

00:19:13,310 --> 00:19:18,870
this output and that way you can start

00:19:16,020 --> 00:19:26,090
finding errors that you would maybe

00:19:18,870 --> 00:19:31,590
otherwise not find a backup service

00:19:26,090 --> 00:19:34,050
which can can go to an okay status after

00:19:31,590 --> 00:19:35,070
you finish a backup you can at the start

00:19:34,050 --> 00:19:37,710
of the backup we were discussing

00:19:35,070 --> 00:19:39,750
yesterday you can either set it to two

00:19:37,710 --> 00:19:41,700
critical and put a downtime on the hosts

00:19:39,750 --> 00:19:44,050
that it doesn't actually raise an alert

00:19:41,700 --> 00:19:47,860
or you can

00:19:44,050 --> 00:19:49,710
set it to unknown because you don't at

00:19:47,860 --> 00:19:52,870
that moment you don't want to raise in a

00:19:49,710 --> 00:19:58,740
an alert yet but the downtime of the

00:19:52,870 --> 00:20:01,150
host needs a final time so you have a

00:19:58,740 --> 00:20:03,340
down time that you set at the start of

00:20:01,150 --> 00:20:05,770
your of your backup of let's say two

00:20:03,340 --> 00:20:07,630
hours and then if after two hours the

00:20:05,770 --> 00:20:09,340
downtime finishes but the backup hasn't

00:20:07,630 --> 00:20:14,590
finished for whatever reason you will

00:20:09,340 --> 00:20:16,320
receive an alert saying hey we we still

00:20:14,590 --> 00:20:21,150
have a problem because the backup hasn't

00:20:16,320 --> 00:20:21,150
sent the OK and that it was finished

00:20:22,740 --> 00:20:28,360
send a custom notification fairly

00:20:25,870 --> 00:20:34,060
straightforward just requires an author

00:20:28,360 --> 00:20:35,410
and a comment you can force it but that

00:20:34,060 --> 00:20:36,940
would probably not make too many people

00:20:35,410 --> 00:20:42,190
happy unless it's a really really really

00:20:36,940 --> 00:20:44,850
really important message it's fairly

00:20:42,190 --> 00:20:50,470
straightforward adding and removing a

00:20:44,850 --> 00:20:53,520
comment is interesting it adds a comment

00:20:50,470 --> 00:20:55,990
to either a service or a host and the

00:20:53,520 --> 00:20:58,150
the issue is that removing the comment

00:20:55,990 --> 00:21:01,390
is normally done with the identifier

00:20:58,150 --> 00:21:04,150
that is returned by the API call that

00:21:01,390 --> 00:21:07,210
adds the comment however we don't have

00:21:04,150 --> 00:21:10,410
access to that because the the logstash

00:21:07,210 --> 00:21:13,870
is stateless so it doesn't know what the

00:21:10,410 --> 00:21:15,570
identifier the the comment was added

00:21:13,870 --> 00:21:19,450
with so the only thing we can do is

00:21:15,570 --> 00:21:22,960
remove all comments by that author from

00:21:19,450 --> 00:21:25,570
that service so it works a bit weird I

00:21:22,960 --> 00:21:29,770
wouldn't personally remove comments

00:21:25,570 --> 00:21:32,770
automatically by from a monitoring

00:21:29,770 --> 00:21:38,410
system so I probably just go and only

00:21:32,770 --> 00:21:42,310
use it for adding comments similarly we

00:21:38,410 --> 00:21:46,180
have scheduled and remove downtime where

00:21:42,310 --> 00:21:48,700
we can you know add and remove a

00:21:46,180 --> 00:21:51,250
downtime so as you see here you have to

00:21:48,700 --> 00:21:55,110
set so these are required the start time

00:21:51,250 --> 00:21:55,110
and end time are required

00:21:56,900 --> 00:22:04,340
so that's a bit not necessarily always

00:22:01,880 --> 00:22:06,170
the way you want to you want to do it I

00:22:04,340 --> 00:22:07,760
would say that if you're starting a

00:22:06,170 --> 00:22:10,070
backup you want to be able to set a down

00:22:07,760 --> 00:22:12,890
time and it has no end time and it will

00:22:10,070 --> 00:22:15,220
be removed the moment you your backup is

00:22:12,890 --> 00:22:15,220
finished

00:22:15,250 --> 00:22:21,110
these are all straightforward mappings

00:22:18,320 --> 00:22:26,360
to the icing API calls so there isn't

00:22:21,110 --> 00:22:27,050
really any magic going on there right

00:22:26,360 --> 00:22:29,800
now

00:22:27,050 --> 00:22:33,050
the you have to download the gem

00:22:29,800 --> 00:22:36,140
directly or is it already is anyone here

00:22:33,050 --> 00:22:38,660
that so it's being added to is that

00:22:36,140 --> 00:22:44,179
already done oh yeah okay so you can

00:22:38,660 --> 00:22:47,570
already do a log stash plug-in install

00:22:44,179 --> 00:22:49,100
without first downloading the gem if

00:22:47,570 --> 00:22:50,840
you're doing this if you happen to be

00:22:49,100 --> 00:22:53,600
doing this because you want to do the

00:22:50,840 --> 00:22:56,300
the latest of the latest or you have

00:22:53,600 --> 00:22:58,600
another reason to do the to download the

00:22:56,300 --> 00:23:01,910
German and install it manually

00:22:58,600 --> 00:23:03,590
the be aware that if you do this in a VM

00:23:01,910 --> 00:23:05,690
this can take a very very long time

00:23:03,590 --> 00:23:07,850
install it this installing the largest

00:23:05,690 --> 00:23:10,929
output I think I think can take a very

00:23:07,850 --> 00:23:15,410
very long time this is not apparently a

00:23:10,929 --> 00:23:16,910
problem of the of this gem specifically

00:23:15,410 --> 00:23:19,550
it has something to do with entropy and

00:23:16,910 --> 00:23:22,340
a virtual machine being low and logstash

00:23:19,550 --> 00:23:24,800
going all crazy over that so I've seen

00:23:22,340 --> 00:23:34,100
it take easily 15 minutes for a gem that

00:23:24,800 --> 00:23:37,130
is 120 K in size in order to use our I

00:23:34,100 --> 00:23:40,309
think output we need at the minimum an

00:23:37,130 --> 00:23:42,140
API user obviously you would set the

00:23:40,309 --> 00:23:44,720
permissions a bit more restrictive based

00:23:42,140 --> 00:23:50,480
on what it is you want the actual API

00:23:44,720 --> 00:23:52,970
use it to to be able to do and in our

00:23:50,480 --> 00:23:56,000
case where we want to do a backup we

00:23:52,970 --> 00:23:58,820
just create a backup service and it

00:23:56,000 --> 00:24:01,550
doesn't really do anything so we can

00:23:58,820 --> 00:24:07,390
manipulate it it's state from the from

00:24:01,550 --> 00:24:07,390
the API this we already looked at

00:24:07,970 --> 00:24:16,060
which means it's demo time which means

00:24:10,250 --> 00:24:16,060
I'll pass it on to Oliver hey hi

00:24:24,090 --> 00:24:28,350
I oh that's good before I was like

00:24:26,580 --> 00:24:32,000
looking at it okay

00:24:28,350 --> 00:24:35,070
so let me get my list of things to do so

00:24:32,000 --> 00:24:39,559
the first thing we want to do is look at

00:24:35,070 --> 00:24:46,620
configuring file beat on the server so

00:24:39,559 --> 00:24:48,539
okay here and we look at Jason logs so

00:24:46,620 --> 00:24:52,919
what we know is that we know that our

00:24:48,539 --> 00:24:54,570
particular script outputs to far log by

00:24:52,919 --> 00:24:59,940
log Jason because everybody logs in

00:24:54,570 --> 00:25:01,679
Jason of course easy to decode so that's

00:24:59,940 --> 00:25:04,020
pretty straightforward basically we just

00:25:01,679 --> 00:25:06,419
you know say the path and then file bit

00:25:04,020 --> 00:25:08,520
goes and schedules to send it off to log

00:25:06,419 --> 00:25:10,380
stash pretty straightforward the next

00:25:08,520 --> 00:25:22,850
thing though is more interesting with

00:25:10,380 --> 00:25:27,480
log stash so if we go to filter so we

00:25:22,850 --> 00:25:29,580
start by saying okay my Jason log is 1

00:25:27,480 --> 00:25:32,820
we specified in file bit we say ok the

00:25:29,580 --> 00:25:37,919
source is just the entire message we

00:25:32,820 --> 00:25:42,720
start looking through the actual output

00:25:37,919 --> 00:25:46,080
of the log so first we need to do

00:25:42,720 --> 00:25:57,120
something like set the syntax so we can

00:25:46,080 --> 00:26:02,970
read it more easily didn't oh yeah what

00:25:57,120 --> 00:26:06,510
is it yeah whatever

00:26:02,970 --> 00:26:10,770
ok so yeah so what we do is we need to

00:26:06,510 --> 00:26:12,029
do some clever things like say ok what's

00:26:10,770 --> 00:26:13,320
the type of thing that we're looking for

00:26:12,029 --> 00:26:16,260
and what are the actions we want to take

00:26:13,320 --> 00:26:18,179
on it so the first thing we want to do

00:26:16,260 --> 00:26:20,700
is we see our code has a backup started

00:26:18,179 --> 00:26:22,860
in the message so we set the start time

00:26:20,700 --> 00:26:25,100
and the end time of the particular thing

00:26:22,860 --> 00:26:29,429
we want to schedule downtime

00:26:25,100 --> 00:26:32,280
so we say ok the start time is now and

00:26:29,429 --> 00:26:37,480
then the end time is going to be 7200

00:26:32,280 --> 00:26:41,290
seconds which is two hours from there we

00:26:37,480 --> 00:26:43,570
set the exit status now we set it to

00:26:41,290 --> 00:26:45,910
three because we say okay the backups

00:26:43,570 --> 00:26:48,250
starting we don't actually know what the

00:26:45,910 --> 00:26:50,919
how it's really going it's not okay

00:26:48,250 --> 00:26:52,000
because we don't really necessarily know

00:26:50,919 --> 00:26:54,880
that because it hasn't finished

00:26:52,000 --> 00:26:56,799
we don't want it warning because what's

00:26:54,880 --> 00:26:59,559
the problem we can't set it to critical

00:26:56,799 --> 00:27:01,360
because well nothing is wrong per se

00:26:59,559 --> 00:27:05,530
it's just running so we set it to

00:27:01,360 --> 00:27:08,110
unknown when the backups finished we

00:27:05,530 --> 00:27:09,640
want to set the exit status to zero so

00:27:08,110 --> 00:27:12,580
the script is going to say are my backup

00:27:09,640 --> 00:27:17,230
is finished so we say okay cool looks

00:27:12,580 --> 00:27:18,850
good let's set it to okay next thing we

00:27:17,230 --> 00:27:22,240
do is actually let's have a look at the

00:27:18,850 --> 00:27:25,330
script that we're running so this is for

00:27:22,240 --> 00:27:28,480
example some script that some DBA wrote

00:27:25,330 --> 00:27:31,410
so which I guess is I guess I should say

00:27:28,480 --> 00:27:34,120
a nice thing about this logstash

00:27:31,410 --> 00:27:35,950
sort of workflow is that say for example

00:27:34,120 --> 00:27:37,840
you have a DBA and they've written this

00:27:35,950 --> 00:27:39,250
thing and you can't you know go and

00:27:37,840 --> 00:27:42,370
change their workflow to make it nice

00:27:39,250 --> 00:27:44,530
and easy to you know say check the

00:27:42,370 --> 00:27:47,169
status of their particular database

00:27:44,530 --> 00:27:48,910
stuff so instead of having to go into

00:27:47,169 --> 00:27:51,250
their code or you know do all this stuff

00:27:48,910 --> 00:27:53,799
you can just say okay well I know

00:27:51,250 --> 00:27:55,330
they're logging in a certain way and I

00:27:53,799 --> 00:27:56,620
can just filter the logs instead of you

00:27:55,330 --> 00:27:57,850
know say testing the application

00:27:56,620 --> 00:27:59,440
database directly because I don't have

00:27:57,850 --> 00:28:00,700
permissions and all this stuff all you

00:27:59,440 --> 00:28:03,910
need is just to be able to read the log

00:28:00,700 --> 00:28:06,580
file anyway so this is our script it

00:28:03,910 --> 00:28:07,630
just puts to the file that we know that

00:28:06,580 --> 00:28:11,460
we're going to be checking that's file

00:28:07,630 --> 00:28:11,460
beats configure to look at this file and

00:28:11,940 --> 00:28:17,620
yeah that's it okay so let's run our

00:28:15,730 --> 00:28:19,720
backup and see how this thing works so

00:28:17,620 --> 00:28:27,070
let me just check that I've got me

00:28:19,720 --> 00:28:29,080
chinga up whoops yeah okay cool

00:28:27,070 --> 00:28:38,010
I'll say one final thing we need to do

00:28:29,080 --> 00:28:38,010
is I'll show you why logs - we need -

00:28:38,549 --> 00:28:44,140
yeah this is the one okay so this is

00:28:42,460 --> 00:28:47,020
what I was saying before we did set the

00:28:44,140 --> 00:28:49,450
downtime in the unknown status so we're

00:28:47,020 --> 00:28:51,460
doing this boring stuff setting you know

00:28:49,450 --> 00:28:56,320
which anger host we're going to

00:28:51,460 --> 00:28:57,670
send our downtime to and all that we

00:28:56,320 --> 00:29:00,570
know we're going to be doing our SQL

00:28:57,670 --> 00:29:03,160
host this service that we're going to

00:29:00,570 --> 00:29:07,720
set the status on or set the downtime of

00:29:03,160 --> 00:29:09,970
is backup we have this start underscore

00:29:07,720 --> 00:29:13,840
time so this is where I set it in the

00:29:09,970 --> 00:29:15,790
log stash filter and time is the same as

00:29:13,840 --> 00:29:19,630
before and as well as I was saying these

00:29:15,790 --> 00:29:23,980
are just the standard API things so no

00:29:19,630 --> 00:29:27,550
magic where else are we going the other

00:29:23,980 --> 00:29:29,950
thing we want to do is set the downtime

00:29:27,550 --> 00:29:31,270
but also set the status of this service

00:29:29,950 --> 00:29:32,860
so we don't just want to set the

00:29:31,270 --> 00:29:35,770
downtime and just let the status be

00:29:32,860 --> 00:29:40,179
whatever it was we need to say ok the

00:29:35,770 --> 00:29:42,280
status of our backup service is exit

00:29:40,179 --> 00:29:45,070
status so this dynamically sets to the

00:29:42,280 --> 00:29:49,020
thing that I set in in the log stash

00:29:45,070 --> 00:29:52,090
filter so I said the exit status was 3

00:29:49,020 --> 00:29:56,260
so it will grab that variable from this

00:29:52,090 --> 00:30:02,290
percent thing is the field the message

00:29:56,260 --> 00:30:06,790
is just the whole log line actually the

00:30:02,290 --> 00:30:10,240
other thing we want to do is when we

00:30:06,790 --> 00:30:17,370
find these things also just set the

00:30:10,240 --> 00:30:21,580
status to ok again we use a dynamic

00:30:17,370 --> 00:30:23,679
field variable thingy and then we also

00:30:21,580 --> 00:30:26,440
remove the downtime because we know well

00:30:23,679 --> 00:30:29,590
when the backup is finished the

00:30:26,440 --> 00:30:30,970
downtimes is actually over so instead of

00:30:29,590 --> 00:30:32,410
just waiting for the downtime to finish

00:30:30,970 --> 00:30:34,710
and maybe something else happens to this

00:30:32,410 --> 00:30:36,910
particular service you can just

00:30:34,710 --> 00:30:40,900
dynamically remove the downtime whenever

00:30:36,910 --> 00:30:42,850
the backup is truly finished so let me

00:30:40,900 --> 00:30:55,590
make a service in a chinga that's going

00:30:42,850 --> 00:30:55,590
to do this ok so usual stuff backup

00:30:59,170 --> 00:31:06,890
we use a dummy because someone else is

00:31:03,980 --> 00:31:08,450
actually setting the stuff this is an

00:31:06,890 --> 00:31:11,540
interesting thing you can do is you can

00:31:08,450 --> 00:31:15,110
say this thing I can't remember now what

00:31:11,540 --> 00:31:18,340
is it what is it

00:31:15,110 --> 00:31:22,610
oh yeah okay

00:31:18,340 --> 00:31:32,690
okay let me know yeah tell me and this

00:31:22,610 --> 00:31:35,060
will stay set it default to unknown we

00:31:32,690 --> 00:31:38,280
just say the default message that cop

00:31:35,060 --> 00:31:41,369
activity and then just assign it to

00:31:38,280 --> 00:31:41,369
[Music]

00:31:44,050 --> 00:32:00,080
backup will out database server yeah

00:31:50,620 --> 00:32:02,780
cool okay so let's get our service in

00:32:00,080 --> 00:32:06,080
your chinga and now hopefully doesn't do

00:32:02,780 --> 00:32:12,320
this weed caching thing which maybe it

00:32:06,080 --> 00:32:13,670
does yeah yeah I don't know why it does

00:32:12,320 --> 00:32:16,460
this sometimes is there something I can

00:32:13,670 --> 00:32:24,860
do to like make it show more quickly or

00:32:16,460 --> 00:32:27,290
is this like a feature of the yeah I

00:32:24,860 --> 00:32:29,660
know this is the same thing I had before

00:32:27,290 --> 00:32:40,060
when I was testing it it's okay just

00:32:29,660 --> 00:32:42,230
actually show up hmm

00:32:40,060 --> 00:32:44,080
are there any itching experts in the

00:32:42,230 --> 00:32:47,840
room

00:32:44,080 --> 00:32:47,840
[Laughter]

00:33:00,980 --> 00:33:12,960
well I find this with the Ginga web yes

00:33:05,220 --> 00:33:30,750
I don't know no I think it's using the

00:33:12,960 --> 00:33:32,700
SQL back-end my MySQL yeah okay maybe

00:33:30,750 --> 00:33:34,230
once I then say you think it takes

00:33:32,700 --> 00:33:42,270
longer to get the service or something

00:33:34,230 --> 00:33:44,370
yeah yeah no but even then I was finding

00:33:42,270 --> 00:33:46,830
it on side last week like that I would

00:33:44,370 --> 00:33:49,380
add things and the daemon would restart

00:33:46,830 --> 00:33:53,809
and all that sort of thing I don't know

00:33:49,380 --> 00:33:56,340
what the yeah anyway

00:33:53,809 --> 00:33:59,660
maybe we can what do you think we can

00:33:56,340 --> 00:33:59,660
look at another example or something

00:34:01,429 --> 00:34:09,080
yeah yeah exactly yeah

00:34:04,049 --> 00:34:09,080
okay so let's actually run our script

00:34:11,899 --> 00:34:17,520
okay so file beat will collect the log

00:34:15,960 --> 00:34:22,609
because it's outputting to this sir

00:34:17,520 --> 00:34:22,609
Jason log file so we go into Cabana and

00:34:22,669 --> 00:34:37,889
what time is it 1004 cool there we go so

00:34:32,000 --> 00:34:41,250
this is from a log file backup started

00:34:37,889 --> 00:34:42,540
okay cool so Edgar's status is three

00:34:41,250 --> 00:34:46,730
because we don't know what the state of

00:34:42,540 --> 00:34:51,060
the backup is yet we've got a message

00:34:46,730 --> 00:34:58,700
and this is our special thing to set the

00:34:51,060 --> 00:34:58,700
status and then in that

00:34:59,750 --> 00:35:19,780
I don't know why this little any other

00:35:02,720 --> 00:35:19,780
ideas you can shout out yeah no ideas oh

00:35:19,900 --> 00:35:39,520
yeah okay I actually wanted I just do

00:35:29,240 --> 00:35:39,520
that yeah well it's not difficult oh

00:35:39,609 --> 00:35:43,000
yeah that's it

00:35:52,140 --> 00:36:00,910
yeah something I'm pretty sure sir I

00:35:54,940 --> 00:36:07,420
want 150 yeah okay

00:36:00,910 --> 00:36:10,210
I don't know why it's not a hold on

00:36:07,420 --> 00:36:14,740
maybe it's this thing yeah the chinga

00:36:10,210 --> 00:36:17,290
demon is dead but it's speaking over the

00:36:14,740 --> 00:36:19,589
idea over the network maybe the maybe

00:36:17,290 --> 00:36:22,260
can I kill this guy

00:36:19,589 --> 00:36:29,619
we run them for four minutes

00:36:22,260 --> 00:36:35,670
well this sucks okay

00:36:29,619 --> 00:36:35,670
well yeah I'll restart it again in

00:37:00,839 --> 00:37:12,069
started cry I'm something to do with the

00:37:04,990 --> 00:37:13,740
time time on our VM it's wrong no no

00:37:12,069 --> 00:37:17,950
okay cool

00:37:13,740 --> 00:37:19,480
we're on the go fellas okay so back up

00:37:17,950 --> 00:37:27,780
finished okay

00:37:19,480 --> 00:37:27,780
let's let's run that again see this so

00:37:29,430 --> 00:37:33,520
it's actually interesting so the backup

00:37:32,020 --> 00:37:37,780
was finished though so the the issue

00:37:33,520 --> 00:37:41,500
here is that the when when Oliver ran it

00:37:37,780 --> 00:37:43,420
the first time the the message came

00:37:41,500 --> 00:37:45,869
through logstash logstash

00:37:43,420 --> 00:37:49,329
saw that it needed to send its output to

00:37:45,869 --> 00:37:52,030
both elasticsearch and I saying yeah

00:37:49,329 --> 00:37:53,500
it sent it to elasticsearch successfully

00:37:52,030 --> 00:37:55,809
and most likely in the logs

00:37:53,500 --> 00:37:58,720
we will see that the API call that sent

00:37:55,809 --> 00:38:00,099
it to Ising was not successful but it

00:37:58,720 --> 00:38:02,440
doesn't mean that logstash doesn't

00:38:00,099 --> 00:38:05,559
consider the message as processed so

00:38:02,440 --> 00:38:08,650
your if we sing is for some reason not

00:38:05,559 --> 00:38:12,309
up and running or there's some other

00:38:08,650 --> 00:38:16,119
issue your alert will not be raised

00:38:12,309 --> 00:38:18,010
because it's a API call gets sent and

00:38:16,119 --> 00:38:21,339
regardless of whether that API call

00:38:18,010 --> 00:38:26,500
succeeds or fails the the message is

00:38:21,339 --> 00:38:28,420
considered handled so here we go back up

00:38:26,500 --> 00:38:33,579
of checkpoint Jenny is finished now so

00:38:28,420 --> 00:38:39,059
you see how it did the down time set the

00:38:33,579 --> 00:38:43,140
status and now when it finishes

00:38:39,059 --> 00:38:43,140
just have a look at the log that arrives

00:38:44,520 --> 00:38:51,190
so it here it started it sat down time

00:38:48,099 --> 00:38:53,500
finished so the stage so okay move the

00:38:51,190 --> 00:38:56,650
down time that's pretty much it I guess

00:38:53,500 --> 00:38:59,680
look at the history of oh yeah that's a

00:38:56,650 --> 00:39:01,660
good idea yeah yeah here we go

00:38:59,680 --> 00:39:03,040
and it has you know it has a little bit

00:39:01,660 --> 00:39:10,000
of like auditing history so it knows

00:39:03,040 --> 00:39:11,500
that logs - did it yeah I mean I guess I

00:39:10,000 --> 00:39:13,000
said really it's a pretty simple thing I

00:39:11,500 --> 00:39:15,880
think the use cases that we would

00:39:13,000 --> 00:39:17,950
think of stuff and it really helps when

00:39:15,880 --> 00:39:20,590
you have these really lockdown like

00:39:17,950 --> 00:39:23,320
devices say you've got some you know

00:39:20,590 --> 00:39:24,940
telephone modem at home or something you

00:39:23,320 --> 00:39:26,650
know that thing can write to syslog so

00:39:24,940 --> 00:39:30,370
if you can write a log stash filter that

00:39:26,650 --> 00:39:33,610
does it and you know then

00:39:30,370 --> 00:39:36,880
correspondingly sets the say the check

00:39:33,610 --> 00:39:38,110
process result then you know you can get

00:39:36,880 --> 00:39:41,020
stuff that you thought would be really

00:39:38,110 --> 00:39:43,600
hard to monitor because you know you

00:39:41,020 --> 00:39:45,790
can't even send any data to it

00:39:43,600 --> 00:39:47,560
as long as that thing can send output

00:39:45,790 --> 00:39:49,690
through the desist log send it to some

00:39:47,560 --> 00:39:52,270
demon somewhere in the network then you

00:39:49,690 --> 00:39:53,890
can do you know nice monitoring and it

00:39:52,270 --> 00:39:56,020
doesn't have to be you know these

00:39:53,890 --> 00:39:57,340
passive checks that sort of you can't

00:39:56,020 --> 00:40:04,570
really trust them because you can set

00:39:57,340 --> 00:40:06,370
you know the service state in advance so

00:40:04,570 --> 00:40:07,840
it means that you know I guess this is

00:40:06,370 --> 00:40:10,150
this nice thing about you know green

00:40:07,840 --> 00:40:13,150
really means green right so you can be

00:40:10,150 --> 00:40:14,980
rest assured that if it's okay the only

00:40:13,150 --> 00:40:16,390
time it can ever go okay is if it's

00:40:14,980 --> 00:40:19,030
gotten this certain log message from

00:40:16,390 --> 00:40:23,910
your modem or something whatever your

00:40:19,030 --> 00:40:27,430
lock down devices so it's pretty simple

00:40:23,910 --> 00:40:28,930
which is nice it's very useful you can

00:40:27,430 --> 00:40:31,750
use it pretty much straight away I think

00:40:28,930 --> 00:40:33,550
I'm gonna do it on probably I'll do it

00:40:31,750 --> 00:40:37,630
on Tuesday I'm gonna implement this

00:40:33,550 --> 00:40:42,220
thing now that we've got it working yeah

00:40:37,630 --> 00:40:53,710
I guess two more slides in there yeah do

00:40:42,220 --> 00:40:57,760
we need to show anything else okay so up

00:40:53,710 --> 00:40:59,170
for improvement obviously one of the one

00:40:57,760 --> 00:41:00,970
of the things is that you saw here a

00:40:59,170 --> 00:41:06,730
fairly simple use case and you already

00:41:00,970 --> 00:41:12,970
saw four I think I output and actually

00:41:06,730 --> 00:41:14,770
it doesn't even have to be so here we

00:41:12,970 --> 00:41:17,590
have four icing outputs and even that

00:41:14,770 --> 00:41:19,240
should possibly already be boiled back

00:41:17,590 --> 00:41:21,400
down to three because we actually have

00:41:19,240 --> 00:41:24,250
two exactly the same logstash outputs if

00:41:21,400 --> 00:41:25,549
you for those of you who are really

00:41:24,250 --> 00:41:30,440
awake

00:41:25,549 --> 00:41:33,229
so you see that the the process check

00:41:30,440 --> 00:41:36,140
result for the for scheduling downtime

00:41:33,229 --> 00:41:38,180
so that one that you see at the bottom

00:41:36,140 --> 00:41:40,069
of the screen there is the is the one

00:41:38,180 --> 00:41:43,069
that we do when the backup starts but if

00:41:40,069 --> 00:41:45,859
you if you scroll down a bit you'll see

00:41:43,069 --> 00:41:47,420
that the the other process check result

00:41:45,859 --> 00:41:50,410
is actually the same because we get the

00:41:47,420 --> 00:41:54,079
exit status from the logstash message

00:41:50,410 --> 00:41:56,509
and so they are the same so with

00:41:54,079 --> 00:42:00,460
different conditionals we could actually

00:41:56,509 --> 00:42:05,989
reduce this to three outputs however

00:42:00,460 --> 00:42:08,479
logically you would imagine that you

00:42:05,989 --> 00:42:10,670
should only ever have one output the

00:42:08,479 --> 00:42:15,049
reason that you have currently the need

00:42:10,670 --> 00:42:19,359
for creating all these different outputs

00:42:15,049 --> 00:42:23,150
is that the while you can set the action

00:42:19,359 --> 00:42:28,640
the action config parameters cannot be

00:42:23,150 --> 00:42:30,410
dynamically retrieved from the from the

00:42:28,640 --> 00:42:33,710
logstash message so what you really want

00:42:30,410 --> 00:42:36,049
to be doing is in the log stash in log

00:42:33,710 --> 00:42:38,690
stash where you receive the messages and

00:42:36,049 --> 00:42:41,749
when you're filtering it at the the

00:42:38,690 --> 00:42:44,420
specific fields to the message that

00:42:41,749 --> 00:42:47,869
allow for the API call to have the right

00:42:44,420 --> 00:42:50,059
parameters however that's not currently

00:42:47,869 --> 00:42:56,809
possible so this would be a great thing

00:42:50,059 --> 00:42:58,609
for a version - yeah for a version - and

00:42:56,809 --> 00:43:01,880
so if you make the action config fields

00:42:58,609 --> 00:43:05,719
dynamic then then that makes it much

00:43:01,880 --> 00:43:08,960
easier the other thing is that and that

00:43:05,719 --> 00:43:13,460
will go away if you have if you have

00:43:08,960 --> 00:43:19,160
these action more dynamic right now we

00:43:13,460 --> 00:43:23,029
specify the credentials for the for the

00:43:19,160 --> 00:43:25,579
API user in a bunch of places and this

00:43:23,029 --> 00:43:29,210
could all be be dynamic actually the I

00:43:25,579 --> 00:43:31,670
single host and I sing a host and I sing

00:43:29,210 --> 00:43:34,269
a service these two already allow for

00:43:31,670 --> 00:43:37,670
dynamic processing so you can actually

00:43:34,269 --> 00:43:39,030
take the host directly from the from the

00:43:37,670 --> 00:43:41,250
host field and

00:43:39,030 --> 00:43:43,260
in the message that comes into logstash

00:43:41,250 --> 00:43:45,089
because that comes from a server and

00:43:43,260 --> 00:43:49,230
logstash knows about that so we can

00:43:45,089 --> 00:43:51,440
easily use that as a value for the for

00:43:49,230 --> 00:43:53,369
the host and the same thing goes for the

00:43:51,440 --> 00:43:55,140
for the service

00:43:53,369 --> 00:44:00,180
we just said it statically here because

00:43:55,140 --> 00:44:03,690
of ease of understanding other than that

00:44:00,180 --> 00:44:04,819
it works quite well the use cases are

00:44:03,690 --> 00:44:06,510
still something that needs to be

00:44:04,819 --> 00:44:09,030
crystallized a little bit somebody

00:44:06,510 --> 00:44:14,400
mentioned SNMP traps would be a an

00:44:09,030 --> 00:44:20,750
interesting use case some alternatives

00:44:14,400 --> 00:44:22,470
so elastic has the watcher which is a

00:44:20,750 --> 00:44:24,839
one of the components of what

00:44:22,470 --> 00:44:27,300
elasticsearch calls or elastic accompany

00:44:24,839 --> 00:44:30,119
provides this thing called x-pac which

00:44:27,300 --> 00:44:31,920
is a commercial extension to elastic

00:44:30,119 --> 00:44:34,800
search and one of the things that are in

00:44:31,920 --> 00:44:36,480
there is watcher but a unit need to be

00:44:34,800 --> 00:44:38,750
aware that it operates on the on the

00:44:36,480 --> 00:44:40,980
elastic search level so you move your

00:44:38,750 --> 00:44:42,780
alerting to elastic search and in my

00:44:40,980 --> 00:44:45,390
opinion it's better to do this on the

00:44:42,780 --> 00:44:48,000
icing aside because icing is already

00:44:45,390 --> 00:44:49,680
equipped with alerting strategies and

00:44:48,000 --> 00:44:51,270
all that kind of kind of stuff so

00:44:49,680 --> 00:44:53,810
there's no need to re-implement that on

00:44:51,270 --> 00:44:56,220
a on a different level

00:44:53,810 --> 00:44:58,589
Splunk which I have no experience with

00:44:56,220 --> 00:45:01,130
no doubt also provides these kind of

00:44:58,589 --> 00:45:03,210
kind of things but then again the

00:45:01,130 --> 00:45:05,579
processing happens on the on the data

00:45:03,210 --> 00:45:11,790
storage level instead of on the on the

00:45:05,579 --> 00:45:14,970
log stash level I wanted to say

00:45:11,790 --> 00:45:17,130
something else but it slipped my mind

00:45:14,970 --> 00:45:19,109
obviously we're hiring if you happen to

00:45:17,130 --> 00:45:20,339
be interested in moving to the

00:45:19,109 --> 00:45:26,640
Netherlands or you already lived there

00:45:20,339 --> 00:45:28,349
and then you can join us we are a not

00:45:26,640 --> 00:45:37,950
the worst employer in the world it's

00:45:28,349 --> 00:45:42,480
kind of a yeah I know we bring our child

00:45:37,950 --> 00:45:45,720
to conferences yeah if you have any

00:45:42,480 --> 00:45:48,630
questions Oliver at Olin datacom Walter

00:45:45,720 --> 00:45:50,190
at Olin datacom pretty much anything at

00:45:48,630 --> 00:45:52,260
all later that com happens to end up in

00:45:50,190 --> 00:45:52,980
my inbox so if you want to tease me just

00:45:52,260 --> 00:45:54,810
send

00:45:52,980 --> 00:45:58,080
after whatever address you think is

00:45:54,810 --> 00:46:03,560
appropriate at Olin datacom I'm on

00:45:58,080 --> 00:46:06,840
Twitter as well and yeah if you have any

00:46:03,560 --> 00:46:08,430
questions get in touch but I think now

00:46:06,840 --> 00:46:10,050
we still have some time for for

00:46:08,430 --> 00:46:16,040
questions or are there any questions at

00:46:10,050 --> 00:46:16,040
the moment are we still awake yeah yep

00:46:16,310 --> 00:46:23,250
okay it seems like a lot of effort all

00:46:21,210 --> 00:46:25,050
these filters and inputs and outputs and

00:46:23,250 --> 00:46:28,200
all that config is there a way of

00:46:25,050 --> 00:46:31,680
sharing that distributing it you know I

00:46:28,200 --> 00:46:32,910
don't know other people you know you

00:46:31,680 --> 00:46:41,430
know sneaking them on github all those

00:46:32,910 --> 00:46:42,780
kind of things do you do that so the

00:46:41,430 --> 00:46:46,200
question is it seems like a lot of

00:46:42,780 --> 00:46:49,020
effort to to set up the filters and and

00:46:46,200 --> 00:46:52,740
the outputs is there any way of sharing

00:46:49,020 --> 00:46:55,800
it so not really because the filters

00:46:52,740 --> 00:46:57,570
depend very much on what kind of logs

00:46:55,800 --> 00:46:59,670
you are having and what your specific

00:46:57,570 --> 00:47:00,869
monitoring scenarios are so it's it's

00:46:59,670 --> 00:47:04,350
kind of the same thing as setting up

00:47:00,869 --> 00:47:06,690
your your checks and your commands on

00:47:04,350 --> 00:47:09,720
any single installation you can have a

00:47:06,690 --> 00:47:11,220
bunch of default ones but a lot of it is

00:47:09,720 --> 00:47:12,900
very custom so what we showed here is

00:47:11,220 --> 00:47:14,910
very custom depending on the backup

00:47:12,900 --> 00:47:16,830
scripts that somebody is creating

00:47:14,910 --> 00:47:18,180
depending on which messages are coming

00:47:16,830 --> 00:47:22,500
out of that backup script you need to

00:47:18,180 --> 00:47:24,090
write your filters to actually activate

00:47:22,500 --> 00:47:26,070
on that specific message in that

00:47:24,090 --> 00:47:29,720
specific source or whatever it is you

00:47:26,070 --> 00:47:34,290
determine the the conditional part on

00:47:29,720 --> 00:47:38,940
the output part is actually fairly

00:47:34,290 --> 00:47:40,380
straightforward this seems like a lot of

00:47:38,940 --> 00:47:47,550
configuration but it's actually fairly

00:47:40,380 --> 00:47:50,369
straightforward and the there's a this

00:47:47,550 --> 00:47:51,960
is the github repo so get up Lacombe / I

00:47:50,369 --> 00:47:54,240
sing a slash logstash output a singer

00:47:51,960 --> 00:47:57,540
that has all the configuration in it and

00:47:54,240 --> 00:47:59,850
so the it's fairly simple actually it

00:47:57,540 --> 00:48:02,840
might be a lot of letters in the early

00:47:59,850 --> 00:48:07,010
morning but it's a it's fairly simple

00:48:02,840 --> 00:48:09,170
when it all comes down to it so earlier

00:48:07,010 --> 00:48:13,400
you're talking about doing some

00:48:09,170 --> 00:48:15,740
processing on the file beat side before

00:48:13,400 --> 00:48:19,730
you send anything through so so I can

00:48:15,740 --> 00:48:22,160
have two questions and some use cases

00:48:19,730 --> 00:48:23,510
I've seen where someone wants they're

00:48:22,160 --> 00:48:26,750
reading a log file and they just want

00:48:23,510 --> 00:48:29,300
every single line pass through to lock

00:48:26,750 --> 00:48:32,330
stash in and then sent sent on to where

00:48:29,300 --> 00:48:33,830
where it needs to go so you know what

00:48:32,330 --> 00:48:35,660
kind of throughput you know can we have

00:48:33,830 --> 00:48:36,680
there based on you know how many logs I

00:48:35,660 --> 00:48:38,090
mean is it good

00:48:36,680 --> 00:48:41,000
you know at what point are you gonna

00:48:38,090 --> 00:48:42,860
break it if reading so many logs the

00:48:41,000 --> 00:48:45,800
other question is how granular can you

00:48:42,860 --> 00:48:51,340
get with what you're sending through

00:48:45,800 --> 00:48:58,580
with the filtering on the agent side so

00:48:51,340 --> 00:49:01,190
the first question yeah how do you do

00:48:58,580 --> 00:49:03,440
well not personally but I do know that

00:49:01,190 --> 00:49:05,480
it's a fairly simple law as I said log

00:49:03,440 --> 00:49:08,600
stash is quite heavy so depending on how

00:49:05,480 --> 00:49:19,970
much messages you you send there that

00:49:08,600 --> 00:49:22,310
the Oh video hi depending on on how much

00:49:19,970 --> 00:49:25,420
messages you send it you could end up

00:49:22,310 --> 00:49:28,100
with multiple log stash demons in a in a

00:49:25,420 --> 00:49:30,470
reasonably sized Network so a couple

00:49:28,100 --> 00:49:33,940
hundred horses is going to easily cost

00:49:30,470 --> 00:49:38,120
you one or two log stash servers

00:49:33,940 --> 00:49:40,670
dedicated servers so it is fairly heavy

00:49:38,120 --> 00:49:42,260
and people for that reason I've started

00:49:40,670 --> 00:49:45,800
looking at other things and log stash

00:49:42,260 --> 00:49:49,580
but I find it personally quite nice and

00:49:45,800 --> 00:49:52,520
usable in in smaller size networks and

00:49:49,580 --> 00:49:54,950
so one of your questions was can i

00:49:52,520 --> 00:49:56,390
discard messages on the file bit level

00:49:54,950 --> 00:49:59,180
so they don't ever have to go to log

00:49:56,390 --> 00:50:01,880
stash yes that's possible it's a it's a

00:49:59,180 --> 00:50:03,500
file weight thing so in in file bu you

00:50:01,880 --> 00:50:06,260
have quite a bit of configuration

00:50:03,500 --> 00:50:08,750
options I just like to keep the logic

00:50:06,260 --> 00:50:10,610
mostly on the log stash level but if you

00:50:08,750 --> 00:50:13,250
if you have messages that you know you

00:50:10,610 --> 00:50:16,220
never want to send through it and yes it

00:50:13,250 --> 00:50:20,210
makes total sense to discard them at the

00:50:16,220 --> 00:50:35,119
at the file beat level the other

00:50:20,210 --> 00:50:39,560
question was what was your second which

00:50:35,119 --> 00:50:41,330
agent side on the log stash on the file

00:50:39,560 --> 00:50:42,170
beat side so what we did what we do is

00:50:41,330 --> 00:50:46,369
unfiltered

00:50:42,170 --> 00:50:48,470
so the idea is that file beat picks up

00:50:46,369 --> 00:50:50,330
on the it's basically just tailing a log

00:50:48,470 --> 00:50:52,849
file and in that simple config that we

00:50:50,330 --> 00:50:54,980
showed where every single line that ends

00:50:52,849 --> 00:50:56,990
up in the message that it's prospecting

00:50:54,980 --> 00:50:59,270
at prospects every 10 seconds I think by

00:50:56,990 --> 00:51:01,160
default and then it just looks at which

00:50:59,270 --> 00:51:05,090
new lines are there and sends them over

00:51:01,160 --> 00:51:06,950
to log stash if you want to do some some

00:51:05,090 --> 00:51:12,650
processing there already that's possible

00:51:06,950 --> 00:51:15,050
but in general I like sending the the

00:51:12,650 --> 00:51:17,599
entire message as a simple line so it's

00:51:15,050 --> 00:51:20,060
it's file bit doesn't know that it's

00:51:17,599 --> 00:51:22,970
what kind of format it's made up of if

00:51:20,060 --> 00:51:24,470
it's Jason or not or if it's a Java

00:51:22,970 --> 00:51:27,500
stack trace it doesn't know it only

00:51:24,470 --> 00:51:29,599
knows that this is the unit of log event

00:51:27,500 --> 00:51:31,070
so to speak and that goes to log session

00:51:29,599 --> 00:51:33,880
and log stash is the one that is

00:51:31,070 --> 00:51:36,260
responsible for cutting it up and and

00:51:33,880 --> 00:51:39,170
making sense of it all and adding

00:51:36,260 --> 00:51:42,910
metadata to it if you want to and then

00:51:39,170 --> 00:51:54,710
from there taking actions in the output

00:51:42,910 --> 00:51:58,700
any other questions I have two questions

00:51:54,710 --> 00:52:01,280
the first is how does knocks - knocks -

00:51:58,700 --> 00:52:03,980
know which downtime to delete after the

00:52:01,280 --> 00:52:07,070
job's done because maybe some

00:52:03,980 --> 00:52:11,750
administrator has had his own downtime

00:52:07,070 --> 00:52:14,700
set and this one and the the other thing

00:52:11,750 --> 00:52:17,589
is can you show the

00:52:14,700 --> 00:52:21,849
service definition of the I think it was

00:52:17,589 --> 00:52:27,249
a dummy session dummy service so would

00:52:21,849 --> 00:52:30,700
be nice okay you should yeah sure okay

00:52:27,249 --> 00:52:31,660
so uh well I'll just answer the first

00:52:30,700 --> 00:52:34,809
one first

00:52:31,660 --> 00:52:38,170
I guess so how do you all okay well this

00:52:34,809 --> 00:52:41,499
is how you know which service or which

00:52:38,170 --> 00:52:46,029
host it's scheduling downtime for is set

00:52:41,499 --> 00:52:48,279
here yeah so you're only setting this

00:52:46,029 --> 00:52:53,460
particular service to downtime so if we

00:52:48,279 --> 00:52:56,920
have a look at this finger this is the

00:52:53,460 --> 00:52:59,789
history of the backup service but if I

00:52:56,920 --> 00:53:01,240
were to look at for example this oh

00:52:59,789 --> 00:53:04,089
there we go

00:53:01,240 --> 00:53:07,690
oh that was our testing okay so let's go

00:53:04,089 --> 00:53:09,880
to this yeah so here there was no

00:53:07,690 --> 00:53:13,660
downtime set for that one the service

00:53:09,880 --> 00:53:14,890
definition so and to answer your

00:53:13,660 --> 00:53:17,529
question a little bit more and so yeah

00:53:14,890 --> 00:53:20,680
removing removing a downtime removes all

00:53:17,529 --> 00:53:25,690
down times by that author on that

00:53:20,680 --> 00:53:28,660
specific service so if you have but it

00:53:25,690 --> 00:53:31,539
you shouldn't really have multiple down

00:53:28,660 --> 00:53:37,299
times active on the same service on the

00:53:31,539 --> 00:53:41,019
same post maybe yeah yeah isn't it the

00:53:37,299 --> 00:53:42,970
author that we set in the that sadly for

00:53:41,019 --> 00:53:45,009
comments no no look there's an author

00:53:42,970 --> 00:53:47,049
there the action config has an author

00:53:45,009 --> 00:53:49,119
and also when you do that when you to

00:53:47,049 --> 00:53:53,200
remove downtime we've got if you go down

00:53:49,119 --> 00:53:57,960
a little bit because then you actually

00:53:53,200 --> 00:54:02,319
specify the author as well yeah whoops

00:53:57,960 --> 00:54:04,779
yeah yeah yeah it's the same API problem

00:54:02,319 --> 00:54:06,670
because the when you set a downtime

00:54:04,779 --> 00:54:08,829
normally the API returns the ID

00:54:06,670 --> 00:54:11,289
identifier of the downtime that you just

00:54:08,829 --> 00:54:14,289
set but the log stash has no place to

00:54:11,289 --> 00:54:15,640
put that downtime identifier so when

00:54:14,289 --> 00:54:17,019
you're removing it it's just like okay

00:54:15,640 --> 00:54:19,900
I'll just remove everything from that

00:54:17,019 --> 00:54:22,210
specific author and as Dave said you can

00:54:19,900 --> 00:54:27,250
easily make the author a little bit

00:54:22,210 --> 00:54:33,230
dynamic and cheat your way out of it

00:54:27,250 --> 00:54:34,670
so this is the our backup service so the

00:54:33,230 --> 00:54:38,810
reason why we're doing a dummy want is

00:54:34,670 --> 00:54:41,870
because somebody else is setting the

00:54:38,810 --> 00:54:43,130
status somebody else is setting the you

00:54:41,870 --> 00:54:47,630
know the comments and all this sort of

00:54:43,130 --> 00:54:50,870
thing so it really can be anything so I

00:54:47,630 --> 00:54:52,220
put some extra stuff here just to you

00:54:50,870 --> 00:54:55,580
know make it so that it's a little bit

00:54:52,220 --> 00:54:56,690
more make some sense but by default it's

00:54:55,580 --> 00:54:58,670
set to okay

00:54:56,690 --> 00:55:03,830
and it just says something like check

00:54:58,670 --> 00:55:06,430
command was successful so I guess in one

00:55:03,830 --> 00:55:09,560
way it's good but then in another way

00:55:06,430 --> 00:55:12,920
well let's say it like this Walter was

00:55:09,560 --> 00:55:16,310
talking about having a catch-all service

00:55:12,920 --> 00:55:19,900
so for this particular thing we could

00:55:16,310 --> 00:55:23,240
have many different logstash

00:55:19,900 --> 00:55:25,670
filter outputs is that the yeah yeah

00:55:23,240 --> 00:55:28,520
outputs that so that track many

00:55:25,670 --> 00:55:31,660
different log files but only set the

00:55:28,520 --> 00:55:33,980
status of this particular service so

00:55:31,660 --> 00:55:36,680
what I am actually going to do on

00:55:33,980 --> 00:55:39,140
Tuesday is I'm setting up this exact

00:55:36,680 --> 00:55:40,310
thing with in syslog any error status

00:55:39,140 --> 00:55:42,350
because I'm just getting to know the

00:55:40,310 --> 00:55:46,400
environment so I can just have this big

00:55:42,350 --> 00:55:49,160
fat you know apply service syslog errors

00:55:46,400 --> 00:55:51,200
or something and any error that happens

00:55:49,160 --> 00:55:54,800
to come up in syslog on this particular

00:55:51,200 --> 00:55:57,740
server then I just wanted to tell me

00:55:54,800 --> 00:56:00,830
here the other thing that we realized

00:55:57,740 --> 00:56:03,950
yesterday is that there is actually a

00:56:00,830 --> 00:56:07,250
flag in in the output in the icing

00:56:03,950 --> 00:56:09,530
output that you can use to set the to

00:56:07,250 --> 00:56:12,560
dynamically create the service if it

00:56:09,530 --> 00:56:16,610
doesn't exist yet however that cannot

00:56:12,560 --> 00:56:18,860
remove the service at a given point in

00:56:16,610 --> 00:56:22,070
time so you'll end up with services that

00:56:18,860 --> 00:56:24,790
get added and will stay there forever

00:56:22,070 --> 00:56:27,890
I personally I'm a bigger fan of

00:56:24,790 --> 00:56:29,330
intentionally creating the service but

00:56:27,890 --> 00:56:31,820
it probably depends a little bit on your

00:56:29,330 --> 00:56:33,800
on your use case I like to not have to

00:56:31,820 --> 00:56:36,700
wonder why where a service came from

00:56:33,800 --> 00:56:36,700
that's

00:56:39,759 --> 00:56:44,349
actually I just wanted to add one more

00:56:41,630 --> 00:56:47,180
thing to Dave's question he asked about

00:56:44,349 --> 00:56:48,950
all the work that goes in so actually in

00:56:47,180 --> 00:56:51,289
the end like for example my demo we just

00:56:48,950 --> 00:56:53,630
had you know setting the downtime and

00:56:51,289 --> 00:56:56,900
then making something go from unknown to

00:56:53,630 --> 00:57:01,309
okay again I guess we'll again we come

00:56:56,900 --> 00:57:04,329
back to this use case stuff I'm

00:57:01,309 --> 00:57:07,099
imagining now for a customer that I'm at

00:57:04,329 --> 00:57:10,069
there's this application that I'm just

00:57:07,099 --> 00:57:12,259
nowhere near being able to put agents on

00:57:10,069 --> 00:57:14,119
the servers and really start writing

00:57:12,259 --> 00:57:16,099
plug-ins for them because the people who

00:57:14,119 --> 00:57:19,880
write the application or in another

00:57:16,099 --> 00:57:21,079
country and communicating with them is

00:57:19,880 --> 00:57:22,609
also really difficult because I don't

00:57:21,079 --> 00:57:23,960
even have an IM account because they

00:57:22,609 --> 00:57:26,809
can't seem to get around to making me

00:57:23,960 --> 00:57:28,400
one and all this sort of stuff so right

00:57:26,809 --> 00:57:30,249
now if I wanted to say hey look I'm

00:57:28,400 --> 00:57:32,779
doing work and it is actually useful

00:57:30,249 --> 00:57:34,279
then I can start learning about the

00:57:32,779 --> 00:57:36,859
application and seeing all the behaviors

00:57:34,279 --> 00:57:38,630
by doing log stash so instead of writing

00:57:36,859 --> 00:57:42,049
plugins I can just start you know

00:57:38,630 --> 00:57:43,039
filtering log messages so I know what

00:57:42,049 --> 00:57:49,700
you mean though there's a lot of like

00:57:43,039 --> 00:57:52,609
sort of stuff but yeah exactly yeah yes

00:57:49,700 --> 00:57:54,769
and it's a very much as with the rest of

00:57:52,609 --> 00:57:57,349
monitoring it's a setup once benefit for

00:57:54,769 --> 00:58:00,739
a very long time kind of thing so you

00:57:57,349 --> 00:58:02,440
know yes it takes a bit of time to get

00:58:00,739 --> 00:58:05,029
the process right and figure out which

00:58:02,440 --> 00:58:09,160
outputs you need to you need to use but

00:58:05,029 --> 00:58:11,719
then it gets quite useful over time I

00:58:09,160 --> 00:58:15,529
think we have time for zero more

00:58:11,719 --> 00:58:18,400
questions so nobody has a question thank

00:58:15,529 --> 00:58:19,950
you very much thanks

00:58:18,400 --> 00:58:22,010
[Applause]

00:58:19,950 --> 00:58:22,010

YouTube URL: https://www.youtube.com/watch?v=XuJZpnLtytI


