Title: OSMC 2017 | Monitoring with Sensu — it’s the sensuble thing to do by Jochen Lillich
Publication date: 2017-12-05
Playlist: OSMC 2017 | Open Source Monitoring Conference
Description: 
	Well, I guess if I don’t get beaten up by the folks at Netways for presenting an alternative to Icinga, I certainly will be for the title of this talk. But my #monitoringlove for this software is just too strong! After suffering from Nagios for too many years, discovering Sensu saved what was left of my mental health. It also allowed us at freistil IT to grow our web hosting infrastructure without worries about check delays and scalability nightmares.
Sensu just went into its sixth year and counts big names like Yelp, GE, GoDaddy, T-Mobile and OpenTable to its large user community. In this update of my OSMC talk from 2014, I’m going to explain the basics of the Sensu monitoring framework and the advantages of its distributed architecture. Attendees of my talk will not only learn how easy Sensu is to use for health checks and metrics collection, but also what its current limitations are and what’s coming with Sensu 2.0 (aka “sensu-go”).
Captions: 
	00:00:11,700 --> 00:00:16,770
so thank you for having you back in the

00:00:13,860 --> 00:00:19,500
board room so we will go straight

00:00:16,770 --> 00:00:22,800
forward with our next talk the next

00:00:19,500 --> 00:00:26,630
speaker is the founder of fresh fly

00:00:22,800 --> 00:00:29,940
steel box which is a managed hosting

00:00:26,630 --> 00:00:32,879
platform company based in Dublin he is

00:00:29,940 --> 00:00:35,820
also a very famous or well-known for his

00:00:32,879 --> 00:00:38,390
talks in on community conferences and

00:00:35,820 --> 00:00:41,010
his activities in the Drupal community

00:00:38,390 --> 00:00:43,530
but today he will not present us

00:00:41,010 --> 00:00:46,260
something about Rupa and he will give us

00:00:43,530 --> 00:00:46,940
a talk about sanzu and so let me

00:00:46,260 --> 00:00:54,920
introduce you

00:00:46,940 --> 00:00:59,239
yo Lilly thank you thanks a lot and

00:00:54,920 --> 00:01:02,220
welcome to my talk monitoring with sensu

00:00:59,239 --> 00:01:05,070
fortunately thanks to recent advances in

00:01:02,220 --> 00:01:07,190
time travel I've already finished this

00:01:05,070 --> 00:01:12,770
talk thanks for the great discussion

00:01:07,190 --> 00:01:16,380
let's go and have lunch thank you okay

00:01:12,770 --> 00:01:20,299
I've covered this topic before at OS MC

00:01:16,380 --> 00:01:20,299
back in 2014

00:01:26,930 --> 00:01:33,170
here we go yeah the year before we had

00:01:30,950 --> 00:01:36,560
run into severe load and queueing

00:01:33,170 --> 00:01:40,130
problems with Nagios and our hosting

00:01:36,560 --> 00:01:43,160
platform started to grow faster and now

00:01:40,130 --> 00:01:45,830
you simply couldn't keep up I learned

00:01:43,160 --> 00:01:48,290
about sensu at DevOps days London where

00:01:45,830 --> 00:01:52,580
Wolfman Sun contradicted the

00:01:48,290 --> 00:01:56,960
then-popular monitoring sucks hashtag in

00:01:52,580 --> 00:01:59,350
his talk monitoring love so we gave

00:01:56,960 --> 00:02:03,200
sensor a try and never looked back

00:01:59,350 --> 00:02:05,270
sensor is still awesome and deserves an

00:02:03,200 --> 00:02:10,399
updated talk at this open source

00:02:05,270 --> 00:02:13,940
monitoring conference Who am I my name

00:02:10,399 --> 00:02:16,790
is Johan I'm the founder and CEO of fry

00:02:13,940 --> 00:02:20,060
steel IT a team of managed hosting

00:02:16,790 --> 00:02:23,989
experts based in Germany where I started

00:02:20,060 --> 00:02:30,950
the company in 2010 and in Ireland where

00:02:23,989 --> 00:02:34,489
I live why would you use sensor well

00:02:30,950 --> 00:02:36,560
first of all it's easy to setup so you

00:02:34,489 --> 00:02:39,650
can give it a try without losing a lot

00:02:36,560 --> 00:02:43,130
of time it can grow with your

00:02:39,650 --> 00:02:48,890
infrastructure with other which other

00:02:43,130 --> 00:02:53,150
monitoring systems Nagios can't you can

00:02:48,890 --> 00:02:55,180
reuse the checks you already have and

00:02:53,150 --> 00:02:58,459
that are readily available on the

00:02:55,180 --> 00:03:02,049
internet checks for nagios for icing for

00:02:58,459 --> 00:03:04,970
zabbix and other monitoring systems

00:03:02,049 --> 00:03:09,790
since you can collect both Health

00:03:04,970 --> 00:03:09,790
metrics the typical status checks and

00:03:10,810 --> 00:03:16,070
metrics in the wider sense that you

00:03:13,940 --> 00:03:20,180
might pass on to something like graphite

00:03:16,070 --> 00:03:24,230
and it has a great open source community

00:03:20,180 --> 00:03:29,030
that is very helpful speaking of open

00:03:24,230 --> 00:03:32,320
source since who does a dual licensing

00:03:29,030 --> 00:03:36,440
thing where they have an open source a

00:03:32,320 --> 00:03:40,340
part which is named central core and is

00:03:36,440 --> 00:03:40,720
licensed under the MIT license you have

00:03:40,340 --> 00:03:46,720
gray

00:03:40,720 --> 00:03:49,510
support on mailing lists on IRC or if

00:03:46,720 --> 00:03:54,250
you are one of these hipstery types you

00:03:49,510 --> 00:03:57,310
can also use slack sensu was created in

00:03:54,250 --> 00:04:01,060
part time by shawn porter at a company

00:03:57,310 --> 00:04:07,000
named Sounion today he is the CTO at

00:04:01,060 --> 00:04:08,770
sensu incorporated the the other side

00:04:07,000 --> 00:04:12,330
the commercial side is sent to

00:04:08,770 --> 00:04:16,180
enterprise where since we incorporated

00:04:12,330 --> 00:04:21,070
does the typical consulting and support

00:04:16,180 --> 00:04:24,100
gig where they offer more of a product

00:04:21,070 --> 00:04:27,490
that is based on the sense of core

00:04:24,100 --> 00:04:29,620
framework and that product comes with

00:04:27,490 --> 00:04:32,620
everything that sense of core has and on

00:04:29,620 --> 00:04:36,460
top of that they add additional things

00:04:32,620 --> 00:04:39,430
like contact routing built-in handlers

00:04:36,460 --> 00:04:42,430
which by with central core you need to

00:04:39,430 --> 00:04:47,040
install manually it does things like

00:04:42,430 --> 00:04:49,930
metrics conversion and has lots of

00:04:47,040 --> 00:04:53,919
service integrations out of the box

00:04:49,930 --> 00:04:56,229
it also has features like a heads-up

00:04:53,919 --> 00:04:59,320
display which is more or less a

00:04:56,229 --> 00:05:01,540
dashboard and of course you can buy

00:04:59,320 --> 00:05:07,930
commercial support and training from

00:05:01,540 --> 00:05:11,890
sensor incorporated if you like to give

00:05:07,930 --> 00:05:16,180
it a try it's very easy to do sensor is

00:05:11,890 --> 00:05:19,360
written in Ruby at the moment and uses a

00:05:16,180 --> 00:05:21,850
concept named omnibus packaging where

00:05:19,360 --> 00:05:23,740
everything you need to run comes with

00:05:21,850 --> 00:05:26,770
sensor so it has its own Ruby

00:05:23,740 --> 00:05:29,830
distribution it has all the gems that

00:05:26,770 --> 00:05:33,040
are necessary and you don't get into the

00:05:29,830 --> 00:05:38,380
dependency health at gems sometimes can

00:05:33,040 --> 00:05:41,380
be and sensor is also easy to configure

00:05:38,380 --> 00:05:44,400
it uses a bunch of JSON files that you

00:05:41,380 --> 00:05:48,430
need to set up and of course that is

00:05:44,400 --> 00:05:51,529
most easily done by automation they are

00:05:48,430 --> 00:05:54,649
our chef coke

00:05:51,529 --> 00:05:57,169
books puppet modules and ansible play

00:05:54,649 --> 00:06:01,639
books that can set up sensor within a

00:05:57,169 --> 00:06:04,459
minute the main advantage of course of

00:06:01,639 --> 00:06:06,559
automation is that you'll never forget

00:06:04,459 --> 00:06:10,639
to add new machines to mock to your

00:06:06,559 --> 00:06:13,129
monitoring systems and that knowledge

00:06:10,639 --> 00:06:17,839
that is already in your automation

00:06:13,129 --> 00:06:21,739
system can be used to set up your sense

00:06:17,839 --> 00:06:28,509
of clients I'll get back to this at a

00:06:21,739 --> 00:06:32,389
later stage so how does sensor work as

00:06:28,509 --> 00:06:35,599
we all know building a monitoring system

00:06:32,389 --> 00:06:38,239
is very easy you will need a monitoring

00:06:35,599 --> 00:06:41,179
client that executes the checks you need

00:06:38,239 --> 00:06:42,949
a server where the server talks to the

00:06:41,179 --> 00:06:46,359
clients and the clients talk to the

00:06:42,949 --> 00:06:52,069
server and you need some place where

00:06:46,359 --> 00:06:55,489
alerts are generated unfortunately some

00:06:52,069 --> 00:06:59,509
people take this literally and then the

00:06:55,489 --> 00:07:03,739
result of that is things like Nagios but

00:06:59,509 --> 00:07:09,619
if you take this more as an abstract

00:07:03,739 --> 00:07:14,239
description you'll get sensu so these

00:07:09,619 --> 00:07:17,209
are the parts of a sensor system you

00:07:14,239 --> 00:07:22,549
have the monitoring agents down there

00:07:17,209 --> 00:07:26,089
you have the central server there is a

00:07:22,549 --> 00:07:29,029
way where which servers and clients can

00:07:26,089 --> 00:07:30,699
use to communicate which is in this case

00:07:29,029 --> 00:07:35,089
a message bus

00:07:30,699 --> 00:07:39,109
mainly rabbitmq you have a system where

00:07:35,089 --> 00:07:42,319
state is persisted sensor uses Redis for

00:07:39,109 --> 00:07:44,769
that and since you also offers an API

00:07:42,319 --> 00:07:51,459
that you can use from command line

00:07:44,769 --> 00:07:51,459
commands or from web user interfaces

00:07:56,320 --> 00:08:05,110
in practice that makes things very

00:08:00,790 --> 00:08:09,370
simple and very performant the

00:08:05,110 --> 00:08:11,110
monitoring agent almost has more

00:08:09,370 --> 00:08:15,460
intelligence than the server in this

00:08:11,110 --> 00:08:18,310
case it registers automatically with the

00:08:15,460 --> 00:08:20,650
server so you simply tell the agent to

00:08:18,310 --> 00:08:24,940
which server it should speak and it

00:08:20,650 --> 00:08:28,690
ratchets registers itself and regularly

00:08:24,940 --> 00:08:33,090
sends keepalive information so that the

00:08:28,690 --> 00:08:33,090
server knows if some client has vanished

00:08:33,419 --> 00:08:43,000
it receives check execution requests

00:08:37,840 --> 00:08:46,620
from the server via the message bus or

00:08:43,000 --> 00:08:51,700
it can also share your checks

00:08:46,620 --> 00:08:54,610
autonomously locally it then executes

00:08:51,700 --> 00:08:59,440
these checks and returns the results of

00:08:54,610 --> 00:09:03,700
these checks back to the server you can

00:08:59,440 --> 00:09:07,060
also feed external events into the

00:09:03,700 --> 00:09:10,150
monitoring client for example from an

00:09:07,060 --> 00:09:14,140
application from shell scripts and other

00:09:10,150 --> 00:09:17,310
places and have these events also be

00:09:14,140 --> 00:09:21,490
transferred to the server that makes

00:09:17,310 --> 00:09:28,860
some things very easy and I'll show you

00:09:21,490 --> 00:09:34,690
an example later so how does the normal

00:09:28,860 --> 00:09:39,160
check cycle work the server has its

00:09:34,690 --> 00:09:42,640
schedule and knows ok some of these

00:09:39,160 --> 00:09:45,340
checks have to be executed minute by

00:09:42,640 --> 00:09:50,490
minute others every 10 minutes or others

00:09:45,340 --> 00:09:54,960
only once a day things like that and

00:09:50,490 --> 00:09:57,700
sense who uses a publish/subscribe

00:09:54,960 --> 00:10:02,520
mechanism and doesn't actually talk

00:09:57,700 --> 00:10:06,850
directly to the clients and that makes

00:10:02,520 --> 00:10:09,960
this very scalable because all the

00:10:06,850 --> 00:10:13,740
server does is publish

00:10:09,960 --> 00:10:17,190
check requests for example hey all you

00:10:13,740 --> 00:10:22,230
web servers out there let's count your

00:10:17,190 --> 00:10:28,260
Apache processes and that's all the

00:10:22,230 --> 00:10:31,700
server does this request is received by

00:10:28,260 --> 00:10:35,340
all the clients that have the web server

00:10:31,700 --> 00:10:41,220
subscription so it's a client setup

00:10:35,340 --> 00:10:45,750
thing here the clients execute the check

00:10:41,220 --> 00:10:50,640
and the check tells them if the result

00:10:45,750 --> 00:10:54,120
is ok warning critical or something else

00:10:50,640 --> 00:10:57,800
but mainly these three states and they

00:10:54,120 --> 00:11:02,550
return these results back to the server

00:10:57,800 --> 00:11:05,070
so basically all the main monitoring

00:11:02,550 --> 00:11:08,190
work is already finished

00:11:05,070 --> 00:11:13,200
a check has been executed and its result

00:11:08,190 --> 00:11:17,160
has been interpreted the server then

00:11:13,200 --> 00:11:21,570
receives these requests and triggers

00:11:17,160 --> 00:11:26,160
event handlers that do something so it

00:11:21,570 --> 00:11:30,210
might be that all kinds of results get

00:11:26,160 --> 00:11:32,790
posted to slack and only the critical

00:11:30,210 --> 00:11:37,290
results get passed on to something like

00:11:32,790 --> 00:11:41,030
pager Duty for example so all the server

00:11:37,290 --> 00:11:45,230
does is basically event handling and

00:11:41,030 --> 00:11:45,230
scheduling new events

00:11:50,930 --> 00:12:00,470
and since things get persisted in Redis

00:11:54,050 --> 00:12:03,529
you have the API that you can use to get

00:12:00,470 --> 00:12:06,830
recent check results to get client state

00:12:03,529 --> 00:12:09,800
to get everything basically that you

00:12:06,830 --> 00:12:13,279
need to display on a dashboard which

00:12:09,800 --> 00:12:17,470
comes included with sensor but you can

00:12:13,279 --> 00:12:22,779
also use third-party dashboards or

00:12:17,470 --> 00:12:22,779
command line interface commands and

00:12:24,970 --> 00:12:33,500
that's how the default dashboard looks

00:12:28,970 --> 00:12:36,920
like it's called Achieva and simply

00:12:33,500 --> 00:12:39,190
lists all the current states of your

00:12:36,920 --> 00:12:39,190
checks

00:12:48,210 --> 00:12:55,950
the API is rest based and gives you

00:12:52,930 --> 00:12:58,390
access to all the internals so you get

00:12:55,950 --> 00:13:02,530
information about the monitoring clients

00:12:58,390 --> 00:13:06,670
about checks if a check is disabled or

00:13:02,530 --> 00:13:10,030
silenced about events results

00:13:06,670 --> 00:13:13,180
you also have an aggregates API that

00:13:10,030 --> 00:13:17,200
allows you to monitor clusters where you

00:13:13,180 --> 00:13:21,790
only are interested in total results not

00:13:17,200 --> 00:13:28,750
the single parts and a stashes API that

00:13:21,790 --> 00:13:31,390
gives you access to the Redis storage as

00:13:28,750 --> 00:13:35,470
I already mentioned checks are the

00:13:31,390 --> 00:13:38,280
central part in a sensor system so let's

00:13:35,470 --> 00:13:38,280
take a closer look

00:13:38,760 --> 00:13:45,370
checks are executed by the sensor client

00:13:42,490 --> 00:13:47,710
are and are simply external applications

00:13:45,370 --> 00:13:50,610
that get launched by the monitoring

00:13:47,710 --> 00:13:56,250
agent so you can use everything that is

00:13:50,610 --> 00:13:56,250
executable shell scripts Nagios checks

00:13:56,460 --> 00:14:01,630
you can build them yourself or you can

00:13:59,110 --> 00:14:04,240
simply download them sensu of course has

00:14:01,630 --> 00:14:07,210
their own github repository where you

00:14:04,240 --> 00:14:08,800
can download a lot of checks for all

00:14:07,210 --> 00:14:12,520
kinds of things that you might want to

00:14:08,800 --> 00:14:21,730
one monitor network appliances servers

00:14:12,520 --> 00:14:26,920
all the whole spectrum by default events

00:14:21,730 --> 00:14:31,780
are only generated if a check returns a

00:14:26,920 --> 00:14:33,839
nonzero result with the exception pardon

00:14:31,780 --> 00:14:33,839
me

00:14:35,610 --> 00:14:45,010
with the exception of metrics checks

00:14:39,730 --> 00:14:52,210
that and events that always trigger an

00:14:45,010 --> 00:14:54,089
event and since the parameters of a

00:14:52,210 --> 00:14:58,920
check might vary between different

00:14:54,089 --> 00:15:00,600
machines since who has a token

00:14:58,920 --> 00:15:04,589
substitution approach

00:15:00,600 --> 00:15:07,880
where you can for example define

00:15:04,589 --> 00:15:11,670
thresholds with a placeholder for

00:15:07,880 --> 00:15:14,819
example the the check the Apache process

00:15:11,670 --> 00:15:17,639
check might have different thresholds on

00:15:14,819 --> 00:15:21,839
different machines and you simply use a

00:15:17,639 --> 00:15:25,259
token process critical for example and

00:15:21,839 --> 00:15:27,959
that token gets replaced with a value

00:15:25,259 --> 00:15:29,790
that depends on the machine for some

00:15:27,959 --> 00:15:35,190
machines it might be 10 for other

00:15:29,790 --> 00:15:38,579
machines it might be a hundred and that

00:15:35,190 --> 00:15:41,880
makes it very easy because you can put

00:15:38,579 --> 00:15:45,149
the actual values in a client specific

00:15:41,880 --> 00:15:47,009
file on each machine and your check

00:15:45,149 --> 00:15:48,899
definitions look the same on every

00:15:47,009 --> 00:15:58,709
machine because they only use these

00:15:48,899 --> 00:16:02,160
placeholders triggered are these checks

00:15:58,709 --> 00:16:04,769
by default by the sensor server so you

00:16:02,160 --> 00:16:09,800
need to define the intervals in which

00:16:04,769 --> 00:16:09,800
these checks need to be executed and

00:16:10,699 --> 00:16:19,680
normally the clients of subscriptions

00:16:15,449 --> 00:16:21,990
reach all the clients that subscribe to

00:16:19,680 --> 00:16:25,430
a certain name for example web servers

00:16:21,990 --> 00:16:29,699
but there's also now a round robin

00:16:25,430 --> 00:16:33,180
subscription where you can define ok web

00:16:29,699 --> 00:16:36,000
servers are these five machines and I'd

00:16:33,180 --> 00:16:39,139
like only one of these machines to

00:16:36,000 --> 00:16:42,120
return a results this time and then

00:16:39,139 --> 00:16:44,790
sensor goes through the list and starts

00:16:42,120 --> 00:16:48,329
from the top and cycles through every

00:16:44,790 --> 00:16:54,600
machine but the check itself is only

00:16:48,329 --> 00:16:57,050
executed by one of them at a time if you

00:16:54,600 --> 00:17:01,529
need to be more flexible you can also

00:16:57,050 --> 00:17:04,110
have the sensor client itself start a

00:17:01,529 --> 00:17:07,470
check you might trigger this with cron

00:17:04,110 --> 00:17:10,140
for example or by simply defining a an

00:17:07,470 --> 00:17:14,130
interval and the sense of client will

00:17:10,140 --> 00:17:16,199
then execute the check autonomously and

00:17:14,130 --> 00:17:17,850
turn the results to the server and of

00:17:16,199 --> 00:17:19,560
course the API also gives you the

00:17:17,850 --> 00:17:26,790
possibility of triggering a check

00:17:19,560 --> 00:17:31,650
manually and that's how a common check

00:17:26,790 --> 00:17:36,750
definition looks like in JSON here I'm

00:17:31,650 --> 00:17:40,620
defining a disk free check it's a status

00:17:36,750 --> 00:17:41,430
check that returns a result in the terms

00:17:40,620 --> 00:17:44,820
of okay

00:17:41,430 --> 00:17:49,920
warning critical I'll have all my

00:17:44,820 --> 00:17:51,830
machines execute this check so all my

00:17:49,920 --> 00:17:55,620
machines need to have the all

00:17:51,830 --> 00:17:58,410
subscription configured I'm using my

00:17:55,620 --> 00:18:00,720
default set of handlers which are

00:17:58,410 --> 00:18:06,240
defined separately so the default set

00:18:00,720 --> 00:18:08,430
might be slack and pager duty and the

00:18:06,240 --> 00:18:12,650
central thing is the command in this

00:18:08,430 --> 00:18:17,660
case I'm reusing a Nagios plug-in and

00:18:12,650 --> 00:18:22,260
here I'm using the substitution tokens

00:18:17,660 --> 00:18:26,760
which get replaced by the sensor client

00:18:22,260 --> 00:18:32,190
and are defined in a client JSON file

00:18:26,760 --> 00:18:37,950
and the default interval for this check

00:18:32,190 --> 00:18:40,680
is 60 seconds and that's what I meant

00:18:37,950 --> 00:18:42,840
that sensor is really easy to install

00:18:40,680 --> 00:18:46,920
because the application itself with all

00:18:42,840 --> 00:18:49,470
that is necessary so Redis and rabbitmq

00:18:46,920 --> 00:18:52,350
come out of the box and then you simply

00:18:49,470 --> 00:18:54,960
add these simple JSON files defying a

00:18:52,350 --> 00:18:57,480
few checks that you most often can

00:18:54,960 --> 00:19:02,880
install from operating system packages

00:18:57,480 --> 00:19:05,400
and you're good to go if you'd like to

00:19:02,880 --> 00:19:08,120
automate that there's for example the

00:19:05,400 --> 00:19:12,930
chef cookbook for sensor where you can

00:19:08,120 --> 00:19:15,960
define where you can use the chef DSL to

00:19:12,930 --> 00:19:18,930
define a check so the sensor cookbook

00:19:15,960 --> 00:19:20,880
has its own sensor check resource here

00:19:18,930 --> 00:19:23,330
I'm defining a check name my sequel

00:19:20,880 --> 00:19:27,900
server I'm defining the command and

00:19:23,330 --> 00:19:31,110
again the handlers and an interval in

00:19:27,900 --> 00:19:33,600
this case 30 second seconds and in this

00:19:31,110 --> 00:19:36,840
case I'd like this to be a standalone

00:19:33,600 --> 00:19:39,390
check which means it gets started by the

00:19:36,840 --> 00:19:43,110
sense of client it doesn't get triggered

00:19:39,390 --> 00:19:46,830
by the server and that's because I'm

00:19:43,110 --> 00:19:49,500
using a very client dependent

00:19:46,830 --> 00:19:57,150
information which in this case is the my

00:19:49,500 --> 00:19:59,700
sequel password and the fact that I'm

00:19:57,150 --> 00:20:07,290
using a password here makes this a bit

00:19:59,700 --> 00:20:10,800
ugly because why well first of all the

00:20:07,290 --> 00:20:14,730
password will be in plain text in the

00:20:10,800 --> 00:20:18,000
check definition on the machine which we

00:20:14,730 --> 00:20:24,750
could use with typical file system

00:20:18,000 --> 00:20:27,630
permissions but also this password since

00:20:24,750 --> 00:20:31,650
it's part of the command will be passed

00:20:27,630 --> 00:20:34,740
back with all the result information so

00:20:31,650 --> 00:20:37,140
the password will go over the network to

00:20:34,740 --> 00:20:39,330
the sensor server and might even be

00:20:37,140 --> 00:20:42,710
displayed on some kind of monitoring

00:20:39,330 --> 00:20:45,540
dashboard and we don't want that and

00:20:42,710 --> 00:20:49,740
that's why sense.you allows you to

00:20:45,540 --> 00:20:52,320
redact information and that's exactly

00:20:49,740 --> 00:20:54,930
what I'm doing here this is a very

00:20:52,320 --> 00:20:57,420
simple client configuration file which

00:20:54,930 --> 00:21:00,540
for example defines its name and address

00:20:57,420 --> 00:21:03,050
it defines it has a list of its

00:21:00,540 --> 00:21:06,590
subscriptions in this case only the base

00:21:03,050 --> 00:21:11,030
subscription whatever that means and

00:21:06,590 --> 00:21:14,100
here I have a redact section that

00:21:11,030 --> 00:21:17,190
defines the list of attributes that I

00:21:14,100 --> 00:21:22,830
don't want to be passed back to the

00:21:17,190 --> 00:21:28,680
server so every attribute that is named

00:21:22,830 --> 00:21:31,800
password will be redacted and in this

00:21:28,680 --> 00:21:33,600
case here I'm defining a client specific

00:21:31,800 --> 00:21:36,300
attribute so all the attributes that

00:21:33,600 --> 00:21:40,440
aren't predefined by sensor will be

00:21:36,300 --> 00:21:41,290
client specific and here it's my sequel

00:21:40,440 --> 00:21:44,650
and

00:21:41,290 --> 00:21:47,950
word together they form an attribute my

00:21:44,650 --> 00:21:50,410
sequel dot password which I can use in

00:21:47,950 --> 00:21:55,060
my check definition so I'll simply use

00:21:50,410 --> 00:21:57,940
the substitution token three columns my

00:21:55,060 --> 00:22:02,200
sequel dot password and again three

00:21:57,940 --> 00:22:04,780
Collins and that's that and when this

00:22:02,200 --> 00:22:07,740
check is executed the information that

00:22:04,780 --> 00:22:14,710
goes back to the server is literally

00:22:07,740 --> 00:22:19,020
command something-something password o

00:22:14,710 --> 00:22:22,690
minus P let's go back to that minus P

00:22:19,020 --> 00:22:24,850
redacted so you see there was a value

00:22:22,690 --> 00:22:32,080
that has been removed by the sensor

00:22:24,850 --> 00:22:36,130
check that's the other kind of check we

00:22:32,080 --> 00:22:38,830
can have a metrics check and that simply

00:22:36,130 --> 00:22:43,150
execute some kind of Ruby script named

00:22:38,830 --> 00:22:45,730
load metrics it gets executed every 10

00:22:43,150 --> 00:22:54,540
seconds by all the subscribers to

00:22:45,730 --> 00:22:54,540
production and all this check does is

00:22:55,140 --> 00:23:03,730
output the current load values in a

00:22:59,610 --> 00:23:06,850
common format this is passed back as

00:23:03,730 --> 00:23:09,330
payload to the server and now it's up to

00:23:06,850 --> 00:23:12,280
the handlers that I'm actually using

00:23:09,330 --> 00:23:16,570
what happens with this information so I

00:23:12,280 --> 00:23:20,650
might have a graphite handler that

00:23:16,570 --> 00:23:23,290
reacts to metrics results and takes

00:23:20,650 --> 00:23:27,030
these results and forwards them to some

00:23:23,290 --> 00:23:27,030
kind of graphite server

00:23:31,870 --> 00:23:40,670
still quite new is the concept of check

00:23:36,170 --> 00:23:45,560
hooks which are additional commands that

00:23:40,670 --> 00:23:48,710
the sensor client can execute if a check

00:23:45,560 --> 00:23:52,400
returns some kind of result in this case

00:23:48,710 --> 00:23:59,300
I've highlighted this hook which does an

00:23:52,400 --> 00:24:04,490
engine ex restart if the check result is

00:23:59,300 --> 00:24:07,670
nonzero if the if the check result is

00:24:04,490 --> 00:24:11,480
critical sorry if it's critical it tries

00:24:07,670 --> 00:24:14,840
to restart nginx and in general if the

00:24:11,480 --> 00:24:16,760
result is nonzero so this will cover

00:24:14,840 --> 00:24:22,220
warnings as well

00:24:16,760 --> 00:24:26,780
I simply execute PS a uux and that will

00:24:22,220 --> 00:24:29,570
be passed back to the server so these

00:24:26,780 --> 00:24:33,740
check hooks allow you to do some kind of

00:24:29,570 --> 00:24:39,590
simple problem mitigation of course this

00:24:33,740 --> 00:24:42,620
is not very intelligent so for example

00:24:39,590 --> 00:24:45,740
the chef the sense of client does not

00:24:42,620 --> 00:24:47,300
know the checks execution history it

00:24:45,740 --> 00:24:50,900
does not know when this check was

00:24:47,300 --> 00:24:54,350
executed last it does not know if this

00:24:50,900 --> 00:24:57,200
check is flapping it does not know the

00:24:54,350 --> 00:25:02,270
number of state occurrences so has this

00:24:57,200 --> 00:25:05,170
been critical for three days now or if

00:25:02,270 --> 00:25:08,900
the check is silenced on the server side

00:25:05,170 --> 00:25:17,630
so you're pretty limited but you can do

00:25:08,900 --> 00:25:20,240
some kind of auto remediation another

00:25:17,630 --> 00:25:22,720
way I can trigger events that get passed

00:25:20,240 --> 00:25:27,890
to the server is by simply using the

00:25:22,720 --> 00:25:32,450
local client port 3030 and you can

00:25:27,890 --> 00:25:38,020
access that port via TCP via UDP and now

00:25:32,450 --> 00:25:41,300
also via HTTP and that allows you to

00:25:38,020 --> 00:25:44,330
feed any information back to the sensor

00:25:41,300 --> 00:25:45,320
server in this case I'm defining some

00:25:44,330 --> 00:25:48,830
kind of

00:25:45,320 --> 00:25:52,909
external event called my check I'll pass

00:25:48,830 --> 00:25:55,309
it some output and status that decides

00:25:52,909 --> 00:25:58,909
if a handler will be triggered on the

00:25:55,309 --> 00:26:02,769
server side and passed that into the TCP

00:25:58,909 --> 00:26:07,610
port 3030 and that's that there's a nice

00:26:02,769 --> 00:26:13,070
helper script on github that makes these

00:26:07,610 --> 00:26:18,409
things easier and there's some nice

00:26:13,070 --> 00:26:25,669
additional possibility that you can use

00:26:18,409 --> 00:26:29,059
if you pass an TT a TTL attribute with

00:26:25,669 --> 00:26:31,429
these information you can build some

00:26:29,059 --> 00:26:35,690
kind of Deadman switch where you can say

00:26:31,429 --> 00:26:38,419
okay this result needs to occur every

00:26:35,690 --> 00:26:40,340
minute for example and as soon as that

00:26:38,419 --> 00:26:41,809
changes the server will know that

00:26:40,340 --> 00:26:44,539
something's fishy

00:26:41,809 --> 00:26:45,919
for example if that script doesn't get

00:26:44,539 --> 00:26:48,620
executed anymore

00:26:45,919 --> 00:26:51,320
the send the sensor server will know

00:26:48,620 --> 00:26:54,259
okay there was some kind of thing named

00:26:51,320 --> 00:26:56,629
my check that had a TTL of one minute

00:26:54,259 --> 00:26:59,870
and hasn't occurred in the last minute

00:26:56,629 --> 00:27:03,169
and then you can also again trigger some

00:26:59,870 --> 00:27:06,740
kind of handler and you don't have even

00:27:03,169 --> 00:27:08,840
to have a proper check for that simply

00:27:06,740 --> 00:27:13,570
the fact that you passed in something

00:27:08,840 --> 00:27:16,669
that had a TTL lets the sensor server

00:27:13,570 --> 00:27:21,200
make some kind of important decisions

00:27:16,669 --> 00:27:24,220
here I've been talking a lot about

00:27:21,200 --> 00:27:29,149
handlers let's take a closer look

00:27:24,220 --> 00:27:32,059
handlers are simply applications that

00:27:29,149 --> 00:27:36,169
get executed by the central server and

00:27:32,059 --> 00:27:39,649
they get past the event information on

00:27:36,169 --> 00:27:42,230
their standard in so again you can use

00:27:39,649 --> 00:27:46,690
any kind of application shell script

00:27:42,230 --> 00:27:57,320
Ruby script whatever as an sensu handler

00:27:46,690 --> 00:27:58,669
they can be accessed via TCP UDP then

00:27:57,320 --> 00:28:02,779
there are

00:27:58,669 --> 00:28:07,039
spot handlers that put information back

00:28:02,779 --> 00:28:12,340
on RabbitMQ and you can define handler

00:28:07,039 --> 00:28:17,059
sets that group a number of handlers so

00:28:12,340 --> 00:28:20,679
you can use that as an alias method

00:28:17,059 --> 00:28:25,909
where you have a handler that also has

00:28:20,679 --> 00:28:28,309
is accessible on another name simply by

00:28:25,909 --> 00:28:31,700
defining a group with that name or you

00:28:28,309 --> 00:28:35,419
can use it in the original sense and

00:28:31,700 --> 00:28:40,220
have some set of handlers that get the

00:28:35,419 --> 00:28:43,429
same event data and of course there are

00:28:40,220 --> 00:28:46,340
a lot of handlers available already for

00:28:43,429 --> 00:28:48,679
example for third-party integrations for

00:28:46,340 --> 00:28:54,529
things like pager duty for things like

00:28:48,679 --> 00:28:58,570
slack IRC handlers everywhere that you'd

00:28:54,529 --> 00:29:04,159
like to pass monitoring events can be

00:28:58,570 --> 00:29:07,759
reached by handlers so the common

00:29:04,159 --> 00:29:10,369
handlers that are regularly used our

00:29:07,759 --> 00:29:12,710
email handlers that simply send your

00:29:10,369 --> 00:29:15,830
event data to an email address

00:29:12,710 --> 00:29:18,950
pager duty you can of course pass

00:29:15,830 --> 00:29:22,369
especially metrics events to graphite or

00:29:18,950 --> 00:29:25,009
you can use slack to get a running log

00:29:22,369 --> 00:29:33,499
of your check results and here's an

00:29:25,009 --> 00:29:35,899
example for that if you'd like to build

00:29:33,499 --> 00:29:41,080
your own handler that's also very easy

00:29:35,899 --> 00:29:44,899
in this case I've done a small Ruby

00:29:41,080 --> 00:29:50,179
example that simply takes the event data

00:29:44,899 --> 00:29:53,299
from standard in parses it as Jason so

00:29:50,179 --> 00:29:56,600
I'll get a nice hash and then I'll

00:29:53,299 --> 00:29:59,059
simply create a file system file and

00:29:56,600 --> 00:30:02,659
write the event data into that file

00:29:59,059 --> 00:30:04,639
that's all this does of course that's

00:30:02,659 --> 00:30:07,269
not very practical but it's a nice

00:30:04,639 --> 00:30:07,269
example

00:30:12,160 --> 00:30:21,770
and the related configuration file is

00:30:18,340 --> 00:30:26,630
very simple as well I'll simply define a

00:30:21,770 --> 00:30:29,900
new handler called file it's of the type

00:30:26,630 --> 00:30:34,100
pipe so it gets the event data via

00:30:29,900 --> 00:30:36,020
standard in and I'll define the path to

00:30:34,100 --> 00:30:40,850
the command that the sensor server

00:30:36,020 --> 00:30:42,830
should execute and that's that other

00:30:40,850 --> 00:30:45,230
hand as might of course have additional

00:30:42,830 --> 00:30:48,920
attributes for example you need an API

00:30:45,230 --> 00:30:52,700
token for page @ut or things like that

00:30:48,920 --> 00:30:56,750
so the JSON hash might get more complex

00:30:52,700 --> 00:30:59,120
with other handlers but still it's easy

00:30:56,750 --> 00:31:07,370
to understand and I like that very much

00:30:59,120 --> 00:31:10,370
about sensor another thing that stands

00:31:07,370 --> 00:31:13,550
between the sensor server and the event

00:31:10,370 --> 00:31:16,700
handlers are filters where you can

00:31:13,550 --> 00:31:20,810
define conditions under which event data

00:31:16,700 --> 00:31:23,750
is passed to a certain handler you will

00:31:20,810 --> 00:31:27,800
simply put a filter in front of a

00:31:23,750 --> 00:31:31,490
handler and can define conditions that

00:31:27,800 --> 00:31:34,130
take parts of the event data to decide

00:31:31,490 --> 00:31:39,110
should this event actually be handled or

00:31:34,130 --> 00:31:41,780
not so that might be handy to cut down

00:31:39,110 --> 00:31:45,350
on the number of slack messages in this

00:31:41,780 --> 00:31:49,310
case this is a handler that only a

00:31:45,350 --> 00:31:53,150
filter sorry that only activates the

00:31:49,310 --> 00:31:56,120
handler if the state has changed so it

00:31:53,150 --> 00:32:01,390
only generate the handler only gets

00:31:56,120 --> 00:32:05,630
executed if the check state changes and

00:32:01,390 --> 00:32:09,290
it only applies to checks that have the

00:32:05,630 --> 00:32:15,490
interval 60 seconds what I've done here

00:32:09,290 --> 00:32:18,770
is mainly I'll filter on the occurrences

00:32:15,490 --> 00:32:21,740
attribute of an event that tells me how

00:32:18,770 --> 00:32:23,230
much how many times has this event

00:32:21,740 --> 00:32:27,249
occurred

00:32:23,230 --> 00:32:31,429
normally I'll simply trigger if

00:32:27,249 --> 00:32:35,509
occurrences are one so just freshly

00:32:31,429 --> 00:32:36,289
changed so normally I'll get okay okay

00:32:35,509 --> 00:32:41,659
okay

00:32:36,289 --> 00:32:44,720
and this filter only lets the first okay

00:32:41,659 --> 00:32:48,859
pass and when the check goes to critical

00:32:44,720 --> 00:32:50,690
again the first critical goes back to

00:32:48,859 --> 00:32:53,869
the handler and the second and all

00:32:50,690 --> 00:32:57,649
subsequent critical don't and if it

00:32:53,869 --> 00:33:02,480
changes back well you know the drill the

00:32:57,649 --> 00:33:08,450
only exception is if I resolve a check

00:33:02,480 --> 00:33:12,019
manually the occurrences value gets

00:33:08,450 --> 00:33:15,019
reset completely and is zero so I'll

00:33:12,019 --> 00:33:16,009
have an or condition there if the action

00:33:15,019 --> 00:33:20,919
was resolved

00:33:16,009 --> 00:33:20,919
I'd like to trigger this handler as well

00:33:21,009 --> 00:33:29,330
you can also use this to only trigger

00:33:24,499 --> 00:33:31,789
events during office times so since you

00:33:29,330 --> 00:33:34,639
also offers conditions that are based on

00:33:31,789 --> 00:33:43,580
on the current time things like that you

00:33:34,639 --> 00:33:46,369
can be very creative here another useful

00:33:43,580 --> 00:33:48,619
function of sense who are aggregates and

00:33:46,369 --> 00:33:57,279
that that gets interesting if you are

00:33:48,619 --> 00:33:57,279
monitoring clusters of a server because

00:33:57,309 --> 00:34:03,769
aggregates require contextual or

00:34:01,039 --> 00:34:05,779
knowledge these are only available on

00:34:03,769 --> 00:34:10,099
the server side you can you can't use

00:34:05,779 --> 00:34:15,859
archicad's on the client side you simply

00:34:10,099 --> 00:34:19,730
define a check as aggregate so since we

00:34:15,859 --> 00:34:23,089
will collect this information and say

00:34:19,730 --> 00:34:27,819
okay for all my web servers for example

00:34:23,089 --> 00:34:32,419
I got three okay's and one critical and

00:34:27,819 --> 00:34:35,840
it writes that into Redis and these in

00:34:32,419 --> 00:34:39,260
this information is then available

00:34:35,840 --> 00:34:43,790
via the sensor API and there is for

00:34:39,260 --> 00:34:48,800
example the Czech aggregate plug-in that

00:34:43,790 --> 00:34:51,350
then queries the API and might for

00:34:48,800 --> 00:34:55,940
example ask okay give me the aggregate

00:34:51,350 --> 00:35:00,050
for web servers and then trigger an

00:34:55,940 --> 00:35:07,370
event if I have more than two criticals

00:35:00,050 --> 00:35:10,400
in there that makes it very easy to

00:35:07,370 --> 00:35:13,670
define cluster checks where you say okay

00:35:10,400 --> 00:35:16,100
I'm fine if only ten percent of my web

00:35:13,670 --> 00:35:19,660
servers are offline I can deal with that

00:35:16,100 --> 00:35:24,110
and that doesn't need to trigger a

00:35:19,660 --> 00:35:27,620
nightly alarm for example and as soon as

00:35:24,110 --> 00:35:30,680
the number of criticals goes above 10

00:35:27,620 --> 00:35:34,520
percent then the czech aggregate check

00:35:30,680 --> 00:35:46,600
will trigger an alert as any critical

00:35:34,520 --> 00:35:50,300
check will do these are the basics and

00:35:46,600 --> 00:35:51,460
so far it should be pretty

00:35:50,300 --> 00:35:54,710
straightforward

00:35:51,460 --> 00:35:58,490
now let's get back let's get to practice

00:35:54,710 --> 00:36:04,850
how do I operate sensu on a day-to-day

00:35:58,490 --> 00:36:06,170
basis well apart from the sensor

00:36:04,850 --> 00:36:10,880
dashboard that I showed you earlier

00:36:06,170 --> 00:36:17,030
there are command line commands that you

00:36:10,880 --> 00:36:23,980
can use for common things mainly the

00:36:17,030 --> 00:36:30,500
sensor CLI command and there are also

00:36:23,980 --> 00:36:33,880
api connected things even chat ops is

00:36:30,500 --> 00:36:37,060
possible there is for example a you bot

00:36:33,880 --> 00:36:40,790
integration for sensu where you can

00:36:37,060 --> 00:36:43,040
request give me all my critical checks

00:36:40,790 --> 00:36:46,330
at the moment and you'll get them listed

00:36:43,040 --> 00:36:46,330
in slack for example

00:37:01,380 --> 00:37:13,630
another thing that's often for cotton is

00:37:04,920 --> 00:37:15,849
monitoring your monitoring sometimes

00:37:13,630 --> 00:37:18,369
there are times when I ask myself is

00:37:15,849 --> 00:37:26,200
monitoring still up because my uncoiled

00:37:18,369 --> 00:37:29,130
shift is eerily quiet so happily sense

00:37:26,200 --> 00:37:32,920
who uses very common components that are

00:37:29,130 --> 00:37:35,829
well proven in practice and where there

00:37:32,920 --> 00:37:39,450
are best practices for monitoring them

00:37:35,829 --> 00:37:42,910
so let's get back to the architecture

00:37:39,450 --> 00:37:45,510
how do you monitor Redis well that's a

00:37:42,910 --> 00:37:49,770
solve problem how do you monitor

00:37:45,510 --> 00:37:54,940
RabbitMQ also solved how do you monitor

00:37:49,770 --> 00:38:01,240
an API that's reachable via HTTP well

00:37:54,940 --> 00:38:09,549
that's easy and the the server can be

00:38:01,240 --> 00:38:13,210
monitored via its port the clients can

00:38:09,549 --> 00:38:16,319
be simply monitored by some kind of

00:38:13,210 --> 00:38:19,890
local software like monitor for example

00:38:16,319 --> 00:38:24,460
that make sure that clients are running

00:38:19,890 --> 00:38:30,869
but if a client disappears its keepalive

00:38:24,460 --> 00:38:33,849
check will trigger the server anyway so

00:38:30,869 --> 00:38:36,880
monitoring sends you in totally it's

00:38:33,849 --> 00:38:40,869
very straightforward one practical tip

00:38:36,880 --> 00:38:44,200
make sure to monitor the RabbitMQ ready

00:38:40,869 --> 00:38:47,200
queue because that might become a

00:38:44,200 --> 00:38:49,420
bottleneck especially if you do metrics

00:38:47,200 --> 00:38:54,309
monitoring there's a huge difference

00:38:49,420 --> 00:39:01,120
between executing say 30 health checks

00:38:54,309 --> 00:39:05,110
on a machine every minute or collect

00:39:01,120 --> 00:39:07,370
250 metrics every 10 seconds and

00:39:05,110 --> 00:39:11,450
especially if you're doing this kind of

00:39:07,370 --> 00:39:13,760
many metrics monitoring via sensu you

00:39:11,450 --> 00:39:17,800
might want to check that rabbitmq

00:39:13,760 --> 00:39:23,210
doesn't back up but other than that

00:39:17,800 --> 00:39:26,870
sensor is very easy to deal with how

00:39:23,210 --> 00:39:30,140
does it scale if your infrastructure is

00:39:26,870 --> 00:39:32,950
growing as ours is it's important that

00:39:30,140 --> 00:39:37,550
your monitoring system can keep up and

00:39:32,950 --> 00:39:40,910
again this is very easy for the standard

00:39:37,550 --> 00:39:46,760
open-source components there are proven

00:39:40,910 --> 00:39:50,240
ways of scaling Redis and since a recent

00:39:46,760 --> 00:39:53,030
version of sensu it supports it has

00:39:50,240 --> 00:39:57,890
native support for Redis Sentinel so

00:39:53,030 --> 00:40:02,150
running Redis in a scalable and even

00:39:57,890 --> 00:40:06,170
highly available configuration is easy

00:40:02,150 --> 00:40:10,340
there are known ways to scale RabbitMQ

00:40:06,170 --> 00:40:13,720
and in with regard to sensor itself

00:40:10,340 --> 00:40:16,970
sensor makes things very easy again

00:40:13,720 --> 00:40:22,280
scaling the API if at all necessary is

00:40:16,970 --> 00:40:25,610
like scaling any web application put H a

00:40:22,280 --> 00:40:30,770
proxy in front of a few web front-ends

00:40:25,610 --> 00:40:32,660
and you're done and in terms of scaling

00:40:30,770 --> 00:40:36,020
the server which might become a

00:40:32,660 --> 00:40:39,440
bottleneck if you have to execute a lot

00:40:36,020 --> 00:40:41,810
of event handlers is easy too because

00:40:39,440 --> 00:40:47,150
you can simply spin up more sensor

00:40:41,810 --> 00:40:50,200
servers connect them to RabbitMQ they'll

00:40:47,150 --> 00:40:53,320
even do an automatic leader election and

00:40:50,200 --> 00:40:53,320
you're good

00:40:58,690 --> 00:41:04,510
what if you have multiple data centers

00:41:07,300 --> 00:41:14,810
the most straightforward approach would

00:41:10,820 --> 00:41:20,090
be to simply run sensu at a central

00:41:14,810 --> 00:41:23,660
location and have every client connect

00:41:20,090 --> 00:41:33,140
to your rabbitmq at the central location

00:41:23,660 --> 00:41:36,170
which then works as normal this

00:41:33,140 --> 00:41:39,320
architecture is very simple you don't

00:41:36,170 --> 00:41:44,630
need any additional infrastructure you

00:41:39,320 --> 00:41:48,620
have centralized alert handling but the

00:41:44,630 --> 00:41:54,190
downsides are pretty easy to spot as

00:41:48,620 --> 00:41:59,420
well you won't be able to distinguish

00:41:54,190 --> 00:42:03,820
the breakdown of a van connection from

00:41:59,420 --> 00:42:07,610
client timeouts so if a client doesn't

00:42:03,820 --> 00:42:10,220
report back within the default of three

00:42:07,610 --> 00:42:12,380
minutes which you can change you don't

00:42:10,220 --> 00:42:14,930
know is it simply because we have some

00:42:12,380 --> 00:42:19,220
kind of network outage or is it actually

00:42:14,930 --> 00:42:21,590
the client that has disappeared and of

00:42:19,220 --> 00:42:24,140
course if you have lots of remote

00:42:21,590 --> 00:42:27,050
clients that will generate a lot of

00:42:24,140 --> 00:42:29,990
network traffic and a lot of TCP

00:42:27,050 --> 00:42:35,060
connections made which might become a

00:42:29,990 --> 00:42:38,930
bottleneck and from a user perspective

00:42:35,060 --> 00:42:41,780
all these clients seem to be in a single

00:42:38,930 --> 00:42:47,060
data center you can't really tell which

00:42:41,780 --> 00:42:49,910
client hails from where so there are two

00:42:47,060 --> 00:42:52,810
other approaches that you can use one is

00:42:49,910 --> 00:42:56,930
the federated approach where you put a

00:42:52,810 --> 00:43:00,830
rapid MQ broker into each single data

00:42:56,930 --> 00:43:04,520
center have the clients report locally

00:43:00,830 --> 00:43:08,720
to this broker and then collect all

00:43:04,520 --> 00:43:11,450
these messages in another central

00:43:08,720 --> 00:43:17,480
eh booze and then have the default

00:43:11,450 --> 00:43:19,700
sensor server setup you have a little

00:43:17,480 --> 00:43:22,760
bit more infrastructure in each data

00:43:19,700 --> 00:43:28,250
center but it's only a rabbitmq an

00:43:22,760 --> 00:43:36,560
instance thus the alerts again get

00:43:28,250 --> 00:43:39,640
triggered from a central instance the

00:43:36,560 --> 00:43:41,810
the downside is your RabbitMQ

00:43:39,640 --> 00:43:43,840
architecture has become a bit more

00:43:41,810 --> 00:43:47,540
complex

00:43:43,840 --> 00:43:51,020
again all clients still seem to appear

00:43:47,540 --> 00:43:55,610
from a single data center yet you don't

00:43:51,020 --> 00:43:58,730
have that separation again and if your

00:43:55,610 --> 00:44:06,820
network connections are in stable you

00:43:58,730 --> 00:44:16,120
might get a lot of keep live alerts that

00:44:06,820 --> 00:44:18,230
actually come from a network outage and

00:44:16,120 --> 00:44:21,230
then you can put even more

00:44:18,230 --> 00:44:23,930
infrastructure into each data center in

00:44:21,230 --> 00:44:26,840
this case the full sense will set up

00:44:23,930 --> 00:44:30,050
with a message boost and a sense of

00:44:26,840 --> 00:44:32,990
server in each data center now you have

00:44:30,050 --> 00:44:36,500
pretty much localized your monitoring

00:44:32,990 --> 00:44:38,210
and aren't as dependent on white area

00:44:36,500 --> 00:44:41,450
network infrastructure anymore and

00:44:38,210 --> 00:44:44,980
simply have your dashboard connect to

00:44:41,450 --> 00:44:50,140
multiple servers which is also possible

00:44:44,980 --> 00:44:53,420
network outages will not create flapping

00:44:50,140 --> 00:44:58,100
sensor will still work if a whole data

00:44:53,420 --> 00:45:00,730
center goes down this infrastructure is

00:44:58,100 --> 00:45:06,280
easier to understand because you have a

00:45:00,730 --> 00:45:06,280
simple setup only per data center and

00:45:08,410 --> 00:45:15,830
now the dashboard can distinguish where

00:45:13,070 --> 00:45:20,870
the events come from because they come

00:45:15,830 --> 00:45:22,640
from different servers but that's that

00:45:20,870 --> 00:45:32,809
comes with its price

00:45:22,640 --> 00:45:36,099
as you can see sensor development makes

00:45:32,809 --> 00:45:40,519
pretty fast progress at the moment and

00:45:36,099 --> 00:45:44,359
so a few things that are down the

00:45:40,519 --> 00:45:46,099
pipeline are scheduled maintenance which

00:45:44,359 --> 00:45:51,339
sensor doesn't have at the moment you

00:45:46,099 --> 00:45:56,359
can silence checks but they this will be

00:45:51,339 --> 00:46:00,430
instantly so so far you'd have to use

00:45:56,359 --> 00:46:04,630
something like the old classic at to

00:46:00,430 --> 00:46:07,940
execute silences at the right time

00:46:04,630 --> 00:46:11,119
scheduled maintenance will be there soon

00:46:07,940 --> 00:46:14,450
and sensor promised official docker

00:46:11,119 --> 00:46:18,579
images that will make setup even easier

00:46:14,450 --> 00:46:21,529
than it is now and in the long-term

00:46:18,579 --> 00:46:25,789
sense the sensor core team is already

00:46:21,529 --> 00:46:30,789
working on a version 2.0 that won't be

00:46:25,789 --> 00:46:30,789
written in Ruby anymore but in golang

00:46:40,370 --> 00:46:53,190
so to summarize it's really easy to get

00:46:45,690 --> 00:46:56,970
started with sensu and it can handle

00:46:53,190 --> 00:46:59,820
growth which basically saved our asses

00:46:56,970 --> 00:47:03,210
back then because we could instantly

00:46:59,820 --> 00:47:05,730
take our Nagios checks use them with

00:47:03,210 --> 00:47:09,960
sensu and it's a much increased

00:47:05,730 --> 00:47:13,140
performance and now we got our check

00:47:09,960 --> 00:47:19,970
results within 30 seconds and not within

00:47:13,140 --> 00:47:19,970
three minutes as it was with Nagios and

00:47:20,420 --> 00:47:26,250
you can use the sensor infrastructure

00:47:23,670 --> 00:47:28,790
that you have in place to collect

00:47:26,250 --> 00:47:32,430
metrics as well so you don't necessarily

00:47:28,790 --> 00:47:34,950
need something like collect D or starts

00:47:32,430 --> 00:47:37,800
D or something like that you can simply

00:47:34,950 --> 00:47:42,360
execute metrics checks like the simple

00:47:37,800 --> 00:47:48,510
script I showed you collect data have

00:47:42,360 --> 00:47:51,030
them travel across RabbitMQ back to the

00:47:48,510 --> 00:47:54,240
server which then passes it on to your

00:47:51,030 --> 00:47:58,110
metrics database which might be in flux

00:47:54,240 --> 00:48:01,830
TB or graphite or something a similar so

00:47:58,110 --> 00:48:04,350
you don't need necessarily need metrics

00:48:01,830 --> 00:48:07,920
collection agents as well if you have

00:48:04,350 --> 00:48:10,740
sensor in place and since who integrates

00:48:07,920 --> 00:48:14,910
with a lot of services that are common

00:48:10,740 --> 00:48:18,420
in in our space so it's it's easy to use

00:48:14,910 --> 00:48:21,290
with existing services and

00:48:18,420 --> 00:48:21,290
infrastructure

00:48:24,760 --> 00:48:40,330
or shortly monitoring love thank you so

00:48:37,210 --> 00:48:43,480
thank you young so we have a lot of time

00:48:40,330 --> 00:48:44,080
for questions left if it's okay yeah

00:48:43,480 --> 00:48:50,140
sure

00:48:44,080 --> 00:48:55,120
question shoot one question can I make

00:48:50,140 --> 00:48:58,810
conditioner filters like if some Network

00:48:55,120 --> 00:49:03,240
metrics are not right I trigger alerts

00:48:58,810 --> 00:49:05,860
bird for example the one broke down and

00:49:03,240 --> 00:49:08,050
I don't want to trigger an alert from

00:49:05,860 --> 00:49:11,380
the data center because it's a problem

00:49:08,050 --> 00:49:13,960
in a when can I filter it on one side so

00:49:11,380 --> 00:49:16,870
it's only excuse me right if there's a

00:49:13,960 --> 00:49:18,700
real problem inside the data center so

00:49:16,870 --> 00:49:21,910
basically you want to avoid false

00:49:18,700 --> 00:49:24,850
positives if your network goes down you

00:49:21,910 --> 00:49:28,180
don't want to have hundreds of checks

00:49:24,850 --> 00:49:31,230
trigger and alert yeah that's possible

00:49:28,180 --> 00:49:36,010
that's actually possible by combining

00:49:31,230 --> 00:49:38,020
event data and well your check will be a

00:49:36,010 --> 00:49:39,700
bit more complex and you'll need a bit

00:49:38,020 --> 00:49:43,210
more complex of a filter but that's

00:49:39,700 --> 00:49:45,760
possible yeah so it happens on the

00:49:43,210 --> 00:49:47,410
client or server side whatever is more

00:49:45,760 --> 00:49:49,450
reasonable it has to happen on the

00:49:47,410 --> 00:49:54,490
server side because the server has all

00:49:49,450 --> 00:50:04,060
the contextual information and then you

00:49:54,490 --> 00:50:08,590
can you can do things like that what

00:50:04,060 --> 00:50:11,350
about hosts that are only API or web

00:50:08,590 --> 00:50:16,840
servers with no agent installed on it

00:50:11,350 --> 00:50:24,220
could I configure and host and sensual

00:50:16,840 --> 00:50:26,740
that is only available via HTTP or just

00:50:24,220 --> 00:50:29,820
like something in the low plans

00:50:26,740 --> 00:50:32,830
appliance also mm-hmm

00:50:29,820 --> 00:50:36,310
the only problem you have to use a to to

00:50:32,830 --> 00:50:38,289
solve here is where does my HTTP check

00:50:36,310 --> 00:50:41,019
get executed if it can't

00:50:38,289 --> 00:50:43,239
beyond the machine itself simply by

00:50:41,019 --> 00:50:45,699
connecting to localhost port 80 for

00:50:43,239 --> 00:50:46,599
example you simply put that check on

00:50:45,699 --> 00:50:51,519
another machine

00:50:46,599 --> 00:50:54,789
and tell it okay execute the normal say

00:50:51,519 --> 00:50:57,549
Nagios HTTP check but with this address

00:50:54,789 --> 00:51:00,160
and then need something like and work a

00:50:57,549 --> 00:51:02,439
node yeah okay you'll always need some

00:51:00,160 --> 00:51:05,169
some place where you execute these

00:51:02,439 --> 00:51:07,359
checks most of the time it will be the

00:51:05,169 --> 00:51:09,999
Machine the service is running on but

00:51:07,359 --> 00:51:12,130
sometimes you may have to do it

00:51:09,999 --> 00:51:15,249
externally or may want to do it

00:51:12,130 --> 00:51:17,559
externally because simply that I can

00:51:15,249 --> 00:51:20,140
talk to port 80 on localhost doesn't

00:51:17,559 --> 00:51:22,359
mean that my website is available is

00:51:20,140 --> 00:51:24,669
available from the outside so you might

00:51:22,359 --> 00:51:28,650
even want to have that external machine

00:51:24,669 --> 00:51:28,650
that does HTTP checks from the outside

00:51:34,949 --> 00:51:41,499
can you please tell a little more about

00:51:37,150 --> 00:51:44,409
the size or the sizing you have in high

00:51:41,499 --> 00:51:48,699
steel or your infrastructure and how you

00:51:44,409 --> 00:51:50,890
solved your setup with sensu because you

00:51:48,699 --> 00:51:54,339
said it solved your problem with mario's

00:51:50,890 --> 00:51:56,589
but it's I think it's it's more clear

00:51:54,339 --> 00:51:58,989
when you explain how you solved it and

00:51:56,589 --> 00:52:00,549
what was your problem exactly and how

00:51:58,989 --> 00:52:03,549
big is your infrastructure to get a

00:52:00,549 --> 00:52:05,289
feeling that so back then our

00:52:03,549 --> 00:52:08,709
infrastructure wasn't that impressive

00:52:05,289 --> 00:52:13,089
but the thing is we had the standard

00:52:08,709 --> 00:52:18,400
Nagios setup and the number of checks we

00:52:13,089 --> 00:52:21,809
had still started to cause check latency

00:52:18,400 --> 00:52:26,380
as I said in in the range of minutes and

00:52:21,809 --> 00:52:29,739
of course that's that's not optimal so

00:52:26,380 --> 00:52:34,589
what we did was move all our critical

00:52:29,739 --> 00:52:37,689
checks over to sensu which took us about

00:52:34,589 --> 00:52:40,359
less than a week to get sensor running

00:52:37,689 --> 00:52:45,489
and get our all our checks configured

00:52:40,359 --> 00:52:48,459
there since we used chef which we

00:52:45,489 --> 00:52:50,709
already had to automate the setup of

00:52:48,459 --> 00:52:51,880
each client that was quite

00:52:50,709 --> 00:52:58,240
straightforward as

00:52:51,880 --> 00:53:00,640
and at that stage our current problem

00:52:58,240 --> 00:53:02,890
was already solved because a sense who

00:53:00,640 --> 00:53:04,630
handled that amount of checks basically

00:53:02,890 --> 00:53:11,080
the same amount that we had with Nagios

00:53:04,630 --> 00:53:13,260
much more gracefully and so the the

00:53:11,080 --> 00:53:16,120
bottleneck was removed at that stage

00:53:13,260 --> 00:53:19,080
since then our infrastructure has grown

00:53:16,120 --> 00:53:23,140
so we we run a few hundred servers

00:53:19,080 --> 00:53:26,290
dedicated servers at heads now on on

00:53:23,140 --> 00:53:33,010
each of these machines we have lots of

00:53:26,290 --> 00:53:36,460
Linux containers that act like hosts so

00:53:33,010 --> 00:53:39,730
they each have even a chef client and a

00:53:36,460 --> 00:53:42,700
sense of client so the number of sensor

00:53:39,730 --> 00:53:46,120
hosts at the moment is probably in the

00:53:42,700 --> 00:53:49,270
range of 5 to 700 somewhere like this

00:53:46,120 --> 00:53:52,300
with the typical status checks that you

00:53:49,270 --> 00:53:55,090
have for each machine load CPU disk

00:53:52,300 --> 00:53:59,650
space and other thing and additional

00:53:55,090 --> 00:54:02,770
checks depending on services and these

00:53:59,650 --> 00:54:08,290
checks are executed between 10 seconds

00:54:02,770 --> 00:54:10,480
and 60 seconds some checks only run in

00:54:08,290 --> 00:54:15,400
intervals of half an hour for example

00:54:10,480 --> 00:54:18,970
disk space has a higher interval and so

00:54:15,400 --> 00:54:23,220
far we haven't run into any scalability

00:54:18,970 --> 00:54:27,130
issue at all so we are still running the

00:54:23,220 --> 00:54:30,690
simple set up with one central server

00:54:27,130 --> 00:54:36,240
one RabbitMQ and one Redis instance and

00:54:30,690 --> 00:54:40,420
there is no load issues at all so we

00:54:36,240 --> 00:54:43,810
expect to be able to grow a lot by a

00:54:40,420 --> 00:54:45,870
multitude without sense of breaking a

00:54:43,810 --> 00:54:45,870
sweat

00:54:53,430 --> 00:55:00,130
the data the size of the of the raddest

00:54:56,859 --> 00:55:04,300
database that sensor uses yeah the size

00:55:00,130 --> 00:55:06,130
of the collected data I wouldn't be able

00:55:04,300 --> 00:55:08,220
to tell you from the top of my head but

00:55:06,130 --> 00:55:12,670
I can look it up for you if you like

00:55:08,220 --> 00:55:16,869
it's interesting but I don't expect it

00:55:12,670 --> 00:55:22,079
to be very big because it's it's only

00:55:16,869 --> 00:55:25,890
check history for for recent checks and

00:55:22,079 --> 00:55:30,130
basically the the collection of states

00:55:25,890 --> 00:55:32,619
if a client has been silenced or if a

00:55:30,130 --> 00:55:36,369
check has been silenced or deactivated

00:55:32,619 --> 00:55:40,089
at all so I don't expect the database to

00:55:36,369 --> 00:55:43,210
be huge in this regard everything that

00:55:40,089 --> 00:55:47,320
is time series data gets pushed to

00:55:43,210 --> 00:55:53,230
graphite in our case and so the problem

00:55:47,320 --> 00:55:57,150
is not on the sense of side hi I have a

00:55:53,230 --> 00:56:00,339
question related to high availability

00:55:57,150 --> 00:56:04,390
capabilities for the sensor server when

00:56:00,339 --> 00:56:07,329
you show the second picture with the two

00:56:04,390 --> 00:56:12,250
sensor servers separated there was no

00:56:07,329 --> 00:56:14,530
communication between them so do you use

00:56:12,250 --> 00:56:18,339
the same configuration from one server

00:56:14,530 --> 00:56:20,800
and copy to the other or how do you do

00:56:18,339 --> 00:56:23,440
well that will probably depend on your

00:56:20,800 --> 00:56:28,270
service architecture if your data

00:56:23,440 --> 00:56:30,700
centers are pretty much separate it you

00:56:28,270 --> 00:56:33,250
don't have the need of central servers

00:56:30,700 --> 00:56:40,240
sharing context because each server is

00:56:33,250 --> 00:56:42,910
its own world so to speak if you have

00:56:40,240 --> 00:56:46,380
things that are interconnected you'd

00:56:42,910 --> 00:56:49,270
probably have to do a bit of engineering

00:56:46,380 --> 00:56:54,910
yourself for example work with

00:56:49,270 --> 00:56:58,450
aggregates on each Island and then have

00:56:54,910 --> 00:57:01,119
some external intelligence in external

00:56:58,450 --> 00:57:03,070
logic that will take these aggregates

00:57:01,119 --> 00:57:08,670
and tell you

00:57:03,070 --> 00:57:11,920
if a server service that is geological

00:57:08,670 --> 00:57:15,190
distri electrically distributed is okay

00:57:11,920 --> 00:57:20,160
or not sensor itself won't be able to

00:57:15,190 --> 00:57:24,700
tell you that I have more questions but

00:57:20,160 --> 00:57:30,630
okay when you were talking about

00:57:24,700 --> 00:57:33,850
aggregation it sounded like a kind of a

00:57:30,630 --> 00:57:35,950
business process model do you have

00:57:33,850 --> 00:57:39,100
something like that where you can set up

00:57:35,950 --> 00:57:42,370
rules saying if these couple databases

00:57:39,100 --> 00:57:46,450
and these couple HTTP services and these

00:57:42,370 --> 00:57:49,030
couple whatever makes a whole thing then

00:57:46,450 --> 00:57:52,450
if this is working or these other is not

00:57:49,030 --> 00:57:54,700
working then I will raise an alert does

00:57:52,450 --> 00:57:58,990
it work like this or now it's just for

00:57:54,700 --> 00:58:05,470
clustering and that's all sensory itself

00:57:58,990 --> 00:58:08,620
is very light on that side it's I I

00:58:05,470 --> 00:58:11,530
didn't point it out or yeah I I didn't

00:58:08,620 --> 00:58:14,170
emphasize it but it's basically if you

00:58:11,530 --> 00:58:16,480
do an aggregator check all that sense

00:58:14,170 --> 00:58:21,390
who does is collect all the check

00:58:16,480 --> 00:58:24,010
results and store them and that's it so

00:58:21,390 --> 00:58:27,010
you'll have to do this recursive

00:58:24,010 --> 00:58:29,980
approach that I mentioned where you have

00:58:27,010 --> 00:58:33,030
another check configured that is

00:58:29,980 --> 00:58:37,060
completely separate in terms of context

00:58:33,030 --> 00:58:40,840
where you execute these check aggregates

00:58:37,060 --> 00:58:42,760
check that goes back to the information

00:58:40,840 --> 00:58:45,490
that had been collected as aggregates

00:58:42,760 --> 00:58:50,800
and then draws its own conclusions from

00:58:45,490 --> 00:58:54,490
that data but so it's basically doing

00:58:50,800 --> 00:58:56,530
checks on checks in this case and with

00:58:54,490 --> 00:58:59,640
that approach you can be as complex as

00:58:56,530 --> 00:59:02,770
you like you can write a whole

00:58:59,640 --> 00:59:06,300
sophisticated application that uses the

00:59:02,770 --> 00:59:09,160
sensor API to get all the raw data and

00:59:06,300 --> 00:59:12,280
draw its conclusions from that and then

00:59:09,160 --> 00:59:14,410
put back another event that might

00:59:12,280 --> 00:59:16,690
trigger some handlers and and things

00:59:14,410 --> 00:59:19,569
like that so

00:59:16,690 --> 00:59:22,569
basically you can make it as complex as

00:59:19,569 --> 00:59:32,410
you like and since who gives you the

00:59:22,569 --> 00:59:36,730
building blocks for that with how many

00:59:32,410 --> 00:59:42,910
checks do you have to put a new broker

00:59:36,730 --> 00:59:45,339
for the rabbit in queue that's hard to

00:59:42,910 --> 00:59:48,579
tell and we'll probably at least depend

00:59:45,339 --> 00:59:52,599
on the hardware you are using so your

00:59:48,579 --> 00:59:56,650
mileage may vary you are not saying any

00:59:52,599 --> 01:00:00,250
numbers any numbers no because it's

00:59:56,650 --> 01:00:04,809
really about the number of events that

01:00:00,250 --> 01:00:08,020
you trigger the number of the the size

01:00:04,809 --> 01:00:13,270
of the payload you generate my simple

01:00:08,020 --> 01:00:14,770
load metric just printed three lines if

01:00:13,270 --> 01:00:17,950
you have something that writes out

01:00:14,770 --> 01:00:21,160
kilobytes of data this will be pretty

01:00:17,950 --> 01:00:31,809
different so my only answer can be at

01:00:21,160 --> 01:00:34,029
this place it depends hello I have two

01:00:31,809 --> 01:00:36,520
questions is the communication between

01:00:34,029 --> 01:00:39,700
the server and the client encrypted and

01:00:36,520 --> 01:00:42,069
the second is when there is a problem at

01:00:39,700 --> 01:00:44,230
the server side will the data be cached

01:00:42,069 --> 01:00:48,010
so the metric data that will be

01:00:44,230 --> 01:00:56,160
collected on the client sensor uses SSL

01:00:48,010 --> 01:00:59,740
encryption so you can set up either a

01:00:56,160 --> 01:01:02,230
common certificate that is deployed to

01:00:59,740 --> 01:01:04,240
all the clients which that would be the

01:01:02,230 --> 01:01:06,279
most simple configuration and you can go

01:01:04,240 --> 01:01:08,789
as far as you like in terms of having

01:01:06,279 --> 01:01:14,410
individual client certificates as well

01:01:08,789 --> 01:01:17,020
and to your second question that's

01:01:14,410 --> 01:01:19,690
already built into the infrastructure if

01:01:17,020 --> 01:01:22,029
your only sense of server goes down what

01:01:19,690 --> 01:01:25,000
happens is that the messages will stay

01:01:22,029 --> 01:01:27,279
in rabbitmq and as soon as your server

01:01:25,000 --> 01:01:30,250
comes back it will process everything

01:01:27,279 --> 01:01:35,530
that has piled up

01:01:30,250 --> 01:01:38,530
so I was only half joking by building a

01:01:35,530 --> 01:01:40,390
by saying a building a monitoring system

01:01:38,530 --> 01:01:42,100
is easy because if you are using the

01:01:40,390 --> 01:01:44,470
right building blocks it actually is

01:01:42,100 --> 01:01:45,730
easy and you're basically standing on

01:01:44,470 --> 01:01:49,330
the shoulders of giants

01:01:45,730 --> 01:01:52,150
named RabbitMQ Redis and so on but the

01:01:49,330 --> 01:01:54,790
client don't cash so that must be

01:01:52,150 --> 01:01:57,130
delivered to repeat and cue or is it

01:01:54,790 --> 01:01:59,950
part of well rabbitmq is the crucial

01:01:57,130 --> 01:02:03,940
part here if your client can't connect

01:01:59,950 --> 01:02:06,640
to RabbitMQ it won't be able to put the

01:02:03,940 --> 01:02:10,330
message on the bus and your result will

01:02:06,640 --> 01:02:13,300
probably get lost so if we are talking

01:02:10,330 --> 01:02:15,040
about high availability RabbitMQ should

01:02:13,300 --> 01:02:27,880
be the first place you look at you look

01:02:15,040 --> 01:02:32,680
at okay thank you sensor 2.0 now

01:02:27,880 --> 01:02:34,510
everything is written in Ruby yeah so

01:02:32,680 --> 01:02:38,290
far and everything has been written in

01:02:34,510 --> 01:02:44,290
Ruby which is nice for us because we had

01:02:38,290 --> 01:02:47,830
to learn Ruby for chef anyway and so a

01:02:44,290 --> 01:02:52,090
nice little detail is that sense of 1.0

01:02:47,830 --> 01:02:57,130
is only a few weeks old now and all they

01:02:52,090 --> 01:03:00,550
did was take sensu all point twenty nine

01:02:57,130 --> 01:03:05,620
I think and and relabeled it with 1.0

01:03:00,550 --> 01:03:09,060
and the only real thing they added to

01:03:05,620 --> 01:03:12,750
the 1.0 release was the promise that

01:03:09,060 --> 01:03:19,420
sensor will now have semantic versioning

01:03:12,750 --> 01:03:25,230
so 1.0 is what was all point 29 no major

01:03:19,420 --> 01:03:28,440
change happened with that release and I

01:03:25,230 --> 01:03:30,610
can imagine that with very large

01:03:28,440 --> 01:03:32,890
infrastructures they'll run into some

01:03:30,610 --> 01:03:35,230
kind of bottlenecks that will be don't

01:03:32,890 --> 01:03:41,470
be easy to resolve using Ruby and that's

01:03:35,230 --> 01:03:43,630
why they now look at gold as their new

01:03:41,470 --> 01:03:49,540
language but 2.0 will

01:03:43,630 --> 01:03:51,910
probably be midterm not short-term do

01:03:49,540 --> 01:03:54,450
you know of any backward compatibility

01:03:51,910 --> 01:03:57,580
or grace periods

01:03:54,450 --> 01:04:00,640
I'm pretty sure that one point one point

01:03:57,580 --> 01:04:05,050
X will be available for a long time and

01:04:00,640 --> 01:04:07,330
again taking this interest at this

01:04:05,050 --> 01:04:10,360
architecture it will be quite easy to

01:04:07,330 --> 01:04:13,450
simply replace a sense of server written

01:04:10,360 --> 01:04:16,600
in Ruby that only executes

01:04:13,450 --> 01:04:19,950
some kind of script as a handler with

01:04:16,600 --> 01:04:30,100
the same thing that's written in go I

01:04:19,950 --> 01:04:32,170
expect this to be pretty easy to do are

01:04:30,100 --> 01:04:34,450
you able to handle external events like

01:04:32,170 --> 01:04:37,600
SNMP traps or syslog messages with

01:04:34,450 --> 01:04:40,750
sensor mm-hmm they are also checks that

01:04:37,600 --> 01:04:43,660
query SNMP or are things like that and

01:04:40,750 --> 01:04:48,520
not query getting eternal events like an

01:04:43,660 --> 01:04:52,450
SNMP trap oh okay I'm not sure because

01:04:48,520 --> 01:04:54,070
we are not using as an MP in our company

01:04:52,450 --> 01:04:58,120
and so I don't have any practical

01:04:54,070 --> 01:05:02,320
experience with that I could be able to

01:04:58,120 --> 01:05:04,150
look it up because the check report or

01:05:02,320 --> 01:05:06,430
on github is pretty extensive and there

01:05:04,150 --> 01:05:10,030
I'm pretty certain there isn't as an MP

01:05:06,430 --> 01:05:11,860
directory in there okay but it must be a

01:05:10,030 --> 01:05:16,060
receiver I think and this is the main

01:05:11,860 --> 01:05:20,650
problem for nearly every now your status

01:05:16,060 --> 01:05:23,590
based monitoring system and yeah would

01:05:20,650 --> 01:05:25,480
be great if it's in sensing yeah you'd

01:05:23,590 --> 01:05:27,820
have to have some component that gets

01:05:25,480 --> 01:05:30,400
the trap and then triggers an event the

01:05:27,820 --> 01:05:33,940
event triggering itself will be just

01:05:30,400 --> 01:05:36,100
talking to 3030 the year but the the

01:05:33,940 --> 01:05:43,570
tricky thing is receiving the trap

01:05:36,100 --> 01:05:43,810
itself okay no more questions okay thank

01:05:43,570 --> 01:05:47,100
you

01:05:43,810 --> 01:05:47,100
oh there's another one back there

01:05:50,200 --> 01:05:57,800
how do you handle user roles in the

01:05:54,200 --> 01:06:02,120
console in the dashboard can you create

01:05:57,800 --> 01:06:06,770
differ different roles for I don't know

01:06:02,120 --> 01:06:08,960
the data database admins or the people

01:06:06,770 --> 01:06:11,900
that is only monitoring the

01:06:08,960 --> 01:06:14,030
infrastructure or the application can

01:06:11,900 --> 01:06:18,050
you create as far as I know the default

01:06:14,030 --> 01:06:20,390
dashboard the achiever Achieva dashboard

01:06:18,050 --> 01:06:22,880
doesn't have that capability but there

01:06:20,390 --> 01:06:25,130
are additional dashboards out there that

01:06:22,880 --> 01:06:29,570
might so there's something called sense

01:06:25,130 --> 01:06:35,690
admin I think which would be worth a

01:06:29,570 --> 01:06:38,510
look if not you'd be able to at least

01:06:35,690 --> 01:06:39,920
build something yourself via the API

01:06:38,510 --> 01:06:43,790
that will give you all the information

01:06:39,920 --> 01:06:46,460
you need as always if it's not in open

01:06:43,790 --> 01:06:53,270
source you will still be able to build

01:06:46,460 --> 01:06:55,360
it yourself okay so thank you young as I

01:06:53,270 --> 01:06:58,480
told you this was a great discussion and

01:06:55,360 --> 01:06:58,480
thank you

01:06:58,530 --> 01:07:03,089

YouTube URL: https://www.youtube.com/watch?v=PA7tbjOrNTM


