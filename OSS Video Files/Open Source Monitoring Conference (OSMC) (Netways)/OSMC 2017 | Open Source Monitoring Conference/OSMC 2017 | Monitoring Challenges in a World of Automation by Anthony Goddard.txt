Title: OSMC 2017 | Monitoring Challenges in a World of Automation by Anthony Goddard
Publication date: 2017-12-05
Playlist: OSMC 2017 | Open Source Monitoring Conference
Description: 
	Public and private cloud infrastructures promise to make fully dynamic infrastructure a reality – compute instances can be provisioned and terminated at a moments notice, all in response to customer demand. Though “auto-scaling” was once held as the pinnacle of infrastructure automation, it is now considered table stakes. And while this has relieved certain operational burdens (developers can now have access to “on-demand” compute!), it has also created new challenges.
Captions: 
	00:00:09,800 --> 00:00:14,880
welcome back everyone Anthony Goddard

00:00:12,750 --> 00:00:17,630
will tell us about monitoring in an

00:00:14,880 --> 00:00:21,720
automated world a lot of scaling and

00:00:17,630 --> 00:00:24,529
cluster cloud things to show us have fun

00:00:21,720 --> 00:00:24,529
and give him a warm welcome

00:00:27,860 --> 00:00:32,369
good morning everyone and thank you very

00:00:30,060 --> 00:00:33,690
much for coming along to this so today

00:00:32,369 --> 00:00:38,940
I'm gonna be talking about monitoring

00:00:33,690 --> 00:00:40,380
challenges in a world of automation so

00:00:38,940 --> 00:00:44,160
my name is Anthony I'm the vice

00:00:40,380 --> 00:00:45,120
president of operations for sensu Inc if

00:00:44,160 --> 00:00:46,920
you wanna follow me on Twitter

00:00:45,120 --> 00:00:48,450
mostly I tweet about like people that do

00:00:46,920 --> 00:00:51,600
crazy adventures around the world it's

00:00:48,450 --> 00:00:53,670
kind of like a hobby thing of mine but

00:00:51,600 --> 00:00:55,620
if you want to follow an awesome

00:00:53,670 --> 00:00:57,510
open-source monitoring company then you

00:00:55,620 --> 00:00:59,070
can also follow Adsense ooh I'm kind of

00:00:57,510 --> 00:01:04,290
guessing everyone here is into open

00:00:59,070 --> 00:01:06,170
source monitoring so about sensu since

00:01:04,290 --> 00:01:09,990
he was an open core monitoring framework

00:01:06,170 --> 00:01:11,250
it was originally released in 2011 and

00:01:09,990 --> 00:01:14,250
it was written by a guy called Shawn

00:01:11,250 --> 00:01:17,220
Porter and when he built it it was to

00:01:14,250 --> 00:01:19,470
replace an eggless cluster that they had

00:01:17,220 --> 00:01:20,760
monitoring their infrastructure and the

00:01:19,470 --> 00:01:23,970
biggest issue they were having at the

00:01:20,760 --> 00:01:25,860
time was false positive alerting so they

00:01:23,970 --> 00:01:28,050
were scaling their nodes up and down

00:01:25,860 --> 00:01:30,000
quite a lot in finding they were getting

00:01:28,050 --> 00:01:32,729
a lot of loads for nodes that didn't

00:01:30,000 --> 00:01:36,030
exist any longer and that's gonna be the

00:01:32,729 --> 00:01:40,370
primary basis for the since you talk

00:01:36,030 --> 00:01:44,340
that I'm doing today we launched a

00:01:40,370 --> 00:01:46,740
sincere Enterprise offering in 2015 and

00:01:44,340 --> 00:01:50,970
then earlier this year in January we

00:01:46,740 --> 00:01:53,460
started sensu Inc so we originally were

00:01:50,970 --> 00:01:56,280
a consulting firm that consulted a lot

00:01:53,460 --> 00:01:59,280
with sensu and then we pivoted to do

00:01:56,280 --> 00:02:01,680
100% sensor so this year we've grown to

00:01:59,280 --> 00:02:03,479
20 staff we've just finished hiring for

00:02:01,680 --> 00:02:05,220
this year but pretty early in the new

00:02:03,479 --> 00:02:11,610
year we're going to start hiring more

00:02:05,220 --> 00:02:14,970
engineers and growing a bit more so

00:02:11,610 --> 00:02:17,189
sense to itself is a we call it a cloud

00:02:14,970 --> 00:02:19,610
native monitoring framework so it's a

00:02:17,189 --> 00:02:23,280
monitoring tool that's designed to work

00:02:19,610 --> 00:02:23,610
specifically with infrastructure that

00:02:23,280 --> 00:02:26,460
you

00:02:23,610 --> 00:02:28,770
dynamically scaling very fast ephemeral

00:02:26,460 --> 00:02:30,930
nodes that may only exist for a few

00:02:28,770 --> 00:02:32,850
minutes for example and we also refer to

00:02:30,930 --> 00:02:34,380
it as the monitoring router this is a

00:02:32,850 --> 00:02:36,990
very uh opinionated piece of software

00:02:34,380 --> 00:02:39,480
the idea is that sensu agents that sit

00:02:36,990 --> 00:02:42,660
on nodes or external systems that are

00:02:39,480 --> 00:02:44,250
sending data into sensu they all just

00:02:42,660 --> 00:02:46,140
throw the data it sends you and then

00:02:44,250 --> 00:02:48,000
since you can take data from let's say

00:02:46,140 --> 00:02:48,840
metrics data from a node and choose

00:02:48,000 --> 00:02:50,490
where to put it

00:02:48,840 --> 00:02:52,050
it can take service check data from a

00:02:50,490 --> 00:02:54,150
node and choose to put it somewhere

00:02:52,050 --> 00:02:55,920
different and then we have corresponding

00:02:54,150 --> 00:02:58,560
routing on notifications as well so we

00:02:55,920 --> 00:03:00,420
can say this team over here are the ones

00:02:58,560 --> 00:03:01,890
that care about these particular servers

00:03:00,420 --> 00:03:05,280
when they go down and they like to be

00:03:01,890 --> 00:03:08,310
notified by slack instead of pages for

00:03:05,280 --> 00:03:10,410
example so conceptually we think about

00:03:08,310 --> 00:03:12,090
it much like a router where there's a

00:03:10,410 --> 00:03:14,040
lot of data coming in there's processing

00:03:12,090 --> 00:03:17,160
being done and decisions logic being

00:03:14,040 --> 00:03:23,160
made and then the output being storing

00:03:17,160 --> 00:03:25,650
events for metrics or notifications so

00:03:23,160 --> 00:03:28,500
it's full stack monitoring everywhere

00:03:25,650 --> 00:03:29,760
from the base of your infrastructure all

00:03:28,500 --> 00:03:31,080
the way through the application codes

00:03:29,760 --> 00:03:33,450
doing application code you can

00:03:31,080 --> 00:03:35,519
instrument your code to emit events or

00:03:33,450 --> 00:03:38,130
emit metrics directly to sensu and then

00:03:35,519 --> 00:03:40,560
have sense who take care of routing that

00:03:38,130 --> 00:03:42,540
data like I mentioned it's designed for

00:03:40,560 --> 00:03:45,570
automation it's across across platforms

00:03:42,540 --> 00:03:47,160
so we have Linux Windows BSD ax Solaris

00:03:45,570 --> 00:03:49,550
and Mac OS I think we even have a

00:03:47,160 --> 00:03:52,650
Raspberry Pi package but it's unofficial

00:03:49,550 --> 00:03:54,480
so quite a lot of spot if you want to

00:03:52,650 --> 00:03:57,540
learn more about sensor itself you can

00:03:54,480 --> 00:03:58,739
hit up the website sensu org so our

00:03:57,540 --> 00:04:01,019
mission statement as company is to

00:03:58,739 --> 00:04:04,530
obviate the need to rebuild custom

00:04:01,019 --> 00:04:08,930
monitoring solutions so we really think

00:04:04,530 --> 00:04:11,370
that the only true path to reinventing

00:04:08,930 --> 00:04:14,130
or to stop reinventing monitoring sorry

00:04:11,370 --> 00:04:16,680
is to have open source software and so

00:04:14,130 --> 00:04:18,209
we have a very firm belief in senshu as

00:04:16,680 --> 00:04:20,570
an open source product with the

00:04:18,209 --> 00:04:22,590
enterprise offering just offering some

00:04:20,570 --> 00:04:25,919
additional services specifically to

00:04:22,590 --> 00:04:27,479
enterprises and has it operate as a open

00:04:25,919 --> 00:04:31,020
source monitoring company we're super

00:04:27,479 --> 00:04:33,840
happy to be here at osm C but this talk

00:04:31,020 --> 00:04:36,180
is not about sensor so we organized a

00:04:33,840 --> 00:04:37,320
bunch of conferences ourselves different

00:04:36,180 --> 00:04:40,340
folks on the team have been over

00:04:37,320 --> 00:04:42,900
of in DevOps days and monitor Armour and

00:04:40,340 --> 00:04:44,790
atomic on and so we know that people

00:04:42,900 --> 00:04:46,980
don't really like going to conferences

00:04:44,790 --> 00:04:48,630
just to talk about vendor stuff so we're

00:04:46,980 --> 00:04:50,400
going to keep this as usual as we can

00:04:48,630 --> 00:04:53,040
and talk about some of the challenges

00:04:50,400 --> 00:04:59,340
that we find in monitoring for ephemeral

00:04:53,040 --> 00:05:02,400
environments so ultra ephemeral systems

00:04:59,340 --> 00:05:03,810
we're going to review the what we see is

00:05:02,400 --> 00:05:06,230
the cloud native monitoring requirements

00:05:03,810 --> 00:05:08,790
and we see them as automated discovery

00:05:06,230 --> 00:05:10,710
automated monitoring and then automated

00:05:08,790 --> 00:05:12,660
decommissioning and the last one is

00:05:10,710 --> 00:05:14,490
probably the most exciting I think of

00:05:12,660 --> 00:05:17,040
the three we're also going to like go

00:05:14,490 --> 00:05:19,530
over a couple of cloud native monitoring

00:05:17,040 --> 00:05:21,720
anti-patterns and we're gonna do a live

00:05:19,530 --> 00:05:23,580
demo which I have like tethered to

00:05:21,720 --> 00:05:30,500
Stefan's phone so it should work what

00:05:23,580 --> 00:05:33,270
could go wrong ready to go so

00:05:30,500 --> 00:05:34,530
traditionally I'm not going to retail

00:05:33,270 --> 00:05:37,140
the DevOps story but traditionally

00:05:34,530 --> 00:05:39,300
there's been a you know IT operations

00:05:37,140 --> 00:05:41,930
and developers used to send these two

00:05:39,300 --> 00:05:45,480
sides of the silo and there was always

00:05:41,930 --> 00:05:46,800
competing interests of each one wanted

00:05:45,480 --> 00:05:49,770
to move fast the other one want to keep

00:05:46,800 --> 00:05:52,740
things stable and obviously as things

00:05:49,770 --> 00:05:54,810
like DevOps and adoption and cloud

00:05:52,740 --> 00:05:56,970
adoption has increased we have these

00:05:54,810 --> 00:05:59,220
situations where developers are

00:05:56,970 --> 00:06:01,830
increasingly able to perform operation

00:05:59,220 --> 00:06:03,650
tasks which means we have servers that

00:06:01,830 --> 00:06:06,390
are just appearing out of nowhere or

00:06:03,650 --> 00:06:09,720
appearing for only short periods of time

00:06:06,390 --> 00:06:11,550
and there's multiple parties in a

00:06:09,720 --> 00:06:13,410
company who were able to provision those

00:06:11,550 --> 00:06:15,300
servers so gone are the days of the

00:06:13,410 --> 00:06:17,190
developers request a server you have to

00:06:15,300 --> 00:06:18,360
provision the hardware and rack it up

00:06:17,190 --> 00:06:20,640
and you have time to do something like

00:06:18,360 --> 00:06:22,620
write a complicated monitoring config

00:06:20,640 --> 00:06:24,540
now a developer could just spin up a

00:06:22,620 --> 00:06:25,530
node with that operations knowing but

00:06:24,540 --> 00:06:27,210
operations is still going to be

00:06:25,530 --> 00:06:33,260
responsible for making sure that node

00:06:27,210 --> 00:06:35,670
doesn't crash or doesn't go down so

00:06:33,260 --> 00:06:38,340
cloud platforms and automation systems

00:06:35,670 --> 00:06:39,750
cause changes in the infrastructure and

00:06:38,340 --> 00:06:43,410
that increases the complexity of the

00:06:39,750 --> 00:06:45,419
monitoring new systems that are created

00:06:43,410 --> 00:06:49,680
need to be discovered quickly and easily

00:06:45,419 --> 00:06:51,030
and monitored and monitoring needs to

00:06:49,680 --> 00:06:52,590
know the difference now

00:06:51,030 --> 00:06:53,670
between something that is down and

00:06:52,590 --> 00:06:56,540
something that has simply been

00:06:53,670 --> 00:06:56,540
decommissioned

00:06:58,590 --> 00:07:01,080
however structure is becoming

00:06:59,820 --> 00:07:03,290
increasingly more automated and

00:07:01,080 --> 00:07:05,970
ephemeral so we should expect that our

00:07:03,290 --> 00:07:10,710
monitoring systems are able to keep up

00:07:05,970 --> 00:07:11,670
pace so basically like have this any

00:07:10,710 --> 00:07:13,680
changes that are made in the

00:07:11,670 --> 00:07:15,960
infrastructure need to be visible as

00:07:13,680 --> 00:07:21,780
quickly as the infrastructure is

00:07:15,960 --> 00:07:23,790
changing so again the three major things

00:07:21,780 --> 00:07:26,040
automated discovery automated monitoring

00:07:23,790 --> 00:07:27,840
and automated decommissioning these what

00:07:26,040 --> 00:07:29,820
we see as cloud native monitoring

00:07:27,840 --> 00:07:32,130
requirements is not an exhaustive list

00:07:29,820 --> 00:07:33,900
obviously of what we think monitoring

00:07:32,130 --> 00:07:35,160
should be but it's the three things that

00:07:33,900 --> 00:07:37,080
we think are the most important when it

00:07:35,160 --> 00:07:38,820
comes to monitoring ephemeral

00:07:37,080 --> 00:07:42,300
infrastructure so we'll step through

00:07:38,820 --> 00:07:45,110
each of those automated discovery first

00:07:42,300 --> 00:07:47,400
so any new system should be

00:07:45,110 --> 00:07:48,930
automatically discovered so in the same

00:07:47,400 --> 00:07:50,880
way that a new system is automatically

00:07:48,930 --> 00:07:53,940
provisioned there shouldn't be any

00:07:50,880 --> 00:07:55,920
process that's required to manually make

00:07:53,940 --> 00:07:59,310
changes in order to monitor the system

00:07:55,920 --> 00:08:00,450
as it comes online some of the cool

00:07:59,310 --> 00:08:05,040
things that the cloud gives us is the

00:08:00,450 --> 00:08:07,320
ability provision have provisioning

00:08:05,040 --> 00:08:11,390
events that do creation of instances and

00:08:07,320 --> 00:08:13,560
replacement of instances automated

00:08:11,390 --> 00:08:16,350
scaling of instances so auto scaling

00:08:13,560 --> 00:08:19,590
groups for example so your cloud

00:08:16,350 --> 00:08:20,880
provider can just go from two instances

00:08:19,590 --> 00:08:23,720
of a web server to a hundred instances

00:08:20,880 --> 00:08:26,490
and back down to two instances and

00:08:23,720 --> 00:08:28,380
external systems are able to use api's

00:08:26,490 --> 00:08:29,880
to your infrastructure to request

00:08:28,380 --> 00:08:31,740
changes that be made so there may be

00:08:29,880 --> 00:08:34,500
changes with networking it may be

00:08:31,740 --> 00:08:35,760
changes with spinning up instances but

00:08:34,500 --> 00:08:38,210
these are all really cool things about

00:08:35,760 --> 00:08:41,460
the cloud and they're things that make

00:08:38,210 --> 00:08:42,360
monitoring a challenge so some of the

00:08:41,460 --> 00:08:44,940
anti-patterns

00:08:42,360 --> 00:08:47,760
for monitoring in this case would be

00:08:44,940 --> 00:08:50,640
polling based discovery so we don't want

00:08:47,760 --> 00:08:52,050
to go and look for things we want when

00:08:50,640 --> 00:08:54,650
the things are created we need them to

00:08:52,050 --> 00:08:57,960
make the system aware of their existence

00:08:54,650 --> 00:09:01,280
any sort of old-school discovery that

00:08:57,960 --> 00:09:04,230
needs so that doesn't allow complex

00:09:01,280 --> 00:09:04,920
network topologies so gone are the days

00:09:04,230 --> 00:09:08,490
of being able to say

00:09:04,920 --> 00:09:10,320
like let's just run a scan across this

00:09:08,490 --> 00:09:12,980
IP subnet and discover all the things

00:09:10,320 --> 00:09:15,630
and write a config from that discovery

00:09:12,980 --> 00:09:19,170
oftentimes now people are using like

00:09:15,630 --> 00:09:21,149
multi a-z multi-region cloud

00:09:19,170 --> 00:09:23,040
environments and also anything that

00:09:21,149 --> 00:09:26,490
requires a hole to be made in a firewall

00:09:23,040 --> 00:09:29,040
in order to see your system so whether

00:09:26,490 --> 00:09:30,630
there's SSH or some other protocol if

00:09:29,040 --> 00:09:31,709
you have to make modifications to your

00:09:30,630 --> 00:09:33,959
firewall in order to allow the

00:09:31,709 --> 00:09:35,730
monitoring system to see it then you're

00:09:33,959 --> 00:09:37,440
introducing delays and you're also

00:09:35,730 --> 00:09:40,440
introducing potential security problems

00:09:37,440 --> 00:09:45,389
in the the rapid modification of

00:09:40,440 --> 00:09:51,000
firewall logic so polling is not a

00:09:45,389 --> 00:09:52,829
reliable discovery solution so new

00:09:51,000 --> 00:09:55,260
systems they need to be discovered in

00:09:52,829 --> 00:09:58,579
real time and they need to provide push

00:09:55,260 --> 00:10:01,079
based or event based discovery and api's

00:09:58,579 --> 00:10:03,120
so push base or event based is critical

00:10:01,079 --> 00:10:05,519
in an ideal world our system should

00:10:03,120 --> 00:10:07,170
register themselves into monitoring and

00:10:05,519 --> 00:10:08,670
some monitoring tools do this with the

00:10:07,170 --> 00:10:11,459
agents other ones make it possible with

00:10:08,670 --> 00:10:14,130
discovery api's the automation systems

00:10:11,459 --> 00:10:16,170
can interact with and publish events -

00:10:14,130 --> 00:10:17,970
in the real world these two things are

00:10:16,170 --> 00:10:20,730
complementary so you should expect both

00:10:17,970 --> 00:10:25,769
push-based and event based with

00:10:20,730 --> 00:10:30,390
discovery api's so automated monitoring

00:10:25,769 --> 00:10:32,910
is the next one so new systems need to

00:10:30,390 --> 00:10:34,920
be monitored automatically and the

00:10:32,910 --> 00:10:37,740
registration of these new systems is is

00:10:34,920 --> 00:10:40,260
the easy part again with the cloud we

00:10:37,740 --> 00:10:43,589
get almost all infrastructures are

00:10:40,260 --> 00:10:46,079
distributed systems so if you have a web

00:10:43,589 --> 00:10:47,850
server and a database then you're

00:10:46,079 --> 00:10:49,170
running I guess it's probably up for

00:10:47,850 --> 00:10:50,760
debate but I would say if you're if you

00:10:49,170 --> 00:10:51,839
have two systems once a web server once

00:10:50,760 --> 00:10:56,040
database then you're running a

00:10:51,839 --> 00:10:57,329
distributed system and so conceptually

00:10:56,040 --> 00:11:01,470
when you have these disparate systems

00:10:57,329 --> 00:11:03,290
the they fulfill specific roles so we'd

00:11:01,470 --> 00:11:05,699
say like one is the database role

00:11:03,290 --> 00:11:07,980
another one is the web browser so

00:11:05,699 --> 00:11:09,630
conceptually we're thinking about things

00:11:07,980 --> 00:11:11,610
more in terms of what the servers or the

00:11:09,630 --> 00:11:15,390
services do rather than the specific

00:11:11,610 --> 00:11:17,550
server itself so you could have a simple

00:11:15,390 --> 00:11:18,780
architecture we have one let's say a

00:11:17,550 --> 00:11:22,010
virtual machine

00:11:18,780 --> 00:11:24,030
that has the entire stack on the machine

00:11:22,010 --> 00:11:25,800
but more and more we're seeing people

00:11:24,030 --> 00:11:28,200
have more complex architectures where

00:11:25,800 --> 00:11:29,880
instead of putting over the on one note

00:11:28,200 --> 00:11:31,620
obviously you have so microservices is

00:11:29,880 --> 00:11:35,640
probably the biggest example of this you

00:11:31,620 --> 00:11:38,000
have a lot of systems and one roll per

00:11:35,640 --> 00:11:40,770
system so to conceptually think about

00:11:38,000 --> 00:11:43,980
how we monitor the rolls versus how we

00:11:40,770 --> 00:11:45,990
monitor the systems themselves I think

00:11:43,980 --> 00:11:49,230
is important and so some anti patterns

00:11:45,990 --> 00:11:51,840
there are mapping configuration to

00:11:49,230 --> 00:11:55,200
individual systems is probably an anti

00:11:51,840 --> 00:11:57,360
patent vs. mapping to rolls and then

00:11:55,200 --> 00:12:00,750
anything that requires remote access to

00:11:57,360 --> 00:12:06,270
a specific system kind of falls in the

00:12:00,750 --> 00:12:12,420
same vein so we feel like that is doing

00:12:06,270 --> 00:12:14,190
it backwards so configuration should be

00:12:12,420 --> 00:12:16,440
mapped to roles and monitoring should

00:12:14,190 --> 00:12:23,390
become should begin at the moment that a

00:12:16,440 --> 00:12:23,390
system comes online it should just work

00:12:24,740 --> 00:12:32,220
and so I think that it should be a given

00:12:28,590 --> 00:12:34,770
that any system these days will be able

00:12:32,220 --> 00:12:36,150
to monitor nodes as they come online the

00:12:34,770 --> 00:12:37,620
more difficult part or the more

00:12:36,150 --> 00:12:40,080
interesting part from our perspective is

00:12:37,620 --> 00:12:43,140
what happens for the automated process

00:12:40,080 --> 00:12:44,040
when those systems go offline so

00:12:43,140 --> 00:12:47,090
terminated system should be

00:12:44,040 --> 00:12:49,530
automatically removed from monitoring

00:12:47,090 --> 00:12:51,180
and figuring out how we do that and when

00:12:49,530 --> 00:12:54,120
we do that is is where it gets a little

00:12:51,180 --> 00:12:55,440
bit more challenging so again with some

00:12:54,120 --> 00:12:58,110
cool things that we have from ephemeral

00:12:55,440 --> 00:13:00,960
infrastructure we have cost incentives

00:12:58,110 --> 00:13:02,760
to say like we don't need not the

00:13:00,960 --> 00:13:05,670
biggest shopping day of the year every

00:13:02,760 --> 00:13:06,930
single day so when it is we need 10,000

00:13:05,670 --> 00:13:08,190
web service when it's not made we're

00:13:06,930 --> 00:13:10,020
going to need a thousand web service so

00:13:08,190 --> 00:13:13,350
there's a big incentive for folks to

00:13:10,020 --> 00:13:16,950
scale down the infrastructures when the

00:13:13,350 --> 00:13:18,600
load is not high or just when systems

00:13:16,950 --> 00:13:21,210
aren't being used at all and in these

00:13:18,600 --> 00:13:23,370
cases intentional acts that people take

00:13:21,210 --> 00:13:25,080
in the infrastructure look a lot like

00:13:23,370 --> 00:13:26,670
failures you know if you have ten

00:13:25,080 --> 00:13:28,230
thousand servers one minute in the next

00:13:26,670 --> 00:13:30,680
minute you only have one thousand

00:13:28,230 --> 00:13:32,470
service that could be because somebody

00:13:30,680 --> 00:13:34,720
intentionally the

00:13:32,470 --> 00:13:36,820
mission 9000 service or there's like a

00:13:34,720 --> 00:13:42,160
heating or serious power problem in your

00:13:36,820 --> 00:13:45,870
data center so the anti-patterns

00:13:42,160 --> 00:13:50,950
correspondingly would be something like

00:13:45,870 --> 00:13:54,970
assuming that the lack of data is is the

00:13:50,950 --> 00:13:58,450
cause or is if you look at the lack of

00:13:54,970 --> 00:13:59,980
data and say well I know that there's

00:13:58,450 --> 00:14:01,420
way less monitoring information right

00:13:59,980 --> 00:14:02,920
now but we also just decommissioned all

00:14:01,420 --> 00:14:03,250
those hosts so everything's probably

00:14:02,920 --> 00:14:05,320
fine

00:14:03,250 --> 00:14:06,640
there would be an aunty pan making

00:14:05,320 --> 00:14:09,670
assumptions about like the lack of

00:14:06,640 --> 00:14:12,460
network connectivity so you might say

00:14:09,670 --> 00:14:13,840
well that service is offline but the

00:14:12,460 --> 00:14:15,670
network over there is offline

00:14:13,840 --> 00:14:18,880
so the service probably offline because

00:14:15,670 --> 00:14:21,250
the network is offline and also using

00:14:18,880 --> 00:14:23,890
monitoring as an absolute source of

00:14:21,250 --> 00:14:26,980
truth so monitoring we feel should

00:14:23,890 --> 00:14:29,200
always accurately reflect the source of

00:14:26,980 --> 00:14:37,630
truth which would never actually be the

00:14:29,200 --> 00:14:40,390
source of truth and so for automated

00:14:37,630 --> 00:14:42,150
decommissioning it should be invoked by

00:14:40,390 --> 00:14:44,590
the system that's being decommissioned

00:14:42,150 --> 00:14:46,090
so for example when the system knows

00:14:44,590 --> 00:14:47,470
it's going offline it should say that

00:14:46,090 --> 00:14:49,390
it's going offline

00:14:47,470 --> 00:14:52,710
it could be triggered by a provisioning

00:14:49,390 --> 00:14:56,290
system so it could be triggered by the

00:14:52,710 --> 00:14:58,300
like they say a chef command to remove

00:14:56,290 --> 00:15:02,350
clients it could be triggered by your

00:14:58,300 --> 00:15:04,570
cloud platforms api's for when nodes

00:15:02,350 --> 00:15:06,250
coming offline it should be it could be

00:15:04,570 --> 00:15:10,270
optionally verified by an external

00:15:06,250 --> 00:15:12,220
source of truth so it may be chef might

00:15:10,270 --> 00:15:16,630
know that a node is offline that you can

00:15:12,220 --> 00:15:17,860
verify that with AWS and this really we

00:15:16,630 --> 00:15:20,980
think should be the most reliable

00:15:17,860 --> 00:15:23,740
function of a monitoring system so

00:15:20,980 --> 00:15:25,090
knowing that when it's telling you your

00:15:23,740 --> 00:15:28,090
node is offline you'll notice actually

00:15:25,090 --> 00:15:32,770
offline we think you have to be able to

00:15:28,090 --> 00:15:34,240
rely on that information if you if you

00:15:32,770 --> 00:15:35,080
can't then this you may as well just not

00:15:34,240 --> 00:15:36,700
monitor stuff at all

00:15:35,080 --> 00:15:37,990
right like if it's paging you and

00:15:36,700 --> 00:15:39,670
telling you stuff that's just not true

00:15:37,990 --> 00:15:40,990
or it's not telling you stuff that you

00:15:39,670 --> 00:15:43,620
need to know then you may as well just

00:15:40,990 --> 00:15:43,620
turn it all off

00:15:44,370 --> 00:15:51,660
all right so we're gonna do a little bit

00:15:48,480 --> 00:15:53,610
of audience participation and I just

00:15:51,660 --> 00:15:57,329
want to kind of get a show of hands

00:15:53,610 --> 00:15:58,860
for who is what people's infrastructures

00:15:57,329 --> 00:16:00,749
look like at the moment and it'll kind

00:15:58,860 --> 00:16:03,209
of guide a little bit about the live

00:16:00,749 --> 00:16:06,389
demo that we do so we have some if

00:16:03,209 --> 00:16:11,490
you've heard of kubernetes all right

00:16:06,389 --> 00:16:13,230
sweet and what about if you have

00:16:11,490 --> 00:16:17,129
kubernetes on your roadmap but some of

00:16:13,230 --> 00:16:18,689
you're planning on using okay cool

00:16:17,129 --> 00:16:22,189
and anyone using kubernetes in

00:16:18,689 --> 00:16:24,420
production right now awesome

00:16:22,189 --> 00:16:34,800
all right put your hands up if you know

00:16:24,420 --> 00:16:37,620
what the cloud is so one thing they

00:16:34,800 --> 00:16:39,240
should go of is pretty hands up if you

00:16:37,620 --> 00:16:40,800
have just a general concept of like

00:16:39,240 --> 00:16:42,720
auto-scaling groups elastic load

00:16:40,800 --> 00:16:43,199
balancers those sort of concepts okay

00:16:42,720 --> 00:16:46,709
cool

00:16:43,199 --> 00:16:49,740
perfect and I'm assuming most of you are

00:16:46,709 --> 00:16:52,139
using or have used address TCP yep all

00:16:49,740 --> 00:16:55,249
right that's the exit the morning

00:16:52,139 --> 00:16:55,249
exercise is done yeah

00:16:56,389 --> 00:17:11,809
all right let me jump over to demo okay

00:17:09,110 --> 00:17:12,890
perfect okay so anyone who's not

00:17:11,809 --> 00:17:16,730
familiar with it this is sensors

00:17:12,890 --> 00:17:18,439
monitoring dashboard so the the setup

00:17:16,730 --> 00:17:23,990
that I have right now that I'm running

00:17:18,439 --> 00:17:25,539
is on my local machine I have a VM and

00:17:23,990 --> 00:17:28,370
that VM is running since we enterprise

00:17:25,539 --> 00:17:30,950
and it's also running one sensor client

00:17:28,370 --> 00:17:35,330
I also have on Google's kubernetes

00:17:30,950 --> 00:17:37,669
engine some web servers and both of

00:17:35,330 --> 00:17:39,559
those represented in the single

00:17:37,669 --> 00:17:41,899
dashboard so since it has this concept

00:17:39,559 --> 00:17:44,750
of a single pane of glass dashboard you

00:17:41,899 --> 00:17:46,309
can have multiple disparate data centers

00:17:44,750 --> 00:17:49,549
that all have their own highly available

00:17:46,309 --> 00:17:53,059
sensor instances in them and then we

00:17:49,549 --> 00:17:54,980
provide a federated dashboard instance

00:17:53,059 --> 00:17:57,639
so you can get visibility into all the

00:17:54,980 --> 00:17:59,840
different ones so in this case we have

00:17:57,639 --> 00:18:02,090
vagrant which is my local one here and

00:17:59,840 --> 00:18:08,210
the kubernetes one which is out on

00:18:02,090 --> 00:18:10,880
Google's cloud now that this demo is in

00:18:08,210 --> 00:18:12,649
Cuban it is not really that important if

00:18:10,880 --> 00:18:15,169
kubernetes isn't something that you're

00:18:12,649 --> 00:18:16,490
using at the moment you can just imagine

00:18:15,169 --> 00:18:18,740
that kubernetes is basically like a

00:18:16,490 --> 00:18:22,190
generic cloud provider so some of the

00:18:18,740 --> 00:18:24,169
commands that I'm going to run would be

00:18:22,190 --> 00:18:26,450
the same for like auto scaling on Amazon

00:18:24,169 --> 00:18:32,110
for example it just provides an easy way

00:18:26,450 --> 00:18:35,029
to demonstrate that so we have a website

00:18:32,110 --> 00:18:38,240
here and it's a very boring website all

00:18:35,029 --> 00:18:40,519
it says is that it is healthy and in the

00:18:38,240 --> 00:18:44,510
ideal world all websites remain in this

00:18:40,519 --> 00:18:45,950
state and nothing breaks but for the

00:18:44,510 --> 00:18:49,549
purposes of its demo we're probably

00:18:45,950 --> 00:18:53,570
going to make it break and that is

00:18:49,549 --> 00:18:55,669
represented here one of these back in

00:18:53,570 --> 00:18:58,130
clients we can see that we're running a

00:18:55,669 --> 00:18:59,990
check against that all we're doing is

00:18:58,130 --> 00:19:02,480
we're saying we want to run the HTTP

00:18:59,990 --> 00:19:04,159
check plug-in against this and this this

00:19:02,480 --> 00:19:06,080
plugin will basically expect a 200

00:19:04,159 --> 00:19:09,260
response if it gets a non 200 response

00:19:06,080 --> 00:19:14,260
then it will start complaining so we're

00:19:09,260 --> 00:19:14,260
going to do is remove this window across

00:19:17,659 --> 00:19:27,179
okay so we can do is Cuba News has this

00:19:23,490 --> 00:19:28,890
some this notion of scaling up a

00:19:27,179 --> 00:19:30,960
deployment but this is exactly the same

00:19:28,890 --> 00:19:33,840
as using a auto scale group for example

00:19:30,960 --> 00:19:36,480
on Amazon so we can do the name of this

00:19:33,840 --> 00:19:38,940
website because it's a pretty not a very

00:19:36,480 --> 00:19:41,130
smart website is dummy back in right now

00:19:38,940 --> 00:19:42,659
we have two of those running so if you

00:19:41,130 --> 00:19:45,200
have a look down the bottom here this is

00:19:42,659 --> 00:19:49,140
the view of the current infrastructure

00:19:45,200 --> 00:19:50,669
and you can see there's two instances of

00:19:49,140 --> 00:19:52,200
this back-end so we're going to do is

00:19:50,669 --> 00:19:57,330
we're going to say actually we want five

00:19:52,200 --> 00:20:03,450
instances of that back-end and we should

00:19:57,330 --> 00:20:06,140
see my window is this is actually

00:20:03,450 --> 00:20:06,140
running the command

00:20:13,480 --> 00:20:30,040
Shh demo okay okay listen ladies we

00:20:20,810 --> 00:20:32,240
don't be smaller okay so then we can see

00:20:30,040 --> 00:20:35,360
it almost looked like I just pretended

00:20:32,240 --> 00:20:37,310
that happened by typing this so we can

00:20:35,360 --> 00:20:38,630
say we added another three so there so

00:20:37,310 --> 00:20:40,040
if we jump back straight across into

00:20:38,630 --> 00:20:42,440
census dashboard and we have a look at

00:20:40,040 --> 00:20:44,390
our clients we can now see there's five

00:20:42,440 --> 00:20:47,270
of those backends each of those backends

00:20:44,390 --> 00:20:50,840
has come online and is already having

00:20:47,270 --> 00:20:52,070
its HTTP endpoint checked so the thing I

00:20:50,840 --> 00:20:54,350
mentioned before about we're monitoring

00:20:52,070 --> 00:20:56,330
roles what we do here is we say like all

00:20:54,350 --> 00:20:57,770
web servers in this case everything that

00:20:56,330 --> 00:20:59,840
has the subscription or all back-end

00:20:57,770 --> 00:21:02,270
service need to run this check those

00:20:59,840 --> 00:21:04,910
servers came online announced themselves

00:21:02,270 --> 00:21:06,590
as back-end hosts and so automatically

00:21:04,910 --> 00:21:08,600
started getting this check running so

00:21:06,590 --> 00:21:10,250
that's like kind of cool but I think

00:21:08,600 --> 00:21:12,380
that's this should be sort of a given

00:21:10,250 --> 00:21:16,280
for if you bring a note online it's

00:21:12,380 --> 00:21:19,340
monitored so what about when we we scale

00:21:16,280 --> 00:21:20,600
down so this might be an event where you

00:21:19,340 --> 00:21:22,040
have an auto scaling group that went up

00:21:20,600 --> 00:21:23,900
to five and now we're going to say that

00:21:22,040 --> 00:21:26,660
I was getting group will go down to one

00:21:23,900 --> 00:21:28,730
so what happens here is we don't

00:21:26,660 --> 00:21:31,480
actually know which one's of those five

00:21:28,730 --> 00:21:33,710
are going to be taken out of production

00:21:31,480 --> 00:21:35,870
Google is going to decide that for us so

00:21:33,710 --> 00:21:38,300
only one of these five will remain at

00:21:35,870 --> 00:21:44,150
the end and once that happens I'm not

00:21:38,300 --> 00:21:45,950
sure why this is running then we will

00:21:44,150 --> 00:21:47,330
have only one but we won't know which

00:21:45,950 --> 00:21:48,650
one it is so there's no way to tell our

00:21:47,330 --> 00:21:51,830
monitoring tool at this point which

00:21:48,650 --> 00:22:01,370
which one we're going to remove having

00:21:51,830 --> 00:22:03,260
internet yeah Wi-Fi somebody said to me

00:22:01,370 --> 00:22:05,450
yesterday if your talk is interesting

00:22:03,260 --> 00:22:07,040
then you'll know because your Wi-Fi

00:22:05,450 --> 00:22:08,450
connection will work because if it's not

00:22:07,040 --> 00:22:11,560
interesting then everyone will be on the

00:22:08,450 --> 00:22:15,700
Wi-Fi and they'll break the Wi-Fi oh

00:22:11,560 --> 00:22:15,700
let's see come on

00:22:20,010 --> 00:22:34,269
we I had a slide prepared just for this

00:22:26,039 --> 00:22:40,179
if this happen okay so let's try that

00:22:34,269 --> 00:22:42,960
again come on okay there we go so now we

00:22:40,179 --> 00:22:45,309
can just see this one back in there and

00:22:42,960 --> 00:22:47,230
if we go back to a monitoring tool we

00:22:45,309 --> 00:22:49,720
can see now there's only one so there's

00:22:47,230 --> 00:22:52,029
no events here we didn't get alerted

00:22:49,720 --> 00:22:53,559
there was no notification sent and what

00:22:52,029 --> 00:22:58,019
actually happened in this case was it

00:22:53,559 --> 00:23:00,760
was a graceful shutdown so sensu is a

00:22:58,019 --> 00:23:03,250
message bus back to monitoring tools so

00:23:00,760 --> 00:23:06,250
everything that happens is through sent

00:23:03,250 --> 00:23:08,470
to it the client will publish messages

00:23:06,250 --> 00:23:10,690
on to the message bus in this case what

00:23:08,470 --> 00:23:13,570
happened was the censor client knew that

00:23:10,690 --> 00:23:15,549
the node was being asked to shutdown so

00:23:13,570 --> 00:23:17,380
just a normal graceful shutdown on the

00:23:15,549 --> 00:23:20,559
node and it's part of the service

00:23:17,380 --> 00:23:22,899
stopping the last message that sends to

00:23:20,559 --> 00:23:24,610
sent basically on to the queue it was

00:23:22,899 --> 00:23:26,470
I'm going away don't worry about me

00:23:24,610 --> 00:23:29,260
anymore don't monitor me everything you

00:23:26,470 --> 00:23:34,690
know about me is gone no events I just

00:23:29,260 --> 00:23:37,440
just cleared out and so at no point was

00:23:34,690 --> 00:23:40,809
did sense we think that these nodes were

00:23:37,440 --> 00:23:42,669
incorrectly down so it knew that they

00:23:40,809 --> 00:23:45,970
were they were going away so again it's

00:23:42,669 --> 00:23:47,889
like a pretty standard instance you want

00:23:45,970 --> 00:23:49,299
to like always make it should be a given

00:23:47,889 --> 00:23:51,519
that like when you bring on a new node

00:23:49,299 --> 00:23:52,809
you don't get notified about it when you

00:23:51,519 --> 00:23:56,350
just take down a node gracefully you

00:23:52,809 --> 00:23:58,090
don't get notified about it either but

00:23:56,350 --> 00:23:59,799
there is a case where we do want to be

00:23:58,090 --> 00:24:02,440
notified that case would be if the node

00:23:59,799 --> 00:24:08,289
itself had a problem and so one example

00:24:02,440 --> 00:24:11,380
of that would be if the node if the

00:24:08,289 --> 00:24:14,440
website itself crashed so the one thing

00:24:11,380 --> 00:24:18,580
that this very basic back-end does is it

00:24:14,440 --> 00:24:19,750
actually if you send a post to it and

00:24:18,580 --> 00:24:22,000
technically anyone can do this because

00:24:19,750 --> 00:24:25,419
there's no security but it might make my

00:24:22,000 --> 00:24:27,730
demo even more difficult then what

00:24:25,419 --> 00:24:28,899
happens is if we jump through this load

00:24:27,730 --> 00:24:31,770
balance so there should be one of these

00:24:28,899 --> 00:24:34,940
at some point that is unhealthy

00:24:31,770 --> 00:24:34,940
it's gonna keep getting the healthy ones

00:24:35,179 --> 00:24:44,100
it's not really that important if this

00:24:37,140 --> 00:24:53,610
doesn't someone's getting paged because

00:24:44,100 --> 00:24:54,720
the websites down all right yeah that's

00:24:53,610 --> 00:24:57,090
right cuz I was worried I was gonna do

00:24:54,720 --> 00:25:00,480
it accidentally before the demo ha ha

00:24:57,090 --> 00:25:04,290
thanks all right there we go oh yeah and

00:25:00,480 --> 00:25:06,600
we got a response okay so yeah we go we

00:25:04,290 --> 00:25:10,020
have unhealthy and so now since he was

00:25:06,600 --> 00:25:14,120
actually going to care about this once

00:25:10,020 --> 00:25:17,490
it runs the next check on that back-end

00:25:14,120 --> 00:25:18,870
there we go critical 500 so in this case

00:25:17,490 --> 00:25:19,830
we want to get paged if that happens at

00:25:18,870 --> 00:25:21,059
3 o'clock in the morning you want to

00:25:19,830 --> 00:25:24,450
know that the website is no longer

00:25:21,059 --> 00:25:26,760
working at 3 o'clock in the morning and

00:25:24,450 --> 00:25:28,559
so we've so three events have happened I

00:25:26,760 --> 00:25:31,770
guess the first event was a new server

00:25:28,559 --> 00:25:34,500
got created the second event was that a

00:25:31,770 --> 00:25:35,790
new server went away intentionally so

00:25:34,500 --> 00:25:37,500
you don't want to get woken up at 3

00:25:35,790 --> 00:25:40,050
o'clock in the morning for the

00:25:37,500 --> 00:25:41,520
intentional server being removed and

00:25:40,050 --> 00:25:42,600
then the website crashed you do want to

00:25:41,520 --> 00:25:45,480
get woken up in the morning if the

00:25:42,600 --> 00:25:48,210
website crashed there's another instance

00:25:45,480 --> 00:25:50,130
where you do want to get woken up which

00:25:48,210 --> 00:25:52,500
is where the server itself crashes

00:25:50,130 --> 00:25:54,090
completely before it has an opportunity

00:25:52,500 --> 00:25:56,850
to announce that it's going away so that

00:25:54,090 --> 00:25:58,590
might be a catastrophic power failure or

00:25:56,850 --> 00:26:02,220
a server failure of some type that

00:25:58,590 --> 00:26:06,090
completely removes at the client it's

00:26:02,220 --> 00:26:08,070
what we do in that case is we would say

00:26:06,090 --> 00:26:10,530
I guess we refer to that as a legitimate

00:26:08,070 --> 00:26:11,670
monitoring event and that's definitely

00:26:10,530 --> 00:26:14,990
something that you need to trust the

00:26:11,670 --> 00:26:19,370
infrastructure to tell you about and so

00:26:14,990 --> 00:26:23,400
country the demo on Cuban Eddie's but

00:26:19,370 --> 00:26:25,740
basically if you have a server that gets

00:26:23,400 --> 00:26:27,210
completely obliterated before it has a

00:26:25,740 --> 00:26:30,150
chance to announce that it's going down

00:26:27,210 --> 00:26:31,800
then sense who is going to alert you

00:26:30,150 --> 00:26:33,900
obviously because it hasn't had that

00:26:31,800 --> 00:26:39,120
that graceful event that pops up and

00:26:33,900 --> 00:26:40,919
says I'm going away but and it should do

00:26:39,120 --> 00:26:43,350
that so you want to get woken up but

00:26:40,919 --> 00:26:45,510
there's also another case where what

00:26:43,350 --> 00:26:48,870
happens if 3 o'clock in the morning

00:26:45,510 --> 00:26:52,380
a server is is taken out of production

00:26:48,870 --> 00:26:54,840
in an ungraceful way but your cloud

00:26:52,380 --> 00:26:58,770
provider for example knows about it and

00:26:54,840 --> 00:27:00,600
replaces it with another instance so if

00:26:58,770 --> 00:27:03,330
you were to query the API for your cloud

00:27:00,600 --> 00:27:05,040
provider to ask them whether that

00:27:03,330 --> 00:27:06,270
instance exists or not then they'll say

00:27:05,040 --> 00:27:08,010
no it doesn't exist because their

00:27:06,270 --> 00:27:09,810
provision a new instance to replace it

00:27:08,010 --> 00:27:11,700
and so that's actually a case where even

00:27:09,810 --> 00:27:12,780
if there's a catastrophic power failure

00:27:11,700 --> 00:27:15,930
at 3 o'clock in the morning you don't

00:27:12,780 --> 00:27:18,810
want to be notified and so what we do in

00:27:15,930 --> 00:27:22,020
that case is we reach out to in that

00:27:18,810 --> 00:27:24,360
case it would be the ec2 API and we say

00:27:22,020 --> 00:27:27,480
so since we cease this server has gone

00:27:24,360 --> 00:27:28,710
away and it'll say I'm going to start a

00:27:27,480 --> 00:27:30,180
loading people but before I started

00:27:28,710 --> 00:27:32,070
loading people I'm gonna check the ec2

00:27:30,180 --> 00:27:34,110
API and I'm going to see the list of

00:27:32,070 --> 00:27:37,500
servers and the states of those service

00:27:34,110 --> 00:27:39,060
that Amazon expects them to be in and if

00:27:37,500 --> 00:27:41,670
the state of the server that I'm about

00:27:39,060 --> 00:27:43,860
to alert on is that it is down then I'm

00:27:41,670 --> 00:27:47,730
not going to alert so that's the other

00:27:43,860 --> 00:27:49,820
edge case where if you have an external

00:27:47,730 --> 00:27:52,920
source of truth that you can reconcile

00:27:49,820 --> 00:27:55,980
the monitoring against then you're able

00:27:52,920 --> 00:27:57,690
to say like I only know this much about

00:27:55,980 --> 00:27:59,280
the the infrastructure but another

00:27:57,690 --> 00:28:02,820
system it might be chef or might be

00:27:59,280 --> 00:28:04,440
puppet or it might be ec2 knows other

00:28:02,820 --> 00:28:05,910
information let's reconcile the

00:28:04,440 --> 00:28:09,920
information together before we start

00:28:05,910 --> 00:28:11,880
tricking alerts and and then you

00:28:09,920 --> 00:28:15,060
hopefully through those we'll end up

00:28:11,880 --> 00:28:18,990
with a case where every alert that you

00:28:15,060 --> 00:28:21,690
get theoretically in in any event of a

00:28:18,990 --> 00:28:24,050
server being intentionally provisioned

00:28:21,690 --> 00:28:27,270
or intentionally or unintentionally

00:28:24,050 --> 00:28:29,370
decommissioned will be an accurate alert

00:28:27,270 --> 00:28:39,000
a legitimate monitoring event that you

00:28:29,370 --> 00:28:43,490
want to be working up for he's an

00:28:39,000 --> 00:28:43,490
exciting slide question time

00:28:43,510 --> 00:28:49,550
thank you a nice presentation my

00:28:46,490 --> 00:28:51,200
question is so it's it's really nice

00:28:49,550 --> 00:28:53,630
demonstration like if you add the

00:28:51,200 --> 00:28:58,040
containers or the pots they are getting

00:28:53,630 --> 00:29:00,770
automatically monitored but is it also

00:28:58,040 --> 00:29:04,400
possible to like say only give me an

00:29:00,770 --> 00:29:06,470
alert of like less than like if the

00:29:04,400 --> 00:29:08,390
number of available pots are like less

00:29:06,470 --> 00:29:10,310
than 80% because this is in fact

00:29:08,390 --> 00:29:12,620
actually what you may be yep are more

00:29:10,310 --> 00:29:14,750
interested in not a specific container

00:29:12,620 --> 00:29:16,340
going down yeah so that's one that's

00:29:14,750 --> 00:29:18,320
probably another level from that the

00:29:16,340 --> 00:29:20,030
concept of taking a server and then

00:29:18,320 --> 00:29:24,500
turning into a role and then you go from

00:29:20,030 --> 00:29:27,280
that role in in the service sense to

00:29:24,500 --> 00:29:31,640
like a wider service so you might say

00:29:27,280 --> 00:29:34,850
this the role of this server is of this

00:29:31,640 --> 00:29:37,280
instance is a web role but the service

00:29:34,850 --> 00:29:40,490
it's providing is like the web service

00:29:37,280 --> 00:29:42,620
and on the web service we need 80% of

00:29:40,490 --> 00:29:44,930
nodes online for example so the way we

00:29:42,620 --> 00:29:47,930
do that in sensitive is a thing called

00:29:44,930 --> 00:29:49,970
named aggregates where you can define an

00:29:47,930 --> 00:29:51,740
aggregate pool and then you can define

00:29:49,970 --> 00:29:54,440
the percentage of nodes in that pool

00:29:51,740 --> 00:29:59,240
that need to be okay in order for it to

00:29:54,440 --> 00:30:03,010
maintain that okay state hi thank you

00:29:59,240 --> 00:30:06,620
for the talk you scaled up your health

00:30:03,010 --> 00:30:09,440
web service to a number of five now

00:30:06,620 --> 00:30:16,070
imagine it auto scales or you scale it

00:30:09,440 --> 00:30:19,070
up to 5,000 how fast will send to run

00:30:16,070 --> 00:30:22,940
the HTTP checks will it run them as fast

00:30:19,070 --> 00:30:25,250
as possible all in parallel yes the

00:30:22,940 --> 00:30:27,560
topology is that the sensory servers

00:30:25,250 --> 00:30:30,230
only job at that point is to publish a

00:30:27,560 --> 00:30:32,660
request to say all web servers need to

00:30:30,230 --> 00:30:35,360
run this HTTP check so it's it publishes

00:30:32,660 --> 00:30:38,030
the one check request whether there's

00:30:35,360 --> 00:30:39,830
one server or 10,000 service and then

00:30:38,030 --> 00:30:41,750
all of the clients jobs is to listen

00:30:39,830 --> 00:30:44,060
they subscribe to that queue and they

00:30:41,750 --> 00:30:46,700
each see the request so there's no

00:30:44,060 --> 00:30:49,400
difference conceptually in in the way

00:30:46,700 --> 00:30:53,050
that it's making that check request

00:30:49,400 --> 00:30:53,050
between one or 10,000

00:30:54,340 --> 00:30:57,860
sorry very quick question what do you

00:30:56,750 --> 00:30:59,720
call that feature that where you

00:30:57,860 --> 00:31:01,670
reconcile the check against another

00:30:59,720 --> 00:31:06,860
external source is that a double check

00:31:01,670 --> 00:31:09,230
so I think so we just refer to them

00:31:06,860 --> 00:31:10,970
internally as integrations so there's an

00:31:09,230 --> 00:31:14,090
ec2 integration a chef integration a

00:31:10,970 --> 00:31:16,210
puppet integration and the job of an

00:31:14,090 --> 00:31:18,560
integration is to do the reconciliation

00:31:16,210 --> 00:31:20,420
I don't know if there's a more specific

00:31:18,560 --> 00:31:24,190
term that we use in that event where

00:31:20,420 --> 00:31:28,480
we're saying like check before you alert

00:31:24,190 --> 00:31:28,480
yeah we could call it double check yeah

00:31:28,870 --> 00:31:34,910
going to need him at OS empty is the

00:31:32,960 --> 00:31:38,780
double check connected to notifications

00:31:34,910 --> 00:31:41,570
or for the individual status so what'll

00:31:38,780 --> 00:31:43,040
actually happen is if we go out to so

00:31:41,570 --> 00:31:46,190
let's say since you sees that a client

00:31:43,040 --> 00:31:48,530
is a instances gone offline and so but

00:31:46,190 --> 00:31:50,750
it hasn't gracefully the Commission

00:31:48,530 --> 00:31:53,120
itself then the first thing that we

00:31:50,750 --> 00:31:56,120
would do ordinarily would be a raise an

00:31:53,120 --> 00:31:57,560
alert but when we go out to ec2 and we

00:31:56,120 --> 00:31:58,850
we check the list of instances and we

00:31:57,560 --> 00:32:00,680
see the state of that instance is

00:31:58,850 --> 00:32:02,690
terminated for example then we delete

00:32:00,680 --> 00:32:04,400
the client so monitoring doesn't care

00:32:02,690 --> 00:32:06,440
about at all and and if there's any

00:32:04,400 --> 00:32:09,760
existing events for it they'll go away

00:32:06,440 --> 00:32:13,450
it's as if it never existed okay

00:32:09,760 --> 00:32:13,450
any more questions

00:32:24,850 --> 00:32:30,010
so let's say you gracefully shut down a

00:32:27,640 --> 00:32:31,810
machine and afterwards I'm interested in

00:32:30,010 --> 00:32:33,880
performance data of that machine is

00:32:31,810 --> 00:32:37,990
there any concept to get it afterwards

00:32:33,880 --> 00:32:39,640
when it's later from monitoring so if

00:32:37,990 --> 00:32:41,710
you gracefully shut it down and then you

00:32:39,640 --> 00:32:43,870
want what the only way to get data from

00:32:41,710 --> 00:32:46,630
it again would be to to bring it back

00:32:43,870 --> 00:32:49,000
online wait hang on if the machines

00:32:46,630 --> 00:32:52,210
totally gone away and you're storing

00:32:49,000 --> 00:32:54,340
state historical data for example in

00:32:52,210 --> 00:32:55,900
something like graphite or open TS DB

00:32:54,340 --> 00:32:58,870
then you would go and look for the

00:32:55,900 --> 00:33:01,540
historical data in that other place one

00:32:58,870 --> 00:33:03,220
thing that we do so without integrations

00:33:01,540 --> 00:33:06,270
and like plugins the open source

00:33:03,220 --> 00:33:08,440
plug-ins for sensor so you can send

00:33:06,270 --> 00:33:10,420
metrics data into all those systems but

00:33:08,440 --> 00:33:12,910
if you have logging systems so you want

00:33:10,420 --> 00:33:14,860
to send that data to we have a thing as

00:33:12,910 --> 00:33:16,630
well called the event stream which

00:33:14,860 --> 00:33:20,170
basically everything that sensing knows

00:33:16,630 --> 00:33:21,340
about it just sends down a pipe in JSON

00:33:20,170 --> 00:33:24,850
format so you can connect that to

00:33:21,340 --> 00:33:28,030
something like elasticsearch so we don't

00:33:24,850 --> 00:33:29,740
have a concept of historical data beyond

00:33:28,030 --> 00:33:32,560
the last 21 executions of a check

00:33:29,740 --> 00:33:34,150
incense to itself and the way that we

00:33:32,560 --> 00:33:35,950
deal with that is to integrate with

00:33:34,150 --> 00:33:38,830
other tools that are way better at doing

00:33:35,950 --> 00:33:41,740
that but if you brought the node back

00:33:38,830 --> 00:33:43,120
online again so and it announced itself

00:33:41,740 --> 00:33:44,260
as the same client and everything we

00:33:43,120 --> 00:33:45,790
would just start monitoring again it

00:33:44,260 --> 00:33:54,420
would start collecting metrics again and

00:33:45,790 --> 00:33:54,420
they go to the same place more homes

00:34:01,520 --> 00:34:07,130
just a question of when you run in

00:34:03,710 --> 00:34:09,169
kubernetes do you run the sensor client

00:34:07,130 --> 00:34:11,810
as part of you clustered I sell as a

00:34:09,169 --> 00:34:13,849
daemon set or does it run outside so

00:34:11,810 --> 00:34:16,970
it's like a sidecar container in the

00:34:13,849 --> 00:34:20,149
same pod in this instance okay so you

00:34:16,970 --> 00:34:22,700
run it in every part yep okay could you

00:34:20,149 --> 00:34:24,950
run it as a daemon set also for the sort

00:34:22,700 --> 00:34:26,690
of every instance of the notes in the

00:34:24,950 --> 00:34:28,099
kubernetes yes yeah there's a couple of

00:34:26,690 --> 00:34:30,619
different patterns or you could also run

00:34:28,099 --> 00:34:32,960
one and have the nodes themselves emit

00:34:30,619 --> 00:34:36,560
data out to that like so you could run

00:34:32,960 --> 00:34:46,460
one somewhere else for example yeah okay

00:34:36,560 --> 00:34:53,760
thank you thank you very much Anthony

00:34:46,460 --> 00:34:53,760

YouTube URL: https://www.youtube.com/watch?v=TQoUwm7ERT0


