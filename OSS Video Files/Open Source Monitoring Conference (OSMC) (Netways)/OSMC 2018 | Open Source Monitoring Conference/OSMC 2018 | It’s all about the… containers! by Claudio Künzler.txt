Title: OSMC 2018 | It’s all about the… containers! by Claudio Künzler
Publication date: 2018-11-19
Playlist: OSMC 2018 | Open Source Monitoring Conference
Description: 
	After the virtualization we’re now living the next migration trend: “containerization”.
Almost everyone is talking about containers. But what are containers and can they be monitored the same way as classical systems?
This presentation gives insights about System Containers (LXC) and Application Containers (Docker) and how they can be monitored using Icinga 2 with check_lxc and check_rancher2.


NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de/
Blog: http://blog.netways.de/
NWS: https://nws.netways.de

Webinare
Archiv Link: https://www.netways.de/webinare/archi...
Aktuell: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh/

www.musicfox.com
Captions: 
	00:00:01,940 --> 00:00:13,270
[Music]

00:00:14,299 --> 00:00:18,240
thank you

00:00:15,690 --> 00:00:22,380
hello everyone do you hear me is it too

00:00:18,240 --> 00:00:25,410
loud is it too good okay good thank you

00:00:22,380 --> 00:00:29,730
so welcome to the talk it's a little

00:00:25,410 --> 00:00:32,820
about containers that's like a pretty

00:00:29,730 --> 00:00:34,860
fake title actually because it would be

00:00:32,820 --> 00:00:37,350
how to integrate modern containers in a

00:00:34,860 --> 00:00:39,239
classical system monitoring but it's all

00:00:37,350 --> 00:00:42,570
about the containers is short and sexy

00:00:39,239 --> 00:00:47,219
so it's probably or more catchy so

00:00:42,570 --> 00:00:49,410
that's why you're here quick agenda here

00:00:47,219 --> 00:00:52,110
we are going to talk about what are

00:00:49,410 --> 00:00:55,020
containers so basically a brief

00:00:52,110 --> 00:00:58,109
introduction into the container world

00:00:55,020 --> 00:01:00,629
we're talking about Lexi how to monitor

00:00:58,109 --> 00:01:03,120
Linux containers and we're of course

00:01:00,629 --> 00:01:08,780
also talking about application

00:01:03,120 --> 00:01:10,799
containers or docker containers quick

00:01:08,780 --> 00:01:12,210
introduction about me my name is Carrie

00:01:10,799 --> 00:01:15,390
Quinn so thank you for the introduction

00:01:12,210 --> 00:01:17,640
before I live in Switzerland I report to

00:01:15,390 --> 00:01:20,670
a massive process for those of you who

00:01:17,640 --> 00:01:23,880
are married you know what I mean and I

00:01:20,670 --> 00:01:26,570
have two kids I work at NC State Media

00:01:23,880 --> 00:01:29,250
Group that's no it's your hots item and

00:01:26,570 --> 00:01:33,930
besides that I also work at Infineon

00:01:29,250 --> 00:01:35,909
root comm I maintain a couple of

00:01:33,930 --> 00:01:38,400
monitoring plugins maybe you know some

00:01:35,909 --> 00:01:41,270
of them the one which is probably most

00:01:38,400 --> 00:01:46,909
known is check ESXi hardware for

00:01:41,270 --> 00:01:50,640
Hardware monitoring of ESXi servers so

00:01:46,909 --> 00:01:53,939
containers quick question into to all of

00:01:50,640 --> 00:01:58,710
you actually so who has heard about

00:01:53,939 --> 00:02:02,159
containers let's reverse that who that

00:01:58,710 --> 00:02:05,060
who hasn't heard of containers good so

00:02:02,159 --> 00:02:09,450
I'm talking to the good audience here

00:02:05,060 --> 00:02:13,350
who has already deployed doing some

00:02:09,450 --> 00:02:16,900
testing of containers

00:02:13,350 --> 00:02:18,340
okay now the the really essential

00:02:16,900 --> 00:02:22,660
question who is using containers in

00:02:18,340 --> 00:02:27,370
production okay already ten percent

00:02:22,660 --> 00:02:29,650
that's good that's a container so when

00:02:27,370 --> 00:02:31,570
you're talking when you're in the cargo

00:02:29,650 --> 00:02:34,900
world we're talking about a unit a

00:02:31,570 --> 00:02:38,200
container is a unit size of one teu

00:02:34,900 --> 00:02:40,990
which means or stands for 20 feet

00:02:38,200 --> 00:02:43,750
equivalent unit so 20 feet container

00:02:40,990 --> 00:02:47,200
size as you can see there is a double

00:02:43,750 --> 00:02:51,670
size 40 feet exactly the double amount

00:02:47,200 --> 00:02:53,770
of the container equals to teu and as

00:02:51,670 --> 00:02:58,810
you can imagine they stacked up quite

00:02:53,770 --> 00:03:03,430
nicely checking them together gives us

00:02:58,810 --> 00:03:06,760
some benefits in the on the vessel there

00:03:03,430 --> 00:03:09,610
is efficiency we eliminate eliminate use

00:03:06,760 --> 00:03:12,640
of space between the containers so more

00:03:09,610 --> 00:03:14,950
containers on the ship and we have

00:03:12,640 --> 00:03:17,440
stability but they don't wobble around

00:03:14,950 --> 00:03:20,470
as if you would just make one large

00:03:17,440 --> 00:03:23,470
Tower of a container so stability also

00:03:20,470 --> 00:03:25,900
equals security for the staff working on

00:03:23,470 --> 00:03:28,060
board so they're not like a container

00:03:25,900 --> 00:03:32,350
falling on the people working on the

00:03:28,060 --> 00:03:36,100
ship comparing to the tech here when we

00:03:32,350 --> 00:03:39,130
stack containers when we are comparing

00:03:36,100 --> 00:03:43,810
this to our technical world we will talk

00:03:39,130 --> 00:03:46,180
about redundancy and scalability so in a

00:03:43,810 --> 00:03:48,750
perfect world the vessel ships out with

00:03:46,180 --> 00:03:52,329
a couple of thousand of containers and

00:03:48,750 --> 00:03:56,739
there's not a storm there are never high

00:03:52,329 --> 00:03:59,140
waves there's never an incident but of

00:03:56,739 --> 00:04:03,329
course there's a but the world is not

00:03:59,140 --> 00:04:06,519
perfect we have several environments

00:04:03,329 --> 00:04:08,980
environmental things we have to consider

00:04:06,519 --> 00:04:11,590
we have ways with storms we have

00:04:08,980 --> 00:04:14,820
technical equipment which can fail we

00:04:11,590 --> 00:04:19,570
have human errors which can occur and

00:04:14,820 --> 00:04:21,130
containers can fall overboard so as a

00:04:19,570 --> 00:04:24,980
matter of fact 10 years ago I was

00:04:21,130 --> 00:04:27,380
working on a tactic container ship

00:04:24,980 --> 00:04:29,510
in company and there was a brief

00:04:27,380 --> 00:04:33,050
introduction into shipping containers

00:04:29,510 --> 00:04:35,420
etc and I just had one question after

00:04:33,050 --> 00:04:38,140
this introduction and it was a two

00:04:35,420 --> 00:04:41,870
container sometime fall overboard and

00:04:38,140 --> 00:04:44,870
the answer was pretty surprising to me

00:04:41,870 --> 00:04:46,790
it was just yes all the time like just

00:04:44,870 --> 00:04:51,050
as if it's the most normal thing in the

00:04:46,790 --> 00:04:54,050
world and actually according to

00:04:51,050 --> 00:04:56,690
statistics 1,400 containers fall

00:04:54,050 --> 00:04:58,400
overboard each year so just imagine

00:04:56,690 --> 00:05:01,760
thousand four hundred containers on the

00:04:58,400 --> 00:05:03,980
ground of the ocean every year so in our

00:05:01,760 --> 00:05:06,290
world our containers don't fall

00:05:03,980 --> 00:05:08,540
overboard but they can carry they can

00:05:06,290 --> 00:05:12,800
crashed it can freeze so we need to

00:05:08,540 --> 00:05:17,180
monitor them so what are containers

00:05:12,800 --> 00:05:19,070
again we need to distinguish first so

00:05:17,180 --> 00:05:21,410
there are currently two types of

00:05:19,070 --> 00:05:25,100
containers we are talking about Lexi

00:05:21,410 --> 00:05:27,980
Linux containers they are also known as

00:05:25,100 --> 00:05:29,930
system containers these days and this is

00:05:27,980 --> 00:05:33,320
more like a lightweight virtual machine

00:05:29,930 --> 00:05:36,020
and we also have docker containers or

00:05:33,320 --> 00:05:39,200
chests containers as most developers

00:05:36,020 --> 00:05:41,720
talk to and these are referred to

00:05:39,200 --> 00:05:46,520
application containers and it's mostly a

00:05:41,720 --> 00:05:48,740
single process started so if we just

00:05:46,520 --> 00:05:52,850
look at in general containers a

00:05:48,740 --> 00:05:54,950
container is a simple process and of

00:05:52,850 --> 00:05:57,950
course every process inside the process

00:05:54,950 --> 00:06:00,350
is obviously a child process now you

00:05:57,950 --> 00:06:02,360
know all that and they use the same

00:06:00,350 --> 00:06:03,740
kernel so we have the host system we

00:06:02,360 --> 00:06:06,250
have the containers on it

00:06:03,740 --> 00:06:09,170
there is the same kernel we don't have

00:06:06,250 --> 00:06:10,400
hardware virtualization in between that

00:06:09,170 --> 00:06:13,040
makes it really fast that's why

00:06:10,400 --> 00:06:16,030
containers are really the way to go

00:06:13,040 --> 00:06:18,890
these days we can also set resource

00:06:16,030 --> 00:06:21,170
resource allocations and limits using C

00:06:18,890 --> 00:06:24,710
groups which is already been awhile

00:06:21,170 --> 00:06:26,900
there in the Linux kernel and if we

00:06:24,710 --> 00:06:31,340
think that containers is something new

00:06:26,900 --> 00:06:35,000
it's actually not so we can go way back

00:06:31,340 --> 00:06:36,980
to FreeBSD and year 2000 I will skip the

00:06:35,000 --> 00:06:38,780
first few months and I will directly go

00:06:36,980 --> 00:06:41,930
to Lexi's since

00:06:38,780 --> 00:06:46,610
2007 currently it's maintained by the

00:06:41,930 --> 00:06:49,910
Ubuntu guys at canonical and they in the

00:06:46,610 --> 00:06:51,770
recent few years they adapted a new an

00:06:49,910 --> 00:06:54,680
additional name for it so system

00:06:51,770 --> 00:06:56,990
containers was primarily used to now

00:06:54,680 --> 00:06:59,480
distinguish between application

00:06:56,990 --> 00:07:02,419
containers and system containers because

00:06:59,480 --> 00:07:06,980
there was miss a little bit of

00:07:02,419 --> 00:07:10,070
misunderstanding and docker came shortly

00:07:06,980 --> 00:07:13,070
afterwards actually and first was based

00:07:10,070 --> 00:07:17,770
on Lexi until they developed their own

00:07:13,070 --> 00:07:22,430
like lip container and container engine

00:07:17,770 --> 00:07:23,450
a couple of years back they or since

00:07:22,430 --> 00:07:27,740
they have been developing their own

00:07:23,450 --> 00:07:31,250
engine they have continuously worked on

00:07:27,740 --> 00:07:35,750
it and out came actually container D and

00:07:31,250 --> 00:07:38,300
container D is now part of the tower

00:07:35,750 --> 00:07:40,880
native compute foundation of the Linux

00:07:38,300 --> 00:07:50,479
Foundation and it's not part of Tokra

00:07:40,880 --> 00:07:52,850
anymore let's start with Lexi so as I

00:07:50,479 --> 00:07:54,710
said Lexi can become compared to a

00:07:52,850 --> 00:07:56,990
classical virtual machine like the

00:07:54,710 --> 00:07:58,760
lightweight virtual machine we don't

00:07:56,990 --> 00:08:01,760
have the hardware virtualization which

00:07:58,760 --> 00:08:04,700
we gain speed from the direct access to

00:08:01,760 --> 00:08:07,880
hardware we usually have a dedicated

00:08:04,700 --> 00:08:12,410
virtual NIC which is a breached virtual

00:08:07,880 --> 00:08:14,600
NIC into the container and we usually

00:08:12,410 --> 00:08:18,169
have the full network access so what you

00:08:14,600 --> 00:08:26,660
usually do is you attach your Lexi

00:08:18,169 --> 00:08:28,460
container sorry you attach your Lexi

00:08:26,660 --> 00:08:30,800
container right into your existing

00:08:28,460 --> 00:08:33,229
network so into your existing subnet or

00:08:30,800 --> 00:08:35,080
wherever you would just plug your

00:08:33,229 --> 00:08:37,760
machine like a virtual machine actually

00:08:35,080 --> 00:08:40,010
you usually or you always have your

00:08:37,760 --> 00:08:43,750
dedicated file system called the route

00:08:40,010 --> 00:08:47,630
FS the best practice is generally to use

00:08:43,750 --> 00:08:49,280
a logical volume for each container so

00:08:47,630 --> 00:08:53,390
you really have your own capacity you

00:08:49,280 --> 00:08:55,279
have your own thresholds in general

00:08:53,390 --> 00:08:58,430
this applies to all containers it's a

00:08:55,279 --> 00:09:02,029
dedicated namespace for process

00:08:58,430 --> 00:09:03,860
isolation so when we're in the container

00:09:02,029 --> 00:09:06,470
you cannot see the processes of the host

00:09:03,860 --> 00:09:10,149
and you cannot see processes of other

00:09:06,470 --> 00:09:12,050
containers running on the same host the

00:09:10,149 --> 00:09:15,410
big difference between language

00:09:12,050 --> 00:09:18,079
containers and docker or application

00:09:15,410 --> 00:09:21,050
containers is the dedicated init system

00:09:18,079 --> 00:09:24,230
so you have a real init system starting

00:09:21,050 --> 00:09:27,589
in Lexi so as I say it basically a

00:09:24,230 --> 00:09:30,200
super-fast VM and as it's like a VM we

00:09:27,589 --> 00:09:32,300
can just treat it like a like a typical

00:09:30,200 --> 00:09:36,860
and classical host we install our

00:09:32,300 --> 00:09:40,100
monitoring agents or demons or SSH or

00:09:36,860 --> 00:09:44,800
whatever just as a typical machine you

00:09:40,100 --> 00:09:44,800
would give it just set up actually so

00:09:45,279 --> 00:09:50,600
monitoring processes inside a container

00:09:48,110 --> 00:09:54,290
is pretty straightforward

00:09:50,600 --> 00:09:55,940
you just launch PS and you see all the

00:09:54,290 --> 00:09:59,750
processes running inside that container

00:09:55,940 --> 00:10:01,820
we also see the init system or the init

00:09:59,750 --> 00:10:05,930
process of that container with pit one

00:10:01,820 --> 00:10:11,390
so as before like a typical host or VM

00:10:05,930 --> 00:10:14,329
just use check procs in that case five

00:10:11,390 --> 00:10:16,880
systems is pretty much the same to just

00:10:14,329 --> 00:10:20,089
run TF you see you're mounted file

00:10:16,880 --> 00:10:24,050
systems you can run check disk as as

00:10:20,089 --> 00:10:27,199
before so now it gets a little bit more

00:10:24,050 --> 00:10:32,600
tricky when you're going into monitoring

00:10:27,199 --> 00:10:35,480
of memory we have one problem we have on

00:10:32,600 --> 00:10:39,470
top we have the free output or the free

00:10:35,480 --> 00:10:42,529
command from running on the host and on

00:10:39,470 --> 00:10:45,649
the container at the same time we see we

00:10:42,529 --> 00:10:48,019
have exactly the same value so how is it

00:10:45,649 --> 00:10:49,760
possible the container is using the same

00:10:48,019 --> 00:10:53,060
amount of the host which is just not

00:10:49,760 --> 00:10:57,230
it's just not true and therefore we

00:10:53,060 --> 00:11:02,990
cannot really use monitoring our memory

00:10:57,230 --> 00:11:05,420
monitoring inside the container none of

00:11:02,990 --> 00:11:06,560
you is when we are using a command like

00:11:05,420 --> 00:11:08,960
H top

00:11:06,560 --> 00:11:10,940
we see exactly the same amount so we see

00:11:08,960 --> 00:11:13,880
the same memory usage we see the same

00:11:10,940 --> 00:11:17,060
CPU load we even have the same uptime of

00:11:13,880 --> 00:11:19,610
the host and the container only the

00:11:17,060 --> 00:11:22,010
tasks the number of tasks inside the

00:11:19,610 --> 00:11:23,900
container differs only the contain

00:11:22,010 --> 00:11:26,450
inside the container you're seeing your

00:11:23,900 --> 00:11:32,260
own processes yet you still see the

00:11:26,450 --> 00:11:36,529
resources use resources of the host now

00:11:32,260 --> 00:11:41,089
let's install like CFS let's try that

00:11:36,529 --> 00:11:43,610
one here so with that package installed

00:11:41,089 --> 00:11:44,990
we now suddenly see something else or

00:11:43,610 --> 00:11:48,020
something different

00:11:44,990 --> 00:11:50,960
suddenly well the CPU load is still the

00:11:48,020 --> 00:11:55,670
same but now we have a different memory

00:11:50,960 --> 00:11:58,070
usage we have a different uptime and as

00:11:55,670 --> 00:12:00,950
before the tasks the number of procs

00:11:58,070 --> 00:12:04,990
they still differ we still have them as

00:12:00,950 --> 00:12:09,140
before so what this leg ZFS doing is it

00:12:04,990 --> 00:12:11,690
virtualizes a part of /proc inside the

00:12:09,140 --> 00:12:16,030
container so it gets the real values

00:12:11,690 --> 00:12:20,390
from the host added inside the container

00:12:16,030 --> 00:12:22,130
you have to make sure that like CFS is

00:12:20,390 --> 00:12:25,910
actually installed it depends on your

00:12:22,130 --> 00:12:28,460
distribution just already Debian and

00:12:25,910 --> 00:12:30,220
Ubuntu is a big difference we see in

00:12:28,460 --> 00:12:34,370
Ubuntu it's coming as a recommendation

00:12:30,220 --> 00:12:37,550
so if you have apt in store the settings

00:12:34,370 --> 00:12:39,080
in apt are to use recommendations then

00:12:37,550 --> 00:12:43,339
it will be installed when you install

00:12:39,080 --> 00:12:49,880
Lexi the base package and in Debian you

00:12:43,339 --> 00:12:52,610
need to install it manually again the

00:12:49,880 --> 00:12:54,980
same thing for a free output from before

00:12:52,610 --> 00:12:59,600
but now this time is like ZFS installed

00:12:54,980 --> 00:13:02,810
so use is not the same anymore

00:12:59,600 --> 00:13:05,390
like we saw before in the h2 powd put so

00:13:02,810 --> 00:13:07,970
now we can really see what is inside the

00:13:05,390 --> 00:13:11,300
container and how much memory it's

00:13:07,970 --> 00:13:14,240
actually consuming but there is one

00:13:11,300 --> 00:13:17,540
problem and you can see on the left hand

00:13:14,240 --> 00:13:20,120
side in the column total we still see

00:13:17,540 --> 00:13:23,630
the total amount of

00:13:20,120 --> 00:13:28,900
Weber memory or total capacity of the

00:13:23,630 --> 00:13:32,600
host so now if you calculate the total

00:13:28,900 --> 00:13:36,370
capacity - used capacity you get an

00:13:32,600 --> 00:13:39,800
available and that's completely off so

00:13:36,370 --> 00:13:42,770
the come inside the container as you're

00:13:39,800 --> 00:13:44,390
not able to see that the order processes

00:13:42,770 --> 00:13:47,300
of the host or the other containers

00:13:44,390 --> 00:13:49,370
there's no way a container can know what

00:13:47,300 --> 00:13:53,480
the other containers are the host itself

00:13:49,370 --> 00:13:57,140
is using so just make sure you're not

00:13:53,480 --> 00:13:59,810
using that value so when you or if you

00:13:57,140 --> 00:14:02,240
want to monitor memory usage inside a

00:13:59,810 --> 00:14:07,100
container just make sure of that one

00:14:02,240 --> 00:14:07,610
column just make the UC : used use

00:14:07,100 --> 00:14:11,420
memory

00:14:07,610 --> 00:14:12,890
don't use pro plugin like check mem for

00:14:11,420 --> 00:14:17,120
example which makes exactly this

00:14:12,890 --> 00:14:22,130
calculation it's completely wrong an

00:14:17,120 --> 00:14:25,730
even more tricky way is to monitor CPU

00:14:22,130 --> 00:14:28,100
usage inside a container so as we saw

00:14:25,730 --> 00:14:30,470
before a container is actually always

00:14:28,100 --> 00:14:33,740
seeing the host usage exactly exactly

00:14:30,470 --> 00:14:36,100
the same value and as of now it's not

00:14:33,740 --> 00:14:38,959
possible to have it another way and

00:14:36,100 --> 00:14:41,570
there is one which I call a clumsy

00:14:38,959 --> 00:14:48,680
approach it's when we compare chiefess

00:14:41,570 --> 00:14:51,620
so Chi face is like the time spent on

00:14:48,680 --> 00:14:56,089
the CPU for example in the CPU usage or

00:14:51,620 --> 00:14:59,570
CPU system in the kernel space so what

00:14:56,089 --> 00:15:03,890
we can do is we just get a current value

00:14:59,570 --> 00:15:08,870
of for example time spent inside kernel

00:15:03,890 --> 00:15:12,020
CPU and wait five seconds get another

00:15:08,870 --> 00:15:14,180
value subtract the second versus the

00:15:12,020 --> 00:15:19,730
first and we get some value in this case

00:15:14,180 --> 00:15:24,200
just 328 so we got 328 chief is inside

00:15:19,730 --> 00:15:27,050
or within five seconds if we compare

00:15:24,200 --> 00:15:31,550
that with the hosts chief is we can get

00:15:27,050 --> 00:15:33,279
like an idea how much CPU is this

00:15:31,550 --> 00:15:40,180
container using of

00:15:33,279 --> 00:15:43,149
host CPU and just two weeks ago I spoke

00:15:40,180 --> 00:15:47,560
with christian browner from the lexi

00:15:43,149 --> 00:15:50,680
core team and he said that there will

00:15:47,560 --> 00:15:54,040
soon be something merged inside L into

00:15:50,680 --> 00:15:57,569
Lexi code that will allow to correctly

00:15:54,040 --> 00:16:01,620
monitor CPU usage inside the container

00:15:57,569 --> 00:16:05,170
so fingers crossed

00:16:01,620 --> 00:16:09,249
so to cope with the problems of memory

00:16:05,170 --> 00:16:12,639
and CPU usage monitoring I developed

00:16:09,249 --> 00:16:14,889
checked Lexi it's actually a workaround

00:16:12,639 --> 00:16:17,379
plug-in because as long as you cannot

00:16:14,889 --> 00:16:19,839
monitor that inside the container you

00:16:17,379 --> 00:16:23,920
can correctly or almost correctly

00:16:19,839 --> 00:16:27,850
monitor it at least on the LXE host it

00:16:23,920 --> 00:16:31,240
uses C group values so it uses a alexei

00:16:27,850 --> 00:16:33,850
C group command to check the C group

00:16:31,240 --> 00:16:37,389
values for each container running on the

00:16:33,850 --> 00:16:39,399
host and it also checks the auto start

00:16:37,389 --> 00:16:43,839
configuration in case you have forgotten

00:16:39,399 --> 00:16:45,970
it the memory usage and swap usage is

00:16:43,839 --> 00:16:50,199
correctly monitored so you really get

00:16:45,970 --> 00:16:52,209
correct value of each container and as I

00:16:50,199 --> 00:16:54,790
said the clumsy approach with the chief

00:16:52,209 --> 00:17:02,559
is it gives you an idea about the

00:16:54,790 --> 00:17:06,250
container usages of the CPU so if you

00:17:02,559 --> 00:17:13,600
deploy check Lexi in let's say I sing a

00:17:06,250 --> 00:17:16,959
- I suggest that you create variable

00:17:13,600 --> 00:17:21,069
like bars containers for each Lexi host

00:17:16,959 --> 00:17:24,360
and you add the number of containers or

00:17:21,069 --> 00:17:24,360
the container names on it

00:17:29,179 --> 00:17:35,669
and to make it a little bit easier we

00:17:32,730 --> 00:17:38,669
can use these values or this array in

00:17:35,669 --> 00:17:41,130
this case just to apply the relics in

00:17:38,669 --> 00:17:47,400
memory check for each container on that

00:17:41,130 --> 00:17:50,100
of that host in icing - in the web

00:17:47,400 --> 00:17:52,559
interface what we actually gain is a

00:17:50,100 --> 00:17:56,429
pretty quick overview of all containers

00:17:52,559 --> 00:17:58,410
and we almost immediately can see which

00:17:56,429 --> 00:18:04,789
ones are using most resources of the

00:17:58,410 --> 00:18:08,789
hosts so that's the workaround for that

00:18:04,789 --> 00:18:11,039
when we recap let's see so it's mostly

00:18:08,789 --> 00:18:16,020
the same way but not exactly the same

00:18:11,039 --> 00:18:18,450
way as a VM we have to monitor some

00:18:16,020 --> 00:18:27,570
resources from outside actually from the

00:18:18,450 --> 00:18:28,440
Lexy host and as I said soon or yeah

00:18:27,570 --> 00:18:30,960
soon

00:18:28,440 --> 00:18:33,630
whatever that exactly means but we are

00:18:30,960 --> 00:18:37,799
talking about weeks as I was told that

00:18:33,630 --> 00:18:46,380
we can use correct CPU resource

00:18:37,799 --> 00:18:52,230
monitoring so let's go to application

00:18:46,380 --> 00:18:54,150
containers so what is application

00:18:52,230 --> 00:18:55,559
container is actually supposed to be

00:18:54,150 --> 00:18:57,840
it's just a single process running

00:18:55,559 --> 00:19:00,500
inside a container so it's a single

00:18:57,840 --> 00:19:04,470
application single process started up

00:19:00,500 --> 00:19:07,110
inside a container it's by default

00:19:04,470 --> 00:19:11,250
stateless there is no data or supposed

00:19:07,110 --> 00:19:14,039
to be data inside a container you also

00:19:11,250 --> 00:19:17,549
have a dedicated virtual Nick as Lexi

00:19:14,039 --> 00:19:21,690
also VA th and here is a difference it's

00:19:17,549 --> 00:19:25,380
knotted so we actually run there's IP

00:19:21,690 --> 00:19:29,760
tables on the docker host which just

00:19:25,380 --> 00:19:32,190
forwards traffic to the container we

00:19:29,760 --> 00:19:34,110
also have a dedicated filesystem the

00:19:32,190 --> 00:19:37,409
difference here is it's by default or

00:19:34,110 --> 00:19:39,450
off or overlay FS and they share the

00:19:37,409 --> 00:19:42,059
same capacity so you have like a top

00:19:39,450 --> 00:19:45,240
level file system and all containers are

00:19:42,059 --> 00:19:47,010
using that top level file system even

00:19:45,240 --> 00:19:51,720
though they have their dedicated root

00:19:47,010 --> 00:19:52,649
file system same as like CFS or with

00:19:51,720 --> 00:19:55,679
Lexie

00:19:52,649 --> 00:19:58,909
dedicated namespaces we have the same

00:19:55,679 --> 00:20:02,039
things we cannot see outside a container

00:19:58,909 --> 00:20:04,409
here the big difference is that there is

00:20:02,039 --> 00:20:07,970
no init system because we are supposed

00:20:04,409 --> 00:20:10,220
to only run one container right and

00:20:07,970 --> 00:20:13,760
application containers are actually

00:20:10,220 --> 00:20:16,529
great solution for quickly scaling up

00:20:13,760 --> 00:20:18,809
applications so obviously behind the

00:20:16,529 --> 00:20:20,750
load balancer otherwise yeah how you

00:20:18,809 --> 00:20:24,899
would access them

00:20:20,750 --> 00:20:27,450
another important or something which

00:20:24,899 --> 00:20:30,570
crossed my mind lately was we rarely

00:20:27,450 --> 00:20:33,659
hear docker anymore these days we hear

00:20:30,570 --> 00:20:35,880
kubernetes we hear container T that's a

00:20:33,659 --> 00:20:38,909
reason the only reason for that is that

00:20:35,880 --> 00:20:42,299
docker actually gave their docker engine

00:20:38,909 --> 00:20:45,330
or container engine to the to container

00:20:42,299 --> 00:20:47,639
D so that's the reason why we're

00:20:45,330 --> 00:20:52,130
currently talking kubernetes and

00:20:47,639 --> 00:20:54,480
container d of course with that solution

00:20:52,130 --> 00:20:58,710
we get some additional monitoring

00:20:54,480 --> 00:21:01,590
challenges we are not allowed or we're

00:20:58,710 --> 00:21:06,860
not supposed to install additional

00:21:01,590 --> 00:21:10,159
agents processes listeners etcetera into

00:21:06,860 --> 00:21:12,299
an application container and

00:21:10,159 --> 00:21:15,059
additionally to that we have no direct

00:21:12,299 --> 00:21:20,549
network access because it's not through

00:21:15,059 --> 00:21:22,980
the IP tables so we could make or

00:21:20,549 --> 00:21:27,480
configure an expose port for each

00:21:22,980 --> 00:21:30,059
container to access it via the network

00:21:27,480 --> 00:21:32,419
but do you really want to do that for if

00:21:30,059 --> 00:21:35,340
you have thousands of containers so it's

00:21:32,419 --> 00:21:38,389
it means a lot of work and it's also

00:21:35,340 --> 00:21:41,760
standalone docker just running docker

00:21:38,389 --> 00:21:44,970
exocrine manually it means a lot of work

00:21:41,760 --> 00:21:46,889
so the add-ins the reason why there's a

00:21:44,970 --> 00:21:49,049
lot of orchestration tools like google

00:21:46,889 --> 00:21:53,040
natives for example out there which help

00:21:49,049 --> 00:21:56,970
you for that the most important

00:21:53,040 --> 00:21:58,860
change of mind actually to monitor

00:21:56,970 --> 00:22:01,080
application containers is that we have

00:21:58,860 --> 00:22:04,260
to stop thinking as a money as an

00:22:01,080 --> 00:22:06,690
application container as a host it's not

00:22:04,260 --> 00:22:09,660
a host in a classical way as it has no

00:22:06,690 --> 00:22:11,280
direct network access we're talking

00:22:09,660 --> 00:22:14,250
about a process we're talking about an

00:22:11,280 --> 00:22:17,130
application so we have to think of it as

00:22:14,250 --> 00:22:24,920
application monitoring and not host

00:22:17,130 --> 00:22:30,270
monitoring anymore Ranger is something

00:22:24,920 --> 00:22:33,600
like a management layer on top of the

00:22:30,270 --> 00:22:37,050
orchestration so we have two containers

00:22:33,600 --> 00:22:39,120
or pots in Cuba natives we have to

00:22:37,050 --> 00:22:43,950
contain a runtime which creates pot

00:22:39,120 --> 00:22:46,380
destroys pot etc and we have the

00:22:43,950 --> 00:22:48,900
orchestration layer like kubernetes is

00:22:46,380 --> 00:22:51,890
one of them managing the container

00:22:48,900 --> 00:22:55,620
runtime and on top of the kubernetes

00:22:51,890 --> 00:22:59,430
environment we have our Ranger

00:22:55,620 --> 00:23:02,130
environment so Ranger is like openshift

00:22:59,430 --> 00:23:07,730
another way of managing your kubernetes

00:23:02,130 --> 00:23:10,260
orchestration what is the reason why is

00:23:07,730 --> 00:23:14,690
it has a really intuitive user interface

00:23:10,260 --> 00:23:21,060
and a cool and we'll really good eight

00:23:14,690 --> 00:23:25,940
API so which we use for CI CD and before

00:23:21,060 --> 00:23:28,490
we were actually implementing docker or

00:23:25,940 --> 00:23:32,370
application containers we've

00:23:28,490 --> 00:23:35,130
investigated more than one year of time

00:23:32,370 --> 00:23:38,790
to really research what kind of docker

00:23:35,130 --> 00:23:41,940
solutions are there how do they cope our

00:23:38,790 --> 00:23:45,690
I still i sensing fee is it open source

00:23:41,940 --> 00:23:50,910
is not etc so finally we decided for

00:23:45,690 --> 00:23:53,940
Ranger and since yeah for 2017 we're in

00:23:50,910 --> 00:23:57,210
production with Ranger environments with

00:23:53,940 --> 00:24:01,070
a total of more than 1000 containers so

00:23:57,210 --> 00:24:03,870
far there's been a big difference also

00:24:01,070 --> 00:24:05,140
in our production system which is Ranger

00:24:03,870 --> 00:24:07,600
00:24:05,140 --> 00:24:10,390
version 1 reduces cattle as an

00:24:07,600 --> 00:24:14,410
orchestration as opposed to the new

00:24:10,390 --> 00:24:16,390
version version number 2 is completely

00:24:14,410 --> 00:24:20,620
built in Cuban eighties so there's a big

00:24:16,390 --> 00:24:22,120
change and because we're currently only

00:24:20,620 --> 00:24:26,560
running testing stage environments of

00:24:22,120 --> 00:24:28,870
render to we but we're planning to go

00:24:26,560 --> 00:24:31,780
into production in the next few weeks we

00:24:28,870 --> 00:24:34,990
need to mony tour our Ranger 2

00:24:31,780 --> 00:24:36,760
environments as well and yeah just if

00:24:34,990 --> 00:24:39,670
you wonder why I'm talking about Rangers

00:24:36,760 --> 00:24:45,030
just I'm just a community user I'm not a

00:24:39,670 --> 00:24:47,410
Salesman or something for a ranger quick

00:24:45,030 --> 00:24:50,050
overview of the Ranger interface so

00:24:47,410 --> 00:24:53,770
that's basically a workload or

00:24:50,050 --> 00:24:56,190
deployment in the kubernetes world you

00:24:53,770 --> 00:25:01,420
have to overview of of your kind of

00:24:56,190 --> 00:25:04,710
workloads or services and if we focus on

00:25:01,420 --> 00:25:08,230
this one here nginx test is just

00:25:04,710 --> 00:25:11,860
workload with nginx image deployed and

00:25:08,230 --> 00:25:16,680
mirror in the next few slides we will

00:25:11,860 --> 00:25:16,680
make health checks of that one workload

00:25:17,850 --> 00:25:25,870
so the container or in this case port in

00:25:22,000 --> 00:25:28,150
Cuba this world we can monitor it using

00:25:25,870 --> 00:25:30,610
readiness and liveness probes so there's

00:25:28,150 --> 00:25:34,210
two ways of making these probes actually

00:25:30,610 --> 00:25:37,180
so the readiness probes means that it

00:25:34,210 --> 00:25:39,310
tells kubernetes when the container is

00:25:37,180 --> 00:25:41,080
ready so when you're starting up an

00:25:39,310 --> 00:25:42,970
application for example a java

00:25:41,080 --> 00:25:47,080
application we takes much longer time

00:25:42,970 --> 00:25:50,650
when is when am i ready to be included

00:25:47,080 --> 00:25:52,300
into the Lopo lensing on the other hand

00:25:50,650 --> 00:25:54,760
we have two lifeless probe which is

00:25:52,300 --> 00:25:58,090
pretty much what we're usually using in

00:25:54,760 --> 00:26:01,750
monitoring world is like a typical HTTP

00:25:58,090 --> 00:26:03,220
GET to a Status page or something like

00:26:01,750 --> 00:26:07,210
that are you alive

00:26:03,220 --> 00:26:09,910
that's all it is in kubernetes we

00:26:07,210 --> 00:26:12,040
currently have three different way to

00:26:09,910 --> 00:26:14,830
make to run a probe or to configure a

00:26:12,040 --> 00:26:18,650
probe we can run a simple command that's

00:26:14,830 --> 00:26:22,520
just inside the container just make

00:26:18,650 --> 00:26:26,180
for example check if a file is available

00:26:22,520 --> 00:26:29,120
or if it's not there we can make HTTP

00:26:26,180 --> 00:26:33,530
checks into inside the container so even

00:26:29,120 --> 00:26:36,050
when there's no when there's an internal

00:26:33,530 --> 00:26:38,500
port only it kubernetes goes into the

00:26:36,050 --> 00:26:42,050
container makes these checks for you and

00:26:38,500 --> 00:26:43,910
really basic TCP check to just see if

00:26:42,050 --> 00:26:50,180
the connection can is to be established

00:26:43,910 --> 00:26:54,200
to a certain port when we configure them

00:26:50,180 --> 00:26:55,580
in in the Ranger interface so we have

00:26:54,200 --> 00:26:58,220
the readiness check and the liveness

00:26:55,580 --> 00:27:00,530
check so we can either set exactly the

00:26:58,220 --> 00:27:03,740
same we can set different ones in this

00:27:00,530 --> 00:27:07,760
case it's just a simple HTTP request to

00:27:03,740 --> 00:27:13,010
slash get to get slash just the base

00:27:07,760 --> 00:27:17,630
path and we expect a response code of

00:27:13,010 --> 00:27:20,930
200 or 300 something so if we get 230 or

00:27:17,630 --> 00:27:24,320
300 response code you for Cuba natives

00:27:20,930 --> 00:27:29,600
that means the application is first of

00:27:24,320 --> 00:27:31,940
all ready and it's alive we also can

00:27:29,600 --> 00:27:36,950
check the same a with cube couple

00:27:31,940 --> 00:27:40,010
directly so if we connect with cube

00:27:36,950 --> 00:27:44,150
cuddle to our cluster and we go exactly

00:27:40,010 --> 00:27:47,750
or we describe the port we can see we

00:27:44,150 --> 00:27:50,600
have here the image nginx and we have

00:27:47,750 --> 00:27:53,570
our life nos and readiness checks which

00:27:50,600 --> 00:27:57,440
we can see in the cube cuddle output as

00:27:53,570 --> 00:28:01,070
well and of course we can also check it

00:27:57,440 --> 00:28:06,020
in API so we just use curl to access the

00:28:01,070 --> 00:28:08,540
HTTP API from Ranger and we see the

00:28:06,020 --> 00:28:10,790
readiness probe further down below there

00:28:08,540 --> 00:28:13,940
would be life in asprova swell and most

00:28:10,790 --> 00:28:15,800
importantly we have the state which

00:28:13,940 --> 00:28:16,430
gives us back the actual state of the

00:28:15,800 --> 00:28:19,550
container

00:28:16,430 --> 00:28:23,240
so if post readiness probe and lifeless

00:28:19,550 --> 00:28:30,110
probe succeed then we have a running

00:28:23,240 --> 00:28:31,910
state so check Ranger 2 is a new

00:28:30,110 --> 00:28:36,110
monitoring plug-in which uses

00:28:31,910 --> 00:28:38,810
the API of Ranger it can as it's just HP

00:28:36,110 --> 00:28:42,110
or htps success it can basically run

00:28:38,810 --> 00:28:45,500
anywhere and it currently checked status

00:28:42,110 --> 00:28:48,170
of the cluster of all projects defining

00:28:45,500 --> 00:28:50,090
it of workloads and of ports so

00:28:48,170 --> 00:28:56,510
basically containers in the queue burn

00:28:50,090 --> 00:28:59,690
edges world and what I would suggest is

00:28:56,510 --> 00:29:02,870
that you add in your monitoring system

00:28:59,690 --> 00:29:07,010
that you add the API endpoint as a host

00:29:02,870 --> 00:29:10,300
and all the service checks like ports or

00:29:07,010 --> 00:29:14,510
clusters for example or workloads as

00:29:10,300 --> 00:29:17,690
services under that host object in the

00:29:14,510 --> 00:29:22,660
future or I at least I hope so

00:29:17,690 --> 00:29:26,390
that I or that we are able to add

00:29:22,660 --> 00:29:29,480
statistics into the to check monitoring

00:29:26,390 --> 00:29:32,090
plug-in or check range to plug in it

00:29:29,480 --> 00:29:35,810
really depends on on an open issue right

00:29:32,090 --> 00:29:40,570
now that the API is actually presenting

00:29:35,810 --> 00:29:40,570
these statistics or resource statistics

00:29:41,440 --> 00:29:53,060
so a live example of it was that

00:29:45,970 --> 00:29:55,490
suddenly we saw that the single single

00:29:53,060 --> 00:30:00,320
port was suddenly in the state removing

00:29:55,490 --> 00:30:03,520
so we saw already that the workload or

00:30:00,320 --> 00:30:05,960
the service importer was already

00:30:03,520 --> 00:30:09,680
redeployed but we still have an old

00:30:05,960 --> 00:30:13,130
importer pot hanging in the state

00:30:09,680 --> 00:30:15,920
removing so of course when something

00:30:13,130 --> 00:30:22,150
like this happens the our monitoring

00:30:15,920 --> 00:30:25,250
needs to alert me of that so I'm not

00:30:22,150 --> 00:30:28,130
explaining all the parameters here but

00:30:25,250 --> 00:30:31,820
most importantly we have the monitoring

00:30:28,130 --> 00:30:34,040
plug-in which alerts me about oh we have

00:30:31,820 --> 00:30:37,910
you have your environment is critical

00:30:34,040 --> 00:30:41,330
you have a thought importer and the

00:30:37,910 --> 00:30:43,040
unique ID of this of this pot which is

00:30:41,330 --> 00:30:43,909
in state removing and because it was

00:30:43,040 --> 00:30:46,819
still hanging

00:30:43,909 --> 00:30:48,739
we had this alert for a couple or yeah a

00:30:46,819 --> 00:30:54,319
couple of hours and of course we got

00:30:48,739 --> 00:30:57,919
alerted it turned actually out to do

00:30:54,319 --> 00:31:02,029
really that it was a pot really frozen

00:30:57,919 --> 00:31:07,849
and we need to use cube cuddle to to

00:31:02,029 --> 00:31:12,159
unfreeze it and to delete it manually so

00:31:07,849 --> 00:31:16,419
as a recap of application containers

00:31:12,159 --> 00:31:20,059
we're not talking talker anymore or

00:31:16,419 --> 00:31:24,169
mostly not anymore these days we really

00:31:20,059 --> 00:31:26,919
have container D or we also have run C

00:31:24,169 --> 00:31:32,109
and Kutta that's pretty new kata as

00:31:26,919 --> 00:31:35,499
container engines and we should not or

00:31:32,109 --> 00:31:37,999
we should consider that an application

00:31:35,499 --> 00:31:41,119
container is not a classical host

00:31:37,999 --> 00:31:43,159
anymore it's really just a process and

00:31:41,119 --> 00:31:46,549
that's how we have to handle it in

00:31:43,159 --> 00:31:48,979
monitoring as well to facilitate your

00:31:46,549 --> 00:31:51,319
work with application containers you

00:31:48,979 --> 00:31:54,829
should use orchestration and management

00:31:51,319 --> 00:31:57,409
tools kubernetes OpenShift Ranger etc to

00:31:54,829 --> 00:32:01,339
really help automate your your

00:31:57,409 --> 00:32:05,929
containers and your environments and the

00:32:01,339 --> 00:32:07,579
most important point here is to set up

00:32:05,929 --> 00:32:10,069
health checks because health checks

00:32:07,579 --> 00:32:11,779
that's really the only way to monitor

00:32:10,069 --> 00:32:15,729
the health of your containers and

00:32:11,779 --> 00:32:19,969
therefore your container environment

00:32:15,729 --> 00:32:22,190
interestingly there is also I saw there

00:32:19,969 --> 00:32:24,469
are some plugins out there which use

00:32:22,190 --> 00:32:26,419
cube cuddled directly so the command

00:32:24,469 --> 00:32:29,269
cube cutter which connects directly to

00:32:26,419 --> 00:32:30,799
the Cuban it is cluster and it's

00:32:29,269 --> 00:32:34,219
delivers the shot

00:32:30,799 --> 00:32:36,829
in our case we're focusing on the Ranger

00:32:34,219 --> 00:32:43,190
management so we're using check ranger

00:32:36,829 --> 00:32:45,739
to pull the API stuff some references

00:32:43,190 --> 00:32:49,369
and links this is mainly for that when

00:32:45,739 --> 00:32:55,889
you download the presentation afterwards

00:32:49,369 --> 00:32:59,190
and yeah time for questions

00:32:55,889 --> 00:33:01,629
and of course I'm also still available

00:32:59,190 --> 00:33:11,079
this evening if you want to have some

00:33:01,629 --> 00:33:12,190
talk about containers afterwards all

00:33:11,079 --> 00:33:17,679
right thank you very much

00:33:12,190 --> 00:33:20,369
so any questions everyone still tired

00:33:17,679 --> 00:33:20,369
from lunch and evening

00:33:21,239 --> 00:33:27,129
all right when there's no questions then

00:33:23,829 --> 00:33:31,779
thank you very much and yes everyone

00:33:27,129 --> 00:33:34,450
else yeah thank you for that you speak

00:33:31,779 --> 00:33:39,390
up for the awesome Zetas yeah thank you

00:33:34,450 --> 00:33:49,259
[Applause]

00:33:39,390 --> 00:33:49,259

YouTube URL: https://www.youtube.com/watch?v=B75r-EPSWCA


