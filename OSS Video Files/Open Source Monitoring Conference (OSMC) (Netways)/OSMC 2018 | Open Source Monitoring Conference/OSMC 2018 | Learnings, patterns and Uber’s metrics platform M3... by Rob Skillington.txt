Title: OSMC 2018 | Learnings, patterns and Uber’s metrics platform M3... by Rob Skillington
Publication date: 2018-11-17
Playlist: OSMC 2018 | Open Source Monitoring Conference
Description: 
	At Uber we use high cardinality monitoring to observe and detect issues with our 4,000 microservices running on Mesos and across our infrastructure systems and servers.  We’ll cover how we put the resulting 6 billion plus time series to work in a variety of different ways, auto-discovering services and their usage of other systems at Uber, setting up and tearing down alerts automatically for services, sending smart alert notifications that rollup different failures into individual high level contextual alerts, and more.  We’ll also talk about how we accomplish all this with a global view of our systems with M3, our open source metrics platform.  We’ll take a deep dive look at how we use M3DB, now available as an open source Prometheus long term storage backend, to horizontally scale our metrics platform in a cost efficient manner with a system that’s still sane to operate with petabytes of metrics data.

NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de/
Blog: http://blog.netways.de/
NWS: https://nws.netways.de

Webinare
Archiv Link: https://www.netways.de/webinare/archi...
Aktuell: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh/

www.musicfox.com
Captions: 
	00:00:01,940 --> 00:00:15,139
[Music]

00:00:12,310 --> 00:00:19,199
[Applause]

00:00:15,139 --> 00:00:19,800
hi thanks Philippi so yeah I'm gonna

00:00:19,199 --> 00:00:21,619
talk

00:00:19,800 --> 00:00:25,680
I changed the title of the talk so

00:00:21,619 --> 00:00:28,470
apologies I mean I think I'm basically

00:00:25,680 --> 00:00:31,199
going to talk about kind of what I what

00:00:28,470 --> 00:00:34,079
I mentioned before and also a little bit

00:00:31,199 --> 00:00:37,770
of a meta point I guess just on metrics

00:00:34,079 --> 00:00:41,190
and monitoring in general so yeah why am

00:00:37,770 --> 00:00:43,500
I here I've been working on running

00:00:41,190 --> 00:00:47,370
computers matter uber for three years

00:00:43,500 --> 00:00:51,600
now I you know monitoring's not a cool

00:00:47,370 --> 00:00:53,430
term anymore observability is said and I

00:00:51,600 --> 00:00:54,930
just kind of I think running computers

00:00:53,430 --> 00:00:59,190
seems to be more simple so that's what I

00:00:54,930 --> 00:01:03,480
say these days I think you know I wrote

00:00:59,190 --> 00:01:05,909
a whole bunch of code within like

00:01:03,480 --> 00:01:08,729
writing databases so that I wouldn't get

00:01:05,909 --> 00:01:12,450
paged at night I really don't recommend

00:01:08,729 --> 00:01:14,880
doing that so I kind of wanted to talk

00:01:12,450 --> 00:01:17,040
about how you know I think reusable

00:01:14,880 --> 00:01:20,520
monitoring software where that's going

00:01:17,040 --> 00:01:22,770
is a really great idea because I I think

00:01:20,520 --> 00:01:26,520
like the from all the code that we

00:01:22,770 --> 00:01:28,460
worked on you know to increase our level

00:01:26,520 --> 00:01:31,979
of like alerting and monitoring it ruber

00:01:28,460 --> 00:01:34,350
there's really only about 20% of it that

00:01:31,979 --> 00:01:36,210
that's actually special source about

00:01:34,350 --> 00:01:39,509
your application you know and all that

00:01:36,210 --> 00:01:40,920
sits close to your application the more

00:01:39,509 --> 00:01:44,430
integrations you can do with your stack

00:01:40,920 --> 00:01:47,610
with like your scheduler maybe using a

00:01:44,430 --> 00:01:50,600
container contain a runtime that's not

00:01:47,610 --> 00:01:53,340
kubernetes or for instance we use mezzos

00:01:50,600 --> 00:01:55,680
but also tinkering with kubernetes as

00:01:53,340 --> 00:01:57,270
well I think like that's where you know

00:01:55,680 --> 00:02:00,060
you get a lot of bang for your buck on

00:01:57,270 --> 00:02:02,490
engineer like software that you write to

00:02:00,060 --> 00:02:04,350
get metrics out of your stack and I've

00:02:02,490 --> 00:02:07,079
yeah and so I guess my recommendation

00:02:04,350 --> 00:02:09,300
here is obviously not working on

00:02:07,079 --> 00:02:11,340
observability tools is is a better way

00:02:09,300 --> 00:02:13,800
to spend your time and more just

00:02:11,340 --> 00:02:16,170
plugging into observability too

00:02:13,800 --> 00:02:19,170
so I did want to talk a little bit about

00:02:16,170 --> 00:02:23,489
the meta point Michael Medina yesterday

00:02:19,170 --> 00:02:27,060
had a great talk he kind of said like

00:02:23,489 --> 00:02:28,620
why'd we go from a place over here to

00:02:27,060 --> 00:02:31,200
another place over there and everything

00:02:28,620 --> 00:02:32,790
kind of actually really is similar even

00:02:31,200 --> 00:02:34,799
though it's all different

00:02:32,790 --> 00:02:38,879
you know I saw I don't really want to

00:02:34,799 --> 00:02:41,190
talk too much about Nagios but you know

00:02:38,879 --> 00:02:42,690
this is kind of obviously what what we

00:02:41,190 --> 00:02:44,459
looked at like it's a you know an

00:02:42,690 --> 00:02:48,930
aggregation of a whole bunch of checks

00:02:44,459 --> 00:02:50,519
these checks get run frequently and you

00:02:48,930 --> 00:02:52,590
know that the Nagios interface is great

00:02:50,519 --> 00:02:55,709
because allows you to drill down into

00:02:52,590 --> 00:02:57,120
single execution whereas like I think

00:02:55,709 --> 00:03:01,079
it's a little bit different to metrics

00:02:57,120 --> 00:03:03,690
which are really pretty broad you get

00:03:01,079 --> 00:03:07,200
some really great 10,000 foot views but

00:03:03,690 --> 00:03:09,690
you don't really get why is this like

00:03:07,200 --> 00:03:13,349
why did I get a 400 bad request or I

00:03:09,690 --> 00:03:15,090
until server era use no easy way to go

00:03:13,349 --> 00:03:16,829
from a graph straight to that so

00:03:15,090 --> 00:03:18,269
actually now give us actually pretty

00:03:16,829 --> 00:03:21,470
powerful in that way that you can get

00:03:18,269 --> 00:03:24,600
directly to information about a failure

00:03:21,470 --> 00:03:27,720
however there's some some knifing nice

00:03:24,600 --> 00:03:30,569
things in this world of metrics for

00:03:27,720 --> 00:03:32,880
instance you know David kind of talked a

00:03:30,569 --> 00:03:35,760
bit about Griffin and where that's going

00:03:32,880 --> 00:03:37,829
the being able to dynamically adjust

00:03:35,760 --> 00:03:40,349
your threshold on certain alerts and

00:03:37,829 --> 00:03:42,030
then also to aggregate kind of the view

00:03:40,349 --> 00:03:45,239
that you're looking at like if we look

00:03:42,030 --> 00:03:46,829
back at Nagios you basically you know

00:03:45,239 --> 00:03:49,380
the the most naive way to do this you

00:03:46,829 --> 00:03:51,269
send it alert for every failure whereas

00:03:49,380 --> 00:03:53,160
with metrics you can do some more

00:03:51,269 --> 00:03:58,049
interesting meta things and say like

00:03:53,160 --> 00:04:01,470
okay over time if there are ten

00:03:58,049 --> 00:04:04,500
individual servers that have more than

00:04:01,470 --> 00:04:05,730
one error per second then alert me and

00:04:04,500 --> 00:04:09,290
that's what this query is doing down

00:04:05,730 --> 00:04:13,579
here it's saying basically if you know

00:04:09,290 --> 00:04:17,700
if the sum of a individual time series

00:04:13,579 --> 00:04:20,909
is greater than one count the unique

00:04:17,700 --> 00:04:23,580
amounts of these per window and then

00:04:20,909 --> 00:04:25,710
then if you send an alert on ten you

00:04:23,580 --> 00:04:26,340
basically mean please only page me when

00:04:25,710 --> 00:04:28,470
ten

00:04:26,340 --> 00:04:31,410
instances are unhealthy which i think is

00:04:28,470 --> 00:04:33,389
you know it's it's it's a it's good you

00:04:31,410 --> 00:04:35,510
can meta program how you want to get

00:04:33,389 --> 00:04:38,400
alerted and I think that that is

00:04:35,510 --> 00:04:40,199
something that is good and and

00:04:38,400 --> 00:04:44,910
definitely an improvement over say

00:04:40,199 --> 00:04:47,580
Nagios so you know how how can we kind

00:04:44,910 --> 00:04:50,010
of get back though what we we took away

00:04:47,580 --> 00:04:52,410
with Nagios there's things like google

00:04:50,010 --> 00:04:54,210
proba and it does similar things it'll

00:04:52,410 --> 00:04:57,060
will actually let you it integrates with

00:04:54,210 --> 00:04:59,070
prometheus and graph owner so it turns

00:04:57,060 --> 00:05:02,960
checks into metrics but it also lets you

00:04:59,070 --> 00:05:07,770
access the details of of a check output

00:05:02,960 --> 00:05:09,960
and and it supports non-http as well the

00:05:07,770 --> 00:05:12,180
the only thing i'd kind of the caveat

00:05:09,960 --> 00:05:14,070
there is that if you can put an HTTP

00:05:12,180 --> 00:05:16,590
service in front of your check that runs

00:05:14,070 --> 00:05:18,960
a check for you and reply is like okay

00:05:16,590 --> 00:05:20,880
or not okay it's kind of nice because

00:05:18,960 --> 00:05:23,460
then you don't have to ssh on to a shane

00:05:20,880 --> 00:05:26,970
to like manually run a check you just

00:05:23,460 --> 00:05:29,130
hit that machines hit that checks HTTP

00:05:26,970 --> 00:05:32,310
endpoint or application that you're

00:05:29,130 --> 00:05:33,479
running so you know i think where are

00:05:32,310 --> 00:05:35,520
the things about where we're going i

00:05:33,479 --> 00:05:38,639
think like you know adding more

00:05:35,520 --> 00:05:41,729
retention and more dimensions is going

00:05:38,639 --> 00:05:44,580
to be a trend like looking back thirty

00:05:41,729 --> 00:05:45,630
days six months one year that's that's

00:05:44,580 --> 00:05:48,090
definitely going to be something that's

00:05:45,630 --> 00:05:49,590
going to become more commonplace and and

00:05:48,090 --> 00:05:53,070
it's helpful when you want to look at

00:05:49,590 --> 00:05:54,720
trends and also adding much higher

00:05:53,070 --> 00:05:57,840
cardinality on things like if you want

00:05:54,720 --> 00:06:00,060
to do you know the p99 of different

00:05:57,840 --> 00:06:03,419
routes broken down by region by endpoint

00:06:00,060 --> 00:06:05,910
by users segments for instance like the

00:06:03,419 --> 00:06:09,389
type of device they're using to to col

00:06:05,910 --> 00:06:12,060
your HTTP server the operating system

00:06:09,389 --> 00:06:13,979
the operating system version the region

00:06:12,060 --> 00:06:14,910
that they're in that all adds up pretty

00:06:13,979 --> 00:06:18,330
quickly when you're talking about

00:06:14,910 --> 00:06:19,950
carnality and so you know i think also a

00:06:18,330 --> 00:06:22,530
plug-and-play is going to be become more

00:06:19,950 --> 00:06:25,050
and more prevalent and you won't have to

00:06:22,530 --> 00:06:27,660
invent the wheel each time when you want

00:06:25,050 --> 00:06:29,610
to monitor the types of infrastructure

00:06:27,660 --> 00:06:33,450
that you're monitoring and then I think

00:06:29,610 --> 00:06:36,150
also merging graphs and logs making them

00:06:33,450 --> 00:06:38,460
easy to bounce between the two some of

00:06:36,150 --> 00:06:39,289
David's demo of where logging is going

00:06:38,460 --> 00:06:41,139
Ingrid

00:06:39,289 --> 00:06:45,710
I think we'll start to help bridge this

00:06:41,139 --> 00:06:47,119
and I think this is the like what's

00:06:45,710 --> 00:06:49,520
interesting is that I actually would

00:06:47,119 --> 00:06:53,389
love the code to merge metrics and logs

00:06:49,520 --> 00:06:56,809
together I don't see there's like really

00:06:53,389 --> 00:07:00,379
any reason why tags and and metric event

00:06:56,809 --> 00:07:03,619
names can't be used and logs the thing

00:07:00,379 --> 00:07:05,659
is is that you see here with the bug

00:07:03,619 --> 00:07:09,050
detail like you don't really want to

00:07:05,659 --> 00:07:11,990
index this they're really gnarly like

00:07:09,050 --> 00:07:13,909
full text stuff but to actually get to

00:07:11,990 --> 00:07:16,099
this type of event with these tags and

00:07:13,909 --> 00:07:17,779
show you some of the areas you were

00:07:16,099 --> 00:07:18,409
experiencing when you go to 400 bad

00:07:17,779 --> 00:07:22,069
request

00:07:18,409 --> 00:07:23,389
I think that's powerful and and so I

00:07:22,069 --> 00:07:24,800
think like we basically need deeper

00:07:23,389 --> 00:07:25,999
deeper integration and the deeper

00:07:24,800 --> 00:07:30,529
integration needs to come all the way

00:07:25,999 --> 00:07:32,749
back to to the metric submission point

00:07:30,529 --> 00:07:35,209
sorry to the login metric emission point

00:07:32,749 --> 00:07:38,449
but new libraries today do anything like

00:07:35,209 --> 00:07:40,459
this and for honestly for now metrics

00:07:38,449 --> 00:07:42,939
and logs are so difficult to deal with

00:07:40,459 --> 00:07:46,430
anyway so maybe this isn't a good idea

00:07:42,939 --> 00:07:47,689
and we'll see but I do think you know

00:07:46,430 --> 00:07:48,979
the other thing as well as that will

00:07:47,689 --> 00:07:53,120
definitely start to see like order

00:07:48,979 --> 00:07:55,729
discovery of your metrics and logs you

00:07:53,120 --> 00:07:56,990
know I think things like console and etc

00:07:55,729 --> 00:07:59,330
D and there's a whole bunch of like

00:07:56,990 --> 00:08:00,889
service discovery for like working out

00:07:59,330 --> 00:08:03,379
what you're running where you're running

00:08:00,889 --> 00:08:04,789
it but there's not as much going on

00:08:03,379 --> 00:08:06,889
there in the metric space you kind of

00:08:04,789 --> 00:08:09,439
get this like flat view of metrics

00:08:06,889 --> 00:08:11,269
across everything that you're monitoring

00:08:09,439 --> 00:08:13,219
and you kind of need to order complete

00:08:11,269 --> 00:08:14,959
the the metric name and then kind of

00:08:13,219 --> 00:08:17,839
like just keep toggling through

00:08:14,959 --> 00:08:20,300
different label values you know and some

00:08:17,839 --> 00:08:21,889
of the Explorer UI work that's going on

00:08:20,300 --> 00:08:24,229
and grifone I think will be good here

00:08:21,889 --> 00:08:26,689
but I also think that honestly the best

00:08:24,229 --> 00:08:29,409
stuff is to just show you a dashboard of

00:08:26,689 --> 00:08:32,689
what you're running with some pre types

00:08:29,409 --> 00:08:35,750
meant some pre type graphs and logs

00:08:32,689 --> 00:08:37,430
based on what you know what is actually

00:08:35,750 --> 00:08:40,339
in your metrics database what is in your

00:08:37,430 --> 00:08:43,600
log database so here you can see like

00:08:40,339 --> 00:08:46,850
there's a kind of a demo of like a

00:08:43,600 --> 00:08:48,709
application at uber which if you deploy

00:08:46,850 --> 00:08:50,630
your service it kind of figures out all

00:08:48,709 --> 00:08:51,950
your running on mezzos so I'll put some

00:08:50,630 --> 00:08:55,340
I know

00:08:51,950 --> 00:08:57,500
graphs about mezzos are important and

00:08:55,340 --> 00:08:59,300
then it also knows all I'm using

00:08:57,500 --> 00:09:01,100
Cassandra so you can see down here that

00:08:59,300 --> 00:09:02,960
it's picked up automatically that an

00:09:01,100 --> 00:09:05,360
application is using Cassandra and you

00:09:02,960 --> 00:09:08,720
can kind of dig dig deep into any of

00:09:05,360 --> 00:09:11,420
these use case into any of these metrics

00:09:08,720 --> 00:09:12,980
and logs without having to have done any

00:09:11,420 --> 00:09:14,960
work setting up any grass or anything

00:09:12,980 --> 00:09:17,990
like that yourself so I think that'll

00:09:14,960 --> 00:09:19,850
that'll probably I don't probably be

00:09:17,990 --> 00:09:22,490
more deep integrations like that coming

00:09:19,850 --> 00:09:23,780
along so now I'm going to speed through

00:09:22,490 --> 00:09:28,310
some content because I've got way too

00:09:23,780 --> 00:09:29,660
many slides so I apologize and I

00:09:28,310 --> 00:09:31,250
hopefully we can do some questions at

00:09:29,660 --> 00:09:33,950
the end so yeah what is high

00:09:31,250 --> 00:09:35,990
dimensionality yeah as I kind of

00:09:33,950 --> 00:09:38,120
mentioned like if you want to you know

00:09:35,990 --> 00:09:41,060
really add more and more dimensions to

00:09:38,120 --> 00:09:46,550
even simple things like an HTTP status

00:09:41,060 --> 00:09:48,080
code response you can the the the amount

00:09:46,550 --> 00:09:51,740
of unique time series start adding up

00:09:48,080 --> 00:09:54,230
very quickly so you know if we say 500

00:09:51,740 --> 00:09:58,010
different URLs that we want to monitor

00:09:54,230 --> 00:10:00,560
five status codes five regions 20 client

00:09:58,010 --> 00:10:03,080
versions say you know that's 250,000

00:10:00,560 --> 00:10:05,260
unique time series that's not that's

00:10:03,080 --> 00:10:08,750
expensive but it's not too bad

00:10:05,260 --> 00:10:11,180
Prometheus is great at this and you

00:10:08,750 --> 00:10:12,620
probably get away with a whole bunch of

00:10:11,180 --> 00:10:15,620
these but if you start having other

00:10:12,620 --> 00:10:18,530
dimensions or you just start monitoring

00:10:15,620 --> 00:10:21,320
a whole ton of more distinct things you

00:10:18,530 --> 00:10:23,660
know blaze up pretty quickly so why use

00:10:21,320 --> 00:10:27,230
them if they're so and like expensive

00:10:23,660 --> 00:10:29,330
and annoying I think there's you know

00:10:27,230 --> 00:10:31,100
it's it's definitely that the world is

00:10:29,330 --> 00:10:34,430
getting more complex there's more

00:10:31,100 --> 00:10:37,040
ephemerality container ID you IDs is

00:10:34,430 --> 00:10:39,850
become like it's a thing so I think

00:10:37,040 --> 00:10:43,280
that's it's not going away anytime soon

00:10:39,850 --> 00:10:44,990
and if we can kind of build technology

00:10:43,280 --> 00:10:48,800
hopefully to simplify at least how you

00:10:44,990 --> 00:10:50,750
use and access it then then it's it's

00:10:48,800 --> 00:10:53,270
fine and why would you kind of really do

00:10:50,750 --> 00:10:55,730
use metrics at all right I think like

00:10:53,270 --> 00:10:57,320
one of the points here is that obviously

00:10:55,730 --> 00:10:59,330
if you have a say it's it's an aggregate

00:10:57,320 --> 00:11:03,350
all essentially on what our events and

00:10:59,330 --> 00:11:04,940
logs so it it's good really great for

00:11:03,350 --> 00:11:07,510
storing a whole bunch of in

00:11:04,940 --> 00:11:12,520
that doesn't take up as much data as

00:11:07,510 --> 00:11:17,000
logs does so you know how do you do this

00:11:12,520 --> 00:11:20,060
I guess like your Prometheus is a pretty

00:11:17,000 --> 00:11:23,860
much mostly what you need most come

00:11:20,060 --> 00:11:26,270
probably most installations ever need

00:11:23,860 --> 00:11:28,160
anything bigger than a single Prometheus

00:11:26,270 --> 00:11:30,470
but once you do need something more than

00:11:28,160 --> 00:11:32,810
a single Prometheus then then it starts

00:11:30,470 --> 00:11:34,070
to get a little bit more interesting in

00:11:32,810 --> 00:11:36,290
terms of like what do you do about that

00:11:34,070 --> 00:11:39,200
how do you manage multiple Prometheus

00:11:36,290 --> 00:11:41,540
instances and then you know even going

00:11:39,200 --> 00:11:44,060
back to that example of 250,000 unique

00:11:41,540 --> 00:11:47,600
time series for a single metric or

00:11:44,060 --> 00:11:50,480
metrics family you hit this point where

00:11:47,600 --> 00:11:53,870
like okay which which target is gonna

00:11:50,480 --> 00:11:56,600
scrape that and which which targets am I

00:11:53,870 --> 00:12:00,800
going to fit into which which Prometheus

00:11:56,600 --> 00:12:02,690
instance so you know can I like do I add

00:12:00,800 --> 00:12:05,090
this really high carnelli use case to an

00:12:02,690 --> 00:12:07,910
existing Prometheus do I set up a new

00:12:05,090 --> 00:12:09,740
one what happens when it eclipses like a

00:12:07,910 --> 00:12:13,040
single Prometheus which which is

00:12:09,740 --> 00:12:17,530
unlikely to be honest again you know

00:12:13,040 --> 00:12:20,450
Prometheus cannot do millions of metrics

00:12:17,530 --> 00:12:23,120
but as you saw it's it's time to get

00:12:20,450 --> 00:12:24,920
even more hard cardinality so kind of

00:12:23,120 --> 00:12:27,830
just to talk a little bit about ubers

00:12:24,920 --> 00:12:31,550
use case and and how we arrived at where

00:12:27,830 --> 00:12:36,440
we are we have 4000 micro services which

00:12:31,550 --> 00:12:40,700
is horrifying to me keeps everyone up at

00:12:36,440 --> 00:12:42,410
night so yeah we do have we try to

00:12:40,700 --> 00:12:44,930
centralize a lot of the things that

00:12:42,410 --> 00:12:47,420
these micro services use because doing

00:12:44,930 --> 00:12:50,720
it on an ad-hoc basis per service is

00:12:47,420 --> 00:12:53,480
obviously very very difficult and you

00:12:50,720 --> 00:12:56,420
start to get a lot of snowflakes so the

00:12:53,480 --> 00:12:59,030
metrics platform is is zero onboarding I

00:12:56,420 --> 00:13:02,870
think that that's probably why it's bein

00:12:59,030 --> 00:13:05,390
used so much ed uber because yeah you

00:13:02,870 --> 00:13:09,530
you can start getting some some data

00:13:05,390 --> 00:13:11,000
straight away logging is is like is how

00:13:09,530 --> 00:13:12,560
do we basically have different instances

00:13:11,000 --> 00:13:15,890
of logging but you can't get an elastic

00:13:12,560 --> 00:13:18,290
search or ELQ instance straightaway

00:13:15,890 --> 00:13:20,209
because it's it's relatively expensive

00:13:18,290 --> 00:13:22,940
to set up a cluster for every single use

00:13:20,209 --> 00:13:24,079
case for elk so but with metric since

00:13:22,940 --> 00:13:26,329
it's a single platform and it's

00:13:24,079 --> 00:13:29,329
multi-tenant onboarding is really simple

00:13:26,329 --> 00:13:31,730
you just not emitting metrics and yeah

00:13:29,329 --> 00:13:34,610
we kind of take the brunt of that for

00:13:31,730 --> 00:13:37,130
every one so which is great because you

00:13:34,610 --> 00:13:38,660
know it's so easy to get started like

00:13:37,130 --> 00:13:40,910
for instance we you know a lot of teams

00:13:38,660 --> 00:13:43,550
use it for tracking business metrics not

00:13:40,910 --> 00:13:47,149
in a way that the rise you wouldn't

00:13:43,550 --> 00:13:49,490
actually use it for accounting purposes

00:13:47,149 --> 00:13:51,290
or anything like that obviously but it's

00:13:49,490 --> 00:13:53,269
a great way to just get started on on

00:13:51,290 --> 00:13:55,550
getting some insight into how your

00:13:53,269 --> 00:13:58,040
application is working for instance you

00:13:55,550 --> 00:14:00,800
have a example here of how simple it

00:13:58,040 --> 00:14:03,230
would be to just start counting rides in

00:14:00,800 --> 00:14:06,380
a certain region it's it's like a

00:14:03,230 --> 00:14:10,850
one-line one-liner and we do like

00:14:06,380 --> 00:14:13,250
capacity planning and system and

00:14:10,850 --> 00:14:15,680
application like Hardware metrics on

00:14:13,250 --> 00:14:19,339
this as well like networking power data

00:14:15,680 --> 00:14:21,380
center stuff and then we also do like

00:14:19,339 --> 00:14:22,760
anomaly detection on on some of this

00:14:21,380 --> 00:14:24,500
stuff which I would love to get open

00:14:22,760 --> 00:14:27,260
source but it's you know Mele detection

00:14:24,500 --> 00:14:28,579
stuff isn't yet you know what does it

00:14:27,260 --> 00:14:30,800
look like you build your services uber

00:14:28,579 --> 00:14:33,079
you use a whole bunch of reusable

00:14:30,800 --> 00:14:36,319
libraries for RPC for storage like

00:14:33,079 --> 00:14:38,300
Cassandra Redis whatever and then you

00:14:36,319 --> 00:14:39,500
basically provide your service name to

00:14:38,300 --> 00:14:41,630
every one of these libraries that you

00:14:39,500 --> 00:14:44,810
use it's like a requirement when you

00:14:41,630 --> 00:14:47,120
open it up and you know a lot of like

00:14:44,810 --> 00:14:49,760
the the Redis Cassandra use case we just

00:14:47,120 --> 00:14:52,610
have a thin wrapper so basically like at

00:14:49,760 --> 00:14:55,399
least to construct the Cassandra or the

00:14:52,610 --> 00:14:58,510
Redis handler you have to pass in some

00:14:55,399 --> 00:15:01,310
standard parameters to to an uber

00:14:58,510 --> 00:15:03,230
constructor library and this is great

00:15:01,310 --> 00:15:06,470
because basically it means your

00:15:03,230 --> 00:15:08,120
dashboards will look the same for every

00:15:06,470 --> 00:15:11,329
single service and they just pivot on

00:15:08,120 --> 00:15:13,310
the service name tag and so this is kind

00:15:11,329 --> 00:15:16,790
of like a further example of you know

00:15:13,310 --> 00:15:18,639
what this is kind of like what a

00:15:16,790 --> 00:15:21,170
discovery world might look like

00:15:18,639 --> 00:15:23,540
hopefully actually one of the teams the

00:15:21,170 --> 00:15:26,569
Derby's hope is hoping to kind of push

00:15:23,540 --> 00:15:30,649
and help work on some of this stuff and

00:15:26,569 --> 00:15:31,329
grow finer so hopefully you know becomes

00:15:30,649 --> 00:15:35,019
a little bit more

00:15:31,329 --> 00:15:36,129
easy to use for out-of-the-box and you

00:15:35,019 --> 00:15:38,350
don't have to build your own proprietary

00:15:36,129 --> 00:15:39,970
application well so we want to actually

00:15:38,350 --> 00:15:41,949
just get away a hundred percent from

00:15:39,970 --> 00:15:45,040
from proprietary applications it doesn't

00:15:41,949 --> 00:15:46,179
doesn't really make sense for for us to

00:15:45,040 --> 00:15:48,249
work on a whole bunch of proprietary

00:15:46,179 --> 00:15:51,220
stuff when we're not a cloud vendor and

00:15:48,249 --> 00:15:55,389
we're not we don't sell any of our

00:15:51,220 --> 00:15:58,360
technology so this is what I think like

00:15:55,389 --> 00:16:00,339
is interesting here though is looking at

00:15:58,360 --> 00:16:02,110
related dashboards is something new is

00:16:00,339 --> 00:16:05,170
kind of like combining tracing with

00:16:02,110 --> 00:16:07,089
metrics and then these you know you can

00:16:05,170 --> 00:16:08,769
see on the right side here these little

00:16:07,089 --> 00:16:11,249
green and yellow dots basically as

00:16:08,769 --> 00:16:13,720
saying like an upstream or a downstream

00:16:11,249 --> 00:16:15,459
which which is either healthy or

00:16:13,720 --> 00:16:18,009
unhealthy based on whether it's alerting

00:16:15,459 --> 00:16:19,269
or not and then obviously it could get a

00:16:18,009 --> 00:16:21,819
little more interesting in the future I

00:16:19,269 --> 00:16:24,699
think you know you could infer a whole

00:16:21,819 --> 00:16:27,429
bunch of things based on combining like

00:16:24,699 --> 00:16:29,889
metrics and tracing data like this but

00:16:27,429 --> 00:16:31,869
anyway so what's so hot about all this I

00:16:29,889 --> 00:16:33,759
think that you know it's it's kind of

00:16:31,869 --> 00:16:36,309
similarly going back to the the Tetra

00:16:33,759 --> 00:16:38,980
C's case it's fine when you start small

00:16:36,309 --> 00:16:41,019
and then and then it's it's really just

00:16:38,980 --> 00:16:43,480
as you continue to add more with more

00:16:41,019 --> 00:16:45,549
things more people more teams that's

00:16:43,480 --> 00:16:47,559
when it gets a little bit more difficult

00:16:45,549 --> 00:16:51,610
we have something like you had a few

00:16:47,559 --> 00:16:54,509
hundred aggregated metrics per second we

00:16:51,610 --> 00:16:57,429
ride 30 million metrics per second to

00:16:54,509 --> 00:17:02,049
two storage that's around about 50 Giga

00:16:57,429 --> 00:17:06,909
bits of network traffic and that that's

00:17:02,049 --> 00:17:08,500
pre replication and then yeah our

00:17:06,909 --> 00:17:11,740
metrics index has something like 9

00:17:08,500 --> 00:17:13,360
billion metric IDs in it and this graph

00:17:11,740 --> 00:17:15,220
is weird it's trending down because

00:17:13,360 --> 00:17:17,889
metrics are actually falling out of

00:17:15,220 --> 00:17:21,039
retention here so a whole bunch of like

00:17:17,889 --> 00:17:22,809
ephemeral metric time series is use

00:17:21,039 --> 00:17:25,539
cases are appearing more often and

00:17:22,809 --> 00:17:27,909
people put these bursts of metrics into

00:17:25,539 --> 00:17:30,850
the system and then like that kind of

00:17:27,909 --> 00:17:32,769
filter out after a 30 day retention we

00:17:30,850 --> 00:17:36,279
also have multi year retention but

00:17:32,769 --> 00:17:38,590
that's opt-in so you have to explicitly

00:17:36,279 --> 00:17:41,200
say that I want to keep these types of

00:17:38,590 --> 00:17:44,110
metrics for longer and then the yet the

00:17:41,200 --> 00:17:45,310
the actual checking you know we do about

00:17:44,110 --> 00:17:49,210
20 gigabits per sec

00:17:45,310 --> 00:17:54,670
thirty billion data points for 150,000

00:17:49,210 --> 00:17:57,220
real-time alerts okay yes ym3 you know I

00:17:54,670 --> 00:17:59,410
think like we just really wanted to

00:17:57,220 --> 00:18:02,740
build on on the shoulder of giants like

00:17:59,410 --> 00:18:05,700
we want to build stuff that's an

00:18:02,740 --> 00:18:07,770
important to us but we don't rebuilds

00:18:05,700 --> 00:18:11,770
everything that's why we try to leverage

00:18:07,770 --> 00:18:14,260
Prometheus graph honor and a whole bunch

00:18:11,770 --> 00:18:16,750
of other technology but it was really

00:18:14,260 --> 00:18:21,000
important for us to to just be able to

00:18:16,750 --> 00:18:24,370
add capacity for this like no onboarding

00:18:21,000 --> 00:18:27,400
style and just emitting metrics from

00:18:24,370 --> 00:18:29,860
your application or your service you

00:18:27,400 --> 00:18:32,410
know we really be need to be able to

00:18:29,860 --> 00:18:34,810
point you at a single data source and

00:18:32,410 --> 00:18:37,330
then and then just kind of scale out

00:18:34,810 --> 00:18:40,240
that data source otherwise it just

00:18:37,330 --> 00:18:42,540
became too unmanageable with having yeah

00:18:40,240 --> 00:18:46,900
cluster per tenant or anything like that

00:18:42,540 --> 00:18:49,900
and the the management of of everything

00:18:46,900 --> 00:18:51,490
just gets really difficult so you know

00:18:49,900 --> 00:18:54,550
the other thing is the best operator of

00:18:51,490 --> 00:18:59,260
computers is computers I think that

00:18:54,550 --> 00:19:03,700
really a lot of what and m3 was and is

00:18:59,260 --> 00:19:06,340
now is basically us being really upset

00:19:03,700 --> 00:19:10,630
with how like hard it is to operate a

00:19:06,340 --> 00:19:12,160
metrics platform so yeah we a lot of

00:19:10,630 --> 00:19:13,510
work went into just making sure that the

00:19:12,160 --> 00:19:17,440
system for the most part looks after

00:19:13,510 --> 00:19:19,510
itself I don't think you know I think

00:19:17,440 --> 00:19:21,640
for the most part is is the important

00:19:19,510 --> 00:19:24,430
part there there's always work to be

00:19:21,640 --> 00:19:26,590
done in that and so we wanted at least a

00:19:24,430 --> 00:19:31,150
project and open source where people

00:19:26,590 --> 00:19:35,080
cared about kind of like zero tuning but

00:19:31,150 --> 00:19:36,670
like scalable metric store so we'd love

00:19:35,080 --> 00:19:38,920
you know I think like the community

00:19:36,670 --> 00:19:42,310
working on like a shared project like

00:19:38,920 --> 00:19:45,580
that will benefit more than just us so

00:19:42,310 --> 00:19:49,450
that's so yeah I mean just like a brief

00:19:45,580 --> 00:19:52,270
history in we used to run Cassandra as I

00:19:49,450 --> 00:19:54,010
backing time series data store we were

00:19:52,270 --> 00:19:56,890
able to push it up to like 80,000 rights

00:19:54,010 --> 00:19:59,210
per second it had pretty poor

00:19:56,890 --> 00:20:03,799
compression it was mainly like LZ

00:19:59,210 --> 00:20:06,110
for like string based compression you

00:20:03,799 --> 00:20:08,620
know it it used very expensive SSDs

00:20:06,110 --> 00:20:12,830
hungry on CPU we actually used like

00:20:08,620 --> 00:20:14,360
special 40 core boxes and you know we

00:20:12,830 --> 00:20:17,570
would have to put in special orders to

00:20:14,360 --> 00:20:21,980
get this type of hardware and it's kind

00:20:17,570 --> 00:20:25,340
of like going to the deep end on AWS or

00:20:21,980 --> 00:20:29,870
or G or Google Cloud and ordering like

00:20:25,340 --> 00:20:31,250
the most expensive VM and and yeah the

00:20:29,870 --> 00:20:32,270
major reason like we kind of had to do

00:20:31,250 --> 00:20:34,760
this was there was a lot of heavy

00:20:32,270 --> 00:20:40,490
compactions even using day time compare

00:20:34,760 --> 00:20:43,250
date tiered compaction strategy so yeah

00:20:40,490 --> 00:20:48,130
it was kind of a unsustainable to really

00:20:43,250 --> 00:20:48,130
run anything but replication factor to

00:20:48,370 --> 00:20:53,720
because of how expensive was and repairs

00:20:51,080 --> 00:20:55,370
were really difficult and I think like

00:20:53,720 --> 00:20:58,100
the major reason why you want more than

00:20:55,370 --> 00:21:01,399
a single replica of all this stuff is

00:20:58,100 --> 00:21:04,669
that you know it goes back to this thing

00:21:01,399 --> 00:21:07,360
of you don't really want to be managing

00:21:04,669 --> 00:21:07,360
everything yourself

00:21:07,520 --> 00:21:11,809
so with when you've only got a single

00:21:09,049 --> 00:21:14,270
replica of things and a like your say

00:21:11,809 --> 00:21:15,950
your Prometheus instance dies there's

00:21:14,270 --> 00:21:19,789
ways to run highly available Prometheus

00:21:15,950 --> 00:21:21,860
setups but there's no way to like

00:21:19,789 --> 00:21:24,080
actually transfer the data from one

00:21:21,860 --> 00:21:26,690
running Prometheus to an to another so

00:21:24,080 --> 00:21:29,120
it's it's kind of like it's just a lot

00:21:26,690 --> 00:21:31,520
nicer to have replication built into to

00:21:29,120 --> 00:21:33,470
these systems because you basically just

00:21:31,520 --> 00:21:36,140
let a machine die

00:21:33,470 --> 00:21:38,750
there's no alerts going off you just

00:21:36,140 --> 00:21:42,350
wait until you know you have the next

00:21:38,750 --> 00:21:44,690
opportunity to kind of replace it so it

00:21:42,350 --> 00:21:47,980
you can kind of run with one node down

00:21:44,690 --> 00:21:50,960
for quite a considerable amount of time

00:21:47,980 --> 00:21:53,510
so then three B yeah some like

00:21:50,960 --> 00:21:54,620
high-level things that are nice now at

00:21:53,510 --> 00:21:56,840
least for us is that

00:21:54,620 --> 00:21:58,429
yeah we're able to push some of these

00:21:56,840 --> 00:22:02,710
nodes up to like five hundred thousand

00:21:58,429 --> 00:22:05,270
rounds per second I I think like

00:22:02,710 --> 00:22:08,029
basically that because we're able to

00:22:05,270 --> 00:22:11,630
optimize for just a very specific

00:22:08,029 --> 00:22:12,460
instrumentation workload hopefully the

00:22:11,630 --> 00:22:14,860
community will help

00:22:12,460 --> 00:22:17,410
help help out on this as well as that we

00:22:14,860 --> 00:22:18,970
can just continue to push this towards a

00:22:17,410 --> 00:22:21,160
high-volume use case so it's it's

00:22:18,970 --> 00:22:26,650
actually as cheap as possible to run

00:22:21,160 --> 00:22:29,250
this type of type of deadlock database

00:22:26,650 --> 00:22:32,500
and and platform and stuff like that

00:22:29,250 --> 00:22:33,880
because yeah that the world is adding

00:22:32,500 --> 00:22:35,710
more and more metrics more and more

00:22:33,880 --> 00:22:38,340
dimensions and I didn't think that's

00:22:35,710 --> 00:22:42,190
going to go away anytime too soon

00:22:38,340 --> 00:22:43,810
so I although like putting limits on on

00:22:42,190 --> 00:22:47,680
what developers do I think is a great

00:22:43,810 --> 00:22:51,010
idea definitely we've definitely kind of

00:22:47,680 --> 00:22:53,770
felt that pain so this is yeah I guess

00:22:51,010 --> 00:22:56,680
like a river you know reduce the cost of

00:22:53,770 --> 00:23:00,400
running m32 about 2.5 percent of all

00:22:56,680 --> 00:23:02,800
hardware and compute at uber you know

00:23:00,400 --> 00:23:05,320
there's definitely other other companies

00:23:02,800 --> 00:23:06,790
I've seen where it's the the entire

00:23:05,320 --> 00:23:08,470
observability platform definitely eats

00:23:06,790 --> 00:23:11,920
up a significant amount more than that

00:23:08,470 --> 00:23:14,770
at some companies and we would kind of

00:23:11,920 --> 00:23:16,510
like continue one of the FS of the

00:23:14,770 --> 00:23:19,870
project just continue to drive down that

00:23:16,510 --> 00:23:23,860
number so that we can give end-users

00:23:19,870 --> 00:23:27,610
basically more for less and I think

00:23:23,860 --> 00:23:30,190
that's I think we need yet more more

00:23:27,610 --> 00:23:31,840
kind of more kind of infrastructure that

00:23:30,190 --> 00:23:34,720
kind of like has the same kind of

00:23:31,840 --> 00:23:36,760
opinionated thing where you shouldn't

00:23:34,720 --> 00:23:40,030
have to pay a whole lot of money just to

00:23:36,760 --> 00:23:45,310
get to get more and more out of your

00:23:40,030 --> 00:23:47,140
review system so you know what is it at

00:23:45,310 --> 00:23:50,890
a high-level architecture it's similar

00:23:47,140 --> 00:23:54,370
to a log structured most tree but with

00:23:50,890 --> 00:23:57,400
much less compaction so log structured

00:23:54,370 --> 00:23:59,380
merge tree is something essentially when

00:23:57,400 --> 00:24:03,610
you bake it down to you take a whole

00:23:59,380 --> 00:24:07,840
bunch of riots and then you ride it to

00:24:03,610 --> 00:24:10,180
this log or right ahead log on your on

00:24:07,840 --> 00:24:12,430
your database node and then you can

00:24:10,180 --> 00:24:14,920
actually keep all the data in sorted

00:24:12,430 --> 00:24:17,680
order in memory and then essentially

00:24:14,920 --> 00:24:21,340
when you hit like a certain boundary you

00:24:17,680 --> 00:24:25,930
flush all that data to disk in one like

00:24:21,340 --> 00:24:26,380
file volume so it's whereas like if you

00:24:25,930 --> 00:24:28,420
think of like

00:24:26,380 --> 00:24:32,170
traditional you know b-tree databases

00:24:28,420 --> 00:24:34,330
like my sequel they actually keep like

00:24:32,170 --> 00:24:36,730
this beat restructure and then when you

00:24:34,330 --> 00:24:39,010
modify the beat restructure it goes and

00:24:36,730 --> 00:24:41,500
and edits it on disk so you're doing

00:24:39,010 --> 00:24:45,100
like a disk I up every arrangement of

00:24:41,500 --> 00:24:46,630
data like as you do rights whereas log

00:24:45,100 --> 00:24:49,660
structure military applications are just

00:24:46,630 --> 00:24:51,790
just basically take all the rights in

00:24:49,660 --> 00:24:54,310
completely unsorted fashion write them

00:24:51,790 --> 00:24:56,200
to disk as like a buffer again the rot

00:24:54,310 --> 00:24:59,380
ahead log and then essentially flush

00:24:56,200 --> 00:25:03,370
them in chunks periodically to the to

00:24:59,380 --> 00:25:07,390
the system to so to the disk rather so

00:25:03,370 --> 00:25:09,550
this is why you know within three DB we

00:25:07,390 --> 00:25:11,350
kind of go so Cassandra does this with

00:25:09,550 --> 00:25:13,570
date/time compaction strategy as well

00:25:11,350 --> 00:25:16,000
and could Prometheus does is a similar

00:25:13,570 --> 00:25:19,360
thing you know you write up these large

00:25:16,000 --> 00:25:21,310
blocks and I think like that the major

00:25:19,360 --> 00:25:23,560
raised them why Prometheus and things

00:25:21,310 --> 00:25:25,300
like n3b are kind of more efficient this

00:25:23,560 --> 00:25:28,390
kind of thing is essentially because

00:25:25,300 --> 00:25:33,010
that that block that you write to disk

00:25:28,390 --> 00:25:35,770
in an aggregated fashion doesn't get

00:25:33,010 --> 00:25:38,170
merged with other data so traditionally

00:25:35,770 --> 00:25:41,560
you have in ll7 databases you have like

00:25:38,170 --> 00:25:43,930
level compaction or size based

00:25:41,560 --> 00:25:45,700
compaction so basically this boundary of

00:25:43,930 --> 00:25:49,870
like when do you write this huge block

00:25:45,700 --> 00:25:52,750
to disk is based on things like size or

00:25:49,870 --> 00:25:56,580
the how much memory in on the heap

00:25:52,750 --> 00:25:59,830
you're using and then essentially you

00:25:56,580 --> 00:26:01,600
you basically put that to disk but if

00:25:59,830 --> 00:26:03,610
you're doing that very frequently now

00:26:01,600 --> 00:26:05,020
you need to merge some of the the chunks

00:26:03,610 --> 00:26:07,090
that are on disk all together so you

00:26:05,020 --> 00:26:10,540
don't read like hundreds of thousands of

00:26:07,090 --> 00:26:12,670
files when and when a query comes in so

00:26:10,540 --> 00:26:15,040
you know with with n 3 billion

00:26:12,670 --> 00:26:17,260
Prometheus similarly like we basically

00:26:15,040 --> 00:26:20,080
do time-based

00:26:17,260 --> 00:26:23,380
compaction and so if you if you can't

00:26:20,080 --> 00:26:26,470
write more than say like an hour into

00:26:23,380 --> 00:26:30,700
the past that means after an hour has

00:26:26,470 --> 00:26:32,950
has been completely rolled out of you

00:26:30,700 --> 00:26:36,160
can take that entire hour chunk and just

00:26:32,950 --> 00:26:39,380
flush it to disk and never have to join

00:26:36,160 --> 00:26:41,510
it with any other chunks later so that

00:26:39,380 --> 00:26:45,530
that's kind of where all these like

00:26:41,510 --> 00:26:47,600
right volume optimization comes from so

00:26:45,530 --> 00:26:49,490
you know the design I guess yeah it's

00:26:47,600 --> 00:26:53,210
it's it's all about hi read and write

00:26:49,490 --> 00:26:55,250
volume per machine reduced cost we

00:26:53,210 --> 00:26:59,300
wanted strongly consistent cluster

00:26:55,250 --> 00:27:01,490
membership like in Cassandra we kind of

00:26:59,300 --> 00:27:03,590
we got bitten by the gossip hassle

00:27:01,490 --> 00:27:06,980
definitely when we got into hundreds of

00:27:03,590 --> 00:27:10,280
machines but you know it doesn't come up

00:27:06,980 --> 00:27:12,710
too often I guess as an issue with

00:27:10,280 --> 00:27:15,680
smaller deployments but that was

00:27:12,710 --> 00:27:17,990
definitely something we wanted and then

00:27:15,680 --> 00:27:19,580
you know we just are increasingly trying

00:27:17,990 --> 00:27:21,950
to push at least what we can get out of

00:27:19,580 --> 00:27:23,690
the system so it has support for high

00:27:21,950 --> 00:27:25,940
cotton le queries server large datasets

00:27:23,690 --> 00:27:29,870
you know we have petabytes of metrics

00:27:25,940 --> 00:27:32,390
data now it over and basically that can

00:27:29,870 --> 00:27:36,560
be randomly accessed in any way any part

00:27:32,390 --> 00:27:38,210
of that metrics volume tends to be we

00:27:36,560 --> 00:27:39,770
don't really separate them and know

00:27:38,210 --> 00:27:42,110
which ones you use more frequently than

00:27:39,770 --> 00:27:44,930
others so they they're kind of really

00:27:42,110 --> 00:27:47,930
being like lukewarm storage for us they

00:27:44,930 --> 00:27:49,400
need to be able to give you back data or

00:27:47,930 --> 00:27:50,960
like pretty quickly and a lot of them

00:27:49,400 --> 00:27:54,200
are starting to use be used in automated

00:27:50,960 --> 00:27:55,640
fashions so you know we especially for

00:27:54,200 --> 00:27:58,250
anomaly detection we take like five

00:27:55,640 --> 00:28:02,750
weeks of data for some queries and then

00:27:58,250 --> 00:28:05,800
compare at week over week so you know

00:28:02,750 --> 00:28:11,030
that that that kind of SLA requires

00:28:05,800 --> 00:28:15,440
pretty fast access for like even quite

00:28:11,030 --> 00:28:16,760
historical data so yes and I've kind of

00:28:15,440 --> 00:28:18,050
talked a little bit about like why we

00:28:16,760 --> 00:28:21,200
want to do less compactions

00:28:18,050 --> 00:28:23,540
and why why that's important the other

00:28:21,200 --> 00:28:25,760
thing you know we're very highly

00:28:23,540 --> 00:28:29,950
optimized for right availability and

00:28:25,760 --> 00:28:32,060
self-recovery essentially you know we

00:28:29,950 --> 00:28:33,500
when there's hardware failures and

00:28:32,060 --> 00:28:36,170
things like that going on the data

00:28:33,500 --> 00:28:38,270
center we want to be that that rock that

00:28:36,170 --> 00:28:39,620
actually stays up and and gives you

00:28:38,270 --> 00:28:41,930
inside when you're doing like post

00:28:39,620 --> 00:28:44,090
mortems and stuff like that so basically

00:28:41,930 --> 00:28:47,230
we try to make sure that like we don't

00:28:44,090 --> 00:28:51,530
really depend on any one thing even if

00:28:47,230 --> 00:28:53,100
our kind of etc D cluster membership is

00:28:51,530 --> 00:28:56,100
down we have like you know

00:28:53,100 --> 00:28:58,890
- of what that cluster state looks like

00:28:56,100 --> 00:29:01,950
so that even if you know you can't reach

00:28:58,890 --> 00:29:06,059
80 CD when when you come back up you can

00:29:01,950 --> 00:29:10,799
still take rights and recover on your

00:29:06,059 --> 00:29:14,010
own so yeah why am I talking about m3

00:29:10,799 --> 00:29:16,500
what money you want to use it for so as

00:29:14,010 --> 00:29:20,190
as I as you can see here we have we kind

00:29:16,500 --> 00:29:22,919
of have this integration now with with

00:29:20,190 --> 00:29:24,960
Prometheus you you can use it as a

00:29:22,919 --> 00:29:29,840
remote long-term storage for Prometheus

00:29:24,960 --> 00:29:32,309
and you don't have to run 80 CD yourself

00:29:29,840 --> 00:29:35,520
you know you can kind of run it in it

00:29:32,309 --> 00:29:37,980
embedded mode instead like basically

00:29:35,520 --> 00:29:41,460
what a single-entry DB server looks like

00:29:37,980 --> 00:29:44,280
is it has it's either optionally running

00:29:41,460 --> 00:29:46,770
or not running the etcd embedded

00:29:44,280 --> 00:29:48,240
embedded node and you can run that

00:29:46,770 --> 00:29:51,590
separately if you don't want to use the

00:29:48,240 --> 00:29:54,720
embedded embedded binary part of that

00:29:51,590 --> 00:29:57,690
but but you you will always be kind of

00:29:54,720 --> 00:30:03,240
using the index and the core time series

00:29:57,690 --> 00:30:04,710
storage part of that so yeah this is

00:30:03,240 --> 00:30:07,049
traditionally how you can use it out of

00:30:04,710 --> 00:30:11,850
the box today we do also want to start

00:30:07,049 --> 00:30:14,730
to support like dedicated queries coming

00:30:11,850 --> 00:30:17,460
to like basically this dedicated service

00:30:14,730 --> 00:30:20,190
called m3 query that we're building and

00:30:17,460 --> 00:30:22,200
that's so that things like refiner and

00:30:20,190 --> 00:30:24,120
other air loading engines can actually

00:30:22,200 --> 00:30:26,220
start to alert off this dater

00:30:24,120 --> 00:30:32,309
alert manager is kind of built into

00:30:26,220 --> 00:30:34,520
Prometheus so you can't really use m3

00:30:32,309 --> 00:30:37,289
and other stuff as an alert source

00:30:34,520 --> 00:30:40,559
unless yeah unless we have this like

00:30:37,289 --> 00:30:42,150
dedicated query service for you so you

00:30:40,559 --> 00:30:44,640
know there's people using this

00:30:42,150 --> 00:30:49,260
configuration and it's it's technically

00:30:44,640 --> 00:30:52,049
Bator there's for instance a big online

00:30:49,260 --> 00:30:54,240
retailer in China that actually has

00:30:52,049 --> 00:30:55,980
started using this recently so it'll

00:30:54,240 --> 00:30:58,200
works in two end and we know it you know

00:30:55,980 --> 00:31:00,720
we've been kind of helping people set

00:30:58,200 --> 00:31:02,850
this up I forget her so we spent a lot

00:31:00,720 --> 00:31:05,400
of time on Gator which is like our IRC

00:31:02,850 --> 00:31:06,450
but it's unfortunately not I wish it was

00:31:05,400 --> 00:31:10,230
Ric

00:31:06,450 --> 00:31:12,870
maybe we should set up a bridge but we

00:31:10,230 --> 00:31:14,100
haven't done that yet but yeah as I kind

00:31:12,870 --> 00:31:15,600
of mentioned here like this pretty

00:31:14,100 --> 00:31:19,560
limited guides on how to use this

00:31:15,600 --> 00:31:20,940
currently today so most people like most

00:31:19,560 --> 00:31:23,160
of the time you pretty much just want to

00:31:20,940 --> 00:31:27,510
do this and leave alerting and and

00:31:23,160 --> 00:31:29,760
everything else and in prometheus but if

00:31:27,510 --> 00:31:32,340
you need to there's there's a different

00:31:29,760 --> 00:31:34,920
way to run it as well and then you know

00:31:32,340 --> 00:31:38,130
what are we kind of like working on we

00:31:34,920 --> 00:31:41,370
again really want to kind of build this

00:31:38,130 --> 00:31:43,230
community for people who do have like

00:31:41,370 --> 00:31:46,020
deployments that are bigger than and

00:31:43,230 --> 00:31:47,700
then a few Prometheus instances and make

00:31:46,020 --> 00:31:51,570
it easier to run this kind of style of

00:31:47,700 --> 00:31:56,490
metrics platform we want to add graphite

00:31:51,570 --> 00:31:59,940
support to the to the dedicated and free

00:31:56,490 --> 00:32:01,500
query service that we've built and yeah

00:31:59,940 --> 00:32:04,350
because gives a true burr we actually do

00:32:01,500 --> 00:32:07,460
have both graphite and prom ql metrics

00:32:04,350 --> 00:32:10,500
and it's all housed just an n3 DP itself

00:32:07,460 --> 00:32:12,090
which is great because yeah we don't

00:32:10,500 --> 00:32:13,650
have to run like multiple different

00:32:12,090 --> 00:32:16,110
types of infrastructure based on the

00:32:13,650 --> 00:32:18,270
types of monitoring that we're doing and

00:32:16,110 --> 00:32:21,000
again I think like the more and more you

00:32:18,270 --> 00:32:22,680
know I we look at it that we look at

00:32:21,000 --> 00:32:25,050
like the observability ecosystem we want

00:32:22,680 --> 00:32:28,140
to build in layers so we want to at

00:32:25,050 --> 00:32:30,120
least provide some layers and then use

00:32:28,140 --> 00:32:32,100
other lights or which is why you know we

00:32:30,120 --> 00:32:35,010
use Prometheus so heavily and Prometheus

00:32:32,100 --> 00:32:38,850
exporters and all the other types of

00:32:35,010 --> 00:32:43,650
great like higher layers in the open

00:32:38,850 --> 00:32:45,240
source stack so yeah I think like better

00:32:43,650 --> 00:32:47,610
guides for the entry aggregator and

00:32:45,240 --> 00:32:49,710
collector so you can kind of run in this

00:32:47,610 --> 00:32:51,960
configuration a little bit more easily

00:32:49,710 --> 00:32:54,570
we're working on a kubernetes operator

00:32:51,960 --> 00:32:57,420
so instead of kind of like standing up

00:32:54,570 --> 00:33:00,090
your instances and deploying and 3db and

00:32:57,420 --> 00:33:04,080
configuring yourself you can just kind

00:33:00,090 --> 00:33:05,580
of like deploy it to a cube cluster and

00:33:04,080 --> 00:33:07,980
kubernetes kind of like takes care of

00:33:05,580 --> 00:33:12,420
like all the adding nodes replacing

00:33:07,980 --> 00:33:14,280
those spinning up-and-down instances so

00:33:12,420 --> 00:33:16,590
you're not doing any of that by hand and

00:33:14,280 --> 00:33:20,010
then you know we want to submit it to CN

00:33:16,590 --> 00:33:21,810
CF as well because again

00:33:20,010 --> 00:33:25,350
would kind of prefer this was in neutral

00:33:21,810 --> 00:33:27,600
territory rather than with just like

00:33:25,350 --> 00:33:29,070
single large contributors that don't

00:33:27,600 --> 00:33:32,090
have like good governance of the

00:33:29,070 --> 00:33:35,820
projects so where we're trying to get

00:33:32,090 --> 00:33:39,630
the project submitted and it seen CF

00:33:35,820 --> 00:33:41,250
dozen yeah I don't know one kind of if

00:33:39,630 --> 00:33:42,480
seen CF doesn't think it's a good

00:33:41,250 --> 00:33:44,250
project for scene CF I guess we probably

00:33:42,480 --> 00:33:47,280
look at Apache and other kind of

00:33:44,250 --> 00:33:48,960
software foundations but yeah I think

00:33:47,280 --> 00:33:51,120
like the for the most part we're really

00:33:48,960 --> 00:33:53,310
just trying to you know take a lot of

00:33:51,120 --> 00:33:56,030
feedback while we make this project more

00:33:53,310 --> 00:33:59,730
useful to other people in the community

00:33:56,030 --> 00:34:03,120
you know what is stuff that it can help

00:33:59,730 --> 00:34:04,500
them with what did what doesn't it - it

00:34:03,120 --> 00:34:06,540
kind of just gather as much of that

00:34:04,500 --> 00:34:08,310
feedback as possible right now you can

00:34:06,540 --> 00:34:11,340
kind of submit any feedback to us

00:34:08,310 --> 00:34:14,580
directly on Gitter which again is like

00:34:11,340 --> 00:34:17,730
IRC but it's on the web or on the

00:34:14,580 --> 00:34:21,840
mailing list or or so just an opening

00:34:17,730 --> 00:34:23,280
like a github issue with us so what does

00:34:21,840 --> 00:34:24,810
the future look like I mean I kind of

00:34:23,280 --> 00:34:27,149
talk a little bit about this you know I

00:34:24,810 --> 00:34:29,610
think basically to support for both ways

00:34:27,149 --> 00:34:31,110
of using it kind of like a push base

00:34:29,610 --> 00:34:33,629
model where you can push directly to

00:34:31,110 --> 00:34:35,700
like a local agent which then forwards

00:34:33,629 --> 00:34:37,490
it to disaggregation to you that lets

00:34:35,700 --> 00:34:41,159
you do roll-up rules and stuff like that

00:34:37,490 --> 00:34:43,169
and and also directly kind of emit

00:34:41,159 --> 00:34:46,169
graphite and stats D metrics to this

00:34:43,169 --> 00:34:48,600
aggregation tier and then kind of like

00:34:46,169 --> 00:34:52,290
support for dedicated queries for both

00:34:48,600 --> 00:34:56,340
from ql graphite and we'll probably add

00:34:52,290 --> 00:35:00,120
an 3ql support which is it's similar to

00:34:56,340 --> 00:35:02,040
like the flux language in that it's pipe

00:35:00,120 --> 00:35:04,020
base it looks like bash essentially you

00:35:02,040 --> 00:35:06,420
do like you know some kind of expression

00:35:04,020 --> 00:35:07,850
you pipe it to some others some

00:35:06,420 --> 00:35:10,500
expression and then you pipe it to

00:35:07,850 --> 00:35:12,060
whatever else so it's it's a bit

00:35:10,500 --> 00:35:14,730
different to procure and graphite which

00:35:12,060 --> 00:35:16,080
both are like functional coal base and

00:35:14,730 --> 00:35:17,490
looks like you're stacking function

00:35:16,080 --> 00:35:20,280
calling functions within functions with

00:35:17,490 --> 00:35:23,040
functions with in 3ql and similarly with

00:35:20,280 --> 00:35:27,360
fox it's like pipe base so you just pipe

00:35:23,040 --> 00:35:29,040
data to the next part of the query so

00:35:27,360 --> 00:35:31,770
the only reason we're really probably

00:35:29,040 --> 00:35:32,490
open source this is just because it's we

00:35:31,770 --> 00:35:35,640
don't then have

00:35:32,490 --> 00:35:41,010
to move any of our existing in 3ql

00:35:35,640 --> 00:35:42,300
queries to see something else and it'll

00:35:41,010 --> 00:35:45,030
be interesting to get some feedback and

00:35:42,300 --> 00:35:47,190
see if you know it because basically

00:35:45,030 --> 00:35:51,390
prompt q on n 3q o operate on the same

00:35:47,190 --> 00:35:53,040
similar metric data types so I think

00:35:51,390 --> 00:35:55,980
it'll be interesting to see if there's

00:35:53,040 --> 00:35:58,220
any actual interest in writing queries

00:35:55,980 --> 00:36:01,950
in a pipe based fashion verse like

00:35:58,220 --> 00:36:04,460
function expansion anyway that's a bit

00:36:01,950 --> 00:36:08,369
of a de rally you can also find us on

00:36:04,460 --> 00:36:11,580
Google hangout a zoom because yeah video

00:36:08,369 --> 00:36:13,740
internet works now also yeah I basically

00:36:11,580 --> 00:36:17,340
kind of like tend to sit on git er a

00:36:13,740 --> 00:36:19,950
fair bit until my manager but like half

00:36:17,340 --> 00:36:22,470
my mornings I usually don't get or at

00:36:19,950 --> 00:36:23,640
least at this like at this stage when

00:36:22,470 --> 00:36:27,090
it's when it's helpful to kind of help

00:36:23,640 --> 00:36:29,190
people and I think it you know it's

00:36:27,090 --> 00:36:31,440
really important to give as much support

00:36:29,190 --> 00:36:35,430
to anyone using this stuff since it's

00:36:31,440 --> 00:36:38,280
it's you know kind of it's it's only

00:36:35,430 --> 00:36:41,010
really been you know with with a lot of

00:36:38,280 --> 00:36:44,520
documentation and support out there for

00:36:41,010 --> 00:36:47,220
probably like three oh three or six

00:36:44,520 --> 00:36:50,280
months so while other big people are

00:36:47,220 --> 00:36:51,450
using it and productively it still still

00:36:50,280 --> 00:36:53,970
a little bit rough around the edges

00:36:51,450 --> 00:36:57,450
probably at the integration points so

00:36:53,970 --> 00:37:03,210
yeah you can book sometime this like a

00:36:57,450 --> 00:37:04,619
point lip thing is a way to find some

00:37:03,210 --> 00:37:10,380
time on the calendar when we can

00:37:04,619 --> 00:37:13,050
definitely talk so yeah I think so just

00:37:10,380 --> 00:37:14,940
keep that URL in mind and if you do

00:37:13,050 --> 00:37:18,840
actually one like a long-form discussion

00:37:14,940 --> 00:37:23,010
with any of us so other than that third

00:37:18,840 --> 00:37:24,030
that's everything yeah I think we've got

00:37:23,010 --> 00:37:28,070
some time for questions

00:37:24,030 --> 00:37:28,070
maybe thank you

00:37:31,530 --> 00:37:45,940
so um are there any questions we have an

00:37:43,360 --> 00:37:48,670
app that currently writes to Cassandra

00:37:45,940 --> 00:37:51,490
if we wanted to switch to M 3 DB how's

00:37:48,670 --> 00:37:53,200
the API different is it can we is it a

00:37:51,490 --> 00:37:57,160
drop-in replacement or will we need to

00:37:53,200 --> 00:38:00,340
tweak it a bit so this is kind of

00:37:57,160 --> 00:38:02,050
talking about an application that

00:38:00,340 --> 00:38:03,880
doesn't store telemetry or are you just

00:38:02,050 --> 00:38:11,950
talking about a general Cassandra use

00:38:03,880 --> 00:38:14,020
case NuStar IO which is new time series

00:38:11,950 --> 00:38:16,450
database commish and then we write to

00:38:14,020 --> 00:38:17,860
Cassandra I say and we're hitting the

00:38:16,450 --> 00:38:20,530
same we have a customer that's doing

00:38:17,860 --> 00:38:22,360
about 20,000 inserts a second yeah and

00:38:20,530 --> 00:38:25,960
you were a bit like 80,000 and like you

00:38:22,360 --> 00:38:27,370
said they want SSDs it gets and and then

00:38:25,960 --> 00:38:30,760
it's if you do any kind of replication

00:38:27,370 --> 00:38:33,070
it's like 3 or 4 X the size of the data

00:38:30,760 --> 00:38:34,570
you trying to store and so I was really

00:38:33,070 --> 00:38:36,130
excited when I was seeing this and I was

00:38:34,570 --> 00:38:38,170
like trying to get a level of effort

00:38:36,130 --> 00:38:40,450
that it would take to take like our

00:38:38,170 --> 00:38:42,550
newts project which is apache license

00:38:40,450 --> 00:38:44,380
and instead of using because we use

00:38:42,550 --> 00:38:46,750
cassandra and silla right now because

00:38:44,380 --> 00:38:48,850
they're pretty much the same and I just

00:38:46,750 --> 00:38:51,100
didn't know I mean this was a pretty

00:38:48,850 --> 00:38:53,680
high level but I didn't know how yeah

00:38:51,100 --> 00:38:57,580
hard it would be to point this to in 3

00:38:53,680 --> 00:39:01,720
dB yeah that's a great question so the

00:38:57,580 --> 00:39:03,970
the interface is so we have a go client

00:39:01,720 --> 00:39:07,540
and that's like the the most deepest

00:39:03,970 --> 00:39:09,730
integration you can get as in like you

00:39:07,540 --> 00:39:13,420
have to do a more work to use that than

00:39:09,730 --> 00:39:17,170
just killing some endpoints there's

00:39:13,420 --> 00:39:20,530
there's actually a dedicated this entry

00:39:17,170 --> 00:39:24,490
coordinator service here has an HCP

00:39:20,530 --> 00:39:28,810
endpoint it's just really poor like

00:39:24,490 --> 00:39:31,420
performance wise so that I definitely

00:39:28,810 --> 00:39:34,270
kind of like until there's significant

00:39:31,420 --> 00:39:36,700
work on that on that service you're

00:39:34,270 --> 00:39:41,350
probably best bet is to use the go

00:39:36,700 --> 00:39:42,660
client but I mean this is why you know I

00:39:41,350 --> 00:39:45,000
think like

00:39:42,660 --> 00:39:47,010
oh I would love to get more community

00:39:45,000 --> 00:39:50,070
involvement because I'm sure if you have

00:39:47,010 --> 00:39:52,740
that case someone else has that case and

00:39:50,070 --> 00:39:55,170
and it wouldn't be too crazy I think to

00:39:52,740 --> 00:39:57,750
create like an optimized ACP endpoint

00:39:55,170 --> 00:40:01,080
similar to how like influx have like a

00:39:57,750 --> 00:40:03,120
bulk HUP in point wouldn't be too too

00:40:01,080 --> 00:40:04,320
difficult to kind of like offer the same

00:40:03,120 --> 00:40:08,700
kind of thing that's just a wrapper

00:40:04,320 --> 00:40:10,410
around the go client begets that's yeah

00:40:08,700 --> 00:40:16,560
I'm that those are the two kind of

00:40:10,410 --> 00:40:19,380
different distinct ways to do it yeah

00:40:16,560 --> 00:40:22,200
it's yeah I definitely unless you want

00:40:19,380 --> 00:40:26,970
to try the coordinator but you know I

00:40:22,200 --> 00:40:29,180
wouldn't super recommend it okay more

00:40:26,970 --> 00:40:29,180
questions

00:40:31,580 --> 00:40:39,280
well then cool thank you for talking up

00:40:36,540 --> 00:40:46,270
the always MC thanks for chess

00:40:39,280 --> 00:40:57,429
[Applause]

00:40:46,270 --> 00:40:57,429

YouTube URL: https://www.youtube.com/watch?v=mrq-TBXpztU


