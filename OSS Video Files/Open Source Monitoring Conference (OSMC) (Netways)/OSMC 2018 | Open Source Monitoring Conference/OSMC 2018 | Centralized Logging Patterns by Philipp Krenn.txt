Title: OSMC 2018 | Centralized Logging Patterns by Philipp Krenn
Publication date: 2018-11-17
Playlist: OSMC 2018 | Open Source Monitoring Conference
Description: 
	Most organizations feel the need to centralize their logs â€” once you have more than a couple of servers or containers, SSH and tail will not serve you well any more. However, the common question or struggle is how to achieve that. This talk presents multiple approaches and patterns with their advantages and disadvantages, so you can pick the one that fits your organization best:

Parse: Take the log files of your applications and extract the relevant pieces of information.
Send directly: Add a log appender to send out your events directly without persisting them to a log file.
Structured file: Write your events in a structured file, which you can then centralize.
From a container: Keep track of short lived containers and configure their logging correctly.
In Kubernetes: Stay on top of your logs even when services are short lived and dynamically allocated.
Each pattern has its own demo, so you can easily try out the different approaches in your environment.

NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de/
Blog: http://blog.netways.de/
NWS: https://nws.netways.de

Webinare
Archiv Link: https://www.netways.de/webinare/archi...
Aktuell: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh/

www.musicfox.com
Captions: 
	00:00:02,230 --> 00:00:13,220
[Music]

00:00:14,900 --> 00:00:24,240
hi thank you by the way do you know what

00:00:18,900 --> 00:00:27,449
my last name name means with one M in

00:00:24,240 --> 00:00:32,130
German obviously yes something to eat

00:00:27,449 --> 00:00:35,100
which is hard hard to eat yes

00:00:32,130 --> 00:00:39,780
horseradish but in in Austrian German

00:00:35,100 --> 00:00:41,579
its grain is the reality thing anyway so

00:00:39,780 --> 00:00:43,920
I want to talk a bit about logging like

00:00:41,579 --> 00:00:45,360
probably you start developing your

00:00:43,920 --> 00:00:47,340
application and it looks something like

00:00:45,360 --> 00:00:49,829
this and everything is good and then

00:00:47,340 --> 00:00:52,559
then you start putting it somewhere to

00:00:49,829 --> 00:00:54,239
run and then you do cat and at some

00:00:52,559 --> 00:00:57,059
point later on you've maybe figure out

00:00:54,239 --> 00:00:59,399
well I want to use tail F because what I

00:00:57,059 --> 00:01:02,219
want to see what is going on and at some

00:00:59,399 --> 00:01:05,430
later point you start using less plus

00:01:02,219 --> 00:01:09,090
capital F anybody knows what less plus

00:01:05,430 --> 00:01:11,280
capital F is doing it's doing the same

00:01:09,090 --> 00:01:13,590
thing as tail F the only thing is you

00:01:11,280 --> 00:01:15,900
can break out of that with ctrl C and

00:01:13,590 --> 00:01:18,420
then you can search for stuff and go up

00:01:15,900 --> 00:01:19,860
and down like in VI for example so slash

00:01:18,420 --> 00:01:23,909
whatever a search term you're searching

00:01:19,860 --> 00:01:25,530
for you can just search in a file or you

00:01:23,909 --> 00:01:27,540
can just go up and down and if you go

00:01:25,530 --> 00:01:30,659
back to plus F mode and it start

00:01:27,540 --> 00:01:32,130
continues tailing your file which can be

00:01:30,659 --> 00:01:34,229
kind of the advanced version but

00:01:32,130 --> 00:01:35,700
obviously this is not really good enough

00:01:34,229 --> 00:01:37,170
because oftentimes we're sitting down

00:01:35,700 --> 00:01:40,829
there and saying this is fine and

00:01:37,170 --> 00:01:42,840
everything is almost on fire but you're

00:01:40,829 --> 00:01:45,780
still saying well this is this is kind

00:01:42,840 --> 00:01:46,979
of fine and then you're going to

00:01:45,780 --> 00:01:48,600
production and probably you have

00:01:46,979 --> 00:01:50,640
something that is more distributed and

00:01:48,600 --> 00:01:53,759
then stuff starts looking like this so

00:01:50,640 --> 00:01:55,979
you have three SSH windows and you just

00:01:53,759 --> 00:01:57,630
I try to see stuff and then you have

00:01:55,979 --> 00:01:59,759
even more services and then it looks

00:01:57,630 --> 00:02:01,590
something like this well you see one

00:01:59,759 --> 00:02:02,640
line per server anymore because you

00:02:01,590 --> 00:02:06,119
don't really know where stuff is

00:02:02,640 --> 00:02:08,220
happening anymore and then everything is

00:02:06,119 --> 00:02:10,920
not fine anymore and then everything is

00:02:08,220 --> 00:02:13,560
kind of on fire and we basically want to

00:02:10,920 --> 00:02:16,680
avoid that place

00:02:13,560 --> 00:02:17,790
and want to get something easier so

00:02:16,680 --> 00:02:20,250
that's the general idea

00:02:17,790 --> 00:02:21,599
so we basically want to log all the

00:02:20,250 --> 00:02:24,959
things and let's see where we can take

00:02:21,599 --> 00:02:27,360
this so yes I work for elastic that's

00:02:24,959 --> 00:02:29,180
why I'm talking about that my official

00:02:27,360 --> 00:02:31,349
title now is developer advocate but

00:02:29,180 --> 00:02:32,670
nobody knows what it is anyway and I

00:02:31,349 --> 00:02:34,019
always use this avocado because it's

00:02:32,670 --> 00:02:35,940
much friendlier and everybody is like

00:02:34,019 --> 00:02:37,860
fruit also if you print your badges you

00:02:35,940 --> 00:02:40,890
can always see which conference can

00:02:37,860 --> 00:02:42,209
print badges properly if you have the

00:02:40,890 --> 00:02:46,350
emoji in there which is kind of an

00:02:42,209 --> 00:02:47,489
interesting test anyway and so I'm not

00:02:46,350 --> 00:02:48,900
sure how many questions there will be

00:02:47,489 --> 00:02:51,360
and how much time we will have at the

00:02:48,900 --> 00:02:54,239
end but if you have any questions and

00:02:51,360 --> 00:02:56,519
you can post them on slider and I can

00:02:54,239 --> 00:02:58,680
look if you're too shy to ask in person

00:02:56,519 --> 00:03:00,150
we can look at slider at the end or if

00:02:58,680 --> 00:03:02,390
you run out of time I will just tweet

00:03:00,150 --> 00:03:05,340
out the answers to various questions

00:03:02,390 --> 00:03:07,620
please don't put any offensive stuff for

00:03:05,340 --> 00:03:09,680
stuff like that there which I might show

00:03:07,620 --> 00:03:12,660
afterwards because it's always awkward

00:03:09,680 --> 00:03:15,090
so if you go to slide or do slash my

00:03:12,660 --> 00:03:16,170
Twitter handle Twitter hand is also

00:03:15,090 --> 00:03:18,090
where the answers if you don't have

00:03:16,170 --> 00:03:20,310
enough time will come out at the end but

00:03:18,090 --> 00:03:21,660
you can just post anything at at any

00:03:20,310 --> 00:03:24,750
point in time and then we'll see how

00:03:21,660 --> 00:03:26,299
stuff progresses okay so I guess

00:03:24,750 --> 00:03:30,329
everybody remembers the good old elk

00:03:26,299 --> 00:03:32,790
elasticsearch la Cubana and well that

00:03:30,329 --> 00:03:34,380
worked well for a long time and this is

00:03:32,790 --> 00:03:37,109
I think this is three of the logos of

00:03:34,380 --> 00:03:39,540
people using us but well let's not cover

00:03:37,109 --> 00:03:41,370
use cases too much and then we figured

00:03:39,540 --> 00:03:44,010
out well there is this beach thing and

00:03:41,370 --> 00:03:46,019
then we had the elk be your Belk but

00:03:44,010 --> 00:03:47,639
since we're always about scaling this

00:03:46,019 --> 00:03:48,630
doesn't scale marketing wise because

00:03:47,639 --> 00:03:50,760
what happens if we have another

00:03:48,630 --> 00:03:52,139
open-source product and then we have

00:03:50,760 --> 00:03:55,410
another letter and then we need to make

00:03:52,139 --> 00:03:56,790
up another animal and it gets harder and

00:03:55,410 --> 00:04:00,030
getting rid of old marketing is always

00:03:56,790 --> 00:04:02,639
hard so this was a the plan to have this

00:04:00,030 --> 00:04:05,040
and we do have some stickers with that

00:04:02,639 --> 00:04:07,019
now but it's not any official branding

00:04:05,040 --> 00:04:09,930
so the bellcore dlp was only kind of a

00:04:07,019 --> 00:04:11,220
temporary solution so we just call it

00:04:09,930 --> 00:04:12,900
now the elastic stack and whatever

00:04:11,220 --> 00:04:17,010
open-source components we have we put

00:04:12,900 --> 00:04:19,709
into that and yeah the artwork we're

00:04:17,010 --> 00:04:21,150
doing is getting better so you may be

00:04:19,709 --> 00:04:22,770
you remember the initial logos for

00:04:21,150 --> 00:04:24,900
logstash and elastic search like the

00:04:22,770 --> 00:04:26,789
wooden log and everything back then

00:04:24,900 --> 00:04:27,430
basically the founders of the project

00:04:26,789 --> 00:04:29,889
we're using

00:04:27,430 --> 00:04:31,840
Photoshop and you could tell by now we

00:04:29,889 --> 00:04:34,270
have a more professional graphics team

00:04:31,840 --> 00:04:36,729
to create stuff like that but it's kind

00:04:34,270 --> 00:04:39,970
of the same thing and we will mostly

00:04:36,729 --> 00:04:42,520
cover like how to use this layer to

00:04:39,970 --> 00:04:44,289
collect stuff and work with stuff to put

00:04:42,520 --> 00:04:46,210
it into elasticsearch and then maybe

00:04:44,289 --> 00:04:49,210
show it in Cabana so that's the general

00:04:46,210 --> 00:04:51,550
idea and all the stuff I'm showing you

00:04:49,210 --> 00:04:53,500
today I think is Apache 2 license or at

00:04:51,550 --> 00:04:55,930
least free so you can go pretty crazy

00:04:53,500 --> 00:04:58,509
with that so I generally build highly

00:04:55,930 --> 00:05:01,870
monitored hello world applications this

00:04:58,509 --> 00:05:03,610
is no exception today though I think I

00:05:01,870 --> 00:05:07,599
just broke my demo of course because I

00:05:03,610 --> 00:05:09,669
upgraded to an unreleased snapshot I can

00:05:07,599 --> 00:05:11,259
show you some new stuff but I have some

00:05:09,669 --> 00:05:12,970
broken stuff in there but we will debug

00:05:11,259 --> 00:05:14,680
it or I will fix it afterwards and you

00:05:12,970 --> 00:05:16,449
can see it in the repository all the

00:05:14,680 --> 00:05:17,740
code I'm showing you will be on github

00:05:16,449 --> 00:05:21,630
of the word so you can just play around

00:05:17,740 --> 00:05:23,979
that as well so who is using Java here

00:05:21,630 --> 00:05:26,199
ok not that many who is using something

00:05:23,979 --> 00:05:31,690
else or who is programming in general

00:05:26,199 --> 00:05:34,240
okay so just to use something hands-on

00:05:31,690 --> 00:05:36,970
I'm I'm using Java because I've used

00:05:34,240 --> 00:05:38,830
that a lot in the past so I'm using the

00:05:36,970 --> 00:05:42,010
login facade SL foj

00:05:38,830 --> 00:05:44,530
my thing that writes out locks is lock

00:05:42,010 --> 00:05:48,099
lock back and I'm using structured locks

00:05:44,530 --> 00:05:50,169
the map diagnostic context MVC to get

00:05:48,099 --> 00:05:51,370
data I will show you some quick Java

00:05:50,169 --> 00:05:52,780
examples but they are very simple and

00:05:51,370 --> 00:05:55,449
they probably translate to any other

00:05:52,780 --> 00:05:57,009
programming language you can do the same

00:05:55,449 --> 00:06:01,150
thing with pretty much any other

00:05:57,009 --> 00:06:04,120
programming language even PHP has a

00:06:01,150 --> 00:06:06,580
proper Locker pendant now so even in PHP

00:06:04,120 --> 00:06:09,280
you don't have any excuses to do println

00:06:06,580 --> 00:06:10,840
or whatever anymore to lock there you

00:06:09,280 --> 00:06:13,270
have monologue which is very widely used

00:06:10,840 --> 00:06:15,280
term and pretty much every other

00:06:13,270 --> 00:06:17,229
programming language has a proper logger

00:06:15,280 --> 00:06:20,229
pen the way you can do similar things

00:06:17,229 --> 00:06:23,169
from what I'm showing you today so first

00:06:20,229 --> 00:06:24,789
off println or print or whatever

00:06:23,169 --> 00:06:26,949
forelocks is an anti-pattern I hope

00:06:24,789 --> 00:06:28,930
everybody agrees otherwise we can

00:06:26,949 --> 00:06:31,509
discuss that later on because you cannot

00:06:28,930 --> 00:06:33,460
really filter on what log level you want

00:06:31,509 --> 00:06:35,349
to get stuff also you cannot add

00:06:33,460 --> 00:06:37,570
structured information and you just very

00:06:35,349 --> 00:06:40,210
limited so let's assume we don't want to

00:06:37,570 --> 00:06:41,000
use print for logging let's say we agree

00:06:40,210 --> 00:06:42,410
on that

00:06:41,000 --> 00:06:43,970
the other thing what you don't want to

00:06:42,410 --> 00:06:46,760
have is tight coupling between your

00:06:43,970 --> 00:06:49,250
logging solution and your application so

00:06:46,760 --> 00:06:52,190
you don't want to have like proprietary

00:06:49,250 --> 00:06:54,170
or like locking specific import

00:06:52,190 --> 00:06:56,900
everywhere where you lock you want to

00:06:54,170 --> 00:06:58,820
use like a regular logging library for

00:06:56,900 --> 00:07:00,710
your programming language and then only

00:06:58,820 --> 00:07:02,780
process the data in a centralized

00:07:00,710 --> 00:07:04,760
fashion afterwards so you don't want to

00:07:02,780 --> 00:07:07,400
kind of couple your application to your

00:07:04,760 --> 00:07:09,290
locking solution tightly I assume

00:07:07,400 --> 00:07:11,840
everybody agrees with that as well so

00:07:09,290 --> 00:07:15,260
the first thing the good old approach is

00:07:11,840 --> 00:07:16,850
basically you start parsing logs who is

00:07:15,260 --> 00:07:19,280
doing that you're basically writing out

00:07:16,850 --> 00:07:22,730
logs and then you start parsing them ok

00:07:19,280 --> 00:07:26,270
that's surprisingly few who is generally

00:07:22,730 --> 00:07:29,480
you working with logs here ok more

00:07:26,270 --> 00:07:30,950
people ok good something so then the

00:07:29,480 --> 00:07:33,830
general architecture that we would have

00:07:30,950 --> 00:07:36,290
nowadays is that we you you write out

00:07:33,830 --> 00:07:38,870
your classical log file then BT spaced

00:07:36,290 --> 00:07:40,550
tailing that file and forwards it to log

00:07:38,870 --> 00:07:42,110
stash logs which is doing the parsing

00:07:40,550 --> 00:07:44,300
stores it into elasticsearch and then

00:07:42,110 --> 00:07:49,640
Cubana is showing you what is happening

00:07:44,300 --> 00:07:54,830
so that's simple enough so what do we

00:07:49,640 --> 00:07:57,470
even have here so I think don't worry we

00:07:54,830 --> 00:08:02,570
won't see too much Java today so I have

00:07:57,470 --> 00:08:04,280
a loop which iterates 20 times and I am

00:08:02,570 --> 00:08:07,100
implementing some pattern here for

00:08:04,280 --> 00:08:08,840
logging basically what pattern or what

00:08:07,100 --> 00:08:12,979
common thing are we doing here with our

00:08:08,840 --> 00:08:15,880
lock pattern it's a very common

00:08:12,979 --> 00:08:15,880
interview question

00:08:18,810 --> 00:08:23,910
it's a good old fizzbuzz basically we're

00:08:22,530 --> 00:08:26,250
doing the fist boss like if it's

00:08:23,910 --> 00:08:28,200
dividable or divisible by three by five

00:08:26,250 --> 00:08:35,339
or by 15 you have different lock

00:08:28,200 --> 00:08:37,589
messages so basically we have 20 20 log

00:08:35,339 --> 00:08:39,810
messages we want to emit we log

00:08:37,589 --> 00:08:41,339
everything on trace where we have some

00:08:39,810 --> 00:08:43,200
iteration and session these are like

00:08:41,339 --> 00:08:46,260
structured information we want to keep

00:08:43,200 --> 00:08:49,200
around I'm also using them here as a

00:08:46,260 --> 00:08:51,029
structured event and then depending on

00:08:49,200 --> 00:08:53,490
what position in the loop we're in we're

00:08:51,029 --> 00:08:56,480
having an error we're having a one and

00:08:53,490 --> 00:08:59,640
influitive back whatever so looking at

00:08:56,480 --> 00:09:01,830
so we have one specific message here and

00:08:59,640 --> 00:09:04,800
one generic message here so we will lock

00:09:01,830 --> 00:09:07,260
two events for every round in the loop

00:09:04,800 --> 00:09:12,720
how many log messages are you expecting

00:09:07,260 --> 00:09:16,920
in total yes that sounds good

00:09:12,720 --> 00:09:20,610
so let's run this no not the bug but I

00:09:16,920 --> 00:09:25,010
don't have a breakpoint anyway so it

00:09:20,610 --> 00:09:28,380
shouldn't make any difference so it well

00:09:25,010 --> 00:09:34,800
that worked we locked something let's

00:09:28,380 --> 00:09:36,630
head over to Cabana so um just large

00:09:34,800 --> 00:09:40,230
enough for everybody to see

00:09:36,630 --> 00:09:41,790
this is a release candidate of 6.5 that

00:09:40,230 --> 00:09:43,680
will come out soon enough you can see we

00:09:41,790 --> 00:09:45,300
we added some more logos and we have

00:09:43,680 --> 00:09:47,850
some more promising things like locks

00:09:45,300 --> 00:09:51,450
but I don't want to dive into logs but

00:09:47,850 --> 00:09:52,980
we have a more focused log UI in the

00:09:51,450 --> 00:09:55,080
future and we have a more focused

00:09:52,980 --> 00:09:59,280
infrastructure UI in the future as well

00:09:55,080 --> 00:10:01,440
and we want to we will get to that in

00:09:59,280 --> 00:10:04,170
the future so we head over to discover

00:10:01,440 --> 00:10:06,839
which should show us that in parse we

00:10:04,170 --> 00:10:10,290
have just collected events the funny

00:10:06,839 --> 00:10:12,540
thing is that we have 42 here and it

00:10:10,290 --> 00:10:17,339
guesses what could have been broken in

00:10:12,540 --> 00:10:20,209
my application and you can actually see

00:10:17,339 --> 00:10:23,730
it in the output here already

00:10:20,209 --> 00:10:25,740
yes multi-line which is kind of a common

00:10:23,730 --> 00:10:27,660
problem that you log line by line and

00:10:25,740 --> 00:10:30,120
then hear that stack trace is very short

00:10:27,660 --> 00:10:31,680
but you still have this these additional

00:10:30,120 --> 00:10:32,730
two lines here and this is why we have

00:10:31,680 --> 00:10:35,100
42 events

00:10:32,730 --> 00:10:37,170
I mean 42 is always the right answer but

00:10:35,100 --> 00:10:40,110
not for our log example here it should

00:10:37,170 --> 00:10:41,820
be 40 because basically what we what

00:10:40,110 --> 00:10:44,940
I've done is I've said like collect

00:10:41,820 --> 00:10:47,160
every single line and so didn't lock but

00:10:44,940 --> 00:10:48,810
breaking up a stack trace is not very

00:10:47,160 --> 00:10:52,380
helpful because one line of a sector s

00:10:48,810 --> 00:10:54,120
is normally not helping you so that's

00:10:52,380 --> 00:10:56,850
the first thing you would need to fix or

00:10:54,120 --> 00:10:58,440
actually before we fix anything let's

00:10:56,850 --> 00:11:02,400
see what we have been doing here

00:10:58,440 --> 00:11:05,460
generally so we have a file beat to log

00:11:02,400 --> 00:11:09,000
stash where I say collect everything in

00:11:05,460 --> 00:11:12,060
that log file and then forward it to log

00:11:09,000 --> 00:11:14,280
stash fair enough and then in log stash

00:11:12,060 --> 00:11:17,250
I have basically said well there is

00:11:14,280 --> 00:11:22,500
something coming here from so we have

00:11:17,250 --> 00:11:24,360
the the beats input we can see here this

00:11:22,500 --> 00:11:27,330
is what we take we apply that pattern to

00:11:24,360 --> 00:11:31,200
parse it and then we basically store it

00:11:27,330 --> 00:11:33,270
into the parse index and that's exactly

00:11:31,200 --> 00:11:36,540
what we have here so here in the parse

00:11:33,270 --> 00:11:39,480
index you can see for the loop you can

00:11:36,540 --> 00:11:42,090
see here 20 appears twice so these were

00:11:39,480 --> 00:11:43,050
the two events in the in the loop event

00:11:42,090 --> 00:11:45,540
where you can see okay

00:11:43,050 --> 00:11:49,470
we had a warning message here and we had

00:11:45,540 --> 00:11:52,560
the message for the general run and you

00:11:49,470 --> 00:11:54,810
can see I have the log levels I saw I've

00:11:52,560 --> 00:11:56,760
extracted the log level the logger that

00:11:54,810 --> 00:11:57,720
the loop event we were structured like

00:11:56,760 --> 00:11:59,520
this is distract these are the

00:11:57,720 --> 00:12:00,780
structured events with MVC if your

00:11:59,520 --> 00:12:02,490
logger can do something like that it's

00:12:00,780 --> 00:12:04,590
very helpful if you have like the user

00:12:02,490 --> 00:12:06,690
that is logged in or the IP address or

00:12:04,590 --> 00:12:09,300
some structured information to pin down

00:12:06,690 --> 00:12:11,430
where that stuff is coming from you have

00:12:09,300 --> 00:12:14,690
the original message you have an offset

00:12:11,430 --> 00:12:18,210
you have the source and all these things

00:12:14,690 --> 00:12:20,040
so that worked pretty well you just need

00:12:18,210 --> 00:12:22,890
to write that stupid rock python right

00:12:20,040 --> 00:12:26,250
who likes writing regular expressions or

00:12:22,890 --> 00:12:28,350
croc patterns and everybody who says yes

00:12:26,250 --> 00:12:31,560
I'm always like this must be Stockholm

00:12:28,350 --> 00:12:33,540
Syndrome well you kind of got so used to

00:12:31,560 --> 00:12:36,600
I'm doing that that's kind of like yes

00:12:33,540 --> 00:12:39,420
we love doing that I'm honestly not a

00:12:36,600 --> 00:12:41,160
big fan but by the way I'm the nicest

00:12:39,420 --> 00:12:42,840
way to write croc patterns is I'm not

00:12:41,160 --> 00:12:45,710
sure if you know that but in dev tools

00:12:42,840 --> 00:12:49,610
we have the croc debugger

00:12:45,710 --> 00:12:52,690
where you can paste let's let's cheat a

00:12:49,610 --> 00:12:57,830
bit and make this simple let's take

00:12:52,690 --> 00:13:02,090
let's just take this line here so this

00:12:57,830 --> 00:13:04,940
is our log line and let's cheat as well

00:13:02,090 --> 00:13:07,460
and let me just steal my log pattern

00:13:04,940 --> 00:13:12,350
because otherwise nobody wants to see me

00:13:07,460 --> 00:13:14,690
type that much and let's go to the

00:13:12,350 --> 00:13:19,420
beginning so you can see something it

00:13:14,690 --> 00:13:21,440
starts with a square bracket oops wrong

00:13:19,420 --> 00:13:24,950
with the bracket then we have a

00:13:21,440 --> 00:13:27,770
timestamp I ISO 8601 which we put into

00:13:24,950 --> 00:13:29,570
the field timestamp then there's square

00:13:27,770 --> 00:13:31,460
bracket is closing then there is one or

00:13:29,570 --> 00:13:34,190
more spaces then there is a log level

00:13:31,460 --> 00:13:36,620
that we put into the field level etc and

00:13:34,190 --> 00:13:38,690
then when you simulate this it will show

00:13:36,620 --> 00:13:41,690
you actually what would come out of your

00:13:38,690 --> 00:13:43,190
croc pattern so this is very handy if

00:13:41,690 --> 00:13:46,400
you need to write croc patterns and it

00:13:43,190 --> 00:13:48,710
will take the pain of croc way a bit but

00:13:46,400 --> 00:13:50,150
you still need to write for every single

00:13:48,710 --> 00:13:52,820
log format that you have you will need

00:13:50,150 --> 00:13:54,200
to write that line so depending on how

00:13:52,820 --> 00:13:55,820
much you write like your regular

00:13:54,200 --> 00:14:01,250
expressions that might be more or less

00:13:55,820 --> 00:14:08,900
fun okay and I have it in the file

00:14:01,250 --> 00:14:10,940
already but it's not an able here this

00:14:08,900 --> 00:14:12,830
would be the rule for the multi-line

00:14:10,940 --> 00:14:14,750
statement so basically we're saying

00:14:12,830 --> 00:14:16,520
everything that doesn't start with a

00:14:14,750 --> 00:14:17,690
square bracket because you can see every

00:14:16,520 --> 00:14:19,700
lock message starts with a square

00:14:17,690 --> 00:14:20,900
bracket unless it's a deck trace then

00:14:19,700 --> 00:14:24,260
it's not a square bracket but something

00:14:20,900 --> 00:14:26,060
else so unless it starts with the square

00:14:24,260 --> 00:14:28,310
so if it starts with a square bracket

00:14:26,060 --> 00:14:30,710
it's a new message otherwise it's the

00:14:28,310 --> 00:14:31,880
continuation of a previous log statement

00:14:30,710 --> 00:14:35,090
and it should be put into one message

00:14:31,880 --> 00:14:37,850
and if you add that multi-line pattern

00:14:35,090 --> 00:14:39,890
it will keep the stack trace in one

00:14:37,850 --> 00:14:42,230
piece and then you would only have 40

00:14:39,890 --> 00:14:44,180
log events and not 42 anymore and you

00:14:42,230 --> 00:14:46,910
will not unnecessarily break up the

00:14:44,180 --> 00:14:49,700
stack traces so that's helpful and you

00:14:46,910 --> 00:14:53,600
could also for example say in your log

00:14:49,700 --> 00:14:55,640
file I'll be to logstash here I had that

00:14:53,600 --> 00:14:57,620
line uncommented that's why I didn't do

00:14:55,640 --> 00:14:59,600
anything you could also include

00:14:57,620 --> 00:15:01,339
or exclude specific lines explicitly for

00:14:59,600 --> 00:15:03,380
example here we could say we want to

00:15:01,339 --> 00:15:05,120
exclude the trace messages because too

00:15:03,380 --> 00:15:05,510
much noise we don't want to centralize

00:15:05,120 --> 00:15:07,720
that

00:15:05,510 --> 00:15:11,540
so we basically kick out the trace here

00:15:07,720 --> 00:15:15,230
so that's the parsing part basically and

00:15:11,540 --> 00:15:16,970
yeah to parse that one line everything

00:15:15,230 --> 00:15:19,430
works by the way and you can see I've

00:15:16,970 --> 00:15:23,810
used an emoji in my lock which doesn't

00:15:19,430 --> 00:15:25,910
look great in in my java application

00:15:23,810 --> 00:15:31,550
because in Java

00:15:25,910 --> 00:15:34,880
it looks this the emoji and this is

00:15:31,550 --> 00:15:38,450
supposed to be loop 15 let's see if we

00:15:34,880 --> 00:15:42,339
can find it in Cabana because I think we

00:15:38,450 --> 00:15:42,339
edited in 6 or in some version like that

00:15:42,430 --> 00:15:47,720
wait but what was the field that we had

00:15:45,310 --> 00:15:49,940
where's my application what is the field

00:15:47,720 --> 00:15:53,630
where we had that user experience so we

00:15:49,940 --> 00:15:55,570
should the user experience and then you

00:15:53,630 --> 00:15:58,550
can see we support emojis everywhere now

00:15:55,570 --> 00:16:03,070
like full parsing Cubana and everything

00:15:58,550 --> 00:16:06,459
yes most important feature of anything

00:16:03,070 --> 00:16:08,630
yeah major change or major improvement

00:16:06,459 --> 00:16:11,959
that that's how you make your users

00:16:08,630 --> 00:16:13,730
happy so generally the nice thing about

00:16:11,959 --> 00:16:15,470
this approach is that you don't need to

00:16:13,730 --> 00:16:16,970
change your application you can just

00:16:15,470 --> 00:16:20,630
keep logging the way you've always

00:16:16,970 --> 00:16:22,459
locked do not need to write regular

00:16:20,630 --> 00:16:25,760
expressions which I personally don't

00:16:22,459 --> 00:16:27,110
like but well depends sometimes your

00:16:25,760 --> 00:16:29,360
time stamps are off and then you need to

00:16:27,110 --> 00:16:30,440
fix the time stamps as well and you

00:16:29,360 --> 00:16:32,089
always need to be careful with

00:16:30,440 --> 00:16:36,170
multi-line statements to not break up

00:16:32,089 --> 00:16:38,690
your stick traces next possibility would

00:16:36,170 --> 00:16:40,850
be to take whatever your application is

00:16:38,690 --> 00:16:42,680
logging and then you basically have

00:16:40,850 --> 00:16:45,020
everything structured when you try to

00:16:42,680 --> 00:16:47,150
write it out to your log so why write it

00:16:45,020 --> 00:16:49,279
out to a log and then parse it back we

00:16:47,150 --> 00:16:51,350
could send it in a structured format to

00:16:49,279 --> 00:16:54,050
our centralized logging system right

00:16:51,350 --> 00:16:55,959
away so basically what you could do is

00:16:54,050 --> 00:16:58,790
you could just add another logger Pender

00:16:55,959 --> 00:17:00,560
so just heading over to the logger

00:16:58,790 --> 00:17:02,089
Pender's here's my lock that

00:17:00,560 --> 00:17:05,300
configurations to what we did initially

00:17:02,089 --> 00:17:07,490
was I said well my my application is

00:17:05,300 --> 00:17:09,110
writing out to a flat file and that is

00:17:07,490 --> 00:17:10,920
the pattern and this is what we were

00:17:09,110 --> 00:17:12,540
writing out but instead we

00:17:10,920 --> 00:17:14,820
you could do is you could say I want to

00:17:12,540 --> 00:17:16,440
send this to logstash right away I don't

00:17:14,820 --> 00:17:18,330
want to put it into a file and then

00:17:16,440 --> 00:17:20,550
parse it back I send it to logstash and

00:17:18,330 --> 00:17:23,670
it knows all the fields already in the

00:17:20,550 --> 00:17:25,650
same format that works I just think I've

00:17:23,670 --> 00:17:28,380
broken it my temp demo so I don't want

00:17:25,650 --> 00:17:31,140
to try this now but it it would do the

00:17:28,380 --> 00:17:32,520
right thing so you said that looks

00:17:31,140 --> 00:17:34,530
something like this your application has

00:17:32,520 --> 00:17:37,290
this the right log appender which talks

00:17:34,530 --> 00:17:39,090
to log stash directly which puts it into

00:17:37,290 --> 00:17:41,490
elasticsearch and include two Cubana and

00:17:39,090 --> 00:17:46,310
you don't need to do any parsing who is

00:17:41,490 --> 00:17:51,360
doing that anybody is doing that okay

00:17:46,310 --> 00:17:56,460
what are the downsides of this any

00:17:51,360 --> 00:17:58,800
downsides no yeah that's true

00:17:56,460 --> 00:18:01,230
I think so it depends on the logger

00:17:58,800 --> 00:18:03,390
Pender for example unlock tech I think

00:18:01,230 --> 00:18:06,900
keeps 200 megabytes worth of logs by

00:18:03,390 --> 00:18:09,240
default but if you reach more if your

00:18:06,900 --> 00:18:11,490
receiver is down for more than 200

00:18:09,240 --> 00:18:14,330
megabytes worth of data you will lose

00:18:11,490 --> 00:18:17,010
those messages yes

00:18:14,330 --> 00:18:19,440
the other thing is kind of if you have

00:18:17,010 --> 00:18:20,760
network trouble on a day you will not

00:18:19,440 --> 00:18:22,890
receive your centralized logs and

00:18:20,760 --> 00:18:25,080
probably then you care most about your

00:18:22,890 --> 00:18:26,310
logs because if stuff is not working

00:18:25,080 --> 00:18:28,740
correctly then you want to see what is

00:18:26,310 --> 00:18:30,450
going on if your logs mainly work in the

00:18:28,740 --> 00:18:31,860
sunshine case it's kind of a stupid

00:18:30,450 --> 00:18:33,330
system because in some it's a shine case

00:18:31,860 --> 00:18:37,380
you don't care about you logs that much

00:18:33,330 --> 00:18:38,670
so the general the process is you don't

00:18:37,380 --> 00:18:43,080
have files and you don't need to do that

00:18:38,670 --> 00:18:44,580
parsing the main downsides are if your

00:18:43,080 --> 00:18:47,160
network is down you will not see any

00:18:44,580 --> 00:18:50,190
logs and you have kind of a tight

00:18:47,160 --> 00:18:52,830
coupling between your log appender and

00:18:50,190 --> 00:18:56,040
the receiving system also if you have a

00:18:52,830 --> 00:18:57,720
bad Locker pendler there are some which

00:18:56,040 --> 00:19:00,720
don't or which see that they cannot

00:18:57,720 --> 00:19:02,940
receive a reach the receiver and they

00:19:00,720 --> 00:19:05,700
might open more and more threads over

00:19:02,940 --> 00:19:07,290
time and we've seen cases where if the

00:19:05,700 --> 00:19:10,470
logging the centralized logging system

00:19:07,290 --> 00:19:11,850
was down the log offender would actually

00:19:10,470 --> 00:19:13,590
kill the production application and

00:19:11,850 --> 00:19:15,480
that's kind of a coupling you don't

00:19:13,590 --> 00:19:16,050
really want it depends on your log

00:19:15,480 --> 00:19:18,960
offender

00:19:16,050 --> 00:19:21,030
so most good what good ones don't do

00:19:18,960 --> 00:19:22,110
that but we have seen cases where logger

00:19:21,030 --> 00:19:24,570
painters killed the production

00:19:22,110 --> 00:19:26,190
application if you centralize logging

00:19:24,570 --> 00:19:28,169
management was down which is not a good

00:19:26,190 --> 00:19:30,840
place to be in okay

00:19:28,169 --> 00:19:33,360
the next thing is what you might want to

00:19:30,840 --> 00:19:34,679
do is you structure your loss um which

00:19:33,360 --> 00:19:36,539
might look something like this

00:19:34,679 --> 00:19:38,730
so you have another log appender which

00:19:36,539 --> 00:19:40,649
writes out in structured format then the

00:19:38,730 --> 00:19:42,059
beats basically tailed it file and store

00:19:40,649 --> 00:19:45,179
it into elasticsearch directly because

00:19:42,059 --> 00:19:47,549
well you don't have to do any parsing

00:19:45,179 --> 00:19:50,190
anymore because it's structured so in my

00:19:47,549 --> 00:19:52,590
application I could just write that out

00:19:50,190 --> 00:19:54,840
to Jason and well I have another logger

00:19:52,590 --> 00:19:56,879
Pender where basically say all those

00:19:54,840 --> 00:19:59,159
fields that I have instead of writing

00:19:56,879 --> 00:20:01,769
them to my lock pattern here and writing

00:19:59,159 --> 00:20:04,379
it into one line my lock pattern should

00:20:01,769 --> 00:20:05,879
just write it into a JSON document and

00:20:04,379 --> 00:20:08,700
then you also don't need to care about

00:20:05,879 --> 00:20:10,830
multi-line parsing in anything because

00:20:08,700 --> 00:20:12,179
well if it knows how to approach that so

00:20:10,830 --> 00:20:13,769
you just put everything into a

00:20:12,179 --> 00:20:19,460
structured format here and then you can

00:20:13,769 --> 00:20:21,330
tail it so that's pretty easy okay

00:20:19,460 --> 00:20:23,370
which is good because it's the right

00:20:21,330 --> 00:20:24,899
format you just have some serialization

00:20:23,370 --> 00:20:26,370
overhead for Jason and you need that

00:20:24,899 --> 00:20:28,620
additional logger Pendrell but generally

00:20:26,370 --> 00:20:30,450
this is an approach we we would

00:20:28,620 --> 00:20:31,710
recommend if you can like if you can

00:20:30,450 --> 00:20:33,210
just add a lock appended for your

00:20:31,710 --> 00:20:35,419
application this is very easy and you

00:20:33,210 --> 00:20:37,529
have very little overhead and it makes

00:20:35,419 --> 00:20:39,210
consuming your locks pretty easy

00:20:37,529 --> 00:20:42,029
afterwards

00:20:39,210 --> 00:20:45,750
next up containerized what happens if

00:20:42,029 --> 00:20:47,850
you need to containerize stuff I guess

00:20:45,750 --> 00:20:52,169
everybody is using docker right is

00:20:47,850 --> 00:20:53,940
anybody not using docker no I mean sorry

00:20:52,169 --> 00:21:00,120
you're using some container technology

00:20:53,940 --> 00:21:02,009
that is not docker 2 3 ok that's more

00:21:00,120 --> 00:21:05,340
than I was expecting because normally

00:21:02,009 --> 00:21:07,169
it's really almost exclusively talker by

00:21:05,340 --> 00:21:11,279
the way I think that the right logo for

00:21:07,169 --> 00:21:13,289
docker is normally this which is a how

00:21:11,279 --> 00:21:16,460
docker behaves and beat this is also the

00:21:13,289 --> 00:21:20,669
size of your average container right

00:21:16,460 --> 00:21:23,870
anyway so docker where where do you

00:21:20,669 --> 00:21:26,909
collect you logs if you use containers

00:21:23,870 --> 00:21:28,110
or yeah making more fun of containers

00:21:26,909 --> 00:21:29,549
previously it only worked in your

00:21:28,110 --> 00:21:32,730
machine and now it only works in your

00:21:29,549 --> 00:21:33,990
containers maybe that's the thing maybe

00:21:32,730 --> 00:21:36,720
that's not a thing but you still want to

00:21:33,990 --> 00:21:37,950
collect those locks so where do you put

00:21:36,720 --> 00:21:40,889
the beat to

00:21:37,950 --> 00:21:42,960
logs generally putting it into your

00:21:40,889 --> 00:21:44,429
container is not not a great solution we

00:21:42,960 --> 00:21:46,980
would recommend a sidecar so you can

00:21:44,429 --> 00:21:50,690
just run the sidecar next to it and it

00:21:46,980 --> 00:21:53,299
will keep tailing your files like that

00:21:50,690 --> 00:21:58,590
which looks something like this

00:21:53,299 --> 00:22:01,230
basically on your operating system

00:21:58,590 --> 00:22:03,210
probably you're talking logs will end up

00:22:01,230 --> 00:22:07,260
in barley docker containers and then dr.

00:22:03,210 --> 00:22:09,480
Eide dr. ID dot log basically you say ok

00:22:07,260 --> 00:22:11,789
this is a log file I would want to

00:22:09,480 --> 00:22:13,799
consume that and one of the nice things

00:22:11,789 --> 00:22:16,409
that we can do or that file bit can do

00:22:13,799 --> 00:22:19,620
is it can rich can enrich the doctor

00:22:16,409 --> 00:22:21,090
information into every log line so you

00:22:19,620 --> 00:22:24,990
actually know where this is coming from

00:22:21,090 --> 00:22:27,240
and the result of that including that

00:22:24,990 --> 00:22:28,590
all at docker metadata might look

00:22:27,240 --> 00:22:30,720
something like this where you see the

00:22:28,590 --> 00:22:32,309
host the port and then you have to stock

00:22:30,720 --> 00:22:36,000
a meta data where you see this is the ID

00:22:32,309 --> 00:22:38,850
of the container you have bless you the

00:22:36,000 --> 00:22:40,679
the name of the image you have the image

00:22:38,850 --> 00:22:42,990
you have the name of the container you

00:22:40,679 --> 00:22:45,149
have any labels that you have and then

00:22:42,990 --> 00:22:47,279
in your in your logs you could filter

00:22:45,149 --> 00:22:48,570
down on specific labels for example to

00:22:47,279 --> 00:22:50,549
say like only give me the job

00:22:48,570 --> 00:22:53,760
applications or only give me stuff that

00:22:50,549 --> 00:22:55,470
is based on a specific image so

00:22:53,760 --> 00:22:57,690
basically we resolve that against the

00:22:55,470 --> 00:22:59,279
docker demon once and then the file bit

00:22:57,690 --> 00:23:02,039
keeps that information cached and

00:22:59,279 --> 00:23:04,529
enriches every following event with that

00:23:02,039 --> 00:23:08,340
additional meta information and then you

00:23:04,529 --> 00:23:11,340
can simply search and filter on that

00:23:08,340 --> 00:23:12,840
information later on if you have more

00:23:11,340 --> 00:23:14,700
like the classical application that

00:23:12,840 --> 00:23:18,360
doesn't write to system out but wanted

00:23:14,700 --> 00:23:21,179
to write to its own log file oh by the

00:23:18,360 --> 00:23:24,450
way one thing if you use the approach of

00:23:21,179 --> 00:23:25,889
the default dr. Jason locks you need to

00:23:24,450 --> 00:23:27,630
enable rotation because that's not

00:23:25,889 --> 00:23:29,250
enabled by default which sometimes

00:23:27,630 --> 00:23:31,440
people forget and then fill up the disks

00:23:29,250 --> 00:23:35,039
because rotation is not enabled on that

00:23:31,440 --> 00:23:36,419
file if you have your own classical

00:23:35,039 --> 00:23:39,929
application that writes out to a path

00:23:36,419 --> 00:23:43,200
what you would do is basically you would

00:23:39,929 --> 00:23:45,360
have your java application and you

00:23:43,200 --> 00:23:47,460
basically write out your logs to a

00:23:45,360 --> 00:23:50,070
specific folder that you mount out of

00:23:47,460 --> 00:23:51,549
that container and then in your sidecar

00:23:50,070 --> 00:23:54,129
container you

00:23:51,549 --> 00:23:57,340
in that those log files and then consume

00:23:54,129 --> 00:23:58,960
that from the sidecar and well your

00:23:57,340 --> 00:24:01,149
application they need to do the rotation

00:23:58,960 --> 00:24:04,029
and everything on its own but we can

00:24:01,149 --> 00:24:05,379
just consume that so this would be where

00:24:04,029 --> 00:24:07,570
the file bit configuration is being

00:24:05,379 --> 00:24:10,749
loaded in the beat and this is where the

00:24:07,570 --> 00:24:13,480
file or lock file to be consumed would

00:24:10,749 --> 00:24:14,739
be and that's all you need so this is

00:24:13,480 --> 00:24:16,269
how you could run that and consume that

00:24:14,739 --> 00:24:18,610
information and for every application

00:24:16,269 --> 00:24:21,220
that you run you would need to add the

00:24:18,610 --> 00:24:23,799
right volumes and mount them out of your

00:24:21,220 --> 00:24:27,759
application container into your log

00:24:23,799 --> 00:24:29,470
container to consume that one thing that

00:24:27,759 --> 00:24:31,029
you might want to do is you want to

00:24:29,470 --> 00:24:32,679
externalize the registry file does

00:24:31,029 --> 00:24:35,919
anybody know why that might be a good

00:24:32,679 --> 00:24:41,649
idea or let's start somewhere else what

00:24:35,919 --> 00:24:44,529
is the registry file in file bit yes

00:24:41,649 --> 00:24:46,960
exactly so basically it keeps the it

00:24:44,529 --> 00:24:48,580
keeps the file though it keeps the inode

00:24:46,960 --> 00:24:50,379
which also makes it work across

00:24:48,580 --> 00:24:52,299
rotations which is another very nice

00:24:50,379 --> 00:24:54,639
feature that even if you rotate the file

00:24:52,299 --> 00:24:57,399
it will know that this specific file I

00:24:54,639 --> 00:24:59,470
have sent everything until this offset

00:24:57,399 --> 00:25:01,450
in the file and everything afterwards is

00:24:59,470 --> 00:25:03,970
still missing and that works across

00:25:01,450 --> 00:25:07,029
rotations I have no idea how that works

00:25:03,970 --> 00:25:09,840
in Windows though or if that works and I

00:25:07,029 --> 00:25:13,029
don't even have a desire to try it out

00:25:09,840 --> 00:25:14,649
anyway and so that registry file keeps a

00:25:13,029 --> 00:25:17,320
pointer of what has been sent and

00:25:14,649 --> 00:25:19,149
acknowledged by the receiver already if

00:25:17,320 --> 00:25:21,730
you replace the container though and

00:25:19,149 --> 00:25:22,210
that registry file is inside the

00:25:21,730 --> 00:25:24,429
container

00:25:22,210 --> 00:25:27,580
you will start Rican suing all the log

00:25:24,429 --> 00:25:29,649
files again that's why you might want to

00:25:27,580 --> 00:25:31,960
keep that registry file outside of the

00:25:29,649 --> 00:25:33,999
container on a bank mounted directory so

00:25:31,960 --> 00:25:36,460
if you replace for example you update

00:25:33,999 --> 00:25:39,460
the file bead you update from six point

00:25:36,460 --> 00:25:41,289
three to six point four if you keep the

00:25:39,460 --> 00:25:42,669
registry file inside the container that

00:25:41,289 --> 00:25:44,679
knowledge is lost and we would try to

00:25:42,669 --> 00:25:47,259
reconsider log files that we can reach

00:25:44,679 --> 00:25:48,820
if you mount the registry file outside

00:25:47,259 --> 00:25:50,739
of the container we will keep the same

00:25:48,820 --> 00:25:53,950
registry file and then we know what has

00:25:50,739 --> 00:25:56,470
already been sent away and not duplicate

00:25:53,950 --> 00:26:00,840
all the locks so that's kind of a neat

00:25:56,470 --> 00:26:03,100
trick you should keep in mind some

00:26:00,840 --> 00:26:05,049
systems lock crap like that

00:26:03,100 --> 00:26:06,759
for example radius and

00:26:05,049 --> 00:26:08,649
nobody wants to centralize that I mean

00:26:06,759 --> 00:26:12,249
it's cute and funny and when you start

00:26:08,649 --> 00:26:13,779
to desire already so nice but nobody

00:26:12,249 --> 00:26:15,879
wants to centralize that because nobody

00:26:13,779 --> 00:26:19,570
wants to search for that we have one

00:26:15,879 --> 00:26:21,489
additional thing we can basically and

00:26:19,570 --> 00:26:25,299
use templates and in those templates

00:26:21,489 --> 00:26:27,159
when we collect the log file we could

00:26:25,299 --> 00:26:29,860
define the condition so for example we

00:26:27,159 --> 00:26:32,559
could say if the image is based on Redis

00:26:29,860 --> 00:26:34,299
we want to apply some rules and those

00:26:32,559 --> 00:26:35,950
are in that configuration block and that

00:26:34,299 --> 00:26:38,259
configuration block basically says I

00:26:35,950 --> 00:26:41,679
want to exclude any line that contains

00:26:38,259 --> 00:26:43,629
that ASCII art crap and so that there's

00:26:41,679 --> 00:26:46,029
one way to get rid of that so you could

00:26:43,629 --> 00:26:48,009
have specific rules based on some label

00:26:46,029 --> 00:26:49,149
or name or base image or whatever meta

00:26:48,009 --> 00:26:52,029
information you have on the docket

00:26:49,149 --> 00:26:55,720
container you could filter down and

00:26:52,029 --> 00:26:57,549
exclude stuff based on that okay good

00:26:55,720 --> 00:26:59,379
thing is it's kind of hot because

00:26:57,549 --> 00:27:02,980
everybody needs to know use containers

00:26:59,379 --> 00:27:04,299
now if you don't know containers it's

00:27:02,980 --> 00:27:06,940
probably not the best place to start

00:27:04,299 --> 00:27:08,859
because I think 50 percent or so of the

00:27:06,940 --> 00:27:10,389
the questions we get around containers

00:27:08,859 --> 00:27:12,609
and elasticsearch or really docker

00:27:10,389 --> 00:27:15,309
questions and not questions about our

00:27:12,609 --> 00:27:18,909
products but we got used to doing part

00:27:15,309 --> 00:27:22,330
time docker support now for better or

00:27:18,909 --> 00:27:23,739
worse okay next up orchestration what

00:27:22,330 --> 00:27:27,249
are castration is everybody using who is

00:27:23,739 --> 00:27:31,539
using kubernetes who is using something

00:27:27,249 --> 00:27:33,879
else okay also fewer because normally we

00:27:31,539 --> 00:27:36,970
see kubernetes and that's what we

00:27:33,879 --> 00:27:38,710
integrate into again so where do you put

00:27:36,970 --> 00:27:39,879
your file bit there is a daemon set

00:27:38,710 --> 00:27:42,070
where you can make sure there is one

00:27:39,879 --> 00:27:45,309
instance running on each host that you

00:27:42,070 --> 00:27:49,269
have and that one can then consume all

00:27:45,309 --> 00:27:52,330
the log files from that host we can

00:27:49,269 --> 00:27:54,669
include metadata so if your run as if

00:27:52,330 --> 00:27:57,340
that container the file bit containers

00:27:54,669 --> 00:27:59,559
running is part of kubernetes it can

00:27:57,340 --> 00:28:01,690
just use the metadata from its own

00:27:59,559 --> 00:28:03,220
cluster if it's running outside of that

00:28:01,690 --> 00:28:04,869
so if you would install that for example

00:28:03,220 --> 00:28:07,809
the file bit explicitly on the host and

00:28:04,869 --> 00:28:10,269
managed to rest with kubernetes you

00:28:07,809 --> 00:28:12,879
could still connect to kubernetes and

00:28:10,269 --> 00:28:15,009
get information from its API what you

00:28:12,879 --> 00:28:18,880
would get is that you have the

00:28:15,009 --> 00:28:20,950
enrichment both for the docker base data

00:28:18,880 --> 00:28:23,080
but you also get stuff like the note the

00:28:20,950 --> 00:28:25,420
pod and the namespace and you can use

00:28:23,080 --> 00:28:28,870
that information afterwards where to put

00:28:25,420 --> 00:28:30,880
stuff and you can filter out the ASCII

00:28:28,870 --> 00:28:34,180
art crap again which looks pretty much

00:28:30,880 --> 00:28:36,700
exactly the same what you can do is for

00:28:34,180 --> 00:28:38,920
example in the output you could have a

00:28:36,700 --> 00:28:41,890
different index for every namespace or

00:28:38,920 --> 00:28:44,350
every port that you have it might be end

00:28:41,890 --> 00:28:45,700
up being a lot of indices in the end but

00:28:44,350 --> 00:28:48,280
you can do that so what we're doing here

00:28:45,700 --> 00:28:50,920
basically is since we enrich that method

00:28:48,280 --> 00:28:53,380
that kubernetes metadata if there is a

00:28:50,920 --> 00:28:55,600
kubernetes namespace use that as the

00:28:53,380 --> 00:28:57,040
index name otherwise use file bits so

00:28:55,600 --> 00:28:58,870
that column means either if this is

00:28:57,040 --> 00:29:03,490
available use this otherwise good use

00:28:58,870 --> 00:29:05,080
file bit - the beat version - whatever

00:29:03,490 --> 00:29:06,760
the data is and we're using a daily

00:29:05,080 --> 00:29:08,220
pattern here which might more sense to

00:29:06,760 --> 00:29:10,840
make that a weekly pattern or whatever

00:29:08,220 --> 00:29:12,250
but that way you could break out break

00:29:10,840 --> 00:29:16,720
up your namespaces into different

00:29:12,250 --> 00:29:19,120
indices for example yeah it's kind of

00:29:16,720 --> 00:29:25,150
even hotter but also the complexity is

00:29:19,120 --> 00:29:28,120
increasing I think we are about released

00:29:25,150 --> 00:29:31,170
like helm charts for everything in the

00:29:28,120 --> 00:29:34,060
kubernetes world so we will even offer

00:29:31,170 --> 00:29:35,410
while the beats can generally work well

00:29:34,060 --> 00:29:39,190
with kubernetes we will also offer

00:29:35,410 --> 00:29:41,380
official helm charts soon to make the

00:29:39,190 --> 00:29:43,110
integrations with the dr kubernetes

00:29:41,380 --> 00:29:45,910
ecosystem better in the future as well

00:29:43,110 --> 00:29:47,790
okay more stuff that you might want to

00:29:45,910 --> 00:29:53,080
keep in mind

00:29:47,790 --> 00:29:55,360
cute architectures hot warm cold anybody

00:29:53,080 --> 00:29:57,760
doing that for logging or anybody knows

00:29:55,360 --> 00:29:59,620
what they are okay that's very few

00:29:57,760 --> 00:30:01,720
people so what you generally want to do

00:29:59,620 --> 00:30:03,940
is when you collect logs and you have

00:30:01,720 --> 00:30:06,250
this daily index pattern for example all

00:30:03,940 --> 00:30:07,870
your write operations will go to today's

00:30:06,250 --> 00:30:10,000
index and you're not writing to

00:30:07,870 --> 00:30:12,580
yesterday's or a week ago cindex anymore

00:30:10,000 --> 00:30:14,140
so you might want to have different note

00:30:12,580 --> 00:30:17,680
types with different hardware

00:30:14,140 --> 00:30:19,660
architectures to make it cheaper so for

00:30:17,680 --> 00:30:21,490
today you might have faster notes or

00:30:19,660 --> 00:30:23,400
notes with fearing this s because you're

00:30:21,490 --> 00:30:25,600
doing a lot of write operations and

00:30:23,400 --> 00:30:28,690
oftentimes you also most researched

00:30:25,600 --> 00:30:30,340
today's log data and maybe yesterday's

00:30:28,690 --> 00:30:32,060
log data but very few people have

00:30:30,340 --> 00:30:34,460
searches that go back in

00:30:32,060 --> 00:30:36,770
so so you could have different tiers of

00:30:34,460 --> 00:30:38,600
architecture so you could have like hot

00:30:36,770 --> 00:30:40,850
would be like today's data where you

00:30:38,600 --> 00:30:43,310
write a lot and you search a lot then

00:30:40,850 --> 00:30:45,020
warm could be like yesterday to a week

00:30:43,310 --> 00:30:47,750
ago where you also do more or less

00:30:45,020 --> 00:30:50,870
frequent searches and then cold might be

00:30:47,750 --> 00:30:52,940
more than a week ago or older than a

00:30:50,870 --> 00:30:54,680
week because you very infrequently

00:30:52,940 --> 00:30:56,300
search those and most people will accept

00:30:54,680 --> 00:30:57,710
if you search for something very old you

00:30:56,300 --> 00:30:59,990
might have to wait a little longer and

00:30:57,710 --> 00:31:03,320
then you would have different types of

00:30:59,990 --> 00:31:05,480
hardware so very fast notes for hot with

00:31:03,320 --> 00:31:07,580
SSDs and maybe you have faster spinning

00:31:05,480 --> 00:31:09,200
discs on the warms and regular spinning

00:31:07,580 --> 00:31:10,760
discs on the cold nodes or whatever to

00:31:09,200 --> 00:31:12,740
keep your hardware costs down you have a

00:31:10,760 --> 00:31:14,380
multi tiered approach especially if you

00:31:12,740 --> 00:31:18,020
have a lot of log data and you have like

00:31:14,380 --> 00:31:21,700
50 nodes in an locking cluster you might

00:31:18,020 --> 00:31:24,290
want to have different tiers of hardware

00:31:21,700 --> 00:31:26,090
anybody knows how you actually put that

00:31:24,290 --> 00:31:28,160
together so what you normally or what

00:31:26,090 --> 00:31:30,560
you generally do is you say one node is

00:31:28,160 --> 00:31:32,060
for example you have an node attribute

00:31:30,560 --> 00:31:34,010
where you say this is a hot node a warm

00:31:32,060 --> 00:31:35,570
node or a code node for example and then

00:31:34,010 --> 00:31:38,090
on an index when you create the index

00:31:35,570 --> 00:31:40,010
you say this index needs to go to a hot

00:31:38,090 --> 00:31:41,870
node so when you create the index it

00:31:40,010 --> 00:31:44,300
will go to today's node and this is the

00:31:41,870 --> 00:31:46,730
hot node and tomorrow you change that

00:31:44,300 --> 00:31:49,100
attribute and say this index now needs

00:31:46,730 --> 00:31:51,350
to go to a warm node and after a week

00:31:49,100 --> 00:31:53,210
you'd say this needs to go to a cold

00:31:51,350 --> 00:31:56,930
node an elastic search based on that tag

00:31:53,210 --> 00:31:59,960
will move the the indices around or the

00:31:56,930 --> 00:32:03,260
shards around accordingly and you will

00:31:59,960 --> 00:32:05,360
also soon be able to manage that in

00:32:03,260 --> 00:32:07,790
Cabana we call that thing index

00:32:05,360 --> 00:32:08,810
lifecycle management and that will make

00:32:07,790 --> 00:32:12,430
your life a lot easier

00:32:08,810 --> 00:32:15,470
anybody using a curator elastic curator

00:32:12,430 --> 00:32:17,090
to tend to your indices it's not a

00:32:15,470 --> 00:32:19,160
hundred percent replacement but it is

00:32:17,090 --> 00:32:21,290
something that will take over a lot of

00:32:19,160 --> 00:32:23,120
what curator is doing so it's basically

00:32:21,290 --> 00:32:24,530
running as part of the elastic search

00:32:23,120 --> 00:32:27,800
cluster and you can manage it through

00:32:24,530 --> 00:32:30,620
kabandha so what you get is you create

00:32:27,800 --> 00:32:32,900
for example a policy and you say like we

00:32:30,620 --> 00:32:36,170
have a hot face this is today's data for

00:32:32,900 --> 00:32:38,330
example and we enable rollover and we

00:32:36,170 --> 00:32:40,100
want to have an index maximum size of

00:32:38,330 --> 00:32:44,420
three gigabytes

00:32:40,100 --> 00:32:45,920
who knows what rollover is okay

00:32:44,420 --> 00:32:47,960
rollover is

00:32:45,920 --> 00:32:50,180
helpful because maybe you have very

00:32:47,960 --> 00:32:52,250
different amounts of log data on

00:32:50,180 --> 00:32:54,140
different days of the week so Monday to

00:32:52,250 --> 00:32:55,970
Friday you might have 50 gigs of logs

00:32:54,140 --> 00:32:58,880
each day and say that they Sunday you

00:32:55,970 --> 00:33:01,160
only have one gigabyte of data and then

00:32:58,880 --> 00:33:03,620
it's kind of wasteful to always create

00:33:01,160 --> 00:33:06,050
let's say three charts for every day

00:33:03,620 --> 00:33:08,480
because Saturday and Sundays data has

00:33:06,050 --> 00:33:10,220
too many charts and there is no really

00:33:08,480 --> 00:33:11,630
good way around to do that because it

00:33:10,220 --> 00:33:13,970
with a daily in X pattern and you say

00:33:11,630 --> 00:33:15,860
always create that many charts there's

00:33:13,970 --> 00:33:18,140
no good way around that with rollover

00:33:15,860 --> 00:33:20,960
basically you can say once you have your

00:33:18,140 --> 00:33:23,090
index has reached three gigabytes create

00:33:20,960 --> 00:33:25,280
a new index and write all the subsequent

00:33:23,090 --> 00:33:27,260
data to that new index and it will

00:33:25,280 --> 00:33:30,020
basically always create an index with a

00:33:27,260 --> 00:33:30,830
certain amount of size regardless of how

00:33:30,020 --> 00:33:32,600
long that takes

00:33:30,830 --> 00:33:34,100
so it could take like three hours until

00:33:32,600 --> 00:33:36,230
you create in your index or it could

00:33:34,100 --> 00:33:39,140
take 48 hours until you create that new

00:33:36,230 --> 00:33:41,330
index it will just know and based on

00:33:39,140 --> 00:33:44,990
that rule it will create the right

00:33:41,330 --> 00:33:48,680
indices next up we could have the warm

00:33:44,990 --> 00:33:51,230
phase where after zero days we basically

00:33:48,680 --> 00:33:53,090
create a rule when we say ok we want to

00:33:51,230 --> 00:33:55,220
move that to the different note type

00:33:53,090 --> 00:33:57,830
which is the norm at the OneNote and

00:33:55,220 --> 00:34:00,080
also we want to shrink down the number

00:33:57,830 --> 00:34:02,510
of shots so basically or for example we

00:34:00,080 --> 00:34:05,060
go from three shots to one shot when we

00:34:02,510 --> 00:34:07,520
move the data over to limit the number

00:34:05,060 --> 00:34:09,500
of shots and do that you could also do a

00:34:07,520 --> 00:34:12,200
forced merge or do various other steps

00:34:09,500 --> 00:34:14,720
and then at the end you could have that

00:34:12,200 --> 00:34:15,830
cold phase where you could activate here

00:34:14,720 --> 00:34:17,540
it's not activated but you could

00:34:15,830 --> 00:34:20,150
activate the cold phase and then we have

00:34:17,540 --> 00:34:21,980
a delete phase after X days you could

00:34:20,150 --> 00:34:24,350
just delete your data or snapshot it

00:34:21,980 --> 00:34:32,210
before you delete it or whatever so that

00:34:24,350 --> 00:34:37,360
will make that a lot easier yeah but

00:34:32,210 --> 00:34:40,720
whenever telling release dates so so I

00:34:37,360 --> 00:34:44,900
had hoped to be able to demo it already

00:34:40,720 --> 00:34:46,640
it slipped twice now unfortunately so

00:34:44,900 --> 00:34:49,970
what I can tell you is that it's not in

00:34:46,640 --> 00:34:52,100
65 yet but we hope to have it soon I

00:34:49,970 --> 00:34:55,429
think that's already pretty specific so

00:34:52,100 --> 00:34:57,770
it's unfortunately not in six five which

00:34:55,429 --> 00:34:59,780
is this one here but you get lots of

00:34:57,770 --> 00:35:02,420
other toys in six five already I will

00:34:59,780 --> 00:35:05,420
another slide on that index lifecycle

00:35:02,420 --> 00:35:07,730
management will be but it will be next

00:35:05,420 --> 00:35:10,070
year it will not be out this year we

00:35:07,730 --> 00:35:12,590
hope early next year but you know no

00:35:10,070 --> 00:35:16,430
promises you will have to wait a little

00:35:12,590 --> 00:35:17,930
longer then we have another thing called

00:35:16,430 --> 00:35:20,440
frozen indices are we running out of

00:35:17,930 --> 00:35:24,530
time okay I'm done in three slides

00:35:20,440 --> 00:35:26,360
anybody heard of frozen indices frozen

00:35:24,530 --> 00:35:29,180
illnesses are kind of a new thing but we

00:35:26,360 --> 00:35:33,020
want to increase the ratio of disk size

00:35:29,180 --> 00:35:35,060
to heap it will play some tricks on how

00:35:33,020 --> 00:35:37,250
many threads we have searching new data

00:35:35,060 --> 00:35:39,170
and it will be much slower but you will

00:35:37,250 --> 00:35:41,720
have a much higher storage density on a

00:35:39,170 --> 00:35:44,060
per node level so basically you will be

00:35:41,720 --> 00:35:46,970
able to pack more data on a single node

00:35:44,060 --> 00:35:50,120
because depending on how far you push it

00:35:46,970 --> 00:35:52,880
maybe you have two to five terabytes of

00:35:50,120 --> 00:35:54,590
data maximum on one node and the idea of

00:35:52,880 --> 00:35:57,050
frozen in SS is to increase that size

00:35:54,590 --> 00:35:59,120
that the storage density is higher on

00:35:57,050 --> 00:36:03,740
the preneur level there is an open issue

00:35:59,120 --> 00:36:05,300
and it's I think it's also about to be

00:36:03,740 --> 00:36:08,360
merged or has already been merged so

00:36:05,300 --> 00:36:11,510
that's already pretty far along you have

00:36:08,360 --> 00:36:12,830
the info you I and lock UI that I didn't

00:36:11,510 --> 00:36:14,660
really show in that example but we have

00:36:12,830 --> 00:36:18,260
more customized and tailored you guys

00:36:14,660 --> 00:36:19,430
for that and we will have in 6.5 there

00:36:18,260 --> 00:36:21,590
will be centralized Pietschmann

00:36:19,430 --> 00:36:24,200
management so so far you only have locks

00:36:21,590 --> 00:36:26,660
- centralized locks - management 6.5

00:36:24,200 --> 00:36:30,020
will bring the centralized beats

00:36:26,660 --> 00:36:31,520
management so I don't know I I like my

00:36:30,020 --> 00:36:32,720
configuration management I always feel

00:36:31,520 --> 00:36:33,920
that this should be a job of the

00:36:32,720 --> 00:36:35,750
configuration management you're using

00:36:33,920 --> 00:36:37,670
but some people really want to have that

00:36:35,750 --> 00:36:40,460
UI view where you put it into a central

00:36:37,670 --> 00:36:42,820
place and it will magically walk around

00:36:40,460 --> 00:36:45,950
your infrastructure any way to wrap up

00:36:42,820 --> 00:36:49,330
if you want to get the code I might push

00:36:45,950 --> 00:36:52,340
some more updates to unbreak my my demo

00:36:49,330 --> 00:36:54,020
this is where the Java example is it

00:36:52,340 --> 00:36:56,930
should be pretty easy to port that to

00:36:54,020 --> 00:36:58,820
any other programming language so we had

00:36:56,930 --> 00:37:00,080
parse if you'd like to write regular

00:36:58,820 --> 00:37:02,060
expressions but you can keep your

00:37:00,080 --> 00:37:03,230
replication the way it was you can send

00:37:02,060 --> 00:37:05,600
it for example with the log upended

00:37:03,230 --> 00:37:07,790
directed locks - problem is what if your

00:37:05,600 --> 00:37:09,620
network is down you can structure it if

00:37:07,790 --> 00:37:11,650
you can add that right lock appender and

00:37:09,620 --> 00:37:13,580
it will make your life easier afterwards

00:37:11,650 --> 00:37:16,100
containerized you can add

00:37:13,580 --> 00:37:18,560
metadata and you probably want to run

00:37:16,100 --> 00:37:21,170
file bit as a sidecar and to orchestrate

00:37:18,560 --> 00:37:22,850
you get metadata again and you can run

00:37:21,170 --> 00:37:24,650
it as a daemon set so here I have one

00:37:22,850 --> 00:37:29,870
instance per node to collect your logs

00:37:24,650 --> 00:37:39,110
and with that any questions any

00:37:29,870 --> 00:37:41,660
questions Thank You Philip questions I

00:37:39,110 --> 00:37:44,480
need Java developers in room yeah it

00:37:41,660 --> 00:37:46,010
doesn't have to be Java I take on by the

00:37:44,480 --> 00:37:47,450
way before you run off let me take a

00:37:46,010 --> 00:37:49,450
quick picture so I can prove to my

00:37:47,450 --> 00:37:51,710
colleagues that I've been working

00:37:49,450 --> 00:37:55,820
because they don't know where I am

00:37:51,710 --> 00:37:58,400
normally a smile everybody okay cool any

00:37:55,820 --> 00:38:09,530
questions now or did anybody post any

00:37:58,400 --> 00:38:13,250
question online no yes wait let's go to

00:38:09,530 --> 00:38:16,640
questions our first name is Esther or do

00:38:13,250 --> 00:38:20,510
they have to be thought first no they

00:38:16,640 --> 00:38:27,890
should be searchable they will just be

00:38:20,510 --> 00:38:29,270
searchable slower there right now we

00:38:27,890 --> 00:38:32,210
have that concept that you can close an

00:38:29,270 --> 00:38:34,100
index then it's not searchable the one

00:38:32,210 --> 00:38:36,320
downside of a closed index is that if a

00:38:34,100 --> 00:38:37,790
node dies we will not replicate it again

00:38:36,320 --> 00:38:39,590
to another node but that will probably

00:38:37,790 --> 00:38:41,480
change in the future so that the kind of

00:38:39,590 --> 00:38:42,800
that the differentiator is you have a

00:38:41,480 --> 00:38:45,290
closed index which is not searchable

00:38:42,800 --> 00:38:47,210
that would need to be kind of fun and

00:38:45,290 --> 00:38:49,910
closed or opened again then it's

00:38:47,210 --> 00:38:52,240
searchable the frozen illnesses are

00:38:49,910 --> 00:38:55,130
searchable but just searchable slower

00:38:52,240 --> 00:38:56,840
and by the way we have another thing

00:38:55,130 --> 00:38:58,280
that comes out that will make your

00:38:56,840 --> 00:39:01,370
snapshot smaller

00:38:58,280 --> 00:39:03,770
it's called source only snapshots

00:39:01,370 --> 00:39:05,120
because right now your snapshots contain

00:39:03,770 --> 00:39:06,980
basically all the information that you

00:39:05,120 --> 00:39:08,570
have in your index and you have all the

00:39:06,980 --> 00:39:10,850
inverted indices and everything in your

00:39:08,570 --> 00:39:13,820
snapshots there will be a new option

00:39:10,850 --> 00:39:16,820
where you basically only snapshot out

00:39:13,820 --> 00:39:18,530
the raw JSON data that you have put into

00:39:16,820 --> 00:39:20,270
your cluster which will have very

00:39:18,530 --> 00:39:22,130
limited search features when you re

00:39:20,270 --> 00:39:24,110
import them and you would need to do

00:39:22,130 --> 00:39:27,200
every index but you can kind of keep a

00:39:24,110 --> 00:39:30,890
more compressed or limited snapshot of

00:39:27,200 --> 00:39:39,100
your data in the future as well any

00:39:30,890 --> 00:39:45,090
other questions okay thank you thank you

00:39:39,100 --> 00:39:56,889
[Applause]

00:39:45,090 --> 00:39:56,889

YouTube URL: https://www.youtube.com/watch?v=B1nL9ZFTYBI


