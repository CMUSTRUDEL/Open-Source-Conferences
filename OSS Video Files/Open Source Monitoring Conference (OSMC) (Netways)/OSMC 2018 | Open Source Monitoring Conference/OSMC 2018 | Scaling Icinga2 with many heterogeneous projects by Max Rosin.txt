Title: OSMC 2018 | Scaling Icinga2 with many heterogeneous projects by Max Rosin
Publication date: 2018-11-16
Playlist: OSMC 2018 | Open Source Monitoring Conference
Description: 
	The main objective of the talk is to give a detailed real world example how we use Icinga 2 at a large scale with all its pros and cons. SysEleven monitors several hundred heterogeneous projects. To migrate our Icinga 1 setup to a high available Icinga 2 setup we developed icingadiff. The new cluster is fully automated with Puppet, deploys over 60000 checks and enables our engineers to fine tune every check if necessary. To integrate further information and custom workflows we modified Icingaweb2.

NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de/
Blog: http://blog.netways.de/
NWS: https://nws.netways.de

Webinare
Archiv Link: https://www.netways.de/webinare/archi...
Aktuell: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh/

www.musicfox.com
Captions: 
	00:00:01,940 --> 00:00:13,270
[Music]

00:00:15,170 --> 00:00:20,699
thank you and good morning I'm max I'm

00:00:18,240 --> 00:00:23,730
here today to speak about the topic

00:00:20,699 --> 00:00:25,470
scaling signatu with many Hitler genius

00:00:23,730 --> 00:00:27,810
projects and still preserving

00:00:25,470 --> 00:00:30,359
configurability it's a hard to pronounce

00:00:27,810 --> 00:00:31,610
title and it's a complicated title but

00:00:30,359 --> 00:00:36,899
it's also for a complicated topic

00:00:31,610 --> 00:00:39,090
because scaling stuff is hard and so I'm

00:00:36,899 --> 00:00:41,820
here for this 11 we are managed hosting

00:00:39,090 --> 00:00:44,070
company from Berlin additionally we have

00:00:41,820 --> 00:00:46,950
an infrastructure as a service offering

00:00:44,070 --> 00:00:48,829
and managed kubernetes on top of it and

00:00:46,950 --> 00:00:51,890
we are also an Internet service provider

00:00:48,829 --> 00:00:54,239
but everything I tell you today about is

00:00:51,890 --> 00:00:57,300
about the managed hosting part of our

00:00:54,239 --> 00:01:00,210
company so we we are using signatu now a

00:00:57,300 --> 00:01:01,980
managed hosting apartment and I'm max

00:01:00,210 --> 00:01:04,769
I'm responsible for everything

00:01:01,980 --> 00:01:07,200
monitoring related inside of the managed

00:01:04,769 --> 00:01:09,659
hosting part if you want to follow me on

00:01:07,200 --> 00:01:12,240
Twitter or github my online Nick is eco

00:01:09,659 --> 00:01:15,479
I feel free to tweet about this talk if

00:01:12,240 --> 00:01:17,220
you want to before I really start I want

00:01:15,479 --> 00:01:20,729
to give you some context why I'm here

00:01:17,220 --> 00:01:23,970
today so why am I telling you all this

00:01:20,729 --> 00:01:26,729
in April I attended the Sigma camp in

00:01:23,970 --> 00:01:29,939
Berlin and it was a nice event where

00:01:26,729 --> 00:01:31,670
interesting people had nice talks but at

00:01:29,939 --> 00:01:34,560
the end of the day I was disappointed

00:01:31,670 --> 00:01:36,780
not about the event itself but I was not

00:01:34,560 --> 00:01:38,790
able to meet people with the same issues

00:01:36,780 --> 00:01:42,450
that we have with our I singer too

00:01:38,790 --> 00:01:44,399
and after that I decided ok if I'm not

00:01:42,450 --> 00:01:47,100
able to find people with our problems

00:01:44,399 --> 00:01:49,140
maybe I should talk about our problems

00:01:47,100 --> 00:01:51,600
and our solutions and our workarounds

00:01:49,140 --> 00:01:54,659
give you some real-world examples our

00:01:51,600 --> 00:01:56,219
irani singer too and then maybe some

00:01:54,659 --> 00:01:59,009
people will start to share their own

00:01:56,219 --> 00:02:00,990
stories so I'm not like only here to

00:01:59,009 --> 00:02:03,360
share our knowledge and also here to

00:02:00,990 --> 00:02:06,899
inspire you to share your experiences

00:02:03,360 --> 00:02:08,520
with singer too few words about our

00:02:06,899 --> 00:02:11,520
infrastructure we have two data centers

00:02:08,520 --> 00:02:14,460
in Berlin and we are monitoring around

00:02:11,520 --> 00:02:17,280
3000 virtualized hosts they are all

00:02:14,460 --> 00:02:22,110
based on Linux so mostly Ubuntu or

00:02:17,280 --> 00:02:25,620
gentle and very few window servers we

00:02:22,110 --> 00:02:27,600
are checking 60000 checks and all our

00:02:25,620 --> 00:02:31,350
checks are distributed in several

00:02:27,600 --> 00:02:33,690
hundred customer projects and the

00:02:31,350 --> 00:02:36,180
projects for our customers are similar

00:02:33,690 --> 00:02:38,010
but every every setup is a little bit

00:02:36,180 --> 00:02:40,920
different so we have a lot of custom

00:02:38,010 --> 00:02:43,410
checks for our different customers which

00:02:40,920 --> 00:02:45,900
is pretty important for the job title

00:02:43,410 --> 00:02:48,150
part about configurability because we

00:02:45,900 --> 00:02:51,960
need to change stuff for different

00:02:48,150 --> 00:02:55,140
customers so in january we went live

00:02:51,960 --> 00:02:57,450
with our signals who set up before we

00:02:55,140 --> 00:03:00,180
migrated our alt infrastructure to it

00:02:57,450 --> 00:03:02,280
and the migration took us about half a

00:03:00,180 --> 00:03:06,450
year and part of the presentation will

00:03:02,280 --> 00:03:09,780
be the process of migrating our setup so

00:03:06,450 --> 00:03:11,730
I promised you a real world example that

00:03:09,780 --> 00:03:13,560
includes technical depth I guess every

00:03:11,730 --> 00:03:16,380
one of you has technical depth in your

00:03:13,560 --> 00:03:18,930
infrastructure so before we had a

00:03:16,380 --> 00:03:21,780
signatu we had a single one and before

00:03:18,930 --> 00:03:24,360
that we had Nagas and we started with

00:03:21,780 --> 00:03:27,090
one mega server in a small setup with

00:03:24,360 --> 00:03:29,010
only a few engineers and then Susie

00:03:27,090 --> 00:03:31,370
Levin started to grow so in the end we

00:03:29,010 --> 00:03:34,350
had six independent signal one service

00:03:31,370 --> 00:03:36,330
they are monitoring each other to make

00:03:34,350 --> 00:03:38,160
sure that if one server fails we at

00:03:36,330 --> 00:03:40,590
least would at least know that one of

00:03:38,160 --> 00:03:43,440
our monitoring service failed but there

00:03:40,590 --> 00:03:46,830
was no automated failover to another

00:03:43,440 --> 00:03:50,550
server the configuration was shared by a

00:03:46,830 --> 00:03:52,380
subversion repository and it was

00:03:50,550 --> 00:03:54,780
synchronized by cron jobs to make sure

00:03:52,380 --> 00:03:58,950
that our monitoring infrastructure of us

00:03:54,780 --> 00:04:00,630
up to date and our engineers if they

00:03:58,950 --> 00:04:02,700
wanted to make a change there connected

00:04:00,630 --> 00:04:05,250
to the life environment changed

00:04:02,700 --> 00:04:07,200
something they really order T Sigma and

00:04:05,250 --> 00:04:09,959
production and hope that it still worked

00:04:07,200 --> 00:04:11,580
and well I think if you want a

00:04:09,959 --> 00:04:14,850
definition of technically that that is

00:04:11,580 --> 00:04:16,770
one and yeah

00:04:14,850 --> 00:04:18,900
in an ideal world our engineers would

00:04:16,770 --> 00:04:20,640
make a subversion commit so we would

00:04:18,900 --> 00:04:23,250
have an idea who changed what and why

00:04:20,640 --> 00:04:24,960
but most of the time there would be a

00:04:23,250 --> 00:04:26,910
cron job to just commit all

00:04:24,960 --> 00:04:30,690
changes and then synchronize them to our

00:04:26,910 --> 00:04:33,780
service now what that was not idea it

00:04:30,690 --> 00:04:36,360
was okay when we had like three

00:04:33,780 --> 00:04:39,030
engineers and 20 servers but definitely

00:04:36,360 --> 00:04:41,789
not met 30 engineers then you just don't

00:04:39,030 --> 00:04:43,710
know who changed what then he singing

00:04:41,789 --> 00:04:45,810
one announced the end of life for the

00:04:43,710 --> 00:04:48,210
end of the year so one reason more to

00:04:45,810 --> 00:04:50,039
migrate to another solution and we had

00:04:48,210 --> 00:04:53,819
an interesting issue with a signal one

00:04:50,039 --> 00:04:56,340
it looked like that that the average

00:04:53,819 --> 00:04:59,130
check latency in our a single one set up

00:04:56,340 --> 00:05:01,169
and despite the fact that the phones are

00:04:59,130 --> 00:05:03,810
very small the interesting part is the

00:05:01,169 --> 00:05:06,930
Green Line it's at three minutes here

00:05:03,810 --> 00:05:08,639
it's a picture I could find from our

00:05:06,930 --> 00:05:12,419
previous set up but we had days when

00:05:08,639 --> 00:05:15,330
this was at 50 minutes or 20 minutes and

00:05:12,419 --> 00:05:18,030
it means that we had 20 minutes latency

00:05:15,330 --> 00:05:20,639
under we actually saw an issue in our

00:05:18,030 --> 00:05:22,680
infrastructure so if your monitoring is

00:05:20,639 --> 00:05:24,449
delayed 20 minutes you don't need a

00:05:22,680 --> 00:05:28,440
monitoring because you will have

00:05:24,449 --> 00:05:30,870
lawsuits all over the place and so

00:05:28,440 --> 00:05:33,509
luckily we found this very early and we

00:05:30,870 --> 00:05:34,979
were able to debug it and we found

00:05:33,509 --> 00:05:37,949
workarounds because we are good at

00:05:34,979 --> 00:05:40,289
finding workarounds so we are still

00:05:37,949 --> 00:05:43,469
about technical depth technical depth

00:05:40,289 --> 00:05:46,110
can look like that in the beginning it

00:05:43,469 --> 00:05:49,800
was a brash one-liner in our internal

00:05:46,110 --> 00:05:52,409
documentation and if you think about

00:05:49,800 --> 00:05:55,440
maintainable software that's not it

00:05:52,409 --> 00:05:57,840
it takes the 60 worst delay checks any

00:05:55,440 --> 00:06:01,199
single one and then forces is single one

00:05:57,840 --> 00:06:05,639
to recheck then it looks crazy it sounds

00:06:01,199 --> 00:06:07,520
crazy but it works it fixed our signal

00:06:05,639 --> 00:06:10,139
one issue in a way that we were able to

00:06:07,520 --> 00:06:14,400
run issing a one for a few more months

00:06:10,139 --> 00:06:16,020
and that way we had any latency in our

00:06:14,400 --> 00:06:18,930
infrastructure so we caught at the

00:06:16,020 --> 00:06:22,250
monitoring we needed to have to provide

00:06:18,930 --> 00:06:23,940
our managed hosting to our customers so

00:06:22,250 --> 00:06:26,310
that's the real world

00:06:23,940 --> 00:06:30,120
I guess everyone has similar experiences

00:06:26,310 --> 00:06:31,680
in larger infrastructures based on that

00:06:30,120 --> 00:06:34,830
we had a few requirements for new

00:06:31,680 --> 00:06:38,099
monitoring solution 100% automation I

00:06:34,830 --> 00:06:38,520
mean it's 2018 we don't want menu a

00:06:38,099 --> 00:06:41,400
chain

00:06:38,520 --> 00:06:43,830
no monitoring setup we also wanted to be

00:06:41,400 --> 00:06:46,410
high available so no six independent

00:06:43,830 --> 00:06:49,530
servers running somewhere we wanted one

00:06:46,410 --> 00:06:51,539
cluster with automatic failover we

00:06:49,530 --> 00:06:53,909
wanted to have stage setups no engineer

00:06:51,539 --> 00:06:58,500
should change the production why I meant

00:06:53,909 --> 00:07:01,110
to test if the idea works also wanted it

00:06:58,500 --> 00:07:03,990
to be maintainable so no crazy bash

00:07:01,110 --> 00:07:06,659
one-liners and we wanted a full state

00:07:03,990 --> 00:07:08,580
migration when we decided that we needed

00:07:06,659 --> 00:07:11,159
a full statement Gration we knew that

00:07:08,580 --> 00:07:13,139
this part would be hard but I think no

00:07:11,159 --> 00:07:15,479
one of us anticipated how hard it would

00:07:13,139 --> 00:07:19,110
be to do a full statement raishin of

00:07:15,479 --> 00:07:21,090
60,000 checks from six service so for

00:07:19,110 --> 00:07:23,940
state migration means that we have a lot

00:07:21,090 --> 00:07:27,560
of runtime changes any single one like

00:07:23,940 --> 00:07:30,150
acknowledged alerts comments down times

00:07:27,560 --> 00:07:32,759
yeah disabled active checks and stuff

00:07:30,150 --> 00:07:34,620
like that it was changed in signal 1 and

00:07:32,759 --> 00:07:36,240
we decided that we needed to migrate all

00:07:34,620 --> 00:07:38,330
of that because we had thousands of

00:07:36,240 --> 00:07:42,840
modifications in the running instance

00:07:38,330 --> 00:07:44,610
and so we needed to way - yeah migrate

00:07:42,840 --> 00:07:47,310
that and later on I will show you some

00:07:44,610 --> 00:07:49,380
ways we were able to compare running a

00:07:47,310 --> 00:07:51,690
signal one to running a signatu set up

00:07:49,380 --> 00:07:55,469
to make sure that we are not losing any

00:07:51,690 --> 00:07:58,590
checks or acknowledgments but first I

00:07:55,469 --> 00:08:01,110
will talk about 100% automation so how

00:07:58,590 --> 00:08:03,029
do we configure Reisinger - that's

00:08:01,110 --> 00:08:04,949
always a question I have for other

00:08:03,029 --> 00:08:06,659
engineers like how do you deploy your

00:08:04,949 --> 00:08:09,900
configuration and signa - because

00:08:06,659 --> 00:08:12,120
there's one way manual editing then

00:08:09,900 --> 00:08:14,310
there's a second way it's a Sangha

00:08:12,120 --> 00:08:16,529
director and then there are hundred

00:08:14,310 --> 00:08:18,900
other ways only a few people seem to

00:08:16,529 --> 00:08:22,229
talk about and we have one of the other

00:08:18,900 --> 00:08:25,770
ways our complete infrastructure is

00:08:22,229 --> 00:08:28,380
configured by puppet so obviously we are

00:08:25,770 --> 00:08:32,789
using pepper to deploy and configure a

00:08:28,380 --> 00:08:34,620
signatu there are two upstream puppet

00:08:32,789 --> 00:08:36,570
modules for missing nets a puppet a

00:08:34,620 --> 00:08:38,219
signatu and puppet escena web - we are

00:08:36,570 --> 00:08:41,399
using both of them and they are pretty

00:08:38,219 --> 00:08:44,640
awesome we've wrote our own puppet wrap

00:08:41,399 --> 00:08:47,699
around them it's 11e signal - and it

00:08:44,640 --> 00:08:50,490
runs on all our monitoring server so

00:08:47,699 --> 00:08:51,650
signal master satellite signal web

00:08:50,490 --> 00:08:53,690
signal database

00:08:51,650 --> 00:08:55,940
we are using this module on those

00:08:53,690 --> 00:08:58,610
servers it will tell you a little bit

00:08:55,940 --> 00:09:01,339
more about data we brought a second part

00:08:58,610 --> 00:09:03,860
modules this 11 monitoring and we are

00:09:01,339 --> 00:09:06,260
using this module on all servers we want

00:09:03,860 --> 00:09:08,839
to have in our monitoring so we're using

00:09:06,260 --> 00:09:12,170
this puppet module on all three thousand

00:09:08,839 --> 00:09:14,330
servers and it is responsible to

00:09:12,170 --> 00:09:18,050
register every house to normal

00:09:14,330 --> 00:09:20,510
monitoring then I said we wanted to

00:09:18,050 --> 00:09:22,700
maintain configurability this means we

00:09:20,510 --> 00:09:24,410
need to away for our engineer to change

00:09:22,700 --> 00:09:27,290
settings in our monitoring environment

00:09:24,410 --> 00:09:29,660
without actually changing our monitoring

00:09:27,290 --> 00:09:31,339
environment and production so we are

00:09:29,660 --> 00:09:33,410
also using puppet for that and we are

00:09:31,339 --> 00:09:35,210
using Huayra if you are familiar with

00:09:33,410 --> 00:09:38,630
puppet is anyone familiar with puppet

00:09:35,210 --> 00:09:42,230
didn't ok so for the one who are not

00:09:38,630 --> 00:09:44,870
familiar with it IRA is like a key value

00:09:42,230 --> 00:09:47,089
store in Yama files so we are having a

00:09:44,870 --> 00:09:49,460
good repository here as fires for every

00:09:47,089 --> 00:09:51,890
customer and every host and you can

00:09:49,460 --> 00:09:53,779
change settings in those Yama files then

00:09:51,890 --> 00:09:57,080
commit them into the git repository and

00:09:53,779 --> 00:09:59,720
they will be used by puppet to deploy

00:09:57,080 --> 00:10:01,580
our configuration this is not monitoring

00:09:59,720 --> 00:10:06,589
specific we are using that for our

00:10:01,580 --> 00:10:08,029
complete puppet infrastructure ok so if

00:10:06,589 --> 00:10:11,360
you are not familiar with puppet puppet

00:10:08,029 --> 00:10:13,730
s1 advantage compared with tools like in

00:10:11,360 --> 00:10:16,550
Cebu it has a persistent state across

00:10:13,730 --> 00:10:18,320
all your puppet runs so you have the

00:10:16,550 --> 00:10:21,740
puppet database that stores information

00:10:18,320 --> 00:10:24,500
about your puppet environment and on the

00:10:21,740 --> 00:10:27,740
left side you can see all our servers we

00:10:24,500 --> 00:10:29,690
want to monitor so that's like 3,000

00:10:27,740 --> 00:10:31,910
servers that are using the system level

00:10:29,690 --> 00:10:35,270
monitoring module to export resources

00:10:31,910 --> 00:10:37,910
into the pop database and then later we

00:10:35,270 --> 00:10:39,860
are using the sizzle Evony signatu

00:10:37,910 --> 00:10:42,589
module on how a single master to

00:10:39,860 --> 00:10:45,380
actually import those raise resources so

00:10:42,589 --> 00:10:47,150
what's the resource for example we are

00:10:45,380 --> 00:10:50,630
using the upstream signature puppet

00:10:47,150 --> 00:10:53,690
module it exports a host resource that

00:10:50,630 --> 00:10:57,140
is that represents one of our monitored

00:10:53,690 --> 00:11:00,920
hosts and puppet adds a few variables to

00:10:57,140 --> 00:11:04,310
it to announce what checks are necessary

00:11:00,920 --> 00:11:05,329
on this host so we are able to run

00:11:04,310 --> 00:11:08,209
puppet on every

00:11:05,329 --> 00:11:11,839
most registers the host and announces

00:11:08,209 --> 00:11:13,759
the necessary services I will tell you

00:11:11,839 --> 00:11:16,309
more about that part because that's an

00:11:13,759 --> 00:11:19,420
interesting one we are able to configure

00:11:16,309 --> 00:11:23,360
all the settings we need for this host

00:11:19,420 --> 00:11:25,549
so if you are deploying 60,000 checks

00:11:23,360 --> 00:11:27,439
you need some same defaults you don't

00:11:25,549 --> 00:11:30,259
want your engineers to configure every

00:11:27,439 --> 00:11:32,749
check manually in some way so we are

00:11:30,259 --> 00:11:35,059
using using other puppet modules for

00:11:32,749 --> 00:11:36,799
other software infrastructure for

00:11:35,059 --> 00:11:39,470
example we have a lot of database

00:11:36,799 --> 00:11:42,049
service based on my squared so we have a

00:11:39,470 --> 00:11:45,019
mile scale puppet module this is a

00:11:42,049 --> 00:11:46,999
snippet of puppet code and at the bottom

00:11:45,019 --> 00:11:49,129
you can see that we are using the city

00:11:46,999 --> 00:11:51,829
level monitoring module inside of our

00:11:49,129 --> 00:11:55,040
MySQL module and we are calling a

00:11:51,829 --> 00:12:00,290
function register variable like record

00:11:55,040 --> 00:12:03,410
string variable my scanner DB so this

00:12:00,290 --> 00:12:05,899
way the nice creme module announces I

00:12:03,410 --> 00:12:09,410
want to have a MySQL in your DB check

00:12:05,899 --> 00:12:11,509
and in this way all of our module a

00:12:09,410 --> 00:12:15,049
puppet modules are using sea level

00:12:11,509 --> 00:12:18,410
monitoring to register their own checks

00:12:15,049 --> 00:12:20,959
so if we have a service a host that is

00:12:18,410 --> 00:12:23,509
newly created we have about fifteen

00:12:20,959 --> 00:12:26,360
default checks and as soon as you are

00:12:23,509 --> 00:12:29,720
using using puppet to deploy most

00:12:26,360 --> 00:12:33,169
services you have like 30 or 35 checks

00:12:29,720 --> 00:12:35,360
on every host with default settings so

00:12:33,169 --> 00:12:37,249
engineers check the software and said

00:12:35,360 --> 00:12:40,220
okay we are needing those thresholds for

00:12:37,249 --> 00:12:43,730
the service we are deploying this our

00:12:40,220 --> 00:12:45,679
default setup that already works pretty

00:12:43,730 --> 00:12:48,439
great but as I said we have different

00:12:45,679 --> 00:12:51,290
customers and every customer has some

00:12:48,439 --> 00:12:54,230
needs to change stuff so we wanted to

00:12:51,290 --> 00:12:57,220
override our same defaults you're using

00:12:54,230 --> 00:13:00,829
Huayra for that so this is a Yama file

00:12:57,220 --> 00:13:03,529
and again we are using sis 11 monitoring

00:13:00,829 --> 00:13:06,290
and then we have this map of settings

00:13:03,529 --> 00:13:09,439
and here we are over writing our mysqldb

00:13:06,290 --> 00:13:12,110
check and we define another value for

00:13:09,439 --> 00:13:15,350
max check attempts retry interval or who

00:13:12,110 --> 00:13:17,419
we want to notify based on that you are

00:13:15,350 --> 00:13:18,950
able to change every setting you can

00:13:17,419 --> 00:13:22,220
change for any signal to check

00:13:18,950 --> 00:13:24,380
so this could like 50 options but it's

00:13:22,220 --> 00:13:26,959
also okay to only define one define one

00:13:24,380 --> 00:13:29,870
option you want to change that is really

00:13:26,959 --> 00:13:32,029
powerful because every one of our

00:13:29,870 --> 00:13:34,850
engineers is able to edit the siamo

00:13:32,029 --> 00:13:37,550
files and then just push them into a git

00:13:34,850 --> 00:13:39,920
repository and it will change the

00:13:37,550 --> 00:13:43,940
monitoring for this specific host or for

00:13:39,920 --> 00:13:45,740
this specific service so in the daily

00:13:43,940 --> 00:13:48,019
business if an engineer wants to change

00:13:45,740 --> 00:13:51,620
our monitoring all he or she has to do

00:13:48,019 --> 00:13:54,260
is to edit Yama file not writing puppet

00:13:51,620 --> 00:13:56,449
code or anything fancy just a gamify and

00:13:54,260 --> 00:13:58,820
then there are a lot of sanity checks to

00:13:56,449 --> 00:14:02,810
make sure that the CNF I actually makes

00:13:58,820 --> 00:14:05,149
sense and then it is deployed so we are

00:14:02,810 --> 00:14:08,540
able to overwrite our same defaults with

00:14:05,149 --> 00:14:10,970
some custom options but what if we want

00:14:08,540 --> 00:14:12,949
to add additional checks we are web

00:14:10,970 --> 00:14:16,279
hosting company primaries so we have a

00:14:12,949 --> 00:14:18,529
lot of HTTP checks and there are no real

00:14:16,279 --> 00:14:20,480
default HTTP checks of course you can

00:14:18,529 --> 00:14:22,699
check that your apache or nginx works

00:14:20,480 --> 00:14:25,459
but usually you want to know if the

00:14:22,699 --> 00:14:28,040
application behind that works so you

00:14:25,459 --> 00:14:31,550
want to have string checks and every one

00:14:28,040 --> 00:14:35,060
of that is custom for a customer so we

00:14:31,550 --> 00:14:36,980
are able to modify all options that are

00:14:35,060 --> 00:14:39,230
necessary for an HTTP check if you want

00:14:36,980 --> 00:14:41,720
to but it would be also enough to just

00:14:39,230 --> 00:14:43,519
define a hostname or IP and maybe define

00:14:41,720 --> 00:14:46,459
a string that needs to be available in

00:14:43,519 --> 00:14:49,190
the response of the server so when one

00:14:46,459 --> 00:14:51,560
of our engineers starts to build a new

00:14:49,190 --> 00:14:54,649
customer set up he or she can easily add

00:14:51,560 --> 00:14:59,000
a few HTTP checks and there's a basic

00:14:54,649 --> 00:15:01,850
monitoring for the web application you

00:14:59,000 --> 00:15:05,209
don't only have HTTP checks we also have

00:15:01,850 --> 00:15:05,750
NRP checks they are deployed in a

00:15:05,209 --> 00:15:07,940
similar way

00:15:05,750 --> 00:15:10,579
Cecille have monitoring modules used

00:15:07,940 --> 00:15:13,339
again inside of a unifier still inside

00:15:10,579 --> 00:15:16,100
of the git repository and just by that

00:15:13,339 --> 00:15:18,430
it will it will be deployed on the

00:15:16,100 --> 00:15:20,990
customer server and a normal monitoring

00:15:18,430 --> 00:15:23,720
so that is how we maintain

00:15:20,990 --> 00:15:28,040
configurability across all those

00:15:23,720 --> 00:15:30,949
different customer setups that's an yeah

00:15:28,040 --> 00:15:32,400
a summary of our export strategy

00:15:30,949 --> 00:15:35,640
strategy into

00:15:32,400 --> 00:15:38,340
pappa database of course we also need to

00:15:35,640 --> 00:15:42,050
import them on in our icing setup

00:15:38,340 --> 00:15:45,330
there's one issue it's quite complicated

00:15:42,050 --> 00:15:47,700
and we are using the upstream using a 2

00:15:45,330 --> 00:15:49,620
module for that and when we started to

00:15:47,700 --> 00:15:53,730
use it every puppet run would take about

00:15:49,620 --> 00:15:55,950
15 minutes and a usual puppet run in our

00:15:53,730 --> 00:15:58,950
infrastructure takes about 5 maybe 10

00:15:55,950 --> 00:16:01,650
seconds tops so 15 minutes is not ideal

00:15:58,950 --> 00:16:04,650
especially as we decided to execute

00:16:01,650 --> 00:16:07,110
Peppard in our a single setup every 30

00:16:04,650 --> 00:16:09,320
minutes to make sure that when a

00:16:07,110 --> 00:16:12,090
configuration is updated soon enough so

00:16:09,320 --> 00:16:15,540
15 minutes is hard when you want to test

00:16:12,090 --> 00:16:18,330
something so we spent about a week to

00:16:15,540 --> 00:16:21,150
profile our puppet server and upstream

00:16:18,330 --> 00:16:23,910
missing a 2 module we found out how it

00:16:21,150 --> 00:16:26,130
works and we found that the module is

00:16:23,910 --> 00:16:28,560
doing some stuff to provide its full

00:16:26,130 --> 00:16:31,440
functionality but that we didn't need

00:16:28,560 --> 00:16:33,090
all of that because it's Ora set up so

00:16:31,440 --> 00:16:37,920
we can make a few assumptions about

00:16:33,090 --> 00:16:40,410
structure of our code so we wrapped some

00:16:37,920 --> 00:16:43,680
parts of the upstream module and now it

00:16:40,410 --> 00:16:47,280
includes a few custom queries into the

00:16:43,680 --> 00:16:49,050
puppet database which took us about a

00:16:47,280 --> 00:16:50,700
week to come up with it and I think it

00:16:49,050 --> 00:16:53,820
would take about two hours to show and

00:16:50,700 --> 00:16:56,160
explain it so I decided to leave it out

00:16:53,820 --> 00:16:59,310
of the talk but if we are interested in

00:16:56,160 --> 00:17:01,320
that feel free to talk to me so in the

00:16:59,310 --> 00:17:04,230
end and we are able now to run puppet in

00:17:01,320 --> 00:17:07,020
five minutes that's still like five

00:17:04,230 --> 00:17:08,940
minutes F like four minutes too much one

00:17:07,020 --> 00:17:11,070
minute would be ok we are now at five

00:17:08,940 --> 00:17:13,440
minutes raised which is not great but

00:17:11,070 --> 00:17:15,630
it's well better than 15 and we are

00:17:13,440 --> 00:17:19,370
still looking for a way to make this

00:17:15,630 --> 00:17:22,709
faster if you have any idea talk to me

00:17:19,370 --> 00:17:27,050
when our puppet imported all the host

00:17:22,709 --> 00:17:29,400
resources from our puppet database the

00:17:27,050 --> 00:17:31,950
signature configuration looks like that

00:17:29,400 --> 00:17:34,740
if you are familiar with it signature is

00:17:31,950 --> 00:17:36,150
anyone here using a signature okay

00:17:34,740 --> 00:17:38,850
that's quite a few people so you

00:17:36,150 --> 00:17:42,390
probably know what that is it's an host

00:17:38,850 --> 00:17:45,000
object in a signatu and there's nothing

00:17:42,390 --> 00:17:46,060
really special here and address a few

00:17:45,000 --> 00:17:47,920
variables for

00:17:46,060 --> 00:17:50,350
sample we have our own variables for

00:17:47,920 --> 00:17:53,080
service level agreements or off on which

00:17:50,350 --> 00:17:55,960
hardware note virtualized house is

00:17:53,080 --> 00:17:59,590
running the interesting part is this

00:17:55,960 --> 00:18:01,660
year MySQL in ODB based on the fact that

00:17:59,590 --> 00:18:05,760
our Myers came out your exported

00:18:01,660 --> 00:18:08,260
variable or a signatu import and puppet

00:18:05,760 --> 00:18:11,170
wrote this line and the configuration

00:18:08,260 --> 00:18:15,100
and that is used to apply our I say our

00:18:11,170 --> 00:18:17,650
my scanner DB check so every module that

00:18:15,100 --> 00:18:22,060
adds a line with Suzy level monitoring

00:18:17,650 --> 00:18:24,130
register variable will well it will lead

00:18:22,060 --> 00:18:27,640
to a line like this in our icing a2

00:18:24,130 --> 00:18:29,350
configuration still this is not anything

00:18:27,640 --> 00:18:32,380
that you check it's just a variable

00:18:29,350 --> 00:18:35,550
which is doing exactly nothing so we

00:18:32,380 --> 00:18:38,170
need a way to actually apply a service

00:18:35,550 --> 00:18:40,780
again we are using puppet to build this

00:18:38,170 --> 00:18:42,460
configuration file and if you are

00:18:40,780 --> 00:18:45,790
familiar with a signatu you know this is

00:18:42,460 --> 00:18:47,980
a service especially this is my scanner

00:18:45,790 --> 00:18:50,350
DB service we are using on our example

00:18:47,980 --> 00:18:53,020
host the interesting part is at the

00:18:50,350 --> 00:18:56,500
bottom is sign where that's a powerful

00:18:53,020 --> 00:18:58,630
way in singer to to have service

00:18:56,500 --> 00:19:01,660
template and then apply it to a bunch of

00:18:58,630 --> 00:19:06,400
hosts so every host that has this

00:19:01,660 --> 00:19:08,620
variable mysqldb enable equals true will

00:19:06,400 --> 00:19:11,740
be monitored with a check my SQL any DB

00:19:08,620 --> 00:19:15,580
in ODB so that's all we need to apply

00:19:11,740 --> 00:19:17,380
this check and we have a lot of service

00:19:15,580 --> 00:19:20,620
templates like this they are in one

00:19:17,380 --> 00:19:23,530
large file generated by puppet so that's

00:19:20,620 --> 00:19:26,560
our default check with the same defaults

00:19:23,530 --> 00:19:28,870
on MySQL in ODB check we need to be able

00:19:26,560 --> 00:19:31,720
to override the settings like I showed

00:19:28,870 --> 00:19:33,970
you in Huayra that's happening inside of

00:19:31,720 --> 00:19:37,960
an template that is included in this

00:19:33,970 --> 00:19:41,710
template that includes only a few a lot

00:19:37,960 --> 00:19:45,010
of if else if else if else if else just

00:19:41,710 --> 00:19:48,160
to override every possible setting but

00:19:45,010 --> 00:19:49,660
this that helps us to ya have our same

00:19:48,160 --> 00:19:53,140
defaults in this template and then

00:19:49,660 --> 00:19:55,200
override settings if necessary and then

00:19:53,140 --> 00:19:58,240
our experience that works pretty great

00:19:55,200 --> 00:19:59,220
so now I showed you how we deploy our

00:19:58,240 --> 00:20:02,250
configuration

00:19:59,220 --> 00:20:04,440
that setup is 100% automated we can

00:20:02,250 --> 00:20:06,510
delete our completely singer to cluster

00:20:04,440 --> 00:20:11,070
and then we just run puppet and we'll be

00:20:06,510 --> 00:20:13,049
there again hopefully we tried it a few

00:20:11,070 --> 00:20:14,909
times so after we are done we are done

00:20:13,049 --> 00:20:17,280
with our migration we deleted everything

00:20:14,909 --> 00:20:20,549
and started to set it up again and it

00:20:17,280 --> 00:20:22,710
worked pretty well so yeah

00:20:20,549 --> 00:20:26,520
the requirement automation was fulfilled

00:20:22,710 --> 00:20:28,230
by that then we wanted to have a hive

00:20:26,520 --> 00:20:32,030
available set up I will give you a

00:20:28,230 --> 00:20:36,450
second to look at the confusing picture

00:20:32,030 --> 00:20:38,429
it's in two parts on the left side we

00:20:36,450 --> 00:20:40,320
have our actually singer - back-end and

00:20:38,429 --> 00:20:42,059
on the right side we have our icing

00:20:40,320 --> 00:20:45,720
upfront and they are separated

00:20:42,059 --> 00:20:48,539
software's so on the left side we are

00:20:45,720 --> 00:20:50,190
using 60 signature zones so that's a lot

00:20:48,539 --> 00:20:52,590
less than you may have in your own

00:20:50,190 --> 00:20:55,559
setups a lot of people have like 200

00:20:52,590 --> 00:20:58,320
zones we are using only six we have one

00:20:55,559 --> 00:21:01,440
master zone which includes two masters

00:20:58,320 --> 00:21:03,809
one in each data center and so if one

00:21:01,440 --> 00:21:06,000
data center goes completely offline the

00:21:03,809 --> 00:21:09,000
second one will take over completely

00:21:06,000 --> 00:21:10,770
we tried that not shutting down the

00:21:09,000 --> 00:21:12,780
whole data center but we try to shut

00:21:10,770 --> 00:21:14,669
down one a signatu master and it worked

00:21:12,780 --> 00:21:17,059
that the other one would execute all

00:21:14,669 --> 00:21:20,419
what schedule all the necessary checks

00:21:17,059 --> 00:21:23,100
then we have two large signature zones

00:21:20,419 --> 00:21:25,289
one in each data center and we are using

00:21:23,100 --> 00:21:27,720
two satellites in each data center to

00:21:25,289 --> 00:21:29,640
make sure if one satellite fails or we

00:21:27,720 --> 00:21:33,270
have to reboot something we still have a

00:21:29,640 --> 00:21:35,700
second one to take over we would like to

00:21:33,270 --> 00:21:38,190
have three of them in each zone but it's

00:21:35,700 --> 00:21:40,470
not possibility singer - we noticed that

00:21:38,190 --> 00:21:45,659
when we tried it and everything exploded

00:21:40,470 --> 00:21:47,549
and so but usually two is enough then we

00:21:45,659 --> 00:21:49,799
have two more zones they are very small

00:21:47,549 --> 00:21:52,020
management zones with only a few hosts

00:21:49,799 --> 00:21:55,080
and we are using only one satellite in

00:21:52,020 --> 00:21:57,570
each of them course we decided if this

00:21:55,080 --> 00:21:59,610
satellite goes down we will notice that

00:21:57,570 --> 00:22:02,130
because they sing our realized that one

00:21:59,610 --> 00:22:03,900
satellite is gone but it's not necessary

00:22:02,130 --> 00:22:06,929
to have a second one but it would be

00:22:03,900 --> 00:22:09,330
easy to add one then we have another

00:22:06,929 --> 00:22:11,399
zone it's a global own across all of our

00:22:09,330 --> 00:22:12,360
signature servers and it includes for

00:22:11,399 --> 00:22:14,220
example the

00:22:12,360 --> 00:22:16,590
this configuration from the last slide

00:22:14,220 --> 00:22:19,200
and stuff like that that just needs to

00:22:16,590 --> 00:22:20,820
be available on all servers so our a

00:22:19,200 --> 00:22:23,730
singer setup usually works like that

00:22:20,820 --> 00:22:26,460
we're executing puppet on the master one

00:22:23,730 --> 00:22:28,260
then it deploys the configuration and

00:22:26,460 --> 00:22:30,120
it's synchronized by the Insignia to

00:22:28,260 --> 00:22:33,000
cluster to all other servers so the

00:22:30,120 --> 00:22:36,840
master 2 and to all satellites if

00:22:33,000 --> 00:22:39,299
necessary then the master 2 starts to

00:22:36,840 --> 00:22:40,110
dump its complete configuration into the

00:22:39,299 --> 00:22:41,790
I do dB

00:22:40,110 --> 00:22:44,400
if you are familiar with a signatu you

00:22:41,790 --> 00:22:47,730
know what a painful part of software

00:22:44,400 --> 00:22:50,160
this can be but it's well at least it

00:22:47,730 --> 00:22:53,160
works all stuff is dumped into this

00:22:50,160 --> 00:22:55,830
database and unfortunately an hour setup

00:22:53,160 --> 00:22:58,110
this takes several minutes so when we

00:22:55,830 --> 00:23:00,299
deploy our new configuration every 30

00:22:58,110 --> 00:23:02,160
minutes it takes a few minutes until

00:23:00,299 --> 00:23:03,419
this information is available in the

00:23:02,160 --> 00:23:06,990
nice Kaido dB

00:23:03,419 --> 00:23:09,090
so in this few minutes we will have

00:23:06,990 --> 00:23:10,799
delayed information in our icinga web

00:23:09,090 --> 00:23:13,290
interface because it's in effect a

00:23:10,799 --> 00:23:16,530
signal app uses the data and the my

00:23:13,290 --> 00:23:19,049
Oscar you know DB to display stuff we

00:23:16,530 --> 00:23:21,080
know about that it's not great but we

00:23:19,049 --> 00:23:24,090
are hoping that someday in the future

00:23:21,080 --> 00:23:26,070
the I do DB role will be replaced in

00:23:24,090 --> 00:23:28,559
that and this problem is hopefully gun

00:23:26,070 --> 00:23:30,590
so I'm looking forward to the talkin

00:23:28,559 --> 00:23:34,770
afternoon maybe they will announce it

00:23:30,590 --> 00:23:36,240
but we will see so for now this works we

00:23:34,770 --> 00:23:38,309
are exporting all events of our

00:23:36,240 --> 00:23:40,290
signature cluster into an elastic

00:23:38,309 --> 00:23:42,720
stretch cluster we are using a signal

00:23:40,290 --> 00:23:44,640
pit for that I will not talk much about

00:23:42,720 --> 00:23:46,830
it because we are not using this

00:23:44,640 --> 00:23:49,770
information at all at the time we are

00:23:46,830 --> 00:23:51,809
looking into it how it could help us to

00:23:49,770 --> 00:23:53,790
analyze or a signal setup are currently

00:23:51,809 --> 00:23:56,090
we are just writing it into it and then

00:23:53,790 --> 00:23:59,450
after a few weeks we delete it

00:23:56,090 --> 00:24:02,010
so hi available on the left side I think

00:23:59,450 --> 00:24:04,200
we have two satellites and two masters

00:24:02,010 --> 00:24:06,660
in the important parts and they are

00:24:04,200 --> 00:24:10,980
scheduling checks to the bottom where we

00:24:06,660 --> 00:24:13,530
have all our customer setups so it's

00:24:10,980 --> 00:24:15,809
Cigna web we are using anycast to route

00:24:13,530 --> 00:24:17,610
our traffic to the different data

00:24:15,809 --> 00:24:21,059
centers to make sure if one data center

00:24:17,610 --> 00:24:23,970
fails we still are Vale or it's still

00:24:21,059 --> 00:24:26,300
possible to reach our web interface then

00:24:23,970 --> 00:24:28,160
we have Hardware load balancer

00:24:26,300 --> 00:24:31,760
each data center the hardware load

00:24:28,160 --> 00:24:33,590
balancers do a lot more than just ya

00:24:31,760 --> 00:24:36,500
doing load balancing for it I sing my

00:24:33,590 --> 00:24:38,690
web setup they are also in doing stuff

00:24:36,500 --> 00:24:40,880
for our customers but we use it to free

00:24:38,690 --> 00:24:44,360
sing adapt to each data center we have

00:24:40,880 --> 00:24:46,670
two we sing about servers and yeah if

00:24:44,360 --> 00:24:48,860
one load balancer fails they also other

00:24:46,670 --> 00:24:51,320
one will take over if one back-end fails

00:24:48,860 --> 00:24:53,210
the other one will take take over so

00:24:51,320 --> 00:24:55,790
that should be high available enough

00:24:53,210 --> 00:24:57,830
right height variable that's what it

00:24:55,790 --> 00:25:00,560
looks like everything that is read can

00:24:57,830 --> 00:25:03,740
fail and our monitoring still works we

00:25:00,560 --> 00:25:06,230
will still have 100% of our alerts and

00:25:03,740 --> 00:25:08,960
of our recovery notifications we will

00:25:06,230 --> 00:25:11,360
not be able to acknowledge checks in the

00:25:08,960 --> 00:25:14,240
web interface we will not be able to see

00:25:11,360 --> 00:25:17,140
in web interface but at least we can see

00:25:14,240 --> 00:25:20,090
the alerts we get SMS and emails and

00:25:17,140 --> 00:25:22,420
when this happens we will notice it

00:25:20,090 --> 00:25:27,050
because two data centers are mostly done

00:25:22,420 --> 00:25:29,300
and yeah but luckily for us that never

00:25:27,050 --> 00:25:32,930
happened and but maybe it was something

00:25:29,300 --> 00:25:35,830
so it's hi available at least in our

00:25:32,930 --> 00:25:38,360
definition of available it's enough and

00:25:35,830 --> 00:25:40,580
we are not only running one of those

00:25:38,360 --> 00:25:42,740
setups we are running to one is the

00:25:40,580 --> 00:25:45,860
production setup and the other one is a

00:25:42,740 --> 00:25:49,630
stage setup we use to test changes or to

00:25:45,860 --> 00:25:53,030
test very similar to updates and I can't

00:25:49,630 --> 00:25:55,880
recommend that enough build yourself a

00:25:53,030 --> 00:25:58,430
stage setup if you're updating your life

00:25:55,880 --> 00:26:00,950
environment it is offline for six hours

00:25:58,430 --> 00:26:04,040
that's bad if you do that in your

00:26:00,950 --> 00:26:05,600
staging environment it's okay nobody

00:26:04,040 --> 00:26:07,130
really cares you have time you can fix

00:26:05,600 --> 00:26:08,750
it and then when you know how it works

00:26:07,130 --> 00:26:11,690
or there was an upstream fix you can

00:26:08,750 --> 00:26:13,880
update your life environment so since we

00:26:11,690 --> 00:26:15,710
went live in January we had a singing

00:26:13,880 --> 00:26:19,960
outage in our production environment and

00:26:15,710 --> 00:26:22,760
yeah I'm really happy about that okay so

00:26:19,960 --> 00:26:26,410
second requirement was high availability

00:26:22,760 --> 00:26:29,950
I think that it is known you may wanna

00:26:26,410 --> 00:26:32,650
how crazy is this 11 to use n RPE in

00:26:29,950 --> 00:26:36,620
2018 to monitor all the infrastructure

00:26:32,650 --> 00:26:38,720
there is the signatu client the signatu

00:26:36,620 --> 00:26:39,680
client has stuff like encryption and

00:26:38,720 --> 00:26:41,690
authentication

00:26:39,680 --> 00:26:44,150
so there are a lot of reasons to use

00:26:41,690 --> 00:26:47,270
that one but we are still using an epi

00:26:44,150 --> 00:26:49,940
why when we decided to migrate or sing a

00:26:47,270 --> 00:26:52,640
tune we knew it would be a big task and

00:26:49,940 --> 00:26:55,250
we didn't want to make it much bigger

00:26:52,640 --> 00:26:58,520
because we know if the project is too

00:26:55,250 --> 00:27:00,950
big it will fail so we decided hey we

00:26:58,520 --> 00:27:03,380
are having an a PE it's running it's

00:27:00,950 --> 00:27:05,270
executing our checks it works and it's

00:27:03,380 --> 00:27:08,000
only running inside of our own data

00:27:05,270 --> 00:27:10,610
centers in our own cables so the

00:27:08,000 --> 00:27:12,560
security issue is not that big that we

00:27:10,610 --> 00:27:15,500
need to change all the stuff at the same

00:27:12,560 --> 00:27:18,140
time I think some one day in the future

00:27:15,500 --> 00:27:20,660
we will replace an MP e but now it works

00:27:18,140 --> 00:27:23,090
and we could just reuse it like it was

00:27:20,660 --> 00:27:25,210
in the previous setup so just if you

00:27:23,090 --> 00:27:28,070
want not while you are so crazy

00:27:25,210 --> 00:27:31,970
ok full state migration was another

00:27:28,070 --> 00:27:34,160
requirement migrating stuff is a lot

00:27:31,970 --> 00:27:35,660
harder than building from scratch when

00:27:34,160 --> 00:27:38,060
you're building your infrastructure from

00:27:35,660 --> 00:27:39,500
scratch you can do whatever you want it

00:27:38,060 --> 00:27:41,930
just has to work if you're migrating

00:27:39,500 --> 00:27:44,450
stuff you have requirements from the old

00:27:41,930 --> 00:27:46,250
set up you have old data you have data

00:27:44,450 --> 00:27:49,550
you don't understand because some

00:27:46,250 --> 00:27:52,250
previous co worker brought it or changed

00:27:49,550 --> 00:27:55,580
stuff you don't understand anymore so it

00:27:52,250 --> 00:27:58,160
took us a lot of time and we had a lot

00:27:55,580 --> 00:28:00,770
of manual and automated work to migrate

00:27:58,160 --> 00:28:02,500
all changes like acknowledgement and

00:28:00,770 --> 00:28:05,810
down times from the a single one set up

00:28:02,500 --> 00:28:09,650
it was in this step when I joined the

00:28:05,810 --> 00:28:11,420
team that was deploying this so I had a

00:28:09,650 --> 00:28:14,750
lot of manual and automated work to

00:28:11,420 --> 00:28:18,920
migrate with parts the co-worker wrote a

00:28:14,750 --> 00:28:20,900
software to to make a diff of a single

00:28:18,920 --> 00:28:24,800
one and a signatu internally it was

00:28:20,900 --> 00:28:26,750
called a signet if and it catches all

00:28:24,800 --> 00:28:28,910
information from your signal one set ups

00:28:26,750 --> 00:28:31,640
then it fetches all information from

00:28:28,910 --> 00:28:33,310
using that who set up and a tries to

00:28:31,640 --> 00:28:37,160
find differences

00:28:33,310 --> 00:28:40,130
it can included a lot of customs this 11

00:28:37,160 --> 00:28:42,830
specific code in the last few weeks we

00:28:40,130 --> 00:28:45,260
extracted then also c11 stuff of it and

00:28:42,830 --> 00:28:47,930
now we are open sourcing at another MIT

00:28:45,260 --> 00:28:50,210
license as the signature migration UTS

00:28:47,930 --> 00:28:52,360
so if you still have any signal one

00:28:50,210 --> 00:28:54,690
tweezing a - migration to do

00:28:52,360 --> 00:28:57,370
should check this github repository out

00:28:54,690 --> 00:28:59,890
there's one small issue if you check the

00:28:57,370 --> 00:29:02,260
website now you will realize that we had

00:28:59,890 --> 00:29:04,030
some scheduling conflict on Friday when

00:29:02,260 --> 00:29:07,600
we wanted to release it so it's not

00:29:04,030 --> 00:29:09,309
released yet I created the repository to

00:29:07,600 --> 00:29:11,590
make sure that you can subscribe to it

00:29:09,309 --> 00:29:14,410
and next week when I'm back in Berlin we

00:29:11,590 --> 00:29:16,420
will release that so a huge shout out to

00:29:14,410 --> 00:29:21,820
my coworker Ingo who wrote all of this

00:29:16,420 --> 00:29:23,260
code and it's not ready to use in

00:29:21,820 --> 00:29:25,840
production solution it's more like a

00:29:23,260 --> 00:29:27,580
framework you can utilize to build your

00:29:25,840 --> 00:29:30,370
own migration there are a lot of

00:29:27,580 --> 00:29:33,490
examples like hey if you have an HTTP

00:29:30,370 --> 00:29:35,919
check in single one you can compare to

00:29:33,490 --> 00:29:38,470
anything that a - an HTTP checking signa

00:29:35,919 --> 00:29:40,450
- like that and you may want to make

00:29:38,470 --> 00:29:43,720
sure that you check this very area and

00:29:40,450 --> 00:29:45,490
stuff like that so it's also able to

00:29:43,720 --> 00:29:48,070
migrate all your acknowledgments

00:29:45,490 --> 00:29:50,380
downtimes and comments so the signature

00:29:48,070 --> 00:29:53,020
migration your tools are able to figure

00:29:50,380 --> 00:29:55,419
out which check is the matching check in

00:29:53,020 --> 00:29:58,020
the new environment which sometimes is

00:29:55,419 --> 00:30:03,070
really hard because a signet who has a

00:29:58,020 --> 00:30:05,290
different configuration logic than any

00:30:03,070 --> 00:30:07,510
single one so there's some fuzzy

00:30:05,290 --> 00:30:09,820
matching like okay this is probably that

00:30:07,510 --> 00:30:12,040
one and if it's not able to detect which

00:30:09,820 --> 00:30:14,080
check it is then it at least tells you I

00:30:12,040 --> 00:30:17,140
don't know where this is you may want to

00:30:14,080 --> 00:30:19,090
check that manually so a lot of code

00:30:17,140 --> 00:30:21,880
examples you should check it out next

00:30:19,090 --> 00:30:25,929
week I'm very sorry for the release read

00:30:21,880 --> 00:30:29,679
delay the last part of my talk is about

00:30:25,929 --> 00:30:32,890
our modifications in singer web we have

00:30:29,679 --> 00:30:36,070
around 30 engineers using a single app

00:30:32,890 --> 00:30:38,770
and 10 to 15 of them use it for incident

00:30:36,070 --> 00:30:40,929
response management so we need to make a

00:30:38,770 --> 00:30:43,030
few changes to adapt it to our workflow

00:30:40,929 --> 00:30:45,040
that's awesome hot message if you have a

00:30:43,030 --> 00:30:48,460
tool and it doesn't do what you want you

00:30:45,040 --> 00:30:51,760
should adapt it instead of rewriting a

00:30:48,460 --> 00:30:53,770
new and if you are familiar with this

00:30:51,760 --> 00:30:56,590
signal app you may notice that this

00:30:53,770 --> 00:30:59,169
looks a little bit different we added

00:30:56,590 --> 00:31:02,020
this blue little marker it says service

00:30:59,169 --> 00:31:04,750
level agreement for this host that helps

00:31:02,020 --> 00:31:05,520
our support engineers to prioritize our

00:31:04,750 --> 00:31:08,010
alerts

00:31:05,520 --> 00:31:10,140
there's a blue marker okay I will have

00:31:08,010 --> 00:31:14,250
to call back this customer in a specific

00:31:10,140 --> 00:31:17,700
time and it was a little bit tricky to

00:31:14,250 --> 00:31:19,140
get this working but it's just a little

00:31:17,700 --> 00:31:22,890
blue dot and it's super helpful for

00:31:19,140 --> 00:31:25,710
engineers we also added additional

00:31:22,890 --> 00:31:29,880
documentation of one in another note

00:31:25,710 --> 00:31:33,420
about the SLA marker it's early I'm a

00:31:29,880 --> 00:31:36,300
saw this sis 11 SLA variable inside of

00:31:33,420 --> 00:31:39,360
the whole store inside of the service

00:31:36,300 --> 00:31:42,830
template you may remember this is used

00:31:39,360 --> 00:31:45,600
biasing a web to draw this marker

00:31:42,830 --> 00:31:47,880
additional documentation our engineers

00:31:45,600 --> 00:31:51,330
need to respond to incidents in a

00:31:47,880 --> 00:31:53,910
specific time and there's one option

00:31:51,330 --> 00:31:56,520
opened five different tabs and using all

00:31:53,910 --> 00:31:59,370
the search interfaces for our hardware

00:31:56,520 --> 00:32:01,170
node monitoring or trending or our

00:31:59,370 --> 00:32:03,540
internet service documentation or

00:32:01,170 --> 00:32:05,310
customer specific documentation that

00:32:03,540 --> 00:32:08,310
takes a lot of time because you need to

00:32:05,310 --> 00:32:10,140
use different weapon defaces in every

00:32:08,310 --> 00:32:14,760
web interface you will just search for

00:32:10,140 --> 00:32:17,490
sis 11 OMC so we decided to add links to

00:32:14,760 --> 00:32:19,440
all the necessary documentation that is

00:32:17,490 --> 00:32:23,490
super helpful for engineers because they

00:32:19,440 --> 00:32:25,650
can start to check all the docs also

00:32:23,490 --> 00:32:28,050
another change we made it's one I'm

00:32:25,650 --> 00:32:29,550
personally proud of because a co-workers

00:32:28,050 --> 00:32:33,750
who is sitting somewhere in the audience

00:32:29,550 --> 00:32:36,360
came to me and said max the history tab

00:32:33,750 --> 00:32:38,460
oficina web 2 is nice but we have a lot

00:32:36,360 --> 00:32:40,590
of stuff that there that we are not

00:32:38,460 --> 00:32:42,870
interested in it we want to know when we

00:32:40,590 --> 00:32:48,630
acknowledged an alert can we do this and

00:32:42,870 --> 00:32:49,950
I was like oh that sounds hard and in

00:32:48,630 --> 00:32:51,900
the beginning it was hard because I

00:32:49,950 --> 00:32:53,970
don't know much about anything about

00:32:51,900 --> 00:32:56,460
internals but I spoke to another

00:32:53,970 --> 00:32:58,410
co-worker who knew how the framework

00:32:56,460 --> 00:33:01,950
works and he showed me how to use it and

00:32:58,410 --> 00:33:05,670
it's about 50 lines of PHP code to get

00:33:01,950 --> 00:33:08,670
this list here and it's also super

00:33:05,670 --> 00:33:10,740
helpful because our engine is now C ok

00:33:08,670 --> 00:33:14,430
this alert was acknowledged yesterday

00:33:10,740 --> 00:33:17,700
and the day before yesterday last week

00:33:14,430 --> 00:33:19,170
and it was always at 3 o'clock so maybe

00:33:17,700 --> 00:33:23,370
there's a pattern

00:33:19,170 --> 00:33:25,020
and it already has links to our customer

00:33:23,370 --> 00:33:27,450
tickets we are using Zendesk for

00:33:25,020 --> 00:33:29,160
customer tickets so he or she can just

00:33:27,450 --> 00:33:31,890
click on it and check out of there's

00:33:29,160 --> 00:33:34,650
already an incident response ongoing

00:33:31,890 --> 00:33:37,820
with our customer or can check out what

00:33:34,650 --> 00:33:41,520
risk co-worker did to solve the issue

00:33:37,820 --> 00:33:44,100
so one last modification the

00:33:41,520 --> 00:33:47,990
acknowledgement of tickets we decided to

00:33:44,100 --> 00:33:52,460
build a feature ACTU ticket it creates a

00:33:47,990 --> 00:33:56,370
Zendesk ticket by our internal API to

00:33:52,460 --> 00:33:58,380
you know to put the complete workflow of

00:33:56,370 --> 00:34:00,450
incident response inside of a single web

00:33:58,380 --> 00:34:02,840
we are able to acknowledge your ticket

00:34:00,450 --> 00:34:05,870
we are able to group it by customer or

00:34:02,840 --> 00:34:09,210
project so maybe we don't have 200

00:34:05,870 --> 00:34:11,429
tickets when we create 200 everyone we

00:34:09,210 --> 00:34:15,810
acknowledge 200 alerts that's helpful

00:34:11,429 --> 00:34:18,210
and then the actor ticket feature also

00:34:15,810 --> 00:34:20,909
adds all the additional documentation in

00:34:18,210 --> 00:34:23,130
an internal comment in the customer

00:34:20,909 --> 00:34:25,350
ticket so the engineer has this

00:34:23,130 --> 00:34:27,570
information also available inside of the

00:34:25,350 --> 00:34:29,340
ticket now we are able to acknowledge

00:34:27,570 --> 00:34:32,399
tickets we are able to see the last

00:34:29,340 --> 00:34:33,990
acknowledgments we are able to see the

00:34:32,399 --> 00:34:36,510
service level agreement with our

00:34:33,990 --> 00:34:38,970
customer and so our engineers are able

00:34:36,510 --> 00:34:42,090
to have it all available in side of

00:34:38,970 --> 00:34:43,620
English pretty awesome so who a few

00:34:42,090 --> 00:34:45,720
things this should be available

00:34:43,620 --> 00:34:47,610
available upstream like the service

00:34:45,720 --> 00:34:52,380
level agreement marker or the last

00:34:47,610 --> 00:34:55,530
acknowledgments yes I agree totally and

00:34:52,380 --> 00:34:59,640
the only reason it is not yet is that we

00:34:55,530 --> 00:35:01,860
need to find the time to yeah put the

00:34:59,640 --> 00:35:03,480
university parts into one place and the

00:35:01,860 --> 00:35:06,890
city level specific parts into another

00:35:03,480 --> 00:35:09,360
one and then we want to release it so

00:35:06,890 --> 00:35:11,490
I'm looking for a co-worker if you would

00:35:09,360 --> 00:35:14,400
like to submit pull requests so using

00:35:11,490 --> 00:35:17,700
about to feel free to talk to me that's

00:35:14,400 --> 00:35:20,580
always in a web set up after one year or

00:35:17,700 --> 00:35:23,730
nearly one year of running the setup the

00:35:20,580 --> 00:35:25,770
yeah the most important tip is spirit a

00:35:23,730 --> 00:35:27,450
state set up seriously if you are

00:35:25,770 --> 00:35:29,400
testing your production in signa

00:35:27,450 --> 00:35:32,760
environment you are doing it wrong in

00:35:29,400 --> 00:35:35,570
some way also automate everything you

00:35:32,760 --> 00:35:37,740
don't want any manual changes whatsoever

00:35:35,570 --> 00:35:40,470
track all related issues

00:35:37,740 --> 00:35:43,500
that's a time-consuming tip I take

00:35:40,470 --> 00:35:45,090
several hours a week to track all is

00:35:43,500 --> 00:35:48,870
similar to issues all it's in my lab

00:35:45,090 --> 00:35:51,390
issues is sing-sing Abid monitoring

00:35:48,870 --> 00:35:53,460
plugins a signal packaging that takes

00:35:51,390 --> 00:35:55,560
time but first of all it's interesting

00:35:53,460 --> 00:35:57,750
to see what issues other people find

00:35:55,560 --> 00:36:00,780
it's interesting to see how they are

00:35:57,750 --> 00:36:03,300
solved and it helps a lot to detect

00:36:00,780 --> 00:36:05,580
problems in your infrastructure so if

00:36:03,300 --> 00:36:07,020
there's an e signal to update and you

00:36:05,580 --> 00:36:09,000
check out the vector again there are

00:36:07,020 --> 00:36:11,220
like 20 issues that something is broken

00:36:09,000 --> 00:36:13,500
now you may don't want to update your

00:36:11,220 --> 00:36:16,860
own infrastructure but you can see if

00:36:13,500 --> 00:36:18,180
you are yeah if there's impact for your

00:36:16,860 --> 00:36:21,780
own infrastructure if you are debugging

00:36:18,180 --> 00:36:24,870
and sometimes I see all that sounds

00:36:21,780 --> 00:36:27,950
familiar maybe I have seen this early

00:36:24,870 --> 00:36:30,450
and the bacteria so it helps a lot too

00:36:27,950 --> 00:36:32,670
yeah keep track of all issues and it

00:36:30,450 --> 00:36:35,310
also gets you engaged in the community

00:36:32,670 --> 00:36:37,650
in some way because at some point we

00:36:35,310 --> 00:36:40,980
will start to answer to back tickets to

00:36:37,650 --> 00:36:43,560
issues and yeah so maybe we'll you will

00:36:40,980 --> 00:36:46,980
end up talking about your journey of

00:36:43,560 --> 00:36:48,420
debugging a signatu and also because

00:36:46,980 --> 00:36:52,500
it's so important please build a stage

00:36:48,420 --> 00:36:55,170
set up is it perfect

00:36:52,500 --> 00:36:57,960
well yesterday I attended a Prometheus

00:36:55,170 --> 00:36:59,550
workshop so there's some irony in there

00:36:57,960 --> 00:37:03,060
that I'm talking about is similar to why

00:36:59,550 --> 00:37:05,990
I attend Prometheus workshop and there's

00:37:03,060 --> 00:37:09,600
one thing I learned yesterday it's

00:37:05,990 --> 00:37:11,280
neither of them it's perfect so Sigma 2

00:37:09,600 --> 00:37:13,500
is not and prometheus is not and

00:37:11,280 --> 00:37:15,600
Prometheus is not an answer for our

00:37:13,500 --> 00:37:17,700
icing a probe to a singer two problems

00:37:15,600 --> 00:37:20,160
for example currently we have some

00:37:17,700 --> 00:37:22,560
issues with signature notifications that

00:37:20,160 --> 00:37:25,560
are sent out despite the fact that they

00:37:22,560 --> 00:37:27,900
shouldn't and that's a problem that was

00:37:25,560 --> 00:37:30,270
fixed in the last release and we updated

00:37:27,900 --> 00:37:32,160
it and it's still broken in our setup

00:37:30,270 --> 00:37:34,500
it's better now but we are still seeing

00:37:32,160 --> 00:37:37,590
notifications that I'm not supposed to

00:37:34,500 --> 00:37:39,960
and I am trying very hard to reproduce

00:37:37,590 --> 00:37:42,090
it - yeah

00:37:39,960 --> 00:37:45,120
right into the upstream issues or create

00:37:42,090 --> 00:37:46,319
new ones to fix that so yeah that's not

00:37:45,120 --> 00:37:48,739
perfect but

00:37:46,319 --> 00:37:50,789
and 100% shoe if you're using another

00:37:48,739 --> 00:37:54,209
monitoring solution we would have other

00:37:50,789 --> 00:37:56,009
problems so I am convinced that is

00:37:54,209 --> 00:37:56,429
signature with the right decision for a

00:37:56,009 --> 00:37:59,699
setup

00:37:56,429 --> 00:38:01,979
and yeah I'm looking forward to make it

00:37:59,699 --> 00:38:04,979
as perfect as possible and I'm trying to

00:38:01,979 --> 00:38:08,939
give back our solutions for problems we

00:38:04,979 --> 00:38:09,509
had so yes that answers the question is

00:38:08,939 --> 00:38:11,969
it perfect

00:38:09,509 --> 00:38:13,769
since generally we had an outage in our

00:38:11,969 --> 00:38:16,679
life environment so that's pretty good

00:38:13,769 --> 00:38:19,759
I'm happy with that and my last message

00:38:16,679 --> 00:38:22,650
for you is please talk about your setups

00:38:19,759 --> 00:38:24,419
talk to each other and have you talked

00:38:22,650 --> 00:38:26,549
to me tell me about your monitoring

00:38:24,419 --> 00:38:29,489
setups tell me about your problems your

00:38:26,549 --> 00:38:32,009
solutions and feel free to ask me any

00:38:29,489 --> 00:38:34,019
questions about our setup I'm very happy

00:38:32,009 --> 00:38:36,059
to answer that but I think this is an

00:38:34,019 --> 00:38:39,140
open source conference so it should be

00:38:36,059 --> 00:38:41,699
about knowledge and we should share our

00:38:39,140 --> 00:38:43,650
different stories and it doesn't really

00:38:41,699 --> 00:38:46,229
matter if you're monitoring 10 hosts or

00:38:43,650 --> 00:38:49,979
10 million hosts I don't really care I

00:38:46,229 --> 00:38:52,140
just want to know how you do it so if

00:38:49,979 --> 00:38:53,669
you want to reach out to me I have an

00:38:52,140 --> 00:38:56,819
email address you can find me on Twitter

00:38:53,669 --> 00:38:59,240
or github thank you for listening and

00:38:56,819 --> 00:39:07,770
I'm happy to answer your questions

00:38:59,240 --> 00:39:07,770
[Applause]

00:39:18,930 --> 00:39:24,580
have you considered allowing customers

00:39:21,610 --> 00:39:27,070
to log into the web interface or status

00:39:24,580 --> 00:39:32,710
page yes we consider that and we are

00:39:27,070 --> 00:39:34,570
still considering it but in the first

00:39:32,710 --> 00:39:36,550
step we decided not to it would be

00:39:34,570 --> 00:39:38,500
helpful because customers would be able

00:39:36,550 --> 00:39:40,750
to schedule down times for their

00:39:38,500 --> 00:39:43,270
deployments now usually they have to

00:39:40,750 --> 00:39:45,010
create an ticket like a next night we

00:39:43,270 --> 00:39:47,080
will do in deployment please down time I

00:39:45,010 --> 00:39:49,240
will set up and it would be great if

00:39:47,080 --> 00:39:51,490
they could do it by but currently not

00:39:49,240 --> 00:39:53,560
mainly because we decided to make a

00:39:51,490 --> 00:39:57,070
single app only available in our own

00:39:53,560 --> 00:39:59,410
network and we would need to manage our

00:39:57,070 --> 00:40:02,100
customer accounts currently using LDAP

00:39:59,410 --> 00:40:07,000
for identification of our co-workers and

00:40:02,100 --> 00:40:09,910
we considered an additional API - as a

00:40:07,000 --> 00:40:22,770
layer above it but it's like maybe in

00:40:09,910 --> 00:40:22,770
the future more questions sorry

00:40:23,260 --> 00:40:28,780
have you ever tried to use the API for

00:40:25,180 --> 00:40:31,240
contact changes yes I wasn't part of the

00:40:28,780 --> 00:40:34,000
team when we tried that but when we are

00:40:31,240 --> 00:40:36,160
we're evaluating the different

00:40:34,000 --> 00:40:38,110
monitoring solutions we tried it and one

00:40:36,160 --> 00:40:40,900
reason was that we wanted to be able to

00:40:38,110 --> 00:40:44,920
rebuild our complete configuration in a

00:40:40,900 --> 00:40:47,050
way we understand it so we know a lot

00:40:44,920 --> 00:40:49,390
about puppet and so we understand how

00:40:47,050 --> 00:40:52,120
the configuration files build and we are

00:40:49,390 --> 00:40:55,240
very happy to be able to debug that also

00:40:52,120 --> 00:40:58,300
when we started to migrate there was an

00:40:55,240 --> 00:41:00,340
issue in the signatu api like when we

00:40:58,300 --> 00:41:03,010
are doing a lot of changes or asking for

00:41:00,340 --> 00:41:05,380
a lot some time it would just stop and

00:41:03,010 --> 00:41:06,550
we would have in invalid response I

00:41:05,380 --> 00:41:10,150
think you fixed it

00:41:06,550 --> 00:41:11,890
recently but it's yeah it should work

00:41:10,150 --> 00:41:14,260
now but that was also a reason for us

00:41:11,890 --> 00:41:17,850
like ok maybe we shouldn't use the API

00:41:14,260 --> 00:41:17,850
for our core infrastructure

00:41:30,680 --> 00:41:35,490
put notifications and things like that

00:41:32,880 --> 00:41:37,799
how do you manage on call times and

00:41:35,490 --> 00:41:41,880
shifts and who gets what notifications

00:41:37,799 --> 00:41:44,490
when we have an internal API with tools

00:41:41,880 --> 00:41:47,609
to define a first level and a second

00:41:44,490 --> 00:41:50,460
level and the third level and singing as

00:41:47,609 --> 00:41:52,529
always just notifying the first level

00:41:50,460 --> 00:41:55,619
contact and then the notification script

00:41:52,529 --> 00:41:59,339
will figure out by our own API which one

00:41:55,619 --> 00:42:01,950
to notify so that's not part of using a

00:41:59,339 --> 00:42:03,930
tool but we have about 400 customer

00:42:01,950 --> 00:42:06,269
contacts inside of a singer too and

00:42:03,930 --> 00:42:09,119
using that who we're sending SMS and

00:42:06,269 --> 00:42:10,680
emails to them if necessary so we have a

00:42:09,119 --> 00:42:16,109
lot of notification objects in the

00:42:10,680 --> 00:42:19,200
configuration I saw in the web interface

00:42:16,109 --> 00:42:21,990
that systems you have virtualized

00:42:19,200 --> 00:42:23,579
systems that there there is a variable

00:42:21,990 --> 00:42:27,299
that points to the hardware they're

00:42:23,579 --> 00:42:30,180
running on are using VMware and are

00:42:27,299 --> 00:42:32,970
using the possibility to shift them

00:42:30,180 --> 00:42:36,769
across the hardware no ok we are using

00:42:32,970 --> 00:42:39,509
virtual it's a container environment and

00:42:36,769 --> 00:42:42,420
yeah we are asking again our internet

00:42:39,509 --> 00:42:45,960
API to ring the puppet run for all met

00:42:42,420 --> 00:42:48,450
pattern for all your matches of one host

00:42:45,960 --> 00:42:50,630
and which hardware is running on so if

00:42:48,450 --> 00:42:53,309
you are migrating one host to another

00:42:50,630 --> 00:42:55,319
it's possible that our information in a

00:42:53,309 --> 00:42:58,799
signal weapons outdated but it will be

00:42:55,319 --> 00:43:01,319
refreshed in 30 minutes and I also saw

00:42:58,799 --> 00:43:03,720
that there was a line about trending a

00:43:01,319 --> 00:43:06,299
link without running what do you use for

00:43:03,720 --> 00:43:24,140
trending we are using sabich's alright

00:43:06,299 --> 00:43:24,140
thank you there's another one

00:43:28,700 --> 00:43:34,020
I'm just curious you're using I do as

00:43:31,730 --> 00:43:36,120
middleware between your monitoring

00:43:34,020 --> 00:43:38,670
subsystem and the web interface to that

00:43:36,120 --> 00:43:40,830
monitoring subsystem are you using the I

00:43:38,670 --> 00:43:43,860
do information for anything else or is

00:43:40,830 --> 00:43:46,290
it just sort of a temporary warehouse

00:43:43,860 --> 00:43:48,360
just temporary voicing on have you

00:43:46,290 --> 00:43:50,310
considered using live status to connect

00:43:48,360 --> 00:43:51,360
the two ends and just losing the I do

00:43:50,310 --> 00:43:53,880
database entirely

00:43:51,360 --> 00:43:58,470
no but oh that's an interesting idea it

00:43:53,880 --> 00:44:00,000
was a complete shift for us so what does

00:43:58,470 --> 00:44:04,710
it called live status

00:44:00,000 --> 00:44:09,000
okay mattias Kettner and his check MK

00:44:04,710 --> 00:44:11,190
project life status basically I think

00:44:09,000 --> 00:44:13,680
Nagios has actually brought it back in

00:44:11,190 --> 00:44:17,490
stream as well but it was originally

00:44:13,680 --> 00:44:20,070
developed as basically an sql-like

00:44:17,490 --> 00:44:21,390
syntax that allows you to do to talk

00:44:20,070 --> 00:44:23,820
directly to what's going on on the

00:44:21,390 --> 00:44:25,890
daemon and it is supported and icinga -

00:44:23,820 --> 00:44:27,980
okay yeah that sounds interesting thank

00:44:25,890 --> 00:44:27,980
you

00:44:36,910 --> 00:44:42,380
you said that you have around 30

00:44:40,130 --> 00:44:45,440
developers are they all developing no

00:44:42,380 --> 00:44:48,349
they are no developers they are admins

00:44:45,440 --> 00:44:49,550
in so they are work sorry people who are

00:44:48,349 --> 00:44:53,630
developing checks

00:44:49,550 --> 00:44:55,730
no no it's mostly me responsible for our

00:44:53,630 --> 00:44:57,740
monitoring and we have an automation

00:44:55,730 --> 00:45:01,430
team that is working with puppet and so

00:44:57,740 --> 00:45:04,520
most the people of this team working on

00:45:01,430 --> 00:45:06,980
checks so we are like I just wonder if

00:45:04,520 --> 00:45:11,720
they're all working on the same stage

00:45:06,980 --> 00:45:14,480
environment well our automation team is

00:45:11,720 --> 00:45:17,030
working on the same yes and sometimes we

00:45:14,480 --> 00:45:19,700
are using you regen boxes to test stuff

00:45:17,030 --> 00:45:21,740
but if you want to actually test our own

00:45:19,700 --> 00:45:23,690
environment we are all using using the

00:45:21,740 --> 00:45:25,609
same one and how that is usually not a

00:45:23,690 --> 00:45:27,859
problem because we are not all working

00:45:25,609 --> 00:45:29,780
on monitoring tricks at the same time so

00:45:27,859 --> 00:45:31,520
we just asked like does anyone need this

00:45:29,780 --> 00:45:34,150
environment now and then we test stuff

00:45:31,520 --> 00:45:34,150
okay

00:45:44,049 --> 00:45:50,089
you reported that you have this ideal

00:45:47,630 --> 00:45:52,940
performance problem yeah we had the same

00:45:50,089 --> 00:45:55,549
problem while we are reading the I do

00:45:52,940 --> 00:45:59,769
database from the thing about to fronted

00:45:55,549 --> 00:46:03,410
because we have over 1000 different

00:45:59,769 --> 00:46:05,539
groups because every project has an

00:46:03,410 --> 00:46:08,329
armed group with an Orion notification

00:46:05,539 --> 00:46:09,859
objects and we had heavily heavy

00:46:08,329 --> 00:46:13,480
performance problems with the read

00:46:09,859 --> 00:46:17,589
performance so we know caching okay and

00:46:13,480 --> 00:46:17,589
do you experienced the same

00:46:17,769 --> 00:46:23,660
well it's I'm not really sure but we

00:46:21,650 --> 00:46:27,170
have also a lot of groups so that could

00:46:23,660 --> 00:46:29,390
be related but I think it's more about

00:46:27,170 --> 00:46:31,730
writing into the database than reading

00:46:29,390 --> 00:46:34,519
from it okay because we had reading

00:46:31,730 --> 00:46:36,289
performance issues because we have lots

00:46:34,519 --> 00:46:39,049
of groups which are only allowed to see

00:46:36,289 --> 00:46:41,839
specific checks check results I so you

00:46:39,049 --> 00:46:43,880
mean groups in a single yet sorry

00:46:41,839 --> 00:46:46,039
filtering text no we don't have any

00:46:43,880 --> 00:46:48,819
groups in this thing about okay that's

00:46:46,039 --> 00:46:48,819
the first time okay

00:46:50,170 --> 00:46:59,850
well then thank you for your time

00:46:52,990 --> 00:47:11,689
[Applause]

00:46:59,850 --> 00:47:11,689

YouTube URL: https://www.youtube.com/watch?v=y58eLfGtjP0


