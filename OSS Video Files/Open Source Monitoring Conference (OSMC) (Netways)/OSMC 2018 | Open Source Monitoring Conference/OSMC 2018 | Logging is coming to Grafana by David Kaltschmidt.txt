Title: OSMC 2018 | Logging is coming to Grafana by David Kaltschmidt
Publication date: 2018-11-16
Playlist: OSMC 2018 | Open Source Monitoring Conference
Description: 
	Grafana is an OSS dashboarding platform with a focus on visualising time.series data as beautiful graphs. Now we’re adding support to show your logs inside Grafana as well. Adding support for log aggregation makes Grafana an even better tool for incident response: First, the metric graphs help in a visual zoning in on the issue. Then you can seamlessly switch over to view and search related log files, allowing you to better understand what your software was doing while the issue was occurring. The main part of this talk shows how to deploy the necessary parts for this integrated experience. In addition I’ll show the latest features of Grafana both for creating dashboards and maintaining their configuration. The last 10-15 will be reserved for a Q&A.

NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de/
Blog: http://blog.netways.de/
NWS: https://nws.netways.de

Webinare
Archiv Link: https://www.netways.de/webinare/archi...
Aktuell: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh/

www.musicfox.com
Captions: 
	00:00:01,940 --> 00:00:13,270
[Music]

00:00:16,279 --> 00:00:23,609
hello hello everyone

00:00:18,270 --> 00:00:25,980
yes I am David and yeah I'm the director

00:00:23,609 --> 00:00:28,949
of UX at Cortana labs right now and

00:00:25,980 --> 00:00:30,269
which basically means if you are stuck

00:00:28,949 --> 00:00:32,520
somewhere and you're clicking and you're

00:00:30,269 --> 00:00:35,840
frustrated with your father you can just

00:00:32,520 --> 00:00:40,290
reach out to me or open an issue

00:00:35,840 --> 00:00:42,390
basically and prometheus is also good a

00:00:40,290 --> 00:00:44,309
good topic well I'm gonna show more

00:00:42,390 --> 00:00:48,840
about this a bit later as well not just

00:00:44,309 --> 00:00:50,370
logging and so quickly what I'm going to

00:00:48,840 --> 00:00:53,190
be talking about today quick intro of

00:00:50,370 --> 00:00:55,500
Cortana then what's new since release

00:00:53,190 --> 00:00:58,530
5-0 then we're talking a bit about

00:00:55,500 --> 00:01:01,879
logging and then finally a bit of an

00:00:58,530 --> 00:01:05,280
outlook of the upcoming release 6.0 so

00:01:01,879 --> 00:01:08,100
first real quick who here is using

00:01:05,280 --> 00:01:12,689
grifone oh yeah oh that is that's a lot

00:01:08,100 --> 00:01:15,390
of people yeah that's very good for the

00:01:12,689 --> 00:01:17,100
other people go fauna is a - boring

00:01:15,390 --> 00:01:20,310
solution but it's currently a bit on a

00:01:17,100 --> 00:01:22,740
journey from - boring solution to

00:01:20,310 --> 00:01:25,259
complete obviously the observability

00:01:22,740 --> 00:01:29,670
platform and logging is going to be a

00:01:25,259 --> 00:01:31,409
big part of this and also we're making

00:01:29,670 --> 00:01:34,590
is making graph on a bit more enterprise

00:01:31,409 --> 00:01:36,990
friendly by offering a light theme so

00:01:34,590 --> 00:01:41,490
maybe it's easier for your boss or

00:01:36,990 --> 00:01:42,810
something and it doesn't look as geeky

00:01:41,490 --> 00:01:47,159
anymore

00:01:42,810 --> 00:01:49,950
and also we have folders for their

00:01:47,159 --> 00:01:52,590
sports and permissions for teams so

00:01:49,950 --> 00:01:57,450
these are all great features for I guess

00:01:52,590 --> 00:01:59,340
mature companies as well and the big

00:01:57,450 --> 00:02:01,530
thing about Cortana is that we're there

00:01:59,340 --> 00:02:05,420
with a single pane of glass to look at

00:02:01,530 --> 00:02:07,380
data from a lot of your data sources and

00:02:05,420 --> 00:02:09,840
Promethea is an influx have I mentioned

00:02:07,380 --> 00:02:11,640
in other talks already today we have we

00:02:09,840 --> 00:02:12,790
have a lot more these are our most

00:02:11,640 --> 00:02:17,890
popular ones

00:02:12,790 --> 00:02:19,720
and if those aren't enough we can also

00:02:17,890 --> 00:02:22,629
define your own data source you

00:02:19,720 --> 00:02:25,420
basically just have to implement those

00:02:22,629 --> 00:02:27,640
four functions or mostly just two top

00:02:25,420 --> 00:02:30,329
ones on how to query sampling and then

00:02:27,640 --> 00:02:33,849
whatever that returns needs to be trance

00:02:30,329 --> 00:02:36,670
transformed into the coroner's own table

00:02:33,849 --> 00:02:38,140
model or time series model or now also

00:02:36,670 --> 00:02:43,299
the logs model if you want to use

00:02:38,140 --> 00:02:45,310
logging as well and we've had we've seen

00:02:43,299 --> 00:02:49,390
pretty wild adoption there so there's

00:02:45,310 --> 00:02:51,519
people writing your eyes for hydropower

00:02:49,390 --> 00:02:54,010
plants for example and they can control

00:02:51,519 --> 00:02:56,170
everything they're just through like

00:02:54,010 --> 00:03:01,180
clicks of buttons within gravano

00:02:56,170 --> 00:03:03,129
and it's all like custom and once you've

00:03:01,180 --> 00:03:05,109
created and at once you've set up your

00:03:03,129 --> 00:03:09,090
data sources you can create their sports

00:03:05,109 --> 00:03:11,829
by by simply selecting a panel type and

00:03:09,090 --> 00:03:15,209
choosing your data source and then you

00:03:11,829 --> 00:03:19,329
can quickly see graphs nicely plotted

00:03:15,209 --> 00:03:21,639
and then once you have the graphs you

00:03:19,329 --> 00:03:29,650
can also define alerts on them alerts

00:03:21,639 --> 00:03:31,410
are panel based and and they are being

00:03:29,650 --> 00:03:33,970
evaluated on the graph on a server and

00:03:31,410 --> 00:03:35,620
this basically makes the monitoring

00:03:33,970 --> 00:03:37,419
story complete because you can see your

00:03:35,620 --> 00:03:38,980
you cannot only just see your graphs but

00:03:37,419 --> 00:03:42,069
you can also get you can also get

00:03:38,980 --> 00:03:45,430
alerted on it and Cortana then acts as

00:03:42,069 --> 00:03:48,430
the adapter between your data source and

00:03:45,430 --> 00:03:51,900
the notification providers that we also

00:03:48,430 --> 00:03:56,409
support so like slack or page of duty

00:03:51,900 --> 00:04:00,760
and another nice thing actually with the

00:03:56,409 --> 00:04:02,889
panel based alert definitions or that

00:04:00,760 --> 00:04:07,180
you have direct manipulation which means

00:04:02,889 --> 00:04:09,790
you can like just drag the threshold and

00:04:07,180 --> 00:04:11,560
then you can see how the threshold also

00:04:09,790 --> 00:04:16,510
relates to your historic data for

00:04:11,560 --> 00:04:21,219
example and what we've also seen is

00:04:16,510 --> 00:04:24,340
pretty good adoption so code of six to

00:04:21,219 --> 00:04:25,590
polled in the last two years and this is

00:04:24,340 --> 00:04:27,310
the number for active daily

00:04:25,590 --> 00:04:29,230
installations

00:04:27,310 --> 00:04:32,890
so basically once a day cortana does a

00:04:29,230 --> 00:04:36,190
check if there's a new release and and

00:04:32,890 --> 00:04:39,820
we yeah we we collect those numbers yon

00:04:36,190 --> 00:04:43,240
yeah and I also think that the number

00:04:39,820 --> 00:04:44,980
may be a bit on the reportage because

00:04:43,240 --> 00:04:51,160
probably some enterprises are blocking

00:04:44,980 --> 00:04:53,170
blocking the the update check so quick

00:04:51,160 --> 00:04:56,860
question here again who's still running

00:04:53,170 --> 00:04:58,720
for da 2x or earlier then maybe later

00:04:56,860 --> 00:05:06,940
I'm curious on what's keeping you from

00:04:58,720 --> 00:05:09,100
for upgrading 2 to 5 time okay yeah

00:05:06,940 --> 00:05:11,410
so let's quickly run through maybe some

00:05:09,100 --> 00:05:14,620
motivations on what you're missing out

00:05:11,410 --> 00:05:17,590
on so there's there's no heat map panel

00:05:14,620 --> 00:05:20,980
which is which is quite good to display

00:05:17,590 --> 00:05:23,050
high density latency data there's a

00:05:20,980 --> 00:05:26,290
great example also on there on how to

00:05:23,050 --> 00:05:28,090
how to query for it because you need to

00:05:26,290 --> 00:05:33,640
for Prometheus for example you need to

00:05:28,090 --> 00:05:36,810
maintain the TLT buckets and one else is

00:05:33,640 --> 00:05:39,850
new we have we've got new data sources

00:05:36,810 --> 00:05:42,700
my Microsoft sequel server is now a new

00:05:39,850 --> 00:05:47,470
internal one then Google stock driver is

00:05:42,700 --> 00:05:49,090
very recent flux from influx is an

00:05:47,470 --> 00:05:51,190
external data source it's still a bit

00:05:49,090 --> 00:05:54,520
better because it's also the language is

00:05:51,190 --> 00:05:56,500
still in flux and then we've added

00:05:54,520 --> 00:05:59,140
elastic search alerting and a new

00:05:56,500 --> 00:06:04,090
Postgres query builder we can just click

00:05:59,140 --> 00:06:05,320
a query together so another thing I'm

00:06:04,090 --> 00:06:07,480
quite excited about is the the

00:06:05,320 --> 00:06:10,960
provisioning story so you can configure

00:06:07,480 --> 00:06:13,480
your your dashboards and data sources

00:06:10,960 --> 00:06:15,970
not just by clicking around but also

00:06:13,480 --> 00:06:17,410
just by a code and then which then

00:06:15,970 --> 00:06:19,150
basically means you will have a text

00:06:17,410 --> 00:06:22,210
file that you can put into a version

00:06:19,150 --> 00:06:24,910
control and you can always recover if in

00:06:22,210 --> 00:06:29,650
case anything happens or someone messes

00:06:24,910 --> 00:06:32,740
up your are your dashboards and but one

00:06:29,650 --> 00:06:34,840
caveat there is if you're deploying grow

00:06:32,740 --> 00:06:36,580
fauna on kubernetes we actually

00:06:34,840 --> 00:06:41,060
encourage the use of config Maps

00:06:36,580 --> 00:06:45,000
and detect reloads there

00:06:41,060 --> 00:06:48,660
so last thing about the update is that

00:06:45,000 --> 00:06:51,750
your phone is now fully CI it so that

00:06:48,660 --> 00:06:54,389
means each new age new each merge to

00:06:51,750 --> 00:06:56,400
master previous docker image so you can

00:06:54,389 --> 00:07:00,150
always get the new the new features if

00:06:56,400 --> 00:07:02,340
you just use that that master tag and if

00:07:00,150 --> 00:07:04,800
you're relying on particular releases I

00:07:02,340 --> 00:07:07,710
encourage you to check out our docket

00:07:04,800 --> 00:07:11,760
hub and then just look for the the

00:07:07,710 --> 00:07:17,510
concrete build that you need also new we

00:07:11,760 --> 00:07:20,310
have arm and windows builds that hello

00:07:17,510 --> 00:07:23,460
okay can everyone can everyone's to hear

00:07:20,310 --> 00:07:27,570
me yeah okay that's good so yeah let's

00:07:23,460 --> 00:07:29,370
get on to the new stuff so a lot of core

00:07:27,570 --> 00:07:33,690
fan engineering is now revolved around

00:07:29,370 --> 00:07:38,010
this graphic which is what we think the

00:07:33,690 --> 00:07:40,169
the the normalized troubleshooting

00:07:38,010 --> 00:07:44,610
journey so basically you can alert and

00:07:40,169 --> 00:07:46,320
then afterwards you or from there there

00:07:44,610 --> 00:07:48,600
will be probably be a link in there to a

00:07:46,320 --> 00:07:51,150
dashboard so you look at your you look

00:07:48,600 --> 00:07:54,870
at your dashboards to kind of Zone in on

00:07:51,150 --> 00:07:58,800
where a new system the the problems are

00:07:54,870 --> 00:08:01,580
and from there you're gonna then quickly

00:07:58,800 --> 00:08:04,289
or you want to modify some queries of

00:08:01,580 --> 00:08:08,250
the charts that you've seen to kind of

00:08:04,289 --> 00:08:11,599
validate your hypothesis and from then

00:08:08,250 --> 00:08:14,580
on you want to also look at the logs to

00:08:11,599 --> 00:08:16,490
for your root cause analysis and then

00:08:14,580 --> 00:08:20,099
maybe also you want to look at traces

00:08:16,490 --> 00:08:22,080
and then then hopefully at that point we

00:08:20,099 --> 00:08:24,240
have before thorough understanding on

00:08:22,080 --> 00:08:27,570
what's going wrong and you can reach and

00:08:24,240 --> 00:08:28,889
roll out the fix yeah so everything is

00:08:27,570 --> 00:08:34,229
going to be revolving around this

00:08:28,889 --> 00:08:36,659
picture we are super fauna is also on

00:08:34,229 --> 00:08:39,270
that journey to basically make all of

00:08:36,659 --> 00:08:42,560
these available and today I guess I'm

00:08:39,270 --> 00:08:52,070
gonna focus on the ad-hoc queries and

00:08:42,560 --> 00:08:52,070
log aggregation so to illustrate this

00:08:52,690 --> 00:08:57,370
the problem is that for the for the at

00:08:55,750 --> 00:09:00,700
work very iteration you're sometimes

00:08:57,370 --> 00:09:01,990
stuck on the on the Cortana edit view

00:09:00,700 --> 00:09:05,800
and there you have all these

00:09:01,990 --> 00:09:07,690
distractions on for that pertain to the

00:09:05,800 --> 00:09:09,820
visualization but not to the actual

00:09:07,690 --> 00:09:11,140
query so a lot of the everything's that

00:09:09,820 --> 00:09:13,480
kind of everything that is kind of read

00:09:11,140 --> 00:09:17,770
there just gets in the way so that's a

00:09:13,480 --> 00:09:21,040
bit annoying and it just keeps you from

00:09:17,770 --> 00:09:23,560
like the creaky reiteration and we

00:09:21,040 --> 00:09:25,660
wondered we want to address this and we

00:09:23,560 --> 00:09:27,520
did some we did some wireframes some

00:09:25,660 --> 00:09:31,720
mock-ups and experimented and how this

00:09:27,520 --> 00:09:36,370
could be done and a more very focused

00:09:31,720 --> 00:09:40,600
view was whether it was a result of that

00:09:36,370 --> 00:09:44,110
exploration and so this new Explorer

00:09:40,600 --> 00:09:50,140
area is all about ephemeral data abuse

00:09:44,110 --> 00:09:54,280
and shareable shareable quickly

00:09:50,140 --> 00:09:56,740
shareable graphs so and like to this I

00:09:54,280 --> 00:09:59,080
guess also we wanted to add logging so

00:09:56,740 --> 00:10:00,760
this is going to be one one view so you

00:09:59,080 --> 00:10:04,240
can have your metrics and your log

00:10:00,760 --> 00:10:07,360
side-by-side to to basically support

00:10:04,240 --> 00:10:10,930
this story where you had your hypothesis

00:10:07,360 --> 00:10:13,390
and your your query iteration and you

00:10:10,930 --> 00:10:15,040
modified some queries and you have an

00:10:13,390 --> 00:10:20,140
idea on where where the problem could be

00:10:15,040 --> 00:10:21,700
and then if you look closely at the at

00:10:20,140 --> 00:10:24,220
the query for example there's a

00:10:21,700 --> 00:10:26,590
Prometheus selector in there for job

00:10:24,220 --> 00:10:29,710
equals AB 1 and then we're going to get

00:10:26,590 --> 00:10:33,990
the the the logs that pertain to that

00:10:29,710 --> 00:10:40,780
job so that's the ultimate goal and

00:10:33,990 --> 00:10:43,180
let's quickly see how far we get so what

00:10:40,780 --> 00:10:48,370
do you think yeah that works all right

00:10:43,180 --> 00:10:51,060
let me quickly bring this over to the

00:10:48,370 --> 00:10:51,060
other screen

00:11:07,580 --> 00:11:15,620
so I think that's gonna be good enough

00:11:13,420 --> 00:11:17,990
I'm just gonna make this a bit bigger so

00:11:15,620 --> 00:11:22,540
there's now there's no a new section

00:11:17,990 --> 00:11:24,740
within core fauna it's still it's still

00:11:22,540 --> 00:11:26,690
beyond a feature flag but it's called

00:11:24,740 --> 00:11:30,290
explore it brings you to this query

00:11:26,690 --> 00:11:31,910
centric view and the idea here is that

00:11:30,290 --> 00:11:34,579
now is everything it's everything

00:11:31,910 --> 00:11:36,050
revolves around the queries and the

00:11:34,579 --> 00:11:39,320
first integration we have is with

00:11:36,050 --> 00:11:42,440
Prometheus so and there were two goals

00:11:39,320 --> 00:11:45,670
we wanted to make it really really easy

00:11:42,440 --> 00:11:49,430
to iterate quickly for our professionals

00:11:45,670 --> 00:11:51,380
through the queries but also you want to

00:11:49,430 --> 00:11:53,540
make it simple for or easier for

00:11:51,380 --> 00:11:55,430
beginners Chris Prometheus has a bit of

00:11:53,540 --> 00:11:57,260
a learning curve and for that for

00:11:55,430 --> 00:12:00,500
example we added a little metrics

00:11:57,260 --> 00:12:01,910
Explorer here so there's the metrics

00:12:00,500 --> 00:12:04,220
that are on your system they're grouped

00:12:01,910 --> 00:12:06,550
by their group by prefix so you can just

00:12:04,220 --> 00:12:10,959
click Li quickly click on one and then

00:12:06,550 --> 00:12:13,610
it'll just notice graph them for you and

00:12:10,959 --> 00:12:18,140
there's some these are they assume to

00:12:13,610 --> 00:12:22,160
help us here so the interesting ones for

00:12:18,140 --> 00:12:24,529
example if I run a query like this so

00:12:22,160 --> 00:12:26,839
the total suffix for example if you know

00:12:24,529 --> 00:12:29,570
Prometheus a bit more it's a counter and

00:12:26,839 --> 00:12:32,360
so it's just numbers then in that

00:12:29,570 --> 00:12:33,589
increase and the the the value here is

00:12:32,360 --> 00:12:35,779
really have a low it doesn't really tell

00:12:33,589 --> 00:12:37,490
me much but we detect this and then we

00:12:35,779 --> 00:12:39,980
say okay well this is should probably

00:12:37,490 --> 00:12:41,930
have a rate to make it to make a bit

00:12:39,980 --> 00:12:45,200
more meaning out of this and then these

00:12:41,930 --> 00:12:47,450
query ins they they can they'll allow

00:12:45,200 --> 00:12:49,850
you to quickly fix the queries and then

00:12:47,450 --> 00:12:53,630
you have more meaningful meaningful data

00:12:49,850 --> 00:12:56,899
there this extends to other things as

00:12:53,630 --> 00:13:00,470
well so for example things with buckets

00:12:56,899 --> 00:13:02,899
so then it's it's obviously okay you

00:13:00,470 --> 00:13:04,220
probably don't want all the bucket

00:13:02,899 --> 00:13:07,130
results what you actually want is a

00:13:04,220 --> 00:13:12,019
histogram so we write out that histogram

00:13:07,130 --> 00:13:14,990
query for you so yeah cuz I think those

00:13:12,019 --> 00:13:17,029
are always a bit hard to to remember and

00:13:14,990 --> 00:13:19,250
then the tab completion has a lot more

00:13:17,029 --> 00:13:20,980
features so for example the history of

00:13:19,250 --> 00:13:23,500
the last queries you

00:13:20,980 --> 00:13:29,010
you did also function function to help

00:13:23,500 --> 00:13:36,089
us here on what those functions mean yep

00:13:29,010 --> 00:13:43,089
okay and then as I demoed in the in the

00:13:36,089 --> 00:13:44,740
in a split view there is there is a

00:13:43,089 --> 00:13:46,060
damage earlier there's a split view so

00:13:44,740 --> 00:13:48,940
you can you can for example compare

00:13:46,060 --> 00:13:54,940
different installations or like fraud

00:13:48,940 --> 00:13:56,050
and fraud and def for example so now I

00:13:54,940 --> 00:13:58,209
have two different prometheus

00:13:56,050 --> 00:14:00,790
installations and I compared the the

00:13:58,209 --> 00:14:05,829
queries they're side-by-side app is not

00:14:00,790 --> 00:14:08,399
an exciting I guess but yeah yeah it's

00:14:05,829 --> 00:14:08,399
more exciting

00:14:14,360 --> 00:14:21,110
okay yeah so so you can you can you can

00:14:17,749 --> 00:14:25,489
quickly see a difference then you just

00:14:21,110 --> 00:14:27,679
close that again we also have is click

00:14:25,489 --> 00:14:30,709
to filter so if you're familiar with the

00:14:27,679 --> 00:14:32,869
Prometheus or if the Prometheus UI the

00:14:30,709 --> 00:14:34,670
classic one you also have the console

00:14:32,869 --> 00:14:36,769
there which quicks which gives you an

00:14:34,670 --> 00:14:39,519
idea on the label space that a metric

00:14:36,769 --> 00:14:41,989
supports and here for example this one

00:14:39,519 --> 00:14:47,269
doesn't have that many labels but if I

00:14:41,989 --> 00:14:49,360
wanted to break down I just changed over

00:14:47,269 --> 00:14:51,619
to this one that's more exciting see I

00:14:49,360 --> 00:14:54,499
what I just did now was I changed the

00:14:51,619 --> 00:14:55,489
data source but it maintained the query

00:14:54,499 --> 00:14:57,649
which is quite nice

00:14:55,489 --> 00:15:00,529
so I can quickly also compare like that

00:14:57,649 --> 00:15:01,999
and then here I see the label space and

00:15:00,529 --> 00:15:04,279
I can just click on one of these here

00:15:01,999 --> 00:15:04,819
and then it'll it'll change the query up

00:15:04,279 --> 00:15:08,869
there for me

00:15:04,819 --> 00:15:11,389
so that now I only look at the memory

00:15:08,869 --> 00:15:15,790
allocation for that we needed a lot

00:15:11,389 --> 00:15:22,069
manager jobs okay so this all works then

00:15:15,790 --> 00:15:25,239
I guess the new part is logging logging

00:15:22,069 --> 00:15:27,799
is also a new built-in data source but

00:15:25,239 --> 00:15:30,889
on how does all works I'm going to show

00:15:27,799 --> 00:15:33,350
bit after but we basically have a

00:15:30,889 --> 00:15:39,220
similar a similar approach here to

00:15:33,350 --> 00:15:42,169
Prometheus because the way the way

00:15:39,220 --> 00:15:44,449
Griffin is looking at logging streams is

00:15:42,169 --> 00:15:46,850
similar to how Prometheus

00:15:44,449 --> 00:15:49,540
looks at metrics so everything is

00:15:46,850 --> 00:15:53,089
organized by labels so you also have

00:15:49,540 --> 00:15:57,610
sort of jobs and apps and instances and

00:15:53,089 --> 00:16:02,179
this is how your block streams are are

00:15:57,610 --> 00:16:04,999
are grouped and then you can just you

00:16:02,179 --> 00:16:06,860
can just query by using a selector to

00:16:04,999 --> 00:16:08,869
say give me all the give me all the

00:16:06,860 --> 00:16:11,089
Cassandra locks for example and then you

00:16:08,869 --> 00:16:12,559
see a quick overview of this one

00:16:11,089 --> 00:16:14,269
currently just returns in frozen

00:16:12,559 --> 00:16:17,929
warnings so we see a bit of a

00:16:14,269 --> 00:16:21,110
distribution there and then and then I

00:16:17,929 --> 00:16:25,309
can just scroll down here and then here

00:16:21,110 --> 00:16:26,899
I have additional labels for each of

00:16:25,309 --> 00:16:27,740
those lines right so for example I can

00:16:26,899 --> 00:16:29,300
again

00:16:27,740 --> 00:16:31,670
see okay this is the instance that this

00:16:29,300 --> 00:16:33,530
particular line came from right so as I

00:16:31,670 --> 00:16:38,330
would expect in in a log aggregation

00:16:33,530 --> 00:16:43,630
system and then if I want to Zone in on

00:16:38,330 --> 00:16:49,840
this I can write so so this all works

00:16:43,630 --> 00:16:55,690
this stage is not changed all right so

00:16:49,840 --> 00:16:55,690
this is how far we've we've got so far

00:16:56,320 --> 00:17:02,930
just quickly return to this so let me

00:17:00,110 --> 00:17:06,080
just talk a bit more about the all the

00:17:02,930 --> 00:17:08,420
idea behind this so the first or the

00:17:06,080 --> 00:17:11,180
first the very first goal that we had

00:17:08,420 --> 00:17:13,790
for this is to just keep it just just

00:17:11,180 --> 00:17:17,930
keep things simple because we've we've

00:17:13,790 --> 00:17:20,030
seen other or we've personally used

00:17:17,930 --> 00:17:22,100
other solutions where a lot of things

00:17:20,030 --> 00:17:25,670
just get in the way and it's it was

00:17:22,100 --> 00:17:28,120
quite complex to to set up and and also

00:17:25,670 --> 00:17:33,440
they had quite a lot of storage

00:17:28,120 --> 00:17:34,940
requirements which probably is although

00:17:33,440 --> 00:17:37,280
I mean the reason for that for that is

00:17:34,940 --> 00:17:39,770
that the index a lot of things and so we

00:17:37,280 --> 00:17:43,460
wanted to have something that doesn't do

00:17:39,770 --> 00:17:45,650
this and crapping is quite powerful in

00:17:43,460 --> 00:17:48,260
itself so that should just be something

00:17:45,650 --> 00:17:51,770
that is that is supported and we're

00:17:48,260 --> 00:17:53,870
currently approaching everything from

00:17:51,770 --> 00:17:55,760
the belief that that that's the most

00:17:53,870 --> 00:17:57,110
important thing and the for the rest

00:17:55,760 --> 00:17:58,640
that for the rest of the features that

00:17:57,110 --> 00:18:01,100
we want we're going to be really careful

00:17:58,640 --> 00:18:02,990
and what we adds because we ultimately

00:18:01,100 --> 00:18:06,080
you want to keep everything

00:18:02,990 --> 00:18:09,680
cost-effective so and the problem is

00:18:06,080 --> 00:18:12,440
when you when you don't index anything

00:18:09,680 --> 00:18:15,470
everything will just be will just be one

00:18:12,440 --> 00:18:17,690
will just be one stream of log files and

00:18:15,470 --> 00:18:19,900
that's I mean that's problematic

00:18:17,690 --> 00:18:22,760
there's problematic as well so

00:18:19,900 --> 00:18:24,500
especially in community in the

00:18:22,760 --> 00:18:27,200
kubernetes world where you now have

00:18:24,500 --> 00:18:28,970
little independent workloads and they

00:18:27,200 --> 00:18:31,040
all have they all don't have individual

00:18:28,970 --> 00:18:32,120
streams of data and they all flow

00:18:31,040 --> 00:18:34,940
together and then it's completely

00:18:32,120 --> 00:18:37,220
unclear on what came from where and so

00:18:34,940 --> 00:18:39,380
this so there is something that needs to

00:18:37,220 --> 00:18:41,090
be maintained from the log streams and

00:18:39,380 --> 00:18:44,570
for us though for

00:18:41,090 --> 00:18:47,450
that's that's the that's the label story

00:18:44,570 --> 00:18:48,890
that is similar to Prometheus so

00:18:47,450 --> 00:18:52,520
essentially what we're trying to do is

00:18:48,890 --> 00:18:57,890
doing a very Prometheus style logging

00:18:52,520 --> 00:19:00,260
solution and actually kubernetes gets

00:18:57,890 --> 00:19:03,200
you quite far because because you

00:19:00,260 --> 00:19:06,740
because it does collect locks for you on

00:19:03,200 --> 00:19:08,060
the nodes but you I mean if the if a

00:19:06,740 --> 00:19:10,400
node dies you have a problem right

00:19:08,060 --> 00:19:13,040
because then the locks are gone and

00:19:10,400 --> 00:19:14,740
usually this is when you when you need

00:19:13,040 --> 00:19:19,220
the loss to find out what happened so

00:19:14,740 --> 00:19:20,780
yeah I guess the idea there is that log

00:19:19,220 --> 00:19:23,960
aggregation is a good solution after all

00:19:20,780 --> 00:19:28,040
and the way we want to approach this is

00:19:23,960 --> 00:19:31,160
by by not just being Prometheus tile but

00:19:28,040 --> 00:19:33,740
actually using the same labeling logic

00:19:31,160 --> 00:19:35,330
so you would you would write Yamla files

00:19:33,740 --> 00:19:40,580
in the same way as you do for Prometheus

00:19:35,330 --> 00:19:42,740
to to map for example Promethea

00:19:40,580 --> 00:19:46,820
kubernetes logging community support

00:19:42,740 --> 00:19:52,010
labels to to app labels for example like

00:19:46,820 --> 00:19:54,770
we've seen in the example and and these

00:19:52,010 --> 00:19:57,710
and these are then all indexed so that

00:19:54,770 --> 00:20:00,260
you can sort it so that the system when

00:19:57,710 --> 00:20:03,560
you query you can you can filter those

00:20:00,260 --> 00:20:05,540
streams apart and the final architecture

00:20:03,560 --> 00:20:07,280
of the solution looks like this you will

00:20:05,540 --> 00:20:11,240
have a logging agent that will run on

00:20:07,280 --> 00:20:13,850
your nodes as a daemon set and that's

00:20:11,240 --> 00:20:17,210
going to attach itself to all the logs

00:20:13,850 --> 00:20:20,750
and to all the to all the outputs of the

00:20:17,210 --> 00:20:24,980
port's for example and this basically

00:20:20,750 --> 00:20:28,150
requires you to have you apps in the 12

00:20:24,980 --> 00:20:33,080
factor app model where where an app just

00:20:28,150 --> 00:20:36,830
spits out everything that alongs that

00:20:33,080 --> 00:20:38,860
being said kubernetes won't be a strict

00:20:36,830 --> 00:20:42,080
requirement to use core final logging um

00:20:38,860 --> 00:20:44,750
we're just gonna you will just have to

00:20:42,080 --> 00:20:47,420
do some some more configuring to set up

00:20:44,750 --> 00:20:49,700
the agent and then agent is just for

00:20:47,420 --> 00:20:52,000
water for real extremes to the logging

00:20:49,700 --> 00:20:54,350
servers and their logging service then

00:20:52,000 --> 00:20:56,510
takes care of the storage

00:20:54,350 --> 00:20:59,630
so in I guess if you want to deploy it

00:20:56,510 --> 00:21:02,510
on your systems you will have to take

00:20:59,630 --> 00:21:04,970
care of this or Agrafena is also going

00:21:02,510 --> 00:21:10,460
to offer a hosted service for this login

00:21:04,970 --> 00:21:12,080
as well yeah so there's a lot of -

00:21:10,460 --> 00:21:15,530
there's a lot of things to do still in

00:21:12,080 --> 00:21:18,380
there just around breakfast we had some

00:21:15,530 --> 00:21:21,020
discussions on what is important to

00:21:18,380 --> 00:21:24,200
people so at the duping logic seems to

00:21:21,020 --> 00:21:25,730
be important then internally we also

00:21:24,200 --> 00:21:29,500
think that what could be really powerful

00:21:25,730 --> 00:21:32,360
is a pattern matching engine that also

00:21:29,500 --> 00:21:34,250
emits time series again which can then

00:21:32,360 --> 00:21:37,130
be fed into your prometheus

00:21:34,250 --> 00:21:42,020
so that based on the logs you will have

00:21:37,130 --> 00:21:44,419
time series them and then and then on

00:21:42,020 --> 00:21:45,700
those you could then define alerts just

00:21:44,419 --> 00:21:48,559
as you would do with any other

00:21:45,700 --> 00:21:49,820
Prometheus time series what's also

00:21:48,559 --> 00:21:52,429
interesting what we're thinking about is

00:21:49,820 --> 00:21:55,520
maybe triggers or web hooks on certain

00:21:52,429 --> 00:21:57,770
pattern matches and then yeah we want to

00:21:55,520 --> 00:22:04,010
do everything as cost-effective as

00:21:57,770 --> 00:22:06,409
possible so yeah this is one beta if you

00:22:04,010 --> 00:22:08,659
if you if you're really interested in

00:22:06,409 --> 00:22:12,740
this we're going to we're going to be

00:22:08,659 --> 00:22:17,240
releasing the the OSS beta next month

00:22:12,740 --> 00:22:20,390
actually and IP curious to hear like

00:22:17,240 --> 00:22:21,620
feedback and maybe maybe there's some

00:22:20,390 --> 00:22:23,330
things and you're in your current

00:22:21,620 --> 00:22:24,830
logging solution that you really care

00:22:23,330 --> 00:22:28,220
about and that you would like to see

00:22:24,830 --> 00:22:33,309
also in a slightly more cost-effective

00:22:28,220 --> 00:22:38,450
solution but supported by go phone oh

00:22:33,309 --> 00:22:41,090
yeah also what I showed earlier for for

00:22:38,450 --> 00:22:43,280
Prometheus itself the Explorer UI is

00:22:41,090 --> 00:22:46,250
still in beta but it's behind a feature

00:22:43,280 --> 00:22:51,309
flag so you can enable this today if you

00:22:46,250 --> 00:22:53,950
use a recent release like 5.3 but I'm

00:22:51,309 --> 00:22:56,870
adding new features there every day so

00:22:53,950 --> 00:22:59,150
basically if you use the if you use the

00:22:56,870 --> 00:23:01,549
master release you will always get the

00:22:59,150 --> 00:23:03,590
you always get the latest the latest

00:23:01,549 --> 00:23:08,420
stuff there and then we're hoping to

00:23:03,590 --> 00:23:13,910
release everything in February 2019

00:23:08,420 --> 00:23:15,050
yes so and then with that let's talk a

00:23:13,910 --> 00:23:18,020
bit more about what else we're working

00:23:15,050 --> 00:23:21,710
on yet the Explorer UI needs to be a bit

00:23:18,020 --> 00:23:23,000
more refined also some performance

00:23:21,710 --> 00:23:27,440
improvements that's going to be in my

00:23:23,000 --> 00:23:29,120
December and and some in some new

00:23:27,440 --> 00:23:31,460
interesting panels are coming up the

00:23:29,120 --> 00:23:33,080
multi-step panel that's going to be

00:23:31,460 --> 00:23:35,780
added in addition to the single step

00:23:33,080 --> 00:23:37,700
panel panel where you can which

00:23:35,780 --> 00:23:39,710
basically gives you a nicer way to to

00:23:37,700 --> 00:23:42,020
group to group

00:23:39,710 --> 00:23:43,940
single stat metrics of a certain kind

00:23:42,020 --> 00:23:47,660
and put them in relation to one another

00:23:43,940 --> 00:23:50,510
and then there's going to be a new way

00:23:47,660 --> 00:23:52,670
to select the visualization of a panel

00:23:50,510 --> 00:23:55,150
itself so that you don't have to decide

00:23:52,670 --> 00:23:58,100
in advance that I want a line panel but

00:23:55,150 --> 00:24:00,170
you just want a graph panel and then you

00:23:58,100 --> 00:24:03,680
can like quickly go through these

00:24:00,170 --> 00:24:06,590
visualizations to see which one is best

00:24:03,680 --> 00:24:08,690
for the data that you have and then

00:24:06,590 --> 00:24:11,240
we're also going to be supporting all

00:24:08,690 --> 00:24:13,850
three major cloud providers and their

00:24:11,240 --> 00:24:17,540
and their export metrics so how's it

00:24:13,850 --> 00:24:19,280
going to be internal data sources and

00:24:17,540 --> 00:24:22,820
then another thing I'm really excited

00:24:19,280 --> 00:24:24,860
about is the github story on which

00:24:22,820 --> 00:24:27,860
basically goes hand in hand with the

00:24:24,860 --> 00:24:30,170
provisioning API where once your

00:24:27,860 --> 00:24:33,380
dashboards are provisioned through files

00:24:30,170 --> 00:24:35,540
then what happens when you when you

00:24:33,380 --> 00:24:39,680
modify them right like where should that

00:24:35,540 --> 00:24:40,820
change go and yeah there's open there's

00:24:39,680 --> 00:24:42,370
an open issue there where we're

00:24:40,820 --> 00:24:46,520
collecting feedback right now and

00:24:42,370 --> 00:24:49,280
another another similar one is reference

00:24:46,520 --> 00:24:52,970
panels which means you can define a

00:24:49,280 --> 00:24:55,400
panel that basically changes how for

00:24:52,970 --> 00:24:57,320
example a line graph should look across

00:24:55,400 --> 00:25:02,600
your organization so that people don't

00:24:57,320 --> 00:25:04,640
go and modify their own graphs and they

00:25:02,600 --> 00:25:08,960
all they all don't look the same anymore

00:25:04,640 --> 00:25:12,110
and people get lost yeah one last thing

00:25:08,960 --> 00:25:16,010
our co-founder cone is happening it's in

00:25:12,110 --> 00:25:18,140
Los Angeles if anyone wants to come and

00:25:16,010 --> 00:25:20,120
hang out there it's going to be at the

00:25:18,140 --> 00:25:20,870
Paramount Studios I think it's going to

00:25:20,120 --> 00:25:27,039
be fun

00:25:20,870 --> 00:25:30,010
it's in February yeah and with that

00:25:27,039 --> 00:25:34,120
thanks for listening

00:25:30,010 --> 00:25:34,120
so there's any questions

00:25:39,860 --> 00:25:47,780
so are there questions in the room so we

00:25:43,260 --> 00:25:47,780
have time for questions still left so

00:25:57,650 --> 00:26:05,250
how do you start locking data are they

00:26:02,610 --> 00:26:09,299
are locking data is stored inside the

00:26:05,250 --> 00:26:13,549
time series TP or is there a way to

00:26:09,299 --> 00:26:17,970
export or forward it to our lock syslog

00:26:13,549 --> 00:26:20,159
machineries or systems yeah yeah no the

00:26:17,970 --> 00:26:21,960
short answer is no I think I think the

00:26:20,159 --> 00:26:28,049
log the logging is going to be stored

00:26:21,960 --> 00:26:29,669
into RAW format and the the metadata the

00:26:28,049 --> 00:26:31,710
labels they're going to be stored in

00:26:29,669 --> 00:26:35,370
inverted index so we can do we can do

00:26:31,710 --> 00:26:36,840
the quick filtering yeah but there I

00:26:35,370 --> 00:26:40,309
think that I think there won't be any

00:26:36,840 --> 00:26:41,429
particular storage requirement for this

00:26:40,309 --> 00:26:44,850
later

00:26:41,429 --> 00:26:46,980
yeah but also I'm not really sure if

00:26:44,850 --> 00:26:48,870
there's going to be any support for

00:26:46,980 --> 00:26:49,650
other logging data that you bring in

00:26:48,870 --> 00:26:53,600
yeah

00:26:49,650 --> 00:26:53,600
so that's a bit unclear at this moment

00:26:54,620 --> 00:27:02,720
okay more questions yeah do you plan to

00:26:58,710 --> 00:27:06,270
support open matrix events as well

00:27:02,720 --> 00:27:08,990
support what open matrix events open

00:27:06,270 --> 00:27:17,730
matrix events I'm guessing we might

00:27:08,990 --> 00:27:20,640
since we just hired Reggie so in the

00:27:17,730 --> 00:27:22,710
open metrics initiative so yeah I think

00:27:20,640 --> 00:27:27,929
he's going to make sure that we probably

00:27:22,710 --> 00:27:34,590
will yeah more questions so there's

00:27:27,929 --> 00:27:37,049
still time left forum no okay then davit

00:27:34,590 --> 00:27:39,480
thank you for this interesting talk so

00:27:37,049 --> 00:27:42,660
thanks for being here so here's a little

00:27:39,480 --> 00:27:49,550
present for you and thank you so thanks

00:27:42,660 --> 00:28:01,390
[Applause]

00:27:49,550 --> 00:28:01,390

YouTube URL: https://www.youtube.com/watch?v=vDXAVZY7rB0


