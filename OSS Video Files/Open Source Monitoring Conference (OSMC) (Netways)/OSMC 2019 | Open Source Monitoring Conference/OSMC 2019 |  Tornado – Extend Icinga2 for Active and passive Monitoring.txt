Title: OSMC 2019 |  Tornado – Extend Icinga2 for Active and passive Monitoring
Publication date: 2019-11-18
Playlist: OSMC 2019 | Open Source Monitoring Conference
Description: 
	of complex heterogeneous IT Environments by Francesco Cina & Patrick Zambelli

The main objective of this talk is to show how you can extend an Icinga2 active monitoring with a passive monitoring engine.
With Icinga2 you focus on active monitoring. With Tornado you can do exactly the opposite and focus on passive monitoring. You receive events from different channels like SNMP Trap, Syslog, Email and match them against a rule engine and decide which action to associate. A very common use case is to set a Icinga2 Service status (critical, warning, ok) based on a matched rule. In addition you could also subscribe the Icinga API Stream and define matching rules which you would like to correlate and associate an executor for example create a new entry in Elasticsearch. Another common use case could be to register Tornado as a webhook for example in Elasticsearch Watcher collect the alarms and set the status on a Icinga Service Check. During the talk I will explain why Tornado was built from Würth Phoenix in rust and what are the common use case we would like to address with it.


NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de/
Blog: http://blog.netways.de/
NWS: https://nws.netways.de

Webinare
Archiv Link: https://www.netways.de/webinare/archi...
Aktuell: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh/

Musik: FRAMETRAXX
Captions: 
	00:00:13,849 --> 00:00:18,960
yeah thank you and welcome to the

00:00:17,310 --> 00:00:22,710
presentation of the event processing

00:00:18,960 --> 00:00:24,900
engine tornado I'm also happy to welcome

00:00:22,710 --> 00:00:27,199
here next to me the core developer of

00:00:24,900 --> 00:00:31,679
data and the engine itself

00:00:27,199 --> 00:00:34,620
Francesco Cena and he in action then in

00:00:31,679 --> 00:00:38,100
a few in a few moments just a few words

00:00:34,620 --> 00:00:40,140
about me so I am Patrick Zambelli I'm

00:00:38,100 --> 00:00:42,390
working for monitoring project so I am

00:00:40,140 --> 00:00:45,360
at a consultant site implementing

00:00:42,390 --> 00:00:48,840
monitoring projects for your clinics

00:00:45,360 --> 00:00:51,510
it's about a decade that I am active in

00:00:48,840 --> 00:00:54,690
this world for monitoring first starting

00:00:51,510 --> 00:00:58,320
by implementing modules developing code

00:00:54,690 --> 00:01:01,020
based on Nagios solution and starting

00:00:58,320 --> 00:01:04,140
and later in the consulting area when I

00:01:01,020 --> 00:01:08,580
guess and now only Singha - we are also

00:01:04,140 --> 00:01:10,619
partners of a singer and my classical

00:01:08,580 --> 00:01:14,939
installations of customer side is

00:01:10,619 --> 00:01:16,920
various various size reaching up to

00:01:14,939 --> 00:01:19,320
10,000 hosts and as you might know a

00:01:16,920 --> 00:01:21,090
couple of more services so I have a

00:01:19,320 --> 00:01:23,220
variegated list of active monitoring

00:01:21,090 --> 00:01:25,770
passive monitoring and always a long

00:01:23,220 --> 00:01:27,540
list of to dos which is coming in hey

00:01:25,770 --> 00:01:30,750
monitor this new device I have other

00:01:27,540 --> 00:01:33,930
services events events events whatever

00:01:30,750 --> 00:01:36,390
is coming from applications just a few

00:01:33,930 --> 00:01:38,579
words about the Phoenix we are a

00:01:36,390 --> 00:01:40,740
consulting company of the broad group we

00:01:38,579 --> 00:01:44,549
are located in the Alps in South Tyrol

00:01:40,740 --> 00:01:46,170
Italy and as I said we are partner of a

00:01:44,549 --> 00:01:50,130
singer we are doing IT operation

00:01:46,170 --> 00:01:51,450
projects and next to our unit as there

00:01:50,130 --> 00:01:53,790
are other unique doing also business

00:01:51,450 --> 00:01:57,689
software we are doing European and CRM

00:01:53,790 --> 00:02:01,250
projects and partner of also other

00:01:57,689 --> 00:02:04,829
software's we are installing this time

00:02:01,250 --> 00:02:08,039
so for today the agenda I would like to

00:02:04,829 --> 00:02:10,610
have give you a view of the challenges I

00:02:08,039 --> 00:02:12,860
am facing during implementation of

00:02:10,610 --> 00:02:15,320
monitoring projects

00:02:12,860 --> 00:02:17,690
and especially argue why we need the

00:02:15,320 --> 00:02:20,180
polling approach and event-driven

00:02:17,690 --> 00:02:23,150
approach for the implementation of the

00:02:20,180 --> 00:02:25,610
projects and finally also leading to our

00:02:23,150 --> 00:02:29,150
decision to implement a tornado as this

00:02:25,610 --> 00:02:31,730
engine in the second part my colleague

00:02:29,150 --> 00:02:34,010
Francesco will introduce about the

00:02:31,730 --> 00:02:36,800
design decisions the architecture of a

00:02:34,010 --> 00:02:39,070
tornado and we will conclude with a

00:02:36,800 --> 00:02:43,220
short presentation and the demo of this

00:02:39,070 --> 00:02:47,470
so just to start in my experience at the

00:02:43,220 --> 00:02:50,180
times of this Inga also nagas before

00:02:47,470 --> 00:02:51,770
traditional project starts with the

00:02:50,180 --> 00:02:55,310
poling approach so I can simply go to

00:02:51,770 --> 00:02:58,790
our customer deploy the various checks

00:02:55,310 --> 00:03:00,170
and get really started with such a

00:02:58,790 --> 00:03:03,020
monitoring in an easy way

00:03:00,170 --> 00:03:05,090
so we can say polling is an approach

00:03:03,020 --> 00:03:07,220
where I schedule at a static time

00:03:05,090 --> 00:03:10,310
interval so I have time periods I have

00:03:07,220 --> 00:03:12,709
checked frequencies the monitoring and I

00:03:10,310 --> 00:03:16,790
get a specific state so this means I

00:03:12,709 --> 00:03:18,680
have a well-defined result I know which

00:03:16,790 --> 00:03:21,650
is the system I am going to monitor I'm

00:03:18,680 --> 00:03:23,810
connecting to and I can therefore create

00:03:21,650 --> 00:03:26,060
already configurations with a good for

00:03:23,810 --> 00:03:30,110
the result I'm receiving back and I can

00:03:26,060 --> 00:03:32,000
derive this is good bad I love and great

00:03:30,110 --> 00:03:35,000
also graphs if I have performance and

00:03:32,000 --> 00:03:36,950
trying to visualize another statement is

00:03:35,000 --> 00:03:38,930
it's a centralized configuration this

00:03:36,950 --> 00:03:40,940
means I am starting with my monitoring

00:03:38,930 --> 00:03:43,700
system at the center and I'm

00:03:40,940 --> 00:03:46,430
distributing this to the monitored host

00:03:43,700 --> 00:03:49,040
a examples here is the agent based

00:03:46,430 --> 00:03:53,630
monitoring with very popular and it's

00:03:49,040 --> 00:03:55,489
client with which is very popular and we

00:03:53,630 --> 00:03:58,820
have other good features still in

00:03:55,489 --> 00:04:01,670
arrived yesterday agent let's approach

00:03:58,820 --> 00:04:04,970
we SSH used on Linux and UNIX systems

00:04:01,670 --> 00:04:07,670
the same agentless approach is working

00:04:04,970 --> 00:04:10,760
via SNMP for especially network services

00:04:07,670 --> 00:04:15,170
or general embedded systems where I can

00:04:10,760 --> 00:04:17,750
just study the documentation and address

00:04:15,170 --> 00:04:20,120
the specific ID to get the count of

00:04:17,750 --> 00:04:22,720
status of a network device and the

00:04:20,120 --> 00:04:25,099
traffic which is going over this

00:04:22,720 --> 00:04:26,180
generally we say it's the historical

00:04:25,099 --> 00:04:27,590
approach it was

00:04:26,180 --> 00:04:30,560
and finding a guess it's still very good

00:04:27,590 --> 00:04:34,009
any singer so we can still say this is

00:04:30,560 --> 00:04:35,330
very good the contrary parody is the

00:04:34,009 --> 00:04:37,669
event based approach

00:04:35,330 --> 00:04:39,560
I accept matrix at all the time so the

00:04:37,669 --> 00:04:42,139
remote system is just saying I have a

00:04:39,560 --> 00:04:43,970
problem and I just need to understand

00:04:42,139 --> 00:04:47,210
the incoming channel so the protocol

00:04:43,970 --> 00:04:49,310
which this system is speaking so I don't

00:04:47,210 --> 00:04:51,470
need to know always in advance advance

00:04:49,310 --> 00:04:54,860
what is coming but I need just to

00:04:51,470 --> 00:04:58,610
understand it so popular example is here

00:04:54,860 --> 00:05:00,889
the SNMP trap which is spoken by most

00:04:58,610 --> 00:05:04,900
network devices I can just enable it and

00:05:00,889 --> 00:05:07,010
it's sending such a messages email

00:05:04,900 --> 00:05:08,330
implementable by most application

00:05:07,010 --> 00:05:10,449
service I can stay on the backup

00:05:08,330 --> 00:05:13,130
software hey send me a report each day

00:05:10,449 --> 00:05:16,820
it was there a failure on my last backup

00:05:13,130 --> 00:05:19,010
last night and I can extract value if

00:05:16,820 --> 00:05:21,410
there is something going wrong switch

00:05:19,010 --> 00:05:23,300
look like out it locks logins or

00:05:21,410 --> 00:05:25,550
application errors I can collect from

00:05:23,300 --> 00:05:28,009
Windows systems via the event log via

00:05:25,550 --> 00:05:30,650
agent I can forward it to a collecting

00:05:28,009 --> 00:05:32,930
system or also via Linux it's in a

00:05:30,650 --> 00:05:36,370
message log or the systemctl I can

00:05:32,930 --> 00:05:39,650
follow it also to our central monitoring

00:05:36,370 --> 00:05:42,020
new protocols telemetry for example

00:05:39,650 --> 00:05:46,039
streaming of data is coming on the Cisco

00:05:42,020 --> 00:05:50,680
devices which has the idea to stand at

00:05:46,039 --> 00:05:53,479
more frequent interval status metric and

00:05:50,680 --> 00:05:56,210
performance data net flow in the world

00:05:53,479 --> 00:05:59,330
of network traffic monitoring I can

00:05:56,210 --> 00:06:02,000
derive who is talking with another

00:05:59,330 --> 00:06:05,870
device that first example implemented by

00:06:02,000 --> 00:06:08,599
Cisco and I can extract also here from

00:06:05,870 --> 00:06:10,099
this type of event which kind of

00:06:08,599 --> 00:06:13,310
communication is appearing in my network

00:06:10,099 --> 00:06:17,630
I poke another very generic way to

00:06:13,310 --> 00:06:21,139
subscribe to events from any kind of web

00:06:17,630 --> 00:06:22,940
based protocol communication I can

00:06:21,139 --> 00:06:24,650
implement by my application and

00:06:22,940 --> 00:06:27,020
developing developing in the

00:06:24,650 --> 00:06:29,389
organization and therefore it's quite a

00:06:27,020 --> 00:06:33,050
nice way to do it also in today's

00:06:29,389 --> 00:06:36,349
projects so we can say this is exactly

00:06:33,050 --> 00:06:39,560
the moment when an event happens I will

00:06:36,349 --> 00:06:39,860
get informed about this so we have to

00:06:39,560 --> 00:06:42,830
power

00:06:39,860 --> 00:06:45,290
and each of those have advantages

00:06:42,830 --> 00:06:48,080
disadvantages just to make it comparing

00:06:45,290 --> 00:06:50,450
it shortly the advantage is I can

00:06:48,080 --> 00:06:53,120
control when a check should be executed

00:06:50,450 --> 00:06:55,130
in case of polling I get exactly the

00:06:53,120 --> 00:06:57,320
information I'm interested in and I

00:06:55,130 --> 00:07:00,110
should know the context of the system

00:06:57,320 --> 00:07:02,930
I'm interacting with the disadvantage I

00:07:00,110 --> 00:07:05,170
could say it's a static configuration we

00:07:02,930 --> 00:07:07,640
have heard also multiple times this day

00:07:05,170 --> 00:07:08,990
architectures are changing and our

00:07:07,640 --> 00:07:10,760
monitoring needs to adapt to this

00:07:08,990 --> 00:07:13,070
especially when I have micro services

00:07:10,760 --> 00:07:14,660
very dynamic architectures I need to

00:07:13,070 --> 00:07:18,230
readapt my monitoring to these

00:07:14,660 --> 00:07:20,960
situations I have always continuous cost

00:07:18,230 --> 00:07:24,890
of resource users especially when I'm

00:07:20,960 --> 00:07:27,050
doing polling day and night and it's not

00:07:24,890 --> 00:07:30,470
always possible to retrieve all the data

00:07:27,050 --> 00:07:33,350
we are polling advantage of the event

00:07:30,470 --> 00:07:35,990
processing it's real-time so I have no

00:07:33,350 --> 00:07:37,910
delay I don't always need to know

00:07:35,990 --> 00:07:39,500
exactly what I will receive but I just

00:07:37,910 --> 00:07:41,720
need to understand data channel and this

00:07:39,500 --> 00:07:44,800
was the idea I need to be able to speak

00:07:41,720 --> 00:07:47,240
many channels and to receive that data

00:07:44,800 --> 00:07:49,520
if I have more hosts added to the

00:07:47,240 --> 00:07:51,320
infrastructure it's no problem I just

00:07:49,520 --> 00:07:53,420
need to speak the channel and new

00:07:51,320 --> 00:07:56,960
devices are integrated as soon as they

00:07:53,420 --> 00:08:00,110
start sending information problems I

00:07:56,960 --> 00:08:01,940
could have it's in case of a big

00:08:00,110 --> 00:08:04,280
disaster that could be a lot a peak of

00:08:01,940 --> 00:08:05,920
incoming events I don't want to lose

00:08:04,280 --> 00:08:10,100
this information and I need to handle

00:08:05,920 --> 00:08:12,820
peaks of incoming data I often have no

00:08:10,100 --> 00:08:14,870
possibility to filter at source and

00:08:12,820 --> 00:08:18,740
especially when the protocol is not

00:08:14,870 --> 00:08:22,670
reliable like UDP and SNMP trap mostly

00:08:18,740 --> 00:08:28,360
is based on this is a risk to lose such

00:08:22,670 --> 00:08:33,290
information but advantages disadvantages

00:08:28,360 --> 00:08:35,570
doing it both of these approaches in my

00:08:33,290 --> 00:08:38,660
and our experience doing projects yes

00:08:35,570 --> 00:08:40,280
because we have the advantage to start a

00:08:38,660 --> 00:08:43,190
project with the following approach very

00:08:40,280 --> 00:08:45,170
quickly easily we have standard checks

00:08:43,190 --> 00:08:48,950
the nagas community was working very

00:08:45,170 --> 00:08:51,770
good say they produced a wide range of

00:08:48,950 --> 00:08:53,330
already ready to use checks for health

00:08:51,770 --> 00:08:53,670
monitoring for standard checks and with

00:08:53,330 --> 00:08:56,460
time

00:08:53,670 --> 00:08:58,860
I can create a very good reusable

00:08:56,460 --> 00:09:01,380
monitoring and we must also say with the

00:08:58,860 --> 00:09:05,520
monitoring automation provided by singer

00:09:01,380 --> 00:09:07,440
we can also already now adapt to changes

00:09:05,520 --> 00:09:10,040
in the architecture by interacting with

00:09:07,440 --> 00:09:12,600
a domain controller with the ombre

00:09:10,040 --> 00:09:15,060
controller and I can read in this

00:09:12,600 --> 00:09:17,130
information and as previous talks today

00:09:15,060 --> 00:09:19,410
also showed us with carpet or other

00:09:17,130 --> 00:09:24,090
automation tools I can already adapt

00:09:19,410 --> 00:09:25,890
somehow to these changes so we see the

00:09:24,090 --> 00:09:29,040
event based monitoring as a complement

00:09:25,890 --> 00:09:32,460
to the polling and started already with

00:09:29,040 --> 00:09:34,680
a first initiative in 2013 with a so

00:09:32,460 --> 00:09:37,440
called event handler we were able to

00:09:34,680 --> 00:09:39,510
speak just four channels email as many

00:09:37,440 --> 00:09:41,570
traps lock we were collecting this

00:09:39,510 --> 00:09:44,730
events we were able to create some rules

00:09:41,570 --> 00:09:46,530
basically regular expressions and when

00:09:44,730 --> 00:09:49,560
an event was coming in we were comparing

00:09:46,530 --> 00:09:51,360
it to the regular expression we have in

00:09:49,560 --> 00:09:53,910
the system and then we can derive an

00:09:51,360 --> 00:09:55,520
action action was forwarded into the

00:09:53,910 --> 00:09:58,320
monitoring system we create a critical

00:09:55,520 --> 00:10:00,390
and therefore we can track the

00:09:58,320 --> 00:10:02,790
historical data for what it allowed to

00:10:00,390 --> 00:10:04,740
an administrator and so on so this was

00:10:02,790 --> 00:10:06,900
the user interface it was looking like

00:10:04,740 --> 00:10:10,740
you know a monitoring solution it was

00:10:06,900 --> 00:10:12,900
the trend of the incoming data and it

00:10:10,740 --> 00:10:16,740
was a very good experience already to

00:10:12,900 --> 00:10:18,570
gain some let's say experience

00:10:16,740 --> 00:10:22,350
backgrounds how to work in this

00:10:18,570 --> 00:10:24,380
environment but we want to face also the

00:10:22,350 --> 00:10:27,230
requirements of tomorrow so the

00:10:24,380 --> 00:10:30,000
monitoring of tomorrow in our opinion is

00:10:27,230 --> 00:10:31,950
requiring also to handle big amount of

00:10:30,000 --> 00:10:34,080
data of events incoming events and

00:10:31,950 --> 00:10:37,590
especially when talking about net flow

00:10:34,080 --> 00:10:39,480
telemetry DNS web hooks we can maybe

00:10:37,590 --> 00:10:42,060
handle thousands and even more than

00:10:39,480 --> 00:10:46,350
thousand events per second so our

00:10:42,060 --> 00:10:48,780
problem we noticed was scaling is not

00:10:46,350 --> 00:10:51,300
the point we can face with the current

00:10:48,780 --> 00:10:55,400
solution and we need to also adapt to

00:10:51,300 --> 00:10:59,430
new channels and therefore we decided

00:10:55,400 --> 00:11:02,100
draw it away rewrite it and this is the

00:10:59,430 --> 00:11:04,140
moment to if the world to Francesco

00:11:02,100 --> 00:11:05,620
thank you he will bring us now a nice

00:11:04,140 --> 00:11:07,810
story how we can

00:11:05,620 --> 00:11:09,220
we were evaluating the introduction and

00:11:07,810 --> 00:11:12,190
design okay

00:11:09,220 --> 00:11:15,160
thank you so thanks Brent the

00:11:12,190 --> 00:11:16,750
introduction as patrick said we

00:11:15,160 --> 00:11:18,580
essentially have two ways of monitoring

00:11:16,750 --> 00:11:20,670
in the first is an active way in which

00:11:18,580 --> 00:11:23,860
our monitoring solution calls our remote

00:11:20,670 --> 00:11:25,510
service or Aust or whatever and try to

00:11:23,860 --> 00:11:27,910
understand if the services app is

00:11:25,510 --> 00:11:29,380
working and how is working the second

00:11:27,910 --> 00:11:32,350
way is the one in which we are passive

00:11:29,380 --> 00:11:34,630
and we receive events from outside now a

00:11:32,350 --> 00:11:39,610
little bit I have to perform a couple of

00:11:34,630 --> 00:11:41,710
steps back in the time just to explain

00:11:39,610 --> 00:11:44,380
I'll write the relation that we have

00:11:41,710 --> 00:11:46,900
between how the system complexity

00:11:44,380 --> 00:11:49,630
evolved during the time now the way we

00:11:46,900 --> 00:11:51,279
monitor the systems evolved to adapt to

00:11:49,630 --> 00:11:55,210
the change in the changes in the systems

00:11:51,279 --> 00:11:57,880
themselves so the 90s in the 90s the

00:11:55,210 --> 00:11:59,410
sisters were very simple and essentially

00:11:57,880 --> 00:12:01,360
when something was low was not

00:11:59,410 --> 00:12:03,850
performing well or used to crash there

00:12:01,360 --> 00:12:05,920
was they are the answer to the problems

00:12:03,850 --> 00:12:07,930
was to buy new artwork because new

00:12:05,920 --> 00:12:10,510
hardware was released nearly every day

00:12:07,930 --> 00:12:12,760
and that was a fantastic period because

00:12:10,510 --> 00:12:14,890
when the boss was angry this dam

00:12:12,760 --> 00:12:17,650
application is low you could just reply

00:12:14,890 --> 00:12:19,570
well buy a new artwork it was fantastic

00:12:17,650 --> 00:12:21,390
you could drink a mojito your beer and

00:12:19,570 --> 00:12:25,060
enjoy your free time

00:12:21,390 --> 00:12:31,360
in the meantime monitoring was not

00:12:25,060 --> 00:12:33,250
really a practice because and that was

00:12:31,360 --> 00:12:35,709
something great so you could write an

00:12:33,250 --> 00:12:37,750
application I'm a developer you could

00:12:35,709 --> 00:12:39,370
write an application and nobody was able

00:12:37,750 --> 00:12:41,320
to find the bugs in your application

00:12:39,370 --> 00:12:43,240
because the systems used to crash much

00:12:41,320 --> 00:12:44,130
before your application and that was

00:12:43,240 --> 00:12:48,640
great

00:12:44,130 --> 00:12:51,339
so again is it working was the as the

00:12:48,640 --> 00:12:53,770
question and the answer was well if the

00:12:51,339 --> 00:12:58,450
system is up more or less we think it's

00:12:53,770 --> 00:13:00,610
working then ten years after there was a

00:12:58,450 --> 00:13:03,070
request for more and more performance we

00:13:00,610 --> 00:13:05,320
wanted applications to be fast too we

00:13:03,070 --> 00:13:07,630
wanted to serve a more user sir and to

00:13:05,320 --> 00:13:10,990
provide feedbacks as fast as possible

00:13:07,630 --> 00:13:13,900
CPUs reached the limits the word CPUs of

00:13:10,990 --> 00:13:15,700
twiggy guards in the 2000 and we still

00:13:13,900 --> 00:13:17,680
are at Reedy gas so it was not possible

00:13:15,700 --> 00:13:19,510
to scale vertically but we discovered

00:13:17,680 --> 00:13:22,180
that it was possible to scale

00:13:19,510 --> 00:13:24,760
essentially so this means that CPU added

00:13:22,180 --> 00:13:26,950
more course and it was possible to use

00:13:24,760 --> 00:13:28,960
all this course to perform a cup

00:13:26,950 --> 00:13:30,820
computations in parallel that was a bad

00:13:28,960 --> 00:13:33,430
moment for the developers because we

00:13:30,820 --> 00:13:35,350
could not say to the boss anymore by new

00:13:33,430 --> 00:13:37,690
hardware because at that point we had to

00:13:35,350 --> 00:13:39,670
change our software so our software from

00:13:37,690 --> 00:13:41,200
single-threaded became multi-threaded

00:13:39,670 --> 00:13:42,720
the support for me parallel operations

00:13:41,200 --> 00:13:45,940
on the same hardware

00:13:42,720 --> 00:13:49,180
luckily we abandoned some strange

00:13:45,940 --> 00:13:52,210
systems we had so more stable systems to

00:13:49,180 --> 00:13:55,030
deploy our software but still at the

00:13:52,210 --> 00:13:59,380
time the usual way of monitoring was

00:13:55,030 --> 00:14:01,300
through single pings simple scripts most

00:13:59,380 --> 00:14:04,480
of the time executed manually or maybe

00:14:01,300 --> 00:14:06,370
in a cron job so really the ability to

00:14:04,480 --> 00:14:08,320
ping go to call a system was enough to

00:14:06,370 --> 00:14:10,330
say that the system was up it was still

00:14:08,320 --> 00:14:11,800
one single machine you could open your

00:14:10,330 --> 00:14:14,220
control on that machine check the

00:14:11,800 --> 00:14:18,370
process was up okay it's working

00:14:14,220 --> 00:14:19,930
suddenly we entered this hell so it was

00:14:18,370 --> 00:14:21,520
not enough to scale right and tally on

00:14:19,930 --> 00:14:23,680
the same machine we had to scale

00:14:21,520 --> 00:14:26,130
horizontally on more machines so we

00:14:23,680 --> 00:14:28,420
enter the world of these terms like

00:14:26,130 --> 00:14:30,910
microservices distributed systems and so

00:14:28,420 --> 00:14:35,980
on at this point the complexity of the

00:14:30,910 --> 00:14:37,900
system exploded so it is low and the

00:14:35,980 --> 00:14:39,940
answer is even more complex than before

00:14:37,900 --> 00:14:42,970
it's not a matter now of parallelized

00:14:39,940 --> 00:14:45,130
but it's to distribute so at this point

00:14:42,970 --> 00:14:46,750
we had the systems that communicates

00:14:45,130 --> 00:14:49,050
with other system introducing lots of

00:14:46,750 --> 00:14:51,570
problems like synchronization

00:14:49,050 --> 00:14:55,930
availability so whatever that comes with

00:14:51,570 --> 00:14:58,660
distributed systems and at that point at

00:14:55,930 --> 00:15:00,580
that point we started using monitoring

00:14:58,660 --> 00:15:01,960
tools because it was not possible to go

00:15:00,580 --> 00:15:03,730
in a machine and simply say ok it's

00:15:01,960 --> 00:15:05,050
working because now our system is

00:15:03,730 --> 00:15:07,330
distributed across multiple machines

00:15:05,050 --> 00:15:09,520
maybe you have multiple instances of the

00:15:07,330 --> 00:15:11,290
same application or maybe you have

00:15:09,520 --> 00:15:13,690
different applications that collaborates

00:15:11,290 --> 00:15:15,820
to produce your result so at this point

00:15:13,690 --> 00:15:18,910
we'll discover that this kind of tools

00:15:15,820 --> 00:15:23,710
were useful most of these tools were

00:15:18,910 --> 00:15:25,600
born with the active monitoring logic

00:15:23,710 --> 00:15:27,220
this means that I know that I have 10

00:15:25,600 --> 00:15:28,750
instances of this application I know

00:15:27,220 --> 00:15:30,790
that they have a hundred servers I

00:15:28,750 --> 00:15:33,220
opened my console I configured these

00:15:30,790 --> 00:15:35,439
servers I send the agent from

00:15:33,220 --> 00:15:37,509
monitoring tool and I expect the agent

00:15:35,439 --> 00:15:40,600
to run every each minutes and Tuesday

00:15:37,509 --> 00:15:43,029
report to me if everything is working at

00:15:40,600 --> 00:15:45,459
this point we were happy again because

00:15:43,029 --> 00:15:47,529
is this is it working yes because the

00:15:45,459 --> 00:15:49,389
board is green so we have an answer that

00:15:47,529 --> 00:15:52,899
can provide the tool its providing girl

00:15:49,389 --> 00:15:55,990
is providing us the answer and then

00:15:52,899 --> 00:15:58,899
again this is our modern situation we

00:15:55,990 --> 00:16:01,449
don't totally have distributed systems

00:15:58,899 --> 00:16:03,970
we have distributed distributed systems

00:16:01,449 --> 00:16:05,920
we have data centers that contains more

00:16:03,970 --> 00:16:07,389
than one clouds clouds that contains

00:16:05,920 --> 00:16:09,970
data centers and so on

00:16:07,389 --> 00:16:12,279
we have geo localized clouds this could

00:16:09,970 --> 00:16:14,709
be in Europe another one in the US so

00:16:12,279 --> 00:16:17,980
the problems of latency synchronization

00:16:14,709 --> 00:16:20,680
the complexity everything exploded to

00:16:17,980 --> 00:16:23,769
the nth power and at this point to the

00:16:20,680 --> 00:16:25,990
answer is low that's the good that's the

00:16:23,769 --> 00:16:28,779
good moment to find a new job even

00:16:25,990 --> 00:16:30,910
because so the complexity of the system

00:16:28,779 --> 00:16:34,000
increased a lot but your salary proverb

00:16:30,910 --> 00:16:36,939
is the same so you can find something

00:16:34,000 --> 00:16:38,649
better maybe simpler so what's happened

00:16:36,939 --> 00:16:41,610
to our monitoring tools as I said before

00:16:38,649 --> 00:16:44,079
we implemented our all of our tools

00:16:41,610 --> 00:16:47,920
essentially based on an active on a

00:16:44,079 --> 00:16:50,379
polling strategy that was fine when

00:16:47,920 --> 00:16:52,629
there was not too many things not so

00:16:50,379 --> 00:16:54,069
many things to monitor but suddenly now

00:16:52,629 --> 00:16:56,680
we are in a situation in which we have

00:16:54,069 --> 00:16:58,269
lots of things to monitor not only we

00:16:56,680 --> 00:17:02,079
have lots of things to monitor but we

00:16:58,269 --> 00:17:04,000
have also things that happen at runtime

00:17:02,079 --> 00:17:06,760
so for example imagine a system in which

00:17:04,000 --> 00:17:08,140
you add and remove nodes based on the

00:17:06,760 --> 00:17:10,030
load of the current load of the

00:17:08,140 --> 00:17:12,130
application so you cannot configure of

00:17:10,030 --> 00:17:13,959
your all your checks upfront in your

00:17:12,130 --> 00:17:15,959
application because you don't know which

00:17:13,959 --> 00:17:18,970
servers are running in a specific moment

00:17:15,959 --> 00:17:21,010
we saw some examples of cuber natives

00:17:18,970 --> 00:17:24,069
for example kubernetes you could say if

00:17:21,010 --> 00:17:26,079
there are not enough traffic codes just

00:17:24,069 --> 00:17:28,150
reduce the number of severa so we can

00:17:26,079 --> 00:17:30,700
save some money this does not work with

00:17:28,150 --> 00:17:33,309
the active logic so we started

00:17:30,700 --> 00:17:34,780
integrating into our tools so we noticed

00:17:33,309 --> 00:17:36,730
that it is important it's from the

00:17:34,780 --> 00:17:39,159
metaphor tools to be able to react

00:17:36,730 --> 00:17:43,030
dynamically to this kind of events and

00:17:39,159 --> 00:17:45,539
situations and here is the point where

00:17:43,030 --> 00:17:47,020
the passive way of checking the systems

00:17:45,539 --> 00:17:49,150
became

00:17:47,020 --> 00:17:53,260
fundamental because we are not able to

00:17:49,150 --> 00:17:56,590
actively check anymore and this is the

00:17:53,260 --> 00:17:59,620
point also in which not only we started

00:17:56,590 --> 00:18:02,800
receiving lots and more events but we

00:17:59,620 --> 00:18:04,720
also have different type of events the

00:18:02,800 --> 00:18:07,150
more and more we go we go on with the

00:18:04,720 --> 00:18:09,309
technology we have IOT devices that are

00:18:07,150 --> 00:18:10,809
released every day we have new servers

00:18:09,309 --> 00:18:13,540
new operating systems we have new

00:18:10,809 --> 00:18:15,130
applications all of them sends message

00:18:13,540 --> 00:18:18,190
and can produce messages which are

00:18:15,130 --> 00:18:21,370
different and when you perform active

00:18:18,190 --> 00:18:23,559
monitoring you call a system and you say

00:18:21,370 --> 00:18:25,210
you must provide me this information but

00:18:23,559 --> 00:18:27,130
when you are passive the system that

00:18:25,210 --> 00:18:29,860
calls you can pass you whatever it wants

00:18:27,130 --> 00:18:31,270
so you potentially you could configure

00:18:29,860 --> 00:18:33,160
your application that works but at

00:18:31,270 --> 00:18:34,390
runtime sometimes are the new nodes to

00:18:33,160 --> 00:18:36,190
the cluster that tries to communicate

00:18:34,390 --> 00:18:38,110
with you in a different format you are

00:18:36,190 --> 00:18:40,780
not able to handle it so what should we

00:18:38,110 --> 00:18:42,580
do should we rewrite our applications to

00:18:40,780 --> 00:18:44,410
enter this kind of load so all of these

00:18:42,580 --> 00:18:47,830
growing and growing and growing the

00:18:44,410 --> 00:18:50,050
velocity variety and so on brought us to

00:18:47,830 --> 00:18:51,700
this beautiful situation we have our

00:18:50,050 --> 00:18:54,400
monitoring tools in the middle that are

00:18:51,700 --> 00:19:00,820
trying to cope with everything is

00:18:54,400 --> 00:19:02,890
happening outside them okay so again we

00:19:00,820 --> 00:19:04,660
lost the ability to use our green board

00:19:02,890 --> 00:19:06,400
and is it working

00:19:04,660 --> 00:19:08,350
when buddy's complaining we have our

00:19:06,400 --> 00:19:10,570
support team nobody's calling them so

00:19:08,350 --> 00:19:11,920
well maybe it's working because at this

00:19:10,570 --> 00:19:13,330
point as I said we are not able to

00:19:11,920 --> 00:19:16,000
monitor this situation with our

00:19:13,330 --> 00:19:19,960
classical tools so we have to guess

00:19:16,000 --> 00:19:23,620
again if something is working or not so

00:19:19,960 --> 00:19:26,380
how can we end with our current tools

00:19:23,620 --> 00:19:28,809
with we've got lots of tools we'd gained

00:19:26,380 --> 00:19:32,230
a lot of experience we have hundreds of

00:19:28,809 --> 00:19:33,760
tools and plug-ins for those tools what

00:19:32,230 --> 00:19:35,440
should we do at this point should we

00:19:33,760 --> 00:19:37,059
throw everything away you will build

00:19:35,440 --> 00:19:39,940
everything from scratch to adapt to the

00:19:37,059 --> 00:19:43,540
new pattern so it is that way to just

00:19:39,940 --> 00:19:47,800
handle these new situations into what we

00:19:43,540 --> 00:19:49,960
already have there are some possible

00:19:47,800 --> 00:19:51,910
solutions to this problem that I'm going

00:19:49,960 --> 00:19:53,530
to analyze here so imagine this is our

00:19:51,910 --> 00:19:55,750
initial configuration this is something

00:19:53,530 --> 00:19:57,550
that we configure ten years ago we have

00:19:55,750 --> 00:20:00,230
one monitoring tool it's one single

00:19:57,550 --> 00:20:03,080
instance maybe is replicated just for

00:20:00,230 --> 00:20:06,429
and recovered and this kind of things

00:20:03,080 --> 00:20:09,950
and suppose we are able to handle

00:20:06,429 --> 00:20:11,870
xks messages let's say 10 KS messages so

00:20:09,950 --> 00:20:13,610
one could say that ok the load is

00:20:11,870 --> 00:20:16,700
increasing we have more server the

00:20:13,610 --> 00:20:19,490
customer is winning on the market and so

00:20:16,700 --> 00:20:22,669
he has to handle a bigger load so let's

00:20:19,490 --> 00:20:24,860
just scale our solution so if we with

00:20:22,669 --> 00:20:27,919
one instance we can handle 10,000

00:20:24,860 --> 00:20:29,870
messages if we can manage 10,000 service

00:20:27,919 --> 00:20:32,929
with Justice Scalia we deploy three

00:20:29,870 --> 00:20:34,340
instances maybe six whatever and we

00:20:32,929 --> 00:20:37,640
handle the load that could be a simple

00:20:34,340 --> 00:20:40,100
solution and in fact the pro of this

00:20:37,640 --> 00:20:41,600
solution is that well it's simple you

00:20:40,100 --> 00:20:43,040
have your solution you already know it

00:20:41,600 --> 00:20:44,360
you worked with it for years

00:20:43,040 --> 00:20:47,030
you just have to perform a new

00:20:44,360 --> 00:20:49,309
deployment and that's all it could work

00:20:47,030 --> 00:20:51,230
out of the box and it's cheap because as

00:20:49,309 --> 00:20:55,820
we said it does not require additional

00:20:51,230 --> 00:20:57,860
knowledge or competence or whatever it

00:20:55,820 --> 00:21:00,500
has some cons of course the first

00:20:57,860 --> 00:21:03,290
context that is not cheap what never

00:21:00,500 --> 00:21:04,880
when you distribute something there is

00:21:03,290 --> 00:21:06,380
only one category of developers that

00:21:04,880 --> 00:21:08,720
thinks that distributing something is

00:21:06,380 --> 00:21:10,309
cheap is the category of developers that

00:21:08,720 --> 00:21:12,320
have never done it before

00:21:10,309 --> 00:21:14,540
because when you go from one instance of

00:21:12,320 --> 00:21:16,549
something to two instances I am NOT

00:21:14,540 --> 00:21:18,679
saying a thousand to two instances of

00:21:16,549 --> 00:21:21,919
the things things you have to handle a

00:21:18,679 --> 00:21:24,260
huge set of problems that you cannot

00:21:21,919 --> 00:21:26,570
imagine before you put your hands in it

00:21:24,260 --> 00:21:28,970
it's never cheap so you don't go to your

00:21:26,570 --> 00:21:32,919
boss saying okay we just deploy a notary

00:21:28,970 --> 00:21:35,270
seeing somewhere you will lost your job

00:21:32,919 --> 00:21:38,030
and the second point is that it does not

00:21:35,270 --> 00:21:39,919
work out of the box so it never works

00:21:38,030 --> 00:21:42,820
out of the box when you go distribute it

00:21:39,919 --> 00:21:46,010
okay so for many reasons first because

00:21:42,820 --> 00:21:48,650
you we developed we created our

00:21:46,010 --> 00:21:53,419
software's our solutions to be a single

00:21:48,650 --> 00:21:55,880
point of trust for the entire network it

00:21:53,419 --> 00:21:59,270
calls everybody who lives there it's not

00:21:55,880 --> 00:22:00,020
built to communicate to synchronize with

00:21:59,270 --> 00:22:02,210
other nodes

00:22:00,020 --> 00:22:05,740
we are currently evolving our software's

00:22:02,210 --> 00:22:08,900
budget I don't want to say that we are

00:22:05,740 --> 00:22:10,220
writing workarounds for this but more or

00:22:08,900 --> 00:22:12,080
less because we started with something

00:22:10,220 --> 00:22:13,940
that was not thought since the beginning

00:22:12,080 --> 00:22:16,730
to work that way

00:22:13,940 --> 00:22:20,000
so for example imagine that your three

00:22:16,730 --> 00:22:22,490
uses a database it does not scare all

00:22:20,000 --> 00:22:24,110
right until a monitoring tool you know

00:22:22,490 --> 00:22:25,970
that you could also replicate the

00:22:24,110 --> 00:22:28,580
database that you use to purchase your

00:22:25,970 --> 00:22:30,440
data but maybe not everybody knows that

00:22:28,580 --> 00:22:33,470
when you actually replicate your

00:22:30,440 --> 00:22:35,750
database the reads become faster but the

00:22:33,470 --> 00:22:37,660
writes actually become this log world

00:22:35,750 --> 00:22:40,010
because this is the overhead of

00:22:37,660 --> 00:22:42,440
synchronization across all the nodes so

00:22:40,010 --> 00:22:45,140
a monitoring tool is something that

00:22:42,440 --> 00:22:47,419
performs lots of rights and few reads

00:22:45,140 --> 00:22:49,370
when you read the console so this is not

00:22:47,419 --> 00:22:52,640
something that helps us to get better

00:22:49,370 --> 00:22:55,760
performance and the last point is that

00:22:52,640 --> 00:22:58,940
all of these leads to the fact that the

00:22:55,760 --> 00:23:01,640
throughput does not grow linearly so the

00:22:58,940 --> 00:23:03,770
first times you can go from one node to

00:23:01,640 --> 00:23:06,140
ten at a certain point you reach a

00:23:03,770 --> 00:23:07,460
saturation for the bandwidth because the

00:23:06,140 --> 00:23:09,020
overhead of communication and

00:23:07,460 --> 00:23:12,140
synchronization between between the

00:23:09,020 --> 00:23:16,190
nodes takes more time than analyzing the

00:23:12,140 --> 00:23:18,770
traffic itself a single solution that is

00:23:16,190 --> 00:23:21,770
sometimes used I saw this one used and

00:23:18,770 --> 00:23:23,690
is this is a real solution is to put

00:23:21,770 --> 00:23:28,429
something like a big data systems in the

00:23:23,690 --> 00:23:30,500
middle of your event system so what it

00:23:28,429 --> 00:23:32,330
means it's this is our system the one we

00:23:30,500 --> 00:23:34,850
saw before that received some events it

00:23:32,330 --> 00:23:38,299
it works at a certain point you have to

00:23:34,850 --> 00:23:42,200
handle tolls and millions of events so

00:23:38,299 --> 00:23:44,240
you can handle only some thousands of

00:23:42,200 --> 00:23:45,919
them so instead of sending all the

00:23:44,240 --> 00:23:47,870
millions events to our system we put

00:23:45,919 --> 00:23:51,409
something in the middle okay someone

00:23:47,870 --> 00:23:54,230
uses CAFTA someone uses pass flume and

00:23:51,409 --> 00:23:58,039
so the idea is that you pre process your

00:23:54,230 --> 00:24:01,220
events okay so in a monitoring tool

00:23:58,039 --> 00:24:02,240
which is built with active checks most

00:24:01,220 --> 00:24:04,760
of the time you want to know if

00:24:02,240 --> 00:24:07,370
something is up so this means that the

00:24:04,760 --> 00:24:09,350
majority of the events the majority of

00:24:07,370 --> 00:24:12,770
the messages are not important for the

00:24:09,350 --> 00:24:16,309
system itself so a system usually zap so

00:24:12,770 --> 00:24:18,590
it says I'm up we don't care I'm up I'm

00:24:16,309 --> 00:24:21,230
down this is important for us so that is

00:24:18,590 --> 00:24:23,450
data we process the messages in a way

00:24:21,230 --> 00:24:28,010
that we send only the important messages

00:24:23,450 --> 00:24:30,110
to the monitoring system so we can

00:24:28,010 --> 00:24:32,180
use the flow that we send to them are

00:24:30,110 --> 00:24:34,160
you going to let can keep working now

00:24:32,180 --> 00:24:36,700
this work this is a resolution is a

00:24:34,160 --> 00:24:39,260
major solution we have tons of

00:24:36,700 --> 00:24:41,390
possibilities to implement this the

00:24:39,260 --> 00:24:46,130
problem of this solution is that this is

00:24:41,390 --> 00:24:48,020
terribly expensive so each one of this

00:24:46,130 --> 00:24:52,130
software which is highlighted here which

00:24:48,020 --> 00:24:53,960
could be Kafka an Adobe distributed file

00:24:52,130 --> 00:24:56,330
system spark or whatever you want to put

00:24:53,960 --> 00:24:57,650
in on top of its cassandra it's

00:24:56,330 --> 00:24:58,790
beautiful piece of software but it's

00:24:57,650 --> 00:25:00,860
very complex

00:24:58,790 --> 00:25:03,800
they are resource hungry so you have to

00:25:00,860 --> 00:25:05,120
create a cluster only for for them so

00:25:03,800 --> 00:25:08,780
you could need the cluster only for

00:25:05,120 --> 00:25:09,440
Kafka and a class only cluster only for

00:25:08,780 --> 00:25:13,070
Cassandra

00:25:09,440 --> 00:25:14,630
and you need to all to get to the

00:25:13,070 --> 00:25:16,940
experience to use these tools because

00:25:14,630 --> 00:25:19,730
they are not only expensive in terms of

00:25:16,940 --> 00:25:22,550
resources but they are also expensive in

00:25:19,730 --> 00:25:24,680
terms of the knowledge that you need to

00:25:22,550 --> 00:25:26,660
gain to start using them in a proper way

00:25:24,680 --> 00:25:28,610
so you have to I specialize people or

00:25:26,660 --> 00:25:31,970
you have to get to the components one of

00:25:28,610 --> 00:25:35,860
the two is expensive anyway as I said

00:25:31,970 --> 00:25:38,480
this is a real solution for the problem

00:25:35,860 --> 00:25:40,490
this is a beautiful sentence I found in

00:25:38,480 --> 00:25:42,320
this link invite you to read it

00:25:40,490 --> 00:25:44,270
lots of people struggle with the

00:25:42,320 --> 00:25:46,430
complexity of getting big data systems

00:25:44,270 --> 00:25:50,300
up and running why they possibly

00:25:46,430 --> 00:25:53,200
shouldn't be using them at all so and in

00:25:50,300 --> 00:25:55,880
this in this paper you can read these

00:25:53,200 --> 00:25:58,250
unbelievable from I will first try first

00:25:55,880 --> 00:26:01,340
time I read it I could not believe it so

00:25:58,250 --> 00:26:03,980
this guy I processed under 28 billions

00:26:01,340 --> 00:26:06,230
edge graph with a cluster of and read

00:26:03,980 --> 00:26:09,950
the spark nodes it was able to process

00:26:06,230 --> 00:26:12,800
them in 107 thousand seconds and then he

00:26:09,950 --> 00:26:15,950
brought a single application single

00:26:12,800 --> 00:26:18,410
threaded in his machine and he was able

00:26:15,950 --> 00:26:21,200
to process exactly same graph of that in

00:26:18,410 --> 00:26:23,780
15 seconds with one single node in a

00:26:21,200 --> 00:26:26,660
notebook you can read and that source

00:26:23,780 --> 00:26:29,660
code is also in the page both the one of

00:26:26,660 --> 00:26:34,070
the edge graph and the one of the

00:26:29,660 --> 00:26:36,200
cluster and process so as I said before

00:26:34,070 --> 00:26:38,450
this is a matrix solution it does the

00:26:36,200 --> 00:26:41,450
taunts of complexity you can go for it

00:26:38,450 --> 00:26:44,090
I'm not here to advise it's but

00:26:41,450 --> 00:26:46,610
your choice so as we said we don't want

00:26:44,090 --> 00:26:49,100
this one because mostly will not work it

00:26:46,610 --> 00:26:51,380
will require us to rewrite big portions

00:26:49,100 --> 00:26:54,260
of this software which is what we want

00:26:51,380 --> 00:26:55,700
to avoid more than probably we don't

00:26:54,260 --> 00:26:57,679
want to go for this solution even if

00:26:55,700 --> 00:26:59,840
this one is a solution we will like

00:26:57,679 --> 00:27:02,750
something maybe like this one but

00:26:59,840 --> 00:27:04,669
simpler so this will allow us to not

00:27:02,750 --> 00:27:06,230
change this part which is the one that

00:27:04,669 --> 00:27:09,169
we already know that it works that it

00:27:06,230 --> 00:27:10,940
finds perfect and so on but we would

00:27:09,169 --> 00:27:13,340
like to have this without all the

00:27:10,940 --> 00:27:14,389
complexity represented by that box so we

00:27:13,340 --> 00:27:17,299
will exactly like that

00:27:14,389 --> 00:27:19,970
okay so why don't we have instead of the

00:27:17,299 --> 00:27:22,399
monster that was before in the picture

00:27:19,970 --> 00:27:24,289
couldn't we have something here which is

00:27:22,399 --> 00:27:27,289
simple which is fast which is

00:27:24,289 --> 00:27:29,210
lightweight which does not require 300

00:27:27,289 --> 00:27:32,750
nodes in parallel to print another word

00:27:29,210 --> 00:27:38,299
that does not need terabytes of RAM and

00:27:32,750 --> 00:27:43,370
so on so ladies and gentlemen please

00:27:38,299 --> 00:27:45,110
welcome Toronado okay so we this is the

00:27:43,370 --> 00:27:48,350
reasoning behind the decryption of

00:27:45,110 --> 00:27:49,639
tornados the idea which one what's the

00:27:48,350 --> 00:27:51,529
idea the idea is that the architecture

00:27:49,639 --> 00:27:53,899
represented before is the one that can

00:27:51,529 --> 00:27:56,299
solve this problem okay so we can have

00:27:53,899 --> 00:27:58,220
an active component that is able to

00:27:56,299 --> 00:27:59,990
receive the events process them and

00:27:58,220 --> 00:28:04,070
somewhere push them to our monitoring

00:27:59,990 --> 00:28:07,370
system but we want it to be simple we

00:28:04,070 --> 00:28:09,380
want it to be simple to deploy simple to

00:28:07,370 --> 00:28:13,789
configure not resource-hungry

00:28:09,380 --> 00:28:15,889
in fact it's not we advertise it as a

00:28:13,789 --> 00:28:17,899
simple complex event process where

00:28:15,889 --> 00:28:22,760
complex refers to event not to the

00:28:17,899 --> 00:28:24,500
environment to process the event so some

00:28:22,760 --> 00:28:26,990
feature before represent the

00:28:24,500 --> 00:28:28,880
architecture so can you handle millions

00:28:26,990 --> 00:28:31,039
of events yes our current implementation

00:28:28,880 --> 00:28:34,399
that I am going to show can under middle

00:28:31,039 --> 00:28:39,110
of events per CPU currently we have the

00:28:34,399 --> 00:28:42,200
problem that the load that is able to

00:28:39,110 --> 00:28:44,799
process is much bigger than the events

00:28:42,200 --> 00:28:47,870
that we are able to pass to it

00:28:44,799 --> 00:28:51,139
it's caralyn early because we remove the

00:28:47,870 --> 00:28:54,649
lots of the complexity of the entire

00:28:51,139 --> 00:28:57,100
big data stock we created it to be

00:28:54,649 --> 00:28:59,840
completely stateless and cloud-ready

00:28:57,100 --> 00:29:00,950
stateless means that if you deploy more

00:28:59,840 --> 00:29:03,559
notes that don't need to communicate

00:29:00,950 --> 00:29:06,649
each other so if one node can handle 1

00:29:03,559 --> 00:29:09,860
million events 2 notes and and 2 million

00:29:06,649 --> 00:29:12,080
events in fact this is a one

00:29:09,860 --> 00:29:14,330
architectural decisions we said that we

00:29:12,080 --> 00:29:16,249
receive events from lots of sources each

00:29:14,330 --> 00:29:18,080
source try to communicate with us with a

00:29:16,249 --> 00:29:19,909
different format so what should we do

00:29:18,080 --> 00:29:22,039
with it we have to create an application

00:29:19,909 --> 00:29:23,960
that is able to endure all the possible

00:29:22,039 --> 00:29:25,820
formats and every time someone's want a

00:29:23,960 --> 00:29:28,249
new format we have to change the source

00:29:25,820 --> 00:29:30,580
code to handle this new format - at the

00:29:28,249 --> 00:29:33,259
end we decided to make it simple and to

00:29:30,580 --> 00:29:35,690
handle one signal event format I'll show

00:29:33,259 --> 00:29:39,049
you that with one single event format we

00:29:35,690 --> 00:29:40,940
can handle whatever kind of code we want

00:29:39,049 --> 00:29:43,429
to be able to take decisions based on

00:29:40,940 --> 00:29:45,619
the event content so it's not not just

00:29:43,429 --> 00:29:47,690
receiving events we need to take

00:29:45,619 --> 00:29:49,700
decisions because as I said before some

00:29:47,690 --> 00:29:51,470
events we don't care at all we wants to

00:29:49,700 --> 00:29:54,950
be able to read the content and to throw

00:29:51,470 --> 00:29:56,840
it away it does not need to burn the CPU

00:29:54,950 --> 00:30:00,049
or use our resources if we don't need it

00:29:56,840 --> 00:30:02,600
someone we want to someone in some

00:30:00,049 --> 00:30:05,299
situations we want just to forward the

00:30:02,600 --> 00:30:07,580
event to a system to anything somewhere

00:30:05,299 --> 00:30:09,320
sometimes we want just to send the event

00:30:07,580 --> 00:30:11,149
to an elastic search someone we want to

00:30:09,320 --> 00:30:15,440
wrap it in a database or do all of these

00:30:11,149 --> 00:30:17,779
things together it must be simple as I

00:30:15,440 --> 00:30:19,970
said before Toronado accepts only this

00:30:17,779 --> 00:30:21,739
format of messages we decided to use

00:30:19,970 --> 00:30:23,830
JSON because it's the universal language

00:30:21,739 --> 00:30:26,269
to communicate across systems and

00:30:23,830 --> 00:30:30,019
essentially is a JSON with three fields

00:30:26,269 --> 00:30:31,429
the type you decide the event type the

00:30:30,019 --> 00:30:32,869
timestamp when it was created and a

00:30:31,429 --> 00:30:34,850
payload in the payload that you can put

00:30:32,869 --> 00:30:36,889
whatever you want which is a message

00:30:34,850 --> 00:30:39,619
specific essentially tornados speaks

00:30:36,889 --> 00:30:41,389
only this language here it as one thing

00:30:39,619 --> 00:30:43,399
is simple because I said before it

00:30:41,389 --> 00:30:45,789
receives one single message it's just on

00:30:43,399 --> 00:30:48,440
it has one single configuration file and

00:30:45,789 --> 00:30:51,289
tornado is one single executable you

00:30:48,440 --> 00:30:54,919
load it you start it in your machine and

00:30:51,289 --> 00:30:56,960
you have it working it's cheap it's

00:30:54,919 --> 00:30:58,669
cheap because on commodity a were after

00:30:56,960 --> 00:31:00,499
where he can handle millions of events

00:30:58,669 --> 00:31:02,749
you don't need to a cluster or anything

00:31:00,499 --> 00:31:04,820
you just have your hardware you have

00:31:02,749 --> 00:31:06,680
your deployment of

00:31:04,820 --> 00:31:08,360
your monitoring tools you take this

00:31:06,680 --> 00:31:10,160
executable you start it in that sucker

00:31:08,360 --> 00:31:12,110
is a machine is very unlikely that you

00:31:10,160 --> 00:31:14,690
will take the resources from a your

00:31:12,110 --> 00:31:16,880
monitoring system because it's very

00:31:14,690 --> 00:31:19,400
lightweight you transmit a couple of

00:31:16,880 --> 00:31:21,530
megabytes of RAM it's compiled down to

00:31:19,400 --> 00:31:23,660
native code it's very oti this would be

00:31:21,530 --> 00:31:25,910
recognized the multi-threaded using

00:31:23,660 --> 00:31:27,560
non-blocking i/o it's intensively tested

00:31:25,910 --> 00:31:29,150
we wrote it from scratch with this new

00:31:27,560 --> 00:31:30,800
language we are in love with it it was

00:31:29,150 --> 00:31:33,980
an experiment if you don't know it you

00:31:30,800 --> 00:31:36,920
should really spend with some time with

00:31:33,980 --> 00:31:38,630
it the more you want to go low level the

00:31:36,920 --> 00:31:40,610
motor performance are important for what

00:31:38,630 --> 00:31:44,920
you're going to develop the more this

00:31:40,610 --> 00:31:48,350
language could be a good choice for you

00:31:44,920 --> 00:31:50,480
so a little introduction about the

00:31:48,350 --> 00:31:55,820
architecture this green box here is

00:31:50,480 --> 00:31:58,460
Toronado is our executable and it has

00:31:55,820 --> 00:32:00,710
also me internal pipelines the idea is

00:31:58,460 --> 00:32:02,360
this one we have multiple pieces on

00:32:00,710 --> 00:32:06,020
different configuration we call data

00:32:02,360 --> 00:32:08,450
sources whatever system can provide the

00:32:06,020 --> 00:32:10,490
information to you okay so other source

00:32:08,450 --> 00:32:13,490
could be a database could be a server

00:32:10,490 --> 00:32:16,040
could be a switch that sends SNMP traps

00:32:13,490 --> 00:32:18,140
could be whatever could be an email I

00:32:16,040 --> 00:32:19,880
know that many of us steal monitors

00:32:18,140 --> 00:32:22,460
email and try to get information from

00:32:19,880 --> 00:32:24,890
emails so a data source could be an

00:32:22,460 --> 00:32:28,100
email so as I said before tornado speaks

00:32:24,890 --> 00:32:30,020
one single language so it receives an

00:32:28,100 --> 00:32:33,890
events which is a JSON that contains

00:32:30,020 --> 00:32:36,320
three fields so that is that for each

00:32:33,890 --> 00:32:38,420
input we just have a simple collector a

00:32:36,320 --> 00:32:40,400
collector is something that listens for

00:32:38,420 --> 00:32:42,380
us an event in a specific format and

00:32:40,400 --> 00:32:44,870
transforms that event in the format

00:32:42,380 --> 00:32:48,110
which is spoken by a tornado so for

00:32:44,870 --> 00:32:50,030
example we can have an SMP trap D

00:32:48,110 --> 00:32:52,760
collector this is the message that

00:32:50,030 --> 00:32:54,500
receives from SMT turdy we send it to

00:32:52,760 --> 00:32:57,410
the collector and the collector simply

00:32:54,500 --> 00:32:59,270
extract the data from that and build

00:32:57,410 --> 00:33:02,510
what turn other's paths

00:32:59,270 --> 00:33:04,190
so an JSON that contains a type a credit

00:33:02,510 --> 00:33:07,970
time-stamped and a payload and that's

00:33:04,190 --> 00:33:09,950
all so why this decision I saw that

00:33:07,970 --> 00:33:13,070
there are some tools like tornado that

00:33:09,950 --> 00:33:14,690
essentially are monolithic blocks in

00:33:13,070 --> 00:33:16,730
which if you can add the new inputs and

00:33:14,690 --> 00:33:18,330
new outputs you can write a new inputs

00:33:16,730 --> 00:33:19,890
and then you rebuild your software

00:33:18,330 --> 00:33:23,669
the input is embedded into the

00:33:19,890 --> 00:33:25,440
executable if we if you do that you are

00:33:23,669 --> 00:33:27,059
forced to use it for example where I

00:33:25,440 --> 00:33:28,769
said this is written in rust you should

00:33:27,059 --> 00:33:31,620
be able to understand the source code

00:33:28,769 --> 00:33:34,140
implements it use rust and so on but

00:33:31,620 --> 00:33:35,940
with this decision maybe you are

00:33:34,140 --> 00:33:38,010
JavaScript developers for some strange

00:33:35,940 --> 00:33:39,929
reasons you love JavaScript and you want

00:33:38,010 --> 00:33:42,960
to send messages from from JavaScript

00:33:39,929 --> 00:33:44,789
what you just build the JSON object and

00:33:42,960 --> 00:33:46,110
you send it to Toronado and you are

00:33:44,789 --> 00:33:49,169
integrated with the pipeline with

00:33:46,110 --> 00:33:51,720
everything okay you are you use PHP you

00:33:49,169 --> 00:33:53,639
love it you write your PHP that sends a

00:33:51,720 --> 00:33:55,950
JSON and you can use it you love both

00:33:53,639 --> 00:33:59,970
PHP and JavaScript you have big problems

00:33:55,950 --> 00:34:03,570
guy anyway when the message enters turn

00:33:59,970 --> 00:34:05,730
atom this is the concept that was

00:34:03,570 --> 00:34:09,089
introduced in the talk before this one

00:34:05,730 --> 00:34:12,839
so great to know that someone think the

00:34:09,089 --> 00:34:15,270
path when you enter Toronado the message

00:34:12,839 --> 00:34:17,339
goes to a pipeline I will show you in a

00:34:15,270 --> 00:34:19,409
while so in a pipeline we have a set of

00:34:17,339 --> 00:34:21,240
rules that says where the messages to go

00:34:19,409 --> 00:34:24,179
if the messages to be taken into account

00:34:21,240 --> 00:34:25,830
or just thrown away if the message is

00:34:24,179 --> 00:34:28,169
accepted by the system then we can

00:34:25,830 --> 00:34:30,000
extract some data and once we extract

00:34:28,169 --> 00:34:32,399
the data we manipulate it we transform

00:34:30,000 --> 00:34:33,929
it we can dispatch an action one or more

00:34:32,399 --> 00:34:36,270
actions for example this message is

00:34:33,929 --> 00:34:38,369
important for racing because it changes

00:34:36,270 --> 00:34:40,169
the status of a system so we call this

00:34:38,369 --> 00:34:42,419
thing I'd say a look at these systems

00:34:40,169 --> 00:34:45,530
now is down oh we can just exhibit a

00:34:42,419 --> 00:34:50,310
script or log archive oh we can know

00:34:45,530 --> 00:34:52,859
call a lock such last extract it in our

00:34:50,310 --> 00:34:55,740
database do whatever you want so every

00:34:52,859 --> 00:34:59,220
message can trigger one or more actions

00:34:55,740 --> 00:35:01,410
out so it's zero or more actions the

00:34:59,220 --> 00:35:03,330
executors are the part of Toronado that

00:35:01,410 --> 00:35:05,670
performs the real action so we have some

00:35:03,330 --> 00:35:09,720
executors that we provide with tornad

00:35:05,670 --> 00:35:11,220
itself and adding new executors it's

00:35:09,720 --> 00:35:12,839
pretty simple it's just by implementing

00:35:11,220 --> 00:35:14,609
one interface oh that can be implemented

00:35:12,839 --> 00:35:16,520
also as an external system that we call

00:35:14,609 --> 00:35:19,260
from to knob itself

00:35:16,520 --> 00:35:21,750
this is an example of a pipeline so

00:35:19,260 --> 00:35:23,670
imagine that we receive an event which

00:35:21,750 --> 00:35:25,020
is actually an email and so we receive

00:35:23,670 --> 00:35:27,119
an email we have a collector that

00:35:25,020 --> 00:35:29,609
transforms this email in an Toronado

00:35:27,119 --> 00:35:31,710
event so we receive an event which is

00:35:29,609 --> 00:35:33,630
type email for example and then

00:35:31,710 --> 00:35:36,059
it contains the parts the body of the

00:35:33,630 --> 00:35:37,950
email as a payload it tenters the root

00:35:36,059 --> 00:35:40,170
of our pipeline and then we start

00:35:37,950 --> 00:35:42,119
checking maybe at the beginning it's

00:35:40,170 --> 00:35:44,670
quite typical to have a ruler that

00:35:42,119 --> 00:35:48,089
allows every message to flow into the

00:35:44,670 --> 00:35:50,190
pipeline and then each time each time we

00:35:48,089 --> 00:35:52,079
encounter an order here in the pipeline

00:35:50,190 --> 00:35:54,059
we take a decision should download the

00:35:52,079 --> 00:35:55,800
message to pass or not in this case

00:35:54,059 --> 00:35:58,260
maybe we check that the event type is

00:35:55,800 --> 00:36:01,559
email so it does not pass here here here

00:35:58,260 --> 00:36:05,280
because this is an SMTP or syslog they

00:36:01,559 --> 00:36:07,319
don't they discard even by this kind in

00:36:05,280 --> 00:36:09,450
this node here we just check that the

00:36:07,319 --> 00:36:11,730
email type is filter and then the

00:36:09,450 --> 00:36:15,990
message flows to the pipeline which can

00:36:11,730 --> 00:36:17,579
have whatever that this is the whole the

00:36:15,990 --> 00:36:19,410
entire configuration for these two nodes

00:36:17,579 --> 00:36:21,869
this is just another which is active so

00:36:19,410 --> 00:36:26,609
the message is passed through the we use

00:36:21,869 --> 00:36:29,099
the JSON to describe the rule that sorry

00:36:26,609 --> 00:36:31,800
the condition which is in the filter and

00:36:29,099 --> 00:36:34,290
this is another filter which is true

00:36:31,800 --> 00:36:36,569
which is active and we are here into the

00:36:34,290 --> 00:36:39,240
filter we just check that even dot type

00:36:36,569 --> 00:36:42,329
is equals to email okay so we can we can

00:36:39,240 --> 00:36:44,940
apply reg Exodus we can perform or less

00:36:42,329 --> 00:36:46,349
than equals done we can perform contains

00:36:44,940 --> 00:36:48,200
operations and this kind of thing so

00:36:46,349 --> 00:36:50,970
this is the simplest one which is equal

00:36:48,200 --> 00:36:53,640
this is actually the entire

00:36:50,970 --> 00:36:55,380
configuration of this pipeline so

00:36:53,640 --> 00:36:57,180
everything is here as I said we wanted

00:36:55,380 --> 00:36:59,160
to make it simple so every node as his

00:36:57,180 --> 00:37:01,559
own JSON file that describe what happens

00:36:59,160 --> 00:37:03,900
into that node and this is not a subset

00:37:01,559 --> 00:37:05,309
of the configuration I wanted it to be

00:37:03,900 --> 00:37:09,079
clear this is the entire configuration

00:37:05,309 --> 00:37:12,690
that you need to create this pipeline

00:37:09,079 --> 00:37:14,670
finally we said even into the image

00:37:12,690 --> 00:37:16,950
filter we have a set of routes every

00:37:14,670 --> 00:37:18,599
route can perform some actions into the

00:37:16,950 --> 00:37:20,520
routes we have additional checks and so

00:37:18,599 --> 00:37:23,160
for example we could check that day make

00:37:20,520 --> 00:37:25,589
a list for the knob the CEO will perform

00:37:23,160 --> 00:37:25,829
some actions is for someone we don't

00:37:25,589 --> 00:37:29,640
know

00:37:25,829 --> 00:37:31,859
discard it and so on and if our rule

00:37:29,640 --> 00:37:34,589
matches the condition then it triggers

00:37:31,859 --> 00:37:36,599
the actions that are bound to that rule

00:37:34,589 --> 00:37:37,980
in this case we have one action if it

00:37:36,599 --> 00:37:42,869
was a good one it would have triggered

00:37:37,980 --> 00:37:44,579
two actions this is an example of the

00:37:42,869 --> 00:37:45,540
rule the rule is the most complex

00:37:44,579 --> 00:37:47,010
pattern

00:37:45,540 --> 00:37:49,410
because it performs many of many

00:37:47,010 --> 00:37:51,870
operations so I repeat again here is the

00:37:49,410 --> 00:37:53,880
entire configuration of the rule I just

00:37:51,870 --> 00:37:57,480
highlighted the important parts so we

00:37:53,880 --> 00:37:59,760
want to that the type is e is email and

00:37:57,480 --> 00:38:02,070
we extract we check that into the

00:37:59,760 --> 00:38:04,470
payload may be this this is an email

00:38:02,070 --> 00:38:06,150
about the the temperature of the several

00:38:04,470 --> 00:38:06,480
rooms to I I don't know something like

00:38:06,150 --> 00:38:11,610
that

00:38:06,480 --> 00:38:14,520
so is too low last certain degrees so if

00:38:11,610 --> 00:38:16,230
the event matches all these rules all

00:38:14,520 --> 00:38:17,520
these conditions we can extract some

00:38:16,230 --> 00:38:19,020
data we don't have an extractor here

00:38:17,520 --> 00:38:23,100
anyway we can extract the manipulate

00:38:19,020 --> 00:38:25,110
some data it's optional and here we have

00:38:23,100 --> 00:38:28,500
a set of actions which are performed if

00:38:25,110 --> 00:38:29,970
the the where Clause is matched in this

00:38:28,500 --> 00:38:32,130
case if we perform an actions which is

00:38:29,970 --> 00:38:33,030
called this ingot - we call the process

00:38:32,130 --> 00:38:35,850
check result

00:38:33,030 --> 00:38:38,250
API from a seeing and we set 0 the

00:38:35,850 --> 00:38:41,190
status of an austere whose last name is

00:38:38,250 --> 00:38:48,240
also extracted from the payload of the

00:38:41,190 --> 00:38:51,410
message itself ok damn time up to you

00:38:48,240 --> 00:38:51,410
yeah great

00:39:03,530 --> 00:39:08,660
so let's use the last couple of minutes

00:39:06,140 --> 00:39:10,460
we have here to show you of what you

00:39:08,660 --> 00:39:12,920
have seen from the architecture of

00:39:10,460 --> 00:39:15,950
Toronado a simple use case where we

00:39:12,920 --> 00:39:18,860
receive an event from outside in this

00:39:15,950 --> 00:39:22,370
case we are taking an email which is a

00:39:18,860 --> 00:39:25,580
quite simple event I can receive via

00:39:22,370 --> 00:39:27,980
text text line in this case I simulate

00:39:25,580 --> 00:39:30,260
simulated very easily on the local

00:39:27,980 --> 00:39:32,540
system by typing it

00:39:30,260 --> 00:39:37,460
we are mirror mode on the local system

00:39:32,540 --> 00:39:39,710
and create a user this users so in this

00:39:37,460 --> 00:39:43,610
case the entry for the channel of the

00:39:39,710 --> 00:39:47,510
email and processing the various rules

00:39:43,610 --> 00:39:49,970
I've created I will receive an event in

00:39:47,510 --> 00:39:52,790
the monitoring so the example here is a

00:39:49,970 --> 00:39:55,250
simple s IP message I have an eye drop

00:39:52,790 --> 00:39:57,200
arrow with an arrow code and there is a

00:39:55,250 --> 00:40:00,200
good error code in this example I take

00:39:57,200 --> 00:40:02,860
zero zero is good and the other example

00:40:00,200 --> 00:40:05,630
is the F 0 1 this would be the critical

00:40:02,860 --> 00:40:08,780
which is then forwarded into e single

00:40:05,630 --> 00:40:11,840
itself to create a critical situation so

00:40:08,780 --> 00:40:14,660
this is the default status I have here

00:40:11,840 --> 00:40:17,300
my host the s IP host and the status of

00:40:14,660 --> 00:40:19,220
I dock in this case is OK so before

00:40:17,300 --> 00:40:21,560
showing your olive oyl steps inside i

00:40:19,220 --> 00:40:25,190
just perform the action for sending this

00:40:21,560 --> 00:40:27,980
message and as soon we pipe this message

00:40:25,190 --> 00:40:31,760
into the system the processing of

00:40:27,980 --> 00:40:34,460
tornado performs the matching identifies

00:40:31,760 --> 00:40:36,950
the error code and forwards the critical

00:40:34,460 --> 00:40:40,160
into the system so how this is working

00:40:36,950 --> 00:40:42,050
if I've few minutes we have it here so I

00:40:40,160 --> 00:40:44,930
can just show you how this was working

00:40:42,050 --> 00:40:48,160
in the inside of the system the message

00:40:44,930 --> 00:40:56,680
itself was forwarded on a local system

00:40:48,160 --> 00:40:59,960
via mail server into a local collector

00:40:56,680 --> 00:41:02,210
as a collector I have various

00:40:59,960 --> 00:41:05,090
possibilities in this case I implemented

00:41:02,210 --> 00:41:08,000
the email collector and this is just

00:41:05,090 --> 00:41:10,250
listening via configuration file on a

00:41:08,000 --> 00:41:13,010
specific socket so I have nothing else

00:41:10,250 --> 00:41:15,470
than a redirect of the incoming mails to

00:41:13,010 --> 00:41:17,480
this we are collecting the

00:41:15,470 --> 00:41:20,540
sucking at the socket of the of the

00:41:17,480 --> 00:41:23,480
tornado collector as soon as this

00:41:20,540 --> 00:41:25,760
collector went in action is forwarding

00:41:23,480 --> 00:41:28,780
the message into the core of tomahto

00:41:25,760 --> 00:41:31,910
here have my various rules as and as

00:41:28,780 --> 00:41:35,390
Francesco has indicated before I can

00:41:31,910 --> 00:41:37,310
create filters so my first intention is

00:41:35,390 --> 00:41:39,560
I differentiate between the various

00:41:37,310 --> 00:41:40,430
types of events coming in I have emails

00:41:39,560 --> 00:41:44,090
I have web hooks

00:41:40,430 --> 00:41:46,820
I have streams of whatever type so what

00:41:44,090 --> 00:41:49,820
I first did is I'm creating a filter

00:41:46,820 --> 00:41:54,110
express an expression exactly the filter

00:41:49,820 --> 00:41:55,760
for the event type email so they collect

00:41:54,110 --> 00:41:58,640
at the beginning is flagging this

00:41:55,760 --> 00:42:00,680
incoming event as a type email so only

00:41:58,640 --> 00:42:04,160
events of this type are passing these

00:42:00,680 --> 00:42:06,740
filters we can go a step inside and here

00:42:04,160 --> 00:42:09,380
I have now the complete rule for

00:42:06,740 --> 00:42:11,200
interpreting this message first of all

00:42:09,380 --> 00:42:17,030
here I have again the where condition

00:42:11,200 --> 00:42:19,300
type end and I extract from the event

00:42:17,030 --> 00:42:23,090
that payload that subject the content

00:42:19,300 --> 00:42:26,870
and compare it to the required sub idoc

00:42:23,090 --> 00:42:28,880
so here I have the first condition as a

00:42:26,870 --> 00:42:32,900
filter I want to say I have a specific

00:42:28,880 --> 00:42:35,690
error code f01 is our code and I decide

00:42:32,900 --> 00:42:38,890
if this is matching an action here you

00:42:35,690 --> 00:42:42,610
can understand the typically singer API

00:42:38,890 --> 00:42:45,140
language I action a process check result

00:42:42,610 --> 00:42:47,420
applying the filter of a host name at a

00:42:45,140 --> 00:42:49,490
service name and providing the output

00:42:47,420 --> 00:42:53,750
from the extracted variables on the top

00:42:49,490 --> 00:42:56,330
you see in the filter here so the ID of

00:42:53,750 --> 00:42:58,460
arrow was a variable created with the

00:42:56,330 --> 00:43:02,120
with condition and this content and this

00:42:58,460 --> 00:43:04,520
payload is extracted and out written out

00:43:02,120 --> 00:43:07,430
to the event I'm creating so we see it

00:43:04,520 --> 00:43:09,230
exactly here if I maybe increase it a

00:43:07,430 --> 00:43:11,300
little bit you see the arrow code as the

00:43:09,230 --> 00:43:15,440
text and then the full output which is

00:43:11,300 --> 00:43:18,670
coming on the bottom I can do exactly

00:43:15,440 --> 00:43:22,760
the same for a recovery I was here

00:43:18,670 --> 00:43:25,550
sending the recovery code and the

00:43:22,760 --> 00:43:29,160
recovery is arriving so how this is

00:43:25,550 --> 00:43:38,940
working inside I can

00:43:29,160 --> 00:43:46,010
open let's see the lock if I enable the

00:43:38,940 --> 00:43:46,010
debug lock of the email collector sorry

00:43:46,460 --> 00:43:51,569
just send again the critical message and

00:43:49,530 --> 00:43:54,540
you see that the incoming message is

00:43:51,569 --> 00:43:57,200
passed by the collector extracted the

00:43:54,540 --> 00:43:59,849
various variables inside this and

00:43:57,200 --> 00:44:01,890
forwarded it into the core of the

00:43:59,849 --> 00:44:09,050
tornado if I have a look at it on add

00:44:01,890 --> 00:44:12,390
itself let's send it again

00:44:09,050 --> 00:44:15,440
you see if I enable the debug level the

00:44:12,390 --> 00:44:18,329
various routes is trying to match and

00:44:15,440 --> 00:44:19,980
also the output if there are some rules

00:44:18,329 --> 00:44:22,290
which are partially matching not

00:44:19,980 --> 00:44:24,359
completely right like it is for the

00:44:22,290 --> 00:44:26,010
recovery so in this case the recovery

00:44:24,359 --> 00:44:28,910
was not matching at him arrow was

00:44:26,010 --> 00:44:32,280
matching and on the top you see also the

00:44:28,910 --> 00:44:34,140
JSON which was extracted as Francesco

00:44:32,280 --> 00:44:39,150
explained before from the incoming

00:44:34,140 --> 00:44:43,349
collector so when doing such integration

00:44:39,150 --> 00:44:45,390
of other channels I just need to respect

00:44:43,349 --> 00:44:52,920
always this language and then able to do

00:44:45,390 --> 00:44:57,260
it easily yeah

00:44:52,920 --> 00:45:00,619
principally saying I would say we can

00:44:57,260 --> 00:45:00,619
stop here

00:45:10,710 --> 00:45:27,440
great so this was exactly we can so you

00:45:17,640 --> 00:45:27,440
can follow again this example the and

00:45:30,150 --> 00:45:40,429
[Applause]

00:45:42,800 --> 00:46:23,550
thank you come again

00:45:56,040 --> 00:46:27,840
I think hello

00:46:23,550 --> 00:46:32,190
I was the talk before was hearing about

00:46:27,840 --> 00:46:34,560
sensu and about he went pipelines and I

00:46:32,190 --> 00:46:37,230
have to admit it sounded a lot similar

00:46:34,560 --> 00:46:39,420
to this talk where would you see the

00:46:37,230 --> 00:46:44,550
main differences between sensu and

00:46:39,420 --> 00:46:46,620
tornado okay so yes we saw the talker

00:46:44,550 --> 00:46:49,170
and it was interest interesting as I

00:46:46,620 --> 00:46:51,600
said before internet we apply some

00:46:49,170 --> 00:46:53,760
patterns to resolve a problem and we

00:46:51,600 --> 00:46:55,080
were quite happy very happy to see that

00:46:53,760 --> 00:46:57,390
someone has the things that these

00:46:55,080 --> 00:46:59,280
patterns actually works so the main

00:46:57,390 --> 00:47:00,990
difference between tornado and the

00:46:59,280 --> 00:47:03,660
sensor is the fact that sensor is a

00:47:00,990 --> 00:47:05,160
complete product so if you have to to

00:47:03,660 --> 00:47:06,810
create you architecture from scratch if

00:47:05,160 --> 00:47:08,940
you search in something from scratch I

00:47:06,810 --> 00:47:13,620
would say that I I think it's a very

00:47:08,940 --> 00:47:16,020
valid solution but the objective of

00:47:13,620 --> 00:47:18,120
tornado is to provide a simple solution

00:47:16,020 --> 00:47:20,310
to a complex problem and the complex

00:47:18,120 --> 00:47:22,720
problem instant EU is the one in which

00:47:20,310 --> 00:47:24,340
you already have maybe a configuration

00:47:22,720 --> 00:47:25,810
you have something that runs at the

00:47:24,340 --> 00:47:27,640
certain points you have a problem your

00:47:25,810 --> 00:47:30,190
problem is that you're not able to

00:47:27,640 --> 00:47:32,980
actively monitor everything you are not

00:47:30,190 --> 00:47:35,140
able to handle a specific road so

00:47:32,980 --> 00:47:36,940
Toronado can complement your current

00:47:35,140 --> 00:47:39,520
architecture without required requiring

00:47:36,940 --> 00:47:42,520
you requesting you to buy new hardware

00:47:39,520 --> 00:47:45,670
or to keep audition out certain these

00:47:42,520 --> 00:47:48,700
kind of things and it can work as a

00:47:45,670 --> 00:47:51,340
bridge between passive and active

00:47:48,700 --> 00:47:53,500
checking along in your current

00:47:51,340 --> 00:47:59,070
architecture to do more with the same

00:47:53,500 --> 00:48:01,810
that you have right now all right thanks

00:47:59,070 --> 00:48:07,390
okay I guess we have time for one last

00:48:01,810 --> 00:48:27,310
question nobody okay oh right in the

00:48:07,390 --> 00:48:28,660
front just right well it's easy try deep

00:48:27,310 --> 00:48:30,730
reproduction yes it is already

00:48:28,660 --> 00:48:33,420
production you know installation sapna

00:48:30,730 --> 00:48:37,990
tile in the latest versions at least and

00:48:33,420 --> 00:48:39,820
we have a roadmap yes it will be

00:48:37,990 --> 00:48:42,340
everything is open source of course here

00:48:39,820 --> 00:48:44,380
you can see the link the roadmap will be

00:48:42,340 --> 00:48:46,240
published soon we cannot have built the

00:48:44,380 --> 00:48:47,680
the core of the software there are many

00:48:46,240 --> 00:48:53,320
new features that we want to integrate

00:48:47,680 --> 00:48:55,510
with it yeah but as you told it's open

00:48:53,320 --> 00:48:58,060
source there are also some nice examples

00:48:55,510 --> 00:48:59,110
so you can read a reference guide so

00:48:58,060 --> 00:49:00,940
there should be at least some

00:48:59,110 --> 00:49:03,370
instructions how to can you build your

00:49:00,940 --> 00:49:05,680
own test cases and maybe if you use

00:49:03,370 --> 00:49:07,360
another monitoring system I don't know

00:49:05,680 --> 00:49:10,470
if you use Nagas or every single or

00:49:07,360 --> 00:49:13,180
whatever so you can also find some

00:49:10,470 --> 00:49:16,420
probably some instructions how you can

00:49:13,180 --> 00:49:22,240
also do your first first example

00:49:16,420 --> 00:49:24,700
specific but I created during my

00:49:22,240 --> 00:49:27,820
consulting activity also some examples

00:49:24,700 --> 00:49:31,150
also last one was of tornado creating a

00:49:27,820 --> 00:49:34,590
simple web request I published it on the

00:49:31,150 --> 00:49:36,860
github of my personal accounts

00:49:34,590 --> 00:49:39,360
to find some ideas of a first

00:49:36,860 --> 00:49:45,480
instruction to create a lab to subscribe

00:49:39,360 --> 00:49:47,790
subscription to this case excellent

00:49:45,480 --> 00:49:51,670
thank you again thank you

00:49:47,790 --> 00:50:02,510
[Applause]

00:49:51,670 --> 00:50:02,510
[Music]

00:50:06,100 --> 00:50:08,889

YouTube URL: https://www.youtube.com/watch?v=NxyAxixlbuw


