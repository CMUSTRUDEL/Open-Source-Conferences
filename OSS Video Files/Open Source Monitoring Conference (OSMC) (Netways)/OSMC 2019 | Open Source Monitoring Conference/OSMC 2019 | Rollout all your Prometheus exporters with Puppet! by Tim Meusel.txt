Title: OSMC 2019 | Rollout all your Prometheus exporters with Puppet! by Tim Meusel
Publication date: 2019-11-18
Playlist: OSMC 2019 | Open Source Monitoring Conference
Description: 
	Everybody loves Prometheus. Many exporters are available to gather specific data. You can download the binaries from GitHub, start them and they will expose data via plain HTTP, without any firewalling or authentication. That would just complicate the whole setup! This is how the usual installation guide looks. It works, but it doesn’t cover all the important parts a systems engineer loves.

– What about authentication?
– What about firewalling?
– How can we make the Prometheus server aware of the exporters?
– How do we integrate custom applications that also provide prometheus compatible metrics?

A secure and automated rollout of exporters isn’t easy. Also an authenticated connection from the prometheus server to the exporters requires some preparation. This talk will cover a proper concept and all details to rollout multiple exporters to many systems, completely automated with Puppet.


NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de/
Blog: http://blog.netways.de/
NWS: https://nws.netways.de

Webinare
Archiv Link: https://www.netways.de/webinare/archi...
Aktuell: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh/

Musik: FRAMETRAXX
Captions: 
	00:00:13,840 --> 00:00:17,960
thank you everybody has already

00:00:16,189 --> 00:00:19,519
mentioned my name is Tim and I would

00:00:17,960 --> 00:00:22,429
like to talk to you today about rolling

00:00:19,519 --> 00:00:25,009
or prometheus exporters with puppet a

00:00:22,429 --> 00:00:26,900
brief introduction I go by basta freak

00:00:25,009 --> 00:00:28,849
on the internet you can find me on

00:00:26,900 --> 00:00:32,119
github Twitter and various other

00:00:28,849 --> 00:00:34,730
platforms I'm heavily using puppet since

00:00:32,119 --> 00:00:37,280
around 2012 and I'm very very active in

00:00:34,730 --> 00:00:40,789
the puppet community named Vox Populi

00:00:37,280 --> 00:00:45,949
since around 2015 who of you has heard

00:00:40,789 --> 00:00:49,329
about Vox Populi okay that's like 60%

00:00:45,949 --> 00:00:49,329
maybe that's quite good

00:00:49,420 --> 00:00:58,100
okay let us go on a monitoring journey

00:00:54,760 --> 00:01:00,699
every journey needs a starting point so

00:00:58,100 --> 00:01:04,220
we need to make some assumptions here

00:01:00,699 --> 00:01:06,710
first one let's assume that we all

00:01:04,220 --> 00:01:09,380
maintain a few thousand servers physical

00:01:06,710 --> 00:01:11,900
servers virtual machines cloud resources

00:01:09,380 --> 00:01:13,490
that you rented on AWS or maybe you're a

00:01:11,900 --> 00:01:17,689
cloud provider it doesn't matter we have

00:01:13,490 --> 00:01:19,909
a few thousand servers and some when our

00:01:17,689 --> 00:01:21,829
company is at least one person that

00:01:19,909 --> 00:01:24,079
claims to be a developer and he writes

00:01:21,829 --> 00:01:26,270
custom software and certainly we are

00:01:24,079 --> 00:01:30,890
responsible for hosting this off in

00:01:26,270 --> 00:01:34,149
production this happens from time to

00:01:30,890 --> 00:01:34,149
time and it's always a bit painful

00:01:35,049 --> 00:01:40,579
monitoring is always complicated

00:01:37,840 --> 00:01:43,520
monitoring is often bad especially in

00:01:40,579 --> 00:01:46,060
our situation the monitoring that we

00:01:43,520 --> 00:01:50,030
have produces a lot of false positives

00:01:46,060 --> 00:01:52,939
it is inefficient it's very hard to

00:01:50,030 --> 00:01:55,490
maintain or maybe we don't have any

00:01:52,939 --> 00:01:58,880
monitoring at all that's basically the

00:01:55,490 --> 00:02:04,869
same does any of those points sound

00:01:58,880 --> 00:02:08,180
familiar to any one of you if you okay

00:02:04,869 --> 00:02:09,769
see you first chapter the exploration of

00:02:08,180 --> 00:02:11,569
the happy face we all noticed our

00:02:09,769 --> 00:02:13,459
monitoring is bad we have many many

00:02:11,569 --> 00:02:15,380
servers we need to monitor those servers

00:02:13,459 --> 00:02:17,540
so what do we do as

00:02:15,380 --> 00:02:20,420
very responsible system administrator or

00:02:17,540 --> 00:02:23,120
DevOps engineer or however we what we

00:02:20,420 --> 00:02:24,950
claim to be we attend our most favorite

00:02:23,120 --> 00:02:29,540
monitoring conference for example

00:02:24,950 --> 00:02:32,030
something in nürnberg in some person

00:02:29,540 --> 00:02:34,280
maybe recommends to us the famous note

00:02:32,030 --> 00:02:38,330
exporter who a few has heard about note

00:02:34,280 --> 00:02:39,980
exporter very good I should do more

00:02:38,330 --> 00:02:43,250
questions it's a good training for your

00:02:39,980 --> 00:02:45,230
arms ok

00:02:43,250 --> 00:02:46,880
people often say well that's a go binary

00:02:45,230 --> 00:02:48,560
you can find it on github just don't let

00:02:46,880 --> 00:02:51,230
it run it as rude and it will provide

00:02:48,560 --> 00:02:55,760
you some metrics and that sounds very

00:02:51,230 --> 00:03:07,970
easy but also bit scary and let's try

00:02:55,760 --> 00:03:10,790
this out if I okay so I just skipped the

00:03:07,970 --> 00:03:12,080
downloading thing because Wi-Fi is well

00:03:10,790 --> 00:03:15,259
Wi-Fi

00:03:12,080 --> 00:03:15,259
[Music]

00:03:19,910 --> 00:03:25,800
awesome so there is no something running

00:03:23,040 --> 00:03:31,430
to TCP port it provides a certain end

00:03:25,800 --> 00:03:32,610
point and we can access this nope

00:03:31,430 --> 00:03:35,129
awesome

00:03:32,610 --> 00:03:39,510
see note export is running torque is

00:03:35,129 --> 00:03:42,390
basically done awesome so node exporter

00:03:39,510 --> 00:03:46,170
very nice output there is always a

00:03:42,390 --> 00:03:48,810
metric a metric can has a value there

00:03:46,170 --> 00:03:52,410
might be some explanation about what

00:03:48,810 --> 00:03:54,060
this metric does and if it's a very good

00:03:52,410 --> 00:03:58,799
software there is also something like a

00:03:54,060 --> 00:04:01,380
type definition yeah that's not exporter

00:03:58,799 --> 00:04:03,569
and you can see I can scroll here for a

00:04:01,380 --> 00:04:05,879
very very long time and it takes very

00:04:03,569 --> 00:04:16,459
long to reach at the end of the page so

00:04:05,879 --> 00:04:20,220
there are many many metrics available so

00:04:16,459 --> 00:04:27,539
let's try to find out how many values we

00:04:20,220 --> 00:04:29,479
actually have okay so accessing with

00:04:27,539 --> 00:04:33,150
curl works as well

00:04:29,479 --> 00:04:36,000
can everybody see this in the back might

00:04:33,150 --> 00:04:37,710
be say we have around 700 and free

00:04:36,000 --> 00:04:40,110
actual values that we get from the node

00:04:37,710 --> 00:04:42,030
exporter just with the default

00:04:40,110 --> 00:04:44,070
configuration I didn't configure

00:04:42,030 --> 00:04:47,639
anything at all I just download at the

00:04:44,070 --> 00:04:49,500
binary earlier executed it locally and I

00:04:47,639 --> 00:04:52,620
already get seven hundred and three

00:04:49,500 --> 00:04:55,169
metrics that's pretty good

00:04:52,620 --> 00:04:58,860
that's a nice starting point everybody's

00:04:55,169 --> 00:05:00,450
happy with this if you take a deeper

00:04:58,860 --> 00:05:02,820
look at the metrics you notice that

00:05:00,450 --> 00:05:04,800
maybe ninety percent are not relevant

00:05:02,820 --> 00:05:07,760
for your infrastructure at all but it's

00:05:04,800 --> 00:05:07,760
always good to have them

00:05:17,940 --> 00:05:25,350
so the knoteks partner works pretty

00:05:23,520 --> 00:05:28,470
pretty simple it reads a bunch of values

00:05:25,350 --> 00:05:31,560
it exposes them locally via an HTTP

00:05:28,470 --> 00:05:33,780
interface that you can pull it doesn't

00:05:31,560 --> 00:05:38,730
provide any authentication or encryption

00:05:33,780 --> 00:05:41,850
like HTTP that's bad if you like

00:05:38,730 --> 00:05:44,130
security but it also makes the daemon

00:05:41,850 --> 00:05:45,510
smaller it has less code it could be

00:05:44,130 --> 00:05:49,740
attacked so there are always some

00:05:45,510 --> 00:05:51,870
trade-offs there are many collectors

00:05:49,740 --> 00:05:52,830
available in the north exported

00:05:51,870 --> 00:05:55,560
documentation that you can

00:05:52,830 --> 00:05:57,960
enable/disable for example scraping your

00:05:55,560 --> 00:05:59,900
local local app cache getting

00:05:57,960 --> 00:06:12,240
information about the used file system

00:05:59,900 --> 00:06:14,520
stuff like this so this is a list of all

00:06:12,240 --> 00:06:16,680
the export know of all the collectors

00:06:14,520 --> 00:06:18,870
that you can enable/disable within the

00:06:16,680 --> 00:06:25,310
node exporter so there is a bunch of

00:06:18,870 --> 00:06:25,310
stuff that can generate metrics for you

00:06:31,469 --> 00:06:37,830
also very nice feature that you can

00:06:33,210 --> 00:06:40,620
generate data in the same format float

00:06:37,830 --> 00:06:42,629
into a text file and the node exporter

00:06:40,620 --> 00:06:49,139
will just read the text file and present

00:06:42,629 --> 00:06:51,689
the data as well via HTTP downloading

00:06:49,139 --> 00:06:54,539
random bynars from the internet it's

00:06:51,689 --> 00:06:56,669
very scary please don't do this

00:06:54,539 --> 00:06:59,039
this might be recommended in many many

00:06:56,669 --> 00:07:03,180
documentation x' but no that's that's

00:06:59,039 --> 00:07:04,830
bad check out for for custom repos or

00:07:03,180 --> 00:07:06,689
boot this software on your own and

00:07:04,830 --> 00:07:18,840
create custom packages that's always a

00:07:06,689 --> 00:07:21,029
very very good approach so after

00:07:18,840 --> 00:07:24,180
attending a conference and getting in

00:07:21,029 --> 00:07:25,620
touch with the Nordics partner we often

00:07:24,180 --> 00:07:27,330
get the impression that it's it's an

00:07:25,620 --> 00:07:29,189
actual very very good software to to

00:07:27,330 --> 00:07:32,279
monitor physical machines to monitor

00:07:29,189 --> 00:07:35,069
virtual machines any boxes that we have

00:07:32,279 --> 00:07:37,259
it's pretty pretty nice but it's also

00:07:35,069 --> 00:07:41,580
important to monitor services as we

00:07:37,259 --> 00:07:43,440
learned in the talk before you so we

00:07:41,580 --> 00:07:46,500
somehow need to get metrics from the

00:07:43,440 --> 00:07:48,060
application that we are operating for

00:07:46,500 --> 00:07:54,180
this person that claims to be a

00:07:48,060 --> 00:07:55,770
developer if we take a look at the

00:07:54,180 --> 00:07:57,930
different software that we have in-house

00:07:55,770 --> 00:08:00,629
we have noticed that a framework is used

00:07:57,930 --> 00:08:02,339
if a framework isn't used you might want

00:08:00,629 --> 00:08:04,080
to throw their custom software way and

00:08:02,339 --> 00:08:07,110
build it from scratch again

00:08:04,080 --> 00:08:11,460
and yeah frameworks always a good

00:08:07,110 --> 00:08:13,259
approach there are many many frameworks

00:08:11,460 --> 00:08:15,389
available for example in the Java

00:08:13,259 --> 00:08:18,180
environment there is the spring

00:08:15,389 --> 00:08:20,550
framework and the spring framework has

00:08:18,180 --> 00:08:23,159
built-in support for Prometheus metrics

00:08:20,550 --> 00:08:26,279
like many many other frameworks as well

00:08:23,159 --> 00:08:28,110
so it might be pretty easy to get

00:08:26,279 --> 00:08:30,300
metrics from your in-house custom

00:08:28,110 --> 00:08:32,880
software that you have it might be a

00:08:30,300 --> 00:08:35,310
single config option that you just need

00:08:32,880 --> 00:08:41,579
to enable during runtime or during the

00:08:35,310 --> 00:08:43,400
compilation also pretty easy to enrich

00:08:41,579 --> 00:08:46,290
the data that you

00:08:43,400 --> 00:08:48,150
expose with the node exporter or any

00:08:46,290 --> 00:08:52,200
other exporter that you find for example

00:08:48,150 --> 00:08:54,630
with dimension text files you can enrich

00:08:52,200 --> 00:08:55,920
it with data from your IP management

00:08:54,630 --> 00:08:57,690
that you have in-house or from

00:08:55,920 --> 00:08:59,670
ServiceNow whatever you have or just

00:08:57,690 --> 00:09:02,130
random cron jobs or systemd timers that

00:08:59,670 --> 00:09:08,190
generate text files that you feed into

00:09:02,130 --> 00:09:10,890
the exporter and you maybe already found

00:09:08,190 --> 00:09:14,280
out that many software that you run open

00:09:10,890 --> 00:09:19,380
source stuff provides Prometheus style

00:09:14,280 --> 00:09:21,750
metrics by default kubernetes for

00:09:19,380 --> 00:09:25,410
example is probably the the most famous

00:09:21,750 --> 00:09:27,720
one and there are hundreds of different

00:09:25,410 --> 00:09:30,120
exporters available that you can check

00:09:27,720 --> 00:09:34,190
out to monitor all the different

00:09:30,120 --> 00:09:34,190
applications that you might have

00:09:44,740 --> 00:09:51,560
so um that's a list of all the different

00:09:48,230 --> 00:09:53,660
exporters that officially listed on the

00:09:51,560 --> 00:09:56,870
internet and there is a bunch available

00:09:53,660 --> 00:09:58,460
for different databases to monitor are

00:09:56,870 --> 00:10:01,940
very very different

00:09:58,460 --> 00:10:02,600
half of endures issue trackers messaging

00:10:01,940 --> 00:10:05,560
systems

00:10:02,600 --> 00:10:08,930
so there are exporters available for

00:10:05,560 --> 00:10:11,990
almost everything if you would like to

00:10:08,930 --> 00:10:14,300
write a known exporter it's still very

00:10:11,990 --> 00:10:16,190
very easy the Prometheus people provide

00:10:14,300 --> 00:10:18,590
and go templates if you would like to

00:10:16,190 --> 00:10:21,310
write something and go but there are

00:10:18,590 --> 00:10:24,050
also many documentation available for

00:10:21,310 --> 00:10:26,710
custom exporters written in different

00:10:24,050 --> 00:10:26,710
languages

00:10:33,560 --> 00:10:40,639
sooo we all agree on that

00:10:38,110 --> 00:10:42,139
going on the the perimeter style might

00:10:40,639 --> 00:10:44,540
be the the best approach that we could

00:10:42,139 --> 00:10:46,879
go to create a new monitoring for all

00:10:44,540 --> 00:10:48,769
the thousands of systems that we have so

00:10:46,879 --> 00:10:51,800
we no need to think about how we can

00:10:48,769 --> 00:10:54,129
integrate the new software into our

00:10:51,800 --> 00:10:54,129
environment

00:10:55,269 --> 00:11:01,309
Prometheus works on a pool approach as

00:10:59,240 --> 00:11:05,089
you might can think of there is a

00:11:01,309 --> 00:11:08,319
central database the Prometheus daemon

00:11:05,089 --> 00:11:10,610
and it connects to all the different

00:11:08,319 --> 00:11:13,639
exporters that might be available and

00:11:10,610 --> 00:11:15,800
scrapes them on a defined time interval

00:11:13,639 --> 00:11:18,079
for example every five seconds every ten

00:11:15,800 --> 00:11:22,160
seconds every minute whatever you think

00:11:18,079 --> 00:11:27,769
might be a good approach to roll out the

00:11:22,160 --> 00:11:29,930
node exporter you just need a single

00:11:27,769 --> 00:11:32,660
line of puppet code which is very very

00:11:29,930 --> 00:11:35,620
neat the Vox Populi people provide a

00:11:32,660 --> 00:11:38,180
very very good puppet module called

00:11:35,620 --> 00:11:39,889
Prometheus that cannot only configure

00:11:38,180 --> 00:11:42,199
the premiere first database but also I

00:11:39,889 --> 00:11:45,290
think around 20 different exporters

00:11:42,199 --> 00:11:48,740
adding support for another exporters

00:11:45,290 --> 00:11:50,240
also very very easy and yeah you just

00:11:48,740 --> 00:11:54,550
need to sing a line of puppet code to

00:11:50,240 --> 00:11:58,990
just deploy it that's very nice

00:11:54,550 --> 00:12:01,699
as it turns out it might be not so nice

00:11:58,990 --> 00:12:04,660
because the deployed exporter is

00:12:01,699 --> 00:12:07,699
publicly available on the internet so

00:12:04,660 --> 00:12:09,860
everybody can access your metrics which

00:12:07,699 --> 00:12:14,449
is nice if you if you like distributed

00:12:09,860 --> 00:12:16,699
backups from other people depending on

00:12:14,449 --> 00:12:19,429
the content of the metrics that might

00:12:16,699 --> 00:12:26,300
not be the best idea and this somehow

00:12:19,429 --> 00:12:29,449
needs to be secured and the Prometheus

00:12:26,300 --> 00:12:32,199
server somehow needs to get a list of

00:12:29,449 --> 00:12:36,129
all the targets that it should scrape

00:12:32,199 --> 00:12:39,230
most people like to highest UHN for this

00:12:36,129 --> 00:12:41,089
it maintains a long text file with a P

00:12:39,230 --> 00:12:43,129
addresses and the premiere server

00:12:41,089 --> 00:12:47,000
iterates on the list of IP addresses and

00:12:43,129 --> 00:12:48,560
well that doesn't scale very very good

00:12:47,000 --> 00:12:53,330
students normally leave the company if

00:12:48,560 --> 00:12:54,890
they have to do such jobs it's it leads

00:12:53,330 --> 00:12:58,790
to a bunch of errors so this somehow

00:12:54,890 --> 00:13:03,310
needs to be automated as well there is a

00:12:58,790 --> 00:13:03,310
very fancy approach that we can go here

00:13:04,420 --> 00:13:08,360
prometheus

00:13:05,540 --> 00:13:12,290
supports service discovery for example

00:13:08,360 --> 00:13:15,200
connecting to a zookeeper or a console

00:13:12,290 --> 00:13:17,960
service account or cluster and it can

00:13:15,200 --> 00:13:21,710
get a bunch of targets from the service

00:13:17,960 --> 00:13:25,460
registry and scrape all the services

00:13:21,710 --> 00:13:28,190
that it can get so the approach that we

00:13:25,460 --> 00:13:30,440
can go Ria's to set up a small console

00:13:28,190 --> 00:13:32,090
instance and again that's just a single

00:13:30,440 --> 00:13:34,400
binary that you could download from the

00:13:32,090 --> 00:13:38,540
internet and run its route but maybe you

00:13:34,400 --> 00:13:42,910
should not do this and instead package

00:13:38,540 --> 00:13:47,360
it properly and deploy it and afterwards

00:13:42,910 --> 00:13:49,130
you can register every of the exporters

00:13:47,360 --> 00:13:51,140
that we deployed to the cons were

00:13:49,130 --> 00:13:54,560
classed and say the cluster hey I'm

00:13:51,140 --> 00:13:59,480
available on this IP on that TCP port

00:13:54,560 --> 00:14:06,230
have fun accessing me so and the whole

00:13:59,480 --> 00:14:08,990
diagram would look like this there is a

00:14:06,230 --> 00:14:10,820
node exporter running on a box or any

00:14:08,990 --> 00:14:12,830
other exporter that you would like to

00:14:10,820 --> 00:14:16,220
deploy or our custom application that

00:14:12,830 --> 00:14:18,589
provides Prometheus style metrics it

00:14:16,220 --> 00:14:22,190
talks to a local console agent and tests

00:14:18,589 --> 00:14:24,230
it on which appeared is accessible those

00:14:22,190 --> 00:14:27,200
information are fitted into a console

00:14:24,230 --> 00:14:29,150
cluster of more than three nodes but my

00:14:27,200 --> 00:14:31,510
slide was too small for more than free

00:14:29,150 --> 00:14:34,220
so yeah

00:14:31,510 --> 00:14:36,260
and somewhere the box running the

00:14:34,220 --> 00:14:38,839
Prometheus agent again talking to a

00:14:36,260 --> 00:14:41,450
local console agent and asking it like

00:14:38,839 --> 00:14:44,030
hey please provide me a list with all

00:14:41,450 --> 00:14:46,580
the IP addresses konso talks to the

00:14:44,030 --> 00:14:51,470
cluster again forwards those information

00:14:46,580 --> 00:14:53,960
to prometheus Prometheus no now's the IP

00:14:51,470 --> 00:14:59,480
addresses and can directly talk to the

00:14:53,960 --> 00:15:00,800
node exporter seems to be a bit more

00:14:59,480 --> 00:15:02,690
complex than

00:15:00,800 --> 00:15:04,880
just downloading random binaries from

00:15:02,690 --> 00:15:06,260
the internet but still this looks like

00:15:04,880 --> 00:15:09,320
something that could be implemented

00:15:06,260 --> 00:15:14,620
pretty pretty easily and we have an

00:15:09,320 --> 00:15:14,620
example that we could maybe show

00:15:17,920 --> 00:15:21,110
[Music]

00:15:29,160 --> 00:15:36,579
so we have a bunch of puppet code here

00:15:32,700 --> 00:15:38,740
we basically start defining a config

00:15:36,579 --> 00:15:42,850
cache that we can later feed onto to

00:15:38,740 --> 00:15:47,860
console IP addresses where the kondal

00:15:42,850 --> 00:15:51,010
bind to its custom host name that will

00:15:47,860 --> 00:15:53,950
be represented to the cluster afterwards

00:15:51,010 --> 00:15:59,100
we can deploy the console agent itself

00:15:53,950 --> 00:16:02,190
and afterwards define a service oops

00:15:59,100 --> 00:16:09,370
and this service basically tells the

00:16:02,190 --> 00:16:12,160
console service registry the fully

00:16:09,370 --> 00:16:15,820
qualified host name where if service is

00:16:12,160 --> 00:16:18,760
available the port word will be

00:16:15,820 --> 00:16:22,420
available and it will also define a

00:16:18,760 --> 00:16:26,310
local check which console uses to ensure

00:16:22,420 --> 00:16:29,529
that the note export is to available

00:16:26,310 --> 00:16:32,140
yeah and in this case since the console

00:16:29,529 --> 00:16:35,019
agent is running on localhost next to

00:16:32,140 --> 00:16:37,630
every exporter we can just go on and

00:16:35,019 --> 00:16:39,880
talk to local host plain text and check

00:16:37,630 --> 00:16:43,269
every 10 second if the Nordics port is

00:16:39,880 --> 00:16:46,600
still available if it's not available

00:16:43,269 --> 00:16:49,180
kwang-soo can send us notifications so

00:16:46,600 --> 00:16:51,250
this is a very neat approach to

00:16:49,180 --> 00:16:56,550
implement monitoring for the monitoring

00:16:51,250 --> 00:16:56,550
solution which isn't a bad idea

00:17:02,770 --> 00:17:08,800
so um we now have a diagram in our head

00:17:06,939 --> 00:17:11,679
we know how we can connect all the

00:17:08,800 --> 00:17:13,329
different services together we also have

00:17:11,679 --> 00:17:16,270
service discovery in place so the

00:17:13,329 --> 00:17:19,360
promethazine knows how to talk to the

00:17:16,270 --> 00:17:28,750
different exporters what we still need

00:17:19,360 --> 00:17:31,390
authentication so happily someone wrote

00:17:28,750 --> 00:17:35,290
out puppet on all our systems that's an

00:17:31,390 --> 00:17:38,020
assumption we also need to make poverty

00:17:35,290 --> 00:17:41,760
on a client-server model and every agent

00:17:38,020 --> 00:17:44,980
has custom TLS client certificates

00:17:41,760 --> 00:17:47,860
puppets a complete private key

00:17:44,980 --> 00:17:50,860
infrastructure and we can make use of

00:17:47,860 --> 00:17:57,490
this and reuse the whole certificates

00:17:50,860 --> 00:18:02,350
which is very very easy again as I

00:17:57,490 --> 00:18:03,610
mentioned the exporter is plaintext it

00:18:02,350 --> 00:18:06,700
doesn't support any encryption at all

00:18:03,610 --> 00:18:08,800
but we can deploy an engineer's in front

00:18:06,700 --> 00:18:12,460
of it on each box and just bind the

00:18:08,800 --> 00:18:15,250
exporter to localhost and the nginx can

00:18:12,460 --> 00:18:16,809
require or end force actually TLS

00:18:15,250 --> 00:18:19,660
certificates for every incoming

00:18:16,809 --> 00:18:23,220
connection that for every system that

00:18:19,660 --> 00:18:26,440
would like to talk to the node exporter

00:18:23,220 --> 00:18:28,240
and it cannot only enforced you let TLS

00:18:26,440 --> 00:18:28,870
but it can also require a client

00:18:28,240 --> 00:18:32,410
certificate

00:18:28,870 --> 00:18:35,020
this means that nginx can check if the

00:18:32,410 --> 00:18:36,670
premier house database connects to the

00:18:35,020 --> 00:18:41,290
node exporter with a valid certificate

00:18:36,670 --> 00:18:44,410
and you know by implementing this we can

00:18:41,290 --> 00:18:47,200
ensure that only boxes that we like are

00:18:44,410 --> 00:18:50,380
able to talk to our different exporters

00:18:47,200 --> 00:18:54,880
or custom applications that provide

00:18:50,380 --> 00:18:57,550
those metrics for us again there is a

00:18:54,880 --> 00:19:00,150
very very neat puppet module to manage

00:18:57,550 --> 00:19:03,429
nginx

00:19:00,150 --> 00:19:06,010
that we can use to deploy all the stuff

00:19:03,429 --> 00:19:09,300
that I just mentioned and there is some

00:19:06,010 --> 00:19:09,300
example code as well

00:19:19,030 --> 00:19:32,050
so what are we doing here um we have

00:19:28,330 --> 00:19:36,970
very simple setup for the exporter as

00:19:32,050 --> 00:19:38,470
well in this case I just define custom

00:19:36,970 --> 00:19:41,350
variables that I would like to read and

00:19:38,470 --> 00:19:43,840
text files not so important for this

00:19:41,350 --> 00:19:47,230
demo but I also tell it to just run on

00:19:43,840 --> 00:19:49,960
localhost and on the default tcp port

00:19:47,230 --> 00:19:53,590
that it normally uses I tell puppet to

00:19:49,960 --> 00:19:55,570
include the nginx module afterwards we

00:19:53,590 --> 00:19:57,670
need to copy a bunch of custom

00:19:55,570 --> 00:20:01,030
certificates that are coming from the

00:19:57,670 --> 00:20:04,830
puppet directory to nginx directories so

00:20:01,030 --> 00:20:04,830
nginx is able to read these certificates

00:20:06,840 --> 00:20:15,610
and at the end we define a very very

00:20:10,120 --> 00:20:21,220
basic nginx v host the most important

00:20:15,610 --> 00:20:23,500
bits here at the end we tell the nginx

00:20:21,220 --> 00:20:25,360
viewers to read all the custom

00:20:23,500 --> 00:20:28,090
certificates and private keys that are

00:20:25,360 --> 00:20:31,240
coming from our puppet agent so we can

00:20:28,090 --> 00:20:34,990
do proper TLS connections and we are

00:20:31,240 --> 00:20:38,970
also enforcing TLS client certificates

00:20:34,990 --> 00:20:44,020
with the last variable that we have here

00:20:38,970 --> 00:20:46,600
in this case the nginx will accept any

00:20:44,020 --> 00:20:48,880
valid certificate so in theory every

00:20:46,600 --> 00:20:52,330
system where a puppet agent is installed

00:20:48,880 --> 00:20:53,910
could talk to this nginx instance but

00:20:52,330 --> 00:20:56,860
you could also update the configuration

00:20:53,910 --> 00:20:58,660
to just accept a very very specific

00:20:56,860 --> 00:21:00,790
common names for example from the

00:20:58,660 --> 00:21:03,270
Prometheus database as I mentioned

00:21:00,790 --> 00:21:03,270
earlier

00:21:09,650 --> 00:21:16,309
so on all exporters no deployed it's now

00:21:13,520 --> 00:21:21,190
properly secured with authentication and

00:21:16,309 --> 00:21:24,370
encryption sadly we will notice that

00:21:21,190 --> 00:21:31,370
console itself ships with no

00:21:24,370 --> 00:21:33,200
authentication at all in the default but

00:21:31,370 --> 00:21:35,180
we can do a very very similar approach

00:21:33,200 --> 00:21:37,730
here with console as well because it

00:21:35,180 --> 00:21:39,800
also supports custom TLS client

00:21:37,730 --> 00:21:44,900
certificates and a secret key that we

00:21:39,800 --> 00:21:47,780
can define in the config file yes so

00:21:44,900 --> 00:21:55,460
that's a very very similar to putting

00:21:47,780 --> 00:21:57,559
nginx in front firewalling so it's nice

00:21:55,460 --> 00:21:59,510
that we have authentication in place but

00:21:57,559 --> 00:22:02,540
it still might be a very very good idea

00:21:59,510 --> 00:22:05,000
to do some actual firewalling with IP

00:22:02,540 --> 00:22:07,070
Turbots on the nodes so that the nginx

00:22:05,000 --> 00:22:10,070
of the premier first database or console

00:22:07,070 --> 00:22:12,380
or whatever we will deploy is properly

00:22:10,070 --> 00:22:16,300
firewalled and only accessible from the

00:22:12,380 --> 00:22:18,860
systems where we want it to be

00:22:16,300 --> 00:22:23,390
firewalling should always be automated

00:22:18,860 --> 00:22:26,330
many work costs a lot of time many work

00:22:23,390 --> 00:22:28,130
doesn't make any fun most important

00:22:26,330 --> 00:22:33,020
reason but it also leads to a bunch of

00:22:28,130 --> 00:22:36,730
errors and nobody likes to debug broken

00:22:33,020 --> 00:22:40,460
firewall rules at 3 a.m. in the morning

00:22:36,730 --> 00:22:42,860
what so IP addresses might change very

00:22:40,460 --> 00:22:45,710
very often if you have a business that

00:22:42,860 --> 00:22:48,970
grows pretty pretty good

00:22:45,710 --> 00:22:51,800
we might constantly add new boxes to our

00:22:48,970 --> 00:22:53,480
system did we like to monitor ah sure so

00:22:51,800 --> 00:22:58,610
we would constantly need to update

00:22:53,480 --> 00:23:02,240
firewall rules also in a very big

00:22:58,610 --> 00:23:05,120
company there might be cases where a box

00:23:02,240 --> 00:23:08,450
will be reprovision so the appeal

00:23:05,120 --> 00:23:09,800
address is still reachable but the box

00:23:08,450 --> 00:23:13,910
might serve a completely different

00:23:09,800 --> 00:23:17,179
purpose and especially if you're using a

00:23:13,910 --> 00:23:18,620
cloud environment like AWS you have a

00:23:17,179 --> 00:23:20,810
bunch of floating IPS that will be

00:23:18,620 --> 00:23:23,289
changed or routed to different boxes so

00:23:20,810 --> 00:23:27,859
we also would need to update firewalling

00:23:23,289 --> 00:23:30,589
again puppet supports something called

00:23:27,859 --> 00:23:36,879
exported resources that we can use in

00:23:30,589 --> 00:23:39,589
this case the both example code

00:23:36,879 --> 00:23:41,809
something that would be executed on the

00:23:39,589 --> 00:23:44,239
actual database host or every database

00:23:41,809 --> 00:23:48,039
every Prometheus database host that we

00:23:44,239 --> 00:23:54,019
have and it will define a firewall rule

00:23:48,039 --> 00:23:57,440
and save it somewhere in puppet that's

00:23:54,019 --> 00:24:01,339
this part but it will just define it

00:23:57,440 --> 00:24:03,019
with the IP address from the Prometheus

00:24:01,339 --> 00:24:05,229
database but the rule will not be

00:24:03,019 --> 00:24:08,869
applied on the Prometheus database

00:24:05,229 --> 00:24:11,869
instead every system where the note

00:24:08,869 --> 00:24:15,459
exporter is running in our case would

00:24:11,869 --> 00:24:19,429
collect this firewall rule from the

00:24:15,459 --> 00:24:23,959
puppet ecosystem and deploy it on the

00:24:19,429 --> 00:24:28,069
Nordics port itself so this converges

00:24:23,959 --> 00:24:30,619
very very good and easily every system

00:24:28,069 --> 00:24:32,719
where we will install Prometheus

00:24:30,619 --> 00:24:35,599
database will generate a firewall rule

00:24:32,719 --> 00:24:38,389
it will not apply it locally but those

00:24:35,599 --> 00:24:40,669
rules will be forwarded to every system

00:24:38,389 --> 00:24:42,859
where the node exporters running the

00:24:40,669 --> 00:24:46,219
node exporter will import the rule apply

00:24:42,859 --> 00:24:49,519
it locally and therefore allow every

00:24:46,219 --> 00:24:52,789
Prometheus database to access the node

00:24:49,519 --> 00:24:55,609
exporter also very very neat skates very

00:24:52,789 --> 00:24:57,379
good and again the box probably people

00:24:55,609 --> 00:25:00,889
have a nice puppet module for this

00:24:57,379 --> 00:25:03,969
available two minutes your firewall

00:25:00,889 --> 00:25:03,969
rules with fam

00:25:05,910 --> 00:25:14,190
as it turns out firewalling for console

00:25:09,820 --> 00:25:16,420
is a bit more complicated the

00:25:14,190 --> 00:25:19,420
recommended approach for console is that

00:25:16,420 --> 00:25:22,090
an agent is installed on every system

00:25:19,420 --> 00:25:25,240
that needs to interact with condoms so

00:25:22,090 --> 00:25:27,640
services talk to a console that's runs

00:25:25,240 --> 00:25:31,780
on localhost so we need to deploy it

00:25:27,640 --> 00:25:34,059
basically on every machine and also

00:25:31,780 --> 00:25:36,010
console creates a mesh network that

00:25:34,059 --> 00:25:37,900
means that every agent no matter if it's

00:25:36,010 --> 00:25:39,790
running in a client on a server mode for

00:25:37,900 --> 00:25:43,809
console needs to talk to every other

00:25:39,790 --> 00:25:47,500
agent if we have 4,000 servers that

00:25:43,809 --> 00:25:50,500
means we need to create 3999 iptables

00:25:47,500 --> 00:25:54,429
routes just to allow every other console

00:25:50,500 --> 00:25:57,220
instance to access the system this leads

00:25:54,429 --> 00:26:01,120
to a bunch of errors or issues that we

00:25:57,220 --> 00:26:04,780
need to fight against know if you ever

00:26:01,120 --> 00:26:10,480
maintained IP table sets with 10000 IP

00:26:04,780 --> 00:26:14,740
addresses or more nobody won ok I'm

00:26:10,480 --> 00:26:16,360
sorry for you see so IP tables doesn't

00:26:14,740 --> 00:26:18,820
scale very very good with a bunch of

00:26:16,360 --> 00:26:23,530
rules the more roots you add the slower

00:26:18,820 --> 00:26:26,559
IP tables gets you might not notice this

00:26:23,530 --> 00:26:29,020
with around 4,000 systems but maybe if

00:26:26,559 --> 00:26:30,280
you had like 10,000 rewards or 50,000

00:26:29,020 --> 00:26:32,410
druids and somewhere there it's a

00:26:30,280 --> 00:26:34,260
bottleneck and restarting IP tables and

00:26:32,410 --> 00:26:37,090
applying all the rules after a change

00:26:34,260 --> 00:26:39,059
might need one minute or five minutes or

00:26:37,090 --> 00:26:43,450
20 minutes just to apply all the

00:26:39,059 --> 00:26:45,520
firewall rules also depending on the

00:26:43,450 --> 00:26:47,770
structure of the rules every incoming

00:26:45,520 --> 00:26:50,400
packet needs to go through most of the

00:26:47,770 --> 00:26:53,890
rules until it finds a rule that matches

00:26:50,400 --> 00:26:58,630
and this also leads to an increased time

00:26:53,890 --> 00:27:02,610
to proceed for every packet there is a

00:26:58,630 --> 00:27:02,610
solution happily

00:27:04,220 --> 00:27:10,260
there is a very nice corner feature

00:27:06,420 --> 00:27:13,200
called AP set and we can feed into the

00:27:10,260 --> 00:27:16,890
Linux kernel a bunch of IP addresses or

00:27:13,200 --> 00:27:21,210
IP ranges and the kernel to save those

00:27:16,890 --> 00:27:24,060
IP addresses as a as a hash map and for

00:27:21,210 --> 00:27:27,360
every incoming packet into one of our IP

00:27:24,060 --> 00:27:30,840
tables chains we can tell IP tables to

00:27:27,360 --> 00:27:33,570
check the source or destination address

00:27:30,840 --> 00:27:37,350
of this specific packet against the hash

00:27:33,570 --> 00:27:39,900
map that's safe in the car no you cannot

00:27:37,350 --> 00:27:41,970
mix ipv4 and ipv6 in one hash map but

00:27:39,900 --> 00:27:44,250
that's not so bad we can just create a

00:27:41,970 --> 00:27:49,620
second hash map one for v6 only and one

00:27:44,250 --> 00:27:52,290
for ipv4 and updating the hash map

00:27:49,620 --> 00:27:53,760
scales pretty good and it really doesn't

00:27:52,290 --> 00:27:56,910
matter how many IP addresses you had to

00:27:53,760 --> 00:27:59,400
an app he said adding my IP address that

00:27:56,910 --> 00:28:03,150
doesn't slow adding more IP addresses to

00:27:59,400 --> 00:28:04,950
an ax pset doesn't slow it down and also

00:28:03,150 --> 00:28:07,290
you don't need to update your IP tables

00:28:04,950 --> 00:28:13,310
rules if you update the hash map in the

00:28:07,290 --> 00:28:13,310
kernel and again we have a nice example

00:28:19,780 --> 00:28:25,950
so again defining this and puppet is

00:28:23,110 --> 00:28:29,050
pretty pretty easy

00:28:25,950 --> 00:28:30,280
we somewhere need to get on the IP

00:28:29,050 --> 00:28:31,810
addresses that we would like to the

00:28:30,280 --> 00:28:34,570
hashmap for example we do a hybrid

00:28:31,810 --> 00:28:36,700
lookup or read a text file talk to any

00:28:34,570 --> 00:28:39,310
database talk to our IP management

00:28:36,700 --> 00:28:40,870
enterprise solution that we bought for

00:28:39,310 --> 00:28:44,170
way too much money

00:28:40,870 --> 00:28:46,390
anything puppet somehow needs to get all

00:28:44,170 --> 00:28:50,020
the IP addresses afterwards we need to

00:28:46,390 --> 00:28:53,310
check that this is just i p v6 or ipv4

00:28:50,020 --> 00:28:56,860
so we can feed this to a single hash map

00:28:53,310 --> 00:28:59,620
defining this all so it's pretty pretty

00:28:56,860 --> 00:29:02,350
easily afterwards

00:28:59,620 --> 00:29:04,720
it makes sense to create a custom IP to

00:29:02,350 --> 00:29:07,810
this chain to form what other related

00:29:04,720 --> 00:29:10,270
packets into you we need to define

00:29:07,810 --> 00:29:13,030
another rule in the input chain that

00:29:10,270 --> 00:29:16,030
tells IP tables to for what packets to

00:29:13,030 --> 00:29:20,010
the custom chain in this case we match

00:29:16,030 --> 00:29:22,480
on all the ports and protocols that

00:29:20,010 --> 00:29:25,660
console uses for its normal agent to

00:29:22,480 --> 00:29:28,840
agent communication and last but not

00:29:25,660 --> 00:29:31,900
least we just tell I P tables to check

00:29:28,840 --> 00:29:38,560
the packets against the IP set that we

00:29:31,900 --> 00:29:43,180
defined earlier and we have a nice heck

00:29:38,560 --> 00:29:47,530
at the end if we go on with that as such

00:29:43,180 --> 00:29:49,600
an approach it might be the case that

00:29:47,530 --> 00:29:53,590
the IP address list that we got earlier

00:29:49,600 --> 00:29:56,020
from wherever example for from a custom

00:29:53,590 --> 00:29:58,660
text file might not be complete and that

00:29:56,020 --> 00:30:00,160
we have systems running somewhere with

00:29:58,660 --> 00:30:02,020
an IP address that we don't know anymore

00:30:00,160 --> 00:30:05,470
that's not correct in our ipam or

00:30:02,020 --> 00:30:08,740
whatever but we Stube on those systems

00:30:05,470 --> 00:30:12,220
to talk properly to our Prometheus

00:30:08,740 --> 00:30:15,070
database and the other way around in

00:30:12,220 --> 00:30:17,590
such such a situation we can just check

00:30:15,070 --> 00:30:21,760
if the IP address from the client is not

00:30:17,590 --> 00:30:24,610
in the list it would be this fancy

00:30:21,760 --> 00:30:28,030
understatement afterwards we can again

00:30:24,610 --> 00:30:31,120
create custom exported resources that

00:30:28,030 --> 00:30:33,520
any other system like hey I'm not in the

00:30:31,120 --> 00:30:35,350
IP tables list so you won't find

00:30:33,520 --> 00:30:38,470
me in the hashmap that we defined

00:30:35,350 --> 00:30:41,100
earlier but you can just again create

00:30:38,470 --> 00:30:49,210
custom iptables rules for me to allow me

00:30:41,100 --> 00:30:51,520
accessing your console agents and if you

00:30:49,210 --> 00:30:52,840
think again about the diagram or the of

00:30:51,520 --> 00:30:55,450
the architecture that I showed earlier

00:30:52,840 --> 00:30:57,070
everything should be now complete

00:30:55,450 --> 00:31:01,630
and we could come to a very short

00:30:57,070 --> 00:31:04,060
conclusion and it would be that rolling

00:31:01,630 --> 00:31:08,860
out parameterize export is pretty pretty

00:31:04,060 --> 00:31:11,080
easy and this is example I only use the

00:31:08,860 --> 00:31:13,680
node exporter but this works pretty

00:31:11,080 --> 00:31:16,030
pretty good with other exporters as well

00:31:13,680 --> 00:31:17,680
deploying the default configuration for

00:31:16,030 --> 00:31:20,830
a definite volume and just a single line

00:31:17,680 --> 00:31:22,510
of puppet code if you would like to

00:31:20,830 --> 00:31:24,840
configure stuff that might be a bit

00:31:22,510 --> 00:31:28,830
longer but it's still pretty pretty easy

00:31:24,840 --> 00:31:32,620
the puppet module for Prometheus

00:31:28,830 --> 00:31:36,970
supports around 20 exporters anymore

00:31:32,620 --> 00:31:41,530
exporters is also pretty easy console a

00:31:36,970 --> 00:31:43,560
service discovery works pretty good on

00:31:41,530 --> 00:31:47,050
the positive side on the negative side

00:31:43,560 --> 00:31:50,080
setting it up and just to provide a way

00:31:47,050 --> 00:31:52,930
for Prometheus to talk to an exporter is

00:31:50,080 --> 00:31:57,400
way more complicated than setting up the

00:31:52,930 --> 00:32:00,670
actual exporters so this was a very very

00:31:57,400 --> 00:32:03,220
short walkthrough and I could not try to

00:32:00,670 --> 00:32:05,560
set up a demo environment to actually

00:32:03,220 --> 00:32:07,890
prove that all the stuff that I said

00:32:05,560 --> 00:32:07,890
works

00:32:11,890 --> 00:32:18,840
I prepared a little vagrant environment

00:32:27,240 --> 00:32:34,559
depending on the Wi-Fi quality this

00:32:29,350 --> 00:32:37,270
needs 2 to 20 minutes you never know I

00:32:34,559 --> 00:32:39,040
will show you the code later this very

00:32:37,270 --> 00:32:41,380
good instance basically whenever setup

00:32:39,040 --> 00:32:43,950
super come to a class prometheus

00:32:41,380 --> 00:32:47,950
database a puppet server so we can get

00:32:43,950 --> 00:32:51,220
certificates deploy the node exporter on

00:32:47,950 --> 00:32:54,100
localhost and afterwards install nginx

00:32:51,220 --> 00:32:56,860
in front of it as a TLS reverse proxy

00:32:54,100 --> 00:32:59,410
this might take some time

00:32:56,860 --> 00:33:03,240
in the meantime are there already any

00:32:59,410 --> 00:33:03,240
questions that I could answer

00:33:14,940 --> 00:33:20,130
good catch orange it's red okay I'm

00:33:17,760 --> 00:33:22,680
sorry English how do you manage updates

00:33:20,130 --> 00:33:25,140
of the exporters if you have several

00:33:22,680 --> 00:33:27,210
ones do you just subscribe to github

00:33:25,140 --> 00:33:30,240
release notifications and do it manually

00:33:27,210 --> 00:33:34,100
or that we have any automation depends a

00:33:30,240 --> 00:33:36,660
bit on the operating system in my case

00:33:34,100 --> 00:33:41,090
when in the company I'm working for we

00:33:36,660 --> 00:33:44,480
actually have around 3000 systems around

00:33:41,090 --> 00:33:46,970
150 of them are optional Knox based and

00:33:44,480 --> 00:33:49,620
the Arch Linux developers provide

00:33:46,970 --> 00:33:49,950
packages for all the exporters that we

00:33:49,620 --> 00:33:52,380
need

00:33:49,950 --> 00:33:54,060
that's easy so I just need to update the

00:33:52,380 --> 00:33:58,650
operating system and I get the newest

00:33:54,060 --> 00:34:01,950
versions the majority of the other

00:33:58,650 --> 00:34:04,680
systems are Red Hat based and in this

00:34:01,950 --> 00:34:09,500
case I'm subscribed to the github

00:34:04,680 --> 00:34:09,500
release pages to get notified thank you

00:34:12,860 --> 00:34:17,360
hi

00:34:14,310 --> 00:34:20,399
can you show again the code where the

00:34:17,360 --> 00:34:23,100
premiership server implements the jobs

00:34:20,399 --> 00:34:32,129
from account from the console registry

00:34:23,100 --> 00:34:38,850
sure so we can take a look at the code

00:34:32,129 --> 00:34:41,730
that will be deployed in my demo can you

00:34:38,850 --> 00:34:44,360
read that in the back or is that too

00:34:41,730 --> 00:34:44,360
small okay

00:34:44,570 --> 00:34:47,749
[Music]

00:34:52,540 --> 00:34:58,590
so the Prometheus setup is pretty simple

00:34:56,530 --> 00:35:06,160
[Music]

00:34:58,590 --> 00:35:09,850
so that should be the related paths to

00:35:06,160 --> 00:35:14,920
talk to a service discovery there is an

00:35:09,850 --> 00:35:17,320
option console SD convicts again I

00:35:14,920 --> 00:35:19,420
totally promise our server that there is

00:35:17,320 --> 00:35:21,910
a console agent available on localhost

00:35:19,420 --> 00:35:23,980
that it can talk to you it should check

00:35:21,910 --> 00:35:26,620
in this case only for services that are

00:35:23,980 --> 00:35:30,010
named node exporter and it can talk to

00:35:26,620 --> 00:35:35,260
console via HTTP because well that's

00:35:30,010 --> 00:35:38,260
final look lost and afterwards I tell

00:35:35,260 --> 00:35:40,690
Prometheus for this service that it

00:35:38,260 --> 00:35:43,750
found via console it should scrape then

00:35:40,690 --> 00:35:48,610
every 10 seconds via HTTP and it should

00:35:43,750 --> 00:35:53,200
use again custom certificates ok thanks

00:35:48,610 --> 00:35:56,350
sure are there any other questions

00:35:53,200 --> 00:35:59,310
yes quick question you mentioned that

00:35:56,350 --> 00:36:02,710
you want to improve the security of the

00:35:59,310 --> 00:36:05,230
access to the knowledge purchase if you

00:36:02,710 --> 00:36:06,610
have a shared system like shared hosting

00:36:05,230 --> 00:36:08,680
which as opposed you have in your

00:36:06,610 --> 00:36:11,710
environment how do you prohibit local

00:36:08,680 --> 00:36:16,900
users but as each access from accessing

00:36:11,710 --> 00:36:20,080
your local not exported I would probably

00:36:16,900 --> 00:36:22,690
try to implement iptables routes to

00:36:20,080 --> 00:36:26,640
probe it this depending on the system

00:36:22,690 --> 00:36:31,570
might also be possible to use as a linux

00:36:26,640 --> 00:36:34,720
to enforce this or to create a custom

00:36:31,570 --> 00:36:37,690
network name space where you can well

00:36:34,720 --> 00:36:39,760
isolate all the different users so they

00:36:37,690 --> 00:36:43,330
all can talk to an IP address that's

00:36:39,760 --> 00:36:46,720
called localhost but in reality it's a

00:36:43,330 --> 00:36:48,940
different one and also the IP address

00:36:46,720 --> 00:36:51,970
range on the loopback interface is

00:36:48,940 --> 00:36:57,660
pretty wide so you could just find the

00:36:51,970 --> 00:37:02,230
node exporter or not to 127 0 0 1 by 2 0

00:36:57,660 --> 00:37:04,270
0.2 for example and use this IP address

00:37:02,230 --> 00:37:05,359
for firewalling it also makes a lot of

00:37:04,270 --> 00:37:15,460
stuff easier

00:37:05,359 --> 00:37:16,630
Thanks yeah any other questions okay

00:37:15,460 --> 00:37:19,279
[Music]

00:37:16,630 --> 00:37:20,839
okay they was still working it's

00:37:19,279 --> 00:37:24,979
donating stuff from the Internet

00:37:20,839 --> 00:37:26,989
it didn't fail yet that's good so we can

00:37:24,979 --> 00:37:38,900
take a closer look to the rest of the

00:37:26,989 --> 00:37:41,390
code it's used in this demo so the demo

00:37:38,900 --> 00:37:43,759
starts by setting of the repository

00:37:41,390 --> 00:37:45,109
because that it's required on that

00:37:43,759 --> 00:37:47,769
CentOS machine to get a bunch of

00:37:45,109 --> 00:37:50,599
packages that we would like to use

00:37:47,769 --> 00:37:57,880
afterwards we install all the mentioned

00:37:50,599 --> 00:38:01,749
packages after death done I am deploying

00:37:57,880 --> 00:38:05,719
the puppet server on my box so I have

00:38:01,749 --> 00:38:10,579
ability to get proper TLS client

00:38:05,719 --> 00:38:13,219
certificates for all the systems and

00:38:10,579 --> 00:38:16,430
again there is configuration of sins are

00:38:13,219 --> 00:38:20,559
mostly default doing a yum installed

00:38:16,430 --> 00:38:20,559
puppet server but probably do the same

00:38:21,009 --> 00:38:29,269
afterwards the console agent gets

00:38:24,619 --> 00:38:35,719
deployed in this case also as a server

00:38:29,269 --> 00:38:40,279
that should be defined yes my laptop

00:38:35,719 --> 00:38:44,359
it's not a 19 19 inch rack so I won't

00:38:40,279 --> 00:38:47,660
deploy full consumer cluster with seven

00:38:44,359 --> 00:38:52,099
notes but just a single instance that

00:38:47,660 --> 00:38:55,190
works pretty good as a demo and again

00:38:52,099 --> 00:38:57,559
Kondo will be told to use custom

00:38:55,190 --> 00:39:00,049
certificates that are available on the

00:38:57,559 --> 00:39:07,009
box because I installed the puppet setup

00:39:00,049 --> 00:39:09,279
earlier the private key from the

00:39:07,009 --> 00:39:12,459
certificate needs to be copied from the

00:39:09,279 --> 00:39:15,349
directory to the console directory

00:39:12,459 --> 00:39:20,319
afterwards the premier server will be

00:39:15,349 --> 00:39:20,319
deployed and the code that I'm just

00:39:20,980 --> 00:39:25,460
and there are a bunch of puppet file

00:39:23,240 --> 00:39:27,470
resources again to copy all these

00:39:25,460 --> 00:39:33,740
certificates again into the Prometheus

00:39:27,470 --> 00:39:36,500
directory and if the demo I use the node

00:39:33,740 --> 00:39:39,530
exporter with all the collectors that I

00:39:36,500 --> 00:39:43,160
could find and somehow make sense just

00:39:39,530 --> 00:39:45,560
to get more metrics and delicate again

00:39:43,160 --> 00:39:49,310
to just is no localhost and use the

00:39:45,560 --> 00:39:52,069
latest version and nginx requires a few

00:39:49,310 --> 00:39:53,990
a zillion of options to be turned on so

00:39:52,069 --> 00:40:02,000
it can talk to you a lot participe

00:39:53,990 --> 00:40:03,770
socket yeah and as I just showed earlier

00:40:02,000 --> 00:40:06,890
again there is an engine X we host

00:40:03,770 --> 00:40:10,940
defined it will be listen in front of

00:40:06,890 --> 00:40:13,609
the not exporter terminate TLS

00:40:10,940 --> 00:40:21,410
connections and enforce it and require a

00:40:13,609 --> 00:40:23,750
valid certificate and at the end there

00:40:21,410 --> 00:40:26,450
is a console service definition and we

00:40:23,750 --> 00:40:29,170
will do some fire warning with firm and

00:40:26,450 --> 00:40:29,170
IP sets

00:40:34,650 --> 00:40:44,670
oh it's almost finished that's very very

00:40:38,460 --> 00:40:46,170
good so there should be a server up and

00:40:44,670 --> 00:40:49,979
running with everything that I would

00:40:46,170 --> 00:40:53,279
like to access if we have enough time we

00:40:49,979 --> 00:40:56,069
could also try to deploy a custom client

00:40:53,279 --> 00:40:57,839
so we get more metrics and I could try

00:40:56,069 --> 00:41:02,359
to prove that my fire warning setup

00:40:57,839 --> 00:41:02,359
actually works in the meantime

00:41:12,140 --> 00:41:17,090
are awesome so the Prometheus database

00:41:14,780 --> 00:41:20,150
is installed in the VM I just deployed

00:41:17,090 --> 00:41:24,470
and I can access it via pod for morning

00:41:20,150 --> 00:41:27,830
and if I search for any metric for

00:41:24,470 --> 00:41:32,480
example load I should be able to find a

00:41:27,830 --> 00:41:34,250
single system and I have a fancy graph

00:41:32,480 --> 00:41:38,360
it's a pretty short one because I just

00:41:34,250 --> 00:41:41,390
deployed the system but yeah that means

00:41:38,360 --> 00:41:44,210
that the whole code I just showed you

00:41:41,390 --> 00:41:47,030
somehow at least works and is able to

00:41:44,210 --> 00:41:52,340
provide the own setup as I mentioned

00:41:47,030 --> 00:41:59,510
yeah yeah are there any other questions

00:41:52,340 --> 00:41:59,840
that came up now no questions everybody

00:41:59,510 --> 00:42:10,870
happy

00:41:59,840 --> 00:42:14,180
or maybe confused very good so this

00:42:10,870 --> 00:42:16,660
presentation showed that well rolling

00:42:14,180 --> 00:42:19,850
out any export is pretty pretty easy

00:42:16,660 --> 00:42:22,010
most of the frameworks that are used for

00:42:19,850 --> 00:42:23,840
custom applications support already

00:42:22,010 --> 00:42:27,620
Prometheus diametric that could also be

00:42:23,840 --> 00:42:30,140
monitored doing proper authentication

00:42:27,620 --> 00:42:33,020
and firewalling takes a bit of time but

00:42:30,140 --> 00:42:35,570
it's also manageable especially if you

00:42:33,020 --> 00:42:38,390
are using puppets if you're interested

00:42:35,570 --> 00:42:40,100
in the talk or in the working demo you

00:42:38,390 --> 00:42:42,830
can check out the github repository that

00:42:40,100 --> 00:42:45,980
contains the vagrant file that are just

00:42:42,830 --> 00:42:47,630
used locally and yeah I think that's it

00:42:45,980 --> 00:42:53,520
from my part and thanks for your

00:42:47,630 --> 00:43:04,340
attention thank you

00:42:53,520 --> 00:43:04,340
[Music]

00:43:07,940 --> 00:43:10,690

YouTube URL: https://www.youtube.com/watch?v=-ijO-g4_7rU


