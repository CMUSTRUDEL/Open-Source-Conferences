Title: Data Migration With Confidence - RedDotRubyConf 2017
Publication date: 2020-01-22
Playlist: RedDotRuby 2017
Description: 
	Speaker: Juanito Fatas, Software Engineer, Cookpad

Finally convinced your new client to switch from X to Rails? Did your company acquire a non-rails site? Then you probably need to migrate their data to your existing system. In this talk, I will share some false starts, lessons, tips, optimisations and decisions from a recent data migration I performed. How to migrate large amount of photos and records? What tools will you need and how to test the data migration. What do you need to do before & after the data migration. What I tried and how I migrated large amounts of data while kept the site up and running.

Speaker's Bio

Juanito is a developer at Cookpad, based in Tokyo, Japan. He spends most of his time programming in Ruby, choosing which emoji to use, and seeking for good ramen.

Event Page: http://www.reddotrubyconf.com/

Produced by Engineers.SG

Help us caption & translate this video!

http://amara.org/v/8HYR/
Captions: 
	00:00:04,800 --> 00:00:14,650
hello everyone my name is bonito photos

00:00:10,270 --> 00:00:19,540
and I'm working at cookware ramen

00:00:14,650 --> 00:00:21,580
specialist and I have a Spanish name but

00:00:19,540 --> 00:00:26,619
actually I'm from Taiwan but live in

00:00:21,580 --> 00:00:29,680
Tokyo so recently I became a salaryman

00:00:26,619 --> 00:00:32,379
so if you can find me in this picture I

00:00:29,680 --> 00:00:37,210
can give you a special present after the

00:00:32,379 --> 00:00:40,300
presentation so I work at a crooked

00:00:37,210 --> 00:00:43,989
global team and we have another team

00:00:40,300 --> 00:00:46,870
Japan team's biggest monoliths of the

00:00:43,989 --> 00:00:49,960
world and I work for a global team and

00:00:46,870 --> 00:00:53,079
this slide is made in Japan

00:00:49,960 --> 00:00:57,670
so it's trustworthy you can rely on the

00:00:53,079 --> 00:01:00,760
content and the other day I was

00:00:57,670 --> 00:01:03,609
preparing the slides and I pull this

00:01:00,760 --> 00:01:07,479
slice to you know think about local jobs

00:01:03,609 --> 00:01:10,689
then a girl behind me just asked me what

00:01:07,479 --> 00:01:13,030
is your joke then I told her I don't

00:01:10,689 --> 00:01:16,720
know I need to think about my joke so I

00:01:13,030 --> 00:01:21,610
still don't figure my joke so I pull the

00:01:16,720 --> 00:01:25,420
slide here so today I'm here to talk

00:01:21,610 --> 00:01:30,040
about data migration and so what is a

00:01:25,420 --> 00:01:33,460
data migration so in Wales we do schema

00:01:30,040 --> 00:01:36,970
migration and the other one is data

00:01:33,460 --> 00:01:39,100
migration but the normal migration we

00:01:36,970 --> 00:01:41,380
talk about is schema migration which

00:01:39,100 --> 00:01:44,470
will change the database schemas over

00:01:41,380 --> 00:01:48,520
time while the data migration is you

00:01:44,470 --> 00:01:52,450
translate data from 68 to system B so

00:01:48,520 --> 00:01:55,720
and there are two types two types of

00:01:52,450 --> 00:01:57,850
data migration tool why is existing data

00:01:55,720 --> 00:02:01,570
migration in salt for example stripe

00:01:57,850 --> 00:02:04,180
they need to migrate earlier data in the

00:02:01,570 --> 00:02:06,460
system to a new model so this is

00:02:04,180 --> 00:02:08,530
existing your migration you can check

00:02:06,460 --> 00:02:11,200
out this talk and today I'm going to

00:02:08,530 --> 00:02:14,230
talk about you my guerrilla external

00:02:11,200 --> 00:02:15,930
data to your existing system for example

00:02:14,230 --> 00:02:18,420
you buy a company or

00:02:15,930 --> 00:02:23,760
you need to reload the site for trying

00:02:18,420 --> 00:02:27,319
so this is my talk today and so first

00:02:23,760 --> 00:02:31,140
why we need to do they have migration

00:02:27,319 --> 00:02:34,650
for example you convince your client to

00:02:31,140 --> 00:02:40,409
switch from the great PHP C website to

00:02:34,650 --> 00:02:43,069
Rails or other you know you have a new

00:02:40,409 --> 00:02:45,780
panel joining your company for example

00:02:43,069 --> 00:02:47,909
you have to your company share the same

00:02:45,780 --> 00:02:50,129
value and they join your company so you

00:02:47,909 --> 00:02:53,159
need to migrate it out to your website

00:02:50,129 --> 00:02:57,419
so you need to do some data migration

00:02:53,159 --> 00:02:59,939
and there's a single goal is to get all

00:02:57,419 --> 00:03:04,560
data to our system but is very hard to

00:02:59,939 --> 00:03:07,319
achieve map saying this morning so the

00:03:04,560 --> 00:03:09,180
single goal is to get all the data we

00:03:07,319 --> 00:03:12,269
only need to do these four things and

00:03:09,180 --> 00:03:15,120
this is very simple so the first thing

00:03:12,269 --> 00:03:17,519
to do is to get the actual data you need

00:03:15,120 --> 00:03:20,669
to migrate and there are two ways to

00:03:17,519 --> 00:03:22,470
gather data first way to if you are

00:03:20,669 --> 00:03:26,280
provided can provide you

00:03:22,470 --> 00:03:29,060
API so you can actually solar data or

00:03:26,280 --> 00:03:33,030
you can just get the data dump from the

00:03:29,060 --> 00:03:35,609
database and Cookeville we already did

00:03:33,030 --> 00:03:38,669
many migrations so if they have

00:03:35,609 --> 00:03:41,099
developed resource to build an API we

00:03:38,669 --> 00:03:43,639
already have a generic migration code to

00:03:41,099 --> 00:03:47,250
make all the things happen automatically

00:03:43,639 --> 00:03:50,459
so today I'm talking about the migration

00:03:47,250 --> 00:03:55,530
for data dump which is impossible to

00:03:50,459 --> 00:03:58,459
automate so let's see how to do it so we

00:03:55,530 --> 00:04:02,310
can start with sync already tossed as

00:03:58,459 --> 00:04:05,699
easy as you just run rails coming and

00:04:02,310 --> 00:04:11,069
hopefully everything will be migrated to

00:04:05,699 --> 00:04:13,199
your system and the Lemaitre you can

00:04:11,069 --> 00:04:15,900
just put whatever you need to microwave

00:04:13,199 --> 00:04:19,829
and start to write a code to make it

00:04:15,900 --> 00:04:23,159
happen but first we need to import a

00:04:19,829 --> 00:04:25,289
data to your system so you my model can

00:04:23,159 --> 00:04:28,050
connect to the database you want to

00:04:25,289 --> 00:04:29,159
migrate and it's as simple as one

00:04:28,050 --> 00:04:33,149
comment

00:04:29,159 --> 00:04:35,819
you just import to my sequel but in

00:04:33,149 --> 00:04:39,479
cookware we support currently is about

00:04:35,819 --> 00:04:42,559
62 countries with 13 million users so

00:04:39,479 --> 00:04:46,860
our production site has a lot of users

00:04:42,559 --> 00:04:49,289
24/7 across every time zone so we cannot

00:04:46,860 --> 00:04:51,839
just import psycho Dom because the

00:04:49,289 --> 00:04:55,050
database will be saturated is too fast

00:04:51,839 --> 00:04:57,959
so I need to add some delay to the

00:04:55,050 --> 00:05:02,849
psycho dump so I'm thinking just add

00:04:57,959 --> 00:05:07,800
some sleep statement before insert it is

00:05:02,849 --> 00:05:11,279
true it is but the synchro dump file is

00:05:07,800 --> 00:05:17,069
very huge I try all the editors below

00:05:11,279 --> 00:05:19,889
and all the editors doesn't work I have

00:05:17,069 --> 00:05:22,919
to use this one call hex friends you can

00:05:19,889 --> 00:05:25,079
actually edit a few gigabyte files but

00:05:22,919 --> 00:05:27,509
still not perfect because I'm a made a

00:05:25,079 --> 00:05:31,919
mistake so instead I write a single

00:05:27,509 --> 00:05:34,649
rupee program which used this amazing

00:05:31,919 --> 00:05:41,279
feature is called lazy that every time

00:05:34,649 --> 00:05:45,209
you only need to put all 2000 lines into

00:05:41,279 --> 00:05:49,529
memory so it's a fast and very easy to

00:05:45,209 --> 00:05:51,509
use and one of the friends say if you

00:05:49,529 --> 00:05:53,999
want to get better at Ruby programming

00:05:51,509 --> 00:05:59,399
you just read about a new level and read

00:05:53,999 --> 00:06:02,969
it again so yeah so this is how I edit

00:05:59,399 --> 00:06:05,729
delay to the psycho dump and so you can

00:06:02,969 --> 00:06:08,429
just connect the database through an

00:06:05,729 --> 00:06:11,429
environment variable and you sell a

00:06:08,429 --> 00:06:13,309
environment variable accordingly for

00:06:11,429 --> 00:06:16,499
development and staging and production

00:06:13,309 --> 00:06:19,769
then you can start to model in your

00:06:16,499 --> 00:06:23,429
database so the data you get from the

00:06:19,769 --> 00:06:26,579
provider is all kinds of formats and you

00:06:23,429 --> 00:06:29,399
just need to map them to your 16 cistern

00:06:26,579 --> 00:06:32,789
in the case of cookies user and recipes

00:06:29,399 --> 00:06:35,399
and other things and with all these five

00:06:32,789 --> 00:06:37,619
methods from rails basically you can

00:06:35,399 --> 00:06:42,089
model anything just these five methods

00:06:37,619 --> 00:06:43,020
is amazing and so let's see example to

00:06:42,089 --> 00:06:46,949
map the data

00:06:43,020 --> 00:06:51,120
to our current system so suppose recipe

00:06:46,949 --> 00:06:53,699
has many steps and sometimes the data is

00:06:51,120 --> 00:06:57,330
as easy you just specify the foreign key

00:06:53,699 --> 00:07:01,590
and create this one line of code and it

00:06:57,330 --> 00:07:06,060
automatically works but sometimes is as

00:07:01,590 --> 00:07:09,479
hard as the association's stores plan

00:07:06,060 --> 00:07:12,210
HTML in the recipes table so you need to

00:07:09,479 --> 00:07:16,500
write some number give a parser to pass

00:07:12,210 --> 00:07:19,530
all the HTML into the Association again

00:07:16,500 --> 00:07:23,060
so it is also not so hard you just need

00:07:19,530 --> 00:07:25,949
to write a parser that make all the data

00:07:23,060 --> 00:07:30,419
aligned with your current associations

00:07:25,949 --> 00:07:33,560
so modeling the database and we can also

00:07:30,419 --> 00:07:36,120
set up the test suite for the migration

00:07:33,560 --> 00:07:39,210
for example you just need to tell your

00:07:36,120 --> 00:07:41,879
test database to run against the

00:07:39,210 --> 00:07:45,120
different database and you just require

00:07:41,879 --> 00:07:48,330
all the files not being read also loaded

00:07:45,120 --> 00:07:52,289
and in the test you just require this

00:07:48,330 --> 00:07:54,840
special helper and you skip the test on

00:07:52,289 --> 00:07:58,050
the CI because you may not want to

00:07:54,840 --> 00:08:02,009
import your confidential valuable data

00:07:58,050 --> 00:08:04,349
to leave CI service but you may ask why

00:08:02,009 --> 00:08:07,680
do we need test because the migration

00:08:04,349 --> 00:08:09,719
code only used once because we we can

00:08:07,680 --> 00:08:14,310
write better code sort of boring test or

00:08:09,719 --> 00:08:17,219
we can use TDD to get things done so now

00:08:14,310 --> 00:08:19,279
you get a test so you can do remodeling

00:08:17,219 --> 00:08:23,340
and write tests and repeat this process

00:08:19,279 --> 00:08:26,250
so you can model all the data of only

00:08:23,340 --> 00:08:29,009
provider into your system so you just

00:08:26,250 --> 00:08:32,130
sold a PHP site will not work like rails

00:08:29,009 --> 00:08:35,969
application then you can start to a real

00:08:32,130 --> 00:08:38,159
migration and for the migration you who

00:08:35,969 --> 00:08:40,620
you need to create a record or update

00:08:38,159 --> 00:08:43,380
records you use all the method I will

00:08:40,620 --> 00:08:45,810
raise exception like safe ban or update

00:08:43,380 --> 00:08:47,839
bed or create ban because we want to

00:08:45,810 --> 00:08:51,209
fail fast to find all the errors that

00:08:47,839 --> 00:08:54,240
could be happen before a real migration

00:08:51,209 --> 00:08:56,730
happens so for example we can look at

00:08:54,240 --> 00:09:01,710
how to migrate recipes

00:08:56,730 --> 00:09:04,950
so I have this structure that I made a

00:09:01,710 --> 00:09:09,060
lot of migrators and each migrator in

00:09:04,950 --> 00:09:12,690
charge of my guerrilla entities so for

00:09:09,060 --> 00:09:17,070
example to my grave recipe you just need

00:09:12,690 --> 00:09:21,000
to write a Migrator for recipe and in

00:09:17,070 --> 00:09:23,700
the recipe model you just tell what

00:09:21,000 --> 00:09:27,960
attributes should be migrated and in

00:09:23,700 --> 00:09:34,040
this migraineurs class

00:09:27,960 --> 00:09:38,430
you just find final recipe from the

00:09:34,040 --> 00:09:40,860
provider data and create a recipe in

00:09:38,430 --> 00:09:44,010
your current system and just update all

00:09:40,860 --> 00:09:47,760
the attributes then it can it will

00:09:44,010 --> 00:09:50,820
migrate to your system is very simple

00:09:47,760 --> 00:09:53,400
and you just keep implementing this

00:09:50,820 --> 00:09:56,430
migrators and they all respond to the

00:09:53,400 --> 00:09:59,970
same interface so I always keep my color

00:09:56,430 --> 00:10:04,800
simple and stupid because I don't know

00:09:59,970 --> 00:10:07,470
how to do later programming so and you

00:10:04,800 --> 00:10:11,460
just keep explore migrators then your

00:10:07,470 --> 00:10:14,760
migration is some but I want to talk

00:10:11,460 --> 00:10:17,010
about how you ensure data integrity so

00:10:14,760 --> 00:10:20,070
the first thing you can do is to wrap

00:10:17,010 --> 00:10:22,380
all your migration code in transaction

00:10:20,070 --> 00:10:24,630
you also make your call it'll be faster

00:10:22,380 --> 00:10:26,340
because in transaction there will be

00:10:24,630 --> 00:10:30,150
less coming to your database

00:10:26,340 --> 00:10:33,570
I also you need to make your caller ID

00:10:30,150 --> 00:10:38,070
more potent it's very hard to pronounce

00:10:33,570 --> 00:10:40,080
and someone put it in your tweets but

00:10:38,070 --> 00:10:42,120
you do the same thing but produce the

00:10:40,080 --> 00:10:44,880
same result because we want to run the

00:10:42,120 --> 00:10:45,780
migration many times but produce the

00:10:44,880 --> 00:10:49,680
same result

00:10:45,780 --> 00:10:52,290
it's like FX equal to FX so basically

00:10:49,680 --> 00:10:56,640
you need to have something called

00:10:52,290 --> 00:11:00,720
observe is when the record is it you

00:10:56,640 --> 00:11:03,090
update when it recognizes you insert in

00:11:00,720 --> 00:11:06,420
my sequel or Postgres secret you have

00:11:03,090 --> 00:11:09,630
this two sequel that can do absurd or

00:11:06,420 --> 00:11:10,680
you can use this gem but I keep it

00:11:09,630 --> 00:11:14,060
simple just you

00:11:10,680 --> 00:11:16,470
is simple REST API to implement it

00:11:14,060 --> 00:11:17,520
beautiful we first want to make it right

00:11:16,470 --> 00:11:21,920
and make it fast

00:11:17,520 --> 00:11:26,070
later so this is absurd implementing

00:11:21,920 --> 00:11:30,600
simple rules and then our accuracy how

00:11:26,070 --> 00:11:33,899
can we ensure data accuracy first I just

00:11:30,600 --> 00:11:36,720
do some manual trick for a few data I

00:11:33,899 --> 00:11:40,350
migrated the this doesn't scale

00:11:36,720 --> 00:11:42,570
so I think about how to automate all the

00:11:40,350 --> 00:11:47,490
tricks so I can have more time to you

00:11:42,570 --> 00:11:50,760
know enjoy I don't need to work so for

00:11:47,490 --> 00:11:54,300
example you let's see how to check user

00:11:50,760 --> 00:11:58,920
with most recipes in some simple Ruby

00:11:54,300 --> 00:12:02,550
code first you just loop all the users

00:11:58,920 --> 00:12:06,209
has a lot of recipes and pass them to a

00:12:02,550 --> 00:12:10,050
user track object and in this user check

00:12:06,209 --> 00:12:15,150
object you just check about the recipes

00:12:10,050 --> 00:12:18,900
and the jars recipe then the recipe

00:12:15,150 --> 00:12:21,720
checker it looks like this so I just

00:12:18,900 --> 00:12:24,959
check if the recipe for a provider I'm

00:12:21,720 --> 00:12:28,740
going to migrate the count is in line

00:12:24,959 --> 00:12:32,520
with the recipes by grady to prepare or

00:12:28,740 --> 00:12:36,930
not so and follow geoff part you just

00:12:32,520 --> 00:12:40,920
change the method to use draft you see

00:12:36,930 --> 00:12:46,020
these two class didn't change much this

00:12:40,920 --> 00:12:50,100
i want to be simple so and the checker

00:12:46,020 --> 00:12:53,850
just gives you a log message to recall

00:12:50,100 --> 00:12:56,520
the information so you can implement

00:12:53,850 --> 00:12:58,709
your check like this in some simple

00:12:56,520 --> 00:13:00,690
movie class and you just keep adding

00:12:58,709 --> 00:13:02,910
more checkers because for example you

00:13:00,690 --> 00:13:07,620
need to check about my grade follows or

00:13:02,910 --> 00:13:11,490
comments or other entities so you use

00:13:07,620 --> 00:13:15,390
many small objects to compose what do

00:13:11,490 --> 00:13:18,300
you want to do and i believe everyone's

00:13:15,390 --> 00:13:21,779
code base you have every everywhere she

00:13:18,300 --> 00:13:23,740
has a lot of objects but it will become

00:13:21,779 --> 00:13:26,410
messy very soon so i

00:13:23,740 --> 00:13:29,560
coming to check about dry Abbey or Roma

00:13:26,410 --> 00:13:36,190
beats for better object design or trap

00:13:29,560 --> 00:13:39,040
laser so now we can make our coat a big

00:13:36,190 --> 00:13:42,490
one job because now we make sure it

00:13:39,040 --> 00:13:45,370
already works and for how many workers

00:13:42,490 --> 00:13:48,640
for how many CPU cause then you can have

00:13:45,370 --> 00:13:54,720
how many workers and you design your job

00:13:48,640 --> 00:13:57,970
to queuing a different cue so you can

00:13:54,720 --> 00:14:03,490
distinguish from the regular jobs and

00:13:57,970 --> 00:14:06,450
the job is the job is just you just call

00:14:03,490 --> 00:14:12,160
another class that will microwave a

00:14:06,450 --> 00:14:17,470
record and you migrate a recurring this

00:14:12,160 --> 00:14:20,170
class so and in the deck on job you need

00:14:17,470 --> 00:14:23,440
to lock every unexpected arrow

00:14:20,170 --> 00:14:28,540
so you just in the bass drop add a

00:14:23,440 --> 00:14:31,750
logger and let's queue every arrow every

00:14:28,540 --> 00:14:33,520
possible arrow then you can fix it but

00:14:31,750 --> 00:14:36,340
I'm not sure if this is good you can

00:14:33,520 --> 00:14:42,630
check out the next talk to handle the

00:14:36,340 --> 00:14:45,700
arrows better and you need to run your

00:14:42,630 --> 00:14:48,280
billing job against all your data to be

00:14:45,700 --> 00:14:51,370
migrated then you can find out all the

00:14:48,280 --> 00:14:55,510
arrows and you can fix them before the

00:14:51,370 --> 00:14:58,360
real migration and you will also need

00:14:55,510 --> 00:15:01,930
some tools during the migration for

00:14:58,360 --> 00:15:05,650
example retry mechanism and in my sequel

00:15:01,930 --> 00:15:08,680
if you have a foreign key or constraint

00:15:05,650 --> 00:15:12,370
when you do an insert or update you will

00:15:08,680 --> 00:15:15,580
have locks and this kind of and because

00:15:12,370 --> 00:15:19,060
you have many workers working to migrate

00:15:15,580 --> 00:15:22,720
data and they may try to actually send

00:15:19,060 --> 00:15:26,760
record so the lock will result in my

00:15:22,720 --> 00:15:30,420
sequel dialog and in rails you can just

00:15:26,760 --> 00:15:33,520
rescue from this exception and

00:15:30,420 --> 00:15:35,820
automatically that if we try after like

00:15:33,520 --> 00:15:41,190
two minutes and

00:15:35,820 --> 00:15:44,790
so after we try we will work or you will

00:15:41,190 --> 00:15:48,360
just retry again but sometimes and you

00:15:44,790 --> 00:15:50,670
need to make sure what you want to retry

00:15:48,360 --> 00:15:52,650
so the first time you don't need to want

00:15:50,670 --> 00:15:56,640
to rescue everything just see how the

00:15:52,650 --> 00:15:59,670
arrow is and to understand if this need

00:15:56,640 --> 00:16:03,330
to be automatically try or not and rails

00:15:59,670 --> 00:16:07,470
provided to API we try on in this car

00:16:03,330 --> 00:16:10,590
you can do anything you can imagine

00:16:07,470 --> 00:16:14,190
you can retry later or just discolored

00:16:10,590 --> 00:16:18,230
exception or retry is potentially longer

00:16:14,190 --> 00:16:20,970
is only two ABI and you have everything

00:16:18,230 --> 00:16:23,580
so but sometimes you cannot

00:16:20,970 --> 00:16:27,170
automatically to try and you need to

00:16:23,580 --> 00:16:30,870
look at the arrow in the fail view and

00:16:27,170 --> 00:16:36,210
in rescue you can implement a simple

00:16:30,870 --> 00:16:42,560
retried object that can retry our arrow

00:16:36,210 --> 00:16:47,310
found from a design later q and this

00:16:42,560 --> 00:16:50,940
retry rescue object just fine hourly

00:16:47,310 --> 00:16:54,270
failures and we in keulen and remove

00:16:50,940 --> 00:16:58,530
length so you can retry the arrows in

00:16:54,270 --> 00:17:00,900
the two you want to retry so in

00:16:58,530 --> 00:17:04,110
migration you also need to see some

00:17:00,900 --> 00:17:06,300
status reporting because you want to

00:17:04,110 --> 00:17:12,800
know how much data you still need to

00:17:06,300 --> 00:17:17,070
migrate and you can implement a simple

00:17:12,800 --> 00:17:20,430
Ruby to us called progress and you just

00:17:17,070 --> 00:17:23,190
loop through all the models and call the

00:17:20,430 --> 00:17:27,810
progress method only model and each

00:17:23,190 --> 00:17:30,680
progress method only model is just check

00:17:27,810 --> 00:17:34,500
how many did how you need to migrate and

00:17:30,680 --> 00:17:37,430
divided by total data you need to my

00:17:34,500 --> 00:17:42,150
grave then you can know the progress and

00:17:37,430 --> 00:17:45,870
you can just wrap another loop be swift

00:17:42,150 --> 00:17:49,290
to make it report every minute so you

00:17:45,870 --> 00:17:49,600
can know the migration progress every

00:17:49,290 --> 00:17:53,559
minute

00:17:49,600 --> 00:17:57,520
it and you also need to monitor the CPU

00:17:53,559 --> 00:18:00,160
usage and in our company we use graph

00:17:57,520 --> 00:18:04,530
Anna I'm not sure if you heard about but

00:18:00,160 --> 00:18:08,260
this is a good service to monitor your

00:18:04,530 --> 00:18:11,169
CPU usage or performance and requests

00:18:08,260 --> 00:18:13,750
okay so the next I want to talk about

00:18:11,169 --> 00:18:17,380
performance how to make your migration

00:18:13,750 --> 00:18:21,190
coal run faster but first performance is

00:18:17,380 --> 00:18:25,720
a ready halt because I spend so much

00:18:21,190 --> 00:18:27,760
time and every chance you make to the

00:18:25,720 --> 00:18:30,340
migration code you need to run through

00:18:27,760 --> 00:18:32,440
every record again because you need to

00:18:30,340 --> 00:18:37,270
make sure you works first then you can

00:18:32,440 --> 00:18:40,840
guarantee it is faster so some

00:18:37,270 --> 00:18:41,289
performance tip I found is you can

00:18:40,840 --> 00:18:45,340
preload

00:18:41,289 --> 00:18:48,250
the associations or you can minimize the

00:18:45,340 --> 00:18:51,429
scope of transaction and you can also

00:18:48,250 --> 00:18:56,250
kill the transaction isolation levels to

00:18:51,429 --> 00:18:59,260
make a faster and you can also avoid

00:18:56,250 --> 00:19:03,669
unnecessary codecs for example you

00:18:59,260 --> 00:19:06,460
create a user or recipe and you can just

00:19:03,669 --> 00:19:08,860
skip the moderation because it is

00:19:06,460 --> 00:19:12,789
finally migration you know it doesn't

00:19:08,860 --> 00:19:13,419
need to be moderate or you can use no

00:19:12,789 --> 00:19:19,150
touching

00:19:13,419 --> 00:19:22,120
so the ko will not touch the Association

00:19:19,150 --> 00:19:24,659
so you will be faster because you can

00:19:22,120 --> 00:19:28,750
touch them all after the migration and

00:19:24,659 --> 00:19:33,690
another tip is to process much more

00:19:28,750 --> 00:19:36,850
records in one single job and it is also

00:19:33,690 --> 00:19:41,980
very easy you just use each slice and

00:19:36,850 --> 00:19:46,929
you process 100 users in one job and you

00:19:41,980 --> 00:19:49,059
will be faster and so if you process 100

00:19:46,929 --> 00:19:52,000
records and some of the records may be

00:19:49,059 --> 00:19:55,570
already created and you can cash them in

00:19:52,000 --> 00:20:00,250
memory or later on I try to cash them in

00:19:55,570 --> 00:20:03,490
Redis so it's even faster and to make it

00:20:00,250 --> 00:20:05,970
faster you can also migrate the

00:20:03,490 --> 00:20:10,390
important things first for example only

00:20:05,970 --> 00:20:14,679
my grader first want to use 10,000 users

00:20:10,390 --> 00:20:16,330
with most recipes or but so my guerrilla

00:20:14,679 --> 00:20:20,679
important things first make your

00:20:16,330 --> 00:20:24,580
migration : faster but later on I run

00:20:20,679 --> 00:20:28,390
into a problem ease IO bond so ruby is

00:20:24,580 --> 00:20:31,780
actually super fast but my database has

00:20:28,390 --> 00:20:35,200
reached the limit of IO you can perform

00:20:31,780 --> 00:20:38,800
so I'll oh I need to fill up the

00:20:35,200 --> 00:20:43,770
database or I find out just decrease the

00:20:38,800 --> 00:20:46,179
workers actually make the IO Bunga

00:20:43,770 --> 00:20:48,460
better because if you have too many

00:20:46,179 --> 00:20:52,690
workers working on the same database

00:20:48,460 --> 00:20:55,120
your i/o will soon to fill up so you

00:20:52,690 --> 00:20:58,510
instead you decrease the worker actually

00:20:55,120 --> 00:21:02,230
can do more than more workers or you can

00:20:58,510 --> 00:21:07,510
do something like pack ether or pack up

00:21:02,230 --> 00:21:13,059
set to insert or absurd many records in

00:21:07,510 --> 00:21:15,790
one go so every time you make it fast

00:21:13,059 --> 00:21:20,410
you need to run the whole migration

00:21:15,790 --> 00:21:23,380
again and when you run a migration you

00:21:20,410 --> 00:21:28,480
need to look out your CPU usage to keep

00:21:23,380 --> 00:21:33,429
it and max 75% from my experience so

00:21:28,480 --> 00:21:36,100
your site one went down and after

00:21:33,429 --> 00:21:39,420
migration you can update all the things

00:21:36,100 --> 00:21:42,280
you need to update like counter cache or

00:21:39,420 --> 00:21:45,610
statistics or you need to touch some

00:21:42,280 --> 00:21:48,309
associations and you can also do a

00:21:45,610 --> 00:21:51,160
redirect because you want to read the

00:21:48,309 --> 00:21:55,300
real old data from the outside to your

00:21:51,160 --> 00:21:58,270
new site and first we do something like

00:21:55,300 --> 00:22:00,850
we generate a redirect tables and we

00:21:58,270 --> 00:22:03,670
hand to the provider and they design a

00:22:00,850 --> 00:22:07,030
redirect programs and their server start

00:22:03,670 --> 00:22:09,700
to redirect but this comes with a cost

00:22:07,030 --> 00:22:13,990
because the migration provider doesn't

00:22:09,700 --> 00:22:17,230
always have developers so recently we

00:22:13,990 --> 00:22:21,990
are working on my reservation service

00:22:17,230 --> 00:22:26,559
open-source to do these simple things

00:22:21,990 --> 00:22:30,220
that doesn't require developers from the

00:22:26,559 --> 00:22:33,600
provider okay so I'm going to share some

00:22:30,220 --> 00:22:38,169
stories about data migrations I've done

00:22:33,600 --> 00:22:40,990
for the email you better remove the

00:22:38,169 --> 00:22:44,440
duplicate emails before the migration or

00:22:40,990 --> 00:22:46,780
remove the invalid emails before the

00:22:44,440 --> 00:22:50,799
migration because this far more

00:22:46,780 --> 00:22:54,520
expensive to handle and after you

00:22:50,799 --> 00:22:56,169
migrate data to your site and you need

00:22:54,520 --> 00:22:59,290
to make sure you can taste all the

00:22:56,169 --> 00:23:03,730
emails because it will cost you a lot of

00:22:59,290 --> 00:23:07,390
troubles and another story is how to get

00:23:03,730 --> 00:23:11,919
the site dump so one of the provider the

00:23:07,390 --> 00:23:16,870
other site is 100 gigabytes ec2 and ec2

00:23:11,919 --> 00:23:19,690
has a bandwidth limit so if you do a SCP

00:23:16,870 --> 00:23:23,110
or a sink or other things you will take

00:23:19,690 --> 00:23:27,040
days that if nothing fell we sing the

00:23:23,110 --> 00:23:29,740
days or you can you know buy a more

00:23:27,040 --> 00:23:33,870
expensive easy to machine so the

00:23:29,740 --> 00:23:37,650
solution was just used pho to deliver

00:23:33,870 --> 00:23:44,790
encrypted prefix to Tokyo is actually

00:23:37,650 --> 00:23:48,910
faster than SVP and another stories

00:23:44,790 --> 00:23:50,679
migrated millions of records so when I

00:23:48,910 --> 00:23:53,559
need to migrate millions of records I

00:23:50,679 --> 00:23:55,210
look into all these solutions so the

00:23:53,559 --> 00:23:59,130
first one is active lurker with

00:23:55,210 --> 00:24:02,890
transaction or back insert or absurd or

00:23:59,130 --> 00:24:06,490
faster one active record import or even

00:24:02,890 --> 00:24:09,400
the fastest load data in file is my

00:24:06,490 --> 00:24:13,390
sequel command you can load data in file

00:24:09,400 --> 00:24:17,919
lattice so fast but it still takes like

00:24:13,390 --> 00:24:21,940
weeks or one month to migrate all these

00:24:17,919 --> 00:24:25,840
millions of datas so instead I think we

00:24:21,940 --> 00:24:30,320
just need to run this migration in very

00:24:25,840 --> 00:24:33,110
low priority job and when the user signs

00:24:30,320 --> 00:24:35,570
we actually migrate their data in a high

00:24:33,110 --> 00:24:38,090
priority queue so in this way I don't

00:24:35,570 --> 00:24:41,120
need to migrate all the millions of

00:24:38,090 --> 00:24:44,500
records to our system which is slowly

00:24:41,120 --> 00:24:48,740
migrating and if they do signing we

00:24:44,500 --> 00:24:51,590
migrate layer bookmarks or some other

00:24:48,740 --> 00:24:55,100
important data so it's simple as just

00:24:51,590 --> 00:24:57,740
check if user need to migrate then you

00:24:55,100 --> 00:24:59,300
run this migration bookmarks so you

00:24:57,740 --> 00:25:02,000
don't need to migrate millions of data

00:24:59,300 --> 00:25:05,990
beforehand you just need to migrate and

00:25:02,000 --> 00:25:10,730
slowly and when user really needs you

00:25:05,990 --> 00:25:15,890
migrating a high priority to another

00:25:10,730 --> 00:25:20,450
stories migrate 100k photos so first

00:25:15,890 --> 00:25:24,140
let's see how our imagery work so every

00:25:20,450 --> 00:25:29,510
time when we upload an image we will

00:25:24,140 --> 00:25:36,200
generate a unique image ID then use this

00:25:29,510 --> 00:25:39,530
ID a CDN to access our photo so but this

00:25:36,200 --> 00:25:42,860
this process takes like five second

00:25:39,530 --> 00:25:47,030
because all the images from the external

00:25:42,860 --> 00:25:49,610
sites you need to you know open a HTTP

00:25:47,030 --> 00:25:53,900
connection and upload to your server the

00:25:49,610 --> 00:25:57,200
generator unique hash so this will slow

00:25:53,900 --> 00:26:02,810
down your migration in very great scale

00:25:57,200 --> 00:26:06,500
so instead I desire way that it always

00:26:02,810 --> 00:26:10,340
produce the same hash but yet it's still

00:26:06,500 --> 00:26:13,760
unique so in the migration I only need

00:26:10,340 --> 00:26:16,280
to set this previously designed hash

00:26:13,760 --> 00:26:19,370
during a migration so I don't need to

00:26:16,280 --> 00:26:22,670
upload a image during a migration I can

00:26:19,370 --> 00:26:25,700
upload our images beforehand then I make

00:26:22,670 --> 00:26:29,900
benchmark how long to finish only image

00:26:25,700 --> 00:26:32,030
upload so it's like 10 days and I just

00:26:29,900 --> 00:26:35,470
microwave all the data ten days before

00:26:32,030 --> 00:26:38,960
in low priority because in the reality

00:26:35,470 --> 00:26:43,580
99% of the photos will not change at all

00:26:38,960 --> 00:26:47,139
so you can just do the things beforehand

00:26:43,580 --> 00:26:49,960
so it won't slow down your migration and

00:26:47,139 --> 00:26:53,739
another story is to migrate a user

00:26:49,960 --> 00:26:57,980
password to secure authentication

00:26:53,739 --> 00:27:02,929
because layer authentication may be like

00:26:57,980 --> 00:27:04,999
use md5 or something simple so first you

00:27:02,929 --> 00:27:07,759
need to figure out what what is the

00:27:04,999 --> 00:27:10,999
algorithm they use to encrypt the

00:27:07,759 --> 00:27:14,960
password and when the migrated user

00:27:10,999 --> 00:27:17,029
signing and they enter the password and

00:27:14,960 --> 00:27:20,470
you will fail in your system because

00:27:17,029 --> 00:27:24,499
their password is encrypted in md5 or

00:27:20,470 --> 00:27:28,070
something not send us your current

00:27:24,499 --> 00:27:29,779
systems password encryption so when the

00:27:28,070 --> 00:27:33,409
when you fail you fall back to the

00:27:29,779 --> 00:27:34,399
legacy authentication and in vitro it

00:27:33,409 --> 00:27:38,480
looks like this

00:27:34,399 --> 00:27:40,940
and when you fail you just use the

00:27:38,480 --> 00:27:43,489
legacy authentication to check if they

00:27:40,940 --> 00:27:46,549
enter the correct password or not and

00:27:43,489 --> 00:27:48,759
when they enter the correct password you

00:27:46,549 --> 00:27:53,299
just set this password through your

00:27:48,759 --> 00:27:57,169
existing secure password screen so you

00:27:53,299 --> 00:28:01,269
can make make their password secure

00:27:57,169 --> 00:28:05,600
again by doing this simple change and

00:28:01,269 --> 00:28:08,539
for the future of recent migration I

00:28:05,600 --> 00:28:11,570
think if I ever need to talk again I

00:28:08,539 --> 00:28:15,230
will look into my guerrilla data in a

00:28:11,570 --> 00:28:18,320
ghost table so I just write into another

00:28:15,230 --> 00:28:22,899
table then when it is ready I swap the

00:28:18,320 --> 00:28:25,879
table or I were looking to make my

00:28:22,899 --> 00:28:28,039
migration called more generic so I only

00:28:25,879 --> 00:28:29,600
need to model all the data base then I

00:28:28,039 --> 00:28:32,749
can just run the migration and

00:28:29,600 --> 00:28:36,230
everything will work I can have more

00:28:32,749 --> 00:28:39,409
holidays so some takeaways rails

00:28:36,230 --> 00:28:43,309
provides shop tools thanks to the dress

00:28:39,409 --> 00:28:45,739
coding and you use small objects to make

00:28:43,309 --> 00:28:49,399
your code more reliable and maintainable

00:28:45,739 --> 00:28:52,369
and my friends say abstraction is the

00:28:49,399 --> 00:28:54,619
god of programming and you always

00:28:52,369 --> 00:28:56,360
remember schedule is more important than

00:28:54,619 --> 00:28:59,120
fast and

00:28:56,360 --> 00:29:03,110
accuracy is more important than schedule

00:28:59,120 --> 00:29:06,110
and then our migration sands up but if

00:29:03,110 --> 00:29:09,740
you keep it simple it can make it easy

00:29:06,110 --> 00:29:13,010
to the simplest thing with my previous

00:29:09,740 --> 00:29:15,919
spot to instance eight and thank you and

00:29:13,010 --> 00:29:23,120
enjoy your tea breaks after my top thank

00:29:15,919 --> 00:29:24,679
you all stop

00:29:23,120 --> 00:29:31,940
you have any questions about the

00:29:24,679 --> 00:29:33,950
immigration for joining yes hi have you

00:29:31,940 --> 00:29:36,200
ever run into a situation where a very

00:29:33,950 --> 00:29:39,740
important data that is not valid under

00:29:36,200 --> 00:29:42,289
the new system yes so we run a migration

00:29:39,740 --> 00:29:44,690
call you will find a lot of records

00:29:42,289 --> 00:29:48,169
okay now migrate to your system because

00:29:44,690 --> 00:29:51,289
you have holy validations or their fear

00:29:48,169 --> 00:29:55,580
is too long this kind of things so you

00:29:51,289 --> 00:30:00,980
can either increase either soft your

00:29:55,580 --> 00:30:03,860
validation or just update them we saw

00:30:00,980 --> 00:30:06,470
running these validations like update

00:30:03,860 --> 00:30:09,289
color and then you're just keeping all

00:30:06,470 --> 00:30:12,519
the data in the database yes until with

00:30:09,289 --> 00:30:16,510
it later somehow and do you have any

00:30:12,519 --> 00:30:19,309
strategy to deal with it afterwards so

00:30:16,510 --> 00:30:21,529
basically before the migration I will

00:30:19,309 --> 00:30:25,580
run through all the data make sure

00:30:21,529 --> 00:30:34,330
everything works so I didn't have much

00:30:25,580 --> 00:30:39,409
to do after the migration okay thank hi

00:30:34,330 --> 00:30:42,289
okay we got to so I think I noticed a

00:30:39,409 --> 00:30:44,450
part there where you were migrating data

00:30:42,289 --> 00:30:46,669
and then conditionally migrating some

00:30:44,450 --> 00:30:48,830
more data based upon the user like the

00:30:46,669 --> 00:30:53,059
bookmarks if the users like wanted that

00:30:48,830 --> 00:30:55,309
with that all beforehand or was that

00:30:53,059 --> 00:30:58,880
based upon input like live from the user

00:30:55,309 --> 00:31:01,010
so a user finally logged in and then you

00:30:58,880 --> 00:31:03,980
in only then you lazily migrate a

00:31:01,010 --> 00:31:05,990
different part okay sorry just about and

00:31:03,980 --> 00:31:07,309
if so do you see yourself moving more

00:31:05,990 --> 00:31:10,639
towards that style of my

00:31:07,309 --> 00:31:14,539
raishin of like kind of lazily migrating

00:31:10,639 --> 00:31:18,740
things along so in our system we have

00:31:14,539 --> 00:31:20,919
some important data like recipes so we

00:31:18,740 --> 00:31:24,679
only migrate important things first and

00:31:20,919 --> 00:31:28,070
things not so important like bookmarks

00:31:24,679 --> 00:31:31,399
and there are many of them so we make

00:31:28,070 --> 00:31:38,139
really kind of large data in lazy

00:31:31,399 --> 00:31:44,360
fashion yeah that's how I try to did it

00:31:38,139 --> 00:31:47,899
thank you we won't question you hey um

00:31:44,360 --> 00:31:50,629
so how do you how do you manage your

00:31:47,899 --> 00:31:53,149
foreign keys if you go if you've got

00:31:50,629 --> 00:31:55,070
users and recipes and you're uploading

00:31:53,149 --> 00:31:57,740
them in parallels you were kind of

00:31:55,070 --> 00:31:59,690
showing what if you're trying to put a

00:31:57,740 --> 00:32:00,649
recipe and that doesn't yet have a user

00:31:59,690 --> 00:32:04,610
that's been created

00:32:00,649 --> 00:32:07,460
ah so when I migrate a user know

00:32:04,610 --> 00:32:10,549
migrated recipe I will actually migrate

00:32:07,460 --> 00:32:17,090
user associated first then migrate your

00:32:10,549 --> 00:32:19,460
recipes it starts going through parallel

00:32:17,090 --> 00:32:23,690
ah thread hmm

00:32:19,460 --> 00:32:25,549
how does the recipe migrator know about

00:32:23,690 --> 00:32:31,070
the user if it's come from different

00:32:25,549 --> 00:32:34,519
data or doesn't so every so even it's

00:32:31,070 --> 00:32:36,679
the job using different workers every

00:32:34,519 --> 00:32:39,590
worker will need to check first if the

00:32:36,679 --> 00:32:42,649
recipe has a user migrated to our system

00:32:39,590 --> 00:32:48,169
or not so we always migrator user first

00:32:42,649 --> 00:32:56,070
so it's kind of more slow but integrity

00:32:48,169 --> 00:32:58,479
is secure thank you thank you yeah

00:32:56,070 --> 00:33:00,279
so do I understand correctly that when

00:32:58,479 --> 00:33:03,820
you migrate from the outside to the new

00:33:00,279 --> 00:33:05,619
side the news there is no downtime there

00:33:03,820 --> 00:33:09,489
is like they enable your site

00:33:05,619 --> 00:33:11,559
immediately and your data is still not

00:33:09,489 --> 00:33:13,179
there you your you will be loading in

00:33:11,559 --> 00:33:14,710
right or there is some scheduled

00:33:13,179 --> 00:33:16,659
maintenance that the owner of the old

00:33:14,710 --> 00:33:17,979
site is K in the three days we come back

00:33:16,659 --> 00:33:19,779
or how does it look like from users

00:33:17,979 --> 00:33:22,090
perspective thank you

00:33:19,779 --> 00:33:24,929
I didn't cover lease so when the

00:33:22,090 --> 00:33:28,499
migration happens we make sure the

00:33:24,929 --> 00:33:31,450
provider website change to read-only and

00:33:28,499 --> 00:33:34,599
so everyone cannot signing they can just

00:33:31,450 --> 00:33:38,019
read and we start a migration uncooked

00:33:34,599 --> 00:33:40,779
pad like few hours then when we ready we

00:33:38,019 --> 00:33:45,729
shut down the outside and redirect all

00:33:40,779 --> 00:33:48,700
the things to our new site it depends on

00:33:45,729 --> 00:33:49,269
the size the short one is like four

00:33:48,700 --> 00:33:53,349
hours

00:33:49,269 --> 00:34:03,460
it can also hours depends on how much

00:33:53,349 --> 00:34:05,609
data you have more questions are the

00:34:03,460 --> 00:34:05,609
back

00:34:08,919 --> 00:34:15,049
so just one also um I don't know

00:34:12,529 --> 00:34:19,460
database to that so as to migrate the

00:34:15,049 --> 00:34:21,409
data excuse me could you say again I

00:34:19,460 --> 00:34:23,719
don't know like database to do we need

00:34:21,409 --> 00:34:24,950
to actually create I think we of course

00:34:23,719 --> 00:34:28,669
could be the general and which you can

00:34:24,950 --> 00:34:32,419
do anyone but are they know like

00:34:28,669 --> 00:34:37,309
database way to dump vitally important

00:34:32,419 --> 00:34:41,079
everybody's so your question is how this

00:34:37,309 --> 00:34:46,719
will be talk to other database no like a

00:34:41,079 --> 00:34:50,839
attorney for example ms SQL Oracle right

00:34:46,719 --> 00:34:53,599
they also provide data tons and data

00:34:50,839 --> 00:34:56,629
transformation ready to maybe for

00:34:53,599 --> 00:34:59,150
example to report another database so we

00:34:56,629 --> 00:35:02,119
will consider using those tools on

00:34:59,150 --> 00:35:05,450
something ah you mean use the database

00:35:02,119 --> 00:35:09,349
tool to do the migration oh yeah yeah I

00:35:05,450 --> 00:35:13,910
have been considering doing that but I'm

00:35:09,349 --> 00:35:16,989
not very good at the database tools so I

00:35:13,910 --> 00:35:24,589
used to be and he still works

00:35:16,989 --> 00:35:26,299
so okay thank you I think this Aaron's

00:35:24,589 --> 00:35:27,890
that sometimes you need to do some

00:35:26,299 --> 00:35:31,160
transformations or validations that you

00:35:27,890 --> 00:35:32,779
do in your Ruby code and or maybe the

00:35:31,160 --> 00:35:34,430
schema your transform yo you're

00:35:32,779 --> 00:35:36,890
migrating from is not the same thing so

00:35:34,430 --> 00:35:38,719
you need to do some transformation bits

00:35:36,890 --> 00:35:40,400
that the database tooling design quite

00:35:38,719 --> 00:35:43,190
support it depends

00:35:40,400 --> 00:35:45,969
yeah so yeah I think we can take one

00:35:43,190 --> 00:35:49,390
more question before I open the break

00:35:45,969 --> 00:35:49,390
one of them okay

00:35:52,650 --> 00:35:57,940
just one asked about that the lazy

00:35:55,120 --> 00:35:59,290
migration so when I use a lot in then

00:35:57,940 --> 00:36:01,450
only you kill data in a high priority

00:35:59,290 --> 00:36:02,860
queues in migrated data so what happens

00:36:01,450 --> 00:36:05,110
if the user never locks in so you don't

00:36:02,860 --> 00:36:08,800
care about the data anymore ah so I

00:36:05,110 --> 00:36:10,810
actually has a low priority queue

00:36:08,800 --> 00:36:13,600
running for everything that would take

00:36:10,810 --> 00:36:16,210
like few weeks but it's late saying

00:36:13,600 --> 00:36:18,580
during these few weeks layer things will

00:36:16,210 --> 00:36:20,110
be migrating in high priority move that

00:36:18,580 --> 00:36:22,840
low priority queue and move them up to

00:36:20,110 --> 00:36:25,240
the high priority queue not its

00:36:22,840 --> 00:36:28,480
duplicated so it's duplicated ok

00:36:25,240 --> 00:36:30,790
yeah but your migration Cori's there

00:36:28,480 --> 00:36:32,920
most of them yeah you can run multiple

00:36:30,790 --> 00:36:37,480
times and it's still the same so no

00:36:32,920 --> 00:36:39,150
problem ok cool thank you very much

00:36:37,480 --> 00:36:42,820
thank you

00:36:39,150 --> 00:36:42,820

YouTube URL: https://www.youtube.com/watch?v=pEXZpuHKyJc


