Title: Big Ruby 2014 - MO' JOBS MO' PROBLEMS - LESSONS LEARNED SCALING TO MILLIONS OF JOBS AN HOUR
Publication date: 2020-01-24
Playlist: Big Ruby 2014
Description: 
	By Tanner Burson

At Tapjoy we process over a million jobs an hour with Ruby. This talk is a discussion of tools, techniques, and interesting problems from the trenches of scaling up over the last two years. Expect to learn a lot about Ruby job queues (beyond Resque/Sidekiq), performance, concurrency and more.

Help us caption & translate this video!

http://amara.org/v/FG3n/
Captions: 
	00:00:19,750 --> 00:00:24,529
alright so thanks for the introduction

00:00:22,250 --> 00:00:26,590
mark as he said I'm here to talk about

00:00:24,529 --> 00:00:28,630
job processing I think this is a really

00:00:26,590 --> 00:00:31,400
exciting and interesting topic that

00:00:28,630 --> 00:00:32,810
doesn't really get the press it deserves

00:00:31,400 --> 00:00:33,860
in the Ruby community everybody knows

00:00:32,810 --> 00:00:36,920
about rescue everybody knows about

00:00:33,860 --> 00:00:38,990
sidekick but much past that it just kind

00:00:36,920 --> 00:00:41,329
of drops off into oblivion so this is

00:00:38,990 --> 00:00:43,610
stuff I've been working on pretty much

00:00:41,329 --> 00:00:44,989
for the last year so I'm really excited

00:00:43,610 --> 00:00:47,149
to get a chance to come talk to

00:00:44,989 --> 00:00:49,250
everybody about this I'm excited to be

00:00:47,149 --> 00:00:52,370
here with an audience of actual people

00:00:49,250 --> 00:00:54,350
today in preparing this talk I've given

00:00:52,370 --> 00:00:56,180
it at home I don't know how many times

00:00:54,350 --> 00:00:58,100
yet but i think my dog is sick of

00:00:56,180 --> 00:01:00,260
hearing about job processing so i'm

00:00:58,100 --> 00:01:03,289
really glad to have people here to

00:01:00,260 --> 00:01:06,230
listen so obviously I'm tanner I work

00:01:03,289 --> 00:01:07,550
for Tapjoy out of our Boston office for

00:01:06,230 --> 00:01:11,630
those of you not familiar with tapjoy

00:01:07,550 --> 00:01:12,890
we're a in app mobile ad platform so

00:01:11,630 --> 00:01:15,260
what does that mean we have an sdk for

00:01:12,890 --> 00:01:19,100
mobile app developers that allows them

00:01:15,260 --> 00:01:21,439
to put ads offers other monetization

00:01:19,100 --> 00:01:23,479
options into their application so that

00:01:21,439 --> 00:01:27,380
users have ways to pay them that aren't

00:01:23,479 --> 00:01:30,140
just give me cash along with that we've

00:01:27,380 --> 00:01:34,460
got about 450 million monthly active

00:01:30,140 --> 00:01:35,600
users this is a big number because we're

00:01:34,460 --> 00:01:37,880
an ad network that means we're dealing

00:01:35,600 --> 00:01:40,340
with lots and lots of requests millions

00:01:37,880 --> 00:01:44,210
and millions of requests a minute things

00:01:40,340 --> 00:01:45,590
get hard so what does that really boil

00:01:44,210 --> 00:01:46,969
down so we're talking about scale right

00:01:45,590 --> 00:01:48,170
this is big ruby conference we're here

00:01:46,969 --> 00:01:51,590
to talk about scale let's talk about

00:01:48,170 --> 00:01:53,869
scale this is this brings up interesting

00:01:51,590 --> 00:01:55,369
problems of scale right so anything we

00:01:53,869 --> 00:01:57,979
put into production goes out to this

00:01:55,369 --> 00:02:01,399
load immediately that means the naive

00:01:57,979 --> 00:02:03,890
solution fails not in weeks or months

00:02:01,399 --> 00:02:05,090
the naive solution fails in minutes we

00:02:03,890 --> 00:02:06,950
have to avoid single points of failure

00:02:05,090 --> 00:02:08,180
we have to have distributed systems from

00:02:06,950 --> 00:02:11,360
the start we have to think about hard

00:02:08,180 --> 00:02:14,300
things upfront immediately every time I

00:02:11,360 --> 00:02:15,770
think this is really fun I think these

00:02:14,300 --> 00:02:17,510
are really great problems to get to go

00:02:15,770 --> 00:02:19,010
solve every day and I'm excited to get

00:02:17,510 --> 00:02:22,610
to share some of these things that we've

00:02:19,010 --> 00:02:25,190
been doing so let's set the scene a

00:02:22,610 --> 00:02:26,569
little bit so this this all started way

00:02:25,190 --> 00:02:30,230
way back in

00:02:26,569 --> 00:02:31,999
2012 so what were we all doing in 2012

00:02:30,230 --> 00:02:34,489
when we were upgrading our apps to Ruby

00:02:31,999 --> 00:02:37,749
19 we were watching gangam style mashups

00:02:34,489 --> 00:02:39,920
and waiting for the Mayan apocalypse

00:02:37,749 --> 00:02:42,430
outside of those things things were

00:02:39,920 --> 00:02:44,870
going you know really pretty well I mean

00:02:42,430 --> 00:02:50,150
everything was good we were pretty happy

00:02:44,870 --> 00:02:52,549
with with the way things were mostly you

00:02:50,150 --> 00:02:54,889
know good news we're an ad network our

00:02:52,549 --> 00:02:56,499
traffic is going up way up we're seeing

00:02:54,889 --> 00:02:59,540
month after month quarter-over-quarter

00:02:56,499 --> 00:03:01,760
huge increases in traffic fifty plus

00:02:59,540 --> 00:03:04,430
percent increases in traffic inside of

00:03:01,760 --> 00:03:06,109
six months pans we're moving really fast

00:03:04,430 --> 00:03:07,819
we're scaling up really big this is

00:03:06,109 --> 00:03:10,639
great for us because more requests means

00:03:07,819 --> 00:03:14,120
more money and we like that but not

00:03:10,639 --> 00:03:16,159
everything is perfect so our job system

00:03:14,120 --> 00:03:18,199
is starting to to kind of show some

00:03:16,159 --> 00:03:19,909
issues predominantly during our

00:03:18,199 --> 00:03:22,069
high-traffic periods jobs are backing up

00:03:19,909 --> 00:03:24,109
and not just a little bit jobs are

00:03:22,069 --> 00:03:26,689
backing way up we're seeing ten plus

00:03:24,109 --> 00:03:28,549
thousand messages and queues that aren't

00:03:26,689 --> 00:03:30,379
clearing out they'll stay that way until

00:03:28,549 --> 00:03:32,449
our peak you know our peak traffic

00:03:30,379 --> 00:03:34,790
starts to trend back down and finally we

00:03:32,449 --> 00:03:37,699
can get all caught back up this is this

00:03:34,790 --> 00:03:39,590
is not awesome and then secondly we have

00:03:37,699 --> 00:03:41,000
kind of an event processing system

00:03:39,590 --> 00:03:44,449
similar to what Coraline was talking

00:03:41,000 --> 00:03:46,459
about just a few minutes ago and much

00:03:44,449 --> 00:03:47,900
like she was saying these are important

00:03:46,459 --> 00:03:49,729
things to us this data is really useful

00:03:47,900 --> 00:03:52,790
but we've discovered that we're losing

00:03:49,729 --> 00:03:55,129
small amounts of this data we don't know

00:03:52,790 --> 00:03:57,019
how much and we don't know exactly where

00:03:55,129 --> 00:03:58,189
in the pipeline we're losing it but

00:03:57,019 --> 00:04:03,349
we're dropping some of it and that's

00:03:58,189 --> 00:04:05,750
real bad and for the worst news all of

00:04:03,349 --> 00:04:07,579
this is our fault because the the job

00:04:05,750 --> 00:04:10,689
system and the event processing system

00:04:07,579 --> 00:04:13,090
are entirely homegrown emphasis on grown

00:04:10,689 --> 00:04:16,940
these are the kinds of systems you

00:04:13,090 --> 00:04:19,250
discover one day are there and you can

00:04:16,940 --> 00:04:22,190
tell no one sat down and thought hard

00:04:19,250 --> 00:04:24,590
and planned and prepared and set up a

00:04:22,190 --> 00:04:27,409
design review this system just kind of

00:04:24,590 --> 00:04:30,759
accreted code over years and now it's

00:04:27,409 --> 00:04:33,380
there in front of you and it's a problem

00:04:30,759 --> 00:04:35,659
particularly to the job system we can't

00:04:33,380 --> 00:04:37,580
scale up individual jobs we can't

00:04:35,659 --> 00:04:39,860
identify that one job that's going slow

00:04:37,580 --> 00:04:40,249
and say hey let's make sure we can do

00:04:39,860 --> 00:04:42,079
more

00:04:40,249 --> 00:04:44,089
those right now let's task more servers

00:04:42,079 --> 00:04:45,949
to do that lets task more processors to

00:04:44,089 --> 00:04:50,029
do that we can't really do that in the

00:04:45,949 --> 00:04:51,649
way it's designed and event processing

00:04:50,029 --> 00:04:54,049
so let's let's talk a little bit about

00:04:51,649 --> 00:04:56,479
what event processing means to us so for

00:04:54,049 --> 00:04:58,159
us this these are data points in our

00:04:56,479 --> 00:05:00,319
application these are things like a user

00:04:58,159 --> 00:05:02,569
has seen an add a user has clicked and

00:05:00,319 --> 00:05:04,279
add if we're lucky a user has converted

00:05:02,569 --> 00:05:05,899
on an ad and that that means we're you

00:05:04,279 --> 00:05:08,059
know getting money this this is good

00:05:05,899 --> 00:05:09,229
stuff this is the core of you know our

00:05:08,059 --> 00:05:12,049
business analytics our business

00:05:09,229 --> 00:05:15,259
intelligence system and we generate a

00:05:12,049 --> 00:05:16,789
lot of those you know this is on the

00:05:15,259 --> 00:05:19,399
order of hundreds and hundreds of

00:05:16,789 --> 00:05:21,860
thousands per minute you know five six

00:05:19,399 --> 00:05:23,360
hundred thousand per minute lots and

00:05:21,860 --> 00:05:25,219
lots of these and we're losing a very

00:05:23,360 --> 00:05:26,869
small percentage of that but we don't

00:05:25,219 --> 00:05:28,369
know how many that is we don't know

00:05:26,869 --> 00:05:30,559
which ones were losing and that's and

00:05:28,369 --> 00:05:32,119
that's bad part of the reason we don't

00:05:30,559 --> 00:05:34,579
know is that the the way the system is

00:05:32,119 --> 00:05:36,469
designed to currently it's it's

00:05:34,579 --> 00:05:37,969
basically syslog we write messages out

00:05:36,469 --> 00:05:40,219
to syslog we have some custom log

00:05:37,969 --> 00:05:41,509
handlers that bundle and aggregate and

00:05:40,219 --> 00:05:43,759
do things to the data and then ship it

00:05:41,509 --> 00:05:46,879
off to our actual you know data

00:05:43,759 --> 00:05:49,249
warehousing system but sis looks kind of

00:05:46,879 --> 00:05:50,469
a mess and it's really not easy to tell

00:05:49,249 --> 00:05:53,179
what's going on in there other than

00:05:50,469 --> 00:05:56,779
sometimes you get exit codes that aren't

00:05:53,179 --> 00:05:58,879
zero and that doesn't tell you much so

00:05:56,779 --> 00:06:00,559
this is this is not awesome it's it's

00:05:58,879 --> 00:06:02,179
time for us to do something about these

00:06:00,559 --> 00:06:03,529
problems we're seeing all this growth

00:06:02,179 --> 00:06:05,749
we're seeing all this coming up ahead of

00:06:03,529 --> 00:06:11,360
us we can't stay this way we've got to

00:06:05,749 --> 00:06:12,709
fix things but change is hard and where

00:06:11,360 --> 00:06:16,039
do you even start with a problem this

00:06:12,709 --> 00:06:17,659
big we can't just dive in and go fix all

00:06:16,039 --> 00:06:20,899
of this all at once these are these are

00:06:17,659 --> 00:06:22,489
big systems with lots of data so we kind

00:06:20,899 --> 00:06:24,019
of need to figure out what's important

00:06:22,489 --> 00:06:27,349
to us first and what we really need to

00:06:24,019 --> 00:06:29,119
be approaching and what we come down to

00:06:27,349 --> 00:06:31,099
is we're talking about message queues

00:06:29,119 --> 00:06:32,360
we're talking about job processing and

00:06:31,099 --> 00:06:34,849
when you start talking about that you're

00:06:32,360 --> 00:06:37,249
really talking two components a queue

00:06:34,849 --> 00:06:38,769
something that takes data in on one side

00:06:37,249 --> 00:06:42,169
and let's data out on the other side

00:06:38,769 --> 00:06:44,149
it's pretty simple and secondly you need

00:06:42,169 --> 00:06:45,739
something to take data out of that queue

00:06:44,149 --> 00:06:48,800
and do something meaningful to your

00:06:45,739 --> 00:06:51,060
application without that it's just data

00:06:48,800 --> 00:06:52,290
flowing through a queue that doesn't

00:06:51,060 --> 00:06:55,020
mean anything we need something to make

00:06:52,290 --> 00:06:56,580
it mean something to us so let's let's

00:06:55,020 --> 00:06:58,800
kind of break this down a little bit so

00:06:56,580 --> 00:07:04,169
when we talk about cues there's some

00:06:58,800 --> 00:07:05,910
choices a lot of choices you may be

00:07:04,169 --> 00:07:07,020
familiar with some of these terms I

00:07:05,910 --> 00:07:08,960
wasn't familiar with all of these I

00:07:07,020 --> 00:07:12,060
actually googled these the other day and

00:07:08,960 --> 00:07:14,010
there's just a crazy amount of them i

00:07:12,060 --> 00:07:16,260
mean i think most people in this room

00:07:14,010 --> 00:07:19,260
have probably heard of RabbitMQ or have

00:07:16,260 --> 00:07:22,380
seen Redis used as a cue or maybe

00:07:19,260 --> 00:07:24,030
Bienstock d if you're really up on some

00:07:22,380 --> 00:07:25,919
of the cool things in Ruby right now but

00:07:24,030 --> 00:07:28,080
there's just a ton of choices how do we

00:07:25,919 --> 00:07:32,370
even figure out which of these things is

00:07:28,080 --> 00:07:33,840
useful and meaningful to us so we decide

00:07:32,370 --> 00:07:36,450
the same way we decide with every

00:07:33,840 --> 00:07:37,680
problem in computer science right we get

00:07:36,450 --> 00:07:42,150
three choices and we only get to pick

00:07:37,680 --> 00:07:44,580
two of them great we do this every time

00:07:42,150 --> 00:07:47,220
but here we go again so in the case of

00:07:44,580 --> 00:07:50,510
cues really there's three things that

00:07:47,220 --> 00:07:53,639
mattered most than anything else we need

00:07:50,510 --> 00:07:55,590
durability we need availability and we

00:07:53,639 --> 00:07:57,030
need throughput so let's break these

00:07:55,590 --> 00:07:59,310
terms down just a little bit so

00:07:57,030 --> 00:08:02,310
durability what would we really mean

00:07:59,310 --> 00:08:03,990
they're all we mean is I want to put a

00:08:02,310 --> 00:08:05,280
message in a queue and I want to be

00:08:03,990 --> 00:08:07,710
reasonably assured I'm going to get that

00:08:05,280 --> 00:08:10,470
message back out so we're talking about

00:08:07,710 --> 00:08:13,680
avoiding you know maybe a disk failure

00:08:10,470 --> 00:08:16,169
or a process crash or the simple kinds

00:08:13,680 --> 00:08:18,630
of things that just happen every day in

00:08:16,169 --> 00:08:21,210
in real systems but we need to have an

00:08:18,630 --> 00:08:23,039
idea of how many of these we can see in

00:08:21,210 --> 00:08:25,560
what the likelihood that these messages

00:08:23,039 --> 00:08:27,630
survive is next we're talking about

00:08:25,560 --> 00:08:30,150
availability this is this is really

00:08:27,630 --> 00:08:31,700
critical we're talking about is this

00:08:30,150 --> 00:08:33,690
thing a single point of failure

00:08:31,700 --> 00:08:35,010
hopefully not because we don't want

00:08:33,690 --> 00:08:37,800
single points of failure we have big

00:08:35,010 --> 00:08:39,419
systems but what kind of tuning can we

00:08:37,800 --> 00:08:40,950
do to that how available can we make it

00:08:39,419 --> 00:08:44,130
and what are some of the trade-offs in

00:08:40,950 --> 00:08:46,950
doing that and then lastly and somewhat

00:08:44,130 --> 00:08:48,300
obviously how many jobs can I mash

00:08:46,950 --> 00:08:50,370
through the thing because we got a lot

00:08:48,300 --> 00:08:52,080
of jobs or get a lot of data to move how

00:08:50,370 --> 00:08:53,640
fast can we push this data in and how

00:08:52,080 --> 00:08:56,190
fast can we get that data back out again

00:08:53,640 --> 00:08:57,360
because if it doesn't matter on all of

00:08:56,190 --> 00:08:58,830
these things if it's going to take way

00:08:57,360 --> 00:09:02,350
too long though that that's not useful

00:08:58,830 --> 00:09:05,830
we need to do this stuff fast oh

00:09:02,350 --> 00:09:07,690
oops I guess that isn't the end right we

00:09:05,830 --> 00:09:10,360
live in the real world and things cost

00:09:07,690 --> 00:09:12,040
money and at some point we have to make

00:09:10,360 --> 00:09:14,050
trade-offs based just on how much will

00:09:12,040 --> 00:09:16,360
it cost to do these things you can

00:09:14,050 --> 00:09:18,640
imagine you know ideal scenarios where

00:09:16,360 --> 00:09:20,500
you can get blazing fast things that are

00:09:18,640 --> 00:09:22,240
always available and will never lose

00:09:20,500 --> 00:09:24,330
your messages and it will cost you 10

00:09:22,240 --> 00:09:27,340
million dollars a server to have it

00:09:24,330 --> 00:09:28,960
that's probably not feasible for most

00:09:27,340 --> 00:09:30,700
anyone in this room that that's not

00:09:28,960 --> 00:09:35,010
realistic so we do have to consider

00:09:30,700 --> 00:09:37,660
costs but it's not the top of our list

00:09:35,010 --> 00:09:40,060
so for us we narrow that list down to

00:09:37,660 --> 00:09:42,130
three choices these are based on both

00:09:40,060 --> 00:09:43,480
the criteria above and then truthfully

00:09:42,130 --> 00:09:45,520
like anyone else things we were familiar

00:09:43,480 --> 00:09:48,760
with internally things we had experience

00:09:45,520 --> 00:09:52,510
with and knew what to do with that list

00:09:48,760 --> 00:09:54,190
was right there RabbitMQ SQ s and r

00:09:52,510 --> 00:09:56,980
ennis if you're not familiar with

00:09:54,190 --> 00:09:58,480
rabbitmq it's a blazing-fast really

00:09:56,980 --> 00:10:00,490
select message queue that is flexible

00:09:58,480 --> 00:10:04,090
and can do almost anything you want to

00:10:00,490 --> 00:10:08,020
it it's really really neat stuff SQS as

00:10:04,090 --> 00:10:10,630
Amazon's hosted Q system it's a little

00:10:08,020 --> 00:10:13,630
bit less flexible but it's really crazy

00:10:10,630 --> 00:10:15,160
available and then lastly read us

00:10:13,630 --> 00:10:16,990
because we're in Ruby and everyone has

00:10:15,160 --> 00:10:20,770
used Retta source I rescuer side cute

00:10:16,990 --> 00:10:22,420
sidekick and we know how this works we

00:10:20,770 --> 00:10:25,960
have read us around for other things

00:10:22,420 --> 00:10:28,960
anyway maybe we can just use that so

00:10:25,960 --> 00:10:32,080
breaking down rabbit is really fast it

00:10:28,960 --> 00:10:34,120
can be really really incredibly fast it

00:10:32,080 --> 00:10:36,160
can also be really really durable you

00:10:34,120 --> 00:10:38,350
can almost ensure that no message will

00:10:36,160 --> 00:10:40,330
ever get dropped both putting them in or

00:10:38,350 --> 00:10:42,700
taking the back out that's awesome

00:10:40,330 --> 00:10:45,220
that's a really great feature and it's

00:10:42,700 --> 00:10:46,600
pretty fault tolerant the problem with

00:10:45,220 --> 00:10:48,490
all of this in the case of rabbit is

00:10:46,600 --> 00:10:50,200
that's entirely and how you configure it

00:10:48,490 --> 00:10:53,170
you can configure it to be completely

00:10:50,200 --> 00:10:56,920
nondurable incredibly fast and not full

00:10:53,170 --> 00:10:58,420
tolerant at all it's a choice so it's a

00:10:56,920 --> 00:10:59,980
little bit complex and have to really

00:10:58,420 --> 00:11:01,990
understand how rabbit fits together and

00:10:59,980 --> 00:11:06,780
how it works to make those choices fit

00:11:01,990 --> 00:11:09,880
what you want then there's SQS it's

00:11:06,780 --> 00:11:12,430
incredibly durable I don't know if it's

00:11:09,880 --> 00:11:14,259
as durable as s3 which Amazon claims

00:11:12,430 --> 00:11:16,600
like somewhere around the heat death of

00:11:14,259 --> 00:11:19,209
universe they'll have lost one bit of

00:11:16,600 --> 00:11:22,089
data out of it as three I don't think

00:11:19,209 --> 00:11:25,569
we're quite that durable but it's pretty

00:11:22,089 --> 00:11:27,669
darn close after that it's incredibly

00:11:25,569 --> 00:11:31,179
available i know amazon gets knocked a

00:11:27,669 --> 00:11:33,519
lot for EBS and some of the failures and

00:11:31,179 --> 00:11:35,439
things they have there but SQS actually

00:11:33,519 --> 00:11:39,519
tends to be the one thing that is always

00:11:35,439 --> 00:11:42,220
up inside of amazon and then lastly it's

00:11:39,519 --> 00:11:43,600
not as fast all of these things boil

00:11:42,220 --> 00:11:46,149
down to it's just not going to be quite

00:11:43,600 --> 00:11:47,350
as fast as rabbit that might be okay it

00:11:46,149 --> 00:11:50,410
might not we'll have to really look at

00:11:47,350 --> 00:11:51,639
the numbers to see and then Redis this

00:11:50,410 --> 00:11:53,439
is this is the one most people are

00:11:51,639 --> 00:11:58,720
probably familiar with right ret Redis

00:11:53,439 --> 00:12:01,209
is a key value ish data store ish it's

00:11:58,720 --> 00:12:04,059
it's the Swiss Army chainsaw of data

00:12:01,209 --> 00:12:06,220
stores right you it can do anything and

00:12:04,059 --> 00:12:07,899
it can do some of those things well it

00:12:06,220 --> 00:12:11,049
turns out that queuing probably isn't

00:12:07,899 --> 00:12:13,989
the thing it does best but it's still

00:12:11,049 --> 00:12:15,579
really useful it's really fast I mean I

00:12:13,989 --> 00:12:17,139
think everybody here is played with it

00:12:15,579 --> 00:12:19,239
enough to know that it is just an

00:12:17,139 --> 00:12:21,039
incredibly fast system it's also

00:12:19,239 --> 00:12:23,679
reasonably durable you can set you know

00:12:21,039 --> 00:12:25,089
how how long stuff will reside in memory

00:12:23,679 --> 00:12:27,189
before it gets flushed to disk and

00:12:25,089 --> 00:12:29,829
manage all of that but remember we're

00:12:27,189 --> 00:12:33,069
back it way back in 2012 here the idea

00:12:29,829 --> 00:12:36,959
of Redis cluster or read a sentinel or

00:12:33,069 --> 00:12:39,970
any of the Redis like clustering tools

00:12:36,959 --> 00:12:40,959
don't exist everyone is running around

00:12:39,970 --> 00:12:42,759
trying to figure out how to make that

00:12:40,959 --> 00:12:44,799
work so we don't really have a good

00:12:42,759 --> 00:12:46,629
story in Redis about how to make it

00:12:44,799 --> 00:12:48,549
distribute it how to make it not a

00:12:46,629 --> 00:12:53,169
single point of failure so that's that's

00:12:48,549 --> 00:12:54,910
a real problem for us so what what we

00:12:53,169 --> 00:12:57,489
ended up deciding is that really we did

00:12:54,910 --> 00:12:59,379
have two very distinct use cases for are

00:12:57,489 --> 00:13:02,470
queuing we have our event processing

00:12:59,379 --> 00:13:04,419
case which as we've talked about we

00:13:02,470 --> 00:13:05,679
actually really care about this data

00:13:04,419 --> 00:13:08,819
this data is really important to us so

00:13:05,679 --> 00:13:10,929
it needs to be very durable also it's

00:13:08,819 --> 00:13:12,519
incredibly fast we have a lot of this

00:13:10,929 --> 00:13:13,629
data coming in so we need to be able to

00:13:12,519 --> 00:13:17,649
match this stuff through the queue

00:13:13,629 --> 00:13:20,019
really really quickly secondly we have

00:13:17,649 --> 00:13:21,759
our just general purpose jobs so they

00:13:20,019 --> 00:13:23,499
talk about general purpose jobs what I'm

00:13:21,759 --> 00:13:25,779
talking about our the things you always

00:13:23,499 --> 00:13:27,010
background in your web app oh we need to

00:13:25,779 --> 00:13:30,040
send an email or we need to

00:13:27,010 --> 00:13:31,750
make an HTTP call to a partner or maybe

00:13:30,040 --> 00:13:33,310
we have this other thing that isn't

00:13:31,750 --> 00:13:35,170
terribly important but it's really

00:13:33,310 --> 00:13:37,150
memory intensive and we just don't want

00:13:35,170 --> 00:13:38,530
to do that interactively in the app or a

00:13:37,150 --> 00:13:40,300
thing that's incredibly failure-prone

00:13:38,530 --> 00:13:42,130
and we're going to want to retry five or

00:13:40,300 --> 00:13:44,230
ten times because it might take that

00:13:42,130 --> 00:13:45,820
many times to get right these are the

00:13:44,230 --> 00:13:48,490
kinds of things we send off into jobs

00:13:45,820 --> 00:13:52,390
and we have a lot of those too but not

00:13:48,490 --> 00:13:56,710
quite as much as we do with with the the

00:13:52,390 --> 00:13:57,970
event processing stuff okay so this is a

00:13:56,710 --> 00:13:59,800
ruby conference I've been up here

00:13:57,970 --> 00:14:01,330
rambling about cues let's let's talk

00:13:59,800 --> 00:14:04,060
about Ruby I think that's what everyone

00:14:01,330 --> 00:14:09,490
would rather hear so let's talk about

00:14:04,060 --> 00:14:11,740
job processors job processors we got a

00:14:09,490 --> 00:14:13,540
lot of those too there's not quite as

00:14:11,740 --> 00:14:16,930
many years as there are in the in the

00:14:13,540 --> 00:14:19,540
queueing case but there's a lot of job

00:14:16,930 --> 00:14:21,250
processors in Ruby so again we have to

00:14:19,540 --> 00:14:23,560
figure out what what makes sense and how

00:14:21,250 --> 00:14:25,360
we can how we can fit these things in

00:14:23,560 --> 00:14:27,610
interestingly in our case we've already

00:14:25,360 --> 00:14:29,260
picked a queue so we can really narrow

00:14:27,610 --> 00:14:31,810
this set of choices down to things that

00:14:29,260 --> 00:14:35,530
can work with the cues that we want to

00:14:31,810 --> 00:14:39,250
work with and the answer is they don't

00:14:35,530 --> 00:14:42,190
so in 2012 we looked at basically every

00:14:39,250 --> 00:14:43,780
job processor you can use in Ruby and we

00:14:42,190 --> 00:14:46,750
discovered that pretty much none of them

00:14:43,780 --> 00:14:52,120
did what we wanted so let's go build one

00:14:46,750 --> 00:14:53,950
or two so what is a job processor what

00:14:52,120 --> 00:14:56,110
really is it at its core it's really

00:14:53,950 --> 00:14:58,840
just three things it's something that

00:14:56,110 --> 00:15:01,330
takes messages out of a queue it's

00:14:58,840 --> 00:15:03,160
something that does some decision based

00:15:01,330 --> 00:15:05,050
on that message and finds a chunk of

00:15:03,160 --> 00:15:07,720
code we want to run and then something

00:15:05,050 --> 00:15:11,170
that runs that code but that's it that's

00:15:07,720 --> 00:15:16,300
all we have to do this is easy we're

00:15:11,170 --> 00:15:17,980
engineers we can do this okay but we're

00:15:16,300 --> 00:15:19,900
talking about scale here right like this

00:15:17,980 --> 00:15:22,780
is this is big stuff we can't just have

00:15:19,900 --> 00:15:25,870
one machine read from a queue and find a

00:15:22,780 --> 00:15:28,690
job and then run that job and do that

00:15:25,870 --> 00:15:30,370
one at a time on a machine that's going

00:15:28,690 --> 00:15:32,260
to take a lot of machines to get through

00:15:30,370 --> 00:15:34,750
hundreds of thousands of these a minute

00:15:32,260 --> 00:15:37,720
that that's that's not feasible so we

00:15:34,750 --> 00:15:39,220
need to run these jobs concurrently okay

00:15:37,720 --> 00:15:40,470
well we're in Ruby so this gets a little

00:15:39,220 --> 00:15:41,880
bit harder but

00:15:40,470 --> 00:15:45,840
we know how concurrency and Ruby works

00:15:41,880 --> 00:15:48,270
so let's let's nail this all right so

00:15:45,840 --> 00:15:51,750
Ruby concurrency we pretty much have

00:15:48,270 --> 00:15:53,490
three choices you have for King which

00:15:51,750 --> 00:15:55,950
has been around since the dawn of UNIX

00:15:53,490 --> 00:15:58,110
you spin up a child process and do

00:15:55,950 --> 00:15:59,670
things in it can't really talk back to

00:15:58,110 --> 00:16:02,790
the parent easily but that's probably

00:15:59,670 --> 00:16:05,340
okay fibers fibers are the cool new

00:16:02,790 --> 00:16:07,650
thing in Ruby 19 we should totally want

00:16:05,340 --> 00:16:11,670
to use these because new stuff is always

00:16:07,650 --> 00:16:14,030
better fibers are really cool maybe we

00:16:11,670 --> 00:16:16,080
can make use of this and then threads

00:16:14,030 --> 00:16:18,660
we've been told forever that we can't

00:16:16,080 --> 00:16:22,590
use threads in Ruby but maybe that's not

00:16:18,660 --> 00:16:23,670
true we need to look at this and see so

00:16:22,590 --> 00:16:25,890
let's talk about our rabbit job

00:16:23,670 --> 00:16:27,210
processor we really looked around at a

00:16:25,890 --> 00:16:29,790
lot of things even played with a few

00:16:27,210 --> 00:16:33,660
that supported rabbit and really decided

00:16:29,790 --> 00:16:35,760
that that just didn't really fit so what

00:16:33,660 --> 00:16:40,650
could go wrong we we can we can handle

00:16:35,760 --> 00:16:42,720
this so a little bit about how you

00:16:40,650 --> 00:16:44,250
interact with with rabbit when you're

00:16:42,720 --> 00:16:46,590
interacting with rabbit it's highly

00:16:44,250 --> 00:16:48,570
asynchronous you make a request for

00:16:46,590 --> 00:16:50,700
something and it sometimes comes back

00:16:48,570 --> 00:16:53,820
later and tells you about it when you

00:16:50,700 --> 00:16:56,610
tell it hey I'm done with this job it'll

00:16:53,820 --> 00:16:58,770
tell you that it accepted that sometime

00:16:56,610 --> 00:17:00,870
later so this is a really asynchronous

00:16:58,770 --> 00:17:02,610
thing because of that all of the

00:17:00,870 --> 00:17:04,710
connections to rabbit are also stateful

00:17:02,610 --> 00:17:07,080
so that means the same connection that

00:17:04,710 --> 00:17:08,550
you open to receive a job is the same

00:17:07,080 --> 00:17:10,830
one you have to say you're done with the

00:17:08,550 --> 00:17:11,970
job on so if we need multiple

00:17:10,830 --> 00:17:13,290
connections to be able to do this

00:17:11,970 --> 00:17:14,790
quickly we have to be able to track all

00:17:13,290 --> 00:17:18,090
of those connections in which job match

00:17:14,790 --> 00:17:19,500
with matches with with each one okay

00:17:18,090 --> 00:17:21,360
that's not terrible but it's an

00:17:19,500 --> 00:17:23,730
important detail to notice we're going

00:17:21,360 --> 00:17:26,010
through and then lastly it doesn't have

00:17:23,730 --> 00:17:28,200
any inherent built-in support for batch

00:17:26,010 --> 00:17:30,450
processing wouldn't that be nice if we

00:17:28,200 --> 00:17:32,130
could maybe speed this up by getting 10

00:17:30,450 --> 00:17:34,530
things at a time instead of doing one at

00:17:32,130 --> 00:17:40,020
a time but rabbit doesn't really support

00:17:34,530 --> 00:17:42,720
that inherently so okay but we've got

00:17:40,020 --> 00:17:45,090
these cool things called fibers and we

00:17:42,720 --> 00:17:48,090
really think fibers can work here so if

00:17:45,090 --> 00:17:50,400
you've never played with fibers hey

00:17:48,090 --> 00:17:53,990
you're probably lucky and secondly

00:17:50,400 --> 00:17:56,880
fibers are kind of like thread

00:17:53,990 --> 00:17:59,670
except they exist in the same stack as

00:17:56,880 --> 00:18:01,260
your main application so what Ruby

00:17:59,670 --> 00:18:04,800
actually does internally is it creates a

00:18:01,260 --> 00:18:06,600
single stack frame for the fiber and for

00:18:04,800 --> 00:18:07,950
each fiber gets its own stack frame and

00:18:06,600 --> 00:18:10,500
so it's able to just jump back and forth

00:18:07,950 --> 00:18:13,110
in its own stack to switch between

00:18:10,500 --> 00:18:15,000
fibers this is much faster than context

00:18:13,110 --> 00:18:16,650
switches and threads this is much

00:18:15,000 --> 00:18:18,690
lighter weight than Forks or anything

00:18:16,650 --> 00:18:21,210
else also because you're in one stack

00:18:18,690 --> 00:18:26,160
you don't have to worry about the same

00:18:21,210 --> 00:18:28,110
kind of thread mutex cross talk nonsense

00:18:26,160 --> 00:18:29,340
that you're always fighting because you

00:18:28,110 --> 00:18:30,960
can't ever end up in another place in

00:18:29,340 --> 00:18:32,400
the stack we only have one stack we

00:18:30,960 --> 00:18:34,850
can't ever end up in those terrible

00:18:32,400 --> 00:18:37,980
terrible places so that's pretty cool

00:18:34,850 --> 00:18:40,170
now because we're only in one stack if

00:18:37,980 --> 00:18:43,440
we're bound on cpu this doesn't do us

00:18:40,170 --> 00:18:45,360
any good because we can do one thing and

00:18:43,440 --> 00:18:46,500
wait for it to finish and then we can do

00:18:45,360 --> 00:18:48,660
the other thing well that's not really

00:18:46,500 --> 00:18:51,510
concurrency that's just the same thing

00:18:48,660 --> 00:18:53,850
we had before but if we're waiting on Io

00:18:51,510 --> 00:18:55,230
we can actually leverage fibers in a

00:18:53,850 --> 00:18:57,480
really neat way and say that we're going

00:18:55,230 --> 00:18:59,130
to send off this data to the server and

00:18:57,480 --> 00:19:00,300
instead of waiting for it to come back

00:18:59,130 --> 00:19:02,040
we're going to go do something on that

00:19:00,300 --> 00:19:04,080
other fiber and we'll come back and

00:19:02,040 --> 00:19:05,550
check on this guy later and we can keep

00:19:04,080 --> 00:19:07,680
doing that and keep doing that and run

00:19:05,550 --> 00:19:09,420
hundreds or even thousands of fibers

00:19:07,680 --> 00:19:12,000
concurrently because you can get an

00:19:09,420 --> 00:19:13,290
amazing amount of work done in the 20

00:19:12,000 --> 00:19:15,390
milliseconds it takes to make that

00:19:13,290 --> 00:19:17,670
network request this is really really

00:19:15,390 --> 00:19:19,470
cool so really fibers are great for

00:19:17,670 --> 00:19:25,280
these asynchronous workloads this is

00:19:19,470 --> 00:19:28,920
really handy sounds perfect except

00:19:25,280 --> 00:19:30,600
always a problem so fibers have this

00:19:28,920 --> 00:19:32,400
little thing so if you think back to

00:19:30,600 --> 00:19:34,500
just a second ago we're talking fibers

00:19:32,400 --> 00:19:36,750
are allocated on the main stack that

00:19:34,500 --> 00:19:40,530
makes them really fast it avoids you

00:19:36,750 --> 00:19:43,860
know a lot of the other pain but ruby

00:19:40,530 --> 00:19:46,290
has a fixed size for an individual frame

00:19:43,860 --> 00:19:47,910
on the stack and depending on your ruby

00:19:46,290 --> 00:19:50,610
version and which bugs that has in the

00:19:47,910 --> 00:19:54,870
19 branch the size of that stack frame

00:19:50,610 --> 00:19:57,030
varies but it's also really small it's

00:19:54,870 --> 00:20:00,030
small enough that you probably can't

00:19:57,030 --> 00:20:02,790
save an act of record object inside of a

00:20:00,030 --> 00:20:05,420
fiber the callback chain inside of an

00:20:02,790 --> 00:20:08,000
active record call can easily be deep

00:20:05,420 --> 00:20:13,430
than the amount of stack you have inside

00:20:08,000 --> 00:20:15,140
of one frame in Ruby this hurts you're

00:20:13,430 --> 00:20:18,550
going along in your application and then

00:20:15,140 --> 00:20:21,500
suddenly BAM system stack error cool

00:20:18,550 --> 00:20:24,800
that was helpful data Ruby thanks I

00:20:21,500 --> 00:20:26,510
appreciate that so on down from that you

00:20:24,800 --> 00:20:28,970
also you know we talked about fibrin

00:20:26,510 --> 00:20:31,310
great when you need the IO stuff but it

00:20:28,970 --> 00:20:33,200
kind of sucks to handle that yourself so

00:20:31,310 --> 00:20:35,000
you kind of want to find some sort of a

00:20:33,200 --> 00:20:39,350
framework or toolkit that manages those

00:20:35,000 --> 00:20:41,750
IO switch offs for you at the time we

00:20:39,350 --> 00:20:43,610
reevaluate and with a vent machine which

00:20:41,750 --> 00:20:44,690
creates kind of an event loop and lets

00:20:43,610 --> 00:20:48,890
you do some neat things with callbacks

00:20:44,690 --> 00:20:51,260
and it's really nice but it also means

00:20:48,890 --> 00:20:52,880
all of your i/o has to know that it's

00:20:51,260 --> 00:20:54,380
being dealt with inside of a fiber

00:20:52,880 --> 00:20:55,970
because it has to know when to make

00:20:54,380 --> 00:20:59,780
those trade-offs and when to say hey I

00:20:55,970 --> 00:21:01,850
can get out of here so your code isn't

00:20:59,780 --> 00:21:03,290
really agnostic to this your code has to

00:21:01,850 --> 00:21:05,600
know that it's running inside of these

00:21:03,290 --> 00:21:06,890
things so it's not a really easy thing

00:21:05,600 --> 00:21:09,890
to just kind of slip into your

00:21:06,890 --> 00:21:11,600
application and then along with that

00:21:09,890 --> 00:21:13,610
there was a really nice set of tools

00:21:11,600 --> 00:21:17,150
around event machine called synchrony

00:21:13,610 --> 00:21:19,160
and it lets you avoid callback chaining

00:21:17,150 --> 00:21:22,190
and a lot of the mess you end up with in

00:21:19,160 --> 00:21:24,350
a vented code and it is the most complex

00:21:22,190 --> 00:21:26,150
bit of Ruby code i've ever seen and we

00:21:24,350 --> 00:21:29,450
stumbled into a couple of bugs in it and

00:21:26,150 --> 00:21:34,250
it was it was somewhat harrowing to go

00:21:29,450 --> 00:21:36,950
to go resolve and then lastly system

00:21:34,250 --> 00:21:40,400
stack error again man these things are

00:21:36,950 --> 00:21:42,830
awful and not only are they really hard

00:21:40,400 --> 00:21:44,900
to expect or to predict or to know

00:21:42,830 --> 00:21:46,790
what's going to happen but Ruby really

00:21:44,900 --> 00:21:48,800
doesn't want to help you with this so

00:21:46,790 --> 00:21:51,200
when you get a system stack error you

00:21:48,800 --> 00:21:53,300
get one line of back-trace so we're all

00:21:51,200 --> 00:21:54,470
used to being able to see the full back

00:21:53,300 --> 00:21:57,590
trace of everything that just happened

00:21:54,470 --> 00:21:59,300
when we get an exception one line and

00:21:57,590 --> 00:22:02,600
that is the line that you blew the stack

00:21:59,300 --> 00:22:04,310
on it could be 20 method calls deep in a

00:22:02,600 --> 00:22:07,550
library you didn't know you were using

00:22:04,310 --> 00:22:09,950
on a line of code you had no idea was

00:22:07,550 --> 00:22:13,040
going to happen that's all the data Ruby

00:22:09,950 --> 00:22:15,020
gives you there's a bug filed in Ruby to

00:22:13,040 --> 00:22:17,150
resolve this it's been there for about

00:22:15,020 --> 00:22:18,530
four years so I wouldn't hold your

00:22:17,150 --> 00:22:20,000
breath on that one getting solved any

00:22:18,530 --> 00:22:22,800
time soon

00:22:20,000 --> 00:22:25,050
so let's let's kind of move on what what

00:22:22,800 --> 00:22:26,970
did we really learn for this well if

00:22:25,050 --> 00:22:29,730
we're going to be running somewhat

00:22:26,970 --> 00:22:30,960
arbitrary code fibers are not going to

00:22:29,730 --> 00:22:32,520
be good for that because we can't

00:22:30,960 --> 00:22:33,960
control that environment well enough to

00:22:32,520 --> 00:22:35,340
make sure we're staying inside of the

00:22:33,960 --> 00:22:37,350
stack and that we have the right network

00:22:35,340 --> 00:22:40,050
libraries enabled and that everything's

00:22:37,350 --> 00:22:41,700
ready to play all night we also learned

00:22:40,050 --> 00:22:44,250
as we went through this that rabbitmq is

00:22:41,700 --> 00:22:45,630
actually pretty complex to get the

00:22:44,250 --> 00:22:48,180
trade-offs of durability and

00:22:45,630 --> 00:22:49,830
availability and clustering it took a

00:22:48,180 --> 00:22:51,990
lot of work it took a lot more work than

00:22:49,830 --> 00:22:55,740
we expected it would take to get all of

00:22:51,990 --> 00:22:57,090
those things fit together and really we

00:22:55,740 --> 00:23:01,290
should have looked harder at celluloid

00:22:57,090 --> 00:23:05,280
so celluloid is another common fiber a

00:23:01,290 --> 00:23:06,630
vented choice and we really should have

00:23:05,280 --> 00:23:07,950
looked harder at that because at the end

00:23:06,630 --> 00:23:09,090
of the day I think it simplifies a lot

00:23:07,950 --> 00:23:12,960
of the things that get hard an event

00:23:09,090 --> 00:23:14,660
machine all right so we're done with

00:23:12,960 --> 00:23:17,280
rabbit we we got all this up and

00:23:14,660 --> 00:23:19,350
honestly it actually all works and it's

00:23:17,280 --> 00:23:21,210
all running and you know it was a little

00:23:19,350 --> 00:23:22,920
rocky but we got there and we're cool so

00:23:21,210 --> 00:23:24,930
now now we need to go tackle our general

00:23:22,920 --> 00:23:26,460
purpose jobs so let's talk a little bit

00:23:24,930 --> 00:23:32,010
about what it takes to process jobs with

00:23:26,460 --> 00:23:33,840
SQS as usual there's nothing in fact I

00:23:32,010 --> 00:23:37,190
doubt anyone in this room has hardly

00:23:33,840 --> 00:23:40,500
used SQS as as a job processor there is

00:23:37,190 --> 00:23:43,320
nothing in Ruby that does this it is it

00:23:40,500 --> 00:23:46,650
is just shocking so let's go build it

00:23:43,320 --> 00:23:48,840
and I mean we we learned our lessons the

00:23:46,650 --> 00:23:54,930
last time we can do this this will be

00:23:48,840 --> 00:23:57,390
easy so SQS is a really slick system

00:23:54,930 --> 00:24:00,420
like everything else Amazon does it's an

00:23:57,390 --> 00:24:02,310
HTTP based API you make calls to it you

00:24:00,420 --> 00:24:05,130
push some data you make a recalled call

00:24:02,310 --> 00:24:07,350
to it you request some data like all

00:24:05,130 --> 00:24:09,660
HTTP things that means it's synchronous

00:24:07,350 --> 00:24:11,640
which also adds a little bit of latency

00:24:09,660 --> 00:24:14,310
so because we have to wait on those

00:24:11,640 --> 00:24:17,700
requests to come back a call to SQS can

00:24:14,310 --> 00:24:20,610
be maybe 20 milliseconds to do a right

00:24:17,700 --> 00:24:22,680
that's a little bit longer than we would

00:24:20,610 --> 00:24:24,960
probably prefer but we can probably get

00:24:22,680 --> 00:24:26,280
away with this for now but it does have

00:24:24,960 --> 00:24:28,140
some really nice things and that it will

00:24:26,280 --> 00:24:31,440
allow us to do batch processing built-in

00:24:28,140 --> 00:24:33,419
we can send it back varying sizes

00:24:31,440 --> 00:24:35,700
and it will treat all of those as a

00:24:33,419 --> 00:24:36,929
batch of messages and it will let us

00:24:35,700 --> 00:24:38,669
fetch them back down as a batch of

00:24:36,929 --> 00:24:40,769
messages but not necessarily the same

00:24:38,669 --> 00:24:42,450
batch we sent in but that's okay we can

00:24:40,769 --> 00:24:44,429
just kind of parallel these requests and

00:24:42,450 --> 00:24:45,960
get a lot more done in that 20

00:24:44,429 --> 00:24:50,490
millisecond window so it probably

00:24:45,960 --> 00:24:53,100
balances out okay so let's talk about

00:24:50,490 --> 00:24:55,440
concurrency again so we realize for

00:24:53,100 --> 00:24:57,720
general purpose jobs fibers are probably

00:24:55,440 --> 00:24:59,519
not it so we're going to now we're down

00:24:57,720 --> 00:25:03,779
to our two choices of you know Forks and

00:24:59,519 --> 00:25:06,450
threads so why not threads well I think

00:25:03,779 --> 00:25:08,610
everybody here has read heard seen been

00:25:06,450 --> 00:25:12,870
berated about Ruby's global interpreter

00:25:08,610 --> 00:25:14,580
lock it does kind of suck it keeps you

00:25:12,870 --> 00:25:17,190
from being able to actually run threads

00:25:14,580 --> 00:25:19,830
completely concurrently the Ruby VM

00:25:17,190 --> 00:25:21,750
blocks on certain operations and will

00:25:19,830 --> 00:25:23,940
not let you have two threads actually

00:25:21,750 --> 00:25:25,409
operating at exactly the same time but

00:25:23,940 --> 00:25:28,919
it can interleave those two things in

00:25:25,409 --> 00:25:31,139
ways that may get you close other

00:25:28,919 --> 00:25:33,269
reasons threads are scary so we have a

00:25:31,139 --> 00:25:37,440
legacy application it's several years

00:25:33,269 --> 00:25:39,450
old is it thread safe I don't know I

00:25:37,440 --> 00:25:40,590
don't know if every Jim we're using is

00:25:39,450 --> 00:25:42,440
thread-safe I don't know if we're using

00:25:40,590 --> 00:25:44,610
them all in their thread safe ways I

00:25:42,440 --> 00:25:46,110
also don't want to be the one who has to

00:25:44,610 --> 00:25:50,070
find that out that is a that is a

00:25:46,110 --> 00:25:51,600
difficult problem to go solve so thread

00:25:50,070 --> 00:25:56,009
safety is a little a little concerning

00:25:51,600 --> 00:25:59,490
here and then lastly general purpose job

00:25:56,009 --> 00:26:00,899
queues can have memory blow we're going

00:25:59,490 --> 00:26:02,820
to be fetching a lot of data we're going

00:26:00,899 --> 00:26:04,500
to be doing a lot of possibly dumb

00:26:02,820 --> 00:26:05,669
things in our jobs because we didn't

00:26:04,500 --> 00:26:08,549
want to do these concurrently in a

00:26:05,669 --> 00:26:11,429
request so it's this process could grow

00:26:08,549 --> 00:26:13,649
pretty big and historically Ruby will

00:26:11,429 --> 00:26:14,850
let you grow that process really big and

00:26:13,649 --> 00:26:20,399
then when you're done with those objects

00:26:14,850 --> 00:26:23,070
stay really big that's not awesome so

00:26:20,399 --> 00:26:25,230
threads don't give us the ability to get

00:26:23,070 --> 00:26:27,960
rid of that extra memory growth easily

00:26:25,230 --> 00:26:29,129
I'm hopeful 21 makes all of this better

00:26:27,960 --> 00:26:31,350
with all of our great new garbage

00:26:29,129 --> 00:26:34,409
collection stuff but at this point we're

00:26:31,350 --> 00:26:36,240
just barely on Ruby 19 I mean we're we

00:26:34,409 --> 00:26:38,399
already think we're pretty we're in much

00:26:36,240 --> 00:26:41,369
better shape than we were in 18 but this

00:26:38,399 --> 00:26:44,169
is still concerning okay so we're not

00:26:41,369 --> 00:26:47,590
going to thread our our job processing

00:26:44,169 --> 00:26:50,950
I guess that means forks okay forks are

00:26:47,590 --> 00:26:52,570
kind of boring but we're all used to

00:26:50,950 --> 00:26:54,970
force right like these have been around

00:26:52,570 --> 00:26:58,169
forever some of the systems we all use

00:26:54,970 --> 00:27:01,359
all the time anyway fork all the time

00:26:58,169 --> 00:27:03,909
rescue notoriously is a forking job

00:27:01,359 --> 00:27:06,309
system and unicorn which is a really

00:27:03,909 --> 00:27:08,769
solid Ruby web servers built around you

00:27:06,309 --> 00:27:10,629
know forking behavior and Ruby so this

00:27:08,769 --> 00:27:12,309
has to be a well-worn path we've got to

00:27:10,629 --> 00:27:16,929
be in good shape this should be easy

00:27:12,309 --> 00:27:20,159
let's let's do this so let's let's talk

00:27:16,929 --> 00:27:24,480
about how we can make our SQS job

00:27:20,159 --> 00:27:27,009
processing work for us let's start by

00:27:24,480 --> 00:27:28,629
fetching with threads I know I know I

00:27:27,009 --> 00:27:30,549
just got done telling you we can't use

00:27:28,629 --> 00:27:31,720
threads and threads are terrible we

00:27:30,549 --> 00:27:34,840
can't use threads to actually process

00:27:31,720 --> 00:27:37,600
the jobs that's scary but if we want to

00:27:34,840 --> 00:27:41,139
use threads to just get more data out of

00:27:37,600 --> 00:27:42,489
SQS faster that's okay and in fact we

00:27:41,139 --> 00:27:44,350
can avoid the global interpreter lock

00:27:42,489 --> 00:27:45,820
problem because this is all I oh we're

00:27:44,350 --> 00:27:47,769
spending 20 milliseconds sending a

00:27:45,820 --> 00:27:49,720
request SQS and waiting on data to come

00:27:47,769 --> 00:27:51,730
back well that's happening we could send

00:27:49,720 --> 00:27:54,159
five other requests to SQS and get more

00:27:51,730 --> 00:27:57,009
data so we can fan that out quite a bit

00:27:54,159 --> 00:27:58,570
wider in a single process and not have

00:27:57,009 --> 00:28:00,399
to fight a lot of the crazy thread

00:27:58,570 --> 00:28:03,399
safety issues because this is isolated

00:28:00,399 --> 00:28:05,230
from our actual code the ones would get

00:28:03,399 --> 00:28:06,999
that list of messages down from SQS

00:28:05,230 --> 00:28:10,059
let's go ahead and fork a process with

00:28:06,999 --> 00:28:12,399
those messages so this gives us a really

00:28:10,059 --> 00:28:14,259
nice way to manage that memory bloat I

00:28:12,399 --> 00:28:15,970
was talking about so we'll process that

00:28:14,259 --> 00:28:17,919
batch of messages and when that batch of

00:28:15,970 --> 00:28:19,989
messages has done that child process

00:28:17,919 --> 00:28:22,320
will just die so any memory bloat that

00:28:19,989 --> 00:28:25,960
that child process gained during its

00:28:22,320 --> 00:28:28,210
hopefully short lifetime is gone and

00:28:25,960 --> 00:28:29,859
we're back to just a small same little

00:28:28,210 --> 00:28:31,239
Ruby process so we can actually manage

00:28:29,859 --> 00:28:33,730
this pretty well we can get a lot of

00:28:31,239 --> 00:28:35,379
these going at one time and then because

00:28:33,730 --> 00:28:36,879
we're building distributed systems here

00:28:35,379 --> 00:28:38,710
we're going to do what all the cool

00:28:36,879 --> 00:28:41,409
distributed systems do and we're going

00:28:38,710 --> 00:28:43,059
to have our own internal stats so we're

00:28:41,409 --> 00:28:45,669
going to have a little web server that

00:28:43,059 --> 00:28:47,139
runs on a thread and we can hit it and

00:28:45,669 --> 00:28:48,820
it'll tell us how many messages its

00:28:47,139 --> 00:28:51,429
processed and how many of error doubt

00:28:48,820 --> 00:28:53,080
and what its throughput looks like and

00:28:51,429 --> 00:28:54,610
what the health of the system is and

00:28:53,080 --> 00:28:56,020
this will be really cool and then we can

00:28:54,610 --> 00:28:56,830
have a nice dash board that aggregates

00:28:56,020 --> 00:28:59,020
this from all of these different

00:28:56,830 --> 00:29:03,280
processes on all these servers and this

00:28:59,020 --> 00:29:06,190
this is a great idea okay so maybe it's

00:29:03,280 --> 00:29:07,570
not a perfect idea so we run into some

00:29:06,190 --> 00:29:11,440
problems pretty quickly with this

00:29:07,570 --> 00:29:13,780
approach first pipe processings actually

00:29:11,440 --> 00:29:15,850
really slow you would think for

00:29:13,780 --> 00:29:17,559
something that is an underpinning of you

00:29:15,850 --> 00:29:20,110
know the UNIX way that it would be

00:29:17,559 --> 00:29:23,530
faster the answer is no it's not it's

00:29:20,110 --> 00:29:26,260
slow in fact we could only get about six

00:29:23,530 --> 00:29:28,210
child processes open before the data

00:29:26,260 --> 00:29:30,220
going back and forth between the

00:29:28,210 --> 00:29:32,730
processes was taking up about

00:29:30,220 --> 00:29:35,470
ninety-eight percent of our CPU that

00:29:32,730 --> 00:29:39,309
meant we had no time left to I don't

00:29:35,470 --> 00:29:41,320
know do work or fetch more jobs so our

00:29:39,309 --> 00:29:44,320
throttling was actually based on the

00:29:41,320 --> 00:29:46,809
faster we got the slower we got this is

00:29:44,320 --> 00:29:49,360
a bad trade-off this does not work so we

00:29:46,809 --> 00:29:51,580
have to do something about that and then

00:29:49,360 --> 00:29:54,280
we're running into this odd issue where

00:29:51,580 --> 00:29:58,450
occasionally the child processes just

00:29:54,280 --> 00:30:01,540
hang they just don't respond to begin

00:29:58,450 --> 00:30:03,010
taking up zero CPU they're stopped at

00:30:01,540 --> 00:30:05,650
whatever memory they had at that moment

00:30:03,010 --> 00:30:07,690
they're writing no log output they're

00:30:05,650 --> 00:30:09,250
doing nothing they're just there they

00:30:07,690 --> 00:30:12,250
don't respond to signal's nothing is

00:30:09,250 --> 00:30:14,169
happening they're just hanging we don't

00:30:12,250 --> 00:30:15,400
know what's going on Google around and

00:30:14,169 --> 00:30:17,590
you can see that there's some issues

00:30:15,400 --> 00:30:19,809
that both rescue and unicorn have had

00:30:17,590 --> 00:30:21,760
when integrating with New Relic that

00:30:19,809 --> 00:30:23,919
maybe are related because we're doing

00:30:21,760 --> 00:30:27,190
something similar so this is this is

00:30:23,919 --> 00:30:29,290
maybe it all right so a couple more

00:30:27,190 --> 00:30:30,700
weeks and we're still doing this and we

00:30:29,290 --> 00:30:33,309
decide to just throw away this whole

00:30:30,700 --> 00:30:36,280
pipe processing stats nonsense we're not

00:30:33,309 --> 00:30:37,390
that cool and it's not that useful so

00:30:36,280 --> 00:30:39,070
we'll just add some more do relic

00:30:37,390 --> 00:30:40,530
integration tie indoor internal

00:30:39,070 --> 00:30:44,410
monitoring system a little better and

00:30:40,530 --> 00:30:47,230
this will be fine we still can't

00:30:44,410 --> 00:30:51,400
reproduce this hanging issue it's there

00:30:47,230 --> 00:30:52,690
and it happens a lot but we can't

00:30:51,400 --> 00:30:54,669
reproduce it under controlled conditions

00:30:52,690 --> 00:30:58,270
we can't actually make this happen when

00:30:54,669 --> 00:31:02,400
we want to and it turns out debugging

00:30:58,270 --> 00:31:05,230
running Ruby processes is really hard

00:31:02,400 --> 00:31:07,040
pry debugger and a lot of those tools

00:31:05,230 --> 00:31:11,120
are really nice

00:31:07,040 --> 00:31:12,590
when Ruby's responding if you're dead

00:31:11,120 --> 00:31:14,660
locked or you have something else going

00:31:12,590 --> 00:31:16,940
on in that process you have you have no

00:31:14,660 --> 00:31:19,550
way to get into it so one of our

00:31:16,940 --> 00:31:23,420
engineers whose way better at sea than I

00:31:19,550 --> 00:31:25,400
ever want to be started just running the

00:31:23,420 --> 00:31:27,320
process and jumping in with gdb and

00:31:25,400 --> 00:31:29,600
poking around what was happening inside

00:31:27,320 --> 00:31:32,300
of the process and inside of the Ruby VM

00:31:29,600 --> 00:31:33,320
at the moment it was hung and that's

00:31:32,300 --> 00:31:38,210
when we found some really interesting

00:31:33,320 --> 00:31:41,360
stuff so it turns out Ruby had a little

00:31:38,210 --> 00:31:45,350
race condition in the early branches of

00:31:41,360 --> 00:31:48,610
193 and what would happen is if you had

00:31:45,350 --> 00:31:52,430
any mutex lock in your parent process

00:31:48,610 --> 00:31:54,320
that was open when you forked there's

00:31:52,430 --> 00:31:56,150
clearly a problem there right so we have

00:31:54,320 --> 00:31:57,710
a lock we're now running some other set

00:31:56,150 --> 00:32:01,130
of code we have no way to go back and

00:31:57,710 --> 00:32:02,870
release that lock so Ruby helpfully knew

00:32:01,130 --> 00:32:04,730
that this would be a problem and it

00:32:02,870 --> 00:32:06,230
would release all the locks as soon as

00:32:04,730 --> 00:32:07,820
you forked a child so in the child

00:32:06,230 --> 00:32:09,890
process one of the first things it does

00:32:07,820 --> 00:32:12,230
when it boot straps that process is to

00:32:09,890 --> 00:32:13,640
remove all the mutex locks they don't

00:32:12,230 --> 00:32:16,340
mean anything anymore so we'll just get

00:32:13,640 --> 00:32:18,320
rid of them but it had a little race

00:32:16,340 --> 00:32:21,980
condition that turned out if you forked

00:32:18,320 --> 00:32:27,530
a couple of processes at nearly the same

00:32:21,980 --> 00:32:29,150
time it didn't do that so what you ended

00:32:27,530 --> 00:32:31,730
up with were processes that were

00:32:29,150 --> 00:32:33,230
literally dead from start they could not

00:32:31,730 --> 00:32:38,780
interact with their code they couldn't

00:32:33,230 --> 00:32:40,880
do anything so that's a real problem

00:32:38,780 --> 00:32:44,000
then we were using the rails Express

00:32:40,880 --> 00:32:45,800
patchset which back in the day was the

00:32:44,000 --> 00:32:48,890
only way to get good garbage collection

00:32:45,800 --> 00:32:50,540
in your in your Ruby app was I think the

00:32:48,890 --> 00:32:53,480
set of patches that went into passenger

00:32:50,540 --> 00:32:56,150
at one point there's a long history of

00:32:53,480 --> 00:32:58,930
these things floating around and this

00:32:56,150 --> 00:33:01,370
was this was fine but it had another bug

00:32:58,930 --> 00:33:04,460
fairly similar that would cause deadlox

00:33:01,370 --> 00:33:05,660
and then lastly and this one is really

00:33:04,460 --> 00:33:08,030
my favorite because it's just

00:33:05,660 --> 00:33:12,560
staggeringly weird that it never came up

00:33:08,030 --> 00:33:14,420
before Ruby's DNS resolve library is

00:33:12,560 --> 00:33:17,090
written in C because you want it to be

00:33:14,420 --> 00:33:20,630
really fast and it uses the system calls

00:33:17,090 --> 00:33:22,130
for looking up DNS addresses and giving

00:33:20,630 --> 00:33:26,060
IP addresses and doing all of that fun

00:33:22,130 --> 00:33:29,750
stuff but it dropped in to see in a way

00:33:26,060 --> 00:33:33,320
that didn't release the GI l which meant

00:33:29,750 --> 00:33:35,570
every time you request a dns address for

00:33:33,320 --> 00:33:39,350
the entire length of that DNS request

00:33:35,570 --> 00:33:40,760
your process is locked nobody else can

00:33:39,350 --> 00:33:43,850
do anything and they're waiting on that

00:33:40,760 --> 00:33:48,110
to come back so if you have a slow DNS

00:33:43,850 --> 00:33:49,990
request or a minor network blip or any

00:33:48,110 --> 00:33:53,450
number of things that will occur

00:33:49,990 --> 00:33:57,320
hundreds of times your process just

00:33:53,450 --> 00:34:00,650
hangs out there for a while that's real

00:33:57,320 --> 00:34:02,150
bad and it turns out the solution which

00:34:00,650 --> 00:34:05,300
is never the solution in ruby was to

00:34:02,150 --> 00:34:07,730
rewrite it in ruby the solution to make

00:34:05,300 --> 00:34:09,860
a CA slow c library faster was to

00:34:07,730 --> 00:34:12,590
rewrite it in ruby and the reason for

00:34:09,860 --> 00:34:14,840
that is the ruby code doesn't trip the

00:34:12,590 --> 00:34:16,790
GI l since it never goes down to a

00:34:14,840 --> 00:34:18,620
system call and it just uses Ruby's

00:34:16,790 --> 00:34:20,990
native i/o libraries to do the DNS

00:34:18,620 --> 00:34:23,750
requests it never actually walks the

00:34:20,990 --> 00:34:26,030
interpreter so it ends up actually being

00:34:23,750 --> 00:34:28,010
way faster if you're doing anything

00:34:26,030 --> 00:34:30,649
concurrently if you have more than one

00:34:28,010 --> 00:34:32,899
thread in your application this library

00:34:30,649 --> 00:34:35,750
should be installed because it

00:34:32,899 --> 00:34:38,870
absolutely makes a difference but this

00:34:35,750 --> 00:34:42,020
was just really surprising so if you're

00:34:38,870 --> 00:34:44,060
following along that means inside of a

00:34:42,020 --> 00:34:47,750
basically a month of debugging we have

00:34:44,060 --> 00:34:49,190
now come across three Ruby bugs and one

00:34:47,750 --> 00:34:50,899
I didn't break down because I don't

00:34:49,190 --> 00:34:54,169
understand it but we ran into a small

00:34:50,899 --> 00:34:55,639
bug in G Lib C over a particular range

00:34:54,169 --> 00:34:58,550
of patches which were the default

00:34:55,639 --> 00:35:01,580
installed on some version of ubuntu were

00:34:58,550 --> 00:35:03,350
fork we're creating a fork was

00:35:01,580 --> 00:35:08,900
significantly slower than it should be

00:35:03,350 --> 00:35:11,720
by like a hundred x who knew turns out

00:35:08,900 --> 00:35:14,090
the fix is upgrade g lipsy because we

00:35:11,720 --> 00:35:16,520
usually look for that if you're

00:35:14,090 --> 00:35:18,230
interested in these things we the

00:35:16,520 --> 00:35:21,100
engineer who did most of this wrote up a

00:35:18,230 --> 00:35:23,990
really nice post in the rescue project

00:35:21,100 --> 00:35:25,340
on one of their issues which was related

00:35:23,990 --> 00:35:27,350
to all of these same sorts of problems

00:35:25,340 --> 00:35:29,570
so i encourage you to go look at that

00:35:27,350 --> 00:35:33,530
and see a little more detail about those

00:35:29,570 --> 00:35:34,190
things so what did we learn when we got

00:35:33,530 --> 00:35:36,170
through

00:35:34,190 --> 00:35:39,079
SQS because we built the thing we got a

00:35:36,170 --> 00:35:40,760
debug we got it running we chose the

00:35:39,079 --> 00:35:44,300
safest set of things that we could and

00:35:40,760 --> 00:35:47,930
it's still really hard scale is hard and

00:35:44,300 --> 00:35:49,220
we have somewhat of a motto in our

00:35:47,930 --> 00:35:51,260
office and it's something you're likely

00:35:49,220 --> 00:35:54,859
to hear us say regularly and that's the

00:35:51,260 --> 00:35:57,140
naive solution fails fast instantly and

00:35:54,859 --> 00:35:58,400
this was another another example of that

00:35:57,140 --> 00:36:00,589
we thought we were doing something

00:35:58,400 --> 00:36:02,150
straightforward and easy but it turns

00:36:00,589 --> 00:36:04,250
out that the subtle differences in the

00:36:02,150 --> 00:36:06,859
way we were doing it versus rescue vs.

00:36:04,250 --> 00:36:08,839
unicorn we're leading us to really

00:36:06,859 --> 00:36:10,400
educate problems that won't really edge

00:36:08,839 --> 00:36:13,849
case for us anymore these are really

00:36:10,400 --> 00:36:16,040
significant things also debugging Ruby

00:36:13,849 --> 00:36:18,589
systems code is hard we don't have good

00:36:16,040 --> 00:36:21,079
tooling for it I recommend reading up on

00:36:18,589 --> 00:36:22,790
gdb and actually learning how to poke

00:36:21,079 --> 00:36:24,230
around a running Ruby process that way

00:36:22,790 --> 00:36:26,510
there's an amazing amount of things you

00:36:24,230 --> 00:36:28,640
can learn from that and Ruby actually

00:36:26,510 --> 00:36:30,410
internally provides some tools when it's

00:36:28,640 --> 00:36:32,839
running under gdb that will let you do

00:36:30,410 --> 00:36:35,270
things like run Ruby code from GD b and

00:36:32,839 --> 00:36:37,190
c results we didn't know any of that

00:36:35,270 --> 00:36:40,069
before we started this and that was some

00:36:37,190 --> 00:36:41,960
really helpful stuff and then lastly

00:36:40,069 --> 00:36:44,750
sometimes the bug really isn't your bug

00:36:41,960 --> 00:36:48,770
I don't know how many hours we spent

00:36:44,750 --> 00:36:51,230
poring over all of our code but we tore

00:36:48,770 --> 00:36:53,420
it inside inside out we had logging on

00:36:51,230 --> 00:36:55,430
every statement we were timing all sorts

00:36:53,420 --> 00:36:57,109
of requests we knew every detail of

00:36:55,430 --> 00:36:59,000
everything and it still wasn't working

00:36:57,109 --> 00:37:03,410
right because it turns out ruby was

00:36:59,000 --> 00:37:05,030
broken it wasn't our bug but we wouldn't

00:37:03,410 --> 00:37:07,010
have been able to get to that point if

00:37:05,030 --> 00:37:09,619
we hadn't done the other work but at

00:37:07,010 --> 00:37:12,829
some level things can be broken and when

00:37:09,619 --> 00:37:14,569
you start getting to significant scale

00:37:12,829 --> 00:37:16,849
and processing things that really

00:37:14,569 --> 00:37:19,160
intense speeds you will find issues that

00:37:16,849 --> 00:37:23,930
are not yours more often than you would

00:37:19,160 --> 00:37:25,910
think so what this boil down to is that

00:37:23,930 --> 00:37:28,160
we actually wrapped up all of what I was

00:37:25,910 --> 00:37:31,670
just talking about into a library it's a

00:37:28,160 --> 00:37:34,099
library we call chore it uses the same

00:37:31,670 --> 00:37:36,589
serialization format as rescue which

00:37:34,099 --> 00:37:38,839
sidekick also uses so it's a simple

00:37:36,589 --> 00:37:41,630
little JSON hash that's what we pass

00:37:38,839 --> 00:37:43,130
across the wire into the queues it lets

00:37:41,630 --> 00:37:44,930
us do some of the things we had problems

00:37:43,130 --> 00:37:47,089
with like being able to configure per

00:37:44,930 --> 00:37:48,140
job what should happen being able to

00:37:47,089 --> 00:37:51,290
scale things up

00:37:48,140 --> 00:37:53,420
her job these these were real problems

00:37:51,290 --> 00:37:56,030
we can control / process and therefore

00:37:53,420 --> 00:37:57,620
per server what cues to listen to what

00:37:56,030 --> 00:38:00,440
priorities effectively to listen to

00:37:57,620 --> 00:38:03,860
those on and how many resources to

00:38:00,440 --> 00:38:05,600
devote to that also because we're

00:38:03,860 --> 00:38:08,240
engineers and we don't like solving the

00:38:05,600 --> 00:38:11,150
same problem three times it's agnostic

00:38:08,240 --> 00:38:12,740
to Q and to concurrency option which

00:38:11,150 --> 00:38:14,660
means you can plug in different cues to

00:38:12,740 --> 00:38:16,370
it it is designed in a way that you

00:38:14,660 --> 00:38:17,630
should be able to plug in multiple cues

00:38:16,370 --> 00:38:20,390
and we already have two implemented in

00:38:17,630 --> 00:38:21,620
there right now and then lastly if you

00:38:20,390 --> 00:38:23,060
actually are lucky and have an

00:38:21,620 --> 00:38:25,430
application that's thread safe and

00:38:23,060 --> 00:38:27,200
doesn't do crazy memory intensive things

00:38:25,430 --> 00:38:28,430
you can run it threaded which will

00:38:27,200 --> 00:38:30,680
actually be faster than going through

00:38:28,430 --> 00:38:33,710
the the forking method of processing

00:38:30,680 --> 00:38:36,410
your jobs the the downside there is it's

00:38:33,710 --> 00:38:38,150
not released yet it's coming very soon

00:38:36,410 --> 00:38:40,640
we're going through our internal process

00:38:38,150 --> 00:38:45,410
of getting all of that open source so

00:38:40,640 --> 00:38:47,270
that should be available soon so let's

00:38:45,410 --> 00:38:49,070
let's look back and let's let's look at

00:38:47,270 --> 00:38:51,380
the issues we had when we started and

00:38:49,070 --> 00:38:55,010
let's let's talk through what those what

00:38:51,380 --> 00:38:58,340
those are so our job queues are backing

00:38:55,010 --> 00:39:00,550
up well we pretty well solved that we

00:38:58,340 --> 00:39:04,580
have a job processor that's much faster

00:39:00,550 --> 00:39:07,340
it's much faster than our old one event

00:39:04,580 --> 00:39:09,620
processing is a little lossy well we've

00:39:07,340 --> 00:39:13,430
replaced all of that crazy syslog stuff

00:39:09,620 --> 00:39:15,020
with RabbitMQ and our new rabbit job

00:39:13,430 --> 00:39:17,960
processing stuff and we're actually able

00:39:15,020 --> 00:39:19,460
to track these and know if we were

00:39:17,960 --> 00:39:22,010
losing messages how many we were losing

00:39:19,460 --> 00:39:27,080
and the answer is zero so that's good

00:39:22,010 --> 00:39:30,470
we've solved that sadly we still built

00:39:27,080 --> 00:39:31,850
all this ourselves so we still own the

00:39:30,470 --> 00:39:34,040
the burden of all of this in the

00:39:31,850 --> 00:39:35,270
debugging burden of all of this so we

00:39:34,040 --> 00:39:36,920
didn't get out of having everything

00:39:35,270 --> 00:39:38,150
homegrown but we do at least feel like

00:39:36,920 --> 00:39:42,160
we got the things that are well designed

00:39:38,150 --> 00:39:45,980
and well understood and a little less

00:39:42,160 --> 00:39:48,890
have a few less years of pain piled on

00:39:45,980 --> 00:39:50,450
top of them went through the last tube

00:39:48,890 --> 00:39:52,040
we can actually scale up our jobs

00:39:50,450 --> 00:39:55,460
individually we can manage those in ways

00:39:52,040 --> 00:39:58,520
that are saying and then the great part

00:39:55,460 --> 00:40:00,740
is so you know now we're here it's 2014

00:39:58,520 --> 00:40:03,050
just crazy but

00:40:00,740 --> 00:40:05,300
we're at more than 2x the amount of

00:40:03,050 --> 00:40:11,660
traffic we were processing before so

00:40:05,300 --> 00:40:13,190
we're processing millions of event of

00:40:11,660 --> 00:40:17,360
millions of events through this system

00:40:13,190 --> 00:40:21,560
per minute millions we're processing

00:40:17,360 --> 00:40:24,980
well over a million of our general

00:40:21,560 --> 00:40:27,530
process jobs per hour this stuff's

00:40:24,980 --> 00:40:29,119
running at scale and it worked so we

00:40:27,530 --> 00:40:32,690
designed something that fit our scale in

00:40:29,119 --> 00:40:35,570
2012 we're here a year later and it

00:40:32,690 --> 00:40:36,920
still holds up that's really exciting as

00:40:35,570 --> 00:40:38,119
someone who got to be in the middle of

00:40:36,920 --> 00:40:40,250
building all that that's always your

00:40:38,119 --> 00:40:41,750
hope is that you've got it figured out

00:40:40,250 --> 00:40:47,510
and so far that's that's gone really

00:40:41,750 --> 00:40:50,450
well so we're really excited for that so

00:40:47,510 --> 00:40:53,869
kind of in conclusion what do we know

00:40:50,450 --> 00:40:56,360
job Processing's hard we thought it was

00:40:53,869 --> 00:40:59,300
not that hard we thought we kind of

00:40:56,360 --> 00:41:02,990
understood this but really things are

00:40:59,300 --> 00:41:05,330
hard and at scale everything is hard

00:41:02,990 --> 00:41:07,790
even things like looking up a DNS

00:41:05,330 --> 00:41:11,119
address apparently are not trivial you

00:41:07,790 --> 00:41:12,770
can assume nothing at scale that was

00:41:11,119 --> 00:41:14,780
that was a painful lesson to learn

00:41:12,770 --> 00:41:17,690
because you don't you don't assume that

00:41:14,780 --> 00:41:19,670
looking up a DNS address is going to be

00:41:17,690 --> 00:41:21,230
your biggest problem and it wasn't our

00:41:19,670 --> 00:41:24,380
biggest problem but it was a big one and

00:41:21,230 --> 00:41:25,820
we had no idea that was coming also we

00:41:24,380 --> 00:41:28,820
didn't treat all of our jobs the same at

00:41:25,820 --> 00:41:30,320
the end we we determined we had really

00:41:28,820 --> 00:41:32,420
different use cases and really different

00:41:30,320 --> 00:41:34,369
q technologies and really different

00:41:32,420 --> 00:41:35,900
trade-offs that we needed we didn't end

00:41:34,369 --> 00:41:37,369
up in a place where everything was doing

00:41:35,900 --> 00:41:38,450
exactly the same thing and we were okay

00:41:37,369 --> 00:41:40,609
with that that gives us a lot more

00:41:38,450 --> 00:41:44,050
flexibility to go forward a lot more

00:41:40,609 --> 00:41:47,060
options on how we manage this later and

00:41:44,050 --> 00:41:49,400
lastly we understood the trade-offs so

00:41:47,060 --> 00:41:50,960
we got through our you know RabbitMQ

00:41:49,400 --> 00:41:52,700
stuff and we understood what trade-offs

00:41:50,960 --> 00:41:54,710
we were making for durability and

00:41:52,700 --> 00:41:57,230
performance we understood when we took

00:41:54,710 --> 00:42:00,050
SQS that we were taking a slight you

00:41:57,230 --> 00:42:01,760
know total throughput hit to get really

00:42:00,050 --> 00:42:03,290
great availability numbers and really

00:42:01,760 --> 00:42:07,609
great durability that we didn't have to

00:42:03,290 --> 00:42:09,680
manage and throw back to our bonus cart

00:42:07,609 --> 00:42:11,150
we also understood the cost flow of

00:42:09,680 --> 00:42:12,650
these things we understood what it was

00:42:11,150 --> 00:42:13,970
going to cost to build these things up

00:42:12,650 --> 00:42:15,680
and to scale them out as they went

00:42:13,970 --> 00:42:17,780
that was one of the things we actually

00:42:15,680 --> 00:42:20,810
really liked about SQS is that it's

00:42:17,780 --> 00:42:25,190
incredibly cheap since you pay per

00:42:20,810 --> 00:42:26,900
message it just scales exactly as your

00:42:25,190 --> 00:42:29,150
load does there are no big incremental

00:42:26,900 --> 00:42:31,010
jumps as you have to go add a second

00:42:29,150 --> 00:42:32,660
cluster or just add more nodes into the

00:42:31,010 --> 00:42:34,430
cluster it's just straight linear with

00:42:32,660 --> 00:42:37,190
as much traffic as you want to throw at

00:42:34,430 --> 00:42:39,920
it we treat it kind of like a magic

00:42:37,190 --> 00:42:41,990
bucket it takes 20 milliseconds to send

00:42:39,920 --> 00:42:44,119
stuff into it takes 20 milliseconds to

00:42:41,990 --> 00:42:45,590
get stuff out of we haven't found the

00:42:44,119 --> 00:42:47,000
next step we haven't found that point

00:42:45,590 --> 00:42:49,369
where we can throw so much at it that it

00:42:47,000 --> 00:42:51,770
now takes 30 we haven't found it and

00:42:49,369 --> 00:42:53,990
that's that's fantastic so those are

00:42:51,770 --> 00:42:56,630
some really really interesting

00:42:53,990 --> 00:43:00,560
trade-offs and things were really happy

00:42:56,630 --> 00:43:02,480
with that we ended up with at the end so

00:43:00,560 --> 00:43:04,280
if any of that sounds really interesting

00:43:02,480 --> 00:43:06,320
we're like lots of people here are

00:43:04,280 --> 00:43:10,369
hiring either come find me later or

00:43:06,320 --> 00:43:12,550
check out our job site that's it thank

00:43:10,369 --> 00:43:12,550
you guys

00:43:22,190 --> 00:43:24,250

YouTube URL: https://www.youtube.com/watch?v=fXyDTkDtQfs


