Title: Kafka and Cookpad - Sheffield Ruby - October 2018
Publication date: 2018-10-23
Playlist: North West Ruby User Group
Description: 
	Presented by Lewis Buckley.

For the past 4 months Lewis has been busy working on integrating Cookpad with Kafka. Lewis shares some of his insights, tips and tricks in a short presentation, to help you integrate Kafka with a Ruby on Rails application.

_

About Pusher Sessions:

We're bringing the meetup to you. With Sessions, you can watch recordings of top-notch talks from developer meetups -- wherever and whenever you want.

Meetups are a great way to learn from our peers and to keep up with the latest trends and technologies. As developers ourselves, we at Pusher wanted to bring this great content to more people... So we built Sessions. On Sessions, you can watch talks that interest you and subscribe to be notified when new content gets added.

If you run a meetup and want to get involved, kindly get in touch.

_

About Pusher:

Pusher is a hosted service with APIs, developer tools and open source libraries that greatly simplify integrating real-time functionality into web and mobile applications. 

Pusher will automatically scale when required, removing all the pain of setting up and maintaining a secure, real-time infrastructure. 

Pusher is already trusted to do so by thousands of developers and companies like GitHub, MailChimp, the Financial Times, Buffer and many more. 

Getting started takes just a few seconds: simply go to pusher.com and create a free account. Happy hacking!
Captions: 
	00:00:00,000 --> 00:00:07,080
hi everyone i'm luis as it said on this

00:00:03,389 --> 00:00:10,320
life so that's five minutes yeah so i'm

00:00:07,080 --> 00:00:12,870
also from cook pads we organized a meet

00:00:10,320 --> 00:00:14,340
up every month called southwest ruby so

00:00:12,870 --> 00:00:16,789
that's going quite well we had a like

00:00:14,340 --> 00:00:18,810
meet up last night which was nice and

00:00:16,789 --> 00:00:20,430
tomorrow we're going to be in manchester

00:00:18,810 --> 00:00:21,660
so sort of like three Ruby meetups so

00:00:20,430 --> 00:00:24,180
it's really nice to see like so many

00:00:21,660 --> 00:00:26,010
rubyists in the UK I got to travel

00:00:24,180 --> 00:00:28,310
around this week and see everyone

00:00:26,010 --> 00:00:33,300
outside of the normal kind of conference

00:00:28,310 --> 00:00:36,989
schedule stuff so yeah and iPad I I sort

00:00:33,300 --> 00:00:38,850
of work on a team responsible for the

00:00:36,989 --> 00:00:40,290
feed like the newsfeed which is that on

00:00:38,850 --> 00:00:43,110
the homepage of the apps and on the

00:00:40,290 --> 00:00:46,500
website and we'll talk a little bit

00:00:43,110 --> 00:00:48,360
about that tonight specifically I was

00:00:46,500 --> 00:00:51,989
going to share about how we've been

00:00:48,360 --> 00:00:54,629
using Kafka at cook path and why we've

00:00:51,989 --> 00:00:57,870
chosen to use it so cook pads just like

00:00:54,629 --> 00:01:00,359
a monolithic rails application mostly

00:00:57,870 --> 00:01:03,300
and we use Ruby for a lot of different

00:01:00,359 --> 00:01:05,250
things we've got various other sort of

00:01:03,300 --> 00:01:06,840
smaller applications apart from the

00:01:05,250 --> 00:01:08,689
monolith so I guess maybe we're moving a

00:01:06,840 --> 00:01:12,840
little bit more towards like a services

00:01:08,689 --> 00:01:14,670
oriented architecture and tonight yeah

00:01:12,840 --> 00:01:16,590
we'll see to why we chose to use it the

00:01:14,670 --> 00:01:18,630
benefits the pros and cons we've had

00:01:16,590 --> 00:01:20,310
like a few little problems getting

00:01:18,630 --> 00:01:22,380
started and there may be you know we can

00:01:20,310 --> 00:01:25,830
talk about like some questions

00:01:22,380 --> 00:01:28,200
afterwards the same as with car and yeah

00:01:25,830 --> 00:01:31,409
so what is Kafka anyone familiar with

00:01:28,200 --> 00:01:38,280
Kafka what it is great this talks not

00:01:31,409 --> 00:01:40,560
for you yeah so the official definition

00:01:38,280 --> 00:01:42,229
is Apache Kafka it's a distributed

00:01:40,560 --> 00:01:45,060
streaming platform capable of handling

00:01:42,229 --> 00:01:47,369
trillions of events a day so that's like

00:01:45,060 --> 00:01:48,540
trillions is a really big number not

00:01:47,369 --> 00:01:50,880
surprising it's sort of like the

00:01:48,540 --> 00:01:52,470
brainchild of LinkedIn and the team

00:01:50,880 --> 00:01:56,189
there and they sort of open sourced it

00:01:52,470 --> 00:01:57,540
back in 2011 to Apache we get the

00:01:56,189 --> 00:02:01,740
scalability from a few really

00:01:57,540 --> 00:02:03,420
interesting architectural choices and we

00:02:01,740 --> 00:02:06,630
we can have a little look at them

00:02:03,420 --> 00:02:10,619
tonight and maybe you've used or heard

00:02:06,630 --> 00:02:12,250
of rabbitmq sort of messaging bus so

00:02:10,619 --> 00:02:13,660
yeah some similarities and is also

00:02:12,250 --> 00:02:15,940
differences that make Africa really

00:02:13,660 --> 00:02:19,090
interesting one of the analogies I

00:02:15,940 --> 00:02:21,870
really like is slack anyone use slack

00:02:19,090 --> 00:02:24,790
yeah anyone in the slack Channel

00:02:21,870 --> 00:02:28,390
yeah very good so I'm also in a splat

00:02:24,790 --> 00:02:31,390
channel and this is just like one for I

00:02:28,390 --> 00:02:32,890
mean uh PSA so like a really cool way to

00:02:31,390 --> 00:02:35,800
think about slack if you if you first

00:02:32,890 --> 00:02:38,200
join in not really direct messages if we

00:02:35,800 --> 00:02:40,660
just talk about channels in slack if you

00:02:38,200 --> 00:02:42,490
join in a slap channel you're like the

00:02:40,660 --> 00:02:44,560
only person in that slack channel it's a

00:02:42,490 --> 00:02:46,450
lonely place to be but you can talk to

00:02:44,560 --> 00:02:48,880
yourself I guess if you wanted to so you

00:02:46,450 --> 00:02:50,950
can send a message to the slack channel

00:02:48,880 --> 00:02:52,420
and then like you can receive a message

00:02:50,950 --> 00:02:54,420
back from the slack channel and that's

00:02:52,420 --> 00:02:56,530
just only you could like kind of see it

00:02:54,420 --> 00:02:57,760
then you can invite somebody else into

00:02:56,530 --> 00:02:59,260
the slack channel and you've kind of got

00:02:57,760 --> 00:03:01,890
like a two-way conversation going on

00:02:59,260 --> 00:03:03,790
three people in the slack Channel and

00:03:01,890 --> 00:03:06,300
then you can all send and receive

00:03:03,790 --> 00:03:08,530
messages to each other on a given topic

00:03:06,300 --> 00:03:10,030
you can then maybe two of you could go

00:03:08,530 --> 00:03:11,590
and sell another slack channel and then

00:03:10,030 --> 00:03:14,680
you're in two slap channels in the same

00:03:11,590 --> 00:03:16,360
slack organization and then like maybe

00:03:14,680 --> 00:03:17,739
six months down the line you know you've

00:03:16,360 --> 00:03:19,750
got a new colleague that joins your team

00:03:17,739 --> 00:03:21,880
and you can add them into one of those

00:03:19,750 --> 00:03:24,549
slack channels and they get to see all

00:03:21,880 --> 00:03:26,049
the new messages that I've been sent

00:03:24,549 --> 00:03:29,440
that are sent and received on that

00:03:26,049 --> 00:03:31,510
channel but also they could scroll back

00:03:29,440 --> 00:03:34,150
up like scroll up the list and go back

00:03:31,510 --> 00:03:37,480
in time and see all the previous

00:03:34,150 --> 00:03:37,840
messages on that channel if they wanted

00:03:37,480 --> 00:03:42,670
to

00:03:37,840 --> 00:03:47,040
so is it with Kefka channels are called

00:03:42,670 --> 00:03:49,690
topics so that's a Kafka topic and

00:03:47,040 --> 00:03:51,910
people are called producers and

00:03:49,690 --> 00:03:53,470
consumers depending on whether you're

00:03:51,910 --> 00:03:54,910
sending or receiving a message and you

00:03:53,470 --> 00:03:58,000
could be both so you could be a producer

00:03:54,910 --> 00:04:01,299
and a consumer of a given application so

00:03:58,000 --> 00:04:04,329
that's the analogy I really like for

00:04:01,299 --> 00:04:06,100
Kafka but like why is that interesting

00:04:04,329 --> 00:04:08,530
to us as like Ruby developers rails

00:04:06,100 --> 00:04:11,500
applications maybe or in any sort of

00:04:08,530 --> 00:04:15,640
typical architecture well a few months

00:04:11,500 --> 00:04:17,799
ago I was sort of tasked with changing

00:04:15,640 --> 00:04:20,380
one of that key features this news feed

00:04:17,799 --> 00:04:23,020
within cook pad and it's like one of our

00:04:20,380 --> 00:04:25,570
busiest times of year is a Ramadan and

00:04:23,020 --> 00:04:27,130
that's a really interesting time because

00:04:25,570 --> 00:04:29,560
like in Indonesia for instance one of

00:04:27,130 --> 00:04:31,690
our largest markets Muslim population

00:04:29,560 --> 00:04:34,270
during that that festival over the

00:04:31,690 --> 00:04:35,920
course of the month it's very important

00:04:34,270 --> 00:04:39,340
time family time in the evening people

00:04:35,920 --> 00:04:42,130
cook together and and eat together and

00:04:39,340 --> 00:04:43,870
so for us that traffic just goes sky

00:04:42,130 --> 00:04:47,350
high for that month everyone's sort of

00:04:43,870 --> 00:04:49,720
sharing recipes looking for interesting

00:04:47,350 --> 00:04:51,610
things to eat and a real celebration for

00:04:49,720 --> 00:04:54,100
the whole month so we want to really

00:04:51,610 --> 00:04:55,540
like us as cook pad we want to make make

00:04:54,100 --> 00:04:58,840
the best experience we can for people

00:04:55,540 --> 00:05:00,310
around cooking and so we were tasked to

00:04:58,840 --> 00:05:07,060
make some changes to our news feed for

00:05:00,310 --> 00:05:09,730
that event this is a feed and this is

00:05:07,060 --> 00:05:11,290
what it looks like on the iOS app I can

00:05:09,730 --> 00:05:15,490
follow people and like see stuff from

00:05:11,290 --> 00:05:16,990
that they've posted we wanted though to

00:05:15,490 --> 00:05:19,390
improve this because at the moment this

00:05:16,990 --> 00:05:22,660
is sort of like a time ordered list of

00:05:19,390 --> 00:05:24,880
things that you your friends or people

00:05:22,660 --> 00:05:28,420
you're connected with cook cheese cakes

00:05:24,880 --> 00:05:30,010
good and but we wanted to make it you

00:05:28,420 --> 00:05:32,020
know instead of it being chronological

00:05:30,010 --> 00:05:33,520
we wanted to like really if you haven't

00:05:32,020 --> 00:05:35,020
visited for a week or two weeks we

00:05:33,520 --> 00:05:36,430
wanted to make it so you don't miss any

00:05:35,020 --> 00:05:38,470
of the important content that you've

00:05:36,430 --> 00:05:39,820
maybe that's come up during that two

00:05:38,470 --> 00:05:42,310
weeks you've been away from Coppa and

00:05:39,820 --> 00:05:43,780
you come back very typical feature but

00:05:42,310 --> 00:05:46,020
trying to think about how we're going to

00:05:43,780 --> 00:05:48,250
build that feature was quite challenging

00:05:46,020 --> 00:05:49,960
so we've got our feed is made up of

00:05:48,250 --> 00:05:52,510
these activity cards we've got like a

00:05:49,960 --> 00:05:56,260
recipe published or you can actually you

00:05:52,510 --> 00:05:58,390
can cook a recipe on cook pad and share

00:05:56,260 --> 00:06:00,970
it and that's what this kind of cook

00:05:58,390 --> 00:06:03,670
snap feature is or in this case Carla's

00:06:00,970 --> 00:06:07,090
liked a recipe and we showed those in

00:06:03,670 --> 00:06:09,370
the feed to so I looked at the current

00:06:07,090 --> 00:06:10,840
architecture and application I began to

00:06:09,370 --> 00:06:12,070
dive into it I wanted to come up with

00:06:10,840 --> 00:06:13,240
like an estimate for how long it was

00:06:12,070 --> 00:06:15,010
going to take us to make these changes

00:06:13,240 --> 00:06:17,500
to see if we can make them in the four

00:06:15,010 --> 00:06:20,200
weeks that we had before Rama de and I

00:06:17,500 --> 00:06:23,080
fired up my sort of text editor

00:06:20,200 --> 00:06:25,990
I found something like this actually I

00:06:23,080 --> 00:06:27,750
found this exact code this is like a

00:06:25,990 --> 00:06:30,130
recipe model in a rails application

00:06:27,750 --> 00:06:34,419
right at the top there is that cool box

00:06:30,130 --> 00:06:35,830
where we generate feed items which are

00:06:34,419 --> 00:06:38,890
just like another database tape

00:06:35,830 --> 00:06:43,210
and that happens like on different

00:06:38,890 --> 00:06:44,950
events so the thing is though as you

00:06:43,210 --> 00:06:46,480
just saw in the previous slide about the

00:06:44,950 --> 00:06:48,070
different activities you know it's not

00:06:46,480 --> 00:06:49,570
just on the recipe model these were like

00:06:48,070 --> 00:06:51,910
on several different models throughout

00:06:49,570 --> 00:06:55,180
the application similar callbacks on

00:06:51,910 --> 00:06:56,620
each and you know it became a real

00:06:55,180 --> 00:06:58,780
challenge to think like how are we going

00:06:56,620 --> 00:07:02,470
to be able to actually decoupled this

00:06:58,780 --> 00:07:03,940
and make this make this feed really easy

00:07:02,470 --> 00:07:06,970
to reason about from an application

00:07:03,940 --> 00:07:08,350
architecture point of view even running

00:07:06,970 --> 00:07:09,730
migrations on the feed table was

00:07:08,350 --> 00:07:11,590
difficult so we got a single feed table

00:07:09,730 --> 00:07:13,180
it's like five hundred million rows so

00:07:11,590 --> 00:07:14,530
like running a database migration on

00:07:13,180 --> 00:07:18,160
that it's like really non-trivial

00:07:14,530 --> 00:07:19,570
exercise so went back to the product

00:07:18,160 --> 00:07:20,740
team I was like sorry we can't do this

00:07:19,570 --> 00:07:22,330
you know we're gonna have to make some

00:07:20,740 --> 00:07:24,790
little cosmetic changes for Ramadan

00:07:22,330 --> 00:07:26,130
we're gonna have to just do some tweaks

00:07:24,790 --> 00:07:31,210
basically we're not gonna be able to do

00:07:26,130 --> 00:07:33,400
relevant feed in time and I really then

00:07:31,210 --> 00:07:35,170
started the next step of the process was

00:07:33,400 --> 00:07:35,920
to go back to the rest of the team and

00:07:35,170 --> 00:07:37,210
say that cool

00:07:35,920 --> 00:07:41,020
what's that architecture looking like

00:07:37,210 --> 00:07:43,960
what's our plans and yeah we started to

00:07:41,020 --> 00:07:45,460
talk about architecture a lot so yeah

00:07:43,960 --> 00:07:47,500
different approaches possible obviously

00:07:45,460 --> 00:07:48,700
we had the current callbacks approach we

00:07:47,500 --> 00:07:50,380
wanted to think about different data

00:07:48,700 --> 00:07:51,670
stores as well we know that like in the

00:07:50,380 --> 00:07:54,280
early days Instagram had a lot of

00:07:51,670 --> 00:07:55,590
success with Redis that's that's

00:07:54,280 --> 00:07:58,330
something I've been looking at a lot and

00:07:55,590 --> 00:08:00,220
we wanted to look at this concept of

00:07:58,330 --> 00:08:01,570
like a services oriented architecture a

00:08:00,220 --> 00:08:03,130
little bit more some people suggested

00:08:01,570 --> 00:08:05,110
you know maybe splitting that out can we

00:08:03,130 --> 00:08:05,650
instead of having it callbacks all over

00:08:05,110 --> 00:08:08,380
our rails

00:08:05,650 --> 00:08:10,270
codebase is this really like a separate

00:08:08,380 --> 00:08:13,300
thing that could live on in a service on

00:08:10,270 --> 00:08:15,670
its own and we thought we'd you know

00:08:13,300 --> 00:08:18,640
it's worth exploring that a little bit

00:08:15,670 --> 00:08:21,790
further so yeah as I've mentioned

00:08:18,640 --> 00:08:23,290
earlier we've got a monolith Ruby on

00:08:21,790 --> 00:08:25,930
Rails about sixty thousand lines of code

00:08:23,290 --> 00:08:27,550
not not the biggest Ruby at the Ruby on

00:08:25,930 --> 00:08:30,400
Rails application but definitely not

00:08:27,550 --> 00:08:32,170
small one either and you know in our

00:08:30,400 --> 00:08:34,690
architecture we have another search

00:08:32,170 --> 00:08:36,910
application so that's sort of the two

00:08:34,690 --> 00:08:38,260
main applications the search application

00:08:36,910 --> 00:08:41,410
is a lot smaller and really it's just a

00:08:38,260 --> 00:08:46,770
proxy to elastic search so I was aware

00:08:41,410 --> 00:08:48,750
of that we you know it's interesting

00:08:46,770 --> 00:08:50,040
quote from DHH there are patterns that

00:08:48,750 --> 00:08:52,140
are less about the code or more about

00:08:50,040 --> 00:08:53,580
how the code is being written by whom

00:08:52,140 --> 00:08:56,880
and within which organizations so that

00:08:53,580 --> 00:09:01,020
was really the case with our web and

00:08:56,880 --> 00:09:03,029
search sort of rails applications we

00:09:01,020 --> 00:09:05,190
have a web team that's being called and

00:09:03,029 --> 00:09:07,290
the rest of the guys and the search team

00:09:05,190 --> 00:09:09,720
so the search team very much more

00:09:07,290 --> 00:09:12,350
focused on elastic search and that sort

00:09:09,720 --> 00:09:14,940
of skill set but also Ruby developers

00:09:12,350 --> 00:09:18,810
but yeah we just however it evolved

00:09:14,940 --> 00:09:20,760
we've got a web team in a search team so

00:09:18,810 --> 00:09:22,140
we began to sort of think about how we

00:09:20,760 --> 00:09:23,640
could slot the feeds into this and we

00:09:22,140 --> 00:09:26,520
didn't want to do the same thing as the

00:09:23,640 --> 00:09:28,709
search service we already run in like

00:09:26,520 --> 00:09:31,890
api's between the two you've got to keep

00:09:28,709 --> 00:09:33,600
the API is up to day in step actually

00:09:31,890 --> 00:09:35,010
there's a shared database between the

00:09:33,600 --> 00:09:37,200
two as well which is a really like

00:09:35,010 --> 00:09:38,399
gnarly piece of the architecture and we

00:09:37,200 --> 00:09:39,990
don't want to run into those same

00:09:38,399 --> 00:09:42,330
pitfalls again so we looked at how we

00:09:39,990 --> 00:09:43,950
could think about the feeders events

00:09:42,330 --> 00:09:47,160
that happen in the application like a

00:09:43,950 --> 00:09:48,899
user likes a recipe or someone comments

00:09:47,160 --> 00:09:52,410
on a recipe or someone cook snaps a

00:09:48,899 --> 00:09:54,180
recipe omit those two Kafka and then

00:09:52,410 --> 00:09:57,029
like if we think of that as a slack

00:09:54,180 --> 00:10:00,660
channel the web application says to the

00:09:57,029 --> 00:10:03,240
slack Channel you know user likes recipe

00:10:00,660 --> 00:10:05,160
the feeds application is member of that

00:10:03,240 --> 00:10:08,459
channel too can consume from that

00:10:05,160 --> 00:10:09,060
Channel and sees oh you know user likes

00:10:08,459 --> 00:10:12,230
recipe

00:10:09,060 --> 00:10:16,980
well we log that then in Redis and

00:10:12,230 --> 00:10:17,640
that's our sort of rudimentary feed so

00:10:16,980 --> 00:10:19,410
to speak

00:10:17,640 --> 00:10:22,260
so once we've kind of figured out this

00:10:19,410 --> 00:10:23,790
what we wanted to try we have to break

00:10:22,260 --> 00:10:25,470
it down actually I said liked recipe

00:10:23,790 --> 00:10:27,330
there there's a ton of different

00:10:25,470 --> 00:10:29,130
messages that we need to send in our

00:10:27,330 --> 00:10:31,770
console in our context we have some

00:10:29,130 --> 00:10:33,660
moderation so things can be approved or

00:10:31,770 --> 00:10:36,240
unapproved bookmarked or run bookmarked

00:10:33,660 --> 00:10:39,230
deleted and so on so we came up with

00:10:36,240 --> 00:10:42,329
this kind of list of events and

00:10:39,230 --> 00:10:44,910
obviously defining your events is quite

00:10:42,329 --> 00:10:46,200
interesting because unlike humans you

00:10:44,910 --> 00:10:48,450
know you can write anything you like in

00:10:46,200 --> 00:10:49,800
a slack channel and anyone else can

00:10:48,450 --> 00:10:53,430
probably understand it if it's in the

00:10:49,800 --> 00:10:55,920
same language with computers obviously

00:10:53,430 --> 00:10:58,050
if there's a slight typo in that slap in

00:10:55,920 --> 00:10:59,550
that message the other receptor

00:10:58,050 --> 00:11:01,440
recipient is not going to understand

00:10:59,550 --> 00:11:02,910
so this is the challenge when you think

00:11:01,440 --> 00:11:06,510
about like an event-driven architecture

00:11:02,910 --> 00:11:09,540
is what formatting your message is going

00:11:06,510 --> 00:11:10,950
to be in and it's the recipient gonna

00:11:09,540 --> 00:11:13,350
understand now and are they going to

00:11:10,950 --> 00:11:16,230
understand in the future so that's that

00:11:13,350 --> 00:11:17,670
that's what we call schema and message

00:11:16,230 --> 00:11:18,930
schema and so we have to start thinking

00:11:17,670 --> 00:11:22,170
about a message scheme where up the

00:11:18,930 --> 00:11:25,230
phone so these are sort of the the way

00:11:22,170 --> 00:11:29,280
we defined it out data are events into

00:11:25,230 --> 00:11:32,790
topics like recipes topic a users topic

00:11:29,280 --> 00:11:35,460
and a recipes recipe visits topic recipe

00:11:32,790 --> 00:11:38,550
visits in interest in one that's our

00:11:35,460 --> 00:11:41,130
super high volume like topic so those

00:11:38,550 --> 00:11:43,020
events happen a way higher volume than

00:11:41,130 --> 00:11:45,180
all the others because all you've got to

00:11:43,020 --> 00:11:47,970
do to make to trigger that event is

00:11:45,180 --> 00:11:50,280
actually just visit a recipe on code pad

00:11:47,970 --> 00:11:53,610
and then that triggers a character event

00:11:50,280 --> 00:11:55,320
straightaway the others are lower volume

00:11:53,610 --> 00:11:57,810
and we split them out like that because

00:11:55,320 --> 00:11:59,550
we wanted to keep not only keep them

00:11:57,810 --> 00:12:00,990
separate but some consumers might not be

00:11:59,550 --> 00:12:02,130
interested in what happens with users

00:12:00,990 --> 00:12:04,190
you know they just want to know about

00:12:02,130 --> 00:12:06,900
what's happening with the recipes and

00:12:04,190 --> 00:12:07,980
and so that's kind of what we decided to

00:12:06,900 --> 00:12:12,620
do there are some other considerations

00:12:07,980 --> 00:12:14,850
as well like an message ordering so

00:12:12,620 --> 00:12:19,020
message ordering is only guaranteed

00:12:14,850 --> 00:12:22,350
within a partition of a topic and so if

00:12:19,020 --> 00:12:23,790
you need like in slack obviously you can

00:12:22,350 --> 00:12:25,530
see by time when the messages were

00:12:23,790 --> 00:12:27,720
received but it is possible with Kafka

00:12:25,530 --> 00:12:30,240
if to receive messages slightly out of

00:12:27,720 --> 00:12:31,350
order and that's something to consider

00:12:30,240 --> 00:12:33,810
as well when you're designing your

00:12:31,350 --> 00:12:38,280
message structure book it's kind of a

00:12:33,810 --> 00:12:39,990
bit more of an advanced topic we in

00:12:38,280 --> 00:12:41,940
terms of what the messages look like you

00:12:39,990 --> 00:12:43,860
know it's not plain English we're just

00:12:41,940 --> 00:12:45,270
going to use Jason at this point there

00:12:43,860 --> 00:12:46,830
are other options available with Kathy

00:12:45,270 --> 00:12:49,770
you know you can send raw bytes you can

00:12:46,830 --> 00:12:52,230
send the images popular one is Avro

00:12:49,770 --> 00:12:54,060
which is kind of enforces the schema

00:12:52,230 --> 00:12:56,100
with it a product called schema registry

00:12:54,060 --> 00:12:57,570
but to start with we just wanted to keep

00:12:56,100 --> 00:12:59,250
it really simple and just stick with

00:12:57,570 --> 00:13:01,080
Jason because that's easy to understand

00:12:59,250 --> 00:13:03,600
from both the producer and consumer side

00:13:01,080 --> 00:13:05,250
and it's easy for us to to kind of keep

00:13:03,600 --> 00:13:09,840
track of it and that's sort of what it

00:13:05,250 --> 00:13:11,070
looks like so yeah as I said you know

00:13:09,840 --> 00:13:12,690
this is what we settled with and those

00:13:11,070 --> 00:13:14,940
messages are flowing they're between

00:13:12,690 --> 00:13:17,850
the Web API Kafka and being consumed by

00:13:14,940 --> 00:13:19,980
the feeds application so went and spoke

00:13:17,850 --> 00:13:21,810
to the infrastructure team it's like hey

00:13:19,980 --> 00:13:23,430
we guys know we want to use Kafka and

00:13:21,810 --> 00:13:26,160
they were like oh great another service

00:13:23,430 --> 00:13:27,900
to support and maintain maintain so that

00:13:26,160 --> 00:13:29,190
was a good conversation but no they were

00:13:27,900 --> 00:13:30,840
they were super helpful and it was all

00:13:29,190 --> 00:13:33,270
fine because they just said oh don't

00:13:30,840 --> 00:13:34,050
worry we'll just use the cloud and it's

00:13:33,270 --> 00:13:37,860
gonna be good

00:13:34,050 --> 00:13:40,440
so confluent are a cloud hosting

00:13:37,860 --> 00:13:43,560
provider they can set up Kafka in your

00:13:40,440 --> 00:13:45,600
AWS region or your Google App Engine

00:13:43,560 --> 00:13:48,150
region so there's like really minimal

00:13:45,600 --> 00:13:52,470
latency between your app for us we just

00:13:48,150 --> 00:13:53,910
set up in u.s. East one AWS and they

00:13:52,470 --> 00:13:55,440
have sort of different tiers so we like

00:13:53,910 --> 00:13:56,640
went in on the professional tier and

00:13:55,440 --> 00:13:58,430
then we just have to worry about any of

00:13:56,640 --> 00:14:00,900
the infrastructure so that's been cool

00:13:58,430 --> 00:14:02,340
right so we've got Kafka we've got our

00:14:00,900 --> 00:14:07,380
messages we understand what we're trying

00:14:02,340 --> 00:14:09,680
to build let's get started first of all

00:14:07,380 --> 00:14:11,700
then we need to produce the messages and

00:14:09,680 --> 00:14:13,010
there's some really cool Ruby libraries

00:14:11,700 --> 00:14:17,070
around this that help us

00:14:13,010 --> 00:14:18,810
Zendesk big users of Kafka a big

00:14:17,070 --> 00:14:19,320
contributors to the open source

00:14:18,810 --> 00:14:22,910
community

00:14:19,320 --> 00:14:26,220
they have several sort of gems available

00:14:22,910 --> 00:14:28,620
Ruby Kafka is like the low level one and

00:14:26,220 --> 00:14:30,870
just reading the readme on that project

00:14:28,620 --> 00:14:33,390
is a great way to go from like knowing

00:14:30,870 --> 00:14:35,820
nothing about Kafka to knowing all the

00:14:33,390 --> 00:14:38,310
main topics so just to read me of that

00:14:35,820 --> 00:14:40,350
gem is is really interesting so we use

00:14:38,310 --> 00:14:41,880
that and actually the only part we found

00:14:40,350 --> 00:14:44,520
was like a little bit lacking was like

00:14:41,880 --> 00:14:46,380
be the glue into our rails code and we

00:14:44,520 --> 00:14:48,590
wanted to do some code specific things

00:14:46,380 --> 00:14:52,920
so we've made a gem it's called Streamy

00:14:48,590 --> 00:14:55,110
it's open source on our github and it

00:14:52,920 --> 00:14:58,230
just supports a few little things like a

00:14:55,110 --> 00:15:01,080
basic kind of base event class for

00:14:58,230 --> 00:15:03,060
producing for Kafka and we also

00:15:01,080 --> 00:15:04,620
implemented the concept of message

00:15:03,060 --> 00:15:07,500
priorities which was something that was

00:15:04,620 --> 00:15:10,230
important to us so if it's a really high

00:15:07,500 --> 00:15:12,510
priority or essential message it's sent

00:15:10,230 --> 00:15:14,880
straight away to Kafka if it's kind of

00:15:12,510 --> 00:15:16,860
less important we're happy to batch them

00:15:14,880 --> 00:15:18,620
up into groups of a thousand or whatever

00:15:16,860 --> 00:15:21,390
and then just send them periodically

00:15:18,620 --> 00:15:22,620
that that improves performance sending

00:15:21,390 --> 00:15:23,579
them in a batch but there's always the

00:15:22,620 --> 00:15:26,259
danger we could lose

00:15:23,579 --> 00:15:27,729
so you know if I follow another user

00:15:26,259 --> 00:15:30,879
that's quite to us that's quite an

00:15:27,729 --> 00:15:32,319
important event if I just like a recipe

00:15:30,879 --> 00:15:34,089
you know maybe that's less importantly

00:15:32,319 --> 00:15:35,949
if we lose a light mmm it's not gonna

00:15:34,089 --> 00:15:37,599
have you know we're not a bank it's not

00:15:35,949 --> 00:15:44,649
going to of course of course a big deal

00:15:37,599 --> 00:15:47,349
so yeah where's my light my likes have

00:15:44,649 --> 00:15:49,329
not been the correct number so no but

00:15:47,349 --> 00:15:50,649
that is important actually like it is

00:15:49,329 --> 00:15:53,349
important to choose the right tool for

00:15:50,649 --> 00:15:54,970
the job and when you are like I think

00:15:53,349 --> 00:15:56,679
Facebook I've got the slogan like move

00:15:54,970 --> 00:16:00,129
fast and break things you know that's

00:15:56,679 --> 00:16:02,079
great for them for us you know it's

00:16:00,129 --> 00:16:04,899
similar we can we can take take risks

00:16:02,079 --> 00:16:06,970
sometimes which is nice but working for

00:16:04,899 --> 00:16:08,709
other places or in other context or

00:16:06,970 --> 00:16:10,479
maybe we do actually do some payment

00:16:08,709 --> 00:16:11,889
stuff we've got a premium service that

00:16:10,479 --> 00:16:14,259
probably won't be going onto category

00:16:11,889 --> 00:16:17,379
anytime soon you know so that's that's

00:16:14,259 --> 00:16:20,109
just for us so yeah in terms of like

00:16:17,379 --> 00:16:22,389
what it looks like in Ruby on Rails we

00:16:20,109 --> 00:16:23,739
actually put it into our app folders you

00:16:22,389 --> 00:16:25,419
know we've got our controllers models

00:16:23,739 --> 00:16:28,059
views we've got events in there now

00:16:25,419 --> 00:16:30,959
we've got every like event type as a

00:16:28,059 --> 00:16:33,009
class kind of a plain old Ruby object

00:16:30,959 --> 00:16:36,459
but that just inherits from our

00:16:33,009 --> 00:16:38,769
application event class which inherits

00:16:36,459 --> 00:16:41,169
from Streamy which is our framework that

00:16:38,769 --> 00:16:44,919
we've we've created we specify the topic

00:16:41,169 --> 00:16:46,419
name we set it up with whatever model or

00:16:44,919 --> 00:16:48,279
whatever we need to send an event for

00:16:46,419 --> 00:16:50,529
and the actual event can generate the

00:16:48,279 --> 00:16:53,439
JSON payload and does it in a really

00:16:50,529 --> 00:16:57,220
consistent way which is important on the

00:16:53,439 --> 00:16:59,979
cats the producing side on the consuming

00:16:57,220 --> 00:17:02,979
side to receive messages and we really

00:16:59,979 --> 00:17:05,470
have committed on to using a gem called

00:17:02,979 --> 00:17:07,389
Kafka somebody mentioned to me earlier

00:17:05,470 --> 00:17:10,059
about race car from Zander which is

00:17:07,389 --> 00:17:11,829
another option but we found like Kafka's

00:17:10,059 --> 00:17:13,509
being really cool the the author of

00:17:11,829 --> 00:17:15,459
Kafka has been over to the southwest

00:17:13,509 --> 00:17:17,589
Ruby meetup he's like a really great guy

00:17:15,459 --> 00:17:21,009
and worked with us to fix like problems

00:17:17,589 --> 00:17:24,459
in it so he's also spoken at Ruby

00:17:21,009 --> 00:17:25,959
Kylie's names macho and yeah I'd highly

00:17:24,459 --> 00:17:28,389
recommend checking out Kafka if you're

00:17:25,959 --> 00:17:30,009
interested in consuming category events

00:17:28,389 --> 00:17:32,830
he's also got a gem called water drop

00:17:30,009 --> 00:17:35,289
for producing kapha events which is

00:17:32,830 --> 00:17:37,000
interesting and so yeah it makes it it

00:17:35,289 --> 00:17:39,700
takes away a lot of the low-level stuff

00:17:37,000 --> 00:17:42,280
performance it makes it sort of like

00:17:39,700 --> 00:17:43,960
when you make a controller a consumer

00:17:42,280 --> 00:17:46,990
sorry it feels a lot like a rails

00:17:43,960 --> 00:17:47,890
controller except in HTTP requests but

00:17:46,990 --> 00:17:50,799
you're not yoke's

00:17:47,890 --> 00:17:53,440
you've got a Casca consumer and instead

00:17:50,799 --> 00:17:54,940
of HTTP params you've got your message

00:17:53,440 --> 00:17:56,350
params that come in so that's like

00:17:54,940 --> 00:18:01,000
really nice it feels like really

00:17:56,350 --> 00:18:03,340
familiar as a rails developer again

00:18:01,000 --> 00:18:05,350
really great documentation wiki readme

00:18:03,340 --> 00:18:08,020
I'd highly recommend checking it out

00:18:05,350 --> 00:18:11,049
it handles this concept of firm consumer

00:18:08,020 --> 00:18:13,210
groups this is like I use this topic we

00:18:11,049 --> 00:18:16,210
got to consumers in a consumer group

00:18:13,210 --> 00:18:19,480
this is a neat feature for performance

00:18:16,210 --> 00:18:21,250
but also for if one of our feeds

00:18:19,480 --> 00:18:22,750
consumers goes down or an instance of

00:18:21,250 --> 00:18:25,090
our feeds consumers find the other ones

00:18:22,750 --> 00:18:26,530
there to pick up and vice-versa so you

00:18:25,090 --> 00:18:29,409
can do like rolling restarts and stuff

00:18:26,530 --> 00:18:31,600
it's pretty neat in terms of like what

00:18:29,409 --> 00:18:33,789
this architecture begin began to enable

00:18:31,600 --> 00:18:35,669
that you know we couldn't do before like

00:18:33,789 --> 00:18:39,700
these are these are new concepts for us

00:18:35,669 --> 00:18:41,559
with reason right separated with Kafka

00:18:39,700 --> 00:18:43,600
we were able to start thinking about how

00:18:41,559 --> 00:18:48,220
we wanted to create like a beta

00:18:43,600 --> 00:18:50,799
environment and we had like a global web

00:18:48,220 --> 00:18:52,270
application producing events - Kafka

00:18:50,799 --> 00:18:54,970
that's the blue that's how production

00:18:52,270 --> 00:18:57,669
stack global feeds reading from our

00:18:54,970 --> 00:18:58,990
production Kafka but we began to think

00:18:57,669 --> 00:19:02,140
well actually if all the production

00:18:58,990 --> 00:19:04,539
events are being written into Kafka what

00:19:02,140 --> 00:19:07,210
if we set up another kind of set of

00:19:04,539 --> 00:19:09,880
instances where their own databases to

00:19:07,210 --> 00:19:11,620
read from that production Kafka well

00:19:09,880 --> 00:19:14,650
then we have like an identical copy of

00:19:11,620 --> 00:19:17,110
the database both my sequel and Redis

00:19:14,650 --> 00:19:19,000
and that that's a great test environment

00:19:17,110 --> 00:19:21,010
because we've got all production data in

00:19:19,000 --> 00:19:22,330
there but if we modify it

00:19:21,010 --> 00:19:24,429
we're not modifying the production

00:19:22,330 --> 00:19:27,760
databases so we're actually able to

00:19:24,429 --> 00:19:29,799
create this really functional test

00:19:27,760 --> 00:19:32,140
environment for our feeds and that's

00:19:29,799 --> 00:19:34,780
really important for feeds specifically

00:19:32,140 --> 00:19:36,970
because if I want to demo a feed if I

00:19:34,780 --> 00:19:39,250
want to demo your feed to you but you

00:19:36,970 --> 00:19:41,919
see like fake data or you see somebody

00:19:39,250 --> 00:19:43,600
else's data you can't judge like how

00:19:41,919 --> 00:19:45,730
good is this feed actually going to be

00:19:43,600 --> 00:19:48,490
for me so this test environment is going

00:19:45,730 --> 00:19:49,549
to be key to like developing the product

00:19:48,490 --> 00:19:51,499
forward with cook

00:19:49,549 --> 00:19:55,220
because we can actually do some user

00:19:51,499 --> 00:19:56,509
testing we can do research even just

00:19:55,220 --> 00:19:57,710
testing it with product manager so they

00:19:56,509 --> 00:20:00,769
can get a feel for like whatever new

00:19:57,710 --> 00:20:03,220
algorithm or ranking formula that we put

00:20:00,769 --> 00:20:05,570
in place this has been really useful

00:20:03,220 --> 00:20:09,049
another use case that popped up for us

00:20:05,570 --> 00:20:10,279
was this idea of recipe activity or

00:20:09,049 --> 00:20:12,289
connection events we call them

00:20:10,279 --> 00:20:13,580
internally and this is sort of a bit

00:20:12,289 --> 00:20:15,619
like on LinkedIn you know when you can

00:20:13,580 --> 00:20:18,350
see on LinkedIn who's like viewed your

00:20:15,619 --> 00:20:22,100
profile and stuff is a bit weird

00:20:18,350 --> 00:20:23,629
sometimes but it's similar to that but

00:20:22,100 --> 00:20:26,179
you can actually see like who's been

00:20:23,629 --> 00:20:28,039
checking out your recipes and it you can

00:20:26,179 --> 00:20:31,279
opt-out this but it's the goal is to

00:20:28,039 --> 00:20:32,749
connect users on code pad and actually

00:20:31,279 --> 00:20:35,179
it turns out this is like another type

00:20:32,749 --> 00:20:36,889
of feed and it turns out that this can

00:20:35,179 --> 00:20:39,109
use Catholic events to all the exact

00:20:36,889 --> 00:20:41,149
same events that we've been storing for

00:20:39,109 --> 00:20:42,859
the past few months when we launched

00:20:41,149 --> 00:20:45,230
this feature rather than starting the

00:20:42,859 --> 00:20:47,359
feature with an empty blank slate we're

00:20:45,230 --> 00:20:48,769
able to go back in time and consume all

00:20:47,359 --> 00:20:51,320
the character events that were stored

00:20:48,769 --> 00:20:54,049
within Kafka and actually rebuild this

00:20:51,320 --> 00:20:56,600
feed for everyone and like previously

00:20:54,049 --> 00:20:58,070
but those events would have been lost we

00:20:56,600 --> 00:20:59,989
would have just had the final state in

00:20:58,070 --> 00:21:04,489
the database this was really like

00:20:59,989 --> 00:21:06,289
interesting and useful feature of Kafka

00:21:04,489 --> 00:21:07,820
this ability to store messages back in

00:21:06,289 --> 00:21:11,899
time like on slack channels that you can

00:21:07,820 --> 00:21:14,210
let's scroll up one of the problems we

00:21:11,899 --> 00:21:16,789
had as a any company in Europe is GDP

00:21:14,210 --> 00:21:19,399
are like CAF cuz they've got this

00:21:16,789 --> 00:21:22,009
storage capability right well how do you

00:21:19,399 --> 00:21:24,889
can't randomly delete stuff from it so

00:21:22,009 --> 00:21:26,869
how do you handle gdpr well there is a

00:21:24,889 --> 00:21:28,840
solution it's a bit complicated but it's

00:21:26,869 --> 00:21:32,299
nice to know it exists and it's this

00:21:28,840 --> 00:21:34,940
concept of log compaction so the events

00:21:32,299 --> 00:21:36,679
come in in order and for instance they

00:21:34,940 --> 00:21:39,950
all have the same key in this case it's

00:21:36,679 --> 00:21:41,659
like user 1 and Casca can actually run a

00:21:39,950 --> 00:21:43,730
like a clean up task every now and then

00:21:41,659 --> 00:21:45,619
if configure to do so where it'll go

00:21:43,730 --> 00:21:48,710
through and just keep the most recent

00:21:45,619 --> 00:21:51,169
event so if you have that all configured

00:21:48,710 --> 00:21:53,149
and your final event is like user closed

00:21:51,169 --> 00:21:55,549
account in this case with the use of one

00:21:53,149 --> 00:21:57,710
all these original events these will all

00:21:55,549 --> 00:22:00,710
have been cleaned up from Kafka and all

00:21:57,710 --> 00:22:01,280
you'll be left with is user closed

00:22:00,710 --> 00:22:03,170
account

00:22:01,280 --> 00:22:07,280
so it was nice to know that we had that

00:22:03,170 --> 00:22:08,810
ability in that solution one of the

00:22:07,280 --> 00:22:11,000
things that's been really important to

00:22:08,810 --> 00:22:14,330
us as high quality monitoring and

00:22:11,000 --> 00:22:17,060
logging because it's not like a rails

00:22:14,330 --> 00:22:19,070
application where users will phone you

00:22:17,060 --> 00:22:22,310
up or tweet when the whole site goes

00:22:19,070 --> 00:22:24,980
down you need to keep track of like is

00:22:22,310 --> 00:22:26,270
this thing working properly and so we

00:22:24,980 --> 00:22:29,120
have to set up like Prometheus and

00:22:26,270 --> 00:22:30,530
gravano to help us do that using Kafka

00:22:29,120 --> 00:22:32,720
that's got a really great section on its

00:22:30,530 --> 00:22:34,370
wiki where you can learn more about the

00:22:32,720 --> 00:22:35,780
different configuration options and one

00:22:34,370 --> 00:22:39,320
of the things actually was also great

00:22:35,780 --> 00:22:41,180
with New Relic so New Relic have I'm

00:22:39,320 --> 00:22:44,000
going to share some code with the author

00:22:41,180 --> 00:22:45,680
of craft go for better New Relic

00:22:44,000 --> 00:22:47,000
integrations I don't if you use New

00:22:45,680 --> 00:22:49,550
Relic but that will show you you know if

00:22:47,000 --> 00:22:51,710
you've got a slow database query in your

00:22:49,550 --> 00:22:53,480
message consuming you can pick that up

00:22:51,710 --> 00:22:55,520
and make sure you don't fall behind you

00:22:53,480 --> 00:22:58,730
know in your in your message processing

00:22:55,520 --> 00:23:01,370
and yeah so that's it really I'm happy

00:22:58,730 --> 00:23:03,410
to take any of your questions and it's

00:23:01,370 --> 00:23:05,330
me on Twitter so thank you very much for

00:23:03,410 --> 00:23:10,740
having me

00:23:05,330 --> 00:23:10,740

YouTube URL: https://www.youtube.com/watch?v=V6KXCdWKYIg


