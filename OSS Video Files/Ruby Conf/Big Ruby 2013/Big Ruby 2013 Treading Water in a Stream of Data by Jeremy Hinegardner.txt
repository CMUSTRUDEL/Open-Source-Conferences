Title: Big Ruby 2013 Treading Water in a Stream of Data by Jeremy Hinegardner
Publication date: 2020-01-28
Playlist: Big Ruby 2013
Description: 
	Data arrives in all sorts of forms, more and more today we are seeing data arrive in event-like systems: server logs, twitter, superfeedr notifications, github events, rubygems web hooks, couchdb change notifications, etc. We want to analyze streams of data, and find useful pieces of information in them.

In this talk, using an existing dataset, we will go through the process of obtaining, manipulating, processing, analyzing and storing a stream of data. We will attempt to touch upon a variety of possible topics including: differences between processing static datasets and stream datasets, pitfalls of stream processing, analyzing data in real time and using the datastream itself as data source.

Help us caption & translate this video!

http://amara.org/v/FGdZ/
Captions: 
	00:00:18,689 --> 00:00:25,390
hi everybody my name is Jeremy also

00:00:22,840 --> 00:00:29,619
copious free time and we're going to

00:00:25,390 --> 00:00:31,480
start by waking up all right everybody's

00:00:29,619 --> 00:00:33,550
here Joe told us a couple different

00:00:31,480 --> 00:00:34,870
things yesterday we need to do to to get

00:00:33,550 --> 00:00:35,920
going in the morning so we're gonna do

00:00:34,870 --> 00:00:37,239
I'm going to have everybody raise your

00:00:35,920 --> 00:00:38,500
hands a little bit because I've got some

00:00:37,239 --> 00:00:39,879
question and answer stuff we're going to

00:00:38,500 --> 00:00:42,550
do we got to make sure both your hands

00:00:39,879 --> 00:00:45,129
work so raise them both up come on both

00:00:42,550 --> 00:00:48,730
of them up I'll clap them do it a few

00:00:45,129 --> 00:00:51,760
more times come on all right is it ready

00:00:48,730 --> 00:00:54,879
getting ready and warmed up all right so

00:00:51,760 --> 00:00:59,649
wake up everybody and I got my applause

00:00:54,879 --> 00:01:01,269
early so it worked out well so if you

00:00:59,649 --> 00:01:03,640
are hoping for the split face with a

00:01:01,269 --> 00:01:05,320
bald side in a hairy side unfortunately

00:01:03,640 --> 00:01:09,130
I had to take the mean and just kind of

00:01:05,320 --> 00:01:10,930
average it out so I'm a bit of a data

00:01:09,130 --> 00:01:12,970
junkie I like looking at people's data

00:01:10,930 --> 00:01:15,430
see what they have is see what kind of

00:01:12,970 --> 00:01:17,350
stuff happens hopefully help them figure

00:01:15,430 --> 00:01:19,540
out something to do with it or get some

00:01:17,350 --> 00:01:20,980
really good information out of it but in

00:01:19,540 --> 00:01:23,260
general i'm always looking for

00:01:20,980 --> 00:01:25,840
interesting data sets and a lot of the

00:01:23,260 --> 00:01:28,780
times of what we have are streaming data

00:01:25,840 --> 00:01:30,280
sets or just public data or but most of

00:01:28,780 --> 00:01:32,830
the really really interesting data is

00:01:30,280 --> 00:01:34,180
people's private corporate data that

00:01:32,830 --> 00:01:36,820
they're not going to share with anybody

00:01:34,180 --> 00:01:40,300
but it helps them do better in their

00:01:36,820 --> 00:01:42,010
business so a couple of survey questions

00:01:40,300 --> 00:01:44,440
so you already hands i'll work so i know

00:01:42,010 --> 00:01:47,470
you can do this so raise your hand or

00:01:44,440 --> 00:01:50,950
both if you consider yourself someone

00:01:47,470 --> 00:01:57,900
who processes streaming data we got a

00:01:50,950 --> 00:01:57,900
few all right quick examples anyway yeah

00:01:59,570 --> 00:02:03,080
Twitter stream all right who else those

00:02:01,070 --> 00:02:04,880
processes the Twitter stream all right

00:02:03,080 --> 00:02:13,100
who does not process the Twitter stream

00:02:04,880 --> 00:02:15,110
yes okay what's going where yes we have

00:02:13,100 --> 00:02:17,030
log processing Twitter stream all sorts

00:02:15,110 --> 00:02:21,260
of stuff along those lines so how many

00:02:17,030 --> 00:02:24,320
of you say you process batch data just

00:02:21,260 --> 00:02:27,650
one two three four all right so on those

00:02:24,320 --> 00:02:29,000
of you that process your batch data how

00:02:27,650 --> 00:02:33,410
many of you think you probably process

00:02:29,000 --> 00:02:36,739
your batch is more than once a day all

00:02:33,410 --> 00:02:40,220
right more than once an hour more than

00:02:36,739 --> 00:02:43,040
once a minute maybe any of your process

00:02:40,220 --> 00:02:44,720
or more than once a second see we'll get

00:02:43,040 --> 00:02:46,760
into the definition of what some people

00:02:44,720 --> 00:02:48,320
call streaming data and you know in

00:02:46,760 --> 00:02:52,420
Italy ill you'll see a little

00:02:48,320 --> 00:02:55,280
correlation it's kind of interesting so

00:02:52,420 --> 00:02:57,739
this is big Ruby so we have to use the

00:02:55,280 --> 00:02:59,840
term big data like Matt I'm not really a

00:02:57,739 --> 00:03:01,820
fan of the term I think it's got a

00:02:59,840 --> 00:03:04,280
little bit of over hype but there is a

00:03:01,820 --> 00:03:08,390
new book out from Nathan Mars anyone

00:03:04,280 --> 00:03:09,860
heard of storm yeah so it's a there was

00:03:08,390 --> 00:03:11,570
a company called back type and they've

00:03:09,860 --> 00:03:13,489
developed this whole system called storm

00:03:11,570 --> 00:03:14,900
and then to process the Twitter fire

00:03:13,489 --> 00:03:17,150
hose and then they got bought by Twitter

00:03:14,900 --> 00:03:18,500
so it's actually it's a pretty

00:03:17,150 --> 00:03:20,329
interesting system i've only played with

00:03:18,500 --> 00:03:22,250
it just a little bit so further

00:03:20,329 --> 00:03:24,560
experiments are necessary but Nathan

00:03:22,250 --> 00:03:27,579
Mars and their co-author of his have a

00:03:24,560 --> 00:03:29,780
new book out for Manny called big data

00:03:27,579 --> 00:03:31,820
it's still in beta so there's only

00:03:29,780 --> 00:03:33,440
there's only it's only like half there

00:03:31,820 --> 00:03:34,459
but it has got some interesting stuff

00:03:33,440 --> 00:03:37,670
and i'll probably mentioned a couple

00:03:34,459 --> 00:03:38,930
times in the talk but first we're going

00:03:37,670 --> 00:03:41,870
to have a little bit of fun with the

00:03:38,930 --> 00:03:45,140
term big data so this is wikipedias

00:03:41,870 --> 00:03:48,440
definition of big data a collection of

00:03:45,140 --> 00:03:50,930
data sets so large and so complex that

00:03:48,440 --> 00:03:52,640
it becomes difficult to process using on

00:03:50,930 --> 00:03:54,410
hand database management tools or

00:03:52,640 --> 00:03:57,140
traditional data processing applications

00:03:54,410 --> 00:03:59,150
I don't know who wrote that so and the

00:03:57,140 --> 00:04:02,570
interesting thing is as I did a talk who

00:03:59,150 --> 00:04:03,680
wrote it change it i don't i'm not i

00:04:02,570 --> 00:04:05,959
don't actually change anything on

00:04:03,680 --> 00:04:07,810
wikipedia so I don't have an off I don't

00:04:05,959 --> 00:04:09,949
have an author bit or however it works

00:04:07,810 --> 00:04:12,169
well the interesting thing is is I did a

00:04:09,949 --> 00:04:12,790
talk more on the definitions of big data

00:04:12,169 --> 00:04:14,290
and if you have

00:04:12,790 --> 00:04:16,209
things about a year and a half ago and

00:04:14,290 --> 00:04:18,190
this wasn't the definition a year and a

00:04:16,209 --> 00:04:22,630
half ago on Wikipedia so which is it

00:04:18,190 --> 00:04:26,080
gets changed I'm sure so I have a

00:04:22,630 --> 00:04:28,330
definition so for that talk a while ago

00:04:26,080 --> 00:04:29,979
I tweet is like hey folks what do you

00:04:28,330 --> 00:04:31,500
think the term big data actually means

00:04:29,979 --> 00:04:35,050
and I got all sorts of interesting

00:04:31,500 --> 00:04:37,000
comments some of them from if if the

00:04:35,050 --> 00:04:40,449
data can't be processed in order log in

00:04:37,000 --> 00:04:42,220
time that was one when a data that

00:04:40,449 --> 00:04:44,169
doesn't fit in memory and everybody had

00:04:42,220 --> 00:04:46,449
all these different definitions and to

00:04:44,169 --> 00:04:49,570
me that kind of boiled down to one basic

00:04:46,449 --> 00:04:52,810
thing if you're scared of it it's big

00:04:49,570 --> 00:04:54,789
data I mean it sounds good because it's

00:04:52,810 --> 00:04:56,919
not for me big data I'm like oh

00:04:54,789 --> 00:04:58,630
something that needs to fit on you know

00:04:56,919 --> 00:05:00,729
a dozen machines okay I know how to deal

00:04:58,630 --> 00:05:03,039
with that I'm comfortable with it so but

00:05:00,729 --> 00:05:04,990
something that requires you know 50 data

00:05:03,039 --> 00:05:06,580
centers to deal with okay that scares me

00:05:04,990 --> 00:05:08,050
you know somebody else something that

00:05:06,580 --> 00:05:11,110
fits on more than one machine it might

00:05:08,050 --> 00:05:13,900
scare them and so I think anyone's term

00:05:11,110 --> 00:05:16,150
for big data is the data levels that

00:05:13,900 --> 00:05:17,380
make you feel uncomfortable so you can

00:05:16,150 --> 00:05:25,389
like it you can take it do whatever you

00:05:17,380 --> 00:05:27,970
like so so this is a model of sort of a

00:05:25,389 --> 00:05:32,289
big data system or a data system as you

00:05:27,970 --> 00:05:34,960
whole so you have this data start off

00:05:32,289 --> 00:05:37,919
with some data and you need to store it

00:05:34,960 --> 00:05:40,840
over here that's okay that's the start

00:05:37,919 --> 00:05:42,789
but when you have this data you realize

00:05:40,840 --> 00:05:45,909
when you look at it that you actually

00:05:42,789 --> 00:05:47,289
also need that data and that's over

00:05:45,909 --> 00:05:49,479
there and you need to store it over

00:05:47,289 --> 00:05:52,060
there now here's where the real magic

00:05:49,479 --> 00:05:54,520
comes in because if you have this data

00:05:52,060 --> 00:05:57,699
and you've got that data and you put

00:05:54,520 --> 00:05:59,620
them together you get other data now

00:05:57,699 --> 00:06:01,690
this other data is where the real magic

00:05:59,620 --> 00:06:04,630
happens because you're going to use that

00:06:01,690 --> 00:06:06,760
everywhere and of course as in any

00:06:04,630 --> 00:06:10,150
recipe that requires a couple of

00:06:06,760 --> 00:06:13,810
ingredients what's the final one profit

00:06:10,150 --> 00:06:16,030
remember so this is as well making fun a

00:06:13,810 --> 00:06:17,409
big data but this is sort of in a

00:06:16,030 --> 00:06:19,270
general scheme what happens you get an

00:06:17,409 --> 00:06:21,190
initial data feed of something you

00:06:19,270 --> 00:06:23,020
realize what that's something you need

00:06:21,190 --> 00:06:24,789
something else to make it complete or

00:06:23,020 --> 00:06:27,219
many something else is to make it

00:06:24,789 --> 00:06:29,229
complete and you put them two together

00:06:27,219 --> 00:06:31,779
and then that ends up being probably

00:06:29,229 --> 00:06:33,639
somebody's data product or something

00:06:31,779 --> 00:06:35,620
that they're willing to give to someone

00:06:33,639 --> 00:06:40,570
else or used for internal analysis or

00:06:35,620 --> 00:06:44,800
something like that so the term

00:06:40,570 --> 00:06:46,930
streaming what do we mean by streaming

00:06:44,800 --> 00:06:48,279
so we go back to what the different

00:06:46,930 --> 00:06:50,349
batch levels people are doing them one

00:06:48,279 --> 00:06:52,750
min at a time one second all those

00:06:50,349 --> 00:06:56,229
different things wikipedia has this

00:06:52,750 --> 00:06:58,630
definition of streaming data a sequence

00:06:56,229 --> 00:07:01,090
of data elements made available over

00:06:58,630 --> 00:07:03,460
time and the elements are processed one

00:07:01,090 --> 00:07:06,550
at a time rather than large batches well

00:07:03,460 --> 00:07:08,200
there's no definition of large so what

00:07:06,550 --> 00:07:10,510
is your data element a minutes worth of

00:07:08,200 --> 00:07:12,820
data of seconds worth of data but if

00:07:10,510 --> 00:07:14,409
your data element is a summary stuff

00:07:12,820 --> 00:07:16,630
from upstream that only gets do you 12

00:07:14,409 --> 00:07:18,279
once a day well it's a sequence of data

00:07:16,630 --> 00:07:20,260
elements that show up so for my

00:07:18,279 --> 00:07:21,849
particular purposes and sort of the way

00:07:20,260 --> 00:07:24,389
I kind of think about it if you are

00:07:21,849 --> 00:07:27,310
doing if you're taking data in

00:07:24,389 --> 00:07:30,039
periodically consistently and

00:07:27,310 --> 00:07:32,949
periodically that's streaming data to me

00:07:30,039 --> 00:07:34,810
and they all because eventually if you'd

00:07:32,949 --> 00:07:36,810
look at that little asymptotic curve you

00:07:34,810 --> 00:07:39,729
get down here eventually it's infinity

00:07:36,810 --> 00:07:44,940
it's all streaming data to one degree or

00:07:39,729 --> 00:07:48,339
another so if you also look at it Jim

00:07:44,940 --> 00:07:50,680
you're flying robot thing yesterday you

00:07:48,339 --> 00:07:52,990
process the data well line by line those

00:07:50,680 --> 00:07:54,940
data packets so technically you know

00:07:52,990 --> 00:07:57,339
Jim's processing streaming data there

00:07:54,940 --> 00:08:00,099
with is flying robots so we do have

00:07:57,339 --> 00:08:05,529
robots processing the data in our world

00:08:00,099 --> 00:08:07,599
just watch out let's see so in my am i

00:08:05,529 --> 00:08:11,010
sort of i came to this sort of

00:08:07,599 --> 00:08:13,360
interesting conclusion that big data is

00:08:11,010 --> 00:08:14,740
streaming at the same time streaming a

00:08:13,360 --> 00:08:16,810
big data go hand in hand because the

00:08:14,740 --> 00:08:19,510
easiest way to get a level of data that

00:08:16,810 --> 00:08:21,940
makes you feel uncomfortable is to have

00:08:19,510 --> 00:08:23,680
to get it all the time and deal with it

00:08:21,940 --> 00:08:25,599
all the time and deal with all the error

00:08:23,680 --> 00:08:28,360
cases and deal with everything that goes

00:08:25,599 --> 00:08:30,190
with accepting data from someone else or

00:08:28,360 --> 00:08:31,719
even your own systems your own system

00:08:30,190 --> 00:08:33,250
logs where you know you don't know

00:08:31,719 --> 00:08:34,330
what's going to happen in the log files

00:08:33,250 --> 00:08:35,830
and you just got to merge them all

00:08:34,330 --> 00:08:37,529
together put them together and see what

00:08:35,830 --> 00:08:39,539
happens

00:08:37,529 --> 00:08:41,249
so this is just a conjecture so you can

00:08:39,539 --> 00:08:43,889
take it for what it's worth whether you

00:08:41,249 --> 00:08:48,839
agree with me or not up for discussion

00:08:43,889 --> 00:08:52,290
so we can chat about it so the first

00:08:48,839 --> 00:08:54,660
things first now that we've made josh

00:08:52,290 --> 00:08:56,129
duster proud of giving our definitions

00:08:54,660 --> 00:08:58,350
before we have our discussion anyroad

00:08:56,129 --> 00:09:00,269
fans in here listen to ruby brooch all

00:08:58,350 --> 00:09:01,709
right Josh gesture is always saying well

00:09:00,269 --> 00:09:04,620
we start with a definition so we've

00:09:01,709 --> 00:09:07,589
started with some definitions now what's

00:09:04,620 --> 00:09:11,519
the first thing we need to do in dealing

00:09:07,589 --> 00:09:15,120
with our data anyone all right I'll give

00:09:11,519 --> 00:09:18,209
you a freebie we need to get the data it

00:09:15,120 --> 00:09:20,749
seems listing but this is actually one

00:09:18,209 --> 00:09:23,249
of the more critical components of

00:09:20,749 --> 00:09:26,220
dealing with streaming data you have to

00:09:23,249 --> 00:09:28,259
get it and getting the data affects

00:09:26,220 --> 00:09:29,699
everything down the road it's basically

00:09:28,259 --> 00:09:31,170
you know you throw a rock in a stream

00:09:29,699 --> 00:09:34,589
it's going to affect things down the

00:09:31,170 --> 00:09:39,600
road a little bit so or getting our

00:09:34,589 --> 00:09:41,970
sequence of data elements so I've

00:09:39,600 --> 00:09:44,370
encountered about four or five or four

00:09:41,970 --> 00:09:46,410
different ways of ending up getting data

00:09:44,370 --> 00:09:48,420
from an external source and that exit

00:09:46,410 --> 00:09:50,610
and by external I mean outside of the

00:09:48,420 --> 00:09:52,379
data system that you're working with so

00:09:50,610 --> 00:09:53,579
if you're processing your own engine x

00:09:52,379 --> 00:09:55,199
log those are coming in I don't know

00:09:53,579 --> 00:09:56,490
maybe over syslog and then I getting

00:09:55,199 --> 00:09:58,470
aggregated up and then dumped into

00:09:56,490 --> 00:10:00,480
another system you've got the Twitter

00:09:58,470 --> 00:10:02,970
stream that you're pulling on all these

00:10:00,480 --> 00:10:04,949
different types of systems so the first

00:10:02,970 --> 00:10:08,250
one and probably quite possibly the

00:10:04,949 --> 00:10:11,069
oldest is polling who does some sort of

00:10:08,250 --> 00:10:14,990
polling of data upstream or outside of a

00:10:11,069 --> 00:10:19,319
system we've got a few in the back I

00:10:14,990 --> 00:10:22,079
sort of like to think of pulling as the

00:10:19,319 --> 00:10:23,730
the data systems way of saying are we

00:10:22,079 --> 00:10:25,230
there yet over there yet do you have

00:10:23,730 --> 00:10:26,819
data for me to have data for me you know

00:10:25,230 --> 00:10:29,129
keep on going over and over again and

00:10:26,819 --> 00:10:32,040
most of the time the answer's no yes no

00:10:29,129 --> 00:10:33,839
yes yes yes yes no no no no no no so you

00:10:32,040 --> 00:10:36,600
keep on going and eventually there'll be

00:10:33,839 --> 00:10:38,309
some data for you now one of the

00:10:36,600 --> 00:10:40,439
problems that you encounter with polling

00:10:38,309 --> 00:10:42,300
is how do you know if you've already

00:10:40,439 --> 00:10:44,420
received the data before there's a

00:10:42,300 --> 00:10:48,660
couple different ways I've dealt with

00:10:44,420 --> 00:10:50,639
retrieving scraped RSS feeds and the

00:10:48,660 --> 00:10:51,390
upstream provider gave you a token

00:10:50,639 --> 00:10:53,340
basically at

00:10:51,390 --> 00:10:55,770
timestamp every time you grabbed a chunk

00:10:53,340 --> 00:10:58,250
of data so your next request you would

00:10:55,770 --> 00:11:01,320
send that essentially call it an e-tag

00:10:58,250 --> 00:11:03,480
you'd say if if modified sense or

00:11:01,320 --> 00:11:05,250
something along those lines with your e

00:11:03,480 --> 00:11:06,990
tag value and then they would give you

00:11:05,250 --> 00:11:08,820
all of the data sense that value so

00:11:06,990 --> 00:11:10,980
that's one way of doing it other places

00:11:08,820 --> 00:11:14,610
the upstream doesn't keep track of any

00:11:10,980 --> 00:11:17,100
location in it and you just have to pull

00:11:14,610 --> 00:11:21,480
the data and then check against what

00:11:17,100 --> 00:11:22,710
you've already received and then there

00:11:21,480 --> 00:11:26,310
are some cases where an upstream

00:11:22,710 --> 00:11:27,810
provider may actually keep your state so

00:11:26,310 --> 00:11:29,760
they know you can just always request

00:11:27,810 --> 00:11:31,260
and they'll always just tell you what

00:11:29,760 --> 00:11:32,640
your next data is because they keep

00:11:31,260 --> 00:11:35,250
track of your state I find that one a

00:11:32,640 --> 00:11:36,420
bit more rare and most likely the two

00:11:35,250 --> 00:11:38,160
you see is when you keep track of your

00:11:36,420 --> 00:11:40,980
own token and you said resubmit it back

00:11:38,160 --> 00:11:45,450
or you keep your track of your own state

00:11:40,980 --> 00:11:46,920
in all manners there are a couple of few

00:11:45,450 --> 00:11:49,410
problems with paulownia we have the same

00:11:46,920 --> 00:11:50,610
thing with a couple the other mechanisms

00:11:49,410 --> 00:11:52,440
and I'll tie or try to talk about all

00:11:50,610 --> 00:11:54,810
the different problems of the different

00:11:52,440 --> 00:11:56,820
mechanisms of getting our data when we

00:11:54,810 --> 00:12:02,940
get to after I do a little review of all

00:11:56,820 --> 00:12:05,550
of them so the other one that I kind of

00:12:02,940 --> 00:12:08,160
like is a notification or web hook type

00:12:05,550 --> 00:12:12,270
system how many people subscribe to

00:12:08,160 --> 00:12:14,490
rubygems do using the web hooks you do

00:12:12,270 --> 00:12:16,350
that so Ruby this is a really good

00:12:14,490 --> 00:12:18,600
example of a notification system or

00:12:16,350 --> 00:12:20,310
pubsubhubbub I believe it works this way

00:12:18,600 --> 00:12:21,420
I'm not actually sure who does who was

00:12:20,310 --> 00:12:25,470
somebody was talking about pubsubhubbub

00:12:21,420 --> 00:12:26,940
the other day all right this is a

00:12:25,470 --> 00:12:30,390
notification system where you're going

00:12:26,940 --> 00:12:34,440
to say pay provider I want to subscribe

00:12:30,390 --> 00:12:38,310
to X or some particular stream or some

00:12:34,440 --> 00:12:40,020
particular mechanism of data and you

00:12:38,310 --> 00:12:43,110
just say ok when you've got data for me

00:12:40,020 --> 00:12:45,090
here's how you get a hold of me and so

00:12:43,110 --> 00:12:47,400
when something on the upstream providers

00:12:45,090 --> 00:12:49,350
side they get data then they're just

00:12:47,400 --> 00:12:51,780
going to like ping you and say I you've

00:12:49,350 --> 00:12:55,050
got data yeah you need to come talk to

00:12:51,780 --> 00:12:56,670
me I've got some data for you so that

00:12:55,050 --> 00:12:58,050
makes it pretty easy because you're not

00:12:56,670 --> 00:12:58,950
actually pulling you're not asking if

00:12:58,050 --> 00:13:00,660
they've got the others you're going to

00:12:58,950 --> 00:13:02,490
tell you when you've got data and then

00:13:00,660 --> 00:13:04,230
you go and get it so the notification

00:13:02,490 --> 00:13:04,690
actually doesn't contain the data that

00:13:04,230 --> 00:13:07,690
you're trying

00:13:04,690 --> 00:13:09,940
yet it is just the notification there's

00:13:07,690 --> 00:13:11,530
data to go pick up so it's you know sort

00:13:09,940 --> 00:13:13,390
of like when the fedex guy leaves here

00:13:11,530 --> 00:13:15,520
leaves the ticket on your door it says

00:13:13,390 --> 00:13:17,830
hey you've got data you need to come get

00:13:15,520 --> 00:13:26,800
come down to the store before 7pm and

00:13:17,830 --> 00:13:29,440
pick it up a third one is sort of it's

00:13:26,800 --> 00:13:31,390
very similar to notification but the

00:13:29,440 --> 00:13:34,930
notification actually contains the data

00:13:31,390 --> 00:13:37,510
you're supposed to get so in this case

00:13:34,930 --> 00:13:39,370
actually you may not believe it but this

00:13:37,510 --> 00:13:41,530
probably the one that most people are

00:13:39,370 --> 00:13:43,090
familiar with this is email hey you've

00:13:41,530 --> 00:13:46,390
got a new message and here's all the

00:13:43,090 --> 00:13:48,100
data that's in it so and i would

00:13:46,390 --> 00:13:50,680
consider email you processing it

00:13:48,100 --> 00:13:54,190
periodically I'll sequence of data at a

00:13:50,680 --> 00:13:56,950
period of time and actually I have a

00:13:54,190 --> 00:13:58,950
really fun thing I learned from a couple

00:13:56,950 --> 00:14:02,080
different people as one of the best

00:13:58,950 --> 00:14:04,090
system like disc cueing storage formats

00:14:02,080 --> 00:14:05,800
is just mailed there because every

00:14:04,090 --> 00:14:07,630
message is in a separate file and then

00:14:05,800 --> 00:14:09,340
if you touch it or you read it then they

00:14:07,630 --> 00:14:11,140
just moves into the other directory so

00:14:09,340 --> 00:14:12,940
using maildir as sort of a cue

00:14:11,140 --> 00:14:14,560
processing system is actually a pretty

00:14:12,940 --> 00:14:16,270
fun thing it works out really simple and

00:14:14,560 --> 00:14:17,500
it's and you don't need it doesn't

00:14:16,270 --> 00:14:20,170
require much code to deal with it all

00:14:17,500 --> 00:14:26,800
but it's actually a persistent Q so

00:14:20,170 --> 00:14:28,780
that's a pretty fun thing so payload

00:14:26,800 --> 00:14:30,190
again is essentially notification but

00:14:28,780 --> 00:14:32,080
with the data but it has some of the

00:14:30,190 --> 00:14:33,730
same problems that notification does and

00:14:32,080 --> 00:14:35,080
I'll get to those in just a minute when

00:14:33,730 --> 00:14:37,210
I talk about all the negatives of all

00:14:35,080 --> 00:14:39,130
the different methods and the file one

00:14:37,210 --> 00:14:45,400
is a push and the one that everyone

00:14:39,130 --> 00:14:47,020
knows about four push is Twitter and how

00:14:45,400 --> 00:14:49,300
many of us processed Twitter or half

00:14:47,020 --> 00:14:51,880
process Twitter in the past there's a

00:14:49,300 --> 00:14:53,770
few do any of you have are you just

00:14:51,880 --> 00:14:55,810
doing the public feed or a ten percent

00:14:53,770 --> 00:15:02,080
feed from Sega nip or the actual full

00:14:55,810 --> 00:15:03,790
fire hose anyone are you just pulling it

00:15:02,080 --> 00:15:05,770
and asking for I need all the stuff that

00:15:03,790 --> 00:15:10,420
subscribe to stuff subscribe to

00:15:05,770 --> 00:15:12,700
different people all right so this is

00:15:10,420 --> 00:15:14,080
the big one push Twitter's twitter is

00:15:12,700 --> 00:15:16,180
probably the one that almost everybody

00:15:14,080 --> 00:15:18,190
knows about this one has different

00:15:16,180 --> 00:15:20,710
problems than the other two

00:15:18,190 --> 00:15:22,960
or the other three actually and I've

00:15:20,710 --> 00:15:25,960
only actually seen it in extremely high

00:15:22,960 --> 00:15:28,480
volume situations where it may not it

00:15:25,960 --> 00:15:30,910
may just not be appropriate for anyone

00:15:28,480 --> 00:15:32,920
to actually store the data anywhere or

00:15:30,910 --> 00:15:35,800
keep track of a position in a stream or

00:15:32,920 --> 00:15:38,590
anything along those lines or actually

00:15:35,800 --> 00:15:40,600
Netflix that's essentially a push stream

00:15:38,590 --> 00:15:42,720
too if you were going to process frame

00:15:40,600 --> 00:15:45,880
data from a movie you can consider

00:15:42,720 --> 00:15:48,490
Netflix or Amazon or any other streaming

00:15:45,880 --> 00:15:52,480
system would be would be a would be a

00:15:48,490 --> 00:15:54,520
push system so in a push system you

00:15:52,480 --> 00:15:56,620
basically hey open up a socket probably

00:15:54,520 --> 00:16:02,640
authenticated in some manner and then

00:15:56,620 --> 00:16:02,640
the data just gets thrown at you so

00:16:03,600 --> 00:16:08,350
there's a couple of we'll start with

00:16:06,010 --> 00:16:09,640
some of the downsides so we've got these

00:16:08,350 --> 00:16:11,260
four different methods of getting the

00:16:09,640 --> 00:16:13,150
data and what are the good things about

00:16:11,260 --> 00:16:17,890
each one and what are the bad things

00:16:13,150 --> 00:16:19,750
about each one so pol has the bad effect

00:16:17,890 --> 00:16:22,990
of you have to manage all of your own

00:16:19,750 --> 00:16:24,940
state and so you need to know where you

00:16:22,990 --> 00:16:27,520
are and you need to know what you've

00:16:24,940 --> 00:16:31,420
retrieved before and of things along

00:16:27,520 --> 00:16:34,270
those lines notifying payload have a

00:16:31,420 --> 00:16:36,700
different problem which is where in

00:16:34,270 --> 00:16:38,380
polling your control of everything so

00:16:36,700 --> 00:16:40,690
you as the client are be able to talk to

00:16:38,380 --> 00:16:42,550
someone and say hey I need some more

00:16:40,690 --> 00:16:45,910
data let me here's where I was last time

00:16:42,550 --> 00:16:48,070
or just give me the index actually a

00:16:45,910 --> 00:16:50,110
really nice one with polling how many

00:16:48,070 --> 00:16:53,380
people did a lot of old ftp work back in

00:16:50,110 --> 00:16:56,260
the day anyone use ftp anymore well do

00:16:53,380 --> 00:16:58,690
you remember in the old archives you

00:16:56,260 --> 00:17:01,120
telnet are you ftp in and then one of

00:16:58,690 --> 00:17:03,940
the top files was LS minus L R was the

00:17:01,120 --> 00:17:06,280
actual file and it was a list of every

00:17:03,940 --> 00:17:07,660
file in the ftp directory so you didn't

00:17:06,280 --> 00:17:10,209
actually have to list it you could just

00:17:07,660 --> 00:17:11,800
get the LS file and then you would know

00:17:10,209 --> 00:17:13,089
everything that was there so that's

00:17:11,800 --> 00:17:14,319
actually a really nice thing with Poland

00:17:13,089 --> 00:17:17,110
you can do the same thing I've seen it a

00:17:14,319 --> 00:17:18,610
lot the you know if you're if you want

00:17:17,110 --> 00:17:22,030
to go really simplistic with a polling

00:17:18,610 --> 00:17:23,410
mechanism just put all the files in the

00:17:22,030 --> 00:17:26,740
directory and let it pass your engine

00:17:23,410 --> 00:17:28,150
next to an automatic index index page so

00:17:26,740 --> 00:17:30,300
you know there's the list of all the

00:17:28,150 --> 00:17:32,370
files that you need to get

00:17:30,300 --> 00:17:33,570
and you know you just get the index you

00:17:32,370 --> 00:17:35,070
look at it was like oh I have all of

00:17:33,570 --> 00:17:37,560
these already and I'll only just get the

00:17:35,070 --> 00:17:39,180
two or three that I'm missing so polling

00:17:37,560 --> 00:17:42,150
is really good in terms of it's about

00:17:39,180 --> 00:17:43,860
the most simplistic one you can do both

00:17:42,150 --> 00:17:45,120
on the provider side so how many of your

00:17:43,860 --> 00:17:48,270
actually data providers that provide

00:17:45,120 --> 00:17:51,690
data to other people you've got a few

00:17:48,270 --> 00:17:59,250
all right so which method do you use to

00:17:51,690 --> 00:18:02,700
deliver your data to your customers yeah

00:17:59,250 --> 00:18:04,830
yeah and yeah that's the thing that is

00:18:02,700 --> 00:18:07,530
the beauty of about about of polling

00:18:04,830 --> 00:18:09,480
it's really easy to implement almost on

00:18:07,530 --> 00:18:11,310
both sides the amount of the amount of

00:18:09,480 --> 00:18:13,320
work you have to do on both sides is

00:18:11,310 --> 00:18:15,720
simplistic and it's also extremely

00:18:13,320 --> 00:18:18,290
asynchronous because as the client I

00:18:15,720 --> 00:18:20,430
will just go get it when I have time or

00:18:18,290 --> 00:18:21,840
but if you are in a situation where

00:18:20,430 --> 00:18:25,200
you've got data updates that are may

00:18:21,840 --> 00:18:26,640
have a more rep rapid requirement it may

00:18:25,200 --> 00:18:27,870
not work the best based on just the

00:18:26,640 --> 00:18:29,040
polling you're like okay do I have more

00:18:27,870 --> 00:18:32,040
data do I have more data do I have more

00:18:29,040 --> 00:18:34,170
data but polling generally seems to be

00:18:32,040 --> 00:18:36,300
the most simplistic and it's the easiest

00:18:34,170 --> 00:18:40,110
for everyone to understand there's the

00:18:36,300 --> 00:18:42,330
fewest moving parts but it is the it

00:18:40,110 --> 00:18:44,910
does have the negative side of saying I

00:18:42,330 --> 00:18:46,470
have to always ask is there more data is

00:18:44,910 --> 00:18:48,900
there more data is the more data and

00:18:46,470 --> 00:18:53,000
then you have to deal with how long does

00:18:48,900 --> 00:18:53,000
the data provider keep their data around

00:18:56,420 --> 00:19:02,250
now notice I and payload are nice and

00:19:00,630 --> 00:19:04,500
the fact you don't have to continually

00:19:02,250 --> 00:19:06,630
ask for more data you're just told if

00:19:04,500 --> 00:19:08,430
there's more data now the downside of

00:19:06,630 --> 00:19:10,170
having to be told when there's more data

00:19:08,430 --> 00:19:12,900
is that you have to be there and

00:19:10,170 --> 00:19:14,820
listening waiting for that message so if

00:19:12,900 --> 00:19:19,410
you're not there to receive the message

00:19:14,820 --> 00:19:22,170
what do you do so both with notifying

00:19:19,410 --> 00:19:23,850
payload you have to you actually have to

00:19:22,170 --> 00:19:26,670
have a persistent set up somewhere

00:19:23,850 --> 00:19:27,900
that's always available so your side of

00:19:26,670 --> 00:19:29,580
the relationship between the data

00:19:27,900 --> 00:19:31,200
provider and you is the client or if

00:19:29,580 --> 00:19:32,850
you're the data provider you have to

00:19:31,200 --> 00:19:34,080
keep a list of every single person you

00:19:32,850 --> 00:19:36,720
need to contact and what their

00:19:34,080 --> 00:19:38,910
subscriptions are so on both sides

00:19:36,720 --> 00:19:41,490
there's more overhead and it depends on

00:19:38,910 --> 00:19:42,750
whether that overhead is is worth it in

00:19:41,490 --> 00:19:45,900
terms of

00:19:42,750 --> 00:19:49,820
the system and the the contract that you

00:19:45,900 --> 00:19:49,820
and that the two parties involved have

00:19:50,270 --> 00:19:55,620
notify web hooks is a really good one

00:19:52,980 --> 00:19:57,420
from the Ruby gems and then payload

00:19:55,620 --> 00:19:58,800
actually the only one I haven't really

00:19:57,420 --> 00:20:01,230
dealt with a lot of ones that actually

00:19:58,800 --> 00:20:03,150
have all of the data in it and from my

00:20:01,230 --> 00:20:04,590
perspective email is actually probably

00:20:03,150 --> 00:20:06,390
the biggest one that's here if you're

00:20:04,590 --> 00:20:09,510
receiving stuff via email and there are

00:20:06,390 --> 00:20:11,100
people that email hey let me tell you it

00:20:09,510 --> 00:20:13,320
knows how to get data from point A to

00:20:11,100 --> 00:20:15,650
point B and it retries for up to a week

00:20:13,320 --> 00:20:18,750
you know all this kind of stuff so don't

00:20:15,650 --> 00:20:20,640
discount email in terms of a really good

00:20:18,750 --> 00:20:23,610
persistent way of sending data to

00:20:20,640 --> 00:20:25,380
different people because it works it's a

00:20:23,610 --> 00:20:29,490
proven technology it's been around a

00:20:25,380 --> 00:20:30,810
very long time so it's it's and there's

00:20:29,490 --> 00:20:32,700
really good servers the deal with

00:20:30,810 --> 00:20:36,180
managing data and giving it from a

00:20:32,700 --> 00:20:38,100
published subscribed perspective notify

00:20:36,180 --> 00:20:40,140
seems to be one that a lot of people use

00:20:38,100 --> 00:20:42,030
pubsubhubbub of web hooks I think even

00:20:40,140 --> 00:20:44,040
couchdb has a built-in sort of

00:20:42,030 --> 00:20:46,680
subscription that's that's based on

00:20:44,040 --> 00:20:50,820
based on a web book thing but don't

00:20:46,680 --> 00:20:52,770
quote me on that and then push so push

00:20:50,820 --> 00:20:58,110
is probably at least from my perspective

00:20:52,770 --> 00:20:59,430
the most brittle because you as the

00:20:58,110 --> 00:21:01,200
client are just going to connect to a

00:20:59,430 --> 00:21:03,750
data stream and you're just going to

00:21:01,200 --> 00:21:06,210
receive data you basically that's it and

00:21:03,750 --> 00:21:07,950
then you have to deal with it most of

00:21:06,210 --> 00:21:09,630
the time like for Twitter it's extremely

00:21:07,950 --> 00:21:12,150
high volume so you have to be able to

00:21:09,630 --> 00:21:13,560
deal with the data volumes they're going

00:21:12,150 --> 00:21:16,650
to happen if I think it was about a year

00:21:13,560 --> 00:21:18,720
and a half ago the Twitter data volume

00:21:16,650 --> 00:21:22,050
if you've got the fire hose it was a

00:21:18,720 --> 00:21:23,580
saturated oc3 line so how many of you

00:21:22,050 --> 00:21:26,490
have that you can network connectivity

00:21:23,580 --> 00:21:28,890
into your systems you do all right we've

00:21:26,490 --> 00:21:32,060
got one over here all right you all yes

00:21:28,890 --> 00:21:34,470
you do Rackspace okay we're good

00:21:32,060 --> 00:21:36,660
Rackspace has enough data is enough

00:21:34,470 --> 00:21:41,310
bandwidth to actually acquire the entire

00:21:36,660 --> 00:21:43,500
Twitter fire hose so I'm not push I

00:21:41,310 --> 00:21:47,970
think is is when you when you get to a

00:21:43,500 --> 00:21:50,160
point where the the data volumes are so

00:21:47,970 --> 00:21:51,780
high that it's actually easier to do

00:21:50,160 --> 00:21:55,260
this than to keep track of the other

00:21:51,780 --> 00:21:56,500
stuff and specifically in a push if

00:21:55,260 --> 00:21:58,330
you're not connected

00:21:56,500 --> 00:22:01,780
to the data stream you're not getting

00:21:58,330 --> 00:22:04,600
any data period so specifically for say

00:22:01,780 --> 00:22:06,580
twitter unless you're paying for gipps

00:22:04,600 --> 00:22:11,110
archival access to all of the old

00:22:06,580 --> 00:22:13,420
twitter then if you miss data it's gone

00:22:11,110 --> 00:22:15,400
and you will never ever ever get it

00:22:13,420 --> 00:22:17,890
again you know that's one of the

00:22:15,400 --> 00:22:21,130
downfalls of a push system so from my

00:22:17,890 --> 00:22:23,320
perspective I more on the I'm I like to

00:22:21,130 --> 00:22:25,060
be assured of my data then I need to

00:22:23,320 --> 00:22:26,500
know and I know that I have all of the

00:22:25,060 --> 00:22:30,010
data or I know that all of my customers

00:22:26,500 --> 00:22:33,820
have the data that they're paying for so

00:22:30,010 --> 00:22:38,680
I actually have an ideal way of doing it

00:22:33,820 --> 00:22:40,000
and it's a combination I sort of like

00:22:38,680 --> 00:22:41,440
the built-in pulling mechanism and I

00:22:40,000 --> 00:22:43,840
actually think rubygems does a really

00:22:41,440 --> 00:22:45,730
good job of doing this because they have

00:22:43,840 --> 00:22:48,040
the web hook method where you can

00:22:45,730 --> 00:22:51,310
subscribe to a certain subset of gems or

00:22:48,040 --> 00:22:52,810
all of the gems as a whole so you just

00:22:51,310 --> 00:22:56,320
get notified when every gem is published

00:22:52,810 --> 00:23:00,130
but because we actually use Ruby gems to

00:22:56,320 --> 00:23:02,230
install stuff rubygems by default

00:23:00,130 --> 00:23:04,060
essentially has a top-level you call it

00:23:02,230 --> 00:23:05,500
the anyone looked at the the Marshall

00:23:04,060 --> 00:23:08,170
file that holds the list of every single

00:23:05,500 --> 00:23:09,520
gems in Ruby gems you know it gets built

00:23:08,170 --> 00:23:11,980
every single time and ruby gem is

00:23:09,520 --> 00:23:14,080
published it's great and it actually has

00:23:11,980 --> 00:23:16,060
the list of every single ruby gem in the

00:23:14,080 --> 00:23:17,470
entire system so you have the list of

00:23:16,060 --> 00:23:19,300
everything so not only do you have the

00:23:17,470 --> 00:23:21,850
benefits of web hook where you don't

00:23:19,300 --> 00:23:24,280
have to pull all the time but you have a

00:23:21,850 --> 00:23:26,800
backup mechanism to get all of the data

00:23:24,280 --> 00:23:29,020
say your web hook your web hook listener

00:23:26,800 --> 00:23:31,690
was down for a week you missed a week's

00:23:29,020 --> 00:23:33,310
worth of data or just a day well you can

00:23:31,690 --> 00:23:36,610
say oh well I missed all this let me

00:23:33,310 --> 00:23:38,830
just go get the top level index it will

00:23:36,610 --> 00:23:40,290
list all of the gems I can ingest it I

00:23:38,830 --> 00:23:43,150
can go through it and say well I've got

00:23:40,290 --> 00:23:44,820
these you know eighty thousand I'm only

00:23:43,150 --> 00:23:47,140
missing the four thousand from yesterday

00:23:44,820 --> 00:23:49,000
or two thousand or whatever the rate

00:23:47,140 --> 00:23:51,220
these days is it's it's getting pretty

00:23:49,000 --> 00:23:54,570
fast and those are we're publishing a

00:23:51,220 --> 00:23:57,670
lot of gems these days so for my ideal

00:23:54,570 --> 00:23:59,110
either a web hook mechanism that gives

00:23:57,670 --> 00:24:02,620
you a notification but with a fallback

00:23:59,110 --> 00:24:05,860
of being able to say of polling or

00:24:02,620 --> 00:24:07,570
something along that use etag that

00:24:05,860 --> 00:24:09,910
allows you to say okay give me all the

00:24:07,570 --> 00:24:10,240
data since this amount of time but the

00:24:09,910 --> 00:24:13,360
up

00:24:10,240 --> 00:24:16,330
provider needs to be able to to provide

00:24:13,360 --> 00:24:18,370
a mechanism for any depth so it's

00:24:16,330 --> 00:24:19,540
basically hey I need all the data that

00:24:18,370 --> 00:24:22,630
you would have given me since last

00:24:19,540 --> 00:24:25,840
Tuesday so that's another those are the

00:24:22,630 --> 00:24:27,640
two biggest ones for me and if you don't

00:24:25,840 --> 00:24:30,550
have a callback mechanism I'm happy with

00:24:27,640 --> 00:24:33,580
straight polling because it's so simple

00:24:30,550 --> 00:24:35,350
that everyone can do it everybody

00:24:33,580 --> 00:24:36,550
receipt everybody is more comfortable in

00:24:35,350 --> 00:24:38,470
a polling situation at least from my

00:24:36,550 --> 00:24:39,640
perspective everyone other people may

00:24:38,470 --> 00:24:42,120
have a different opinion but from my

00:24:39,640 --> 00:24:44,920
perspective polling works out the best

00:24:42,120 --> 00:24:46,900
just from both the data provider and the

00:24:44,920 --> 00:24:48,910
data consumers because it's just as most

00:24:46,900 --> 00:24:51,820
simplest mechanism possible and I'm a

00:24:48,910 --> 00:24:55,330
big fan of the simplest way that works

00:24:51,820 --> 00:24:58,870
so i use cron and maildir to get ship

00:24:55,330 --> 00:25:00,760
get stuff done and you know it works out

00:24:58,870 --> 00:25:02,110
great or actually probably going to do a

00:25:00,760 --> 00:25:03,970
lot with systemd in the future because

00:25:02,110 --> 00:25:06,790
it started sort of the has anyone done a

00:25:03,970 --> 00:25:09,520
lot of work with systemd linux 9 okay I

00:25:06,790 --> 00:25:12,040
might want to chat with you guys later a

00:25:09,520 --> 00:25:13,480
lot of people do like the push but again

00:25:12,040 --> 00:25:15,400
i'm not as fan because i think it's more

00:25:13,480 --> 00:25:17,800
brittle but i do believe it's it's there

00:25:15,400 --> 00:25:19,720
it's needed it's there as a as a good

00:25:17,800 --> 00:25:23,110
when you just reach a certain data

00:25:19,720 --> 00:25:24,790
volume level so when I was working on

00:25:23,110 --> 00:25:25,990
this talk I thought there was actually

00:25:24,790 --> 00:25:27,370
going to be more code involved and I

00:25:25,990 --> 00:25:28,690
realized that the fundamental concepts

00:25:27,370 --> 00:25:30,490
that we're talking about might be a

00:25:28,690 --> 00:25:31,840
better introduction of where everybody's

00:25:30,490 --> 00:25:35,290
going and keeping your head above water

00:25:31,840 --> 00:25:37,390
by understanding the ecosystem so the I

00:25:35,290 --> 00:25:39,880
was trying to deal with a public data

00:25:37,390 --> 00:25:43,059
stream that wasn't Twitter I've dealt

00:25:39,880 --> 00:25:44,860
with Twitter too much I'm not i think

00:25:43,059 --> 00:25:46,570
it's it's an edge case in terms of data

00:25:44,860 --> 00:25:48,790
streams I mean everybody wants it but in

00:25:46,570 --> 00:25:50,860
terms of the possibilities of data

00:25:48,790 --> 00:25:54,240
acquisition from other locations it's an

00:25:50,860 --> 00:25:56,410
outlier in terms of how you get the data

00:25:54,240 --> 00:26:00,250
so i was looking for a good data source

00:25:56,410 --> 00:26:03,070
is win here somewhere i don't see his

00:26:00,250 --> 00:26:05,140
hand so github events github the new

00:26:03,070 --> 00:26:08,470
github version 3 API has an event stream

00:26:05,140 --> 00:26:10,900
if you just go to api github com /

00:26:08,470 --> 00:26:13,840
events you're going to get basically a

00:26:10,900 --> 00:26:16,210
JSON dump of the last 30 events that

00:26:13,840 --> 00:26:17,740
happened to github and then it's got

00:26:16,210 --> 00:26:19,000
this really nice links header for you

00:26:17,740 --> 00:26:21,730
giving you the next in the previous and

00:26:19,000 --> 00:26:23,920
you go up to 10 pages back but it does a

00:26:21,730 --> 00:26:25,480
nice etag mechanisms

00:26:23,920 --> 00:26:26,680
you can say okay give me the events

00:26:25,480 --> 00:26:29,470
it'll send you an e-tag head are you

00:26:26,680 --> 00:26:31,390
doing if modified and it'll give you the

00:26:29,470 --> 00:26:32,980
next set and then you can page backwards

00:26:31,390 --> 00:26:36,340
in time and get the data that you've

00:26:32,980 --> 00:26:38,290
missed now the bad part is it is a

00:26:36,340 --> 00:26:39,730
pulling mechanism but their events are

00:26:38,290 --> 00:26:41,620
getting published you know one a second

00:26:39,730 --> 00:26:42,670
or something like that so polling is

00:26:41,620 --> 00:26:44,500
going to work you're pretty much always

00:26:42,670 --> 00:26:45,610
guaranteed to get data but there are

00:26:44,500 --> 00:26:47,320
some interesting stuff when you're

00:26:45,610 --> 00:26:50,980
pulling different people you have to

00:26:47,320 --> 00:26:55,090
deal with their or use the data provider

00:26:50,980 --> 00:26:57,610
you are giving out your terms of service

00:26:55,090 --> 00:26:59,350
so for github for instance they have a

00:26:57,610 --> 00:27:00,970
couple headers I think it's X poll

00:26:59,350 --> 00:27:02,530
interval which says how often you're

00:27:00,970 --> 00:27:04,440
allowed to pull me and asked if there's

00:27:02,530 --> 00:27:06,880
new data and it's 60 seconds right now

00:27:04,440 --> 00:27:08,680
and then the other one is sort of a rate

00:27:06,880 --> 00:27:11,080
limit from your API perspective like how

00:27:08,680 --> 00:27:13,720
many requests of the API you allowed to

00:27:11,080 --> 00:27:16,150
make per hour so you as a data provider

00:27:13,720 --> 00:27:17,920
or you as the data consumer need to work

00:27:16,150 --> 00:27:19,600
around and make sure that everyone can

00:27:17,920 --> 00:27:25,300
get the data they need within the

00:27:19,600 --> 00:27:27,670
constraints provided by your API and

00:27:25,300 --> 00:27:30,160
github actually provides an interesting

00:27:27,670 --> 00:27:34,210
backup mechanism so for me I always like

00:27:30,160 --> 00:27:35,680
to be able to pull and then if I'm

00:27:34,210 --> 00:27:37,510
missing or I'm not there to pull

00:27:35,680 --> 00:27:39,400
something's down I like a backup

00:27:37,510 --> 00:27:41,140
mechanism to get the data well it's not

00:27:39,400 --> 00:27:46,510
me but has anyone looked at github

00:27:41,140 --> 00:27:49,540
archive yet so Ilya gregorich he has

00:27:46,510 --> 00:27:52,090
been scraping github github as another

00:27:49,540 --> 00:27:54,100
endpoint called timeline JSON which

00:27:52,090 --> 00:27:56,620
gives you the events but in a slightly

00:27:54,100 --> 00:27:58,390
different robustness there's more data

00:27:56,620 --> 00:28:01,180
in those than there is in the standard

00:27:58,390 --> 00:28:02,830
events API on the it should be the same

00:28:01,180 --> 00:28:06,940
events but there's a little bit more

00:28:02,830 --> 00:28:09,520
salt call its ID card data is there but

00:28:06,940 --> 00:28:12,010
he's been scraping it since a februari

00:28:09,520 --> 00:28:14,110
of 2011 I believe and so all of its

00:28:12,010 --> 00:28:17,650
available online and he's got it in big

00:28:14,110 --> 00:28:20,230
tar balls of JSON gzip files one per

00:28:17,650 --> 00:28:21,460
hour so if I'm and he's pulling

00:28:20,230 --> 00:28:24,970
separately on a different network

00:28:21,460 --> 00:28:28,150
actually at Google I think so when I

00:28:24,970 --> 00:28:29,590
pull for the github events I I do have

00:28:28,150 --> 00:28:32,080
the knowledge that hey the github

00:28:29,590 --> 00:28:36,220
archive is also scraping a different

00:28:32,080 --> 00:28:37,750
part of github and getting the same data

00:28:36,220 --> 00:28:39,340
so if I fail then

00:28:37,750 --> 00:28:43,090
I could fall back and go get his data

00:28:39,340 --> 00:28:46,060
and fill up the holes that I have which

00:28:43,090 --> 00:28:47,380
is pretty cool so in this case I like

00:28:46,060 --> 00:28:49,570
the mechanism be able to get the same

00:28:47,380 --> 00:28:51,100
this provides an example of basically

00:28:49,570 --> 00:28:53,670
being able to get the data in multiple

00:28:51,100 --> 00:28:55,750
ways and I think that's really important

00:28:53,670 --> 00:28:58,450
specifically for a data consistency

00:28:55,750 --> 00:29:01,630
perspective and the ability oh there's

00:28:58,450 --> 00:29:04,480
kid of archive and the ability to say I

00:29:01,630 --> 00:29:08,590
know I've received all the data that was

00:29:04,480 --> 00:29:11,440
actually available so we've completed

00:29:08,590 --> 00:29:12,760
step one get the data so we're not going

00:29:11,440 --> 00:29:15,220
to go through all of this that and the

00:29:12,760 --> 00:29:16,510
other maybe I'll do some more talks and

00:29:15,220 --> 00:29:18,310
develop a little series or something

00:29:16,510 --> 00:29:20,950
like that but we're just going to go

00:29:18,310 --> 00:29:26,100
over the first two so now we have this

00:29:20,950 --> 00:29:26,100
data and now we need to store this data

00:29:27,510 --> 00:29:30,880
one of the things that a lot of people

00:29:29,290 --> 00:29:32,830
are interested in is ok how much

00:29:30,880 --> 00:29:37,710
processing should i do on the data

00:29:32,830 --> 00:29:37,710
before I actually put it on disk and

00:29:38,370 --> 00:29:43,000
there's there's a data quality level

00:29:40,660 --> 00:29:45,610
where you say I I want to introduce the

00:29:43,000 --> 00:29:47,950
least amount of change possible because

00:29:45,610 --> 00:29:51,700
I might screw something up that I don't

00:29:47,950 --> 00:29:53,470
know about so from my perspective and a

00:29:51,700 --> 00:29:55,540
few other folks and actually the the big

00:29:53,470 --> 00:29:59,290
data book also mentions this to you want

00:29:55,540 --> 00:30:01,390
to do as little data processing of the

00:29:59,290 --> 00:30:03,280
original input stream as possible you

00:30:01,390 --> 00:30:04,510
may have the cases once you have the

00:30:03,280 --> 00:30:07,440
data you can do whatever you need to

00:30:04,510 --> 00:30:10,120
with it but making sure you have it is

00:30:07,440 --> 00:30:12,610
very important because if you don't have

00:30:10,120 --> 00:30:14,200
it you can't use it so making sure you

00:30:12,610 --> 00:30:16,840
have it and it's on disk and you can

00:30:14,200 --> 00:30:18,640
access it at will is probably one of the

00:30:16,840 --> 00:30:23,740
most important pieces of this entire

00:30:18,640 --> 00:30:26,400
system or any any type of data so I'm

00:30:23,740 --> 00:30:29,050
playing with the github event API and

00:30:26,400 --> 00:30:32,020
the data you get is just a JSON stream

00:30:29,050 --> 00:30:34,450
or a JSON chunk of array of events so

00:30:32,020 --> 00:30:36,520
all I'm doing in it is splitting the

00:30:34,450 --> 00:30:39,250
event splitting that big array up into

00:30:36,520 --> 00:30:42,430
individual JSON hashes and then sending

00:30:39,250 --> 00:30:43,600
it to the thing that the piece of my

00:30:42,430 --> 00:30:47,020
program that's actually going to store

00:30:43,600 --> 00:30:48,460
it to disk so in this way I'm doing the

00:30:47,020 --> 00:30:50,170
minimal amount of processing because if

00:30:48,460 --> 00:30:51,340
I actually did some amount of processing

00:30:50,170 --> 00:30:53,230
with it the

00:30:51,340 --> 00:30:55,600
and I don't save as much of the original

00:30:53,230 --> 00:30:58,060
as possible I've changed something and

00:30:55,600 --> 00:31:01,330
this is a case where you sort of can't

00:30:58,060 --> 00:31:05,040
roll back unless you go and retrieve the

00:31:01,330 --> 00:31:09,190
data again and that may not be available

00:31:05,040 --> 00:31:13,990
so where do you store it I I consider

00:31:09,190 --> 00:31:18,370
this this is the raw fundamental truth

00:31:13,990 --> 00:31:21,760
data of your system so making sure it's

00:31:18,370 --> 00:31:23,920
available all the time and and backed up

00:31:21,760 --> 00:31:26,500
and all of the wonderful things we need

00:31:23,920 --> 00:31:28,840
to do I don't really care where it

00:31:26,500 --> 00:31:32,520
happens maybe you're using you've got a

00:31:28,840 --> 00:31:35,220
anybody using that apps I salon Hadoop

00:31:32,520 --> 00:31:37,060
anyone on any of those things yeah

00:31:35,220 --> 00:31:39,400
anywhere that where you essentially have

00:31:37,060 --> 00:31:41,590
redundant fault tolerant storage that's

00:31:39,400 --> 00:31:43,300
the big piece that I think is really

00:31:41,590 --> 00:31:45,910
important when you get this initial data

00:31:43,300 --> 00:31:49,750
stream just making sure it's completely

00:31:45,910 --> 00:31:53,680
available and is available for later

00:31:49,750 --> 00:31:55,870
processing and Hadoop is probably one of

00:31:53,680 --> 00:31:59,230
the well of the three I mentioned it's

00:31:55,870 --> 00:32:00,670
probably the cheapest by far and it

00:31:59,230 --> 00:32:03,190
provides you fault tolerant redundant

00:32:00,670 --> 00:32:05,350
file system that's distributed and can

00:32:03,190 --> 00:32:08,530
grow linearly with the amount of data

00:32:05,350 --> 00:32:10,150
you're going to acquire and it's also

00:32:08,530 --> 00:32:14,860
amenable for later processing if you're

00:32:10,150 --> 00:32:18,220
going to do HBase just straight-up

00:32:14,860 --> 00:32:19,360
MapReduce for you just we caught the

00:32:18,220 --> 00:32:21,910
other one you know something like Pig

00:32:19,360 --> 00:32:24,430
Casca log any of these other things that

00:32:21,910 --> 00:32:26,530
are wrappers around MapReduce they're

00:32:24,430 --> 00:32:28,390
all going to work with this and so just

00:32:26,530 --> 00:32:31,060
being able to store your data as a the

00:32:28,390 --> 00:32:34,960
raw data as it's coming in into a nice

00:32:31,060 --> 00:32:36,940
system is a good starting point for for

00:32:34,960 --> 00:32:38,500
later processing and specifically if you

00:32:36,940 --> 00:32:40,150
want an actual file format to store

00:32:38,500 --> 00:32:42,910
stuff in I'm a fan of Avro how many

00:32:40,150 --> 00:32:44,380
people have heard of Avro all right how

00:32:42,910 --> 00:32:48,790
many people have heard of message pack

00:32:44,380 --> 00:32:52,000
oh yeah thrift okay these are all other

00:32:48,790 --> 00:32:54,130
sort of serialization mechanisms afro is

00:32:52,000 --> 00:32:56,320
another one I actually particularly like

00:32:54,130 --> 00:32:57,460
Avro file formats because of all the

00:32:56,320 --> 00:32:59,860
ones that are these serialization

00:32:57,460 --> 00:33:01,750
mechanisms I think it's the only one

00:32:59,860 --> 00:33:03,190
that actually has a file standard all

00:33:01,750 --> 00:33:05,320
the others are if you want just call it

00:33:03,190 --> 00:33:06,909
records levels so the Avro files I

00:33:05,320 --> 00:33:08,860
we have built the schema is built into

00:33:06,909 --> 00:33:10,240
the file system or built into the file

00:33:08,860 --> 00:33:12,490
so you actually don't need like the

00:33:10,240 --> 00:33:15,039
special headers that say you know record

00:33:12,490 --> 00:33:18,100
a is this record B is this and it also

00:33:15,039 --> 00:33:21,340
works very well with MapReduce and it

00:33:18,100 --> 00:33:22,899
also has built-in small sub chunks in

00:33:21,340 --> 00:33:24,610
the file for check something so if you

00:33:22,899 --> 00:33:27,100
do happen to have a catastrophic

00:33:24,610 --> 00:33:31,299
eruption of certain amounts of your

00:33:27,100 --> 00:33:33,159
system avro files are correcting enough

00:33:31,299 --> 00:33:34,659
so that you you only lose the piece

00:33:33,159 --> 00:33:35,830
that's corrupted you don't say lose

00:33:34,659 --> 00:33:37,269
everything after it you know its

00:33:35,830 --> 00:33:38,620
standard text file if you have a

00:33:37,269 --> 00:33:39,940
corruption at one point you basically

00:33:38,620 --> 00:33:42,460
lost everything from that point to the

00:33:39,940 --> 00:33:44,919
end of the file with Avro if a file gets

00:33:42,460 --> 00:33:48,970
corrupted some gets bit some gets some

00:33:44,919 --> 00:33:50,830
bits get screwed up you can basically

00:33:48,970 --> 00:33:52,690
isolate it to a chunk of the file and

00:33:50,830 --> 00:33:55,389
skip it and move on so I think it's a

00:33:52,690 --> 00:33:57,789
really good format in general for doing

00:33:55,389 --> 00:34:01,929
any sort of record level type of record

00:33:57,789 --> 00:34:03,610
type of stuff so why do we go to all

00:34:01,929 --> 00:34:07,990
this trouble you know we're just getting

00:34:03,610 --> 00:34:09,700
the data and putting it on disk and we

00:34:07,990 --> 00:34:11,470
really haven't even looked at it yet we

00:34:09,700 --> 00:34:13,300
haven't done any analysis on it yet we

00:34:11,470 --> 00:34:14,919
haven't the while we're doing this we're

00:34:13,300 --> 00:34:16,510
probably getting counts we're seeing how

00:34:14,919 --> 00:34:17,829
fast we're getting it and maybe bite

00:34:16,510 --> 00:34:19,980
sizes and different things like that

00:34:17,829 --> 00:34:22,389
because we need to know that data and

00:34:19,980 --> 00:34:25,000
it's because this data we're acquiring

00:34:22,389 --> 00:34:27,790
is the fundamental truth of the rest of

00:34:25,000 --> 00:34:29,909
the system any alteration we do have it

00:34:27,790 --> 00:34:35,020
before we actually start processing

00:34:29,909 --> 00:34:36,250
introduces either a flaw a mistake you

00:34:35,020 --> 00:34:39,220
might think you're doing the right thing

00:34:36,250 --> 00:34:41,619
but if you screw something up at this

00:34:39,220 --> 00:34:44,200
point you've affected the entire rest of

00:34:41,619 --> 00:34:47,730
the system so being able to have this

00:34:44,200 --> 00:34:50,260
raw upstream data archived for

00:34:47,730 --> 00:34:55,750
essentially as long as is necessary to

00:34:50,260 --> 00:34:56,980
keep it you can fall back to it so if

00:34:55,750 --> 00:34:58,720
you if you're taking some of this

00:34:56,980 --> 00:35:01,329
processing a little bit then throwing it

00:34:58,720 --> 00:35:03,010
away then you can never go back if that

00:35:01,329 --> 00:35:04,810
processing had a mistake in it and

00:35:03,010 --> 00:35:06,849
that's one of the biggest things you

00:35:04,810 --> 00:35:09,190
have to deal with in a big data or

00:35:06,849 --> 00:35:10,810
streaming system is human error we're

00:35:09,190 --> 00:35:12,160
going to make a mistake and I'll

00:35:10,810 --> 00:35:15,550
guarantee you're going to make a mistake

00:35:12,160 --> 00:35:17,140
two weeks later you realize oh my gosh

00:35:15,550 --> 00:35:18,880
that processing we did on two week old

00:35:17,140 --> 00:35:21,039
data yeah we were wrong

00:35:18,880 --> 00:35:23,259
we did the calculation wrong we need to

00:35:21,039 --> 00:35:25,359
redo them all so to be able to redo them

00:35:23,259 --> 00:35:31,180
all you have to roll back to where it

00:35:25,359 --> 00:35:33,369
came from and you don't know what you're

00:35:31,180 --> 00:35:35,950
going to find out next depending on the

00:35:33,369 --> 00:35:37,390
archive the the archive level of your

00:35:35,950 --> 00:35:40,059
data do you need to keep it for a year

00:35:37,390 --> 00:35:41,740
do you need to keep it for two years say

00:35:40,059 --> 00:35:43,390
you come up with a new algorithm that

00:35:41,740 --> 00:35:44,319
you're looking at on your data with and

00:35:43,390 --> 00:35:45,970
you realize you know what it would be

00:35:44,319 --> 00:35:47,619
really good to look at how this is

00:35:45,970 --> 00:35:50,079
progressed over time since the beginning

00:35:47,619 --> 00:35:52,029
of our data stream so going back to the

00:35:50,079 --> 00:35:53,259
beginning and being able to process that

00:35:52,029 --> 00:35:55,690
algorithm again through the entire

00:35:53,259 --> 00:35:56,589
system allows for future discovery so

00:35:55,690 --> 00:35:58,660
you don't know what you're going to use

00:35:56,589 --> 00:36:01,210
it for them in the future and there's

00:35:58,660 --> 00:36:03,910
actually a good quote and I might have a

00:36:01,210 --> 00:36:05,680
and from the big data book is nearly

00:36:03,910 --> 00:36:09,160
every large data set has unanticipated

00:36:05,680 --> 00:36:11,259
value within it and ultimately you can't

00:36:09,160 --> 00:36:13,210
discover interesting things with your

00:36:11,259 --> 00:36:15,940
data unless you can ask arbitrary

00:36:13,210 --> 00:36:18,880
questions of it which i think is it

00:36:15,940 --> 00:36:20,769
makes it pretty it sounds pretty good

00:36:18,880 --> 00:36:22,869
and it's sort of from my perspective the

00:36:20,769 --> 00:36:24,490
follow-on conclusion that is you cannot

00:36:22,869 --> 00:36:30,250
ask interesting things of data that you

00:36:24,490 --> 00:36:32,829
don't have logical conclusion to me so

00:36:30,250 --> 00:36:34,720
paranoia this it sounds like I'm

00:36:32,829 --> 00:36:37,000
paranoid about this but I am because if

00:36:34,720 --> 00:36:38,470
you make a mistake here a lot of the

00:36:37,000 --> 00:36:41,230
times you can't recover from it unless

00:36:38,470 --> 00:36:43,589
the only way to recover from a mistake

00:36:41,230 --> 00:36:45,849
here is if you're upstream data-provider

00:36:43,589 --> 00:36:48,099
allows you to go get the data again

00:36:45,849 --> 00:36:50,410
which is why I always like to have that

00:36:48,099 --> 00:36:51,700
mechanism available because I'm going to

00:36:50,410 --> 00:36:53,109
make a mistake in here while I'm

00:36:51,700 --> 00:36:55,420
developing the system or something like

00:36:53,109 --> 00:36:57,009
that so being able to roll back to a

00:36:55,420 --> 00:36:59,049
known point but you're up stream data

00:36:57,009 --> 00:37:00,880
provider may only be keeping the data

00:36:59,049 --> 00:37:02,460
for one month so or two months or

00:37:00,880 --> 00:37:07,420
something like that so whatever that

00:37:02,460 --> 00:37:10,119
relationship is so if for my perspective

00:37:07,420 --> 00:37:14,200
the monitoring the development this is

00:37:10,119 --> 00:37:19,150
an area where it's paced off to be extra

00:37:14,200 --> 00:37:21,220
paranoid yeah so for instance processing

00:37:19,150 --> 00:37:22,750
the twitter feed i used to work at a

00:37:21,220 --> 00:37:24,339
company called collective intellect we

00:37:22,750 --> 00:37:28,990
processed a ten percent Twitter fire

00:37:24,339 --> 00:37:32,770
hose if our collector went down for a

00:37:28,990 --> 00:37:34,390
minute or two minutes that day is gone

00:37:32,770 --> 00:37:37,360
you're never going to get it again

00:37:34,390 --> 00:37:39,850
although it is Twitter so 99% of it is

00:37:37,360 --> 00:37:41,290
junk so it doesn't really matter so the

00:37:39,850 --> 00:37:43,180
probability that you're actually going

00:37:41,290 --> 00:37:46,020
to get you're going to miss that really

00:37:43,180 --> 00:37:47,680
important mega tweet is low but you

00:37:46,020 --> 00:37:50,080
sometimes your customers don't

00:37:47,680 --> 00:37:54,010
understand that or say your collector

00:37:50,080 --> 00:37:55,720
was down for an hour something happened

00:37:54,010 --> 00:37:58,600
you you lost your primary and failover

00:37:55,720 --> 00:38:01,000
the data your your your data center had

00:37:58,600 --> 00:38:02,710
an issue and there was a fire you know

00:38:01,000 --> 00:38:04,690
sometimes these things happen I've had

00:38:02,710 --> 00:38:06,730
we had the fans go out in our data

00:38:04,690 --> 00:38:08,860
center once and all of the machines sent

00:38:06,730 --> 00:38:11,530
their cpu core temperature alerts often

00:38:08,860 --> 00:38:14,020
we had to crash the system so and while

00:38:11,530 --> 00:38:15,670
that was done we didn't collect data so

00:38:14,020 --> 00:38:18,100
when we brought up the system the next

00:38:15,670 --> 00:38:21,370
day after the fans were done the air

00:38:18,100 --> 00:38:24,100
conditioner was back on we lost

00:38:21,370 --> 00:38:25,510
essentially a period of data that we

00:38:24,100 --> 00:38:27,700
will never be able to get back again

00:38:25,510 --> 00:38:30,730
your customers are paying for analysis

00:38:27,700 --> 00:38:32,590
of data there's a whole they don't

00:38:30,730 --> 00:38:35,200
understand that they're like I paid for

00:38:32,590 --> 00:38:37,000
that so making sure that you're paranoid

00:38:35,200 --> 00:38:39,370
in this and having primary and secondary

00:38:37,000 --> 00:38:41,500
and tertiary acquisition methods for

00:38:39,370 --> 00:38:42,790
your data I think is one of the most

00:38:41,500 --> 00:38:44,410
fundamental things that's required for

00:38:42,790 --> 00:38:48,700
dealing with your streaming data or just

00:38:44,410 --> 00:38:50,020
any data in general I don't think we

00:38:48,700 --> 00:38:54,190
have time to look at the code but it's a

00:38:50,020 --> 00:38:58,630
uh it's on copious free time / ghent ghe

00:38:54,190 --> 00:39:00,100
NT on github and that's me I can take a

00:38:58,630 --> 00:39:03,280
couple quick questions or I have a quick

00:39:00,100 --> 00:39:05,260
bonus track if you want alright sounds

00:39:03,280 --> 00:39:08,590
like we want a bonus track I got a

00:39:05,260 --> 00:39:10,240
minute and a half so here we go so while

00:39:08,590 --> 00:39:13,660
i was doing this i was i was reading

00:39:10,240 --> 00:39:15,040
this big data book and a lot of the

00:39:13,660 --> 00:39:16,570
stuff that was happening in it and it's

00:39:15,040 --> 00:39:19,210
not even all the way written yet so this

00:39:16,570 --> 00:39:20,290
is just like the first third or half and

00:39:19,210 --> 00:39:21,880
I was like wait a second all these

00:39:20,290 --> 00:39:25,120
things sound really familiar where have

00:39:21,880 --> 00:39:27,640
I read this stuff before so this is sort

00:39:25,120 --> 00:39:29,110
of the what if everybody read XKCD you

00:39:27,640 --> 00:39:30,700
know the what if numbers you know he's

00:39:29,110 --> 00:39:31,570
got is what if session it's like these

00:39:30,700 --> 00:39:33,520
are the interesting things I've

00:39:31,570 --> 00:39:35,050
discovered along the lines as I've been

00:39:33,520 --> 00:39:38,440
working on these what if things this is

00:39:35,050 --> 00:39:42,460
my what if track so on the left we have

00:39:38,440 --> 00:39:44,890
the quote i just read from 2013 and on

00:39:42,460 --> 00:39:46,380
the right we have it's a bit more

00:39:44,890 --> 00:39:49,240
verbiage

00:39:46,380 --> 00:39:51,369
but to me the biggest one is listening

00:39:49,240 --> 00:39:53,530
right here the one on the far right but

00:39:51,369 --> 00:39:55,600
perhaps the largest benefit of the data

00:39:53,530 --> 00:39:57,210
warehouse foundation is that future

00:39:55,600 --> 00:39:59,860
unknown requirements can be accommodated

00:39:57,210 --> 00:40:05,410
wow that sounds a lot like this doesn't

00:39:59,860 --> 00:40:07,960
it and that's from 2002 or this is the

00:40:05,410 --> 00:40:09,640
term from a real-time egl refers to

00:40:07,960 --> 00:40:11,530
software that moves data asynchronously

00:40:09,640 --> 00:40:13,480
within a data warehouse with some

00:40:11,530 --> 00:40:15,820
urgency so you know data is coming

00:40:13,480 --> 00:40:17,710
actually pretty fast how this is called

00:40:15,820 --> 00:40:20,619
the data warehouse ETL tool kit from

00:40:17,710 --> 00:40:21,970
ralph kimball in 2004 so what is old is

00:40:20,619 --> 00:40:23,710
new again it's really interesting that

00:40:21,970 --> 00:40:26,410
basically what we're calling and this is

00:40:23,710 --> 00:40:27,790
my conjecture as though we're calling

00:40:26,410 --> 00:40:29,680
big data is actually we've been calling

00:40:27,790 --> 00:40:33,330
data warehousing an ETL for a long time

00:40:29,680 --> 00:40:33,330
so I'll leave you with that thought

00:40:33,480 --> 00:40:36,480
enjoy

00:40:47,400 --> 00:40:49,460

YouTube URL: https://www.youtube.com/watch?v=As1bB-vbD0s


