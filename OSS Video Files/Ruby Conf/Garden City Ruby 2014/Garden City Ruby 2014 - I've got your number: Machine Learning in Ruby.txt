Title: Garden City Ruby 2014 - I've got your number: Machine Learning in Ruby
Publication date: 2020-01-24
Playlist: Garden City Ruby 2014
Description: 
	By Arnab Deka

Would you like to do some OCR using Ruby and learn some machine-learning along the way?

In this talk, first up, we'll have a quick demo where an attendee writes down a digit on a sticky note and a Ruby program tries to to recognize it. Then we'll pick it apart, covering:

the basics of machine-learning (and different applications)
brief look into the Math behind supervised learning (classification)
(mostly) hand-rolled code applying this Math
libraries/tools available to Rubyists (and choice of Ruby platforms) including weka, mahout, libsvm, and the Ruby Matrix class :)
ways to dive deeper into ML
Captions: 
	00:00:25,310 --> 00:00:27,390
ARNAD DEKA: Hey guys.

00:00:27,390 --> 00:00:30,240
Good morning? Yeah. So I'll talk about machine learning.

00:00:30,240 --> 00:00:31,840
The promise and all that was just to get

00:00:31,840 --> 00:00:36,290
the CFE expected, so - accepted, so. Don't expect

00:00:36,290 --> 00:00:38,440
too much. Anyway, so I started with machine learning

00:00:38,440 --> 00:00:41,460
about six months to a year back, and this

00:00:41,460 --> 00:00:43,809
world like amazes me with how much stuff you

00:00:43,809 --> 00:00:45,989
can do and how awesome it is. So at

00:00:45,989 --> 00:00:47,410
the end of this talk I just want you

00:00:47,410 --> 00:00:51,309
to get excited about that too and go and

00:00:51,309 --> 00:00:55,510
try it out. You can read more about me

00:00:55,510 --> 00:00:57,790
there. Don't want to talk about that. The slides

00:00:57,790 --> 00:00:59,600
and the code that I'll show are up on

00:00:59,600 --> 00:01:02,790
GitHub and slides and all that. Yeah, so I

00:01:02,790 --> 00:01:05,290
was preparing this thing and then Dejas, yesterday -

00:01:05,290 --> 00:01:09,150
everybody knows Dejas here, right? He had a couple

00:01:09,150 --> 00:01:11,220
of jokes that were exactly what I had, and

00:01:11,220 --> 00:01:12,670
I was like oh god I have to prepare

00:01:12,670 --> 00:01:15,530
new jokes now. But at night I couldn't so,

00:01:15,530 --> 00:01:18,390
it was too late anyway. So I didn't, so

00:01:18,390 --> 00:01:21,140
my jokes are still lame, but still laugh like

00:01:21,140 --> 00:01:26,490
you did for his talk, all right. First of

00:01:26,490 --> 00:01:28,460
all, happy new year. Has anybody ever had two

00:01:28,460 --> 00:01:30,630
hundred people shout happy new year to you at

00:01:30,630 --> 00:01:33,289
the same time? No? Want to try that? All

00:01:33,289 --> 00:01:37,700
right, so, on a count of three, right. One,

00:01:37,700 --> 00:01:38,679
two, three.

00:01:38,679 --> 00:01:41,130
AUDIENCE & A.D.: Happy new year!

00:01:41,130 --> 00:01:44,740
A.D.: That was actually much better than what I expected. Cause

00:01:44,740 --> 00:01:47,799
I had two more steps after this. I'll just

00:01:47,799 --> 00:01:50,030
do one of them anyway. Let's try even better,

00:01:50,030 --> 00:01:51,899
right. Let the hotel know that we are doing

00:01:51,899 --> 00:01:55,689
something awesome in here. So shout happy new year

00:01:55,689 --> 00:01:57,539
and stomp your feet. Don't worry, the floor is

00:01:57,539 --> 00:02:01,200
pretty strong, except probably this part. So, all right,

00:02:01,200 --> 00:02:06,549
like this. Three times, all right. So one, two,

00:02:06,549 --> 00:02:07,499
three.

00:02:07,499 --> 00:02:12,220
AUDIENCE & A.D.: Happy new year!

00:02:12,220 --> 00:02:13,400
A.D.: All right.

00:02:13,400 --> 00:02:14,660
You guys are already awake I know but, anyway.

00:02:14,660 --> 00:02:16,700
So we'll take a very brief look at machine

00:02:16,700 --> 00:02:20,220
learning. And there are many awesome talks that tell

00:02:20,220 --> 00:02:22,360
you what machine learning can do. This is not

00:02:22,360 --> 00:02:25,350
what this talk will do. I'll specifically focus on

00:02:25,350 --> 00:02:29,069
classification, which is one type of machine learning, and

00:02:29,069 --> 00:02:31,560
an algorithm that you can use to do that,

00:02:31,560 --> 00:02:34,000
which is SVM. And then at the end we'll

00:02:34,000 --> 00:02:37,129
see how you can do this in Ruby. So

00:02:37,129 --> 00:02:37,470
yeah. This is my first talk, and I thought

00:02:37,470 --> 00:02:42,860
I should do a live demo. Right? Before we

00:02:42,860 --> 00:02:46,629
actually go- so, what we are trying to do

00:02:46,629 --> 00:02:50,090
today is, we have all these large data set

00:02:50,090 --> 00:02:53,450
of about 60,000 images that look like that 6

00:02:53,450 --> 00:02:56,560
and 5 and 7. Each one is 28 by

00:02:56,560 --> 00:02:59,260
28 pixels, and what we have is raw gray

00:02:59,260 --> 00:03:03,230
scale intensity. So you have zero to 255. And

00:03:03,230 --> 00:03:05,640
the, there's this data set available that you can

00:03:05,640 --> 00:03:09,860
use to practice your algorithm on, or train your

00:03:09,860 --> 00:03:11,930
algorithm on. So it's hard to read, it's all

00:03:11,930 --> 00:03:14,680
binary and I don't understand binary. So I put

00:03:14,680 --> 00:03:17,209
it out in CSV like this, and as you

00:03:17,209 --> 00:03:19,349
can see most of the pixels here at the

00:03:19,349 --> 00:03:22,760
top, bottom, sides, they're all going to be zero,

00:03:22,760 --> 00:03:24,650
and in the middle you'll see some numbers that's

00:03:24,650 --> 00:03:27,319
going to range between zero and 255. So what

00:03:27,319 --> 00:03:32,340
we're trying to do is, take these 60,000 images,

00:03:32,340 --> 00:03:36,409
basically create, have a algorithm learn those images, and

00:03:36,409 --> 00:03:38,569
then when we give it a new input, like

00:03:38,569 --> 00:03:41,469
another image, that sort of looks like one of

00:03:41,469 --> 00:03:43,840
these, or maybe a little dissimilar, it should be

00:03:43,840 --> 00:03:47,390
able to tell us what it is, right. So

00:03:47,390 --> 00:03:51,180
that, the MNIST set also comes with 10,000 things

00:03:51,180 --> 00:03:53,590
for testing. So first we'll run it through that

00:03:53,590 --> 00:03:55,939
and see what our accuracy is, and then I'll

00:03:55,939 --> 00:03:58,409
show you some of my bad, bad handwriting. All

00:03:58,409 --> 00:04:05,409
right, so. I will talk about most of these

00:04:05,609 --> 00:04:08,359
things later, but I'm basically - you don't need

00:04:08,359 --> 00:04:10,739
to know what linear is, I'm just showing that

00:04:10,739 --> 00:04:12,739
I'm doing it on 60,000 samples, and I won't

00:04:12,739 --> 00:04:14,549
do the all 10,000 right now I'll just do

00:04:14,549 --> 00:04:20,750
500 of them. And that's going to give, come

00:04:20,750 --> 00:04:24,580
back and say hey I'm 85% accuracy and I

00:04:24,580 --> 00:04:26,710
didn't train the algorithm right now because that takes

00:04:26,710 --> 00:04:30,220
a lot of time. I already have it saved.

00:04:30,220 --> 00:04:31,160
And you can see the code and all that,

00:04:31,160 --> 00:04:34,930
the saved models in GitHub, but for now the

00:04:34,930 --> 00:04:37,690
main important parts are this algorithm, it says it's

00:04:37,690 --> 00:04:41,850
85% accurate, on the 500 samples that we tested.

00:04:41,850 --> 00:04:43,759
And the, all these things that you see, like

00:04:43,759 --> 00:04:46,380
for example this guy, we got two images that

00:04:46,380 --> 00:04:50,280
are actually 9, but are reported as zero. Right,

00:04:50,280 --> 00:04:52,620
so those are bad. Everything that you see along

00:04:52,620 --> 00:04:56,050
the diagonal are good, everything else is bad. And

00:04:56,050 --> 00:04:58,139
I didn't tune the algorithm at all because I

00:04:58,139 --> 00:05:00,060
wanted to keep it simplistic and all that, but

00:05:00,060 --> 00:05:02,289
you could do a lot more stuff and as

00:05:02,289 --> 00:05:03,870
VM I think it has been shown that it

00:05:03,870 --> 00:05:09,000
can get about 93% accuracy with this. All right,

00:05:09,000 --> 00:05:14,080
so. Another thing is, so I wrote down some

00:05:14,080 --> 00:05:21,080
of these numbers on a sticky note. That's probably

00:05:23,760 --> 00:05:27,460
not visible because it's all white. I'll see here.

00:05:28,340 --> 00:05:30,940
Right. So. Something like this, right. They wrote it

00:05:30,980 --> 00:05:33,080
on sticky, and then scanned it in gray scale.

00:05:33,080 --> 00:05:34,940
Then I'm trying to get it to read it

00:05:34,940 --> 00:05:40,780
from my own handwriting. So I did, some of

00:05:40,780 --> 00:05:43,360
this Rails Magick stuff. Who's familiar with Rails Magick?

00:05:43,360 --> 00:05:45,800
Right, so I did some of that stuff to

00:05:45,800 --> 00:05:48,960
get the raw gray scale intensity and all that

00:05:48,960 --> 00:05:51,320
and so I make my picture look a little

00:05:51,320 --> 00:05:56,600
bit like more gray scale. Right. So yeah, I'm

00:05:56,600 --> 00:05:59,620
going to use this kind of - say zero

00:05:59,620 --> 00:06:01,740
is pretty easy for an example because not many

00:06:01,740 --> 00:06:04,260
shapes are like that. So let's see if it

00:06:04,260 --> 00:06:07,760
knows what's written. It says the answer is zero.

00:06:07,760 --> 00:06:10,620
That's good. And I'm going to open this so

00:06:10,620 --> 00:06:12,060
I know which ones are working and which ones

00:06:12,060 --> 00:06:13,300
are not. The 1 is

00:06:13,300 --> 00:06:16,860
working. And some of these are not working. For

00:06:16,900 --> 00:06:19,740
example I'll show you a 3. Look at it

00:06:19,800 --> 00:06:23,000
on the tables. That's 9. And then we'll get

00:06:23,100 --> 00:06:27,000
the 9. The way I write 9 is very

00:06:27,100 --> 00:06:29,100
similar to 3, except this part, right. So I'm

00:06:29,200 --> 00:06:31,700
going to try 9 and it's gonna tell me

00:06:31,800 --> 00:06:34,740
that it's a 3, right. So that's when you

00:06:34,800 --> 00:06:39,620
need more accuracy to train your algorithm, right. Anyway,

00:06:39,700 --> 00:06:42,400
I hope you sort of understand some what you

00:06:42,500 --> 00:06:45,380
can do with machine learning. So what we're doing

00:06:45,400 --> 00:06:49,360
here is, we're not explicitly programming the learning algorithm.

00:06:49,400 --> 00:06:51,940
We're just giving it a lot of data, and

00:06:52,000 --> 00:06:54,800
it automatically figures out how to solve a problem

00:06:54,900 --> 00:06:57,920
given unknown data. Which is, I thought, amazing when

00:06:58,000 --> 00:07:02,760
I learned it. And it's basically statistics-driven algorithms to

00:07:02,760 --> 00:07:06,000
find patterns in data. So there are two types

00:07:06,000 --> 00:07:11,480
of machine learning algorithms. The more widely-known ones are

00:07:11,500 --> 00:07:15,260
probably house, like supervised learning, where it has

00:07:15,260 --> 00:07:19,000
lots of data. For example, let's say you were

00:07:19,000 --> 00:07:21,000
doing house pricing, right. You have to have the

00:07:21,000 --> 00:07:23,000
area in square feet, you know the location of

00:07:23,000 --> 00:07:25,060
the house, you know how many bedrooms it has.

00:07:25,060 --> 00:07:26,960
Is it an apartment, does it have a swimming

00:07:26,960 --> 00:07:29,360
pool - things like that. These are all attributes

00:07:29,360 --> 00:07:32,260
of our problem. When you know these attributes, you

00:07:32,260 --> 00:07:35,000
can basically, and without some set of data where

00:07:35,000 --> 00:07:37,780
you know the prices, you can now feed it

00:07:37,780 --> 00:07:40,000
to an algorithm, and when you have, I don't

00:07:40,000 --> 00:07:41,520
know, like, it's just some made up a two

00:07:41,520 --> 00:07:44,000
bedroom apartment, throw that in the algorithm, what should

00:07:44,000 --> 00:07:45,580
be the price? And it can give you the

00:07:45,580 --> 00:07:48,300
price, right. Of course it'll need a lot of

00:07:48,300 --> 00:07:51,320
work, but that's essentially what supervised learning is. You

00:07:51,320 --> 00:07:53,320
are given some known data, and then you predict

00:07:53,320 --> 00:07:57,180
unknown data. Unsupervised is, on the other hand, there's

00:07:57,180 --> 00:08:00,220
no, like, classified data or something like that. You

00:08:00,220 --> 00:08:02,000
just give it a lot of data, and you're

00:08:02,000 --> 00:08:05,660
trying to find patterns in that. And for example,

00:08:05,660 --> 00:08:09,080
recommendation engines, like Amazon or Flipkart - everybody -

00:08:09,080 --> 00:08:12,340
recommendation engines, right, they go do something like that.

00:08:12,340 --> 00:08:15,640
Now Flipkart does do something like that, I'm

00:08:15,640 --> 00:08:18,940
guessing. And they do finding groups or certain groups

00:08:18,940 --> 00:08:21,300
within your social network, things like that can be

00:08:21,300 --> 00:08:25,740
learned. So today we'll just talk about classification. And,

00:08:25,740 --> 00:08:30,560
so that example I talked about house pricing. Let's

00:08:30,560 --> 00:08:32,120
say that we'll need one attribute, we're gonna, we

00:08:32,120 --> 00:08:37,000
can barely sample here. And can everybody see this

00:08:37,000 --> 00:08:39,000
side, or should I be talking on the other

00:08:39,000 --> 00:08:42,020
side of this room? This side is fine? This

00:08:42,020 --> 00:08:45,360
- that side? All right. OK, so, just one

00:08:45,360 --> 00:08:48,000
attribute we are focusing on, and I'm plotting all

00:08:48,000 --> 00:08:50,800
these houses. So this much area, this is the

00:08:50,800 --> 00:08:52,800
price, this much area, this is the price, sort

00:08:52,800 --> 00:08:55,000
of something like that, right. And this is not

00:08:55,000 --> 00:08:58,220
exactly linear because there are other attributes. We are

00:08:58,220 --> 00:09:01,000
just considering one attribute. And we have this red

00:09:01,000 --> 00:09:03,000
dot here that we want to predict the right

00:09:03,000 --> 00:09:05,000
price on. What do you think is the best

00:09:05,000 --> 00:09:09,000
way to do that? AUDIENCE MEMBER: Extrapolation. A.D.: Right,

00:09:09,000 --> 00:09:13,000
extrapolation. So this is a linear extrapolation, and it's

00:09:13,000 --> 00:09:16,000
finding a linear thing, the progression will give linear

00:09:16,000 --> 00:09:19,000
data. The dots will be here, there, and there,

00:09:19,000 --> 00:09:22,000
and in that case you look data up on

00:09:22,000 --> 00:09:25,000
the only other fitting curve. In this case it's

00:09:25,000 --> 00:09:27,000
a linear curve. And then once you do that,

00:09:27,000 --> 00:09:30,000
you figure out the dot will probably lie somewhere

00:09:30,000 --> 00:09:32,000
here. So it's running a price over here. Which

00:09:32,000 --> 00:09:35,000
is not bad. Given the limited sample here.

00:09:35,000 --> 00:09:39,000
So how do you actually solve this problem? How

00:09:39,000 --> 00:09:41,000
do you find these dots that are away from

00:09:41,000 --> 00:09:44,900
the line? The thing you do - before I

00:09:44,900 --> 00:09:48,000
show you the math, for every one of these

00:09:48,000 --> 00:09:52,000
points, you say that if I knew what the

00:09:52,000 --> 00:09:55,000
slope was, how do I compare it to this?

00:09:55,000 --> 00:09:59,500
So that's the lead hypothesis, right. Hypothesis on every

00:09:59,500 --> 00:10:03,000
dot is going to be this line equation here.

00:10:03,000 --> 00:10:06,000
You probably know this as V and this as

00:10:06,000 --> 00:10:11,000
M, right. V, V, V plus Mx. But I'm

00:10:11,000 --> 00:10:12,500
probably gonna use the theta one because you can

00:10:12,500 --> 00:10:15,000
have more than one attribute. You actually missed one.

00:10:15,000 --> 00:10:18,000
So that way you can rotate out, quantate and

00:10:18,000 --> 00:10:20,000
filtrate one, blah blah blah. So you can't

00:10:20,000 --> 00:10:23,000
believe the hypothesis. Well right now you don't know

00:10:23,000 --> 00:10:26,000
the answer on data alone, right. So you know

00:10:26,000 --> 00:10:29,000
that Y there means is the actual price. So

00:10:29,000 --> 00:10:32,000
you take the Y and you find the difference

00:10:32,000 --> 00:10:35,000
between the hypothesis and the Y, while not knowing

00:10:35,000 --> 00:10:39,000
the hypothesis, right? So you're just forming the formula.

00:10:39,000 --> 00:10:41,000
And then you try to minimize that part of

00:10:41,000 --> 00:10:45,000
the whole picture, right. So what you do is

00:10:45,000 --> 00:10:49,000
a declaration for every point. You take the Y

00:10:49,000 --> 00:10:52,000
out, square it, because, and then it's absolute, and

00:10:52,000 --> 00:10:54,000
then you find the sum of it. And this

00:10:54,000 --> 00:10:57,000
is your minimization problem. How many of you have

00:10:57,000 --> 00:11:02,000
heard about linear programming, or optimization minimization. Yeah, so

00:11:02,000 --> 00:11:06,000
this is basically a minimization problem, and I know

00:11:06,000 --> 00:11:08,000
this, we'll figure out what equals zero and we

00:11:08,000 --> 00:11:10,000
can go on. And this was a linear

00:11:10,000 --> 00:11:12,000
thing, so linear, but we can also have

00:11:12,000 --> 00:11:18,000
polynomial curves. Next up is, that was linear, so

00:11:18,000 --> 00:11:20,000
the price is a real number, so you're trying

00:11:20,000 --> 00:11:24,220
to predict the exact numbers, whereas this kind is

00:11:24,220 --> 00:11:26,000
saying is my- is this email spam or not

00:11:26,000 --> 00:11:29,000
spam? And this is a very famous machine learning

00:11:29,000 --> 00:11:34,000
thing that Gmail started. So I'm going to

00:11:34,000 --> 00:11:37,000
keep it simple, just considering two attributes of an

00:11:37,000 --> 00:11:40,000
email. Length of the email, and does it have

00:11:40,000 --> 00:11:45,000
a dollar symbol, right. So based on those two

00:11:45,000 --> 00:11:48,000
attributes, I have plotted out, let's say something like

00:11:48,000 --> 00:11:50,000
- the red stars are spam emails. The blue

00:11:50,000 --> 00:11:53,000
ones are non-spam emails. Now how do you get

00:11:53,000 --> 00:11:56,000
an unknown email, how do you classify whether it's

00:11:56,000 --> 00:11:59,000
a red star or not? It's very similar with

00:11:59,000 --> 00:12:02,000
the previous problem. You are again going to find

00:12:02,000 --> 00:12:06,000
the curve that's sort of a different, there's a

00:12:06,000 --> 00:12:08,000
difference between the red stars and not, and this

00:12:08,000 --> 00:12:11,000
one thing you do by computing some of these

00:12:11,000 --> 00:12:13,000
here, which both will show that this is

00:12:13,000 --> 00:12:16,000
probably not exactly a linear solution. You'll need better

00:12:16,000 --> 00:12:21,000
than one attribute. So like the previous one, we

00:12:21,000 --> 00:12:23,000
are going to need a hypothesis. This time we

00:12:23,000 --> 00:12:25,000
need the hypothesis to be between zero and 1.

00:12:25,000 --> 00:12:28,000
So zero means it's not spam, and it doesn't

00:12:28,000 --> 00:12:31,000
matter what it means as long as they're consistent.

00:12:31,000 --> 00:12:35,000
So the Y value will be either zero or

00:12:35,000 --> 00:12:37,000
one, so the hypothesis is also zero or one.

00:12:37,000 --> 00:12:41,000
And this is a very famous sigma function that

00:12:41,000 --> 00:12:43,000
I will deliver to you after you figure it

00:12:43,000 --> 00:12:46,000
out, but this for me is easier. And

00:12:46,000 --> 00:12:50,000
here, like the previous one, we're just using theta

00:12:50,000 --> 00:12:53,000
zero plus theta 1 in base form right.

00:12:53,000 --> 00:12:57,000
This side, that whole thing, is Z and Z

00:12:57,000 --> 00:13:00,000
here. And then you solve that minimizer equation I

00:13:00,000 --> 00:13:02,000
guarantee you'll find equals theta zero or theta 1,

00:13:02,000 --> 00:13:04,000
and then you find that you are right where

00:13:04,000 --> 00:13:06,000
you should be. So it will come somewhere between

00:13:06,000 --> 00:13:09,000
zero and 1, and you take that from. That's

00:13:09,000 --> 00:13:13,000
how you classify it. Right. And how you exactly

00:13:13,000 --> 00:13:16,000
minimize those problems is, SVM is a powerful way

00:13:16,000 --> 00:13:19,000
to do that. Along with numeric constants are

00:13:19,000 --> 00:13:24,000
widely-used in machine learning and It's called a

00:13:24,000 --> 00:13:27,000
large margin classifier, and the full-form stands for Support

00:13:27,000 --> 00:13:29,000
Vector Machines. We'll see what all of that means. When I

00:13:29,000 --> 00:13:33,000
started, I thought, why are these things formed the

00:13:33,000 --> 00:13:37,000
way they are here? So again, red stars and

00:13:37,000 --> 00:13:40,000
green squares and we want to segregate them. We

00:13:40,000 --> 00:13:42,000
want to find the line that divides them. We

00:13:42,000 --> 00:13:45,000
can think of many lines. There's a pink line

00:13:45,000 --> 00:13:46,000
here, there's a light blue line there, then there's

00:13:46,000 --> 00:13:50,000
a deep blue line. So, what SVM does is

00:13:50,000 --> 00:13:53,000
it finds all the lines that it potentially can

00:13:53,000 --> 00:13:56,000
use, finds the critical points that matters as the

00:13:56,000 --> 00:13:59,000
distance between the line and the blue, right.

00:13:59,000 --> 00:14:02,000
So in the case of the pink one, this

00:14:02,000 --> 00:14:06,000
is the closest one for this side, and that's

00:14:06,000 --> 00:14:09,000
probably, let me identify it, so one of these

00:14:09,000 --> 00:14:12,000
points is the closest - so this, the closest

00:14:12,000 --> 00:14:18,000
one, is called the support vector for that group,

00:14:18,000 --> 00:14:20,000
right. All these other points, the difference between the

00:14:20,000 --> 00:14:23,000
pink line, and all these other points don't matter,

00:14:23,000 --> 00:14:25,000
because they are the minimal distance from the group.

00:14:25,000 --> 00:14:28,000
So it finds out the line that best segregates

00:14:28,000 --> 00:14:32,000
these things. And this is the support vector, and

00:14:32,000 --> 00:14:33,000
this is the margin, the length of that is

00:14:33,000 --> 00:14:39,000
the margin, hence, large margin classifier, right. So that's

00:14:39,000 --> 00:14:41,000
SVM, we are going, it will get exactly the

00:14:41,000 --> 00:14:48,000
algorithm ??, right. So, and if you're feeling that,

00:14:48,000 --> 00:14:51,000
like that, it's OK, because this is about six

00:14:51,000 --> 00:14:56,000
times the ?? But I know a lot of

00:14:56,000 --> 00:14:58,000
you love coding, so let's slow down and get

00:14:58,000 --> 00:15:03,000
coding. Right. So before we start coding, one more

00:15:03,000 --> 00:15:07,000
small theoretical segue. We learn how to move zero

00:15:07,000 --> 00:15:10,000
or 1, then do plusses, right. Spec or not

00:15:10,000 --> 00:15:12,000
spec. How do you do when there are multiple

00:15:12,000 --> 00:15:15,000
plusses? So in our example, we have ten plusses

00:15:15,000 --> 00:15:17,000
- the numbers zero through 9, we need to

00:15:17,000 --> 00:15:20,000
find out which number is it, right. What you

00:15:20,000 --> 00:15:23,000
do in this case is, we have three classes

00:15:23,000 --> 00:15:25,000
here. First we'll read it three linear - three

00:15:25,000 --> 00:15:29,000
classification problems. The first one is whether it's a

00:15:29,000 --> 00:15:32,000
red star or not. That's going to give you

00:15:32,000 --> 00:15:35,000
a line like this, right. It's a red star

00:15:35,000 --> 00:15:38,000
or not. And then you do another one that's

00:15:38,000 --> 00:15:41,000
a blue circle or not. And then a third

00:15:41,000 --> 00:15:44,000
one, whether it's a green square or not. Then

00:15:44,000 --> 00:15:47,000
you give that your unknown input, you ask all

00:15:47,000 --> 00:15:49,000
these three things, what do you think? So they'll

00:15:49,000 --> 00:15:51,900
say I think it's a red star or not,

00:15:51,900 --> 00:15:54,000
and then similarly for the other ones. Also remember,

00:15:54,000 --> 00:15:57,000
it'll also give you a number right, between zero

00:15:57,000 --> 00:16:00,000
and 1. So you take the one that is

00:16:00,000 --> 00:16:04,000
the most ?? So if you get point 9

00:16:04,000 --> 00:16:07,000
from the red, versus like point 4 or point

00:16:07,000 --> 00:16:09,000
3 from the others, and then you would think,

00:16:09,000 --> 00:16:11,000
you would think that it's a red star, right.

00:16:11,000 --> 00:16:16,000
So that's multi-class, and right, so we'll get down

00:16:16,000 --> 00:16:18,000
to Ruby now, which is what you guys all

00:16:18,000 --> 00:16:21,000
love. There is a very famous library called libsvm,

00:16:21,000 --> 00:16:24,000
which is available on the Unix and there's

00:16:24,000 --> 00:16:27,000
a gem called rb - Ruby - libsvm, which

00:16:27,000 --> 00:16:29,000
shows how the code works, and that's what the

00:16:29,000 --> 00:16:33,000
code that I showed on GitHub is actually using

00:16:33,000 --> 00:16:36,000
that. We will also talk about weka, which is

00:16:36,000 --> 00:16:40,000
a, some Java library, only good for ??

00:16:40,000 --> 00:16:41,520
and is awesome. I'll show you how we use

00:16:41,520 --> 00:16:45,000
that for Ruby. And you could, of course, so

00:16:45,000 --> 00:16:48,000
all we are doing here is matrix or linear algebra.

00:16:48,000 --> 00:16:51,000
you could ?? you could find out either -

00:16:51,000 --> 00:16:55,000
or figure it out yourself, don't try to do

00:16:55,000 --> 00:16:58,000
that because it's very complicated, right. And there is

00:16:58,000 --> 00:17:03,000
- who has heard of Apache Mahout? That's probably

00:17:03,000 --> 00:17:05,000
very, very well well-known because it's highly scale-able but

00:17:05,000 --> 00:17:08,000
it's work setting up like a Apache Mahout and

00:17:08,000 --> 00:17:12,000
all that so I'll move on. Right, so

00:17:12,000 --> 00:17:14,000
using libsvm, and the exact code is in this

00:17:14,000 --> 00:17:20,000
model file, since we're still using Rails, right. So

00:17:20,000 --> 00:17:22,000
there you create a problem, and then you take

00:17:22,000 --> 00:17:25,000
in a parameter. And the parameter means things like

00:17:25,000 --> 00:17:28,000
using a linear or a parabola the shape of

00:17:28,000 --> 00:17:31,000
that line is called a curve, to put it

00:17:31,000 --> 00:17:33,000
simply. So when I'm using a linear curve, or

00:17:33,000 --> 00:17:37,000
a polynomial or something like that, as you can

00:17:37,000 --> 00:17:40,000
see I'm just doing this most simple thing, which

00:17:40,000 --> 00:17:43,000
is taking the ?? and then our examples, which

00:17:43,000 --> 00:17:45,000
I showed you, is taking about 60,000 images, or

00:17:45,000 --> 00:17:50,000
I'm sorry, 60,000 items, where every item is again

00:17:50,000 --> 00:17:54,000
28 by 28, so 785 numbers. But every number

00:17:54,000 --> 00:17:57,000
is between zero and 255, right. So if you

00:17:57,000 --> 00:18:01,000
wrap around in SVM mode, feed it into our

00:18:01,000 --> 00:18:04,000
problem, as labels and features. Features are these guys,

00:18:04,000 --> 00:18:07,000
labels are the known labels, like is it the

00:18:07,000 --> 00:18:09,000
ones, is it the twos, things like that. And

00:18:09,000 --> 00:18:12,000
then the training algorithm. The training takes some time,

00:18:12,000 --> 00:18:16,000
so usually it would say interrupt persisted, and then

00:18:16,000 --> 00:18:19,000
leave that data later on, and then use that

00:18:19,000 --> 00:18:22,000
to predict when you've been given it new data,

00:18:22,000 --> 00:18:26,000
like for example, my zero in my own handwriting,

00:18:26,000 --> 00:18:30,000
you take the array of 28 by 28 pixels,

00:18:30,000 --> 00:18:34,000
and you create the predict based on what it

00:18:34,000 --> 00:18:39,000
is, right. Simple stuff. Right. Weka is another awesome

00:18:39,000 --> 00:18:43,000
tool, it's from a university in New Zealand's name

00:18:43,000 --> 00:18:46,000
I cannot pronounce well, it goes something like Waikato,

00:18:46,000 --> 00:18:54,000
I don't get it, anyway. So another label. Instead

00:18:54,000 --> 00:18:56,000
of showing the screenshots of this I'm going to

00:18:56,000 --> 00:19:00,000
show you Weka. Weka is awesome because it is

00:19:00,000 --> 00:19:04,000
so versatile, right. It comes with a ?? as

00:19:04,000 --> 00:19:06,000
well as a Java API. And I'll just show

00:19:06,000 --> 00:19:09,000
you the ?? part so. This is my training

00:19:09,000 --> 00:19:12,000
data, right, it reads something on the arff format,

00:19:12,000 --> 00:19:15,500
but it can read CSV also. And I already

00:19:15,500 --> 00:19:19,000
read it in here, and it lets you picture

00:19:19,000 --> 00:19:21,000
like how your data loads like, first of all.

00:19:21,000 --> 00:19:26,000
So you can see I sent 4,784 images from

00:19:26,000 --> 00:19:31,000
that file, and it's pretty evenly distributed along the

00:19:31,000 --> 00:19:36,900
different classes and numbers, right, and I wanted to

00:19:36,900 --> 00:19:39,000
use something called Normalize, so that's a step where,

00:19:39,000 --> 00:19:42,800
so my numbers, attributes, go from zero to 255.

00:19:42,800 --> 00:19:47,000
That's good, but, think about having, in the house

00:19:47,000 --> 00:19:50,000
pricing, from your one attribute which is square feet

00:19:50,000 --> 00:19:53,000
as 1,800, and the scale of thousands, and another

00:19:53,000 --> 00:19:56,000
attribute which uses the scale 1 or 2 or

00:19:56,000 --> 00:19:59,000
5, right. So you will need to normalize it

00:19:59,000 --> 00:20:02,000
so every attribute is between a certain scale. And

00:20:02,000 --> 00:20:05,000
Weka extracts that from any of the emergent sliders

00:20:05,000 --> 00:20:08,700
in the- that way, and you can see these

00:20:08,700 --> 00:20:13,000
are all my 784 attributes. And I will show

00:20:13,000 --> 00:20:16,000
you something in the middle. So you can see

00:20:16,000 --> 00:20:18,000
max is zero - min is zero, max is

00:20:18,000 --> 00:20:21,000
one, and how many mistake values and all that.

00:20:21,000 --> 00:20:24,000
Most awesome part, and I'm going to say this

00:20:24,000 --> 00:20:27,000
multiple times, the most awesome part of Weka, is

00:20:27,000 --> 00:20:31,800
you choose any algorithm that you want, right. So

00:20:31,800 --> 00:20:33,000
you can do a miles?? someone of you

00:20:33,000 --> 00:20:35,000
may have heard this name, like a random format

00:20:35,000 --> 00:20:38,000
?? or viewer samples?? and all that. You can

00:20:38,000 --> 00:20:40,900
pick any of that. And I pick the libSVM,

00:20:40,900 --> 00:20:43,000
which is the SVM. And I'm saying, I'm gonna

00:20:43,000 --> 00:20:47,000
use 90% of the ?? instances to train it,

00:20:47,000 --> 00:20:49,000
and the rest can push it to test it

00:20:49,000 --> 00:20:54,000
on. And then it's going to build a model,

00:20:54,000 --> 00:20:56,000
and then it's going to also scale the model

00:20:56,000 --> 00:20:58,000
and all that, and more - but it will

00:20:58,000 --> 00:21:02,000
probably take about seventeen seconds, yes, six seconds, and

00:21:02,000 --> 00:21:04,000
then now it's testing on those instances, and soon

00:21:04,000 --> 00:21:09,000
it will report what the accuracy was. This is

00:21:09,000 --> 00:21:11,000
probably not readable from the back, but I didn't

00:21:11,000 --> 00:21:14,000
build this software so don't worry ?? 91%

00:21:14,000 --> 00:21:20,000
accuracy, using SVM. AUDIENCE MEMBER: [indecipherable] A.D.: And

00:21:23,100 --> 00:21:24,000
then this is like the builder feature, we already

00:21:24,000 --> 00:21:28,000
saw that. Almost everything is along the background, so

00:21:28,000 --> 00:21:32,000
that's good. And this is in part the best

00:21:32,000 --> 00:21:34,000
thing about Weka. It comes with something called the

00:21:34,000 --> 00:21:38,000
Experimenter, where you can feed it the data that

00:21:38,000 --> 00:21:41,000
you have, and you give it multiple algorithms, or

00:21:41,000 --> 00:21:44,000
the same number in different values and it's going

00:21:44,000 --> 00:21:46,000
to compare all this and tell you which is

00:21:46,000 --> 00:21:49,000
the best, it's pretty amazing, right. So this is

00:21:49,000 --> 00:21:51,000
really, really awesome stuff. We use this a lot.

00:21:51,000 --> 00:21:57,000
It comes with a Java API, and this is

00:21:57,000 --> 00:22:00,000
how you use it. So yesterday Dejas told you

00:22:00,000 --> 00:22:02,000
about the best way to write Python. The best

00:22:02,000 --> 00:22:05,000
way to write Java is also in JRuby actually,

00:22:05,000 --> 00:22:09,000
so a class like this, Weka will classify something,

00:22:09,000 --> 00:22:13,000
becomes a new module. And Weka in some cases

00:22:13,000 --> 00:22:16,000
become this, and I'm so used to it I

00:22:16,000 --> 00:22:20,000
think that I hate ?? also. Yeah. This is

00:22:20,000 --> 00:22:23,000
awesome. So this is on the API, Java and

00:22:23,000 --> 00:22:27,000
initial data. So yeah, there are 70 options like

00:22:27,000 --> 00:22:31,000
linear, the building it. Any time you

00:22:31,000 --> 00:22:35,000
have new data you classify it. Right. And then

00:22:35,000 --> 00:22:39,000
we didn't really touch up on what we didn't

00:22:39,000 --> 00:22:42,000
do today, and there's a lot of stuff. This

00:22:42,000 --> 00:22:45,000
is just like the tip of the iceburg, this

00:22:45,000 --> 00:22:47,000
list is not complete. There's something called regularization where,

00:22:47,000 --> 00:22:51,000
if you have lots of points, like almost spread

00:22:51,000 --> 00:22:55,000
over it, and you, like your input is not

00:22:55,000 --> 00:22:58,000
even enough, it's going to create the curve that

00:22:58,000 --> 00:23:01,000
goes through it, every living example. So when you

00:23:01,000 --> 00:23:05,000
have a Weka problem where your data is very

00:23:05,000 --> 00:23:06,000
similar to a training?? example, it's going to

00:23:06,000 --> 00:23:10,000
predict exactly that data. When it is, like, here

00:23:10,000 --> 00:23:12,500
or there, it's going to be completely wrong. So

00:23:12,500 --> 00:23:15,000
you need something called regularization to prevent that. It's

00:23:15,000 --> 00:23:18,000
not going to be very easy. We talked about

00:23:18,000 --> 00:23:23,000
normalization, we talked about polynomial classification. And I also

00:23:23,000 --> 00:23:26,000
think another thing which is sort of important, so

00:23:26,000 --> 00:23:29,000
we have 784 attributes in our problem. And it's

00:23:29,000 --> 00:23:31,000
print-line comes into like the end of of all

00:23:31,000 --> 00:23:38,000
this and all that, given 60,000 elements. So you

00:23:38,000 --> 00:23:41,000
do something called principle ?? analysis which figures

00:23:41,000 --> 00:23:44,000
out which are important attributes are in this. Because

00:23:44,000 --> 00:23:48,000
if you remember the image, the top, bottom, and

00:23:48,000 --> 00:23:51,000
the sides are all zero, right, almost always. So

00:23:51,000 --> 00:23:53,000
those are really not important to figure out what

00:23:53,000 --> 00:23:55,000
it is. And data generation. And so we won't

00:23:55,000 --> 00:23:59,000
talk about that today. And then, if you're not

00:23:59,000 --> 00:24:01,000
inspired, go and take this class. This is where

00:24:01,000 --> 00:24:05,000
I started. Machine Learning Coursera class, it was the

00:24:05,000 --> 00:24:09,000
It was the first Coursera class. pretty, pretty awesome, and it goes very deep.

00:24:09,000 --> 00:24:13,000
so that's good. And then the Collective Intelligence book is really, really good.

00:24:13,000 --> 00:24:16,000
And there's this, a website called Candle.

00:24:16,000 --> 00:24:20,000
And you can, when you are reading this stuff,

00:24:20,000 --> 00:24:24,000
they have something called competitions, 

00:24:24,000 --> 00:24:27,000
and people, like companies, actually post their

00:24:27,000 --> 00:24:30,000
machine learning problems here. Right. And if you solve

00:24:30,000 --> 00:24:33,000
it then you get that much money. But there

00:24:33,000 --> 00:24:38,000
are lots of free competitions as well, where you

00:24:38,000 --> 00:24:40,000
can test out. So this is one, Digit Register

00:24:40,000 --> 00:24:42,000
is one of those, and you don't get any

00:24:42,000 --> 00:24:46,000
money for that, but yeah. So that's pretty awesome,

00:24:46,000 --> 00:24:48,000
trying, you can try and go through that. And

00:24:48,000 --> 00:24:52,000
that's it. Thank you.

00:24:57,800 --> 00:24:59,000
V.O.: We have time for questions.

00:24:59,000 --> 00:25:03,000
QUESTION: I have a question. Yeah. So in

00:25:03,000 --> 00:25:05,000
the case of when you are learning from this

00:25:05,000 --> 00:25:09,000
data, where do, how do you, how do you

00:25:09,000 --> 00:25:11,000
get known citation- ??

00:25:11,000 --> 00:25:12,000
A.D.: How do you find data?

00:25:12,000 --> 00:25:14,000
QUESTION: How do you fix the emphasis of

00:25:14,000 --> 00:25:17,000
data, they have a lot of images, but only

00:25:17,000 --> 00:25:19,000
a few of them get passed in. For

00:25:19,000 --> 00:25:21,000
example, if you take the example of the pricing

00:25:21,000 --> 00:25:22,000
of the flats-

00:25:22,000 --> 00:25:23,000
A.D.: Right.

00:25:23,000 --> 00:25:24,000
QUESTION: So each image

00:25:24,000 --> 00:25:26,000
the same- depending on the same code, and

00:25:26,000 --> 00:25:30,000
they're probably offering at vastly different prices. And in

00:25:30,000 --> 00:25:34,440
such cases, ??

00:25:34,440 --> 00:25:35,000
A.D.: So basically what you're saying

00:25:35,000 --> 00:25:40,000
is your data sort of implements, right. That's probably

00:25:40,000 --> 00:25:43,000
not going to happen in the real world, where

00:25:43,000 --> 00:25:46,000
you probably have not found the right attributes, because,

00:25:46,000 --> 00:25:48,000
why is your neighbor giving you a flat fee

00:25:48,000 --> 00:25:50,000
in the first place, right. So that needs

00:25:50,000 --> 00:25:52,000
to be one of your attributes as well, right.

00:25:52,000 --> 00:25:56,000
But if there really, like, well simulated, then- So,

00:25:56,000 --> 00:26:00,000
what I showed was just the linear curve. You

00:26:00,000 --> 00:26:03,000
can have lots of polynomial , and by polynomial

00:26:03,000 --> 00:26:05,000
I mean weird shapes, so you could have, like,

00:26:05,000 --> 00:26:09,000
the center is that part, and again the outside

00:26:09,000 --> 00:26:12,000
part falls under one classification, the middle part does

00:26:12,000 --> 00:26:16,000
not, things like that. So it's basically going to

00:26:16,000 --> 00:26:18,000
try out a lot of things, basically to figure

00:26:18,000 --> 00:26:20,000
out what is the algorithm that you need.

00:26:33,600 --> 00:26:37,000
QUESTION: Can you, can you do the same pattern on

00:26:37,000 --> 00:26:41,000
any other media within the data, right? We have,

00:26:41,000 --> 00:26:44,000
like, extra data, like it's some extra number that

00:26:44,000 --> 00:26:50,040
is going into it. It is something ??

00:26:50,100 --> 00:26:52,000
A.D.: So the same way here, like we showed, what

00:26:52,000 --> 00:26:57,000
you are working with, right, images, right. Images, ultimately,

00:26:57,000 --> 00:26:59,000
just not ours, right, when it comes down to

00:26:59,000 --> 00:27:02,000
it. Similarly for vice we would either find the

00:27:02,000 --> 00:27:06,000
frequency or they, you know, amplitude, things like that,

00:27:06,000 --> 00:27:08,000
which I know I'm not gonna get into right

00:27:08,000 --> 00:27:11,000
now, but, you would find numerical fences at that

00:27:11,000 --> 00:27:15,000
points. And actually one of the widely-known uses of

00:27:15,000 --> 00:27:18,000
machine learning is filtering out speech from the noise

00:27:18,000 --> 00:27:21,000
in the background. Right. So that code data

00:27:21,000 --> 00:27:25,000
you're trying to figure out my voice, versus, like

00:27:25,000 --> 00:27:27,000
if there's aircraft in the hangar. How do you

00:27:27,000 --> 00:27:30,000
get that out? So yeah. You can ??.

00:27:33,300 --> 00:27:46,000
QUESTION: Like if you ??

00:27:46,000 --> 00:27:47,000
A.D.: OK, so first of

00:27:47,000 --> 00:27:50,000
all this is a Ruby conference. So that's one

00:27:50,000 --> 00:27:54,000
big reason. Second thing is, when you're, when you're

00:27:54,000 --> 00:27:57,000
using libSVM, like you saw, and that's at the

00:27:57,000 --> 00:28:00,000
C level, so it doesn't really matter what I'm

00:28:00,000 --> 00:28:02,000
using. If you are using Weka, or using the

00:28:02,000 --> 00:28:06,000
JRuby, right. So at that point it doesn't really

00:28:06,000 --> 00:28:09,000
matter. I would still say that most people who

00:28:09,000 --> 00:28:12,000
are doing machine learning are not using Ruby, to

00:28:12,000 --> 00:28:15,000
be fair, because even the reading on the data

00:28:15,000 --> 00:28:18,000
might get, because you're working with Gigabytes or Terabytes

00:28:18,000 --> 00:28:22,000
of data, it might take awhile. But my point

00:28:22,000 --> 00:28:25,000
here is, you guys all know Ruby, or have

00:28:25,000 --> 00:28:28,000
visited Ruby, and this is another thing that I

00:28:28,000 --> 00:28:31,000
want to introduce you to, so this is probably

00:28:31,000 --> 00:28:33,000
the best way to learn it. Because if you're

00:28:33,000 --> 00:28:37,000
going to learn like Julia or R along with

00:28:37,000 --> 00:28:39,000
all this, you're probably going to be a little

00:28:39,000 --> 00:28:41,000
lost. So it's maybe this could be your first

00:28:41,000 --> 00:28:44,000
step, trying it out, like with something you already

00:28:44,000 --> 00:28:46,000
know. Then progress if you get really interested in

00:28:46,000 --> 00:28:49,000
this stuff. So progress a lot more towards the

00:28:49,000 --> 00:28:53,000
tools you are going to use ??. Any more

00:28:53,000 --> 00:28:58,000
questions? That's it then.

00:28:58,000 --> 00:29:00,000
V.O.: Thank you Ardak.

00:29:00,000 --> 00:29:02,000

YouTube URL: https://www.youtube.com/watch?v=aCIoerxtQ3w


