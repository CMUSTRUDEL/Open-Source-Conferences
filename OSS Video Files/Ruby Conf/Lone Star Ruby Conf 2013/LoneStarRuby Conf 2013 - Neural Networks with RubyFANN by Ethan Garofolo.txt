Title: LoneStarRuby Conf 2013 - Neural Networks with RubyFANN by Ethan Garofolo
Publication date: 2020-01-28
Playlist: Lone Star Ruby Conf 2013
Description: 
	Neural networks (NNs) not only sound really cool, but they can also solve some pretty interesting problems ranging from driving cars to spam detection to facial recognition.

Solving problems with NNs is challenging, because actually implementing a NN from scratch is difficult, and knowing how to apply it is more difficult. Fortunately, libraries, such as RubyFANN, exist to handle the first problem. Solving the second problem comes from experience.

This talk will show a few different approaches to applying NNs to such problems as spam detection and games, as well as discussing other areas where NNs might be a useful solution.

Help us caption & translate this video!

http://amara.org/v/FG8g/
Captions: 
	00:00:14,790 --> 00:00:21,960
welcome to neural networks with Ruby fan

00:00:17,609 --> 00:00:24,960
I am Ethan Garr follow your humble

00:00:21,960 --> 00:00:27,500
presenter here today I own Big O studios

00:00:24,960 --> 00:00:30,419
it order of complexity there that's a

00:00:27,500 --> 00:00:33,390
consultancy up in Round Rock do web

00:00:30,419 --> 00:00:35,129
application in a ruby base web framework

00:00:33,390 --> 00:00:38,070
that must not be named because this is a

00:00:35,129 --> 00:00:39,900
ruby conference right also you use

00:00:38,070 --> 00:00:42,360
nodejs writing a book in that and I do

00:00:39,900 --> 00:00:45,030
unity3d development as well sometimes

00:00:42,360 --> 00:00:46,110
that all is in the same project and of

00:00:45,030 --> 00:00:48,420
course you know I've got my super-secret

00:00:46,110 --> 00:00:51,870
projects I can't talk about but we all

00:00:48,420 --> 00:00:54,289
have those right anyway we're gonna talk

00:00:51,870 --> 00:00:57,120
about neural networks today obviously

00:00:54,289 --> 00:00:59,550
hitting on what they are in general how

00:00:57,120 --> 00:01:01,230
they operate strategies for how to apply

00:00:59,550 --> 00:01:03,989
them to the problems you might be trying

00:01:01,230 --> 00:01:06,119
to solve yourselves how do you apply

00:01:03,989 --> 00:01:07,979
them to some specific examples that

00:01:06,119 --> 00:01:10,050
we'll get into in the talk then we'll

00:01:07,979 --> 00:01:11,250
talk about other problems you could try

00:01:10,050 --> 00:01:13,640
to solve with the neural network and

00:01:11,250 --> 00:01:17,130
then of course we'll have time for Q&A

00:01:13,640 --> 00:01:19,200
so I do want to ask how many of you here

00:01:17,130 --> 00:01:20,670
have formal computer science training

00:01:19,200 --> 00:01:25,140
like you went to school for and you got

00:01:20,670 --> 00:01:26,550
a degree okay okay or you got some

00:01:25,140 --> 00:01:28,590
degree of exposure to it I'm glad that

00:01:26,550 --> 00:01:30,810
wasn't all of the hands because chances

00:01:28,590 --> 00:01:32,640
are if you did see us training you

00:01:30,810 --> 00:01:34,950
probably got some exposure turn your

00:01:32,640 --> 00:01:37,110
neural networks but we're gonna review

00:01:34,950 --> 00:01:39,390
it anyway because it's really useful and

00:01:37,110 --> 00:01:41,520
the one other caveat I want to give is

00:01:39,390 --> 00:01:43,709
people go to graduate school to study

00:01:41,520 --> 00:01:45,840
neural networks they can spend three

00:01:43,709 --> 00:01:49,020
four years doing that to get a PhD I

00:01:45,840 --> 00:01:50,280
have not done that and we're probably

00:01:49,020 --> 00:01:51,899
not gonna get that deep of an

00:01:50,280 --> 00:01:53,789
understanding of neural networks anyway

00:01:51,899 --> 00:01:55,649
we could try it I suppose but we'd

00:01:53,789 --> 00:01:58,080
probably be here until a lone star Ruby

00:01:55,649 --> 00:02:00,229
2017 and I imagine that you all have

00:01:58,080 --> 00:02:03,450
better things to do in the mean time so

00:02:00,229 --> 00:02:06,119
let's get into it what are using what

00:02:03,450 --> 00:02:08,459
are neural networks they are function

00:02:06,119 --> 00:02:10,739
approximator x' boy that really explains

00:02:08,459 --> 00:02:13,379
it doesn't it two parts their functions

00:02:10,739 --> 00:02:15,330
and approximator you've probably seen

00:02:13,379 --> 00:02:17,520
functions before right I'm not talking

00:02:15,330 --> 00:02:20,940
about the like function keyword and

00:02:17,520 --> 00:02:23,310
JavaScript or C++ the concept of mapping

00:02:20,940 --> 00:02:24,780
a domain to arrange and here's an

00:02:23,310 --> 00:02:26,489
example of one that's the identity

00:02:24,780 --> 00:02:26,830
function you probably saw that in high

00:02:26,489 --> 00:02:29,230
school

00:02:26,830 --> 00:02:30,460
even now there are two things about the

00:02:29,230 --> 00:02:33,430
way we learned functions in high school

00:02:30,460 --> 00:02:35,440
one I kind of left high school thinking

00:02:33,430 --> 00:02:37,690
that all functions map numbers to other

00:02:35,440 --> 00:02:40,330
numbers that doesn't have to be the case

00:02:37,690 --> 00:02:42,970
it just maps domains to range values

00:02:40,330 --> 00:02:45,250
here that's numbers but what if my input

00:02:42,970 --> 00:02:47,290
was like a low cat or something it as

00:02:45,250 --> 00:02:49,360
long as it also output a low cat that's

00:02:47,290 --> 00:02:50,800
still the identity function right the

00:02:49,360 --> 00:02:52,270
other thing was that when you worked

00:02:50,800 --> 00:02:54,580
problems in high school you tended to

00:02:52,270 --> 00:02:56,230
like you got an input you got the

00:02:54,580 --> 00:02:58,390
function you were supposed to calculate

00:02:56,230 --> 00:03:00,040
the output or vice-versa so you had the

00:02:58,390 --> 00:03:02,260
output and the function what was the

00:03:00,040 --> 00:03:03,940
input that produces that well in the

00:03:02,260 --> 00:03:06,700
real world you don't always know what

00:03:03,940 --> 00:03:08,350
the function is right that's the role of

00:03:06,700 --> 00:03:10,720
a neural network you have some set of

00:03:08,350 --> 00:03:13,120
observations and you want to find what

00:03:10,720 --> 00:03:16,630
is the rule that takes me from this here

00:03:13,120 --> 00:03:19,060
to this over here so we approximate that

00:03:16,630 --> 00:03:21,130
with the neural network and why is that

00:03:19,060 --> 00:03:24,430
an approximator because they're very

00:03:21,130 --> 00:03:25,720
rarely 100% accurate it's not an exact

00:03:24,430 --> 00:03:28,870
science there is a lot of

00:03:25,720 --> 00:03:31,060
experimentation scott bell ware who

00:03:28,870 --> 00:03:33,280
spoke earlier today he spoke last year

00:03:31,060 --> 00:03:36,340
at Lone Star as well and he gave a

00:03:33,280 --> 00:03:37,989
comparison of like the engineering

00:03:36,340 --> 00:03:39,640
console in a submarine I don't know if

00:03:37,989 --> 00:03:41,260
there's a single console in a submarine

00:03:39,640 --> 00:03:43,420
for engineering but the point was that

00:03:41,260 --> 00:03:45,610
you have this infinite array of dials

00:03:43,420 --> 00:03:47,470
and switches and valves or whatever and

00:03:45,610 --> 00:03:48,550
you can tweak all these little things to

00:03:47,470 --> 00:03:50,680
get the machine running just how you

00:03:48,550 --> 00:03:52,209
want to neural networks are kind of that

00:03:50,680 --> 00:03:54,940
way too there are all sorts of little

00:03:52,209 --> 00:03:56,769
things you can tweak and it just it

00:03:54,940 --> 00:03:59,080
takes experimentation to find what works

00:03:56,769 --> 00:04:01,030
it takes experimentation to find out if

00:03:59,080 --> 00:04:02,470
it's even going to work for your problem

00:04:01,030 --> 00:04:05,950
you don't even know that going into it

00:04:02,470 --> 00:04:07,180
which is you know great right so it's

00:04:05,950 --> 00:04:10,090
good to try to time box these

00:04:07,180 --> 00:04:12,730
investigations I suppose so how do they

00:04:10,090 --> 00:04:14,430
work this right here is an example of

00:04:12,730 --> 00:04:18,130
what a neural network might look like

00:04:14,430 --> 00:04:21,010
the circles are what we call nodes and

00:04:18,130 --> 00:04:22,840
over on the left where it's n sub I and

00:04:21,010 --> 00:04:25,050
that doesn't show up very well but that

00:04:22,840 --> 00:04:28,270
is AI for input those are your inputs

00:04:25,050 --> 00:04:29,979
you get to choose what those are subject

00:04:28,270 --> 00:04:32,530
to the constraint that it has to be a

00:04:29,979 --> 00:04:33,910
numeric value whatever your input is you

00:04:32,530 --> 00:04:36,310
have to somehow represent that as a

00:04:33,910 --> 00:04:38,860
number and then over on the very right

00:04:36,310 --> 00:04:40,630
you have output nodes you're going to

00:04:38,860 --> 00:04:42,550
have one or more of those

00:04:40,630 --> 00:04:43,870
obviously if you had no output nodes

00:04:42,550 --> 00:04:45,940
what's the point of having the neural

00:04:43,870 --> 00:04:47,260
network but there's no set number of

00:04:45,940 --> 00:04:50,110
them that you have to have that just

00:04:47,260 --> 00:04:52,660
depends on your problem those things in

00:04:50,110 --> 00:04:54,820
the middle then are zero or more what we

00:04:52,660 --> 00:04:56,400
call hidden neurons hidden in the sense

00:04:54,820 --> 00:05:00,490
that neither your input or output

00:04:56,400 --> 00:05:03,160
influences them directly now in this

00:05:00,490 --> 00:05:04,900
picture the three hidden layers there

00:05:03,160 --> 00:05:06,940
they all have four neurons they don't

00:05:04,900 --> 00:05:09,280
have to have the same number per layer

00:05:06,940 --> 00:05:12,570
of hidden neurons that's up to you but

00:05:09,280 --> 00:05:14,980
what they do is if you can imagine a

00:05:12,570 --> 00:05:17,230
neural network approximates a function

00:05:14,980 --> 00:05:18,760
and functions can be graphed right well

00:05:17,230 --> 00:05:20,260
the more hidden neurons that you have

00:05:18,760 --> 00:05:23,620
the more complicated of a graph your

00:05:20,260 --> 00:05:25,690
function can produce now unlike in metal

00:05:23,620 --> 00:05:29,110
music where more volume is always better

00:05:25,690 --> 00:05:30,370
in neural networks more hidden neurons

00:05:29,110 --> 00:05:32,620
are not always better

00:05:30,370 --> 00:05:34,180
it really depends on your application it

00:05:32,620 --> 00:05:35,920
depends on how much training data you

00:05:34,180 --> 00:05:38,260
have available we'll talk about what

00:05:35,920 --> 00:05:41,740
that is in just a second it depends on a

00:05:38,260 --> 00:05:44,200
lot of factors so input in training I

00:05:41,740 --> 00:05:46,150
mentioned those a minute ago the goal of

00:05:44,200 --> 00:05:49,360
a neural network is you have to train it

00:05:46,150 --> 00:05:50,680
to correctly recognize data and the way

00:05:49,360 --> 00:05:52,750
that it works is when you first

00:05:50,680 --> 00:05:54,400
initialize a network every one of those

00:05:52,750 --> 00:05:57,550
nodes is going to have some weight

00:05:54,400 --> 00:06:00,100
assigned to it and that's weight in the

00:05:57,550 --> 00:06:01,990
sense of importance not the hey wait a

00:06:00,100 --> 00:06:07,270
second type of weight for those of you

00:06:01,990 --> 00:06:09,700
who aren't reading so you basically you

00:06:07,270 --> 00:06:11,410
have some corpus of training data where

00:06:09,700 --> 00:06:12,880
you have your observations and you know

00:06:11,410 --> 00:06:15,700
what the output is already what you

00:06:12,880 --> 00:06:17,710
should expect so you take some subset of

00:06:15,700 --> 00:06:19,570
those and you feed them into your neural

00:06:17,710 --> 00:06:21,370
network and it's going to propagate

00:06:19,570 --> 00:06:23,710
through there and get to the end and

00:06:21,370 --> 00:06:26,350
most likely it will be wrong for a long

00:06:23,710 --> 00:06:28,330
time but there are algorithms that then

00:06:26,350 --> 00:06:30,070
go back and adjust that weight and one

00:06:28,330 --> 00:06:31,990
of them is back propagation and it

00:06:30,070 --> 00:06:34,300
basically just takes all the error from

00:06:31,990 --> 00:06:36,280
the more right side of the graph I'm

00:06:34,300 --> 00:06:38,440
sorry of the network and then feeds it

00:06:36,280 --> 00:06:41,560
back forward to help calculate the

00:06:38,440 --> 00:06:43,240
correct weights so I mean there's

00:06:41,560 --> 00:06:44,410
complicated math behind that that's well

00:06:43,240 --> 00:06:46,960
beyond the scope of what we're doing

00:06:44,410 --> 00:06:49,390
here but the gist of it is that it being

00:06:46,960 --> 00:06:51,220
wrong at first is okay that's how it

00:06:49,390 --> 00:06:53,260
learns kind of like you know how humans

00:06:51,220 --> 00:06:54,280
learn if you've ever learned to walk you

00:06:53,260 --> 00:06:58,240
probably made a lot of

00:06:54,280 --> 00:07:00,370
they accept first so here's just like a

00:06:58,240 --> 00:07:02,590
pseudocode e-type example if I were

00:07:00,370 --> 00:07:04,450
trying to teach a neural network the

00:07:02,590 --> 00:07:07,150
guide entity function I would give it

00:07:04,450 --> 00:07:10,090
input of you know one with an output of

00:07:07,150 --> 00:07:10,419
one two and two three and three and so

00:07:10,090 --> 00:07:13,450
forth

00:07:10,419 --> 00:07:16,180
and then ideally over time in iterations

00:07:13,450 --> 00:07:18,760
the network would learn oh if they had

00:07:16,180 --> 00:07:22,090
me for I should just produce four again

00:07:18,760 --> 00:07:23,560
and that's great then you have a learn

00:07:22,090 --> 00:07:25,750
to network which you could go deploy to

00:07:23,560 --> 00:07:28,060
your production servers just in case you

00:07:25,750 --> 00:07:31,800
needed to write an application that spit

00:07:28,060 --> 00:07:35,080
back exactly what people put into it so

00:07:31,800 --> 00:07:36,790
now how would you know how to apply a

00:07:35,080 --> 00:07:40,000
neural network to a problem or whether

00:07:36,790 --> 00:07:42,790
or not you should and like many of the

00:07:40,000 --> 00:07:46,510
answers I could give here today it's

00:07:42,790 --> 00:07:49,270
really hard to give a recipe for doing

00:07:46,510 --> 00:07:51,190
this I can give you one big gotcha to

00:07:49,270 --> 00:07:53,560
consider a couple of them actually the

00:07:51,190 --> 00:07:56,050
first one is more technical and that is

00:07:53,560 --> 00:07:58,210
let's say that your input is dealing

00:07:56,050 --> 00:08:00,729
with enumerated data let's say that for

00:07:58,210 --> 00:08:03,040
some reason whatever your for example

00:08:00,729 --> 00:08:04,810
one of your inputs is what type of food

00:08:03,040 --> 00:08:07,180
something is being Italian food Mexican

00:08:04,810 --> 00:08:09,100
food Korean food I don't know you pick

00:08:07,180 --> 00:08:11,560
it right well you have to reduce that

00:08:09,100 --> 00:08:13,090
all to some numeric input and let's say

00:08:11,560 --> 00:08:15,460
you choose to do that on a single node

00:08:13,090 --> 00:08:16,930
you might start assigning numbers to

00:08:15,460 --> 00:08:19,300
those different types of food like you

00:08:16,930 --> 00:08:21,820
might say Mexican food is zero Italian

00:08:19,300 --> 00:08:23,919
food is 0.5 or whatever right so you've

00:08:21,820 --> 00:08:24,310
created sort of a scale there but look

00:08:23,919 --> 00:08:26,710
out

00:08:24,310 --> 00:08:28,900
you've just introduced information there

00:08:26,710 --> 00:08:31,570
that may not be correct whatever food

00:08:28,900 --> 00:08:33,610
you put at zero to the network it's

00:08:31,570 --> 00:08:35,620
gonna be closer to whatever food is at

00:08:33,610 --> 00:08:38,800
point one then whatever food is at point

00:08:35,620 --> 00:08:40,120
seven and that's a bias is a good bad

00:08:38,800 --> 00:08:41,950
I don't know you'll have to experiment

00:08:40,120 --> 00:08:43,839
and find out but the point is you've

00:08:41,950 --> 00:08:47,110
told the network something about these

00:08:43,839 --> 00:08:48,460
foods that may or may not be true one

00:08:47,110 --> 00:08:50,440
way to overcome that then is for an

00:08:48,460 --> 00:08:52,180
enumerated type just give a separate

00:08:50,440 --> 00:08:54,940
node for each possible value and set it

00:08:52,180 --> 00:08:57,370
to one or zero that's up to you the

00:08:54,940 --> 00:08:59,050
other one is not a technical problem I

00:08:57,370 --> 00:09:01,990
worked at a company called seven billion

00:08:59,050 --> 00:09:03,339
people and the Ruby fan gem that we're

00:09:01,990 --> 00:09:05,709
going to talk about here was actually

00:09:03,339 --> 00:09:08,410
born from this company and basically

00:09:05,709 --> 00:09:08,690
what we did was you have we were taking

00:09:08,410 --> 00:09:10,820
clique

00:09:08,690 --> 00:09:12,470
stream data on websites using the actual

00:09:10,820 --> 00:09:14,570
words that people clicked on to try to

00:09:12,470 --> 00:09:16,760
infer things about the people and then

00:09:14,570 --> 00:09:18,620
dynamically change the website to be

00:09:16,760 --> 00:09:19,940
more communicative with them you know

00:09:18,620 --> 00:09:21,050
you have you ever been to a website

00:09:19,940 --> 00:09:22,160
you're just like I hate this thing I'm

00:09:21,050 --> 00:09:22,430
not even gonna read it and just shut it

00:09:22,160 --> 00:09:25,370
down

00:09:22,430 --> 00:09:26,810
without looking at it anyone I've done

00:09:25,370 --> 00:09:28,460
that a bunch of times we wanted to

00:09:26,810 --> 00:09:30,710
prevent that and we had a neural network

00:09:28,460 --> 00:09:32,210
behind it in the second iteration but

00:09:30,710 --> 00:09:34,250
the first time we just use a simple what

00:09:32,210 --> 00:09:36,020
we called inference engine and our CEO

00:09:34,250 --> 00:09:38,180
really liked the inference engine

00:09:36,020 --> 00:09:40,370
because it was predictable he knew

00:09:38,180 --> 00:09:42,080
exactly how it operated when you've

00:09:40,370 --> 00:09:44,720
trained a neural network let's say even

00:09:42,080 --> 00:09:46,390
if it's 100 percent accurate and you

00:09:44,720 --> 00:09:49,310
save that off do you know what you have

00:09:46,390 --> 00:09:51,370
you have a text file with some numbers

00:09:49,310 --> 00:09:53,540
of how many nodes and a bunch of weights

00:09:51,370 --> 00:09:55,490
so if you open that up even if you're

00:09:53,540 --> 00:09:57,560
the guy who wrote it and you look at

00:09:55,490 --> 00:09:59,570
that you say I had no idea how this

00:09:57,560 --> 00:10:01,550
thing works you're trying to secure

00:09:59,570 --> 00:10:03,440
investment dollars may not be the best

00:10:01,550 --> 00:10:05,690
way to go about doing that so but

00:10:03,440 --> 00:10:06,920
unfortunately every problem that we

00:10:05,690 --> 00:10:09,290
solve even though technical is

00:10:06,920 --> 00:10:11,330
eventually a human problem keep that in

00:10:09,290 --> 00:10:14,020
mind as well that may matter to you at

00:10:11,330 --> 00:10:17,420
your stage of whatever company you're at

00:10:14,020 --> 00:10:19,640
so those are just some general things to

00:10:17,420 --> 00:10:21,500
think about so now let's break it down

00:10:19,640 --> 00:10:24,050
into some specific problems I went at

00:10:21,500 --> 00:10:27,740
the first one I did was spam detection

00:10:24,050 --> 00:10:29,720
and emails now if any of you have ever

00:10:27,740 --> 00:10:30,830
worked in spam detection you already

00:10:29,720 --> 00:10:32,570
know that neural networks are a

00:10:30,830 --> 00:10:34,310
suboptimal solution to that problem

00:10:32,570 --> 00:10:36,440
so let's just skip that part right now

00:10:34,310 --> 00:10:38,060
and just say hey we are bent on using

00:10:36,440 --> 00:10:41,690
neural networks to solve this how might

00:10:38,060 --> 00:10:44,180
we do it this is where Ruby fan comes in

00:10:41,690 --> 00:10:45,770
and Ruby fan is two things

00:10:44,180 --> 00:10:47,030
Ruby you should already know what that

00:10:45,770 --> 00:10:49,580
is or else I don't know what you're

00:10:47,030 --> 00:10:52,100
doing here especially on a Saturday fan

00:10:49,580 --> 00:10:54,260
is fast artificial neural network it's a

00:10:52,100 --> 00:10:55,280
seed library that's its link you can go

00:10:54,260 --> 00:10:57,800
check it out it's great

00:10:55,280 --> 00:10:59,900
Ruby fan deniz Ruby bindings into that I

00:10:57,800 --> 00:11:02,720
was written by a guy named Steven Myers

00:10:59,900 --> 00:11:05,870
he runs tangled path now here in town

00:11:02,720 --> 00:11:07,100
it's good guy a friend of mine so we

00:11:05,870 --> 00:11:10,160
would obviously need to get that onto

00:11:07,100 --> 00:11:11,810
our computer and you can do that the

00:11:10,160 --> 00:11:13,910
same way you would install any gem gem

00:11:11,810 --> 00:11:16,070
install it stick it in your gem file or

00:11:13,910 --> 00:11:19,100
I don't know however else you install

00:11:16,070 --> 00:11:22,040
gems but those are the only two ways I

00:11:19,100 --> 00:11:22,570
use so if you don't happen to have fan

00:11:22,040 --> 00:11:24,160
installed on

00:11:22,570 --> 00:11:25,570
system Ruby fan will take care of that

00:11:24,160 --> 00:11:27,640
for you it'll go grab the source

00:11:25,570 --> 00:11:30,250
download it compile it and so forth and

00:11:27,640 --> 00:11:32,680
I don't if you've ever installed the

00:11:30,250 --> 00:11:35,290
Postgres gem you might really like the

00:11:32,680 --> 00:11:36,340
fact that ruby fan would take care of

00:11:35,290 --> 00:11:37,900
just installing its underlying

00:11:36,340 --> 00:11:40,270
dependence before you Postgres has

00:11:37,900 --> 00:11:41,860
always given me problems but I don't

00:11:40,270 --> 00:11:44,620
know we all have different experiences

00:11:41,860 --> 00:11:49,420
and I know this works on Ubuntu and OSX

00:11:44,620 --> 00:11:51,250
your mileage may vary on Windows so I do

00:11:49,420 --> 00:11:53,290
you have code up for these experiments

00:11:51,250 --> 00:11:55,090
yes that does say tic-tac-toe that's

00:11:53,290 --> 00:11:57,910
because I did two examples and I did

00:11:55,090 --> 00:12:01,300
tic-tac-toe first so the repository got

00:11:57,910 --> 00:12:03,100
named tic-tac-toe not spam detection so

00:12:01,300 --> 00:12:04,810
whatever so let's say you were gonna do

00:12:03,100 --> 00:12:07,810
it you were going to build a spam

00:12:04,810 --> 00:12:09,640
detector using a neural network open to

00:12:07,810 --> 00:12:11,740
you guys what inputs what

00:12:09,640 --> 00:12:17,800
characteristics of an email do you think

00:12:11,740 --> 00:12:20,530
might indicate spam or ham yeah that

00:12:17,800 --> 00:12:22,060
would be a great one to consider you

00:12:20,530 --> 00:12:24,510
could like link it to the sender it came

00:12:22,060 --> 00:12:28,930
from and start growing a history there

00:12:24,510 --> 00:12:32,350
any other thoughts you know that's

00:12:28,930 --> 00:12:35,920
awesome that who said that yeah that

00:12:32,350 --> 00:12:38,590
Akismet does that the great wordpress

00:12:35,920 --> 00:12:40,330
spam filtering plug-in you can link up

00:12:38,590 --> 00:12:41,740
to it as an independent api and ryan

00:12:40,330 --> 00:12:43,810
baits did that in went up sort of rails

00:12:41,740 --> 00:12:45,550
cass and that's actually how you test if

00:12:43,810 --> 00:12:47,740
you've wired up to the API correctly put

00:12:45,550 --> 00:12:49,090
viagra in there and if it doesn't say

00:12:47,740 --> 00:12:52,240
span you know you did something wrong

00:12:49,090 --> 00:12:53,770
but yeah that's exactly and going back

00:12:52,240 --> 00:12:55,570
to that original thing about enumerated

00:12:53,770 --> 00:12:57,610
data that would probably be like a

00:12:55,570 --> 00:13:02,470
single node on or off did it have viagra

00:12:57,610 --> 00:13:03,900
in there so any other thoughts yeah

00:13:02,470 --> 00:13:06,190
exactly

00:13:03,900 --> 00:13:07,990
you've seen how many different things

00:13:06,190 --> 00:13:11,140
there are for what seems like a rather

00:13:07,990 --> 00:13:13,900
simple problem I ended up with 82

00:13:11,140 --> 00:13:15,970
different inputs I didn't use the viagra

00:13:13,900 --> 00:13:18,130
one or the addresses for this particular

00:13:15,970 --> 00:13:20,440
problem it was inspired by an answer and

00:13:18,130 --> 00:13:22,450
Stack Overflow but examples of what I

00:13:20,440 --> 00:13:24,610
did consider our character count word

00:13:22,450 --> 00:13:26,590
count alphanumeric character count the

00:13:24,610 --> 00:13:28,990
ratio of alphanumeric characters to non

00:13:26,590 --> 00:13:30,760
so like you know links and stuff in

00:13:28,990 --> 00:13:33,550
there because you know it takes nine

00:13:30,760 --> 00:13:35,920
alpha numerics to make links other

00:13:33,550 --> 00:13:37,959
things is that I considered in there was

00:13:35,920 --> 00:13:40,600
like how many words only appeared once

00:13:37,959 --> 00:13:44,260
what was their ratio to the rest of the

00:13:40,600 --> 00:13:48,519
word count and so forth just all kinds

00:13:44,260 --> 00:13:50,470
of stuff and so I got a corpus of emails

00:13:48,519 --> 00:13:52,779
from that link up there at CES mining

00:13:50,470 --> 00:13:54,850
org great website if you're ever looking

00:13:52,779 --> 00:13:57,690
for dated to mine because they have lots

00:13:54,850 --> 00:14:01,180
of it I processed that into a database

00:13:57,690 --> 00:14:03,250
turning basically text into the relevant

00:14:01,180 --> 00:14:05,620
inputs and that code is in the github

00:14:03,250 --> 00:14:08,110
repo I'm assuming that most people here

00:14:05,620 --> 00:14:09,699
have opened a file in Ruby and that's

00:14:08,110 --> 00:14:11,410
about as complicated as that code gets

00:14:09,699 --> 00:14:13,570
not really worth actually putting up on

00:14:11,410 --> 00:14:16,600
a slide so if you want to find out how I

00:14:13,570 --> 00:14:19,180
did it go to the repo now here we get

00:14:16,600 --> 00:14:21,699
into the actual use of Ruby fan here it

00:14:19,180 --> 00:14:25,300
has a class called train data in the

00:14:21,699 --> 00:14:28,269
Ruby fan module there and you basically

00:14:25,300 --> 00:14:30,519
just give it input and the outputs that

00:14:28,269 --> 00:14:33,250
should be there right and those are

00:14:30,519 --> 00:14:34,389
parallel arrays if you've never don't

00:14:33,250 --> 00:14:35,560
know what a parallel array is it's

00:14:34,389 --> 00:14:36,220
essentially when you have one or more

00:14:35,560 --> 00:14:38,350
arrays

00:14:36,220 --> 00:14:40,180
they're all the same length and the enth

00:14:38,350 --> 00:14:41,589
element of each corresponds with one

00:14:40,180 --> 00:14:43,269
another it's not enforced by the

00:14:41,589 --> 00:14:44,589
language or anything and it can be kind

00:14:43,269 --> 00:14:46,300
of tricky to follow that if you're

00:14:44,589 --> 00:14:49,449
reading someone else's code but that's

00:14:46,300 --> 00:14:51,310
the way the gem works so here's an

00:14:49,449 --> 00:14:52,959
example of it those numbers I just made

00:14:51,310 --> 00:14:54,519
up to show that the ones are related to

00:14:52,959 --> 00:14:57,519
the ones and the twos are related to the

00:14:54,519 --> 00:15:00,420
twos and now all of our training data is

00:14:57,519 --> 00:15:02,560
encapsulated in one nice object which in

00:15:00,420 --> 00:15:05,230
just a second you'll see what we do with

00:15:02,560 --> 00:15:08,019
it we actually create our network here

00:15:05,230 --> 00:15:10,360
using the standard class and it takes a

00:15:08,019 --> 00:15:12,070
hash with three different key value

00:15:10,360 --> 00:15:14,769
pairs in it one is the number of inputs

00:15:12,070 --> 00:15:15,610
I already said that was 82 so that's why

00:15:14,769 --> 00:15:18,220
that numbers there

00:15:15,610 --> 00:15:20,620
the hidden neurons are an array where

00:15:18,220 --> 00:15:24,579
each element is how many hidden neurons

00:15:20,620 --> 00:15:26,800
go in that layer so here the zeroeth

00:15:24,579 --> 00:15:29,260
element there's the first layer of

00:15:26,800 --> 00:15:31,959
hidden neurons I put five in there and

00:15:29,260 --> 00:15:33,819
this was rather arbitrarily decided I

00:15:31,959 --> 00:15:36,089
did not do expensive extensive testing

00:15:33,819 --> 00:15:38,889
on that and in this particular example

00:15:36,089 --> 00:15:41,769
there's only one output was it a spam or

00:15:38,889 --> 00:15:43,510
not right so that just creates the

00:15:41,769 --> 00:15:46,810
structure of our network it still

00:15:43,510 --> 00:15:49,470
doesn't know anything so we trained it

00:15:46,810 --> 00:15:52,480
by using our trading data from before

00:15:49,470 --> 00:15:55,450
we just call this train on data method

00:15:52,480 --> 00:15:58,240
here and the other parameters that

00:15:55,450 --> 00:16:00,519
number of epochs that's just basically

00:15:58,240 --> 00:16:02,170
you have your big chunk of data right

00:16:00,519 --> 00:16:04,690
and you need to run through it to train

00:16:02,170 --> 00:16:07,180
your network that says how many times

00:16:04,690 --> 00:16:10,300
it's going to try to train itself to get

00:16:07,180 --> 00:16:12,490
within this error threshold before

00:16:10,300 --> 00:16:14,440
giving up if it can't get to your

00:16:12,490 --> 00:16:19,209
desired error threshold before that

00:16:14,440 --> 00:16:21,130
number it stops and then that ten is if

00:16:19,209 --> 00:16:22,899
you're running this from a console every

00:16:21,130 --> 00:16:25,959
tenth iteration and will output to the

00:16:22,899 --> 00:16:28,269
console how well it's doing now for what

00:16:25,959 --> 00:16:30,070
error threshold you want to have that

00:16:28,269 --> 00:16:32,920
still main dependent again like if

00:16:30,070 --> 00:16:34,600
you're doing something with tweets more

00:16:32,920 --> 00:16:36,760
error might be acceptable but if you're

00:16:34,600 --> 00:16:38,709
say running a space shuttles launch or

00:16:36,760 --> 00:16:41,010
something there's much less of a

00:16:38,709 --> 00:16:44,190
tolerance level there that you can have

00:16:41,010 --> 00:16:48,040
so I wrap that all up into a method and

00:16:44,190 --> 00:16:49,959
then tried training this and it was

00:16:48,040 --> 00:16:53,170
really interesting seeing this check out

00:16:49,959 --> 00:16:57,220
these error numbers that one in

00:16:53,170 --> 00:17:00,459
particular so it was always wrong when

00:16:57,220 --> 00:17:03,190
it was trying to train itself so why

00:17:00,459 --> 00:17:05,829
could that be bad inputs I mean

00:17:03,190 --> 00:17:08,199
obviously errors encoding it to generate

00:17:05,829 --> 00:17:09,669
the input to the neural network but it

00:17:08,199 --> 00:17:12,939
could have chosen bad inputs it could

00:17:09,669 --> 00:17:14,589
have been misclassified data I'm not

00:17:12,939 --> 00:17:18,309
blaming CS mining it's more likely I had

00:17:14,589 --> 00:17:19,540
bad inputs but you know that this is a

00:17:18,309 --> 00:17:20,860
good thing to have though because then

00:17:19,540 --> 00:17:22,780
you could start analyzing what went

00:17:20,860 --> 00:17:25,150
wrong is this the right solution what

00:17:22,780 --> 00:17:26,919
else could i tweak I could change the

00:17:25,150 --> 00:17:28,809
number of hidden neurons I could get

00:17:26,919 --> 00:17:32,950
additional inputs I could cut out some

00:17:28,809 --> 00:17:35,679
of the inputs and so forth and so that

00:17:32,950 --> 00:17:38,860
was I was fun to see how badly that was

00:17:35,679 --> 00:17:41,200
doing but then elsewhere in the code I

00:17:38,860 --> 00:17:43,240
wrapped up into this classify emails

00:17:41,200 --> 00:17:44,830
that took all of my data that wasn't

00:17:43,240 --> 00:17:47,140
used for training and I said okay now

00:17:44,830 --> 00:17:49,120
that we have a so-called trained neural

00:17:47,140 --> 00:17:51,700
network let's test it against other

00:17:49,120 --> 00:17:53,650
emails and see how well it does and the

00:17:51,700 --> 00:17:55,660
way I did that was when it was training

00:17:53,650 --> 00:17:58,660
a positive one meant it was a ham a

00:17:55,660 --> 00:18:01,570
negative one meant it was a spam so as

00:17:58,660 --> 00:18:02,440
an example when I actually gave it some

00:18:01,570 --> 00:18:04,930
new stuff to look

00:18:02,440 --> 00:18:08,410
it hovered right at zero which was the

00:18:04,930 --> 00:18:10,600
network saying yeah I don't know so in

00:18:08,410 --> 00:18:12,580
this particular example it worked about

00:18:10,600 --> 00:18:14,890
as well as random guessing would but hey

00:18:12,580 --> 00:18:19,300
at least it did no worse than random

00:18:14,890 --> 00:18:21,520
guessing right now going back to that

00:18:19,300 --> 00:18:24,010
earlier point neural networks are not

00:18:21,520 --> 00:18:27,400
the right approach where I define right

00:18:24,010 --> 00:18:28,810
as gives accurate results there's an

00:18:27,400 --> 00:18:30,760
article by Paul Graham on that that

00:18:28,810 --> 00:18:32,380
explains why it explains how Bayesian

00:18:30,760 --> 00:18:35,500
networks are a better choice for that

00:18:32,380 --> 00:18:37,390
but I think it's useful here to see the

00:18:35,500 --> 00:18:39,220
process of breaking the problem down and

00:18:37,390 --> 00:18:41,650
so forth and then seeing why it would

00:18:39,220 --> 00:18:44,050
fail right because if you go to apply

00:18:41,650 --> 00:18:47,230
this you may run into that same sort of

00:18:44,050 --> 00:18:58,330
issue any questions on the email side of

00:18:47,230 --> 00:19:12,820
it at this point Mike yes sir do you

00:18:58,330 --> 00:19:14,590
mean this um yeah um yeah I'm not I just

00:19:12,820 --> 00:19:17,440
assumed that Matt it was getting it

00:19:14,590 --> 00:19:19,510
wrong a whole lot so yeah maybe I could

00:19:17,440 --> 00:19:21,540
have just bit flicked it and called it a

00:19:19,510 --> 00:19:21,540
day

00:19:22,800 --> 00:19:29,290
yeah what there is some I forget there

00:19:27,520 --> 00:19:31,240
was some movie no as an episode of

00:19:29,290 --> 00:19:33,180
numbers of all things where there is

00:19:31,240 --> 00:19:35,170
this guy pretending to be a psychic and

00:19:33,180 --> 00:19:36,520
they were doing one of those experience

00:19:35,170 --> 00:19:38,980
experiments where they hold up a card

00:19:36,520 --> 00:19:40,420
and say what suit is this card and he

00:19:38,980 --> 00:19:41,980
was saying exactly the opposite one

00:19:40,420 --> 00:19:45,880
every single time over a hundred times

00:19:41,980 --> 00:19:47,350
just to antagonize Charlie Eppes but he

00:19:45,880 --> 00:19:52,230
was cheating and using a mirror anyway

00:19:47,350 --> 00:19:52,230
so anyway thank you for

00:20:02,810 --> 00:20:14,220
um yeah I think they do yeah that neural

00:20:12,840 --> 00:20:16,290
networks are good at that if you give it

00:20:14,220 --> 00:20:18,390
input that tends that doesn't actually

00:20:16,290 --> 00:20:21,210
have anything to do with the solution it

00:20:18,390 --> 00:20:22,500
will start to ignore that so that

00:20:21,210 --> 00:20:23,820
probably wouldn't have that great of an

00:20:22,500 --> 00:20:37,350
effect on attack you remove them because

00:20:23,820 --> 00:20:49,350
it'll do it anyway do you mean this one

00:20:37,350 --> 00:20:52,200
yeah they do that was just more of a

00:20:49,350 --> 00:20:53,970
like general structure picture yeah okay

00:20:52,200 --> 00:20:55,920
yeah definitely a backpropagation that's

00:20:53,970 --> 00:20:58,050
exactly how it works that the errors in

00:20:55,920 --> 00:21:00,810
the later nodes are fed back into the

00:20:58,050 --> 00:21:04,140
previous ones to try to get it correct

00:21:00,810 --> 00:21:08,480
any other questions or anything on that

00:21:04,140 --> 00:21:08,480
example let me get back to where we were

00:21:11,240 --> 00:21:16,290
all right cool so the next problem

00:21:14,310 --> 00:21:17,400
tic-tac-toe I thought this was a cool

00:21:16,290 --> 00:21:19,170
one because we all pretty much

00:21:17,400 --> 00:21:20,520
understand what tic-tac-toe is unless

00:21:19,170 --> 00:21:22,530
you're from England then it would be

00:21:20,520 --> 00:21:24,360
knots and crosses I guess is what they

00:21:22,530 --> 00:21:26,940
call it there I won't try the accent

00:21:24,360 --> 00:21:28,080
because that won't work very well this

00:21:26,940 --> 00:21:30,000
was a really cool problem though because

00:21:28,080 --> 00:21:32,070
I was able to take the same problem and

00:21:30,000 --> 00:21:35,040
model it with two different networks the

00:21:32,070 --> 00:21:37,050
first one basically just considered the

00:21:35,040 --> 00:21:38,880
board state as is where a negative one

00:21:37,050 --> 00:21:40,890
meant the opponent controlled it zero

00:21:38,880 --> 00:21:43,880
was neutral and a positive one was I

00:21:40,890 --> 00:21:45,870
control it and then the outputs were

00:21:43,880 --> 00:21:48,120
there were nine of them one for each

00:21:45,870 --> 00:21:50,250
square and essentially I gave a fitness

00:21:48,120 --> 00:21:51,750
for that one so whichever square gets

00:21:50,250 --> 00:21:53,670
the highest score is the move that you

00:21:51,750 --> 00:21:56,280
should take the drawback to that

00:21:53,670 --> 00:21:58,230
approach was I was hand programming the

00:21:56,280 --> 00:22:01,640
examples so the network was going to be

00:21:58,230 --> 00:22:03,690
no better at tic-tac-toe than I am and

00:22:01,640 --> 00:22:05,370
in my opinion the only perfect

00:22:03,690 --> 00:22:07,980
tic-tac-toe player is the one who

00:22:05,370 --> 00:22:08,909
doesn't play as the word work I'm sorry

00:22:07,980 --> 00:22:11,580
is the movie work

00:22:08,909 --> 00:22:13,200
Tata's great way to avoid thermonuclear

00:22:11,580 --> 00:22:14,419
warfare but there are much better ways

00:22:13,200 --> 00:22:17,639
to pass your time

00:22:14,419 --> 00:22:21,090
the second approach then it considered

00:22:17,639 --> 00:22:23,460
the board state and then what move was

00:22:21,090 --> 00:22:26,039
taken given that board state and then

00:22:23,460 --> 00:22:30,299
what the outcome of the match was given

00:22:26,039 --> 00:22:31,529
that state and that move and it got some

00:22:30,299 --> 00:22:34,649
really cool results which we'll get to

00:22:31,529 --> 00:22:36,149
in just a couple of slides but this code

00:22:34,649 --> 00:22:37,529
here is going through that first one

00:22:36,149 --> 00:22:40,139
that just considered the current board

00:22:37,529 --> 00:22:42,599
state and part of that of course was

00:22:40,139 --> 00:22:44,820
getting a CSV file into useable input

00:22:42,599 --> 00:22:47,519
there is no actual object called thing

00:22:44,820 --> 00:22:49,979
that converts CSV available that's just

00:22:47,519 --> 00:22:51,389
some presentational hand-waving to save

00:22:49,979 --> 00:22:53,519
that part of the code doesn't matter for

00:22:51,389 --> 00:22:55,619
today but same sort of thing here that

00:22:53,519 --> 00:22:57,389
spit out the thing that actually does

00:22:55,619 --> 00:22:58,830
that spit out a hash so I'm not doing

00:22:57,389 --> 00:23:02,639
something different there it was just a

00:22:58,830 --> 00:23:04,499
hash instead of local parallel arrays

00:23:02,639 --> 00:23:06,809
there in the function so we get the

00:23:04,499 --> 00:23:09,929
training data we get the network with

00:23:06,809 --> 00:23:12,229
the correct number of inputs and outputs

00:23:09,929 --> 00:23:15,899
and then of course that same arbitrary

00:23:12,229 --> 00:23:19,200
number of hidden neurons there's nothing

00:23:15,899 --> 00:23:23,099
magical about 5 & 5 at all and then we

00:23:19,200 --> 00:23:24,659
train it and so here is one of the

00:23:23,099 --> 00:23:28,590
interesting results this is the state of

00:23:24,659 --> 00:23:29,849
the board and then that's what I didn't

00:23:28,590 --> 00:23:32,700
put this code in the first bit of slides

00:23:29,849 --> 00:23:34,440
and I should have but once you have the

00:23:32,700 --> 00:23:36,749
trained Network and that's what fan is

00:23:34,440 --> 00:23:38,309
up there to actually then generate

00:23:36,749 --> 00:23:40,289
output given a set of input you just

00:23:38,309 --> 00:23:43,259
call the run method on it so I mean

00:23:40,289 --> 00:23:45,179
that's pretty intuitive I would say but

00:23:43,259 --> 00:23:47,249
so given this state of the board that's

00:23:45,179 --> 00:23:49,649
what it outputs so its first choice was

00:23:47,249 --> 00:23:51,059
the center Square which you know you

00:23:49,649 --> 00:23:53,789
have to make sure that it isn't already

00:23:51,059 --> 00:23:58,470
occupied but then it's next best choice

00:23:53,789 --> 00:24:00,960
was this one and it's X's turn and so

00:23:58,470 --> 00:24:05,369
you're thinking well why didn't it win

00:24:00,960 --> 00:24:06,749
why didn't it go for victory that was

00:24:05,369 --> 00:24:08,099
one of the observations I made it was

00:24:06,749 --> 00:24:11,129
hard to train the neural network to

00:24:08,099 --> 00:24:12,960
block an opponent or seize victory for

00:24:11,129 --> 00:24:15,809
whatever reason it just didn't go for it

00:24:12,960 --> 00:24:17,190
very well so then I ran a tournament

00:24:15,809 --> 00:24:18,749
there was actually a third computer

00:24:17,190 --> 00:24:20,909
opponent which just randomly picked a

00:24:18,749 --> 00:24:22,070
square because going back to the spam

00:24:20,909 --> 00:24:25,250
detection thing if you

00:24:22,070 --> 00:24:25,820
beat random guessing then that's bad

00:24:25,250 --> 00:24:29,420
right

00:24:25,820 --> 00:24:30,980
so I pitted each opponent against each

00:24:29,420 --> 00:24:34,310
of the others and gave them both the

00:24:30,980 --> 00:24:36,200
chance to be X&Y and so in a game of

00:24:34,310 --> 00:24:38,000
tic-tac-toe the X player has a huge

00:24:36,200 --> 00:24:40,790
advantage that's just the nature of the

00:24:38,000 --> 00:24:42,680
game and so random would beat the board

00:24:40,790 --> 00:24:46,520
State only opponent most of the time

00:24:42,680 --> 00:24:48,530
well ok when there was a victor player X

00:24:46,520 --> 00:24:50,630
tended to win in against the random

00:24:48,530 --> 00:24:53,240
opponent but there were a significant

00:24:50,630 --> 00:24:55,970
number of draws as well but you see that

00:24:53,240 --> 00:24:59,090
even though the board state with result

00:24:55,970 --> 00:25:01,460
1 was why it's still won a whole lot and

00:24:59,090 --> 00:25:02,960
then you can see how it breaks down but

00:25:01,460 --> 00:25:05,360
what was really cool to see he was the

00:25:02,960 --> 00:25:07,610
board state with result absolutely

00:25:05,360 --> 00:25:10,910
trounced the board state only player he

00:25:07,610 --> 00:25:12,230
won every time whether it was X or Y so

00:25:10,910 --> 00:25:13,580
I thought that was interesting that

00:25:12,230 --> 00:25:15,080
probably has a lot to do with the fact

00:25:13,580 --> 00:25:17,180
that that opponent had a lot more data

00:25:15,080 --> 00:25:18,770
to learn from to generate the data for

00:25:17,180 --> 00:25:20,810
that I pitted two randoms against each

00:25:18,770 --> 00:25:22,700
other and played him a thousand times so

00:25:20,810 --> 00:25:25,460
it really got to have a lot of exposure

00:25:22,700 --> 00:25:28,700
to tic-tac-toe games whereas the board

00:25:25,460 --> 00:25:32,060
state only guy had like 27 examples to

00:25:28,700 --> 00:25:33,710
go off of and I don't know it'd be kind

00:25:32,060 --> 00:25:36,260
of hard to learn tic-tac-toe after 27

00:25:33,710 --> 00:25:39,200
moves I suppose if you're a computer but

00:25:36,260 --> 00:25:42,770
I don't know that was breaking that down

00:25:39,200 --> 00:25:44,690
pretty interesting so he's going back to

00:25:42,770 --> 00:25:47,390
the fact that there are so many things

00:25:44,690 --> 00:25:48,740
you can tweak in neural networks it's a

00:25:47,390 --> 00:25:51,350
really good idea to look at the relief

00:25:48,740 --> 00:25:53,480
and documentation and one page in

00:25:51,350 --> 00:25:55,130
particular of that this shows up really

00:25:53,480 --> 00:25:57,470
poorly on the slide in my apologies for

00:25:55,130 --> 00:26:00,140
that but if you remember back to the

00:25:57,470 --> 00:26:02,120
graph each of those nodes has an

00:26:00,140 --> 00:26:03,560
activation function on it that basically

00:26:02,120 --> 00:26:05,900
affects how it deals with the numeric

00:26:03,560 --> 00:26:07,640
inputs that come in and for example

00:26:05,900 --> 00:26:09,680
there is one that will squash all the

00:26:07,640 --> 00:26:11,900
data into the negative 1 to 1 range is

00:26:09,680 --> 00:26:12,860
that good or bad I don't know it depends

00:26:11,900 --> 00:26:15,650
on your application and the

00:26:12,860 --> 00:26:17,930
experimentation that you do but every

00:26:15,650 --> 00:26:19,640
single one of these symbols is another

00:26:17,930 --> 00:26:22,040
option for an activation function that

00:26:19,640 --> 00:26:24,170
you could have and you can tweak that on

00:26:22,040 --> 00:26:25,670
the a per node basis you can do it per

00:26:24,170 --> 00:26:27,620
layer of nodes or you can do it for the

00:26:25,670 --> 00:26:28,970
whole network so you really have no

00:26:27,620 --> 00:26:31,130
shortage of things to experiment with

00:26:28,970 --> 00:26:32,720
and if you were really enterprising you

00:26:31,130 --> 00:26:34,460
could probably just generate a big loop

00:26:32,720 --> 00:26:35,840
that would go through all of those

00:26:34,460 --> 00:26:38,570
and then spit it all out and find out

00:26:35,840 --> 00:26:41,810
which one works the best for you but

00:26:38,570 --> 00:26:45,800
again tine box it because that is the

00:26:41,810 --> 00:26:47,300
rabbit's hole of rabbit holes so other

00:26:45,800 --> 00:26:49,970
applications that you could use neural

00:26:47,300 --> 00:26:52,070
networks for facial recognition it's

00:26:49,970 --> 00:26:55,190
been used for that handwriting

00:26:52,070 --> 00:26:57,590
recognition driving a read an example of

00:26:55,190 --> 00:26:59,030
like Toyota they were trying to help

00:26:57,590 --> 00:27:01,550
alert drivers whether or not they were

00:26:59,030 --> 00:27:03,560
about to have a collision and they base

00:27:01,550 --> 00:27:05,990
that off things like distance to nearest

00:27:03,560 --> 00:27:07,790
objects and so forth you could maybe

00:27:05,990 --> 00:27:09,470
even do a gesture recognition and not

00:27:07,790 --> 00:27:11,150
like you know that kind of gesture but

00:27:09,470 --> 00:27:13,730
the kind that you have on your

00:27:11,150 --> 00:27:16,340
smartphone or whatever so one way you

00:27:13,730 --> 00:27:18,830
might do that is breaking your screen

00:27:16,340 --> 00:27:21,410
down into a grid the blue dots represent

00:27:18,830 --> 00:27:23,450
where the the first points of contact

00:27:21,410 --> 00:27:25,160
for each finger were and then trace out

00:27:23,450 --> 00:27:26,450
the rest of the path and then every

00:27:25,160 --> 00:27:29,210
single one of those squares could be an

00:27:26,450 --> 00:27:31,700
input to your network now that's a whole

00:27:29,210 --> 00:27:33,580
lot of input so maybe you'd do a larger

00:27:31,700 --> 00:27:36,170
resolution on they're not quite so fine

00:27:33,580 --> 00:27:37,550
but I don't know what do you guys think

00:27:36,170 --> 00:27:40,460
if you were to take on one of these

00:27:37,550 --> 00:27:43,820
problems what sorts of inputs might you

00:27:40,460 --> 00:27:45,380
consider no all right then let's go back

00:27:43,820 --> 00:27:48,310
to the go forth and work much awesome

00:27:45,380 --> 00:27:48,310

YouTube URL: https://www.youtube.com/watch?v=M9w7OMxmRv4


