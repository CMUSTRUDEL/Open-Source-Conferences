Title: Berlin Buzzwords 2017: Michael Noll - Rethinking Stream Processing with Apache Kafka #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Michael Noll talking about "Rethinking Stream Processing with Apache Kafka: Applications vs. Clusters, Streams vs. Databases".

Modern businesses have data at their core, and this data is changing continuously. How can we harness this torrent of information in real-time? The answer is stream processing, and the technology that has since become the core platform for streaming data is Apache Kafka. Among the thousands of companies that use Kafka to transform and reshape their industries are the likes of Netflix, Uber, PayPal, and AirBnB, but also established players such as Goldman Sachs, Cisco, and Oracle.

Unfortunately, today’s common architectures for real-time data processing at scale suffer from complexity: there are many technologies that need to be stitched and operated together, and each individual technology is often complex by itself. This has led to a strong discrepancy between how we, as engineers, would like to work vs. how we actually end up working in practice.

In this session we talk about how Apache Kafka helps you to radically simplify your data architectures. We cover how you can now build normal applications to serve your real-time processing needs — rather than building clusters or similar special-purpose infrastructure — and still benefit from properties such as high scalability, distributed computing, and fault-tolerance, which are typically associated exclusively with cluster technologies. 

This talk is presented by our Gold partner Confluent.

Read more:
https://2017.berlinbuzzwords.de/17/session/rethinking-stream-processing-apache-kafka-applications-vs-clusters-streams-vs-databases

About Michael Noll:
https://2017.berlinbuzzwords.de/users/michael-noll

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,370 --> 00:00:12,000
I haven't even started yet you know you

00:00:08,460 --> 00:00:13,260
don't know what you're in for right yeah

00:00:12,000 --> 00:00:16,710
thanks for the introduction

00:00:13,260 --> 00:00:17,910
a few words about myself I'm an engineer

00:00:16,710 --> 00:00:21,029
turned product manager working at

00:00:17,910 --> 00:00:22,739
confluent which is the company founded

00:00:21,029 --> 00:00:26,160
by the creators of Apache Kafka we're

00:00:22,739 --> 00:00:31,590
based in Palo Alto on the u.s. west

00:00:26,160 --> 00:00:32,910
coast and as I said I'm not doing

00:00:31,590 --> 00:00:34,440
product management work which is bit

00:00:32,910 --> 00:00:36,390
different to what I did before I am a

00:00:34,440 --> 00:00:38,339
contributor to Apache Kafka obviously

00:00:36,390 --> 00:00:40,079
I'm also committed to projects like

00:00:38,339 --> 00:00:42,300
Apache storm and a few others but

00:00:40,079 --> 00:00:46,019
nowadays most of my work is is really

00:00:42,300 --> 00:00:48,089
focused on Kafka so before we start let

00:00:46,019 --> 00:00:49,589
me ask a question though who of you here

00:00:48,089 --> 00:00:52,440
in the audience is familiar with Apache

00:00:49,589 --> 00:00:58,140
Kafka all right well let me take a

00:00:52,440 --> 00:01:00,210
picture okay maybe that was one person

00:00:58,140 --> 00:01:04,589
that is not familiar with Apache Kafka

00:01:00,210 --> 00:01:06,479
so for that person let me briefly show

00:01:04,589 --> 00:01:08,400
this slide here and what I want to show

00:01:06,479 --> 00:01:10,080
here is that Kafka started out as a

00:01:08,400 --> 00:01:11,700
messaging system a pop subsystem

00:01:10,080 --> 00:01:12,930
whatever you want to call it a few years

00:01:11,700 --> 00:01:14,850
ago but nowadays it's like a

00:01:12,930 --> 00:01:16,410
full-fledged stream platform which you

00:01:14,850 --> 00:01:18,240
can also use to process the data in

00:01:16,410 --> 00:01:20,100
casco for example or integrate cupca

00:01:18,240 --> 00:01:22,760
with you know technology such as you

00:01:20,100 --> 00:01:25,200
know Hadoop Cassandra and so on

00:01:22,760 --> 00:01:27,660
and so in this talk I will focus on the

00:01:25,200 --> 00:01:29,730
part that edit stream processing

00:01:27,660 --> 00:01:32,730
capabilities to Apache Kafka the so

00:01:29,730 --> 00:01:35,400
called Kafka streams API so another

00:01:32,730 --> 00:01:39,360
motivational example imagine we were to

00:01:35,400 --> 00:01:41,100
build a hotel business we would need to

00:01:39,360 --> 00:01:43,620
implement a few roles such as a security

00:01:41,100 --> 00:01:44,820
guard and on the technology side we

00:01:43,620 --> 00:01:46,380
could do this with let's say a single

00:01:44,820 --> 00:01:49,020
application or micro service that runs

00:01:46,380 --> 00:01:50,670
on one machine of course relatives a bit

00:01:49,020 --> 00:01:53,310
more complex so eventually we need to

00:01:50,670 --> 00:01:55,590
add further roles like receptionist room

00:01:53,310 --> 00:01:57,600
service and so on again you know more

00:01:55,590 --> 00:02:01,920
applications running on you know more of

00:01:57,600 --> 00:02:03,330
these machines this is already

00:02:01,920 --> 00:02:05,010
complicated but it gets even further

00:02:03,330 --> 00:02:07,110
complicated on the technology side as

00:02:05,010 --> 00:02:08,459
soon as we need to scale out so if you

00:02:07,110 --> 00:02:09,780
reach the point where one security guard

00:02:08,459 --> 00:02:11,879
is not enough but we need a team of

00:02:09,780 --> 00:02:13,799
security guards suddenly we need to run

00:02:11,879 --> 00:02:16,290
distributed applications across many

00:02:13,799 --> 00:02:18,299
machines VMs or containers and at that

00:02:16,290 --> 00:02:19,230
point it becomes to believe very

00:02:18,299 --> 00:02:20,730
challenging for us

00:02:19,230 --> 00:02:24,930
because then you're solving a

00:02:20,730 --> 00:02:27,300
distributed systems problem and what we

00:02:24,930 --> 00:02:30,269
realized when we set out to design the

00:02:27,300 --> 00:02:31,680
kafka streams API for Apache Tosca was

00:02:30,269 --> 00:02:33,239
that we wanted to give something to

00:02:31,680 --> 00:02:35,160
developers that is part of the solution

00:02:33,239 --> 00:02:36,540
and not part of the problem because

00:02:35,160 --> 00:02:38,099
oftentimes this with your yak shaving

00:02:36,540 --> 00:02:39,510
you have your actual problem and you

00:02:38,099 --> 00:02:41,310
start digging into these technologies

00:02:39,510 --> 00:02:42,840
and it gets even more complicated the

00:02:41,310 --> 00:02:46,680
more you add to your like portfolio of

00:02:42,840 --> 00:02:48,989
technologies and I think a key part that

00:02:46,680 --> 00:02:50,790
we realized is that as developers we

00:02:48,989 --> 00:02:54,900
want to build applications we don't want

00:02:50,790 --> 00:02:57,239
an infrastructure or clusters so we want

00:02:54,900 --> 00:02:58,860
our applications still to be a bit like

00:02:57,239 --> 00:03:01,680
these new technologies we want them to

00:02:58,860 --> 00:03:04,440
be elastic scalable for taller and you

00:03:01,680 --> 00:03:06,120
know and if you know of these things but

00:03:04,440 --> 00:03:08,700
at the same time we wanted to question

00:03:06,120 --> 00:03:11,310
know the commonly held belief do we need

00:03:08,700 --> 00:03:13,349
separate shared processing clusters do

00:03:11,310 --> 00:03:16,769
we need separate databases if you want

00:03:13,349 --> 00:03:18,780
to do stateful processing on streams in

00:03:16,769 --> 00:03:20,430
Casca do we need something like reddit

00:03:18,780 --> 00:03:24,389
or Cassandra just in order to do some

00:03:20,430 --> 00:03:26,310
accounting for example so essentially

00:03:24,389 --> 00:03:28,530
what we want to do is something like

00:03:26,310 --> 00:03:30,389
this analogy we want to enjoy the taste

00:03:28,530 --> 00:03:31,920
of beer but we don't want to have the

00:03:30,389 --> 00:03:36,420
alcohol because we want to still you

00:03:31,920 --> 00:03:38,400
know try it home safely and this is what

00:03:36,420 --> 00:03:41,370
led to the creation of the kafka streams

00:03:38,400 --> 00:03:42,930
API and I will talk a little bit mod now

00:03:41,370 --> 00:03:44,250
there's the androids that we did the

00:03:42,930 --> 00:03:47,370
things that you can do with it and so on

00:03:44,250 --> 00:03:49,079
but the point here is that it is a tool

00:03:47,370 --> 00:03:51,120
so that you can go to recent application

00:03:49,079 --> 00:03:53,310
support your company's core business if

00:03:51,120 --> 00:03:55,530
this thing isn't working your company

00:03:53,310 --> 00:03:56,940
isn't working and that meant for example

00:03:55,530 --> 00:03:58,319
that we build fault tolerance built-in

00:03:56,940 --> 00:04:00,780
it's not something that you need to

00:03:58,319 --> 00:04:02,870
enable in an obscure way you explicitly

00:04:00,780 --> 00:04:05,040
need to disable it to get rid of it

00:04:02,870 --> 00:04:06,260
say for elasticity in zone so we talked

00:04:05,040 --> 00:04:08,280
about it in a second

00:04:06,260 --> 00:04:10,829
so you might be familiar with this

00:04:08,280 --> 00:04:12,870
concept of enriched my library say if

00:04:10,829 --> 00:04:14,220
you're using Scala and you're turning

00:04:12,870 --> 00:04:16,650
this upside down we're giving you a

00:04:14,220 --> 00:04:18,599
library with the casco streams API that

00:04:16,650 --> 00:04:22,079
enriches your applications and what does

00:04:18,599 --> 00:04:24,419
that mean it means you get features such

00:04:22,079 --> 00:04:25,860
as a cluster to go I have an example in

00:04:24,419 --> 00:04:28,080
the next slide the kind of did a

00:04:25,860 --> 00:04:29,970
database to go you can run it everywhere

00:04:28,080 --> 00:04:31,969
you can use it for very small use cases

00:04:29,970 --> 00:04:34,189
all the way to very large use case and

00:04:31,969 --> 00:04:35,809
ually go in between those you get

00:04:34,189 --> 00:04:37,819
exactly ones processing even under

00:04:35,809 --> 00:04:39,799
failure you can do in your event Time

00:04:37,819 --> 00:04:43,189
Protocol joins aggregation and so on and

00:04:39,799 --> 00:04:45,289
a whole lot of more so as an example

00:04:43,189 --> 00:04:47,179
here's an application that uses the cast

00:04:45,289 --> 00:04:48,649
of streams API and I just want to

00:04:47,179 --> 00:04:50,089
highlight this is everything that

00:04:48,649 --> 00:04:51,649
doesn't look to run in the koshkas

00:04:50,089 --> 00:04:53,179
servers it doesn't run in the Kafka

00:04:51,649 --> 00:04:55,399
progress but at the pyramid of it in a

00:04:53,179 --> 00:04:57,349
Vienna container or something like that

00:04:55,399 --> 00:04:58,939
and if we want to scale out this

00:04:57,349 --> 00:05:01,759
application we just spawn additional

00:04:58,939 --> 00:05:03,319
instances of the application so I'll

00:05:01,759 --> 00:05:05,329
talk about that later on but the point

00:05:03,319 --> 00:05:07,549
here is that this should be super easy

00:05:05,329 --> 00:05:12,079
you shouldn't install yarn just to run

00:05:07,549 --> 00:05:13,789
it on two machines for example so

00:05:12,079 --> 00:05:15,799
motivating example here

00:05:13,789 --> 00:05:19,159
imagine your task to create a real-time

00:05:15,799 --> 00:05:21,019
analytics paper like this one my common

00:05:19,159 --> 00:05:22,579
use case would be no cybersecurity you

00:05:21,019 --> 00:05:24,469
want to monitor whether any of your

00:05:22,579 --> 00:05:26,659
services is currently under attack by

00:05:24,469 --> 00:05:29,539
some adversary how would you implement

00:05:26,659 --> 00:05:30,649
this use case so typically what people

00:05:29,539 --> 00:05:34,039
have done and I've done that didn't pass

00:05:30,649 --> 00:05:35,899
myself you would collect data in real

00:05:34,039 --> 00:05:37,939
time in order to additional analytics

00:05:35,899 --> 00:05:39,889
and so on then you would stand up a

00:05:37,939 --> 00:05:42,169
separate processing cluster you know

00:05:39,889 --> 00:05:45,139
like spark or storm then you would

00:05:42,169 --> 00:05:46,339
submit a job to that cluster and some of

00:05:45,139 --> 00:05:48,229
the cluster would write the results back

00:05:46,339 --> 00:05:50,209
to a database and then you would have a

00:05:48,229 --> 00:05:51,649
front-end application that would create

00:05:50,209 --> 00:05:55,489
the database in order to show some

00:05:51,649 --> 00:05:57,559
results like that roadmap now what's the

00:05:55,489 --> 00:05:59,059
problem with that approach when the

00:05:57,559 --> 00:06:00,409
problem is and I think this is one of

00:05:59,059 --> 00:06:02,929
the key problems that are not necessary

00:06:00,409 --> 00:06:05,749
technological technique technical

00:06:02,929 --> 00:06:07,009
problems but people problems here we

00:06:05,749 --> 00:06:10,339
what you can see is like the parts in

00:06:07,009 --> 00:06:11,869
clue this is your application it spread

00:06:10,339 --> 00:06:13,519
all over the place it spread across

00:06:11,869 --> 00:06:15,979
systems and typically it's spread across

00:06:13,519 --> 00:06:17,509
organizational boundaries like there is

00:06:15,979 --> 00:06:19,369
a cluster team that runs the cluster a

00:06:17,509 --> 00:06:21,199
database team that runs the database and

00:06:19,369 --> 00:06:22,819
then you you are the lonely soul that

00:06:21,199 --> 00:06:25,059
tries to stitch everything together just

00:06:22,819 --> 00:06:27,769
in order to make this simple dashboard

00:06:25,059 --> 00:06:29,509
so with Kafka streams you can collapse

00:06:27,769 --> 00:06:31,219
all of that into single application set

00:06:29,509 --> 00:06:33,499
if you want to you can docker eyes and

00:06:31,219 --> 00:06:35,119
then deploying kubernetes you can use it

00:06:33,499 --> 00:06:36,679
locally on your laptop in order to test

00:06:35,119 --> 00:06:39,439
it and so on I have few more examples

00:06:36,679 --> 00:06:42,439
like that so one point here is that you

00:06:39,439 --> 00:06:44,719
can simplify your architecture a lot but

00:06:42,439 --> 00:06:45,830
it also has radical implications how we

00:06:44,719 --> 00:06:47,390
actually developed

00:06:45,830 --> 00:06:48,800
deploy your applications in the first

00:06:47,390 --> 00:06:53,000
place and that is what I want to focus

00:06:48,800 --> 00:06:54,590
on but before I talk about that how do

00:06:53,000 --> 00:06:56,270
you actually write an application in the

00:06:54,590 --> 00:06:58,460
Calico streams API well you have two

00:06:56,270 --> 00:07:00,140
options if you have a DSL which is what

00:06:58,460 --> 00:07:01,670
is shown on the left this is more in a

00:07:00,140 --> 00:07:03,350
functional programming style way to

00:07:01,670 --> 00:07:05,810
write your application bit like new

00:07:03,350 --> 00:07:07,520
Scala collections for example and then

00:07:05,810 --> 00:07:09,950
you have the processor API on the right

00:07:07,520 --> 00:07:12,410
which is often used for more complex

00:07:09,950 --> 00:07:14,690
event processing the cool thing is you

00:07:12,410 --> 00:07:16,910
can actually combine the two so you can

00:07:14,690 --> 00:07:18,830
have like the normal DSL flow on the

00:07:16,910 --> 00:07:20,210
left but there are some certain steps in

00:07:18,830 --> 00:07:21,440
there that you need to optimize it with

00:07:20,210 --> 00:07:27,140
something very specific to your company

00:07:21,440 --> 00:07:29,740
and you can just plug those in so let's

00:07:27,140 --> 00:07:32,450
take the no canonical word count example

00:07:29,740 --> 00:07:34,730
simple thing we get inputs as text lines

00:07:32,450 --> 00:07:36,640
and then we want to split the text lines

00:07:34,730 --> 00:07:40,520
into verse and counter we've seen word

00:07:36,640 --> 00:07:42,760
now how does that problem translate to

00:07:40,520 --> 00:07:45,980
how you would work with Kafka strange

00:07:42,760 --> 00:07:47,780
first if you want to develop this

00:07:45,980 --> 00:07:50,540
application you can do it on linux mac

00:07:47,780 --> 00:07:52,490
and windows I think Linux and Mac we can

00:07:50,540 --> 00:07:53,930
take for granted nowadays but oftentimes

00:07:52,490 --> 00:07:56,420
people are stuck on Windows if they're

00:07:53,930 --> 00:07:58,280
working for a bank or an insurance

00:07:56,420 --> 00:07:59,930
company in these other known tightly

00:07:58,280 --> 00:08:01,400
regulated industries so you can use all

00:07:59,930 --> 00:08:04,640
of these no common operating systems to

00:08:01,400 --> 00:08:07,370
do that here's the key code that you

00:08:04,640 --> 00:08:09,560
would write first line is the one that

00:08:07,370 --> 00:08:11,000
reads from capita in the scale away the

00:08:09,560 --> 00:08:12,380
second statement is the actual word

00:08:11,000 --> 00:08:14,570
count and the third line at the bottom

00:08:12,380 --> 00:08:16,880
is writing back to Kafka in the scale or

00:08:14,570 --> 00:08:19,130
where this is not super interesting to

00:08:16,880 --> 00:08:21,380
talk about but there is something I want

00:08:19,130 --> 00:08:22,910
to highlight hard to see but we have

00:08:21,380 --> 00:08:28,670
something called a case to indicate

00:08:22,910 --> 00:08:31,520
table what is that so the thing that you

00:08:28,670 --> 00:08:33,530
realize when you start to get motivated

00:08:31,520 --> 00:08:35,590
to look into Kafka is that often times

00:08:33,530 --> 00:08:37,700
you realize my business is actually

00:08:35,590 --> 00:08:40,430
real-time screens of information from

00:08:37,700 --> 00:08:41,630
customers users and so on so like an

00:08:40,430 --> 00:08:43,460
ongoing conversation break and walk

00:08:41,630 --> 00:08:45,440
between your customer so screens are

00:08:43,460 --> 00:08:47,240
everywhere people here are familiar with

00:08:45,440 --> 00:08:49,310
Kafka so I assume that you kind of

00:08:47,240 --> 00:08:52,160
understand the idea of streams of data

00:08:49,310 --> 00:08:53,450
and do this in real time but the other

00:08:52,160 --> 00:08:55,250
thing that is important to understand is

00:08:53,450 --> 00:08:57,590
that tables and databases are everywhere

00:08:55,250 --> 00:08:58,940
- there's a reason why we've been using

00:08:57,590 --> 00:09:00,550
databases for

00:08:58,940 --> 00:09:04,610
kids in order to build applications

00:09:00,550 --> 00:09:06,890
products and so on so the key point here

00:09:04,610 --> 00:09:09,730
is to realize that in practice you need

00:09:06,890 --> 00:09:11,750
both streams and tables all the time

00:09:09,730 --> 00:09:14,930
whatever you're doing whether your

00:09:11,750 --> 00:09:17,540
logistics and finance in fraud whatever

00:09:14,930 --> 00:09:19,430
you need both strings in tables to order

00:09:17,540 --> 00:09:20,960
to solve your problem so if you pick a

00:09:19,430 --> 00:09:22,760
stream processing technology this would

00:09:20,960 --> 00:09:26,060
be first last concept in the API for

00:09:22,760 --> 00:09:29,170
example even for word count

00:09:26,060 --> 00:09:31,040
you need streams and tables and

00:09:29,170 --> 00:09:32,780
something important to realize here is

00:09:31,040 --> 00:09:34,940
that there is a close relationship

00:09:32,780 --> 00:09:36,290
between the two if you look in the left

00:09:34,940 --> 00:09:38,120
column there is a table that is

00:09:36,290 --> 00:09:40,220
undergoing mutations if you capture

00:09:38,120 --> 00:09:42,530
those mutations mutations into a stream

00:09:40,220 --> 00:09:44,240
a change structuring of the table you

00:09:42,530 --> 00:09:46,280
can reconstruct the table as where and

00:09:44,240 --> 00:09:47,930
we're exploiting that functionality in

00:09:46,280 --> 00:09:50,300
numerous ways behind the scenes in

00:09:47,930 --> 00:09:51,770
Apache Casca and we also give you the

00:09:50,300 --> 00:09:55,040
option to exploit it in your own

00:09:51,770 --> 00:09:56,390
application logic so for example in

00:09:55,040 --> 00:09:58,010
capita streams you have a case tree and

00:09:56,390 --> 00:09:59,330
a cake table you have operations that

00:09:58,010 --> 00:10:01,990
take you back and forth between those

00:09:59,330 --> 00:10:04,730
two or those that retain the shape of it

00:10:01,990 --> 00:10:08,120
where count is a trivial example where

00:10:04,730 --> 00:10:09,860
you already need this so for example

00:10:08,120 --> 00:10:11,870
sometimes you have a stream and you want

00:10:09,860 --> 00:10:13,280
a table like you want to get customer

00:10:11,870 --> 00:10:15,290
information in real time and you want to

00:10:13,280 --> 00:10:17,180
build a continuously updating profile of

00:10:15,290 --> 00:10:19,760
every customer from a variety of sources

00:10:17,180 --> 00:10:22,790
this is taking a stream and outputting a

00:10:19,760 --> 00:10:24,530
table sometimes you have a table and you

00:10:22,790 --> 00:10:26,680
want to get back a stream for examle in

00:10:24,530 --> 00:10:28,670
order to make real-time alerts work

00:10:26,680 --> 00:10:30,110
there are use cases where you have a

00:10:28,670 --> 00:10:33,230
stream in the table you started with the

00:10:30,110 --> 00:10:35,090
table get a changelog stream transform

00:10:33,230 --> 00:10:36,770
that stream and create a new table like

00:10:35,090 --> 00:10:42,020
a new materialized view of the original

00:10:36,770 --> 00:10:44,480
table or you have a stream of data

00:10:42,020 --> 00:10:46,310
coming in and you want to enrich this

00:10:44,480 --> 00:10:49,550
incoming stream with additional context

00:10:46,310 --> 00:10:51,860
information like customer information

00:10:49,550 --> 00:10:53,480
about information about the customer

00:10:51,860 --> 00:10:55,880
that just paid something on you know

00:10:53,480 --> 00:10:57,230
Apple in the Apple store if there's a

00:10:55,880 --> 00:10:59,480
fraudulent transaction or not if this

00:10:57,230 --> 00:11:01,520
customer is never paid out of I don't

00:10:59,480 --> 00:11:02,870
know Australia and so far only paid out

00:11:01,520 --> 00:11:06,010
of Europe then most probably the

00:11:02,870 --> 00:11:06,010
Australian transaction is fraudulent

00:11:06,070 --> 00:11:11,600
so going back to the work out an example

00:11:08,860 --> 00:11:12,290
typically you know other talks stop here

00:11:11,600 --> 00:11:13,940
show

00:11:12,290 --> 00:11:16,009
these few lines and say that it's just

00:11:13,940 --> 00:11:19,459
one line or two lines but let's do out

00:11:16,009 --> 00:11:20,540
of this a bit what do we need to do in

00:11:19,459 --> 00:11:22,490
order to actually deploy this in

00:11:20,540 --> 00:11:24,920
production in casco screens you only

00:11:22,490 --> 00:11:26,209
need a little bit to add first we need

00:11:24,920 --> 00:11:29,449
to configure for example where to find

00:11:26,209 --> 00:11:32,569
Casca and at the bottom we just tell it

00:11:29,449 --> 00:11:33,649
to start doing the work that's it we

00:11:32,569 --> 00:11:36,079
have now an application that you can

00:11:33,649 --> 00:11:37,490
deploy locally on your laptop and you

00:11:36,079 --> 00:11:38,750
can run at large scale to process

00:11:37,490 --> 00:11:41,180
millions of messages per second

00:11:38,750 --> 00:11:44,420
literally like the code that I've just

00:11:41,180 --> 00:11:46,339
shown you so that means whether you have

00:11:44,420 --> 00:11:48,199
a small use case it's a good fit for

00:11:46,339 --> 00:11:50,240
cascading at extremes if you have a very

00:11:48,199 --> 00:11:51,500
large use case it too is a very good fit

00:11:50,240 --> 00:11:53,660
for castle and casco streams and

00:11:51,500 --> 00:11:56,600
everything in between in the company I

00:11:53,660 --> 00:11:58,940
worked before I would say that maybe 90%

00:11:56,600 --> 00:12:00,440
of the use cases are in large scale will

00:11:58,940 --> 00:12:02,480
still send our very small scale but

00:12:00,440 --> 00:12:04,970
still important data on maybe medium

00:12:02,480 --> 00:12:06,110
scale but still important data but we

00:12:04,970 --> 00:12:07,610
were kind of forced to use completely

00:12:06,110 --> 00:12:08,839
different technologies depending on

00:12:07,610 --> 00:12:10,430
whether we reach that threshold of

00:12:08,839 --> 00:12:12,980
having to push it across let's say from

00:12:10,430 --> 00:12:14,600
1 to 2 machines or at larger scale and

00:12:12,980 --> 00:12:16,250
discuss the screens use the same

00:12:14,600 --> 00:12:19,180
technology all the way through from

00:12:16,250 --> 00:12:21,410
local prototyping a manually installed

00:12:19,180 --> 00:12:25,389
proof-of-concept in production all the

00:12:21,410 --> 00:12:27,500
way to large-scale automatic deployments

00:12:25,389 --> 00:12:29,569
but before we deploy to production right

00:12:27,500 --> 00:12:30,589
we want to test stuff with custard

00:12:29,569 --> 00:12:32,720
streams what you're doing is you're

00:12:30,589 --> 00:12:34,670
deploying a normal Java Scala closure

00:12:32,720 --> 00:12:36,589
application so you can use any kind of

00:12:34,670 --> 00:12:38,660
unit testing tool and methodology that

00:12:36,589 --> 00:12:40,220
you like you can also integration

00:12:38,660 --> 00:12:41,750
testing with it and system testing as

00:12:40,220 --> 00:12:43,190
well so since unit testing is a de

00:12:41,750 --> 00:12:45,649
bourree to talk about let's talk about

00:12:43,190 --> 00:12:47,300
integration testing so one thing you can

00:12:45,649 --> 00:12:48,620
do for example is if you have an

00:12:47,300 --> 00:12:51,649
application at work on that reach on

00:12:48,620 --> 00:12:52,970
Kafka does stuff right back to Kafka for

00:12:51,649 --> 00:12:55,519
integration tests you can spin up in

00:12:52,970 --> 00:12:56,630
memory processes of Kafka and you know

00:12:55,519 --> 00:12:58,189
services like conference give me a

00:12:56,630 --> 00:12:59,600
registry if you working with ever for

00:12:58,189 --> 00:13:01,939
example it's your schemas for your data

00:12:59,600 --> 00:13:04,130
and any other thing that your company

00:13:01,939 --> 00:13:06,709
already has and to the integration test

00:13:04,130 --> 00:13:09,500
against that yes you can also buckeyes

00:13:06,709 --> 00:13:11,240
that and you know run it locally run it

00:13:09,500 --> 00:13:12,800
on Jenkins which also means that

00:13:11,240 --> 00:13:15,410
whatever your company is already using

00:13:12,800 --> 00:13:16,880
in order to do testing in our continuous

00:13:15,410 --> 00:13:19,069
development continues in equations you

00:13:16,880 --> 00:13:20,990
can continue to use that just because

00:13:19,069 --> 00:13:22,370
you're working on some medium to large

00:13:20,990 --> 00:13:23,630
scale data doesn't necessarily mean you

00:13:22,370 --> 00:13:24,579
have to completely change your tool

00:13:23,630 --> 00:13:26,470
chain

00:13:24,579 --> 00:13:30,029
a completely custom get approval for

00:13:26,470 --> 00:13:32,829
that from your intersecting whatever

00:13:30,029 --> 00:13:34,779
now exhuming we tested everything we

00:13:32,829 --> 00:13:36,459
want to deploy it what would we need to

00:13:34,779 --> 00:13:38,439
deploy well you can pick whatever you

00:13:36,459 --> 00:13:40,779
want it's a normal java application

00:13:38,439 --> 00:13:42,790
anything that can run or deploy a JVM

00:13:40,779 --> 00:13:44,739
application works which means basically

00:13:42,790 --> 00:13:46,239
everything on the planet so you can

00:13:44,739 --> 00:13:48,279
using those stuff like a puppet and a

00:13:46,239 --> 00:13:50,079
little you can use enough a current you

00:13:48,279 --> 00:13:52,360
can use cloud services you can use

00:13:50,079 --> 00:13:54,639
acrimony these missiles you can yarn if

00:13:52,360 --> 00:13:56,589
you really want to so all of that works

00:13:54,639 --> 00:13:57,040
and why because there's nothing special

00:13:56,589 --> 00:13:58,329
to it

00:13:57,040 --> 00:13:59,949
you're just deploying a normal

00:13:58,329 --> 00:14:01,089
application as it should be you're not

00:13:59,949 --> 00:14:04,809
building infrastructure you're building

00:14:01,089 --> 00:14:06,759
applications so for example with docker

00:14:04,809 --> 00:14:08,110
how would that work well you have this

00:14:06,759 --> 00:14:10,269
code that I just showed you you can

00:14:08,110 --> 00:14:12,819
compile it into a jar and create a

00:14:10,269 --> 00:14:15,369
docker image for it and then you just

00:14:12,819 --> 00:14:17,110
launch one or more containers as many as

00:14:15,369 --> 00:14:18,879
you need and you can do this your life

00:14:17,110 --> 00:14:21,040
operations without disrupting you know

00:14:18,879 --> 00:14:23,410
the data flow and talk about that in a

00:14:21,040 --> 00:14:25,179
second so super simple it's so simple

00:14:23,410 --> 00:14:26,679
that people typically don't believe that

00:14:25,179 --> 00:14:30,879
it's actually working before they try it

00:14:26,679 --> 00:14:36,730
out so but let's stop talking about work

00:14:30,879 --> 00:14:38,410
count now how does that work so this is

00:14:36,730 --> 00:14:42,429
a 20 minute talk so I can only give you

00:14:38,410 --> 00:14:44,079
a glimpse of it like a trailer but I try

00:14:42,429 --> 00:14:47,110
to at least give you some pointers to

00:14:44,079 --> 00:14:49,389
start with so something super important

00:14:47,110 --> 00:14:51,459
for screen processing is fault tolerance

00:14:49,389 --> 00:14:53,709
and particularly for tolerant state

00:14:51,459 --> 00:14:54,929
which I'll talk about in a second let's

00:14:53,709 --> 00:14:56,980
start with fault tolerance

00:14:54,929 --> 00:14:59,110
imagine you have an application that

00:14:56,980 --> 00:15:00,970
does stuff like word count it remembers

00:14:59,110 --> 00:15:02,860
things that is seen in the past and then

00:15:00,970 --> 00:15:05,139
it needs to do something if this machine

00:15:02,860 --> 00:15:08,049
fails we want to make sure that we can

00:15:05,139 --> 00:15:11,619
migrate this this work to a running

00:15:08,049 --> 00:15:13,899
machine how do we do that well the easy

00:15:11,619 --> 00:15:15,100
part is transferring the code logic like

00:15:13,899 --> 00:15:17,949
you know your dollar file that is super

00:15:15,100 --> 00:15:19,720
easy the tricky part is because is how

00:15:17,949 --> 00:15:22,239
to transfer the state in a way that you

00:15:19,720 --> 00:15:24,639
don't lose data and what we're doing

00:15:22,239 --> 00:15:26,499
here behind the scenes is exploding this

00:15:24,639 --> 00:15:28,899
relationship of the table is a stream as

00:15:26,499 --> 00:15:30,970
a table is a stream and so on so we're

00:15:28,899 --> 00:15:32,079
changed logging the state of the

00:15:30,970 --> 00:15:34,269
application behind the scenes

00:15:32,079 --> 00:15:36,249
continuously into changelog streams or

00:15:34,269 --> 00:15:37,160
change our table using Kafka and when we

00:15:36,249 --> 00:15:39,350
need it

00:15:37,160 --> 00:15:44,269
you reconstruct it from there and all

00:15:39,350 --> 00:15:46,399
this without data loss so this is super

00:15:44,269 --> 00:15:48,319
cool for failure scenarios but this is

00:15:46,399 --> 00:15:50,060
actually even cooler for normal

00:15:48,319 --> 00:15:51,410
operations imagine you have an

00:15:50,060 --> 00:15:54,230
application and you just run it on a

00:15:51,410 --> 00:15:57,199
single container or single VM or single

00:15:54,230 --> 00:15:58,939
their metal machine and this application

00:15:57,199 --> 00:16:00,769
does some stateful processing now you

00:15:58,939 --> 00:16:02,420
realize oh this one machine or this one

00:16:00,769 --> 00:16:05,149
container is not sufficient I need more

00:16:02,420 --> 00:16:06,230
power to process the data the only thing

00:16:05,149 --> 00:16:08,389
that you need to do is to spawn

00:16:06,230 --> 00:16:12,170
additional containers spawn additional

00:16:08,389 --> 00:16:13,790
instances of your application and Casca

00:16:12,170 --> 00:16:16,370
streams will automatically distribute

00:16:13,790 --> 00:16:18,620
you know and like split and partition

00:16:16,370 --> 00:16:19,850
the state and make it available to the

00:16:18,620 --> 00:16:22,910
various instances and they start

00:16:19,850 --> 00:16:24,290
collaborating on the work this works to

00:16:22,910 --> 00:16:26,209
rely for per agency you don't need to

00:16:24,290 --> 00:16:28,310
take down your core business just

00:16:26,209 --> 00:16:30,649
because you need to you know add one

00:16:28,310 --> 00:16:31,910
more machine to the mix I mentioned that

00:16:30,649 --> 00:16:33,019
at the very beginning we really wanted

00:16:31,910 --> 00:16:36,009
to design something that is thoughtful

00:16:33,019 --> 00:16:37,550
and out-of-the-box that is fitting an

00:16:36,009 --> 00:16:40,040
environment where you need to be running

00:16:37,550 --> 00:16:42,829
24/7 so all of this of course works

00:16:40,040 --> 00:16:44,569
during live operations and likewise if

00:16:42,829 --> 00:16:45,589
you want to save money you don't no

00:16:44,569 --> 00:16:47,300
longer need like three of these

00:16:45,589 --> 00:16:51,130
containers you just stop some of them

00:16:47,300 --> 00:16:54,259
and the remaining ones basically get all

00:16:51,130 --> 00:16:56,930
the data all the state and resume the

00:16:54,259 --> 00:17:00,790
work and continue working to

00:16:56,930 --> 00:17:03,350
superelastic same for scalability

00:17:00,790 --> 00:17:05,089
imagine you're having a large incoming

00:17:03,350 --> 00:17:06,860
stream of data and you need to do

00:17:05,089 --> 00:17:08,179
lookups against the database you know

00:17:06,860 --> 00:17:10,490
like in Florida Texas one exam that I

00:17:08,179 --> 00:17:13,039
mentioned earlier one of the drawbacks

00:17:10,490 --> 00:17:15,409
of such an architecture under data paths

00:17:13,039 --> 00:17:17,179
myself is that for every incoming

00:17:15,409 --> 00:17:19,880
request you have to hit a remote

00:17:17,179 --> 00:17:21,650
database across the network so one of

00:17:19,880 --> 00:17:24,020
the downsides is you have a very hyper

00:17:21,650 --> 00:17:25,520
record latency and for some use case

00:17:24,020 --> 00:17:26,510
that is actually quite prohibitive you

00:17:25,520 --> 00:17:28,069
really need to have like a few

00:17:26,510 --> 00:17:30,890
milliseconds and not like seconds of

00:17:28,069 --> 00:17:32,990
time but also you're kind of coupling

00:17:30,890 --> 00:17:35,299
the elevated availability of your

00:17:32,990 --> 00:17:37,340
application with the availability of all

00:17:35,299 --> 00:17:40,400
these external systems so what do you do

00:17:37,340 --> 00:17:42,770
if the database is down do you wait do

00:17:40,400 --> 00:17:44,510
you retry is it acceptable in your case

00:17:42,770 --> 00:17:47,690
are you violating a delay if you wait

00:17:44,510 --> 00:17:49,250
for more than five seconds so and for

00:17:47,690 --> 00:17:50,960
that what we allow you to do with

00:17:49,250 --> 00:17:52,789
calculus changes again unifying the

00:17:50,960 --> 00:17:54,679
the ideas of streams and databases and

00:17:52,789 --> 00:17:56,779
tables and kind of lift the database

00:17:54,679 --> 00:17:58,190
information into your application so

00:17:56,779 --> 00:17:59,690
you're doing local lookups you can

00:17:58,190 --> 00:18:02,240
exploit data locality which is super

00:17:59,690 --> 00:18:03,649
fast and also you know coupling your

00:18:02,240 --> 00:18:06,850
application with external systems and

00:18:03,649 --> 00:18:09,350
dependencies beyond the scalability

00:18:06,850 --> 00:18:11,090
improvements this also means you can

00:18:09,350 --> 00:18:12,340
containerize your application there is

00:18:11,090 --> 00:18:15,200
nothing that you need to like

00:18:12,340 --> 00:18:16,580
orchestrate across different teams in

00:18:15,200 --> 00:18:19,419
your company in order to make your

00:18:16,580 --> 00:18:22,789
application work and of course

00:18:19,419 --> 00:18:25,159
elasticity works in that case as well to

00:18:22,789 --> 00:18:25,580
start more or fewer instances as you see

00:18:25,159 --> 00:18:29,059
fit

00:18:25,580 --> 00:18:30,350
I actually have a demo to show that but

00:18:29,059 --> 00:18:36,950
since it's just a twenty minutes talk I

00:18:30,350 --> 00:18:38,899
can't do that so so the last thing I

00:18:36,950 --> 00:18:41,690
wanna mention here is what we call

00:18:38,899 --> 00:18:43,700
interactive queries so what we allow you

00:18:41,690 --> 00:18:45,529
to do is if your application just some

00:18:43,700 --> 00:18:47,899
state will work you know like the read

00:18:45,529 --> 00:18:50,059
database icon in here we allow you to

00:18:47,899 --> 00:18:51,409
directly expose that information the

00:18:50,059 --> 00:18:53,590
latest processing result of your

00:18:51,409 --> 00:18:55,730
application to other applications

00:18:53,590 --> 00:18:59,990
forward countries could be like what is

00:18:55,730 --> 00:19:01,370
the latest count of the word hello this

00:18:59,990 --> 00:19:02,990
gives you a lot of flexibility because

00:19:01,370 --> 00:19:04,340
you can now use two different approaches

00:19:02,990 --> 00:19:06,200
and it can combine the two approaches as

00:19:04,340 --> 00:19:07,309
well if you need to exchange data

00:19:06,200 --> 00:19:09,350
between your application and

00:19:07,309 --> 00:19:10,940
microservice with another application or

00:19:09,350 --> 00:19:12,860
micro service you can either do this

00:19:10,940 --> 00:19:14,000
indirectly through Kafka as the

00:19:12,860 --> 00:19:14,570
communication channel for all this

00:19:14,000 --> 00:19:17,809
information

00:19:14,570 --> 00:19:19,909
or you can expose it directly to this

00:19:17,809 --> 00:19:22,159
other application and you can also you

00:19:19,909 --> 00:19:24,140
know mixing and combine these two

00:19:22,159 --> 00:19:25,640
approaches as you see fit for some use

00:19:24,140 --> 00:19:27,799
case it makes sense to decouple for some

00:19:25,640 --> 00:19:28,789
it makes sense to a couple and for some

00:19:27,799 --> 00:19:34,190
it makes sense to a couple at the

00:19:28,789 --> 00:19:35,690
beginning but because later on and just

00:19:34,190 --> 00:19:36,950
to highlight here like these other

00:19:35,690 --> 00:19:38,210
applications when using directive

00:19:36,950 --> 00:19:39,890
Curie's that doesn't need to be a Java

00:19:38,210 --> 00:19:41,779
or scale application but can be like any

00:19:39,890 --> 00:19:44,390
programming language that you want the

00:19:41,779 --> 00:19:49,279
JavaScript Python in PHP if you're into

00:19:44,390 --> 00:19:50,659
that or COBOL and remember this earlier

00:19:49,279 --> 00:19:53,169
example that I showed with this roadmap

00:19:50,659 --> 00:19:55,700
they're continuously updated therefore

00:19:53,169 --> 00:19:58,039
the things that allowed us to get rid of

00:19:55,700 --> 00:19:59,960
the database in this use case is this

00:19:58,039 --> 00:20:01,340
interactive curious feature because you

00:19:59,960 --> 00:20:02,960
no longer need to have a database just

00:20:01,340 --> 00:20:04,100
as a handover point between your three

00:20:02,960 --> 00:20:06,559
processing part and

00:20:04,100 --> 00:20:10,940
disturbing part might be the user-facing

00:20:06,559 --> 00:20:12,740
service serving part and you can expose

00:20:10,940 --> 00:20:14,179
this for example for REST API you can

00:20:12,740 --> 00:20:15,679
use trick for that you can use a custom

00:20:14,179 --> 00:20:18,380
interest protocol that you have whatever

00:20:15,679 --> 00:20:20,960
your favor you can implement that so

00:20:18,380 --> 00:20:23,059
here this is an example of REST API that

00:20:20,960 --> 00:20:25,820
we built for a music demo that we called

00:20:23,059 --> 00:20:27,770
cascade music we are generating charts

00:20:25,820 --> 00:20:29,750
in real time based on incoming events

00:20:27,770 --> 00:20:31,640
how which song has been played by which

00:20:29,750 --> 00:20:36,950
user and you can create that live

00:20:31,640 --> 00:20:40,100
through REST API and of course to sum

00:20:36,950 --> 00:20:41,570
this up securities are supported as well

00:20:40,100 --> 00:20:43,039
so if you want to encrypt it and

00:20:41,570 --> 00:20:44,539
translate when you're talking to me

00:20:43,039 --> 00:20:46,669
application to your Kafka cluster when

00:20:44,539 --> 00:20:48,049
indicate or authorize I oughta write

00:20:46,669 --> 00:20:49,490
your applications or only some of your

00:20:48,049 --> 00:20:52,100
application they get the sensitive data

00:20:49,490 --> 00:20:57,110
in your cluster that is supported as

00:20:52,100 --> 00:20:59,270
well so to wrap this up as application

00:20:57,110 --> 00:21:01,039
developers we really want to build

00:20:59,270 --> 00:21:03,530
applications and not infrastructure

00:21:01,039 --> 00:21:06,679
because there's too complex to tag them

00:21:03,530 --> 00:21:08,179
sealing and too painful and with casco

00:21:06,679 --> 00:21:09,740
streams what we wanted to do in the

00:21:08,179 --> 00:21:12,320
Apache Kafka community is give you a

00:21:09,740 --> 00:21:14,150
tool to build these cool applications

00:21:12,320 --> 00:21:15,620
that power your core business without

00:21:14,150 --> 00:21:18,620
having to say you know shoot yourself in

00:21:15,620 --> 00:21:21,799
the foot and if you want to learn more

00:21:18,620 --> 00:21:24,950
about this I give another talk at a cop

00:21:21,799 --> 00:21:26,720
comida later today so you can drop by if

00:21:24,950 --> 00:21:29,090
you have the time and you can also you

00:21:26,720 --> 00:21:30,409
know correctly at our booth we have one

00:21:29,090 --> 00:21:33,350
like in the the main conference area

00:21:30,409 --> 00:21:37,120
where I have time for questions more

00:21:33,350 --> 00:21:37,120
than I might have here enough thank you

00:21:39,680 --> 00:21:41,740

YouTube URL: https://www.youtube.com/watch?v=ACwnrnVJXuE


