Title: Berlin Buzzwords 2017: Thomas Weise - From Batch to Streaming ET(L) with Apache Apex #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Stream data processing is increasingly required to support business needs for faster actionable insight with growing volume of information from more sources. Apache Apex is a true stream processing framework for low-latency, high-throughput and reliable processing of complex analytics pipelines on clusters. Apex is designed for quick time-to-production, and is used in production by large companies for real-time and batch processing at scale.

This session will use an Apex production use case to walk through the incremental transition from a batch pipeline with hours of latency to an end-to-end streaming architecture with billions of events per day which are processed to deliver real-time analytical reports. The example is representative for many similar extract-transform-load (ETL) use cases with other data sets that can use a common library of building blocks. The transform (or analytics) piece of such pipelines varies in complexity and often involves business logic specific, custom components.

Topics include:
- Pipeline functionality from event source through queryable state for real-time insights.
- API for application development and development process.
- Library of building blocks including connectors for sources and sinks such as Kafka, JMS, Cassandra, HBase, JDBC and how they enable end-to-end exactly-once results.
- Stateful processing with event time windowing.
- Fault tolerance with exactly-once result semantics, checkpointing, incremental recovery
- Scalability and low-latency, high-throughput processing with advanced engine features for auto-scaling, dynamic changes, compute locality.
- Who is using Apex in production, and roadmap.

Read more:
https://2017.berlinbuzzwords.de/17/session/batch-streaming-etl-apache-apex

About Thomas Weise:
https://2017.berlinbuzzwords.de/users/thomas-weise

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,750 --> 00:00:11,879
apache apex is a stream processing

00:00:08,639 --> 00:00:14,850
platform actually and so maybe first

00:00:11,879 --> 00:00:17,390
clarification regarding what is the ETL

00:00:14,850 --> 00:00:20,580
doing in the title right so ETL is

00:00:17,390 --> 00:00:22,499
something that extract state are does

00:00:20,580 --> 00:00:24,449
transformation on the data and then

00:00:22,499 --> 00:00:27,320
loads data somewhere right and this is

00:00:24,449 --> 00:00:29,490
very broad actually it applies to simply

00:00:27,320 --> 00:00:31,739
mirroring data from one database to

00:00:29,490 --> 00:00:34,230
another but the transformations can be

00:00:31,739 --> 00:00:35,700
also very complex and this is the type

00:00:34,230 --> 00:00:38,670
of use case that I'm going to talk about

00:00:35,700 --> 00:00:40,859
here so it's not a simple move data from

00:00:38,670 --> 00:00:43,170
A to B but actually take the data from

00:00:40,859 --> 00:00:45,030
somewhere then does some do some more

00:00:43,170 --> 00:00:46,890
processing with it and in this case

00:00:45,030 --> 00:00:49,559
actually not loaded into a target

00:00:46,890 --> 00:00:53,100
environment but use the data directly

00:00:49,559 --> 00:00:56,370
for visualization in the front end so

00:00:53,100 --> 00:00:59,149
where does Apex fit in you can see here

00:00:56,370 --> 00:01:01,739
that you have sources of events

00:00:59,149 --> 00:01:04,790
continuous streams of data mobile

00:01:01,739 --> 00:01:08,540
devices log sensors and so on you have

00:01:04,790 --> 00:01:11,130
systems that transport the data and

00:01:08,540 --> 00:01:14,100
Kafka is the most prominent one store

00:01:11,130 --> 00:01:16,409
and also transport the data there are

00:01:14,100 --> 00:01:18,540
other message queuing systems and those

00:01:16,409 --> 00:01:22,409
are the most common sources for stream

00:01:18,540 --> 00:01:24,899
data applications and then you point

00:01:22,409 --> 00:01:26,880
what the process the data and there are

00:01:24,899 --> 00:01:29,640
several options out there we just had

00:01:26,880 --> 00:01:31,829
presentation about spark there are the

00:01:29,640 --> 00:01:35,700
frameworks out there strong flaying and

00:01:31,829 --> 00:01:38,700
so on and really many choices just in

00:01:35,700 --> 00:01:42,090
the Apache Software Foundation alone so

00:01:38,700 --> 00:01:46,530
apex is a native stream processing

00:01:42,090 --> 00:01:50,130
engine and a framework - it has api's it

00:01:46,530 --> 00:01:54,000
has a library and it runs on clusters so

00:01:50,130 --> 00:01:57,659
right now it's a Hadoop based system

00:01:54,000 --> 00:02:00,390
that means it requires Yann how to be on

00:01:57,659 --> 00:02:04,049
as a resource manager just to get the

00:02:00,390 --> 00:02:08,220
compute resources it is also using HDFS

00:02:04,049 --> 00:02:10,649
for some storage needs but the platform

00:02:08,220 --> 00:02:13,560
itself the streaming of the data between

00:02:10,649 --> 00:02:17,280
different processes that is part of

00:02:13,560 --> 00:02:18,880
Apache apex and you can define your

00:02:17,280 --> 00:02:21,130
pipelines as direct

00:02:18,880 --> 00:02:23,110
a cyclic graph and that means you can

00:02:21,130 --> 00:02:25,870
take these smaller building blocks that

00:02:23,110 --> 00:02:27,880
we call operators and arrange them in

00:02:25,870 --> 00:02:29,770
various ways the year you see a simple

00:02:27,880 --> 00:02:31,990
sequence but most applications wouldn't

00:02:29,770 --> 00:02:34,270
be that simple that you built with Apex

00:02:31,990 --> 00:02:38,350
they would contain branches and joins

00:02:34,270 --> 00:02:41,970
and to express more complex logic and

00:02:38,350 --> 00:02:45,340
then the API there is a lower-level

00:02:41,970 --> 00:02:48,550
deck API in case you're familiar with

00:02:45,340 --> 00:02:49,990
storm you compose you take building

00:02:48,550 --> 00:02:51,940
blocks and you connect them with streams

00:02:49,990 --> 00:02:54,730
and then the other style of API that you

00:02:51,940 --> 00:02:57,730
find in spark for example is declarative

00:02:54,730 --> 00:03:00,190
API so fluent style API you would do

00:02:57,730 --> 00:03:03,160
your chain different calls and you

00:03:00,190 --> 00:03:05,890
define implicitly that a dak dak

00:03:03,160 --> 00:03:08,080
representation is present anyways in

00:03:05,890 --> 00:03:09,820
those systems just the style of

00:03:08,080 --> 00:03:11,640
declaration that is different and

00:03:09,820 --> 00:03:13,870
different style of API is actually

00:03:11,640 --> 00:03:17,230
preferred for different type of use

00:03:13,870 --> 00:03:21,550
cases and users as well apex is also

00:03:17,230 --> 00:03:26,320
Java based by the first iteration of

00:03:21,550 --> 00:03:28,960
allowing of supporting SQL has also come

00:03:26,320 --> 00:03:32,800
in in the last year and that's based on

00:03:28,960 --> 00:03:35,170
Apache color side so the operator

00:03:32,800 --> 00:03:37,960
library is pretty comprehensive it

00:03:35,170 --> 00:03:41,760
covers the connectors for the systems

00:03:37,960 --> 00:03:45,370
that you see here and more of those and

00:03:41,760 --> 00:03:48,850
just like you need a connect connector

00:03:45,370 --> 00:03:52,330
to read data or to write out data read

00:03:48,850 --> 00:03:54,130
from sources write to things what you

00:03:52,330 --> 00:03:56,320
really need them to express what you

00:03:54,130 --> 00:03:58,150
wanted to do or transformations so and

00:03:56,320 --> 00:04:02,350
some of those transformations are of

00:03:58,150 --> 00:04:06,430
course also built into the library most

00:04:02,350 --> 00:04:08,590
of the time data comes from a streaming

00:04:06,430 --> 00:04:12,430
source or from files which is also very

00:04:08,590 --> 00:04:14,890
common and then it goes into databases

00:04:12,430 --> 00:04:16,380
and those are the results and they are

00:04:14,890 --> 00:04:19,510
stored in databases for downstream

00:04:16,380 --> 00:04:22,480
pipelines one more though we see

00:04:19,510 --> 00:04:24,520
patterns there you have some smaller

00:04:22,480 --> 00:04:26,470
pipelines or microservices and they are

00:04:24,520 --> 00:04:28,990
connected with Kafka queues for example

00:04:26,470 --> 00:04:31,600
or what I believe will also happen more

00:04:28,990 --> 00:04:32,110
and more and you hear this mentioned in

00:04:31,600 --> 00:04:36,939
the last

00:04:32,110 --> 00:04:38,620
year also Kerbal state stream puzzles as

00:04:36,939 --> 00:04:40,330
the database their various names for

00:04:38,620 --> 00:04:42,819
those things but the idea is that you

00:04:40,330 --> 00:04:44,050
have a stateful platform the memory that

00:04:42,819 --> 00:04:45,849
you already have allocated to the

00:04:44,050 --> 00:04:48,270
platform can hold data and if you can

00:04:45,849 --> 00:04:51,340
get the data from there and use it for

00:04:48,270 --> 00:04:54,610
visualization directly that's also a

00:04:51,340 --> 00:04:58,319
nice idea so the use case that I want to

00:04:54,610 --> 00:05:01,810
talk about is from online advertising

00:04:58,319 --> 00:05:05,229
collection of impression and click data

00:05:01,810 --> 00:05:08,199
and then aggregation of the data and

00:05:05,229 --> 00:05:11,409
making it available for users to users

00:05:08,199 --> 00:05:13,330
through dashboard and that is exactly

00:05:11,409 --> 00:05:15,430
the case that I just mentioned where the

00:05:13,330 --> 00:05:17,680
data is not first written into a

00:05:15,430 --> 00:05:19,990
database but it's a real-time dashboard

00:05:17,680 --> 00:05:23,349
where the user wants to see the data

00:05:19,990 --> 00:05:26,159
basically as it's being computed the

00:05:23,349 --> 00:05:30,219
reason why such system is needed in

00:05:26,159 --> 00:05:33,129
attack the round-trip time from when an

00:05:30,219 --> 00:05:35,650
event occurs in the front end until a

00:05:33,129 --> 00:05:38,229
business user has insight in campaign

00:05:35,650 --> 00:05:40,330
performance is critical so this is not

00:05:38,229 --> 00:05:43,650
milliseconds but it's it should be fast

00:05:40,330 --> 00:05:46,750
it shouldn't take five hours which the

00:05:43,650 --> 00:05:48,849
system took when it was implemented as a

00:05:46,750 --> 00:05:51,370
batch of pipeline first so there are

00:05:48,849 --> 00:05:54,509
several components of the system that

00:05:51,370 --> 00:05:57,370
the user or the company has implemented

00:05:54,509 --> 00:05:59,800
the first part which is the focus here

00:05:57,370 --> 00:06:01,330
is the real-time reporting get the

00:05:59,800 --> 00:06:03,400
streams capture the streams to

00:06:01,330 --> 00:06:06,909
aggregation have the data for time

00:06:03,400 --> 00:06:09,750
series and have it available for one for

00:06:06,909 --> 00:06:14,529
real-time reporting on monitoring

00:06:09,750 --> 00:06:18,159
monitoring also to to define alerts that

00:06:14,529 --> 00:06:20,800
is another use case then also for the

00:06:18,159 --> 00:06:23,020
real-time learning to take the data that

00:06:20,800 --> 00:06:25,409
is being aggregated and computed and

00:06:23,020 --> 00:06:29,860
feed it back into the machine learning

00:06:25,409 --> 00:06:31,690
and the final part and that's the most

00:06:29,860 --> 00:06:33,849
important part is actually to be able to

00:06:31,690 --> 00:06:37,210
take the data back into the allocation

00:06:33,849 --> 00:06:40,089
engine which is where the decision is

00:06:37,210 --> 00:06:43,320
made which ad will show up on which

00:06:40,089 --> 00:06:48,480
website and that of course affects

00:06:43,320 --> 00:06:51,090
the bottom line of the business what

00:06:48,480 --> 00:06:56,040
what we bid for and how much we bid for

00:06:51,090 --> 00:06:58,440
right so the transition from the batch

00:06:56,040 --> 00:07:01,890
to the food streaming was done in stages

00:06:58,440 --> 00:07:05,220
and that was because not everything

00:07:01,890 --> 00:07:08,910
could be changed at one time the first

00:07:05,220 --> 00:07:11,640
cut was to leave the e part of the ETL

00:07:08,910 --> 00:07:14,100
there as it is that means the data comes

00:07:11,640 --> 00:07:16,230
from files and just change the

00:07:14,100 --> 00:07:19,770
transformation part so transformation

00:07:16,230 --> 00:07:22,650
part means we move MapReduce jobs that

00:07:19,770 --> 00:07:25,200
are in defined in service orchestration

00:07:22,650 --> 00:07:28,020
chain together multiple MapReduce jobs

00:07:25,200 --> 00:07:29,850
and other things take that away plus the

00:07:28,020 --> 00:07:32,720
part that loads the data finally into a

00:07:29,850 --> 00:07:36,210
database and replace that with the

00:07:32,720 --> 00:07:38,370
parsing in Apex so that is the middle

00:07:36,210 --> 00:07:40,740
part that took it to 20 minutes which is

00:07:38,370 --> 00:07:43,170
still a lot of time right 20 minutes but

00:07:40,740 --> 00:07:45,870
it's better than 5 hours when you think

00:07:43,170 --> 00:07:47,850
of a typical day let's take it a busy

00:07:45,870 --> 00:07:50,070
shopping day right when you know

00:07:47,850 --> 00:07:52,130
something within 20 minutes isn't doing

00:07:50,070 --> 00:07:54,960
that well I can still make a change and

00:07:52,130 --> 00:07:56,670
rest of the day might be better but if I

00:07:54,960 --> 00:08:00,090
know only after 5 hours that things are

00:07:56,670 --> 00:08:03,960
not going so well then probably the peak

00:08:00,090 --> 00:08:07,290
shopping day is sort of wasted for that

00:08:03,960 --> 00:08:09,420
particular campaign nevertheless that

00:08:07,290 --> 00:08:10,920
was an intermediate step it brought the

00:08:09,420 --> 00:08:12,600
better transform it brought the

00:08:10,920 --> 00:08:15,000
dashboard already with the real-time

00:08:12,600 --> 00:08:17,010
reporting but it wasn't enter and

00:08:15,000 --> 00:08:19,350
streaming yet and that was really cool

00:08:17,010 --> 00:08:22,320
so the enter and streaming required them

00:08:19,350 --> 00:08:24,660
to take out this extract thing that was

00:08:22,320 --> 00:08:26,760
there that would take data actually form

00:08:24,660 --> 00:08:28,500
a stream because it appears as a stream

00:08:26,760 --> 00:08:30,690
the clicks and the impressions

00:08:28,500 --> 00:08:34,110
that's a stream that that you have

00:08:30,690 --> 00:08:35,760
naturally but what had to be done to

00:08:34,110 --> 00:08:38,000
make it work earlier was the batch

00:08:35,760 --> 00:08:41,070
system is to chop that stream into files

00:08:38,000 --> 00:08:44,130
files that are I think it was 15 minutes

00:08:41,070 --> 00:08:46,980
worth each each in 15 minutes it was

00:08:44,130 --> 00:08:49,680
chopped off stored into s3 and unloaded

00:08:46,980 --> 00:08:51,840
from there so that had to go go away and

00:08:49,680 --> 00:08:54,120
the stream metric capture directly and

00:08:51,840 --> 00:08:56,920
fed into the stream processing pipeline

00:08:54,120 --> 00:08:59,019
so here this was the

00:08:56,920 --> 00:09:02,380
first part and you see there's no

00:08:59,019 --> 00:09:04,360
connection between the front end these

00:09:02,380 --> 00:09:06,310
are the ad servers on the left side then

00:09:04,360 --> 00:09:08,889
the data through rest box he pushed into

00:09:06,310 --> 00:09:12,100
Kafka clusters and it's geo distributed

00:09:08,889 --> 00:09:14,290
it's in all regions of the world they

00:09:12,100 --> 00:09:17,709
have had data that have data centers

00:09:14,290 --> 00:09:20,620
with those car clusters and then no link

00:09:17,709 --> 00:09:22,360
between the Kafka cluster and the apex

00:09:20,620 --> 00:09:24,820
the stream processing pipeline and

00:09:22,360 --> 00:09:27,519
that's because the link wasn't there the

00:09:24,820 --> 00:09:29,829
link was really the chopping of files

00:09:27,519 --> 00:09:32,889
and the batching so I think they use

00:09:29,829 --> 00:09:35,290
Camus and then Camus would pump it into

00:09:32,889 --> 00:09:37,209
oh it would go into a MapReduce job in

00:09:35,290 --> 00:09:39,940
the MapReduce job with dump it into s3

00:09:37,209 --> 00:09:41,529
and once it's in s3 there's a directory

00:09:39,940 --> 00:09:43,959
scanner running and picks up the files

00:09:41,529 --> 00:09:45,639
as soon as they show up they will be

00:09:43,959 --> 00:09:47,260
processed and it will be turned back

00:09:45,639 --> 00:09:49,839
into the stream right that's the irony

00:09:47,260 --> 00:09:51,940
and but here you can see the processing

00:09:49,839 --> 00:09:54,070
what happens the file reader then the

00:09:51,940 --> 00:09:55,930
file reader will after the file read or

00:09:54,070 --> 00:09:59,850
somethi compress and parsing will happen

00:09:55,930 --> 00:10:03,940
some filtering pre aggregation shuffle

00:09:59,850 --> 00:10:05,980
finally the full aggregation of the

00:10:03,940 --> 00:10:09,220
partial aggregates and the in-memory

00:10:05,980 --> 00:10:11,980
store and from the in-memory store the

00:10:09,220 --> 00:10:14,470
results can be accessed of to the front

00:10:11,980 --> 00:10:16,949
end so this myth box that is labeled

00:10:14,470 --> 00:10:20,350
middleware as a as a front-end server

00:10:16,949 --> 00:10:23,620
that has HTTP interface for the

00:10:20,350 --> 00:10:26,230
dashboard and the back end is really

00:10:23,620 --> 00:10:28,660
Kafka in this case then when you see a

00:10:26,230 --> 00:10:32,949
widget on this screen and this will be

00:10:28,660 --> 00:10:34,480
the this year I show what the end result

00:10:32,949 --> 00:10:36,310
is first before I keep on talking about

00:10:34,480 --> 00:10:38,620
it so you have this dashboard you can

00:10:36,310 --> 00:10:41,529
see time series you can see top endless

00:10:38,620 --> 00:10:43,990
of the data that you are interested in

00:10:41,529 --> 00:10:47,980
you can filter and you can also define

00:10:43,990 --> 00:10:50,199
time time arranges right so when you use

00:10:47,980 --> 00:10:53,889
on growth here and open the dashboard

00:10:50,199 --> 00:10:57,209
for particular combination of keys then

00:10:53,889 --> 00:11:03,519
a query will automatically be placed

00:10:57,209 --> 00:11:05,380
into this Kafka topic and it will be

00:11:03,519 --> 00:11:07,890
received like any other data from Kafka

00:11:05,380 --> 00:11:09,550
by the FX application and it's a pops up

00:11:07,890 --> 00:11:11,620
mechanism as

00:11:09,550 --> 00:11:13,600
somebody registers interest for a

00:11:11,620 --> 00:11:15,880
particular data point or for time range

00:11:13,600 --> 00:11:17,110
and key combination and everybody could

00:11:15,880 --> 00:11:21,340
look at different things at the same

00:11:17,110 --> 00:11:23,860
time and this dimension store operator

00:11:21,340 --> 00:11:26,080
keeps track of it and sends periodic LED

00:11:23,860 --> 00:11:28,720
responses and there's a keepalive on it

00:11:26,080 --> 00:11:31,000
so if the browser gets closed and the

00:11:28,720 --> 00:11:33,100
user goes away or just closes the laptop

00:11:31,000 --> 00:11:35,320
that we don't do unnecessary work it

00:11:33,100 --> 00:11:37,600
will just stop after some time there's a

00:11:35,320 --> 00:11:41,230
countdown but this is how we can process

00:11:37,600 --> 00:11:44,020
the queries pretty fast and it may look

00:11:41,230 --> 00:11:46,240
very complicated well it has to go

00:11:44,020 --> 00:11:49,300
through many hops front-end server Kafka

00:11:46,240 --> 00:11:51,310
and then into apex out of AP expects

00:11:49,300 --> 00:11:53,680
ROKAF back into wanted server but the

00:11:51,310 --> 00:11:56,620
entire run to actually is only like 50

00:11:53,680 --> 00:11:58,690
milliseconds and the benefit is that you

00:11:56,620 --> 00:12:01,150
get to see the data immediately so once

00:11:58,690 --> 00:12:03,310
there is a change here in that in-memory

00:12:01,150 --> 00:12:05,110
state which happens all the time

00:12:03,310 --> 00:12:07,720
impressions new clicks they're getting

00:12:05,110 --> 00:12:09,370
aggregated so within second within a

00:12:07,720 --> 00:12:12,300
second or so you can see changes right

00:12:09,370 --> 00:12:16,930
on the front end and you get this nice

00:12:12,300 --> 00:12:21,040
feedback loop working so the second

00:12:16,930 --> 00:12:22,260
phase was a move away from s3 and take

00:12:21,040 --> 00:12:25,210
Kafka right now

00:12:22,260 --> 00:12:28,060
consume the Kafka topics directly why

00:12:25,210 --> 00:12:29,800
was this not done in the first part it's

00:12:28,060 --> 00:12:31,630
actually the worst challenges right

00:12:29,800 --> 00:12:36,160
first of all at that point this was in

00:12:31,630 --> 00:12:37,990
2014 and so it's a while ago the there

00:12:36,160 --> 00:12:40,090
was work that needed to be done on the

00:12:37,990 --> 00:12:42,670
Kafka connector in Apex to to make it

00:12:40,090 --> 00:12:44,650
read from multiple clusters because the

00:12:42,670 --> 00:12:46,570
multiple Kafka clusters and we don't

00:12:44,650 --> 00:12:49,090
want to waste resources by having a set

00:12:46,570 --> 00:12:50,860
of connectors and partitions for each of

00:12:49,090 --> 00:12:52,360
the Kafka clusters because they have

00:12:50,860 --> 00:12:54,430
peak times at different days in the

00:12:52,360 --> 00:12:57,340
world and people sleep in Asia they are

00:12:54,430 --> 00:12:59,440
awake in the US and they were interested

00:12:57,340 --> 00:13:02,050
in even resource utilization rather than

00:12:59,440 --> 00:13:05,320
allocating extra so that support had to

00:13:02,050 --> 00:13:08,170
be added and this was also Kafka 0.8

00:13:05,320 --> 00:13:10,000
still right 0.8 and they also had

00:13:08,170 --> 00:13:12,250
challenges with the stability of the

00:13:10,000 --> 00:13:14,650
Kafka class so so it was tricky because

00:13:12,250 --> 00:13:16,480
when we go in here this potentially

00:13:14,650 --> 00:13:19,330
affects the answer was if they cannot

00:13:16,480 --> 00:13:20,800
push the data out anymore and there's a

00:13:19,330 --> 00:13:22,079
backlog then it's not a good situation

00:13:20,800 --> 00:13:23,579
because the real

00:13:22,079 --> 00:13:26,489
the real work that they do is serving

00:13:23,579 --> 00:13:29,009
ads not to produce logs for data

00:13:26,489 --> 00:13:30,989
aggregation which is very important but

00:13:29,009 --> 00:13:35,040
it cannot disturb the primary function

00:13:30,989 --> 00:13:37,649
so the switch took a bit longer and

00:13:35,040 --> 00:13:39,389
happened slower but the rest of the

00:13:37,649 --> 00:13:42,949
pipeline remained the same right and

00:13:39,389 --> 00:13:45,929
then finally we got rid of s3 and

00:13:42,949 --> 00:13:48,689
everything is Kafka based and we got

00:13:45,929 --> 00:13:51,749
this nice end to end streaming a

00:13:48,689 --> 00:13:55,350
pipeline also some interesting learning

00:13:51,749 --> 00:13:57,239
was an Amazon unpredictable behavior at

00:13:55,350 --> 00:13:59,819
least at that time with network

00:13:57,239 --> 00:14:02,309
performance which really showed up with

00:13:59,819 --> 00:14:05,610
a shared environment really yes young

00:14:02,309 --> 00:14:07,350
controls young locate CPU and the

00:14:05,610 --> 00:14:10,259
allocate memory and that's all good but

00:14:07,350 --> 00:14:12,959
the network we saw really surprising

00:14:10,259 --> 00:14:14,459
things happen that later and this when

00:14:12,959 --> 00:14:18,449
some of these things are moved to

00:14:14,459 --> 00:14:19,949
unprimed they magically disappeared so I

00:14:18,449 --> 00:14:21,689
showed the dashboard already how that

00:14:19,949 --> 00:14:23,610
looks like and here's some more detail

00:14:21,689 --> 00:14:27,929
about the transformation pipeline so

00:14:23,610 --> 00:14:30,269
data coming in from Kafka for efficiency

00:14:27,929 --> 00:14:32,249
multiple log entries are actually

00:14:30,269 --> 00:14:35,009
batched together combined together and

00:14:32,249 --> 00:14:37,230
compressed into one Kafka message so

00:14:35,009 --> 00:14:41,809
that needs to be unwrapped on this side

00:14:37,230 --> 00:14:44,670
here so he comprised path split and then

00:14:41,809 --> 00:14:47,220
enrichment to inject additional data

00:14:44,670 --> 00:14:50,129
from lookup source into the data triples

00:14:47,220 --> 00:14:53,100
and transformation and then the

00:14:50,129 --> 00:14:55,379
aggregation finally pre aggregation to

00:14:53,100 --> 00:14:57,929
shrink as much data as possible while

00:14:55,379 --> 00:15:00,089
it's in this parallel pipeline I chose

00:14:57,929 --> 00:15:02,249
three but I think in the final

00:15:00,089 --> 00:15:04,799
configuration there were 64 parallel

00:15:02,249 --> 00:15:09,509
pipes so we try to do as much as we can

00:15:04,799 --> 00:15:11,490
in this fused or chained pipeline before

00:15:09,509 --> 00:15:13,079
we have to hit a first shuffle we goes

00:15:11,490 --> 00:15:16,589
to shuffle is expensive everything has

00:15:13,079 --> 00:15:18,419
moved as we moved over the network so

00:15:16,589 --> 00:15:22,470
the shuffle happens after pre

00:15:18,419 --> 00:15:24,509
aggregation and then we have keyed data

00:15:22,470 --> 00:15:27,709
so everything that belongs to one key

00:15:24,509 --> 00:15:29,759
goes to one store I show one store this

00:15:27,709 --> 00:15:32,419
actually there are many of those there

00:15:29,759 --> 00:15:35,330
are 32 partitions there are 64 of these

00:15:32,419 --> 00:15:39,980
ingest and pre aggregate pipes

00:15:35,330 --> 00:15:42,740
and there are 32 stores and yes I

00:15:39,980 --> 00:15:45,380
already explained what this year is the

00:15:42,740 --> 00:15:47,420
visualization this front-end server that

00:15:45,380 --> 00:15:53,540
is interacting with calf you to

00:15:47,420 --> 00:15:56,930
push Curie's in and retrieve results so

00:15:53,540 --> 00:16:00,350
what is the aggregation doing think of

00:15:56,930 --> 00:16:02,329
it as when you have a data warehouse you

00:16:00,350 --> 00:16:06,529
might know fact tables and dimension

00:16:02,329 --> 00:16:08,720
tables and those type of things right it

00:16:06,529 --> 00:16:10,070
really is a matter of defining certain

00:16:08,720 --> 00:16:10,760
key combinations that we want to

00:16:10,070 --> 00:16:13,100
pre-compute

00:16:10,760 --> 00:16:16,190
a for the reporting that means we don't

00:16:13,100 --> 00:16:18,769
go to a relational database and do a

00:16:16,190 --> 00:16:20,570
giant drawing over many many rows of

00:16:18,769 --> 00:16:22,160
source data the impressions and the

00:16:20,570 --> 00:16:24,380
clicks are not stored here all right

00:16:22,160 --> 00:16:27,920
we are just storing aggregates the data

00:16:24,380 --> 00:16:31,700
is really reduced a lot it's B if the

00:16:27,920 --> 00:16:35,480
input is 450,000 in K events per second

00:16:31,700 --> 00:16:37,250
then the updates here or the data that

00:16:35,480 --> 00:16:39,709
is being sought is much less just the

00:16:37,250 --> 00:16:42,310
aggregate is not the source data so we

00:16:39,709 --> 00:16:45,200
have these different dimension

00:16:42,310 --> 00:16:47,360
combinations for example one is here

00:16:45,200 --> 00:16:50,779
time is implicit based on the time stamp

00:16:47,360 --> 00:16:52,310
then we have time and advertiser those

00:16:50,779 --> 00:16:56,750
are the yellow columns then we have time

00:16:52,310 --> 00:16:58,910
and location and we have time advertiser

00:16:56,750 --> 00:17:01,670
and location those are the combinations

00:16:58,910 --> 00:17:03,140
this is just an example of course there

00:17:01,670 --> 00:17:04,819
are also different time pockets that are

00:17:03,140 --> 00:17:08,510
being computed not just hours there are

00:17:04,819 --> 00:17:10,880
minutes and there are days and so on so

00:17:08,510 --> 00:17:13,189
you got all these things basically you

00:17:10,880 --> 00:17:14,990
can show in here as a flat list and

00:17:13,189 --> 00:17:17,360
actually in memory it looks like that

00:17:14,990 --> 00:17:19,550
the in-memory store think of it like a

00:17:17,360 --> 00:17:22,429
hash table hash table is composite keys

00:17:19,550 --> 00:17:26,260
and when lookup has to happen we know

00:17:22,429 --> 00:17:29,330
the time range we know exactly for which

00:17:26,260 --> 00:17:31,070
time bucket to look for the keys will

00:17:29,330 --> 00:17:34,309
tell us let's say we are looking for

00:17:31,070 --> 00:17:37,850
subway we will have that key and we will

00:17:34,309 --> 00:17:39,559
have a location maybe or maybe not the

00:17:37,850 --> 00:17:42,350
point is we know these are just lookups

00:17:39,559 --> 00:17:45,919
in the hash table then a new event comes

00:17:42,350 --> 00:17:47,820
in we know what we have to update so we

00:17:45,919 --> 00:17:50,010
know the advertise we know ok

00:17:47,820 --> 00:17:52,200
we know the time and then it's a matter

00:17:50,010 --> 00:17:53,909
of updating the metrics that are the

00:17:52,200 --> 00:17:55,440
blue columns and then these are

00:17:53,909 --> 00:17:57,090
available for reporting there's no

00:17:55,440 --> 00:17:59,330
computation that needs to be done when

00:17:57,090 --> 00:18:02,549
the data is retrieved into the front end

00:17:59,330 --> 00:18:05,700
so scale 6 geographically distributed

00:18:02,549 --> 00:18:08,039
data centers the all the systems they

00:18:05,700 --> 00:18:10,230
collect a lot of stuff a lot of data 10

00:18:08,039 --> 00:18:12,210
petabytes of data what is more

00:18:10,230 --> 00:18:17,460
interesting here's 50 terabytes of data

00:18:12,210 --> 00:18:21,480
that move in in a day and those are

00:18:17,460 --> 00:18:26,909
about 40 billion ad impressions and 350

00:18:21,480 --> 00:18:29,070
billion 350 billion bits they collect

00:18:26,909 --> 00:18:33,630
individual bits right to analyze those

00:18:29,070 --> 00:18:37,230
two and then average data flow of 20

00:18:33,630 --> 00:18:39,570
days of 150 K events per second and I

00:18:37,230 --> 00:18:41,690
said this is handled with 64 parallel

00:18:39,570 --> 00:18:44,610
partitions to do the Kafka read

00:18:41,690 --> 00:18:48,090
decompress filter enriched pre aggregate

00:18:44,610 --> 00:18:50,549
and then 32 of those store store

00:18:48,090 --> 00:18:52,590
instances that just keep the data in

00:18:50,549 --> 00:18:55,620
memory so when I say keep data memory is

00:18:52,590 --> 00:18:57,570
not just leave it there and and and it's

00:18:55,620 --> 00:18:59,460
good of course things can fail right

00:18:57,570 --> 00:19:01,890
process can go down this continuous

00:18:59,460 --> 00:19:04,470
processing system continuous operator

00:19:01,890 --> 00:19:06,630
models so the data has to be there when

00:19:04,470 --> 00:19:10,110
it fails so the data is check pointed

00:19:06,630 --> 00:19:13,110
it's safe periodically but it's after

00:19:10,110 --> 00:19:15,510
the after the aggregation it's small

00:19:13,110 --> 00:19:16,830
enough to actually do that while with

00:19:15,510 --> 00:19:20,520
the source data would be very difficult

00:19:16,830 --> 00:19:24,270
a lot of storage systems they max out is

00:19:20,520 --> 00:19:26,279
5 digits throughput numbers and district

00:19:24,270 --> 00:19:27,840
scale right and in this case there's no

00:19:26,279 --> 00:19:30,149
need to save all the source data and if

00:19:27,840 --> 00:19:32,159
if there was then they are already safe

00:19:30,149 --> 00:19:34,799
there in the Kafka topic you don't need

00:19:32,159 --> 00:19:39,450
a database for that and total memory

00:19:34,799 --> 00:19:42,419
consumption is 1.2 terabyte for the apex

00:19:39,450 --> 00:19:44,429
pipeline so different processes that

00:19:42,419 --> 00:19:46,980
contain those building blocks those

00:19:44,429 --> 00:19:48,690
operators distributed over the cluster

00:19:46,980 --> 00:19:51,960
and the total memory that those take up

00:19:48,690 --> 00:19:55,169
is 1.2 terabyte in this case so why was

00:19:51,960 --> 00:19:57,270
epic use for this it's providing state

00:19:55,169 --> 00:19:59,549
management and fault tolerance which is

00:19:57,270 --> 00:20:02,240
needed for exactly this function to

00:19:59,549 --> 00:20:04,860
serve the data out of memory

00:20:02,240 --> 00:20:06,900
exactly once results semantics that we

00:20:04,860 --> 00:20:10,380
don't double account since money is

00:20:06,900 --> 00:20:12,900
involved here it's kind of important

00:20:10,380 --> 00:20:14,610
it provides checkpointing windowing you

00:20:12,900 --> 00:20:17,580
can do processing based on event time

00:20:14,610 --> 00:20:19,020
it's the computations need to be done

00:20:17,580 --> 00:20:21,270
based on the time stems that are there

00:20:19,020 --> 00:20:23,280
in the source events they need to go to

00:20:21,270 --> 00:20:25,140
the right time buckets if I run the same

00:20:23,280 --> 00:20:26,310
computation tomorrow again I have to end

00:20:25,140 --> 00:20:30,020
up with the same result

00:20:26,310 --> 00:20:33,960
so that's event I'm processing then

00:20:30,020 --> 00:20:36,690
recovery fine grained recovery you can

00:20:33,960 --> 00:20:38,160
have an SLA because of the way of the

00:20:36,690 --> 00:20:40,770
Veda recovery work and how you can

00:20:38,160 --> 00:20:43,170
paralyze the processing if you use it

00:20:40,770 --> 00:20:45,120
for speculative execution for example

00:20:43,170 --> 00:20:47,670
not applicable here but you can do that

00:20:45,120 --> 00:20:49,800
with Apex but then the option to do the

00:20:47,670 --> 00:20:52,190
queryable state to do queryable state

00:20:49,800 --> 00:20:55,470
you need the data in memory and

00:20:52,190 --> 00:20:58,490
accessible directly right so you can do

00:20:55,470 --> 00:21:01,170
it as a stateful stream processor only

00:20:58,490 --> 00:21:03,060
processing based on event time I

00:21:01,170 --> 00:21:05,220
mentioned that it's native streaming

00:21:03,060 --> 00:21:07,470
with native streaming you can do low

00:21:05,220 --> 00:21:10,530
latency processing you know micro

00:21:07,470 --> 00:21:12,930
batching no unnecessary delay and as if

00:21:10,530 --> 00:21:14,700
locking operations but in this case

00:21:12,930 --> 00:21:18,690
nothing is blocking time buckets are

00:21:14,700 --> 00:21:21,450
updated as the data comes in and the the

00:21:18,690 --> 00:21:22,980
results even the SDS deck rates are

00:21:21,450 --> 00:21:24,900
being computed you can see the changes

00:21:22,980 --> 00:21:27,000
in the dashboard it's just constantly

00:21:24,900 --> 00:21:30,840
changing that was the requirement here

00:21:27,000 --> 00:21:33,260
on its pipeline processing so the data

00:21:30,840 --> 00:21:35,130
moves through the pipeline it's not the

00:21:33,260 --> 00:21:36,720
processing doesn't move it to where the

00:21:35,130 --> 00:21:38,520
data is but it's the opposite the data

00:21:36,720 --> 00:21:41,280
moves to the pipeline and because this

00:21:38,520 --> 00:21:44,850
is done in a streaming way there are no

00:21:41,280 --> 00:21:48,300
spikes read spikes write spikes and so

00:21:44,850 --> 00:21:51,300
on this continues flow nicely evens out

00:21:48,300 --> 00:21:54,180
the resource consumption it's scalable

00:21:51,300 --> 00:21:56,730
you can add more processes to a Hadoop

00:21:54,180 --> 00:21:58,260
Aeons a store and they could be used by

00:21:56,730 --> 00:22:01,230
apex and that can also be done

00:21:58,260 --> 00:22:03,960
dynamically apex lets you do it and then

00:22:01,230 --> 00:22:05,520
it has the library of connectors many of

00:22:03,960 --> 00:22:08,940
the things that I talked about here when

00:22:05,520 --> 00:22:11,100
three years ago they were not there some

00:22:08,940 --> 00:22:13,830
of the things actually built out of

00:22:11,100 --> 00:22:14,700
learning from from that use case the

00:22:13,830 --> 00:22:17,550
connectors

00:22:14,700 --> 00:22:21,840
some of the file readers and so they've

00:22:17,550 --> 00:22:23,820
improved a lot but the basic ideas they

00:22:21,840 --> 00:22:25,560
basically they are reflected also in

00:22:23,820 --> 00:22:28,770
ready-to-use building blocks the

00:22:25,560 --> 00:22:30,230
different today in the library so this

00:22:28,770 --> 00:22:33,170
is the library some categories

00:22:30,230 --> 00:22:36,270
connectors for messaging Kafka and so on

00:22:33,170 --> 00:22:38,040
file read file system in and out by the

00:22:36,270 --> 00:22:40,440
reading for writing those are the common

00:22:38,040 --> 00:22:43,110
very common things that people need and

00:22:40,440 --> 00:22:45,480
do also database reading database

00:22:43,110 --> 00:22:47,880
writing database some other connectors

00:22:45,480 --> 00:22:49,320
no sequel of course and then the

00:22:47,880 --> 00:22:50,910
transformation so stateless

00:22:49,320 --> 00:22:53,310
transformations simple things that you

00:22:50,910 --> 00:22:55,380
would also know from ETL tools like

00:22:53,310 --> 00:22:56,130
filters and all of those things pauses

00:22:55,380 --> 00:22:58,290
and so on

00:22:56,130 --> 00:23:00,960
but then also the things that require

00:22:58,290 --> 00:23:02,580
stateful platform alright and state

00:23:00,960 --> 00:23:06,000
management and fault tolerance

00:23:02,580 --> 00:23:10,710
windowing the accumulations the

00:23:06,000 --> 00:23:13,620
triggering watermarks and so on so the

00:23:10,710 --> 00:23:16,590
remaining time of when you used to just

00:23:13,620 --> 00:23:18,000
mention a few things or show how you

00:23:16,590 --> 00:23:20,280
could build something like this yourself

00:23:18,000 --> 00:23:22,920
with Apex some of the components because

00:23:20,280 --> 00:23:24,780
this was really a use case a case study

00:23:22,920 --> 00:23:28,920
what a custom of the dashboard was

00:23:24,780 --> 00:23:31,410
proprietary so let's get to some of the

00:23:28,920 --> 00:23:34,470
details how you would do it if you were

00:23:31,410 --> 00:23:36,030
using apex so I picked something that is

00:23:34,470 --> 00:23:40,250
easy to understand for everyone because

00:23:36,030 --> 00:23:43,920
it's real time consuming tweets from the

00:23:40,250 --> 00:23:46,950
Twitter developer API you can tap into

00:23:43,920 --> 00:23:50,010
that API you can create a developer

00:23:46,950 --> 00:23:52,830
account and you can get 1% of the tweet

00:23:50,010 --> 00:23:54,840
stream for free so you can write your

00:23:52,830 --> 00:23:57,990
own application and you can do some look

00:23:54,840 --> 00:24:03,410
at the tweets do some analysis and it's

00:23:57,990 --> 00:24:07,710
a fun project so this particular

00:24:03,410 --> 00:24:10,470
application does two things it computes

00:24:07,710 --> 00:24:13,650
the top hashtags of tweets over a

00:24:10,470 --> 00:24:17,850
5-minute window and in the other branch

00:24:13,650 --> 00:24:20,580
it computes the sum counts for the

00:24:17,850 --> 00:24:24,360
tweets so on the top you see the top end

00:24:20,580 --> 00:24:25,490
it extracts hashtags first then count by

00:24:24,360 --> 00:24:30,200
key

00:24:25,490 --> 00:24:32,240
top end and then a conversion that is

00:24:30,200 --> 00:24:34,460
just necessary to you reuse another

00:24:32,240 --> 00:24:37,789
operator that is this snapshot server

00:24:34,460 --> 00:24:42,860
which is the piece that enables the

00:24:37,789 --> 00:24:44,779
queryable State in Apex those two are

00:24:42,860 --> 00:24:47,179
windows operation so Kentucky is a

00:24:44,779 --> 00:24:50,480
window operation on top n2 current by

00:24:47,179 --> 00:24:52,789
key is a Keith windows operation and top

00:24:50,480 --> 00:24:54,970
n is you need all the need to see all

00:24:52,789 --> 00:24:59,990
the keys to decide which one are the top

00:24:54,970 --> 00:25:03,520
worlds right and in the other branch you

00:24:59,990 --> 00:25:07,490
have timestamp assignment then we

00:25:03,520 --> 00:25:09,440
compute counts just three starts just

00:25:07,490 --> 00:25:13,399
three starts is a simple example total

00:25:09,440 --> 00:25:16,460
counts in a window total number of total

00:25:13,399 --> 00:25:18,590
tweets in the window total tweets with

00:25:16,460 --> 00:25:22,190
hashtags and total tweets with URLs

00:25:18,590 --> 00:25:25,600
three metrics we get there and then we

00:25:22,190 --> 00:25:29,659
output those as time series into a

00:25:25,600 --> 00:25:31,970
WebSocket operator and in the end after

00:25:29,659 --> 00:25:34,640
the WebSocket there is a pub sub server

00:25:31,970 --> 00:25:36,620
so in if you build your own system you

00:25:34,640 --> 00:25:39,980
would probably use Kafka in this case

00:25:36,620 --> 00:25:42,260
I'm using a simple WebSocket server that

00:25:39,980 --> 00:25:44,720
is connecting HTTP on one side and

00:25:42,260 --> 00:25:46,909
WebSocket on the other side to tap in

00:25:44,720 --> 00:25:50,210
the visualization and the visualization

00:25:46,909 --> 00:25:53,659
is done with Cortana Cortana is actually

00:25:50,210 --> 00:25:55,760
very nice for such things as fast to set

00:25:53,659 --> 00:25:58,610
up something and it was made for time

00:25:55,760 --> 00:26:00,289
series for monitoring in time series and

00:25:58,610 --> 00:26:02,390
it's really easy to visualize such data

00:26:00,289 --> 00:26:05,570
and you can use it for tabular data too

00:26:02,390 --> 00:26:06,950
and so the source code of the

00:26:05,570 --> 00:26:09,380
application you can find it here I will

00:26:06,950 --> 00:26:11,840
share the slides after the talk the end

00:26:09,380 --> 00:26:15,399
product will be this simple dashboards

00:26:11,840 --> 00:26:21,020
so on the top you see the top hash tags

00:26:15,399 --> 00:26:23,539
you see on the left side the the hash

00:26:21,020 --> 00:26:25,340
tags on the right side accounts and this

00:26:23,539 --> 00:26:27,080
extra column is just there so that I can

00:26:25,340 --> 00:26:28,760
sort it in Co Farnham that's really the

00:26:27,080 --> 00:26:31,429
only reason why this label column is

00:26:28,760 --> 00:26:33,260
here and then the lower panel you see

00:26:31,429 --> 00:26:35,270
time series and what you will see later

00:26:33,260 --> 00:26:38,190
when I run it you will see that the last

00:26:35,270 --> 00:26:40,350
is a minute minute intervals

00:26:38,190 --> 00:26:42,149
was altered yes these are minute

00:26:40,350 --> 00:26:44,190
intervals and you will see that the last

00:26:42,149 --> 00:26:46,679
interval already delight the last minute

00:26:44,190 --> 00:26:49,190
will always update because that's in

00:26:46,679 --> 00:26:51,090
basically under computation but we

00:26:49,190 --> 00:26:52,919
visualize it just as in the previous

00:26:51,090 --> 00:26:58,019
case we visualize the data as it's being

00:26:52,919 --> 00:27:01,740
computed so you see the changes okay so

00:26:58,019 --> 00:27:03,509
in terms of code and this is how it

00:27:01,740 --> 00:27:05,759
would look like you have a window

00:27:03,509 --> 00:27:07,440
operator in the apex library and then

00:27:05,759 --> 00:27:08,970
you set all the different options that

00:27:07,440 --> 00:27:10,950
you need on that window operator you

00:27:08,970 --> 00:27:14,179
tell how large the window should be that

00:27:10,950 --> 00:27:17,429
is happening here the five-minute window

00:27:14,179 --> 00:27:19,080
you also set you have to define in this

00:27:17,429 --> 00:27:21,419
case that you want to emit results

00:27:19,080 --> 00:27:22,710
before the window is complete we do not

00:27:21,419 --> 00:27:24,539
know want to know after five minutes

00:27:22,710 --> 00:27:25,889
what it was but we want to know

00:27:24,539 --> 00:27:28,230
immediately and we want to see the

00:27:25,889 --> 00:27:30,840
changes for the visualizations or after

00:27:28,230 --> 00:27:33,419
the current based trigger after every 25

00:27:30,840 --> 00:27:35,039
changes is the limit the intermediate

00:27:33,419 --> 00:27:38,220
result and it will keep on accumulating

00:27:35,039 --> 00:27:40,070
that's what this option means this

00:27:38,220 --> 00:27:43,470
actually follows closely how the beam

00:27:40,070 --> 00:27:46,259
model is defined in terms of windowing

00:27:43,470 --> 00:27:47,490
so I will not go into details here but

00:27:46,259 --> 00:27:49,169
this is where you would find more

00:27:47,490 --> 00:27:53,149
information what these things are and

00:27:49,169 --> 00:27:55,860
why they are useful and then the second

00:27:53,149 --> 00:27:59,690
windowed operation is the top and it

00:27:55,860 --> 00:28:06,929
gets all the key key and count pals and

00:27:59,690 --> 00:28:09,840
image the top and top 10 so the next

00:28:06,929 --> 00:28:12,120
thing is then we have the count we have

00:28:09,840 --> 00:28:14,820
counted we have the top and the data

00:28:12,120 --> 00:28:17,399
goes into this snapshot server operator

00:28:14,820 --> 00:28:19,799
and the snapshot server operators job is

00:28:17,399 --> 00:28:22,139
to get a query and then to emit the

00:28:19,799 --> 00:28:24,750
result and this result operator is just

00:28:22,139 --> 00:28:26,970
a WebSocket think that could be

00:28:24,750 --> 00:28:29,580
WebSocket thing that could be Kafka

00:28:26,970 --> 00:28:31,409
think it could be anything that can talk

00:28:29,580 --> 00:28:34,259
to the in this case it's WebSocket

00:28:31,409 --> 00:28:36,690
because this is the pops up mechanism

00:28:34,259 --> 00:28:40,409
that is used in this in this small

00:28:36,690 --> 00:28:42,840
example WebSocket - HTTP and then qivana

00:28:40,409 --> 00:28:45,720
on the other end - are continuously

00:28:42,840 --> 00:28:47,460
pause I had that on the screen when I

00:28:45,720 --> 00:28:49,710
show it again look on the upper right

00:28:47,460 --> 00:28:51,080
corner you see one-second refresh so it

00:28:49,710 --> 00:28:53,029
continuously will hit

00:28:51,080 --> 00:28:55,850
pops-up server and look at the latest

00:28:53,029 --> 00:28:57,499
data if you have something that is of

00:28:55,850 --> 00:28:59,149
course push-based you could do that too

00:28:57,499 --> 00:29:04,149
in different clients but this is

00:28:59,149 --> 00:29:06,440
horrifying works the Cortana data source

00:29:04,149 --> 00:29:10,489
adapter and the pops up server they are

00:29:06,440 --> 00:29:12,919
here in get up to you can look at that

00:29:10,489 --> 00:29:15,710
so the queryable state parts we

00:29:12,919 --> 00:29:18,200
instantiate the snapshot server and then

00:29:15,710 --> 00:29:19,850
we instantiate the WebSocket output

00:29:18,200 --> 00:29:23,029
operator we need to tell the WebSocket

00:29:19,850 --> 00:29:24,559
output operator via the addresses of the

00:29:23,029 --> 00:29:26,809
snapshot server that's a configuration

00:29:24,559 --> 00:29:29,749
that needs to happen we also need to

00:29:26,809 --> 00:29:34,340
tell the snapshot server what the schema

00:29:29,749 --> 00:29:38,210
is so remember upstream was computation

00:29:34,340 --> 00:29:40,159
of hashtag count and in this extra label

00:29:38,210 --> 00:29:42,499
column so we need to tell that those are

00:29:40,159 --> 00:29:45,139
the fields that are available in the

00:29:42,499 --> 00:29:47,929
incoming data stream and then the query

00:29:45,139 --> 00:29:50,450
will say which fields we want in this

00:29:47,929 --> 00:29:53,690
case pores are the same but they could

00:29:50,450 --> 00:29:55,850
be different so I will I have recorded

00:29:53,690 --> 00:29:57,379
the steps of just bringing up the

00:29:55,850 --> 00:29:59,809
different bits and pieces so this is

00:29:57,379 --> 00:30:01,159
launching the apex application it is not

00:29:59,809 --> 00:30:03,710
running on the cluster is running as a

00:30:01,159 --> 00:30:06,139
it was a unit test driver locally so you

00:30:03,710 --> 00:30:08,210
see these exceptions rolling by this is

00:30:06,139 --> 00:30:11,149
because it cannot connect to the pops-up

00:30:08,210 --> 00:30:13,940
server yet so it fails to connect to

00:30:11,149 --> 00:30:18,859
that address so it pops up server after

00:30:13,940 --> 00:30:21,529
it started we will see those go away the

00:30:18,859 --> 00:30:27,470
exceptions and we will see tweets being

00:30:21,529 --> 00:30:28,999
processed okay so there's some extra

00:30:27,470 --> 00:30:30,980
logging the other choice you see there

00:30:28,999 --> 00:30:34,369
are not many tweets right it's just it

00:30:30,980 --> 00:30:36,379
was just running on the laptop and and

00:30:34,369 --> 00:30:38,989
also many tweets to consume from the API

00:30:36,379 --> 00:30:40,940
but you can imagine any other data

00:30:38,989 --> 00:30:43,039
source that produces more data like the

00:30:40,940 --> 00:30:46,039
attack use case so here this is just a

00:30:43,039 --> 00:30:48,559
test with curl to hit the HT with HTTP

00:30:46,039 --> 00:30:50,179
now the pops up so the pops-up server

00:30:48,559 --> 00:30:52,519
receives the data from the application

00:30:50,179 --> 00:30:54,919
with WebSocket you can use curl to check

00:30:52,519 --> 00:30:59,059
whether the data is available now we

00:30:54,919 --> 00:31:01,850
will start the Ravana adapter and that

00:30:59,059 --> 00:31:04,009
will now talk to the WebSocket or to the

00:31:01,850 --> 00:31:04,850
Pops of server and do what we did

00:31:04,009 --> 00:31:06,769
manually with

00:31:04,850 --> 00:31:09,529
before it would put pull the data from

00:31:06,769 --> 00:31:11,990
there and then you can see in the front

00:31:09,529 --> 00:31:13,730
end the data updating but you probably

00:31:11,990 --> 00:31:18,110
see more change than the patent panel

00:31:13,730 --> 00:31:21,399
the top text they change slower but you

00:31:18,110 --> 00:31:24,289
see the last two intervals changing I

00:31:21,399 --> 00:31:26,690
think it's going to go back there this

00:31:24,289 --> 00:31:28,009
was just a shawl so the multiple

00:31:26,690 --> 00:31:29,480
components right there's the apex

00:31:28,009 --> 00:31:32,509
application that would normally run on a

00:31:29,480 --> 00:31:34,940
cluster in this case one as a insider

00:31:32,509 --> 00:31:39,679
j-unit driver and embedded mode there is

00:31:34,940 --> 00:31:42,259
a pops up mechanism that it provides an

00:31:39,679 --> 00:31:44,419
endpoint that we can talk to from

00:31:42,259 --> 00:31:47,840
Cortana and then the Cortana

00:31:44,419 --> 00:31:50,360
sync and then here you see now the

00:31:47,840 --> 00:31:52,610
updates happening as the data is being

00:31:50,360 --> 00:31:55,940
computed right it's always the last two

00:31:52,610 --> 00:31:59,570
intervals the change and then they will

00:31:55,940 --> 00:32:02,899
be static so that's a way how you can do

00:31:59,570 --> 00:32:07,399
time time serious computation and

00:32:02,899 --> 00:32:11,120
aggregation and visualization with apex

00:32:07,399 --> 00:32:13,399
so apex recent additions and roadmap

00:32:11,120 --> 00:32:16,340
quickly what has happened over the over

00:32:13,399 --> 00:32:19,039
the last year approximately there's an

00:32:16,340 --> 00:32:21,500
apex runner in Apache beam now we added

00:32:19,039 --> 00:32:23,950
support for iterative processing it

00:32:21,500 --> 00:32:27,529
means you can do machine learning

00:32:23,950 --> 00:32:29,500
algorithms too and to prove that an

00:32:27,529 --> 00:32:32,419
integration was done with Apache Samoa

00:32:29,500 --> 00:32:34,879
then the SQL supported already mentioned

00:32:32,419 --> 00:32:37,039
the state management incremental state

00:32:34,879 --> 00:32:39,169
management that means stateful

00:32:37,039 --> 00:32:41,059
processing is good but we also need to

00:32:39,169 --> 00:32:43,820
handle it very cope it's very large

00:32:41,059 --> 00:32:45,590
state if historical data is involved if

00:32:43,820 --> 00:32:47,090
you have to keep a lot and have to have

00:32:45,590 --> 00:32:49,039
an efficient way to store that so

00:32:47,090 --> 00:32:52,100
there's a component to do that in the

00:32:49,039 --> 00:32:55,639
library the support for control tupis

00:32:52,100 --> 00:32:57,860
was also added which enables podcasting

00:32:55,639 --> 00:32:59,450
of tuples across partitions in a

00:32:57,860 --> 00:33:01,159
consistent way which is needed for

00:32:59,450 --> 00:33:06,019
watermarks but also for batch control

00:33:01,159 --> 00:33:07,549
and then some things on the roadmap apex

00:33:06,019 --> 00:33:09,500
is native streaming platform and it

00:33:07,549 --> 00:33:11,090
really started as a with the goal of

00:33:09,500 --> 00:33:12,889
doing real-time streaming right but you

00:33:11,090 --> 00:33:14,360
can do batch processing too but there

00:33:12,889 --> 00:33:17,149
are some enhancements to make this

00:33:14,360 --> 00:33:19,220
simpler from a user's perspective then

00:33:17,149 --> 00:33:21,230
support for other cluster manager

00:33:19,220 --> 00:33:22,910
and also support for Python because

00:33:21,230 --> 00:33:24,500
there really a lot of people that know

00:33:22,910 --> 00:33:26,390
Python and they want to use the

00:33:24,500 --> 00:33:28,010
libraries that are available and they

00:33:26,390 --> 00:33:31,040
would like to be able to do that in a

00:33:28,010 --> 00:33:34,010
JVM based environment also so and then a

00:33:31,040 --> 00:33:49,700
few links and a few minutes for

00:33:34,010 --> 00:33:52,250
questions hi so my question is if you

00:33:49,700 --> 00:33:55,430
were to write this application now would

00:33:52,250 --> 00:33:58,400
you write it in native apex or using the

00:33:55,430 --> 00:34:01,430
beam API well today I would still use

00:33:58,400 --> 00:34:05,630
the native apex API because the runner

00:34:01,430 --> 00:34:08,120
isn't really on par with what FX

00:34:05,630 --> 00:34:10,550
underneath can do right there is still

00:34:08,120 --> 00:34:12,980
work that needs to go into Toronto but

00:34:10,550 --> 00:34:15,080
what you see is these things are

00:34:12,980 --> 00:34:18,590
converging right the beam model and the

00:34:15,080 --> 00:34:20,210
semantics you already see them repeated

00:34:18,590 --> 00:34:22,970
in multiple more than one stream

00:34:20,210 --> 00:34:25,760
processing framework out there right so

00:34:22,970 --> 00:34:27,110
the native API is and how beam looks

00:34:25,760 --> 00:34:28,850
like they are getting closer to each

00:34:27,110 --> 00:34:31,340
other and I think it will happen with

00:34:28,850 --> 00:34:34,190
other features too that are more

00:34:31,340 --> 00:34:36,890
non-functional right the scalability out

00:34:34,190 --> 00:34:39,440
partitioning both efficiency so you will

00:34:36,890 --> 00:34:41,450
as of today and I think this is not just

00:34:39,440 --> 00:34:42,950
the case for apex but also for for the

00:34:41,450 --> 00:34:46,190
other runner implementations you will

00:34:42,950 --> 00:34:48,440
still see that the native API czar give

00:34:46,190 --> 00:34:50,659
you better performance or certain things

00:34:48,440 --> 00:34:52,490
that are not exposed to being yet or not

00:34:50,659 --> 00:34:54,050
available so being yet but I think

00:34:52,490 --> 00:34:58,010
that's probably going to change over

00:34:54,050 --> 00:35:02,350
time so in other words you don't think

00:34:58,010 --> 00:35:05,690
beam is really ready for primetime you

00:35:02,350 --> 00:35:07,940
you'd be doing it on native and well I

00:35:05,690 --> 00:35:10,130
would I would do it of course I would I

00:35:07,940 --> 00:35:11,690
would do native because that's what I

00:35:10,130 --> 00:35:15,710
know right from but from a user's

00:35:11,690 --> 00:35:17,630
perspective look at your use case that

00:35:15,710 --> 00:35:19,430
you have in what is your data volume

00:35:17,630 --> 00:35:22,640
right that's the efficiency matter at

00:35:19,430 --> 00:35:26,360
all because it's a similar discussion

00:35:22,640 --> 00:35:29,210
like the latency right we say nginx does

00:35:26,360 --> 00:35:30,460
so in so milliseconds and the other one

00:35:29,210 --> 00:35:31,960
only seconds

00:35:30,460 --> 00:35:34,330
doesn't matter for you that's the real

00:35:31,960 --> 00:35:37,060
question so what kind of processing

00:35:34,330 --> 00:35:40,690
logic do you need do you really need to

00:35:37,060 --> 00:35:43,119
have a very large state it doesn't

00:35:40,690 --> 00:35:45,790
matter if if the native API is a little

00:35:43,119 --> 00:35:47,560
bit more efficient in your case so I

00:35:45,790 --> 00:35:50,640
think that's what I would probably

00:35:47,560 --> 00:35:53,200
approach it I would not say completely

00:35:50,640 --> 00:35:55,930
go with native API because there are the

00:35:53,200 --> 00:35:57,940
portability advantages to that certain

00:35:55,930 --> 00:36:00,339
users like right for some users is

00:35:57,940 --> 00:36:02,740
important for others not your pipeline

00:36:00,339 --> 00:36:06,160
portability if you know how to run

00:36:02,740 --> 00:36:08,619
multiple platforms in your opera in your

00:36:06,160 --> 00:36:10,240
operational environment then the

00:36:08,619 --> 00:36:12,550
portability might be interesting to you

00:36:10,240 --> 00:36:14,260
right if you only know one thing and you

00:36:12,550 --> 00:36:17,470
already know that you're going to run

00:36:14,260 --> 00:36:19,300
spark right then well why not use the

00:36:17,470 --> 00:36:21,400
native spark API if you decided that you

00:36:19,300 --> 00:36:23,470
want to use the effects why not use the

00:36:21,400 --> 00:36:31,210
native epics API and maybe it makes

00:36:23,470 --> 00:36:33,609
certain things easier hi thanks for talk

00:36:31,210 --> 00:36:35,500
at the in your work example you had a

00:36:33,609 --> 00:36:40,030
Pope's observer your own Pope's observer

00:36:35,500 --> 00:36:42,550
what why didn't you use in 60 B well

00:36:40,030 --> 00:36:44,050
because pops-up Silva was already there

00:36:42,550 --> 00:36:46,359
I would say right

00:36:44,050 --> 00:36:48,520
we have pops-up operators yes you can

00:36:46,359 --> 00:36:52,800
you can use probably also Reddy's right

00:36:48,520 --> 00:36:56,140
fault for this it's it's a demo alright

00:36:52,800 --> 00:36:58,420
so in the real production application

00:36:56,140 --> 00:37:00,250
that I talked about it's using Kafka and

00:36:58,420 --> 00:37:02,109
the reason why is using Kafka is does a

00:37:00,250 --> 00:37:05,849
cover class already they know how to run

00:37:02,109 --> 00:37:08,710
Kafka and Kafka is good and in for this

00:37:05,849 --> 00:37:11,130
so pick pick what is good and so if X

00:37:08,710 --> 00:37:13,720
has the connectors to use different

00:37:11,130 --> 00:37:20,980
message process right you can use active

00:37:13,720 --> 00:37:23,890
MQ any JMS based thing RabbitMQ so it's

00:37:20,980 --> 00:37:26,760
it's an example I'm not recommending you

00:37:23,890 --> 00:37:26,760
to use WebSockets

00:37:29,630 --> 00:37:38,769
okay so thank you thank you for now

00:37:34,230 --> 00:37:38,769

YouTube URL: https://www.youtube.com/watch?v=mstkxTh18ME


