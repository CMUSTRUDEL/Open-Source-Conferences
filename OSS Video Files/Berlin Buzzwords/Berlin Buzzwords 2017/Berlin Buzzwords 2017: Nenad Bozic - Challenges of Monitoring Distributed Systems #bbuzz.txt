Title: Berlin Buzzwords 2017: Nenad Bozic - Challenges of Monitoring Distributed Systems #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Back in the days, you had a single machine and you could scroll down the single log file to figure out what is going on. In this Big Data world you need to combine a lot of logs together to figure out what is going on. Data is coming in huge volumes, with high speed so choosing important information and getting rid of noise becomes real challenge. There is a need for a centralized monitoring platform which will aid the engineers operating the systems, and serve the right information at the right time.

This talk will try to help you understand all the challenges and you will get an idea which tools and technology stacks are good fit to successfully monitor Big Data systems. The focus will be on open source and free solutions. The problem can be separated in two domains which both are the subject of this talk: metrics stack to gather simple metrics on central place and log stack to aggregate logs from different machines to central place. We will finish up with a combined stack and ideas how it can be improved even further with alerting and automated failover scenarios.

Read more:
https://2017.berlinbuzzwords.de/17/session/challenges-monitoring-distributed-systems

About Nenad Bozic:
https://2017.berlinbuzzwords.de/users/nenad-bozic

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,380 --> 00:00:10,809
hi and welcome to my presentation today

00:00:08,559 --> 00:00:11,870
we'll be talking about the challenges of

00:00:10,809 --> 00:00:14,719
monitoring this

00:00:11,870 --> 00:00:17,450
systems my name is Nana borage and I'm

00:00:14,719 --> 00:00:20,450
coming from Serbia from company's market

00:00:17,450 --> 00:00:23,300
where I'm I co-founded that company and

00:00:20,450 --> 00:00:25,610
I'm working as a technical consultant we

00:00:23,300 --> 00:00:27,650
are dealing with we are doing big data

00:00:25,610 --> 00:00:31,419
engineering consulting in the data

00:00:27,650 --> 00:00:34,340
science and in our day to day business

00:00:31,419 --> 00:00:36,710
clients call us when problem arises and

00:00:34,340 --> 00:00:38,780
we need the good and powerful insights

00:00:36,710 --> 00:00:43,070
from monitoring existence in order to

00:00:38,780 --> 00:00:46,219
pinpoint the problem and fix stuff it is

00:00:43,070 --> 00:00:47,870
sort of similar like when your car is

00:00:46,219 --> 00:00:51,109
broken first thing that you will do is

00:00:47,870 --> 00:00:54,769
go to mechanic modern cars these days

00:00:51,109 --> 00:00:57,859
have powerful board computers and you

00:00:54,769 --> 00:01:00,769
will hook in laptop mechanic wheel who

00:00:57,859 --> 00:01:03,920
can laptop check out certain graphs and

00:01:00,769 --> 00:01:07,970
figure out what part of the car is

00:01:03,920 --> 00:01:11,030
broken and get on with fixing this part

00:01:07,970 --> 00:01:14,450
of your engine we need the same things

00:01:11,030 --> 00:01:16,520
in IT systems and we're we are building

00:01:14,450 --> 00:01:18,800
many many companies these days are

00:01:16,520 --> 00:01:20,990
building and trying to get the best

00:01:18,800 --> 00:01:24,290
monitoring solution out there it's

00:01:20,990 --> 00:01:27,350
really hot topic how many of you are

00:01:24,290 --> 00:01:27,860
working with distributed systems these

00:01:27,350 --> 00:01:31,130
days

00:01:27,860 --> 00:01:33,350
can you raise you can so the through the

00:01:31,130 --> 00:01:34,970
system starts to be an new normal and

00:01:33,350 --> 00:01:38,930
how many of you are operating those

00:01:34,970 --> 00:01:41,300
systems so maintaining monitoring yeah

00:01:38,930 --> 00:01:44,660
feel your pain monitoring distributed

00:01:41,300 --> 00:01:47,000
systems is really hard when you have

00:01:44,660 --> 00:01:49,940
single instance basically if that the

00:01:47,000 --> 00:01:52,250
instance dies I think it's easy to

00:01:49,940 --> 00:01:54,560
figure out why when you have distributed

00:01:52,250 --> 00:01:56,720
systems failure on one one not the

00:01:54,560 --> 00:01:59,150
propagates to the other node and so on

00:01:56,720 --> 00:02:01,490
so things get complicated when you try

00:01:59,150 --> 00:02:04,760
to correlate the failures from different

00:02:01,490 --> 00:02:07,160
machines so here is the agenda we will

00:02:04,760 --> 00:02:10,550
first run and set some ground rules in

00:02:07,160 --> 00:02:12,470
monitoring the main and we will explain

00:02:10,550 --> 00:02:15,200
important terms then we will run through

00:02:12,470 --> 00:02:18,950
metrics data stream and tools which may

00:02:15,200 --> 00:02:22,219
help you visualize metrics then we will

00:02:18,950 --> 00:02:24,890
run through log data stream and tools

00:02:22,219 --> 00:02:27,440
which can help you collect log events

00:02:24,890 --> 00:02:28,730
after that we will show you how I will

00:02:27,440 --> 00:02:32,390
show you how you can combine both

00:02:28,730 --> 00:02:35,900
metrics and logs because looking at the

00:02:32,390 --> 00:02:38,390
metrics alone or logs alone is not

00:02:35,900 --> 00:02:40,700
enough to explain all the failures I

00:02:38,390 --> 00:02:43,040
think it's much better to combine those

00:02:40,700 --> 00:02:45,920
two and be in full control and last but

00:02:43,040 --> 00:02:48,110
not least we'll cover alerting and we

00:02:45,920 --> 00:02:50,270
will explain how you can step away from

00:02:48,110 --> 00:02:52,489
dashboards and continue on building your

00:02:50,270 --> 00:02:56,330
business logic while your let machine

00:02:52,489 --> 00:02:59,300
alert when certain failures happens I'm

00:02:56,330 --> 00:03:02,540
a big fan of domain driven design and

00:02:59,300 --> 00:03:04,160
first thing that you will do is set some

00:03:02,540 --> 00:03:06,110
ground rules and create the main

00:03:04,160 --> 00:03:08,630
language so let's do the same thing with

00:03:06,110 --> 00:03:11,060
monitoring how we like to look at

00:03:08,630 --> 00:03:13,700
monitoring is it consists of three parts

00:03:11,060 --> 00:03:16,070
you have metrics logs and alerts metrics

00:03:13,700 --> 00:03:19,400
are showing you that your system is

00:03:16,070 --> 00:03:21,290
performing within thresholds or outside

00:03:19,400 --> 00:03:24,560
of thresholds and then it's time to

00:03:21,290 --> 00:03:26,390
investigate log data string is providing

00:03:24,560 --> 00:03:29,000
context so usually when you have certain

00:03:26,390 --> 00:03:31,519
peaks and metrics you want to go to log

00:03:29,000 --> 00:03:35,959
and figure out what is going on what

00:03:31,519 --> 00:03:39,560
caused this peak to rise alerts as I

00:03:35,959 --> 00:03:41,660
said it is important not look when you

00:03:39,560 --> 00:03:44,329
don't have money for IT department

00:03:41,660 --> 00:03:46,070
looking at that for 24/7 you need to let

00:03:44,329 --> 00:03:49,220
machine you need to figure out certain

00:03:46,070 --> 00:03:52,100
rules let machine a large events problem

00:03:49,220 --> 00:03:56,030
is happening and then go and browse and

00:03:52,100 --> 00:03:58,730
a rest of the time spent developing new

00:03:56,030 --> 00:04:01,489
features which are driving your business

00:03:58,730 --> 00:04:03,590
all these three parts doing all these

00:04:01,489 --> 00:04:06,290
three parts right is hard you need both

00:04:03,590 --> 00:04:08,630
technical skill set and you need to

00:04:06,290 --> 00:04:11,510
understand business domain at this

00:04:08,630 --> 00:04:14,510
picture I am going back to our example

00:04:11,510 --> 00:04:17,390
with mechanic he is looking at their

00:04:14,510 --> 00:04:22,190
board on computer and this Airport to us

00:04:17,390 --> 00:04:24,650
doesn't explain much I would say he he

00:04:22,190 --> 00:04:27,050
knows the mechanic domain and he knows

00:04:24,650 --> 00:04:30,080
certain parameters and what is normal

00:04:27,050 --> 00:04:32,120
what is outside of normal his expert in

00:04:30,080 --> 00:04:33,800
his field so he is both technical

00:04:32,120 --> 00:04:35,720
experts and probably understand the main

00:04:33,800 --> 00:04:38,540
so he will ask you whether you're

00:04:35,720 --> 00:04:40,340
driving frequently whether your drive

00:04:38,540 --> 00:04:42,200
in Serbia where you see a bunch of bumps

00:04:40,340 --> 00:04:46,430
or you're driving in Germany where

00:04:42,200 --> 00:04:47,870
everything is flat so he needs this

00:04:46,430 --> 00:04:50,990
domain knowledge and technical knowledge

00:04:47,870 --> 00:04:53,420
to reason about those insights that

00:04:50,990 --> 00:04:53,950
monitoring solutions are giving giving

00:04:53,420 --> 00:04:57,160
us

00:04:53,950 --> 00:05:00,740
so let's first touch metric data stream

00:04:57,160 --> 00:05:03,050
as I said they sell metrics data stream

00:05:00,740 --> 00:05:07,250
is indicator that everything is working

00:05:03,050 --> 00:05:09,260
within expected boundaries in my

00:05:07,250 --> 00:05:11,510
experience it is easily forgotten

00:05:09,260 --> 00:05:14,080
feature while you're developing your

00:05:11,510 --> 00:05:16,610
application metrics are pushed aside and

00:05:14,080 --> 00:05:19,250
we will do this when we are in

00:05:16,610 --> 00:05:21,320
production let's build feature X feature

00:05:19,250 --> 00:05:22,940
Y and stuff and then you go to

00:05:21,320 --> 00:05:26,090
production without any metrics and

00:05:22,940 --> 00:05:27,230
failure starts happening and then it is

00:05:26,090 --> 00:05:30,410
already too late

00:05:27,230 --> 00:05:32,780
having good dashboards having good

00:05:30,410 --> 00:05:35,420
visualization it's hard probably you

00:05:32,780 --> 00:05:36,950
will first start graphing everything

00:05:35,420 --> 00:05:39,260
then you will see that there is a lot of

00:05:36,950 --> 00:05:40,370
noise and you cannot look at that

00:05:39,260 --> 00:05:42,560
anymore

00:05:40,370 --> 00:05:44,240
then you will delete stuff then failure

00:05:42,560 --> 00:05:46,520
will happen and you will figure out that

00:05:44,240 --> 00:05:50,030
you're missing important information so

00:05:46,520 --> 00:05:52,130
it's iterative process and try to I

00:05:50,030 --> 00:05:55,400
don't know try to cut retrospect this

00:05:52,130 --> 00:05:58,160
talk with your team and try to try to

00:05:55,400 --> 00:06:00,050
add stuff which is missing and delete

00:05:58,160 --> 00:06:02,930
stuff which is creating noise in

00:06:00,050 --> 00:06:04,550
distributed systems it is even harder

00:06:02,930 --> 00:06:07,850
because you have many graphs to watch

00:06:04,550 --> 00:06:10,640
and it is easy to fall into information

00:06:07,850 --> 00:06:13,070
overload trap having bunch of machines

00:06:10,640 --> 00:06:15,380
sending bunch of metrics we can add

00:06:13,070 --> 00:06:18,230
bunch of graphs so you really need to

00:06:15,380 --> 00:06:21,590
have these retrospectives and tighten up

00:06:18,230 --> 00:06:23,510
your monitoring stack when you're

00:06:21,590 --> 00:06:25,970
thinking about metrics data stream

00:06:23,510 --> 00:06:28,150
you're faced with decisions which stack

00:06:25,970 --> 00:06:31,160
to choose where should you vote with

00:06:28,150 --> 00:06:33,680
self-managed solution or software is a

00:06:31,160 --> 00:06:35,780
service solution also there is a

00:06:33,680 --> 00:06:37,700
discretion whether to pay for something

00:06:35,780 --> 00:06:40,250
or use something which is free

00:06:37,700 --> 00:06:44,090
open-source those decisions are

00:06:40,250 --> 00:06:46,190
important and mostly they are made based

00:06:44,090 --> 00:06:49,460
on your use case so you know you need to

00:06:46,190 --> 00:06:51,950
know use case I can give you context how

00:06:49,460 --> 00:06:54,830
we how we reasoned about it

00:06:51,950 --> 00:06:58,060
we needed to provide the guarantees the

00:06:54,830 --> 00:07:01,820
latest guarantees in 24 hours banned for

00:06:58,060 --> 00:07:04,670
99.999% of requests so they need it to

00:07:01,820 --> 00:07:07,760
be under certain threshold every went

00:07:04,670 --> 00:07:10,970
with the data dog anyone from data dog

00:07:07,760 --> 00:07:13,160
in audience I had this thought three

00:07:10,970 --> 00:07:15,440
weeks ago and on a patch gone and the

00:07:13,160 --> 00:07:17,510
guy from data dog was in audience so I

00:07:15,440 --> 00:07:18,590
need to I needed to pay attention what

00:07:17,510 --> 00:07:23,270
I'm talking about

00:07:18,590 --> 00:07:27,170
data dog has 300 dots on your on your

00:07:23,270 --> 00:07:30,530
graph so well if you look at 24-hour

00:07:27,170 --> 00:07:33,890
period it will show you averages not raw

00:07:30,530 --> 00:07:37,820
data and in order to provide the SLA on

00:07:33,890 --> 00:07:40,220
this high 99.999% of request during a

00:07:37,820 --> 00:07:43,220
day you need really raw data and data

00:07:40,220 --> 00:07:46,340
dog didn't work for us so we decided to

00:07:43,220 --> 00:07:48,980
go with the self managed solution it is

00:07:46,340 --> 00:07:51,290
trade-off for self managed solution you

00:07:48,980 --> 00:07:53,990
will spend most of your time maintaining

00:07:51,290 --> 00:07:57,200
one more system but then again you will

00:07:53,990 --> 00:08:00,530
have better level of control and you can

00:07:57,200 --> 00:08:03,200
put monitoring stack inside your V PC

00:08:00,530 --> 00:08:05,840
and you can secure your data it is not

00:08:03,200 --> 00:08:10,250
always an option to send metrics to some

00:08:05,840 --> 00:08:12,080
third-party cloud-based provider so you

00:08:10,250 --> 00:08:16,310
need to know your use case and make a

00:08:12,080 --> 00:08:17,870
decision based on use case so let's run

00:08:16,310 --> 00:08:19,790
through stack that we choose the

00:08:17,870 --> 00:08:23,240
building this self managed solution

00:08:19,790 --> 00:08:26,690
which is the ratio chosen Rieman and

00:08:23,240 --> 00:08:30,320
sync server Ramon is the processing

00:08:26,690 --> 00:08:32,360
framework which is acting as a server on

00:08:30,320 --> 00:08:35,090
matrix machine and you're using

00:08:32,360 --> 00:08:37,520
different clients on your applications

00:08:35,090 --> 00:08:39,620
Raymond Cal Java client Python client

00:08:37,520 --> 00:08:41,810
and all bunch of clients and you're

00:08:39,620 --> 00:08:43,849
basically sending selection of your

00:08:41,810 --> 00:08:46,730
metrics to Raymond server and Raymond

00:08:43,849 --> 00:08:50,420
server is processing and storing those

00:08:46,730 --> 00:08:51,800
events it has integration with influx DB

00:08:50,420 --> 00:08:54,950
and we choose a name for the baby

00:08:51,800 --> 00:08:58,160
because it has a measurement in its core

00:08:54,950 --> 00:09:01,070
info DB is a time series database which

00:08:58,160 --> 00:09:03,470
is storing measurements with timestamp

00:09:01,070 --> 00:09:05,540
value fields and tags so it's perfect

00:09:03,470 --> 00:09:08,389
for time series

00:09:05,540 --> 00:09:11,779
it's built for that this is its sole

00:09:08,389 --> 00:09:13,370
purpose and it has good integration with

00:09:11,779 --> 00:09:15,350
gray fauna which we are using for

00:09:13,370 --> 00:09:17,690
visualization your fauna is a

00:09:15,350 --> 00:09:20,389
visualization open-source visualization

00:09:17,690 --> 00:09:22,880
tool with many visual aids you can plot

00:09:20,389 --> 00:09:26,630
graphs time series tables gouges

00:09:22,880 --> 00:09:28,759
whatever and you can plug data sources

00:09:26,630 --> 00:09:32,319
and in fact DB is one of those data

00:09:28,759 --> 00:09:36,139
sources and this is how one of

00:09:32,319 --> 00:09:40,519
dashboards that we build looks like this

00:09:36,139 --> 00:09:43,339
is Cassandra cluster and on top you can

00:09:40,519 --> 00:09:46,670
see easily that 12 nodes are up so it is

00:09:43,339 --> 00:09:48,680
basically a shell check and on the left

00:09:46,670 --> 00:09:51,319
and right this is a total request rate

00:09:48,680 --> 00:09:54,610
on our cluster and below that we plotted

00:09:51,319 --> 00:09:57,139
the request rate per host because

00:09:54,610 --> 00:09:59,690
dealing with this kind of hassle a and

00:09:57,139 --> 00:10:02,060
high nines we needed to figure out

00:09:59,690 --> 00:10:04,910
whether we have bad hosts in in our

00:10:02,060 --> 00:10:06,940
infrastructure so we needed really pair

00:10:04,910 --> 00:10:10,220
host request rate and you can see a peak

00:10:06,940 --> 00:10:12,709
in the beginning of the graph on single

00:10:10,220 --> 00:10:15,410
host there was increased request rate

00:10:12,709 --> 00:10:17,389
this is probably a good place to

00:10:15,410 --> 00:10:19,610
investigate further you can see that

00:10:17,389 --> 00:10:22,100
based on this therefore you cannot

00:10:19,610 --> 00:10:26,720
explain why this peak happened so you

00:10:22,100 --> 00:10:30,410
will either go to I don't know memory or

00:10:26,720 --> 00:10:33,949
disk or network statistics or your you

00:10:30,410 --> 00:10:36,740
will start start browsing locks when

00:10:33,949 --> 00:10:38,449
matrix Peaks metrics Peaks are

00:10:36,740 --> 00:10:42,589
indicators that something bad happened

00:10:38,449 --> 00:10:45,019
and log log log events our next thing

00:10:42,589 --> 00:10:47,690
that you will look look at log log will

00:10:45,019 --> 00:10:50,480
logs will provide context they will

00:10:47,690 --> 00:10:52,550
explain from technical standpoint what

00:10:50,480 --> 00:10:56,420
happened working caused across this peak

00:10:52,550 --> 00:10:59,240
and usually you're looking at metrics

00:10:56,420 --> 00:11:01,759
you're glancing it to into metrics but

00:10:59,240 --> 00:11:04,279
when problem arises you want to check

00:11:01,759 --> 00:11:08,120
check logs it has same challenges that

00:11:04,279 --> 00:11:10,880
with met with as with modern metrics you

00:11:08,120 --> 00:11:13,579
need not to have too much information

00:11:10,880 --> 00:11:16,279
you need not have too few information so

00:11:13,579 --> 00:11:18,110
it's again iterative process you will

00:11:16,279 --> 00:11:19,220
first start logging everything with info

00:11:18,110 --> 00:11:22,040
level then you will

00:11:19,220 --> 00:11:24,709
realize that you have a lot of noise and

00:11:22,040 --> 00:11:27,139
you will after you go to production you

00:11:24,709 --> 00:11:30,110
probably will narrow down log events

00:11:27,139 --> 00:11:33,139
only two important things in distributed

00:11:30,110 --> 00:11:36,470
systems it is again much harder because

00:11:33,139 --> 00:11:38,180
you have many terminals open and it's

00:11:36,470 --> 00:11:42,740
getting easy to fall into information

00:11:38,180 --> 00:11:46,490
overall threat this is how our initial

00:11:42,740 --> 00:11:51,339
approach looked like so with the help of

00:11:46,490 --> 00:11:53,899
CSS HX Mac tool we opened up 12

00:11:51,339 --> 00:11:57,139
terminals for 12 knots and we were

00:11:53,899 --> 00:12:00,560
tailing logs and no one could figure out

00:11:57,139 --> 00:12:02,689
what is going on because it's impossible

00:12:00,560 --> 00:12:05,240
to look at logs and distributed systems

00:12:02,689 --> 00:12:08,740
like this so we needed the better

00:12:05,240 --> 00:12:11,660
solution then again we were faced with

00:12:08,740 --> 00:12:13,879
decisions we needed to decide again

00:12:11,660 --> 00:12:15,649
whether to go with the self managed

00:12:13,879 --> 00:12:17,540
solution or software as a service

00:12:15,649 --> 00:12:22,040
solution and again we needed to figure

00:12:17,540 --> 00:12:24,490
out whether it it is good to pay for

00:12:22,040 --> 00:12:28,819
some solution or to use free solution

00:12:24,490 --> 00:12:31,160
decision based on our experience with

00:12:28,819 --> 00:12:34,069
data dog we decided to go with self

00:12:31,160 --> 00:12:35,829
managed solution but it boils down to

00:12:34,069 --> 00:12:38,180
the same things as with matrix tech

00:12:35,829 --> 00:12:39,709
technical team skill set the level of

00:12:38,180 --> 00:12:40,879
control that you want to have and

00:12:39,709 --> 00:12:43,120
security of your data

00:12:40,879 --> 00:12:47,360
I emphasize here security of your data

00:12:43,120 --> 00:12:49,220
matrix are just arbitrary values with

00:12:47,360 --> 00:12:52,610
timestamps so they are not providing

00:12:49,220 --> 00:12:55,490
many insight from your application with

00:12:52,610 --> 00:12:57,769
log story is different I seen many

00:12:55,490 --> 00:12:59,689
things in logs people are logging email

00:12:57,769 --> 00:13:02,420
addresses and stuff and it's not always

00:12:59,689 --> 00:13:05,449
an option to go with the cloud based

00:13:02,420 --> 00:13:07,879
provider and push many insights about

00:13:05,449 --> 00:13:11,660
your business to some arbitrary

00:13:07,879 --> 00:13:17,240
cloud-based the tool so we decided to go

00:13:11,660 --> 00:13:20,689
with anyone not familiar with LK here so

00:13:17,240 --> 00:13:23,480
yeah okay I guess became pretty much the

00:13:20,689 --> 00:13:26,000
standard when when you want to collect

00:13:23,480 --> 00:13:28,699
log events from a bunch of machines and

00:13:26,000 --> 00:13:30,649
visualize them on single place and the

00:13:28,699 --> 00:13:33,170
distributed systems it's a must you saw

00:13:30,649 --> 00:13:36,620
this 12 terminals example

00:13:33,170 --> 00:13:39,649
so it consists of elasticsearch location

00:13:36,620 --> 00:13:41,570
cabana file bit is placed on each of

00:13:39,649 --> 00:13:45,139
machines and it is listening to log

00:13:41,570 --> 00:13:47,899
events when new event arise it triggers

00:13:45,139 --> 00:13:50,209
it gets trigger and shifts it to log

00:13:47,899 --> 00:13:54,019
stash log stash is there to process your

00:13:50,209 --> 00:13:56,870
log events to split a log event into a

00:13:54,019 --> 00:13:59,240
bunch of parts or to do something about

00:13:56,870 --> 00:14:02,180
log event and then you really index your

00:13:59,240 --> 00:14:06,079
log event to elasticsearch which is good

00:14:02,180 --> 00:14:08,959
for for freeform free text searches

00:14:06,079 --> 00:14:11,899
which is usually what you do in with log

00:14:08,959 --> 00:14:14,690
event messages and on top of that you

00:14:11,899 --> 00:14:17,449
have cabanas visualization tool which

00:14:14,690 --> 00:14:20,329
will which is look which looks something

00:14:17,449 --> 00:14:25,160
like this so we on top you have a time

00:14:20,329 --> 00:14:28,130
span time span so you can you you can

00:14:25,160 --> 00:14:31,600
configure what to look here it is 15

00:14:28,130 --> 00:14:37,130
minutes below that there is this white

00:14:31,600 --> 00:14:39,589
leucine query freeform edit box so you

00:14:37,130 --> 00:14:41,779
can type any loose in query and in the

00:14:39,589 --> 00:14:44,240
middle you have log events sorted by

00:14:41,779 --> 00:14:47,870
time from different machines this way

00:14:44,240 --> 00:14:49,579
you can figure out at certain point in

00:14:47,870 --> 00:14:52,399
time what happened on each of your

00:14:49,579 --> 00:14:56,240
machines and how failure probably on one

00:14:52,399 --> 00:14:58,839
machine did reflect on other machines in

00:14:56,240 --> 00:14:58,839
your system

00:14:58,850 --> 00:15:04,040
Kuban also has visual visualization

00:15:01,519 --> 00:15:06,310
similar to what we showed with Griffin

00:15:04,040 --> 00:15:10,160
Oz so this is a application errors

00:15:06,310 --> 00:15:13,370
through time on our stack and you can

00:15:10,160 --> 00:15:15,709
see these hi graphs near the end of the

00:15:13,370 --> 00:15:18,260
time span and this is probably a good

00:15:15,709 --> 00:15:22,550
place to research a bit further and go

00:15:18,260 --> 00:15:26,540
into row logs but now we were faced with

00:15:22,550 --> 00:15:28,250
the - - visualization tools so in

00:15:26,540 --> 00:15:30,050
Griffin of you're looking metrics then

00:15:28,250 --> 00:15:33,290
we were jumping to cabana to look at

00:15:30,050 --> 00:15:37,040
logs and having to look at the two tools

00:15:33,290 --> 00:15:38,769
is not convenient we wanted better

00:15:37,040 --> 00:15:43,930
solutions we wanted to combine

00:15:38,769 --> 00:15:46,690
everything under one umbrella so

00:15:43,930 --> 00:15:49,300
again I want to return to real world

00:15:46,690 --> 00:15:53,020
example why we did all of this to give

00:15:49,300 --> 00:15:56,320
you a more context we needed to we

00:15:53,020 --> 00:16:00,520
needed to provide reliable guarantees

00:15:56,320 --> 00:16:02,710
that we'll hear latest you'll 99.999% of

00:16:00,520 --> 00:16:04,960
requests during 24 hours under hundred

00:16:02,710 --> 00:16:07,180
milliseconds and whole infrastructure

00:16:04,960 --> 00:16:10,150
was deployed to AWS and we had a bunch

00:16:07,180 --> 00:16:13,810
of machines 20 Cassandra not 10

00:16:10,150 --> 00:16:17,020
applications and it was really complex a

00:16:13,810 --> 00:16:19,870
really complex infrastructure so we

00:16:17,020 --> 00:16:21,700
needed the really good insight from our

00:16:19,870 --> 00:16:24,850
monitoring system in order to figure out

00:16:21,700 --> 00:16:29,470
because single peak can mess up our 24

00:16:24,850 --> 00:16:32,770
hour wrestle a girl fauna is a good tool

00:16:29,470 --> 00:16:34,920
for combination it has pluggable data

00:16:32,770 --> 00:16:37,779
sources and both in flux DB and

00:16:34,920 --> 00:16:40,360
elasticsearch can be used as data

00:16:37,779 --> 00:16:42,490
sources in your fauna and this is what

00:16:40,360 --> 00:16:44,650
triggered us we all reduce we are

00:16:42,490 --> 00:16:47,230
already using in flex DBS data source

00:16:44,650 --> 00:16:50,020
for our metrics and we wanted to bring

00:16:47,230 --> 00:16:52,209
our logs close to our metrics so we can

00:16:50,020 --> 00:16:54,190
visualize everything on a single screen

00:16:52,209 --> 00:16:59,500
and reason about it looking at single

00:16:54,190 --> 00:17:02,260
dashboard this is our stack I already

00:16:59,500 --> 00:17:05,620
explained the pieces of it so I will

00:17:02,260 --> 00:17:08,709
just run through it fast

00:17:05,620 --> 00:17:11,380
you remain clients on on machines fear

00:17:08,709 --> 00:17:14,770
these days switching to Telegraph as

00:17:11,380 --> 00:17:18,910
well and testing boat in parallel there

00:17:14,770 --> 00:17:21,550
are telegraphy the process open source

00:17:18,910 --> 00:17:23,980
processing tool by influx data it has a

00:17:21,550 --> 00:17:26,020
bunch of input plugins and bunch of

00:17:23,980 --> 00:17:29,080
output plugins and you can collect

00:17:26,020 --> 00:17:31,540
metrics I guess more easily than with

00:17:29,080 --> 00:17:33,340
than with Rieman because with dreamand

00:17:31,540 --> 00:17:35,940
you need to build application and the

00:17:33,340 --> 00:17:39,850
telegrapher example has out-of-the-box

00:17:35,940 --> 00:17:43,300
OS stack net starts these starts

00:17:39,850 --> 00:17:45,510
collecting and a lot of good things and

00:17:43,300 --> 00:17:48,220
we're storing metrics in influx DB and

00:17:45,510 --> 00:17:50,250
oranges are log data stream we are

00:17:48,220 --> 00:17:53,380
having file based sitting on machines

00:17:50,250 --> 00:17:55,900
shipping to log log stash log stash is

00:17:53,380 --> 00:17:58,460
processing stuff storing in LS research

00:17:55,900 --> 00:17:59,990
and we are still having

00:17:58,460 --> 00:18:02,390
to look at our roll logs because

00:17:59,990 --> 00:18:05,299
sometimes we need to jump in to roll

00:18:02,390 --> 00:18:07,190
logs and figure out what is going on we

00:18:05,299 --> 00:18:09,769
are using your phone on top of both in

00:18:07,190 --> 00:18:14,059
Flex DB and elasticsearch and we are

00:18:09,769 --> 00:18:14,980
creating dashboards like this on top we

00:18:14,059 --> 00:18:19,039
chef

00:18:14,980 --> 00:18:22,669
it looks a bit foggy but on top it's

00:18:19,039 --> 00:18:25,220
slow queries to extend or not basically

00:18:22,669 --> 00:18:27,799
we are storing each and every query

00:18:25,220 --> 00:18:30,710
about 25 milliseconds and you can see

00:18:27,799 --> 00:18:33,350
that around 11:30 there are two purple

00:18:30,710 --> 00:18:36,830
dots which are around 12 cycles which is

00:18:33,350 --> 00:18:39,380
not good so and below below that we are

00:18:36,830 --> 00:18:43,519
story we're showing drop messages on

00:18:39,380 --> 00:18:45,860
Cassandra Cassandra is good for for

00:18:43,519 --> 00:18:49,070
performance and it's full power tolerant

00:18:45,860 --> 00:18:51,919
and it is usually dropping messages when

00:18:49,070 --> 00:18:54,250
it has high load so it can serve more

00:18:51,919 --> 00:18:57,760
messages so we wanted to check how

00:18:54,250 --> 00:19:01,519
slowness on machine infla looks at

00:18:57,760 --> 00:19:03,559
through in Cassandra logs you can see

00:19:01,519 --> 00:19:06,110
that at the exact same time when we have

00:19:03,559 --> 00:19:08,210
12 cycles queries we have dropped

00:19:06,110 --> 00:19:12,350
messages on Cassandra and below that you

00:19:08,210 --> 00:19:16,190
can see what actual details from logs

00:19:12,350 --> 00:19:19,250
what kind of messages were drop mutation

00:19:16,190 --> 00:19:22,190
read messages and how many of them so

00:19:19,250 --> 00:19:25,100
looking at this soul dashboard you can

00:19:22,190 --> 00:19:29,029
figure out that latest single machine

00:19:25,100 --> 00:19:31,309
cause the cause the Cassandra to drop

00:19:29,029 --> 00:19:33,500
messages but looking at this dashboard

00:19:31,309 --> 00:19:36,380
you cannot figure out what was the root

00:19:33,500 --> 00:19:39,080
cause of this latency I deliberately

00:19:36,380 --> 00:19:43,490
didn't put our latest dashboard here we

00:19:39,080 --> 00:19:46,490
added the disk disk IO wait times on top

00:19:43,490 --> 00:19:48,649
because we sat down we look at this

00:19:46,490 --> 00:19:50,510
dashboard we realized that we cannot

00:19:48,649 --> 00:19:52,519
figure out what was the cause and we

00:19:50,510 --> 00:19:54,889
need we figured out that we needed to

00:19:52,519 --> 00:19:57,950
improve this dashboard so we added disk

00:19:54,889 --> 00:20:00,769
IO stats on top and then you save the

00:19:57,950 --> 00:20:03,710
full picture disk there was display to

00:20:00,769 --> 00:20:06,440
see on AWS EBS volume which caused the

00:20:03,710 --> 00:20:09,380
latency and center not responded with

00:20:06,440 --> 00:20:11,540
drop messages and this is whole journey

00:20:09,380 --> 00:20:14,030
and it's easy to reason about

00:20:11,540 --> 00:20:17,870
whole problem looking at single single

00:20:14,030 --> 00:20:19,940
debt world and last but not least

00:20:17,870 --> 00:20:23,770
because now we have combined metrics and

00:20:19,940 --> 00:20:26,600
logs we wanna as I said step away from

00:20:23,770 --> 00:20:28,220
building this man monitoring stack

00:20:26,600 --> 00:20:32,390
playing with that but no one wants to

00:20:28,220 --> 00:20:35,780
pay pay for us to visualize stuff and do

00:20:32,390 --> 00:20:40,190
these cool things people you need to

00:20:35,780 --> 00:20:43,160
develop features and leave machine to do

00:20:40,190 --> 00:20:46,310
alerting and let you know when it is

00:20:43,160 --> 00:20:49,190
your time to investigate it is similar

00:20:46,310 --> 00:20:54,080
like encar encar example usual your

00:20:49,190 --> 00:20:56,390
dashboard is dark and when you have

00:20:54,080 --> 00:20:59,960
certain failures or the problem it with

00:20:56,390 --> 00:21:01,760
the car certain lights will pop up and

00:20:59,960 --> 00:21:04,400
you know that it is time to call

00:21:01,760 --> 00:21:08,720
mechanic it is same Indian IT systems

00:21:04,400 --> 00:21:12,440
you will define your alerts and you will

00:21:08,720 --> 00:21:17,300
have your support team react on on those

00:21:12,440 --> 00:21:18,890
alerts a letter in alerting as I said is

00:21:17,300 --> 00:21:22,700
giving you freedom to step away from

00:21:18,890 --> 00:21:25,130
dashboards this is also important

00:21:22,700 --> 00:21:28,340
someone else place the domain knowledge

00:21:25,130 --> 00:21:31,550
about alert when designing alerts so

00:21:28,340 --> 00:21:34,790
this makes it possible not to have only

00:21:31,550 --> 00:21:37,940
experts looking at certain death words

00:21:34,790 --> 00:21:39,860
knowing which are thresholds of good

00:21:37,940 --> 00:21:42,170
healthy system and which are thresholds

00:21:39,860 --> 00:21:44,630
of faulty system so somebody else

00:21:42,170 --> 00:21:48,110
created is alert and you can just react

00:21:44,630 --> 00:21:52,160
and bug this guy hey investigate this

00:21:48,110 --> 00:21:54,980
timestamp same story with as with logs

00:21:52,160 --> 00:21:58,190
and metrics alerting is must be known

00:21:54,980 --> 00:22:01,010
must not be too frequent because you

00:21:58,190 --> 00:22:03,290
will tend start ignoring alerts so this

00:22:01,010 --> 00:22:09,530
is really important to keep in mind keep

00:22:03,290 --> 00:22:14,060
your alerts just just triggering on

00:22:09,530 --> 00:22:16,730
important events it is similar like all

00:22:14,060 --> 00:22:20,120
of you know the story about the boy who

00:22:16,730 --> 00:22:22,720
cried wolf it was boring he needed to

00:22:20,120 --> 00:22:25,040
guard the sheep's and he was crying wolf

00:22:22,720 --> 00:22:27,020
two or three times

00:22:25,040 --> 00:22:29,390
people from villa form nearby village

00:22:27,020 --> 00:22:35,150
game every time less and less people

00:22:29,390 --> 00:22:39,170
came from from village and after three

00:22:35,150 --> 00:22:42,530
times actual wolf came and he again

00:22:39,170 --> 00:22:45,350
cried wolf but no one no one came from

00:22:42,530 --> 00:22:49,070
village they were people from village or

00:22:45,350 --> 00:22:51,560
getting used to false alerts and he used

00:22:49,070 --> 00:22:53,960
all kind of false alerts and when

00:22:51,560 --> 00:22:57,590
failure happens when wolf came no one

00:22:53,960 --> 00:22:58,850
reacted this is scary scary situation

00:22:57,590 --> 00:23:00,950
and you don't want to be in that

00:22:58,850 --> 00:23:02,960
situation to get a failure of your

00:23:00,950 --> 00:23:05,320
database in production and no one

00:23:02,960 --> 00:23:08,420
noticing because they have a bunch of

00:23:05,320 --> 00:23:13,040
emails inside certain certain folder

00:23:08,420 --> 00:23:15,530
which are getting ignored we were we

00:23:13,040 --> 00:23:17,690
realized that we wanted some better

00:23:15,530 --> 00:23:20,180
solution here as well so we started

00:23:17,690 --> 00:23:23,960
building Sentinel smart alerting

00:23:20,180 --> 00:23:26,390
solution we wanted to have machine

00:23:23,960 --> 00:23:29,870
learning algorithm crunch these metrics

00:23:26,390 --> 00:23:33,680
and figure out doing anomaly detection

00:23:29,870 --> 00:23:37,040
and we I think real problem with alert

00:23:33,680 --> 00:23:40,100
is having humans make assumptions about

00:23:37,040 --> 00:23:44,080
the systems we are designing we're

00:23:40,100 --> 00:23:48,290
designing alerts and we can say that the

00:23:44,080 --> 00:23:51,110
PC CPU on 60% is good but after 60% we

00:23:48,290 --> 00:23:56,900
were fire alert but then again who knows

00:23:51,110 --> 00:24:00,680
if we have trend of our instances using

00:23:56,900 --> 00:24:02,240
more and more CPUs so this needs to move

00:24:00,680 --> 00:24:07,580
through time and we needed a better

00:24:02,240 --> 00:24:10,250
solution so we decided to hook spark

00:24:07,580 --> 00:24:13,580
streaming on our stack that I explained

00:24:10,250 --> 00:24:15,230
so we have this system which we are we

00:24:13,580 --> 00:24:16,850
are monitoring we are storing our

00:24:15,230 --> 00:24:19,220
metrics into database and we are

00:24:16,850 --> 00:24:21,620
visualizing metrics using your fauna but

00:24:19,220 --> 00:24:24,740
we also we are also pushing metrics to

00:24:21,620 --> 00:24:26,960
spark and sharing data models crunching

00:24:24,740 --> 00:24:31,280
those metrics and checking out when

00:24:26,960 --> 00:24:35,120
certain values are outside of boundaries

00:24:31,280 --> 00:24:38,480
after that happens we want email to be

00:24:35,120 --> 00:24:40,520
fired to maintenance guys providing full

00:24:38,480 --> 00:24:43,790
I've shot of the systems this email

00:24:40,520 --> 00:24:46,640
looks like this it has all the important

00:24:43,790 --> 00:24:51,370
numbers for someone to reason about it

00:24:46,640 --> 00:24:54,140
says in rip colored in red the stat that

00:24:51,370 --> 00:24:57,679
triggered the trigger alert in this case

00:24:54,140 --> 00:25:01,370
disk evade disk IO was the main cause of

00:24:57,679 --> 00:25:05,720
this alert and we wanted the machine to

00:25:01,370 --> 00:25:07,429
have died and diagnostics message so in

00:25:05,720 --> 00:25:09,860
the last sentence it is said that disk

00:25:07,429 --> 00:25:12,260
IO was higher than usual during that

00:25:09,860 --> 00:25:16,160
time and there is a link to brief on a

00:25:12,260 --> 00:25:20,059
dashboard with specific disk statistics

00:25:16,160 --> 00:25:21,820
so it is good for someone who don't want

00:25:20,059 --> 00:25:25,250
to browse through all the graphs and

00:25:21,820 --> 00:25:29,270
stuff this this is a summary and it

00:25:25,250 --> 00:25:32,240
provides one click to dashboard which

00:25:29,270 --> 00:25:35,630
which probably has the source of the

00:25:32,240 --> 00:25:39,169
problem this is still working in

00:25:35,630 --> 00:25:41,690
progress we are planning to this is

00:25:39,169 --> 00:25:43,490
alert so some something already happened

00:25:41,690 --> 00:25:45,980
what we want to do is to push this

00:25:43,490 --> 00:25:46,700
matrix through some neural network or

00:25:45,980 --> 00:25:49,970
something like that

00:25:46,700 --> 00:25:52,600
and try to predict based on trends when

00:25:49,970 --> 00:25:55,520
certain to do predictive maintenance

00:25:52,600 --> 00:25:57,919
well through certain trends to figure

00:25:55,520 --> 00:25:59,750
out when problem will happend and then

00:25:57,919 --> 00:26:03,220
there is time to react

00:25:59,750 --> 00:26:08,720
then then you can do something about it

00:26:03,220 --> 00:26:11,210
ok and to conclude I want to emphasize a

00:26:08,720 --> 00:26:15,130
couple of important things that I

00:26:11,210 --> 00:26:19,130
touched upon this presentation I

00:26:15,130 --> 00:26:21,710
emphasize this in matrix stack in log

00:26:19,130 --> 00:26:23,630
stack and in alerting stack you need to

00:26:21,710 --> 00:26:26,840
have right amount of information you

00:26:23,630 --> 00:26:28,940
know it's scary to have too much

00:26:26,840 --> 00:26:31,850
information because this will be noise

00:26:28,940 --> 00:26:33,590
and you will ignore things it's scary to

00:26:31,850 --> 00:26:37,690
have too few information because you

00:26:33,590 --> 00:26:40,760
could you will not be able to explain

00:26:37,690 --> 00:26:43,280
problematic situation you need to have a

00:26:40,760 --> 00:26:45,140
good selection of metrics on your

00:26:43,280 --> 00:26:47,750
dashboards you need to have good

00:26:45,140 --> 00:26:49,700
selection of logs and this is iterative

00:26:47,750 --> 00:26:50,989
process I mentioned that we in company

00:26:49,700 --> 00:26:54,389
have

00:26:50,989 --> 00:26:57,629
matrix monitoring retrospective meetings

00:26:54,389 --> 00:27:00,690
we sit down all of us in team and they

00:26:57,629 --> 00:27:04,169
speak which - do you look which locks

00:27:00,690 --> 00:27:06,989
are polluting your locks and which are

00:27:04,169 --> 00:27:09,600
providing informational important

00:27:06,989 --> 00:27:11,639
information is there something that is

00:27:09,600 --> 00:27:16,440
missing that you would like to add and

00:27:11,639 --> 00:27:18,600
we are constantly changing monitoring

00:27:16,440 --> 00:27:20,669
stack I think it's important to have

00:27:18,600 --> 00:27:24,629
this up-to-date because it's your main

00:27:20,669 --> 00:27:26,460
source of truth when failure happens do

00:27:24,629 --> 00:27:29,129
not end up fixing monitoring machine

00:27:26,460 --> 00:27:32,269
instead of fixing application business

00:27:29,129 --> 00:27:36,659
logic this is also important I spoke

00:27:32,269 --> 00:27:38,970
today a lot about open source stack that

00:27:36,659 --> 00:27:41,580
we use self managed stack but this is

00:27:38,970 --> 00:27:45,509
the and you can think that it is silver

00:27:41,580 --> 00:27:47,359
bullet but we ended up working on

00:27:45,509 --> 00:27:49,710
monitoring stack more than on our

00:27:47,359 --> 00:27:52,679
application stack its distributed

00:27:49,710 --> 00:27:55,019
systems system monitoring stack is one

00:27:52,679 --> 00:27:57,269
more system under your hand and your IT

00:27:55,019 --> 00:28:03,570
department will have to scale this

00:27:57,269 --> 00:28:05,539
system to fix stuff - even worse we had

00:28:03,570 --> 00:28:08,609
the situation that failure on our

00:28:05,539 --> 00:28:11,429
monitoring stack propagated to failure

00:28:08,609 --> 00:28:14,190
on our instances because logs were

00:28:11,429 --> 00:28:16,739
piling up and we took all of our disk

00:28:14,190 --> 00:28:19,350
space luckily it was in staging

00:28:16,739 --> 00:28:22,019
environment but these things can happen

00:28:19,350 --> 00:28:24,299
when while on software is a service

00:28:22,019 --> 00:28:28,529
solution somebody else takes care of

00:28:24,299 --> 00:28:30,239
that be proactive not reactive this is

00:28:28,529 --> 00:28:32,940
something that we are trying to do with

00:28:30,239 --> 00:28:36,749
central so we are trying to push our

00:28:32,940 --> 00:28:38,159
metrics to mail to neural network and do

00:28:36,749 --> 00:28:42,330
predictive and predictive maintenance

00:28:38,159 --> 00:28:43,950
and telematics by our needs every use

00:28:42,330 --> 00:28:47,789
case is different if you have specific

00:28:43,950 --> 00:28:49,950
use cases we did the build stuff we

00:28:47,789 --> 00:28:52,649
build the diagnostic solution for

00:28:49,950 --> 00:28:56,070
Cassandra because solutions out there

00:28:52,649 --> 00:28:58,470
couldn't provide the latencies for all

00:28:56,070 --> 00:29:01,259
the queries usually they were working

00:28:58,470 --> 00:29:03,720
through percentiles and highest

00:29:01,259 --> 00:29:05,880
percentile is 99.9

00:29:03,720 --> 00:29:09,270
sent requests and we needed two more

00:29:05,880 --> 00:29:12,299
nines so we needed to look at really all

00:29:09,270 --> 00:29:15,090
all queries and we decided to go and

00:29:12,299 --> 00:29:20,549
build build build a tool which will aid

00:29:15,090 --> 00:29:23,789
us in the in this links I blogged about

00:29:20,549 --> 00:29:26,130
this topic a lot so I covered the three

00:29:23,789 --> 00:29:28,679
parts of this presentation in more

00:29:26,130 --> 00:29:29,340
details if you if you want you can check

00:29:28,679 --> 00:29:33,120
it out

00:29:29,340 --> 00:29:35,789
explain this monitoring stack which we

00:29:33,120 --> 00:29:39,210
are using I explained distributed

00:29:35,789 --> 00:29:42,570
logging and matrix stream here is my

00:29:39,210 --> 00:29:44,010
Twitter channel maybe I could say the

00:29:42,570 --> 00:29:47,270
head this emphasize this on the

00:29:44,010 --> 00:29:50,100
beginning of presentation I posted the

00:29:47,270 --> 00:29:53,580
this presentational SlideShare so I

00:29:50,100 --> 00:29:56,580
could save you a couple of pictures you

00:29:53,580 --> 00:29:59,070
can check me on Twitter and you can get

00:29:56,580 --> 00:30:02,460
this presentation and last link is our

00:29:59,070 --> 00:30:04,860
github project where we automated this

00:30:02,460 --> 00:30:08,400
monitoring machine there is ansible

00:30:04,860 --> 00:30:11,070
project which and you can spin up this

00:30:08,400 --> 00:30:15,299
monitoring machine and play around with

00:30:11,070 --> 00:30:18,600
it it has Riemann influx graph on i lk

00:30:15,299 --> 00:30:21,419
on it and you can check it out and you

00:30:18,600 --> 00:30:25,740
can play locally with it and check out

00:30:21,419 --> 00:30:30,000
how this how everything works and that's

00:30:25,740 --> 00:30:32,250
basically the I ran through it a bit

00:30:30,000 --> 00:30:35,520
faster than I expected but it's a

00:30:32,250 --> 00:30:39,169
lunchtime so yes it's a good thing I'm

00:30:35,520 --> 00:30:39,169
open for any questions

00:30:45,000 --> 00:30:54,280
yeah questions I want to give you the

00:30:48,460 --> 00:30:55,540
microbe thanks for the talk what I'm

00:30:54,280 --> 00:30:58,450
about to ask you is more of a thought

00:30:55,540 --> 00:30:59,950
experiment than anything else so some of

00:30:58,450 --> 00:31:02,440
the logging that I concern myself with

00:30:59,950 --> 00:31:04,840
have to do with machine learning stuff

00:31:02,440 --> 00:31:06,190
so recommend recommendation engines and

00:31:04,840 --> 00:31:07,450
you name it and some of the stuff you

00:31:06,190 --> 00:31:08,770
can measure like the click-through rate

00:31:07,450 --> 00:31:10,270
how well are we doing that's something

00:31:08,770 --> 00:31:12,160
you can plot over time and that works

00:31:10,270 --> 00:31:13,900
there's also other internal stuff

00:31:12,160 --> 00:31:15,880
numerix are things converging kind of

00:31:13,900 --> 00:31:17,440
nicely and you name it and these plus at

00:31:15,880 --> 00:31:18,550
some point become kind of complicated to

00:31:17,440 --> 00:31:21,040
the extent that you need more than just

00:31:18,550 --> 00:31:22,690
a simple line chart I've been playing in

00:31:21,040 --> 00:31:24,550
my head with a thought of you know if

00:31:22,690 --> 00:31:26,710
all the logs goes to some nice central

00:31:24,550 --> 00:31:28,270
place I could have an eyepin notebook

00:31:26,710 --> 00:31:30,100
like one of those Jupiter notebooks with

00:31:28,270 --> 00:31:31,390
the plotting and stuff and put that in

00:31:30,100 --> 00:31:32,620
the chrome thing that would just run

00:31:31,390 --> 00:31:34,990
every day where I get all the other

00:31:32,620 --> 00:31:36,520
stuff I might be interested in and is

00:31:34,990 --> 00:31:38,260
this something you guess could be

00:31:36,520 --> 00:31:40,650
something you could consider because it

00:31:38,260 --> 00:31:42,460
it feels like it is a little bit hacky

00:31:40,650 --> 00:31:44,500
just wondering what your opinion on

00:31:42,460 --> 00:31:46,990
something like that might be if you're

00:31:44,500 --> 00:31:49,570
doing the same thing with metrics we

00:31:46,990 --> 00:31:51,670
currently concentrated on metrics and

00:31:49,570 --> 00:31:54,160
build the spark stream where we are

00:31:51,670 --> 00:31:56,320
doing machine learning on metrics so but

00:31:54,160 --> 00:31:58,060
you have this math you will have logs in

00:31:56,320 --> 00:32:02,770
elasticsearch so I guess you can use

00:31:58,060 --> 00:32:06,840
elasticsearch api's and start doing some

00:32:02,770 --> 00:32:11,740
clever stuff from logs and return either

00:32:06,840 --> 00:32:16,810
metrics to influx VB roll-ups or I don't

00:32:11,740 --> 00:32:18,970
know how many times certain X appears or

00:32:16,810 --> 00:32:22,120
something like that so make sense I

00:32:18,970 --> 00:32:24,130
don't know if elasticsearch is not the

00:32:22,120 --> 00:32:26,010
best tool for the job locks can be

00:32:24,130 --> 00:32:28,000
stored in some other place but

00:32:26,010 --> 00:32:30,520
elasticsearch is good because it

00:32:28,000 --> 00:32:32,260
provides full-text search so you can

00:32:30,520 --> 00:32:35,110
work with these strings also you can

00:32:32,260 --> 00:32:37,180
push logs in parallel to elastic search

00:32:35,110 --> 00:32:41,290
and to something else which you are

00:32:37,180 --> 00:32:44,640
using for machine learning so it makes

00:32:41,290 --> 00:32:47,790
sense it makes perfect sense

00:32:44,640 --> 00:32:52,270
next question was here somewhere

00:32:47,790 --> 00:32:54,730
hi do you have any insights on file bit

00:32:52,270 --> 00:32:56,519
semantics or guarantees I mean you don't

00:32:54,730 --> 00:32:59,969
want duplicate log and

00:32:56,519 --> 00:33:03,389
in case of network failure or if five

00:32:59,969 --> 00:33:09,629
classes we do miss log entries from your

00:33:03,389 --> 00:33:11,999
log yeah that's that's a good question

00:33:09,629 --> 00:33:16,039
running to monitoring your monitoring

00:33:11,999 --> 00:33:20,099
stack so file bit you can configure

00:33:16,039 --> 00:33:23,209
amount things with file bit and it has

00:33:20,099 --> 00:33:26,609
back pressure it says it is saving

00:33:23,209 --> 00:33:28,529
events if your elasticsearch for example

00:33:26,609 --> 00:33:31,799
log stash is not responsive it will

00:33:28,529 --> 00:33:34,709
start start collecting events and just

00:33:31,799 --> 00:33:39,809
push them when there is a network

00:33:34,709 --> 00:33:42,239
connection in between but yet this is

00:33:39,809 --> 00:33:44,549
the important thing I guess you need to

00:33:42,239 --> 00:33:46,649
monitor your monitoring stack you it

00:33:44,549 --> 00:33:49,739
would be wise to have certain insights

00:33:46,649 --> 00:33:51,929
even on file bit on each machine to to

00:33:49,739 --> 00:33:54,149
figure out we managed to fill up space

00:33:51,929 --> 00:33:56,729
with file bit events while elasticsearch

00:33:54,149 --> 00:33:58,979
was not responsive this is the situation

00:33:56,729 --> 00:34:01,889
which I mentioned on staging environment

00:33:58,979 --> 00:34:05,159
so file bit logs which were waiting for

00:34:01,889 --> 00:34:08,700
network connection started piling up and

00:34:05,159 --> 00:34:11,730
this caused a failure on instance where

00:34:08,700 --> 00:34:14,700
your databases which is scary which is

00:34:11,730 --> 00:34:18,720
really scary so it makes Rieman for

00:34:14,700 --> 00:34:21,839
example comes with its own monitor is in

00:34:18,720 --> 00:34:24,419
its all its own metrics and you can

00:34:21,839 --> 00:34:26,490
visualize metrics about Rieman client

00:34:24,419 --> 00:34:28,950
and server so you always know how your

00:34:26,490 --> 00:34:30,809
remand server performs which is part of

00:34:28,950 --> 00:34:35,299
your monitoring stack but it is also

00:34:30,809 --> 00:34:35,299
emitting metrics about its performance

00:34:36,409 --> 00:34:42,599
any questions left for minute then

00:34:40,460 --> 00:34:45,170
enough thank you very much for this

00:34:42,599 --> 00:34:50,769
interesting talk thank you

00:34:45,170 --> 00:34:50,769

YouTube URL: https://www.youtube.com/watch?v=a_E4irrqSMY


