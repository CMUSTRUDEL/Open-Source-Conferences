Title: Berlin Buzzwords 2017: Ted Dunning - Update on the t-digest: Finding Faults in Real Data #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Is your system working? Really? Average response times and throughputs donâ€™t tell the whole story. To really understand what is happening, you probably need measurements like the 99.9%-ile response time. A growing number of systems are using the t-digest to do this. 

I will explain the algorithm with practical examples, talk about how it is much simpler and faster than before, talk about integration in systems like Elastic, Solar and streamlib, tell some real-world deployment stories and show some pretty pictures.

Read more:
https://2017.berlinbuzzwords.de/17/session/update-t-digest-finding-faults-real-data

About Ted Dunning:
https://2017.berlinbuzzwords.de/users/ted-dunning

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,400 --> 00:00:10,799
I don't suppose I wind up famous for it

00:00:09,540 --> 00:00:17,880
but I guess this is the modern way of

00:00:10,799 --> 00:00:20,849
saying HP an avid Berliner right so what

00:00:17,880 --> 00:00:28,800
I want to talk about today is tea digest

00:00:20,849 --> 00:00:32,009
but I should hmm there it is okay so I'm

00:00:28,800 --> 00:00:35,219
Ted dying I work for map are I also do a

00:00:32,009 --> 00:00:37,110
lot of work with Apache lately on the

00:00:35,219 --> 00:00:39,859
board of directors previously incubator

00:00:37,110 --> 00:00:43,109
previously mentor to lots of projects

00:00:39,859 --> 00:00:44,730
and you'll see many of the projects that

00:00:43,109 --> 00:00:47,850
I was involved with actually here

00:00:44,730 --> 00:00:50,640
because this community is a big part of

00:00:47,850 --> 00:00:52,800
the stuff that I've been working on for

00:00:50,640 --> 00:00:55,589
quite some time

00:00:52,800 --> 00:00:57,569
vampire makes a data platform and Apache

00:00:55,589 --> 00:00:59,699
I assume that everybody here knows this

00:00:57,569 --> 00:01:02,010
is a slide I lifted from a more

00:00:59,699 --> 00:01:05,700
commercial audience who didn't really

00:01:02,010 --> 00:01:08,580
know what Apache was today I want to

00:01:05,700 --> 00:01:10,560
talk about tea digests but I want to

00:01:08,580 --> 00:01:14,490
talk more generally about why we should

00:01:10,560 --> 00:01:17,520
measure distributions not just single

00:01:14,490 --> 00:01:20,010
data points why we should be able to

00:01:17,520 --> 00:01:22,380
combine them and visualize them I'm

00:01:20,010 --> 00:01:26,280
going to talk about some of the basic

00:01:22,380 --> 00:01:28,470
ideas for how to digest is built and how

00:01:26,280 --> 00:01:31,020
some alternative algorithms work as well

00:01:28,470 --> 00:01:32,970
I'm going to talk about the recent

00:01:31,020 --> 00:01:35,310
results for the the upcoming release of

00:01:32,970 --> 00:01:38,910
tea digests and then show some practical

00:01:35,310 --> 00:01:41,940
applications so this starts of course

00:01:38,910 --> 00:01:43,920
like all good software with a CO an the

00:01:41,940 --> 00:01:46,710
student comes to the master you can

00:01:43,920 --> 00:01:49,370
pronounce student as user and you can

00:01:46,710 --> 00:01:51,390
pronounce you know a master as

00:01:49,370 --> 00:01:54,810
administrator or something like that or

00:01:51,390 --> 00:01:56,630
or ops person and the the student says

00:01:54,810 --> 00:01:59,640
it's broken

00:01:56,630 --> 00:02:03,270
this is a time-honored statement of a

00:01:59,640 --> 00:02:05,420
student or a user it's broken and of

00:02:03,270 --> 00:02:09,440
course the response to that is always

00:02:05,420 --> 00:02:12,330
what did you do I mean what what changed

00:02:09,440 --> 00:02:15,329
of course you've blamed the user that's

00:02:12,330 --> 00:02:18,480
a tradition as well and what has changed

00:02:15,329 --> 00:02:21,030
is this key really important question

00:02:18,480 --> 00:02:22,490
did you make a change did somebody else

00:02:21,030 --> 00:02:25,910
make a change

00:02:22,490 --> 00:02:28,460
the universe changed somehow that's the

00:02:25,910 --> 00:02:32,390
key to finding what went wrong and of

00:02:28,460 --> 00:02:36,110
course in our wildest dreams by saying

00:02:32,390 --> 00:02:38,090
what has changed we imagine the user is

00:02:36,110 --> 00:02:39,170
enlightened and they go off and figure

00:02:38,090 --> 00:02:42,200
out what the problem is

00:02:39,170 --> 00:02:46,300
doesn't usually work that way but it's

00:02:42,200 --> 00:02:52,130
just a story so finding that change is

00:02:46,300 --> 00:02:54,740
absolutely the key but it's hard to find

00:02:52,130 --> 00:02:59,090
that change and more and more we work in

00:02:54,740 --> 00:03:01,610
things that are real-time ish and so the

00:02:59,090 --> 00:03:06,530
things that we need to find the change

00:03:01,610 --> 00:03:09,080
in our values that change quickly and so

00:03:06,530 --> 00:03:11,060
it isn't a single value that we have to

00:03:09,080 --> 00:03:13,940
see we have to see somehow a

00:03:11,060 --> 00:03:16,280
distribution of values here's some data

00:03:13,940 --> 00:03:19,970
the stuff on the right these are the

00:03:16,280 --> 00:03:20,780
ping times back to Google from my hotel

00:03:19,970 --> 00:03:23,270
room last night

00:03:20,780 --> 00:03:27,050
I often wind up doing this trying threw

00:03:23,270 --> 00:03:29,030
out what just happened but look at it

00:03:27,050 --> 00:03:31,130
it's like 200 milliseconds or so

00:03:29,030 --> 00:03:33,670
sometimes a bit faster sometimes a

00:03:31,130 --> 00:03:36,950
little bit slower looks pretty decent

00:03:33,670 --> 00:03:37,880
yeah that's okay so it should be just

00:03:36,950 --> 00:03:41,270
fine

00:03:37,880 --> 00:03:44,660
now if we look at the mean like data

00:03:41,270 --> 00:03:45,770
science that we are it's about 200

00:03:44,660 --> 00:03:48,110
milliseconds look at the standard

00:03:45,770 --> 00:03:49,940
deviation that's it so that's the end of

00:03:48,110 --> 00:03:52,010
the story right that's that's a lot the

00:03:49,940 --> 00:03:55,270
way an awful lot of people think about

00:03:52,010 --> 00:04:01,240
it but if we actually plotted the ping x

00:03:55,270 --> 00:04:04,880
over many seconds it's not that simple

00:04:01,240 --> 00:04:06,260
real life isn't that simple there's

00:04:04,880 --> 00:04:09,230
there something that goes on

00:04:06,260 --> 00:04:12,290
occasionally in these spurts when it

00:04:09,230 --> 00:04:17,470
goes from 200 milliseconds to 1,200 or

00:04:12,290 --> 00:04:20,420
longer so a single value a single mean

00:04:17,470 --> 00:04:22,790
mean with a de standard deviation it

00:04:20,420 --> 00:04:24,920
doesn't tell us what's going on it

00:04:22,790 --> 00:04:27,710
doesn't tell us the true picture of the

00:04:24,920 --> 00:04:31,640
data and we can't have all of the data

00:04:27,710 --> 00:04:33,919
this is only a thousand samples in real

00:04:31,640 --> 00:04:35,660
life of course we don't have a thousand

00:04:33,919 --> 00:04:37,670
samples we have a billion

00:04:35,660 --> 00:04:40,100
and we have a billion from many

00:04:37,670 --> 00:04:42,490
different sources and so on and what's

00:04:40,100 --> 00:04:46,670
worse is that this kind of data is

00:04:42,490 --> 00:04:48,110
long-tailed it has this big peak that

00:04:46,670 --> 00:04:52,760
fools the mean and standard deviation

00:04:48,110 --> 00:04:56,330
then it has this very long tail of weird

00:04:52,760 --> 00:04:59,390
 and we need to understand that you

00:04:56,330 --> 00:05:01,280
have to know the distribution of it now

00:04:59,390 --> 00:05:03,290
knowing the distribution here's the

00:05:01,280 --> 00:05:05,960
distribution now that isn't a simple

00:05:03,290 --> 00:05:09,770
either because you can't really see all

00:05:05,960 --> 00:05:13,580
the data out here and you don't have

00:05:09,770 --> 00:05:16,490
really very good accuracy back here so

00:05:13,580 --> 00:05:21,050
we can see already that a single number

00:05:16,490 --> 00:05:24,440
is absolutely not enough to understand

00:05:21,050 --> 00:05:27,800
the quality of what we're doing even

00:05:24,440 --> 00:05:31,100
just with ping x from a hotel room and

00:05:27,800 --> 00:05:35,270
of course we want to deal with much more

00:05:31,100 --> 00:05:37,190
complex situations so as I see it what

00:05:35,270 --> 00:05:39,550
we need what we really really really

00:05:37,190 --> 00:05:43,280
need in time series databases and

00:05:39,550 --> 00:05:46,460
monitoring systems and just OLAP

00:05:43,280 --> 00:05:49,370
databases in general is I want to be

00:05:46,460 --> 00:05:53,260
able to compute the distribution of some

00:05:49,370 --> 00:05:55,970
value over any time period for any

00:05:53,260 --> 00:05:58,210
restriction that I might want I'd like

00:05:55,970 --> 00:06:01,970
to be able to take elasticsearch in and

00:05:58,210 --> 00:06:04,070
find out the distribution of some value

00:06:01,970 --> 00:06:07,070
for any particular subset of

00:06:04,070 --> 00:06:08,870
measurements that I'd like and I don't

00:06:07,070 --> 00:06:11,990
want it to take up the same amount of

00:06:08,870 --> 00:06:14,900
space as the original data I wanted to

00:06:11,990 --> 00:06:17,300
take up a tiny tiny fraction of that and

00:06:14,900 --> 00:06:20,030
I don't want it to take as much time as

00:06:17,300 --> 00:06:22,730
it would take to grovel through all the

00:06:20,030 --> 00:06:26,030
original data I want to take a tiny tiny

00:06:22,730 --> 00:06:29,090
fraction of that time I want to do any

00:06:26,030 --> 00:06:33,260
query kind of along this idea give me

00:06:29,090 --> 00:06:37,280
the distribution from this data source

00:06:33,260 --> 00:06:41,570
where I have some filter give me all the

00:06:37,280 --> 00:06:44,080
latencies that Tom had yesterday with

00:06:41,570 --> 00:06:46,689
his Frodo application

00:06:44,080 --> 00:06:50,860
well what about last month

00:06:46,689 --> 00:06:55,539
that sort of thing so that's the goal so

00:06:50,860 --> 00:06:59,219
idea number one would be to do this kind

00:06:55,539 --> 00:07:05,139
of histogram these are fixed width bins

00:06:59,219 --> 00:07:07,299
that are essentially predefined so let's

00:07:05,139 --> 00:07:09,459
assume we have the bins we can now just

00:07:07,299 --> 00:07:11,439
decide which bin it's in and then we can

00:07:09,459 --> 00:07:13,449
count whenever a sample comes in we

00:07:11,439 --> 00:07:16,089
decide which bin it goes to increment

00:07:13,449 --> 00:07:20,139
account and we just have this big long

00:07:16,089 --> 00:07:23,289
thing of integers it's useful I produced

00:07:20,139 --> 00:07:26,589
a graph things like that but it's just

00:07:23,289 --> 00:07:30,219
not very general because we have to pre

00:07:26,589 --> 00:07:31,539
define the bins which bins whose data

00:07:30,219 --> 00:07:35,159
what range

00:07:31,539 --> 00:07:39,039
what accuracy and furthermore we have

00:07:35,159 --> 00:07:41,110
very difficult dynamic range if we if we

00:07:39,039 --> 00:07:44,469
just talk about Layton sees we might

00:07:41,110 --> 00:07:48,489
talk about from 0.1 millisecond 0.1

00:07:44,469 --> 00:07:50,469
milliseconds to 10 seconds if we want to

00:07:48,489 --> 00:07:52,929
have bins to handle all that we've got

00:07:50,469 --> 00:07:55,239
very very very large number of bins and

00:07:52,929 --> 00:07:57,369
that's almost comparable to the amount

00:07:55,239 --> 00:07:59,139
of data we had it's not good enough it's

00:07:57,369 --> 00:07:59,919
not what we want so let's try another

00:07:59,139 --> 00:08:02,769
example

00:07:59,919 --> 00:08:05,589
these are exponentially distributed bins

00:08:02,769 --> 00:08:09,519
so certainly with certain kinds of data

00:08:05,589 --> 00:08:12,519
say latent fees the data we just saw the

00:08:09,519 --> 00:08:17,349
accuracy that we want is relative to the

00:08:12,519 --> 00:08:20,259
measurement itself and that implies that

00:08:17,349 --> 00:08:23,279
we could have bin widths that are

00:08:20,259 --> 00:08:26,529
relative to the size of the measurement

00:08:23,279 --> 00:08:30,839
one point one millisecond isn't that big

00:08:26,529 --> 00:08:33,729
a difference from one millisecond and

00:08:30,839 --> 00:08:35,589
1,100 milliseconds isn't that different

00:08:33,729 --> 00:08:38,769
from a thousand milliseconds it's the

00:08:35,589 --> 00:08:41,079
same relative accuracy and so if that's

00:08:38,769 --> 00:08:44,350
the kind of accuracy we want we could

00:08:41,079 --> 00:08:49,059
make a bin which is roughly say ten

00:08:44,350 --> 00:08:52,990
percent increase in size and now we can

00:08:49,059 --> 00:08:55,569
cheat a little bit it's not exactly that

00:08:52,990 --> 00:08:58,209
perfect ten percent in this cheating but

00:08:55,569 --> 00:08:59,889
it's very very close and the trick is

00:08:58,209 --> 00:09:02,079
that we can use peas

00:08:59,889 --> 00:09:04,059
is just a few bits out of the

00:09:02,079 --> 00:09:06,009
floating-point number that represents

00:09:04,059 --> 00:09:08,290
the measurement itself and that will

00:09:06,009 --> 00:09:12,040
give us the bin number so it's almost as

00:09:08,290 --> 00:09:14,019
fast as the first example just take the

00:09:12,040 --> 00:09:16,389
range divide the measurement by the

00:09:14,019 --> 00:09:18,100
minimum take those bits out of the

00:09:16,389 --> 00:09:22,389
floating-point number that's our bin and

00:09:18,100 --> 00:09:23,679
we count typically we have eight bins

00:09:22,389 --> 00:09:25,899
per octave

00:09:23,679 --> 00:09:28,569
every time the measurement doubles we

00:09:25,899 --> 00:09:30,519
have eight little bins it doubles eight

00:09:28,569 --> 00:09:33,279
more bins but the bins are twice as

00:09:30,519 --> 00:09:35,139
large and it doubles again and we have

00:09:33,279 --> 00:09:37,029
eight more bins but the bins are now

00:09:35,139 --> 00:09:42,089
four times as large as the ones over

00:09:37,029 --> 00:09:46,499
here simple it's fast it's OK for

00:09:42,089 --> 00:09:50,199
positive data it's OK for data that has

00:09:46,499 --> 00:09:54,100
resolution that's small very tight near

00:09:50,199 --> 00:09:57,069
zero and large far from zero but that's

00:09:54,100 --> 00:09:59,589
not general it's really not general it

00:09:57,069 --> 00:10:02,019
is a nice property have good relative

00:09:59,589 --> 00:10:04,809
error in the measurement space but it

00:10:02,019 --> 00:10:08,949
isn't a general property that applies to

00:10:04,809 --> 00:10:14,470
all of these things here's a picture of

00:10:08,949 --> 00:10:18,069
it and so here is simulated long tailed

00:10:14,470 --> 00:10:21,040
data and you can see the accuracy the

00:10:18,069 --> 00:10:23,230
bin width here is half the measurement

00:10:21,040 --> 00:10:25,899
but out here it's a tiny fraction of the

00:10:23,230 --> 00:10:28,209
measurement and here it is with a float

00:10:25,899 --> 00:10:29,980
histogram you can see that we have big

00:10:28,209 --> 00:10:32,559
guns out here and the bins get bigger

00:10:29,980 --> 00:10:34,989
and bigger and bigger until they're kind

00:10:32,559 --> 00:10:36,489
of the size of the number there and you

00:10:34,989 --> 00:10:38,649
can see they get smaller and smaller and

00:10:36,489 --> 00:10:42,910
smaller so that the relative accuracy

00:10:38,649 --> 00:10:45,429
stays about 10% plus or minus 5% all the

00:10:42,910 --> 00:10:47,199
way down so these are better

00:10:45,429 --> 00:10:49,389
these are definitely better than the

00:10:47,199 --> 00:10:53,309
fixed size bins for a lot of

00:10:49,389 --> 00:10:56,230
applications but they're not everything

00:10:53,309 --> 00:11:00,369
so let's talk about how we could have

00:10:56,230 --> 00:11:03,489
completely adaptive bin sizes that adapt

00:11:00,369 --> 00:11:05,589
to the data that we have and let us do

00:11:03,489 --> 00:11:07,539
all of those OLAP type of queries that

00:11:05,589 --> 00:11:10,389
we're talking about and the first

00:11:07,539 --> 00:11:13,250
intuition is that we may not really want

00:11:10,389 --> 00:11:16,129
accuracy relative to the measurement

00:11:13,250 --> 00:11:19,160
we may want to have an accuracy relative

00:11:16,129 --> 00:11:21,230
to the distribution and so if you think

00:11:19,160 --> 00:11:24,800
about it if you want the median you

00:11:21,230 --> 00:11:29,180
might want like 50th percentile plus or

00:11:24,800 --> 00:11:30,889
minus 0.1 percentile forty nine point

00:11:29,180 --> 00:11:32,209
nine two fifty point one but if you

00:11:30,889 --> 00:11:35,350
wanted ninety-nine point nine nine

00:11:32,209 --> 00:11:37,819
percent I'll that point one percent in

00:11:35,350 --> 00:11:38,870
quantile space is completely

00:11:37,819 --> 00:11:40,790
unacceptable

00:11:38,870 --> 00:11:44,959
it makes the ninety-nine point nine nine

00:11:40,790 --> 00:11:47,170
percent I'll completely meaningless so

00:11:44,959 --> 00:11:51,290
let's put the ACT you see in terms of

00:11:47,170 --> 00:11:53,839
percentile and in particular let's make

00:11:51,290 --> 00:11:57,740
the accuracy very very fine on the edges

00:11:53,839 --> 00:12:00,439
and much more coarse in the middle so we

00:11:57,740 --> 00:12:02,870
need to have bins with very small counts

00:12:00,439 --> 00:12:05,240
in the edge and large amount of counts

00:12:02,870 --> 00:12:07,339
in the middle and that has to be

00:12:05,240 --> 00:12:10,990
adaptive to the data here's a picture of

00:12:07,339 --> 00:12:15,079
why that is this is a synthetic

00:12:10,990 --> 00:12:18,199
distribution it's it's a skewed

00:12:15,079 --> 00:12:21,230
distribution and this is the first one

00:12:18,199 --> 00:12:25,009
percent of the data on the Left we've

00:12:21,230 --> 00:12:28,759
taken equal sized bins in terms of

00:12:25,009 --> 00:12:31,040
samples and we take a hundred bins each

00:12:28,759 --> 00:12:33,199
one has a hundred samples and you could

00:12:31,040 --> 00:12:36,889
imagine they're trying to estimate the

00:12:33,199 --> 00:12:40,490
quantile in this range is quite

00:12:36,889 --> 00:12:43,100
difficult from that one bin from a bin

00:12:40,490 --> 00:12:44,779
alone all we can do is in linear

00:12:43,100 --> 00:12:49,279
interpolation so we're going to have

00:12:44,779 --> 00:12:51,740
really crummy results here but if we

00:12:49,279 --> 00:12:54,110
were to have adaptive size bins say with

00:12:51,740 --> 00:12:57,589
two samples here eight samples there

00:12:54,110 --> 00:13:00,290
nineteen samples there we can follow the

00:12:57,589 --> 00:13:07,399
curve much more precisely and get very

00:13:00,290 --> 00:13:09,709
very much more accuracy and the variable

00:13:07,399 --> 00:13:12,230
size bins even though they have much

00:13:09,709 --> 00:13:14,870
more accuracy there's only a few more

00:13:12,230 --> 00:13:17,410
bins that are required so we kind of get

00:13:14,870 --> 00:13:20,689
something for nothing practically it

00:13:17,410 --> 00:13:22,579
turns out the original t digest

00:13:20,689 --> 00:13:25,550
algorithms this is the beginning of the

00:13:22,579 --> 00:13:28,160
actual update the original t digest

00:13:25,550 --> 00:13:31,759
algorithm was kind of complex and the

00:13:28,160 --> 00:13:34,100
idea was that we had bins and cinders of

00:13:31,759 --> 00:13:36,639
bins that we remembered and for each new

00:13:34,100 --> 00:13:39,559
sample we would find the nearest bin and

00:13:36,639 --> 00:13:41,089
we would add it to that bin or not

00:13:39,559 --> 00:13:43,610
depending on whether or not the bin

00:13:41,089 --> 00:13:46,839
matched the size constraints that we

00:13:43,610 --> 00:13:49,759
have to implement that required trees

00:13:46,839 --> 00:13:52,670
fact adriennes right there I think he's

00:13:49,759 --> 00:13:55,130
the author of the the well current

00:13:52,670 --> 00:13:58,009
champion algorithm which is a balance

00:13:55,130 --> 00:14:00,019
tree and such but it turns out that

00:13:58,009 --> 00:14:02,179
there's a much simpler way to phrase

00:14:00,019 --> 00:14:04,490
these algorithms and that's one of the

00:14:02,179 --> 00:14:07,009
big updates you can take a bunch of data

00:14:04,490 --> 00:14:10,160
you can sort it now if you just walk

00:14:07,009 --> 00:14:13,790
through that sorted data I can group it

00:14:10,160 --> 00:14:17,870
into bins of the right size really very

00:14:13,790 --> 00:14:20,929
easily and I can then remember the

00:14:17,870 --> 00:14:22,819
centroid and the amount of data in each

00:14:20,929 --> 00:14:23,470
one of those bins and that's a tea

00:14:22,819 --> 00:14:27,110
digest

00:14:23,470 --> 00:14:31,579
now we don't can't do this for all the

00:14:27,110 --> 00:14:33,589
data and so to update what we do is we

00:14:31,579 --> 00:14:36,399
start with the tea digest from the

00:14:33,589 --> 00:14:40,459
previous slide and a bunch of new data

00:14:36,399 --> 00:14:43,490
we can sort those together and do the

00:14:40,459 --> 00:14:45,170
same clumping operation now some of the

00:14:43,490 --> 00:14:47,329
previous centroids are going to be big

00:14:45,170 --> 00:14:49,579
enough that we can't add data to them

00:14:47,329 --> 00:14:52,670
without making them too big fine we let

00:14:49,579 --> 00:14:54,559
them stand sometimes the actual data

00:14:52,670 --> 00:14:56,689
points themselves will need to clump

00:14:54,559 --> 00:14:59,209
together to make things that are big and

00:14:56,689 --> 00:15:01,639
as big as they can be without being too

00:14:59,209 --> 00:15:04,399
big that's fine - we clump those and

00:15:01,639 --> 00:15:07,189
sometimes the data new data will fit

00:15:04,399 --> 00:15:10,819
next to an old centroid and it will fit

00:15:07,189 --> 00:15:13,269
within the size constraints so it's

00:15:10,819 --> 00:15:16,100
really a simple algorithm we buffer data

00:15:13,269 --> 00:15:18,769
we add the centroids into that and sort

00:15:16,100 --> 00:15:21,589
it and then we clump it and that's our

00:15:18,769 --> 00:15:23,740
news tea digests so we can update this

00:15:21,589 --> 00:15:27,019
fairly fast

00:15:23,740 --> 00:15:30,350
and in fact we can update it in ways

00:15:27,019 --> 00:15:33,529
that amortize many of the costs relative

00:15:30,350 --> 00:15:36,769
to the final size of the tea digests not

00:15:33,529 --> 00:15:37,519
the input size that's cool because the

00:15:36,769 --> 00:15:38,570
finites

00:15:37,519 --> 00:15:41,720
the output size

00:15:38,570 --> 00:15:44,060
is now bounded and fixed that also gives

00:15:41,720 --> 00:15:45,950
us an advantage in that all of the data

00:15:44,060 --> 00:15:48,350
structures in this new algorithm are

00:15:45,950 --> 00:15:51,980
completely static there's no heap

00:15:48,350 --> 00:15:53,930
allocation at all during operation we

00:15:51,980 --> 00:15:56,330
can also now merge these things very

00:15:53,930 --> 00:15:59,300
very efficiently the merge algorithm

00:15:56,330 --> 00:16:01,550
just takes a bunch of tea digests takes

00:15:59,300 --> 00:16:04,580
all of the centroids all of the bins in

00:16:01,550 --> 00:16:07,690
there sorts them together and then goes

00:16:04,580 --> 00:16:11,600
through and clumps them as possible as

00:16:07,690 --> 00:16:14,480
necessary so the new algorithms are very

00:16:11,600 --> 00:16:16,640
very simple they give us the full

00:16:14,480 --> 00:16:19,850
adaptive non-linear bins that we want

00:16:16,640 --> 00:16:21,980
and they give us grouping and regrouping

00:16:19,850 --> 00:16:24,620
the grouping and regrouping and the

00:16:21,980 --> 00:16:27,530
merging is key because that lets this

00:16:24,620 --> 00:16:30,410
all be parallelized lets us build

00:16:27,530 --> 00:16:33,050
large-scale OLAP databases on this and

00:16:30,410 --> 00:16:36,890
the speed is very good the previous

00:16:33,050 --> 00:16:40,790
champion AVL tree digests had speed that

00:16:36,890 --> 00:16:43,820
grew slightly with size whereas the new

00:16:40,790 --> 00:16:45,950
one is just dead flat that's how fast

00:16:43,820 --> 00:16:49,370
how long it takes because the

00:16:45,950 --> 00:16:51,680
amortization is adaptive relative to the

00:16:49,370 --> 00:16:54,440
compression size and the speeds good

00:16:51,680 --> 00:16:57,050
this is a hundred nanoseconds that's ten

00:16:54,440 --> 00:16:58,790
million data points per core if you have

00:16:57,050 --> 00:17:02,800
a hundred cores doing this we're talking

00:16:58,790 --> 00:17:06,800
about a billion data points per second

00:17:02,800 --> 00:17:10,340
that's not bad the accuracy is as good

00:17:06,800 --> 00:17:12,950
or better than it was we see point per

00:17:10,340 --> 00:17:17,380
million sorts of accuracies at the tails

00:17:12,950 --> 00:17:21,350
that's in in quantile space and larger

00:17:17,380 --> 00:17:23,540
errors in the middle near the median but

00:17:21,350 --> 00:17:29,270
River that a thousand parts per million

00:17:23,540 --> 00:17:32,120
is still only 0.01% in quantile space so

00:17:29,270 --> 00:17:35,510
the accuracy is still good so the status

00:17:32,120 --> 00:17:37,760
then with a three point ax release is we

00:17:35,510 --> 00:17:41,390
have a stable system there are some

00:17:37,760 --> 00:17:44,480
known bugs in corner cases for data

00:17:41,390 --> 00:17:46,790
distribution and the AVL tree digest is

00:17:44,480 --> 00:17:48,680
still the best in the upcoming release

00:17:46,790 --> 00:17:52,330
where we have some changes in meaning

00:17:48,680 --> 00:17:55,340
for the API the accuracy is fixed

00:17:52,330 --> 00:17:57,290
pathological cases this speed is

00:17:55,340 --> 00:17:59,300
improved

00:17:57,290 --> 00:18:02,950
there's no dynamic allocation with the

00:17:59,300 --> 00:18:05,570
primary algorithm and for the float

00:18:02,950 --> 00:18:07,700
histogram which is now added into the

00:18:05,570 --> 00:18:10,370
package the speed is even better about

00:18:07,700 --> 00:18:14,180
five nanoseconds per data point it's

00:18:10,370 --> 00:18:16,610
coming real soon now if I had more

00:18:14,180 --> 00:18:17,180
evenings and longer nights it would come

00:18:16,610 --> 00:18:19,220
sooner

00:18:17,180 --> 00:18:22,510
so probably winter will accelerate it

00:18:19,220 --> 00:18:26,150
summer is a hard time here's some some

00:18:22,510 --> 00:18:29,720
ideas about a real cuffs company these

00:18:26,150 --> 00:18:32,420
guys have a very large cloud service

00:18:29,720 --> 00:18:33,950
that starts with a I can't say any more

00:18:32,420 --> 00:18:39,200
letters than that because that still

00:18:33,950 --> 00:18:42,380
leaves some ambiguity and they want to

00:18:39,200 --> 00:18:45,230
do metrics across all those machines

00:18:42,380 --> 00:18:47,120
across all the services both internal

00:18:45,230 --> 00:18:49,820
and external and they want to be able to

00:18:47,120 --> 00:18:52,040
query any combination of metrics and

00:18:49,820 --> 00:18:56,450
find out what the distribution is and

00:18:52,040 --> 00:18:58,810
display it in roughly a second million

00:18:56,450 --> 00:19:01,010
machines 10 million services

00:18:58,810 --> 00:19:02,600
measurements sometimes thousands per

00:19:01,010 --> 00:19:05,060
second and they want to be able to

00:19:02,600 --> 00:19:08,660
aggregate that on any period from

00:19:05,060 --> 00:19:12,800
seconds up to months and get a clean

00:19:08,660 --> 00:19:16,850
distribution with high accuracy and they

00:19:12,800 --> 00:19:19,040
can do that they can store a very high

00:19:16,850 --> 00:19:23,390
resolution or 5-minute resolution

00:19:19,040 --> 00:19:25,190
histograms at high compression level so

00:19:23,390 --> 00:19:27,590
they're all very small they can then

00:19:25,190 --> 00:19:30,350
combine them and merge them millions per

00:19:27,590 --> 00:19:34,550
second in order to get the aggregates

00:19:30,350 --> 00:19:36,470
over there very large time frame and

00:19:34,550 --> 00:19:39,320
they want to be able to say what has

00:19:36,470 --> 00:19:41,570
changed and the way that actually comes

00:19:39,320 --> 00:19:44,090
out what was the distribution of some

00:19:41,570 --> 00:19:47,230
key performance indicator like launch

00:19:44,090 --> 00:19:50,600
times or Layton sees our query times

00:19:47,230 --> 00:19:51,920
yesterday what was it last month what

00:19:50,600 --> 00:19:54,170
was it on these machines

00:19:51,920 --> 00:19:56,480
what was it in North America what was it

00:19:54,170 --> 00:19:59,900
in Asia they can now do those queries

00:19:56,480 --> 00:20:02,810
using this system we also have customers

00:19:59,900 --> 00:20:04,710
who have remote data centers and they

00:20:02,810 --> 00:20:06,690
have stuff I mean

00:20:04,710 --> 00:20:09,029
doesn't really matter what they stick it

00:20:06,690 --> 00:20:11,279
into a stream they can stick entire

00:20:09,029 --> 00:20:14,100
distributions which are only like a few

00:20:11,279 --> 00:20:17,039
hundred bytes into a stream they can

00:20:14,100 --> 00:20:20,460
mirror that off into analytics land into

00:20:17,039 --> 00:20:23,429
Galactic headquarters and because the

00:20:20,460 --> 00:20:25,470
stream app replication doesn't really

00:20:23,429 --> 00:20:27,870
care it's all multi master they can

00:20:25,470 --> 00:20:30,480
build a very very simple architecture at

00:20:27,870 --> 00:20:35,100
least as far as they're concerned what

00:20:30,480 --> 00:20:37,230
they do is add to a histogram until the

00:20:35,100 --> 00:20:39,450
time period has passed and send the

00:20:37,230 --> 00:20:42,029
histogram into a stream on the analytic

00:20:39,450 --> 00:20:44,640
side they just collect the histograms

00:20:42,029 --> 00:20:45,990
that they find of interest and put them

00:20:44,640 --> 00:20:48,059
together and this is the kind of

00:20:45,990 --> 00:20:51,120
simplicity you want for the application

00:20:48,059 --> 00:20:55,380
side of a metrics monitoring system now

00:20:51,120 --> 00:20:57,929
do you remember that graph earlier that

00:20:55,380 --> 00:20:59,429
was the distribution the trouble is you

00:20:57,929 --> 00:21:02,330
can't see these so I'm going to talk a

00:20:59,429 --> 00:21:06,720
little bit about how do you visualize

00:21:02,330 --> 00:21:07,649
these sorts of systems remember that

00:21:06,720 --> 00:21:10,380
data down there

00:21:07,649 --> 00:21:12,210
we have no idea what that is because you

00:21:10,380 --> 00:21:14,690
just can't see it anything less than a

00:21:12,210 --> 00:21:18,179
pixel really doesn't matter

00:21:14,690 --> 00:21:20,210
here's good results here's simulated

00:21:18,179 --> 00:21:23,909
query times or something like that for

00:21:20,210 --> 00:21:29,669
good system and here it is for a bad

00:21:23,909 --> 00:21:31,409
system good bad and can you even tell if

00:21:29,669 --> 00:21:34,080
I sum if I didn't have the title there I

00:21:31,409 --> 00:21:35,159
wouldn't be able to tell one percent of

00:21:34,080 --> 00:21:37,500
the measurements imagine we have a

00:21:35,159 --> 00:21:39,149
cluster of a hundred machines and one of

00:21:37,500 --> 00:21:42,899
them is three times slower than it used

00:21:39,149 --> 00:21:44,789
to be that means one percent or less

00:21:42,899 --> 00:21:49,110
because it's slower it may even get less

00:21:44,789 --> 00:21:51,779
traffic one percent of those query times

00:21:49,110 --> 00:21:53,250
are out here now and we can't see it

00:21:51,779 --> 00:21:55,950
because it's only one percent of the

00:21:53,250 --> 00:21:59,460
data we want to be able to see that data

00:21:55,950 --> 00:22:01,320
we want to see the difference well one

00:21:59,460 --> 00:22:03,360
of the things you can do is you can log

00:22:01,320 --> 00:22:05,220
the vertical axis you have to be careful

00:22:03,360 --> 00:22:08,760
because there's a lot of zeros and then

00:22:05,220 --> 00:22:12,149
you can see red is bad black is good the

00:22:08,760 --> 00:22:14,820
vertical axis this is what is it two

00:22:12,149 --> 00:22:18,509
orders of magnitudes so that's like one

00:22:14,820 --> 00:22:21,209
percent and 0.0

00:22:18,509 --> 00:22:23,969
one percent and you could see we're kind

00:22:21,209 --> 00:22:26,129
of in the one-percent range where bad

00:22:23,969 --> 00:22:28,789
stuff happens now if we think about it

00:22:26,129 --> 00:22:31,859
we started with uniform bins here's what

00:22:28,789 --> 00:22:33,949
non-uniform bins do one of the cool

00:22:31,859 --> 00:22:38,669
things about the non-uniform bins is

00:22:33,949 --> 00:22:41,399
that they are big out here and so where

00:22:38,669 --> 00:22:44,489
we had really spiky up and down and up

00:22:41,399 --> 00:22:47,249
and down stuff neighboring unit counts

00:22:44,489 --> 00:22:49,619
here get merged together so that when we

00:22:47,249 --> 00:22:51,809
log that vertical axis we have something

00:22:49,619 --> 00:22:53,999
very nice here and this is a uniform

00:22:51,809 --> 00:22:56,909
characteristic of all these nonlinear

00:22:53,999 --> 00:22:59,369
binning is that they give you data that

00:22:56,909 --> 00:23:01,919
average as well and so it visualizes

00:22:59,369 --> 00:23:05,099
well and so now we can say here's

00:23:01,919 --> 00:23:09,329
yesterday here's today something is

00:23:05,099 --> 00:23:11,309
wrong we can now combine this with other

00:23:09,329 --> 00:23:14,249
techniques for finding changes things

00:23:11,309 --> 00:23:16,679
like LLR and and so on for finding

00:23:14,249 --> 00:23:21,149
differences in counts and that will let

00:23:16,679 --> 00:23:23,759
us say what went wrong remember the ping

00:23:21,149 --> 00:23:28,229
data here it is on the log scale and we

00:23:23,759 --> 00:23:31,949
can now see that there's a long tail 90%

00:23:28,229 --> 00:23:34,649
99% somewhere down in the ninety-nine

00:23:31,949 --> 00:23:38,579
point eight percent of the time it's

00:23:34,649 --> 00:23:43,369
good but about 0.2% of the time it's bad

00:23:38,579 --> 00:23:48,179
so we have ability to do these counts

00:23:43,369 --> 00:23:50,039
wrong summary the pictures are right so

00:23:48,179 --> 00:23:53,849
we have the ability to do the summaries

00:23:50,039 --> 00:23:56,269
build histograms very high speed merge

00:23:53,849 --> 00:23:59,579
them in parallel store them concisely

00:23:56,269 --> 00:24:01,969
build query systems on that and now the

00:23:59,579 --> 00:24:04,949
ability to see what the differences are

00:24:01,969 --> 00:24:06,929
so that we can actually answer the

00:24:04,949 --> 00:24:09,239
questions that we had at the beginning

00:24:06,929 --> 00:24:12,449
which is why does Netflix doesn't not

00:24:09,239 --> 00:24:15,449
work or why is the system broken what

00:24:12,449 --> 00:24:18,269
has changed and I wanted to spend more

00:24:15,449 --> 00:24:21,389
time here than just me talking we have a

00:24:18,269 --> 00:24:22,859
lot of time now and not have you guys

00:24:21,389 --> 00:24:25,469
asked questions I wanted to ask you guys

00:24:22,859 --> 00:24:29,069
questions about where this ought to go

00:24:25,469 --> 00:24:32,010
what's the applicability of this sort of

00:24:29,069 --> 00:24:35,220
thing so who here has systems to monitor

00:24:32,010 --> 00:24:40,680
who here is not a real theoretician okay

00:24:35,220 --> 00:24:43,740
we I think the the colloquial term there

00:24:40,680 --> 00:24:45,300
is everybody everybody has stuff that

00:24:43,740 --> 00:24:47,970
they'll get in trouble for if it doesn't

00:24:45,300 --> 00:24:48,770
work tomorrow or tonight at 2:00 in the

00:24:47,970 --> 00:24:52,050
morning

00:24:48,770 --> 00:24:56,700
who here has Layton sees that they

00:24:52,050 --> 00:24:59,430
measure well this is a new world isn't

00:24:56,700 --> 00:25:01,200
it you know six years ago at buzzwords

00:24:59,430 --> 00:25:03,450
it was all MapReduce sort of thing and

00:25:01,200 --> 00:25:07,050
nobody cared about Layton sees now

00:25:03,450 --> 00:25:10,350
essentially everybody does who has other

00:25:07,050 --> 00:25:13,350
stuff measurements that users give you

00:25:10,350 --> 00:25:15,840
that you have no idea really what it is

00:25:13,350 --> 00:25:17,910
so if you know it's Layton sees you know

00:25:15,840 --> 00:25:21,930
how to analyze it but how many have data

00:25:17,910 --> 00:25:26,280
that users give you this just stuff

00:25:21,930 --> 00:25:29,250
that's user stuff yeah so fewer say like

00:25:26,280 --> 00:25:31,680
a quarter or so of the people I have a

00:25:29,250 --> 00:25:33,650
lot of data like that here's the data

00:25:31,680 --> 00:25:36,540
Ted deal with it

00:25:33,650 --> 00:25:38,730
so these things could make a difference

00:25:36,540 --> 00:25:41,760
than it sounds like who here has to

00:25:38,730 --> 00:25:45,690
query that data and retain it retention

00:25:41,760 --> 00:25:47,340
is a big deal right anybody have a

00:25:45,690 --> 00:25:51,090
system like that they like to describe

00:25:47,340 --> 00:25:56,580
and tell me how this wouldn't work or

00:25:51,090 --> 00:26:00,300
how it would work okay I'm going to

00:25:56,580 --> 00:26:05,670
assign questions Adrian does this look

00:26:00,300 --> 00:26:08,640
like it it'd help you yeah the you've

00:26:05,670 --> 00:26:13,460
got to digest now in your system but

00:26:08,640 --> 00:26:17,610
this is faster and and more static

00:26:13,460 --> 00:26:19,410
easier to combine it but it helps for

00:26:17,610 --> 00:26:21,690
the reasons you mentioned that it had a

00:26:19,410 --> 00:26:23,880
pathetic to the value that you have all

00:26:21,690 --> 00:26:27,170
right so may take advantage of this

00:26:23,880 --> 00:26:30,420
question to do future requests please so

00:26:27,170 --> 00:26:32,400
often our users so currently with today

00:26:30,420 --> 00:26:35,610
dress you try the centered' and account

00:26:32,400 --> 00:26:37,140
white bucket and our users would like

00:26:35,610 --> 00:26:38,940
try to forget to have Barbara gates

00:26:37,140 --> 00:26:41,610
about some other fields that they have

00:26:38,940 --> 00:26:43,530
in double comments and tea digests will

00:26:41,610 --> 00:26:46,130
would allow to do that but the API does

00:26:43,530 --> 00:26:51,690
not allow okay today so

00:26:46,130 --> 00:26:53,340
you know it is not a feature in 4.01 let

00:26:51,690 --> 00:26:56,280
me explain a little bit what the

00:26:53,340 --> 00:27:00,570
question means the idea here is that

00:26:56,280 --> 00:27:03,540
from one measurement one machine one

00:27:00,570 --> 00:27:05,400
place there's some characteristics of

00:27:03,540 --> 00:27:08,010
that measurement

00:27:05,400 --> 00:27:09,780
what machine did it come from what is

00:27:08,010 --> 00:27:11,730
the metric what is the application what

00:27:09,780 --> 00:27:14,010
is the user what is the table what is

00:27:11,730 --> 00:27:15,960
the file so it has a bunch of

00:27:14,010 --> 00:27:21,120
characteristics that apply to the entire

00:27:15,960 --> 00:27:23,160
histogram but then when we combine

00:27:21,120 --> 00:27:26,660
histograms with different

00:27:23,160 --> 00:27:30,150
characteristics some of these samples

00:27:26,660 --> 00:27:33,420
like the red ones obviously come from

00:27:30,150 --> 00:27:36,270
the red machines and the black ones come

00:27:33,420 --> 00:27:38,550
from the black machines when I combine

00:27:36,270 --> 00:27:41,060
these two I will get a histogram that

00:27:38,550 --> 00:27:44,580
looks a lot like the red distribution

00:27:41,060 --> 00:27:47,400
but I will forget that all of the black

00:27:44,580 --> 00:27:49,850
measurements are over here and some of

00:27:47,400 --> 00:27:52,650
the red measurements are over here and

00:27:49,850 --> 00:27:55,230
so I think what Adrian's asking for is

00:27:52,650 --> 00:27:59,100
it would be nice if those columns if

00:27:55,230 --> 00:28:04,050
those buckets remembered the tags that

00:27:59,100 --> 00:28:07,830
were on the measurements he's nodding

00:28:04,050 --> 00:28:09,930
thank goodness and so then what you

00:28:07,830 --> 00:28:13,140
could do is you could ideally draw a

00:28:09,930 --> 00:28:18,450
circle around this part of the histogram

00:28:13,140 --> 00:28:22,470
and say what is different why are these

00:28:18,450 --> 00:28:25,170
data different from all others and does

00:28:22,470 --> 00:28:26,010
that sound good to everybody else yeah

00:28:25,170 --> 00:28:30,210
okay

00:28:26,010 --> 00:28:32,640
well then I'd better do it huh barking

00:28:30,210 --> 00:28:34,860
follow to the window yeah or I'll work

00:28:32,640 --> 00:28:38,310
together with you on it the trick is to

00:28:34,860 --> 00:28:45,510
do that economic like Ellen's got a hand

00:28:38,310 --> 00:28:47,790
up and she picked on Adrian and said you

00:28:45,510 --> 00:28:49,890
knew he was using tea digest could he

00:28:47,790 --> 00:28:52,170
just say what his system is or his

00:28:49,890 --> 00:28:54,540
company for I can say it for him he

00:28:52,170 --> 00:28:56,640
works for elasticsearch and the reason I

00:28:54,540 --> 00:28:58,650
pick on him is because he contributed a

00:28:56,640 --> 00:28:59,440
much better implementation than my first

00:28:58,650 --> 00:29:01,690
implement

00:28:59,440 --> 00:29:04,060
and so I have to be mean to him now in

00:29:01,690 --> 00:29:06,790
retribution for for showing me up a

00:29:04,060 --> 00:29:08,800
little bit yeah he made it three or four

00:29:06,790 --> 00:29:11,980
times fast or maybe five times faster in

00:29:08,800 --> 00:29:13,960
the original implementation so that he

00:29:11,980 --> 00:29:15,940
could put it into elasticsearch and so

00:29:13,960 --> 00:29:18,910
with this change that he suggesting

00:29:15,940 --> 00:29:20,460
users would like is that feasible for

00:29:18,910 --> 00:29:24,610
you and do you see it in the near future

00:29:20,460 --> 00:29:28,200
I think it is feasible the the questions

00:29:24,610 --> 00:29:31,440
really come down to space time dynamism

00:29:28,200 --> 00:29:33,970
it isn't necessarily clear right away

00:29:31,440 --> 00:29:38,580
how will you make that as efficient as

00:29:33,970 --> 00:29:43,660
we would like it might be that we use a

00:29:38,580 --> 00:29:45,850
digest for the the tags or it may be

00:29:43,660 --> 00:29:48,670
good enough just to have an extensible

00:29:45,850 --> 00:29:50,650
list associated with each thing because

00:29:48,670 --> 00:29:53,650
we don't necessarily need to know how

00:29:50,650 --> 00:29:55,060
often the tags occurred we could have a

00:29:53,650 --> 00:29:57,430
vote right here so with that

00:29:55,060 --> 00:30:01,450
implementation we could remember the

00:29:57,430 --> 00:30:05,050
counts for all the tags or not we could

00:30:01,450 --> 00:30:11,160
instead remember just the tags for each

00:30:05,050 --> 00:30:11,160
bucket anybody have thoughts on that

00:30:11,580 --> 00:30:20,620
we'll take it to the mailing list so

00:30:15,040 --> 00:30:24,990
yeah here's somebody over here yes we

00:30:20,620 --> 00:30:28,060
run a fairly large open T is DB cluster

00:30:24,990 --> 00:30:30,250
we have like 300 thousand writes per

00:30:28,060 --> 00:30:32,320
second now or something I think and it

00:30:30,250 --> 00:30:35,050
becomes very painful with yeah yeah you

00:30:32,320 --> 00:30:37,120
know it does where we are you know we've

00:30:35,050 --> 00:30:40,090
built a bit of a low latency solution to

00:30:37,120 --> 00:30:42,310
get that second thing that we wanted a

00:30:40,090 --> 00:30:44,040
second solution thing so we also have a

00:30:42,310 --> 00:30:46,600
like a lower latency in-memory version

00:30:44,040 --> 00:30:49,690
that is not open these DB that we built

00:30:46,600 --> 00:30:51,820
ourselves but the question is is anybody

00:30:49,690 --> 00:30:55,330
working on getting digests into

00:30:51,820 --> 00:30:57,490
something like open DSTV or some more

00:30:55,330 --> 00:31:01,390
you know where we don't have to build

00:30:57,490 --> 00:31:03,700
everything ourselves so I've seen I've

00:31:01,390 --> 00:31:06,070
seen some work on that and basically

00:31:03,700 --> 00:31:08,710
what people do is they say I'll take a

00:31:06,070 --> 00:31:12,570
float histogram with really coarse

00:31:08,710 --> 00:31:15,450
buckets so like two buckets per octave

00:31:12,570 --> 00:31:19,190
that means I only have 32 buckets or

00:31:15,450 --> 00:31:22,610
something for a decent dynamic range and

00:31:19,190 --> 00:31:26,340
then I just make 32 metrics out of it

00:31:22,610 --> 00:31:29,570
which just makes the port open DSD be

00:31:26,340 --> 00:31:32,429
even worse because now instead of

00:31:29,570 --> 00:31:34,080
300,000 measurements well 10,000 of

00:31:32,429 --> 00:31:36,059
those I want distributions but now they

00:31:34,080 --> 00:31:39,299
each have 30 times that so I just

00:31:36,059 --> 00:31:42,240
doubled tripled quadrupled load on it so

00:31:39,299 --> 00:31:45,200
it just makes things worse I think a

00:31:42,240 --> 00:31:46,889
much more promising point of view is

00:31:45,200 --> 00:31:49,730
something like the streaming

00:31:46,889 --> 00:31:52,649
architecture that I talked about here I

00:31:49,730 --> 00:31:54,179
don't have a map R hat and it hits red

00:31:52,649 --> 00:31:56,879
with white letters on it so I can't wear

00:31:54,179 --> 00:32:00,149
it anyway but the fact is that these

00:31:56,879 --> 00:32:03,570
streams are very very fast and they can

00:32:00,149 --> 00:32:07,559
handle very very large cardinality of

00:32:03,570 --> 00:32:10,500
topics and so we could easily build a

00:32:07,559 --> 00:32:14,129
system on that where each topic is the

00:32:10,500 --> 00:32:16,679
metric and the stream then holds

00:32:14,129 --> 00:32:18,750
whatever kind of data we want that could

00:32:16,679 --> 00:32:21,870
be a measurement or it could easily be a

00:32:18,750 --> 00:32:23,549
distribution of measurements and it

00:32:21,870 --> 00:32:26,909
would be pretty easy to put something a

00:32:23,549 --> 00:32:30,539
lot like the open T's to be API over the

00:32:26,909 --> 00:32:32,909
top of that it's just a REST API and you

00:32:30,539 --> 00:32:35,970
could query a stream by thing

00:32:32,909 --> 00:32:37,860
give me these offsets well which offsets

00:32:35,970 --> 00:32:40,169
I can have a side stream which is an

00:32:37,860 --> 00:32:42,809
index into time so I can figure out

00:32:40,169 --> 00:32:44,700
which offsets I want and I just get the

00:32:42,809 --> 00:32:47,700
raw data then and I can combine them any

00:32:44,700 --> 00:32:50,899
way I'd like so that's another

00:32:47,700 --> 00:32:53,549
alternative and have an intern

00:32:50,899 --> 00:32:56,250
experimenting with that basic idea and

00:32:53,549 --> 00:32:58,769
the the difference in performance should

00:32:56,250 --> 00:33:01,200
be stunning because even just one node

00:32:58,769 --> 00:33:04,230
of these kind of streams can do 10

00:33:01,200 --> 00:33:06,360
million inserts per second instead of a

00:33:04,230 --> 00:33:09,870
hundred thousand three hundred thousand

00:33:06,360 --> 00:33:12,090
is very very good on open TS DB we have

00:33:09,870 --> 00:33:15,899
some customers who get it over a couple

00:33:12,090 --> 00:33:21,240
million but it's scary you know at any

00:33:15,899 --> 00:33:23,490
moment the wheels could come off yeah I

00:33:21,240 --> 00:33:25,230
mean it's also I think for King at least

00:33:23,490 --> 00:33:26,370
a bit of the fact that HBase has been an

00:33:25,230 --> 00:33:30,420
awkward thing for

00:33:26,370 --> 00:33:32,130
us I mean we do run a Hadoop cluster but

00:33:30,420 --> 00:33:33,660
HBase needs a completely different set

00:33:32,130 --> 00:33:35,100
of settings for the Hadoop cluster and

00:33:33,660 --> 00:33:38,670
like it becomes a bit of an awkward

00:33:35,100 --> 00:33:41,610
thing yeah eh basis is not a good

00:33:38,670 --> 00:33:43,050
neighbor it is the one that you don't

00:33:41,610 --> 00:33:46,680
want in the apartment house you want it

00:33:43,050 --> 00:33:48,120
in its own house and to plug again you

00:33:46,680 --> 00:33:50,460
can do better with map our DB because

00:33:48,120 --> 00:33:53,520
it's more of a friendly neighbor but the

00:33:50,460 --> 00:33:55,920
fact of the textual storage the fact

00:33:53,520 --> 00:33:57,990
that open T is to be is so oriented

00:33:55,920 --> 00:34:01,290
around one number per measurement per

00:33:57,990 --> 00:34:02,730
time really really limits the whole idea

00:34:01,290 --> 00:34:05,880
and putting it on a more general

00:34:02,730 --> 00:34:08,490
streaming platform does a couple of

00:34:05,880 --> 00:34:12,210
database is all designed around mutation

00:34:08,490 --> 00:34:16,740
and this data once it's done it's static

00:34:12,210 --> 00:34:19,679
so that it's crazy to pay the cost of

00:34:16,740 --> 00:34:23,250
died neck dynamic content when you have

00:34:19,679 --> 00:34:25,740
no dynamic content a stream is optimized

00:34:23,250 --> 00:34:30,360
all around static content and is much

00:34:25,740 --> 00:34:32,550
more natural for time series what's the

00:34:30,360 --> 00:34:45,000
normal size of the T dices that depends

00:34:32,550 --> 00:34:46,820
on your your your parameter right so if

00:34:45,000 --> 00:34:49,520
you wanted say ninety-nine point nine

00:34:46,820 --> 00:34:52,290
then I was set at around forty or fifty

00:34:49,520 --> 00:34:56,310
the total size of it is twice the

00:34:52,290 --> 00:34:58,860
compression rate so if fifty there could

00:34:56,310 --> 00:35:02,310
be a hundred centroids typically there's

00:34:58,860 --> 00:35:04,980
1.2 times the compression size so there

00:35:02,310 --> 00:35:07,170
were typically the 60 centroids one

00:35:04,980 --> 00:35:09,750
floating point for the centroid one

00:35:07,170 --> 00:35:14,750
floating point typically for the count

00:35:09,750 --> 00:35:18,240
so that we can go above two billion and

00:35:14,750 --> 00:35:22,820
a little bit of other data so you can

00:35:18,240 --> 00:35:25,110
you can guess that's about 15 times 50

00:35:22,820 --> 00:35:27,770
uncompressed and then these compress

00:35:25,110 --> 00:35:32,550
pretty well because the counts very

00:35:27,770 --> 00:35:42,160
often are small so

00:35:32,550 --> 00:35:44,680
some good ideas here yeah a patient

00:35:42,160 --> 00:35:47,800
ma'am hi thank you thank you for the

00:35:44,680 --> 00:35:50,230
talk um what for what I understood the

00:35:47,800 --> 00:35:52,780
only aggregation happening in time-based

00:35:50,230 --> 00:35:54,220
periods is the counting is the histogram

00:35:52,780 --> 00:35:57,640
making right

00:35:54,220 --> 00:36:00,460
I had only low-level aggregation yet

00:35:57,640 --> 00:36:04,420
happening yeah at that point you lose

00:36:00,460 --> 00:36:06,670
the shape of the signals you lose the

00:36:04,420 --> 00:36:08,860
time shape yes you lose a lot of the

00:36:06,670 --> 00:36:12,060
time shape so if you're doing it say

00:36:08,860 --> 00:36:15,550
every 10 seconds you lose the sub-second

00:36:12,060 --> 00:36:17,590
variation in that oh I was considering

00:36:15,550 --> 00:36:20,050
doing only like minute or five-minute

00:36:17,590 --> 00:36:22,540
intervals that's why I thought we would

00:36:20,050 --> 00:36:24,190
be losing more of the time shape or even

00:36:22,540 --> 00:36:26,680
devolve okay

00:36:24,190 --> 00:36:29,140
have you considered or would it be

00:36:26,680 --> 00:36:31,320
possible to do like Fourier analysis

00:36:29,140 --> 00:36:35,140
frequency analysis and store the

00:36:31,320 --> 00:36:38,440
frequency coefficients the decomposition

00:36:35,140 --> 00:36:40,140
on frequency well my own feeling is that

00:36:38,440 --> 00:36:45,100
if you really want the time evolution

00:36:40,140 --> 00:36:47,080
store the friggin data so you know the

00:36:45,100 --> 00:36:50,740
reason for doing something that makes

00:36:47,080 --> 00:36:52,390
the data smaller in my mind because I

00:36:50,740 --> 00:36:54,370
sell things that store data of course

00:36:52,390 --> 00:36:57,400
but the reason that I think that's

00:36:54,370 --> 00:37:00,400
making data smaller is good is so that

00:36:57,400 --> 00:37:02,050
you could query it faster but if you're

00:37:00,400 --> 00:37:04,630
sitting there saying what actually

00:37:02,050 --> 00:37:06,520
happened in this three-second interval I

00:37:04,630 --> 00:37:09,880
think what you should be doing is

00:37:06,520 --> 00:37:12,400
storing the actual data because you

00:37:09,880 --> 00:37:14,740
cannot reasonably summarize that data

00:37:12,400 --> 00:37:19,260
and still see those dynamics

00:37:14,740 --> 00:37:21,940
and and so you know in all of these

00:37:19,260 --> 00:37:26,170
systems the point of it is compression

00:37:21,940 --> 00:37:27,970
and sketching we can compress by by

00:37:26,170 --> 00:37:31,930
saying what is the distribution and we

00:37:27,970 --> 00:37:34,330
forget exactly when that happened or we

00:37:31,930 --> 00:37:37,660
could say here's the time dynamics and

00:37:34,330 --> 00:37:39,820
it's like a physical resonance system so

00:37:37,660 --> 00:37:42,750
that phooey a analysis would compress

00:37:39,820 --> 00:37:45,520
the data but in all of these systems

00:37:42,750 --> 00:37:48,430
compression is in by

00:37:45,520 --> 00:37:51,580
implication a lossy compression we're

00:37:48,430 --> 00:37:54,070
aggregating data and and by nature that

00:37:51,580 --> 00:37:56,500
is lossy if you don't want the loss if

00:37:54,070 --> 00:37:58,270
you really want to know that data it's

00:37:56,500 --> 00:38:00,430
going to be very hard to have a lossy

00:37:58,270 --> 00:38:03,340
thing that gives you exactly the

00:38:00,430 --> 00:38:07,240
original data unless you know a lot

00:38:03,340 --> 00:38:08,650
about the signal so I would I would

00:38:07,240 --> 00:38:11,500
think the right answer just store the

00:38:08,650 --> 00:38:13,810
real data look think I'm worried the

00:38:11,500 --> 00:38:15,280
other stuff and then Ellen has a hand

00:38:13,810 --> 00:38:17,200
I hope she's got somebody over there

00:38:15,280 --> 00:38:20,110
we're almost out of time I think one

00:38:17,200 --> 00:38:21,850
more question yeah just about two more

00:38:20,110 --> 00:38:22,920
minutes is there one more question back

00:38:21,850 --> 00:38:26,710
there

00:38:22,920 --> 00:38:28,990
somebody's wiggling it's like an auction

00:38:26,710 --> 00:38:30,400
if you wiggle too much you bought it so

00:38:28,990 --> 00:38:31,840
you better better have a question ready

00:38:30,400 --> 00:38:38,740
if you're going to wiggle here we go

00:38:31,840 --> 00:38:41,680
Adrian again have you thought about

00:38:38,740 --> 00:38:45,630
moving to digest to an organization such

00:38:41,680 --> 00:38:48,220
as yep it Apache Software Foundation oh

00:38:45,630 --> 00:38:51,430
let me put this hat back on this was my

00:38:48,220 --> 00:38:54,220
Apache hat yeah so we should always

00:38:51,430 --> 00:38:58,180
consider what is the right course for a

00:38:54,220 --> 00:39:00,490
project and my guess is the tea digest

00:38:58,180 --> 00:39:04,330
is going to be a very stable project

00:39:00,490 --> 00:39:06,850
there won't be big changes over time

00:39:04,330 --> 00:39:10,000
adding the tokens and the histories that

00:39:06,850 --> 00:39:12,790
way would be a significant change adding

00:39:10,000 --> 00:39:14,590
a whole new class of algorithm to make

00:39:12,790 --> 00:39:16,420
it five times faster that's a

00:39:14,590 --> 00:39:20,140
significant change I don't expect that

00:39:16,420 --> 00:39:23,020
to happen very often anymore as such

00:39:20,140 --> 00:39:25,360
it's going to be boring the mailing list

00:39:23,020 --> 00:39:28,120
will be very quiet and so on and also

00:39:25,360 --> 00:39:30,400
now I've been a limiting factor so far

00:39:28,120 --> 00:39:32,530
because the releases are not as often as

00:39:30,400 --> 00:39:34,450
I would like if we had a bigger

00:39:32,530 --> 00:39:37,300
community that might happen more often

00:39:34,450 --> 00:39:40,350
but the community is very very small we

00:39:37,300 --> 00:39:43,240
have five to ten contributors so far and

00:39:40,350 --> 00:39:45,280
some have come and some have gone so

00:39:43,240 --> 00:39:49,540
it'd be very very hard to build a real

00:39:45,280 --> 00:39:51,490
community around this and so the point

00:39:49,540 --> 00:39:55,090
of Apache is to build communities that

00:39:51,490 --> 00:39:58,570
will outlive people and I think that's

00:39:55,090 --> 00:39:59,990
totally a good idea if the project is

00:39:58,570 --> 00:40:02,630
the kind that would act

00:39:59,990 --> 00:40:04,460
I have a community around it I don't

00:40:02,630 --> 00:40:07,130
think that tea digest is going to build

00:40:04,460 --> 00:40:08,780
a big community so I don't think Apache

00:40:07,130 --> 00:40:10,850
is appropriate for it

00:40:08,780 --> 00:40:13,070
I do think that open source is

00:40:10,850 --> 00:40:15,590
appropriate but for now I think it's

00:40:13,070 --> 00:40:19,640
kind of a appropriate out of one-person

00:40:15,590 --> 00:40:20,810
show level of effort if I ever get tired

00:40:19,640 --> 00:40:23,630
of it then we need to figure out

00:40:20,810 --> 00:40:26,570
something and maybe Commons mass at

00:40:23,630 --> 00:40:30,470
Apache might be a sort of thing but even

00:40:26,570 --> 00:40:32,750
there you have risk of orphan code that

00:40:30,470 --> 00:40:37,310
nobody currently in the Commons project

00:40:32,750 --> 00:40:40,970
understands or updates and so I think

00:40:37,310 --> 00:40:43,640
it'll become static I think that it

00:40:40,970 --> 00:40:45,440
won't matter that it's static and so I

00:40:43,640 --> 00:40:48,950
don't think that another structure

00:40:45,440 --> 00:40:51,230
unless you have an idea but so far I've

00:40:48,950 --> 00:40:53,960
been very open about it I believe very

00:40:51,230 --> 00:40:56,030
strongly an open community if it is open

00:40:53,960 --> 00:40:58,820
source open source close community I

00:40:56,030 --> 00:41:00,980
think is a bad contradiction but right

00:40:58,820 --> 00:41:03,170
now I think it's appropriate having gone

00:41:00,980 --> 00:41:05,840
through an incubation of projects that

00:41:03,170 --> 00:41:09,109
just couldn't build a community I think

00:41:05,840 --> 00:41:12,040
it's better as a non Apache low overhead

00:41:09,109 --> 00:41:13,850
low friction amorphous thing

00:41:12,040 --> 00:41:15,680
unfortunately we're out of time but

00:41:13,850 --> 00:41:17,390
sounds like a lot of great discussions

00:41:15,680 --> 00:41:20,980
you can find Ted afterwards let's give

00:41:17,390 --> 00:41:20,980

YouTube URL: https://www.youtube.com/watch?v=Y0eF7SuMQjk


