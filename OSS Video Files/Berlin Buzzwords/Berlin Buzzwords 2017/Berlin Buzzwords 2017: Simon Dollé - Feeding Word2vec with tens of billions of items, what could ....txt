Title: Berlin Buzzwords 2017: Simon Dollé - Feeding Word2vec with tens of billions of items, what could ...
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Simon Dollé-Feeding talking about "Feeding Word2vec with tens of billions of items, what could possibly go wrong?".

Word2vec is an unsupervised algorithm which is able to represent words as vectors, the so-called word embeddings. It computes them so that words with close meanings are close together in the vectorial space. Originally developed for NLP applications, it has recently proven to be extremely relevant in numerous contexts such as biology, speech recognition, recommendation, etc.

At Criteo, an ad retargeting company, we use Word2vec to compute product embeddings. This allows us to compute similarities between products and consequently improve the relevance of the products we recommend to our users.

Spark MLlib provides a distributed implementation of Word2vec. However, using it daily on tens of billions of item as we do, raises challenges in terms of scalability and quality. In this talkI will share with you how we moved from POC implementation to A/B-Test. I will tell you the numerous things we broke during this journey and how we fixed them. Finally, I will expose how we plan to improve our product embeddings and use them in different ways.

Read more:
https://2017.berlinbuzzwords.de/17/session/feeding-word2vec-tens-billions-items-what-could-possibly-go-wrong

About Simon Dollé:
https://2017.berlinbuzzwords.de/users/simon-dolle

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:07,069 --> 00:00:12,180
hi thank you

00:00:09,240 --> 00:00:15,090
yeah I'm Simon a software engineer at

00:00:12,180 --> 00:00:16,920
créteil and ad retargeting company and

00:00:15,090 --> 00:00:19,859
today I want to show you how expect our

00:00:16,920 --> 00:00:21,810
experience with using water web at a

00:00:19,859 --> 00:00:24,380
very large scale so first I would like

00:00:21,810 --> 00:00:28,110
to know who knows what water works here

00:00:24,380 --> 00:00:31,110
in the room and who actually used it in

00:00:28,110 --> 00:00:33,360
production okay what other people but

00:00:31,110 --> 00:00:36,390
I'll still do an introduction about what

00:00:33,360 --> 00:00:39,270
is water EXO water vac is an algorithm

00:00:36,390 --> 00:00:42,329
that was originally invented by Google

00:00:39,270 --> 00:00:44,090
researchers in an NLP context and the

00:00:42,329 --> 00:00:47,090
goal is to be able to compute

00:00:44,090 --> 00:00:49,890
similarities between words like

00:00:47,090 --> 00:00:53,010
introducing what they do it like they

00:00:49,890 --> 00:00:55,890
compute a mathematical representation of

00:00:53,010 --> 00:01:00,930
each word like your high dimensional

00:00:55,890 --> 00:01:03,149
vector and then we try to place it in

00:01:00,930 --> 00:01:04,949
this high dimensional vector here the

00:01:03,149 --> 00:01:07,020
dimension is 2 which is not very high

00:01:04,949 --> 00:01:09,780
but for the his of representation I'll

00:01:07,020 --> 00:01:13,460
keep this way and the goal then is to be

00:01:09,780 --> 00:01:15,869
able to group the the representation of

00:01:13,460 --> 00:01:18,659
words with similar meeting together so

00:01:15,869 --> 00:01:21,540
that we able to compute the distances

00:01:18,659 --> 00:01:24,840
over world so here we have fast speed

00:01:21,540 --> 00:01:28,760
instantaneous house building it all AZ

00:01:24,840 --> 00:01:31,200
like different cluster of of words and

00:01:28,760 --> 00:01:33,840
how does it work actually how do we

00:01:31,200 --> 00:01:35,280
learn where to put the words into this

00:01:33,840 --> 00:01:35,759
high dimensional space that's the

00:01:35,280 --> 00:01:39,509
question

00:01:35,759 --> 00:01:41,729
so accuse Jesus training data well let's

00:01:39,509 --> 00:01:44,340
use a very simple train data like a

00:01:41,729 --> 00:01:47,220
simple sent sentence the quick brown fox

00:01:44,340 --> 00:01:52,740
jumped over the lazy dog what it does so

00:01:47,220 --> 00:01:54,479
it first attribute a numbering so a high

00:01:52,740 --> 00:01:57,390
dimensional vector to each word and

00:01:54,479 --> 00:01:59,490
place them randomly in the space so here

00:01:57,390 --> 00:02:03,390
I have quick jump etc everything is

00:01:59,490 --> 00:02:05,850
random and then what it does is like it

00:02:03,390 --> 00:02:08,369
it's a word for the first word or the

00:02:05,850 --> 00:02:10,259
second word select it and then it masks

00:02:08,369 --> 00:02:13,499
the surrounding words so here the

00:02:10,259 --> 00:02:16,290
previews and um the next world

00:02:13,499 --> 00:02:18,600
it might SEM and it has the algorithm to

00:02:16,290 --> 00:02:22,170
predict this surrounding words so

00:02:18,600 --> 00:02:25,500
like hey I have what quick what do you

00:02:22,170 --> 00:02:29,070
think is more probable to to be before

00:02:25,500 --> 00:02:33,510
and then to answer this question the

00:02:29,070 --> 00:02:34,890
algorithm looks at the closest world to

00:02:33,510 --> 00:02:38,460
the quick actual quick word

00:02:34,890 --> 00:02:41,370
representation and then it figures out

00:02:38,460 --> 00:02:43,500
if it got it wrong or not if it got it

00:02:41,370 --> 00:02:46,500
right then change nothing if it got it

00:02:43,500 --> 00:02:50,370
wrong then it modified the position of

00:02:46,500 --> 00:02:52,230
the of the underlings so that the word

00:02:50,370 --> 00:02:53,970
to be predicted gets closer to the

00:02:52,230 --> 00:02:57,000
actual representation so here we are

00:02:53,970 --> 00:02:59,700
going to move the the embedding slightly

00:02:57,000 --> 00:03:01,370
closer and we do the same thing for

00:02:59,700 --> 00:03:04,800
Brown because which was the next word

00:03:01,370 --> 00:03:07,200
but what we actually also do is like we

00:03:04,800 --> 00:03:09,980
also move the quick quick word so that

00:03:07,200 --> 00:03:13,650
it gets closer to the other ones and

00:03:09,980 --> 00:03:16,560
finally jumped was really close to quick

00:03:13,650 --> 00:03:18,240
here and we don't want to be that close

00:03:16,560 --> 00:03:22,020
because in this context is not really

00:03:18,240 --> 00:03:24,540
relevant so we pulled this away so we

00:03:22,020 --> 00:03:28,040
have here or gradients like we know how

00:03:24,540 --> 00:03:30,870
to update our weights and then we move

00:03:28,040 --> 00:03:32,640
we move the the corresponding embeddings

00:03:30,870 --> 00:03:35,250
and then we have done what step of the

00:03:32,640 --> 00:03:38,790
algorithm and we move to the other step

00:03:35,250 --> 00:03:41,630
by selects the next word Brown and we do

00:03:38,790 --> 00:03:45,860
something like updating our weights and

00:03:41,630 --> 00:03:50,100
so on and so on and at the end we'll get

00:03:45,860 --> 00:03:52,260
groups cluster of words like here in

00:03:50,100 --> 00:03:54,540
this specific sentence that Fox is

00:03:52,260 --> 00:03:56,430
really rated too quick and brown dog is

00:03:54,540 --> 00:04:01,230
related to lazy jumped is somehow

00:03:56,430 --> 00:04:03,150
neutral and V is also neutral and I told

00:04:01,230 --> 00:04:05,190
you we do this over and over and the

00:04:03,150 --> 00:04:07,890
point is back to HUD the algorithm

00:04:05,190 --> 00:04:09,690
converge we reduce the size of the

00:04:07,890 --> 00:04:15,030
displacement over the time to be sure

00:04:09,690 --> 00:04:17,250
that it converts so this ought to work

00:04:15,030 --> 00:04:19,680
algorithm has proven to be extremely

00:04:17,250 --> 00:04:21,989
successful in the NLP context and many

00:04:19,680 --> 00:04:26,790
people started to use it in other

00:04:21,989 --> 00:04:29,070
contexts and we at créteil we do ad

00:04:26,790 --> 00:04:31,050
retargeting this is what I told you so

00:04:29,070 --> 00:04:31,840
what we do actually is like we display

00:04:31,050 --> 00:04:34,570
halves to the

00:04:31,840 --> 00:04:39,130
user and then we place products inside

00:04:34,570 --> 00:04:41,620
this hat and we are we bill our clients

00:04:39,130 --> 00:04:44,889
only if the user click on the product so

00:04:41,620 --> 00:04:47,740
we have a relief strong incentive to our

00:04:44,889 --> 00:04:51,610
place relevant products inside our hats

00:04:47,740 --> 00:04:53,860
and we thought hey could we use this

00:04:51,610 --> 00:04:56,350
water back to improve our product

00:04:53,860 --> 00:04:58,540
recommendation and by doing the simple

00:04:56,350 --> 00:05:00,699
analogy we managed to do this so the

00:04:58,540 --> 00:05:03,340
analogy is the following flag instead of

00:05:00,699 --> 00:05:06,880
talking about words will consider

00:05:03,340 --> 00:05:10,720
products and it's instead of considering

00:05:06,880 --> 00:05:13,870
sentences will consider user stations so

00:05:10,720 --> 00:05:15,550
for instance here we have a user

00:05:13,870 --> 00:05:18,669
stations that we capture for our

00:05:15,550 --> 00:05:21,729
trackers so we have one user and he has

00:05:18,669 --> 00:05:25,690
been yes in an iphone and then a headset

00:05:21,729 --> 00:05:30,220
and then trousers t-shirts etc and so

00:05:25,690 --> 00:05:32,650
this is our new sentence and then we can

00:05:30,220 --> 00:05:35,740
apply exactly the same algorithm try to

00:05:32,650 --> 00:05:38,740
pick the headphone try to predict which

00:05:35,740 --> 00:05:40,900
a word which product are going to be the

00:05:38,740 --> 00:05:45,039
next one and the previous one and we

00:05:40,900 --> 00:05:48,280
this way we can place the product inside

00:05:45,039 --> 00:05:50,080
a high dimensional space and then figure

00:05:48,280 --> 00:05:51,970
out that these are clauses and we are

00:05:50,080 --> 00:05:53,650
these are close together and these

00:05:51,970 --> 00:05:58,479
products are IT products and these are

00:05:53,650 --> 00:06:02,940
close together the point is that we work

00:05:58,479 --> 00:06:06,789
at a very large scale and to give you

00:06:02,940 --> 00:06:11,919
another magnitude like we have to

00:06:06,789 --> 00:06:16,840
process 100 and and 20 billion worth or

00:06:11,919 --> 00:06:18,760
billions events and to give you another

00:06:16,840 --> 00:06:22,870
of magnitude like if you figure out a

00:06:18,760 --> 00:06:24,789
Wikipedia then it's almost for the whole

00:06:22,870 --> 00:06:27,669
Wikipedia in whole languages this is

00:06:24,789 --> 00:06:29,260
four times less even is to give you an

00:06:27,669 --> 00:06:33,430
example so in the original

00:06:29,260 --> 00:06:36,150
implementation of what to Veck it was a

00:06:33,430 --> 00:06:38,590
simple binary distributed well

00:06:36,150 --> 00:06:42,430
distributed over the web where you could

00:06:38,590 --> 00:06:44,080
only train it on a single desktop so it

00:06:42,430 --> 00:06:45,069
obviously doesn't in here it will take

00:06:44,080 --> 00:06:46,810
four

00:06:45,069 --> 00:06:51,330
so we have to distribute this

00:06:46,810 --> 00:06:55,930
computation luckily a spark Apache spark

00:06:51,330 --> 00:06:58,539
and the as in this ml lib project an

00:06:55,930 --> 00:07:00,930
implementation of the distributed water

00:06:58,539 --> 00:07:04,360
back I'll explain you how it works

00:07:00,930 --> 00:07:07,599
so the problem is the following we have

00:07:04,360 --> 00:07:11,139
now many user sessions and we also have

00:07:07,599 --> 00:07:13,599
many computers and we want Q compute

00:07:11,139 --> 00:07:17,500
embeddings for these user sessions using

00:07:13,599 --> 00:07:19,599
the cursor of machines that we have what

00:07:17,500 --> 00:07:22,900
we do is actually what the algorithm do

00:07:19,599 --> 00:07:24,699
is actually pretty simple well it split

00:07:22,900 --> 00:07:27,970
the timelines and create some partitions

00:07:24,699 --> 00:07:30,009
for corresponding to the number of nodes

00:07:27,970 --> 00:07:33,669
that you have to process and then it

00:07:30,009 --> 00:07:38,530
sends this data over one single computer

00:07:33,669 --> 00:07:40,060
each time it computes unveilings so like

00:07:38,530 --> 00:07:42,250
each computer is in slightly different

00:07:40,060 --> 00:07:43,870
products in a different order so the

00:07:42,250 --> 00:07:50,169
computation buildings are largely

00:07:43,870 --> 00:07:54,789
different and then it merge them by

00:07:50,169 --> 00:07:57,039
simply averaging them so by doing so

00:07:54,789 --> 00:07:59,139
obviously you lose some quality because

00:07:57,039 --> 00:08:02,190
each computer art had only pass your

00:07:59,139 --> 00:08:08,680
knowledge of the full distribution but

00:08:02,190 --> 00:08:11,440
somehow it works so it was all good like

00:08:08,680 --> 00:08:12,940
we had our data we had the algorithm and

00:08:11,440 --> 00:08:15,490
we had the distribution using a purchase

00:08:12,940 --> 00:08:17,409
battle with with or take we run on it we

00:08:15,490 --> 00:08:19,389
have also very large closer more than

00:08:17,409 --> 00:08:22,659
1,000 note like shouldn't your program

00:08:19,389 --> 00:08:28,479
at all will run it and should work and

00:08:22,659 --> 00:08:31,060
obviously it crashed and and to figure

00:08:28,479 --> 00:08:33,159
out well we had to figure out why I'm

00:08:31,060 --> 00:08:37,209
the point is like okay we were able to

00:08:33,159 --> 00:08:40,000
scale and had were able to do

00:08:37,209 --> 00:08:43,990
computation over overall terms of events

00:08:40,000 --> 00:08:47,500
etc but actually the limitation was the

00:08:43,990 --> 00:08:50,320
number of unique products that we had so

00:08:47,500 --> 00:08:52,690
we have around 1 billion unique products

00:08:50,320 --> 00:08:55,959
and if you consider the implementation

00:08:52,690 --> 00:08:57,880
that I've described somehow here you

00:08:55,959 --> 00:08:58,529
have to have all the embeddings

00:08:57,880 --> 00:09:00,360
corresponding

00:08:58,529 --> 00:09:02,730
to hold the different products in memory

00:09:00,360 --> 00:09:06,060
because you want to be able to update

00:09:02,730 --> 00:09:08,220
the weight pretty fast and if you put

00:09:06,060 --> 00:09:09,810
them in a energy that it will take

00:09:08,220 --> 00:09:12,540
forever again so you have to have

00:09:09,810 --> 00:09:15,779
everything in memory and then for each

00:09:12,540 --> 00:09:17,939
single note so as we add 1 medium nose

00:09:15,779 --> 00:09:20,790
it couldn't fit and we couldn't buy

00:09:17,939 --> 00:09:23,459
machines to be able to have such around

00:09:20,790 --> 00:09:27,480
that it would fit in it so we are the

00:09:23,459 --> 00:09:29,399
program so how did we solve it so we had

00:09:27,480 --> 00:09:31,860
a closer look at this timeline let's go

00:09:29,399 --> 00:09:32,610
back to this one and we hold a here we

00:09:31,860 --> 00:09:35,449
have a user

00:09:32,610 --> 00:09:41,129
he has been to a tech website and then

00:09:35,449 --> 00:09:43,949
es move to a fashion website actually

00:09:41,129 --> 00:09:47,069
does it make sense to be able to compute

00:09:43,949 --> 00:09:50,269
a similarity between this headphone and

00:09:47,069 --> 00:09:53,069
these trousers with what for us this is

00:09:50,269 --> 00:09:55,860
absolutely not redone first I don't know

00:09:53,069 --> 00:10:00,029
exactly what it would mean plus in our

00:09:55,860 --> 00:10:01,829
specific case we don't display head for

00:10:00,029 --> 00:10:03,870
different clients like we decide to

00:10:01,829 --> 00:10:06,059
design out for gap and then we pick

00:10:03,870 --> 00:10:10,410
closes we don't mix headphones and

00:10:06,059 --> 00:10:14,819
clothes so we say oh what if we split

00:10:10,410 --> 00:10:18,029
the user sessions with the different

00:10:14,819 --> 00:10:21,629
clients so we have now for one given

00:10:18,029 --> 00:10:24,059
user we have one timeline for the for

00:10:21,629 --> 00:10:28,740
the closes and one timeline for the for

00:10:24,059 --> 00:10:30,420
the hi-tech we learn embeddings for the

00:10:28,740 --> 00:10:32,639
closes and then we are able to detect

00:10:30,420 --> 00:10:34,319
that t-shirt comes together and then the

00:10:32,639 --> 00:10:37,920
trousers is slightly different and

00:10:34,319 --> 00:10:42,329
something for the headset and by doing

00:10:37,920 --> 00:10:44,730
so then we are we don't have that much

00:10:42,329 --> 00:10:47,429
unique products to process for each note

00:10:44,730 --> 00:10:49,500
leg here this is I don't know let's say

00:10:47,429 --> 00:10:50,670
for fashion we have maximum 10,000

00:10:49,500 --> 00:10:52,679
different products for each of the

00:10:50,670 --> 00:10:56,459
clients and then we can simply fit them

00:10:52,679 --> 00:10:58,800
in memory and process that so we were

00:10:56,459 --> 00:11:02,279
able to process most of our clients

00:10:58,800 --> 00:11:07,139
still there are some clients with a huge

00:11:02,279 --> 00:11:09,750
number of of this tank product and for

00:11:07,139 --> 00:11:12,360
those we for thought that couldn't find

00:11:09,750 --> 00:11:14,429
any solution so for our city

00:11:12,360 --> 00:11:16,290
we simply decided not to drop them and

00:11:14,429 --> 00:11:19,199
not to compute any underlings for them

00:11:16,290 --> 00:11:21,509
so it will it was a strong decision but

00:11:19,199 --> 00:11:23,220
would say it's better to have something

00:11:21,509 --> 00:11:28,559
that not to have any underlings for

00:11:23,220 --> 00:11:30,869
anyone and we also used another trick to

00:11:28,559 --> 00:11:34,980
reduce the number of different products

00:11:30,869 --> 00:11:38,189
so actually we consider the very

00:11:34,980 --> 00:11:40,049
unpopular products like in each

00:11:38,189 --> 00:11:43,949
catalogue especially for the big

00:11:40,049 --> 00:11:46,529
partners for the big website there are

00:11:43,949 --> 00:11:48,809
really bestsellers and then you have the

00:11:46,529 --> 00:11:50,850
rest of the catalogue web we have really

00:11:48,809 --> 00:11:52,589
unpopular products and this popular

00:11:50,850 --> 00:11:54,989
products have two interesting product

00:11:52,589 --> 00:11:56,730
properties for us first there are many

00:11:54,989 --> 00:11:58,889
of them if you look at the longtail then

00:11:56,730 --> 00:12:01,759
they represent actually the longtail

00:11:58,889 --> 00:12:03,959
they represent a lot of distinct product

00:12:01,759 --> 00:12:06,089
secondly we don't think this these are

00:12:03,959 --> 00:12:07,649
really relevant because if they are not

00:12:06,089 --> 00:12:09,299
really popular we don't think it's worth

00:12:07,649 --> 00:12:14,429
recommending them and they won't perform

00:12:09,299 --> 00:12:16,709
well in how has so we decided to drop

00:12:14,429 --> 00:12:18,649
them and so it drastically reduced the

00:12:16,709 --> 00:12:22,860
number of events that we considered and

00:12:18,649 --> 00:12:25,669
made the computation even faster so at

00:12:22,860 --> 00:12:29,069
this point we were able to compute our

00:12:25,669 --> 00:12:31,319
bedding for most of our clients because

00:12:29,069 --> 00:12:34,139
we by removing the very large client we

00:12:31,319 --> 00:12:37,649
remove only less than 1% of our clients

00:12:34,139 --> 00:12:42,809
till we removed lots of the around 40%

00:12:37,649 --> 00:12:44,339
of the of the events and yeah so we were

00:12:42,809 --> 00:12:47,249
able to compute the embedding for most

00:12:44,339 --> 00:12:50,639
of their clients so we well we thought

00:12:47,249 --> 00:12:53,189
we were done and then we realize that in

00:12:50,639 --> 00:12:55,230
the implementation of what track then

00:12:53,189 --> 00:12:57,720
what you have you have plenty of

00:12:55,230 --> 00:13:00,059
parameters to tune you have to tune the

00:12:57,720 --> 00:13:02,100
dimension of the of the embeddings you

00:13:00,059 --> 00:13:03,869
have to attune the number of epochs you

00:13:02,100 --> 00:13:05,819
use like the number of time you go

00:13:03,869 --> 00:13:08,489
through your training data and we have

00:13:05,819 --> 00:13:10,439
no idea on how to tune them like they

00:13:08,489 --> 00:13:11,939
are default but obviously we use

00:13:10,439 --> 00:13:15,480
different kind of data so the default

00:13:11,939 --> 00:13:18,629
would must be a difference so there was

00:13:15,480 --> 00:13:20,249
no metric actually in the implemented in

00:13:18,629 --> 00:13:21,689
the in the water to the Xbox

00:13:20,249 --> 00:13:26,269
implementation so we decided to

00:13:21,689 --> 00:13:29,660
implement two of them the first one

00:13:26,269 --> 00:13:32,929
it's called llh so ll8 stands for

00:13:29,660 --> 00:13:35,240
log-likelihood and actually it's modules

00:13:32,929 --> 00:13:37,189
are where we predict the probabilities

00:13:35,240 --> 00:13:38,569
of the surrounding world so exactly the

00:13:37,189 --> 00:13:41,300
case that I described at the beginning

00:13:38,569 --> 00:13:43,519
so we pick a word and when we ask the

00:13:41,300 --> 00:13:47,569
algorithm to predict the probability of

00:13:43,519 --> 00:13:49,759
the surrounding world and this is

00:13:47,569 --> 00:13:52,699
exactly what's alleged Mercer and this

00:13:49,759 --> 00:13:56,149
is pretty close to what we optimize in

00:13:52,699 --> 00:13:58,279
the in the law during the learning step

00:13:56,149 --> 00:14:00,589
this is actually the matrix actually

00:13:58,279 --> 00:14:02,449
which we optimize and then on the other

00:14:00,589 --> 00:14:05,749
side we have decided to use another

00:14:02,449 --> 00:14:08,839
metric called recall at K here this is a

00:14:05,749 --> 00:14:11,509
measure that somehow Mazur R where will

00:14:08,839 --> 00:14:13,819
you rank the product so or does it work

00:14:11,509 --> 00:14:17,540
we take a user timeline we pick a

00:14:13,819 --> 00:14:19,009
product and then so we pick its

00:14:17,540 --> 00:14:21,699
unveiling and then we consider the K

00:14:19,009 --> 00:14:24,079
nearest neighbors in the embedding space

00:14:21,699 --> 00:14:27,230
we have a set of products and then we

00:14:24,079 --> 00:14:29,389
consider the actual next event in the

00:14:27,230 --> 00:14:32,420
user station and then we try to figure

00:14:29,389 --> 00:14:35,209
out if it is in the K closest products

00:14:32,420 --> 00:14:37,160
or not so if it is indicated closest

00:14:35,209 --> 00:14:39,379
product then we counter one because we

00:14:37,160 --> 00:14:41,600
decided that somehow our ranking is good

00:14:39,379 --> 00:14:43,369
and if not we count to zero and then we

00:14:41,600 --> 00:14:46,100
average stuff so it really measures

00:14:43,369 --> 00:14:48,319
through oh well our underlings are

00:14:46,100 --> 00:14:51,350
useful to run the products and this is

00:14:48,319 --> 00:14:53,509
actually much closer to the finite final

00:14:51,350 --> 00:14:56,360
good that we have which is predicting

00:14:53,509 --> 00:14:59,629
the next product to be able to have

00:14:56,360 --> 00:15:02,209
relevant recommendations so given this

00:14:59,629 --> 00:15:04,610
we have the matrix and then we were able

00:15:02,209 --> 00:15:06,499
to bench some parameters like the number

00:15:04,610 --> 00:15:08,720
of partitions that we use like it also

00:15:06,499 --> 00:15:10,850
has impact on the quality because as as

00:15:08,720 --> 00:15:12,619
I have told you when you may have the

00:15:10,850 --> 00:15:15,259
data on just preach the data then

00:15:12,619 --> 00:15:16,839
somehow you lose some quality the number

00:15:15,259 --> 00:15:19,339
of epochs when we figure out that

00:15:16,839 --> 00:15:22,819
actually one it works for our case was

00:15:19,339 --> 00:15:25,309
enough so we don't have to waste much

00:15:22,819 --> 00:15:28,579
computation power on this the learning

00:15:25,309 --> 00:15:30,829
rate like how well how fast we decreased

00:15:28,579 --> 00:15:33,920
the displacements over the training set

00:15:30,829 --> 00:15:37,069
and also the embedding summation so we

00:15:33,920 --> 00:15:39,649
were able to tune this but at one point

00:15:37,069 --> 00:15:40,130
we Forte okay we have seen these data so

00:15:39,649 --> 00:15:42,440
the

00:15:40,130 --> 00:15:46,520
on the record we have improve them by X

00:15:42,440 --> 00:15:50,810
percent but actually is it really what

00:15:46,520 --> 00:15:54,230
we care about so actually as I told you

00:15:50,810 --> 00:15:56,420
what we care is about generating

00:15:54,230 --> 00:15:58,010
relevant recommendation and how do we

00:15:56,420 --> 00:16:00,860
measure rate that recommendation is for

00:15:58,010 --> 00:16:02,030
clicks or for sales so like we're

00:16:00,860 --> 00:16:03,890
generating with our organization we

00:16:02,030 --> 00:16:05,570
should invent with a user and we see

00:16:03,890 --> 00:16:08,390
whether people click on the product and

00:16:05,570 --> 00:16:12,140
then buy them together buy them and like

00:16:08,390 --> 00:16:14,960
so far we had no idea of our an

00:16:12,140 --> 00:16:18,080
improvement on the llh or the recoil at

00:16:14,960 --> 00:16:20,990
K was related to the actual clicks or

00:16:18,080 --> 00:16:24,770
the actual sales so what we did is we

00:16:20,990 --> 00:16:28,250
took the opportunity of one of our ad

00:16:24,770 --> 00:16:31,250
castle online a/b tests to actually

00:16:28,250 --> 00:16:33,560
measure for each of our clients the

00:16:31,250 --> 00:16:36,410
uplift in terms of click and in term of

00:16:33,560 --> 00:16:38,630
sales and compare them to the quality of

00:16:36,410 --> 00:16:41,510
the embed to the of the related

00:16:38,630 --> 00:16:43,850
embeddings like the LH precision at k

00:16:41,510 --> 00:16:48,340
and we plot the correlation and we saw

00:16:43,850 --> 00:16:51,650
that very a coalition between them so

00:16:48,340 --> 00:16:53,450
it's still kind of fuzzy we are in the

00:16:51,650 --> 00:16:58,670
process of trying to refine them and

00:16:53,450 --> 00:17:00,320
we'll refine the correlation over time

00:16:58,670 --> 00:17:03,230
we'll see if they are really correct and

00:17:00,320 --> 00:17:06,010
it's still too early to decide which one

00:17:03,230 --> 00:17:08,720
of the true metric is the best for the

00:17:06,010 --> 00:17:12,140
online test so we have decided to keep

00:17:08,720 --> 00:17:13,430
both for right now but over the time we

00:17:12,140 --> 00:17:16,189
are quite confident that we'll be able

00:17:13,430 --> 00:17:18,350
to tune our offline matrix so that they

00:17:16,189 --> 00:17:21,470
correct well with the online matrix and

00:17:18,350 --> 00:17:24,470
then we can do much more offline work

00:17:21,470 --> 00:17:28,040
and which costs less than an actual a B

00:17:24,470 --> 00:17:30,890
test so so far so good we are able to

00:17:28,040 --> 00:17:32,960
tune our parameters we had a nice

00:17:30,890 --> 00:17:35,480
implementation then we add a nice

00:17:32,960 --> 00:17:39,260
distributed algorithm plus a use cursor

00:17:35,480 --> 00:17:41,120
so well we thought we were done and then

00:17:39,260 --> 00:17:43,790
we add these embeddings and well

00:17:41,120 --> 00:17:46,640
everything was fine and this is when

00:17:43,790 --> 00:17:48,020
something kind of surprising Adams like

00:17:46,640 --> 00:17:50,600
we are good algorithm good

00:17:48,020 --> 00:17:53,720
infrastructure etc it's like the data

00:17:50,600 --> 00:17:56,240
betrayed us like we

00:17:53,720 --> 00:17:58,490
I was well everything was running fine

00:17:56,240 --> 00:18:01,190
and I was working on the next iteration

00:17:58,490 --> 00:18:04,580
of the embeddings and the idea there was

00:18:01,190 --> 00:18:07,220
to try to improve them by combining them

00:18:04,580 --> 00:18:09,169
with the description of the product so

00:18:07,220 --> 00:18:11,240
there was our some of our researchers

00:18:09,169 --> 00:18:13,970
are done some research like you can have

00:18:11,240 --> 00:18:15,919
the user stations and you can also have

00:18:13,970 --> 00:18:17,720
mix them with the description of the

00:18:15,919 --> 00:18:19,909
products and try to improve the

00:18:17,720 --> 00:18:23,000
relevance of the embedding so I was

00:18:19,909 --> 00:18:25,010
trying to implement this unless I notice

00:18:23,000 --> 00:18:29,299
that I couldn't reproduce the uplift

00:18:25,010 --> 00:18:32,780
that our researchers add on their own

00:18:29,299 --> 00:18:35,960
data sets so I tried to dig it too a

00:18:32,780 --> 00:18:39,020
little bit and try to understand what

00:18:35,960 --> 00:18:40,909
was happening and at what point I figure

00:18:39,020 --> 00:18:44,620
out that for one specific client I

00:18:40,909 --> 00:18:47,990
decided to plot the evolution of the lnh

00:18:44,620 --> 00:18:49,909
with the number of samples that we use

00:18:47,990 --> 00:18:53,090
in our training sets and this is what I

00:18:49,909 --> 00:18:56,510
got so here this is the learning process

00:18:53,090 --> 00:18:58,929
and here this is the llh so here we see

00:18:56,510 --> 00:19:01,909
that it increases and then at some point

00:18:58,929 --> 00:19:05,390
then it decreases and this is completely

00:19:01,909 --> 00:19:07,460
unexpected so as we are trying to

00:19:05,390 --> 00:19:09,820
optimize for the llh we would have

00:19:07,460 --> 00:19:13,010
expected like trouble really smooth

00:19:09,820 --> 00:19:15,260
curve going up that and then I think two

00:19:13,010 --> 00:19:18,230
things or slightly increasing till then

00:19:15,260 --> 00:19:21,049
so while this is was really surprising

00:19:18,230 --> 00:19:24,200
and then I checked for over website and

00:19:21,049 --> 00:19:28,460
I figure out that add many different

00:19:24,200 --> 00:19:30,909
clients having the same behavior so when

00:19:28,460 --> 00:19:34,039
I had a closer look at the data and

00:19:30,909 --> 00:19:36,980
realize that actually the data that we

00:19:34,039 --> 00:19:41,330
use visualisations is actually very

00:19:36,980 --> 00:19:43,970
different from the NLP data that it's

00:19:41,330 --> 00:19:46,970
usually used with well Trebek for

00:19:43,970 --> 00:19:50,299
instance well you have much less variety

00:19:46,970 --> 00:19:53,390
in each user station that you have in a

00:19:50,299 --> 00:19:54,559
sentence like usually you have a someone

00:19:53,390 --> 00:19:57,169
looking for I don't know

00:19:54,559 --> 00:19:59,630
red red closes and then it looked for

00:19:57,169 --> 00:20:02,210
red t-shirts and then for red trousers

00:19:59,630 --> 00:20:04,789
but you only have two shirts and

00:20:02,210 --> 00:20:07,070
trousers in these user sessions like

00:20:04,789 --> 00:20:09,230
whereas for NLP you have much more

00:20:07,070 --> 00:20:10,700
via tea like you have in each sentence

00:20:09,230 --> 00:20:14,779
you have a word you have some known some

00:20:10,700 --> 00:20:19,429
adverbs you clitoral and because of this

00:20:14,779 --> 00:20:22,009
like imagine that we had a user fan of

00:20:19,429 --> 00:20:27,169
Red Cloud so he has a user session and

00:20:22,009 --> 00:20:28,940
then we pick this user session first so

00:20:27,169 --> 00:20:31,669
we are trying to we are going to group

00:20:28,940 --> 00:20:33,889
read items together and then we are

00:20:31,669 --> 00:20:38,690
going to process the data over and over

00:20:33,889 --> 00:20:40,850
and then a new new timeline and as I

00:20:38,690 --> 00:20:43,309
have told you we have decreasing schemes

00:20:40,850 --> 00:20:45,799
like over the time we decrease the size

00:20:43,309 --> 00:20:47,750
of the update so it means that we have

00:20:45,799 --> 00:20:50,480
we will have grouped the red items

00:20:47,750 --> 00:20:54,379
together and then it will be somehow too

00:20:50,480 --> 00:20:58,100
late and group them with the time

00:20:54,379 --> 00:21:00,320
because of the the learnt representation

00:20:58,100 --> 00:21:02,090
is actually really dependent on the

00:21:00,320 --> 00:21:05,629
order of the timelines that we that we

00:21:02,090 --> 00:21:09,679
had so we will somehow overfitting for

00:21:05,629 --> 00:21:12,769
the first data and then for the data

00:21:09,679 --> 00:21:15,350
afterward it wasn't the case so we

00:21:12,769 --> 00:21:17,840
thought ok we think we have this issue

00:21:15,350 --> 00:21:19,639
how can we settle this like the whole

00:21:17,840 --> 00:21:23,379
program here is the order in which we

00:21:19,639 --> 00:21:26,330
process the data so we thought hey are

00:21:23,379 --> 00:21:28,789
we should shuffle the data somehow the

00:21:26,330 --> 00:21:30,830
point is like the way the algorithm

00:21:28,789 --> 00:21:35,210
worked like what the way I've described

00:21:30,830 --> 00:21:37,429
them is like we split our sentences we

00:21:35,210 --> 00:21:39,470
shall fall them across the network and

00:21:37,429 --> 00:21:41,509
across the nodes when we process each

00:21:39,470 --> 00:21:46,029
sentence and then we generate the pairs

00:21:41,509 --> 00:21:48,320
like the tails of products like the the

00:21:46,029 --> 00:21:51,409
the some short words and then with the

00:21:48,320 --> 00:21:53,990
word for which we try to predict the the

00:21:51,409 --> 00:21:56,059
one we try to predict and then we feed

00:21:53,990 --> 00:21:59,990
work to back with us with we thought hey

00:21:56,059 --> 00:22:03,559
we could actually have all the user

00:21:59,990 --> 00:22:05,929
timelines generate the tails shut of

00:22:03,559 --> 00:22:08,059
them and then feed what you like with it

00:22:05,929 --> 00:22:10,250
and this way we'll be sure that the

00:22:08,059 --> 00:22:12,230
pairs are completely run or shuffled and

00:22:10,250 --> 00:22:14,870
then for the the pairs denoted by the

00:22:12,230 --> 00:22:16,760
red closes fan then if it will be spread

00:22:14,870 --> 00:22:18,440
over the whole data set and then we

00:22:16,760 --> 00:22:20,190
won't have this overfitting scheme and

00:22:18,440 --> 00:22:24,629
we did we did

00:22:20,190 --> 00:22:28,409
and this is actually what we obtained so

00:22:24,629 --> 00:22:30,210
this is a much better looking shape this

00:22:28,409 --> 00:22:32,399
is much closer to what we are expected

00:22:30,210 --> 00:22:35,820
is the while this is an actual curve and

00:22:32,399 --> 00:22:39,419
it looks like a notebook curve actually

00:22:35,820 --> 00:22:41,580
and so this is the quality increase and

00:22:39,419 --> 00:22:43,980
when we checked it on different clients

00:22:41,580 --> 00:22:46,039
and it was fixing everything so one for

00:22:43,980 --> 00:22:49,139
a which we what we found the solution

00:22:46,039 --> 00:22:52,529
the point like here is that it takes

00:22:49,139 --> 00:22:55,710
almost six time or longer than for the

00:22:52,529 --> 00:22:58,799
previous iteration and why so it's

00:22:55,710 --> 00:23:02,159
because we generate the pairs before the

00:22:58,799 --> 00:23:03,899
shuffling step and actually when you

00:23:02,159 --> 00:23:05,730
generate the path length you add some

00:23:03,899 --> 00:23:08,129
rather than see it generates much more

00:23:05,730 --> 00:23:13,019
volumes and it means that you have much

00:23:08,129 --> 00:23:15,450
more data to shuffle and so we couldn't

00:23:13,019 --> 00:23:17,610
afford it so even if the quality was

00:23:15,450 --> 00:23:21,779
better we had to find another solution

00:23:17,610 --> 00:23:23,850
and then we thought okay we know what

00:23:21,779 --> 00:23:26,340
the problem is we know that it comes

00:23:23,850 --> 00:23:27,690
from the ordering of the pairs but we

00:23:26,340 --> 00:23:30,120
know that we cannot generate the

00:23:27,690 --> 00:23:32,389
bell--the upstream so I thought what

00:23:30,120 --> 00:23:37,620
about mixing the two schemes actually

00:23:32,389 --> 00:23:40,470
what we can do is we can split the data

00:23:37,620 --> 00:23:43,769
like the way we did before or shuffle

00:23:40,470 --> 00:23:46,559
them but here instead of processing all

00:23:43,769 --> 00:23:50,009
the sentences one after the other in

00:23:46,559 --> 00:23:53,480
inside one partition we could read a

00:23:50,009 --> 00:23:56,490
batch of data a batch of user sessions

00:23:53,480 --> 00:23:59,039
generate the pairs for them shuffle them

00:23:56,490 --> 00:24:03,389
in memory and then feed water back with

00:23:59,039 --> 00:24:06,330
this update our ways and then use

00:24:03,389 --> 00:24:08,370
another batch of data etc etc this way

00:24:06,330 --> 00:24:11,940
we could have reduced the correlation

00:24:08,370 --> 00:24:15,419
between the between the consecutive

00:24:11,940 --> 00:24:18,659
pairs so this is what we did and this is

00:24:15,419 --> 00:24:20,730
what we obtained like because you see

00:24:18,659 --> 00:24:23,789
much better looking so we have a huge

00:24:20,730 --> 00:24:25,980
increase then we all we have an

00:24:23,789 --> 00:24:28,019
asymptote and then which in the maximum

00:24:25,980 --> 00:24:30,179
and if you look at look at the absolute

00:24:28,019 --> 00:24:32,920
value then this is likely the quality

00:24:30,179 --> 00:24:37,600
slightly lower for the previous case but

00:24:32,920 --> 00:24:40,600
which is okay and what we had a small

00:24:37,600 --> 00:24:42,340
decrease and here what's also

00:24:40,600 --> 00:24:45,430
interesting is like you see these bumps

00:24:42,340 --> 00:24:47,530
this actually corresponds to the time

00:24:45,430 --> 00:24:49,030
when we change the batch so we we

00:24:47,530 --> 00:24:52,030
generate a new batch and then we

00:24:49,030 --> 00:24:55,540
decrease etc and then this is nice to to

00:24:52,030 --> 00:24:57,610
series still the quality was not optimal

00:24:55,540 --> 00:24:59,560
and we thought we could do better so

00:24:57,610 --> 00:25:02,110
what we did is what we bench this

00:24:59,560 --> 00:25:05,800
parameter about all the size of the

00:25:02,110 --> 00:25:07,660
batches but we had to use and then we

00:25:05,800 --> 00:25:10,810
thought that we could use what the

00:25:07,660 --> 00:25:12,490
larger the batches the the motion of

00:25:10,810 --> 00:25:13,960
thing you do but then you have a limit

00:25:12,490 --> 00:25:16,480
about the memory that you use because

00:25:13,960 --> 00:25:19,540
you cannot show fall all the other

00:25:16,480 --> 00:25:23,590
partition so we thought a nice trade-off

00:25:19,540 --> 00:25:26,230
and this is what we obtained like here

00:25:23,590 --> 00:25:28,690
we have only one to two and cells

00:25:26,230 --> 00:25:32,470
matches and this is really close to the

00:25:28,690 --> 00:25:34,030
ideal to the ideal case that we have

00:25:32,470 --> 00:25:36,130
seen before and if you look at the

00:25:34,030 --> 00:25:40,840
quality this is we have a minimal drop

00:25:36,130 --> 00:25:43,540
of quality or when you do this so yeah

00:25:40,840 --> 00:25:45,850
so far we had solve this issue and we

00:25:43,540 --> 00:25:48,160
checked on several clients and the

00:25:45,850 --> 00:25:50,610
problem was fixed for most of them so

00:25:48,160 --> 00:25:53,530
this is where we stand now so we have

00:25:50,610 --> 00:25:55,180
this into production we have abt

00:25:53,530 --> 00:25:59,560
students we are really promising with

00:25:55,180 --> 00:26:01,510
also now and what I wanted to share with

00:25:59,560 --> 00:26:06,420
you here is a few lessons that I

00:26:01,510 --> 00:26:06,420
consider important in this experiment

00:26:07,020 --> 00:26:12,580
the first lesson is that you don't have

00:26:10,270 --> 00:26:14,710
you don't have to fear traders trade

00:26:12,580 --> 00:26:16,840
offs actually you have to embrace them

00:26:14,710 --> 00:26:18,790
like here in this situation there are

00:26:16,840 --> 00:26:20,860
many in this use case there are many

00:26:18,790 --> 00:26:23,680
situation where we had to do trade offs

00:26:20,860 --> 00:26:26,320
like we had to drop some of our clients

00:26:23,680 --> 00:26:29,230
to make the whole thing works we are to

00:26:26,320 --> 00:26:30,370
trade the quality versus the the

00:26:29,230 --> 00:26:33,280
learning time to have something

00:26:30,370 --> 00:26:35,530
acceptable etc and this is okay when you

00:26:33,280 --> 00:26:39,880
deal with such large amount of data then

00:26:35,530 --> 00:26:42,700
it's somehow okay to remove that of it

00:26:39,880 --> 00:26:44,560
or to skew slightly the data to get some

00:26:42,700 --> 00:26:46,110
result is it's not ideal but something

00:26:44,560 --> 00:26:48,520
is better than no

00:26:46,110 --> 00:26:50,380
the second point that I would like to

00:26:48,520 --> 00:26:53,220
share with you it like you have to

00:26:50,380 --> 00:26:58,330
measure everything like you have to have

00:26:53,220 --> 00:27:00,310
relevant metrics and these are these

00:26:58,330 --> 00:27:03,460
metrics that allow you to do trade-offs

00:27:00,310 --> 00:27:05,380
if you try to change the parameters

00:27:03,460 --> 00:27:06,880
without adding any quality metrics or

00:27:05,380 --> 00:27:08,650
performance metric then you don't do

00:27:06,880 --> 00:27:10,540
trade off you're going blindly and

00:27:08,650 --> 00:27:14,410
you're modifying your algorithm blindly

00:27:10,540 --> 00:27:16,030
and one thing which is important to here

00:27:14,410 --> 00:27:17,890
to notice also is that you also have to

00:27:16,030 --> 00:27:20,530
challenge your metrics like you have to

00:27:17,890 --> 00:27:22,810
consider different are different ones

00:27:20,530 --> 00:27:24,400
and then to try to understand what we

00:27:22,810 --> 00:27:26,410
capture well and that what we don't

00:27:24,400 --> 00:27:30,730
capture well and then you also have to

00:27:26,410 --> 00:27:32,800
ask yourself are they really correlated

00:27:30,730 --> 00:27:35,920
or related to the final goal that I'm

00:27:32,800 --> 00:27:38,590
trying to achieve so this was the second

00:27:35,920 --> 00:27:39,820
lesson and the third lesson that I want

00:27:38,590 --> 00:27:41,800
to share with you is like you have to

00:27:39,820 --> 00:27:43,390
play with your data like even if you

00:27:41,800 --> 00:27:46,300
have very good metrics and then you have

00:27:43,390 --> 00:27:49,660
processed everything nice and sweet then

00:27:46,300 --> 00:27:51,850
at one point where you are both a new

00:27:49,660 --> 00:27:56,800
issue within your algorithm they are

00:27:51,850 --> 00:27:58,480
always both and flow data etc and you if

00:27:56,800 --> 00:28:01,450
you always use the same matrix and

00:27:58,480 --> 00:28:04,210
you'll notable to find them so a nice

00:28:01,450 --> 00:28:05,800
way to find them is like trying to

00:28:04,210 --> 00:28:07,930
manipulate your data in different ways

00:28:05,800 --> 00:28:10,900
try try to use them for different use

00:28:07,930 --> 00:28:13,270
cases implement new algorithm and by

00:28:10,900 --> 00:28:15,160
doing this you will have yourself new

00:28:13,270 --> 00:28:17,650
questions and then you'll be able to

00:28:15,160 --> 00:28:20,670
find flows into your algorithm and then

00:28:17,650 --> 00:28:23,860
you'll be able to improve yourself on it

00:28:20,670 --> 00:28:27,100
so what was the lesson the lesson that I

00:28:23,860 --> 00:28:29,680
wanted to share with you and that's what

00:28:27,100 --> 00:28:31,870
I asked for today so I want to mention

00:28:29,680 --> 00:28:33,310
that well if you want to work with large

00:28:31,870 --> 00:28:35,950
amounts of data a machine learning etc

00:28:33,310 --> 00:28:39,570
then we I ring if you want to have some

00:28:35,950 --> 00:28:41,890
questions well please ask me offline and

00:28:39,570 --> 00:28:44,550
thank you for your attention if you have

00:28:41,890 --> 00:28:44,550
questions them

00:28:48,750 --> 00:29:00,039
thank you for the talk so we don't have

00:28:51,490 --> 00:29:01,780
time for questions anyone one of the

00:29:00,039 --> 00:29:03,760
things for the talk by the way first

00:29:01,780 --> 00:29:05,289
thing it was really interesting one of

00:29:03,760 --> 00:29:06,910
the things you mentioned and I'm I've

00:29:05,289 --> 00:29:09,700
been wondering about is that you have to

00:29:06,910 --> 00:29:14,039
train with this large amount of data and

00:29:09,700 --> 00:29:14,039
I'm wondering have you tried sampling

00:29:15,510 --> 00:29:20,919
not that I know well I was not the only

00:29:18,130 --> 00:29:23,590
one doing some experiments but maybe we

00:29:20,919 --> 00:29:25,870
did the implemented inception that we

00:29:23,590 --> 00:29:28,510
take from our stuff that we do that

00:29:25,870 --> 00:29:30,130
doing sampling usually reduce the

00:29:28,510 --> 00:29:32,380
quality reduce the quality of the

00:29:30,130 --> 00:29:35,710
mailings but I don't think we have done

00:29:32,380 --> 00:29:37,390
it on our actual letter but from all the

00:29:35,710 --> 00:29:39,850
stuff that we do predictive stuff that

00:29:37,390 --> 00:29:42,880
we do we have learned that sampling is

00:29:39,850 --> 00:29:44,830
often not that good and we didn't try it

00:29:42,880 --> 00:29:49,360
on this to to the best of my knowledge

00:29:44,830 --> 00:29:59,260
but could be an idea to speed up other

00:29:49,360 --> 00:30:01,960
questions all the data stored so yeah so

00:29:59,260 --> 00:30:04,330
you mentioned that you have the initial

00:30:01,960 --> 00:30:06,039
set that you change the system on so

00:30:04,330 --> 00:30:09,159
what happens if you want to add to the

00:30:06,039 --> 00:30:10,990
set if you if I want sorry if you want

00:30:09,159 --> 00:30:14,080
to add to the set so if you want to add

00:30:10,990 --> 00:30:17,679
data to for the set to be to operate on

00:30:14,080 --> 00:30:20,820
larger volume of initial data other can

00:30:17,679 --> 00:30:23,080
you add that after the initial

00:30:20,820 --> 00:30:25,020
unlearning is done well that's a good

00:30:23,080 --> 00:30:28,140
question something I forgot to mention

00:30:25,020 --> 00:30:31,030
it's like we do this training every day

00:30:28,140 --> 00:30:32,380
so like to be able to be to a fresh data

00:30:31,030 --> 00:30:33,760
and to have written ten bathing's

00:30:32,380 --> 00:30:35,110
because like you know the correlation

00:30:33,760 --> 00:30:37,289
between the products the change over

00:30:35,110 --> 00:30:40,000
time sometimes two products get really

00:30:37,289 --> 00:30:41,409
correlated for one reason or one over so

00:30:40,000 --> 00:30:44,049
but something also that we learnt that

00:30:41,409 --> 00:30:47,890
we have to refresh to refresh it over

00:30:44,049 --> 00:30:49,990
the time so this is the easy way to add

00:30:47,890 --> 00:30:53,320
vet to add some new data to tour that

00:30:49,990 --> 00:30:55,390
asset still the question that is behind

00:30:53,320 --> 00:30:57,580
I think well one interesting questions

00:30:55,390 --> 00:30:59,680
like we retrain it and then the point is

00:30:57,580 --> 00:31:01,990
like when we retain the data then the

00:30:59,680 --> 00:31:04,930
initial position are purely random also

00:31:01,990 --> 00:31:07,270
so the final configuration of the

00:31:04,930 --> 00:31:09,550
embeddings from one day to another may

00:31:07,270 --> 00:31:11,260
change drastically the assumption that

00:31:09,550 --> 00:31:15,660
we've made and we have to check at one

00:31:11,260 --> 00:31:17,440
point like we we we even if the

00:31:15,660 --> 00:31:19,090
distribution of the embeddings in the

00:31:17,440 --> 00:31:21,490
space changes we hope that the

00:31:19,090 --> 00:31:24,580
similarities computed between them won't

00:31:21,490 --> 00:31:26,440
change much so this could be an ID

00:31:24,580 --> 00:31:29,860
something we want to ask for trying to

00:31:26,440 --> 00:31:35,200
seed the training sets like trying to

00:31:29,860 --> 00:31:37,060
inside our in instead of spread

00:31:35,200 --> 00:31:39,070
embedding randomly at the very beginning

00:31:37,060 --> 00:31:45,970
that use the accurate and Eddings and

00:31:39,070 --> 00:31:48,670
try to update the weights this way so

00:31:45,970 --> 00:31:50,800
these videos model averaging after all

00:31:48,670 --> 00:31:53,470
the splitting and you said you have

00:31:50,800 --> 00:31:55,750
different works vector kind of domain or

00:31:53,470 --> 00:31:56,110
category of subcategory of your products

00:31:55,750 --> 00:31:59,680
right

00:31:56,110 --> 00:32:01,720
do you use model averaging for like a

00:31:59,680 --> 00:32:03,760
user session so the new user session

00:32:01,720 --> 00:32:06,340
comes in and we have like a manager

00:32:03,760 --> 00:32:08,350
network that pcap the right word too

00:32:06,340 --> 00:32:09,970
vague or you combine it afterwards

00:32:08,350 --> 00:32:12,250
how does work well the combination is

00:32:09,970 --> 00:32:14,830
shortly purely averaging because we have

00:32:12,250 --> 00:32:16,450
not found anything better for now but if

00:32:14,830 --> 00:32:18,550
you have any suggestions and you love

00:32:16,450 --> 00:32:20,520
and welcome but yeah we know that we we

00:32:18,550 --> 00:32:23,200
have a use of quality I haven't not

00:32:20,520 --> 00:32:25,090
plotted here that at the end you have

00:32:23,200 --> 00:32:26,950
the of the curves and I have floated

00:32:25,090 --> 00:32:29,590
like you have the merging step and then

00:32:26,950 --> 00:32:31,510
you see big drop and then well it's rich

00:32:29,590 --> 00:32:33,940
with so we know that we lose some quite

00:32:31,510 --> 00:32:38,320
here using this it's the best solution

00:32:33,940 --> 00:32:44,500
that we have found so far yes maybe the

00:32:38,320 --> 00:32:47,170
radical thank you for the call and the

00:32:44,500 --> 00:32:50,950
talk is a very good person okay thank

00:32:47,170 --> 00:32:53,080
you very talk can you tell us about the

00:32:50,950 --> 00:32:54,550
time scale how much time is from one

00:32:53,080 --> 00:32:58,090
experiment to the next one to the next

00:32:54,550 --> 00:33:01,960
one is months years days are for our

00:32:58,090 --> 00:33:05,200
experiments yeah well the whole projects

00:33:01,960 --> 00:33:08,020
I think it started one year ago so I

00:33:05,200 --> 00:33:10,960
think this is somehow a Content view on

00:33:08,020 --> 00:33:13,430
what we did so for example for the

00:33:10,960 --> 00:33:16,880
convergence issue like it took us

00:33:13,430 --> 00:33:19,040
well I will say from one to two months

00:33:16,880 --> 00:33:21,200
is to actually figure out what to do but

00:33:19,040 --> 00:33:23,809
you got the trickiest one and then

00:33:21,200 --> 00:33:25,430
usually we try to what to do this on the

00:33:23,809 --> 00:33:28,190
spring basis like we use creme select

00:33:25,430 --> 00:33:30,440
two weeks - oh for - so those are the

00:33:28,190 --> 00:33:32,330
issues and this is assuming which this

00:33:30,440 --> 00:33:34,100
works and this is helped by the fact

00:33:32,330 --> 00:33:36,050
that we have this really are closer also

00:33:34,100 --> 00:33:38,809
we are able to learn the embeddings oh

00:33:36,050 --> 00:33:40,580
well in a matter of hours so we can

00:33:38,809 --> 00:33:42,200
launch multiple experiments at the same

00:33:40,580 --> 00:33:55,040
time to get the result the day after and

00:33:42,200 --> 00:33:57,890
then compare the results so you said

00:33:55,040 --> 00:34:00,110
it's like 300 billion words so what

00:33:57,890 --> 00:34:03,190
would you train on it correct or is it

00:34:00,110 --> 00:34:06,740
we know it was our events yeah like us

00:34:03,190 --> 00:34:08,600
120 million sorry did you normalize here

00:34:06,740 --> 00:34:10,970
your input data so for example your

00:34:08,600 --> 00:34:14,149
friends so that would be excellent did

00:34:10,970 --> 00:34:15,800
you remove it and clean it sorry when

00:34:14,149 --> 00:34:17,600
you have accent like it's a different

00:34:15,800 --> 00:34:19,970
word you would generate a different

00:34:17,600 --> 00:34:22,550
embedding for for a word of accent or

00:34:19,970 --> 00:34:25,129
boom load no actually what we process

00:34:22,550 --> 00:34:27,110
here is like we process the product IDs

00:34:25,129 --> 00:34:28,220
so we do like at the beginning I

00:34:27,110 --> 00:34:31,340
describe route to that which actually

00:34:28,220 --> 00:34:33,230
works with worth button here now we do

00:34:31,340 --> 00:34:36,139
this analogy would say a product is a

00:34:33,230 --> 00:34:37,760
word so we only have one ID okay so this

00:34:36,139 --> 00:34:40,070
is option UNIX we don't have this

00:34:37,760 --> 00:34:42,290
normalization issue sorry I missed it

00:34:40,070 --> 00:34:44,600
but did you did you also try past text

00:34:42,290 --> 00:34:46,220
from Facebook I know we didn't I have

00:34:44,600 --> 00:34:47,840
heard about it there are times that's

00:34:46,220 --> 00:34:50,149
pretty fast I trained it with a few

00:34:47,840 --> 00:34:52,669
hundred thousand comments and on my

00:34:50,149 --> 00:34:55,850
laptop in a few like half an hour of

00:34:52,669 --> 00:34:57,380
your books okay so yeah no we I've got

00:34:55,850 --> 00:35:02,840
civil time about this this is definitely

00:34:57,380 --> 00:35:05,380
something we should try any other

00:35:02,840 --> 00:35:05,380
question

00:35:07,610 --> 00:35:14,960
oh that was it

00:35:12,590 --> 00:35:22,619
Thank You salmon was a really good talk

00:35:14,960 --> 00:35:22,619

YouTube URL: https://www.youtube.com/watch?v=qYpdW9cyEqY


