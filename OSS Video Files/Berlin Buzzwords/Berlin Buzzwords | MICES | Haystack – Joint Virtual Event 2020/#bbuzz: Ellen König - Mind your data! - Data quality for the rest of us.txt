Title: #bbuzz: Ellen König - Mind your data! - Data quality for the rest of us
Publication date: 2020-06-24
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/mind-your-data-data-quality-rest-us

As software engineers, we take pride in our code quality. As data scientists, in the quality of our models and analyses. As a data engineer, I take pride in the quality of the datasets I provide access to.

Everyone in IT works with data one way or another, be it producing, managing or using it. Yet like water for fish, we often fail to notice data because it is all around us. And, just like fish in the water suffer bad water quality, we suffer if our data quality decreases. Unlike the fish in the water though, we can actually all contribute to addressing data quality issues.

In my talk, I want to encourage you to become more aware of data quality concerns. I will discuss what data quality is, how we can identify data quality issues and some strategies for addressing them. As a practical example, I will share our experiences with monitoring data quality using Amazon Research's deequ framework.
Captions: 
	00:00:08,080 --> 00:00:12,719
hello and uh

00:00:09,120 --> 00:00:16,320
welcome to the myself live stream um

00:00:12,719 --> 00:00:21,439
we have um ellen kernish up to give the

00:00:16,320 --> 00:00:21,439
next talk um ellen

00:00:22,320 --> 00:00:25,519
is the senior data engineer at

00:00:24,160 --> 00:00:28,960
thoughtworks

00:00:25,519 --> 00:00:31,519
um and we'll be giving a talk on

00:00:28,960 --> 00:00:32,480
data quality the title is mind your data

00:00:31,519 --> 00:00:36,000
data quality

00:00:32,480 --> 00:00:36,559
for the rest of us let me just introduce

00:00:36,000 --> 00:00:41,280
ellen

00:00:36,559 --> 00:00:44,000
thank you very much cool so

00:00:41,280 --> 00:00:45,440
whenever i talk about data and data

00:00:44,000 --> 00:00:47,200
quality with the engineers

00:00:45,440 --> 00:00:48,879
i sometimes feel a bit like the fish in

00:00:47,200 --> 00:00:50,239
this this kind of story

00:00:48,879 --> 00:00:51,840
which you may be familiar with so

00:00:50,239 --> 00:00:53,600
there's an older fish meeting a younger

00:00:51,840 --> 00:00:55,760
fish in the water

00:00:53,600 --> 00:00:57,520
and the older fish is asking the younger

00:00:55,760 --> 00:00:59,199
fish what is water

00:00:57,520 --> 00:01:01,120
and the younger fish is very confused by

00:00:59,199 --> 00:01:02,559
this question he's asking

00:01:01,120 --> 00:01:04,239
no sorry the older fish is asking the

00:01:02,559 --> 00:01:06,000
younger fish how is the water today

00:01:04,239 --> 00:01:07,360
and the younger fish is very confused by

00:01:06,000 --> 00:01:09,600
this this question is

00:01:07,360 --> 00:01:10,560
asking like what is water and sometimes

00:01:09,600 --> 00:01:12,080
especially when we

00:01:10,560 --> 00:01:14,000
for us that work a lot of data i

00:01:12,080 --> 00:01:16,240
sometimes have the same feeling

00:01:14,000 --> 00:01:18,080
that we also so immersed in our data and

00:01:16,240 --> 00:01:20,479
our technology our software

00:01:18,080 --> 00:01:22,240
that we're not really always very

00:01:20,479 --> 00:01:23,680
precisely clear about what we're exactly

00:01:22,240 --> 00:01:25,200
dealing with and i think for especially

00:01:23,680 --> 00:01:26,640
when we talk about data quality it's

00:01:25,200 --> 00:01:28,720
really useful to have a

00:01:26,640 --> 00:01:30,240
very precise understanding of what data

00:01:28,720 --> 00:01:31,680
is that's why

00:01:30,240 --> 00:01:33,360
i would like to zoom out in the

00:01:31,680 --> 00:01:33,759
beginning of my talk a little bit and

00:01:33,360 --> 00:01:36,400
just

00:01:33,759 --> 00:01:38,799
clarify for a second what can how can we

00:01:36,400 --> 00:01:41,759
understand data quality

00:01:38,799 --> 00:01:43,200
and so if we look at data up in the

00:01:41,759 --> 00:01:44,960
dictionary the interesting thing is

00:01:43,200 --> 00:01:47,520
there are two definitions

00:01:44,960 --> 00:01:48,960
of data one is information is specially

00:01:47,520 --> 00:01:50,720
 on numbers

00:01:48,960 --> 00:01:53,280
collected to be examined and considered

00:01:50,720 --> 00:01:55,200
and used to help with making decisions

00:01:53,280 --> 00:01:57,040
for example the data shows that more

00:01:55,200 --> 00:01:58,960
than 80 percent of the agricultural

00:01:57,040 --> 00:02:00,240
workforce is hispanic

00:01:58,960 --> 00:02:02,159
on the other hand there's also a

00:02:00,240 --> 00:02:04,240
technical definition

00:02:02,159 --> 00:02:06,000
and that's information in an electronic

00:02:04,240 --> 00:02:07,680
form that can be stored and processed by

00:02:06,000 --> 00:02:09,440
a computer for example

00:02:07,680 --> 00:02:11,920
the student's task was to prepare all

00:02:09,440 --> 00:02:14,480
the posters and electronic data

00:02:11,920 --> 00:02:15,840
for the publicity campaign and what i

00:02:14,480 --> 00:02:17,599
found really interesting when i looked

00:02:15,840 --> 00:02:19,440
this up was that these things are really

00:02:17,599 --> 00:02:21,200
two sides of the same code it's not one

00:02:19,440 --> 00:02:22,239
is more correct or one only applies in

00:02:21,200 --> 00:02:24,160
one circumstance

00:02:22,239 --> 00:02:26,319
but whenever we talk about data it's

00:02:24,160 --> 00:02:28,239
really both things that are relevant and

00:02:26,319 --> 00:02:30,160
while we as engineers we tend to look at

00:02:28,239 --> 00:02:31,920
data more in the sense of we can restore

00:02:30,160 --> 00:02:33,519
it and we process and that's all

00:02:31,920 --> 00:02:35,440
the whole team of this conflict is like

00:02:33,519 --> 00:02:36,640
throwing and scaling and streaming

00:02:35,440 --> 00:02:38,239
so it's all about this technical

00:02:36,640 --> 00:02:40,480
definition of data but when we talk

00:02:38,239 --> 00:02:42,239
about data quality it's really

00:02:40,480 --> 00:02:43,840
i find it really useful to keep in mind

00:02:42,239 --> 00:02:46,000
this other more business oriented

00:02:43,840 --> 00:02:47,840
definition of data which is

00:02:46,000 --> 00:02:49,200
data is about making decisions and it's

00:02:47,840 --> 00:02:51,360
helping us making these

00:02:49,200 --> 00:02:53,040
decisions and it's really both at the

00:02:51,360 --> 00:02:56,080
same time

00:02:53,040 --> 00:02:57,360
and so just like when about data there's

00:02:56,080 --> 00:02:59,440
different ways to look at it

00:02:57,360 --> 00:03:01,760
also if you look at data quality there's

00:02:59,440 --> 00:03:04,560
different ways to look at it

00:03:01,760 --> 00:03:05,519
so and so really what data quality is

00:03:04,560 --> 00:03:07,760
and what

00:03:05,519 --> 00:03:09,760
high quality data means is depends on

00:03:07,760 --> 00:03:12,480
who you ask for instances you ask

00:03:09,760 --> 00:03:15,040
the data consumers they usually take a

00:03:12,480 --> 00:03:16,879
more usage-oriented perspective

00:03:15,040 --> 00:03:19,200
they will ask things like does our data

00:03:16,879 --> 00:03:21,200
made our consumer's expectations

00:03:19,200 --> 00:03:23,519
does our data satisfy the requirements

00:03:21,200 --> 00:03:27,200
of its usage

00:03:23,519 --> 00:03:29,200
on the other hand if we look at

00:03:27,200 --> 00:03:30,640
it from the perspective of a business of

00:03:29,200 --> 00:03:32,879
the business as a whole

00:03:30,640 --> 00:03:34,799
then we usually might take a more value

00:03:32,879 --> 00:03:36,480
oriented perspective

00:03:34,799 --> 00:03:38,319
and we might ask ourselves things like

00:03:36,480 --> 00:03:39,360
how much value are we getting out of our

00:03:38,319 --> 00:03:41,120
data

00:03:39,360 --> 00:03:42,879
how much are we willing to invest into

00:03:41,120 --> 00:03:44,799
our data

00:03:42,879 --> 00:03:46,879
and finally but certainly not last there

00:03:44,799 --> 00:03:47,840
is an engineering perspective which is

00:03:46,879 --> 00:03:49,519
probably the one

00:03:47,840 --> 00:03:51,519
perspective on data quality that we are

00:03:49,519 --> 00:03:54,239
most familiar with which is

00:03:51,519 --> 00:03:56,080
oriented around standards and there we

00:03:54,239 --> 00:03:59,040
might ask things like to which degree

00:03:56,080 --> 00:04:01,120
does our data fulfill its specifications

00:03:59,040 --> 00:04:04,080
and in particular how accurate complete

00:04:01,120 --> 00:04:06,000
and timely is our data

00:04:04,080 --> 00:04:07,840
and again it's not an either or thing

00:04:06,000 --> 00:04:08,640
and all of these perspectives on data

00:04:07,840 --> 00:04:11,360
have the

00:04:08,640 --> 00:04:12,799
data quality have their own validity and

00:04:11,360 --> 00:04:14,879
it's really about

00:04:12,799 --> 00:04:15,920
from which perspective are we looking at

00:04:14,879 --> 00:04:17,919
it and how can we be

00:04:15,920 --> 00:04:20,479
empathetic if we if we are working

00:04:17,919 --> 00:04:23,680
together on data quality

00:04:20,479 --> 00:04:26,800
challenges how can we make use of all

00:04:23,680 --> 00:04:26,800
these different perspectives

00:04:26,840 --> 00:04:30,720
and in in the rest of my talk i would

00:04:29,680 --> 00:04:32,160
like to

00:04:30,720 --> 00:04:33,919
introduce you to a kind of running

00:04:32,160 --> 00:04:35,840
example that i use

00:04:33,919 --> 00:04:37,840
and it's even though this talk is based

00:04:35,840 --> 00:04:40,160
a lot on the experience i have as a

00:04:37,840 --> 00:04:41,360
data engineering consultant the example

00:04:40,160 --> 00:04:42,960
i picked is

00:04:41,360 --> 00:04:44,800
completely unrelated to my client

00:04:42,960 --> 00:04:46,720
because it's unconfidential and i can

00:04:44,800 --> 00:04:50,240
easily talk about it and kind of simple

00:04:46,720 --> 00:04:53,199
and easy to illustrate things with it

00:04:50,240 --> 00:04:53,600
and the example is about this point you

00:04:53,199 --> 00:04:55,040
might

00:04:53,600 --> 00:04:56,560
be wondering what's so interesting about

00:04:55,040 --> 00:04:57,919
this front and nothing particularly

00:04:56,560 --> 00:04:59,680
interesting about it it's just that's

00:04:57,919 --> 00:05:01,919
the point in my mom's garden

00:04:59,680 --> 00:05:03,759
and it's a pretty old pond it's about

00:05:01,919 --> 00:05:05,199
two or three decades old my grandfather

00:05:03,759 --> 00:05:06,800
built it originally

00:05:05,199 --> 00:05:08,400
and as you can see it's full of algae

00:05:06,800 --> 00:05:09,520
that's why my mom isn't really fond of

00:05:08,400 --> 00:05:11,199
this pond

00:05:09,520 --> 00:05:12,880
and she wants to get rid of it but i'm

00:05:11,199 --> 00:05:14,160
personally as you might have seen from

00:05:12,880 --> 00:05:14,880
the example with the fish in the

00:05:14,160 --> 00:05:16,639
beginning

00:05:14,880 --> 00:05:18,240
i'm really fond of ponds and i really

00:05:16,639 --> 00:05:20,400
like this pond

00:05:18,240 --> 00:05:21,680
and so i tried to convince my mom to

00:05:20,400 --> 00:05:23,360
keep it and she told me

00:05:21,680 --> 00:05:25,600
we can keep it if i've managed to get

00:05:23,360 --> 00:05:26,080
rid of the algae so i cleaned up the

00:05:25,600 --> 00:05:29,120
pond

00:05:26,080 --> 00:05:30,800
and i i took out some algae but of

00:05:29,120 --> 00:05:32,479
course the algae keep growing back and

00:05:30,800 --> 00:05:34,080
so the question is now how can i stop

00:05:32,479 --> 00:05:36,720
the algae growth

00:05:34,080 --> 00:05:38,080
and i'm not really big on on pond

00:05:36,720 --> 00:05:39,120
knowledge so i thought i could do an

00:05:38,080 --> 00:05:41,440
experiment

00:05:39,120 --> 00:05:43,520
to figure out why do these algae grow

00:05:41,440 --> 00:05:45,199
and what what can i do to stop the algae

00:05:43,520 --> 00:05:46,080
growth or at least reduce the algae

00:05:45,199 --> 00:05:49,120
growth

00:05:46,080 --> 00:05:49,680
and so i thought what i could do is do

00:05:49,120 --> 00:05:51,440
some kind

00:05:49,680 --> 00:05:53,120
turn this into some kind of data science

00:05:51,440 --> 00:05:56,080
or analytics challenge

00:05:53,120 --> 00:05:57,840
where we look at the different factors

00:05:56,080 --> 00:05:59,199
that contribute to algae growth like the

00:05:57,840 --> 00:06:00,160
water chemistry and the weather

00:05:59,199 --> 00:06:04,240
conditions

00:06:00,160 --> 00:06:06,240
and correlated or related to the

00:06:04,240 --> 00:06:08,240
algae coverage to find figure out which

00:06:06,240 --> 00:06:09,280
vectors are particularly contributing to

00:06:08,240 --> 00:06:11,280
the algorithm

00:06:09,280 --> 00:06:13,919
and then i might look monitor things

00:06:11,280 --> 00:06:15,919
like the ph level the nitrate phosphate

00:06:13,919 --> 00:06:18,000
and the hardness of the water

00:06:15,919 --> 00:06:19,120
and also the sun hours per day and the

00:06:18,000 --> 00:06:22,160
temperature each day

00:06:19,120 --> 00:06:23,919
and the rainfall and then of course

00:06:22,160 --> 00:06:25,919
what part of the pond is covered in

00:06:23,919 --> 00:06:29,199
algae

00:06:25,919 --> 00:06:31,280
and of course

00:06:29,199 --> 00:06:33,280
as with any data collection challenge

00:06:31,280 --> 00:06:34,960
there will be

00:06:33,280 --> 00:06:36,800
data processing thing there will be data

00:06:34,960 --> 00:06:38,960
quality issues that might come up

00:06:36,800 --> 00:06:40,720
so the first challenge is that i don't

00:06:38,960 --> 00:06:43,039
live next to the point so i would

00:06:40,720 --> 00:06:44,319
have to ask my mom to get me the data

00:06:43,039 --> 00:06:46,319
every day

00:06:44,319 --> 00:06:48,639
and that might introduce a lot of issues

00:06:46,319 --> 00:06:49,919
into the into the this whole into this

00:06:48,639 --> 00:06:52,000
whole process

00:06:49,919 --> 00:06:54,000
so for instance we don't really have

00:06:52,000 --> 00:06:55,680
expensive equipment to measure

00:06:54,000 --> 00:06:57,440
the water quality we have to rely on

00:06:55,680 --> 00:07:01,199
these kind of hobbyist data called

00:06:57,440 --> 00:07:02,800
um water water chemical kits

00:07:01,199 --> 00:07:05,360
and they're not super reliable so the

00:07:02,800 --> 00:07:07,840
measurements might be really noisy

00:07:05,360 --> 00:07:08,639
another thing is that the algae coverage

00:07:07,840 --> 00:07:11,039
is really

00:07:08,639 --> 00:07:12,720
difficult to estimate consistently it's

00:07:11,039 --> 00:07:14,720
easy to get say

00:07:12,720 --> 00:07:16,240
one day estimate the same i'll recover

00:07:14,720 --> 00:07:20,080
as 55

00:07:16,240 --> 00:07:21,919
and the next day at 65 percent which

00:07:20,080 --> 00:07:23,840
to us wouldn't be a big difference but

00:07:21,919 --> 00:07:25,440
if you show this and say a

00:07:23,840 --> 00:07:27,520
logistic regression it might become

00:07:25,440 --> 00:07:30,000
really sensitive to such inconsistently

00:07:27,520 --> 00:07:32,639
estimated numbers

00:07:30,000 --> 00:07:34,240
and also my mom might just forget to do

00:07:32,639 --> 00:07:35,840
the measurements some days or maybe

00:07:34,240 --> 00:07:37,120
she's having a bad day and she doesn't

00:07:35,840 --> 00:07:38,720
want to leave the house or whatever

00:07:37,120 --> 00:07:40,080
happens or maybe it's raining all day

00:07:38,720 --> 00:07:43,120
there might be reasons why she doesn't

00:07:40,080 --> 00:07:43,120
want to collect the data

00:07:43,440 --> 00:07:47,759
and also as as she sends me the

00:07:45,759 --> 00:07:49,360
measurements every day via email

00:07:47,759 --> 00:07:51,360
they might get mixed up i might save

00:07:49,360 --> 00:07:53,199
them in the wrong file i might

00:07:51,360 --> 00:07:55,280
confuse them in my excel spreadsheet all

00:07:53,199 --> 00:07:57,680
sorts of things might happen that

00:07:55,280 --> 00:08:00,240
might get us to the wrong data and

00:07:57,680 --> 00:08:02,720
finally the data might even get lost

00:08:00,240 --> 00:08:04,160
some days maybe it's stuck in the email

00:08:02,720 --> 00:08:05,919
ends up in my spam folder

00:08:04,160 --> 00:08:08,000
or maybe she forgets to take a

00:08:05,919 --> 00:08:09,680
measurement or maybe accidentally delete

00:08:08,000 --> 00:08:11,199
the number or think kinds of things

00:08:09,680 --> 00:08:13,520
might happen and even more things of

00:08:11,199 --> 00:08:15,840
course might happen

00:08:13,520 --> 00:08:17,440
and all of this would make the

00:08:15,840 --> 00:08:18,800
prediction that i'm trying to run all

00:08:17,440 --> 00:08:19,360
the correlations that i'm trying to

00:08:18,800 --> 00:08:21,360
compute

00:08:19,360 --> 00:08:23,199
really really unreliable and might not

00:08:21,360 --> 00:08:26,160
help me much with my

00:08:23,199 --> 00:08:27,039
undertaking and so that for that i was

00:08:26,160 --> 00:08:29,280
asking myself

00:08:27,039 --> 00:08:31,759
how can we invite that data quality

00:08:29,280 --> 00:08:36,240
avoid data quality issues

00:08:31,759 --> 00:08:38,159
and again i would like to

00:08:36,240 --> 00:08:40,320
first take a bit into the look into the

00:08:38,159 --> 00:08:43,120
theory to clarify some

00:08:40,320 --> 00:08:44,880
some answer define some roles here and

00:08:43,120 --> 00:08:46,000
so when we talk about data quality

00:08:44,880 --> 00:08:48,959
management

00:08:46,000 --> 00:08:50,560
in in the literature there's you usually

00:08:48,959 --> 00:08:52,399
find three different kind of roles which

00:08:50,560 --> 00:08:54,720
is the data producers and the data

00:08:52,399 --> 00:08:56,720
consumers as the business world

00:08:54,720 --> 00:08:58,560
and they're kind of what what the name

00:08:56,720 --> 00:09:00,080
implies so the data producers are the

00:08:58,560 --> 00:09:02,959
people that create collect and

00:09:00,080 --> 00:09:04,240
maintain the data so in the example that

00:09:02,959 --> 00:09:06,160
might be my mom gathering the

00:09:04,240 --> 00:09:07,839
measurements in the spreadsheet

00:09:06,160 --> 00:09:09,360
and the data consumers are the people

00:09:07,839 --> 00:09:10,720
that use the data in their work

00:09:09,360 --> 00:09:12,800
activities

00:09:10,720 --> 00:09:15,680
so in our example that might be me

00:09:12,800 --> 00:09:17,200
trying to understand the correlations

00:09:15,680 --> 00:09:19,519
on the other hand there's also a

00:09:17,200 --> 00:09:20,720
technical role and those are called data

00:09:19,519 --> 00:09:23,519
custodians

00:09:20,720 --> 00:09:25,120
and those people are what we commonly

00:09:23,519 --> 00:09:26,640
also understand nowadays as data

00:09:25,120 --> 00:09:28,880
engineers

00:09:26,640 --> 00:09:31,200
they design develop and operate the data

00:09:28,880 --> 00:09:32,959
infrastructure

00:09:31,200 --> 00:09:34,720
and that might be me building a python

00:09:32,959 --> 00:09:37,839
script for the analysis based on the

00:09:34,720 --> 00:09:40,240
email spreadsheet

00:09:37,839 --> 00:09:42,480
and the interesting thing is each of

00:09:40,240 --> 00:09:43,040
these rules can contribute to data

00:09:42,480 --> 00:09:45,600
quality

00:09:43,040 --> 00:09:47,360
for example as a data producer it's

00:09:45,600 --> 00:09:49,600
important to enter the right data

00:09:47,360 --> 00:09:50,480
or to validate the label data correctly

00:09:49,600 --> 00:09:54,160
correct eight

00:09:50,480 --> 00:09:56,399
errors in entries and labels

00:09:54,160 --> 00:09:56,399
and

00:09:58,880 --> 00:10:06,160
and sorry enter the data on time

00:10:03,360 --> 00:10:07,360
and as a data consumer on the other hand

00:10:06,160 --> 00:10:08,800
it's important that i check the

00:10:07,360 --> 00:10:10,720
plausibility of the data if there's

00:10:08,800 --> 00:10:12,880
nothing really weird in it

00:10:10,720 --> 00:10:14,880
that i interpret the data carefully and

00:10:12,880 --> 00:10:16,399
data report data quality issues so that

00:10:14,880 --> 00:10:18,720
they can be fixed

00:10:16,399 --> 00:10:20,160
and last but not least as a custodian

00:10:18,720 --> 00:10:22,320
the person that's kind of in between the

00:10:20,160 --> 00:10:25,920
producers and the consumers

00:10:22,320 --> 00:10:26,480
um we we can we need to make sure that

00:10:25,920 --> 00:10:28,640
we've done

00:10:26,480 --> 00:10:30,800
validate our data transformations that

00:10:28,640 --> 00:10:32,399
we monitor data quality that we report

00:10:30,800 --> 00:10:35,600
data quality issues

00:10:32,399 --> 00:10:36,880
and that will deliver data on time

00:10:35,600 --> 00:10:38,480
and of course there are other things so

00:10:36,880 --> 00:10:39,920
we can contribute these are just a few

00:10:38,480 --> 00:10:42,399
examples

00:10:39,920 --> 00:10:42,959
and now i would like to dig a bit deeper

00:10:42,399 --> 00:10:44,320
because

00:10:42,959 --> 00:10:46,640
of the audience of this conference

00:10:44,320 --> 00:10:48,320
because into the data custodian world

00:10:46,640 --> 00:10:51,120
and i would like to dig deeper in how we

00:10:48,320 --> 00:10:54,000
can measure data quality

00:10:51,120 --> 00:10:55,839
and so for the first the tool we usually

00:10:54,000 --> 00:10:57,600
use to

00:10:55,839 --> 00:11:00,000
measure data quality there's a concept

00:10:57,600 --> 00:11:02,160
of the data quality dimension

00:11:00,000 --> 00:11:04,640
and the data quality dimension is a set

00:11:02,160 --> 00:11:06,720
of data quality attributes

00:11:04,640 --> 00:11:09,200
that represents a single aspect or

00:11:06,720 --> 00:11:11,040
construct of data quality again that's a

00:11:09,200 --> 00:11:13,120
definition that's commonly used in the

00:11:11,040 --> 00:11:15,279
literature on this topic

00:11:13,120 --> 00:11:17,279
and the interesting thing about the data

00:11:15,279 --> 00:11:18,720
quality dimension is that it connects a

00:11:17,279 --> 00:11:20,800
lot of different concepts in data

00:11:18,720 --> 00:11:22,800
quality management

00:11:20,800 --> 00:11:24,800
so for example the data quality

00:11:22,800 --> 00:11:26,640
dimension is tied to a data quality

00:11:24,800 --> 00:11:28,000
perspective you remember that we had the

00:11:26,640 --> 00:11:29,120
usage perspective the value

00:11:28,000 --> 00:11:31,920
perspective and the standards

00:11:29,120 --> 00:11:33,839
perspective and for each of them we can

00:11:31,920 --> 00:11:35,440
define different dimensions that help us

00:11:33,839 --> 00:11:37,600
understand these perspectives so for

00:11:35,440 --> 00:11:39,839
instance for the user's perspective

00:11:37,600 --> 00:11:42,399
it might be relevance for the value

00:11:39,839 --> 00:11:44,160
perspective it might be value added

00:11:42,399 --> 00:11:46,720
and for the standard perspective it

00:11:44,160 --> 00:11:49,120
might be uniqueness

00:11:46,720 --> 00:11:51,120
and then based on these dimensions we

00:11:49,120 --> 00:11:52,880
can find specific metrics that let us

00:11:51,120 --> 00:11:54,399
measure the quality for each dimension

00:11:52,880 --> 00:11:57,040
so we can get an

00:11:54,399 --> 00:11:58,560
estimate about how good the data is in

00:11:57,040 --> 00:12:00,399
each of these dimensions

00:11:58,560 --> 00:12:02,240
and then based on these findings we can

00:12:00,399 --> 00:12:03,839
finally design a data quality

00:12:02,240 --> 00:12:06,480
improvement strategy that's specific to

00:12:03,839 --> 00:12:06,480
the dimension

00:12:08,000 --> 00:12:11,120
and of course now the interesting

00:12:10,399 --> 00:12:13,200
question in

00:12:11,120 --> 00:12:15,040
a tech conference is of course we don't

00:12:13,200 --> 00:12:16,480
want to measure all of this by hand

00:12:15,040 --> 00:12:18,160
we could in theory but it would be

00:12:16,480 --> 00:12:19,600
really really tedious especially if we

00:12:18,160 --> 00:12:22,959
have large data sets it might be

00:12:19,600 --> 00:12:24,639
even really painful and really expensive

00:12:22,959 --> 00:12:26,480
so the question is what dimension can we

00:12:24,639 --> 00:12:28,399
measure automatically and for me it was

00:12:26,480 --> 00:12:29,839
a really interesting realization of

00:12:28,399 --> 00:12:31,440
finding that

00:12:29,839 --> 00:12:33,360
a lot of dimensions can really not be

00:12:31,440 --> 00:12:34,800
measured automatically so in

00:12:33,360 --> 00:12:36,880
if we want to measure them we need some

00:12:34,800 --> 00:12:39,120
kind of human judgment involved

00:12:36,880 --> 00:12:40,639
that might be things common dimensions

00:12:39,120 --> 00:12:42,240
that are mentioned in

00:12:40,639 --> 00:12:44,240
as desirable are things like

00:12:42,240 --> 00:12:47,440
interpretability of data

00:12:44,240 --> 00:12:50,320
appropriate volume ease of understanding

00:12:47,440 --> 00:12:51,760
ease of access security relevance value

00:12:50,320 --> 00:12:54,079
added believability

00:12:51,760 --> 00:12:55,680
and so on and so forth but there are a

00:12:54,079 --> 00:12:58,240
few things that we can measure and we

00:12:55,680 --> 00:13:00,079
can measure them at different levels

00:12:58,240 --> 00:13:02,480
so we can for instance measure them at a

00:13:00,079 --> 00:13:05,040
data point level things like accuracy

00:13:02,480 --> 00:13:07,360
completeness of field values and other

00:13:05,040 --> 00:13:09,200
things are on the data set level things

00:13:07,360 --> 00:13:12,000
like completeness of the data set

00:13:09,200 --> 00:13:13,920
uniqueness of data sets and timeliness

00:13:12,000 --> 00:13:15,519
and again other things

00:13:13,920 --> 00:13:17,760
but i found it really interesting that a

00:13:15,519 --> 00:13:19,680
lot of what is considered desirable in

00:13:17,760 --> 00:13:20,079
terms of data quality really cannot be

00:13:19,680 --> 00:13:22,000
measured

00:13:20,079 --> 00:13:23,279
technically but there are a few things

00:13:22,000 --> 00:13:25,040
nevertheless that we can measure

00:13:23,279 --> 00:13:27,279
automatically and so we

00:13:25,040 --> 00:13:29,519
while we can get a full picture with our

00:13:27,279 --> 00:13:31,519
automated delta quality reports

00:13:29,519 --> 00:13:35,839
we can at least get some kind of picture

00:13:31,519 --> 00:13:35,839
from our data quality reports

00:13:37,839 --> 00:13:45,120
and so as the last kind of theoretical

00:13:43,279 --> 00:13:46,839
input that i'll give today there's the

00:13:45,120 --> 00:13:48,320
two strategies for data quality

00:13:46,839 --> 00:13:50,240
validation

00:13:48,320 --> 00:13:51,519
it's really there's a lot of different a

00:13:50,240 --> 00:13:52,880
lot of different ways and a lot of

00:13:51,519 --> 00:13:54,480
different tooling out there to measure

00:13:52,880 --> 00:13:56,160
data quality

00:13:54,480 --> 00:13:58,160
but really they all boil down to two

00:13:56,160 --> 00:14:00,880
different strategies i found one is

00:13:58,160 --> 00:14:01,279
rule based which work well whenever we

00:14:00,880 --> 00:14:03,360
can

00:14:01,279 --> 00:14:04,880
define absolute references for data

00:14:03,360 --> 00:14:08,399
quality

00:14:04,880 --> 00:14:10,160
so for example if we had

00:14:08,399 --> 00:14:11,839
if we look at my metric of algae

00:14:10,160 --> 00:14:13,680
coverage we can say the aggregate

00:14:11,839 --> 00:14:15,279
coverage must never be empty

00:14:13,680 --> 00:14:18,160
because as soon as it's empty we cannot

00:14:15,279 --> 00:14:20,320
use the whole day daily data point

00:14:18,160 --> 00:14:21,519
because we can correlate our other

00:14:20,320 --> 00:14:25,680
fields with the

00:14:21,519 --> 00:14:27,600
what we want to predict or call it and

00:14:25,680 --> 00:14:30,800
another thing that's very a kind of

00:14:27,600 --> 00:14:33,360
obvious fixed rule is sun hours must be

00:14:30,800 --> 00:14:35,040
between 0 and 24 even 24 7 hours it's

00:14:33,360 --> 00:14:37,279
pretty unlikely but definitely

00:14:35,040 --> 00:14:39,760
25 7 hours per day is definitely an

00:14:37,279 --> 00:14:41,199
error so we use rule-based validation

00:14:39,760 --> 00:14:42,959
strategies

00:14:41,199 --> 00:14:45,600
for conditions this must be met in any

00:14:42,959 --> 00:14:46,959
case for the data to be valid

00:14:45,600 --> 00:14:48,880
and there's other things we can also

00:14:46,959 --> 00:14:50,959
define on the data set level

00:14:48,880 --> 00:14:52,240
for example there must be exactly seven

00:14:50,959 --> 00:14:54,160
entries per week

00:14:52,240 --> 00:14:55,680
and all dates must be unique these are

00:14:54,160 --> 00:14:57,120
all routes we can define and if they're

00:14:55,680 --> 00:14:59,440
broken there's definitely something

00:14:57,120 --> 00:15:00,639
wrong now interestingly there's also a

00:14:59,440 --> 00:15:02,959
more fuzzy way to

00:15:00,639 --> 00:15:04,000
measure define data quality or validate

00:15:02,959 --> 00:15:06,639
data

00:15:04,000 --> 00:15:08,639
and that's with anonymous detection

00:15:06,639 --> 00:15:10,160
which is a conflict that's

00:15:08,639 --> 00:15:11,680
or at least used to be i'm not sure if

00:15:10,160 --> 00:15:13,120
it's still as widely talked about but

00:15:11,680 --> 00:15:14,720
for a while it wasn't a really hype

00:15:13,120 --> 00:15:17,920
topic

00:15:14,720 --> 00:15:18,639
and but really what it boils down to is

00:15:17,920 --> 00:15:21,440
whenever

00:15:18,639 --> 00:15:23,279
it it's data anonymously based detection

00:15:21,440 --> 00:15:24,240
works well whenever we can define data

00:15:23,279 --> 00:15:27,040
quality

00:15:24,240 --> 00:15:27,920
relative to other data points so that's

00:15:27,040 --> 00:15:29,759
often used

00:15:27,920 --> 00:15:31,120
when we have spikes or drops in time

00:15:29,759 --> 00:15:33,199
serious data

00:15:31,120 --> 00:15:34,800
so then we the relative means we look at

00:15:33,199 --> 00:15:37,839
relative to the past

00:15:34,800 --> 00:15:40,000
through the historic data

00:15:37,839 --> 00:15:40,880
and another way to cite it according to

00:15:40,000 --> 00:15:43,199
wikipedia is

00:15:40,880 --> 00:15:45,199
anonymously detection is the event

00:15:43,199 --> 00:15:47,759
identification of rare

00:15:45,199 --> 00:15:49,040
items events or observations which rise

00:15:47,759 --> 00:15:51,440
suspicions

00:15:49,040 --> 00:15:52,639
so again that points to the fact the

00:15:51,440 --> 00:15:55,360
interesting thing is

00:15:52,639 --> 00:15:56,959
with the fixed rules we definitely if

00:15:55,360 --> 00:15:57,839
they are violated we definitely know

00:15:56,959 --> 00:15:59,680
something is wrong

00:15:57,839 --> 00:16:01,199
whereas with anonymous detection we just

00:15:59,680 --> 00:16:03,040
know there is something off

00:16:01,199 --> 00:16:04,639
but it's not necessarily wrong it might

00:16:03,040 --> 00:16:06,160
actually be a valid measure

00:16:04,639 --> 00:16:08,000
just something might have drastically

00:16:06,160 --> 00:16:10,000
changed in the data and collecting

00:16:08,000 --> 00:16:13,199
environment that made the

00:16:10,000 --> 00:16:15,759
that made the data change for example

00:16:13,199 --> 00:16:17,519
if we you remember the ph value of the

00:16:15,759 --> 00:16:19,600
water quality

00:16:17,519 --> 00:16:21,040
that we could say the ph value should

00:16:19,600 --> 00:16:24,000
not change dramatically

00:16:21,040 --> 00:16:25,759
saying not more than 50 for each day it

00:16:24,000 --> 00:16:26,880
could change more for instance if you

00:16:25,759 --> 00:16:29,759
pull a bunch of

00:16:26,880 --> 00:16:30,560
acid or some other chemical into the

00:16:29,759 --> 00:16:32,079
water or

00:16:30,560 --> 00:16:34,639
if it just rains a lot and the whole

00:16:32,079 --> 00:16:36,079
water gets flooded out

00:16:34,639 --> 00:16:37,680
then the ph value might change

00:16:36,079 --> 00:16:38,639
dramatically and it would be a perfectly

00:16:37,680 --> 00:16:40,560
very

00:16:38,639 --> 00:16:42,160
valid measure to say that a test chained

00:16:40,560 --> 00:16:44,000
more than 50

00:16:42,160 --> 00:16:45,360
but unless something really dramatic

00:16:44,000 --> 00:16:47,519
happened to the point

00:16:45,360 --> 00:16:49,360
probably if it changes more than 50 it

00:16:47,519 --> 00:16:52,560
might be off

00:16:49,360 --> 00:16:54,399
and similarly the number of measures

00:16:52,560 --> 00:16:56,079
should only be increasing over time if

00:16:54,399 --> 00:16:58,639
you want to have an example on the data

00:16:56,079 --> 00:16:58,639
set level

00:17:00,639 --> 00:17:04,480
and now i would like to share some

00:17:02,959 --> 00:17:05,439
experiences with you to make it a bit

00:17:04,480 --> 00:17:08,400
more practical

00:17:05,439 --> 00:17:10,160
about how that i had with monitoring or

00:17:08,400 --> 00:17:12,000
my team had with monitoring

00:17:10,160 --> 00:17:13,839
data quality and data patterns i'll

00:17:12,000 --> 00:17:15,679
stick to the example but the

00:17:13,839 --> 00:17:17,679
the evaluation and the tooling

00:17:15,679 --> 00:17:18,079
experiences are really based on a client

00:17:17,679 --> 00:17:20,640
project

00:17:18,079 --> 00:17:22,319
i had i just anonymized the tools to

00:17:20,640 --> 00:17:24,959
conform with the

00:17:22,319 --> 00:17:25,520
with the pond example and so let's

00:17:24,959 --> 00:17:28,400
imagine

00:17:25,520 --> 00:17:29,520
right now my mom sends me an email with

00:17:28,400 --> 00:17:31,200
the data

00:17:29,520 --> 00:17:33,919
and then i have around a small titan

00:17:31,200 --> 00:17:34,320
script and it put outputs some graphs

00:17:33,919 --> 00:17:36,000
and some

00:17:34,320 --> 00:17:38,799
some nice correlation or regression

00:17:36,000 --> 00:17:41,760
coefficients or whatever i want to do

00:17:38,799 --> 00:17:43,039
and that tells me the water quality but

00:17:41,760 --> 00:17:46,080
the thing is

00:17:43,039 --> 00:17:48,240
this thing needs to run a lot of times

00:17:46,080 --> 00:17:50,240
because you can't based on just a few

00:17:48,240 --> 00:17:52,640
data points you can't really analyze

00:17:50,240 --> 00:17:54,240
correlations that well so we need to run

00:17:52,640 --> 00:17:54,720
this say over three months or something

00:17:54,240 --> 00:17:56,320
somewhere

00:17:54,720 --> 00:17:58,480
some longer period and that means a lot

00:17:56,320 --> 00:18:01,039
of data and it's getting really serious

00:17:58,480 --> 00:18:02,960
and tesios also means error prone and

00:18:01,039 --> 00:18:04,880
that's all things we can avoid by just

00:18:02,960 --> 00:18:06,880
automating things

00:18:04,880 --> 00:18:08,320
and so let's imagine i build a kind of

00:18:06,880 --> 00:18:12,240
small data pipeline

00:18:08,320 --> 00:18:14,000
with a and i build this kind of cdci

00:18:12,240 --> 00:18:15,760
pipeline to deploy my code

00:18:14,000 --> 00:18:18,000
automatically in all the things we like

00:18:15,760 --> 00:18:20,720
to do in software engineering

00:18:18,000 --> 00:18:22,640
and then what we can introduce is the

00:18:20,720 --> 00:18:25,360
idea of a data quality gate

00:18:22,640 --> 00:18:26,960
so data quality gates are an idea that

00:18:25,360 --> 00:18:28,720
comes from continuous improvement

00:18:26,960 --> 00:18:30,640
continuous deployment

00:18:28,720 --> 00:18:32,160
and it's something that can be used to

00:18:30,640 --> 00:18:34,080
increase the confidence towards the

00:18:32,160 --> 00:18:37,039
deployed service

00:18:34,080 --> 00:18:38,960
and most precisely a quality gate is an

00:18:37,039 --> 00:18:42,000
automated checkpoint which the deployed

00:18:38,960 --> 00:18:43,760
artifact needs to pass to girl life

00:18:42,000 --> 00:18:45,600
for code these are things we know very

00:18:43,760 --> 00:18:47,679
well so this might be codes at different

00:18:45,600 --> 00:18:49,360
levels so integration tests unit tests

00:18:47,679 --> 00:18:52,320
acceptance tests

00:18:49,360 --> 00:18:53,840
service level tests usability tests all

00:18:52,320 --> 00:18:55,280
sorts of level of tests that i think

00:18:53,840 --> 00:18:57,600
something has to pass

00:18:55,280 --> 00:18:59,039
but it might also be things like study

00:18:57,600 --> 00:19:00,960
code analysis

00:18:59,039 --> 00:19:03,440
so different kind of things that our

00:19:00,960 --> 00:19:05,679
code has to pass in order to

00:19:03,440 --> 00:19:07,360
to be considered production ready and

00:19:05,679 --> 00:19:08,960
for data we can define something very

00:19:07,360 --> 00:19:10,880
similar actually

00:19:08,960 --> 00:19:12,320
which is we can use those different

00:19:10,880 --> 00:19:14,960
validation strategies

00:19:12,320 --> 00:19:16,240
and define based on those our automatic

00:19:14,960 --> 00:19:19,520
checks and our nominal

00:19:16,240 --> 00:19:21,600
base detect validation to figure out

00:19:19,520 --> 00:19:23,200
is this data set ready to go put into

00:19:21,600 --> 00:19:23,760
production or is there something that we

00:19:23,200 --> 00:19:26,880
should check

00:19:23,760 --> 00:19:29,919
or fix and so

00:19:26,880 --> 00:19:31,679
i could extend my little pipeline by not

00:19:29,919 --> 00:19:34,960
only having a script that reads the

00:19:31,679 --> 00:19:37,919
the spreadsheets from the

00:19:34,960 --> 00:19:39,760
email account but also have my add my

00:19:37,919 --> 00:19:42,559
validation checks

00:19:39,760 --> 00:19:43,120
and anonymous detection steps and only

00:19:42,559 --> 00:19:47,760
then

00:19:43,120 --> 00:19:47,760
use the data to compute my analysis

00:19:48,400 --> 00:19:52,080
and then i was wondering what tool

00:19:50,640 --> 00:19:55,679
should i use and i came up with this

00:19:52,080 --> 00:19:58,960
kind of little checklist which tells me

00:19:55,679 --> 00:20:01,360
um what are the requirements for data

00:19:58,960 --> 00:20:03,039
quality monitoring tools

00:20:01,360 --> 00:20:04,559
and there are few things i came up with

00:20:03,039 --> 00:20:05,039
and i think i have to speed up a little

00:20:04,559 --> 00:20:06,640
bit

00:20:05,039 --> 00:20:10,480
things like it can compute the needed

00:20:06,640 --> 00:20:12,720
metrics can perform static checks

00:20:10,480 --> 00:20:14,480
can alert on validation failures can

00:20:12,720 --> 00:20:15,360
visualize the current and historic state

00:20:14,480 --> 00:20:18,320
of the metrics

00:20:15,360 --> 00:20:19,440
for and can integrate into our existing

00:20:18,320 --> 00:20:21,280
tool chain

00:20:19,440 --> 00:20:23,200
and there were a few there's actually

00:20:21,280 --> 00:20:24,480
quite a few data quality companies

00:20:23,200 --> 00:20:27,520
coming up right now

00:20:24,480 --> 00:20:30,559
but some examples i looked at include

00:20:27,520 --> 00:20:30,960
spark for scalabase dq which is the one

00:20:30,559 --> 00:20:33,200
i'll

00:20:30,960 --> 00:20:34,640
talk about later in the rest of my talk

00:20:33,200 --> 00:20:36,480
and there's also a patent-based

00:20:34,640 --> 00:20:39,360
framework called great expectations from

00:20:36,480 --> 00:20:39,360
superconductive

00:20:39,919 --> 00:20:43,760
and the queue which we used in our

00:20:42,000 --> 00:20:46,799
project

00:20:43,760 --> 00:20:48,159
is as i mentioned it's it's produced by

00:20:46,799 --> 00:20:50,400
aws labs

00:20:48,159 --> 00:20:52,320
and it's a scala library for data

00:20:50,400 --> 00:20:53,280
quality validation on large data sets

00:20:52,320 --> 00:20:55,360
using spark

00:20:53,280 --> 00:20:58,000
so you wouldn't actually use it on a

00:20:55,360 --> 00:21:00,480
small use case like my point example

00:20:58,000 --> 00:21:02,080
but usually when we when we don't

00:21:00,480 --> 00:21:03,919
usually build data pipelines for such

00:21:02,080 --> 00:21:05,919
small examples so when we have a

00:21:03,919 --> 00:21:07,520
reasonably large data set it makes sense

00:21:05,919 --> 00:21:09,039
to you but it actually makes sense to

00:21:07,520 --> 00:21:10,720
deploy spark it also

00:21:09,039 --> 00:21:12,320
could be a good idea to use something

00:21:10,720 --> 00:21:14,080
like the queue

00:21:12,320 --> 00:21:15,440
and the nice thing about the queue is it

00:21:14,080 --> 00:21:18,240
provides both rule

00:21:15,440 --> 00:21:19,919
based validations and anomaly based data

00:21:18,240 --> 00:21:23,039
validation so it provides a lot of

00:21:19,919 --> 00:21:23,039
features that we can use

00:21:24,240 --> 00:21:27,919
and it's also fairly easy to use

00:21:26,640 --> 00:21:31,120
actually which was plus

00:21:27,919 --> 00:21:32,559
was a was a pleasant experience so

00:21:31,120 --> 00:21:34,799
for instance if we want to do a rule

00:21:32,559 --> 00:21:37,520
based validation all we have to do is

00:21:34,799 --> 00:21:39,520
the whole example fits onto one slide so

00:21:37,520 --> 00:21:40,640
first we instantiate the verification

00:21:39,520 --> 00:21:44,159
suit which is the

00:21:40,640 --> 00:21:44,720
kind of wrapper that we use to institute

00:21:44,159 --> 00:21:48,240
call

00:21:44,720 --> 00:21:49,360
start validation then we define the data

00:21:48,240 --> 00:21:52,320
set

00:21:49,360 --> 00:21:54,080
we add a check with the daisy chain that

00:21:52,320 --> 00:21:54,960
and for instance here we define three

00:21:54,080 --> 00:21:57,120
checks

00:21:54,960 --> 00:21:59,520
one that the data set has seven rows so

00:21:57,120 --> 00:22:02,880
one for each day

00:21:59,520 --> 00:22:05,280
um that the algae coverage field

00:22:02,880 --> 00:22:06,799
should never be empty and that the data

00:22:05,280 --> 00:22:07,200
is unique which you might remember were

00:22:06,799 --> 00:22:09,760
all

00:22:07,200 --> 00:22:11,280
things we defined earlier as examples

00:22:09,760 --> 00:22:13,360
then we run the whole thing

00:22:11,280 --> 00:22:15,440
and then we just check the status of the

00:22:13,360 --> 00:22:17,280
verification result

00:22:15,440 --> 00:22:19,039
and if it's not a success we can do

00:22:17,280 --> 00:22:20,640
something here we're just printing out

00:22:19,039 --> 00:22:22,480
that we found an error

00:22:20,640 --> 00:22:24,159
but we could also throw an exception or

00:22:22,480 --> 00:22:26,240
we could

00:22:24,159 --> 00:22:28,960
notify our logging system or whatever

00:22:26,240 --> 00:22:31,440
you want to do with it

00:22:28,960 --> 00:22:32,960
and very similarly an anonymously based

00:22:31,440 --> 00:22:34,960
validation

00:22:32,960 --> 00:22:36,000
uses a similar schema it's not much

00:22:34,960 --> 00:22:38,559
longer it's a little bit

00:22:36,000 --> 00:22:40,080
more complicated but not by much again

00:22:38,559 --> 00:22:44,000
we start our verification

00:22:40,080 --> 00:22:46,000
suit then we define uh call our own data

00:22:44,000 --> 00:22:47,280
method to define on the dataset we want

00:22:46,000 --> 00:22:48,640
to use

00:22:47,280 --> 00:22:50,080
and then we do something slightly

00:22:48,640 --> 00:22:51,360
different which is we use in the

00:22:50,080 --> 00:22:53,360
repository

00:22:51,360 --> 00:22:55,280
and the idea of a repository is that we

00:22:53,360 --> 00:22:56,559
need some place to keep track of the

00:22:55,280 --> 00:22:58,320
historic data

00:22:56,559 --> 00:23:00,799
that's what we do in this repository

00:22:58,320 --> 00:23:01,840
it's it's usually a kind of json file

00:23:00,799 --> 00:23:04,080
where you just append

00:23:01,840 --> 00:23:05,600
the data it could be residing say on s3

00:23:04,080 --> 00:23:07,919
or someplace

00:23:05,600 --> 00:23:10,720
and then you'd with the next call you

00:23:07,919 --> 00:23:12,880
append save or append the results of the

00:23:10,720 --> 00:23:14,880
validation that you just run so your

00:23:12,880 --> 00:23:17,120
match computed metrics

00:23:14,880 --> 00:23:20,159
and then then you do the actual

00:23:17,120 --> 00:23:22,960
anonymity detection

00:23:20,159 --> 00:23:25,120
and the queue here uses a very simple

00:23:22,960 --> 00:23:28,559
strategy it just compares to

00:23:25,120 --> 00:23:30,000
the last day data point with the current

00:23:28,559 --> 00:23:33,200
data point and says

00:23:30,000 --> 00:23:34,720
and looks at what the decrease is and as

00:23:33,200 --> 00:23:36,320
you might remember we wanted to make

00:23:34,720 --> 00:23:38,000
sure that our data set size never

00:23:36,320 --> 00:23:39,840
decreases so we just define it should

00:23:38,000 --> 00:23:41,679
never decrease

00:23:39,840 --> 00:23:42,960
and we run this on the size of the data

00:23:41,679 --> 00:23:46,000
set

00:23:42,960 --> 00:23:46,000
and finally again

00:23:46,159 --> 00:23:49,200
we check whether our status was

00:23:47,919 --> 00:23:51,360
successful or not and

00:23:49,200 --> 00:23:53,600
if it wasn't successful we just say okay

00:23:51,360 --> 00:23:55,440
there was an anonymity detected

00:23:53,600 --> 00:23:58,159
or we do something whatever else we want

00:23:55,440 --> 00:24:01,039
to do with the status

00:23:58,159 --> 00:24:03,120
so that's so that is pretty cool

00:24:01,039 --> 00:24:04,799
actually we found and it's very easy to

00:24:03,120 --> 00:24:07,520
add this to any kind of spark

00:24:04,799 --> 00:24:08,720
based data pipeline and also the other

00:24:07,520 --> 00:24:10,880
really cool thing about the queues

00:24:08,720 --> 00:24:12,960
there's a lot of these different kind of

00:24:10,880 --> 00:24:14,960
metrics that you can use

00:24:12,960 --> 00:24:17,440
and they can operate both on columns of

00:24:14,960 --> 00:24:19,279
the data set and on the entire data sets

00:24:17,440 --> 00:24:21,039
and they can they can be fairly

00:24:19,279 --> 00:24:24,640
consistently used for both rule and

00:24:21,039 --> 00:24:24,640
anonymously based validation

00:24:25,600 --> 00:24:28,640
and they provide a lot of example

00:24:27,679 --> 00:24:31,039
metrics

00:24:28,640 --> 00:24:33,039
so for data point level dimensions on

00:24:31,039 --> 00:24:35,279
data set level dimensions

00:24:33,039 --> 00:24:37,760
and that is also pretty that that means

00:24:35,279 --> 00:24:40,240
you can in theory run some very powerful

00:24:37,760 --> 00:24:41,919
data validation if you want to but of

00:24:40,240 --> 00:24:43,840
course like everything everything has

00:24:41,919 --> 00:24:46,960
drawbacks as well

00:24:43,840 --> 00:24:48,400
and so what we found is

00:24:46,960 --> 00:24:50,080
that the queue is a really promising

00:24:48,400 --> 00:24:52,320
framework but

00:24:50,080 --> 00:24:54,320
it has some quirks still where it might

00:24:52,320 --> 00:24:55,679
not be completely ready for production

00:24:54,320 --> 00:24:59,120
or at least for complex

00:24:55,679 --> 00:25:02,080
use cases so on the positive side

00:24:59,120 --> 00:25:04,799
the queue is really fast because it's

00:25:02,080 --> 00:25:06,640
directly implemented in spark and

00:25:04,799 --> 00:25:08,640
it computes its rules checks and both

00:25:06,640 --> 00:25:09,919
and even the anonymous detection really

00:25:08,640 --> 00:25:11,600
fast

00:25:09,919 --> 00:25:13,760
as you can have seen the validation can

00:25:11,600 --> 00:25:15,279
be implemented with very little code

00:25:13,760 --> 00:25:17,200
there are lots of metrics to choose from

00:25:15,279 --> 00:25:19,039
as you've also seen

00:25:17,200 --> 00:25:20,640
and the library code is fairly easy to

00:25:19,039 --> 00:25:22,320
understand for digging deeper than the

00:25:20,640 --> 00:25:24,320
examples

00:25:22,320 --> 00:25:26,240
and it's under it's on the very active

00:25:24,320 --> 00:25:27,520
development which is also nice so

00:25:26,240 --> 00:25:30,000
even some of the things i'm saying right

00:25:27,520 --> 00:25:32,080
now might be already outdated because we

00:25:30,000 --> 00:25:34,880
we did a project couple of months ago

00:25:32,080 --> 00:25:37,600
and the library has already changed

00:25:34,880 --> 00:25:39,120
but the limitations are that right still

00:25:37,600 --> 00:25:41,600
at this very point

00:25:39,120 --> 00:25:43,200
the documentation is very limited so

00:25:41,600 --> 00:25:44,159
there's very few documentation of

00:25:43,200 --> 00:25:46,559
concepts

00:25:44,159 --> 00:25:48,320
and most of the usage is just basic

00:25:46,559 --> 00:25:49,919
examples so if you if you want to use

00:25:48,320 --> 00:25:50,559
all the power that the queue seems to

00:25:49,919 --> 00:25:52,159
provide

00:25:50,559 --> 00:25:53,840
you really need to dig into the code to

00:25:52,159 --> 00:25:56,080
understand what it's doing and figure it

00:25:53,840 --> 00:25:57,200
out by yourself what it's doing and

00:25:56,080 --> 00:25:59,279
there were also a few

00:25:57,200 --> 00:26:01,279
bugs or at least one book where we found

00:25:59,279 --> 00:26:04,000
that we had a false positive due to a

00:26:01,279 --> 00:26:04,000
sampling issue

00:26:06,159 --> 00:26:10,080
and now i would like to wrap up my talk

00:26:09,039 --> 00:26:12,240
so

00:26:10,080 --> 00:26:13,360
as you might remember from the beginning

00:26:12,240 --> 00:26:17,039
data has to cite

00:26:13,360 --> 00:26:20,559
informat information for decision making

00:26:17,039 --> 00:26:22,799
and information in electronic forms and

00:26:20,559 --> 00:26:24,159
both of these really are two sides of

00:26:22,799 --> 00:26:25,600
the same coin

00:26:24,159 --> 00:26:28,240
and when we talk about data quality

00:26:25,600 --> 00:26:32,080
management both of these definitions are

00:26:28,240 --> 00:26:34,240
useful and relevant and likewise

00:26:32,080 --> 00:26:35,840
data quality also has different three

00:26:34,240 --> 00:26:36,720
different perspectives that we can take

00:26:35,840 --> 00:26:38,559
on it

00:26:36,720 --> 00:26:39,919
and it we can look at it from a user's

00:26:38,559 --> 00:26:41,120
perspective from a business value

00:26:39,919 --> 00:26:43,600
perspective and from an

00:26:41,120 --> 00:26:45,760
engineering standard perspective and all

00:26:43,600 --> 00:26:47,520
of us contribute to high data quality

00:26:45,760 --> 00:26:50,720
whether we're data consumers data

00:26:47,520 --> 00:26:52,720
producers or data processors

00:26:50,720 --> 00:26:55,919
and those engineers our main role is to

00:26:52,720 --> 00:26:57,919
automate the validation and monitoring

00:26:55,919 --> 00:27:02,080
that was my talk and now i'm happy to

00:26:57,919 --> 00:27:02,080
answer questions if there were any

00:27:02,880 --> 00:27:06,080
hi there ellen uh thanks for that that

00:27:04,720 --> 00:27:09,440
was a really interesting talk

00:27:06,080 --> 00:27:12,000
um i'm a conference organizer and

00:27:09,440 --> 00:27:13,120
not a data scientist but i still got

00:27:12,000 --> 00:27:16,480
quite a lot out of that

00:27:13,120 --> 00:27:17,200
um we have some time for some questions

00:27:16,480 --> 00:27:20,799
now so

00:27:17,200 --> 00:27:24,480
um if you want to ask a question you

00:27:20,799 --> 00:27:28,480
can ask it in the slack we have one from

00:27:24,480 --> 00:27:30,960
matthias um atheist asks um

00:27:28,480 --> 00:27:32,799
do you use any machine learning models

00:27:30,960 --> 00:27:36,799
to validate your

00:27:32,799 --> 00:27:38,240
data quality i personally do not i'm

00:27:36,799 --> 00:27:40,240
sure

00:27:38,240 --> 00:27:41,840
i mean especially if you we talk about

00:27:40,240 --> 00:27:43,440
complex anomaly detection there are

00:27:41,840 --> 00:27:45,200
definitely machine learning

00:27:43,440 --> 00:27:47,440
models you could use but i don't have

00:27:45,200 --> 00:27:51,279
experience with that so i

00:27:47,440 --> 00:27:52,000
i don't i would also try whenever i can

00:27:51,279 --> 00:27:54,559
to keep the

00:27:52,000 --> 00:27:56,159
data validation methods as simple as

00:27:54,559 --> 00:27:58,240
possible and there's a lot of things you

00:27:56,159 --> 00:28:00,559
can do with this basic checks and basic

00:27:58,240 --> 00:28:02,240
strategies for anonymous detections

00:28:00,559 --> 00:28:04,159
because you don't really want to add

00:28:02,240 --> 00:28:05,679
extra complexity and something where you

00:28:04,159 --> 00:28:06,960
already aren't totally sure whether it's

00:28:05,679 --> 00:28:08,799
an issue or not

00:28:06,960 --> 00:28:10,640
and the simpler the strategy the easier

00:28:08,799 --> 00:28:14,000
it is to figure out what happened

00:28:10,640 --> 00:28:17,440
and why the issue occurred

00:28:14,000 --> 00:28:17,760
okay um i can see that um some people

00:28:17,440 --> 00:28:19,840
that

00:28:17,760 --> 00:28:21,520
are typing some questions in the at the

00:28:19,840 --> 00:28:22,240
moment so hopefully they will come

00:28:21,520 --> 00:28:25,840
through

00:28:22,240 --> 00:28:27,919
soon um while we wait i wonder

00:28:25,840 --> 00:28:30,080
if you can give us an update on the pond

00:28:27,919 --> 00:28:32,559
how is the pond doing now

00:28:30,080 --> 00:28:34,799
the pond is still kind of full of algae

00:28:32,559 --> 00:28:39,039
and i really need to clean it again so

00:28:34,799 --> 00:28:45,840
we're still collecting data okay

00:28:39,039 --> 00:28:45,840
in process good um

00:28:47,039 --> 00:28:50,159
so um

00:28:51,120 --> 00:28:56,159
we have a more a comment from lars

00:28:53,919 --> 00:28:58,640
lazadson says finally a talk that's fast

00:28:56,159 --> 00:29:00,000
enough to keep me awake

00:28:58,640 --> 00:29:04,080
i have the habit of watching recorded

00:29:00,000 --> 00:29:04,080
presentations at 1.75 speed

00:29:04,559 --> 00:29:11,120
um so do we have

00:29:08,480 --> 00:29:12,240
any more questions write them down we

00:29:11,120 --> 00:29:14,960
have someone

00:29:12,240 --> 00:29:14,960
writing a question

00:29:25,039 --> 00:29:29,200
uh yep we have a question from alex oh

00:29:27,360 --> 00:29:31,200
it says um are there methods

00:29:29,200 --> 00:29:32,240
uh to assigning data quality at the

00:29:31,200 --> 00:29:35,200
point of use

00:29:32,240 --> 00:29:35,200
based on usage

00:29:35,600 --> 00:29:39,919
i'm not sure i totally got the question

00:29:37,600 --> 00:29:41,279
could you repeat the question perhaps

00:29:39,919 --> 00:29:43,679
yeah are there methods

00:29:41,279 --> 00:29:44,720
of assessing data quality at the point

00:29:43,679 --> 00:29:48,000
of use

00:29:44,720 --> 00:29:48,000
or based on usage

00:29:48,720 --> 00:29:55,120
i don't know to be honest there's

00:29:52,080 --> 00:29:57,120
i mean in the sense of what you

00:29:55,120 --> 00:29:58,799
what i could imagine yourself doing is

00:29:57,120 --> 00:30:02,880
you can definitely if you have a

00:29:58,799 --> 00:30:05,279
say in an application that uses data

00:30:02,880 --> 00:30:06,880
you can track where your data is used

00:30:05,279 --> 00:30:07,520
and where people get confused and say if

00:30:06,880 --> 00:30:08,720
you have

00:30:07,520 --> 00:30:10,880
if you have a dashboard in your

00:30:08,720 --> 00:30:12,000
application you can see where people get

00:30:10,880 --> 00:30:14,399
stuck

00:30:12,000 --> 00:30:15,440
and that's something we've actually

00:30:14,399 --> 00:30:17,520
tried out at the

00:30:15,440 --> 00:30:19,520
at the client where i was previously

00:30:17,520 --> 00:30:21,360
working

00:30:19,520 --> 00:30:23,120
but i'm not sure if they're systematic

00:30:21,360 --> 00:30:25,279
methods so it's it's a very

00:30:23,120 --> 00:30:26,880
custom field way of figuring and it's a

00:30:25,279 --> 00:30:28,559
very high level

00:30:26,880 --> 00:30:30,000
it's a very abstract because you might

00:30:28,559 --> 00:30:31,279
find out so for example that at the

00:30:30,000 --> 00:30:32,799
point of usage

00:30:31,279 --> 00:30:34,559
people always get stuck at a certain

00:30:32,799 --> 00:30:35,919
metrics that they look at

00:30:34,559 --> 00:30:37,840
but then you still don't know why

00:30:35,919 --> 00:30:38,480
they're they've they get stuck at this

00:30:37,840 --> 00:30:39,679
matter so

00:30:38,480 --> 00:30:41,039
it's def there's definitely some

00:30:39,679 --> 00:30:42,640
indicators you can get safe from

00:30:41,039 --> 00:30:45,520
tracking data

00:30:42,640 --> 00:30:47,360
but it's it's further removed than if

00:30:45,520 --> 00:30:49,440
you say analyze the data

00:30:47,360 --> 00:30:50,960
somewhere in your pipeline or the cl

00:30:49,440 --> 00:30:53,520
generally the closer the

00:30:50,960 --> 00:30:55,279
to your to production you ano you figure

00:30:53,520 --> 00:30:57,039
out the data quality issue

00:30:55,279 --> 00:30:59,039
the easier it is also to fix and the

00:30:57,039 --> 00:31:00,720
easier it is to detect

00:30:59,039 --> 00:31:02,320
does that answer a question or did i get

00:31:00,720 --> 00:31:05,840
in a totally different direction than

00:31:02,320 --> 00:31:05,840
what you were asking for

00:31:06,320 --> 00:31:10,960
we will see um

00:31:09,440 --> 00:31:13,200
while we wait uh we have another

00:31:10,960 --> 00:31:15,679
question um is there any relation

00:31:13,200 --> 00:31:19,600
between how you code test your pipelines

00:31:15,679 --> 00:31:24,080
and the data quality tests

00:31:19,600 --> 00:31:24,080
um yes there is actually so

00:31:24,240 --> 00:31:27,840
if you when we especially when we talk

00:31:26,159 --> 00:31:29,760
about data quality

00:31:27,840 --> 00:31:32,000
sorry data transformations that we do in

00:31:29,760 --> 00:31:33,679
pipeline say we aggregate some data

00:31:32,000 --> 00:31:35,039
we combine some data sets and all these

00:31:33,679 --> 00:31:36,960
kind of things

00:31:35,039 --> 00:31:38,799
those testing those transformations

00:31:36,960 --> 00:31:39,840
definitely contributes to higher quality

00:31:38,799 --> 00:31:41,760
of data so

00:31:39,840 --> 00:31:44,240
as soon as you as you have any bugs in

00:31:41,760 --> 00:31:46,159
in your basic data transformations

00:31:44,240 --> 00:31:48,960
then of course those propagate into your

00:31:46,159 --> 00:31:48,960
data quality

00:31:49,120 --> 00:31:54,480
so so data so the testing the pipeline

00:31:52,320 --> 00:31:57,679
the code tests are definitely a part of

00:31:54,480 --> 00:31:57,679
ensuring data quality

00:31:58,240 --> 00:32:04,399
okay and we have one more um

00:32:01,760 --> 00:32:07,840
so from a higher level who needs to take

00:32:04,399 --> 00:32:10,799
care of data quality in a company

00:32:07,840 --> 00:32:12,000
well i would say anyone who's involved

00:32:10,799 --> 00:32:13,519
in the data

00:32:12,000 --> 00:32:15,679
that's why i have this point on my

00:32:13,519 --> 00:32:18,799
summary slide um

00:32:15,679 --> 00:32:20,480
i mean there's becau

00:32:18,799 --> 00:32:22,480
there's different layers to this at this

00:32:20,480 --> 00:32:24,080
question so the first level is of course

00:32:22,480 --> 00:32:26,159
everybody who's involved somewhere in

00:32:24,080 --> 00:32:28,559
either producing data or consuming data

00:32:26,159 --> 00:32:30,320
or then using the data say by

00:32:28,559 --> 00:32:32,799
making sure that the data is plausible

00:32:30,320 --> 00:32:34,399
and all these kind of things

00:32:32,799 --> 00:32:35,679
that they also that deal directly with

00:32:34,399 --> 00:32:37,440
the data they're definitely all

00:32:35,679 --> 00:32:40,559
responsible for data quality

00:32:37,440 --> 00:32:42,320
but my experience is that data quality

00:32:40,559 --> 00:32:44,799
initiatives because they are kind of

00:32:42,320 --> 00:32:45,840
across different departments and that

00:32:44,799 --> 00:32:48,480
they cannot just be

00:32:45,840 --> 00:32:49,200
located in inside the data team or i.t

00:32:48,480 --> 00:32:51,440
department

00:32:49,200 --> 00:32:53,120
traditionally or wherever them whoever

00:32:51,440 --> 00:32:54,559
might own the databases and maybe the

00:32:53,120 --> 00:32:57,760
applications

00:32:54,559 --> 00:32:59,440
so that definitely needs to be if the

00:32:57,760 --> 00:33:01,039
data is considered valuable enough to

00:32:59,440 --> 00:33:02,480
actually invest in data quality there

00:33:01,039 --> 00:33:05,360
also needs to be some

00:33:02,480 --> 00:33:08,399
senior level commitment in into the data

00:33:05,360 --> 00:33:08,399
quality management

00:33:09,039 --> 00:33:15,919
okay cool thank you um

00:33:13,279 --> 00:33:17,600
uh we can uh continue this chat in the

00:33:15,919 --> 00:33:20,559
breakout room afterwards i will

00:33:17,600 --> 00:33:21,919
um post a link to the jitsi room so if

00:33:20,559 --> 00:33:22,559
anyone would like to continue the

00:33:21,919 --> 00:33:25,679
discussion

00:33:22,559 --> 00:33:27,919
that's where it will take place um yeah

00:33:25,679 --> 00:33:30,240
check the slack for that and

00:33:27,919 --> 00:33:31,279
just wanted to say thank you again ellen

00:33:30,240 --> 00:33:32,880
for a great talk

00:33:31,279 --> 00:33:39,840
thanks for being with us thanks for

00:33:32,880 --> 00:33:39,840
having me

00:33:53,679 --> 00:33:55,760

YouTube URL: https://www.youtube.com/watch?v=98BkfAF4zHY


