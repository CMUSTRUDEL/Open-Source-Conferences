Title: Lightning Talk: Nick Pentreath - Native Dense Vector Scoring in Elasticsearch
Publication date: 2020-07-01
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	Lightning talk from Berlin Buzzwords | MICES | Haystack – Joint Event 2020
Captions: 
	00:00:07,740 --> 00:00:13,950
hello everyone welcome to the lightning

00:00:11,340 --> 00:00:17,250
talk session at Berlin buzzword 2020

00:00:13,950 --> 00:00:23,970
I'm your host a theorem and I would be

00:00:17,250 --> 00:00:26,000
your host for I would be hosting the

00:00:23,970 --> 00:00:28,320
lightning talk session tonight

00:00:26,000 --> 00:00:31,110
personally looking at the list of topics

00:00:28,320 --> 00:00:33,989
that we have for the sessions today I

00:00:31,110 --> 00:00:36,360
think I'm super excited and I look

00:00:33,989 --> 00:00:39,149
forward to each one of these talks I

00:00:36,360 --> 00:00:40,800
hope you guys have a good time just to

00:00:39,149 --> 00:00:42,750
let you know if you have any questions

00:00:40,800 --> 00:00:47,339
please keep them posting on in my sis

00:00:42,750 --> 00:00:50,309
life channel and I think without further

00:00:47,339 --> 00:00:56,039
delay let's welcome the first speaker

00:00:50,309 --> 00:01:01,050
for tonight Nick over to you thank you

00:00:56,039 --> 00:01:03,179
very much Peter and welcome to yeah this

00:01:01,050 --> 00:01:04,800
is very quick lightning talk on native

00:01:03,179 --> 00:01:07,260
Dean's vector scoring in elastics which

00:01:04,800 --> 00:01:09,420
it likes to be by my first lightning

00:01:07,260 --> 00:01:10,590
talk I normally do longer one so let's

00:01:09,420 --> 00:01:13,259
see if we can get through everything on

00:01:10,590 --> 00:01:15,360
time I'm ml Beck on Twitter our gets up

00:01:13,259 --> 00:01:18,180
and LinkedIn principal engineer working

00:01:15,360 --> 00:01:20,159
at IBM Center for open source data and

00:01:18,180 --> 00:01:22,530
AI technologies focus on machine

00:01:20,159 --> 00:01:24,930
learning and artificial intelligence of

00:01:22,530 --> 00:01:27,540
a principal software how we're a team of

00:01:24,930 --> 00:01:29,700
over opened three open source developers

00:01:27,540 --> 00:01:32,540
at IBM focusing on a wide variety of

00:01:29,700 --> 00:01:37,890
data and AI open source software

00:01:32,540 --> 00:01:40,680
frameworks and projects so we'll start

00:01:37,890 --> 00:01:43,920
just talking about what are vectors and

00:01:40,680 --> 00:01:46,860
you know Victor is essentially a ordered

00:01:43,920 --> 00:01:49,979
an indexed set of numbers very much like

00:01:46,860 --> 00:01:52,500
an array you have different you know

00:01:49,979 --> 00:01:54,329
gain some sparse vectors but the dense

00:01:52,500 --> 00:01:57,030
vectors we'll be talking about today are

00:01:54,329 --> 00:02:00,509
very much narrow and why did I matter

00:01:57,030 --> 00:02:04,649
they actually arise in many different

00:02:00,509 --> 00:02:09,210
scenarios so we can represent various

00:02:04,649 --> 00:02:13,170
things as vectors including images music

00:02:09,210 --> 00:02:15,390
movies it's used they can be used in

00:02:13,170 --> 00:02:19,340
e-commerce and recommendations social

00:02:15,390 --> 00:02:21,920
networks and even documents

00:02:19,340 --> 00:02:23,480
so one example that we're vectors are

00:02:21,920 --> 00:02:26,180
very common is in recommender systems

00:02:23,480 --> 00:02:29,330
and in recommenders we have a set of

00:02:26,180 --> 00:02:32,150
users and we have a set of items so for

00:02:29,330 --> 00:02:35,840
example in movie recommendations the set

00:02:32,150 --> 00:02:38,360
of items might be a list of the set of

00:02:35,840 --> 00:02:43,250
movies or videos and you can see here on

00:02:38,360 --> 00:02:45,769
the bottom left we represent the user

00:02:43,250 --> 00:02:49,670
ratings given to the set of movies as a

00:02:45,769 --> 00:02:51,319
matrix and it's it's a sparse matrix not

00:02:49,670 --> 00:02:55,370
every entry is fold so that means that

00:02:51,319 --> 00:02:58,069
might be user as rated every movie and a

00:02:55,370 --> 00:02:59,959
typical approach for doing recommended

00:02:58,069 --> 00:03:01,940
recommendation systems and recommender

00:02:59,959 --> 00:03:03,470
models is matrix factorization and that

00:03:01,940 --> 00:03:06,049
takes this matrix that we see on the

00:03:03,470 --> 00:03:09,620
left and it splits it up into two

00:03:06,049 --> 00:03:12,079
smaller matrices and first is a user

00:03:09,620 --> 00:03:15,799
matrix and a second is a movie or item

00:03:12,079 --> 00:03:17,540
matrix and each entry eat each column or

00:03:15,799 --> 00:03:19,340
row as it as the case may be in one of

00:03:17,540 --> 00:03:22,610
these matrix that matrices is actually a

00:03:19,340 --> 00:03:24,920
vector so it turns out that you create

00:03:22,610 --> 00:03:27,290
to compute a predicted rating for a

00:03:24,920 --> 00:03:30,319
movie and a user combination we just

00:03:27,290 --> 00:03:33,190
take a user vector and we perform a

00:03:30,319 --> 00:03:35,660
linear algebra operation dot product

00:03:33,190 --> 00:03:38,410
between the user vector and the item

00:03:35,660 --> 00:03:41,030
effective and as a part of this matrix

00:03:38,410 --> 00:03:43,310
and similarly if we want to find similar

00:03:41,030 --> 00:03:47,440
items which power things like products

00:03:43,310 --> 00:03:51,519
you may want to buy we do a similar

00:03:47,440 --> 00:03:53,209
cosine similarity so this looks very

00:03:51,519 --> 00:03:54,709
conceptually similar to you the way

00:03:53,209 --> 00:03:57,200
search ranking works we start with a

00:03:54,709 --> 00:03:59,090
query we represented as a term vector

00:03:57,200 --> 00:04:01,970
kind of primary term vector perhaps we

00:03:59,090 --> 00:04:05,299
compute a similarity and then we sort

00:04:01,970 --> 00:04:07,700
the results by similarity effectively so

00:04:05,299 --> 00:04:10,489
can we use the same set of machinery to

00:04:07,700 --> 00:04:13,790
to compute arbitrary vectors that are

00:04:10,489 --> 00:04:16,549
not necessarily the typical of search

00:04:13,790 --> 00:04:18,849
query vectors so for example in

00:04:16,549 --> 00:04:21,049
recommendations we have a user vector

00:04:18,849 --> 00:04:23,150
the dot product cosine similarity

00:04:21,049 --> 00:04:26,510
Machinery is quite quite similar to what

00:04:23,150 --> 00:04:28,550
we use in search ranking it's a scoring

00:04:26,510 --> 00:04:30,830
and ranking work but the term vectors

00:04:28,550 --> 00:04:31,700
don't necessarily work because the way

00:04:30,830 --> 00:04:34,700
they took that

00:04:31,700 --> 00:04:36,230
arrays and vectors are stored in the

00:04:34,700 --> 00:04:39,380
elastic surgeon you know natively up

00:04:36,230 --> 00:04:42,410
until now has has effectively been meant

00:04:39,380 --> 00:04:45,980
that the the array gets stored as an

00:04:42,410 --> 00:04:47,450
unordered set of numbers so we lose that

00:04:45,980 --> 00:04:50,120
ordering and we lose the ability to

00:04:47,450 --> 00:04:54,650
direct the operations we need on those

00:04:50,120 --> 00:04:57,440
vectors so here's a way around this and

00:04:54,650 --> 00:04:59,390
up until now a way to do this was to use

00:04:57,440 --> 00:05:02,180
a different representation for the term

00:04:59,390 --> 00:05:04,660
vectors and then use a custom plugin

00:05:02,180 --> 00:05:07,130
which would be your scoring function

00:05:04,660 --> 00:05:09,680
which would allow you to do things like

00:05:07,130 --> 00:05:11,960
dot products cosine similarities and

00:05:09,680 --> 00:05:13,880
other I would treat functions but

00:05:11,960 --> 00:05:17,530
there's the third require custom code

00:05:13,880 --> 00:05:21,710
and loading a custom plugin in Java and

00:05:17,530 --> 00:05:25,130
that adds complexity so definitely it

00:05:21,710 --> 00:05:27,830
works but it's not necessarily the easy

00:05:25,130 --> 00:05:29,420
way to do things and for example in an

00:05:27,830 --> 00:05:32,840
environments where you don't control

00:05:29,420 --> 00:05:34,400
necessarily the the cluster and you

00:05:32,840 --> 00:05:40,340
can't add plugins it makes it difficult

00:05:34,400 --> 00:05:43,910
to use so this has been solved recently

00:05:40,340 --> 00:05:46,220
in less elasticsearch 7 a native dance

00:05:43,910 --> 00:05:49,850
vector type effectively an ordered array

00:05:46,220 --> 00:05:51,320
stored as a binary with added in ES 7.0

00:05:49,850 --> 00:05:53,750
and then vector functions in

00:05:51,320 --> 00:05:55,130
seven-point-three built-in effects

00:05:53,750 --> 00:05:57,250
depict the functions of the type that we

00:05:55,130 --> 00:06:00,080
need for for doing things like

00:05:57,250 --> 00:06:02,150
similarity and scoring top rights and

00:06:00,080 --> 00:06:04,790
cosine similarity so you can see there

00:06:02,150 --> 00:06:06,890
that we've got some attains vector that

00:06:04,790 --> 00:06:09,340
we can we can have as a mapping you know

00:06:06,890 --> 00:06:13,550
properties and we just need to specify a

00:06:09,340 --> 00:06:15,440
dimension so this gives us everything we

00:06:13,550 --> 00:06:18,350
need if we compared to the what we had

00:06:15,440 --> 00:06:20,120
previously we can take a vector that

00:06:18,350 --> 00:06:21,830
represents let's say user what item in

00:06:20,120 --> 00:06:24,980
recommender systems or what image if we

00:06:21,830 --> 00:06:26,360
doing image search or documents that is

00:06:24,980 --> 00:06:28,730
the output of a machine learning model

00:06:26,360 --> 00:06:30,470
we'll be planning model and we can apply

00:06:28,730 --> 00:06:32,510
native scoring functions like top

00:06:30,470 --> 00:06:34,520
products and cosine similarities and so

00:06:32,510 --> 00:06:36,020
on and get our ranking and this is all

00:06:34,520 --> 00:06:37,840
now all built in we don't need to do

00:06:36,020 --> 00:06:41,810
anything funny

00:06:37,840 --> 00:06:44,050
so I'm going to very quickly try and

00:06:41,810 --> 00:06:44,050
show

00:06:45,520 --> 00:06:55,210
example of this and this is a

00:06:52,540 --> 00:06:59,440
elasticsearch spark recommender that

00:06:55,210 --> 00:07:01,670
I've credited for my work at IBM this

00:06:59,440 --> 00:07:04,490
very long notebook I won't go through

00:07:01,670 --> 00:07:07,310
all of it but works through hard to kind

00:07:04,490 --> 00:07:09,320
of load data into spark from realistic

00:07:07,310 --> 00:07:12,320
switching to spark run recommendations

00:07:09,320 --> 00:07:13,940
and then other movies using this exact

00:07:12,320 --> 00:07:15,410
functionality that we talked about so

00:07:13,940 --> 00:07:18,410
the key here that I just want to

00:07:15,410 --> 00:07:21,320
highlight is that as you see we can

00:07:18,410 --> 00:07:23,690
simply create a vector type for users

00:07:21,320 --> 00:07:25,700
which specifies the dense vector type in

00:07:23,690 --> 00:07:28,520
the dimension and similarly for for

00:07:25,700 --> 00:07:31,370
items or movies in this case and then

00:07:28,520 --> 00:07:33,470
all we have to do is actually plug

00:07:31,370 --> 00:07:36,560
straight into a standard script query

00:07:33,470 --> 00:07:41,260
let's go to scroll query where the score

00:07:36,560 --> 00:07:45,380
function is exactly cosine similarity or

00:07:41,260 --> 00:07:47,690
top product and that's it at the end of

00:07:45,380 --> 00:07:51,500
just using that functionality we have we

00:07:47,690 --> 00:07:57,999
get recommended movies and we get

00:07:51,500 --> 00:07:59,709
similar movies okay

00:07:57,999 --> 00:08:01,509
yes thank you for sticking with me for

00:07:59,709 --> 00:08:03,309
these probably a little bit more than

00:08:01,509 --> 00:08:04,959
five minutes now I encourage you to go

00:08:03,309 --> 00:08:07,659
and check out the code Padma I mentioned

00:08:04,959 --> 00:08:11,369
and check out code a.org and find me on

00:08:07,659 --> 00:08:11,369
Twitter github thank you

00:08:17,090 --> 00:08:19,150

YouTube URL: https://www.youtube.com/watch?v=vbyEYnEFgZQ


