Title: #bbuzz: Nick Pentreath - Scaling up Deep Learning by Scaling Down
Publication date: 2020-06-24
Playlist: Berlin Buzzwords | MICES | Haystack â€“ Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/scaling-deep-learning-scaling-down


In the last few years, deep learning has achieved dramatic success in a wide range of domains, including computer vision, artificial intelligence, speech recognition, natural language processing and reinforcement learning.

However, good performance comes at a significant computational cost. This makes scaling training expensive, but an even more pertinent issue is inference, in particular for real-time applications (where runtime latency is critical) and edge devices (where computational and storage resources may be limited).

This talk will explore common techniques and emerging advances for dealing with these challenges, including best practices for batching; quantization and other methods for trading off computational cost at training vs inference performance; architecture optimization and graph manipulation approaches.
Captions: 
	00:00:08,960 --> 00:00:13,040
hello everyone and welcome to this

00:00:10,400 --> 00:00:15,519
berlin buzzwords 2020 talk on scaling up

00:00:13,040 --> 00:00:17,680
deep learning by scaling down

00:00:15,519 --> 00:00:19,039
i'm nick pentrith i'm ml nick on twitter

00:00:17,680 --> 00:00:21,119
github linkedin

00:00:19,039 --> 00:00:22,960
i'm a principal engineer working at ibm

00:00:21,119 --> 00:00:24,480
center for open source data and ai

00:00:22,960 --> 00:00:26,080
technologies code

00:00:24,480 --> 00:00:27,599
where i work on machine learning and ai

00:00:26,080 --> 00:00:29,359
open source software

00:00:27,599 --> 00:00:31,119
i'm an apache spark committee and pmc

00:00:29,359 --> 00:00:33,120
member and author of machine learning

00:00:31,119 --> 00:00:35,280
with spark

00:00:33,120 --> 00:00:37,040
before we begin a little bit about code

00:00:35,280 --> 00:00:38,320
or the center for open source data and

00:00:37,040 --> 00:00:40,239
ai technologies

00:00:38,320 --> 00:00:41,920
we're a team of over 30 open source

00:00:40,239 --> 00:00:44,239
developers within ibm

00:00:41,920 --> 00:00:45,440
and we work on contributing to an

00:00:44,239 --> 00:00:46,800
advocating for

00:00:45,440 --> 00:00:48,879
open source projects that are

00:00:46,800 --> 00:00:50,239
foundational to ibm's data and ai

00:00:48,879 --> 00:00:51,840
product offerings

00:00:50,239 --> 00:00:54,719
this includes the python data science

00:00:51,840 --> 00:00:56,879
stack apache spark is a core component

00:00:54,719 --> 00:00:59,199
of this stack

00:00:56,879 --> 00:01:00,079
open exchanges for data and deep

00:00:59,199 --> 00:01:01,359
learning models

00:01:00,079 --> 00:01:03,280
deep learning frameworks including

00:01:01,359 --> 00:01:06,880
tensorflow and pytorch

00:01:03,280 --> 00:01:10,640
kubeflow ai fairness and ethics

00:01:06,880 --> 00:01:10,640
as well as open standards for model

00:01:10,840 --> 00:01:14,080
deployment

00:01:12,080 --> 00:01:16,080
today we'll start with a deep learning

00:01:14,080 --> 00:01:17,520
overview and discuss the computational

00:01:16,080 --> 00:01:18,799
challenges involved

00:01:17,520 --> 00:01:21,280
with training and deploying deep

00:01:18,799 --> 00:01:23,040
learning models we'll then look at three

00:01:21,280 --> 00:01:24,240
broad classes of approach

00:01:23,040 --> 00:01:26,400
for dealing with these challenges

00:01:24,240 --> 00:01:27,759
including model architectures model

00:01:26,400 --> 00:01:29,680
compression techniques

00:01:27,759 --> 00:01:31,920
and model distillation and then we'll

00:01:29,680 --> 00:01:33,600
wrap up with the conclusion

00:01:31,920 --> 00:01:36,079
we'll start with the basic machine

00:01:33,600 --> 00:01:38,400
learning workflow we start with data

00:01:36,079 --> 00:01:40,159
we analyze that data and we typically

00:01:38,400 --> 00:01:43,920
want to train a machine learning model

00:01:40,159 --> 00:01:45,680
using that data now

00:01:43,920 --> 00:01:47,520
our data typically doesn't arrive in a

00:01:45,680 --> 00:01:48,399
nicely packaged format ready for machine

00:01:47,520 --> 00:01:50,159
learning

00:01:48,399 --> 00:01:51,520
it arrives in a raw format and we need

00:01:50,159 --> 00:01:53,680
to convert it

00:01:51,520 --> 00:01:55,280
pre-process it and do feature

00:01:53,680 --> 00:01:57,200
transformation and extraction to get it

00:01:55,280 --> 00:02:00,399
into a form amenable to machine learning

00:01:57,200 --> 00:02:02,799
typically feature vectors and

00:02:00,399 --> 00:02:03,759
tensors we then train a machine learning

00:02:02,799 --> 00:02:06,799
model

00:02:03,759 --> 00:02:08,000
or deploy it to a live environment where

00:02:06,799 --> 00:02:11,120
it predicts

00:02:08,000 --> 00:02:12,080
on new data coming in and new data comes

00:02:11,120 --> 00:02:15,120
into the process

00:02:12,080 --> 00:02:18,000
really turning this workflow into a loop

00:02:15,120 --> 00:02:19,040
now as part of this workflow the three

00:02:18,000 --> 00:02:21,680
main areas that are

00:02:19,040 --> 00:02:23,120
compute intensive are processing data in

00:02:21,680 --> 00:02:26,080
particular training models

00:02:23,120 --> 00:02:26,080
and model deployment

00:02:26,720 --> 00:02:30,400
deep learning is a branch of machine

00:02:29,120 --> 00:02:31,840
learning that has been around for quite

00:02:30,400 --> 00:02:33,760
a number of years the original theory

00:02:31,840 --> 00:02:35,360
dates back to the 1940s

00:02:33,760 --> 00:02:36,560
and some of the computer models

00:02:35,360 --> 00:02:38,879
originated around the middle of the

00:02:36,560 --> 00:02:41,519
1960s and on the right here we can see

00:02:38,879 --> 00:02:44,319
an old perceptron machine a neural

00:02:41,519 --> 00:02:45,840
network from the 1960s

00:02:44,319 --> 00:02:47,360
neural networks fell out of favor during

00:02:45,840 --> 00:02:49,599
the 80s and 90s

00:02:47,360 --> 00:02:51,599
in part because of a lack of world

00:02:49,599 --> 00:02:53,840
success in applications

00:02:51,599 --> 00:02:57,360
and partly because of the inability to

00:02:53,840 --> 00:02:59,440
actually compute and train these models

00:02:57,360 --> 00:03:01,120
they've seen a recent resurgence gt3

00:02:59,440 --> 00:03:03,360
factors the first being bigger and

00:03:01,120 --> 00:03:05,440
better and more open data sets

00:03:03,360 --> 00:03:06,800
as mobile phones edge devices and

00:03:05,440 --> 00:03:08,080
internet scale data collection have

00:03:06,800 --> 00:03:09,760
really proliferated

00:03:08,080 --> 00:03:12,480
as well as standardized data sets for

00:03:09,760 --> 00:03:13,920
competition such as imagenet

00:03:12,480 --> 00:03:16,080
combined with this we've seen better

00:03:13,920 --> 00:03:17,840
hardware gpus and now

00:03:16,080 --> 00:03:19,599
custom hardware focused on deep learning

00:03:17,840 --> 00:03:21,360
applications such as tensor processing

00:03:19,599 --> 00:03:23,360
units tpus

00:03:21,360 --> 00:03:24,799
and the third leg is improvements to the

00:03:23,360 --> 00:03:26,560
algorithms architectures and

00:03:24,799 --> 00:03:28,080
optimization techniques and the software

00:03:26,560 --> 00:03:29,760
side

00:03:28,080 --> 00:03:31,280
and these combined have led to new

00:03:29,760 --> 00:03:34,239
state-of-the-art results across

00:03:31,280 --> 00:03:35,680
computer vision speech recognition

00:03:34,239 --> 00:03:36,799
natural language processing language

00:03:35,680 --> 00:03:40,799
translation

00:03:36,799 --> 00:03:43,040
and many more modern neural networks

00:03:40,799 --> 00:03:44,000
are really called deep learning because

00:03:43,040 --> 00:03:47,120
they are

00:03:44,000 --> 00:03:48,720
neural networks made up multiple layers

00:03:47,120 --> 00:03:50,799
and in computer vision convolutional

00:03:48,720 --> 00:03:52,480
neural networks are a core building

00:03:50,799 --> 00:03:53,439
block for state-of-the-art models

00:03:52,480 --> 00:03:55,280
and have been used in image

00:03:53,439 --> 00:03:56,640
classification object detection

00:03:55,280 --> 00:03:58,879
segmentation

00:03:56,640 --> 00:04:00,000
and many other applications for

00:03:58,879 --> 00:04:02,959
sequences and time

00:04:00,000 --> 00:04:05,120
series applications such as machine

00:04:02,959 --> 00:04:06,799
translation text generation

00:04:05,120 --> 00:04:09,519
recurrent neural nets have been

00:04:06,799 --> 00:04:11,680
extremely successful

00:04:09,519 --> 00:04:13,280
and in other natural language processing

00:04:11,680 --> 00:04:14,879
applications word embeddings

00:04:13,280 --> 00:04:16,880
transformers and attention mechanisms

00:04:14,879 --> 00:04:18,160
have seen success

00:04:16,880 --> 00:04:20,560
and finally modern deep learning

00:04:18,160 --> 00:04:21,199
frameworks provide computation graph

00:04:20,560 --> 00:04:23,520
abstraction

00:04:21,199 --> 00:04:24,880
automatic differentiation hardware

00:04:23,520 --> 00:04:27,840
acceleration support

00:04:24,880 --> 00:04:29,919
and high levels of flexibility allowing

00:04:27,840 --> 00:04:32,479
practitioners and researchers

00:04:29,919 --> 00:04:34,479
to create state-of-the-art models in a

00:04:32,479 --> 00:04:38,240
much easier fashion than previously

00:04:34,479 --> 00:04:38,240
instead of handcrafting solutions

00:04:39,600 --> 00:04:44,080
in the previous era of ai and deep

00:04:42,240 --> 00:04:46,160
learning applications

00:04:44,080 --> 00:04:48,639
the compute required roughly followed a

00:04:46,160 --> 00:04:50,800
two-year doubling time of moore's law

00:04:48,639 --> 00:04:52,080
but in the modern era as we've seen an

00:04:50,800 --> 00:04:55,280
explosion in

00:04:52,080 --> 00:04:56,479
very complex and large solutions that

00:04:55,280 --> 00:04:58,560
doubling time

00:04:56,479 --> 00:05:00,320
is something like three to four months

00:04:58,560 --> 00:05:02,479
and this really highlights that we need

00:05:00,320 --> 00:05:03,680
software-based solutions we cannot just

00:05:02,479 --> 00:05:05,919
simply rely

00:05:03,680 --> 00:05:08,479
on improved hardware to really solve

00:05:05,919 --> 00:05:08,479
these problems

00:05:09,039 --> 00:05:15,120
so we'll use an example today throughout

00:05:13,120 --> 00:05:16,960
and that is image classification which

00:05:15,120 --> 00:05:20,960
is a very common

00:05:16,960 --> 00:05:23,919
deep learning computer vision task

00:05:20,960 --> 00:05:25,520
we start with an input image we send

00:05:23,919 --> 00:05:27,520
that through our large neural network

00:05:25,520 --> 00:05:29,440
for inference

00:05:27,520 --> 00:05:31,680
and we get a prediction for the class

00:05:29,440 --> 00:05:31,680
out

00:05:31,919 --> 00:05:38,560
a very common and highly performance

00:05:36,240 --> 00:05:40,320
modern deep learning neural network for

00:05:38,560 --> 00:05:41,759
image classification is called inception

00:05:40,320 --> 00:05:44,720
v3

00:05:41,759 --> 00:05:45,840
and we can see here that the core model

00:05:44,720 --> 00:05:48,240
is made up of these

00:05:45,840 --> 00:05:49,600
convolutional blocks typically a

00:05:48,240 --> 00:05:50,639
convolution operator followed by

00:05:49,600 --> 00:05:52,479
normalization

00:05:50,639 --> 00:05:54,240
and then an activation function often

00:05:52,479 --> 00:05:55,759
rectified linear unit or something like

00:05:54,240 --> 00:05:57,280
that

00:05:55,759 --> 00:05:59,120
and we can actually boil this

00:05:57,280 --> 00:06:00,240
computation down to effectively a set of

00:05:59,120 --> 00:06:02,800
matrix multiplier

00:06:00,240 --> 00:06:04,479
and addition operations and if we look

00:06:02,800 --> 00:06:07,280
at the entire network we can see we have

00:06:04,479 --> 00:06:09,840
many many of these

00:06:07,280 --> 00:06:10,800
blocks that are sitting in the network

00:06:09,840 --> 00:06:12,720
and that

00:06:10,800 --> 00:06:14,560
if we count up all the parameters this

00:06:12,720 --> 00:06:16,560
inception model has 24 million

00:06:14,560 --> 00:06:20,000
parameters and achieves a 78.8 percent

00:06:16,560 --> 00:06:22,160
accuracy on imagenet

00:06:20,000 --> 00:06:24,080
if we look at the accuracy versus

00:06:22,160 --> 00:06:27,039
computational complexity

00:06:24,080 --> 00:06:27,840
of various networks the early models

00:06:27,039 --> 00:06:30,400
tended to have

00:06:27,840 --> 00:06:31,440
quite a lot of parameters not so much

00:06:30,400 --> 00:06:33,680
operations

00:06:31,440 --> 00:06:34,479
and achieve moderate performance we then

00:06:33,680 --> 00:06:37,680
moved into

00:06:34,479 --> 00:06:39,759
a new phase which was

00:06:37,680 --> 00:06:40,720
increasing the number of parameters and

00:06:39,759 --> 00:06:43,280
operational

00:06:40,720 --> 00:06:46,960
computational complexity of these models

00:06:43,280 --> 00:06:49,440
in order to move up the accuracy curve

00:06:46,960 --> 00:06:51,039
and then the next phase was starting to

00:06:49,440 --> 00:06:53,360
have more and more efficient

00:06:51,039 --> 00:06:54,240
network architectures so trying to

00:06:53,360 --> 00:06:56,400
achieve

00:06:54,240 --> 00:06:59,120
high accuracy with relatively fewer

00:06:56,400 --> 00:07:00,400
computations

00:06:59,120 --> 00:07:02,479
another way of looking at this is to

00:07:00,400 --> 00:07:04,960
look at the information density

00:07:02,479 --> 00:07:06,319
or level of computational efficiency in

00:07:04,960 --> 00:07:08,880
each of these networks

00:07:06,319 --> 00:07:09,680
and we can see that the models that

00:07:08,880 --> 00:07:11,919
achieve

00:07:09,680 --> 00:07:14,080
a relatively high accuracy for

00:07:11,919 --> 00:07:16,639
relatively low number of operations are

00:07:14,080 --> 00:07:17,599
clearly the most efficient they have a

00:07:16,639 --> 00:07:19,840
relatively high

00:07:17,599 --> 00:07:22,000
density or accuracy percentage per

00:07:19,840 --> 00:07:23,599
parameter effectively

00:07:22,000 --> 00:07:25,280
but in absolute terms these are still

00:07:23,599 --> 00:07:27,599
quite low numbers

00:07:25,280 --> 00:07:29,440
maxing out just below 12 this is really

00:07:27,599 --> 00:07:33,120
telling us that these large

00:07:29,440 --> 00:07:35,440
networks are in fact over parameterized

00:07:33,120 --> 00:07:38,160
and not very efficient in terms of their

00:07:35,440 --> 00:07:38,160
representation

00:07:39,680 --> 00:07:43,039
when you think about deep learning

00:07:40,880 --> 00:07:44,400
deployments typically on the model

00:07:43,039 --> 00:07:46,639
training side

00:07:44,400 --> 00:07:50,639
we use substantial hardware typically

00:07:46,639 --> 00:07:53,599
gpu or multi-gpu or clusters of gpu

00:07:50,639 --> 00:07:56,160
or other customer hardware or specific

00:07:53,599 --> 00:07:58,879
hardware like tpus

00:07:56,160 --> 00:08:00,560
and we can throw a lot of computes

00:07:58,879 --> 00:08:02,400
obviously at a cost

00:08:00,560 --> 00:08:04,080
at this training and when it comes to

00:08:02,400 --> 00:08:05,280
cloud-based deployment scenarios or

00:08:04,080 --> 00:08:07,680
on-premise

00:08:05,280 --> 00:08:09,680
deployment scenarios we can use similar

00:08:07,680 --> 00:08:12,160
hardware to deploy models

00:08:09,680 --> 00:08:13,360
and we can then trade off cost versus

00:08:12,160 --> 00:08:15,360
performance

00:08:13,360 --> 00:08:17,039
if we want to use more gpus we can just

00:08:15,360 --> 00:08:18,800
throw some more

00:08:17,039 --> 00:08:20,960
potentially some more money at the

00:08:18,800 --> 00:08:23,360
problem but when it comes to

00:08:20,960 --> 00:08:25,199
deploying into edge devices we cannot

00:08:23,360 --> 00:08:27,520
just simply do that

00:08:25,199 --> 00:08:29,120
edge devices are very diverse and have

00:08:27,520 --> 00:08:29,440
far more limited resources in general

00:08:29,120 --> 00:08:31,680
than

00:08:29,440 --> 00:08:32,800
available on cloud or on-premise

00:08:31,680 --> 00:08:34,479
hardware

00:08:32,800 --> 00:08:36,240
they have limited memory and that memory

00:08:34,479 --> 00:08:37,599
is not all available to our application

00:08:36,240 --> 00:08:40,479
we typically have to compete with other

00:08:37,599 --> 00:08:42,240
applications on the device similarly

00:08:40,479 --> 00:08:44,320
with compute there may be

00:08:42,240 --> 00:08:45,440
limited computes there may be some

00:08:44,320 --> 00:08:47,920
fairly powerful

00:08:45,440 --> 00:08:48,800
mobile gpus or edge gpus but we still

00:08:47,920 --> 00:08:51,279
are needing to

00:08:48,800 --> 00:08:52,320
compete with other applications in terms

00:08:51,279 --> 00:08:54,560
of a

00:08:52,320 --> 00:08:56,080
computational resource and finally

00:08:54,560 --> 00:08:56,640
network bandwidth can be extremely

00:08:56,080 --> 00:09:00,320
limited

00:08:56,640 --> 00:09:02,399
and variable so we cannot just deploy

00:09:00,320 --> 00:09:03,440
large powerful models in order to get

00:09:02,399 --> 00:09:06,000
accuracy in these

00:09:03,440 --> 00:09:08,959
scenarios we need to be cognizant of

00:09:06,000 --> 00:09:09,200
memory footprint we need to be cognizant

00:09:08,959 --> 00:09:11,279
of

00:09:09,200 --> 00:09:12,480
the compute efficiency and the latency

00:09:11,279 --> 00:09:14,160
involved

00:09:12,480 --> 00:09:15,519
and the latency requirements from our

00:09:14,160 --> 00:09:18,320
users

00:09:15,519 --> 00:09:21,120
and in fact getting the model onto the

00:09:18,320 --> 00:09:22,640
device can be a problem due to bandwidth

00:09:21,120 --> 00:09:24,720
so many of these considerations also

00:09:22,640 --> 00:09:25,839
apply to low latency applications such

00:09:24,720 --> 00:09:28,160
as

00:09:25,839 --> 00:09:30,160
high frequency trading financial

00:09:28,160 --> 00:09:31,680
applications

00:09:30,160 --> 00:09:33,600
programmatic trading and advertising

00:09:31,680 --> 00:09:37,279
where we have to make decisions

00:09:33,600 --> 00:09:39,120
in very much real time

00:09:37,279 --> 00:09:41,120
and that could be low single-digit

00:09:39,120 --> 00:09:41,920
milliseconds even microsecond latency

00:09:41,120 --> 00:09:44,160
requirements

00:09:41,920 --> 00:09:45,680
so again we can't just have a huge model

00:09:44,160 --> 00:09:47,600
that takes a long time to compute even

00:09:45,680 --> 00:09:49,600
if it's highly accurate

00:09:47,600 --> 00:09:51,200
so how do we improve the performance

00:09:49,600 --> 00:09:52,880
efficiency in order to meet these

00:09:51,200 --> 00:09:55,200
requirements

00:09:52,880 --> 00:09:57,920
we'll discuss four approaches today the

00:09:55,200 --> 00:10:00,560
first is just improving architectures

00:09:57,920 --> 00:10:01,519
second is and third compression

00:10:00,560 --> 00:10:03,920
techniques for models

00:10:01,519 --> 00:10:07,519
model pruning and quantization and the

00:10:03,920 --> 00:10:09,360
final one is model distillation

00:10:07,519 --> 00:10:11,440
the first thing we can do is potentially

00:10:09,360 --> 00:10:12,560
try to make networks more efficient in

00:10:11,440 --> 00:10:14,000
the way that they are designed and

00:10:12,560 --> 00:10:17,200
indeed this has been

00:10:14,000 --> 00:10:19,200
a key focus of research recently so if

00:10:17,200 --> 00:10:22,640
we look at the basic inception model

00:10:19,200 --> 00:10:24,160
on the left we can see that as we saw

00:10:22,640 --> 00:10:26,640
the standard convolutional building

00:10:24,160 --> 00:10:28,240
blocks is what makes up this model

00:10:26,640 --> 00:10:29,680
and on the right we have mobile net

00:10:28,240 --> 00:10:31,440
version one which is

00:10:29,680 --> 00:10:33,600
one of the more famous recent

00:10:31,440 --> 00:10:35,279
specialized architectures targeting

00:10:33,600 --> 00:10:36,959
edge devices and other low resource

00:10:35,279 --> 00:10:38,560
environments

00:10:36,959 --> 00:10:40,000
and the key difference here is that the

00:10:38,560 --> 00:10:42,560
building block is no longer

00:10:40,000 --> 00:10:44,000
a standard convolutional block but

00:10:42,560 --> 00:10:44,959
instead a depth wise convolutional

00:10:44,000 --> 00:10:46,480
building block

00:10:44,959 --> 00:10:48,800
so this is effectively splitting the

00:10:46,480 --> 00:10:50,160
convolutional operator into a depth-wise

00:10:48,800 --> 00:10:53,680
convolution followed by

00:10:50,160 --> 00:10:54,079
another convolution and this is leads to

00:10:53,680 --> 00:10:56,000
about

00:10:54,079 --> 00:10:57,519
eight times less computation taking

00:10:56,000 --> 00:10:59,600
place with

00:10:57,519 --> 00:11:01,040
giving up a little bit of accuracy so if

00:10:59,600 --> 00:11:02,640
we saw that the image

00:11:01,040 --> 00:11:04,560
inception model had 24 million

00:11:02,640 --> 00:11:08,240
parameters with a 78.8

00:11:04,560 --> 00:11:10,560
accuracy then the mobilenet model has

00:11:08,240 --> 00:11:12,839
over 80 percent fewer parameters and we

00:11:10,560 --> 00:11:15,360
give up about eight percent

00:11:12,839 --> 00:11:18,640
accuracy

00:11:15,360 --> 00:11:19,200
another key advance in in this area of

00:11:18,640 --> 00:11:22,160
research

00:11:19,200 --> 00:11:22,720
is to set up the core backbone of the

00:11:22,160 --> 00:11:25,839
network

00:11:22,720 --> 00:11:28,160
in such a way that it can be scaled so

00:11:25,839 --> 00:11:31,360
this allows us to scale

00:11:28,160 --> 00:11:32,079
mobilenet for example to be thinner or

00:11:31,360 --> 00:11:34,959
wider

00:11:32,079 --> 00:11:37,360
so we can scale the number of parameters

00:11:34,959 --> 00:11:38,720
at each layer and we can also scale the

00:11:37,360 --> 00:11:40,800
resolution of the input image

00:11:38,720 --> 00:11:41,680
representation and this allows us to

00:11:40,800 --> 00:11:43,680
target

00:11:41,680 --> 00:11:45,760
the environment that we want to so if we

00:11:43,680 --> 00:11:47,040
have a an environment with plenty of

00:11:45,760 --> 00:11:48,800
compute available

00:11:47,040 --> 00:11:50,880
we can scale that up and get more

00:11:48,800 --> 00:11:52,079
accuracy if we have a much more resource

00:11:50,880 --> 00:11:53,839
constrained environment

00:11:52,079 --> 00:11:55,519
we can scale it down and give up

00:11:53,839 --> 00:11:59,279
accuracy but still be able to run

00:11:55,519 --> 00:12:01,519
in that environment so this idea was

00:11:59,279 --> 00:12:02,399
taken through to mobilenetv2 which is an

00:12:01,519 --> 00:12:03,920
improvement

00:12:02,399 --> 00:12:06,079
it uses the same depth wise

00:12:03,920 --> 00:12:08,240
convolutional backbone but add some

00:12:06,079 --> 00:12:10,399
some further algorithmic and network

00:12:08,240 --> 00:12:11,200
tricks linear bottlenecks and shortcut

00:12:10,399 --> 00:12:12,399
connections

00:12:11,200 --> 00:12:14,320
and effectively we're just trying to

00:12:12,399 --> 00:12:15,440
move up the curve here more more to the

00:12:14,320 --> 00:12:18,160
upper left

00:12:15,440 --> 00:12:19,839
part of this this chart and again we can

00:12:18,160 --> 00:12:22,720
scale to

00:12:19,839 --> 00:12:24,720
to higher latency and higher accuracy or

00:12:22,720 --> 00:12:26,079
lower latency and low accuracy

00:12:24,720 --> 00:12:27,360
depending on the environment that we're

00:12:26,079 --> 00:12:29,920
in and the requirements of the

00:12:27,360 --> 00:12:29,920
application

00:12:30,079 --> 00:12:33,360
so mobile mid v2 gives us roughly the

00:12:32,000 --> 00:12:35,920
same number of parameters

00:12:33,360 --> 00:12:37,279
for an extra couple of percent accuracy

00:12:35,920 --> 00:12:38,800
of course measured on

00:12:37,279 --> 00:12:41,360
the standard benchmark dataset of

00:12:38,800 --> 00:12:41,360
imagenet

00:12:42,160 --> 00:12:46,320
so you can see here that these model

00:12:44,880 --> 00:12:49,040
classes

00:12:46,320 --> 00:12:50,480
are trying to achieve a much higher

00:12:49,040 --> 00:12:52,320
efficiency

00:12:50,480 --> 00:12:54,720
so they may not be the best performers

00:12:52,320 --> 00:12:57,440
but they are very strong performers with

00:12:54,720 --> 00:13:00,079
very low parameters uh number of

00:12:57,440 --> 00:13:02,399
parameters and computational overhead

00:13:00,079 --> 00:13:03,519
and indeed we can see that the class of

00:13:02,399 --> 00:13:06,000
mobile nets

00:13:03,519 --> 00:13:07,519
as well as other efficient backbone

00:13:06,000 --> 00:13:08,480
based architectures such as shuffle and

00:13:07,519 --> 00:13:10,560
squeeze net

00:13:08,480 --> 00:13:12,000
are much more efficient in terms of the

00:13:10,560 --> 00:13:13,920
information density within the

00:13:12,000 --> 00:13:17,120
parameters

00:13:13,920 --> 00:13:20,800
still these numbers are around the

00:13:17,120 --> 00:13:23,920
20 to 50 percent mark so there's clearly

00:13:20,800 --> 00:13:23,920
still work that can be done

00:13:24,720 --> 00:13:28,880
another recent advance which has become

00:13:27,440 --> 00:13:30,399
more and more popular is the use of

00:13:28,880 --> 00:13:32,880
neural architecture research

00:13:30,399 --> 00:13:35,120
to find these backbone models so the

00:13:32,880 --> 00:13:37,440
idea here is to effectively let

00:13:35,120 --> 00:13:40,480
deep learning do some of the work and to

00:13:37,440 --> 00:13:42,720
search the space of available

00:13:40,480 --> 00:13:44,800
network architectures in order to find

00:13:42,720 --> 00:13:46,959
the the best backbone architecture that

00:13:44,800 --> 00:13:48,639
can then be scaled up and down

00:13:46,959 --> 00:13:50,160
and this is done by optimizing both for

00:13:48,639 --> 00:13:50,639
accuracy as well as efficiency in the

00:13:50,160 --> 00:13:52,959
term of

00:13:50,639 --> 00:13:54,079
in the form of floating point operations

00:13:52,959 --> 00:13:56,720
efficientnet is one

00:13:54,079 --> 00:13:58,160
recent example of this and we can

00:13:56,720 --> 00:14:00,959
effectively

00:13:58,160 --> 00:14:02,800
outperform previous models and scale it

00:14:00,959 --> 00:14:05,199
from the b0 model

00:14:02,800 --> 00:14:07,040
which has a few more parameters than a

00:14:05,199 --> 00:14:08,800
mobile net for example but achieves

00:14:07,040 --> 00:14:10,399
about five percent more accuracy

00:14:08,800 --> 00:14:11,920
and we can scale that up all the way

00:14:10,399 --> 00:14:14,160
through to the b7

00:14:11,920 --> 00:14:15,760
which is effectively still the same

00:14:14,160 --> 00:14:18,480
architecture just much bigger

00:14:15,760 --> 00:14:20,240
and deeper and that goes up to 60

00:14:18,480 --> 00:14:20,639
million parameters and gives us a boost

00:14:20,240 --> 00:14:24,079
of

00:14:20,639 --> 00:14:25,839
up to 84.5 accuracy

00:14:24,079 --> 00:14:28,399
the same idea has been applied to the

00:14:25,839 --> 00:14:30,240
mobile net architecture for example so

00:14:28,399 --> 00:14:31,680
again applying a neural architecture

00:14:30,240 --> 00:14:34,720
search paradigm

00:14:31,680 --> 00:14:35,120
that is hardware aware so targeting both

00:14:34,720 --> 00:14:38,399
the

00:14:35,120 --> 00:14:38,399
performance and the efficiency

00:14:38,480 --> 00:14:43,360
and here we can get a network with about

00:14:41,519 --> 00:14:46,000
five million parameters

00:14:43,360 --> 00:14:46,720
and sort of three roughly three percent

00:14:46,000 --> 00:14:51,279
increase

00:14:46,720 --> 00:14:51,279
in accuracy versus the old mobile nets

00:14:52,079 --> 00:14:54,959
now a key challenge with neural

00:14:53,440 --> 00:14:56,880
architecture search is that it requires

00:14:54,959 --> 00:14:58,560
a huge amount of computational resources

00:14:56,880 --> 00:14:59,680
in order to find the best architecture

00:14:58,560 --> 00:15:02,320
and in fact to train

00:14:59,680 --> 00:15:04,320
each different architectural sub

00:15:02,320 --> 00:15:07,760
architecture

00:15:04,320 --> 00:15:09,279
now this is also the case for

00:15:07,760 --> 00:15:11,279
manual design if you're trying to

00:15:09,279 --> 00:15:12,399
manually design these networks and these

00:15:11,279 --> 00:15:14,560
backbone networks

00:15:12,399 --> 00:15:16,800
there's a lot of effort involved a lot

00:15:14,560 --> 00:15:19,839
of testing a lot of training

00:15:16,800 --> 00:15:21,680
and a lot of experimentation so

00:15:19,839 --> 00:15:22,959
the question asked by some researchers

00:15:21,680 --> 00:15:26,079
at mit in the

00:15:22,959 --> 00:15:29,199
ibm mit data lab is can

00:15:26,079 --> 00:15:31,440
we train one network to do all of this

00:15:29,199 --> 00:15:33,519
for us and this is the idea behind once

00:15:31,440 --> 00:15:34,959
for all train one network and specialize

00:15:33,519 --> 00:15:36,560
it for efficient deployment

00:15:34,959 --> 00:15:38,560
so the idea is to train one large

00:15:36,560 --> 00:15:40,240
network once

00:15:38,560 --> 00:15:42,000
and then be able to effectively cherry

00:15:40,240 --> 00:15:45,040
pick out the sub-network

00:15:42,000 --> 00:15:45,600
targeting a specific environment or each

00:15:45,040 --> 00:15:48,079
device

00:15:45,600 --> 00:15:50,000
or operating system or hardware

00:15:48,079 --> 00:15:52,240
accelerator for example

00:15:50,000 --> 00:15:54,399
and in this way we have a much more

00:15:52,240 --> 00:15:56,160
efficient mechanism for

00:15:54,399 --> 00:15:58,160
achieving the same result and still

00:15:56,160 --> 00:16:00,880
being able to

00:15:58,160 --> 00:16:02,160
target each one of these environments

00:16:00,880 --> 00:16:04,639
and trading off

00:16:02,160 --> 00:16:05,680
the efficiency versus and the

00:16:04,639 --> 00:16:08,399
computational

00:16:05,680 --> 00:16:12,000
considerations versus the accuracy but

00:16:08,399 --> 00:16:12,000
still achieve state of the art results

00:16:15,120 --> 00:16:18,880
so the second approach general approach

00:16:18,079 --> 00:16:21,600
that we'll look at

00:16:18,880 --> 00:16:23,839
is that of trying to compress the model

00:16:21,600 --> 00:16:25,600
we've seen effectively that

00:16:23,839 --> 00:16:27,279
many of these models and especially the

00:16:25,600 --> 00:16:30,000
large networks are

00:16:27,279 --> 00:16:30,880
effectively over parameterized a lot of

00:16:30,000 --> 00:16:32,399
the

00:16:30,880 --> 00:16:34,160
weights in there are maybe not that

00:16:32,399 --> 00:16:36,079
important so

00:16:34,160 --> 00:16:37,759
we have to ask the question can we

00:16:36,079 --> 00:16:39,199
perhaps take some of these weights out

00:16:37,759 --> 00:16:41,120
of the model

00:16:39,199 --> 00:16:43,199
but still have the model perform pretty

00:16:41,120 --> 00:16:45,839
well and this is the idea behind

00:16:43,199 --> 00:16:47,440
model pruning the idea is to reduce the

00:16:45,839 --> 00:16:49,199
number of model parameters by removing

00:16:47,440 --> 00:16:50,880
the ones that are not important

00:16:49,199 --> 00:16:52,880
in fact in other words the ones that are

00:16:50,880 --> 00:16:54,480
have a small impact on prediction

00:16:52,880 --> 00:16:56,639
this is very similar to the idea of

00:16:54,480 --> 00:16:58,720
regularization with the l1 norm

00:16:56,639 --> 00:17:00,320
we want to shrink down small weights

00:16:58,720 --> 00:17:01,199
that have little impact on the

00:17:00,320 --> 00:17:03,759
prediction

00:17:01,199 --> 00:17:05,039
and set them to zero if we can set

00:17:03,759 --> 00:17:06,720
weights to zero

00:17:05,039 --> 00:17:08,880
then we can effectively ignore them we

00:17:06,720 --> 00:17:10,959
can ignore them when we save the model

00:17:08,880 --> 00:17:13,280
so that gives us a much smaller size on

00:17:10,959 --> 00:17:15,439
disk and across the wire for

00:17:13,280 --> 00:17:16,959
sending the model back and forth so we

00:17:15,439 --> 00:17:19,280
compress it in that form

00:17:16,959 --> 00:17:20,240
and it can also give us lower latency if

00:17:19,280 --> 00:17:22,559
we

00:17:20,240 --> 00:17:24,240
can do the compute in a sparsity aware

00:17:22,559 --> 00:17:26,799
manner

00:17:24,240 --> 00:17:28,319
so there are two broad types of pruning

00:17:26,799 --> 00:17:29,280
the first is one shot pruning which is

00:17:28,319 --> 00:17:32,640
effectively

00:17:29,280 --> 00:17:35,039
doing this process post training so you

00:17:32,640 --> 00:17:36,480
run through the network once and you

00:17:35,039 --> 00:17:38,400
prune the weights

00:17:36,480 --> 00:17:40,480
and hopefully at the end you get a more

00:17:38,400 --> 00:17:42,320
efficient model that can still perform

00:17:40,480 --> 00:17:44,080
reasonably well and the second is

00:17:42,320 --> 00:17:45,760
iterative pruning

00:17:44,080 --> 00:17:48,080
and the idea here is to prune and then

00:17:45,760 --> 00:17:50,960
retrain and this follows a schedule

00:17:48,080 --> 00:17:52,480
an iterative approach so step one is to

00:17:50,960 --> 00:17:53,840
do the pruning drop the least important

00:17:52,480 --> 00:17:57,039
weights

00:17:53,840 --> 00:17:59,600
and then retrain and

00:17:57,039 --> 00:18:01,360
then go through and drop the the least

00:17:59,600 --> 00:18:02,880
important weights and retrain and so on

00:18:01,360 --> 00:18:05,120
and so on

00:18:02,880 --> 00:18:06,960
and the idea would be to target a

00:18:05,120 --> 00:18:07,679
specific sparsity that you want to

00:18:06,960 --> 00:18:10,880
achieve

00:18:07,679 --> 00:18:12,320
or a computational budget that you want

00:18:10,880 --> 00:18:14,160
to be able to fit into

00:18:12,320 --> 00:18:16,080
and stop the pruning when either of

00:18:14,160 --> 00:18:18,960
those or when one of those

00:18:16,080 --> 00:18:18,960
conditions is met

00:18:19,840 --> 00:18:23,520
now what's quite interesting is that we

00:18:22,880 --> 00:18:25,600
can actually

00:18:23,520 --> 00:18:26,720
achieve quite a high level of sparsity

00:18:25,600 --> 00:18:30,000
via model pruning

00:18:26,720 --> 00:18:32,160
without giving up much so this is for

00:18:30,000 --> 00:18:33,039
imagenet based image classification

00:18:32,160 --> 00:18:35,919
models

00:18:33,039 --> 00:18:37,360
from the tensorflow model optimization

00:18:35,919 --> 00:18:40,080
library docs

00:18:37,360 --> 00:18:40,480
and as you can see here in an inception

00:18:40,080 --> 00:18:42,720
model

00:18:40,480 --> 00:18:44,160
in fact both for inception and mobilenet

00:18:42,720 --> 00:18:46,559
we can achieve a 50

00:18:44,160 --> 00:18:48,320
sparsity by uh while giving up a very

00:18:46,559 --> 00:18:49,679
very small amount of accuracy

00:18:48,320 --> 00:18:51,760
so that really is showing us that these

00:18:49,679 --> 00:18:54,240
models are over parameterized and

00:18:51,760 --> 00:18:56,960
there's a lot of effectively superfluous

00:18:54,240 --> 00:19:00,559
information in their superfluous weights

00:18:56,960 --> 00:19:02,480
now if we want to to further trade off

00:19:00,559 --> 00:19:04,799
accuracy and get a much smaller model

00:19:02,480 --> 00:19:06,240
then we can make models more and more as

00:19:04,799 --> 00:19:07,919
fast do more and more pruning

00:19:06,240 --> 00:19:09,360
and move down this curve to the right

00:19:07,919 --> 00:19:10,559
getting a smaller model but at that

00:19:09,360 --> 00:19:13,520
point starting to give up

00:19:10,559 --> 00:19:14,880
accuracy and this doesn't just work for

00:19:13,520 --> 00:19:16,320
image classification

00:19:14,880 --> 00:19:18,559
here's an example from language

00:19:16,320 --> 00:19:20,960
translation as you can see here we can

00:19:18,559 --> 00:19:22,480
in fact achieve a very high level of

00:19:20,960 --> 00:19:25,200
sparsity while in fact

00:19:22,480 --> 00:19:26,400
getting a very small gain in our model

00:19:25,200 --> 00:19:28,559
performance

00:19:26,400 --> 00:19:29,440
and thereafter again we're trading off

00:19:28,559 --> 00:19:32,240
performance for

00:19:29,440 --> 00:19:33,200
for model size so this is indicating

00:19:32,240 --> 00:19:35,600
that

00:19:33,200 --> 00:19:37,039
model pruning is playing almost a

00:19:35,600 --> 00:19:40,720
regularization role

00:19:37,039 --> 00:19:42,880
and again these large models for

00:19:40,720 --> 00:19:46,640
natural language processing tasks are

00:19:42,880 --> 00:19:46,640
also very much over parameterized

00:19:46,799 --> 00:19:53,280
the next model compression technique

00:19:48,400 --> 00:19:55,679
we'll discuss is called quantization

00:19:53,280 --> 00:19:57,520
the idea behind quantization is that

00:19:55,679 --> 00:20:00,480
most deep learning computation uses

00:19:57,520 --> 00:20:02,000
32-bit or even 64-bit floating point

00:20:00,480 --> 00:20:03,919
numbers

00:20:02,000 --> 00:20:05,520
and quantization reduces the numerical

00:20:03,919 --> 00:20:06,159
precision of the weights and the

00:20:05,520 --> 00:20:08,559
operator

00:20:06,159 --> 00:20:09,760
operations on that weight by binning

00:20:08,559 --> 00:20:12,799
values

00:20:09,760 --> 00:20:13,280
so if we start with a 32-bit floating

00:20:12,799 --> 00:20:16,159
point

00:20:13,280 --> 00:20:18,840
representation we want to effectively

00:20:16,159 --> 00:20:20,080
turn that into a much sparser

00:20:18,840 --> 00:20:21,600
representation

00:20:20,080 --> 00:20:23,120
takes up a lot less size and if you can

00:20:21,600 --> 00:20:25,600
go down from 32 bits

00:20:23,120 --> 00:20:26,559
to say 16 bits we're making a

00:20:25,600 --> 00:20:30,320
effectively

00:20:26,559 --> 00:20:32,640
two times a saving on the size

00:20:30,320 --> 00:20:34,400
so the idea of behind binning is that if

00:20:32,640 --> 00:20:38,240
we look at the distribution of the

00:20:34,400 --> 00:20:40,640
weight values we can effectively try and

00:20:38,240 --> 00:20:43,039
approximate those weight values by a

00:20:40,640 --> 00:20:48,400
much smaller number of pins

00:20:43,039 --> 00:20:52,080
now the main complexity here

00:20:48,400 --> 00:20:55,679
is in dealing with overflow so as we

00:20:52,080 --> 00:20:59,280
turn the computation and the

00:20:55,679 --> 00:20:59,280
weight representation to smaller

00:20:59,360 --> 00:21:03,520
precision representations in particular

00:21:02,480 --> 00:21:06,320
when accumulating

00:21:03,520 --> 00:21:08,080
things like gradients during training

00:21:06,320 --> 00:21:09,039
these can be very very small numbers and

00:21:08,080 --> 00:21:10,880
small updates

00:21:09,039 --> 00:21:12,559
so we risk overflowing the

00:21:10,880 --> 00:21:14,880
representation so the

00:21:12,559 --> 00:21:16,400
the the key solution here is to use some

00:21:14,880 --> 00:21:18,960
sort of intermediate

00:21:16,400 --> 00:21:20,559
representation for storage of things

00:21:18,960 --> 00:21:22,720
such as gradient updates

00:21:20,559 --> 00:21:23,760
and accumulations so we can use a larger

00:21:22,720 --> 00:21:26,320
size for example

00:21:23,760 --> 00:21:27,520
32-bit integer or something like that

00:21:26,320 --> 00:21:29,440
during that process

00:21:27,520 --> 00:21:31,039
and then re-quantize it at the end to

00:21:29,440 --> 00:21:31,919
eight bits or throw it away if we don't

00:21:31,039 --> 00:21:34,320
need it

00:21:31,919 --> 00:21:36,240
for example after training so popular

00:21:34,320 --> 00:21:37,200
targets for quantization are 16-bit

00:21:36,240 --> 00:21:40,640
floating point

00:21:37,200 --> 00:21:40,640
and eight-bit integer coding

00:21:41,520 --> 00:21:44,559
there are two types of quantization you

00:21:43,200 --> 00:21:47,200
can do this post training

00:21:44,559 --> 00:21:48,720
very much similar to model pruning where

00:21:47,200 --> 00:21:51,200
you take a pre-trained model

00:21:48,720 --> 00:21:54,000
and you just apply a quantization for

00:21:51,200 --> 00:21:56,720
effectively for influence only

00:21:54,000 --> 00:21:58,080
so this is useful if you can't retrain

00:21:56,720 --> 00:22:00,159
the model for whatever reason

00:21:58,080 --> 00:22:01,600
or it's not computationally feasible

00:22:00,159 --> 00:22:05,120
perhaps to retrain the model

00:22:01,600 --> 00:22:07,840
seeing as it's a very large model

00:22:05,120 --> 00:22:09,360
model and very complex and expensive to

00:22:07,840 --> 00:22:11,200
train

00:22:09,360 --> 00:22:12,480
so in this case you typically would give

00:22:11,200 --> 00:22:14,480
up accuracy

00:22:12,480 --> 00:22:15,679
and most of the targets are typically

00:22:14,480 --> 00:22:18,400
going to be float

00:22:15,679 --> 00:22:21,200
16 16 bit floating point a dynamic range

00:22:18,400 --> 00:22:22,960
quantization or 8 bit integer

00:22:21,200 --> 00:22:24,880
the second approach is to do it in a

00:22:22,960 --> 00:22:27,280
training aware manner

00:22:24,880 --> 00:22:28,720
so this is much more complex and for

00:22:27,280 --> 00:22:29,200
example as we mentioned you have to deal

00:22:28,720 --> 00:22:32,320
with

00:22:29,200 --> 00:22:32,640
overflows potential overflows but this

00:22:32,320 --> 00:22:34,720
can

00:22:32,640 --> 00:22:36,480
really provide a huge efficiency gain

00:22:34,720 --> 00:22:38,840
with effectively

00:22:36,480 --> 00:22:41,280
minimal or close to zero loss in

00:22:38,840 --> 00:22:43,280
accuracy so here on the right we see

00:22:41,280 --> 00:22:44,799
an example of this for image

00:22:43,280 --> 00:22:45,760
classification models again from the

00:22:44,799 --> 00:22:49,120
tensorflow

00:22:45,760 --> 00:22:51,039
model optimization documentation

00:22:49,120 --> 00:22:52,640
and for the inception model we can see

00:22:51,039 --> 00:22:54,000
that in fact both

00:22:52,640 --> 00:22:56,640
post training and training aware

00:22:54,000 --> 00:22:59,120
quantization give a pretty similar

00:22:56,640 --> 00:23:01,120
accuracy degradation whereas for

00:22:59,120 --> 00:23:02,080
mobilenet the post training quantization

00:23:01,120 --> 00:23:04,559
results in a much

00:23:02,080 --> 00:23:06,480
larger degradation and accuracy but if

00:23:04,559 --> 00:23:10,000
we do training aware quantization we

00:23:06,480 --> 00:23:11,919
give up very little so this again is

00:23:10,000 --> 00:23:14,080
showing us that very large models very

00:23:11,919 --> 00:23:16,799
complex models like conception

00:23:14,080 --> 00:23:18,080
are effectively somewhat over

00:23:16,799 --> 00:23:20,080
parametrized they've got a lot of

00:23:18,080 --> 00:23:22,240
superfluous information in there

00:23:20,080 --> 00:23:23,120
because if we are reducing the precision

00:23:22,240 --> 00:23:25,520
and effectively

00:23:23,120 --> 00:23:27,600
increasing the level of approximation in

00:23:25,520 --> 00:23:29,440
the weights and the operations

00:23:27,600 --> 00:23:31,200
it's actually not having much impact for

00:23:29,440 --> 00:23:33,440
a much more efficient and sort of leaner

00:23:31,200 --> 00:23:35,440
model like a mobile net

00:23:33,440 --> 00:23:36,559
they are much more informationally dense

00:23:35,440 --> 00:23:38,640
so adding

00:23:36,559 --> 00:23:39,679
further levels of approximation

00:23:38,640 --> 00:23:41,919
effectively

00:23:39,679 --> 00:23:44,799
into the weights and operations is going

00:23:41,919 --> 00:23:44,799
to have more impact

00:23:45,919 --> 00:23:50,640
we can see here the impact on latency

00:23:48,159 --> 00:23:53,360
and model size

00:23:50,640 --> 00:23:54,960
so in some cases post-training

00:23:53,360 --> 00:23:56,480
quantization might even might in fact

00:23:54,960 --> 00:23:58,080
increase latency as we see here for

00:23:56,480 --> 00:23:59,840
mobilenet

00:23:58,080 --> 00:24:01,440
but in in both cases we get a

00:23:59,840 --> 00:24:04,559
significant 75

00:24:01,440 --> 00:24:06,000
reduction in the model size and for very

00:24:04,559 --> 00:24:09,039
large models typically

00:24:06,000 --> 00:24:10,480
you'll get a decrease in latency for

00:24:09,039 --> 00:24:14,480
training aware again

00:24:10,480 --> 00:24:16,400
we get very large efficiency gains and

00:24:14,480 --> 00:24:19,840
forward for giving up very little in

00:24:16,400 --> 00:24:19,840
terms of accuracy

00:24:19,919 --> 00:24:23,919
so quantization used to be a very tricky

00:24:22,240 --> 00:24:26,480
thing to do

00:24:23,919 --> 00:24:27,679
involving very large amounts of custom

00:24:26,480 --> 00:24:30,240
code

00:24:27,679 --> 00:24:32,480
but now it's much much easier it's baked

00:24:30,240 --> 00:24:34,880
into tensorflow and pi torch itself

00:24:32,480 --> 00:24:37,200
with and as well as third-party toolkits

00:24:34,880 --> 00:24:39,039
such as distiller for pytorch

00:24:37,200 --> 00:24:41,360
and it's much much easier to use and you

00:24:39,039 --> 00:24:44,000
can just run the optimization code

00:24:41,360 --> 00:24:47,520
on your model or train using training

00:24:44,000 --> 00:24:48,880
aware quantization

00:24:47,520 --> 00:24:51,039
the final technique we'll discuss

00:24:48,880 --> 00:24:52,960
briefly is called model distillation

00:24:51,039 --> 00:24:54,559
and as we've seen large models may be

00:24:52,960 --> 00:24:56,960
quite over parametrized and relatively

00:24:54,559 --> 00:24:59,679
inefficient in their representations

00:24:56,960 --> 00:25:01,440
so the idea here is can we take such a

00:24:59,679 --> 00:25:02,320
large complex model and use it as a

00:25:01,440 --> 00:25:04,320
teacher

00:25:02,320 --> 00:25:06,000
to to effectively teach a smaller

00:25:04,320 --> 00:25:07,840
simpler student model

00:25:06,000 --> 00:25:09,600
so we want to distill down the core

00:25:07,840 --> 00:25:12,080
knowledge of that large model

00:25:09,600 --> 00:25:13,120
or potentially large set of models in an

00:25:12,080 --> 00:25:14,799
ensemble

00:25:13,120 --> 00:25:16,400
and see if we can represent that

00:25:14,799 --> 00:25:20,480
knowledge by a much

00:25:16,400 --> 00:25:22,960
simpler student model so we see here

00:25:20,480 --> 00:25:24,720
a representation of this where we have a

00:25:22,960 --> 00:25:27,840
teacher model typically with a lot

00:25:24,720 --> 00:25:29,200
more layers or more complexity or depth

00:25:27,840 --> 00:25:30,880
than a student model

00:25:29,200 --> 00:25:32,880
and we want to transfer that knowledge

00:25:30,880 --> 00:25:34,000
to the student model so we're training

00:25:32,880 --> 00:25:35,919
the student model on

00:25:34,000 --> 00:25:37,200
the teacher model's predictions there

00:25:35,919 --> 00:25:40,320
are a number of tricks

00:25:37,200 --> 00:25:42,720
involved here but effectively

00:25:40,320 --> 00:25:44,320
what we see at the bottom right is

00:25:42,720 --> 00:25:46,400
results from the original jeff hinton

00:25:44,320 --> 00:25:49,200
paper on model distillation

00:25:46,400 --> 00:25:50,559
and we can see that a distilled single

00:25:49,200 --> 00:25:51,760
model can actually outperform the

00:25:50,559 --> 00:25:55,200
baseline

00:25:51,760 --> 00:25:57,039
model and architecture are used

00:25:55,200 --> 00:25:59,200
and gets very close to the 10 times

00:25:57,039 --> 00:26:01,840
ensemble which is actually used as the

00:25:59,200 --> 00:26:01,840
teacher model

00:26:02,240 --> 00:26:07,120
model distillation is lesser used in

00:26:05,039 --> 00:26:09,440
image classification

00:26:07,120 --> 00:26:11,279
this day i mean certainly around but but

00:26:09,440 --> 00:26:14,240
probably less

00:26:11,279 --> 00:26:14,559
popular but it's been quite successful

00:26:14,240 --> 00:26:16,240
in

00:26:14,559 --> 00:26:17,760
natural language natural language

00:26:16,240 --> 00:26:19,679
processing in particular

00:26:17,760 --> 00:26:20,880
for recent transformer based models such

00:26:19,679 --> 00:26:23,840
as birds

00:26:20,880 --> 00:26:25,520
some distal models such as distilberts

00:26:23,840 --> 00:26:27,840
and tiny bird for example

00:26:25,520 --> 00:26:28,799
have achieved very very small model

00:26:27,840 --> 00:26:32,799
sizes and

00:26:28,799 --> 00:26:34,480
much more efficient models for

00:26:32,799 --> 00:26:35,760
either giving up very little in terms of

00:26:34,480 --> 00:26:36,640
accuracy or in some cases even

00:26:35,760 --> 00:26:37,760
outperforming

00:26:36,640 --> 00:26:41,039
some of the some of the previous

00:26:37,760 --> 00:26:43,279
state-of-the-art larger bird models

00:26:41,039 --> 00:26:44,720
so again this is really indicating that

00:26:43,279 --> 00:26:46,640
there's a lot of

00:26:44,720 --> 00:26:47,919
inefficiency in the model architectures

00:26:46,640 --> 00:26:50,640
and these large

00:26:47,919 --> 00:26:52,400
image classification models as well as

00:26:50,640 --> 00:26:55,919
large language models

00:26:52,400 --> 00:26:58,480
are really over parameterizing

00:26:55,919 --> 00:27:00,000
and there's a lot more that can be done

00:26:58,480 --> 00:27:00,880
to improve these architectures and

00:27:00,000 --> 00:27:03,039
that'll be

00:27:00,880 --> 00:27:04,799
definitely something of a focus for

00:27:03,039 --> 00:27:07,279
research going forward

00:27:04,799 --> 00:27:07,919
so we've seen that this space is rapidly

00:27:07,279 --> 00:27:10,159
evolving

00:27:07,919 --> 00:27:11,919
it's an area of very very very rapid

00:27:10,159 --> 00:27:13,679
development in research

00:27:11,919 --> 00:27:15,440
and new efficient model architectures

00:27:13,679 --> 00:27:18,320
are always being released

00:27:15,440 --> 00:27:19,200
especially for targeting edge devices

00:27:18,320 --> 00:27:22,080
and

00:27:19,200 --> 00:27:24,240
no resource deployment targets so if one

00:27:22,080 --> 00:27:27,919
fits your need if you are doing image

00:27:24,240 --> 00:27:30,159
based models computer vision and

00:27:27,919 --> 00:27:31,679
is one available that's pre-trained

00:27:30,159 --> 00:27:34,080
great you can use that

00:27:31,679 --> 00:27:35,279
in many cases your particular use case

00:27:34,080 --> 00:27:37,039
may not actually fit

00:27:35,279 --> 00:27:38,720
well with something that is out there

00:27:37,039 --> 00:27:41,279
and been made

00:27:38,720 --> 00:27:42,960
public and released so in that case you

00:27:41,279 --> 00:27:45,279
can look at these compression techniques

00:27:42,960 --> 00:27:46,399
pruning and quantization and as you've

00:27:45,279 --> 00:27:48,399
seen they can yield very large

00:27:46,399 --> 00:27:51,200
efficiency gains

00:27:48,399 --> 00:27:53,039
and without giving up necessarily too

00:27:51,200 --> 00:27:55,840
much in terms of accuracy but

00:27:53,039 --> 00:27:57,039
you typically can you have a lot of

00:27:55,840 --> 00:27:59,360
tweaks and tools

00:27:57,039 --> 00:28:00,880
and knobs that you can turn to trade off

00:27:59,360 --> 00:28:03,840
how much accuracy you're willing to give

00:28:00,880 --> 00:28:06,159
up versus your computational budget

00:28:03,840 --> 00:28:07,279
and your computational resources of your

00:28:06,159 --> 00:28:09,360
target

00:28:07,279 --> 00:28:10,880
it's very easy to do this with a very

00:28:09,360 --> 00:28:13,039
strong built-in support in the deep

00:28:10,880 --> 00:28:15,520
learning frameworks these days

00:28:13,039 --> 00:28:17,200
one could even look at combining pruning

00:28:15,520 --> 00:28:18,000
and quantization that is obviously a lot

00:28:17,200 --> 00:28:21,200
trickier

00:28:18,000 --> 00:28:22,960
which you do first probably you prune

00:28:21,200 --> 00:28:26,799
and then quantize and so on

00:28:22,960 --> 00:28:28,720
um this is it's not clear exactly

00:28:26,799 --> 00:28:30,000
the best way to go about doing this so

00:28:28,720 --> 00:28:31,600
that'll be an

00:28:30,000 --> 00:28:33,279
area of much more kind of

00:28:31,600 --> 00:28:34,159
experimentation to figure out what works

00:28:33,279 --> 00:28:36,960
well

00:28:34,159 --> 00:28:39,600
model distillation is generally less

00:28:36,960 --> 00:28:42,159
popular it's a lot more complex to do

00:28:39,600 --> 00:28:44,000
and it does require training and

00:28:42,159 --> 00:28:45,600
training effectively

00:28:44,000 --> 00:28:47,279
more models so again more

00:28:45,600 --> 00:28:48,559
computationally intensive at training

00:28:47,279 --> 00:28:50,399
time

00:28:48,559 --> 00:28:51,760
but it can be very you know potentially

00:28:50,399 --> 00:28:53,039
very compelling for natural language

00:28:51,760 --> 00:28:54,480
processing tasks especially where

00:28:53,039 --> 00:28:55,600
there's an existing model that's been

00:28:54,480 --> 00:28:59,679
released

00:28:55,600 --> 00:29:01,120
like a distal bit

00:28:59,679 --> 00:29:02,720
thank you very very much for joining me

00:29:01,120 --> 00:29:06,399
today i encourage you to check out

00:29:02,720 --> 00:29:08,240
code.org which lists all the open source

00:29:06,399 --> 00:29:09,520
projects within the data and ai space

00:29:08,240 --> 00:29:11,760
that we work on

00:29:09,520 --> 00:29:13,919
at code you can find me on twitter and

00:29:11,760 --> 00:29:15,200
github at ml nick

00:29:13,919 --> 00:29:16,720
and i also encourage you to check out

00:29:15,200 --> 00:29:18,000
the model asset exchange which is a free

00:29:16,720 --> 00:29:19,440
and open

00:29:18,000 --> 00:29:21,520
exchange for deep learning models

00:29:19,440 --> 00:29:22,399
state-of-the-art models where we have a

00:29:21,520 --> 00:29:24,159
number of

00:29:22,399 --> 00:29:26,399
different image classification models as

00:29:24,159 --> 00:29:28,399
examples and you can try out

00:29:26,399 --> 00:29:29,679
both the large architectures and the

00:29:28,399 --> 00:29:31,520
small

00:29:29,679 --> 00:29:33,760
mobile and edge device focused

00:29:31,520 --> 00:29:34,720
architectures for example for image

00:29:33,760 --> 00:29:37,760
classification

00:29:34,720 --> 00:29:40,240
and segmentation i've also

00:29:37,760 --> 00:29:40,960
left a set of references in the slides

00:29:40,240 --> 00:29:43,919
for some of the

00:29:40,960 --> 00:29:45,919
topics that we've discussed today thank

00:29:43,919 --> 00:29:47,760
you very much

00:29:45,919 --> 00:29:49,679
uh thank you again for the talk um that

00:29:47,760 --> 00:29:52,080
was a bit kind of technical difficulties

00:29:49,679 --> 00:29:54,000
so there is one question that the person

00:29:52,080 --> 00:29:55,679
asking on slack

00:29:54,000 --> 00:29:58,559
why do you think it's uh pruning and

00:29:55,679 --> 00:30:00,880
quantization is tricky

00:29:58,559 --> 00:30:01,600
uh yeah yeah so firstly thanks to

00:30:00,880 --> 00:30:05,200
everyone for

00:30:01,600 --> 00:30:05,200
for listening and your questions

00:30:05,679 --> 00:30:09,039
pruning and quantization is a bit tricky

00:30:07,679 --> 00:30:12,080
to do um

00:30:09,039 --> 00:30:13,279
together just because it's it's kind of

00:30:12,080 --> 00:30:15,520
not clear

00:30:13,279 --> 00:30:16,720
what is the principled approach to take

00:30:15,520 --> 00:30:20,480
um so typically

00:30:16,720 --> 00:30:22,559
you know so for example if you um

00:30:20,480 --> 00:30:25,360
do you do pruning first and then

00:30:22,559 --> 00:30:28,480
quantize or quantizing and then pruning

00:30:25,360 --> 00:30:29,600
so if you quantize the the all the

00:30:28,480 --> 00:30:32,080
weights

00:30:29,600 --> 00:30:34,000
um the pruning mechanism may not work

00:30:32,080 --> 00:30:36,159
quite as you expect and you you may get

00:30:34,000 --> 00:30:37,279
you know an end result which is

00:30:36,159 --> 00:30:40,399
degrading too much

00:30:37,279 --> 00:30:42,640
um accuracy um and

00:30:40,399 --> 00:30:43,440
and but but i think that if you do it

00:30:42,640 --> 00:30:44,720
the other way around

00:30:43,440 --> 00:30:47,200
it's going to be probably a little bit

00:30:44,720 --> 00:30:47,760
better in other words specify the model

00:30:47,200 --> 00:30:49,600
first

00:30:47,760 --> 00:30:51,120
and then once you're happy with that you

00:30:49,600 --> 00:30:54,960
can quantize and

00:30:51,120 --> 00:30:56,799
and and get get the uh

00:30:54,960 --> 00:30:58,480
the slightly you know compressed model

00:30:56,799 --> 00:31:02,080
without losing too much performance

00:30:58,480 --> 00:31:05,360
but either way it's not a um

00:31:02,080 --> 00:31:07,600
there's no kind of

00:31:05,360 --> 00:31:09,039
good formula for saying exactly how you

00:31:07,600 --> 00:31:11,600
should do it

00:31:09,039 --> 00:31:13,039
both of them would probably work there's

00:31:11,600 --> 00:31:15,440
a couple of papers

00:31:13,039 --> 00:31:17,279
out there where they actually use all of

00:31:15,440 --> 00:31:20,559
these techniques together so

00:31:17,279 --> 00:31:22,960
quantization pruning

00:31:20,559 --> 00:31:24,159
but coming up with a kind of scheme for

00:31:22,960 --> 00:31:27,279
doing it

00:31:24,159 --> 00:31:29,440
is is not

00:31:27,279 --> 00:31:32,480
there's no kind of accepted one one way

00:31:29,440 --> 00:31:34,320
to do it i guess

00:31:32,480 --> 00:31:35,760
cool awesome um there was another

00:31:34,320 --> 00:31:37,679
question from nicola

00:31:35,760 --> 00:31:39,679
uh is there like any good implementation

00:31:37,679 --> 00:31:43,279
of nas and aas

00:31:39,679 --> 00:31:44,840
or capital uh sure neural architecture

00:31:43,279 --> 00:31:47,120
search is

00:31:44,840 --> 00:31:50,399
generally um

00:31:47,120 --> 00:31:50,960
kind of auto auto ml or auto ai for deep

00:31:50,399 --> 00:31:53,600
learning

00:31:50,960 --> 00:31:54,159
there's there's a lot of stuff out there

00:31:53,600 --> 00:31:57,039
um

00:31:54,159 --> 00:31:59,200
i haven't i've put i haven't put any any

00:31:57,039 --> 00:32:00,159
direct links to that in the presentation

00:31:59,200 --> 00:32:03,200
but

00:32:00,159 --> 00:32:06,240
um in most of the

00:32:03,200 --> 00:32:09,679
the main uh kind of research labs

00:32:06,240 --> 00:32:11,840
i mean ibm research included uh google

00:32:09,679 --> 00:32:13,360
you know facebook and everyone have it's

00:32:11,840 --> 00:32:15,120
got quite a lot of stuff around

00:32:13,360 --> 00:32:16,960
the way that they do neural architecture

00:32:15,120 --> 00:32:18,960
search

00:32:16,960 --> 00:32:20,000
google has published some of the details

00:32:18,960 --> 00:32:21,679
about how they

00:32:20,000 --> 00:32:24,159
generate things like the efficient nets

00:32:21,679 --> 00:32:25,600
and the mobile nets v3 and so on

00:32:24,159 --> 00:32:27,440
so those are all actually published

00:32:25,600 --> 00:32:28,480
papers and and if you go if you follow

00:32:27,440 --> 00:32:31,840
the links

00:32:28,480 --> 00:32:33,200
um in in the

00:32:31,840 --> 00:32:35,679
presentation which which i think will be

00:32:33,200 --> 00:32:37,600
posted for for the google blog post for

00:32:35,679 --> 00:32:39,039
example for efficient mobile net

00:32:37,600 --> 00:32:41,440
they'll they have links to the papers

00:32:39,039 --> 00:32:43,360
there that have um internal

00:32:41,440 --> 00:32:44,880
details about how they do the

00:32:43,360 --> 00:32:48,240
architecture search

00:32:44,880 --> 00:32:51,279
um similarly i actually posted the

00:32:48,240 --> 00:32:54,840
once for all model in the slack channel

00:32:51,279 --> 00:32:56,080
and in that github repo at the bottom

00:32:54,840 --> 00:32:58,399
actually

00:32:56,080 --> 00:33:00,080
you'll find a bunch of related work

00:32:58,399 --> 00:33:02,000
which includes

00:33:00,080 --> 00:33:05,200
automl for architecting efficient and

00:33:02,000 --> 00:33:07,360
specialized neural networks

00:33:05,200 --> 00:33:09,679
and also ml for model compression and

00:33:07,360 --> 00:33:12,000
acceleration on mobile devices

00:33:09,679 --> 00:33:14,240
architecture search on target tasks and

00:33:12,000 --> 00:33:17,760
hardware so there's a few

00:33:14,240 --> 00:33:21,600
there's a bit of detail there about

00:33:17,760 --> 00:33:23,519
some of the specifically hardware when

00:33:21,600 --> 00:33:24,960
your architecture search and then

00:33:23,519 --> 00:33:28,720
finally the the cloud

00:33:24,960 --> 00:33:30,880
providers again including ibm google

00:33:28,720 --> 00:33:32,000
aws and everybody have their own

00:33:30,880 --> 00:33:34,559
versions of auto

00:33:32,000 --> 00:33:35,440
ai and kind of neural architecture

00:33:34,559 --> 00:33:37,519
search

00:33:35,440 --> 00:33:39,840
as part of their cloud machine learning

00:33:37,519 --> 00:33:39,840
platforms

00:33:40,240 --> 00:33:44,480
cool awesome and i guess more in general

00:33:42,320 --> 00:33:45,919
also narrow architecture search is also

00:33:44,480 --> 00:33:47,679
like slightly different direction right

00:33:45,919 --> 00:33:48,799
because i mean the distillation would be

00:33:47,679 --> 00:33:50,240
like very similar

00:33:48,799 --> 00:33:51,679
once you already have modeling trying to

00:33:50,240 --> 00:33:53,039
scale it up otherwise it's just like

00:33:51,679 --> 00:33:54,159
trying to solve problems in general

00:33:53,039 --> 00:33:56,799
right so slightly

00:33:54,159 --> 00:33:57,600
different direction yeah yeah yeah

00:33:56,799 --> 00:34:00,559
correct

00:33:57,600 --> 00:34:02,320
yeah awesome um i guess uh people are

00:34:00,559 --> 00:34:05,440
typing so let's see if they're gonna be

00:34:02,320 --> 00:34:07,200
fast enough to type the question do you

00:34:05,440 --> 00:34:08,720
like use it also like in production for

00:34:07,200 --> 00:34:10,560
your i don't know your own project so

00:34:08,720 --> 00:34:11,839
it's like mostly like research and uh

00:34:10,560 --> 00:34:13,599
you know investigation like what is

00:34:11,839 --> 00:34:14,320
possible what is the usual like life

00:34:13,599 --> 00:34:16,560
cycle

00:34:14,320 --> 00:34:18,159
of you uh starting with a model and

00:34:16,560 --> 00:34:19,760
after bringing it to

00:34:18,159 --> 00:34:21,280
like distilled version and more

00:34:19,760 --> 00:34:22,879
inference how long it takes for you

00:34:21,280 --> 00:34:25,760
usually

00:34:22,879 --> 00:34:26,399
uh i mean most of the the work that we

00:34:25,760 --> 00:34:29,440
do

00:34:26,399 --> 00:34:31,040
uh in our group is is really related to

00:34:29,440 --> 00:34:34,480
open source projects

00:34:31,040 --> 00:34:35,679
so one of the one of the

00:34:34,480 --> 00:34:38,000
projects that we work on that i

00:34:35,679 --> 00:34:39,839
mentioned is the model asset exchange

00:34:38,000 --> 00:34:41,440
within our group which is a free and

00:34:39,839 --> 00:34:42,560
open source resource for deep learning

00:34:41,440 --> 00:34:45,839
models

00:34:42,560 --> 00:34:49,119
and for some of the image models

00:34:45,839 --> 00:34:51,359
we are working with this

00:34:49,119 --> 00:34:52,879
so all those models are completely free

00:34:51,359 --> 00:34:54,639
and we typically

00:34:52,879 --> 00:34:56,240
take take the state-of-the-art model for

00:34:54,639 --> 00:34:58,079
object detection or

00:34:56,240 --> 00:34:59,599
you know image segmentation or name

00:34:58,079 --> 00:35:00,480
density recognition whatever the case

00:34:59,599 --> 00:35:01,920
may be

00:35:00,480 --> 00:35:03,920
and we package it up in a docker

00:35:01,920 --> 00:35:05,680
container exposing a standardized rest

00:35:03,920 --> 00:35:07,359
api and that's all kind of completely

00:35:05,680 --> 00:35:10,560
free and open

00:35:07,359 --> 00:35:13,200
we work with a few internal groups

00:35:10,560 --> 00:35:14,240
we sometimes approach us looking for

00:35:13,200 --> 00:35:16,320
open source solutions for their

00:35:14,240 --> 00:35:19,200
customers for example um

00:35:16,320 --> 00:35:21,119
and one such group has been the uh

00:35:19,200 --> 00:35:23,200
working on edge device deployment so

00:35:21,119 --> 00:35:26,000
like arm architectures and

00:35:23,200 --> 00:35:26,640
uh and similar so there you know you

00:35:26,000 --> 00:35:28,880
have this

00:35:26,640 --> 00:35:30,480
this problem where you your default

00:35:28,880 --> 00:35:33,440
model might be the large

00:35:30,480 --> 00:35:34,960
you know very accurate model for um or

00:35:33,440 --> 00:35:36,640
image classification but

00:35:34,960 --> 00:35:38,480
but they want a you know highly targeted

00:35:36,640 --> 00:35:40,320
mobile network so

00:35:38,480 --> 00:35:41,359
we allow them to kind of just plug

00:35:40,320 --> 00:35:42,480
different models into the docker

00:35:41,359 --> 00:35:44,000
container when they build it for

00:35:42,480 --> 00:35:45,760
different architectures

00:35:44,000 --> 00:35:47,520
as of yet we haven't directly applied

00:35:45,760 --> 00:35:50,320
the quantization side

00:35:47,520 --> 00:35:51,839
but if they you know if for example in

00:35:50,320 --> 00:35:53,680
this particular use case it turns out

00:35:51,839 --> 00:35:55,839
that they need a lot more efficiency

00:35:53,680 --> 00:35:56,960
they may come back to us and say can we

00:35:55,839 --> 00:35:58,960
actually

00:35:56,960 --> 00:36:00,720
train a customer model with quantization

00:35:58,960 --> 00:36:02,000
and real either kind of advisory

00:36:00,720 --> 00:36:05,599
capacity so

00:36:02,000 --> 00:36:08,560
most of this is um is you know

00:36:05,599 --> 00:36:10,480
some of it's applied mostly in in

00:36:08,560 --> 00:36:12,800
selecting the different

00:36:10,480 --> 00:36:13,520
architectures and the the best kind of

00:36:12,800 --> 00:36:16,720
uh

00:36:13,520 --> 00:36:19,119
target model for each target platform uh

00:36:16,720 --> 00:36:21,280
for quantization and model pruning it's

00:36:19,119 --> 00:36:23,440
it's mostly what's available in the

00:36:21,280 --> 00:36:25,359
in the deep learning libraries and that

00:36:23,440 --> 00:36:26,800
but we we're not using it directly in

00:36:25,359 --> 00:36:29,359
our projects at the moment

00:36:26,800 --> 00:36:30,960
okay awesome so another maybe one of the

00:36:29,359 --> 00:36:32,640
last questions from peter

00:36:30,960 --> 00:36:34,320
uh what kind of metrics you're looking

00:36:32,640 --> 00:36:36,400
during the optimization

00:36:34,320 --> 00:36:37,920
while scaling down besides size accuracy

00:36:36,400 --> 00:36:39,920
and required operations right

00:36:37,920 --> 00:36:42,000
because uh i guess if i extend this

00:36:39,920 --> 00:36:43,680
question um you can do pruning right but

00:36:42,000 --> 00:36:46,079
it only works like if you're

00:36:43,680 --> 00:36:47,040
um you know like edge device or whatever

00:36:46,079 --> 00:36:48,960
like a cpu

00:36:47,040 --> 00:36:50,320
supporting like sparse operations right

00:36:48,960 --> 00:36:51,359
so like do you also sometimes like

00:36:50,320 --> 00:36:54,160
looking more

00:36:51,359 --> 00:36:55,760
what is a specific uh you know executor

00:36:54,160 --> 00:36:57,440
are going to be running that or is like

00:36:55,760 --> 00:36:58,880
any technique actually to get it closer

00:36:57,440 --> 00:37:00,480
and what other metrics are helpful for

00:36:58,880 --> 00:37:02,400
that

00:37:00,480 --> 00:37:03,760
yeah i mean for the most part you do

00:37:02,400 --> 00:37:05,680
need to be aware of what your

00:37:03,760 --> 00:37:07,440
what the capabilities of your execution

00:37:05,680 --> 00:37:08,960
environment on the software side so yeah

00:37:07,440 --> 00:37:10,480
you're right

00:37:08,960 --> 00:37:12,560
there's no point in pruning if you can't

00:37:10,480 --> 00:37:14,160
take advantage of the sparsity

00:37:12,560 --> 00:37:16,160
um so most of the time you're going to

00:37:14,160 --> 00:37:19,280
be targeting uh

00:37:16,160 --> 00:37:22,960
you know something like tf light or

00:37:19,280 --> 00:37:24,880
you know or something so it depends on

00:37:22,960 --> 00:37:26,240
if you're going to an edge device or

00:37:24,880 --> 00:37:27,680
mobile device

00:37:26,240 --> 00:37:29,680
and you only work in tensorflow

00:37:27,680 --> 00:37:31,200
obviously it depends um

00:37:29,680 --> 00:37:32,720
and suddenly you know javascript if

00:37:31,200 --> 00:37:35,680
you're looking up that maybe maybe the

00:37:32,720 --> 00:37:38,480
browser or mobile browser

00:37:35,680 --> 00:37:39,280
onyx.js or whatever you want to be using

00:37:38,480 --> 00:37:41,040
um

00:37:39,280 --> 00:37:42,560
so that's a part of it but i mean

00:37:41,040 --> 00:37:46,800
generally you'd

00:37:42,560 --> 00:37:48,240
look at not just size on disk for memory

00:37:46,800 --> 00:37:51,280
but also

00:37:48,240 --> 00:37:53,119
memory you know the the memory footprint

00:37:51,280 --> 00:37:56,240
of the running program

00:37:53,119 --> 00:37:58,560
um and that's often quite different and

00:37:56,240 --> 00:37:59,440
and sometimes a bit unexpected so a lot

00:37:58,560 --> 00:38:03,280
of the time

00:37:59,440 --> 00:38:05,760
um you might find that the

00:38:03,280 --> 00:38:07,599
network on disk is a certain size but

00:38:05,760 --> 00:38:08,720
when but it kind of explodes memory when

00:38:07,599 --> 00:38:10,480
you when you're loading it

00:38:08,720 --> 00:38:12,079
typically that's that's going to be some

00:38:10,480 --> 00:38:13,440
inefficiency or bug somewhere

00:38:12,079 --> 00:38:15,119
and yeah we actually come across an

00:38:13,440 --> 00:38:16,640
issue like that and a project we're

00:38:15,119 --> 00:38:17,200
working on right now it's you've got

00:38:16,640 --> 00:38:19,200
something

00:38:17,200 --> 00:38:20,480
20 megabit megabytes on disk but it

00:38:19,200 --> 00:38:23,680
exposed to

00:38:20,480 --> 00:38:25,200
whatever gigabytes in memory so you know

00:38:23,680 --> 00:38:26,480
things like that obviously you need to

00:38:25,200 --> 00:38:27,839
be checked it's not enough to just say

00:38:26,480 --> 00:38:30,160
oh i've got a small

00:38:27,839 --> 00:38:31,839
serialized model it must be good but

00:38:30,160 --> 00:38:35,040
apart from that pretty much yeah

00:38:31,839 --> 00:38:37,359
size accuracy and you know

00:38:35,040 --> 00:38:38,160
or some other metric of uh compute

00:38:37,359 --> 00:38:41,280
resources

00:38:38,160 --> 00:38:44,480
is pretty much what you look at

00:38:41,280 --> 00:38:46,960
um federating learning setup

00:38:44,480 --> 00:38:48,000
uh yeah i mean that's a good point i

00:38:46,960 --> 00:38:49,599
mean i

00:38:48,000 --> 00:38:51,010
i haven't worked directly on federated

00:38:49,599 --> 00:38:52,240
learning at this point

00:38:51,010 --> 00:38:55,200
[Music]

00:38:52,240 --> 00:38:57,359
and that's mostly something where you

00:38:55,200 --> 00:38:57,359
know

00:38:58,560 --> 00:39:02,320
companies or organizations that are that

00:39:00,240 --> 00:39:05,359
are kind of have access to

00:39:02,320 --> 00:39:07,040
a network of of edge devices or you know

00:39:05,359 --> 00:39:09,440
user devices where there's there's

00:39:07,040 --> 00:39:10,480
user data and there's a cape on the

00:39:09,440 --> 00:39:12,640
device and this

00:39:10,480 --> 00:39:14,000
capacity for federated learning we're

00:39:12,640 --> 00:39:17,200
not really working on stuff like

00:39:14,000 --> 00:39:18,320
that um i think they you know

00:39:17,200 --> 00:39:22,000
effectively

00:39:18,320 --> 00:39:25,359
the considerations are similar

00:39:22,000 --> 00:39:27,359
you you have to then take into account

00:39:25,359 --> 00:39:30,640
not just training but inference

00:39:27,359 --> 00:39:31,599
because typically you want to do a

00:39:30,640 --> 00:39:33,359
gradient update

00:39:31,599 --> 00:39:37,359
on the on the mobile device and then

00:39:33,359 --> 00:39:37,359
send it back for federated learning

00:39:37,599 --> 00:39:43,920
the reality is the the compute for

00:39:40,880 --> 00:39:46,160
for computing a gradient step versus

00:39:43,920 --> 00:39:47,440
uh versus just inference is not that

00:39:46,160 --> 00:39:48,640
much extra event so

00:39:47,440 --> 00:39:50,560
but you you would have to definitely

00:39:48,640 --> 00:39:52,160
take it into account

00:39:50,560 --> 00:39:53,839
um as well as the fact that you don't

00:39:52,160 --> 00:39:54,720
have access necessarily to all of the

00:39:53,839 --> 00:39:57,760
cpu

00:39:54,720 --> 00:39:58,480
or gpu you know for doing that so that's

00:39:57,760 --> 00:40:00,640
more of a

00:39:58,480 --> 00:40:02,320
challenge of i don't think it's so much

00:40:00,640 --> 00:40:04,960
an architectural

00:40:02,320 --> 00:40:07,200
and network architecture challenges as

00:40:04,960 --> 00:40:09,920
just the software challenge by saying

00:40:07,200 --> 00:40:10,800
schedule the update for a dance you know

00:40:09,920 --> 00:40:13,920
a time where the

00:40:10,800 --> 00:40:15,280
the device is not busy so yeah

00:40:13,920 --> 00:40:16,720
completely makes sense maybe last

00:40:15,280 --> 00:40:18,240
question for me is you mentioned like

00:40:16,720 --> 00:40:20,079
memory footprint right and how things

00:40:18,240 --> 00:40:22,319
are kind of like exploding in memory

00:40:20,079 --> 00:40:23,280
um and i've seen also some groups also

00:40:22,319 --> 00:40:25,839
working on the

00:40:23,280 --> 00:40:26,880
having some ml specific like compiler

00:40:25,839 --> 00:40:29,359
let's say google with

00:40:26,880 --> 00:40:30,560
mlar right and you know when you confuse

00:40:29,359 --> 00:40:33,119
operations like while

00:40:30,560 --> 00:40:34,240
actually compiling that um how do you

00:40:33,119 --> 00:40:35,680
feel about that right do you think it's

00:40:34,240 --> 00:40:37,280
something that like in i don't know a

00:40:35,680 --> 00:40:39,440
couple years is gonna be

00:40:37,280 --> 00:40:41,119
replacing all of those uh model

00:40:39,440 --> 00:40:42,880
converters and like pruning and

00:40:41,119 --> 00:40:45,040
everything else or this kind of uh

00:40:42,880 --> 00:40:47,520
yet like a bit far-fetched and we're not

00:40:45,040 --> 00:40:49,599
gonna see first result pretty soon

00:40:47,520 --> 00:40:50,960
uh yeah interesting question i mean i i

00:40:49,599 --> 00:40:53,200
think i think we're gonna see a lot

00:40:50,960 --> 00:40:56,640
happening there um

00:40:53,200 --> 00:40:58,640
yeah the the with compilers it's always

00:40:56,640 --> 00:40:59,760
there's always going to be edge cases um

00:40:58,640 --> 00:41:01,440
so i don't think it's ever going to be

00:40:59,760 --> 00:41:03,040
perfect

00:41:01,440 --> 00:41:04,960
but i think you can achieve a lot and

00:41:03,040 --> 00:41:08,960
it's certainly very clear that there's

00:41:04,960 --> 00:41:11,839
already and if you just just training a

00:41:08,960 --> 00:41:12,880
model in you know tensorflow and keras

00:41:11,839 --> 00:41:14,640
and all the kind of

00:41:12,880 --> 00:41:16,000
there's a lot of magic happening with

00:41:14,640 --> 00:41:17,280
graph construction and you end up i

00:41:16,000 --> 00:41:19,359
think with a lot of time

00:41:17,280 --> 00:41:20,400
something quite inefficient so being

00:41:19,359 --> 00:41:23,839
able to com you know

00:41:20,400 --> 00:41:26,720
to run a kind of compiler type of

00:41:23,839 --> 00:41:27,119
process over that um or at least even

00:41:26,720 --> 00:41:29,040
just

00:41:27,119 --> 00:41:30,319
doing pruning not from a from the weight

00:41:29,040 --> 00:41:31,200
perspective but just from a graph

00:41:30,319 --> 00:41:34,400
operator

00:41:31,200 --> 00:41:38,480
perspective and doing post optimization

00:41:34,400 --> 00:41:39,760
is important so yeah i mean uh

00:41:38,480 --> 00:41:41,920
i don't think it's going to be a silver

00:41:39,760 --> 00:41:43,760
bullet like it's going to

00:41:41,920 --> 00:41:47,040
solve every problem but certainly i

00:41:43,760 --> 00:41:47,040
think that's definitely going to be

00:41:47,280 --> 00:41:50,720
there's going to be a lot of progress

00:41:48,400 --> 00:41:52,400
there and i mean if you look at

00:41:50,720 --> 00:41:55,920
the the new the new stuff coming out

00:41:52,400 --> 00:41:57,760
like uh uh

00:41:55,920 --> 00:41:59,599
uh what is it the area have you

00:41:57,760 --> 00:42:02,880
obviously got the xla stuff

00:41:59,599 --> 00:42:02,880
and um

00:42:03,920 --> 00:42:06,640
new frameworks which are effectively

00:42:05,359 --> 00:42:08,079
trying to more and more move to

00:42:06,640 --> 00:42:11,839
intermediate representation

00:42:08,079 --> 00:42:12,640
forms yeah um yeah so i mean i think we

00:42:11,839 --> 00:42:15,280
will

00:42:12,640 --> 00:42:16,800
see that yeah awesome thank you again

00:42:15,280 --> 00:42:19,119
for your talk and uh for

00:42:16,800 --> 00:42:20,400
awesome uh answers for the questions um

00:42:19,119 --> 00:42:21,200
i guess we're gonna continue like in

00:42:20,400 --> 00:42:23,200
breakout room

00:42:21,200 --> 00:42:24,640
and uh for the live stream we are done

00:42:23,200 --> 00:42:31,839
thank you again see ya

00:42:24,640 --> 00:42:31,839
thank you everyone

00:42:45,680 --> 00:42:47,760

YouTube URL: https://www.youtube.com/watch?v=irClz_qZOL4


