Title: #bbuzz: Nishith Agarwal - Building large scale, transactional data lakes using Apache Hudi
Publication date: 2020-06-29
Playlist: Berlin Buzzwords | MICES | Haystack – Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/building-large-scale-transactional-data-lakes-using-apache-hudi

With the proliferation of data in the past years, most business critical decisions are heavily influenced by deep data analysis. As companies rely more on data for their functioning; storing, managing and accessing data intelligently and efficiently is more important than ever before. 

As more business decisions are driven by data in real time, we require strong guarantees such as acceptable latencies, high data quality and system reliability. Moving from a full-reload to a delta model of ingesting quickly became the primary way to ingest large amounts of data at scale. A number of such ingest patterns showcased how a transaction support on such datasets could benefit use-cases immensely. 

Hudi, an apache project is attempting to introduce uniform data lake standards. Hudi is a storage abstraction library that uses Spark as an execution framework. In this talk, we will discuss how Hudi can provide ACID semantics to a data lake. We will discuss some of the basic primitives such as upsert & delete required to achieve acceptable latencies in ingestion while at the same time providing high quality data by enforcing schematization on datasets. Additionally, we will also discuss more advanced primitives such as restore, delta-pull, compaction & file sizing required for reliability, efficient storage management and to build incremental ETL pipelines. We will dig deeper into Hudi’s metadata model that allows for O(1) query planning as well as how it helps support Time-Travel queries to facilitate building feature stores for machine learning use-cases. Apache Hudi builds on open-source file formats; we will discuss how to easily onboard your existing dataset to Hudi format while keeping the same open-source formats so you can start utilizing all the features provided by Hudi without needing to make any drastic changes to your data lake. We will talk about the challenges faced in productionizing large Spark based Hudi jobs @scale at Uber and discuss how we addressed them. 

Finally, we will make the case for the future, discussing various other primitives that will facilitate in building rich and portable data applications.
Captions: 
	00:00:07,780 --> 00:00:14,590
hello again

00:00:10,120 --> 00:00:17,470
a session by Agarwal nish it needs

00:00:14,590 --> 00:00:20,890
the hoodie project at Huber and is a

00:00:17,470 --> 00:00:22,420
main contributor on Apache hoodie

00:00:20,890 --> 00:00:26,169
project

00:00:22,420 --> 00:00:29,320
he's also blogger and he will tell us

00:00:26,169 --> 00:00:33,670
about building large-scale transactional

00:00:29,320 --> 00:00:37,270
data leaks using Apache hoodie this

00:00:33,670 --> 00:00:39,309
it's your turn alright thank you I mean

00:00:37,270 --> 00:00:41,260
so I'll just hard over again yeah

00:00:39,309 --> 00:00:44,559
welcome folks my name is Nisha Agarwal

00:00:41,260 --> 00:00:46,300
and today I'm going to talk to you about

00:00:44,559 --> 00:00:48,999
building a large-scale transactional

00:00:46,300 --> 00:00:51,670
database with the passivity so a little

00:00:48,999 --> 00:00:54,640
bit about me I work as a particle EPMC

00:00:51,670 --> 00:00:56,679
and have some happy news to share with

00:00:54,640 --> 00:00:59,530
you who do you recently graduated to an

00:00:56,679 --> 00:01:01,420
apache a top-level project I also work

00:00:59,530 --> 00:01:03,159
as an engineering manager uber will

00:01:01,420 --> 00:01:04,870
largely working on problems in the data

00:01:03,159 --> 00:01:07,510
world such as standardized suggestion

00:01:04,870 --> 00:01:10,060
estimate ization data latency and sort

00:01:07,510 --> 00:01:11,890
of defining primitives or materially I

00:01:10,060 --> 00:01:14,410
have a couple of links here for other

00:01:11,890 --> 00:01:16,000
relevant blogs that maybe you know I'm

00:01:14,410 --> 00:01:18,700
useful in the context of this

00:01:16,000 --> 00:01:20,770
presentation all right so let's dive

00:01:18,700 --> 00:01:22,600
into it so what is the data rate so

00:01:20,770 --> 00:01:23,830
Deary Lake is a personalized deposit

00:01:22,600 --> 00:01:25,510
chain that allows you to store your

00:01:23,830 --> 00:01:28,390
structure as well as unstructured data

00:01:25,510 --> 00:01:31,090
at skew so the idea is to store data as

00:01:28,390 --> 00:01:33,100
is without having to first you know run

00:01:31,090 --> 00:01:35,260
structuring on that either so that you

00:01:33,100 --> 00:01:36,370
can turn on different types of you know

00:01:35,260 --> 00:01:38,260
workloads on it

00:01:36,370 --> 00:01:40,390
analytics dashboards visualizations

00:01:38,260 --> 00:01:44,530
another dictator processing to make

00:01:40,390 --> 00:01:47,110
better decisions so let's look at some

00:01:44,530 --> 00:01:48,400
of the requirements from Adelaide so the

00:01:47,110 --> 00:01:52,030
first requirement I want to talk about

00:01:48,400 --> 00:01:54,430
is incremental database ingestion so you

00:01:52,030 --> 00:01:55,840
know think about you know some you know

00:01:54,430 --> 00:01:59,080
upstream yield is that you might have

00:01:55,840 --> 00:02:01,090
which you are using for business

00:01:59,080 --> 00:02:02,800
critical functions and you're performing

00:02:01,090 --> 00:02:05,620
a bunch of inserts updates and deletes

00:02:02,800 --> 00:02:08,500
you know to that table to you know to

00:02:05,620 --> 00:02:09,970
maintain your business use cases and

00:02:08,500 --> 00:02:12,879
then you want this represented in your

00:02:09,970 --> 00:02:14,500
briefly so this this the data that's

00:02:12,879 --> 00:02:16,090
that's contained in the in these kinds

00:02:14,500 --> 00:02:18,010
of tables are usually high value deal

00:02:16,090 --> 00:02:21,430
maybe user information may be other

00:02:18,010 --> 00:02:23,740
transactional information and you know

00:02:21,430 --> 00:02:25,750
one of the ways you stood like

00:02:23,740 --> 00:02:28,540
have this data represented on to the

00:02:25,750 --> 00:02:30,910
data late is by bulk loading you know

00:02:28,540 --> 00:02:33,070
these tables on to the data rate some of

00:02:30,910 --> 00:02:35,530
the problems that plague this approach

00:02:33,070 --> 00:02:37,540
are in a bulk loadstone scale they add

00:02:35,530 --> 00:02:39,520
more load to the database and at the

00:02:37,540 --> 00:02:41,170
same time they involve a lot of base for

00:02:39,520 --> 00:02:44,820
rewriting of data depending on how you

00:02:41,170 --> 00:02:46,930
how much of data you are called floating

00:02:44,820 --> 00:02:50,020
the second requirement I want to talk

00:02:46,930 --> 00:02:51,910
about is you know D duping log events so

00:02:50,020 --> 00:02:53,410
now think about a bunch of you know

00:02:51,910 --> 00:02:55,390
impression events that may have been

00:02:53,410 --> 00:02:57,310
produced by your web application or your

00:02:55,390 --> 00:02:59,620
mobile application and you want to

00:02:57,310 --> 00:03:00,910
replicate this on to let you donate and

00:02:59,620 --> 00:03:03,160
at the end of the day you don't want any

00:03:00,910 --> 00:03:04,450
duplicates on the deal so it could be a

00:03:03,160 --> 00:03:07,420
message something like which has an

00:03:04,450 --> 00:03:09,010
event ID some it's a date and time you

00:03:07,420 --> 00:03:11,709
know values and a bunch of information

00:03:09,010 --> 00:03:13,390
about the interaction so this kind of

00:03:11,709 --> 00:03:15,459
data is generally like really high scale

00:03:13,390 --> 00:03:17,140
time series zero you know order of

00:03:15,459 --> 00:03:18,780
several billions or even a trillion

00:03:17,140 --> 00:03:21,820
begins with messages per day nowadays

00:03:18,780 --> 00:03:24,459
and you know off the order of few

00:03:21,820 --> 00:03:26,170
millions per second and some of the

00:03:24,459 --> 00:03:28,300
causes for these duplicates could be you

00:03:26,170 --> 00:03:30,580
know resize on the client you know

00:03:28,300 --> 00:03:32,200
failures on the network as well as data

00:03:30,580 --> 00:03:35,920
pipelines that may be ingesting this

00:03:32,200 --> 00:03:37,600
which are at least once only so the

00:03:35,920 --> 00:03:39,850
problems that happen with you know

00:03:37,600 --> 00:03:41,920
duplicates are you know problems such as

00:03:39,850 --> 00:03:44,200
over counting so say you're you know

00:03:41,920 --> 00:03:46,269
evaluating these metrics to drive some

00:03:44,200 --> 00:03:50,610
business metric those these more

00:03:46,269 --> 00:03:53,590
impressions leads to low fidelity data

00:03:50,610 --> 00:03:55,870
third requirement is around storage

00:03:53,590 --> 00:03:58,000
management you know when we developed

00:03:55,870 --> 00:04:01,000
early it was initially developed for

00:03:58,000 --> 00:04:02,920
HDFS and you know one of the previous

00:04:01,000 --> 00:04:04,840
things about HDFS is it does not like a

00:04:02,920 --> 00:04:06,700
lot of small files given some of the

00:04:04,840 --> 00:04:09,790
limitations of how the metadata is

00:04:06,700 --> 00:04:11,739
handled but the same time you know who

00:04:09,790 --> 00:04:13,989
he runs on other route stores as well

00:04:11,739 --> 00:04:16,540
but like you know small files in general

00:04:13,989 --> 00:04:18,430
are a problem because they end up

00:04:16,540 --> 00:04:20,470
controlling you know how fast or slow

00:04:18,430 --> 00:04:21,700
your queries can run or you know you're

00:04:20,470 --> 00:04:24,940
stressing some of the file system

00:04:21,700 --> 00:04:27,310
metadata anyways and so the other

00:04:24,940 --> 00:04:28,630
solution could be to write big files but

00:04:27,310 --> 00:04:30,729
the problem is writing big files

00:04:28,630 --> 00:04:31,900
involves waiting for these files to be

00:04:30,729 --> 00:04:34,840
written so for example if you're writing

00:04:31,900 --> 00:04:36,760
a 2 GB park' file it could take between

00:04:34,840 --> 00:04:40,570
5 to 10 minutes depending on the

00:04:36,760 --> 00:04:42,310
say your schema so the solution could be

00:04:40,570 --> 00:04:44,410
okay we write small files but we want to

00:04:42,310 --> 00:04:46,690
convert them into big files so can we do

00:04:44,410 --> 00:04:48,220
file stitching you know the problem is

00:04:46,690 --> 00:04:51,130
file stitching is okay it's

00:04:48,220 --> 00:04:53,020
non-standardized you know how do we get

00:04:51,130 --> 00:04:55,210
consistency of all applications to do

00:04:53,020 --> 00:04:57,280
file stitching and even if you do pants

00:04:55,210 --> 00:04:59,080
which in your queries the small files

00:04:57,280 --> 00:05:01,420
that anything is exposed to be exposed

00:04:59,080 --> 00:05:03,760
to the small to the cooler to the to the

00:05:01,420 --> 00:05:07,870
query engines and so you anyway see some

00:05:03,760 --> 00:05:09,520
sort of next query deputation the fourth

00:05:07,870 --> 00:05:11,920
in the fourth an important requirement

00:05:09,520 --> 00:05:13,660
is transactional rights the idea is to

00:05:11,920 --> 00:05:16,360
build bring acid semantics to the tea

00:05:13,660 --> 00:05:17,950
delay so as in when you know we ingest

00:05:16,360 --> 00:05:20,170
more and more change data capture and

00:05:17,950 --> 00:05:21,910
late-arriving events and at the same

00:05:20,170 --> 00:05:23,830
time you want to have content readers

00:05:21,910 --> 00:05:26,650
and writers how we support acid

00:05:23,830 --> 00:05:28,630
semantics on the table so in the context

00:05:26,650 --> 00:05:31,420
of the dynamic like let me just define

00:05:28,630 --> 00:05:33,130
asset so a stance for a promise City

00:05:31,420 --> 00:05:35,230
which is essentially the publishing of

00:05:33,130 --> 00:05:37,090
data so say you're ingesting a data and

00:05:35,230 --> 00:05:39,220
the data fields Midway we should

00:05:37,090 --> 00:05:41,170
basically either so things that have

00:05:39,220 --> 00:05:43,420
been atomic be published or things that

00:05:41,170 --> 00:05:47,440
are have not and then not so things that

00:05:43,420 --> 00:05:49,300
have not been consistency is about you

00:05:47,440 --> 00:05:51,820
know only the valid data is saved so

00:05:49,300 --> 00:05:56,530
anything that's invalid is rolled back

00:05:51,820 --> 00:05:58,360
and not exposed to the users snapchat

00:05:56,530 --> 00:06:00,340
eyes so isolation is essentially like

00:05:58,360 --> 00:06:01,960
okay we have read committed data and at

00:06:00,340 --> 00:06:03,520
the same time we have confidence writers

00:06:01,960 --> 00:06:06,010
and readers how do we provide isolation

00:06:03,520 --> 00:06:08,520
of you know read committed either from

00:06:06,010 --> 00:06:11,470
from these content operations and

00:06:08,520 --> 00:06:15,820
finally Bute ability we do not want any

00:06:11,470 --> 00:06:18,520
data loss or idiotic the fifth

00:06:15,820 --> 00:06:21,310
requirement is around faster Drive ETL

00:06:18,520 --> 00:06:22,630
leader so now so now think about you

00:06:21,310 --> 00:06:24,790
know you have these unstructured data

00:06:22,630 --> 00:06:27,190
that you've ingested on your reader lake

00:06:24,790 --> 00:06:30,130
and you essentially wants to perform

00:06:27,190 --> 00:06:31,960
some derived analysis on these on this

00:06:30,130 --> 00:06:34,780
data for example you have a Rob demon

00:06:31,960 --> 00:06:36,430
stable you know well and you want to

00:06:34,780 --> 00:06:38,680
standardize these payments across let's

00:06:36,430 --> 00:06:41,260
say multiple series a multiple countries

00:06:38,680 --> 00:06:44,350
and you want to develop a device table

00:06:41,260 --> 00:06:45,850
on top of that so these these kinds of

00:06:44,350 --> 00:06:48,400
pipelines generally involve like

00:06:45,850 --> 00:06:49,310
multiple stage ptl's you know very

00:06:48,400 --> 00:06:51,260
prominent back

00:06:49,310 --> 00:06:54,350
and with a fairly large amount of Peter

00:06:51,260 --> 00:06:56,210
and one of the you know problems with

00:06:54,350 --> 00:06:57,800
you know working with the right video

00:06:56,210 --> 00:07:01,070
sets is how do you keep them fresh as

00:06:57,800 --> 00:07:02,960
your upstream data is changing and you

00:07:01,070 --> 00:07:06,790
know how do you scale them as you want

00:07:02,960 --> 00:07:06,790
to do V computations and window joins

00:07:07,720 --> 00:07:13,130
and then finally I think you know one of

00:07:10,669 --> 00:07:14,780
the very upcoming requirements for

00:07:13,130 --> 00:07:17,630
Madeira Lake is how do we handle data

00:07:14,780 --> 00:07:19,430
deletions and you know compliance you

00:07:17,630 --> 00:07:20,810
know in the recent days there are a lot

00:07:19,430 --> 00:07:22,520
of flight requirements from like

00:07:20,810 --> 00:07:24,890
following strict rules on data at

00:07:22,520 --> 00:07:27,830
retention or having to delete records

00:07:24,890 --> 00:07:31,010
reg data and you want to do this across

00:07:27,830 --> 00:07:32,840
all your data sets on on the develop so

00:07:31,010 --> 00:07:34,880
you essentially want an efficient way to

00:07:32,840 --> 00:07:36,680
be able to delete this data which

00:07:34,880 --> 00:07:39,590
involves maybe like a point is look up

00:07:36,680 --> 00:07:40,940
on index but I want on the data but at

00:07:39,590 --> 00:07:43,010
the same time you still want app

00:07:40,940 --> 00:07:44,450
optimized scans on this data because

00:07:43,010 --> 00:07:47,330
that's the bulk of the use cases that

00:07:44,450 --> 00:07:49,130
are running on these deer ticks and

00:07:47,330 --> 00:07:52,490
finally we want to propagate you monkeys

00:07:49,130 --> 00:07:56,210
deleted records downstream to other you

00:07:52,490 --> 00:07:58,190
know derive tables so recapping all

00:07:56,210 --> 00:08:00,229
these requirements you know we need

00:07:58,190 --> 00:08:02,479
incrementally to be suggestion you know

00:08:00,229 --> 00:08:05,210
for you know fresh data avoiding the

00:08:02,479 --> 00:08:07,130
rewrites of large amounts of data we

00:08:05,210 --> 00:08:09,140
want to be able to like we do blog

00:08:07,130 --> 00:08:11,300
events and at the same time boots for is

00:08:09,140 --> 00:08:14,750
management to make sure that as our log

00:08:11,300 --> 00:08:16,460
events scale we also scale our you know

00:08:14,750 --> 00:08:19,669
the file the distribute file system that

00:08:16,460 --> 00:08:22,010
we use and then we wants transactional

00:08:19,669 --> 00:08:24,919
writes along with faster Drive ETL for

00:08:22,010 --> 00:08:27,470
you know improved and faster

00:08:24,919 --> 00:08:29,150
you know warehousing and then we want

00:08:27,470 --> 00:08:30,710
compliance and some unique key

00:08:29,150 --> 00:08:32,450
constraints to handle late for having

00:08:30,710 --> 00:08:36,140
data for example that set deletes and

00:08:32,450 --> 00:08:39,050
director actions so at this point let me

00:08:36,140 --> 00:08:41,750
introduce Apache query so for e stands

00:08:39,050 --> 00:08:43,159
for Hadoop absurd cell in terminals so

00:08:41,750 --> 00:08:44,600
think of a bunch of cough cough stream

00:08:43,159 --> 00:08:47,750
Philippines change logs that you can

00:08:44,600 --> 00:08:49,730
ingest onto a holy table and at the same

00:08:47,750 --> 00:08:52,880
time you can also ingest

00:08:49,730 --> 00:08:55,370
you know on chain foodie upstream tables

00:08:52,880 --> 00:08:58,130
into downstream another other three

00:08:55,370 --> 00:09:00,680
tables and all of this data is available

00:08:58,130 --> 00:09:02,990
in different forms you could use query

00:09:00,680 --> 00:09:05,330
engines such as - spark for in

00:09:02,990 --> 00:09:08,660
leader by plans or you can use art and

00:09:05,330 --> 00:09:10,130
pesto for interactive queries and fully

00:09:08,660 --> 00:09:11,960
exposes different types of queries to be

00:09:10,130 --> 00:09:14,330
able to you know meet different use

00:09:11,960 --> 00:09:16,850
cases and all of this data is can be

00:09:14,330 --> 00:09:18,770
stored in like any cloud store or HDFS

00:09:16,850 --> 00:09:22,820
basically any data distributed file

00:09:18,770 --> 00:09:24,830
system compatible storage all right so

00:09:22,820 --> 00:09:26,570
so who really supports like two

00:09:24,830 --> 00:09:28,430
different people types and I quickly

00:09:26,570 --> 00:09:30,050
wanted to get into the details of this

00:09:28,430 --> 00:09:32,090
to give you some sort of an idea you

00:09:30,050 --> 00:09:34,070
know where things are going

00:09:32,090 --> 00:09:37,250
so the first table type that really

00:09:34,070 --> 00:09:38,840
supports is coffee on right so think of

00:09:37,250 --> 00:09:40,280
a bunch of data a batch of data that

00:09:38,840 --> 00:09:43,130
you're actually wanting to ingest into

00:09:40,280 --> 00:09:44,570
equity managed table and you know he

00:09:43,130 --> 00:09:46,040
will say okay you know I want to write

00:09:44,570 --> 00:09:47,780
these data into two different files

00:09:46,040 --> 00:09:50,300
based on some sort of file sizing depth

00:09:47,780 --> 00:09:53,300
you may have defined what he manages all

00:09:50,300 --> 00:09:56,330
of these operations on a timeline and so

00:09:53,300 --> 00:09:58,400
he starts a intense and shows an intent

00:09:56,330 --> 00:09:59,890
of this operation by opening a commit

00:09:58,400 --> 00:10:01,790
and saying that this summit is implied

00:09:59,890 --> 00:10:03,710
finally when the data is actually

00:10:01,790 --> 00:10:05,750
written this summit is atomically

00:10:03,710 --> 00:10:08,450
published and these versions of files

00:10:05,750 --> 00:10:09,920
are available to be queried and I say

00:10:08,450 --> 00:10:11,930
this as a read of Timaeus query because

00:10:09,920 --> 00:10:14,690
you know these are like just pure party

00:10:11,930 --> 00:10:16,460
files so this this so copy-on-write

00:10:14,690 --> 00:10:19,340
could be looked at as a chopped and

00:10:16,460 --> 00:10:21,590
replacement for your you know canonical

00:10:19,340 --> 00:10:23,780
park' tables and hive and you get

00:10:21,590 --> 00:10:26,300
basically any columnar read performance

00:10:23,780 --> 00:10:27,500
that you would get from part yet now

00:10:26,300 --> 00:10:29,750
let's say there's another batch of data

00:10:27,500 --> 00:10:32,510
that comes in that essentially wants to

00:10:29,750 --> 00:10:34,760
provide a absurd sum of the entries in

00:10:32,510 --> 00:10:36,800
the already existing data Hoodie

00:10:34,760 --> 00:10:39,140
maintains an index with which it can

00:10:36,800 --> 00:10:40,880
route the data into you know where

00:10:39,140 --> 00:10:42,650
should it be written to and then in

00:10:40,880 --> 00:10:45,230
copy-on-write where we basically that

00:10:42,650 --> 00:10:47,390
the contents of like a version at c12 a

00:10:45,230 --> 00:10:49,640
new version which is attempting to be

00:10:47,390 --> 00:10:51,350
written at c2 now so that all the

00:10:49,640 --> 00:10:53,000
updates are possible and at this point

00:10:51,350 --> 00:10:55,880
since the data is not atomically

00:10:53,000 --> 00:10:58,160
committed the only contents of version

00:10:55,880 --> 00:11:00,110
at c1 are visible and one this this data

00:10:58,160 --> 00:11:04,370
is atomic weight emitted you now are

00:11:00,110 --> 00:11:06,680
able to see the version at c2 so this

00:11:04,370 --> 00:11:08,720
worked well for many use cases until it

00:11:06,680 --> 00:11:11,420
did it forward and you know some of the

00:11:08,720 --> 00:11:14,150
cases that it didn't this is what i'm

00:11:11,420 --> 00:11:15,800
going to talk about next so so what

00:11:14,150 --> 00:11:16,360
problems will be faced so let's say we

00:11:15,800 --> 00:11:18,510
have a bunch

00:11:16,360 --> 00:11:21,970
of you know new and updated trips very

00:11:18,510 --> 00:11:24,370
common use case for uber where you have

00:11:21,970 --> 00:11:26,170
a bunch of incremental updates and you

00:11:24,370 --> 00:11:28,660
know these updates end up spanning over

00:11:26,170 --> 00:11:30,279
a bunch of you know these updates and

00:11:28,660 --> 00:11:33,040
inserts spanning over a bunch of

00:11:30,279 --> 00:11:34,480
partitions on on the data so now the

00:11:33,040 --> 00:11:36,730
copy online you'd end up you know

00:11:34,480 --> 00:11:38,560
rewriting these kinds of files whichever

00:11:36,730 --> 00:11:41,589
are affected due to updates and writing

00:11:38,560 --> 00:11:43,720
files as in search now the average

00:11:41,589 --> 00:11:46,209
interest time could be about 15 minutes

00:11:43,720 --> 00:11:48,670
where a lot of time is spent on writing

00:11:46,209 --> 00:11:51,940
you know equal equitable sized particles

00:11:48,670 --> 00:11:53,050
for example now think of another

00:11:51,940 --> 00:11:55,089
scenario where there are a lot of

00:11:53,050 --> 00:11:57,279
historical ships being updated for some

00:11:55,089 --> 00:11:58,899
certain reasons and then you have a

00:11:57,279 --> 00:12:01,060
bunch of incremental updates that now

00:11:58,899 --> 00:12:02,829
spans over like you know all sorts of

00:12:01,060 --> 00:12:06,190
partitions and like thousands and

00:12:02,829 --> 00:12:07,269
thousands of files now that's a ton of

00:12:06,190 --> 00:12:08,980
i/o with copy-on-write

00:12:07,269 --> 00:12:11,079
because you end up be writing all these

00:12:08,980 --> 00:12:13,510
files multiple times as these updates

00:12:11,079 --> 00:12:15,399
are happening and at the same time you

00:12:13,510 --> 00:12:17,320
know the job has to do a lot more work

00:12:15,399 --> 00:12:18,940
so you really need to scale your job so

00:12:17,320 --> 00:12:20,110
depending on you know how you have what

00:12:18,940 --> 00:12:22,990
kind of resources are at your disposal

00:12:20,110 --> 00:12:26,890
this could cause a serious ingestion

00:12:22,990 --> 00:12:28,959
Regency so at this point if you look up

00:12:26,890 --> 00:12:30,310
if you think of the typical day level in

00:12:28,959 --> 00:12:32,140
the dirac as I talked about that it

00:12:30,310 --> 00:12:35,440
device data sets that are built on top

00:12:32,140 --> 00:12:37,240
of your raw data sets now all of these

00:12:35,440 --> 00:12:39,250
you know meter sets experience the same

00:12:37,240 --> 00:12:43,630
problem and this is the sense of being a

00:12:39,250 --> 00:12:45,550
very fascinating effect so this so that

00:12:43,630 --> 00:12:48,310
in summary the problems that we see are

00:12:45,550 --> 00:12:50,980
in a write amplification due to you know

00:12:48,310 --> 00:12:52,600
files beginning be written higher

00:12:50,980 --> 00:12:55,050
ingestion latency depending on how you

00:12:52,600 --> 00:12:58,209
how what resources are at your disposal

00:12:55,050 --> 00:13:00,699
and you know given the fact that we

00:12:58,209 --> 00:13:02,890
write party files and reread party files

00:13:00,699 --> 00:13:05,620
if you end up writing larger and larger

00:13:02,890 --> 00:13:07,570
files your ingestion latency becomes

00:13:05,620 --> 00:13:11,410
more and more because a lot of time time

00:13:07,570 --> 00:13:13,240
is spent in writing those files so let's

00:13:11,410 --> 00:13:15,730
find a solution so if you think beyond

00:13:13,240 --> 00:13:17,260
what we have so one of the key problems

00:13:15,730 --> 00:13:19,600
that we noticed is okay how are we

00:13:17,260 --> 00:13:23,199
handling updates so instead of you know

00:13:19,600 --> 00:13:25,270
updating in you know inline updating

00:13:23,199 --> 00:13:27,000
those files in mind how about we append

00:13:25,270 --> 00:13:29,649
these updates to get out the file and

00:13:27,000 --> 00:13:31,119
you know that

00:13:29,649 --> 00:13:32,529
depending these updates lowers injection

00:13:31,119 --> 00:13:34,839
latency directly because we are not

00:13:32,529 --> 00:13:36,160
spending time rewriting those files at

00:13:34,839 --> 00:13:39,360
the same time since we're not rewriting

00:13:36,160 --> 00:13:41,740
those files and if we can you know keep

00:13:39,360 --> 00:13:43,689
cross-match data into you know these

00:13:41,740 --> 00:13:46,779
delta files we can actually reduce write

00:13:43,689 --> 00:13:48,220
amplification and at the same time maybe

00:13:46,779 --> 00:13:50,230
writing larger files will be feasible

00:13:48,220 --> 00:13:54,519
depending on how we route these updates

00:13:50,230 --> 00:13:56,740
and how we route inserts so at this

00:13:54,519 --> 00:14:00,100
point what we implemented is more

00:13:56,740 --> 00:14:02,259
gyeongree and a high level this is hard

00:14:00,100 --> 00:14:03,759
words so you have the same bunch of data

00:14:02,259 --> 00:14:06,189
batch of data that's incoming into a

00:14:03,759 --> 00:14:08,079
fully managed table the timeline says

00:14:06,189 --> 00:14:09,550
okay there is a new data new data that

00:14:08,079 --> 00:14:11,470
statement that needs to be written it

00:14:09,550 --> 00:14:13,779
writes them into let's say your choices

00:14:11,470 --> 00:14:15,819
that salmon or I can't park a files so

00:14:13,779 --> 00:14:19,720
it dies irae into party files it is

00:14:15,819 --> 00:14:21,699
committed and now a bunch of updates

00:14:19,720 --> 00:14:23,259
come along and at this point in

00:14:21,699 --> 00:14:27,220
copy-on-write we would have reversion

00:14:23,259 --> 00:14:29,410
file at c1 to another file but instead

00:14:27,220 --> 00:14:31,089
here we essentially write this data to

00:14:29,410 --> 00:14:33,999
an unmatched

00:14:31,089 --> 00:14:35,949
you know delta file so and once the

00:14:33,999 --> 00:14:38,920
commit has done these this data is

00:14:35,949 --> 00:14:41,230
available but now you can choose between

00:14:38,920 --> 00:14:42,699
you know obviously this this helps us

00:14:41,230 --> 00:14:45,040
achieve you know the lesser light

00:14:42,699 --> 00:14:46,689
amplification by injection agency so now

00:14:45,040 --> 00:14:49,360
there's a trade-off because between okay

00:14:46,689 --> 00:14:51,249
you want fresh data and if you do then

00:14:49,360 --> 00:14:53,769
you use the real-time queries on putting

00:14:51,249 --> 00:14:55,360
to actually merge the base data which is

00:14:53,769 --> 00:14:58,779
the party data which the Delta read bits

00:14:55,360 --> 00:15:01,179
the Delta data to serve on the fly parts

00:14:58,779 --> 00:15:03,759
one at the same time if you are very

00:15:01,179 --> 00:15:06,429
sensitive to you know very fast you

00:15:03,759 --> 00:15:07,990
continue to have you know use read

00:15:06,429 --> 00:15:11,649
optimized queries but that comes at a

00:15:07,990 --> 00:15:14,589
cost of latency but what if you know you

00:15:11,649 --> 00:15:17,980
wanted both you wanted you know to have

00:15:14,589 --> 00:15:21,040
read optimize data bit fresh data so so

00:15:17,980 --> 00:15:23,049
this is where we want to bound the query

00:15:21,040 --> 00:15:27,309
side cost of being able to merge on the

00:15:23,049 --> 00:15:28,329
fly so so as so as we ingest we wanna

00:15:27,309 --> 00:15:31,269
keep ingesting fast

00:15:28,329 --> 00:15:34,149
we don't wanna like you know let that go

00:15:31,269 --> 00:15:35,769
but can we actually bound this cost and

00:15:34,149 --> 00:15:39,179
that's where we introduced asynchronous

00:15:35,769 --> 00:15:41,769
compactions so the goal is basically to

00:15:39,179 --> 00:15:42,980
for merge so generate is speed up

00:15:41,769 --> 00:15:45,380
ingestion

00:15:42,980 --> 00:15:47,780
because in line compaction slows it down

00:15:45,380 --> 00:15:51,200
while at the same time bound the right

00:15:47,780 --> 00:15:53,300
side my side cost so what he supports

00:15:51,200 --> 00:15:55,130
lock-free MVCC based at asynchronous

00:15:53,300 --> 00:15:56,360
compaction to redouble ingestion and

00:15:55,130 --> 00:15:59,840
compaction while keeping injection

00:15:56,360 --> 00:16:01,400
latency are affected so yeah so this

00:15:59,840 --> 00:16:02,870
these are all you know details about

00:16:01,400 --> 00:16:05,120
buddy you know what what we've

00:16:02,870 --> 00:16:06,110
implemented work works so let me talk

00:16:05,120 --> 00:16:07,940
about finally what are the guarantees

00:16:06,110 --> 00:16:10,190
that woody provides across all these

00:16:07,940 --> 00:16:13,040
phenotypes so how do you provide the

00:16:10,190 --> 00:16:15,350
Adamic multirow commits it is basically

00:16:13,040 --> 00:16:17,510
supported by a monotonically increasing

00:16:15,350 --> 00:16:22,070
timestamp to atomic we publish new file

00:16:17,510 --> 00:16:24,770
versions we only expose valid data to

00:16:22,070 --> 00:16:27,560
the place any failed or infrared data is

00:16:24,770 --> 00:16:29,930
rolled back and not exposed it provides

00:16:27,560 --> 00:16:31,760
not for isolation using MVCC so you can

00:16:29,930 --> 00:16:33,890
have confidence leaders compactors and

00:16:31,760 --> 00:16:35,810
you can have the writer and then you

00:16:33,890 --> 00:16:38,570
know the isolation will be maintained

00:16:35,810 --> 00:16:40,220
across all of these and finally we use

00:16:38,570 --> 00:16:41,660
the commit protocol that we have along

00:16:40,220 --> 00:16:42,920
with the guarantees of a distributed

00:16:41,660 --> 00:16:48,500
file system to guarantee guarantee

00:16:42,920 --> 00:16:49,970
durability all right so so these are

00:16:48,500 --> 00:16:52,190
things that you know what he supports

00:16:49,970 --> 00:16:54,050
right now and you know in terms of like

00:16:52,190 --> 00:16:56,660
what set of table types you can have for

00:16:54,050 --> 00:16:59,000
your ingestion use cases and we have a

00:16:56,660 --> 00:17:00,710
bunch of you know use CSS ad over which

00:16:59,000 --> 00:17:02,930
use other you know out-of-the-box

00:17:00,710 --> 00:17:05,360
solutions which internally use the end

00:17:02,930 --> 00:17:06,350
of these table types so the fact so let

00:17:05,360 --> 00:17:09,740
me talk about some facts and figures

00:17:06,350 --> 00:17:11,449
about three a to burn so i do where we

00:17:09,740 --> 00:17:13,160
run about 150 plus terabytes

00:17:11,449 --> 00:17:15,620
transactional tyranny

00:17:13,160 --> 00:17:19,640
ingesting into more than 10 thousand

00:17:15,620 --> 00:17:23,150
people's and writing about 500 billion

00:17:19,640 --> 00:17:25,910
plus records per day so we have a body

00:17:23,150 --> 00:17:28,280
based data leak so it's a unified

00:17:25,910 --> 00:17:30,110
analytical storage so at uber it looks

00:17:28,280 --> 00:17:32,600
something like this we have a bunch of

00:17:30,110 --> 00:17:35,690
upstream database change logs and rocks

00:17:32,600 --> 00:17:37,910
after events which are pushed both into

00:17:35,690 --> 00:17:39,740
top and then you know our batch

00:17:37,910 --> 00:17:41,420
ingestion can range anywhere between 5

00:17:39,740 --> 00:17:43,850
minutes to 30 minutes and all of this

00:17:41,420 --> 00:17:46,370
data is written into HDFS and then

00:17:43,850 --> 00:17:47,960
because of the you know because of

00:17:46,370 --> 00:17:49,220
features provided by everybody provides

00:17:47,960 --> 00:17:51,080
you know three different types of

00:17:49,220 --> 00:17:53,330
queries for you to be able to serve

00:17:51,080 --> 00:17:54,980
different use cases so column there will

00:17:53,330 --> 00:17:56,350
be performance queries to be used by you

00:17:54,980 --> 00:17:58,389
know data science ml or

00:17:56,350 --> 00:18:00,880
you know other like you know quite

00:17:58,389 --> 00:18:02,830
sensitive you know use cases real-time

00:18:00,880 --> 00:18:05,259
queries are you know like provide low

00:18:02,830 --> 00:18:07,720
latency DS data in dashboards and maybe

00:18:05,259 --> 00:18:09,639
ad hoc queries and incremental queries

00:18:07,720 --> 00:18:12,899
could be used to build like you know the

00:18:09,639 --> 00:18:16,690
RAI BTL datasets and like warehouse

00:18:12,899 --> 00:18:19,090
so so apart from like building out the

00:18:16,690 --> 00:18:21,220
disk and were really get a duper there

00:18:19,090 --> 00:18:22,809
are other use cases are over you know

00:18:21,220 --> 00:18:25,779
which is supported by out-of-the-box

00:18:22,809 --> 00:18:28,029
solutions a Bible so I'll talk about a

00:18:25,779 --> 00:18:30,070
couple of them one of them is the hoodie

00:18:28,029 --> 00:18:31,960
Delta streamer so every Delta streamer

00:18:30,070 --> 00:18:34,870
is basically a mini ingestion framework

00:18:31,960 --> 00:18:38,169
implementing inquiry to be able to you

00:18:34,870 --> 00:18:41,769
know ingest from different sources to to

00:18:38,169 --> 00:18:43,899
let's say and so you know the ubers

00:18:41,769 --> 00:18:45,669
global network stream is basically

00:18:43,899 --> 00:18:48,580
powered in near real-time using for

00:18:45,669 --> 00:18:49,990
iterative streamer the the high-level

00:18:48,580 --> 00:18:52,509
architecture looks something like this

00:18:49,990 --> 00:18:55,419
so we have a bunch of rowdy the raw

00:18:52,509 --> 00:18:57,370
event tables free Delta streamer you

00:18:55,419 --> 00:19:00,370
know instrumentally poles you know

00:18:57,370 --> 00:19:02,620
changes from this raw table and then you

00:19:00,370 --> 00:19:04,929
know for the ubers network analytics use

00:19:02,620 --> 00:19:07,000
cases they transform the entities into

00:19:04,929 --> 00:19:09,070
you know Delta somebody's again using

00:19:07,000 --> 00:19:11,350
transformation functions provided by the

00:19:09,070 --> 00:19:13,690
Delta streamer and then they essentially

00:19:11,350 --> 00:19:15,549
up straight into a final summary table

00:19:13,690 --> 00:19:17,769
where they merge you know their deltas

00:19:15,549 --> 00:19:20,320
into a in other Delta on the summary

00:19:17,769 --> 00:19:22,090
table and this you know this allows the

00:19:20,320 --> 00:19:24,850
global networks analytics team to

00:19:22,090 --> 00:19:27,429
actually maintain a good history of

00:19:24,850 --> 00:19:31,570
what's happening in the in the network I

00:19:27,429 --> 00:19:33,789
suppose storing many raw events another

00:19:31,570 --> 00:19:36,309
use case is essentially building et ELLs

00:19:33,789 --> 00:19:38,440
you know downstream derived data sets

00:19:36,309 --> 00:19:41,350
and spot data source integration is

00:19:38,440 --> 00:19:43,629
highly popular there so with a lot of

00:19:41,350 --> 00:19:45,190
the folks using you know ETL you know

00:19:43,629 --> 00:19:47,529
they're more proficient in like five

00:19:45,190 --> 00:19:49,360
boys and Python programming so in that

00:19:47,529 --> 00:19:52,120
world you know hoody is used with PI

00:19:49,360 --> 00:19:53,679
spark on the bounty resource and look

00:19:52,120 --> 00:19:55,870
something like this so there is a bunch

00:19:53,679 --> 00:19:58,690
of you know high queries that implement

00:19:55,870 --> 00:20:00,700
people from rajid a table they transform

00:19:58,690 --> 00:20:02,919
this data and rights to let's say you

00:20:00,700 --> 00:20:04,600
know intermediate staging tables at this

00:20:02,919 --> 00:20:06,820
point they may want to join this

00:20:04,600 --> 00:20:10,280
intermediate data with other you know

00:20:06,820 --> 00:20:12,380
into the other tables to produce a model

00:20:10,280 --> 00:20:16,040
and finally all the information the

00:20:12,380 --> 00:20:18,350
model table can be absurd using three so

00:20:16,040 --> 00:20:21,350
these are some of the use cases that you

00:20:18,350 --> 00:20:22,760
know are popular at uber we built the

00:20:21,350 --> 00:20:23,660
holy data Lake and then you know

00:20:22,760 --> 00:20:26,470
different kinds of out-of-the-box

00:20:23,660 --> 00:20:29,060
solutions used to support use cases and

00:20:26,470 --> 00:20:31,580
so we also use a bunch of advanced

00:20:29,060 --> 00:20:34,160
primitives that when he provides and

00:20:31,580 --> 00:20:35,810
I'll talk about a few of them so one of

00:20:34,160 --> 00:20:38,390
the most primitive is how do you recover

00:20:35,810 --> 00:20:40,040
from data corruption so some common

00:20:38,390 --> 00:20:41,480
questions and production systems are you

00:20:40,040 --> 00:20:42,950
know if what if a bug resulted in

00:20:41,480 --> 00:20:45,140
incorrect data being pushed to my

00:20:42,950 --> 00:20:47,120
injection system or what is an obscene

00:20:45,140 --> 00:20:49,550
system incorrectly marked some common

00:20:47,120 --> 00:20:51,380
values is not so how do you prove how do

00:20:49,550 --> 00:20:53,780
you be so this is something that could

00:20:51,380 --> 00:20:55,910
happen and how do you basically adjust

00:20:53,780 --> 00:20:57,320
easy what he does that for you who do

00:20:55,910 --> 00:20:59,480
you have has an ability to restore a

00:20:57,320 --> 00:21:02,540
table to a last known correct time and

00:20:59,480 --> 00:21:05,210
it provides two of the two you know two

00:21:02,540 --> 00:21:07,550
ways to do this one is using safe points

00:21:05,210 --> 00:21:10,370
which is basically cheating checkpoints

00:21:07,550 --> 00:21:11,720
and different instants of time or you

00:21:10,370 --> 00:21:13,460
know you could basically use the file

00:21:11,720 --> 00:21:16,340
versions retained where you could go

00:21:13,460 --> 00:21:18,590
back as far as in time as needed so the

00:21:16,340 --> 00:21:20,840
the positive benefits of using safe

00:21:18,590 --> 00:21:22,310
points is that it optimizes the number

00:21:20,840 --> 00:21:25,250
of file versions you need to keep so

00:21:22,310 --> 00:21:27,170
reduces the disk space while this you

00:21:25,250 --> 00:21:30,230
know the downside is doesn't work right

00:21:27,170 --> 00:21:31,670
now fortunately the file versions works

00:21:30,230 --> 00:21:33,980
for both copy-on-write in more general

00:21:31,670 --> 00:21:38,390
but the downside is it requires extra

00:21:33,980 --> 00:21:39,890
storage capacity the other you know

00:21:38,390 --> 00:21:42,670
advanced primitive is incremental Poe

00:21:39,890 --> 00:21:44,870
which is you know very popular for

00:21:42,670 --> 00:21:46,310
reducing the amount of data that you

00:21:44,870 --> 00:21:48,560
scan as well as reducing the amount of

00:21:46,310 --> 00:21:51,560
data that you write in many direct data

00:21:48,560 --> 00:21:53,870
sets so there are two ways to you know

00:21:51,560 --> 00:21:56,120
use the increment lakeil one of them is

00:21:53,870 --> 00:21:58,400
using spark so you use your spark data

00:21:56,120 --> 00:22:01,070
source you initialize some of the you

00:21:58,400 --> 00:22:02,900
know some of the parameters that we need

00:22:01,070 --> 00:22:04,670
to do and you know the one of the

00:22:02,900 --> 00:22:06,530
important things to note is basically

00:22:04,670 --> 00:22:08,660
you can give a beginning instant line

00:22:06,530 --> 00:22:10,310
which is basically hey like from what

00:22:08,660 --> 00:22:12,320
time do I want to instrument three pull

00:22:10,310 --> 00:22:14,420
all of this data and then you can have a

00:22:12,320 --> 00:22:16,520
data set register that data set as a

00:22:14,420 --> 00:22:20,060
table if you want to and then use the

00:22:16,520 --> 00:22:22,220
sparks equal to pirate that you we also

00:22:20,060 --> 00:22:23,570
have a port that using hive and you have

00:22:22,220 --> 00:22:26,450
to set a bunch of

00:22:23,570 --> 00:22:29,870
you know a high connection parameters

00:22:26,450 --> 00:22:32,510
and you know you you can hire you can

00:22:29,870 --> 00:22:34,430
also provide a bunch of batches that you

00:22:32,510 --> 00:22:36,920
want to consume from this begin instant

00:22:34,430 --> 00:22:38,540
time and then you can go and select you

00:22:36,920 --> 00:22:41,950
know and then you can run a query on

00:22:38,540 --> 00:22:41,950
that on top of that instrumental tea

00:22:42,200 --> 00:22:46,790
so the other cool feature that you know

00:22:45,230 --> 00:22:48,860
buddy provides is the ability to time

00:22:46,790 --> 00:22:50,960
travel and basically quite different

00:22:48,860 --> 00:22:52,760
snapshots in time and you know this

00:22:50,960 --> 00:22:55,130
lecture works in congestion conjunction

00:22:52,760 --> 00:22:57,830
with incremental poll so now that you

00:22:55,130 --> 00:22:59,390
know using spired what you can do is you

00:22:57,830 --> 00:23:01,730
know redefine you already define the

00:22:59,390 --> 00:23:04,100
begin instant time what you can do in

00:23:01,730 --> 00:23:06,230
addition to that is a is a sign an end

00:23:04,100 --> 00:23:07,880
instant time and essentially in that

00:23:06,230 --> 00:23:09,410
time range is whatever snapshot you

00:23:07,880 --> 00:23:12,110
wanna play you can go and query that

00:23:09,410 --> 00:23:15,410
snapshot you can do a similar thing

00:23:12,110 --> 00:23:17,540
using hive but there is a slight caveat

00:23:15,410 --> 00:23:19,640
to that there is no such an instant and

00:23:17,540 --> 00:23:21,530
support in the hive

00:23:19,640 --> 00:23:23,930
so you have to convert the else instant

00:23:21,530 --> 00:23:25,940
time to like num commits to read and

00:23:23,930 --> 00:23:28,820
then use that but we are looking at

00:23:25,940 --> 00:23:30,260
adding a consume and timestamp so that

00:23:28,820 --> 00:23:32,480
you know you have like sort of the same

00:23:30,260 --> 00:23:33,950
feature parity between spark invites and

00:23:32,480 --> 00:23:35,510
this is very this is a very popular

00:23:33,950 --> 00:23:37,790
thing used for like you know machine

00:23:35,510 --> 00:23:40,600
learning feature source which which is

00:23:37,790 --> 00:23:43,670
becoming more and more popular

00:23:40,600 --> 00:23:46,130
right so now that you know running all

00:23:43,670 --> 00:23:49,160
of these features at scale as especially

00:23:46,130 --> 00:23:51,830
at were you know we ran into a lot of

00:23:49,160 --> 00:23:54,050
fly you know we did a lot of performance

00:23:51,830 --> 00:23:55,670
tuning in spark and spark applications

00:23:54,050 --> 00:23:59,630
and I want to share some of the

00:23:55,670 --> 00:24:01,370
learnings with you so you know one of

00:23:59,630 --> 00:24:02,780
the things we realized is you know the

00:24:01,370 --> 00:24:05,990
the type of sterilizer that you use

00:24:02,780 --> 00:24:07,610
matters a lot and so we have ruber we we

00:24:05,990 --> 00:24:10,070
use class utiliser for all our

00:24:07,610 --> 00:24:12,800
civilzation work especially in smart

00:24:10,070 --> 00:24:15,170
applications it has a lesser memory

00:24:12,800 --> 00:24:16,540
footprint than java serializer as well

00:24:15,170 --> 00:24:19,330
as much possible

00:24:16,540 --> 00:24:23,840
the second thing I want to touch upon is

00:24:19,330 --> 00:24:24,950
shuffle service so you know social

00:24:23,840 --> 00:24:26,600
service is something that's used by

00:24:24,950 --> 00:24:27,890
spark for example to you know move

00:24:26,600 --> 00:24:30,620
around data when you are performing

00:24:27,890 --> 00:24:32,050
shuffles and so if you have the

00:24:30,620 --> 00:24:34,340
opportunity we use your resource

00:24:32,050 --> 00:24:36,470
scheduler or business manager like yarn

00:24:34,340 --> 00:24:37,410
you know consider using of external

00:24:36,470 --> 00:24:39,390
shuffle service

00:24:37,410 --> 00:24:42,360
especially for large memory requirements

00:24:39,390 --> 00:24:44,340
it's the reliability of shuttle service

00:24:42,360 --> 00:24:46,080
is challenging and it contributes

00:24:44,340 --> 00:24:48,330
directly to job stability and you know

00:24:46,080 --> 00:24:50,640
doing this helped us in that initially

00:24:48,330 --> 00:24:53,220
and you know how do I manage you know

00:24:50,640 --> 00:24:57,180
how spot applications are reading large

00:24:53,220 --> 00:24:59,280
amounts of data the third thing I want

00:24:57,180 --> 00:25:01,890
to touch upon is a sparse memory model

00:24:59,280 --> 00:25:04,830
so understanding how to effectively use

00:25:01,890 --> 00:25:07,170
heat versus non heat is extremely

00:25:04,830 --> 00:25:09,270
important and as well as you know how do

00:25:07,170 --> 00:25:11,760
you - you know the heat memory that you

00:25:09,270 --> 00:25:13,620
use which basically could be as naive as

00:25:11,760 --> 00:25:15,900
the executed numpy that you have how do

00:25:13,620 --> 00:25:18,210
you tune that because we noticed that a

00:25:15,900 --> 00:25:20,010
lot of times you know people run smart

00:25:18,210 --> 00:25:22,590
applications where there is lack of

00:25:20,010 --> 00:25:24,660
understanding of whether then the the

00:25:22,590 --> 00:25:26,520
throughput of the job is high given the

00:25:24,660 --> 00:25:28,650
resources that the job has been provided

00:25:26,520 --> 00:25:30,240
so so Google did implement an open

00:25:28,650 --> 00:25:32,430
source profiler to be able to profile

00:25:30,240 --> 00:25:34,620
your forms and to see if you're actually

00:25:32,430 --> 00:25:37,380
using the number that you know you you

00:25:34,620 --> 00:25:39,060
given to your spark shop and lastly your

00:25:37,380 --> 00:25:41,640
back configurations can make or break

00:25:39,060 --> 00:25:44,880
the day so investing in contact tunings

00:25:41,640 --> 00:25:46,110
constantly is is extremely important and

00:25:44,880 --> 00:25:48,630
if you have an automated feedback

00:25:46,110 --> 00:25:49,830
mechanism to tune that parts of

00:25:48,630 --> 00:25:53,430
something that we have an uber is

00:25:49,830 --> 00:25:55,230
extremely valuable and as you run like

00:25:53,430 --> 00:25:56,970
larger jobs you choose between different

00:25:55,230 --> 00:25:59,940
table types are tuning boutique contigs

00:25:56,970 --> 00:26:02,850
is also you know like extremely

00:25:59,940 --> 00:26:04,920
important is as your based on use cases

00:26:02,850 --> 00:26:10,050
different configs can give you different

00:26:04,920 --> 00:26:11,820
set of performances right so that's

00:26:10,050 --> 00:26:14,250
that's what we have you know up until

00:26:11,820 --> 00:26:15,840
now I'll talk to you about what's coming

00:26:14,250 --> 00:26:20,730
soon and the and what's in the roadmap

00:26:15,840 --> 00:26:23,520
for you next so here we have a release

00:26:20,730 --> 00:26:25,980
0.6 is you know that's coming up in the

00:26:23,520 --> 00:26:28,020
next couple of months and there are two

00:26:25,980 --> 00:26:30,480
super cool features that are that are

00:26:28,020 --> 00:26:33,570
being worked on so one is you know we've

00:26:30,480 --> 00:26:36,660
had a lot of you know how do you convert

00:26:33,570 --> 00:26:39,000
an existing canonical five table to a

00:26:36,660 --> 00:26:40,650
party table essentially the people want

00:26:39,000 --> 00:26:42,630
to migrate the canonical tables to

00:26:40,650 --> 00:26:45,210
overeat Abel's to you know support up

00:26:42,630 --> 00:26:47,340
search the mental pose and we want an

00:26:45,210 --> 00:26:48,900
efficient way to do that so I'll

00:26:47,340 --> 00:26:51,330
encourage you to read the full RFC which

00:26:48,900 --> 00:26:54,360
also has your examples and

00:26:51,330 --> 00:26:56,250
you know scale numbers on how how fast

00:26:54,360 --> 00:26:58,289
and how efficient this is but I'd like

00:26:56,250 --> 00:27:00,960
to give you a very quick you know tidbit

00:26:58,289 --> 00:27:03,450
of how it works essentially you know you

00:27:00,960 --> 00:27:06,509
use a data frame to trigger an action on

00:27:03,450 --> 00:27:09,240
the table and all you tell the action is

00:27:06,509 --> 00:27:11,789
that the operation type is a bootstrap

00:27:09,240 --> 00:27:13,169
operation and Anthony will basically go

00:27:11,789 --> 00:27:17,070
ahead in the background and do that for

00:27:13,169 --> 00:27:19,590
you and the second very cool feature is

00:27:17,070 --> 00:27:21,779
you know hoodie has invested a lot in

00:27:19,590 --> 00:27:23,789
you know how do you optimize rights how

00:27:21,779 --> 00:27:27,419
do you ingest data how do you manage

00:27:23,789 --> 00:27:28,590
your latency ride amplification we have

00:27:27,419 --> 00:27:31,559
so much of meta data that we can

00:27:28,590 --> 00:27:33,600
actually improve queries drastically so

00:27:31,559 --> 00:27:35,700
there is a RFC that's like that's an

00:27:33,600 --> 00:27:37,529
implementation right now which is to

00:27:35,700 --> 00:27:39,870
provide order of one very planning and

00:27:37,529 --> 00:27:41,519
what I mean by that is you know as your

00:27:39,870 --> 00:27:44,070
table cells drawers instead of letting

00:27:41,519 --> 00:27:46,080
pipes and like you know to do this query

00:27:44,070 --> 00:27:48,179
planning for you by scanning like

00:27:46,080 --> 00:27:50,759
different partitions or you can do this

00:27:48,179 --> 00:27:53,759
in constant time with the metadata

00:27:50,759 --> 00:27:55,980
that's already available we also want to

00:27:53,759 --> 00:27:59,129
eliminate file listing something that

00:27:55,980 --> 00:28:02,159
you know many cloud stores and HDFS

00:27:59,129 --> 00:28:04,799
format like and this is the the PR is

00:28:02,159 --> 00:28:08,129
already out the RFC has more details

00:28:04,799 --> 00:28:09,600
around it finally you know given that we

00:28:08,129 --> 00:28:11,970
have all of this information in this

00:28:09,600 --> 00:28:14,370
metadata you know we want to expose some

00:28:11,970 --> 00:28:18,179
column column indexes on holy datasets

00:28:14,370 --> 00:28:20,879
which can allow you to you know perform

00:28:18,179 --> 00:28:23,009
where clauses so as to say on you know

00:28:20,879 --> 00:28:28,379
non partition columns anti-foreign

00:28:23,009 --> 00:28:30,509
custom or inspect so that's that those

00:28:28,379 --> 00:28:34,289
are the upcoming features and you know

00:28:30,509 --> 00:28:37,440
some of the notable items that we have

00:28:34,289 --> 00:28:39,179
in the roadmap are as follows so you

00:28:37,440 --> 00:28:41,549
know who he provides different types of

00:28:39,179 --> 00:28:43,049
pluggable indexing mechanisms and we're

00:28:41,549 --> 00:28:45,120
working working on one which will

00:28:43,049 --> 00:28:47,580
basically provide blazing-fast absurds

00:28:45,120 --> 00:28:49,710
for you know all large workloads where a

00:28:47,580 --> 00:28:51,570
large number of data might be be mine

00:28:49,710 --> 00:28:53,429
might have been sorted

00:28:51,570 --> 00:28:55,529
we're working on a couple of other

00:28:53,429 --> 00:28:57,809
things around you know data clustering

00:28:55,529 --> 00:28:59,460
on you know we we already do storage

00:28:57,809 --> 00:29:01,889
management of you know how do you size

00:28:59,460 --> 00:29:04,360
files how do you make sure you know you

00:29:01,889 --> 00:29:06,040
write equitable like size files but

00:29:04,360 --> 00:29:08,590
same time we also want to see okay how

00:29:06,040 --> 00:29:10,360
if based on like applications how does

00:29:08,590 --> 00:29:12,400
this should this deal be clustered sort

00:29:10,360 --> 00:29:14,230
of like the couple you know how do you

00:29:12,400 --> 00:29:16,299
ingest data from how do you actually lay

00:29:14,230 --> 00:29:19,600
out this data you know based on like

00:29:16,299 --> 00:29:21,610
application requirements there is a PR

00:29:19,600 --> 00:29:24,460
out for real-time queries and custo and

00:29:21,610 --> 00:29:27,370
in the longer term there is a a lot of

00:29:24,460 --> 00:29:29,260
ask in the community to have integrate

00:29:27,370 --> 00:29:33,520
in twittering and there is some initial

00:29:29,260 --> 00:29:34,660
work that stuff around that with this

00:29:33,520 --> 00:29:37,960
that's all I had

00:29:34,660 --> 00:29:40,030
thank you you know watch as I mentioned

00:29:37,960 --> 00:29:43,000
a partially recently recently graduated

00:29:40,030 --> 00:29:45,880
to a top-level project we have a very

00:29:43,000 --> 00:29:48,640
exciting an ambitious roadmap and a very

00:29:45,880 --> 00:29:50,559
budding and community so if you're

00:29:48,640 --> 00:29:52,390
interested please you don't connect with

00:29:50,559 --> 00:29:54,760
us at develop a a put it out of pocket

00:29:52,390 --> 00:29:56,919
or or follow us on Twitter or if you're

00:29:54,760 --> 00:29:58,600
interested in trying out buddy please go

00:29:56,919 --> 00:30:01,150
to her eat or the path she toured there

00:29:58,600 --> 00:30:03,280
is a very cool docker you know

00:30:01,150 --> 00:30:06,299
QuickStart that you can actually you

00:30:03,280 --> 00:30:08,380
know use to star start like using and

00:30:06,299 --> 00:30:13,390
realizing your use case in a matter of

00:30:08,380 --> 00:30:18,580
minutes all right at this time I'm happy

00:30:13,390 --> 00:30:23,140
to answer any questions and we have one

00:30:18,580 --> 00:30:26,320
question from Nicola who's telling us

00:30:23,140 --> 00:30:30,750
that he's already tried hive with hoodie

00:30:26,320 --> 00:30:33,429
and can't wait for time travel there to

00:30:30,750 --> 00:30:36,070
his question his first question is about

00:30:33,429 --> 00:30:42,460
spark can you make an example of

00:30:36,070 --> 00:30:45,570
external shuttle service can I make an

00:30:42,460 --> 00:30:47,770
example of external chapel service so I

00:30:45,570 --> 00:30:50,530
so I'll tell you what I mean by external

00:30:47,770 --> 00:30:52,900
shuttle service so you know whenever you

00:30:50,530 --> 00:30:54,510
shuffle between you know when it's foggy

00:30:52,900 --> 00:30:58,120
up starts and you want to shuffle data

00:30:54,510 --> 00:31:00,400
you know there will be some sort of the

00:30:58,120 --> 00:31:02,320
server that froggies which reads data

00:31:00,400 --> 00:31:04,299
from the local disk that you flex fill

00:31:02,320 --> 00:31:07,299
to and then provides it from one mapper

00:31:04,299 --> 00:31:09,460
to reducer so as to say and so you know

00:31:07,299 --> 00:31:11,500
you could either use the stuff that's

00:31:09,460 --> 00:31:12,790
provided by spark standalone or you

00:31:11,500 --> 00:31:15,130
could have a resource manager do that

00:31:12,790 --> 00:31:16,450
for you and you know depending on you

00:31:15,130 --> 00:31:17,490
know what kind of memory requirements

00:31:16,450 --> 00:31:19,140
you have and

00:31:17,490 --> 00:31:20,730
what reliability you want you you may

00:31:19,140 --> 00:31:23,100
want to choose one with one of the other

00:31:20,730 --> 00:31:25,110
and for us you know we externally

00:31:23,100 --> 00:31:27,420
shuffle service on yarn work we have

00:31:25,110 --> 00:31:28,830
some more work which we did on top of

00:31:27,420 --> 00:31:32,960
that which is not part of the stock but

00:31:28,830 --> 00:31:36,270
that's something that would for us okay

00:31:32,960 --> 00:31:40,410
then how do you plan to get rid of

00:31:36,270 --> 00:31:43,650
finest thing yeah so I think you know

00:31:40,410 --> 00:31:47,070
that if you go and read the RFC the RFC

00:31:43,650 --> 00:31:48,420
number is RFC fifteen essentially you

00:31:47,070 --> 00:31:49,920
know we already have all of this

00:31:48,420 --> 00:31:51,450
metadata since hoodie is the writer to

00:31:49,920 --> 00:31:54,300
be stables we already have all of the

00:31:51,450 --> 00:31:56,160
metadata so if you look at the dory

00:31:54,300 --> 00:31:57,809
folder all these command files already

00:31:56,160 --> 00:31:59,850
have them already does so the idea is

00:31:57,809 --> 00:32:02,100
basically consolidate that metadata and

00:31:59,850 --> 00:32:04,140
be able to read entries from that

00:32:02,100 --> 00:32:07,110
metadata as opposed to you know asking

00:32:04,140 --> 00:32:08,460
hive matter store and then going to HDFS

00:32:07,110 --> 00:32:13,050
to figure out which files the peasant on

00:32:08,460 --> 00:32:18,420
this okay have you evaluated other file

00:32:13,050 --> 00:32:22,320
formats like OLC or carbon data yeah I

00:32:18,420 --> 00:32:25,380
think there is it PR on RC I think there

00:32:22,320 --> 00:32:27,420
is some back and forth on that so so

00:32:25,380 --> 00:32:29,190
I'll talk about it in to two aspects one

00:32:27,420 --> 00:32:31,320
is well how will fully support these

00:32:29,190 --> 00:32:35,370
file formats so there is a push to

00:32:31,320 --> 00:32:37,380
support of RC at uber we had you know RC

00:32:35,370 --> 00:32:39,720
and party initially we had both and then

00:32:37,380 --> 00:32:42,600
we moved to party for you know some of

00:32:39,720 --> 00:32:44,160
the community work that's been done some

00:32:42,600 --> 00:32:47,010
sort of like in coatings that were not

00:32:44,160 --> 00:32:49,170
available I think at this point it may

00:32:47,010 --> 00:32:50,340
be worthwhile to like reevaluate oversee

00:32:49,170 --> 00:32:52,950
part carries a lot of work has been done

00:32:50,340 --> 00:32:56,550
and put years will eventually support

00:32:52,950 --> 00:32:57,330
both okay cool that's so for the

00:32:56,550 --> 00:33:01,050
questions

00:32:57,330 --> 00:33:04,740
I think the breakout room will stay open

00:33:01,050 --> 00:33:07,170
for for all the night if you want to

00:33:04,740 --> 00:33:10,650
continue discussion with Nicola who is

00:33:07,170 --> 00:33:13,140
following the talk actively I think he's

00:33:10,650 --> 00:33:15,179
typing maybe he said he's got another

00:33:13,140 --> 00:33:19,140
question no it was a thanks from him

00:33:15,179 --> 00:33:20,910
okay so I think you can yeah you can

00:33:19,140 --> 00:33:23,610
continue discussion freely on the

00:33:20,910 --> 00:33:25,470
breakout room and this was the last

00:33:23,610 --> 00:33:29,340
session of the day I think I'm deepest

00:33:25,470 --> 00:33:30,899
to at least so thank you a lot for this

00:33:29,340 --> 00:33:33,899
this this

00:33:30,899 --> 00:33:35,789
so I didn't really know about the

00:33:33,899 --> 00:33:37,799
subject but learned a lot and it was

00:33:35,789 --> 00:33:40,619
clear for me so thank you

00:33:37,799 --> 00:33:43,379
I guess it was for the others - thank

00:33:40,619 --> 00:33:44,820
you initiate and have a great end of day

00:33:43,379 --> 00:33:46,019
thank you bye thank you I am for

00:33:44,820 --> 00:33:48,349
moderating thank you for all your

00:33:46,019 --> 00:33:48,349
patience

00:33:54,279 --> 00:33:56,340

YouTube URL: https://www.youtube.com/watch?v=_XCu-zpKG6A


