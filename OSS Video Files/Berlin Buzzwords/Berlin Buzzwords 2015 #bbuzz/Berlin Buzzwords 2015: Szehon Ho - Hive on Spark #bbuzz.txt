Title: Berlin Buzzwords 2015: Szehon Ho - Hive on Spark #bbuzz
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Apache Hive is a popular SQL interface for batch processing and ETL using Apache Hadoop.  Until recently, MapReduce was the only Hadoop execution engine for Hive queries. But today, alternative execution engines are available â€” such as Apache Spark and Apache Tez. The Hive and Spark communities are joining forces to introduce Spark as a new execution engine option for Hive.

In this talk we'll discuss the Hive on Spark project. Topics include the motivations, such as improving Hive user experience and streamlining operational management for Spark shops, some background and comparisons of MapReduce and Spark, and the technical process of porting a complex real-world application from MapReduce to Spark. 

Read more:
https://2015.berlinbuzzwords.de/session/hive-spark

About Szehon Ho:
https://2015.berlinbuzzwords.de/users/szehon-ho

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:00,000 --> 00:00:02,030
Oh

00:00:05,979 --> 00:00:14,570
thank you good morning everyone my name

00:00:11,360 --> 00:00:17,420
is Sehun ho the topic of my

00:00:14,570 --> 00:00:20,150
sparked and first I will try to

00:00:17,420 --> 00:00:20,810
introduce myself and my company that I

00:00:20,150 --> 00:00:24,289
work for

00:00:20,810 --> 00:00:26,960
so I come from cloud era yeah so I work

00:00:24,289 --> 00:00:31,269
in the Palo Alto office in cloud era and

00:00:26,960 --> 00:00:34,370
our company has a distribution of Hadoop

00:00:31,269 --> 00:00:36,230
called CDH and basically we bundle

00:00:34,370 --> 00:00:38,180
Hadoop along with dozens of other

00:00:36,230 --> 00:00:40,699
open-source projects that work together

00:00:38,180 --> 00:00:43,210
very well with Hadoop and on top of that

00:00:40,699 --> 00:00:46,250
we also sell some enterprise and

00:00:43,210 --> 00:00:49,250
management security tools for people to

00:00:46,250 --> 00:00:51,890
use with their Hadoop cluster in cloud

00:00:49,250 --> 00:00:53,180
era I am a member of the hive team so we

00:00:51,890 --> 00:00:54,829
have a lot of people working on

00:00:53,180 --> 00:00:57,500
different projects I work on project

00:00:54,829 --> 00:00:59,510
called hive as part of my job I'm also

00:00:57,500 --> 00:01:01,969
working with the hive community which is

00:00:59,510 --> 00:01:03,890
a very big worldwide community of people

00:01:01,969 --> 00:01:07,070
working on the product so I'm a

00:01:03,890 --> 00:01:09,470
committer and pmc on the project and a

00:01:07,070 --> 00:01:11,180
little thing about myself I'm very

00:01:09,470 --> 00:01:14,030
excited to be here in Berlin

00:01:11,180 --> 00:01:16,340
I visited Germany last year I really

00:01:14,030 --> 00:01:17,990
liked it and I'm very grateful for

00:01:16,340 --> 00:01:19,729
Berlin buzzwords to give me the

00:01:17,990 --> 00:01:22,310
opportunity to come back here and

00:01:19,729 --> 00:01:27,020
hopefully this talk will also be

00:01:22,310 --> 00:01:30,020
interesting to some people here so the

00:01:27,020 --> 00:01:34,189
talk is gonna be split into three parts

00:01:30,020 --> 00:01:36,619
first is a background hive what is hive

00:01:34,189 --> 00:01:39,229
what is spark and why we think hive on

00:01:36,619 --> 00:01:41,390
spark is a good project then some

00:01:39,229 --> 00:01:43,670
technical deep dive into the project

00:01:41,390 --> 00:01:46,310
some of the details and finally end with

00:01:43,670 --> 00:01:48,979
a user view like how this project will

00:01:46,310 --> 00:01:53,390
help users of hive get better

00:01:48,979 --> 00:01:56,930
performance out of the their jobs so so

00:01:53,390 --> 00:01:58,759
first is about hive what is hive hive

00:01:56,930 --> 00:02:02,119
really is a project that came with the

00:01:58,759 --> 00:02:05,240
whole Hadoop kind of revolution in 2005

00:02:02,119 --> 00:02:07,189
when the open source version of Google's

00:02:05,240 --> 00:02:10,759
MapReduce was first introduced to the

00:02:07,189 --> 00:02:13,220
world as an Apache project at that time

00:02:10,759 --> 00:02:15,739
it was quite a ground groundbreaker

00:02:13,220 --> 00:02:18,019
because a lot of organizations had this

00:02:15,739 --> 00:02:20,090
need to process huge amounts of data and

00:02:18,019 --> 00:02:23,150
the open source version of MapReduce

00:02:20,090 --> 00:02:26,380
allowed them to do so to scale to

00:02:23,150 --> 00:02:28,430
thousands of nodes and terabytes of data

00:02:26,380 --> 00:02:30,950
so a lot of web company

00:02:28,430 --> 00:02:33,950
like Yahoo Facebook LinkedIn they all

00:02:30,950 --> 00:02:36,140
took this up but really there was a need

00:02:33,950 --> 00:02:38,659
as part of this project to have a sequel

00:02:36,140 --> 00:02:41,239
access layer because a lot of people

00:02:38,659 --> 00:02:43,129
still I mean MapReduce you can write

00:02:41,239 --> 00:02:45,769
programs against but in order to bring

00:02:43,129 --> 00:02:47,719
MapReduce to power to ordinary people

00:02:45,769 --> 00:02:48,950
who might not program or even people who

00:02:47,719 --> 00:02:51,349
can program but don't want to write a

00:02:48,950 --> 00:02:52,819
complicated program you need a SQL

00:02:51,349 --> 00:02:58,430
access layer on top of this so that's

00:02:52,819 --> 00:02:59,780
really the story of hive and hive the

00:02:58,430 --> 00:03:01,579
main use case

00:02:59,780 --> 00:03:04,280
I mean hive has been in production right

00:03:01,579 --> 00:03:07,879
now for almost eight years I guess so

00:03:04,280 --> 00:03:09,500
it's a very feature-rich very mature it

00:03:07,879 --> 00:03:11,450
can be used in a lot of different areas

00:03:09,500 --> 00:03:13,790
but the main use case is really the

00:03:11,450 --> 00:03:15,769
online analytics where you know Hadoop

00:03:13,790 --> 00:03:17,810
strong suit is processing a lots of data

00:03:15,769 --> 00:03:19,489
and doing analysis across the data set

00:03:17,810 --> 00:03:23,329
kind of like what some data warehouse

00:03:19,489 --> 00:03:25,459
tools do and because it's really been

00:03:23,329 --> 00:03:27,319
around for so long hive has become kind

00:03:25,459 --> 00:03:31,099
of the de facto standard for sicko and

00:03:27,319 --> 00:03:33,530
Hadoop meaning that all you know sequin

00:03:31,099 --> 00:03:35,829
jhin's that coming out who wants to work

00:03:33,530 --> 00:03:38,780
on how to really look to hive and try to

00:03:35,829 --> 00:03:43,819
make sure they support the hive features

00:03:38,780 --> 00:03:48,019
and it's the popularity is very amazing

00:03:43,819 --> 00:03:50,689
like in cloud era about 100 almost 100%

00:03:48,019 --> 00:03:53,540
of our enterprise customers use hive

00:03:50,689 --> 00:03:55,970
just again because of the ease it allows

00:03:53,540 --> 00:03:59,269
ordinary people to be able to run these

00:03:55,970 --> 00:04:01,699
huge MapReduce jobs so a little diagram

00:03:59,269 --> 00:04:04,370
of where hive sits it's on the top of

00:04:01,699 --> 00:04:06,949
the stack so map reduces the processing

00:04:04,370 --> 00:04:09,590
engine backing hive and then HDFS of

00:04:06,949 --> 00:04:13,639
course is the storage for the Hadoop

00:04:09,590 --> 00:04:17,539
file system so the second part of the

00:04:13,639 --> 00:04:20,900
story is spark so spark really came as a

00:04:17,539 --> 00:04:23,449
second wave of big data innovation the

00:04:20,900 --> 00:04:25,340
problem with MapReduce it was like it's

00:04:23,449 --> 00:04:28,250
very powerful but it wasn't it wasn't

00:04:25,340 --> 00:04:30,320
designed with the general purpose in

00:04:28,250 --> 00:04:32,479
mind it was more designed for googles it

00:04:30,320 --> 00:04:35,150
was an internal Google project to help

00:04:32,479 --> 00:04:37,909
them do the search indices so a lot of

00:04:35,150 --> 00:04:39,770
projects like tears like fling like

00:04:37,909 --> 00:04:42,200
spark a lot of these new buzz projects

00:04:39,770 --> 00:04:44,330
kind of have a hindsight

00:04:42,200 --> 00:04:46,010
of hindsight to look at Map Reduce and

00:04:44,330 --> 00:04:49,850
be able to do things a lot better than

00:04:46,010 --> 00:04:53,390
what is being done in Map Reduce so in

00:04:49,850 --> 00:04:55,100
cloud era we have a lot of time kept our

00:04:53,390 --> 00:04:57,140
eyes on spark and we think it's really

00:04:55,100 --> 00:05:00,230
the kind of the front-runner to be the

00:04:57,140 --> 00:05:03,080
next MapReduce and the industry has kind

00:05:00,230 --> 00:05:04,940
of come to that as well because if you

00:05:03,080 --> 00:05:06,410
look at the activity among the Apache

00:05:04,940 --> 00:05:09,020
projects you know Apache Software

00:05:06,410 --> 00:05:12,230
Foundation has probably hundreds of

00:05:09,020 --> 00:05:15,020
projects under the umbrella and by now

00:05:12,230 --> 00:05:17,300
spark is the most active one and we I

00:05:15,020 --> 00:05:19,760
mean the success of a project in Apache

00:05:17,300 --> 00:05:21,950
really is the bigness the largeness of

00:05:19,760 --> 00:05:24,920
the community so in that sense I think

00:05:21,950 --> 00:05:27,020
spark really is the front-runner and

00:05:24,920 --> 00:05:29,030
also around the community other Apache

00:05:27,020 --> 00:05:31,580
projects are all moving a lot of them

00:05:29,030 --> 00:05:33,500
are moving to spark I know a lot of

00:05:31,580 --> 00:05:34,880
people and this Berlin buzzwords gave

00:05:33,500 --> 00:05:36,680
talks about how they're moving their

00:05:34,880 --> 00:05:39,230
projects over to spark as well they're

00:05:36,680 --> 00:05:42,200
not on this list so it's very catching a

00:05:39,230 --> 00:05:45,170
lot of ground very quickly and the

00:05:42,200 --> 00:05:47,840
reason spark is so popular I think is

00:05:45,170 --> 00:05:50,600
just because it was designed with very

00:05:47,840 --> 00:05:53,540
powerful API is very expressive api's

00:05:50,600 --> 00:05:55,520
and abstractions really meant to make it

00:05:53,540 --> 00:05:57,940
very easy to program a distributed

00:05:55,520 --> 00:06:02,900
processing pipeline just the way that

00:05:57,940 --> 00:06:05,480
you want it so some details of that so

00:06:02,900 --> 00:06:07,760
three things that I think that spark can

00:06:05,480 --> 00:06:11,090
do better than that produce and that you

00:06:07,760 --> 00:06:14,060
know users of spark also steal our first

00:06:11,090 --> 00:06:14,870
is the the file the data format so in

00:06:14,060 --> 00:06:18,920
Map Reduce

00:06:14,870 --> 00:06:21,380
if you remember everything is HDFS files

00:06:18,920 --> 00:06:24,200
it's the HDFS files that are fed into

00:06:21,380 --> 00:06:26,930
the job and then output of a job whereas

00:06:24,200 --> 00:06:29,240
in SPARC they made a new concept called

00:06:26,930 --> 00:06:31,460
RTD which you treated almost like a file

00:06:29,240 --> 00:06:34,100
in that you've passed it around between

00:06:31,460 --> 00:06:36,020
jobs but the point is our T DS is

00:06:34,100 --> 00:06:38,300
abstraction because a lot of times they

00:06:36,020 --> 00:06:41,090
can be kept into memory so spark you

00:06:38,300 --> 00:06:43,370
know being built in around 2009 had the

00:06:41,090 --> 00:06:45,350
benefit of being around at a time when

00:06:43,370 --> 00:06:47,450
the systems today have a lot more memory

00:06:45,350 --> 00:06:51,770
than they did when MapReduce was there

00:06:47,450 --> 00:06:53,180
so by having an RDD concept and hiding

00:06:51,770 --> 00:06:55,280
the fact that you're dealing with a file

00:06:53,180 --> 00:06:55,669
or in memory you're really making more

00:06:55,280 --> 00:06:58,249
Bennett

00:06:55,669 --> 00:07:01,520
of the resources on your cluster another

00:06:58,249 --> 00:07:03,169
advantage of SPARC is just the fact that

00:07:01,520 --> 00:07:04,849
in the programming in SPARC there's a

00:07:03,169 --> 00:07:07,969
lot of more transformations you can do

00:07:04,849 --> 00:07:09,919
so if you ever see a spark program to do

00:07:07,969 --> 00:07:11,810
a word count for example it's much

00:07:09,919 --> 00:07:15,560
easier to write then in MapReduce and

00:07:11,810 --> 00:07:17,029
also MapReduce has a kind of a design

00:07:15,560 --> 00:07:19,610
where you have to write a mapper or

00:07:17,029 --> 00:07:21,560
reducer and then the framework execute

00:07:19,610 --> 00:07:24,499
it in that order map the shuffle reduce

00:07:21,560 --> 00:07:26,210
whereas in SPARC you can run things in

00:07:24,499 --> 00:07:28,000
any order that you want so you can get a

00:07:26,210 --> 00:07:30,680
lot more efficient program that way and

00:07:28,000 --> 00:07:33,409
we use these two quite heavily on hive

00:07:30,680 --> 00:07:37,189
on SPARC to make hive run a lot faster

00:07:33,409 --> 00:07:38,779
and the last one is something maybe

00:07:37,189 --> 00:07:42,289
people don't realize is that in

00:07:38,779 --> 00:07:44,870
MapReduce every tasks so like a map task

00:07:42,289 --> 00:07:47,000
or reduce tasks are these Java processes

00:07:44,870 --> 00:07:49,189
and they all come up when they need to

00:07:47,000 --> 00:07:51,770
be run and then the idea shut down once

00:07:49,189 --> 00:07:54,080
the job is over so if you have a

00:07:51,770 --> 00:07:55,939
thousand you know mappers and 500

00:07:54,080 --> 00:07:57,439
reducers you're spawning and killing one

00:07:55,939 --> 00:08:00,229
thousand five hundred processes every

00:07:57,439 --> 00:08:01,879
time so this is if you add it up you

00:08:00,229 --> 00:08:04,550
know the JVM overhead can be kind of

00:08:01,879 --> 00:08:06,139
substantial for SPARC they again

00:08:04,550 --> 00:08:08,050
abstract it out so that you have these

00:08:06,139 --> 00:08:11,060
things called executors which are

00:08:08,050 --> 00:08:13,969
long-lived processes and they can be

00:08:11,060 --> 00:08:15,680
there and run multiple tasks so you kind

00:08:13,969 --> 00:08:17,870
of don't have to pay the cost every time

00:08:15,680 --> 00:08:19,729
of when you run a spark job of spawning

00:08:17,870 --> 00:08:22,639
all these processes and then destroying

00:08:19,729 --> 00:08:24,050
them so these three things really are

00:08:22,639 --> 00:08:26,240
just I mean this is just kind of the tip

00:08:24,050 --> 00:08:28,279
but these three things we really take

00:08:26,240 --> 00:08:33,199
advantage of in hi one spark to make

00:08:28,279 --> 00:08:35,510
hive much more efficient so it brings us

00:08:33,199 --> 00:08:37,940
to hive on spark which is the project

00:08:35,510 --> 00:08:41,599
that we've been working on for probably

00:08:37,940 --> 00:08:44,120
half a year or two a year and the goal

00:08:41,599 --> 00:08:46,190
is well hive is a sequel access layer

00:08:44,120 --> 00:08:47,240
and as I mentioned there's probably you

00:08:46,190 --> 00:08:49,250
know a lot of people there's probably

00:08:47,240 --> 00:08:51,220
thousands of hive deployments around the

00:08:49,250 --> 00:08:54,140
world solving real world problems today

00:08:51,220 --> 00:08:56,630
and as an access layer we want the user

00:08:54,140 --> 00:08:59,390
to not care underneath hive what the

00:08:56,630 --> 00:09:02,540
processing engine is so by switch

00:08:59,390 --> 00:09:03,980
swapping out MapReduce with spark we're

00:09:02,540 --> 00:09:05,660
hoping that they can keep all the

00:09:03,980 --> 00:09:08,209
functionality that they have today and

00:09:05,660 --> 00:09:09,560
also be take advantage of the new spark

00:09:08,209 --> 00:09:12,650
concepts like you know better

00:09:09,560 --> 00:09:15,920
in memory processing better process

00:09:12,650 --> 00:09:18,830
latency and so forth so the goal is to

00:09:15,920 --> 00:09:21,560
reiterate is one we want hive to be able

00:09:18,830 --> 00:09:24,620
to run seamlessly across execution

00:09:21,560 --> 00:09:26,450
engines so the same hive instance if you

00:09:24,620 --> 00:09:28,160
point it to a MapReduce cluster or a

00:09:26,450 --> 00:09:30,380
spark cluster or even test clusters

00:09:28,160 --> 00:09:33,290
should be able to run the same jobs and

00:09:30,380 --> 00:09:34,940
secondly obviously I've on spark we will

00:09:33,290 --> 00:09:37,790
need to support the full range of

00:09:34,940 --> 00:09:40,040
existing hive features so that's kind of

00:09:37,790 --> 00:09:43,220
a background about why we decided to do

00:09:40,040 --> 00:09:45,320
the project and the next section is

00:09:43,220 --> 00:09:47,720
going to be about some technical details

00:09:45,320 --> 00:09:52,190
about how we achieve hive and spark

00:09:47,720 --> 00:09:54,200
project so the Highland spark as I

00:09:52,190 --> 00:09:56,960
mentioned before the real challenge I

00:09:54,200 --> 00:09:58,700
mean it's one thing to kind of build a

00:09:56,960 --> 00:10:00,710
new system that's obviously very

00:09:58,700 --> 00:10:02,630
challenging but I think for this project

00:10:00,710 --> 00:10:04,910
the challenge was to powder pour a very

00:10:02,630 --> 00:10:06,740
mature system that again is solving

00:10:04,910 --> 00:10:09,790
real-life problems today a huge scale

00:10:06,740 --> 00:10:13,790
onto a new processing engine without any

00:10:09,790 --> 00:10:16,610
loss of functionality and there's a lot

00:10:13,790 --> 00:10:18,170
of functionality built into hive if you

00:10:16,610 --> 00:10:20,240
ever work with the database you know

00:10:18,170 --> 00:10:22,960
that the power of the database is how

00:10:20,240 --> 00:10:25,490
much of the sequel syntaxes supports and

00:10:22,960 --> 00:10:27,140
sequel syntax well it's deceptively

00:10:25,490 --> 00:10:30,589
simple it can get you know very

00:10:27,140 --> 00:10:32,570
complicated very quickly and also a lot

00:10:30,589 --> 00:10:34,310
of times databases they they have things

00:10:32,570 --> 00:10:35,120
that are not specified in the sequel

00:10:34,310 --> 00:10:37,310
syntax called

00:10:35,120 --> 00:10:39,910
things called dialects sequel dialects

00:10:37,310 --> 00:10:42,290
and so people have gotten really used to

00:10:39,910 --> 00:10:44,600
you know hive sequel dialects for

00:10:42,290 --> 00:10:46,690
example because sequels syntax you know

00:10:44,600 --> 00:10:49,339
sequel standard only supports up to some

00:10:46,690 --> 00:10:50,630
specification beyond that you need your

00:10:49,339 --> 00:10:52,910
database itself to have this

00:10:50,630 --> 00:10:55,820
functionality and other things like data

00:10:52,910 --> 00:10:58,580
types so how many how hive treats data

00:10:55,820 --> 00:11:01,280
types when you'd want to define a int or

00:10:58,580 --> 00:11:02,839
a string or a you know carve our car

00:11:01,280 --> 00:11:04,970
binary things like that how hives

00:11:02,839 --> 00:11:06,890
implicitly cast things from one data

00:11:04,970 --> 00:11:08,600
type to another these things people have

00:11:06,890 --> 00:11:10,990
gotten really used to that we want don't

00:11:08,600 --> 00:11:14,120
want to break when we do have on spark

00:11:10,990 --> 00:11:15,740
people might know hive has really a real

00:11:14,120 --> 00:11:17,959
big library of functions that you can

00:11:15,740 --> 00:11:19,880
you can define yourself and then use in

00:11:17,959 --> 00:11:22,070
your sequel like for example of some

00:11:19,880 --> 00:11:23,660
mathematical standard deviation function

00:11:22,070 --> 00:11:25,970
or linear functions

00:11:23,660 --> 00:11:28,790
thing you can define and hive has a very

00:11:25,970 --> 00:11:30,920
big library of those and also file

00:11:28,790 --> 00:11:32,960
formats the fact that hive has these sir

00:11:30,920 --> 00:11:36,530
days that you can swap out your favorite

00:11:32,960 --> 00:11:40,130
file format like RC file part K file etc

00:11:36,530 --> 00:11:42,500
so all these things in order to keep the

00:11:40,130 --> 00:11:45,230
same what we ended up doing is we try to

00:11:42,500 --> 00:11:47,600
keep most of the execution code in the

00:11:45,230 --> 00:11:49,130
hive operators to be the same across

00:11:47,600 --> 00:11:51,020
different processing engines we don't

00:11:49,130 --> 00:11:56,000
want to rewrite every single operator

00:11:51,020 --> 00:11:57,680
just for SPARC so the idea is that in

00:11:56,000 --> 00:12:01,370
general we will run the same code that

00:11:57,680 --> 00:12:02,960
we used to run in MapReduce on - on the

00:12:01,370 --> 00:12:05,390
equivalent thing in spark which is a

00:12:02,960 --> 00:12:06,890
spark transformation so for example in

00:12:05,390 --> 00:12:09,110
the mapper if we used to run a filter

00:12:06,890 --> 00:12:11,450
operator that dealt with a filter

00:12:09,110 --> 00:12:14,060
function in sequel now we run the same

00:12:11,450 --> 00:12:15,770
operator in SPARC similar in reducer we

00:12:14,060 --> 00:12:19,460
run the same in a spark transformation

00:12:15,770 --> 00:12:21,380
so it's by doing this we integrate a

00:12:19,460 --> 00:12:25,160
spark at a low level and managed to keep

00:12:21,380 --> 00:12:27,140
all the functionality of hive now people

00:12:25,160 --> 00:12:29,420
might be wondering then like if we're

00:12:27,140 --> 00:12:31,580
just running the same code in spark

00:12:29,420 --> 00:12:34,670
where's the benefit come from and so I'm

00:12:31,580 --> 00:12:37,490
gonna spend the next few slides talking

00:12:34,670 --> 00:12:39,040
about different ways that just by doing

00:12:37,490 --> 00:12:41,900
this we can still get a lot of savings

00:12:39,040 --> 00:12:44,090
in terms of performance for hive queries

00:12:41,900 --> 00:12:45,830
written in spark and I think this might

00:12:44,090 --> 00:12:48,110
be also interesting to people who are

00:12:45,830 --> 00:12:50,420
you know not hive users but just in

00:12:48,110 --> 00:12:53,060
general want to migrate MapReduce jobs

00:12:50,420 --> 00:12:54,620
on to spark or people who you know once

00:12:53,060 --> 00:12:56,780
you write spark jobs for the first time

00:12:54,620 --> 00:12:59,150
this might also give you some insight

00:12:56,780 --> 00:13:03,200
about how you can optimize things in

00:12:59,150 --> 00:13:05,240
when when doing in spark so the first

00:13:03,200 --> 00:13:07,370
improvement is that we can eliminate a

00:13:05,240 --> 00:13:10,280
lot of redundant phases that we have in

00:13:07,370 --> 00:13:12,620
MapReduce as I mentioned before in

00:13:10,280 --> 00:13:14,720
MapReduce if you know you have a mapper

00:13:12,620 --> 00:13:16,970
you have to have a reducer and in

00:13:14,720 --> 00:13:19,250
between there's a shuffle phase so if

00:13:16,970 --> 00:13:21,140
that doesn't work for your job and a lot

00:13:19,250 --> 00:13:23,540
of hive queries actually go away beyond

00:13:21,140 --> 00:13:25,160
just one job you have to have another

00:13:23,540 --> 00:13:26,510
job so if you want another shuffle for

00:13:25,160 --> 00:13:29,750
example you have to have another mapper

00:13:26,510 --> 00:13:31,160
another shuffle another reducer and you

00:13:29,750 --> 00:13:33,110
know typical hive queries that we've

00:13:31,160 --> 00:13:35,870
seen from customers are probably

00:13:33,110 --> 00:13:36,980
averaged 10 MapReduce jobs and then you

00:13:35,870 --> 00:13:39,139
know some really big

00:13:36,980 --> 00:13:44,060
whereas maybe got 250 or you know more

00:13:39,139 --> 00:13:46,639
MapReduce jobs so a lot of this is kind

00:13:44,060 --> 00:13:49,459
of wasted because like between the

00:13:46,639 --> 00:13:51,620
reducer and mapper there's really not

00:13:49,459 --> 00:13:53,269
much there so if we can in spark because

00:13:51,620 --> 00:13:55,790
we can run the transformations in any

00:13:53,269 --> 00:13:57,829
order we can go ahead and eliminate the

00:13:55,790 --> 00:14:00,380
redundant phase and just on the bottom

00:13:57,829 --> 00:14:02,899
have three kind of phases of spark

00:14:00,380 --> 00:14:05,000
transformations and run the same code

00:14:02,899 --> 00:14:07,519
that we used to do in MapReduce so the

00:14:05,000 --> 00:14:09,079
rule of thumb is for every stage that

00:14:07,519 --> 00:14:10,820
used to have every job you used to have

00:14:09,079 --> 00:14:15,949
in MapReduce we can eliminate one phase

00:14:10,820 --> 00:14:18,290
in the spark transformation and so to

00:14:15,949 --> 00:14:20,959
give kind of a maybe an example of this

00:14:18,290 --> 00:14:22,670
so one query in highest that will boil

00:14:20,959 --> 00:14:25,220
down to more than one MapReduce job is

00:14:22,670 --> 00:14:27,740
if you have apologized some of this cut

00:14:25,220 --> 00:14:30,230
off but if you have a joint statement

00:14:27,740 --> 00:14:32,149
like in the inside I have select key

00:14:30,230 --> 00:14:34,310
from source one joint source two and

00:14:32,149 --> 00:14:36,470
after that I feed that into an order by

00:14:34,310 --> 00:14:38,570
statement you're gonna get two MapReduce

00:14:36,470 --> 00:14:41,329
jobs the first one to do the join the

00:14:38,570 --> 00:14:44,240
second one to do the order by but in

00:14:41,329 --> 00:14:47,269
spark we can just do it in one big spark

00:14:44,240 --> 00:14:49,639
job and also combine the second the

00:14:47,269 --> 00:14:52,279
first reducer and second mapper into the

00:14:49,639 --> 00:14:53,930
same the same transformation there to

00:14:52,279 --> 00:14:56,300
get so we actually run the same

00:14:53,930 --> 00:14:58,850
operators that we used to run but now we

00:14:56,300 --> 00:15:02,560
eliminate a whole stage in the data

00:14:58,850 --> 00:15:06,529
pipeline using just by using spark so

00:15:02,560 --> 00:15:10,730
that's some savings pretty significant

00:15:06,529 --> 00:15:13,699
savings and it actually gets better than

00:15:10,730 --> 00:15:16,160
that because as I mentioned before in

00:15:13,699 --> 00:15:18,920
MapReduce there's this concept of files

00:15:16,160 --> 00:15:20,990
and every MapReduce job will write out

00:15:18,920 --> 00:15:22,610
this huge file and then the next

00:15:20,990 --> 00:15:24,920
MapReduce job has to read back in this

00:15:22,610 --> 00:15:26,839
huge file just because that's the only

00:15:24,920 --> 00:15:29,690
way for the two jobs to communicate with

00:15:26,839 --> 00:15:31,040
each other and in fact in hive if you

00:15:29,690 --> 00:15:33,410
ever used hive you have to define a

00:15:31,040 --> 00:15:35,750
temporary directory where what it really

00:15:33,410 --> 00:15:37,699
is is like the the partial results from

00:15:35,750 --> 00:15:39,649
one MapReduce job are kept there and

00:15:37,699 --> 00:15:43,519
then they're read back in when the next

00:15:39,649 --> 00:15:46,730
MapReduce job runs so in spark again

00:15:43,519 --> 00:15:48,620
this problem doesn't exist anymore one

00:15:46,730 --> 00:15:49,570
because we've combined the reducer and

00:15:48,620 --> 00:15:52,450
mapper so

00:15:49,570 --> 00:15:53,830
really there's no need to kind of you

00:15:52,450 --> 00:15:56,890
know channel data from one job to

00:15:53,830 --> 00:16:00,010
another but even when we do like for

00:15:56,890 --> 00:16:01,960
example from transform to the shuffle to

00:16:00,010 --> 00:16:04,900
the transform where we do need to pass

00:16:01,960 --> 00:16:07,330
data along we use the spark RTD concept

00:16:04,900 --> 00:16:10,570
so spark already anyway keeps that into

00:16:07,330 --> 00:16:12,910
the memory so the way RTD works is

00:16:10,570 --> 00:16:15,490
unless you explicitly ask to spill to

00:16:12,910 --> 00:16:17,080
disk or if you run out of memory across

00:16:15,490 --> 00:16:20,320
all your processes only then does it

00:16:17,080 --> 00:16:22,180
spill to this so if you configure enough

00:16:20,320 --> 00:16:24,790
memory for your cluster a lot of this

00:16:22,180 --> 00:16:27,400
stuff can be kept in in the memory and

00:16:24,790 --> 00:16:29,200
at very low low low latency when you

00:16:27,400 --> 00:16:32,260
move from one transformation to another

00:16:29,200 --> 00:16:35,110
so that's one another improvement that

00:16:32,260 --> 00:16:38,980
we can achieve by doing the high bond

00:16:35,110 --> 00:16:43,800
spark running the same operators but at

00:16:38,980 --> 00:16:46,750
a more efficient transformations and

00:16:43,800 --> 00:16:48,910
kind of the next one is the shuffle

00:16:46,750 --> 00:16:50,380
phase so this is the last thing I'm

00:16:48,910 --> 00:16:53,200
going to talk about for the execution

00:16:50,380 --> 00:16:57,670
part of the of the problem I've coded

00:16:53,200 --> 00:16:59,080
so inside so that was talking about if

00:16:57,670 --> 00:17:01,170
you have more than one MapReduce job

00:16:59,080 --> 00:17:03,310
even if you have one MapReduce job

00:17:01,170 --> 00:17:05,860
there's a shuffle phase that we can

00:17:03,310 --> 00:17:08,230
improve on and if you know a little bit

00:17:05,860 --> 00:17:10,540
about MapReduce you know that shuffle is

00:17:08,230 --> 00:17:12,310
really after the mapper is done the

00:17:10,540 --> 00:17:16,150
output is kind of shuffled to different

00:17:12,310 --> 00:17:18,370
nodes and then sent to the reducers so a

00:17:16,150 --> 00:17:21,430
lot of MapReduce jobs this is one of the

00:17:18,370 --> 00:17:23,770
more expensive parts because shuffling I

00:17:21,430 --> 00:17:25,449
mean it's not file i/o but it's still a

00:17:23,770 --> 00:17:27,790
lot of network i/o you're still moving

00:17:25,449 --> 00:17:30,010
around potentially you know gigabytes or

00:17:27,790 --> 00:17:33,190
terabytes of data across the network

00:17:30,010 --> 00:17:35,440
so in SPARC you can't get around

00:17:33,190 --> 00:17:37,450
shuffling but you can do sometimes you

00:17:35,440 --> 00:17:40,900
can get more efficient shuffling because

00:17:37,450 --> 00:17:43,000
MapReduce in my next slide so MapReduce

00:17:40,900 --> 00:17:44,980
has only one thing called a shuffle sort

00:17:43,000 --> 00:17:46,750
so when the shuffle phase happens what

00:17:44,980 --> 00:17:49,780
it means is that if it does a hash

00:17:46,750 --> 00:17:52,870
partitioning of all the data and then it

00:17:49,780 --> 00:17:55,030
sorts each of the partition so this

00:17:52,870 --> 00:17:56,710
works for all the problems but then it

00:17:55,030 --> 00:17:59,320
sometimes in fact a lot of times it's

00:17:56,710 --> 00:18:00,910
overkill for what you want to do so to

00:17:59,320 --> 00:18:02,639
give some examples to help illustrate

00:18:00,910 --> 00:18:05,549
that point

00:18:02,639 --> 00:18:08,909
so for a query like select so it's

00:18:05,549 --> 00:18:10,889
aggregation query if you know SQL so the

00:18:08,909 --> 00:18:14,190
point is you group by a key and then you

00:18:10,889 --> 00:18:16,080
select average from each key group so

00:18:14,190 --> 00:18:18,269
basically in this example we don't

00:18:16,080 --> 00:18:20,340
really need to sort each key group after

00:18:18,269 --> 00:18:22,109
you have all the rows of the same key in

00:18:20,340 --> 00:18:24,599
one group all you need to do is take the

00:18:22,109 --> 00:18:26,789
average of them but in MapReduce it will

00:18:24,599 --> 00:18:29,820
do secondary sorting anyway which is a

00:18:26,789 --> 00:18:32,460
little unnecessary and not able to turn

00:18:29,820 --> 00:18:33,509
off so in spark you can call just by

00:18:32,460 --> 00:18:37,080
calling a different transformation

00:18:33,509 --> 00:18:38,669
shuffle transformation like group by you

00:18:37,080 --> 00:18:41,039
can get the exact behavior you want

00:18:38,669 --> 00:18:43,529
which is just to hatch partition by key

00:18:41,039 --> 00:18:45,269
just make sure the data rows with the

00:18:43,529 --> 00:18:48,359
same key go to the same partition and

00:18:45,269 --> 00:18:51,809
stop stop at that so we get some freedom

00:18:48,359 --> 00:18:53,700
that way another one is the ordered by

00:18:51,809 --> 00:18:56,279
so another example is select key from

00:18:53,700 --> 00:18:58,320
table order by key so this query means

00:18:56,279 --> 00:19:00,239
you just have you know you have this

00:18:58,320 --> 00:19:03,899
huge table and you just want to sort get

00:19:00,239 --> 00:19:06,299
a total order sorting of it inspark you

00:19:03,899 --> 00:19:08,700
can call an order by transformation that

00:19:06,299 --> 00:19:10,229
does that in parallel so spark has

00:19:08,700 --> 00:19:13,080
already you know just by calling this

00:19:10,229 --> 00:19:14,759
API has a lot of logic already to do so

00:19:13,080 --> 00:19:16,649
what it does is it takes samplings of

00:19:14,759 --> 00:19:19,830
all your data makes appropriate range

00:19:16,649 --> 00:19:22,529
partitions for example 1 to 10 11 to 20

00:19:19,830 --> 00:19:24,539
and then have the data partition in each

00:19:22,529 --> 00:19:28,409
range and then parallel sort those to

00:19:24,539 --> 00:19:30,059
get a total sorted result in MapReduce

00:19:28,409 --> 00:19:32,399
the traditional way if you wanted to

00:19:30,059 --> 00:19:36,059
write a sword in MapReduce is you just

00:19:32,399 --> 00:19:38,219
Park it's a kind of a you know a stupid

00:19:36,059 --> 00:19:40,289
way is like you actually just partition

00:19:38,219 --> 00:19:41,580
everything to one partition and then the

00:19:40,289 --> 00:19:43,169
MapReduce anyway sorts all the

00:19:41,580 --> 00:19:46,080
partitions so you get it sorted by

00:19:43,169 --> 00:19:49,710
default but it's in serial instead of

00:19:46,080 --> 00:19:51,239
parallel so again by running this

00:19:49,710 --> 00:19:54,779
transformation in spark you achieve a

00:19:51,239 --> 00:19:58,019
more savings for the shuffle phase so

00:19:54,779 --> 00:19:59,519
that was that was all I had about the

00:19:58,019 --> 00:20:02,249
execution part just the fact that

00:19:59,519 --> 00:20:04,619
between MapReduce jobs and inside

00:20:02,249 --> 00:20:06,659
MapReduce jobs just by using spark were

00:20:04,619 --> 00:20:09,479
able to achieve a lot of savings that

00:20:06,659 --> 00:20:12,359
way as I mentioned before the second

00:20:09,479 --> 00:20:14,339
part is this process life cycle so if

00:20:12,359 --> 00:20:15,990
you remember MapReduce all the mappers

00:20:14,339 --> 00:20:18,450
and all the reducers

00:20:15,990 --> 00:20:20,400
are actually one process and then when

00:20:18,450 --> 00:20:22,050
they're done they just shut down so

00:20:20,400 --> 00:20:24,540
they're a lot of the MapReduce job is

00:20:22,050 --> 00:20:27,150
just spawning and killing processes so

00:20:24,540 --> 00:20:29,640
in spark again the executors are what

00:20:27,150 --> 00:20:31,800
they call processes and they're quite

00:20:29,640 --> 00:20:35,610
long-lived and they can run one or more

00:20:31,800 --> 00:20:37,320
tasks and another terminology in spark

00:20:35,610 --> 00:20:39,540
is called an application so a set of

00:20:37,320 --> 00:20:42,059
executors for a given user is called a

00:20:39,540 --> 00:20:44,030
spark application and a user can choose

00:20:42,059 --> 00:20:46,080
you know how many executors he wants

00:20:44,030 --> 00:20:48,900
Tunas so that he can increase or

00:20:46,080 --> 00:20:51,120
decrease it and so what we do in haiwan

00:20:48,900 --> 00:20:53,510
spark is that for every hive user

00:20:51,120 --> 00:20:56,190
session so every time you log into hive

00:20:53,510 --> 00:20:58,440
and you run a spark query we're gonna

00:20:56,190 --> 00:21:00,330
start a spark application for that user

00:20:58,440 --> 00:21:02,580
and based on what you configure we're

00:21:00,330 --> 00:21:05,460
going to pre start that many executors

00:21:02,580 --> 00:21:07,260
for you so that only the first query

00:21:05,460 --> 00:21:09,570
will incur the cost of starting all

00:21:07,260 --> 00:21:12,420
these processes subsequent queries will

00:21:09,570 --> 00:21:17,030
just reuse the processes and not pay the

00:21:12,420 --> 00:21:20,130
cost of spawning all these processes so

00:21:17,030 --> 00:21:23,220
just to give kind of a graphical thing

00:21:20,130 --> 00:21:25,320
about what I talked about in MapReduce

00:21:23,220 --> 00:21:30,059
every stage will have a lot of process

00:21:25,320 --> 00:21:33,179
kind of life going up and down and but

00:21:30,059 --> 00:21:35,240
in spark what you do is you define for a

00:21:33,179 --> 00:21:37,320
given application like initial number of

00:21:35,240 --> 00:21:40,200
executors the minimum and the maximum

00:21:37,320 --> 00:21:41,610
and then or you can even give us up just

00:21:40,200 --> 00:21:43,970
you know everything is the same number

00:21:41,610 --> 00:21:47,520
and then the per number of processes

00:21:43,970 --> 00:21:49,920
stays more constant throughout the whole

00:21:47,520 --> 00:21:53,429
spark jobs in fact across the whole

00:21:49,920 --> 00:21:55,770
session in multiple spark jobs processes

00:21:53,429 --> 00:21:58,530
don't need to be spawned anymore so

00:21:55,770 --> 00:22:02,490
that's kind of for the technical part so

00:21:58,530 --> 00:22:05,250
for the user part so I'm just gonna

00:22:02,490 --> 00:22:06,780
spend a little time talking about you

00:22:05,250 --> 00:22:09,270
know after that what kind of results we

00:22:06,780 --> 00:22:11,460
got and how users might be able to

00:22:09,270 --> 00:22:16,679
leverage hyewon spark if you are a hive

00:22:11,460 --> 00:22:20,040
user so to leverage this thing again we

00:22:16,679 --> 00:22:22,410
try to make the installation not as more

00:22:20,040 --> 00:22:24,030
complicated than it is today so anyway

00:22:22,410 --> 00:22:25,860
when you install hive today you have to

00:22:24,030 --> 00:22:28,720
install first hadoop which is the file

00:22:25,860 --> 00:22:30,580
system another thing is

00:22:28,720 --> 00:22:33,130
spark can actually run on different

00:22:30,580 --> 00:22:35,530
resource managers so you know there's

00:22:33,130 --> 00:22:38,020
yarn there's mesos and these things what

00:22:35,530 --> 00:22:39,850
they do is they make sure that users of

00:22:38,020 --> 00:22:41,710
spark different users of spark

00:22:39,850 --> 00:22:43,360
don't you know collide with each other

00:22:41,710 --> 00:22:46,720
and that their jobs are scheduled

00:22:43,360 --> 00:22:49,120
properly so we we recommend in Qatar to

00:22:46,720 --> 00:22:51,100
use the yarn mode in and in fact we

00:22:49,120 --> 00:22:53,830
shipped with the yarn mode as default so

00:22:51,100 --> 00:22:56,950
the reason is it gives you a little more

00:22:53,830 --> 00:22:59,200
control over the number of processes

00:22:56,950 --> 00:23:02,169
that an application can have so all this

00:22:59,200 --> 00:23:04,360
stuff of dynamically increasing or

00:23:02,169 --> 00:23:09,340
decreasing the number of processes is

00:23:04,360 --> 00:23:10,780
only in spark on yarn mode and so the

00:23:09,340 --> 00:23:12,789
only extra step here is you have to

00:23:10,780 --> 00:23:16,990
install spark and then after that you

00:23:12,789 --> 00:23:18,220
install hive which is so if your install

00:23:16,990 --> 00:23:19,960
hives before you know that if you

00:23:18,220 --> 00:23:21,700
install hive on a MapReduce node and

00:23:19,960 --> 00:23:23,919
that Hadoop node has you know

00:23:21,700 --> 00:23:26,110
configuration so just pick up those to

00:23:23,919 --> 00:23:29,650
find out where the MapReduce master is

00:23:26,110 --> 00:23:32,020
so similarly when you install hive on a

00:23:29,650 --> 00:23:33,520
spark node it will just pick up from the

00:23:32,020 --> 00:23:35,860
spark configurations where the spark

00:23:33,520 --> 00:23:37,780
master is what the spark sterilizer is

00:23:35,860 --> 00:23:40,539
so forth so you really don't need to

00:23:37,780 --> 00:23:42,130
configure anything the versions you have

00:23:40,539 --> 00:23:43,990
to run with I mean these are the ones

00:23:42,130 --> 00:23:46,330
that we laid the latest ones that we

00:23:43,990 --> 00:23:47,770
shipped with is these versions here hive

00:23:46,330 --> 00:23:50,799
one point one spark one point three

00:23:47,770 --> 00:23:52,900
hundred two point six if you download

00:23:50,799 --> 00:23:54,309
the latest cloud era five point four in

00:23:52,900 --> 00:23:55,450
fact we actually shipped with all these

00:23:54,309 --> 00:23:58,539
versions so you don't have to worry

00:23:55,450 --> 00:24:00,370
about compatibility issue and then the

00:23:58,539 --> 00:24:02,980
hive client all you have to do is set

00:24:00,370 --> 00:24:04,419
the execution engine to spark as I

00:24:02,980 --> 00:24:06,520
mentioned for one of our goals is to

00:24:04,419 --> 00:24:10,150
have the same hive be able to run jobs

00:24:06,520 --> 00:24:12,880
in MapReduce and spark so you can switch

00:24:10,150 --> 00:24:14,559
that dynamically what cluster you want

00:24:12,880 --> 00:24:16,690
you to run your job with then you just

00:24:14,559 --> 00:24:18,280
run the query and keep in mind the first

00:24:16,690 --> 00:24:19,990
query may take a little more time to

00:24:18,280 --> 00:24:21,700
execute because we're again we're

00:24:19,990 --> 00:24:24,429
pre-warming the cluster and starting up

00:24:21,700 --> 00:24:26,590
these executors but subsequent jobs will

00:24:24,429 --> 00:24:29,559
be able to reuse the executors and get a

00:24:26,590 --> 00:24:33,190
lot of performance benefit so when you

00:24:29,559 --> 00:24:35,409
run the job if you ever run a hive job

00:24:33,190 --> 00:24:38,200
you run the job and then a lot of logs

00:24:35,409 --> 00:24:41,260
come and basically this they used to

00:24:38,200 --> 00:24:42,310
show the MapReduce job status like how

00:24:41,260 --> 00:24:44,890
many mappers are running

00:24:42,310 --> 00:24:46,600
how many reducers are running it looks a

00:24:44,890 --> 00:24:48,340
little different in spark but the idea

00:24:46,600 --> 00:24:51,430
is the same just how many spark tests

00:24:48,340 --> 00:24:57,780
are running at any time so not too much

00:24:51,430 --> 00:25:00,520
different there then in the spark UI

00:24:57,780 --> 00:25:02,170
sorry the yarn UI you can actually find

00:25:00,520 --> 00:25:06,580
the spark application that you started

00:25:02,170 --> 00:25:08,650
click on that and then you're able to

00:25:06,580 --> 00:25:10,810
drill down into the spark UI which is

00:25:08,650 --> 00:25:13,210
again it's a little cut off but the

00:25:10,810 --> 00:25:14,650
spark you I will show it kind of shows

00:25:13,210 --> 00:25:16,180
the same as what the console prints out

00:25:14,650 --> 00:25:18,510
in terms of job progress but you can

00:25:16,180 --> 00:25:21,430
click to see more details about what

00:25:18,510 --> 00:25:22,930
tasks are running how many are there and

00:25:21,430 --> 00:25:25,480
also like environment variables

00:25:22,930 --> 00:25:28,570
executors what whatever one is doing and

00:25:25,480 --> 00:25:29,020
so forth so pretty easy to monitor your

00:25:28,570 --> 00:25:32,950
job

00:25:29,020 --> 00:25:36,670
also on spark and things you might want

00:25:32,950 --> 00:25:38,950
to tune so again I talked about a spark

00:25:36,670 --> 00:25:41,650
application how it's pre starts a number

00:25:38,950 --> 00:25:43,480
of executors for you and the idea is you

00:25:41,650 --> 00:25:46,060
can set like a constant number which is

00:25:43,480 --> 00:25:48,310
static allocation or on yarn mode you

00:25:46,060 --> 00:25:50,230
can set a dynamic number so the two

00:25:48,310 --> 00:25:52,480
differences is if you set a static

00:25:50,230 --> 00:25:55,210
number obviously you get everyone gets

00:25:52,480 --> 00:25:56,890
ten executors okay well then the

00:25:55,210 --> 00:25:58,900
performance is gonna be more stable

00:25:56,890 --> 00:26:01,480
because you're always gonna run on those

00:25:58,900 --> 00:26:03,010
nodes but then you might end up with

00:26:01,480 --> 00:26:04,960
some users not being able to get in

00:26:03,010 --> 00:26:09,100
because if 10 people already allocated

00:26:04,960 --> 00:26:11,260
100 executors you only have 100 then the

00:26:09,100 --> 00:26:13,900
next guy will have to he will not be

00:26:11,260 --> 00:26:16,480
able to get any resource so dynamic

00:26:13,900 --> 00:26:18,430
allocation is a better way there's

00:26:16,480 --> 00:26:21,580
slightly more unpredictability for

00:26:18,430 --> 00:26:24,100
performance but then what you can do is

00:26:21,580 --> 00:26:26,170
you can specify a range like I start at

00:26:24,100 --> 00:26:28,600
1 but I can go up to 10 and if no jobs

00:26:26,170 --> 00:26:31,000
are running it will go back down to the

00:26:28,600 --> 00:26:34,420
minimum number so these are two options

00:26:31,000 --> 00:26:36,550
you can use in hive to to and spark in

00:26:34,420 --> 00:26:38,260
fact even if you don't use hive you're

00:26:36,550 --> 00:26:39,790
just running spark jobs you can you can

00:26:38,260 --> 00:26:44,320
use these two options - - and how many

00:26:39,790 --> 00:26:47,470
executors given that application has and

00:26:44,320 --> 00:26:50,140
then again spark executors you can

00:26:47,470 --> 00:26:53,320
another thing to tune is like the memory

00:26:50,140 --> 00:26:54,790
so the memory will affect obviously how

00:26:53,320 --> 00:26:57,190
much can cash into

00:26:54,790 --> 00:26:59,500
memory how much of the data of the RTD

00:26:57,190 --> 00:27:02,560
can be kept into memory so this affects

00:26:59,500 --> 00:27:04,420
the performance so spot has kind of a

00:27:02,560 --> 00:27:07,480
lot of settings for memory one is the

00:27:04,420 --> 00:27:09,160
cores but also has memory settings and

00:27:07,480 --> 00:27:12,460
then another one is the driver which is

00:27:09,160 --> 00:27:14,950
the spark like the master executor I

00:27:12,460 --> 00:27:19,890
guess and you can tune that one maybe it

00:27:14,950 --> 00:27:22,870
has a little more so you can tune those

00:27:19,890 --> 00:27:25,330
so so that's kind of it for the user

00:27:22,870 --> 00:27:29,170
view if you want to play around with

00:27:25,330 --> 00:27:31,810
Highland spark now I like to share some

00:27:29,170 --> 00:27:33,730
performance benchmarks that we made so

00:27:31,810 --> 00:27:36,220
we have people working from Intel China

00:27:33,730 --> 00:27:39,220
and they helped us do these benchmarks

00:27:36,220 --> 00:27:40,870
so I'm just taking their numbers so they

00:27:39,220 --> 00:27:44,110
played it around with a cluster of eight

00:27:40,870 --> 00:27:47,620
nodes where every node has 32 cores 64

00:27:44,110 --> 00:27:49,990
gigs of ram 10,000 megabytes per second

00:27:47,620 --> 00:27:51,790
for network so kind of a little large

00:27:49,990 --> 00:27:54,070
but typical like a data center

00:27:51,790 --> 00:27:58,080
configuration and these are the versions

00:27:54,070 --> 00:28:00,910
that we use for the different components

00:27:58,080 --> 00:28:04,360
and we played around with the TPC DS

00:28:00,910 --> 00:28:06,640
data sets with two data sizes 320 gigs

00:28:04,360 --> 00:28:07,960
which is kind of a average data size and

00:28:06,640 --> 00:28:11,650
four terabytes which is a little on the

00:28:07,960 --> 00:28:13,930
larger side and TPC TS is if you don't

00:28:11,650 --> 00:28:16,480
know is the standard kind of

00:28:13,930 --> 00:28:18,550
benchmarking for all the sequel like

00:28:16,480 --> 00:28:20,860
databases use it quite heavily and now

00:28:18,550 --> 00:28:24,610
some of the big data on how sequel

00:28:20,860 --> 00:28:26,620
solutions are also using it so and we

00:28:24,610 --> 00:28:30,250
also have some of these configurations

00:28:26,620 --> 00:28:32,140
that we enable to make sure all the

00:28:30,250 --> 00:28:34,990
processing engines get equal kind of

00:28:32,140 --> 00:28:37,570
play in fact and maybe as interesting

00:28:34,990 --> 00:28:39,640
side like all these optimizations that

00:28:37,570 --> 00:28:41,350
are in hive code is able to be run

00:28:39,640 --> 00:28:43,990
across different processing engines

00:28:41,350 --> 00:28:45,700
without any loss of functionality memory

00:28:43,990 --> 00:28:47,500
vectorization for example is just to

00:28:45,700 --> 00:28:49,840
make sure hive operators when they do

00:28:47,500 --> 00:28:51,790
loops that there's in such a way that

00:28:49,840 --> 00:28:55,480
they can benefit from the multi

00:28:51,790 --> 00:28:58,210
instruction of a CPU CBO is the fact

00:28:55,480 --> 00:28:59,530
that you're so in all the execution

00:28:58,210 --> 00:29:01,930
engines we're running the operators in

00:28:59,530 --> 00:29:03,670
the same order but CBO makes sure that

00:29:01,930 --> 00:29:06,370
the operators are run in optimal order

00:29:03,670 --> 00:29:07,650
so it does things like maybe reorder

00:29:06,370 --> 00:29:09,620
some join or

00:29:07,650 --> 00:29:12,540
push down some predicate to the front

00:29:09,620 --> 00:29:15,600
and then finally the last one is kind of

00:29:12,540 --> 00:29:17,420
a strangely named flag but what it is is

00:29:15,600 --> 00:29:20,430
make sure some joints get turned into

00:29:17,420 --> 00:29:22,020
map joints if there's enough memory it's

00:29:20,430 --> 00:29:23,760
a different joint implementation that's

00:29:22,020 --> 00:29:28,200
a lot faster but you have to have enough

00:29:23,760 --> 00:29:30,240
memory to do it and so not gonna spend

00:29:28,200 --> 00:29:33,300
too much time but these are the kind of

00:29:30,240 --> 00:29:35,940
what we the Intel guys set to make sure

00:29:33,300 --> 00:29:41,370
that both these tears and spark are

00:29:35,940 --> 00:29:43,410
tuned very properly and again we because

00:29:41,370 --> 00:29:45,200
the first query is the one that kind of

00:29:43,410 --> 00:29:47,970
pre allocates a lot of these executors

00:29:45,200 --> 00:29:50,610
what we do is run each quarry twice so

00:29:47,970 --> 00:29:51,870
the first is to warm up the cluster and

00:29:50,610 --> 00:29:54,390
then the second is the one that we

00:29:51,870 --> 00:29:56,120
actually measure the results and so not

00:29:54,390 --> 00:29:58,559
only spark has this issue but even tez

00:29:56,120 --> 00:30:01,500
some tests has something called pre warm

00:29:58,559 --> 00:30:03,480
containers which is basically the first

00:30:01,500 --> 00:30:05,309
query will spawn a lot of containers up

00:30:03,480 --> 00:30:09,929
and then the second one will be able to

00:30:05,309 --> 00:30:13,200
use those so for 320 gigabytes we have

00:30:09,929 --> 00:30:16,350
some nice results I mean we don't beat

00:30:13,200 --> 00:30:18,780
so we beat map spark - spark is faster

00:30:16,350 --> 00:30:21,690
than have on MapReduce for all the

00:30:18,780 --> 00:30:23,429
queries but I mean and then we're at

00:30:21,690 --> 00:30:25,290
faster than high vantes on a lot of

00:30:23,429 --> 00:30:29,010
queries but there are some queries that

00:30:25,290 --> 00:30:32,040
we're still working on - to catch up to

00:30:29,010 --> 00:30:34,200
two days and if you look at four

00:30:32,040 --> 00:30:38,970
terabytes the results is more pronounced

00:30:34,200 --> 00:30:41,429
so for I mean the the ratios is most

00:30:38,970 --> 00:30:42,630
mostly the same but then the the gains

00:30:41,429 --> 00:30:44,550
is more pronounced and that's just

00:30:42,630 --> 00:30:46,380
because again I mentioned a lot of the

00:30:44,550 --> 00:30:48,660
savings we have in hi one spark is you

00:30:46,380 --> 00:30:51,150
know not flushing everything to disk or

00:30:48,660 --> 00:30:54,030
not shuffling everything in this kind of

00:30:51,150 --> 00:30:57,390
a inefficient way so if the data is more

00:30:54,030 --> 00:31:02,309
you actually get more savings from some

00:30:57,390 --> 00:31:04,170
of this stuff so and tazed so some of

00:31:02,309 --> 00:31:06,300
the I think what they did is some of the

00:31:04,170 --> 00:31:08,070
range is a little small so they blew it

00:31:06,300 --> 00:31:12,630
up and then just have another comparison

00:31:08,070 --> 00:31:15,330
for spark versus tazed so so for most of

00:31:12,630 --> 00:31:17,400
the queries were we're faster than the

00:31:15,330 --> 00:31:20,670
tazed but some of them were slower than

00:31:17,400 --> 00:31:21,270
tazed and all I'll talk about that in

00:31:20,670 --> 00:31:24,720
the next slide

00:31:21,270 --> 00:31:26,760
which is so spark is faster at most

00:31:24,720 --> 00:31:28,860
queries we're working on some

00:31:26,760 --> 00:31:30,150
optimizations to make spark faster for

00:31:28,860 --> 00:31:32,460
some queries that are slower than

00:31:30,150 --> 00:31:35,670
today's stem things like dynamic

00:31:32,460 --> 00:31:37,710
partition pruning which is kind of a

00:31:35,670 --> 00:31:39,510
hive concept means when you join two

00:31:37,710 --> 00:31:41,580
tables you can prune away some

00:31:39,510 --> 00:31:43,380
partitions of another table based on the

00:31:41,580 --> 00:31:45,960
fact that the first table you know only

00:31:43,380 --> 00:31:47,670
has these join keys so you don't

00:31:45,960 --> 00:31:50,310
actually need to read these partitions

00:31:47,670 --> 00:31:53,580
or the second table to join it's kind of

00:31:50,310 --> 00:31:56,160
a more advanced optimization that we're

00:31:53,580 --> 00:31:57,960
working on for a headline spark in fact

00:31:56,160 --> 00:32:01,800
one of my colleagues is already working

00:31:57,960 --> 00:32:05,160
on this and has a patch ready and spark

00:32:01,800 --> 00:32:06,480
is also kind of a newer project so a lot

00:32:05,160 --> 00:32:08,730
of some of the shuffle that they do

00:32:06,480 --> 00:32:12,030
shuffle sort that they do is not as fast

00:32:08,730 --> 00:32:14,160
as it could be so but the the real

00:32:12,030 --> 00:32:16,050
benefit of spark not really is is not

00:32:14,160 --> 00:32:17,970
like that spark as fast today but also

00:32:16,050 --> 00:32:19,890
the promise that spark can be faster in

00:32:17,970 --> 00:32:23,190
the future just because it's such a

00:32:19,890 --> 00:32:25,290
healthy and a vibrant community and they

00:32:23,190 --> 00:32:26,880
already you know have a lot of things in

00:32:25,290 --> 00:32:28,350
the works like shuffle start

00:32:26,880 --> 00:32:29,700
improvements already in the works and

00:32:28,350 --> 00:32:31,950
then they have things like project

00:32:29,700 --> 00:32:34,170
tungsten that can you know promise to do

00:32:31,950 --> 00:32:36,180
a lot of more improvements for spark so

00:32:34,170 --> 00:32:37,650
we're very confident that just by doing

00:32:36,180 --> 00:32:40,350
this project in the future we're gonna

00:32:37,650 --> 00:32:46,800
get even more benefits for hive users on

00:32:40,350 --> 00:32:48,660
top of spark so so actually I leave some

00:32:46,800 --> 00:32:50,760
time for questions so actually this is

00:32:48,660 --> 00:32:53,100
perfect because I the last slides just

00:32:50,760 --> 00:32:57,060
to say that hive on spark is available

00:32:53,100 --> 00:33:00,540
in these versions hive 1.1 and CDH 5.4

00:32:57,060 --> 00:33:01,770
and if you want to look at the code or

00:33:00,540 --> 00:33:04,410
any of the work we did

00:33:01,770 --> 00:33:07,320
there's hive uses the JIRA system so

00:33:04,410 --> 00:33:10,200
high of 72 92 is the JIRA umbrella JIRA

00:33:07,320 --> 00:33:12,510
for all the hive on spark work and then

00:33:10,200 --> 00:33:14,880
it's not just a one company effort in

00:33:12,510 --> 00:33:16,080
fact we had many companies like Intel

00:33:14,880 --> 00:33:18,810
people from China

00:33:16,080 --> 00:33:22,350
IBM app are some help from data brings

00:33:18,810 --> 00:33:25,290
and from our company too so it's kind of

00:33:22,350 --> 00:33:27,390
a joint effort and yeah so I think

00:33:25,290 --> 00:33:30,410
that's pretty much all I had so I'll

00:33:27,390 --> 00:33:30,410
leave time for some questions

00:33:36,000 --> 00:33:38,060

YouTube URL: https://www.youtube.com/watch?v=U8xQnGslkFI


