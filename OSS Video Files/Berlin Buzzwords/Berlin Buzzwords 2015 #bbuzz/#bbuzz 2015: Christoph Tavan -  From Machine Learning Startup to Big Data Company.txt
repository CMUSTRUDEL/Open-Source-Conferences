Title: #bbuzz 2015: Christoph Tavan -  From Machine Learning Startup to Big Data Company
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	How to start a company based on a machine learning idea? And how to scale it into the "Big Data" region?

In this talk I want to share some insights that I gathered during the last 3 years while founding and successfully scaling a real-time bidding (RTB) company from a two-person startup to a leading technology provider in the field:

- From fancy algorithms to production-proof algorithms.
- From thousands of model evaluations per day to trillions.
- From megabytes to petabytes.
- From real-time to batch to real-time.
- From two people to entire teams of data scientists and engineers.

Read more:
https://2015.berlinbuzzwords.de/session/machine-learning-startup-big-data-company

About Christoph Tavan:
https://2015.berlinbuzzwords.de/users/christoph-tavan

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:00,000 --> 00:00:02,030
Oh

00:00:05,720 --> 00:00:10,920
yeah hi welcome everybody so I hope I

00:00:09,059 --> 00:00:12,750
will keep you awake although it's

00:00:10,920 --> 00:00:14,940
already in the evening and second

00:00:12,750 --> 00:00:17,520
conference day thank you for joining so

00:00:14,940 --> 00:00:20,240
my topic today will be from machine

00:00:17,520 --> 00:00:22,890
learning startup to big data company and

00:00:20,240 --> 00:00:25,019
while it will be probably a little less

00:00:22,890 --> 00:00:27,330
technical than many of the talks that

00:00:25,019 --> 00:00:28,820
you've seen here so far I still hope

00:00:27,330 --> 00:00:32,099
that you will be able to gain some

00:00:28,820 --> 00:00:33,780
relevant insights and before we get

00:00:32,099 --> 00:00:36,300
started I'd like to get to know you a

00:00:33,780 --> 00:00:38,549
little better so maybe a little hands up

00:00:36,300 --> 00:00:43,500
around who of you is currently working

00:00:38,549 --> 00:00:45,299
in a start-up yeah at least some who

00:00:43,500 --> 00:00:52,229
would say that he's working in a sort of

00:00:45,299 --> 00:00:53,400
an established company Oh more maybe due

00:00:52,229 --> 00:00:55,920
to the fact that it's a big data

00:00:53,400 --> 00:01:00,839
conference who's working with machine

00:00:55,920 --> 00:01:02,940
learning ok also quite a lot and who's

00:01:00,839 --> 00:01:09,030
working with Hadoop like anything from

00:01:02,940 --> 00:01:11,520
the ecosystem alright good so yeah my

00:01:09,030 --> 00:01:13,830
talk today I want to share some lessons

00:01:11,520 --> 00:01:15,680
that we learned while founding and

00:01:13,830 --> 00:01:19,020
scaling a machine learning startup and

00:01:15,680 --> 00:01:21,750
in particular I want to talk about all

00:01:19,020 --> 00:01:23,490
the major mistakes that we made so if

00:01:21,750 --> 00:01:25,910
you happen to still be in a startup

00:01:23,490 --> 00:01:35,540
phase then you can maybe avoid them

00:01:25,910 --> 00:01:35,540
before I get started what does it work

00:01:36,080 --> 00:01:43,530
ok yeah I'm Christoph and that's me in

00:01:41,730 --> 00:01:47,580
San Francisco where I had a great

00:01:43,530 --> 00:01:52,770
holiday recently I got into developing

00:01:47,580 --> 00:01:56,000
and about 2001 and I still or yet

00:01:52,770 --> 00:01:58,229
decided to study physics after school

00:01:56,000 --> 00:02:01,650
and I also finished my bachelor's degree

00:01:58,229 --> 00:02:05,040
and then sometime in 2011 while I was

00:02:01,650 --> 00:02:07,890
doing my master studies there was this

00:02:05,040 --> 00:02:09,450
great and interesting startup

00:02:07,890 --> 00:02:11,569
opportunity and this is basically also

00:02:09,450 --> 00:02:15,270
where the story begins

00:02:11,569 --> 00:02:17,090
so back then I was getting in touch with

00:02:15,270 --> 00:02:21,060
a few physicists

00:02:17,090 --> 00:02:25,620
they were doing research and machine

00:02:21,060 --> 00:02:27,680
learning for about I guess something

00:02:25,620 --> 00:02:29,820
more than 10 years at that point in time

00:02:27,680 --> 00:02:31,950
because I mean a lot of stuff that he

00:02:29,820 --> 00:02:33,920
doing theoretical physics is what you

00:02:31,950 --> 00:02:36,210
would now adays call machine learning

00:02:33,920 --> 00:02:39,870
they were particularly into clustering

00:02:36,210 --> 00:02:42,060
and it was already some years ago that

00:02:39,870 --> 00:02:45,150
they had done an interesting research on

00:02:42,060 --> 00:02:47,910
a large online market which was eBay and

00:02:45,150 --> 00:02:50,370
then they realized it with their machine

00:02:47,910 --> 00:02:52,110
learning techniques they could find out

00:02:50,370 --> 00:02:54,660
lots of stuff about the behavior of

00:02:52,110 --> 00:02:59,610
users on this on this on its market and

00:02:54,660 --> 00:03:01,260
they said hey our our algorithms

00:02:59,610 --> 00:03:02,430
actually seem to be really really useful

00:03:01,260 --> 00:03:08,310
for marketing

00:03:02,430 --> 00:03:11,040
we should make money with it and they

00:03:08,310 --> 00:03:13,950
had the idea that basically all it takes

00:03:11,040 --> 00:03:15,120
is just to build some API around it and

00:03:13,950 --> 00:03:17,130
you have a great product because you

00:03:15,120 --> 00:03:20,490
have great we have great algorithms and

00:03:17,130 --> 00:03:22,140
we will have a useful product and to do

00:03:20,490 --> 00:03:25,769
this they basically found some business

00:03:22,140 --> 00:03:27,930
guy for the making money part and they

00:03:25,769 --> 00:03:33,209
found me for the just building an API

00:03:27,930 --> 00:03:35,190
part and we founded a company and well

00:03:33,209 --> 00:03:35,700
at that point in time I thought sounds

00:03:35,190 --> 00:03:38,310
great

00:03:35,700 --> 00:03:41,720
making money but we're just doing API I

00:03:38,310 --> 00:03:44,370
should do that so I decided to quit my

00:03:41,720 --> 00:03:47,700
university studies and started that

00:03:44,370 --> 00:03:51,750
startup and as it turned out well not

00:03:47,700 --> 00:03:53,700
quite so the first thing that this

00:03:51,750 --> 00:03:56,820
business guy and me who were basically

00:03:53,700 --> 00:03:59,190
actively starting this startup was to do

00:03:56,820 --> 00:04:00,900
was like finding a product I mean we had

00:03:59,190 --> 00:04:02,910
bread algorithms Ben knew kind of what

00:04:00,900 --> 00:04:05,160
to do with it but we didn't really have

00:04:02,910 --> 00:04:06,989
a product yet and we had to find out

00:04:05,160 --> 00:04:08,940
about it the first thing we did was

00:04:06,989 --> 00:04:11,280
building recommend the engines you know

00:04:08,940 --> 00:04:14,340
that from e-commerce sites you you

00:04:11,280 --> 00:04:16,310
recommend some products it was

00:04:14,340 --> 00:04:19,470
relatively easy to start with

00:04:16,310 --> 00:04:23,850
straightforward we had good results we

00:04:19,470 --> 00:04:25,910
also get got in touch with some with

00:04:23,850 --> 00:04:28,180
some test customers who were willing to

00:04:25,910 --> 00:04:31,740
include our

00:04:28,180 --> 00:04:34,660
our technology could do a quick go live

00:04:31,740 --> 00:04:37,150
but we also realized that with each new

00:04:34,660 --> 00:04:40,389
customer that we wanted to implement

00:04:37,150 --> 00:04:43,120
with we had a lot of effort and we

00:04:40,389 --> 00:04:45,400
realized it simply wasn't a business

00:04:43,120 --> 00:04:47,259
that we could really scale because it

00:04:45,400 --> 00:04:51,639
was just too much effort for too little

00:04:47,259 --> 00:04:54,699
money so actually we found out that it

00:04:51,639 --> 00:04:55,720
wasn't really about finding a product to

00:04:54,699 --> 00:04:56,530
make use of our machine learning

00:04:55,720 --> 00:04:58,509
algorithms

00:04:56,530 --> 00:05:03,340
it was rather about finding the right

00:04:58,509 --> 00:05:05,919
product and what we did is we went a lot

00:05:03,340 --> 00:05:09,759
to conferences presented the concepts

00:05:05,919 --> 00:05:13,210
that we had we talked to potential

00:05:09,759 --> 00:05:15,250
customers and we were actually trying to

00:05:13,210 --> 00:05:19,030
find a product that also scales

00:05:15,250 --> 00:05:21,759
business-wise and it was at some

00:05:19,030 --> 00:05:23,710
conference here in Berlin as well the

00:05:21,759 --> 00:05:25,030
next conference where we got in touch

00:05:23,710 --> 00:05:28,900
with somebody from online advertising

00:05:25,030 --> 00:05:30,610
from a company called at meld who says

00:05:28,900 --> 00:05:32,320
well our marketplaces and online

00:05:30,610 --> 00:05:35,110
advertising are incredibly inefficient

00:05:32,320 --> 00:05:37,530
and we need machine learning technology

00:05:35,110 --> 00:05:40,740
like yours to make this more efficient

00:05:37,530 --> 00:05:44,400
so we got involved with this company and

00:05:40,740 --> 00:05:47,740
want you to start a project with them

00:05:44,400 --> 00:05:49,930
which didn't work out unfortunately

00:05:47,740 --> 00:05:51,370
because they were acquired by Google but

00:05:49,930 --> 00:05:54,849
it showed to us that it's actually

00:05:51,370 --> 00:05:56,770
apparently a market where you can build

00:05:54,849 --> 00:05:59,409
a business that business business that

00:05:56,770 --> 00:06:02,409
scales so basically we settled down to

00:05:59,409 --> 00:06:05,380
building a real time bidding system for

00:06:02,409 --> 00:06:07,659
online advertising but that also came

00:06:05,380 --> 00:06:09,009
with a cost that we realized Wow

00:06:07,659 --> 00:06:11,229
now we have to build even more

00:06:09,009 --> 00:06:13,030
infrastructure and it's even further

00:06:11,229 --> 00:06:15,630
away from the initial idea of just

00:06:13,030 --> 00:06:19,539
building an API around some algorithms

00:06:15,630 --> 00:06:21,610
so and that's basically my first lesson

00:06:19,539 --> 00:06:23,889
learned from an algorithm to a product

00:06:21,610 --> 00:06:27,009
so if you're coming from this maybe

00:06:23,889 --> 00:06:29,259
scientific or tech side and think you

00:06:27,009 --> 00:06:31,659
have great technology but you want and

00:06:29,259 --> 00:06:35,590
want to make money with it the first

00:06:31,659 --> 00:06:37,990
crucial thing is to find out about your

00:06:35,590 --> 00:06:39,780
actual product because even if

00:06:37,990 --> 00:06:41,580
everything is great and

00:06:39,780 --> 00:06:43,290
lucien arey that doesn't mean that you

00:06:41,580 --> 00:06:44,970
will immediately make make money with it

00:06:43,290 --> 00:06:50,460
you really have to build a product with

00:06:44,970 --> 00:06:53,220
it and also one of the key lessons was

00:06:50,460 --> 00:06:55,500
that probably we would have been a lot

00:06:53,220 --> 00:06:57,450
faster if back at that time we would

00:06:55,500 --> 00:07:00,120
have found somebody from this field from

00:06:57,450 --> 00:07:04,470
online advertising to get into the

00:07:00,120 --> 00:07:06,240
startup team because we had basically to

00:07:04,470 --> 00:07:08,100
find out about all the needs of our

00:07:06,240 --> 00:07:10,380
product the hard way by talking to

00:07:08,100 --> 00:07:13,260
customers by making lots of mistakes and

00:07:10,380 --> 00:07:15,480
fixing them if we had had somebody at

00:07:13,260 --> 00:07:17,280
that point in time who was more into

00:07:15,480 --> 00:07:20,940
this whole thing we would probably have

00:07:17,280 --> 00:07:23,280
been a lot faster so let me come to the

00:07:20,940 --> 00:07:26,820
three stages of a machine learning

00:07:23,280 --> 00:07:30,240
company that I or we have identified

00:07:26,820 --> 00:07:32,280
during the course of the year so first

00:07:30,240 --> 00:07:36,450
stage I would call the bootstrapping

00:07:32,280 --> 00:07:39,720
stage and I always have put up pictures

00:07:36,450 --> 00:07:41,070
of our offices onto these slides so this

00:07:39,720 --> 00:07:43,500
was basically two or three people

00:07:41,070 --> 00:07:46,290
sitting in a room trying to find the

00:07:43,500 --> 00:07:48,630
right product first and then trying to

00:07:46,290 --> 00:07:50,400
build a proof of concept and don't worry

00:07:48,630 --> 00:07:51,800
these beer bottles were not always on

00:07:50,400 --> 00:07:55,169
this desk

00:07:51,800 --> 00:07:59,490
and then we somehow managed to build

00:07:55,169 --> 00:08:01,710
this this proof of concept and then the

00:07:59,490 --> 00:08:04,500
next challenge is to enter the market

00:08:01,710 --> 00:08:08,250
and at that point in time hopefully your

00:08:04,500 --> 00:08:10,110
proof of concept shows enough the power

00:08:08,250 --> 00:08:12,570
of your innovation that you will

00:08:10,110 --> 00:08:15,150
convince some venture capitalists to

00:08:12,570 --> 00:08:17,130
give you some money because I mean you

00:08:15,150 --> 00:08:20,130
might you might make it without them but

00:08:17,130 --> 00:08:21,750
in many cases you simply need more

00:08:20,130 --> 00:08:24,810
resources to be able to enter the market

00:08:21,750 --> 00:08:28,320
and to start making customers happy so

00:08:24,810 --> 00:08:30,870
you will need some venture capital and

00:08:28,320 --> 00:08:32,700
this is basically the stage to where

00:08:30,870 --> 00:08:35,640
it's really about to establish your

00:08:32,700 --> 00:08:37,169
product get into the market and then

00:08:35,640 --> 00:08:39,030
eventually if you're successful with

00:08:37,169 --> 00:08:41,849
that you will get more and more

00:08:39,030 --> 00:08:44,769
customers this will also mean you will

00:08:41,849 --> 00:08:47,559
gather more and more data

00:08:44,769 --> 00:08:50,350
and hopefully you will have the need for

00:08:47,559 --> 00:08:51,970
scaling your product and of course you

00:08:50,350 --> 00:08:54,879
should also add some more features make

00:08:51,970 --> 00:08:56,739
everything more robust etc and this is

00:08:54,879 --> 00:09:02,860
basically the the point in time where

00:08:56,739 --> 00:09:05,129
you getting this big data region so

00:09:02,860 --> 00:09:07,350
quick summary the three steps

00:09:05,129 --> 00:09:10,660
bootstrapping find a proof-of-concept

00:09:07,350 --> 00:09:12,939
funded startup may make the market entry

00:09:10,660 --> 00:09:17,410
and pick data company it's all about

00:09:12,939 --> 00:09:18,850
scaling so now this what is the

00:09:17,410 --> 00:09:21,189
structure of a machine-learning company

00:09:18,850 --> 00:09:23,709
well first of all you need a team I will

00:09:21,189 --> 00:09:25,600
briefly talk about it in the end you

00:09:23,709 --> 00:09:29,040
need a product I already talked about it

00:09:25,600 --> 00:09:31,119
and you need an architecture because

00:09:29,040 --> 00:09:32,920
this is what it's all about and since

00:09:31,119 --> 00:09:35,290
this is always also tech company I want

00:09:32,920 --> 00:09:38,559
to also focus on this technology part

00:09:35,290 --> 00:09:42,160
for the rest of my talk and I think you

00:09:38,559 --> 00:09:43,899
have seen similar images to that already

00:09:42,160 --> 00:09:46,059
quite a lot of times and other talks on

00:09:43,899 --> 00:09:48,489
this conferences because this is what

00:09:46,059 --> 00:09:51,009
typical architectures for machine

00:09:48,489 --> 00:09:53,920
learning stacks look like so you have

00:09:51,009 --> 00:09:55,720
some source of events I heard the term

00:09:53,920 --> 00:09:59,740
internet of everything yesterday because

00:09:55,720 --> 00:10:02,470
it can be like the classical internet it

00:09:59,740 --> 00:10:06,309
can be Internet of Things sensor stock

00:10:02,470 --> 00:10:07,749
market data whatever so this is the

00:10:06,309 --> 00:10:09,249
outside this is where the data comes

00:10:07,749 --> 00:10:11,259
from then you usually have some real

00:10:09,249 --> 00:10:14,499
time component which accepts these

00:10:11,259 --> 00:10:18,809
events and which is basically the

00:10:14,499 --> 00:10:21,429
gateway into your internal architecture

00:10:18,809 --> 00:10:23,679
then these events are somehow promote

00:10:21,429 --> 00:10:25,119
moated into a pipeline where they are

00:10:23,679 --> 00:10:27,699
processed and they are usually also

00:10:25,119 --> 00:10:29,079
stored and then in the case of a machine

00:10:27,699 --> 00:10:32,049
learning company you will have some

00:10:29,079 --> 00:10:33,910
machine learning algorithms that make

00:10:32,049 --> 00:10:36,309
use of this data or be it in a batch

00:10:33,910 --> 00:10:38,819
fashion by accessing the stored data or

00:10:36,309 --> 00:10:41,559
in a streaming fashion by simply

00:10:38,819 --> 00:10:44,679
listening to these streams and they will

00:10:41,559 --> 00:10:46,779
do some math generate what I call models

00:10:44,679 --> 00:10:48,819
in this slide and feed that back into

00:10:46,779 --> 00:10:52,209
the real time part because of course you

00:10:48,819 --> 00:10:53,679
want to answer questions from the

00:10:52,209 --> 00:10:56,220
outside world in real time do

00:10:53,679 --> 00:10:56,220
predictions

00:10:56,589 --> 00:11:02,110
what was that in our case or for the

00:11:00,070 --> 00:11:04,510
rest of the talk I mean basically I

00:11:02,110 --> 00:11:06,760
think any of these parts would be at

00:11:04,510 --> 00:11:08,050
work on its own so I will focus on the

00:11:06,760 --> 00:11:10,360
data part for the rest

00:11:08,050 --> 00:11:13,720
what was it in our case of the product

00:11:10,360 --> 00:11:15,850
was a real-time bidding engine so the

00:11:13,720 --> 00:11:20,170
data source is the internet classic

00:11:15,850 --> 00:11:22,180
internet our real-time component

00:11:20,170 --> 00:11:25,510
consists of a bidding engine and a

00:11:22,180 --> 00:11:27,730
tracking engine these events are

00:11:25,510 --> 00:11:31,269
receiving store and processed and stored

00:11:27,730 --> 00:11:33,970
and our models are predictions for

00:11:31,269 --> 00:11:39,399
clicks conversions and that sort of

00:11:33,970 --> 00:11:41,829
stuff yeah maybe very brief interlude

00:11:39,399 --> 00:11:43,810
what is real-time bidding maybe some of

00:11:41,829 --> 00:11:48,000
you have seen the presentation by

00:11:43,810 --> 00:11:50,740
critter yesterday morning well many

00:11:48,000 --> 00:11:52,899
website owners or most website owners

00:11:50,740 --> 00:11:55,149
actually make their money selling ad

00:11:52,899 --> 00:11:59,890
spaces so you have a dedicated space on

00:11:55,149 --> 00:12:02,589
this website that that that is sold for

00:11:59,890 --> 00:12:05,230
for advertisement and how that usually

00:12:02,589 --> 00:12:06,790
works nowadays is that the request is

00:12:05,230 --> 00:12:08,829
forwarded to something called a

00:12:06,790 --> 00:12:12,490
supply-side platform which is basically

00:12:08,829 --> 00:12:14,920
a broker that offers this ad space to a

00:12:12,490 --> 00:12:17,680
handful or a couple of hundred so-called

00:12:14,920 --> 00:12:20,649
you mind sets demand-side platforms this

00:12:17,680 --> 00:12:23,050
is what we are doing and then we have a

00:12:20,649 --> 00:12:24,970
couple of milliseconds time to decide

00:12:23,050 --> 00:12:27,640
whether we want to buy this ad

00:12:24,970 --> 00:12:30,420
impression or not specialty website

00:12:27,640 --> 00:12:34,570
owners sell their ads basis advertisers

00:12:30,420 --> 00:12:38,770
buy the impressions yeah this is what

00:12:34,570 --> 00:12:40,630
we're doing just to show again that this

00:12:38,770 --> 00:12:44,500
has a real time component this round

00:12:40,630 --> 00:12:46,660
trip is bound to 100 milliseconds and

00:12:44,500 --> 00:12:48,699
let's look at the events again basically

00:12:46,660 --> 00:12:50,890
the events that concern us so basically

00:12:48,699 --> 00:12:53,589
it's the bid requests coming from the

00:12:50,890 --> 00:12:56,620
supply side platform then we figure out

00:12:53,589 --> 00:12:59,440
if we want to buy an advertiser in

00:12:56,620 --> 00:13:02,290
compression if we do so we sent back a

00:12:59,440 --> 00:13:04,899
bit and if we happen to have placed the

00:13:02,290 --> 00:13:07,010
highest bid we will receive the

00:13:04,899 --> 00:13:09,260
impression and later on we can track

00:13:07,010 --> 00:13:13,490
the user clicked or in the best case

00:13:09,260 --> 00:13:15,680
even bought something in a shop so the

00:13:13,490 --> 00:13:18,370
goal for us from a machine learning

00:13:15,680 --> 00:13:23,389
perspective is to predict these clicks

00:13:18,370 --> 00:13:27,199
or even better conversions so now let's

00:13:23,389 --> 00:13:30,649
have a look at the events or alone let's

00:13:27,199 --> 00:13:35,480
sleep for for the goal form for a one

00:13:30,649 --> 00:13:37,459
moment so what we have to pay is

00:13:35,480 --> 00:13:40,310
basically every impression each time we

00:13:37,459 --> 00:13:42,889
buy and displayed advertisement we have

00:13:40,310 --> 00:13:45,769
to pay what our advertisers actually

00:13:42,889 --> 00:13:47,750
want to want to pay is only for

00:13:45,769 --> 00:13:51,500
successes so basically only when they

00:13:47,750 --> 00:13:53,839
get a click or conversion later on that

00:13:51,500 --> 00:13:58,100
means that the revenue that we gain is

00:13:53,839 --> 00:13:59,269
only whenever success happens but so we

00:13:58,100 --> 00:14:02,570
have to figure out what we're actually

00:13:59,269 --> 00:14:05,630
willing to pay each time and this is why

00:14:02,570 --> 00:14:07,760
we have to predict the probability of a

00:14:05,630 --> 00:14:12,519
clink and this probability of click of

00:14:07,760 --> 00:14:14,930
course is dependant of lots of features

00:14:12,519 --> 00:14:17,209
via the user or the web site where is

00:14:14,930 --> 00:14:20,949
browsing the browser with is using etc

00:14:17,209 --> 00:14:23,720
so basically it's we predicted from data

00:14:20,949 --> 00:14:26,540
yeah let's go into this data pipeline

00:14:23,720 --> 00:14:27,529
now how does this data typically

00:14:26,540 --> 00:14:31,339
typically look like

00:14:27,529 --> 00:14:34,269
so it's essentially HTTP requests which

00:14:31,339 --> 00:14:37,220
means that it comes with a cookie IDE or

00:14:34,269 --> 00:14:39,980
with some user agent string with an IP

00:14:37,220 --> 00:14:41,750
address with the URL that this user is

00:14:39,980 --> 00:14:44,209
currently visiting all that sort of

00:14:41,750 --> 00:14:47,899
stuff so this is basically the raw data

00:14:44,209 --> 00:14:50,930
and well let's of course somehow unhandy

00:14:47,899 --> 00:14:53,029
at least for human beings so one of the

00:14:50,930 --> 00:14:55,370
very typical steps that happens is of

00:14:53,029 --> 00:14:57,620
course that you take such a data point

00:14:55,370 --> 00:14:59,750
and you see what features you can

00:14:57,620 --> 00:15:02,510
extract from it it's basically from a

00:14:59,750 --> 00:15:08,269
URL for example you can extract domain

00:15:02,510 --> 00:15:10,760
and or even just a private part or from

00:15:08,269 --> 00:15:12,980
an IP address you can extract you

00:15:10,760 --> 00:15:15,939
information or you can extract the ISP

00:15:12,980 --> 00:15:19,400
that this user is currently surfing with

00:15:15,939 --> 00:15:23,270
or from a user agent string you can

00:15:19,400 --> 00:15:28,310
extract information about the device

00:15:23,270 --> 00:15:32,000
that this guy's using it's basically to

00:15:28,310 --> 00:15:35,960
summarize the incoming events they sort

00:15:32,000 --> 00:15:38,570
of have what I would call they they they

00:15:35,960 --> 00:15:40,220
make up for the analytics dimensions so

00:15:38,570 --> 00:15:42,050
this is basically where data scientists

00:15:40,220 --> 00:15:44,089
will look into and will try to figure

00:15:42,050 --> 00:15:46,700
out what interesting features are

00:15:44,089 --> 00:15:48,650
actually in that raw data and on the

00:15:46,700 --> 00:15:51,950
right-hand side after a step that we

00:15:48,650 --> 00:15:53,570
call feature ization you sort of have

00:15:51,950 --> 00:15:55,310
these extracted features something that

00:15:53,570 --> 00:15:57,980
also humans can actually often reason

00:15:55,310 --> 00:16:00,170
about so for example somebody or a

00:15:57,980 --> 00:16:03,440
customer might want to know how many

00:16:00,170 --> 00:16:06,760
impressions and clicks he or she

00:16:03,440 --> 00:16:09,680
received in Berlin in the last few days

00:16:06,760 --> 00:16:14,140
yeah so this is basically the data that

00:16:09,680 --> 00:16:18,170
we were dealing with yeah and now let's

00:16:14,140 --> 00:16:21,890
have a look at how this data processing

00:16:18,170 --> 00:16:26,350
evolved in the three stages and what we

00:16:21,890 --> 00:16:26,350
all what we did wrong on the way

00:16:28,930 --> 00:16:36,680
all right so in the bootstrapping phase

00:16:34,209 --> 00:16:40,310
you have to imagine we were just like

00:16:36,680 --> 00:16:46,190
three people and we had to build such a

00:16:40,310 --> 00:16:47,660
system so we did it please don't laugh

00:16:46,190 --> 00:16:50,240
at me now when looking at these slides

00:16:47,660 --> 00:16:51,800
so we did it very very simple we just

00:16:50,240 --> 00:16:54,290
said this real-time tracking component

00:16:51,800 --> 00:16:58,760
it was getting these events and it was

00:16:54,290 --> 00:17:01,040
writing them as Jason to lock files we

00:16:58,760 --> 00:17:04,459
then eventually compressed these log

00:17:01,040 --> 00:17:06,679
files and rsync them to a separate

00:17:04,459 --> 00:17:08,480
server because we didn't want to do our

00:17:06,679 --> 00:17:12,020
math on the same server that was doing

00:17:08,480 --> 00:17:14,600
the real-time part then for reporting

00:17:12,020 --> 00:17:18,949
purposes we were basically just reading

00:17:14,600 --> 00:17:20,900
these files and batches extracting all

00:17:18,949 --> 00:17:23,780
the features that I showed you and

00:17:20,900 --> 00:17:27,319
basically piping them into a my sequel

00:17:23,780 --> 00:17:29,090
load data command so that we had at

00:17:27,319 --> 00:17:29,950
least some sequel database where we

00:17:29,090 --> 00:17:34,300
could could

00:17:29,950 --> 00:17:36,370
reports and for analytics those who are

00:17:34,300 --> 00:17:38,410
doing we were concerned with the data

00:17:36,370 --> 00:17:40,150
science part of our product they were

00:17:38,410 --> 00:17:43,660
actually taking enter our log files

00:17:40,150 --> 00:17:45,700
parsing them into Python extracting the

00:17:43,660 --> 00:17:48,340
stuff from the from the from the JSON

00:17:45,700 --> 00:17:50,280
objects and then doing their machine

00:17:48,340 --> 00:17:53,770
learning stuff with it

00:17:50,280 --> 00:17:57,190
so this is of course I mean it's obvious

00:17:53,770 --> 00:17:59,110
that this will not scale because I mean

00:17:57,190 --> 00:18:04,210
having just one storage server at some

00:17:59,110 --> 00:18:07,300
point the disk will simply run full also

00:18:04,210 --> 00:18:08,560
we were ingesting single events into my

00:18:07,300 --> 00:18:11,590
sequel and we were doing all

00:18:08,560 --> 00:18:15,420
aggregations life there which of course

00:18:11,590 --> 00:18:15,420
also became slow at some point in time

00:18:16,650 --> 00:18:23,310
yeah and this whole handling these raw

00:18:20,140 --> 00:18:26,110
lock messages with Python wasn't really

00:18:23,310 --> 00:18:30,490
feasible or at least we wasted a lot of

00:18:26,110 --> 00:18:33,400
time there but still with this very

00:18:30,490 --> 00:18:36,490
simple setup we succeeded in building a

00:18:33,400 --> 00:18:40,840
proof-of-concept and convincing some

00:18:36,490 --> 00:18:42,760
investors to give us money so we were

00:18:40,840 --> 00:18:46,300
able to hire more people have a bigger

00:18:42,760 --> 00:18:49,990
team and of course what do you do if you

00:18:46,300 --> 00:18:54,160
are stuck with your conventional sequel

00:18:49,990 --> 00:18:55,360
staff you go to go Hadoop so let's have

00:18:54,160 --> 00:18:59,280
a look what we did to these particular

00:18:55,360 --> 00:19:03,310
problems that we identified okay for the

00:18:59,280 --> 00:19:06,280
storage part straight forward we moved

00:19:03,310 --> 00:19:08,140
to HDFS of course but we still did like

00:19:06,280 --> 00:19:09,250
this first of all writing the data to

00:19:08,140 --> 00:19:15,070
load the disk and then eventually

00:19:09,250 --> 00:19:18,940
copying it to HDFS for the problem with

00:19:15,070 --> 00:19:21,730
the live aggregations we switched to an

00:19:18,940 --> 00:19:22,600
ETL process on top of Hadoop we already

00:19:21,730 --> 00:19:25,750
back then

00:19:22,600 --> 00:19:30,040
basically used hive on top of the gzip

00:19:25,750 --> 00:19:32,200
json files that were in HDFS to generate

00:19:30,040 --> 00:19:36,190
aggregates which we then simply exported

00:19:32,200 --> 00:19:42,580
to phosphorus and for also some more

00:19:36,190 --> 00:19:43,299
complex reporting queries we wrote some

00:19:42,580 --> 00:19:47,940
MapReduce

00:19:43,299 --> 00:19:51,639
jobs ourselves in Java and for these

00:19:47,940 --> 00:19:54,369
analytics workloads

00:19:51,639 --> 00:19:57,129
well now we had hive on top of the Jesus

00:19:54,369 --> 00:20:02,019
JSON files and it was finally possible

00:19:57,129 --> 00:20:06,220
to write queries on these on this data

00:20:02,019 --> 00:20:10,690
on the raw data this unfortunately also

00:20:06,220 --> 00:20:12,549
had problems with that because we were

00:20:10,690 --> 00:20:14,919
writing these files in the on the

00:20:12,549 --> 00:20:17,200
front-end service so we often had lots

00:20:14,919 --> 00:20:20,409
of small files and you know lots of

00:20:17,200 --> 00:20:24,359
small files in Hadoop is a problem which

00:20:20,409 --> 00:20:29,769
is why we sort of build a heck of

00:20:24,359 --> 00:20:31,570
materialized views in parkade format so

00:20:29,769 --> 00:20:34,330
that we were able to query the raw data

00:20:31,570 --> 00:20:36,330
faster so it was basically an

00:20:34,330 --> 00:20:39,399
intermediate HEC at this stage already

00:20:36,330 --> 00:20:46,350
but eventually the system grew and grew

00:20:39,399 --> 00:20:50,200
and also this setup had its problems

00:20:46,350 --> 00:20:52,950
actually many of them first of all we

00:20:50,200 --> 00:20:56,230
were using Jason from the beginning and

00:20:52,950 --> 00:20:58,139
I will come back to that later but at

00:20:56,230 --> 00:21:01,869
some point we realized having this

00:20:58,139 --> 00:21:03,730
flexibility in Jason can be a huge

00:21:01,869 --> 00:21:07,149
benefit but it can also be a huge pain

00:21:03,730 --> 00:21:12,820
because well you can't really rely on on

00:21:07,149 --> 00:21:15,850
anything I mentioned already we had this

00:21:12,820 --> 00:21:17,590
small file problem because basically the

00:21:15,850 --> 00:21:20,580
files that ended up in HDFS they were

00:21:17,590 --> 00:21:22,840
generated on front end servers which

00:21:20,580 --> 00:21:26,799
well we had to find this trade-off

00:21:22,840 --> 00:21:28,119
between timely ingestion like not

00:21:26,799 --> 00:21:29,590
waiting too long on the front end

00:21:28,119 --> 00:21:35,139
servers until the files are big enough

00:21:29,590 --> 00:21:41,820
and yeah and managing somehow the file

00:21:35,139 --> 00:21:41,820
size of the ground truth raw log files

00:21:43,080 --> 00:21:48,129
but still we had not how many ingestion

00:21:45,669 --> 00:21:50,619
then well this is just a detail we were

00:21:48,129 --> 00:21:56,710
really unhappy with the Uzi like a Uzi

00:21:50,619 --> 00:21:57,159
ADSL caused us a lot of pain I mentioned

00:21:56,710 --> 00:21:58,749
it before

00:21:57,159 --> 00:22:01,359
this whole ETL process was extremely

00:21:58,749 --> 00:22:03,220
slow because it was running on the raw

00:22:01,359 --> 00:22:04,929
data but we really wanted it to run on

00:22:03,220 --> 00:22:07,269
the raw data because we wanted to make

00:22:04,929 --> 00:22:09,340
sure that our reports are always based

00:22:07,269 --> 00:22:12,809
on the ground truth which are our logs

00:22:09,340 --> 00:22:12,809
that we will never touch again

00:22:13,470 --> 00:22:22,509
also these MapReduce jobs were really

00:22:16,119 --> 00:22:27,609
not not a lot of fun to maintain okay I

00:22:22,509 --> 00:22:31,299
mentioned that already okay so what's

00:22:27,609 --> 00:22:33,399
next oh there was one more of course we

00:22:31,299 --> 00:22:38,830
had basically all our data duplicated

00:22:33,399 --> 00:22:46,090
which is also not nice how to fix that

00:22:38,830 --> 00:22:55,359
stuff for the no reliable data structure

00:22:46,090 --> 00:22:57,429
part we moved to protobuf well you have

00:22:55,359 --> 00:22:59,590
a schema there and this is something you

00:22:57,429 --> 00:23:02,169
can rely on this becomes important when

00:22:59,590 --> 00:23:04,899
you have many teams working with the

00:23:02,169 --> 00:23:09,879
data it becomes more important that

00:23:04,899 --> 00:23:11,619
people can rely on it for the small file

00:23:09,879 --> 00:23:14,409
problem and the node family ingestion

00:23:11,619 --> 00:23:19,529
problem and we just heard about it again

00:23:14,409 --> 00:23:23,169
in the talk before we moved to Kafka

00:23:19,529 --> 00:23:25,749
because that basically decouples the the

00:23:23,169 --> 00:23:28,179
real-time component from from the from

00:23:25,749 --> 00:23:31,330
this whole data pipeline because the

00:23:28,179 --> 00:23:35,019
consumer of the events that come from

00:23:31,330 --> 00:23:38,409
Kafka can finally decide how much it

00:23:35,019 --> 00:23:40,590
will consume until it's bills to HDFS so

00:23:38,409 --> 00:23:46,029
we actually have much better

00:23:40,590 --> 00:23:48,249
manageability of file sizes in HDFS

00:23:46,029 --> 00:23:51,039
which is actually a still I think one of

00:23:48,249 --> 00:23:57,070
the most crucial parts unfortunately for

00:23:51,039 --> 00:23:59,499
optimizing query speed well you see just

00:23:57,070 --> 00:24:03,970
a tiny detail we switch to Luigi which

00:23:59,499 --> 00:24:07,130
feels much more productive and for this

00:24:03,970 --> 00:24:09,560
whole MapReduce stuff we started to

00:24:07,130 --> 00:24:12,380
change that to spark because it just

00:24:09,560 --> 00:24:18,380
also feels much more productive to work

00:24:12,380 --> 00:24:20,150
with well now that we had this have this

00:24:18,380 --> 00:24:23,540
whole Kafka pipeline in place we were

00:24:20,150 --> 00:24:28,130
actually able to write our data directly

00:24:23,540 --> 00:24:30,050
into Parque format which basically also

00:24:28,130 --> 00:24:33,260
solves the last few problems because

00:24:30,050 --> 00:24:36,380
we're no longer forced to do our whole

00:24:33,260 --> 00:24:40,370
ETL stuff on top of gzip Jason but

00:24:36,380 --> 00:24:43,010
instead we can basically do it quite

00:24:40,370 --> 00:24:45,320
fast directly on pork eight tables and

00:24:43,010 --> 00:24:50,060
this also made this whole analytics

00:24:45,320 --> 00:24:52,310
workloads a lot faster because well we

00:24:50,060 --> 00:24:55,480
can not only use all the tools that are

00:24:52,310 --> 00:25:01,510
available now on in the Hadoop ecosystem

00:24:55,480 --> 00:25:07,550
directly on top of Parque tables good so

00:25:01,510 --> 00:25:10,520
let's summarize that the three stages we

00:25:07,550 --> 00:25:14,990
basically managed to build a proof of

00:25:10,520 --> 00:25:20,030
concept without a dupe but we of course

00:25:14,990 --> 00:25:23,750
weren't able to to scale that but it was

00:25:20,030 --> 00:25:25,070
fine because we still our main focus in

00:25:23,750 --> 00:25:26,960
the beginning was still our machine

00:25:25,070 --> 00:25:30,380
learning algorithms was not so much

00:25:26,960 --> 00:25:34,820
about scale it was just about proving in

00:25:30,380 --> 00:25:37,670
a smaller scale that that our technology

00:25:34,820 --> 00:25:41,120
actually works but then eventually of

00:25:37,670 --> 00:25:46,940
course we had to move to something

00:25:41,120 --> 00:25:48,410
bigger had to introduce Hadoop and the

00:25:46,940 --> 00:25:50,960
biggest mistake that we did back then

00:25:48,410 --> 00:25:53,390
was basically that we kind of naively

00:25:50,960 --> 00:25:56,450
just thought well we will we will manage

00:25:53,390 --> 00:25:59,270
it and we didn't really take a lot of

00:25:56,450 --> 00:26:01,640
consultant consultancy or hire any

00:25:59,270 --> 00:26:05,080
experts so we also did a lot of mistakes

00:26:01,640 --> 00:26:09,530
there and this basically forced us to do

00:26:05,080 --> 00:26:11,240
another huge step of changes for finally

00:26:09,530 --> 00:26:15,920
being able to to get to a scalable

00:26:11,240 --> 00:26:18,740
infrastructure maybe some numbers just

00:26:15,920 --> 00:26:20,220
that you have an idea and the first day

00:26:18,740 --> 00:26:23,480
something like five

00:26:20,220 --> 00:26:27,500
hundred requests per second on average

00:26:23,480 --> 00:26:30,659
just ten gigabytes per day additional

00:26:27,500 --> 00:26:33,150
data set growth which we handled with

00:26:30,659 --> 00:26:35,190
with one server for a while then that

00:26:33,150 --> 00:26:45,330
basically all multiplied by a factor of

00:26:35,190 --> 00:26:48,350
10 in each stage so what are the main

00:26:45,330 --> 00:26:53,640
lessons that we learned in this process

00:26:48,350 --> 00:26:55,890
I don't know maybe some of you know this

00:26:53,640 --> 00:26:58,230
comic so it's basically an analyst

00:26:55,890 --> 00:26:58,740
asking an engineer how to query a

00:26:58,230 --> 00:27:01,200
database

00:26:58,740 --> 00:27:02,760
the new database and the engineer

00:27:01,200 --> 00:27:05,700
replies well it's not a database it's a

00:27:02,760 --> 00:27:06,860
key value store okay so the database how

00:27:05,700 --> 00:27:09,690
directquery it

00:27:06,860 --> 00:27:11,610
well you don't query it because you

00:27:09,690 --> 00:27:16,409
write a distributed MapReduce job in

00:27:11,610 --> 00:27:19,110
Erlang and the analyst asks did you just

00:27:16,409 --> 00:27:21,059
tell me to go screw myself and the

00:27:19,110 --> 00:27:23,669
engineer actually realizes well probably

00:27:21,059 --> 00:27:27,630
he did exactly that what we learned is

00:27:23,669 --> 00:27:31,890
that it will help you incredibly when

00:27:27,630 --> 00:27:33,750
you're really concerned with with the

00:27:31,890 --> 00:27:36,090
data business with a machine learning

00:27:33,750 --> 00:27:38,580
business if you make all your raw data

00:27:36,090 --> 00:27:42,360
accessible through SQL and if you make

00:27:38,580 --> 00:27:46,200
sure that this that it is accessible in

00:27:42,360 --> 00:27:48,630
a fast manner because this will allow

00:27:46,200 --> 00:27:54,450
your data scientists to query that rate

00:27:48,630 --> 00:27:56,340
data and to to look into it and they

00:27:54,450 --> 00:27:58,909
will find out great things and if they

00:27:56,340 --> 00:28:02,130
can't do it and they will also not do it

00:27:58,909 --> 00:28:04,169
and this is also why I think it's so

00:28:02,130 --> 00:28:06,570
great that we see so much progress on

00:28:04,169 --> 00:28:07,919
this whole SQL on Hadoop thing and I

00:28:06,570 --> 00:28:10,020
also think that it's basically the

00:28:07,919 --> 00:28:12,150
explanation on why we see so much

00:28:10,020 --> 00:28:17,340
progress on this SQL Hadoop same thing

00:28:12,150 --> 00:28:20,940
because it's just so damn useful and I'm

00:28:17,340 --> 00:28:22,650
also actually quite excited about what

00:28:20,940 --> 00:28:25,110
for example Apache drill might might

00:28:22,650 --> 00:28:28,080
bring here because that just goes into

00:28:25,110 --> 00:28:30,740
the same direction what we also learned

00:28:28,080 --> 00:28:32,350
is that you should probably leave other

00:28:30,740 --> 00:28:34,810
ESL's like

00:28:32,350 --> 00:28:40,480
more complex ones MapReduce peak

00:28:34,810 --> 00:28:43,980
etcetera to the engineers because SQL is

00:28:40,480 --> 00:28:46,900
actually almost always good enough

00:28:43,980 --> 00:28:49,150
there's one interesting exception which

00:28:46,900 --> 00:28:50,740
we are realizing at the moment which is

00:28:49,150 --> 00:28:51,060
spark I mean everybody's talking about

00:28:50,740 --> 00:28:55,540
it

00:28:51,060 --> 00:28:56,890
anyways at least if they or if we will

00:28:55,540 --> 00:29:00,760
see an improvement in the Python

00:28:56,890 --> 00:29:02,920
interface that might be an exception to

00:29:00,760 --> 00:29:06,430
this rule of thumb because it integrates

00:29:02,920 --> 00:29:08,830
very nicely into all these machine

00:29:06,430 --> 00:29:18,070
learning tools that many data scientists

00:29:08,830 --> 00:29:21,600
really love to use another lesson

00:29:18,070 --> 00:29:27,000
learned about introducing Hadoop well

00:29:21,600 --> 00:29:30,510
maybe this is a bit outdated now from my

00:29:27,000 --> 00:29:34,030
memories it was quite a rocky road to

00:29:30,510 --> 00:29:38,800
introduce her to three years ago I think

00:29:34,030 --> 00:29:43,720
it has become a lot simpler so maybe the

00:29:38,800 --> 00:29:45,400
barrier is much lower now but you should

00:29:43,720 --> 00:29:47,800
really ask yourself at what point in

00:29:45,400 --> 00:29:49,870
time do you really need a dupe because

00:29:47,800 --> 00:29:52,750
usually at least when you're doing

00:29:49,870 --> 00:29:54,790
machine learning as a startup you will

00:29:52,750 --> 00:29:57,970
not have big data because you don't have

00:29:54,790 --> 00:30:00,160
the customers and the question is really

00:29:57,970 --> 00:30:01,600
do you really need to build your whole

00:30:00,160 --> 00:30:03,040
infrastructure based on Big Data

00:30:01,600 --> 00:30:05,080
technologies right from the beginning

00:30:03,040 --> 00:30:11,860
because before you have your first

00:30:05,080 --> 00:30:16,180
customer so my answer to this question

00:30:11,860 --> 00:30:18,220
is simply well when conventional SQL

00:30:16,180 --> 00:30:19,660
stuff becomes too slow than the probably

00:30:18,220 --> 00:30:25,420
the weapon of choice nowadays is

00:30:19,660 --> 00:30:26,680
something Hadoop based yeah as I

00:30:25,420 --> 00:30:29,860
mentioned many things can still be

00:30:26,680 --> 00:30:33,190
achieved without it but when you do

00:30:29,860 --> 00:30:34,890
introduces unless you happen to have

00:30:33,190 --> 00:30:37,960
some Hadoop experts in your team already

00:30:34,890 --> 00:30:40,720
make sure to find some because that will

00:30:37,960 --> 00:30:44,890
allow you to avoid lots of mistakes that

00:30:40,720 --> 00:30:46,269
stayed around for us with us for much

00:30:44,890 --> 00:30:48,309
longer than necessary

00:30:46,269 --> 00:30:51,460
for example the small-time problem that

00:30:48,309 --> 00:30:53,139
I mentioned before or Big Al's political

00:30:51,460 --> 00:30:57,039
files which also slow down the whole

00:30:53,139 --> 00:30:59,889
thing or MapReduce jobs that we wrote in

00:30:57,039 --> 00:31:05,309
Java where we have actually should have

00:30:59,889 --> 00:31:05,309
chosen a better higher level abstraction

00:31:06,779 --> 00:31:16,990
another lesson learned data format I

00:31:11,940 --> 00:31:19,210
think Jason is great because it's human

00:31:16,990 --> 00:31:20,950
readable it's very flexible

00:31:19,210 --> 00:31:23,470
you will need this flexibility in the

00:31:20,950 --> 00:31:28,179
beginning because you will change your

00:31:23,470 --> 00:31:31,029
product over and over again you can even

00:31:28,179 --> 00:31:32,769
I don't know listen to TCP streams and

00:31:31,029 --> 00:31:37,779
parse it directly out of it because it's

00:31:32,769 --> 00:31:40,419
human readable and not binary but at

00:31:37,779 --> 00:31:44,289
some point it might also become a

00:31:40,419 --> 00:31:46,690
problem and format with a schema like

00:31:44,289 --> 00:31:48,789
proto before Avro or whatever might

00:31:46,690 --> 00:31:51,460
become beneficial especially when you

00:31:48,789 --> 00:31:53,799
have a growing team you need to maintain

00:31:51,460 --> 00:31:59,470
compatibility between the teams and

00:31:53,799 --> 00:32:01,299
components but if you do introduce

00:31:59,470 --> 00:32:02,889
something like that you should really

00:32:01,299 --> 00:32:04,539
make sure and we saw that in the talk

00:32:02,889 --> 00:32:07,779
before as well you should really make

00:32:04,539 --> 00:32:09,759
sure that you provide tooling for your

00:32:07,779 --> 00:32:12,850
users to look into this binary format

00:32:09,759 --> 00:32:14,889
because otherwise you are sitting in

00:32:12,850 --> 00:32:17,440
front of your data and in the case you

00:32:14,889 --> 00:32:22,659
have to debug it and you can just can't

00:32:17,440 --> 00:32:26,350
look into it I linked a nice article by

00:32:22,659 --> 00:32:33,639
Twitter here which also discusses this

00:32:26,350 --> 00:32:36,759
topic alright so before I conclude let

00:32:33,639 --> 00:32:38,740
me talk a little bit about team which

00:32:36,759 --> 00:32:40,450
also goes hand-in-hand with the

00:32:38,740 --> 00:32:43,179
technology decisions that we have seen

00:32:40,450 --> 00:32:45,159
over time when you're bootstrapping

00:32:43,179 --> 00:32:51,460
you're usually just a handful of

00:32:45,159 --> 00:32:54,639
founders and with without money and so

00:32:51,460 --> 00:32:56,530
you I don't know if you if you saw the

00:32:54,639 --> 00:33:00,740
keynote this morning

00:32:56,530 --> 00:33:02,809
then it's Eric Eric mentioned that you

00:33:00,740 --> 00:33:04,820
will always have very short run runways

00:33:02,809 --> 00:33:06,200
you will always have deadlines so you

00:33:04,820 --> 00:33:09,410
better use the tools your most

00:33:06,200 --> 00:33:11,540
productive with once we have some

00:33:09,410 --> 00:33:14,750
funding and you can afford hiring people

00:33:11,540 --> 00:33:16,700
I would say pays off to have a small

00:33:14,750 --> 00:33:17,990
team of generalists because as I

00:33:16,700 --> 00:33:20,450
mentioned before your product will

00:33:17,990 --> 00:33:23,230
change over and over again so you rather

00:33:20,450 --> 00:33:26,720
have some people that are flexible and

00:33:23,230 --> 00:33:29,870
can change technologies and and work

00:33:26,720 --> 00:33:31,910
flexibly on the on the product so you

00:33:29,870 --> 00:33:35,630
also better use widespread technologies

00:33:31,910 --> 00:33:37,690
where you can Google all problems but

00:33:35,630 --> 00:33:41,809
then of course at some point eventually

00:33:37,690 --> 00:33:46,429
for the sake of scalability you will be

00:33:41,809 --> 00:33:47,900
able to build up specialized teams or

00:33:46,429 --> 00:33:49,970
you will eventually write your own

00:33:47,900 --> 00:33:52,360
database like many of the big companies

00:33:49,970 --> 00:34:03,590
at all Twitter a build storm and

00:33:52,360 --> 00:34:08,000
Facebook build high etc conclusion for

00:34:03,590 --> 00:34:10,700
building a machine-learning company yeah

00:34:08,000 --> 00:34:13,869
first of all most important thing find

00:34:10,700 --> 00:34:18,020
the right product find something that

00:34:13,869 --> 00:34:19,580
has the potential to scale otherwise you

00:34:18,020 --> 00:34:24,820
will not become a big data company at

00:34:19,580 --> 00:34:26,780
least then don't underestimate the whole

00:34:24,820 --> 00:34:30,560
infrastructure development that you will

00:34:26,780 --> 00:34:33,109
have to do around your core machine

00:34:30,560 --> 00:34:38,389
learning ideas this will take up a huge

00:34:33,109 --> 00:34:41,510
part of your effort something that we

00:34:38,389 --> 00:34:44,990
realized too late and which would have

00:34:41,510 --> 00:34:47,060
sped up a lot provide fast sequel access

00:34:44,990 --> 00:34:51,109
to all your data at all time focus on

00:34:47,060 --> 00:34:53,090
this right from the beginning when

00:34:51,109 --> 00:34:55,450
introducing Hadoop either you already

00:34:53,090 --> 00:35:01,369
have an export expert in your team or

00:34:55,450 --> 00:35:07,100
find one and also important takeaway I

00:35:01,369 --> 00:35:09,850
would say is start with a flexible yet

00:35:07,100 --> 00:35:14,150
you and human readable data format

00:35:09,850 --> 00:35:16,040
and that's already it I thank you for

00:35:14,150 --> 00:35:20,300
your attention and I think oh well you

00:35:16,040 --> 00:35:22,730
can reach me on Twitter or email if you

00:35:20,300 --> 00:35:29,890
like otherwise I think we have a few

00:35:22,730 --> 00:35:29,890

YouTube URL: https://www.youtube.com/watch?v=a5zLmRMfS9Y


