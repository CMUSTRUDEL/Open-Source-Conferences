Title: Berlin Buzzwords 2015: Chris Ward - Understanding Databases for Distributed Docker Applications
Publication date: 2015-06-05
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	In this talk we'll focus on the use of Crate alongside Weave in Docker containers, the technical challenges, best practices learned, and getting a simple web application running alongside it. You'll learn about the reasons why Crate.IO is building "yet another NoSQL database" and why it's unique and important when running containerized applications. 

We'll show why the shared-nothing architecture is so important when deploying large clusters and talk about the ways we've leveraged Lucene, Elasticsearch, and built an optimized distributed SQL planner. You will learn how to deploy a Crate cluster within minutes in the cloud using Docker, some of the challenges you'll encounter, and how to overcome them. Crate focuses on super simple integrations with any cloud provider, striving to be as turnkey as possible with minimal up-front configuration required to establish a cluster. Once established, we'll show how to scale the cluster horizontally by simply adding more nodes. 

The session will also give you examples when you should use Crate compared to other similar technologies such as MongoDB, Hadoop, Cassandra or FoundationDB. We'll talk about Crate's strengths and what types of applications are well-suited for this type of data store, as well what is not. Finally we'll outline how to architect an application that is easy to scale using Crate, Docker, Weave, and a simple web application.

Read more:
https://2015.berlinbuzzwords.de/session/understanding-databases-distributed-docker-applications

About Chris Ward:
https://2015.berlinbuzzwords.de/users/chris-ward

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,839 --> 00:00:14,090
hello so I'm not yo doc as moved

00:00:14,450 --> 00:00:22,160
I do apologize but so we're going to

00:00:17,869 --> 00:00:26,619
look at an open source a piece of

00:00:22,160 --> 00:00:29,599
software that we've created at Crate and

00:00:26,619 --> 00:00:31,160
it's a distributed database and we're

00:00:29,599 --> 00:00:33,620
going to look at how to handle

00:00:31,160 --> 00:00:36,079
distributed databases with docker that

00:00:33,620 --> 00:00:37,850
said other containers are available

00:00:36,079 --> 00:00:39,980
other distributed databases available

00:00:37,850 --> 00:00:43,640
but there the specific ones we're going

00:00:39,980 --> 00:00:47,600
to focus on and how we got to what we

00:00:43,640 --> 00:00:50,210
created and why and then how we have the

00:00:47,600 --> 00:00:52,850
recommended way we've solved that well

00:00:50,210 --> 00:00:56,510
solved is a big word but helped with the

00:00:52,850 --> 00:01:00,260
distributed database solutions within a

00:00:56,510 --> 00:01:03,920
docker or container context so a little

00:01:00,260 --> 00:01:05,630
bit about me I'm originally from London

00:01:03,920 --> 00:01:07,340
I lived in Australia for a while and now

00:01:05,630 --> 00:01:10,490
in Germany I'm actually a developer

00:01:07,340 --> 00:01:13,579
advocate for crate somewhere between

00:01:10,490 --> 00:01:17,360
technical and non-technical so I'm going

00:01:13,579 --> 00:01:19,939
to mainly talk about how to use what had

00:01:17,360 --> 00:01:21,469
to use the database how to integrate

00:01:19,939 --> 00:01:23,600
with it and how it functions and

00:01:21,469 --> 00:01:25,939
actually have one of our core team here

00:01:23,600 --> 00:01:27,079
who will help answer any of the more

00:01:25,939 --> 00:01:29,299
technical questions you might have about

00:01:27,079 --> 00:01:31,249
how crate is actually made because that

00:01:29,299 --> 00:01:35,630
may interest you it's not questions I

00:01:31,249 --> 00:01:38,299
can answer and I've always loved open

00:01:35,630 --> 00:01:40,429
source I've worked in open source

00:01:38,299 --> 00:01:43,249
software for about 15 years mainly in a

00:01:40,429 --> 00:01:45,649
more traditional field and i also have

00:01:43,249 --> 00:01:48,289
been working on other applications of

00:01:45,649 --> 00:01:51,799
open source like a board game and works

00:01:48,289 --> 00:01:54,619
of fiction so a bit of a generalist I

00:01:51,799 --> 00:01:58,099
guess so let's start with the sort of

00:01:54,619 --> 00:02:00,679
big statements and why we took ourselves

00:01:58,099 --> 00:02:03,459
down the path of building a distributed

00:02:00,679 --> 00:02:06,829
database another distributed database

00:02:03,459 --> 00:02:10,819
the founders of crate come from a long

00:02:06,829 --> 00:02:16,790
background of scaling large applications

00:02:10,819 --> 00:02:19,580
and basically for applications of any

00:02:16,790 --> 00:02:22,280
type of size and scale now having one

00:02:19,580 --> 00:02:25,849
database is not enough anymore I think

00:02:22,280 --> 00:02:28,250
we all know that here we can't scale it

00:02:25,849 --> 00:02:29,960
quick enough it's not resilient enough

00:02:28,250 --> 00:02:32,080
not dynamic enough to cope with the

00:02:29,960 --> 00:02:35,360
sorts of applications we're now creating

00:02:32,080 --> 00:02:37,340
and of course the solution to this is

00:02:35,360 --> 00:02:41,030
distributing them multiple instances of

00:02:37,340 --> 00:02:43,030
a database and then multiple ways of

00:02:41,030 --> 00:02:45,410
keeping those multiple instances

00:02:43,030 --> 00:02:48,740
synchronized and we'll look at some of

00:02:45,410 --> 00:02:52,990
those in a minute along the same lines

00:02:48,740 --> 00:02:55,280
in respect to this presentation

00:02:52,990 --> 00:02:58,040
traditionally and I use the word

00:02:55,280 --> 00:03:00,920
traditionally loosely because tradition

00:02:58,040 --> 00:03:04,580
in computer programming is not very long

00:03:00,920 --> 00:03:06,470
ago really data centers would be the way

00:03:04,580 --> 00:03:07,880
we would scale applications we needed

00:03:06,470 --> 00:03:10,820
more capacity we build another data

00:03:07,880 --> 00:03:12,770
center that's not an easy thing to do it

00:03:10,820 --> 00:03:17,480
requires a lot of money a lot of time a

00:03:12,770 --> 00:03:19,400
lot of very specialized skills so we

00:03:17,480 --> 00:03:22,070
then got to the step of virtualization

00:03:19,400 --> 00:03:25,370
and cloud computing which is a bit of a

00:03:22,070 --> 00:03:29,770
buzz word if you forgive the pun but we

00:03:25,370 --> 00:03:31,970
understand what that means computers and

00:03:29,770 --> 00:03:33,799
instances of computers that we can

00:03:31,970 --> 00:03:35,330
access very easily and very quickly when

00:03:33,799 --> 00:03:38,330
we need them and add capacity when we

00:03:35,330 --> 00:03:40,700
need them then more recently we've had

00:03:38,330 --> 00:03:42,380
things like containers again not a new

00:03:40,700 --> 00:03:44,780
concept but the concept has been

00:03:42,380 --> 00:03:48,709
understood and more widely used in the

00:03:44,780 --> 00:03:50,000
past 18 months even and finally we're

00:03:48,709 --> 00:03:52,459
ending up with something like compute

00:03:50,000 --> 00:03:55,519
service is something like lambda AWS

00:03:52,459 --> 00:03:58,220
lambda where we've even reduced the

00:03:55,519 --> 00:04:01,370
overhead of services we require for

00:03:58,220 --> 00:04:03,170
application even more to small discreet

00:04:01,370 --> 00:04:04,850
little services that do one particular

00:04:03,170 --> 00:04:08,090
thing for a very quick amount of time

00:04:04,850 --> 00:04:13,519
come and go as quickly as we need them

00:04:08,090 --> 00:04:16,130
and this slide is thanks to this concept

00:04:13,519 --> 00:04:18,169
of where we're going is thanks to aging

00:04:16,130 --> 00:04:21,440
Cockroft whose ex Netflix and it's a

00:04:18,169 --> 00:04:23,840
company that understands complex scaling

00:04:21,440 --> 00:04:30,790
I I guess streaming video is pretty hard

00:04:23,840 --> 00:04:32,960
and traditionally when we've needed to

00:04:30,790 --> 00:04:35,600
change our requirements in an

00:04:32,960 --> 00:04:37,880
application or increase it in size we've

00:04:35,600 --> 00:04:39,770
sort of followed this path this is a

00:04:37,880 --> 00:04:40,920
path that many developers have followed

00:04:39,770 --> 00:04:45,150
in the past and

00:04:40,920 --> 00:04:47,130
would guess that many are here we start

00:04:45,150 --> 00:04:48,870
with something traditional again there's

00:04:47,130 --> 00:04:53,250
that word again a relational database

00:04:48,870 --> 00:04:55,710
something like my sequel we need more

00:04:53,250 --> 00:04:58,770
capacity on that my sequel database we

00:04:55,710 --> 00:05:00,270
add more instances which is great but

00:04:58,770 --> 00:05:02,850
then we have to worry about master and

00:05:00,270 --> 00:05:05,880
node setups sharding keeping the

00:05:02,850 --> 00:05:08,370
information in synchronization and how

00:05:05,880 --> 00:05:10,530
we're going to do that we realized then

00:05:08,370 --> 00:05:12,900
we need some document storage so we add

00:05:10,530 --> 00:05:14,820
a document storage solution and then we

00:05:12,900 --> 00:05:17,430
need to keep those two pieces working

00:05:14,820 --> 00:05:20,610
together and again scaling and

00:05:17,430 --> 00:05:23,310
synchronizing then we need search to

00:05:20,610 --> 00:05:26,010
search across those two databases and

00:05:23,310 --> 00:05:28,170
again when we need to add capacity we

00:05:26,010 --> 00:05:30,300
now have three pieces all interlocking

00:05:28,170 --> 00:05:32,280
with each other and figuring out how to

00:05:30,300 --> 00:05:35,730
scale and synchronize and then we might

00:05:32,280 --> 00:05:37,380
add more and more and more and all of

00:05:35,730 --> 00:05:40,260
this is of course possible to

00:05:37,380 --> 00:05:41,910
synchronize and scale but it becomes

00:05:40,260 --> 00:05:43,140
more and more complicated and we spend

00:05:41,910 --> 00:05:46,890
less and less time on actually building

00:05:43,140 --> 00:05:48,300
a grid application and it hasn't really

00:05:46,890 --> 00:05:49,770
solved the problem that we were trying

00:05:48,300 --> 00:05:52,550
to solve in the first place we're just

00:05:49,770 --> 00:05:58,760
creating more problems for ourselves

00:05:52,550 --> 00:06:02,550
which sometimes we like to do but and

00:05:58,760 --> 00:06:07,020
with containers and with docker in this

00:06:02,550 --> 00:06:09,900
specific example keeping data persistent

00:06:07,020 --> 00:06:12,270
in containers here's a concept where we

00:06:09,900 --> 00:06:14,160
have an instance of a computer service

00:06:12,270 --> 00:06:16,290
or an instance of an application or an

00:06:14,160 --> 00:06:19,710
instance of a database that we may add

00:06:16,290 --> 00:06:22,410
to we may remove we may remove all of

00:06:19,710 --> 00:06:24,990
them and double the capacity how when

00:06:22,410 --> 00:06:29,580
these things are transient do we keep

00:06:24,990 --> 00:06:32,460
data persistent how does a new instance

00:06:29,580 --> 00:06:34,440
know what the data even was and where to

00:06:32,460 --> 00:06:38,610
find it from a previous instance and

00:06:34,440 --> 00:06:41,340
again there are solutions but they're

00:06:38,610 --> 00:06:43,400
not simple they take time and depending

00:06:41,340 --> 00:06:46,230
on the complexity of your application

00:06:43,400 --> 00:06:48,930
they could take a lot of time this is

00:06:46,230 --> 00:06:53,070
kind of where we are and why we decided

00:06:48,930 --> 00:06:55,070
to try something new or a new

00:06:53,070 --> 00:06:58,440
alternative anyway

00:06:55,070 --> 00:06:59,640
and crate is an open source project and

00:06:58,440 --> 00:07:01,890
I'll show you some of the components

00:06:59,640 --> 00:07:04,080
stack in a minute there's mountains

00:07:01,890 --> 00:07:08,910
everywhere because it started in Austria

00:07:04,080 --> 00:07:11,600
actually we now sort of half German half

00:07:08,910 --> 00:07:14,460
Austrian with a couple of other

00:07:11,600 --> 00:07:16,650
ethnicities thrown in of course it

00:07:14,460 --> 00:07:18,840
started out originally as working on

00:07:16,650 --> 00:07:22,350
elastic search plugins about four years

00:07:18,840 --> 00:07:24,030
ago when the founders realized actually

00:07:22,350 --> 00:07:26,400
what we're working on could be more than

00:07:24,030 --> 00:07:30,360
that and decided to break it out into a

00:07:26,400 --> 00:07:31,650
separate open source project and still

00:07:30,360 --> 00:07:33,030
contributing back to elasticsearch one

00:07:31,650 --> 00:07:35,490
on the way and we'll have a look at some

00:07:33,030 --> 00:07:37,530
of those contributions in a minute so

00:07:35,490 --> 00:07:39,090
what is it and this slide is interesting

00:07:37,530 --> 00:07:41,040
because my colleague and I were in a

00:07:39,090 --> 00:07:45,000
previous talk where lots of the things

00:07:41,040 --> 00:07:48,090
we do it was a contradictory opinion on

00:07:45,000 --> 00:07:51,480
them so so but we've sort of taking some

00:07:48,090 --> 00:07:54,210
of them a bit further you get a lot of

00:07:51,480 --> 00:07:58,370
different features and functionalities

00:07:54,210 --> 00:08:01,620
or combined into one package basically

00:07:58,370 --> 00:08:05,100
so it's a no sequel style architecture

00:08:01,620 --> 00:08:06,390
but you can actually use standard sequel

00:08:05,100 --> 00:08:08,070
for your query language you don't have

00:08:06,390 --> 00:08:10,860
to learn any new query languages if

00:08:08,070 --> 00:08:16,380
you're migrating from something like my

00:08:10,860 --> 00:08:19,650
sequel it's distributed sequel so the

00:08:16,380 --> 00:08:22,890
various instances of the data can be

00:08:19,650 --> 00:08:25,410
queried with one normal style sequel

00:08:22,890 --> 00:08:27,240
query it doesn't matter too much where

00:08:25,410 --> 00:08:28,680
the data is you'll get the same results

00:08:27,240 --> 00:08:31,140
and we'll look at a demonstration of

00:08:28,680 --> 00:08:33,390
that soon we have semi structured

00:08:31,140 --> 00:08:36,810
records the structures and the schema

00:08:33,390 --> 00:08:39,270
can change relatively easily and a whole

00:08:36,810 --> 00:08:44,850
bunch of other features here and the I

00:08:39,270 --> 00:08:47,070
guess one of the key the key features

00:08:44,850 --> 00:08:48,570
that we have by default which can be

00:08:47,070 --> 00:08:51,180
changed through configuration is that

00:08:48,570 --> 00:08:54,360
all nodes are equal there is no master

00:08:51,180 --> 00:08:57,180
slave concept by default at extreme

00:08:54,360 --> 00:08:59,520
scale you may need to start changing

00:08:57,180 --> 00:09:02,460
that but that medium scale you can

00:08:59,520 --> 00:09:05,090
generally stick with this type of

00:09:02,460 --> 00:09:05,090
infrastructure

00:09:06,420 --> 00:09:13,230
I think the one thing that isn't noted

00:09:09,540 --> 00:09:16,920
on there is as well that document

00:09:13,230 --> 00:09:20,070
storage and normal database storage are

00:09:16,920 --> 00:09:21,750
also included in the same package so you

00:09:20,070 --> 00:09:23,850
don't have these multiple technologies

00:09:21,750 --> 00:09:27,980
interlocking with each other they're all

00:09:23,850 --> 00:09:31,699
in the same crate instances as it were

00:09:27,980 --> 00:09:33,899
so what's underneath everything as I say

00:09:31,699 --> 00:09:36,920
create itself is open source but it

00:09:33,899 --> 00:09:42,209
builds upon other open source components

00:09:36,920 --> 00:09:45,449
the ones in grey are the light grey or

00:09:42,209 --> 00:09:50,399
elasticsearch so we're using elastic

00:09:45,449 --> 00:09:53,370
search at the storage layer and and the

00:09:50,399 --> 00:09:56,100
dark gray are other components the blue

00:09:53,370 --> 00:09:59,130
are custom crate components that we have

00:09:56,100 --> 00:10:03,769
created ourselves but they're all open

00:09:59,130 --> 00:10:08,310
sourced so all of this is viewable and

00:10:03,769 --> 00:10:09,990
contributed ball so the crate component

00:10:08,310 --> 00:10:13,140
and the storage level is handling blob

00:10:09,990 --> 00:10:14,519
storage lucene is handling index storage

00:10:13,140 --> 00:10:17,160
and elastic search pretty much

00:10:14,519 --> 00:10:20,640
everything else at the network layer

00:10:17,160 --> 00:10:23,279
again custom crate components for blob

00:10:20,640 --> 00:10:26,339
streaming elastic search for sharding

00:10:23,279 --> 00:10:29,610
discovery and transport and nettie for

00:10:26,339 --> 00:10:32,480
the internode communication at the

00:10:29,610 --> 00:10:37,140
aggregation level so the distributed

00:10:32,480 --> 00:10:39,029
level it's all custom crepe at the query

00:10:37,140 --> 00:10:41,820
level we're using facebook presto to

00:10:39,029 --> 00:10:45,570
pass the sequel elastic search for the

00:10:41,820 --> 00:10:48,240
scatter gather and I guess one of the

00:10:45,570 --> 00:10:51,899
most useful crate components is the bulk

00:10:48,240 --> 00:10:54,000
import export for migration into and out

00:10:51,899 --> 00:10:56,579
of which hopefully you won't need but

00:10:54,000 --> 00:11:01,079
into hopefully you will need and at the

00:10:56,579 --> 00:11:03,420
top again all custom with the dashboard

00:11:01,079 --> 00:11:05,670
which will look at soon a python shell

00:11:03,420 --> 00:11:07,069
and in various client libraries which

00:11:05,670 --> 00:11:10,649
will also have a quick look at for

00:11:07,069 --> 00:11:12,600
programming languages ok so that's a

00:11:10,649 --> 00:11:17,790
quick overview let's now look into how

00:11:12,600 --> 00:11:19,140
it fits in with lovely docker and I

00:11:17,790 --> 00:11:20,310
think this is always something I have to

00:11:19,140 --> 00:11:24,120
be clear of

00:11:20,310 --> 00:11:25,800
we've decided to talk a reasonable

00:11:24,120 --> 00:11:28,920
amount about how create works with Daka

00:11:25,800 --> 00:11:30,600
but it works with many other containers

00:11:28,920 --> 00:11:32,040
and also without containers as well it's

00:11:30,600 --> 00:11:33,960
important to point out you don't need

00:11:32,040 --> 00:11:35,820
docker but dr. is also very good for

00:11:33,960 --> 00:11:38,970
demos because I don't need to have

00:11:35,820 --> 00:11:42,020
anything special set up on a computer

00:11:38,970 --> 00:11:45,029
you can just get it running immediately

00:11:42,020 --> 00:11:46,470
so anyone who's used docker will

00:11:45,029 --> 00:11:49,620
probably recognize this sequence of

00:11:46,470 --> 00:11:53,279
commands it's basically saying get the

00:11:49,620 --> 00:11:57,360
latest image then run it and the first

00:11:53,279 --> 00:11:59,310
two are running the crate creating a new

00:11:57,360 --> 00:12:02,490
crate container and letting docker

00:11:59,310 --> 00:12:04,770
handle the port allocation the final one

00:12:02,490 --> 00:12:07,500
is then letting us manually set the

00:12:04,770 --> 00:12:10,170
ports to the two ports that we like to

00:12:07,500 --> 00:12:12,420
use and I'm not going to just leave that

00:12:10,170 --> 00:12:19,200
up there I'm actually going to do this

00:12:12,420 --> 00:12:20,250
so we can see how it is always the

00:12:19,200 --> 00:12:29,010
challenge of finding your mouse pointer

00:12:20,250 --> 00:12:30,510
there it is ok I think I have done this

00:12:29,010 --> 00:12:32,250
demo enough that I can remember this

00:12:30,510 --> 00:12:34,890
without having to copy and paste it but

00:12:32,250 --> 00:12:37,020
we'll see i've already done the first

00:12:34,890 --> 00:12:44,480
step they want to rely on a conference

00:12:37,020 --> 00:12:47,400
Wi-Fi to pull the image down already and

00:12:44,480 --> 00:12:50,150
the dash D is so it runs in the

00:12:47,400 --> 00:12:50,150
background basically

00:12:56,499 --> 00:13:02,049
ok then we'll run it again we'll run it

00:12:59,649 --> 00:13:12,699
a third time and i'll explain what has

00:13:02,049 --> 00:13:14,879
what has happened here so we run the

00:13:12,699 --> 00:13:18,129
command the dog command to list the

00:13:14,879 --> 00:13:22,239
instances we now have and we have three

00:13:18,129 --> 00:13:27,729
instances of craic running and as you

00:13:22,239 --> 00:13:29,319
can see from here we've let these are

00:13:27,729 --> 00:13:30,989
the two we relate docker allocate the

00:13:29,319 --> 00:13:33,399
port and here is the one where we

00:13:30,989 --> 00:13:36,039
statically said we want to use these

00:13:33,399 --> 00:13:38,679
ones and because we're on the same host

00:13:36,039 --> 00:13:41,439
and they will all discover each other

00:13:38,679 --> 00:13:45,699
and we'll have a three instance cluster

00:13:41,439 --> 00:13:49,179
just refresh that and there it is and

00:13:45,699 --> 00:13:50,319
you can see the three up I think my

00:13:49,179 --> 00:13:52,989
laser pointer actually works

00:13:50,319 --> 00:13:56,549
unfortunately it's you can see the three

00:13:52,989 --> 00:13:58,629
up in the sort of middle to top right

00:13:56,549 --> 00:14:00,309
there's nothing in here at the moment

00:13:58,629 --> 00:14:03,159
i'll give you our sort of hello world

00:14:00,309 --> 00:14:04,779
demo will import some tweets and then

00:14:03,159 --> 00:14:09,429
i'll give you a very quick overview of

00:14:04,779 --> 00:14:14,829
this admin console i don't really need

00:14:09,429 --> 00:14:17,139
to do too much ok so this is the admin

00:14:14,829 --> 00:14:18,459
console i mean it's good for getting up

00:14:17,139 --> 00:14:20,889
and running but to be honest with you

00:14:18,459 --> 00:14:23,409
once you start creating a more complex

00:14:20,889 --> 00:14:24,489
application it becomes less useful i

00:14:23,409 --> 00:14:27,129
guess that apart from just in the

00:14:24,489 --> 00:14:30,879
overview it gives us an overview of the

00:14:27,129 --> 00:14:33,789
data we have this health indicator is an

00:14:30,879 --> 00:14:37,689
indicator of how the instances of the

00:14:33,789 --> 00:14:39,549
database of rebalancing when so this is

00:14:37,689 --> 00:14:41,949
currently telling us that all three

00:14:39,549 --> 00:14:44,589
nodes in the cluster have the same data

00:14:41,949 --> 00:14:46,419
and then a later example will start

00:14:44,589 --> 00:14:48,549
destroying some instances and you'll see

00:14:46,419 --> 00:14:50,679
it rebalancing but this is so such a

00:14:48,549 --> 00:14:55,239
small data set that it's not nothing's

00:14:50,679 --> 00:14:58,329
really happening we have a console which

00:14:55,239 --> 00:15:01,049
just lets us do some basic playing

00:14:58,329 --> 00:15:01,049
around with queries

00:15:07,610 --> 00:15:17,100
yep as you'd expect we can see the table

00:15:13,439 --> 00:15:21,149
structure we have here also blob tables

00:15:17,100 --> 00:15:25,050
if we had any and the usually the schema

00:15:21,149 --> 00:15:28,790
is let's go anyway and the cluster all

00:15:25,050 --> 00:15:31,529
reasonably straightforward not a

00:15:28,790 --> 00:15:33,290
particularly thorough example so will

00:15:31,529 --> 00:15:39,059
step up to something a little more

00:15:33,290 --> 00:15:42,089
complex I think I will jump straight

00:15:39,059 --> 00:15:49,680
into it and then i will talk through in

00:15:42,089 --> 00:15:51,600
a bit more detail so this is not the one

00:15:49,680 --> 00:15:54,480
I wanted to know it's this one and they

00:15:51,600 --> 00:15:58,470
all look the same as I'm time that's the

00:15:54,480 --> 00:16:01,379
one I wanted okay so this is a larger

00:15:58,470 --> 00:16:09,600
cluster on AWS we currently have 11

00:16:01,379 --> 00:16:12,329
nodes we have 666 660 a little bit more

00:16:09,600 --> 00:16:14,579
of an interesting data set we have the

00:16:12,329 --> 00:16:19,319
main table we're interested in use this

00:16:14,579 --> 00:16:23,240
steps to a foe step tracker application

00:16:19,319 --> 00:16:27,509
so it has steps taken by users each day

00:16:23,240 --> 00:16:31,589
we have 50 gig of data and you can also

00:16:27,509 --> 00:16:35,160
see why I can't seem to scroll down

00:16:31,589 --> 00:16:37,680
anything as things down here which I

00:16:35,160 --> 00:16:41,279
want to show you but I can't get to them

00:16:37,680 --> 00:16:43,290
for some reason normally down on that

00:16:41,279 --> 00:16:45,839
right hand side it also see partition

00:16:43,290 --> 00:16:50,160
data and the schema but entirely sure

00:16:45,839 --> 00:16:52,319
why it's not scrolling there okay let's

00:16:50,160 --> 00:16:56,069
run some queries on this and then we'll

00:16:52,319 --> 00:17:00,420
illustrate the distributed nature and

00:16:56,069 --> 00:17:03,779
what that means really so i'm going to

00:17:00,420 --> 00:17:10,819
log into the if you don't run a query

00:17:03,779 --> 00:17:13,819
first sorry so in this instance we are

00:17:10,819 --> 00:17:13,819
sorry

00:17:14,839 --> 00:17:20,250
in this instance we're looking at the

00:17:17,070 --> 00:17:21,600
number of steps for a user and adding

00:17:20,250 --> 00:17:24,000
the sum of all the steps for a

00:17:21,600 --> 00:17:30,810
particular user and we've got about 50

00:17:24,000 --> 00:17:32,550
to 60 gig of data that's reasonably

00:17:30,810 --> 00:17:34,770
quick we normally get that kind of

00:17:32,550 --> 00:17:38,310
number naught point naught 18 seconds

00:17:34,770 --> 00:17:42,000
great that's great let's try something a

00:17:38,310 --> 00:17:48,810
little bit more complicated this will

00:17:42,000 --> 00:17:52,740
take the the steps from that user again

00:17:48,810 --> 00:17:55,710
and then for a particular month and give

00:17:52,740 --> 00:17:58,050
the total for each day it's a little bit

00:17:55,710 --> 00:18:00,000
more complicated but still pretty quick

00:17:58,050 --> 00:18:02,040
let's remember those numbers naught

00:18:00,000 --> 00:18:04,200
point naught 12 seconds I think that's

00:18:02,040 --> 00:18:07,890
actually the quickest it's done it at so

00:18:04,200 --> 00:18:10,830
far which is good I suppose okay great

00:18:07,890 --> 00:18:19,560
let's go back to the overview and what

00:18:10,830 --> 00:18:27,330
we're going to do is firstly i'm going

00:18:19,560 --> 00:18:29,910
to recreate an instance of the cluster

00:18:27,330 --> 00:18:31,440
and bring it up to 12 and this is going

00:18:29,910 --> 00:18:32,640
to look complicated and there's a reason

00:18:31,440 --> 00:18:36,420
i'm sort of making it look complicated

00:18:32,640 --> 00:18:42,960
to illustrate a later point but will

00:18:36,420 --> 00:18:53,400
login to the AWS instance and i want to

00:18:42,960 --> 00:18:54,960
switch into here and at the moment there

00:18:53,400 --> 00:18:56,310
is you know there's nothing running on

00:18:54,960 --> 00:18:59,340
this one this this one is doing nothing

00:18:56,310 --> 00:19:07,680
at the moment so we're going to create a

00:18:59,340 --> 00:19:11,580
new instance okay this this comes back

00:19:07,680 --> 00:19:13,650
to what I alluded to earlier in that the

00:19:11,580 --> 00:19:16,440
service discovery going on between the

00:19:13,650 --> 00:19:20,280
nodes generally will only work if

00:19:16,440 --> 00:19:24,690
everything is on the same host and this

00:19:20,280 --> 00:19:27,280
is a slight restriction with

00:19:24,690 --> 00:19:28,960
with docker but it's also a slight

00:19:27,280 --> 00:19:31,210
restriction with a lot of cloud hosting

00:19:28,960 --> 00:19:33,040
and this is why we're starting with

00:19:31,210 --> 00:19:34,090
problems because we're going to talk

00:19:33,040 --> 00:19:37,000
through the solutions to these problems

00:19:34,090 --> 00:19:40,110
but and this is one solution but it's a

00:19:37,000 --> 00:19:44,020
fairly long-winded one so we have to get

00:19:40,110 --> 00:19:49,500
the publishable IP address from AWS we

00:19:44,020 --> 00:19:58,410
have to get all the hosts in the cluster

00:19:49,500 --> 00:20:01,720
then we can run this command which I

00:19:58,410 --> 00:20:03,370
will clear even though it's on a slide

00:20:01,720 --> 00:20:06,250
I'll just talk it through here so this

00:20:03,370 --> 00:20:08,260
is again running it with the static port

00:20:06,250 --> 00:20:10,240
it's there all on separate hosts

00:20:08,260 --> 00:20:13,060
separate instances so we don't have to

00:20:10,240 --> 00:20:15,520
worry about pork conflicts we're mapping

00:20:13,060 --> 00:20:17,950
it to an existing data directory that

00:20:15,520 --> 00:20:19,900
exists on disk already this is keeping

00:20:17,950 --> 00:20:22,210
that persistence the data is already

00:20:19,900 --> 00:20:23,500
there nothing's happening with it but

00:20:22,210 --> 00:20:27,880
it's already there available for us to

00:20:23,500 --> 00:20:30,460
reuse we're setting a heap size because

00:20:27,880 --> 00:20:33,690
we got to a large data set and here

00:20:30,460 --> 00:20:35,650
we've got some configuration options

00:20:33,690 --> 00:20:37,570
that were actually sending to create

00:20:35,650 --> 00:20:39,850
itself and some of these are

00:20:37,570 --> 00:20:44,070
recognizable from the underlying

00:20:39,850 --> 00:20:49,570
elasticsearch and some others that are

00:20:44,070 --> 00:20:56,340
create specific so that has has run

00:20:49,570 --> 00:21:01,480
already okay so we go back to our

00:20:56,340 --> 00:21:03,160
cluster we now have 12 and because I was

00:21:01,480 --> 00:21:04,900
so slow in explaining things we didn't

00:21:03,160 --> 00:21:06,460
see the rebalancing but when we destroy

00:21:04,900 --> 00:21:08,590
it it'll be a lot quicker and we could

00:21:06,460 --> 00:21:15,690
see it rebalancing but if we go and run

00:21:08,590 --> 00:21:19,320
some of the same queries will see that I

00:21:15,690 --> 00:21:23,500
took slightly longer that's interesting

00:21:19,320 --> 00:21:25,090
but we've got the same dataset even

00:21:23,500 --> 00:21:28,510
though we just introduced a new instance

00:21:25,090 --> 00:21:33,850
okay great let's let's have some fun

00:21:28,510 --> 00:21:35,460
let's actually destroy things I'll just

00:21:33,850 --> 00:21:37,950
find the ID

00:21:35,460 --> 00:21:40,649
and let's watch the screen in the

00:21:37,950 --> 00:21:44,370
background as fun as terminals are watch

00:21:40,649 --> 00:21:50,010
the thing in the background so killing

00:21:44,370 --> 00:21:52,740
this instance we go and we see the

00:21:50,010 --> 00:21:55,409
overview suddenly switch into warning

00:21:52,740 --> 00:21:57,990
and we don't have fully replicate freely

00:21:55,409 --> 00:21:59,520
replicated data at the moment it's

00:21:57,990 --> 00:22:01,260
jumping up and if i can move quick

00:21:59,520 --> 00:22:03,690
enough we can actually jump into the

00:22:01,260 --> 00:22:06,020
table level and see specifically what

00:22:03,690 --> 00:22:08,850
table is under replicated as well i

00:22:06,020 --> 00:22:10,020
would hazard a guess pretty soon

00:22:08,850 --> 00:22:14,039
definitely by the end of the talk

00:22:10,020 --> 00:22:15,720
everything will be rebalanced but i hope

00:22:14,039 --> 00:22:17,279
from that example you see the point of

00:22:15,720 --> 00:22:20,340
the simplicity here we've got a

00:22:17,279 --> 00:22:22,890
distributed set of nodes of a database

00:22:20,340 --> 00:22:25,409
that we can bring up and down at will or

00:22:22,890 --> 00:22:27,390
when something goes wrong and we don't

00:22:25,409 --> 00:22:29,130
have to worry too much about that

00:22:27,390 --> 00:22:31,320
synchronization and replication it's

00:22:29,130 --> 00:22:34,500
happening automatically for us oh there

00:22:31,320 --> 00:22:41,130
we go one hundred percent great ok so

00:22:34,500 --> 00:22:43,740
that was that's very useful at the at as

00:22:41,130 --> 00:22:47,850
an example but we wanted to talk about

00:22:43,740 --> 00:22:49,559
how how to do this better with Daka so

00:22:47,850 --> 00:22:51,659
I'm gonna take a slight diversion back a

00:22:49,559 --> 00:22:56,130
step and then we'll get to a better

00:22:51,659 --> 00:22:59,309
solution than doing this sort of this is

00:22:56,130 --> 00:23:03,480
great it works setting these host names

00:22:59,309 --> 00:23:12,260
manually but it's not as dynamic as it

00:23:03,480 --> 00:23:12,260
could be let some jump back into ok

00:23:13,040 --> 00:23:22,950
so very quickly coming back we have

00:23:18,050 --> 00:23:25,890
client libraries for most programming

00:23:22,950 --> 00:23:29,190
languages some created by our team some

00:23:25,890 --> 00:23:33,510
created by community and others improved

00:23:29,190 --> 00:23:35,430
by community as well and as we've

00:23:33,510 --> 00:23:37,020
already mentioned is familiar sequel so

00:23:35,430 --> 00:23:39,570
if you're thinking of switching from

00:23:37,020 --> 00:23:42,330
something that pre-exists that sequel

00:23:39,570 --> 00:23:48,840
style then development team doesn't have

00:23:42,330 --> 00:23:51,630
to relearn anything completely i will

00:23:48,840 --> 00:23:54,420
show you yeah i will do this very

00:23:51,630 --> 00:23:57,660
quickly I'd like to get on to the more

00:23:54,420 --> 00:24:03,860
darker specific things but as an example

00:23:57,660 --> 00:24:07,050
i have three code examples here of

00:24:03,860 --> 00:24:11,010
fairly simple applications but they're

00:24:07,050 --> 00:24:15,240
using that same data set of how you

00:24:11,010 --> 00:24:16,950
would start to use the the database in

00:24:15,240 --> 00:24:20,520
your application so here's a very simple

00:24:16,950 --> 00:24:23,610
python example it's it's supposed to

00:24:20,520 --> 00:24:26,100
emulate the steps being increased so it

00:24:23,610 --> 00:24:29,220
generates a random amount of steps and a

00:24:26,100 --> 00:24:31,530
timestamp and writes it to the database

00:24:29,220 --> 00:24:35,460
that's basically all that's doing but

00:24:31,530 --> 00:24:37,890
it's a reasonably real-world example and

00:24:35,460 --> 00:24:41,310
i guess you could see from the top here

00:24:37,890 --> 00:24:43,530
that we have a specific client adapter

00:24:41,310 --> 00:24:45,650
for python and it gives you some

00:24:43,530 --> 00:24:48,300
functionality to connect to the database

00:24:45,650 --> 00:24:57,540
but then it largely comes down to making

00:24:48,300 --> 00:24:59,250
sequel style execution statements i will

00:24:57,540 --> 00:25:04,200
run this because there's one other

00:24:59,250 --> 00:25:06,450
factor about and about the nature of

00:25:04,200 --> 00:25:07,830
crate and the way it works and this may

00:25:06,450 --> 00:25:10,590
be a question you'd like to ask in a bit

00:25:07,830 --> 00:25:15,300
more detail is we are what is called

00:25:10,590 --> 00:25:21,930
eventually consistent to keep that speed

00:25:15,300 --> 00:25:24,150
and performance data obviously it's not

00:25:21,930 --> 00:25:25,900
magic data has to synchronize somehow

00:25:24,150 --> 00:25:28,120
and it takes some time

00:25:25,900 --> 00:25:30,730
so I'm going to run this Python example

00:25:28,120 --> 00:25:33,250
to illustrate the fact that we create a

00:25:30,730 --> 00:25:36,370
new record query for that record but

00:25:33,250 --> 00:25:38,620
it's not actually there yet so again I'm

00:25:36,370 --> 00:25:40,900
intentionally doing something wrong to

00:25:38,620 --> 00:25:47,070
sort of illustrate a point but let me

00:25:40,900 --> 00:25:47,070
just cordon

00:25:50,740 --> 00:25:57,940
so this will just generate the random

00:25:55,960 --> 00:25:59,860
timestamp generates the steps I've

00:25:57,940 --> 00:26:03,580
written to the database and I've queried

00:25:59,860 --> 00:26:06,490
the database for that line but it's it's

00:26:03,580 --> 00:26:09,250
not available yet as we saw this is that

00:26:06,490 --> 00:26:11,170
same database we saw on the AWS cluster

00:26:09,250 --> 00:26:13,030
it took a few minutes to synchronize if

00:26:11,170 --> 00:26:14,830
we came back later we'd be able to query

00:26:13,030 --> 00:26:16,000
for it but let's do illustrate a point

00:26:14,830 --> 00:26:18,070
that the data isn't available

00:26:16,000 --> 00:26:19,480
immediately and the negatives and

00:26:18,070 --> 00:26:20,950
positives of that type of infrastructure

00:26:19,480 --> 00:26:24,880
will come back to at the end of the

00:26:20,950 --> 00:26:28,120
presentation we also have a again a

00:26:24,880 --> 00:26:30,400
fairly basic node example again there's

00:26:28,120 --> 00:26:33,190
a library available the node libraries

00:26:30,400 --> 00:26:36,370
are probably some of the some of the

00:26:33,190 --> 00:26:40,800
more thorough implementations as well we

00:26:36,370 --> 00:26:44,110
connect to the instance here we have

00:26:40,800 --> 00:26:45,550
with a sort of more simple query we can

00:26:44,110 --> 00:26:47,770
do this sort of structure of querying

00:26:45,550 --> 00:26:49,990
what columns we want the query we want

00:26:47,770 --> 00:26:52,030
the limit we want and we just log it to

00:26:49,990 --> 00:26:56,920
console and if we want something a bit

00:26:52,030 --> 00:26:58,420
more complicated we at the moment as all

00:26:56,920 --> 00:27:01,900
of these open source so open to

00:26:58,420 --> 00:27:06,190
improvement we have to run a standard

00:27:01,900 --> 00:27:08,910
sort of sequel query and again I mean

00:27:06,190 --> 00:27:08,910
this will be

00:27:28,850 --> 00:27:34,340
yeah as you'd expect it outputs the

00:27:31,460 --> 00:27:38,570
results of the queries to console that's

00:27:34,340 --> 00:27:40,910
all it was doing but this is all to that

00:27:38,570 --> 00:27:45,050
live database and one final example

00:27:40,910 --> 00:27:47,630
which I am moderately happy with is an

00:27:45,050 --> 00:27:50,930
Android example I'll get the emulator

00:27:47,630 --> 00:27:53,570
running whilst I'm explaining the

00:27:50,930 --> 00:27:56,600
emulator has got faster but maybe we'll

00:27:53,570 --> 00:27:58,700
come back to it later but here I'm just

00:27:56,600 --> 00:28:02,360
clearing the HTTP endpoint we have

00:27:58,700 --> 00:28:04,190
available normally with an application

00:28:02,360 --> 00:28:05,720
you probably wouldn't do that you'd have

00:28:04,190 --> 00:28:08,600
your own back-end but this is just a

00:28:05,720 --> 00:28:10,430
demo so and demos aren't the real world

00:28:08,600 --> 00:28:12,920
but just to show you an example of how

00:28:10,430 --> 00:28:14,990
if there isn't a client library there is

00:28:12,920 --> 00:28:18,800
an HTTP endpoint available and you can

00:28:14,990 --> 00:28:22,520
just issue again queries to it as we can

00:28:18,800 --> 00:28:30,020
see here and this again is doing much

00:28:22,520 --> 00:28:32,990
the same yeah I don't think I've got

00:28:30,020 --> 00:28:35,270
time for that is it but it just pulls

00:28:32,990 --> 00:28:37,880
that list of records and displays them

00:28:35,270 --> 00:28:40,450
in a list and actually the android

00:28:37,880 --> 00:28:43,010
emulator is very slow but the

00:28:40,450 --> 00:28:46,100
application and the query comes back

00:28:43,010 --> 00:28:49,930
very fast so to shame that that it lets

00:28:46,100 --> 00:28:52,700
me down oh here we go again not too bad

00:28:49,930 --> 00:28:55,280
you always have there we go so that's

00:28:52,700 --> 00:28:57,170
the same data set again there's no fancy

00:28:55,280 --> 00:28:59,480
visualization but there's pretty quick

00:28:57,170 --> 00:29:01,490
and this is it emulated firing as well

00:28:59,480 --> 00:29:04,970
so there's all that lack of performance

00:29:01,490 --> 00:29:09,310
there so anyway that's some very quick

00:29:04,970 --> 00:29:13,460
coding examples let's get back to

00:29:09,310 --> 00:29:16,580
tadakha so very quickly I'm going to

00:29:13,460 --> 00:29:21,890
look at compose this is a Dockers way of

00:29:16,580 --> 00:29:25,550
and creating like a text description of

00:29:21,890 --> 00:29:28,010
the the components in your application

00:29:25,550 --> 00:29:29,960
infrastructure and I use this it's a

00:29:28,010 --> 00:29:32,020
very basic example but just because of

00:29:29,960 --> 00:29:34,580
course if you're going to be creating a

00:29:32,020 --> 00:29:37,780
micro service type application of many

00:29:34,580 --> 00:29:40,940
parts something like this helps you just

00:29:37,780 --> 00:29:43,940
construct that

00:29:40,940 --> 00:29:48,440
and here's how you would do some of

00:29:43,940 --> 00:29:50,570
those custom commands with with crate

00:29:48,440 --> 00:29:54,200
but here's where we want to really get

00:29:50,570 --> 00:29:57,230
to machine and swarm so dr. machine is

00:29:54,200 --> 00:30:00,100
its tool of creating virtual machines

00:29:57,230 --> 00:30:03,380
very easily it's not that hard to create

00:30:00,100 --> 00:30:04,910
virtual machines without documenting but

00:30:03,380 --> 00:30:09,290
it makes it even simpler and you can

00:30:04,910 --> 00:30:12,170
choose the various drivers you want to

00:30:09,290 --> 00:30:14,590
use be at VirtualBox AWS digitalocean

00:30:12,170 --> 00:30:17,480
and a whole bunch of other options and

00:30:14,590 --> 00:30:19,430
this is all fairly standard I'm not

00:30:17,480 --> 00:30:21,470
going to run this code I'm kind of

00:30:19,430 --> 00:30:24,970
reaching a conclusion here which is to

00:30:21,470 --> 00:30:27,620
illustrate the steps along the way so

00:30:24,970 --> 00:30:32,170
when that comes in more useful is when

00:30:27,620 --> 00:30:37,430
we want to create a swarm now dr. swarm

00:30:32,170 --> 00:30:39,230
is a way of creating multiple documents

00:30:37,430 --> 00:30:40,700
and in grouping them together and this

00:30:39,230 --> 00:30:42,350
is kind of we want to go this is what we

00:30:40,700 --> 00:30:45,430
were looking at we want to create a

00:30:42,350 --> 00:30:48,710
fully distributed database that is

00:30:45,430 --> 00:30:51,140
everything is aware of each other and

00:30:48,710 --> 00:30:52,670
when we remove and add instances the

00:30:51,140 --> 00:30:55,580
data is synchronized that is the point

00:30:52,670 --> 00:30:58,730
and docker swarm feels like we could

00:30:55,580 --> 00:31:00,880
take us take us there but if we're on

00:30:58,730 --> 00:31:04,820
multiple hosts we have that same problem

00:31:00,880 --> 00:31:09,740
there are ways around it we saw the

00:31:04,820 --> 00:31:11,000
manual declaration earlier and you can

00:31:09,740 --> 00:31:14,810
automate that threw a bash script or

00:31:11,000 --> 00:31:16,490
something like that but it's still may

00:31:14,810 --> 00:31:18,980
be more complicated than it really needs

00:31:16,490 --> 00:31:22,070
to be so this is just going through some

00:31:18,980 --> 00:31:26,210
of those steps we create a discovery

00:31:22,070 --> 00:31:30,260
token of the the master in the swarm we

00:31:26,210 --> 00:31:34,040
then create the master give it the token

00:31:30,260 --> 00:31:38,300
and add it to the swarm we then start

00:31:34,040 --> 00:31:42,230
creating the the nodes or the slaves

00:31:38,300 --> 00:31:44,060
give them the same token and at that

00:31:42,230 --> 00:31:45,890
point in time theoretically we could

00:31:44,060 --> 00:31:48,230
then go docker info and we'd see a list

00:31:45,890 --> 00:31:52,160
of all the nodes and masters in that

00:31:48,230 --> 00:31:54,080
swamp but if we're on AWS or if we're on

00:31:52,160 --> 00:31:56,779
any other cloud hosting

00:31:54,080 --> 00:31:59,059
it's not going to work unless again we

00:31:56,779 --> 00:32:03,289
do something like this that we saw

00:31:59,059 --> 00:32:04,880
earlier which is fine but yeah so I

00:32:03,289 --> 00:32:07,039
guess our recommended way at the moment

00:32:04,880 --> 00:32:10,789
of making this truly distributed

00:32:07,039 --> 00:32:13,360
database with with docker is we at the

00:32:10,789 --> 00:32:15,860
moment are using something called weave

00:32:13,360 --> 00:32:17,029
I'm not going to step through all the

00:32:15,860 --> 00:32:19,070
points but I'll show you the end result

00:32:17,029 --> 00:32:20,990
but here's the blog posts that are

00:32:19,070 --> 00:32:25,010
basically followed to create this

00:32:20,990 --> 00:32:27,649
example so we've is a software-defined

00:32:25,010 --> 00:32:30,380
networking technology and integrates

00:32:27,649 --> 00:32:32,990
with docker and is pretty easy to use

00:32:30,380 --> 00:32:37,299
and it lets you create overlay networks

00:32:32,990 --> 00:32:40,519
over all your other networks to

00:32:37,299 --> 00:32:43,070
basically enabling multicast to work in

00:32:40,519 --> 00:32:46,399
it claims eddie cloud that's a big claim

00:32:43,070 --> 00:32:50,049
will say most clouds so let's have a

00:32:46,399 --> 00:32:50,049
quick look at how that looks

00:32:57,440 --> 00:33:04,669
so in this instance we've used compute

00:33:00,350 --> 00:33:06,350
engine we have three we've three we've

00:33:04,669 --> 00:33:07,970
instances three instances in this

00:33:06,350 --> 00:33:09,409
demonstration down here all of death of

00:33:07,970 --> 00:33:12,740
different IP addresses different

00:33:09,409 --> 00:33:14,330
machines normally if we then ran a crate

00:33:12,740 --> 00:33:16,220
and docker honor those they wouldn't be

00:33:14,330 --> 00:33:18,490
able to find each other we've installed

00:33:16,220 --> 00:33:20,450
we've on each one and got them

00:33:18,490 --> 00:33:24,799
communicating through the same channels

00:33:20,450 --> 00:33:27,590
and we end up with this and this this is

00:33:24,799 --> 00:33:29,690
the end result again I've imported a few

00:33:27,590 --> 00:33:32,450
tweets into it we have the three

00:33:29,690 --> 00:33:35,620
instances which are the same three we

00:33:32,450 --> 00:33:37,759
can see at the bottom of that list and

00:33:35,620 --> 00:33:40,940
that's it it doesn't really look any

00:33:37,759 --> 00:33:44,389
different but but there are three nodes

00:33:40,940 --> 00:33:48,320
on three different machine machines or

00:33:44,389 --> 00:33:51,200
communicating with each other and if you

00:33:48,320 --> 00:33:53,659
follow that blog post here it's

00:33:51,200 --> 00:33:55,490
reasonably straightforward to get it to

00:33:53,659 --> 00:33:58,909
get it working and we truly have a

00:33:55,490 --> 00:34:02,419
distributed database across networks and

00:33:58,909 --> 00:34:05,570
across cloud networks as well it's a

00:34:02,419 --> 00:34:08,240
slightly underwhelming sort of end point

00:34:05,570 --> 00:34:13,609
but it looks exactly the same but that

00:34:08,240 --> 00:34:16,210
is what we have there so just to to wrap

00:34:13,609 --> 00:34:18,349
up and have some time for questions and

00:34:16,210 --> 00:34:20,450
if you have any more questions we don't

00:34:18,349 --> 00:34:23,659
have time to answer I will be here

00:34:20,450 --> 00:34:25,099
tomorrow as well so what are the use

00:34:23,659 --> 00:34:27,530
cases for this let's start with the

00:34:25,099 --> 00:34:29,540
negatives systems that require strong

00:34:27,530 --> 00:34:32,780
consistency we have that eventual

00:34:29,540 --> 00:34:34,909
consistency so anything that you one

00:34:32,780 --> 00:34:37,129
hundred percent need the record to be

00:34:34,909 --> 00:34:40,040
available the very second it's been

00:34:37,129 --> 00:34:41,659
written then no and there are use cases

00:34:40,040 --> 00:34:44,030
where that would be the case say for

00:34:41,659 --> 00:34:47,050
example financial banks something like

00:34:44,030 --> 00:34:50,659
that equally the same applies with

00:34:47,050 --> 00:34:53,089
transactions crate will keep going when

00:34:50,659 --> 00:34:55,429
when queries fail which may mean that

00:34:53,089 --> 00:34:57,940
other subsequent queries are not

00:34:55,429 --> 00:35:00,020
accurate anymore and with a lot of

00:34:57,940 --> 00:35:01,849
application structures that's fine but

00:35:00,020 --> 00:35:06,400
there are many is that that isn't fine

00:35:01,849 --> 00:35:10,010
and strong relational data and currently

00:35:06,400 --> 00:35:11,150
we don't support proper joins but

00:35:10,010 --> 00:35:13,490
they're coming very

00:35:11,150 --> 00:35:16,150
soon and we're sort of ironing out the

00:35:13,490 --> 00:35:19,130
kinks in the performance that that

00:35:16,150 --> 00:35:20,180
effects but they're coming very soon so

00:35:19,130 --> 00:35:22,390
that's on the negative but on the

00:35:20,180 --> 00:35:24,500
positive things like high volume

00:35:22,390 --> 00:35:26,329
semi-structured data that changes all

00:35:24,500 --> 00:35:29,359
the time Internet of Things applications

00:35:26,329 --> 00:35:30,170
big data applications we have a lot of

00:35:29,359 --> 00:35:32,930
people using it for business

00:35:30,170 --> 00:35:35,270
intelligence marketing intelligence so

00:35:32,930 --> 00:35:38,089
data that it is coming all the time and

00:35:35,270 --> 00:35:40,130
is changing all the time and the the

00:35:38,089 --> 00:35:45,369
amount that it comes in that is changing

00:35:40,130 --> 00:35:48,829
all the time and because of that sequel

00:35:45,369 --> 00:35:51,170
type language it's very easy to to

00:35:48,829 --> 00:35:54,109
migrate from my sequel or something like

00:35:51,170 --> 00:35:57,440
that that's what you've been using so I

00:35:54,109 --> 00:35:58,730
hope that was a reasonable summary of

00:35:57,440 --> 00:36:01,640
the concept and how to get it working

00:35:58,730 --> 00:36:03,260
here's some of our contact details also

00:36:01,640 --> 00:36:05,510
have some lovely t-shirts and stickers

00:36:03,260 --> 00:36:09,650
and all that kind of stuff if anyone

00:36:05,510 --> 00:36:11,710
wants any and a good a few minutes for

00:36:09,650 --> 00:36:11,710

YouTube URL: https://www.youtube.com/watch?v=gILCPu6lGZo


