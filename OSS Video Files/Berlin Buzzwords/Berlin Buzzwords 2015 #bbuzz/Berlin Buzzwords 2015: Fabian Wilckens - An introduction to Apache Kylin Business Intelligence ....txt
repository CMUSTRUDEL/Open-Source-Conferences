Title: Berlin Buzzwords 2015: Fabian Wilckens - An introduction to Apache Kylin Business Intelligence ...
Publication date: 2015-06-05
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Fabian Wilckens talking about "An introduction to Apache Kylin - Business Intelligence meets Big Data".

Apache Kylin is an open source distributed analytics engine that originated at ebay, Inc. and provides a SQL-interface and multi-dimensional analysis for online analytical processing. Kylin was recently accepted by the Apache Software Foundation as an incubator project and already has a number of integrations with HDFS, MapReduce, Hive, HBase and Apache Drill.

This session will provide an overview about Apache Kylin, how it works, what it does and will give an introduction to Online Analytical Processing (OLAP), how Business Intelligence works and how Kylin + Hadoop can help in analyzing extremely large datasets.

Read more:
https://2015.berlinbuzzwords.de/session/introduction-apache-kylin-business-intelligence-meets-big-data

About Fabian Wilckens:
https://2015.berlinbuzzwords.de/users/fabian-wilckens

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,950 --> 00:00:14,169
okay so what I wanted to

00:00:10,269 --> 00:00:15,820
out today is Apache chi-lin so a lot of

00:00:14,169 --> 00:00:17,590
people may have heard about

00:00:15,820 --> 00:00:19,330
number of apache projects coming up

00:00:17,590 --> 00:00:21,490
every week essentially so the whole

00:00:19,330 --> 00:00:23,349
community changes but one of the most

00:00:21,490 --> 00:00:25,180
interesting projects which I find is

00:00:23,349 --> 00:00:29,079
very very interesting at the moment is

00:00:25,180 --> 00:00:30,640
Apache khylin so and that comes with a

00:00:29,079 --> 00:00:33,280
challenge at the same time so when I

00:00:30,640 --> 00:00:35,769
propose the session today I thought well

00:00:33,280 --> 00:00:37,809
it might be doable in 20 minutes then I

00:00:35,769 --> 00:00:40,900
walk through all the slides and said

00:00:37,809 --> 00:00:42,730
well there's certain assumptions you can

00:00:40,900 --> 00:00:44,860
make because I believe everybody knows

00:00:42,730 --> 00:00:47,950
what Hadoop is right everybody knows

00:00:44,860 --> 00:00:50,770
what Big Data is but does everybody know

00:00:47,950 --> 00:00:52,960
what olav is does everybody know what oh

00:00:50,770 --> 00:00:54,730
Lt P is and this is a bit of the

00:00:52,960 --> 00:00:57,100
challenge that I want to cover today so

00:00:54,730 --> 00:01:00,309
I walk through a bit of OLAP so what is

00:00:57,100 --> 00:01:02,260
it why it's important what challenges

00:01:00,309 --> 00:01:04,960
occur when you're dealing with all up in

00:01:02,260 --> 00:01:06,940
a big data space and how kaylynn solve

00:01:04,960 --> 00:01:08,979
these challenges integrates what to do

00:01:06,940 --> 00:01:10,869
and then I gonna talk a little bit about

00:01:08,979 --> 00:01:12,850
the Rope Maps or where Carlene said it

00:01:10,869 --> 00:01:15,970
and of course if we have some time left

00:01:12,850 --> 00:01:18,160
which I hope I may be able to answer

00:01:15,970 --> 00:01:20,080
some questions afterwards and I'll be

00:01:18,160 --> 00:01:22,780
there for the next couple of days as

00:01:20,080 --> 00:01:24,909
well so very quickly all up all up

00:01:22,780 --> 00:01:28,630
stands for online analytical processing

00:01:24,909 --> 00:01:32,020
and to give you one example one could

00:01:28,630 --> 00:01:34,630
say it's about beer in fact it's about a

00:01:32,020 --> 00:01:37,240
question that may be how many beers were

00:01:34,630 --> 00:01:40,329
ordered in Germany on a yearly basis or

00:01:37,240 --> 00:01:41,770
to be even more specific how many beers

00:01:40,329 --> 00:01:44,320
were ordered in Germany on a yearly

00:01:41,770 --> 00:01:46,119
basis broken down by month but this is

00:01:44,320 --> 00:01:48,070
questions that everybody wants to answer

00:01:46,119 --> 00:01:49,810
right being in retail being in business

00:01:48,070 --> 00:01:52,000
analytics being in business intelligence

00:01:49,810 --> 00:01:54,250
this is the most important questions

00:01:52,000 --> 00:01:56,350
Germans drink beer how much beer do they

00:01:54,250 --> 00:01:58,450
drink when do they drink beer where do

00:01:56,350 --> 00:02:00,610
they drink beer etc etc just to give you

00:01:58,450 --> 00:02:03,399
an example and what you can do with all

00:02:00,610 --> 00:02:06,100
up is you can build something which is

00:02:03,399 --> 00:02:07,990
called an OLAP cube and all of cube may

00:02:06,100 --> 00:02:10,299
look like this this is just a graphical

00:02:07,990 --> 00:02:14,530
representation and I may actually be

00:02:10,299 --> 00:02:17,079
able to ask a question so which or which

00:02:14,530 --> 00:02:19,420
amounted liters or whatever it is right

00:02:17,079 --> 00:02:21,220
whilst ranked in Hamburg in May so

00:02:19,420 --> 00:02:24,310
that's the representation of an OLAP

00:02:21,220 --> 00:02:26,560
cube it consists of two basic things the

00:02:24,310 --> 00:02:29,700
first one is it consists of measures so

00:02:26,560 --> 00:02:32,580
measures of numeric values that

00:02:29,700 --> 00:02:34,800
so-called facts and it consists of

00:02:32,580 --> 00:02:36,900
something which is called a dimension as

00:02:34,800 --> 00:02:39,239
you can see that maybe products that

00:02:36,900 --> 00:02:41,580
maybe some things about orders there may

00:02:39,239 --> 00:02:42,930
be some more granular things that you

00:02:41,580 --> 00:02:45,360
can actually filter on so that's

00:02:42,930 --> 00:02:46,860
basically the filter criterias and you

00:02:45,360 --> 00:02:48,870
have the numeric the facts the

00:02:46,860 --> 00:02:51,209
measurements to actually have an

00:02:48,870 --> 00:02:52,769
aggregated sum or an aggregated number

00:02:51,209 --> 00:02:57,630
of something that could represent

00:02:52,769 --> 00:02:59,519
whatever it may be so if you look at

00:02:57,630 --> 00:03:01,500
that now from all a perspective once

00:02:59,519 --> 00:03:03,420
again you can do certain things which

00:03:01,500 --> 00:03:05,489
are called all up operations right you

00:03:03,420 --> 00:03:09,510
can do things like well I want to know

00:03:05,489 --> 00:03:11,610
how much beer was consumed in Berlin

00:03:09,510 --> 00:03:14,519
over a certain time frame and this is

00:03:11,610 --> 00:03:16,890
incredibly great for analysts because

00:03:14,519 --> 00:03:18,860
it's a model they can greatly work with

00:03:16,890 --> 00:03:21,569
right so you have a relational data

00:03:18,860 --> 00:03:23,700
structure in the background which

00:03:21,569 --> 00:03:26,579
typically works in a relational database

00:03:23,700 --> 00:03:29,310
and this is very good that's a great

00:03:26,579 --> 00:03:31,829
idea but now we came to a world where we

00:03:29,310 --> 00:03:35,430
talked about big data right so what is

00:03:31,829 --> 00:03:38,100
that relational model q building etc in

00:03:35,430 --> 00:03:41,190
the world of big data right so how can

00:03:38,100 --> 00:03:43,319
we do that so and some folks at eBay

00:03:41,190 --> 00:03:46,079
started the project called chi-lin and

00:03:43,319 --> 00:03:48,739
chi-lin is supposed to be the extreme

00:03:46,079 --> 00:03:52,019
all up engine for big data

00:03:48,739 --> 00:03:54,389
Kylene is far as I remember it's sort of

00:03:52,019 --> 00:03:56,790
a dragon thing something mythical like a

00:03:54,389 --> 00:03:59,459
strange creature that existed sometime

00:03:56,790 --> 00:04:01,620
100 years ago and for some reason was

00:03:59,459 --> 00:04:04,739
chosen as the project name however it

00:04:01,620 --> 00:04:07,440
got open-sourced late last year it's

00:04:04,739 --> 00:04:09,569
currently incubating at Apache and they

00:04:07,440 --> 00:04:12,510
are targeting an initial release fairly

00:04:09,569 --> 00:04:14,970
soon so expect to see more out of

00:04:12,510 --> 00:04:19,590
curling in the near future and the goals

00:04:14,970 --> 00:04:23,340
are really to provide subsequent queries

00:04:19,590 --> 00:04:25,789
on let's say really large volumes of

00:04:23,340 --> 00:04:29,039
data like billions and trillions of rows

00:04:25,789 --> 00:04:31,200
you want to be fully NC compliance and

00:04:29,039 --> 00:04:32,700
say sequel compliant because all the

00:04:31,200 --> 00:04:34,500
analysts all the people in data

00:04:32,700 --> 00:04:38,159
warehouse all the business intelligence

00:04:34,500 --> 00:04:41,430
people they all know NC sequel and they

00:04:38,159 --> 00:04:42,810
also want fall all up statements they

00:04:41,430 --> 00:04:43,569
also want to have full all-out

00:04:42,810 --> 00:04:46,300
compliance

00:04:43,569 --> 00:04:47,979
so they want to work with a database or

00:04:46,300 --> 00:04:50,710
with a system that may look like a

00:04:47,979 --> 00:04:53,050
database the way they have worked with

00:04:50,710 --> 00:04:56,080
the database before and what they also

00:04:53,050 --> 00:04:57,879
want is no one in the BI world wants to

00:04:56,080 --> 00:04:59,830
work directly on Hadoop right they want

00:04:57,879 --> 00:05:01,179
to use their fancy applications they

00:04:59,830 --> 00:05:03,099
want to use the tableau as the platform

00:05:01,179 --> 00:05:05,199
as the datum years this SAS whatever

00:05:03,099 --> 00:05:08,110
systems exist on the planet and they

00:05:05,199 --> 00:05:09,429
want them to smoothly integrate they may

00:05:08,110 --> 00:05:11,770
change the engine so they don't

00:05:09,429 --> 00:05:12,459
necessarily care if it's a database in

00:05:11,770 --> 00:05:14,289
the backend

00:05:12,459 --> 00:05:16,629
but they want their front-end tools they

00:05:14,289 --> 00:05:19,629
wanted beautiful they wanted fancy

00:05:16,629 --> 00:05:21,879
whatever you may call it and the other

00:05:19,629 --> 00:05:25,149
thing is what they also want to target

00:05:21,879 --> 00:05:27,939
is to have really a platform that scales

00:05:25,149 --> 00:05:29,439
to thousands of users right you have the

00:05:27,939 --> 00:05:31,059
thing of the scale of eBay right so

00:05:29,439 --> 00:05:33,879
there's not something they played around

00:05:31,059 --> 00:05:35,919
with but they use internally to produce

00:05:33,879 --> 00:05:38,889
exactly these cubes on large-scale data

00:05:35,919 --> 00:05:40,509
sets you can imagine eBay what that's a

00:05:38,889 --> 00:05:43,029
bit of data what they have right in

00:05:40,509 --> 00:05:46,809
terms of articles auctions categories

00:05:43,029 --> 00:05:49,209
etc whatever they can do so what Carlene

00:05:46,809 --> 00:05:51,969
also does it uses a lot of the

00:05:49,209 --> 00:05:56,139
components that exist in Hadoop so it

00:05:51,969 --> 00:05:58,749
basically takes hive to sort of pre join

00:05:56,139 --> 00:06:02,589
pre articulate pre aggregate views and

00:05:58,749 --> 00:06:06,069
for queue building it uses MapReduce to

00:06:02,589 --> 00:06:08,829
generate these excerpts if you will it

00:06:06,069 --> 00:06:12,339
uses HDFS and HBase to store the values

00:06:08,829 --> 00:06:13,990
so imagine a cube being a key value pair

00:06:12,339 --> 00:06:16,839
so there's a number of combinations

00:06:13,990 --> 00:06:19,869
writing key values in the cube and these

00:06:16,839 --> 00:06:22,059
things actually stored in HBase so they

00:06:19,869 --> 00:06:24,879
query through an Z sequel interface and

00:06:22,059 --> 00:06:25,839
they interfere with HBase directly so

00:06:24,879 --> 00:06:27,449
when you look at that

00:06:25,839 --> 00:06:31,300
more from an architectural standpoint

00:06:27,449 --> 00:06:33,819
you have wife on the one hand side which

00:06:31,300 --> 00:06:37,389
is essentially used from a cube building

00:06:33,819 --> 00:06:40,539
engine so it takes star schema data the

00:06:37,389 --> 00:06:44,019
data you've seen builds the key value

00:06:40,539 --> 00:06:45,849
data stores that in HBase and then we

00:06:44,019 --> 00:06:48,099
have a metadata central metadata

00:06:45,849 --> 00:06:49,689
repository and there's the number of

00:06:48,099 --> 00:06:50,919
chi-ling components that does for

00:06:49,689 --> 00:06:52,539
instance occuring engine does the

00:06:50,919 --> 00:06:55,120
routing there's a rest interface

00:06:52,539 --> 00:06:57,460
provided there's the ability to plug in

00:06:55,120 --> 00:06:59,350
through party apps or just you

00:06:57,460 --> 00:07:01,479
using standard JDBC already received

00:06:59,350 --> 00:07:04,270
pipe connections to interface with the

00:07:01,479 --> 00:07:06,250
system however what it can also do it

00:07:04,270 --> 00:07:08,530
can route the low latency queries

00:07:06,250 --> 00:07:10,720
through HBase which are really low

00:07:08,530 --> 00:07:13,300
latency so I need my result in seconds

00:07:10,720 --> 00:07:16,449
on large-scale data right but it can

00:07:13,300 --> 00:07:19,500
also theoretically interface with hive

00:07:16,449 --> 00:07:24,490
to get some mid latency some well

00:07:19,500 --> 00:07:26,110
minutes to hours ok curious results and

00:07:24,490 --> 00:07:28,150
if you look at the data flown I think

00:07:26,110 --> 00:07:29,710
this is one of the most important things

00:07:28,150 --> 00:07:31,720
about the architecture of Kylene is

00:07:29,710 --> 00:07:33,280
really to have we have an online data

00:07:31,720 --> 00:07:35,229
flow which is everything that is in

00:07:33,280 --> 00:07:37,479
green so these are the active components

00:07:35,229 --> 00:07:40,300
and you get the blue data flow which is

00:07:37,479 --> 00:07:43,120
basically the pre calculation that has

00:07:40,300 --> 00:07:45,280
been done to make the data accessible

00:07:43,120 --> 00:07:48,430
right through a front-end application

00:07:45,280 --> 00:07:50,349
and the more important thing is that the

00:07:48,430 --> 00:07:53,530
OLAP cube stays transparent for the user

00:07:50,349 --> 00:07:55,900
so what they see is a database kind of

00:07:53,530 --> 00:07:57,400
structure so it looks like a database

00:07:55,900 --> 00:07:59,409
and this is the beauty of Hadoop it can

00:07:57,400 --> 00:08:00,759
pretty much look like everything can

00:07:59,409 --> 00:08:03,070
look like a database can look like a

00:08:00,759 --> 00:08:05,680
file system in this case it pretty much

00:08:03,070 --> 00:08:08,860
looks like what you would expect from a

00:08:05,680 --> 00:08:10,780
relational database standpoint some

00:08:08,860 --> 00:08:12,699
highlights about chi-lin because there's

00:08:10,780 --> 00:08:16,270
so much things to talk about so just

00:08:12,699 --> 00:08:19,150
some highlights it's extremely fast at

00:08:16,270 --> 00:08:21,430
scale it was made for scale it was made

00:08:19,150 --> 00:08:23,800
to scale out there's no let's say

00:08:21,430 --> 00:08:25,360
there's no no thinking of having at

00:08:23,800 --> 00:08:26,770
single-threaded so it all runs in

00:08:25,360 --> 00:08:29,139
parallel there's multi parallel

00:08:26,770 --> 00:08:32,260
operations and it just scales as you

00:08:29,139 --> 00:08:34,390
grow it should also provide NC sequel

00:08:32,260 --> 00:08:37,000
compliance because all these tools are

00:08:34,390 --> 00:08:39,459
very good at creating NCC per complain

00:08:37,000 --> 00:08:41,229
statements all these front-end tools are

00:08:39,459 --> 00:08:44,020
not necessarily good at creating hive

00:08:41,229 --> 00:08:46,870
statements because hive is just a subset

00:08:44,020 --> 00:08:48,880
or hive QL is just a subset of the ANSI

00:08:46,870 --> 00:08:51,029
sequel standard so that's really one of

00:08:48,880 --> 00:08:53,380
the other design goals they had and

00:08:51,029 --> 00:08:56,680
seamless integration I talked about that

00:08:53,380 --> 00:08:58,450
some time now but they want it to work

00:08:56,680 --> 00:09:01,870
with existing applications so they

00:08:58,450 --> 00:09:05,020
really want to use it together with

00:09:01,870 --> 00:09:08,620
whatever front-end application and they

00:09:05,020 --> 00:09:11,170
want to provide as I said large scale

00:09:08,620 --> 00:09:14,470
record scans large scale cube building

00:09:11,170 --> 00:09:16,149
kind of applications so more highlights

00:09:14,470 --> 00:09:17,829
so it's actually pretty powerful for an

00:09:16,149 --> 00:09:21,279
incubating project right so this is

00:09:17,829 --> 00:09:22,990
something that is not particularly

00:09:21,279 --> 00:09:25,329
community driven it's something that

00:09:22,990 --> 00:09:27,250
Yvette donated if you want to the

00:09:25,329 --> 00:09:30,190
community and it's picking up great

00:09:27,250 --> 00:09:31,899
crazily fast because it just runs on the

00:09:30,190 --> 00:09:34,779
Hadoop infrastructure that you just have

00:09:31,899 --> 00:09:36,459
there's no new no fancy component

00:09:34,779 --> 00:09:38,529
involved which is not proven for years

00:09:36,459 --> 00:09:41,110
so it uses what has been in production

00:09:38,529 --> 00:09:44,589
right and this makes it very very

00:09:41,110 --> 00:09:47,949
interesting to implement Kylian what it

00:09:44,589 --> 00:09:50,800
also does is it provides a very very

00:09:47,949 --> 00:09:52,630
great graphical interface that can do

00:09:50,800 --> 00:09:55,209
certain things so the whole cube design

00:09:52,630 --> 00:09:58,120
process can be done through a graphical

00:09:55,209 --> 00:09:59,860
interface so there's no UI start where

00:09:58,120 --> 00:10:01,600
there's no command line stuff that you

00:09:59,860 --> 00:10:03,790
need to do so you can actually say well

00:10:01,600 --> 00:10:05,949
here bi admin you know all the words you

00:10:03,790 --> 00:10:10,050
know what bi looks like here go ahead

00:10:05,949 --> 00:10:12,970
design your cubes the way you want it so

00:10:10,050 --> 00:10:14,560
this is how it looks like you can scroll

00:10:12,970 --> 00:10:16,600
to that you can work through the process

00:10:14,560 --> 00:10:19,360
and it will come up with a graphical

00:10:16,600 --> 00:10:21,310
representation so as you can see it's

00:10:19,360 --> 00:10:23,500
it's easy to use right it provides some

00:10:21,310 --> 00:10:25,420
granular security it provides the

00:10:23,500 --> 00:10:28,089
ability to do job management in Hadoop

00:10:25,420 --> 00:10:30,370
so ultimately you build the cubes and it

00:10:28,089 --> 00:10:32,649
will result in a MapReduce job in the

00:10:30,370 --> 00:10:35,620
end so this is all the way down to the

00:10:32,649 --> 00:10:37,149
Hadoop integration and furthermore and I

00:10:35,620 --> 00:10:39,850
think that's a really that's a really

00:10:37,149 --> 00:10:41,079
cool feature looks a bit like hue so for

00:10:39,850 --> 00:10:44,350
those of you who have played with you

00:10:41,079 --> 00:10:47,980
Hadoop user experience it actually lets

00:10:44,350 --> 00:10:50,529
you query the data in a browser but it

00:10:47,980 --> 00:10:52,120
will also give you some sort of our

00:10:50,529 --> 00:10:54,519
interactive feeling because it can

00:10:52,120 --> 00:10:56,829
produce the results and fancy charts pie

00:10:54,519 --> 00:10:58,120
charts bar charts whatever you want and

00:10:56,829 --> 00:11:01,420
it's all open source so it makes it

00:10:58,120 --> 00:11:03,819
really easy to start with so you don't

00:11:01,420 --> 00:11:05,829
have to start with let's say building

00:11:03,819 --> 00:11:07,480
everything from scratch in terms of a bi

00:11:05,829 --> 00:11:09,610
application but you can say well I may

00:11:07,480 --> 00:11:13,389
try something using that interface first

00:11:09,610 --> 00:11:15,250
and then we may see if we can go further

00:11:13,389 --> 00:11:17,860
or if we really want to put it to

00:11:15,250 --> 00:11:20,860
production once it's really GA once it's

00:11:17,860 --> 00:11:23,230
a full top-level Apache project but it

00:11:20,860 --> 00:11:24,819
will get there eventually it's a little

00:11:23,230 --> 00:11:27,369
bit on the roadmap and the history

00:11:24,819 --> 00:11:29,949
was a fairly new project so they started

00:11:27,369 --> 00:11:32,679
in September essentially two years ago

00:11:29,949 --> 00:11:35,619
or one and a half years ago and what

00:11:32,679 --> 00:11:38,319
they achieved so far is in January last

00:11:35,619 --> 00:11:40,239
year they announced a prototype so take

00:11:38,319 --> 00:11:42,850
that took them four months to develop a

00:11:40,239 --> 00:11:44,919
prototype and it kind of escalated from

00:11:42,850 --> 00:11:47,859
there really so they added more and more

00:11:44,919 --> 00:11:49,569
and more features to it to provide what

00:11:47,859 --> 00:11:52,329
I've showed you but this is where we are

00:11:49,569 --> 00:11:55,209
now the red line should basically show

00:11:52,329 --> 00:11:58,149
mid of 2015 so chipped it a little bit

00:11:55,209 --> 00:11:59,350
to the left and what they also want to

00:11:58,149 --> 00:12:00,639
do they want to have it more

00:11:59,350 --> 00:12:03,489
enterprise-grade so they want to

00:12:00,639 --> 00:12:05,499
integrate it with Excel right it's crazy

00:12:03,489 --> 00:12:07,569
how many people use Excel to run these

00:12:05,499 --> 00:12:10,209
kind of analytics right I think

00:12:07,569 --> 00:12:12,579
excellest more or less the most use bi

00:12:10,209 --> 00:12:14,709
tool that exists on the planet and I

00:12:12,579 --> 00:12:17,169
believe it will still be and they are

00:12:14,709 --> 00:12:20,169
thinking about taking the hive part

00:12:17,169 --> 00:12:21,789
maybe off and shifted towards drill for

00:12:20,169 --> 00:12:23,379
those of you who have talked who have

00:12:21,789 --> 00:12:26,919
watched Ed session about drill which

00:12:23,379 --> 00:12:28,899
just ended a couple of minutes ago they

00:12:26,919 --> 00:12:32,589
may think about that for a faster queue

00:12:28,899 --> 00:12:35,229
building or to query data ad hoc from a

00:12:32,589 --> 00:12:36,789
cube standpoint and they may be able to

00:12:35,229 --> 00:12:39,069
add some capacity management some

00:12:36,789 --> 00:12:41,169
automation routines and possibly and I

00:12:39,069 --> 00:12:43,720
know that's a big topic nowadays to

00:12:41,169 --> 00:12:45,609
integrate with spark so provide in

00:12:43,720 --> 00:12:48,399
memory MapReduce to use the spark

00:12:45,609 --> 00:12:50,379
framework to do all the aggregations etc

00:12:48,399 --> 00:12:55,139
so there's a lot of things these guys

00:12:50,379 --> 00:12:58,689
are actually doing right now so how do

00:12:55,139 --> 00:13:00,579
you know or how some resources so

00:12:58,689 --> 00:13:02,739
there's a kind insight it's now on the

00:13:00,579 --> 00:13:05,259
incubate and Apache as well there's a

00:13:02,739 --> 00:13:07,239
Twitter account associated with it so if

00:13:05,259 --> 00:13:08,979
you want you can just participate

00:13:07,239 --> 00:13:11,319
there's a source code rapid there's some

00:13:08,979 --> 00:13:13,629
Google Groups etc and everything is

00:13:11,319 --> 00:13:15,699
about khylin and they are very very

00:13:13,629 --> 00:13:18,489
happy if you could just help them

00:13:15,699 --> 00:13:20,350
contribute test it feedback to the

00:13:18,489 --> 00:13:23,049
community feedback to the developers to

00:13:20,350 --> 00:13:25,449
the committers and this is how I want to

00:13:23,049 --> 00:13:27,720
conclude and I'm open to take questions

00:13:25,449 --> 00:13:27,720

YouTube URL: https://www.youtube.com/watch?v=gn8hafmEenU


