Title: Berlin Buzzwords 2015: Adrien Grand – Algorithms & data-structures that power Lucene & ElasticSearch
Publication date: 2015-06-02
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	When you want to make search fast, 80% of the job involves organizing your data so that it can be accessed with as little work as possible. This is the exact reason why Lucene is based on an inverted index. But there are some very interesting algorithms and data structures involved in that last 20% of the job. 

In this talk, you will gain insights into some internals of Lucene and ElasticSearch, and see how priority queues, finite state machines, bit twiddling hacks and several other algorithms and data structures help make them fast.

Read more:
https://2015.berlinbuzzwords.de/session/algorithms-and-data-structures-power-lucene-and-elasticsearch

About Adrien Grand:
https://2015.berlinbuzzwords.de/users/jpountz

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,930 --> 00:00:13,100
okay so thank you so has it just said

00:00:09,980 --> 00:00:15,890
I'm working at elastic where I'm looking

00:00:13,100 --> 00:00:17,870
that I have the chance twerkin

00:00:15,890 --> 00:00:20,110
elasticsearch and listen as part of my

00:00:17,870 --> 00:00:23,090
job and today we are going to some time

00:00:20,110 --> 00:00:25,880
diving into some internals of Huson

00:00:23,090 --> 00:00:29,270
elasticsearch but instead of focusing on

00:00:25,880 --> 00:00:31,010
how loosen elasticsearch we are a very

00:00:29,270 --> 00:00:32,330
high level perspective we are doing to

00:00:31,010 --> 00:00:34,070
do we are going to do something which is

00:00:32,330 --> 00:00:35,660
very different which is that we are

00:00:34,070 --> 00:00:38,360
going to focus on four very specific

00:00:35,660 --> 00:00:40,699
features and try to understand the way

00:00:38,360 --> 00:00:42,290
that they work this four specific

00:00:40,699 --> 00:00:45,680
features that we are going to talk about

00:00:42,290 --> 00:00:47,780
are first how does routine execute

00:00:45,680 --> 00:00:49,909
conjunctions for instant if you've built

00:00:47,780 --> 00:00:51,650
an index and you want to search on

00:00:49,909 --> 00:00:54,650
document that contain both foo and bar

00:00:51,650 --> 00:00:56,900
how does it work then we are going to

00:00:54,650 --> 00:00:58,909
spend some time trying to understand how

00:00:56,900 --> 00:01:01,940
loosen execute queries which are based

00:00:58,909 --> 00:01:04,299
on regular expressions then we are going

00:01:01,940 --> 00:01:06,979
to spend some time understanding how

00:01:04,299 --> 00:01:09,229
loosen compresses the red dot values if

00:01:06,979 --> 00:01:10,580
you don't know what the values are don't

00:01:09,229 --> 00:01:12,620
worry I'm going to introduce it and

00:01:10,580 --> 00:01:14,740
finally we are going to spend some time

00:01:12,620 --> 00:01:17,149
explaining how elastic search

00:01:14,740 --> 00:01:19,159
giardiniera to cardinality aggregation

00:01:17,149 --> 00:01:20,509
works the cutting edge aggregation

00:01:19,159 --> 00:01:22,640
beings the aggregation that you can use

00:01:20,509 --> 00:01:26,509
if you want to compute counts of

00:01:22,640 --> 00:01:29,659
distinct elements in a set so let's

00:01:26,509 --> 00:01:32,060
start but before we start actually

00:01:29,659 --> 00:01:34,670
explaining how conjunctions work I need

00:01:32,060 --> 00:01:38,359
to explain how an inverted index works

00:01:34,670 --> 00:01:41,509
and so here it is so an inverted index

00:01:38,359 --> 00:01:43,820
is composed of three main parts on the

00:01:41,509 --> 00:01:45,829
one hand on the left you have the term

00:01:43,820 --> 00:01:49,460
dictionary this term dictionary is

00:01:45,829 --> 00:01:51,890
composed of all the terms that have been

00:01:49,460 --> 00:01:53,689
found in your content sorted in

00:01:51,890 --> 00:01:56,350
alphabetical order so for instance that

00:01:53,689 --> 00:01:59,539
case we have elastic index Gaussian shot

00:01:56,350 --> 00:02:03,439
then to every term in his term

00:01:59,539 --> 00:02:06,109
dictionary we are going to welcome to

00:02:03,439 --> 00:02:08,690
map these terms to two things first a

00:02:06,109 --> 00:02:10,429
document frequency which is the normal

00:02:08,690 --> 00:02:13,190
the number of documents that contain

00:02:10,429 --> 00:02:16,250
this term and then pushing list on the

00:02:13,190 --> 00:02:18,890
right which is the list of document IDs

00:02:16,250 --> 00:02:21,230
in sorted order for all document that

00:02:18,890 --> 00:02:23,410
contain this term so for instance you

00:02:21,230 --> 00:02:26,430
can read this inverted index as

00:02:23,410 --> 00:02:28,640
Bluestein is contained in

00:02:26,430 --> 00:02:32,939
five documents and these five documents

00:02:28,640 --> 00:02:36,329
identified by de cádiz - 549 fifty and

00:02:32,939 --> 00:02:41,549
fifty - okay this is how an inverted

00:02:36,329 --> 00:02:43,530
index works and then there are two basic

00:02:41,549 --> 00:02:45,530
operations that you can apply an

00:02:43,530 --> 00:02:47,909
inadvertent Dex so disadvantage index

00:02:45,530 --> 00:02:50,189
supports iterators and you could create

00:02:47,909 --> 00:02:52,139
in Terry iterator halls on the terms

00:02:50,189 --> 00:02:54,450
dictionary and of the posting list and

00:02:52,139 --> 00:02:57,540
the first operation which is spotted on

00:02:54,450 --> 00:02:59,909
this traitor is called next and next is

00:02:57,540 --> 00:03:01,620
just about reading the next element so

00:02:59,909 --> 00:03:03,780
for instance if you cure create an

00:03:01,620 --> 00:03:06,060
iterator on the term dictionary first

00:03:03,780 --> 00:03:08,819
it's going to be and positioned then you

00:03:06,060 --> 00:03:10,439
connect is going to go to elastic then

00:03:08,819 --> 00:03:13,200
you connect again it's going to index

00:03:10,439 --> 00:03:15,090
etc and you can do exactly the same with

00:03:13,200 --> 00:03:16,799
posting lists when you create an

00:03:15,090 --> 00:03:19,200
iterator its first going to be on

00:03:16,799 --> 00:03:21,599
positioned then you connect for instance

00:03:19,200 --> 00:03:25,579
in the case of elastic is going to be

00:03:21,599 --> 00:03:27,780
positioned on 2010 and then on 49 and

00:03:25,579 --> 00:03:30,419
then there is a second operation which

00:03:27,780 --> 00:03:33,269
is spotted which we call advance advance

00:03:30,419 --> 00:03:35,609
is useful because it allows you to not

00:03:33,269 --> 00:03:37,500
consume all the terms which are your

00:03:35,609 --> 00:03:40,440
posting list or transitionary and to

00:03:37,500 --> 00:03:42,780
directly skip to the term a posting that

00:03:40,440 --> 00:03:45,569
you're interested in for instance in

00:03:42,780 --> 00:03:47,639
this term dictionary if you create an

00:03:45,569 --> 00:03:50,189
iterator and immediately call advanced

00:03:47,639 --> 00:03:52,349
on the term which is search it's going

00:03:50,189 --> 00:03:55,290
to jump to the first term which is

00:03:52,349 --> 00:03:57,599
greater than or equal to search in that

00:03:55,290 --> 00:03:59,729
case it would be shot okay

00:03:57,599 --> 00:04:02,370
and you can do exactly the same with

00:03:59,729 --> 00:04:05,159
posting list if you are positioned under

00:04:02,370 --> 00:04:07,650
kd2 and call advanced dock ad 30 is

00:04:05,159 --> 00:04:10,019
going to go to the further KD which is

00:04:07,650 --> 00:04:13,859
greater than or equal to 30 in that case

00:04:10,019 --> 00:04:16,739
it would default 9 the inverted index is

00:04:13,859 --> 00:04:18,419
almost completely stored on disk and the

00:04:16,739 --> 00:04:21,030
reason how we managed to make it

00:04:18,419 --> 00:04:23,669
efficient that the term dictionary is

00:04:21,030 --> 00:04:25,800
going to have very tiny in memory index

00:04:23,669 --> 00:04:27,840
which is going to contain prefixes of

00:04:25,800 --> 00:04:30,030
the term of the terms which is going to

00:04:27,840 --> 00:04:33,900
be used in order to do this skipping and

00:04:30,030 --> 00:04:36,150
for the posting list as part of the NDC

00:04:33,900 --> 00:04:40,190
encoding we store everything into blocks

00:04:36,150 --> 00:04:42,830
and in front of if it's block we storm

00:04:40,190 --> 00:04:45,860
escape list and the skip list allows you

00:04:42,830 --> 00:04:48,380
to know things like the first dock ID

00:04:45,860 --> 00:04:51,890
which is greater than 1000 is stored

00:04:48,380 --> 00:04:53,450
after a offset maybe 1 million and this

00:04:51,890 --> 00:04:57,230
is a way that we can manage to keep

00:04:53,450 --> 00:05:00,650
efficiently as posting lists so now that

00:04:57,230 --> 00:05:02,180
we know basics about inverted index we

00:05:00,650 --> 00:05:04,010
can start to understand how we can

00:05:02,180 --> 00:05:05,690
execute queries and the first way I'm

00:05:04,010 --> 00:05:08,210
going to talk about this to you an

00:05:05,690 --> 00:05:11,600
example is the term crane you want to

00:05:08,210 --> 00:05:12,770
search unsealer term for instance let's

00:05:11,600 --> 00:05:15,950
imagine that you want to search

00:05:12,770 --> 00:05:18,710
everything that contain the term which

00:05:15,950 --> 00:05:20,360
would be search you would call advanced

00:05:18,710 --> 00:05:23,450
in the term dictionary on search and

00:05:20,360 --> 00:05:25,220
then they are two k2 cases is there the

00:05:23,450 --> 00:05:26,840
term that you reach is the one that we

00:05:25,220 --> 00:05:28,940
are searching for which means that this

00:05:26,840 --> 00:05:30,590
term it exists it's not going to be the

00:05:28,940 --> 00:05:32,720
case for search so you know that this

00:05:30,590 --> 00:05:34,460
term doesn't match any document but on

00:05:32,720 --> 00:05:36,800
the other hand if you were calling

00:05:34,460 --> 00:05:38,930
advanced or losing for instance you

00:05:36,800 --> 00:05:40,910
would find a term this term dictionary

00:05:38,930 --> 00:05:42,920
and then in order to find all the

00:05:40,910 --> 00:05:44,780
matching documents everything that you

00:05:42,920 --> 00:05:47,030
would need to do would be to create an

00:05:44,780 --> 00:05:49,070
iterator on the posting list which is

00:05:47,030 --> 00:05:51,590
associated to listen and to call next

00:05:49,070 --> 00:05:53,750
and redock ad in order to find all the

00:05:51,590 --> 00:05:56,240
matches all documents that contain this

00:05:53,750 --> 00:05:58,520
term so this would be for a very simple

00:05:56,240 --> 00:06:02,660
term query now let's see what happens

00:05:58,520 --> 00:06:04,580
with conventions so we have three

00:06:02,660 --> 00:06:07,010
posting list in that case and we would

00:06:04,580 --> 00:06:09,410
like to figure out which documents are

00:06:07,010 --> 00:06:12,290
contained in all this posting list this

00:06:09,410 --> 00:06:14,060
is what we call conjunction and again we

00:06:12,290 --> 00:06:16,580
have a need to operation that we can

00:06:14,060 --> 00:06:18,320
execute one is next which is going to

00:06:16,580 --> 00:06:20,690
move to the next document in the posting

00:06:18,320 --> 00:06:23,810
list and the other one is advance which

00:06:20,690 --> 00:06:25,310
is going to help us keep of our document

00:06:23,810 --> 00:06:27,440
that we don't need in this posting list

00:06:25,310 --> 00:06:28,820
the first thing that we are going to do

00:06:27,440 --> 00:06:31,220
with this posting list is that we are

00:06:28,820 --> 00:06:33,919
going to solve them by by cost and in

00:06:31,220 --> 00:06:35,870
the case of posting list the cost is

00:06:33,919 --> 00:06:37,910
simply the document frequency which is

00:06:35,870 --> 00:06:41,900
the number of documents that match this

00:06:37,910 --> 00:06:44,030
term and then we are going to use the

00:06:41,900 --> 00:06:46,340
posting list which has the fewer

00:06:44,030 --> 00:06:48,590
documents as a lead for the iteration

00:06:46,340 --> 00:06:50,990
and we are only going to use user

00:06:48,590 --> 00:06:52,610
posting list as checks in order to make

00:06:50,990 --> 00:06:54,080
sure that document that we have found in

00:06:52,610 --> 00:06:56,479
the lead iterator can also

00:06:54,080 --> 00:06:59,210
we found in the other posting list so

00:06:56,479 --> 00:07:01,699
let's go through a concrete example we

00:06:59,210 --> 00:07:05,210
want to merge this posting list first we

00:07:01,699 --> 00:07:06,830
are going to advance the iterator of the

00:07:05,210 --> 00:07:10,099
first question is which is going to move

00:07:06,830 --> 00:07:12,080
to document 2 then we want to know if

00:07:10,099 --> 00:07:14,240
document 2 is also contained in the

00:07:12,080 --> 00:07:16,069
other person list so we are going to go

00:07:14,240 --> 00:07:19,370
to this other passing list and to call

00:07:16,069 --> 00:07:21,259
advance document - the first one doesn't

00:07:19,370 --> 00:07:23,479
have to come in - and as we said before

00:07:21,259 --> 00:07:25,069
advance is going to move to the first

00:07:23,479 --> 00:07:27,500
document which is greater than or equal

00:07:25,069 --> 00:07:30,319
to two so in that case it would move to

00:07:27,500 --> 00:07:33,349
searching so we know that the first

00:07:30,319 --> 00:07:36,409
match for the conjunction can actually

00:07:33,349 --> 00:07:38,300
not be less than 13 so we go back to a

00:07:36,409 --> 00:07:41,090
first iterator that we use as a leader

00:07:38,300 --> 00:07:43,879
and we tell that we tell it that it

00:07:41,090 --> 00:07:47,330
should advance to document 13 well Rocky

00:07:43,879 --> 00:07:49,129
13 is contained in this posting list so

00:07:47,330 --> 00:07:51,349
we cap we can keep going with the other

00:07:49,129 --> 00:07:52,580
person list we go back to the second we

00:07:51,349 --> 00:07:54,740
are going to notice that we are already

00:07:52,580 --> 00:07:57,800
on 13 and then we can go to the third

00:07:54,740 --> 00:08:02,029
one and advance to 13 and we have a

00:07:57,800 --> 00:08:03,740
match which checked all our posting list

00:08:02,029 --> 00:08:07,610
and the all contain 13 so we know that

00:08:03,740 --> 00:08:09,830
13 is going to be a match for for our

00:08:07,610 --> 00:08:12,199
conjunction and then we just need to

00:08:09,830 --> 00:08:14,449
keep this process going until we consume

00:08:12,199 --> 00:08:16,460
posting list entirely so we are going to

00:08:14,449 --> 00:08:19,520
connect on the first post in week again

00:08:16,460 --> 00:08:21,860
which gives us 17 as an advance on the

00:08:19,520 --> 00:08:23,960
second posting list which gives us 22 so

00:08:21,860 --> 00:08:25,819
we know that 17 is not going to be a

00:08:23,960 --> 00:08:29,539
match we need to go back to first

00:08:25,819 --> 00:08:32,959
posting list and advanced 22 and the

00:08:29,539 --> 00:08:34,490
next match is 98 then we again go back

00:08:32,959 --> 00:08:38,419
to second passing list it's also

00:08:34,490 --> 00:08:40,940
contains 98 as well as a salt one so we

00:08:38,419 --> 00:08:43,269
know that 98 is the second document

00:08:40,940 --> 00:08:46,940
which is going to be matched by our

00:08:43,269 --> 00:08:49,640
conjunction and when we call next again

00:08:46,940 --> 00:08:51,620
on the first posting list leucine is

00:08:49,640 --> 00:08:53,839
going to give us a dark ID which is

00:08:51,620 --> 00:08:55,640
maximum value which we use as place

00:08:53,839 --> 00:08:57,890
holder in order to know that the posting

00:08:55,640 --> 00:08:59,930
list has been exhausted and since since

00:08:57,890 --> 00:09:01,850
we don't have any more documents in this

00:08:59,930 --> 00:09:03,350
first person is we know that there can't

00:09:01,850 --> 00:09:05,529
be any more matches for the conjunction

00:09:03,350 --> 00:09:07,730
so this is a way that loosing runs

00:09:05,529 --> 00:09:13,070
consumption and we

00:09:07,730 --> 00:09:16,910
Curly's algorithms a leapfrog so I hope

00:09:13,070 --> 00:09:19,399
it was clear so now we are moving to a

00:09:16,910 --> 00:09:21,170
signal algorithm which is even more

00:09:19,399 --> 00:09:23,139
interesting in my opinion which is how

00:09:21,170 --> 00:09:27,889
does routine execute regular expression

00:09:23,139 --> 00:09:29,810
queries okay so we still have our

00:09:27,889 --> 00:09:32,329
inverted index this time I omitted the

00:09:29,810 --> 00:09:34,550
document frequencies we have terms on

00:09:32,329 --> 00:09:36,940
the left and postings on the right so

00:09:34,550 --> 00:09:41,209
again postings are the list of documents

00:09:36,940 --> 00:09:42,920
document IDs that contain this term so

00:09:41,209 --> 00:09:45,769
the challenge when it comes to running a

00:09:42,920 --> 00:09:47,750
Korean regular expression is to find all

00:09:45,769 --> 00:09:49,490
the terms that match this regular

00:09:47,750 --> 00:09:53,029
expression and to merge the posting list

00:09:49,490 --> 00:09:55,190
a knave way to execute such a career

00:09:53,029 --> 00:09:58,070
would be to go over every term in the

00:09:55,190 --> 00:10:00,230
term dictionary to evaluate direct the

00:09:58,070 --> 00:10:02,779
regular expression against it and then

00:10:00,230 --> 00:10:06,649
if it matches to merge the posting list

00:10:02,779 --> 00:10:09,470
together when issue with this approach

00:10:06,649 --> 00:10:11,540
is that is going to be very strong so

00:10:09,470 --> 00:10:14,360
when I said very slow it's a bit of an

00:10:11,540 --> 00:10:16,519
acceleration because if you compare it

00:10:14,360 --> 00:10:19,130
to the knave way of doing it which would

00:10:16,519 --> 00:10:21,290
be to run it on every term of every

00:10:19,130 --> 00:10:23,480
document the benefit of having an

00:10:21,290 --> 00:10:26,389
inverted index is that we are only going

00:10:23,480 --> 00:10:29,480
to evaluate the regular expression once

00:10:26,389 --> 00:10:31,430
per unit term okay so if you have one

00:10:29,480 --> 00:10:33,980
term that appears millions of time

00:10:31,430 --> 00:10:35,480
across your document collection we would

00:10:33,980 --> 00:10:38,269
only evaluate the regular expression

00:10:35,480 --> 00:10:40,399
once again its term but still it's very

00:10:38,269 --> 00:10:42,500
slow for the reason that we still need

00:10:40,399 --> 00:10:46,160
to evaluate the rigor expression against

00:10:42,500 --> 00:10:48,440
every term and it's very frequent to

00:10:46,160 --> 00:10:51,649
have millions or tens of millions of

00:10:48,440 --> 00:10:54,860
unique terms in an inverted index so can

00:10:51,649 --> 00:10:56,959
we do better for instance let's imagine

00:10:54,860 --> 00:10:58,459
that this is a regular expression that

00:10:56,959 --> 00:11:01,850
you would like to evaluate against your

00:10:58,459 --> 00:11:04,819
index it matches everything that starts

00:11:01,850 --> 00:11:07,430
with elastic with either an upper s

00:11:04,819 --> 00:11:09,230
or lowers and actually regular

00:11:07,430 --> 00:11:10,940
expressions internally when you want to

00:11:09,230 --> 00:11:13,040
evaluate them they are going to be

00:11:10,940 --> 00:11:15,290
translated into an automaton that can

00:11:13,040 --> 00:11:17,689
match this content for instance if you

00:11:15,290 --> 00:11:20,420
look at the beautiful we have a start

00:11:17,689 --> 00:11:21,410
date on the left and then we have some

00:11:20,420 --> 00:11:23,929
transitions

00:11:21,410 --> 00:11:25,759
you know that a term can only match this

00:11:23,929 --> 00:11:28,189
regular expression if you can be

00:11:25,759 --> 00:11:30,019
evaluated against this automaton so

00:11:28,189 --> 00:11:32,329
first you are going to read the first

00:11:30,019 --> 00:11:36,379
character if it is an upper e it matches

00:11:32,329 --> 00:11:37,669
then L a etc then come something

00:11:36,379 --> 00:11:39,769
interesting which is that you have two

00:11:37,669 --> 00:11:42,999
transitions at this stage where you can

00:11:39,769 --> 00:11:46,309
read either an upper s all over s and

00:11:42,999 --> 00:11:49,159
then you reach the final state which has

00:11:46,309 --> 00:11:51,559
a loop which means that as soon as you

00:11:49,159 --> 00:11:54,529
consume the elastic with an upper s or

00:11:51,559 --> 00:11:58,669
low s we accept any content and would

00:11:54,529 --> 00:12:00,529
still match the regular expression so

00:11:58,669 --> 00:12:04,669
when you look at this regular expression

00:12:00,529 --> 00:12:06,559
as human and still sing about the term

00:12:04,669 --> 00:12:10,279
dictionary that we saw on the previous

00:12:06,559 --> 00:12:11,689
slide you know that there is no purpose

00:12:10,279 --> 00:12:14,479
to evaluate this regular expression

00:12:11,689 --> 00:12:16,369
against every term because all the terms

00:12:14,479 --> 00:12:19,069
that we are interested in are actually

00:12:16,369 --> 00:12:21,319
going to start either with elastic with

00:12:19,069 --> 00:12:24,409
an upper s or with elastic with the low

00:12:21,319 --> 00:12:26,600
s okay and although other terms are not

00:12:24,409 --> 00:12:28,729
going to match anyway and so the way it

00:12:26,600 --> 00:12:31,639
works that Lucene has some very specific

00:12:28,729 --> 00:12:34,759
logic which allows it to intersect a

00:12:31,639 --> 00:12:36,799
term dictionary and an ultimatum if you

00:12:34,759 --> 00:12:39,109
look at this automaton actually it could

00:12:36,799 --> 00:12:41,720
be considered an infinite iterator of

00:12:39,109 --> 00:12:44,209
our set of string the first one which is

00:12:41,720 --> 00:12:46,939
going to match is elastic with an upper

00:12:44,209 --> 00:12:49,609
s then you are going to have also fixes

00:12:46,939 --> 00:12:51,799
of elastic with a number s then you are

00:12:49,609 --> 00:12:53,959
going to have elastic with a lower s and

00:12:51,799 --> 00:12:56,179
then you are going to have elastic sorry

00:12:53,959 --> 00:12:58,879
every term that starts with elastic with

00:12:56,179 --> 00:13:00,109
the low F so this return is interesting

00:12:58,879 --> 00:13:03,109
because it's something that you can also

00:13:00,109 --> 00:13:05,600
iterate on and so the logic that we just

00:13:03,109 --> 00:13:07,999
saw when it comes to merging personally

00:13:05,600 --> 00:13:10,249
posting lists using a leapfrog approach

00:13:07,999 --> 00:13:12,139
we are going to reuse this logic in

00:13:10,249 --> 00:13:14,569
order to intersect a term dictionary

00:13:12,139 --> 00:13:16,189
with an ultimatum and again it works

00:13:14,569 --> 00:13:18,919
because we support exactly the same

00:13:16,189 --> 00:13:21,499
operation next which allows us to go to

00:13:18,919 --> 00:13:23,359
the next term that is contain either in

00:13:21,499 --> 00:13:25,579
the term dictionary or which matches its

00:13:23,359 --> 00:13:28,909
automaton in advance which refers you to

00:13:25,579 --> 00:13:31,669
skip efficiently and so this regular

00:13:28,909 --> 00:13:34,129
expression that you would as human have

00:13:31,669 --> 00:13:35,270
a very legit efficiently by first going

00:13:34,129 --> 00:13:37,220
to elastic and

00:13:35,270 --> 00:13:39,320
everything that start with something

00:13:37,220 --> 00:13:41,330
different we are also going to be able

00:13:39,320 --> 00:13:44,420
to evaluate to run this query this way

00:13:41,330 --> 00:13:47,770
with Racine okay

00:13:44,420 --> 00:13:50,120
and when it becomes interesting is that

00:13:47,770 --> 00:13:53,450
ultimate turns are not something that

00:13:50,120 --> 00:13:56,000
you can only generate from regular

00:13:53,450 --> 00:13:57,770
expressions if you look at physicalism

00:13:56,000 --> 00:13:59,660
you can also generate ultimate sense

00:13:57,770 --> 00:14:01,790
from physique race and for instance if

00:13:59,660 --> 00:14:04,610
you look at this query which would find

00:14:01,790 --> 00:14:07,970
all document that contain the term which

00:14:04,610 --> 00:14:11,060
is add an added distance of 1 for term

00:14:07,970 --> 00:14:13,750
which is es this particular physique or

00:14:11,060 --> 00:14:15,920
e can be represented with this automaton

00:14:13,750 --> 00:14:16,220
you can have a look you can check if you

00:14:15,920 --> 00:14:18,890
want

00:14:16,220 --> 00:14:22,100
but this automaton is going to accept

00:14:18,890 --> 00:14:27,050
every term and just every term which is

00:14:22,100 --> 00:14:28,550
at a distance of 1 of s and this is the

00:14:27,050 --> 00:14:30,920
way that we see neurons fuzzy queries

00:14:28,550 --> 00:14:32,870
and so fuzzy grades actually very close

00:14:30,920 --> 00:14:34,339
to queries on regular expressions

00:14:32,870 --> 00:14:37,279
because they are actually evaluated the

00:14:34,339 --> 00:14:45,079
same way using animation as an

00:14:37,279 --> 00:14:48,410
intermediate representation ok so that's

00:14:45,079 --> 00:14:50,839
it for regular expressions now we are

00:14:48,410 --> 00:14:55,160
going to go to our third algorithm which

00:14:50,839 --> 00:14:56,750
is numeric values compression before I

00:14:55,160 --> 00:15:00,680
start I need to introduce what the

00:14:56,750 --> 00:15:02,390
values are might not be abuse and so if

00:15:00,680 --> 00:15:03,649
you are using elastic search let's

00:15:02,390 --> 00:15:05,480
imagine that you won't run an

00:15:03,649 --> 00:15:08,870
aggregation which is going to compute

00:15:05,480 --> 00:15:10,790
the average price of all your green

00:15:08,870 --> 00:15:12,860
documents what I mean with green

00:15:10,790 --> 00:15:16,040
documents is all documented match green

00:15:12,860 --> 00:15:18,100
as a term the way it works at first we

00:15:16,040 --> 00:15:21,440
are going to go to the inverting index

00:15:18,100 --> 00:15:24,200
in order to figure out which the KDS

00:15:21,440 --> 00:15:25,700
contain green as a term so for instance

00:15:24,200 --> 00:15:29,750
in that case we can see that documents

00:15:25,700 --> 00:15:32,390
one four and five match green and then

00:15:29,750 --> 00:15:34,070
the daka diese are going to be reusable

00:15:32,390 --> 00:15:36,410
in a different data structure that we

00:15:34,070 --> 00:15:39,110
called fill data elastic search fill

00:15:36,410 --> 00:15:42,709
data is nothing more than a column store

00:15:39,110 --> 00:15:44,600
that is built into your index and for

00:15:42,709 --> 00:15:47,570
instance in that case we are mapping

00:15:44,600 --> 00:15:49,370
every duck ID to the value of a price

00:15:47,570 --> 00:15:51,589
field and for instance we can see that

00:15:49,370 --> 00:15:53,839
I D 0 as 10 a surprised okay do you want

00:15:51,589 --> 00:15:55,250
us to enter the price it ran and then we

00:15:53,839 --> 00:15:57,889
are going to reuse this information

00:15:55,250 --> 00:15:59,680
which comes from the inverted index in

00:15:57,889 --> 00:16:02,120
order to read the values of the price

00:15:59,680 --> 00:16:05,839
for all the doc IDs that match Karina

00:16:02,120 --> 00:16:08,360
and we are going to sum up these prices

00:16:05,839 --> 00:16:10,069
divide it by the number of matches 3 in

00:16:08,360 --> 00:16:13,670
that case and this is going to give us

00:16:10,069 --> 00:16:16,250
the average price for all documents that

00:16:13,670 --> 00:16:18,680
contain green this this is exactly the

00:16:16,250 --> 00:16:22,699
way that elastic search would run an

00:16:18,680 --> 00:16:24,379
average aggregation on your data okay

00:16:22,699 --> 00:16:26,689
Sophie later is what we are interested

00:16:24,379 --> 00:16:28,430
in and I feel you have two options when

00:16:26,689 --> 00:16:30,709
it comes to storing fill data in less

00:16:28,430 --> 00:16:33,170
exchange the first one is a default one

00:16:30,709 --> 00:16:36,560
when this field data column no entered

00:16:33,170 --> 00:16:38,389
store is going to be computed on demand

00:16:36,560 --> 00:16:39,980
and loading into memory and I create

00:16:38,389 --> 00:16:44,509
something that would not fake match and

00:16:39,980 --> 00:16:45,949
this is a reason why as a 60.0 we are

00:16:44,509 --> 00:16:48,290
going to in for that it's going to be

00:16:45,949 --> 00:16:50,120
stored in the index and it's going to

00:16:48,290 --> 00:16:52,639
use a feature which is called dock

00:16:50,120 --> 00:16:54,529
values in missing dock values in Lucian

00:16:52,639 --> 00:16:57,170
are just about storing Phil later

00:16:54,529 --> 00:16:59,420
so first computing it at the next time

00:16:57,170 --> 00:17:01,759
and then storing it as part of the index

00:16:59,420 --> 00:17:03,800
which means that in your index you are

00:17:01,759 --> 00:17:06,350
going to have a column oriented store a

00:17:03,800 --> 00:17:08,630
common origin view of hadiza stored in

00:17:06,350 --> 00:17:10,669
the index and just a map that you read

00:17:08,630 --> 00:17:14,270
directly from disk so you can leverage

00:17:10,669 --> 00:17:17,870
as five same cache in the page in and

00:17:14,270 --> 00:17:21,589
out of out of the Faxon cash depending

00:17:17,870 --> 00:17:24,470
on how hard are called your data are and

00:17:21,589 --> 00:17:26,449
also the fact that it is computed I had

00:17:24,470 --> 00:17:28,030
indexing time is so interesting for us

00:17:26,449 --> 00:17:30,049
because it means that we have more

00:17:28,030 --> 00:17:31,429
opportunities for compression because we

00:17:30,049 --> 00:17:33,350
know that we are only going to do this

00:17:31,429 --> 00:17:35,450
operation once so we can potentially

00:17:33,350 --> 00:17:38,210
spend more time trying to compress data

00:17:35,450 --> 00:17:44,890
and we are going to see what kind of

00:17:38,210 --> 00:17:44,890
cool things we can do one sec

00:17:49,820 --> 00:17:57,890
so first not all data is really

00:17:55,020 --> 00:18:00,270
compressible and by the way by default

00:17:57,890 --> 00:18:03,120
we are going to use a default strategy

00:18:00,270 --> 00:18:05,250
which is very simple for instance let's

00:18:03,120 --> 00:18:08,820
imagine that we have the following

00:18:05,250 --> 00:18:11,340
values for Vedic ad by default we are

00:18:08,820 --> 00:18:14,160
only going to use simple Delta encoding

00:18:11,340 --> 00:18:15,660
a bit backing the way it works that we

00:18:14,160 --> 00:18:17,430
would go over all the values and compute

00:18:15,660 --> 00:18:20,880
the minimum value and the maximum value

00:18:17,430 --> 00:18:23,370
in that case it would be 2 for document

00:18:20,880 --> 00:18:25,380
1 2 and 9 for document 4 which would be

00:18:23,370 --> 00:18:31,890
the maximum value and the difference

00:18:25,380 --> 00:18:33,840
between Max and min would give us the

00:18:31,890 --> 00:18:36,630
maximum Delta in our dataset

00:18:33,840 --> 00:18:39,270
ok and this maximum Delta can be used in

00:18:36,630 --> 00:18:41,250
a lot to figure out how many bits we

00:18:39,270 --> 00:18:44,340
need to use per value in order to store

00:18:41,250 --> 00:18:47,040
our values it happens that 7 can be

00:18:44,340 --> 00:18:50,040
stored on only 3 bits which mean that

00:18:47,040 --> 00:18:52,170
all the values in our dog values can be

00:18:50,040 --> 00:18:54,750
stored on 3 bit and it's exactly what we

00:18:52,170 --> 00:18:56,790
are going to do first we are going to

00:18:54,750 --> 00:18:58,590
encode the minimum value for all our

00:18:56,790 --> 00:19:01,140
values in that case it would be 2 and

00:18:58,590 --> 00:19:03,360
then we are going to encode all the

00:19:01,140 --> 00:19:05,220
deltas with 2 so for instance for

00:19:03,360 --> 00:19:07,380
document 0 we are going to encode 5

00:19:05,220 --> 00:19:09,510
minus 2 which would be 3 for document

00:19:07,380 --> 00:19:11,970
one we would encode 2 minus 2 which

00:19:09,510 --> 00:19:14,400
would be 0 etc and all these values are

00:19:11,970 --> 00:19:17,610
between 0 and 7 and so they can be

00:19:14,400 --> 00:19:20,820
encoded on 3 on exactly 3 bits this is

00:19:17,610 --> 00:19:22,920
how we save space by default and then if

00:19:20,820 --> 00:19:24,420
you want to read a value so this needs

00:19:22,920 --> 00:19:26,490
to support random access ok

00:19:24,420 --> 00:19:28,770
and so if you want to read the value for

00:19:26,490 --> 00:19:32,820
document 2 for instance you would go to

00:19:28,770 --> 00:19:35,580
offset 2 times 3 which is 6 in bits then

00:19:32,820 --> 00:19:37,200
you would read exactly 3 bits add the

00:19:35,580 --> 00:19:38,850
minimum value and this would give you

00:19:37,200 --> 00:19:41,730
exactly the value that you provide it at

00:19:38,850 --> 00:19:45,080
next time this is a default strategy

00:19:41,730 --> 00:19:48,690
that we apply when it comes to storing

00:19:45,080 --> 00:19:51,990
numeric fields in the same but not as

00:19:48,690 --> 00:19:53,280
special cases and not all details I call

00:19:51,990 --> 00:19:57,240
and sometimes you can make better

00:19:53,280 --> 00:19:58,840
decisions for instance sometimes it

00:19:57,240 --> 00:20:02,080
might happen that

00:19:58,840 --> 00:20:03,340
you have a few values which are very

00:20:02,080 --> 00:20:07,060
different for instance let's imagine

00:20:03,340 --> 00:20:09,430
that you're stirring some kind of daruka

00:20:07,060 --> 00:20:11,410
liza and for everything you just you're

00:20:09,430 --> 00:20:13,810
stirring a zip code but you happen to

00:20:11,410 --> 00:20:16,660
only store zip codes about a couple of

00:20:13,810 --> 00:20:19,930
places so you have very different values

00:20:16,660 --> 00:20:21,580
but only maybe 10 or 20 unique ones in

00:20:19,930 --> 00:20:23,890
that case we're going to use something

00:20:21,580 --> 00:20:26,350
that we call table and coding in table

00:20:23,890 --> 00:20:28,360
encoding is just about using one level

00:20:26,350 --> 00:20:29,770
of indirection in your data so we are

00:20:28,360 --> 00:20:32,350
going to something which is very similar

00:20:29,770 --> 00:20:35,140
but instead of using the values as they

00:20:32,350 --> 00:20:36,940
are we are first going to build a table

00:20:35,140 --> 00:20:40,000
of unique values and this table can be

00:20:36,940 --> 00:20:42,280
built by just computing a set of all

00:20:40,000 --> 00:20:43,870
unique values and sorting them so for

00:20:42,280 --> 00:20:46,840
instance that case we have as unique

00:20:43,870 --> 00:20:50,980
values to for document 1 and 2 for for

00:20:46,840 --> 00:20:53,260
document 5 5 for document 0 & 3 & 9 for

00:20:50,980 --> 00:20:56,080
document for this would be our table of

00:20:53,260 --> 00:20:57,880
unique values and then for every

00:20:56,080 --> 00:21:00,220
document everything that we need to

00:20:57,880 --> 00:21:02,980
store is not the value itself but its

00:21:00,220 --> 00:21:05,350
index in the table of unique values and

00:21:02,980 --> 00:21:07,690
what is interesting is that as you can

00:21:05,350 --> 00:21:09,760
see we have exactly the same values a

00:21:07,690 --> 00:21:11,980
bit as before but we only have a couple

00:21:09,760 --> 00:21:14,200
of unique values we only have four of

00:21:11,980 --> 00:21:16,210
them which means that we don't need

00:21:14,200 --> 00:21:18,340
three bits anymore in order to store

00:21:16,210 --> 00:21:21,940
information per document we only need

00:21:18,340 --> 00:21:24,700
two bits because 0 1 2 & 3 which are the

00:21:21,940 --> 00:21:26,860
IDS in our table only need 2 bit to be

00:21:24,700 --> 00:21:28,840
stalled so compared to the previous

00:21:26,860 --> 00:21:32,290
strategy which is a default we would

00:21:28,840 --> 00:21:35,560
save 1 bit per value and this is what

00:21:32,290 --> 00:21:38,860
works when it comes to decoding it very

00:21:35,560 --> 00:21:40,390
similar you would go to offset D times

00:21:38,860 --> 00:21:42,610
to beat which is the number of bits per

00:21:40,390 --> 00:21:44,440
value that you are using then you would

00:21:42,610 --> 00:21:46,780
read two bits which would give you and

00:21:44,440 --> 00:21:48,550
your index in the table and then

00:21:46,780 --> 00:21:50,260
everything that you need to do is to

00:21:48,550 --> 00:21:52,890
read the value at this offset in your

00:21:50,260 --> 00:21:56,140
table this gives you the original value

00:21:52,890 --> 00:21:58,930
so this is a second tragedy that we use

00:21:56,140 --> 00:22:00,400
for numeric values and cunning and then

00:21:58,930 --> 00:22:03,250
I would like to talk about the third one

00:22:00,400 --> 00:22:05,260
which is even more interesting and it's

00:22:03,250 --> 00:22:07,320
typically something which is going to be

00:22:05,260 --> 00:22:09,130
useful when you stole time stamps

00:22:07,320 --> 00:22:11,260
illusion elasticsearch

00:22:09,130 --> 00:22:12,760
when issue is time stamps that is very

00:22:11,260 --> 00:22:15,100
common to store them using

00:22:12,760 --> 00:22:17,140
Michigan precision but yet from your

00:22:15,100 --> 00:22:18,610
application perspective you might not be

00:22:17,140 --> 00:22:21,490
interested in Michigan precision and

00:22:18,610 --> 00:22:25,120
maybe are only interested in second or

00:22:21,490 --> 00:22:27,190
daily precision which means that you

00:22:25,120 --> 00:22:28,480
have a lot of beads and at the end of

00:22:27,190 --> 00:22:30,940
your values which are going to be unique

00:22:28,480 --> 00:22:33,460
and used and to use space in your index

00:22:30,940 --> 00:22:35,530
for nursing and for this kind of data we

00:22:33,460 --> 00:22:37,780
are using something which is one tracing

00:22:35,530 --> 00:22:39,610
that we call DC decompression and the

00:22:37,780 --> 00:22:42,190
way it works that we are going to try to

00:22:39,610 --> 00:22:44,470
compute a common TVs or across all our

00:22:42,190 --> 00:22:46,660
values and to use it in order to convert

00:22:44,470 --> 00:22:49,870
them for instance let's assume that you

00:22:46,660 --> 00:22:51,040
have the following values the first

00:22:49,870 --> 00:22:53,320
thing we are going to do is to compute

00:22:51,040 --> 00:22:55,210
the minimum value in that case it is e

00:22:53,320 --> 00:22:58,390
the minimum value is reached for

00:22:55,210 --> 00:23:00,220
document 3 its value 1 then we are going

00:22:58,390 --> 00:23:02,140
to compute the deltas so we are going to

00:23:00,220 --> 00:23:05,380
subtract the minimum value to all the

00:23:02,140 --> 00:23:07,179
values that we have in the price field

00:23:05,380 --> 00:23:09,340
and so for instance it would give 10 for

00:23:07,179 --> 00:23:12,220
the command 0 30 for document 120 for

00:23:09,340 --> 00:23:14,740
the common two etc so then we have this

00:23:12,220 --> 00:23:16,720
list of deltas and we are going to try

00:23:14,740 --> 00:23:20,620
to see if we can find a common divisor

00:23:16,720 --> 00:23:22,600
across these values then two options is

00:23:20,620 --> 00:23:24,160
that the common divisor is 1 we don't

00:23:22,600 --> 00:23:25,450
have any actual common divisor and we

00:23:24,160 --> 00:23:28,419
are going to fall back to the first rata

00:23:25,450 --> 00:23:31,059
G that I was describing when we just use

00:23:28,419 --> 00:23:32,410
bit packing in all to encode values but

00:23:31,059 --> 00:23:34,480
then that can be something more

00:23:32,410 --> 00:23:36,850
interesting happening and for instance

00:23:34,480 --> 00:23:38,350
we can find a common divisor this would

00:23:36,850 --> 00:23:40,900
be typically the case if you store

00:23:38,350 --> 00:23:43,059
timestamps with second precision Lusine

00:23:40,900 --> 00:23:45,540
would detect a common Divis of 1000

00:23:43,059 --> 00:23:50,500
since one second is 1000 milliseconds

00:23:45,540 --> 00:23:52,330
okay and then in the header of your dock

00:23:50,500 --> 00:23:55,240
values routine is going to store first

00:23:52,330 --> 00:23:57,610
the minimum value then the common

00:23:55,240 --> 00:24:00,040
divisor and then instead of using the

00:23:57,610 --> 00:24:02,020
values directly it's going to only incur

00:24:00,040 --> 00:24:06,460
the questions so for instance for value

00:24:02,020 --> 00:24:10,120
0 the KD 0 it would be 1 then 3 2 0 7

00:24:06,460 --> 00:24:11,650
and 5 and then a decoding time we are

00:24:10,120 --> 00:24:16,030
going to use exactly the opposite

00:24:11,650 --> 00:24:18,610
approach and first for the KD D so these

00:24:16,030 --> 00:24:20,350
values are between 0 & 7 so they can be

00:24:18,610 --> 00:24:22,299
encoded on three bits per value we are

00:24:20,350 --> 00:24:24,970
going to use exactly what we did before

00:24:22,299 --> 00:24:26,530
and and code values and exactly the

00:24:24,970 --> 00:24:29,260
number of bits that we need

00:24:26,530 --> 00:24:31,570
then a decoding time for the kdd we just

00:24:29,260 --> 00:24:33,550
need to go to a set D times the number

00:24:31,570 --> 00:24:36,340
of ID's that we used at the next time 3

00:24:33,550 --> 00:24:38,440
in that case read 3 bits and this is

00:24:36,340 --> 00:24:40,450
going to give us a quotient then we can

00:24:38,440 --> 00:24:42,340
multiply it again with the GCD that we

00:24:40,450 --> 00:24:44,620
computed in the first phase add the

00:24:42,340 --> 00:24:46,180
minimum value and that's it we just

00:24:44,620 --> 00:24:50,080
recomputed the original value which has

00:24:46,180 --> 00:24:54,220
been provided at the next time so this

00:24:50,080 --> 00:24:58,420
is how loosin performs compression of

00:24:54,220 --> 00:25:01,510
numeric values and finally I'd like to

00:24:58,420 --> 00:25:03,640
talk about a force algorithm which is a

00:25:01,510 --> 00:25:05,080
bit more sophisticated and which is

00:25:03,640 --> 00:25:07,660
behind the way that the CalNet

00:25:05,080 --> 00:25:09,370
irrigation works so the cardia

00:25:07,660 --> 00:25:11,680
terra-tory is the cardinality

00:25:09,370 --> 00:25:14,680
aggregation elasticsearch is just about

00:25:11,680 --> 00:25:17,020
computing unique counts of distinct

00:25:14,680 --> 00:25:18,820
elements so if you run a sequel query it

00:25:17,020 --> 00:25:26,110
could be equivalent to running a count

00:25:18,820 --> 00:25:28,030
distinct on some field on a table and a

00:25:26,110 --> 00:25:30,910
nave approach to run this aggregation

00:25:28,030 --> 00:25:33,520
would be to use a set a hash set for

00:25:30,910 --> 00:25:36,220
instance to put all the values in there

00:25:33,520 --> 00:25:38,560
the set by its nature is going to

00:25:36,220 --> 00:25:40,360
duplicate all the values and then you'll

00:25:38,560 --> 00:25:42,100
figure out how many unique values you

00:25:40,360 --> 00:25:45,910
have you just need to compute the size

00:25:42,100 --> 00:25:49,570
of this set this works and this even

00:25:45,910 --> 00:25:50,800
gives you an accurate answer but one

00:25:49,570 --> 00:25:52,410
first issue that you are going to

00:25:50,800 --> 00:25:55,420
encounter is that if you want to count

00:25:52,410 --> 00:25:56,950
tens of millions of unique values it

00:25:55,420 --> 00:25:58,870
might require a lot of memory for

00:25:56,950 --> 00:26:00,580
instance imagine that you have hundreds

00:25:58,870 --> 00:26:04,480
of millions of unique values and each

00:26:00,580 --> 00:26:05,940
value requires 10 bytes this is going to

00:26:04,480 --> 00:26:10,570
require more than one gigabyte of memory

00:26:05,940 --> 00:26:13,180
ok which is a lot and this is ignoring

00:26:10,570 --> 00:26:14,770
the overhead of the set data structure

00:26:13,180 --> 00:26:17,980
that we use in order to store these

00:26:14,770 --> 00:26:19,480
unique values and even worse if you are

00:26:17,980 --> 00:26:22,180
working in a distributed environment

00:26:19,480 --> 00:26:25,420
like elastic sub - it's even worse

00:26:22,180 --> 00:26:27,760
because all these sets of unique values

00:26:25,420 --> 00:26:30,220
need to be propagated propagating

00:26:27,760 --> 00:26:31,870
propagated three to a single node which

00:26:30,220 --> 00:26:33,880
is going to merge them in order to know

00:26:31,870 --> 00:26:35,830
how many unique values you have not

00:26:33,880 --> 00:26:38,530
prasada but for your entire debt data

00:26:35,830 --> 00:26:40,410
set for your entire collection and so it

00:26:38,530 --> 00:26:45,610
makes the problem even worse

00:26:40,410 --> 00:26:48,180
so what can we do so the way that let's

00:26:45,610 --> 00:26:50,710
search service this issue is by using

00:26:48,180 --> 00:26:52,240
and it's really an interesting algorithm

00:26:50,710 --> 00:26:54,880
which is called hyper log log plus plus

00:26:52,240 --> 00:26:56,260
which is an improvement of another

00:26:54,880 --> 00:26:57,580
algorithm which is called a hyper log

00:26:56,260 --> 00:26:58,650
log and a hyper log log is what I'm

00:26:57,580 --> 00:27:04,600
going to talk right now

00:26:58,650 --> 00:27:06,760
hyper log log is about using information

00:27:04,600 --> 00:27:08,620
is about using her she's in a lot of

00:27:06,760 --> 00:27:10,180
computing accounts and it does it in a

00:27:08,620 --> 00:27:12,040
very smart way which I'm going to

00:27:10,180 --> 00:27:15,280
describe so why is it useful

00:27:12,040 --> 00:27:17,500
it's useful because hyper log log only

00:27:15,280 --> 00:27:18,910
uses a few kilobytes of memory even if

00:27:17,500 --> 00:27:21,730
you want to evaluate cardinalities of

00:27:18,910 --> 00:27:23,650
billions of elements it's fast and you

00:27:21,730 --> 00:27:25,030
can perform lossless engines which is

00:27:23,650 --> 00:27:26,950
very important in the case of a

00:27:25,030 --> 00:27:29,080
distributed system which means that you

00:27:26,950 --> 00:27:31,960
would get exactly the same result if you

00:27:29,080 --> 00:27:34,810
ran a hyper log log on a single shard

00:27:31,960 --> 00:27:38,170
that holds all your data or on 10 shard

00:27:34,810 --> 00:27:39,970
that each hold one-tenth of the data you

00:27:38,170 --> 00:27:42,190
would get exactly the same result there

00:27:39,970 --> 00:27:44,170
is no I crazy loves when you run this

00:27:42,190 --> 00:27:46,630
algorithm in the distributed fashion

00:27:44,170 --> 00:27:52,630
which is one of the major arguments were

00:27:46,630 --> 00:27:54,970
why we decided to use it so before

00:27:52,630 --> 00:27:56,890
talking about how it works let's talk

00:27:54,970 --> 00:28:01,360
about flipping coins in order to give

00:27:56,890 --> 00:28:03,400
you an ID height works let's imagine

00:28:01,360 --> 00:28:07,030
that you are flipping a coin okay and

00:28:03,400 --> 00:28:08,410
your goal is to make a run of head which

00:28:07,030 --> 00:28:11,230
is as long as possible

00:28:08,410 --> 00:28:17,290
okay the probability of going your run

00:28:11,230 --> 00:28:19,630
of n heads in a row is 1/2 for n okay

00:28:17,290 --> 00:28:22,540
which means that if you want to get 5

00:28:19,630 --> 00:28:26,500
head in a row the likelihood of reaching

00:28:22,540 --> 00:28:30,190
that state is 1 out of 32 and if you

00:28:26,500 --> 00:28:33,160
want to get 20 head in a row the

00:28:30,190 --> 00:28:36,280
probability to get to 20 head in a row

00:28:33,160 --> 00:28:38,470
is 1 out of 1 million why is it

00:28:36,280 --> 00:28:41,560
interesting it's interesting because if

00:28:38,470 --> 00:28:42,970
someone goes to a room tries to do how

00:28:41,560 --> 00:28:45,130
many heads in a row is possible and then

00:28:42,970 --> 00:28:47,560
comes back to you and tells you how many

00:28:45,130 --> 00:28:48,910
runs it managed to do you can use this

00:28:47,560 --> 00:28:50,830
information in order to know how much

00:28:48,910 --> 00:28:52,840
time is spent flipping the coin for

00:28:50,830 --> 00:28:53,810
instance if someone comes back to me and

00:28:52,840 --> 00:28:56,360
tells me

00:28:53,810 --> 00:28:58,610
I managed to only do three heads in a

00:28:56,360 --> 00:29:00,560
row it probably means that it didn't try

00:28:58,610 --> 00:29:02,660
for a very long time on the other hand

00:29:00,560 --> 00:29:06,200
if someone comes back to me and tells me

00:29:02,660 --> 00:29:10,040
hey I managed to run to do a run for

00:29:06,200 --> 00:29:11,930
instant of 15 head in a row it probably

00:29:10,040 --> 00:29:14,620
means that you spent the day flipping

00:29:11,930 --> 00:29:17,060
the coins in order to reach this result

00:29:14,620 --> 00:29:19,640
which is very important because it means

00:29:17,060 --> 00:29:23,120
that given the number of heads that you

00:29:19,640 --> 00:29:28,780
can do in a row you can almost know how

00:29:23,120 --> 00:29:32,270
long someone has spent flipping the coin

00:29:28,780 --> 00:29:35,300
so this is exactly the keen side of this

00:29:32,270 --> 00:29:38,450
experiment to power the length of the

00:29:35,300 --> 00:29:41,330
round is proportional to the duration of

00:29:38,450 --> 00:29:43,840
the coin flipping and we can reuse this

00:29:41,330 --> 00:29:45,590
information a lot to count unique values

00:29:43,840 --> 00:29:47,810
how does it work

00:29:45,590 --> 00:29:50,690
let's imagine that we have a hash

00:29:47,810 --> 00:29:53,420
function which for any value is going to

00:29:50,690 --> 00:29:56,090
give us a hash on 64 bits with a very

00:29:53,420 --> 00:29:58,400
important property in the binary

00:29:56,090 --> 00:30:01,580
representation of this hash once and

00:29:58,400 --> 00:30:06,890
DeRosa are as likely thank you

00:30:01,580 --> 00:30:08,150
1 & 0 are both exactly this is actually

00:30:06,890 --> 00:30:10,490
something which is not very hard to

00:30:08,150 --> 00:30:12,590
treat and you don't need to use a

00:30:10,490 --> 00:30:14,120
cryptographic hash function something as

00:30:12,590 --> 00:30:15,770
simple as member 3 for instance would

00:30:14,120 --> 00:30:17,780
work in memory's actually what we are

00:30:15,770 --> 00:30:19,700
using elasticsearch and then you have

00:30:17,780 --> 00:30:22,010
this hash in binary representation and

00:30:19,700 --> 00:30:24,590
you are going to count how many zeros

00:30:22,010 --> 00:30:26,660
you have in the end for instant here we

00:30:24,590 --> 00:30:28,970
have 1 0 so we are going to set the

00:30:26,660 --> 00:30:30,620
value of the register to 1 then we have

00:30:28,970 --> 00:30:34,580
3 zeros we are going to update the value

00:30:30,620 --> 00:30:36,590
of the register to 1 3 to 3 and then we

00:30:34,580 --> 00:30:38,810
only have 2 zeros and we are not going

00:30:36,590 --> 00:30:41,000
to the register the register is going to

00:30:38,810 --> 00:30:43,420
store the maximum number of zeros that

00:30:41,000 --> 00:30:46,700
we can see at the end of a hash value

00:30:43,420 --> 00:30:49,580
and actually the number of zeros that

00:30:46,700 --> 00:30:51,110
you can see at the end of a hash is very

00:30:49,580 --> 00:30:53,330
comparable to the number of head that

00:30:51,110 --> 00:30:57,200
you can do in a row and so we are going

00:30:53,330 --> 00:30:58,940
to reuse this inside that we have for

00:30:57,200 --> 00:31:02,060
con flipping to Kearney artist

00:30:58,940 --> 00:31:03,560
estimation and to power the length of

00:31:02,060 --> 00:31:05,480
the run so the number of zeros that you

00:31:03,560 --> 00:31:06,880
can count the maximum number of zeros

00:31:05,480 --> 00:31:09,330
that you can have in the end

00:31:06,880 --> 00:31:14,470
is proportional to cattle ADT of a field

00:31:09,330 --> 00:31:14,800
okay but now this approach has an issue

00:31:14,470 --> 00:31:16,660
okay

00:31:14,800 --> 00:31:19,780
so first example let's imagine that

00:31:16,660 --> 00:31:22,690
someone runs has a set of value computes

00:31:19,780 --> 00:31:25,390
hashes on every value and said that the

00:31:22,690 --> 00:31:26,650
maximum number of zeros that it could

00:31:25,390 --> 00:31:29,080
find was 32

00:31:26,650 --> 00:31:31,000
sorry it was five it probably means that

00:31:29,080 --> 00:31:34,030
the number of unique values was about to

00:31:31,000 --> 00:31:36,070
power 5 which is 32 and this is how high

00:31:34,030 --> 00:31:38,380
power log log works but not even there

00:31:36,070 --> 00:31:40,150
is an issue because it's not that

00:31:38,380 --> 00:31:42,580
unlikely that even if you have a single

00:31:40,150 --> 00:31:45,040
value you can get unlucky and get a hash

00:31:42,580 --> 00:31:47,530
value which in that case 10 zeros in the

00:31:45,040 --> 00:31:50,140
end and this case is bad because I only

00:31:47,530 --> 00:31:54,390
had one value as an input but yet the

00:31:50,140 --> 00:31:57,220
estimator computed to power 10 which is

00:31:54,390 --> 00:32:00,070
1024 as a cardinality which is totally

00:31:57,220 --> 00:32:01,900
wrong and there are nice tricks that you

00:32:00,070 --> 00:32:04,210
can use in order to improve accuracy of

00:32:01,900 --> 00:32:06,310
this hyper log logarithm and this

00:32:04,210 --> 00:32:07,960
solution is to keep multiple controls so

00:32:06,310 --> 00:32:10,210
for instance will we still have this bad

00:32:07,960 --> 00:32:12,240
value okay we were still only collecting

00:32:10,210 --> 00:32:14,950
one value but counting 10 zeros and

00:32:12,240 --> 00:32:16,420
instead of using only one counter we are

00:32:14,950 --> 00:32:18,160
going to use several ones and the

00:32:16,420 --> 00:32:20,820
contact use is going to be computed

00:32:18,160 --> 00:32:23,470
depending on the first bits of the hash

00:32:20,820 --> 00:32:25,750
in that case we decided to use four

00:32:23,470 --> 00:32:27,580
counters so we only need to have a look

00:32:25,750 --> 00:32:30,940
at the first two bit of the hash in

00:32:27,580 --> 00:32:32,320
order to decide in which register the

00:32:30,940 --> 00:32:33,940
number of zeros that we can compute in

00:32:32,320 --> 00:32:36,430
the end should go for instance in that

00:32:33,940 --> 00:32:39,400
case the last bits sorry the first bits

00:32:36,430 --> 00:32:41,440
are 0 0 which means that the register

00:32:39,400 --> 00:32:43,660
which would be ablated is register 0

00:32:41,440 --> 00:32:45,220
then we have another value this one for

00:32:43,660 --> 00:32:48,160
instance which is hacked at this value

00:32:45,220 --> 00:32:49,690
at the two first bits are 1 and 0 which

00:32:48,160 --> 00:32:53,650
means that the register that should be

00:32:49,690 --> 00:32:55,810
updated it register 2 given that 1 0 is

00:32:53,650 --> 00:32:57,610
the representation of 2 in binary form

00:32:55,810 --> 00:32:59,830
so we are going to update we just

00:32:57,610 --> 00:33:01,990
register 1 etc which means that

00:32:59,830 --> 00:33:04,420
essentially we are going to split a

00:33:01,990 --> 00:33:05,710
stream of value into different counters

00:33:04,420 --> 00:33:09,250
and we are going to get different

00:33:05,710 --> 00:33:12,220
estimates then so for instance we reach

00:33:09,250 --> 00:33:14,650
this for count this four registers 10 2

00:33:12,220 --> 00:33:18,040
3 in 1 and we have send the first one

00:33:14,650 --> 00:33:19,840
because again we gotten lucky and fell

00:33:18,040 --> 00:33:21,010
upon a heart value which just happened

00:33:19,840 --> 00:33:23,200
to have lot of zeros in

00:33:21,010 --> 00:33:26,770
even if we only collected one one or two

00:33:23,200 --> 00:33:29,470
of them so each register is going to

00:33:26,770 --> 00:33:32,710
give us a calculate e so the first one

00:33:29,470 --> 00:33:35,049
is going to give us 1024 so second one's

00:33:32,710 --> 00:33:38,409
been to you for third one eight and four

00:33:35,049 --> 00:33:40,480
one two and as we said we had an

00:33:38,409 --> 00:33:43,149
independent stream of values going to

00:33:40,480 --> 00:33:45,490
each read register so the naive approach

00:33:43,149 --> 00:33:48,940
would be to get overall cardinality to

00:33:45,490 --> 00:33:52,299
sum up the calculate is for each

00:33:48,940 --> 00:33:53,890
register which gives us 1038 but it

00:33:52,299 --> 00:33:56,409
doesn't help with the issue that we had

00:33:53,890 --> 00:33:58,840
which is that we just get unlucky on the

00:33:56,409 --> 00:34:00,760
first value so what hypergolic does

00:33:58,840 --> 00:34:03,580
instead is that is going to apply a

00:34:00,760 --> 00:34:05,289
mnemonic mean on all the cardinalities

00:34:03,580 --> 00:34:07,809
which have been computed and to multiply

00:34:05,289 --> 00:34:10,300
it by the number of registers this

00:34:07,809 --> 00:34:12,399
approach is interesting because the

00:34:10,300 --> 00:34:15,099
harmonic mean has the property of giving

00:34:12,399 --> 00:34:17,619
less weight to outliers and for instance

00:34:15,099 --> 00:34:20,080
in that case we are multiplying 4 which

00:34:17,619 --> 00:34:22,510
is a number of registers times harmonic

00:34:20,080 --> 00:34:26,379
mean which is computed based on this

00:34:22,510 --> 00:34:28,619
formula and using this approach we

00:34:26,379 --> 00:34:31,270
compute another whole cardinality of

00:34:28,619 --> 00:34:34,060
18.3 which is interesting because it

00:34:31,270 --> 00:34:35,950
almost means that we disregarded the

00:34:34,060 --> 00:34:38,320
value which was given by the first

00:34:35,950 --> 00:34:41,080
counter again that we only got because

00:34:38,320 --> 00:34:44,740
we were unlucky and this is how we can

00:34:41,080 --> 00:34:48,190
improve the accuracy of the epilogue

00:34:44,740 --> 00:34:50,109
lock algorithm there are other nice

00:34:48,190 --> 00:34:53,619
tribute of these algorithms like we said

00:34:50,109 --> 00:34:57,040
it needs very little memory in all

00:34:53,619 --> 00:34:59,589
Trenholm and for instance each register

00:34:57,040 --> 00:35:02,619
is only going to store a number of bits

00:34:59,589 --> 00:35:04,030
in a longer and there are only 54 bits

00:35:02,619 --> 00:35:06,310
in a long which means that you can store

00:35:04,030 --> 00:35:10,020
a register only 6 bits

00:35:06,310 --> 00:35:13,210
5 if you use hashes on such two beats

00:35:10,020 --> 00:35:16,270
another important feature is that unions

00:35:13,210 --> 00:35:18,099
are lossless like I said as an

00:35:16,270 --> 00:35:21,099
introduction to this algorithm you can

00:35:18,099 --> 00:35:23,560
run the same algorithm on one shot that

00:35:21,099 --> 00:35:24,820
holds all your data on on 10 shot and

00:35:23,560 --> 00:35:27,220
then merging the results and you're

00:35:24,820 --> 00:35:29,290
going to give exactly the same value and

00:35:27,220 --> 00:35:33,490
the reason is that the reason for that

00:35:29,290 --> 00:35:34,660
is that the maximum of two set is a

00:35:33,490 --> 00:35:37,599
maximum of the maximum

00:35:34,660 --> 00:35:39,760
sets so when you compute a union of two

00:35:37,599 --> 00:35:42,220
hyper log-log instances you don't have

00:35:39,760 --> 00:35:44,619
to take the maximum of the more bits

00:35:42,220 --> 00:35:46,180
that you could count in gender and it's

00:35:44,619 --> 00:35:48,160
going to give you the number of bits

00:35:46,180 --> 00:35:50,859
that could be counted on your overall

00:35:48,160 --> 00:35:53,079
data set okay and this is a way that you

00:35:50,859 --> 00:35:54,430
can merge hyper log-log instances that

00:35:53,079 --> 00:35:57,160
have been built on different charts

00:35:54,430 --> 00:36:01,270
together and going to give you an answer

00:35:57,160 --> 00:36:02,950
for your whole data set and this is

00:36:01,270 --> 00:36:04,530
perfect for this ribbet environments

00:36:02,950 --> 00:36:07,839
such as elasticsearch

00:36:04,530 --> 00:36:09,660
so this was just about hyper log log so

00:36:07,839 --> 00:36:12,099
just for your information

00:36:09,660 --> 00:36:14,049
yeah algorithm at elasticsearch is based

00:36:12,099 --> 00:36:15,819
on it's a critical hyper log log press

00:36:14,049 --> 00:36:19,240
press which is an improvement of a Bella

00:36:15,819 --> 00:36:20,920
glug we try to correct even more bias of

00:36:19,240 --> 00:36:22,119
hyper log log and to improve accuracy

00:36:20,920 --> 00:36:25,839
especially when you have small

00:36:22,119 --> 00:36:27,549
communities and if you'd like to hear

00:36:25,839 --> 00:36:29,920
more about it there are papers which

00:36:27,549 --> 00:36:32,410
have been published and that's

00:36:29,920 --> 00:36:34,390
everything I wanted to say today before

00:36:32,410 --> 00:36:36,549
before I stop I just want to say that if

00:36:34,390 --> 00:36:38,890
you found his talk interesting I took a

00:36:36,549 --> 00:36:40,599
bit about compression and ran out who is

00:36:38,890 --> 00:36:43,240
here is going to have a deeper talk

00:36:40,599 --> 00:36:45,910
about compression loosing tomorrow

00:36:43,240 --> 00:36:47,799
afternoon and I talked about hyper log

00:36:45,910 --> 00:36:49,539
log and there is actually an algorithm

00:36:47,799 --> 00:36:51,190
which can be used for computing

00:36:49,539 --> 00:36:53,289
percentiles which has very similar

00:36:51,190 --> 00:36:55,329
features which is called to digest and

00:36:53,289 --> 00:36:56,740
there is going to be a talk about today

00:36:55,329 --> 00:36:58,240
just this afternoon by deadening

00:36:56,740 --> 00:37:01,000
so if you found this interesting I would

00:36:58,240 --> 00:37:04,410
recommend you to attend this talks so

00:37:01,000 --> 00:37:04,410
that's it for me thank you very much

00:37:07,920 --> 00:37:09,980

YouTube URL: https://www.youtube.com/watch?v=eQ-rXP-D80U


