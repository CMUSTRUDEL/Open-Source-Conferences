Title: Berlin Buzzwords 2015: Andrew Psaltis - Going deep with Spark Streaming #bbuzz
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Today if a byte of data were a gallon of water, in only 10 seconds there would be enough data to fill an average home, in 2020 it will only take 2 seconds. The Internet of Things is driving a tremendous amount of this growth, providing more data at a higher rate then weâ€™ve ever seen. With this explosive growth comes the demand from consumers and businesses to leverage and act on what is happening right now. 

Without stream processing these demands will never be met, and there will be no big data and no Internet of Things. Apache Spark, and Spark Streaming in particular can be used to fulfill this stream processing need now and in the future. 

In this talk I will peel back the covers and we will take a deep dive into the inner workings of Spark Streaming; discussing topics such as DStreams, input and output operations, transformations, and fault tolerance.  After this talk you will be ready to take on the world of stream processing using Apache Spark.

Read more:
https://2015.berlinbuzzwords.de/session/going-deep-spark-streaming

About Andrew Psaltis:
https://2015.berlinbuzzwords.de/users/andrew-psaltis

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,930 --> 00:00:12,010
hi my name is Andrew selfish thoughts

00:00:09,970 --> 00:00:15,120
going to be about going deep with spark

00:00:12,010 --> 00:00:17,100
streaming it's kind of an outline of

00:00:15,120 --> 00:00:18,539
this talk got their introduction talk

00:00:17,100 --> 00:00:20,910
about d streams which is a fundamental

00:00:18,539 --> 00:00:23,400
component of spark streaming talk about

00:00:20,910 --> 00:00:24,540
how we think about time and streaming go

00:00:23,400 --> 00:00:26,099
over a little about recovery of all

00:00:24,540 --> 00:00:29,400
tolerance and then conclude and have

00:00:26,099 --> 00:00:31,910
questions quick little bit about myself

00:00:29,400 --> 00:00:34,140
I'm a data engineering shutterstock

00:00:31,910 --> 00:00:36,360
outsider shutterstock sometimes ramble

00:00:34,140 --> 00:00:39,149
on Twitter not often author of streaming

00:00:36,360 --> 00:00:40,200
data I started dreaming about streaming

00:00:39,149 --> 00:00:43,079
since about two thousand eight and

00:00:40,200 --> 00:00:45,059
working with streaming systems speaking

00:00:43,079 --> 00:00:46,710
provide content for skillsoft and

00:00:45,059 --> 00:00:54,600
outside of that I live in la crosse

00:00:46,710 --> 00:00:57,449
crazed family so why streaming this is a

00:00:54,600 --> 00:00:59,339
great quote from a Danice and do that

00:00:57,449 --> 00:01:01,649
without stream processing there's no big

00:00:59,339 --> 00:01:03,870
data and no internet of things it's

00:01:01,649 --> 00:01:07,080
really kind of interesting that this is

00:01:03,870 --> 00:01:10,110
a little device from Texas Instruments

00:01:07,080 --> 00:01:13,229
call it smart tag it's got ten different

00:01:10,110 --> 00:01:16,020
sensors so everything from Mike

00:01:13,229 --> 00:01:19,350
temperature accelerometer all sorts of

00:01:16,020 --> 00:01:21,630
movement so ten of them in here $29 29

00:01:19,350 --> 00:01:26,190
US dollars for this little device a

00:01:21,630 --> 00:01:29,790
little chip if you will if you haven't

00:01:26,190 --> 00:01:31,890
art enough cases this last day and a

00:01:29,790 --> 00:01:35,910
half as to why streaming here's a couple

00:01:31,890 --> 00:01:37,890
more operational efficiency one extra

00:01:35,910 --> 00:01:40,380
mile an hour on a locomotive on its

00:01:37,890 --> 00:01:42,240
daily route can save 200 million dollars

00:01:40,380 --> 00:01:44,040
in US dollars and that's when Norfolk

00:01:42,240 --> 00:01:45,270
Southern and it's not that they can't

00:01:44,040 --> 00:01:47,220
get the locomotives to go fast enough

00:01:45,270 --> 00:01:51,120
it's the logistics of getting them in of

00:01:47,220 --> 00:01:54,390
the yard and routing them from the EU

00:01:51,120 --> 00:01:57,240
Commission bit congestion accounts for

00:01:54,390 --> 00:01:59,970
about a hundred billion euros or one

00:01:57,240 --> 00:02:01,800
percent of EU GTP annually so improving

00:01:59,970 --> 00:02:04,860
these things with streaming systems

00:02:01,800 --> 00:02:09,569
becomes not just advertising but real

00:02:04,860 --> 00:02:12,360
money so I think we have a shared

00:02:09,569 --> 00:02:14,340
problem here either today of a byte of

00:02:12,360 --> 00:02:18,540
data was a gallon of water here we could

00:02:14,340 --> 00:02:21,709
fill an average house in 10 seconds by

00:02:18,540 --> 00:02:24,060
about twenty twenty it'll take only two

00:02:21,709 --> 00:02:26,969
so the goal of this talk is to really

00:02:24,060 --> 00:02:28,680
kind of go through how perhaps heart

00:02:26,969 --> 00:02:29,250
streaming could help us solve this

00:02:28,680 --> 00:02:31,050
problem

00:02:29,250 --> 00:02:33,150
and this is only going to continue to

00:02:31,050 --> 00:02:35,040
grow and it's not the data that we're

00:02:33,150 --> 00:02:37,290
producing browsing the web or doing

00:02:35,040 --> 00:02:41,120
social things but stuff like this thats

00:02:37,290 --> 00:02:41,120
everywhere and becoming more pervasive

00:02:43,130 --> 00:02:46,920
so if you haven't used spark stream it

00:02:45,150 --> 00:02:50,310
just kind of quick little background on

00:02:46,920 --> 00:02:52,740
it so the typical stack you see with

00:02:50,310 --> 00:02:55,740
spark spark streaming just being one

00:02:52,740 --> 00:02:58,160
component of it provides an efficient

00:02:55,740 --> 00:03:02,070
fault tolerant staple stream processing

00:02:58,160 --> 00:03:03,989
provides a simple API integrates with

00:03:02,070 --> 00:03:07,140
sparks batch processing and the other

00:03:03,989 --> 00:03:09,239
libraries okay so you could use mlib

00:03:07,140 --> 00:03:11,040
could use graphics you could use sparks

00:03:09,239 --> 00:03:14,160
equal along with the spark streaming

00:03:11,040 --> 00:03:18,540
today we're just going to focus on spark

00:03:14,160 --> 00:03:22,260
streaming is a high level architecture

00:03:18,540 --> 00:03:23,670
of how it all works so in the far left

00:03:22,260 --> 00:03:25,980
you have your program that's running

00:03:23,670 --> 00:03:29,700
it's a driver no different than a

00:03:25,980 --> 00:03:31,920
regular SPARC program it gets sent out

00:03:29,700 --> 00:03:33,540
to the workers and that's where your

00:03:31,920 --> 00:03:35,519
algorithm runs and your sports streaming

00:03:33,540 --> 00:03:38,280
job is running and then you have data

00:03:35,519 --> 00:03:40,250
sources that are coming as input okay

00:03:38,280 --> 00:03:44,640
those could be anything from Twitter a

00:03:40,250 --> 00:03:50,790
tag a device network whatever it is you

00:03:44,640 --> 00:03:53,310
want to consume data from saudi streams

00:03:50,790 --> 00:03:55,459
discretized streams are the basic

00:03:53,310 --> 00:03:58,440
abstraction provided by spark streaming

00:03:55,459 --> 00:04:01,170
in the way it pretty much works we look

00:03:58,440 --> 00:04:03,690
at it is if this is time going across it

00:04:01,170 --> 00:04:05,820
really is just a little micro batches of

00:04:03,690 --> 00:04:13,350
our DD is being generated as time is

00:04:05,820 --> 00:04:14,970
progressing okay as three things that we

00:04:13,350 --> 00:04:16,260
want to do with them and this is really

00:04:14,970 --> 00:04:17,760
similar to the three things we're going

00:04:16,260 --> 00:04:18,870
to do with almost any data processing

00:04:17,760 --> 00:04:20,940
system right we want to be able to

00:04:18,870 --> 00:04:22,440
ingest data we want to do some sort of

00:04:20,940 --> 00:04:23,760
transformation or something on it and

00:04:22,440 --> 00:04:32,010
then we're going to want to output it

00:04:23,760 --> 00:04:34,800
somewhere to something ingestion of d

00:04:32,010 --> 00:04:37,800
streams there's three ways you could get

00:04:34,800 --> 00:04:41,360
data in there's basic sources advanced

00:04:37,800 --> 00:04:44,580
sources and those custom sources

00:04:41,360 --> 00:04:46,740
the basic sources are things that are

00:04:44,580 --> 00:04:48,479
built in to spark streaming come with it

00:04:46,740 --> 00:04:50,550
there's nothing you have to include it's

00:04:48,479 --> 00:04:53,280
things like the file system network

00:04:50,550 --> 00:04:56,070
socket acha actors stuff that's not

00:04:53,280 --> 00:04:59,090
built in but is available is things like

00:04:56,070 --> 00:05:01,199
Avro and CSB there's a spark projects

00:04:59,090 --> 00:05:04,919
site out there that you can find other

00:05:01,199 --> 00:05:07,740
libraries one thing to note about the

00:05:04,919 --> 00:05:09,330
basic sources is they're not reliable or

00:05:07,740 --> 00:05:12,180
ever think about it a file system or

00:05:09,330 --> 00:05:16,229
reading a file or off of a socket it's

00:05:12,180 --> 00:05:17,430
probably not a reliable operation the

00:05:16,229 --> 00:05:20,460
advanced dip which streams on the other

00:05:17,430 --> 00:05:24,360
hand are things like Twitter Kafka flume

00:05:20,460 --> 00:05:26,759
Kinesis MQTT or other protocols that you

00:05:24,360 --> 00:05:29,400
may see those all require an external

00:05:26,759 --> 00:05:30,539
library not really a big deal but just

00:05:29,400 --> 00:05:32,639
something to be aware of that you'll

00:05:30,539 --> 00:05:35,090
have to get a library for that it's not

00:05:32,639 --> 00:05:38,340
part of the core Spartan package and

00:05:35,090 --> 00:05:42,960
depending upon the source these could be

00:05:38,340 --> 00:05:44,940
the reliable or unreliable and we'll go

00:05:42,960 --> 00:05:49,680
over the difference between them and how

00:05:44,940 --> 00:05:53,909
you could do each one and then there's

00:05:49,680 --> 00:05:55,979
custom input streams to implement a

00:05:53,909 --> 00:05:57,720
custom input stream really just have two

00:05:55,979 --> 00:05:59,729
things to do you have to input a dish

00:05:57,720 --> 00:06:06,240
and input D stream and you have to

00:05:59,729 --> 00:06:09,539
implement a receiver putting together a

00:06:06,240 --> 00:06:11,970
custom input stream is pretty much as

00:06:09,539 --> 00:06:14,610
simple as this you would extend this

00:06:11,970 --> 00:06:16,680
receiver input stream you have one

00:06:14,610 --> 00:06:20,009
method to get receiver and you're going

00:06:16,680 --> 00:06:22,340
to return back the receiver and that's

00:06:20,009 --> 00:06:22,340
really it

00:06:28,820 --> 00:06:34,070
the receiver side of it is a little bit

00:06:31,280 --> 00:06:37,100
more work involved in this case you

00:06:34,070 --> 00:06:40,370
extend receiver there's an on start

00:06:37,100 --> 00:06:41,870
method which has to be non blocking case

00:06:40,370 --> 00:06:45,020
it can get called when it gets pushed to

00:06:41,870 --> 00:06:47,720
your workers when it starts there's a

00:06:45,020 --> 00:06:51,950
stop method and on stop clean everything

00:06:47,720 --> 00:06:53,900
up and then there's a store method and

00:06:51,950 --> 00:06:56,420
that's defined in the receiver class in

00:06:53,900 --> 00:06:57,790
that store method is where you have

00:06:56,420 --> 00:07:00,140
decisions about reliability

00:06:57,790 --> 00:07:01,400
unreliability right kind of where the

00:07:00,140 --> 00:07:09,320
rubber meets the road as to what you're

00:07:01,400 --> 00:07:11,660
going to do so let's talk about the

00:07:09,320 --> 00:07:14,000
receiver reliabilities as we mention

00:07:11,660 --> 00:07:18,980
there's unreliable and reliable

00:07:14,000 --> 00:07:22,340
receivers unreliable pretty simple too

00:07:18,980 --> 00:07:24,500
intimate implement sorry no fault

00:07:22,340 --> 00:07:26,840
tolerance but you're going to lose data

00:07:24,500 --> 00:07:29,660
whenever the receiver fails you pretty

00:07:26,840 --> 00:07:31,550
much just consume it you lose messages

00:07:29,660 --> 00:07:32,930
you lose messages you want but pretty

00:07:31,550 --> 00:07:34,040
simple to get up and running all right

00:07:32,930 --> 00:07:39,110
you're going to consume data from a file

00:07:34,040 --> 00:07:40,540
from a network socket really simple the

00:07:39,110 --> 00:07:42,170
reliable receiver on the other hand

00:07:40,540 --> 00:07:43,880
complexity could depend on the source

00:07:42,170 --> 00:07:45,050
all right depends on where you're going

00:07:43,880 --> 00:07:47,660
to try and read that data from what you

00:07:45,050 --> 00:07:49,100
need to do it could have strong fault

00:07:47,660 --> 00:07:53,000
tolerance guarantee so you can have a

00:07:49,100 --> 00:07:55,180
zero data loss but the data source must

00:07:53,000 --> 00:07:57,410
also support acknowledgment to have that

00:07:55,180 --> 00:07:58,850
okay so you have a data source that does

00:07:57,410 --> 00:08:00,500
not support acknowledgment you're going

00:07:58,850 --> 00:08:07,040
to have trouble having this fault

00:08:00,500 --> 00:08:10,460
tolerance and zero data loss it's kind

00:08:07,040 --> 00:08:12,020
of a walkthrough of when you have a D

00:08:10,460 --> 00:08:13,790
string input D stream and receiver

00:08:12,020 --> 00:08:15,170
whether it's a custom one or those on

00:08:13,790 --> 00:08:18,080
the built-in ones of how this kind of

00:08:15,170 --> 00:08:20,750
works so when you submit your job to the

00:08:18,080 --> 00:08:23,270
master it's going to go ahead and get

00:08:20,750 --> 00:08:25,280
call that get receiver on your input D

00:08:23,270 --> 00:08:28,160
stream it's then going to turn around

00:08:25,280 --> 00:08:29,450
and send it to the spark workers so one

00:08:28,160 --> 00:08:31,790
thing to note which is probably

00:08:29,450 --> 00:08:33,919
potentially obvious here is that when it

00:08:31,790 --> 00:08:35,360
does send it that receiver has to be

00:08:33,919 --> 00:08:37,400
serializable right that's going across

00:08:35,360 --> 00:08:40,070
the network to this other machine so

00:08:37,400 --> 00:08:41,390
it's going to send this receiver you

00:08:40,070 --> 00:08:42,560
receive its going to start running it's

00:08:41,390 --> 00:08:45,290
going to consume data from

00:08:42,560 --> 00:08:47,330
ever your data source maybe it's going

00:08:45,290 --> 00:08:50,740
to call a store method that we saw right

00:08:47,330 --> 00:08:52,790
that's going to store the blocks

00:08:50,740 --> 00:08:55,340
potentially going to replicate the

00:08:52,790 --> 00:08:57,650
blocks to other nodes and other workers

00:08:55,340 --> 00:09:01,310
so now you have that data replicated

00:08:57,650 --> 00:09:03,050
across if it's a reliable receiver this

00:09:01,310 --> 00:09:05,690
would be your opportunity to say that

00:09:03,050 --> 00:09:07,220
when these blocks were replicated you

00:09:05,690 --> 00:09:09,710
could then acknowledged back that source

00:09:07,220 --> 00:09:11,830
system that you process that data you

00:09:09,710 --> 00:09:14,690
have it right because when this returns

00:09:11,830 --> 00:09:19,130
your guarantee that data is now multiple

00:09:14,690 --> 00:09:21,560
nodes and it's been persistent after

00:09:19,130 --> 00:09:24,050
that happens then you return back to the

00:09:21,560 --> 00:09:28,220
spark master and two drivers saying that

00:09:24,050 --> 00:09:30,160
here's the block IDs that I have okay so

00:09:28,220 --> 00:09:33,380
from that point it could then scheduled

00:09:30,160 --> 00:09:38,060
jobs to get run okay if we remember

00:09:33,380 --> 00:09:40,339
spark streaming is spark most part in

00:09:38,060 --> 00:09:42,860
micro batches and some things are

00:09:40,339 --> 00:09:44,330
different with the most part it's parked

00:09:42,860 --> 00:09:49,940
with micro batches right like a half a

00:09:44,330 --> 00:09:54,080
second this is two ways that we could

00:09:49,940 --> 00:09:56,390
create a d stream one we saw some sort

00:09:54,080 --> 00:09:58,010
of streaming source basic advanced the

00:09:56,390 --> 00:10:00,700
custom one the others by doing a

00:09:58,010 --> 00:10:00,700
transformation

00:10:05,570 --> 00:10:09,890
creating a d stream via transformation

00:10:08,000 --> 00:10:11,660
is really any sort of transformative

00:10:09,890 --> 00:10:12,860
operation you're doing right you get a

00:10:11,660 --> 00:10:14,270
look through the spark documentation

00:10:12,860 --> 00:10:17,810
there's a whole bunch of transformations

00:10:14,270 --> 00:10:19,670
that you could perform map flatmap and

00:10:17,810 --> 00:10:21,800
so forth right so you'd have an input D

00:10:19,670 --> 00:10:29,030
stream you perform some transformation

00:10:21,800 --> 00:10:31,480
on it and you get an output D string you

00:10:29,030 --> 00:10:33,890
can see these general classifications

00:10:31,480 --> 00:10:35,900
just the standard ones that you'd expect

00:10:33,890 --> 00:10:38,900
to see in spark and expect to see in

00:10:35,900 --> 00:10:41,900
other places map count by value reduced

00:10:38,900 --> 00:10:44,780
by key join and so forth and then

00:10:41,900 --> 00:10:48,040
there's stateful operations window

00:10:44,780 --> 00:10:49,970
updates state by key a transforming one

00:10:48,040 --> 00:10:52,130
count by value and window and some

00:10:49,970 --> 00:10:59,180
others and we'll go through a couple of

00:10:52,130 --> 00:11:05,210
those in more detail so we look at

00:10:59,180 --> 00:11:07,430
transforming input here we have a stream

00:11:05,210 --> 00:11:10,010
that we created just call create custom

00:11:07,430 --> 00:11:12,500
stream passing the streaming context and

00:11:10,010 --> 00:11:13,670
then we have my streamed up map and do

00:11:12,500 --> 00:11:18,950
whatever going to do there and get back

00:11:13,670 --> 00:11:21,590
save these events if this is time

00:11:18,950 --> 00:11:24,050
running across I recited every interval

00:11:21,590 --> 00:11:26,600
we're basically producing an RDD right

00:11:24,050 --> 00:11:30,650
so you'd have this batch here at time T

00:11:26,600 --> 00:11:32,540
plus one plus two it's what every time

00:11:30,650 --> 00:11:34,190
this runs you can get your input stream

00:11:32,540 --> 00:11:36,050
you perform your map and you can get

00:11:34,190 --> 00:11:38,570
your r DD coming out and this can help

00:11:36,050 --> 00:11:41,110
it on every single time that that batch

00:11:38,570 --> 00:11:41,110
is running

00:11:47,250 --> 00:11:53,470
let's talk a little bit about stateful

00:11:49,390 --> 00:11:56,260
operations the first one is update state

00:11:53,470 --> 00:11:58,960
by key so this is interesting it

00:11:56,260 --> 00:12:01,990
provides you a way to maintain arbitrary

00:11:58,960 --> 00:12:05,710
state and continuously update it while

00:12:01,990 --> 00:12:07,900
your jobs running say you had you know

00:12:05,710 --> 00:12:09,220
in session advertising or in track

00:12:07,900 --> 00:12:10,960
Twitter sentiment or there's something

00:12:09,220 --> 00:12:12,940
else you want to do you want to the

00:12:10,960 --> 00:12:17,010
state around as data is flowing through

00:12:12,940 --> 00:12:17,010
this would be a way that you could do it

00:12:19,020 --> 00:12:25,510
two things that have to be done in order

00:12:22,870 --> 00:12:27,220
for you to take advantage of that one

00:12:25,510 --> 00:12:30,400
you to define the state this could be

00:12:27,220 --> 00:12:34,300
anything that you want it to be and yet

00:12:30,400 --> 00:12:35,500
to find the update function and your

00:12:34,300 --> 00:12:37,510
update function is going to need to know

00:12:35,500 --> 00:12:40,450
how to update the state using the

00:12:37,510 --> 00:12:41,770
previous and the current okay so it

00:12:40,450 --> 00:12:43,960
could be your data whatever you want it

00:12:41,770 --> 00:12:45,280
to be maybe you're tracking visitors are

00:12:43,960 --> 00:12:47,500
tracking sentiment and as the sentiments

00:12:45,280 --> 00:12:49,300
changing and you need to have a way that

00:12:47,500 --> 00:12:50,410
you could update it and pass in the

00:12:49,300 --> 00:12:54,520
function that's going to perform that

00:12:50,410 --> 00:12:57,910
operation the other thing that's

00:12:54,520 --> 00:12:59,920
required is the check pointing has to be

00:12:57,910 --> 00:13:02,410
configured if you think about this is

00:12:59,920 --> 00:13:03,520
keeping state as a job is running and

00:13:02,410 --> 00:13:08,100
you're going to want to have checkpoints

00:13:03,520 --> 00:13:08,100
going right so that you don't lose data

00:13:13,680 --> 00:13:19,720
here's an example of using it let's say

00:13:17,770 --> 00:13:21,820
we wanted to do that sentiment right we

00:13:19,720 --> 00:13:23,950
want to keep track of per user mode of

00:13:21,820 --> 00:13:25,120
state and want to update it with his or

00:13:23,950 --> 00:13:28,840
her tweets as they're flowing through

00:13:25,120 --> 00:13:31,030
the system meant to say we have an RDD

00:13:28,840 --> 00:13:33,580
that's called tweets we're going to call

00:13:31,030 --> 00:13:35,620
update state by key we pass in this

00:13:33,580 --> 00:13:39,370
function that's going to be update mood

00:13:35,620 --> 00:13:40,660
and takes a tweet okay so update mood is

00:13:39,370 --> 00:13:45,360
going to take the new tweets and the

00:13:40,660 --> 00:13:49,510
last mood and produce the new mood

00:13:45,360 --> 00:13:51,700
kisses good see this is going along each

00:13:49,510 --> 00:13:53,260
batch here as it's executing right we're

00:13:51,700 --> 00:13:55,000
going to be updating these moods and

00:13:53,260 --> 00:13:56,560
keeping them all right so every batch

00:13:55,000 --> 00:13:58,000
interval that's running we're going to

00:13:56,560 --> 00:14:06,910
have these moods that world and onto and

00:13:58,000 --> 00:14:10,590
constantly updating as its flowing the

00:14:06,910 --> 00:14:14,830
other stateful operation that requires

00:14:10,590 --> 00:14:16,840
more discussion is transform so

00:14:14,830 --> 00:14:20,260
transformed allows you to do arbitrary

00:14:16,840 --> 00:14:23,880
RDD to our DD functions on your D string

00:14:20,260 --> 00:14:23,880
so you can really do anything you want

00:14:25,320 --> 00:14:32,010
so this transform takes in some

00:14:27,640 --> 00:14:32,010
transform function that produces an RDD

00:14:32,760 --> 00:14:37,720
returning by that d stream so if you had

00:14:35,230 --> 00:14:39,130
it like we say have here they want to

00:14:37,720 --> 00:14:41,710
eliminate noise words from crawl

00:14:39,130 --> 00:14:44,590
documents right say we sucked in data

00:14:41,710 --> 00:14:47,470
from Hadoop we have this noise words our

00:14:44,590 --> 00:14:48,820
DD and then we have this crawled corpus

00:14:47,470 --> 00:14:52,540
that we're crawling and we want to

00:14:48,820 --> 00:14:54,220
transform it and get rid of noise all

00:14:52,540 --> 00:14:56,650
right this is adi stream that's here

00:14:54,220 --> 00:14:59,710
we're going to call transform on it for

00:14:56,650 --> 00:15:01,810
every RTD in that batch we're going to

00:14:59,710 --> 00:15:05,580
basically call join on that nose word

00:15:01,810 --> 00:15:08,530
nose noise word our DD and then filter

00:15:05,580 --> 00:15:11,410
mask you do whatever you want there this

00:15:08,530 --> 00:15:14,170
could be a regular old r DD that you got

00:15:11,410 --> 00:15:16,480
from anywhere ok so it gives you an

00:15:14,170 --> 00:15:20,410
opportunity to have historical data that

00:15:16,480 --> 00:15:23,800
you're combining with a stream ok to

00:15:20,410 --> 00:15:26,260
note when this gets called this gets

00:15:23,800 --> 00:15:27,440
evaluated and this joint or whatever it

00:15:26,260 --> 00:15:29,180
is you're going to do here

00:15:27,440 --> 00:15:31,790
is going to get evaluated on every

00:15:29,180 --> 00:15:33,920
single batch run so if you want it to be

00:15:31,790 --> 00:15:36,860
updating historical data as the day's

00:15:33,920 --> 00:15:40,370
going along you'd have an opportunity to

00:15:36,860 --> 00:15:41,900
do that here so you could be updating

00:15:40,370 --> 00:15:43,730
historical data you could have a stream

00:15:41,900 --> 00:15:46,310
going and you could transform that

00:15:43,730 --> 00:15:50,270
incoming stream with the historical data

00:15:46,310 --> 00:15:51,470
maybe tack on to it maybe it's the

00:15:50,270 --> 00:15:52,700
weather maybe it's a visitor or

00:15:51,470 --> 00:16:00,220
something that you want to do and tack

00:15:52,700 --> 00:16:04,450
on to and we saw on the previous screen

00:16:00,220 --> 00:16:04,450
that we have that join that we're using

00:16:04,570 --> 00:16:10,280
you could also do a join on 2d streams

00:16:08,170 --> 00:16:12,440
the joint on the previous scream of the

00:16:10,280 --> 00:16:15,440
transform again is an RTD to our DD in

00:16:12,440 --> 00:16:19,940
this case we're going to join two

00:16:15,440 --> 00:16:21,200
streams so say as an example you know

00:16:19,940 --> 00:16:23,780
you have a group of users that are using

00:16:21,200 --> 00:16:26,120
some Fitbit device and maybe an

00:16:23,780 --> 00:16:28,640
application like say map my run and you

00:16:26,120 --> 00:16:31,910
have two streams coming in and both of

00:16:28,640 --> 00:16:34,310
them have some user ID on them then you

00:16:31,910 --> 00:16:36,520
could call say this Fitbit stream until

00:16:34,310 --> 00:16:38,630
to join with your map my run strain

00:16:36,520 --> 00:16:42,770
that's going to turn around and if you

00:16:38,630 --> 00:16:46,340
have to dees trims say Fitbit stream has

00:16:42,770 --> 00:16:48,920
an ID and the data and let's say the map

00:16:46,340 --> 00:16:50,720
my run stream has an ID in the data key

00:16:48,920 --> 00:16:52,970
and the value what you're going to get

00:16:50,720 --> 00:16:54,680
coming back from this joint is a key

00:16:52,970 --> 00:16:56,990
that has that user ID that was found in

00:16:54,680 --> 00:16:59,630
both with a tuple that has the values

00:16:56,990 --> 00:17:01,010
from each okay so now you've joined

00:16:59,630 --> 00:17:06,110
those two streams of data as it's

00:17:01,010 --> 00:17:08,480
flowing you can't do this with addie

00:17:06,110 --> 00:17:10,220
stream at an RDD you have to do

00:17:08,480 --> 00:17:19,760
transforms that you're looking at each

00:17:10,220 --> 00:17:22,069
rtd in the batch is after we've ingested

00:17:19,760 --> 00:17:25,060
data we've transformed the data and then

00:17:22,069 --> 00:17:27,650
we're going to want to output the data

00:17:25,060 --> 00:17:29,540
okay so let's just say soon we create

00:17:27,650 --> 00:17:31,670
this customer stream again we perform

00:17:29,540 --> 00:17:34,640
some sort of map and then we're going to

00:17:31,670 --> 00:17:36,679
perform some sort of action right if you

00:17:34,640 --> 00:17:39,650
spark before sparks trimming is the same

00:17:36,679 --> 00:17:41,000
way right it's very lazy all right it's

00:17:39,650 --> 00:17:43,790
going to do everything that

00:17:41,000 --> 00:17:45,950
you do as a transformation is really

00:17:43,790 --> 00:17:47,840
just a recording of yep need to do that

00:17:45,950 --> 00:17:49,610
need to do that need to do that but

00:17:47,840 --> 00:17:54,560
nothing really happens into actions

00:17:49,610 --> 00:17:56,450
performed so when we hit this for each

00:17:54,560 --> 00:17:58,640
right we have some action that's

00:17:56,450 --> 00:18:01,310
performed so the same thing we're going

00:17:58,640 --> 00:18:03,140
across these batches of time can be

00:18:01,310 --> 00:18:05,120
looking these days r dds can perform

00:18:03,140 --> 00:18:07,460
some map we're going to get ahead and

00:18:05,120 --> 00:18:11,050
have this events d stream and then we're

00:18:07,460 --> 00:18:11,050
going to eventually do this for each rtd

00:18:11,380 --> 00:18:16,010
thing to keep in mind with that for each

00:18:13,640 --> 00:18:17,210
and you're outputting data right you

00:18:16,010 --> 00:18:19,190
want to make sure that that's non

00:18:17,210 --> 00:18:20,120
blocking and it's performing because

00:18:19,190 --> 00:18:21,860
that's going to be writing to some

00:18:20,120 --> 00:18:24,530
external system and so you start to hold

00:18:21,860 --> 00:18:26,090
things up if that's not performing ok so

00:18:24,530 --> 00:18:28,720
that's going to pull in data and start

00:18:26,090 --> 00:18:28,720
writing somewhere

00:18:38,380 --> 00:18:44,170
so this is the representation of that

00:18:41,500 --> 00:18:48,570
little spark streaming job of how you go

00:18:44,170 --> 00:18:48,570
from this D string to that spark job

00:18:50,490 --> 00:18:56,440
rights we create this input stream we

00:18:54,520 --> 00:18:59,320
perform some sort of map form some sort

00:18:56,440 --> 00:19:01,990
of count and then we have a for each

00:18:59,320 --> 00:19:05,800
which is the action under the covers

00:19:01,990 --> 00:19:08,020
that's converted into a spark job for

00:19:05,800 --> 00:19:09,670
each execution of the batch so you're

00:19:08,020 --> 00:19:12,430
going to have an action that caused it

00:19:09,670 --> 00:19:14,290
at 4-h you can have the count happening

00:19:12,430 --> 00:19:18,010
and you can have a map and you have the

00:19:14,290 --> 00:19:20,320
input so in every batch cycle that's

00:19:18,010 --> 00:19:23,470
what's going to end up happening so you

00:19:20,320 --> 00:19:26,040
go from here to here and this is what's

00:19:23,470 --> 00:19:26,040
executing

00:19:34,380 --> 00:19:38,550
let's talk about time a little bit

00:19:38,760 --> 00:19:43,230
there's a couple of things that need to

00:19:40,990 --> 00:19:45,850
think about when thinking about time

00:19:43,230 --> 00:19:47,650
windowing which you've probably heard

00:19:45,850 --> 00:19:48,730
discussed in any of the streaming talks

00:19:47,650 --> 00:19:51,280
you've been to in the last day and a

00:19:48,730 --> 00:19:53,770
half the notion of string time versa

00:19:51,280 --> 00:19:56,340
vent time and then how you deal with out

00:19:53,770 --> 00:19:56,340
of order data

00:20:03,520 --> 00:20:08,430
is a couple of common windowing types

00:20:06,010 --> 00:20:12,010
that you see out there a tumbling and

00:20:08,430 --> 00:20:15,910
sliding spark streaming today only

00:20:12,010 --> 00:20:17,920
implements sliding windows for better or

00:20:15,910 --> 00:20:19,600
for worse that's where it has there's

00:20:17,920 --> 00:20:21,850
ways to work around somas limitations

00:20:19,600 --> 00:20:26,620
but that's the windowing that's there

00:20:21,850 --> 00:20:28,150
and will step into that so let's first

00:20:26,620 --> 00:20:29,560
look at tumbling just so we have some

00:20:28,150 --> 00:20:33,640
perspective of the difference between

00:20:29,560 --> 00:20:36,160
the two so in a tumbling window and this

00:20:33,640 --> 00:20:39,310
would be like a tumbling count the

00:20:36,160 --> 00:20:41,410
window size is fixed and the data is

00:20:39,310 --> 00:20:45,640
evicted from the window at a specified

00:20:41,410 --> 00:20:47,650
size right so if we set here that the

00:20:45,640 --> 00:20:51,310
window length is going to be say one

00:20:47,650 --> 00:20:54,310
second whatever data came in that's it

00:20:51,310 --> 00:20:55,420
it's ejected okay and that doesn't

00:20:54,310 --> 00:21:00,030
matter that's considered the window be

00:20:55,420 --> 00:21:00,030
in full and the next window okay

00:21:11,770 --> 00:21:19,180
this is tumbling in a temple way and I

00:21:16,240 --> 00:21:23,440
think I so the wrong thing to you on the

00:21:19,180 --> 00:21:26,380
slide before the tumbling count is based

00:21:23,440 --> 00:21:28,810
upon the size of the window not the time

00:21:26,380 --> 00:21:30,790
of the window okay so in this case say

00:21:28,810 --> 00:21:32,170
we have a window size of two it's going

00:21:30,790 --> 00:21:33,850
to be two elements in the window and

00:21:32,170 --> 00:21:42,280
then it'll be a new window that gets

00:21:33,850 --> 00:21:43,960
created temporal windowing again in

00:21:42,280 --> 00:21:46,000
tumbling this is going to be based on

00:21:43,960 --> 00:21:47,650
time so we have this being several

00:21:46,000 --> 00:21:49,720
seconds it doesn't matter whether

00:21:47,650 --> 00:21:51,400
there's three elements of data or five

00:21:49,720 --> 00:21:54,460
elements of data you're going to have a

00:21:51,400 --> 00:22:03,220
window that tumbles at every demarcation

00:21:54,460 --> 00:22:05,620
of time that was sent the sliding window

00:22:03,220 --> 00:22:10,240
is what's provided in spark streaming

00:22:05,620 --> 00:22:12,540
today and a way that this works is you

00:22:10,240 --> 00:22:15,820
have an overall length of the window and

00:22:12,540 --> 00:22:19,030
then you have an interval okay so we may

00:22:15,820 --> 00:22:21,400
be looking at a window length of two

00:22:19,030 --> 00:22:22,840
seconds but our sliding interval of when

00:22:21,400 --> 00:22:26,350
we're going to execute is going to be

00:22:22,840 --> 00:22:28,300
over one second so you'd see this data

00:22:26,350 --> 00:22:36,250
for the length of the window but you're

00:22:28,300 --> 00:22:39,070
executing in the interval and in this

00:22:36,250 --> 00:22:41,140
case of vic shins happening as the data

00:22:39,070 --> 00:22:44,560
is sliding as far as this window is

00:22:41,140 --> 00:22:46,270
sliding across and time is continuing to

00:22:44,560 --> 00:22:52,930
run then data is going to continue to

00:22:46,270 --> 00:22:56,280
get evicted so a little bit different

00:22:52,930 --> 00:22:56,280
than temporal way of doing it

00:23:01,770 --> 00:23:08,500
there's two types of sliding window in

00:23:04,480 --> 00:23:11,110
that spark shimming supports there's non

00:23:08,500 --> 00:23:18,910
incremental and incremental that's just

00:23:11,110 --> 00:23:24,970
a slight twist on sliding window so non

00:23:18,910 --> 00:23:26,680
incremental is the easiest way in this

00:23:24,970 --> 00:23:30,760
case we have this reduced by key and

00:23:26,680 --> 00:23:33,280
window we're going to pass in a function

00:23:30,760 --> 00:23:35,020
that's going to go ahead and aggregate

00:23:33,280 --> 00:23:37,570
the data and we're going to tell it that

00:23:35,020 --> 00:23:41,110
we want a length of five and that we're

00:23:37,570 --> 00:23:44,860
going to run every second so you can see

00:23:41,110 --> 00:23:49,180
as this happens we're doing interval

00:23:44,860 --> 00:23:52,000
counts every time and then go ahead and

00:23:49,180 --> 00:23:55,830
sending them up because you can see how

00:23:52,000 --> 00:23:55,830
that could get expensive over time

00:24:01,080 --> 00:24:09,760
incremental sliding is a little bit more

00:24:06,670 --> 00:24:12,640
efficient right instead of having to do

00:24:09,760 --> 00:24:15,220
the aggregate for the previous time of

00:24:12,640 --> 00:24:18,450
our window we're only having to look at

00:24:15,220 --> 00:24:21,100
at first time and the time we're at now

00:24:18,450 --> 00:24:23,760
the caveat is you need to provide an

00:24:21,100 --> 00:24:25,960
inverse function to be able to do that

00:24:23,760 --> 00:24:28,090
okay so if we look at this previous

00:24:25,960 --> 00:24:30,820
slide but we just need to provide an

00:24:28,090 --> 00:24:37,420
aggregate of how to get to this number

00:24:30,820 --> 00:24:40,120
by aggregating days but when we have

00:24:37,420 --> 00:24:41,470
incremental right not only do we need to

00:24:40,120 --> 00:24:43,180
know how to aggregate it but we need to

00:24:41,470 --> 00:24:44,710
have it basically you know subtract it

00:24:43,180 --> 00:24:48,070
right do the inverse of whatever that

00:24:44,710 --> 00:24:50,350
computation is so at that window we

00:24:48,070 --> 00:24:53,500
could go from this value and this value

00:24:50,350 --> 00:24:56,380
and produce the right some butter rid of

00:24:53,500 --> 00:25:00,640
the outputs going to be so if you could

00:24:56,380 --> 00:25:02,560
do it and if your problem supports you

00:25:00,640 --> 00:25:04,300
doing that the data you're storing that

00:25:02,560 --> 00:25:06,670
could be much more efficient way of

00:25:04,300 --> 00:25:09,240
having the window lang than having a non

00:25:06,670 --> 00:25:09,240
incremental

00:25:16,909 --> 00:25:24,419
so steak mode talk about string time

00:25:19,379 --> 00:25:26,700
verse event time it's extreme time but

00:25:24,419 --> 00:25:28,649
is the time when the record arrives into

00:25:26,700 --> 00:25:30,720
the streaming system so when spark

00:25:28,649 --> 00:25:33,299
streaming gets the data that stream time

00:25:30,720 --> 00:25:37,320
right all the windowing is based upon

00:25:33,299 --> 00:25:39,149
the time it receives the data event time

00:25:37,320 --> 00:25:42,090
is a time that the event was actually

00:25:39,149 --> 00:25:44,849
generated at so if I have this you know

00:25:42,090 --> 00:25:47,159
smart tag here and it's sending data

00:25:44,849 --> 00:25:50,249
pulses when that's getting created

00:25:47,159 --> 00:25:51,989
that's the event time that may end up in

00:25:50,249 --> 00:25:54,239
Kafka and they're getting sucked into

00:25:51,989 --> 00:25:55,679
spark streaming when it gets into spark

00:25:54,239 --> 00:25:57,840
streaming again that's going to be

00:25:55,679 --> 00:26:01,109
streamed time and you can have the same

00:25:57,840 --> 00:26:03,450
thing in Kafka unless you set up your

00:26:01,109 --> 00:26:05,729
data to all go to one partition right

00:26:03,450 --> 00:26:08,609
that the guarantee is all the data is in

00:26:05,729 --> 00:26:10,619
order in the partition that you're

00:26:08,609 --> 00:26:12,720
reading it from but that has nothing to

00:26:10,619 --> 00:26:15,840
do with the time that your data was

00:26:12,720 --> 00:26:18,809
actually created right so you have the

00:26:15,840 --> 00:26:19,979
same type of thing so in the case of

00:26:18,809 --> 00:26:22,379
spark streaming you're dealing with

00:26:19,979 --> 00:26:24,539
string time so if you wanted to be able

00:26:22,379 --> 00:26:26,369
to do computations based upon when the

00:26:24,539 --> 00:26:28,409
event was created you're going to have

00:26:26,369 --> 00:26:30,599
to be conscious of that and make sure

00:26:28,409 --> 00:26:31,950
you're handling it correctly because as

00:26:30,599 --> 00:26:34,470
you're scrolling across those windows a

00:26:31,950 --> 00:26:36,979
time there's nothing to do with when you

00:26:34,470 --> 00:26:39,960
created it and so that's where a

00:26:36,979 --> 00:26:41,220
tumbling window or other windowing would

00:26:39,960 --> 00:26:42,749
be able to help where you could say you

00:26:41,220 --> 00:26:45,599
know what's that the window based upon

00:26:42,749 --> 00:26:47,220
this time stamp on my data and have the

00:26:45,599 --> 00:26:49,499
window based upon that not based upon

00:26:47,220 --> 00:26:54,059
the time that the data was received into

00:26:49,499 --> 00:26:56,729
the system other thing to think about is

00:26:54,059 --> 00:27:00,539
out of order data in some applications

00:26:56,729 --> 00:27:02,940
it doesn't matter in other applications

00:27:00,539 --> 00:27:05,129
it's a big deal of whether the data in

00:27:02,940 --> 00:27:07,590
order out of order okay again this is

00:27:05,129 --> 00:27:10,139
left up to you to figure out how you're

00:27:07,590 --> 00:27:14,489
going to handle that situation as the

00:27:10,139 --> 00:27:18,419
data is flowing through your system from

00:27:14,489 --> 00:27:19,919
sparks streaming standpoint if the data

00:27:18,419 --> 00:27:23,220
source you're reading from is providing

00:27:19,919 --> 00:27:24,450
the data in order it's an order right

00:27:23,220 --> 00:27:27,259
but it really has no understanding as to

00:27:24,450 --> 00:27:27,259
what that data is

00:27:35,180 --> 00:27:40,640
here's a way that you could handle out

00:27:38,090 --> 00:27:43,880
of order data so say we want to track ad

00:27:40,640 --> 00:27:46,550
impressions between some time and time

00:27:43,880 --> 00:27:48,710
plus i right you could have this

00:27:46,550 --> 00:27:51,740
interval count that you're keeping and

00:27:48,710 --> 00:27:53,420
keeping track of this state right so you

00:27:51,740 --> 00:27:55,130
always be keeping track of what it was

00:27:53,420 --> 00:27:58,310
you know you have this sliding interval

00:27:55,130 --> 00:28:00,380
here of T plus 1 T plus 2 you'd have

00:27:58,310 --> 00:28:02,990
these interval counts for that time plus

00:28:00,380 --> 00:28:05,900
time plus 1 right you keep that going as

00:28:02,990 --> 00:28:07,970
you go along this is based on this

00:28:05,900 --> 00:28:11,330
continuous analytics / discontinuous

00:28:07,970 --> 00:28:12,950
streams so if the problem that you need

00:28:11,330 --> 00:28:15,680
to deal with that's a pretty interesting

00:28:12,950 --> 00:28:16,910
paper to take a look at and you can

00:28:15,680 --> 00:28:19,610
think of ways that you could apply this

00:28:16,910 --> 00:28:23,390
to your data by following this basically

00:28:19,610 --> 00:28:27,430
keeping the count the current count and

00:28:23,390 --> 00:28:27,430
the previous count as you're going along

00:28:37,610 --> 00:28:43,680
let's take a moment to look at the check

00:28:39,930 --> 00:28:46,710
pointing in part of the recovery and

00:28:43,680 --> 00:28:48,720
fault tolerance this is two types of

00:28:46,710 --> 00:28:51,120
check pointing that you can turn on in

00:28:48,720 --> 00:28:52,980
spark streaming one deals with metadata

00:28:51,120 --> 00:28:58,140
check pointing and one deals with data

00:28:52,980 --> 00:29:01,320
check pointing right so if we imagine

00:28:58,140 --> 00:29:03,510
this being our application running and

00:29:01,320 --> 00:29:06,000
we're keeping all this state at every

00:29:03,510 --> 00:29:07,770
time interval we want to make sure that

00:29:06,000 --> 00:29:12,350
if something goes south which is going

00:29:07,770 --> 00:29:12,350
to that we don't lose data

00:29:18,230 --> 00:29:21,919
so without having checkpointing it looks

00:29:20,450 --> 00:29:23,870
just like we saw in the previous slide

00:29:21,919 --> 00:29:25,520
all right we're going along we have the

00:29:23,870 --> 00:29:27,440
state that's building up and if

00:29:25,520 --> 00:29:30,549
something fails on our application we're

00:29:27,440 --> 00:29:30,549
going to lose that state information

00:29:34,660 --> 00:29:42,890
when you enable check pointing at

00:29:40,669 --> 00:29:45,830
regular intervals it's going to write to

00:29:42,890 --> 00:29:48,650
some Brazilian store it could be HDFS it

00:29:45,830 --> 00:29:51,230
could be s3 but it's going to write that

00:29:48,650 --> 00:29:53,000
data somewhere in the case of metadata

00:29:51,230 --> 00:29:55,400
checkpointing which is writing

00:29:53,000 --> 00:29:57,410
information about the lineage and just a

00:29:55,400 --> 00:29:59,720
metadata of what's running in the case

00:29:57,410 --> 00:30:04,210
of data will see that we're writing data

00:29:59,720 --> 00:30:04,210
okay so that we know what's being run

00:30:07,720 --> 00:30:15,620
it's a balancing act if we're writing

00:30:12,049 --> 00:30:18,380
too quickly to HDFS then that's going to

00:30:15,620 --> 00:30:21,020
be a bottleneck if we run too

00:30:18,380 --> 00:30:23,450
infrequently then the lineages that

00:30:21,020 --> 00:30:25,610
we're building out right they are dd's

00:30:23,450 --> 00:30:27,559
again just like spark right so you have

00:30:25,610 --> 00:30:29,330
this lineage that's building imagine

00:30:27,559 --> 00:30:31,250
these are running say every half second

00:30:29,330 --> 00:30:33,770
you've got this lineage that's building

00:30:31,250 --> 00:30:36,559
up over time if you're not writing

00:30:33,770 --> 00:30:43,070
checkpoints often enough then that's

00:30:36,559 --> 00:30:44,900
going to grow the default setting today

00:30:43,070 --> 00:30:47,260
in sparks dreaming is the multiple

00:30:44,900 --> 00:30:52,070
imaginable that's at least ten seconds

00:30:47,260 --> 00:30:54,620
and the recommendation currently is to

00:30:52,070 --> 00:30:57,910
have an interval of five to ten times

00:30:54,620 --> 00:30:57,910
you're sliding interval

00:31:06,379 --> 00:31:12,359
so fault tolerance the effusion layer

00:31:10,259 --> 00:31:14,789
would spark and all the properties of

00:31:12,359 --> 00:31:16,979
our td's that they're immutable their

00:31:14,789 --> 00:31:19,409
deterministically computed and they're

00:31:16,979 --> 00:31:21,509
distributed data set spark streaming has

00:31:19,409 --> 00:31:24,629
all those properties that tag along with

00:31:21,509 --> 00:31:27,450
it right there's two things that we're

00:31:24,629 --> 00:31:30,570
trying to prevent we're trying to

00:31:27,450 --> 00:31:31,919
prevent the failure of the worker that's

00:31:30,570 --> 00:31:33,690
going to have all the data that's there

00:31:31,919 --> 00:31:35,190
and we're trying to prevent the failure

00:31:33,690 --> 00:31:37,349
of your driver that's going to have the

00:31:35,190 --> 00:31:39,209
block IDs and the lineage information

00:31:37,349 --> 00:31:41,820
and everything else that contained right

00:31:39,209 --> 00:31:45,209
scheduling the jobs so trying to prevent

00:31:41,820 --> 00:31:51,089
failure of both of those checkpointing

00:31:45,209 --> 00:31:52,409
provides you a way to do that these

00:31:51,089 --> 00:31:55,079
different semantics that you could

00:31:52,409 --> 00:31:57,299
achieve as well we have at most once

00:31:55,079 --> 00:31:58,709
right that at most the data is can be

00:31:57,299 --> 00:32:01,379
processed one time or that message

00:31:58,709 --> 00:32:05,959
coming through is once at least once and

00:32:01,379 --> 00:32:08,549
then exactly once depending upon the

00:32:05,959 --> 00:32:10,859
data source that you're using and the

00:32:08,549 --> 00:32:14,239
provider you're using the receiver you

00:32:10,859 --> 00:32:20,129
could achieve different levels of this

00:32:14,239 --> 00:32:21,419
if you use write ahead logging and you

00:32:20,129 --> 00:32:23,219
use a data source that allows

00:32:21,419 --> 00:32:29,339
acknowledgment likes a Kafka and you

00:32:23,219 --> 00:32:34,559
could get exactly once if it's an

00:32:29,339 --> 00:32:36,899
unreliable than not right the places

00:32:34,559 --> 00:32:38,989
you'll need to think about it or it's

00:32:36,899 --> 00:32:40,769
going to be in your receivers in the

00:32:38,989 --> 00:32:43,679
transformations that you're doing and

00:32:40,769 --> 00:32:45,599
then in the output but if you spend all

00:32:43,679 --> 00:32:47,039
this time getting data from say Kafka

00:32:45,599 --> 00:32:50,429
making sure you're processing message is

00:32:47,039 --> 00:32:53,070
one time if your output is not reliable

00:32:50,429 --> 00:32:57,389
and it can't handle the same message

00:32:53,070 --> 00:32:59,039
coming multiple times that's going to be

00:32:57,389 --> 00:33:02,690
a problem right so those places to think

00:32:59,039 --> 00:33:02,690
about it all along the path

00:33:05,969 --> 00:33:09,609
so in conclusion just coming over you no

00:33:08,589 --> 00:33:12,459
introduction just high level

00:33:09,609 --> 00:33:14,379
architecture d streams and kind of some

00:33:12,459 --> 00:33:16,449
under the covers about those talked

00:33:14,379 --> 00:33:17,889
about how we think about time and then

00:33:16,449 --> 00:33:24,339
just touch on recovery and fault

00:33:17,889 --> 00:33:26,429
tolerance so thanks and take any

00:33:24,339 --> 00:33:26,429

YouTube URL: https://www.youtube.com/watch?v=vSSGhqWqP9E


