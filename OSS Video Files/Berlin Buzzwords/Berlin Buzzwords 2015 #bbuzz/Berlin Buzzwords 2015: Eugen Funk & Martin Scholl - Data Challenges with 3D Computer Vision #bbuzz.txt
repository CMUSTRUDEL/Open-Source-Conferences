Title: Berlin Buzzwords 2015: Eugen Funk & Martin Scholl - Data Challenges with 3D Computer Vision #bbuzz
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	A wide range of end user and industrial applications rely on accurate 3D scene representation. Automated 3D modelling from optical sensors such as LIDAR scanners, stereo or RGBD cameras became inevitable since this enables to replace the time consuming manual 3D modelling process. 

However, optical sensors deliver extremely high amount of data which is required to be processed to information. An 8 bit stereo system with 9MPx colour images delivers 4GB of data per second. Being able to make sense out of this enormous flow of data is a huge and ongoing research and development task. 

This talk will give an overview of the challenges which naturally involve efficient search, storage and scalable algorithms for information extraction from 3D data. Also an overview of practical implications on 3D reconstruction of cities, autonomous driving in public and industrial facilities, or inspection and monitoring as part of security strategies will be presented.

Read more:
https://2015.berlinbuzzwords.de/session/data-challenges-3d-computer-vision

About Eugen Funk:
https://2015.berlinbuzzwords.de/users/eugen-funk

About Martin Scholl:
https://2015.berlinbuzzwords.de/users/martin-scholl

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,340 --> 00:00:08,880
thank you

00:00:09,150 --> 00:00:15,599
this is eigen and i'm martin and the

00:00:12,780 --> 00:00:17,579
reason we are here today is for a very

00:00:15,599 --> 00:00:19,529
simple fact and that is when we are

00:00:17,579 --> 00:00:22,560
talking about 3d computer vision and

00:00:19,529 --> 00:00:24,180
with that we mean that computers try to

00:00:22,560 --> 00:00:27,420
perceive the real world the physical

00:00:24,180 --> 00:00:29,580
world then the the challenges that we

00:00:27,420 --> 00:00:31,800
are that we are facing with the

00:00:29,580 --> 00:00:35,670
technology is very similar to what web

00:00:31,800 --> 00:00:37,800
people are facing it it starts with the

00:00:35,670 --> 00:00:40,500
perception part meaning the data

00:00:37,800 --> 00:00:42,780
gathering of the that is really a big

00:00:40,500 --> 00:00:45,510
challenge as we will see and the models

00:00:42,780 --> 00:00:47,309
and the data that we collect by using 3d

00:00:45,510 --> 00:00:50,970
perception technology they are extremely

00:00:47,309 --> 00:00:53,489
large and what is even a bigger problem

00:00:50,970 --> 00:00:55,350
is not only just capturing of the data

00:00:53,489 --> 00:00:57,119
but actually making sense of it was

00:00:55,350 --> 00:01:00,180
sense meaning some kind of understanding

00:00:57,119 --> 00:01:02,040
what what is being captured and last but

00:01:00,180 --> 00:01:04,909
not least visualizing all this free data

00:01:02,040 --> 00:01:08,490
all this 3d data is also a big challenge

00:01:04,909 --> 00:01:11,939
but first of all why should you care

00:01:08,490 --> 00:01:14,040
about 3d technology which i think is one

00:01:11,939 --> 00:01:16,950
of the most important parts to realize

00:01:14,040 --> 00:01:20,250
whenever you start to be interested in

00:01:16,950 --> 00:01:23,010
your new technology I time when I had a

00:01:20,250 --> 00:01:24,600
desktop computer and this computer was

00:01:23,010 --> 00:01:28,080
not connected with any other computer

00:01:24,600 --> 00:01:30,990
and only recently we really started to

00:01:28,080 --> 00:01:33,900
to to connect these systems with each

00:01:30,990 --> 00:01:35,700
other from a technological standpoint of

00:01:33,900 --> 00:01:37,409
the world wide web is not really new but

00:01:35,700 --> 00:01:40,229
the interesting thing about the world

00:01:37,409 --> 00:01:42,780
wide web is as soon as you start to

00:01:40,229 --> 00:01:45,659
connect machines together so some is

00:01:42,780 --> 00:01:48,650
much greater than the individual parts

00:01:45,659 --> 00:01:51,390
that make up the world wide web and

00:01:48,650 --> 00:01:53,790
really the value lies in connecting

00:01:51,390 --> 00:01:55,979
different realms being computers

00:01:53,790 --> 00:01:58,440
together and forming the world wide web

00:01:55,979 --> 00:02:00,690
now when we are talking about modern

00:01:58,440 --> 00:02:03,570
devices like smartphones or any kind of

00:02:00,690 --> 00:02:06,479
mobile device we are now facing a very

00:02:03,570 --> 00:02:08,729
similar situation where this technology

00:02:06,479 --> 00:02:09,619
is now being started to be connected to

00:02:08,729 --> 00:02:11,629
a new realm

00:02:09,619 --> 00:02:14,049
but this time it is not the digital

00:02:11,629 --> 00:02:17,540
realm if you will but the physical world

00:02:14,049 --> 00:02:20,239
through 3d censoring and this is really

00:02:17,540 --> 00:02:23,030
a big shift when it comes to the

00:02:20,239 --> 00:02:25,180
advancement of technology I don't know

00:02:23,030 --> 00:02:28,209
what the next big thing will be when it

00:02:25,180 --> 00:02:31,879
was regard to these mobile devices and

00:02:28,209 --> 00:02:34,129
and the perception of the physical world

00:02:31,879 --> 00:02:36,379
but i'm pretty sure that the effect of

00:02:34,129 --> 00:02:39,470
on on society and technology will be

00:02:36,379 --> 00:02:42,980
very similar in size and impact in

00:02:39,470 --> 00:02:45,590
general so what is 3d technology being

00:02:42,980 --> 00:02:47,030
used for at this point already for

00:02:45,590 --> 00:02:50,090
instance there's a startup called kiva

00:02:47,030 --> 00:02:52,760
systems they were bought by amazon just

00:02:50,090 --> 00:02:55,970
some years ago and as you can see these

00:02:52,760 --> 00:02:59,329
little orange things at the bottom these

00:02:55,970 --> 00:03:01,370
are autonomous systems who bring the

00:02:59,329 --> 00:03:03,799
individual goods that Emerson has in its

00:03:01,370 --> 00:03:06,409
warehouses to the people packaging it

00:03:03,799 --> 00:03:08,780
there's another startup called local

00:03:06,409 --> 00:03:10,760
mortos and they are really into

00:03:08,780 --> 00:03:13,459
autonomous driving and how you can use

00:03:10,760 --> 00:03:16,989
3d perception technology to have a car

00:03:13,459 --> 00:03:19,669
driven by itself so to say they are also

00:03:16,989 --> 00:03:21,590
started often companies using 3d

00:03:19,669 --> 00:03:23,510
technology for inspection and

00:03:21,590 --> 00:03:25,250
maintenance for instance here to read

00:03:23,510 --> 00:03:28,609
regions which are really hard to reach

00:03:25,250 --> 00:03:31,370
for humans or even in mining to drive

00:03:28,609 --> 00:03:34,150
these really big trucks or they are

00:03:31,370 --> 00:03:37,000
already being driven autonomously and

00:03:34,150 --> 00:03:39,859
how doin it how to achieve this through

00:03:37,000 --> 00:03:42,440
3d technology is actually not a super

00:03:39,859 --> 00:03:45,199
new thing if you will especially from a

00:03:42,440 --> 00:03:47,019
consumer perspective in 2010 Microsoft

00:03:45,199 --> 00:03:49,699
Kinect was introduced to the market

00:03:47,019 --> 00:03:51,889
featuring a 3 megapixel regular camera

00:03:49,699 --> 00:03:53,599
and the depth sensor and the the cool

00:03:51,889 --> 00:03:56,090
thing of course was was was connect

00:03:53,599 --> 00:03:59,959
which I guess plenty of you here already

00:03:56,090 --> 00:04:01,699
have used is that it that you have a

00:03:59,959 --> 00:04:03,919
completely different way to interact

00:04:01,699 --> 00:04:07,190
with a gaming console through through

00:04:03,919 --> 00:04:11,150
connect and now what is remarkable to me

00:04:07,190 --> 00:04:14,120
and in just five years i mean the the

00:04:11,150 --> 00:04:16,910
kinect required an external power supply

00:04:14,120 --> 00:04:20,000
because it took so many power that the

00:04:16,910 --> 00:04:21,850
USB bus wasn't enough to power it now

00:04:20,000 --> 00:04:24,370
there's project tango

00:04:21,850 --> 00:04:27,010
Google and just within five years they

00:04:24,370 --> 00:04:29,470
have basically shrink-wrapped here

00:04:27,010 --> 00:04:31,570
Microsoft Kinect technology into a

00:04:29,470 --> 00:04:35,950
mobile device a phablet to be precise

00:04:31,570 --> 00:04:38,380
and but but but both these devices they

00:04:35,950 --> 00:04:40,920
generally only work indoor as far as we

00:04:38,380 --> 00:04:43,540
can tell form for the project tango part

00:04:40,920 --> 00:04:46,090
there's also of course technologies that

00:04:43,540 --> 00:04:47,710
can be used outside and it's one where i

00:04:46,090 --> 00:04:49,390
can actually is working on and it is

00:04:47,710 --> 00:04:53,290
called integrated positioning system

00:04:49,390 --> 00:04:55,450
from the German Space Agency and T you

00:04:53,290 --> 00:04:58,180
can probably see this blue thing on on

00:04:55,450 --> 00:05:00,580
the helmet right over here this is the

00:04:58,180 --> 00:05:03,250
integrated positioning system and it

00:05:00,580 --> 00:05:07,900
uses two cameras to perceive the world

00:05:03,250 --> 00:05:10,870
and 3d and of course employees various

00:05:07,900 --> 00:05:13,870
other sensors the key thing to take with

00:05:10,870 --> 00:05:15,820
you here is that it because of these

00:05:13,870 --> 00:05:17,770
different sensors that it uses namely

00:05:15,820 --> 00:05:21,160
just regular camera technology basically

00:05:17,770 --> 00:05:24,340
it can not only be used in door as in

00:05:21,160 --> 00:05:26,680
case was connect but also outdoor but

00:05:24,340 --> 00:05:28,930
you probably have noticed that all of

00:05:26,680 --> 00:05:31,870
these devices have basically more than

00:05:28,930 --> 00:05:34,810
one sensor to grasp what's going on is a

00:05:31,870 --> 00:05:38,890
physical world and this is for a good

00:05:34,810 --> 00:05:42,850
reason namely we as humans we have

00:05:38,890 --> 00:05:45,550
extremely sophisticated visual system if

00:05:42,850 --> 00:05:47,440
you will and just by looking at a

00:05:45,550 --> 00:05:51,790
two-dimensional picture like this one

00:05:47,440 --> 00:05:53,350
you can get the kind of get to physical

00:05:51,790 --> 00:05:55,470
dimensions in us in a certain way at

00:05:53,350 --> 00:05:59,250
least but when we are really just

00:05:55,470 --> 00:06:01,510
considering or the computer would see

00:05:59,250 --> 00:06:04,060
well the computer would basically just

00:06:01,510 --> 00:06:08,080
get a two-dimensional picture literally

00:06:04,060 --> 00:06:10,420
of the world and would not have enough

00:06:08,080 --> 00:06:13,060
information to really get the third

00:06:10,420 --> 00:06:14,950
dimension so to say the depths so to

00:06:13,060 --> 00:06:17,050
visualize what is missing in this

00:06:14,950 --> 00:06:19,720
picture although we kind of grasp it as

00:06:17,050 --> 00:06:23,350
a human beings as being depicted in the

00:06:19,720 --> 00:06:26,110
second picture titled depth map here the

00:06:23,350 --> 00:06:28,090
white parts in the image are the parts

00:06:26,110 --> 00:06:30,460
of the of the image of the left side

00:06:28,090 --> 00:06:31,510
which are nearer to the camera so the

00:06:30,460 --> 00:06:35,080
darker the color

00:06:31,510 --> 00:06:37,350
the more away from from the perspective

00:06:35,080 --> 00:06:40,750
of the camera is the individual pixel

00:06:37,350 --> 00:06:43,060
and the reason I come up with this is to

00:06:40,750 --> 00:06:45,850
understand basically due to base the

00:06:43,060 --> 00:06:48,160
technology and the basic things for the

00:06:45,850 --> 00:06:50,140
second part of the talk and because when

00:06:48,160 --> 00:06:53,050
you combine this information that is

00:06:50,140 --> 00:06:55,270
being visualized in both images into one

00:06:53,050 --> 00:06:57,730
you get something that is called a 3d

00:06:55,270 --> 00:06:59,470
point cloud and this is basically just

00:06:57,730 --> 00:07:02,260
the extension of the two-dimensional

00:06:59,470 --> 00:07:05,440
image with a third dimension and namely

00:07:02,260 --> 00:07:09,040
the depth but again you can probably see

00:07:05,440 --> 00:07:14,430
a certain issues arising there's a very

00:07:09,040 --> 00:07:17,200
white part right over here and this is

00:07:14,430 --> 00:07:21,250
yeah this is one of the key things to

00:07:17,200 --> 00:07:22,990
take with here that even though we have

00:07:21,250 --> 00:07:25,120
a two-dimensional picture that is being

00:07:22,990 --> 00:07:27,070
added to certain mention it doesn't mean

00:07:25,120 --> 00:07:29,110
that the the image that we get from the

00:07:27,070 --> 00:07:30,430
surrounding is complete so everything

00:07:29,110 --> 00:07:34,030
that is wide over they are in the

00:07:30,430 --> 00:07:35,560
rightmost picture is basically data that

00:07:34,030 --> 00:07:39,730
is missing where we know nothing about

00:07:35,560 --> 00:07:41,950
oh this is important to understand when

00:07:39,730 --> 00:07:43,990
we come to this technology of course the

00:07:41,950 --> 00:07:45,970
image that you just saw and the the

00:07:43,990 --> 00:07:49,420
depths information this is what you can

00:07:45,970 --> 00:07:52,030
get if you use sensors like we just

00:07:49,420 --> 00:07:54,430
described in terms of connect or off the

00:07:52,030 --> 00:07:57,550
integrated positioning system when you

00:07:54,430 --> 00:08:00,490
are photographing aesthetic thing like

00:07:57,550 --> 00:08:03,520
like for instance what you can see over

00:08:00,490 --> 00:08:06,340
there you can actually derive a 3d model

00:08:03,520 --> 00:08:08,830
just by taking multiple images from

00:08:06,340 --> 00:08:11,260
different perspective and this is being

00:08:08,830 --> 00:08:13,720
briefly described here so just by having

00:08:11,260 --> 00:08:16,360
the same thing being photographed from

00:08:13,720 --> 00:08:17,680
different angles we can deduce a 3d

00:08:16,360 --> 00:08:22,540
model of the thing which is I think

00:08:17,680 --> 00:08:26,080
quite a remarkable achievement so the

00:08:22,540 --> 00:08:29,350
talk is titled it tighter challenges and

00:08:26,080 --> 00:08:30,760
the thing about 3d in general is that we

00:08:29,350 --> 00:08:32,890
are talking really about a lot of data

00:08:30,760 --> 00:08:35,830
right so for instance when we have a9

00:08:32,890 --> 00:08:38,229
mega mega pixel image with 8-bit collar

00:08:35,830 --> 00:08:40,270
and 16-bit death which is kind of

00:08:38,229 --> 00:08:42,580
state-of-the-art then each of these

00:08:40,270 --> 00:08:45,250
images has a row size of 4

00:08:42,580 --> 00:08:47,320
three megabytes which translates into

00:08:45,250 --> 00:08:49,630
more than 500 megabytes per second if we

00:08:47,320 --> 00:08:52,270
just start to capture the the world with

00:08:49,630 --> 00:08:54,790
10 images per second which is not super

00:08:52,270 --> 00:08:57,010
much right in other words when we

00:08:54,790 --> 00:08:59,710
continue with that 10 frames per second

00:08:57,010 --> 00:09:02,170
capturing of the whole world around us

00:08:59,710 --> 00:09:05,100
we end up with something like 36

00:09:02,170 --> 00:09:07,870
terabyte of data captured the day of

00:09:05,100 --> 00:09:11,080
course this is really just raw data but

00:09:07,870 --> 00:09:14,800
you can probably imagine that that this

00:09:11,080 --> 00:09:17,050
is a huge problem to deal with first

00:09:14,800 --> 00:09:19,420
question of course is what are we going

00:09:17,050 --> 00:09:22,540
to do with all these terabytes of data

00:09:19,420 --> 00:09:25,120
and more importantly of course we are

00:09:22,540 --> 00:09:26,260
mean you're not just into collecting all

00:09:25,120 --> 00:09:29,530
this data of course we want to derive

00:09:26,260 --> 00:09:30,790
information from it and even more

00:09:29,530 --> 00:09:33,220
importantly when we are talking about

00:09:30,790 --> 00:09:34,600
autonomous systems we not only want to

00:09:33,220 --> 00:09:37,480
understand what is there what we also

00:09:34,600 --> 00:09:39,250
want to weave to relate it which it

00:09:37,480 --> 00:09:42,280
means that we want to be able to search

00:09:39,250 --> 00:09:44,410
it too so that we can find things again

00:09:42,280 --> 00:09:47,250
which we have seen before for instance

00:09:44,410 --> 00:09:50,050
these are just one of the few high-level

00:09:47,250 --> 00:09:53,410
generators that one faces when dealing

00:09:50,050 --> 00:09:56,680
with 3d data and with this I would like

00:09:53,410 --> 00:09:59,260
to to introduce to the stage because he

00:09:56,680 --> 00:10:00,700
is really easy expert and he can give

00:09:59,260 --> 00:10:07,750
you more information on all these issues

00:10:00,700 --> 00:10:10,270
so um they're coming on to the question

00:10:07,750 --> 00:10:13,300
how to store to model the data

00:10:10,270 --> 00:10:16,780
structures and this is a topic where I

00:10:13,300 --> 00:10:18,880
particularly work on and this is

00:10:16,780 --> 00:10:20,560
actually a current state of the art it's

00:10:18,880 --> 00:10:23,590
about the idea how to deal with the data

00:10:20,560 --> 00:10:28,120
not about frameworks and the core

00:10:23,590 --> 00:10:30,310
concept is to apply volume modeling for

00:10:28,120 --> 00:10:34,270
our world as you can see here for

00:10:30,310 --> 00:10:36,880
somebody's Mario and a voxel is

00:10:34,270 --> 00:10:40,510
basically a 3d pixel like a cube here

00:10:36,880 --> 00:10:42,550
and the difference to do to this

00:10:40,510 --> 00:10:44,890
approach which is kind of common in

00:10:42,550 --> 00:10:47,200
computer graphics we divide the full

00:10:44,890 --> 00:10:50,170
world in voxels and represent the

00:10:47,200 --> 00:10:53,170
surface which is actually observed by

00:10:50,170 --> 00:10:56,920
the camera by interface between negative

00:10:53,170 --> 00:10:59,689
and positive values so actually we

00:10:56,920 --> 00:11:01,129
use each measurement from the camera to

00:10:59,689 --> 00:11:04,970
set this value is negative and positive

00:11:01,129 --> 00:11:07,999
and to represent the surface here as an

00:11:04,970 --> 00:11:11,660
example in 3d you can see positive boxes

00:11:07,999 --> 00:11:14,209
in green and negative in red and the

00:11:11,660 --> 00:11:16,100
surface is between them is also

00:11:14,209 --> 00:11:20,899
visualized hear correctly so actually

00:11:16,100 --> 00:11:23,449
why do we use this approach because it

00:11:20,899 --> 00:11:25,790
helps us to integrate new measurements

00:11:23,449 --> 00:11:28,129
really fast without to perform large

00:11:25,790 --> 00:11:31,220
recomputation of the environment so

00:11:28,129 --> 00:11:34,910
every time when a new image arrives from

00:11:31,220 --> 00:11:38,649
the sensor this approach helps us to

00:11:34,910 --> 00:11:42,079
integrate the new information easily and

00:11:38,649 --> 00:11:43,939
in order to store it like three based

00:11:42,079 --> 00:11:45,889
data structures are usually applied so

00:11:43,939 --> 00:11:48,920
in this case an oak tree where each

00:11:45,889 --> 00:11:51,949
voxel is stored only when there is a

00:11:48,920 --> 00:11:53,809
measurement so maybe you can think why

00:11:51,949 --> 00:11:55,040
should i store something allocate memory

00:11:53,809 --> 00:11:57,350
for something which is not being

00:11:55,040 --> 00:11:59,749
measured but this is no such trivial

00:11:57,350 --> 00:12:03,079
because for years ago we didn't even

00:11:59,749 --> 00:12:06,110
have this approach to store the data so

00:12:03,079 --> 00:12:09,709
using oak trees or binary splitting

00:12:06,110 --> 00:12:12,319
trees it's kind of a research right now

00:12:09,709 --> 00:12:14,629
but whatever right now we're in the

00:12:12,319 --> 00:12:18,429
situation that where we can store the

00:12:14,629 --> 00:12:21,470
data using linear memory allocation and

00:12:18,429 --> 00:12:23,660
we increase the access time by combining

00:12:21,470 --> 00:12:27,410
it with the hash tables so actually an

00:12:23,660 --> 00:12:29,029
octree stored in a hashmap and if you

00:12:27,410 --> 00:12:33,529
are looking for a specific coordinate

00:12:29,029 --> 00:12:35,720
like in its ready position X Y Z first

00:12:33,529 --> 00:12:37,399
of all we find the corresponding cache

00:12:35,720 --> 00:12:39,589
entry the link to the oak tree and then

00:12:37,399 --> 00:12:42,829
we find by going down the oak tree

00:12:39,589 --> 00:12:46,459
they're required like cube like a walk

00:12:42,829 --> 00:12:50,959
so and the reason why I'm Nash maps I

00:12:46,459 --> 00:12:53,329
use this when we traverse the full oak

00:12:50,959 --> 00:12:56,899
tree then the complexity of existing a

00:12:53,329 --> 00:12:59,269
data point depends on the haight on the

00:12:56,899 --> 00:13:01,730
depth of the oak tree actually so we

00:12:59,269 --> 00:13:03,799
will use it here or two to maybe maximum

00:13:01,730 --> 00:13:08,169
three levels and apply the constant

00:13:03,799 --> 00:13:11,059
search using hash maps so this is quite

00:13:08,169 --> 00:13:12,919
efficient and of course there are some

00:13:11,059 --> 00:13:14,480
demonstrations some examples where this

00:13:12,919 --> 00:13:19,189
is applied here you can see I Chappell

00:13:14,480 --> 00:13:21,949
Chappell close to Munich with a

00:13:19,189 --> 00:13:24,139
resolution of three centimeters a drone

00:13:21,949 --> 00:13:28,339
a UAV was flying around taking pictures

00:13:24,139 --> 00:13:33,290
and this tower I would just not well

00:13:28,339 --> 00:13:35,720
seen here is like zoomed in so you can

00:13:33,290 --> 00:13:39,319
see the visualized mesh of this and also

00:13:35,720 --> 00:13:41,509
what I described implicit voxel approach

00:13:39,319 --> 00:13:43,669
we are positive a green and negative a

00:13:41,509 --> 00:13:46,819
red and actually the structure in

00:13:43,669 --> 00:13:48,649
between them is shown here as a surface

00:13:46,819 --> 00:13:52,069
like it's common for computer graphic

00:13:48,649 --> 00:13:54,439
for rendering with OpenGL and the same

00:13:52,069 --> 00:13:56,569
tech technology is being applied for

00:13:54,439 --> 00:13:59,929
making from from a vehicle this is a

00:13:56,569 --> 00:14:03,019
test run from braunschweig where the

00:13:59,929 --> 00:14:06,829
stereo system was mounted with a car and

00:14:03,019 --> 00:14:11,569
I was living streets yeah along the

00:14:06,829 --> 00:14:15,259
travel path ok so what do we have now by

00:14:11,569 --> 00:14:21,589
introducing this technique right now we

00:14:15,259 --> 00:14:25,160
can access a voxel by 0.4 microseconds

00:14:21,589 --> 00:14:27,169
this enables us to model to tour to

00:14:25,160 --> 00:14:28,790
integrate the new measurements with the

00:14:27,169 --> 00:14:32,059
speed of two frames per second using

00:14:28,790 --> 00:14:34,910
this small resolution so actually did

00:14:32,059 --> 00:14:36,850
not so much and in order to be able to

00:14:34,910 --> 00:14:39,919
deploy these technologies to some

00:14:36,850 --> 00:14:43,220
realistic scenarios we would like to

00:14:39,919 --> 00:14:47,329
achieve like 40 nanoseconds of excess

00:14:43,220 --> 00:14:49,160
time and a generation of measures for

00:14:47,329 --> 00:14:52,549
visualization what I showed you in a

00:14:49,160 --> 00:14:56,480
slide before from the chapel is also

00:14:52,549 --> 00:14:58,220
another issue I mean they are approaches

00:14:56,480 --> 00:15:01,189
but they are still not scalable in order

00:14:58,220 --> 00:15:04,509
to be used in production and object

00:15:01,189 --> 00:15:07,549
recognition is there actually yeah next

00:15:04,509 --> 00:15:10,759
logic topic but it's very application

00:15:07,549 --> 00:15:14,089
dependent and yeah not covered here ok

00:15:10,759 --> 00:15:16,939
so and in summary we have this oak tree

00:15:14,089 --> 00:15:19,160
implicit voxel modeling framework and it

00:15:16,939 --> 00:15:22,339
enables us to represent

00:15:19,160 --> 00:15:24,470
very efficient geordie model and also

00:15:22,339 --> 00:15:26,920
the storage requirement is not so high

00:15:24,470 --> 00:15:29,990
so actually for 100 * hundred meters

00:15:26,920 --> 00:15:34,910
plane of three centimeters we would only

00:15:29,990 --> 00:15:40,370
require 70 megabyte yeah as a conclusion

00:15:34,910 --> 00:15:43,399
to the whole stuff today we are able to

00:15:40,370 --> 00:15:46,129
reconstruct a 3d environment from images

00:15:43,399 --> 00:15:47,990
using algorithms and some

00:15:46,129 --> 00:15:51,649
state-of-the-art computer vision

00:15:47,990 --> 00:15:55,279
techniques budget is extremely

00:15:51,649 --> 00:15:59,180
challenging and the infrastructure which

00:15:55,279 --> 00:16:01,730
is actually required to take the value

00:15:59,180 --> 00:16:04,759
of the out of the results doesn't exist

00:16:01,730 --> 00:16:07,850
so we have no standards here no

00:16:04,759 --> 00:16:10,040
providers and there are only a few

00:16:07,850 --> 00:16:13,939
internal solutions in some companies

00:16:10,040 --> 00:16:17,740
which mainly work on computer vision and

00:16:13,939 --> 00:16:22,459
in order to present the results some

00:16:17,740 --> 00:16:26,630
fast making solutions are developed just

00:16:22,459 --> 00:16:29,110
to present it's for showcases and from

00:16:26,630 --> 00:16:33,769
our opinion is that the 3d

00:16:29,110 --> 00:16:37,040
digitalization with mobile systems is

00:16:33,769 --> 00:16:40,430
today in the state like yeah the web in

00:16:37,040 --> 00:16:43,459
the 90s so I guess there is much more to

00:16:40,430 --> 00:16:46,730
come and yeah you are welcome for

00:16:43,459 --> 00:16:51,319
discussion of your ideas how would you

00:16:46,730 --> 00:16:57,130
maybe approach this issue these problems

00:16:51,319 --> 00:16:57,130
with large data and ya thang

00:17:00,910 --> 00:17:02,970

YouTube URL: https://www.youtube.com/watch?v=2deOwspoonI


