Title: Berlin Buzzwords 2015: Toke Eskildsen - Solr Sparse Faceting #bbuzz
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	netarchive.dk maintains a historical archive of Danish net resources. We are indexing its 500TB of raw data into Solr. One of the requirements is to provide faceting on several fields, the largest having billions of unique String values. Stock Solr is not capable of doing that with satisfiable performance on our hardware. Inspection of Solr's core faceting code has led to multiple performance improvements for high cardinality faceting.

- Less memory overhead, using packed counters
- Less garbage collection, reusing counters
- Better performance for small result sets, using sparse counters
- Better performance overall with distribution, rewriting fine-counting logic

Performance gains relative to stock Solr varies with result size. A rule of thumb is 2x for single shard indexes and 4x for multi shard. The principles behind the improvements will be presented and their influence on the faceting performance curve will be discussed and visualized with data from tests and production systems.

Read more:
https://2015.berlinbuzzwords.de/session/solr-sparse-faceting

About Toke Eskildsen:
https://2015.berlinbuzzwords.de/users/toke-eskildsen

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:00,000 --> 00:00:02,030
I

00:00:05,670 --> 00:00:11,600
hello thank you for coming applause

00:00:09,240 --> 00:00:11,600
already

00:00:12,389 --> 00:00:20,200
it's actually solar sparse faceting but

00:00:15,100 --> 00:00:24,640
sorry in before we start I like to

00:00:20,200 --> 00:00:26,590
inform you how this will play out so the

00:00:24,640 --> 00:00:30,189
funny things are in the end of this

00:00:26,590 --> 00:00:32,770
presentation I come from the press enter

00:00:30,189 --> 00:00:34,720
the state and university library and

00:00:32,770 --> 00:00:37,750
you're thinking what does a library do

00:00:34,720 --> 00:00:40,329
with data well the point is that the

00:00:37,750 --> 00:00:43,510
part of a job is harvesting the Danish

00:00:40,329 --> 00:00:47,410
internet if four times a year so we

00:00:43,510 --> 00:00:49,149
accumulate a lot of data and of course

00:00:47,410 --> 00:00:51,519
we stole them and recently we were

00:00:49,149 --> 00:00:53,859
allowed to try and index them so the

00:00:51,519 --> 00:00:57,999
researchers they could actually find

00:00:53,859 --> 00:01:01,059
some data and we don't have a lot of

00:00:57,999 --> 00:01:03,159
hardware a big budget for it so we

00:01:01,059 --> 00:01:05,890
bought the cheap machines relative to

00:01:03,159 --> 00:01:08,170
what we could get out from them and we

00:01:05,890 --> 00:01:10,420
crammed a lot of data down in them fair

00:01:08,170 --> 00:01:13,240
enough it works fine Solar solar works

00:01:10,420 --> 00:01:16,030
with this without a problem except that

00:01:13,240 --> 00:01:17,770
the researchers actually wanted to use

00:01:16,030 --> 00:01:23,590
more than just plain search for the data

00:01:17,770 --> 00:01:28,030
they wanted to pass it on them problem

00:01:23,590 --> 00:01:33,990
is as soon as you start faceting on high

00:01:28,030 --> 00:01:36,580
Cardinals it feels in solar well whoops

00:01:33,990 --> 00:01:40,480
those charged slide switch the bill have

00:01:36,580 --> 00:01:44,860
been reversed and we get response times

00:01:40,480 --> 00:01:48,370
which are less than ideal now this graph

00:01:44,860 --> 00:01:51,700
and I had to explain at the x axis we

00:01:48,370 --> 00:01:54,700
have the results set the numbers of hits

00:01:51,700 --> 00:01:57,400
in the result set and on the y-axis we

00:01:54,700 --> 00:02:00,750
have response times in milliseconds this

00:01:57,400 --> 00:02:05,920
is just for a single shot on our machine

00:02:00,750 --> 00:02:08,080
and it was performed on a field that is

00:02:05,920 --> 00:02:11,319
called URL from a harvested web search

00:02:08,080 --> 00:02:13,180
it website web resources which is

00:02:11,319 --> 00:02:15,879
basically just the address of the

00:02:13,180 --> 00:02:19,269
resource we have 200 millions of those

00:02:15,879 --> 00:02:20,970
in a single chart and well this is what

00:02:19,269 --> 00:02:24,630
happens

00:02:20,970 --> 00:02:27,330
so I like to go back to this life I just

00:02:24,630 --> 00:02:32,040
skipped over and say what is it that

00:02:27,330 --> 00:02:35,880
happens when you facet well faceting and

00:02:32,040 --> 00:02:38,370
so r is for four strings is really

00:02:35,880 --> 00:02:40,920
extremely simple it only has three

00:02:38,370 --> 00:02:43,820
phases at least with one shot we

00:02:40,920 --> 00:02:47,370
allocate a counter that's the first line

00:02:43,820 --> 00:02:49,430
then it run through all the results are

00:02:47,370 --> 00:02:53,880
all the documents in our results and

00:02:49,430 --> 00:02:55,920
extracts references to the terms in the

00:02:53,880 --> 00:02:58,980
facet that are in the results and we

00:02:55,920 --> 00:03:01,170
increment the counters which will give

00:02:58,980 --> 00:03:03,180
us a counter structure like the one to

00:03:01,170 --> 00:03:05,130
the right and we when we are finished

00:03:03,180 --> 00:03:08,040
incrementing all the counters from our

00:03:05,130 --> 00:03:11,720
result set well we extract the top 10 20

00:03:08,040 --> 00:03:16,290
100 1000 I don't know what we need so

00:03:11,720 --> 00:03:19,560
it's a simple thing but my leg works

00:03:16,290 --> 00:03:22,380
very well for a few million in values in

00:03:19,560 --> 00:03:24,570
this facet field as we can see we do

00:03:22,380 --> 00:03:30,000
have a bit of a problem with this 200

00:03:24,570 --> 00:03:32,340
million Cardinals you think one of the

00:03:30,000 --> 00:03:36,480
problems is that such counter structures

00:03:32,340 --> 00:03:38,730
chat tend to be a fairly large so when

00:03:36,480 --> 00:03:40,260
we're doing this these requests these

00:03:38,730 --> 00:03:42,450
are by the way three threats that are

00:03:40,260 --> 00:03:46,530
hammering at the same time just to to

00:03:42,450 --> 00:03:49,320
stress it a bit what we have here on the

00:03:46,530 --> 00:03:50,790
blue line is memory usage and what we

00:03:49,320 --> 00:03:55,920
have down in the button is garbage

00:03:50,790 --> 00:03:59,400
collecting so in and the delight of pass

00:03:55,920 --> 00:04:01,640
at the bottom those are what you call it

00:03:59,400 --> 00:04:04,290
normal garbage collections and the

00:04:01,640 --> 00:04:06,360
darker ones are the full garbage

00:04:04,290 --> 00:04:10,410
collections which means a pause in the

00:04:06,360 --> 00:04:12,989
request so that influences the result a

00:04:10,410 --> 00:04:16,130
lot and I must say account for a lot of

00:04:12,989 --> 00:04:20,340
the fluctuations in the response times

00:04:16,130 --> 00:04:22,229
now I'm sure that somebody could tune

00:04:20,340 --> 00:04:24,570
the doc garbage collector a lot better

00:04:22,229 --> 00:04:26,100
than we can especially since we haven't

00:04:24,570 --> 00:04:30,510
tuned anything at all with just using

00:04:26,100 --> 00:04:32,990
the default one but the thought is why

00:04:30,510 --> 00:04:36,780
do garbage collecting at all

00:04:32,990 --> 00:04:39,240
well if you remember the old code there

00:04:36,780 --> 00:04:41,580
were three phases allocate a counter

00:04:39,240 --> 00:04:46,020
fill the counter extracted sub X result

00:04:41,580 --> 00:04:49,440
okay simple idea let's just allocate a

00:04:46,020 --> 00:04:51,740
counter from a pool fill it extract and

00:04:49,440 --> 00:04:54,630
then deliver it back to the pool and

00:04:51,740 --> 00:04:57,240
then reach you reuse it on the next call

00:04:54,630 --> 00:04:59,100
of course we need to clear the counter

00:04:57,240 --> 00:05:00,510
between the poles but it turns out

00:04:59,100 --> 00:05:05,190
that's a lot cheaper to do than

00:05:00,510 --> 00:05:07,020
reallocating it from the heap so what we

00:05:05,190 --> 00:05:09,720
have here on the left is standard

00:05:07,020 --> 00:05:11,669
behavior with the memory and the gaps

00:05:09,720 --> 00:05:13,889
collectings and what we have here on the

00:05:11,669 --> 00:05:15,510
right is the behavior of the garbage

00:05:13,889 --> 00:05:18,510
collector after we have turned this

00:05:15,510 --> 00:05:22,580
thing on still not perfect there are

00:05:18,510 --> 00:05:26,190
some ugly spikes but we'll get to that

00:05:22,580 --> 00:05:29,370
response times i know this is messy it

00:05:26,190 --> 00:05:32,460
will be better so the blue stuff is

00:05:29,370 --> 00:05:37,110
standard solar and the pink stuff is

00:05:32,460 --> 00:05:39,330
well when we reuse the counters we

00:05:37,110 --> 00:05:42,750
cannot make I can make cannot make head

00:05:39,330 --> 00:05:44,880
or tail all this so if you used to

00:05:42,750 --> 00:05:47,190
switch to this visualization instead

00:05:44,880 --> 00:05:50,310
you'll see this more than once in this

00:05:47,190 --> 00:05:53,070
presentation in each of these boxes we

00:05:50,310 --> 00:05:56,789
have the quartiles so the top of the box

00:05:53,070 --> 00:05:59,760
is the area if 75% of the request will

00:05:56,789 --> 00:06:03,000
be served below this amount of time and

00:05:59,760 --> 00:06:05,550
as we can see the blue boxes again is

00:06:03,000 --> 00:06:09,990
without reuse of the counters and the

00:06:05,550 --> 00:06:12,449
pink boxes are will reuse we get in a

00:06:09,990 --> 00:06:14,520
better as smaller spread and the

00:06:12,449 --> 00:06:17,760
response times and the median this is

00:06:14,520 --> 00:06:24,930
the black line is moving downwards so it

00:06:17,760 --> 00:06:26,729
seems to help now there's a slight

00:06:24,930 --> 00:06:29,070
interesting thing about this because if

00:06:26,729 --> 00:06:30,840
we look at the response time with the

00:06:29,070 --> 00:06:33,930
minimum at the very bottom of these

00:06:30,840 --> 00:06:36,150
candlesticks it starts at five hundred

00:06:33,930 --> 00:06:43,530
milliseconds even for the very small

00:06:36,150 --> 00:06:45,830
result sets so why's that well the point

00:06:43,530 --> 00:06:49,380
is in our third part

00:06:45,830 --> 00:06:51,420
which repeats allocate counter or we use

00:06:49,380 --> 00:06:53,580
a counter update the counter then

00:06:51,420 --> 00:06:56,100
extract the topix results we actually

00:06:53,580 --> 00:06:59,850
its rate all the counters to the right

00:06:56,100 --> 00:07:06,900
to extract the top 25 that's a problem

00:06:59,850 --> 00:07:10,560
if we had 200 million counters so the

00:07:06,900 --> 00:07:13,650
next step we want to do is say well if

00:07:10,560 --> 00:07:15,690
we knew which counters were updated we

00:07:13,650 --> 00:07:23,310
could only count those when we needed to

00:07:15,690 --> 00:07:29,190
extract the top 25 so for this will be

00:07:23,310 --> 00:07:30,690
making a tracker besides the counter we

00:07:29,190 --> 00:07:35,940
have the tracker from the start it's

00:07:30,690 --> 00:07:40,380
empty and we update one counter that was

00:07:35,940 --> 00:07:42,900
count of three in this case good what we

00:07:40,380 --> 00:07:44,940
do here is say okay count of three it's

00:07:42,900 --> 00:07:47,070
updated we'll just put three in the

00:07:44,940 --> 00:07:50,610
tracker so now we know condo three is

00:07:47,070 --> 00:07:52,740
updated we do another update that's kind

00:07:50,610 --> 00:07:54,720
of one and the count and the tracker

00:07:52,740 --> 00:07:58,169
says now we know counter one is updated

00:07:54,720 --> 00:08:00,930
it's quite a simple principle we update

00:07:58,169 --> 00:08:03,030
the count of three once again but hey it

00:08:00,930 --> 00:08:09,510
was already updated so we don't need to

00:08:03,030 --> 00:08:12,510
update the tracker and we go on when you

00:08:09,510 --> 00:08:14,789
have finished this in collected collect

00:08:12,510 --> 00:08:17,400
all the counts face we now have tracker

00:08:14,789 --> 00:08:21,240
which this gives us exactly the the

00:08:17,400 --> 00:08:22,919
counters which has been updated and it

00:08:21,240 --> 00:08:24,660
looks like this in the code the yellow

00:08:22,919 --> 00:08:28,530
parts are the new parts of the codes

00:08:24,660 --> 00:08:30,870
which changed from the previous one by

00:08:28,530 --> 00:08:33,630
the way am i right we are very very

00:08:30,870 --> 00:08:38,339
quickly exiting standard solar lent now

00:08:33,630 --> 00:08:40,020
I won't go through code but as you can

00:08:38,339 --> 00:08:42,060
see it doesn't take much to add this

00:08:40,020 --> 00:08:44,520
counter method there's a core of course

00:08:42,060 --> 00:08:47,040
it down side to counting it introduces

00:08:44,520 --> 00:08:49,920
an extra step in updating the values

00:08:47,040 --> 00:08:52,020
which has a speed penalty and in it

00:08:49,920 --> 00:08:54,980
introduces a memory overhead which

00:08:52,020 --> 00:08:57,570
depends on how large you want to count

00:08:54,980 --> 00:08:59,610
we found out that about eight percent of

00:08:57,570 --> 00:09:01,260
the the counter size works fine

00:08:59,610 --> 00:09:04,890
for this tracker mechanism beyond that

00:09:01,260 --> 00:09:07,320
there's really no win in it and the

00:09:04,890 --> 00:09:08,910
speed penalty well again as long as the

00:09:07,320 --> 00:09:16,200
count tracker is below a certain point

00:09:08,910 --> 00:09:18,690
we win so this is a result the blue

00:09:16,200 --> 00:09:21,480
stuff is when we do not do you this

00:09:18,690 --> 00:09:23,910
sparse faceting which is all about the

00:09:21,480 --> 00:09:27,380
500 milliseconds response time and the

00:09:23,910 --> 00:09:31,860
pink stuff is when we turn on this pass

00:09:27,380 --> 00:09:34,110
this looks like a hockey stick now the

00:09:31,860 --> 00:09:39,540
good thing about this is that this area

00:09:34,110 --> 00:09:41,550
oh I can't use this we have here down in

00:09:39,540 --> 00:09:43,350
the middle where everything is good and

00:09:41,550 --> 00:09:45,420
perfect and we have this area where

00:09:43,350 --> 00:09:48,390
things sent to go thoughts for the very

00:09:45,420 --> 00:09:52,279
last result sets luckily most clear is

00:09:48,390 --> 00:09:54,750
tipped to be down in this very low area

00:09:52,279 --> 00:09:58,320
well depending on your Corp will send

00:09:54,750 --> 00:10:00,720
you search patterns and and all that so

00:09:58,320 --> 00:10:03,839
we're happy we removed the garbage

00:10:00,720 --> 00:10:07,649
problem or at least minimized it we got

00:10:03,839 --> 00:10:12,750
the response times down so and this

00:10:07,649 --> 00:10:16,260
works but in these days Solar is snow so

00:10:12,750 --> 00:10:20,670
Lars alone practically it's all in solar

00:10:16,260 --> 00:10:23,339
cloud what happens in the solar cloud

00:10:20,670 --> 00:10:26,519
well there's an extra step to faceting

00:10:23,339 --> 00:10:29,100
when when doing cloud searches we have a

00:10:26,519 --> 00:10:33,420
face one it's exactly as a single shot

00:10:29,100 --> 00:10:35,279
set up to the facet pole maybe ask for a

00:10:33,420 --> 00:10:37,860
bit more more faces than the one you

00:10:35,279 --> 00:10:39,899
request because well statistics and all

00:10:37,860 --> 00:10:43,740
that but basically do a standard

00:10:39,899 --> 00:10:46,230
physical then the calling note it gets

00:10:43,740 --> 00:10:49,140
resolved back it merges them it extracts

00:10:46,230 --> 00:10:52,019
it calculates to the top 25 whatever we

00:10:49,140 --> 00:10:55,440
use and then we have the list small

00:10:52,019 --> 00:10:57,449
problem that we are not sure that we

00:10:55,440 --> 00:10:59,250
have the right counts because some of

00:10:57,449 --> 00:11:03,740
the shots might not have returned

00:10:59,250 --> 00:11:08,930
account for the term that was in top 25

00:11:03,740 --> 00:11:11,970
good so the simple thing is well the

00:11:08,930 --> 00:11:12,940
merger says those charts that did not

00:11:11,970 --> 00:11:16,000
deliver

00:11:12,940 --> 00:11:19,150
a result for for the top one of the top

00:11:16,000 --> 00:11:22,300
25 terms or more of them well they get

00:11:19,150 --> 00:11:24,400
to recount them the way that is done in

00:11:22,300 --> 00:11:27,190
vanilla solar we see in this code

00:11:24,400 --> 00:11:31,030
section here so the code of course I'm

00:11:27,190 --> 00:11:33,310
lying a bit but it's really simple we

00:11:31,030 --> 00:11:35,980
just for each term that we need the fine

00:11:33,310 --> 00:11:38,890
count from four in this yard we perform

00:11:35,980 --> 00:11:41,590
an extremely simple search which is just

00:11:38,890 --> 00:11:45,310
for the field with the term and then do

00:11:41,590 --> 00:11:52,570
an intersection i think the word is with

00:11:45,310 --> 00:11:58,000
the full results sets well it works but

00:11:52,570 --> 00:12:00,910
it has a peculiar property excuse me see

00:11:58,000 --> 00:12:03,070
now these are response times again this

00:12:00,910 --> 00:12:06,880
time we switched to cloud mode we have

00:12:03,070 --> 00:12:09,340
nine shots here and be quiet but on the

00:12:06,880 --> 00:12:11,740
other hand just to scale it down scale

00:12:09,340 --> 00:12:14,560
up in shots scale down in faceting I'll

00:12:11,740 --> 00:12:17,380
feel now only has 1 million values it's

00:12:14,560 --> 00:12:22,450
a domain field if in our index by the

00:12:17,380 --> 00:12:25,330
way to the right we see this natural

00:12:22,450 --> 00:12:27,760
well as the count goes up well the

00:12:25,330 --> 00:12:30,190
response time goes up but what is

00:12:27,760 --> 00:12:32,440
happening in the middle because our nice

00:12:30,190 --> 00:12:34,660
response times which were down below 50

00:12:32,440 --> 00:12:38,290
milliseconds well we have a lot of them

00:12:34,660 --> 00:12:41,640
up in the seconds that's pure statistics

00:12:38,290 --> 00:12:44,200
if you had nine shots that deliver

00:12:41,640 --> 00:12:47,230
results and they have very small result

00:12:44,200 --> 00:12:50,500
sets well chances are that each shot has

00:12:47,230 --> 00:12:53,080
the live has returned less than the top

00:12:50,500 --> 00:12:55,720
25 it would ask it has returned a

00:12:53,080 --> 00:12:58,510
complete set of facets possible for this

00:12:55,720 --> 00:13:00,250
result set in that case it won't be

00:12:58,510 --> 00:13:02,710
asked again because the merchant knows

00:13:00,250 --> 00:13:07,390
it has no more information that's over

00:13:02,710 --> 00:13:09,600
here to the left when we have more more

00:13:07,390 --> 00:13:14,020
hits well we have the opposite situation

00:13:09,600 --> 00:13:17,710
the chances are that the top 25 terms

00:13:14,020 --> 00:13:19,180
from each shot will be the same and as

00:13:17,710 --> 00:13:21,400
they have already delivered counter

00:13:19,180 --> 00:13:24,310
that's that term there's no need to ask

00:13:21,400 --> 00:13:25,640
and again so be happy here but in the

00:13:24,310 --> 00:13:28,190
middle we have this

00:13:25,640 --> 00:13:30,260
pit of pain which was coined by Angie

00:13:28,190 --> 00:13:32,120
Jackson from the British Library which

00:13:30,260 --> 00:13:34,400
unfortunately means that statistically

00:13:32,120 --> 00:13:37,700
nearly every term needs to be fine

00:13:34,400 --> 00:13:40,010
counted from nearly every shot and if

00:13:37,700 --> 00:13:44,840
you have a top 25 at behind niall charts

00:13:40,010 --> 00:13:48,080
it's our uh no above 200 request that we

00:13:44,840 --> 00:13:53,240
need to add or this request have a lot

00:13:48,080 --> 00:13:56,030
of these intersections so that's bad one

00:13:53,240 --> 00:14:00,590
solution which actually works pretty

00:13:56,030 --> 00:14:02,570
well is not to find count but for this

00:14:00,590 --> 00:14:09,650
talk we are focusing on getting exact

00:14:02,570 --> 00:14:12,620
results what if we made it more

00:14:09,650 --> 00:14:15,230
efficient to find count because as we

00:14:12,620 --> 00:14:18,350
remember faceting the first phase of

00:14:15,230 --> 00:14:22,520
assessing was quite fast so we could

00:14:18,350 --> 00:14:25,640
just repeat that and then as we have the

00:14:22,520 --> 00:14:28,220
counts is rate each term one at the time

00:14:25,640 --> 00:14:33,470
and just look it up in the counter

00:14:28,220 --> 00:14:37,400
structure which we have now okay that

00:14:33,470 --> 00:14:40,340
works that was a level one of this until

00:14:37,400 --> 00:14:43,760
we realized that the actually we already

00:14:40,340 --> 00:14:46,240
did the counting in phase one if you

00:14:43,760 --> 00:14:49,010
just remember the results the the

00:14:46,240 --> 00:14:51,440
counter structure from phase 1 all the

00:14:49,010 --> 00:14:55,700
counts are there so we just need to take

00:14:51,440 --> 00:15:03,710
it from a pool and then get the counts

00:14:55,700 --> 00:15:06,080
directly sounds efficient and this so by

00:15:03,710 --> 00:15:08,930
doing this little trick looking up in

00:15:06,080 --> 00:15:12,380
another way we are virtually eliminating

00:15:08,930 --> 00:15:18,350
this pit of pain this this large hill in

00:15:12,380 --> 00:15:25,430
the middle that was about it for

00:15:18,350 --> 00:15:29,870
performance now we have a setup we had a

00:15:25,430 --> 00:15:31,940
200 million values and it's it's all

00:15:29,870 --> 00:15:35,120
about the memory and the worst thing is

00:15:31,940 --> 00:15:38,030
that URL is just one of our fields one

00:15:35,120 --> 00:15:40,480
other field we really like is the links

00:15:38,030 --> 00:15:45,260
outgoing links from webpage

00:15:40,480 --> 00:15:48,560
which has 600 million unique values per

00:15:45,260 --> 00:15:50,330
shot and it's reused a lot so we have

00:15:48,560 --> 00:15:53,810
about 6 billion references from

00:15:50,330 --> 00:15:56,180
documents to these values now the number

00:15:53,810 --> 00:15:57,800
of references doesn't really matter with

00:15:56,180 --> 00:16:00,200
regard to memory I'm just mentioning to

00:15:57,800 --> 00:16:03,860
get the complete picture but the

00:16:00,200 --> 00:16:06,470
cardinality of of the terms certainly

00:16:03,860 --> 00:16:13,640
matters as well as how many document a

00:16:06,470 --> 00:16:15,140
single term can be hot belong to well we

00:16:13,640 --> 00:16:16,730
still want to fast it on this if it's

00:16:15,140 --> 00:16:18,380
possible because the researchers would

00:16:16,730 --> 00:16:21,850
like to build nice graphs between web

00:16:18,380 --> 00:16:25,820
pages in a quick and responsive manner

00:16:21,850 --> 00:16:28,250
so and we don't want to to allocate a

00:16:25,820 --> 00:16:33,920
lot of memory so maybe we could look at

00:16:28,250 --> 00:16:38,030
that see if you try and explode such a

00:16:33,920 --> 00:16:40,310
if what you call a FS facet fields we

00:16:38,030 --> 00:16:42,470
have three examples in our corpus one

00:16:40,310 --> 00:16:44,900
was the domain which is fairly low cut

00:16:42,470 --> 00:16:46,940
in LC with the 1 million values the

00:16:44,900 --> 00:16:48,860
other one why do L with 200 million and

00:16:46,940 --> 00:16:51,680
the third one will links with about 600

00:16:48,860 --> 00:16:53,510
billions this view this histogram view

00:16:51,680 --> 00:16:57,260
you can see it directly in the solar

00:16:53,510 --> 00:16:59,720
admin front end I really recommend it it

00:16:57,260 --> 00:17:01,670
says a lot what it says among other

00:16:59,720 --> 00:17:04,820
things is that we have a lot of values

00:17:01,670 --> 00:17:06,740
which you only count 21 they're unique

00:17:04,820 --> 00:17:10,430
in the index only a single document has

00:17:06,740 --> 00:17:12,530
them that's the one on the top what we

00:17:10,430 --> 00:17:14,990
can also see I'm sorry I don't think

00:17:12,530 --> 00:17:16,790
half of you can see that is that at

00:17:14,990 --> 00:17:20,030
least four links well we have a few

00:17:16,790 --> 00:17:23,840
values that are references referenced a

00:17:20,030 --> 00:17:25,790
lot of places which is about by a lot I

00:17:23,840 --> 00:17:27,110
mean two percent of the pages or

00:17:25,790 --> 00:17:29,770
something like that I think it's a

00:17:27,110 --> 00:17:35,570
google tracker thingy or something or

00:17:29,770 --> 00:17:39,980
robots.txt arenal so how these are

00:17:35,570 --> 00:17:42,980
represented well ideally in an ideal

00:17:39,980 --> 00:17:46,070
world the Platonic ideal of a counter

00:17:42,980 --> 00:17:49,370
well if we have at the bottom the

00:17:46,070 --> 00:17:51,650
different terms at at the top we have

00:17:49,370 --> 00:17:53,070
some bits needed to count them we can

00:17:51,650 --> 00:17:55,710
see here that

00:17:53,070 --> 00:18:00,570
a only requires one bit because there's

00:17:55,710 --> 00:18:04,400
only one pace 2010b requires three bits

00:18:00,570 --> 00:18:08,550
so we know there is a least what is that

00:18:04,400 --> 00:18:10,650
57 there's a maximum of seven pages that

00:18:08,550 --> 00:18:13,110
points to it we can we know these maxima

00:18:10,650 --> 00:18:16,620
because well it's an index we can just

00:18:13,110 --> 00:18:20,610
analyze it but this ideal world has

00:18:16,620 --> 00:18:23,820
little to do with reality know how its

00:18:20,610 --> 00:18:26,910
represented in the classic vanilla solar

00:18:23,820 --> 00:18:31,650
is simple we just allocate one integer

00:18:26,910 --> 00:18:33,600
to count each term okay it's not a

00:18:31,650 --> 00:18:36,360
problem with low cardinality because who

00:18:33,600 --> 00:18:39,420
cares if it's no one megabyte or two

00:18:36,360 --> 00:18:41,520
megabyte but when we're talking about a

00:18:39,420 --> 00:18:42,960
cardinality of 200 million then we

00:18:41,520 --> 00:18:45,960
suddenly have a counter structure of

00:18:42,960 --> 00:18:47,580
nearly eight hundred megabytes to make

00:18:45,960 --> 00:18:49,260
matters worse we need one counter

00:18:47,580 --> 00:18:54,930
structure for each concurrent requests

00:18:49,260 --> 00:18:59,100
so well it all counts up first thing

00:18:54,930 --> 00:19:01,110
with it it was pretty simple if you know

00:18:59,100 --> 00:19:03,510
if we look at the structure to the left

00:19:01,110 --> 00:19:06,420
this by the way goes to 32 which is

00:19:03,510 --> 00:19:09,360
about the slides to visualize the wasted

00:19:06,420 --> 00:19:11,370
space in red well we can see that the

00:19:09,360 --> 00:19:14,520
highest counter in this example only

00:19:11,370 --> 00:19:16,580
takes 11 bits but we used 32 bits for

00:19:14,520 --> 00:19:21,060
all the counters that's a waste of space

00:19:16,580 --> 00:19:23,040
so simple enough we just use 11 bits for

00:19:21,060 --> 00:19:26,850
the counters that's the maximum we will

00:19:23,040 --> 00:19:28,740
need to count works fine if you look at

00:19:26,850 --> 00:19:31,800
the statistics from our sample chart we

00:19:28,740 --> 00:19:33,720
can see that the well for URL which is a

00:19:31,800 --> 00:19:35,940
nice fit we get int down to half the

00:19:33,720 --> 00:19:38,310
mirror usage which and for links we sell

00:19:35,940 --> 00:19:40,650
is a better fit because we have very

00:19:38,310 --> 00:19:43,620
high counter some some a few very high

00:19:40,650 --> 00:19:46,410
counters we only get down to 75 percent

00:19:43,620 --> 00:19:52,710
of many memory usage so not really a

00:19:46,410 --> 00:19:55,840
game changer so

00:19:52,710 --> 00:19:58,240
that will come to the fun part because

00:19:55,840 --> 00:20:02,860
as everything red is wasted let's try

00:19:58,240 --> 00:20:05,200
and minimize that this is an idea that i

00:20:02,860 --> 00:20:08,710
called in plain counting and glad of a

00:20:05,200 --> 00:20:10,630
peasant better name it really in

00:20:08,710 --> 00:20:13,360
principle is very simple and it doesn't

00:20:10,630 --> 00:20:14,590
really work which we'll see later but we

00:20:13,360 --> 00:20:18,520
need this as a building blocks for

00:20:14,590 --> 00:20:21,370
lipstick next step what we have held

00:20:18,520 --> 00:20:23,740
here in the button is simple the first

00:20:21,370 --> 00:20:26,230
bits of all the counters that's plain

00:20:23,740 --> 00:20:28,210
one if you look at the ideal or to the

00:20:26,230 --> 00:20:31,780
left well every counter needs at least

00:20:28,210 --> 00:20:35,200
one bit fine so we have them on the

00:20:31,780 --> 00:20:38,800
right to now for all these counters that

00:20:35,200 --> 00:20:42,640
needs more than one bit we keep track of

00:20:38,800 --> 00:20:46,750
that by having a second an overflow

00:20:42,640 --> 00:20:49,420
bitmap on top this is the blue line for

00:20:46,750 --> 00:20:51,700
every slightly darker blue square we

00:20:49,420 --> 00:20:56,820
have here that means that the counter it

00:20:51,700 --> 00:20:56,820
belongs to continues on the upper Plains

00:20:56,970 --> 00:21:01,900
okay this is something the blue

00:21:00,130 --> 00:21:05,470
structures are static when we open an

00:21:01,900 --> 00:21:09,400
index is we can construct them so how do

00:21:05,470 --> 00:21:11,140
we use this well this is the same

00:21:09,400 --> 00:21:13,780
illustration as before I'll try

00:21:11,140 --> 00:21:19,000
incrementing a counter here in this case

00:21:13,780 --> 00:21:21,880
counter l so first time counter Ellis

00:21:19,000 --> 00:21:25,540
visits it we just set the pits at the

00:21:21,880 --> 00:21:29,980
first plane simple enough now counter

00:21:25,540 --> 00:21:34,030
LLS visits it again what happens is we

00:21:29,980 --> 00:21:36,780
visit counter l ok it's said that means

00:21:34,030 --> 00:21:40,170
we need to set the next bit higher up

00:21:36,780 --> 00:21:42,670
well we can see that it overflows

00:21:40,170 --> 00:21:45,250
hopefully it should because I will have

00:21:42,670 --> 00:21:49,780
an error and we can't film the lift with

00:21:45,250 --> 00:21:53,830
the blue pits that says 1 2 3 4 5 ok we

00:21:49,780 --> 00:21:57,190
go to plane to at entry 1 2 3 4 5 are we

00:21:53,830 --> 00:22:00,490
know that L continues here so that was

00:21:57,190 --> 00:22:03,790
simple and that means we know where to

00:22:00,490 --> 00:22:06,160
set the bit number two because we

00:22:03,790 --> 00:22:09,340
counted 22 now and

00:22:06,160 --> 00:22:12,100
count to three well then we just put

00:22:09,340 --> 00:22:14,260
down in El and so on so forth so now we

00:22:12,100 --> 00:22:18,220
can count to 12 and it's represented

00:22:14,260 --> 00:22:21,250
that way in the planes to make this work

00:22:18,220 --> 00:22:24,550
we need to be really quick at counting

00:22:21,250 --> 00:22:26,890
these overflow bits fortunately there's

00:22:24,550 --> 00:22:29,040
a function called rank which with very

00:22:26,890 --> 00:22:32,170
low overhead three percent of memory and

00:22:29,040 --> 00:22:36,730
dust that in constant time so no worries

00:22:32,170 --> 00:22:42,640
there the problem was that it did not

00:22:36,730 --> 00:22:45,220
work because we use tracking you

00:22:42,640 --> 00:22:47,290
remember the tracker is very simple if

00:22:45,220 --> 00:22:51,190
the previous value of the counter was 0

00:22:47,290 --> 00:22:53,980
update the tracker but how do we know

00:22:51,190 --> 00:22:57,120
that the previous value of the counter

00:22:53,980 --> 00:23:00,490
is 0 in this scenario turns out we don't

00:22:57,120 --> 00:23:02,380
because with this setup when we have

00:23:00,490 --> 00:23:05,920
counted to 12 and we try to incremental

00:23:02,380 --> 00:23:09,520
again we cannot see if this is 0

00:23:05,920 --> 00:23:11,230
represents 0 I'll something else unless

00:23:09,520 --> 00:23:17,160
we read all the way up in the planes

00:23:11,230 --> 00:23:17,160
which is dark slow what we do about that

00:23:17,670 --> 00:23:32,580
well we introduce a plus one and one and

00:23:22,630 --> 00:23:35,140
a half bit plain good you got it this

00:23:32,580 --> 00:23:38,350
performance wise it will work i can i

00:23:35,140 --> 00:23:40,450
can tell you in memory wise it has a

00:23:38,350 --> 00:23:44,530
worst-case overhead of one bit per

00:23:40,450 --> 00:23:46,990
counter which is what it is fortunately

00:23:44,530 --> 00:23:50,020
as we remember most of our counters two

00:23:46,990 --> 00:23:52,990
thirds and is in this when we're talking

00:23:50,020 --> 00:23:55,090
about links were really only one bit and

00:23:52,990 --> 00:23:57,130
all the one bit counters they take up

00:23:55,090 --> 00:23:58,870
exactly as much space and then the

00:23:57,130 --> 00:24:00,310
previous version of this structure so

00:23:58,870 --> 00:24:02,920
it's really all the counters that are

00:24:00,310 --> 00:24:05,650
above one that takes this one extra bit

00:24:02,920 --> 00:24:08,410
hit so what you have in the bottom plane

00:24:05,650 --> 00:24:11,380
is one that kids track of is the counter

00:24:08,410 --> 00:24:13,690
one or more and above that you have the

00:24:11,380 --> 00:24:16,150
standard counters that was confusing

00:24:13,690 --> 00:24:17,630
fortunately we'll just do the same

00:24:16,150 --> 00:24:21,420
example

00:24:17,630 --> 00:24:23,550
okay we wizard counter ill we updated

00:24:21,420 --> 00:24:26,610
once we set the bit we are happy to

00:24:23,550 --> 00:24:30,260
update the tracker now we try and update

00:24:26,610 --> 00:24:33,570
counter air again in this case we see oh

00:24:30,260 --> 00:24:37,470
this bit is already said let's not touch

00:24:33,570 --> 00:24:41,040
that instead we move up at once do the

00:24:37,470 --> 00:24:44,790
counting thing and update the entry up

00:24:41,040 --> 00:24:47,090
in the one-and-a-half bit plain so this

00:24:44,790 --> 00:24:50,340
big pattern confusing at it may seem

00:24:47,090 --> 00:24:55,650
actually represents the number 2 its 1

00:24:50,340 --> 00:24:59,400
plus 2 power to the power function of

00:24:55,650 --> 00:25:01,860
one minus one so up here in plain two

00:24:59,400 --> 00:25:04,800
three and four we have a standard a bit

00:25:01,860 --> 00:25:06,720
representation by Nibiru system and in

00:25:04,800 --> 00:25:11,190
the the bottom which simply has a flag

00:25:06,720 --> 00:25:14,760
saying plus one so let's update it once

00:25:11,190 --> 00:25:17,430
again we hit the the l we don't charge

00:25:14,760 --> 00:25:19,710
that we go sorry in plain one we go to

00:25:17,430 --> 00:25:22,980
plane to and see okay there's a set bit

00:25:19,710 --> 00:25:26,460
we need to change that bit pattern 22

00:25:22,980 --> 00:25:28,920
and this make is 3s again so we updated

00:25:26,460 --> 00:25:31,860
once again and once again and all that

00:25:28,920 --> 00:25:39,030
absolutely we had swell well which is

00:25:31,860 --> 00:25:41,670
represented in this funny way okay now

00:25:39,030 --> 00:25:44,190
this is all well and good but it's a lot

00:25:41,670 --> 00:25:48,030
of Hoops to rack up through so what does

00:25:44,190 --> 00:25:50,550
it gives us well the point is it's quite

00:25:48,030 --> 00:25:53,700
a massive space saving if we look to the

00:25:50,550 --> 00:25:56,490
right and ce4 links which of course is

00:25:53,700 --> 00:25:59,460
our poster child for this we get it down

00:25:56,490 --> 00:26:01,740
to 300 megabytes to count six hundred

00:25:59,460 --> 00:26:04,200
million values as opposed to if we had

00:26:01,740 --> 00:26:06,540
just used the integer counter in in

00:26:04,200 --> 00:26:10,560
plain vanilla sola which would be above

00:26:06,540 --> 00:26:14,190
two gigabytes okay there are more good

00:26:10,560 --> 00:26:16,260
news as remember that the the blue stuff

00:26:14,190 --> 00:26:19,470
the overflow bits they were actually

00:26:16,260 --> 00:26:22,050
static they never changed so that really

00:26:19,470 --> 00:26:23,880
means that if we need another counter

00:26:22,050 --> 00:26:26,130
because we have multiple concurrent

00:26:23,880 --> 00:26:29,360
requests we only need about half the

00:26:26,130 --> 00:26:32,120
space so that means that in

00:26:29,360 --> 00:26:34,490
slightly less than 200 megabytes for the

00:26:32,120 --> 00:26:38,530
extra counters we can do the counting of

00:26:34,490 --> 00:26:41,870
these six hundred million values okay

00:26:38,530 --> 00:26:47,540
nice we save memory how hot does it hit

00:26:41,870 --> 00:26:50,990
us performance-wise well here are three

00:26:47,540 --> 00:26:54,950
comparisons the blue one is the integer

00:26:50,990 --> 00:26:58,640
representation the purple one was the

00:26:54,950 --> 00:27:00,590
maximum bit representation we use the

00:26:58,640 --> 00:27:03,140
structure called pectins from France

00:27:00,590 --> 00:27:05,240
Allah and the green one is the in-plane

00:27:03,140 --> 00:27:08,450
representation and as we can see not

00:27:05,240 --> 00:27:10,970
surprisingly there is an overhead with

00:27:08,450 --> 00:27:14,270
with using the same plane representation

00:27:10,970 --> 00:27:17,059
but on the other hand we can actually do

00:27:14,270 --> 00:27:21,020
it in quite a little quad little

00:27:17,059 --> 00:27:24,410
requiring memory so we have all these

00:27:21,020 --> 00:27:26,809
nice stuff those nice things so and we

00:27:24,410 --> 00:27:29,020
have this little trust our let's let's

00:27:26,809 --> 00:27:32,030
try and combine it all see what happens

00:27:29,020 --> 00:27:34,610
so we have these three fields domain URL

00:27:32,030 --> 00:27:38,540
and links with this cardinality we also

00:27:34,610 --> 00:27:40,190
have nine shots and in order to get

00:27:38,540 --> 00:27:46,820
anything going with just hitting it with

00:27:40,190 --> 00:27:48,350
free concurrent requests oh well the

00:27:46,820 --> 00:27:50,929
problem is that what we have under the

00:27:48,350 --> 00:27:53,480
y-axis is is actually moving in two

00:27:50,929 --> 00:27:56,450
minutes no not on this chart but it

00:27:53,480 --> 00:28:00,620
keeps going we have a worst case of I

00:27:56,450 --> 00:28:04,340
don't know five ten minutes so that was

00:28:00,620 --> 00:28:07,100
not so good on the other hand we can get

00:28:04,340 --> 00:28:10,490
by with this faceting on this one of the

00:28:07,100 --> 00:28:14,049
shots with 8 gigabytes of heat memory

00:28:10,490 --> 00:28:19,010
for nine hundred gigabytes in of shots

00:28:14,049 --> 00:28:21,470
with this so we now have two situations

00:28:19,010 --> 00:28:24,410
we have fast for smaller cardinality by

00:28:21,470 --> 00:28:26,480
which I mean below 200 million and we

00:28:24,410 --> 00:28:32,960
have memory saving for higher

00:28:26,480 --> 00:28:36,790
cardinality and we can use that that's

00:28:32,960 --> 00:28:39,700
not all there are tricks but

00:28:36,790 --> 00:28:43,780
frankly I don't think I have the time to

00:28:39,700 --> 00:28:57,220
go through them instead I think that I

00:28:43,780 --> 00:29:01,800
will let's see say try this at all now

00:28:57,220 --> 00:29:01,800

YouTube URL: https://www.youtube.com/watch?v=gcwOh9z4tj4


