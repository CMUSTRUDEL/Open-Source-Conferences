Title: Berlin Buzzwords 2015: Andre Lynum - Beyond Significant Terms #bbuzz
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	In Comperio we work on projects that aim to learn from the documents and social activity published on the web. The challenge is to summarize recent activity, drawing together current events with past activity and finding new sources to learn from. We base our on ElasticSearch and its significant terms technology. 

In this presentation we show how we expand on the base functionality provided in ElasticSearch to focus on areas such as immediate trends, entity identification and topic building using additional techniques from Information Retrieval (IR) and Natural Language Processing (NLP).

Reda more:
https://2015.berlinbuzzwords.de/session/beyond-significant-terms

About Andre Lynum:
https://2015.berlinbuzzwords.de/users/andre-lynum

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,700 --> 00:00:13,580
so I call this talk be honest

00:00:10,590 --> 00:00:16,190
maybe a bit ambitious title but

00:00:13,580 --> 00:00:21,320
it's about using elastic search for

00:00:16,190 --> 00:00:23,439
level analysis so first briefly about me

00:00:21,320 --> 00:00:25,610
and the company i work in compare you

00:00:23,439 --> 00:00:28,970
comparison a region consulting company

00:00:25,610 --> 00:00:31,939
specializing in search and related

00:00:28,970 --> 00:00:34,460
technologies it focuses and search

00:00:31,939 --> 00:00:38,510
recommendations analytics and other

00:00:34,460 --> 00:00:40,460
related technologies aside from that I'm

00:00:38,510 --> 00:00:43,790
a current 50 candidate in natural

00:00:40,460 --> 00:00:46,460
language processing focusing in machine

00:00:43,790 --> 00:00:49,460
learning and statistical techniques and

00:00:46,460 --> 00:00:51,860
I work in compare our work on projects

00:00:49,460 --> 00:00:59,330
using elastic certainty for J among

00:00:51,860 --> 00:01:05,540
other things so looking a bit into the

00:00:59,330 --> 00:01:08,600
talk we're going to look at how search

00:01:05,540 --> 00:01:11,270
it's a great metaphor for making robust

00:01:08,600 --> 00:01:15,439
analysis we'd have it right a few

00:01:11,270 --> 00:01:16,610
sources and hold aggregations and

00:01:15,439 --> 00:01:19,130
especially the significant turf

00:01:16,610 --> 00:01:23,440
segregation gives us a real-time

00:01:19,130 --> 00:01:23,440
retrieval of key lexical statistics and

00:01:25,630 --> 00:01:30,130
considering time you know as time allows

00:01:27,950 --> 00:01:36,860
we're going to look at some examples of

00:01:30,130 --> 00:01:39,860
lexical analysis so this is perhaps not

00:01:36,860 --> 00:01:41,480
a new for anybody here but elastic

00:01:39,860 --> 00:01:46,850
search is a search engine free text

00:01:41,480 --> 00:01:50,090
search engine and it focuses on data

00:01:46,850 --> 00:01:53,960
aggregation and analytics and has kind

00:01:50,090 --> 00:01:57,170
of moved on from research roots into

00:01:53,960 --> 00:02:02,560
this kind of areas it's also a highly

00:01:57,170 --> 00:02:04,610
scalable kind of no SQL data store so

00:02:02,560 --> 00:02:09,590
what I'm going to talk about here is

00:02:04,610 --> 00:02:11,030
based on a current project I'm sad to

00:02:09,590 --> 00:02:12,319
say that I can't really say that much

00:02:11,030 --> 00:02:16,670
about it this is kind of a one of the

00:02:12,319 --> 00:02:18,860
screens and but it's an open source

00:02:16,670 --> 00:02:23,300
intelligence collection and analysis

00:02:18,860 --> 00:02:25,709
tool that is open source as in public

00:02:23,300 --> 00:02:28,269
information

00:02:25,709 --> 00:02:33,069
and the project places them rather tight

00:02:28,269 --> 00:02:35,140
constraints on how we solve problems the

00:02:33,069 --> 00:02:38,560
scope is rather ambitious and and we

00:02:35,140 --> 00:02:46,000
don't really have too much money to

00:02:38,560 --> 00:02:48,069
spend and also we had to handle some

00:02:46,000 --> 00:02:53,799
languages that are not really commonly

00:02:48,069 --> 00:02:56,170
available in in tools and also we have

00:02:53,799 --> 00:02:58,319
to handle some sources and document

00:02:56,170 --> 00:03:01,739
types that are also kind of a bit

00:02:58,319 --> 00:03:06,879
different from what you might normally a

00:03:01,739 --> 00:03:09,730
normal meet so just briefly gonna to

00:03:06,879 --> 00:03:11,980
introduce the setting this is a kind of

00:03:09,730 --> 00:03:14,530
detective pipeline it has RabbitMQ

00:03:11,980 --> 00:03:18,280
ingestion pipeline there's an enrichment

00:03:14,530 --> 00:03:20,620
step and it populates elastic search

00:03:18,280 --> 00:03:23,260
indexes in near forge a graph database

00:03:20,620 --> 00:03:26,969
there's a application layer which feeds

00:03:23,260 --> 00:03:37,629
an analysis front and then and and

00:03:26,969 --> 00:03:40,510
various external feeds so and in this

00:03:37,629 --> 00:03:41,799
context to this application significant

00:03:40,510 --> 00:03:43,180
terms and other aggregations

00:03:41,799 --> 00:03:45,250
elasticsearch are kind of fit perfectly

00:03:43,180 --> 00:03:53,889
with our philosophy of making kind of

00:03:45,250 --> 00:03:56,949
dump systems act intelligent so kind of

00:03:53,889 --> 00:03:58,540
you might want to call this the beauty

00:03:56,949 --> 00:04:01,150
of not having to be correct all the time

00:03:58,540 --> 00:04:03,669
actually and what does this mean well it

00:04:01,150 --> 00:04:09,549
means that you might want to simplify

00:04:03,669 --> 00:04:12,790
the system scope so what does that mean

00:04:09,549 --> 00:04:14,290
exactly you know can we predict more

00:04:12,790 --> 00:04:17,680
coarsely for example instead of

00:04:14,290 --> 00:04:19,090
predicting precise positions might we

00:04:17,680 --> 00:04:22,330
just kind of predict that something is

00:04:19,090 --> 00:04:23,860
there at all and if you're doing

00:04:22,330 --> 00:04:25,270
something over time window we might want

00:04:23,860 --> 00:04:30,370
to do it over lots of time innocent

00:04:25,270 --> 00:04:31,870
aggregate results and this kind of

00:04:30,370 --> 00:04:35,229
thinking brings in more relevant data

00:04:31,870 --> 00:04:37,120
and kind of adds robustness and it and

00:04:35,229 --> 00:04:38,500
it decreases the degrees of freedom in

00:04:37,120 --> 00:04:40,340
the system in the model

00:04:38,500 --> 00:04:42,800
if you're going to put it in the kind of

00:04:40,340 --> 00:04:45,740
machine learning statistical terms also

00:04:42,800 --> 00:04:49,819
another aspect is can be elicit from the

00:04:45,740 --> 00:04:52,430
users knowledge that we can use so this

00:04:49,819 --> 00:04:54,319
is going to time-honored old artificial

00:04:52,430 --> 00:05:00,139
intelligence technique goes back to

00:04:54,319 --> 00:05:02,750
Eliza and and I think it's a pretty much

00:05:00,139 --> 00:05:04,340
popular today big thing with the systems

00:05:02,750 --> 00:05:06,110
like Google is kind of an expert kind of

00:05:04,340 --> 00:05:11,180
shoving you very even to go based on

00:05:06,110 --> 00:05:13,490
where you bin so back to a significant

00:05:11,180 --> 00:05:17,960
term segregation I don't know if

00:05:13,490 --> 00:05:19,160
everyone is familiar with it but its

00:05:17,960 --> 00:05:20,780
presentation in elasticsearch

00:05:19,160 --> 00:05:22,190
documentation and in the presentation is

00:05:20,780 --> 00:05:23,930
going to find it a bit disappointing

00:05:22,190 --> 00:05:26,150
because going to present this as magic

00:05:23,930 --> 00:05:29,180
it's like ten commonly commonly common

00:05:26,150 --> 00:05:32,330
in common and and the details is kind of

00:05:29,180 --> 00:05:36,680
glossed over a bit this is not really

00:05:32,330 --> 00:05:39,800
very magic at all so it's mostly our

00:05:36,680 --> 00:05:41,509
comparison the term frequencies you get

00:05:39,800 --> 00:05:43,340
our background set of documents with

00:05:41,509 --> 00:05:45,740
that you take the frequency of term vid

00:05:43,340 --> 00:05:48,500
this is kind of like the dark green line

00:05:45,740 --> 00:05:50,389
air you take your four grams of the

00:05:48,500 --> 00:05:53,090
documents which is what you're gonna do

00:05:50,389 --> 00:05:55,909
focus and that's kind of like the light

00:05:53,090 --> 00:05:59,030
green line and any kind of positive

00:05:55,909 --> 00:06:07,909
deviation as you see indicates of the

00:05:59,030 --> 00:06:11,539
term is significant so so why do we come

00:06:07,909 --> 00:06:13,009
here with this well person i like the

00:06:11,539 --> 00:06:18,050
significant turns a creation quite a lot

00:06:13,009 --> 00:06:20,090
it's and this comes from a background i

00:06:18,050 --> 00:06:23,780
find it the kind of bear kind of lexical

00:06:20,090 --> 00:06:26,570
signal is very powerful and if you hear

00:06:23,780 --> 00:06:28,780
talks or get presented an LP it is you

00:06:26,570 --> 00:06:32,000
get part of speech tag you get parsing

00:06:28,780 --> 00:06:33,680
but often the kind of the bear lexical

00:06:32,000 --> 00:06:35,659
signal the relationships in verse in the

00:06:33,680 --> 00:06:37,909
context are in is kind of very hard to

00:06:35,659 --> 00:06:39,469
beat for a lot of tasks if you're going

00:06:37,909 --> 00:06:47,000
to produce system that behaves in robust

00:06:39,469 --> 00:06:49,409
manner so it's also its kind of

00:06:47,000 --> 00:06:50,699
relationship to search

00:06:49,409 --> 00:06:52,199
which is kind of arguably the most

00:06:50,699 --> 00:06:53,789
successful a traditional artificial

00:06:52,199 --> 00:06:57,389
intelligence or information retrieval

00:06:53,789 --> 00:07:01,649
application of all time so we retrieve a

00:06:57,389 --> 00:07:03,389
large set of documents and we use an

00:07:01,649 --> 00:07:04,769
algorithm to rank them and then we kind

00:07:03,389 --> 00:07:09,419
of just present the top that kind of

00:07:04,769 --> 00:07:10,499
most most sure about so we take the part

00:07:09,419 --> 00:07:12,629
of result that we like the best

00:07:10,499 --> 00:07:14,129
represent it the thing you're not sure

00:07:12,629 --> 00:07:16,349
about it is going to put down below the

00:07:14,129 --> 00:07:20,610
fold and and the youth doesn't get a

00:07:16,349 --> 00:07:24,360
seat that's kind of nice space to be in

00:07:20,610 --> 00:07:27,360
I think when you're making a system so

00:07:24,360 --> 00:07:29,099
i'm going to talk briefly about using

00:07:27,360 --> 00:07:31,469
significant returns on single documents

00:07:29,099 --> 00:07:34,110
so elastic CEO doesn't really support

00:07:31,469 --> 00:07:36,360
this they have come like a minimum

00:07:34,110 --> 00:07:38,629
document count and you can go below it

00:07:36,360 --> 00:07:42,979
but as it turns out it never kind of

00:07:38,629 --> 00:07:45,719
does anything useful and that is because

00:07:42,979 --> 00:07:47,339
basically wasn't is slide is the JLH

00:07:45,719 --> 00:07:49,039
it's kind of the default scoring

00:07:47,339 --> 00:07:51,269
algorithm used for significant terms

00:07:49,039 --> 00:07:53,699
it's not really very well-documented if

00:07:51,269 --> 00:07:57,149
you look at it it's the frequent fork is

00:07:53,699 --> 00:07:59,759
the first equation different of

00:07:57,149 --> 00:08:01,679
frequency so 24 in the background i

00:07:59,759 --> 00:08:04,860
multiplied by the ratio or the

00:08:01,679 --> 00:08:06,240
foreground and background so it turns

00:08:04,860 --> 00:08:08,009
out it's kind of line your term it's

00:08:06,240 --> 00:08:09,389
kind of pretty relevance with basically

00:08:08,009 --> 00:08:10,860
boils down to this the rate of between

00:08:09,389 --> 00:08:14,279
the square the foreground in the

00:08:10,860 --> 00:08:16,860
background it turns out that the term

00:08:14,279 --> 00:08:19,919
segregation kind of lies behind there so

00:08:16,860 --> 00:08:21,239
it retrieves document frequencies and if

00:08:19,919 --> 00:08:23,699
you have a single document that is one

00:08:21,239 --> 00:08:25,709
and you end up with a inverse document

00:08:23,699 --> 00:08:28,829
frequency of the background set which

00:08:25,709 --> 00:08:32,399
means what you see from the significant

00:08:28,829 --> 00:08:33,930
terms are gregation is actually a list

00:08:32,399 --> 00:08:36,180
of the various storm in the background

00:08:33,930 --> 00:08:38,729
said in your document to add insult to

00:08:36,180 --> 00:08:41,669
injury it's awesome at alphabetically so

00:08:38,729 --> 00:08:46,490
we kind of get all the rare terms of the

00:08:41,669 --> 00:08:46,490
background set that begins with a so

00:08:46,519 --> 00:08:51,990
often it doesn't isn't real very useful

00:08:49,050 --> 00:08:55,769
so what we had to do is to create a new

00:08:51,990 --> 00:08:58,500
score that will let us do significant

00:08:55,769 --> 00:09:00,829
terms style AG scoring on a single

00:08:58,500 --> 00:09:00,829
document

00:09:02,000 --> 00:09:06,540
so we want to keep these that we want to

00:09:04,649 --> 00:09:11,040
bring things the four things that are in

00:09:06,540 --> 00:09:15,570
common but still are common enough

00:09:11,040 --> 00:09:17,790
basically and much summarized that is

00:09:15,570 --> 00:09:23,579
kind of like a Pareto style scoring this

00:09:17,790 --> 00:09:26,070
kind of 5 15 ad kind kind of separate of

00:09:23,579 --> 00:09:27,720
this kind of curve here where we want is

00:09:26,070 --> 00:09:30,810
juicy kind of informative it's not too

00:09:27,720 --> 00:09:32,730
common not to be a common the scoring

00:09:30,810 --> 00:09:35,579
used is the one you see here it's

00:09:32,730 --> 00:09:50,160
basically a tf-idf in an additional

00:09:35,579 --> 00:09:52,019
factor and and so the TF kind of it

00:09:50,160 --> 00:09:57,570
brings the document in the IDF kind of

00:09:52,019 --> 00:09:59,070
brings out common things and the middle

00:09:57,570 --> 00:10:00,690
term is actually a bit kind of

00:09:59,070 --> 00:10:03,959
situational because if you have a very

00:10:00,690 --> 00:10:07,579
noisy data set this is kind of an in

00:10:03,959 --> 00:10:11,160
domain IDF IDF which will kind of

00:10:07,579 --> 00:10:12,630
suppress noise that is in the domain if

00:10:11,160 --> 00:10:14,160
you're very clean data you might want to

00:10:12,630 --> 00:10:15,300
bring for structure from the domains

00:10:14,160 --> 00:10:20,640
then you use a document frequency

00:10:15,300 --> 00:10:22,500
instead this time to works a lot better

00:10:20,640 --> 00:10:25,740
than kind of just running this internal

00:10:22,500 --> 00:10:27,750
single document it's also nice to

00:10:25,740 --> 00:10:32,550
mention that in elasticsearch I can do

00:10:27,750 --> 00:10:34,680
very kind of you can actually do the TF

00:10:32,550 --> 00:10:39,720
IDF calculations in arbitrary terms as a

00:10:34,680 --> 00:10:41,430
query so tf-idf is kind of the central

00:10:39,720 --> 00:10:44,790
waiting term for a lot of relevant

00:10:41,430 --> 00:10:46,170
scoring for text and it's kind of a

00:10:44,790 --> 00:10:47,459
basic building blocks for a lot of

00:10:46,170 --> 00:10:49,230
machinery that you might want to build

00:10:47,459 --> 00:10:50,910
it's really nice to have it you can

00:10:49,230 --> 00:10:52,529
actually use the term segregation

00:10:50,910 --> 00:10:58,440
actively has all the things you need to

00:10:52,529 --> 00:11:00,089
build an IDF score and then you can give

00:10:58,440 --> 00:11:02,850
it three of the term frequency from the

00:11:00,089 --> 00:11:04,380
term vectors and you can do that it is

00:11:02,850 --> 00:11:05,730
in batches so everything you need is

00:11:04,380 --> 00:11:07,110
there and it's very very fast it's a lot

00:11:05,730 --> 00:11:10,949
faster than the original significant

00:11:07,110 --> 00:11:13,450
term square actually so so these are the

00:11:10,949 --> 00:11:15,640
building licks building blocks we use

00:11:13,450 --> 00:11:19,540
and then we're going to see a bit odd

00:11:15,640 --> 00:11:26,950
kind of some applications the first is

00:11:19,540 --> 00:11:29,460
entered extraction and end of the

00:11:26,950 --> 00:11:32,080
extraction is kind of coming from

00:11:29,460 --> 00:11:40,810
academic standpoint and in practice it's

00:11:32,080 --> 00:11:44,380
a pretty difficult problem so just a

00:11:40,810 --> 00:11:48,490
brief introduction is basically finding

00:11:44,380 --> 00:11:50,860
things in text and as you see this kind

00:11:48,490 --> 00:11:52,810
of example there's you're not only

00:11:50,860 --> 00:11:59,890
finding the things you're saying what it

00:11:52,810 --> 00:12:01,510
is and where they are in the text and as

00:11:59,890 --> 00:12:03,610
I said it was difficult but there aren't

00:12:01,510 --> 00:12:10,480
very good solutions for English and

00:12:03,610 --> 00:12:13,330
other major languages for us it's that's

00:12:10,480 --> 00:12:15,520
kind of a not very comforting because we

00:12:13,330 --> 00:12:17,050
have languages besides English that is

00:12:15,520 --> 00:12:19,900
not really covered there's no training

00:12:17,050 --> 00:12:23,590
data of this sort to build a machine

00:12:19,900 --> 00:12:25,120
learning solution and also you need a

00:12:23,590 --> 00:12:31,000
lot of kind of entities list guess it

00:12:25,120 --> 00:12:35,110
tears public indexes or whatever these

00:12:31,000 --> 00:12:36,970
these can be hard to obtain actually so

00:12:35,110 --> 00:12:40,030
in addition we will kind of cheat a bit

00:12:36,970 --> 00:12:41,650
doing our cheap as a person so instead

00:12:40,030 --> 00:12:43,450
of just kind of pinpointing exactly

00:12:41,650 --> 00:12:45,400
dissing what it is we're just going to

00:12:43,450 --> 00:12:47,500
list them it's gonna list the top entity

00:12:45,400 --> 00:12:48,910
see no text you're knotting and even

00:12:47,500 --> 00:12:50,140
going to do what I going to indicate in

00:12:48,910 --> 00:12:51,400
the figure is not going to say what it

00:12:50,140 --> 00:12:55,660
is even it just can state as an entity

00:12:51,400 --> 00:12:59,050
and that's it and ranked entities by

00:12:55,660 --> 00:13:00,460
relevance and this gives us a lot of

00:12:59,050 --> 00:13:03,330
flexibility in terms of cloudiest

00:13:00,460 --> 00:13:05,950
experience what he used to because the

00:13:03,330 --> 00:13:07,660
he just get to see the part of the

00:13:05,950 --> 00:13:11,350
result is we kind of pretty confident

00:13:07,660 --> 00:13:18,520
about and and if we even showed him to

00:13:11,350 --> 00:13:20,050
him which which we do and and it's not

00:13:18,520 --> 00:13:22,240
going to laugh that we didn't find

00:13:20,050 --> 00:13:24,540
anything or that we you did something

00:13:22,240 --> 00:13:24,540
wrong

00:13:24,690 --> 00:13:33,120
oh yeah some air is there ok so the to

00:13:31,950 --> 00:13:39,060
summarize the idea we're going to use

00:13:33,120 --> 00:13:40,950
high recall heuristics to to scoop up as

00:13:39,060 --> 00:13:44,070
many entities as possible along we had a

00:13:40,950 --> 00:13:45,510
lot of other stuff actually and it's

00:13:44,070 --> 00:13:47,730
going to collect potential entities with

00:13:45,510 --> 00:13:49,890
a very wide net they hope to kind of

00:13:47,730 --> 00:13:53,000
have hundred percent disposal and

00:13:49,890 --> 00:13:55,680
written recall and then we're going to

00:13:53,000 --> 00:13:59,070
score these entities with significant

00:13:55,680 --> 00:14:01,590
terms and finally if you're actually

00:13:59,070 --> 00:14:04,200
gonna we're going to use whatever kind

00:14:01,590 --> 00:14:10,380
of cheap knowledge resources we got to

00:14:04,200 --> 00:14:11,880
kind of accept the result we get so for

00:14:10,380 --> 00:14:13,110
a kind of concrete example we're going

00:14:11,880 --> 00:14:15,840
to scoop up a lot of capitalized terms

00:14:13,110 --> 00:14:17,870
it's like a 25 character of a gangster

00:14:15,840 --> 00:14:20,250
it's not a big piece of engineering

00:14:17,870 --> 00:14:24,180
we're going to using different terms

00:14:20,250 --> 00:14:26,400
based on IDF domain IDF I subscribe to

00:14:24,180 --> 00:14:28,680
earlier so this is very noisy me only

00:14:26,400 --> 00:14:31,070
this data it contains a lot of crap you

00:14:28,680 --> 00:14:34,650
can hold these things a lot of kind of

00:14:31,070 --> 00:14:36,090
sightings and all miners signatures and

00:14:34,650 --> 00:14:39,030
ugly stuff which makes actually makes it

00:14:36,090 --> 00:14:40,530
difficult and then we're gonna I say

00:14:39,030 --> 00:14:42,690
reranking but we're going to pick up

00:14:40,530 --> 00:14:45,570
entries from the dbpedia that are

00:14:42,690 --> 00:14:47,490
actually relevant and and bring out

00:14:45,570 --> 00:14:51,330
those so this is kind of how it looks

00:14:47,490 --> 00:14:54,440
it's a bit small but so it takes the top

00:14:51,330 --> 00:14:59,100
is is the text i'm gonna i highlighted

00:14:54,440 --> 00:15:02,220
the virus results and the first line is

00:14:59,100 --> 00:15:04,050
the 10 most relevant into this it's got

00:15:02,220 --> 00:15:08,760
what i ate that i consider good it's

00:15:04,050 --> 00:15:12,810
machine types technical technical staff

00:15:08,760 --> 00:15:15,360
it's it's got a nickname and a name it's

00:15:12,810 --> 00:15:16,380
also got two entities that are kind of

00:15:15,360 --> 00:15:17,670
just capitalized springs in the

00:15:16,380 --> 00:15:22,020
beginning of sentences which are not

00:15:17,670 --> 00:15:23,640
useful and the blue ones in their

00:15:22,020 --> 00:15:25,980
offerings in the middle of the ranking

00:15:23,640 --> 00:15:27,750
the 10 at the bottom is the 10 bottom of

00:15:25,980 --> 00:15:32,130
the rankings which is which is mostly

00:15:27,750 --> 00:15:35,839
annoying stuff but it's got to kind of

00:15:32,130 --> 00:15:38,550
mrs. here it's got into which is common

00:15:35,839 --> 00:15:40,529
which is common in in

00:15:38,550 --> 00:15:42,839
in general domain and gets dragged down

00:15:40,529 --> 00:15:45,300
by IDF there and gets not this actually

00:15:42,839 --> 00:15:49,200
is not mailing list so it gets dragged

00:15:45,300 --> 00:15:51,839
down but in the main IDF thankfully

00:15:49,200 --> 00:15:55,230
these can be carried out from wikipedia

00:15:51,839 --> 00:15:57,300
and and because this is software

00:15:55,230 --> 00:16:01,620
entities software companies is produced

00:15:57,300 --> 00:16:02,910
to get out and and along with red hat

00:16:01,620 --> 00:16:05,820
enterprise linux at the top which is

00:16:02,910 --> 00:16:07,350
also which is also kind of log in the

00:16:05,820 --> 00:16:09,990
ranking and we can bring those out and

00:16:07,350 --> 00:16:11,660
kind of save these from being forgotten

00:16:09,990 --> 00:16:13,920
at the bottom on the ranking list

00:16:11,660 --> 00:16:15,779
exactly help is that very common things

00:16:13,920 --> 00:16:18,899
it's actually more likely to be listed

00:16:15,779 --> 00:16:22,700
in such resources as dbpedia then the

00:16:18,899 --> 00:16:22,700
most common things that we bring up

00:16:25,370 --> 00:16:29,880
dbpedia and wikipedia something i should

00:16:27,899 --> 00:16:31,320
be a bit careful about actually because

00:16:29,880 --> 00:16:34,019
there's a lot of kind of odd structure

00:16:31,320 --> 00:16:36,120
and collections of stuff if you start

00:16:34,019 --> 00:16:38,940
just matching of things you'll get

00:16:36,120 --> 00:16:41,970
you'll match up in there anything so it

00:16:38,940 --> 00:16:43,920
tends to be a bit careful there and just

00:16:41,970 --> 00:16:48,660
kind of pick out the domains that you

00:16:43,920 --> 00:16:51,300
know very interesting you're doing

00:16:48,660 --> 00:16:53,010
training keywords this is kind of our

00:16:51,300 --> 00:16:54,720
key function out and applications it's

00:16:53,010 --> 00:17:00,450
kind of discovering new topics of

00:16:54,720 --> 00:17:04,550
interest that kind of having an getting

00:17:00,450 --> 00:17:04,550
a lot of interest in our sources and

00:17:05,510 --> 00:17:13,919
it's kind of like a short introduction

00:17:10,079 --> 00:17:17,189
and basically you look for increasing

00:17:13,919 --> 00:17:19,620
recent usage and it's actually a very

00:17:17,189 --> 00:17:21,900
important functionality because this is

00:17:19,620 --> 00:17:23,880
something that makes a system living

00:17:21,900 --> 00:17:25,439
self-developing system for analysts to

00:17:23,880 --> 00:17:29,580
kind of pick up on new things add new

00:17:25,439 --> 00:17:31,620
sources and and an add new functionality

00:17:29,580 --> 00:17:33,120
they need to know what's developing and

00:17:31,620 --> 00:17:36,990
this is one function allottees help them

00:17:33,120 --> 00:17:40,200
do that and the problem is that we often

00:17:36,990 --> 00:17:44,550
have very few documents to base or

00:17:40,200 --> 00:17:48,000
transcend and some trends some source is

00:17:44,550 --> 00:17:50,130
kind of vary in topics so it kind of

00:17:48,000 --> 00:17:52,470
degenerates to just the last top last

00:17:50,130 --> 00:17:57,630
message the last posting over

00:17:52,470 --> 00:18:01,679
being the trend which would you know

00:17:57,630 --> 00:18:03,270
isn't helpful to us so they kind of

00:18:01,679 --> 00:18:06,120
trying to tend to spike on there just

00:18:03,270 --> 00:18:07,980
the immediate activity all the time so

00:18:06,120 --> 00:18:09,450
what we do is that we smooth the time

00:18:07,980 --> 00:18:11,539
interval so do significant terms they

00:18:09,450 --> 00:18:13,830
can drag this out but we smooth them out

00:18:11,539 --> 00:18:15,539
to do that we need a relevant term

00:18:13,830 --> 00:18:16,950
scoring that significant term scoring

00:18:15,539 --> 00:18:18,990
that is actually comparable between

00:18:16,950 --> 00:18:20,580
different foreground and background sets

00:18:18,990 --> 00:18:22,919
which the original scoring doesn't do

00:18:20,580 --> 00:18:24,780
and then we want to compare different

00:18:22,919 --> 00:18:31,470
sources there's also key step to kind of

00:18:24,780 --> 00:18:33,390
make nice robust so I as a third

00:18:31,470 --> 00:18:36,240
increasing windows and we have inverse

00:18:33,390 --> 00:18:38,549
Pavel of sellers all of the rank which

00:18:36,240 --> 00:18:42,950
is comparable and then we look for

00:18:38,549 --> 00:18:46,140
things that are trending along different

00:18:42,950 --> 00:18:48,090
different sources is a small example I

00:18:46,140 --> 00:18:51,390
think we don't really have a lot of time

00:18:48,090 --> 00:18:54,840
to look about it you see like in mid

00:18:51,390 --> 00:18:56,970
month there's a exploit kind of we have

00:18:54,840 --> 00:19:03,120
two sources here and the correlated

00:18:56,970 --> 00:19:04,830
terms and and you see there's it is for

00:19:03,120 --> 00:19:09,870
example an exploit trending mid mid

00:19:04,830 --> 00:19:15,750
October in 2012 we're just about out of

00:19:09,870 --> 00:19:17,039
time so I'm know if you get to talk a

00:19:15,750 --> 00:19:19,049
lot about the guide is filtering it's

00:19:17,039 --> 00:19:21,990
about creating filters for specialized

00:19:19,049 --> 00:19:24,059
topics this is our analyst he has an

00:19:21,990 --> 00:19:25,350
internal entity called Hal and he gets a

00:19:24,059 --> 00:19:27,450
lot of messages that are not about what

00:19:25,350 --> 00:19:30,059
you want actually the rage of this can

00:19:27,450 --> 00:19:31,830
be from one to ten thousand to one two

00:19:30,059 --> 00:19:33,990
hundred thousand for over relevant

00:19:31,830 --> 00:19:35,460
documents relevant documents so it makes

00:19:33,990 --> 00:19:40,230
the source completely useless to the

00:19:35,460 --> 00:19:48,270
analyst and we like to use significant

00:19:40,230 --> 00:19:49,650
terms to highlight the terms that makes

00:19:48,270 --> 00:19:51,929
up the first stage of forgetting the

00:19:49,650 --> 00:19:53,669
filter that is actually based on the

00:19:51,929 --> 00:19:56,549
data and it's actually relevant to the

00:19:53,669 --> 00:20:02,490
data sources it also describes a topic

00:19:56,549 --> 00:20:04,310
and for analyst be doing something like

00:20:02,490 --> 00:20:06,860
this that

00:20:04,310 --> 00:20:09,050
the significant term looks for exploits

00:20:06,860 --> 00:20:10,790
at the background that that is Linux it

00:20:09,050 --> 00:20:13,490
is need interns query you get a lot of

00:20:10,790 --> 00:20:15,890
terms and we group them by topic mode

00:20:13,490 --> 00:20:17,420
for example the circles we also given by

00:20:15,890 --> 00:20:19,100
a lexical similarity model where for

00:20:17,420 --> 00:20:22,130
things where we can't find as seinem

00:20:19,100 --> 00:20:24,980
topics so these are for example plugins

00:20:22,130 --> 00:20:26,450
plug-in modules are similar exploit and

00:20:24,980 --> 00:20:28,250
scrapes an nmap similar threat and

00:20:26,450 --> 00:20:29,870
attacks are similar and you kind of

00:20:28,250 --> 00:20:31,640
colored the things that we think that

00:20:29,870 --> 00:20:37,100
the for this kind of topic that animals

00:20:31,640 --> 00:20:39,290
will pick up in red hair there's also a

00:20:37,100 --> 00:20:44,900
lot of kind of and sordid kind of

00:20:39,290 --> 00:20:47,300
entities on the bottom so so i can't

00:20:44,900 --> 00:20:54,200
really hard time to explain it fully but

00:20:47,300 --> 00:20:58,850
i think we based our time yeah so thank

00:20:54,200 --> 00:21:00,110
you for watching we're going to thank

00:20:58,850 --> 00:21:01,640
you for watching you're going to present

00:21:00,110 --> 00:21:03,800
a lot of the technical it's about it's

00:21:01,640 --> 00:21:05,660
on a blog in the coming weeks so if

00:21:03,800 --> 00:21:09,080
you're interesting this you might want

00:21:05,660 --> 00:21:11,200
to google that and check it out thank

00:21:09,080 --> 00:21:11,200

YouTube URL: https://www.youtube.com/watch?v=yYFFlyHPGlg


