Title: Berlin Buzzwords 2015: Andrew Clegg - Signatures, patterns and trends: Timeseries data mining @ Etsy
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Etsy loves metrics. Everything that happens in our data centres gets recorded, graphed and stored. But with over a million metrics flowing in constantly, it’s hard for any team to keep on top of all that information. Graphing everything doesn’t scale, and traditional alerting methods based on thresholds become very prone to false positives.

That’s why we started Kale, an open-source software suite for pattern mining and anomaly detection in operational data streams. These are big topics with decades of research, but many of the methods in the literature are ineffective on terabytes of noisy data with unusual statistical characteristics, and techniques that require extensive manual analysis are unsuitable when your ops teams have service levels to maintain.

In this talk I’ll briefly cover the main challenges that traditional statistical methods face in this environment, and introduce some pragmatic alternatives that scale well and are easy to implement (and automate) on ElasticSearch and similar platforms. I’ll talk about the stumbling blocks we encountered with the first release of Kale, and the resulting architectural changes coming in version 2.0. 

And I’ll go into a little technical detail on the algorithms we use for fingerprinting and searching metrics, and detecting different kinds of unusual activity. These techniques have potential applications in clustering, outlier detection, similarity search and supervised learning, and they are not limited to the data centre but can be applied to any high-volume timeseries data.

Read more:
https://2015.berlinbuzzwords.de/session/signatures-patterns-and-trends-timeseries-data-mining-etsy

About Andrew Clegg:
https://2015.berlinbuzzwords.de/users/andrew-clegg

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,470 --> 00:00:08,759
hi everyone

00:00:11,430 --> 00:00:14,830
hi I'm glad I've got so many people in

00:00:13,630 --> 00:00:16,950
here when I'm competing with weather

00:00:14,830 --> 00:00:19,630
like that outside so thanks for coming

00:00:16,950 --> 00:00:23,080
so hi yes I'm a data scientist at Etsy

00:00:19,630 --> 00:00:26,710
and if you're not familiar with Etsy

00:00:23,080 --> 00:00:30,369
we're an online marketplace for handmade

00:00:26,710 --> 00:00:32,020
goods vintage goods arts and crafts so I

00:00:30,369 --> 00:00:35,590
was going to put in some visual joke

00:00:32,020 --> 00:00:37,540
here about signature jewelry sewing

00:00:35,590 --> 00:00:38,980
patterns and fashion trends but I

00:00:37,540 --> 00:00:42,070
couldn't make it funny so let's just

00:00:38,980 --> 00:00:43,390
pretend pretend I did that so we're

00:00:42,070 --> 00:00:46,150
headquartered in Brooklyn

00:00:43,390 --> 00:00:49,840
we've got offices in Toronto San

00:00:46,150 --> 00:00:52,750
Francisco London Paris Dublin and also

00:00:49,840 --> 00:00:57,310
here in Berlin maybe one or two others

00:00:52,750 --> 00:00:58,330
I've forgotten as well so just before we

00:00:57,310 --> 00:00:59,980
get started I'll give a bit of

00:00:58,330 --> 00:01:04,360
background about the data science team

00:00:59,980 --> 00:01:08,080
at C we do two main kind of strands of

00:01:04,360 --> 00:01:10,869
work I suppose the first one is what I

00:01:08,080 --> 00:01:13,869
would describe as helping visitors to

00:01:10,869 --> 00:01:16,470
Etsy find personally relevant items that

00:01:13,869 --> 00:01:21,150
they're loved so that means things like

00:01:16,470 --> 00:01:25,390
shop item and user recommendations

00:01:21,150 --> 00:01:27,850
finding trends calculating data-driven

00:01:25,390 --> 00:01:29,950
ranking factors using things like

00:01:27,850 --> 00:01:32,530
personalization information or location

00:01:29,950 --> 00:01:35,170
information that can help drive search

00:01:32,530 --> 00:01:37,930
or ad placement or even navigation on

00:01:35,170 --> 00:01:40,479
the site and also some text mining

00:01:37,930 --> 00:01:42,790
activities to kind of complement search

00:01:40,479 --> 00:01:45,040
to build things like autosuggest and

00:01:42,790 --> 00:01:48,729
related queries and that kind of thing

00:01:45,040 --> 00:01:50,979
so this is more kind of product facing

00:01:48,729 --> 00:01:55,840
engineering work what the engineering

00:01:50,979 --> 00:01:57,490
end of data science and we also do some

00:01:55,840 --> 00:02:00,130
stuff which is more like internal tool

00:01:57,490 --> 00:02:03,159
building developing like algorithms and

00:02:00,130 --> 00:02:06,760
methodologies and pipelines and

00:02:03,159 --> 00:02:10,269
workflows to help with security or

00:02:06,760 --> 00:02:12,370
growth or operations just to helping

00:02:10,269 --> 00:02:14,140
increase the value of Etsy and make make

00:02:12,370 --> 00:02:16,810
it easier for other teams to do their

00:02:14,140 --> 00:02:18,970
jobs using kind of data mining methods

00:02:16,810 --> 00:02:21,190
machine learning methods and so on so

00:02:18,970 --> 00:02:22,610
they might include things like image

00:02:21,190 --> 00:02:25,960
similarity search for

00:02:22,610 --> 00:02:29,930
finding you know images that have been

00:02:25,960 --> 00:02:31,640
stolen or reused or water marked images

00:02:29,930 --> 00:02:34,760
that shouldn't be used for products on

00:02:31,640 --> 00:02:36,200
the sides things I kept building spam

00:02:34,760 --> 00:02:39,080
like classifiers for things like

00:02:36,200 --> 00:02:41,630
marketplace abuse suspicious behavior

00:02:39,080 --> 00:02:43,940
fraud that kind of thing building

00:02:41,630 --> 00:02:46,130
customer lifetime value models for

00:02:43,940 --> 00:02:48,230
estimating how long the customer is

00:02:46,130 --> 00:02:50,150
going to stay with Etsy how often

00:02:48,230 --> 00:02:51,980
they're going to buy and what we're what

00:02:50,150 --> 00:02:55,190
return we're likely to see from them and

00:02:51,980 --> 00:02:57,980
also detecting and diagnosing anomalies

00:02:55,190 --> 00:02:59,480
on the site or on the machines the bakit

00:02:57,980 --> 00:03:02,090
so that's what I'm going to be talking

00:02:59,480 --> 00:03:03,470
about today in the context of a project

00:03:02,090 --> 00:03:08,480
called kale which has been kind of

00:03:03,470 --> 00:03:10,730
ongoing since 2013 so here's a quick

00:03:08,480 --> 00:03:11,930
overview of what the talks about start

00:03:10,730 --> 00:03:15,880
off by talking a little bit about kale

00:03:11,930 --> 00:03:18,230
version one what we try to do with it

00:03:15,880 --> 00:03:18,800
what succeeded and what didn't work so

00:03:18,230 --> 00:03:21,800
well

00:03:18,800 --> 00:03:24,830
why I joined the company in December

00:03:21,800 --> 00:03:26,570
last year so a lot of kale version one

00:03:24,830 --> 00:03:28,730
happened before I started but I'm still

00:03:26,570 --> 00:03:29,989
going to just say we for convenience I'm

00:03:28,730 --> 00:03:31,730
not trying to take any credit for the

00:03:29,989 --> 00:03:34,459
things that worked or blame for the

00:03:31,730 --> 00:03:36,560
things that didn't I mean I talk a

00:03:34,459 --> 00:03:38,720
little bit more about anomaly detection

00:03:36,560 --> 00:03:40,100
in particular some background on why

00:03:38,720 --> 00:03:43,910
it's a difficult problem but why it's

00:03:40,100 --> 00:03:46,900
also a useful thing to attempt and also

00:03:43,910 --> 00:03:48,590
why a lot of treatments of the topic

00:03:46,900 --> 00:03:49,880
oversimplify the issue and I think

00:03:48,590 --> 00:03:53,030
missed some of them some of the

00:03:49,880 --> 00:03:54,320
important points there and then I'm

00:03:53,030 --> 00:03:58,070
going to talk a little bit about kale

00:03:54,320 --> 00:04:01,280
version 2 which is in the works at the

00:03:58,070 --> 00:04:03,830
moment how we're approaching the problem

00:04:01,280 --> 00:04:05,660
a little bit differently and why a bit

00:04:03,830 --> 00:04:07,370
of an overview of the architecture and

00:04:05,660 --> 00:04:09,380
why we came to the architectural

00:04:07,370 --> 00:04:11,269
decisions that we did for it and then

00:04:09,380 --> 00:04:13,640
also if I'm feeling brave enough and we

00:04:11,269 --> 00:04:16,580
have enough time by then a live demo of

00:04:13,640 --> 00:04:18,620
the work-in-progress for it and then

00:04:16,580 --> 00:04:20,239
I'll talk a little bit about what

00:04:18,620 --> 00:04:22,940
lessons we learnt from from the

00:04:20,239 --> 00:04:25,490
successes and failures of ko1 and how

00:04:22,940 --> 00:04:26,750
you can kind of generalize from them if

00:04:25,490 --> 00:04:28,160
you're doing a normally detection but

00:04:26,750 --> 00:04:30,500
also if you're launching an open source

00:04:28,160 --> 00:04:34,450
project and how to make that work or

00:04:30,500 --> 00:04:34,450
avoid some of the mistakes we made

00:04:34,760 --> 00:04:41,970
so we we launched kale in almost exactly

00:04:38,790 --> 00:04:45,750
two years ago in fact at a velocity

00:04:41,970 --> 00:04:49,500
conference we accompanied with a blog

00:04:45,750 --> 00:04:51,480
post and release on github and it

00:04:49,500 --> 00:04:53,730
consisted of two parts really the first

00:04:51,480 --> 00:04:56,130
part is called skyline which was for

00:04:53,730 --> 00:04:57,360
anomaly detection and the second part

00:04:56,130 --> 00:05:01,110
was called oculus which was for

00:04:57,360 --> 00:05:02,940
similarity search on time series data so

00:05:01,110 --> 00:05:06,180
the idea was we would have data flowing

00:05:02,940 --> 00:05:09,120
in from sources like graphite or ganglia

00:05:06,180 --> 00:05:12,780
where machine level or application level

00:05:09,120 --> 00:05:14,970
metrics are logged from our servers they

00:05:12,780 --> 00:05:17,490
would go into skyline and skyline would

00:05:14,970 --> 00:05:19,500
look for anomalous behavior in those

00:05:17,490 --> 00:05:22,500
metrics you know for example that peak

00:05:19,500 --> 00:05:26,400
in that little graph there and then once

00:05:22,500 --> 00:05:28,350
a anomaly had been found you could then

00:05:26,400 --> 00:05:30,780
use oculus which is a kind of diagnostic

00:05:28,350 --> 00:05:34,410
root cause analysis tool to search

00:05:30,780 --> 00:05:36,630
through the time series database for the

00:05:34,410 --> 00:05:38,790
backing it for similar metrics from

00:05:36,630 --> 00:05:40,560
other machines or other metrics from the

00:05:38,790 --> 00:05:43,140
same machine that showed similar

00:05:40,560 --> 00:05:45,810
properties so the idea of being that

00:05:43,140 --> 00:05:47,430
it's a seizure process you look for

00:05:45,810 --> 00:05:48,900
something that's gone wrong and then you

00:05:47,430 --> 00:05:51,930
find other things which are behaving the

00:05:48,900 --> 00:05:53,840
same to try and determine what the cause

00:05:51,930 --> 00:05:59,100
was it will put together a picture of

00:05:53,840 --> 00:06:01,290
what was what's actually at fault now we

00:05:59,100 --> 00:06:04,740
were working some kind of fairly strong

00:06:01,290 --> 00:06:06,690
constraints we had about a quarter of a

00:06:04,740 --> 00:06:09,270
million metrics coming in in parallel

00:06:06,690 --> 00:06:11,040
each of those were sampled with

00:06:09,270 --> 00:06:15,030
ten-second or 60 second resolution

00:06:11,040 --> 00:06:17,580
depending on where it came from and we

00:06:15,030 --> 00:06:19,320
had to build something that was fast

00:06:17,580 --> 00:06:20,660
enough and responsive enough to be used

00:06:19,320 --> 00:06:23,610
in kind of an incident management

00:06:20,660 --> 00:06:25,230
timeframe so doing things has a kind of

00:06:23,610 --> 00:06:27,600
batch operation in Hadoop or something

00:06:25,230 --> 00:06:30,060
like that where you might be waiting you

00:06:27,600 --> 00:06:31,680
know getting an hourly job or a nightly

00:06:30,060 --> 00:06:33,330
job or something was kind of out of the

00:06:31,680 --> 00:06:35,700
window we really wanted something that

00:06:33,330 --> 00:06:37,260
people could use you know if they got

00:06:35,700 --> 00:06:38,850
paged at 3:00 in the morning to figure

00:06:37,260 --> 00:06:41,220
out what was going on

00:06:38,850 --> 00:06:44,550
and so it had to have a nice user

00:06:41,220 --> 00:06:45,660
interface as well and not require lots

00:06:44,550 --> 00:06:49,590
of fiddly tools

00:06:45,660 --> 00:06:53,610
also fit into our workflow well so that

00:06:49,590 --> 00:06:55,890
sounds easy right as it turned out it

00:06:53,610 --> 00:06:57,720
was a lot harder than we thought that's

00:06:55,890 --> 00:07:03,990
sarcasm by the way that's not actual

00:06:57,720 --> 00:07:05,340
advice so it's it's not an easy problem

00:07:03,990 --> 00:07:09,600
and it turned out to be a lot more

00:07:05,340 --> 00:07:13,110
difficult than then we first thought it

00:07:09,600 --> 00:07:14,580
would be but it's also not going away we

00:07:13,110 --> 00:07:16,350
now have about four times as many

00:07:14,580 --> 00:07:18,420
metrics about a million metrics coming

00:07:16,350 --> 00:07:22,670
in in parallel all the time and it's not

00:07:18,420 --> 00:07:24,930
just us that has this issue so Twitter

00:07:22,670 --> 00:07:27,330
Netflix and Yahoo have all recently

00:07:24,930 --> 00:07:30,270
released tools for anomaly detection

00:07:27,330 --> 00:07:33,120
that face different kinds of challenges

00:07:30,270 --> 00:07:34,860
with different solutions and all address

00:07:33,120 --> 00:07:38,160
slightly different use cases as well I

00:07:34,860 --> 00:07:40,530
think all of them are not quite there

00:07:38,160 --> 00:07:45,840
which is obviously why I why we're

00:07:40,530 --> 00:07:48,780
working on this at the moment so let's

00:07:45,840 --> 00:07:53,730
talk a little bit about the what worked

00:07:48,780 --> 00:07:55,290
well in kale first of all so time series

00:07:53,730 --> 00:07:56,670
similarity search which is often

00:07:55,290 --> 00:07:58,860
considered quite a hard problem

00:07:56,670 --> 00:08:03,300
especially across that many different

00:07:58,860 --> 00:08:06,810
time series actually we got that working

00:08:03,300 --> 00:08:10,940
fairly well we we did it with the

00:08:06,810 --> 00:08:14,580
two-stage process the idea was you would

00:08:10,940 --> 00:08:17,520
retrieve a fuzzy kind of approximate set

00:08:14,580 --> 00:08:20,190
of results using a fingerprinting method

00:08:17,520 --> 00:08:22,740
or signature based method first and then

00:08:20,190 --> 00:08:25,830
from those candidate resolves use a more

00:08:22,740 --> 00:08:27,690
exact search method that would give you

00:08:25,830 --> 00:08:31,260
very precise rack similarity rankings

00:08:27,690 --> 00:08:33,020
but was a lot more costly so the first

00:08:31,260 --> 00:08:36,870
stage method that were used for

00:08:33,020 --> 00:08:39,750
retrieving roughly similar time series

00:08:36,870 --> 00:08:41,400
by fingerprint was called a shape

00:08:39,750 --> 00:08:43,860
description alphabet which is an idea

00:08:41,400 --> 00:08:45,810
that's been around for decades but it's

00:08:43,860 --> 00:08:47,310
kind of hasn't really seen a lot of use

00:08:45,810 --> 00:08:49,680
and recently has been revived a little

00:08:47,310 --> 00:08:52,640
bit it's quite a simple idea the idea is

00:08:49,680 --> 00:08:54,870
you map line segments in the data so the

00:08:52,640 --> 00:08:57,770
gradient between successive pairs of

00:08:54,870 --> 00:09:00,630
points two tokens

00:08:57,770 --> 00:09:03,390
typically five and it's it's quite sort

00:09:00,630 --> 00:09:06,090
of quite simple you'd go from depending

00:09:03,390 --> 00:09:07,980
whether it's a steep upward slope or

00:09:06,090 --> 00:09:10,650
shallow upward stoat slope a roughly

00:09:07,980 --> 00:09:12,960
level slope a shallow downward slope or

00:09:10,650 --> 00:09:14,610
sleep steep downward slope you map it to

00:09:12,960 --> 00:09:17,670
a different token affected just a

00:09:14,610 --> 00:09:20,100
different letter and then you can index

00:09:17,670 --> 00:09:22,770
those with in our case elasticsearch or

00:09:20,100 --> 00:09:24,930
really any any search engine any loosing

00:09:22,770 --> 00:09:27,510
based tool or any other similar search

00:09:24,930 --> 00:09:30,870
tool will do that once you've done that

00:09:27,510 --> 00:09:33,390
you can use sloppy phrase queries from a

00:09:30,870 --> 00:09:36,540
time series of interest represented by a

00:09:33,390 --> 00:09:39,330
set of tokens to through your database

00:09:36,540 --> 00:09:41,280
of indexed time series and by adjusting

00:09:39,330 --> 00:09:44,190
the slope factor you can be more or less

00:09:41,280 --> 00:09:47,270
precise about how many you get back then

00:09:44,190 --> 00:09:53,450
after that you use you have to use a

00:09:47,270 --> 00:09:55,920
slightly more costly but accurate

00:09:53,450 --> 00:09:58,170
ranking method to rank them all by

00:09:55,920 --> 00:09:59,850
similarity to the search sequence so in

00:09:58,170 --> 00:10:01,560
our case we use one called fast dynamic

00:09:59,850 --> 00:10:02,730
time warping I'm not gonna go into the

00:10:01,560 --> 00:10:05,010
details but you can google it it's

00:10:02,730 --> 00:10:08,220
basically a kind of dynamic programming

00:10:05,010 --> 00:10:11,490
based method for aligning to two time

00:10:08,220 --> 00:10:13,620
series and it's kind of robust to things

00:10:11,490 --> 00:10:16,650
like than being out of phase and having

00:10:13,620 --> 00:10:22,830
insertions and deletions and that kind

00:10:16,650 --> 00:10:26,220
of thing so that worked pretty well we

00:10:22,830 --> 00:10:28,920
also developed quite a good user

00:10:26,220 --> 00:10:30,600
experience for kale so it was designed

00:10:28,920 --> 00:10:32,210
for kind of busy ops people in a hurry

00:10:30,600 --> 00:10:35,820
to be able to see things quite quickly

00:10:32,210 --> 00:10:37,590
both parts skyline for normal detection

00:10:35,820 --> 00:10:39,540
and oculus for similarity search both

00:10:37,590 --> 00:10:41,550
had nice web interfaces with

00:10:39,540 --> 00:10:43,320
visualization you know you could see

00:10:41,550 --> 00:10:48,090
recent behavior you could see past

00:10:43,320 --> 00:10:50,310
history you could see what metrics were

00:10:48,090 --> 00:10:51,570
currently kind of in an anomalous state

00:10:50,310 --> 00:10:54,660
you could click through to visualize

00:10:51,570 --> 00:11:01,260
them and so on so that also worked

00:10:54,660 --> 00:11:02,820
pretty well but we didn't really address

00:11:01,260 --> 00:11:05,250
some of the more fundamental points that

00:11:02,820 --> 00:11:06,540
well which is why it's no longer in use

00:11:05,250 --> 00:11:08,010
at Etsy and it hasn't really seen a lot

00:11:06,540 --> 00:11:10,189
of adoption out there in the open source

00:11:08,010 --> 00:11:12,569
community either

00:11:10,189 --> 00:11:14,699
one of the things that proved to be a

00:11:12,569 --> 00:11:18,360
bit of a blocker to adoption was it had

00:11:14,699 --> 00:11:20,459
quite a complicated architecture so the

00:11:18,360 --> 00:11:22,079
parts on the left here are more or less

00:11:20,459 --> 00:11:23,579
the normally detection parts and the

00:11:22,079 --> 00:11:25,980
parts on the right here are the

00:11:23,579 --> 00:11:28,139
similarity search parts but you can see

00:11:25,980 --> 00:11:30,209
you know to get the whole stack working

00:11:28,139 --> 00:11:33,509
you needed to install two data stores

00:11:30,209 --> 00:11:37,679
ready and elasticsearch you needed to

00:11:33,509 --> 00:11:40,350
have two application servers one was

00:11:37,679 --> 00:11:43,410
flask which is Python based on the

00:11:40,350 --> 00:11:45,179
Sinatra which was ruby based you needed

00:11:43,410 --> 00:11:47,249
to use rescue which is a queuing system

00:11:45,179 --> 00:11:48,269
there are a total of four languages in

00:11:47,249 --> 00:11:50,579
use in it because there was a

00:11:48,269 --> 00:11:52,829
elasticsearch plugin to do the dynamic

00:11:50,579 --> 00:11:55,559
time warping there was JavaScript used

00:11:52,829 --> 00:11:57,989
in both of the web interfaces the there

00:11:55,559 --> 00:11:59,610
was a lot of kind of pipeline plumbing

00:11:57,989 --> 00:12:01,230
code that isn't really shown on this

00:11:59,610 --> 00:12:03,839
diagram to kind of wire the bits

00:12:01,230 --> 00:12:07,019
together now I don't really know of any

00:12:03,839 --> 00:12:08,970
open source project that has that many

00:12:07,019 --> 00:12:11,160
different components in and has been a

00:12:08,970 --> 00:12:12,839
success I mean you might say Hadoop but

00:12:11,160 --> 00:12:14,730
in Hadoop they are all designed from

00:12:12,839 --> 00:12:16,679
scratch to work together right they're

00:12:14,730 --> 00:12:18,949
all part of a coherent ecosystem more or

00:12:16,679 --> 00:12:21,689
less in this case these were all

00:12:18,949 --> 00:12:24,689
components that we took from elsewhere

00:12:21,689 --> 00:12:27,149
and wired them together now even with

00:12:24,689 --> 00:12:29,459
the aid of things like chef and puppet

00:12:27,149 --> 00:12:30,779
recipes it was quite hard to actually

00:12:29,459 --> 00:12:32,549
install this and get this working

00:12:30,779 --> 00:12:33,839
because most of these needed to be

00:12:32,549 --> 00:12:37,860
installed on separate machines because

00:12:33,839 --> 00:12:39,689
they're quite heavy right processes so

00:12:37,860 --> 00:12:41,399
that was kind of a bit of a blocker to

00:12:39,689 --> 00:12:42,629
adoption and then also once you've

00:12:41,399 --> 00:12:43,949
released something like this into the

00:12:42,629 --> 00:12:45,360
wild you kind of put yourself in the

00:12:43,949 --> 00:12:46,980
position of having to support something

00:12:45,360 --> 00:12:49,259
with all of these components which is

00:12:46,980 --> 00:12:50,519
really difficult as if somebody is

00:12:49,259 --> 00:12:54,239
installing it and they don't already

00:12:50,519 --> 00:12:57,660
have knowledge of things like Python or

00:12:54,239 --> 00:12:59,189
Ruby or elastic search or Redis they're

00:12:57,660 --> 00:13:01,589
not going to know where the problems are

00:12:59,189 --> 00:13:04,169
part a isn't talking to Part B that kind

00:13:01,589 --> 00:13:05,189
of thing so it's if you do something

00:13:04,169 --> 00:13:06,929
like this you're going to let yourself

00:13:05,189 --> 00:13:09,059
in for a lot of problems down the line

00:13:06,929 --> 00:13:11,249
potentially now all of these things were

00:13:09,059 --> 00:13:14,009
kind of chosen with the right intentions

00:13:11,249 --> 00:13:15,720
in mind and with the right each

00:13:14,009 --> 00:13:17,579
individual choice kind of made sense on

00:13:15,720 --> 00:13:19,679
its own but it was as a coherent whole

00:13:17,579 --> 00:13:21,559
and we didn't really think through the

00:13:19,679 --> 00:13:23,579
consequences of that

00:13:21,559 --> 00:13:25,319
and also we've never really got the

00:13:23,579 --> 00:13:28,079
anomaly detection part working that well

00:13:25,319 --> 00:13:29,790
and I'm gonna dwell on this a little bit

00:13:28,079 --> 00:13:32,579
because coming from a data science point

00:13:29,790 --> 00:13:33,600
of view this is something that this is

00:13:32,579 --> 00:13:35,369
where I think I can make the biggest

00:13:33,600 --> 00:13:36,749
difference and where I've been thinking

00:13:35,369 --> 00:13:41,009
a lot about how to do a better job of it

00:13:36,749 --> 00:13:43,290
so the the methods that were used by

00:13:41,009 --> 00:13:45,209
skyline were kind of mostly inspired by

00:13:43,290 --> 00:13:48,300
kind of statistical process control that

00:13:45,209 --> 00:13:49,439
kind of thing where you're your 10 you

00:13:48,300 --> 00:13:51,509
tend to be looking like the output of

00:13:49,439 --> 00:13:53,100
manufacturing pipelines production lines

00:13:51,509 --> 00:13:54,389
that kind of thing actually your

00:13:53,100 --> 00:13:56,970
tolerances are fairly small your

00:13:54,389 --> 00:13:58,559
expected variation is fairly small and

00:13:56,970 --> 00:14:02,429
your distributions generally tend to be

00:13:58,559 --> 00:14:04,439
like Gaussian distributions data from

00:14:02,429 --> 00:14:06,839
machines and data from websites in

00:14:04,439 --> 00:14:08,970
general just isn't like that you get

00:14:06,839 --> 00:14:10,259
lots of non-normal distributions you get

00:14:08,970 --> 00:14:12,689
power laws you get things which are

00:14:10,259 --> 00:14:15,929
mixtures of multiple distributions you

00:14:12,689 --> 00:14:18,779
get like roughness spikes periodic

00:14:15,929 --> 00:14:21,420
cyclic behavior you get the problem

00:14:18,779 --> 00:14:23,429
where if you've got half quarter of a

00:14:21,420 --> 00:14:27,629
million metrics or these days a million

00:14:23,429 --> 00:14:30,869
metrics then even if you're say look

00:14:27,629 --> 00:14:33,959
your thresholds for significance is one

00:14:30,869 --> 00:14:35,639
in a million you know your p-value like

00:14:33,959 --> 00:14:39,389
ten to the minus six you're going to

00:14:35,639 --> 00:14:40,920
expect to see false alarms basically all

00:14:39,389 --> 00:14:43,410
the time just because of the number of

00:14:40,920 --> 00:14:44,850
comparisons you're doing right so so

00:14:43,410 --> 00:14:46,439
you've got to think of other ways part

00:14:44,850 --> 00:14:47,879
from like you can't just keep on turning

00:14:46,439 --> 00:14:49,259
the p-value down because eventually run

00:14:47,879 --> 00:14:51,869
into rounding errors and things like

00:14:49,259 --> 00:14:54,509
that so there was some mitigations that

00:14:51,869 --> 00:14:57,179
ko1 used including things like majority

00:14:54,509 --> 00:14:58,589
voting ensembles and auto silencing of

00:14:57,179 --> 00:15:02,160
repeated alerts which helped a little

00:14:58,589 --> 00:15:04,679
bit but they they were limited in what

00:15:02,160 --> 00:15:06,749
they could do because of the kinds of

00:15:04,679 --> 00:15:08,569
methods that were used to actually

00:15:06,749 --> 00:15:11,009
detect the nominees in the first place

00:15:08,569 --> 00:15:13,470
if basically what you're doing is

00:15:11,009 --> 00:15:16,139
looking at point outliers so individual

00:15:13,470 --> 00:15:18,240
readings which are more than some Delta

00:15:16,139 --> 00:15:20,339
away from what you expect even if you're

00:15:18,240 --> 00:15:22,319
kind of measuring that or threshold in a

00:15:20,339 --> 00:15:23,610
load of different ways then if you get a

00:15:22,319 --> 00:15:24,959
false alarm it's going to tend to be a

00:15:23,610 --> 00:15:28,379
false alarm for a lot of the things in

00:15:24,959 --> 00:15:30,209
the ensemble at the same time but also

00:15:28,379 --> 00:15:32,040
there's a bit of a broader problem I

00:15:30,209 --> 00:15:33,760
think which is which is missed by a lot

00:15:32,040 --> 00:15:35,950
of presentations and tools

00:15:33,760 --> 00:15:38,830
on this topic which is that not every

00:15:35,950 --> 00:15:40,690
anomaly is a point outlier so just for

00:15:38,830 --> 00:15:42,430
clarity what I mean by point outlier is

00:15:40,690 --> 00:15:43,930
something like either of these two

00:15:42,430 --> 00:15:45,640
really big spikes which kind of

00:15:43,930 --> 00:15:48,310
immediately looks quite a lot higher

00:15:45,640 --> 00:15:50,020
than the others in there right but so

00:15:48,310 --> 00:15:51,730
that's that's a fairly obvious kind of

00:15:50,020 --> 00:15:54,240
spike or it could be a dip going down

00:15:51,730 --> 00:15:59,920
below where you would expect to see it

00:15:54,240 --> 00:16:02,290
but not every not the four start there

00:15:59,920 --> 00:16:04,060
are lots of other ways that machines can

00:16:02,290 --> 00:16:06,790
misbehave or that users can misbehave

00:16:04,060 --> 00:16:09,010
even I'll talk about some of the signals

00:16:06,790 --> 00:16:10,690
for those in a sec but it's important

00:16:09,010 --> 00:16:13,390
for us as well that not every point

00:16:10,690 --> 00:16:15,430
outlier still looks like one if you step

00:16:13,390 --> 00:16:17,200
back and look at more data I mean maybe

00:16:15,430 --> 00:16:19,600
if we step back and look at ten

00:16:17,200 --> 00:16:21,940
sequences of this length we would see

00:16:19,600 --> 00:16:23,740
these happening fairly regularly you

00:16:21,940 --> 00:16:25,210
know if I only looked at this bit here

00:16:23,740 --> 00:16:26,920
zoomed in then that would look like a

00:16:25,210 --> 00:16:27,490
point outlier whereas in the context of

00:16:26,920 --> 00:16:30,400
everything else

00:16:27,490 --> 00:16:31,930
it actually looks fairly standard so you

00:16:30,400 --> 00:16:34,540
have to be able to take into account

00:16:31,930 --> 00:16:36,760
enough data to get a realistic picture

00:16:34,540 --> 00:16:38,020
of that oh by the way all of the brass

00:16:36,760 --> 00:16:39,820
I'm gonna show from various different

00:16:38,020 --> 00:16:42,310
kinds of web errors that I scraped out

00:16:39,820 --> 00:16:43,480
of our graphite server it doesn't

00:16:42,310 --> 00:16:44,440
actually matter what kind they are

00:16:43,480 --> 00:16:46,300
really I'm just trying to demonstrate

00:16:44,440 --> 00:16:50,110
the kind of behaviour that you see in

00:16:46,300 --> 00:16:54,600
these graphs so what else can you see

00:16:50,110 --> 00:16:57,640
apart from point outliers so a lot of a

00:16:54,600 --> 00:17:00,610
lot of metrics show kind of periodic

00:16:57,640 --> 00:17:02,830
cyclic oscillations they could be kind

00:17:00,610 --> 00:17:05,320
of reflecting the daily cycle of the use

00:17:02,830 --> 00:17:07,810
of your site or you know weekly or even

00:17:05,320 --> 00:17:09,490
annual cycles but on this much full

00:17:07,810 --> 00:17:11,260
scale they can reflect things going on

00:17:09,490 --> 00:17:13,000
at the level of machines you can get

00:17:11,260 --> 00:17:14,890
things like garbage collection cycles

00:17:13,000 --> 00:17:17,500
which so show spikes on a much shorter

00:17:14,890 --> 00:17:18,880
time scale now if if something suddenly

00:17:17,500 --> 00:17:21,610
starts to oscillate that wasn't

00:17:18,880 --> 00:17:23,709
oscillating before or a previously

00:17:21,610 --> 00:17:26,290
reliable predictable oscillation goes

00:17:23,709 --> 00:17:30,640
away then that can be a sign that that

00:17:26,290 --> 00:17:32,820
something odd is happening right and you

00:17:30,640 --> 00:17:35,470
get distributions that change completely

00:17:32,820 --> 00:17:37,570
between the same bounds that they're

00:17:35,470 --> 00:17:39,190
usually in and a something that detects

00:17:37,570 --> 00:17:41,500
point out lies is kind of useless there

00:17:39,190 --> 00:17:43,240
because no individual value will be high

00:17:41,500 --> 00:17:46,590
enough or low enough to actually trigger

00:17:43,240 --> 00:17:48,900
an alarm because you can still be within

00:17:46,590 --> 00:17:54,809
range but suddenly see a lot of much

00:17:48,900 --> 00:17:57,900
higher values near the original peak you

00:17:54,809 --> 00:17:59,820
get trends that can change as well so

00:17:57,900 --> 00:18:01,650
something that might be on a healthy

00:17:59,820 --> 00:18:03,450
upward slope and you know total

00:18:01,650 --> 00:18:04,980
registrations or something like that

00:18:03,450 --> 00:18:07,110
could suddenly flatten out that might be

00:18:04,980 --> 00:18:09,150
a sign that something's gone wrong with

00:18:07,110 --> 00:18:10,830
your signup process you get things where

00:18:09,150 --> 00:18:13,169
the baseline of a trend can jump around

00:18:10,830 --> 00:18:15,059
like in this graph you can get things

00:18:13,169 --> 00:18:16,620
that should be flat but suddenly start

00:18:15,059 --> 00:18:21,539
going up or going down that kind of

00:18:16,620 --> 00:18:23,429
thing and you get I have to admit I've

00:18:21,539 --> 00:18:25,559
reused a graph here but you get a rare

00:18:23,429 --> 00:18:28,100
discrete events where you might see them

00:18:25,559 --> 00:18:30,929
only occasionally with a certain

00:18:28,100 --> 00:18:32,850
predictable level of regularity suddenly

00:18:30,929 --> 00:18:37,669
becoming much more regular or regular

00:18:32,850 --> 00:18:37,669
ones suddenly becoming much less regular

00:18:37,909 --> 00:18:42,929
and the best bit of all of this is quite

00:18:41,490 --> 00:18:44,549
often it's not even a problem I mean

00:18:42,929 --> 00:18:45,779
even setting aside the issue of false

00:18:44,549 --> 00:18:48,659
positive so you can get something that's

00:18:45,779 --> 00:18:51,510
a statistical anomaly the genuine

00:18:48,659 --> 00:18:53,909
anomaly but actually in the grand scheme

00:18:51,510 --> 00:18:55,649
of things hasn't affected users of your

00:18:53,909 --> 00:18:58,110
site very much at all if you've got a

00:18:55,649 --> 00:19:00,090
web farm of hundreds of web servers or a

00:18:58,110 --> 00:19:02,429
database farm of hundreds of database

00:19:00,090 --> 00:19:03,899
servers and individual metric

00:19:02,429 --> 00:19:06,090
misbehaving on one of those machines

00:19:03,899 --> 00:19:11,460
won't necessarily actually affect

00:19:06,090 --> 00:19:14,700
people's user experience so what we're

00:19:11,460 --> 00:19:16,110
doing differently this time so part of

00:19:14,700 --> 00:19:18,120
the problem with Catwoman's it was kind

00:19:16,110 --> 00:19:21,120
of difficult to experiment with with

00:19:18,120 --> 00:19:22,950
different kinds of tests they were all

00:19:21,120 --> 00:19:24,510
sort of baked into this big

00:19:22,950 --> 00:19:27,480
infrastructure that you had to install

00:19:24,510 --> 00:19:29,490
all of in order to really use and also

00:19:27,480 --> 00:19:31,440
in the similar way to how people who

00:19:29,490 --> 00:19:33,419
first get into machine learning they

00:19:31,440 --> 00:19:34,860
think yeah I'll get all of the data I've

00:19:33,419 --> 00:19:36,360
got loads of data then I'll throw it at

00:19:34,860 --> 00:19:37,860
an algorithm and the algorithm will sort

00:19:36,360 --> 00:19:40,350
it out I've got so much data that will

00:19:37,860 --> 00:19:41,549
just work right and then when you've

00:19:40,350 --> 00:19:42,809
been doing it for a while you realize

00:19:41,549 --> 00:19:44,130
that actually most of the work is about

00:19:42,809 --> 00:19:46,169
pre-processing the data

00:19:44,130 --> 00:19:47,970
extracting the right features that kind

00:19:46,169 --> 00:19:49,350
of thing and then that gives the

00:19:47,970 --> 00:19:52,919
algorithm much better chance of

00:19:49,350 --> 00:19:54,419
succeeding my hunch is the anomaly

00:19:52,919 --> 00:19:57,510
detection is actually much like that

00:19:54,419 --> 00:19:58,970
really being able to pre-process and

00:19:57,510 --> 00:20:00,530
filter the data in the right way

00:19:58,970 --> 00:20:02,750
is going to give you a much better

00:20:00,530 --> 00:20:05,330
chance of getting you know reducing

00:20:02,750 --> 00:20:09,110
false positives without also missing

00:20:05,330 --> 00:20:11,030
things so I'm putting together a library

00:20:09,110 --> 00:20:13,760
of algorithms called time which is kind

00:20:11,030 --> 00:20:17,510
of the first phase of Kol - it's gonna

00:20:13,760 --> 00:20:19,340
be platform agnostic so the idea is you

00:20:17,510 --> 00:20:20,750
know you don't have to install any

00:20:19,340 --> 00:20:22,370
number of different open-source

00:20:20,750 --> 00:20:24,770
components that we've said this is all

00:20:22,370 --> 00:20:26,809
one big stack you might want to run it

00:20:24,770 --> 00:20:29,240
as a standalone service you might want

00:20:26,809 --> 00:20:30,860
to embed it in another in a system

00:20:29,240 --> 00:20:32,150
you've already got you may just want to

00:20:30,860 --> 00:20:33,590
experiment with it see what works on

00:20:32,150 --> 00:20:38,059
your data before you commit to using

00:20:33,590 --> 00:20:40,340
some some platform it's aims to be as as

00:20:38,059 --> 00:20:42,140
memory efficient and as cache friendly

00:20:40,340 --> 00:20:44,720
as you can get away with in Java so not

00:20:42,140 --> 00:20:46,700
doing unnecessary memory allocations

00:20:44,720 --> 00:20:48,200
trying to allocate buffers upfront and

00:20:46,700 --> 00:20:50,150
reuse them that kind of thing instead of

00:20:48,200 --> 00:20:53,299
letting the garbage collector have lots

00:20:50,150 --> 00:20:55,100
of work to do and it's built on the

00:20:53,299 --> 00:20:56,659
framework called reactive X which I'm

00:20:55,100 --> 00:20:58,340
going to talk a bit more about in a

00:20:56,659 --> 00:21:02,000
minute because it's it's actually really

00:20:58,340 --> 00:21:04,870
helped with kind of clarifying how how

00:21:02,000 --> 00:21:07,730
this kind of project should work I think

00:21:04,870 --> 00:21:09,559
which come from the the dotnet world but

00:21:07,730 --> 00:21:12,919
has been ported to loads of languages

00:21:09,559 --> 00:21:14,659
including there's rx Java which is the

00:21:12,919 --> 00:21:16,190
work was done by Netflix they call it a

00:21:14,659 --> 00:21:17,659
Netflix original which I guess in the

00:21:16,190 --> 00:21:25,610
same way as house of cards means it's a

00:21:17,659 --> 00:21:26,780
remake remake of something else so the

00:21:25,610 --> 00:21:29,750
components that I'm looking to put into

00:21:26,780 --> 00:21:32,740
the first release are fall into kind of

00:21:29,750 --> 00:21:34,970
four main categories really this there's

00:21:32,740 --> 00:21:36,500
components for signal processing and

00:21:34,970 --> 00:21:39,980
feature extraction I've been inspired

00:21:36,500 --> 00:21:44,059
quite a lot by like the DSP world the

00:21:39,980 --> 00:21:47,000
digital audio world here so things like

00:21:44,059 --> 00:21:49,309
wavelet decomposition and fast Fourier

00:21:47,000 --> 00:21:51,350
transforms that you kind of extract or

00:21:49,309 --> 00:21:53,720
filter out behavior at certain frequency

00:21:51,350 --> 00:21:55,760
levels that lets you get a handle on

00:21:53,720 --> 00:21:57,140
those kind of cyclic oscillatory

00:21:55,760 --> 00:21:58,880
behaviors I was talking about earlier on

00:21:57,140 --> 00:22:00,470
but also some simpler stuff for things

00:21:58,880 --> 00:22:02,600
like centering and rescaling and

00:22:00,470 --> 00:22:06,049
smoothing and trends really all that

00:22:02,600 --> 00:22:08,480
kind of thing I'm looking including

00:22:06,049 --> 00:22:11,419
probably just those three statistical

00:22:08,480 --> 00:22:12,169
tests to start with generalized DSD is a

00:22:11,419 --> 00:22:15,230
point out

00:22:12,169 --> 00:22:18,320
test which is also used in in Twitter's

00:22:15,230 --> 00:22:20,929
anomaly detection our library and the

00:22:18,320 --> 00:22:23,359
kolmogorov-smirnov test which is for

00:22:20,929 --> 00:22:25,429
looking at differences in distributions

00:22:23,359 --> 00:22:26,600
and the mann-whitney u test which is

00:22:25,429 --> 00:22:28,489
kind of looking at differences in the

00:22:26,600 --> 00:22:30,919
midpoint of distributions so there they

00:22:28,489 --> 00:22:32,929
were they tend to work better or worse

00:22:30,919 --> 00:22:35,359
on data depending on whether the

00:22:32,929 --> 00:22:37,940
differences are in the tails or in the

00:22:35,359 --> 00:22:39,200
center the fingerprinting in search

00:22:37,940 --> 00:22:42,200
components are going to be basically the

00:22:39,200 --> 00:22:43,639
same as in Cal version 1.0 so shape

00:22:42,200 --> 00:22:46,009
description alphabet component for

00:22:43,639 --> 00:22:47,389
mapping time series into tokens binary

00:22:46,009 --> 00:22:49,009
clipping which is an even simpler way of

00:22:47,389 --> 00:22:51,200
doing this which just you sent to the

00:22:49,009 --> 00:22:52,789
data around zero and and you you map it

00:22:51,200 --> 00:22:54,499
to one if it's above the line or a zero

00:22:52,789 --> 00:22:56,389
if it's below the line which actually on

00:22:54,499 --> 00:22:58,070
certain kinds of data is a very very

00:22:56,389 --> 00:23:00,200
efficient way to filter out things that

00:22:58,070 --> 00:23:02,389
can't possibly be matches and then

00:23:00,200 --> 00:23:05,359
dynamic time warping to do the exact

00:23:02,389 --> 00:23:07,100
similarity search and I'm also putting

00:23:05,359 --> 00:23:08,720
together into the interactive demo app

00:23:07,100 --> 00:23:10,249
which shows how some of these things can

00:23:08,720 --> 00:23:12,100
fit together and I'll show you that in a

00:23:10,249 --> 00:23:14,720
moment

00:23:12,100 --> 00:23:17,509
so a quick word on the design choices

00:23:14,720 --> 00:23:19,039
and why Java I speak to a lot of ops

00:23:17,509 --> 00:23:20,929
people about things like this and in the

00:23:19,039 --> 00:23:24,080
devops world these days things like go

00:23:20,929 --> 00:23:25,669
they're very popular but I think you're

00:23:24,080 --> 00:23:27,169
a lot of good reasons cystic would you

00:23:25,669 --> 00:23:30,350
are for something like this for start it

00:23:27,169 --> 00:23:31,929
gives us interoperability with platforms

00:23:30,350 --> 00:23:35,450
that we're using already like

00:23:31,929 --> 00:23:37,879
elasticsearch log stash and cabana and

00:23:35,450 --> 00:23:39,710
Kafka and platforms that we've been

00:23:37,879 --> 00:23:42,649
looking at for doing stream processing

00:23:39,710 --> 00:23:44,749
including Samsa and spark streaming it

00:23:42,649 --> 00:23:46,940
also means we can embed it into our

00:23:44,749 --> 00:23:49,789
Hadoop workflows if we want to work on

00:23:46,940 --> 00:23:51,710
much bigger datasets more slowly so we

00:23:49,789 --> 00:23:55,039
do most of our Big Data work on Hadoop

00:23:51,710 --> 00:23:57,049
with scalding and conjecture so scalding

00:23:55,039 --> 00:23:59,330
is a scholar library for putting

00:23:57,049 --> 00:24:01,340
together data processing pipelines and

00:23:59,330 --> 00:24:04,369
conjecture is a machine learning library

00:24:01,340 --> 00:24:08,450
in Scala the there is another Etsy open

00:24:04,369 --> 00:24:09,950
source project I think some of the some

00:24:08,450 --> 00:24:11,859
of the components of time would actually

00:24:09,950 --> 00:24:15,139
fit quite well into conjecture for doing

00:24:11,859 --> 00:24:17,299
very very large parallel time series

00:24:15,139 --> 00:24:19,659
mining also Android I put that in as a

00:24:17,299 --> 00:24:22,009
bit of a joke that also demonstrates a

00:24:19,659 --> 00:24:23,179
something which is important to realize

00:24:22,009 --> 00:24:25,520
about this which is none of the things

00:24:23,179 --> 00:24:27,590
in here are really specific to server me

00:24:25,520 --> 00:24:29,720
Drix we have a team called the office

00:24:27,590 --> 00:24:33,140
hackers that build kind of like I guess

00:24:29,720 --> 00:24:35,270
workflow work space productivity gadgets

00:24:33,140 --> 00:24:37,160
that kind of thing I was talking to them

00:24:35,270 --> 00:24:38,300
about how you could use things like this

00:24:37,160 --> 00:24:40,570
for built for doing environmental

00:24:38,300 --> 00:24:42,920
sensing and detecting I don't know

00:24:40,570 --> 00:24:44,420
unusual behavior in an office

00:24:42,920 --> 00:24:45,770
after-hours that might indicate that

00:24:44,420 --> 00:24:49,970
someone's there who shouldn't be there

00:24:45,770 --> 00:24:53,390
that kind of thing so yeah nothing in

00:24:49,970 --> 00:24:54,380
this really I think would be heavyweight

00:24:53,390 --> 00:24:56,200
enough that you couldn't run it on

00:24:54,380 --> 00:24:58,790
Android I think that actually kind of

00:24:56,200 --> 00:25:00,830
fits in quite nicely with Levinas talk

00:24:58,790 --> 00:25:03,320
yesterday morning you know so she was

00:25:00,830 --> 00:25:06,470
collecting sensor metrics from a phone

00:25:03,320 --> 00:25:08,450
and then pushing it to Cassandra to do

00:25:06,470 --> 00:25:10,040
data mining on but there's also no

00:25:08,450 --> 00:25:12,680
reason why you couldn't do data mining

00:25:10,040 --> 00:25:15,620
on a single stream of metrics on the

00:25:12,680 --> 00:25:18,620
phone these days and it means if user

00:25:15,620 --> 00:25:20,360
another time series database like say

00:25:18,620 --> 00:25:22,370
you've got open TS DB which is based on

00:25:20,360 --> 00:25:24,530
HBase or data stacks which is based on

00:25:22,370 --> 00:25:26,390
Cassandra because they're both Java it

00:25:24,530 --> 00:25:28,310
should be fairly easy to to plug the

00:25:26,390 --> 00:25:30,200
stuff into them and there's a lot more

00:25:28,310 --> 00:25:33,610
maths and stats resources available in

00:25:30,200 --> 00:25:33,610
Java than there are and something I go

00:25:33,670 --> 00:25:37,520
you might ask why nots Karla because we

00:25:36,020 --> 00:25:40,610
use Scala quite a lot internally but

00:25:37,520 --> 00:25:42,110
it's Javas much more widespread so if

00:25:40,610 --> 00:25:43,520
we're looking at producing something

00:25:42,110 --> 00:25:45,680
that people can contribute to it's less

00:25:43,520 --> 00:25:47,540
of a barrier to entry and it's also

00:25:45,680 --> 00:25:51,770
easier to write kind of low-level

00:25:47,540 --> 00:25:54,200
fast number crunching code in Java so

00:25:51,770 --> 00:25:55,460
also reactive X so this thing was kind

00:25:54,200 --> 00:25:57,920
of new to me I discovered it by accident

00:25:55,460 --> 00:26:01,880
and it seemed to be a perfect fit for

00:25:57,920 --> 00:26:03,530
this kind of project it's an instance of

00:26:01,880 --> 00:26:06,590
the functional reactive programming

00:26:03,530 --> 00:26:09,170
pattern which is kind of an extension of

00:26:06,590 --> 00:26:14,330
the type safe observer pattern where you

00:26:09,170 --> 00:26:16,340
have components that emit messages that

00:26:14,330 --> 00:26:19,270
encapsulate data and other components

00:26:16,340 --> 00:26:21,530
that process them and it does all the

00:26:19,270 --> 00:26:22,700
the plumbing and takes out the

00:26:21,530 --> 00:26:24,860
boilerplate of wiring them together

00:26:22,700 --> 00:26:27,620
gives you tools to kind of manage those

00:26:24,860 --> 00:26:30,050
pipelines and apply sort of functional

00:26:27,620 --> 00:26:32,810
programming paradigms to them so you get

00:26:30,050 --> 00:26:35,390
operations like map and flatmap and

00:26:32,810 --> 00:26:36,800
reduce and collect and zip and split and

00:26:35,390 --> 00:26:37,930
merge and can count and all of those

00:26:36,800 --> 00:26:40,760
things

00:26:37,930 --> 00:26:43,550
so it's it's very very well suited to

00:26:40,760 --> 00:26:45,590
writing these data flow based programs

00:26:43,550 --> 00:26:50,390
it's a little bit like writing something

00:26:45,590 --> 00:26:52,520
like a spark work flow or you know a

00:26:50,390 --> 00:26:55,820
scalding work flow but just within one

00:26:52,520 --> 00:26:58,580
JVM on a single thread so basically all

00:26:55,820 --> 00:27:00,020
you need to do is you define all of the

00:26:58,580 --> 00:27:02,930
things I was talking about earlier on as

00:27:00,020 --> 00:27:06,730
operators and they all they all obey

00:27:02,930 --> 00:27:06,730
well most of them about one of these two

00:27:06,850 --> 00:27:10,460
interfaces so they're either function

00:27:08,900 --> 00:27:12,920
that returns that takes a block of data

00:27:10,460 --> 00:27:14,900
just an array of doubles and returns an

00:27:12,920 --> 00:27:16,160
array of doubles so that's how you'd

00:27:14,900 --> 00:27:18,620
implement one of the pre-processing

00:27:16,160 --> 00:27:20,810
operators or you get ones that take an

00:27:18,620 --> 00:27:22,520
array of doubles and return some result

00:27:20,810 --> 00:27:23,930
of type T so that's how you'd implement

00:27:22,520 --> 00:27:25,970
kind of the statistical tests and things

00:27:23,930 --> 00:27:28,580
then all you need to just wrap your data

00:27:25,970 --> 00:27:30,890
source as an observable of type double

00:27:28,580 --> 00:27:32,830
that just admits these arrays of data

00:27:30,890 --> 00:27:35,300
when it's buffered up enough to work on

00:27:32,830 --> 00:27:38,840
and you can just then compose these

00:27:35,300 --> 00:27:40,430
things together to make a pipeline one

00:27:38,840 --> 00:27:44,210
nice thing about it is it provides nice

00:27:40,430 --> 00:27:47,870
kind of idiomatic sort of friendly

00:27:44,210 --> 00:27:51,230
looking DSL for Java right with lambdas

00:27:47,870 --> 00:27:53,150
also for Scala closure groovy and Kotlin

00:27:51,230 --> 00:27:56,720
all from the same Rx Java package

00:27:53,150 --> 00:27:58,670
basically so if you wanted to write a

00:27:56,720 --> 00:27:59,690
third-party extension or a plug-in for

00:27:58,670 --> 00:28:01,730
time and you're using one of those

00:27:59,690 --> 00:28:05,510
languages you could actually write it in

00:28:01,730 --> 00:28:06,860
a much more familiar thing that was more

00:28:05,510 --> 00:28:09,260
familiar to your frame of reference that

00:28:06,860 --> 00:28:11,300
used syntax that was basically exactly

00:28:09,260 --> 00:28:13,100
the same as your own kind of connections

00:28:11,300 --> 00:28:14,480
library and then just package it as a

00:28:13,100 --> 00:28:19,940
jar and drop it in as long as it follows

00:28:14,480 --> 00:28:21,590
the same interfaces so this is kind of a

00:28:19,940 --> 00:28:23,510
schematic of what a pipeline in time

00:28:21,590 --> 00:28:24,860
might look like so you have raw data

00:28:23,510 --> 00:28:26,300
coming from somewhere could be a

00:28:24,860 --> 00:28:28,670
database could be a stream and that kind

00:28:26,300 --> 00:28:31,310
of thing and you'd then split it into

00:28:28,670 --> 00:28:32,750
blocks you don't set data points one by

00:28:31,310 --> 00:28:35,600
one through time it's kind of really

00:28:32,750 --> 00:28:37,880
like a micro batch system rather than a

00:28:35,600 --> 00:28:39,620
true streaming system then you might do

00:28:37,880 --> 00:28:40,910
something like apply a wavelet filter

00:28:39,620 --> 00:28:42,470
which would let you split it into the

00:28:40,910 --> 00:28:45,500
high frequency components of the signal

00:28:42,470 --> 00:28:47,120
you know like the rough noise and then

00:28:45,500 --> 00:28:48,490
the low frequency components so like the

00:28:47,120 --> 00:28:51,580
underlying trends

00:28:48,490 --> 00:28:53,679
and long period cycles then you might

00:28:51,580 --> 00:28:55,330
apply the same a couple of

00:28:53,679 --> 00:28:57,370
pre-processing steps to both of those

00:28:55,330 --> 00:28:59,050
streams in parallel they're sorting the

00:28:57,370 --> 00:29:00,490
blocks and then building overlapping

00:28:59,050 --> 00:29:03,760
windows out of them so you can compare

00:29:00,490 --> 00:29:06,370
recent to historical data and then you

00:29:03,760 --> 00:29:08,530
might want to send the the output of

00:29:06,370 --> 00:29:10,900
those two parallel pathways into two

00:29:08,530 --> 00:29:13,750
different statistical tests so something

00:29:10,900 --> 00:29:15,070
like the coal ma graph Smirnov test for

00:29:13,750 --> 00:29:17,260
difference and distributions if you

00:29:15,070 --> 00:29:19,059
don't take out the underlying trend that

00:29:17,260 --> 00:29:20,140
will go off all the time because you

00:29:19,059 --> 00:29:23,380
know you're not you're not centered

00:29:20,140 --> 00:29:24,910
around zero the readings for now might

00:29:23,380 --> 00:29:26,740
be higher or lower than the readings

00:29:24,910 --> 00:29:31,900
from earlier on just by dint of that

00:29:26,740 --> 00:29:32,710
that trend changing whereas the so you

00:29:31,900 --> 00:29:34,120
might want to just send the

00:29:32,710 --> 00:29:35,530
high-frequency data through that to look

00:29:34,120 --> 00:29:37,870
for a differences in kind of local

00:29:35,530 --> 00:29:39,520
roughness or or noisiness behavior and

00:29:37,870 --> 00:29:42,720
send the low frequency data for

00:29:39,520 --> 00:29:45,070
something that looks for sudden changes

00:29:42,720 --> 00:29:48,100
so I'm going to give like a quick demo

00:29:45,070 --> 00:29:53,400
of that using the demo which is a work

00:29:48,100 --> 00:29:58,690
in progress so what this does did to do

00:29:53,400 --> 00:30:00,970
right so this actually in lieu of having

00:29:58,690 --> 00:30:04,000
a server to listen to reads data from

00:30:00,970 --> 00:30:07,179
the microphone over my laptop it sends

00:30:04,000 --> 00:30:09,400
it through a wavelet filter and then

00:30:07,179 --> 00:30:15,179
sends the output of that to a comb over

00:30:09,400 --> 00:30:15,179
of smirnoff test so if I'm sorry

00:30:17,590 --> 00:30:23,289
oh yes sure sorry the the code isn't the

00:30:20,230 --> 00:30:25,270
important bit but I can try I don't know

00:30:23,289 --> 00:30:27,789
how you actually do that actually let me

00:30:25,270 --> 00:30:29,320
start it running first because it takes

00:30:27,789 --> 00:30:31,779
a little while to what it's actually

00:30:29,320 --> 00:30:33,820
doing is it's it's listening to my

00:30:31,779 --> 00:30:36,520
microphone and taking a volume reading

00:30:33,820 --> 00:30:38,080
every few milliseconds I don't know if

00:30:36,520 --> 00:30:40,570
the acoustics here will mess with it at

00:30:38,080 --> 00:30:42,820
all but yeah don't don't worry too much

00:30:40,570 --> 00:30:44,559
about looking at the code because it's

00:30:42,820 --> 00:30:46,870
not a finished product but what this is

00:30:44,559 --> 00:30:48,520
doing is taking a rms volume reading

00:30:46,870 --> 00:30:50,080
every few milliseconds and graphing

00:30:48,520 --> 00:30:53,830
those so this isn't the raw Soundwave

00:30:50,080 --> 00:30:55,000
data this is the average volume level of

00:30:53,830 --> 00:30:56,950
what it's picking up from me speaking

00:30:55,000 --> 00:30:58,510
but then because I've removed the low

00:30:56,950 --> 00:31:01,000
frequency components with a wavelet

00:30:58,510 --> 00:31:04,690
filter it's centered around zero so if

00:31:01,000 --> 00:31:06,580
it gets kind of like generally louder or

00:31:04,690 --> 00:31:07,840
quieter if I talk a little bit quieter

00:31:06,580 --> 00:31:11,409
it will still be centered around zero

00:31:07,840 --> 00:31:14,260
because it's taking out the kind of

00:31:11,409 --> 00:31:16,899
baseline so it's building up I think ten

00:31:14,260 --> 00:31:19,360
of these blocks and then after a while

00:31:16,899 --> 00:31:21,210
it will start showing a p-value readout

00:31:19,360 --> 00:31:24,820
at the bottom of the screen which is

00:31:21,210 --> 00:31:27,429
basically how likely the current data is

00:31:24,820 --> 00:31:30,250
it in the context of any of the previous

00:31:27,429 --> 00:31:31,840
blocks so if that gets too low right so

00:31:30,250 --> 00:31:33,159
now it started showing those at the

00:31:31,840 --> 00:31:34,450
bottom of the screen I wish I could make

00:31:33,159 --> 00:31:36,070
I don't know how actually made that a

00:31:34,450 --> 00:31:39,549
bit bigger but we've got one that's

00:31:36,070 --> 00:31:41,529
point one a point zero four etc etc you

00:31:39,549 --> 00:31:43,570
know I've set the the p value threshold

00:31:41,529 --> 00:31:45,309
quite low because I don't want lots of

00:31:43,570 --> 00:31:54,279
false positives before I suddenly go

00:31:45,309 --> 00:31:55,570
quiet now you'll see it's it's produced

00:31:54,279 --> 00:31:56,919
a whoop-whoop at the bottom of the

00:31:55,570 --> 00:31:58,630
screen there if you can't see from the

00:31:56,919 --> 00:32:00,760
back you have to trust me it's got zero

00:31:58,630 --> 00:32:03,279
point zero because the p-value so low

00:32:00,760 --> 00:32:05,409
and its alerted on that but similarly if

00:32:03,279 --> 00:32:07,299
I if I do something which is just

00:32:05,409 --> 00:32:09,010
completely unexpected and doesn't

00:32:07,299 --> 00:32:10,240
actually match but isn't kind of a

00:32:09,010 --> 00:32:18,760
complete silence like if I suddenly

00:32:10,240 --> 00:32:20,260
start doing this then okay it's had a

00:32:18,760 --> 00:32:21,549
very low p-value but I think it's

00:32:20,260 --> 00:32:24,370
because of the echoes in here it's

00:32:21,549 --> 00:32:25,720
actually it's it's never really kind of

00:32:24,370 --> 00:32:28,480
picking up the silence between the

00:32:25,720 --> 00:32:30,050
sounds it's the echoes kind of filtering

00:32:28,480 --> 00:32:32,220
out a little bit

00:32:30,050 --> 00:32:34,470
so it's an example of how you would need

00:32:32,220 --> 00:32:36,630
to tune your thresholds and to tune your

00:32:34,470 --> 00:32:39,990
filtering parameters to suit the data

00:32:36,630 --> 00:32:42,210
set and the behavior you expect to see

00:32:39,990 --> 00:32:44,070
in it a little bit so yeah that's a

00:32:42,210 --> 00:32:47,010
that's just a quick demo it's a bit of a

00:32:44,070 --> 00:32:51,000
work in progress but I hope that gives

00:32:47,010 --> 00:32:52,530
you a flavor of how it can work so just

00:32:51,000 --> 00:32:53,970
quick summary what if we learned well

00:32:52,530 --> 00:32:55,410
keep your architecture simple especially

00:32:53,970 --> 00:32:58,050
if you want to release it as an open

00:32:55,410 --> 00:32:59,610
source project if something's good fit

00:32:58,050 --> 00:33:01,530
for your problem it doesn't necessarily

00:32:59,610 --> 00:33:03,000
mean add it to your stack straight away

00:33:01,530 --> 00:33:04,890
because you've got to think about

00:33:03,000 --> 00:33:06,540
support operability that kind of thing

00:33:04,890 --> 00:33:08,960
we have a bit of a meme at Etsy which is

00:33:06,540 --> 00:33:11,160
prefer boring software as possible

00:33:08,960 --> 00:33:13,290
interesting problems solved with boring

00:33:11,160 --> 00:33:14,760
software is better than having to solve

00:33:13,290 --> 00:33:17,520
boring problems because your software's

00:33:14,760 --> 00:33:19,170
too new and interesting and don't open

00:33:17,520 --> 00:33:20,550
source a platform or app unless you've

00:33:19,170 --> 00:33:22,200
been running it in production for I'll

00:33:20,550 --> 00:33:23,880
have no plans to stop because otherwise

00:33:22,200 --> 00:33:27,840
you're left with like an orphaned

00:33:23,880 --> 00:33:29,490
package that you can't really support so

00:33:27,840 --> 00:33:31,440
we've also learned Amelie detection is

00:33:29,490 --> 00:33:33,480
more than just out the Ida Texan okay a

00:33:31,440 --> 00:33:35,670
one-size-fits-all pipeline probably

00:33:33,480 --> 00:33:37,080
won't really fit anything that well and

00:33:35,670 --> 00:33:39,180
also you probably shouldn't actually

00:33:37,080 --> 00:33:41,250
alert on all the normal ease but think

00:33:39,180 --> 00:33:43,830
about maybe alerting on business

00:33:41,250 --> 00:33:46,320
critical anomalies you know things like

00:33:43,830 --> 00:33:47,670
sign-up rate conversion rate bounce rate

00:33:46,320 --> 00:33:50,400
the kind of things that you might test

00:33:47,670 --> 00:33:52,440
in an a/b test right look for anomalies

00:33:50,400 --> 00:33:54,690
in all of your metrics registered them

00:33:52,440 --> 00:33:56,430
all but then use those for root cause

00:33:54,690 --> 00:33:57,750
analysis and diagnostics when you've

00:33:56,430 --> 00:34:01,820
actually found an anomaly which is

00:33:57,750 --> 00:34:01,820
affecting user experience for your users

00:34:01,850 --> 00:34:06,570
and we've also the good UI and workflow

00:34:04,830 --> 00:34:08,880
will be a really good boost to

00:34:06,570 --> 00:34:10,410
production and adoption I think kale one

00:34:08,880 --> 00:34:12,120
was a victim of its own success in that

00:34:10,410 --> 00:34:14,430
regard because it did look very polished

00:34:12,120 --> 00:34:15,990
so lots of people when hey this is great

00:34:14,430 --> 00:34:18,060
it's a fully featured package and

00:34:15,990 --> 00:34:20,220
actually it turned out under the covers

00:34:18,060 --> 00:34:23,520
to not be as current as the interface

00:34:20,220 --> 00:34:26,310
made it seem ensemble methods auto

00:34:23,520 --> 00:34:29,010
calibration and so on seemed to work

00:34:26,310 --> 00:34:30,690
quite well for reducing reducing noise

00:34:29,010 --> 00:34:32,010
from things like this so I'm going to be

00:34:30,690 --> 00:34:35,280
exploring a bit how we can make those a

00:34:32,010 --> 00:34:36,660
little bit smarter and it turns out you

00:34:35,280 --> 00:34:38,250
can get away with doing time series

00:34:36,660 --> 00:34:39,840
similarity search but you have to

00:34:38,250 --> 00:34:41,910
constrain the search space a bit at

00:34:39,840 --> 00:34:42,690
first by fingerprinting in order to make

00:34:41,910 --> 00:34:45,869
the problem

00:34:42,690 --> 00:34:47,990
patiently tractable so that's it thank

00:34:45,869 --> 00:34:47,990

YouTube URL: https://www.youtube.com/watch?v=sn-btkORIxg


