Title: Berlin Buzzwords 2015: Radu Gheorghe & Rafał Kuć – Side by Side with Elasticsearch & Solr part 2
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	A brand new take on our Solr & Elasticsearch talk from last year! As the title suggests, this time we’ll dive deeper into how these two search engines scale and perform. And of course, we’ll take into account all the goodies that came with Elasticsearch and Solr since.

Both search engines are based on Lucene, so you’d expect similar numbers, but:
- at scale, small differences can give different numbers
- as with most functionality, Elasticsearch and Solr take different paths to achieve similar results, so you’d tune them differently
- their distributed models are quite different

We’ll show how you’d tune Elasticsearch and Solr for 2 common use-cases - logging and product search - and what numbers we got after tuning. Also, we’ll share some best practices for scaling out massive Elasticsearch and Solr clusters. For example, how to divide data into shards and indices/collections that account for growth, when to use routing and how to make sure that coordinating nodes don’t become unresponsive.

By the end you’ll see how Elasticsearch and Solr compare when you dive deeper into their functionality. You’ll know which important Lucene knobs to turn and how to do that in each search engine. Also, you’ll know how to use specific scaling features such as automatic rebalancing for Elasticsearch and shard splitting for Solr.

Read more:
https://2015.berlinbuzzwords.de/session/side-side-elasticsearch-solr-part-2-performance-scalability

About Radu Gheorghe:
https://2015.berlinbuzzwords.de/users/radu-gheorghe

About Rafał Kuć:
https://2015.berlinbuzzwords.de/users/gr0

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:00,000 --> 00:00:02,030
Oh

00:00:05,690 --> 00:00:11,140
thank you very much for coming welcome

00:00:08,930 --> 00:00:11,140
to

00:00:12,250 --> 00:00:20,830
Chancellor the lettuce this time I will

00:00:16,660 --> 00:00:24,340
focus on a bit of the scaling and a bit

00:00:20,830 --> 00:00:27,849
more of the performance first let us

00:00:24,340 --> 00:00:32,730
introduce ourselves we are both search

00:00:27,849 --> 00:00:32,730
consultants at SEMATECH stoke-on-trent

00:00:39,570 --> 00:00:45,790
is toward the latest he's also a Family

00:00:45,579 --> 00:00:48,370
Guy

00:00:45,790 --> 00:00:51,059
these are hips his two kids he's only

00:00:48,370 --> 00:00:54,840
two kids not only playtest

00:00:51,059 --> 00:00:58,180
yeah thanks

00:00:54,840 --> 00:00:59,379
okay and this is Raju as he said we

00:00:58,180 --> 00:01:02,379
worked together

00:00:59,379 --> 00:01:04,089
he's also consultant the trainer he also

00:01:02,379 --> 00:01:06,220
happened to write a book elasticsearch

00:01:04,089 --> 00:01:08,800
in action which are actually personally

00:01:06,220 --> 00:01:12,009
like if you want there is a discount

00:01:08,800 --> 00:01:14,890
code he also is a father and the family

00:01:12,009 --> 00:01:18,330
guy and but that's for the introduction

00:01:14,890 --> 00:01:21,159
let's ask continued why do you do okay

00:01:18,330 --> 00:01:24,220
so last year we did the side by side

00:01:21,159 --> 00:01:27,880
with soul and elasticsearch talk in buzz

00:01:24,220 --> 00:01:30,790
words and we talked briefly about both

00:01:27,880 --> 00:01:33,040
of those great products so we took the

00:01:30,790 --> 00:01:35,140
opportunity to talk to tell you about

00:01:33,040 --> 00:01:37,780
how the indexing works how the querying

00:01:35,140 --> 00:01:40,090
works what are the other functionalities

00:01:37,780 --> 00:01:42,070
like that data analysis faceting

00:01:40,090 --> 00:01:44,850
consoler site aggregations on

00:01:42,070 --> 00:01:48,310
elasticsearch we also we also looked at

00:01:44,850 --> 00:01:50,439
the tools available and other things

00:01:48,310 --> 00:01:52,840
like the api's we also compared the

00:01:50,439 --> 00:01:56,680
community engagement in for those both

00:01:52,840 --> 00:01:58,869
products like committers code the

00:01:56,680 --> 00:02:01,780
mailing list activities and stuff like

00:01:58,869 --> 00:02:04,329
that basically the last year conclusions

00:02:01,780 --> 00:02:06,670
were that most project worked well with

00:02:04,329 --> 00:02:10,660
both solar and elasticsearch there are

00:02:06,670 --> 00:02:13,269
very very small amount of showstoppers

00:02:10,660 --> 00:02:15,190
that would say don't go elasticsearch

00:02:13,269 --> 00:02:17,260
don't go solar but there are a very

00:02:15,190 --> 00:02:19,030
large number in small differences

00:02:17,260 --> 00:02:20,310
because those are different products

00:02:19,030 --> 00:02:22,900
based on leucine

00:02:20,310 --> 00:02:25,300
in addition we said that choose the best

00:02:22,900 --> 00:02:27,610
that you like that your comfort

00:02:25,300 --> 00:02:32,020
with choose the one that you worked for

00:02:27,610 --> 00:02:34,660
your use case and that's how it go went

00:02:32,020 --> 00:02:36,670
last year however we wanted to also

00:02:34,660 --> 00:02:38,770
bring you back with the retrospection of

00:02:36,670 --> 00:02:41,170
what happened during that year between

00:02:38,770 --> 00:02:43,630
those two talks so last year which we've

00:02:41,170 --> 00:02:45,790
you've seen a slide like this so we

00:02:43,630 --> 00:02:47,710
talked that wind solar we will see the

00:02:45,790 --> 00:02:50,740
faceting by function analytics component

00:02:47,710 --> 00:02:52,810
and solar a standalone application now

00:02:50,740 --> 00:02:55,690
when the release candidate tree is

00:02:52,810 --> 00:02:58,750
already in the works for Solar version

00:02:55,690 --> 00:03:02,080
point five point two all those things

00:02:58,750 --> 00:03:04,390
are active and we can use them with a

00:03:02,080 --> 00:03:06,130
stable release almost the on the side of

00:03:04,390 --> 00:03:09,400
elasticsearch we said the top hits

00:03:06,130 --> 00:03:12,160
aggregation to enable field collapsing

00:03:09,400 --> 00:03:16,180
and not only the minimum should match on

00:03:12,160 --> 00:03:18,190
his child to allow us to actually match

00:03:16,180 --> 00:03:20,860
documents on the number basic on the

00:03:18,190 --> 00:03:22,840
number of the children hits and the

00:03:20,860 --> 00:03:25,450
filters aggregation which we can use

00:03:22,840 --> 00:03:28,540
filters to pocket the aggregations and

00:03:25,450 --> 00:03:31,450
that's again everything in current in

00:03:28,540 --> 00:03:32,830
the current versions we use which for

00:03:31,450 --> 00:03:35,830
elasticsearch is one point five

00:03:32,830 --> 00:03:39,190
something there but that's not all

00:03:35,830 --> 00:03:41,709
in the upcoming releases actually we'll

00:03:39,190 --> 00:03:43,180
be seeing a lot more from those two

00:03:41,709 --> 00:03:45,910
products starting from elasticsearch

00:03:43,180 --> 00:03:47,830
into o will see the computational

00:03:45,910 --> 00:03:49,930
aggregations introduced which allow us

00:03:47,830 --> 00:03:52,000
to do computation on the pockets of

00:03:49,930 --> 00:03:54,610
aggregation which is again great because

00:03:52,000 --> 00:03:56,620
it for example allows us to do more

00:03:54,610 --> 00:03:58,300
sophisticated data analysis like the

00:03:56,620 --> 00:04:00,459
moving averages aggregation which is

00:03:58,300 --> 00:04:02,170
already a part of to over release which

00:04:00,459 --> 00:04:04,480
will be released hopefully sometimes

00:04:02,170 --> 00:04:07,720
this year we will have the cluster

00:04:04,480 --> 00:04:09,700
different support to easier handle

00:04:07,720 --> 00:04:13,120
accuse cases where you have a large

00:04:09,700 --> 00:04:16,239
number of shards in your cluster and the

00:04:13,120 --> 00:04:19,090
shadow replicas are already here which

00:04:16,239 --> 00:04:20,980
means that you can just reuse the binary

00:04:19,090 --> 00:04:23,860
copy of loosing index instead of using

00:04:20,980 --> 00:04:26,110
the transaction log and index on each of

00:04:23,860 --> 00:04:28,900
the replicas on solar side five to

00:04:26,110 --> 00:04:31,570
version and five one starting from five

00:04:28,900 --> 00:04:33,850
one we have the JSON facets that we can

00:04:31,570 --> 00:04:36,100
use JSON similar to what we have already

00:04:33,850 --> 00:04:39,230
in elasticsearch to construct our files

00:04:36,100 --> 00:04:41,450
that will say have the nested facet

00:04:39,230 --> 00:04:43,490
point two will have the backup and

00:04:41,450 --> 00:04:45,470
restore functionality with your

00:04:43,490 --> 00:04:49,370
applications and allowing us to just

00:04:45,470 --> 00:04:51,800
back up and use the backup with an API

00:04:49,370 --> 00:04:53,480
call again similar to what we already

00:04:51,800 --> 00:04:55,310
have in elasticsearch the streaming

00:04:53,480 --> 00:04:58,160
aggregations which allows us to build

00:04:55,310 --> 00:05:00,230
very fast processing for the data like

00:04:58,160 --> 00:05:02,420
sorting engines and the cross data

00:05:00,230 --> 00:05:04,670
center replication which is hopefully

00:05:02,420 --> 00:05:07,730
coming in five point three which allows

00:05:04,670 --> 00:05:09,350
us to replicate the data between data

00:05:07,730 --> 00:05:13,270
centers and have more fault tolerant

00:05:09,350 --> 00:05:16,040
setups but for this year we wanted to

00:05:13,270 --> 00:05:18,440
tell you about three things first of all

00:05:16,040 --> 00:05:21,020
some kind of theory behind horizontal

00:05:18,440 --> 00:05:24,110
scaling of both because they are similar

00:05:21,020 --> 00:05:26,240
when it comes to the theory and quite

00:05:24,110 --> 00:05:28,010
different when it comes to practice then

00:05:26,240 --> 00:05:31,310
we'll show you two use case the product

00:05:28,010 --> 00:05:36,290
ones and the logs use case will show you

00:05:31,310 --> 00:05:38,960
the performance in those cases and a few

00:05:36,290 --> 00:05:40,730
nice lights hopefully so when it comes

00:05:38,960 --> 00:05:42,980
to horizontal scaling in theory we have

00:05:40,730 --> 00:05:46,820
the nodes in both elastic certain sore

00:05:42,980 --> 00:05:51,050
will each index in elastic social

00:05:46,820 --> 00:05:53,780
collection in solar is built of shard at

00:05:51,050 --> 00:05:56,180
least one we can have multiple nodes

00:05:53,780 --> 00:05:59,060
then we need multiple shards so our

00:05:56,180 --> 00:06:01,430
index or collection needs to be built of

00:05:59,060 --> 00:06:03,050
multiple shards divided and put into

00:06:01,430 --> 00:06:05,060
onto the nodes this is done

00:06:03,050 --> 00:06:07,100
automatically of course if we need to

00:06:05,060 --> 00:06:11,950
plan for the future we can have multiple

00:06:07,100 --> 00:06:15,140
shots already put already created nodes

00:06:11,950 --> 00:06:18,320
so we have over sharding to allow us to

00:06:15,140 --> 00:06:20,750
split to place the shard on new nodes in

00:06:18,320 --> 00:06:23,090
in the future whenever we walk of course

00:06:20,750 --> 00:06:26,270
for a rich throughput increase we have

00:06:23,090 --> 00:06:28,430
the replicas for each shard we can

00:06:26,270 --> 00:06:30,500
create both in elasticsearch and solar

00:06:28,430 --> 00:06:32,510
we can use the API and trade the

00:06:30,500 --> 00:06:35,900
replicas whatever we want on demand and

00:06:32,510 --> 00:06:38,910
that will be done automatically I'd like

00:06:35,900 --> 00:06:41,349
to add a few things about how these

00:06:38,910 --> 00:06:43,210
sharks and replicas are being managed

00:06:41,349 --> 00:06:44,919
with both elasticsearch and solar

00:06:43,210 --> 00:06:48,000
so with elasticsearch have the nice

00:06:44,919 --> 00:06:50,710
thing that it automatically balances

00:06:48,000 --> 00:06:52,659
charge the replicas across your cluster

00:06:50,710 --> 00:06:55,930
and when you add nodes and remove them

00:06:52,659 --> 00:06:58,479
things are balanced on the fly if they

00:06:55,930 --> 00:07:00,220
are not as you want you can always move

00:06:58,479 --> 00:07:04,930
shards around by using the cluster

00:07:00,220 --> 00:07:08,500
reroute API if you want to change number

00:07:04,930 --> 00:07:11,280
of replicas or other index settings you

00:07:08,500 --> 00:07:14,500
can use the update indices settings API

00:07:11,280 --> 00:07:17,470
and the other cool thing that it has

00:07:14,500 --> 00:07:22,449
that solar hasn't at least right now you

00:07:17,470 --> 00:07:24,370
can use all sorts of rules to do to

00:07:22,449 --> 00:07:25,630
place your shards for example you can

00:07:24,370 --> 00:07:28,300
have no tears

00:07:25,630 --> 00:07:31,120
for example you can have fast nodes to

00:07:28,300 --> 00:07:36,190
to hold your recent data and slow nodes

00:07:31,120 --> 00:07:38,860
to hold your archived data and then you

00:07:36,190 --> 00:07:42,509
can tag them and have elasticsearch

00:07:38,860 --> 00:07:44,949
automatically place indices in those

00:07:42,509 --> 00:07:47,770
servers for you or you can have rack

00:07:44,949 --> 00:07:50,050
awareness and things like that on the

00:07:47,770 --> 00:07:53,430
solar side you have the collections API

00:07:50,050 --> 00:07:56,469
which you can use to create indices

00:07:53,430 --> 00:07:58,569
create replicas and things like that

00:07:56,469 --> 00:08:00,610
you can also use the collections API to

00:07:58,569 --> 00:08:02,919
move shards around though this is a bit

00:08:00,610 --> 00:08:04,900
more clumsy you have to create a new

00:08:02,919 --> 00:08:07,599
replica on the node you want to move the

00:08:04,900 --> 00:08:10,419
shard to and you have to then after it's

00:08:07,599 --> 00:08:14,080
completed delete the old replica from

00:08:10,419 --> 00:08:16,360
the old node but you have some nice

00:08:14,080 --> 00:08:17,919
features here as well that are not

00:08:16,360 --> 00:08:20,319
present in elasticsearch at least for

00:08:17,919 --> 00:08:22,210
now you can split a shard on-the-fly

00:08:20,319 --> 00:08:26,289
though this is an expensive operation

00:08:22,210 --> 00:08:28,300
because it has to sort of copy data but

00:08:26,289 --> 00:08:30,849
anyway you can do that and you can also

00:08:28,300 --> 00:08:34,240
migrate a slice of your data for example

00:08:30,849 --> 00:08:38,050
if you have multiple users data into the

00:08:34,240 --> 00:08:40,089
same shard and you use a routing key for

00:08:38,050 --> 00:08:44,130
each user you can take one users data

00:08:40,089 --> 00:08:48,330
out migrate it to its own collection

00:08:44,130 --> 00:08:51,270
again using the API okay so when it

00:08:48,330 --> 00:08:54,450
comes to product we did a few

00:08:51,270 --> 00:08:56,430
assumptions before the tests so we

00:08:54,450 --> 00:08:59,040
assumed and when you have a product's

00:08:56,430 --> 00:09:01,140
use case the data growth is quite steady

00:08:59,040 --> 00:09:04,470
it grows over time but it's not

00:09:01,140 --> 00:09:07,830
something like daily data or anything

00:09:04,470 --> 00:09:09,810
like that so we usually use one or few

00:09:07,830 --> 00:09:11,760
in this is not like in the logs use case

00:09:09,810 --> 00:09:13,680
in which rather we'll talk about we also

00:09:11,760 --> 00:09:15,570
have spikes in traffic's that we need to

00:09:13,680 --> 00:09:19,170
be prepared for because during

00:09:15,570 --> 00:09:22,050
promotions or holidays we can see a lot

00:09:19,170 --> 00:09:25,410
of traffic more than we are usually used

00:09:22,050 --> 00:09:28,380
to everyday in addition to that we need

00:09:25,410 --> 00:09:32,100
to be prepared for large QPS because the

00:09:28,380 --> 00:09:35,570
it in that use case you usually don't

00:09:32,100 --> 00:09:38,610
index that the number of data that you

00:09:35,570 --> 00:09:40,380
index when it comes to logs usually have

00:09:38,610 --> 00:09:43,290
a static index with small number of

00:09:40,380 --> 00:09:45,900
updates like the prices or the likes or

00:09:43,290 --> 00:09:48,690
views or anything like that it's also

00:09:45,900 --> 00:09:51,300
the data is also common because it has a

00:09:48,690 --> 00:09:54,090
common structure and the thing to

00:09:51,300 --> 00:09:57,780
remember is that it's mostly steady when

00:09:54,090 --> 00:09:59,310
it comes to indexing so the caches and

00:09:57,780 --> 00:10:03,170
all that stuff is not refreshed

00:09:59,310 --> 00:10:06,270
very often the hardware we've used for

00:10:03,170 --> 00:10:09,060
tests were too easy to see three large

00:10:06,270 --> 00:10:12,390
instances each having to see two virtual

00:10:09,060 --> 00:10:15,810
CPUs three and a half gigs of RAM few

00:10:12,390 --> 00:10:18,840
Meg's of SSDs actually two of them two

00:10:15,810 --> 00:10:21,660
of sixteen gigabytes SSDs in right:0 and

00:10:18,840 --> 00:10:23,910
we used Wikipedia data Spanish Wikipedia

00:10:21,660 --> 00:10:27,570
with more or less than gigabytes per

00:10:23,910 --> 00:10:29,630
node of of index the test requests look

00:10:27,570 --> 00:10:31,980
like this we have the one common query

00:10:29,630 --> 00:10:36,180
that were run using jmeter

00:10:31,980 --> 00:10:37,980
hammering the instances and we have a

00:10:36,180 --> 00:10:42,090
dictionary of common and uncommon terms

00:10:37,980 --> 00:10:44,460
that we just used to simulate more or

00:10:42,090 --> 00:10:46,620
less simulate what we would expect from

00:10:44,460 --> 00:10:48,900
the users however remember those were

00:10:46,620 --> 00:10:51,030
syntactic tests you can do them you can

00:10:48,900 --> 00:10:53,339
see the here is a github account that

00:10:51,030 --> 00:10:56,440
holds the data and all that stuff that

00:10:53,339 --> 00:10:58,810
we used for tests but still this is

00:10:56,440 --> 00:11:01,200
a synthetic test nowhere near production

00:10:58,810 --> 00:11:04,510
we wanted to show a few things actually

00:11:01,200 --> 00:11:06,820
so when it came to the product search at

00:11:04,510 --> 00:11:10,690
five threats of Demeter we had a true

00:11:06,820 --> 00:11:13,960
put like this about five hundred queries

00:11:10,690 --> 00:11:16,750
per second for elastic search almost

00:11:13,960 --> 00:11:19,330
three hundred twenty queries per solar

00:11:16,750 --> 00:11:22,780
so we should say that in this case

00:11:19,330 --> 00:11:24,790
elastic search is faster than solar but

00:11:22,780 --> 00:11:27,460
we've noticed one thing yesterday

00:11:24,790 --> 00:11:29,760
everyone makes mistakes and we also did

00:11:27,460 --> 00:11:33,100
one actually when we were indexing data

00:11:29,760 --> 00:11:37,030
we noticed that solar had about forty

00:11:33,100 --> 00:11:38,830
percent more Wikipedia data than elastic

00:11:37,030 --> 00:11:44,380
search we don't know why we used the

00:11:38,830 --> 00:11:46,870
same XML but it was five o'clock

00:11:44,380 --> 00:11:52,540
yesterday so we needed to do a few more

00:11:46,870 --> 00:11:55,360
tests with it yeah so a scrub not to say

00:11:52,540 --> 00:11:58,300
it harder so we did another test the

00:11:55,360 --> 00:12:00,580
second try and this time we took simpler

00:11:58,300 --> 00:12:05,800
data we took a video sir to index 10

00:12:00,580 --> 00:12:07,870
million of documents for each and now

00:12:05,800 --> 00:12:10,420
the funny thing happened when we start a

00:12:07,870 --> 00:12:13,480
jmeter with 20 threads we've seen

00:12:10,420 --> 00:12:16,750
something like this the blue one is the

00:12:13,480 --> 00:12:18,880
solar throughput and orange one is

00:12:16,750 --> 00:12:21,490
elasticsearch and now i would like to

00:12:18,880 --> 00:12:22,090
ask you a question why that did that

00:12:21,490 --> 00:12:24,660
happen

00:12:22,090 --> 00:12:24,660
anyone knows

00:12:30,439 --> 00:12:38,759
close close close but not close enough

00:12:34,819 --> 00:12:43,829
so basic or mop run first and then do

00:12:38,759 --> 00:12:48,420
yes basically you how many of you know

00:12:43,829 --> 00:12:52,740
solar okay I suppose the rest of us

00:12:48,420 --> 00:12:56,009
normal a stick search right okay so

00:12:52,740 --> 00:12:59,970
basically in solar you have those top

00:12:56,009 --> 00:13:02,550
level caches the ones that we are

00:12:59,970 --> 00:13:06,620
usually used to not counting the per

00:13:02,550 --> 00:13:10,980
segment one for nested and block and

00:13:06,620 --> 00:13:13,769
blocked okay furnace the documents let's

00:13:10,980 --> 00:13:15,360
let's end it we have three x types of

00:13:13,769 --> 00:13:18,170
caches the filter catch the query result

00:13:15,360 --> 00:13:22,050
cache and document cache in that case

00:13:18,170 --> 00:13:26,459
because of the query is were not diverse

00:13:22,050 --> 00:13:28,649
enough we just after the first time the

00:13:26,459 --> 00:13:32,249
query was run it was put into the cache

00:13:28,649 --> 00:13:34,860
and when the second query of similar

00:13:32,249 --> 00:13:38,879
structure came with the same phrase the

00:13:34,860 --> 00:13:40,949
same star trolls and sort soul didn't

00:13:38,879 --> 00:13:42,629
have to go to the same index it just

00:13:40,949 --> 00:13:45,269
took an entry from the query result

00:13:42,629 --> 00:13:47,399
cache and returned and return the data

00:13:45,269 --> 00:13:49,589
this is why you see the difference when

00:13:47,399 --> 00:13:51,839
we turned on the caches the situation

00:13:49,589 --> 00:13:54,389
was completely different you can now see

00:13:51,839 --> 00:13:57,779
that we ran exactly the same test but

00:13:54,389 --> 00:13:59,579
without solar caching enabled we throw

00:13:57,779 --> 00:14:01,680
it away from the configuration from

00:13:59,579 --> 00:14:03,600
Solar config completely and now you can

00:14:01,680 --> 00:14:05,490
see that the queries per minute still

00:14:03,600 --> 00:14:07,410
for elastic search we have the same the

00:14:05,490 --> 00:14:09,209
machines were overloaded so it's not

00:14:07,410 --> 00:14:12,389
something that you should expect your

00:14:09,209 --> 00:14:15,300
elastic search to all only have 300

00:14:12,389 --> 00:14:17,100
words per minute of throughput but for

00:14:15,300 --> 00:14:22,709
that use case it was it was like this

00:14:17,100 --> 00:14:25,170
and Solar hat 156 when it comes to

00:14:22,709 --> 00:14:28,439
Triple A queries per minute so you can

00:14:25,170 --> 00:14:31,709
see that the caches here play a major

00:14:28,439 --> 00:14:33,550
role and what we actually wanted to say

00:14:31,709 --> 00:14:36,130
is that

00:14:33,550 --> 00:14:38,470
they're about fast you need to plan for

00:14:36,130 --> 00:14:41,230
the very fact road and prepare for a

00:14:38,470 --> 00:14:44,440
hike ups but remember that configuration

00:14:41,230 --> 00:14:48,060
matters a lot in both cases so if you go

00:14:44,440 --> 00:14:50,800
with your use case and you take the

00:14:48,060 --> 00:14:54,970
older all the stuff that is out of the

00:14:50,800 --> 00:14:57,730
box you may actually not have the

00:14:54,970 --> 00:14:59,410
optimal performance or for both when it

00:14:57,730 --> 00:15:02,950
comes to elasticsearch and when it comes

00:14:59,410 --> 00:15:04,899
to solar all right let's move on to the

00:15:02,950 --> 00:15:06,610
logs use case which typically has

00:15:04,899 --> 00:15:08,440
completely different pattern you have

00:15:06,610 --> 00:15:11,410
lots of data coming in lots of data

00:15:08,440 --> 00:15:14,200
being deleted every day because you have

00:15:11,410 --> 00:15:16,360
a fixed retention usually you don't have

00:15:14,200 --> 00:15:18,010
that many queries but those queries may

00:15:16,360 --> 00:15:20,740
be more expensive if you run them one

00:15:18,010 --> 00:15:23,399
more data there are no updates and

00:15:20,740 --> 00:15:26,770
things like that once you write a log in

00:15:23,399 --> 00:15:28,990
we have the same clusters for testing

00:15:26,770 --> 00:15:33,149
only this time we indexed Apache logs

00:15:28,990 --> 00:15:35,520
over and over again on the settings side

00:15:33,149 --> 00:15:37,750
we tried to have equivalent

00:15:35,520 --> 00:15:40,570
configurations again like with the

00:15:37,750 --> 00:15:45,910
products we had the same sort of schemas

00:15:40,570 --> 00:15:50,230
we had Dogg values in both use cases for

00:15:45,910 --> 00:15:52,270
not analyzed fields we did soft commits

00:15:50,230 --> 00:15:54,370
every 5 seconds as we were indexing and

00:15:52,270 --> 00:15:57,060
hard commits every 200 megabytes and

00:15:54,370 --> 00:16:00,070
because elasticsearch has that under

00:15:57,060 --> 00:16:03,690
underscore all field to index everything

00:16:00,070 --> 00:16:05,860
by default we did the same for for solar

00:16:03,690 --> 00:16:11,190
sort of the same we had a catch-all

00:16:05,860 --> 00:16:16,180
field where we copied everything so the

00:16:11,190 --> 00:16:19,089
queries we had 7 queries we have a bunch

00:16:16,180 --> 00:16:22,950
of filters from starting from a simple

00:16:19,089 --> 00:16:25,750
filtering by a client IP to a more

00:16:22,950 --> 00:16:28,180
expensive filter like filtering by well

00:16:25,750 --> 00:16:32,470
card and then we had a bunch of

00:16:28,180 --> 00:16:34,810
analytics queries from a date histogram

00:16:32,470 --> 00:16:38,649
to the most expensive one which was a

00:16:34,810 --> 00:16:40,209
nested a nested aggregation if you were

00:16:38,649 --> 00:16:41,980
you're wondering how we did the nested

00:16:40,209 --> 00:16:45,570
aggregation on Soler is because we use

00:16:41,980 --> 00:16:48,750
the 5.2 snapshot which will

00:16:45,570 --> 00:16:52,530
that feature for elasticsearch we use

00:16:48,750 --> 00:16:56,790
the snapshot of the upcoming 2.0 of

00:16:52,530 --> 00:16:59,670
about two weeks to a month ago so we did

00:16:56,790 --> 00:17:01,800
three tests in this use case one the

00:16:59,670 --> 00:17:05,550
first one was to write as much as we can

00:17:01,800 --> 00:17:08,339
to a single index and then the second

00:17:05,550 --> 00:17:12,270
one would be to write steadily at a

00:17:08,339 --> 00:17:15,390
lower rate and do a more realistic

00:17:12,270 --> 00:17:17,209
measure of query latency and then we

00:17:15,390 --> 00:17:19,829
moved to an even more realistic

00:17:17,209 --> 00:17:22,290
production setup where we had time-based

00:17:19,829 --> 00:17:25,230
indices and we also have node layers

00:17:22,290 --> 00:17:28,199
like the hot notes to hold the latest

00:17:25,230 --> 00:17:32,160
index and to get all the indexing hits

00:17:28,199 --> 00:17:35,100
and spikes and then cold notes to hold

00:17:32,160 --> 00:17:39,030
the archive data and we ran the queries

00:17:35,100 --> 00:17:41,220
on on this setup as well all right so

00:17:39,030 --> 00:17:43,320
the first test we we just had a single

00:17:41,220 --> 00:17:47,310
index or single collection and wrote as

00:17:43,320 --> 00:17:49,830
much as we could both have very good

00:17:47,310 --> 00:17:52,110
indexing throughput I think it's much

00:17:49,830 --> 00:17:55,470
more than it would probably need because

00:17:52,110 --> 00:17:59,780
the index will just fill up in terms of

00:17:55,470 --> 00:17:59,780
disk space in in a few hours

00:18:00,410 --> 00:18:06,330
solar is slightly better here indexing

00:18:03,950 --> 00:18:09,120
but you can see both that our

00:18:06,330 --> 00:18:13,020
performance is degrading as the index

00:18:09,120 --> 00:18:14,880
grows and this happens because merges

00:18:13,020 --> 00:18:17,580
are happening in the background so for

00:18:14,880 --> 00:18:19,140
those who don't know when you're right

00:18:17,580 --> 00:18:21,330
to loosen index

00:18:19,140 --> 00:18:23,490
this happens in segments which are

00:18:21,330 --> 00:18:25,680
immutable and searches go through all

00:18:23,490 --> 00:18:27,420
these segments so in order to keep the

00:18:25,680 --> 00:18:29,100
number of segments in check in

00:18:27,420 --> 00:18:31,470
background you have to merge smaller

00:18:29,100 --> 00:18:35,070
segments into bigger ones and this is IO

00:18:31,470 --> 00:18:39,090
intensive and also quite CPU intensive

00:18:35,070 --> 00:18:41,790
zuv this bites from the from the

00:18:39,090 --> 00:18:44,490
indexing performance you can see that

00:18:41,790 --> 00:18:47,490
the solar throughput falls down a bit

00:18:44,490 --> 00:18:51,540
more and I think this is because the

00:18:47,490 --> 00:18:53,820
index grows a bit more so a bit faster

00:18:51,540 --> 00:18:56,400
yeah a bit

00:18:53,820 --> 00:18:59,070
and during that run we also did the

00:18:56,400 --> 00:19:02,510
queries and you can see here that there

00:18:59,070 --> 00:19:02,510
are quite a few differences already

00:19:03,200 --> 00:19:08,940
elasticsearch does a lot better when it

00:19:06,270 --> 00:19:12,930
comes to filter in performance and this

00:19:08,940 --> 00:19:15,200
is again caused by caching because we

00:19:12,930 --> 00:19:17,790
run the same queries over and over again

00:19:15,200 --> 00:19:20,130
elastic search was able to catch them in

00:19:17,790 --> 00:19:22,050
per segment caches solar doesn't have

00:19:20,130 --> 00:19:24,480
that it has the overall caches which are

00:19:22,050 --> 00:19:26,310
invalidated with each right so we can

00:19:24,480 --> 00:19:28,350
see with solar we have more or less the

00:19:26,310 --> 00:19:32,910
same latency no matter how expensive the

00:19:28,350 --> 00:19:34,470
query is however solar is seems to be

00:19:32,910 --> 00:19:39,990
better when it comes to nesting at

00:19:34,470 --> 00:19:42,750
nested aggregations and both are quite

00:19:39,990 --> 00:19:45,480
spiky because you know the the servers

00:19:42,750 --> 00:19:47,460
were were hammered quite a lot but you

00:19:45,480 --> 00:19:50,270
can see solar is even more spike here

00:19:47,460 --> 00:19:53,630
and when you see the zero

00:19:50,270 --> 00:19:59,850
yeah the zero Laden sees that is when

00:19:53,630 --> 00:20:02,250
queries actually timed out so now the

00:19:59,850 --> 00:20:03,960
second test we indexed at 400 events per

00:20:02,250 --> 00:20:06,510
second which may not sound like a lot

00:20:03,960 --> 00:20:09,240
but for this cluster it would fill up

00:20:06,510 --> 00:20:14,010
the disk space of nodes in two or three

00:20:09,240 --> 00:20:15,570
days and usually if you need more you

00:20:14,010 --> 00:20:16,800
need more usually you need more

00:20:15,570 --> 00:20:19,980
attention than that so you would

00:20:16,800 --> 00:20:21,540
probably have a bigger cluster so we

00:20:19,980 --> 00:20:25,320
have for this cluster 400 events per

00:20:21,540 --> 00:20:27,180
second is quite a lot in terms of being

00:20:25,320 --> 00:20:30,420
realistic it's not a lot in terms of

00:20:27,180 --> 00:20:32,370
load it barely feels it but anyway we

00:20:30,420 --> 00:20:35,010
were interested in query latencies and

00:20:32,370 --> 00:20:37,710
we can see more of the same elastic

00:20:35,010 --> 00:20:42,450
search is faster at filtering because of

00:20:37,710 --> 00:20:44,460
caching and solar is faster at the

00:20:42,450 --> 00:20:46,650
nested aggregation the simple

00:20:44,460 --> 00:20:48,950
aggregations take more or less the same

00:20:46,650 --> 00:20:48,950
time

00:20:52,120 --> 00:20:56,170
then we move to time-based indices and I

00:20:54,580 --> 00:20:58,420
would like to take a minute to explain

00:20:56,170 --> 00:21:00,520
why that is a good idea because if

00:20:58,420 --> 00:21:02,740
you're writing to a smaller index only

00:21:00,520 --> 00:21:04,690
to today's index for example you're

00:21:02,740 --> 00:21:07,450
invalidating less caches you're doing

00:21:04,690 --> 00:21:09,550
less merges so indexing will be faster

00:21:07,450 --> 00:21:11,910
also when you're searching because data

00:21:09,550 --> 00:21:14,170
is sliced up you can for example

00:21:11,910 --> 00:21:16,120
normally when you're searching for logs

00:21:14,170 --> 00:21:18,400
you're searching only on the recent data

00:21:16,120 --> 00:21:20,440
and only occasionally you do like

00:21:18,400 --> 00:21:23,440
analytics over the whole data set and

00:21:20,440 --> 00:21:25,270
things like that and if you have time

00:21:23,440 --> 00:21:27,040
based indices then you can just search

00:21:25,270 --> 00:21:29,680
the chunk of indices that you're

00:21:27,040 --> 00:21:34,360
interested in so these searches will be

00:21:29,680 --> 00:21:37,030
faster and also when you when you delete

00:21:34,360 --> 00:21:38,770
data you can delete entire indices

00:21:37,030 --> 00:21:40,480
instead of deleting documents from

00:21:38,770 --> 00:21:44,050
within an index and causing even more

00:21:40,480 --> 00:21:47,020
merging and more load of course having

00:21:44,050 --> 00:21:49,180
too small indices has an overhead

00:21:47,020 --> 00:21:51,840
because each index has an overhead in

00:21:49,180 --> 00:21:54,100
terms of memory when you're monitoring

00:21:51,840 --> 00:21:57,120
elasticsearch and solar you have stats

00:21:54,100 --> 00:21:59,410
that are per index or per collection and

00:21:57,120 --> 00:22:00,820
you know the cluster state is bigger and

00:21:59,410 --> 00:22:05,230
things like that so it's a trade-off

00:22:00,820 --> 00:22:07,420
there but yeah in this case we just went

00:22:05,230 --> 00:22:10,450
with maximum indexing throughput which

00:22:07,420 --> 00:22:18,270
would which would spanned the whole test

00:22:10,450 --> 00:22:21,430
in a few hours and our index slices were

00:22:18,270 --> 00:22:24,460
100,000 seconds each so three indices

00:22:21,430 --> 00:22:29,140
per hour we had about 15 to 20 indices

00:22:24,460 --> 00:22:31,360
in total okay so let me give you a few

00:22:29,140 --> 00:22:34,090
words about how this works in practice

00:22:31,360 --> 00:22:36,520
actually so we start - we'll start with

00:22:34,090 --> 00:22:38,650
elastic search in elastic search we have

00:22:36,520 --> 00:22:41,020
the notion of properties we can set

00:22:38,650 --> 00:22:45,100
properties for a node and we will use

00:22:41,020 --> 00:22:47,560
that in called the cold node setup so

00:22:45,100 --> 00:22:51,130
for the code nodes we just said for

00:22:47,560 --> 00:22:53,440
example node dot tag property - just

00:22:51,130 --> 00:22:57,700
explicit in the hot and the same we do

00:22:53,440 --> 00:23:01,210
for cold nodes so we have a from two

00:22:57,700 --> 00:23:03,520
types of nodes once one set with not dr.

00:23:01,210 --> 00:23:05,350
called and the other called

00:23:03,520 --> 00:23:08,320
then we use the index templates and

00:23:05,350 --> 00:23:10,180
short allocation awareness to actually

00:23:08,320 --> 00:23:12,730
create new indices the good thing about

00:23:10,180 --> 00:23:15,130
the elasticsearch here is that whenever

00:23:12,730 --> 00:23:17,440
your data comes in and the index is not

00:23:15,130 --> 00:23:20,380
created by default elasticsearch would

00:23:17,440 --> 00:23:22,570
create a new index so if we would for

00:23:20,380 --> 00:23:24,430
example start logging start sending logs

00:23:22,570 --> 00:23:26,950
right now to elasticsearch that was just

00:23:24,430 --> 00:23:29,740
started it would create the proper index

00:23:26,950 --> 00:23:32,980
use the index templates that we put

00:23:29,740 --> 00:23:36,610
there which has the short allocation

00:23:32,980 --> 00:23:40,510
awareness attributes so we say that for

00:23:36,610 --> 00:23:45,280
new indices that are newly created they

00:23:40,510 --> 00:23:47,860
should be they should have that node dot

00:23:45,280 --> 00:23:50,290
tag equals to hot that means that

00:23:47,860 --> 00:23:53,140
elasticsearch will create them on the

00:23:50,290 --> 00:23:56,290
hot nodes when that happens and that

00:23:53,140 --> 00:23:59,530
happens automatically we don't have to

00:23:56,290 --> 00:24:02,700
care about it at all then when there is

00:23:59,530 --> 00:24:05,470
a time that we want that hot index to be

00:24:02,700 --> 00:24:07,210
actually moved to a cold storage that

00:24:05,470 --> 00:24:11,350
happens after one day two days depending

00:24:07,210 --> 00:24:14,650
on what we call the hot storage because

00:24:11,350 --> 00:24:18,070
we are not indexing history called the

00:24:14,650 --> 00:24:20,740
historical data usually we can force

00:24:18,070 --> 00:24:22,630
merge the index so we lower down the

00:24:20,740 --> 00:24:24,790
number of segments to a single segment

00:24:22,630 --> 00:24:26,920
to speed up the queries because the more

00:24:24,790 --> 00:24:29,800
segments we have in Lucene index the

00:24:26,920 --> 00:24:32,170
slower the queries will be at least

00:24:29,800 --> 00:24:34,600
slightly so we have a crunch up on this

00:24:32,170 --> 00:24:37,840
elastic search side that will optimize

00:24:34,600 --> 00:24:40,810
the index to do the first merge with max

00:24:37,840 --> 00:24:44,710
segments equal to 1 and update the shard

00:24:40,810 --> 00:24:48,010
allocation attribute to point to no dot

00:24:44,710 --> 00:24:50,260
tag code which means that the elastic

00:24:48,010 --> 00:24:53,050
search will just move automatically the

00:24:50,260 --> 00:24:55,900
index from the hot nodes to the cold one

00:24:53,050 --> 00:24:57,580
and again that happened automatically we

00:24:55,900 --> 00:25:00,490
don't have to do anything about that

00:24:57,580 --> 00:25:02,710
when it comes to solar things are a bit

00:25:00,490 --> 00:25:04,600
more complicated because we have to do

00:25:02,710 --> 00:25:07,000
everything with a cron job we have the

00:25:04,600 --> 00:25:09,970
API but still we need to do the

00:25:07,000 --> 00:25:12,040
automation ourselves so first of all

00:25:09,970 --> 00:25:14,560
solar won't create collections in

00:25:12,040 --> 00:25:16,120
advance or when the data comes in if you

00:25:14,560 --> 00:25:16,560
pull the data to a collection that is

00:25:16,120 --> 00:25:19,320
not

00:25:16,560 --> 00:25:22,590
and he's not grated soul we'll just say

00:25:19,320 --> 00:25:25,980
go away with the late I don't like it so

00:25:22,590 --> 00:25:30,270
we need to create in the collections in

00:25:25,980 --> 00:25:33,300
advance to point out to a given nodes we

00:25:30,270 --> 00:25:35,640
again use property in a in a command

00:25:33,300 --> 00:25:37,680
that creates the collection called the

00:25:35,640 --> 00:25:39,240
create node set and we said and we

00:25:37,680 --> 00:25:42,630
specify the names of the notes for

00:25:39,240 --> 00:25:45,870
example so node 1 2 3 will be hot and

00:25:42,630 --> 00:25:50,370
this is where the core hot the

00:25:45,870 --> 00:25:52,860
collection will be deployed then we have

00:25:50,370 --> 00:25:55,890
the again the situation that after a few

00:25:52,860 --> 00:25:59,040
days we want to move that to that

00:25:55,890 --> 00:26:00,960
collection to cold nodes and first we of

00:25:59,040 --> 00:26:04,140
course optimize again with the API call

00:26:00,960 --> 00:26:07,590
and here we can't update the properties

00:26:04,140 --> 00:26:09,480
we just need to create new replicas for

00:26:07,590 --> 00:26:12,870
that particular collection on the cold

00:26:09,480 --> 00:26:15,570
node allow solar to replicate the data

00:26:12,870 --> 00:26:19,110
using the replication functionality and

00:26:15,570 --> 00:26:22,260
after the application has been done we

00:26:19,110 --> 00:26:25,620
can remove the collection from hot nodes

00:26:22,260 --> 00:26:28,800
and this is what the process for all for

00:26:25,620 --> 00:26:31,260
both of them so as we can see in in that

00:26:28,800 --> 00:26:35,040
case elasticsearch is allows us to

00:26:31,260 --> 00:26:39,480
easier handle the process of moving

00:26:35,040 --> 00:26:40,920
between hot and cold notes so to give

00:26:39,480 --> 00:26:43,230
you a notion of how the indexing was

00:26:40,920 --> 00:26:46,760
working during that time we can see that

00:26:43,230 --> 00:26:50,310
the top top two graphs are the before

00:26:46,760 --> 00:26:53,820
before implementing the procedure of hot

00:26:50,310 --> 00:26:56,190
and cold note after we've implemented it

00:26:53,820 --> 00:26:59,330
we can see that force or we have almost

00:26:56,190 --> 00:27:02,730
constant wearing

00:26:59,330 --> 00:27:05,850
indexing throughput but spiky why spiky

00:27:02,730 --> 00:27:07,830
we are not sure yet whether there is a

00:27:05,850 --> 00:27:10,920
bargain sore or something like that but

00:27:07,830 --> 00:27:14,220
when we were running that with a cron

00:27:10,920 --> 00:27:17,220
job if we were switching the collections

00:27:14,220 --> 00:27:19,710
from hope to cold notes when the

00:27:17,220 --> 00:27:22,380
indexing happened we get indexed

00:27:19,710 --> 00:27:24,270
indexing stalled so we need to figure

00:27:22,380 --> 00:27:26,190
out yet why that happened but if we

00:27:24,270 --> 00:27:28,320
would actually stop the indexing for a

00:27:26,190 --> 00:27:30,060
second everything would be ok and this

00:27:28,320 --> 00:27:33,800
is actually the spikes are

00:27:30,060 --> 00:27:36,710
what because we stopped the indexing I

00:27:33,800 --> 00:27:40,170
suppose that if when we found the case

00:27:36,710 --> 00:27:41,850
cause it will be mostly static and in

00:27:40,170 --> 00:27:43,650
elasticsearch again we can see the

00:27:41,850 --> 00:27:46,800
increasing throughput because we are

00:27:43,650 --> 00:27:51,180
indexing to smaller indices the segment

00:27:46,800 --> 00:27:52,920
merging is less stressful for the

00:27:51,180 --> 00:27:55,470
operating system and for us in itself

00:27:52,920 --> 00:27:59,490
and we can see the increase in

00:27:55,470 --> 00:28:04,520
throughput from 3.2 K per second to

00:27:59,490 --> 00:28:07,380
about 4.6 K so that the growth is pretty

00:28:04,520 --> 00:28:12,240
pretty nice when it comes to indexing

00:28:07,380 --> 00:28:15,900
logs and for the queries which we can

00:28:12,240 --> 00:28:21,660
see more of what we've seen before in

00:28:15,900 --> 00:28:23,640
terms of how they scale but with a hot

00:28:21,660 --> 00:28:26,760
and cold setup because the cold nodes

00:28:23,640 --> 00:28:29,190
hold data that is now optimized and

00:28:26,760 --> 00:28:33,120
they're not bothered by any indexing or

00:28:29,190 --> 00:28:36,320
cache invalidation now they can hold up

00:28:33,120 --> 00:28:40,200
to three or four times more data than

00:28:36,320 --> 00:28:43,080
than the nodes that we had before in in

00:28:40,200 --> 00:28:46,760
both cases and the elasticsearch again

00:28:43,080 --> 00:28:51,810
is very good at doing filtering and

00:28:46,760 --> 00:28:54,180
solar now in this in this case was about

00:28:51,810 --> 00:28:57,750
twice as fast when it comes to simple

00:28:54,180 --> 00:28:59,700
aggregations or facets and about ten

00:28:57,750 --> 00:29:02,910
times as fast when it comes to the

00:28:59,700 --> 00:29:06,200
nested irrigation yeah the filters on

00:29:02,910 --> 00:29:10,700
the elastic search side are are about

00:29:06,200 --> 00:29:14,850
two times two times faster yeah still

00:29:10,700 --> 00:29:17,310
okay so to wrap this up with with the

00:29:14,850 --> 00:29:20,400
logs use case using time based indices

00:29:17,310 --> 00:29:24,120
and hot and cold nodes policy helps in

00:29:20,400 --> 00:29:26,760
both use cases and elastic search tends

00:29:24,120 --> 00:29:29,790
to be faster at filtering and solar

00:29:26,760 --> 00:29:31,620
tends to be faster at faceting it is

00:29:29,790 --> 00:29:35,700
worth noting here that elastic search

00:29:31,620 --> 00:29:37,200
has more capable with solar 5.2 with the

00:29:35,700 --> 00:29:39,770
nested facet and things like that

00:29:37,200 --> 00:29:42,900
with elastic search you can do more

00:29:39,770 --> 00:29:45,450
kinds of aggregations so we

00:29:42,900 --> 00:29:46,860
in this case we tried to compare common

00:29:45,450 --> 00:29:52,350
functionality just to look at the

00:29:46,860 --> 00:29:55,860
performance so just as a summary we

00:29:52,350 --> 00:29:59,730
speed up things slightly so just as a

00:29:55,860 --> 00:30:03,120
summary for the things we did for the

00:29:59,730 --> 00:30:05,460
talk the tests we've noticed that in

00:30:03,120 --> 00:30:08,340
most cases the difference in

00:30:05,460 --> 00:30:10,410
configuration you views matters more

00:30:08,340 --> 00:30:12,450
than it comes then when it comes to the

00:30:10,410 --> 00:30:16,350
product you use because if you configure

00:30:12,450 --> 00:30:19,620
soror elasticsearch in a bad way it will

00:30:16,350 --> 00:30:21,450
perform bad so this is this is what you

00:30:19,620 --> 00:30:24,510
what you will get for example you've

00:30:21,450 --> 00:30:28,440
seen the querying quids or having the

00:30:24,510 --> 00:30:30,840
curl result cache by default set of 512

00:30:28,440 --> 00:30:33,510
and you've seen the throughput increase

00:30:30,840 --> 00:30:34,740
in a in a large in a large way however

00:30:33,510 --> 00:30:37,890
if you would remove the caches

00:30:34,740 --> 00:30:40,110
completely then we have lower lower

00:30:37,890 --> 00:30:42,510
performance way lower performance but in

00:30:40,110 --> 00:30:44,670
a real-world scenario in such case this

00:30:42,510 --> 00:30:46,800
cash would be invalidated constantly

00:30:44,670 --> 00:30:48,510
because you would probably have updates

00:30:46,800 --> 00:30:51,660
to the data because this is a top-level

00:30:48,510 --> 00:30:53,490
cache so whatever it segments is

00:30:51,660 --> 00:30:55,620
refreshed actually that cache needs to

00:30:53,490 --> 00:30:58,170
be invalidated and because of that that

00:30:55,620 --> 00:31:02,370
performance won't be as good as as

00:30:58,170 --> 00:31:04,020
you've seen we have to be honest we

00:31:02,370 --> 00:31:06,240
expect two slightly different results

00:31:04,020 --> 00:31:09,390
from the tests we did we were amazed

00:31:06,240 --> 00:31:11,640
about some some results we repeated the

00:31:09,390 --> 00:31:15,030
test to just make sure that everything

00:31:11,640 --> 00:31:17,550
is ok but we have the constant results

00:31:15,030 --> 00:31:19,620
like we've shown so if you would like to

00:31:17,550 --> 00:31:21,600
repeat that we've shown the github

00:31:19,620 --> 00:31:23,640
account please do that and let us know

00:31:21,600 --> 00:31:26,010
if that if you get the similar results

00:31:23,640 --> 00:31:28,080
or at least a similar pattern results

00:31:26,010 --> 00:31:29,460
because on different machines you'll get

00:31:28,080 --> 00:31:32,130
the different results that's that's

00:31:29,460 --> 00:31:34,290
normal and finally we would really

00:31:32,130 --> 00:31:36,510
encourage you to do your own test don't

00:31:34,290 --> 00:31:38,910
believe even us in the slides we have

00:31:36,510 --> 00:31:40,950
don't believe everything you see just do

00:31:38,910 --> 00:31:43,770
your own your own tests with your own

00:31:40,950 --> 00:31:46,650
data with your own queries and then you

00:31:43,770 --> 00:31:48,909
can say that for your use case this is

00:31:46,650 --> 00:31:51,249
how solar elasticsearch works

00:31:48,909 --> 00:31:54,070
take your numbers if you like to fake

00:31:51,249 --> 00:31:56,440
numbers like we do we are hiring this is

00:31:54,070 --> 00:31:58,239
a slide that I saw three years ago from

00:31:56,440 --> 00:32:02,409
a fall stock and this is how I got here

00:31:58,239 --> 00:32:05,470
so I think this is it colas might be and

00:32:02,409 --> 00:32:08,159
if you have any questions we have time

00:32:05,470 --> 00:32:08,159

YouTube URL: https://www.youtube.com/watch?v=01mXpZ0F-_o


