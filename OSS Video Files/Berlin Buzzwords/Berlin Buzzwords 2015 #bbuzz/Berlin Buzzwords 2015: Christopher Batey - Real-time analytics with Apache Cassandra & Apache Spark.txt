Title: Berlin Buzzwords 2015: Christopher Batey - Real-time analytics with Apache Cassandra & Apache Spark
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Time series data is everywhere: IoT, sensor data, financial transactions. The industry has moved to databases like Cassandra to handle the high velocity and high volume of data that is now common place. However data is pointless without being able to process it in near real time. That's where Spark combined with Cassandra comes in, what was one just your storage system can be transformed into your analytics system, and you'll be surprised how easy it is!

So, join me for a whirl wind tour of how to use these two awesome open source projects for time series data. We'll cover:
- An overview of Cassandra - Why is it so good for time series?
- An introduction to Spark 
- What canâ€™t be done in Cassandra and how Spark can fill in the gaps
- How to build analytics on top of your operational data without the typical Extract-Transform-Load
- Specific use cases: sensor data, customer event data, financial transactions

Read more:
https://2015.berlinbuzzwords.de/session/real-time-analytics-apache-cassandra-and-apache-spark

About Christopher Batey:
https://2015.berlinbuzzwords.de/users/christopher-batey

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,570 --> 00:00:10,920
okay so thanks everyone for

00:00:08,220 --> 00:00:12,570
for coming along to this talk the the

00:00:10,920 --> 00:00:15,030
keynote introduced it really well

00:00:12,570 --> 00:00:16,830
because it is essentially a technical

00:00:15,030 --> 00:00:20,700
deep dive into the stock that was

00:00:16,830 --> 00:00:22,050
mentioned in the in the keynote you

00:00:20,700 --> 00:00:23,880
might have to bear with me a little bit

00:00:22,050 --> 00:00:26,759
we've had to have an emergency laptop

00:00:23,880 --> 00:00:29,669
change so my normally animated slides

00:00:26,759 --> 00:00:31,680
are now coming out on a PDF so sometimes

00:00:29,669 --> 00:00:33,570
if there's lots of things on the slide I

00:00:31,680 --> 00:00:34,980
have to pretend there's only a few

00:00:33,570 --> 00:00:38,100
things and i'll tell you when they need

00:00:34,980 --> 00:00:39,780
to appear okay little introduction

00:00:38,100 --> 00:00:42,870
before i get going so you know who knew

00:00:39,780 --> 00:00:45,030
who I am so I work for datastax there's

00:00:42,870 --> 00:00:47,790
a company the kind of like the Cassandra

00:00:45,030 --> 00:00:50,310
company we contribute an awful amount of

00:00:47,790 --> 00:00:51,780
code to Apache Cassandra and we build a

00:00:50,310 --> 00:00:54,780
product on top of Cassandra that

00:00:51,780 --> 00:00:57,240
includes spark and we open source the

00:00:54,780 --> 00:00:58,740
connection between the two so the the

00:00:57,240 --> 00:01:00,420
spark Cassandra connector that was

00:00:58,740 --> 00:01:03,540
mentioned in the keynote was open

00:01:00,420 --> 00:01:05,640
sourced by datastax my background before

00:01:03,540 --> 00:01:08,430
datastax is building lots of systems

00:01:05,640 --> 00:01:10,920
with Cassandra okay so I've spent a

00:01:08,430 --> 00:01:12,960
couple of years basically deprecating

00:01:10,920 --> 00:01:14,400
large Oracle systems with Cassandra

00:01:12,960 --> 00:01:18,180
systems and then we started using

00:01:14,400 --> 00:01:19,920
sparkle on top of that so the warning

00:01:18,180 --> 00:01:21,690
sign would normally would normally fly

00:01:19,920 --> 00:01:23,670
up afterward but this is what we're

00:01:21,690 --> 00:01:26,159
hoping to get through in 40 minutes and

00:01:23,670 --> 00:01:29,190
it is quite a lot so if you're new to

00:01:26,159 --> 00:01:31,380
Cassandra spark stream processing this

00:01:29,190 --> 00:01:32,790
is going to be a very large brain dump

00:01:31,380 --> 00:01:35,340
okay it's going to be quite hard to

00:01:32,790 --> 00:01:36,690
assimilate if you're familiar with one

00:01:35,340 --> 00:01:38,760
or two of them then hopefully you can

00:01:36,690 --> 00:01:41,670
you can follow quite well but it's it's

00:01:38,760 --> 00:01:43,950
a lot to cover in 40 minutes I'm going

00:01:41,670 --> 00:01:46,290
to do it by that of an example ok so

00:01:43,950 --> 00:01:47,729
it's an example it's another type of IOT

00:01:46,290 --> 00:01:50,640
walnut about the processing of weather

00:01:47,729 --> 00:01:52,229
data going to introduce that then we're

00:01:50,640 --> 00:01:54,150
going to go through a bit of a technical

00:01:52,229 --> 00:01:57,210
deep dive into cassandra and how the

00:01:54,150 --> 00:01:59,460
spark connector works into cassandra and

00:01:57,210 --> 00:02:03,000
then show you some code ok and show you

00:01:59,460 --> 00:02:05,130
a spark streaming application so these

00:02:03,000 --> 00:02:07,799
do fly up in some crazy order which I've

00:02:05,130 --> 00:02:09,629
now forgotten but essentially it was

00:02:07,799 --> 00:02:13,319
like roughly to work out who in the room

00:02:09,629 --> 00:02:15,599
does what ok and I come from the left

00:02:13,319 --> 00:02:17,400
hand side of this ok so the online

00:02:15,599 --> 00:02:20,099
transaction processing which you can use

00:02:17,400 --> 00:02:21,990
to any kind of relational database any

00:02:20,099 --> 00:02:23,610
of these no sequel ones like coming

00:02:21,990 --> 00:02:26,340
there's a new database every week on

00:02:23,610 --> 00:02:28,290
Apache I think we've also got things

00:02:26,340 --> 00:02:30,420
like online transaction processing okay

00:02:28,290 --> 00:02:33,510
so that's being able to do things in the

00:02:30,420 --> 00:02:35,730
semi near real-time report generation

00:02:33,510 --> 00:02:37,500
learning from your customer data but

00:02:35,730 --> 00:02:40,020
eventually that gets too large and then

00:02:37,500 --> 00:02:41,850
we throw our data to Hadoop okay today

00:02:40,020 --> 00:02:44,400
we're going to talk about Cassandra for

00:02:41,850 --> 00:02:47,400
your operational database spark to do

00:02:44,400 --> 00:02:49,590
but jobs directly on top of Cassandra to

00:02:47,400 --> 00:02:50,610
do your kind of batch work but then

00:02:49,590 --> 00:02:52,770
we're going to talk about spark

00:02:50,610 --> 00:02:54,660
streaming for kind of keeping these

00:02:52,770 --> 00:02:58,440
botched things up to date throughout the

00:02:54,660 --> 00:03:00,630
course of the day okay and this is the

00:02:58,440 --> 00:03:02,190
this is the use case so this is a

00:03:00,630 --> 00:03:05,520
reference application you can go and

00:03:02,190 --> 00:03:08,910
play around with it uses Kafka for the

00:03:05,520 --> 00:03:11,160
ingestion of messages and I'm not going

00:03:08,910 --> 00:03:13,050
to talk about apache Kafka that's too

00:03:11,160 --> 00:03:14,370
many Apache projects in one day you just

00:03:13,050 --> 00:03:16,890
got just as if you don't know what it is

00:03:14,370 --> 00:03:18,270
just assume it's a but it's just a piece

00:03:16,890 --> 00:03:20,550
of queuing software think of it like

00:03:18,270 --> 00:03:22,650
that we're going to look about Cassandra

00:03:20,550 --> 00:03:24,680
for the straw storage of the data and

00:03:22,650 --> 00:03:28,140
we're gonna look at spark streaming for

00:03:24,680 --> 00:03:31,560
ingesting the data storing the raw data

00:03:28,140 --> 00:03:33,540
and also building up kind of on the fly

00:03:31,560 --> 00:03:36,600
like aggregates the kind of things which

00:03:33,540 --> 00:03:38,250
you you could do inside say my sequel or

00:03:36,600 --> 00:03:40,440
postgres if the data size was small

00:03:38,250 --> 00:03:42,360
enough but if your data size is big

00:03:40,440 --> 00:03:43,950
enough you can't do but we can we can we

00:03:42,360 --> 00:03:46,440
can replace that with them spike

00:03:43,950 --> 00:03:48,510
streaming and essentially the end goal

00:03:46,440 --> 00:03:50,340
is to build up a set of materialized

00:03:48,510 --> 00:03:52,890
views that we constantly keep up to date

00:03:50,340 --> 00:03:56,460
that an application could read directly

00:03:52,890 --> 00:03:59,370
out of Cassandra so though I'm by no

00:03:56,460 --> 00:04:01,260
means a UI developer i'm pretty much i'm

00:03:59,370 --> 00:04:03,270
pretty much terrible at front ends but i

00:04:01,260 --> 00:04:07,050
did just put a little front end on it so

00:04:03,270 --> 00:04:08,550
you can visualize it we can get all the

00:04:07,050 --> 00:04:10,980
weather stations out we can get their

00:04:08,550 --> 00:04:12,360
longitude and latitude and you can get

00:04:10,980 --> 00:04:13,950
you can actually get software for a

00:04:12,360 --> 00:04:17,160
Raspberry Pi which can collect all the

00:04:13,950 --> 00:04:19,260
data we're talking about but we want to

00:04:17,160 --> 00:04:21,630
keep things like this up to date so for

00:04:19,260 --> 00:04:24,210
an individual weather station can we

00:04:21,630 --> 00:04:27,000
keep things up to date like the daily

00:04:24,210 --> 00:04:29,100
monthly precipitation even when we're

00:04:27,000 --> 00:04:31,620
talking about things at very very large

00:04:29,100 --> 00:04:33,840
scale okay hi low temperature for the

00:04:31,620 --> 00:04:35,400
year for the month you know is it likely

00:04:33,840 --> 00:04:38,460
to rain is it likely to

00:04:35,400 --> 00:04:39,990
windy should I wear a coat you can go

00:04:38,460 --> 00:04:43,320
and play around with this Tom github

00:04:39,990 --> 00:04:44,729
like everything is these days so now i'm

00:04:43,320 --> 00:04:47,120
just going to introduce the stack which

00:04:44,729 --> 00:04:50,430
this thing which this application uses

00:04:47,120 --> 00:04:52,860
so to build something like that you

00:04:50,430 --> 00:04:54,449
first need a database and if you've got

00:04:52,860 --> 00:04:56,990
a small amount of data which fits on a

00:04:54,449 --> 00:04:59,699
single server so hundreds of gigabytes

00:04:56,990 --> 00:05:02,550
it doesn't really matter what database

00:04:59,699 --> 00:05:04,410
you pick right take anyone who cares as

00:05:02,550 --> 00:05:06,840
soon as you start building like systems

00:05:04,410 --> 00:05:09,389
at a larger scale then we need something

00:05:06,840 --> 00:05:10,919
different from a database and because

00:05:09,389 --> 00:05:12,389
I've built lots of things with Cassandra

00:05:10,919 --> 00:05:15,870
and work for datastax we're going to use

00:05:12,389 --> 00:05:19,490
Cassandra okay who here has used off

00:05:15,870 --> 00:05:22,020
because who's used Cassandra heard of it

00:05:19,490 --> 00:05:23,910
wow that's changed over the last when I

00:05:22,020 --> 00:05:25,320
start first I using Cassandra I went to

00:05:23,910 --> 00:05:27,630
a meet up in London those about six

00:05:25,320 --> 00:05:31,020
people and now it seems to be that it's

00:05:27,630 --> 00:05:32,520
it's almost becoming mainstream so if

00:05:31,020 --> 00:05:34,860
you're even if you haven't heard of it

00:05:32,520 --> 00:05:36,750
you're probably using it today there's

00:05:34,860 --> 00:05:38,580
up we'll have tens of thousands of nodes

00:05:36,750 --> 00:05:39,960
in production it's heavily used in

00:05:38,580 --> 00:05:42,090
finance because if we're going to see

00:05:39,960 --> 00:05:43,470
why it's good for time series data so

00:05:42,090 --> 00:05:45,090
but we're going to talk about this top

00:05:43,470 --> 00:05:49,800
left corner which is kind of you know

00:05:45,090 --> 00:05:52,470
storing sensor data because Sandra

00:05:49,800 --> 00:05:53,760
allows you to store things on order in

00:05:52,470 --> 00:05:55,680
disk you get to pick that as an

00:05:53,760 --> 00:05:57,539
application developer which is why

00:05:55,680 --> 00:05:59,310
people tend to use it for time series

00:05:57,539 --> 00:06:01,949
and it's why it's a good pic foot for

00:05:59,310 --> 00:06:03,900
this problem ok however when I start

00:06:01,949 --> 00:06:06,030
using Cassandra a few years ago I didn't

00:06:03,900 --> 00:06:07,889
think who I've got some order data I'm

00:06:06,030 --> 00:06:10,860
gonna look for a database which is

00:06:07,889 --> 00:06:13,169
really good at order data it was for the

00:06:10,860 --> 00:06:15,300
non-functional requirements so Cassandra

00:06:13,169 --> 00:06:18,270
can run across many many servers it can

00:06:15,300 --> 00:06:20,849
run across multi data centers and it can

00:06:18,270 --> 00:06:22,470
scale to pretty large volumes and you

00:06:20,849 --> 00:06:24,659
can always be assured that whatever your

00:06:22,470 --> 00:06:26,190
problem is someone like Apple are doing

00:06:24,659 --> 00:06:29,760
something bigger so they'll fall they'll

00:06:26,190 --> 00:06:31,800
fall into the problems before you um it

00:06:29,760 --> 00:06:34,020
comes about two different papers one

00:06:31,800 --> 00:06:35,639
came from google one came from amazon

00:06:34,020 --> 00:06:37,470
we're really only going to talk about

00:06:35,639 --> 00:06:39,449
the first one which is the dynamo paper

00:06:37,470 --> 00:06:41,789
because what that talks about is how to

00:06:39,449 --> 00:06:43,889
distribute data and that's really

00:06:41,789 --> 00:06:46,349
important if we're going to run spark on

00:06:43,889 --> 00:06:48,330
top of it because if we've got a large

00:06:46,349 --> 00:06:49,740
quantity of data we know we need to move

00:06:48,330 --> 00:06:51,900
the computation to the

00:06:49,740 --> 00:06:54,120
later not the other way around and if

00:06:51,900 --> 00:06:56,520
there's a if there's a technology which

00:06:54,120 --> 00:06:58,560
can move you know our computation and be

00:06:56,520 --> 00:07:00,690
aware of how Cassandra distributes data

00:06:58,560 --> 00:07:04,289
then we can we can do some really cool

00:07:00,690 --> 00:07:06,750
things okay dynamo actually describes a

00:07:04,289 --> 00:07:08,340
key value store cassandra is not a key

00:07:06,750 --> 00:07:10,410
value stores what we call it a wide

00:07:08,340 --> 00:07:12,270
columnstore that's the that's where a

00:07:10,410 --> 00:07:15,090
big table comes from but that's not too

00:07:12,270 --> 00:07:16,919
relevant for today what we're going to

00:07:15,090 --> 00:07:18,479
use extensively when we're running when

00:07:16,919 --> 00:07:20,520
we're trying to do analytics on top of

00:07:18,479 --> 00:07:22,979
cassandra with spark is the fact that

00:07:20,520 --> 00:07:25,319
cassandra is data central where every

00:07:22,979 --> 00:07:27,060
single node is like hey i'm in the USA

00:07:25,319 --> 00:07:30,449
data center or I'm in the Europe data

00:07:27,060 --> 00:07:32,940
center these don't have to be real data

00:07:30,449 --> 00:07:34,560
centers okay in the example that we're

00:07:32,940 --> 00:07:36,419
going to go through it's actually going

00:07:34,560 --> 00:07:38,220
to be going to lie to Cassandra we're

00:07:36,419 --> 00:07:41,280
going to use this data center where

00:07:38,220 --> 00:07:42,900
feature to do workload isolation okay so

00:07:41,280 --> 00:07:45,330
we're going to we're going to segregate

00:07:42,900 --> 00:07:47,699
a few nodes of our operational database

00:07:45,330 --> 00:07:49,500
we're not going to let our customers go

00:07:47,699 --> 00:07:51,180
to those we're not going to and we're

00:07:49,500 --> 00:07:53,430
going to run analytics directly on top

00:07:51,180 --> 00:07:55,229
of them okay so this is where you can

00:07:53,430 --> 00:07:57,719
avoid that you know take all the data

00:07:55,229 --> 00:07:59,430
shove it somewhere else like HDFS and

00:07:57,719 --> 00:08:01,560
then do the analytics there with

00:07:59,430 --> 00:08:03,330
workload isolation on Cassandra we can

00:08:01,560 --> 00:08:05,909
actually you know do it directly on top

00:08:03,330 --> 00:08:09,570
of our operational database sounds crazy

00:08:05,909 --> 00:08:12,659
doesn't it so the dynamo paper is pretty

00:08:09,570 --> 00:08:13,800
awesome who's read it yeah you should go

00:08:12,659 --> 00:08:15,419
and read it it's very it's an awesome

00:08:13,800 --> 00:08:16,740
domain because we all understand it

00:08:15,419 --> 00:08:19,139
because it's about amazon shopping

00:08:16,740 --> 00:08:20,880
baskets ok and they concluded that

00:08:19,139 --> 00:08:22,500
that's a pretty large scale service and

00:08:20,880 --> 00:08:24,180
they don't make a great deal of money

00:08:22,500 --> 00:08:27,270
when the shopping basket service goes

00:08:24,180 --> 00:08:28,469
down okay you know shock horror so they

00:08:27,270 --> 00:08:31,169
third we need to design a debate

00:08:28,469 --> 00:08:32,700
database differently and it was very

00:08:31,169 --> 00:08:33,930
much around fault tolerance we're not

00:08:32,700 --> 00:08:36,060
going to talk about fault tolerance

00:08:33,930 --> 00:08:37,829
today but the fact that we end up just

00:08:36,060 --> 00:08:40,409
two buting our data across many servers

00:08:37,829 --> 00:08:42,690
and across you know many data centers

00:08:40,409 --> 00:08:45,120
allowed us to basically you know get the

00:08:42,690 --> 00:08:48,540
the data distribution we need for doing

00:08:45,120 --> 00:08:50,010
this distributed computation the two

00:08:48,540 --> 00:08:51,810
features of that paper that are really

00:08:50,010 --> 00:08:53,790
important for this topic are the

00:08:51,810 --> 00:08:57,570
consistent hashing and how cassandra

00:08:53,790 --> 00:08:59,910
replicates data okay every every

00:08:57,570 --> 00:09:01,949
database every no sequel store in the

00:08:59,910 --> 00:09:03,329
world is jumping up and shouting please

00:09:01,949 --> 00:09:05,220
run spark on top of me

00:09:03,329 --> 00:09:07,049
okay it's it's the hot topic right

00:09:05,220 --> 00:09:08,429
there's no way people haven't heard of

00:09:07,049 --> 00:09:11,279
spark at the moment unless you're hiding

00:09:08,429 --> 00:09:13,350
under a rock what's really important is

00:09:11,279 --> 00:09:16,230
that you can get workload isolation and

00:09:13,350 --> 00:09:19,199
that you can make spark and build spark

00:09:16,230 --> 00:09:20,999
partitions that are aware of how the

00:09:19,199 --> 00:09:25,410
underlying data store you know

00:09:20,999 --> 00:09:26,970
partitions data so what is consistent

00:09:25,410 --> 00:09:29,189
hashing how does Cassandra do it why is

00:09:26,970 --> 00:09:32,279
it useful for running spark on top of it

00:09:29,189 --> 00:09:34,949
so if you do a full table scan on a

00:09:32,279 --> 00:09:37,230
single node database okay so big large

00:09:34,949 --> 00:09:40,110
relational a few million rows it's going

00:09:37,230 --> 00:09:42,239
to be slow okay if you've got a thousand

00:09:40,110 --> 00:09:44,549
no database it's going to be really slow

00:09:42,239 --> 00:09:46,379
so Cassandra kind of like removes

00:09:44,549 --> 00:09:49,019
features that you'd expect in a normal

00:09:46,379 --> 00:09:51,629
like relational database system to stop

00:09:49,019 --> 00:09:54,269
you ever having to do something like a

00:09:51,629 --> 00:09:56,549
full table scan all right and it does

00:09:54,269 --> 00:09:58,799
this just by taking part of your data

00:09:56,549 --> 00:10:01,230
hashing it and that working out which

00:09:58,799 --> 00:10:03,959
node it's on right and spark can do the

00:10:01,230 --> 00:10:06,809
same so if we have some data here like

00:10:03,959 --> 00:10:09,089
Jim and Carol and Johnny and Susie what

00:10:06,809 --> 00:10:11,759
we do or what Cassandra does is it

00:10:09,089 --> 00:10:13,919
hashes that part the key and then you

00:10:11,759 --> 00:10:15,419
know that goes to a value now the real

00:10:13,919 --> 00:10:17,220
value is very big and I can't say the

00:10:15,419 --> 00:10:19,079
numbers so we're going to pretend it's

00:10:17,220 --> 00:10:21,600
between zero and a thousand so we'll

00:10:19,079 --> 00:10:24,839
pretend that Jim goes to 350 Carol goes

00:10:21,600 --> 00:10:26,939
to 998 the second piece is that each

00:10:24,839 --> 00:10:29,399
node in a Cassandra cluster owns a range

00:10:26,939 --> 00:10:31,350
of this this dish hush range so zero to

00:10:29,399 --> 00:10:35,579
a thousand we're going to say that node

00:10:31,350 --> 00:10:39,600
a owns you know 02 249 node B owns 250

00:10:35,579 --> 00:10:41,699
to 499 etc okay this means that drivers

00:10:39,600 --> 00:10:43,499
and other nodes in the Cassandra cluster

00:10:41,699 --> 00:10:46,079
can always work where your work out

00:10:43,499 --> 00:10:48,209
where your data is okay so not only do

00:10:46,079 --> 00:10:50,399
Cassandra nodes do this but drivers do

00:10:48,209 --> 00:10:52,230
so the driver says oh I'm going to store

00:10:50,399 --> 00:10:57,029
Jim I'm going to go directly to note a

00:10:52,230 --> 00:10:58,739
okay this is why you often see the funky

00:10:57,029 --> 00:11:00,360
picture whenever you look at a dynamo

00:10:58,739 --> 00:11:02,100
based system you often see your ring

00:11:00,360 --> 00:11:04,259
okay and you often see people talking

00:11:02,100 --> 00:11:06,600
about hush rings and things and it's

00:11:04,259 --> 00:11:09,209
simply because you can put the nodes in

00:11:06,600 --> 00:11:12,149
a nice ring and say you know know a owns

00:11:09,209 --> 00:11:13,919
this section no be owns this section the

00:11:12,149 --> 00:11:15,869
reality is in Cassandra that it actually

00:11:13,919 --> 00:11:17,670
split up vastly more into lots of little

00:11:15,869 --> 00:11:19,290
sections and that's so we can

00:11:17,670 --> 00:11:23,940
easily scale out and scale in without

00:11:19,290 --> 00:11:26,610
having to it to rebalance that's so this

00:11:23,940 --> 00:11:27,960
this basically result in a table which

00:11:26,610 --> 00:11:29,790
can easily be worked out it's a

00:11:27,960 --> 00:11:31,800
deterministic algorithm for working out

00:11:29,790 --> 00:11:33,750
where data is and this is exactly how

00:11:31,800 --> 00:11:36,870
the spark connector is going to build

00:11:33,750 --> 00:11:40,680
putts park partitions too much Cassandra

00:11:36,870 --> 00:11:42,480
partitions but that's not it it would be

00:11:40,680 --> 00:11:44,160
a pretty terrible database if it told

00:11:42,480 --> 00:11:47,090
you to separate your data across many

00:11:44,160 --> 00:11:50,880
nodes and then it didn't replicate

00:11:47,090 --> 00:11:52,380
because poor old Johnny if no day went

00:11:50,880 --> 00:11:54,690
down would vanish and we'd cease to ever

00:11:52,380 --> 00:11:57,150
have Johnny again so the next thing that

00:11:54,690 --> 00:12:00,000
we do in Cassandra is we replicate it

00:11:57,150 --> 00:12:02,280
and replication has to be clever if you

00:12:00,000 --> 00:12:05,160
want to build an AP system from say

00:12:02,280 --> 00:12:08,280
savior of your cup literate you need to

00:12:05,160 --> 00:12:09,780
be topology aware ok there's no point

00:12:08,280 --> 00:12:12,210
putting three replicas all on the same

00:12:09,780 --> 00:12:13,620
rack inside your data center ok then if

00:12:12,210 --> 00:12:16,050
you have a network partition between the

00:12:13,620 --> 00:12:19,350
rocks then you know it's all hidden over

00:12:16,050 --> 00:12:21,420
here so what Cassandra does is because

00:12:19,350 --> 00:12:24,630
it's its data center where it's also

00:12:21,420 --> 00:12:26,700
Rock aware and if you ask for say a

00:12:24,630 --> 00:12:29,760
replication factor of three inside the

00:12:26,700 --> 00:12:31,950
data center what it will do is it will

00:12:29,760 --> 00:12:34,230
put you a copy of your data on each rack

00:12:31,950 --> 00:12:35,820
so that you can handle Network

00:12:34,230 --> 00:12:37,320
partitions between the rack so you can

00:12:35,820 --> 00:12:41,910
handle four trucks going down without

00:12:37,320 --> 00:12:44,160
losing data ok this is where the real

00:12:41,910 --> 00:12:46,110
animation has gone away because I'm no

00:12:44,160 --> 00:12:47,460
longer in keynote and I'm on a PDF so

00:12:46,110 --> 00:12:48,780
I'll just have to tell you the order at

00:12:47,460 --> 00:12:50,640
which these lines would have come in

00:12:48,780 --> 00:12:52,440
because this is kind of explaining the

00:12:50,640 --> 00:12:55,590
whole system of how a right works in

00:12:52,440 --> 00:12:59,250
Cassandra so we start off with the line

00:12:55,590 --> 00:13:00,930
up at the top here and this is the you

00:12:59,250 --> 00:13:05,040
can just about see that so this line

00:13:00,930 --> 00:13:08,190
here ok a client decides to do a write

00:13:05,040 --> 00:13:10,530
for a particular key or a read it goes

00:13:08,190 --> 00:13:12,930
directly to a node in a data into its

00:13:10,530 --> 00:13:15,630
local data center which owns that data

00:13:12,930 --> 00:13:17,760
ok so it knows that it needs to store it

00:13:15,630 --> 00:13:20,250
here and it know that if it was reading

00:13:17,760 --> 00:13:21,570
it would read it from there when you do

00:13:20,250 --> 00:13:23,790
a writing Cassandra you pick a

00:13:21,570 --> 00:13:25,590
consistency level we'll talk about

00:13:23,790 --> 00:13:27,210
consistency levels brief briefly later

00:13:25,590 --> 00:13:29,850
but essentially if we do it a

00:13:27,210 --> 00:13:31,500
consistency level one then the cut this

00:13:29,850 --> 00:13:33,510
guy here the first note that we

00:13:31,500 --> 00:13:36,330
up to can return right away and say yep

00:13:33,510 --> 00:13:38,340
I've got your data what Cassandra then

00:13:36,330 --> 00:13:40,140
does is this coordinator as we call it

00:13:38,340 --> 00:13:42,690
just it's a coordinated just for this

00:13:40,140 --> 00:13:45,480
request what it does is it replicates

00:13:42,690 --> 00:13:47,010
the data out to the other two replicas

00:13:45,480 --> 00:13:48,900
and it can always work out what these

00:13:47,010 --> 00:13:50,730
are two deterministic algorithm for

00:13:48,900 --> 00:13:54,540
looking for particular nodes on

00:13:50,730 --> 00:13:56,250
different racks at the same time I'm the

00:13:54,540 --> 00:13:58,080
coordinator selects what I call a remote

00:13:56,250 --> 00:14:00,060
coordinator in a different data center

00:13:58,080 --> 00:14:01,890
okay so here we've got two data centers

00:14:00,060 --> 00:14:05,400
and we want to replicate our data three

00:14:01,890 --> 00:14:07,500
times in each we only copy it over once

00:14:05,400 --> 00:14:09,510
because we're exhuming that the the

00:14:07,500 --> 00:14:11,790
network link between data centers is

00:14:09,510 --> 00:14:14,610
slow or expensive etc I mean is more

00:14:11,790 --> 00:14:16,860
likely to go down its then the remote

00:14:14,610 --> 00:14:19,020
coordinators responsibility to replicate

00:14:16,860 --> 00:14:21,330
it to two other nodes in the other data

00:14:19,020 --> 00:14:24,750
center okay and at that point you pretty

00:14:21,330 --> 00:14:27,000
much you have replication the tunable

00:14:24,750 --> 00:14:29,550
consistency comes about about when you

00:14:27,000 --> 00:14:31,680
want to say that request is complete so

00:14:29,550 --> 00:14:33,600
if you want to be really super if you

00:14:31,680 --> 00:14:36,660
want to be really quick then you say one

00:14:33,600 --> 00:14:38,550
and it returns right away if you want to

00:14:36,660 --> 00:14:41,010
wait until more replicas have the data

00:14:38,550 --> 00:14:43,470
you can say say all right that would be

00:14:41,010 --> 00:14:44,670
all six nodes this is a terrible idea

00:14:43,470 --> 00:14:46,680
and you should never do it because

00:14:44,670 --> 00:14:48,450
you're losing all of the availability

00:14:46,680 --> 00:14:50,280
suddenly right as soon as a single node

00:14:48,450 --> 00:14:52,980
goes down queries queries are going to

00:14:50,280 --> 00:14:54,900
start failing more likely you're going

00:14:52,980 --> 00:14:57,120
to start using things like quorums which

00:14:54,900 --> 00:15:00,060
is a majority so in this previous

00:14:57,120 --> 00:15:03,839
example we have six replicas a quorum

00:15:00,060 --> 00:15:05,550
would be four okay to majority you often

00:15:03,839 --> 00:15:08,580
don't want to do that though because if

00:15:05,550 --> 00:15:10,260
we were going to do a get it 24 in the

00:15:08,580 --> 00:15:12,000
previous example we would have to go

00:15:10,260 --> 00:15:14,310
across the one before completing that

00:15:12,000 --> 00:15:16,860
request that could be slow so you have

00:15:14,310 --> 00:15:18,720
these local local quorums or local one

00:15:16,860 --> 00:15:20,490
well that basically says it's in the

00:15:18,720 --> 00:15:22,680
local data center get it to a majority

00:15:20,490 --> 00:15:24,690
there and that means that in this

00:15:22,680 --> 00:15:26,760
previous example we get it to two out of

00:15:24,690 --> 00:15:28,950
the three nodes in the local data center

00:15:26,760 --> 00:15:31,890
and let the rest happen asynchronously

00:15:28,950 --> 00:15:34,500
when we're doing when sparks doing

00:15:31,890 --> 00:15:36,839
writing writing to Cassandra it's always

00:15:34,500 --> 00:15:39,300
doing things locally okay because what

00:15:36,839 --> 00:15:41,490
you don't want is for your spark jobs to

00:15:39,300 --> 00:15:42,630
suddenly start hammering your remote

00:15:41,490 --> 00:15:44,010
data center which would be your

00:15:42,630 --> 00:15:45,120
operational the one that say your

00:15:44,010 --> 00:15:47,400
customers are facing

00:15:45,120 --> 00:15:49,260
so by default you can override it the

00:15:47,400 --> 00:15:50,880
spark connected as everything at local

00:15:49,260 --> 00:15:52,980
one which basically means i want

00:15:50,880 --> 00:15:56,790
acknowledgments from one mode in my

00:15:52,980 --> 00:15:58,620
local data center at that point I

00:15:56,790 --> 00:16:00,810
normally like wave my hands and say

00:15:58,620 --> 00:16:02,610
Cassandra doesn't have any masters okay

00:16:00,810 --> 00:16:04,800
Cassandra is a there's no single point

00:16:02,610 --> 00:16:06,540
of failure because my first I'd learning

00:16:04,800 --> 00:16:07,830
about Cassandra I started like seeing is

00:16:06,540 --> 00:16:11,730
that a single point of failure is that a

00:16:07,830 --> 00:16:14,850
master etc so that coordinator in the in

00:16:11,730 --> 00:16:17,310
the previous example it is not special

00:16:14,850 --> 00:16:20,430
okay if that coordinator were to go down

00:16:17,310 --> 00:16:22,110
then the driver has a load-balancing

00:16:20,430 --> 00:16:25,170
policy and it would just use a different

00:16:22,110 --> 00:16:26,640
coordinator the other node which

00:16:25,170 --> 00:16:28,380
sometimes people think is that one

00:16:26,640 --> 00:16:31,470
special does that one need to be up is

00:16:28,380 --> 00:16:33,029
the node which your data hashes to okay

00:16:31,470 --> 00:16:35,339
rather than the other two selected

00:16:33,029 --> 00:16:37,680
replicas and the answer is no that one's

00:16:35,339 --> 00:16:39,330
down it's a deterministic algorithm for

00:16:37,680 --> 00:16:45,330
working out the other two replicas and

00:16:39,330 --> 00:16:47,430
clients can just use those two so that's

00:16:45,330 --> 00:16:49,500
my quickest fire introduction to

00:16:47,430 --> 00:16:51,180
Cassandra to make so we make sure that

00:16:49,500 --> 00:16:53,130
people understand how the spark

00:16:51,180 --> 00:16:55,290
connectors going to work does anyone

00:16:53,130 --> 00:16:56,880
have any questions I realize that we've

00:16:55,290 --> 00:16:58,500
only spent 10 minutes or so but does

00:16:56,880 --> 00:17:00,300
anyone have any questions on Cassandra

00:16:58,500 --> 00:17:08,569
before we before we jump into the spark

00:17:00,300 --> 00:17:12,240
lund yeah what if all de nodes which

00:17:08,569 --> 00:17:13,740
handles one single hash are down so

00:17:12,240 --> 00:17:15,209
you've got two options so the question

00:17:13,740 --> 00:17:17,640
was basically yeah what if what if

00:17:15,209 --> 00:17:19,319
selectively one replica in each of my

00:17:17,640 --> 00:17:21,059
racks was down and like I can't right

00:17:19,319 --> 00:17:22,500
you are you have two options and it's

00:17:21,059 --> 00:17:25,199
you as an application developer that's

00:17:22,500 --> 00:17:27,679
got to decide you either fail or there's

00:17:25,199 --> 00:17:30,780
a special consistency level called any

00:17:27,679 --> 00:17:33,360
what any is is it gives it to another

00:17:30,780 --> 00:17:37,230
node to look after until those nodes

00:17:33,360 --> 00:17:39,330
come back up okay and it keeps it for a

00:17:37,230 --> 00:17:42,300
configured amount of time by default I

00:17:39,330 --> 00:17:44,480
believe it's three hours so it's like

00:17:42,300 --> 00:17:47,700
you change the hashing power to

00:17:44,480 --> 00:17:49,290
temporary right the hashing remains the

00:17:47,700 --> 00:17:51,090
same it's still those are the three

00:17:49,290 --> 00:17:53,820
replicas but what you're telling the

00:17:51,090 --> 00:17:55,440
driver is give it to another node and it

00:17:53,820 --> 00:17:57,480
that node will keep it in what's called

00:17:55,440 --> 00:17:58,680
as hinted hand off just for a short

00:17:57,480 --> 00:17:59,970
amount of time

00:17:58,680 --> 00:18:03,210
you pick the time the reason it's not

00:17:59,970 --> 00:18:05,280
unlimited is because let's say this is

00:18:03,210 --> 00:18:07,170
happening right it's bad times you've

00:18:05,280 --> 00:18:10,710
got a third of your data center down or

00:18:07,170 --> 00:18:12,750
however many if you do this for too long

00:18:10,710 --> 00:18:14,520
what you end up doing is giving the

00:18:12,750 --> 00:18:17,130
existing node so you're half of your

00:18:14,520 --> 00:18:19,140
data center you're giving them more work

00:18:17,130 --> 00:18:20,760
so you're almost overloading the node

00:18:19,140 --> 00:18:22,500
when there's half the number of nodes

00:18:20,760 --> 00:18:23,850
available so you can you can turn this

00:18:22,500 --> 00:18:27,240
off and you can configure the amount of

00:18:23,850 --> 00:18:29,490
time did it happens for any more

00:18:27,240 --> 00:18:35,400
Cassandra questions before we get into

00:18:29,490 --> 00:18:38,010
spark nope let's move on to to move on

00:18:35,400 --> 00:18:39,810
to spark then so given the keynote and

00:18:38,010 --> 00:18:41,130
the fact that no one put their hand up

00:18:39,810 --> 00:18:43,290
that could have been because she asked

00:18:41,130 --> 00:18:46,140
who doesn't know spark who does no spark

00:18:43,290 --> 00:18:48,690
will do the positive question instead so

00:18:46,140 --> 00:18:50,190
most people have heard of it well I will

00:18:48,690 --> 00:18:51,510
do will spend a few minutes on it and

00:18:50,190 --> 00:18:53,430
then we'll go on to the integration

00:18:51,510 --> 00:18:57,750
between between spark and Cassandra will

00:18:53,430 --> 00:18:59,820
start looking at spark streaming so if

00:18:57,750 --> 00:19:01,770
we were to store our data in cassandra

00:18:59,820 --> 00:19:03,780
cassandra is very good at storing vast

00:19:01,770 --> 00:19:06,480
quantities of immutable kind of like

00:19:03,780 --> 00:19:08,940
time series data if we were going to

00:19:06,480 --> 00:19:10,710
store data for say all the weather

00:19:08,940 --> 00:19:12,690
updates of all the weather stations in

00:19:10,710 --> 00:19:16,380
the world it might look a bit like this

00:19:12,690 --> 00:19:18,630
okay weather station ID I'm sometime so

00:19:16,380 --> 00:19:20,220
this is where your month day hour so

00:19:18,630 --> 00:19:22,500
we're assuming one event per weather

00:19:20,220 --> 00:19:25,250
station per hour and then some cool

00:19:22,500 --> 00:19:28,860
stuff pressure precipitation arm

00:19:25,250 --> 00:19:31,050
temperature alright this gets very big

00:19:28,860 --> 00:19:33,180
very quickly how do we actually how to

00:19:31,050 --> 00:19:35,520
actually do some more like like

00:19:33,180 --> 00:19:37,410
aggregate queries on this okay how do we

00:19:35,520 --> 00:19:38,460
how do we work out the daily how do we

00:19:37,410 --> 00:19:41,040
work out the daily I can get

00:19:38,460 --> 00:19:42,600
precipitation right Cassandra doesn't

00:19:41,040 --> 00:19:44,100
support a lot of the features that you

00:19:42,600 --> 00:19:47,160
might expect of a relational database

00:19:44,100 --> 00:19:49,650
because they typically don't scale okay

00:19:47,160 --> 00:19:52,380
so the next section is going to be how

00:19:49,650 --> 00:19:56,850
do we use spark to turn the data into

00:19:52,380 --> 00:19:58,950
something more useful and it's at that

00:19:56,850 --> 00:20:00,960
point where I get into my defense of

00:19:58,950 --> 00:20:04,440
Cassandra like why doesn't it support

00:20:00,960 --> 00:20:06,620
this stuff okay as I said if you if

00:20:04,440 --> 00:20:09,540
you're your problem fits on a computer

00:20:06,620 --> 00:20:11,550
don't use a distributed database like

00:20:09,540 --> 00:20:12,240
Cassandra use a relational database okay

00:20:11,550 --> 00:20:15,090
use something

00:20:12,240 --> 00:20:17,670
with lots of fancy secondary indexing if

00:20:15,090 --> 00:20:19,200
you've got this if you've got a scale

00:20:17,670 --> 00:20:22,410
problem and you know you want it you

00:20:19,200 --> 00:20:24,330
want to run across many servers and if a

00:20:22,410 --> 00:20:26,309
technology offers you behavior like

00:20:24,330 --> 00:20:28,530
secondary indexing and aggregates and

00:20:26,309 --> 00:20:31,170
all these types of things think to

00:20:28,530 --> 00:20:33,300
yourself how they scale on a 100 nodes

00:20:31,170 --> 00:20:35,640
how do you think a secondary index works

00:20:33,300 --> 00:20:38,220
on a hundred node cluster okay wherever

00:20:35,640 --> 00:20:40,890
that secondary index might be it has to

00:20:38,220 --> 00:20:43,050
go to that that's going to link to many

00:20:40,890 --> 00:20:45,450
hundreds of rows in your database across

00:20:43,050 --> 00:20:47,610
many hundreds of computers it's going to

00:20:45,450 --> 00:20:49,020
be slow if you take Cassandra actually

00:20:47,610 --> 00:20:51,240
does have secondary indexing I just tell

00:20:49,020 --> 00:20:52,950
people not to use them because it works

00:20:51,240 --> 00:20:54,780
on a three node cluster it might work on

00:20:52,950 --> 00:20:56,250
a six node cluster it doesn't work on a

00:20:54,780 --> 00:20:58,050
hundred note or a thousand node cluster

00:20:56,250 --> 00:21:00,630
and that's simply because you end up

00:20:58,050 --> 00:21:03,809
having to go to too many computers to

00:21:00,630 --> 00:21:05,550
answer your query so if you denormalize

00:21:03,809 --> 00:21:07,830
and you duplicate your data inside

00:21:05,550 --> 00:21:09,900
cassandra you get it you get the

00:21:07,830 --> 00:21:11,610
advantages of the fact that you get to

00:21:09,900 --> 00:21:13,530
hash it you get to go directly to a

00:21:11,610 --> 00:21:15,360
single node this works if it's three

00:21:13,530 --> 00:21:17,280
nodes or a thousand nodes and then you

00:21:15,360 --> 00:21:19,559
get to do as few seeks on disk as

00:21:17,280 --> 00:21:21,480
possible so it's irrelevant whether

00:21:19,559 --> 00:21:24,330
you've got you know a petabyte or a

00:21:21,480 --> 00:21:26,520
gigabyte the actual satisfying your

00:21:24,330 --> 00:21:28,170
query is the same it goes to a single

00:21:26,520 --> 00:21:29,610
node it does a couple of disk seeks it

00:21:28,170 --> 00:21:31,770
brings your data back it's called what I

00:21:29,610 --> 00:21:35,120
call this a scale of be performant query

00:21:31,770 --> 00:21:37,290
if that's if that's a real thing however

00:21:35,120 --> 00:21:39,720
when we want to do things like

00:21:37,290 --> 00:21:42,570
aggregates and analysis and things we

00:21:39,720 --> 00:21:45,000
don't really care about a 10 millisecond

00:21:42,570 --> 00:21:46,590
response time we're may be happy to you

00:21:45,000 --> 00:21:49,200
know to actually you know it take you

00:21:46,590 --> 00:21:51,860
know seconds or maybe minutes or hours

00:21:49,200 --> 00:21:54,179
or days if you're if you're using Hadoop

00:21:51,860 --> 00:21:56,070
also we want to come up with a solution

00:21:54,179 --> 00:21:58,140
to keep these things up to date we want

00:21:56,070 --> 00:22:00,090
to maybe do stream processing so we're

00:21:58,140 --> 00:22:02,880
not doing a query what is the daily

00:22:00,090 --> 00:22:05,190
aggregate you know precipitation we just

00:22:02,880 --> 00:22:07,080
constantly kept it up to date okay

00:22:05,190 --> 00:22:08,640
that's where spark on top of Cassandra

00:22:07,080 --> 00:22:12,150
comes in this is one of my favorite

00:22:08,640 --> 00:22:13,620
cartoons so we've all kind of heard of

00:22:12,150 --> 00:22:15,210
it most people put their hands up but

00:22:13,620 --> 00:22:17,490
it's pretty much you can think of it as

00:22:15,210 --> 00:22:19,710
a replacement for Hadoop MapReduce right

00:22:17,490 --> 00:22:21,840
it's a distributed computation engine

00:22:19,710 --> 00:22:24,929
with some really cool fancy bits which

00:22:21,840 --> 00:22:26,070
make it slightly nicer my favorite part

00:22:24,929 --> 00:22:28,530
of spark is essential

00:22:26,070 --> 00:22:31,200
you get the same API for stream

00:22:28,530 --> 00:22:33,060
analytics micro batch analytics and that

00:22:31,200 --> 00:22:35,040
you do for you know proper batch

00:22:33,060 --> 00:22:38,880
analytics okay that makes my life as a

00:22:35,040 --> 00:22:41,490
developer vastly easier okay and it's

00:22:38,880 --> 00:22:43,830
obviously hit the world by storm no pun

00:22:41,490 --> 00:22:45,960
intended because it's already in all of

00:22:43,830 --> 00:22:47,580
the Hadoop distributions but it's not

00:22:45,960 --> 00:22:49,380
just making its way into Hadoop

00:22:47,580 --> 00:22:51,450
distributions it made its way into

00:22:49,380 --> 00:22:54,060
datastax this product which is very much

00:22:51,450 --> 00:22:55,860
an operational database okay so it's

00:22:54,060 --> 00:22:57,840
useful on for both the kind of business

00:22:55,860 --> 00:23:00,180
intelligence the data warehousing and

00:22:57,840 --> 00:23:02,730
it's also useful for augmenting your

00:23:00,180 --> 00:23:04,820
kind of operational system okay that's

00:23:02,730 --> 00:23:06,660
what we're going to talk about today

00:23:04,820 --> 00:23:09,120
we're going to talk about a couple of

00:23:06,660 --> 00:23:11,190
the components so obviously spark is not

00:23:09,120 --> 00:23:14,460
a database right it needs somewhere to

00:23:11,190 --> 00:23:15,360
get its data that's typically HDFS today

00:23:14,460 --> 00:23:18,090
we're going to be talking about

00:23:15,360 --> 00:23:20,160
Cassandra all right the general compute

00:23:18,090 --> 00:23:22,170
engine which is the scholar API or

00:23:20,160 --> 00:23:24,750
there's Python and Java as well we saw

00:23:22,170 --> 00:23:25,860
some of the Java compute engine in the

00:23:24,750 --> 00:23:27,630
in the keynote we're going to look at

00:23:25,860 --> 00:23:30,390
going to look a scholar today okay

00:23:27,630 --> 00:23:31,710
that's a nice Skylar API it makes you

00:23:30,390 --> 00:23:34,230
look like you're just dealing with a

00:23:31,710 --> 00:23:35,490
collection which is very nice okay we're

00:23:34,230 --> 00:23:39,750
also going to look at streaming and

00:23:35,490 --> 00:23:41,490
maybe some SQL if we've got time so our

00:23:39,750 --> 00:23:43,290
DD you've already seen an RDD you've

00:23:41,490 --> 00:23:44,580
seen like three of them already today so

00:23:43,290 --> 00:23:46,830
I don't need to tell you what on our DD

00:23:44,580 --> 00:23:48,960
is okay it's just an abstraction across

00:23:46,830 --> 00:23:50,700
a vast quantity of data think of it as a

00:23:48,960 --> 00:23:52,440
list it just happens to be an entire

00:23:50,700 --> 00:23:55,290
cassandra table with a petabyte of data

00:23:52,440 --> 00:23:58,380
in it or a very large file on some on a

00:23:55,290 --> 00:24:00,360
distributed file system okay you get to

00:23:58,380 --> 00:24:02,820
do if you're used to any of say the

00:24:00,360 --> 00:24:04,200
scholar API sort of collection its kind

00:24:02,820 --> 00:24:06,120
of immutable collections in general

00:24:04,200 --> 00:24:08,060
you're going to get things very familiar

00:24:06,120 --> 00:24:10,110
like filter and map and flat map etc

00:24:08,060 --> 00:24:12,270
okay we're going to see quite a few of

00:24:10,110 --> 00:24:14,100
those today what spark does when you do

00:24:12,270 --> 00:24:16,380
all of these things is precisely nothing

00:24:14,100 --> 00:24:17,970
okay you do all these things ignores you

00:24:16,380 --> 00:24:20,100
until you do an action something which

00:24:17,970 --> 00:24:22,800
has a side effect and the side effects

00:24:20,100 --> 00:24:23,850
could be writing it to a file but today

00:24:22,800 --> 00:24:25,320
it's going to be writing it to a

00:24:23,850 --> 00:24:27,990
cassandra table so we're going to see

00:24:25,320 --> 00:24:31,640
data coming in to Cassandra from Spike

00:24:27,990 --> 00:24:33,510
streaming but also out of Cassandra

00:24:31,640 --> 00:24:35,280
everyone has to have a word count

00:24:33,510 --> 00:24:37,620
example I'm amazed at the Kino doesn't

00:24:35,280 --> 00:24:39,539
have a boil count example stolen

00:24:37,620 --> 00:24:41,820
directly from the spark docks so this

00:24:39,539 --> 00:24:43,619
not the most exciting spark example in

00:24:41,820 --> 00:24:45,659
the world but it gives you an idea if

00:24:43,619 --> 00:24:48,539
you haven't seen sparco before of

00:24:45,659 --> 00:24:50,460
exactly what we mean by an RDD right you

00:24:48,539 --> 00:24:52,019
take a file you turn it into an idd

00:24:50,460 --> 00:24:54,720
we're going to see the same but

00:24:52,019 --> 00:24:57,090
Cassandra table very shortly you can do

00:24:54,720 --> 00:24:59,970
operations like split you know split

00:24:57,090 --> 00:25:01,619
each line you know by white space map it

00:24:59,970 --> 00:25:03,989
to a tuple with the word and then one

00:25:01,619 --> 00:25:06,210
and then your typical reduce right and

00:25:03,989 --> 00:25:07,769
if you've seen must produce hadoop

00:25:06,210 --> 00:25:09,570
mapreduce of the equivalent program

00:25:07,769 --> 00:25:12,840
you'll realize this is pretty quite nice

00:25:09,570 --> 00:25:16,009
there's also a ripple right I spent most

00:25:12,840 --> 00:25:18,809
of yesterday on a rebel working on

00:25:16,009 --> 00:25:20,159
gigabytes of data inside Cassandra and I

00:25:18,809 --> 00:25:21,989
was just hacking away doing ad-hoc

00:25:20,159 --> 00:25:24,720
queries things I wouldn't normally be

00:25:21,989 --> 00:25:26,309
able to do with pure Cassandra because

00:25:24,720 --> 00:25:30,479
it kind of restricts you to do only the

00:25:26,309 --> 00:25:32,460
performant queries that example was what

00:25:30,479 --> 00:25:35,220
we call a batch job right you're dealing

00:25:32,460 --> 00:25:36,690
with a huge file right we could we're

00:25:35,220 --> 00:25:38,639
going to show you how to go across that

00:25:36,690 --> 00:25:40,499
entire weather data and do a bad job on

00:25:38,639 --> 00:25:42,419
it that might take minutes it's going to

00:25:40,499 --> 00:25:44,399
take proportional to how much data you

00:25:42,419 --> 00:25:46,799
have that might be you know quick for

00:25:44,399 --> 00:25:49,440
100 gigabytes but it's going to slow

00:25:46,799 --> 00:25:50,759
down for a spark streaming comes in so

00:25:49,440 --> 00:25:52,799
we get to actually keep things up to

00:25:50,759 --> 00:25:54,629
date you know in the near real time

00:25:52,799 --> 00:25:55,859
because we can't say real time because

00:25:54,629 --> 00:26:00,119
that word means that something very

00:25:55,859 --> 00:26:01,440
strongly to set certain people it's not

00:26:00,119 --> 00:26:02,909
the full functionality of you're

00:26:01,440 --> 00:26:04,590
familiar with something like storm it's

00:26:02,909 --> 00:26:06,299
not going to process things as they come

00:26:04,590 --> 00:26:08,580
on the queue it does what's called micro

00:26:06,299 --> 00:26:10,229
batches so we're going to see a for our

00:26:08,580 --> 00:26:12,539
example with them weather data will show

00:26:10,229 --> 00:26:14,399
you a 500 milliseconds microbe Attridge

00:26:12,539 --> 00:26:17,159
as small as it gets so if you need

00:26:14,399 --> 00:26:19,229
results to be updated more often than

00:26:17,159 --> 00:26:21,809
every 500 milliseconds look elsewhere

00:26:19,229 --> 00:26:23,489
okay this is quick but not the

00:26:21,809 --> 00:26:25,109
single-digit milliseconds the kind of

00:26:23,489 --> 00:26:29,369
response times we'd want directly from

00:26:25,109 --> 00:26:31,109
from a cassandra database so let's get

00:26:29,369 --> 00:26:33,989
on to some example code okay we've got a

00:26:31,109 --> 00:26:36,239
database we've got a stream processing

00:26:33,989 --> 00:26:37,889
platform and batch processing platform

00:26:36,239 --> 00:26:42,749
inspired let's start looking at some

00:26:37,889 --> 00:26:44,190
code so a note on deployment first so if

00:26:42,749 --> 00:26:46,649
you in deploy a system like this in

00:26:44,190 --> 00:26:48,659
production what you're actually going to

00:26:46,649 --> 00:26:51,059
do is co-locate spark and cassandra on

00:26:48,659 --> 00:26:52,889
the same servers right if you're if

00:26:51,059 --> 00:26:53,430
you're an open-source junkie you have to

00:26:52,889 --> 00:26:55,170
do this yours

00:26:53,430 --> 00:26:57,510
self if you're using datastax enterprise

00:26:55,170 --> 00:26:59,190
you get this bye-bye free you just click

00:26:57,510 --> 00:27:01,260
I want a spark work or inside that

00:26:59,190 --> 00:27:04,650
Cassandra node and you get it okay

00:27:01,260 --> 00:27:07,200
little shameless plug ler the reason for

00:27:04,650 --> 00:27:08,940
this is the spark connector now if your

00:27:07,200 --> 00:27:10,890
spark literate you know that spark has

00:27:08,940 --> 00:27:12,420
thing called partition and a partition

00:27:10,890 --> 00:27:15,390
is the level at which concurrency and

00:27:12,420 --> 00:27:17,880
tasks are executed on what the cassandra

00:27:15,390 --> 00:27:21,630
spark connector does is build spark

00:27:17,880 --> 00:27:23,970
partitions of cassandra partitions same

00:27:21,630 --> 00:27:27,270
terminology gets confusing that live on

00:27:23,970 --> 00:27:29,700
the same node so the connector is never

00:27:27,270 --> 00:27:32,070
going to build a spark partition which

00:27:29,700 --> 00:27:35,100
is made up of cassandra partitions that

00:27:32,070 --> 00:27:36,750
live on two different nodes why so we

00:27:35,100 --> 00:27:39,120
can send the computation to the worker

00:27:36,750 --> 00:27:41,220
that's local to that node right that's

00:27:39,120 --> 00:27:45,330
the important thing if the driver did

00:27:41,220 --> 00:27:46,800
not do that it would be useless I back

00:27:45,330 --> 00:27:48,750
to the example so we'll do batch

00:27:46,800 --> 00:27:50,490
processing first so we're going to

00:27:48,750 --> 00:27:53,100
assume that the data is ingested into

00:27:50,490 --> 00:27:54,930
Cassandra by some other means and we're

00:27:53,100 --> 00:27:56,880
going to have a look at maybe what kind

00:27:54,930 --> 00:27:58,290
of batch jobs we could run on top okay

00:27:56,880 --> 00:28:01,590
remember the data look like it was a big

00:27:58,290 --> 00:28:03,060
wall of numbers this is the table so

00:28:01,590 --> 00:28:05,610
this is the schema for the table i

00:28:03,060 --> 00:28:07,320
showed you before um when we introduced

00:28:05,610 --> 00:28:09,240
Cassandra i didn't actually talk about

00:28:07,320 --> 00:28:10,440
exactly how you model data in Cassandra

00:28:09,240 --> 00:28:14,430
cuz i thought we might as well wait to

00:28:10,440 --> 00:28:16,290
the example so the only hard thing you

00:28:14,430 --> 00:28:18,180
have to worry about when you do things

00:28:16,290 --> 00:28:21,090
with cassandra is selecting the primary

00:28:18,180 --> 00:28:23,730
key alright so this looks a bit like an

00:28:21,090 --> 00:28:25,710
SQL table we've got things like wind

00:28:23,730 --> 00:28:27,510
direction and speed and temperature and

00:28:25,710 --> 00:28:29,640
all these types of things the really

00:28:27,510 --> 00:28:30,960
important thing is the primary key well

00:28:29,640 --> 00:28:33,720
then I spend a couple of minutes on this

00:28:30,960 --> 00:28:35,460
the primary key is exactly how you tell

00:28:33,720 --> 00:28:38,610
Cassandra how to distribute your data

00:28:35,460 --> 00:28:40,710
and how to order it on disk now because

00:28:38,610 --> 00:28:42,900
you're picking how data is ordered on

00:28:40,710 --> 00:28:45,150
disk in Cassandra you obviously can't

00:28:42,900 --> 00:28:46,380
query it in different orders which is

00:28:45,150 --> 00:28:51,390
very important that's why you end up

00:28:46,380 --> 00:28:53,370
duplicating so primary key looks simple

00:28:51,390 --> 00:28:55,650
enough the primary key is made up of two

00:28:53,370 --> 00:28:57,060
parts and I've already kind of explained

00:28:55,650 --> 00:28:59,820
this a little bit but we're going to see

00:28:57,060 --> 00:29:02,100
a concrete example here the primary key

00:28:59,820 --> 00:29:04,020
is made up of the first bit that's the

00:29:02,100 --> 00:29:06,270
part which is called the partition key

00:29:04,020 --> 00:29:07,230
which is hushed everything with the same

00:29:06,270 --> 00:29:08,970
partition ki will

00:29:07,230 --> 00:29:11,040
end up on the same node in your

00:29:08,970 --> 00:29:12,780
Cassandra cluster so anything any

00:29:11,040 --> 00:29:16,200
queries in the same partition are going

00:29:12,780 --> 00:29:17,490
to be quick even without spark right the

00:29:16,200 --> 00:29:19,290
next thing you pick is all the other

00:29:17,490 --> 00:29:21,510
fields which you want to be part of your

00:29:19,290 --> 00:29:23,730
your primary key they actually dictate

00:29:21,510 --> 00:29:26,100
your order on disk right so if you do a

00:29:23,730 --> 00:29:31,140
range query which is for like a time

00:29:26,100 --> 00:29:33,210
period it's going to be really quick so

00:29:31,140 --> 00:29:36,059
if we actually look if we this this

00:29:33,210 --> 00:29:38,730
primary key here ends up looking on disk

00:29:36,059 --> 00:29:41,220
a bit like this so we have a partition

00:29:38,730 --> 00:29:42,840
key and then I'm only showing 1 field

00:29:41,220 --> 00:29:44,730
temperature otherwise the slide will get

00:29:42,840 --> 00:29:46,650
too big and then we have everything in

00:29:44,730 --> 00:29:48,480
order so if you have a look here the

00:29:46,650 --> 00:29:49,770
temperature has actually been prefixed

00:29:48,480 --> 00:29:52,080
with all of the members of the

00:29:49,770 --> 00:29:54,299
clustering column all of these guys okay

00:29:52,080 --> 00:29:56,850
so we can see that the tenth ninth

00:29:54,299 --> 00:29:59,309
eighth seventh so if we did a limit

00:29:56,850 --> 00:30:01,440
query on this table get me the last five

00:29:59,309 --> 00:30:03,720
events from this weather station it's

00:30:01,440 --> 00:30:08,250
going to be really quick okay if we

00:30:03,720 --> 00:30:11,549
asked for a if we didn't include the

00:30:08,250 --> 00:30:12,809
cost the partition key in our query your

00:30:11,549 --> 00:30:14,700
query or you'd have to go to every

00:30:12,809 --> 00:30:16,049
single node in the cluster for that

00:30:14,700 --> 00:30:17,850
reason Cassandra doesn't really let you

00:30:16,049 --> 00:30:19,500
do it there are some cases where it will

00:30:17,850 --> 00:30:24,150
but that's when we're going to use spark

00:30:19,500 --> 00:30:25,799
to do it all okay I'm say yeah thousand

00:30:24,150 --> 00:30:28,350
no cluster we just go directly to one

00:30:25,799 --> 00:30:30,299
node all right this also had fancy

00:30:28,350 --> 00:30:31,650
animation but it's all gone so

00:30:30,299 --> 00:30:33,419
essentially if we want to do a query

00:30:31,650 --> 00:30:35,760
which is say greater than 7 and less

00:30:33,419 --> 00:30:38,160
than 10 and this is our former on disk

00:30:35,760 --> 00:30:40,559
what Cassandra do is go directly in you

00:30:38,160 --> 00:30:43,530
know start at ten over the indexing on

00:30:40,559 --> 00:30:45,210
file and then slice it and it gives you

00:30:43,530 --> 00:30:46,559
it back is a nice table so you don't

00:30:45,210 --> 00:30:48,059
have to worry about that format when

00:30:46,559 --> 00:30:50,429
you're programming you know if you're

00:30:48,059 --> 00:30:52,320
used to tabular data from a relational

00:30:50,429 --> 00:30:54,179
database the programming API is the same

00:30:52,320 --> 00:30:57,630
just know is that's why it performs

00:30:54,179 --> 00:30:59,280
really quite quickly you can also just

00:30:57,630 --> 00:31:01,020
store you don't have to use the

00:30:59,280 --> 00:31:03,150
clustering column functionality you can

00:31:01,020 --> 00:31:05,850
just have a primary key which you know

00:31:03,150 --> 00:31:07,380
with just the partition key so an

00:31:05,850 --> 00:31:09,150
example of that is your reference data

00:31:07,380 --> 00:31:10,799
or you know something where you're

00:31:09,150 --> 00:31:13,440
always you're not going to be wanting to

00:31:10,799 --> 00:31:17,100
do range queries so the weather station

00:31:13,440 --> 00:31:19,080
data is like that in Cassandra where we

00:31:17,100 --> 00:31:20,610
would start to lose out in Cassandra is

00:31:19,080 --> 00:31:22,679
when we wanted to build

00:31:20,610 --> 00:31:24,720
table like this now I've already told

00:31:22,679 --> 00:31:26,520
you can't do an aggregate style query in

00:31:24,720 --> 00:31:29,179
Cassandra so what we want to be able to

00:31:26,520 --> 00:31:31,470
do is keep a materialized view like a

00:31:29,179 --> 00:31:34,290
materialized view up to date in

00:31:31,470 --> 00:31:36,570
Cassandra this one here is for the daily

00:31:34,290 --> 00:31:39,630
aggregate temperature so it has the same

00:31:36,570 --> 00:31:42,000
primary key but we're actually going to

00:31:39,630 --> 00:31:44,510
keep things like the day high the day

00:31:42,000 --> 00:31:48,179
low you know averages things like that

00:31:44,510 --> 00:31:50,340
and there's no real way to to build this

00:31:48,179 --> 00:31:51,540
okay there's um you could obviously do

00:31:50,340 --> 00:31:53,370
some fancy things inside your

00:31:51,540 --> 00:31:56,220
application but we're going to use spark

00:31:53,370 --> 00:31:59,370
for that another example might be of a

00:31:56,220 --> 00:32:01,530
materialized view in an air quotes is a

00:31:59,370 --> 00:32:03,419
daily aggregate precipitation to

00:32:01,530 --> 00:32:07,590
constantly keep up to date with how much

00:32:03,419 --> 00:32:09,299
rain has happened every day okay and we

00:32:07,590 --> 00:32:10,380
could do a batch job for that and I'm

00:32:09,299 --> 00:32:11,549
going to show you an example we are

00:32:10,380 --> 00:32:14,220
going to show you about example of a

00:32:11,549 --> 00:32:15,720
batch job for that where but in reality

00:32:14,220 --> 00:32:17,130
what we want to do was with spike

00:32:15,720 --> 00:32:18,960
streaming to just keep them up to date

00:32:17,130 --> 00:32:20,490
forever so we're building like a

00:32:18,960 --> 00:32:22,049
dashboard application it can just go

00:32:20,490 --> 00:32:24,000
directly to Cassandra do one of the

00:32:22,049 --> 00:32:25,500
performant queries and the spark

00:32:24,000 --> 00:32:28,890
streaming job is going to is going to

00:32:25,500 --> 00:32:30,150
keep it up to date so I'm sorry if you

00:32:28,890 --> 00:32:31,919
don't like Scala I realize some people

00:32:30,150 --> 00:32:33,929
don't but that's quickly just to raise

00:32:31,919 --> 00:32:35,250
free some of the code so this is

00:32:33,929 --> 00:32:36,870
actually what the raw data looks like

00:32:35,250 --> 00:32:40,440
this is real data by the way its

00:32:36,870 --> 00:32:41,850
historic data from from the US you can

00:32:40,440 --> 00:32:44,720
you can go and get years worth of it

00:32:41,850 --> 00:32:47,880
this is a case class in Scala so you can

00:32:44,720 --> 00:32:50,520
take a this basis it's like a POJO ver

00:32:47,880 --> 00:32:51,900
in Java it's or a struct it's just how

00:32:50,520 --> 00:32:53,460
we're going to represent the data inside

00:32:51,900 --> 00:32:56,429
their application so it's in the same

00:32:53,460 --> 00:32:58,470
order as the comma separated value you

00:32:56,429 --> 00:33:00,150
create a streaming context this is where

00:32:58,470 --> 00:33:04,320
you select how often you want your code

00:33:00,150 --> 00:33:06,660
to run right ignore the kafka bit okays

00:33:04,320 --> 00:33:09,030
are just connecting to Kafka everything

00:33:06,660 --> 00:33:10,679
after the cuff cab it is would be the

00:33:09,030 --> 00:33:14,390
same whether you're getting it from MQTT

00:33:10,679 --> 00:33:17,429
or zeromq or from a soccer or whatever

00:33:14,390 --> 00:33:20,070
we can split this comma separated values

00:33:17,429 --> 00:33:22,140
you know by x comma and then shove it

00:33:20,070 --> 00:33:23,970
into the case class this is actually

00:33:22,140 --> 00:33:25,860
going to give us a stream overall

00:33:23,970 --> 00:33:27,630
weather data artifacts and then it's our

00:33:25,860 --> 00:33:31,770
responsibility to do something funky

00:33:27,630 --> 00:33:34,050
with that one really cool feature of the

00:33:31,770 --> 00:33:35,970
connector is that you can just if you're

00:33:34,050 --> 00:33:37,590
case class or if you've got a couple

00:33:35,970 --> 00:33:41,250
matches your cassandra table directly

00:33:37,590 --> 00:33:43,020
all you do is save to Cassandra with the

00:33:41,250 --> 00:33:45,030
key space and a table name and it will

00:33:43,020 --> 00:33:46,530
just put it in there so even though i'm

00:33:45,030 --> 00:33:48,960
talking about stream analytics here we

00:33:46,530 --> 00:33:50,460
are leaving the raw data so now actually

00:33:48,960 --> 00:33:54,390
spy extreme and has taken over our

00:33:50,460 --> 00:33:56,310
ingestion of raw data the next thing I'm

00:33:54,390 --> 00:33:59,040
going to do is keep a date the daily

00:33:56,310 --> 00:34:01,650
aggregate table up to date all right so

00:33:59,040 --> 00:34:03,330
what we can do is we we take that stream

00:34:01,650 --> 00:34:06,210
so remember that's a stream of raw

00:34:03,330 --> 00:34:08,940
weather data case classes we can then

00:34:06,210 --> 00:34:11,670
map it to look exactly like this table

00:34:08,940 --> 00:34:15,780
right so it's got the weather station

00:34:11,670 --> 00:34:18,240
the year month and date and it's got the

00:34:15,780 --> 00:34:21,570
same primary key now there's a really

00:34:18,240 --> 00:34:22,890
funky con type in Cassandra which used

00:34:21,570 --> 00:34:25,800
to be quite buggy but works really well

00:34:22,890 --> 00:34:27,510
in 2.1 called the counter now I cut what

00:34:25,800 --> 00:34:29,630
a counter is is like imagine it as a

00:34:27,510 --> 00:34:32,400
normal integer field inside a database

00:34:29,630 --> 00:34:35,400
but every time you update it just adds

00:34:32,400 --> 00:34:36,990
on the value okay so what were acts what

00:34:35,400 --> 00:34:39,510
this code is actually going to do is

00:34:36,990 --> 00:34:43,440
every 500 milliseconds it's going to

00:34:39,510 --> 00:34:45,360
save all of the raw data it's going to

00:34:43,440 --> 00:34:46,620
map all of the raw data into this it's

00:34:45,360 --> 00:34:48,750
going to take out the one hour

00:34:46,620 --> 00:34:50,940
precipitation and it's going to put it

00:34:48,750 --> 00:34:52,560
in Cassandra and the Cassandra counter

00:34:50,940 --> 00:34:54,960
is going to keep it up to date it's

00:34:52,560 --> 00:34:57,120
going to add them all up for you so you

00:34:54,960 --> 00:34:58,110
can go to this table at any time and it

00:34:57,120 --> 00:35:00,980
will be at most five hundred

00:34:58,110 --> 00:35:05,460
milliseconds behind behind real time

00:35:00,980 --> 00:35:07,800
what about a batch job okay so this is

00:35:05,460 --> 00:35:09,840
what good old this isn't streaming this

00:35:07,800 --> 00:35:13,290
is just a regular loading a cassandra

00:35:09,840 --> 00:35:16,470
table from into an RDD we're selecting

00:35:13,290 --> 00:35:17,820
the temperature now the important bit

00:35:16,470 --> 00:35:20,580
here is we're selecting the weather

00:35:17,820 --> 00:35:22,140
station ID in this query okay what does

00:35:20,580 --> 00:35:23,790
that mean in Cassandra means it's going

00:35:22,140 --> 00:35:26,580
to be performance it's going to go to a

00:35:23,790 --> 00:35:28,230
single node right so this batch job will

00:35:26,580 --> 00:35:29,940
actually collect will actually be really

00:35:28,230 --> 00:35:32,190
quickly you could make this as part of

00:35:29,940 --> 00:35:34,170
your operational system because it's

00:35:32,190 --> 00:35:36,930
going to a single Cassandra partition

00:35:34,170 --> 00:35:38,400
all right and here's an example in spark

00:35:36,930 --> 00:35:40,440
you have the stats counter which can

00:35:38,400 --> 00:35:42,630
then you know keep give you a you know

00:35:40,440 --> 00:35:45,720
account and means and things so we could

00:35:42,630 --> 00:35:47,850
then save this back to Cassandra if we

00:35:45,720 --> 00:35:50,430
don't include the weather station I do

00:35:47,850 --> 00:35:53,430
in this example here right this is going

00:35:50,430 --> 00:35:57,210
to scan the entire cassandra table this

00:35:53,430 --> 00:35:58,650
one is going to take time depending on

00:35:57,210 --> 00:35:59,970
how big your partition is how many

00:35:58,650 --> 00:36:02,220
weather events do you have for a

00:35:59,970 --> 00:36:04,320
particular station this one is going to

00:36:02,220 --> 00:36:05,970
get slower you know as you add more

00:36:04,320 --> 00:36:08,730
weather stations and as you add more

00:36:05,970 --> 00:36:11,610
data all right you would not make this

00:36:08,730 --> 00:36:12,990
one part of your operational system

00:36:11,610 --> 00:36:15,030
because it has to scan an entire

00:36:12,990 --> 00:36:17,160
Cassandra cluster this would be a bad

00:36:15,030 --> 00:36:18,240
job which you might run at night this

00:36:17,160 --> 00:36:20,430
would be the one that you could run

00:36:18,240 --> 00:36:23,520
whenever if you wanted to keep this one

00:36:20,430 --> 00:36:25,080
up to date which would be the mean and

00:36:23,520 --> 00:36:26,910
the high-low for all of your weather

00:36:25,080 --> 00:36:28,590
stations you would then write a spark

00:36:26,910 --> 00:36:30,150
streaming job so you in the same

00:36:28,590 --> 00:36:32,220
application you can have a mixture of a

00:36:30,150 --> 00:36:37,620
spark streaming and spark batch jobs

00:36:32,220 --> 00:36:39,210
okay so that's pretty much it you can

00:36:37,620 --> 00:36:41,520
see here that you can easily include

00:36:39,210 --> 00:36:44,190
both okay so when you're building a

00:36:41,520 --> 00:36:46,260
streaming system with arm kafka the

00:36:44,190 --> 00:36:48,420
story with um spike streaming you need

00:36:46,260 --> 00:36:49,560
somewhere to put the data okay i'm

00:36:48,420 --> 00:36:50,880
saying cassandra is a pretty good

00:36:49,560 --> 00:36:53,700
solution especially if you have vast

00:36:50,880 --> 00:36:55,500
quantities for the raw data for the

00:36:53,700 --> 00:36:56,910
materialized views you can put those in

00:36:55,500 --> 00:36:58,890
Cassandra as well or you could put them

00:36:56,910 --> 00:37:01,470
in a caching system or you know because

00:36:58,890 --> 00:37:04,830
you know there's not the data size there

00:37:01,470 --> 00:37:06,180
isn't as great and don't forget that if

00:37:04,830 --> 00:37:07,920
you're doing spy extremely you can also

00:37:06,180 --> 00:37:09,750
combine it with running batch jobs on

00:37:07,920 --> 00:37:11,310
the fly some batch job as if they're in

00:37:09,750 --> 00:37:13,140
a single Cassandra partition are going

00:37:11,310 --> 00:37:14,730
to be quick however the jobs which are

00:37:13,140 --> 00:37:16,110
going to be over your entire cassandra

00:37:14,730 --> 00:37:18,450
tables with you know gigabytes terabytes

00:37:16,110 --> 00:37:19,650
petabytes of data remember they're going

00:37:18,450 --> 00:37:21,810
to be the ones you're going to run at

00:37:19,650 --> 00:37:23,730
the end of a day and the whole goal is

00:37:21,810 --> 00:37:25,590
essentially to keep a bunch of views

00:37:23,730 --> 00:37:27,780
ready so this guy can just pop in here

00:37:25,590 --> 00:37:30,060
whenever he likes and see an up-to-date

00:37:27,780 --> 00:37:32,070
version of these aggregates even if

00:37:30,060 --> 00:37:33,780
you're talking about you know many tens

00:37:32,070 --> 00:37:35,790
of thousands of events per second and

00:37:33,780 --> 00:37:40,650
you know many terabytes of data under

00:37:35,790 --> 00:37:41,790
the covers so on that note I am done if

00:37:40,650 --> 00:37:43,470
you've got any questions I think we have

00:37:41,790 --> 00:37:45,990
a couple of minutes for questions so

00:37:43,470 --> 00:37:47,970
thanks for listening and hopefully you

00:37:45,990 --> 00:37:49,380
have a good idea now of whether things

00:37:47,970 --> 00:37:50,760
something like spark and Cassandra's

00:37:49,380 --> 00:37:55,070
would be a good text up for your next

00:37:50,760 --> 00:37:55,070

YouTube URL: https://www.youtube.com/watch?v=rvaRH0LgzJU


