Title: Philipp Krenn – Log Management: From grep to Full-Text Search and Back
Publication date: 2021-06-28
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Logs are everywhere. But they have gone through an interesting development over the years:

- grep: This works well as long as you have a single instance to search on. Once you need to SSH into many machines and try to piece together the results of multiple grep commands, things tend not to work that well anymore.
- Splunk: Centralizing those logs and letting users search through them with a piped language in Splunk is the logical step to fix that issue. However, the more data you centralize, the slower this will get.
- ELK: The solution to that idleness is using full-text search. Elasticsearch, in combination with Logstash and Kibana (plus Beats), gave logs a major performance boost. But at what cost?
- Loki: Reducing the scope and going back to a smart data structure combined with grep gives Loki the possibility to reduce costs while still providing good performance.
- Closing the gap: So what are the tradeoffs between the different systems, and are they potentially closing some gaps between performance and cost?

Speaker:
Philipp Krenn – https://2021.berlinbuzzwords.de/member/philipp-krenn

More: https://2021.berlinbuzzwords.de/session/log-management-grep-full-text-search-and-back
Captions: 
	00:00:09,280 --> 00:00:12,320
hi

00:00:09,760 --> 00:00:13,280
i'm philip let's talk about log

00:00:12,320 --> 00:00:14,960
management or

00:00:13,280 --> 00:00:17,199
actually before we dive into log

00:00:14,960 --> 00:00:18,320
management let's take a look at

00:00:17,199 --> 00:00:20,400
technology and

00:00:18,320 --> 00:00:21,520
moves and counter moves as an

00:00:20,400 --> 00:00:24,560
introduction

00:00:21,520 --> 00:00:27,599
so back in the very old days

00:00:24,560 --> 00:00:29,920
well before my time we had mainframes

00:00:27,599 --> 00:00:30,880
and terminals that were connecting to

00:00:29,920 --> 00:00:33,280
them

00:00:30,880 --> 00:00:35,520
so you wouldn't have the power of a full

00:00:33,280 --> 00:00:36,399
computer on your desk but you would only

00:00:35,520 --> 00:00:39,760
connect to that

00:00:36,399 --> 00:00:42,879
mainframe remotely and

00:00:39,760 --> 00:00:44,480
after some time then we moved over to

00:00:42,879 --> 00:00:47,200
personal computers sometimes those are

00:00:44,480 --> 00:00:50,480
called fat clients heavy rich or

00:00:47,200 --> 00:00:52,719
thick clients and part of that was

00:00:50,480 --> 00:00:54,640
you could work offline you wouldn't need

00:00:52,719 --> 00:00:57,440
that mainframe and all of those

00:00:54,640 --> 00:00:58,079
centralized server capacities and it was

00:00:57,440 --> 00:01:01,359
for example

00:00:58,079 --> 00:01:03,600
better in graphics heavy use cases

00:01:01,359 --> 00:01:06,000
so it was kind of a move away from that

00:01:03,600 --> 00:01:09,280
only centralized resource to a more

00:01:06,000 --> 00:01:12,320
decentralized resource and then

00:01:09,280 --> 00:01:15,200
some time later on we came over to

00:01:12,320 --> 00:01:17,439
thin clients so i remember back when i

00:01:15,200 --> 00:01:20,159
started university for example we had

00:01:17,439 --> 00:01:22,400
citrix all over the the campus and

00:01:20,159 --> 00:01:22,720
everybody could connect to their setup

00:01:22,400 --> 00:01:25,280
or

00:01:22,720 --> 00:01:26,240
load their setup basically on the thin

00:01:25,280 --> 00:01:28,400
clients

00:01:26,240 --> 00:01:29,920
which was fascinating from a technology

00:01:28,400 --> 00:01:30,799
point of view and it gave you a lot of

00:01:29,920 --> 00:01:33,360
mobility

00:01:30,799 --> 00:01:35,040
but it was also quite a pain to actually

00:01:33,360 --> 00:01:36,799
get started because i remember every

00:01:35,040 --> 00:01:38,320
time we wanted to load this up it would

00:01:36,799 --> 00:01:39,920
take a couple of minutes until

00:01:38,320 --> 00:01:40,960
everything was there and actually

00:01:39,920 --> 00:01:44,000
workable

00:01:40,960 --> 00:01:46,960
so while interesting this was still

00:01:44,000 --> 00:01:48,320
kind of like painful and again another

00:01:46,960 --> 00:01:51,119
counter move happened after that

00:01:48,320 --> 00:01:53,040
so after that we kind of all got our own

00:01:51,119 --> 00:01:54,720
laptops and suddenly we had everything

00:01:53,040 --> 00:01:57,040
that we wanted

00:01:54,720 --> 00:01:58,960
offline again could carry it with us and

00:01:57,040 --> 00:02:00,799
we wouldn't need to connect to any

00:01:58,960 --> 00:02:02,640
remote servers again

00:02:00,799 --> 00:02:04,479
the university campus was saving on

00:02:02,640 --> 00:02:06,960
centralized server capacities

00:02:04,479 --> 00:02:09,360
and only needed to provide wi-fi while

00:02:06,960 --> 00:02:11,680
we all carried our own laptops around

00:02:09,360 --> 00:02:12,720
so it was kind of like the counter move

00:02:11,680 --> 00:02:14,959
and now

00:02:12,720 --> 00:02:16,400
today we have kind of like moved in the

00:02:14,959 --> 00:02:19,520
opposite direction again

00:02:16,400 --> 00:02:20,160
where pretty much everything we do is in

00:02:19,520 --> 00:02:22,319
the cloud

00:02:20,160 --> 00:02:23,599
it's maybe i don't know some shared

00:02:22,319 --> 00:02:26,640
google doc it's

00:02:23,599 --> 00:02:30,080
something on github nothing is

00:02:26,640 --> 00:02:32,400
really anywhere um on our end

00:02:30,080 --> 00:02:34,480
devices anymore but it's really just

00:02:32,400 --> 00:02:37,200
spread out all over the place again

00:02:34,480 --> 00:02:38,959
and i think this kind of like move and

00:02:37,200 --> 00:02:41,280
counter move in technology

00:02:38,959 --> 00:02:42,160
is a very nice example of how

00:02:41,280 --> 00:02:44,640
technologies

00:02:42,160 --> 00:02:45,760
often reach some limit or you hit some

00:02:44,640 --> 00:02:47,519
pain point

00:02:45,760 --> 00:02:50,400
and then you need to kind of like change

00:02:47,519 --> 00:02:52,720
concept or switch over to something

00:02:50,400 --> 00:02:54,640
um to make that pain point away and you

00:02:52,720 --> 00:02:56,800
normally trade that pain point for

00:02:54,640 --> 00:02:59,120
something else and then at some later

00:02:56,800 --> 00:03:00,879
point maybe the pendulum is swinging

00:02:59,120 --> 00:03:02,959
back again that you make that counter

00:03:00,879 --> 00:03:04,640
move and i think that centralized versus

00:03:02,959 --> 00:03:05,680
decentralized in computing is a nice

00:03:04,640 --> 00:03:08,800
example of that

00:03:05,680 --> 00:03:10,400
pendulum swinging back and forth and

00:03:08,800 --> 00:03:11,760
maybe at some point we'll move away from

00:03:10,400 --> 00:03:14,400
the cloud again

00:03:11,760 --> 00:03:16,239
and say like for privacy reasons or

00:03:14,400 --> 00:03:18,800
because our devices are so powerful

00:03:16,239 --> 00:03:19,440
we want to have everything locally again

00:03:18,800 --> 00:03:22,640
maybe

00:03:19,440 --> 00:03:24,319
not necessarily but what does all of

00:03:22,640 --> 00:03:27,040
that have to do with logs

00:03:24,319 --> 00:03:28,400
so that general topic of change and

00:03:27,040 --> 00:03:30,480
requirements and trade-off

00:03:28,400 --> 00:03:33,120
is also something that i want to discuss

00:03:30,480 --> 00:03:35,760
now in more detail around logs and

00:03:33,120 --> 00:03:37,840
how those are also are moving kind of

00:03:35,760 --> 00:03:38,799
like back and forth between different

00:03:37,840 --> 00:03:40,720
paradigms

00:03:38,799 --> 00:03:41,920
and how you can always trade your pain

00:03:40,720 --> 00:03:44,000
points for

00:03:41,920 --> 00:03:46,000
new opportunities and potentially also

00:03:44,000 --> 00:03:48,560
new pain points again

00:03:46,000 --> 00:03:50,400
so why am i talking about that i work

00:03:48,560 --> 00:03:51,760
for elastic as a developer advocate so i

00:03:50,400 --> 00:03:54,000
normally talk about the good

00:03:51,760 --> 00:03:55,760
stuff that elastic is doing or all the

00:03:54,000 --> 00:03:58,080
technologies that we have

00:03:55,760 --> 00:04:00,239
and while i will cover part of that i

00:03:58,080 --> 00:04:02,720
want to take a bit of a step back today

00:04:00,239 --> 00:04:04,480
and show you the bigger picture of what

00:04:02,720 --> 00:04:05,680
is generally happening in the log

00:04:04,480 --> 00:04:08,799
ecosystem since

00:04:05,680 --> 00:04:11,840
logging is kind of like pretty clear or

00:04:08,799 --> 00:04:14,239
dear to our heart we want to keep that

00:04:11,840 --> 00:04:15,840
and kind of like focus on what is

00:04:14,239 --> 00:04:17,840
generally happening in the ecosystem

00:04:15,840 --> 00:04:19,280
what are we doing what are others doing

00:04:17,840 --> 00:04:21,919
and how can that

00:04:19,280 --> 00:04:23,120
kind of like move log management forward

00:04:21,919 --> 00:04:25,919
so this is kind of the

00:04:23,120 --> 00:04:27,520
the gist of this talk so starting with

00:04:25,919 --> 00:04:29,759
log management probably the

00:04:27,520 --> 00:04:31,440
the first thing a lot of people are

00:04:29,759 --> 00:04:34,000
doing or were using

00:04:31,440 --> 00:04:35,759
was something like tail f or maybe you

00:04:34,000 --> 00:04:38,240
were a little more advanced and used

00:04:35,759 --> 00:04:39,680
less plus capital f so while you could

00:04:38,240 --> 00:04:40,960
still follow the logs as they were

00:04:39,680 --> 00:04:43,280
flowing in

00:04:40,960 --> 00:04:44,080
um you could break out of that follow

00:04:43,280 --> 00:04:47,040
basically

00:04:44,080 --> 00:04:48,639
move up and down even search in that

00:04:47,040 --> 00:04:50,800
file in the error log here

00:04:48,639 --> 00:04:52,639
and then could jump back into following

00:04:50,800 --> 00:04:53,199
whereas with tail you would always need

00:04:52,639 --> 00:04:54,960
to

00:04:53,199 --> 00:04:56,639
move out of tail switch to another

00:04:54,960 --> 00:04:58,639
program to search for anything or move

00:04:56,639 --> 00:05:01,120
up and down and then head back to tail

00:04:58,639 --> 00:05:02,880
so less is kind of like the slightly

00:05:01,120 --> 00:05:06,000
more advanced version of

00:05:02,880 --> 00:05:09,120
tailing or following that file um

00:05:06,000 --> 00:05:10,160
and that worked until you had this very

00:05:09,120 --> 00:05:12,160
large log file

00:05:10,160 --> 00:05:14,160
and you're basically that little

00:05:12,160 --> 00:05:17,199
submarine trying to find

00:05:14,160 --> 00:05:20,240
anything in that huge titanic

00:05:17,199 --> 00:05:21,759
log file that we have here so

00:05:20,240 --> 00:05:23,280
we had to kind of like switch our tools

00:05:21,759 --> 00:05:26,240
a little bit and

00:05:23,280 --> 00:05:27,199
one way to do that probably was or is

00:05:26,240 --> 00:05:30,000
grep

00:05:27,199 --> 00:05:30,479
where we can say for example we have an

00:05:30,000 --> 00:05:32,240
error

00:05:30,479 --> 00:05:34,400
and we want to find that error in that

00:05:32,240 --> 00:05:35,039
log file but we just don't want to find

00:05:34,400 --> 00:05:37,120
the error

00:05:35,039 --> 00:05:39,440
but we want to find the most common

00:05:37,120 --> 00:05:40,800
errors or common unique errors in that

00:05:39,440 --> 00:05:44,080
log file

00:05:40,800 --> 00:05:44,639
so we are sorting on the the unique

00:05:44,080 --> 00:05:47,039
lines

00:05:44,639 --> 00:05:47,759
um sorting recursively and getting them

00:05:47,039 --> 00:05:50,240
the five

00:05:47,759 --> 00:05:51,039
top ones of those so we would get the

00:05:50,240 --> 00:05:54,400
top five

00:05:51,039 --> 00:05:57,520
unique errors from our log file so

00:05:54,400 --> 00:05:58,479
this was nice and worked and many people

00:05:57,520 --> 00:06:00,080
are still using that

00:05:58,479 --> 00:06:01,840
but it still has some problems so for

00:06:00,080 --> 00:06:02,560
example if you have any horizontal

00:06:01,840 --> 00:06:05,680
scaling

00:06:02,560 --> 00:06:07,759
and you have stuff or not just one

00:06:05,680 --> 00:06:10,160
node or container or whatever you have

00:06:07,759 --> 00:06:12,720
but five or ten or more

00:06:10,160 --> 00:06:14,160
it somehow gets painful to grab the log

00:06:12,720 --> 00:06:15,919
files there it like

00:06:14,160 --> 00:06:17,759
well maybe you could try to download all

00:06:15,919 --> 00:06:21,199
the log wise and grab them or you can

00:06:17,759 --> 00:06:24,240
have 10 parallel ssh sessions to those

00:06:21,199 --> 00:06:25,120
servers and then grab there but that's

00:06:24,240 --> 00:06:27,680
still all

00:06:25,120 --> 00:06:29,280
not a lot of fun also if you have

00:06:27,680 --> 00:06:29,600
distributed applications where it's not

00:06:29,280 --> 00:06:31,120
just

00:06:29,600 --> 00:06:33,520
one log file but you need to piece

00:06:31,120 --> 00:06:35,520
together 20 different log files

00:06:33,520 --> 00:06:37,680
the wrap alone is probably not going to

00:06:35,520 --> 00:06:38,800
give you a very clear picture of what is

00:06:37,680 --> 00:06:41,280
happening there

00:06:38,800 --> 00:06:43,039
also if you have anything containerized

00:06:41,280 --> 00:06:45,280
or using kubernetes where

00:06:43,039 --> 00:06:47,520
containers come and go and the logs are

00:06:45,280 --> 00:06:49,520
more ephemeral

00:06:47,520 --> 00:06:50,720
is also not that great in that regard

00:06:49,520 --> 00:06:52,639
because if

00:06:50,720 --> 00:06:54,000
the log file for a specific container

00:06:52,639 --> 00:06:55,520
has been removed um

00:06:54,000 --> 00:06:57,199
it's gone and you won't find what has

00:06:55,520 --> 00:07:00,319
been up there anymore

00:06:57,199 --> 00:07:02,000
so we need a new solution um that is

00:07:00,319 --> 00:07:04,639
kind of like moving out of like

00:07:02,000 --> 00:07:05,360
just searching through a centralized log

00:07:04,639 --> 00:07:08,800
file

00:07:05,360 --> 00:07:11,520
um and long comes or came

00:07:08,800 --> 00:07:12,319
splunk for example being relatively

00:07:11,520 --> 00:07:15,360
early on

00:07:12,319 --> 00:07:17,680
in that log game there splunk provided

00:07:15,360 --> 00:07:18,479
the centralization so if you have an

00:07:17,680 --> 00:07:20,639
application

00:07:18,479 --> 00:07:22,160
that you need to horizontally scale or

00:07:20,639 --> 00:07:24,479
that is more distributed

00:07:22,160 --> 00:07:26,240
you could just have your log forwarders

00:07:24,479 --> 00:07:28,639
that would forward your logs

00:07:26,240 --> 00:07:30,000
to the so-called peer nodes that store

00:07:28,639 --> 00:07:32,240
the data potentially

00:07:30,000 --> 00:07:34,160
replicate between them so if one of them

00:07:32,240 --> 00:07:36,400
dies you still have the logs

00:07:34,160 --> 00:07:38,240
so you will not lose any important log

00:07:36,400 --> 00:07:39,840
messages even if a node dies and then

00:07:38,240 --> 00:07:42,160
you have a search head

00:07:39,840 --> 00:07:44,879
that allows you to search through all of

00:07:42,160 --> 00:07:45,520
those log files or logs that have been

00:07:44,879 --> 00:07:49,280
forwarded

00:07:45,520 --> 00:07:51,520
by your log forwarders so

00:07:49,280 --> 00:07:53,520
that is both more scalable and also much

00:07:51,520 --> 00:07:53,919
easier to work in a distributed fashion

00:07:53,520 --> 00:07:57,039
or

00:07:53,919 --> 00:08:00,560
horizontally scaled fashion

00:07:57,039 --> 00:08:01,520
so this was a big step forward for log

00:08:00,560 --> 00:08:04,240
management

00:08:01,520 --> 00:08:05,759
and it built in terms of queries on

00:08:04,240 --> 00:08:07,759
relatively similar

00:08:05,759 --> 00:08:09,440
concepts so for example i could say the

00:08:07,759 --> 00:08:12,879
source is this specific

00:08:09,440 --> 00:08:15,039
error log file here and then i run

00:08:12,879 --> 00:08:16,000
regexquery and just search for

00:08:15,039 --> 00:08:18,319
everything that is

00:08:16,000 --> 00:08:19,759
fatal in there or i say oh i take the

00:08:18,319 --> 00:08:22,080
ngxs logs

00:08:19,759 --> 00:08:22,960
i take the one the thousand first

00:08:22,080 --> 00:08:25,919
entries

00:08:22,960 --> 00:08:26,639
and then get the top 50 client ips in

00:08:25,919 --> 00:08:29,280
there

00:08:26,639 --> 00:08:31,440
so it still looks kind of similar what

00:08:29,280 --> 00:08:33,839
we were doing grep before and where we

00:08:31,440 --> 00:08:35,680
were piping one command into the next

00:08:33,839 --> 00:08:37,919
so it was kind of like a very natural

00:08:35,680 --> 00:08:38,719
evolution going from that single log

00:08:37,919 --> 00:08:40,880
file to a more

00:08:38,719 --> 00:08:42,640
centralized approach and then working

00:08:40,880 --> 00:08:46,560
with more or less similar

00:08:42,640 --> 00:08:46,560
queries or concepts at least

00:08:46,880 --> 00:08:51,600
also splunk generally has two approaches

00:08:49,440 --> 00:08:51,920
the event index is minimally structured

00:08:51,600 --> 00:08:54,720
where

00:08:51,920 --> 00:08:55,600
very few fields are indexed or

00:08:54,720 --> 00:08:57,760
structured

00:08:55,600 --> 00:08:58,720
and the rest is just free-flowing and

00:08:57,760 --> 00:09:02,080
you can

00:08:58,720 --> 00:09:04,880
grab or just search through it on in

00:09:02,080 --> 00:09:06,080
any unstructured way you they also have

00:09:04,880 --> 00:09:08,160
now

00:09:06,080 --> 00:09:09,920
metrics index which allows you a more

00:09:08,160 --> 00:09:11,760
highly structured approach

00:09:09,920 --> 00:09:13,279
which for example if you want to show

00:09:11,760 --> 00:09:15,600
any metrics um

00:09:13,279 --> 00:09:16,480
might add a lot of performance gain

00:09:15,600 --> 00:09:18,720
there

00:09:16,480 --> 00:09:21,200
but by default events are minimally

00:09:18,720 --> 00:09:22,880
structured so you don't need to

00:09:21,200 --> 00:09:24,480
parse your logs for example you don't

00:09:22,880 --> 00:09:26,240
need to extract all the different

00:09:24,480 --> 00:09:28,640
pieces to make the most out of that so

00:09:26,240 --> 00:09:29,200
it was pretty easy to go from there just

00:09:28,640 --> 00:09:32,720
from a

00:09:29,200 --> 00:09:36,320
regular log file the problem is

00:09:32,720 --> 00:09:36,800
speed reached kind of a bump at some

00:09:36,320 --> 00:09:38,800
point

00:09:36,800 --> 00:09:39,839
especially if you have a large amount of

00:09:38,800 --> 00:09:41,360
log files

00:09:39,839 --> 00:09:43,279
where your searches would just get

00:09:41,360 --> 00:09:45,680
slower and slower because

00:09:43,279 --> 00:09:47,440
with minimal structure means you have to

00:09:45,680 --> 00:09:48,160
search through very large amounts of

00:09:47,440 --> 00:09:50,160
data

00:09:48,160 --> 00:09:51,440
um to find what you might be looking for

00:09:50,160 --> 00:09:54,160
the other thing is

00:09:51,440 --> 00:09:56,399
it can get pretty expensive since splunk

00:09:54,160 --> 00:09:58,320
is a commercial offering so you

00:09:56,399 --> 00:10:01,360
will pay a lot of money if you have a

00:09:58,320 --> 00:10:03,760
lot of ingestion of data going on there

00:10:01,360 --> 00:10:05,440
so it was probably time for the pendulum

00:10:03,760 --> 00:10:08,320
to swing another direction again

00:10:05,440 --> 00:10:08,720
and the thing that came i don't want to

00:10:08,320 --> 00:10:11,040
say

00:10:08,720 --> 00:10:12,800
after it but that kind of like evolved

00:10:11,040 --> 00:10:15,440
out of that situation was the

00:10:12,800 --> 00:10:16,399
famous elk stack elasticsearch logstash

00:10:15,440 --> 00:10:19,600
kibana

00:10:16,399 --> 00:10:20,000
to get logs parse them store them and

00:10:19,600 --> 00:10:23,200
then

00:10:20,000 --> 00:10:24,000
visualize them coming from an eco and

00:10:23,200 --> 00:10:27,040
open source

00:10:24,000 --> 00:10:27,760
uh background it provided a much cheaper

00:10:27,040 --> 00:10:30,320
alternative

00:10:27,760 --> 00:10:31,600
and also technology wise it just took a

00:10:30,320 --> 00:10:34,399
very different approach

00:10:31,600 --> 00:10:36,399
because elasticsearch being the data

00:10:34,399 --> 00:10:38,560
store storing all the data comes from

00:10:36,399 --> 00:10:40,000
full text search which leads to one of

00:10:38,560 --> 00:10:42,240
the very common questions like

00:10:40,000 --> 00:10:43,680
why would anybody put logs into a full

00:10:42,240 --> 00:10:46,320
text search engine i

00:10:43,680 --> 00:10:46,959
get it for if i want to search wikipedia

00:10:46,320 --> 00:10:49,519
i use

00:10:46,959 --> 00:10:51,600
full text search engine but do i really

00:10:49,519 --> 00:10:54,320
do that for my logs as well

00:10:51,600 --> 00:10:54,720
well kind of like the way you can see it

00:10:54,320 --> 00:10:56,800
is

00:10:54,720 --> 00:10:58,720
that storing the logs is kind of the

00:10:56,800 --> 00:10:59,360
boring part what you want is you want to

00:10:58,720 --> 00:11:01,920
find

00:10:59,360 --> 00:11:03,920
your relevant logs quickly and that is

00:11:01,920 --> 00:11:05,600
exactly what a full text search engine

00:11:03,920 --> 00:11:07,760
basically is doing so that's why full

00:11:05,600 --> 00:11:11,519
text search makes sense for logs

00:11:07,760 --> 00:11:13,440
as well and the way that the data is

00:11:11,519 --> 00:11:15,839
structured there if you have never

00:11:13,440 --> 00:11:17,440
seen elasticsearch lucine or any other

00:11:15,839 --> 00:11:20,640
systems built on them

00:11:17,440 --> 00:11:21,600
is you take whatever log messages for

00:11:20,640 --> 00:11:23,680
example you get in

00:11:21,600 --> 00:11:25,279
you tokenize them which basically means

00:11:23,680 --> 00:11:27,279
breaking up them up into

00:11:25,279 --> 00:11:29,680
individual words at least in western

00:11:27,279 --> 00:11:31,760
languages

00:11:29,680 --> 00:11:33,200
so you have that dictionary with the

00:11:31,760 --> 00:11:35,519
individual words

00:11:33,200 --> 00:11:36,800
um you have the frequency of how many

00:11:35,519 --> 00:11:39,600
times they are appearing and

00:11:36,800 --> 00:11:40,959
in which documents so if i'm afterwards

00:11:39,600 --> 00:11:44,480
searching for

00:11:40,959 --> 00:11:45,760
coming or fury or whatever i have i

00:11:44,480 --> 00:11:48,640
don't have to search through

00:11:45,760 --> 00:11:49,120
all that text body anymore but i can

00:11:48,640 --> 00:11:51,200
just

00:11:49,120 --> 00:11:53,200
look up that term in a dictionary and

00:11:51,200 --> 00:11:54,399
then find the documents where i have

00:11:53,200 --> 00:11:58,480
those hits and retrieve

00:11:54,399 --> 00:12:00,880
just those so by structuring the data

00:11:58,480 --> 00:12:01,680
upfront more and doing some more work

00:12:00,880 --> 00:12:03,839
there

00:12:01,680 --> 00:12:05,040
it makes searching much more performant

00:12:03,839 --> 00:12:09,040
and targeted

00:12:05,040 --> 00:12:10,639
in the end so you have these full text

00:12:09,040 --> 00:12:12,320
search capabilities it

00:12:10,639 --> 00:12:14,000
lucine also allows you to do

00:12:12,320 --> 00:12:16,639
aggregations filtering

00:12:14,000 --> 00:12:18,320
sorting on that data so you are not

00:12:16,639 --> 00:12:20,000
limited you could for example get the

00:12:18,320 --> 00:12:24,079
statistics of how many

00:12:20,000 --> 00:12:27,120
errors uh ones and debugs you have

00:12:24,079 --> 00:12:28,959
in your your logs and just to see how

00:12:27,120 --> 00:12:29,440
the distribution is and changes over

00:12:28,959 --> 00:12:32,399
time

00:12:29,440 --> 00:12:33,200
or you could filter down just on error

00:12:32,399 --> 00:12:35,920
or fatal

00:12:33,200 --> 00:12:36,560
messages or you could sort by the most

00:12:35,920 --> 00:12:38,560
recent

00:12:36,560 --> 00:12:40,160
log events that you have been collecting

00:12:38,560 --> 00:12:40,800
so you have all the features that you

00:12:40,160 --> 00:12:44,320
basically

00:12:40,800 --> 00:12:47,279
need for log management a query though

00:12:44,320 --> 00:12:47,680
looks very different than what you had

00:12:47,279 --> 00:12:49,920
in

00:12:47,680 --> 00:12:51,440
other systems before so here for example

00:12:49,920 --> 00:12:54,639
i'm searching

00:12:51,440 --> 00:12:56,800
logs and i'm using a boolean query using

00:12:54,639 --> 00:12:57,760
the more structured nature of

00:12:56,800 --> 00:12:59,600
elasticsearch

00:12:57,760 --> 00:13:01,120
then you have fields for example in the

00:12:59,600 --> 00:13:04,079
field log dot

00:13:01,120 --> 00:13:05,440
level so we have a field level in the

00:13:04,079 --> 00:13:08,959
sub document in

00:13:05,440 --> 00:13:11,200
the object log um and that one

00:13:08,959 --> 00:13:13,600
is limited just to error so we are

00:13:11,200 --> 00:13:14,480
filtering and making better use of that

00:13:13,600 --> 00:13:16,399
structured

00:13:14,480 --> 00:13:17,680
nature of the data that is stored in

00:13:16,399 --> 00:13:20,240
elasticsearch

00:13:17,680 --> 00:13:21,120
um you could also in kibana then just

00:13:20,240 --> 00:13:23,600
for example

00:13:21,120 --> 00:13:24,560
filter down on a container image name

00:13:23,600 --> 00:13:28,160
and i'm having a

00:13:24,560 --> 00:13:29,839
java logging 1.0 image here and then for

00:13:28,160 --> 00:13:30,320
example in kibana i've retrieved all of

00:13:29,839 --> 00:13:32,959
those

00:13:30,320 --> 00:13:33,600
blocks with whatever timestamp and then

00:13:32,959 --> 00:13:35,839
i could

00:13:33,600 --> 00:13:38,160
only highlight for example only the

00:13:35,839 --> 00:13:40,639
arrows since i have nicely broken those

00:13:38,160 --> 00:13:42,560
out here so all of that structure can be

00:13:40,639 --> 00:13:44,959
used to make your search

00:13:42,560 --> 00:13:45,680
faster and potentially also more

00:13:44,959 --> 00:13:49,519
powerful

00:13:45,680 --> 00:13:51,199
and get to what you want in a faster way

00:13:49,519 --> 00:13:52,800
the problem with that is that that

00:13:51,199 --> 00:13:55,120
parsing is potentially

00:13:52,800 --> 00:13:57,440
complicated and expensive unless you

00:13:55,120 --> 00:13:58,720
write in a structured format like json

00:13:57,440 --> 00:13:59,760
right away which would be highly

00:13:58,720 --> 00:14:02,079
recommended

00:13:59,760 --> 00:14:03,680
if you don't have that you might need to

00:14:02,079 --> 00:14:06,240
write some passing rules

00:14:03,680 --> 00:14:06,880
and there is of course some overhead for

00:14:06,240 --> 00:14:08,959
all of that

00:14:06,880 --> 00:14:10,560
structure and indexing and creating all

00:14:08,959 --> 00:14:12,160
those index structures

00:14:10,560 --> 00:14:14,480
to make your searches more performant

00:14:12,160 --> 00:14:16,959
afterwards so it will take extra

00:14:14,480 --> 00:14:18,560
overhead in terms of computation it will

00:14:16,959 --> 00:14:19,279
take extra disk space and it will

00:14:18,560 --> 00:14:21,360
potentially

00:14:19,279 --> 00:14:22,399
take some extra memory as well to keep

00:14:21,360 --> 00:14:25,680
all of those

00:14:22,399 --> 00:14:27,279
index structures in memory and keep them

00:14:25,680 --> 00:14:30,480
quickly available

00:14:27,279 --> 00:14:32,480
so there is a cost to all of that

00:14:30,480 --> 00:14:33,600
so once again it's kind of time that our

00:14:32,480 --> 00:14:35,360
pendulum

00:14:33,600 --> 00:14:37,600
is swinging another way or that people

00:14:35,360 --> 00:14:40,880
were like okay this is this is good

00:14:37,600 --> 00:14:44,240
but i still have pain points here so

00:14:40,880 --> 00:14:46,480
loki from grafana um is

00:14:44,240 --> 00:14:47,920
trying to take some of these pain points

00:14:46,480 --> 00:14:50,079
away in a different way

00:14:47,920 --> 00:14:51,279
so that the structure that loki is

00:14:50,079 --> 00:14:53,360
trying to do is where

00:14:51,279 --> 00:14:54,399
while elasticsearch is a general purpose

00:14:53,360 --> 00:14:57,600
data store

00:14:54,399 --> 00:15:00,000
loki is exactly focused on logs and is

00:14:57,600 --> 00:15:03,040
trying to take some clever trade-offs to

00:15:00,000 --> 00:15:07,040
make that specific problem around logs

00:15:03,040 --> 00:15:07,839
more or cheaper and keep the performance

00:15:07,040 --> 00:15:10,079
good while

00:15:07,839 --> 00:15:12,720
picking some trade-offs there so it has

00:15:10,079 --> 00:15:14,720
broken up right and read path

00:15:12,720 --> 00:15:16,800
it can ingest data though it doesn't

00:15:14,720 --> 00:15:18,800
need all of that high structure

00:15:16,800 --> 00:15:20,800
that for example elastic surgical lucine

00:15:18,800 --> 00:15:21,120
would expect and it will store the data

00:15:20,800 --> 00:15:24,240
then

00:15:21,120 --> 00:15:25,600
in an index which allows

00:15:24,240 --> 00:15:28,079
easy access in this kind of like the

00:15:25,600 --> 00:15:30,800
metadata around it and then chunks

00:15:28,079 --> 00:15:31,360
which are like the actual message that

00:15:30,800 --> 00:15:33,279
you have

00:15:31,360 --> 00:15:35,279
in there and those chunks can for

00:15:33,279 --> 00:15:38,399
example be stored on very cheap

00:15:35,279 --> 00:15:40,160
object stores like amazon s3 and then

00:15:38,399 --> 00:15:42,639
with the help of that

00:15:40,160 --> 00:15:43,600
index finding the right chunks that you

00:15:42,639 --> 00:15:46,079
want to search

00:15:43,600 --> 00:15:47,600
um you can query that and by taking

00:15:46,079 --> 00:15:50,240
these right trade-offs

00:15:47,600 --> 00:15:52,320
having chunks in the indices and

00:15:50,240 --> 00:15:55,519
potentially some result caching

00:15:52,320 --> 00:15:57,600
you can get very good results um while

00:15:55,519 --> 00:15:59,519
basically cutting some corners or

00:15:57,600 --> 00:16:01,759
avoiding some things that

00:15:59,519 --> 00:16:03,199
lucine is doing that are not strictly

00:16:01,759 --> 00:16:06,399
necessary for the

00:16:03,199 --> 00:16:10,560
the log use case so that what

00:16:06,399 --> 00:16:13,120
loki is doing is it has a key value hash

00:16:10,560 --> 00:16:14,639
that forms the so-called data stream id

00:16:13,120 --> 00:16:16,560
um so for example i'm saying my

00:16:14,639 --> 00:16:17,600
component is the printer and this is the

00:16:16,560 --> 00:16:19,920
specific location

00:16:17,600 --> 00:16:21,279
and i'm grouping the log level together

00:16:19,920 --> 00:16:24,160
so i take all of those

00:16:21,279 --> 00:16:25,920
three attributes i hash them together

00:16:24,160 --> 00:16:28,720
and that's the stream id and

00:16:25,920 --> 00:16:29,360
all log messages for this stream then

00:16:28,720 --> 00:16:32,639
end up

00:16:29,360 --> 00:16:35,440
in this chunk based on this id

00:16:32,639 --> 00:16:35,759
and then you take that chunk compress it

00:16:35,440 --> 00:16:38,000
and

00:16:35,759 --> 00:16:38,800
store it for example on an object store

00:16:38,000 --> 00:16:41,040
now if you

00:16:38,800 --> 00:16:42,720
change any one of these attributes you

00:16:41,040 --> 00:16:45,839
would get a different

00:16:42,720 --> 00:16:46,399
stream id and that would go to another

00:16:45,839 --> 00:16:49,440
chunk

00:16:46,399 --> 00:16:50,000
so if you know for example that you want

00:16:49,440 --> 00:16:52,000
to search

00:16:50,000 --> 00:16:53,199
for printers in one location and that

00:16:52,000 --> 00:16:56,160
specific error

00:16:53,199 --> 00:16:56,639
um the index will allow you to go to a

00:16:56,160 --> 00:16:59,440
chunk

00:16:56,639 --> 00:16:59,839
like a small subset of all the logs and

00:16:59,440 --> 00:17:02,480
then

00:16:59,839 --> 00:17:03,440
search through them you don't have the

00:17:02,480 --> 00:17:05,760
complicated

00:17:03,440 --> 00:17:06,880
index structure for all of the messages

00:17:05,760 --> 00:17:09,199
within that

00:17:06,880 --> 00:17:10,000
but you can pinpoint to that right

00:17:09,199 --> 00:17:11,919
subsection

00:17:10,000 --> 00:17:14,000
and then run a regular expression

00:17:11,919 --> 00:17:16,079
through that very efficiently

00:17:14,000 --> 00:17:18,559
so that's kind of a trade-off that loki

00:17:16,079 --> 00:17:22,000
is doing to keep search

00:17:18,559 --> 00:17:23,199
fast without having all of those index

00:17:22,000 --> 00:17:26,720
structures in the background

00:17:23,199 --> 00:17:30,320
by limiting it through to that stream id

00:17:26,720 --> 00:17:33,360
and basically grouping relevant sections

00:17:30,320 --> 00:17:35,360
of course the important part is that

00:17:33,360 --> 00:17:37,120
your searches are targeted enough that

00:17:35,360 --> 00:17:39,039
you don't have to search all the chunks

00:17:37,120 --> 00:17:40,960
but that you can find the right subset

00:17:39,039 --> 00:17:44,559
of chunks that you want to search

00:17:40,960 --> 00:17:48,000
to make the most out of them the queries

00:17:44,559 --> 00:17:51,039
look kind of familiar to what you had in

00:17:48,000 --> 00:17:54,400
grep so for example for my job

00:17:51,039 --> 00:17:57,440
i'm searching for all errors or in kafka

00:17:54,400 --> 00:18:00,720
i'm running a regular expression and

00:17:57,440 --> 00:18:04,160
search for ts db ops

00:18:00,720 --> 00:18:05,120
whatever i o colon 2003 or within

00:18:04,160 --> 00:18:07,600
cassandra

00:18:05,120 --> 00:18:09,840
i'm just searching for error equals

00:18:07,600 --> 00:18:13,440
whatever

00:18:09,840 --> 00:18:16,720
word i find after that so the syntax

00:18:13,440 --> 00:18:19,679
is rather similar to what you were

00:18:16,720 --> 00:18:20,320
used from grep already and then trying

00:18:19,679 --> 00:18:23,600
to take

00:18:20,320 --> 00:18:25,600
that centralizing the data

00:18:23,600 --> 00:18:26,640
limiting it down to kind of like the

00:18:25,600 --> 00:18:29,840
right subset

00:18:26,640 --> 00:18:32,000
and then running a grep through that

00:18:29,840 --> 00:18:33,679
subject section to avoid the pain points

00:18:32,000 --> 00:18:37,120
from what we had in grab earlier

00:18:33,679 --> 00:18:39,679
to make that work better again now

00:18:37,120 --> 00:18:41,440
the problem here is that it's optimized

00:18:39,679 --> 00:18:45,520
for this specific use case

00:18:41,440 --> 00:18:47,840
so if you have extracted the the wrong

00:18:45,520 --> 00:18:49,520
key value pairs for the stream id you

00:18:47,840 --> 00:18:50,160
will need to search a large amount of

00:18:49,520 --> 00:18:52,080
data

00:18:50,160 --> 00:18:53,200
which will be slower if you have like

00:18:52,080 --> 00:18:55,200
high network traffic

00:18:53,200 --> 00:18:57,520
um it might cost you more on a cloud

00:18:55,200 --> 00:18:58,080
provider so there are trade-offs for

00:18:57,520 --> 00:19:00,720
that

00:18:58,080 --> 00:19:02,160
but it is very optimized for that logs

00:19:00,720 --> 00:19:05,360
use case

00:19:02,160 --> 00:19:07,280
so where does that leave us are we

00:19:05,360 --> 00:19:09,120
at the end of this journey like

00:19:07,280 --> 00:19:11,520
everything is in the cloud we're done

00:19:09,120 --> 00:19:13,679
or maybe we're not done not really we're

00:19:11,520 --> 00:19:16,799
still kind of like on the way to

00:19:13,679 --> 00:19:17,360
to evolve that system so there are a

00:19:16,799 --> 00:19:20,160
couple of

00:19:17,360 --> 00:19:20,880
interesting things going on in parallel

00:19:20,160 --> 00:19:23,280
right now

00:19:20,880 --> 00:19:24,080
so one interesting project that has come

00:19:23,280 --> 00:19:27,200
up recently

00:19:24,080 --> 00:19:29,120
is lucien grep which is calling itself a

00:19:27,200 --> 00:19:29,840
greplight utility based on the scene

00:19:29,120 --> 00:19:32,320
monitor

00:19:29,840 --> 00:19:32,960
compiled with girl vm native image so

00:19:32,320 --> 00:19:36,640
you can

00:19:32,960 --> 00:19:39,120
invoke it very quickly from the shell

00:19:36,640 --> 00:19:39,919
and you don't have a long jbm starter

00:19:39,120 --> 00:19:43,679
time

00:19:39,919 --> 00:19:46,240
also the i the general idea is um or

00:19:43,679 --> 00:19:46,720
the author's idea here is that maybe you

00:19:46,240 --> 00:19:48,320
don't

00:19:46,720 --> 00:19:49,840
always want to write a regular

00:19:48,320 --> 00:19:50,720
expression because writing regular

00:19:49,840 --> 00:19:53,440
expressions

00:19:50,720 --> 00:19:54,960
can be a bit sometimes challenging or

00:19:53,440 --> 00:19:56,880
painful

00:19:54,960 --> 00:19:58,799
and there's always this old joke that

00:19:56,880 --> 00:20:01,360
what is the plural of regex

00:19:58,799 --> 00:20:03,039
it's regret because nobody can ever read

00:20:01,360 --> 00:20:04,720
their regular expressions again

00:20:03,039 --> 00:20:06,960
so maybe you don't want to write

00:20:04,720 --> 00:20:10,400
everything as a regular expression

00:20:06,960 --> 00:20:12,400
and you kind of want to marry the the

00:20:10,400 --> 00:20:14,400
power of lucine that you have

00:20:12,400 --> 00:20:16,400
tokenization and you maybe have

00:20:14,400 --> 00:20:18,559
stemming and some other advanced lucian

00:20:16,400 --> 00:20:21,120
concepts

00:20:18,559 --> 00:20:21,760
put those on top of the boolean query

00:20:21,120 --> 00:20:23,840
and then

00:20:21,760 --> 00:20:25,200
maybe still add the regular expression

00:20:23,840 --> 00:20:27,600
feature to that

00:20:25,200 --> 00:20:29,039
and have like that more powerful feature

00:20:27,600 --> 00:20:31,360
set combined

00:20:29,039 --> 00:20:32,320
with quick invocation times on the

00:20:31,360 --> 00:20:34,559
command line

00:20:32,320 --> 00:20:36,159
and you don't need that entire server

00:20:34,559 --> 00:20:36,720
infrastructure running to search your

00:20:36,159 --> 00:20:39,280
logs

00:20:36,720 --> 00:20:40,640
it's a very early or in a very early

00:20:39,280 --> 00:20:42,080
stage as a project

00:20:40,640 --> 00:20:43,919
but it is kind of an interesting

00:20:42,080 --> 00:20:46,720
approach where these different

00:20:43,919 --> 00:20:48,159
things try to come together in a new way

00:20:46,720 --> 00:20:50,559
to kind of avoid

00:20:48,159 --> 00:20:52,720
or to to avoid pain points that we had

00:20:50,559 --> 00:20:55,440
in the past

00:20:52,720 --> 00:20:57,120
another topic that is always swimming

00:20:55,440 --> 00:20:58,000
back and forth i would say is this more

00:20:57,120 --> 00:21:00,720
structure and

00:20:58,000 --> 00:21:02,159
and less structure so splunk also for

00:21:00,720 --> 00:21:04,159
performance reasons

00:21:02,159 --> 00:21:05,679
is adding more and more structured

00:21:04,159 --> 00:21:07,520
fields and i think some of their machine

00:21:05,679 --> 00:21:10,000
learning capabilities for example

00:21:07,520 --> 00:21:11,440
require a pretty high structure in their

00:21:10,000 --> 00:21:13,360
data to actually have

00:21:11,440 --> 00:21:15,039
a machine learning algorithm make the

00:21:13,360 --> 00:21:16,799
most out of that data

00:21:15,039 --> 00:21:18,799
and elasticsearch which comes from that

00:21:16,799 --> 00:21:19,280
highly structured site has recently

00:21:18,799 --> 00:21:22,240
added

00:21:19,280 --> 00:21:23,600
runtime fields where you can have a

00:21:22,240 --> 00:21:27,360
message and you can

00:21:23,600 --> 00:21:28,960
extract some parts like a virtual field

00:21:27,360 --> 00:21:31,600
basically at runtime

00:21:28,960 --> 00:21:32,799
so you could run a clock pattern or a

00:21:31,600 --> 00:21:35,760
regular expression

00:21:32,799 --> 00:21:36,240
to extract a specific part of a message

00:21:35,760 --> 00:21:37,679
a

00:21:36,240 --> 00:21:39,280
so you don't have to create all those

00:21:37,679 --> 00:21:42,080
index structures

00:21:39,280 --> 00:21:43,919
and b because sometimes you come up with

00:21:42,080 --> 00:21:45,919
or you figure out that your log pattern

00:21:43,919 --> 00:21:48,159
needs to extract some information

00:21:45,919 --> 00:21:49,520
that you didn't think about at first and

00:21:48,159 --> 00:21:52,400
that you want to extract

00:21:49,520 --> 00:21:53,200
later on remixing all that data in the

00:21:52,400 --> 00:21:54,960
right

00:21:53,200 --> 00:21:56,400
index structure would be very expensive

00:21:54,960 --> 00:21:57,360
so you might want to do that with a

00:21:56,400 --> 00:22:00,080
runtime field

00:21:57,360 --> 00:22:00,559
so it can take some pain to get started

00:22:00,080 --> 00:22:02,640
away

00:22:00,559 --> 00:22:03,919
and it fixes up potentially your data

00:22:02,640 --> 00:22:07,039
while keeping the cost

00:22:03,919 --> 00:22:08,880
maybe lower whereas splunk is trying to

00:22:07,039 --> 00:22:09,679
add more structure to take some of their

00:22:08,880 --> 00:22:12,960
speed

00:22:09,679 --> 00:22:14,240
and um around not having enough

00:22:12,960 --> 00:22:16,880
structure away

00:22:14,240 --> 00:22:18,080
so it's kind of like adding features

00:22:16,880 --> 00:22:20,720
from the other side

00:22:18,080 --> 00:22:21,919
to make the most out of that or to avoid

00:22:20,720 --> 00:22:25,039
the pain points that

00:22:21,919 --> 00:22:28,400
each system had in the past um

00:22:25,039 --> 00:22:31,200
another topic is for example features

00:22:28,400 --> 00:22:32,960
loki has in just in the most recent

00:22:31,200 --> 00:22:35,360
version in 2.2

00:22:32,960 --> 00:22:37,840
um added multi-line logs which was i

00:22:35,360 --> 00:22:40,240
think the most highly requested feature

00:22:37,840 --> 00:22:41,280
in loki because mud line logs are a

00:22:40,240 --> 00:22:44,000
thing that

00:22:41,280 --> 00:22:46,000
happened quite frequently and

00:22:44,000 --> 00:22:48,000
elasticsearch at the same time has

00:22:46,000 --> 00:22:49,919
a lot of the features and can do i don't

00:22:48,000 --> 00:22:50,720
want to say everything but can do many

00:22:49,919 --> 00:22:52,720
things

00:22:50,720 --> 00:22:54,559
there it's more about making it more log

00:22:52,720 --> 00:22:56,159
specific and getting away from that

00:22:54,559 --> 00:22:57,840
general purpose data store

00:22:56,159 --> 00:23:00,000
and having more optimized data

00:22:57,840 --> 00:23:01,520
structures for example for logs

00:23:00,000 --> 00:23:03,679
so for example elasticsearch has

00:23:01,520 --> 00:23:05,760
recently added a wildcard field which

00:23:03,679 --> 00:23:09,520
allows more performant wildcards or

00:23:05,760 --> 00:23:11,440
prepped like or grep queries

00:23:09,520 --> 00:23:13,039
it has recently added searchable

00:23:11,440 --> 00:23:13,919
snapshots which basically takes the

00:23:13,039 --> 00:23:16,400
snapshot the

00:23:13,919 --> 00:23:17,440
backup which can be stored on an object

00:23:16,400 --> 00:23:20,320
store and can

00:23:17,440 --> 00:23:21,679
mount them to let elasticsearch actually

00:23:20,320 --> 00:23:24,559
search that data

00:23:21,679 --> 00:23:26,480
so you can also move data to an object

00:23:24,559 --> 00:23:29,600
store and don't need to keep it

00:23:26,480 --> 00:23:31,760
on a hot running node anymore but you

00:23:29,600 --> 00:23:34,480
can search in object store

00:23:31,760 --> 00:23:35,280
or match on the text field that one is

00:23:34,480 --> 00:23:38,400
pretty clever

00:23:35,280 --> 00:23:40,640
so coming from the full text search side

00:23:38,400 --> 00:23:42,159
um what you want in full text search is

00:23:40,640 --> 00:23:43,679
you will always want to search be able

00:23:42,159 --> 00:23:45,760
to search for phrases

00:23:43,679 --> 00:23:47,520
and you want to have that scored the

00:23:45,760 --> 00:23:49,760
relevancy like how relevant

00:23:47,520 --> 00:23:51,600
is what i'm searching for how relevant

00:23:49,760 --> 00:23:54,640
are these different documents

00:23:51,600 --> 00:23:56,480
in logs you normally don't have that

00:23:54,640 --> 00:23:58,240
much relevancy like you don't

00:23:56,480 --> 00:24:00,159
really care if that log line is so much

00:23:58,240 --> 00:24:00,640
more relevant than that other log line

00:24:00,159 --> 00:24:02,159
it's like

00:24:00,640 --> 00:24:04,480
they contain the error message that i'm

00:24:02,159 --> 00:24:05,520
looking for i want those i don't care

00:24:04,480 --> 00:24:08,320
about that score

00:24:05,520 --> 00:24:09,039
so match only text requires less disk

00:24:08,320 --> 00:24:10,880
space

00:24:09,039 --> 00:24:12,559
by not extracting all of that

00:24:10,880 --> 00:24:13,200
information and storing it in the index

00:24:12,559 --> 00:24:15,520
structures

00:24:13,200 --> 00:24:16,320
to make it more performant and cheaper

00:24:15,520 --> 00:24:19,600
to run

00:24:16,320 --> 00:24:20,559
for the log specific use case so there

00:24:19,600 --> 00:24:23,840
it's

00:24:20,559 --> 00:24:24,159
competition is keeping you sharp to make

00:24:23,840 --> 00:24:26,080
you

00:24:24,159 --> 00:24:27,919
move in the right direction that's

00:24:26,080 --> 00:24:30,000
really what is happening here

00:24:27,919 --> 00:24:31,039
it's either adding or broadening the

00:24:30,000 --> 00:24:33,440
feature set or

00:24:31,039 --> 00:24:35,360
sharpening the specific use case and

00:24:33,440 --> 00:24:38,559
what is in there

00:24:35,360 --> 00:24:41,440
so to wrap this up

00:24:38,559 --> 00:24:42,080
there is this um nice quote from edward

00:24:41,440 --> 00:24:44,480
stemming

00:24:42,080 --> 00:24:46,000
um true or not there's some debate about

00:24:44,480 --> 00:24:48,240
that but i still like it

00:24:46,000 --> 00:24:49,600
it is not necessary to change survival

00:24:48,240 --> 00:24:52,400
is not mandatory

00:24:49,600 --> 00:24:54,000
so as a system if you stop changing and

00:24:52,400 --> 00:24:55,600
adapting to the pain points and

00:24:54,000 --> 00:24:57,600
requirements

00:24:55,600 --> 00:24:59,279
either because ephemeral logs are

00:24:57,600 --> 00:25:01,600
becoming more and more

00:24:59,279 --> 00:25:02,559
common or because scale is becoming more

00:25:01,600 --> 00:25:04,960
and more common

00:25:02,559 --> 00:25:06,720
if your tooling cannot adapt to that

00:25:04,960 --> 00:25:08,159
maybe it's not made for the future or

00:25:06,720 --> 00:25:10,559
for these scenarios

00:25:08,159 --> 00:25:11,600
so while it's not necessary to adapt to

00:25:10,559 --> 00:25:13,279
that um

00:25:11,600 --> 00:25:16,400
if you want to survive as a tool you

00:25:13,279 --> 00:25:18,720
probably still have to do that

00:25:16,400 --> 00:25:20,159
the one question that i'm maybe

00:25:18,720 --> 00:25:21,520
expecting now is

00:25:20,159 --> 00:25:23,279
what is the benchmark between the

00:25:21,520 --> 00:25:27,120
different tools and show me

00:25:23,279 --> 00:25:29,440
how a is much more or much faster than b

00:25:27,120 --> 00:25:30,880
and i'm afraid there is no good way to

00:25:29,440 --> 00:25:33,279
benchmark that

00:25:30,880 --> 00:25:34,240
my favorite comic is here about the two

00:25:33,279 --> 00:25:36,960
different or

00:25:34,240 --> 00:25:38,400
two systems that are benchmarked on

00:25:36,960 --> 00:25:40,000
under similar conditions

00:25:38,400 --> 00:25:42,159
and one is much better than the other

00:25:40,000 --> 00:25:44,480
and you can see under similar conditions

00:25:42,159 --> 00:25:45,520
the house camp has been killed and the

00:25:44,480 --> 00:25:48,000
squid is

00:25:45,520 --> 00:25:49,200
thriving so even though this conditions

00:25:48,000 --> 00:25:52,400
might be the same

00:25:49,200 --> 00:25:54,320
maybe those are not your conditions so

00:25:52,400 --> 00:25:56,799
while these vendor benchmarks or

00:25:54,320 --> 00:25:59,200
so-called bench marketing can always be

00:25:56,799 --> 00:26:00,159
fun it is not necessarily helpful for

00:25:59,200 --> 00:26:02,159
you because

00:26:00,159 --> 00:26:04,799
you potentially need a specific feature

00:26:02,159 --> 00:26:06,320
set or you have specific requirements in

00:26:04,799 --> 00:26:08,799
what hardware you have

00:26:06,320 --> 00:26:10,640
and what your read and write ratio is or

00:26:08,799 --> 00:26:11,679
what specific types of query you're

00:26:10,640 --> 00:26:13,120
actually running

00:26:11,679 --> 00:26:15,120
and if you change any of these

00:26:13,120 --> 00:26:16,000
parameters it changes how all the

00:26:15,120 --> 00:26:18,240
systems behave

00:26:16,000 --> 00:26:19,919
in comparison to each other so while you

00:26:18,240 --> 00:26:22,080
can always do benchmarks

00:26:19,919 --> 00:26:24,320
for yourself and your use case and that

00:26:22,080 --> 00:26:26,320
totally makes sense

00:26:24,320 --> 00:26:28,480
as a vendor you cannot do that so i will

00:26:26,320 --> 00:26:30,640
not provide any benchmarks here

00:26:28,480 --> 00:26:32,320
so of course you should always benchmark

00:26:30,640 --> 00:26:33,200
your own tools to make sure that they

00:26:32,320 --> 00:26:35,679
don't get

00:26:33,200 --> 00:26:36,480
slower or worse over time like the slow

00:26:35,679 --> 00:26:40,080
boiling frog

00:26:36,480 --> 00:26:42,159
problem so benchmark yourself strictly

00:26:40,080 --> 00:26:44,480
um but vendor benchmarking against

00:26:42,159 --> 00:26:46,320
competitors can be informative to find

00:26:44,480 --> 00:26:47,919
gaps and improve them

00:26:46,320 --> 00:26:49,760
but it's not what you should put out

00:26:47,919 --> 00:26:51,600
because probably nobody has exactly the

00:26:49,760 --> 00:26:54,080
same scenario that you were benchmarking

00:26:51,600 --> 00:26:56,640
in the first place

00:26:54,080 --> 00:26:57,840
to wrap it up let's have a discussion

00:26:56,640 --> 00:27:00,799
about features

00:26:57,840 --> 00:27:02,720
speed and cost and what is the right

00:27:00,799 --> 00:27:05,679
trade-off for you

00:27:02,720 --> 00:27:06,799
and where should tools be headed thanks

00:27:05,679 --> 00:27:08,640
so much for joining

00:27:06,799 --> 00:27:10,159
i hope you learned something if you have

00:27:08,640 --> 00:27:12,240
any feedback let me know

00:27:10,159 --> 00:27:21,840
and let's hop over to the discussion

00:27:12,240 --> 00:27:21,840
thanks a lot for joining

00:27:34,399 --> 00:27:36,480

YouTube URL: https://www.youtube.com/watch?v=1ERZ1tjLVoo


