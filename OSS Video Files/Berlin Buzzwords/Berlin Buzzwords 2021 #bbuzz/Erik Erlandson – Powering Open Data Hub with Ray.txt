Title: Erik Erlandson – Powering Open Data Hub with Ray
Publication date: 2021-06-30
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Ray is quickly gaining momentum as a distributed computing platform that combines a powerful parallel compute model with a cloud native serverless-style scaling model. Open Data Hub (ODH) is a flexible and customizable federation of open source data science tools that is a great fit for taking advantage of Ray compute clusters.

In this talk, Erik will explain how to integrate Ray with Open Data Hub, by configuring ODH profiles that deploy on-demand Ray clusters for Jupyter notebooks. He’ll demonstrate Ray in action as a scalable compute resource for ODH, and explore the potential use cases opened up by self-service notebooks backed by Ray distributed computing. Along the way he’ll also discuss the logistics of adapting Ray to OpenShift’s security features.

Attendees will learn how Ray integrates with Open Data Hub’s architecture, and how they can power ODH with Ray to solve distributed computing problems in the popular Jupyter environment.

Speaker: 
Erik Erlandson – https://2021.berlinbuzzwords.de/member/erik-erlandson

More: https://2021.berlinbuzzwords.de/session/powering-open-data-hub-ray
Captions: 
	00:00:07,200 --> 00:00:10,160
um

00:00:07,759 --> 00:00:11,360
thanks everybody for attending i know a

00:00:10,160 --> 00:00:13,360
lot of you are

00:00:11,360 --> 00:00:15,040
dealing with a time zone issue so i

00:00:13,360 --> 00:00:17,440
appreciate the extra effort

00:00:15,040 --> 00:00:18,720
i'm eric rollinson and i work at red hat

00:00:17,440 --> 00:00:21,520
and i

00:00:18,720 --> 00:00:22,720
work in emerging technologies for uh the

00:00:21,520 --> 00:00:24,960
intersection of the

00:00:22,720 --> 00:00:26,080
machine learning ecosystem with the

00:00:24,960 --> 00:00:29,119
kubernetes

00:00:26,080 --> 00:00:30,800
ecosystem so today

00:00:29,119 --> 00:00:33,040
i want to talk to you about a quote

00:00:30,800 --> 00:00:35,050
unquote powering open data hub

00:00:33,040 --> 00:00:38,140
with ray

00:00:35,050 --> 00:00:38,140
[Music]

00:00:39,200 --> 00:00:47,680
another possible title the talk could be

00:00:43,600 --> 00:00:47,680
using jupiter and ray in the cloud

00:00:48,879 --> 00:00:51,920
so the uh sort of road map for the talk

00:00:51,520 --> 00:00:53,840
is

00:00:51,920 --> 00:00:55,520
i'm going to talk a little about ray at

00:00:53,840 --> 00:00:57,600
10 000 meters

00:00:55,520 --> 00:00:59,120
and then i'm going to sort of like place

00:00:57,600 --> 00:01:02,160
jupiter and open

00:00:59,120 --> 00:01:03,920
data hub and in context for the talk and

00:01:02,160 --> 00:01:04,320
then talk about sort of the architecture

00:01:03,920 --> 00:01:07,360
of

00:01:04,320 --> 00:01:09,840
what i did to get ray working on odh

00:01:07,360 --> 00:01:11,439
and then follow that with an actual demo

00:01:09,840 --> 00:01:12,080
of the architecture and then i want to

00:01:11,439 --> 00:01:14,240
close with

00:01:12,080 --> 00:01:16,960
some of the collaborations that made

00:01:14,240 --> 00:01:20,640
this work possible

00:01:16,960 --> 00:01:21,600
so um the design goal for ray was to

00:01:20,640 --> 00:01:24,799
occupy

00:01:21,600 --> 00:01:28,080
a sort of niche and ecosystem where

00:01:24,799 --> 00:01:29,439
it's a higher level and better

00:01:28,080 --> 00:01:32,159
abstraction than say

00:01:29,439 --> 00:01:33,520
raw mpi for parallel programming but

00:01:32,159 --> 00:01:36,000
also

00:01:33,520 --> 00:01:37,920
allow a slightly lower level and more

00:01:36,000 --> 00:01:40,079
flexible

00:01:37,920 --> 00:01:42,240
series of computational representations

00:01:40,079 --> 00:01:43,840
than apache spark

00:01:42,240 --> 00:01:46,399
so it wants to sort of live in the

00:01:43,840 --> 00:01:46,399
middle there

00:01:47,119 --> 00:01:53,600
its compute model is consisted of

00:01:50,159 --> 00:01:56,479
tasks and actors and

00:01:53,600 --> 00:01:56,960
there's a nice parallelism with python

00:01:56,479 --> 00:02:00,960
here

00:01:56,960 --> 00:02:01,680
a task is simply analogous to a python

00:02:00,960 --> 00:02:04,799
function

00:02:01,680 --> 00:02:08,160
and actors are direct analogs of

00:02:04,799 --> 00:02:10,160
python classes and the programming

00:02:08,160 --> 00:02:10,720
ergonomics are really very easy you can

00:02:10,160 --> 00:02:14,160
take

00:02:10,720 --> 00:02:15,200
these python definitions and uh decorate

00:02:14,160 --> 00:02:18,400
them with ray

00:02:15,200 --> 00:02:23,599
dot remote and ray will then know how to

00:02:18,400 --> 00:02:26,959
execute them out on its cluster

00:02:23,599 --> 00:02:30,560
um ray operates by

00:02:26,959 --> 00:02:32,720
allowing you to set up uh compute dags

00:02:30,560 --> 00:02:34,239
um like on the left here you can see

00:02:32,720 --> 00:02:36,720
they uh

00:02:34,239 --> 00:02:37,599
sort of highly over engineered way of

00:02:36,720 --> 00:02:40,239
setting up

00:02:37,599 --> 00:02:41,920
summing the numbers one through eight

00:02:40,239 --> 00:02:45,040
you can see that you

00:02:41,920 --> 00:02:46,959
create a bunch of add calls using the

00:02:45,040 --> 00:02:48,879
dot remote

00:02:46,959 --> 00:02:50,480
ray decoration function it's

00:02:48,879 --> 00:02:54,879
automatically provided to you

00:02:50,480 --> 00:02:58,159
if you use ray dot remote

00:02:54,879 --> 00:03:01,440
and you're building up sort of like a

00:02:58,159 --> 00:03:04,959
computation now i want to talk about

00:03:01,440 --> 00:03:06,800
this part at the end here array.get

00:03:04,959 --> 00:03:08,840
ray has something in common with spark

00:03:06,800 --> 00:03:10,080
which is that it has sort of a lazy

00:03:08,840 --> 00:03:12,640
declarative

00:03:10,080 --> 00:03:13,519
compute model and so the first the first

00:03:12,640 --> 00:03:15,440
seven

00:03:13,519 --> 00:03:17,440
lines up there are just setting up a

00:03:15,440 --> 00:03:20,319
computation nothing at all actually

00:03:17,440 --> 00:03:23,040
happens until you say ray.get

00:03:20,319 --> 00:03:24,400
so it's a again allows like spark it

00:03:23,040 --> 00:03:27,040
allows ray to

00:03:24,400 --> 00:03:30,159
sort of decide for you how best to run

00:03:27,040 --> 00:03:32,959
the compute that you ask it to

00:03:30,159 --> 00:03:35,440
i took these diagrams from a great blog

00:03:32,959 --> 00:03:37,360
post by uh robert nishihara um the link

00:03:35,440 --> 00:03:38,799
there is the bottom if you

00:03:37,360 --> 00:03:42,799
recommend it if you want to read more

00:03:38,799 --> 00:03:42,799
about how ray does its job

00:03:43,680 --> 00:03:51,040
um so ray's primary data model

00:03:47,920 --> 00:03:54,400
is um the plasma object store

00:03:51,040 --> 00:03:57,360
um and it is a distributed object store

00:03:54,400 --> 00:03:59,200
it's like most data structures uh in

00:03:57,360 --> 00:04:02,879
python it's just fundamentally

00:03:59,200 --> 00:04:05,920
typeless and schema-less

00:04:02,879 --> 00:04:07,840
the the store prefers to operate local

00:04:05,920 --> 00:04:09,439
first and what that means is it'll pull

00:04:07,840 --> 00:04:11,120
remote data

00:04:09,439 --> 00:04:12,879
only if it needs to and otherwise it

00:04:11,120 --> 00:04:15,680
will always prefer to

00:04:12,879 --> 00:04:17,519
get its data locally on running on ray

00:04:15,680 --> 00:04:20,079
workers

00:04:17,519 --> 00:04:22,400
so most read write is local to worker

00:04:20,079 --> 00:04:22,400
nodes

00:04:23,680 --> 00:04:30,000
similarly ray's work scheduling model is

00:04:26,639 --> 00:04:33,280
local first so it will always prefer to

00:04:30,000 --> 00:04:34,960
run a compute job on the work node that

00:04:33,280 --> 00:04:37,280
is already running

00:04:34,960 --> 00:04:39,360
otherwise only if it has to we'll take

00:04:37,280 --> 00:04:41,199
work elements and push them back to the

00:04:39,360 --> 00:04:43,040
global scheduler to get rescheduled

00:04:41,199 --> 00:04:44,880
somewhere else and so

00:04:43,040 --> 00:04:48,639
anyways local first principle allows

00:04:44,880 --> 00:04:48,639
rate operate fairly efficiently

00:04:49,360 --> 00:04:56,160
so to set up the context

00:04:52,560 --> 00:04:58,080
for you know why rey with jupiter

00:04:56,160 --> 00:05:00,240
um i just want to talk a little bit

00:04:58,080 --> 00:05:02,560
about the library ecosystem around ray

00:05:00,240 --> 00:05:03,840
um out of the box it comes with hyper

00:05:02,560 --> 00:05:06,960
parameter tuning

00:05:03,840 --> 00:05:10,240
um a reinforcement learning package

00:05:06,960 --> 00:05:13,600
a basic stochastic gradient descent

00:05:10,240 --> 00:05:16,800
um and ray serve for basically doing

00:05:13,600 --> 00:05:18,800
model serving uh an array cluster

00:05:16,800 --> 00:05:20,560
and then in addition there is a large

00:05:18,800 --> 00:05:23,520
ecosystem of community

00:05:20,560 --> 00:05:26,240
integrations most of the packages you'd

00:05:23,520 --> 00:05:30,240
be familiar with in ml space are there

00:05:26,240 --> 00:05:33,039
xgboost desk horvath sklearn

00:05:30,240 --> 00:05:34,000
um almost everything at this point has

00:05:33,039 --> 00:05:36,240
got some kind of

00:05:34,000 --> 00:05:38,479
great integration the link at the bottom

00:05:36,240 --> 00:05:41,199
you can look at the full list of

00:05:38,479 --> 00:05:41,199
integrations

00:05:41,680 --> 00:05:46,320
so if you look at all these things

00:05:44,800 --> 00:05:48,240
these are of course all packages that

00:05:46,320 --> 00:05:49,440
data scientists have been using with

00:05:48,240 --> 00:05:52,320
jupiter

00:05:49,440 --> 00:05:53,600
for quite a while now and so when you

00:05:52,320 --> 00:05:56,639
look at it that way

00:05:53,600 --> 00:05:57,039
it sort of begs for ray to be driven

00:05:56,639 --> 00:05:59,759
from

00:05:57,039 --> 00:06:03,280
jupiter notebooks to use these packages

00:05:59,759 --> 00:06:03,280
accelerated with ray compute

00:06:04,000 --> 00:06:07,520
and additionally you know it gives you

00:06:06,080 --> 00:06:09,600
the promise of

00:06:07,520 --> 00:06:12,160
literary literate programming and

00:06:09,600 --> 00:06:15,600
interactive programming with ray using

00:06:12,160 --> 00:06:18,479
jupiter's environment um

00:06:15,600 --> 00:06:20,800
and more specifically um you know

00:06:18,479 --> 00:06:22,800
getting this environment as

00:06:20,800 --> 00:06:25,120
automatically provided to you with a

00:06:22,800 --> 00:06:27,680
cloud deployment and in my case

00:06:25,120 --> 00:06:30,560
today i'm talking about the kubernetes

00:06:27,680 --> 00:06:32,720
container orchestration platform

00:06:30,560 --> 00:06:34,080
and uh more specifically the flavor of

00:06:32,720 --> 00:06:37,280
kubernetes i'm using

00:06:34,080 --> 00:06:37,280
is a open shift

00:06:38,960 --> 00:06:43,680
so when i first embarked on this uh

00:06:41,919 --> 00:06:46,000
study

00:06:43,680 --> 00:06:46,880
the primary way of connecting to a ray

00:06:46,000 --> 00:06:49,440
cluster was a

00:06:46,880 --> 00:06:51,840
the ray.init function and this had an

00:06:49,440 --> 00:06:54,000
interesting property where it only works

00:06:51,840 --> 00:06:55,039
if you're connecting on the physical

00:06:54,000 --> 00:06:57,919
node

00:06:55,039 --> 00:06:59,599
that the ray head node is running on and

00:06:57,919 --> 00:07:03,120
so you look over on the left

00:06:59,599 --> 00:07:04,319
to use jupiter you know in something

00:07:03,120 --> 00:07:05,520
like kubernetes

00:07:04,319 --> 00:07:07,199
uh with ray you'd actually have to

00:07:05,520 --> 00:07:09,680
create a single pod that has both

00:07:07,199 --> 00:07:12,080
jupiter running and the right head node

00:07:09,680 --> 00:07:14,000
um of course architecturally this is not

00:07:12,080 --> 00:07:15,039
very appealing it's not good separation

00:07:14,000 --> 00:07:18,000
of concerns

00:07:15,039 --> 00:07:20,000
um and the logistics of actually

00:07:18,000 --> 00:07:20,319
installing and running jupiter and ray

00:07:20,000 --> 00:07:24,319
are

00:07:20,319 --> 00:07:26,880
just very awkward to deal with

00:07:24,319 --> 00:07:27,599
however after i reached out to the array

00:07:26,880 --> 00:07:29,599
community

00:07:27,599 --> 00:07:31,599
and it turns out they've been also

00:07:29,599 --> 00:07:34,880
creating a new way to connect

00:07:31,599 --> 00:07:36,400
essentially a true client server

00:07:34,880 --> 00:07:38,639
connection where

00:07:36,400 --> 00:07:40,560
anything on a network visible to the

00:07:38,639 --> 00:07:42,639
right head pod can connect to it using

00:07:40,560 --> 00:07:44,479
the array connect function

00:07:42,639 --> 00:07:45,919
so this is a total game changer because

00:07:44,479 --> 00:07:49,039
allows much better

00:07:45,919 --> 00:07:51,280
cloud native architectures and uh

00:07:49,039 --> 00:07:53,280
not just jupiter but other you know

00:07:51,280 --> 00:07:55,039
applications so running you know in the

00:07:53,280 --> 00:07:55,599
cluster can actually connect hooray and

00:07:55,039 --> 00:07:59,680
use it

00:07:55,599 --> 00:08:02,319
as a resource

00:07:59,680 --> 00:08:03,039
so as i mentioned um you know i chose

00:08:02,319 --> 00:08:07,680
that

00:08:03,039 --> 00:08:11,039
that i wanted to try to consume jupiter

00:08:07,680 --> 00:08:12,400
through the open data hub project

00:08:11,039 --> 00:08:14,800
and i want to talk a little bit about

00:08:12,400 --> 00:08:17,520
what that means um so

00:08:14,800 --> 00:08:18,560
what is open data hub um you know

00:08:17,520 --> 00:08:20,400
firstly

00:08:18,560 --> 00:08:22,800
open data hub is an open source

00:08:20,400 --> 00:08:26,000
downstream of kubeflow

00:08:22,800 --> 00:08:28,560
with a few modifications uh

00:08:26,000 --> 00:08:30,720
tuning it for running on openshift and

00:08:28,560 --> 00:08:32,800
other aspects

00:08:30,720 --> 00:08:34,240
um it operates as a sort of reference

00:08:32,800 --> 00:08:37,039
platform for

00:08:34,240 --> 00:08:37,599
using open source machine learning

00:08:37,039 --> 00:08:41,360
tooling

00:08:37,599 --> 00:08:42,959
in the cloud and it's fairly federated

00:08:41,360 --> 00:08:46,000
which means that these

00:08:42,959 --> 00:08:46,959
projects um are relatively loosely

00:08:46,000 --> 00:08:50,640
integrated

00:08:46,959 --> 00:08:51,839
and um while you get you lose something

00:08:50,640 --> 00:08:53,839
a little bit you know you lose

00:08:51,839 --> 00:08:55,839
possibilities of tight tight integration

00:08:53,839 --> 00:08:57,360
however it makes for a very very

00:08:55,839 --> 00:08:59,200
flexible

00:08:57,360 --> 00:09:00,959
environment and in fact it's the

00:08:59,200 --> 00:09:02,160
flexibility here that allowed me to very

00:09:00,959 --> 00:09:04,800
easily

00:09:02,160 --> 00:09:07,120
run you know ray and integrate it with

00:09:04,800 --> 00:09:09,920
odh

00:09:07,120 --> 00:09:10,800
um the components you can get um do a

00:09:09,920 --> 00:09:13,200
pretty good job

00:09:10,800 --> 00:09:15,519
covering like both the different uh

00:09:13,200 --> 00:09:16,480
stages of a typical machine learning

00:09:15,519 --> 00:09:18,959
workflow

00:09:16,480 --> 00:09:20,720
for cloud native development and they

00:09:18,959 --> 00:09:23,440
also do a pretty good job of covering

00:09:20,720 --> 00:09:25,279
all the different persona

00:09:23,440 --> 00:09:27,120
not just data scientists obviously but

00:09:25,279 --> 00:09:30,880
business stakeholders

00:09:27,120 --> 00:09:32,399
you know app developers and i t

00:09:30,880 --> 00:09:34,720
of course jupiter is at the center of

00:09:32,399 --> 00:09:34,720
all that

00:09:34,959 --> 00:09:39,839
um red hat actually uses its own

00:09:38,000 --> 00:09:41,200
internal deployment of open data hub

00:09:39,839 --> 00:09:43,760
i've done various

00:09:41,200 --> 00:09:45,680
projects including you know clustering

00:09:43,760 --> 00:09:47,920
operational metrics from openshift

00:09:45,680 --> 00:09:51,360
clusters

00:09:47,920 --> 00:09:54,000
analyzing customer support data and

00:09:51,360 --> 00:09:56,320
doing anomaly detection on application

00:09:54,000 --> 00:09:56,320
logs

00:09:56,800 --> 00:10:01,760
so my job was made easier by the fact

00:09:59,760 --> 00:10:02,560
that i had a sort of analogy to work

00:10:01,760 --> 00:10:04,640
from

00:10:02,560 --> 00:10:08,160
there's already a spark integration for

00:10:04,640 --> 00:10:10,399
odh and it works like this if you

00:10:08,160 --> 00:10:11,680
bring up the open data hub jupiter hub

00:10:10,399 --> 00:10:14,800
launcher

00:10:11,680 --> 00:10:16,640
and you pick a spark enabled image

00:10:14,800 --> 00:10:18,480
um the first thing it will do as always

00:10:16,640 --> 00:10:20,640
is give you a jupiter environment

00:10:18,480 --> 00:10:22,800
but it will also go out and look for a

00:10:20,640 --> 00:10:23,600
uh what's called a single user profile

00:10:22,800 --> 00:10:26,720
and i'll talk

00:10:23,600 --> 00:10:27,440
more about that in a second which tells

00:10:26,720 --> 00:10:30,320
ray

00:10:27,440 --> 00:10:33,839
how to spin up things like spark

00:10:30,320 --> 00:10:36,560
clusters to go with your notebooks

00:10:33,839 --> 00:10:38,480
and it in turn goes and references a

00:10:36,560 --> 00:10:39,920
service template which basically gives

00:10:38,480 --> 00:10:42,399
you all the yaml

00:10:39,920 --> 00:10:44,160
for things running in kubernetes or

00:10:42,399 --> 00:10:46,560
openshift

00:10:44,160 --> 00:10:47,760
and using that information it then

00:10:46,560 --> 00:10:50,320
creates those objects

00:10:47,760 --> 00:10:51,760
and spins you a little spark cluster and

00:10:50,320 --> 00:10:53,519
once you have that

00:10:51,760 --> 00:10:55,200
you're working in jupiter and you can

00:10:53,519 --> 00:10:55,920
simply connect to that so you have your

00:10:55,200 --> 00:10:59,040
own

00:10:55,920 --> 00:11:00,640
self-service personalized data science

00:10:59,040 --> 00:11:05,360
with spark backing you

00:11:00,640 --> 00:11:08,240
um so at a high level um

00:11:05,360 --> 00:11:10,160
you know single user profiles and

00:11:08,240 --> 00:11:13,519
service templates are nothing but

00:11:10,160 --> 00:11:16,640
kubernetes config maps there's data

00:11:13,519 --> 00:11:17,279
and so it obviously you know begs for an

00:11:16,640 --> 00:11:20,720
attempt

00:11:17,279 --> 00:11:24,079
to replace these with

00:11:20,720 --> 00:11:25,519
you know config maps that tell odh how

00:11:24,079 --> 00:11:28,720
to spin up a ray cluster

00:11:25,519 --> 00:11:28,720
and connect to that

00:11:29,120 --> 00:11:33,839
and so spoiler it turned out to be

00:11:32,000 --> 00:11:35,920
relatively easy to do

00:11:33,839 --> 00:11:37,839
so here's a single user profile you can

00:11:35,920 --> 00:11:38,959
see there's nothing real special here

00:11:37,839 --> 00:11:42,000
it's

00:11:38,959 --> 00:11:43,760
images and resource specifications so

00:11:42,000 --> 00:11:45,680
it's all fairly standard you know

00:11:43,760 --> 00:11:48,640
objects and parameters you'll find

00:11:45,680 --> 00:11:51,760
working with kubernetes

00:11:48,640 --> 00:11:54,480
and likewise the service template

00:11:51,760 --> 00:11:56,320
is just showing you know odh how to

00:11:54,480 --> 00:11:59,839
initiate a ray cluster

00:11:56,320 --> 00:12:02,240
custom resource which the ray operator

00:11:59,839 --> 00:12:04,079
knows how to use to spin clusters

00:12:02,240 --> 00:12:05,920
these are very very large objects with

00:12:04,079 --> 00:12:07,360
many parameters so i'm not going to show

00:12:05,920 --> 00:12:09,920
them all here but

00:12:07,360 --> 00:12:11,519
you can see a few at the top again this

00:12:09,920 --> 00:12:15,120
is just

00:12:11,519 --> 00:12:17,519
standard non-magical kubernetes type

00:12:15,120 --> 00:12:17,519
ammo

00:12:18,399 --> 00:12:23,279
and so with that i will now actually

00:12:21,600 --> 00:12:25,839
run the run an example of this

00:12:23,279 --> 00:12:25,839
architecture

00:12:26,800 --> 00:12:30,959
first i've already logged into the

00:12:29,040 --> 00:12:35,040
cluster

00:12:30,959 --> 00:12:36,959
and you can see when you log in you get

00:12:35,040 --> 00:12:38,399
a dashboard it looks like this and

00:12:36,959 --> 00:12:41,760
you can see over on the left is jupiter

00:12:38,399 --> 00:12:43,279
hub so if you launch that

00:12:41,760 --> 00:12:44,880
it takes a little while so i'm not going

00:12:43,279 --> 00:12:48,160
to do it for you now if you launch that

00:12:44,880 --> 00:12:50,959
and choose a ray enabled image

00:12:48,160 --> 00:12:51,680
you can see that it is spun up not just

00:12:50,959 --> 00:12:53,519
a

00:12:51,680 --> 00:12:56,079
jupiter hub environment up here but it

00:12:53,519 --> 00:12:59,839
is spun up a little ray cluster for me

00:12:56,079 --> 00:12:59,839
here's the head node running in a pod

00:13:00,959 --> 00:13:06,560
it also automatically produces

00:13:04,560 --> 00:13:08,079
a little service and route that allows

00:13:06,560 --> 00:13:10,560
you to view the

00:13:08,079 --> 00:13:14,320
ray dashboard which is kind of similar

00:13:10,560 --> 00:13:14,320
to like think the spark dashboard

00:13:15,360 --> 00:13:20,079
so if we go and look at a notebook let's

00:13:18,480 --> 00:13:23,519
imagine we want to do some data science

00:13:20,079 --> 00:13:26,000
with ray um and again see i as i

00:13:23,519 --> 00:13:26,560
will add comments here uh leveraging the

00:13:26,000 --> 00:13:29,920
full

00:13:26,560 --> 00:13:32,399
literate jupiter environment

00:13:29,920 --> 00:13:34,320
so of course these uh create some images

00:13:32,399 --> 00:13:36,240
that have the ray dependencies

00:13:34,320 --> 00:13:38,399
pre-installed so i can simply import

00:13:36,240 --> 00:13:38,399
them

00:13:39,519 --> 00:13:45,120
i told the jupiter hub launcher how to

00:13:43,360 --> 00:13:45,760
give it an environment variable here

00:13:45,120 --> 00:13:47,680
which has

00:13:45,760 --> 00:13:49,440
the actual name of the array cluster

00:13:47,680 --> 00:13:51,279
that it spun up and so i can just use

00:13:49,440 --> 00:13:52,800
that to connect

00:13:51,279 --> 00:13:54,240
and so here you can see i'm testing to

00:13:52,800 --> 00:13:55,600
see if i'm already connected because as

00:13:54,240 --> 00:13:58,160
we know in jupiter

00:13:55,600 --> 00:13:59,600
you can run cells more than once and you

00:13:58,160 --> 00:14:00,880
don't want to like try to reconnect

00:13:59,600 --> 00:14:02,880
twice so ray

00:14:00,880 --> 00:14:04,720
doesn't really like that and so here we

00:14:02,880 --> 00:14:05,519
can see giving me a little information

00:14:04,720 --> 00:14:08,880
about the

00:14:05,519 --> 00:14:10,720
head node that i connected to

00:14:08,880 --> 00:14:12,399
i'm going to be doing an xg boost

00:14:10,720 --> 00:14:15,440
example today and

00:14:12,399 --> 00:14:16,320
there's an xg boost ray integration you

00:14:15,440 --> 00:14:18,800
can see here

00:14:16,320 --> 00:14:20,240
and it just provides some drop-in

00:14:18,800 --> 00:14:21,920
replacements

00:14:20,240 --> 00:14:23,279
that work just like all the normal xg

00:14:21,920 --> 00:14:26,399
boost objects but

00:14:23,279 --> 00:14:29,040
are able to leverage ray

00:14:26,399 --> 00:14:30,480
and so here we're going to load up the

00:14:29,040 --> 00:14:32,880
sklearn predefined

00:14:30,480 --> 00:14:34,720
uh breast cancer data set just as a

00:14:32,880 --> 00:14:38,000
simple example

00:14:34,720 --> 00:14:39,279
and we're going to create a ray d matrix

00:14:38,000 --> 00:14:43,040
object so that

00:14:39,279 --> 00:14:46,399
can actually be easily parallelized

00:14:43,040 --> 00:14:48,639
now that we have our data you can run a

00:14:46,399 --> 00:14:51,519
little xg boost training run

00:14:48,639 --> 00:14:52,079
and here you can see we're using the

00:14:51,519 --> 00:14:54,240
actual

00:14:52,079 --> 00:14:56,079
overloaded train function and we're also

00:14:54,240 --> 00:14:57,600
giving it some right parameters and

00:14:56,079 --> 00:14:58,160
there are many parameters here you can

00:14:57,600 --> 00:15:00,480
use but

00:14:58,160 --> 00:15:02,480
the fundamental one is what kind of

00:15:00,480 --> 00:15:04,480
parallelism do you want to use i'm just

00:15:02,480 --> 00:15:05,839
asking it to produce two actors here a

00:15:04,480 --> 00:15:08,880
very small little

00:15:05,839 --> 00:15:08,880
parallelism case

00:15:08,959 --> 00:15:13,839
so we'll let it kick off here

00:15:14,560 --> 00:15:18,320
ray likes to uh give you lots of log

00:15:16,320 --> 00:15:20,320
output um

00:15:18,320 --> 00:15:21,600
it's producing its actors to do a

00:15:20,320 --> 00:15:23,279
training run

00:15:21,600 --> 00:15:25,550
and even before i can finish talking

00:15:23,279 --> 00:15:26,880
about it it's come back and finished

00:15:25,550 --> 00:15:29,360
[Music]

00:15:26,880 --> 00:15:32,079
and so i've got a model that was trained

00:15:29,360 --> 00:15:35,600
in parallel using a self-service

00:15:32,079 --> 00:15:37,279
ray cluster um but of course that's my

00:15:35,600 --> 00:15:39,360
part of the story um the reason you like

00:15:37,279 --> 00:15:42,320
to be in jupiter is you know to

00:15:39,360 --> 00:15:44,480
use things you can do in jupiter as well

00:15:42,320 --> 00:15:46,959
as ray so we can do things like examine

00:15:44,480 --> 00:15:50,240
training results and cells so this thing

00:15:46,959 --> 00:15:55,440
actually has a very low error rate

00:15:50,240 --> 00:15:56,880
and better yet visualization um

00:15:55,440 --> 00:15:58,560
so we can take this and do a little

00:15:56,880 --> 00:16:02,720
scatter plot of the

00:15:58,560 --> 00:16:04,079
raw uh logistic regression output versus

00:16:02,720 --> 00:16:05,680
the truth and

00:16:04,079 --> 00:16:07,600
of course we'd like all the things that

00:16:05,680 --> 00:16:08,079
are one to be to the right of our dashed

00:16:07,600 --> 00:16:10,480
line

00:16:08,079 --> 00:16:12,560
and things that are 0 to the left we can

00:16:10,480 --> 00:16:13,920
see it's almost always true

00:16:12,560 --> 00:16:16,720
so there's a little visual

00:16:13,920 --> 00:16:16,720
representation

00:16:19,519 --> 00:16:26,079
and it also provides you with

00:16:22,959 --> 00:16:26,720
a the name of the actual uh url to

00:16:26,079 --> 00:16:28,480
connect to

00:16:26,720 --> 00:16:36,320
the ray dashboard which as you can see i

00:16:28,480 --> 00:16:38,639
already have done

00:16:36,320 --> 00:16:38,639
so

00:16:41,759 --> 00:16:46,880
um what's the story here i think that in

00:16:44,720 --> 00:16:50,560
addition to just simply having easy

00:16:46,880 --> 00:16:52,800
parallelism backing your compute um

00:16:50,560 --> 00:16:54,880
i think the bigger story is that it's

00:16:52,800 --> 00:16:58,160
not just easy it's flexible

00:16:54,880 --> 00:17:01,120
um ray can support all kinds of you know

00:16:58,160 --> 00:17:02,160
tooling from the ml space xg boost but

00:17:01,120 --> 00:17:05,360
also horovod

00:17:02,160 --> 00:17:07,360
pandas scikit-learn many more

00:17:05,360 --> 00:17:09,199
and so this architecture of simply

00:17:07,360 --> 00:17:10,000
connecting to your familiar tooling with

00:17:09,199 --> 00:17:14,160
jupiter hub

00:17:10,000 --> 00:17:17,280
but backing all the compute with ray

00:17:14,160 --> 00:17:19,679
is a very unified platform and

00:17:17,280 --> 00:17:21,360
not just unified but simplified so in a

00:17:19,679 --> 00:17:23,360
sense it can actually simplify your

00:17:21,360 --> 00:17:24,480
cluster deployments for data science

00:17:23,360 --> 00:17:27,600
you're running

00:17:24,480 --> 00:17:28,160
a single backend cluster array cluster

00:17:27,600 --> 00:17:31,520
to do

00:17:28,160 --> 00:17:35,200
all of your compute instead of bespoke

00:17:31,520 --> 00:17:35,200
operators for each of the tools

00:17:36,960 --> 00:17:42,080
so the demo you just saw was running up

00:17:39,360 --> 00:17:44,880
on the massachusetts open cloud

00:17:42,080 --> 00:17:46,320
which is run in the partnership between

00:17:44,880 --> 00:17:49,120
boston university

00:17:46,320 --> 00:17:50,640
and red hat and also a large

00:17:49,120 --> 00:17:53,919
collaborative consortium

00:17:50,640 --> 00:17:58,880
of different universities and some

00:17:53,919 --> 00:18:00,799
interested businesses and

00:17:58,880 --> 00:18:02,720
anybody can get up on the massachusetts

00:18:00,799 --> 00:18:03,679
open cloud i'll show you a link in a

00:18:02,720 --> 00:18:05,520
minute

00:18:03,679 --> 00:18:06,720
and if you do this you can run all these

00:18:05,520 --> 00:18:09,360
examples plus

00:18:06,720 --> 00:18:09,840
some others that i've created um and

00:18:09,360 --> 00:18:11,840
you'll get

00:18:09,840 --> 00:18:13,200
a smallish cluster with a maximum of

00:18:11,840 --> 00:18:14,960
five workers

00:18:13,200 --> 00:18:17,440
i recently upgraded the memory to 3

00:18:14,960 --> 00:18:18,320
gigabytes and the images have some

00:18:17,440 --> 00:18:20,799
common tooling

00:18:18,320 --> 00:18:20,799
installed

00:18:22,799 --> 00:18:28,240
in addition the particular openshift

00:18:26,240 --> 00:18:30,960
deployment that i ran on the mass

00:18:28,240 --> 00:18:31,520
open cloud is being maintained using the

00:18:30,960 --> 00:18:35,039
operate

00:18:31,520 --> 00:18:36,799
first project this is a project

00:18:35,039 --> 00:18:39,360
being run at red hat where we're

00:18:36,799 --> 00:18:41,919
exploring how to extend the

00:18:39,360 --> 00:18:42,559
familiar open source principles of

00:18:41,919 --> 00:18:45,919
developing

00:18:42,559 --> 00:18:47,600
software in the open to also operating

00:18:45,919 --> 00:18:50,160
the software and the services

00:18:47,600 --> 00:18:51,840
in the open and you can read more about

00:18:50,160 --> 00:18:53,360
that the link at the bottom

00:18:51,840 --> 00:18:54,960
so as you might guess this has a very

00:18:53,360 --> 00:18:57,360
strong get

00:18:54,960 --> 00:18:58,160
ops flavor to it and so here you can

00:18:57,360 --> 00:19:01,760
actually

00:18:58,160 --> 00:19:03,760
go view the pull requests that i produce

00:19:01,760 --> 00:19:05,280
to create this deployment of ray with

00:19:03,760 --> 00:19:10,480
open data hub

00:19:05,280 --> 00:19:14,000
the link is at the bottom

00:19:10,480 --> 00:19:14,559
uh lastly you know there's a lot more to

00:19:14,000 --> 00:19:17,120
do

00:19:14,559 --> 00:19:17,840
um it'd be good to get like the ray

00:19:17,120 --> 00:19:20,000
operator

00:19:17,840 --> 00:19:22,720
as a community or operator through the

00:19:20,000 --> 00:19:25,679
operator catalog

00:19:22,720 --> 00:19:27,760
you know it might be nice to get

00:19:25,679 --> 00:19:30,000
standardized build pipelines

00:19:27,760 --> 00:19:32,000
for all the right imagery uh hopefully

00:19:30,000 --> 00:19:35,120
using red hat's project toff

00:19:32,000 --> 00:19:36,799
to do the building and i'm hoping for

00:19:35,120 --> 00:19:38,320
more community use cases

00:19:36,799 --> 00:19:40,559
i'm hoping more people try to use

00:19:38,320 --> 00:19:42,640
deployment to do data science with

00:19:40,559 --> 00:19:43,840
and i definitely want to get a formal

00:19:42,640 --> 00:19:46,880
integration

00:19:43,840 --> 00:19:48,880
of ray with kubeflow and open data hub

00:19:46,880 --> 00:19:49,919
and also there's a lot of potential here

00:19:48,880 --> 00:19:53,200
for

00:19:49,919 --> 00:19:54,160
running nodes in ml pipelines like kf

00:19:53,200 --> 00:19:56,080
pipelines

00:19:54,160 --> 00:19:58,880
or each node could actually reference a

00:19:56,080 --> 00:19:58,880
ray cluster

00:20:00,640 --> 00:20:04,640
and so again thank you for coming to my

00:20:02,559 --> 00:20:07,679
talk i invite you to

00:20:04,640 --> 00:20:09,520
play with ray up on the mass open cloud

00:20:07,679 --> 00:20:11,760
you can see the link at the bottom there

00:20:09,520 --> 00:20:15,120
to go look at it on the instructions for

00:20:11,760 --> 00:20:17,440
using the ray imagery is also

00:20:15,120 --> 00:20:18,799
at the second link and if you have

00:20:17,440 --> 00:20:21,200
problems or suggestions

00:20:18,799 --> 00:20:23,039
file issues or pull requests with the

00:20:21,200 --> 00:20:25,760
operate first

00:20:23,039 --> 00:20:27,440
environment and please feel free to

00:20:25,760 --> 00:20:29,120
reach out to me at my email if you have

00:20:27,440 --> 00:20:35,840
questions or comments

00:20:29,120 --> 00:20:35,840
thanks everybody

00:20:50,400 --> 00:20:52,480

YouTube URL: https://www.youtube.com/watch?v=mzKtQEU7yxg


