Title: Francesco Tisiot – Event-Driven applications: Apache Kafka and Python
Publication date: 2021-06-25
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Code and data go together like tomato and basil; not many applications work without moving data in some way. As our applications modernise and evolve to become more event-driven, the requirements for data are changing. In this session we will explore Apache Kafka, a data streaming platform, to enable reliable real-time data integration for your applications.

We will look at the types of problems that Kafka is best at solving, and show how to use it in your own applications. Whether you have a new application or are looking to upgrade an existing one, this session includes advice on adding Kafka using the Python libraries and includes code examples (with bonus discussion of pizza toppings) to use.

With Kafka in place, many things are possible so this session also introduces Kafka Connect, a selection of pre-built connectors that you can use to route events between systems and integrate with other tools. This session is recommended for engineers and architects whose applications are ready for next-level data abilities.

Speaker:
Francesco Tisiot – https://2021.berlinbuzzwords.de/member/francesco-tisiot

More: https://2021.berlinbuzzwords.de/session/event-driven-applications-apache-kafka-and-python
Captions: 
	00:00:06,960 --> 00:00:10,400
technology

00:00:07,919 --> 00:00:12,400
so let's start by talking about event

00:00:10,400 --> 00:00:14,000
driven application with apache kafka

00:00:12,400 --> 00:00:16,320
python

00:00:14,000 --> 00:00:18,320
as scott said i'm francis concealed

00:00:16,320 --> 00:00:19,680
developer advocate ivan which is a nice

00:00:18,320 --> 00:00:21,840
company building

00:00:19,680 --> 00:00:22,960
open source things in the cloud of your

00:00:21,840 --> 00:00:25,439
choice all

00:00:22,960 --> 00:00:26,240
managed for you so let's not waste any

00:00:25,439 --> 00:00:27,519
more time

00:00:26,240 --> 00:00:29,760
if you're here you are somehow

00:00:27,519 --> 00:00:31,439
interested in kind of event-driven

00:00:29,760 --> 00:00:35,280
application and probably

00:00:31,439 --> 00:00:38,559
you are familiar with python

00:00:35,280 --> 00:00:42,879
on the other side you may wonder

00:00:38,559 --> 00:00:46,160
what is apache kafka why should i

00:00:42,879 --> 00:00:50,480
be interested in apache kafka

00:00:46,160 --> 00:00:53,680
well let me tell you the full story

00:00:50,480 --> 00:00:56,559
if you are a developer you will be

00:00:53,680 --> 00:00:58,320
either working on a kind of new shiny

00:00:56,559 --> 00:01:01,600
application that

00:00:58,320 --> 00:01:03,280
you just created out of nothing and you

00:01:01,600 --> 00:01:06,320
are exposing to the world

00:01:03,280 --> 00:01:09,200
or you are part of a large company and

00:01:06,320 --> 00:01:10,479
you're inherited an old application and

00:01:09,200 --> 00:01:13,200
you have to support it

00:01:10,479 --> 00:01:14,400
to extend it and maybe to take it to the

00:01:13,200 --> 00:01:18,560
new world

00:01:14,400 --> 00:01:20,799
no matter if new or old i've never seen

00:01:18,560 --> 00:01:22,640
an application working in complete

00:01:20,799 --> 00:01:24,400
isolation

00:01:22,640 --> 00:01:26,400
you will have components within the

00:01:24,400 --> 00:01:27,520
application that needs to talk with each

00:01:26,400 --> 00:01:30,079
other

00:01:27,520 --> 00:01:32,000
and if you expose this application into

00:01:30,079 --> 00:01:35,600
the word you will have this application

00:01:32,000 --> 00:01:37,920
talking with other applications around

00:01:35,600 --> 00:01:38,880
so let me share with you a secret apache

00:01:37,920 --> 00:01:42,159
kafka

00:01:38,880 --> 00:01:46,720
is what makes this communication easy

00:01:42,159 --> 00:01:48,960
and reliable but now let's check

00:01:46,720 --> 00:01:51,119
a little bit more what we were doing in

00:01:48,960 --> 00:01:52,159
the past and why kafka is relevant

00:01:51,119 --> 00:01:55,360
nowadays

00:01:52,159 --> 00:01:58,240
if we go back in history few years

00:01:55,360 --> 00:01:59,040
we had our application that were already

00:01:58,240 --> 00:02:01,840
there

00:01:59,040 --> 00:02:02,960
and they were already kind of creating

00:02:01,840 --> 00:02:06,240
events

00:02:02,960 --> 00:02:09,119
and most of them were storing events in

00:02:06,240 --> 00:02:11,360
backend database or other applications

00:02:09,119 --> 00:02:14,160
when we're reading from the database

00:02:11,360 --> 00:02:16,239
and passing those up those events into

00:02:14,160 --> 00:02:19,280
the application itself

00:02:16,239 --> 00:02:21,840
most of the times however the

00:02:19,280 --> 00:02:24,239
communication of the events wasn't done

00:02:21,840 --> 00:02:26,400
for every single message

00:02:24,239 --> 00:02:27,920
the application was piling up a series

00:02:26,400 --> 00:02:30,879
of events and then

00:02:27,920 --> 00:02:32,640
pushing them to the database or if the

00:02:30,879 --> 00:02:34,560
same application or another application

00:02:32,640 --> 00:02:37,680
was reading from the databases

00:02:34,560 --> 00:02:41,280
in batches this meant that we were

00:02:37,680 --> 00:02:44,160
adding a constant delay between when

00:02:41,280 --> 00:02:45,680
an event was created in the real life

00:02:44,160 --> 00:02:47,360
and when we were pushing it to the

00:02:45,680 --> 00:02:50,239
database

00:02:47,360 --> 00:02:52,000
or from when the event was already

00:02:50,239 --> 00:02:54,879
available in the database

00:02:52,000 --> 00:02:55,360
and we were able to read it on the other

00:02:54,879 --> 00:02:58,159
side

00:02:55,360 --> 00:03:00,080
we now live in kind of a fast word and

00:02:58,159 --> 00:03:03,519
we cannot wait batch time

00:03:00,080 --> 00:03:06,000
we cannot wait 5 hours 20 minutes

00:03:03,519 --> 00:03:06,640
10 minutes in order to receive a certain

00:03:06,000 --> 00:03:08,560
event

00:03:06,640 --> 00:03:10,800
we need to build event driven

00:03:08,560 --> 00:03:12,959
application what are those

00:03:10,800 --> 00:03:14,400
applications that as soon as an event

00:03:12,959 --> 00:03:17,120
happen in real life

00:03:14,400 --> 00:03:19,200
they will immediately need to know about

00:03:17,120 --> 00:03:21,360
it start parsing it

00:03:19,200 --> 00:03:23,519
and probably take the outcome of the

00:03:21,360 --> 00:03:25,360
passing and push it to

00:03:23,519 --> 00:03:26,640
another downstream application which

00:03:25,360 --> 00:03:28,400
will act probably

00:03:26,640 --> 00:03:31,280
as another event-driven application

00:03:28,400 --> 00:03:33,920
creating a chain of those

00:03:31,280 --> 00:03:35,760
but before going in deep into how to

00:03:33,920 --> 00:03:38,799
create an event-driven application

00:03:35,760 --> 00:03:41,680
let's start by defining what is an event

00:03:38,799 --> 00:03:43,680
and you know we are all used now with

00:03:41,680 --> 00:03:45,360
for example mobile phones

00:03:43,680 --> 00:03:47,440
and we are all used to receive

00:03:45,360 --> 00:03:50,159
notification about

00:03:47,440 --> 00:03:50,799
something the notification tell us that

00:03:50,159 --> 00:03:54,000
an event

00:03:50,799 --> 00:03:54,560
happened did someone else send us a

00:03:54,000 --> 00:03:57,280
message

00:03:54,560 --> 00:03:58,480
we receive a notification did we made a

00:03:57,280 --> 00:04:00,400
purchase

00:03:58,480 --> 00:04:01,599
with our credit card we receive a

00:04:00,400 --> 00:04:03,840
notification

00:04:01,599 --> 00:04:05,040
then someone else stole our credit card

00:04:03,840 --> 00:04:07,200
and made a payment

00:04:05,040 --> 00:04:09,040
we receive a notification as you can

00:04:07,200 --> 00:04:13,120
understand we cannot wait

00:04:09,040 --> 00:04:15,040
six hours five minutes two minutes

00:04:13,120 --> 00:04:16,880
of batch time before receiving that

00:04:15,040 --> 00:04:19,759
information we want to receive it

00:04:16,880 --> 00:04:20,079
immediately and even more in this time

00:04:19,759 --> 00:04:22,880
of

00:04:20,079 --> 00:04:23,600
for example pandemic we i have been

00:04:22,880 --> 00:04:27,040
using

00:04:23,600 --> 00:04:28,000
a lot our mobile phones in order for

00:04:27,040 --> 00:04:31,919
example to

00:04:28,000 --> 00:04:35,440
uh create food orders and to receive

00:04:31,919 --> 00:04:38,960
food at home and by the time that we

00:04:35,440 --> 00:04:40,880
switch on our mobile we select the app

00:04:38,960 --> 00:04:41,440
we select the restaurant and we make the

00:04:40,880 --> 00:04:44,720
order

00:04:41,440 --> 00:04:46,800
we are creating a chain or a first event

00:04:44,720 --> 00:04:49,280
that will create a chain of other events

00:04:46,800 --> 00:04:50,080
so when we create the order the first

00:04:49,280 --> 00:04:52,000
thing that

00:04:50,080 --> 00:04:55,440
it will be done is that the order will

00:04:52,000 --> 00:04:58,560
be sent for example to your favorite p3

00:04:55,440 --> 00:05:00,479
and that pizzeria will start reacting as

00:04:58,560 --> 00:05:03,039
an event-driven application and you will

00:05:00,479 --> 00:05:05,680
start preparing your pizza

00:05:03,039 --> 00:05:07,280
at a certain point your pizza will be

00:05:05,680 --> 00:05:09,280
ready

00:05:07,280 --> 00:05:11,199
and it will be ready at the pizzeria not

00:05:09,280 --> 00:05:11,680
at your place so it will be another

00:05:11,199 --> 00:05:13,440
event

00:05:11,680 --> 00:05:14,880
for the delivery people to come and pick

00:05:13,440 --> 00:05:18,160
it up and take it

00:05:14,880 --> 00:05:20,320
to your place now if

00:05:18,160 --> 00:05:21,360
you start thinking about events you also

00:05:20,320 --> 00:05:24,320
start realizing

00:05:21,360 --> 00:05:27,440
that we need to communicate those events

00:05:24,320 --> 00:05:30,000
past those events as quickly as we can

00:05:27,440 --> 00:05:30,880
because we as i said before we live in a

00:05:30,000 --> 00:05:33,039
fast word

00:05:30,880 --> 00:05:34,960
so the value of the information itself

00:05:33,039 --> 00:05:37,440
is strictly related to the time that it

00:05:34,960 --> 00:05:41,919
takes to be delivered

00:05:37,440 --> 00:05:44,479
going back to the delivery of the pizza

00:05:41,919 --> 00:05:45,840
the information is useful only from the

00:05:44,479 --> 00:05:48,960
time that

00:05:45,840 --> 00:05:51,120
the position of the driver from the time

00:05:48,960 --> 00:05:52,479
that the position is taken from the

00:05:51,120 --> 00:05:55,039
driver's cell phone to the time that it

00:05:52,479 --> 00:05:56,639
lands on our map the delay is minimalist

00:05:55,039 --> 00:05:58,560
10 20 seconds

00:05:56,639 --> 00:05:59,680
if the delay is 5 minutes the

00:05:58,560 --> 00:06:02,080
information about the

00:05:59,680 --> 00:06:03,600
position of the delivery is not relevant

00:06:02,080 --> 00:06:05,520
anymore

00:06:03,600 --> 00:06:07,360
so we need a tool that makes this

00:06:05,520 --> 00:06:10,479
communication reliable

00:06:07,360 --> 00:06:13,680
easy and quick and well kafka

00:06:10,479 --> 00:06:14,560
is exactly the tool but if we have to

00:06:13,680 --> 00:06:18,479
think about

00:06:14,560 --> 00:06:21,280
what is kafka well the basic of kafka

00:06:18,479 --> 00:06:22,639
is really simple is the concept of a log

00:06:21,280 --> 00:06:26,400
file

00:06:22,639 --> 00:06:28,960
oh sorry a log a log where we start

00:06:26,400 --> 00:06:30,880
storing our events as soon as they

00:06:28,960 --> 00:06:32,560
happen so event number zero happens and

00:06:30,880 --> 00:06:35,120
we store it in the log

00:06:32,560 --> 00:06:36,639
event number one happens and we store it

00:06:35,120 --> 00:06:39,759
two three and four

00:06:36,639 --> 00:06:42,400
even more kafka is a log which is

00:06:39,759 --> 00:06:44,720
up and only and immutable this means

00:06:42,400 --> 00:06:45,759
that we can write only at the end of the

00:06:44,720 --> 00:06:48,479
log

00:06:45,759 --> 00:06:50,080
and once we write event zero in the log

00:06:48,479 --> 00:06:51,759
as message

00:06:50,080 --> 00:06:53,759
it's not like a record in the database

00:06:51,759 --> 00:06:56,319
we cannot go there and change

00:06:53,759 --> 00:06:58,400
the pizza from margarita to diavola

00:06:56,319 --> 00:07:00,080
event zero will be always like that

00:06:58,400 --> 00:07:02,479
if something changes the reality of

00:07:00,080 --> 00:07:05,199
event number zero we will store it as a

00:07:02,479 --> 00:07:08,319
new event in our log

00:07:05,199 --> 00:07:11,840
kafka allow us not only to store

00:07:08,319 --> 00:07:14,080
one type of events allow us to store

00:07:11,840 --> 00:07:15,520
many type of events because we all know

00:07:14,080 --> 00:07:18,080
that events happen in

00:07:15,520 --> 00:07:19,199
different types for example we have the

00:07:18,080 --> 00:07:22,240
pizza orders

00:07:19,199 --> 00:07:24,800
and we can have the delivery position so

00:07:22,240 --> 00:07:27,199
we will store the pizza orders in

00:07:24,800 --> 00:07:28,720
one log and the delivery position in our

00:07:27,199 --> 00:07:31,280
log in another log

00:07:28,720 --> 00:07:32,000
and those logs in kafka terms are called

00:07:31,280 --> 00:07:34,240
topics

00:07:32,000 --> 00:07:37,680
so i have topic a and topic b it's

00:07:34,240 --> 00:07:41,199
orders and delivered position

00:07:37,680 --> 00:07:45,599
kafka in the back is not meant to

00:07:41,199 --> 00:07:47,759
run on just you a unique huge server

00:07:45,599 --> 00:07:49,360
is meant to be a distributed system this

00:07:47,759 --> 00:07:50,960
means that when you create a kafka

00:07:49,360 --> 00:07:54,879
instance you are most of the time

00:07:50,960 --> 00:07:57,120
actually creating a set of nodes

00:07:54,879 --> 00:07:59,280
a set of nodes that in kafka terms are

00:07:57,120 --> 00:08:01,840
called brokers

00:07:59,280 --> 00:08:02,560
and now if we go back to our log

00:08:01,840 --> 00:08:05,360
information

00:08:02,560 --> 00:08:07,440
to our topics the topics the topics are

00:08:05,360 --> 00:08:10,479
stored across the brokers

00:08:07,440 --> 00:08:13,039
but not only one time they we store

00:08:10,479 --> 00:08:13,919
the topics across the brokers multiple

00:08:13,039 --> 00:08:17,360
times

00:08:13,919 --> 00:08:18,080
so for example we have our sharp pages

00:08:17,360 --> 00:08:20,319
log

00:08:18,080 --> 00:08:22,000
that is stored three times following a

00:08:20,319 --> 00:08:22,960
parameter which is called replication

00:08:22,000 --> 00:08:26,160
factor

00:08:22,960 --> 00:08:28,240
of three and then we have the other

00:08:26,160 --> 00:08:30,000
lock the other topic round edges that

00:08:28,240 --> 00:08:32,159
could be delivery position

00:08:30,000 --> 00:08:33,519
which has only two copies so replication

00:08:32,159 --> 00:08:35,599
factor of two

00:08:33,519 --> 00:08:37,919
why do we store multiple copies of the

00:08:35,599 --> 00:08:37,919
same

00:08:38,320 --> 00:08:42,000
well do we store multiple copies of the

00:08:40,159 --> 00:08:45,120
same topic

00:08:42,000 --> 00:08:46,560
of our brokers well because we know that

00:08:45,120 --> 00:08:49,279
computers are not entirely

00:08:46,560 --> 00:08:50,000
reliable and we could lose a node but

00:08:49,279 --> 00:08:51,760
still

00:08:50,000 --> 00:08:53,760
if you check here we are not going to

00:08:51,760 --> 00:08:55,440
lose any data both the sharp edges topic

00:08:53,760 --> 00:08:57,360
and the round edges topic are still

00:08:55,440 --> 00:09:00,560
there available

00:08:57,360 --> 00:09:02,959
so we understood how kafka works

00:09:00,560 --> 00:09:03,920
on the back end now let's try to

00:09:02,959 --> 00:09:07,279
understand

00:09:03,920 --> 00:09:09,360
what is an event for kafka

00:09:07,279 --> 00:09:11,040
it's a platform meant to store and

00:09:09,360 --> 00:09:15,120
propagate events

00:09:11,040 --> 00:09:18,160
well kafka is really easy when

00:09:15,120 --> 00:09:21,279
it defines an event all you need to know

00:09:18,160 --> 00:09:22,240
about an event if in kafka is that it is

00:09:21,279 --> 00:09:25,120
composed by

00:09:22,240 --> 00:09:26,640
a key and a value and kafka really

00:09:25,120 --> 00:09:28,880
doesn't care about what you put

00:09:26,640 --> 00:09:31,519
both in the key and the value for kafka

00:09:28,880 --> 00:09:34,720
is just a series of bytes

00:09:31,519 --> 00:09:36,640
you can start with really easy messages

00:09:34,720 --> 00:09:38,240
for example where you store the maximum

00:09:36,640 --> 00:09:42,080
temperature as key

00:09:38,240 --> 00:09:45,040
and the value of it 35 the three sorry

00:09:42,080 --> 00:09:45,440
revive the three as value or you could

00:09:45,040 --> 00:09:48,560
go

00:09:45,440 --> 00:09:49,279
pretty wild where you store both in json

00:09:48,560 --> 00:09:52,160
in the key

00:09:49,279 --> 00:09:54,080
and in value and in the key you store

00:09:52,160 --> 00:09:55,680
the shop name receiving the pizza order

00:09:54,080 --> 00:09:56,480
together with the phone line used to

00:09:55,680 --> 00:09:58,399
make the call

00:09:56,480 --> 00:10:00,320
and in the value you store all the order

00:09:58,399 --> 00:10:02,320
details containing the id

00:10:00,320 --> 00:10:05,600
the name of the person making the order

00:10:02,320 --> 00:10:07,839
and the list of pizzas

00:10:05,600 --> 00:10:09,680
you can use any format that you want

00:10:07,839 --> 00:10:11,680
because for kafka it's just a series of

00:10:09,680 --> 00:10:12,880
bytes so for example you could use json

00:10:11,680 --> 00:10:15,120
like in this case

00:10:12,880 --> 00:10:16,880
which is beautiful because i can read as

00:10:15,120 --> 00:10:18,320
human the world message

00:10:16,880 --> 00:10:20,959
but if we have to think about

00:10:18,320 --> 00:10:23,200
transmitting json over the network

00:10:20,959 --> 00:10:25,360
is a little bit heavy because for every

00:10:23,200 --> 00:10:28,399
field it contains both the field name

00:10:25,360 --> 00:10:30,399
and the fill value if we want to send

00:10:28,399 --> 00:10:32,560
the same information in a more compacted

00:10:30,399 --> 00:10:35,200
way we can use other data formats

00:10:32,560 --> 00:10:37,120
like for example avro that detach the

00:10:35,200 --> 00:10:38,800
schema from the payload and use a schema

00:10:37,120 --> 00:10:40,640
registry in order to compact

00:10:38,800 --> 00:10:42,079
the message and on the other side when

00:10:40,640 --> 00:10:46,640
we consume the data

00:10:42,079 --> 00:10:49,200
to re it rotate the message itself

00:10:46,640 --> 00:10:51,040
so now that we understood what an event

00:10:49,200 --> 00:10:53,040
what a message for kafka is

00:10:51,040 --> 00:10:54,160
it's time to understand how we can write

00:10:53,040 --> 00:10:56,800
to kafka

00:10:54,160 --> 00:10:59,360
and if we have an application in our

00:10:56,800 --> 00:11:01,680
case it will be a python application

00:10:59,360 --> 00:11:04,240
that writes to kafka well then it's

00:11:01,680 --> 00:11:08,000
called a producer

00:11:04,240 --> 00:11:10,320
and a producer produces data to a topic

00:11:08,000 --> 00:11:12,000
in order to produce data to a topic all

00:11:10,320 --> 00:11:14,240
the producer needs to know

00:11:12,000 --> 00:11:15,440
is where to find kafka or name and port

00:11:14,240 --> 00:11:17,920
of the brokers

00:11:15,440 --> 00:11:19,279
how to authenticate to kafka for example

00:11:17,920 --> 00:11:22,000
using ssl

00:11:19,279 --> 00:11:24,000
and how to encode the data from for for

00:11:22,000 --> 00:11:25,839
example the json format that was using

00:11:24,000 --> 00:11:27,760
internally to the raw series of bytes

00:11:25,839 --> 00:11:30,640
that kafka understands

00:11:27,760 --> 00:11:32,720
old so simple on the other side if we

00:11:30,640 --> 00:11:34,720
have data in a kafka topic

00:11:32,720 --> 00:11:36,480
and we want to read with an application

00:11:34,720 --> 00:11:38,000
well this application will be called a

00:11:36,480 --> 00:11:41,120
consumer

00:11:38,000 --> 00:11:43,519
and what the consumer does is it reads

00:11:41,120 --> 00:11:45,440
event number zero and then communicates

00:11:43,519 --> 00:11:48,640
back to kafka event number zero done

00:11:45,440 --> 00:11:50,560
let's move to one reads event number one

00:11:48,640 --> 00:11:53,120
communicates back to kafka one done

00:11:50,560 --> 00:11:55,440
let's move to two and so on so forth

00:11:53,120 --> 00:11:56,560
why communicating back to kafka is

00:11:55,440 --> 00:11:59,839
really important

00:11:56,560 --> 00:12:00,720
well because we know again computers are

00:11:59,839 --> 00:12:03,200
not entirely

00:12:00,720 --> 00:12:04,800
reliable so we know that for example the

00:12:03,200 --> 00:12:07,519
consumer could fail

00:12:04,800 --> 00:12:08,000
but still kafka will know until what

00:12:07,519 --> 00:12:11,600
point

00:12:08,000 --> 00:12:13,440
that consumer read that specific log

00:12:11,600 --> 00:12:15,200
so the next time that the consumer pops

00:12:13,440 --> 00:12:17,600
up kafka will know

00:12:15,200 --> 00:12:20,800
which is the next item that the consumer

00:12:17,600 --> 00:12:20,800
didn't pass previously

00:12:21,040 --> 00:12:24,240
in order to consume data from kafka all

00:12:23,279 --> 00:12:26,240
that the

00:12:24,240 --> 00:12:28,320
consumer needs to know is pretty much

00:12:26,240 --> 00:12:31,440
the same information as the producer

00:12:28,320 --> 00:12:34,240
first of all hostname and port

00:12:31,440 --> 00:12:35,440
second how to authenticate to kafka as

00:12:34,240 --> 00:12:37,600
before

00:12:35,440 --> 00:12:38,560
but before when we were producing data

00:12:37,600 --> 00:12:41,200
we were

00:12:38,560 --> 00:12:42,720
serializing now it's time to this

00:12:41,200 --> 00:12:44,800
realize the code

00:12:42,720 --> 00:12:45,839
from row series of bytes to for example

00:12:44,800 --> 00:12:48,240
json

00:12:45,839 --> 00:12:49,279
and the last information is where to

00:12:48,240 --> 00:12:51,920
find the data so

00:12:49,279 --> 00:12:54,079
which topic or which list of topics the

00:12:51,920 --> 00:12:57,120
consumer wants to read from

00:12:54,079 --> 00:12:57,760
now these were a lot of slides now it's

00:12:57,120 --> 00:13:00,880
time to

00:12:57,760 --> 00:13:03,440
see some actual code so it's time for a

00:13:00,880 --> 00:13:07,839
nice pizza demo

00:13:03,440 --> 00:13:10,800
what i created so far for this kind of

00:13:07,839 --> 00:13:12,399
event is a series of notebooks a series

00:13:10,800 --> 00:13:15,760
of notebooks that we will use in order

00:13:12,399 --> 00:13:18,399
to interact with kafka

00:13:15,760 --> 00:13:19,440
how to get kafka it's another discussion

00:13:18,399 --> 00:13:21,600
you can

00:13:19,440 --> 00:13:23,120
have it in on premises or you can

00:13:21,600 --> 00:13:24,480
install yourself in the cloud of your

00:13:23,120 --> 00:13:26,560
choice or you can use

00:13:24,480 --> 00:13:27,519
a managed service like the one that ivan

00:13:26,560 --> 00:13:30,079
offers

00:13:27,519 --> 00:13:32,240
what they did in the first notebook here

00:13:30,079 --> 00:13:35,200
is pre-create the environment

00:13:32,240 --> 00:13:36,240
if we go and check on the ivan side i

00:13:35,200 --> 00:13:38,959
created

00:13:36,240 --> 00:13:40,560
two items one kafka instance which is

00:13:38,959 --> 00:13:43,920
fully managed for you

00:13:40,560 --> 00:13:45,839
and a postgres instance what we will see

00:13:43,920 --> 00:13:46,959
now in the rest of the demo is how we

00:13:45,839 --> 00:13:49,199
can interact with

00:13:46,959 --> 00:13:50,959
kafka and then also a little secret

00:13:49,199 --> 00:13:52,079
about having kafka interacting with

00:13:50,959 --> 00:13:55,120
postgres

00:13:52,079 --> 00:13:57,360
so let's start

00:13:55,120 --> 00:13:58,320
now the first item on our list is to

00:13:57,360 --> 00:14:00,959
create a producer

00:13:58,320 --> 00:14:02,800
so let's click on the producer and let's

00:14:00,959 --> 00:14:04,880
check what we have to do

00:14:02,800 --> 00:14:06,800
we will use the kafka python library

00:14:04,880 --> 00:14:08,399
which is the basic library allowing you

00:14:06,800 --> 00:14:12,160
to integrate

00:14:08,399 --> 00:14:15,040
python and kafka so let's install it

00:14:12,160 --> 00:14:17,279
now that the library is installed we can

00:14:15,040 --> 00:14:19,920
create a kafka producer

00:14:17,279 --> 00:14:21,279
and in order to produce data to kafka

00:14:19,920 --> 00:14:24,320
all we need to know is

00:14:21,279 --> 00:14:26,320
where to find kafka hostname and port

00:14:24,320 --> 00:14:28,399
how to connect using ssl and threat

00:14:26,320 --> 00:14:29,440
certificates and how to serialize the

00:14:28,399 --> 00:14:32,560
data

00:14:29,440 --> 00:14:35,440
from json to a row series of bytes both

00:14:32,560 --> 00:14:38,160
for the key and the value so we will use

00:14:35,440 --> 00:14:39,120
key and value which will be json and we

00:14:38,160 --> 00:14:42,639
will send them

00:14:39,120 --> 00:14:46,079
as row series of bytes to kafka

00:14:42,639 --> 00:14:48,240
so let's create the producer now

00:14:46,079 --> 00:14:50,800
and now that the producer is created

00:14:48,240 --> 00:14:54,079
let's send our first message to kafka

00:14:50,800 --> 00:14:57,360
and the first message is a pizza order

00:14:54,079 --> 00:14:59,680
is francesco myself sending an order for

00:14:57,360 --> 00:15:03,279
pizza margarita my favorite one

00:14:59,680 --> 00:15:06,079
so let's send it now

00:15:03,279 --> 00:15:08,240
the record is in kafka well how can i be

00:15:06,079 --> 00:15:11,279
sure that the record is in kafka

00:15:08,240 --> 00:15:13,680
well i need to probably read so let's

00:15:11,279 --> 00:15:16,240
create a consumer now

00:15:13,680 --> 00:15:16,880
let's take the consumer and the beauty

00:15:16,240 --> 00:15:19,839
of

00:15:16,880 --> 00:15:21,040
jupiter notebooks is i can have multiple

00:15:19,839 --> 00:15:23,120
notebooks running

00:15:21,040 --> 00:15:26,560
one against the other so i create the

00:15:23,120 --> 00:15:28,880
consumer alongside the producer

00:15:26,560 --> 00:15:30,720
in order to create the consumer there is

00:15:28,880 --> 00:15:32,480
the kafka consumer and they need to pass

00:15:30,720 --> 00:15:34,480
kind of the same information

00:15:32,480 --> 00:15:35,839
the client id is just the name that they

00:15:34,480 --> 00:15:38,079
give to the consumer

00:15:35,839 --> 00:15:40,480
let's leave out the group id for now we

00:15:38,079 --> 00:15:42,079
will come back to this topic later

00:15:40,480 --> 00:15:43,920
the rest of the information is kind of

00:15:42,079 --> 00:15:44,800
the same as the producer where to find

00:15:43,920 --> 00:15:46,480
kafka

00:15:44,800 --> 00:15:48,959
how to connect using the ssl

00:15:46,480 --> 00:15:50,480
certificates and now how to deserialize

00:15:48,959 --> 00:15:51,519
the information that i was serializing

00:15:50,480 --> 00:15:54,560
before

00:15:51,519 --> 00:15:55,360
so let's create the consumer now that

00:15:54,560 --> 00:15:57,600
the consumer

00:15:55,360 --> 00:16:00,320
is created is ready i can check which

00:15:57,600 --> 00:16:01,839
topics are available in kafka

00:16:00,320 --> 00:16:04,000
and i can see that there are some

00:16:01,839 --> 00:16:06,079
internal topics together with a nice

00:16:04,000 --> 00:16:08,240
francesco pizza topic that i just

00:16:06,079 --> 00:16:10,480
created for this purpose

00:16:08,240 --> 00:16:12,240
now i can subscribe to it and confirm

00:16:10,480 --> 00:16:15,199
the subscription

00:16:12,240 --> 00:16:18,079
i subscribe to francesco pizza and now i

00:16:15,199 --> 00:16:20,560
start reading from it

00:16:18,079 --> 00:16:22,079
okay we started the thread that reads

00:16:20,560 --> 00:16:25,680
from kafka and we can immediately

00:16:22,079 --> 00:16:29,120
see two things the first one being that

00:16:25,680 --> 00:16:30,880
the thread never ends

00:16:29,120 --> 00:16:32,800
this is because we are creating now an

00:16:30,880 --> 00:16:35,440
event-driven application so we

00:16:32,800 --> 00:16:37,360
will be always there waiting for kafka

00:16:35,440 --> 00:16:39,600
to have a message ready for us

00:16:37,360 --> 00:16:40,639
so we will check with kafka hey do you

00:16:39,600 --> 00:16:42,880
have a message for me

00:16:40,639 --> 00:16:44,079
no we will wait and then we will check

00:16:42,880 --> 00:16:46,959
again

00:16:44,079 --> 00:16:47,360
the second thing that we can notice is

00:16:46,959 --> 00:16:51,120
that

00:16:47,360 --> 00:16:53,279
we produced an order for francesco

00:16:51,120 --> 00:16:55,680
requesting a pizza margarita but we

00:16:53,279 --> 00:16:58,160
don't see it on the consumer

00:16:55,680 --> 00:17:00,720
well this is because by default when you

00:16:58,160 --> 00:17:03,040
attach with a consumer to kafka

00:17:00,720 --> 00:17:04,720
you start reading from the point in time

00:17:03,040 --> 00:17:06,799
that you attach to kafka

00:17:04,720 --> 00:17:08,720
so since we attach with our consumer

00:17:06,799 --> 00:17:12,160
later than we produce our first

00:17:08,720 --> 00:17:13,919
message we are not going to read it

00:17:12,160 --> 00:17:16,160
this is the default behavior and you can

00:17:13,919 --> 00:17:18,480
change that but just

00:17:16,160 --> 00:17:19,839
remember that if you're attached to

00:17:18,480 --> 00:17:21,839
kafka without

00:17:19,839 --> 00:17:23,280
going back in history you will start

00:17:21,839 --> 00:17:24,480
reading from the point in time that you

00:17:23,280 --> 00:17:27,280
attached to it

00:17:24,480 --> 00:17:28,880
in order to provide you a demonstration

00:17:27,280 --> 00:17:30,400
that the wall pipeline producer and

00:17:28,880 --> 00:17:34,320
consumer is working

00:17:30,400 --> 00:17:37,840
i will now send another couple of events

00:17:34,320 --> 00:17:40,160
and this now it comes the most kind of

00:17:37,840 --> 00:17:43,120
problematic part of my talk

00:17:40,160 --> 00:17:43,919
because i have two new orders one from

00:17:43,120 --> 00:17:46,640
adele

00:17:43,919 --> 00:17:48,720
ordering a pizza y and another full mark

00:17:46,640 --> 00:17:52,080
or during a pizza with chocolate

00:17:48,720 --> 00:17:52,960
now um i believe you understood that i'm

00:17:52,080 --> 00:17:54,960
italian

00:17:52,960 --> 00:17:56,880
so ordering a pizza wire or a pizza with

00:17:54,960 --> 00:17:57,679
chocolate in italy is not the best thing

00:17:56,880 --> 00:18:00,000
that you can do

00:17:57,679 --> 00:18:01,440
however it's your personal choice you

00:18:00,000 --> 00:18:02,080
can do whatever you want i'm just

00:18:01,440 --> 00:18:05,039
suggesting

00:18:02,080 --> 00:18:05,520
if you come to italy try to avoid them

00:18:05,039 --> 00:18:08,160
so

00:18:05,520 --> 00:18:08,559
let's now send the orders and let's

00:18:08,160 --> 00:18:11,039
check

00:18:08,559 --> 00:18:13,520
if the pipeline is working by checking

00:18:11,039 --> 00:18:15,919
on the consumer side if we can see them

00:18:13,520 --> 00:18:16,799
so let's run the producer and

00:18:15,919 --> 00:18:19,520
immediately

00:18:16,799 --> 00:18:21,360
we see that we receive both a delay mark

00:18:19,520 --> 00:18:22,000
order so the wall pipeline producer and

00:18:21,360 --> 00:18:25,600
consumer

00:18:22,000 --> 00:18:27,679
is working now that we check the basics

00:18:25,600 --> 00:18:29,760
let's go back to a little bit more

00:18:27,679 --> 00:18:32,799
slides

00:18:29,760 --> 00:18:36,799
and let's talk now

00:18:32,799 --> 00:18:39,679
about the data in kafka so we said that

00:18:36,799 --> 00:18:41,039
kafka and the topics in kafka are up and

00:18:39,679 --> 00:18:43,600
only and immutable

00:18:41,039 --> 00:18:45,760
this means that we will put data there

00:18:43,600 --> 00:18:48,880
and once the data is there

00:18:45,760 --> 00:18:51,200
we cannot change it but of course

00:18:48,880 --> 00:18:52,640
we maybe don't want to store the data in

00:18:51,200 --> 00:18:55,120
kafka forever

00:18:52,640 --> 00:18:56,160
maybe we want only to use kafka storage

00:18:55,120 --> 00:18:58,880
for like

00:18:56,160 --> 00:19:00,240
the la the the latest six months of our

00:18:58,880 --> 00:19:03,039
data set

00:19:00,240 --> 00:19:04,720
for this we can use what are called

00:19:03,039 --> 00:19:07,039
topic retention policies

00:19:04,720 --> 00:19:08,320
so we can tell kafka keep the events in

00:19:07,039 --> 00:19:11,520
kafka

00:19:08,320 --> 00:19:12,400
either for for example six months three

00:19:11,520 --> 00:19:16,000
weeks

00:19:12,400 --> 00:19:19,760
two hours or if we want to be sure about

00:19:16,000 --> 00:19:22,000
how much disk is used is used we can say

00:19:19,760 --> 00:19:23,679
keep the data in kafka until the

00:19:22,000 --> 00:19:26,080
specific topic

00:19:23,679 --> 00:19:27,919
reaches for example 10 gigabytes and

00:19:26,080 --> 00:19:29,919
then delete the oldest chunk

00:19:27,919 --> 00:19:32,640
and let it grow again it reaches 10

00:19:29,919 --> 00:19:34,640
gigabytes and delete the old chunk

00:19:32,640 --> 00:19:36,960
you can also use both if you want to

00:19:34,640 --> 00:19:40,960
have better control over both

00:19:36,960 --> 00:19:44,480
disk and side and timing so

00:19:40,960 --> 00:19:48,000
let's now think a little bit more about

00:19:44,480 --> 00:19:52,000
the size and the size of

00:19:48,000 --> 00:19:55,280
the log the topic we know and i've

00:19:52,000 --> 00:19:58,559
just told you that kafka is a platform

00:19:55,280 --> 00:20:01,760
in order to that allow you to host

00:19:58,559 --> 00:20:04,400
a lot of events but if you think that

00:20:01,760 --> 00:20:05,200
we store events in a topic and that

00:20:04,400 --> 00:20:09,440
topic

00:20:05,200 --> 00:20:12,720
is stored in a broker this means that

00:20:09,440 --> 00:20:15,120
either we have to define the amount of

00:20:12,720 --> 00:20:17,280
events that we want to start based on

00:20:15,120 --> 00:20:19,039
the size of the disk of the broker

00:20:17,280 --> 00:20:22,640
or we have to size the disk of the

00:20:19,039 --> 00:20:24,159
broker depending on how many

00:20:22,640 --> 00:20:26,000
events we want to store in a precise

00:20:24,159 --> 00:20:28,240
topic which

00:20:26,000 --> 00:20:30,400
is a weird trade-off for a platform that

00:20:28,240 --> 00:20:33,200
aims to store a huge amount of data

00:20:30,400 --> 00:20:35,440
because in case we have a huge topic we

00:20:33,200 --> 00:20:37,120
will need huge disk sizes

00:20:35,440 --> 00:20:38,559
in order to store all the events related

00:20:37,120 --> 00:20:41,039
to the topic

00:20:38,559 --> 00:20:43,280
likely for us kafka doesn't really

00:20:41,039 --> 00:20:46,240
impose this trade-off

00:20:43,280 --> 00:20:46,640
between number of events or size of the

00:20:46,240 --> 00:20:49,600
the

00:20:46,640 --> 00:20:52,240
topic and disk size because it has the

00:20:49,600 --> 00:20:55,679
concept of topic partitions

00:20:52,240 --> 00:20:58,240
topic partition is a way of

00:20:55,679 --> 00:21:00,159
dividing the events of the same type

00:20:58,240 --> 00:21:03,440
belonging to the same topic

00:21:00,159 --> 00:21:05,360
into subtopics so if we go back to the

00:21:03,440 --> 00:21:09,280
analogy of my pizza orders

00:21:05,360 --> 00:21:11,280
i could divide the events pizza orders

00:21:09,280 --> 00:21:12,640
into different partitions depending for

00:21:11,280 --> 00:21:14,559
example from the

00:21:12,640 --> 00:21:16,960
restaurant receiving the order so it

00:21:14,559 --> 00:21:17,840
could have mario's order landing the

00:21:16,960 --> 00:21:20,880
blue partition

00:21:17,840 --> 00:21:22,960
luigi's order landing in the uh

00:21:20,880 --> 00:21:25,520
yellow partition in francesco's order

00:21:22,960 --> 00:21:28,960
landing in the red partition

00:21:25,520 --> 00:21:32,720
now if we go back to our

00:21:28,960 --> 00:21:36,000
cluster what is stored in each node

00:21:32,720 --> 00:21:37,360
is not the wall topic is just a

00:21:36,000 --> 00:21:40,159
partition

00:21:37,360 --> 00:21:41,200
so this means that the trade-off is not

00:21:40,159 --> 00:21:45,360
actually between the

00:21:41,200 --> 00:21:47,360
wall topic disk size and the

00:21:45,360 --> 00:21:49,440
sorry the wall topic size and the disk

00:21:47,360 --> 00:21:50,559
size of a broker but it is between just

00:21:49,440 --> 00:21:53,039
a partition

00:21:50,559 --> 00:21:54,320
and the this size of a broker this also

00:21:53,039 --> 00:21:56,640
means that if we

00:21:54,320 --> 00:21:58,640
know that we will have a huge topic with

00:21:56,640 --> 00:22:00,720
a huge amount of data

00:21:58,640 --> 00:22:03,120
but we have a smaller disk well we just

00:22:00,720 --> 00:22:05,679
need more partition to fit in

00:22:03,120 --> 00:22:06,559
and as you can see here also partitions

00:22:05,679 --> 00:22:09,039
are not stored

00:22:06,559 --> 00:22:10,880
once but are stored following the

00:22:09,039 --> 00:22:12,720
replication factor of configuration so

00:22:10,880 --> 00:22:13,679
this means that even here if we lose a

00:22:12,720 --> 00:22:16,720
node

00:22:13,679 --> 00:22:19,440
we are not going to lose any data

00:22:16,720 --> 00:22:20,880
now it's interesting to understand how

00:22:19,440 --> 00:22:22,880
you

00:22:20,880 --> 00:22:24,799
select a partition when you create a

00:22:22,880 --> 00:22:27,600
topic with multiple partitions

00:22:24,799 --> 00:22:28,320
well you do that usually in the usu

00:22:27,600 --> 00:22:31,600
using

00:22:28,320 --> 00:22:34,080
usually the key part of the message

00:22:31,600 --> 00:22:35,039
and what kafka does by default is it

00:22:34,080 --> 00:22:37,280
takes the key

00:22:35,039 --> 00:22:39,679
attaches it and takes the result of the

00:22:37,280 --> 00:22:42,000
hash to drive the partition selection

00:22:39,679 --> 00:22:42,960
ensuring that all messages having the

00:22:42,000 --> 00:22:45,679
same key will

00:22:42,960 --> 00:22:47,120
end up in the same partition you can

00:22:45,679 --> 00:22:49,600
however write your

00:22:47,120 --> 00:22:51,039
own custom partitioner or you can send

00:22:49,600 --> 00:22:53,440
messages without the key

00:22:51,039 --> 00:22:56,000
in that case kafka will select a

00:22:53,440 --> 00:22:58,799
partition in round robin fashion

00:22:56,000 --> 00:22:59,280
but you may want to think carefully

00:22:58,799 --> 00:23:01,679
about

00:22:59,280 --> 00:23:02,640
which partition you select when you send

00:23:01,679 --> 00:23:06,080
the message

00:23:02,640 --> 00:23:09,440
why is that well it's because ordering

00:23:06,080 --> 00:23:12,720
let me show you a little example we have

00:23:09,440 --> 00:23:14,960
our producer in python that produces

00:23:12,720 --> 00:23:17,600
data to a topic with two partitions

00:23:14,960 --> 00:23:18,320
and we have our consumer now we will

00:23:17,600 --> 00:23:20,880
have a really

00:23:18,320 --> 00:23:22,080
simple case where we have only three

00:23:20,880 --> 00:23:24,480
events

00:23:22,080 --> 00:23:25,200
the blue event first the yellow event

00:23:24,480 --> 00:23:27,760
second

00:23:25,200 --> 00:23:29,520
and the red event third when pushing

00:23:27,760 --> 00:23:31,840
those events to kafka

00:23:29,520 --> 00:23:33,200
the event number the blue event will

00:23:31,840 --> 00:23:34,960
land on partition zero

00:23:33,200 --> 00:23:36,799
the yellow event will land on partition

00:23:34,960 --> 00:23:38,720
1 and the red event will land on

00:23:36,799 --> 00:23:41,039
partition 0 again

00:23:38,720 --> 00:23:43,039
now when reading from kafka it could

00:23:41,039 --> 00:23:44,799
happen will not always be the case but

00:23:43,039 --> 00:23:46,960
it could happen that we will read the

00:23:44,799 --> 00:23:50,880
events in this sequence

00:23:46,960 --> 00:23:54,000
blue event first red event second

00:23:50,880 --> 00:23:54,720
yellow and third why is that well

00:23:54,000 --> 00:23:57,120
because

00:23:54,720 --> 00:23:59,120
when we start using partitioning we have

00:23:57,120 --> 00:24:02,480
to give up on global ordering

00:23:59,120 --> 00:24:05,200
when we use partition kafka ensures that

00:24:02,480 --> 00:24:06,400
the correct ordering of events only per

00:24:05,200 --> 00:24:09,200
partition

00:24:06,400 --> 00:24:11,360
so when we start using partitions we

00:24:09,200 --> 00:24:12,640
have to think about for which order we

00:24:11,360 --> 00:24:14,960
care about the

00:24:12,640 --> 00:24:15,760
related ordering in the case of pizza

00:24:14,960 --> 00:24:17,360
orders

00:24:15,760 --> 00:24:19,520
it's a good example to use the

00:24:17,360 --> 00:24:23,520
restaurant name because we

00:24:19,520 --> 00:24:26,960
care about if client a made the order

00:24:23,520 --> 00:24:29,919
before client b at the same restaurant

00:24:26,960 --> 00:24:32,240
but we don't care if client a made the

00:24:29,919 --> 00:24:35,039
order before or after client b on a

00:24:32,240 --> 00:24:38,640
different restaurant

00:24:35,039 --> 00:24:40,720
so we understood that partitioning

00:24:38,640 --> 00:24:42,799
is good because allow us to have a

00:24:40,720 --> 00:24:46,080
better trade-off between

00:24:42,799 --> 00:24:48,240
topic size and this space on the other

00:24:46,080 --> 00:24:50,080
side with partitioning we have to give

00:24:48,240 --> 00:24:52,480
up on global ordering

00:24:50,080 --> 00:24:54,480
in order and kafka only ensures the

00:24:52,480 --> 00:24:58,240
correct ordering per partition

00:24:54,480 --> 00:25:00,559
but if we think about a topic

00:24:58,240 --> 00:25:02,480
we can safely assume that he is just

00:25:00,559 --> 00:25:06,000
kind of a thread

00:25:02,480 --> 00:25:09,520
writing one event after the other

00:25:06,000 --> 00:25:10,559
as messages so we can also somehow

00:25:09,520 --> 00:25:13,600
assume that

00:25:10,559 --> 00:25:16,240
the throughput of writing to kafka

00:25:13,600 --> 00:25:18,640
is given by this thread writing one

00:25:16,240 --> 00:25:20,240
event after the other

00:25:18,640 --> 00:25:22,720
on the other side if we have more

00:25:20,240 --> 00:25:24,720
partition we can scale out because

00:25:22,720 --> 00:25:26,480
now with more partition we have more

00:25:24,720 --> 00:25:29,279
independent threads

00:25:26,480 --> 00:25:29,919
that can write in parallel so we can

00:25:29,279 --> 00:25:33,600
have

00:25:29,919 --> 00:25:37,279
much many producer

00:25:33,600 --> 00:25:40,960
write into kafka and also we can have

00:25:37,279 --> 00:25:42,960
many consumer reading from kafka

00:25:40,960 --> 00:25:45,200
still when we have many consumers

00:25:42,960 --> 00:25:47,440
reading from kafka we want to

00:25:45,200 --> 00:25:49,039
read all the data which are present in

00:25:47,440 --> 00:25:51,120
the precise topic

00:25:49,039 --> 00:25:52,960
but probably we don't want to read the

00:25:51,120 --> 00:25:54,960
same message twice

00:25:52,960 --> 00:25:57,039
we don't for example want to make the

00:25:54,960 --> 00:25:59,200
same pizza order twice

00:25:57,039 --> 00:26:00,480
in order to avoid that situation what

00:25:59,200 --> 00:26:04,640
kafka does is

00:26:00,480 --> 00:26:07,840
that it assigns to each consumer

00:26:04,640 --> 00:26:10,799
a non-overlapping subset of partitions

00:26:07,840 --> 00:26:11,840
if these words doesn't make much sense

00:26:10,799 --> 00:26:14,559
well let me show you

00:26:11,840 --> 00:26:16,640
in our example we have three partitions

00:26:14,559 --> 00:26:19,039
blue yellow and red and two consumers

00:26:16,640 --> 00:26:20,720
what kafka will do for example is to

00:26:19,039 --> 00:26:21,440
assign the blue partition to consumer

00:26:20,720 --> 00:26:23,360
one

00:26:21,440 --> 00:26:25,120
and the yellow and red partition to

00:26:23,360 --> 00:26:27,919
consumer too

00:26:25,120 --> 00:26:28,720
now again after all this talk it's time

00:26:27,919 --> 00:26:31,760
to

00:26:28,720 --> 00:26:34,240
show a little bit of demo so let's go

00:26:31,760 --> 00:26:37,600
back to our notebook

00:26:34,240 --> 00:26:41,279
and let's check out now

00:26:37,600 --> 00:26:42,480
the partitions let's create a new

00:26:41,279 --> 00:26:46,480
producer

00:26:42,480 --> 00:26:48,320
there we are we create a new producer

00:26:46,480 --> 00:26:49,679
is not really needed but let's create

00:26:48,320 --> 00:26:52,880
for the sake of creating

00:26:49,679 --> 00:26:54,880
and now we will create a new topic

00:26:52,880 --> 00:26:56,720
and we will interact with kafka admin

00:26:54,880 --> 00:26:58,720
client because for this new topic

00:26:56,720 --> 00:27:00,960
we want to set the number of partitions

00:26:58,720 --> 00:27:01,760
to choose so we create the kafka admin

00:27:00,960 --> 00:27:03,520
client

00:27:01,760 --> 00:27:05,120
and then we create a new topic called

00:27:03,520 --> 00:27:06,080
the same francesco pizza underscore

00:27:05,120 --> 00:27:09,440
partition

00:27:06,080 --> 00:27:13,679
with two partitions

00:27:09,440 --> 00:27:16,720
let's run this okay we didn't have any

00:27:13,679 --> 00:27:18,640
errors so the topic is created now

00:27:16,720 --> 00:27:22,559
before start sending data

00:27:18,640 --> 00:27:26,000
let me create two consumers

00:27:22,559 --> 00:27:28,320
let's go back to my set of notebooks

00:27:26,000 --> 00:27:30,640
i can create the first consumer over

00:27:28,320 --> 00:27:30,640
here

00:27:30,960 --> 00:27:37,600
and just to show you i'm using kind of

00:27:34,159 --> 00:27:37,600
the same settings as before

00:27:38,320 --> 00:27:42,080
let me start the first consumer and now

00:27:41,120 --> 00:27:46,240
let me start

00:27:42,080 --> 00:27:46,240
the second consumer down there

00:27:47,600 --> 00:27:54,320
there we are let's close this okay

00:27:51,039 --> 00:27:57,440
so i have two consumer top

00:27:54,320 --> 00:28:00,559
and bottom reading from kafka

00:27:57,440 --> 00:28:03,679
i have a topic with two partitions

00:28:00,559 --> 00:28:06,880
now when i'm sending data

00:28:03,679 --> 00:28:09,279
since i have two consumers i'm sending

00:28:06,880 --> 00:28:10,000
two messages with a slightly different

00:28:09,279 --> 00:28:13,039
key

00:28:10,000 --> 00:28:15,520
the top one has the key ed zero

00:28:13,039 --> 00:28:17,679
the bottom one has the key ed1 if

00:28:15,520 --> 00:28:20,000
everything works as i told you so far

00:28:17,679 --> 00:28:22,880
i would expect that for example the top

00:28:20,000 --> 00:28:24,399
message lands in partition zero

00:28:22,880 --> 00:28:27,039
and the bottom message lands in

00:28:24,399 --> 00:28:29,279
partition one on the consumer side i

00:28:27,039 --> 00:28:31,440
expect since i have two consumers

00:28:29,279 --> 00:28:32,640
that one will only consume data from

00:28:31,440 --> 00:28:34,559
partition zero

00:28:32,640 --> 00:28:35,919
and the other one will consume only data

00:28:34,559 --> 00:28:39,760
from partition one

00:28:35,919 --> 00:28:43,039
so let's check if all the theory is true

00:28:39,760 --> 00:28:44,159
let's send the data set and exactly as i

00:28:43,039 --> 00:28:46,960
told you so far

00:28:44,159 --> 00:28:47,760
i have the top consumer reading only one

00:28:46,960 --> 00:28:49,600
of the message

00:28:47,760 --> 00:28:51,200
and the bottom consumer reading only the

00:28:49,600 --> 00:28:54,399
other if we check

00:28:51,200 --> 00:28:56,000
what those two little fields mean this

00:28:54,399 --> 00:28:58,240
means that the top consumer is reading

00:28:56,000 --> 00:29:01,600
from partition zero offset 0

00:28:58,240 --> 00:29:03,760
first message of partition 0 the second

00:29:01,600 --> 00:29:04,559
consumer is reading from partition 1

00:29:03,760 --> 00:29:07,760
offset 0

00:29:04,559 --> 00:29:11,039
first message of partition 1. now if

00:29:07,760 --> 00:29:12,880
on the producer side i send couple more

00:29:11,039 --> 00:29:16,960
messages

00:29:12,880 --> 00:29:21,360
reusing the same keys i will expect

00:29:16,960 --> 00:29:23,039
that the uh order from mac ordering a

00:29:21,360 --> 00:29:25,360
nice pizza with banana

00:29:23,039 --> 00:29:26,320
since it's reusing the same key as the

00:29:25,360 --> 00:29:28,720
frame holder

00:29:26,320 --> 00:29:29,360
will land on the same partition and the

00:29:28,720 --> 00:29:32,720
same for

00:29:29,360 --> 00:29:34,000
jan and adele so expect frank sorry i

00:29:32,720 --> 00:29:36,559
expect mac

00:29:34,000 --> 00:29:38,080
to be received by the same consumer that

00:29:36,559 --> 00:29:40,080
received the order for frank

00:29:38,080 --> 00:29:41,600
and the same for jan and nadella let's

00:29:40,080 --> 00:29:44,640
try this out

00:29:41,600 --> 00:29:46,799
let's run this as expected

00:29:44,640 --> 00:29:48,880
the top consumer is reading from

00:29:46,799 --> 00:29:50,080
partition zero offset one second message

00:29:48,880 --> 00:29:52,720
of partition zero

00:29:50,080 --> 00:29:53,520
or reading the order for mac and the

00:29:52,720 --> 00:29:56,799
bottom one

00:29:53,520 --> 00:29:58,399
same thing jan and adele so all working

00:29:56,799 --> 00:30:03,440
as expected

00:29:58,399 --> 00:30:03,440
now let's go to a little bit more slides

00:30:05,200 --> 00:30:11,279
so far we saw something very simple

00:30:08,640 --> 00:30:14,399
one or more threads of a producer one or

00:30:11,279 --> 00:30:17,760
more threads of a consumer

00:30:14,399 --> 00:30:18,880
however when we read a message from

00:30:17,760 --> 00:30:20,480
kafka

00:30:18,880 --> 00:30:22,880
kafka is not going to delete that

00:30:20,480 --> 00:30:23,679
message making it available for other

00:30:22,880 --> 00:30:27,520
application

00:30:23,679 --> 00:30:30,640
to read it again if we go back to our

00:30:27,520 --> 00:30:33,279
pizza analogy we could have

00:30:30,640 --> 00:30:35,760
our topic with in this case true

00:30:33,279 --> 00:30:37,679
consumer that could be our pizza makers

00:30:35,760 --> 00:30:38,960
they want to consume all the pizza

00:30:37,679 --> 00:30:42,399
orders

00:30:38,960 --> 00:30:44,399
from the topic but they don't want to

00:30:42,399 --> 00:30:46,799
read the same order twice they don't

00:30:44,399 --> 00:30:48,880
want to make the same pizza twice

00:30:46,799 --> 00:30:51,840
on the other side since the pizza order

00:30:48,880 --> 00:30:53,679
is there i could have my billing person

00:30:51,840 --> 00:30:56,159
that will want to receive a separate

00:30:53,679 --> 00:30:57,679
copy of all the pizza orders in order to

00:30:56,159 --> 00:31:00,240
make the bills

00:30:57,679 --> 00:31:00,960
and this pizza this billing person will

00:31:00,240 --> 00:31:03,120
want to

00:31:00,960 --> 00:31:06,799
read the topic at its own pace that has

00:31:03,120 --> 00:31:09,360
nothing to do with the two pizza makers

00:31:06,799 --> 00:31:11,279
with kafka it's extremely easy to do

00:31:09,360 --> 00:31:12,720
that because it has the concept of

00:31:11,279 --> 00:31:14,640
consumer group

00:31:12,720 --> 00:31:16,399
so i just need to define the two

00:31:14,640 --> 00:31:18,159
consumer groups the two pizza makers

00:31:16,399 --> 00:31:20,960
part of the same consumer group

00:31:18,159 --> 00:31:23,679
and then i will define my billing person

00:31:20,960 --> 00:31:26,080
as part of the new consumer group

00:31:23,679 --> 00:31:27,039
if i do this setting well kafka will

00:31:26,080 --> 00:31:30,399
send

00:31:27,039 --> 00:31:30,880
another copy of the same data in the

00:31:30,399 --> 00:31:33,200
topic

00:31:30,880 --> 00:31:33,919
also to the other consumer and if you

00:31:33,200 --> 00:31:35,600
remember

00:31:33,919 --> 00:31:38,080
at the beginning when i created the

00:31:35,600 --> 00:31:40,320
first consumer i told you

00:31:38,080 --> 00:31:41,519
well the group id was something that we

00:31:40,320 --> 00:31:43,760
will discuss later

00:31:41,519 --> 00:31:47,039
well that group id is actually how you

00:31:43,760 --> 00:31:50,080
define different consumer groups

00:31:47,039 --> 00:31:52,240
so just a string lets you have

00:31:50,080 --> 00:31:54,159
multiple consumer working one against

00:31:52,240 --> 00:31:56,640
the other not to consume all messages

00:31:54,159 --> 00:31:58,000
or if you change the string you will

00:31:56,640 --> 00:32:01,679
have the consumers

00:31:58,000 --> 00:32:03,279
defined as different applications

00:32:01,679 --> 00:32:05,360
the last bit that i want to talk you

00:32:03,279 --> 00:32:09,039
about is the fact that

00:32:05,360 --> 00:32:11,600
so far we basically wrote the call to do

00:32:09,039 --> 00:32:12,799
everything we wrote the code to push

00:32:11,600 --> 00:32:15,600
data to kafka

00:32:12,799 --> 00:32:16,799
we wrote the code to read the data from

00:32:15,600 --> 00:32:19,919
kafka

00:32:16,799 --> 00:32:21,760
on the other side especially if you are

00:32:19,919 --> 00:32:22,720
if kafka is not the first tool in your

00:32:21,760 --> 00:32:24,880
company

00:32:22,720 --> 00:32:26,159
you will have other tools in your data

00:32:24,880 --> 00:32:28,720
ecosystem

00:32:26,159 --> 00:32:29,840
and you will want to integrate kafka

00:32:28,720 --> 00:32:31,840
with those tools

00:32:29,840 --> 00:32:34,399
and probably you don't want to reinvent

00:32:31,840 --> 00:32:36,240
the wheel again you don't want to code

00:32:34,399 --> 00:32:38,399
all the interactions between any of your

00:32:36,240 --> 00:32:41,279
data tools and kafka

00:32:38,399 --> 00:32:42,720
it would be really nice if a pre-built

00:32:41,279 --> 00:32:45,840
framework could exist

00:32:42,720 --> 00:32:46,399
in order to make the integration really

00:32:45,840 --> 00:32:48,240
easy

00:32:46,399 --> 00:32:49,519
and reliable well let me tell you

00:32:48,240 --> 00:32:51,840
another secret

00:32:49,519 --> 00:32:53,200
that framework exists and is called

00:32:51,840 --> 00:32:55,440
kafka connect

00:32:53,200 --> 00:32:57,279
with kafka connect i can start

00:32:55,440 --> 00:33:00,720
integrating kafka with

00:32:57,279 --> 00:33:03,039
any source or target available

00:33:00,720 --> 00:33:05,760
for example i have my data in postgres

00:33:03,039 --> 00:33:06,720
database or in cassandra in google pub

00:33:05,760 --> 00:33:09,120
sub

00:33:06,720 --> 00:33:10,159
i can use kafka connect to ingest those

00:33:09,120 --> 00:33:12,720
that data

00:33:10,159 --> 00:33:14,159
into topics on the other side i have

00:33:12,720 --> 00:33:17,200
data

00:33:14,159 --> 00:33:19,679
installed in kafka topics and i want to

00:33:17,200 --> 00:33:21,200
send this data set to another technology

00:33:19,679 --> 00:33:23,440
in my company ecosystem

00:33:21,200 --> 00:33:24,960
again kafka connect enables you to do

00:33:23,440 --> 00:33:27,519
that

00:33:24,960 --> 00:33:28,880
kafka connect also enables you to evolve

00:33:27,519 --> 00:33:31,120
existing application

00:33:28,880 --> 00:33:32,399
so if you have your old application in

00:33:31,120 --> 00:33:34,960
python that was

00:33:32,399 --> 00:33:36,720
right into a database and you want to

00:33:34,960 --> 00:33:38,399
take this application into the kind of

00:33:36,720 --> 00:33:40,880
event-driven world

00:33:38,399 --> 00:33:41,840
well you want to do that but still you

00:33:40,880 --> 00:33:44,399
don't

00:33:41,840 --> 00:33:45,200
want to change the working code you want

00:33:44,399 --> 00:33:47,440
to leave

00:33:45,200 --> 00:33:49,039
the application and the database as they

00:33:47,440 --> 00:33:50,640
are working now

00:33:49,039 --> 00:33:53,039
what you can do in order to include

00:33:50,640 --> 00:33:54,640
kafka in the picture is to use

00:33:53,039 --> 00:33:56,720
some sort of change data capture

00:33:54,640 --> 00:33:58,720
mechanism that will track any changes

00:33:56,720 --> 00:34:00,000
happening in a table or a set of tables

00:33:58,720 --> 00:34:03,360
in the database and propagate

00:34:00,000 --> 00:34:05,120
those as messages in a kafka topic

00:34:03,360 --> 00:34:06,799
and you can do that easily with kafka

00:34:05,120 --> 00:34:09,440
connect

00:34:06,799 --> 00:34:10,720
also kafka connect allows you to

00:34:09,440 --> 00:34:12,720
distribute events

00:34:10,720 --> 00:34:14,720
so once you have your application

00:34:12,720 --> 00:34:17,520
writing data to a kafka topic

00:34:14,720 --> 00:34:19,440
and you have for example your team a

00:34:17,520 --> 00:34:20,240
that wants the data in a particular gdbc

00:34:19,440 --> 00:34:22,320
database

00:34:20,240 --> 00:34:23,839
well you can send a copy of that topic

00:34:22,320 --> 00:34:27,200
data to that database

00:34:23,839 --> 00:34:27,760
you have another team that wants another

00:34:27,200 --> 00:34:31,119
copy

00:34:27,760 --> 00:34:32,000
in bigquery it's just another kafka

00:34:31,119 --> 00:34:35,119
connect

00:34:32,000 --> 00:34:37,760
you want to use um

00:34:35,119 --> 00:34:38,720
s3 storage for long-term term storage of

00:34:37,760 --> 00:34:40,560
your data set

00:34:38,720 --> 00:34:42,000
well again it's just another kafka

00:34:40,560 --> 00:34:44,639
connect

00:34:42,000 --> 00:34:46,079
the beauty of connect is that it makes

00:34:44,639 --> 00:34:49,599
all this

00:34:46,079 --> 00:34:52,320
push and pull operation available just

00:34:49,599 --> 00:34:53,599
as config file and a thread or multiple

00:34:52,320 --> 00:34:55,760
threads of kafka connect

00:34:53,599 --> 00:34:57,440
the other beauty that i will show you

00:34:55,760 --> 00:34:59,599
now is that with ivan you have also

00:34:57,440 --> 00:35:02,960
kafka connect as a managed service

00:34:59,599 --> 00:35:06,320
so let me show you quickly

00:35:02,960 --> 00:35:08,720
if we go back to our notebook let's now

00:35:06,320 --> 00:35:09,760
get rid of a little bit of things that

00:35:08,720 --> 00:35:14,800
are not useful

00:35:09,760 --> 00:35:14,800
anymore and let me create

00:35:15,040 --> 00:35:20,000
uh new producer again

00:35:18,480 --> 00:35:22,400
just for the sake of creating another

00:35:20,000 --> 00:35:24,400
new producer and this time

00:35:22,400 --> 00:35:25,760
i will not send only francesco is

00:35:24,400 --> 00:35:28,880
ordering a pizza margarita

00:35:25,760 --> 00:35:29,200
but okay together with that payload i

00:35:28,880 --> 00:35:31,920
will

00:35:29,200 --> 00:35:33,359
also send some information about how the

00:35:31,920 --> 00:35:37,040
message is structured

00:35:33,359 --> 00:35:40,880
so i will tell to kafka look that the

00:35:37,040 --> 00:35:43,599
key is composed by an integer called id

00:35:40,880 --> 00:35:44,160
and the value is composed by two strings

00:35:43,599 --> 00:35:45,760
one

00:35:44,160 --> 00:35:47,200
containing the name of the person

00:35:45,760 --> 00:35:50,640
ordering the pizza and the other

00:35:47,200 --> 00:35:52,720
is the pizza itself why i'm doing that

00:35:50,640 --> 00:35:54,320
well because i want to make the life

00:35:52,720 --> 00:35:56,560
easier for kafka connect

00:35:54,320 --> 00:35:58,160
to be able to understand what's the

00:35:56,560 --> 00:36:02,320
structure of each message

00:35:58,160 --> 00:36:04,640
and populate guess what a postgres

00:36:02,320 --> 00:36:05,680
table with the content of this kafka

00:36:04,640 --> 00:36:09,599
topic

00:36:05,680 --> 00:36:12,960
so let's define the key and the value

00:36:09,599 --> 00:36:14,400
and now i'm sending three nice messages

00:36:12,960 --> 00:36:16,079
to kafka

00:36:14,400 --> 00:36:17,760
i'm sending both the schema and the

00:36:16,079 --> 00:36:19,359
payload as i said before and i'm sending

00:36:17,760 --> 00:36:21,760
three messages one for

00:36:19,359 --> 00:36:22,400
frank ordering a piece of margarita one

00:36:21,760 --> 00:36:24,320
for dan

00:36:22,400 --> 00:36:27,920
ordering a pizza with fries and one for

00:36:24,320 --> 00:36:31,920
jan ordering a pizza with mushrooms

00:36:27,920 --> 00:36:34,960
so the data now is in a kafka topic

00:36:31,920 --> 00:36:35,440
let's check out kafka connect i told you

00:36:34,960 --> 00:36:37,599
before

00:36:35,440 --> 00:36:39,280
kafka connect if you ever manage service

00:36:37,599 --> 00:36:41,920
like the one in ivan

00:36:39,280 --> 00:36:43,760
you just need a config file that tells

00:36:41,920 --> 00:36:45,440
which is the topic that you want to

00:36:43,760 --> 00:36:47,440
take the data from and which is for

00:36:45,440 --> 00:36:49,440
example your postgres target

00:36:47,440 --> 00:36:51,760
let me show you the config file in our

00:36:49,440 --> 00:36:51,760
case

00:36:51,839 --> 00:36:56,720
if we check the kafka connect setup this

00:36:54,800 --> 00:36:59,520
is all the information that we have to

00:36:56,720 --> 00:37:01,599
give to kafka connect so we are saying

00:36:59,520 --> 00:37:04,640
that we want to take the data from

00:37:01,599 --> 00:37:05,680
the francesco pizza schema topic and we

00:37:04,640 --> 00:37:08,400
want to create

00:37:05,680 --> 00:37:10,320
a kafka connect connector called sync

00:37:08,400 --> 00:37:13,040
kafka postgres

00:37:10,320 --> 00:37:15,760
which is a jdbc sync connector we are

00:37:13,040 --> 00:37:18,320
syncing the data to a jdbc

00:37:15,760 --> 00:37:21,040
database which is a postgres database we

00:37:18,320 --> 00:37:25,359
are also telling that the value

00:37:21,040 --> 00:37:28,160
what we store in the value is a json and

00:37:25,359 --> 00:37:30,400
this is the url that we are going to use

00:37:28,160 --> 00:37:33,440
in order to connect

00:37:30,400 --> 00:37:34,240
to postgres using a very secure new pg

00:37:33,440 --> 00:37:37,280
user

00:37:34,240 --> 00:37:41,200
and new password password123s password

00:37:37,280 --> 00:37:41,839
so let me copy this json configuration

00:37:41,200 --> 00:37:45,920
file

00:37:41,839 --> 00:37:50,880
and let me now go to my

00:37:45,920 --> 00:37:54,480
ivan services in the kafka service

00:37:50,880 --> 00:37:54,480
now i can go to the connectors

00:37:54,560 --> 00:37:57,760
and create a new connector

00:37:59,680 --> 00:38:03,200
this is fetching now the list of

00:38:01,440 --> 00:38:05,040
available connectors and i'm selecting

00:38:03,200 --> 00:38:07,680
the jdbc sync

00:38:05,040 --> 00:38:08,400
now here i can see a list of fields that

00:38:07,680 --> 00:38:10,560
i could fill

00:38:08,400 --> 00:38:11,680
manually all things have my json

00:38:10,560 --> 00:38:14,800
configuration file

00:38:11,680 --> 00:38:17,040
i can copy and paste

00:38:14,800 --> 00:38:18,000
in here and click apply and this will

00:38:17,040 --> 00:38:21,599
parse and fill

00:38:18,000 --> 00:38:23,520
all the parameters for me so now i can

00:38:21,599 --> 00:38:25,200
create the connector but before doing

00:38:23,520 --> 00:38:25,680
that i want to show you that i'm not

00:38:25,200 --> 00:38:27,599
lying

00:38:25,680 --> 00:38:28,880
so i want to show you the postgres

00:38:27,599 --> 00:38:32,079
database which is

00:38:28,880 --> 00:38:36,000
behind so i have my berlin buzzwords

00:38:32,079 --> 00:38:39,280
database i'm connecting to it

00:38:36,000 --> 00:38:42,880
i have my default database

00:38:39,280 --> 00:38:46,800
the publish public schema which has

00:38:42,880 --> 00:38:51,920
no tables so now let me go back to

00:38:46,800 --> 00:38:51,920
my console and create my new connector

00:38:52,560 --> 00:38:56,640
the new connector is created and now is

00:38:55,520 --> 00:39:00,079
running

00:38:56,640 --> 00:39:03,359
all good let's go and check if something

00:39:00,079 --> 00:39:07,119
happens happen on the database so let me

00:39:03,359 --> 00:39:07,119
refresh now the list of tables

00:39:09,040 --> 00:39:15,839
oops not this one

00:39:17,440 --> 00:39:22,480
just a second the gui is doing tricks on

00:39:20,240 --> 00:39:22,480
me

00:39:26,000 --> 00:39:31,760
let's refresh the table

00:39:29,520 --> 00:39:31,760
no

00:39:40,079 --> 00:39:45,760
yeah sorry this is the classic demo

00:39:43,280 --> 00:39:45,760
effect

00:39:54,480 --> 00:40:02,480
now wait um

00:39:58,160 --> 00:40:05,599
no and

00:40:02,480 --> 00:40:07,920
no okay

00:40:05,599 --> 00:40:09,680
let me try now to refresh the list of

00:40:07,920 --> 00:40:10,560
tables and i have my francesco pizza

00:40:09,680 --> 00:40:14,560
schema table

00:40:10,560 --> 00:40:17,280
if i double click on it i can see that

00:40:14,560 --> 00:40:17,280
in the data

00:40:18,000 --> 00:40:22,880
i have my three rows as expected the

00:40:20,480 --> 00:40:26,480
same three rows that i sent before

00:40:22,880 --> 00:40:29,200
with my python producer now if i go back

00:40:26,480 --> 00:40:33,040
to my python producer

00:40:29,200 --> 00:40:35,920
and now i send another record with

00:40:33,040 --> 00:40:37,920
giuseppe ordering a nice pizza y

00:40:35,920 --> 00:40:39,200
this is now produced let me go back to

00:40:37,920 --> 00:40:42,480
the database

00:40:39,200 --> 00:40:43,599
and now i now try to refresh and i

00:40:42,480 --> 00:40:46,319
immediately see

00:40:43,599 --> 00:40:47,119
also giuseppe holder appearing in the

00:40:46,319 --> 00:40:50,160
database

00:40:47,119 --> 00:40:52,079
so what kafka connect did is

00:40:50,160 --> 00:40:53,599
it took the data from the topic it went

00:40:52,079 --> 00:40:55,920
to postgres

00:40:53,599 --> 00:40:57,920
it found out that there was no target

00:40:55,920 --> 00:41:00,079
table it created the target table and it

00:40:57,920 --> 00:41:02,319
started populating the target table

00:41:00,079 --> 00:41:04,560
and now every time there is a new event

00:41:02,319 --> 00:41:08,240
a new record in the kafka topic

00:41:04,560 --> 00:41:12,480
it will send it to the postgres table

00:41:08,240 --> 00:41:15,680
so in order to go back to a little bit

00:41:12,480 --> 00:41:17,200
understand what we told tonight

00:41:15,680 --> 00:41:18,960
we understood a lot of things we

00:41:17,200 --> 00:41:20,880
understood why create

00:41:18,960 --> 00:41:22,640
event driven application is crucial what

00:41:20,880 --> 00:41:24,960
is kafka role in this

00:41:22,640 --> 00:41:26,960
how we can use python in order to

00:41:24,960 --> 00:41:30,400
interact with kafka to create

00:41:26,960 --> 00:41:31,119
producers consumers to have multiple

00:41:30,400 --> 00:41:32,560
consumers

00:41:31,119 --> 00:41:34,640
working one against the other and using

00:41:32,560 --> 00:41:36,800
partitions and also to declare

00:41:34,640 --> 00:41:39,200
multiple applications and we finished

00:41:36,800 --> 00:41:42,560
with some nice concept about

00:41:39,200 --> 00:41:45,599
kafka connect that allow us to integrate

00:41:42,560 --> 00:41:48,160
kafka within the existing data ecosystem

00:41:45,599 --> 00:41:50,000
if you want to have some more resources

00:41:48,160 --> 00:41:53,040
well let me give you them

00:41:50,000 --> 00:41:54,800
first of all it's my twitter handle you

00:41:53,040 --> 00:41:56,400
can reach out to me if you have any

00:41:54,800 --> 00:42:00,720
questions regarding

00:41:56,400 --> 00:42:03,599
kafka python or pizza choices

00:42:00,720 --> 00:42:05,440
then uh you have the first link which is

00:42:03,599 --> 00:42:06,880
a github repository containing the

00:42:05,440 --> 00:42:07,440
notebooks that i've been showing you

00:42:06,880 --> 00:42:09,359
today

00:42:07,440 --> 00:42:11,520
so you will be able to create the

00:42:09,359 --> 00:42:13,200
resources automatically in ivan

00:42:11,520 --> 00:42:15,200
start playing with kafka and understand

00:42:13,200 --> 00:42:17,359
the beauties of kafka within the python

00:42:15,200 --> 00:42:20,560
notebooks

00:42:17,359 --> 00:42:23,119
then if you want to try kafka

00:42:20,560 --> 00:42:25,359
but you don't have any streaming data

00:42:23,119 --> 00:42:29,040
set well i also created for you

00:42:25,359 --> 00:42:32,560
a nice fake pizza

00:42:29,040 --> 00:42:35,040
creation fake pizza orders

00:42:32,560 --> 00:42:35,920
producer that will start producing fake

00:42:35,040 --> 00:42:38,560
pizza orders

00:42:35,920 --> 00:42:39,280
in a streaming mode for you the last bit

00:42:38,560 --> 00:42:41,359
is

00:42:39,280 --> 00:42:42,800
if you want to try kafka but you don't

00:42:41,359 --> 00:42:45,920
have kafka

00:42:42,800 --> 00:42:47,520
well as said before ivan my company

00:42:45,920 --> 00:42:49,760
offers that as a managed service

00:42:47,520 --> 00:42:51,280
and we offer not only kafka but also as

00:42:49,760 --> 00:42:54,720
said kafka connect and

00:42:51,280 --> 00:42:57,520
mirror maker so just come there we have

00:42:54,720 --> 00:42:59,520
a nice free trial that lasts one month

00:42:57,520 --> 00:43:01,280
just check us out and please let me know

00:42:59,520 --> 00:43:03,839
what you think about it

00:43:01,280 --> 00:43:06,800
i hope in this like 40 minutes i gave

00:43:03,839 --> 00:43:09,200
you an idea of event driven applications

00:43:06,800 --> 00:43:11,200
python and kafka if you have any

00:43:09,200 --> 00:43:15,280
questions i'm here to

00:43:11,200 --> 00:43:15,280
reply to all of them thank you very much

00:43:16,400 --> 00:43:20,480
thank you francesco uh we have one

00:43:18,160 --> 00:43:22,319
question the first is um

00:43:20,480 --> 00:43:23,920
i saw you use jupiter notebooks in your

00:43:22,319 --> 00:43:26,560
demo are there other tools that you like

00:43:23,920 --> 00:43:29,760
to use with kafka

00:43:26,560 --> 00:43:33,119
um yeah so there is a

00:43:29,760 --> 00:43:36,800
really huge ecosystem around kafka

00:43:33,119 --> 00:43:39,040
one which i like is kafka cat

00:43:36,800 --> 00:43:41,119
which is a tool that allows you to

00:43:39,040 --> 00:43:43,599
basically from the common line

00:43:41,119 --> 00:43:45,520
to interact with kafka so you can start

00:43:43,599 --> 00:43:47,040
producing data to kafka you can start

00:43:45,520 --> 00:43:50,880
reading data from kafka and

00:43:47,040 --> 00:43:52,319
is especially useful when you start with

00:43:50,880 --> 00:43:54,079
kafka

00:43:52,319 --> 00:43:56,560
especially when you start writing your

00:43:54,079 --> 00:43:58,560
own code against kafka because you could

00:43:56,560 --> 00:44:00,960
as i said earlier on for example you can

00:43:58,560 --> 00:44:03,599
start reading from kafka

00:44:00,960 --> 00:44:05,599
but you miss all the messages that were

00:44:03,599 --> 00:44:06,560
sent before you attached to kafka with a

00:44:05,599 --> 00:44:08,960
consumer

00:44:06,560 --> 00:44:10,240
with kafka with kafka cat allows you

00:44:08,960 --> 00:44:13,200
with few

00:44:10,240 --> 00:44:14,480
flags and fields to fix all these items

00:44:13,200 --> 00:44:17,599
so you can actually

00:44:14,480 --> 00:44:19,760
use it in order to understand if your

00:44:17,599 --> 00:44:22,480
code is doing what you're expecting

00:44:19,760 --> 00:44:37,839
if the data looks as you expect and so

00:44:22,480 --> 00:44:37,839
on and so forth

00:44:43,200 --> 00:44:45,280

YouTube URL: https://www.youtube.com/watch?v=uXd0CBAjnqw


