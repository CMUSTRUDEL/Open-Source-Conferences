Title: Chinmay Soman – Faster Analytics for Fast Data with Apache Pinot and Flink SQL
Publication date: 2021-06-29
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	OLAP data stores like Apache Pinot are emerging to serve low-latency analytical queries at web scale. With its columnar data format and rich indexing strategies, Pinot is a perfect fit for running complex, interactive queries on multi-dimensional data within milliseconds. In some cases, though, streaming data will require non-trivial pre-processing that is not supported in Pinot, like joins and pre-aggregations. What then? 

In this talk, we’ll cover the benefits of combining Pinot and stream processing with Flink SQL to power near real-time OLAP use cases, and build a simple demo to analyze streaming Twitch data (#meta) — from ingestion to visualization!

Speaker:
Chinmay Soman – https://2021.berlinbuzzwords.de/member/chinmay-soman

More: https://2021.berlinbuzzwords.de/session/faster-analytics-fast-data-apache-pinot-and-flink-sql
Captions: 
	00:00:07,040 --> 00:00:09,440
so

00:00:07,600 --> 00:00:11,040
yeah the welcome to the session where

00:00:09,440 --> 00:00:13,840
i'll be talking about

00:00:11,040 --> 00:00:15,200
how to build complex real-time

00:00:13,840 --> 00:00:18,640
analytical use cases

00:00:15,200 --> 00:00:19,039
using flink and apache pino i guess i

00:00:18,640 --> 00:00:21,199
can just

00:00:19,039 --> 00:00:22,560
skip the intro thanks to fabian for

00:00:21,199 --> 00:00:25,599
introducing

00:00:22,560 --> 00:00:26,640
uh so today i'll begin by discussing the

00:00:25,599 --> 00:00:30,480
use cases of

00:00:26,640 --> 00:00:33,440
real-time analytics and uh

00:00:30,480 --> 00:00:34,880
shed light on why this is fast becoming

00:00:33,440 --> 00:00:37,760
an important need for

00:00:34,880 --> 00:00:40,320
most of the modern businesses today um

00:00:37,760 --> 00:00:43,760
i'll give an overview of apache pinot

00:00:40,320 --> 00:00:44,480
and uh you know explain why it's fit for

00:00:43,760 --> 00:00:47,920
building

00:00:44,480 --> 00:00:50,000
such fast real-time analytical use cases

00:00:47,920 --> 00:00:52,160
i'll discuss the ingestion challenges

00:00:50,000 --> 00:00:54,800
that pino faces today

00:00:52,160 --> 00:00:55,920
which it's not able to overcome on its

00:00:54,800 --> 00:00:58,079
own

00:00:55,920 --> 00:00:58,960
and next i'll talk about apache flink

00:00:58,079 --> 00:01:02,399
and how

00:00:58,960 --> 00:01:04,000
uh it it can be used to overcome uh

00:01:02,399 --> 00:01:05,680
some of these complex ingestion

00:01:04,000 --> 00:01:08,320
challenges in pino

00:01:05,680 --> 00:01:09,600
and finally uh we can conclude with a

00:01:08,320 --> 00:01:14,400
cool demo with with

00:01:09,600 --> 00:01:16,640
twitch streams okay so let's get started

00:01:14,400 --> 00:01:17,600
when we talk about real-time analytics

00:01:16,640 --> 00:01:19,920
there's actually

00:01:17,600 --> 00:01:21,040
many different sub-categories of use

00:01:19,920 --> 00:01:23,360
cases

00:01:21,040 --> 00:01:24,799
and each one has its own unique

00:01:23,360 --> 00:01:26,400
requirements

00:01:24,799 --> 00:01:29,119
we'll go through some of this in the

00:01:26,400 --> 00:01:31,360
next few slides

00:01:29,119 --> 00:01:33,119
one of the most important category is

00:01:31,360 --> 00:01:35,200
user facing analytics

00:01:33,119 --> 00:01:36,960
where you are exposing your analytical

00:01:35,200 --> 00:01:40,079
capabilities directly

00:01:36,960 --> 00:01:41,600
to your customers or or end users so for

00:01:40,079 --> 00:01:43,520
example linkedin

00:01:41,600 --> 00:01:45,119
has this who viewed your profile

00:01:43,520 --> 00:01:47,360
dashboard which it

00:01:45,119 --> 00:01:48,479
provides to all its 700 million plus

00:01:47,360 --> 00:01:51,520
members

00:01:48,479 --> 00:01:54,159
where you can get a personalized view of

00:01:51,520 --> 00:01:55,680
profile views sliced across multiple

00:01:54,159 --> 00:01:59,040
dimensions such as time

00:01:55,680 --> 00:02:02,640
industry segment excuse me

00:01:59,040 --> 00:02:05,759
geographical location and so on

00:02:02,640 --> 00:02:07,520
another example is the linkedin feed

00:02:05,759 --> 00:02:09,520
relevance

00:02:07,520 --> 00:02:10,720
where in order to make sure you're not

00:02:09,520 --> 00:02:13,440
seeing the same thing

00:02:10,720 --> 00:02:14,000
again and again we want to know for a

00:02:13,440 --> 00:02:17,120
given

00:02:14,000 --> 00:02:19,520
story your content how many times has

00:02:17,120 --> 00:02:20,800
has a user seen this in the last 14 days

00:02:19,520 --> 00:02:23,280
or so

00:02:20,800 --> 00:02:25,440
and then this can be done with a sql

00:02:23,280 --> 00:02:28,160
query something like this

00:02:25,440 --> 00:02:29,760
now this may seem straightforward but

00:02:28,160 --> 00:02:32,400
you're executing this query

00:02:29,760 --> 00:02:33,599
on a huge database of 700 million plus

00:02:32,400 --> 00:02:36,720
users

00:02:33,599 --> 00:02:39,040
and every time you visit linkedin for

00:02:36,720 --> 00:02:41,360
all active members uh this has to be

00:02:39,040 --> 00:02:44,080
executed which translates to

00:02:41,360 --> 00:02:45,599
several tens of thousands of qps on your

00:02:44,080 --> 00:02:48,319
underlying database

00:02:45,599 --> 00:02:49,599
and then each such query must execute

00:02:48,319 --> 00:02:51,599
very quickly

00:02:49,599 --> 00:02:53,200
in the order of milliseconds otherwise

00:02:51,599 --> 00:02:55,360
it's going to be a bad experience for

00:02:53,200 --> 00:02:57,360
the users

00:02:55,360 --> 00:02:59,120
another good example is a restaurant

00:02:57,360 --> 00:03:01,200
manager by ubereats

00:02:59,120 --> 00:03:02,959
this is a dashboard given to restaurant

00:03:01,200 --> 00:03:04,800
owners across the globe

00:03:02,959 --> 00:03:06,319
where they can see different things like

00:03:04,800 --> 00:03:08,800
sales metrics on a week

00:03:06,319 --> 00:03:11,920
week or week manner the inaccurate

00:03:08,800 --> 00:03:14,239
orders uh top selling items and so on

00:03:11,920 --> 00:03:16,080
and and you can imagine to build

00:03:14,239 --> 00:03:18,000
something like this you're also doing a

00:03:16,080 --> 00:03:20,800
lot of concurrent queries

00:03:18,000 --> 00:03:23,440
and again each such query must execute

00:03:20,800 --> 00:03:23,440
very quickly

00:03:24,080 --> 00:03:28,560
another important category of real-time

00:03:26,159 --> 00:03:31,200
analytics is business metrics

00:03:28,560 --> 00:03:32,799
this is where you're tracking the key

00:03:31,200 --> 00:03:35,440
indicators of your business

00:03:32,799 --> 00:03:36,319
in a real-time manner and then doing

00:03:35,440 --> 00:03:39,280
this in real time

00:03:36,319 --> 00:03:40,080
is important uh for day-to-day operation

00:03:39,280 --> 00:03:42,959
and also

00:03:40,080 --> 00:03:43,760
things like anomaly detection so for

00:03:42,959 --> 00:03:46,720
example

00:03:43,760 --> 00:03:48,400
uh page views is an important business

00:03:46,720 --> 00:03:50,720
metric for uber

00:03:48,400 --> 00:03:52,000
or demand and supply ratios is another

00:03:50,720 --> 00:03:54,480
one for

00:03:52,000 --> 00:03:55,280
uh sorry page views is an example of

00:03:54,480 --> 00:03:57,200
linkedin

00:03:55,280 --> 00:03:59,360
and demand and supply ratios is a

00:03:57,200 --> 00:04:01,599
business metric for uber

00:03:59,360 --> 00:04:03,120
and here you see an example where the

00:04:01,599 --> 00:04:05,200
number of page views suddenly

00:04:03,120 --> 00:04:08,400
dropped and you want to be able to

00:04:05,200 --> 00:04:10,480
detect this in real time

00:04:08,400 --> 00:04:13,200
more importantly you also want to know

00:04:10,480 --> 00:04:16,720
why that anomaly happened

00:04:13,200 --> 00:04:19,280
in other words which dimension resulted

00:04:16,720 --> 00:04:20,079
in in the page views to drop and

00:04:19,280 --> 00:04:21,919
detecting

00:04:20,079 --> 00:04:25,199
doing the root cause analysis in real

00:04:21,919 --> 00:04:26,960
time is also very important

00:04:25,199 --> 00:04:28,960
and finally we have dashboards which

00:04:26,960 --> 00:04:30,800
everyone pretty much knows about

00:04:28,960 --> 00:04:32,320
you know this is one place where you can

00:04:30,800 --> 00:04:35,040
track all your application

00:04:32,320 --> 00:04:35,600
and system metrics uh and as you can

00:04:35,040 --> 00:04:37,280
imagine

00:04:35,600 --> 00:04:39,040
you know this can also result in a lot

00:04:37,280 --> 00:04:40,960
of concurrent queries

00:04:39,040 --> 00:04:42,720
and having a real-time view of this is

00:04:40,960 --> 00:04:45,919
extremely important for

00:04:42,720 --> 00:04:46,720
for your operational needs so all such

00:04:45,919 --> 00:04:50,160
use cases

00:04:46,720 --> 00:04:53,919
and many more can be built on top of

00:04:50,160 --> 00:04:56,080
apache pino for those who haven't

00:04:53,919 --> 00:04:58,639
haven't heard of this apache pino is an

00:04:56,080 --> 00:05:01,680
open source distributed data store

00:04:58,639 --> 00:05:03,680
that can ingest data from a wide variety

00:05:01,680 --> 00:05:07,120
of sources such as kafka

00:05:03,680 --> 00:05:09,360
s3 hdfs and so on and make it available

00:05:07,120 --> 00:05:11,600
for querying in real time

00:05:09,360 --> 00:05:12,800
at the heart of pinot is is a column

00:05:11,600 --> 00:05:16,320
store

00:05:12,800 --> 00:05:18,639
and it features a rich set of indexes

00:05:16,320 --> 00:05:22,639
and aggregation strategies that make it

00:05:18,639 --> 00:05:25,039
a great fit for all such use cases

00:05:22,639 --> 00:05:26,720
and it's it's quite a mature product as

00:05:25,039 --> 00:05:28,880
of now it's being used in a lot of big

00:05:26,720 --> 00:05:31,199
data companies around the globe

00:05:28,880 --> 00:05:32,560
and has a rapidly growing community as

00:05:31,199 --> 00:05:34,400
well

00:05:32,560 --> 00:05:36,880
uh some of the largest pinot clusters

00:05:34,400 --> 00:05:40,160
can do upwards of million plus

00:05:36,880 --> 00:05:41,360
queries events per second ingestion can

00:05:40,160 --> 00:05:42,960
easily do

00:05:41,360 --> 00:05:45,039
hundreds of thousands of queries per

00:05:42,960 --> 00:05:48,560
second while still maintaining

00:05:45,039 --> 00:05:51,120
millisecond level latency

00:05:48,560 --> 00:05:52,160
so this is an exact an overview of how

00:05:51,120 --> 00:05:54,800
pinot fits in

00:05:52,160 --> 00:05:55,360
in your overall data ecosystem and we

00:05:54,800 --> 00:05:58,639
can take

00:05:55,360 --> 00:06:00,639
the example of linkedin uh so every time

00:05:58,639 --> 00:06:03,520
people visit linkedin.com

00:06:00,639 --> 00:06:04,080
all the events generated will be emitted

00:06:03,520 --> 00:06:06,960
to

00:06:04,080 --> 00:06:08,000
a streaming system like kafka and all

00:06:06,960 --> 00:06:10,479
the entity data

00:06:08,000 --> 00:06:11,360
around users and companies can be stored

00:06:10,479 --> 00:06:14,800
in something

00:06:11,360 --> 00:06:17,919
in some oltp store from here

00:06:14,800 --> 00:06:19,919
data is continuously being archived uh

00:06:17,919 --> 00:06:21,199
and into a long retention store like

00:06:19,919 --> 00:06:24,479
htfs

00:06:21,199 --> 00:06:27,199
for variety of other use cases

00:06:24,479 --> 00:06:28,400
as i mentioned before pinot can actually

00:06:27,199 --> 00:06:30,479
ingest data

00:06:28,400 --> 00:06:32,000
from all these sources so within

00:06:30,479 --> 00:06:35,840
linkedin

00:06:32,000 --> 00:06:38,720
we can ingest data from kafka and hdfs

00:06:35,840 --> 00:06:41,440
and provide a consolidated logical view

00:06:38,720 --> 00:06:44,160
to the user so we hide the complexity

00:06:41,440 --> 00:06:45,120
of of the actual data sources and then

00:06:44,160 --> 00:06:49,840
you can build

00:06:45,120 --> 00:06:51,919
all these different use cases on top

00:06:49,840 --> 00:06:52,880
uh if you look under the hood of pinot

00:06:51,919 --> 00:06:55,360
let's look at

00:06:52,880 --> 00:06:56,639
what are the different components so the

00:06:55,360 --> 00:06:59,360
incoming data

00:06:56,639 --> 00:07:00,720
from the data source is organized in a

00:06:59,360 --> 00:07:03,840
column format

00:07:00,720 --> 00:07:06,000
and sprayed out across the what we call

00:07:03,840 --> 00:07:07,680
as a pinot server

00:07:06,000 --> 00:07:09,599
and and you know you can have you can

00:07:07,680 --> 00:07:11,440
add as many window servers as you want

00:07:09,599 --> 00:07:13,680
and you can configure replication

00:07:11,440 --> 00:07:16,240
amongst all these servers

00:07:13,680 --> 00:07:17,919
there's a pinot controller which is

00:07:16,240 --> 00:07:19,440
responsible for all the cluster

00:07:17,919 --> 00:07:22,800
coordination

00:07:19,440 --> 00:07:26,400
functions such as membership replication

00:07:22,800 --> 00:07:28,160
and partitioning and so on

00:07:26,400 --> 00:07:30,720
and finally we have the pinot broker

00:07:28,160 --> 00:07:32,080
which can take a user query or

00:07:30,720 --> 00:07:34,319
application query

00:07:32,080 --> 00:07:37,039
and then do a distributed scatter gather

00:07:34,319 --> 00:07:39,759
across all the servers

00:07:37,039 --> 00:07:41,599
so what it does is it will identify

00:07:39,759 --> 00:07:44,639
which servers are responsible

00:07:41,599 --> 00:07:47,599
for serving this query and then send

00:07:44,639 --> 00:07:49,120
the query directly to those servers all

00:07:47,599 --> 00:07:51,919
these servers will then do

00:07:49,120 --> 00:07:53,039
local processing and then return an

00:07:51,919 --> 00:07:55,759
intermediate result

00:07:53,039 --> 00:07:57,759
to the broker the broker will then do a

00:07:55,759 --> 00:08:00,160
final aggregation and return it back to

00:07:57,759 --> 00:08:00,160
the user

00:08:01,440 --> 00:08:05,680
so as i mentioned what makes pino really

00:08:04,720 --> 00:08:09,599
fast

00:08:05,680 --> 00:08:12,240
for the real-time analytics is is the

00:08:09,599 --> 00:08:14,160
all the rich indexing strategies that is

00:08:12,240 --> 00:08:17,199
available out of the box

00:08:14,160 --> 00:08:19,840
so for example you can configure

00:08:17,199 --> 00:08:21,360
inverted sorted or range index for any

00:08:19,840 --> 00:08:24,560
of the numerical columns

00:08:21,360 --> 00:08:26,800
in in your schema json index

00:08:24,560 --> 00:08:30,080
lets you do fast queries on on

00:08:26,800 --> 00:08:32,719
semi-structured or unstructured data

00:08:30,080 --> 00:08:35,599
as the name implies geo index will

00:08:32,719 --> 00:08:37,680
accelerate your geospatial queries

00:08:35,599 --> 00:08:39,760
and there is a special index called star

00:08:37,680 --> 00:08:42,719
tree which is also how

00:08:39,760 --> 00:08:44,720
our company is named which lets you

00:08:42,719 --> 00:08:47,839
pre-aggregate

00:08:44,720 --> 00:08:51,040
values across a range of dimensions

00:08:47,839 --> 00:08:54,160
so this makes complex aggregation

00:08:51,040 --> 00:08:55,920
queries really really fast and

00:08:54,160 --> 00:08:57,519
one other feature i want to call out

00:08:55,920 --> 00:08:59,600
here is the

00:08:57,519 --> 00:09:00,720
something that we added recently in pino

00:08:59,600 --> 00:09:04,720
is the ability to

00:09:00,720 --> 00:09:06,720
observe data so you can actually have

00:09:04,720 --> 00:09:08,240
real-time data coming through kafka

00:09:06,720 --> 00:09:11,120
which has mutations

00:09:08,240 --> 00:09:12,240
and be able to update your pinot table

00:09:11,120 --> 00:09:13,600
in real time

00:09:12,240 --> 00:09:16,160
and this is something i'll actually be

00:09:13,600 --> 00:09:19,200
demoing today

00:09:16,160 --> 00:09:20,080
okay so now that we know a brief theory

00:09:19,200 --> 00:09:22,959
of pinot

00:09:20,080 --> 00:09:24,480
let's see how it uh let's see it in

00:09:22,959 --> 00:09:27,279
action

00:09:24,480 --> 00:09:28,080
uh so what i have here is a local uh

00:09:27,279 --> 00:09:31,760
docker

00:09:28,080 --> 00:09:34,880
instances for pino kafka and zookeeper

00:09:31,760 --> 00:09:37,680
oh and i forgot to mention the demo

00:09:34,880 --> 00:09:38,320
so what in the demo what we'll do is uh

00:09:37,680 --> 00:09:40,880
we'll

00:09:38,320 --> 00:09:41,680
we'll consume the twitch stream

00:09:40,880 --> 00:09:44,800
information

00:09:41,680 --> 00:09:47,120
uh using using its api and emit all

00:09:44,800 --> 00:09:49,200
these events into kafka

00:09:47,120 --> 00:09:50,399
and then subsequently you will ingest it

00:09:49,200 --> 00:09:53,600
in pinot and

00:09:50,399 --> 00:09:56,959
query the data in real time

00:09:53,600 --> 00:09:59,120
okay so i have this nifty

00:09:56,959 --> 00:10:00,480
python script which all it does is it

00:09:59,120 --> 00:10:03,680
queries twitch

00:10:00,480 --> 00:10:08,240
api and then emits the events to kafka

00:10:03,680 --> 00:10:10,399
so let's let's start that and if you

00:10:08,240 --> 00:10:12,160
if you look at the kafka topics we

00:10:10,399 --> 00:10:15,680
should see a

00:10:12,160 --> 00:10:19,760
something called as twitch streams

00:10:15,680 --> 00:10:21,680
and just to see how um

00:10:19,760 --> 00:10:22,800
the events look like they look something

00:10:21,680 --> 00:10:26,399
like this

00:10:22,800 --> 00:10:29,360
so we have an id which is the which

00:10:26,399 --> 00:10:30,079
uniquely identifies a twitch stream you

00:10:29,360 --> 00:10:33,200
have all the

00:10:30,079 --> 00:10:34,800
user information you have the game

00:10:33,200 --> 00:10:38,000
information

00:10:34,800 --> 00:10:40,880
and also has an event time attribute

00:10:38,000 --> 00:10:41,920
which defines a point and time at which

00:10:40,880 --> 00:10:45,920
this event was

00:10:41,920 --> 00:10:47,040
generated okay so now that the events

00:10:45,920 --> 00:10:50,160
are

00:10:47,040 --> 00:10:53,040
in in kafka we can go ahead and

00:10:50,160 --> 00:10:54,800
and start querying it in pino so when

00:10:53,040 --> 00:10:56,000
you deploy pino it comes with a

00:10:54,800 --> 00:10:58,880
convenient

00:10:56,000 --> 00:11:00,800
ui to do different things like manage

00:10:58,880 --> 00:11:04,000
your cluster topology

00:11:00,800 --> 00:11:05,680
and also create tables so let us go

00:11:04,000 --> 00:11:06,959
ahead and create a table for our twitch

00:11:05,680 --> 00:11:09,040
stream

00:11:06,959 --> 00:11:10,880
first things first is to add a schema

00:11:09,040 --> 00:11:15,839
for our table

00:11:10,880 --> 00:11:19,040
so we'll add id as as our dimension

00:11:15,839 --> 00:11:20,880
we can add the game name as another

00:11:19,040 --> 00:11:24,079
dimension

00:11:20,880 --> 00:11:24,560
and then these are both strings we can

00:11:24,079 --> 00:11:29,839
add

00:11:24,560 --> 00:11:29,839
a viewer count which is a metric

00:11:31,600 --> 00:11:34,959
and then the event time which is

00:11:33,839 --> 00:11:37,360
currently

00:11:34,959 --> 00:11:39,600
in the form of a string so we'll add it

00:11:37,360 --> 00:11:42,720
as a dimension

00:11:39,600 --> 00:11:43,760
and one last thing we'll do is add a

00:11:42,720 --> 00:11:45,839
special column

00:11:43,760 --> 00:11:48,480
called event time ms which is actually

00:11:45,839 --> 00:11:51,839
not in in your input kafka stream

00:11:48,480 --> 00:11:54,160
this will be a derived column and

00:11:51,839 --> 00:11:56,720
and this will be designated as a time

00:11:54,160 --> 00:11:57,519
column within pinot so the time column

00:11:56,720 --> 00:11:59,440
is currently

00:11:57,519 --> 00:12:01,440
by default how the data is partitioned

00:11:59,440 --> 00:12:04,639
and in pino

00:12:01,440 --> 00:12:07,120
so that's our resulting schema let's go

00:12:04,639 --> 00:12:10,320
ahead and save that

00:12:07,120 --> 00:12:14,240
so now we can add a real time table

00:12:10,320 --> 00:12:16,160
with the same name and and within this

00:12:14,240 --> 00:12:18,240
we can configure

00:12:16,160 --> 00:12:20,240
how we want to generate the derived

00:12:18,240 --> 00:12:21,760
column which is event time ms

00:12:20,240 --> 00:12:24,720
and this is really useful when your

00:12:21,760 --> 00:12:26,399
input stream does not have

00:12:24,720 --> 00:12:29,040
does not have the fields in the right

00:12:26,399 --> 00:12:32,079
format so what i'm going to do here

00:12:29,040 --> 00:12:33,279
is using a built-in function called from

00:12:32,079 --> 00:12:35,440
date time

00:12:33,279 --> 00:12:37,920
and all it does is takes an input column

00:12:35,440 --> 00:12:38,880
which is event time which is in string

00:12:37,920 --> 00:12:41,839
and convert that

00:12:38,880 --> 00:12:43,760
in in milliseconds so let's go ahead and

00:12:41,839 --> 00:12:46,000
do that

00:12:43,760 --> 00:12:47,839
so what this is going to do is for every

00:12:46,000 --> 00:12:50,079
record ingested in p node it's going to

00:12:47,839 --> 00:12:52,480
apply this transformation and generate a

00:12:50,079 --> 00:12:55,200
derived column called event time ms

00:12:52,480 --> 00:12:57,519
and then we will partition data on the

00:12:55,200 --> 00:13:00,959
new column

00:12:57,519 --> 00:13:02,320
we also want to specify the kafka topic

00:13:00,959 --> 00:13:07,680
name

00:13:02,320 --> 00:13:08,880
and the kafka url

00:13:07,680 --> 00:13:10,160
there are other things that you can do

00:13:08,880 --> 00:13:11,680
which i want to go through right now

00:13:10,160 --> 00:13:15,519
like retention

00:13:11,680 --> 00:13:18,320
quotas for your query and so on um

00:13:15,519 --> 00:13:18,720
so let's go ahead and save that okay so

00:13:18,320 --> 00:13:22,079
now

00:13:18,720 --> 00:13:23,920
we're ready to uh query the the data

00:13:22,079 --> 00:13:26,720
coming from twitch

00:13:23,920 --> 00:13:28,560
and keep in mind this is real uh real

00:13:26,720 --> 00:13:31,760
live data which is actually happening on

00:13:28,560 --> 00:13:34,079
twitch right now so if i do pound star

00:13:31,760 --> 00:13:34,959
you should see the total count

00:13:34,079 --> 00:13:39,279
increasing

00:13:34,959 --> 00:13:41,600
as you can see below okay

00:13:39,279 --> 00:13:42,720
so this is cool but let's do a slightly

00:13:41,600 --> 00:13:46,079
complicated query

00:13:42,720 --> 00:13:46,079
which is um

00:13:46,160 --> 00:13:54,320
we want to do a total count of streams

00:13:50,880 --> 00:13:57,519
um grouped on the id

00:13:54,320 --> 00:13:58,240
on the stream id and intuitively you

00:13:57,519 --> 00:14:01,279
expect

00:13:58,240 --> 00:14:03,600
this the count per

00:14:01,279 --> 00:14:06,000
id to be one there should be only one

00:14:03,600 --> 00:14:08,079
unique stream per id

00:14:06,000 --> 00:14:09,279
but as you can see we currently have an

00:14:08,079 --> 00:14:11,440
issue here

00:14:09,279 --> 00:14:12,800
what you see is there's multiple events

00:14:11,440 --> 00:14:16,160
happening

00:14:12,800 --> 00:14:16,480
for a given id and let's take a deeper

00:14:16,160 --> 00:14:19,680
look

00:14:16,480 --> 00:14:19,680
why this is happening

00:14:23,040 --> 00:14:26,079
okay so what you can see here is for the

00:14:25,440 --> 00:14:29,120
same

00:14:26,079 --> 00:14:29,760
twitch the stream id you see multiple

00:14:29,120 --> 00:14:32,560
events

00:14:29,760 --> 00:14:34,480
with with different event time and also

00:14:32,560 --> 00:14:36,000
different viewer count

00:14:34,480 --> 00:14:37,600
and and what's happening is this the

00:14:36,000 --> 00:14:40,720
twitch stream is

00:14:37,600 --> 00:14:43,680
constantly being updated and we are

00:14:40,720 --> 00:14:46,320
injecting these often duplicate or

00:14:43,680 --> 00:14:48,240
observable events in the kafka stream

00:14:46,320 --> 00:14:50,480
and in at the moment we haven't

00:14:48,240 --> 00:14:54,160
configured pino to handle absurds

00:14:50,480 --> 00:14:55,600
um so this is currently just with pino

00:14:54,160 --> 00:14:56,480
and the current kafka stream we are

00:14:55,600 --> 00:15:01,040
unable to

00:14:56,480 --> 00:15:04,639
to handle upsets so let me go back

00:15:01,040 --> 00:15:04,639
to my presentation

00:15:06,399 --> 00:15:13,199
so in order to handle upsets within pino

00:15:10,160 --> 00:15:15,600
the prerequisite is the input kafka

00:15:13,199 --> 00:15:16,480
stream must be partitioned on the

00:15:15,600 --> 00:15:18,800
primary key

00:15:16,480 --> 00:15:19,920
and in this case that's the id column

00:15:18,800 --> 00:15:22,079
which was not happening

00:15:19,920 --> 00:15:24,480
right now now of course you know i could

00:15:22,079 --> 00:15:27,120
have done that in my in my python script

00:15:24,480 --> 00:15:28,800
but oftentimes you don't control the

00:15:27,120 --> 00:15:30,399
input kafka streams right

00:15:28,800 --> 00:15:32,800
so you need a mechanism to do

00:15:30,399 --> 00:15:35,600
re-partitioning of your data

00:15:32,800 --> 00:15:38,079
even more complex scenarios is when your

00:15:35,600 --> 00:15:39,920
input stream or table does not contain

00:15:38,079 --> 00:15:42,160
all your data that you want to analyze

00:15:39,920 --> 00:15:43,920
um and you want to do

00:15:42,160 --> 00:15:45,839
either a stream stream join or stream

00:15:43,920 --> 00:15:48,079
table join to to compute this

00:15:45,839 --> 00:15:50,320
materialization

00:15:48,079 --> 00:15:51,839
and finally you can have decoration

00:15:50,320 --> 00:15:54,000
requirement where you have

00:15:51,839 --> 00:15:56,240
events coming in through your data

00:15:54,000 --> 00:15:58,880
source and you want to decorate it

00:15:56,240 --> 00:16:01,600
using an external rpc either with

00:15:58,880 --> 00:16:04,639
something sitting in an oltp store or

00:16:01,600 --> 00:16:07,920
behind an api for all such

00:16:04,639 --> 00:16:09,519
ingestion challenges we rely on apache

00:16:07,920 --> 00:16:12,079
flink

00:16:09,519 --> 00:16:13,759
um for again and hopefully you all know

00:16:12,079 --> 00:16:14,959
apache flink already it's an extremely

00:16:13,759 --> 00:16:17,199
popular

00:16:14,959 --> 00:16:18,480
uh stream processing framework which

00:16:17,199 --> 00:16:21,360
lets you

00:16:18,480 --> 00:16:24,320
perform computational tasks on bounded

00:16:21,360 --> 00:16:27,040
and unbounded streams of data

00:16:24,320 --> 00:16:28,720
it comes with a wide variety of input

00:16:27,040 --> 00:16:31,600
and output connectors

00:16:28,720 --> 00:16:34,160
and features a rich api and also

00:16:31,600 --> 00:16:37,839
includes things like state management

00:16:34,160 --> 00:16:39,680
which makes it a great fit for

00:16:37,839 --> 00:16:41,440
building different applications such as

00:16:39,680 --> 00:16:46,000
event driven applications

00:16:41,440 --> 00:16:48,480
uh streaming etl and analytics and so on

00:16:46,000 --> 00:16:50,320
uh of course flink is a quite a mature

00:16:48,480 --> 00:16:53,600
product and it's used

00:16:50,320 --> 00:16:55,680
in a lot of companies around the globe

00:16:53,600 --> 00:16:58,240
especially what i want to focus is the

00:16:55,680 --> 00:17:00,720
alibaba's numbers from 2019

00:16:58,240 --> 00:17:02,639
this is quite a while ago the decent

00:17:00,720 --> 00:17:05,360
numbers are probably much higher

00:17:02,639 --> 00:17:06,000
but it was able to do 2.5 billion events

00:17:05,360 --> 00:17:07,439
per second

00:17:06,000 --> 00:17:09,919
at peak which is really really

00:17:07,439 --> 00:17:09,919
impressive

00:17:10,079 --> 00:17:14,160
for this particular talk i want to focus

00:17:12,880 --> 00:17:15,919
on

00:17:14,160 --> 00:17:17,839
one important aspect of link which is

00:17:15,919 --> 00:17:19,600
the flink sql

00:17:17,839 --> 00:17:21,679
and as the name implies it lets you

00:17:19,600 --> 00:17:23,520
express your computational logic

00:17:21,679 --> 00:17:26,079
using a declarative way something

00:17:23,520 --> 00:17:28,079
something like this

00:17:26,079 --> 00:17:29,679
this is based on the apache calcite

00:17:28,079 --> 00:17:32,720
grammar which is

00:17:29,679 --> 00:17:34,320
very similar to ansi sql but also adds

00:17:32,720 --> 00:17:36,640
some advanced things like window

00:17:34,320 --> 00:17:38,880
semantics which is required for

00:17:36,640 --> 00:17:40,960
continuous queries

00:17:38,880 --> 00:17:43,679
and as you can see here link sql is

00:17:40,960 --> 00:17:46,880
actually built on top of the existing

00:17:43,679 --> 00:17:48,559
primitives and in fact given a fling sql

00:17:46,880 --> 00:17:51,200
query would be

00:17:48,559 --> 00:17:52,000
translated into the underlying api and

00:17:51,200 --> 00:17:55,440
executed

00:17:52,000 --> 00:17:58,160
as a as regular flink job

00:17:55,440 --> 00:17:59,520
uh and again you know you can execute it

00:17:58,160 --> 00:18:02,000
on both

00:17:59,520 --> 00:18:03,440
on unbounded and bounded streams so when

00:18:02,000 --> 00:18:04,400
you're running it again something like

00:18:03,440 --> 00:18:06,960
kafka

00:18:04,400 --> 00:18:08,080
it runs as a continuous query so it

00:18:06,960 --> 00:18:11,520
keeps generating

00:18:08,080 --> 00:18:12,240
output continuously and as opposed to

00:18:11,520 --> 00:18:15,600
something like

00:18:12,240 --> 00:18:18,720
a standard s3 or hdfs file it will be

00:18:15,600 --> 00:18:20,240
executed as a traditional sql query

00:18:18,720 --> 00:18:21,760
so this is just a very high level

00:18:20,240 --> 00:18:24,240
overview of link i

00:18:21,760 --> 00:18:25,039
i highly recommend the talk from mata

00:18:24,240 --> 00:18:27,280
pes and

00:18:25,039 --> 00:18:29,440
other people from vervetica for for

00:18:27,280 --> 00:18:32,000
flink and flings equal

00:18:29,440 --> 00:18:32,000
excuse me

00:18:32,880 --> 00:18:39,039
okay so what we'll do now is to

00:18:36,320 --> 00:18:41,360
i'll show how we can solve some of the

00:18:39,039 --> 00:18:43,919
ingestion challenges we saw in pino

00:18:41,360 --> 00:18:45,840
using flink so we'll go back to our

00:18:43,919 --> 00:18:48,880
twitch api

00:18:45,840 --> 00:18:52,320
and we'll continue generating the the

00:18:48,880 --> 00:18:53,600
real the twitch stream information into

00:18:52,320 --> 00:18:55,520
kafka

00:18:53,600 --> 00:18:58,080
but what we're also doing here is to

00:18:55,520 --> 00:19:03,120
pre-fetch the tags information

00:18:58,080 --> 00:19:05,120
and store it as a json file and an s3

00:19:03,120 --> 00:19:06,559
at this point we'll be using flink to do

00:19:05,120 --> 00:19:08,880
a join between

00:19:06,559 --> 00:19:11,760
this kafka topic and then this and this

00:19:08,880 --> 00:19:14,480
s3 file sort of like a stream table join

00:19:11,760 --> 00:19:16,080
and and emit the information uh back to

00:19:14,480 --> 00:19:17,840
kafka

00:19:16,080 --> 00:19:19,520
the other thing the fling jaw will be

00:19:17,840 --> 00:19:22,000
doing is repartitioning

00:19:19,520 --> 00:19:25,280
this this data on the primary key that

00:19:22,000 --> 00:19:28,320
we need for pnom sorts so it'll be

00:19:25,280 --> 00:19:29,039
partitioning on the id column and

00:19:28,320 --> 00:19:31,200
finally

00:19:29,039 --> 00:19:32,080
i'll show how this can be ingested into

00:19:31,200 --> 00:19:34,480
pino

00:19:32,080 --> 00:19:36,000
and we'll do a cool visualization using

00:19:34,480 --> 00:19:38,960
superset

00:19:36,000 --> 00:19:41,280
so let me switch back to my demo

00:19:38,960 --> 00:19:44,240
environment

00:19:41,280 --> 00:19:45,360
so first thing i'll do is start a fling

00:19:44,240 --> 00:19:48,080
sql client

00:19:45,360 --> 00:19:48,640
uh tool which is a very convenient way

00:19:48,080 --> 00:19:51,600
of

00:19:48,640 --> 00:19:54,799
submitting your flink queries and

00:19:51,600 --> 00:19:57,280
starting the actual flink job

00:19:54,799 --> 00:19:58,160
okay so first thing we want to do is

00:19:57,280 --> 00:20:01,200
create a table

00:19:58,160 --> 00:20:03,919
to read from the kafka topic for

00:20:01,200 --> 00:20:05,520
uh which has the real time to extreme

00:20:03,919 --> 00:20:07,360
information

00:20:05,520 --> 00:20:09,120
let's go ahead and do that so it

00:20:07,360 --> 00:20:12,240
contains all the dimensions

00:20:09,120 --> 00:20:13,919
from the twitch stream api and also

00:20:12,240 --> 00:20:18,159
defines where the data is coming from

00:20:13,919 --> 00:20:18,159
which is our kafka local kafka cluster

00:20:18,240 --> 00:20:22,960
next we'll create a table to consume

00:20:21,120 --> 00:20:26,799
data from the

00:20:22,960 --> 00:20:30,320
um the json file stored in s3

00:20:26,799 --> 00:20:32,400
let's do that it has only two dimensions

00:20:30,320 --> 00:20:35,039
the tag id and description

00:20:32,400 --> 00:20:36,559
and as you can imagine we'll be joining

00:20:35,039 --> 00:20:39,600
on the tag id

00:20:36,559 --> 00:20:40,080
column and again here we are showing

00:20:39,600 --> 00:20:41,919
okay

00:20:40,080 --> 00:20:45,840
the connector is file system will be

00:20:41,919 --> 00:20:45,840
reading from from s3

00:20:46,159 --> 00:20:50,799
and finally we'll create a table which

00:20:48,000 --> 00:20:53,840
is a result of the join operation

00:20:50,799 --> 00:20:56,720
of these two things so it has

00:20:53,840 --> 00:20:57,520
the dimensions from both the kafka and

00:20:56,720 --> 00:21:00,400
tags

00:20:57,520 --> 00:21:01,120
file and we want to emit it back to

00:21:00,400 --> 00:21:03,840
kafka

00:21:01,120 --> 00:21:04,880
hence we're using the kafka connector

00:21:03,840 --> 00:21:07,840
the other thing

00:21:04,880 --> 00:21:09,200
if you notice here we're defining we're

00:21:07,840 --> 00:21:11,919
specifying

00:21:09,200 --> 00:21:13,360
the key field as id so what we want to

00:21:11,919 --> 00:21:16,400
do is partition the data

00:21:13,360 --> 00:21:19,120
on the id column in other words

00:21:16,400 --> 00:21:19,679
all records with the same twitch stream

00:21:19,120 --> 00:21:22,559
id

00:21:19,679 --> 00:21:23,840
will end up in the same kafka partition

00:21:22,559 --> 00:21:26,880
and will enable pino

00:21:23,840 --> 00:21:28,799
to do upserts okay

00:21:26,880 --> 00:21:30,080
so now we're ready to actually execute

00:21:28,799 --> 00:21:33,520
our join query

00:21:30,080 --> 00:21:36,400
which looks something like this

00:21:33,520 --> 00:21:38,159
and and again you know it looks pretty

00:21:36,400 --> 00:21:41,280
identical to a regular

00:21:38,159 --> 00:21:43,200
ansi sql query we select

00:21:41,280 --> 00:21:44,640
we project all the dimensions from from

00:21:43,200 --> 00:21:46,960
the two tables

00:21:44,640 --> 00:21:48,799
and then define the join criteria which

00:21:46,960 --> 00:21:50,559
is the tag id

00:21:48,799 --> 00:21:53,600
and i'm doing a simple inner join here

00:21:50,559 --> 00:21:55,440
but flink has a lot of advanced ways of

00:21:53,600 --> 00:21:57,120
doing defining windows for your join

00:21:55,440 --> 00:22:00,240
function

00:21:57,120 --> 00:22:00,960
okay so at this point the join was

00:22:00,240 --> 00:22:02,400
executed

00:22:00,960 --> 00:22:04,960
we have a job running which is

00:22:02,400 --> 00:22:05,840
continuously joining data from kafka and

00:22:04,960 --> 00:22:08,880
s3

00:22:05,840 --> 00:22:13,840
and emitting events to kafka

00:22:08,880 --> 00:22:13,840
so if i look at

00:22:14,320 --> 00:22:18,080
my kafka topics i should see a new topic

00:22:17,760 --> 00:22:20,480
pop

00:22:18,080 --> 00:22:21,840
up here which is twitch streams with

00:22:20,480 --> 00:22:24,080
tags

00:22:21,840 --> 00:22:25,280
and this is the topic which includes the

00:22:24,080 --> 00:22:28,480
join as well as the

00:22:25,280 --> 00:22:29,120
repartitioning thanks to flink at this

00:22:28,480 --> 00:22:31,679
point

00:22:29,120 --> 00:22:33,840
what i can do is to save time i've

00:22:31,679 --> 00:22:36,720
already created a pinot schema

00:22:33,840 --> 00:22:39,440
which has all the dimensions we want and

00:22:36,720 --> 00:22:43,039
i'm also specifying a primary key here

00:22:39,440 --> 00:22:43,440
which is the id column similarly i also

00:22:43,039 --> 00:22:46,559
have

00:22:43,440 --> 00:22:47,200
an absurd table which looks similar to

00:22:46,559 --> 00:22:50,880
the table

00:22:47,200 --> 00:22:51,760
that we created before let's go and add

00:22:50,880 --> 00:22:57,200
this

00:22:51,760 --> 00:22:57,200
to pino using the convenient rest api

00:23:02,080 --> 00:23:05,520
so now we are ready to query our upsell

00:23:04,960 --> 00:23:08,559
table

00:23:05,520 --> 00:23:10,559
so as you can see it has the

00:23:08,559 --> 00:23:11,679
all the dimensions as a result of the

00:23:10,559 --> 00:23:15,520
join uh

00:23:11,679 --> 00:23:18,559
from from kafka and s3 we can re-execute

00:23:15,520 --> 00:23:21,520
our group by query and and see what the

00:23:18,559 --> 00:23:25,200
result looks like now

00:23:21,520 --> 00:23:28,080
as i mentioned before um

00:23:25,200 --> 00:23:30,000
this the new pinot table we we have

00:23:28,080 --> 00:23:32,080
partitioned the date on id column and

00:23:30,000 --> 00:23:36,080
enabled observed within pino

00:23:32,080 --> 00:23:39,919
so now the result of the group by

00:23:36,080 --> 00:23:44,080
oh sorry i'm still using the old table

00:23:39,919 --> 00:23:47,520
pardon me as you can see now

00:23:44,080 --> 00:23:49,279
the group by is indeed one and pino is

00:23:47,520 --> 00:23:51,679
actually successfully

00:23:49,279 --> 00:23:53,440
uh either doing deduplicating data or

00:23:51,679 --> 00:23:56,080
handling up search correctly

00:23:53,440 --> 00:23:56,640
from the input stream uh so in this

00:23:56,080 --> 00:23:59,440
manner

00:23:56,640 --> 00:24:01,520
you know we we saw how you know flink

00:23:59,440 --> 00:24:03,200
can easily do join and repartitioning in

00:24:01,520 --> 00:24:04,960
a matter of minutes and then this was

00:24:03,200 --> 00:24:07,919
all uh real

00:24:04,960 --> 00:24:09,440
data from twitch just to re-emphasize uh

00:24:07,919 --> 00:24:12,960
at this point what we can do

00:24:09,440 --> 00:24:16,080
is uh use superset to visualize

00:24:12,960 --> 00:24:19,760
uh the information so let's go

00:24:16,080 --> 00:24:21,200
and add the new table that we created in

00:24:19,760 --> 00:24:24,159
pino which is

00:24:21,200 --> 00:24:24,159
the stream subsert

00:24:24,240 --> 00:24:28,480
uh one thing if you haven't used

00:24:26,080 --> 00:24:30,880
superset before i need to

00:24:28,480 --> 00:24:32,000
let superset know which is your time

00:24:30,880 --> 00:24:34,000
column and in our

00:24:32,000 --> 00:24:35,919
case that's event time milliseconds

00:24:34,000 --> 00:24:38,640
that's our temporal column

00:24:35,919 --> 00:24:39,279
and we also want to tell superset the

00:24:38,640 --> 00:24:43,279
format

00:24:39,279 --> 00:24:43,279
which is epochs in millisecond

00:24:43,760 --> 00:24:49,760
okay so now we can start visualizing

00:24:47,279 --> 00:24:52,000
this data coming from twitch so i'll

00:24:49,760 --> 00:24:55,520
pick a line chart

00:24:52,000 --> 00:24:56,080
bucket it by every second and let's say

00:24:55,520 --> 00:24:59,279
we

00:24:56,080 --> 00:25:01,840
want to see everything from now to minus

00:24:59,279 --> 00:25:01,840
seven days

00:25:03,440 --> 00:25:07,760
okay so you can see the current demo

00:25:06,559 --> 00:25:09,440
that we ran

00:25:07,760 --> 00:25:10,880
and something i was testing in the

00:25:09,440 --> 00:25:12,720
morning

00:25:10,880 --> 00:25:14,080
and then the queries are returning

00:25:12,720 --> 00:25:16,640
obviously very fast because what

00:25:14,080 --> 00:25:18,799
superset is doing is sending it to pino

00:25:16,640 --> 00:25:20,320
and then querying the twitch stream data

00:25:18,799 --> 00:25:22,159
in real time

00:25:20,320 --> 00:25:23,919
we can also do a little bit more complex

00:25:22,159 --> 00:25:25,279
things like figure out what are the most

00:25:23,919 --> 00:25:28,080
popular streams

00:25:25,279 --> 00:25:28,480
happening right now so let's do a group

00:25:28,080 --> 00:25:31,520
by

00:25:28,480 --> 00:25:33,919
on on the game name

00:25:31,520 --> 00:25:34,720
uh and i also you can again see this is

00:25:33,919 --> 00:25:37,520
really fast

00:25:34,720 --> 00:25:38,720
because of pino and flink and then this

00:25:37,520 --> 00:25:41,120
is the current

00:25:38,720 --> 00:25:43,840
um popular streams happening on twitch

00:25:41,120 --> 00:25:43,840
as of now

00:25:46,080 --> 00:25:50,320
so overall uh what we saw let me switch

00:25:49,840 --> 00:25:54,320
back

00:25:50,320 --> 00:25:56,720
um just to reiterate what we just saw we

00:25:54,320 --> 00:25:57,520
have we had a real stream information

00:25:56,720 --> 00:26:00,400
being emitted

00:25:57,520 --> 00:26:00,799
into kafka and then tags information

00:26:00,400 --> 00:26:05,120
going

00:26:00,799 --> 00:26:07,600
to s3 we did a join using flink sql

00:26:05,120 --> 00:26:11,120
uh repartitioned data also using the

00:26:07,600 --> 00:26:13,360
same flinksql query uh ingested

00:26:11,120 --> 00:26:14,480
into pino and pino was able to do handle

00:26:13,360 --> 00:26:16,880
the absorbs correctly

00:26:14,480 --> 00:26:18,480
and at this point and you can use

00:26:16,880 --> 00:26:20,400
anything something like superset to

00:26:18,480 --> 00:26:23,279
visualize all your data

00:26:20,400 --> 00:26:25,120
um so i can conclude stop here and then

00:26:23,279 --> 00:26:28,000
take any questions but

00:26:25,120 --> 00:26:28,799
overall fling sql is a really powerful

00:26:28,000 --> 00:26:31,919
construct

00:26:28,799 --> 00:26:33,840
which lets you do complex things in a in

00:26:31,919 --> 00:26:34,559
a very very fast manner as you saw right

00:26:33,840 --> 00:26:36,880
now

00:26:34,559 --> 00:26:38,000
and is being used at a massive scale and

00:26:36,880 --> 00:26:41,840
alibaba and

00:26:38,000 --> 00:26:43,520
other companies apache p is also

00:26:41,840 --> 00:26:45,760
we saw the distributed and scale out

00:26:43,520 --> 00:26:46,400
design uh and and the rich indexing

00:26:45,760 --> 00:26:48,640
support

00:26:46,400 --> 00:26:50,840
that it features and it's also being

00:26:48,640 --> 00:26:52,320
used in a lot of companies around the

00:26:50,840 --> 00:26:54,480
world uh

00:26:52,320 --> 00:26:56,960
before i stop i do want to acknowledge

00:26:54,480 --> 00:26:58,320
uh martha pace who was taking a bow here

00:26:56,960 --> 00:27:00,880
as she should

00:26:58,320 --> 00:27:01,840
um so she she helped me a lot with the

00:27:00,880 --> 00:27:04,640
initial demo

00:27:01,840 --> 00:27:05,360
and answering the fling questions that i

00:27:04,640 --> 00:27:09,120
had

00:27:05,360 --> 00:27:11,279
so thank you martha uh at this point i

00:27:09,120 --> 00:27:12,720
i can stop here and take any questions

00:27:11,279 --> 00:27:15,679
that you guys have

00:27:12,720 --> 00:27:17,520
thanks a lot i guess we're waiting for

00:27:15,679 --> 00:27:21,679
fabian to be on the stage

00:27:17,520 --> 00:27:24,799
uh while while that is um

00:27:21,679 --> 00:27:25,760
while fabian is coming back um since i'm

00:27:24,799 --> 00:27:28,080
unable to see

00:27:25,760 --> 00:27:28,799
see the questions i can talk a little

00:27:28,080 --> 00:27:32,840
bit more

00:27:28,799 --> 00:27:35,760
on uh you know the the pinot

00:27:32,840 --> 00:27:36,799
architecture and and i mentioned the

00:27:35,760 --> 00:27:40,399
scale out

00:27:36,799 --> 00:27:42,159
design um so we i'll quickly

00:27:40,399 --> 00:27:44,720
talk a little bit more on that while we

00:27:42,159 --> 00:27:47,760
wait um

00:27:44,720 --> 00:27:48,640
so as i mentioned uh you know the data

00:27:47,760 --> 00:27:51,360
is laid out

00:27:48,640 --> 00:27:53,200
in a column format across across all

00:27:51,360 --> 00:27:56,320
these servers

00:27:53,200 --> 00:27:57,760
so this forms this makes it very easy to

00:27:56,320 --> 00:28:01,200
expand capacity

00:27:57,760 --> 00:28:03,279
uh on the pinot side anytime you are

00:28:01,200 --> 00:28:04,399
facing a bottleneck we can just add more

00:28:03,279 --> 00:28:08,159
servers

00:28:04,399 --> 00:28:08,159
the controller will automatically

00:28:09,120 --> 00:28:16,320
get the the new identify the new servers

00:28:13,440 --> 00:28:18,559
and start putting segments uh pinot

00:28:16,320 --> 00:28:21,440
segments on onto these new servers

00:28:18,559 --> 00:28:22,480
similarly you can add brokers uh at any

00:28:21,440 --> 00:28:24,880
point

00:28:22,480 --> 00:28:25,679
uh and and this is how we can keep

00:28:24,880 --> 00:28:30,880
scaling out

00:28:25,679 --> 00:28:33,360
the pinot cluster at will

00:28:30,880 --> 00:28:34,000
okay i can again stop here and take any

00:28:33,360 --> 00:28:37,200
questions

00:28:34,000 --> 00:28:37,200
uh if there any

00:28:40,159 --> 00:28:44,240
yeah thanks for this awesome talk uh

00:28:42,559 --> 00:28:44,799
sorry for the uh for the technical

00:28:44,240 --> 00:28:48,000
problems

00:28:44,799 --> 00:28:50,640
no problem um yeah that was really uh

00:28:48,000 --> 00:28:51,520
really awesome awesome demo i have a

00:28:50,640 --> 00:28:55,200
question so

00:28:51,520 --> 00:28:56,880
sure um have you have you thought about

00:28:55,200 --> 00:28:58,000
uh or do you think it would make sense

00:28:56,880 --> 00:29:00,880
to um

00:28:58,000 --> 00:29:01,440
integrate uh flink with uh pinot a

00:29:00,880 --> 00:29:04,559
little bit

00:29:01,440 --> 00:29:05,200
tighter yeah similar as you uh as you

00:29:04,559 --> 00:29:09,840
did with the

00:29:05,200 --> 00:29:11,760
um like um leveraging uh presto for the

00:29:09,840 --> 00:29:13,200
uh for for the joint capability would

00:29:11,760 --> 00:29:16,480
that be an option to like

00:29:13,200 --> 00:29:19,520
somehow um fuse the systems together

00:29:16,480 --> 00:29:22,320
yeah great question yeah so this this is

00:29:19,520 --> 00:29:22,640
a common ask um from from many folks

00:29:22,320 --> 00:29:25,520
where

00:29:22,640 --> 00:29:27,679
you want to basically skip an

00:29:25,520 --> 00:29:29,520
intermediate stage between flink and

00:29:27,679 --> 00:29:31,760
pino right currently

00:29:29,520 --> 00:29:33,760
uh as in the demo also i mentioned we

00:29:31,760 --> 00:29:34,159
have to emit the events to kafka and

00:29:33,760 --> 00:29:37,520
then

00:29:34,159 --> 00:29:39,279
in just into pino so currently there is

00:29:37,520 --> 00:29:42,559
one way

00:29:39,279 --> 00:29:45,840
that that we are working on right now

00:29:42,559 --> 00:29:47,039
which is a segment writer api that is

00:29:45,840 --> 00:29:50,159
available

00:29:47,039 --> 00:29:53,440
uh for for flink jobs to directly use

00:29:50,159 --> 00:29:54,240
uh and and produce to pino uh the

00:29:53,440 --> 00:29:58,159
downside

00:29:54,240 --> 00:30:01,679
is uh so let me maybe step back into how

00:29:58,159 --> 00:30:04,240
pinot actually ingests the data

00:30:01,679 --> 00:30:05,520
um so when we are fetching data from

00:30:04,240 --> 00:30:08,480
real time

00:30:05,520 --> 00:30:09,360
the records are ingested one at a time

00:30:08,480 --> 00:30:11,840
and

00:30:09,360 --> 00:30:12,880
and and they are being converted into a

00:30:11,840 --> 00:30:17,120
column format

00:30:12,880 --> 00:30:20,640
for the corresponding segments um

00:30:17,120 --> 00:30:23,360
but when you in in the offline world

00:30:20,640 --> 00:30:24,720
we create the segments outside of pino

00:30:23,360 --> 00:30:28,240
and then copy it

00:30:24,720 --> 00:30:30,799
into pinot so that's essentially what we

00:30:28,240 --> 00:30:32,399
do with the current integration between

00:30:30,799 --> 00:30:35,200
flink and pino which is

00:30:32,399 --> 00:30:36,240
use the segment writer api to generate a

00:30:35,200 --> 00:30:38,159
local segment

00:30:36,240 --> 00:30:39,279
and then push the local segment to

00:30:38,159 --> 00:30:42,640
peanut

00:30:39,279 --> 00:30:46,080
um so the trade-off here is you know the

00:30:42,640 --> 00:30:48,720
the freshness of your data depends on

00:30:46,080 --> 00:30:50,720
how big your segment is right so if you

00:30:48,720 --> 00:30:52,880
keep producing so you can keep

00:30:50,720 --> 00:30:54,240
appending to your local segment within

00:30:52,880 --> 00:30:56,159
your fling job

00:30:54,240 --> 00:30:57,600
uh and let's say you do that for 10

00:30:56,159 --> 00:30:59,760
minutes uh

00:30:57,600 --> 00:31:01,279
so so the data will be available for

00:30:59,760 --> 00:31:04,399
querying 10 minutes later

00:31:01,279 --> 00:31:05,840
so it's more like a micro batch more

00:31:04,399 --> 00:31:07,519
today

00:31:05,840 --> 00:31:09,039
so so that that's the current mechanism

00:31:07,519 --> 00:31:10,960
right so you can you can

00:31:09,039 --> 00:31:12,559
create segments within within flink and

00:31:10,960 --> 00:31:15,519
push to pino and actually uber

00:31:12,559 --> 00:31:16,480
is playing around with that um as we

00:31:15,519 --> 00:31:18,799
speak

00:31:16,480 --> 00:31:20,320
the other one that we want to get to is

00:31:18,799 --> 00:31:23,360
a write api

00:31:20,320 --> 00:31:24,799
in in pino so be able to write one

00:31:23,360 --> 00:31:27,840
record at a time

00:31:24,799 --> 00:31:30,000
um directly into pino and this is

00:31:27,840 --> 00:31:32,320
something that we're still working on

00:31:30,000 --> 00:31:33,679
and once that's available then fling can

00:31:32,320 --> 00:31:36,320
directly start writing

00:31:33,679 --> 00:31:38,240
into pino and make it available in real

00:31:36,320 --> 00:31:41,519
time

00:31:38,240 --> 00:31:43,840
yeah awesome thanks um

00:31:41,519 --> 00:31:46,640
so in the meantime we also got a few

00:31:43,840 --> 00:31:49,360
questions from the audience um

00:31:46,640 --> 00:31:51,600
the first one is uh how is apache pino

00:31:49,360 --> 00:31:54,799
different from google bigquery or

00:31:51,600 --> 00:31:56,159
aws athena got it um

00:31:54,799 --> 00:31:58,840
i don't really have any slide for that

00:31:56,159 --> 00:32:02,720
but i can talk about it um

00:31:58,840 --> 00:32:06,000
so when you compare uh so pinot

00:32:02,720 --> 00:32:09,360
um the the emphasis or the

00:32:06,000 --> 00:32:12,159
pino is really optimized for um

00:32:09,360 --> 00:32:13,120
accelerating the real-time uh analytics

00:32:12,159 --> 00:32:16,240
right so

00:32:13,120 --> 00:32:17,440
the the focus is on reducing the

00:32:16,240 --> 00:32:19,760
ingestion latency

00:32:17,440 --> 00:32:21,039
of the data coming in so to basically

00:32:19,760 --> 00:32:22,720
make the

00:32:21,039 --> 00:32:24,880
data available to query within

00:32:22,720 --> 00:32:27,760
milliseconds from when it is generated

00:32:24,880 --> 00:32:29,279
from the source uh and also the query

00:32:27,760 --> 00:32:31,679
latency is also

00:32:29,279 --> 00:32:32,480
uh the focus is to uh keep it on the

00:32:31,679 --> 00:32:36,720
millisecond

00:32:32,480 --> 00:32:38,799
range uh and if you look at bigquery

00:32:36,720 --> 00:32:40,480
the it's optimized for a different set

00:32:38,799 --> 00:32:42,320
of problems right it's optimized more

00:32:40,480 --> 00:32:45,200
for more complex

00:32:42,320 --> 00:32:46,480
sql sql queries where the ingestion

00:32:45,200 --> 00:32:49,679
latency

00:32:46,480 --> 00:32:51,760
is may or may not be that important it's

00:32:49,679 --> 00:32:52,720
okay to have the data coming in minutes

00:32:51,760 --> 00:32:55,679
later

00:32:52,720 --> 00:32:57,600
or hours later and the focus is on

00:32:55,679 --> 00:33:00,640
executing more warehouse style

00:32:57,600 --> 00:33:02,320
uh complex equal queries uh and again

00:33:00,640 --> 00:33:04,159
you know the throughput and latency that

00:33:02,320 --> 00:33:06,000
that bigquery can do uh uh

00:33:04,159 --> 00:33:08,640
you know to do something like 100 000

00:33:06,000 --> 00:33:11,600
qps can get prohibitively expensive

00:33:08,640 --> 00:33:12,480
on bigquery whereas pino is designed for

00:33:11,600 --> 00:33:16,080
handling

00:33:12,480 --> 00:33:19,440
you know massive qps on for olap

00:33:16,080 --> 00:33:22,480
cubes cube style queries um with

00:33:19,440 --> 00:33:25,039
amazon athena i think that's more of um

00:33:22,480 --> 00:33:26,159
it's it's more like uh i guess you can

00:33:25,039 --> 00:33:29,360
compare that to

00:33:26,159 --> 00:33:33,200
flink uh more so than than pino here

00:33:29,360 --> 00:33:35,279
um it's it's uh pino is a data store so

00:33:33,200 --> 00:33:37,039
you can put your data in and query it

00:33:35,279 --> 00:33:39,360
whenever you want so you can have

00:33:37,039 --> 00:33:40,960
seven months you know one one year in

00:33:39,360 --> 00:33:44,000
with an uber we have

00:33:40,960 --> 00:33:45,279
something that has uh one uh almost two

00:33:44,000 --> 00:33:46,960
years worth of data

00:33:45,279 --> 00:33:48,320
in pino and some use cases that you can

00:33:46,960 --> 00:33:50,399
query um

00:33:48,320 --> 00:33:52,320
and then it's it's a traditional data

00:33:50,399 --> 00:33:53,919
store right so it's a full semantics

00:33:52,320 --> 00:33:56,880
whereas amazon athena has more on the

00:33:53,919 --> 00:33:56,880
push semantic side

00:33:57,760 --> 00:34:01,120
yeah thanks for the uh yeah so there's

00:34:00,240 --> 00:34:05,279
one more question

00:34:01,120 --> 00:34:08,320
um so how rich are the um

00:34:05,279 --> 00:34:11,599
curing capabilities compared to a

00:34:08,320 --> 00:34:14,639
regular sql yeah yeah great question so

00:34:11,599 --> 00:34:15,520
uh we so pino again is optimized for

00:34:14,639 --> 00:34:18,639
olap

00:34:15,520 --> 00:34:20,639
queries so it can speed up you know

00:34:18,639 --> 00:34:22,240
aggregation functions grouped by an

00:34:20,639 --> 00:34:25,919
order by and all that

00:34:22,240 --> 00:34:29,119
um but what we don't do effectively

00:34:25,919 --> 00:34:31,919
is uh anyway joints for example right we

00:34:29,119 --> 00:34:33,359
the focus is not to support complex

00:34:31,919 --> 00:34:36,639
joints within pino

00:34:33,359 --> 00:34:37,599
um so for that we as fabian already

00:34:36,639 --> 00:34:40,720
mentioned we

00:34:37,599 --> 00:34:43,599
integrated closely with presto to do

00:34:40,720 --> 00:34:44,560
all those complex things in in the

00:34:43,599 --> 00:34:46,839
presto layer

00:34:44,560 --> 00:34:48,079
and pino can handle the filtering

00:34:46,839 --> 00:34:50,879
aggregation uh

00:34:48,079 --> 00:34:51,919
some of the basic window functions uh so

00:34:50,879 --> 00:34:53,440
that that would be

00:34:51,919 --> 00:34:55,919
that would be one like joints is one

00:34:53,440 --> 00:34:58,880
example which is not supported today

00:34:55,919 --> 00:35:00,720
we do have lookup joints within pino um

00:34:58,880 --> 00:35:02,480
so what we what we do is

00:35:00,720 --> 00:35:05,040
let's say you have a small dimension

00:35:02,480 --> 00:35:07,680
table and you want to decorate

00:35:05,040 --> 00:35:09,280
your larger fact tables in pino with

00:35:07,680 --> 00:35:12,400
this small dimension table

00:35:09,280 --> 00:35:14,960
that is supported today so you can

00:35:12,400 --> 00:35:16,880
have do a lookup join locally within

00:35:14,960 --> 00:35:19,839
each pinot server

00:35:16,880 --> 00:35:21,359
so but having you know large fact fact

00:35:19,839 --> 00:35:23,680
join i think that that's not supported

00:35:21,359 --> 00:35:23,680
today

00:35:24,800 --> 00:35:29,359
okay thank you um one more there's even

00:35:28,079 --> 00:35:32,400
two more questions um

00:35:29,359 --> 00:35:33,440
one is a larger one partitioning on the

00:35:32,400 --> 00:35:35,760
upset key

00:35:33,440 --> 00:35:37,440
seems like a strong constraint what's

00:35:35,760 --> 00:35:39,520
the advantage compared to

00:35:37,440 --> 00:35:41,280
writing every event and then using an

00:35:39,520 --> 00:35:46,079
analytical function on it

00:35:41,280 --> 00:35:48,160
like and over partition by order by

00:35:46,079 --> 00:35:49,680
uh additional key order by event time

00:35:48,160 --> 00:35:52,800
and then last function on that

00:35:49,680 --> 00:35:56,240
right yeah so it really comes down to um

00:35:52,800 --> 00:36:00,079
query latency right uh what what we want

00:35:56,240 --> 00:36:01,839
uh to do is minimize the work

00:36:00,079 --> 00:36:03,920
that can that needs to happen at query

00:36:01,839 --> 00:36:06,640
time um as i mentioned

00:36:03,920 --> 00:36:08,240
pino is used for a lot of like user

00:36:06,640 --> 00:36:10,800
facing analytics

00:36:08,240 --> 00:36:12,800
um and and it's embedded in the core

00:36:10,800 --> 00:36:15,760
business flow within linkedin and uber

00:36:12,800 --> 00:36:16,800
so any you know any query latency delays

00:36:15,760 --> 00:36:18,880
will actually affect

00:36:16,800 --> 00:36:20,560
the overall site latency for linkedin

00:36:18,880 --> 00:36:23,040
and uber uh so it's

00:36:20,560 --> 00:36:24,079
it's imperative that the latency sla is

00:36:23,040 --> 00:36:26,880
within

00:36:24,079 --> 00:36:28,720
100 milliseconds so so from that frame

00:36:26,880 --> 00:36:31,920
of thought we wanted to

00:36:28,720 --> 00:36:33,760
minimize what we do at query time

00:36:31,920 --> 00:36:35,680
so we came up with this model where we

00:36:33,760 --> 00:36:37,760
assume that the data is partitioned on

00:36:35,680 --> 00:36:40,720
the primary key beforehand

00:36:37,760 --> 00:36:41,839
um and then within a server we use a

00:36:40,720 --> 00:36:44,079
simple bitmap

00:36:41,839 --> 00:36:46,000
uh to keep track of like we are

00:36:44,079 --> 00:36:46,800
co-locating all the observable events

00:36:46,000 --> 00:36:49,839
together

00:36:46,800 --> 00:36:53,040
which enables pino to do handle upsets

00:36:49,839 --> 00:36:53,599
um you know the on the question of you

00:36:53,040 --> 00:36:56,480
know

00:36:53,599 --> 00:36:57,040
pre-partitioning is expensive uh yes

00:36:56,480 --> 00:36:59,359
yeah

00:36:57,040 --> 00:37:00,480
there is an additional cost to it but

00:36:59,359 --> 00:37:02,480
oftentimes

00:37:00,480 --> 00:37:04,400
you can it's it's a matter of selecting

00:37:02,480 --> 00:37:06,320
a key within your kafka producer

00:37:04,400 --> 00:37:08,240
and that's all if you looked at my demo

00:37:06,320 --> 00:37:10,720
all i did was select a key

00:37:08,240 --> 00:37:12,240
for my kafka producer and that was it

00:37:10,720 --> 00:37:15,200
and this can be done with

00:37:12,240 --> 00:37:15,920
your existing applications or even if

00:37:15,200 --> 00:37:17,359
you're

00:37:15,920 --> 00:37:19,280
you know ingesting change log from

00:37:17,359 --> 00:37:22,880
division you can select a key

00:37:19,280 --> 00:37:25,760
there and so on

00:37:22,880 --> 00:37:26,480
so uh how is pinot different from apache

00:37:25,760 --> 00:37:29,200
droid

00:37:26,480 --> 00:37:30,240
yeah yeah great question this is pino

00:37:29,200 --> 00:37:33,280
and druid

00:37:30,240 --> 00:37:35,520
architecturally very similar both um you

00:37:33,280 --> 00:37:38,079
know ingest data in the same way

00:37:35,520 --> 00:37:39,760
they both column store the differences i

00:37:38,079 --> 00:37:42,480
mentioned in my previous slide

00:37:39,760 --> 00:37:43,680
uh i can just read it out since we don't

00:37:42,480 --> 00:37:46,000
have much time

00:37:43,680 --> 00:37:48,480
uh the one of the main differences is

00:37:46,000 --> 00:37:50,480
the the different set of indexes

00:37:48,480 --> 00:37:51,680
that that we already have and we keep

00:37:50,480 --> 00:37:53,920
adding uh

00:37:51,680 --> 00:37:55,200
pino's pluggable architecture makes it

00:37:53,920 --> 00:37:58,880
very easy to add

00:37:55,200 --> 00:38:01,280
new indexes in a very easy manner so

00:37:58,880 --> 00:38:02,079
currently you know range index json

00:38:01,280 --> 00:38:04,160
index

00:38:02,079 --> 00:38:05,520
geospatial index star tree index this is

00:38:04,160 --> 00:38:08,079
not available in druid

00:38:05,520 --> 00:38:08,800
and this is what makes pinot really fast

00:38:08,079 --> 00:38:11,520
tech search

00:38:08,800 --> 00:38:12,640
is not available in druid um uh that the

00:38:11,520 --> 00:38:14,800
leucine index that

00:38:12,640 --> 00:38:16,160
that we've added in pino being able to

00:38:14,800 --> 00:38:18,079
observe data

00:38:16,160 --> 00:38:22,400
that's actually architectural difference

00:38:18,079 --> 00:38:22,400
that that's there in pino and not into

00:38:30,839 --> 00:38:33,839
it

00:38:40,960 --> 00:38:43,040

YouTube URL: https://www.youtube.com/watch?v=0byVuWrwOhw


