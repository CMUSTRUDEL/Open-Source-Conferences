Title: Yaroslav Tkachenko – It's Time To Stop Using Lambda Architecture
Publication date: 2021-07-01
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Lambda Architecture has been a common way to build data pipelines for a long time, despite difficulties in maintaining two complex systems. An alternative, Kappa Architecture, was proposed in 2014, but many companies are still reluctant to switch to Kappa. And there is a reason for that: even though Kappa generally provides a simpler design and similar or lower latency, there are a lot of practical challenges in areas like exactly-once delivery, late-arriving data, historical backfill and reprocessing.

In this talk, I want to show how you can solve those challenges by embracing Apache Kafka as a foundation of your data pipeline and leveraging modern stream-processing frameworks like Apache Flink.

Speaker:
Yaroslav Tkachenko – https://2021.berlinbuzzwords.de/session/its-time-stop-using-lambda-architecture

More: https://2021.berlinbuzzwords.de/session/its-time-stop-using-lambda-architecture
Captions: 
	00:00:07,279 --> 00:00:10,559
uh thank you for coming

00:00:08,400 --> 00:00:12,960
um my name is yaroslav yeah and i'm

00:00:10,559 --> 00:00:14,960
studying internet shopify and i like

00:00:12,960 --> 00:00:16,160
moving things from batch to streaming a

00:00:14,960 --> 00:00:18,400
lot

00:00:16,160 --> 00:00:20,880
and i've been doing this over the last

00:00:18,400 --> 00:00:23,840
few years and today i just wanted to

00:00:20,880 --> 00:00:25,920
uh share uh some of the things i learned

00:00:23,840 --> 00:00:28,640
i hope it's going to be useful

00:00:25,920 --> 00:00:29,279
right but you know before we start just

00:00:28,640 --> 00:00:30,960
a quick

00:00:29,279 --> 00:00:33,200
uh reminder about the lambda

00:00:30,960 --> 00:00:36,399
architecture you know this is one of the

00:00:33,200 --> 00:00:39,360
very classic uh definitions you can find

00:00:36,399 --> 00:00:40,079
um basically about you know 10 years ago

00:00:39,360 --> 00:00:42,960
when

00:00:40,079 --> 00:00:44,239
you know people came up with this idea

00:00:42,960 --> 00:00:46,160
we didn't have

00:00:44,239 --> 00:00:48,320
a lot of good streaming technologies

00:00:46,160 --> 00:00:50,960
right so batch rolled the world

00:00:48,320 --> 00:00:51,680
um batch technologies were you know very

00:00:50,960 --> 00:00:54,239
reliable

00:00:51,680 --> 00:00:55,280
scalable um and so in this kind of

00:00:54,239 --> 00:00:58,320
architecture

00:00:55,280 --> 00:01:01,600
you implement a batch layer

00:00:58,320 --> 00:01:04,720
with different batch transformations but

00:01:01,600 --> 00:01:05,840
to compensate for the very high latency

00:01:04,720 --> 00:01:07,920
because it can you know

00:01:05,840 --> 00:01:09,040
it can run for a few hours for example

00:01:07,920 --> 00:01:10,479
you know we introduced some kind of

00:01:09,040 --> 00:01:13,360
speed layer

00:01:10,479 --> 00:01:14,400
and then you combine those in various

00:01:13,360 --> 00:01:16,159
different ways

00:01:14,400 --> 00:01:17,600
um and as you can see there's a lot of

00:01:16,159 --> 00:01:19,439
complexity in this kind of

00:01:17,600 --> 00:01:20,880
architecture you know you need to come

00:01:19,439 --> 00:01:22,479
up with two different systems and

00:01:20,880 --> 00:01:25,600
maintain them separately

00:01:22,479 --> 00:01:26,000
and then you need to reconcile so this

00:01:25,600 --> 00:01:28,080
is quite

00:01:26,000 --> 00:01:29,439
complex uh nevertheless you know a lot

00:01:28,080 --> 00:01:31,600
of companies

00:01:29,439 --> 00:01:32,960
used it over the years and it's still

00:01:31,600 --> 00:01:36,560
still in use

00:01:32,960 --> 00:01:39,759
but i think we can do better right and

00:01:36,560 --> 00:01:43,119
i also want to share some of the things

00:01:39,759 --> 00:01:43,520
i saw you know when maybe um this is not

00:01:43,119 --> 00:01:45,360
a

00:01:43,520 --> 00:01:48,479
full land architecture but still people

00:01:45,360 --> 00:01:50,880
use badge in places where you can avoid

00:01:48,479 --> 00:01:51,840
you know for example uh maybe you have a

00:01:50,880 --> 00:01:53,520
streaming job

00:01:51,840 --> 00:01:56,000
and you write a lot of data some kind of

00:01:53,520 --> 00:01:57,439
object store and then occasionally you

00:01:56,000 --> 00:02:00,000
know you detect some bad data

00:01:57,439 --> 00:02:02,640
and you end up using a batch job to go

00:02:00,000 --> 00:02:05,759
and fix it you know maybe go back

00:02:02,640 --> 00:02:08,080
in history another very common use case

00:02:05,759 --> 00:02:09,360
uh maybe you're writing a bunch of files

00:02:08,080 --> 00:02:12,400
in object store

00:02:09,360 --> 00:02:12,879
and uh the small files problem is uh you

00:02:12,400 --> 00:02:14,879
know

00:02:12,879 --> 00:02:16,640
very very common one to have when you

00:02:14,879 --> 00:02:19,280
something like spark or presto

00:02:16,640 --> 00:02:20,400
to query those uh files so you can come

00:02:19,280 --> 00:02:22,640
up with a batch job

00:02:20,400 --> 00:02:24,560
you know to optimize this uh you know

00:02:22,640 --> 00:02:26,319
file size for you to compact those small

00:02:24,560 --> 00:02:27,599
files and this is something that's very

00:02:26,319 --> 00:02:29,680
common as well

00:02:27,599 --> 00:02:31,599
or you know classic example uh

00:02:29,680 --> 00:02:33,360
historical reprocessing like

00:02:31,599 --> 00:02:35,360
something's changing in business logic

00:02:33,360 --> 00:02:36,640
and you need to go back in time and

00:02:35,360 --> 00:02:39,280
basically reprocess

00:02:36,640 --> 00:02:41,120
all data right so i i see all those

00:02:39,280 --> 00:02:41,599
slanted incarnations all those uh

00:02:41,120 --> 00:02:44,800
different

00:02:41,599 --> 00:02:47,760
uh bad use cases all over the place

00:02:44,800 --> 00:02:49,040
and i think again we can do better um so

00:02:47,760 --> 00:02:51,599
cup architecture

00:02:49,040 --> 00:02:53,760
is a very simple idea right you

00:02:51,599 --> 00:02:57,120
essentially take the lambda architecture

00:02:53,760 --> 00:02:59,280
you drop the badge and that you know

00:02:57,120 --> 00:03:00,640
what's left is a gap architect right you

00:02:59,280 --> 00:03:04,720
rely on streaming

00:03:00,640 --> 00:03:06,319
quite a bit and your streaming layer

00:03:04,720 --> 00:03:08,560
basically handles all the new data

00:03:06,319 --> 00:03:09,040
that's coming you have some kind of data

00:03:08,560 --> 00:03:10,879
store

00:03:09,040 --> 00:03:12,879
where you write your results and you

00:03:10,879 --> 00:03:15,440
query the data store right so

00:03:12,879 --> 00:03:17,040
again very similar uh just uh really

00:03:15,440 --> 00:03:19,360
focus on streaming as

00:03:17,040 --> 00:03:21,040
a first-class citizen and rely on

00:03:19,360 --> 00:03:24,480
streaming in all those

00:03:21,040 --> 00:03:27,120
interesting situations right and so

00:03:24,480 --> 00:03:27,519
very very typical concerns that people

00:03:27,120 --> 00:03:29,280
uh

00:03:27,519 --> 00:03:30,799
you know come up with when you talk

00:03:29,280 --> 00:03:32,159
about kappa

00:03:30,799 --> 00:03:34,319
you know and first one is data

00:03:32,159 --> 00:03:36,879
availability and retention right so

00:03:34,319 --> 00:03:37,599
maybe use something like apache kafka as

00:03:36,879 --> 00:03:40,000
your

00:03:37,599 --> 00:03:40,799
uh streaming platform and maybe you have

00:03:40,000 --> 00:03:42,959
a default

00:03:40,799 --> 00:03:45,360
seven days retention for a bunch of

00:03:42,959 --> 00:03:47,200
kafka topics uh so when the data

00:03:45,360 --> 00:03:49,280
is gone you know you can't really do

00:03:47,200 --> 00:03:51,599
much you can't go back to history

00:03:49,280 --> 00:03:53,280
um what do you do right so that that's a

00:03:51,599 --> 00:03:55,680
that's a problem

00:03:53,280 --> 00:03:57,360
data consistency is still sometime uh

00:03:55,680 --> 00:03:58,239
you know you can hear people say oh this

00:03:57,360 --> 00:04:00,159
is streaming

00:03:58,239 --> 00:04:01,840
so you maybe expect some duplicates or

00:04:00,159 --> 00:04:04,080
even like missing data

00:04:01,840 --> 00:04:06,159
um and we'll talk more about that today

00:04:04,080 --> 00:04:07,120
um handling driving data is also very

00:04:06,159 --> 00:04:09,519
common topic

00:04:07,120 --> 00:04:10,480
right um and batch and streaming you

00:04:09,519 --> 00:04:11,920
know can can

00:04:10,480 --> 00:04:14,319
i can do it differently depending on the

00:04:11,920 --> 00:04:15,840
variety of different use cases um and

00:04:14,319 --> 00:04:17,759
finally i think this is

00:04:15,840 --> 00:04:19,680
also something that's very common you

00:04:17,759 --> 00:04:20,639
know how do you reprocess how do you

00:04:19,680 --> 00:04:22,560
backfill data

00:04:20,639 --> 00:04:24,960
like if you have truly streaming system

00:04:22,560 --> 00:04:26,880
end-to-end no batch whatsoever

00:04:24,960 --> 00:04:28,720
how do you go back and like reprocess

00:04:26,880 --> 00:04:31,600
years of data potentially in a

00:04:28,720 --> 00:04:32,320
in a reliable and reasonable way right

00:04:31,600 --> 00:04:34,160
so

00:04:32,320 --> 00:04:37,360
all those concerns something we want to

00:04:34,160 --> 00:04:38,880
try to address but before we continue

00:04:37,360 --> 00:04:41,680
i just want to share you know my

00:04:38,880 --> 00:04:44,400
thoughts why do i like streaming so much

00:04:41,680 --> 00:04:46,639
and typically when you compare batch and

00:04:44,400 --> 00:04:49,360
streaming people think about latency

00:04:46,639 --> 00:04:50,400
right and it's it's a nice one but it's

00:04:49,360 --> 00:04:52,880
not the main goal

00:04:50,400 --> 00:04:54,160
right because you never know how much

00:04:52,880 --> 00:04:56,240
latency is okay

00:04:54,160 --> 00:04:58,400
like imagine you have this batch job

00:04:56,240 --> 00:05:00,880
that's running every 12 hours

00:04:58,400 --> 00:05:01,840
and you decide to transition to a

00:05:00,880 --> 00:05:04,880
streaming system

00:05:01,840 --> 00:05:07,520
you know uh full kappa what is

00:05:04,880 --> 00:05:07,919
enough latency or what is a good latency

00:05:07,520 --> 00:05:09,759
here

00:05:07,919 --> 00:05:11,759
it's essentially between 12 hours and

00:05:09,759 --> 00:05:14,320
zero but it can't really say

00:05:11,759 --> 00:05:15,280
what's fine right so yes decreasing

00:05:14,320 --> 00:05:19,199
latency is good

00:05:15,280 --> 00:05:21,759
but i don't think it's the main benefit

00:05:19,199 --> 00:05:22,400
and uh late driving data is something

00:05:21,759 --> 00:05:24,560
that

00:05:22,400 --> 00:05:26,320
actually can be very simple with

00:05:24,560 --> 00:05:28,240
streaming depending on the use case

00:05:26,320 --> 00:05:30,400
because with batch usually you're on

00:05:28,240 --> 00:05:31,039
some kind of incremental batch job maybe

00:05:30,400 --> 00:05:34,240
every day

00:05:31,039 --> 00:05:36,000
maybe every few hours and if you have

00:05:34,240 --> 00:05:38,240
light driving data

00:05:36,000 --> 00:05:40,320
the typical approach is to just wait

00:05:38,240 --> 00:05:44,160
longer right so maybe

00:05:40,320 --> 00:05:46,960
your daily badge starts at uh 4 am udc

00:05:44,160 --> 00:05:48,000
and you allow this 4-hour buffer right

00:05:46,960 --> 00:05:50,560
and it's very common

00:05:48,000 --> 00:05:52,479
and it's not great because you rarely

00:05:50,560 --> 00:05:54,400
have that guarantee that all

00:05:52,479 --> 00:05:55,759
late data will arrive in this four hour

00:05:54,400 --> 00:05:58,080
window so

00:05:55,759 --> 00:05:59,199
occasionally it actually arrives later

00:05:58,080 --> 00:06:00,000
and then you need to go back and

00:05:59,199 --> 00:06:02,319
reprocess

00:06:00,000 --> 00:06:04,160
and how much time is it okay like can

00:06:02,319 --> 00:06:06,960
they go back and we process many days

00:06:04,160 --> 00:06:08,160
a week more than that all those

00:06:06,960 --> 00:06:10,400
questions arise and

00:06:08,160 --> 00:06:11,360
it's just not clear you know with

00:06:10,400 --> 00:06:12,880
streaming

00:06:11,360 --> 00:06:14,639
if you for example have stainless

00:06:12,880 --> 00:06:16,720
transformations and you have

00:06:14,639 --> 00:06:18,800
your sinks or destinations that support

00:06:16,720 --> 00:06:19,520
updates that's extremely easy right

00:06:18,800 --> 00:06:22,319
because

00:06:19,520 --> 00:06:23,440
no matter how old is the message or the

00:06:22,319 --> 00:06:25,520
event the

00:06:23,440 --> 00:06:27,600
piece of data that you receive you know

00:06:25,520 --> 00:06:30,160
you can always shout it to that

00:06:27,600 --> 00:06:30,960
historical uh you know partition or date

00:06:30,160 --> 00:06:33,600
or whatever

00:06:30,960 --> 00:06:34,000
like that location um and it's gonna be

00:06:33,600 --> 00:06:36,160
fine

00:06:34,000 --> 00:06:37,600
right because you can just update that

00:06:36,160 --> 00:06:40,639
historical um

00:06:37,600 --> 00:06:42,479
circle um uh record right

00:06:40,639 --> 00:06:44,560
with stateful transformations it's a bit

00:06:42,479 --> 00:06:46,800
more tricky right because it depends on

00:06:44,560 --> 00:06:48,720
how exactly you state how long they keep

00:06:46,800 --> 00:06:50,479
it how how do you expire it can you

00:06:48,720 --> 00:06:52,960
recover state somehow

00:06:50,479 --> 00:06:53,840
and small state is general generally

00:06:52,960 --> 00:06:56,560
fine but

00:06:53,840 --> 00:06:57,919
uh the larger the state becomes it can

00:06:56,560 --> 00:07:00,479
be tricky

00:06:57,919 --> 00:07:02,160
and if you have uh you know destinations

00:07:00,479 --> 00:07:03,759
in your data pipeline that don't support

00:07:02,160 --> 00:07:05,840
updates

00:07:03,759 --> 00:07:08,240
at all so for example you just store a

00:07:05,840 --> 00:07:09,680
lot of porky files on s3

00:07:08,240 --> 00:07:11,520
you know this is not something you can

00:07:09,680 --> 00:07:12,000
easily update from a streaming job you

00:07:11,520 --> 00:07:14,000
know

00:07:12,000 --> 00:07:17,120
this this depends so we'll talk more

00:07:14,000 --> 00:07:18,800
about this as well but i think the main

00:07:17,120 --> 00:07:20,720
reason why you would like to use

00:07:18,800 --> 00:07:23,120
streaming over badge nowadays

00:07:20,720 --> 00:07:24,479
is just operations and observability and

00:07:23,120 --> 00:07:27,680
mentality around it

00:07:24,479 --> 00:07:29,680
right with badge it's fine when things

00:07:27,680 --> 00:07:31,919
fail you know just wait another six

00:07:29,680 --> 00:07:33,039
hours and hopefully you know the retry

00:07:31,919 --> 00:07:35,440
drug will fix it

00:07:33,039 --> 00:07:37,599
if not we might retry one more time or

00:07:35,440 --> 00:07:39,759
we'll start investigating

00:07:37,599 --> 00:07:41,680
or maybe someone disabled the wrong job

00:07:39,759 --> 00:07:42,960
and because you don't really have good

00:07:41,680 --> 00:07:45,919
monitoring in place

00:07:42,960 --> 00:07:46,479
or you know good alerts for a bad job

00:07:45,919 --> 00:07:49,120
you know

00:07:46,479 --> 00:07:50,879
nobody noticed um and maybe you don't

00:07:49,120 --> 00:07:52,960
really have good metrics

00:07:50,879 --> 00:07:54,639
like how much metrics do you instrument

00:07:52,960 --> 00:07:56,720
for a batch job

00:07:54,639 --> 00:07:58,560
um and with streaming you know when you

00:07:56,720 --> 00:08:00,000
use modern frameworks like kafka streams

00:07:58,560 --> 00:08:01,120
apache link and you deploy it on

00:08:00,000 --> 00:08:02,960
kubernetes

00:08:01,120 --> 00:08:05,360
uh you change your mentality right

00:08:02,960 --> 00:08:08,400
because now you start treating those

00:08:05,360 --> 00:08:10,160
as real like normal applications

00:08:08,400 --> 00:08:11,680
web applications perhaps right the

00:08:10,160 --> 00:08:13,440
server real traffic

00:08:11,680 --> 00:08:15,520
and you start thinking about uptime

00:08:13,440 --> 00:08:17,440
expectations and slos and

00:08:15,520 --> 00:08:19,919
you can fully embrace all the things

00:08:17,440 --> 00:08:21,919
around say cd and observability

00:08:19,919 --> 00:08:23,759
all this all these good practices right

00:08:21,919 --> 00:08:24,560
so i think what's important to keep in

00:08:23,759 --> 00:08:26,560
mind

00:08:24,560 --> 00:08:27,680
it's not just latency right it's that

00:08:26,560 --> 00:08:30,080
mentality that

00:08:27,680 --> 00:08:31,919
uh allows you to treat your uh data

00:08:30,080 --> 00:08:33,760
processing applications as

00:08:31,919 --> 00:08:35,599
you know normal applications or certain

00:08:33,760 --> 00:08:38,839
traffic and that's quite important

00:08:35,599 --> 00:08:40,479
if you want to have a reliable data

00:08:38,839 --> 00:08:42,399
pipeline

00:08:40,479 --> 00:08:44,159
so now i want to cover a few different

00:08:42,399 --> 00:08:44,959
building blocks uh that you would need

00:08:44,159 --> 00:08:46,560
for kappa

00:08:44,959 --> 00:08:49,360
and i'm going to focus on three key

00:08:46,560 --> 00:08:52,080
areas here so i'm going to focus on the

00:08:49,360 --> 00:08:53,040
actual log abstraction or the streaming

00:08:52,080 --> 00:08:56,000
platform

00:08:53,040 --> 00:08:56,560
in in my case apache kafka um i'm going

00:08:56,000 --> 00:08:59,760
to focus

00:08:56,560 --> 00:09:00,480
on the streaming framework of link and

00:08:59,760 --> 00:09:02,560
just

00:09:00,480 --> 00:09:05,040
uh share a little bit about the data

00:09:02,560 --> 00:09:06,959
syncs or destinations and i'm going to

00:09:05,040 --> 00:09:09,120
share a few things about iceberg about

00:09:06,959 --> 00:09:11,440
apache iceberg

00:09:09,120 --> 00:09:13,200
so uh let's start with kafka and the

00:09:11,440 --> 00:09:13,839
very basic building block that you might

00:09:13,200 --> 00:09:15,920
require

00:09:13,839 --> 00:09:17,680
is kafkatopic compaction you know this

00:09:15,920 --> 00:09:19,920
is something that's

00:09:17,680 --> 00:09:20,959
pretty common nowadays people use it

00:09:19,920 --> 00:09:24,480
quite a bit

00:09:20,959 --> 00:09:26,959
um and this uh allows you to essentially

00:09:24,480 --> 00:09:29,519
compact your history in a in a kafka

00:09:26,959 --> 00:09:32,320
topic and only keep the latest version

00:09:29,519 --> 00:09:33,680
if you use a message key right so by

00:09:32,320 --> 00:09:35,120
that message key

00:09:33,680 --> 00:09:36,880
you can essentially come back to the

00:09:35,120 --> 00:09:39,120
historical updates and this

00:09:36,880 --> 00:09:40,720
enables infinite retention right so if

00:09:39,120 --> 00:09:42,560
you can use compaction

00:09:40,720 --> 00:09:44,800
um this allows you to keep your data in

00:09:42,560 --> 00:09:46,399
kafka forever and this is kind of one of

00:09:44,800 --> 00:09:49,839
the prerequisites for

00:09:46,399 --> 00:09:52,959
true copper architecture

00:09:49,839 --> 00:09:54,480
alternative approach would be to use

00:09:52,959 --> 00:09:56,399
kafka tiered search

00:09:54,480 --> 00:09:58,240
and this is something that still work in

00:09:56,399 --> 00:10:00,080
progress there's a kafka improvement

00:09:58,240 --> 00:10:02,640
proposal for that

00:10:00,080 --> 00:10:05,040
and you know in theory this feature

00:10:02,640 --> 00:10:07,839
allows you to

00:10:05,040 --> 00:10:10,000
keep uh hot data in kafka and call data

00:10:07,839 --> 00:10:11,760
in an object store like s3

00:10:10,000 --> 00:10:13,839
but from a consumer perspective you

00:10:11,760 --> 00:10:16,720
still interact with a single kafka topic

00:10:13,839 --> 00:10:18,000
and all the data movement happens in the

00:10:16,720 --> 00:10:20,079
background right so if

00:10:18,000 --> 00:10:21,760
you start consuming from the earliest

00:10:20,079 --> 00:10:24,240
offset uh kafka

00:10:21,760 --> 00:10:25,440
will know how to get the data from s3

00:10:24,240 --> 00:10:29,200
visualize it

00:10:25,440 --> 00:10:31,839
send it your way right and um you know

00:10:29,200 --> 00:10:33,760
that's still working progress but people

00:10:31,839 --> 00:10:34,560
have been using this topic archive

00:10:33,760 --> 00:10:36,640
pattern

00:10:34,560 --> 00:10:38,800
for years where you simply do that

00:10:36,640 --> 00:10:42,079
yourself right and then either

00:10:38,800 --> 00:10:44,480
you consume from kafka ns3 sort of

00:10:42,079 --> 00:10:45,600
in parallel have a union of two

00:10:44,480 --> 00:10:48,640
different sources

00:10:45,600 --> 00:10:50,079
or you recover data from s3 and send it

00:10:48,640 --> 00:10:50,800
to a kafka top you can consume from

00:10:50,079 --> 00:10:52,720
there

00:10:50,800 --> 00:10:54,640
but this also enables that instant

00:10:52,720 --> 00:10:55,360
retention because the object store is

00:10:54,640 --> 00:10:57,600
very cheap

00:10:55,360 --> 00:10:59,360
comparing to storing all the data in

00:10:57,600 --> 00:11:01,839
kafka so this is something you you might

00:10:59,360 --> 00:11:04,160
need as well

00:11:01,839 --> 00:11:05,680
um kafka has introduced transactions

00:11:04,160 --> 00:11:08,800
about four years ago

00:11:05,680 --> 00:11:09,839
uh so yes this uh syntax on the right is

00:11:08,800 --> 00:11:11,440
available to you

00:11:09,839 --> 00:11:13,279
if you want to use um you know with the

00:11:11,440 --> 00:11:15,360
kafka producer um

00:11:13,279 --> 00:11:16,800
and yeah it's been introduced quite a

00:11:15,360 --> 00:11:20,399
bit uh ago

00:11:16,800 --> 00:11:22,880
uh 411 and with kafka transactions

00:11:20,399 --> 00:11:25,519
uh you can eliminate duplicates right

00:11:22,880 --> 00:11:28,240
and this ensures your data consistency

00:11:25,519 --> 00:11:29,279
uh those transaction transactions um

00:11:28,240 --> 00:11:30,800
need to work

00:11:29,279 --> 00:11:33,040
uh with the consumer itself like

00:11:30,800 --> 00:11:34,720
consumer will need to

00:11:33,040 --> 00:11:37,440
tweak a few different parameters on

00:11:34,720 --> 00:11:39,519
their site to understand you know if

00:11:37,440 --> 00:11:40,720
the message is fully committed or not

00:11:39,519 --> 00:11:42,800
within the transaction

00:11:40,720 --> 00:11:45,040
and it also needs some kafka broker

00:11:42,800 --> 00:11:47,279
tuning but not that much and

00:11:45,040 --> 00:11:49,120
you know you can just start using it

00:11:47,279 --> 00:11:52,880
it's been it's been

00:11:49,120 --> 00:11:52,880
it's been used for years now

00:11:53,120 --> 00:11:57,680
another very useful building block is

00:11:55,839 --> 00:11:59,600
some form of data integration

00:11:57,680 --> 00:12:01,920
that can bring all types of data to

00:11:59,600 --> 00:12:04,160
kafka right and this solves this

00:12:01,920 --> 00:12:05,120
but i don't have this data in kafka

00:12:04,160 --> 00:12:07,360
question

00:12:05,120 --> 00:12:09,040
um you can use frameworks like kafka

00:12:07,360 --> 00:12:12,480
connect for example

00:12:09,040 --> 00:12:16,160
to bring the data in a very

00:12:12,480 --> 00:12:18,399
uh sort of unified fashion

00:12:16,160 --> 00:12:19,279
it's important to avoid building one-off

00:12:18,399 --> 00:12:21,279
integrations

00:12:19,279 --> 00:12:23,200
because this can get messy and

00:12:21,279 --> 00:12:24,880
unmanageable very quickly

00:12:23,200 --> 00:12:26,480
but if you something like kafka connect

00:12:24,880 --> 00:12:29,040
or some other um

00:12:26,480 --> 00:12:30,160
similar tools you have essentially a

00:12:29,040 --> 00:12:32,399
platform for

00:12:30,160 --> 00:12:34,160
integrating kafka with your sources but

00:12:32,399 --> 00:12:36,079
as well as syncs right so if you need to

00:12:34,160 --> 00:12:39,839
send data to different destinations

00:12:36,079 --> 00:12:39,839
kafka connect can help there as well

00:12:40,079 --> 00:12:46,639
uh finally here um in order to

00:12:43,200 --> 00:12:49,279
reproduce um sorry in order to reprocess

00:12:46,639 --> 00:12:51,200
a lot of data uh with kafka you might

00:12:49,279 --> 00:12:52,000
need to have a pretty large cluster

00:12:51,200 --> 00:12:54,639
right that that

00:12:52,000 --> 00:12:56,000
historical reprocessing is is a tricky

00:12:54,639 --> 00:12:58,959
uh tricky concept

00:12:56,000 --> 00:12:59,920
and you know uh if you ever managed

00:12:58,959 --> 00:13:03,040
kafka you know

00:12:59,920 --> 00:13:05,279
that scaling kafka is hard like when you

00:13:03,040 --> 00:13:07,040
increase number of workers in a cluster

00:13:05,279 --> 00:13:09,120
it needs to do a lot of reshuffling and

00:13:07,040 --> 00:13:11,440
it can take sour seven days

00:13:09,120 --> 00:13:13,680
so there is this idea of essentially

00:13:11,440 --> 00:13:17,600
treating kafka clusters as immutable

00:13:13,680 --> 00:13:21,120
entities and scaling your capacity by

00:13:17,600 --> 00:13:22,720
adding more clusters right and

00:13:21,120 --> 00:13:24,399
this is something that netflix has done

00:13:22,720 --> 00:13:26,079
a few years ago um

00:13:24,399 --> 00:13:27,519
this is pretty advanced i would say

00:13:26,079 --> 00:13:30,079
because you would need

00:13:27,519 --> 00:13:31,440
to uh you know implement producers and

00:13:30,079 --> 00:13:33,440
consumers certain way

00:13:31,440 --> 00:13:35,120
like they would need to discover new

00:13:33,440 --> 00:13:36,160
clusters and understand how to route

00:13:35,120 --> 00:13:38,720
data between

00:13:36,160 --> 00:13:39,279
multiple uh clusters but if you can't do

00:13:38,720 --> 00:13:42,240
it

00:13:39,279 --> 00:13:43,839
uh you have this elasticity now where

00:13:42,240 --> 00:13:44,959
you know for a big historical

00:13:43,839 --> 00:13:47,040
reprocessing

00:13:44,959 --> 00:13:48,079
you might wanna you know add a lot the

00:13:47,040 --> 00:13:50,399
capacity

00:13:48,079 --> 00:13:51,360
quickly go through the backlog maybe in

00:13:50,399 --> 00:13:53,199
a few hours

00:13:51,360 --> 00:13:55,040
and shut down all the clusters you don't

00:13:53,199 --> 00:13:57,600
need and that happens very quickly

00:13:55,040 --> 00:13:59,199
so this is pretty aspirational i would

00:13:57,600 --> 00:14:03,360
say but if you can get it right

00:13:59,199 --> 00:14:05,199
it's going to be a huge huge benefit

00:14:03,360 --> 00:14:07,279
so uh now switching to the streaming

00:14:05,199 --> 00:14:08,320
engine um and one of the mandatory

00:14:07,279 --> 00:14:10,480
things you need

00:14:08,320 --> 00:14:12,480
for any kind of complex copper's case is

00:14:10,480 --> 00:14:15,440
reliable and scalable state

00:14:12,480 --> 00:14:16,880
as a part of the streaming engine and um

00:14:15,440 --> 00:14:18,959
you know with a patch of link you have

00:14:16,880 --> 00:14:20,959
access to a keyed state

00:14:18,959 --> 00:14:23,279
which is extremely useful concept

00:14:20,959 --> 00:14:24,880
because uh it allows you to essentially

00:14:23,279 --> 00:14:27,519
scale your state

00:14:24,880 --> 00:14:28,480
you know um not forever but to a very

00:14:27,519 --> 00:14:30,320
large amount

00:14:28,480 --> 00:14:32,639
uh just because uh all the state is

00:14:30,320 --> 00:14:34,480
partitioned by definition right

00:14:32,639 --> 00:14:35,680
you use some some kind of key to

00:14:34,480 --> 00:14:38,959
partition your state

00:14:35,680 --> 00:14:39,920
and then you would uh you know have that

00:14:38,959 --> 00:14:42,320
state kind of

00:14:39,920 --> 00:14:43,680
distributed among the workers in your

00:14:42,320 --> 00:14:46,000
cluster

00:14:43,680 --> 00:14:47,040
and checkpointing guarantees that the

00:14:46,000 --> 00:14:49,199
state is fault

00:14:47,040 --> 00:14:50,880
tolerant right it will send the

00:14:49,199 --> 00:14:53,760
checkpoint data to some kind of

00:14:50,880 --> 00:14:55,839
persistent object store like s3 or gcs

00:14:53,760 --> 00:14:58,480
so you can recover from failures

00:14:55,839 --> 00:14:59,600
and this kind of state is actually not

00:14:58,480 --> 00:15:02,480
something you

00:14:59,600 --> 00:15:04,399
you know you maybe see everywhere but

00:15:02,480 --> 00:15:04,800
it's used as a building block for a lot

00:15:04,399 --> 00:15:08,160
of

00:15:04,800 --> 00:15:10,560
higher level concepts so um the next one

00:15:08,160 --> 00:15:12,560
i want to cover is flank exactly once

00:15:10,560 --> 00:15:14,720
very similar to kafka one right we want

00:15:12,560 --> 00:15:15,680
to have end to end exactly once

00:15:14,720 --> 00:15:18,720
guarantees

00:15:15,680 --> 00:15:20,959
and the link exactly once uh

00:15:18,720 --> 00:15:22,079
has this um you know function that's

00:15:20,959 --> 00:15:24,320
implemented uh

00:15:22,079 --> 00:15:26,399
for essentially you know having uh

00:15:24,320 --> 00:15:27,760
two-phase commit semantics

00:15:26,399 --> 00:15:29,519
and as you can see based on the

00:15:27,760 --> 00:15:31,839
signature it uses some of the

00:15:29,519 --> 00:15:33,040
uh state functionality to actually

00:15:31,839 --> 00:15:35,440
achieve that

00:15:33,040 --> 00:15:36,320
and it combines that with the kafka

00:15:35,440 --> 00:15:38,800
transactions

00:15:36,320 --> 00:15:40,480
right so it now said the leverage kafka

00:15:38,800 --> 00:15:42,880
transactions it knows how to leverage

00:15:40,480 --> 00:15:44,639
internal state and it combines those two

00:15:42,880 --> 00:15:47,519
concepts in order to provide you

00:15:44,639 --> 00:15:48,079
exactly once delivery all the way right

00:15:47,519 --> 00:15:50,000
so you

00:15:48,079 --> 00:15:51,600
you can consume something from kafka

00:15:50,000 --> 00:15:53,120
transform and produce it and you will

00:15:51,600 --> 00:15:55,519
pursue that guarantee

00:15:53,120 --> 00:15:58,399
uh throughout the process and what's

00:15:55,519 --> 00:16:00,480
useful about this uh it's actually um

00:15:58,399 --> 00:16:02,800
pretty generic and you can implement

00:16:00,480 --> 00:16:06,000
your own custom sources and sinks

00:16:02,800 --> 00:16:09,199
uh to have that exactly once uh delivery

00:16:06,000 --> 00:16:11,920
guarantees as well

00:16:09,199 --> 00:16:12,880
um flink state management so again state

00:16:11,920 --> 00:16:14,639
is important

00:16:12,880 --> 00:16:16,880
and typically when we talk about

00:16:14,639 --> 00:16:18,000
stateful streaming you know you

00:16:16,880 --> 00:16:20,079
might want to use something like

00:16:18,000 --> 00:16:21,839
windowing aggregations joins

00:16:20,079 --> 00:16:23,120
uh you know very classic example some

00:16:21,839 --> 00:16:26,320
kind of window

00:16:23,120 --> 00:16:29,360
uh for your uh computation that

00:16:26,320 --> 00:16:32,560
stores uh state in each of the segments

00:16:29,360 --> 00:16:35,120
um and i think um that that can

00:16:32,560 --> 00:16:36,800
work really well for simple use cases

00:16:35,120 --> 00:16:38,880
but with more complex use cases you

00:16:36,800 --> 00:16:40,959
realize that you need something like

00:16:38,880 --> 00:16:43,519
you know state variables timers and side

00:16:40,959 --> 00:16:45,600
outputs like those three building blocks

00:16:43,519 --> 00:16:47,279
is something that flink provides you to

00:16:45,600 --> 00:16:48,399
create very complex and advanced

00:16:47,279 --> 00:16:50,399
workflows

00:16:48,399 --> 00:16:52,160
like some of the uh scenarios that we

00:16:50,399 --> 00:16:54,720
built at shopify you know you can

00:16:52,160 --> 00:16:56,720
use a state as a database essentially uh

00:16:54,720 --> 00:16:59,440
right because you can manage that state

00:16:56,720 --> 00:17:00,880
um in a very fine-grained way or you can

00:16:59,440 --> 00:17:02,160
even come up with workflows where you

00:17:00,880 --> 00:17:05,600
can repopulate state

00:17:02,160 --> 00:17:07,120
um by handling layer writing messages

00:17:05,600 --> 00:17:08,880
again this is something that's pretty

00:17:07,120 --> 00:17:11,199
low level and you need to

00:17:08,880 --> 00:17:13,280
embrace you know custom state variables

00:17:11,199 --> 00:17:14,880
timers maybe side outputs

00:17:13,280 --> 00:17:16,799
but by doing that you actually have

00:17:14,880 --> 00:17:20,319
access to more workflows

00:17:16,799 --> 00:17:22,959
if you just uh use let's say window with

00:17:20,319 --> 00:17:22,959
aggregations

00:17:23,280 --> 00:17:28,079
um this is one of my favorite features

00:17:25,360 --> 00:17:30,080
flink state processor api

00:17:28,079 --> 00:17:31,440
you know with historical backfill you

00:17:30,080 --> 00:17:32,720
have this challenge

00:17:31,440 --> 00:17:34,720
where you know you might need to

00:17:32,720 --> 00:17:35,280
reproduce a lot of data a lot of typical

00:17:34,720 --> 00:17:37,120
data

00:17:35,280 --> 00:17:38,640
when your business logic has changed and

00:17:37,120 --> 00:17:40,320
you know all the state that you have in

00:17:38,640 --> 00:17:43,760
your pipeline is essentially

00:17:40,320 --> 00:17:45,520
you know no longer valid um this api

00:17:43,760 --> 00:17:47,679
actually allows you to avoid that

00:17:45,520 --> 00:17:49,679
historical processing right because

00:17:47,679 --> 00:17:51,120
it gives you programmatic access to the

00:17:49,679 --> 00:17:53,919
state itself

00:17:51,120 --> 00:17:55,840
so what you can do instead of going and

00:17:53,919 --> 00:17:57,360
reprocessing the two years of dating

00:17:55,840 --> 00:18:00,480
kafka

00:17:57,360 --> 00:18:01,760
you can write your uh you know little

00:18:00,480 --> 00:18:03,760
application to

00:18:01,760 --> 00:18:05,600
read the state update exactly what you

00:18:03,760 --> 00:18:08,960
need kind of in place

00:18:05,600 --> 00:18:10,480
and create a new save point and this can

00:18:08,960 --> 00:18:13,120
combine the power of badge

00:18:10,480 --> 00:18:14,799
uh because flink provides that api as a

00:18:13,120 --> 00:18:17,360
batch api at the moment

00:18:14,799 --> 00:18:18,320
and streaming um so the batch can

00:18:17,360 --> 00:18:21,520
process the

00:18:18,320 --> 00:18:24,000
um save point uh this the state

00:18:21,520 --> 00:18:24,799
file quickly and you can bootstrap a new

00:18:24,000 --> 00:18:26,799
streaming job

00:18:24,799 --> 00:18:28,000
right so you avoid that very expensive

00:18:26,799 --> 00:18:29,600
historical backfill

00:18:28,000 --> 00:18:31,200
and this is something i think that's

00:18:29,600 --> 00:18:34,400
unique to flink and

00:18:31,200 --> 00:18:37,440
very very cool feature that i

00:18:34,400 --> 00:18:40,240
i just love um

00:18:37,440 --> 00:18:42,320
let's see so the next i just want to

00:18:40,240 --> 00:18:44,559
mention briefly about data syncs

00:18:42,320 --> 00:18:45,840
because i think it's pretty obvious that

00:18:44,559 --> 00:18:47,919
some data syncs

00:18:45,840 --> 00:18:49,200
work better for streaming use cases than

00:18:47,919 --> 00:18:50,559
others and

00:18:49,200 --> 00:18:52,720
one of the very important things you

00:18:50,559 --> 00:18:53,360
need to keep in mind is the ability to

00:18:52,720 --> 00:18:56,000
support

00:18:53,360 --> 00:18:57,679
updates and absurds right because if

00:18:56,000 --> 00:19:00,320
your data destination

00:18:57,679 --> 00:19:00,960
uh you know supports those uh those

00:19:00,320 --> 00:19:03,440
features

00:19:00,960 --> 00:19:05,120
uh they can be used for data correction

00:19:03,440 --> 00:19:08,799
and this is extremely useful

00:19:05,120 --> 00:19:10,799
uh in case of a um late driving messages

00:19:08,799 --> 00:19:12,240
and so some of these typical examples

00:19:10,799 --> 00:19:12,880
right pretty much any relational

00:19:12,240 --> 00:19:15,760
database

00:19:12,880 --> 00:19:18,160
can support that uh some nosql databases

00:19:15,760 --> 00:19:20,320
like hbase or cassandra

00:19:18,160 --> 00:19:22,880
with all up engines you have pinot that

00:19:20,320 --> 00:19:25,120
support supports upserts

00:19:22,880 --> 00:19:27,039
when you have compacted kafka topics you

00:19:25,120 --> 00:19:29,760
can also support that update

00:19:27,039 --> 00:19:30,960
per key essentially right and there's

00:19:29,760 --> 00:19:32,640
also this notion of

00:19:30,960 --> 00:19:35,440
lake house object stores which i'm going

00:19:32,640 --> 00:19:38,000
to cover in a second

00:19:35,440 --> 00:19:39,919
and it can be problematic for you know

00:19:38,000 --> 00:19:42,240
some all-up engines like druid or click

00:19:39,919 --> 00:19:43,200
house where updates maybe not first part

00:19:42,240 --> 00:19:46,400
citizen

00:19:43,200 --> 00:19:47,200
uh not there by design uh non-conducted

00:19:46,400 --> 00:19:50,320
topics

00:19:47,200 --> 00:19:52,240
is also tricky and as i mentioned

00:19:50,320 --> 00:19:53,760
at the beginning you know if you have a

00:19:52,240 --> 00:19:57,120
classic object store

00:19:53,760 --> 00:19:58,799
um something like s3 or gcs or hdfs

00:19:57,120 --> 00:20:00,240
with a lot of parque files which are

00:19:58,799 --> 00:20:01,919
immutable can be

00:20:00,240 --> 00:20:03,360
can be very hard to go back and update

00:20:01,919 --> 00:20:06,720
that historical data

00:20:03,360 --> 00:20:08,400
right but uh just to

00:20:06,720 --> 00:20:10,080
you know cover that use case because

00:20:08,400 --> 00:20:11,760
it's extremely common you know this is

00:20:10,080 --> 00:20:15,360
how a bunch of data lakes

00:20:11,760 --> 00:20:16,240
are designed um i really like the idea

00:20:15,360 --> 00:20:18,320
of using

00:20:16,240 --> 00:20:19,280
this new uh you know white house

00:20:18,320 --> 00:20:21,760
technologies

00:20:19,280 --> 00:20:22,799
maybe heard about apache iceberg delta

00:20:21,760 --> 00:20:25,919
or hoodie

00:20:22,799 --> 00:20:28,240
you know those engines they allow you to

00:20:25,919 --> 00:20:29,760
provide a transactional journal on top

00:20:28,240 --> 00:20:31,120
of the object store right so you no

00:20:29,760 --> 00:20:33,760
longer just

00:20:31,120 --> 00:20:34,880
uh write a bunch of files to s3 to a

00:20:33,760 --> 00:20:36,799
certain location

00:20:34,880 --> 00:20:38,640
but all those modifications they go

00:20:36,799 --> 00:20:41,760
through that transactional journal

00:20:38,640 --> 00:20:45,200
right and this uh allows updates

00:20:41,760 --> 00:20:47,440
this allows small file uh compaction

00:20:45,200 --> 00:20:48,559
even time traveling if you want to right

00:20:47,440 --> 00:20:52,159
and

00:20:48,559 --> 00:20:53,200
flink has uh iceberg has a flank uh

00:20:52,159 --> 00:20:56,000
integration

00:20:53,200 --> 00:20:56,799
uh where it supports uh streaming uh

00:20:56,000 --> 00:20:58,640
updates

00:20:56,799 --> 00:21:00,159
uh and yeah this this is something you

00:20:58,640 --> 00:21:00,960
can just say not all of those

00:21:00,159 --> 00:21:03,600
technologies

00:21:00,960 --> 00:21:04,080
support link out of the box but i think

00:21:03,600 --> 00:21:07,120
this is

00:21:04,080 --> 00:21:09,760
where where it's going um

00:21:07,120 --> 00:21:10,720
so as a summary i want to say that you

00:21:09,760 --> 00:21:13,280
know we

00:21:10,720 --> 00:21:14,880
try to address all those concerns for

00:21:13,280 --> 00:21:17,360
data availability and redemption

00:21:14,880 --> 00:21:19,360
we can use things like data integration

00:21:17,360 --> 00:21:22,000
and compacted topics and tiered storage

00:21:19,360 --> 00:21:22,720
to have all the data in kafka and have

00:21:22,000 --> 00:21:25,440
it there

00:21:22,720 --> 00:21:26,960
forever or for very long time we can

00:21:25,440 --> 00:21:29,679
address data consistency

00:21:26,960 --> 00:21:31,280
uh guarant data consistency concern with

00:21:29,679 --> 00:21:32,720
exactly once end-to-end deliver

00:21:31,280 --> 00:21:34,960
semantics

00:21:32,720 --> 00:21:36,000
we can simplify late driving data

00:21:34,960 --> 00:21:37,360
handling

00:21:36,000 --> 00:21:39,440
by using proper state management

00:21:37,360 --> 00:21:40,960
primitives and proper data syncs that

00:21:39,440 --> 00:21:42,960
support updates

00:21:40,960 --> 00:21:45,039
and for data you're processing and

00:21:42,960 --> 00:21:45,840
backfill you know we can either use

00:21:45,039 --> 00:21:49,360
something like

00:21:45,840 --> 00:21:51,600
dynamic kafka clusters for elasticity

00:21:49,360 --> 00:21:53,840
or we can use save points and state

00:21:51,600 --> 00:21:57,200
processor api

00:21:53,840 --> 00:22:00,240
for flink to update that data kind of

00:21:57,200 --> 00:22:01,919
in place and just to finish

00:22:00,240 --> 00:22:03,280
i also wanted to show like a couple of

00:22:01,919 --> 00:22:05,760
use cases and

00:22:03,280 --> 00:22:07,840
how you can use those building blocks so

00:22:05,760 --> 00:22:10,400
this is a very common architecture

00:22:07,840 --> 00:22:12,640
you know you have let's say a stateless

00:22:10,400 --> 00:22:12,960
set of transformations uh that you want

00:22:12,640 --> 00:22:15,520
to

00:22:12,960 --> 00:22:16,320
apply to some data that's coming from

00:22:15,520 --> 00:22:19,280
like little

00:22:16,320 --> 00:22:21,039
ingestion api everything is in kafka and

00:22:19,280 --> 00:22:22,559
in the end you want to route this to

00:22:21,039 --> 00:22:24,640
multiple data sources

00:22:22,559 --> 00:22:27,200
like maybe there's a data lake some kind

00:22:24,640 --> 00:22:28,400
of uh databases search engine like this

00:22:27,200 --> 00:22:30,640
this is very common

00:22:28,400 --> 00:22:32,320
right so if you look at the the building

00:22:30,640 --> 00:22:34,320
blocks we cover

00:22:32,320 --> 00:22:35,679
you can use tiered storage for keeping

00:22:34,320 --> 00:22:38,080
the data in kafka

00:22:35,679 --> 00:22:40,000
you know for very long time you can

00:22:38,080 --> 00:22:42,480
embrace exactly once delivery

00:22:40,000 --> 00:22:44,240
uh you can use kafka connect to route

00:22:42,480 --> 00:22:46,960
and integrate data with different

00:22:44,240 --> 00:22:47,360
destinations you can use iceberg on top

00:22:46,960 --> 00:22:50,080
of that

00:22:47,360 --> 00:22:51,200
s3 data lake for uh transactional

00:22:50,080 --> 00:22:54,240
guarantees

00:22:51,200 --> 00:22:57,520
and you can use absurds in other

00:22:54,240 --> 00:22:58,320
data syncs and finally whenever you need

00:22:57,520 --> 00:23:00,640
to reprocess

00:22:58,320 --> 00:23:01,679
maybe you decide to introduce that

00:23:00,640 --> 00:23:03,760
elasticity

00:23:01,679 --> 00:23:07,039
at your kafka layer and just quickly

00:23:03,760 --> 00:23:09,679
bring those large clusters

00:23:07,039 --> 00:23:10,400
uh and the second use case let's say we

00:23:09,679 --> 00:23:12,080
need to apply

00:23:10,400 --> 00:23:14,720
a set of stateful transformations like

00:23:12,080 --> 00:23:17,760
joints window in aggregations

00:23:14,720 --> 00:23:20,400
um on top of the same data stream

00:23:17,760 --> 00:23:22,559
but also another one where we consume

00:23:20,400 --> 00:23:24,960
data from a relational database maybe

00:23:22,559 --> 00:23:26,640
something that we have in-house

00:23:24,960 --> 00:23:28,320
and we perform some kind of analytics

00:23:26,640 --> 00:23:28,640
and you write it in all of the engine

00:23:28,320 --> 00:23:32,080
like

00:23:28,640 --> 00:23:32,960
uh in this case again we use steer

00:23:32,080 --> 00:23:35,280
storage

00:23:32,960 --> 00:23:36,400
but for the relational stream we use

00:23:35,280 --> 00:23:37,919
compacted topics

00:23:36,400 --> 00:23:40,159
just because it's a very natural fit

00:23:37,919 --> 00:23:40,799
when you have a relational database as a

00:23:40,159 --> 00:23:43,039
source

00:23:40,799 --> 00:23:44,240
your primary keys can serve as message

00:23:43,039 --> 00:23:46,559
keys

00:23:44,240 --> 00:23:47,919
and perhaps use something like division

00:23:46,559 --> 00:23:50,159
for change data capture

00:23:47,919 --> 00:23:51,360
right so you capture all of that as a

00:23:50,159 --> 00:23:53,919
data stream

00:23:51,360 --> 00:23:56,000
you use uh flink for transformation to

00:23:53,919 --> 00:23:58,000
use exactly once delivery

00:23:56,000 --> 00:23:59,039
at the flank layer as well as pino you

00:23:58,000 --> 00:24:01,440
know it needs to

00:23:59,039 --> 00:24:03,760
configure something for that uh you use

00:24:01,440 --> 00:24:04,799
save points and state processor api in

00:24:03,760 --> 00:24:07,120
order to

00:24:04,799 --> 00:24:08,880
you know backfill and reprocess uh some

00:24:07,120 --> 00:24:11,279
historical data in place

00:24:08,880 --> 00:24:12,880
if you if you want to and finally pino

00:24:11,279 --> 00:24:15,279
supports uh absurds

00:24:12,880 --> 00:24:15,919
so you can do that for data correction

00:24:15,279 --> 00:24:19,039
and

00:24:15,919 --> 00:24:21,360
late traffic data

00:24:19,039 --> 00:24:24,159
and that's pretty much it i'm happy to

00:24:21,360 --> 00:24:24,159
answer your questions

00:24:26,880 --> 00:24:30,400
yeah thanks for this awesome talk i

00:24:29,039 --> 00:24:31,520
think you've covered a lot of good

00:24:30,400 --> 00:24:34,960
building blocks uh

00:24:31,520 --> 00:24:38,240
in in in this in in the last

00:24:34,960 --> 00:24:40,480
25 minutes uh really really impressed by

00:24:38,240 --> 00:24:41,840
by the depth and uh like how you put the

00:24:40,480 --> 00:24:44,080
pieces together

00:24:41,840 --> 00:24:47,840
um yeah let's see if there's uh if there

00:24:44,080 --> 00:24:47,840
are questions from the audience

00:24:49,039 --> 00:24:56,640
not yet um yeah i have a question

00:24:53,120 --> 00:24:59,520
so um like given all these uh this this

00:24:56,640 --> 00:25:00,720
nice building blocks to like compose

00:24:59,520 --> 00:25:03,279
your architecture

00:25:00,720 --> 00:25:04,240
uh where do you see uh is the biggest

00:25:03,279 --> 00:25:07,440
gap still

00:25:04,240 --> 00:25:09,440
or what where do you where where do you

00:25:07,440 --> 00:25:11,919
think or is still like significant

00:25:09,440 --> 00:25:14,000
improvement possible

00:25:11,919 --> 00:25:14,960
good question yeah um so i i think

00:25:14,000 --> 00:25:17,840
couple areas

00:25:14,960 --> 00:25:18,960
right so the first one is um you know

00:25:17,840 --> 00:25:21,679
streaming engines

00:25:18,960 --> 00:25:22,240
and support for a large state or complex

00:25:21,679 --> 00:25:24,000
state

00:25:22,240 --> 00:25:26,480
like i think flink is pretty you know

00:25:24,000 --> 00:25:28,320
probably the most advanced out there

00:25:26,480 --> 00:25:30,159
but you definitely have some challenges

00:25:28,320 --> 00:25:32,720
if you use some some other

00:25:30,159 --> 00:25:33,600
engines and even with flank you know it

00:25:32,720 --> 00:25:36,240
doesn't have

00:25:33,600 --> 00:25:38,000
a lot of support for like let's say you

00:25:36,240 --> 00:25:40,480
have a windowing

00:25:38,000 --> 00:25:42,480
uh transformation and the window is

00:25:40,480 --> 00:25:44,400
closed then we have weight driving data

00:25:42,480 --> 00:25:45,760
like kind of the state of the art you

00:25:44,400 --> 00:25:47,440
just just drop that

00:25:45,760 --> 00:25:49,440
right and i think some of the data

00:25:47,440 --> 00:25:51,840
pipelines would require you to

00:25:49,440 --> 00:25:53,200
somehow reprocess that right so maybe

00:25:51,840 --> 00:25:55,360
like reopen the window

00:25:53,200 --> 00:25:57,039
i know it sounds tricky but i think that

00:25:55,360 --> 00:25:57,840
that feature would be really cool to

00:25:57,039 --> 00:26:00,400
essentially

00:25:57,840 --> 00:26:02,640
just handle that late driving uh data uh

00:26:00,400 --> 00:26:05,919
problem you know once once and for all

00:26:02,640 --> 00:26:06,320
um and the second area of improvement is

00:26:05,919 --> 00:26:09,360
that

00:26:06,320 --> 00:26:10,159
uh data lake uh integration right like

00:26:09,360 --> 00:26:12,080
what iceberg

00:26:10,159 --> 00:26:13,440
is doing with flink integration i think

00:26:12,080 --> 00:26:15,760
is very impressive

00:26:13,440 --> 00:26:16,559
and i think we need to see more of that

00:26:15,760 --> 00:26:18,559
and uh

00:26:16,559 --> 00:26:19,600
hopefully more you know engines decide

00:26:18,559 --> 00:26:21,760
to support that

00:26:19,600 --> 00:26:23,760
so just like making sure the data can

00:26:21,760 --> 00:26:26,000
flow to all those

00:26:23,760 --> 00:26:27,120
data lakes and object stores in a

00:26:26,000 --> 00:26:29,279
transactional fashion

00:26:27,120 --> 00:26:31,919
and updates and compaction and time

00:26:29,279 --> 00:26:35,039
travel is supported

00:26:31,919 --> 00:26:35,360
yeah thank you um in the meantime there

00:26:35,039 --> 00:26:38,640
was

00:26:35,360 --> 00:26:39,039
uh another question came in and that's

00:26:38,640 --> 00:26:40,880
um

00:26:39,039 --> 00:26:43,679
are you using the absolute feature of

00:26:40,880 --> 00:26:47,279
pinot in production

00:26:43,679 --> 00:26:47,840
so we actually uh decided to use apache

00:26:47,279 --> 00:26:50,799
druid

00:26:47,840 --> 00:26:51,440
quite a bit ago and this is kind of how

00:26:50,799 --> 00:26:54,000
i learned

00:26:51,440 --> 00:26:54,720
that you know if you don't have absurds

00:26:54,000 --> 00:26:57,520
it can be

00:26:54,720 --> 00:26:58,960
uh can be a big problem um but i know

00:26:57,520 --> 00:27:00,400
about some other companies that use

00:26:58,960 --> 00:27:02,640
pinot and absurds

00:27:00,400 --> 00:27:04,159
um like i think stripe has presented

00:27:02,640 --> 00:27:06,159
something like that recently so you can

00:27:04,159 --> 00:27:08,000
try to find it online um

00:27:06,159 --> 00:27:09,520
yeah like with droid unfortunately you

00:27:08,000 --> 00:27:11,039
don't have absurd's

00:27:09,520 --> 00:27:13,200
functionality so you need to come up

00:27:11,039 --> 00:27:14,400
with the system of you know reductions

00:27:13,200 --> 00:27:17,520
and corrections

00:27:14,400 --> 00:27:21,840
which is quite tricky um yeah so no not

00:27:17,520 --> 00:27:21,840
a lot of experience with pinot myself

00:27:38,399 --> 00:27:40,480

YouTube URL: https://www.youtube.com/watch?v=sG8QD8AeWH4


