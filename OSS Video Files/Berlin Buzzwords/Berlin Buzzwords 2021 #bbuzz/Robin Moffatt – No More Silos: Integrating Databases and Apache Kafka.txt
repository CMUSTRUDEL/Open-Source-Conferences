Title: Robin Moffatt – No More Silos: Integrating Databases and Apache Kafka
Publication date: 2021-07-01
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Companies new and old are all recognising the importance of a low-latency, scalable, fault-tolerant data backbone, in the form of the Apache Kafka streaming platform. With Kafka, developers can integrate multiple sources and systems, which enables low latency analytics, event-driven architectures and the population of multiple downstream systems.

In this talk, we’ll look at one of the most common integration requirements - connecting databases to Kafka. We’ll consider the concept that all data is a stream of events, including that residing within a database. We’ll look at why we’d want to stream data from a database, including driving applications in Kafka from events upstream. We’ll discuss the different methods for connecting databases to Kafka, and the pros and cons of each. Techniques including Change-Data-Capture (CDC) and Kafka Connect will be covered, as well as an exploration of the power of ksqlDB for performing transformations such as joins on the inbound data.

Attendees of this talk will learn:
- why events, not just state, matter
- the difference between log-based CDC and query-based CDC
- how to chose which CDC approach to use

Speaker:
Robin Moffatt – https://2021.berlinbuzzwords.de/member/robin-moffatt

More: https://2021.berlinbuzzwords.de/session/no-more-silos-integrating-databases-and-apache-kafka-1
Captions: 
	00:00:07,120 --> 00:00:10,400
my name is robert muffett i work at

00:00:08,559 --> 00:00:10,880
confluence one of the companies behind

00:00:10,400 --> 00:00:13,599
the

00:00:10,880 --> 00:00:15,280
open source apache kafka project and i

00:00:13,599 --> 00:00:16,880
wish i was in berlin today it's a

00:00:15,280 --> 00:00:17,600
glorious sunshine in yorkshire but i'm

00:00:16,880 --> 00:00:19,840
sure it's the

00:00:17,600 --> 00:00:21,520
glorious skies like this in berlin too

00:00:19,840 --> 00:00:22,320
and but we're stuck at home at the

00:00:21,520 --> 00:00:24,800
moment

00:00:22,320 --> 00:00:25,599
but today i'd like to talk about getting

00:00:24,800 --> 00:00:28,560
data

00:00:25,599 --> 00:00:29,359
from a database into kafka and i'm going

00:00:28,560 --> 00:00:31,119
to talk about

00:00:29,359 --> 00:00:33,120
why we want to do that i'm going to talk

00:00:31,119 --> 00:00:34,399
about how we go about doing that because

00:00:33,120 --> 00:00:38,160
like it's important to understand

00:00:34,399 --> 00:00:39,520
both sides of this so let's start off by

00:00:38,160 --> 00:00:40,640
thinking about like why would you even

00:00:39,520 --> 00:00:42,719
want to get data

00:00:40,640 --> 00:00:44,559
from your database into kafka because

00:00:42,719 --> 00:00:45,680
like perhaps that data is quite happy in

00:00:44,559 --> 00:00:46,160
the database perhaps it doesn't want to

00:00:45,680 --> 00:00:48,320
be

00:00:46,160 --> 00:00:50,160
in kafka but it turns out that getting

00:00:48,320 --> 00:00:52,239
data from your databases

00:00:50,160 --> 00:00:54,559
into kafka powers a bunch of really

00:00:52,239 --> 00:00:56,399
useful things that we can do with it

00:00:54,559 --> 00:00:58,559
so sometimes people want to say well

00:00:56,399 --> 00:00:59,680
i've got data set in my database and i

00:00:58,559 --> 00:01:02,239
want to build

00:00:59,680 --> 00:01:04,239
analytics against it i'm querying that

00:01:02,239 --> 00:01:04,640
database directly perhaps it's kind of

00:01:04,239 --> 00:01:06,640
like

00:01:04,640 --> 00:01:08,640
overhead and performance all those kind

00:01:06,640 --> 00:01:09,439
of things why we don't want to do it on

00:01:08,640 --> 00:01:11,200
the database

00:01:09,439 --> 00:01:13,040
so you say well let's offload it from

00:01:11,200 --> 00:01:13,520
that database and we'll put it somewhere

00:01:13,040 --> 00:01:15,280
else

00:01:13,520 --> 00:01:17,280
and nowadays that's probably going to be

00:01:15,280 --> 00:01:18,799
a cloud data warehouse or a cloud object

00:01:17,280 --> 00:01:21,119
store or somewhere like that

00:01:18,799 --> 00:01:23,040
but we can use kafka as the piece in the

00:01:21,119 --> 00:01:24,159
middle to kind of get that data and

00:01:23,040 --> 00:01:26,240
buffer it and

00:01:24,159 --> 00:01:27,520
store it and push it out to the places

00:01:26,240 --> 00:01:30,320
where we want to use it

00:01:27,520 --> 00:01:31,520
so that's one very common use case data

00:01:30,320 --> 00:01:34,079
sat in our database

00:01:31,520 --> 00:01:36,560
we want it somewhere else kafka acts as

00:01:34,079 --> 00:01:39,040
the perfect broker a medium platform

00:01:36,560 --> 00:01:40,000
for doing that data transfer like

00:01:39,040 --> 00:01:41,680
getting the data out

00:01:40,000 --> 00:01:44,799
and pushing the data somewhere else in a

00:01:41,680 --> 00:01:45,920
reliable and scalable way

00:01:44,799 --> 00:01:47,520
another thing we want to do with the

00:01:45,920 --> 00:01:48,399
data is not just go and shove it

00:01:47,520 --> 00:01:51,360
somewhere else

00:01:48,399 --> 00:01:52,320
but actually use that data in our stream

00:01:51,360 --> 00:01:54,399
processing

00:01:52,320 --> 00:01:56,479
to enrich other data that we've got

00:01:54,399 --> 00:01:58,880
flowing through our streams

00:01:56,479 --> 00:02:00,719
so perhaps we've got a microservice

00:01:58,880 --> 00:02:02,799
somewhere there's writing information or

00:02:00,719 --> 00:02:04,880
orders that have been taken in a service

00:02:02,799 --> 00:02:07,840
or whatever and that's writing onto

00:02:04,880 --> 00:02:08,879
a kafka topic and we want to enrich that

00:02:07,840 --> 00:02:10,879
information that we've got

00:02:08,879 --> 00:02:13,040
about the orders being placed with

00:02:10,879 --> 00:02:14,640
things like well who is the customer

00:02:13,040 --> 00:02:16,400
who placed that order we probably know

00:02:14,640 --> 00:02:17,840
it's customer id 42

00:02:16,400 --> 00:02:19,840
but what's their email address or their

00:02:17,840 --> 00:02:20,720
shipping address or their loyalty club

00:02:19,840 --> 00:02:22,239
status or

00:02:20,720 --> 00:02:24,319
all that kind of stuff that we tend to

00:02:22,239 --> 00:02:26,800
keep on databases

00:02:24,319 --> 00:02:27,599
so by pulling in data from a database we

00:02:26,800 --> 00:02:30,160
can stream it

00:02:27,599 --> 00:02:31,440
into kafka and use stream processing to

00:02:30,160 --> 00:02:33,920
join it to events

00:02:31,440 --> 00:02:35,040
as they arrive from other places and we

00:02:33,920 --> 00:02:37,440
can use that to drive

00:02:35,040 --> 00:02:39,120
applications we can use this drive like

00:02:37,440 --> 00:02:39,599
real-time dashboards and things like

00:02:39,120 --> 00:02:42,560
that

00:02:39,599 --> 00:02:43,840
about the orders that are being placed

00:02:42,560 --> 00:02:46,160
the other thing we can do

00:02:43,840 --> 00:02:46,959
with getting data from a database into

00:02:46,160 --> 00:02:49,280
kafka

00:02:46,959 --> 00:02:50,720
is to actually start to evolve our

00:02:49,280 --> 00:02:53,680
existing systems

00:02:50,720 --> 00:02:54,319
towards a new way of building them so

00:02:53,680 --> 00:02:57,040
many

00:02:54,319 --> 00:02:58,000
many um systems nowadays are built as

00:02:57,040 --> 00:02:59,040
monoliths

00:02:58,000 --> 00:03:00,400
nothing wrong with monoliths but

00:02:59,040 --> 00:03:01,040
sometimes people decide that's not

00:03:00,400 --> 00:03:02,879
actually

00:03:01,040 --> 00:03:04,319
the way they want to continue with it

00:03:02,879 --> 00:03:07,120
but instead of going to just like

00:03:04,319 --> 00:03:08,480
ditch all of that and start afresh we

00:03:07,120 --> 00:03:10,080
can say well we've got our existing

00:03:08,480 --> 00:03:11,360
monolith our existing application up

00:03:10,080 --> 00:03:13,519
here in the top left

00:03:11,360 --> 00:03:15,440
and we say well almost certainly it's

00:03:13,519 --> 00:03:17,120
gonna have a database underpinning it

00:03:15,440 --> 00:03:18,720
something happens that monolith it gets

00:03:17,120 --> 00:03:21,040
written to the database

00:03:18,720 --> 00:03:22,159
well we can capture the events out of

00:03:21,040 --> 00:03:24,480
that database

00:03:22,159 --> 00:03:25,440
into kafka and so as stuff happens in

00:03:24,480 --> 00:03:28,319
the database like

00:03:25,440 --> 00:03:29,840
record at a time those are events which

00:03:28,319 --> 00:03:32,080
we can capture into kafka

00:03:29,840 --> 00:03:33,920
we can use that to drive new services

00:03:32,080 --> 00:03:35,599
and applications that we're building

00:03:33,920 --> 00:03:36,959
so that we can start to chip away at

00:03:35,599 --> 00:03:38,879
bits of functionality

00:03:36,959 --> 00:03:40,319
that have got in that existing system

00:03:38,879 --> 00:03:41,200
and replace it with new bits of

00:03:40,319 --> 00:03:43,840
functionality

00:03:41,200 --> 00:03:44,840
but driven by the same events from the

00:03:43,840 --> 00:03:47,760
same

00:03:44,840 --> 00:03:48,480
database now getting the idea of getting

00:03:47,760 --> 00:03:50,640
data

00:03:48,480 --> 00:03:52,560
out of a database into an event

00:03:50,640 --> 00:03:54,560
streaming platform like kafka

00:03:52,560 --> 00:03:56,560
may sound a little bit kind of like is

00:03:54,560 --> 00:03:58,319
this like squares and circles like do

00:03:56,560 --> 00:04:00,640
these things actually match up

00:03:58,319 --> 00:04:02,159
because kafka is about data and motion

00:04:00,640 --> 00:04:04,480
and streams and events

00:04:02,159 --> 00:04:06,879
and databases are about data at rest

00:04:04,480 --> 00:04:08,799
like static lumps of data

00:04:06,879 --> 00:04:10,319
but there's actually a very very tight

00:04:08,799 --> 00:04:12,959
relationship between the two

00:04:10,319 --> 00:04:13,519
let's have a little look at that if you

00:04:12,959 --> 00:04:15,760
think about

00:04:13,519 --> 00:04:16,639
a database table that holds account

00:04:15,760 --> 00:04:18,799
balances

00:04:16,639 --> 00:04:20,639
we say for account id what is the

00:04:18,799 --> 00:04:21,359
current balance account id one two three

00:04:20,639 --> 00:04:24,320
four five

00:04:21,359 --> 00:04:25,520
the current balance is 50 euros how did

00:04:24,320 --> 00:04:27,600
that balance get there well

00:04:25,520 --> 00:04:29,199
we deposited some money into that bank

00:04:27,600 --> 00:04:31,440
account so we deposited

00:04:29,199 --> 00:04:32,320
50 euros so we credited it to there and

00:04:31,440 --> 00:04:35,040
now the balance

00:04:32,320 --> 00:04:36,479
is 50 euros and then we deposited some

00:04:35,040 --> 00:04:38,880
more money so we deposited

00:04:36,479 --> 00:04:39,520
another 25 euros now the balance has

00:04:38,880 --> 00:04:41,199
changed

00:04:39,520 --> 00:04:42,560
if i query the database table and say

00:04:41,199 --> 00:04:45,280
what's the current balance it says

00:04:42,560 --> 00:04:46,479
the current balance is 75 euros and then

00:04:45,280 --> 00:04:49,680
i spend some money

00:04:46,479 --> 00:04:51,600
and the balance changes again so what we

00:04:49,680 --> 00:04:54,400
have here is the idea of a stream

00:04:51,600 --> 00:04:55,120
of events and you can replay that stream

00:04:54,400 --> 00:04:57,360
to build

00:04:55,120 --> 00:04:58,720
the table at any point in that stream

00:04:57,360 --> 00:05:01,440
you have the actual state

00:04:58,720 --> 00:05:02,000
that those events roll up to and the

00:05:01,440 --> 00:05:04,479
table

00:05:02,000 --> 00:05:05,520
is our state so you can go from a stream

00:05:04,479 --> 00:05:07,520
to a table

00:05:05,520 --> 00:05:10,080
but you can also go from a table to a

00:05:07,520 --> 00:05:10,560
stream every change to that table is an

00:05:10,080 --> 00:05:12,080
event

00:05:10,560 --> 00:05:14,000
and if you capture all of those events

00:05:12,080 --> 00:05:15,199
you could replay them to build the table

00:05:14,000 --> 00:05:16,720
state

00:05:15,199 --> 00:05:19,199
so this is called the stream table

00:05:16,720 --> 00:05:21,759
duality the duality because it goes both

00:05:19,199 --> 00:05:22,960
ways so we can take tables and existing

00:05:21,759 --> 00:05:24,880
databases

00:05:22,960 --> 00:05:26,479
capture the changes from those to give

00:05:24,880 --> 00:05:28,639
us a stream of events

00:05:26,479 --> 00:05:29,600
from a stream of events we can then roll

00:05:28,639 --> 00:05:32,160
through to a state

00:05:29,600 --> 00:05:33,600
to a table at any time we want to so

00:05:32,160 --> 00:05:34,639
that gives rise to this great quotation

00:05:33,600 --> 00:05:37,360
from pat helens

00:05:34,639 --> 00:05:38,880
the truth is the log the database is a

00:05:37,360 --> 00:05:41,360
cache of a subset

00:05:38,880 --> 00:05:43,120
of the log all you actually need like

00:05:41,360 --> 00:05:45,280
the fundamental pieces that you need

00:05:43,120 --> 00:05:46,160
are the events with the events you can

00:05:45,280 --> 00:05:47,840
then build

00:05:46,160 --> 00:05:49,440
a database table if you want to but you

00:05:47,840 --> 00:05:52,000
don't have to you can use those events

00:05:49,440 --> 00:05:53,919
on their own also so these are some of

00:05:52,000 --> 00:05:56,240
the reasons why we want to get data

00:05:53,919 --> 00:05:58,160
from a database into kafka and

00:05:56,240 --> 00:05:59,440
conceptually how we can even think about

00:05:58,160 --> 00:06:00,800
translating a table

00:05:59,440 --> 00:06:02,960
into a stream because they're actually

00:06:00,800 --> 00:06:04,720
intrinsically linked

00:06:02,960 --> 00:06:06,240
but now let's think about how we're

00:06:04,720 --> 00:06:07,759
going to go and do that what are the

00:06:06,240 --> 00:06:09,360
different pieces that we've got

00:06:07,759 --> 00:06:11,759
at our disposal to actually go and do

00:06:09,360 --> 00:06:12,319
this well the simple answer is kafka

00:06:11,759 --> 00:06:14,560
connect

00:06:12,319 --> 00:06:16,560
and kafka connect is part of apache

00:06:14,560 --> 00:06:17,680
kafka so if you're using apache kafka

00:06:16,560 --> 00:06:20,639
you already have

00:06:17,680 --> 00:06:22,000
kafka connect and it comes with source

00:06:20,639 --> 00:06:22,479
connectors that you can use to getting

00:06:22,000 --> 00:06:24,240
data

00:06:22,479 --> 00:06:25,680
from systems into kafka which is what

00:06:24,240 --> 00:06:27,039
we're going to talk about here there's

00:06:25,680 --> 00:06:29,680
also sync connectors

00:06:27,039 --> 00:06:30,639
so pushing data from kafka down to other

00:06:29,680 --> 00:06:32,960
places

00:06:30,639 --> 00:06:34,960
so it gives rise to this great ecosystem

00:06:32,960 --> 00:06:37,280
of connectors built around

00:06:34,960 --> 00:06:38,960
kafka connect and the really nice thing

00:06:37,280 --> 00:06:39,280
about kafka connectors as well as kind

00:06:38,960 --> 00:06:40,960
of like

00:06:39,280 --> 00:06:42,639
someone else having invented that wheel

00:06:40,960 --> 00:06:43,759
and perfected that wheel of doing

00:06:42,639 --> 00:06:46,080
scalable

00:06:43,759 --> 00:06:48,400
resilient stream stream integration

00:06:46,080 --> 00:06:50,639
between kafka and other systems

00:06:48,400 --> 00:06:51,840
is that you don't have to write any code

00:06:50,639 --> 00:06:52,880
the people who wrote the connectors

00:06:51,840 --> 00:06:55,759
wrote the code

00:06:52,880 --> 00:06:56,400
as users we just write configuration we

00:06:55,759 --> 00:06:58,319
declare

00:06:56,400 --> 00:07:00,000
we have data in this place and we would

00:06:58,319 --> 00:07:02,080
like to put it into that place

00:07:00,000 --> 00:07:03,840
and get data from this table on this

00:07:02,080 --> 00:07:04,319
database and put it into this kafka

00:07:03,840 --> 00:07:05,919
topic

00:07:04,319 --> 00:07:08,319
or if you're doing a sync from this

00:07:05,919 --> 00:07:09,680
kafka topic down to this other system

00:07:08,319 --> 00:07:12,160
but we're all about the source

00:07:09,680 --> 00:07:15,840
connectors here get data from a database

00:07:12,160 --> 00:07:17,360
into kafka now many people familiar with

00:07:15,840 --> 00:07:18,240
databases will have heard of the term

00:07:17,360 --> 00:07:21,120
cdc

00:07:18,240 --> 00:07:21,680
change data capture and a lot of people

00:07:21,120 --> 00:07:24,160
assume

00:07:21,680 --> 00:07:26,080
there is just one way of doing cdc but

00:07:24,160 --> 00:07:27,599
it turns out there's two

00:07:26,080 --> 00:07:29,520
and the purpose of this talk is to

00:07:27,599 --> 00:07:30,080
really dig into the difference between

00:07:29,520 --> 00:07:32,560
these two

00:07:30,080 --> 00:07:33,520
options and help you understand how you

00:07:32,560 --> 00:07:35,680
can choose

00:07:33,520 --> 00:07:36,639
which is the best one to use for your

00:07:35,680 --> 00:07:39,680
requirements

00:07:36,639 --> 00:07:41,440
so as with most things in it it depends

00:07:39,680 --> 00:07:43,440
there are two different ways of doing it

00:07:41,440 --> 00:07:44,000
you'll probably find most people use one

00:07:43,440 --> 00:07:45,759
of them

00:07:44,000 --> 00:07:47,680
but that's not to say the other one

00:07:45,759 --> 00:07:50,319
isn't useful in many different cases

00:07:47,680 --> 00:07:50,960
also so we have query based change dates

00:07:50,319 --> 00:07:53,039
capture

00:07:50,960 --> 00:07:54,879
and we have log based change data

00:07:53,039 --> 00:07:56,639
capture if you're like familiar with

00:07:54,879 --> 00:07:58,160
databases and cdc

00:07:56,639 --> 00:07:59,759
you're probably thinking of log-based

00:07:58,160 --> 00:08:00,800
change data capture but i'm going to

00:07:59,759 --> 00:08:03,039
walk through now

00:08:00,800 --> 00:08:05,520
what the two different types are and how

00:08:03,039 --> 00:08:08,000
you can choose which one to use

00:08:05,520 --> 00:08:09,039
so query based change data capture it

00:08:08,000 --> 00:08:11,199
runs a query

00:08:09,039 --> 00:08:12,319
against the database so it queries it

00:08:11,199 --> 00:08:14,240
say what's changed

00:08:12,319 --> 00:08:16,240
since i last queried so it pulls the

00:08:14,240 --> 00:08:17,840
database so it says select everything

00:08:16,240 --> 00:08:19,840
from this particular table

00:08:17,840 --> 00:08:21,599
where there's a particular column which

00:08:19,840 --> 00:08:23,599
is going to indicate what's changed

00:08:21,599 --> 00:08:25,440
so it could be a timestamp it could be

00:08:23,599 --> 00:08:27,520
an id but something which is going to go

00:08:25,440 --> 00:08:28,800
up that we can compare against like

00:08:27,520 --> 00:08:31,440
based on last time

00:08:28,800 --> 00:08:32,159
has something changed so if we have a

00:08:31,440 --> 00:08:33,519
database

00:08:32,159 --> 00:08:35,360
and it looks like this on the left hand

00:08:33,519 --> 00:08:36,000
side we've got two rows of data denoted

00:08:35,360 --> 00:08:38,000
by those red

00:08:36,000 --> 00:08:39,120
rectangles there we can pull the

00:08:38,000 --> 00:08:42,320
database and say

00:08:39,120 --> 00:08:43,919
select everything that's changed since

00:08:42,320 --> 00:08:45,760
our little watermark there our little

00:08:43,919 --> 00:08:46,880
last previous timestamp that we pulled

00:08:45,760 --> 00:08:48,640
the database they said well

00:08:46,880 --> 00:08:50,320
there are two rows they've been created

00:08:48,640 --> 00:08:53,040
so we get two events

00:08:50,320 --> 00:08:54,240
in our topic two messages in our plasma

00:08:53,040 --> 00:08:55,600
topic

00:08:54,240 --> 00:08:58,240
something changes in the database we

00:08:55,600 --> 00:09:00,320
insert a new row we poll the database

00:08:58,240 --> 00:09:01,760
a moment later it says well everything

00:09:00,320 --> 00:09:02,560
that's changed since we last checked

00:09:01,760 --> 00:09:04,320
which is now

00:09:02,560 --> 00:09:06,080
this watermark here this offset here

00:09:04,320 --> 00:09:08,480
says well what's changed there

00:09:06,080 --> 00:09:10,160
is this particular row and we get that

00:09:08,480 --> 00:09:12,240
onto our kafka topic

00:09:10,160 --> 00:09:13,600
that's conceptually what query based

00:09:12,240 --> 00:09:15,680
change data capture is doing

00:09:13,600 --> 00:09:16,959
it's querying the database well it uses

00:09:15,680 --> 00:09:19,440
jdbc it's literally

00:09:16,959 --> 00:09:20,640
running a jdbc query against the query

00:09:19,440 --> 00:09:22,320
using jdbc

00:09:20,640 --> 00:09:23,920
against the database to work out what's

00:09:22,320 --> 00:09:25,920
changed

00:09:23,920 --> 00:09:27,600
log-based change data capture on the

00:09:25,920 --> 00:09:28,560
other hand looks a lot more scary it's

00:09:27,600 --> 00:09:29,920
like one of those scenes for like a

00:09:28,560 --> 00:09:31,519
hacker films i should have done it with

00:09:29,920 --> 00:09:33,040
green text on the black background i

00:09:31,519 --> 00:09:34,399
made it look proper like that

00:09:33,040 --> 00:09:36,399
but what this is doing is using the

00:09:34,399 --> 00:09:38,480
databases transaction log

00:09:36,399 --> 00:09:40,399
so the way that relational databases

00:09:38,480 --> 00:09:41,440
work and implementations differ but

00:09:40,399 --> 00:09:42,880
broadly speaking

00:09:41,440 --> 00:09:44,880
they have what's called a transaction

00:09:42,880 --> 00:09:47,040
log which is what the database

00:09:44,880 --> 00:09:48,640
writes information to when you make a

00:09:47,040 --> 00:09:50,640
change and that gives you

00:09:48,640 --> 00:09:52,640
your ability to recover things and roll

00:09:50,640 --> 00:09:54,240
back changes and replay changes

00:09:52,640 --> 00:09:55,760
and like recover from failures and all

00:09:54,240 --> 00:09:58,320
sorts of stuff like that but the

00:09:55,760 --> 00:10:00,000
transaction log holds that information

00:09:58,320 --> 00:10:01,839
so some things in the database we've got

00:10:00,000 --> 00:10:04,880
two rows of data as before

00:10:01,839 --> 00:10:07,120
using log-based change data capture

00:10:04,880 --> 00:10:08,160
we can capture those changes into our

00:10:07,120 --> 00:10:10,000
kafka topic

00:10:08,160 --> 00:10:11,760
and something changes in the database

00:10:10,000 --> 00:10:13,040
and that goes onto the transaction log

00:10:11,760 --> 00:10:16,480
and we can capture that

00:10:13,040 --> 00:10:19,760
into our kafka topic so we've got

00:10:16,480 --> 00:10:21,839
two viable ways of doing the same thing

00:10:19,760 --> 00:10:23,040
we want to capture changes to the data

00:10:21,839 --> 00:10:24,800
so most of these ones you can end up

00:10:23,040 --> 00:10:26,399
with a snapshot first like here's

00:10:24,800 --> 00:10:27,920
everything in the table and then we're

00:10:26,399 --> 00:10:29,920
going to capture the changes

00:10:27,920 --> 00:10:31,200
of everything that happens after that

00:10:29,920 --> 00:10:33,519
snapshot

00:10:31,200 --> 00:10:34,399
so now we need to understand how to

00:10:33,519 --> 00:10:36,399
actually choose

00:10:34,399 --> 00:10:37,440
which one to use which one's going to be

00:10:36,399 --> 00:10:40,320
most appropriate

00:10:37,440 --> 00:10:40,320
for what we're trying to do

00:10:42,640 --> 00:10:46,079
so query based change data capture we

00:10:44,800 --> 00:10:46,720
need to understand a little bit more

00:10:46,079 --> 00:10:48,480
about

00:10:46,720 --> 00:10:50,399
how does it actually work and what are

00:10:48,480 --> 00:10:52,399
some of the limitations around it

00:10:50,399 --> 00:10:53,519
so query based change data capture

00:10:52,399 --> 00:10:56,320
you've got to have

00:10:53,519 --> 00:10:56,959
a field in your schema which is going to

00:10:56,320 --> 00:10:59,120
change

00:10:56,959 --> 00:11:00,079
when your record does so this could be

00:10:59,120 --> 00:11:02,240
an id field

00:11:00,079 --> 00:11:03,120
which goes up like an incrementing id

00:11:02,240 --> 00:11:04,959
field

00:11:03,120 --> 00:11:06,560
but usually you'll have a timestamp

00:11:04,959 --> 00:11:08,959
field which is going to get

00:11:06,560 --> 00:11:10,320
set when you insert the row and change

00:11:08,959 --> 00:11:12,160
when you update it

00:11:10,320 --> 00:11:13,519
so got like a create table statement

00:11:12,160 --> 00:11:14,640
here some ddl

00:11:13,519 --> 00:11:16,320
different languages or different

00:11:14,640 --> 00:11:16,560
databases we'll use different flavors of

00:11:16,320 --> 00:11:17,760
it

00:11:16,560 --> 00:11:20,160
but here we're saying the timestamp

00:11:17,760 --> 00:11:21,920
column is a timestamp surprise surprise

00:11:20,160 --> 00:11:23,839
i'm saying by default it's got a current

00:11:21,920 --> 00:11:25,040
timestamp so we insert a row and have

00:11:23,839 --> 00:11:27,200
the current timestamp

00:11:25,040 --> 00:11:29,040
on update when it gets updated we're

00:11:27,200 --> 00:11:31,360
going to also use the current timestamp

00:11:29,040 --> 00:11:32,720
so when we update that row the timestamp

00:11:31,360 --> 00:11:34,160
on it will be set so i think this is the

00:11:32,720 --> 00:11:35,600
mysql implementation

00:11:34,160 --> 00:11:37,440
you can use triggers or whatever to

00:11:35,600 --> 00:11:38,959
achieve similar things on other

00:11:37,440 --> 00:11:41,040
relational databases

00:11:38,959 --> 00:11:42,560
but you have to have this field in your

00:11:41,040 --> 00:11:44,399
schema and

00:11:42,560 --> 00:11:45,760
it'd be fairly uncommon not to have

00:11:44,399 --> 00:11:47,200
something like that anyway just because

00:11:45,760 --> 00:11:48,079
it makes life easy when we're building

00:11:47,200 --> 00:11:49,760
applications

00:11:48,079 --> 00:11:51,279
but if you don't you can't use query

00:11:49,760 --> 00:11:53,040
based changes data capture

00:11:51,279 --> 00:11:54,720
all you can do is like capture the whole

00:11:53,040 --> 00:11:56,240
table each time

00:11:54,720 --> 00:11:58,160
which might be useful once but kind of

00:11:56,240 --> 00:12:00,079
like snapchatting the entire table

00:11:58,160 --> 00:12:01,760
every however many seconds doesn't

00:12:00,079 --> 00:12:03,920
always make a great deal of sense

00:12:01,760 --> 00:12:05,200
so we have to have the ability to have

00:12:03,920 --> 00:12:06,800
that in the schema already

00:12:05,200 --> 00:12:08,399
or make those modifications to the

00:12:06,800 --> 00:12:10,320
schema which if it's your own

00:12:08,399 --> 00:12:12,000
application maybe fair enough if it's a

00:12:10,320 --> 00:12:13,200
third party one or a different team in

00:12:12,000 --> 00:12:15,920
the organization

00:12:13,200 --> 00:12:17,920
sometimes that can be a sticking point

00:12:15,920 --> 00:12:18,800
so we have an insert that gets made into

00:12:17,920 --> 00:12:20,399
the database

00:12:18,800 --> 00:12:22,000
we can query the database say what's

00:12:20,399 --> 00:12:22,639
changed since we last checked and we

00:12:22,000 --> 00:12:24,959
capture

00:12:22,639 --> 00:12:26,720
that insert we've got other dml

00:12:24,959 --> 00:12:28,800
operations we make an update

00:12:26,720 --> 00:12:30,240
so that existing road that we captured

00:12:28,800 --> 00:12:32,959
into our kafka topic

00:12:30,240 --> 00:12:34,800
it gets updated and our timestamp column

00:12:32,959 --> 00:12:35,760
in the database whether a trigger or our

00:12:34,800 --> 00:12:38,079
application

00:12:35,760 --> 00:12:39,760
it updates that timestamp column so when

00:12:38,079 --> 00:12:40,880
we poll the database again and say where

00:12:39,760 --> 00:12:42,959
the timestamp column

00:12:40,880 --> 00:12:44,480
value is greater than when we last

00:12:42,959 --> 00:12:45,600
checked and the polling interval you can

00:12:44,480 --> 00:12:47,600
customize but

00:12:45,600 --> 00:12:49,600
whatever the interval set to has the

00:12:47,600 --> 00:12:51,279
timestamp column got a greater value

00:12:49,600 --> 00:12:53,440
than when we last checked or if we're

00:12:51,279 --> 00:12:55,200
using id columns as the id column value

00:12:53,440 --> 00:12:57,440
gone up since the one that we checked

00:12:55,200 --> 00:13:00,079
before and we can capture the updates

00:12:57,440 --> 00:13:01,200
into our kafka topic like that if we

00:13:00,079 --> 00:13:02,959
delete a message

00:13:01,200 --> 00:13:04,800
we see okay we do a delete from the

00:13:02,959 --> 00:13:06,320
table and then we query the database and

00:13:04,800 --> 00:13:07,200
say tell me about the rows in the table

00:13:06,320 --> 00:13:09,040
have changed

00:13:07,200 --> 00:13:10,240
since we last pulled the table that says

00:13:09,040 --> 00:13:11,680
well okay well

00:13:10,240 --> 00:13:12,720
that row's deleted obviously we don't

00:13:11,680 --> 00:13:14,000
know about it because it's not in the

00:13:12,720 --> 00:13:16,240
table anymore

00:13:14,000 --> 00:13:17,839
you can't query a table for data that

00:13:16,240 --> 00:13:19,839
doesn't exist

00:13:17,839 --> 00:13:21,279
now they kind of like smart alexa works

00:13:19,839 --> 00:13:22,800
you're probably saying ah well in this

00:13:21,279 --> 00:13:24,000
particular relational database you can

00:13:22,800 --> 00:13:25,760
use triggers or

00:13:24,000 --> 00:13:27,519
uh flashback or all these sorts of

00:13:25,760 --> 00:13:28,160
different things and conceptually you

00:13:27,519 --> 00:13:29,839
could

00:13:28,160 --> 00:13:31,519
but fundamentally query based change

00:13:29,839 --> 00:13:32,959
data capture cannots

00:13:31,519 --> 00:13:34,639
unless you go and customize it to

00:13:32,959 --> 00:13:35,920
whatever capture deletes

00:13:34,639 --> 00:13:38,639
you can go and fork it and write your

00:13:35,920 --> 00:13:40,880
own but out of the box query based

00:13:38,639 --> 00:13:42,560
capture cannot capture deletes because

00:13:40,880 --> 00:13:45,279
you cannot query a database

00:13:42,560 --> 00:13:46,720
for data which doesn't exist so that's

00:13:45,279 --> 00:13:48,079
one of the wrinkles with creator-based

00:13:46,720 --> 00:13:49,279
jgj's capture

00:13:48,079 --> 00:13:51,040
the other one is a little bit more

00:13:49,279 --> 00:13:52,959
subtle but it's potentially the most

00:13:51,040 --> 00:13:56,240
crucial point in deciding

00:13:52,959 --> 00:13:57,279
which method are we going to use so

00:13:56,240 --> 00:13:59,440
let's imagine we're capturing

00:13:57,279 --> 00:14:01,199
information our orders so we've got an

00:13:59,440 --> 00:14:02,880
order table and our application rights

00:14:01,199 --> 00:14:04,720
orders and we're going to replicate that

00:14:02,880 --> 00:14:07,920
table into a kafka topic so we could

00:14:04,720 --> 00:14:08,800
feed an analytics system so here we call

00:14:07,920 --> 00:14:10,639
the database

00:14:08,800 --> 00:14:12,639
and we say well the previous timestamp

00:14:10,639 --> 00:14:13,440
was at 53 minutes past the hour and 30

00:14:12,639 --> 00:14:16,399
seconds

00:14:13,440 --> 00:14:17,040
it's a 30 second poll so 54 minutes past

00:14:16,399 --> 00:14:18,959
the hour

00:14:17,040 --> 00:14:20,800
that's 54 minutes past the hour zero

00:14:18,959 --> 00:14:22,560
seconds we run the query

00:14:20,800 --> 00:14:24,160
we don't get anything which is fair

00:14:22,560 --> 00:14:26,880
enough no orders have been placed

00:14:24,160 --> 00:14:27,360
in that 30 second interval 30 seconds

00:14:26,880 --> 00:14:30,639
later

00:14:27,360 --> 00:14:31,360
we pilot again so this is 54 seconds 54

00:14:30,639 --> 00:14:33,199
minutes

00:14:31,360 --> 00:14:34,399
and 30 seconds past the hour and we

00:14:33,199 --> 00:14:36,560
capture a new row

00:14:34,399 --> 00:14:37,760
so it says okay all drive 42 is being

00:14:36,560 --> 00:14:39,600
shipped to this address

00:14:37,760 --> 00:14:41,040
this is the time stamp at which it was

00:14:39,600 --> 00:14:44,160
updated so

00:14:41,040 --> 00:14:45,199
10 54 and 29 seconds so one second

00:14:44,160 --> 00:14:48,079
before we pulled

00:14:45,199 --> 00:14:50,000
this record was there that's fine we've

00:14:48,079 --> 00:14:52,480
captured the current state of the table

00:14:50,000 --> 00:14:53,680
and we're doing so every 30 seconds and

00:14:52,480 --> 00:14:56,639
if we're capturing that

00:14:53,680 --> 00:14:57,760
into an analytic system like i say that

00:14:56,639 --> 00:14:59,199
may be sufficient

00:14:57,760 --> 00:15:01,199
if all that analytic system wants to

00:14:59,199 --> 00:15:02,399
know about the state of orders right

00:15:01,199 --> 00:15:03,279
tell me about all of the orders that

00:15:02,399 --> 00:15:06,079
we've shipped and

00:15:03,279 --> 00:15:07,920
whatever that's that's totally fine but

00:15:06,079 --> 00:15:09,199
if we're starting to build applications

00:15:07,920 --> 00:15:12,000
around the concept of

00:15:09,199 --> 00:15:13,440
events or if we're doing analytics based

00:15:12,000 --> 00:15:14,480
on the progress of an order through the

00:15:13,440 --> 00:15:16,320
system

00:15:14,480 --> 00:15:17,920
we want to know about everything that

00:15:16,320 --> 00:15:20,000
happened to that order not just its

00:15:17,920 --> 00:15:21,440
current state when we checked the table

00:15:20,000 --> 00:15:23,120
so if you think about it the order

00:15:21,440 --> 00:15:25,839
probably got created so maybe

00:15:23,120 --> 00:15:26,240
it was one second after we last polled

00:15:25,839 --> 00:15:28,480
so

00:15:26,240 --> 00:15:30,320
54 minutes past the hour and one second

00:15:28,480 --> 00:15:32,000
so just after we polled

00:15:30,320 --> 00:15:33,440
someone placed the order or like they

00:15:32,000 --> 00:15:35,600
clicked on a button which sent it into a

00:15:33,440 --> 00:15:38,160
pending status so the order gets created

00:15:35,600 --> 00:15:39,600
it's an insert into the table there's no

00:15:38,160 --> 00:15:41,839
address at this point

00:15:39,600 --> 00:15:43,440
so then a few seconds later the users

00:15:41,839 --> 00:15:44,959
obviously created their customer profile

00:15:43,440 --> 00:15:47,600
and they put an address into it

00:15:44,959 --> 00:15:48,959
so the record gets updated and then a

00:15:47,600 --> 00:15:50,560
couple of seconds after that user

00:15:48,959 --> 00:15:52,399
realizes oh that's the delivery address

00:15:50,560 --> 00:15:54,320
i wanted like my home address

00:15:52,399 --> 00:15:56,079
so they update the address on the order

00:15:54,320 --> 00:15:57,920
again and then

00:15:56,079 --> 00:15:59,360
it gets shipped okay so it's like super

00:15:57,920 --> 00:16:00,959
quick processing it's been

00:15:59,360 --> 00:16:02,720
created we've set the address a couple

00:16:00,959 --> 00:16:05,839
of times and then we've shipped it

00:16:02,720 --> 00:16:07,600
all within the space of 28 seconds

00:16:05,839 --> 00:16:09,199
so when we first checked the table there

00:16:07,600 --> 00:16:11,040
was no data

00:16:09,199 --> 00:16:12,560
30 seconds later we checked the table we

00:16:11,040 --> 00:16:14,959
captured the state

00:16:12,560 --> 00:16:15,839
but in between it turns out a whole

00:16:14,959 --> 00:16:18,480
bunch of stuff

00:16:15,839 --> 00:16:19,120
has happened so query based change data

00:16:18,480 --> 00:16:21,120
capture

00:16:19,120 --> 00:16:22,399
is easier to run because we're just

00:16:21,120 --> 00:16:23,920
clearing the database

00:16:22,399 --> 00:16:25,600
all you need are the credentials for the

00:16:23,920 --> 00:16:26,240
table of select credentials against that

00:16:25,600 --> 00:16:28,000
table

00:16:26,240 --> 00:16:29,920
connection details for the database and

00:16:28,000 --> 00:16:31,920
off we go we can capture the state

00:16:29,920 --> 00:16:34,320
of the table at the point at which we

00:16:31,920 --> 00:16:37,040
call it but we cannot capture

00:16:34,320 --> 00:16:38,079
or guarantee to capture is every single

00:16:37,040 --> 00:16:39,839
event

00:16:38,079 --> 00:16:41,680
so what's actually happened during that

00:16:39,839 --> 00:16:42,399
polling interval is four different

00:16:41,680 --> 00:16:44,079
events

00:16:42,399 --> 00:16:45,680
we created an order we changed the

00:16:44,079 --> 00:16:47,279
address we changed the address again

00:16:45,680 --> 00:16:49,199
and then we shipped the order those four

00:16:47,279 --> 00:16:50,320
different events which depending on the

00:16:49,199 --> 00:16:51,839
system we're building

00:16:50,320 --> 00:16:53,519
like four completely different things

00:16:51,839 --> 00:16:54,800
maybe four different microservices want

00:16:53,519 --> 00:16:56,240
to know about that there's the

00:16:54,800 --> 00:16:58,000
fraud checking one which needs to know

00:16:56,240 --> 00:16:59,440
when the address gets changed and the

00:16:58,000 --> 00:17:01,839
fulfillment one needs to know when it's

00:16:59,440 --> 00:17:03,759
been shipped these are events and events

00:17:01,839 --> 00:17:06,559
matter because events model the world

00:17:03,759 --> 00:17:07,600
around us so capturing the individual

00:17:06,559 --> 00:17:10,559
events

00:17:07,600 --> 00:17:12,240
oftentimes is super important so this is

00:17:10,559 --> 00:17:14,480
where it comes down to like oh we can

00:17:12,240 --> 00:17:15,280
use query based or log-based change data

00:17:14,480 --> 00:17:17,199
capture

00:17:15,280 --> 00:17:19,360
if we're quite happy simply taking a

00:17:17,199 --> 00:17:19,919
snapshot of the table the state of the

00:17:19,360 --> 00:17:21,439
table

00:17:19,919 --> 00:17:23,120
at that point in time that we've pulled

00:17:21,439 --> 00:17:25,199
it that's fine

00:17:23,120 --> 00:17:26,720
but if we need the events then we're

00:17:25,199 --> 00:17:29,120
gonna have to use log based

00:17:26,720 --> 00:17:30,240
change data capture so query based

00:17:29,120 --> 00:17:32,240
change data capture

00:17:30,240 --> 00:17:33,679
it's much easier to set up it needs

00:17:32,240 --> 00:17:35,039
fewer permissions because we're simply

00:17:33,679 --> 00:17:37,120
querying the database it's

00:17:35,039 --> 00:17:38,320
not much different from logging into sql

00:17:37,120 --> 00:17:39,679
plus and running a query against the

00:17:38,320 --> 00:17:41,679
database but we're just doing it

00:17:39,679 --> 00:17:43,600
repeatedly and we are doing it

00:17:41,679 --> 00:17:44,240
repeatedly so we're actually putting a

00:17:43,600 --> 00:17:45,760
load

00:17:44,240 --> 00:17:48,080
on the database because we're going to

00:17:45,760 --> 00:17:50,240
be calling the database frequently

00:17:48,080 --> 00:17:51,760
or we call it less frequently because we

00:17:50,240 --> 00:17:52,480
get a phone call from our friendly dba

00:17:51,760 --> 00:17:53,840
who says like

00:17:52,480 --> 00:17:55,280
what's this query that's running every

00:17:53,840 --> 00:17:56,559
second against the database on a column

00:17:55,280 --> 00:17:58,080
which i forgot to index

00:17:56,559 --> 00:18:00,160
it's like oh yeah we just wanted to

00:17:58,080 --> 00:18:01,360
change data capture against it but well

00:18:00,160 --> 00:18:02,320
no you're not going to do that against

00:18:01,360 --> 00:18:04,480
my database

00:18:02,320 --> 00:18:06,320
says okay we'll dial it down or just

00:18:04,480 --> 00:18:09,039
like pull the database every minute

00:18:06,320 --> 00:18:10,240
or every 10 minutes which again comes

00:18:09,039 --> 00:18:11,840
down to requirements

00:18:10,240 --> 00:18:13,280
if you don't need the data other than

00:18:11,840 --> 00:18:15,520
every 10 minute snapshot

00:18:13,280 --> 00:18:17,840
that's probably fine if you want to do

00:18:15,520 --> 00:18:20,160
actual events driven processing

00:18:17,840 --> 00:18:21,200
when an event happens respond to it 10

00:18:20,160 --> 00:18:21,919
minutes isn't going to really cut the

00:18:21,200 --> 00:18:23,280
mustard

00:18:21,919 --> 00:18:25,039
you need something which is much more

00:18:23,280 --> 00:18:26,720
instantaneous

00:18:25,039 --> 00:18:28,240
we need to have access to the schema to

00:18:26,720 --> 00:18:30,320
change the schema or

00:18:28,240 --> 00:18:31,919
certain demands on the schema in terms

00:18:30,320 --> 00:18:33,280
of having a field there like an

00:18:31,919 --> 00:18:35,600
incrementing id column

00:18:33,280 --> 00:18:37,520
and or a timestamp column which we can

00:18:35,600 --> 00:18:38,400
use to check against for has the row of

00:18:37,520 --> 00:18:41,120
change

00:18:38,400 --> 00:18:42,480
and we can't track deletes so tracking

00:18:41,120 --> 00:18:43,120
deletes all right something being

00:18:42,480 --> 00:18:45,360
deleted

00:18:43,120 --> 00:18:46,720
is also an event it's like the absence

00:18:45,360 --> 00:18:49,280
of data is also something

00:18:46,720 --> 00:18:50,799
we want to know about so query based

00:18:49,280 --> 00:18:52,320
change data capture is fine

00:18:50,799 --> 00:18:54,400
log base change data capture is kinda

00:18:52,320 --> 00:18:57,360
like it's just like a more refined way

00:18:54,400 --> 00:18:58,559
of dealing with the data so log base

00:18:57,360 --> 00:19:00,320
change data capture

00:18:58,559 --> 00:19:02,559
we have the snapshot of what's in the

00:19:00,320 --> 00:19:03,679
table currently that mirrors over into

00:19:02,559 --> 00:19:06,400
our kafka topic

00:19:03,679 --> 00:19:07,520
we make an update an update goes into

00:19:06,400 --> 00:19:09,679
the transaction log

00:19:07,520 --> 00:19:11,520
so that transaction log has information

00:19:09,679 --> 00:19:13,120
about what row has been updated

00:19:11,520 --> 00:19:15,360
usually it captures what was the state

00:19:13,120 --> 00:19:17,679
of the row before the update

00:19:15,360 --> 00:19:18,559
and after so not only are we capturing

00:19:17,679 --> 00:19:20,480
that here's

00:19:18,559 --> 00:19:21,679
what the table currently looks like but

00:19:20,480 --> 00:19:23,600
we're capturing the fact that

00:19:21,679 --> 00:19:24,799
something got changed here's what got

00:19:23,600 --> 00:19:27,360
changed too

00:19:24,799 --> 00:19:28,799
here's what it was before so the

00:19:27,360 --> 00:19:29,840
customer changed their address and their

00:19:28,799 --> 00:19:32,799
address is now

00:19:29,840 --> 00:19:35,120
this and they've changed it from that so

00:19:32,799 --> 00:19:38,559
that you actually get really rich

00:19:35,120 --> 00:19:40,480
events through from the transaction log

00:19:38,559 --> 00:19:41,760
we capture those into our kafka topic

00:19:40,480 --> 00:19:44,000
and we can also

00:19:41,760 --> 00:19:46,160
capture deletes because the delete is an

00:19:44,000 --> 00:19:48,720
event in the database it's a

00:19:46,160 --> 00:19:49,760
dml uh statement it gets written to the

00:19:48,720 --> 00:19:51,120
transaction log

00:19:49,760 --> 00:19:52,960
so that database needs to capture that

00:19:51,120 --> 00:19:55,120
into its transaction log because the

00:19:52,960 --> 00:19:57,360
transaction log is what it replays

00:19:55,120 --> 00:19:59,840
against the kind of the backup snapshot

00:19:57,360 --> 00:20:02,320
files we need to rule forward the states

00:19:59,840 --> 00:20:04,240
of the database so we have the delete in

00:20:02,320 --> 00:20:07,280
the transaction log we can capture that

00:20:04,240 --> 00:20:09,039
into our kafka topic also

00:20:07,280 --> 00:20:10,240
if you take a look at that picture there

00:20:09,039 --> 00:20:11,760
on the bottom left we've got a

00:20:10,240 --> 00:20:13,600
transaction log with our kind of

00:20:11,760 --> 00:20:15,840
our inserts and our updates and our

00:20:13,600 --> 00:20:17,600
deletes on the right hand side

00:20:15,840 --> 00:20:19,200
we've got our inserts and our updates

00:20:17,600 --> 00:20:20,480
and our deletes so

00:20:19,200 --> 00:20:21,919
what's kind of interesting for me in

00:20:20,480 --> 00:20:22,480
this is that on the right hand side

00:20:21,919 --> 00:20:24,799
we've got

00:20:22,480 --> 00:20:25,600
apache kafka with this idea of like a

00:20:24,799 --> 00:20:28,240
immutable

00:20:25,600 --> 00:20:29,760
app end only log of events on the left

00:20:28,240 --> 00:20:31,360
hand side we've got the concept of a

00:20:29,760 --> 00:20:34,720
relational database for like

00:20:31,360 --> 00:20:36,320
decades ago of an immutable series of

00:20:34,720 --> 00:20:37,360
events you can't go into a relational

00:20:36,320 --> 00:20:39,039
database and kind of like

00:20:37,360 --> 00:20:41,520
hack around on the transaction log or if

00:20:39,039 --> 00:20:43,840
you can you're a braver person than i

00:20:41,520 --> 00:20:45,440
the databases transaction log is this

00:20:43,840 --> 00:20:47,840
immutable series of events

00:20:45,440 --> 00:20:49,200
apache kafka is an immutable series of

00:20:47,840 --> 00:20:51,679
events it's just that it's

00:20:49,200 --> 00:20:53,360
distributed and highly scalable and so

00:20:51,679 --> 00:20:54,159
this gives you this interesting idea

00:20:53,360 --> 00:20:56,720
that like

00:20:54,159 --> 00:20:57,840
could kafka be the fundamental basis for

00:20:56,720 --> 00:20:59,679
like the concept of

00:20:57,840 --> 00:21:01,440
a database and there's like i know it's

00:20:59,679 --> 00:21:02,880
kind of it's kind of a clickbaity idea

00:21:01,440 --> 00:21:04,880
like there's articles written around it

00:21:02,880 --> 00:21:07,200
is kafka database and so on

00:21:04,880 --> 00:21:08,880
but conceptually we're doing the same

00:21:07,200 --> 00:21:11,200
thing we're capturing events

00:21:08,880 --> 00:21:12,799
in our data and then building stuff on

00:21:11,200 --> 00:21:13,919
top of it it's just a relational

00:21:12,799 --> 00:21:15,520
database we kind of like

00:21:13,919 --> 00:21:17,919
ship the whole package and like with a

00:21:15,520 --> 00:21:19,039
nice sql layer on top is the api for

00:21:17,919 --> 00:21:20,400
people to interact with it

00:21:19,039 --> 00:21:22,320
whereas kafka kind of gives you the

00:21:20,400 --> 00:21:24,320
framework and the platform but things

00:21:22,320 --> 00:21:26,640
like key sql db are being built

00:21:24,320 --> 00:21:27,840
around that to give you a sql interface

00:21:26,640 --> 00:21:29,360
to those events

00:21:27,840 --> 00:21:31,200
anyway i kind of like digress i'm going

00:21:29,360 --> 00:21:33,520
off on a bit of a tangent

00:21:31,200 --> 00:21:36,240
so log base change here to capture it

00:21:33,520 --> 00:21:38,000
gives us this the fidelity of the data

00:21:36,240 --> 00:21:39,520
it captures every single event that

00:21:38,000 --> 00:21:40,960
happens in the database

00:21:39,520 --> 00:21:43,440
so we can actually guarantee we've got a

00:21:40,960 --> 00:21:45,039
snapshot of the table we've quested at a

00:21:43,440 --> 00:21:46,720
certain point of the snapshot

00:21:45,039 --> 00:21:48,480
and then we've captured every single

00:21:46,720 --> 00:21:50,320
change that's happened to that data

00:21:48,480 --> 00:21:52,480
since that snapshot so we've got a full

00:21:50,320 --> 00:21:53,760
replica of what's happening to our data

00:21:52,480 --> 00:21:56,559
in the source system

00:21:53,760 --> 00:21:58,480
in our kafka topic so we can use that to

00:21:56,559 --> 00:22:00,000
drive our event-driven applications

00:21:58,480 --> 00:22:02,159
so there's that great talk from gunner

00:22:00,000 --> 00:22:02,720
hans peter beforehand about patterns

00:22:02,159 --> 00:22:04,880
which

00:22:02,720 --> 00:22:06,640
change data capture and that's the idea

00:22:04,880 --> 00:22:07,360
of building applications of run on this

00:22:06,640 --> 00:22:09,679
data

00:22:07,360 --> 00:22:11,679
you can get out of a database it's much

00:22:09,679 --> 00:22:13,919
lower latency and lower impact on the

00:22:11,679 --> 00:22:17,200
source system because it's a lower level

00:22:13,919 --> 00:22:19,039
api but because it's a lower level api

00:22:17,200 --> 00:22:20,320
you need much greater access to the

00:22:19,039 --> 00:22:21,679
system you need to

00:22:20,320 --> 00:22:22,640
make friends with your dba who you

00:22:21,679 --> 00:22:23,760
should be friends with anyway because

00:22:22,640 --> 00:22:25,120
they're lovely people

00:22:23,760 --> 00:22:27,120
and you need to get the appropriate

00:22:25,120 --> 00:22:28,640
permissions to install it and

00:22:27,120 --> 00:22:31,520
access to the database because it is a

00:22:28,640 --> 00:22:34,559
low-level api that you're working with

00:22:31,520 --> 00:22:36,080
so that gives us an idea of what query

00:22:34,559 --> 00:22:36,880
based change data capture is what

00:22:36,080 --> 00:22:39,120
log-based

00:22:36,880 --> 00:22:41,039
change data capture is and how you

00:22:39,120 --> 00:22:42,400
decide which one to use

00:22:41,039 --> 00:22:44,080
if you're building event driven

00:22:42,400 --> 00:22:45,280
applications you're going to almost

00:22:44,080 --> 00:22:47,440
certainly want to be using

00:22:45,280 --> 00:22:49,280
log-based change data capture if you

00:22:47,440 --> 00:22:51,440
just want to capture the state

00:22:49,280 --> 00:22:53,200
of a table and you may be taking like

00:22:51,440 --> 00:22:54,559
first baby steps with kafka and

00:22:53,200 --> 00:22:56,400
databases and like

00:22:54,559 --> 00:22:57,679
what's the easiest like lowest friction

00:22:56,400 --> 00:23:00,080
way of setting this kind of stuff

00:22:57,679 --> 00:23:02,000
up then query based change aids capture

00:23:00,080 --> 00:23:03,600
gives you a nice easy route into it

00:23:02,000 --> 00:23:05,280
and it may well get you plenty of the

00:23:03,600 --> 00:23:07,039
way along with like if a proof of

00:23:05,280 --> 00:23:08,400
concept of like what we can get out what

00:23:07,039 --> 00:23:09,280
can we do with this data what can we

00:23:08,400 --> 00:23:12,080
build with it

00:23:09,280 --> 00:23:12,799
once it's in kafka so having made that

00:23:12,080 --> 00:23:14,720
decision

00:23:12,799 --> 00:23:15,840
which one you're going to use let's

00:23:14,720 --> 00:23:16,960
actually look at some of the tools that

00:23:15,840 --> 00:23:19,039
are available

00:23:16,960 --> 00:23:20,000
so query based change data capture is

00:23:19,039 --> 00:23:22,159
provided by

00:23:20,000 --> 00:23:23,440
the jdbc connector so you can go to

00:23:22,159 --> 00:23:24,799
confluence hub and you can see all of

00:23:23,440 --> 00:23:25,840
these different connectors for kafka

00:23:24,799 --> 00:23:28,159
connect

00:23:25,840 --> 00:23:29,840
so the gdbc connector gives you a source

00:23:28,159 --> 00:23:32,000
connector there's also a sync for

00:23:29,840 --> 00:23:33,840
pushing data from kafka into a database

00:23:32,000 --> 00:23:37,200
but that's an entirely different talk

00:23:33,840 --> 00:23:39,360
so jdbc connector lets you do uh

00:23:37,200 --> 00:23:40,960
query based change data capture against

00:23:39,360 --> 00:23:42,960
your source database

00:23:40,960 --> 00:23:44,720
for log based change data capture

00:23:42,960 --> 00:23:45,679
there's the oracle connector cdc

00:23:44,720 --> 00:23:47,600
connector

00:23:45,679 --> 00:23:48,960
from confluence and for all the other

00:23:47,600 --> 00:23:51,200
databases and oracle

00:23:48,960 --> 00:23:52,559
also there's the debesium project which

00:23:51,200 --> 00:23:53,360
provides some excellent excellent

00:23:52,559 --> 00:23:55,440
connectors

00:23:53,360 --> 00:23:56,400
gonna enhance pizza both work on the

00:23:55,440 --> 00:23:58,559
with division

00:23:56,400 --> 00:24:00,559
uh lots of guns their project lead on it

00:23:58,559 --> 00:24:02,080
and this gives you really really good

00:24:00,559 --> 00:24:03,520
connectors for getting data out of my

00:24:02,080 --> 00:24:05,520
sequel and postgres and

00:24:03,520 --> 00:24:07,200
a bunch of other ones as well so those

00:24:05,520 --> 00:24:09,679
are some really useful tools

00:24:07,200 --> 00:24:11,679
there are tons of other connectors and

00:24:09,679 --> 00:24:14,559
like third-party applications

00:24:11,679 --> 00:24:16,480
for doing getting data out of databases

00:24:14,559 --> 00:24:17,600
into kafka but those are like the three

00:24:16,480 --> 00:24:19,440
there which i kind of like particularly

00:24:17,600 --> 00:24:21,200
choose to highlight

00:24:19,440 --> 00:24:23,440
hopefully that's been useful i think

00:24:21,200 --> 00:24:25,360
i've got a few minutes left for q a so

00:24:23,440 --> 00:24:26,960
i'll turn to that in just a moment but a

00:24:25,360 --> 00:24:29,279
couple of resources for you

00:24:26,960 --> 00:24:30,640
um confluent developer is where you can

00:24:29,279 --> 00:24:33,120
go and learn all about

00:24:30,640 --> 00:24:35,120
apache kafka there's tutorials there's

00:24:33,120 --> 00:24:37,440
blogs there's podcasts there's videos

00:24:35,120 --> 00:24:38,720
there's all sorts of useful stuff there

00:24:37,440 --> 00:24:42,000
and then you can find

00:24:38,720 --> 00:24:43,279
me online i'm on twitter i'm at i'm off

00:24:42,000 --> 00:24:44,000
there's my handle there in the bottom

00:24:43,279 --> 00:24:45,440
left

00:24:44,000 --> 00:24:46,880
you can find the slides for this and a

00:24:45,440 --> 00:24:47,600
bunch of other talks and recordings that

00:24:46,880 --> 00:24:50,960
i do

00:24:47,600 --> 00:24:52,000
at uh armoff.dev talks and you can also

00:24:50,960 --> 00:24:53,679
find me on youtube

00:24:52,000 --> 00:24:55,600
so like lockdown has been with us for a

00:24:53,679 --> 00:24:57,039
while now but last year when i kind of

00:24:55,600 --> 00:24:58,559
got taken off the road and couldn't go

00:24:57,039 --> 00:25:00,480
to conferences in person

00:24:58,559 --> 00:25:01,679
i started doing a youtube channel so

00:25:00,480 --> 00:25:03,360
there's a bunch of talks on there

00:25:01,679 --> 00:25:03,919
there's a lot of stuff about kafka

00:25:03,360 --> 00:25:06,159
connect

00:25:03,919 --> 00:25:07,600
so if the idea of kafka connect is

00:25:06,159 --> 00:25:08,240
something that you like or you use this

00:25:07,600 --> 00:25:09,919
already

00:25:08,240 --> 00:25:11,600
go and check out that channel make sure

00:25:09,919 --> 00:25:13,440
you subscribe let's get the subscriber

00:25:11,600 --> 00:25:14,720
number going up and to the right

00:25:13,440 --> 00:25:17,120
but there's a ton of useful content

00:25:14,720 --> 00:25:18,799
there also so with that

00:25:17,120 --> 00:25:20,640
thank you very much for your time and

00:25:18,799 --> 00:25:23,120
i'll be delighted to take any questions

00:25:20,640 --> 00:25:23,120
that we have

00:25:25,120 --> 00:25:28,960
thanks robin that was a really really an

00:25:27,440 --> 00:25:31,520
awesome talk um

00:25:28,960 --> 00:25:32,000
um i i like the uh distinguishing

00:25:31,520 --> 00:25:35,520
between

00:25:32,000 --> 00:25:38,880
query based and log based cdc

00:25:35,520 --> 00:25:41,520
um never thought about it that way so

00:25:38,880 --> 00:25:42,240
we have um a question from the from the

00:25:41,520 --> 00:25:44,400
audience

00:25:42,240 --> 00:25:46,559
it's a bit longer so i'll just read it

00:25:44,400 --> 00:25:49,039
so in a microservice world

00:25:46,559 --> 00:25:49,600
local databases state is very much the

00:25:49,039 --> 00:25:52,159
internal

00:25:49,600 --> 00:25:53,360
representation for service versus the

00:25:52,159 --> 00:25:56,400
api that this

00:25:53,360 --> 00:25:58,080
microservice exposes for example there

00:25:56,400 --> 00:26:00,640
could be some data model changes in the

00:25:58,080 --> 00:26:02,240
database where the api doesn't change

00:26:00,640 --> 00:26:05,440
what would be a good pattern for

00:26:02,240 --> 00:26:07,039
exposing the same api data in kafka

00:26:05,440 --> 00:26:08,960
so business logic won't have to be

00:26:07,039 --> 00:26:12,240
replicated between services and

00:26:08,960 --> 00:26:14,240
event processing oh

00:26:12,240 --> 00:26:16,159
uh okay i've got the question up here so

00:26:14,240 --> 00:26:21,039
just give me a second so uh

00:26:16,159 --> 00:26:21,039
in the microsoft as well okay um

00:26:21,200 --> 00:26:24,960
a good pattern for exposing the same api

00:26:23,279 --> 00:26:28,799
data in kafka

00:26:24,960 --> 00:26:28,799
so business logic won't have replicated

00:26:29,919 --> 00:26:34,480
i don't actually know i'm afraid i do

00:26:32,880 --> 00:26:35,520
know who would know and that's gonna

00:26:34,480 --> 00:26:37,039
enhance better

00:26:35,520 --> 00:26:38,400
um so that'll be a great question to

00:26:37,039 --> 00:26:39,039
take to them i know they're on twitter

00:26:38,400 --> 00:26:41,760
also

00:26:39,039 --> 00:26:43,440
um so if you are on twitter then tag me

00:26:41,760 --> 00:26:46,240
on twitter and i'll pass it on to them

00:26:43,440 --> 00:26:47,840
um or just reach out to them directly um

00:26:46,240 --> 00:26:49,039
i would need to have a set and a scratch

00:26:47,840 --> 00:26:50,080
my head about that one which i'll i'll

00:26:49,039 --> 00:26:52,000
do after this but

00:26:50,080 --> 00:27:07,840
um i don't have an answer straight away

00:26:52,000 --> 00:27:07,840
for that i'm sorry

00:27:13,360 --> 00:27:15,440

YouTube URL: https://www.youtube.com/watch?v=4-MeJJt3B2Q


