Title: Prashanth Babu â€“ Simplifying upserts and deletes on Delta Lake tables
Publication date: 2021-06-25
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Data Engineers face many challenges with Data Lakes. GDPR requests, data quality issues, handling large metadata, merges and deletes are a few of the tough challenges usually every Data Engineer encounters with a Data Lake with formats like Parquet, ORC, Avro, etc. This session showcases how you can effortlessly apply updates, upserts and deletes on a Delta Lake table with a very few lines of code and use time travel to go back in time for reproducing experiments & reports very easily, how we can avoid challenges due to small files as well. Delta Lake was developed by Databricks and has been donated to Linux Foundation, the code for which could be found at http://delta.io. Delta Lake is being used by a huge number of companies across the world due to its advantages for Data Lakes. We will discuss, demo and showcase how Delta Lake can be helpful for your Data Lakes because of which many enterprises have Delta Lake as the default data format in their architecture. We will will use SQL or its equivalent Python or Scala API to perform showcase various Delta Lake features.

Speaker: https://2021.berlinbuzzwords.de/member/prashanth-babu

More: https://2021.berlinbuzzwords.de/session/simplifying-upserts-and-deletes-delta-lake-tables
Captions: 
	00:00:07,600 --> 00:00:11,360
thanks for having me here

00:00:08,960 --> 00:00:12,400
uh today we are going to discuss uh

00:00:11,360 --> 00:00:14,480
simplifying

00:00:12,400 --> 00:00:16,480
upsets and deletes on delta lake so

00:00:14,480 --> 00:00:20,080
that's our topic for today

00:00:16,480 --> 00:00:21,920
so a brief agenda is

00:00:20,080 --> 00:00:24,800
first initially we'll talk about the

00:00:21,920 --> 00:00:27,920
topic and then we'll discuss a bit about

00:00:24,800 --> 00:00:29,039
challenges with data links uh and also

00:00:27,920 --> 00:00:31,279
we'll discuss about

00:00:29,039 --> 00:00:32,719
next comes the features of delta lake

00:00:31,279 --> 00:00:35,840
and how it helps

00:00:32,719 --> 00:00:39,280
tackle solve the challenges with

00:00:35,840 --> 00:00:41,280
the data lakes usually and finally

00:00:39,280 --> 00:00:43,360
we get into the meat of the topic like

00:00:41,280 --> 00:00:44,399
update delete and upset on a data lake

00:00:43,360 --> 00:00:47,120
table and how

00:00:44,399 --> 00:00:48,160
how easy it is and how simplified it is

00:00:47,120 --> 00:00:50,160
to run

00:00:48,160 --> 00:00:52,480
any of these commands and ensure your

00:00:50,160 --> 00:00:53,600
data lake is fine and general data lake

00:00:52,480 --> 00:00:55,440
is pristine

00:00:53,600 --> 00:00:57,760
and could be used for downstream uh

00:00:55,440 --> 00:00:59,840
analytics and also machine learning

00:00:57,760 --> 00:01:00,960
i'm also touching a bit about optimize

00:00:59,840 --> 00:01:03,920
and vacuum

00:01:00,960 --> 00:01:05,760
these are essential for ensuring you can

00:01:03,920 --> 00:01:08,000
delete some data and also

00:01:05,760 --> 00:01:09,280
ensure you can pinpoint the files and so

00:01:08,000 --> 00:01:11,760
on and so forth and

00:01:09,280 --> 00:01:12,479
i round it up by couple of demos uh

00:01:11,760 --> 00:01:15,840
these are

00:01:12,479 --> 00:01:17,600
i'm showcasing uh update delete absurd

00:01:15,840 --> 00:01:19,759
uh in two different notebooks and

00:01:17,600 --> 00:01:20,560
finally uh optimize and vacuum also is

00:01:19,759 --> 00:01:23,840
also part of

00:01:20,560 --> 00:01:24,720
a notebook so that's about uh our agenda

00:01:23,840 --> 00:01:26,320
for today

00:01:24,720 --> 00:01:28,240
and finally i leave a number of

00:01:26,320 --> 00:01:30,079
references so that you can you can

00:01:28,240 --> 00:01:31,439
go through them in detail i mean this is

00:01:30,079 --> 00:01:33,680
kind of a

00:01:31,439 --> 00:01:34,880
lightning talk i would say a 30 minute

00:01:33,680 --> 00:01:37,360
session

00:01:34,880 --> 00:01:38,079
there are enough ample number of videos

00:01:37,360 --> 00:01:39,520
and

00:01:38,079 --> 00:01:41,439
blog posts which are rare in my

00:01:39,520 --> 00:01:42,159
references section which would help you

00:01:41,439 --> 00:01:44,720
to

00:01:42,159 --> 00:01:45,280
dig deep into any of the information

00:01:44,720 --> 00:01:48,000
which

00:01:45,280 --> 00:01:49,840
you might be interested in uh with that

00:01:48,000 --> 00:01:52,720
i'm getting into the topic

00:01:49,840 --> 00:01:54,240
so uh very brief about me i'm prashant

00:01:52,720 --> 00:01:55,680
i've been with databricks for almost

00:01:54,240 --> 00:01:58,479
like three years now

00:01:55,680 --> 00:02:00,159
uh i'm uh email practice lead for rsa

00:01:58,479 --> 00:02:01,119
rsa stands for recent solutions

00:02:00,159 --> 00:02:03,840
architect

00:02:01,119 --> 00:02:04,479
and my linkedin profile is i showcased

00:02:03,840 --> 00:02:07,040
on under

00:02:04,479 --> 00:02:08,000
on the slide here i would be i would

00:02:07,040 --> 00:02:10,560
love to interest

00:02:08,000 --> 00:02:12,560
i would love to connect with you with

00:02:10,560 --> 00:02:15,440
any of you and all of you

00:02:12,560 --> 00:02:16,640
i'm a very very very brief about

00:02:15,440 --> 00:02:18,080
databricks if you are not aware

00:02:16,640 --> 00:02:21,360
databricks is a

00:02:18,080 --> 00:02:23,599
platform to unify data

00:02:21,360 --> 00:02:24,720
machine learning and a workloads

00:02:23,599 --> 00:02:26,319
basically

00:02:24,720 --> 00:02:28,080
you can do everything in a single

00:02:26,319 --> 00:02:28,800
platform which is where i'm going to

00:02:28,080 --> 00:02:31,200
showcase

00:02:28,800 --> 00:02:32,560
my demos as well and databricks you

00:02:31,200 --> 00:02:34,160
might be aware already are the original

00:02:32,560 --> 00:02:37,200
creators of spark

00:02:34,160 --> 00:02:39,120
data lake ml flow and coils and we

00:02:37,200 --> 00:02:40,239
almost have 5000 plus customers across

00:02:39,120 --> 00:02:43,920
the globe using

00:02:40,239 --> 00:02:46,239
our databricks um again

00:02:43,920 --> 00:02:48,560
one simple slide to explain what data

00:02:46,239 --> 00:02:52,000
break series and what lakos platform is

00:02:48,560 --> 00:02:54,319
you might have data in aws also or gcp

00:02:52,000 --> 00:02:55,599
the three main cloud vendors in

00:02:54,319 --> 00:02:57,840
structured format

00:02:55,599 --> 00:02:59,519
unstructured format are semi-structured

00:02:57,840 --> 00:03:01,040
formatter streaming i mean these are the

00:02:59,519 --> 00:03:02,000
kind of workloads which could be

00:03:01,040 --> 00:03:03,440
processed

00:03:02,000 --> 00:03:05,680
with data breaks for data science and

00:03:03,440 --> 00:03:07,760
engineering or bi and sql analytics

00:03:05,680 --> 00:03:10,480
machine learning and finally real-time

00:03:07,760 --> 00:03:13,280
data applications also so this

00:03:10,480 --> 00:03:14,080
all this is underpinned mostly with

00:03:13,280 --> 00:03:15,519
delta lake

00:03:14,080 --> 00:03:17,680
uh which is what we are going to talk

00:03:15,519 --> 00:03:21,120
about a bit more bit in detail uh

00:03:17,680 --> 00:03:23,440
in today's session um these are the

00:03:21,120 --> 00:03:24,400
uh i mean this is a very very simple

00:03:23,440 --> 00:03:27,360
sample of

00:03:24,400 --> 00:03:28,400
uh our customers and you can see couple

00:03:27,360 --> 00:03:30,560
of uh

00:03:28,400 --> 00:03:31,599
german heavy customers also represent

00:03:30,560 --> 00:03:34,159
like uh

00:03:31,599 --> 00:03:35,440
daimler for example or zelando et cetera

00:03:34,159 --> 00:03:38,560
so on so forth so

00:03:35,440 --> 00:03:40,159
these are that is a very brief about uh

00:03:38,560 --> 00:03:42,239
data breaks and finally getting on to

00:03:40,159 --> 00:03:43,200
the topic uh what are the typical

00:03:42,239 --> 00:03:45,200
challenges with

00:03:43,200 --> 00:03:47,360
data lakes and how data breaks delta

00:03:45,200 --> 00:03:48,879
solves these challenges is what

00:03:47,360 --> 00:03:50,640
we are going to discuss in the next few

00:03:48,879 --> 00:03:53,840
slides so

00:03:50,640 --> 00:03:56,959
um as you can see most of the

00:03:53,840 --> 00:03:59,360
uh i mean as per many servers in fact

00:03:56,959 --> 00:04:00,560
mit's law on management review says 83

00:03:59,360 --> 00:04:03,360
percent of cos say

00:04:00,560 --> 00:04:04,480
ai is a strategic priority at the same

00:04:03,360 --> 00:04:07,120
time gartner says

00:04:04,480 --> 00:04:08,560
3.9 trillion dollars business value

00:04:07,120 --> 00:04:11,360
could be created by ai

00:04:08,560 --> 00:04:11,760
by end of next year 2022. the future is

00:04:11,360 --> 00:04:14,319
here

00:04:11,760 --> 00:04:15,280
but but there are there are a bit of

00:04:14,319 --> 00:04:17,680
problems here like

00:04:15,280 --> 00:04:19,600
it is very hard to get right and it is

00:04:17,680 --> 00:04:21,600
just not evenly distributed so

00:04:19,600 --> 00:04:24,000
the same gardener which predicts 3.9 may

00:04:21,600 --> 00:04:24,800
uh trillion dollars business value also

00:04:24,000 --> 00:04:28,240
says

00:04:24,800 --> 00:04:29,919
85 of the data projects fail and the

00:04:28,240 --> 00:04:32,160
venture build also says 87

00:04:29,919 --> 00:04:33,840
of data science projects never make it

00:04:32,160 --> 00:04:36,800
into production and

00:04:33,840 --> 00:04:37,440
and some companies like uber google

00:04:36,800 --> 00:04:39,840
amazon

00:04:37,440 --> 00:04:40,560
etc are making are having huge huge

00:04:39,840 --> 00:04:44,160
success

00:04:40,560 --> 00:04:45,759
but a lot of them struggle and most of

00:04:44,160 --> 00:04:48,960
the reasons would be around

00:04:45,759 --> 00:04:51,040
the data are the data which is

00:04:48,960 --> 00:04:52,639
hit sitting in the data lake and which

00:04:51,040 --> 00:04:54,080
is causing some challenges and we are

00:04:52,639 --> 00:04:57,280
going to drive down

00:04:54,080 --> 00:04:58,720
drill down and discuss in deep dive on a

00:04:57,280 --> 00:05:02,000
couple of challenges with

00:04:58,720 --> 00:05:04,639
data lakes the first one is something

00:05:02,000 --> 00:05:05,600
very very simple like uh appending new

00:05:04,639 --> 00:05:08,240
data using spark

00:05:05,600 --> 00:05:08,639
into a data lake but at the same time

00:05:08,240 --> 00:05:11,440
some

00:05:08,639 --> 00:05:12,560
other processor or some other pipeline

00:05:11,440 --> 00:05:14,479
is also trying to read

00:05:12,560 --> 00:05:16,800
the same data so that usually

00:05:14,479 --> 00:05:19,919
essentially causes a ton of issues

00:05:16,800 --> 00:05:22,639
and user users usually want all the data

00:05:19,919 --> 00:05:23,520
all their changes to appear all at once

00:05:22,639 --> 00:05:25,440
this is

00:05:23,520 --> 00:05:27,600
very very hard to achieve making

00:05:25,440 --> 00:05:29,600
multiple files all appear at once

00:05:27,600 --> 00:05:30,800
or even a single file to appear in full

00:05:29,600 --> 00:05:32,880
form and

00:05:30,800 --> 00:05:35,440
it's not supposed to be was not supposed

00:05:32,880 --> 00:05:38,320
to be work or supported out of the box

00:05:35,440 --> 00:05:40,080
with uh data lakes that is uh the first

00:05:38,320 --> 00:05:42,400
and foremost problem with uh

00:05:40,080 --> 00:05:44,960
data lakes the second problem is of

00:05:42,400 --> 00:05:47,199
about modifying existing data

00:05:44,960 --> 00:05:49,199
is very very difficult i mean take the

00:05:47,199 --> 00:05:52,080
classic case of gdpr where

00:05:49,199 --> 00:05:53,360
some someone sends a request for

00:05:52,080 --> 00:05:56,400
deleting their data

00:05:53,360 --> 00:05:58,000
i mean from any of the organizations and

00:05:56,400 --> 00:05:58,960
that implies you have to read all the

00:05:58,000 --> 00:06:01,360
data

00:05:58,960 --> 00:06:02,560
and then filter that particular row or

00:06:01,360 --> 00:06:04,319
those particular rows

00:06:02,560 --> 00:06:05,600
from the data and then rewrite the data

00:06:04,319 --> 00:06:08,160
into the data lake

00:06:05,600 --> 00:06:10,000
so that is again a huge a big big

00:06:08,160 --> 00:06:10,800
problem i mean gdpr run ccpa for that

00:06:10,000 --> 00:06:13,919
matter

00:06:10,800 --> 00:06:15,440
so there are many many manual techniques

00:06:13,919 --> 00:06:17,120
which are applied and which are very

00:06:15,440 --> 00:06:18,639
very unreliable

00:06:17,120 --> 00:06:20,800
one of which we are going to discuss

00:06:18,639 --> 00:06:23,520
here today uh in the demos

00:06:20,800 --> 00:06:24,720
and the third and and third option the

00:06:23,520 --> 00:06:27,280
third challenge with

00:06:24,720 --> 00:06:29,440
data lakes is jobs failing midway i mean

00:06:27,280 --> 00:06:30,880
most of the big data pipelines and spark

00:06:29,440 --> 00:06:33,360
pipelines you would easily

00:06:30,880 --> 00:06:34,000
understand half of the data appears in

00:06:33,360 --> 00:06:35,680
the data

00:06:34,000 --> 00:06:37,039
and rush might be missing so it is

00:06:35,680 --> 00:06:39,360
usually a problem with

00:06:37,039 --> 00:06:41,440
jobs failing midway which call which

00:06:39,360 --> 00:06:45,199
caused this particular challenge

00:06:41,440 --> 00:06:48,639
um another another problem is mixing of

00:06:45,199 --> 00:06:51,599
batch and real time that is usually

00:06:48,639 --> 00:06:53,039
turns out to actually heal usually and

00:06:51,599 --> 00:06:55,680
it is very very tough to

00:06:53,039 --> 00:06:57,039
uh mix them and it leads to a lot of

00:06:55,680 --> 00:06:59,520
inconsistencies and

00:06:57,039 --> 00:07:01,520
and one of the variations of the first

00:06:59,520 --> 00:07:02,000
problem is with appends but at the same

00:07:01,520 --> 00:07:03,759
time

00:07:02,000 --> 00:07:05,120
streaming also adds a bit more

00:07:03,759 --> 00:07:07,120
inconsistency and

00:07:05,120 --> 00:07:08,560
basically you're reading partial results

00:07:07,120 --> 00:07:12,240
if i can say so

00:07:08,560 --> 00:07:14,240
um fifth topic uh fifth challenge being

00:07:12,240 --> 00:07:16,240
it is very very costly to keep

00:07:14,240 --> 00:07:18,400
historical versions of the data i mean

00:07:16,240 --> 00:07:19,280
usually all the regulated organizations

00:07:18,400 --> 00:07:22,400
need

00:07:19,280 --> 00:07:23,840
some or many of the versions of the data

00:07:22,400 --> 00:07:27,759
to be available in the data lake that

00:07:23,840 --> 00:07:30,080
is bound to be costly as well as

00:07:27,759 --> 00:07:31,280
leads to a lot of governance issues as

00:07:30,080 --> 00:07:33,199
well i mean auditing and

00:07:31,280 --> 00:07:34,479
governance issues as well and it is very

00:07:33,199 --> 00:07:37,360
hard to do

00:07:34,479 --> 00:07:38,880
uh sixth challenge is about uh

00:07:37,360 --> 00:07:42,160
difficulty to handle

00:07:38,880 --> 00:07:44,960
large metadata i mean if i used uh

00:07:42,160 --> 00:07:45,520
hadoop hdfs for that matter where you

00:07:44,960 --> 00:07:48,560
would

00:07:45,520 --> 00:07:51,039
have a huge amount of data in your

00:07:48,560 --> 00:07:51,919
hdfs that internally causes i mean

00:07:51,039 --> 00:07:54,400
that's because

00:07:51,919 --> 00:07:56,000
a huge amount of data implies large

00:07:54,400 --> 00:07:58,080
metadata to be stored at

00:07:56,000 --> 00:08:00,000
name node for example and all such

00:07:58,080 --> 00:08:02,000
problems uh would

00:08:00,000 --> 00:08:03,680
magnify the moment you use petabyte of

00:08:02,000 --> 00:08:06,240
data in the data lake and

00:08:03,680 --> 00:08:07,520
it's very very tough and even the

00:08:06,240 --> 00:08:09,680
metadata itself

00:08:07,520 --> 00:08:11,120
lands into gigabytes and gigabytes of

00:08:09,680 --> 00:08:13,280
data

00:08:11,120 --> 00:08:14,400
um this is one of the most classical

00:08:13,280 --> 00:08:16,479
problems i would say

00:08:14,400 --> 00:08:18,080
like too many small files uh too many

00:08:16,479 --> 00:08:19,919
files i mean

00:08:18,080 --> 00:08:21,759
because of your using streaming for

00:08:19,919 --> 00:08:23,919
example that implies

00:08:21,759 --> 00:08:25,120
too much of data is landing in and you

00:08:23,919 --> 00:08:26,800
are processing the data

00:08:25,120 --> 00:08:28,479
at a very very breakneck speed like

00:08:26,800 --> 00:08:29,680
every 10 minutes every five minutes for

00:08:28,479 --> 00:08:32,080
example or even

00:08:29,680 --> 00:08:34,159
every minute and you are saving the data

00:08:32,080 --> 00:08:37,120
into the data lake that implies

00:08:34,159 --> 00:08:37,519
you are storing two tiny small files too

00:08:37,120 --> 00:08:40,800
many

00:08:37,519 --> 00:08:42,719
small files are some are sometimes

00:08:40,800 --> 00:08:45,360
gigantic files also i mean either of

00:08:42,719 --> 00:08:47,760
them are usually a big big challenge and

00:08:45,360 --> 00:08:49,360
most of the time is spent by spark just

00:08:47,760 --> 00:08:50,160
opening and reading the files rather

00:08:49,360 --> 00:08:51,519
than

00:08:50,160 --> 00:08:53,680
opening and closing files rather than

00:08:51,519 --> 00:08:56,000
reading the file usually

00:08:53,680 --> 00:08:56,880
on the same note it is very very tough

00:08:56,000 --> 00:09:00,320
to get

00:08:56,880 --> 00:09:01,120
great performance and it is it has to be

00:09:00,320 --> 00:09:03,120
manually done

00:09:01,120 --> 00:09:05,200
and it is error prone to get right

00:09:03,120 --> 00:09:08,560
partitioning and ensuring

00:09:05,200 --> 00:09:10,800
manual techniques applied to get a very

00:09:08,560 --> 00:09:11,920
decent performance not not so great

00:09:10,800 --> 00:09:13,600
performance i would say

00:09:11,920 --> 00:09:15,040
it's more of getting a decent

00:09:13,600 --> 00:09:18,240
performance here

00:09:15,040 --> 00:09:20,720
and finally data quality issues like

00:09:18,240 --> 00:09:22,160
usually uh i mean it's happened that

00:09:20,720 --> 00:09:24,959
data evolves

00:09:22,160 --> 00:09:26,399
and that implies as the schema evolves

00:09:24,959 --> 00:09:29,200
the underlying storage

00:09:26,399 --> 00:09:31,519
would either have to read the data or

00:09:29,200 --> 00:09:33,279
store the data and that implies the

00:09:31,519 --> 00:09:34,959
downstream pipelines would have problem

00:09:33,279 --> 00:09:35,680
in reading that particular data which

00:09:34,959 --> 00:09:37,760
has

00:09:35,680 --> 00:09:39,680
different metadata different columns

00:09:37,760 --> 00:09:42,880
compared to the earlier data so

00:09:39,680 --> 00:09:43,600
all these are usual challenges which you

00:09:42,880 --> 00:09:46,720
will face

00:09:43,600 --> 00:09:47,680
uh with any of the existing data links

00:09:46,720 --> 00:09:50,160
which are stored in

00:09:47,680 --> 00:09:51,279
any of the formats like pake for example

00:09:50,160 --> 00:09:55,040
so this is where

00:09:51,279 --> 00:09:57,279
our delta comes into the picture and

00:09:55,040 --> 00:09:58,959
i'm going to explain why delta lake

00:09:57,279 --> 00:09:59,680
solves these particular problems

00:09:58,959 --> 00:10:01,839
whatever we

00:09:59,680 --> 00:10:02,880
discussed the name challenges and how it

00:10:01,839 --> 00:10:05,839
solves also

00:10:02,880 --> 00:10:07,040
is what we are going to discuss now so

00:10:05,839 --> 00:10:08,959
first and foremost it is

00:10:07,040 --> 00:10:10,880
based on open format and it is open

00:10:08,959 --> 00:10:14,079
source you can find all the code

00:10:10,880 --> 00:10:14,959
of data at delta dot io uh it is

00:10:14,079 --> 00:10:18,560
basically

00:10:14,959 --> 00:10:21,839
an opinionated approach

00:10:18,560 --> 00:10:22,160
for building a robust data x and what i

00:10:21,839 --> 00:10:25,279
mean

00:10:22,160 --> 00:10:26,320
that is like it has its own transaction

00:10:25,279 --> 00:10:28,880
log mechanism

00:10:26,320 --> 00:10:30,320
i mean it we i will briefly show in the

00:10:28,880 --> 00:10:33,360
next slide itself where

00:10:30,320 --> 00:10:34,160
how the transaction log looks like so it

00:10:33,360 --> 00:10:37,040
brings

00:10:34,160 --> 00:10:39,120
both the best of data fire housing and

00:10:37,040 --> 00:10:39,839
data lakes all together into one single

00:10:39,120 --> 00:10:43,279
format

00:10:39,839 --> 00:10:43,760
and it helps for ensuring the downstream

00:10:43,279 --> 00:10:45,360
reading

00:10:43,760 --> 00:10:47,360
is perfectly fine even when you're

00:10:45,360 --> 00:10:50,240
writing some data to the same table

00:10:47,360 --> 00:10:51,200
same table same location databricks

00:10:50,240 --> 00:10:53,680
delta adds

00:10:51,200 --> 00:10:54,640
reliability quality and performance to

00:10:53,680 --> 00:10:57,360
data lakes

00:10:54,640 --> 00:10:59,360
how it how it does is what we are going

00:10:57,360 --> 00:11:02,399
to discuss in the next few slides

00:10:59,360 --> 00:11:05,839
um delta lake is comprised of

00:11:02,399 --> 00:11:08,959
only three three important topics one is

00:11:05,839 --> 00:11:11,200
uh delta tables which is where the

00:11:08,959 --> 00:11:12,240
data is stored and the delta

00:11:11,200 --> 00:11:14,560
optimization engine

00:11:12,240 --> 00:11:15,839
which is where the it allows to do

00:11:14,560 --> 00:11:18,959
mergers upsets

00:11:15,839 --> 00:11:21,440
deeds it allows to do vacuuming and

00:11:18,959 --> 00:11:23,360
and optimizing so on and so forth and

00:11:21,440 --> 00:11:24,079
finally delta x storage layer so these

00:11:23,360 --> 00:11:27,440
are the three

00:11:24,079 --> 00:11:29,760
uh components of delta lake now

00:11:27,440 --> 00:11:31,440
to add on top of what i briefly

00:11:29,760 --> 00:11:34,000
mentioned before

00:11:31,440 --> 00:11:35,120
uh telelac offers all these important

00:11:34,000 --> 00:11:38,000
features like

00:11:35,120 --> 00:11:38,560
asset transactions and spark i mean you

00:11:38,000 --> 00:11:40,720
can

00:11:38,560 --> 00:11:42,480
can ensure that whatever you're writing

00:11:40,720 --> 00:11:44,959
to delta table

00:11:42,480 --> 00:11:46,480
that will not be read by another

00:11:44,959 --> 00:11:47,839
pipeline which is reading at the same

00:11:46,480 --> 00:11:49,920
time i mean that's how

00:11:47,839 --> 00:11:52,000
that is all transaction isolation is

00:11:49,920 --> 00:11:54,800
maintained on tilde lake

00:11:52,000 --> 00:11:55,600
it allows for unifying streaming and

00:11:54,800 --> 00:11:57,680
batch

00:11:55,600 --> 00:11:58,880
with the same table you could write to

00:11:57,680 --> 00:12:01,519
the same table

00:11:58,880 --> 00:12:03,440
and a bad chance can also write to the

00:12:01,519 --> 00:12:04,800
same location and also streaming also

00:12:03,440 --> 00:12:05,279
can write to the same location and it

00:12:04,800 --> 00:12:07,760
allows

00:12:05,279 --> 00:12:09,440
uh both uh both the both the patterns to

00:12:07,760 --> 00:12:11,120
be to be done at the same time

00:12:09,440 --> 00:12:12,959
so basically lambda architecture being

00:12:11,120 --> 00:12:16,800
resolved just by using

00:12:12,959 --> 00:12:19,920
uh delta format and it also talks about

00:12:16,800 --> 00:12:22,480
uh schema enforcement and where required

00:12:19,920 --> 00:12:23,120
you can also enable schema evolution

00:12:22,480 --> 00:12:25,440
which is

00:12:23,120 --> 00:12:27,600
what we have is simple demo uh

00:12:25,440 --> 00:12:30,240
showcasing that particular feature today

00:12:27,600 --> 00:12:32,079
uh it also allows you to do time travel

00:12:30,240 --> 00:12:34,560
like you can go back in time

00:12:32,079 --> 00:12:36,560
and look at the data and how it who

00:12:34,560 --> 00:12:38,480
processed it who added it and

00:12:36,560 --> 00:12:40,240
on which cluster on which date so on and

00:12:38,480 --> 00:12:43,120
so forth could also be seen uh

00:12:40,240 --> 00:12:44,560
on using time travel uh upsets and

00:12:43,120 --> 00:12:47,040
delays are one of the major

00:12:44,560 --> 00:12:48,320
major important factor important options

00:12:47,040 --> 00:12:51,920
of

00:12:48,320 --> 00:12:52,639
delta uh suggestion support also is a

00:12:51,920 --> 00:12:54,639
palatal

00:12:52,639 --> 00:12:55,920
so just going back to the all the nine

00:12:54,639 --> 00:12:59,120
challenge challenges

00:12:55,920 --> 00:13:00,560
uh how delta tackles those challenges is

00:12:59,120 --> 00:13:02,880
what we'll discuss in the next few

00:13:00,560 --> 00:13:05,279
slides basically so

00:13:02,880 --> 00:13:06,399
there are asset transactions and all

00:13:05,279 --> 00:13:09,839
these

00:13:06,399 --> 00:13:11,920
the first five channel challenges are

00:13:09,839 --> 00:13:13,440
are resolved by delta lake by using

00:13:11,920 --> 00:13:15,200
asset transactions

00:13:13,440 --> 00:13:17,519
each and every table whenever you write

00:13:15,200 --> 00:13:20,079
data it sits

00:13:17,519 --> 00:13:21,040
in the cloud object storage or hdfs for

00:13:20,079 --> 00:13:24,720
that matter

00:13:21,040 --> 00:13:26,240
and there is a small uh metadata folder

00:13:24,720 --> 00:13:26,880
which gets created like as you can see

00:13:26,240 --> 00:13:29,839
here

00:13:26,880 --> 00:13:31,519
in the uh in the location slash path

00:13:29,839 --> 00:13:34,079
slash table slash

00:13:31,519 --> 00:13:34,639
slash uh underscore delta underscore

00:13:34,079 --> 00:13:36,480
that's the

00:13:34,639 --> 00:13:37,920
folder location i mean wherever you

00:13:36,480 --> 00:13:40,959
write a table

00:13:37,920 --> 00:13:42,399
uh say say customers table for example

00:13:40,959 --> 00:13:44,079
and in within that customer's table

00:13:42,399 --> 00:13:45,680
there will be a subfolder created

00:13:44,079 --> 00:13:47,920
by name underscore delta underscore log

00:13:45,680 --> 00:13:49,680
and within that you would have for each

00:13:47,920 --> 00:13:51,839
transaction there will be a separate

00:13:49,680 --> 00:13:54,800
file created a json file connected

00:13:51,839 --> 00:13:55,440
so which is what the is the heart and

00:13:54,800 --> 00:13:58,720
soul of

00:13:55,440 --> 00:14:00,240
uh delta basically so whenever you write

00:13:58,720 --> 00:14:03,360
any entry any any

00:14:00,240 --> 00:14:04,480
row or delete or merge or do anything on

00:14:03,360 --> 00:14:06,880
a particular table

00:14:04,480 --> 00:14:08,639
all that is recorded as transactions in

00:14:06,880 --> 00:14:11,680
uh in that particular table

00:14:08,639 --> 00:14:14,480
so going ahead uh yeah

00:14:11,680 --> 00:14:16,000
and finally whenever uh the number of

00:14:14,480 --> 00:14:18,560
transactions increase

00:14:16,000 --> 00:14:20,720
what uh database delta does is it

00:14:18,560 --> 00:14:21,920
checkpoints them as a packet file so

00:14:20,720 --> 00:14:23,760
that is also done

00:14:21,920 --> 00:14:25,279
implicitly by databricks you wouldn't

00:14:23,760 --> 00:14:27,120
need to do that you wouldn't need to

00:14:25,279 --> 00:14:30,000
worry or bother about that so

00:14:27,120 --> 00:14:31,600
this is how it does all this like it is

00:14:30,000 --> 00:14:33,519
hard to append data and all these

00:14:31,600 --> 00:14:37,040
problems are resolved just by using

00:14:33,519 --> 00:14:37,680
uh data and as we discussed a bit about

00:14:37,040 --> 00:14:39,920
time travel

00:14:37,680 --> 00:14:41,680
yeah it allows time travel and all the

00:14:39,920 --> 00:14:42,800
transactions are recorded because of the

00:14:41,680 --> 00:14:45,680
transaction log

00:14:42,800 --> 00:14:46,240
and it will allow you to go back and

00:14:45,680 --> 00:14:49,360
play

00:14:46,240 --> 00:14:51,360
basically play play forward um

00:14:49,360 --> 00:14:53,440
difficult to append uh difficult to

00:14:51,360 --> 00:14:54,320
handle large metadata all large metadata

00:14:53,440 --> 00:14:56,320
as i mentioned

00:14:54,320 --> 00:14:57,680
it metadata is stored in open pocket

00:14:56,320 --> 00:14:59,279
format and

00:14:57,680 --> 00:15:01,199
and it is resolved by just by reading

00:14:59,279 --> 00:15:03,040
the file and also

00:15:01,199 --> 00:15:05,360
portions of it can be cached and

00:15:03,040 --> 00:15:08,079
optimized for faster fast access

00:15:05,360 --> 00:15:09,519
um this is a huge huge problem usually

00:15:08,079 --> 00:15:12,079
too many small files or

00:15:09,519 --> 00:15:13,600
profiles this is where things get get

00:15:12,079 --> 00:15:15,920
very very very interesting like

00:15:13,600 --> 00:15:18,320
with delta you can just write a simple

00:15:15,920 --> 00:15:20,240
command optimize so once a table

00:15:18,320 --> 00:15:21,920
that implies it will bring back all the

00:15:20,240 --> 00:15:23,440
entire file in

00:15:21,920 --> 00:15:25,360
entire data in that particular folder

00:15:23,440 --> 00:15:27,519
where possible to 1 gb each

00:15:25,360 --> 00:15:28,880
one file each i mean and it works within

00:15:27,519 --> 00:15:31,199
the partitions also so

00:15:28,880 --> 00:15:32,880
this way we are going to uh we are going

00:15:31,199 --> 00:15:34,959
to resolve the problem of too many small

00:15:32,880 --> 00:15:36,639
files as well

00:15:34,959 --> 00:15:38,079
uh finally data quality issues like

00:15:36,639 --> 00:15:41,839
schema validation and

00:15:38,079 --> 00:15:44,240
evolution um delta supports

00:15:41,839 --> 00:15:45,360
schema validation as well as evolution

00:15:44,240 --> 00:15:47,680
even in merge and

00:15:45,360 --> 00:15:49,040
merge scenarios mergers and absorb

00:15:47,680 --> 00:15:51,519
scenarios as well

00:15:49,040 --> 00:15:52,639
so this is what we are going to discuss

00:15:51,519 --> 00:15:54,399
today i mean that

00:15:52,639 --> 00:15:57,199
exactly the same topic we are going to

00:15:54,399 --> 00:15:58,959
talk today updates deletes and upsets on

00:15:57,199 --> 00:16:02,639
a delta lag table

00:15:58,959 --> 00:16:03,279
so um the after the nine challenges we

00:16:02,639 --> 00:16:05,600
discussed

00:16:03,279 --> 00:16:06,320
uh i'm going to touch upon heart open

00:16:05,600 --> 00:16:08,800
data

00:16:06,320 --> 00:16:09,600
modification of existing data being uh

00:16:08,800 --> 00:16:11,920
difficult

00:16:09,600 --> 00:16:12,639
and finally too many small files uh uh

00:16:11,920 --> 00:16:14,560
to main file

00:16:12,639 --> 00:16:16,320
small file problem as well as poor

00:16:14,560 --> 00:16:18,079
performance so

00:16:16,320 --> 00:16:19,839
what are the sample use cases for

00:16:18,079 --> 00:16:22,800
updates deletes and

00:16:19,839 --> 00:16:24,480
upsets so first and foremost would be uh

00:16:22,800 --> 00:16:25,120
whenever you want to do a delete or a

00:16:24,480 --> 00:16:26,560
merge

00:16:25,120 --> 00:16:28,959
there might be a problem there might be

00:16:26,560 --> 00:16:30,639
a case where someone sent a request for

00:16:28,959 --> 00:16:32,000
uh right to be forgotten so gdpr

00:16:30,639 --> 00:16:34,079
compliance should be the

00:16:32,000 --> 00:16:36,720
might be the one of the simplest use

00:16:34,079 --> 00:16:39,600
cases you would you couldn't imagine

00:16:36,720 --> 00:16:40,800
and uh duplication deduplication you

00:16:39,600 --> 00:16:43,120
would like to read

00:16:40,800 --> 00:16:45,120
your entire data lake even that is

00:16:43,120 --> 00:16:48,079
simply easily possible with

00:16:45,120 --> 00:16:49,519
delta i and finally what are the

00:16:48,079 --> 00:16:52,800
challenges with

00:16:49,519 --> 00:16:56,000
with this without delta lake is

00:16:52,800 --> 00:16:56,639
it is inefficient probably possibly

00:16:56,000 --> 00:16:58,399
incorrect

00:16:56,639 --> 00:17:00,800
and it is very very hard to maintain and

00:16:58,399 --> 00:17:03,839
unreliable any of these upsells

00:17:00,800 --> 00:17:04,880
mostly more so with merges it is very

00:17:03,839 --> 00:17:08,959
inefficient

00:17:04,880 --> 00:17:10,959
and it is very very manual to do it so

00:17:08,959 --> 00:17:13,199
which is what will come to the first

00:17:10,959 --> 00:17:15,199
topic which is update

00:17:13,199 --> 00:17:16,799
update on our delta table now if you see

00:17:15,199 --> 00:17:18,799
the syntax it almost

00:17:16,799 --> 00:17:19,919
looks like exactly what you would do in

00:17:18,799 --> 00:17:22,959
a rdbms

00:17:19,919 --> 00:17:25,039
uh query rdbms any of the rdms

00:17:22,959 --> 00:17:26,079
rdbms you might have used so the key

00:17:25,039 --> 00:17:28,240
features are

00:17:26,079 --> 00:17:29,760
updates any column for the rows that

00:17:28,240 --> 00:17:32,960
match a predicate which is what

00:17:29,760 --> 00:17:35,679
it's a pretty simple statement to do it

00:17:32,960 --> 00:17:36,080
and similarly for delete exactly the

00:17:35,679 --> 00:17:38,559
same

00:17:36,080 --> 00:17:39,600
like what you would do in a rdbms delete

00:17:38,559 --> 00:17:42,080
from so-called

00:17:39,600 --> 00:17:43,679
customer table where some column

00:17:42,080 --> 00:17:45,840
predicate is what you would provide

00:17:43,679 --> 00:17:47,440
i mean in both the cases updates the

00:17:45,840 --> 00:17:48,880
column values for the rows that match a

00:17:47,440 --> 00:17:50,799
predicate but if you don't

00:17:48,880 --> 00:17:52,480
provide any predicate it updates all

00:17:50,799 --> 00:17:54,320
values for all rows whatever

00:17:52,480 --> 00:17:56,400
your you have mentioned here like update

00:17:54,320 --> 00:17:58,240
languages set name is called python 35

00:17:56,400 --> 00:18:00,240
specified without a predicate

00:17:58,240 --> 00:18:02,480
it will just blindly update everything

00:18:00,240 --> 00:18:04,160
so that's the same thing even for delete

00:18:02,480 --> 00:18:05,520
if there is no predicate given then

00:18:04,160 --> 00:18:07,760
deletes all the rows

00:18:05,520 --> 00:18:08,880
and which is finally we come to the

00:18:07,760 --> 00:18:12,559
important topic of

00:18:08,880 --> 00:18:14,160
upsets so merge without data lake would

00:18:12,559 --> 00:18:15,520
be very very painful to

00:18:14,160 --> 00:18:18,400
just to walk through the simplest

00:18:15,520 --> 00:18:21,440
possible approach which is approach to

00:18:18,400 --> 00:18:23,679
uh with with merge analyzing the

00:18:21,440 --> 00:18:25,280
updates on a table and find out the

00:18:23,679 --> 00:18:27,200
partitions to overwrite

00:18:25,280 --> 00:18:28,640
that will be the first step and read all

00:18:27,200 --> 00:18:30,080
the data in the relevant partitions in

00:18:28,640 --> 00:18:32,320
the target table

00:18:30,080 --> 00:18:33,840
then joining these two tables overrate

00:18:32,320 --> 00:18:34,559
all those partitions in exchanging

00:18:33,840 --> 00:18:37,360
location

00:18:34,559 --> 00:18:38,559
and then atomically publish this is what

00:18:37,360 --> 00:18:41,600
it would look like

00:18:38,559 --> 00:18:44,960
i mean emerge will look like without

00:18:41,600 --> 00:18:46,160
delta lake and how merge is resolved

00:18:44,960 --> 00:18:47,919
with data lake

00:18:46,160 --> 00:18:49,200
it is a pretty pretty simple uh

00:18:47,919 --> 00:18:51,280
statement to do it

00:18:49,200 --> 00:18:52,400
like if you do merge if you have a

00:18:51,280 --> 00:18:53,440
customer's table and if you have an

00:18:52,400 --> 00:18:56,880
update stable

00:18:53,440 --> 00:18:59,520
now you would like to do update

00:18:56,880 --> 00:19:00,720
some customers whose customer id and

00:18:59,520 --> 00:19:02,720
source id are present

00:19:00,720 --> 00:19:05,039
and you have a new address for all those

00:19:02,720 --> 00:19:07,679
customers you can just do this like

00:19:05,039 --> 00:19:08,400
it in this case we are both doing an

00:19:07,679 --> 00:19:10,880
update

00:19:08,400 --> 00:19:12,160
set and if it is not available then we

00:19:10,880 --> 00:19:15,520
are inserting so basically

00:19:12,160 --> 00:19:17,360
up cert update and insert update

00:19:15,520 --> 00:19:19,280
or insert is what is happening and we

00:19:17,360 --> 00:19:20,160
can also do uh in fact delete also in

00:19:19,280 --> 00:19:23,039
this

00:19:20,160 --> 00:19:23,360
so behind the scenes what merge does is

00:19:23,039 --> 00:19:25,760
it

00:19:23,360 --> 00:19:26,480
basically does an inner join between

00:19:25,760 --> 00:19:29,600
update and

00:19:26,480 --> 00:19:30,480
target and it is not doing it on the

00:19:29,600 --> 00:19:32,160
entire data

00:19:30,480 --> 00:19:33,919
it's actually going and looking at the

00:19:32,160 --> 00:19:36,480
min and max files of

00:19:33,919 --> 00:19:37,039
max values of the file and getting those

00:19:36,480 --> 00:19:39,679
values

00:19:37,039 --> 00:19:40,720
and trying to do uh some intelligent

00:19:39,679 --> 00:19:43,280
analysis there

00:19:40,720 --> 00:19:44,000
so i'm not working through everything

00:19:43,280 --> 00:19:45,600
here but

00:19:44,000 --> 00:19:47,120
i'll just walk through so that i can get

00:19:45,600 --> 00:19:49,760
the demo sooner

00:19:47,120 --> 00:19:51,520
um optimize and vacuum are very

00:19:49,760 --> 00:19:53,840
important concepts as i said

00:19:51,520 --> 00:19:55,360
it is bing packing compact compaction

00:19:53,840 --> 00:19:57,679
and also

00:19:55,360 --> 00:19:59,120
it allows data skipping with these with

00:19:57,679 --> 00:20:02,400
optimized events where

00:19:59,120 --> 00:20:05,280
so date and zero duraby and

00:20:02,400 --> 00:20:06,000
similarly vacuum you have vacuum it is

00:20:05,280 --> 00:20:08,720
pretty simple

00:20:06,000 --> 00:20:09,200
to do vacuum and so on so table it will

00:20:08,720 --> 00:20:12,000
clean up

00:20:09,200 --> 00:20:12,960
all the old undropped untracked files of

00:20:12,000 --> 00:20:16,080
delta

00:20:12,960 --> 00:20:18,000
so that it can limit the storage cost

00:20:16,080 --> 00:20:19,840
i will quickly jump on to the demos it

00:20:18,000 --> 00:20:23,520
is pretty pretty simple demos

00:20:19,840 --> 00:20:25,520
i'm using this is a simple cluster

00:20:23,520 --> 00:20:27,200
i'm using this is database platform by

00:20:25,520 --> 00:20:30,400
the way and

00:20:27,200 --> 00:20:34,000
i'm using a simplest possible

00:20:30,400 --> 00:20:36,640
use case here sorry i think i'm sharing

00:20:34,000 --> 00:20:36,640
wrong screen

00:20:38,880 --> 00:20:43,360
so i'm showcasing uh update uh columns

00:20:41,919 --> 00:20:45,600
of a delta table

00:20:43,360 --> 00:20:47,840
and delete rows away delta double so

00:20:45,600 --> 00:20:48,480
basically what i'm trying to do here is

00:20:47,840 --> 00:20:51,840
uh

00:20:48,480 --> 00:20:54,640
i have a small data set like i have

00:20:51,840 --> 00:20:55,200
i have a small data set where i i have

00:20:54,640 --> 00:20:57,280
spark

00:20:55,200 --> 00:20:58,799
database and data i mean by mistake

00:20:57,280 --> 00:21:02,320
someone wrote a code

00:20:58,799 --> 00:21:05,360
which ensure which cost the pipeline

00:21:02,320 --> 00:21:07,520
uh to fair to to uh right

00:21:05,360 --> 00:21:09,039
to write incorrect values like as you

00:21:07,520 --> 00:21:12,559
can see here it is

00:21:09,039 --> 00:21:15,200
data now this is a over simplified

00:21:12,559 --> 00:21:16,320
example per se so that we can walk

00:21:15,200 --> 00:21:19,120
through

00:21:16,320 --> 00:21:20,640
uh the use case to to explain what

00:21:19,120 --> 00:21:22,559
databricks filter does so

00:21:20,640 --> 00:21:23,760
what i'm doing here is i'm just writing

00:21:22,559 --> 00:21:25,520
the data to

00:21:23,760 --> 00:21:27,600
a data table in the data format and

00:21:25,520 --> 00:21:30,559
providing a path so that implies

00:21:27,600 --> 00:21:31,600
it is writing to the uh external table

00:21:30,559 --> 00:21:34,400
basically

00:21:31,600 --> 00:21:35,600
now i'm displaying the data in from the

00:21:34,400 --> 00:21:38,400
delta table here

00:21:35,600 --> 00:21:38,720
as you can see it is it is it is coming

00:21:38,400 --> 00:21:41,919
as

00:21:38,720 --> 00:21:44,080
data as you can see now as we

00:21:41,919 --> 00:21:45,200
saw earlier now what i'm trying to do

00:21:44,080 --> 00:21:47,840
here is i have

00:21:45,200 --> 00:21:48,240
a id column which i know there is a tree

00:21:47,840 --> 00:21:51,039
and

00:21:48,240 --> 00:21:52,480
for which there is data here now this is

00:21:51,039 --> 00:21:56,000
where we are going to do

00:21:52,480 --> 00:21:58,799
some magic like updates onto table set

00:21:56,000 --> 00:22:00,640
some column is equal to some value where

00:21:58,799 --> 00:22:02,159
condition i mean you can specify

00:22:00,640 --> 00:22:04,480
either this condition or this condition

00:22:02,159 --> 00:22:06,799
but basically both are exactly same

00:22:04,480 --> 00:22:07,840
and it is atomically doing everything

00:22:06,799 --> 00:22:10,400
behind the scenes so

00:22:07,840 --> 00:22:10,960
it it is also showing number of affected

00:22:10,400 --> 00:22:13,760
rows here

00:22:10,960 --> 00:22:14,880
this is how delta performs uh in real

00:22:13,760 --> 00:22:17,679
world so

00:22:14,880 --> 00:22:19,280
you can see the value got changed and

00:22:17,679 --> 00:22:20,880
the same thing which i'm going to do

00:22:19,280 --> 00:22:22,080
here is i'm going to delete that

00:22:20,880 --> 00:22:25,919
particular row

00:22:22,080 --> 00:22:27,360
uh from from delta so again it shows how

00:22:25,919 --> 00:22:28,960
how many rows it got affected i mean

00:22:27,360 --> 00:22:31,679
this is your own simplified

00:22:28,960 --> 00:22:33,440
example but you can get the gist out of

00:22:31,679 --> 00:22:36,799
what i'm trying to do here

00:22:33,440 --> 00:22:39,440
and uh let me display the table

00:22:36,799 --> 00:22:40,000
again so you get a spark and data breaks

00:22:39,440 --> 00:22:43,039
because we

00:22:40,000 --> 00:22:45,280
deleted uh id 3 here now

00:22:43,039 --> 00:22:46,880
behind the scenes as i mentioned

00:22:45,280 --> 00:22:47,679
databricks is maintaining a transaction

00:22:46,880 --> 00:22:50,480
log which is

00:22:47,679 --> 00:22:51,919
what would look like and this is the

00:22:50,480 --> 00:22:55,039
visual representation of

00:22:51,919 --> 00:22:57,840
transaction log every operation i mean

00:22:55,039 --> 00:22:59,039
this is my email id and this is my user

00:22:57,840 --> 00:23:01,919
id and this is the time stamp

00:22:59,039 --> 00:23:03,600
i'm based out of london so it is showing

00:23:01,919 --> 00:23:06,480
gmt basically

00:23:03,600 --> 00:23:07,679
here and you can see the what kind of

00:23:06,480 --> 00:23:09,039
operation was done

00:23:07,679 --> 00:23:10,720
and what are the predicates what the

00:23:09,039 --> 00:23:13,280
operation parameters

00:23:10,720 --> 00:23:14,559
etc etc all this information is at a

00:23:13,280 --> 00:23:16,480
single snapshot like

00:23:14,559 --> 00:23:18,799
a single single source of truth

00:23:16,480 --> 00:23:20,720
basically and if i want to do

00:23:18,799 --> 00:23:22,320
some time travel for example i can go

00:23:20,720 --> 00:23:25,120
back in time and play

00:23:22,320 --> 00:23:26,080
play on the time data like you can see

00:23:25,120 --> 00:23:28,640
here i

00:23:26,080 --> 00:23:30,640
initially have uh ingested data as data

00:23:28,640 --> 00:23:34,080
so which is what it is showing

00:23:30,640 --> 00:23:37,440
now the next step is showcasing uh

00:23:34,080 --> 00:23:40,559
delta so if you see in the third when

00:23:37,440 --> 00:23:42,320
we deleted the third row we can see

00:23:40,559 --> 00:23:44,000
delta is not present but if we go back

00:23:42,320 --> 00:23:46,320
in time we can see that

00:23:44,000 --> 00:23:47,039
and finally the most recent version is

00:23:46,320 --> 00:23:48,640
dead

00:23:47,039 --> 00:23:50,080
you can see the most recent question so

00:23:48,640 --> 00:23:53,120
this is how it is

00:23:50,080 --> 00:23:57,440
so pretty simple and name and easy to do

00:23:53,120 --> 00:24:01,120
uh with delta update delete and all this

00:23:57,440 --> 00:24:05,039
very very simply so let me go to my next

00:24:01,120 --> 00:24:07,120
notebook which is more schema evolution

00:24:05,039 --> 00:24:08,400
probably i'm zipping through because i

00:24:07,120 --> 00:24:11,679
just have five more minutes

00:24:08,400 --> 00:24:12,480
um so this is schema enforcement during

00:24:11,679 --> 00:24:15,600
merge

00:24:12,480 --> 00:24:17,840
and the the use case here is

00:24:15,600 --> 00:24:18,960
i have two columns id and name the

00:24:17,840 --> 00:24:22,080
latest data has

00:24:18,960 --> 00:24:23,919
three columns id name and year so a new

00:24:22,080 --> 00:24:26,320
column has been added to the data set

00:24:23,919 --> 00:24:27,600
year which is what we are showcasing in

00:24:26,320 --> 00:24:29,919
this particular case

00:24:27,600 --> 00:24:31,520
but the same time my requirement my

00:24:29,919 --> 00:24:34,480
business case says

00:24:31,520 --> 00:24:36,799
i want to insert all the data merge all

00:24:34,480 --> 00:24:39,279
the new data to the existing delta table

00:24:36,799 --> 00:24:40,080
but also enforcing the schema so that

00:24:39,279 --> 00:24:43,120
implies

00:24:40,080 --> 00:24:44,559
i need to discard the new column in the

00:24:43,120 --> 00:24:46,640
delta table

00:24:44,559 --> 00:24:47,760
so if i can quickly run through the

00:24:46,640 --> 00:24:50,320
entire notebook

00:24:47,760 --> 00:24:50,960
what i would do here is i'm just running

00:24:50,320 --> 00:24:52,799
the entire

00:24:50,960 --> 00:24:54,559
notebook and i have couple of tables

00:24:52,799 --> 00:24:57,039
source and target

00:24:54,559 --> 00:24:57,919
same as before i have three rows here i

00:24:57,039 --> 00:24:59,679
mean again

00:24:57,919 --> 00:25:01,440
over simplified example so that it is

00:24:59,679 --> 00:25:04,400
easy to explain

00:25:01,440 --> 00:25:05,840
now i wrote the table into data into a

00:25:04,400 --> 00:25:09,360
data table

00:25:05,840 --> 00:25:11,360
and i'm displaying the data here now

00:25:09,360 --> 00:25:12,640
the new data frame after couple of days

00:25:11,360 --> 00:25:16,000
assume after a couple of days

00:25:12,640 --> 00:25:18,000
has a new column as year now there are

00:25:16,000 --> 00:25:21,360
two use cases here one

00:25:18,000 --> 00:25:24,640
you want schema enforcement strictly

00:25:21,360 --> 00:25:26,880
adhered schema validation is done and

00:25:24,640 --> 00:25:28,080
it shouldn't add this new column into

00:25:26,880 --> 00:25:31,039
the data into the

00:25:28,080 --> 00:25:32,640
data link so which is what we are seeing

00:25:31,039 --> 00:25:36,000
here so basically

00:25:32,640 --> 00:25:38,960
i am doing a merge into some table using

00:25:36,000 --> 00:25:40,640
a source table based on a condition of a

00:25:38,960 --> 00:25:41,440
predicate condition of target.id is

00:25:40,640 --> 00:25:44,880
equal to

00:25:41,440 --> 00:25:46,159
source.id so i have id 2 which is

00:25:44,880 --> 00:25:49,039
already present in the

00:25:46,159 --> 00:25:51,120
data in the new data frame id 2 has a

00:25:49,039 --> 00:25:54,320
column 2 2013.

00:25:51,120 --> 00:25:56,159
now when i run merge syntax merge

00:25:54,320 --> 00:25:57,600
command on this table because this

00:25:56,159 --> 00:26:00,240
syntax you can see

00:25:57,600 --> 00:26:00,960
it's pretty simple it will look at all

00:26:00,240 --> 00:26:03,919
the rows

00:26:00,960 --> 00:26:05,039
and populate uh and update rows where

00:26:03,919 --> 00:26:07,360
it's required

00:26:05,039 --> 00:26:09,360
and where it is uh that particular row

00:26:07,360 --> 00:26:12,559
is not present it will insert the row

00:26:09,360 --> 00:26:15,520
so going back and you can see

00:26:12,559 --> 00:26:18,240
it is schema is enforced strictly so

00:26:15,520 --> 00:26:20,640
there is no new column here

00:26:18,240 --> 00:26:22,640
and if i go to the third notebook then

00:26:20,640 --> 00:26:25,279
my final notebook here

00:26:22,640 --> 00:26:26,240
here is where i am doing a schema

00:26:25,279 --> 00:26:29,679
evolution

00:26:26,240 --> 00:26:32,799
it's exactly the same uh notebook with

00:26:29,679 --> 00:26:35,039
no changes at all but

00:26:32,799 --> 00:26:36,159
only change i'm doing here is there is

00:26:35,039 --> 00:26:38,000
an option

00:26:36,159 --> 00:26:39,679
if you want schema evolution to be

00:26:38,000 --> 00:26:42,159
available even in the

00:26:39,679 --> 00:26:42,960
merge you can you just need to enable

00:26:42,159 --> 00:26:46,720
auto merge

00:26:42,960 --> 00:26:49,679
as syntax the moment you set this config

00:26:46,720 --> 00:26:51,200
what delta is doing behind the scenes is

00:26:49,679 --> 00:26:53,039
it is allowing you like

00:26:51,200 --> 00:26:55,120
as mentioned before it's exactly the

00:26:53,039 --> 00:26:57,200
same uh data frame

00:26:55,120 --> 00:26:58,720
the first data frame as well as the

00:26:57,200 --> 00:27:01,840
second data frame second data frame

00:26:58,720 --> 00:27:04,400
also has a year column now now

00:27:01,840 --> 00:27:04,960
with the same exact statement what i'm

00:27:04,400 --> 00:27:08,159
doing

00:27:04,960 --> 00:27:09,200
here is because i enabled schema

00:27:08,159 --> 00:27:11,760
evolution

00:27:09,200 --> 00:27:14,240
you could see here is coming out to be

00:27:11,760 --> 00:27:16,720
the value what we ingested so basically

00:27:14,240 --> 00:27:17,279
we have replaced databricks earlier

00:27:16,720 --> 00:27:19,760
value

00:27:17,279 --> 00:27:20,559
with 2013 so that's how the updates are

00:27:19,760 --> 00:27:23,360
happening

00:27:20,559 --> 00:27:23,679
behind the scenes you can you can also

00:27:23,360 --> 00:27:26,720
uh

00:27:23,679 --> 00:27:29,200
look at look at the way it is

00:27:26,720 --> 00:27:30,720
working and finally let me go to the

00:27:29,200 --> 00:27:33,279
last last thing here

00:27:30,720 --> 00:27:34,000
which is uh showcasing time travel and

00:27:33,279 --> 00:27:37,200
optimization

00:27:34,000 --> 00:27:39,279
vacuum so this is uh as

00:27:37,200 --> 00:27:41,360
i mentioned create table and merge which

00:27:39,279 --> 00:27:44,799
is what we are we are doing

00:27:41,360 --> 00:27:46,840
at the same time we can do a time travel

00:27:44,799 --> 00:27:48,399
i mean we can go back to zero and one

00:27:46,840 --> 00:27:50,640
zero

00:27:48,399 --> 00:27:52,080
didn't have the year column here but now

00:27:50,640 --> 00:27:54,399
the latest version has

00:27:52,080 --> 00:27:55,120
here column and finally there is an

00:27:54,399 --> 00:27:57,520
option called

00:27:55,120 --> 00:27:59,440
optimize as i mentioned optimize you can

00:27:57,520 --> 00:28:00,240
see the beauty of optimize in a single

00:27:59,440 --> 00:28:02,480
command like

00:28:00,240 --> 00:28:03,279
here i have number of files as three the

00:28:02,480 --> 00:28:05,760
moment i

00:28:03,279 --> 00:28:07,679
run optimize command if i do a describe

00:28:05,760 --> 00:28:09,600
detail again on the same table

00:28:07,679 --> 00:28:11,279
to optimize it will optimize all the

00:28:09,600 --> 00:28:13,520
files into a single file i mean

00:28:11,279 --> 00:28:15,039
in this case we are using a smaller file

00:28:13,520 --> 00:28:16,640
but that's how it is

00:28:15,039 --> 00:28:18,399
and finally the last thing which i

00:28:16,640 --> 00:28:20,480
wanted to showcase is vacuum

00:28:18,399 --> 00:28:21,760
now before i run vacuum there are so

00:28:20,480 --> 00:28:24,080
many files

00:28:21,760 --> 00:28:25,919
the moment i mean database doesn't allow

00:28:24,080 --> 00:28:28,640
you to run vacuum as is

00:28:25,919 --> 00:28:30,000
so you have to enable uh if you want to

00:28:28,640 --> 00:28:32,640
run it as zero

00:28:30,000 --> 00:28:33,520
ratings you have to enable sp special

00:28:32,640 --> 00:28:35,840
flag

00:28:33,520 --> 00:28:36,880
once you enable special flag all the

00:28:35,840 --> 00:28:40,000
untracked files

00:28:36,880 --> 00:28:40,960
will be deleted from uh the the cloud

00:28:40,000 --> 00:28:43,360
object storage are

00:28:40,960 --> 00:28:44,640
the local storage so this is these are

00:28:43,360 --> 00:28:46,799
the three

00:28:44,640 --> 00:28:48,240
uh notebooks three use cases which i

00:28:46,799 --> 00:28:51,919
wanted to showcase

00:28:48,240 --> 00:28:54,399
and uh uh if you have any questions or

00:28:51,919 --> 00:28:55,520
uh any anything you would like to know i

00:28:54,399 --> 00:28:58,159
would be very happy

00:28:55,520 --> 00:28:59,840
to answer them and these are i am

00:28:58,159 --> 00:29:02,399
leaving further references

00:28:59,840 --> 00:29:02,960
so you can take a look at it this is a

00:29:02,399 --> 00:29:06,240
very

00:29:02,960 --> 00:29:08,480
new book like uh the three early release

00:29:06,240 --> 00:29:10,640
chapters were released just last week

00:29:08,480 --> 00:29:12,000
uh you can take a look at this uh the

00:29:10,640 --> 00:29:14,080
new book on delta lake

00:29:12,000 --> 00:29:15,679
and finally learning spark also has a

00:29:14,080 --> 00:29:18,399
chapter on data lake so

00:29:15,679 --> 00:29:18,720
there are a couple of uh docs and talks

00:29:18,399 --> 00:29:20,720
and

00:29:18,720 --> 00:29:23,120
webinars and so on and so forth i'm

00:29:20,720 --> 00:29:25,039
leaving all this uh for your reference

00:29:23,120 --> 00:29:26,640
please do uh let me know if you have any

00:29:25,039 --> 00:29:28,640
questions uh and

00:29:26,640 --> 00:29:30,159
thanks for having me uh thank you very

00:29:28,640 --> 00:29:33,039
much for uh giving me

00:29:30,159 --> 00:29:33,039
uh time today

00:29:34,240 --> 00:29:37,520
um hi thanks again for for your

00:29:36,240 --> 00:29:38,960
presentation uh

00:29:37,520 --> 00:29:40,640
it's really interesting to see all the

00:29:38,960 --> 00:29:41,919
things that delta can do it's pretty

00:29:40,640 --> 00:29:44,559
interesting format and

00:29:41,919 --> 00:29:46,159
well that's what you show is pretty cool

00:29:44,559 --> 00:29:47,840
i'm just going to check to see if there

00:29:46,159 --> 00:29:50,240
are some questions the last

00:29:47,840 --> 00:29:52,000
they were not so maybe i could start

00:29:50,240 --> 00:29:55,440
with just one from mine

00:29:52,000 --> 00:29:56,960
uh what are i mean well we talk about

00:29:55,440 --> 00:29:58,720
all these really nice things that delta

00:29:56,960 --> 00:29:59,760
can do what are some limitations that

00:29:58,720 --> 00:30:01,679
that has

00:29:59,760 --> 00:30:04,000
at the moment or things that you plan to

00:30:01,679 --> 00:30:07,360
improve or add in the future

00:30:04,000 --> 00:30:10,240
uh delta e is evolving as we as we

00:30:07,360 --> 00:30:11,039
continuously like i mean because mergers

00:30:10,240 --> 00:30:12,799
usually cause

00:30:11,039 --> 00:30:14,480
a lot of problem i mean they create

00:30:12,799 --> 00:30:16,880
multiple small files so

00:30:14,480 --> 00:30:18,880
as time goes on their database is adding

00:30:16,880 --> 00:30:20,480
more and more features into delta like

00:30:18,880 --> 00:30:22,000
low shuffle merge for example i'm just

00:30:20,480 --> 00:30:25,200
giving one simple

00:30:22,000 --> 00:30:25,679
example um which will allow which will

00:30:25,200 --> 00:30:27,520
not

00:30:25,679 --> 00:30:29,679
remove the ordering which is there in

00:30:27,520 --> 00:30:32,320
the local files i mean the files

00:30:29,679 --> 00:30:33,520
which are red so so that it will not

00:30:32,320 --> 00:30:35,200
relate the files

00:30:33,520 --> 00:30:37,039
in a different order rather it will

00:30:35,200 --> 00:30:38,799
retain the exact same order

00:30:37,039 --> 00:30:40,159
which was there before because of the z

00:30:38,799 --> 00:30:42,799
ordering it will help

00:30:40,159 --> 00:30:43,679
for data skipping for example so as time

00:30:42,799 --> 00:30:45,360
progresses

00:30:43,679 --> 00:30:47,120
delta is adding i mean databricks is

00:30:45,360 --> 00:30:49,279
adding more and more features into

00:30:47,120 --> 00:30:50,960
delta like change data feed is one more

00:30:49,279 --> 00:31:05,840
new feature which is coming in

00:30:50,960 --> 00:31:05,840
which is in private review to be correct

00:31:12,960 --> 00:31:15,039

YouTube URL: https://www.youtube.com/watch?v=BcOImrLimWw


