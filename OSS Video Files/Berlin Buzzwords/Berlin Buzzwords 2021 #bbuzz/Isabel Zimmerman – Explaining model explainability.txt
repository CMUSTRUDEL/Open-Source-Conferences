Title: Isabel Zimmerman – Explaining model explainability
Publication date: 2021-06-25
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Machine learning doesn’t have the same objectives as its users. While models look to optimize a function using the given data, humans look to gain insight into their problems. At best, these two objectives align; at worst, machine learning models make the front page of the news for unintended, but astonishing bias. Model explainability algorithms allow data scientists to understand not only what the model outcome is, but why it is being made. This talk will explain what model explainability is, who should care, and show participants how/when to use multiple types of explainability algorithms. 

This session shows the usefulness of a variety of algorithms, but also discusses the limitations. Told from a data scientist’s point of view, this session provides a use case scenario exposing unintended bias using healthcare data. The audience will learn: the basics of model explainability, why this is a relevant issue, how model explainability offers insight into unintended bias, and know how to deploy explainability algorithms in Python with alibi, the open-source library from Seldon.

Speaker:
Isabel Zimmerman – https://2021.berlinbuzzwords.de/member/isabel-zimmerman

More: https://2021.berlinbuzzwords.de/session/explaining-model-explainability
Captions: 
	00:00:07,759 --> 00:00:11,679
i am isabel zimmerman

00:00:09,200 --> 00:00:13,440
and i will be teaching about model

00:00:11,679 --> 00:00:15,759
explainability today

00:00:13,440 --> 00:00:17,840
i am a software engineer at red hat's

00:00:15,759 --> 00:00:18,960
artificial intelligence center of

00:00:17,840 --> 00:00:20,880
excellence

00:00:18,960 --> 00:00:22,960
with their ai opso artificial

00:00:20,880 --> 00:00:25,439
intelligence operations team

00:00:22,960 --> 00:00:27,599
i'm also a very recent graduate from

00:00:25,439 --> 00:00:29,199
florida polytechnic university's data

00:00:27,599 --> 00:00:30,880
science program

00:00:29,199 --> 00:00:32,239
and of course the most important

00:00:30,880 --> 00:00:35,040
personal tidbit um

00:00:32,239 --> 00:00:37,280
what was my pandemic hobby i've been

00:00:35,040 --> 00:00:40,160
spending hours as a homemade chemist

00:00:37,280 --> 00:00:41,840
to build the absolute perfect recipe for

00:00:40,160 --> 00:00:43,920
chocolate chip cookies

00:00:41,840 --> 00:00:45,760
so in my every day at work i get to live

00:00:43,920 --> 00:00:46,239
the dream and play around with code and

00:00:45,760 --> 00:00:48,399
break

00:00:46,239 --> 00:00:51,440
code and i get to spend a lot of time

00:00:48,399 --> 00:00:54,160
doing data science work as well

00:00:51,440 --> 00:00:55,920
and i really stumbled upon this topic of

00:00:54,160 --> 00:00:58,399
model explainability

00:00:55,920 --> 00:01:00,239
actually when i was working as an intern

00:00:58,399 --> 00:01:01,600
and it's just a topic that stuck with me

00:01:00,239 --> 00:01:04,159
ever since

00:01:01,600 --> 00:01:05,600
because people often times think that

00:01:04,159 --> 00:01:08,640
machine learning answers

00:01:05,600 --> 00:01:10,640
all of our questions but in reality

00:01:08,640 --> 00:01:13,600
machine learning doesn't even have the

00:01:10,640 --> 00:01:16,000
same objectives as its users

00:01:13,600 --> 00:01:18,799
we'll dive into this mismatch of

00:01:16,000 --> 00:01:21,360
objectives between models and humans

00:01:18,799 --> 00:01:24,560
and we'll talk about how explainably

00:01:21,360 --> 00:01:26,479
explainability helps to close this gap

00:01:24,560 --> 00:01:28,080
but i also want everyone to be able to

00:01:26,479 --> 00:01:29,680
walk away from today

00:01:28,080 --> 00:01:31,680
being able to tell their colleagues

00:01:29,680 --> 00:01:33,280
about what explainability is and why you

00:01:31,680 --> 00:01:34,880
should be implementing it

00:01:33,280 --> 00:01:37,119
and i also want everyone to gain

00:01:34,880 --> 00:01:39,920
knowledge of just a few algorithms

00:01:37,119 --> 00:01:41,119
you can add into your own models and

00:01:39,920 --> 00:01:44,320
where you can add them

00:01:41,119 --> 00:01:45,360
into your machine learning workflow so

00:01:44,320 --> 00:01:47,439
before we go

00:01:45,360 --> 00:01:49,439
any further i think it's time to give

00:01:47,439 --> 00:01:51,360
you kind of the 30 second elevator pitch

00:01:49,439 --> 00:01:53,840
for explainability

00:01:51,360 --> 00:01:55,920
we know a machine learning model is just

00:01:53,840 --> 00:01:57,119
some sort of algorithm that uses an

00:01:55,920 --> 00:02:00,079
input of data

00:01:57,119 --> 00:02:00,560
such as a cat picture to give an output

00:02:00,079 --> 00:02:03,759
of

00:02:00,560 --> 00:02:06,399
some insight into that data such as

00:02:03,759 --> 00:02:08,640
identifying the photo contents

00:02:06,399 --> 00:02:10,080
the output of the model really only

00:02:08,640 --> 00:02:13,040
answers the question of

00:02:10,080 --> 00:02:14,480
what in this example what is this a

00:02:13,040 --> 00:02:16,800
photo of

00:02:14,480 --> 00:02:18,160
a model will tell you that it's a cat

00:02:16,800 --> 00:02:20,879
the output of the model

00:02:18,160 --> 00:02:21,200
told you the contents of the photo but

00:02:20,879 --> 00:02:24,959
not

00:02:21,200 --> 00:02:28,080
the logic behind making the prediction

00:02:24,959 --> 00:02:30,160
explainability focuses on the why why

00:02:28,080 --> 00:02:33,280
did this model say this was a cat

00:02:30,160 --> 00:02:34,640
was it the eyes the ears the paws the

00:02:33,280 --> 00:02:36,959
color

00:02:34,640 --> 00:02:38,879
so models look to optimize a

00:02:36,959 --> 00:02:39,760
mathematical function using the given

00:02:38,879 --> 00:02:41,840
data

00:02:39,760 --> 00:02:43,840
and humans look to gain insight into

00:02:41,840 --> 00:02:46,959
their problems

00:02:43,840 --> 00:02:49,360
at best these two objectives align

00:02:46,959 --> 00:02:50,080
but at worst machine learning models

00:02:49,360 --> 00:02:52,959
make the front

00:02:50,080 --> 00:02:55,120
page news for unintended but really

00:02:52,959 --> 00:02:56,879
astonishing bias

00:02:55,120 --> 00:02:59,280
explainable machine learning is

00:02:56,879 --> 00:03:01,840
necessary to close this gap

00:02:59,280 --> 00:03:02,959
between the machine goal of output and

00:03:01,840 --> 00:03:05,680
the human goal of

00:03:02,959 --> 00:03:07,599
understanding and we need it when it's

00:03:05,680 --> 00:03:08,720
not enough to just get the prediction

00:03:07,599 --> 00:03:11,680
from your model

00:03:08,720 --> 00:03:14,239
the model must also justify how it came

00:03:11,680 --> 00:03:16,560
to this prediction

00:03:14,239 --> 00:03:18,400
admittedly i'm probably never going to

00:03:16,560 --> 00:03:19,120
convince you that explainability is

00:03:18,400 --> 00:03:22,239
important

00:03:19,120 --> 00:03:23,120
by showing you pictures of cats in fact

00:03:22,239 --> 00:03:25,920
one of the most

00:03:23,120 --> 00:03:26,799
powerful reasons to use explainability

00:03:25,920 --> 00:03:30,319
is that it can help

00:03:26,799 --> 00:03:33,280
uncover bias and improve people's lives

00:03:30,319 --> 00:03:35,440
so let's say a lender uses machine

00:03:33,280 --> 00:03:37,440
learning to determine someone's ability

00:03:35,440 --> 00:03:40,319
to pay back a loan

00:03:37,440 --> 00:03:42,560
if the lender is using a black box model

00:03:40,319 --> 00:03:45,440
someone who is denied a loan

00:03:42,560 --> 00:03:47,680
may not be given a reason why this

00:03:45,440 --> 00:03:50,239
unknown reasoning is problematic because

00:03:47,680 --> 00:03:52,239
one people will never know how they can

00:03:50,239 --> 00:03:53,360
change their habits to be approved for a

00:03:52,239 --> 00:03:55,760
loan

00:03:53,360 --> 00:03:57,040
and two it could be covering up this

00:03:55,760 --> 00:04:00,560
unintentional

00:03:57,040 --> 00:04:02,400
but harmful bias so let's think about

00:04:00,560 --> 00:04:03,519
this entire workflow a little bit more

00:04:02,400 --> 00:04:06,080
carefully

00:04:03,519 --> 00:04:07,280
here's your vanilla you know data

00:04:06,080 --> 00:04:09,360
science workflow

00:04:07,280 --> 00:04:11,920
and we know that data science starts

00:04:09,360 --> 00:04:13,200
with having a well-defined problem

00:04:11,920 --> 00:04:15,760
and then you're going to gather your

00:04:13,200 --> 00:04:17,600
relevant data um you'll do some feature

00:04:15,760 --> 00:04:19,199
engineering to make sure you know what

00:04:17,600 --> 00:04:21,199
you know is all inside of it

00:04:19,199 --> 00:04:22,880
maybe put it into a workable state

00:04:21,199 --> 00:04:24,639
you'll do some model tuning

00:04:22,880 --> 00:04:26,240
and then we'll validate the model and

00:04:24,639 --> 00:04:29,120
we're going to take a pause at this

00:04:26,240 --> 00:04:30,479
model validation step because in your

00:04:29,120 --> 00:04:33,280
validation step

00:04:30,479 --> 00:04:34,160
you'll be looking at some sort of metric

00:04:33,280 --> 00:04:37,759
such as

00:04:34,160 --> 00:04:40,639
root mean squared error or accuracy

00:04:37,759 --> 00:04:42,080
or maybe precision or recall and these

00:04:40,639 --> 00:04:45,360
help you know that your model

00:04:42,080 --> 00:04:47,360
is performing well in our example

00:04:45,360 --> 00:04:49,199
we'd be making sure that the cat model

00:04:47,360 --> 00:04:50,560
can robustly tell the difference between

00:04:49,199 --> 00:04:53,199
cats and dogs

00:04:50,560 --> 00:04:55,360
or that the people who are getting loans

00:04:53,199 --> 00:04:58,000
are able to pay them off

00:04:55,360 --> 00:05:00,000
after tuning you'll deploy your model

00:04:58,000 --> 00:05:02,960
and continue to look at it to ensure

00:05:00,000 --> 00:05:05,199
that it continues to perform as expected

00:05:02,960 --> 00:05:06,639
so there's no data drift or any weird

00:05:05,199 --> 00:05:08,800
strange things happening in the model

00:05:06,639 --> 00:05:10,320
once it's out in production

00:05:08,800 --> 00:05:12,560
so this seems like a pretty you know

00:05:10,320 --> 00:05:14,080
well-rounded end-to-end machine learning

00:05:12,560 --> 00:05:17,360
workflow

00:05:14,080 --> 00:05:20,560
where does explainability fit in

00:05:17,360 --> 00:05:23,360
and in general explainability happens

00:05:20,560 --> 00:05:25,120
after a model has been validated you can

00:05:23,360 --> 00:05:26,880
see there's really arrows all over

00:05:25,120 --> 00:05:28,320
um this is a process that's repeated

00:05:26,880 --> 00:05:30,320
many times you may have to go back to

00:05:28,320 --> 00:05:31,120
feature engineering model training model

00:05:30,320 --> 00:05:33,840
tuning

00:05:31,120 --> 00:05:35,600
revalidate the model but you have to

00:05:33,840 --> 00:05:38,320
start with a model that is

00:05:35,600 --> 00:05:40,560
tuned and validated in order to have

00:05:38,320 --> 00:05:42,560
useful explanations

00:05:40,560 --> 00:05:44,560
i want to be very careful here because i

00:05:42,560 --> 00:05:46,479
don't want people to think

00:05:44,560 --> 00:05:47,680
that you can add an explainability

00:05:46,479 --> 00:05:49,280
algorithm

00:05:47,680 --> 00:05:50,880
rather than making a thoughtfully

00:05:49,280 --> 00:05:52,320
engineered model

00:05:50,880 --> 00:05:54,160
you may have heard that if you put

00:05:52,320 --> 00:05:57,360
garbage data into a

00:05:54,160 --> 00:06:00,240
good model you just have a garbage model

00:05:57,360 --> 00:06:02,240
and the same principles apply here a bad

00:06:00,240 --> 00:06:03,680
model will never give actionable

00:06:02,240 --> 00:06:06,800
explanations

00:06:03,680 --> 00:06:08,639
model explainability does not fix poorly

00:06:06,800 --> 00:06:11,120
engineered models

00:06:08,639 --> 00:06:12,639
it simply explains why decisions are

00:06:11,120 --> 00:06:14,479
being made

00:06:12,639 --> 00:06:16,400
but let's work it let's look at this

00:06:14,479 --> 00:06:18,560
workflow step by step

00:06:16,400 --> 00:06:20,319
and we can see why models might need

00:06:18,560 --> 00:06:23,680
some more exploration

00:06:20,319 --> 00:06:25,199
with explanation all right so we're all

00:06:23,680 --> 00:06:26,960
data scientists here today

00:06:25,199 --> 00:06:29,759
and this talk we have our data science

00:06:26,960 --> 00:06:31,759
hat on and we're looking at our data

00:06:29,759 --> 00:06:33,520
we can kind of do a visual you know

00:06:31,759 --> 00:06:35,120
exploratory data analysis

00:06:33,520 --> 00:06:38,160
we see that the data is kind of all

00:06:35,120 --> 00:06:41,360
trending up and to the right

00:06:38,160 --> 00:06:42,560
so we've done our that we understand our

00:06:41,360 --> 00:06:44,400
data we can do some

00:06:42,560 --> 00:06:45,680
fake feature engineering looking at our

00:06:44,400 --> 00:06:47,280
data points here

00:06:45,680 --> 00:06:50,400
and so now we're at our step where we're

00:06:47,280 --> 00:06:53,440
deciding which model to use

00:06:50,400 --> 00:06:55,120
and let's start off with the simplest

00:06:53,440 --> 00:06:57,360
algorithm we can

00:06:55,120 --> 00:06:59,360
if you can recall your favorite middle

00:06:57,360 --> 00:07:02,240
school maths teacher

00:06:59,360 --> 00:07:03,120
hopefully y equals mx plus b rings a

00:07:02,240 --> 00:07:06,400
bell

00:07:03,120 --> 00:07:08,319
it's a it's a classic um i'm going to

00:07:06,400 --> 00:07:09,599
take a moment to give a very important

00:07:08,319 --> 00:07:11,840
reminder

00:07:09,599 --> 00:07:13,440
this is a reminder that machine learning

00:07:11,840 --> 00:07:16,880
is at its core

00:07:13,440 --> 00:07:19,599
just math sometimes we get so

00:07:16,880 --> 00:07:21,360
caught up with accuracy or whatever

00:07:19,599 --> 00:07:23,360
other evaluation metrics

00:07:21,360 --> 00:07:24,880
we forget what a model is and it's it's

00:07:23,360 --> 00:07:26,639
just math

00:07:24,880 --> 00:07:28,479
um we can look at the options for

00:07:26,639 --> 00:07:31,039
fitting this model and our

00:07:28,479 --> 00:07:33,039
thoughtfully engineered solution is the

00:07:31,039 --> 00:07:34,400
middle one

00:07:33,039 --> 00:07:35,919
so we've trained and tested our

00:07:34,400 --> 00:07:38,160
algorithm with our data and our

00:07:35,919 --> 00:07:38,400
algorithm is now our model y equals five

00:07:38,160 --> 00:07:40,639
x

00:07:38,400 --> 00:07:41,599
plus one and it's best suited for our

00:07:40,639 --> 00:07:43,360
data set

00:07:41,599 --> 00:07:44,879
and we know that it's best suited for

00:07:43,360 --> 00:07:48,560
our data set because

00:07:44,879 --> 00:07:50,720
on a very basic level it minimized the

00:07:48,560 --> 00:07:51,840
average distance between each data point

00:07:50,720 --> 00:07:53,680
and the line

00:07:51,840 --> 00:07:56,000
it was the line of best fit it kind of

00:07:53,680 --> 00:07:58,400
goes in the middle of everything

00:07:56,000 --> 00:07:59,520
we did a good job and this is math we

00:07:58,400 --> 00:08:01,680
can interpret

00:07:59,520 --> 00:08:02,800
we can confidently look at our model and

00:08:01,680 --> 00:08:05,280
see that this

00:08:02,800 --> 00:08:05,919
is the best one and we can see why it's

00:08:05,280 --> 00:08:08,080
the best

00:08:05,919 --> 00:08:09,120
and we understand what happens to our

00:08:08,080 --> 00:08:12,000
prediction

00:08:09,120 --> 00:08:14,000
if we have different inputs we know that

00:08:12,000 --> 00:08:14,800
if we test a data point that's further

00:08:14,000 --> 00:08:15,919
to the right

00:08:14,800 --> 00:08:18,319
we're probably going to be getting a

00:08:15,919 --> 00:08:20,000
higher prediction and this is intuitive

00:08:18,319 --> 00:08:21,919
and it makes sense especially when we

00:08:20,000 --> 00:08:24,400
have the graph right in front of us

00:08:21,919 --> 00:08:26,240
and it's a great example of a very

00:08:24,400 --> 00:08:28,240
interpretable model

00:08:26,240 --> 00:08:30,479
that doesn't need an explainability

00:08:28,240 --> 00:08:31,759
algorithm for you to understand why it's

00:08:30,479 --> 00:08:33,360
making decisions

00:08:31,759 --> 00:08:35,440
we can give ourselves all a pat on the

00:08:33,360 --> 00:08:36,320
back of being superstar data scientists

00:08:35,440 --> 00:08:39,039
right now

00:08:36,320 --> 00:08:40,479
because we 100 understand why this model

00:08:39,039 --> 00:08:42,399
is giving the output

00:08:40,479 --> 00:08:44,480
um and how to manipulate the inputs as

00:08:42,399 --> 00:08:46,560
well and this is a really great

00:08:44,480 --> 00:08:48,320
incredible moment and a win for us all

00:08:46,560 --> 00:08:50,640
as data scientists

00:08:48,320 --> 00:08:51,760
until you realize that models really

00:08:50,640 --> 00:08:55,440
don't look like

00:08:51,760 --> 00:08:58,000
y equals five plus 1 in production

00:08:55,440 --> 00:08:58,560
they might be part of a larger pipeline

00:08:58,000 --> 00:09:01,040
or

00:08:58,560 --> 00:09:03,200
maybe they're just too complex to be

00:09:01,040 --> 00:09:05,440
understood at a glance

00:09:03,200 --> 00:09:06,560
as our data sets grow in size and

00:09:05,440 --> 00:09:10,480
complexity

00:09:06,560 --> 00:09:12,800
so does the math that makes up our model

00:09:10,480 --> 00:09:13,519
and when this math is no longer as

00:09:12,800 --> 00:09:16,560
simple as

00:09:13,519 --> 00:09:19,279
y equals mx plus b it can become

00:09:16,560 --> 00:09:20,720
difficult to interpret and explain how

00:09:19,279 --> 00:09:23,519
our model is working

00:09:20,720 --> 00:09:24,640
and why it's making decisions and there

00:09:23,519 --> 00:09:26,480
are many different

00:09:24,640 --> 00:09:27,920
types of problems and many different

00:09:26,480 --> 00:09:29,279
types of models

00:09:27,920 --> 00:09:31,440
that can really benefit from

00:09:29,279 --> 00:09:33,680
explainability however

00:09:31,440 --> 00:09:36,560
there is one type of model that

00:09:33,680 --> 00:09:39,040
especially benefits and that's your

00:09:36,560 --> 00:09:41,680
black box models

00:09:39,040 --> 00:09:43,839
in fact this is probably the most

00:09:41,680 --> 00:09:45,519
important time to use explainability

00:09:43,839 --> 00:09:48,080
algorithms

00:09:45,519 --> 00:09:50,640
so black box models are models where you

00:09:48,080 --> 00:09:51,519
don't explicitly see how the variables

00:09:50,640 --> 00:09:53,200
of the model are

00:09:51,519 --> 00:09:54,800
interacting with each other just being

00:09:53,200 --> 00:09:57,120
used in general

00:09:54,800 --> 00:09:58,880
they're fairly commonplace in industry

00:09:57,120 --> 00:10:01,839
since they're quite easy to implement

00:09:58,880 --> 00:10:03,600
and algorithms exist for nearly every

00:10:01,839 --> 00:10:04,800
type of machine learning problem that's

00:10:03,600 --> 00:10:07,360
needed to be solved

00:10:04,800 --> 00:10:07,920
and they're fairly easy to implement but

00:10:07,360 --> 00:10:10,720
they can

00:10:07,920 --> 00:10:12,480
also be very dangerous and i'll repeat

00:10:10,720 --> 00:10:14,000
that again to make sure it sits in black

00:10:12,480 --> 00:10:17,120
box models can be

00:10:14,000 --> 00:10:19,680
very dangerous

00:10:17,120 --> 00:10:21,839
when you don't know exactly how the

00:10:19,680 --> 00:10:24,240
model is creating predictions

00:10:21,839 --> 00:10:24,880
and you're solely relying on evaluation

00:10:24,240 --> 00:10:27,680
metrics

00:10:24,880 --> 00:10:29,279
like accuracy to determine if your model

00:10:27,680 --> 00:10:31,600
is doing well or not

00:10:29,279 --> 00:10:34,480
it can be easy to have unintended

00:10:31,600 --> 00:10:34,480
consequences

00:10:34,640 --> 00:10:38,320
some common household names for black

00:10:37,040 --> 00:10:41,360
box models are

00:10:38,320 --> 00:10:44,320
neural networks or gradient boosts or

00:10:41,360 --> 00:10:45,200
ensembles these models give very high

00:10:44,320 --> 00:10:46,959
accuracy

00:10:45,200 --> 00:10:49,279
at the expense of not knowing their

00:10:46,959 --> 00:10:51,279
inner machinations

00:10:49,279 --> 00:10:52,480
beyond your naturally occurring black

00:10:51,279 --> 00:10:54,640
box models

00:10:52,480 --> 00:10:55,760
even some models that are technically

00:10:54,640 --> 00:10:58,560
interpretable

00:10:55,760 --> 00:10:59,200
are treated as black boxes and that's

00:10:58,560 --> 00:11:01,440
because

00:10:59,200 --> 00:11:02,560
no data scientist is looking at you know

00:11:01,440 --> 00:11:05,519
a thousand different

00:11:02,560 --> 00:11:05,920
inputs into a random forest model even

00:11:05,519 --> 00:11:08,880
though

00:11:05,920 --> 00:11:09,279
decision trees are classically labeled

00:11:08,880 --> 00:11:12,160
as

00:11:09,279 --> 00:11:14,640
interpretable models our problems and

00:11:12,160 --> 00:11:17,200
our data are getting more complex

00:11:14,640 --> 00:11:18,959
and data scientists have to rely heavily

00:11:17,200 --> 00:11:22,959
on evaluation metrics

00:11:18,959 --> 00:11:25,200
such as our accuracy for model building

00:11:22,959 --> 00:11:28,000
and we're once again reminded that a

00:11:25,200 --> 00:11:29,680
model and a human have different goals

00:11:28,000 --> 00:11:32,079
models want to optimize their math

00:11:29,680 --> 00:11:33,680
problem humans want to understand their

00:11:32,079 --> 00:11:36,000
human problem

00:11:33,680 --> 00:11:38,000
for some industries and especially

00:11:36,000 --> 00:11:41,040
highly regulated industries

00:11:38,000 --> 00:11:41,360
such as healthcare or financials it's

00:11:41,040 --> 00:11:44,160
not

00:11:41,360 --> 00:11:45,040
enough to just get the prediction the

00:11:44,160 --> 00:11:47,440
model must

00:11:45,040 --> 00:11:49,200
also justify how it came to the

00:11:47,440 --> 00:11:52,240
prediction

00:11:49,200 --> 00:11:53,760
explanations can help with this machine

00:11:52,240 --> 00:11:56,720
learning models can only be

00:11:53,760 --> 00:11:57,760
audited when they can be interpreted

00:11:56,720 --> 00:12:00,880
explanations

00:11:57,760 --> 00:12:00,880
can help with this too

00:12:01,279 --> 00:12:06,480
so to recap up until now we know that

00:12:04,480 --> 00:12:07,920
machine learning models have a different

00:12:06,480 --> 00:12:10,880
goal than humans

00:12:07,920 --> 00:12:12,880
we know that explainability comes after

00:12:10,880 --> 00:12:15,360
thoughtfully engineered models

00:12:12,880 --> 00:12:16,240
and we know that we need explainability

00:12:15,360 --> 00:12:19,839
because of the

00:12:16,240 --> 00:12:23,200
increasing complexity of models and the

00:12:19,839 --> 00:12:24,959
increasing use of black box models

00:12:23,200 --> 00:12:26,399
so i'm not an explainability sales

00:12:24,959 --> 00:12:26,880
person even though it might sound like

00:12:26,399 --> 00:12:29,040
it

00:12:26,880 --> 00:12:30,800
and i also want to make sure that you

00:12:29,040 --> 00:12:33,440
guys understand or you all

00:12:30,800 --> 00:12:34,240
understand that you do not need these

00:12:33,440 --> 00:12:37,279
every time

00:12:34,240 --> 00:12:37,279
you create a model

00:12:37,360 --> 00:12:40,959
first of all not all models need

00:12:39,839 --> 00:12:43,279
explainability

00:12:40,959 --> 00:12:46,079
some models are simple we saw this a few

00:12:43,279 --> 00:12:49,120
slides back in y equals mx plus b

00:12:46,079 --> 00:12:52,079
we don't need to add a compu complicated

00:12:49,120 --> 00:12:53,120
computational effort if it's just not

00:12:52,079 --> 00:12:54,880
needed

00:12:53,120 --> 00:12:57,120
if we have really robust future

00:12:54,880 --> 00:13:00,240
engineering or if we have very

00:12:57,120 --> 00:13:02,240
um in-depth subject matter expertise

00:13:00,240 --> 00:13:03,279
or if we just have a model that we can

00:13:02,240 --> 00:13:06,399
fully explain

00:13:03,279 --> 00:13:09,440
it's not worth our time

00:13:06,399 --> 00:13:12,560
there's another limitation in use cases

00:13:09,440 --> 00:13:16,000
not all use cases need explainability um

00:13:12,560 --> 00:13:19,360
in all honesty sometimes you just

00:13:16,000 --> 00:13:21,279
don't care um this is most noticeable in

00:13:19,360 --> 00:13:23,760
some types of forecasting

00:13:21,279 --> 00:13:25,279
or times where the output just isn't

00:13:23,760 --> 00:13:27,680
really important enough

00:13:25,279 --> 00:13:29,279
to justify the added complexity of these

00:13:27,680 --> 00:13:31,440
algorithms

00:13:29,279 --> 00:13:34,320
additionally there's some use cases

00:13:31,440 --> 00:13:36,720
where the problem is very well studied

00:13:34,320 --> 00:13:37,839
and feature interactions are just

00:13:36,720 --> 00:13:39,680
overall

00:13:37,839 --> 00:13:43,360
really well understood and so

00:13:39,680 --> 00:13:43,360
explainability isn't needed there

00:13:44,880 --> 00:13:48,000
finally you need to set very healthy

00:13:47,279 --> 00:13:50,720
boundaries

00:13:48,000 --> 00:13:51,920
with your explainability algorithms

00:13:50,720 --> 00:13:54,320
explanations help

00:13:51,920 --> 00:13:56,120
when you need to justify how a model

00:13:54,320 --> 00:14:00,079
came to a prediction

00:13:56,120 --> 00:14:05,519
explanations can expose bias

00:14:00,079 --> 00:14:05,519
explanations cannot fix biased models

00:14:05,600 --> 00:14:09,839
they do not make changes to the

00:14:07,440 --> 00:14:11,519
underlying model or data

00:14:09,839 --> 00:14:15,600
so you need to really remember what

00:14:11,519 --> 00:14:17,120
explainability can and cannot do for you

00:14:15,600 --> 00:14:19,120
all right we've gone through our fine

00:14:17,120 --> 00:14:21,839
print for the limits of explainability

00:14:19,120 --> 00:14:23,839
and it's time to look at an application

00:14:21,839 --> 00:14:25,040
as a disclaimer um i don't work for a

00:14:23,839 --> 00:14:27,839
healthcare company

00:14:25,040 --> 00:14:29,760
and i really don't claim to be giving

00:14:27,839 --> 00:14:31,760
sound medical advice

00:14:29,760 --> 00:14:33,440
but in my free time i'm really

00:14:31,760 --> 00:14:35,279
interested in the world of health care

00:14:33,440 --> 00:14:36,720
and the hospital ecosystem

00:14:35,279 --> 00:14:38,959
i think they have really interesting

00:14:36,720 --> 00:14:40,880
problems that can really benefit from

00:14:38,959 --> 00:14:43,040
machine learning especially

00:14:40,880 --> 00:14:44,240
so i looked at this openly available

00:14:43,040 --> 00:14:46,880
sample data from

00:14:44,240 --> 00:14:49,120
mimic which is de-identified patient

00:14:46,880 --> 00:14:50,240
data from a real hospital in the united

00:14:49,120 --> 00:14:52,800
states

00:14:50,240 --> 00:14:53,959
i was particularly interested in better

00:14:52,800 --> 00:14:56,800
understanding

00:14:53,959 --> 00:14:57,199
healthcare-acquired infections and these

00:14:56,800 --> 00:14:59,120
are all

00:14:57,199 --> 00:15:00,240
infections that patients are not

00:14:59,120 --> 00:15:01,680
admitted with

00:15:00,240 --> 00:15:03,600
but they contract from being in the

00:15:01,680 --> 00:15:05,040
healthcare environment you know just

00:15:03,600 --> 00:15:08,320
being in a hospital room and all the

00:15:05,040 --> 00:15:10,639
bacteria that's moving around in there

00:15:08,320 --> 00:15:13,120
and i'm focusing in on one healthcare

00:15:10,639 --> 00:15:15,680
acquired infection called mrsa

00:15:13,120 --> 00:15:17,279
this data has a lot of inputs on patient

00:15:15,680 --> 00:15:21,519
demographics such as

00:15:17,279 --> 00:15:24,560
age gender ethnicity as well as medical

00:15:21,519 --> 00:15:27,839
information such as symptoms

00:15:24,560 --> 00:15:30,240
diagnosis length of stay i did a pretty

00:15:27,839 --> 00:15:31,519
vanilla feature engineering for this and

00:15:30,240 --> 00:15:34,079
i made my model

00:15:31,519 --> 00:15:35,440
it's a support vector classifier and

00:15:34,079 --> 00:15:36,639
even though that the data set is

00:15:35,440 --> 00:15:39,759
unbalanced it's for

00:15:36,639 --> 00:15:43,279
it's about 90 accuracy

00:15:39,759 --> 00:15:46,320
90 accurate for predicting mrsa

00:15:43,279 --> 00:15:46,959
um and we can say that my model is

00:15:46,320 --> 00:15:49,680
performing

00:15:46,959 --> 00:15:51,519
fairly well and it could be you know in

00:15:49,680 --> 00:15:53,279
theory ready to be in production

00:15:51,519 --> 00:15:56,240
putting in production in quotes because

00:15:53,279 --> 00:15:59,360
this is a fake in production moment

00:15:56,240 --> 00:16:00,079
um it's not a perfect example of a very

00:15:59,360 --> 00:16:03,519
big

00:16:00,079 --> 00:16:06,800
or highly complex data set or model

00:16:03,519 --> 00:16:09,279
but this is our pseudo black box

00:16:06,800 --> 00:16:10,959
the inputs are very wide and i can't see

00:16:09,279 --> 00:16:12,240
exactly how the model is making these

00:16:10,959 --> 00:16:15,199
decisions

00:16:12,240 --> 00:16:17,680
so it's a good contender for my actual

00:16:15,199 --> 00:16:20,079
absolute favorite use of explainers

00:16:17,680 --> 00:16:22,560
which is the i'm just curious to see

00:16:20,079 --> 00:16:24,800
what's happening here usage curiosity

00:16:22,560 --> 00:16:27,120
can get you quite far

00:16:24,800 --> 00:16:29,199
my next step in this problem is to look

00:16:27,120 --> 00:16:30,959
at the contending algorithms

00:16:29,199 --> 00:16:32,240
for explaining the prediction of these

00:16:30,959 --> 00:16:34,399
infections

00:16:32,240 --> 00:16:37,040
i do have to claim that all the examples

00:16:34,399 --> 00:16:39,519
i use in the next slides are fictitious

00:16:37,040 --> 00:16:41,120
and completely hypothetical but i want

00:16:39,519 --> 00:16:41,680
to look at this model at a few different

00:16:41,120 --> 00:16:44,800
levels

00:16:41,680 --> 00:16:46,000
levels and the first level is at a

00:16:44,800 --> 00:16:48,639
global scope

00:16:46,000 --> 00:16:49,360
this is kind of your 5 000 foot view of

00:16:48,639 --> 00:16:52,399
your

00:16:49,360 --> 00:16:54,240
model understanding how the model itself

00:16:52,399 --> 00:16:56,800
is making decisions

00:16:54,240 --> 00:16:58,639
it can answer questions such as which

00:16:56,800 --> 00:17:01,920
features are important and what's the

00:16:58,639 --> 00:17:04,959
interaction between these features

00:17:01,920 --> 00:17:08,000
next we have our local scope so

00:17:04,959 --> 00:17:08,400
this level of explainability is all

00:17:08,000 --> 00:17:11,199
about

00:17:08,400 --> 00:17:12,160
understanding how the model has made a

00:17:11,199 --> 00:17:15,280
decision

00:17:12,160 --> 00:17:15,839
for a particular instance there are many

00:17:15,280 --> 00:17:19,280
more

00:17:15,839 --> 00:17:21,520
local explainers than global explainers

00:17:19,280 --> 00:17:22,319
and that's partially because global

00:17:21,520 --> 00:17:25,439
explainers

00:17:22,319 --> 00:17:26,640
are very computationally complex and

00:17:25,439 --> 00:17:30,400
therefore very

00:17:26,640 --> 00:17:32,640
time consuming and very expensive to run

00:17:30,400 --> 00:17:33,440
the lack of global explainers also

00:17:32,640 --> 00:17:36,720
exists

00:17:33,440 --> 00:17:38,400
because not all instances have the same

00:17:36,720 --> 00:17:40,640
weights for each feature

00:17:38,400 --> 00:17:42,880
so we can't generalize individual

00:17:40,640 --> 00:17:44,720
explanations to the whole model

00:17:42,880 --> 00:17:46,559
and that's kind of a mouthful to wrap

00:17:44,720 --> 00:17:49,360
your brain around so maybe a

00:17:46,559 --> 00:17:51,280
clearer way of saying this is in our

00:17:49,360 --> 00:17:52,960
healthcare frame of mind

00:17:51,280 --> 00:17:55,760
not all patients come in with the same

00:17:52,960 --> 00:17:57,520
symptoms you wouldn't want to generalize

00:17:55,760 --> 00:17:58,960
the symptoms of one patient to the

00:17:57,520 --> 00:18:00,480
entire hospital

00:17:58,960 --> 00:18:03,600
since you would lose your ability to

00:18:00,480 --> 00:18:06,160
have a unique and accurate diagnosis

00:18:03,600 --> 00:18:07,760
the same idea applies here you want to

00:18:06,160 --> 00:18:10,240
diagnose a particular

00:18:07,760 --> 00:18:10,799
instance and hope that looking at the

00:18:10,240 --> 00:18:13,039
smaller

00:18:10,799 --> 00:18:14,240
area will give better results and

00:18:13,039 --> 00:18:17,440
explanations

00:18:14,240 --> 00:18:18,799
to other similar data points that you

00:18:17,440 --> 00:18:20,960
have

00:18:18,799 --> 00:18:23,200
local explainers can answer questions

00:18:20,960 --> 00:18:25,039
such as you know which features

00:18:23,200 --> 00:18:26,640
factored most heavily into this

00:18:25,039 --> 00:18:29,440
classification

00:18:26,640 --> 00:18:32,559
and how would the prediction change if

00:18:29,440 --> 00:18:35,600
certain features changed

00:18:32,559 --> 00:18:37,679
so i'll first look at um some or a

00:18:35,600 --> 00:18:39,520
global explainability algorithm

00:18:37,679 --> 00:18:41,520
as a reminder this is your super high

00:18:39,520 --> 00:18:45,360
level overview of how your model

00:18:41,520 --> 00:18:47,200
is giving predictions and the first

00:18:45,360 --> 00:18:50,799
uh algorithm we're looking at is called

00:18:47,200 --> 00:18:55,120
shapley and the goal of shapley is to

00:18:50,799 --> 00:18:56,000
explain the um or i guess contribute the

00:18:55,120 --> 00:18:59,200
contribution

00:18:56,000 --> 00:19:01,120
of each feature to the model so if we

00:18:59,200 --> 00:19:02,880
were looking at this graph here

00:19:01,120 --> 00:19:04,799
your shapley algorithm would give the

00:19:02,880 --> 00:19:07,600
output that um

00:19:04,799 --> 00:19:08,720
globally higher y values are light

00:19:07,600 --> 00:19:13,200
purple

00:19:08,720 --> 00:19:16,400
and higher x values are dark purple

00:19:13,200 --> 00:19:18,640
in the very simplest terms shapley tells

00:19:16,400 --> 00:19:22,160
me what inputs are important

00:19:18,640 --> 00:19:23,520
and how important they are

00:19:22,160 --> 00:19:25,600
so there's some pros and cons to

00:19:23,520 --> 00:19:28,080
shackley and the first

00:19:25,600 --> 00:19:30,320
pro that is really important is it's one

00:19:28,080 --> 00:19:31,360
of the few algorithms that can be used

00:19:30,320 --> 00:19:33,520
globally

00:19:31,360 --> 00:19:35,039
um again not many algorithms are able to

00:19:33,520 --> 00:19:37,600
be global

00:19:35,039 --> 00:19:38,559
uh there are there is kind of like a

00:19:37,600 --> 00:19:41,919
hack because

00:19:38,559 --> 00:19:43,600
tree based models are kind of fast but

00:19:41,919 --> 00:19:45,360
if you're not wandering through a random

00:19:43,600 --> 00:19:46,799
forest it's going to take a long time to

00:19:45,360 --> 00:19:49,679
train this

00:19:46,799 --> 00:19:50,000
so i could see myself using shapley to

00:19:49,679 --> 00:19:52,640
ask

00:19:50,000 --> 00:19:54,960
questions to my model such as what are

00:19:52,640 --> 00:19:56,960
the most important features my model is

00:19:54,960 --> 00:20:00,160
using to predict mrsa

00:19:56,960 --> 00:20:03,919
could it be age location of admission or

00:20:00,160 --> 00:20:06,440
some sort of specific diagnosis

00:20:03,919 --> 00:20:08,960
so if i'm looking for a less

00:20:06,440 --> 00:20:11,840
computationally expensive

00:20:08,960 --> 00:20:12,960
um explainability algorithm i might be

00:20:11,840 --> 00:20:16,400
looking into more

00:20:12,960 --> 00:20:19,039
local algorithms and

00:20:16,400 --> 00:20:20,000
we'll look at a few different algorithms

00:20:19,039 --> 00:20:23,120
but we're going to

00:20:20,000 --> 00:20:25,600
start at and surprise yay

00:20:23,120 --> 00:20:26,799
um shapley values can also be used

00:20:25,600 --> 00:20:29,360
locally

00:20:26,799 --> 00:20:31,679
the benefit of this approach is that

00:20:29,360 --> 00:20:32,960
local versions of this algorithm are a

00:20:31,679 --> 00:20:36,320
lot quicker

00:20:32,960 --> 00:20:38,480
but the negative side hopefully

00:20:36,320 --> 00:20:40,000
um is that this is the number one

00:20:38,480 --> 00:20:43,200
favorite explainer for

00:20:40,000 --> 00:20:43,600
our evil data scientists research has

00:20:43,200 --> 00:20:46,799
shown

00:20:43,600 --> 00:20:50,240
that is possible to hide bias

00:20:46,799 --> 00:20:52,799
in shapley with adversarial attacks

00:20:50,240 --> 00:20:53,520
this really isn't a big concern for

00:20:52,799 --> 00:20:55,520
those who

00:20:53,520 --> 00:20:56,799
are attempting to use this algorithm

00:20:55,520 --> 00:20:59,120
truthfully

00:20:56,799 --> 00:21:00,320
but it could cause mistrust with

00:20:59,120 --> 00:21:02,559
interpreters

00:21:00,320 --> 00:21:04,080
of shop explanations you'd always have

00:21:02,559 --> 00:21:05,120
to walk into looking at a shackly

00:21:04,080 --> 00:21:06,720
explanation

00:21:05,120 --> 00:21:09,120
with the possibility that it's fake in

00:21:06,720 --> 00:21:11,200
the back of your head

00:21:09,120 --> 00:21:13,280
so whereas for the global shapley

00:21:11,200 --> 00:21:14,240
explainer i would see what features were

00:21:13,280 --> 00:21:16,400
most important

00:21:14,240 --> 00:21:17,919
for the model in general or like the

00:21:16,400 --> 00:21:20,080
hospital in general

00:21:17,919 --> 00:21:21,919
i can use local shaft to see what

00:21:20,080 --> 00:21:25,200
features were most important

00:21:21,919 --> 00:21:26,799
for a specific patient to be flagged as

00:21:25,200 --> 00:21:30,000
someone who would likely become

00:21:26,799 --> 00:21:32,080
versa positive and again

00:21:30,000 --> 00:21:34,960
shapley is still kind of expensive to

00:21:32,080 --> 00:21:37,520
run it's still computationally

00:21:34,960 --> 00:21:39,919
robust so for those looking for a less

00:21:37,520 --> 00:21:43,039
computationally expensive technique

00:21:39,919 --> 00:21:45,840
uh local interpretable model agnostic

00:21:43,039 --> 00:21:47,600
explanations and that is such a mouthful

00:21:45,840 --> 00:21:48,640
you understand why they shorten it to

00:21:47,600 --> 00:21:51,360
lime

00:21:48,640 --> 00:21:52,559
um is an algorithm where you assume

00:21:51,360 --> 00:21:54,799
linearity

00:21:52,559 --> 00:21:57,200
around a particular instance and create

00:21:54,799 --> 00:22:00,000
a simpler model

00:21:57,200 --> 00:22:00,640
the key intuition behind this one is

00:22:00,000 --> 00:22:03,360
that

00:22:00,640 --> 00:22:04,559
it's much easier to approximate a black

00:22:03,360 --> 00:22:07,039
box model

00:22:04,559 --> 00:22:08,480
by using a simple model locally in the

00:22:07,039 --> 00:22:09,760
neighborhood of the prediction we want

00:22:08,480 --> 00:22:12,320
to explain

00:22:09,760 --> 00:22:13,440
as opposed to trying to approximate the

00:22:12,320 --> 00:22:17,440
entire model

00:22:13,440 --> 00:22:18,480
globally so it creates a local version

00:22:17,440 --> 00:22:21,200
of a linear model

00:22:18,480 --> 00:22:22,880
because y equals mx plus b is the gift

00:22:21,200 --> 00:22:26,159
that your middle school teacher

00:22:22,880 --> 00:22:28,000
just keeps giving in this case it's our

00:22:26,159 --> 00:22:31,120
middle purple line

00:22:28,000 --> 00:22:33,600
we might not have any idea of the

00:22:31,120 --> 00:22:34,799
equation for our red line but we

00:22:33,600 --> 00:22:37,520
understand and can

00:22:34,799 --> 00:22:39,039
easily interpret y equals five x plus

00:22:37,520 --> 00:22:42,480
one

00:22:39,039 --> 00:22:45,120
um and this is really a great moment for

00:22:42,480 --> 00:22:46,159
uh local explainability to be able to

00:22:45,120 --> 00:22:49,280
interpret this

00:22:46,159 --> 00:22:50,480
instance lime is also model agnostic

00:22:49,280 --> 00:22:52,799
so it doesn't matter what your

00:22:50,480 --> 00:22:54,000
underlying model is as long as you have

00:22:52,799 --> 00:22:57,039
a prediction function

00:22:54,000 --> 00:23:00,240
lime can be used however

00:22:57,039 --> 00:23:02,080
there is some danger in using lime

00:23:00,240 --> 00:23:03,760
because you don't know how far this

00:23:02,080 --> 00:23:06,080
explanation holds

00:23:03,760 --> 00:23:07,440
we can see that there's end points in

00:23:06,080 --> 00:23:09,679
this image

00:23:07,440 --> 00:23:11,919
but the fit is not always as

00:23:09,679 --> 00:23:15,280
straightforward as this graph may

00:23:11,919 --> 00:23:17,760
lead you to believe there is fear that

00:23:15,280 --> 00:23:19,679
um you can be over confident in your

00:23:17,760 --> 00:23:21,039
explanations in line

00:23:19,679 --> 00:23:22,880
and you could have misleading

00:23:21,039 --> 00:23:27,360
conclusions for

00:23:22,880 --> 00:23:27,360
unseen but similar instances

00:23:28,000 --> 00:23:33,760
anchors actually work a lot like lyme um

00:23:31,120 --> 00:23:36,000
they both proxy local behavior of the

00:23:33,760 --> 00:23:38,480
model in a linear way

00:23:36,000 --> 00:23:39,360
but remember that line broke because we

00:23:38,480 --> 00:23:42,799
didn't know

00:23:39,360 --> 00:23:45,600
to what extent the explanation held up

00:23:42,799 --> 00:23:46,080
and anchor kind of steps up to the plate

00:23:45,600 --> 00:23:48,960
and

00:23:46,080 --> 00:23:50,000
incorporates coverage i like to think of

00:23:48,960 --> 00:23:53,120
this coverage as

00:23:50,000 --> 00:23:57,600
going from a one-dimensional line

00:23:53,120 --> 00:24:00,000
in lime to a two-dimensional area

00:23:57,600 --> 00:24:00,640
which we can see since our small purple

00:24:00,000 --> 00:24:03,440
line

00:24:00,640 --> 00:24:04,480
has turned into a more robust rectangle

00:24:03,440 --> 00:24:07,919
that's implementing

00:24:04,480 --> 00:24:11,039
coverage anchors explain

00:24:07,919 --> 00:24:12,880
individual predictions of any black box

00:24:11,039 --> 00:24:15,679
classification model

00:24:12,880 --> 00:24:16,880
by finding a decision rule or set of

00:24:15,679 --> 00:24:20,080
features

00:24:16,880 --> 00:24:22,240
or range of features that anchors

00:24:20,080 --> 00:24:23,520
the prediction sufficiently so

00:24:22,240 --> 00:24:26,640
everything that's within

00:24:23,520 --> 00:24:29,760
this per light purple rectangle

00:24:26,640 --> 00:24:32,640
would be classified as light purple

00:24:29,760 --> 00:24:33,039
anchor's main selling point is that the

00:24:32,640 --> 00:24:35,760
they

00:24:33,039 --> 00:24:36,480
address the shortcomings of lime but the

00:24:35,760 --> 00:24:39,039
boundaries

00:24:36,480 --> 00:24:40,480
are still approximate and occasionally

00:24:39,039 --> 00:24:43,039
don't even exist

00:24:40,480 --> 00:24:44,000
in which case inkers become no different

00:24:43,039 --> 00:24:45,600
than lime

00:24:44,000 --> 00:24:47,120
um and they still carry the same pros

00:24:45,600 --> 00:24:50,320
and cons

00:24:47,120 --> 00:24:53,120
in the context of my mrsa exploration

00:24:50,320 --> 00:24:53,760
i could see myself using anchors to see

00:24:53,120 --> 00:24:57,039
what

00:24:53,760 --> 00:24:57,360
ranges of different features factor into

00:24:57,039 --> 00:25:00,480
a

00:24:57,360 --> 00:25:03,200
mrsa positive prediction for example

00:25:00,480 --> 00:25:04,000
an anchor might be able to tell me that

00:25:03,200 --> 00:25:07,600
um

00:25:04,000 --> 00:25:12,799
the output is like ages 20 to 30

00:25:07,600 --> 00:25:12,799
are at a 20 risk of risk of contraction

00:25:13,200 --> 00:25:19,520
so lime shop and anchors all look

00:25:16,640 --> 00:25:20,799
at what features are important and look

00:25:19,520 --> 00:25:23,279
at interpretable models

00:25:20,799 --> 00:25:25,440
for a particular data point but

00:25:23,279 --> 00:25:26,559
contrastive explanation methods kind of

00:25:25,440 --> 00:25:29,039
shift gears

00:25:26,559 --> 00:25:31,360
and focus more on cause and effect so

00:25:29,039 --> 00:25:31,679
you can test how your model output can

00:25:31,360 --> 00:25:35,120
be

00:25:31,679 --> 00:25:37,840
changed for each instance

00:25:35,120 --> 00:25:38,480
contrast of explanation methods focus on

00:25:37,840 --> 00:25:41,120
explaining

00:25:38,480 --> 00:25:43,679
instances in terms of pertinent

00:25:41,120 --> 00:25:46,320
positives and pertinent negatives

00:25:43,679 --> 00:25:47,200
pertinent positives refer to features

00:25:46,320 --> 00:25:49,360
that should be

00:25:47,200 --> 00:25:50,320
minimally and sufficiently present to

00:25:49,360 --> 00:25:53,440
predict

00:25:50,320 --> 00:25:56,559
the same class as the original instance

00:25:53,440 --> 00:25:57,360
and pertinent negatives identify which

00:25:56,559 --> 00:25:59,840
features

00:25:57,360 --> 00:26:00,880
should be minimally and necessarily

00:25:59,840 --> 00:26:03,760
absent

00:26:00,880 --> 00:26:05,440
from the instance in order to maintain

00:26:03,760 --> 00:26:07,600
the original prediction class

00:26:05,440 --> 00:26:09,520
kind of a mouthful but humans kind they

00:26:07,600 --> 00:26:11,600
really think in

00:26:09,520 --> 00:26:12,880
contrastive explanation methods

00:26:11,600 --> 00:26:14,480
naturally

00:26:12,880 --> 00:26:16,000
if i was trying to get my partner to

00:26:14,480 --> 00:26:19,120
locate me at a restaurant

00:26:16,000 --> 00:26:20,960
i might say that i have a hat on but i'm

00:26:19,120 --> 00:26:22,799
not wearing red

00:26:20,960 --> 00:26:24,880
if i wanted to have an explanation for

00:26:22,799 --> 00:26:27,840
this graph my cem

00:26:24,880 --> 00:26:28,240
algorithm would tell me that the star is

00:26:27,840 --> 00:26:31,760
light

00:26:28,240 --> 00:26:35,679
purple because y is greater than 10

00:26:31,760 --> 00:26:35,679
but x is less than 12.

00:26:36,159 --> 00:26:39,520
well cem is useful for human

00:26:38,400 --> 00:26:42,240
understanding

00:26:39,520 --> 00:26:43,600
they get less useful when the classes

00:26:42,240 --> 00:26:45,679
aren't similar

00:26:43,600 --> 00:26:46,720
and this is kind of intuitive you can

00:26:45,679 --> 00:26:49,919
distinguish

00:26:46,720 --> 00:26:51,279
clear yet subtle differences between a

00:26:49,919 --> 00:26:54,000
cat and a dog

00:26:51,279 --> 00:26:55,200
but the differences between a cat and

00:26:54,000 --> 00:26:57,120
the flu

00:26:55,200 --> 00:26:59,600
are so numerous that it doesn't even

00:26:57,120 --> 00:27:02,799
seem logical to list them all out

00:26:59,600 --> 00:27:04,400
if my cem output is that all people who

00:27:02,799 --> 00:27:07,120
contract mrsa

00:27:04,400 --> 00:27:08,320
are admitted through the emergency room

00:27:07,120 --> 00:27:10,960
but did not show

00:27:08,320 --> 00:27:13,120
signs of respiratory distress the

00:27:10,960 --> 00:27:15,039
location would be a pertinent positive

00:27:13,120 --> 00:27:17,520
and symptoms would be a pertinent

00:27:15,039 --> 00:27:17,520
negative

00:27:17,840 --> 00:27:23,039
counter factual explanation is the

00:27:20,159 --> 00:27:26,000
minimum possible change required

00:27:23,039 --> 00:27:28,399
to generate the desired output so kind

00:27:26,000 --> 00:27:29,919
of think if x had not occurred y would

00:27:28,399 --> 00:27:32,159
not have occurred

00:27:29,919 --> 00:27:33,039
in our example here if i wanted to

00:27:32,159 --> 00:27:35,440
change my

00:27:33,039 --> 00:27:36,480
light purple classification to dark

00:27:35,440 --> 00:27:38,960
purple

00:27:36,480 --> 00:27:39,919
the star would have had to move down at

00:27:38,960 --> 00:27:43,600
least

00:27:39,919 --> 00:27:46,320
that set amount on the positive side

00:27:43,600 --> 00:27:47,039
counter factuals don't require access to

00:27:46,320 --> 00:27:49,120
the data

00:27:47,039 --> 00:27:50,640
or even the model they only need the

00:27:49,120 --> 00:27:53,279
predict function to generate

00:27:50,640 --> 00:27:54,240
outputs but the cons for counter

00:27:53,279 --> 00:27:57,840
factuals are

00:27:54,240 --> 00:28:00,880
similar to cems oftentimes there are

00:27:57,840 --> 00:28:01,360
so many ways to change the output in

00:28:00,880 --> 00:28:02,640
fact

00:28:01,360 --> 00:28:04,480
there's so many ways to change the

00:28:02,640 --> 00:28:07,360
output that it's no longer useful

00:28:04,480 --> 00:28:08,240
to list all of them out a counter

00:28:07,360 --> 00:28:11,039
factual that

00:28:08,240 --> 00:28:12,080
i might uncover with a patient is maybe

00:28:11,039 --> 00:28:13,760
someone

00:28:12,080 --> 00:28:15,919
who was admitted through the emergency

00:28:13,760 --> 00:28:16,880
room would not have acquired an

00:28:15,919 --> 00:28:18,960
infection

00:28:16,880 --> 00:28:21,360
if they had entered through urgent care

00:28:18,960 --> 00:28:21,360
instead

00:28:21,760 --> 00:28:26,159
and that's just a few of the many

00:28:24,559 --> 00:28:28,799
possible algorithms

00:28:26,159 --> 00:28:30,159
but i'm ready to test a few out and i

00:28:28,799 --> 00:28:32,720
have just a small

00:28:30,159 --> 00:28:34,399
sample here of the many different open

00:28:32,720 --> 00:28:35,679
source python libraries that are

00:28:34,399 --> 00:28:37,919
available

00:28:35,679 --> 00:28:38,799
not all libraries have the functions we

00:28:37,919 --> 00:28:40,720
went over

00:28:38,799 --> 00:28:42,640
so it's good to do some research for

00:28:40,720 --> 00:28:44,399
what best suits your needs

00:28:42,640 --> 00:28:45,760
um if you see the link at the bottom

00:28:44,399 --> 00:28:49,039
there's a really cool

00:28:45,760 --> 00:28:51,600
uh github repo called ethical ml that

00:28:49,039 --> 00:28:54,240
has a giant list of all these different

00:28:51,600 --> 00:28:56,080
open source libraries you can use

00:28:54,240 --> 00:28:58,000
and today in my demo i'm going to be

00:28:56,080 --> 00:28:59,440
using alibi explain

00:28:58,000 --> 00:29:00,880
since it contains a handful of

00:28:59,440 --> 00:29:02,559
algorithms that i'm interested in

00:29:00,880 --> 00:29:06,559
implementing

00:29:02,559 --> 00:29:06,559
so let's hop over to my notebook

00:29:08,080 --> 00:29:12,159
and just to give you a little bit of

00:29:10,320 --> 00:29:14,880
background of where i'm at is

00:29:12,159 --> 00:29:15,279
i'm using something called operate first

00:29:14,880 --> 00:29:18,559
which

00:29:15,279 --> 00:29:22,240
is an open source production grade

00:29:18,559 --> 00:29:24,840
open shift environment so anyone who is

00:29:22,240 --> 00:29:26,000
wanting to type in

00:29:24,840 --> 00:29:27,440
odh.operatefirst.cloud

00:29:26,000 --> 00:29:29,520
you're able to pull up you know your

00:29:27,440 --> 00:29:30,159
jupiter notebooks and demo everything

00:29:29,520 --> 00:29:33,039
here

00:29:30,159 --> 00:29:34,880
you can clone my github repo and play

00:29:33,039 --> 00:29:37,279
with my code yourself

00:29:34,880 --> 00:29:37,919
and we can see here that i start out

00:29:37,279 --> 00:29:40,880
doing my

00:29:37,919 --> 00:29:43,200
super normal data science stuff i get to

00:29:40,880 --> 00:29:43,919
import my libraries split into train and

00:29:43,200 --> 00:29:46,799
test

00:29:43,919 --> 00:29:47,840
sets i'm loading my model i'm fitting my

00:29:46,799 --> 00:29:51,120
model

00:29:47,840 --> 00:29:52,480
um building predict functions and i'm

00:29:51,120 --> 00:29:55,919
going to be doing

00:29:52,480 --> 00:29:57,360
shop to start out with and it'll be the

00:29:55,919 --> 00:29:59,600
local version first

00:29:57,360 --> 00:30:00,720
i'm going to take a pause here and first

00:29:59,600 --> 00:30:03,360
of all it's my cause

00:30:00,720 --> 00:30:05,120
um call to action if you want to see if

00:30:03,360 --> 00:30:07,200
i'm an evil data scientist or not you

00:30:05,120 --> 00:30:09,679
can play with my code yourself

00:30:07,200 --> 00:30:12,399
but also we want to look at this i'm

00:30:09,679 --> 00:30:14,320
only using 108 data samples

00:30:12,399 --> 00:30:15,840
and it's already warning me about having

00:30:14,320 --> 00:30:18,799
slower run times

00:30:15,840 --> 00:30:19,679
shap is quite slow but it's fun to use

00:30:18,799 --> 00:30:22,720
and it's

00:30:19,679 --> 00:30:23,120
interesting to interpret so once i fit

00:30:22,720 --> 00:30:25,360
my

00:30:23,120 --> 00:30:26,799
shop explainer we see it's coming from a

00:30:25,360 --> 00:30:27,760
black box model

00:30:26,799 --> 00:30:30,799
it's going to be doing some

00:30:27,760 --> 00:30:33,520
classification and it'll be at a local

00:30:30,799 --> 00:30:35,039
and global level there's also some other

00:30:33,520 --> 00:30:36,399
parameters you're more than welcome to

00:30:35,039 --> 00:30:38,960
play around with if you

00:30:36,399 --> 00:30:39,520
desire and i'm not running this live

00:30:38,960 --> 00:30:42,720
again

00:30:39,520 --> 00:30:44,640
this is quite long to sit and watch

00:30:42,720 --> 00:30:46,720
load so we're going to jump right into

00:30:44,640 --> 00:30:49,039
the visualizations

00:30:46,720 --> 00:30:51,279
and this is our local visualization and

00:30:49,039 --> 00:30:54,320
how you would interpret this force plot

00:30:51,279 --> 00:30:57,360
is we started our base value

00:30:54,320 --> 00:30:58,000
if the final output is below the base

00:30:57,360 --> 00:31:00,559
value

00:30:58,000 --> 00:31:03,279
the patient would be mrsa negative if

00:31:00,559 --> 00:31:05,760
the final value was above the base value

00:31:03,279 --> 00:31:07,360
the patient is versa positive and we can

00:31:05,760 --> 00:31:10,240
see that here

00:31:07,360 --> 00:31:10,960
and each one of these bars is a feature

00:31:10,240 --> 00:31:13,120
and

00:31:10,960 --> 00:31:14,640
you can see there's uh the features are

00:31:13,120 --> 00:31:15,600
so small on this side it doesn't even

00:31:14,640 --> 00:31:17,200
give a name

00:31:15,600 --> 00:31:19,360
but the larger the bar the more

00:31:17,200 --> 00:31:22,799
important the feature was

00:31:19,360 --> 00:31:25,760
and we're going to see that for

00:31:22,799 --> 00:31:27,279
the classification of mrsa positive the

00:31:25,760 --> 00:31:31,440
most important features

00:31:27,279 --> 00:31:35,120
are ethnicity and admission location

00:31:31,440 --> 00:31:37,279
and ethnicity and gender

00:31:35,120 --> 00:31:38,320
and when i first ran this model i kind

00:31:37,279 --> 00:31:40,960
of had some

00:31:38,320 --> 00:31:41,360
data science red flags coming up i was

00:31:40,960 --> 00:31:44,480
like

00:31:41,360 --> 00:31:46,000
i had done pretty basic um feature

00:31:44,480 --> 00:31:49,679
engineering i hadn't done anything

00:31:46,000 --> 00:31:52,480
weird with my model so it didn't quite

00:31:49,679 --> 00:31:55,279
make sense why ethnicity and gender were

00:31:52,480 --> 00:31:56,880
three of the top four features for this

00:31:55,279 --> 00:31:58,960
model's output

00:31:56,880 --> 00:32:00,000
so i wanted to make sure that maybe this

00:31:58,960 --> 00:32:02,159
was a fluke maybe this

00:32:00,000 --> 00:32:05,120
is just the one patient i wanted to look

00:32:02,159 --> 00:32:05,120
at a global level

00:32:05,279 --> 00:32:09,440
and this is my shop output at a global

00:32:08,399 --> 00:32:12,480
level

00:32:09,440 --> 00:32:15,279
and we can see here that

00:32:12,480 --> 00:32:17,600
each one of the points is a particular

00:32:15,279 --> 00:32:20,640
instance or a particular patient

00:32:17,600 --> 00:32:22,640
uh these are all the top

00:32:20,640 --> 00:32:24,480
features the higher up on the list the

00:32:22,640 --> 00:32:27,039
more important the feature was

00:32:24,480 --> 00:32:28,320
for creating predictions um overall for

00:32:27,039 --> 00:32:30,960
the model

00:32:28,320 --> 00:32:33,760
and we see here that uh once again we

00:32:30,960 --> 00:32:36,480
have ethnicity gender and ethnicity

00:32:33,760 --> 00:32:37,600
and this was my accidental and now on

00:32:36,480 --> 00:32:39,519
purpose

00:32:37,600 --> 00:32:40,960
call to action to kind of check out your

00:32:39,519 --> 00:32:43,039
models every now and then

00:32:40,960 --> 00:32:45,120
because this was not what i had expected

00:32:43,039 --> 00:32:47,279
if i was in the healthcare world i might

00:32:45,120 --> 00:32:50,720
pull in a subject matter expert

00:32:47,279 --> 00:32:52,320
to see if this was right as a non-expert

00:32:50,720 --> 00:32:53,840
i wouldn't think that ethnicity and

00:32:52,320 --> 00:32:56,559
gender are the most

00:32:53,840 --> 00:32:57,519
important factors in receiving an

00:32:56,559 --> 00:33:00,559
infection

00:32:57,519 --> 00:33:03,760
but this is what my model's output was

00:33:00,559 --> 00:33:05,919
so it's a great example for why

00:33:03,760 --> 00:33:07,760
explainers can expose bias that was

00:33:05,919 --> 00:33:10,240
completely unintended

00:33:07,760 --> 00:33:12,640
my accuracy was incredible and it looked

00:33:10,240 --> 00:33:16,000
like this model could be ready to be put

00:33:12,640 --> 00:33:18,240
into production but my explainers say

00:33:16,000 --> 00:33:25,840
slow down there take a pause and look at

00:33:18,240 --> 00:33:25,840
what's happening with your data

00:33:27,760 --> 00:33:32,080
so let's recap the last 35 minutes we've

00:33:30,720 --> 00:33:34,880
had together

00:33:32,080 --> 00:33:36,480
so to recap models are complex

00:33:34,880 --> 00:33:37,279
explainable machine learning is

00:33:36,480 --> 00:33:39,600
necessary

00:33:37,279 --> 00:33:40,799
to close the gap between the machine

00:33:39,600 --> 00:33:43,279
goal of output

00:33:40,799 --> 00:33:45,360
and the human goal of understanding and

00:33:43,279 --> 00:33:47,360
we need explainability when it's not

00:33:45,360 --> 00:33:50,960
enough to just get the prediction

00:33:47,360 --> 00:33:53,760
the model must also justify how it came

00:33:50,960 --> 00:33:54,720
to the prediction explainability helps

00:33:53,760 --> 00:33:57,840
us understand

00:33:54,720 --> 00:34:01,039
the why of models

00:33:57,840 --> 00:34:03,919
to recap black boxes are models where

00:34:01,039 --> 00:34:04,399
you input data and some magic occurs and

00:34:03,919 --> 00:34:06,799
you get an

00:34:04,399 --> 00:34:08,560
output they're really great for speed

00:34:06,799 --> 00:34:12,159
running a data science workflow

00:34:08,560 --> 00:34:14,079
and making really high performing models

00:34:12,159 --> 00:34:15,599
they're less great when you're trying to

00:34:14,079 --> 00:34:18,159
understand what's happening

00:34:15,599 --> 00:34:19,760
in the some magic occurs portion of the

00:34:18,159 --> 00:34:22,720
black box

00:34:19,760 --> 00:34:23,200
explainability algorithms help crack

00:34:22,720 --> 00:34:26,000
open

00:34:23,200 --> 00:34:29,040
the black box and peer in to see how

00:34:26,000 --> 00:34:31,919
decisions are being made

00:34:29,040 --> 00:34:33,520
to recap we have algorithms that focus

00:34:31,919 --> 00:34:36,399
on future importance

00:34:33,520 --> 00:34:37,040
such as shapley which gets a gold star

00:34:36,399 --> 00:34:39,200
for being

00:34:37,040 --> 00:34:41,040
able to aggregate future importance both

00:34:39,200 --> 00:34:44,159
locally and globally

00:34:41,040 --> 00:34:48,079
we have lime which creates a simple and

00:34:44,159 --> 00:34:51,520
easily understood y equals mx plus b

00:34:48,079 --> 00:34:53,599
to explain a local instance and anchors

00:34:51,520 --> 00:34:55,919
which does everything line does but

00:34:53,599 --> 00:34:55,919
better

00:34:56,320 --> 00:34:59,440
to recap we also learned about

00:34:58,480 --> 00:35:01,520
algorithms

00:34:59,440 --> 00:35:02,640
that take a look at the cause and effect

00:35:01,520 --> 00:35:05,359
view on a model

00:35:02,640 --> 00:35:05,760
such as contrastive explanation methods

00:35:05,359 --> 00:35:09,520
and

00:35:05,760 --> 00:35:11,520
counter factuals to recap

00:35:09,520 --> 00:35:13,440
explainability algorithms are just

00:35:11,520 --> 00:35:15,200
they're just really cool

00:35:13,440 --> 00:35:16,560
and they can be very insightful when you

00:35:15,200 --> 00:35:18,480
use them carefully

00:35:16,560 --> 00:35:20,640
i hope you're all curious enough to try

00:35:18,480 --> 00:35:21,760
an explainability algorithm or two on

00:35:20,640 --> 00:35:24,079
your models

00:35:21,760 --> 00:35:25,680
and i'm excited to hear about your new

00:35:24,079 --> 00:35:31,839
insights and experiences

00:35:25,680 --> 00:35:31,839
so thank you all for your time

00:35:34,960 --> 00:35:40,400
yes isabel thank you for this great talk

00:35:38,400 --> 00:35:42,880
i think that was very interesting a lot

00:35:40,400 --> 00:35:46,000
of insights a lot to learn

00:35:42,880 --> 00:35:49,520
um okay let me have a look

00:35:46,000 --> 00:35:53,119
if there are any questions in our

00:35:49,520 --> 00:35:55,680
questions tool uh yes there's someone in

00:35:53,119 --> 00:35:55,680
the chat

00:35:56,000 --> 00:36:04,240
andrew asks how to evaluate explanation

00:36:01,119 --> 00:36:06,960
so that is a very good question um

00:36:04,240 --> 00:36:08,800
there is a incredible book that i've

00:36:06,960 --> 00:36:10,960
read like three times now that's

00:36:08,800 --> 00:36:12,640
called interpretable machine learning um

00:36:10,960 --> 00:36:14,000
and it's all online and i recommend

00:36:12,640 --> 00:36:16,640
looking into that

00:36:14,000 --> 00:36:17,599
a lot of it is you can implement

00:36:16,640 --> 00:36:20,880
something called

00:36:17,599 --> 00:36:24,079
trust scores which is kind of

00:36:20,880 --> 00:36:26,560
explainability for explainability

00:36:24,079 --> 00:36:27,359
and you bring up a really good point

00:36:26,560 --> 00:36:30,320
that

00:36:27,359 --> 00:36:31,040
it is kind of an interesting paradox

00:36:30,320 --> 00:36:34,079
that we're using

00:36:31,040 --> 00:36:34,320
black box explainability models to look

00:36:34,079 --> 00:36:37,520
at

00:36:34,320 --> 00:36:39,520
black box regular models

00:36:37,520 --> 00:36:41,359
um so there's still a lot into

00:36:39,520 --> 00:36:43,280
production this is fairly new

00:36:41,359 --> 00:36:45,200
so i don't have a strong we look at

00:36:43,280 --> 00:36:47,359
accuracy answer for you

00:36:45,200 --> 00:36:50,960
but there are kind of additional steps

00:36:47,359 --> 00:36:50,960
to look at the robustness of your

00:36:52,839 --> 00:36:55,839
explanation

00:37:10,960 --> 00:37:13,040

YouTube URL: https://www.youtube.com/watch?v=kUJuAuS_ais


