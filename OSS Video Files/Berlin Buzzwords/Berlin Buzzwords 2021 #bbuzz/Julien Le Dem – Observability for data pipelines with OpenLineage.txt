Title: Julien Le Dem – Observability for data pipelines with OpenLineage
Publication date: 2021-06-25
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Data is increasingly becoming core to many products. Whether to provide recommendations for users, getting insights on how they use the product or using machine learning to improve the experience. This creates a critical need for reliable data operations and understanding how data is flowing through our systems. Data pipelines must be auditable, reliable and run on time. This proves particularly difficult in a constantly changing, fast paced environment. 

Collecting this lineage metadata as data pipelines are running provides understanding of dependencies between many teams consuming and producing data and how constant changes impact them. It is the underlying foundation that enables the many use cases related to data operations.

The OpenLineage project is an API standardizing this metadata across the ecosystem, reducing complexity and duplicate work in collecting lineage information. It enables many projects, consumers of lineage in the ecosystem whether they focus on operations, governance or security.

Marquez is an open source projects part of the LF AI & Data foundation which instruments data pipelines to collect lineage and metadata and enable those use cases. It implements the OpenLineage API and provides context by making visible dependencies across organisations and technologies as they change over time. 

Speaker:
Julien Le Dem – https://2021.berlinbuzzwords.de/member/julien-le-dem

More: https://2021.berlinbuzzwords.de/session/observability-data-pipelines-openlineage
Captions: 
	00:00:08,080 --> 00:00:11,759
hello

00:00:08,720 --> 00:00:12,559
i'm julian i'm the ceo and co-founder of

00:00:11,759 --> 00:00:14,719
datakin

00:00:12,559 --> 00:00:17,279
and today i'm going to talk about data

00:00:14,719 --> 00:00:19,359
lineage and observability with opening

00:00:17,279 --> 00:00:19,359
it

00:00:19,680 --> 00:00:23,600
so to start um i'll start by talking a

00:00:23,199 --> 00:00:26,160
bit

00:00:23,600 --> 00:00:28,240
about the need for metadata and state

00:00:26,160 --> 00:00:31,279
the problem we're solving here

00:00:28,240 --> 00:00:33,840
and then i'll talk about the solution um

00:00:31,279 --> 00:00:35,440
in particular talking about open lineage

00:00:33,840 --> 00:00:37,840
the open standard for linear collection

00:00:35,440 --> 00:00:41,120
and mark as its reference implementation

00:00:37,840 --> 00:00:43,440
and i'll finish by uh discuss a bit of

00:00:41,120 --> 00:00:45,680
how that works in practice

00:00:43,440 --> 00:00:48,030
and show you a couple of examples of

00:00:45,680 --> 00:00:49,600
usage of open lineage

00:00:48,030 --> 00:00:52,640
[Music]

00:00:49,600 --> 00:00:56,239
so first i'll start by talking about

00:00:52,640 --> 00:00:56,239
the need for metadata

00:00:56,320 --> 00:01:02,800
and this raises from the fact that

00:01:00,480 --> 00:01:04,080
we are consuming and producing more and

00:01:02,800 --> 00:01:08,080
more data and it's not

00:01:04,080 --> 00:01:09,520
necessarily about the volume of data

00:01:08,080 --> 00:01:11,680
even though like there are more and more

00:01:09,520 --> 00:01:12,080
different data sets and tables and

00:01:11,680 --> 00:01:15,040
different

00:01:12,080 --> 00:01:15,840
locations they are stored in but it's

00:01:15,040 --> 00:01:19,439
also about

00:01:15,840 --> 00:01:22,640
um an organization uh growth

00:01:19,439 --> 00:01:24,960
like in a company in an organization

00:01:22,640 --> 00:01:26,080
there are a lot of different teams that

00:01:24,960 --> 00:01:29,119
consume and produce

00:01:26,080 --> 00:01:32,159
data and within their own

00:01:29,119 --> 00:01:34,560
team their own practice they're very

00:01:32,159 --> 00:01:36,560
they know fairly well how it works and

00:01:34,560 --> 00:01:38,159
how they depend on each other but across

00:01:36,560 --> 00:01:41,520
teams

00:01:38,159 --> 00:01:44,880
it's often where their friction happens

00:01:41,520 --> 00:01:45,520
um and this is you know like people talk

00:01:44,880 --> 00:01:48,640
about

00:01:45,520 --> 00:01:50,320
data ops data mesh uh different of those

00:01:48,640 --> 00:01:53,520
practices and really treating

00:01:50,320 --> 00:01:56,079
the data as a product and

00:01:53,520 --> 00:01:57,520
if we draw a parallel to service service

00:01:56,079 --> 00:01:59,759
oriented architecture

00:01:57,520 --> 00:02:00,799
or microservices you know you have

00:01:59,759 --> 00:02:03,840
different teams

00:02:00,799 --> 00:02:06,079
that expose interfaces and

00:02:03,840 --> 00:02:08,080
in a service world the interface is an

00:02:06,079 --> 00:02:11,440
api in the data world the

00:02:08,080 --> 00:02:14,000
interface is the data set

00:02:11,440 --> 00:02:15,280
so it's really important to start

00:02:14,000 --> 00:02:16,879
understanding

00:02:15,280 --> 00:02:19,040
where the data you're consuming is

00:02:16,879 --> 00:02:20,720
coming from of who's consuming

00:02:19,040 --> 00:02:23,360
the data you're producing and have

00:02:20,720 --> 00:02:26,400
visibility across those teams

00:02:23,360 --> 00:02:28,400
otherwise every time something changes

00:02:26,400 --> 00:02:30,160
it's hard to communicate and it's hard

00:02:28,400 --> 00:02:32,400
to understand how we depend on each

00:02:30,160 --> 00:02:32,400
other

00:02:33,120 --> 00:02:36,160
and so today very often we have a

00:02:35,519 --> 00:02:39,840
limited

00:02:36,160 --> 00:02:43,280
context right we know we have to compute

00:02:39,840 --> 00:02:44,959
um and consume data but it's

00:02:43,280 --> 00:02:47,440
you know it's not always clear where is

00:02:44,959 --> 00:02:50,640
the data source what is the schema

00:02:47,440 --> 00:02:52,239
who owns it how often it's updated

00:02:50,640 --> 00:02:53,920
uh where it's coming from was using the

00:02:52,239 --> 00:02:55,360
data but that's changed right like some

00:02:53,920 --> 00:02:57,840
like

00:02:55,360 --> 00:02:59,840
where is it coming from where who's

00:02:57,840 --> 00:03:01,680
consuming the data i'm producing

00:02:59,840 --> 00:03:02,879
what the schema supposed to be how fun

00:03:01,680 --> 00:03:06,879
is it changing

00:03:02,879 --> 00:03:08,879
all those things are very unclear which

00:03:06,879 --> 00:03:12,400
makes it very difficult

00:03:08,879 --> 00:03:13,760
uh to consume data and transform it and

00:03:12,400 --> 00:03:18,959
produce metrics some

00:03:13,760 --> 00:03:21,680
models or value things in a reliable way

00:03:18,959 --> 00:03:22,879
and so that leads us to i've borrowed

00:03:21,680 --> 00:03:24,720
this maslow's

00:03:22,879 --> 00:03:26,720
heart key of needs and you know the

00:03:24,720 --> 00:03:28,319
original master's hierarchy of needs it

00:03:26,720 --> 00:03:31,920
starts with

00:03:28,319 --> 00:03:32,239
before you start looking into happiness

00:03:31,920 --> 00:03:34,959
right

00:03:32,239 --> 00:03:36,879
there's some basic needs you need before

00:03:34,959 --> 00:03:40,080
you can even like

00:03:36,879 --> 00:03:41,280
have a live a happy life first you worry

00:03:40,080 --> 00:03:44,879
about

00:03:41,280 --> 00:03:47,599
um food and shelter right once

00:03:44,879 --> 00:03:48,640
your food and shelter you worry about

00:03:47,599 --> 00:03:51,360
safety

00:03:48,640 --> 00:03:52,720
um and then and so on right and once you

00:03:51,360 --> 00:03:56,080
those basic needs

00:03:52,720 --> 00:03:59,920
are fulfilled then you can think about

00:03:56,080 --> 00:04:01,200
happiness uh how do i become the best

00:03:59,920 --> 00:04:03,439
version of myself

00:04:01,200 --> 00:04:04,319
or whatever goals you set for yourself

00:04:03,439 --> 00:04:07,680
and

00:04:04,319 --> 00:04:09,599
so in this data hierarchy of needs

00:04:07,680 --> 00:04:11,040
there are lots of basic things that need

00:04:09,599 --> 00:04:14,720
to be covered

00:04:11,040 --> 00:04:17,519
before you can even think about

00:04:14,720 --> 00:04:20,160
taking advantage of your data of uh

00:04:17,519 --> 00:04:22,320
getting value out of it

00:04:20,160 --> 00:04:23,440
first the data needs to be available you

00:04:22,320 --> 00:04:26,400
know you need to

00:04:23,440 --> 00:04:26,960
wherever it's coming from you need to be

00:04:26,400 --> 00:04:30,400
able

00:04:26,960 --> 00:04:32,800
to collect this data

00:04:30,400 --> 00:04:33,759
second it needs to be updated in a

00:04:32,800 --> 00:04:36,240
timely fashion

00:04:33,759 --> 00:04:37,919
right like if you have steel data you

00:04:36,240 --> 00:04:41,040
can't make good decision

00:04:37,919 --> 00:04:44,400
and third it needs to be correct

00:04:41,040 --> 00:04:46,960
right you need your data to be available

00:04:44,400 --> 00:04:49,120
to show up on time and be correct and

00:04:46,960 --> 00:04:52,720
once you have all those things achieved

00:04:49,120 --> 00:04:55,680
then you can start into okay what are

00:04:52,720 --> 00:04:57,440
business optimizations we need to do how

00:04:55,680 --> 00:04:58,800
do we find new business opportunities

00:04:57,440 --> 00:05:02,080
how do we actually get

00:04:58,800 --> 00:05:05,520
value out of our data and this is

00:05:02,080 --> 00:05:09,039
really getting those three first levels

00:05:05,520 --> 00:05:10,000
uh correct is really what's going to

00:05:09,039 --> 00:05:12,400
help you get your

00:05:10,000 --> 00:05:14,320
head above the water so that's why i was

00:05:12,400 --> 00:05:16,800
drawing this line here it's like

00:05:14,320 --> 00:05:18,160
as long as you're spending all your time

00:05:16,800 --> 00:05:19,919
figuring out why

00:05:18,160 --> 00:05:21,600
you know your pipelines are broken or

00:05:19,919 --> 00:05:24,479
are they taking a long time

00:05:21,600 --> 00:05:25,440
hawaii is making sure your data is

00:05:24,479 --> 00:05:28,240
correct

00:05:25,440 --> 00:05:28,720
you are not actually spending your time

00:05:28,240 --> 00:05:33,360
on

00:05:28,720 --> 00:05:33,360
actually getting value out of your data

00:05:34,880 --> 00:05:39,039
and so that's why we started building

00:05:37,759 --> 00:05:43,280
open lineage right

00:05:39,039 --> 00:05:46,720
and in the past uh you know so obviously

00:05:43,280 --> 00:05:50,400
i've um been the co-creator of parque

00:05:46,720 --> 00:05:54,000
and later on i've been involved in um

00:05:50,400 --> 00:05:56,960
the start of the apache aero project

00:05:54,000 --> 00:05:57,759
um and really building those standard in

00:05:56,960 --> 00:05:59,680
open source

00:05:57,759 --> 00:06:02,080
and some of the things we did for

00:05:59,680 --> 00:06:02,479
apechiero is to reach out to a wide

00:06:02,080 --> 00:06:05,520
group

00:06:02,479 --> 00:06:07,199
of people who were interested

00:06:05,520 --> 00:06:10,319
in a standard having a standard

00:06:07,199 --> 00:06:12,639
representation to enable

00:06:10,319 --> 00:06:14,400
exchange of data in a performant way

00:06:12,639 --> 00:06:16,720
between various systems

00:06:14,400 --> 00:06:17,759
and so we already took a page out of

00:06:16,720 --> 00:06:20,000
this

00:06:17,759 --> 00:06:22,080
for the open lineage project and so it

00:06:20,000 --> 00:06:26,319
came from the realization

00:06:22,080 --> 00:06:29,120
well if we want to enable observability

00:06:26,319 --> 00:06:30,880
and really enable collection of lineage

00:06:29,120 --> 00:06:32,880
for data pipeline lineage and all the

00:06:30,880 --> 00:06:36,319
related metadata

00:06:32,880 --> 00:06:38,800
this needs to be a standard that can be

00:06:36,319 --> 00:06:40,319
shared across the ecosystem and so you

00:06:38,800 --> 00:06:43,039
know open lineage

00:06:40,319 --> 00:06:43,600
draws a parallel with open telemetry

00:06:43,039 --> 00:06:45,680
which

00:06:43,600 --> 00:06:47,039
open telemetry is this project part of

00:06:45,680 --> 00:06:50,240
the cncf

00:06:47,039 --> 00:06:51,360
about collecting traces and metrics for

00:06:50,240 --> 00:06:53,360
services

00:06:51,360 --> 00:06:55,599
and that helps understanding

00:06:53,360 --> 00:06:57,360
dependencies between microservices for

00:06:55,599 --> 00:06:58,720
example or any service oriented

00:06:57,360 --> 00:07:01,039
architecture

00:06:58,720 --> 00:07:03,520
and old and building observability for

00:07:01,039 --> 00:07:06,080
that so opening edge is really

00:07:03,520 --> 00:07:07,360
the same thing for data pipelines and

00:07:06,080 --> 00:07:10,800
data pipelines work

00:07:07,360 --> 00:07:12,400
in a fairly different way so we started

00:07:10,800 --> 00:07:16,000
the open lineage

00:07:12,400 --> 00:07:19,120
by reaching out to a wide group of uh

00:07:16,000 --> 00:07:22,479
people in the data ecosystem um

00:07:19,120 --> 00:07:25,680
and we've been and talking to them

00:07:22,479 --> 00:07:29,360
um 90 of the time

00:07:25,680 --> 00:07:30,080
people agree that yes we need a standard

00:07:29,360 --> 00:07:32,960
for collecting

00:07:30,080 --> 00:07:35,199
lineage and why don't we have it already

00:07:32,960 --> 00:07:37,120
and so

00:07:35,199 --> 00:07:38,240
well the reason we didn't have that yet

00:07:37,120 --> 00:07:40,800
is because

00:07:38,240 --> 00:07:41,520
we need to get together and actually

00:07:40,800 --> 00:07:44,879
start it

00:07:41,520 --> 00:07:45,599
right like if nobody starts defining a

00:07:44,879 --> 00:07:47,520
standard for

00:07:45,599 --> 00:07:49,680
collecting lineage in the ecosystem it's

00:07:47,520 --> 00:07:52,960
not going to happen by itself

00:07:49,680 --> 00:07:55,680
and so by getting together and really

00:07:52,960 --> 00:07:57,840
discussing how we can define this this

00:07:55,680 --> 00:08:00,080
standard and make it happen

00:07:57,840 --> 00:08:01,520
rarely get this effort started and so

00:08:00,080 --> 00:08:04,639
that's what we did last year

00:08:01,520 --> 00:08:08,000
um in starting this open lineage project

00:08:04,639 --> 00:08:11,280
and getting contributors and creators

00:08:08,000 --> 00:08:12,720
of many of the important open source

00:08:11,280 --> 00:08:15,919
data project

00:08:12,720 --> 00:08:15,919
to contribute to the spec

00:08:17,199 --> 00:08:20,879
and so the purpose of open lineage right

00:08:20,319 --> 00:08:23,680
so

00:08:20,879 --> 00:08:26,160
defining it as an open standard for

00:08:23,680 --> 00:08:27,360
metadata a lineage collection by

00:08:26,160 --> 00:08:29,919
instrumenting data

00:08:27,360 --> 00:08:31,680
pipelines as they're running and so to

00:08:29,919 --> 00:08:34,800
make it simple

00:08:31,680 --> 00:08:36,479
you know how in your digital camera or

00:08:34,800 --> 00:08:38,479
smartphones today when you take a

00:08:36,479 --> 00:08:40,959
picture it actually saves

00:08:38,479 --> 00:08:42,959
a lot of metadata in the picture itself

00:08:40,959 --> 00:08:44,800
right it's going to save the

00:08:42,959 --> 00:08:47,120
gps coordinates where the picture was

00:08:44,800 --> 00:08:48,640
taken when it was taken

00:08:47,120 --> 00:08:50,880
some other information about the

00:08:48,640 --> 00:08:53,440
characteristics of the lens of the mode

00:08:50,880 --> 00:08:55,680
being used and the best time to capture

00:08:53,440 --> 00:08:58,640
this metadata but the picture

00:08:55,680 --> 00:09:00,480
is when it's taken and so you know

00:08:58,640 --> 00:09:02,959
that's what open lineage is

00:09:00,480 --> 00:09:04,000
open lineage is the exif for data

00:09:02,959 --> 00:09:07,279
pipelines

00:09:04,000 --> 00:09:10,880
it is standard to capture the metadata

00:09:07,279 --> 00:09:14,000
about the job at the time

00:09:10,880 --> 00:09:16,000
it ran and so

00:09:14,000 --> 00:09:17,920
and this is the best time to understand

00:09:16,000 --> 00:09:21,279
you know the characteristic of the job

00:09:17,920 --> 00:09:22,640
what was the state of the data set when

00:09:21,279 --> 00:09:24,880
you consume it

00:09:22,640 --> 00:09:26,480
uh what was the schema some other

00:09:24,880 --> 00:09:28,100
runtime information

00:09:26,480 --> 00:09:29,680
and so on

00:09:28,100 --> 00:09:33,360
[Music]

00:09:29,680 --> 00:09:35,760
and and having this starter itself

00:09:33,360 --> 00:09:37,279
really an architectural problem right

00:09:35,760 --> 00:09:39,920
there are a lot of projects

00:09:37,279 --> 00:09:41,680
that are interested in consuming lineage

00:09:39,920 --> 00:09:44,080
and there are a lot of project

00:09:41,680 --> 00:09:44,959
that may produce lineage or have

00:09:44,080 --> 00:09:47,760
inherently

00:09:44,959 --> 00:09:49,440
lineage because they're transforming

00:09:47,760 --> 00:09:52,000
data

00:09:49,440 --> 00:09:54,160
and before upper lineage there was a lot

00:09:52,000 --> 00:09:57,040
of duplication of effort

00:09:54,160 --> 00:09:58,560
each project that cares about metadata

00:09:57,040 --> 00:10:01,440
lineage has to instrument

00:09:58,560 --> 00:10:02,320
all possible types of job to extract

00:10:01,440 --> 00:10:05,360
information

00:10:02,320 --> 00:10:08,079
so this picture on the left here where

00:10:05,360 --> 00:10:09,360
whether your data catalog or a

00:10:08,079 --> 00:10:12,640
governance product

00:10:09,360 --> 00:10:15,760
or a discovery a data discovery

00:10:12,640 --> 00:10:19,519
any type of system of operational system

00:10:15,760 --> 00:10:22,800
you need to integrate with all those as

00:10:19,519 --> 00:10:23,760
transformation framework processing

00:10:22,800 --> 00:10:26,800
systems

00:10:23,760 --> 00:10:28,959
warehouses to collect the lineage

00:10:26,800 --> 00:10:29,839
and so introduce a lot of duplication of

00:10:28,959 --> 00:10:31,519
effort

00:10:29,839 --> 00:10:33,040
each project has to instrument all the

00:10:31,519 --> 00:10:35,279
jobs

00:10:33,040 --> 00:10:36,720
and also it makes it the whole thing

00:10:35,279 --> 00:10:39,839
very brittle

00:10:36,720 --> 00:10:43,040
because integrations are external

00:10:39,839 --> 00:10:45,120
and can break with new versions not only

00:10:43,040 --> 00:10:47,760
you have to deal with spark versus

00:10:45,120 --> 00:10:50,240
bigquery versus snowflake versus presto

00:10:47,760 --> 00:10:50,959
you also have to deal with evolution of

00:10:50,240 --> 00:10:53,519
all those things

00:10:50,959 --> 00:10:54,480
all the versions of spark and so on

00:10:53,519 --> 00:10:57,600
right

00:10:54,480 --> 00:10:59,200
and so depending on the internal of each

00:10:57,600 --> 00:11:00,399
of the system to understand lineage

00:10:59,200 --> 00:11:03,680
becomes very brittle

00:11:00,399 --> 00:11:04,480
very expensive very complex so with open

00:11:03,680 --> 00:11:07,920
lineage

00:11:04,480 --> 00:11:10,880
there are a lot of multiple advantages

00:11:07,920 --> 00:11:11,360
one is that the effort of integration is

00:11:10,880 --> 00:11:13,279
shared

00:11:11,360 --> 00:11:14,959
right once we expose lineage in the

00:11:13,279 --> 00:11:17,839
common standard

00:11:14,959 --> 00:11:18,800
you can leverage that across multiple

00:11:17,839 --> 00:11:22,560
projects

00:11:18,800 --> 00:11:24,880
that consume lineage and second

00:11:22,560 --> 00:11:26,320
integration can also be pushed in each

00:11:24,880 --> 00:11:28,480
project instead

00:11:26,320 --> 00:11:31,040
now open lineage becomes a standard

00:11:28,480 --> 00:11:33,200
interface right so you can pull that

00:11:31,040 --> 00:11:34,399
into each of those projects and instead

00:11:33,200 --> 00:11:36,399
of having

00:11:34,399 --> 00:11:38,240
external integration that pull the

00:11:36,399 --> 00:11:39,040
lineage from each of the system by

00:11:38,240 --> 00:11:41,760
enters

00:11:39,040 --> 00:11:42,560
understanding their internals you can

00:11:41,760 --> 00:11:46,399
have each

00:11:42,560 --> 00:11:49,040
of those lineage producing system

00:11:46,399 --> 00:11:50,320
to expose it in the open lineage or

00:11:49,040 --> 00:11:52,959
presentation

00:11:50,320 --> 00:11:54,639
so that it evolves with them right you

00:11:52,959 --> 00:11:57,440
don't have to worry about

00:11:54,639 --> 00:11:57,839
evolution of the internal because now if

00:11:57,440 --> 00:12:00,480
the

00:11:57,839 --> 00:12:01,920
opening exposure of the lineage evolves

00:12:00,480 --> 00:12:04,800
with each of the project

00:12:01,920 --> 00:12:05,839
so it's really kind of you know in

00:12:04,800 --> 00:12:08,959
software design

00:12:05,839 --> 00:12:11,600
this is known as inversion

00:12:08,959 --> 00:12:12,560
of dependency right we instead of having

00:12:11,600 --> 00:12:15,839
each of those

00:12:12,560 --> 00:12:17,920
uh lineage interested consumer project

00:12:15,839 --> 00:12:20,639
depend on each of those projects all of

00:12:17,920 --> 00:12:23,040
those projects depend on open lineage

00:12:20,639 --> 00:12:24,880
which uh remove the dependency and

00:12:23,040 --> 00:12:27,920
everybody depends on open lineage

00:12:24,880 --> 00:12:33,040
and it removes this uh n square

00:12:27,920 --> 00:12:33,040
so to speak dependencies um here

00:12:33,360 --> 00:12:36,480
so to clarify the scope right open

00:12:35,200 --> 00:12:40,160
lineage is really like

00:12:36,480 --> 00:12:42,560
open telemetry it is the spec

00:12:40,160 --> 00:12:43,920
and the integration and the ability to

00:12:42,560 --> 00:12:47,040
expose lineage

00:12:43,920 --> 00:12:48,880
it is not any of the storage or

00:12:47,040 --> 00:12:50,240
use cases that you can build on top of

00:12:48,880 --> 00:12:53,200
the lineage so

00:12:50,240 --> 00:12:54,959
in the consumers of course marquez is

00:12:53,200 --> 00:12:58,079
the reference implementation

00:12:54,959 --> 00:13:01,120
of the open lineage spec

00:12:58,079 --> 00:13:03,839
and so openlineage defines how you

00:13:01,120 --> 00:13:05,839
collect metadata how you represent it

00:13:03,839 --> 00:13:07,760
and then projects like marquez de la

00:13:05,839 --> 00:13:10,880
hoya munson in nigeria

00:13:07,760 --> 00:13:14,800
can consume open lineage um

00:13:10,880 --> 00:13:16,480
and um to leverage it to make lineage um

00:13:14,800 --> 00:13:18,320
available in their system and that here

00:13:16,480 --> 00:13:21,839
i'm putting examples

00:13:18,320 --> 00:13:24,880
of the open source world but really

00:13:21,839 --> 00:13:28,320
um that's applicable to many

00:13:24,880 --> 00:13:31,600
to any other um lineage

00:13:28,320 --> 00:13:33,760
focused uh system

00:13:31,600 --> 00:13:35,680
and the interesting part party here is

00:13:33,760 --> 00:13:37,680
like when you talk about lineage

00:13:35,680 --> 00:13:39,040
um different people will have different

00:13:37,680 --> 00:13:42,959
definitions

00:13:39,040 --> 00:13:46,079
of what lineage is and so very often

00:13:42,959 --> 00:13:49,279
you have dedicated products

00:13:46,079 --> 00:13:50,079
or projects or uis for various projects

00:13:49,279 --> 00:13:53,440
right

00:13:50,079 --> 00:13:56,399
you may be interested more in operations

00:13:53,440 --> 00:13:57,920
uh reliability of your data data quality

00:13:56,399 --> 00:14:00,079
you may be interested more in data

00:13:57,920 --> 00:14:03,839
discovery data catalog

00:14:00,079 --> 00:14:05,199
you may be more interested in governance

00:14:03,839 --> 00:14:08,320
compliance

00:14:05,199 --> 00:14:12,079
um or in privacy and like

00:14:08,320 --> 00:14:15,040
enforcing things like gdpr or ccpa

00:14:12,079 --> 00:14:16,240
and all those use cases are very

00:14:15,040 --> 00:14:19,680
different but they all

00:14:16,240 --> 00:14:20,800
rely in some way on also understanding

00:14:19,680 --> 00:14:23,760
lineage and

00:14:20,800 --> 00:14:25,839
some of them may define lineage as

00:14:23,760 --> 00:14:26,160
understanding how this column is derived

00:14:25,839 --> 00:14:28,639
from

00:14:26,160 --> 00:14:29,600
other columns in car data set it could

00:14:28,639 --> 00:14:32,399
be understanding

00:14:29,600 --> 00:14:34,160
as oh the update of this data set

00:14:32,399 --> 00:14:36,000
depends on the execution of this job

00:14:34,160 --> 00:14:37,839
that consuming those also data set or it

00:14:36,000 --> 00:14:41,199
may be interested in

00:14:37,839 --> 00:14:45,120
knowing how this um

00:14:41,199 --> 00:14:49,680
pii user private information column

00:14:45,120 --> 00:14:53,920
is being consumed and where is being

00:14:49,680 --> 00:14:53,920
copied in your warehouse

00:14:56,560 --> 00:15:03,440
so if we look at the core model of um

00:15:00,240 --> 00:15:06,800
open lineage open lineage is defined

00:15:03,440 --> 00:15:10,240
as a json schema spec so which

00:15:06,800 --> 00:15:13,279
as a core model of understanding jobs

00:15:10,240 --> 00:15:16,160
and runs and data sets

00:15:13,279 --> 00:15:18,320
and from that we can attach various

00:15:16,160 --> 00:15:20,480
pieces of metadata around it

00:15:18,320 --> 00:15:21,760
so the core model is very simple you

00:15:20,480 --> 00:15:24,000
have a notion of job

00:15:21,760 --> 00:15:24,880
and a job is a recurring transformation

00:15:24,000 --> 00:15:27,680
that can happen

00:15:24,880 --> 00:15:30,320
it could be a sql query it could be a

00:15:27,680 --> 00:15:33,680
spark job it could be a flink job

00:15:30,320 --> 00:15:36,240
um it could be a python script

00:15:33,680 --> 00:15:37,440
that reads data and produced data and is

00:15:36,240 --> 00:15:42,320
recurring

00:15:37,440 --> 00:15:45,759
um is running um multiple times

00:15:42,320 --> 00:15:47,519
a run is a particular instance of this

00:15:45,759 --> 00:15:49,440
job running

00:15:47,519 --> 00:15:50,720
so it could be running at a specific

00:15:49,440 --> 00:15:53,759
time

00:15:50,720 --> 00:15:56,240
for a specific schedule

00:15:53,759 --> 00:15:57,040
it has um it could have specific

00:15:56,240 --> 00:16:00,320
parameters

00:15:57,040 --> 00:16:03,519
and so on the data set is

00:16:00,320 --> 00:16:04,320
um a representation of data so it could

00:16:03,519 --> 00:16:07,600
be

00:16:04,320 --> 00:16:09,680
a table it could be a folder in the

00:16:07,600 --> 00:16:14,639
distributed file system

00:16:09,680 --> 00:16:17,680
something like s3 or hdfs or gcs

00:16:14,639 --> 00:16:20,720
and so we have this core model of

00:16:17,680 --> 00:16:23,680
lineage where you will have a job

00:16:20,720 --> 00:16:25,440
is running for a specific run id and

00:16:23,680 --> 00:16:26,240
it's reading and writing specific data

00:16:25,440 --> 00:16:29,279
sets

00:16:26,240 --> 00:16:31,680
so that the core of the model and

00:16:29,279 --> 00:16:32,399
dc some of the very important things

00:16:31,680 --> 00:16:35,279
here

00:16:32,399 --> 00:16:36,480
is how we build consistent naming for

00:16:35,279 --> 00:16:39,360
jobs

00:16:36,480 --> 00:16:41,680
based on what scheduler is scheduling it

00:16:39,360 --> 00:16:44,399
so typically you can build

00:16:41,680 --> 00:16:45,839
hierarchical name from scheduler job

00:16:44,399 --> 00:16:47,839
tasks

00:16:45,839 --> 00:16:50,560
and for data set in a similar way to be

00:16:47,839 --> 00:16:53,040
able to stitch the lineage back together

00:16:50,560 --> 00:16:54,560
understanding consistent names for data

00:16:53,040 --> 00:16:56,160
set is really important

00:16:54,560 --> 00:16:57,600
and that's part of the spec how do you

00:16:56,160 --> 00:17:00,079
consistently name

00:16:57,600 --> 00:17:01,759
jobs and data set so once you have this

00:17:00,079 --> 00:17:04,079
core model

00:17:01,759 --> 00:17:05,520
of knowing that there's a particular run

00:17:04,079 --> 00:17:08,079
for a particular job that

00:17:05,520 --> 00:17:09,199
read and wrote to dataset then the

00:17:08,079 --> 00:17:14,079
notion of asset

00:17:09,199 --> 00:17:17,839
is what enables attaching values uh

00:17:14,079 --> 00:17:19,120
aspect of metadata to this core model

00:17:17,839 --> 00:17:22,079
and that's where you could have

00:17:19,120 --> 00:17:23,520
run facet job facet dataset facet that

00:17:22,079 --> 00:17:26,319
are about metadata

00:17:23,520 --> 00:17:28,559
specific to each of those elements and

00:17:26,319 --> 00:17:30,840
i'm going to

00:17:28,559 --> 00:17:33,840
go in the details about facet in a

00:17:30,840 --> 00:17:33,840
second

00:17:34,640 --> 00:17:38,160
and really the the only thing that open

00:17:36,640 --> 00:17:40,640
lineage depends on

00:17:38,160 --> 00:17:41,200
is having some asynchronous protocol so

00:17:40,640 --> 00:17:44,240
it could be

00:17:41,200 --> 00:17:46,320
http it could be a kafka queue

00:17:44,240 --> 00:17:48,080
it could be something like that

00:17:46,320 --> 00:17:51,280
basically you're going to start

00:17:48,080 --> 00:17:54,400
send a start event when the job starts

00:17:51,280 --> 00:17:56,320
and a unique run id is going to help

00:17:54,400 --> 00:18:00,559
correlate the start event

00:17:56,320 --> 00:18:02,000
and the end event um

00:18:00,559 --> 00:18:04,799
so when the job starts you may be

00:18:02,000 --> 00:18:07,679
sending a start event saying this job id

00:18:04,799 --> 00:18:10,240
uh is starting a run with this run id

00:18:07,679 --> 00:18:13,440
and it's reading from those data sets

00:18:10,240 --> 00:18:15,440
and then you start a complete event um

00:18:13,440 --> 00:18:16,480
that will tell you that the job is

00:18:15,440 --> 00:18:19,600
finished

00:18:16,480 --> 00:18:20,640
um and you may capture uh what was the

00:18:19,600 --> 00:18:22,400
input data set

00:18:20,640 --> 00:18:24,160
what was the output data set version and

00:18:22,400 --> 00:18:26,160
schema the time the job ran

00:18:24,160 --> 00:18:28,480
and really making observation about what

00:18:26,160 --> 00:18:30,960
happened this particular job

00:18:28,480 --> 00:18:31,520
and this is a version of the source code

00:18:30,960 --> 00:18:34,400
that

00:18:31,520 --> 00:18:34,960
was run uh possibly this particular

00:18:34,400 --> 00:18:38,240
sequel

00:18:34,960 --> 00:18:39,440
those were the parameters um this is the

00:18:38,240 --> 00:18:41,360
query profile

00:18:39,440 --> 00:18:43,360
uh that we collected when the job was

00:18:41,360 --> 00:18:44,559
finished here's the schema of the input

00:18:43,360 --> 00:18:46,799
at the time it

00:18:44,559 --> 00:18:48,559
it ran here's the schema of the output

00:18:46,799 --> 00:18:51,679
at the time it run

00:18:48,559 --> 00:18:55,039
and so on and so that helps really

00:18:51,679 --> 00:18:58,880
understand what the run did and

00:18:55,039 --> 00:19:01,200
collect all the media data around it

00:18:58,880 --> 00:19:02,480
and so this notion of asset is really a

00:19:01,200 --> 00:19:05,520
mechanism

00:19:02,480 --> 00:19:07,840
uh to make the model flexible

00:19:05,520 --> 00:19:10,080
right one of the goals of the opening

00:19:07,840 --> 00:19:11,600
each spec is to avoid having this big

00:19:10,080 --> 00:19:13,039
monolithic spec

00:19:11,600 --> 00:19:14,799
that takes a long time to build

00:19:13,039 --> 00:19:17,120
consensus on and

00:19:14,799 --> 00:19:18,720
instead by having the notion of facet

00:19:17,120 --> 00:19:22,080
each of those facets

00:19:18,720 --> 00:19:22,960
is its own atomic spec that focuses on

00:19:22,080 --> 00:19:26,559
one specific

00:19:22,960 --> 00:19:28,799
aspect of the metadata for example

00:19:26,559 --> 00:19:31,600
you can have a data set facet that

00:19:28,799 --> 00:19:34,400
focuses on the schema

00:19:31,600 --> 00:19:35,520
and we model the schema of a data set in

00:19:34,400 --> 00:19:38,799
a certain way

00:19:35,520 --> 00:19:42,160
and that's one facet you may have um

00:19:38,799 --> 00:19:45,840
another facet which is

00:19:42,160 --> 00:19:48,480
what's the query plan for

00:19:45,840 --> 00:19:48,880
the particular job you can have a facet

00:19:48,480 --> 00:19:52,080
which

00:19:48,880 --> 00:19:55,280
is what was the version

00:19:52,080 --> 00:19:57,120
of the input uh if you use um

00:19:55,280 --> 00:19:58,480
underlying storage format like delta

00:19:57,120 --> 00:20:01,280
lake or iceberg

00:19:58,480 --> 00:20:02,640
that actually capture the version of the

00:20:01,280 --> 00:20:05,360
data set

00:20:02,640 --> 00:20:06,000
and so it's really make this extensible

00:20:05,360 --> 00:20:07,600
model

00:20:06,000 --> 00:20:09,679
that it's really easy to focus on

00:20:07,600 --> 00:20:11,679
various aspects of the metadata

00:20:09,679 --> 00:20:14,480
and we may have a conversation about

00:20:11,679 --> 00:20:17,760
what's a data quality metric facet

00:20:14,480 --> 00:20:21,120
uh what's the schema facet what's the

00:20:17,760 --> 00:20:23,200
model quality

00:20:21,120 --> 00:20:24,320
metric facet and it's different for

00:20:23,200 --> 00:20:26,640
machine learning

00:20:24,320 --> 00:20:29,600
or a lot of those different aspects of

00:20:26,640 --> 00:20:32,480
metadata we may want to capture

00:20:29,600 --> 00:20:34,000
and so it's really makes it easier to

00:20:32,480 --> 00:20:37,120
have focused discussion

00:20:34,000 --> 00:20:40,240
and optional aspect to the data and it

00:20:37,120 --> 00:20:41,919
also makes it easier to specialize

00:20:40,240 --> 00:20:43,679
uh the spec for different type of

00:20:41,919 --> 00:20:46,000
storage for example

00:20:43,679 --> 00:20:48,000
a streaming data set like a kafka topic

00:20:46,000 --> 00:20:50,480
with a very different facet

00:20:48,000 --> 00:20:51,440
from a table data set like a table in a

00:20:50,480 --> 00:20:54,000
warehouse

00:20:51,440 --> 00:20:55,679
or a folder uh in the distributed file

00:20:54,000 --> 00:20:58,799
system so that lets you

00:20:55,679 --> 00:21:01,120
facilitate adding a various pieces of

00:20:58,799 --> 00:21:02,080
optional metadata depending of the type

00:21:01,120 --> 00:21:05,760
of underlying

00:21:02,080 --> 00:21:07,200
job or data set and it has lots of

00:21:05,760 --> 00:21:10,240
flexibility

00:21:07,200 --> 00:21:13,039
it's also decentralized because

00:21:10,240 --> 00:21:14,720
to help people experiment and be able to

00:21:13,039 --> 00:21:16,000
create facets without having to go

00:21:14,720 --> 00:21:19,600
through a validation

00:21:16,000 --> 00:21:22,559
on a building consensus process

00:21:19,600 --> 00:21:23,360
you can make your own custom facet all

00:21:22,559 --> 00:21:27,120
you have to do

00:21:23,360 --> 00:21:30,240
is publish adjacent schemas

00:21:27,120 --> 00:21:32,720
for it and then you can add custom facet

00:21:30,240 --> 00:21:34,000
to the spec and start collecting them as

00:21:32,720 --> 00:21:36,480
part of the model

00:21:34,000 --> 00:21:38,320
so this is really about making it really

00:21:36,480 --> 00:21:41,360
easy for people to adapt

00:21:38,320 --> 00:21:44,640
and experiment with open image

00:21:41,360 --> 00:21:47,600
and to give you some examples of assets

00:21:44,640 --> 00:21:50,080
you know at a data set level you may be

00:21:47,600 --> 00:21:54,000
interested in capturing statistics

00:21:50,080 --> 00:21:56,400
for data quality for example um you know

00:21:54,000 --> 00:21:58,320
mean max number of nulls distribution of

00:21:56,400 --> 00:22:00,480
a column for example

00:21:58,320 --> 00:22:02,240
you'd be able to capture schema so that

00:22:00,480 --> 00:22:04,159
you can can capture how the schema if a

00:22:02,240 --> 00:22:06,720
data set changes over time

00:22:04,159 --> 00:22:08,000
the verb the version if the underlying

00:22:06,720 --> 00:22:10,480
format

00:22:08,000 --> 00:22:12,640
is version the column level lineage of

00:22:10,480 --> 00:22:15,200
that important to you

00:22:12,640 --> 00:22:16,960
at the job level you may be capturing

00:22:15,200 --> 00:22:18,720
what was the version of the job at the

00:22:16,960 --> 00:22:20,320
time it ran

00:22:18,720 --> 00:22:22,159
what were the dependencies or the

00:22:20,320 --> 00:22:25,120
versions of the dependencies

00:22:22,159 --> 00:22:27,679
um where is it stored in source control

00:22:25,120 --> 00:22:32,159
what was the query plan

00:22:27,679 --> 00:22:34,960
at the run level you may care about

00:22:32,159 --> 00:22:35,440
the schedule time the batch id if that's

00:22:34,960 --> 00:22:37,840
the thing

00:22:35,440 --> 00:22:40,159
in their scheduling model what was the

00:22:37,840 --> 00:22:41,520
query profiles or what parameters were

00:22:40,159 --> 00:22:44,400
passed

00:22:41,520 --> 00:22:47,440
to the particular run or hyper

00:22:44,400 --> 00:22:47,440
parameters for example

00:22:48,000 --> 00:22:53,679
and so that's upper lineage making

00:22:51,200 --> 00:22:54,720
observation about a job that's running

00:22:53,679 --> 00:22:56,400
and marquez

00:22:54,720 --> 00:22:57,760
is a reference implementation that

00:22:56,400 --> 00:23:00,080
consumes

00:22:57,760 --> 00:23:03,039
those open lineage events and keeps

00:23:00,080 --> 00:23:06,159
track of all the changes

00:23:03,039 --> 00:23:10,240
so really the way you use markets

00:23:06,159 --> 00:23:13,600
is this uh you have a marquez instance

00:23:10,240 --> 00:23:14,799
and you may be monitoring like building

00:23:13,600 --> 00:23:18,000
observability for

00:23:14,799 --> 00:23:20,960
an entire data platform right so if here

00:23:18,000 --> 00:23:22,240
i'm making this hypothetical example of

00:23:20,960 --> 00:23:25,039
a data platform

00:23:22,240 --> 00:23:27,760
so typically you have an ingest layer

00:23:25,039 --> 00:23:29,120
where data comes in and you ingest data

00:23:27,760 --> 00:23:31,200
and then you would have storage and

00:23:29,120 --> 00:23:33,600
compute layer so typically in your

00:23:31,200 --> 00:23:36,720
storage layer you have two tiers

00:23:33,600 --> 00:23:37,200
one streaming you may be using kafka and

00:23:36,720 --> 00:23:39,280
one

00:23:37,200 --> 00:23:41,440
for data in motion and then one data at

00:23:39,280 --> 00:23:44,240
rest storage where data gets

00:23:41,440 --> 00:23:45,440
archived into a distributed file system

00:23:44,240 --> 00:23:48,880
something like s3

00:23:45,440 --> 00:23:52,159
hdfs gcs you might be using iceberg

00:23:48,880 --> 00:23:56,000
for making sure you have a good

00:23:52,159 --> 00:23:57,200
uh underlying table abstraction for your

00:23:56,000 --> 00:23:59,440
data storage

00:23:57,200 --> 00:24:01,360
and then in the compute layer you may

00:23:59,440 --> 00:24:02,559
use something like flink or spark

00:24:01,360 --> 00:24:06,159
streaming

00:24:02,559 --> 00:24:08,640
uh for streaming and the batch side

00:24:06,159 --> 00:24:11,600
you may be using something like spark or

00:24:08,640 --> 00:24:13,039
a warehouse like snowflake or bigquery

00:24:11,600 --> 00:24:16,400
or redshift

00:24:13,039 --> 00:24:18,799
and maybe probably python

00:24:16,400 --> 00:24:19,840
pandas different type of transformation

00:24:18,799 --> 00:24:21,679
as well and might be

00:24:19,840 --> 00:24:23,520
using a scheduler like airflow to

00:24:21,679 --> 00:24:25,520
schedule all those things

00:24:23,520 --> 00:24:27,520
and then typically you have a bi layer

00:24:25,520 --> 00:24:29,520
or you may also use your data into your

00:24:27,520 --> 00:24:30,720
product

00:24:29,520 --> 00:24:32,880
as through machine learning

00:24:30,720 --> 00:24:36,480
recommendation engine and so on

00:24:32,880 --> 00:24:38,400
and so open lineage is about collecting

00:24:36,480 --> 00:24:39,760
lineage information and metadata about

00:24:38,400 --> 00:24:43,279
everything that's happening

00:24:39,760 --> 00:24:45,919
in this platform and marquez

00:24:43,279 --> 00:24:48,400
is a central way to store it and keep

00:24:45,919 --> 00:24:51,600
track of what has changed

00:24:48,400 --> 00:24:52,880
so you'd see that the market law reuses

00:24:51,600 --> 00:24:55,679
some of those same notion

00:24:52,880 --> 00:24:57,520
but where opel lineage is making

00:24:55,679 --> 00:24:58,320
observation and modeling the job that

00:24:57,520 --> 00:25:00,080
he's writing is

00:24:58,320 --> 00:25:01,600
it's telling you well this job is

00:25:00,080 --> 00:25:03,520
running and it's reading from this data

00:25:01,600 --> 00:25:05,840
set and running to this data set

00:25:03,520 --> 00:25:07,840
markers is really about keeping track of

00:25:05,840 --> 00:25:11,039
how that changes over time

00:25:07,840 --> 00:25:12,640
right so every time the job runs

00:25:11,039 --> 00:25:14,880
and writes your data set it's going to

00:25:12,640 --> 00:25:15,520
create a new version of the metadata in

00:25:14,880 --> 00:25:17,840
the engine

00:25:15,520 --> 00:25:19,840
because this job has run and it's

00:25:17,840 --> 00:25:21,279
creating a new version of the data set

00:25:19,840 --> 00:25:23,120
and we're going to keep track of the

00:25:21,279 --> 00:25:25,279
metadata so if

00:25:23,120 --> 00:25:26,799
this particular run of a job changes the

00:25:25,279 --> 00:25:28,880
schema of its output

00:25:26,799 --> 00:25:30,720
we're going to capture it but then the

00:25:28,880 --> 00:25:32,799
run will also have like the facet

00:25:30,720 --> 00:25:35,679
information about performance like

00:25:32,799 --> 00:25:37,279
the query profiles for example and the

00:25:35,679 --> 00:25:38,960
job will keep track of what was the

00:25:37,279 --> 00:25:40,880
current version of the job

00:25:38,960 --> 00:25:43,279
uh what's the current git cha for

00:25:40,880 --> 00:25:46,320
example and so on so you keep track of

00:25:43,279 --> 00:25:49,360
everything that has changed

00:25:46,320 --> 00:25:52,480
as things are running

00:25:49,360 --> 00:25:54,720
and so open lineage is about collecting

00:25:52,480 --> 00:25:56,159
lineage information markets keep track

00:25:54,720 --> 00:25:57,760
of all the changes

00:25:56,159 --> 00:25:59,440
and what we do at dedication to kind of

00:25:57,760 --> 00:26:02,320
situate all those things

00:25:59,440 --> 00:26:03,120
is just build on top of marquez to

00:26:02,320 --> 00:26:04,880
enable

00:26:03,120 --> 00:26:06,159
understanding of operational

00:26:04,880 --> 00:26:08,960
dependencies

00:26:06,159 --> 00:26:11,039
impact analysis troubleshooting and like

00:26:08,960 --> 00:26:13,279
really building this lineage analysis

00:26:11,039 --> 00:26:15,520
and enabling troubleshooting what has

00:26:13,279 --> 00:26:17,360
changed since the last time it worked

00:26:15,520 --> 00:26:18,640
where is the bottleneck why is my

00:26:17,360 --> 00:26:21,120
pipeline slow why

00:26:18,640 --> 00:26:22,159
is the data showing up late and it's all

00:26:21,120 --> 00:26:24,159
leveraging

00:26:22,159 --> 00:26:27,440
all the data collected is through

00:26:24,159 --> 00:26:27,440
opening engine markers

00:26:27,600 --> 00:26:31,760
and so now i'm going to go a little bit

00:26:29,760 --> 00:26:33,840
over now we talked about the problem

00:26:31,760 --> 00:26:35,039
and why we built open lineage and where

00:26:33,840 --> 00:26:37,279
it's coming from

00:26:35,039 --> 00:26:38,240
i'll talk a bit about what you can do in

00:26:37,279 --> 00:26:41,360
practice

00:26:38,240 --> 00:26:46,640
with open lineage

00:26:41,360 --> 00:26:50,240
and marquez using this metadata

00:26:46,640 --> 00:26:51,520
and so today as of integration that are

00:26:50,240 --> 00:26:55,120
covered

00:26:51,520 --> 00:26:57,120
with open lineage you can integrate with

00:26:55,120 --> 00:26:59,039
apache airflow

00:26:57,120 --> 00:27:01,440
as part of that you will have

00:26:59,039 --> 00:27:02,960
integration with warehouses like

00:27:01,440 --> 00:27:06,240
snowflake bigquery

00:27:02,960 --> 00:27:08,559
redshift um and

00:27:06,240 --> 00:27:09,440
uh we also have integration with apache

00:27:08,559 --> 00:27:12,240
spark

00:27:09,440 --> 00:27:13,919
and so all those things are covered out

00:27:12,240 --> 00:27:15,840
of the box and so if you're using those

00:27:13,919 --> 00:27:19,600
tools you will really be able to

00:27:15,840 --> 00:27:21,279
see lineage um and understanding how

00:27:19,600 --> 00:27:22,080
things depend on each other and how

00:27:21,279 --> 00:27:25,200
things change

00:27:22,080 --> 00:27:28,000
over time what's currently in beta and

00:27:25,200 --> 00:27:29,919
uh being added is the great expectation

00:27:28,000 --> 00:27:33,200
and dbt coverage so dbt

00:27:29,919 --> 00:27:36,240
is his newer framework um

00:27:33,200 --> 00:27:40,080
for building sql transformations

00:27:36,240 --> 00:27:43,679
um that's becoming very popular and so

00:27:40,080 --> 00:27:46,720
there's a currently um beta

00:27:43,679 --> 00:27:49,039
integration with dbt in progress

00:27:46,720 --> 00:27:51,039
and great expectation is a popular

00:27:49,039 --> 00:27:53,919
library

00:27:51,039 --> 00:27:55,600
to build data quality right you can

00:27:53,919 --> 00:27:58,399
define

00:27:55,600 --> 00:27:59,200
assertions build expectations on your

00:27:58,399 --> 00:28:01,440
data

00:27:59,200 --> 00:28:02,559
and validate them and at the same time

00:28:01,440 --> 00:28:06,559
it also collects

00:28:02,559 --> 00:28:08,240
um statistics and so you can keep track

00:28:06,559 --> 00:28:10,320
with opening asian markets

00:28:08,240 --> 00:28:11,279
of the evaluation of those statistics

00:28:10,320 --> 00:28:14,320
over time

00:28:11,279 --> 00:28:15,039
so it's kind of like the current

00:28:14,320 --> 00:28:16,399
ecosystem

00:28:15,039 --> 00:28:18,080
and of course there's a lot of

00:28:16,399 --> 00:28:19,279
integration that are being planned or

00:28:18,080 --> 00:28:22,399
people want to

00:28:19,279 --> 00:28:26,000
contribute things like prefect

00:28:22,399 --> 00:28:30,399
um other data quality libraries like dq

00:28:26,000 --> 00:28:33,200
um other processing framework like flink

00:28:30,399 --> 00:28:35,440
or looking into streaming as well and

00:28:33,200 --> 00:28:37,039
dealing with kafka and so on

00:28:35,440 --> 00:28:38,880
and so that's kind of to give you an

00:28:37,039 --> 00:28:40,799
overview of uh

00:28:38,880 --> 00:28:43,520
what already works with the project at

00:28:40,799 --> 00:28:43,520
the current time

00:28:43,600 --> 00:28:47,440
so when using with airflow um the way

00:28:46,240 --> 00:28:50,960
this work is you just

00:28:47,440 --> 00:28:54,720
install the marquez airflow

00:28:50,960 --> 00:28:58,640
uh python package and so to add

00:28:54,720 --> 00:28:59,360
um marquez as a plugin so to speak to

00:28:58,640 --> 00:29:01,279
your

00:28:59,360 --> 00:29:03,360
airflow integration and you can just

00:29:01,279 --> 00:29:04,960
configure it to point to your marquez

00:29:03,360 --> 00:29:08,240
instance and start collecting

00:29:04,960 --> 00:29:10,240
uh open lineage event

00:29:08,240 --> 00:29:11,440
when you use the spark integration

00:29:10,240 --> 00:29:14,240
there's a similar

00:29:11,440 --> 00:29:16,000
mechanism when you can use the spark

00:29:14,240 --> 00:29:18,720
driver extra java option

00:29:16,000 --> 00:29:19,600
to add the markers integration and point

00:29:18,720 --> 00:29:22,240
it to

00:29:19,600 --> 00:29:23,279
um the end point where you're going to

00:29:22,240 --> 00:29:26,559
be able to

00:29:23,279 --> 00:29:29,360
call it clean it

00:29:26,559 --> 00:29:31,840
and so metadata being collected uh

00:29:29,360 --> 00:29:33,840
across the board so of course lineage

00:29:31,840 --> 00:29:34,880
for each job what are the inputs and

00:29:33,840 --> 00:29:38,000
outputs

00:29:34,880 --> 00:29:39,520
um what was the data volume in the input

00:29:38,000 --> 00:29:40,399
and outputs and that's very important to

00:29:39,520 --> 00:29:42,880
start

00:29:40,399 --> 00:29:44,720
building you know the first step of data

00:29:42,880 --> 00:29:47,039
quality and keeping track of

00:29:44,720 --> 00:29:49,120
how much volume was produced and if

00:29:47,039 --> 00:29:51,440
there's any suspicious change

00:29:49,120 --> 00:29:54,080
and so that works across things like

00:29:51,440 --> 00:29:56,640
spark and bigquery for example

00:29:54,080 --> 00:29:58,399
it keeps track of the logical plan uh

00:29:56,640 --> 00:30:00,480
for example

00:29:58,399 --> 00:30:02,799
you can keep track of what the plan was

00:30:00,480 --> 00:30:06,000
for bigquery what the plan was for

00:30:02,799 --> 00:30:06,559
um spark and see how that changes over

00:30:06,000 --> 00:30:10,159
time

00:30:06,559 --> 00:30:13,679
if that impacts your performance um

00:30:10,159 --> 00:30:15,600
it keeps track of the schema to see the

00:30:13,679 --> 00:30:17,520
schema the inputs and output has changed

00:30:15,600 --> 00:30:19,840
in what particular run of a job

00:30:17,520 --> 00:30:23,840
has changed the schema of the output how

00:30:19,840 --> 00:30:23,840
long it took to process and so on

00:30:24,320 --> 00:30:27,600
and so when we look at the model there's

00:30:26,720 --> 00:30:31,279
also

00:30:27,600 --> 00:30:34,159
often a notion of nested jobs

00:30:31,279 --> 00:30:36,240
right if we think about airflow airflow

00:30:34,159 --> 00:30:38,080
will have a notion of dag which is the

00:30:36,240 --> 00:30:39,679
top level job in airflow and then

00:30:38,080 --> 00:30:42,720
individual tasks

00:30:39,679 --> 00:30:46,159
inside your airflow dag are executed

00:30:42,720 --> 00:30:47,760
as independent run instance

00:30:46,159 --> 00:30:50,000
and then when you run a spark job for

00:30:47,760 --> 00:30:52,960
example you may have a python

00:30:50,000 --> 00:30:54,960
task in airflow that spawns a jvm

00:30:52,960 --> 00:30:56,960
process which is your spark job

00:30:54,960 --> 00:30:58,720
and this process will itself run

00:30:56,960 --> 00:31:00,960
multiple actions right like

00:30:58,720 --> 00:31:02,720
every time you write to something in

00:31:00,960 --> 00:31:04,159
your spark job it's it starts a

00:31:02,720 --> 00:31:06,320
different job

00:31:04,159 --> 00:31:07,679
so in the open lineage model each of

00:31:06,320 --> 00:31:09,600
those actions

00:31:07,679 --> 00:31:12,080
is collected independently and we keep

00:31:09,600 --> 00:31:15,600
track of this nesting notion

00:31:12,080 --> 00:31:20,240
through uh this notion of parent job

00:31:15,600 --> 00:31:20,240
and keeping track how this nesting works

00:31:21,919 --> 00:31:25,760
and so to give you an example you know

00:31:24,559 --> 00:31:28,799
since we

00:31:25,760 --> 00:31:30,960
thanks to this consistent naming of

00:31:28,799 --> 00:31:33,360
jobs and data set we can stitch back the

00:31:30,960 --> 00:31:35,440
lineage so you may have multiple

00:31:33,360 --> 00:31:36,720
workflows or multiple spark jobs that do

00:31:35,440 --> 00:31:38,480
multiple transformation

00:31:36,720 --> 00:31:40,720
and that helps teaching these things

00:31:38,480 --> 00:31:43,039
back together and build this lineage

00:31:40,720 --> 00:31:43,039
graph

00:31:44,320 --> 00:31:47,840
and so some examples and because we keep

00:31:46,799 --> 00:31:49,440
track of

00:31:47,840 --> 00:31:52,080
you know i talk about how we're keeping

00:31:49,440 --> 00:31:54,640
track of the road count in the output

00:31:52,080 --> 00:31:57,039
you can start keeping track of the

00:31:54,640 --> 00:32:00,159
evolution of the size of the data set

00:31:57,039 --> 00:32:02,159
over time right and for each of those

00:32:00,159 --> 00:32:03,279
updates it's actually connected to the

00:32:02,159 --> 00:32:06,960
lineage graph

00:32:03,279 --> 00:32:07,840
so you know exactly uh if the number of

00:32:06,960 --> 00:32:09,919
rows drop

00:32:07,840 --> 00:32:12,480
in a data set for example you know

00:32:09,919 --> 00:32:15,679
exactly what particular run

00:32:12,480 --> 00:32:16,640
of the job produce this drop and so that

00:32:15,679 --> 00:32:18,640
will help you

00:32:16,640 --> 00:32:20,159
actually investigate and keep track of

00:32:18,640 --> 00:32:21,919
that one level lineage

00:32:20,159 --> 00:32:23,440
to know exactly where this problem is

00:32:21,919 --> 00:32:26,080
coming from and you can

00:32:23,440 --> 00:32:27,279
easily correlate this to the upstream

00:32:26,080 --> 00:32:30,640
dependencies

00:32:27,279 --> 00:32:33,760
that what data set you depending on

00:32:30,640 --> 00:32:34,320
is there a correlated drop in number of

00:32:33,760 --> 00:32:36,399
rows

00:32:34,320 --> 00:32:39,360
in an upstream data set for this

00:32:36,399 --> 00:32:39,360
particular version

00:32:39,679 --> 00:32:44,000
and um so that's you know the rock on by

00:32:42,240 --> 00:32:47,519
count is what is collected

00:32:44,000 --> 00:32:49,039
um across the board through the regular

00:32:47,519 --> 00:32:50,880
integration and if you use something

00:32:49,039 --> 00:32:53,120
like great expectation

00:32:50,880 --> 00:32:54,240
on top of that you can start also

00:32:53,120 --> 00:32:57,120
collecting

00:32:54,240 --> 00:32:57,600
column level metrics and start keeping

00:32:57,120 --> 00:32:59,519
track

00:32:57,600 --> 00:33:01,279
uh for each individual method so not

00:32:59,519 --> 00:33:03,600
just like having a course growing data

00:33:01,279 --> 00:33:06,640
quality matrix like number of rows

00:33:03,600 --> 00:33:08,480
but also adding uh

00:33:06,640 --> 00:33:10,480
information like the distribution of a

00:33:08,480 --> 00:33:13,600
column a number of nulls

00:33:10,480 --> 00:33:14,320
and so on and keep track of those

00:33:13,600 --> 00:33:16,880
changes

00:33:14,320 --> 00:33:16,880
over time

00:33:21,200 --> 00:33:25,919
and so that's for the information we

00:33:24,159 --> 00:33:28,240
collect so of course you can join

00:33:25,919 --> 00:33:29,440
the conversation open lineage and

00:33:28,240 --> 00:33:32,720
marquez are part

00:33:29,440 --> 00:33:35,039
of the lfi and data foundation

00:33:32,720 --> 00:33:37,039
uh which is uh you know a sister

00:33:35,039 --> 00:33:39,600
foundation of the cncf they're both

00:33:37,039 --> 00:33:40,720
under the umbrella of the linux

00:33:39,600 --> 00:33:43,919
foundation

00:33:40,720 --> 00:33:46,960
um which really enforces that

00:33:43,919 --> 00:33:48,880
those projects have a clear governance

00:33:46,960 --> 00:33:50,000
they're not owned by anybody and it's

00:33:48,880 --> 00:33:52,880
really kind of

00:33:50,000 --> 00:33:54,000
a community driven effort and it's

00:33:52,880 --> 00:33:57,120
really

00:33:54,000 --> 00:33:59,360
owned by the community for the community

00:33:57,120 --> 00:34:00,399
so you can check out those projects on

00:33:59,360 --> 00:34:02,720
github

00:34:00,399 --> 00:34:04,240
um you can join the slack if you have

00:34:02,720 --> 00:34:05,840
questions if you're curious about

00:34:04,240 --> 00:34:08,079
learning more

00:34:05,840 --> 00:34:09,200
they also of course have a twitter

00:34:08,079 --> 00:34:13,760
account

00:34:09,200 --> 00:34:17,119
um and so we are looking forward

00:34:13,760 --> 00:34:17,119
um to see you

00:34:17,679 --> 00:34:21,839
join the conversation

00:34:24,320 --> 00:34:31,200
and on this um this is it for my

00:34:28,000 --> 00:34:31,200
presentation today

00:34:31,440 --> 00:34:36,879
okay so i am just checking within a half

00:34:35,280 --> 00:34:39,760
question so far so maybe

00:34:36,879 --> 00:34:41,119
i will start with one question um i'm

00:34:39,760 --> 00:34:42,960
really excited to see that

00:34:41,119 --> 00:34:44,159
somehow it's like in the last decade we

00:34:42,960 --> 00:34:48,159
have been trying to solve

00:34:44,159 --> 00:34:48,480
uh how to do that uh in a massive scale

00:34:48,159 --> 00:34:50,800
but

00:34:48,480 --> 00:34:52,960
not resolving how to do data proper data

00:34:50,800 --> 00:34:55,040
engineering like the top level

00:34:52,960 --> 00:34:56,720
of this and and finally it's nice to see

00:34:55,040 --> 00:34:57,839
that this is starting to happen so this

00:34:56,720 --> 00:35:00,640
is pretty cool

00:34:57,839 --> 00:35:02,560
uh my question is about because since

00:35:00,640 --> 00:35:04,480
this is observability

00:35:02,560 --> 00:35:06,160
what sort of synergies you see with the

00:35:04,480 --> 00:35:07,520
other observability projects from the

00:35:06,160 --> 00:35:10,960
cncf site

00:35:07,520 --> 00:35:14,079
because some are similar like well

00:35:10,960 --> 00:35:14,079
in the same space let's say

00:35:14,320 --> 00:35:19,920
oh um yes so i think

00:35:17,760 --> 00:35:21,200
clearly to me the only way we can solve

00:35:19,920 --> 00:35:23,040
this is through

00:35:21,200 --> 00:35:26,160
open source and having this kind of

00:35:23,040 --> 00:35:27,440
project that helps defining a standard

00:35:26,160 --> 00:35:29,680
because otherwise there's so much

00:35:27,440 --> 00:35:32,160
complexity in all those projects and so

00:35:29,680 --> 00:35:34,960
much fragmentation

00:35:32,160 --> 00:35:36,480
you really want to have uh develop this

00:35:34,960 --> 00:35:39,040
standard and really enable

00:35:36,480 --> 00:35:40,480
everybody to expose the lineage in a

00:35:39,040 --> 00:35:42,480
standard way

00:35:40,480 --> 00:35:43,839
so which is what we're doing and really

00:35:42,480 --> 00:35:46,880
yes there's lots

00:35:43,839 --> 00:35:48,560
like in many ways the service space

00:35:46,880 --> 00:35:49,920
is a lot more mature like service

00:35:48,560 --> 00:35:52,880
observability

00:35:49,920 --> 00:35:54,240
is a lot more mature than data pipelines

00:35:52,880 --> 00:35:55,599
observability

00:35:54,240 --> 00:35:57,760
like you were mentioning right we've

00:35:55,599 --> 00:35:59,520
been focusing on this big data aspect

00:35:57,760 --> 00:36:01,680
and people were caring about

00:35:59,520 --> 00:36:03,040
how do we scale processing how do we

00:36:01,680 --> 00:36:06,160
consume produce

00:36:03,040 --> 00:36:08,720
so me so much data but the other problem

00:36:06,160 --> 00:36:10,000
is not just the the volume of data

00:36:08,720 --> 00:36:11,839
aspect but it's like

00:36:10,000 --> 00:36:14,880
how many data sets we have like the

00:36:11,839 --> 00:36:16,079
volume of many tables many jobs many

00:36:14,880 --> 00:36:18,079
transformations

00:36:16,079 --> 00:36:19,440
and many people being involved in it

00:36:18,079 --> 00:36:22,480
right so it's kind of like

00:36:19,440 --> 00:36:24,400
the big data problem is also a size of

00:36:22,480 --> 00:36:25,440
the organization of like the lack of

00:36:24,400 --> 00:36:28,480
visibility

00:36:25,440 --> 00:36:30,000
a number of data set problem which is

00:36:28,480 --> 00:36:33,440
what observability

00:36:30,000 --> 00:36:35,920
um helps solving and so right that's why

00:36:33,440 --> 00:36:37,040
opening it's really his name it's really

00:36:35,920 --> 00:36:40,240
taking

00:36:37,040 --> 00:36:41,200
like top open telemetry as a template

00:36:40,240 --> 00:36:45,040
right

00:36:41,200 --> 00:36:47,680
um it's like open telemetry standardizes

00:36:45,040 --> 00:36:48,160
collecting traces and these traces are

00:36:47,680 --> 00:36:50,720
really

00:36:48,160 --> 00:36:51,920
traces of services is really the lineage

00:36:50,720 --> 00:36:55,520
of data

00:36:51,920 --> 00:36:58,560
and so eventually you know you ever

00:36:55,520 --> 00:36:59,040
even would uh connect both right when

00:36:58,560 --> 00:37:01,040
you

00:36:59,040 --> 00:37:03,119
um collect your lineage if you're caring

00:37:01,040 --> 00:37:05,520
about the sql level lineage

00:37:03,119 --> 00:37:07,920
and then you go up to the data that as

00:37:05,520 --> 00:37:09,119
it comes in it may be coming from a

00:37:07,920 --> 00:37:11,839
kafka topic

00:37:09,119 --> 00:37:13,920
and a disk topic some service is writing

00:37:11,839 --> 00:37:15,839
to it because you're instrumenting them

00:37:13,920 --> 00:37:18,000
and the service is writing to this kafka

00:37:15,839 --> 00:37:19,359
topic because it's receiving a request

00:37:18,000 --> 00:37:21,760
from another service

00:37:19,359 --> 00:37:23,599
so really the lineage goes all the way

00:37:21,760 --> 00:37:24,960
upstream in services as well and that's

00:37:23,599 --> 00:37:27,520
where you start

00:37:24,960 --> 00:37:28,880
you could start connecting open

00:37:27,520 --> 00:37:32,240
telemetry traces

00:37:28,880 --> 00:37:34,839
with the open lineage uh lineage events

00:37:32,240 --> 00:37:37,680
and start like understanding lineage

00:37:34,839 --> 00:37:40,880
from a click on the website

00:37:37,680 --> 00:37:42,800
all the way to your tableau dashboard or

00:37:40,880 --> 00:37:44,000
you know whether you use tableau or look

00:37:42,800 --> 00:37:46,560
here or

00:37:44,000 --> 00:37:47,680
all of those things right so it's really

00:37:46,560 --> 00:37:50,720
the eventual

00:37:47,680 --> 00:37:52,400
goal of open lineage is to

00:37:50,720 --> 00:37:54,640
really be able to cover this whole

00:37:52,400 --> 00:37:57,839
picture and connect everything

00:37:54,640 --> 00:37:59,520
uh together and understand from an

00:37:57,839 --> 00:38:00,720
observability standpoint every

00:37:59,520 --> 00:38:03,839
everything that's happening and where

00:38:00,720 --> 00:38:04,720
it's coming from and why it's happening

00:38:03,839 --> 00:38:06,720
okay

00:38:04,720 --> 00:38:09,839
thank you i'm just going to check that's

00:38:06,720 --> 00:38:09,839
another question maybe

00:38:10,240 --> 00:38:14,160
yes there is more i'm going to ask first

00:38:12,400 --> 00:38:16,800
one who has three votes

00:38:14,160 --> 00:38:18,640
any challenges specific to streaming how

00:38:16,800 --> 00:38:21,680
jobs would be identified by

00:38:18,640 --> 00:38:22,640
for instance yeah so you know in

00:38:21,680 --> 00:38:25,839
streaming

00:38:22,640 --> 00:38:26,720
i think the the the intuition is like a

00:38:25,839 --> 00:38:29,119
streaming job

00:38:26,720 --> 00:38:30,960
is a constantly running job right versus

00:38:29,119 --> 00:38:32,560
a bad job that runs like every hour or

00:38:30,960 --> 00:38:35,200
something

00:38:32,560 --> 00:38:36,000
but actually a streaming job still has

00:38:35,200 --> 00:38:39,040
discrete

00:38:36,000 --> 00:38:41,040
runs right so you can identify a

00:38:39,040 --> 00:38:42,960
streaming job and the high level lineage

00:38:41,040 --> 00:38:44,480
still works the same like as a streaming

00:38:42,960 --> 00:38:47,760
job will have a

00:38:44,480 --> 00:38:50,160
unique name that identifies it and

00:38:47,760 --> 00:38:52,160
kafka topics can be identified you know

00:38:50,160 --> 00:38:53,520
you have a topic in a broker so you can

00:38:52,160 --> 00:38:55,839
still

00:38:53,520 --> 00:38:57,760
build those data sets out of them so of

00:38:55,839 --> 00:39:00,000
course the metadata is different

00:38:57,760 --> 00:39:02,400
then when you start your streaming job

00:39:00,000 --> 00:39:04,480
you'll have a start event

00:39:02,400 --> 00:39:06,400
that says like oh this particular one of

00:39:04,480 --> 00:39:08,240
the streaming job is starting

00:39:06,400 --> 00:39:10,079
are you interested in capturing what's

00:39:08,240 --> 00:39:12,560
the version of the source code

00:39:10,079 --> 00:39:13,200
um words started reading in the topic

00:39:12,560 --> 00:39:15,280
you may be

00:39:13,200 --> 00:39:16,960
an on the kafka level for example you'd

00:39:15,280 --> 00:39:18,880
be interested in capturing the offsets

00:39:16,960 --> 00:39:20,560
of where you started reading

00:39:18,880 --> 00:39:22,079
and then at some point in the future

00:39:20,560 --> 00:39:24,640
you're going to stop this job

00:39:22,079 --> 00:39:25,359
and maybe upgrade dependencies or like

00:39:24,640 --> 00:39:28,079
update

00:39:25,359 --> 00:39:28,880
and redeploy it and so it's going to

00:39:28,079 --> 00:39:30,560
stop and you

00:39:28,880 --> 00:39:32,400
are going to redeploy a new version of

00:39:30,560 --> 00:39:34,320
the code and start it again and it's

00:39:32,400 --> 00:39:34,880
going to resume where it stops reading

00:39:34,320 --> 00:39:37,680
from

00:39:34,880 --> 00:39:39,359
the kafka topic so typically when you

00:39:37,680 --> 00:39:40,560
map it to open lineage you would think

00:39:39,359 --> 00:39:43,359
about

00:39:40,560 --> 00:39:45,040
having a start event and an end event

00:39:43,359 --> 00:39:46,800
and capturing where you stop trading in

00:39:45,040 --> 00:39:48,079
a kafka topic like by capturing the

00:39:46,800 --> 00:39:50,720
offsets for example

00:39:48,079 --> 00:39:52,000
from an observability standpoint uh and

00:39:50,720 --> 00:39:54,720
then we start again

00:39:52,000 --> 00:39:56,160
with a new version and possibly it might

00:39:54,720 --> 00:39:58,640
be changing the schema

00:39:56,160 --> 00:40:00,800
in its output if you updated your logic

00:39:58,640 --> 00:40:03,280
and you're writing different information

00:40:00,800 --> 00:40:04,480
so depending of what type of data

00:40:03,280 --> 00:40:06,560
modeling you use

00:40:04,480 --> 00:40:08,319
you may be using avro you might be using

00:40:06,560 --> 00:40:09,599
protobuf you might be using a schema

00:40:08,319 --> 00:40:11,920
registry

00:40:09,599 --> 00:40:13,119
you know this is all sort of metadata

00:40:11,920 --> 00:40:15,520
you would want to capture

00:40:13,119 --> 00:40:17,280
at the time of the job restarted is

00:40:15,520 --> 00:40:19,040
starting writing this new schema

00:40:17,280 --> 00:40:21,359
right and make those observations and

00:40:19,040 --> 00:40:23,040
starting setting up a lineage event

00:40:21,359 --> 00:40:24,560
and opening each event don't have to be

00:40:23,040 --> 00:40:27,920
just the start at the end

00:40:24,560 --> 00:40:29,760
so you can send more event along the way

00:40:27,920 --> 00:40:31,440
for capturing information about oh the

00:40:29,760 --> 00:40:32,960
schema has changed at this particular

00:40:31,440 --> 00:40:36,480
point in time

00:40:32,960 --> 00:40:39,760
and keeping track of those things um

00:40:36,480 --> 00:40:41,680
as part of the metadata so um

00:40:39,760 --> 00:40:43,760
opening it started more on the batch

00:40:41,680 --> 00:40:45,040
side but definitely it's in the scope of

00:40:43,760 --> 00:40:47,520
capturing streaming

00:40:45,040 --> 00:40:48,480
and those are some like thoughts uh we

00:40:47,520 --> 00:40:50,960
had

00:40:48,480 --> 00:41:07,839
about where streaming modeling could be

00:40:50,960 --> 00:41:07,839
going to be part of that

00:41:13,280 --> 00:41:15,359

YouTube URL: https://www.youtube.com/watch?v=HEJFCQLwdtk


