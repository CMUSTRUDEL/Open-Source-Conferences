Title: Neelesh Salian – Building a metadata ecosystem using the Hive Metastore
Publication date: 2021-06-29
Playlist: Berlin Buzzwords 2021 #bbuzz
Description: 
	Metadata has been a key data infrastructure need since the beginning of our team's history at Stitch Fix.

We began this journey in 2015 with the setup of the Hive Metastore to work with Spark, Presto, and the rest of the platform infrastructure. But as our business needs grew, we felt the need to enhance and extend our metadata ecosystem.

In this talk, we want to share our journey of building additional capabilities with metadata to solve data and business challenges. Starting with our base infrastructure - the Hive Metastore, we will highlight each capability that led us to build the extensions into our present day metadata infrastructure. This includes improvements made to the Hive Metastore itself, extending the use of metadata beyond table schemas, and additional microservices we added to make access and use of metadata easier.

Building these capabilities has helped our team use metadata to power internal use cases. We want to share how we went about building this ecosystem and the lessons we learned along the way.

Speaker:
Neelesh Salian – https://2021.berlinbuzzwords.de/member/neelesh-salian

More: https://2021.berlinbuzzwords.de/session/building-metadata-ecosystem-using-hive-metastore
Captions: 
	00:00:08,559 --> 00:00:13,040
hi i'm nilesh and i want to talk to you

00:00:10,559 --> 00:00:13,840
today about how we built a metadata

00:00:13,040 --> 00:00:17,840
ecosystem

00:00:13,840 --> 00:00:20,160
using the hive metastore at stitchfix

00:00:17,840 --> 00:00:21,520
a little bit about myself i'm a software

00:00:20,160 --> 00:00:23,119
engineer in the stitch fixes data

00:00:21,520 --> 00:00:25,039
platform team where i currently work

00:00:23,119 --> 00:00:26,000
with things like apache spark patchy

00:00:25,039 --> 00:00:27,760
hive

00:00:26,000 --> 00:00:29,439
and solving data problems for data

00:00:27,760 --> 00:00:32,079
scientists i used to work

00:00:29,439 --> 00:00:33,120
cloudera where i worked on spark and

00:00:32,079 --> 00:00:35,040
mapreduce

00:00:33,120 --> 00:00:37,120
and i'm a contributor to the apache

00:00:35,040 --> 00:00:40,800
software foundation for a few projects

00:00:37,120 --> 00:00:41,760
for a long time so here's what i want to

00:00:40,800 --> 00:00:43,600
talk to you about

00:00:41,760 --> 00:00:45,440
today just giving you an overview of

00:00:43,600 --> 00:00:47,039
what stitch fix is

00:00:45,440 --> 00:00:50,000
talk to you about metadata particularly

00:00:47,039 --> 00:00:52,480
how we see it and what's useful to us

00:00:50,000 --> 00:00:54,079
the hype meta store where we began and

00:00:52,480 --> 00:00:56,239
how we started off with

00:00:54,079 --> 00:00:57,280
understanding the problem of metadata

00:00:56,239 --> 00:00:59,280
and then

00:00:57,280 --> 00:01:00,800
talking about building a metadata

00:00:59,280 --> 00:01:02,480
ecosystem around it

00:01:00,800 --> 00:01:03,840
and then finally leaving you off with

00:01:02,480 --> 00:01:05,600
some of the learnings we had

00:01:03,840 --> 00:01:08,080
and the future work that's planned in

00:01:05,600 --> 00:01:09,840
this area

00:01:08,080 --> 00:01:12,159
so what is stitchfix what does the

00:01:09,840 --> 00:01:13,119
company do it is a personalized styling

00:01:12,159 --> 00:01:15,360
service

00:01:13,119 --> 00:01:16,880
we have two avenues of the business you

00:01:15,360 --> 00:01:17,360
create your style profile essentially

00:01:16,880 --> 00:01:19,520
tell

00:01:17,360 --> 00:01:20,799
tell us what your style is and on the

00:01:19,520 --> 00:01:22,880
top you can either

00:01:20,799 --> 00:01:24,159
get five hand-picked items sent to you

00:01:22,880 --> 00:01:27,840
and you can keep what you like

00:01:24,159 --> 00:01:30,240
and send back the rest or

00:01:27,840 --> 00:01:31,200
versus you can have a personalized

00:01:30,240 --> 00:01:32,400
curated store

00:01:31,200 --> 00:01:34,640
where you can check out whatever you

00:01:32,400 --> 00:01:34,640
like

00:01:34,960 --> 00:01:38,560
data science has been the backbone of

00:01:36,400 --> 00:01:40,479
what we do behind the scenes and so

00:01:38,560 --> 00:01:42,960
i'm part of the algorithms organization

00:01:40,479 --> 00:01:44,159
which has about 145 plus data scientists

00:01:42,960 --> 00:01:45,840
and platform engineers

00:01:44,159 --> 00:01:47,280
these data scientists are mainly split

00:01:45,840 --> 00:01:50,159
into three verticals merge

00:01:47,280 --> 00:01:51,600
client and styling and the data platform

00:01:50,159 --> 00:01:56,240
sits horizontal to these

00:01:51,600 --> 00:01:58,240
organizations and this is

00:01:56,240 --> 00:01:59,840
this is embodied in the algorithms tour

00:01:58,240 --> 00:02:00,719
so feel free to check out when you get a

00:01:59,840 --> 00:02:02,240
chance to

00:02:00,719 --> 00:02:04,960
understand how data science is used

00:02:02,240 --> 00:02:06,560
behind the scenes

00:02:04,960 --> 00:02:08,160
jumping to the topic and giving you a

00:02:06,560 --> 00:02:10,000
bit more introduction i want to

00:02:08,160 --> 00:02:11,599
talk about metadata and what it means to

00:02:10,000 --> 00:02:13,520
us and

00:02:11,599 --> 00:02:16,720
start off there about the conversation

00:02:13,520 --> 00:02:18,560
about building an ecosystem

00:02:16,720 --> 00:02:20,000
so what does it look like essentially

00:02:18,560 --> 00:02:22,239
barebones definition would be

00:02:20,000 --> 00:02:24,480
data about your data and stored as a

00:02:22,239 --> 00:02:27,040
high level entity that can be consumed

00:02:24,480 --> 00:02:27,760
but in our case it's hive tables do we

00:02:27,040 --> 00:02:29,920
use that

00:02:27,760 --> 00:02:31,120
via the hive metastore to store

00:02:29,920 --> 00:02:33,840
structured data

00:02:31,120 --> 00:02:34,800
and what metadata holds for us is things

00:02:33,840 --> 00:02:38,000
like structure

00:02:34,800 --> 00:02:39,920
schema columns data types and location

00:02:38,000 --> 00:02:41,680
where in the underlying where is the

00:02:39,920 --> 00:02:42,640
underlying data located within the data

00:02:41,680 --> 00:02:44,319
warehouse

00:02:42,640 --> 00:02:46,319
and any additional contextual

00:02:44,319 --> 00:02:49,120
information about the entity

00:02:46,319 --> 00:02:49,120
that you might add

00:02:49,360 --> 00:02:52,720
so how do data scientists actually

00:02:50,800 --> 00:02:54,720
interact with metadata they

00:02:52,720 --> 00:02:56,080
they have two avenues to do that one is

00:02:54,720 --> 00:02:57,599
the rest server

00:02:56,080 --> 00:02:59,920
directly with the rest layer consuming

00:02:57,599 --> 00:03:01,680
metadata like looking at tables or

00:02:59,920 --> 00:03:02,640
interacting with whatever metadata is

00:03:01,680 --> 00:03:06,239
available

00:03:02,640 --> 00:03:07,760
or enter indirectly using it via engines

00:03:06,239 --> 00:03:12,560
so let's say you want to read a table

00:03:07,760 --> 00:03:12,560
right into a table via spark or presto

00:03:13,599 --> 00:03:19,840
so why is this important to us as a

00:03:16,800 --> 00:03:21,440
team in an organization and a company we

00:03:19,840 --> 00:03:23,599
we can read and write data and add

00:03:21,440 --> 00:03:25,280
metadata to it it gives meaning

00:03:23,599 --> 00:03:26,799
and the path to creating metadata

00:03:25,280 --> 00:03:29,280
essentially is sort of singular

00:03:26,799 --> 00:03:30,879
the source of truth is consistent and

00:03:29,280 --> 00:03:31,760
that's why the whole organization has

00:03:30,879 --> 00:03:34,480
the same view of

00:03:31,760 --> 00:03:36,080
all of the metadata and all our engines

00:03:34,480 --> 00:03:36,400
are distributed frameworks everything

00:03:36,080 --> 00:03:39,440
else

00:03:36,400 --> 00:03:40,400
uses the same metastore to read metadata

00:03:39,440 --> 00:03:42,480
each time

00:03:40,400 --> 00:03:44,080
and we're seeing the same structure all

00:03:42,480 --> 00:03:46,640
throughout the organization

00:03:44,080 --> 00:03:47,519
and finally metadata is helpful to

00:03:46,640 --> 00:03:49,920
document

00:03:47,519 --> 00:03:50,640
certain entities help and even auditing

00:03:49,920 --> 00:03:52,879
and help

00:03:50,640 --> 00:03:55,120
source information for e-tails like uh

00:03:52,879 --> 00:03:58,720
understanding where data lives and

00:03:55,120 --> 00:03:58,720
how to get certain pieces of data

00:03:58,799 --> 00:04:02,319
so jumping to the high metastore itself

00:04:00,640 --> 00:04:04,239
like where where we started off and

00:04:02,319 --> 00:04:05,360
uh i want to talk about some things that

00:04:04,239 --> 00:04:07,439
it couldn't do for us

00:04:05,360 --> 00:04:10,159
and uh that sort of gives you uh gives

00:04:07,439 --> 00:04:12,720
you an idea of where we're going

00:04:10,159 --> 00:04:14,000
so we chose the hive meta store the

00:04:12,720 --> 00:04:15,599
metastore metastores the the

00:04:14,000 --> 00:04:17,919
hypermetastore is the

00:04:15,599 --> 00:04:19,919
portion within hive that allows you to

00:04:17,919 --> 00:04:21,840
store and discover metadata

00:04:19,919 --> 00:04:24,320
but we chose it for its compatibility

00:04:21,840 --> 00:04:26,720
with things like apache spark and presto

00:04:24,320 --> 00:04:28,080
it helps provide basic metadata storage

00:04:26,720 --> 00:04:31,520
and discovery and it was easy to

00:04:28,080 --> 00:04:33,280
stand up so we set up a standalone

00:04:31,520 --> 00:04:34,639
hive metastore currently we're on the 2

00:04:33,280 --> 00:04:37,120
3x version

00:04:34,639 --> 00:04:38,560
with its own mysql database we use rds

00:04:37,120 --> 00:04:40,400
behind the scenes

00:04:38,560 --> 00:04:41,759
and i want to give you an idea of where

00:04:40,400 --> 00:04:44,320
where it's actually positioned in our

00:04:41,759 --> 00:04:47,600
infrastructure

00:04:44,320 --> 00:04:49,199
so going left to right this you'll see

00:04:47,600 --> 00:04:51,280
spark jobs notebooks

00:04:49,199 --> 00:04:53,199
we have a pandas interface where data

00:04:51,280 --> 00:04:55,199
scientists used to read and write data

00:04:53,199 --> 00:04:56,240
so all of this reads and writes data

00:04:55,199 --> 00:04:59,120
from the data warehouse

00:04:56,240 --> 00:05:00,639
which is a combination of amazon s3 for

00:04:59,120 --> 00:05:02,639
the actual files

00:05:00,639 --> 00:05:04,880
and the high metastore that we talked

00:05:02,639 --> 00:05:07,120
about at the center where

00:05:04,880 --> 00:05:08,080
metadata is stored about the data stored

00:05:07,120 --> 00:05:10,479
in s3

00:05:08,080 --> 00:05:11,360
presto is used to only read and kafka

00:05:10,479 --> 00:05:13,280
the event bus

00:05:11,360 --> 00:05:15,280
is writing data into the data warehouse

00:05:13,280 --> 00:05:16,080
as well and storing it as high level

00:05:15,280 --> 00:05:21,199
tables

00:05:16,080 --> 00:05:21,199
hive tables that are consumable by etls

00:05:21,440 --> 00:05:25,039
so where does this fall short it's a

00:05:22,800 --> 00:05:27,039
useful tool but we found

00:05:25,039 --> 00:05:29,360
being a bit limited and these limited

00:05:27,039 --> 00:05:31,919
limitations can be categorized as i

00:05:29,360 --> 00:05:33,199
think of them as absence of abstraction

00:05:31,919 --> 00:05:36,080
a bit more

00:05:33,199 --> 00:05:37,199
lack of stability and uh finally

00:05:36,080 --> 00:05:38,880
difficulty to

00:05:37,199 --> 00:05:40,880
sort of customize for any additional

00:05:38,880 --> 00:05:41,440
patterns than non-traditional patterns

00:05:40,880 --> 00:05:44,320
and

00:05:41,440 --> 00:05:45,199
the ones that we need so talking about

00:05:44,320 --> 00:05:47,600
abstraction

00:05:45,199 --> 00:05:49,600
we since we were only using the meta

00:05:47,600 --> 00:05:52,639
store from hive we had only thrift

00:05:49,600 --> 00:05:54,160
as an interface to use so we definitely

00:05:52,639 --> 00:05:54,960
needed an abstraction to use the

00:05:54,160 --> 00:05:56,560
metastore

00:05:54,960 --> 00:05:59,280
we couldn't expect data scientists for

00:05:56,560 --> 00:06:00,960
our customers to read create and write

00:05:59,280 --> 00:06:02,319
automated data without a sort of layer

00:06:00,960 --> 00:06:04,319
of abstraction and so

00:06:02,319 --> 00:06:06,000
we had to build something on our own

00:06:04,319 --> 00:06:09,360
rather than depend on something that was

00:06:06,000 --> 00:06:11,759
already available with stability

00:06:09,360 --> 00:06:12,880
larger deletions and updates were sort

00:06:11,759 --> 00:06:17,120
of concerning

00:06:12,880 --> 00:06:19,360
and uh some of the deletions took um

00:06:17,120 --> 00:06:21,039
took a concerning look because we we

00:06:19,360 --> 00:06:22,880
observed database locking that happened

00:06:21,039 --> 00:06:26,080
into some of those deletions

00:06:22,880 --> 00:06:27,120
we um we observed polling and repeated

00:06:26,080 --> 00:06:29,440
reads for certain

00:06:27,120 --> 00:06:31,120
objects in the meta store and so that

00:06:29,440 --> 00:06:33,039
was also a concern

00:06:31,120 --> 00:06:34,479
and some recalls were observably more

00:06:33,039 --> 00:06:36,000
faster if they go directly to the

00:06:34,479 --> 00:06:37,919
database and so we observed that the

00:06:36,000 --> 00:06:39,360
thrift layer itself was a bit slower and

00:06:37,919 --> 00:06:42,000
less stable for some of these larger

00:06:39,360 --> 00:06:42,000
method calls

00:06:42,240 --> 00:06:46,319
and finally the third limitation the

00:06:44,560 --> 00:06:48,080
support for custom patterns

00:06:46,319 --> 00:06:49,520
we had specific business cases that

00:06:48,080 --> 00:06:50,240
couldn't be directly solved by the

00:06:49,520 --> 00:06:52,000
metastore

00:06:50,240 --> 00:06:54,000
i'll address them in the upcoming slides

00:06:52,000 --> 00:06:55,440
but to give you an idea that we didn't

00:06:54,000 --> 00:06:57,520
want to address these by

00:06:55,440 --> 00:07:00,080
either forking high or patching it and

00:06:57,520 --> 00:07:01,680
changing its structure we'd be sort of

00:07:00,080 --> 00:07:03,520
hurting ourselves with a lot of the

00:07:01,680 --> 00:07:05,039
patches that we had to do

00:07:03,520 --> 00:07:06,960
and we didn't want to create something

00:07:05,039 --> 00:07:07,680
that is specific and out of the box that

00:07:06,960 --> 00:07:09,680
might be

00:07:07,680 --> 00:07:10,720
breaking compatibility with spark or

00:07:09,680 --> 00:07:14,479
presto

00:07:10,720 --> 00:07:16,240
eventually but despite this

00:07:14,479 --> 00:07:18,560
with these limitations didn't stop us

00:07:16,240 --> 00:07:20,800
from using the metastore we thought

00:07:18,560 --> 00:07:23,280
let's just build something around it to

00:07:20,800 --> 00:07:25,759
address these limitations uh since the

00:07:23,280 --> 00:07:28,479
metastore is more hive and be like we

00:07:25,759 --> 00:07:31,120
ended up naming most of our services

00:07:28,479 --> 00:07:35,039
somewhat be like and more appearing

00:07:31,120 --> 00:07:35,840
in nature so coming to the actual

00:07:35,039 --> 00:07:37,440
building of the

00:07:35,840 --> 00:07:38,960
the metadata ecosystem i want to give

00:07:37,440 --> 00:07:41,440
you an overview of what it is

00:07:38,960 --> 00:07:44,639
and i'll take you to each part and talk

00:07:41,440 --> 00:07:46,960
about why we built each of them

00:07:44,639 --> 00:07:48,800
so this is how it stands today this is

00:07:46,960 --> 00:07:50,479
how it looks going from left to right

00:07:48,800 --> 00:07:53,520
you have the user facing layer where

00:07:50,479 --> 00:07:55,599
clients are used by data scientists

00:07:53,520 --> 00:07:57,759
and those go into the back end layer be

00:07:55,599 --> 00:07:58,000
it bumblebee and i'll talk about each of

00:07:57,759 --> 00:08:00,639
those

00:07:58,000 --> 00:08:01,199
red uh red marked boxes because those

00:08:00,639 --> 00:08:04,319
are the

00:08:01,199 --> 00:08:05,840
the key elements uh we have spark emr we

00:08:04,319 --> 00:08:08,400
have presto clusters

00:08:05,840 --> 00:08:10,080
and these go into our metastar proxy and

00:08:08,400 --> 00:08:11,840
then finally towards the extreme right

00:08:10,080 --> 00:08:13,599
you see the

00:08:11,840 --> 00:08:15,759
standalone high metastore that i spoke

00:08:13,599 --> 00:08:16,479
about and that there's a kafka topic

00:08:15,759 --> 00:08:18,160
emitting

00:08:16,479 --> 00:08:19,680
events from there i'll talk about each

00:08:18,160 --> 00:08:22,800
of these things but i wanted to give you

00:08:19,680 --> 00:08:24,240
an overview at the beginning

00:08:22,800 --> 00:08:28,160
so let's look at how we tackled

00:08:24,240 --> 00:08:28,160
specifically abstraction and stability

00:08:28,800 --> 00:08:32,959
so data scientists needed more

00:08:31,120 --> 00:08:34,479
expressiveness with metadata like actual

00:08:32,959 --> 00:08:36,640
crud operations like create

00:08:34,479 --> 00:08:38,320
read any kind of update delete for

00:08:36,640 --> 00:08:39,279
metadata and how do we do that so that

00:08:38,320 --> 00:08:40,880
was sort of a

00:08:39,279 --> 00:08:42,479
question mark in our ecosystem at the

00:08:40,880 --> 00:08:43,360
beginning since the metastore stood

00:08:42,479 --> 00:08:45,040
alone

00:08:43,360 --> 00:08:47,120
and uh it didn't have sort of this

00:08:45,040 --> 00:08:48,880
abstraction layer

00:08:47,120 --> 00:08:50,480
and so what we needed was a layer that

00:08:48,880 --> 00:08:52,800
wraps the class structure

00:08:50,480 --> 00:08:55,440
of hive table like table database

00:08:52,800 --> 00:08:58,320
partitions the basic metadata objects

00:08:55,440 --> 00:08:59,600
and essentially we needed a readily

00:08:58,320 --> 00:09:01,760
accessible api

00:08:59,600 --> 00:09:03,519
and a python client some python is a the

00:09:01,760 --> 00:09:04,480
default language for our data scientist

00:09:03,519 --> 00:09:07,200
teams

00:09:04,480 --> 00:09:09,760
and this wanted to we wanted to allow

00:09:07,200 --> 00:09:12,080
them more expressiveness with metadata

00:09:09,760 --> 00:09:13,839
and especially one other thing was

00:09:12,080 --> 00:09:16,399
making metadata available as a first

00:09:13,839 --> 00:09:19,040
class field so the hive table would have

00:09:16,399 --> 00:09:19,760
a sort of generic metadata field to add

00:09:19,040 --> 00:09:21,760
additional

00:09:19,760 --> 00:09:25,440
data that would come from data

00:09:21,760 --> 00:09:27,920
scientists specifically for their needs

00:09:25,440 --> 00:09:29,200
with regards to stability uh we noticed

00:09:27,920 --> 00:09:30,800
the hive metastore couldn't handle all

00:09:29,200 --> 00:09:32,720
the requests like we couldn't

00:09:30,800 --> 00:09:34,320
bombard it directly with all the

00:09:32,720 --> 00:09:35,760
department's needs

00:09:34,320 --> 00:09:37,440
yes it's the source of truth but we

00:09:35,760 --> 00:09:38,720
needed some sort of layering before it

00:09:37,440 --> 00:09:41,440
we had to handle

00:09:38,720 --> 00:09:42,880
requests with spark presto dashboards

00:09:41,440 --> 00:09:44,320
and things like that so we needed sort

00:09:42,880 --> 00:09:45,600
of this layer to handle some of those

00:09:44,320 --> 00:09:48,480
requests

00:09:45,600 --> 00:09:49,120
a caching mechanism was sought for uh

00:09:48,480 --> 00:09:50,959
thought to be

00:09:49,120 --> 00:09:52,800
the easier solution to relieve some of

00:09:50,959 --> 00:09:55,279
those repeated calls

00:09:52,800 --> 00:09:57,040
and for some method calls they were

00:09:55,279 --> 00:09:58,959
observably larger and we needed to

00:09:57,040 --> 00:10:01,360
bypass thrift to go directly to the

00:09:58,959 --> 00:10:03,760
database

00:10:01,360 --> 00:10:05,519
so enter bumblebee like you see the be

00:10:03,760 --> 00:10:07,600
naming that's coming here

00:10:05,519 --> 00:10:09,600
it's a rest server with its python

00:10:07,600 --> 00:10:12,000
client and what we did was

00:10:09,600 --> 00:10:12,720
first of all abstracted the hive classes

00:10:12,000 --> 00:10:14,399
and

00:10:12,720 --> 00:10:15,760
to make it more convenient for usage we

00:10:14,399 --> 00:10:16,720
returned the python dictionaries to

00:10:15,760 --> 00:10:19,120
represent them

00:10:16,720 --> 00:10:20,560
so each of the table database partition

00:10:19,120 --> 00:10:21,839
would all be represented as python

00:10:20,560 --> 00:10:25,279
dictionaries

00:10:21,839 --> 00:10:26,880
and like i said metadata was importantly

00:10:25,279 --> 00:10:28,000
needed to be a structure within the hive

00:10:26,880 --> 00:10:29,839
table and so

00:10:28,000 --> 00:10:31,519
that again became a python dictionary

00:10:29,839 --> 00:10:32,000
within a table for any auxiliary

00:10:31,519 --> 00:10:34,480
information

00:10:32,000 --> 00:10:36,079
that you might need to add with respect

00:10:34,480 --> 00:10:37,120
to stability like i mentioned some of

00:10:36,079 --> 00:10:38,959
those calls were

00:10:37,120 --> 00:10:40,800
things like get partitions get all

00:10:38,959 --> 00:10:43,680
tables which are more heavier

00:10:40,800 --> 00:10:44,399
and it made sense to revert them into

00:10:43,680 --> 00:10:47,760
making it

00:10:44,399 --> 00:10:49,920
read for mysql and so

00:10:47,760 --> 00:10:50,959
that was done for those particular calls

00:10:49,920 --> 00:10:53,440
especially

00:10:50,959 --> 00:10:54,480
and finally bumblebee added a tiny cache

00:10:53,440 --> 00:10:56,000
that allowed

00:10:54,480 --> 00:10:57,279
quicker loads of table objects that were

00:10:56,000 --> 00:10:59,839
frequently accessed so some of the

00:10:57,279 --> 00:11:01,120
production tables were heavy hit and so

00:10:59,839 --> 00:11:02,959
this cache sort of relieved that

00:11:01,120 --> 00:11:06,079
pressure of reading

00:11:02,959 --> 00:11:08,720
a repeated object

00:11:06,079 --> 00:11:10,240
so the this is how it looks the

00:11:08,720 --> 00:11:12,000
bumblebee server essentially talks to

00:11:10,240 --> 00:11:14,079
the metastore directly

00:11:12,000 --> 00:11:16,320
and it makes separate calls to the my

00:11:14,079 --> 00:11:17,440
sequel database for some operations like

00:11:16,320 --> 00:11:20,000
i mentioned

00:11:17,440 --> 00:11:20,560
and to the bottom the bumblebee client

00:11:20,000 --> 00:11:22,720
had

00:11:20,560 --> 00:11:24,560
the ability to crud operations on hive

00:11:22,720 --> 00:11:26,320
artifacts set metadata

00:11:24,560 --> 00:11:28,079
and i'll talk about the ownership bit in

00:11:26,320 --> 00:11:30,399
a bit but uh it allowed

00:11:28,079 --> 00:11:33,360
data scientists to do that as well by

00:11:30,399 --> 00:11:36,560
interacting with the server

00:11:33,360 --> 00:11:38,000
so in a python job this was how you you

00:11:36,560 --> 00:11:39,440
would use bumblebee

00:11:38,000 --> 00:11:41,680
you would import the client and try to

00:11:39,440 --> 00:11:42,079
get an object and that's what's returned

00:11:41,680 --> 00:11:44,000
to you

00:11:42,079 --> 00:11:45,680
as a python dictionary like i mentioned

00:11:44,000 --> 00:11:46,560
so to access each of this information

00:11:45,680 --> 00:11:49,519
you would just do

00:11:46,560 --> 00:11:50,399
a key search on the the object return

00:11:49,519 --> 00:11:53,360
and you would

00:11:50,399 --> 00:11:54,959
get returned the um the need the object

00:11:53,360 --> 00:11:58,320
that you need like metadata column

00:11:54,959 --> 00:12:00,079
or name itself so

00:11:58,320 --> 00:12:02,079
we have empowered the data scientists

00:12:00,079 --> 00:12:03,920
with metadata but

00:12:02,079 --> 00:12:05,120
like uncle ben and spider-man said with

00:12:03,920 --> 00:12:06,880
great metadata comes great

00:12:05,120 --> 00:12:09,200
responsibilities and so

00:12:06,880 --> 00:12:10,560
it it made sense to sort of keep these

00:12:09,200 --> 00:12:12,800
things in

00:12:10,560 --> 00:12:14,079
in balance and so we had to think a

00:12:12,800 --> 00:12:16,399
little bit more beyond

00:12:14,079 --> 00:12:18,000
just expressiveness so we started off

00:12:16,399 --> 00:12:19,920
with the idea of ownership

00:12:18,000 --> 00:12:21,839
and making sure that data scientists

00:12:19,920 --> 00:12:23,680
owned each artifact that they create

00:12:21,839 --> 00:12:25,680
and so the bumblebee client was sort of

00:12:23,680 --> 00:12:26,880
augmented to set these ownerships on

00:12:25,680 --> 00:12:28,959
hive tables

00:12:26,880 --> 00:12:31,040
and data scientists had to declare this

00:12:28,959 --> 00:12:33,760
ownership widely create and

00:12:31,040 --> 00:12:35,519
to indicate ownership of a table but we

00:12:33,760 --> 00:12:37,519
needed to create maintain a clean

00:12:35,519 --> 00:12:38,000
metadata ecosystem and not just like

00:12:37,519 --> 00:12:40,240
having

00:12:38,000 --> 00:12:42,480
own tables but sort of an accountability

00:12:40,240 --> 00:12:44,639
of why tables existed

00:12:42,480 --> 00:12:47,519
so clearly ownership wasn't enough we

00:12:44,639 --> 00:12:50,560
had we needed to have more context and

00:12:47,519 --> 00:12:51,040
more purposeful metadata so we asked

00:12:50,560 --> 00:12:53,519
ourselves

00:12:51,040 --> 00:12:55,360
questions like why this table exists can

00:12:53,519 --> 00:12:56,800
we declare its purpose what is its what

00:12:55,360 --> 00:13:00,320
is it used for

00:12:56,800 --> 00:13:01,600
what code writes to this table can we

00:13:00,320 --> 00:13:03,360
clean this table up if it's not

00:13:01,600 --> 00:13:05,440
referenced or not used

00:13:03,360 --> 00:13:06,399
and finally we we came up with these

00:13:05,440 --> 00:13:08,240
questions as a

00:13:06,399 --> 00:13:10,639
sort of a workflow to add additional

00:13:08,240 --> 00:13:13,200
metadata to hive tables so that we

00:13:10,639 --> 00:13:14,320
we come to a point where we know why a

00:13:13,200 --> 00:13:16,000
table exists

00:13:14,320 --> 00:13:18,880
in in the meta store and it's not just

00:13:16,000 --> 00:13:20,720
there for uh for no reason and so

00:13:18,880 --> 00:13:22,480
this workflow is really helpful to

00:13:20,720 --> 00:13:24,959
address that

00:13:22,480 --> 00:13:25,839
and this is how it looks like you create

00:13:24,959 --> 00:13:30,240
a hive table

00:13:25,839 --> 00:13:32,480
a metadata service would notify the

00:13:30,240 --> 00:13:34,639
table owner and they go wrong they go

00:13:32,480 --> 00:13:37,440
with two rounds of metadata update

00:13:34,639 --> 00:13:38,160
so in the first round uh the owner would

00:13:37,440 --> 00:13:40,480
essentially

00:13:38,160 --> 00:13:42,079
link the table code add comments check

00:13:40,480 --> 00:13:44,880
schema or any kind of

00:13:42,079 --> 00:13:46,240
sort of auditing work of the of the

00:13:44,880 --> 00:13:48,639
table itself

00:13:46,240 --> 00:13:51,040
and pass it on to another team member to

00:13:48,639 --> 00:13:53,040
confirm it and to just sort of uh

00:13:51,040 --> 00:13:54,560
have a second look at whatever the

00:13:53,040 --> 00:13:56,959
metadata is included

00:13:54,560 --> 00:13:57,760
and then they could either reject it or

00:13:56,959 --> 00:13:59,440
notify it

00:13:57,760 --> 00:14:00,959
and say hey you could change something

00:13:59,440 --> 00:14:03,839
or they could confirm it and then

00:14:00,959 --> 00:14:05,600
finally the metadata itself is updated

00:14:03,839 --> 00:14:07,120
on the table and so now we have this

00:14:05,600 --> 00:14:08,880
complete picture of why this table

00:14:07,120 --> 00:14:12,160
exists

00:14:08,880 --> 00:14:13,920
so this is where increased metadata

00:14:12,160 --> 00:14:15,680
fit in like the metadata object i

00:14:13,920 --> 00:14:17,600
mentioned in the hive table

00:14:15,680 --> 00:14:19,279
is where all these pieces fit together

00:14:17,600 --> 00:14:21,279
so when it was confirmed

00:14:19,279 --> 00:14:22,399
let's say the review date and the table

00:14:21,279 --> 00:14:24,720
code

00:14:22,399 --> 00:14:26,480
and the owner so there's sort of this

00:14:24,720 --> 00:14:28,959
accountability of

00:14:26,480 --> 00:14:30,639
why this table exists and surprise

00:14:28,959 --> 00:14:32,639
surprise it's a table that stores data

00:14:30,639 --> 00:14:35,760
about clothes for an apparel company

00:14:32,639 --> 00:14:37,440
but that's um that's how we sort of gave

00:14:35,760 --> 00:14:39,680
this notion of uh

00:14:37,440 --> 00:14:40,959
why uh giving a purpose for a table with

00:14:39,680 --> 00:14:42,959
like things like comments

00:14:40,959 --> 00:14:44,320
as simple as that and so we built a

00:14:42,959 --> 00:14:46,079
service around this

00:14:44,320 --> 00:14:47,440
increased metadata that we already have

00:14:46,079 --> 00:14:49,199
now uh to

00:14:47,440 --> 00:14:51,040
allow us to check if the table has been

00:14:49,199 --> 00:14:52,639
referenced if there's jobs connected to

00:14:51,040 --> 00:14:54,079
it if it's been right written to and

00:14:52,639 --> 00:14:55,040
that's more complicated behind the

00:14:54,079 --> 00:14:57,279
scenes but

00:14:55,040 --> 00:15:00,639
the idea is that it helped us clean up a

00:14:57,279 --> 00:15:03,680
lot of those unused tables

00:15:00,639 --> 00:15:06,079
so now we talk about uh the third

00:15:03,680 --> 00:15:08,079
uh and final limitation that we observed

00:15:06,079 --> 00:15:12,000
essentially building custom patterns

00:15:08,079 --> 00:15:14,639
and support for them in our ecosystem

00:15:12,000 --> 00:15:16,560
so solving these um these are

00:15:14,639 --> 00:15:18,560
non-traditional ones we observe these as

00:15:16,560 --> 00:15:19,920
we went and grew as a business and so i

00:15:18,560 --> 00:15:21,600
want to talk about what

00:15:19,920 --> 00:15:22,959
what these two patterns were they sort

00:15:21,600 --> 00:15:24,880
of limited the

00:15:22,959 --> 00:15:26,000
related with the solutions that we came

00:15:24,880 --> 00:15:27,760
up with and so

00:15:26,000 --> 00:15:29,680
i sort of clubbed them together in terms

00:15:27,760 --> 00:15:31,600
of the the need and the

00:15:29,680 --> 00:15:33,040
the sort of piece of the infrastructure

00:15:31,600 --> 00:15:34,880
it generated

00:15:33,040 --> 00:15:36,480
the first one is essentially test data

00:15:34,880 --> 00:15:38,399
test data was coming in

00:15:36,480 --> 00:15:40,160
with regular data and was stored in the

00:15:38,399 --> 00:15:40,880
data warehouse so think of like sample

00:15:40,160 --> 00:15:43,440
client

00:15:40,880 --> 00:15:44,800
or any kind of testing you do with any

00:15:43,440 --> 00:15:46,639
kind of table

00:15:44,800 --> 00:15:49,120
and that passed on into a production

00:15:46,639 --> 00:15:50,880
table but we didn't want this

00:15:49,120 --> 00:15:52,720
test data to essentially create pro

00:15:50,880 --> 00:15:54,560
training for our algorithms

00:15:52,720 --> 00:15:56,000
so we needed a way that this test data

00:15:54,560 --> 00:15:58,560
would be isolated

00:15:56,000 --> 00:15:59,440
from within a hive table so if you read

00:15:58,560 --> 00:16:02,320
a table

00:15:59,440 --> 00:16:04,000
not you would not necessarily get you

00:16:02,320 --> 00:16:05,120
shouldn't necessarily get something with

00:16:04,000 --> 00:16:07,360
the test data

00:16:05,120 --> 00:16:08,399
and so we had to isolate that in a way

00:16:07,360 --> 00:16:11,120
that it was not

00:16:08,399 --> 00:16:12,320
hurting some of the algorithms and on

00:16:11,120 --> 00:16:14,720
the second one

00:16:12,320 --> 00:16:16,320
we observed and this was a pattern for

00:16:14,720 --> 00:16:18,079
growing up for a while that

00:16:16,320 --> 00:16:19,920
data scientists would create table views

00:16:18,079 --> 00:16:22,639
essentially tables that were mere

00:16:19,920 --> 00:16:25,600
pointers pointers to a tables partition

00:16:22,639 --> 00:16:27,360
so let's say a historical table has been

00:16:25,600 --> 00:16:30,720
storing data every day

00:16:27,360 --> 00:16:32,240
partitioned by date so a table view

00:16:30,720 --> 00:16:34,079
is something like a table that just

00:16:32,240 --> 00:16:35,440
points to whatever the latest partition

00:16:34,079 --> 00:16:38,320
of that table is

00:16:35,440 --> 00:16:39,600
and so these tables became abundant and

00:16:38,320 --> 00:16:41,360
the the cumbersome part

00:16:39,600 --> 00:16:42,959
was to actually having to update these

00:16:41,360 --> 00:16:43,440
tables each time a new partition was

00:16:42,959 --> 00:16:45,600
created

00:16:43,440 --> 00:16:46,480
so if the historical table wrote data

00:16:45,600 --> 00:16:49,120
for yesterday

00:16:46,480 --> 00:16:51,040
you would be updating uh your view table

00:16:49,120 --> 00:16:52,000
to act accordingly and point to the new

00:16:51,040 --> 00:16:54,240
partition

00:16:52,000 --> 00:16:55,279
and so it's sort of this extra piece of

00:16:54,240 --> 00:16:59,600
workflow that was

00:16:55,279 --> 00:17:01,680
uh becoming cumbersome and unnecessary

00:16:59,600 --> 00:17:04,000
so how do we solve this we couldn't

00:17:01,680 --> 00:17:05,360
essentially modify bumblebee it was it

00:17:04,000 --> 00:17:06,400
was something that needed to come from

00:17:05,360 --> 00:17:08,720
the meta store

00:17:06,400 --> 00:17:10,240
it would take a lot more work if we went

00:17:08,720 --> 00:17:11,280
through the hive route and patching the

00:17:10,240 --> 00:17:13,760
binaries

00:17:11,280 --> 00:17:14,480
and we sort of needed them in our

00:17:13,760 --> 00:17:16,480
thinking that

00:17:14,480 --> 00:17:18,640
we needed something like a metastore an

00:17:16,480 --> 00:17:21,039
interface that was both compatible

00:17:18,640 --> 00:17:22,400
which was important in spark and presto

00:17:21,039 --> 00:17:23,120
but it was sort of malleable and

00:17:22,400 --> 00:17:25,039
changeable

00:17:23,120 --> 00:17:26,319
in our way that it functions according

00:17:25,039 --> 00:17:28,079
to our needs

00:17:26,319 --> 00:17:29,840
and so we thought of this sort of

00:17:28,079 --> 00:17:32,480
interac indirection layer that

00:17:29,840 --> 00:17:34,160
worked behind the scenes and we sort of

00:17:32,480 --> 00:17:35,919
thought about thought of it as a proxy

00:17:34,160 --> 00:17:38,240
metastore i'll explain it that

00:17:35,919 --> 00:17:38,240
a bit

00:17:39,360 --> 00:17:43,919
so enter yellowjacket a bit of a

00:17:42,320 --> 00:17:46,400
superior insect if you will

00:17:43,919 --> 00:17:48,320
but we started off with to experiment

00:17:46,400 --> 00:17:51,200
with basic thrift code

00:17:48,320 --> 00:17:52,160
to see if we can route traffic uh from

00:17:51,200 --> 00:17:55,200
the meta store

00:17:52,160 --> 00:17:58,720
and act as an intermediary between

00:17:55,200 --> 00:18:00,799
uh our back end and a meta store and so

00:17:58,720 --> 00:18:02,559
every service now thought of this proxy

00:18:00,799 --> 00:18:04,240
as the meta store so to begin with we

00:18:02,559 --> 00:18:05,280
wanted to check if the methods were

00:18:04,240 --> 00:18:07,360
working and so we

00:18:05,280 --> 00:18:09,039
checked this with both spark and presto

00:18:07,360 --> 00:18:12,000
and they were both fine since we

00:18:09,039 --> 00:18:13,840
supported the method methods available

00:18:12,000 --> 00:18:15,679
so we call it yellowjacket a thrift

00:18:13,840 --> 00:18:17,120
server that essentially proxies the load

00:18:15,679 --> 00:18:19,600
balancer of the hype metastore

00:18:17,120 --> 00:18:20,640
and uses its database for internal

00:18:19,600 --> 00:18:22,720
queries

00:18:20,640 --> 00:18:23,760
and what it did was it supported all the

00:18:22,720 --> 00:18:26,080
queries

00:18:23,760 --> 00:18:27,120
uh that are available in all the methods

00:18:26,080 --> 00:18:29,760
sorry in the

00:18:27,120 --> 00:18:32,000
in the high metastore thrift layer so

00:18:29,760 --> 00:18:34,559
anything requests coming from spark or

00:18:32,000 --> 00:18:36,400
presto or bumblebee or any any other

00:18:34,559 --> 00:18:38,480
metadata service you would have

00:18:36,400 --> 00:18:40,320
it would get resolved because it's

00:18:38,480 --> 00:18:43,840
essentially the similar layer like the

00:18:40,320 --> 00:18:46,480
um like the meta store itself so now

00:18:43,840 --> 00:18:48,000
given this flexibility of having these

00:18:46,480 --> 00:18:49,440
method calls right in front of us we

00:18:48,000 --> 00:18:52,559
could overload these methods

00:18:49,440 --> 00:18:55,120
to adjust to our needs and be suitable

00:18:52,559 --> 00:18:56,799
to some of these patterns

00:18:55,120 --> 00:18:58,480
so this is where it fit right at the

00:18:56,799 --> 00:19:00,080
center yellow jacket uh

00:18:58,480 --> 00:19:01,760
you can see this is the back end layer

00:19:00,080 --> 00:19:04,400
coming from that earlier

00:19:01,760 --> 00:19:06,320
diagram i showed you with the bumblebee

00:19:04,400 --> 00:19:08,000
server the emr cluster

00:19:06,320 --> 00:19:09,679
the presto cluster all pointing to

00:19:08,000 --> 00:19:10,400
yellowjacket as if it were the meta

00:19:09,679 --> 00:19:12,400
store

00:19:10,400 --> 00:19:14,880
but yellow jacket would talk to a

00:19:12,400 --> 00:19:17,600
standalone metastore for all its calls

00:19:14,880 --> 00:19:18,720
and would call the db for certain uh

00:19:17,600 --> 00:19:20,640
aspects of the

00:19:18,720 --> 00:19:22,320
test data and the view solution and i'll

00:19:20,640 --> 00:19:24,559
talk about that in a bit

00:19:22,320 --> 00:19:25,520
and so this ended up becoming sort of

00:19:24,559 --> 00:19:27,600
the meta store

00:19:25,520 --> 00:19:28,559
where every service would hit and the

00:19:27,600 --> 00:19:30,720
standalone method

00:19:28,559 --> 00:19:31,600
uh standalone metastore would be left

00:19:30,720 --> 00:19:34,000
alone

00:19:31,600 --> 00:19:35,919
what it helped here is uh in addition we

00:19:34,000 --> 00:19:38,880
could upgrade things like the meta store

00:19:35,919 --> 00:19:40,000
we came from one to one one two two to

00:19:38,880 --> 00:19:42,400
two three x

00:19:40,000 --> 00:19:43,200
fairly recently and so yellow yellow

00:19:42,400 --> 00:19:47,360
jacket was

00:19:43,200 --> 00:19:50,799
uh key in that upgrade process as well

00:19:47,360 --> 00:19:52,960
so isolating um test data is the first

00:19:50,799 --> 00:19:55,360
solution that we tackled

00:19:52,960 --> 00:19:57,039
uh let's say we made this part of the

00:19:55,360 --> 00:20:00,240
workflow that if a high table

00:19:57,039 --> 00:20:00,799
was expecting or had expected test data

00:20:00,240 --> 00:20:02,400
the user

00:20:00,799 --> 00:20:04,240
who was creating the table would have to

00:20:02,400 --> 00:20:06,320
create a test data partition

00:20:04,240 --> 00:20:07,679
so just a partition column that was sort

00:20:06,320 --> 00:20:10,080
of towards the end

00:20:07,679 --> 00:20:12,000
and specifically we wrote one for the

00:20:10,080 --> 00:20:13,840
presence of test data in a record and

00:20:12,000 --> 00:20:15,280
zero for the absence of test data in a

00:20:13,840 --> 00:20:19,039
record so

00:20:15,280 --> 00:20:20,480
if a row had zero in a test data column

00:20:19,039 --> 00:20:23,679
it wouldn't have test data it was more

00:20:20,480 --> 00:20:26,000
production and one would be

00:20:23,679 --> 00:20:27,520
classifying it as test data and behind

00:20:26,000 --> 00:20:30,240
the scenes what we did was

00:20:27,520 --> 00:20:31,840
uh we hid this partition columns this

00:20:30,240 --> 00:20:35,520
column particularly so that we

00:20:31,840 --> 00:20:36,159
know that um we can hide test data when

00:20:35,520 --> 00:20:38,480
needed

00:20:36,159 --> 00:20:41,280
and we did this by overloading the get

00:20:38,480 --> 00:20:43,679
table method and adding a decorator so

00:20:41,280 --> 00:20:44,880
if the name included a sort of pattern

00:20:43,679 --> 00:20:46,799
like like i should

00:20:44,880 --> 00:20:48,000
like it's listed below the double

00:20:46,799 --> 00:20:51,440
underscore include test

00:20:48,000 --> 00:20:53,280
data either yellowjacket method would

00:20:51,440 --> 00:20:54,480
know that you're asking explicitly for

00:20:53,280 --> 00:20:57,840
test data and it would

00:20:54,480 --> 00:20:59,280
react as a react the appropriate way

00:20:57,840 --> 00:21:01,919
i'll give you an example to highlight

00:20:59,280 --> 00:21:02,320
this but this decorator pattern helped

00:21:01,919 --> 00:21:05,679
us

00:21:02,320 --> 00:21:08,080
um save a lot of pain while

00:21:05,679 --> 00:21:09,200
doing this sort of isolation so let's

00:21:08,080 --> 00:21:12,400
look at the table

00:21:09,200 --> 00:21:14,240
test table with a test data partition

00:21:12,400 --> 00:21:16,000
so if you read this table naturally you

00:21:14,240 --> 00:21:17,679
would get always the production data so

00:21:16,000 --> 00:21:19,919
yellowjacket will just think of it as

00:21:17,679 --> 00:21:21,280
a regular table and would the

00:21:19,919 --> 00:21:24,000
indirection would just return

00:21:21,280 --> 00:21:24,320
all test data equals zero because you're

00:21:24,000 --> 00:21:26,799
not

00:21:24,320 --> 00:21:28,640
explicitly asking for it and if you're

00:21:26,799 --> 00:21:30,640
asking for test data you would

00:21:28,640 --> 00:21:31,760
do the decorator pattern like include

00:21:30,640 --> 00:21:34,559
test data

00:21:31,760 --> 00:21:35,280
and or explicitly set test data equals 1

00:21:34,559 --> 00:21:37,679
in a sql

00:21:35,280 --> 00:21:39,760
filter clause and yellowjacket would

00:21:37,679 --> 00:21:42,720
know behind the scenes given the name

00:21:39,760 --> 00:21:44,559
and using just by the name that it would

00:21:42,720 --> 00:21:46,240
surface the test data equals one

00:21:44,559 --> 00:21:48,000
data so this indirection was really

00:21:46,240 --> 00:21:50,080
useful to sort of isolate

00:21:48,000 --> 00:21:51,600
test data from production data and so

00:21:50,080 --> 00:21:53,440
anybody who doesn't even know the

00:21:51,600 --> 00:21:55,280
structure would read the table as it is

00:21:53,440 --> 00:21:57,120
would always get production but let's

00:21:55,280 --> 00:22:01,039
say somebody explicitly wants test data

00:21:57,120 --> 00:22:04,000
then they can query it in this way

00:22:01,039 --> 00:22:04,799
so the view solution was also something

00:22:04,000 --> 00:22:08,159
we addressed

00:22:04,799 --> 00:22:10,080
which we thought of let's just automate

00:22:08,159 --> 00:22:11,280
that and make it make it more easier for

00:22:10,080 --> 00:22:13,360
data scientists

00:22:11,280 --> 00:22:15,039
so an example of that is let's look at

00:22:13,360 --> 00:22:17,200
test table again which has

00:22:15,039 --> 00:22:19,520
a date partition and i told you this

00:22:17,200 --> 00:22:22,559
would be a pattern where historical data

00:22:19,520 --> 00:22:24,960
stored date wise would be

00:22:22,559 --> 00:22:26,080
made into a pointer table and so that

00:22:24,960 --> 00:22:27,840
was the the

00:22:26,080 --> 00:22:29,919
use case that we tried to solve and so

00:22:27,840 --> 00:22:31,679
let's say you were addressing that today

00:22:29,919 --> 00:22:33,440
yesterday's data would be latest for

00:22:31,679 --> 00:22:36,480
today morning and so

00:22:33,440 --> 00:22:38,320
um looking at that instead of having a

00:22:36,480 --> 00:22:40,640
separate table to address this and

00:22:38,320 --> 00:22:43,039
represent this latest data all we did

00:22:40,640 --> 00:22:45,039
was modify the get table to return a

00:22:43,039 --> 00:22:47,360
quote-unquote view

00:22:45,039 --> 00:22:48,240
which is a decorator pattern that we

00:22:47,360 --> 00:22:50,559
used for

00:22:48,240 --> 00:22:52,480
exhibiting the view of a table so view

00:22:50,559 --> 00:22:54,320
would be the latest numerical partition

00:22:52,480 --> 00:22:56,400
of a table um

00:22:54,320 --> 00:22:58,159
and it would be auto-generated when

00:22:56,400 --> 00:22:59,440
required and so you don't need to

00:22:58,159 --> 00:23:01,919
actually specify

00:22:59,440 --> 00:23:03,679
or update or do anything this view

00:23:01,919 --> 00:23:04,320
doesn't exist in the meta store as an

00:23:03,679 --> 00:23:05,919
entity

00:23:04,320 --> 00:23:08,320
and it would be generated via yellow

00:23:05,919 --> 00:23:09,679
jacket when requested and so essentially

00:23:08,320 --> 00:23:12,000
we would take the

00:23:09,679 --> 00:23:13,039
the historical table and present the

00:23:12,000 --> 00:23:16,080
latest partition

00:23:13,039 --> 00:23:18,080
as um as the table so this indirection

00:23:16,080 --> 00:23:18,799
was really useful to solve this uh use

00:23:18,080 --> 00:23:20,480
case

00:23:18,799 --> 00:23:22,720
and in spark and presto we did it in

00:23:20,480 --> 00:23:23,520
such a way that with the methods behind

00:23:22,720 --> 00:23:26,240
the scenes

00:23:23,520 --> 00:23:26,799
we had to adjust a few things and so

00:23:26,240 --> 00:23:28,799
test

00:23:26,799 --> 00:23:31,120
you could do test table underscoring

00:23:28,799 --> 00:23:33,120
school view and read the

00:23:31,120 --> 00:23:35,039
latest data available to you so you

00:23:33,120 --> 00:23:37,200
didn't need any updating happening

00:23:35,039 --> 00:23:38,320
and the same goes for test data it was

00:23:37,200 --> 00:23:41,919
uh same one

00:23:38,320 --> 00:23:43,840
spark and presto as well so you'd ask

00:23:41,919 --> 00:23:44,080
like how do we track this ecosystem it's

00:23:43,840 --> 00:23:45,840
a

00:23:44,080 --> 00:23:47,679
it's a large ecosystem with things like

00:23:45,840 --> 00:23:48,480
abstraction there's a direction layer

00:23:47,679 --> 00:23:51,120
now

00:23:48,480 --> 00:23:52,559
and metadata operations were happening

00:23:51,120 --> 00:23:54,880
everywhere and so tracking this

00:23:52,559 --> 00:23:56,799
needed more context just by just by not

00:23:54,880 --> 00:23:59,120
by simple logging

00:23:56,799 --> 00:24:00,320
so to understand context like what

00:23:59,120 --> 00:24:03,039
operation was done

00:24:00,320 --> 00:24:04,400
maybe a name or type when it was done

00:24:03,039 --> 00:24:06,720
where did it come from did it come from

00:24:04,400 --> 00:24:09,520
spark or presto or bumblebee

00:24:06,720 --> 00:24:10,880
who did it was was it some team or user

00:24:09,520 --> 00:24:13,200
who had performed it

00:24:10,880 --> 00:24:14,640
and so tracking this was became a real

00:24:13,200 --> 00:24:17,360
concern

00:24:14,640 --> 00:24:18,320
so we called it buzzcom a cheeky name

00:24:17,360 --> 00:24:20,720
for something like

00:24:18,320 --> 00:24:21,919
think of a telecom network within a hive

00:24:20,720 --> 00:24:23,520
and so

00:24:21,919 --> 00:24:25,360
we observed that the metastore itself

00:24:23,520 --> 00:24:28,480
came with its event listener

00:24:25,360 --> 00:24:32,320
and had to support free operations

00:24:28,480 --> 00:24:34,720
that fired off events so you would

00:24:32,320 --> 00:24:36,159
you could produce them in a log and get

00:24:34,720 --> 00:24:37,120
an idea of which events actually

00:24:36,159 --> 00:24:40,400
happened

00:24:37,120 --> 00:24:42,559
so but it didn't it didn't have an extra

00:24:40,400 --> 00:24:44,960
context that was needed in our case for

00:24:42,559 --> 00:24:47,600
understanding more about the operations

00:24:44,960 --> 00:24:50,000
so we observed that every

00:24:47,600 --> 00:24:52,559
create add drop delete method for either

00:24:50,000 --> 00:24:54,960
table database or partition had

00:24:52,559 --> 00:24:56,000
something equivalent of a method within

00:24:54,960 --> 00:24:58,640
environment context

00:24:56,000 --> 00:24:59,679
it's literally called that and so we

00:24:58,640 --> 00:25:02,400
noticed that this

00:24:59,679 --> 00:25:04,320
environment context object had a sort of

00:25:02,400 --> 00:25:06,159
map structure called properties

00:25:04,320 --> 00:25:08,320
which was sort of open-ended and we

00:25:06,159 --> 00:25:10,799
could pass any information we wanted

00:25:08,320 --> 00:25:12,960
and so we we decided to use those behind

00:25:10,799 --> 00:25:14,880
the scenes and we patched the ones

00:25:12,960 --> 00:25:16,640
that were missing we we noticed that

00:25:14,880 --> 00:25:18,320
altered database was not an event so we

00:25:16,640 --> 00:25:20,880
added that explicitly

00:25:18,320 --> 00:25:22,640
and we published this uh these events to

00:25:20,880 --> 00:25:24,240
a kafka topic so it ends up being more

00:25:22,640 --> 00:25:27,520
consumable than just writing it

00:25:24,240 --> 00:25:29,120
into a log and so we parsed that we

00:25:27,520 --> 00:25:30,880
cleaned up the object screened up the

00:25:29,120 --> 00:25:32,480
structure and passed it into a topic so

00:25:30,880 --> 00:25:35,440
it becomes consumable

00:25:32,480 --> 00:25:37,840
downstream by any sinks written on kafka

00:25:35,440 --> 00:25:39,279
and so we had to patch spark and hive in

00:25:37,840 --> 00:25:42,320
our libraries to

00:25:39,279 --> 00:25:44,080
and bumblebee to use this but uh all in

00:25:42,320 --> 00:25:47,039
all it ended up

00:25:44,080 --> 00:25:47,600
using uh this environment context method

00:25:47,039 --> 00:25:50,159
and

00:25:47,600 --> 00:25:51,919
passing additional information uh that

00:25:50,159 --> 00:25:54,960
was useful

00:25:51,919 --> 00:25:56,640
so finally when you do pass this all the

00:25:54,960 --> 00:25:58,080
towards the end you would record an

00:25:56,640 --> 00:26:00,480
event something like this

00:25:58,080 --> 00:26:01,520
so create database would be the

00:26:00,480 --> 00:26:03,600
operation source

00:26:01,520 --> 00:26:05,919
an id to trace it back to where it came

00:26:03,600 --> 00:26:07,760
from so bumblebee's id would be stored

00:26:05,919 --> 00:26:08,960
the operation timestamp to indicate the

00:26:07,760 --> 00:26:11,600
timing of the

00:26:08,960 --> 00:26:13,520
the operation and finally the source uh

00:26:11,600 --> 00:26:15,520
which is bumblebee and in other cases

00:26:13,520 --> 00:26:17,200
would be spark or something else

00:26:15,520 --> 00:26:18,640
and there's more information in other

00:26:17,200 --> 00:26:20,400
kinds of events but i wanted to give you

00:26:18,640 --> 00:26:22,159
an example so things like add partition

00:26:20,400 --> 00:26:23,360
would store which keys and values were

00:26:22,159 --> 00:26:25,440
actually added

00:26:23,360 --> 00:26:26,960
and altered table would store both the

00:26:25,440 --> 00:26:30,640
new and the old structure so

00:26:26,960 --> 00:26:32,480
things like that so

00:26:30,640 --> 00:26:35,039
coming coming finally to the learnings

00:26:32,480 --> 00:26:37,120
in the future work this was

00:26:35,039 --> 00:26:38,559
sort of a multi-year effort of building

00:26:37,120 --> 00:26:40,159
this ecosystem and i wanted to share

00:26:38,559 --> 00:26:42,240
some of the learnings that

00:26:40,159 --> 00:26:44,240
came along the way for us and the future

00:26:42,240 --> 00:26:46,559
work that's planned

00:26:44,240 --> 00:26:48,240
so what did we learn here though getting

00:26:46,559 --> 00:26:50,000
essentially the main

00:26:48,240 --> 00:26:51,440
purpose of getting these things were

00:26:50,000 --> 00:26:53,360
useful for our business but

00:26:51,440 --> 00:26:55,200
compatibility with spartan presto or

00:26:53,360 --> 00:26:55,840
essential and so a lot of testing was

00:26:55,200 --> 00:26:58,640
done while

00:26:55,840 --> 00:26:59,919
rolling these out with bumblebee's inter

00:26:58,640 --> 00:27:01,679
introduction it meant

00:26:59,919 --> 00:27:03,120
a lot of documentation a lot of helping

00:27:01,679 --> 00:27:05,520
data scientists to migrate

00:27:03,120 --> 00:27:06,880
to use this new interface to manage

00:27:05,520 --> 00:27:08,320
metadata so that was

00:27:06,880 --> 00:27:10,159
we observed a lot of pull requests

00:27:08,320 --> 00:27:12,720
everywhere and so

00:27:10,159 --> 00:27:14,720
um that was a that was a big change in

00:27:12,720 --> 00:27:17,840
the team and so migration of code was

00:27:14,720 --> 00:27:19,440
uh took a lot of a lot of effort with

00:27:17,840 --> 00:27:20,799
yellowjacket it seemed more behind the

00:27:19,440 --> 00:27:24,000
scenes but

00:27:20,799 --> 00:27:26,000
specifically challenging for us to

00:27:24,000 --> 00:27:27,360
implement because we noticed that spark

00:27:26,000 --> 00:27:28,559
and presto interacted differently with

00:27:27,360 --> 00:27:29,279
the metastore so i'll give you an

00:27:28,559 --> 00:27:31,600
example like

00:27:29,279 --> 00:27:32,799
get partitions is called different

00:27:31,600 --> 00:27:34,799
methods by

00:27:32,799 --> 00:27:35,840
different methods are called whether

00:27:34,799 --> 00:27:37,600
you're coming from spark or whether

00:27:35,840 --> 00:27:39,120
they're coming from presto so

00:27:37,600 --> 00:27:41,679
some of those need to be needed to be

00:27:39,120 --> 00:27:44,320
adjusted and so testing that was also

00:27:41,679 --> 00:27:45,919
uh quite challenging and finally we

00:27:44,320 --> 00:27:47,360
wouldn't be anywhere without alarms and

00:27:45,919 --> 00:27:49,360
alerts but

00:27:47,360 --> 00:27:51,840
they saved us from disasters and they

00:27:49,360 --> 00:27:53,440
still continue to do so and so

00:27:51,840 --> 00:27:55,120
that's something we've always learned

00:27:53,440 --> 00:27:57,200
and kept behind the uh

00:27:55,120 --> 00:27:58,480
at the back of our heads to make sure

00:27:57,200 --> 00:28:01,919
that we set up these things

00:27:58,480 --> 00:28:02,399
more accurately and finally looking

00:28:01,919 --> 00:28:04,720
ahead

00:28:02,399 --> 00:28:05,600
uh this effort with dedicated logging

00:28:04,720 --> 00:28:08,159
now in place

00:28:05,600 --> 00:28:08,880
with buzzcom we have eventually a plan

00:28:08,159 --> 00:28:12,000
to sort of

00:28:08,880 --> 00:28:14,080
trigger workflows based on metadata

00:28:12,000 --> 00:28:14,720
changes or any kind of availability of

00:28:14,080 --> 00:28:16,000
data

00:28:14,720 --> 00:28:19,120
and so that's something that's being

00:28:16,000 --> 00:28:20,559
worked on a recovery mechanism for hive

00:28:19,120 --> 00:28:23,120
is being worked on as well where

00:28:20,559 --> 00:28:25,120
disaster strikes if something goes wrong

00:28:23,120 --> 00:28:26,640
using some of this logging that we have

00:28:25,120 --> 00:28:29,440
now so that we can replace some of the

00:28:26,640 --> 00:28:31,679
events to understand that we

00:28:29,440 --> 00:28:32,720
what happened at the time of let's say a

00:28:31,679 --> 00:28:35,520
crash

00:28:32,720 --> 00:28:37,520
and i i gave a talk at data ai summit

00:28:35,520 --> 00:28:38,240
about data quality so feel free to have

00:28:37,520 --> 00:28:40,640
a look if you

00:28:38,240 --> 00:28:42,399
if you get a chance but uh i want to

00:28:40,640 --> 00:28:44,480
mention that we are doing data quality

00:28:42,399 --> 00:28:46,399
powered by metadata in the tables and so

00:28:44,480 --> 00:28:47,120
you can declare tests and set rules for

00:28:46,399 --> 00:28:50,320
tests

00:28:47,120 --> 00:28:51,840
in metadata and we have a mechanism to

00:28:50,320 --> 00:28:53,200
read that and perform tests and so

00:28:51,840 --> 00:28:54,880
that's been expanding

00:28:53,200 --> 00:28:56,559
as an effort within the company as well

00:28:54,880 --> 00:28:58,720
so that's uh that's something that

00:28:56,559 --> 00:29:02,240
metadata has proven to be useful for

00:28:58,720 --> 00:29:04,240
as well to summarize

00:29:02,240 --> 00:29:06,399
metadata has been important for us at

00:29:04,240 --> 00:29:07,440
stitch fix we began with the metastore

00:29:06,399 --> 00:29:08,480
but we understood some of the

00:29:07,440 --> 00:29:10,240
limitations

00:29:08,480 --> 00:29:12,480
that made us expand it into a more

00:29:10,240 --> 00:29:13,760
full-fledged ecosystem to improve

00:29:12,480 --> 00:29:15,840
abstraction

00:29:13,760 --> 00:29:18,080
uh the first one of the limitations we

00:29:15,840 --> 00:29:19,679
built a rest server and a client

00:29:18,080 --> 00:29:21,200
that allowed data scientists to be more

00:29:19,679 --> 00:29:23,200
control of the metadata

00:29:21,200 --> 00:29:26,240
and give them more expressiveness we

00:29:23,200 --> 00:29:29,120
added caching additional optimizations

00:29:26,240 --> 00:29:31,120
to help keep the metastore more stable

00:29:29,120 --> 00:29:32,720
especially for larger calls or repeated

00:29:31,120 --> 00:29:34,640
calls specifically

00:29:32,720 --> 00:29:36,480
and finally we added a proxy meta store

00:29:34,640 --> 00:29:38,880
that allowed us the ability to

00:29:36,480 --> 00:29:40,799
provide indirection and support for some

00:29:38,880 --> 00:29:42,480
business logic that was not

00:29:40,799 --> 00:29:44,320
while maintaining sort of compatibility

00:29:42,480 --> 00:29:46,240
with spark and presto and solving the

00:29:44,320 --> 00:29:49,039
problems that we had

00:29:46,240 --> 00:29:55,840
so that's all from me thank you happy to

00:29:49,039 --> 00:29:55,840
take any questions

00:30:11,840 --> 00:30:13,919

YouTube URL: https://www.youtube.com/watch?v=xXwxM6ydDD8


