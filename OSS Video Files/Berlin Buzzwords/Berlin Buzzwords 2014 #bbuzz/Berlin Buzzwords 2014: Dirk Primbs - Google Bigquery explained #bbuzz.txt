Title: Berlin Buzzwords 2014: Dirk Primbs - Google Bigquery explained #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Google Bigquery is a data analysis tool, which can crunch terabytes of data on demand in seconds using SQL queries without using expensive in-memory technology.

It has been used extensively inside of Google for analyzing large datasets and log files for years and is also available externally.

The scientific paper about Dremel ("Dremel: Interactive Analysis of Web-Scale Datasets") explains the algorithms behind the tool.

This talk goes through the algorithms in a simplified and accessible way by visualizing how Dremel executes a query on a small dataset.

Read more:
https://2014.berlinbuzzwords.de/session/google-bigquery-explained

About Dirk Primbs:
https://2014.berlinbuzzwords.de/user/363/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,980 --> 00:00:16,350
alright so Google's back on stage with

00:00:12,000 --> 00:00:18,510
the subject on big data again for the

00:00:16,350 --> 00:00:20,880
ones in room that followed my colleague

00:00:18,510 --> 00:00:25,170
earlier on you probably have seen a few

00:00:20,880 --> 00:00:28,020
of the bigquery well the curie queries

00:00:25,170 --> 00:00:30,150
okay already my name is Dirk prince i'm

00:00:28,020 --> 00:00:32,070
working for google and a developer

00:00:30,150 --> 00:00:34,670
relations department and i'm responsible

00:00:32,070 --> 00:00:38,280
for the whole german-speaking region and

00:00:34,670 --> 00:00:40,320
well cloud subjects are one of my

00:00:38,280 --> 00:00:43,500
favorites because you you get such a

00:00:40,320 --> 00:00:46,320
massive scale at eight points and for

00:00:43,500 --> 00:00:48,210
the next 20 minutes what i like to to do

00:00:46,320 --> 00:00:51,930
with you is I like to have a look at

00:00:48,210 --> 00:00:54,420
bigquery from the point of view for how

00:00:51,930 --> 00:00:56,730
do we actually accomplish queries that

00:00:54,420 --> 00:00:59,040
go over massive scales of data like

00:00:56,730 --> 00:01:03,090
gigabytes of data terabytes of data

00:00:59,040 --> 00:01:07,979
without having to well wait minutes and

00:01:03,090 --> 00:01:10,219
and and go grab a coffee or such but

00:01:07,979 --> 00:01:13,770
instead get a really almost real-time

00:01:10,219 --> 00:01:15,539
responses like in the 5 to 20 seconds

00:01:13,770 --> 00:01:18,600
range as you have seen in n examples

00:01:15,539 --> 00:01:20,039
area so if you're interested in that

00:01:18,600 --> 00:01:22,289
type of thing then you probably have

00:01:20,039 --> 00:01:25,709
browsed the google for developers

00:01:22,289 --> 00:01:27,299
websites already might have browse for

00:01:25,709 --> 00:01:28,499
the cloud technology and you find

00:01:27,299 --> 00:01:31,259
something they are it is called the

00:01:28,499 --> 00:01:33,630
Dremel white paper so dremel is the code

00:01:31,259 --> 00:01:36,840
name or the former code name of bigquery

00:01:33,630 --> 00:01:39,749
and it explains in quite computer

00:01:36,840 --> 00:01:43,229
science tific terms how we actually do

00:01:39,749 --> 00:01:45,270
what we do in big fairy now when i

00:01:43,229 --> 00:01:48,450
prepared for that session i downloaded

00:01:45,270 --> 00:01:51,840
that paper and i had a good long look at

00:01:48,450 --> 00:01:54,959
that paper and i tell you that's kind of

00:01:51,840 --> 00:01:56,729
a little bit painful go to the

00:01:54,959 --> 00:01:59,880
experience yourself but the whole point

00:01:56,729 --> 00:02:02,369
of this session is actually to walk you

00:01:59,880 --> 00:02:04,200
through the through the algorithm in a

00:02:02,369 --> 00:02:07,829
way that's not that painful and

00:02:04,200 --> 00:02:09,660
interesting and well it's hopefully

00:02:07,829 --> 00:02:13,050
interesting and not that painful that

00:02:09,660 --> 00:02:15,210
was proper english i guess and well so

00:02:13,050 --> 00:02:17,340
you get an unfair advantage above me and

00:02:15,210 --> 00:02:18,569
others who have prepared for that

00:02:17,340 --> 00:02:22,430
session so you

00:02:18,569 --> 00:02:22,430
you have a general understanding already

00:02:22,700 --> 00:02:31,680
all right um so bigquery what is it for

00:02:28,739 --> 00:02:34,530
those of you who haven't been here on

00:02:31,680 --> 00:02:36,739
the session before how many of you in

00:02:34,530 --> 00:02:40,950
the audience have played with bigquery

00:02:36,739 --> 00:02:44,730
already like five or six people who

00:02:40,950 --> 00:02:46,200
knows what be curious yeah those were

00:02:44,730 --> 00:02:48,030
the ones who bear in the session earlier

00:02:46,200 --> 00:02:52,049
I guess and the few of those who played

00:02:48,030 --> 00:02:56,250
with it all right so you find bigquery

00:02:52,049 --> 00:02:58,500
on well or the interface to be query if

00:02:56,250 --> 00:03:00,449
you go to developers of google com you

00:02:58,500 --> 00:03:03,269
just follow the signs that this week are

00:03:00,449 --> 00:03:05,909
the links to the bigquery dashboard and

00:03:03,269 --> 00:03:08,579
basically think of it as a read-only

00:03:05,909 --> 00:03:11,609
database in the cloud that you can use

00:03:08,579 --> 00:03:14,250
to analyze your data to to fire or

00:03:11,609 --> 00:03:16,319
queries against to work with it and what

00:03:14,250 --> 00:03:18,299
you what you find is as an interface a

00:03:16,319 --> 00:03:20,489
bit query of course is an API that you

00:03:18,299 --> 00:03:22,909
can use a developer as well but if you

00:03:20,489 --> 00:03:25,349
go to the developer console you you find

00:03:22,909 --> 00:03:27,470
basically this type of interface you can

00:03:25,349 --> 00:03:31,139
type in a query you can select your data

00:03:27,470 --> 00:03:32,759
you can well upload your own your data

00:03:31,139 --> 00:03:35,370
that you want to analyze and such and

00:03:32,759 --> 00:03:39,000
the type of data set that we like to use

00:03:35,370 --> 00:03:41,900
for the next 20 minutes will be the

00:03:39,000 --> 00:03:44,310
github archive so github basically

00:03:41,900 --> 00:03:46,379
publishes the whole the whole set of

00:03:44,310 --> 00:03:48,540
commits as a big database and we have it

00:03:46,379 --> 00:03:50,340
as a public data set in bigquery for you

00:03:48,540 --> 00:03:52,049
to play as well and it's a pretty useful

00:03:50,340 --> 00:03:56,699
data set if you like to play a little

00:03:52,049 --> 00:03:59,009
bit bit with it so put the starter um to

00:03:56,699 --> 00:04:02,250
get a feel for the type of technology we

00:03:59,009 --> 00:04:05,009
talked about I prepared a statement a

00:04:02,250 --> 00:04:09,269
query that basically counts all the

00:04:05,009 --> 00:04:11,790
languages or the commit in in github

00:04:09,269 --> 00:04:16,409
according to the languages that they are

00:04:11,790 --> 00:04:18,930
using and what you see here is kind of

00:04:16,409 --> 00:04:22,620
like runs through the data net process

00:04:18,930 --> 00:04:25,530
like well one dot 1 4 gigabytes of data

00:04:22,620 --> 00:04:28,860
in about three seconds so it's not too

00:04:25,530 --> 00:04:31,529
bad can also enable catching I disabled

00:04:28,860 --> 00:04:32,370
it for the for the for the demo to to

00:04:31,529 --> 00:04:34,590
get a real

00:04:32,370 --> 00:04:37,590
real timing here and what we see is the

00:04:34,590 --> 00:04:40,949
Curie results at the top at the bottom

00:04:37,590 --> 00:04:43,500
which basically is no surprise it's a

00:04:40,949 --> 00:04:45,030
well it basically says javascript is the

00:04:43,500 --> 00:04:47,639
most common language in github

00:04:45,030 --> 00:04:49,229
repository followed by Java followed by

00:04:47,639 --> 00:04:52,410
Ruby followed by pipes and followed by

00:04:49,229 --> 00:04:56,940
PHP and well I guess followed by all the

00:04:52,410 --> 00:04:59,430
other stuff down there so we can loop

00:04:56,940 --> 00:05:02,250
through 180 pages i spare you the pain

00:04:59,430 --> 00:05:05,070
right now but of course we can download

00:05:02,250 --> 00:05:06,960
it as a CSV as you seen might have seen

00:05:05,070 --> 00:05:10,380
earlier and we can continue to work with

00:05:06,960 --> 00:05:12,900
that type of data so to give you an idea

00:05:10,380 --> 00:05:15,449
about what's possible with it I prepared

00:05:12,900 --> 00:05:17,760
a few other statements I thought that we

00:05:15,449 --> 00:05:21,389
do a little scientific experiment here

00:05:17,760 --> 00:05:24,690
and have a look at how you guys behave

00:05:21,389 --> 00:05:29,520
when you submit code to the github

00:05:24,690 --> 00:05:33,840
repository so I assemble the list of

00:05:29,520 --> 00:05:36,690
quite colorful language read it

00:05:33,840 --> 00:05:42,960
carefully you never used one of those in

00:05:36,690 --> 00:05:45,150
your comments right and well make a make

00:05:42,960 --> 00:05:48,300
a run through the github repository and

00:05:45,150 --> 00:05:51,389
look for the overall setup in the

00:05:48,300 --> 00:05:53,580
commitments and again it takes a few

00:05:51,389 --> 00:06:00,950
seconds so we processed about six

00:05:53,580 --> 00:06:00,950
gigabytes of data right now and yeah

00:06:01,370 --> 00:06:08,669
you know god damn game developers and I

00:06:05,539 --> 00:06:12,180
i I'm not I'm not sure any rimmel

00:06:08,669 --> 00:06:14,789
developers in the room I've seen one

00:06:12,180 --> 00:06:17,819
hand one hand I didn't even know that it

00:06:14,789 --> 00:06:21,150
exists but when I think of why I I'm

00:06:17,819 --> 00:06:23,430
yeah I can understand why you're in the

00:06:21,150 --> 00:06:26,159
mood of colorful language and c-sharp

00:06:23,430 --> 00:06:28,439
see some in the middle and again there

00:06:26,159 --> 00:06:33,029
are there are more more data sets to

00:06:28,439 --> 00:06:35,610
discover if he if we pay true that well

00:06:33,029 --> 00:06:37,889
looking at this varying in github maybe

00:06:35,610 --> 00:06:40,069
maybe we should have something to

00:06:37,889 --> 00:06:44,340
balance it and look for the most polite

00:06:40,069 --> 00:06:46,650
developer as well and the most polite

00:06:44,340 --> 00:06:49,919
developer of course would be someone who

00:06:46,650 --> 00:06:52,169
says thank you for while checking your

00:06:49,919 --> 00:06:54,900
data before submitting it to my function

00:06:52,169 --> 00:06:58,319
and please be respectfully when you add

00:06:54,900 --> 00:07:03,659
and something to my data by a database

00:06:58,319 --> 00:07:09,810
and so on and so forth and we do the

00:07:03,659 --> 00:07:12,389
same exercise again and who would have

00:07:09,810 --> 00:07:14,789
thought that the see developers are the

00:07:12,389 --> 00:07:17,879
most polite developers in the github

00:07:14,789 --> 00:07:20,009
repository so well that's data data

00:07:17,879 --> 00:07:22,110
never lies as we all know so this is

00:07:20,009 --> 00:07:24,120
scientific fact now and next time you

00:07:22,110 --> 00:07:26,219
meet a see developer in your team think

00:07:24,120 --> 00:07:29,509
of it when you are such swearing he

00:07:26,219 --> 00:07:32,219
might just say thank you respectfully

00:07:29,509 --> 00:07:34,080
yeah so the whole point of that exercise

00:07:32,219 --> 00:07:36,300
is to give you an idea about the the

00:07:34,080 --> 00:07:38,819
stunning speed the whole big hurry

00:07:36,300 --> 00:07:41,879
engine on it's working behind the scenes

00:07:38,819 --> 00:07:43,500
so that's not the only public data set

00:07:41,879 --> 00:07:47,099
you can play with a number of public

00:07:43,500 --> 00:07:49,050
data sets or your own data as well so

00:07:47,099 --> 00:07:51,330
have a look you can browse through all

00:07:49,050 --> 00:07:53,339
the Shakespeare works or through

00:07:51,330 --> 00:07:56,039
Wikipedia as you as you might have seen

00:07:53,339 --> 00:07:58,080
earlier and those are very interesting

00:07:56,039 --> 00:08:01,339
data sets to work with and the question

00:07:58,080 --> 00:08:07,319
raises how we are pulling that off

00:08:01,339 --> 00:08:11,129
getting this type of speed and all now

00:08:07,319 --> 00:08:13,660
it would be great if I get a internet

00:08:11,129 --> 00:08:16,340
connection here

00:08:13,660 --> 00:08:18,980
because I would like to continue

00:08:16,340 --> 00:08:22,070
explaining the algorithm well anyway I

00:08:18,980 --> 00:08:24,770
think it might not work that way may as

00:08:22,070 --> 00:08:26,360
well so for the next 15 minutes or so I

00:08:24,770 --> 00:08:29,240
walk you through how the whole thing

00:08:26,360 --> 00:08:31,760
works and I were um basically you see

00:08:29,240 --> 00:08:35,060
and there are two two types of data that

00:08:31,760 --> 00:08:37,729
we can actually attempt to analyse and

00:08:35,060 --> 00:08:41,450
especially in the web world one is the

00:08:37,729 --> 00:08:43,310
realm of the rational database so you

00:08:41,450 --> 00:08:45,260
have tables you have columns your froze

00:08:43,310 --> 00:08:48,500
in there and it's a very straightforward

00:08:45,260 --> 00:08:51,020
but struck a strict and structured type

00:08:48,500 --> 00:08:53,840
of approach how you how you work with

00:08:51,020 --> 00:08:55,580
your data the other way of date and the

00:08:53,840 --> 00:08:57,920
other kind of data that you're often see

00:08:55,580 --> 00:09:00,800
is the type of data use it might see in

00:08:57,920 --> 00:09:02,960
in JSON objects or in XML or in that

00:09:00,800 --> 00:09:07,520
example here and I try to get you that

00:09:02,960 --> 00:09:09,350
that noise out of that should stop

00:09:07,520 --> 00:09:13,910
breathing when I talk with that

00:09:09,350 --> 00:09:15,530
microphone yeah that the type of type of

00:09:13,910 --> 00:09:18,370
information that you find in JSON data

00:09:15,530 --> 00:09:21,170
sets or in XML which is basically a

00:09:18,370 --> 00:09:23,630
hierarchy call kind of data

00:09:21,170 --> 00:09:26,390
representation where you you have not a

00:09:23,630 --> 00:09:28,520
fixed structure up front but mere and

00:09:26,390 --> 00:09:30,710
more or less you you add nodes as you

00:09:28,520 --> 00:09:34,580
come along to your to your data items

00:09:30,710 --> 00:09:36,860
both types of data can be found in in

00:09:34,580 --> 00:09:39,710
bigquery but the more interesting ones

00:09:36,860 --> 00:09:41,330
are certainly those hierarchical type of

00:09:39,710 --> 00:09:45,260
data sets because if you think about it

00:09:41,330 --> 00:09:47,750
if we if you try to solve the task to

00:09:45,260 --> 00:09:49,550
analyze the web that which is kind of

00:09:47,750 --> 00:09:53,240
what Google does on a day-to-day basis

00:09:49,550 --> 00:09:56,270
and firing searches on the whole public

00:09:53,240 --> 00:09:58,580
Internet what you find there is HTML and

00:09:56,270 --> 00:10:00,950
XML which is that kind of hierarchical

00:09:58,580 --> 00:10:04,820
representation of data that you need to

00:10:00,950 --> 00:10:07,940
analyze so for the for the example we

00:10:04,820 --> 00:10:11,870
walk you I walk you through a list of

00:10:07,940 --> 00:10:14,360
books for simplicity it's a very simple

00:10:11,870 --> 00:10:16,250
list of books but you you kind of walk

00:10:14,360 --> 00:10:18,350
out here with the idea of the algorithm

00:10:16,250 --> 00:10:21,500
and you can go as complex as you like to

00:10:18,350 --> 00:10:24,020
be and those three books in my example

00:10:21,500 --> 00:10:26,229
have a number of different items and

00:10:24,020 --> 00:10:29,929
elements on it like

00:10:26,229 --> 00:10:34,339
author title pricing and so on and so

00:10:29,929 --> 00:10:36,949
forth and this is the kind of query that

00:10:34,339 --> 00:10:39,559
we like to analyze here so when you

00:10:36,949 --> 00:10:41,449
think about bigquery or when you think

00:10:39,559 --> 00:10:43,669
about database in general but you try to

00:10:41,449 --> 00:10:46,069
avoid it's usually a full table scan

00:10:43,669 --> 00:10:48,409
right so you try to be efficient in your

00:10:46,069 --> 00:10:51,529
query and normally databases build

00:10:48,409 --> 00:10:56,029
indices and try to optimize your speed

00:10:51,529 --> 00:10:58,069
of of selecting data by well by

00:10:56,029 --> 00:11:00,349
selecting specifically what feels to

00:10:58,069 --> 00:11:02,359
touch but in case of bigquery what you

00:11:00,349 --> 00:11:04,819
what you essentially try to do is you do

00:11:02,359 --> 00:11:07,579
a full table scan you analyze all there

00:11:04,819 --> 00:11:09,799
is in their data set so if you use

00:11:07,579 --> 00:11:12,649
bigquery then you say the battle is lost

00:11:09,799 --> 00:11:15,199
I need a full table scan and bigquery is

00:11:12,649 --> 00:11:17,599
the way to make the most efficient full

00:11:15,199 --> 00:11:19,309
table scan that you can can use today

00:11:17,599 --> 00:11:23,229
and that's the type of query that we

00:11:19,309 --> 00:11:25,819
walk through in the next few slides okay

00:11:23,229 --> 00:11:30,949
remember the books that I that I mention

00:11:25,819 --> 00:11:33,379
and so I think I have yeah I have a

00:11:30,949 --> 00:11:36,559
connection again so we can make that a

00:11:33,379 --> 00:11:38,269
little bit larger so there are three

00:11:36,559 --> 00:11:42,109
books with authors and titles attached

00:11:38,269 --> 00:11:44,779
to it and the very first trick big big

00:11:42,109 --> 00:11:47,239
furious is doing is it changes the

00:11:44,779 --> 00:11:49,549
representation of that data from a row

00:11:47,239 --> 00:11:52,759
based representation to a column based

00:11:49,549 --> 00:11:54,609
presentation so instead of having all

00:11:52,759 --> 00:11:57,049
those rules with all the data in it it

00:11:54,609 --> 00:11:59,269
changes the way it stores the data

00:11:57,049 --> 00:12:01,009
efficiently to column so we have an

00:11:59,269 --> 00:12:03,739
author column a title column a price

00:12:01,009 --> 00:12:07,789
column price that Europe rise have used

00:12:03,739 --> 00:12:10,009
d and so on and so forth and if you look

00:12:07,789 --> 00:12:12,289
at the the elements here those are the

00:12:10,009 --> 00:12:14,509
data stored within and just ignore the

00:12:12,289 --> 00:12:17,089
common the numbers in the apparent ease

00:12:14,509 --> 00:12:19,729
is right now those are the magic cells

00:12:17,089 --> 00:12:22,129
that make bigquery work be we go through

00:12:19,729 --> 00:12:24,379
it in a bit but essentially that's the

00:12:22,129 --> 00:12:31,239
very first trick changing the

00:12:24,379 --> 00:12:35,239
representation of the data all right and

00:12:31,239 --> 00:12:39,209
by thinking of the data the next the

00:12:35,239 --> 00:12:42,829
next step bigquery is taking is

00:12:39,209 --> 00:12:45,059
it analyzes the data set in it adds

00:12:42,829 --> 00:12:49,199
metaphorical speaking the missing

00:12:45,059 --> 00:12:52,139
elements to the other the other rows so

00:12:49,199 --> 00:12:53,670
to speak so if you look at book 2 we see

00:12:52,139 --> 00:12:55,920
we have authors we have a title but

00:12:53,670 --> 00:12:58,019
there is no price but there is a prize

00:12:55,920 --> 00:13:01,040
in book 1 and there's a prize in book 3

00:12:58,019 --> 00:13:03,689
so it adds the price and the responsible

00:13:01,040 --> 00:13:05,759
subnodes to the the representation I

00:13:03,689 --> 00:13:08,040
write it in in paranthesis to make it

00:13:05,759 --> 00:13:10,490
make it clear that they are actually not

00:13:08,040 --> 00:13:12,480
data knots data in the bigquery

00:13:10,490 --> 00:13:14,429
representation but is an added element

00:13:12,480 --> 00:13:16,230
to the whole thing it start very

00:13:14,429 --> 00:13:18,569
efficiently so it doesn't add up to the

00:13:16,230 --> 00:13:20,339
data but at least conceptually for

00:13:18,569 --> 00:13:22,259
walking through the data they need to be

00:13:20,339 --> 00:13:28,529
there so they be query adds those

00:13:22,259 --> 00:13:31,319
elements then in the next step um it

00:13:28,529 --> 00:13:34,110
gives names to all elements and those

00:13:31,319 --> 00:13:35,910
names are following a typical pattern

00:13:34,110 --> 00:13:37,829
that you might recognize as a developer

00:13:35,910 --> 00:13:39,569
so if you if you look at author well

00:13:37,829 --> 00:13:42,540
it's pretty straightforward author title

00:13:39,569 --> 00:13:46,019
or named quite directly then we have the

00:13:42,540 --> 00:13:49,439
price dot discount property the price

00:13:46,019 --> 00:13:52,110
USD property price toward euro and so on

00:13:49,439 --> 00:13:55,290
and so forth in cases where we have more

00:13:52,110 --> 00:13:58,139
than one element like here with authors

00:13:55,290 --> 00:14:01,889
or down there with prices it's like an

00:13:58,139 --> 00:14:05,189
array representation so you simply count

00:14:01,889 --> 00:14:08,399
through the elements and again for

00:14:05,189 --> 00:14:10,410
completeness here are the names in

00:14:08,399 --> 00:14:16,079
parentheses further for the elements

00:14:10,410 --> 00:14:18,029
that actually don't carry any value all

00:14:16,079 --> 00:14:21,329
right now let's have a look at those

00:14:18,029 --> 00:14:22,559
numbers and stay with me because we walk

00:14:21,329 --> 00:14:24,779
through it in a bit and then it makes

00:14:22,559 --> 00:14:27,540
all sense when it comes together there

00:14:24,779 --> 00:14:32,220
are two numbers with each data element

00:14:27,540 --> 00:14:36,420
and one is called the repetition counter

00:14:32,220 --> 00:14:40,679
we repeat count and which is basically

00:14:36,420 --> 00:14:43,230
the number of elements following or the

00:14:40,679 --> 00:14:45,179
number of repetition a data element has

00:14:43,230 --> 00:14:48,149
so if you look through that if you have

00:14:45,179 --> 00:14:50,850
a repeat count of 0 at the title element

00:14:48,149 --> 00:14:52,329
this means this is the first occurrence

00:14:50,850 --> 00:14:55,540
of title

00:14:52,329 --> 00:14:58,059
and the same is true for for price that

00:14:55,540 --> 00:15:00,399
discount priced at USD price ural those

00:14:58,059 --> 00:15:03,029
are all repeat count 0 because they are

00:15:00,399 --> 00:15:06,249
the first instance in that data set

00:15:03,029 --> 00:15:10,269
going to book two it's pretty similar

00:15:06,249 --> 00:15:13,569
author is the first occurrence so it's a

00:15:10,269 --> 00:15:15,639
repeat cannot 0 but it's repeated it's

00:15:13,569 --> 00:15:17,259
repeated once here and it's repeated

00:15:15,639 --> 00:15:20,019
once there though it's a repetition of

00:15:17,259 --> 00:15:24,009
that element so it's a repeat count 1 in

00:15:20,019 --> 00:15:26,649
both cases and if you go all the way

00:15:24,009 --> 00:15:29,860
down you find the same pattern like here

00:15:26,649 --> 00:15:32,079
with the prices soda the price priced

00:15:29,860 --> 00:15:41,879
element is curl is counted here as a

00:15:32,079 --> 00:15:41,879
repetition the second number is it is a

00:15:44,369 --> 00:15:52,600
definition number so what it basically

00:15:47,829 --> 00:15:57,129
says how many of those elements are

00:15:52,600 --> 00:15:59,230
defined so if we look here price dot

00:15:57,129 --> 00:16:00,999
discounts are two elements as a prize

00:15:59,230 --> 00:16:03,369
element in the discount element and if

00:16:00,999 --> 00:16:06,579
it carries a value then prizes defined

00:16:03,369 --> 00:16:10,149
and discount is defined so that the

00:16:06,579 --> 00:16:12,549
definition count is two same here same

00:16:10,149 --> 00:16:15,669
here if we go down here then we have an

00:16:12,549 --> 00:16:18,160
author element which is just one element

00:16:15,669 --> 00:16:22,480
and it is defined so it the finishing

00:16:18,160 --> 00:16:23,889
count is 1 let's go down here here we

00:16:22,480 --> 00:16:26,669
have an interesting one which is a

00:16:23,889 --> 00:16:29,949
definition count of two for price

00:16:26,669 --> 00:16:32,529
discount I'm followed by a definition

00:16:29,949 --> 00:16:34,989
count of one priced at USD this is

00:16:32,529 --> 00:16:37,959
because the price tag actually is there

00:16:34,989 --> 00:16:40,720
in that element but USD is not defined

00:16:37,959 --> 00:16:45,910
so it's just one defined element and not

00:16:40,720 --> 00:16:48,869
the second one I hope I you're still

00:16:45,910 --> 00:16:53,699
with me did those of you who are

00:16:48,869 --> 00:16:53,699
shortener yeah some nodding that's good

00:16:53,790 --> 00:16:59,439
all right so let's bring it all together

00:16:57,429 --> 00:17:03,879
because it's quite theoretical right now

00:16:59,439 --> 00:17:05,949
and we play it through with one in the

00:17:03,879 --> 00:17:06,280
first example with one column and then

00:17:05,949 --> 00:17:08,560
we

00:17:06,280 --> 00:17:10,780
use two columns because remember we are

00:17:08,560 --> 00:17:15,100
actually trying to do a select star from

00:17:10,780 --> 00:17:16,750
all books their prices in urine and USD

00:17:15,100 --> 00:17:20,410
are in a certain amount that was a query

00:17:16,750 --> 00:17:23,500
that I showed you earlier and we are

00:17:20,410 --> 00:17:29,080
doing that right now by with the

00:17:23,500 --> 00:17:32,770
author's column so um the author of the

00:17:29,080 --> 00:17:35,520
first book what we see here is we see a

00:17:32,770 --> 00:17:39,400
repeat count of 0 so it's a new book and

00:17:35,520 --> 00:17:42,400
we see a definition count of one so yes

00:17:39,400 --> 00:17:50,050
the author is defined the value is Dumas

00:17:42,400 --> 00:17:55,030
you have a book one author Dumas next

00:17:50,050 --> 00:17:57,670
book because we have a repetition count

00:17:55,030 --> 00:18:00,010
of zero it's a new book and we have a

00:17:57,670 --> 00:18:04,570
definition kind of one so it's defined

00:18:00,010 --> 00:18:06,820
and we have an author and now the author

00:18:04,570 --> 00:18:09,040
gets repeated that's because the

00:18:06,820 --> 00:18:11,440
repetition we see that in the repetition

00:18:09,040 --> 00:18:13,870
come which is one now and it's also

00:18:11,440 --> 00:18:18,250
defined and this of course is the same

00:18:13,870 --> 00:18:23,140
case in the next author element then we

00:18:18,250 --> 00:18:25,630
have a null and this is an undefined

00:18:23,140 --> 00:18:27,970
author element and we see it's a new

00:18:25,630 --> 00:18:29,800
book we see it's not copied so it's

00:18:27,970 --> 00:18:36,390
essentially a new book which happens to

00:18:29,800 --> 00:18:39,100
have no author attached to it all right

00:18:36,390 --> 00:18:41,050
still are you still with me or anyone in

00:18:39,100 --> 00:18:44,770
the room that likes me to go over that

00:18:41,050 --> 00:18:55,350
one more time no everyone in room no

00:18:44,770 --> 00:18:55,350
please um okay so now let's yeah yeah

00:18:55,590 --> 00:19:00,390
I'm can say it again

00:19:03,640 --> 00:19:09,290
the question was the the repetition

00:19:06,860 --> 00:19:12,740
count can it be higher than one or can

00:19:09,290 --> 00:19:18,410
it just true or false yeah it's true

00:19:12,740 --> 00:19:20,210
false all right and so let's do it with

00:19:18,410 --> 00:19:22,130
the euro price taken the u.s. dollar

00:19:20,210 --> 00:19:24,470
price tag because that was what the

00:19:22,130 --> 00:19:26,600
query was selecting for and that's where

00:19:24,470 --> 00:19:29,270
the whole magic basically happens and it

00:19:26,600 --> 00:19:33,610
gets more interesting or at least my

00:19:29,270 --> 00:19:37,850
definition of interesting here so first

00:19:33,610 --> 00:19:40,580
first item we see its repetition count

00:19:37,850 --> 00:19:43,700
of 0 so it's a new book and it's a

00:19:40,580 --> 00:19:45,560
definition count of 2 so price is

00:19:43,700 --> 00:19:47,510
defined and euro is defined we have a

00:19:45,560 --> 00:19:49,640
Europe rise and here's the same see

00:19:47,510 --> 00:19:52,370
Europe because it's a new book and two

00:19:49,640 --> 00:19:59,840
because price is defined and USD is

00:19:52,370 --> 00:20:04,010
defined the next element we see it's a

00:19:59,840 --> 00:20:06,530
new book because repetition count is 0

00:20:04,010 --> 00:20:10,130
and definition count 0 means there is no

00:20:06,530 --> 00:20:12,140
euro price element to it and it's the

00:20:10,130 --> 00:20:19,150
same case down here so we don't have a

00:20:12,140 --> 00:20:23,470
price tag attached to book to next

00:20:19,150 --> 00:20:26,420
because it's 0 we know it's a new book

00:20:23,470 --> 00:20:28,670
here the definition count is 2 prize and

00:20:26,420 --> 00:20:31,730
euro are defined so we have an item here

00:20:28,670 --> 00:20:34,220
the definition count is 1 because price

00:20:31,730 --> 00:20:36,080
is defined we do have a price but it

00:20:34,220 --> 00:20:43,850
happens to be not a US dollar price but

00:20:36,080 --> 00:20:47,840
a Europe rise and this is a pretty

00:20:43,850 --> 00:20:50,390
similar case so here we have we know Bo

00:20:47,840 --> 00:20:53,270
another similar cases a similar case

00:20:50,390 --> 00:20:55,880
than before we have a repetition so this

00:20:53,270 --> 00:20:58,760
element happens to have another price in

00:20:55,880 --> 00:21:01,100
Europe and again price and eurotech are

00:20:58,760 --> 00:21:03,440
defined so it's two and we know it's a

00:21:01,100 --> 00:21:06,200
euro price and down here we have a null

00:21:03,440 --> 00:21:10,340
element which basically means the value

00:21:06,200 --> 00:21:12,080
for USD is 0 or is not defined and but

00:21:10,340 --> 00:21:14,150
we know it's a repetition we know that

00:21:12,080 --> 00:21:15,309
price is defined but USD is not defined

00:21:14,150 --> 00:21:22,360
so it's 1

00:21:15,309 --> 00:21:24,410
11 all right why are we doing all that

00:21:22,360 --> 00:21:27,320
essentially what we are doing with that

00:21:24,410 --> 00:21:29,900
type of algorithm is we optimize what

00:21:27,320 --> 00:21:33,170
columns we need to touch in order to do

00:21:29,900 --> 00:21:35,270
the full table scan instead of running

00:21:33,170 --> 00:21:37,160
through the whole data set of I don't

00:21:35,270 --> 00:21:39,679
know how many gigabyte or terabyte of

00:21:37,160 --> 00:21:41,690
data you're essentially just touching

00:21:39,679 --> 00:21:45,860
those elements that are used in the

00:21:41,690 --> 00:21:47,660
query and those two counters and the way

00:21:45,860 --> 00:21:50,690
you walk through the data enables you to

00:21:47,660 --> 00:21:52,940
do that very efficiently without losing

00:21:50,690 --> 00:21:56,120
the overall structure of the data so you

00:21:52,940 --> 00:21:58,370
get as a as an reply you get the real

00:21:56,120 --> 00:22:00,110
data set that you asked for and instead

00:21:58,370 --> 00:22:02,150
of browsing through let's say five

00:22:00,110 --> 00:22:05,090
gigabytes of data you're parsing just a

00:22:02,150 --> 00:22:07,250
few hundred megabytes nevertheless those

00:22:05,090 --> 00:22:10,850
few hundred megabytes need to be still

00:22:07,250 --> 00:22:13,160
crawled and run through very efficiently

00:22:10,850 --> 00:22:15,400
and of course we do that with the power

00:22:13,160 --> 00:22:18,049
of the google data centers so that

00:22:15,400 --> 00:22:20,780
usually what you do you spread out the

00:22:18,049 --> 00:22:22,700
whole data you learn through across a

00:22:20,780 --> 00:22:25,429
number of servers depending on the size

00:22:22,700 --> 00:22:27,770
of your data set and then you have leaf

00:22:25,429 --> 00:22:30,590
servers that do that process the query

00:22:27,770 --> 00:22:32,390
for you accumulate that and sometimes

00:22:30,590 --> 00:22:33,890
you have met several levels where you

00:22:32,390 --> 00:22:35,690
run that query through and by the way

00:22:33,890 --> 00:22:39,770
that was the query we just parse with

00:22:35,690 --> 00:22:42,200
our three books of course this this is

00:22:39,770 --> 00:22:44,210
quite efficient and how efficient can be

00:22:42,200 --> 00:22:46,220
also seen in that a white paper I

00:22:44,210 --> 00:22:48,820
mentioned and I took a few of the

00:22:46,220 --> 00:22:51,440
statistics out there for you to look at

00:22:48,820 --> 00:22:55,570
for instance there is a day of a

00:22:51,440 --> 00:22:59,450
benchmark using 87 terabytes of data and

00:22:55,570 --> 00:23:03,890
by using bigquery you you official

00:22:59,450 --> 00:23:05,990
efficiently eliminate like 86 dot five

00:23:03,890 --> 00:23:07,760
terabytes of the data that doesn't need

00:23:05,990 --> 00:23:10,280
to be processed any more of you if you

00:23:07,760 --> 00:23:12,850
use bigquery as a data processing engine

00:23:10,280 --> 00:23:16,190
and here's a comparison of the data sets

00:23:12,850 --> 00:23:18,559
or the data retriever time the execution

00:23:16,190 --> 00:23:20,410
time using different or a number of

00:23:18,559 --> 00:23:23,450
technologies to retrieve the data

00:23:20,410 --> 00:23:25,539
MapReduce being the first one then them

00:23:23,450 --> 00:23:28,010
you have

00:23:25,539 --> 00:23:30,919
dremel would just be query essentially

00:23:28,010 --> 00:23:32,840
and the middle one is map reduced when

00:23:30,919 --> 00:23:35,210
you use a color more representation so

00:23:32,840 --> 00:23:39,260
you see that's a potential optimization

00:23:35,210 --> 00:23:41,690
in its own right just to use a kilometer

00:23:39,260 --> 00:23:43,700
presentation in dremel is it is able to

00:23:41,690 --> 00:23:46,220
eliminate a lot of the data processing

00:23:43,700 --> 00:23:49,130
that you usually have to do and this is

00:23:46,220 --> 00:23:51,559
a similar case so here 24 billion

00:23:49,130 --> 00:23:54,919
records 13 terabytes of data and you see

00:23:51,559 --> 00:23:58,250
the number of levels that you that you

00:23:54,919 --> 00:24:01,820
use also defines how fast you basically

00:23:58,250 --> 00:24:03,559
run your query all right and before you

00:24:01,820 --> 00:24:07,159
start throwing things at me because I

00:24:03,559 --> 00:24:10,010
run over time this was a quick tour

00:24:07,159 --> 00:24:12,950
through the algorithm of bigquery I hope

00:24:10,010 --> 00:24:14,630
you you kind of like well get interested

00:24:12,950 --> 00:24:16,760
enough to have a look at bigquery and

00:24:14,630 --> 00:24:18,799
maybe even at the right paper it's a

00:24:16,760 --> 00:24:21,380
kind of like an interesting read if you

00:24:18,799 --> 00:24:23,929
if you're in computer science and in any

00:24:21,380 --> 00:24:25,730
case big furious an API should be worth

00:24:23,929 --> 00:24:29,690
playing around with if you if you like

00:24:25,730 --> 00:24:31,730
to analyze big data sets I'm around in

00:24:29,690 --> 00:24:35,690
the in the break and today and tomorrow

00:24:31,730 --> 00:24:38,299
you can find me on the on premise also

00:24:35,690 --> 00:24:41,090
you can of course find me online and

00:24:38,299 --> 00:24:44,870
shop me a question that might come later

00:24:41,090 --> 00:24:46,640
to you and with that thank you very much

00:24:44,870 --> 00:24:49,929
for your attention hope it was a little

00:24:46,640 --> 00:24:49,929
bit of fun using decree

00:24:52,029 --> 00:25:06,519
I might allow two questions if there are

00:24:56,950 --> 00:25:11,139
a medient he might so only if the

00:25:06,519 --> 00:25:14,669
questions right yeah so I'm I saw in the

00:25:11,139 --> 00:25:18,460
previous demonstration that we was using

00:25:14,669 --> 00:25:19,809
bigquery to do bigquery joins of

00:25:18,460 --> 00:25:21,549
datasets and I'm wondering how that's

00:25:19,809 --> 00:25:23,529
implemented because the way the

00:25:21,549 --> 00:25:26,109
execution tree stuff works like only

00:25:23,529 --> 00:25:28,509
imagine how it works for scanning over a

00:25:26,109 --> 00:25:30,099
single data source obviously we joins

00:25:28,509 --> 00:25:31,299
you go to scan multiple data sources i'm

00:25:30,099 --> 00:25:33,789
already don't understand how that

00:25:31,299 --> 00:25:35,559
combined works yeah that that's

00:25:33,789 --> 00:25:37,299
basically where the magic of the Google

00:25:35,559 --> 00:25:39,519
Data Center happens how we spread out

00:25:37,299 --> 00:25:41,259
the data and run it simultaneously but

00:25:39,519 --> 00:25:43,599
what I try to cover here is the

00:25:41,259 --> 00:25:45,219
essential way how how the algorithm in

00:25:43,599 --> 00:25:48,159
in South works and as you say it's not

00:25:45,219 --> 00:25:50,169
that that complicated and if you if you

00:25:48,159 --> 00:25:52,839
do joins or multiple joints you

00:25:50,169 --> 00:25:54,429
basically pre process the data in the

00:25:52,839 --> 00:25:56,379
data center where you have a larger

00:25:54,429 --> 00:25:58,869
datasets then but again you can have

00:25:56,379 --> 00:26:03,609
columns where you select items and bring

00:25:58,869 --> 00:26:06,609
it together that way there was a second

00:26:03,609 --> 00:26:08,830
one over there or here related question

00:26:06,609 --> 00:26:10,659
how do you limit so is that could be

00:26:08,830 --> 00:26:12,009
simple query could be complex square as

00:26:10,659 --> 00:26:14,679
far as I understand to charge card

00:26:12,009 --> 00:26:16,960
storage and / a size of data set which

00:26:14,679 --> 00:26:18,700
was done audit but I can imagine that

00:26:16,960 --> 00:26:20,109
you could very complex queries which

00:26:18,700 --> 00:26:22,089
would consume a lot of processing power

00:26:20,109 --> 00:26:24,489
how do you limit to how they solve these

00:26:22,089 --> 00:26:29,399
cases we wouldn't become water net for

00:26:24,489 --> 00:26:32,859
you or very cost efficient so oh my god

00:26:29,399 --> 00:26:35,589
sorry so the question is how do you

00:26:32,859 --> 00:26:37,330
control costs essentially i'm there

00:26:35,589 --> 00:26:40,710
there are a number of things that year

00:26:37,330 --> 00:26:44,080
that you can can look at there's a

00:26:40,710 --> 00:26:46,749
control center where you can define

00:26:44,080 --> 00:26:48,669
limits and how you how you actually want

00:26:46,749 --> 00:26:50,139
to allow how many transactions you want

00:26:48,669 --> 00:26:53,049
to allow how much workload you wanna

00:26:50,139 --> 00:26:54,940
allow and secondly you have a full set

00:26:53,049 --> 00:26:56,409
of api that you can script against the

00:26:54,940 --> 00:26:58,899
right code against if you want to

00:26:56,409 --> 00:27:00,909
control your your crease like how much

00:26:58,899 --> 00:27:02,979
how much time is allowed how much

00:27:00,909 --> 00:27:04,389
consumption before you get for instance

00:27:02,979 --> 00:27:05,149
of warning in your you structure your

00:27:04,389 --> 00:27:07,700
data

00:27:05,149 --> 00:27:09,529
but on there I'm not I'm not really

00:27:07,700 --> 00:27:12,409
familiar right now with the current

00:27:09,529 --> 00:27:15,469
pricing on bigquery but uh that I think

00:27:12,409 --> 00:27:17,389
there is quite a number of crew that you

00:27:15,469 --> 00:27:21,499
can do at a relatively low price point

00:27:17,389 --> 00:27:28,609
before it really becomes a virtual okay

00:27:21,499 --> 00:27:30,440
one last one last I think yeah oh I have

00:27:28,609 --> 00:27:32,839
a question about the filling out the

00:27:30,440 --> 00:27:35,059
gaps so it seems that your records I

00:27:32,839 --> 00:27:36,950
mean you infer the structure from the

00:27:35,059 --> 00:27:40,009
data you've God so what if a new record

00:27:36,950 --> 00:27:41,299
comes in and has a alters the existing

00:27:40,009 --> 00:27:43,039
structure how do you feel it fill out

00:27:41,299 --> 00:27:44,359
the gaps effectively because it feels

00:27:43,039 --> 00:27:48,139
like it could be a massive fan out of

00:27:44,359 --> 00:27:50,269
updates yeah so so what you do here is

00:27:48,139 --> 00:27:52,309
read the only operation so you pump up

00:27:50,269 --> 00:27:55,460
the whole thing and then you do analysis

00:27:52,309 --> 00:27:57,589
against it if that would be a live data

00:27:55,460 --> 00:27:59,570
set like processing in the back while

00:27:57,589 --> 00:28:01,279
you're doing queries then then I'm with

00:27:59,570 --> 00:28:02,889
you then it's probably a little bit

00:28:01,279 --> 00:28:05,809
complicated to fill out the whole thing

00:28:02,889 --> 00:28:08,179
so it happens once when you push the

00:28:05,809 --> 00:28:13,159
data update and then the new video you

00:28:08,179 --> 00:28:16,609
all set all right um I'm here for

00:28:13,159 --> 00:28:20,419
questions after the session for those of

00:28:16,609 --> 00:28:23,119
you who like to to show up I'm pretty

00:28:20,419 --> 00:28:26,809
happy too oh yeah you you like to me to

00:28:23,119 --> 00:28:30,529
give away those t-shirts anyone who

00:28:26,809 --> 00:28:34,159
likes to have a t-shirt I tried to get

00:28:30,529 --> 00:28:36,289
one really far oh yeah that was not

00:28:34,159 --> 00:28:41,479
really far that was like middle and that

00:28:36,289 --> 00:28:44,679
it's like in front row and all right

00:28:41,479 --> 00:28:44,679
thank you very much guys

00:28:46,260 --> 00:28:48,320

YouTube URL: https://www.youtube.com/watch?v=Pjd-ld1BI7A


