Title: Berlin Buzzwords 14: Martin Kleppmann - Samza @ LinkedIn: Taking Stream Processing to the Next Level
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Stream processing is an essential part of real-time data systems, such as news feeds, live search indexes, real-time analytics, metrics and monitoring. But writing stream processes is still hard, especially when you're dealing with so much data that you have to distribute it across multiple machines. How can you keep the system running smoothly, even when machines fail and bugs occur?

Apache Samza is a new framework for writing scalable stream processing jobs. Like Hadoop and MapReduce for batch processing, it takes care of the hard parts of running your message-processing code on a distributed infrastructure, so that you can concentrate on writing your application using simple APIs. It is in production use at LinkedIn.

This talk will introduce Samza, and show how to use it to solve a range of different problems. Samza has some unique features that make it especially interesting for large deployments, and in this talk we will dig into how they work under the hood.

Read more:
https://2014.berlinbuzzwords.de/session/samza-linkedin-taking-stream-processing-next-level

About Martin Kleppmann:
https://2014.berlinbuzzwords.de/user/227/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,500 --> 00:00:11,500
ok hello everybody thank you very much

00:00:09,309 --> 00:00:14,050
for coming I'm so delighted to be here I

00:00:11,500 --> 00:00:16,420
know it's getting towards the end of a

00:00:14,050 --> 00:00:17,680
really packet conference so everybody's

00:00:16,420 --> 00:00:19,869
a bit tired but i'll try and keep it

00:00:17,680 --> 00:00:24,100
interesting keep everyone awake are you

00:00:19,869 --> 00:00:26,740
awake hello yay good i want to talk

00:00:24,100 --> 00:00:30,250
about apache Sansa an open source

00:00:26,740 --> 00:00:32,710
project which I work on for distributed

00:00:30,250 --> 00:00:35,590
stream processing processing high volume

00:00:32,710 --> 00:00:37,750
data streams just very quickly about

00:00:35,590 --> 00:00:40,510
myself so did you know who's this idiot

00:00:37,750 --> 00:00:42,940
talking to you hear my name is Martin

00:00:40,510 --> 00:00:45,340
clapman I am currently working at

00:00:42,940 --> 00:00:48,520
LinkedIn and LinkedIn is sponsoring this

00:00:45,340 --> 00:00:50,829
development on Samsa before that i

00:00:48,520 --> 00:00:53,050
co-founded to startups so I've seen

00:00:50,829 --> 00:00:55,450
things from the small company side as

00:00:53,050 --> 00:00:57,910
well the second which was called

00:00:55,450 --> 00:01:00,250
rapportive it was acquired by linkedin a

00:00:57,910 --> 00:01:03,160
couple of years ago and still working at

00:01:00,250 --> 00:01:06,570
linkedin now I'm also a bit active in

00:01:03,160 --> 00:01:08,649
Apache working on avro and Sam sir now

00:01:06,570 --> 00:01:10,930
also trying to write a book on

00:01:08,649 --> 00:01:13,000
data-intensive applications for O'Reilly

00:01:10,930 --> 00:01:16,530
so that's a whole lot of stuff anyway

00:01:13,000 --> 00:01:19,450
you can find me online as well I

00:01:16,530 --> 00:01:21,729
primarily want to talk about Sam sir but

00:01:19,450 --> 00:01:23,979
I can't really talk about Sam sir

00:01:21,729 --> 00:01:26,920
without talking about kafka these are

00:01:23,979 --> 00:01:29,290
two separate projects but both have come

00:01:26,920 --> 00:01:31,720
out of LinkedIn both share kind of the

00:01:29,290 --> 00:01:34,240
same underlying mindset and although

00:01:31,720 --> 00:01:37,420
they do different things Sam sir is a

00:01:34,240 --> 00:01:39,130
stream processing project kafka is a

00:01:37,420 --> 00:01:41,530
kind of a message broker so it's the

00:01:39,130 --> 00:01:45,100
thing that transports any messages from

00:01:41,530 --> 00:01:47,110
A to B they you can use each

00:01:45,100 --> 00:01:49,000
individually separately they don't have

00:01:47,110 --> 00:01:50,950
a strict dependency on each other but

00:01:49,000 --> 00:01:53,290
they do do they do go together really

00:01:50,950 --> 00:01:56,079
well i kind of think of it like you know

00:01:53,290 --> 00:01:58,360
the like beer and curry bus you can have

00:01:56,079 --> 00:02:00,189
each taken by itself and it's okay by

00:01:58,360 --> 00:02:03,270
itself but take the two together and

00:02:00,189 --> 00:02:05,140
really perfect so a bit of a background

00:02:03,270 --> 00:02:09,700
what kind of things are we actually

00:02:05,140 --> 00:02:11,290
trying to do with these projects one

00:02:09,700 --> 00:02:13,060
example might be if you have a website

00:02:11,290 --> 00:02:15,610
with some kind of news feed like

00:02:13,060 --> 00:02:18,310
features you want you know various

00:02:15,610 --> 00:02:19,330
people posting updates or maybe liking

00:02:18,310 --> 00:02:21,310
things or commenting

00:02:19,330 --> 00:02:24,700
on things or changing their job title or

00:02:21,310 --> 00:02:25,840
whatever it be a lot of stuff happening

00:02:24,700 --> 00:02:27,610
and you want to make sure that that

00:02:25,840 --> 00:02:29,380
information is shown to the right people

00:02:27,610 --> 00:02:32,230
to people who will find it interesting

00:02:29,380 --> 00:02:34,870
who will likely find it valuable engage

00:02:32,230 --> 00:02:36,400
with it it needs to be reasonably timely

00:02:34,870 --> 00:02:38,350
so you don't want to have to wait four

00:02:36,400 --> 00:02:40,060
hours from posting an update until

00:02:38,350 --> 00:02:42,220
people can see it there might be

00:02:40,060 --> 00:02:44,860
complicated business logic associated

00:02:42,220 --> 00:02:46,690
with this like privacy settings so this

00:02:44,860 --> 00:02:49,350
is one example of the kind of things we

00:02:46,690 --> 00:02:52,060
want to do another which is a bit more

00:02:49,350 --> 00:02:55,990
internal two systems would be something

00:02:52,060 --> 00:02:57,430
like updating a search index so if you

00:02:55,990 --> 00:02:59,860
think about LinkedIn it's kind of like

00:02:57,430 --> 00:03:02,290
one massive search index really so you

00:02:59,860 --> 00:03:04,330
add a keyword to your profile and then

00:03:02,290 --> 00:03:06,250
you go to the search box and you search

00:03:04,330 --> 00:03:08,470
for that keywords to see if you appear

00:03:06,250 --> 00:03:11,290
in the search results so that means we

00:03:08,470 --> 00:03:14,860
need pretty near real-time indexing of

00:03:11,290 --> 00:03:16,840
any updates of data that happens here so

00:03:14,860 --> 00:03:18,730
that you can find yourself again doesn't

00:03:16,840 --> 00:03:20,709
have to be totally instantaneous but

00:03:18,730 --> 00:03:23,440
within a couple of seconds is usually

00:03:20,709 --> 00:03:25,630
what we aim for their even more

00:03:23,440 --> 00:03:30,040
internally to the system you know we

00:03:25,630 --> 00:03:31,750
aggregate a lot of logs metrics internal

00:03:30,040 --> 00:03:33,820
data but which service is calling which

00:03:31,750 --> 00:03:35,950
other data that's a huge volume of stuff

00:03:33,820 --> 00:03:38,380
it's really valuable to able to analyze

00:03:35,950 --> 00:03:40,480
that do things like if suddenly there's

00:03:38,380 --> 00:03:43,780
a spike of exceptions being able to

00:03:40,480 --> 00:03:46,269
quickly react to that so again latency

00:03:43,780 --> 00:03:47,950
is fairly critical there within a couple

00:03:46,269 --> 00:03:51,130
of seconds or minutes we want to be able

00:03:47,950 --> 00:03:54,070
to respond to things now if you think

00:03:51,130 --> 00:03:56,680
about sort of data processing systems in

00:03:54,070 --> 00:03:59,590
general there's kind of this spectrum on

00:03:56,680 --> 00:04:02,170
the one extreme you've got synchronous

00:03:59,590 --> 00:04:04,030
tightly coupled services where every

00:04:02,170 --> 00:04:06,580
time something happens you make say a

00:04:04,030 --> 00:04:09,880
rest call on RPC call or whatever to be

00:04:06,580 --> 00:04:11,650
to some other service and that that

00:04:09,880 --> 00:04:14,140
obviously communicates the information

00:04:11,650 --> 00:04:16,359
immediately to that other service which

00:04:14,140 --> 00:04:18,669
is fine for a lot of things but the more

00:04:16,359 --> 00:04:20,890
of these calls you add the more you

00:04:18,669 --> 00:04:22,270
tightly couple all of these services and

00:04:20,890 --> 00:04:25,990
the whole thing can become a bit of a

00:04:22,270 --> 00:04:27,820
nightmare if there's any individual part

00:04:25,990 --> 00:04:29,229
of the system that slows down the entire

00:04:27,820 --> 00:04:32,080
thing slows down so it's really

00:04:29,229 --> 00:04:33,190
problematic on the other extreme you've

00:04:32,080 --> 00:04:35,470
got systems like

00:04:33,190 --> 00:04:38,410
hadoop mapreduce and all of the

00:04:35,470 --> 00:04:40,690
additional kind of data analysis tools

00:04:38,410 --> 00:04:42,700
that are coming out which work in a sort

00:04:40,690 --> 00:04:45,490
of batch processing fashion right where

00:04:42,700 --> 00:04:48,880
you accumulate a whole bunch of data of

00:04:45,490 --> 00:04:51,160
some kind of fixed size and then you run

00:04:48,880 --> 00:04:54,310
some kind of query or analysis on it and

00:04:51,160 --> 00:04:57,180
then sometime minutes hours maybe even

00:04:54,310 --> 00:04:59,350
days later you get some results so

00:04:57,180 --> 00:05:01,810
that's quite nice because you can

00:04:59,350 --> 00:05:04,960
decouple applications you can have a

00:05:01,810 --> 00:05:06,840
data set in HDFS once and all sorts of

00:05:04,960 --> 00:05:09,970
different analyses can be run on that

00:05:06,840 --> 00:05:13,360
but on the downside there's a lot of

00:05:09,970 --> 00:05:15,760
latency so we're Samsa fits in is kind

00:05:13,360 --> 00:05:17,830
of in between these two so it's not

00:05:15,760 --> 00:05:20,110
synchronous it's for processing that

00:05:17,830 --> 00:05:22,090
happens asynchronously you still want

00:05:20,110 --> 00:05:25,060
that nice decoupling that you would get

00:05:22,090 --> 00:05:27,220
from data pipelines in something like

00:05:25,060 --> 00:05:31,570
Hadoop but you don't want to wait as

00:05:27,220 --> 00:05:33,280
long and the model of communication will

00:05:31,570 --> 00:05:35,710
be very familiar to many of you you know

00:05:33,280 --> 00:05:39,010
you've got a bunch of users making

00:05:35,710 --> 00:05:41,470
requests to services those requests are

00:05:39,010 --> 00:05:44,080
served but as a side effect various

00:05:41,470 --> 00:05:47,530
events get emitted and at LinkedIn we

00:05:44,080 --> 00:05:49,030
use Kafka heavily for this and then

00:05:47,530 --> 00:05:50,350
these events can then in turn be

00:05:49,030 --> 00:05:52,240
consumed for all sorts of different

00:05:50,350 --> 00:05:54,669
purposes it might be for purposes of

00:05:52,240 --> 00:05:57,310
analytics both to show to users or

00:05:54,669 --> 00:05:59,590
internal analytics you might be updating

00:05:57,310 --> 00:06:02,290
caches or maintaining indexes you might

00:05:59,590 --> 00:06:03,970
be sending out push notifications email

00:06:02,290 --> 00:06:05,950
notifications all sorts of things can

00:06:03,970 --> 00:06:08,830
happen so this is a kind of pop

00:06:05,950 --> 00:06:12,010
publish-subscribe communication model

00:06:08,830 --> 00:06:14,110
right and the types of events we can

00:06:12,010 --> 00:06:15,700
deal with is it's a very broad spectrum

00:06:14,110 --> 00:06:17,860
you can think of anything you can think

00:06:15,700 --> 00:06:20,860
of tracking events which would be

00:06:17,860 --> 00:06:22,780
clickstream type things so a user

00:06:20,860 --> 00:06:25,300
clicked on a particular item at a

00:06:22,780 --> 00:06:29,530
particular time and within a particular

00:06:25,300 --> 00:06:31,180
session you can even think of database

00:06:29,530 --> 00:06:33,880
changes you know whenever you make a

00:06:31,180 --> 00:06:36,130
right to a database you can think of

00:06:33,880 --> 00:06:39,700
that also as an event something happened

00:06:36,130 --> 00:06:43,210
namely for primary key X some value

00:06:39,700 --> 00:06:45,190
changed from Y to Z you can think of any

00:06:43,210 --> 00:06:47,139
logs you have even system level metrics

00:06:45,190 --> 00:06:49,749
all of this is kind of event

00:06:47,139 --> 00:06:51,219
that we can be that we can process and

00:06:49,749 --> 00:06:55,270
indeed at LinkedIn all of these things

00:06:51,219 --> 00:06:58,060
do go through Kafka and just to scope

00:06:55,270 --> 00:07:00,159
the problem a bit so that's the style of

00:06:58,060 --> 00:07:03,249
system that this is that we're designing

00:07:00,159 --> 00:07:04,539
here we're thinking of many independent

00:07:03,249 --> 00:07:07,810
consumers which might be run by

00:07:04,539 --> 00:07:09,069
different teams across a big company we

00:07:07,810 --> 00:07:11,139
want the system to have very high

00:07:09,069 --> 00:07:13,919
throughput of millions of messages per

00:07:11,139 --> 00:07:17,919
second we want reasonably low latency

00:07:13,919 --> 00:07:20,379
usually sub 1 second is roughly often

00:07:17,919 --> 00:07:21,939
it's like single-digit milliseconds but

00:07:20,379 --> 00:07:24,659
we're not aiming for you know micro

00:07:21,939 --> 00:07:27,159
second type things that's out of scope

00:07:24,659 --> 00:07:28,960
just to give a bit of an idea of the

00:07:27,159 --> 00:07:31,240
kind of scale we're talking about here a

00:07:28,960 --> 00:07:33,340
couple of big numbers from LinkedIn's

00:07:31,240 --> 00:07:37,029
production system as of a week or two

00:07:33,340 --> 00:07:39,159
ago we use cathcart very heavily for

00:07:37,029 --> 00:07:40,750
these kind of things we pumped hundreds

00:07:39,159 --> 00:07:45,039
of billions of messages per day through

00:07:40,750 --> 00:07:48,550
the system at peak its latest i saw was

00:07:45,039 --> 00:07:50,560
4.4 million messages per second each

00:07:48,550 --> 00:07:52,240
message is you can probably work that

00:07:50,560 --> 00:07:54,939
out I think just under 100 bytes on

00:07:52,240 --> 00:07:57,189
average or so so that adds up to quite a

00:07:54,939 --> 00:07:59,740
bit of network bandwidth actually this

00:07:57,189 --> 00:08:01,919
is distributed across hundreds of

00:07:59,740 --> 00:08:04,899
machines across multiple data centers

00:08:01,919 --> 00:08:06,370
all of that is just kind of to give a

00:08:04,899 --> 00:08:10,180
context of what sort of scale we're

00:08:06,370 --> 00:08:12,879
dealing with so now to Sansa some says a

00:08:10,180 --> 00:08:15,250
framework that allows you to take all of

00:08:12,879 --> 00:08:17,529
these message streams and process them

00:08:15,250 --> 00:08:20,319
and to make that processing as simple as

00:08:17,529 --> 00:08:22,509
possible while still giving you really

00:08:20,319 --> 00:08:24,969
powerful tools to work with but the base

00:08:22,509 --> 00:08:26,860
level API is actually very simple it it

00:08:24,969 --> 00:08:28,449
looks somewhat like a matter in the

00:08:26,860 --> 00:08:32,740
MapReduce programming model so at the

00:08:28,449 --> 00:08:34,539
moment there's this Java API sams itself

00:08:32,740 --> 00:08:37,409
is actually implemented in Scala you can

00:08:34,539 --> 00:08:39,310
use whatever JVM language you fancy

00:08:37,409 --> 00:08:40,959
interface is very simple there's one

00:08:39,310 --> 00:08:43,029
method which is called process and

00:08:40,959 --> 00:08:45,610
that's called every time the message

00:08:43,029 --> 00:08:48,329
comes in and that's it really so every

00:08:45,610 --> 00:08:50,829
message consists of a key and the value

00:08:48,329 --> 00:08:53,860
the key is useful for partitioning which

00:08:50,829 --> 00:08:55,510
I can explain later the message

00:08:53,860 --> 00:08:57,850
collector allows you to send messages

00:08:55,510 --> 00:09:00,490
out again so the mess you get one

00:08:57,850 --> 00:09:00,970
message in zero or more messages can be

00:09:00,490 --> 00:09:03,579
out

00:09:00,970 --> 00:09:06,459
as a result of this process call and the

00:09:03,579 --> 00:09:11,310
coordinator lets you do kind of cluster

00:09:06,459 --> 00:09:14,889
level management stuff so if you want to

00:09:11,310 --> 00:09:18,040
implement a data processing or analysis

00:09:14,889 --> 00:09:21,000
pipeline with this it's useful to take a

00:09:18,040 --> 00:09:23,139
step back and just as a reference point

00:09:21,000 --> 00:09:25,629
think about what we do on the batch

00:09:23,139 --> 00:09:26,949
processing side first okay so with

00:09:25,629 --> 00:09:31,029
MapReduce or with a higher-level

00:09:26,949 --> 00:09:36,100
languages like Pig cascading etc similar

00:09:31,029 --> 00:09:38,500
things apply to spark to any any of

00:09:36,100 --> 00:09:41,199
these many tools that we've heard about

00:09:38,500 --> 00:09:43,300
actually these days there are a couple

00:09:41,199 --> 00:09:46,269
of kind of base level operations they do

00:09:43,300 --> 00:09:47,290
so number one is filtering records you

00:09:46,269 --> 00:09:49,839
know either something matches a

00:09:47,290 --> 00:09:52,089
condition or not that's clear mapping

00:09:49,839 --> 00:09:54,310
which is just taking one and producing

00:09:52,089 --> 00:09:58,089
some transformed version of the of the

00:09:54,310 --> 00:09:59,139
record again one record at a time next

00:09:58,089 --> 00:10:01,089
now it's starting to get more

00:09:59,139 --> 00:10:03,550
interesting you might want to join

00:10:01,089 --> 00:10:06,670
multiple data sets together so take a

00:10:03,550 --> 00:10:08,769
key of a record from one data set key

00:10:06,670 --> 00:10:10,959
from record in the other data set and

00:10:08,769 --> 00:10:13,839
match those up where the key is equal or

00:10:10,959 --> 00:10:16,389
some function of the key very similar to

00:10:13,839 --> 00:10:18,370
join is grouping that would be finding

00:10:16,389 --> 00:10:21,699
all of the items with the same key in

00:10:18,370 --> 00:10:23,199
one data set then once you've grouped of

00:10:21,699 --> 00:10:26,309
course you can aggregate as you know

00:10:23,199 --> 00:10:29,319
some count average whatever you like and

00:10:26,309 --> 00:10:32,110
crucially you can take the output of one

00:10:29,319 --> 00:10:35,379
job and feed it in to the input of the

00:10:32,110 --> 00:10:37,689
next job so in MapReduce you know you

00:10:35,379 --> 00:10:39,699
would write the output to HDFS to some

00:10:37,689 --> 00:10:42,160
directory and then you can start a new

00:10:39,699 --> 00:10:44,529
job or somebody else can start a new job

00:10:42,160 --> 00:10:46,689
which reads that directory and uses that

00:10:44,529 --> 00:10:50,259
as its input like that you can build

00:10:46,689 --> 00:10:52,899
these nice pipelines so we really like

00:10:50,259 --> 00:10:56,620
that way of operating so can we do the

00:10:52,899 --> 00:10:58,089
same thing for streams with streams you

00:10:56,620 --> 00:11:00,490
know there's no beginning there's no end

00:10:58,089 --> 00:11:03,879
it's just constantly data coming in and

00:11:00,490 --> 00:11:06,610
we need to deal with it as it happens so

00:11:03,879 --> 00:11:09,250
if we take each of these operations who

00:11:06,610 --> 00:11:11,170
want to do and adapt it to streams well

00:11:09,250 --> 00:11:13,990
filtering it's easy okay you know you

00:11:11,170 --> 00:11:14,930
take one record in either you pass it on

00:11:13,990 --> 00:11:18,800
or you throw it away

00:11:14,930 --> 00:11:21,740
no problem again map operation again

00:11:18,800 --> 00:11:23,600
very simple you take in one record do a

00:11:21,740 --> 00:11:25,910
bit of processing on it locally and then

00:11:23,600 --> 00:11:28,490
pass the modified version on again no

00:11:25,910 --> 00:11:31,670
problem at all join is where it gets

00:11:28,490 --> 00:11:35,089
more interesting so you have two streams

00:11:31,670 --> 00:11:38,510
coming in each of which has some key

00:11:35,089 --> 00:11:40,399
inside the records now you know what

00:11:38,510 --> 00:11:43,850
what are you actually joining you have

00:11:40,399 --> 00:11:46,610
one event which keep with key X on one

00:11:43,850 --> 00:11:49,010
stream comes in and then at some point

00:11:46,610 --> 00:11:51,350
later you don't know when another event

00:11:49,010 --> 00:11:53,660
with the same key X may come in on the

00:11:51,350 --> 00:11:55,250
other stream somehow we want to match

00:11:53,660 --> 00:11:57,740
that up we'll talk about that in a

00:11:55,250 --> 00:11:59,330
minute grouping again eyes I said is

00:11:57,740 --> 00:12:01,130
actually a very similar operation to

00:11:59,330 --> 00:12:02,810
joining except that you're just doing

00:12:01,130 --> 00:12:04,490
the same thing on one stream you're

00:12:02,810 --> 00:12:07,220
going to have to somehow remember the

00:12:04,490 --> 00:12:09,430
messages because you don't know when the

00:12:07,220 --> 00:12:12,740
next message is going to come up

00:12:09,430 --> 00:12:15,410
aggregation well once you've got the joy

00:12:12,740 --> 00:12:18,500
nor the the grouping aggregation is kind

00:12:15,410 --> 00:12:20,270
of doable but you do there's this kind

00:12:18,500 --> 00:12:22,279
of tricky question about when do you

00:12:20,270 --> 00:12:24,440
know when you're done you know if you

00:12:22,279 --> 00:12:26,900
want to aggregate all of the events from

00:12:24,440 --> 00:12:28,430
one user session for example if you

00:12:26,900 --> 00:12:30,860
don't know when the user of closed their

00:12:28,430 --> 00:12:32,360
browser you know you kind of wait and

00:12:30,860 --> 00:12:34,400
then probably after some time out you

00:12:32,360 --> 00:12:36,650
say maybe in some cases there's some

00:12:34,400 --> 00:12:41,000
kind of end event that you can pick up

00:12:36,650 --> 00:12:44,720
but not in general so oh yes the final

00:12:41,000 --> 00:12:47,390
of these points the output of one job

00:12:44,720 --> 00:12:49,820
becoming the input of the next job again

00:12:47,390 --> 00:12:52,250
we want to use that because it lets us

00:12:49,820 --> 00:12:54,830
build these really scalable composable

00:12:52,250 --> 00:12:57,589
data pipelines but we do need to think

00:12:54,830 --> 00:13:00,529
quite carefully what happens in the case

00:12:57,589 --> 00:13:02,300
of faults so if a machine dies if

00:13:00,529 --> 00:13:05,000
somebody deploys a bad version of the

00:13:02,300 --> 00:13:07,820
code any number of bad things can happen

00:13:05,000 --> 00:13:11,149
we just want to maintain sanity and that

00:13:07,820 --> 00:13:13,610
as well so what I want to focus on right

00:13:11,149 --> 00:13:15,740
now is the stateful parts of stream

00:13:13,610 --> 00:13:18,860
processing as i said the kind of basic

00:13:15,740 --> 00:13:20,810
filtering mapping that's easy but the

00:13:18,860 --> 00:13:22,910
joining the grouping the aggregation

00:13:20,810 --> 00:13:24,829
that's what's hard so let's work with an

00:13:22,910 --> 00:13:27,860
example makes it a bit easier to

00:13:24,829 --> 00:13:29,150
understand say you have a website which

00:13:27,860 --> 00:13:30,710
has ads on it

00:13:29,150 --> 00:13:33,890
and you want to know the click-through

00:13:30,710 --> 00:13:35,870
rates for your ads so you've got events

00:13:33,890 --> 00:13:38,150
coming in saying oh I had an ad

00:13:35,870 --> 00:13:41,390
impression i simply loaded a page with

00:13:38,150 --> 00:13:43,400
an ad on it and on the other side you've

00:13:41,390 --> 00:13:45,560
got ad clicks which is every time

00:13:43,400 --> 00:13:47,030
somebody clicked on an ad so if you want

00:13:45,560 --> 00:13:48,500
to know the click-through rate you need

00:13:47,030 --> 00:13:50,630
both of those numbers you need to know

00:13:48,500 --> 00:13:53,060
how many times the ad was shown and

00:13:50,630 --> 00:13:56,510
which of those times it was clicked on

00:13:53,060 --> 00:13:58,160
so the joy in here needs to happen but

00:13:56,510 --> 00:14:01,160
if you think about it there's a problem

00:13:58,160 --> 00:14:05,870
here so firstly I might load a website

00:14:01,160 --> 00:14:08,480
load a page with an ad on it then go off

00:14:05,870 --> 00:14:10,970
for lunch then an hour later come back

00:14:08,480 --> 00:14:12,650
to my machine and the website is still

00:14:10,970 --> 00:14:15,650
there and I see the ad and click on it

00:14:12,650 --> 00:14:17,780
so now there might be an hour delay or

00:14:15,650 --> 00:14:20,360
even more delay between the impression

00:14:17,780 --> 00:14:22,370
event and the click event do you still

00:14:20,360 --> 00:14:24,440
want to join that you know that probably

00:14:22,370 --> 00:14:26,810
needs to be some kind of maximum window

00:14:24,440 --> 00:14:28,490
of time over which you're willing to

00:14:26,810 --> 00:14:31,130
join otherwise you would just end up

00:14:28,490 --> 00:14:32,600
waiting forever the opposite can happen

00:14:31,130 --> 00:14:34,400
as well it could happen that you

00:14:32,600 --> 00:14:37,160
received the click event before you

00:14:34,400 --> 00:14:39,320
receive the impression event if the

00:14:37,160 --> 00:14:41,660
impression of a queue is backlogged a

00:14:39,320 --> 00:14:43,670
bit well you know they'll they'll be

00:14:41,660 --> 00:14:45,470
some delay in the processing there so

00:14:43,670 --> 00:14:48,470
you can get out of order delivery that

00:14:45,470 --> 00:14:50,540
also makes it interesting and so for

00:14:48,470 --> 00:14:53,210
stream processing we call this a window

00:14:50,540 --> 00:14:56,000
join where you say okay there's some

00:14:53,210 --> 00:14:58,070
window of time which may be a minute or

00:14:56,000 --> 00:15:00,020
an hour or whatever you is appropriate

00:14:58,070 --> 00:15:02,450
for you over which you willing to make

00:15:00,020 --> 00:15:05,330
that join but if you want to do that you

00:15:02,450 --> 00:15:06,800
have to actually buffer the events for

00:15:05,330 --> 00:15:11,150
that period of time you have to remember

00:15:06,800 --> 00:15:14,390
that I saw an impression event for some

00:15:11,150 --> 00:15:16,220
impression ID some unique ID up to an

00:15:14,390 --> 00:15:19,580
hour ago and then when the click event

00:15:16,220 --> 00:15:22,160
comes in you can see our I remembered

00:15:19,580 --> 00:15:24,650
earlier that this event was here so now

00:15:22,160 --> 00:15:27,200
I can join it so you require state you

00:15:24,650 --> 00:15:29,240
have to remember for each key that you

00:15:27,200 --> 00:15:32,750
want to join on what were the messages

00:15:29,240 --> 00:15:34,460
on the other stream that you saw and for

00:15:32,750 --> 00:15:38,390
that you need some kind of key value

00:15:34,460 --> 00:15:39,740
store so how do we implement this in the

00:15:38,390 --> 00:15:41,630
simplest case you know you could just

00:15:39,740 --> 00:15:42,840
keep it in memory if it's a small amount

00:15:41,630 --> 00:15:44,790
of data but

00:15:42,840 --> 00:15:47,160
let's assume you want a larger window of

00:15:44,790 --> 00:15:50,280
time so you can't actually fit the data

00:15:47,160 --> 00:15:51,720
in memory so well one option is you

00:15:50,280 --> 00:15:55,080
actually put all of that in an external

00:15:51,720 --> 00:15:58,560
database so you choose Cassandra HBase

00:15:55,080 --> 00:16:02,580
or MongoDB or react whatever takes your

00:15:58,560 --> 00:16:05,160
fancy every time a message comes in you

00:16:02,580 --> 00:16:07,350
all you then go and store it in this key

00:16:05,160 --> 00:16:09,300
value store and then every time a

00:16:07,350 --> 00:16:11,700
message comes in on the other stream you

00:16:09,300 --> 00:16:13,890
take that take the key check do I have

00:16:11,700 --> 00:16:16,860
something to join with in my store and

00:16:13,890 --> 00:16:19,110
if so are yes join it and omit the

00:16:16,860 --> 00:16:20,940
result if not oh okay or maybe you have

00:16:19,110 --> 00:16:25,020
to store it in order to handle the

00:16:20,940 --> 00:16:27,030
ordering the problem with using this

00:16:25,020 --> 00:16:31,430
kind of key value store is the things

00:16:27,030 --> 00:16:34,200
can get rather slow so samsar is

00:16:31,430 --> 00:16:37,050
optimized for processing for

00:16:34,200 --> 00:16:38,730
high-throughput and we can actually get

00:16:37,050 --> 00:16:43,740
like hundreds of thousands of messages

00:16:38,730 --> 00:16:45,420
per node through samsa and however if

00:16:43,740 --> 00:16:48,030
you're talking over the network for

00:16:45,420 --> 00:16:49,710
every single message you know the QPS

00:16:48,030 --> 00:16:51,990
you can get out of a database will vary

00:16:49,710 --> 00:16:54,150
of course a lot by database what kind of

00:16:51,990 --> 00:16:57,030
hardware you run it on but it can easily

00:16:54,150 --> 00:16:59,460
be orders of magnitude lower than the

00:16:57,030 --> 00:17:02,130
throughput so if you do it this way you

00:16:59,460 --> 00:17:04,890
really risk dropping your throughput of

00:17:02,130 --> 00:17:08,610
your stream processor massively we don't

00:17:04,890 --> 00:17:10,530
want to do that so alternative this is

00:17:08,610 --> 00:17:12,570
where samsar is different from some of

00:17:10,530 --> 00:17:15,720
the other stream processing frameworks

00:17:12,570 --> 00:17:18,570
you might have seen like storm what we

00:17:15,720 --> 00:17:22,050
do is actually every stream processor

00:17:18,570 --> 00:17:24,750
has with it a little key value store

00:17:22,050 --> 00:17:27,420
which is right there on the same machine

00:17:24,750 --> 00:17:29,190
in fact it's in the same process at the

00:17:27,420 --> 00:17:31,110
moment we're using leveldb for that

00:17:29,190 --> 00:17:32,250
although actually we're having a few

00:17:31,110 --> 00:17:34,620
performance problems so we're looking

00:17:32,250 --> 00:17:37,320
into rocks DB other as in which is

00:17:34,620 --> 00:17:41,580
Facebook's fork of leveldb but the idea

00:17:37,320 --> 00:17:43,950
is very much the same because this is in

00:17:41,580 --> 00:17:46,470
process we can read and write to their

00:17:43,950 --> 00:17:48,800
things super fast which is absolutely

00:17:46,470 --> 00:17:50,850
wonderful there's a problem with it

00:17:48,800 --> 00:17:52,610
which you might be thinking of already

00:17:50,850 --> 00:17:54,900
but I will come to that in a minute

00:17:52,610 --> 00:17:56,370
first let me give another example to

00:17:54,900 --> 00:17:58,290
just make clear

00:17:56,370 --> 00:18:01,890
quite how useful this kind of stateful

00:17:58,290 --> 00:18:05,490
stream processing is say you wanted to

00:18:01,890 --> 00:18:07,500
implement Twitter in Twitter in the

00:18:05,490 --> 00:18:09,900
simplest kind of possible incarnation

00:18:07,500 --> 00:18:12,830
you've got two types of events that can

00:18:09,900 --> 00:18:16,080
happen one is a follow or unfollow event

00:18:12,830 --> 00:18:19,020
which is which happens every time some

00:18:16,080 --> 00:18:20,730
kind of follow status changes and the

00:18:19,020 --> 00:18:24,360
other is somebody tweeted or something

00:18:20,730 --> 00:18:27,059
and so this is quite interesting now

00:18:24,360 --> 00:18:29,700
because you can take these follow events

00:18:27,059 --> 00:18:32,070
and build up the social graph the

00:18:29,700 --> 00:18:33,960
follower graph and you can take the

00:18:32,070 --> 00:18:36,030
tweet events and every time someone

00:18:33,960 --> 00:18:38,520
tweets something you know the list of

00:18:36,030 --> 00:18:40,410
all of their followers so you write that

00:18:38,520 --> 00:18:43,230
message out to each of their followers

00:18:40,410 --> 00:18:46,830
and they can then get notified or it can

00:18:43,230 --> 00:18:48,780
be streamed to their mobile app or web

00:18:46,830 --> 00:18:51,660
socket to their browser whatever it be

00:18:48,780 --> 00:18:54,510
right so how do we implement something

00:18:51,660 --> 00:18:57,660
like this you have to input streams

00:18:54,510 --> 00:19:02,400
which we want to join and we need to

00:18:57,660 --> 00:19:04,260
maintain again some kind of state so say

00:19:02,400 --> 00:19:06,990
you have two messages coming in first

00:19:04,260 --> 00:19:10,530
the follower vent so first the event

00:19:06,990 --> 00:19:15,030
saying user 138 is now following user

00:19:10,530 --> 00:19:18,000
582 and you take that and you record it

00:19:15,030 --> 00:19:21,390
in your key value store so you do

00:19:18,000 --> 00:19:24,900
something like have a mapping save from

00:19:21,390 --> 00:19:27,600
user 582 to the list of all of their

00:19:24,900 --> 00:19:31,700
followers and that list now includes 138

00:19:27,600 --> 00:19:34,440
because we said 138 is now following 582

00:19:31,700 --> 00:19:36,450
and I'll use a fine day to tweet

00:19:34,440 --> 00:19:39,420
something they say I'm at Berlin

00:19:36,450 --> 00:19:41,460
buzzwords and it rocks and so all of the

00:19:39,420 --> 00:19:44,010
followers need to be that that message

00:19:41,460 --> 00:19:47,610
now needs to be delivered to all of

00:19:44,010 --> 00:19:49,620
those followers of user 582 so our

00:19:47,610 --> 00:19:51,600
stream processor looks in our key value

00:19:49,620 --> 00:19:54,000
store sees that list of followers and

00:19:51,600 --> 00:19:57,990
for each of the followers writes out a

00:19:54,000 --> 00:20:00,480
new message saying notify this user in

00:19:57,990 --> 00:20:02,820
this case 138 there's a new tweet

00:20:00,480 --> 00:20:05,130
waiting for them so the result is then

00:20:02,820 --> 00:20:08,520
some kind of inbox for each user or

00:20:05,130 --> 00:20:09,840
timeline as Twitter call sir and once

00:20:08,520 --> 00:20:12,330
you've got that

00:20:09,840 --> 00:20:14,220
you condense chain further things off it

00:20:12,330 --> 00:20:16,470
so you could have a job which sends out

00:20:14,220 --> 00:20:21,330
push notifications or streams to a

00:20:16,470 --> 00:20:25,350
browser or whatever it be right so this

00:20:21,330 --> 00:20:27,600
idea of keeping the state is a very

00:20:25,350 --> 00:20:30,179
powerful one because it allows you to do

00:20:27,600 --> 00:20:32,159
these kind of joins and what Sam's that

00:20:30,179 --> 00:20:34,679
tries to do here is to move the

00:20:32,159 --> 00:20:38,279
computation and the data into the same

00:20:34,679 --> 00:20:42,020
place it's maybe a bit comparable to the

00:20:38,279 --> 00:20:43,919
you know the placement of mappers in in

00:20:42,020 --> 00:20:47,100
MapReduce where you try to put the

00:20:43,919 --> 00:20:49,559
mapper locale with locality to the data

00:20:47,100 --> 00:20:52,020
on HDFS here it's kind of the other way

00:20:49,559 --> 00:20:53,730
around we've started up our protests on

00:20:52,020 --> 00:20:56,870
a machine and we make sure that the

00:20:53,730 --> 00:20:59,549
state stays there with the process

00:20:56,870 --> 00:21:02,909
however there's a big problem with this

00:20:59,549 --> 00:21:06,990
and that is how do we make this whole

00:21:02,909 --> 00:21:08,730
thing fault-tolerant now for that I need

00:21:06,990 --> 00:21:10,320
to explain a bit about the architecture

00:21:08,730 --> 00:21:12,000
of Sansa how it actually works

00:21:10,320 --> 00:21:15,539
internally so you might actually find

00:21:12,000 --> 00:21:18,870
this quite interesting on the basis you

00:21:15,539 --> 00:21:20,850
have multiple machines and on each the

00:21:18,870 --> 00:21:24,899
machine probably the first thing you

00:21:20,850 --> 00:21:27,590
will deploy is Kafka so Kafka acts as

00:21:24,899 --> 00:21:31,799
the message transport mechanism here

00:21:27,590 --> 00:21:33,779
Kafka is itself replicated so every item

00:21:31,799 --> 00:21:36,570
of data every message you write to

00:21:33,779 --> 00:21:38,309
kathcar will be copied onto however

00:21:36,570 --> 00:21:40,710
machines you configure say three

00:21:38,309 --> 00:21:42,630
machines so even if one of those

00:21:40,710 --> 00:21:45,179
machines go away you know that the data

00:21:42,630 --> 00:21:48,120
won't be lost the answer the first thing

00:21:45,179 --> 00:21:51,210
you install the second thing you install

00:21:48,120 --> 00:21:53,610
is yarn so Sam's actually runs on top of

00:21:51,210 --> 00:21:55,409
yarn if you have an existing Hadoop to

00:21:53,610 --> 00:21:58,130
cluster then you can just run it on

00:21:55,409 --> 00:22:00,510
there and it should work absolutely fine

00:21:58,130 --> 00:22:01,919
there have been various talks about yarn

00:22:00,510 --> 00:22:04,200
already so i won't go into too much

00:22:01,919 --> 00:22:07,649
detail of how it works but the general

00:22:04,200 --> 00:22:09,750
idea is that each machine has a node

00:22:07,649 --> 00:22:12,480
manager running on it which is in charge

00:22:09,750 --> 00:22:15,779
of all of the processes running on that

00:22:12,480 --> 00:22:17,990
machine and these processes in yarn

00:22:15,779 --> 00:22:22,080
terminology are called containers and

00:22:17,990 --> 00:22:23,670
Samsa provides a yarn container and your

00:22:22,080 --> 00:22:26,100
code your processing

00:22:23,670 --> 00:22:28,410
code is loaded into those containers and

00:22:26,100 --> 00:22:31,350
started up so within each of those

00:22:28,410 --> 00:22:34,200
containers you've got a task instance

00:22:31,350 --> 00:22:36,360
which is running your code and each of

00:22:34,200 --> 00:22:38,700
these task instances has this little

00:22:36,360 --> 00:22:42,690
embedded key value store that I was

00:22:38,700 --> 00:22:46,200
talking about now now what happens if an

00:22:42,690 --> 00:22:50,130
entire machine goes boom and all of this

00:22:46,200 --> 00:22:51,930
is lost so now okay throw away that

00:22:50,130 --> 00:22:54,120
machine we've got another machine over

00:22:51,930 --> 00:22:57,060
here on the cluster it's really got

00:22:54,120 --> 00:22:58,590
Kafka installed on it so at least Kafka

00:22:57,060 --> 00:22:59,670
we don't need to worry about that will

00:22:58,590 --> 00:23:03,060
have already been part of the

00:22:59,670 --> 00:23:05,550
replication but at the moment there's

00:23:03,060 --> 00:23:08,670
just an empty young node manager sitting

00:23:05,550 --> 00:23:10,980
there yarn will notice that oh we've

00:23:08,670 --> 00:23:13,110
lost some containers I guess we should

00:23:10,980 --> 00:23:15,560
start those up again so it goes and

00:23:13,110 --> 00:23:18,690
starts up some Samsa containers again

00:23:15,560 --> 00:23:20,340
Sansa goes ah yeah we've got some empty

00:23:18,690 --> 00:23:22,920
containers which tasks should they be

00:23:20,340 --> 00:23:24,990
running and those tasks which were

00:23:22,920 --> 00:23:27,960
previously running on the failed machine

00:23:24,990 --> 00:23:31,460
and are we started in the containers on

00:23:27,960 --> 00:23:34,230
this new machine so far so good but

00:23:31,460 --> 00:23:36,420
these little key value stores that are

00:23:34,230 --> 00:23:39,060
attached to each of the stream

00:23:36,420 --> 00:23:40,950
processing tasks and are empty because

00:23:39,060 --> 00:23:43,410
we didn't replicate that data you know

00:23:40,950 --> 00:23:45,690
that those key value stores they're just

00:23:43,410 --> 00:23:48,900
on the local file system of each machine

00:23:45,690 --> 00:23:52,170
and that is sadness because we don't

00:23:48,900 --> 00:23:54,480
like losing data so how do we make sure

00:23:52,170 --> 00:23:57,090
that we don't lose data this is one of

00:23:54,480 --> 00:23:58,830
the points where I think Samson's really

00:23:57,090 --> 00:24:00,570
cool actually I can say that it's really

00:23:58,830 --> 00:24:02,940
cool because I didn't invent it so I'm

00:24:00,570 --> 00:24:09,600
not taking any any credit for this at

00:24:02,940 --> 00:24:12,000
all the idea is every time you write to

00:24:09,600 --> 00:24:15,050
this local key value store that's

00:24:12,000 --> 00:24:20,460
embedded in your process you also write

00:24:15,050 --> 00:24:22,950
to Kafka as I said Kafka is replicated

00:24:20,460 --> 00:24:24,720
and durable whenever you write something

00:24:22,950 --> 00:24:28,440
to Kafka you can be sure that it won't

00:24:24,720 --> 00:24:30,450
be lost it has a key value model you

00:24:28,440 --> 00:24:32,030
can't look up things by key so it

00:24:30,450 --> 00:24:35,760
doesn't provide a key value interface

00:24:32,030 --> 00:24:38,700
all you can do is append to the log

00:24:35,760 --> 00:24:40,110
but it can do that very very fast it can

00:24:38,700 --> 00:24:41,970
append to the log incredibly quickly

00:24:40,110 --> 00:24:44,370
with millions of messages per second as

00:24:41,970 --> 00:24:47,820
I was saying so what we're effectively

00:24:44,370 --> 00:24:51,030
doing here is building our own database

00:24:47,820 --> 00:24:52,350
replication log kind of you know a bit

00:24:51,030 --> 00:24:54,900
like a writer head log that you would

00:24:52,350 --> 00:24:57,840
get in a relational database every time

00:24:54,900 --> 00:24:59,850
you write to your key value store you

00:24:57,840 --> 00:25:02,100
also write to their stream of things and

00:24:59,850 --> 00:25:04,650
most of the time nobody's reading from

00:25:02,100 --> 00:25:07,140
it but that's totally fine Kafka just

00:25:04,650 --> 00:25:09,570
sits there Kafka has a few optimizations

00:25:07,140 --> 00:25:11,250
for exactly this kind of thing so if you

00:25:09,570 --> 00:25:13,410
write the same key over and over and

00:25:11,250 --> 00:25:15,780
over again that can get compacted in the

00:25:13,410 --> 00:25:17,580
background so that stops this log from

00:25:15,780 --> 00:25:20,880
growing unbounded and it means that the

00:25:17,580 --> 00:25:22,860
restore time is then bounded as well and

00:25:20,880 --> 00:25:25,860
that's a nice new feature in Kafka 081

00:25:22,860 --> 00:25:28,550
if you haven't seen it yet anyway with

00:25:25,860 --> 00:25:32,160
all of those rights replicated to Kafka

00:25:28,550 --> 00:25:34,980
we can now go back to this Samsa starts

00:25:32,160 --> 00:25:37,950
up okay we've got these key value stores

00:25:34,980 --> 00:25:41,070
but they're empty so let's just consume

00:25:37,950 --> 00:25:42,990
that replication topic all of those data

00:25:41,070 --> 00:25:45,240
all of those messages those change

00:25:42,990 --> 00:25:47,970
messages every time we wrote to the key

00:25:45,240 --> 00:25:50,190
value store are still there we can just

00:25:47,970 --> 00:25:52,700
suck all of that in apply them in order

00:25:50,190 --> 00:25:55,140
and once we've done that we've restored

00:25:52,700 --> 00:25:57,540
those key value stores to their former

00:25:55,140 --> 00:25:59,340
glory then I'll contain just what they

00:25:57,540 --> 00:26:02,970
did beforehand and we are happy so

00:25:59,340 --> 00:26:05,340
that's quite nice just to recap so the

00:26:02,970 --> 00:26:08,400
idea here is we replicate all of the

00:26:05,340 --> 00:26:12,360
right to Kafka we can restore from that

00:26:08,400 --> 00:26:14,430
and compaction built into Kafka means

00:26:12,360 --> 00:26:17,490
that we don't end up using all the disk

00:26:14,430 --> 00:26:19,830
space in the world so I was talking

00:26:17,490 --> 00:26:22,860
about fault tolerance there's another

00:26:19,830 --> 00:26:24,660
aspect of full tolerance that's not as

00:26:22,860 --> 00:26:26,370
often talked about as machines blowing

00:26:24,660 --> 00:26:31,040
up but it's actually at least as

00:26:26,370 --> 00:26:34,110
important and that is things go slow and

00:26:31,040 --> 00:26:35,880
when things go slow you know it's still

00:26:34,110 --> 00:26:38,250
kind of working but actually everything

00:26:35,880 --> 00:26:41,460
falls apart and it's the hardest thing

00:26:38,250 --> 00:26:43,410
to debug because you know sometimes just

00:26:41,460 --> 00:26:45,630
one thing going slow and cause another

00:26:43,410 --> 00:26:48,120
thing to go slow and suddenly everything

00:26:45,630 --> 00:26:48,500
is going slow and you run out of threads

00:26:48,120 --> 00:26:50,450
and

00:26:48,500 --> 00:26:54,620
everything is bad so we don't want that

00:26:50,450 --> 00:27:01,760
to happen in a stream processing

00:26:54,620 --> 00:27:05,270
environment as I said we want to be able

00:27:01,760 --> 00:27:09,710
to change jobs together and they might

00:27:05,270 --> 00:27:12,350
be might be consuming multiple inputs

00:27:09,710 --> 00:27:14,360
may be producing multiple outputs the

00:27:12,350 --> 00:27:16,280
output of one job could be consumed by

00:27:14,360 --> 00:27:17,930
multiple different jobs and you want to

00:27:16,280 --> 00:27:20,120
be able to compose these things very

00:27:17,930 --> 00:27:22,280
freely without any constraints in

00:27:20,120 --> 00:27:23,810
particular you might want different jobs

00:27:22,280 --> 00:27:26,690
to be maintained by different teams

00:27:23,810 --> 00:27:28,430
within your company because actually you

00:27:26,690 --> 00:27:32,330
know this output of one job it's a very

00:27:28,430 --> 00:27:34,310
nice interface I can you know team X can

00:27:32,330 --> 00:27:36,650
just say ok we have these jobs two and

00:27:34,310 --> 00:27:39,230
three and you can consume our output and

00:27:36,650 --> 00:27:41,570
there'll be a certain SLA to you know

00:27:39,230 --> 00:27:44,720
the reliability or the speed with which

00:27:41,570 --> 00:27:47,270
data goes through there and so team y

00:27:44,720 --> 00:27:49,250
and team said can rely on that build

00:27:47,270 --> 00:27:51,410
their own jobs which which consume that

00:27:49,250 --> 00:27:54,260
data team X doesn't need to worry about

00:27:51,410 --> 00:27:56,570
the fact that it has these consumers you

00:27:54,260 --> 00:27:59,810
know t-max just provides the data anyone

00:27:56,570 --> 00:28:02,390
can read it now what happens if this job

00:27:59,810 --> 00:28:05,210
here maintained by team y goes slow and

00:28:02,390 --> 00:28:09,220
the turtle is sticking its arms in the

00:28:05,210 --> 00:28:12,980
leg looking really sad in this case well

00:28:09,220 --> 00:28:16,100
there are a couple of options option one

00:28:12,980 --> 00:28:17,720
is you can drop data so you can say okay

00:28:16,100 --> 00:28:21,110
sorry you weren't fast enough to pick up

00:28:17,720 --> 00:28:22,730
the data it's gone now sorry but we

00:28:21,110 --> 00:28:25,910
don't really like that I don't like

00:28:22,730 --> 00:28:27,710
losing data the second option is back

00:28:25,910 --> 00:28:30,020
pressure and this is very commonly

00:28:27,710 --> 00:28:32,720
applied so storm for example again uses

00:28:30,020 --> 00:28:34,970
this model which is well okay if you're

00:28:32,720 --> 00:28:38,180
not consuming fast enough let's just

00:28:34,970 --> 00:28:39,920
wait and give you some time to catch up

00:28:38,180 --> 00:28:41,330
and then when you're ready to consume

00:28:39,920 --> 00:28:44,900
some more data will give you some more

00:28:41,330 --> 00:28:48,830
data the problem with that pressure is

00:28:44,900 --> 00:28:50,600
that now the producer of this data has

00:28:48,830 --> 00:28:53,270
to wait for the consumer of this data

00:28:50,600 --> 00:28:55,880
and when the producer of the data is

00:28:53,270 --> 00:28:58,700
waiting all other consumers of the same

00:28:55,880 --> 00:29:00,110
data also have to wait and all producers

00:28:58,700 --> 00:29:02,120
who are feeding into that producer also

00:29:00,110 --> 00:29:02,300
have to wait and suddenly everything is

00:29:02,120 --> 00:29:04,040
way

00:29:02,300 --> 00:29:06,620
thing for everything just because of one

00:29:04,040 --> 00:29:09,140
stupid slow job so we don't want that

00:29:06,620 --> 00:29:11,840
either okay then that would cause the

00:29:09,140 --> 00:29:14,180
entire system to grind to a halt so the

00:29:11,840 --> 00:29:17,510
only option we have here is to queue up

00:29:14,180 --> 00:29:19,600
the data so if somebody's slow to

00:29:17,510 --> 00:29:22,130
consume it will just store it somewhere

00:29:19,600 --> 00:29:24,590
so that when they come back and they

00:29:22,130 --> 00:29:26,930
start protesting fast again that's they

00:29:24,590 --> 00:29:29,690
can get the data that they missed in the

00:29:26,930 --> 00:29:31,850
interim now if you're queuing again

00:29:29,690 --> 00:29:34,460
there are two options either you can

00:29:31,850 --> 00:29:36,080
queue up in memory and then if you have

00:29:34,460 --> 00:29:38,270
high volume streams you will run out of

00:29:36,080 --> 00:29:41,030
memory eventually and then again we have

00:29:38,270 --> 00:29:43,280
sadness so that leaves actually only one

00:29:41,030 --> 00:29:46,520
remaining option which is you have to

00:29:43,280 --> 00:29:48,650
spill this data to disk which kind of

00:29:46,520 --> 00:29:51,620
sounds like you know the best of a bad

00:29:48,650 --> 00:29:54,560
bunch but actually Kafka writes

00:29:51,620 --> 00:29:56,510
everything to disk anyway every single

00:29:54,560 --> 00:29:58,520
message you write to Kafka is already

00:29:56,510 --> 00:30:00,500
written to disk and it has specialized

00:29:58,520 --> 00:30:03,290
in making this disk based architecture

00:30:00,500 --> 00:30:05,930
work really well so actually what we do

00:30:03,290 --> 00:30:09,820
with samsar is simply every single job

00:30:05,930 --> 00:30:12,200
always write its output streams to Kafka

00:30:09,820 --> 00:30:14,150
this is really nice because anyone can

00:30:12,200 --> 00:30:17,000
then just consume those and Kafka acts

00:30:14,150 --> 00:30:19,130
as the buffer the queue in between those

00:30:17,000 --> 00:30:21,020
jobs and it decouples the jobs from

00:30:19,130 --> 00:30:23,420
another warrant from one another and

00:30:21,020 --> 00:30:26,720
Kafka can keep like days or even weeks

00:30:23,420 --> 00:30:30,080
worth of buffer because disks are cheap

00:30:26,720 --> 00:30:32,150
and you can use SSDs or you can use

00:30:30,080 --> 00:30:34,190
spinning hard drives it does all

00:30:32,150 --> 00:30:37,400
sequential I oh so it actually works

00:30:34,190 --> 00:30:39,950
remarkably well Sam's are always right

00:30:37,400 --> 00:30:42,740
its job output to Kafka which kind of by

00:30:39,950 --> 00:30:44,710
analogy you can think of as MapReduce

00:30:42,740 --> 00:30:48,050
every single processing stage

00:30:44,710 --> 00:30:49,700
materializes its output to HDFS which I

00:30:48,050 --> 00:30:51,830
realize it's kind of out of fashion

00:30:49,700 --> 00:30:55,220
these days with things like spark which

00:30:51,830 --> 00:30:57,160
try to not materialize to disk so I

00:30:55,220 --> 00:30:59,180
guess this turn couldn't just be a

00:30:57,160 --> 00:31:01,880
counterpoint to that not saying that

00:30:59,180 --> 00:31:03,740
they're wrong just saying that actually

00:31:01,880 --> 00:31:05,990
there are advantages if you write to

00:31:03,740 --> 00:31:08,780
disk because you can then give the

00:31:05,990 --> 00:31:11,930
stream a name you can tell anyone that

00:31:08,780 --> 00:31:14,450
they can consume it there's no buffering

00:31:11,930 --> 00:31:16,320
no back back pressure no dropping of

00:31:14,450 --> 00:31:18,789
data required

00:31:16,320 --> 00:31:21,879
it's durable which means that even when

00:31:18,789 --> 00:31:23,649
things go away when machines go away you

00:31:21,879 --> 00:31:25,570
can still be available if you want to

00:31:23,649 --> 00:31:27,340
debug your system and figure out why

00:31:25,570 --> 00:31:28,929
you're getting bad data you can just

00:31:27,340 --> 00:31:31,210
attach to one of them you can just look

00:31:28,929 --> 00:31:33,519
at it it's really beautiful actually and

00:31:31,210 --> 00:31:38,649
it's a very clean interface between jobs

00:31:33,519 --> 00:31:41,649
so to recap what I talked about were a

00:31:38,649 --> 00:31:44,289
few things in Sansa and how we solve

00:31:41,649 --> 00:31:47,470
those problems one problem we talked

00:31:44,289 --> 00:31:50,200
about was this buffering one jobs output

00:31:47,470 --> 00:31:52,600
becomes another jobs in put our solution

00:31:50,200 --> 00:31:55,330
is simple we write it to kafka kafka

00:31:52,600 --> 00:31:57,970
takes care of that buffering the other

00:31:55,330 --> 00:32:00,759
problem I talked about beforehand if you

00:31:57,970 --> 00:32:02,379
remember the key value store and this

00:32:00,759 --> 00:32:04,480
state and how do we make that full

00:32:02,379 --> 00:32:07,330
tolerance even though it's on the same

00:32:04,480 --> 00:32:09,879
machine and just on one machine we want

00:32:07,330 --> 00:32:12,639
it not to die no not to lose that state

00:32:09,879 --> 00:32:14,919
if the machine dies the answer is also

00:32:12,639 --> 00:32:17,019
we write it to Kafka because Kafka

00:32:14,919 --> 00:32:20,320
replicates it to multiple machines and

00:32:17,019 --> 00:32:23,399
makes it durable one final thing that I

00:32:20,320 --> 00:32:25,629
didn't talk about big out of time but

00:32:23,399 --> 00:32:28,059
it's also quite interesting to look at

00:32:25,629 --> 00:32:31,210
is actually the checkpointing so if you

00:32:28,059 --> 00:32:32,889
need if at some job dies either by

00:32:31,210 --> 00:32:35,049
hardware or software failure and you

00:32:32,889 --> 00:32:36,580
need to bring it back up again how do

00:32:35,049 --> 00:32:38,740
you know where it should start well you

00:32:36,580 --> 00:32:40,570
need some kind of checkpoints you could

00:32:38,740 --> 00:32:42,549
write those checkpoints to zookeeper but

00:32:40,570 --> 00:32:44,350
we found the zookeeper can easily become

00:32:42,549 --> 00:32:46,659
a bottleneck if you're writing to it too

00:32:44,350 --> 00:32:48,519
much actually we've got a system that we

00:32:46,659 --> 00:32:51,610
can write to really well which handles

00:32:48,519 --> 00:32:53,470
really writes really well guess what we

00:32:51,610 --> 00:32:55,149
can write it to Casca and that's why

00:32:53,470 --> 00:32:57,639
there's this great relationship between

00:32:55,149 --> 00:33:00,009
samsar and Kafka okay so I do encourage

00:32:57,639 --> 00:33:01,809
you to give it a try it's all open

00:33:00,009 --> 00:33:05,259
source Kafka is a top level Apache

00:33:01,809 --> 00:33:08,379
project Samsa is in the incubator at the

00:33:05,259 --> 00:33:11,200
moment and definitely looking for

00:33:08,379 --> 00:33:12,789
contributors as well I suggest the first

00:33:11,200 --> 00:33:15,639
thing to take a look at is actually

00:33:12,789 --> 00:33:17,529
hello Samsa which is a just a little

00:33:15,639 --> 00:33:20,139
script which installed the local cluster

00:33:17,529 --> 00:33:23,080
for you it downloads yarn it downloads

00:33:20,139 --> 00:33:25,509
zookeeper and the downloads kafka starts

00:33:23,080 --> 00:33:27,820
those three up and then runs a job which

00:33:25,509 --> 00:33:30,220
consumes a live feed of edits on

00:33:27,820 --> 00:33:32,169
Wikipedia so every time someone edits

00:33:30,220 --> 00:33:33,549
page on Wikipedia did you know they

00:33:32,169 --> 00:33:36,370
actually published this to an IRC

00:33:33,549 --> 00:33:38,890
channel we can consume that IRC channel

00:33:36,370 --> 00:33:40,780
as an input here and then write some

00:33:38,890 --> 00:33:42,970
Sam's our jobs which do some analytics

00:33:40,780 --> 00:33:45,370
on that you can just run it in five

00:33:42,970 --> 00:33:46,990
minutes it's it's really neat so here

00:33:45,370 --> 00:33:49,480
are some links for you to get started

00:33:46,990 --> 00:33:51,549
there's a nice blog post by Jake reps

00:33:49,480 --> 00:33:53,559
the second point here one of my

00:33:51,549 --> 00:33:56,700
colleagues at LinkedIn who kind of has

00:33:53,559 --> 00:33:59,549
set out the underlying thinking behind

00:33:56,700 --> 00:34:03,010
samsa and this mode of stream processing

00:33:59,549 --> 00:34:05,049
and there's me on Twitter and my blog as

00:34:03,010 --> 00:34:08,700
well if you fancy so hopefully we have a

00:34:05,049 --> 00:34:08,700
little bit of time for questions as well

00:34:15,000 --> 00:34:21,300
a while ago I had some hoping that some

00:34:19,379 --> 00:34:23,570
say is still quite new and quite

00:34:21,300 --> 00:34:25,530
immature what's the current state how

00:34:23,570 --> 00:34:28,860
production-ready is it when will you

00:34:25,530 --> 00:34:30,480
expect production readiness it's rapidly

00:34:28,860 --> 00:34:32,149
maturing I should say so it's still a

00:34:30,480 --> 00:34:33,960
new project that's absolutely true

00:34:32,149 --> 00:34:37,139
linkedin is running it in production

00:34:33,960 --> 00:34:39,090
though so we've currently got I think to

00:34:37,139 --> 00:34:41,010
production jobs and working very

00:34:39,090 --> 00:34:44,460
actively right now to move more into

00:34:41,010 --> 00:34:46,139
production as that happens of course we

00:34:44,460 --> 00:34:49,440
discover issues and we're ironing them

00:34:46,139 --> 00:34:51,570
out so it's linkedin is betting very

00:34:49,440 --> 00:34:54,270
heavily on this actually so important

00:34:51,570 --> 00:34:56,070
jobs are being put on it so if it's not

00:34:54,270 --> 00:35:04,620
totally mature yet then it will be

00:34:56,070 --> 00:35:08,810
pretty soon if you compared to other

00:35:04,620 --> 00:35:12,090
things like rabbitmq activemq or this

00:35:08,810 --> 00:35:14,400
terrible commercial USB buses palawan

00:35:12,090 --> 00:35:16,590
could you replace it so I could you

00:35:14,400 --> 00:35:18,450
imagine that it would be able to replace

00:35:16,590 --> 00:35:22,980
it in production was the same

00:35:18,450 --> 00:35:24,930
reliability so while rabbitmq activemq

00:35:22,980 --> 00:35:27,450
they are more like message brokers so

00:35:24,930 --> 00:35:29,280
they don't give you a a framework for

00:35:27,450 --> 00:35:32,040
actually processing the data that would

00:35:29,280 --> 00:35:35,250
depend on some libraries so Sam's are

00:35:32,040 --> 00:35:37,950
focuses on the processing side kafka on

00:35:35,250 --> 00:35:40,110
the message broker side so you could

00:35:37,950 --> 00:35:42,180
definitely have a sams a job that

00:35:40,110 --> 00:35:44,340
consumes from something like RabbitMQ or

00:35:42,180 --> 00:35:47,820
activemq use that as an input that's

00:35:44,340 --> 00:35:51,420
totally fine some of the fault tolerance

00:35:47,820 --> 00:35:54,140
things in Sansa rely on semantics of

00:35:51,420 --> 00:35:56,580
Kafka if you can implement those same

00:35:54,140 --> 00:35:58,710
semantics with a different queuing with

00:35:56,580 --> 00:36:02,460
a different broker you can get the same

00:35:58,710 --> 00:36:07,530
end result but it with some it's easier

00:36:02,460 --> 00:36:09,660
with some it's harder we we are we're

00:36:07,530 --> 00:36:11,670
pushing most of our data through Kafka

00:36:09,660 --> 00:36:15,420
rather than one of the other message

00:36:11,670 --> 00:36:17,720
queues mainly for scale reasons so Kafka

00:36:15,420 --> 00:36:21,060
can just handle vastly bigger throughput

00:36:17,720 --> 00:36:22,740
at reasonably low cost for the for the

00:36:21,060 --> 00:36:25,140
kind of message throughput we need and

00:36:22,740 --> 00:36:27,260
the semantics are actually very nice as

00:36:25,140 --> 00:36:27,260
well

00:36:27,759 --> 00:36:35,420
hi I came across an interesting solution

00:36:32,420 --> 00:36:37,490
to the slowness problem which may or may

00:36:35,420 --> 00:36:40,640
not work up to the slowness problem the

00:36:37,490 --> 00:36:42,650
slowness Paul tell about where they they

00:36:40,640 --> 00:36:46,490
applied back pressure which then caused

00:36:42,650 --> 00:36:47,930
the the sender to start aggregating now

00:36:46,490 --> 00:36:51,160
that that's an interest it becomes an

00:36:47,930 --> 00:36:53,059
interesting a data structure problem but

00:36:51,160 --> 00:36:54,650
it seemed to work for them pretty well

00:36:53,059 --> 00:36:56,809
and it seems like something that sums

00:36:54,650 --> 00:36:58,249
that could actually implement as opposed

00:36:56,809 --> 00:37:01,910
to something like for you know Kafka or

00:36:58,249 --> 00:37:03,259
rabbitmq potentially yes so at the

00:37:01,910 --> 00:37:05,930
moment we've deliberately made the

00:37:03,259 --> 00:37:07,430
framework really simple and made sure

00:37:05,930 --> 00:37:10,009
that those foundations are really

00:37:07,430 --> 00:37:13,039
reliable so actually focusing a lot more

00:37:10,009 --> 00:37:15,049
on the operational side than on the cool

00:37:13,039 --> 00:37:17,900
research ideas that we have for the

00:37:15,049 --> 00:37:19,730
future right now but there are loads of

00:37:17,900 --> 00:37:21,619
potential extensions we could make to

00:37:19,730 --> 00:37:24,829
the framework that's a good one to keep

00:37:21,619 --> 00:37:27,559
in mind yes how do you compare some cell

00:37:24,829 --> 00:37:30,049
for example two espurr with facilitating

00:37:27,559 --> 00:37:32,390
window joints like Esper has its own

00:37:30,049 --> 00:37:35,089
query language and can do whatever

00:37:32,390 --> 00:37:37,190
window joins with in two hours or

00:37:35,089 --> 00:37:40,069
something do I have to do it myself in

00:37:37,190 --> 00:37:41,779
Samsa or sir yes so it sounds it

00:37:40,069 --> 00:37:44,089
provides a very low level interface at

00:37:41,779 --> 00:37:46,220
the moment just a Java API that you saw

00:37:44,089 --> 00:37:47,930
and similarly you could have a Java API

00:37:46,220 --> 00:37:50,269
for reading and writing to the yorkies

00:37:47,930 --> 00:37:52,759
window joins I just have to do myself

00:37:50,269 --> 00:37:54,859
and using this key value store yes at

00:37:52,759 --> 00:37:56,900
the moment those kind of high level

00:37:54,859 --> 00:37:58,700
operations you have to build yourself we

00:37:56,900 --> 00:38:00,890
have been thinking a lot about what a

00:37:58,700 --> 00:38:02,359
good stream processing language a

00:38:00,890 --> 00:38:05,299
high-level language would look like

00:38:02,359 --> 00:38:07,339
which then compiles down to these kind

00:38:05,299 --> 00:38:09,289
of things we haven't rushed into

00:38:07,339 --> 00:38:10,999
building one yet because we want to make

00:38:09,289 --> 00:38:13,759
sure that we really understand the

00:38:10,999 --> 00:38:15,920
problem domain well but this is an

00:38:13,759 --> 00:38:17,329
invitation to all of you if you think

00:38:15,920 --> 00:38:19,279
you have good ideas for what a

00:38:17,329 --> 00:38:21,200
high-level stream processing language

00:38:19,279 --> 00:38:23,480
would look like by all means please

00:38:21,200 --> 00:38:25,609
implement them share your ideas that the

00:38:23,480 --> 00:38:28,460
more ideas we get in this space the

00:38:25,609 --> 00:38:29,869
better so something like Esper would

00:38:28,460 --> 00:38:31,400
probably be a good starting point I

00:38:29,869 --> 00:38:34,160
don't know how well it would work with

00:38:31,400 --> 00:38:37,130
the distributed nature of samsar but I'm

00:38:34,160 --> 00:38:39,200
not an expert in Esper so it seems that

00:38:37,130 --> 00:38:40,130
the philosophy for me all right your

00:38:39,200 --> 00:38:43,070
architecture

00:38:40,130 --> 00:38:46,460
like write everything in Kafka which is

00:38:43,070 --> 00:38:49,280
like a really fast thing so why do you

00:38:46,460 --> 00:38:51,800
also need another local database to

00:38:49,280 --> 00:38:54,320
store your data for aggregates and not

00:38:51,800 --> 00:38:57,920
just write everything in calf kind

00:38:54,320 --> 00:39:00,410
really from there Kafka has deliberately

00:38:57,920 --> 00:39:03,260
the simplest possible data structure

00:39:00,410 --> 00:39:06,740
that could work so the philosophy there

00:39:03,260 --> 00:39:09,020
is all you can do really is a pen to

00:39:06,740 --> 00:39:11,810
file all that there two operations one

00:39:09,020 --> 00:39:14,480
is a pen to file that's the only right

00:39:11,810 --> 00:39:17,750
you can do and the only read you can do

00:39:14,480 --> 00:39:20,150
is take a file off set somewhere in this

00:39:17,750 --> 00:39:22,490
linear sequence and start reading from

00:39:20,150 --> 00:39:24,650
there so it doesn't provide key value

00:39:22,490 --> 00:39:27,230
access at all the only thing you can do

00:39:24,650 --> 00:39:29,420
is sequentially read messages in the

00:39:27,230 --> 00:39:31,940
order that they were published and

00:39:29,420 --> 00:39:34,250
because it has this really simple model

00:39:31,940 --> 00:39:36,770
it can do that really really well but

00:39:34,250 --> 00:39:38,480
then if you want arbitrary random access

00:39:36,770 --> 00:39:41,030
to it you then kind of need to index

00:39:38,480 --> 00:39:43,250
this so you can kind of think of it like

00:39:41,030 --> 00:39:47,120
a heap file versus and index in a in a

00:39:43,250 --> 00:39:48,710
relational database and the index is not

00:39:47,120 --> 00:39:53,540
provided by Kafka so that's something

00:39:48,710 --> 00:39:58,340
you can then build with Sansa one

00:39:53,540 --> 00:40:02,570
question for my side please crucial

00:39:58,340 --> 00:40:07,750
crucial feature is this time window

00:40:02,570 --> 00:40:12,140
based storage where is it implemented

00:40:07,750 --> 00:40:14,990
sorry the crucial is what it sees time

00:40:12,140 --> 00:40:17,900
window they did a lifetime of the events

00:40:14,990 --> 00:40:20,390
in the stream time window April time

00:40:17,900 --> 00:40:22,730
time minus baseline lifetime where these

00:40:20,390 --> 00:40:25,220
features implemented so at the moment

00:40:22,730 --> 00:40:27,560
there's no built-in implementation of

00:40:25,220 --> 00:40:30,350
window joins we give you just these

00:40:27,560 --> 00:40:32,810
low-level api's of receive message

00:40:30,350 --> 00:40:35,600
published message read from key value

00:40:32,810 --> 00:40:37,070
store right to key value store and as

00:40:35,600 --> 00:40:39,710
you can do range queries and a few

00:40:37,070 --> 00:40:41,810
things like that so that again is

00:40:39,710 --> 00:40:45,170
deliberate just wanting to make sure

00:40:41,810 --> 00:40:47,060
that we understand what the API should

00:40:45,170 --> 00:40:49,550
look like really well before rushing

00:40:47,060 --> 00:40:52,790
into building something so at the moment

00:40:49,550 --> 00:40:53,900
each job would have to build the window

00:40:52,790 --> 00:40:57,020
joint implementation

00:40:53,900 --> 00:40:59,300
self but it means you can have any kind

00:40:57,020 --> 00:41:00,830
of implementation you want and then we

00:40:59,300 --> 00:41:02,450
reckon that you know maybe in six months

00:41:00,830 --> 00:41:05,150
time or years time we will have seen

00:41:02,450 --> 00:41:07,910
okay from our experience of seeing these

00:41:05,150 --> 00:41:10,010
15 different production jobs this

00:41:07,910 --> 00:41:11,600
implementation works really well now we

00:41:10,010 --> 00:41:14,840
can take that out and put it in the

00:41:11,600 --> 00:41:21,650
framework but yes it's deliberately

00:41:14,840 --> 00:41:23,330
focusing on simplicity right now okay

00:41:21,650 --> 00:41:24,560
are we done then you can always still

00:41:23,330 --> 00:41:26,770
thank me later thank you very much for

00:41:24,560 --> 00:41:26,770

YouTube URL: https://www.youtube.com/watch?v=d63kSjxVsGA


