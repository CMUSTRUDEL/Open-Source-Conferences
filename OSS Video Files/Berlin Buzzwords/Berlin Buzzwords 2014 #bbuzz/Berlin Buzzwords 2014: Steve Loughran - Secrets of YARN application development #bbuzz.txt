Title: Berlin Buzzwords 2014: Steve Loughran - Secrets of YARN application development #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Learning from the mistakes of others is better than learning from your own -this talk lets anyone writing a Hadoop YARN application learn from mine. Client, Application Master, and worker design, RPC service interfaces, secure operation, failure-handling and test strategies, are all key issues you need to get right -the key points being "Model-View-Controller" is still a good architecture, while mock tests are the secret to testing that model.

This talk tells people writing YARN applications what they need to know -to help the build YARN applications that can work with all the data waiting for them in Hadoop clusters.

Read more:
https://2014.berlinbuzzwords.de/session/secrets-yarn-application-development

About Steve Loughran:
https://2014.berlinbuzzwords.de/user/228/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,629 --> 00:00:15,459
hello everyone I am Steve breaker of

00:00:11,650 --> 00:00:17,560
things at horton works and i'm here to

00:00:15,459 --> 00:00:21,700
talk about the secret of yarn

00:00:17,560 --> 00:00:24,099
application development I've been doing

00:00:21,700 --> 00:00:26,039
some of this myself and these are these

00:00:24,099 --> 00:00:30,490
are some of the lessons I've learned

00:00:26,039 --> 00:00:32,640
before I start hands up who's heard of

00:00:30,490 --> 00:00:34,930
yarn tool hands up who's heard were this

00:00:32,640 --> 00:00:37,440
okay keep your hands off you've actually

00:00:34,930 --> 00:00:40,180
coded applications specifically for it

00:00:37,440 --> 00:00:42,400
okay those look at the hecklers all

00:00:40,180 --> 00:00:44,559
right over there a cow I will I will

00:00:42,400 --> 00:00:46,150
maybe except questions from you but I'm

00:00:44,559 --> 00:00:52,449
a bit hung over so don't shout them too

00:00:46,150 --> 00:00:56,080
much okay hey this is me back in the

00:00:52,449 --> 00:00:57,909
beginning there was Hadoop and there was

00:00:56,080 --> 00:01:01,030
Matt produce and it was incredibly

00:00:57,909 --> 00:01:03,070
successful because it it made writing

00:01:01,030 --> 00:01:04,689
distribute applications easier for the

00:01:03,070 --> 00:01:06,580
first time you didn't have to understand

00:01:04,689 --> 00:01:11,500
all the problems you just wrote a mapper

00:01:06,580 --> 00:01:13,659
and reducer and it works oh it worked

00:01:11,500 --> 00:01:16,750
provided your code was either a map or a

00:01:13,659 --> 00:01:18,690
reducer but as the amount of data the

00:01:16,750 --> 00:01:21,100
cluster collective got bigger and bigger

00:01:18,690 --> 00:01:23,920
other people in the organization wanted

00:01:21,100 --> 00:01:25,630
to run their code on it too and the only

00:01:23,920 --> 00:01:29,020
way you could do this was by having your

00:01:25,630 --> 00:01:31,300
code pretend to be a map or a reducer

00:01:29,020 --> 00:01:34,000
with her a hack known as the long-lived

00:01:31,300 --> 00:01:37,000
mapper we'd start a MapReduce job that

00:01:34,000 --> 00:01:38,610
would never ever finish it was an ugly

00:01:37,000 --> 00:01:41,290
hat with just about got things to work

00:01:38,610 --> 00:01:42,970
it but I had bad failure modes you

00:01:41,290 --> 00:01:45,370
couldn't really expand it on demand or

00:01:42,970 --> 00:01:46,780
shrink and or choose where things would

00:01:45,370 --> 00:01:49,300
be it would just be where your data

00:01:46,780 --> 00:01:52,360
walls around them and the Ops teams hate

00:01:49,300 --> 00:01:54,729
it too that job tracker it just wasn't

00:01:52,360 --> 00:01:56,409
designed for long live code it couldn't

00:01:54,729 --> 00:01:59,650
deal with the scam and failure so it was

00:01:56,409 --> 00:02:03,430
an ugly huh so along along comes a

00:01:59,650 --> 00:02:07,330
solution yarn yet another resource

00:02:03,430 --> 00:02:10,030
negotiator its aim in life is to let you

00:02:07,330 --> 00:02:11,950
run other other algorithms alongside

00:02:10,030 --> 00:02:13,959
MapReduce so Matt produces there it

00:02:11,950 --> 00:02:16,750
still runs happily we try and run

00:02:13,959 --> 00:02:18,099
existing code but we also you can rather

00:02:16,750 --> 00:02:20,379
the code there and now there are

00:02:18,099 --> 00:02:23,189
projects actually building the other

00:02:20,379 --> 00:02:23,189
tools to go alongside

00:02:25,590 --> 00:02:31,900
so if you view Hadoop as a kind of data

00:02:30,040 --> 00:02:34,980
central that they descend to level OS

00:02:31,900 --> 00:02:37,540
yarn yarn is part the execution engine

00:02:34,980 --> 00:02:39,550
right down the bottom there's almost a

00:02:37,540 --> 00:02:42,220
device driver level of the host OS and

00:02:39,550 --> 00:02:44,319
the networking stuff you don't need to

00:02:42,220 --> 00:02:46,090
understand the details you don't need to

00:02:44,319 --> 00:02:48,220
look at linux device driver internals

00:02:46,090 --> 00:02:51,630
but it is kind of handy to have a vague

00:02:48,220 --> 00:02:53,769
idea what's going on same for networking

00:02:51,630 --> 00:02:55,600
HDFS you can go back and read the

00:02:53,769 --> 00:02:57,489
original papers from the early 80s and

00:02:55,600 --> 00:02:59,019
Juji computing you don't really care

00:02:57,489 --> 00:03:01,720
about that all you care about is it's

00:02:59,019 --> 00:03:05,440
very cost effective to store lots of

00:03:01,720 --> 00:03:07,420
data and it can come in remotely so

00:03:05,440 --> 00:03:10,420
yarns the same thing we've been trying

00:03:07,420 --> 00:03:12,760
while my colleagues I've been working on

00:03:10,420 --> 00:03:15,580
something lets you run your code within

00:03:12,760 --> 00:03:16,930
a hooded cluster it deals with the

00:03:15,580 --> 00:03:19,480
problem of getting de binary's to the

00:03:16,930 --> 00:03:23,590
machines to run the stuff to keep track

00:03:19,480 --> 00:03:25,360
of the health and you you can focus on

00:03:23,590 --> 00:03:27,250
the layers above you can decide what you

00:03:25,360 --> 00:03:29,709
want to run where you want to run it how

00:03:27,250 --> 00:03:31,209
failures happen so I like you this is

00:03:29,709 --> 00:03:32,590
kind of the land port there you work at

00:03:31,209 --> 00:03:35,530
this you have to go and read lamport's

00:03:32,590 --> 00:03:37,180
papers and then run away screaming you

00:03:35,530 --> 00:03:39,670
really want to focus at this level above

00:03:37,180 --> 00:03:42,340
the algorithms and even if you can the

00:03:39,670 --> 00:03:44,530
really high levels because ultimately

00:03:42,340 --> 00:03:47,170
you're trying to do useful work you know

00:03:44,530 --> 00:03:49,359
and MapReduce worked really well because

00:03:47,170 --> 00:03:51,630
it hid all the complexity all the stuff

00:03:49,359 --> 00:03:53,829
down below and said mappers and reducers

00:03:51,630 --> 00:03:55,420
ass that's what everything should we

00:03:53,829 --> 00:03:57,930
think about here is what can they do in

00:03:55,420 --> 00:04:00,880
terms of more reusable systems like this

00:03:57,930 --> 00:04:02,980
happy do inside yarn and just generally

00:04:00,880 --> 00:04:06,850
get your work done with the petabytes of

00:04:02,980 --> 00:04:09,579
data you're collecting so yawn it runs

00:04:06,850 --> 00:04:11,950
the code across the cluster summer in

00:04:09,579 --> 00:04:13,299
there there's a resource manager one of

00:04:11,950 --> 00:04:15,970
them or now there's two of them know

00:04:13,299 --> 00:04:18,579
Gotye I'll use machine your cluster you

00:04:15,970 --> 00:04:21,389
something called a node manager it talks

00:04:18,579 --> 00:04:23,470
the resource manager says it's there and

00:04:21,389 --> 00:04:25,479
it manages these things called

00:04:23,470 --> 00:04:29,350
containers and a container is really

00:04:25,479 --> 00:04:31,240
currently a sea group managed execution

00:04:29,350 --> 00:04:33,280
kind of process tree of code that you

00:04:31,240 --> 00:04:35,520
running we're playing with docker a bit

00:04:33,280 --> 00:04:35,520
and

00:04:35,550 --> 00:04:40,660
the containers are the code that yarn

00:04:37,990 --> 00:04:42,460
tells it to run resource manager sighs

00:04:40,660 --> 00:04:46,840
what's going to run where tells no

00:04:42,460 --> 00:04:50,290
managers they run it and something

00:04:46,840 --> 00:04:51,640
happens here youngest told about again

00:04:50,290 --> 00:04:56,580
resource manager and it gets to deal

00:04:51,640 --> 00:04:59,080
with it the way it works is your code

00:04:56,580 --> 00:05:02,260
your code talks to yarn say I want to

00:04:59,080 --> 00:05:04,480
run something and you run something

00:05:02,260 --> 00:05:05,620
called the application master it's worth

00:05:04,480 --> 00:05:07,540
noting you have a whole new set of

00:05:05,620 --> 00:05:09,250
acronyms here if you thought you

00:05:07,540 --> 00:05:11,410
understood about data know the name node

00:05:09,250 --> 00:05:14,200
and task tracker you've got more to

00:05:11,410 --> 00:05:15,760
learn so the application master is

00:05:14,200 --> 00:05:18,430
effectively the successor to the

00:05:15,760 --> 00:05:20,800
jobtracker it's your own personal job

00:05:18,430 --> 00:05:23,920
tracker to run your code with your

00:05:20,800 --> 00:05:25,480
algorithms in your policies so what you

00:05:23,920 --> 00:05:28,180
doing is tell yarn to say check your

00:05:25,480 --> 00:05:30,760
application somewhere deploy this yarn

00:05:28,180 --> 00:05:32,620
will deploy it it will keep an eye on it

00:05:30,760 --> 00:05:35,890
if it fails and restart it somewhere

00:05:32,620 --> 00:05:37,960
else based on the policy and I just it's

00:05:35,890 --> 00:05:41,890
it's your code to make your decisions

00:05:37,960 --> 00:05:44,410
about what you're going to do how do you

00:05:41,890 --> 00:05:46,570
run this well the thing is what we're

00:05:44,410 --> 00:05:48,130
doing there is we are running a remote

00:05:46,570 --> 00:05:51,880
application to cluster and what we have

00:05:48,130 --> 00:05:53,620
to do is tell you on what to do this is

00:05:51,880 --> 00:05:54,880
something if you end up staring at a

00:05:53,620 --> 00:05:57,070
code you'll discover these call to

00:05:54,880 --> 00:05:59,170
contain the launch context but the key

00:05:57,070 --> 00:06:01,630
point is is that you you just build up a

00:05:59,170 --> 00:06:03,070
command to run you say oh yeah here my

00:06:01,630 --> 00:06:06,280
environment variables i want to set up

00:06:03,070 --> 00:06:07,420
here is my bash command line and here

00:06:06,280 --> 00:06:10,360
are some of ours i want you to download

00:06:07,420 --> 00:06:12,670
and that's basically it's a Utah yonce

00:06:10,360 --> 00:06:15,580
download these binaries on ptolem if

00:06:12,670 --> 00:06:17,620
need be run this command with this

00:06:15,580 --> 00:06:19,390
environment variable and it goes away

00:06:17,620 --> 00:06:21,310
and running there's some extra things

00:06:19,390 --> 00:06:23,260
that get passed down various environment

00:06:21,310 --> 00:06:27,250
variables and stuff to let your running

00:06:23,260 --> 00:06:28,990
application bind bind to yarn bind to

00:06:27,250 --> 00:06:31,660
the file system pick up things like

00:06:28,990 --> 00:06:33,310
Kerberos keys but generally generally

00:06:31,660 --> 00:06:37,210
that's it so you can run arbitrary code

00:06:33,310 --> 00:06:39,760
there it does not have to be Java ok the

00:06:37,210 --> 00:06:41,710
yarn code we help you do that but you

00:06:39,760 --> 00:06:43,419
can run other bits of code in there I've

00:06:41,710 --> 00:06:45,760
done groovy ones are then scar the ones

00:06:43,419 --> 00:06:47,410
which was same jvm but also people done

00:06:45,760 --> 00:06:48,540
them and go as well actually so it's

00:06:47,410 --> 00:06:54,270
really it is a shade

00:06:48,540 --> 00:06:55,440
of arbitrary applications one of the

00:06:54,270 --> 00:06:57,600
cute things that you know where your

00:06:55,440 --> 00:06:59,970
binaries come from the answer is you

00:06:57,600 --> 00:07:02,820
copy all the artifacts you actually want

00:06:59,970 --> 00:07:05,280
to run into HDFS or any other file

00:07:02,820 --> 00:07:06,750
system that her dude can grab here's an

00:07:05,280 --> 00:07:10,160
example here where I'm basically saying

00:07:06,750 --> 00:07:13,050
I want to download an age-based our ball

00:07:10,160 --> 00:07:15,300
somewhere and I just say right you're

00:07:13,050 --> 00:07:16,590
going to download my H based are but

00:07:15,300 --> 00:07:19,680
we're going to pick it up actually from

00:07:16,590 --> 00:07:22,380
an Amazon s3 URL it's a URL that your

00:07:19,680 --> 00:07:24,240
coat and hat that you can handle and it

00:07:22,380 --> 00:07:26,550
will quite happily do this and that's

00:07:24,240 --> 00:07:30,510
worth knowing if you'll say playing with

00:07:26,550 --> 00:07:32,760
yarn on an Amazon EMR cluster is you

00:07:30,510 --> 00:07:35,520
keep all your binary somewhere just on

00:07:32,760 --> 00:07:40,080
s3 and pull them down on them on anyway

00:07:35,520 --> 00:07:41,910
so I say here's a tarball I say it's an

00:07:40,080 --> 00:07:45,330
archive which tells the node manager

00:07:41,910 --> 00:07:48,030
when this thing gets installed on tyrant

00:07:45,330 --> 00:07:50,430
unzip it whatever and also give the

00:07:48,030 --> 00:07:53,760
relative path I say install this into

00:07:50,430 --> 00:07:56,310
the lib HBase what that means is when my

00:07:53,760 --> 00:08:00,600
container comes up it's going to be

00:07:56,310 --> 00:08:03,860
untied and stuck in the relative path of

00:08:00,600 --> 00:08:07,320
somewhere somewhere in local filesystem

00:08:03,860 --> 00:08:09,060
note manager when it's it gets told to

00:08:07,320 --> 00:08:11,430
launch your container it gets that list

00:08:09,060 --> 00:08:14,000
of resources there pulls them all down

00:08:11,430 --> 00:08:17,340
expands them copies and whatever and

00:08:14,000 --> 00:08:19,350
then it does it it's into a bit of the

00:08:17,340 --> 00:08:21,480
file system that gets deleted once your

00:08:19,350 --> 00:08:23,970
container finishes and then execs your

00:08:21,480 --> 00:08:26,910
bash command line at the base of that

00:08:23,970 --> 00:08:28,920
path so now that I've installed HBase

00:08:26,910 --> 00:08:32,040
into the page base if I set my bash

00:08:28,920 --> 00:08:35,310
command to be lib HBase fresh HBase no

00:08:32,040 --> 00:08:37,979
point 9 8 / HBase it would actually

00:08:35,310 --> 00:08:39,900
exact age base and that's the secret is

00:08:37,979 --> 00:08:43,349
you download your binary you set

00:08:39,900 --> 00:08:45,060
environmental and you run your code that

00:08:43,349 --> 00:08:47,400
is the core concept there's one little

00:08:45,060 --> 00:08:49,230
surprised they're called class parts ok

00:08:47,400 --> 00:08:52,710
I won't go into details except to say it

00:08:49,230 --> 00:08:56,490
hurt I will go into details actually he

00:08:52,710 --> 00:08:58,290
does have the bear it if you're running

00:08:56,490 --> 00:09:02,370
java code you want to set the classpath

00:08:58,290 --> 00:09:04,260
of what to do it makes sense to use the

00:09:02,370 --> 00:09:07,980
dhoop and other binaries that are on the

00:09:04,260 --> 00:09:09,960
system at the far end which you can't

00:09:07,980 --> 00:09:12,180
necessarily predict it on your client

00:09:09,960 --> 00:09:13,500
because it's somebody else's cluster so

00:09:12,180 --> 00:09:16,140
there's an environment variable Y on

00:09:13,500 --> 00:09:18,420
application class path that you should

00:09:16,140 --> 00:09:21,089
be able to grab from your yarn settings

00:09:18,420 --> 00:09:23,310
site settings to say here's my classpath

00:09:21,089 --> 00:09:25,920
the default one says use the big-top

00:09:23,310 --> 00:09:28,230
path but different installations behave

00:09:25,920 --> 00:09:30,480
differently you build your classpath

00:09:28,230 --> 00:09:31,800
from that if it's wrong your application

00:09:30,480 --> 00:09:33,960
doesn't start with a relatively

00:09:31,800 --> 00:09:35,750
meaningless error message so that class

00:09:33,960 --> 00:09:38,370
not found or something like that so it's

00:09:35,750 --> 00:09:43,170
it's actually one of the the bigger

00:09:38,370 --> 00:09:44,490
sources of paint you've got that little

00:09:43,170 --> 00:09:46,830
problem there's another one which is

00:09:44,490 --> 00:09:49,940
that that class path tends to include

00:09:46,830 --> 00:09:52,650
everything that had to decide you want

00:09:49,940 --> 00:09:54,570
that comes down to the version of log4j

00:09:52,650 --> 00:09:56,550
you on the version of Jackson you want

00:09:54,570 --> 00:09:58,910
whatever that rose in there you get all

00:09:56,550 --> 00:10:02,880
the transient craft on the classpath

00:09:58,910 --> 00:10:04,980
which due to an original to break things

00:10:02,880 --> 00:10:07,440
when we ship to deep last year loop 2.2

00:10:04,980 --> 00:10:08,580
is pretty out of date Anna that means

00:10:07,440 --> 00:10:10,410
you really ought to be aware of what

00:10:08,580 --> 00:10:13,560
you're building against and kind of keep

00:10:10,410 --> 00:10:15,240
the old code the good news is it's been

00:10:13,560 --> 00:10:16,950
annoying me so much of a right man code

00:10:15,240 --> 00:10:19,680
up you've been upgrading your jars as I

00:10:16,950 --> 00:10:23,160
go along so that's where I've used my

00:10:19,680 --> 00:10:25,830
power so a loop 999 1 is a jira to say

00:10:23,160 --> 00:10:28,020
let's update everything and we've done

00:10:25,830 --> 00:10:30,510
that as much as we can now we've reached

00:10:28,020 --> 00:10:32,339
the impasse we said we are stuck with

00:10:30,510 --> 00:10:37,440
the latest versions of the binaries we

00:10:32,339 --> 00:10:41,040
can do that still run on java 6 we want

00:10:37,440 --> 00:10:42,930
to get rid of that too and generally

00:10:41,040 --> 00:10:45,270
work the thing was scared of his google

00:10:42,930 --> 00:10:46,830
guava that's incredibly brittle but

00:10:45,270 --> 00:10:49,260
you're going to have to deal with that

00:10:46,830 --> 00:10:50,910
for now I'm afraid I would like someone

00:10:49,260 --> 00:10:52,230
and that's particularly someone in the

00:10:50,910 --> 00:10:54,690
audience over there that's their phone

00:10:52,230 --> 00:10:57,120
over there to go and add osgi support

00:10:54,690 --> 00:10:59,850
for us instead so we could run John run

00:10:57,120 --> 00:11:01,709
yarn apps in osgi container that will

00:10:59,850 --> 00:11:04,470
make life a lot simpler I think

00:11:01,709 --> 00:11:06,620
everybody agrees that it's just nobody's

00:11:04,470 --> 00:11:10,110
sat down to put in the hours from free

00:11:06,620 --> 00:11:12,680
anyway ignoring little detail your

00:11:10,110 --> 00:11:15,010
application master comes up and running

00:11:12,680 --> 00:11:17,500
and was it doing

00:11:15,010 --> 00:11:20,110
it's like the jobtracker its aim in life

00:11:17,500 --> 00:11:23,530
is now to manage the work doesn't do the

00:11:20,110 --> 00:11:25,360
work so much is coordinated it says it

00:11:23,530 --> 00:11:27,730
works out it decides what it has to do

00:11:25,360 --> 00:11:29,350
based in your request or whatever talks

00:11:27,730 --> 00:11:32,100
to yarn and says okay I need some

00:11:29,350 --> 00:11:34,720
containers to do the actual work it can

00:11:32,100 --> 00:11:37,630
specify the capacity amount of memory

00:11:34,720 --> 00:11:40,810
and CPU those containers have it can say

00:11:37,630 --> 00:11:44,100
where you want them to be it builds up

00:11:40,810 --> 00:11:47,560
the binaries and execute it it's also

00:11:44,100 --> 00:11:52,090
somewhere you can provide IPC and rest

00:11:47,560 --> 00:11:53,440
api calls web you eyes and it has to

00:11:52,090 --> 00:11:54,760
handle responsibility dealing with

00:11:53,440 --> 00:11:57,370
failures a little bit of work on the

00:11:54,760 --> 00:12:00,700
side so it it's like it is exactly your

00:11:57,370 --> 00:12:04,660
version of the job tracker in Hadoop to

00:12:00,700 --> 00:12:06,910
the jobtracker replacement is a yarn am

00:12:04,660 --> 00:12:10,770
it has all these features and other

00:12:06,910 --> 00:12:10,770
applications do exactly the same thing

00:12:10,830 --> 00:12:16,120
we ask for containers all we do is say

00:12:14,230 --> 00:12:18,280
your requirements terms of memory and

00:12:16,120 --> 00:12:21,970
CPU and you also get the same way you

00:12:18,280 --> 00:12:23,410
want them so MapReduce it says it looks

00:12:21,970 --> 00:12:26,680
at where the data is first here's my

00:12:23,410 --> 00:12:28,330
data sources looks at the files that

00:12:26,680 --> 00:12:29,830
looks at where the blocks are which you

00:12:28,330 --> 00:12:34,150
can ask age do first phone and says I

00:12:29,830 --> 00:12:35,350
want to run my code in it here in the

00:12:34,150 --> 00:12:36,790
project I've been working on something

00:12:35,350 --> 00:12:38,380
called slider we do it completely

00:12:36,790 --> 00:12:40,720
differently we just say I want something

00:12:38,380 --> 00:12:43,150
a random we don't really care where it

00:12:40,720 --> 00:12:44,740
is at first but then we try and remember

00:12:43,150 --> 00:12:46,690
where it is so next time your

00:12:44,740 --> 00:12:48,970
application comes up a claim baked in

00:12:46,690 --> 00:12:52,930
the same place it was that's so we can

00:12:48,970 --> 00:12:54,940
reuse the data and we just say yeah we

00:12:52,930 --> 00:12:58,240
try and remember but it's best effort as

00:12:54,940 --> 00:13:01,120
a little flag here saying relax locality

00:12:58,240 --> 00:13:03,820
versus strict strict says it must be on

00:13:01,120 --> 00:13:06,040
a specific container relax as I'd like

00:13:03,820 --> 00:13:07,630
it if you're asked for strict and the

00:13:06,040 --> 00:13:08,380
machine's not there you're not good

00:13:07,630 --> 00:13:10,900
enough you're not going to get a

00:13:08,380 --> 00:13:13,900
container if that machine's busy you're

00:13:10,900 --> 00:13:17,430
not going to get it so generally relax

00:13:13,900 --> 00:13:17,430
is the only option that makes sense now

00:13:17,520 --> 00:13:21,970
one of these yours but you you are

00:13:19,870 --> 00:13:25,390
solution you get back something that may

00:13:21,970 --> 00:13:26,650
be what you wanted or maybe close one of

00:13:25,390 --> 00:13:28,690
the things that we've been working on

00:13:26,650 --> 00:13:31,390
we've got improve as a failure tracking

00:13:28,690 --> 00:13:33,850
if something fails what do we do in that

00:13:31,390 --> 00:13:35,470
world can we say when we get it back we

00:13:33,850 --> 00:13:37,590
don't want it do we list everything else

00:13:35,470 --> 00:13:41,230
say I want everything but these nodes

00:13:37,590 --> 00:13:44,550
right it's an interesting area for for

00:13:41,230 --> 00:13:47,140
more code at least in my project but it

00:13:44,550 --> 00:13:49,090
if you look at the big MapReduce engines

00:13:47,140 --> 00:13:50,920
the job tracker in that they have a nice

00:13:49,090 --> 00:13:52,600
simple notion of blacklisting so this is

00:13:50,920 --> 00:13:54,520
machine is too slow I'm not going to use

00:13:52,600 --> 00:13:56,740
it I think where they were being a bit

00:13:54,520 --> 00:13:59,830
more subtle about it as we're trying to

00:13:56,740 --> 00:14:01,750
keep moving averages I mind just say a

00:13:59,830 --> 00:14:03,430
box works or doesn't work we have a

00:14:01,750 --> 00:14:06,400
notion of this box is a bit unreliable

00:14:03,430 --> 00:14:08,230
and you don't want to use it but you

00:14:06,400 --> 00:14:10,600
have no other choice you might as well

00:14:08,230 --> 00:14:11,890
take it in a small cluster don't sit

00:14:10,600 --> 00:14:14,050
there saying oh no these things are no

00:14:11,890 --> 00:14:17,560
good you want to say well I hate it but

00:14:14,050 --> 00:14:20,740
i'll use it anyway okay its application

00:14:17,560 --> 00:14:23,700
master it asks yarn for containers what

00:14:20,740 --> 00:14:26,770
you want maybe where you want it and

00:14:23,700 --> 00:14:29,110
eventually eventually i get satisfied

00:14:26,770 --> 00:14:31,510
you get some containers and if it comes

00:14:29,110 --> 00:14:34,600
up to your code you that you get them

00:14:31,510 --> 00:14:36,820
back and you run them all right exactly

00:14:34,600 --> 00:14:39,160
that same setup as for application

00:14:36,820 --> 00:14:41,290
master where you say here's my

00:14:39,160 --> 00:14:44,260
environment here my binaries here's mine

00:14:41,290 --> 00:14:49,360
here's my command line and you run them

00:14:44,260 --> 00:14:51,550
again they start off in there if you tip

00:14:49,360 --> 00:14:54,460
the switches you get see group isolation

00:14:51,550 --> 00:14:56,680
here that's a subset of what things like

00:14:54,460 --> 00:14:58,720
darker does it doesn't hide the oh the

00:14:56,680 --> 00:15:02,110
OS and the file system but it does put

00:14:58,720 --> 00:15:04,450
limits on process and a CPU and memory

00:15:02,110 --> 00:15:06,340
consumption and the policy there is if

00:15:04,450 --> 00:15:09,340
you are if you start using more memory

00:15:06,340 --> 00:15:11,800
allowed your program gets killed cpu get

00:15:09,340 --> 00:15:13,810
throttled and that's that's a nice way

00:15:11,800 --> 00:15:16,720
of stopping your application going wild

00:15:13,810 --> 00:15:19,300
in a cluster and that what that does is

00:15:16,720 --> 00:15:21,220
then let you you rock and run your

00:15:19,300 --> 00:15:24,070
programs in a cluster without the ops

00:15:21,220 --> 00:15:25,180
team getting too unhappy you know yeah

00:15:24,070 --> 00:15:27,670
you're not going to kill the things

00:15:25,180 --> 00:15:29,770
memory we're not doing i/o throttling

00:15:27,670 --> 00:15:32,020
yet and that's an interesting problem

00:15:29,770 --> 00:15:35,470
because the i/o is actually going here

00:15:32,020 --> 00:15:37,180
in ex gfs not the container so we've got

00:15:35,470 --> 00:15:39,010
to come up some plan fitting in that but

00:15:37,180 --> 00:15:41,890
otherwise your containers run rather to

00:15:39,010 --> 00:15:43,030
be isolated there is also project

00:15:41,890 --> 00:15:44,950
underway running the contain

00:15:43,030 --> 00:15:47,380
in Dhaka so you will just say run these

00:15:44,950 --> 00:15:50,080
docker images around the cluster that

00:15:47,380 --> 00:15:53,140
gives you better isolation although it

00:15:50,080 --> 00:15:58,090
complicated networking setup no that's

00:15:53,140 --> 00:16:00,100
it you run your containers now what

00:15:58,090 --> 00:16:03,940
happens if something fails and node goes

00:16:00,100 --> 00:16:07,330
away that is something that is not

00:16:03,940 --> 00:16:10,450
directly your problem it happens it

00:16:07,330 --> 00:16:13,390
happens the larger the cluster is just

00:16:10,450 --> 00:16:14,680
based on machine failures disk failures

00:16:13,390 --> 00:16:17,800
are kind of proportional number of disks

00:16:14,680 --> 00:16:20,170
you have and then there's the risk that

00:16:17,800 --> 00:16:22,390
the code fails as well the most

00:16:20,170 --> 00:16:23,650
unreliable piece of code in the Hadoop

00:16:22,390 --> 00:16:27,280
cluster is likely to be your own

00:16:23,650 --> 00:16:30,340
application right so sometimes your

00:16:27,280 --> 00:16:31,870
containers crash they fail no matter

00:16:30,340 --> 00:16:36,370
what happens where the entire machine

00:16:31,870 --> 00:16:38,440
goes away or just your process exit yon

00:16:36,370 --> 00:16:41,610
finds out about it your process exits

00:16:38,440 --> 00:16:44,830
the node manager says that process don't

00:16:41,610 --> 00:16:46,600
entire machine fails yawn says hang on

00:16:44,830 --> 00:16:49,600
this thing here hasn't heart beated in

00:16:46,600 --> 00:16:54,220
for a while so that let's assume is dead

00:16:49,600 --> 00:16:56,350
again the RM said is dead either way

00:16:54,220 --> 00:16:58,570
your application master gets told what

00:16:56,350 --> 00:17:04,960
what happens and it chooses how to react

00:16:58,570 --> 00:17:07,350
to it ain't for us the slider stuff we

00:17:04,960 --> 00:17:10,630
just asked a replacement but that's

00:17:07,350 --> 00:17:16,030
that's now policy-driven if you look at

00:17:10,630 --> 00:17:18,550
things like the jobtracker they try and

00:17:16,030 --> 00:17:19,930
add some extra things to say this bit of

00:17:18,550 --> 00:17:22,120
date to were working on caused the

00:17:19,930 --> 00:17:23,950
failure they also look at the machine as

00:17:22,120 --> 00:17:25,630
well and do blacklisting but you could

00:17:23,950 --> 00:17:27,850
imagine application and says if any

00:17:25,630 --> 00:17:29,440
machine fails if any container fails

00:17:27,850 --> 00:17:32,110
then I just died completely enroll back

00:17:29,440 --> 00:17:33,550
and that may seem a stupid policy but

00:17:32,110 --> 00:17:35,440
for some applications it actually makes

00:17:33,550 --> 00:17:39,640
a lot of sense so some of the people

00:17:35,440 --> 00:17:42,040
doing MPI over yon project called

00:17:39,640 --> 00:17:43,300
hamster I believe they do that they

00:17:42,040 --> 00:17:45,880
basically say I'm going to run a job

00:17:43,300 --> 00:17:48,310
anything happens to it will stop and

00:17:45,880 --> 00:17:51,510
restart and they rely on the fact that

00:17:48,310 --> 00:17:53,680
actually if your job is fast enough or

00:17:51,510 --> 00:17:56,679
short-lived enough you don't need to

00:17:53,680 --> 00:17:58,720
bother with checkpointing and restart is

00:17:56,679 --> 00:18:00,519
good failure mode and if you don't save

00:17:58,720 --> 00:18:02,740
things to disk you can actually get by a

00:18:00,519 --> 00:18:04,240
lot faster so they're cheating and

00:18:02,740 --> 00:18:06,190
saying actually our failure policy is

00:18:04,240 --> 00:18:11,049
start from scratch so don't don't

00:18:06,190 --> 00:18:14,309
dismiss the simple policies darling I

00:18:11,049 --> 00:18:16,600
recommend is don't forget

00:18:14,309 --> 00:18:19,059
model-view-controller as your

00:18:16,600 --> 00:18:21,490
architecture for an application this is

00:18:19,059 --> 00:18:23,619
very important because most people

00:18:21,490 --> 00:18:25,210
writing yarn applications start with an

00:18:23,619 --> 00:18:28,210
example piece of code called distributed

00:18:25,210 --> 00:18:30,490
shell in the new code base and whoever

00:18:28,210 --> 00:18:34,210
wrote it forgot about Model View

00:18:30,490 --> 00:18:36,789
controller so you cut and paste that

00:18:34,210 --> 00:18:38,619
code you start running with it and your

00:18:36,789 --> 00:18:40,480
code gets a bit messy and then you add a

00:18:38,619 --> 00:18:42,279
bit more stuff like fairy handling and

00:18:40,480 --> 00:18:44,440
it gets a bit messy and you end up with

00:18:42,279 --> 00:18:47,230
a class here that's about eight thousand

00:18:44,440 --> 00:18:48,940
lines long with all these various data

00:18:47,230 --> 00:18:51,509
structures in and synchronized blocks

00:18:48,940 --> 00:18:55,059
and you have no idea what's happening

00:18:51,509 --> 00:18:56,320
then you spend a week maybe even 10 days

00:18:55,059 --> 00:18:59,110
stripping it all out and putting in two

00:18:56,320 --> 00:19:01,960
places unless you like doing that I'd

00:18:59,110 --> 00:19:04,090
say start from beginning and come up

00:19:01,960 --> 00:19:07,899
with a model of what you're doing and

00:19:04,090 --> 00:19:09,249
that's in a yarn application that piece

00:19:07,899 --> 00:19:11,259
of code becomes your model of the

00:19:09,249 --> 00:19:13,029
cluster what's happening in the notes

00:19:11,259 --> 00:19:15,279
what their failure rate is that kind of

00:19:13,029 --> 00:19:17,169
thing and what you actually want to do

00:19:15,279 --> 00:19:20,320
so for our code we basically take a

00:19:17,169 --> 00:19:22,649
specification saying I want to run this

00:19:20,320 --> 00:19:26,409
binary like age base on these machines

00:19:22,649 --> 00:19:28,470
now specification we asked for it and

00:19:26,409 --> 00:19:30,190
then we keep track of where things are

00:19:28,470 --> 00:19:32,649
let me have some we have some other

00:19:30,190 --> 00:19:34,600
stats there and what's going on run a

00:19:32,649 --> 00:19:37,029
bit of knowledge what we're doing but

00:19:34,600 --> 00:19:39,850
it's all isolated and that lets us do a

00:19:37,029 --> 00:19:43,509
few things one of the best things is we

00:19:39,850 --> 00:19:45,279
can now test it heavily and simulate

00:19:43,509 --> 00:19:46,809
scale and failure handling without

00:19:45,279 --> 00:19:48,820
actually putting you on a real cluster

00:19:46,809 --> 00:19:50,710
so even though I have access to big

00:19:48,820 --> 00:19:53,200
clusters I have to argue with people to

00:19:50,710 --> 00:19:54,970
get that time here I can just say right

00:19:53,200 --> 00:19:57,190
I'm going to simulate a 10,000 node

00:19:54,970 --> 00:19:58,809
cluster with some mock code that

00:19:57,190 --> 00:20:01,259
generates the requests and the failures

00:19:58,809 --> 00:20:03,940
and tries to even simulate kind of

00:20:01,259 --> 00:20:05,440
asynchronous calls into it just just to

00:20:03,940 --> 00:20:08,049
stress the code to find those failures

00:20:05,440 --> 00:20:09,609
before you go into production that's

00:20:08,049 --> 00:20:10,220
important because when you get into the

00:20:09,609 --> 00:20:11,539
big distri

00:20:10,220 --> 00:20:14,390
system you're into the world of

00:20:11,539 --> 00:20:15,799
distributed debugging and it is a lot

00:20:14,390 --> 00:20:19,789
easier to find the things on your local

00:20:15,799 --> 00:20:21,770
machine first so do that Model View

00:20:19,789 --> 00:20:25,130
controller the other thing is is that

00:20:21,770 --> 00:20:27,919
you can add api's on top for us we're

00:20:25,130 --> 00:20:31,549
hooking into we have a rest api an optic

00:20:27,919 --> 00:20:33,710
capi and some zookeeper stuff on the

00:20:31,549 --> 00:20:36,650
side here is some chatting going on with

00:20:33,710 --> 00:20:38,870
yarn itself resource manager and then

00:20:36,650 --> 00:20:40,880
the old manager that's all handled for

00:20:38,870 --> 00:20:43,429
us by classes that they're coming yawn

00:20:40,880 --> 00:20:46,250
so we we just some class something that

00:20:43,429 --> 00:20:49,070
handles all that conversation this extra

00:20:46,250 --> 00:20:50,419
stuff we added on ourselves again I

00:20:49,070 --> 00:20:52,400
think we got a bit too late to splitting

00:20:50,419 --> 00:20:57,140
it up so this this application master

00:20:52,400 --> 00:20:58,600
class is just over large and it's become

00:20:57,140 --> 00:21:00,830
the piece of code was scared of the most

00:20:58,600 --> 00:21:02,330
so when we added the rest stuff we at

00:21:00,830 --> 00:21:06,559
least it did it slightly better and

00:21:02,330 --> 00:21:08,090
stuck it on the side well then we are

00:21:06,559 --> 00:21:09,230
doing now we've got a separate model is

00:21:08,090 --> 00:21:10,730
we're trying to do something which is

00:21:09,230 --> 00:21:12,380
very leading edge which is handle

00:21:10,730 --> 00:21:19,490
failures of the application master

00:21:12,380 --> 00:21:22,330
itself until now yan hopes that your

00:21:19,490 --> 00:21:25,190
application keeps running if it fails

00:21:22,330 --> 00:21:28,850
you're at the key if your application

00:21:25,190 --> 00:21:31,039
master fails then all your containers

00:21:28,850 --> 00:21:34,070
get destroyed your application gets

00:21:31,039 --> 00:21:37,850
queued for restart and yarn keeps track

00:21:34,070 --> 00:21:41,419
of the fact your code failed if it has

00:21:37,850 --> 00:21:43,340
not failed more than the cluster policy

00:21:41,419 --> 00:21:45,409
says phase loud it will get restarted

00:21:43,340 --> 00:21:47,270
somewhere else eventually when the space

00:21:45,409 --> 00:21:49,880
on the cluster and you have to start

00:21:47,270 --> 00:21:51,919
from scratch again that's actually fine

00:21:49,880 --> 00:21:55,370
for things like say a job tracker or

00:21:51,919 --> 00:21:57,049
similar where you may as well start and

00:21:55,370 --> 00:21:59,919
rebuild all your complicated state in

00:21:57,049 --> 00:22:01,850
your application master from scratch

00:21:59,919 --> 00:22:04,100
what we were doing we tryna have long

00:22:01,850 --> 00:22:06,289
live services we actually wanted to keep

00:22:04,100 --> 00:22:08,299
things running the point being at say

00:22:06,289 --> 00:22:11,030
for example running HBase or storm if

00:22:08,299 --> 00:22:12,919
our out master fails we don't want HBase

00:22:11,030 --> 00:22:15,470
to go down we want the storm session to

00:22:12,919 --> 00:22:19,490
keep running so there is a new feature

00:22:15,470 --> 00:22:21,830
my colleagues put in where we can say

00:22:19,490 --> 00:22:23,570
set a flag called set to keep containers

00:22:21,830 --> 00:22:29,180
across application attempts

00:22:23,570 --> 00:22:33,230
in short well at least it does say what

00:22:29,180 --> 00:22:34,970
it does you know in his favor and in

00:22:33,230 --> 00:22:36,890
fact Lee what happens is the containers

00:22:34,970 --> 00:22:39,110
keep running when you're am gets

00:22:36,890 --> 00:22:41,060
restarted it gets given a list back of

00:22:39,110 --> 00:22:43,040
what containers come in of what

00:22:41,060 --> 00:22:44,540
containers you ready had and you can get

00:22:43,040 --> 00:22:47,180
told what containers failed while you

00:22:44,540 --> 00:22:49,340
were down if anyone's going to implement

00:22:47,180 --> 00:22:51,230
this young people the corn if you're

00:22:49,340 --> 00:22:52,940
going to do that synchronize everything

00:22:51,230 --> 00:22:55,340
because it turns out you actually end up

00:22:52,940 --> 00:22:58,190
getting those farrier callbacks before

00:22:55,340 --> 00:23:00,200
you finished processing the answer but

00:22:58,190 --> 00:23:02,270
in fact the you get this and you've got

00:23:00,200 --> 00:23:04,460
to try and rebuild your state and that

00:23:02,270 --> 00:23:05,510
there's an interesting problem if you're

00:23:04,460 --> 00:23:07,250
going to do that you've got to think

00:23:05,510 --> 00:23:10,760
where do I keep my state that that's

00:23:07,250 --> 00:23:12,680
persistent we keep some of the stuff in

00:23:10,760 --> 00:23:15,320
HDFS we have the kind of the original

00:23:12,680 --> 00:23:18,560
what it is we want and we keep a history

00:23:15,320 --> 00:23:20,350
of where things off if you've got

00:23:18,560 --> 00:23:23,660
anything else I'd say look at zookeeper

00:23:20,350 --> 00:23:25,280
but of course you cannot keep this in

00:23:23,660 --> 00:23:27,680
one though zookeeper ephemeral notes

00:23:25,280 --> 00:23:30,260
because once your RM goes down all its

00:23:27,680 --> 00:23:35,600
snake goes away so there's one other

00:23:30,260 --> 00:23:39,200
hook we actually do here which is we we

00:23:35,600 --> 00:23:41,720
use the single field in a yarn container

00:23:39,200 --> 00:23:43,970
its priority for when you ain't allocate

00:23:41,720 --> 00:23:47,300
it as our single index into what kind of

00:23:43,970 --> 00:23:48,890
role a container has in the cluster so

00:23:47,300 --> 00:23:50,390
have different roles here like a master

00:23:48,890 --> 00:23:52,310
like a worker like a monitor like a

00:23:50,390 --> 00:23:53,960
garbage collector we would give them

00:23:52,310 --> 00:23:55,970
four different priorities one two three

00:23:53,960 --> 00:23:58,640
four and all we have to do is in

00:23:55,970 --> 00:24:01,580
numerate that cluster and see what they

00:23:58,640 --> 00:24:04,160
are we've gone a bit beyond that now

00:24:01,580 --> 00:24:05,600
we're actually the code inside has a bit

00:24:04,160 --> 00:24:07,480
of minimal state so what we're going to

00:24:05,600 --> 00:24:10,100
do is ask them where they think they are

00:24:07,480 --> 00:24:12,740
whether i say running or not running and

00:24:10,100 --> 00:24:14,600
if then if they're not running we just

00:24:12,740 --> 00:24:15,950
destroy them we don't bother to worry

00:24:14,600 --> 00:24:18,140
about why they're running or what song

00:24:15,950 --> 00:24:22,700
but if they if they are running we just

00:24:18,140 --> 00:24:23,750
leave them alone anyway it is it's an

00:24:22,700 --> 00:24:25,640
interesting feature if you're running

00:24:23,750 --> 00:24:27,230
long live code if you're not running

00:24:25,640 --> 00:24:29,410
long-lived applications I would just say

00:24:27,230 --> 00:24:31,520
don't go near this all right it's just

00:24:29,410 --> 00:24:32,900
extra pain and suffering and the real

00:24:31,520 --> 00:24:35,140
problem is rebuilding your staked on a

00:24:32,900 --> 00:24:35,140
phone

00:24:35,420 --> 00:24:40,400
that's important because you have enough

00:24:37,400 --> 00:24:45,050
to do and the a extra thing you have to

00:24:40,400 --> 00:24:47,720
do is actually testing um I big fan of

00:24:45,050 --> 00:24:49,940
testing I like writing tests I think

00:24:47,720 --> 00:24:53,330
everyone should write more tests I don't

00:24:49,940 --> 00:24:54,920
like waiting for tests to finish that's

00:24:53,330 --> 00:24:56,120
where you can spend a lot of my life is

00:24:54,920 --> 00:25:00,640
actually spent waiting for tests to

00:24:56,120 --> 00:25:03,410
finish these days so like I said before

00:25:00,640 --> 00:25:04,910
we move on application state into unit

00:25:03,410 --> 00:25:09,530
tests and that's really nice because

00:25:04,910 --> 00:25:10,910
they finish in about five minutes where

00:25:09,530 --> 00:25:15,230
it gets harder it's actually the real

00:25:10,910 --> 00:25:17,060
production tests in this world you

00:25:15,230 --> 00:25:19,520
actually want to simulate a yarn cluster

00:25:17,060 --> 00:25:23,240
you want your programs to be downloaded

00:25:19,520 --> 00:25:24,770
from HDFS you want to talk to HDFS you

00:25:23,240 --> 00:25:28,700
want to exact things you want to get the

00:25:24,770 --> 00:25:30,020
errors back and there are there's one

00:25:28,700 --> 00:25:32,510
thing can help you here is something

00:25:30,020 --> 00:25:37,010
called mini yarn cluster zilla class

00:25:32,510 --> 00:25:39,320
that actually runs all the yarn cluster

00:25:37,010 --> 00:25:43,340
inside your jvm process so you can

00:25:39,320 --> 00:25:44,690
actually host the the Resource Manager

00:25:43,340 --> 00:25:47,210
the node manager the real running in

00:25:44,690 --> 00:25:49,400
process you can also bring up HDFS

00:25:47,210 --> 00:25:52,370
alongside that with a mini HDFS cluster

00:25:49,400 --> 00:25:56,240
which I would not recommend doing it

00:25:52,370 --> 00:25:58,820
first because when your test runs HDFS

00:25:56,240 --> 00:26:00,230
gets taken away and any interesting logs

00:25:58,820 --> 00:26:04,820
and other data you've connected goes

00:26:00,230 --> 00:26:06,680
away too so did just run locally I only

00:26:04,820 --> 00:26:08,210
discovered recently because I hadn't

00:26:06,680 --> 00:26:10,220
read the guide propping up the something

00:26:08,210 --> 00:26:11,810
called an unmanaged application last as

00:26:10,220 --> 00:26:14,810
well which actually runs your

00:26:11,810 --> 00:26:19,160
application master in in your J unit

00:26:14,810 --> 00:26:20,690
code I I've not played with that but I

00:26:19,160 --> 00:26:24,560
think it sounds like it would have made

00:26:20,690 --> 00:26:25,850
my life a lot easier as it is a lot of

00:26:24,560 --> 00:26:28,670
our simple tests are in the mini on

00:26:25,850 --> 00:26:31,730
cluster it's nice it works with simple

00:26:28,670 --> 00:26:34,100
tests but as every every test class

00:26:31,730 --> 00:26:35,960
starts that cluster up and tears it down

00:26:34,100 --> 00:26:38,600
it makes your application slow and slow

00:26:35,960 --> 00:26:40,310
and slow so nowadays what we're actually

00:26:38,600 --> 00:26:42,160
doing for most of our work is we're

00:26:40,310 --> 00:26:45,200
actually we've designed it all to run

00:26:42,160 --> 00:26:48,750
functionally against real hooded

00:26:45,200 --> 00:26:50,790
clusters starting at VMS so

00:26:48,750 --> 00:26:54,420
we have a whole test suite which

00:26:50,790 --> 00:26:56,340
actually we're build process works is we

00:26:54,420 --> 00:27:00,150
build up by Nuri's we build our archive

00:26:56,340 --> 00:27:02,040
a tarball we untie r it then our

00:27:00,150 --> 00:27:05,820
functional test suite actually exacts

00:27:02,040 --> 00:27:08,400
the binary script as you would real real

00:27:05,820 --> 00:27:09,870
client applications point it at some

00:27:08,400 --> 00:27:12,900
settings files that dictates the real

00:27:09,870 --> 00:27:14,490
clusters locally i run the amp but we

00:27:12,900 --> 00:27:17,220
can run it against production clusters

00:27:14,490 --> 00:27:18,840
to whether they're things on ec2 and

00:27:17,220 --> 00:27:24,120
Rackspace or where they're actually real

00:27:18,840 --> 00:27:25,380
physical clusters over ssh tunnels it's

00:27:24,120 --> 00:27:28,770
notable here that I actually have three

00:27:25,380 --> 00:27:32,640
VMs a red hat machine with helloo 2.4

00:27:28,770 --> 00:27:35,000
ubuntu 12 java rate last basically

00:27:32,640 --> 00:27:37,140
up-to-date branch to and kerberos

00:27:35,000 --> 00:27:39,540
windows thing in the corner by go near

00:27:37,140 --> 00:27:41,790
sometimes that's enough to pretty much

00:27:39,540 --> 00:27:43,110
create most of the configuration

00:27:41,790 --> 00:27:46,340
problems in grief you're going to

00:27:43,110 --> 00:27:49,260
encounter and particularly Kerberos and

00:27:46,340 --> 00:27:52,650
Hadoop security hands up who's got a

00:27:49,260 --> 00:27:55,080
Kerberos enabled secure secure Hadoop

00:27:52,650 --> 00:27:57,690
cluster okay keep your hands up if you

00:27:55,080 --> 00:27:59,460
like it okay one person the back it does

00:27:57,690 --> 00:28:01,890
actually make sense and i would

00:27:59,460 --> 00:28:03,930
recommend everybody stop being scared of

00:28:01,890 --> 00:28:06,780
Kerberos and learn to understand it all

00:28:03,930 --> 00:28:08,910
right it's just painful but well it

00:28:06,780 --> 00:28:11,580
actually does make sense in some way but

00:28:08,910 --> 00:28:13,890
it a crazed problem take great problems

00:28:11,580 --> 00:28:16,140
of long-lived services it creates extra

00:28:13,890 --> 00:28:19,440
work in your test and it creates lots of

00:28:16,140 --> 00:28:22,410
interesting obscure messages a good run

00:28:19,440 --> 00:28:24,510
being updated my cluster last week with

00:28:22,410 --> 00:28:27,270
apt-get update and everything stopped

00:28:24,510 --> 00:28:29,040
working and it turned out that there was

00:28:27,270 --> 00:28:31,950
a new Java 8 update which then got

00:28:29,040 --> 00:28:34,950
installed by Ubuntu which had not

00:28:31,950 --> 00:28:36,840
included the latest US enabled

00:28:34,950 --> 00:28:39,390
encryption mechanism so I can handle

00:28:36,840 --> 00:28:41,340
long secure keys which is then causing

00:28:39,390 --> 00:28:42,870
the client to fail with some error

00:28:41,340 --> 00:28:44,340
message like couldn't talk to the server

00:28:42,870 --> 00:28:45,540
you know things like that so that's why

00:28:44,340 --> 00:28:46,830
I'd recommend you start playing with

00:28:45,540 --> 00:28:49,890
this stuff sooner rather than later is

00:28:46,830 --> 00:28:53,940
because you want that pain before it

00:28:49,890 --> 00:28:55,140
ships anyway so testing testing is one

00:28:53,940 --> 00:29:00,260
of the areas where we reading to a lot

00:28:55,140 --> 00:29:03,040
more work I think it will be good if

00:29:00,260 --> 00:29:04,840
someone and it might actually be a

00:29:03,040 --> 00:29:07,330
he sits down and writes about a test

00:29:04,840 --> 00:29:09,040
framework for this stuff I also think

00:29:07,330 --> 00:29:11,380
actually testing large-scale distribute

00:29:09,040 --> 00:29:13,240
systems is probably harder than actually

00:29:11,380 --> 00:29:14,980
writing them in the first place it's

00:29:13,240 --> 00:29:16,900
rare these to write an application it

00:29:14,980 --> 00:29:19,270
runs across 100 machines it's a lot

00:29:16,900 --> 00:29:20,620
harder to show it work that's apparently

00:29:19,270 --> 00:29:22,150
what testing is doing is trying to show

00:29:20,620 --> 00:29:25,750
your code worked across a big cluster

00:29:22,150 --> 00:29:27,130
and given that it doesn't for the first

00:29:25,750 --> 00:29:29,020
few months of its life or whatever

00:29:27,130 --> 00:29:32,650
trying to get the logs back and trying

00:29:29,020 --> 00:29:33,730
to understand why it failed now and well

00:29:32,650 --> 00:29:35,770
you know we're still in the dark ages

00:29:33,730 --> 00:29:38,440
we're still using log statements you

00:29:35,770 --> 00:29:41,740
know and that's basically printf for a

00:29:38,440 --> 00:29:43,120
thousand machines so testing is fun if

00:29:41,740 --> 00:29:44,710
you really want to work on it come and

00:29:43,120 --> 00:29:45,940
find me and that's not just in

00:29:44,710 --> 00:29:48,250
Hortonworks but if you're writing yarn

00:29:45,940 --> 00:29:53,080
clusters bike to test frameworks and

00:29:48,250 --> 00:29:54,870
share them so key point trying to avoid

00:29:53,080 --> 00:29:57,280
doing as much of the work yourself okay

00:29:54,870 --> 00:29:59,290
application masters are complicated and

00:29:57,280 --> 00:30:00,550
the best way to avoid doing them is to

00:29:59,290 --> 00:30:03,940
let somebody else do all the heavy

00:30:00,550 --> 00:30:05,650
lifting there are various people

00:30:03,940 --> 00:30:07,720
projects working on things like this and

00:30:05,650 --> 00:30:09,760
working on this slide of stuff to run

00:30:07,720 --> 00:30:11,590
existing apps colleagues are doing tez

00:30:09,760 --> 00:30:13,210
there's a pipeline thing give read the

00:30:11,590 --> 00:30:15,040
Microsoft dryad paper you'll understand

00:30:13,210 --> 00:30:16,180
what they're doing there and there are

00:30:15,040 --> 00:30:18,340
there are other things going along on

00:30:16,180 --> 00:30:19,720
top Apache twill is going to be spoken

00:30:18,340 --> 00:30:23,890
about next and I'm going to give a quick

00:30:19,720 --> 00:30:26,020
demo of it here so twill is probably the

00:30:23,890 --> 00:30:29,440
simplest way to run a yarn application

00:30:26,020 --> 00:30:31,270
where it takes a normal was pretty much

00:30:29,440 --> 00:30:33,130
looks like a normal Java runnable and

00:30:31,270 --> 00:30:34,990
runs it elsewhere so this is me

00:30:33,130 --> 00:30:36,520
launching it a yarn application master

00:30:34,990 --> 00:30:39,340
is the client code I basically say

00:30:36,520 --> 00:30:41,920
create some 12 things create an instance

00:30:39,340 --> 00:30:43,870
of my render class and something locally

00:30:41,920 --> 00:30:47,400
to catch their logs and then just run it

00:30:43,870 --> 00:30:49,750
and that runs that runs in the cluster

00:30:47,400 --> 00:30:51,970
so this one mounted demo I've got a

00:30:49,750 --> 00:30:54,010
little frame render app which will take

00:30:51,970 --> 00:30:57,370
some an image file and some parameters

00:30:54,010 --> 00:30:59,380
and it will render render a frame the

00:30:57,370 --> 00:31:01,150
lies about as it scales well it's the

00:30:59,380 --> 00:31:03,370
opposite of MapReduce because it goes

00:31:01,150 --> 00:31:06,970
from a small file it goes normal map

00:31:03,370 --> 00:31:09,700
reduces big input small output this goes

00:31:06,970 --> 00:31:12,190
to small input massive output you can be

00:31:09,700 --> 00:31:14,770
generating gigabytes a second if you've

00:31:12,190 --> 00:31:15,790
got a big busy cluster and so it changes

00:31:14,770 --> 00:31:16,570
the whole notion of where you want to

00:31:15,790 --> 00:31:19,539
place things

00:31:16,570 --> 00:31:21,220
and you know and Germany is your

00:31:19,539 --> 00:31:22,690
scheduling and failure mode the nice

00:31:21,220 --> 00:31:24,730
thing is you can restart anything that

00:31:22,690 --> 00:31:27,429
fails you want to place things so that

00:31:24,730 --> 00:31:29,440
time consecutive frames are close to

00:31:27,429 --> 00:31:31,559
each other so that if your next step is

00:31:29,440 --> 00:31:34,929
actually merging frames into a video

00:31:31,559 --> 00:31:39,340
everything is reasonably local and this

00:31:34,929 --> 00:31:42,039
is my demo the code is online and this

00:31:39,340 --> 00:31:43,659
is where we actually have a piece of

00:31:42,039 --> 00:31:49,480
code this is the runner I'm going to

00:31:43,659 --> 00:31:51,970
find a string here show is not a rig

00:31:49,480 --> 00:31:55,330
demo I want somebody in the audience to

00:31:51,970 --> 00:31:59,830
come up with a sentence you come up with

00:31:55,330 --> 00:32:03,009
a phrase to say developers sleepless

00:31:59,830 --> 00:32:05,080
okay everybody saw that was what my

00:32:03,009 --> 00:32:09,570
person I given this quote to earlier

00:32:05,080 --> 00:32:16,419
said that it was sleep less okay right

00:32:09,570 --> 00:32:18,070
let's bring up my terminal window this

00:32:16,419 --> 00:32:21,360
is me running a mini yarn plus the test

00:32:18,070 --> 00:32:23,700
here so it's starting up the cluster in

00:32:21,360 --> 00:32:27,549
processes running on the local machine

00:32:23,700 --> 00:32:29,620
if I bring a separate window I can do

00:32:27,549 --> 00:32:32,919
see what Java proceeds are running and

00:32:29,620 --> 00:32:38,590
it will tell me what tell me what's

00:32:32,919 --> 00:32:40,299
happening GPS to launch is running okay

00:32:38,590 --> 00:32:43,120
so that that's that's my application

00:32:40,299 --> 00:32:45,820
master running there okay or yeah so

00:32:43,120 --> 00:32:48,370
that's running my code this is still

00:32:45,820 --> 00:32:51,639
busy running away this is all log junk

00:32:48,370 --> 00:32:53,620
that comes out of the yarn application

00:32:51,639 --> 00:32:57,220
master and don't managers it is all

00:32:53,620 --> 00:32:58,600
generally meaningless until you're

00:32:57,220 --> 00:33:00,429
trying to find out why your code doesn't

00:32:58,600 --> 00:33:01,919
work and then you will start end up

00:33:00,429 --> 00:33:04,210
learning to understand this stuff

00:33:01,919 --> 00:33:11,019
they're just just an observation okay

00:33:04,210 --> 00:33:12,549
now here we go developers sleepless see

00:33:11,019 --> 00:33:14,470
so that was me that was me that is a

00:33:12,549 --> 00:33:18,639
yarn application running there locally

00:33:14,470 --> 00:33:20,590
to open a JPEG render some text over it

00:33:18,639 --> 00:33:23,259
and then save it again to and from a

00:33:20,590 --> 00:33:25,240
file system Viet local or do and that

00:33:23,259 --> 00:33:27,190
was all it took so even though I've been

00:33:25,240 --> 00:33:29,169
scaring people with all this stuff about

00:33:27,190 --> 00:33:30,340
how it's hard and painful the rest of it

00:33:29,169 --> 00:33:31,870
the point is you can

00:33:30,340 --> 00:33:34,409
my applications that run on the yarn

00:33:31,870 --> 00:33:37,539
cluster that was me rendering one frame

00:33:34,409 --> 00:33:39,010
locally but if I extend that a bit to

00:33:37,539 --> 00:33:40,919
take things like different texts

00:33:39,010 --> 00:33:50,830
different frames I could run that over

00:33:40,919 --> 00:33:52,150
20 30 nodes and generate real videos all

00:33:50,830 --> 00:33:55,029
right that's what the render looks like

00:33:52,150 --> 00:33:56,980
actually I run I've got a little

00:33:55,029 --> 00:33:58,270
runnable I got a context to get on my

00:33:56,980 --> 00:34:01,120
application arguments on the command

00:33:58,270 --> 00:34:02,740
line passed in I just create my little

00:34:01,120 --> 00:34:04,720
render a code here run it and save it

00:34:02,740 --> 00:34:09,280
and that's it it's a runnable it's not

00:34:04,720 --> 00:34:13,359
doing anything complicated at all so

00:34:09,280 --> 00:34:15,190
there you go yawn we now we can now let

00:34:13,359 --> 00:34:17,619
you run whatever you want to inside the

00:34:15,190 --> 00:34:20,080
cluster it hides a lot of the details

00:34:17,619 --> 00:34:21,909
over the LAN port layering below but it

00:34:20,080 --> 00:34:23,980
still takes work ok you have to start

00:34:21,909 --> 00:34:26,190
handling policies of basement and fair

00:34:23,980 --> 00:34:28,330
you're handling you have to deal with

00:34:26,190 --> 00:34:31,149
building up those command lines and

00:34:28,330 --> 00:34:33,940
executing it so my main recommendation

00:34:31,149 --> 00:34:35,200
is find someone else to do the work and

00:34:33,940 --> 00:34:37,300
that's the one fact that's the secret of

00:34:35,200 --> 00:34:38,740
software engineering journal is find a

00:34:37,300 --> 00:34:40,230
volunteer and those people that put

00:34:38,740 --> 00:34:42,580
their hands up they are the volunteers

00:34:40,230 --> 00:34:43,960
especially overs talking on twill next

00:34:42,580 --> 00:34:46,570
basically someone you should take you

00:34:43,960 --> 00:34:48,040
listening to because really what you

00:34:46,570 --> 00:34:50,320
want to think about is what algorithms

00:34:48,040 --> 00:34:52,179
you're going to run not how to integrate

00:34:50,320 --> 00:34:54,369
with yarn but what is my code going to

00:34:52,179 --> 00:34:57,099
do what is the high level stuff what do

00:34:54,369 --> 00:34:59,410
I want to do to process data to generate

00:34:57,099 --> 00:35:00,910
data to do useful things rather than

00:34:59,410 --> 00:35:03,700
what do I need to do to integrate the

00:35:00,910 --> 00:35:06,700
system and that that's what you have to

00:35:03,700 --> 00:35:09,070
go home and do now and we're very short

00:35:06,700 --> 00:35:11,369
amount of time for questions five

00:35:09,070 --> 00:35:11,369
minutes

00:35:13,510 --> 00:35:18,410
you see with slider you can run

00:35:15,860 --> 00:35:20,780
arbitrary code has anyone tried running

00:35:18,410 --> 00:35:23,000
are something like tom cat right

00:35:20,780 --> 00:35:25,370
question there was with slide ecológico

00:35:23,000 --> 00:35:27,920
um that is the goal of slider win that

00:35:25,370 --> 00:35:29,210
you run arbitrary code ok all right what

00:35:27,920 --> 00:35:31,040
real friendly doing that is we've got a

00:35:29,210 --> 00:35:32,540
new package format that you put things

00:35:31,040 --> 00:35:34,460
in there with various metadata saying

00:35:32,540 --> 00:35:36,590
here are the parameters you have to do

00:35:34,460 --> 00:35:38,450
to help build up and execute the script

00:35:36,590 --> 00:35:40,160
we've got some Python scripts through

00:35:38,450 --> 00:35:42,200
launching and some templates and things

00:35:40,160 --> 00:35:44,270
like that yeah people we have played a

00:35:42,200 --> 00:35:47,900
tomcat we're doing HBase accumulo and

00:35:44,270 --> 00:35:49,490
storm first and we're not actually

00:35:47,900 --> 00:35:50,810
incubated project which I would

00:35:49,490 --> 00:35:53,540
encourage you to get involved in if you

00:35:50,810 --> 00:35:58,490
can well I think about Tom cotton that

00:35:53,540 --> 00:36:00,710
is the goal is to let you run existing

00:35:58,490 --> 00:36:02,450
up the caging as HDFS with saying you

00:36:00,710 --> 00:36:06,200
know the nice idea of application is

00:36:02,450 --> 00:36:07,700
saying Tom cut talking to hbase with my

00:36:06,200 --> 00:36:09,770
client applications talking to tom cat

00:36:07,700 --> 00:36:11,210
we've got to do a lot dynamic binding

00:36:09,770 --> 00:36:13,040
there actually you don't know where

00:36:11,210 --> 00:36:14,960
Tomcats going to be what ports is going

00:36:13,040 --> 00:36:16,190
to be in front on that so one of the

00:36:14,960 --> 00:36:18,170
things I mean a lot to work on is

00:36:16,190 --> 00:36:20,390
service registries and getting

00:36:18,170 --> 00:36:22,040
configurations so whilst we deploy

00:36:20,390 --> 00:36:24,350
Tomcat we have to provide binding

00:36:22,040 --> 00:36:26,540
information so that Tomcat clients can

00:36:24,350 --> 00:36:29,020
work out what's going on so I mean also

00:36:26,540 --> 00:36:31,900
serve as registry stuff I'm going to be

00:36:29,020 --> 00:36:34,970
implementing a chunk of that in yon

00:36:31,900 --> 00:36:37,160
later on this summer actually yarn 896

00:36:34,970 --> 00:36:41,750
if you like so as registries get

00:36:37,160 --> 00:36:45,440
involved ok another question over there

00:36:41,750 --> 00:36:47,000
Stefan Stefan is one of the people of

00:36:45,440 --> 00:36:48,560
volunteered he's doing stratosphere he

00:36:47,000 --> 00:36:53,360
will gladly field your support calls

00:36:48,560 --> 00:36:55,580
Stefan I'm how fast would you say Ken

00:36:53,360 --> 00:36:57,830
yarn in its current architecture get to

00:36:55,580 --> 00:36:59,150
you know bring up an application master

00:36:57,830 --> 00:37:00,920
given that the application muscle starts

00:36:59,150 --> 00:37:04,460
really faster locates a handful of

00:37:00,920 --> 00:37:06,530
containers ok of course can it get well

00:37:04,460 --> 00:37:08,630
what what delayed this yarn add on

00:37:06,530 --> 00:37:10,430
whatever delay your application has ok

00:37:08,630 --> 00:37:12,680
question is what is the startup delay

00:37:10,430 --> 00:37:14,270
well your application starts okay

00:37:12,680 --> 00:37:15,620
there's the overhead to download the

00:37:14,270 --> 00:37:19,160
binaries which is why we're doing some

00:37:15,620 --> 00:37:20,720
caching thing and then there is the

00:37:19,160 --> 00:37:24,080
problem of just telling the load manager

00:37:20,720 --> 00:37:25,460
what it is to have to run no manage is

00:37:24,080 --> 00:37:27,859
reporting the heartbeat

00:37:25,460 --> 00:37:29,839
that's when they get given the work yarn

00:37:27,859 --> 00:37:33,770
can start the most up if it allocates as

00:37:29,839 --> 00:37:36,170
many containers in one go and then when

00:37:33,770 --> 00:37:38,510
and that that startup delay that

00:37:36,170 --> 00:37:40,640
heartbeat can can slow things down the

00:37:38,510 --> 00:37:42,440
problem being the bigger the cluster the

00:37:40,640 --> 00:37:43,849
longer that heartbeat has to be in a

00:37:42,440 --> 00:37:46,220
small class you can just a report in

00:37:43,849 --> 00:37:48,560
faster as you get bigger you have to

00:37:46,220 --> 00:37:50,690
make them slow but good news is also

00:37:48,560 --> 00:37:52,520
that no managers report in whenever they

00:37:50,690 --> 00:37:54,290
finish work so you've got a cluster

00:37:52,520 --> 00:37:56,869
that's busy doing analytics applications

00:37:54,290 --> 00:37:59,089
where containers finish rapidly then no

00:37:56,869 --> 00:38:01,130
managers report in more which generates

00:37:59,089 --> 00:38:02,960
more space a bigger issue is in a big

00:38:01,130 --> 00:38:04,640
cluster that's busy you're not going to

00:38:02,960 --> 00:38:05,930
get space for the containers especially

00:38:04,640 --> 00:38:07,880
if you start asking for lots of memory

00:38:05,930 --> 00:38:09,710
and lots of CPU you're not going to get

00:38:07,880 --> 00:38:18,010
compute time because other people are

00:38:09,710 --> 00:38:21,260
using it silence your question okay okay

00:38:18,010 --> 00:38:23,660
so so yarn is pushing out work in

00:38:21,260 --> 00:38:25,400
response two heartbeats it's pretty much

00:38:23,660 --> 00:38:29,930
doing it like a dupe use to do right

00:38:25,400 --> 00:38:32,390
yeah so Lisa in contrast in systems like

00:38:29,930 --> 00:38:34,760
like spark and Status you try an eagerly

00:38:32,390 --> 00:38:37,580
push out work from the from the Masters

00:38:34,760 --> 00:38:40,460
to the workers which which just get down

00:38:37,580 --> 00:38:41,960
the deployment latency yeah if you

00:38:40,460 --> 00:38:43,400
actually get the trend one of the things

00:38:41,960 --> 00:38:44,630
that we're going to mold away from

00:38:43,400 --> 00:38:47,510
classic MapReduce is to hold

00:38:44,630 --> 00:38:48,920
session-based things so storm and tears

00:38:47,510 --> 00:38:50,359
in that they bring up a set of

00:38:48,920 --> 00:38:52,369
containers that hang around for a while

00:38:50,359 --> 00:38:54,530
and I stopped you having this workflow

00:38:52,369 --> 00:38:56,359
saying bring up a lot of containers tear

00:38:54,530 --> 00:38:59,060
them down req stuff bring them up again

00:38:56,359 --> 00:39:00,740
so it's just if you can if your

00:38:59,060 --> 00:39:03,230
application master can bring up a pool

00:39:00,740 --> 00:39:04,760
of containers and hang onto them you can

00:39:03,230 --> 00:39:07,070
then do a lot of productive work and

00:39:04,760 --> 00:39:08,839
push work out to them however you want

00:39:07,070 --> 00:39:10,339
zookeeper being a good example you can

00:39:08,839 --> 00:39:12,890
just push things in the zookeeper saying

00:39:10,339 --> 00:39:15,440
do this and as the information gets

00:39:12,890 --> 00:39:17,089
picked up by watches the client nodes in

00:39:15,440 --> 00:39:18,500
the watches they can pick things up and

00:39:17,089 --> 00:39:21,040
actually think that's how to older stuff

00:39:18,500 --> 00:39:24,040
actually they use zookeeper a lot there

00:39:21,040 --> 00:39:24,040
okay

00:39:24,690 --> 00:39:29,650
just how does have this young compared

00:39:27,640 --> 00:39:33,430
with measles as a good question how does

00:39:29,650 --> 00:39:34,750
young compared measles I'm not entirely

00:39:33,430 --> 00:39:38,110
sure I haven't played enough with me

00:39:34,750 --> 00:39:40,570
sauce I know measles come out saying we

00:39:38,110 --> 00:39:42,940
are purely an execution framework yarn

00:39:40,570 --> 00:39:46,330
is an evolution of more than my produce

00:39:42,940 --> 00:39:49,270
wealth so I know yarn is very good at

00:39:46,330 --> 00:39:51,400
running short to medium their services

00:39:49,270 --> 00:39:54,700
and applications and announces jobs and

00:39:51,400 --> 00:39:56,170
it has that integral notion of data or

00:39:54,700 --> 00:39:59,650
where placement you want to run your

00:39:56,170 --> 00:40:01,870
code near the data where we're your only

00:39:59,650 --> 00:40:03,910
means it's a good of that but measles is

00:40:01,870 --> 00:40:06,180
better at long-lived applications which

00:40:03,910 --> 00:40:08,560
is where yarn is weaker so we're pushing

00:40:06,180 --> 00:40:10,660
yarn I'm kind of leading in some of the

00:40:08,560 --> 00:40:12,280
work and saying let's make yarn better

00:40:10,660 --> 00:40:13,660
at hosting long-lived applications I

00:40:12,280 --> 00:40:15,430
think it means it's going to the other

00:40:13,660 --> 00:40:20,170
way of saying okay let's let's handle

00:40:15,430 --> 00:40:25,500
short-lived applications better okay

00:40:20,170 --> 00:40:25,500

YouTube URL: https://www.youtube.com/watch?v=zQUGlgBfXzE


