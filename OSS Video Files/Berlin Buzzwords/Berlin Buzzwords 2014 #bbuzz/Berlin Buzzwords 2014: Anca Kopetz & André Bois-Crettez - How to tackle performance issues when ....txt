Title: Berlin Buzzwords 2014: Anca Kopetz & André Bois-Crettez - How to tackle performance issues when ...
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Anca Kopetz & André Bois-Crettez talking about "How to tackle performance issues when implementing high traffic multi-language search engine with Solr/Lucene"

This presentation will summarize the experience gained during an amazing journey of our team that implemented, deployed, and monitored the new search platform in production, which replaced a proprietary search engine with the popular open source Apache Solr.

Our company Kelkoo is an e-shopping platform that connects merchants and customers in different countries all over the world. The core of this platform is the search engine that allows clients and partners to execute full-text queries in order to find the best offers for their search. The queries could be pretty complex: range, filter & function queries, facets etc. They are executed on indexes of more than 15 millions of documents.

We used scalable and feature-rich technologies (Apache Solr/Lucene) to implement the search platform. We had to deal with exciting problems ranging from which features to implement, how to scale out and up the system, to SOLR and JVM tweaking in order to guarantee fast responses with high traffic on search cluster.

About Anca Kopetz:
https://2014.berlinbuzzwords.de/user/172/event/1

About André Bois-Crettez:
https://2014.berlinbuzzwords.de/user/212/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,960 --> 00:00:12,200
hello everyone I am on the way back fit

00:00:09,059 --> 00:00:14,879
is I'm a software architect at Cal group

00:00:12,200 --> 00:00:17,670
hi I'm Monica kopites i'm a software

00:00:14,879 --> 00:00:19,910
engineer at calico and today we are

00:00:17,670 --> 00:00:23,160
going to talk about query performance

00:00:19,910 --> 00:00:25,350
optimization actually we are going to

00:00:23,160 --> 00:00:28,220
talk about it our experience gained

00:00:25,350 --> 00:00:34,010
during one year of trying to implement

00:00:28,220 --> 00:00:37,020
our search engine with a solar you see

00:00:34,010 --> 00:00:39,750
before we start the talk and maybe a

00:00:37,020 --> 00:00:43,190
quick rise of hands how many of you have

00:00:39,750 --> 00:00:46,770
already used the solar or elastic search

00:00:43,190 --> 00:00:48,960
there are most everybody nice and the

00:00:46,770 --> 00:00:53,700
old many of you add the performance

00:00:48,960 --> 00:00:58,760
problems not everybody but quite at

00:00:53,700 --> 00:00:58,760
thanks so you are at the right talk a

00:00:59,780 --> 00:01:06,659
quick got outline first we will explain

00:01:03,690 --> 00:01:10,170
the context and the set up with what is

00:01:06,659 --> 00:01:14,340
calc you what is our on engine usage and

00:01:10,170 --> 00:01:17,399
why it's important 12 performance we

00:01:14,340 --> 00:01:22,649
will present the the way we benchmarked

00:01:17,399 --> 00:01:25,709
on our our system and what we should be

00:01:22,649 --> 00:01:29,630
prepared because we knew that that

00:01:25,709 --> 00:01:32,479
furnace was critical we will present

00:01:29,630 --> 00:01:35,459
performance traditions we came up with

00:01:32,479 --> 00:01:38,670
most of the performance solution depend

00:01:35,459 --> 00:01:43,679
on the usage so not all will be o'clock

00:01:38,670 --> 00:01:46,829
appt liable for you but I hope you will

00:01:43,679 --> 00:01:49,919
see the way we walked and solve problems

00:01:46,829 --> 00:01:52,109
and the last we will explain what

00:01:49,919 --> 00:01:54,749
happened in production because of course

00:01:52,109 --> 00:01:59,090
if even if you are well prepared and

00:01:54,749 --> 00:01:59,090
there are still surprises in prediction

00:01:59,299 --> 00:02:06,509
so before going into the details so

00:02:02,880 --> 00:02:10,340
brief description of Cal cool calc with

00:02:06,509 --> 00:02:13,620
a shopping platform that connects

00:02:10,340 --> 00:02:17,460
customers with merchants actually we

00:02:13,620 --> 00:02:19,319
have a repository of merchants offers

00:02:17,460 --> 00:02:22,530
that are indexed on our

00:02:19,319 --> 00:02:25,499
search platform and then the end users

00:02:22,530 --> 00:02:29,040
via calcio website and the partner

00:02:25,499 --> 00:02:31,379
website they can search for good as you

00:02:29,040 --> 00:02:35,579
can see the source platform is the core

00:02:31,379 --> 00:02:39,959
of our system we have an index of more

00:02:35,579 --> 00:02:44,359
than 70 million documents and calc quiz

00:02:39,959 --> 00:02:47,459
deployed worldwide in 12 countries and

00:02:44,359 --> 00:02:53,909
from the traffic point of view we had

00:02:47,459 --> 00:02:56,489
Peaks over 3,000 queries per second to

00:02:53,909 --> 00:03:00,180
better understand what we do a small

00:02:56,489 --> 00:03:04,290
example French people love cheese fondue

00:03:00,180 --> 00:03:07,530
oh yes yes they do a lot so they can

00:03:04,290 --> 00:03:10,349
connect on the cal cool website and type

00:03:07,530 --> 00:03:14,569
in a query like a pariah Fond du UNS

00:03:10,349 --> 00:03:18,269
arabic and they will find plenty of

00:03:14,569 --> 00:03:21,359
fondue kids as you can see we are

00:03:18,269 --> 00:03:24,689
displayed the offers from different

00:03:21,359 --> 00:03:28,169
merchants and the user have the

00:03:24,689 --> 00:03:30,870
possibility to filter by category we

00:03:28,169 --> 00:03:36,030
display statistic like a min max price

00:03:30,870 --> 00:03:38,759
and they can do faceted navigation you

00:03:36,030 --> 00:03:42,000
may be already familiar with the facets

00:03:38,759 --> 00:03:45,090
oh yeah I think so because we are going

00:03:42,000 --> 00:03:47,939
to talk a lot I'll just say that we we

00:03:45,090 --> 00:03:50,400
display all the margins that have offers

00:03:47,939 --> 00:03:53,009
associated to this query and for each

00:03:50,400 --> 00:04:00,509
member merchant will display the number

00:03:53,009 --> 00:04:02,430
of offers that are indexed and so as you

00:04:00,509 --> 00:04:08,340
can see a search engine is central to

00:04:02,430 --> 00:04:11,250
the way it works and in the past we as

00:04:08,340 --> 00:04:16,009
we were bought by yahoo we had access to

00:04:11,250 --> 00:04:19,049
ya there yahoo internal search engine

00:04:16,009 --> 00:04:21,630
worked pretty well but when they sold us

00:04:19,049 --> 00:04:24,300
we no longer had updates and the

00:04:21,630 --> 00:04:25,889
technology and nobody knew it so it was

00:04:24,300 --> 00:04:30,139
not even possible to very support for

00:04:25,889 --> 00:04:32,700
that and in 2013 it were decided to go

00:04:30,139 --> 00:04:36,740
with the implementation using solar

00:04:32,700 --> 00:04:40,440
for in the past month we selected the

00:04:36,740 --> 00:04:45,150
what was the best engine we could try

00:04:40,440 --> 00:04:48,930
and registry to go with Sura 29

00:04:45,150 --> 00:04:52,470
important points where we should be able

00:04:48,930 --> 00:04:55,740
to sustain the Christmas high traffic at

00:04:52,470 --> 00:04:58,560
the end of the year but we don't have

00:04:55,740 --> 00:05:05,520
strong constraints around the near

00:04:58,560 --> 00:05:10,440
real-time indexation Cassandra explained

00:05:05,520 --> 00:05:12,420
for 2013 we had an important target it

00:05:10,440 --> 00:05:15,990
was the Christmas traffic we are

00:05:12,420 --> 00:05:17,970
concentrated on query performance so we

00:05:15,990 --> 00:05:19,860
said before putting everything into

00:05:17,970 --> 00:05:23,160
production we like to do some

00:05:19,860 --> 00:05:25,940
performance evaluation so we chose as a

00:05:23,160 --> 00:05:30,060
target our big bigger country cluster

00:05:25,940 --> 00:05:32,730
which was composed of 12 servers with

00:05:30,060 --> 00:05:36,060
more than 10 millions documents in the

00:05:32,730 --> 00:05:38,460
index and we expected a traffic of more

00:05:36,060 --> 00:05:41,900
than 600 queries per second and we

00:05:38,460 --> 00:05:46,920
wanted an average query time less than

00:05:41,900 --> 00:05:48,570
200 milliseconds so we said first of all

00:05:46,920 --> 00:05:50,550
we have to develop our search

00:05:48,570 --> 00:05:52,460
application deploy it on the pre

00:05:50,550 --> 00:05:55,860
production servers the pre production

00:05:52,460 --> 00:05:57,990
servers should be as close as possible

00:05:55,860 --> 00:06:00,510
to the production servers from the

00:05:57,990 --> 00:06:04,140
hardware point of view and then we had

00:06:00,510 --> 00:06:06,720
to identify the monitoring tools and we

00:06:04,140 --> 00:06:10,680
wanted to execute the benchmarks in

00:06:06,720 --> 00:06:13,110
order to simulate the real traffic we

00:06:10,680 --> 00:06:15,390
interacted frequently with the solar

00:06:13,110 --> 00:06:22,610
community which is a very active and

00:06:15,390 --> 00:06:27,660
useful community so we decided to

00:06:22,610 --> 00:06:30,750
structure our clusters separately for

00:06:27,660 --> 00:06:32,670
each country most of you should be

00:06:30,750 --> 00:06:34,890
familiar with the the wall structure but

00:06:32,670 --> 00:06:38,220
I will work up so we have such clients

00:06:34,890 --> 00:06:40,590
in our case it's a each server using

00:06:38,220 --> 00:06:44,760
HTTP so our server connecting through a

00:06:40,590 --> 00:06:46,200
virtual IP 12 a-12 a lot balancer that

00:06:44,760 --> 00:06:51,320
distributes the query

00:06:46,200 --> 00:06:53,430
all the server's of country cluster

00:06:51,320 --> 00:06:56,130
depending on the countries we have

00:06:53,430 --> 00:06:59,940
different volume different crazies

00:06:56,130 --> 00:07:03,450
performance so keeping that separated

00:06:59,940 --> 00:07:05,700
was easier for us for prediction on the

00:07:03,450 --> 00:07:08,670
other side we have our affairs

00:07:05,700 --> 00:07:12,630
repository that goes through Houston

00:07:08,670 --> 00:07:15,840
enjoy by indexers with Ted concurred in

00:07:12,630 --> 00:07:19,980
it using cloud server to distribute the

00:07:15,840 --> 00:07:26,730
annexation of documents in the in the

00:07:19,980 --> 00:07:30,060
clusters for the benchmarks we had to

00:07:26,730 --> 00:07:33,330
find the stress tool in order to execute

00:07:30,060 --> 00:07:38,250
them so we chose Gatling we get link you

00:07:33,330 --> 00:07:39,720
can define sonari that read the data set

00:07:38,250 --> 00:07:41,970
and then they will send many many

00:07:39,720 --> 00:07:43,860
queries in parallel to to the pre

00:07:41,970 --> 00:07:47,100
production servers in order to simulate

00:07:43,860 --> 00:07:49,950
the real traffic for the data set what

00:07:47,100 --> 00:07:52,470
we did we took the production log looks

00:07:49,950 --> 00:07:55,050
from the previous search engine Yahoo

00:07:52,470 --> 00:07:57,690
search engine and then we transformed

00:07:55,050 --> 00:08:00,890
into solar query in order to be as close

00:07:57,690 --> 00:08:03,990
as a possible to the production traffic

00:08:00,890 --> 00:08:05,670
when setting the benchmarks there are

00:08:03,990 --> 00:08:08,490
two parameters that we found very

00:08:05,670 --> 00:08:11,490
important is the number of users in

00:08:08,490 --> 00:08:13,770
parallel and the duration of test for

00:08:11,490 --> 00:08:16,380
the number of users in parallel just pay

00:08:13,770 --> 00:08:21,870
attention when you set it because if it

00:08:16,380 --> 00:08:23,910
is too small then your cpu load on the

00:08:21,870 --> 00:08:26,940
pre production servers will be small so

00:08:23,910 --> 00:08:28,620
the servers will be under loaded so for

00:08:26,940 --> 00:08:32,010
example a good beginning we had a lot of

00:08:28,620 --> 00:08:35,970
forty percent and then if it is too high

00:08:32,010 --> 00:08:39,120
the servers are overloaded so it has an

00:08:35,970 --> 00:08:41,160
impact on the average response times of

00:08:39,120 --> 00:08:43,800
the benchmark the results were were

00:08:41,160 --> 00:08:47,240
irrelevant so what we did with chose a

00:08:43,800 --> 00:08:51,630
value so that the cpu load to be of

00:08:47,240 --> 00:08:54,270
eighty percent the duration of test it

00:08:51,630 --> 00:08:57,120
should be long enough in order to have

00:08:54,270 --> 00:08:59,220
relevant results and not too long in

00:08:57,120 --> 00:09:00,120
order not to have to wait too much for

00:08:59,220 --> 00:09:02,640
your results

00:09:00,120 --> 00:09:05,250
it has an impact of the productivity so

00:09:02,640 --> 00:09:09,500
for example what we choose we set it one

00:09:05,250 --> 00:09:13,310
hour two hours and it was so enough

00:09:09,500 --> 00:09:17,790
jetLink generates reports of this kind

00:09:13,310 --> 00:09:21,210
so these are the metrics that we used in

00:09:17,790 --> 00:09:23,820
order to evaluate the performance is the

00:09:21,210 --> 00:09:27,450
mean response time for example or or the

00:09:23,820 --> 00:09:30,930
mean number of requests per second get

00:09:27,450 --> 00:09:34,650
link shows the graphs like response time

00:09:30,930 --> 00:09:41,220
distribution of our number of queries

00:09:34,650 --> 00:09:43,050
per second distribution and so at the

00:09:41,220 --> 00:09:45,029
end of the benchmark of course we have

00:09:43,050 --> 00:09:48,089
the average Crispus account as first

00:09:45,029 --> 00:09:52,770
time in and those graph but it was also

00:09:48,089 --> 00:09:54,930
important to closely monitor a lot of

00:09:52,770 --> 00:09:57,570
matrix about what happened on the

00:09:54,930 --> 00:10:00,000
servers to verify where on the

00:09:57,570 --> 00:10:05,880
bottlenecks because and if you opened

00:10:00,000 --> 00:10:08,010
benchmarks its lot the disk everything

00:10:05,880 --> 00:10:11,010
else will not be used at its full

00:10:08,010 --> 00:10:14,610
potential so having those graphs during

00:10:11,010 --> 00:10:16,890
the benchmarks allowed to verify if the

00:10:14,610 --> 00:10:21,290
load was evenly distributed among all

00:10:16,890 --> 00:10:26,100
the resources memory CPU discs etc and

00:10:21,290 --> 00:10:29,300
not only the system matrix but also the

00:10:26,100 --> 00:10:32,640
matrix from solar using genomics to have

00:10:29,300 --> 00:10:34,890
caches it ratio verified by that they

00:10:32,640 --> 00:10:38,520
are config you are configured properly

00:10:34,890 --> 00:10:40,620
and as well we monitored the the one

00:10:38,520 --> 00:10:43,800
that time is the time after commit

00:10:40,620 --> 00:10:48,060
before the data is available and caches

00:10:43,800 --> 00:10:50,010
around it should not be too long we

00:10:48,060 --> 00:10:53,040
monitor the numbers and text segments

00:10:50,010 --> 00:10:56,279
too because the most index signals you

00:10:53,040 --> 00:10:59,670
have is the way the loosing index is

00:10:56,279 --> 00:11:04,820
done the more segments you have the less

00:10:59,670 --> 00:11:08,310
performance you get so this was the

00:11:04,820 --> 00:11:11,160
phase where we prepared our benchmarks

00:11:08,310 --> 00:11:13,860
we identify the tools to monitor our

00:11:11,160 --> 00:11:16,200
system we installed our products

00:11:13,860 --> 00:11:18,600
servers we developed hours the first

00:11:16,200 --> 00:11:21,839
features on our search application and

00:11:18,600 --> 00:11:24,149
we prepared the benchmark scenario so we

00:11:21,839 --> 00:11:28,050
already we said let's go let's launch

00:11:24,149 --> 00:11:30,660
the benchmarks in order to do the query

00:11:28,050 --> 00:11:35,480
performance evaluation to end to explore

00:11:30,660 --> 00:11:38,760
new ways to improve it so what happened

00:11:35,480 --> 00:11:40,709
first we explode different possibility

00:11:38,760 --> 00:11:43,680
around the shutting and replicating and

00:11:40,709 --> 00:11:47,250
I guess the most of you should have an

00:11:43,680 --> 00:11:50,959
idea about that but simply simple sketch

00:11:47,250 --> 00:11:54,060
is the replication what happens is that

00:11:50,959 --> 00:11:57,540
the index is the same of on each server

00:11:54,060 --> 00:12:00,690
and the queries are load balanced among

00:11:57,540 --> 00:12:03,180
all of those it scales linearly it's

00:12:00,690 --> 00:12:07,140
very efficient when you have a very high

00:12:03,180 --> 00:12:12,300
number of crisp a seconds that's a very

00:12:07,140 --> 00:12:14,540
good scalability system however when

00:12:12,300 --> 00:12:18,620
each of the queries is too long for

00:12:14,540 --> 00:12:22,880
example when the index is very big and

00:12:18,620 --> 00:12:26,880
what you can do is shard tion splits the

00:12:22,880 --> 00:12:30,300
documents among the different entities

00:12:26,880 --> 00:12:33,060
so here we have servers that contains

00:12:30,300 --> 00:12:37,019
two short one is our replicas of shot

00:12:33,060 --> 00:12:39,510
one same for Shawn to here and when the

00:12:37,019 --> 00:12:42,480
crew arrives on any of the servers it

00:12:39,510 --> 00:12:46,649
will be split into two or more depending

00:12:42,480 --> 00:12:50,279
on number of shots surprise so each of

00:12:46,649 --> 00:12:53,279
those will run in parallel and the

00:12:50,279 --> 00:12:56,850
results are merged and we turn to the

00:12:53,279 --> 00:12:58,980
new client so the croix times gets

00:12:56,850 --> 00:13:05,790
roughly equivalent to the maximum tank

00:12:58,980 --> 00:13:08,360
on each of the shots so doing benchmarks

00:13:05,790 --> 00:13:11,430
we have the situation where the hardware

00:13:08,360 --> 00:13:13,220
used either in this configuration on

00:13:11,430 --> 00:13:16,500
this one with the same number of servers

00:13:13,220 --> 00:13:19,699
the results were fairly the same but

00:13:16,500 --> 00:13:23,390
that wasn't the ideal case what happens

00:13:19,699 --> 00:13:23,390
in case of a failure

00:13:25,170 --> 00:13:32,820
in this case it's very different when

00:13:29,010 --> 00:13:37,139
purely replicating somewhere down means

00:13:32,820 --> 00:13:40,560
that the other servers will have to work

00:13:37,139 --> 00:13:44,040
a bit more but all others will be loaded

00:13:40,560 --> 00:13:47,370
the same what happens with shouting is

00:13:44,040 --> 00:13:51,089
that the load is distributed on the

00:13:47,370 --> 00:13:54,600
remaining observers of the shot so here

00:13:51,089 --> 00:13:57,899
we have only one poor server doing all

00:13:54,600 --> 00:14:02,279
the work and it means that the cluster

00:13:57,899 --> 00:14:05,699
will be water depth much faster despite

00:14:02,279 --> 00:14:08,820
having the same hard way so when you

00:14:05,699 --> 00:14:12,540
need resilience keep a lot of

00:14:08,820 --> 00:14:20,730
replication and avoid shouting if you

00:14:12,540 --> 00:14:22,889
can during back benchmarks we are the

00:14:20,730 --> 00:14:26,329
surprise too we expect it to have very

00:14:22,889 --> 00:14:28,800
emotional hardware and configuration and

00:14:26,329 --> 00:14:33,660
sometimes we saw some servers

00:14:28,800 --> 00:14:38,399
moreloading than others and what happens

00:14:33,660 --> 00:14:41,390
is that with the solar and cluster all

00:14:38,399 --> 00:14:43,890
the servers will work the same

00:14:41,390 --> 00:14:47,130
especially with reputation the load is

00:14:43,890 --> 00:14:48,959
really really intent car and perfectly

00:14:47,130 --> 00:14:52,649
disparate distributed among all the

00:14:48,959 --> 00:14:54,480
servers and shouting if you have bad

00:14:52,649 --> 00:14:57,540
luck you can have a sharp slightly more

00:14:54,480 --> 00:14:59,850
legit than the others but mostly the the

00:14:57,540 --> 00:15:03,510
load is evenly distributed that means

00:14:59,850 --> 00:15:06,810
that if any server is a bit slower than

00:15:03,510 --> 00:15:11,120
losers the others will not work for him

00:15:06,810 --> 00:15:15,060
so it will be a bottleneck very quickly

00:15:11,120 --> 00:15:18,390
that's how we discovered that some of

00:15:15,060 --> 00:15:21,209
the servers had energy saving option in

00:15:18,390 --> 00:15:27,810
the viewers so the CPU was not fully

00:15:21,209 --> 00:15:29,910
used we were able to fix that another

00:15:27,810 --> 00:15:33,480
aspect that we analyzed was the

00:15:29,910 --> 00:15:38,050
indexation actually the question was how

00:15:33,480 --> 00:15:40,269
often should we commit how often our

00:15:38,050 --> 00:15:43,390
months should be visible to search and

00:15:40,269 --> 00:15:48,640
how often they should be stored in the

00:15:43,390 --> 00:15:52,060
index for the visibility part we choose

00:15:48,640 --> 00:15:55,360
the soft commit implemented with

00:15:52,060 --> 00:15:59,079
committee within common of 30 minutes

00:15:55,360 --> 00:16:01,089
and open search r equals true so open

00:15:59,079 --> 00:16:05,290
search i equals to it means that offers

00:16:01,089 --> 00:16:07,450
are immediately visible to search what

00:16:05,290 --> 00:16:10,269
happens during soft commit is that the

00:16:07,450 --> 00:16:13,839
transaction logs are not truncated so

00:16:10,269 --> 00:16:16,329
they are accumulating and the caches are

00:16:13,839 --> 00:16:18,640
flushed and auto one so just pay

00:16:16,329 --> 00:16:23,399
attention because it has an impact on

00:16:18,640 --> 00:16:26,709
your performance in terms of query time

00:16:23,399 --> 00:16:29,680
for the durability we implemented the

00:16:26,709 --> 00:16:32,260
hardcore meet via autocommit of 15

00:16:29,680 --> 00:16:35,350
minutes and open searcher equals false

00:16:32,260 --> 00:16:38,829
so it means that the document will be

00:16:35,350 --> 00:16:41,829
stored on the on the index own on the

00:16:38,829 --> 00:16:44,860
illusion index but they won't be visible

00:16:41,829 --> 00:16:47,440
to search what happens here is that the

00:16:44,860 --> 00:16:50,350
transaction logs are truncated and the

00:16:47,440 --> 00:16:53,529
segment merges are initiated which was

00:16:50,350 --> 00:16:56,470
very good because actually during our

00:16:53,529 --> 00:17:01,060
benchmarks we realized that the number

00:16:56,470 --> 00:17:05,319
of em in the same segment impacts the

00:17:01,060 --> 00:17:09,069
query performance so we said okay let's

00:17:05,319 --> 00:17:12,790
optimize our index let's optimize it

00:17:09,069 --> 00:17:15,640
very often so if you launch the

00:17:12,790 --> 00:17:19,980
optimized command the number of segments

00:17:15,640 --> 00:17:19,980
in your index will be reduced to 1 so

00:17:20,669 --> 00:17:26,169
immediately after executing this comment

00:17:23,679 --> 00:17:28,690
we had pretty good results in terms of

00:17:26,169 --> 00:17:32,410
query performance but actually we

00:17:28,690 --> 00:17:35,830
realized very fast that this operation

00:17:32,410 --> 00:17:40,600
has short-term benefits because after

00:17:35,830 --> 00:17:42,760
one hour of commit the every response

00:17:40,600 --> 00:17:48,010
time and the average number of query per

00:17:42,760 --> 00:17:51,430
seconds in our benchmark increased so we

00:17:48,010 --> 00:17:54,190
said let's try something

00:17:51,430 --> 00:17:57,580
let's try something else let's set a

00:17:54,190 --> 00:18:01,600
more aggressive Marsh policy so what we

00:17:57,580 --> 00:18:04,900
did we modify the parameters of the key

00:18:01,600 --> 00:18:07,180
our mesh policy in order to have during

00:18:04,900 --> 00:18:11,140
the committee when the segments are

00:18:07,180 --> 00:18:14,320
married to for loose listen to merge as

00:18:11,140 --> 00:18:17,410
many segments as possible and actually

00:18:14,320 --> 00:18:21,640
what happened is that we had pretty good

00:18:17,410 --> 00:18:25,360
results in terms of number of queries

00:18:21,640 --> 00:18:31,090
per second and every response time so

00:18:25,360 --> 00:18:34,150
actually it worked for us in our case so

00:18:31,090 --> 00:18:37,680
how to explain what we did on the

00:18:34,150 --> 00:18:40,930
exertion side to improve the performance

00:18:37,680 --> 00:18:45,130
to improve performance also important to

00:18:40,930 --> 00:18:48,940
look at search caches first of all the

00:18:45,130 --> 00:18:51,940
recent files are accessed so through a

00:18:48,940 --> 00:18:54,430
memory mapping system it allows the

00:18:51,940 --> 00:18:58,510
operating system to cache the files in

00:18:54,430 --> 00:19:00,670
once so you have much less I oh wait and

00:18:58,510 --> 00:19:04,810
you don't have to increase the JVM

00:19:00,670 --> 00:19:10,720
memory and all cases we had enough run

00:19:04,810 --> 00:19:13,230
to not even need SS these disks we were

00:19:10,720 --> 00:19:17,530
still using the spinning disks we have

00:19:13,230 --> 00:19:19,810
from the previous project and the

00:19:17,530 --> 00:19:22,630
document cash was the disabled because

00:19:19,810 --> 00:19:27,010
it was quite redundant with the memory

00:19:22,630 --> 00:19:30,430
mapping of recent files and we didn't

00:19:27,010 --> 00:19:35,800
see any improvement in in the benchmark

00:19:30,430 --> 00:19:40,420
of using discussion and the credit cash

00:19:35,800 --> 00:19:44,560
was kept very small and as we already

00:19:40,420 --> 00:19:48,190
have a great gash above the calculor API

00:19:44,560 --> 00:19:50,350
it was redundant with Sola and I didn't

00:19:48,190 --> 00:19:53,410
bring any visible improvement in

00:19:50,350 --> 00:19:58,960
benchmarks and it just consumed too much

00:19:53,410 --> 00:20:02,440
java memory so in the given query

00:19:58,960 --> 00:20:03,890
different caches are used here we have a

00:20:02,440 --> 00:20:06,740
query with the

00:20:03,890 --> 00:20:10,190
fulltext an iphone and we have a filter

00:20:06,740 --> 00:20:16,520
on the merchant ID we asked for facets

00:20:10,190 --> 00:20:20,120
and in category as well as colors under

00:20:16,520 --> 00:20:23,780
offers displayed so filters used filter

00:20:20,120 --> 00:20:26,030
cash of course and facets on single

00:20:23,780 --> 00:20:29,750
valued feel like we have here for

00:20:26,030 --> 00:20:34,150
category ID use the field cache of

00:20:29,750 --> 00:20:37,540
listen you cannot tune it through solar

00:20:34,150 --> 00:20:40,880
by default the facets on dynamic feeds

00:20:37,540 --> 00:20:43,460
dynamic multivalued fields like we have

00:20:40,880 --> 00:20:47,600
here with the future star cetera to

00:20:43,460 --> 00:20:49,640
store a great number of different fields

00:20:47,600 --> 00:20:52,870
that depends on new category except i'll

00:20:49,640 --> 00:20:57,050
uncle chris not limited by the schema

00:20:52,870 --> 00:21:01,700
but by default the FC means that field

00:20:57,050 --> 00:21:05,060
value cash is used and the cash on trees

00:21:01,700 --> 00:21:07,910
are very big wet issues the on memory

00:21:05,060 --> 00:21:10,850
usage with that and it's not very good

00:21:07,910 --> 00:21:13,520
for local energy fields no case the

00:21:10,850 --> 00:21:18,850
color could be maybe a dozen different

00:21:13,520 --> 00:21:22,130
value so we tried the set method inner

00:21:18,850 --> 00:21:26,120
which is not always advised that but it

00:21:22,130 --> 00:21:29,360
works very well for the the fields with

00:21:26,120 --> 00:21:31,940
the law cardinality and in benchmarks we

00:21:29,360 --> 00:21:37,970
saw that the memorization what was much

00:21:31,940 --> 00:21:41,270
lower and position pain was the same and

00:21:37,970 --> 00:21:45,410
just spared more memory it's just that

00:21:41,270 --> 00:21:49,820
it's it uses the filter cash same as the

00:21:45,410 --> 00:21:52,970
filters but you have to put a lot of

00:21:49,820 --> 00:21:56,030
countries each value of the facet will

00:21:52,970 --> 00:22:01,340
have a occasion tree so we get to a

00:21:56,030 --> 00:22:05,480
better at we also analyze the impact of

00:22:01,340 --> 00:22:08,150
the query features on the performance so

00:22:05,480 --> 00:22:10,280
we you have to add many features in

00:22:08,150 --> 00:22:13,010
order to meet your relevancy

00:22:10,280 --> 00:22:16,280
requirements but what happens is that

00:22:13,010 --> 00:22:17,420
most of the time or sometimes these

00:22:16,280 --> 00:22:21,170
features have

00:22:17,420 --> 00:22:22,730
an impact on the performance so what you

00:22:21,170 --> 00:22:26,060
should do you should get a good balance

00:22:22,730 --> 00:22:27,800
between relevancy and performance when

00:22:26,060 --> 00:22:32,060
you measure the relevancy you measure

00:22:27,800 --> 00:22:33,680
video manual test or a be testing but

00:22:32,060 --> 00:22:36,770
what you should do you should also

00:22:33,680 --> 00:22:40,250
measure the impact on the performance so

00:22:36,770 --> 00:22:43,580
you should monitor your cluster in order

00:22:40,250 --> 00:22:47,090
to see what's the impact on each future

00:22:43,580 --> 00:22:50,180
on the mini response time or a query

00:22:47,090 --> 00:22:53,650
time to better understand that I would

00:22:50,180 --> 00:22:53,650
like to give you an example of

00:22:53,860 --> 00:22:59,780
transformations of features that we

00:22:56,090 --> 00:23:02,810
implemented a tail at Cal cool so if the

00:22:59,780 --> 00:23:06,320
user types in a query like leather

00:23:02,810 --> 00:23:08,480
accessories for iphone will transform

00:23:06,320 --> 00:23:11,930
this query we will apply some

00:23:08,480 --> 00:23:16,310
transformations like lower case leather

00:23:11,930 --> 00:23:19,370
becomes leather stemming the accessories

00:23:16,310 --> 00:23:22,160
is reduced to accessory will remove the

00:23:19,370 --> 00:23:24,650
stop words and then we split on modeling

00:23:22,160 --> 00:23:29,360
so iphone 5 it will become towards

00:23:24,650 --> 00:23:32,720
iphone & 5 by default the query

00:23:29,360 --> 00:23:36,400
operatories end so all search terms are

00:23:32,720 --> 00:23:40,520
mandatory but what happens if the query

00:23:36,400 --> 00:23:44,840
doesn't return any result what we do we

00:23:40,520 --> 00:23:47,570
execute an or query and actually during

00:23:44,840 --> 00:23:49,880
our benchmarks we realize that the or

00:23:47,570 --> 00:23:53,020
query is very expensive it takes a lot

00:23:49,880 --> 00:23:56,540
of time because it returns lots of a

00:23:53,020 --> 00:24:00,340
result and it searches through the whole

00:23:56,540 --> 00:24:02,540
index so what we did for example we

00:24:00,340 --> 00:24:05,590
implemented the mean should max that

00:24:02,540 --> 00:24:09,770
will match that reduce the number of

00:24:05,590 --> 00:24:13,070
work queries the facets are also very

00:24:09,770 --> 00:24:15,470
expensive the fact that we display

00:24:13,070 --> 00:24:19,160
offers from different merchants so we

00:24:15,470 --> 00:24:23,090
had to youth group I merchant ID it's

00:24:19,160 --> 00:24:26,390
also expensive the main idea behind this

00:24:23,090 --> 00:24:30,820
every time you add a new feature try to

00:24:26,390 --> 00:24:30,820
test the impact on the performance

00:24:30,840 --> 00:24:38,289
so this was the end of our benchmarks

00:24:34,389 --> 00:24:41,070
phase we analyzed many aspects we did

00:24:38,289 --> 00:24:43,929
many improvements on the solar cloud

00:24:41,070 --> 00:24:47,740
configuration on the indexation policy

00:24:43,929 --> 00:24:53,980
Marsh policy search caches and solar

00:24:47,740 --> 00:24:57,130
features so we were all ready to go in

00:24:53,980 --> 00:25:00,130
prediction right and of course we had

00:24:57,130 --> 00:25:04,630
surprises for example we had out of

00:25:00,130 --> 00:25:07,269
memories due to filter cash we tune the

00:25:04,630 --> 00:25:10,809
values so that the memory usage was

00:25:07,269 --> 00:25:12,880
adequate in the benchmark but the actual

00:25:10,809 --> 00:25:15,570
number you put in the configuration is

00:25:12,880 --> 00:25:18,490
not doing more used the memo euros will

00:25:15,570 --> 00:25:21,190
depend on the data you have in the index

00:25:18,490 --> 00:25:24,510
as well as what are the queries that are

00:25:21,190 --> 00:25:27,460
done and this despite we used the

00:25:24,510 --> 00:25:31,330
prediction initiation data and pollution

00:25:27,460 --> 00:25:34,419
queries we were quite surprised also by

00:25:31,330 --> 00:25:37,510
that and so we increase the memory it's

00:25:34,419 --> 00:25:40,779
important that you also wash game as the

00:25:37,510 --> 00:25:44,769
same start and maximum given each

00:25:40,779 --> 00:25:46,659
settings that was very evident in the

00:25:44,769 --> 00:25:49,080
benchmarks 20 something like twenty

00:25:46,659 --> 00:25:52,899
twenty percent person performance

00:25:49,080 --> 00:25:56,649
improvement and we turn down the a bit

00:25:52,899 --> 00:25:59,409
the number of education trees and we

00:25:56,649 --> 00:26:02,230
solve that about garbage collecting

00:25:59,409 --> 00:26:05,139
parameters and they are quite a few it's

00:26:02,230 --> 00:26:08,250
a bit of a black art I refer you to the

00:26:05,139 --> 00:26:11,200
page of Shannon I see that some apps the

00:26:08,250 --> 00:26:13,990
various settings that works some are

00:26:11,200 --> 00:26:16,179
more detail than others and the main

00:26:13,990 --> 00:26:19,149
takeaway is that confront my mark and

00:26:16,179 --> 00:26:24,639
sweep CMS is important for solar that's

00:26:19,149 --> 00:26:26,590
the group that works to the best we had

00:26:24,639 --> 00:26:29,860
a few bad surprises with the service

00:26:26,590 --> 00:26:33,880
going to recovery that means that one of

00:26:29,860 --> 00:26:37,809
the server in a cluster things is it's

00:26:33,880 --> 00:26:40,059
lagging behind the indexation so it will

00:26:37,809 --> 00:26:42,480
stop serving queries and a lot would be

00:26:40,059 --> 00:26:44,460
distributed among those us

00:26:42,480 --> 00:26:48,900
and it will stop serving queries and

00:26:44,460 --> 00:26:52,500
retrieve index from another server and

00:26:48,900 --> 00:26:55,650
during the during that time the index is

00:26:52,500 --> 00:27:00,750
retrieved it means that you have one one

00:26:55,650 --> 00:27:04,559
less server doing the query work and we

00:27:00,750 --> 00:27:07,880
followed closely the solar versions that

00:27:04,559 --> 00:27:12,169
were released to benefit from

00:27:07,880 --> 00:27:16,440
improvement and features bug fixes or

00:27:12,169 --> 00:27:22,620
performance improvements son we had some

00:27:16,440 --> 00:27:24,480
cases of regression in your cases so we

00:27:22,620 --> 00:27:28,950
discussed on the solo is our mailing

00:27:24,480 --> 00:27:33,150
list file server and i'll put fix the

00:27:28,950 --> 00:27:39,650
those case and the subsequent versions

00:27:33,150 --> 00:27:43,590
were perfect for us so this is our story

00:27:39,650 --> 00:27:46,710
it was a success story with a leucine

00:27:43,590 --> 00:27:51,150
solar it took was one year we had a high

00:27:46,710 --> 00:27:53,910
traffic peaks of more 3000 queries per

00:27:51,150 --> 00:27:57,630
second with an index of more than 70

00:27:53,910 --> 00:28:00,720
millions documents the main ideas that

00:27:57,630 --> 00:28:04,410
we would like to share with you when

00:28:00,720 --> 00:28:06,630
tackling performance issues is benchmark

00:28:04,410 --> 00:28:10,530
your system before putting into

00:28:06,630 --> 00:28:13,110
production keep monitoring your

00:28:10,530 --> 00:28:16,110
production cluster just pay attention

00:28:13,110 --> 00:28:19,500
that the configuration that you do

00:28:16,110 --> 00:28:21,809
during the benchmark might not be the

00:28:19,500 --> 00:28:24,150
same in the production as Andre

00:28:21,809 --> 00:28:26,820
explained with the setting of the filter

00:28:24,150 --> 00:28:30,540
cash we had an out of memory in

00:28:26,820 --> 00:28:32,700
production and identify the bad guy and

00:28:30,540 --> 00:28:35,460
deal with him it wasn't one of my

00:28:32,700 --> 00:28:37,710
colleagues at killed who actually what

00:28:35,460 --> 00:28:41,160
I'm trying to say is that try to

00:28:37,710 --> 00:28:44,850
identify the resource or the component

00:28:41,160 --> 00:28:48,600
or the behavior that might have an

00:28:44,850 --> 00:28:51,990
impact on your performance we have new

00:28:48,600 --> 00:28:53,820
ideas for performance optimization that

00:28:51,990 --> 00:28:55,900
we would like to implement them like

00:28:53,820 --> 00:28:58,180
analyzing the incoming

00:28:55,900 --> 00:29:01,120
african see if we can remove some bad

00:28:58,180 --> 00:29:05,190
queries or test some other types of

00:29:01,120 --> 00:29:09,730
caches so we still have work to do

00:29:05,190 --> 00:29:12,100
that's it thank you a lot so if you are

00:29:09,730 --> 00:29:14,410
interested as you can see we are doing

00:29:12,100 --> 00:29:16,570
many interesting stuff would kill you if

00:29:14,410 --> 00:29:19,390
you are interested in sort or big data

00:29:16,570 --> 00:29:21,790
please join us the engineering team is

00:29:19,390 --> 00:29:24,730
in Grenoble in France which is a very

00:29:21,790 --> 00:29:31,150
nice city near the mountains and now if

00:29:24,730 --> 00:29:41,130
you have questions yeah okay any

00:29:31,150 --> 00:29:46,270
questions so finally you use short or

00:29:41,130 --> 00:29:51,220
what was the decision it depends so for

00:29:46,270 --> 00:29:56,110
one of most loaded cluster we as we were

00:29:51,220 --> 00:29:59,440
using bit old servers we had two shot so

00:29:56,110 --> 00:30:01,390
I guess it was something like no more

00:29:59,440 --> 00:30:05,320
than two shots and and don't remember

00:30:01,390 --> 00:30:08,740
the number of replicas maybe 44 files it

00:30:05,320 --> 00:30:11,230
was so 12 I think awesome yeah it was 12

00:30:08,740 --> 00:30:14,920
service for example it was really the

00:30:11,230 --> 00:30:17,590
one of the biggest cluster most

00:30:14,920 --> 00:30:20,100
problematic one and all those were only

00:30:17,590 --> 00:30:26,140
for example two servers in a reputation

00:30:20,100 --> 00:30:29,440
and such and we ended it with bringing

00:30:26,140 --> 00:30:32,370
new hardware so that's why quite old and

00:30:29,440 --> 00:30:36,760
this time we did not lead the shouting

00:30:32,370 --> 00:30:42,330
to simplify that so when when you use

00:30:36,760 --> 00:30:45,730
the for the case where you used shorts

00:30:42,330 --> 00:30:51,220
did you use something like routing like

00:30:45,730 --> 00:30:54,670
I mean to yeah yeah it was the default

00:30:51,220 --> 00:30:57,280
word witching so we didn't custom it

00:30:54,670 --> 00:30:59,620
that that to your question yeah yeah my

00:30:57,280 --> 00:31:01,690
question is what are you customize the

00:30:59,620 --> 00:31:03,940
routing or no no no it was the default

00:31:01,690 --> 00:31:05,430
one another side solo crowd you're

00:31:03,940 --> 00:31:18,820
seeing

00:31:05,430 --> 00:31:23,520
no it didn't thank you hi on one slide

00:31:18,820 --> 00:31:26,190
you showed that you have regionalized

00:31:23,520 --> 00:31:30,700
server cluster so you there was one

00:31:26,190 --> 00:31:35,650
cluster for France and one for Russia do

00:31:30,700 --> 00:31:37,480
you split any content according to

00:31:35,650 --> 00:31:39,970
language to each cluster so does the

00:31:37,480 --> 00:31:42,520
Russian cluster just has Russian

00:31:39,970 --> 00:31:45,280
language content or do you have a multi

00:31:42,520 --> 00:31:49,690
language environment in each cluster

00:31:45,280 --> 00:31:52,090
know right now we separate that the

00:31:49,690 --> 00:31:55,720
configuration for Sara we have a common

00:31:52,090 --> 00:32:00,130
part and customized part for each each

00:31:55,720 --> 00:32:02,170
region country and there is no inside

00:32:00,130 --> 00:32:05,860
the cluster there is no multi-language

00:32:02,170 --> 00:32:08,080
stuff and the only thing we shared

00:32:05,860 --> 00:32:12,310
between clusters was the sole keeper in

00:32:08,080 --> 00:32:17,170
December so with five servers spread

00:32:12,310 --> 00:32:20,530
among all those and even for the follow

00:32:17,170 --> 00:32:22,390
dogs / even if it was only one and

00:32:20,530 --> 00:32:25,030
somewhere for all the countries there

00:32:22,390 --> 00:32:28,090
are split using a zookeeper would I

00:32:25,030 --> 00:32:30,370
don't know if you are familiar so it

00:32:28,090 --> 00:32:33,160
allows to supply the configuration and

00:32:30,370 --> 00:32:36,250
for us it was much easier to change the

00:32:33,160 --> 00:32:43,300
configuration for a country sobriety you

00:32:36,250 --> 00:32:48,310
with that okay so and so any any content

00:32:43,300 --> 00:32:51,460
you have in for example French you put

00:32:48,310 --> 00:32:54,610
in just one cluster and have a language

00:32:51,460 --> 00:32:57,460
specific indexing and schema in this

00:32:54,610 --> 00:33:00,040
cluster with Zola yes yeah for example

00:32:57,460 --> 00:33:03,040
the stabbing we apply different stammers

00:33:00,040 --> 00:33:06,670
for each language so depending on the

00:33:03,040 --> 00:33:08,790
country cluster we the schema is adapted

00:33:06,670 --> 00:33:12,460
to that country so it will include the

00:33:08,790 --> 00:33:17,500
appropriate stemmer okay and have you

00:33:12,460 --> 00:33:18,520
ever thought about mixing it up and how

00:33:17,500 --> 00:33:22,090
that would have an

00:33:18,520 --> 00:33:25,270
impact on performance the advantage of

00:33:22,090 --> 00:33:29,590
mixing different countries and the only

00:33:25,270 --> 00:33:33,690
single cluster would be to spread the

00:33:29,590 --> 00:33:38,250
load in an easier way problems are that

00:33:33,690 --> 00:33:43,600
memory wise it's not it's not easy and

00:33:38,250 --> 00:33:46,120
the the caching is different and we

00:33:43,600 --> 00:33:52,060
added a bit with the previous engine to

00:33:46,120 --> 00:33:54,600
keep a country separate so we kinda went

00:33:52,060 --> 00:33:57,220
with the same type of architecture

00:33:54,600 --> 00:34:00,700
sometimes we discuss about what can we

00:33:57,220 --> 00:34:02,800
merge what can happen is that when the

00:34:00,700 --> 00:34:05,290
single server we can have multiple

00:34:02,800 --> 00:34:09,760
instances of solar some four countries

00:34:05,290 --> 00:34:14,200
on furniture when we don't mix the two

00:34:09,760 --> 00:34:17,230
settings or the the JVM we don't mix

00:34:14,200 --> 00:34:24,250
languages in a single garage again thank

00:34:17,230 --> 00:34:27,850
you how do you replicate your index

00:34:24,250 --> 00:34:30,730
using the HTTP replication or the arson

00:34:27,850 --> 00:34:33,820
connection yeah we are using the solar

00:34:30,730 --> 00:34:37,600
cloud feature so it's all done by nice

00:34:33,820 --> 00:34:40,570
flow itself that your question we don't

00:34:37,600 --> 00:34:42,820
use the old style replication or first

00:34:40,570 --> 00:34:46,890
prototypes use the master-slave and

00:34:42,820 --> 00:34:50,080
search and it was a bit of going back

00:34:46,890 --> 00:34:53,230
compared to the yahoo proprietary search

00:34:50,080 --> 00:34:56,530
engine and we were all glad that solar

00:34:53,230 --> 00:34:59,560
cloud arrived and we moved any single

00:34:56,530 --> 00:35:01,570
point of failure in case compared to the

00:34:59,560 --> 00:35:06,400
previous search engine there is much

00:35:01,570 --> 00:35:11,860
less sports in in solar compared to a

00:35:06,400 --> 00:35:16,090
lot of photos hi I'm question about

00:35:11,860 --> 00:35:18,160
hardware so those clusters look really

00:35:16,090 --> 00:35:20,170
impressive so do you host everything

00:35:18,160 --> 00:35:25,090
internally or maybe use some services

00:35:20,170 --> 00:35:27,640
like Amazon services no we have our own

00:35:25,090 --> 00:35:31,840
space in data centers with a physical

00:35:27,640 --> 00:35:35,950
hardware it's a submitter for

00:35:31,840 --> 00:35:40,090
early gassy we are studying what we can

00:35:35,950 --> 00:35:42,700
move to a amazon services or in such

00:35:40,090 --> 00:35:49,390
it's not that easy because the volume of

00:35:42,700 --> 00:35:53,920
that I windex as a cost and well for

00:35:49,390 --> 00:35:59,970
formations that one's 24 days etc it's a

00:35:53,920 --> 00:35:59,970
it's not so interesting to go to Amazon

00:36:01,470 --> 00:36:07,510
you have change the feather message to

00:36:03,790 --> 00:36:09,810
enum and did it reduce the amount of

00:36:07,510 --> 00:36:13,270
memory you have to need further caches

00:36:09,810 --> 00:36:17,080
so what do you say you change the pass

00:36:13,270 --> 00:36:19,930
up method yeah from FC to enum yeah it

00:36:17,080 --> 00:36:22,000
reduces this is this ridiculous amount

00:36:19,930 --> 00:36:26,370
of memory unique occasion yeah because

00:36:22,000 --> 00:36:29,200
actually the for each type of cash the

00:36:26,370 --> 00:36:30,940
type of information that is stored is

00:36:29,200 --> 00:36:33,220
different from one to another so it

00:36:30,940 --> 00:36:36,490
reduced the amount of memory how much

00:36:33,220 --> 00:36:39,670
good actually depends a lot on the

00:36:36,490 --> 00:36:45,150
number of entries that you said but for

00:36:39,670 --> 00:36:49,270
the same for the same number of entries

00:36:45,150 --> 00:36:53,260
yeah sorry in fact it really depends on

00:36:49,270 --> 00:36:57,580
the data so if you have a few different

00:36:53,260 --> 00:37:01,600
values for fill the field value cash is

00:36:57,580 --> 00:37:03,970
really not efficient and the enemy

00:37:01,600 --> 00:37:07,900
really shined for that it's some

00:37:03,970 --> 00:37:13,150
difference is that photo setting you you

00:37:07,900 --> 00:37:16,900
enter one for each field in field value

00:37:13,150 --> 00:37:21,550
cash whereas for the enemy mode with

00:37:16,900 --> 00:37:23,520
filter cash it's one for each value for

00:37:21,550 --> 00:37:27,730
each field so the dough number is a

00:37:23,520 --> 00:37:29,860
minor thousands more sound sound bigger

00:37:27,730 --> 00:37:35,020
it just that the memory usage is much

00:37:29,860 --> 00:37:37,140
smaller / eaten thanks any more

00:37:35,020 --> 00:37:37,140
questions

00:37:42,860 --> 00:37:48,150
hi so I know you said during your

00:37:45,120 --> 00:37:50,250
benchmarking you were targeting meme

00:37:48,150 --> 00:37:53,010
response time did you ever have to

00:37:50,250 --> 00:37:55,200
consider things like 95th percentile and

00:37:53,010 --> 00:37:58,410
maximum response time as well just for

00:37:55,200 --> 00:38:02,120
looking at those yes it was to simplify

00:37:58,410 --> 00:38:06,920
the presentation but of course the the

00:38:02,120 --> 00:38:11,460
95 and ninety-nine percent are important

00:38:06,920 --> 00:38:15,420
with with the gvm garbage collector it

00:38:11,460 --> 00:38:19,050
can also that the 99 performance what we

00:38:15,420 --> 00:38:22,380
saw in the 95 it was a pretty decent so

00:38:19,050 --> 00:38:31,010
I guess one of the benchmarks under the

00:38:22,380 --> 00:38:34,710
value is it at the beginning begin

00:38:31,010 --> 00:38:37,620
actually in this way we realize that the

00:38:34,710 --> 00:38:40,080
all queries takes a lot of time so we

00:38:37,620 --> 00:38:42,390
had to analyze them in details and you

00:38:40,080 --> 00:38:45,870
had to find a solution to D reduce the

00:38:42,390 --> 00:38:48,630
number of or queries as i said the

00:38:45,870 --> 00:38:50,550
metrics that i show there it was just an

00:38:48,630 --> 00:38:52,590
example of the metrics that we are using

00:38:50,550 --> 00:38:55,170
but actually we are analyzing the whole

00:38:52,590 --> 00:39:04,980
report it's very important as you as you

00:38:55,170 --> 00:39:09,360
are saying ok any more questions going

00:39:04,980 --> 00:39:12,280
once twice three salt ok thank you thank

00:39:09,360 --> 00:39:14,340
you guys enjoy your lunch

00:39:12,280 --> 00:39:14,340

YouTube URL: https://www.youtube.com/watch?v=AFbWhjr5bho


