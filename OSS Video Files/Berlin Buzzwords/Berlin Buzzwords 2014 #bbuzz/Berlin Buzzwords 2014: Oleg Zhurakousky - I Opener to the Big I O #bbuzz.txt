Title: Berlin Buzzwords 2014: Oleg Zhurakousky - I Opener to the Big I O #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	There are many mechanisms for storing and processing a collection of data sets so large and complex that we collectively refer to it as Big Data. From NoSQL data stores to the Distributed File Systems and Computation engines to columnar stores to flat files - its all about capture, storage, analysis, searches etc. 

We want it all and we want it fast and traditional data processing applications can no longer support our demands. And while technologies such as Hadoop and its ecosystem derivatives paved an initial path to solving Big Data problems the approaches and assumptions they are built on starting to show its limitations one could only overcome by radically changing the way we think about storing and accessing data in general. In the end its all about I/O and how to make it more efficient.

Read more:
https://2014.berlinbuzzwords.de/session/iopener-big-io

About Oleg Zhurakousky:
https://2014.berlinbuzzwords.de/user/226/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,620 --> 00:00:12,930
so hello everybody my name is Allegra

00:00:10,250 --> 00:00:18,880
Kowski I work for Hortonworks

00:00:12,930 --> 00:00:20,170
prior that i was at spring source gas

00:00:18,880 --> 00:00:24,099
must be familiar with spring framework

00:00:20,170 --> 00:00:26,500
so i was one of the developers of it and

00:00:24,099 --> 00:00:28,689
what I do at Hortonworks actual is a

00:00:26,500 --> 00:00:31,180
very interesting story I quickly tell

00:00:28,689 --> 00:00:33,010
you about it after leaving springsource

00:00:31,180 --> 00:00:35,579
one of the reasons why I lab spring

00:00:33,010 --> 00:00:38,110
source for VMware at the time was that

00:00:35,579 --> 00:00:42,040
we kind of start looking into Hadoop and

00:00:38,110 --> 00:00:44,020
I just realized that while I can you

00:00:42,040 --> 00:00:45,910
know their own technology I can read the

00:00:44,020 --> 00:00:49,390
comment ation I really didn't know much

00:00:45,910 --> 00:00:52,930
about how Hadoop is applied how and what

00:00:49,390 --> 00:00:54,430
is big data altogether so um when I came

00:00:52,930 --> 00:00:56,170
to Horton rocks actual specifically

00:00:54,430 --> 00:00:58,300
requested that I don't want to go to

00:00:56,170 --> 00:01:00,460
engineering just yet I wanted to join

00:00:58,300 --> 00:01:03,340
the field so i spent about a year and a

00:01:00,460 --> 00:01:05,680
half doing various different consulting

00:01:03,340 --> 00:01:08,200
engagements for Hortonworks related to

00:01:05,680 --> 00:01:13,030
the big data and kind of have a pretty

00:01:08,200 --> 00:01:14,680
good grasp of on things related to how

00:01:13,030 --> 00:01:18,300
Hadoop is being applied in modern

00:01:14,680 --> 00:01:20,950
enterprises and as I was sharing with

00:01:18,300 --> 00:01:23,140
the previous speaker just a few minutes

00:01:20,950 --> 00:01:25,479
ago one of the things that I found very

00:01:23,140 --> 00:01:29,799
interesting while doing a lot of

00:01:25,479 --> 00:01:34,049
consulting around Hadoop is that I spent

00:01:29,799 --> 00:01:37,240
a lot of times writing custom code which

00:01:34,049 --> 00:01:39,729
was kind of strange to me because here's

00:01:37,240 --> 00:01:42,690
the hortonworks we are selling product

00:01:39,729 --> 00:01:45,490
and Hadoop ecosystem seems to be very

00:01:42,690 --> 00:01:47,970
rich with variety of products around

00:01:45,490 --> 00:01:52,440
could do you think around Hadoop itself

00:01:47,970 --> 00:01:55,869
but for whatever reason I can't find a

00:01:52,440 --> 00:01:59,290
specific match to fit a specific problem

00:01:55,869 --> 00:02:02,140
or even if I do find the match to fit a

00:01:59,290 --> 00:02:04,479
specific problem then that particular

00:02:02,140 --> 00:02:06,040
product or technology doesn't really do

00:02:04,479 --> 00:02:07,540
everything that I've wanted to do or

00:02:06,040 --> 00:02:09,340
doesn't perform the way I wanted to

00:02:07,540 --> 00:02:11,140
perform and when I use the word want

00:02:09,340 --> 00:02:13,659
it's not necessarily what I want it's

00:02:11,140 --> 00:02:15,640
what my customer needed and one of the

00:02:13,659 --> 00:02:17,830
problems we'll all talk a little later

00:02:15,640 --> 00:02:20,800
on was related to

00:02:17,830 --> 00:02:24,220
you know super fast data capture or data

00:02:20,800 --> 00:02:27,880
in jest but anyway so today the topic is

00:02:24,220 --> 00:02:32,440
I opener to the Big Al kind of a catchy

00:02:27,880 --> 00:02:34,240
title what is the big I oh I don't think

00:02:32,440 --> 00:02:36,190
it's all that different actually I think

00:02:34,240 --> 00:02:37,900
it's not different at all from a regular

00:02:36,190 --> 00:02:40,030
io because at the end of the day you

00:02:37,900 --> 00:02:41,950
have variety of distribution technology

00:02:40,030 --> 00:02:45,070
whether it's a hadoop hazel cast great

00:02:41,950 --> 00:02:47,770
gain what have you right but at the end

00:02:45,070 --> 00:02:49,990
of the day your data may be distributed

00:02:47,770 --> 00:02:51,850
your process may be distributed but at

00:02:49,990 --> 00:02:53,830
some point of time you have to get the

00:02:51,850 --> 00:02:58,000
data from the disk and you have to read

00:02:53,830 --> 00:02:59,410
it right so and a lot of the times when

00:02:58,000 --> 00:03:02,830
you're relying on some of the technology

00:02:59,410 --> 00:03:05,830
and this is what I noticed things aren't

00:03:02,830 --> 00:03:07,150
done very efficiently and what I'm going

00:03:05,830 --> 00:03:10,270
to do today I'm going to quickly go

00:03:07,150 --> 00:03:11,740
through the slide deck typically you

00:03:10,270 --> 00:03:13,720
know I'm used to sort of a spring one

00:03:11,740 --> 00:03:16,510
format where we got like 90 minutes to

00:03:13,720 --> 00:03:18,910
talk you can show a lot of things you

00:03:16,510 --> 00:03:21,670
can go on a tangent and come back and go

00:03:18,910 --> 00:03:25,000
again when you do 40 minutes it's not

00:03:21,670 --> 00:03:26,950
that simple and the other extreme of

00:03:25,000 --> 00:03:30,070
that is what i did ask allah days which

00:03:26,950 --> 00:03:31,660
was 20 minutes so that's actually the

00:03:30,070 --> 00:03:34,360
talk that i really had to prepare and

00:03:31,660 --> 00:03:38,290
rehearse but anyway so let's talk about

00:03:34,360 --> 00:03:39,880
the big I oh and here's kind of the

00:03:38,290 --> 00:03:42,040
agenda so we're going to talk a little

00:03:39,880 --> 00:03:43,630
bit about what is a big data kind of

00:03:42,040 --> 00:03:45,820
touched on a topic of what is structured

00:03:43,630 --> 00:03:48,880
versus unstructured how do we measure it

00:03:45,820 --> 00:03:51,730
store it access it and kind of looked at

00:03:48,880 --> 00:03:53,320
some of the high level architecture I'm

00:03:51,730 --> 00:03:56,620
going to talk about compute mechanics

00:03:53,320 --> 00:03:59,440
why things are slow how easy it is to

00:03:56,620 --> 00:04:02,560
either fixed time or get caught in them

00:03:59,440 --> 00:04:04,920
right and how many of you familiar with

00:04:02,560 --> 00:04:09,550
this concept called mechanical sympathy

00:04:04,920 --> 00:04:11,230
so good couple people I actually got

00:04:09,550 --> 00:04:13,450
very fascinated when I start reading it

00:04:11,230 --> 00:04:15,820
their blogs and things that they did

00:04:13,450 --> 00:04:18,340
with the helm a disrupter and other

00:04:15,820 --> 00:04:20,970
things it's very interesting but like I

00:04:18,340 --> 00:04:24,310
said we'll talk about it in a bit so um

00:04:20,970 --> 00:04:26,100
what is a big data I mean it's

00:04:24,310 --> 00:04:28,380
definitely a buzz word which is chi

00:04:26,100 --> 00:04:30,960
appropriate for this conference right so

00:04:28,380 --> 00:04:33,390
um it means many things to many

00:04:30,960 --> 00:04:35,760
different people just like SOA did just

00:04:33,390 --> 00:04:38,280
like ESP edit and many other acronyms

00:04:35,760 --> 00:04:40,290
and words that we have right so

00:04:38,280 --> 00:04:42,450
according to wikipedia it's a data set

00:04:40,290 --> 00:04:44,730
data sets there are two large and

00:04:42,450 --> 00:04:48,870
complex the manipulator interrogate with

00:04:44,730 --> 00:04:52,170
the standards methods or tools do you

00:04:48,870 --> 00:04:56,040
agree with that definition I mean it's

00:04:52,170 --> 00:04:59,690
not wrong but is it complete something

00:04:56,040 --> 00:05:04,080
missing we'll get to it in a few minutes

00:04:59,690 --> 00:05:06,720
so how do we measure big data is it just

00:05:04,080 --> 00:05:08,100
by its size and again the next few

00:05:06,720 --> 00:05:11,400
slides are going to be basically raising

00:05:08,100 --> 00:05:14,910
some of these questions so um so what

00:05:11,400 --> 00:05:19,230
does it mean big and if it's not the

00:05:14,910 --> 00:05:21,060
size then what other parameters we look

00:05:19,230 --> 00:05:23,990
at to define it we have a big data

00:05:21,060 --> 00:05:27,180
problem so how do we store the data

00:05:23,990 --> 00:05:29,190
again in other questions that was asked

00:05:27,180 --> 00:05:33,180
very often because when i came to

00:05:29,190 --> 00:05:36,060
Hortonworks I you know the sales pitch

00:05:33,180 --> 00:05:37,170
was Hadoop is so great just dump the

00:05:36,060 --> 00:05:39,600
data we provide the distribution

00:05:37,170 --> 00:05:42,380
technology and the MapReduce will take

00:05:39,600 --> 00:05:44,310
care of everything well you know

00:05:42,380 --> 00:05:46,410
MapReduce can't take care of a lot of

00:05:44,310 --> 00:05:48,560
things but at the end of the day it's

00:05:46,410 --> 00:05:51,510
nothing more than just processing

00:05:48,560 --> 00:05:53,100
individual parts of the data in a

00:05:51,510 --> 00:05:54,900
distributed environment and if that if

00:05:53,100 --> 00:05:57,960
such data is not written in the

00:05:54,900 --> 00:06:00,570
efficient way then it's only a matter of

00:05:57,960 --> 00:06:02,730
time before you're distributed

00:06:00,570 --> 00:06:06,780
environment will perform as a single

00:06:02,730 --> 00:06:11,100
thread on the cheap laptop right so how

00:06:06,780 --> 00:06:15,570
do we access the data you know blind

00:06:11,100 --> 00:06:16,860
scanning everything not necessarily all

00:06:15,570 --> 00:06:20,570
that efficient I actually have an

00:06:16,860 --> 00:06:23,580
interesting story on my wife and I flew

00:06:20,570 --> 00:06:26,910
last year from Europe back home to the

00:06:23,580 --> 00:06:29,250
United States and I actually had to go

00:06:26,910 --> 00:06:30,960
straight on the business trip so I never

00:06:29,250 --> 00:06:32,200
even made it home so we just landed in

00:06:30,960 --> 00:06:35,800
the Philadelphia and I

00:06:32,200 --> 00:06:37,780
to board another flight so I gave it our

00:06:35,800 --> 00:06:40,000
passports so she went on the car and

00:06:37,780 --> 00:06:41,650
left so obviously in the state's I don't

00:06:40,000 --> 00:06:43,120
have to use passport to fly around I can

00:06:41,650 --> 00:06:44,620
use my driver license but all of a

00:06:43,120 --> 00:06:46,510
sudden I'm checking on my pockets I

00:06:44,620 --> 00:06:48,910
could not find my driver license so I

00:06:46,510 --> 00:06:51,070
can get on the flight so I'm looking

00:06:48,910 --> 00:06:53,620
around everywhere and the question I'm

00:06:51,070 --> 00:06:57,520
asking myself have I lost it or haven't

00:06:53,620 --> 00:06:59,140
misplaced it because if I lost it if I

00:06:57,520 --> 00:07:00,340
knew that I lost it then I would have

00:06:59,140 --> 00:07:01,870
caught her back she would make a loop

00:07:00,340 --> 00:07:04,150
come back give me my passport everything

00:07:01,870 --> 00:07:06,550
would be fine right but if I misplaced

00:07:04,150 --> 00:07:09,010
it well I don't know what I did or

00:07:06,550 --> 00:07:11,230
didn't right so and that's kind of

00:07:09,010 --> 00:07:13,210
gaming an interesting idea which is

00:07:11,230 --> 00:07:16,330
actually very related to the data access

00:07:13,210 --> 00:07:19,150
how how cool would it was every time we

00:07:16,330 --> 00:07:21,960
look for something we may find it we may

00:07:19,150 --> 00:07:25,090
not but wouldn't be easier for us to

00:07:21,960 --> 00:07:27,880
store data organized data in such way

00:07:25,090 --> 00:07:30,640
where the questions such as it's not

00:07:27,880 --> 00:07:34,660
there or answered before the questions

00:07:30,640 --> 00:07:37,660
that it is there and where it is think

00:07:34,660 --> 00:07:39,460
about that so and then you can bring a

00:07:37,660 --> 00:07:41,800
third-party technology to make sense of

00:07:39,460 --> 00:07:44,110
the data that you're dealing with and a

00:07:41,800 --> 00:07:47,200
big presence here from elasticsearch and

00:07:44,110 --> 00:07:50,680
I actually talked to one of the guys

00:07:47,200 --> 00:07:51,610
today the interesting thing is that what

00:07:50,680 --> 00:07:52,630
I'm going to be showing you what I'm

00:07:51,610 --> 00:07:56,040
going to be talking about not

00:07:52,630 --> 00:07:58,270
necessarily contradict some of these

00:07:56,040 --> 00:07:59,830
indexing and search algorithm that

00:07:58,270 --> 00:08:01,180
provided by elastic search another

00:07:59,830 --> 00:08:03,370
company it's actually more of a

00:08:01,180 --> 00:08:05,770
compliment thing because at the end of

00:08:03,370 --> 00:08:07,660
the day you can put elastic search you

00:08:05,770 --> 00:08:09,820
can put other technologies on top of

00:08:07,660 --> 00:08:13,030
your data but if your data is stored in

00:08:09,820 --> 00:08:16,780
a very inefficient way at the end of the

00:08:13,030 --> 00:08:21,340
day your data access and data analysis

00:08:16,780 --> 00:08:23,200
will be affected so one thing is for

00:08:21,340 --> 00:08:26,170
sure the data will live somewhere for us

00:08:23,200 --> 00:08:28,420
to use so historically big data problem

00:08:26,170 --> 00:08:30,130
was defined by its size and to speed up

00:08:28,420 --> 00:08:32,740
the processing hadoop gave us two things

00:08:30,130 --> 00:08:34,360
it gave us a distributed file system and

00:08:32,740 --> 00:08:39,580
it gave us the distributed computation

00:08:34,360 --> 00:08:42,220
frameworks such as MapReduce so um you

00:08:39,580 --> 00:08:43,930
know the old moto is this you know we

00:08:42,220 --> 00:08:44,870
distribute the data we also give you the

00:08:43,930 --> 00:08:47,630
frame up to

00:08:44,870 --> 00:08:50,060
bring your coat to data and that's kind

00:08:47,630 --> 00:08:52,580
of the opposite of what we sort of used

00:08:50,060 --> 00:08:54,260
to or being used at a time where you

00:08:52,580 --> 00:08:55,790
know the data could be spread around and

00:08:54,260 --> 00:08:58,550
we bring it to a centralized processing

00:08:55,790 --> 00:09:02,060
so kind of innovative but was done

00:08:58,550 --> 00:09:04,550
before many times but I found actually a

00:09:02,060 --> 00:09:05,900
problem with that approach the problem

00:09:04,550 --> 00:09:07,460
is not necessarily with the pro which

00:09:05,900 --> 00:09:10,880
itself but rather with the fact that

00:09:07,460 --> 00:09:16,400
there is a kind of a compute versus I yo

00:09:10,880 --> 00:09:18,230
a mismatch right because you know blocks

00:09:16,400 --> 00:09:20,150
we have blocks in Hadoop then we're good

00:09:18,230 --> 00:09:22,040
the blocks you know we got into splits

00:09:20,150 --> 00:09:23,779
then input formats reading dis plates

00:09:22,040 --> 00:09:26,720
and then passing the units of data for

00:09:23,779 --> 00:09:28,550
processing to the MapReduce task right

00:09:26,720 --> 00:09:30,800
if you think about it these are two

00:09:28,550 --> 00:09:33,290
orthogonal problems speed of serving the

00:09:30,800 --> 00:09:34,880
unit of data right basically reading it

00:09:33,290 --> 00:09:37,279
from the disk and then speed of

00:09:34,880 --> 00:09:39,500
processing and if there is a mismatch

00:09:37,279 --> 00:09:43,550
between the speed of one over the other

00:09:39,500 --> 00:09:45,770
then you can actually have a problem

00:09:43,550 --> 00:09:48,140
because you know we always very nice to

00:09:45,770 --> 00:09:50,330
live in the world where let's just get

00:09:48,140 --> 00:09:53,000
more hardware but are we using in this

00:09:50,330 --> 00:09:56,720
is where that we go back to mechanical

00:09:53,000 --> 00:10:01,480
sympathy are we using our existing

00:09:56,720 --> 00:10:04,130
hardware efficiently to do things so um

00:10:01,480 --> 00:10:06,290
how do we solve mismatch and this is

00:10:04,130 --> 00:10:09,680
where we get to this very simple things

00:10:06,290 --> 00:10:11,240
however we tend to forget about them a

00:10:09,680 --> 00:10:12,680
lot especially when dealing with the

00:10:11,240 --> 00:10:15,709
high level of frameworks such as may

00:10:12,680 --> 00:10:17,420
produce and Hadoop in general so custom

00:10:15,709 --> 00:10:19,160
data buffering in fact one of the first

00:10:17,420 --> 00:10:21,500
samples i'm going to show you it kind of

00:10:19,160 --> 00:10:24,410
deals with that data encoding and

00:10:21,500 --> 00:10:25,940
compression so generalization of roses

00:10:24,410 --> 00:10:31,220
specialization how many of you use

00:10:25,940 --> 00:10:36,620
google protocol buffers all right do you

00:10:31,220 --> 00:10:38,900
like it I do but certain things I've

00:10:36,620 --> 00:10:40,940
learned that could be done much much

00:10:38,900 --> 00:10:44,360
better and you're going to see it today

00:10:40,940 --> 00:10:46,520
so data organization pages and efficient

00:10:44,360 --> 00:10:47,870
page creation meta information about

00:10:46,520 --> 00:10:53,060
your data this is actually very

00:10:47,870 --> 00:10:55,190
interesting a point you guys familiar

00:10:53,060 --> 00:10:57,440
with compression right so let's say

00:10:55,190 --> 00:10:58,700
you've you ingesting the data you're

00:10:57,440 --> 00:11:00,500
storing it and you decided

00:10:58,700 --> 00:11:02,390
save space or whatever other reason you

00:11:00,500 --> 00:11:04,640
decided to compress the data and then

00:11:02,390 --> 00:11:08,000
your analysts asked you how many records

00:11:04,640 --> 00:11:13,490
do we have how would you answer the

00:11:08,000 --> 00:11:16,610
question all you have to unzip the file

00:11:13,490 --> 00:11:21,080
and do a line count right just to answer

00:11:16,610 --> 00:11:22,730
that one question some of the other

00:11:21,080 --> 00:11:25,190
information about the data for example

00:11:22,730 --> 00:11:27,230
how when it was written so and so forth

00:11:25,190 --> 00:11:29,780
so a lot of this meta information is

00:11:27,230 --> 00:11:31,910
being completely missed the irony of it

00:11:29,780 --> 00:11:33,890
is during the actual capture of the data

00:11:31,910 --> 00:11:35,750
that information was available to you

00:11:33,890 --> 00:11:37,910
for free whether you capturing one

00:11:35,750 --> 00:11:40,010
record at a time from socket you can

00:11:37,910 --> 00:11:43,130
maintain a counter whether you're just

00:11:40,010 --> 00:11:45,020
reading or copying the entire file all

00:11:43,130 --> 00:11:50,170
that information was available to you

00:11:45,020 --> 00:11:52,940
during the capture and in fact if you if

00:11:50,170 --> 00:11:54,620
well if you're using a custom code a lot

00:11:52,940 --> 00:11:55,970
of times you maintain those counters for

00:11:54,620 --> 00:11:59,360
other reasons and then you completely

00:11:55,970 --> 00:12:02,540
dismiss their values right so page

00:11:59,360 --> 00:12:04,310
caching a lot of times you know when you

00:12:02,540 --> 00:12:07,150
you're familiar with a page caching when

00:12:04,310 --> 00:12:09,320
you read the data you know the way the

00:12:07,150 --> 00:12:11,630
data is right into the page cache and

00:12:09,320 --> 00:12:12,980
then essentially when you let's say try

00:12:11,630 --> 00:12:15,020
to read it again all of a sudden becomes

00:12:12,980 --> 00:12:16,310
much faster but that's because it's not

00:12:15,020 --> 00:12:19,220
giving to you from the disk but rather

00:12:16,310 --> 00:12:21,440
from the page cache so and another thing

00:12:19,220 --> 00:12:24,070
is a data sampling which I had two

00:12:21,440 --> 00:12:27,710
clients to work with and they wanted to

00:12:24,070 --> 00:12:29,840
you know talk about do a simple sample

00:12:27,710 --> 00:12:31,940
data right so the sample data means that

00:12:29,840 --> 00:12:34,220
let's say they capture 24 hours worth of

00:12:31,940 --> 00:12:36,800
data for something and they wanted to

00:12:34,220 --> 00:12:40,160
get ten percent of it but equally

00:12:36,800 --> 00:12:42,140
representing you know evenly distributed

00:12:40,160 --> 00:12:43,970
so I it's not so it means that I cannot

00:12:42,140 --> 00:12:46,310
just take the first attempt the first

00:12:43,970 --> 00:12:47,960
block of Records and say up to the ten

00:12:46,310 --> 00:12:49,880
percent or the last block of Records up

00:12:47,960 --> 00:12:53,090
to the ten percent so i have to equally

00:12:49,880 --> 00:12:55,100
distribute it right but which I can't do

00:12:53,090 --> 00:12:57,650
you know count how many records and then

00:12:55,100 --> 00:13:01,610
divide by 10 and then skip so many right

00:12:57,650 --> 00:13:03,020
it's doable but if you are already know

00:13:01,610 --> 00:13:05,120
for example that you're going to be

00:13:03,020 --> 00:13:07,040
doing certain operations on this data

00:13:05,120 --> 00:13:09,590
then certain things could be taken care

00:13:07,040 --> 00:13:11,570
of in advance and for example with data

00:13:09,590 --> 00:13:12,680
sampling all with data set

00:13:11,570 --> 00:13:14,900
how you going to sample a ten percent

00:13:12,680 --> 00:13:16,850
then let's just do one two three four in

00:13:14,900 --> 00:13:18,380
the round robin and you know right ten

00:13:16,850 --> 00:13:21,020
different files representing the same

00:13:18,380 --> 00:13:24,200
data not the solution that will fit

00:13:21,020 --> 00:13:26,660
every need but again to the custom

00:13:24,200 --> 00:13:28,880
requirements that was a very easy way to

00:13:26,660 --> 00:13:31,640
fix it now they have ten chunks

00:13:28,880 --> 00:13:36,290
representing actually equal distribution

00:13:31,640 --> 00:13:38,270
of data so efficient input formats

00:13:36,290 --> 00:13:43,100
that's another interesting thing when it

00:13:38,270 --> 00:13:46,160
comes to i/o how many of you use the

00:13:43,100 --> 00:13:49,010
word count from Hadoop examples all

00:13:46,160 --> 00:13:50,810
right that's not how you do word count I

00:13:49,010 --> 00:13:52,700
mean that's the most inefficient way of

00:13:50,810 --> 00:13:54,950
doing it in fact the first example will

00:13:52,700 --> 00:13:57,440
kind of demonstrate some of the issues

00:13:54,950 --> 00:14:00,670
with that so arm here is the mechanical

00:13:57,440 --> 00:14:02,840
sympathy slide which is basically I

00:14:00,670 --> 00:14:04,340
provided my own definition which is an

00:14:02,840 --> 00:14:06,050
ability to write software with a deep

00:14:04,340 --> 00:14:08,180
understanding of its impact or lack of

00:14:06,050 --> 00:14:11,270
impact on the hardware it's running on

00:14:08,180 --> 00:14:12,440
here's a link to the blog and the bottom

00:14:11,270 --> 00:14:14,210
line is you want to ensure that all

00:14:12,440 --> 00:14:16,220
available resources hardware and

00:14:14,210 --> 00:14:19,970
software working balance to help achieve

00:14:16,220 --> 00:14:21,920
your end goal and in this particular

00:14:19,970 --> 00:14:23,660
case it all starts from a data capture

00:14:21,920 --> 00:14:25,670
in fact I'm not going to spend too much

00:14:23,660 --> 00:14:30,760
time giving the lack of time that we

00:14:25,670 --> 00:14:34,340
have here but i did a series of talks on

00:14:30,760 --> 00:14:35,720
fast ingest last year and one of them

00:14:34,340 --> 00:14:38,660
was a hadoop summit another one was

00:14:35,720 --> 00:14:42,170
looking for in the q con so you can get

00:14:38,660 --> 00:14:45,680
it from there and i'll give it with

00:14:42,170 --> 00:14:48,830
history of why this dog came about and

00:14:45,680 --> 00:14:51,140
although the history goes back to my

00:14:48,830 --> 00:14:53,150
original talk but and this one is sort

00:14:51,140 --> 00:14:54,890
of a derivative but let's quickly talk

00:14:53,150 --> 00:14:56,390
about the problem and why all of a

00:14:54,890 --> 00:14:59,260
sudden some of the existing Hadoop

00:14:56,390 --> 00:15:02,120
technologies couldn't help me so um I

00:14:59,260 --> 00:15:04,520
was my first client when I came to

00:15:02,120 --> 00:15:06,680
hortonworks worth one of the biggest or

00:15:04,520 --> 00:15:09,350
if not the biggest wireless provider in

00:15:06,680 --> 00:15:13,730
the US struts with the name a you can

00:15:09,350 --> 00:15:15,470
guess what the rest of it is sorry i

00:15:13,730 --> 00:15:16,970
can't look funny things that you can't

00:15:15,470 --> 00:15:20,690
really mention but you can mention what

00:15:16,970 --> 00:15:22,190
it is anyway so what the problem was

00:15:20,690 --> 00:15:23,120
kind of described here but let me just

00:15:22,190 --> 00:15:26,810
kind of

00:15:23,120 --> 00:15:29,660
doing my own words so they had a devices

00:15:26,810 --> 00:15:32,290
that were producing data records you

00:15:29,660 --> 00:15:36,589
know the ones that NSA likes to look at

00:15:32,290 --> 00:15:39,020
so and each device would produce

00:15:36,589 --> 00:15:42,080
anywhere between the number here says

00:15:39,020 --> 00:15:43,880
300,000 actually between 200,000 but it

00:15:42,080 --> 00:15:46,070
could double or triple during the spikes

00:15:43,880 --> 00:15:49,820
right so that's why I was kind of

00:15:46,070 --> 00:15:53,330
averaging out between 300 300 thousand

00:15:49,820 --> 00:15:55,910
to 400,000 so that's per second each

00:15:53,330 --> 00:16:00,830
record was about 250 bytes in length and

00:15:55,910 --> 00:16:04,910
they had to for the first implementation

00:16:00,830 --> 00:16:07,160
handle 52 devices so you can do the math

00:16:04,910 --> 00:16:10,220
and realize how much data we had to

00:16:07,160 --> 00:16:13,010
ingest so when we try to use floom floom

00:16:10,220 --> 00:16:15,650
even flume ng couldn't really handle

00:16:13,010 --> 00:16:17,060
that volume of data with a hardware that

00:16:15,650 --> 00:16:20,330
was provisioned for that particular

00:16:17,060 --> 00:16:22,850
system so we had to improvise and we've

00:16:20,330 --> 00:16:24,680
succeeded we actually got on our ec2

00:16:22,850 --> 00:16:27,230
cluster successful test where we were

00:16:24,680 --> 00:16:29,540
ingesting 10 million of these records

00:16:27,230 --> 00:16:31,790
per second for a continuous period of 24

00:16:29,540 --> 00:16:35,089
hours course the company a lot of money

00:16:31,790 --> 00:16:39,050
but at least we knew it's doable so and

00:16:35,089 --> 00:16:41,750
that's really when I start one and again

00:16:39,050 --> 00:16:43,970
the the way we did it and it's all

00:16:41,750 --> 00:16:46,220
described in that talk is through a very

00:16:43,970 --> 00:16:47,720
custom approach right but that's what I

00:16:46,220 --> 00:16:50,540
that's the whole point of this

00:16:47,720 --> 00:16:52,040
conversation is that a lot of times you

00:16:50,540 --> 00:16:54,260
have products that will solve certain

00:16:52,040 --> 00:16:57,110
problems and those are general

00:16:54,260 --> 00:16:59,060
technologies and frameworks right just

00:16:57,110 --> 00:17:01,430
like Google protocol buffers but then

00:16:59,060 --> 00:17:04,250
there is certain things when you kind of

00:17:01,430 --> 00:17:06,620
narrow down your domain could be done

00:17:04,250 --> 00:17:09,740
much more efficient much better much

00:17:06,620 --> 00:17:15,069
faster so we're pretty much approaching

00:17:09,740 --> 00:17:19,490
the demo part so any questions so far

00:17:15,069 --> 00:17:20,990
all right feel free to you know raise

00:17:19,490 --> 00:17:22,959
your hand if you want to I don't want to

00:17:20,990 --> 00:17:24,679
really wait till the very end because

00:17:22,959 --> 00:17:29,559
you know we're not going to have enough

00:17:24,679 --> 00:17:34,270
time anyway so here's my four Musketeers

00:17:29,559 --> 00:17:37,510
the CPU the network the disk

00:17:34,270 --> 00:17:41,260
and the memory right so these are the

00:17:37,510 --> 00:17:46,120
four core resources of a computer right

00:17:41,260 --> 00:17:48,660
and the speed of development or

00:17:46,120 --> 00:17:53,260
evolution is not the same we might have

00:17:48,660 --> 00:17:55,270
you know fast disk but slow processor or

00:17:53,260 --> 00:17:56,740
maybe a processor is fast but what we're

00:17:55,270 --> 00:17:59,740
trying to do with this needed requires

00:17:56,740 --> 00:18:01,570
more power and so on and so forth the

00:17:59,740 --> 00:18:04,120
same goes for the memory and other

00:18:01,570 --> 00:18:06,130
things so in other words what I always

00:18:04,120 --> 00:18:07,780
wanted to be able to do is to say listen

00:18:06,130 --> 00:18:10,240
if I'm going to go to my company and say

00:18:07,780 --> 00:18:12,460
I really need more hardware I want to be

00:18:10,240 --> 00:18:14,620
able to prove and measure that what i

00:18:12,460 --> 00:18:17,350
have is used up to the limit that i

00:18:14,620 --> 00:18:19,480
cannot squeeze anything more out of my

00:18:17,350 --> 00:18:21,730
hard work and in fact on that

00:18:19,480 --> 00:18:23,890
presentation i'm showing some of the

00:18:21,730 --> 00:18:27,790
things where I'm doing the ingest and

00:18:23,890 --> 00:18:30,730
I'm choking but my cpu is relaxing it's

00:18:27,790 --> 00:18:33,160
only operating at about four percent why

00:18:30,730 --> 00:18:35,860
is that well because you know i started

00:18:33,160 --> 00:18:38,800
my demo written in such way where I was

00:18:35,860 --> 00:18:40,450
really I Oh bound so I had to start

00:18:38,800 --> 00:18:42,460
improvising doing various different

00:18:40,450 --> 00:18:44,110
things and all of a sudden i bringed it

00:18:42,460 --> 00:18:47,890
brought it to balance where my memory

00:18:44,110 --> 00:18:49,540
CPU and and I yo was working in tact and

00:18:47,890 --> 00:18:52,390
essentially speed up the processing but

00:18:49,540 --> 00:18:53,980
about five times so and that's

00:18:52,390 --> 00:18:57,220
essentially what we wanted to do want to

00:18:53,980 --> 00:19:00,580
be able to understand what our resources

00:18:57,220 --> 00:19:02,740
are doing to address all these problems

00:19:00,580 --> 00:19:06,100
so in other words while the topic is

00:19:02,740 --> 00:19:08,920
title io or big big data i/o or whatever

00:19:06,100 --> 00:19:11,080
it is titled right now at the end of the

00:19:08,920 --> 00:19:14,250
day it's really more about understanding

00:19:11,080 --> 00:19:19,000
how your resources are utilized during

00:19:14,250 --> 00:19:20,320
big data processing so um for example I

00:19:19,000 --> 00:19:21,550
mean like I said everything kind of

00:19:20,320 --> 00:19:23,710
starts with the capture of the data

00:19:21,550 --> 00:19:25,930
because how you capture the data will

00:19:23,710 --> 00:19:27,400
affect how everything else is will

00:19:25,930 --> 00:19:31,650
affect everything else down stream data

00:19:27,400 --> 00:19:34,300
access searches and what have you so and

00:19:31,650 --> 00:19:35,860
you know bond of things when it comes to

00:19:34,300 --> 00:19:39,070
data capture especially when it comes to

00:19:35,860 --> 00:19:41,190
streaming data capture you can never be

00:19:39,070 --> 00:19:43,270
slower than the data source right

00:19:41,190 --> 00:19:45,550
imagine what would happen if the data

00:19:43,270 --> 00:19:46,519
source producing data faster than you

00:19:45,550 --> 00:19:50,700
can

00:19:46,519 --> 00:19:52,890
I'm going to have a big problem right so

00:19:50,700 --> 00:19:54,630
like I said else is not good you always

00:19:52,890 --> 00:19:56,340
have to be kind of come up with an

00:19:54,630 --> 00:19:58,260
approach and you don't want to be at the

00:19:56,340 --> 00:20:00,049
limit you want to be by several orders

00:19:58,260 --> 00:20:03,090
of magnitude because like i said before

00:20:00,049 --> 00:20:05,399
we had to deal with spikes right

00:20:03,090 --> 00:20:06,779
sometimes you know it could be a natural

00:20:05,399 --> 00:20:08,220
spike because of the holiday or

00:20:06,779 --> 00:20:09,779
something like this or because you know

00:20:08,220 --> 00:20:11,789
something going on in this big

00:20:09,779 --> 00:20:13,860
metropolitan area it could be unnatural

00:20:11,789 --> 00:20:16,260
spikes because a certain device was down

00:20:13,860 --> 00:20:18,809
in itself accumulated the back like now

00:20:16,260 --> 00:20:21,149
it's throws everything at you right so

00:20:18,809 --> 00:20:24,990
you want to be able to handle that so

00:20:21,149 --> 00:20:27,539
and this is where we get to the code

00:20:24,990 --> 00:20:32,279
part which I'll try to spend the rest of

00:20:27,539 --> 00:20:35,190
the time is doing coding so these demos

00:20:32,279 --> 00:20:38,429
are all sort of a very trivial kind of a

00:20:35,190 --> 00:20:42,000
simple demos and the whole purpose was

00:20:38,429 --> 00:20:44,159
to kind of a draw awareness that simple

00:20:42,000 --> 00:20:46,730
things that we sort of in the Big Data

00:20:44,159 --> 00:20:51,630
world tend to forget are still relevant

00:20:46,730 --> 00:20:53,940
right and you know even when you're

00:20:51,630 --> 00:20:57,320
dealing with the third party technology

00:20:53,940 --> 00:21:00,779
in your shop you kind of have to

00:20:57,320 --> 00:21:03,029
understand what it does how it does it

00:21:00,779 --> 00:21:06,360
and if it's going to fit if it's going

00:21:03,029 --> 00:21:08,549
to fit your need so remember I said that

00:21:06,360 --> 00:21:11,510
Ward count is not how you do things

00:21:08,549 --> 00:21:14,460
right so here's a very simple demo

00:21:11,510 --> 00:21:16,590
basically what I'm doing here I have a

00:21:14,460 --> 00:21:18,750
test file i'm reading just a thousand

00:21:16,590 --> 00:21:21,409
records or ten thousand records from it

00:21:18,750 --> 00:21:24,360
loading it up in memory and basically

00:21:21,409 --> 00:21:26,789
doing a regular expression search on

00:21:24,360 --> 00:21:28,500
that file one on that what I can relate

00:21:26,789 --> 00:21:31,200
is so now I have a data in memory it's

00:21:28,500 --> 00:21:33,870
not even IL it's all in memory but the

00:21:31,200 --> 00:21:36,450
first attempt I'm doing one line at a

00:21:33,870 --> 00:21:39,029
time right the second attempt I'm doing

00:21:36,450 --> 00:21:40,559
it in badge so let me run it a real

00:21:39,029 --> 00:21:43,110
quick and we can set if you interested

00:21:40,559 --> 00:21:47,220
we can look at the code more deeper so

00:21:43,110 --> 00:21:48,899
it ran so here is the first set of

00:21:47,220 --> 00:21:51,659
results I do it like ten times just to

00:21:48,899 --> 00:21:54,659
get a good figure so it did get a little

00:21:51,659 --> 00:21:58,220
faster that's one line at a time but

00:21:54,659 --> 00:21:58,220
look at processing the same

00:21:58,640 --> 00:22:04,250
accumulation the same buffer but doing

00:22:01,310 --> 00:22:06,260
it as a whole the entire chunk we

00:22:04,250 --> 00:22:10,100
basically improve our performance by

00:22:06,260 --> 00:22:13,760
about five times right just a very

00:22:10,100 --> 00:22:15,740
simple thing that could in the city if

00:22:13,760 --> 00:22:16,910
you remember the word count in word

00:22:15,740 --> 00:22:21,830
count one of the things that they do

00:22:16,910 --> 00:22:24,620
they the reader passes you one record at

00:22:21,830 --> 00:22:26,330
a time right and I understand if the

00:22:24,620 --> 00:22:28,220
record is huge couple kilobytes or

00:22:26,330 --> 00:22:30,050
something like that but in my case for

00:22:28,220 --> 00:22:32,770
example when I was dealing with the 250

00:22:30,050 --> 00:22:32,770
bytes worth of

00:22:41,730 --> 00:22:44,360
look

00:23:03,930 --> 00:23:08,860
spin off the entire map task just the

00:23:07,060 --> 00:23:12,670
process one record to do string

00:23:08,860 --> 00:23:16,870
tokenization and searches for searches

00:23:12,670 --> 00:23:19,540
for words right so if if you simply were

00:23:16,870 --> 00:23:21,520
to write at a different input file

00:23:19,540 --> 00:23:23,040
format different reader then you would

00:23:21,520 --> 00:23:26,050
end up in a situation where you would

00:23:23,040 --> 00:23:29,890
tremendously improve the performance of

00:23:26,050 --> 00:23:31,840
your of your MapReduce for this

00:23:29,890 --> 00:23:33,340
particular problem but like I said this

00:23:31,840 --> 00:23:36,070
is a trivial sample let's get into

00:23:33,340 --> 00:23:38,080
something more interesting so yesterday

00:23:36,070 --> 00:23:39,640
I actually wrote I have it had a

00:23:38,080 --> 00:23:41,860
different demo but I decided to make it

00:23:39,640 --> 00:23:44,320
a little more interesting so I have this

00:23:41,860 --> 00:23:48,280
unoptimized right and I have an

00:23:44,320 --> 00:23:50,440
optimized right so let me kind of give

00:23:48,280 --> 00:23:52,300
you a little more real estate here so if

00:23:50,440 --> 00:23:55,410
you look at optimize right and look at

00:23:52,300 --> 00:24:01,740
unoptimized write the code is identical

00:23:55,410 --> 00:24:01,740
right basically I am you know trying to

00:24:02,640 --> 00:24:07,990
what I'm doing here I'm basically i had

00:24:05,860 --> 00:24:12,190
this quote from Einstein and I'm just

00:24:07,990 --> 00:24:14,860
writing it to a file 100,000 times so

00:24:12,190 --> 00:24:17,050
creating a file files 100,000 records

00:24:14,860 --> 00:24:19,120
each right and I'm creating multiple

00:24:17,050 --> 00:24:22,120
files i'm creating a thousand files and

00:24:19,120 --> 00:24:25,240
i'm going to be doing it first with one

00:24:22,120 --> 00:24:27,220
thread and then we're going to see what

00:24:25,240 --> 00:24:29,050
the difference is between optimized

00:24:27,220 --> 00:24:30,850
versus on optimized and then we're going

00:24:29,050 --> 00:24:33,910
to increase the thread and see what our

00:24:30,850 --> 00:24:41,650
improvements are going to be so let's

00:24:33,910 --> 00:24:44,530
run unoptimized first and i expected to

00:24:41,650 --> 00:24:49,320
run in about 20 30 40 seconds something

00:24:44,530 --> 00:24:54,880
along those lines and while we waiting

00:24:49,320 --> 00:24:58,300
let's look at the code here and this is

00:24:54,880 --> 00:25:00,880
where this is pretty much all the code

00:24:58,300 --> 00:25:04,570
right this is actually the i/o task that

00:25:00,880 --> 00:25:07,660
does the actual right anybody I think

00:25:04,570 --> 00:25:09,780
it's done now we're still writing come

00:25:07,660 --> 00:25:09,780
on

00:25:19,140 --> 00:25:27,790
alright um here we go it took 42 seconds

00:25:23,250 --> 00:25:29,830
sometimes it's a guess my computer and I

00:25:27,790 --> 00:25:32,200
don't know why but sometimes it goes

00:25:29,830 --> 00:25:36,160
like the 30 seconds but now it's like 42

00:25:32,200 --> 00:25:38,710
seconds so fine anybody can look

00:25:36,160 --> 00:25:44,980
actually before we do that let's do the

00:25:38,710 --> 00:25:46,360
same with optimized like I said code is

00:25:44,980 --> 00:25:50,680
the same but the difference in the i/o

00:25:46,360 --> 00:25:54,250
task and so 42 seconds versus six

00:25:50,680 --> 00:25:56,080
seconds right so obviously the one is

00:25:54,250 --> 00:25:59,170
faster than the other one by about five

00:25:56,080 --> 00:26:03,520
six times right so can anybody spot the

00:25:59,170 --> 00:26:06,070
problem i mean i'm using buffered output

00:26:03,520 --> 00:26:09,430
stream i am you know kind of doing

00:26:06,070 --> 00:26:17,860
everything that I'm supposed to do when

00:26:09,430 --> 00:26:22,840
I'm writing the file all right can

00:26:17,860 --> 00:26:28,030
somebody spot the solution here it's

00:26:22,840 --> 00:26:31,000
optimized right now all right let me

00:26:28,030 --> 00:26:33,910
help you so let's look at this

00:26:31,000 --> 00:26:37,390
particular line of code I'm creating a

00:26:33,910 --> 00:26:41,290
new buffer output stream what's going on

00:26:37,390 --> 00:26:43,750
behind the scenes buffered out with

00:26:41,290 --> 00:26:45,490
stream by itself is just an object is

00:26:43,750 --> 00:26:47,710
easier to create but internally because

00:26:45,490 --> 00:26:50,380
it's buffered output stream it creates a

00:26:47,710 --> 00:26:54,990
buffer which means it allocates the

00:26:50,380 --> 00:26:54,990
memory what happens when I say close

00:26:55,170 --> 00:27:00,130
garbage collection right and then the

00:26:57,670 --> 00:27:03,130
next time I want to write another file

00:27:00,130 --> 00:27:07,030
it creates another buffer instead of

00:27:03,130 --> 00:27:10,300
possibly using or reusing the existing

00:27:07,030 --> 00:27:14,320
buffer right look at the optimized

00:27:10,300 --> 00:27:16,630
rights just that fact alone that I just

00:27:14,320 --> 00:27:19,800
explained to you i'm using a byte

00:27:16,630 --> 00:27:22,780
buffers how many of you using java niÃ±o

00:27:19,800 --> 00:27:24,940
package okay so you're familiar with a

00:27:22,780 --> 00:27:26,800
byte buffer so i'm using a byte buffer

00:27:24,940 --> 00:27:30,730
but what I'm also doing sure I'm

00:27:26,800 --> 00:27:32,200
allocating here right just like before

00:27:30,730 --> 00:27:33,549
but as you can see it's for the if

00:27:32,200 --> 00:27:35,530
statement which means I'm doing some

00:27:33,549 --> 00:27:38,530
type of caching and you can see I have a

00:27:35,530 --> 00:27:42,580
context object which is nothing more

00:27:38,530 --> 00:27:43,630
than a threadlocal because it's only for

00:27:42,580 --> 00:27:46,720
this thread i don't i'm going to have to

00:27:43,630 --> 00:27:49,059
synchronize it right so thread once as

00:27:46,720 --> 00:27:51,970
soon as a threat finishes by syncing the

00:27:49,059 --> 00:27:55,500
file for the file in output stream it

00:27:51,970 --> 00:27:58,900
releases that buffer that the next

00:27:55,500 --> 00:28:02,260
attempt on this thread can reuse the

00:27:58,900 --> 00:28:05,679
same buffer the same memory that was

00:28:02,260 --> 00:28:19,870
allocated before right so all of a

00:28:05,679 --> 00:28:21,280
sudden all of a sudden I've yes okay so

00:28:19,870 --> 00:28:24,940
i got the question so the question is

00:28:21,280 --> 00:28:27,100
that Emma locating the same size or and

00:28:24,940 --> 00:28:30,120
what happens if I have a variable length

00:28:27,100 --> 00:28:30,120
is a correct

00:28:40,850 --> 00:28:51,920
yeah it's AK well no so I could play

00:28:50,130 --> 00:28:54,330
though that ok the question is that a

00:28:51,920 --> 00:28:56,550
buffer output stream the default buffer

00:28:54,330 --> 00:28:58,320
size 8 k and you know I could definitely

00:28:56,550 --> 00:29:00,210
increase it but the bottom line is that

00:28:58,320 --> 00:29:02,370
even if I were to increase it I would

00:29:00,210 --> 00:29:04,230
have to as soon as I get as soon as I

00:29:02,370 --> 00:29:06,870
issue the clothes on the buffered output

00:29:04,230 --> 00:29:09,030
stream its and create a new buffer

00:29:06,870 --> 00:29:11,190
output stream the next the next cycle is

00:29:09,030 --> 00:29:12,840
going to locate a new memory right we're

00:29:11,190 --> 00:29:16,050
in this particular case I'm actually

00:29:12,840 --> 00:29:18,980
reusing the same buffer because now I'm

00:29:16,050 --> 00:29:21,360
managing the buffer separately from the

00:29:18,980 --> 00:29:23,910
actual input from the actual output

00:29:21,360 --> 00:29:25,680
stream so and then the other question I

00:29:23,910 --> 00:29:28,080
think that I kind of formulated myself

00:29:25,680 --> 00:29:29,610
in this case I kind of know the length

00:29:28,080 --> 00:29:31,920
of my record and I know that I'm doing

00:29:29,610 --> 00:29:33,930
it hundred thousand times so I've

00:29:31,920 --> 00:29:38,190
calculated the size of the buffer but i

00:29:33,930 --> 00:29:40,170
also have and implementation of byte

00:29:38,190 --> 00:29:42,090
buffer kind of extended version where

00:29:40,170 --> 00:29:43,470
you can create initial size and if it

00:29:42,090 --> 00:29:45,780
has to expand they will expand

00:29:43,470 --> 00:29:47,520
automatically by essentially copying

00:29:45,780 --> 00:29:49,710
itself to a bigger buffer and so on and

00:29:47,520 --> 00:29:51,660
so forth but again that's just in other

00:29:49,710 --> 00:29:53,220
ways of doing things it's kind of

00:29:51,660 --> 00:29:57,990
irrelevant to this particular topic so

00:29:53,220 --> 00:30:00,450
so anyway um you see how simple things

00:29:57,990 --> 00:30:04,560
like this could either fix your problem

00:30:00,450 --> 00:30:07,500
or get you into a problem right so but

00:30:04,560 --> 00:30:09,960
let's do something else let's try to

00:30:07,500 --> 00:30:12,150
increase the amount of threads right

00:30:09,960 --> 00:30:14,550
saying okay well I'm probably dealing

00:30:12,150 --> 00:30:18,030
with IO intensive task so if I bump the

00:30:14,550 --> 00:30:22,790
threads by four I should you know make

00:30:18,030 --> 00:30:25,200
it much faster so let's try that and

00:30:22,790 --> 00:30:26,940
again for the sake of saving time I

00:30:25,200 --> 00:30:28,020
might as well keep on talking and I'll

00:30:26,940 --> 00:30:32,460
tell you that it's not going to be that

00:30:28,020 --> 00:30:34,050
much faster why because there is certain

00:30:32,460 --> 00:30:36,270
things that are happening behind the

00:30:34,050 --> 00:30:39,060
scene that will never give you give you

00:30:36,270 --> 00:30:40,590
one plus one equals two so obviously I

00:30:39,060 --> 00:30:42,150
your resources you're still contending

00:30:40,590 --> 00:30:45,060
for the shared resource which is my

00:30:42,150 --> 00:30:48,130
single disk on this machine right there

00:30:45,060 --> 00:30:50,440
is still some string system.arraycopy

00:30:48,130 --> 00:30:53,770
on with an internal implementation which

00:30:50,440 --> 00:30:55,210
I actually learned the hard way that no

00:30:53,770 --> 00:30:56,920
matter how many threads you throw at it

00:30:55,210 --> 00:30:59,500
when you're dealing with native calls

00:30:56,920 --> 00:31:01,810
it's not always going to equate to even

00:30:59,500 --> 00:31:03,520
close to be one plus one equals two so

00:31:01,810 --> 00:31:06,310
as you can see we saved were three

00:31:03,520 --> 00:31:08,440
seconds right the same thing if I do an

00:31:06,310 --> 00:31:12,550
optimized actually will show us a better

00:31:08,440 --> 00:31:19,720
percentage which essentially shows you

00:31:12,550 --> 00:31:21,700
or proves to you that the memory

00:31:19,720 --> 00:31:23,980
allocation that we were requesting from

00:31:21,700 --> 00:31:26,550
our buffered output stream was quite a

00:31:23,980 --> 00:31:30,640
big problem for us so here we saved

00:31:26,550 --> 00:31:34,870
almost a half right not necessarily four

00:31:30,640 --> 00:31:37,150
times but almost a half and that's but

00:31:34,870 --> 00:31:39,010
in quite interesting so another thing

00:31:37,150 --> 00:31:41,800
that I was talking about and I think we

00:31:39,010 --> 00:31:43,090
all have ten minutes left unfortunately

00:31:41,800 --> 00:31:45,940
won't be able to show you everything but

00:31:43,090 --> 00:31:48,400
another thing that I was going to talk

00:31:45,940 --> 00:31:50,410
to you about is what I learned when I

00:31:48,400 --> 00:31:53,350
terms of data organization and data

00:31:50,410 --> 00:31:55,360
storage is that a lot of times like how

00:31:53,350 --> 00:31:57,820
many how many of you one book one was

00:31:55,360 --> 00:32:02,080
last time your accounting bits when it

00:31:57,820 --> 00:32:06,220
came to the data like bits not bytes

00:32:02,080 --> 00:32:08,260
bits okay so you think a little big data

00:32:06,220 --> 00:32:10,600
why would I care about bits well think

00:32:08,260 --> 00:32:12,310
about that way most of the day the

00:32:10,600 --> 00:32:14,080
business data or machine data is going

00:32:12,310 --> 00:32:16,690
to be in ASCII characters I know I'm in

00:32:14,080 --> 00:32:18,820
Germany so I can be really you know but

00:32:16,690 --> 00:32:20,860
you know most of the machine data will

00:32:18,820 --> 00:32:22,930
deal with the ASCII characters how many

00:32:20,860 --> 00:32:28,600
bits do I really need to store any ascii

00:32:22,930 --> 00:32:32,110
character seven right so what is one bit

00:32:28,600 --> 00:32:36,430
or 48 bits like twelve percent right so

00:32:32,110 --> 00:32:38,050
that fact alone could actually save you

00:32:36,430 --> 00:32:40,420
a lot of storage and obviously improve

00:32:38,050 --> 00:32:42,580
your because you're now 12 times smaller

00:32:40,420 --> 00:32:44,290
but what I also learned that most of the

00:32:42,580 --> 00:32:46,570
data most of the characters that are

00:32:44,290 --> 00:32:48,250
used or majority of them could be storm

00:32:46,570 --> 00:32:51,040
or between five and six and that

00:32:48,250 --> 00:32:54,000
actually gives you into a better

00:32:51,040 --> 00:32:56,080
situation now another thing is that

00:32:54,000 --> 00:32:58,620
might as well turn back to the slides

00:32:56,080 --> 00:32:58,620
real quick

00:32:59,929 --> 00:33:05,450
kind of skipping a little bit but data

00:33:03,110 --> 00:33:06,590
organization so for one of my clients we

00:33:05,450 --> 00:33:09,340
have to come up with a custom file

00:33:06,590 --> 00:33:11,869
format and this is kind of a pseudocode

00:33:09,340 --> 00:33:13,940
showing you what this file format looked

00:33:11,869 --> 00:33:15,769
like and I was trying to explain it I

00:33:13,940 --> 00:33:18,169
was trying before to explain it and then

00:33:15,769 --> 00:33:19,909
I decided I gotta put it in the slide so

00:33:18,169 --> 00:33:22,220
you can guys it's better to understand

00:33:19,909 --> 00:33:24,019
so here's kind of a sample record is it

00:33:22,220 --> 00:33:25,669
slimmed down but this is actually the

00:33:24,019 --> 00:33:27,710
records that I was dealing with from

00:33:25,669 --> 00:33:30,110
call detail records right this is what

00:33:27,710 --> 00:33:32,629
they kind of look like there's more to

00:33:30,110 --> 00:33:34,490
that to the right but anyway so and so

00:33:32,629 --> 00:33:36,499
that pro which is actually not new so

00:33:34,490 --> 00:33:38,269
think about this way when you look at

00:33:36,499 --> 00:33:39,200
this data when you analyze the data that

00:33:38,269 --> 00:33:41,779
you're dealing with and it's a very

00:33:39,200 --> 00:33:44,210
important topic you kind of understand

00:33:41,779 --> 00:33:45,470
that because machine data and because of

00:33:44,210 --> 00:33:47,690
what it represents there is going to be

00:33:45,470 --> 00:33:49,129
huge amount of repetition and you can

00:33:47,690 --> 00:33:51,830
say well fine if it's a huge amount of

00:33:49,129 --> 00:33:54,110
repetition I can compress it right well

00:33:51,830 --> 00:33:56,570
compression will give you certain

00:33:54,110 --> 00:33:58,700
benefit but again when it comes to data

00:33:56,570 --> 00:33:59,960
access you need to be able to you need

00:33:58,700 --> 00:34:02,029
to decompress it before you can start

00:33:59,960 --> 00:34:04,009
using it so what I start thinking about

00:34:02,029 --> 00:34:06,440
what if I come up with a different sort

00:34:04,009 --> 00:34:09,500
of a organization style which allows me

00:34:06,440 --> 00:34:11,419
to achieve close to the same ratio as

00:34:09,500 --> 00:34:14,260
compression or even beat the compression

00:34:11,419 --> 00:34:16,790
ratio but at the same time give me an

00:34:14,260 --> 00:34:20,629
uncompressed access to this data as a

00:34:16,790 --> 00:34:22,819
sort of custom binary file formats so a

00:34:20,629 --> 00:34:24,109
simple approach was to all those many

00:34:22,819 --> 00:34:25,429
different variations that I had

00:34:24,109 --> 00:34:27,500
throughout the year but one of the

00:34:25,429 --> 00:34:30,169
approach that I'm kind of like more than

00:34:27,500 --> 00:34:32,329
others is first of all you you you

00:34:30,169 --> 00:34:34,490
chunking the data into blocks in this

00:34:32,329 --> 00:34:36,589
case let's say 100,000 records per block

00:34:34,490 --> 00:34:38,450
right and for each block you're going to

00:34:36,589 --> 00:34:40,730
create a dictionary of what's in that

00:34:38,450 --> 00:34:42,679
block of values however you want to

00:34:40,730 --> 00:34:44,809
parse them right and then you're going

00:34:42,679 --> 00:34:46,819
to represent these values in other words

00:34:44,809 --> 00:34:48,440
this is your dictionary and in green you

00:34:46,819 --> 00:34:51,049
have basically the index just worried

00:34:48,440 --> 00:34:53,210
it's not written but it's a for you to

00:34:51,049 --> 00:34:55,099
read write and now I can actually write

00:34:53,210 --> 00:34:57,680
the same for records as array of

00:34:55,099 --> 00:34:59,690
integers or in a calendar format and

00:34:57,680 --> 00:35:01,790
when you do that actually when I was

00:34:59,690 --> 00:35:04,240
sort of playing with that I start

00:35:01,790 --> 00:35:06,650
noticing that these things for example

00:35:04,240 --> 00:35:08,569
here I have the same value this priority

00:35:06,650 --> 00:35:09,119
code I've haven't seen them changing it

00:35:08,569 --> 00:35:10,980
was kind of

00:35:09,119 --> 00:35:13,650
radically encoded there the the

00:35:10,980 --> 00:35:15,690
timestamp was constantly increasing but

00:35:13,650 --> 00:35:17,400
because they produce 100,000 records per

00:35:15,690 --> 00:35:18,779
second that means that it would be

00:35:17,400 --> 00:35:20,670
increasing you know I may have the

00:35:18,779 --> 00:35:23,309
entire batch that will either have one

00:35:20,670 --> 00:35:25,650
branch or will have the same because I

00:35:23,309 --> 00:35:27,329
have you know several hundred thousand

00:35:25,650 --> 00:35:29,970
records per second and then you have

00:35:27,329 --> 00:35:31,680
other values and so and so forth so the

00:35:29,970 --> 00:35:33,599
bouncing black in the random and so on

00:35:31,680 --> 00:35:35,460
and you know the the blue ones are same

00:35:33,599 --> 00:35:37,680
and the orange ones are range which is

00:35:35,460 --> 00:35:38,940
something that will change but will

00:35:37,680 --> 00:35:41,390
continue to be the same until the

00:35:38,940 --> 00:35:44,369
changes again so why am I saying that

00:35:41,390 --> 00:35:56,039
well here's the demo so remember I

00:35:44,369 --> 00:35:58,230
mentioned go ahead sure and well

00:35:56,039 --> 00:36:00,240
actually not necessarily because while

00:35:58,230 --> 00:36:01,980
you're you I agree with it it's

00:36:00,240 --> 00:36:04,200
something confessional glutens do

00:36:01,980 --> 00:36:06,029
compression algorithms some of them do

00:36:04,200 --> 00:36:07,529
some of them don't give me access

00:36:06,029 --> 00:36:09,299
because right now in order for me to

00:36:07,529 --> 00:36:10,859
read this data and that's what I was

00:36:09,299 --> 00:36:13,499
about to show you I don't have to

00:36:10,859 --> 00:36:24,749
decompress the file I could actually

00:36:13,499 --> 00:36:27,329
read the data as is right so no I want

00:36:24,749 --> 00:36:29,579
to compress everything so so for example

00:36:27,329 --> 00:36:31,230
the questions is it there or is it not

00:36:29,579 --> 00:36:32,849
could be answered by simply doing a look

00:36:31,230 --> 00:36:42,269
up in the dictionary without your

00:36:32,849 --> 00:36:43,700
compressing anything okay some

00:36:42,269 --> 00:36:50,430
compression algorithm do some don't

00:36:43,700 --> 00:36:51,599
right but again these were I'm sort of

00:36:50,430 --> 00:36:53,700
generalizing it for the purpose of

00:36:51,599 --> 00:36:55,200
discussion there was some specific

00:36:53,700 --> 00:36:57,059
requirements for example this and again

00:36:55,200 --> 00:36:59,609
I can't unfortunately go through all the

00:36:57,059 --> 00:37:01,650
damage they had specific requirement to

00:36:59,609 --> 00:37:04,140
access data by certain fields and there

00:37:01,650 --> 00:37:05,999
was other algorithms in place right but

00:37:04,140 --> 00:37:08,160
the bottom line is that you know I'm not

00:37:05,999 --> 00:37:11,099
saying that that that was the most

00:37:08,160 --> 00:37:14,069
perfect approach actually it's not

00:37:11,099 --> 00:37:16,410
because I'm working on something that

00:37:14,069 --> 00:37:17,999
makes it even smoother but the bottom

00:37:16,410 --> 00:37:20,430
line is that when you start thinking

00:37:17,999 --> 00:37:22,060
about that it really the what I'm trying

00:37:20,430 --> 00:37:23,830
to drive to is that

00:37:22,060 --> 00:37:25,840
start thinking about how to organize

00:37:23,830 --> 00:37:27,700
data more efficiently for the purpose of

00:37:25,840 --> 00:37:29,710
doing something with it whatever it is

00:37:27,700 --> 00:37:32,260
might be either use existing compression

00:37:29,710 --> 00:37:33,580
algorithm that is you know as genocide

00:37:32,260 --> 00:37:35,830
is splittable that you can read without

00:37:33,580 --> 00:37:39,190
decompressing or a lot of times you

00:37:35,830 --> 00:37:41,110
can't for variety of reasons and a lot

00:37:39,190 --> 00:37:42,490
of times coming up with this simple file

00:37:41,110 --> 00:37:43,840
formats they're not that difficult to

00:37:42,490 --> 00:37:45,520
implement or not that difficult to

00:37:43,840 --> 00:37:47,500
understand but let me show you for

00:37:45,520 --> 00:37:52,830
example so when I had that string array

00:37:47,500 --> 00:37:52,830
the column right so let me show you

00:37:54,360 --> 00:37:59,200
integer encoding them so um Google

00:37:57,580 --> 00:38:00,640
protocol buffers right so initially I

00:37:59,200 --> 00:38:03,330
start using google protocol buffers to

00:38:00,640 --> 00:38:06,250
encode my integers in the column so

00:38:03,330 --> 00:38:11,170
here's the protocol buffer and I'm going

00:38:06,250 --> 00:38:13,240
to do with it with the protocol buffer

00:38:11,170 --> 00:38:14,980
i'm basically encoding i'm generating a

00:38:13,240 --> 00:38:16,600
hundred thousand integers within the

00:38:14,980 --> 00:38:22,950
range randomly within the range of fifty

00:38:16,600 --> 00:38:22,950
thousand and we'll see from the output

00:38:24,420 --> 00:38:28,990
that our sort of packing compression

00:38:27,760 --> 00:38:33,430
ratio whatever it is you want to call it

00:38:28,990 --> 00:38:36,870
is you know one point 1 point 5 1.5

00:38:33,430 --> 00:38:42,940
basically so out of 400,000 possible

00:38:36,870 --> 00:38:45,790
values Google started Google protocol

00:38:42,940 --> 00:38:47,320
buffers store to this 266,000 bites

00:38:45,790 --> 00:38:51,060
right instead of four hundred thousand

00:38:47,320 --> 00:38:53,620
bytes now if I go to and run the same so

00:38:51,060 --> 00:38:55,710
but the thing is that what what I wanted

00:38:53,620 --> 00:38:58,180
to pay attention to you too is that

00:38:55,710 --> 00:39:01,000
while Google handles any type of

00:38:58,180 --> 00:39:03,490
integers right in my case since I'm

00:39:01,000 --> 00:39:05,560
storing offsets here I'm only dealing

00:39:03,490 --> 00:39:06,670
with positive numbers so when you're

00:39:05,560 --> 00:39:07,510
dealing with the positive numbers

00:39:06,670 --> 00:39:12,280
there's a lot of room for improvement

00:39:07,510 --> 00:39:14,460
and if you look at this demo then all of

00:39:12,280 --> 00:39:14,460
a sudden

00:39:21,130 --> 00:39:27,260
so we're almost to folks right almost

00:39:24,260 --> 00:39:28,580
double with a google right then what the

00:39:27,260 --> 00:39:31,520
Google did for this particular

00:39:28,580 --> 00:39:33,620
specialized use case well now let's do

00:39:31,520 --> 00:39:37,460
different things let's say like in my

00:39:33,620 --> 00:39:44,080
case where I had the same value right if

00:39:37,460 --> 00:39:44,080
I go with Google protocol buffers then I

00:39:46,390 --> 00:39:54,820
got two times improvement but if I do

00:39:49,880 --> 00:39:54,820
the same via my sort of a custom encoder

00:40:02,770 --> 00:40:06,730
you see the numbers right all of a

00:40:04,780 --> 00:40:09,310
sudden the entire what could be four

00:40:06,730 --> 00:40:12,460
hundred thousand bytes array restored as

00:40:09,310 --> 00:40:15,550
nine bites right and i have another sort

00:40:12,460 --> 00:40:25,840
of a sample where I'm kind of addressing

00:40:15,550 --> 00:40:28,320
the ranges of values so with the custom

00:40:25,840 --> 00:40:33,550
I got you know 19 bites out of the range

00:40:28,320 --> 00:40:38,140
and here i will have well i will have a

00:40:33,550 --> 00:40:44,710
bigger number so you had a question you

00:40:38,140 --> 00:40:46,480
had a question go ahead I mean I mean it

00:40:44,710 --> 00:40:48,910
unfortunately we are getting to the end

00:40:46,480 --> 00:40:51,370
of yeah I think we're on for you have 40

00:40:48,910 --> 00:41:01,270
minutes is but let me just do a quick

00:40:51,370 --> 00:41:04,780
conclusion so let me just throw I guess

00:41:01,270 --> 00:41:07,150
yeah there was only a few sort of

00:41:04,780 --> 00:41:11,410
conclusion slides left but I guess the

00:41:07,150 --> 00:41:14,050
idea is that there's a lot of things

00:41:11,410 --> 00:41:15,370
that you know products can handle or

00:41:14,050 --> 00:41:17,260
product doesn't handle all that

00:41:15,370 --> 00:41:19,240
efficiently and there's a lot of things

00:41:17,260 --> 00:41:22,450
that simple things that you can do

00:41:19,240 --> 00:41:24,130
yourself whether you meant in a custom

00:41:22,450 --> 00:41:26,080
component within a framework within a

00:41:24,130 --> 00:41:29,140
technology or whether it's a custom

00:41:26,080 --> 00:41:34,000
solution all together that would allow

00:41:29,140 --> 00:41:36,460
you to make your data simpler to use so

00:41:34,000 --> 00:41:38,740
i guess that's it thank you very much

00:41:36,460 --> 00:41:41,760
and I'll be around if you have asked and

00:41:38,740 --> 00:41:41,760
you guys have any questions

00:41:42,860 --> 00:41:44,920

YouTube URL: https://www.youtube.com/watch?v=fXCNz7ZKR-A


