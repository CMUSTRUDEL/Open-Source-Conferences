Title: Berlin Buzzwords 2014: Andrew Psaltis - Real-time Map Reduce: Exploring Clickstream Analytics ...
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Andrew Psaltis talking about "Real-time Map Reduce: Exploring Clickstream Analytics with Spark Streaming, Kafka and WebSockets"

Spark Streaming is an extension to Apache Spark that lets users seamlessly intermix streaming, batch and interactive queries through the use of a new programming model. Coupling this with strong consistency and efficient fault recovery, the opportunities to build robust streaming analytics systems is limited only by imagination. 

In this talk I'll show you how to practically use Apache Kafka to store clickstream data, Apache Spark Streaming to perform click stream analysis, and WebSockets to stream the results out to a client. 

Read more:
https://2014.berlinbuzzwords.de/session/real-time-map-reduce-exploring-clickstream-analytics-spark-streaming-kafka-and-websockets

About Andrew Psaltis:
https://2014.berlinbuzzwords.de/user/207/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,589 --> 00:00:10,389
hi my name is Andrew sultisz title it

00:00:08,540 --> 00:00:11,830
talks can be real-time MapReduce

00:00:10,389 --> 00:00:13,809
exploring

00:00:11,830 --> 00:00:17,770
analytics with Kafka spark streaming and

00:00:13,809 --> 00:00:20,050
web sockets kind of a mouthful supposed

00:00:17,770 --> 00:00:22,270
to stay a loop about myself currently

00:00:20,050 --> 00:00:25,000
working at ensighten on agile marketing

00:00:22,270 --> 00:00:27,130
platform as of three months ago the

00:00:25,000 --> 00:00:29,380
previous five almost five years worked

00:00:27,130 --> 00:00:31,660
at web trends on visit on the linux and

00:00:29,380 --> 00:00:33,070
streaming analytics platforms some is

00:00:31,660 --> 00:00:35,440
where I kind of first fell in love with

00:00:33,070 --> 00:00:36,820
spark and then spark streaming so some

00:00:35,440 --> 00:00:38,769
of the stuff we'll talk about relates to

00:00:36,820 --> 00:00:43,269
that experiencing kind of just using

00:00:38,769 --> 00:00:46,390
spark in spark sharing kind of where

00:00:43,269 --> 00:00:48,460
we're going is talk about spark briefly

00:00:46,390 --> 00:00:50,170
spark streaming more particular kind of

00:00:48,460 --> 00:00:51,850
how I fell in love with it give a brief

00:00:50,170 --> 00:00:53,799
overview of an architectural picture I

00:00:51,850 --> 00:00:55,930
have in mind of the streaming platform

00:00:53,799 --> 00:00:57,280
give a birds eye view of kafka really

00:00:55,930 --> 00:00:59,710
quick in case people just haven't used

00:00:57,280 --> 00:01:01,119
it discuss spark streaming in more

00:00:59,710 --> 00:01:03,850
detail and they kind of walk through

00:01:01,119 --> 00:01:05,229
some clickstream examples and then kind

00:01:03,850 --> 00:01:11,740
of discuss getting data out of spark

00:01:05,229 --> 00:01:13,170
streaming so I'll FL for spark em it's

00:01:11,740 --> 00:01:15,070
kind of like three worlds collided on

00:01:13,170 --> 00:01:16,510
october fourteenth when felix

00:01:15,070 --> 00:01:18,850
baumgartner jumped from the edge of

00:01:16,510 --> 00:01:20,770
space for red bull red bull happen to be

00:01:18,850 --> 00:01:23,650
a web trends customer it was the largest

00:01:20,770 --> 00:01:25,840
single hour of traffic collected for web

00:01:23,650 --> 00:01:27,190
trends its 15-year history just so

00:01:25,840 --> 00:01:28,450
happens that it also crash all the

00:01:27,190 --> 00:01:31,510
analytics machines trying to process

00:01:28,450 --> 00:01:34,840
that data at the same time around at the

00:01:31,510 --> 00:01:36,010
same time matei announced that spark was

00:01:34,840 --> 00:01:38,110
going to be standalone and no longer

00:01:36,010 --> 00:01:39,730
required mezzos so which was great for

00:01:38,110 --> 00:01:42,070
us because we had a Hadoop cluster

00:01:39,730 --> 00:01:44,080
didn't want to deploy mezzos and all of

00:01:42,070 --> 00:01:46,420
a sudden we had a problem that our

00:01:44,080 --> 00:01:48,100
production engines were crashing so we

00:01:46,420 --> 00:01:50,800
had a pretty big data set and we had a

00:01:48,100 --> 00:01:54,729
new toy to play with so we got spark

00:01:50,800 --> 00:01:57,040
installed locally got a local data set

00:01:54,729 --> 00:01:59,020
of the Red Bull data and started to play

00:01:57,040 --> 00:02:00,940
and realize that wow you could express

00:01:59,020 --> 00:02:02,650
things so easily we could actually

00:02:00,940 --> 00:02:05,380
answer questions that were crashing

00:02:02,650 --> 00:02:07,870
production and analytics tensions so

00:02:05,380 --> 00:02:09,729
from there just kind of continued and we

00:02:07,870 --> 00:02:12,069
start to pursue using spark for various

00:02:09,729 --> 00:02:15,489
things read the D streams paper and

00:02:12,069 --> 00:02:17,769
we're interested in then using spark

00:02:15,489 --> 00:02:19,540
streaming when that came out because we

00:02:17,769 --> 00:02:21,610
already running a storm cluster and

00:02:19,540 --> 00:02:25,800
already did some streaming analytics and

00:02:21,610 --> 00:02:25,800
want to see what this new thing could be

00:02:28,020 --> 00:02:32,340
so why spark streaming you know said we

00:02:30,670 --> 00:02:34,240
already had storm running in production

00:02:32,340 --> 00:02:35,680
at providing a streaming analytics

00:02:34,240 --> 00:02:37,450
solution for our customers really kind

00:02:35,680 --> 00:02:39,580
of event based so it's you know clicks

00:02:37,450 --> 00:02:41,410
coming in some light augmentation some

00:02:39,580 --> 00:02:43,540
light analytics and then data going out

00:02:41,410 --> 00:02:46,870
as fast as possible so pretty much from

00:02:43,540 --> 00:02:49,090
click to dashboard as quick as we can so

00:02:46,870 --> 00:02:50,860
we already have that yeah but we wanted

00:02:49,090 --> 00:02:52,660
to deliver an aggregate stream so we

00:02:50,860 --> 00:02:54,700
could do things like a rolling top end

00:02:52,660 --> 00:02:58,360
of pages or the top business on the side

00:02:54,700 --> 00:03:00,010
or the top countries or top browsers we

00:02:58,360 --> 00:03:02,410
wanted exactly once semantics to be able

00:03:00,010 --> 00:03:03,940
to do that we didn't want to risk double

00:03:02,410 --> 00:03:06,580
counting or have any issues like that

00:03:03,940 --> 00:03:09,850
and we were okay with things being at

00:03:06,580 --> 00:03:12,130
second scale latency we didn't need the

00:03:09,850 --> 00:03:13,570
storm speed of an event stream we're

00:03:12,130 --> 00:03:17,710
okay if things are delayed we want to

00:03:13,570 --> 00:03:19,930
see top pages say last 10 seconds last

00:03:17,710 --> 00:03:21,430
minute so things could be delayed it was

00:03:19,930 --> 00:03:24,640
just an aggregate of what's happening

00:03:21,430 --> 00:03:25,780
for your business at that time we also

00:03:24,640 --> 00:03:27,970
want to have state management for

00:03:25,780 --> 00:03:30,610
computations so if things did go south

00:03:27,970 --> 00:03:35,320
as they do we want to be able to recover

00:03:30,610 --> 00:03:37,810
and not lose our state and since we've

00:03:35,320 --> 00:03:39,130
been using spark and start a kind of

00:03:37,810 --> 00:03:41,080
fall in love with that and really

00:03:39,130 --> 00:03:43,390
appreciate using that coming from a

00:03:41,080 --> 00:03:46,390
hadoop mapreduce world we wanted the

00:03:43,390 --> 00:03:48,340
ability to combine that historical data

00:03:46,390 --> 00:03:50,200
that we're doing aggregations on with

00:03:48,340 --> 00:03:53,350
stuff that we're streaming so we could

00:03:50,200 --> 00:03:55,360
get a more holistic picture of a user do

00:03:53,350 --> 00:04:02,170
some other analytics on the data or do

00:03:55,360 --> 00:04:03,610
something with it so in my mind this is

00:04:02,170 --> 00:04:05,710
kind of a generic streaming data

00:04:03,610 --> 00:04:08,550
pipeline if you will that we have a

00:04:05,710 --> 00:04:11,410
browsers and device click on something

00:04:08,550 --> 00:04:12,910
ends up going to some tags every wheel

00:04:11,410 --> 00:04:15,280
some analytics vendor could be anything

00:04:12,910 --> 00:04:18,100
else that collect that data possibly

00:04:15,280 --> 00:04:20,430
drop it into the message queue of some

00:04:18,100 --> 00:04:22,810
sort goes through some analysis tier

00:04:20,430 --> 00:04:25,090
possibly in some sort of in-memory store

00:04:22,810 --> 00:04:27,340
and then some data accessed here we

00:04:25,090 --> 00:04:29,290
could get the data back out all rights

00:04:27,340 --> 00:04:31,150
from the storm architecture I had talked

00:04:29,290 --> 00:04:32,860
about it we pretty much had this type of

00:04:31,150 --> 00:04:35,169
thing with the analysis here bein storm

00:04:32,860 --> 00:04:37,720
and the data access to your being you

00:04:35,169 --> 00:04:39,910
know a web sockets web service

00:04:37,720 --> 00:04:41,050
we would then serve data through so as

00:04:39,910 --> 00:04:42,880
we start to play with spark streaming we

00:04:41,050 --> 00:04:48,340
thought can we do a similar type of

00:04:42,880 --> 00:04:50,620
thing so that this is pretty much the

00:04:48,340 --> 00:04:52,270
pipeline that I was thinking about you

00:04:50,620 --> 00:04:54,070
know putting together code for this and

00:04:52,270 --> 00:04:55,210
just kind of walking through it of you

00:04:54,070 --> 00:04:57,580
know I wasn't going to put together a

00:04:55,210 --> 00:05:01,330
real collection server but I just got

00:04:57,580 --> 00:05:03,790
like MSNBC click traffic from the UCI QD

00:05:01,330 --> 00:05:06,070
data set that has like you know real

00:05:03,790 --> 00:05:08,140
visitor patterns for a day and you know

00:05:06,070 --> 00:05:11,650
in September basically took that

00:05:08,140 --> 00:05:13,810
replayed it into Kafka run it through

00:05:11,650 --> 00:05:22,360
spark streaming back into Kafka and the

00:05:13,810 --> 00:05:24,760
out via a WebSocket server okay it's

00:05:22,360 --> 00:05:27,700
just quick overview of Kafka rights a

00:05:24,760 --> 00:05:30,100
neutral develop it linkedin distributed

00:05:27,700 --> 00:05:31,300
pub sub type messaging system is

00:05:30,100 --> 00:05:33,820
specifically designed for real-time

00:05:31,300 --> 00:05:36,610
activity streams it doesn't follow any

00:05:33,820 --> 00:05:40,570
JMS standards use a JMS api's has AP is

00:05:36,610 --> 00:05:43,270
in most languages some key features of

00:05:40,570 --> 00:05:46,480
it persistent messaging that's high

00:05:43,270 --> 00:05:49,630
throughput low overhead use a zookeeper

00:05:46,480 --> 00:05:51,820
whether that's good or bad yeah it does

00:05:49,630 --> 00:05:55,900
and supports both kind of queuing and

00:05:51,820 --> 00:06:00,220
topic semantics as it really kind of is

00:05:55,900 --> 00:06:01,960
designed to take this it kind of the

00:06:00,220 --> 00:06:03,790
couple this mess into something that

00:06:01,960 --> 00:06:06,070
looks more like this so it sits in the

00:06:03,790 --> 00:06:08,740
middle because step will walk there is

00:06:06,070 --> 00:06:10,300
you know coming in through here going to

00:06:08,740 --> 00:06:12,190
Kafka in this case they go into

00:06:10,300 --> 00:06:14,350
real-time or spark streaming you know

00:06:12,190 --> 00:06:16,210
these arrows could go both ways you know

00:06:14,350 --> 00:06:25,120
of having you come in this way and then

00:06:16,210 --> 00:06:28,060
feedback through and come back out so

00:06:25,120 --> 00:06:30,340
spark streaming sits on top of spark

00:06:28,060 --> 00:06:32,830
with the goal of delivering large-scale

00:06:30,340 --> 00:06:35,050
stream processing it's pretty efficient

00:06:32,830 --> 00:06:37,960
and fault-tolerant staple stream

00:06:35,050 --> 00:06:40,570
processing I integrates with sparks

00:06:37,960 --> 00:06:42,520
batch interactive processing so you're

00:06:40,570 --> 00:06:44,229
working with a stream of data and it

00:06:42,520 --> 00:06:46,030
really feels no different than if you're

00:06:44,229 --> 00:06:48,220
working with a file that's coming out if

00:06:46,030 --> 00:06:49,750
I do so everything kind of feels the

00:06:48,220 --> 00:06:51,070
same and they could kind of mix them and

00:06:49,750 --> 00:06:52,810
you really

00:06:51,070 --> 00:06:55,570
kind of lose sight of where that is and

00:06:52,810 --> 00:06:57,640
it blends it really well provides a

00:06:55,570 --> 00:06:59,350
simple batch like API so you can

00:06:57,640 --> 00:07:00,970
implement complex algorithm so the same

00:06:59,350 --> 00:07:02,980
thing you really don't know you're

00:07:00,970 --> 00:07:04,780
working with a stream you just realize

00:07:02,980 --> 00:07:09,300
you're working with a different type of

00:07:04,780 --> 00:07:09,300
our dd's that's storm that spark exposes

00:07:10,800 --> 00:07:15,280
you know with that it provides a certain

00:07:12,940 --> 00:07:16,870
programming model so in spark there's a

00:07:15,280 --> 00:07:20,590
notion of these are td's is resilient

00:07:16,870 --> 00:07:22,230
data sets in spark streaming there's

00:07:20,590 --> 00:07:25,720
these streams which is discretized

00:07:22,230 --> 00:07:28,240
stream this is very similar ap is it

00:07:25,720 --> 00:07:30,520
looks and feels very similar to it and

00:07:28,240 --> 00:07:32,500
you pretty much live in a world that you

00:07:30,520 --> 00:07:34,900
have input we have D streams are being

00:07:32,500 --> 00:07:37,240
created from an input stream or from

00:07:34,900 --> 00:07:38,860
another stream all right then you deal

00:07:37,240 --> 00:07:41,260
with operations on them we're going to

00:07:38,860 --> 00:07:42,880
transform and do something with them and

00:07:41,260 --> 00:07:51,580
then you're going to output the result

00:07:42,880 --> 00:07:54,820
to somewhere out of the box the data

00:07:51,580 --> 00:07:59,220
sources for input are HDFS kafka flume

00:07:54,820 --> 00:08:01,540
Twitter TCP sockets acha actor zeromq

00:07:59,220 --> 00:08:03,820
and pretty easy to kind of roll your own

00:08:01,540 --> 00:08:05,500
as well it's not that not that hard it's

00:08:03,820 --> 00:08:08,110
pretty well extracted so they got a lot

00:08:05,500 --> 00:08:09,400
of things pretty covered from and

00:08:08,110 --> 00:08:12,190
out-of-the-box standpoint on getting

00:08:09,400 --> 00:08:14,320
data in and as we'll see getting data

00:08:12,190 --> 00:08:19,690
out is a little bit of a different story

00:08:14,320 --> 00:08:22,990
at this time these are some of the

00:08:19,690 --> 00:08:24,610
operations so I say transform it really

00:08:22,990 --> 00:08:28,090
just allows you to build new strings

00:08:24,610 --> 00:08:29,770
from existing ones so there's our DD

00:08:28,090 --> 00:08:31,030
like operations it's the same thing

00:08:29,770 --> 00:08:34,750
you've seen if you've looked at spark

00:08:31,030 --> 00:08:37,800
before or other similar platforms where

00:08:34,750 --> 00:08:41,919
this map flatmap filter count by value

00:08:37,800 --> 00:08:43,300
reduce group bike key you know so a

00:08:41,919 --> 00:08:45,400
variety of things so the ones in bold

00:08:43,300 --> 00:08:47,050
we'll see as we kind of walk through to

00:08:45,400 --> 00:08:48,580
some code examples of it some of the

00:08:47,050 --> 00:08:50,380
things that are new that it does bring

00:08:48,580 --> 00:08:53,770
to the table that don't exist in spark

00:08:50,380 --> 00:08:55,060
are the windowing operations all right

00:08:53,770 --> 00:08:58,510
we could do a window you could count by

00:08:55,060 --> 00:09:01,000
window get reduced by window count by

00:08:58,510 --> 00:09:03,930
value and the window reduced by key in

00:09:01,000 --> 00:09:05,640
the window and you can update state

00:09:03,930 --> 00:09:09,450
which is pretty interesting for doing

00:09:05,640 --> 00:09:12,180
certain things so we saw that there were

00:09:09,450 --> 00:09:13,970
a lot of different input options this is

00:09:12,180 --> 00:09:16,170
all you get for output out of the box

00:09:13,970 --> 00:09:19,320
you get print which will print to the

00:09:16,170 --> 00:09:22,459
drivers screen so that doesn't really do

00:09:19,320 --> 00:09:24,779
a whole lot for you you get for each rdd

00:09:22,459 --> 00:09:27,209
where you could perform an arbitrary

00:09:24,779 --> 00:09:29,940
operation on every rdd that came out of

00:09:27,209 --> 00:09:31,770
the batch so it kind of gives you the

00:09:29,940 --> 00:09:34,709
point that you could do something you

00:09:31,770 --> 00:09:36,800
can save as an object file you save as a

00:09:34,709 --> 00:09:38,910
text file you can save us a dupe files

00:09:36,800 --> 00:09:41,779
so somewhat limited for a streaming

00:09:38,910 --> 00:09:44,310
platform to only have that as the output

00:09:41,779 --> 00:09:46,920
there's some different projects out

00:09:44,310 --> 00:09:48,660
there to have other outputs it's just so

00:09:46,920 --> 00:09:50,700
those been the focus to get data in and

00:09:48,660 --> 00:09:53,339
so far not a whole lot that's going on

00:09:50,700 --> 00:09:55,410
to get data out episodes we'll see what

00:09:53,339 --> 00:09:57,420
what I've put in place at least what I

00:09:55,410 --> 00:10:00,870
have here is it's just leveraging that

00:09:57,420 --> 00:10:03,660
for each to send data out but it's still

00:10:00,870 --> 00:10:06,149
not as clean as the inputs that they

00:10:03,660 --> 00:10:08,010
offer because the inputs all seem the

00:10:06,149 --> 00:10:10,589
same it's a generic way of getting data

00:10:08,010 --> 00:10:16,380
in there's no really good generic way to

00:10:10,589 --> 00:10:19,529
get data out so the discretized stream

00:10:16,380 --> 00:10:20,790
processing we talked about so really

00:10:19,529 --> 00:10:23,310
kind of how this looks in the picture

00:10:20,790 --> 00:10:25,980
that we had as you imagine the stream of

00:10:23,310 --> 00:10:28,290
data coming through Kafka and its really

00:10:25,980 --> 00:10:30,990
kind of breaking it up into these batch

00:10:28,290 --> 00:10:32,730
sizes the rights imagine each one of

00:10:30,990 --> 00:10:34,290
these as the stream is flowing each one

00:10:32,730 --> 00:10:35,730
of these representing a batch of data

00:10:34,290 --> 00:10:37,760
coming through you know could have a

00:10:35,730 --> 00:10:40,770
granularity of say like a half a second

00:10:37,760 --> 00:10:42,270
right so as it flows through you end up

00:10:40,770 --> 00:10:44,279
inside of spark streaming say we're

00:10:42,270 --> 00:10:46,080
consuming this from Kafka working to do

00:10:44,279 --> 00:10:48,180
some processing on it and then we're

00:10:46,080 --> 00:10:50,839
going to send out these process results

00:10:48,180 --> 00:10:53,459
again in some sort of batch increment

00:10:50,839 --> 00:10:56,310
right so it's a stream that gets broken

00:10:53,459 --> 00:11:04,770
up these little discretized pieces you

00:10:56,310 --> 00:11:06,570
process them and send them out some of

00:11:04,770 --> 00:11:08,640
the clickstream examples want to walk

00:11:06,570 --> 00:11:10,829
through some common ones that you'll see

00:11:08,640 --> 00:11:12,150
it just like pageviews per batch the

00:11:10,829 --> 00:11:13,740
neck really just be anything rather just

00:11:12,150 --> 00:11:16,709
counting number of page views in one of

00:11:13,740 --> 00:11:17,630
those batches looking at page views by

00:11:16,709 --> 00:11:19,610
URL over

00:11:17,630 --> 00:11:21,290
I'm so maybe you want to see things like

00:11:19,610 --> 00:11:24,020
I said if you want to see what are the

00:11:21,290 --> 00:11:26,030
top page is over in the last 10 seconds

00:11:24,020 --> 00:11:28,370
or the last 30 seconds or whatever time

00:11:26,030 --> 00:11:30,680
for you may want to do the top end page

00:11:28,370 --> 00:11:32,480
views over time so in that case they

00:11:30,680 --> 00:11:33,590
just want like a top 10 they don't care

00:11:32,480 --> 00:11:35,960
about seeing all of them but you just

00:11:33,590 --> 00:11:37,880
want the top 10 for a dashboard so maybe

00:11:35,960 --> 00:11:39,260
you want to see the top 10 pages maybe

00:11:37,880 --> 00:11:43,430
want to see the top 10 countries or

00:11:39,260 --> 00:11:45,980
browsers or cities another one is

00:11:43,430 --> 00:11:47,990
keeping a session up to date you just

00:11:45,980 --> 00:11:49,910
say you have something where someone's

00:11:47,990 --> 00:11:51,710
on a site and there's traffic coming

00:11:49,910 --> 00:11:53,210
through and you have their session in

00:11:51,710 --> 00:11:55,250
hand and you want to be able to keep

00:11:53,210 --> 00:11:57,080
track of that session as more data flows

00:11:55,250 --> 00:11:59,420
through so that's where we'll see where

00:11:57,080 --> 00:12:02,120
update state by key we could hold on to

00:11:59,420 --> 00:12:03,680
a session that's live and keep updating

00:12:02,120 --> 00:12:08,840
that session as more traffic flows

00:12:03,680 --> 00:12:11,930
through joining the current session with

00:12:08,840 --> 00:12:13,700
historical so in this case say you have

00:12:11,930 --> 00:12:15,500
again that same session someone's on the

00:12:13,700 --> 00:12:16,970
site or doing something and then you

00:12:15,500 --> 00:12:19,280
also want to be able to reach back and

00:12:16,970 --> 00:12:21,260
grab whatever their history maybe and

00:12:19,280 --> 00:12:23,000
pull that together maybe you want to run

00:12:21,260 --> 00:12:24,320
some other computation on that maybe

00:12:23,000 --> 00:12:25,790
want to do some sort of prediction as to

00:12:24,320 --> 00:12:27,740
likelihood someone may buy the

00:12:25,790 --> 00:12:29,090
likelihood they may abandon the

00:12:27,740 --> 00:12:31,940
likelihood that they may do something

00:12:29,090 --> 00:12:33,380
provide a recommendation forum so a lot

00:12:31,940 --> 00:12:35,420
of different things you could do you

00:12:33,380 --> 00:12:37,100
could also decide that maybe it's not

00:12:35,420 --> 00:12:39,350
historical but in just this general

00:12:37,100 --> 00:12:41,810
joining of two streams maybe you want to

00:12:39,350 --> 00:12:43,820
join the current visitors stream with

00:12:41,810 --> 00:12:45,890
the weather so now you could grab their

00:12:43,820 --> 00:12:47,540
zip code grab the weather in their zip

00:12:45,890 --> 00:12:48,920
code and have an idea if you're a pool

00:12:47,540 --> 00:12:51,320
company of whether or not you should

00:12:48,920 --> 00:12:53,930
provide them an offer or make some other

00:12:51,320 --> 00:12:55,250
decision based upon this other data so

00:12:53,930 --> 00:12:57,710
it could really be used to join any two

00:12:55,250 --> 00:13:00,230
streams we could also be used to join a

00:12:57,710 --> 00:13:02,630
stream with existing spark rdd as well

00:13:00,230 --> 00:13:04,280
so it can be coming out of Hadoop or two

00:13:02,630 --> 00:13:11,720
streams coming in from say Kafka or

00:13:04,280 --> 00:13:15,080
cough gun Twitter so the first thing to

00:13:11,720 --> 00:13:17,450
get things going to be able to do kind

00:13:15,080 --> 00:13:20,090
of our page views that are just in each

00:13:17,450 --> 00:13:22,460
batch you got to create a stream from

00:13:20,090 --> 00:13:26,900
Kafka so this is another java api which

00:13:22,460 --> 00:13:28,310
is painful at times m it shows you more

00:13:26,900 --> 00:13:29,870
i think of what's going on and somewhat

00:13:28,310 --> 00:13:31,550
gets hidden from scholar but this is

00:13:29,870 --> 00:13:33,740
kind of working with this source of

00:13:31,550 --> 00:13:35,959
feel like the old argument of C++ or

00:13:33,740 --> 00:13:38,180
maybe they always start to write a whole

00:13:35,959 --> 00:13:41,000
bunch here and the Scala API is a lot

00:13:38,180 --> 00:13:42,890
more succinct but really where it all

00:13:41,000 --> 00:13:45,410
starts is you have this coffee utilities

00:13:42,890 --> 00:13:46,760
create stream once you do that and

00:13:45,410 --> 00:13:47,870
that's going to take some configuration

00:13:46,760 --> 00:13:50,240
that's going to be typically what you

00:13:47,870 --> 00:13:53,029
would provide to have a coffee consumed

00:13:50,240 --> 00:13:55,160
that's going to consume data so this

00:13:53,029 --> 00:13:57,110
we're saying that the input sources are

00:13:55,160 --> 00:13:58,910
really clean this is all you have to do

00:13:57,110 --> 00:14:00,890
and you could have them for all

00:13:58,910 --> 00:14:02,779
different types of sources you know from

00:14:00,890 --> 00:14:05,570
a file from anything it's really clean

00:14:02,779 --> 00:14:07,700
like that so it makes it nice so what

00:14:05,570 --> 00:14:10,820
this is going to return to us is this

00:14:07,700 --> 00:14:12,920
messages d stream right this is going to

00:14:10,820 --> 00:14:13,880
have in it you know two pieces that's

00:14:12,920 --> 00:14:15,680
going to be the couple that's coming out

00:14:13,880 --> 00:14:17,480
of Kafka that our data is going to be in

00:14:15,680 --> 00:14:20,480
here so we're going to do with that is

00:14:17,480 --> 00:14:23,570
turn around in this case I had stuff the

00:14:20,480 --> 00:14:26,860
data in that it was tab delimited with a

00:14:23,570 --> 00:14:31,279
visitor ID and the URL that they're on

00:14:26,860 --> 00:14:33,260
so we're going to is just map that to a

00:14:31,279 --> 00:14:36,579
new rdd that's going to be composed of

00:14:33,260 --> 00:14:38,570
this couple of visitor ID and URL

00:14:36,579 --> 00:14:40,070
because all this does when it goes

00:14:38,570 --> 00:14:44,089
through it's just basically split the

00:14:40,070 --> 00:14:46,700
data and assign it ok so what it looks

00:14:44,089 --> 00:14:47,899
like down here is pretty much what's

00:14:46,700 --> 00:14:49,970
happening in this code is you have this

00:14:47,899 --> 00:14:54,380
consumer you have some batch happening

00:14:49,970 --> 00:14:56,660
at time T at time T plus 1 T plus 2 so x

00:14:54,380 --> 00:14:59,450
going on this way we create this stream

00:14:56,660 --> 00:15:01,490
and at each batch we're going through we

00:14:59,450 --> 00:15:04,130
have this message d stream that we

00:15:01,490 --> 00:15:06,740
created so here right we're going to

00:15:04,130 --> 00:15:08,360
perform that map operation which is here

00:15:06,740 --> 00:15:12,050
and we're going to end up with this

00:15:08,360 --> 00:15:13,459
events d stream at this point it's all

00:15:12,050 --> 00:15:15,020
just there nothing's really going on

00:15:13,459 --> 00:15:19,360
right it's just we have this data in

00:15:15,020 --> 00:15:19,360
hand or will once we go to act upon it

00:15:21,399 --> 00:15:28,310
okay so now we have that we have our

00:15:26,390 --> 00:15:30,290
stream setup from Kafka and we have just

00:15:28,310 --> 00:15:31,760
all the raw events coming in from Kafka

00:15:30,290 --> 00:15:34,490
now we can start to actually do

00:15:31,760 --> 00:15:36,620
something with it so the first would be

00:15:34,490 --> 00:15:39,800
ok let's do the page views per batch see

00:15:36,620 --> 00:15:41,270
how much is coming in so again we take

00:15:39,800 --> 00:15:43,910
that events d string that we had before

00:15:41,270 --> 00:15:45,279
and now we're going to perform a map

00:15:43,910 --> 00:15:47,499
operation on it

00:15:45,279 --> 00:15:49,810
alright and we're going to take visitor

00:15:47,499 --> 00:15:51,790
idea in the or other on and we're going

00:15:49,810 --> 00:15:53,649
to return back the URL from here and

00:15:51,790 --> 00:15:57,279
then we're just going to account by

00:15:53,649 --> 00:15:59,230
value okay so what's going to end up

00:15:57,279 --> 00:16:02,920
happening is we're going to get a count

00:15:59,230 --> 00:16:06,490
of the URLs with their account of all

00:16:02,920 --> 00:16:08,290
the URLs and their values ok so the same

00:16:06,490 --> 00:16:11,529
thing that applies is data is moving

00:16:08,290 --> 00:16:14,740
across this way you know at T equals 1 T

00:16:11,529 --> 00:16:16,689
plus 12 plus two as a map operation

00:16:14,740 --> 00:16:19,029
happens and then we turn around do a

00:16:16,689 --> 00:16:22,629
count by value we end up with this page

00:16:19,029 --> 00:16:25,809
counts d string okay so at this point we

00:16:22,629 --> 00:16:38,139
have the count that's URL and the number

00:16:25,809 --> 00:16:39,519
of times it appeared in that patch if we

00:16:38,139 --> 00:16:40,990
want to take that little further and say

00:16:39,519 --> 00:16:42,279
well now we have that that really

00:16:40,990 --> 00:16:43,540
doesn't give you a whole lot except

00:16:42,279 --> 00:16:44,860
knowing how many are elves were in a

00:16:43,540 --> 00:16:46,689
batch so you could maybe see like a

00:16:44,860 --> 00:16:47,949
number of events that are coming in but

00:16:46,689 --> 00:16:50,079
you're really not doing a whole lot with

00:16:47,949 --> 00:16:52,360
it so now if you actually want to say

00:16:50,079 --> 00:16:55,930
you want to see the page views per Earl

00:16:52,360 --> 00:16:59,290
overtime now we could take again that

00:16:55,930 --> 00:17:00,279
same events d string that we had we're

00:16:59,290 --> 00:17:04,179
going to perform a different map

00:17:00,279 --> 00:17:05,799
operation on it do the same thing of

00:17:04,179 --> 00:17:08,799
returning back the URL this time we're

00:17:05,799 --> 00:17:10,299
going to account by value and window so

00:17:08,799 --> 00:17:11,230
the same before we get account by value

00:17:10,299 --> 00:17:13,510
this time we're going to count the

00:17:11,230 --> 00:17:16,329
values as well which of the URLs and the

00:17:13,510 --> 00:17:18,130
window and time ok so this is going to

00:17:16,329 --> 00:17:20,589
be our window length that we want to do

00:17:18,130 --> 00:17:24,730
it or a 30-second window and we want our

00:17:20,589 --> 00:17:27,010
batch interval to be at five seconds ok

00:17:24,730 --> 00:17:29,230
so we'll have 30 seconds of data and

00:17:27,010 --> 00:17:33,309
compute this computation every five

00:17:29,230 --> 00:17:39,159
seconds when that's done we'll go ahead

00:17:33,309 --> 00:17:41,020
and reduce it by key and then what we

00:17:39,159 --> 00:17:44,590
end up here right is so coming out of

00:17:41,020 --> 00:17:46,929
this every five seconds so mini

00:17:44,590 --> 00:17:49,600
MapReduce job if you will runs bad job

00:17:46,929 --> 00:17:52,510
that runs that we end up with the URLs

00:17:49,600 --> 00:17:53,919
and their counts and then we're going to

00:17:52,510 --> 00:17:55,750
turn around and reduce it by the key

00:17:53,919 --> 00:17:57,460
which is the Earl's we get the values

00:17:55,750 --> 00:18:00,670
which are coming in as the counts and we

00:17:57,460 --> 00:18:03,020
just add them together and just return

00:18:00,670 --> 00:18:05,840
okay so it ends up happening when this

00:18:03,020 --> 00:18:09,200
comes out let me go to use this is we

00:18:05,840 --> 00:18:11,720
end up having the sum of counts for URL

00:18:09,200 --> 00:18:13,310
every five seconds so from here you

00:18:11,720 --> 00:18:15,350
could just see all the URLs happening

00:18:13,310 --> 00:18:18,530
every five seconds you have access to

00:18:15,350 --> 00:18:22,670
all the URLs with their accounts of how

00:18:18,530 --> 00:18:27,460
many times they were visited the one

00:18:22,670 --> 00:18:30,470
thing that's different between one way

00:18:27,460 --> 00:18:36,080
between say these pageviews per batch or

00:18:30,470 --> 00:18:37,610
and this this doesn't do any sort of win

00:18:36,080 --> 00:18:39,110
doing operation at all so there's

00:18:37,610 --> 00:18:42,320
nothing that needs to get persisted

00:18:39,110 --> 00:18:45,530
anywhere for failover but it sees notice

00:18:42,320 --> 00:18:47,900
we do this and then we want to count by

00:18:45,530 --> 00:18:50,420
value in window and we're going to need

00:18:47,900 --> 00:18:52,460
to have a window of 30 seconds and a

00:18:50,420 --> 00:18:54,520
batch interval of every five seconds

00:18:52,460 --> 00:18:56,720
that data needs to be stored somewhere

00:18:54,520 --> 00:18:59,150
okay so there's a check pointing that

00:18:56,720 --> 00:19:00,560
you need to basically set up in spark

00:18:59,150 --> 00:19:03,260
streaming to tell it where to checkpoint

00:19:00,560 --> 00:19:05,000
and then check point HDFS by default if

00:19:03,260 --> 00:19:06,140
you run it locally just give it a local

00:19:05,000 --> 00:19:12,650
directory and it'll check point two

00:19:06,140 --> 00:19:16,480
there by default with count by value and

00:19:12,650 --> 00:19:23,780
window it'll checkpoint every 10 seconds

00:19:16,480 --> 00:19:26,360
and that again is that's controllable so

00:19:23,780 --> 00:19:28,310
now we've got all the page views with

00:19:26,360 --> 00:19:29,930
their accounts going on something that's

00:19:28,310 --> 00:19:31,280
maybe more interesting or more useful

00:19:29,930 --> 00:19:33,020
instead of just streaming a bunch of

00:19:31,280 --> 00:19:35,180
data at someone is actually just have

00:19:33,020 --> 00:19:36,170
the top and maybe you want the bottom in

00:19:35,180 --> 00:19:39,530
this case we're just going to like the

00:19:36,170 --> 00:19:41,150
top so in this we're going to take that

00:19:39,530 --> 00:19:43,370
sliding page counts that we just had

00:19:41,150 --> 00:19:46,100
before we're going to turn around and

00:19:43,370 --> 00:19:48,950
basically just swap them so now instead

00:19:46,100 --> 00:19:51,260
of having the Earl and its count we're

00:19:48,950 --> 00:19:55,060
going to end up with a mat with a d

00:19:51,260 --> 00:19:55,060
stream that has the count in the Earl

00:19:55,600 --> 00:20:02,260
once we have them swapped we're going to

00:19:58,130 --> 00:20:02,260
turn around transform that

00:20:05,040 --> 00:20:14,910
and basically sort by key and then I

00:20:11,010 --> 00:20:16,380
come I mistakenly deleted code that

00:20:14,910 --> 00:20:17,670
should've been there have the last thing

00:20:16,380 --> 00:20:20,250
that you would do is from there you

00:20:17,670 --> 00:20:23,370
could do a take and take 10 and you'll

00:20:20,250 --> 00:20:24,660
get back just the top 10 and instance

00:20:23,370 --> 00:20:28,890
already in sorted order just can give

00:20:24,660 --> 00:20:31,800
you back the first 10 from there there's

00:20:28,890 --> 00:20:34,560
also an API to make this simpler that

00:20:31,800 --> 00:20:35,820
you could just do a top you could do its

00:20:34,560 --> 00:20:46,320
other ways you could solve the same

00:20:35,820 --> 00:20:47,880
problem so say in this case that's all

00:20:46,320 --> 00:20:49,320
great and you have a dashboard that has

00:20:47,880 --> 00:20:51,660
a top end of things that are going on

00:20:49,320 --> 00:20:53,490
maybe its pages maybe its cities

00:20:51,660 --> 00:20:56,010
whatever you want it to be some other

00:20:53,490 --> 00:20:57,330
data and now you want to basically have

00:20:56,010 --> 00:20:59,010
the situation if you want to update a

00:20:57,330 --> 00:21:01,110
current session as something's flowing

00:20:59,010 --> 00:21:03,990
by say it's a user session as they're

00:21:01,110 --> 00:21:08,030
active alright so what you could do with

00:21:03,990 --> 00:21:12,360
update state by key is really specify a

00:21:08,030 --> 00:21:16,590
generic function to modify previous

00:21:12,360 --> 00:21:17,700
state with new data so in this case

00:21:16,590 --> 00:21:20,100
we're going to declare this update

00:21:17,700 --> 00:21:21,540
function that really all it's going to

00:21:20,100 --> 00:21:22,920
do is just going to take this page view

00:21:21,540 --> 00:21:26,360
assume that we have some page view

00:21:22,920 --> 00:21:28,110
object is going to take the session and

00:21:26,360 --> 00:21:30,690
then we're going to perform an operation

00:21:28,110 --> 00:21:32,670
on it update in the session and return

00:21:30,690 --> 00:21:36,380
that updated data so now we could take

00:21:32,670 --> 00:21:38,610
in the new pages that were viewed and

00:21:36,380 --> 00:21:40,800
the session object that we have that's

00:21:38,610 --> 00:21:43,050
representing our state and update that

00:21:40,800 --> 00:21:48,360
session with a new page information and

00:21:43,050 --> 00:21:50,310
then return it okay so when we do that

00:21:48,360 --> 00:21:52,860
this is the update function that will be

00:21:50,310 --> 00:21:55,740
passed in and we just basically takes a

00:21:52,860 --> 00:21:59,970
this page view d stream that we may have

00:21:55,740 --> 00:22:02,010
call update state by key and pass in the

00:21:59,970 --> 00:22:04,620
state information the function that

00:22:02,010 --> 00:22:06,900
represents that ok when that returns

00:22:04,620 --> 00:22:08,820
then we have visitor history in a

00:22:06,900 --> 00:22:10,940
current visitor session that's currently

00:22:08,820 --> 00:22:12,990
being kept as data is flowing through

00:22:10,940 --> 00:22:14,660
you know there's still things to work

00:22:12,990 --> 00:22:16,740
out that you would need to do as far as

00:22:14,660 --> 00:22:18,090
what happens when you know that states

00:22:16,740 --> 00:22:19,830
longer valid

00:22:18,090 --> 00:22:21,750
you know in the case of say a visitor

00:22:19,830 --> 00:22:23,159
session it times out so you still need

00:22:21,750 --> 00:22:24,450
to do other things outside of this to

00:22:23,159 --> 00:22:32,429
make sure that that's cleaned up or

00:22:24,450 --> 00:22:35,220
taken care of so now you have a current

00:22:32,429 --> 00:22:37,049
session maybe we want to be able to join

00:22:35,220 --> 00:22:39,090
it with history so we can start to see

00:22:37,049 --> 00:22:40,799
what's this user done over the last 28

00:22:39,090 --> 00:22:42,210
days compared to what they're doing

00:22:40,799 --> 00:22:43,740
today I maybe you want to make

00:22:42,210 --> 00:22:45,480
predictions for maybe you want to

00:22:43,740 --> 00:22:46,950
recommend something maybe you just want

00:22:45,480 --> 00:22:50,669
to update some information not show them

00:22:46,950 --> 00:22:53,460
anything so let's assume that we created

00:22:50,669 --> 00:22:56,520
this current sessions d stream and we

00:22:53,460 --> 00:23:00,210
created a historical sessions stream

00:22:56,520 --> 00:23:02,429
this shouldn't say D string and that's

00:23:00,210 --> 00:23:04,500
just an RDD from spark and say our

00:23:02,429 --> 00:23:06,750
current sessions looks like this we have

00:23:04,500 --> 00:23:08,789
you know it's composed of tuples a

00:23:06,750 --> 00:23:10,830
visitor ID and some JSON object that

00:23:08,789 --> 00:23:12,960
says that's current session another

00:23:10,830 --> 00:23:15,120
visitor ID with a current session and

00:23:12,960 --> 00:23:17,399
then we have historical sessions that

00:23:15,120 --> 00:23:19,830
look something you know not too

00:23:17,399 --> 00:23:22,440
different from it visitor ID one with a

00:23:19,830 --> 00:23:25,980
historical session visitor ID 2 with a

00:23:22,440 --> 00:23:29,549
historical session so now we go ahead

00:23:25,980 --> 00:23:31,830
and we just call current sessions join

00:23:29,549 --> 00:23:34,169
and pass in the historical sections

00:23:31,830 --> 00:23:36,659
what's going to end up doing is joining

00:23:34,169 --> 00:23:39,240
on the keys so as long as these keys

00:23:36,659 --> 00:23:41,820
match where they do here you're going to

00:23:39,240 --> 00:23:44,159
end up coming back with a couple that

00:23:41,820 --> 00:23:48,750
has a list in it of the sessions so you

00:23:44,159 --> 00:23:50,760
end up with the visitors ID along with a

00:23:48,750 --> 00:23:54,270
couple of all the sessions from the

00:23:50,760 --> 00:23:55,679
joining of those two streams so in this

00:23:54,270 --> 00:23:58,020
case it works if you want to do stuff

00:23:55,679 --> 00:24:01,409
with visitors is that this could be from

00:23:58,020 --> 00:24:03,720
a spark rtd that you created it could be

00:24:01,409 --> 00:24:05,820
from another stream coming in from say

00:24:03,720 --> 00:24:07,529
Twitter and you have someone's ID and

00:24:05,820 --> 00:24:08,760
you want to combine things it could be

00:24:07,529 --> 00:24:12,929
weather data it could be anything you

00:24:08,760 --> 00:24:15,299
want where you have an RDD or D stream

00:24:12,929 --> 00:24:17,779
in hand and the keys common you want to

00:24:15,299 --> 00:24:17,779
combine them

00:24:24,250 --> 00:24:29,930
so so where are we so we kind of walk

00:24:26,720 --> 00:24:32,150
through kind of this flowing through

00:24:29,930 --> 00:24:34,340
we're going to get these chunks of data

00:24:32,150 --> 00:24:36,620
coming out right in these batches of a

00:24:34,340 --> 00:24:38,690
half a second we did some processing on

00:24:36,620 --> 00:24:42,080
it and then we have the process results

00:24:38,690 --> 00:24:44,270
going out and then back into Kafka so in

00:24:42,080 --> 00:24:45,500
this case that Joyce use Kafka there's

00:24:44,270 --> 00:24:47,660
other ways that you could do it as well

00:24:45,500 --> 00:24:49,520
you know so you see that there's some

00:24:47,660 --> 00:24:51,470
problems that you need to consider if

00:24:49,520 --> 00:24:53,660
you end up using Kafka and say you have

00:24:51,470 --> 00:24:55,490
a WebSocket that's connecting and you

00:24:53,660 --> 00:24:57,410
have some WebSocket server that's going

00:24:55,490 --> 00:25:00,170
to be managing clients there's things

00:24:57,410 --> 00:25:03,020
that you need to consider within spark

00:25:00,170 --> 00:25:05,720
streaming that are unlike spark right so

00:25:03,020 --> 00:25:07,790
for instance a regular spark job when

00:25:05,720 --> 00:25:11,000
you run it it's like a MapReduce job the

00:25:07,790 --> 00:25:13,940
computation is done the job's done in a

00:25:11,000 --> 00:25:17,330
spark streaming job it runs forever

00:25:13,940 --> 00:25:18,890
until you kill it so it doesn't end as

00:25:17,330 --> 00:25:20,210
soon as the computations done so the

00:25:18,890 --> 00:25:21,530
batches that you run it and it's just

00:25:20,210 --> 00:25:24,170
sitting there in a loop if you will and

00:25:21,530 --> 00:25:27,380
constantly running so imagine if we did

00:25:24,170 --> 00:25:28,700
have say this going on we had those you

00:25:27,380 --> 00:25:30,890
know handful of streams that we're going

00:25:28,700 --> 00:25:32,650
over of say top end and we're updating

00:25:30,890 --> 00:25:35,020
visitor history we're doing these things

00:25:32,650 --> 00:25:38,900
those are always going to be running and

00:25:35,020 --> 00:25:40,790
producing data to somewhere so there are

00:25:38,900 --> 00:25:42,230
some things you need to consider you

00:25:40,790 --> 00:25:44,330
know when you have it set up where

00:25:42,230 --> 00:25:45,680
they're launched and run and they're

00:25:44,330 --> 00:25:47,360
just going to be pushing data out of

00:25:45,680 --> 00:25:55,610
there's no way to kind of shut things

00:25:47,360 --> 00:25:58,550
down to clean it up so getting data out

00:25:55,610 --> 00:26:00,770
of spark streaming so those are so

00:25:58,550 --> 00:26:03,770
before that it only supports you know

00:26:00,770 --> 00:26:06,730
print for each RDD you know save objects

00:26:03,770 --> 00:26:09,740
to file save to HDFS save the text

00:26:06,730 --> 00:26:12,530
doesn't and doesn't do anything more

00:26:09,740 --> 00:26:14,300
than that so kind of the next step is

00:26:12,530 --> 00:26:17,170
once we have this here of how do you get

00:26:14,300 --> 00:26:17,170
it to Kafka

00:26:22,480 --> 00:26:30,040
so this is one example of how you could

00:26:25,480 --> 00:26:32,049
use a for each rdd em which again is not

00:26:30,040 --> 00:26:34,870
a generic way of output but it's the

00:26:32,049 --> 00:26:36,370
only way currently to be able to have

00:26:34,870 --> 00:26:38,169
access to the collection of our duties

00:26:36,370 --> 00:26:44,049
from a badge and to do something with

00:26:38,169 --> 00:26:46,210
them this Kafka producer show is really

00:26:44,049 --> 00:26:48,700
nothing more than just typical standard

00:26:46,210 --> 00:26:52,809
Kafka producer code and just go through

00:26:48,700 --> 00:26:54,660
and in this case you know we had say the

00:26:52,809 --> 00:26:57,520
top end stream that we were doing and

00:26:54,660 --> 00:26:58,780
we're going to attach this operation to

00:26:57,520 --> 00:27:00,940
these sort accounts that we had from

00:26:58,780 --> 00:27:04,440
before and we're going to loop over

00:27:00,940 --> 00:27:08,230
these are dd's build up this map and

00:27:04,440 --> 00:27:10,390
then basically turn around and send the

00:27:08,230 --> 00:27:11,830
top 10 list so and this is where I said

00:27:10,390 --> 00:27:15,160
before that you know you could do this

00:27:11,830 --> 00:27:18,160
if this rdd take so we had that sorted

00:27:15,160 --> 00:27:19,960
counts you guys execute this take to be

00:27:18,160 --> 00:27:24,940
able to take the top 10 elements from

00:27:19,960 --> 00:27:26,620
that sorted rdd so this is nothing

00:27:24,940 --> 00:27:28,419
different here except just walking

00:27:26,620 --> 00:27:30,340
through it then just call it a cough

00:27:28,419 --> 00:27:34,210
great to produce and just to send it all

00:27:30,340 --> 00:27:46,330
right so not as generic as getting data

00:27:34,210 --> 00:27:48,340
in so websockets kind of chose of had

00:27:46,330 --> 00:27:49,870
success with it it's kind of an abuse of

00:27:48,340 --> 00:27:51,220
it at times if you use it for storm with

00:27:49,870 --> 00:27:53,799
very high volume to have all that data

00:27:51,220 --> 00:27:55,870
going out to web sockets you know it

00:27:53,799 --> 00:27:57,340
really causes pain sometimes two

00:27:55,870 --> 00:28:00,370
browsers when you're pushing a fast

00:27:57,340 --> 00:28:03,120
stream across it yeah but it looks fun

00:28:00,370 --> 00:28:05,290
it shows well it does pretty cool things

00:28:03,120 --> 00:28:08,040
so it's a standard way to get data in

00:28:05,290 --> 00:28:14,820
and out easy to prototype with a browser

00:28:08,040 --> 00:28:18,120
so one way handling it oh wow sorry

00:28:14,820 --> 00:28:18,120
wrong button

00:28:27,200 --> 00:28:35,360
oh so one way of handling this WebSocket

00:28:33,260 --> 00:28:37,100
server and someone connects it's

00:28:35,360 --> 00:28:40,130
basically just starting to consume data

00:28:37,100 --> 00:28:42,649
from Kafka I starting to consume on say

00:28:40,130 --> 00:28:44,210
a top 10 or top end topic and start

00:28:42,649 --> 00:28:47,269
pulling that data back and sending it to

00:28:44,210 --> 00:28:49,700
the browser as I said where this could

00:28:47,269 --> 00:28:51,500
end up with problems is you run into

00:28:49,700 --> 00:28:53,630
issues potentially with multi tendency

00:28:51,500 --> 00:28:54,440
of how do you know how many clients are

00:28:53,630 --> 00:28:56,330
going to have and you have different

00:28:54,440 --> 00:28:58,010
data that's being processed through here

00:28:56,330 --> 00:29:00,200
and then I got to have different data

00:28:58,010 --> 00:29:01,880
that's through here and you have

00:29:00,200 --> 00:29:03,529
resources being consumed in spark

00:29:01,880 --> 00:29:05,419
streaming safer it's multiple customers

00:29:03,529 --> 00:29:06,950
of how do you have all those jobs

00:29:05,419 --> 00:29:09,049
running at one time and how do you make

00:29:06,950 --> 00:29:12,169
sure you have enough resources and what

00:29:09,049 --> 00:29:15,049
do you do if you're running computations

00:29:12,169 --> 00:29:16,700
for say a customer's data but there's no

00:29:15,049 --> 00:29:18,470
one over here even listening for it and

00:29:16,700 --> 00:29:20,360
wanting it right something you sitting

00:29:18,470 --> 00:29:24,230
there just computing stuff I'm not going

00:29:20,360 --> 00:29:28,190
and isn't it going nowhere so another

00:29:24,230 --> 00:29:29,840
option as opposed to doing this that

00:29:28,190 --> 00:29:31,789
have seen be successful is half of that

00:29:29,840 --> 00:29:35,110
WebSocket server have a client that

00:29:31,789 --> 00:29:37,309
comes in and since you're already

00:29:35,110 --> 00:29:41,240
infested with zookeeper if you have

00:29:37,309 --> 00:29:46,820
Kafka of turning around and registering

00:29:41,240 --> 00:29:48,679
this server in zookeeper with some sort

00:29:46,820 --> 00:29:49,909
of ID about the browser that's connected

00:29:48,679 --> 00:29:53,179
to her the WebSocket client that's

00:29:49,909 --> 00:29:55,429
connected to it and then having spark

00:29:53,179 --> 00:29:57,490
streaming and the code you have in there

00:29:55,429 --> 00:30:00,019
watching zookeeper for changes and

00:29:57,490 --> 00:30:04,909
seeing who has the data and who needs

00:30:00,019 --> 00:30:07,130
the data and then via zeromq going from

00:30:04,909 --> 00:30:09,080
spark streaming back to WebSocket server

00:30:07,130 --> 00:30:11,450
so then you could handle things up

00:30:09,080 --> 00:30:13,279
starting up a stream if someone connects

00:30:11,450 --> 00:30:15,830
you could tear down the stream when

00:30:13,279 --> 00:30:17,960
someone disconnects and you could handle

00:30:15,830 --> 00:30:21,830
state and here as far as resource

00:30:17,960 --> 00:30:24,950
consumption so this works well as this

00:30:21,830 --> 00:30:27,649
kind of demo but it is it does have some

00:30:24,950 --> 00:30:29,059
potential issues based on what you're

00:30:27,649 --> 00:30:31,870
going to do here because those jobs

00:30:29,059 --> 00:30:31,870
never stop running

00:30:37,540 --> 00:30:41,000
skins is the summer and then we could

00:30:39,410 --> 00:30:43,730
walk to the code show the demo running

00:30:41,000 --> 00:30:45,530
if we want and spark streaming works

00:30:43,730 --> 00:30:46,910
well for click stream analytics you know

00:30:45,530 --> 00:30:48,470
I use it for doing those types of things

00:30:46,910 --> 00:30:49,910
use it for trying to do some sort of

00:30:48,470 --> 00:30:53,060
predictions it's kind of working with

00:30:49,910 --> 00:30:55,310
that data it is still there's no good

00:30:53,060 --> 00:30:58,730
out-of-the-box output operations for a

00:30:55,310 --> 00:31:02,090
stream you know for each is just not

00:30:58,730 --> 00:31:03,680
that great multi-tenancy it needs to be

00:31:02,090 --> 00:31:06,110
thought through you know so there's

00:31:03,680 --> 00:31:08,300
because those jobs have no end of life

00:31:06,110 --> 00:31:10,730
you're stuck in the position of having

00:31:08,300 --> 00:31:11,870
to control what happens to them how do

00:31:10,730 --> 00:31:14,660
you bring them up how do you bring them

00:31:11,870 --> 00:31:16,070
down with regular spark if you notice

00:31:14,660 --> 00:31:18,290
there's now a job server that you could

00:31:16,070 --> 00:31:20,210
submit jobs to we've done things or it

00:31:18,290 --> 00:31:22,610
compiles MapReduce you know spark jobs

00:31:20,210 --> 00:31:24,140
on the fly and launches them that's nice

00:31:22,610 --> 00:31:25,790
that's easy because as soon as that job

00:31:24,140 --> 00:31:28,640
finishes you get back the results it's

00:31:25,790 --> 00:31:30,050
gone in this case even if you did the

00:31:28,640 --> 00:31:32,150
same thing and you programmatically

00:31:30,050 --> 00:31:34,490
launched a spark streaming job you have

00:31:32,150 --> 00:31:44,240
to know when it needs to die or skin

00:31:34,490 --> 00:31:51,700
continue to just kill resources that's

00:31:44,240 --> 00:31:57,650
all that I had right now for know if I

00:31:51,700 --> 00:32:02,990
good it's time for your Nathan sure okay

00:31:57,650 --> 00:32:04,610
just give me a second so if I understand

00:32:02,990 --> 00:32:07,430
you were using zookeeper as a control

00:32:04,610 --> 00:32:09,080
channel in order to control what parts

00:32:07,430 --> 00:32:12,440
of the streaming jobs were running or

00:32:09,080 --> 00:32:14,690
not yeah yes could you zookeeper as a

00:32:12,440 --> 00:32:16,340
control so have you considered using

00:32:14,690 --> 00:32:18,230
kavkaza control Channel I've heard

00:32:16,340 --> 00:32:21,530
people do that and published requests on

00:32:18,230 --> 00:32:24,380
kefka and and then this spark in this

00:32:21,530 --> 00:32:26,960
case would respond with a new stream I

00:32:24,380 --> 00:32:30,760
haven't no actually have not looked at

00:32:26,960 --> 00:32:30,760
dinner i'll show you where we did use it

00:32:33,100 --> 00:32:38,960
we'd use zookeeper you know in place of

00:32:37,310 --> 00:32:41,360
Kafka here so just kind of take this

00:32:38,960 --> 00:32:44,690
other picture so if you imagine this

00:32:41,360 --> 00:32:46,850
arrow going to hear / 0 mq we do

00:32:44,690 --> 00:32:47,810
zookeeper here and we do things where if

00:32:46,850 --> 00:32:50,000
like say a client

00:32:47,810 --> 00:32:51,920
connected via socket server we'd have

00:32:50,000 --> 00:32:52,940
the client idea of who it is we know

00:32:51,920 --> 00:32:55,670
what their rights are what they're

00:32:52,940 --> 00:32:58,850
allowed to and then register in

00:32:55,670 --> 00:33:00,890
zookeeper the IP address of this host

00:32:58,850 --> 00:33:04,640
along with the client that's connected

00:33:00,890 --> 00:33:06,590
and then from there inside of inside the

00:33:04,640 --> 00:33:08,510
streaming basically listen for changes

00:33:06,590 --> 00:33:12,140
in zookeeper and know that this hose

00:33:08,510 --> 00:33:15,140
services this client and there's no data

00:33:12,140 --> 00:33:17,090
through yeah that's the Samara furring

00:33:15,140 --> 00:33:19,820
to I've heard people use Africa for

00:33:17,090 --> 00:33:22,070
precisely that in your spark streaming

00:33:19,820 --> 00:33:23,960
code you could have a like a calf occur

00:33:22,070 --> 00:33:25,190
consumer listening to essentially

00:33:23,960 --> 00:33:27,920
request servic Africa and then you

00:33:25,190 --> 00:33:29,420
wouldn't have to connect spark and

00:33:27,920 --> 00:33:33,140
zookeeper right yeah it's interesting

00:33:29,420 --> 00:33:34,490
yeah I can see that working yeah then we

00:33:33,140 --> 00:33:35,960
just handle like a disconnect which

00:33:34,490 --> 00:33:43,820
could be another message in the kafka

00:33:35,960 --> 00:33:47,360
topic certainly yes good idea we started

00:33:43,820 --> 00:33:50,960
prototyping something similar with Spock

00:33:47,360 --> 00:33:56,570
streaming but instead of Kafka we are

00:33:50,960 --> 00:33:58,550
using the flume out the oval what

00:33:56,570 --> 00:34:01,760
happens when the message is out dropped

00:33:58,550 --> 00:34:04,700
our if what is the water than see on the

00:34:01,760 --> 00:34:08,410
kafka side not to measure family always

00:34:04,700 --> 00:34:12,350
kefka and up there are duplicated the

00:34:08,410 --> 00:34:14,360
elements or drop to demands so you can

00:34:12,350 --> 00:34:16,690
store the data based on time we had it

00:34:14,360 --> 00:34:19,220
set up over to store data for 72 hours

00:34:16,690 --> 00:34:24,140
so we'd hold on to messages for 72 hours

00:34:19,220 --> 00:34:25,520
before anything we get deleted I'm sorry

00:34:24,140 --> 00:34:30,770
to repeat your question think about you

00:34:25,520 --> 00:34:36,860
does no no I was saying the flow of

00:34:30,770 --> 00:34:40,220
events from the input of sparks trimming

00:34:36,860 --> 00:34:43,910
so you say that kefka is what I ever

00:34:40,220 --> 00:34:47,770
right mm-hmm / input she asking about

00:34:43,910 --> 00:34:47,770
input into spark streaming yeah

00:34:52,750 --> 00:34:58,250
so yeah Kafka is one of the input

00:34:55,340 --> 00:35:00,800
sources yeah under compound to flume

00:34:58,250 --> 00:35:03,590
that in case of a problem you can have

00:35:00,800 --> 00:35:06,770
duplicated messages that happened with

00:35:03,590 --> 00:35:07,760
kefka you could I mean it doesn't know

00:35:06,770 --> 00:35:09,500
right so you could have multiple

00:35:07,760 --> 00:35:13,850
producers producing the same data onto

00:35:09,500 --> 00:35:18,700
Kafka yeah that's not my question but

00:35:13,850 --> 00:35:18,700
what do it sorry

00:35:24,060 --> 00:35:31,090
you were explaining how you were merging

00:35:27,910 --> 00:35:33,250
or joining data with his own law on

00:35:31,090 --> 00:35:36,280
streaming data we have the historical

00:35:33,250 --> 00:35:38,860
data right right and you have pretty

00:35:36,280 --> 00:35:41,590
short batches right and you can have

00:35:38,860 --> 00:35:43,030
quite a bit of historical data so how

00:35:41,590 --> 00:35:45,760
does this joint happen actually

00:35:43,030 --> 00:35:47,920
efficiently because I thought it should

00:35:45,760 --> 00:35:51,310
happen more or less immediately but you

00:35:47,920 --> 00:35:53,260
can have lots of historical data so I

00:35:51,310 --> 00:35:54,430
didn't quite get it right so the

00:35:53,260 --> 00:35:57,040
question was how do you combine

00:35:54,430 --> 00:36:01,480
historical data with the in flight data

00:35:57,040 --> 00:36:03,100
fast that right so the historical day

00:36:01,480 --> 00:36:05,350
that we had was just visitors in

00:36:03,100 --> 00:36:07,000
partition by visitor so we would have

00:36:05,350 --> 00:36:09,940
data that's routed to the right notes

00:36:07,000 --> 00:36:11,590
with a visitor and then have all the

00:36:09,940 --> 00:36:14,560
data that's there and in reality a lot

00:36:11,590 --> 00:36:16,180
of visitors don't have that much data so

00:36:14,560 --> 00:36:17,320
it ends up not being that big and

00:36:16,180 --> 00:36:18,910
aggregate it's a lot there's a lot of

00:36:17,320 --> 00:36:21,310
data across all the visitors but as

00:36:18,910 --> 00:36:23,470
someone's trimming flip through it's

00:36:21,310 --> 00:36:25,030
really not often do they have a lot of

00:36:23,470 --> 00:36:27,370
data if you look at a lot of web

00:36:25,030 --> 00:36:29,230
analytics everyone believes that people

00:36:27,370 --> 00:36:31,510
come back all the time but the reality

00:36:29,230 --> 00:36:32,860
is a lot of people don't and sessions

00:36:31,510 --> 00:36:36,520
are small so the history really

00:36:32,860 --> 00:36:38,980
sometimes isn't that big but can I kind

00:36:36,520 --> 00:36:42,820
of pile up actually still if you are

00:36:38,980 --> 00:36:43,780
lucky and certainly so in you're stuck

00:36:42,820 --> 00:36:45,940
there right because if you're getting

00:36:43,780 --> 00:36:47,530
this data from HDFS and that becomes a

00:36:45,940 --> 00:36:49,630
ball neck of getting it into spark and

00:36:47,530 --> 00:36:58,240
then at that point right you're bound by

00:36:49,630 --> 00:37:00,420
memory or a putting it to disk addition

00:36:58,240 --> 00:37:05,590
to this question if you have for example

00:37:00,420 --> 00:37:08,350
3 billion user IDs and have only 10 10

00:37:05,590 --> 00:37:10,210
million coming in in parallel so how can

00:37:08,350 --> 00:37:14,410
be is this really efficient to figure

00:37:10,210 --> 00:37:16,840
out do continuously filtering through

00:37:14,410 --> 00:37:20,380
this even if it's partitioned by server

00:37:16,840 --> 00:37:22,030
so so you have to go to a huge amount of

00:37:20,380 --> 00:37:24,570
data every time because the data is

00:37:22,030 --> 00:37:28,420
start and probably in something more

00:37:24,570 --> 00:37:30,550
sequel right hammer in terms a partition

00:37:28,420 --> 00:37:32,170
right to think question was so if the

00:37:30,550 --> 00:37:33,850
data just kept exploding how do you do

00:37:32,170 --> 00:37:35,410
it efficiently so we used a bloom filter

00:37:33,850 --> 00:37:36,160
to keep track of whether or not the

00:37:35,410 --> 00:37:38,680
visitor is poss

00:37:36,160 --> 00:37:40,000
we on that note and to keep trying to do

00:37:38,680 --> 00:37:43,390
checks to see if through there before

00:37:40,000 --> 00:37:45,190
doing the operations so we would build

00:37:43,390 --> 00:37:46,960
those and then have that there in hand

00:37:45,190 --> 00:37:53,410
to see whether or not we actually had to

00:37:46,960 --> 00:37:57,010
try and do some massive join what was

00:37:53,410 --> 00:37:58,599
the problem with the mazes so why did

00:37:57,010 --> 00:38:02,530
you have to wait for the standalone

00:37:58,599 --> 00:38:03,970
feature in spark the question is why is

00:38:02,530 --> 00:38:06,730
there a problem with my sauce and we had

00:38:03,970 --> 00:38:09,460
a way there's no problem we're just a

00:38:06,730 --> 00:38:11,230
small shop at least and we had a Hadoop

00:38:09,460 --> 00:38:15,309
cluster that was just plain vanilla

00:38:11,230 --> 00:38:17,020
Apache and we weren't we just weren't in

00:38:15,309 --> 00:38:19,630
a position to be able to also deploy a

00:38:17,020 --> 00:38:21,039
cluster that head meadows so once it was

00:38:19,630 --> 00:38:23,500
stand alone and that was one less thing

00:38:21,039 --> 00:38:25,690
we need to go asking for it just became

00:38:23,500 --> 00:38:27,760
much easier for us so there was no

00:38:25,690 --> 00:38:29,289
problem with it we didn't have issues

00:38:27,760 --> 00:38:31,599
with it technically it was more of an

00:38:29,289 --> 00:38:39,579
operational thing of chemical asked for

00:38:31,599 --> 00:38:41,319
yet one more cluster so that was all if

00:38:39,579 --> 00:38:45,039
I remember correctly you used to have a

00:38:41,319 --> 00:38:47,319
storm cluster is that correct will you

00:38:45,039 --> 00:38:48,760
try to out spark no so the question is

00:38:47,319 --> 00:38:50,740
we used to have a storm cluster actually

00:38:48,760 --> 00:38:52,599
still do it was kind of side by side so

00:38:50,740 --> 00:38:55,270
we're using storm for an event stream

00:38:52,599 --> 00:38:56,319
and then we want to explore using spark

00:38:55,270 --> 00:38:58,180
streaming for doing more like an

00:38:56,319 --> 00:39:00,819
aggregate stream and things that didn't

00:38:58,180 --> 00:39:03,789
need that from click the dashboard as

00:39:00,819 --> 00:39:06,099
short as possible okay so it was

00:39:03,789 --> 00:39:08,260
something that's continues to run and is

00:39:06,099 --> 00:39:11,289
out there one of them if the feature

00:39:08,260 --> 00:39:13,630
some storm is is the you can push stuff

00:39:11,289 --> 00:39:16,150
to backing store and have a cash in

00:39:13,630 --> 00:39:18,460
front of it if you use tridon that is is

00:39:16,150 --> 00:39:20,740
that something that you use have you

00:39:18,460 --> 00:39:22,869
missed it in spark or how did you solve

00:39:20,740 --> 00:39:24,700
it in that case M so the state you get

00:39:22,869 --> 00:39:26,619
from trident we you know I had looked at

00:39:24,700 --> 00:39:28,750
it didn't play much with it but you get

00:39:26,619 --> 00:39:30,369
pretty efficient you know execution of

00:39:28,750 --> 00:39:32,980
it with spark streaming so it's going to

00:39:30,369 --> 00:39:35,680
store that data you could push data to

00:39:32,980 --> 00:39:37,180
HDFS from spark streaming as well so it

00:39:35,680 --> 00:39:41,099
doesn't always have to go out you could

00:39:37,180 --> 00:39:41,099
save data off every time a batch runs

00:39:43,130 --> 00:39:56,930
okay a question on join joining to these

00:39:53,210 --> 00:39:59,570
streams a seal problem that when you

00:39:56,930 --> 00:40:02,090
joined to the strips the problem is that

00:39:59,570 --> 00:40:06,830
you have to have the same keys at the

00:40:02,090 --> 00:40:10,010
same batch right do you see it really a

00:40:06,830 --> 00:40:12,410
problem for you or do you solve it

00:40:10,010 --> 00:40:14,270
somehow right so the question was

00:40:12,410 --> 00:40:16,490
joining 2d streams you have to have the

00:40:14,270 --> 00:40:17,960
same key in the same batch I the only

00:40:16,490 --> 00:40:19,370
thing that we were attempting to do is

00:40:17,960 --> 00:40:21,350
just make sure that visitors were routed

00:40:19,370 --> 00:40:23,630
the same way and just that we'd end up

00:40:21,350 --> 00:40:26,570
with the same visitor ideas on the same

00:40:23,630 --> 00:40:27,800
spark notes so and I think it could be a

00:40:26,570 --> 00:40:30,800
problem though if you don't have the key

00:40:27,800 --> 00:40:32,210
there you kind of not gonna join and so

00:40:30,800 --> 00:40:36,770
we handled it by trying to route visitor

00:40:32,210 --> 00:40:40,610
IDs I believe we still have time for one

00:40:36,770 --> 00:40:43,090
last question if no thank you very much

00:40:40,610 --> 00:40:43,090

YouTube URL: https://www.youtube.com/watch?v=1QLVMwtoDog


