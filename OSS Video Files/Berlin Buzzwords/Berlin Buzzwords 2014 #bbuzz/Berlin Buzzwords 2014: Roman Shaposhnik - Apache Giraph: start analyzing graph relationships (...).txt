Title: Berlin Buzzwords 2014: Roman Shaposhnik - Apache Giraph: start analyzing graph relationships (...)
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Roman Shaposhnik talking about "Apache Giraph: start analyzing graph relationships in your big data in 45 minutes"

The genesis of Hadoop was in analyzing massive amounts of data with a mapreduce framework. SQL-­on-­Hadoop has followed shortly after that, paving a way to the whole schema­on­read notion. Discovering graph relationship in your data is the next logical step. Apache Giraph (modeled on Google’s Pregel) lets you apply the power of BSP approach to the unstructured data. 

In this talk we will focus on practical advice of how to get up and running with Apache Giraph in minutes, start analyzing simple data sets with built-­in algorithms and finally how to implement your own graph processing using the APIs provided by the project. We will then dive into how Giraph integrates with the Hadoop ecosystem projects (Hive, HBase, Accumulo, etc.) and will also provide a whirlwind tour of Giraph architecture. 

Read more:
https://2014.berlinbuzzwords.de/session/apache-giraph-start-analyzing-graph-relationships-your-bigdata-45-minutes-or-your-money-back

About Roman Shaposhnik:
https://2014.berlinbuzzwords.de/user/214/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,919 --> 00:00:12,289
all right hello thank you for coming to

00:00:10,639 --> 00:00:15,830
my talk today we will be talking about

00:00:12,289 --> 00:00:17,510
apache giraffe and this will be a

00:00:15,830 --> 00:00:19,100
presentation you know which is a

00:00:17,510 --> 00:00:23,330
combination of sort of graph processing

00:00:19,100 --> 00:00:24,740
and big data a lot of times you know you

00:00:23,330 --> 00:00:26,330
can actually do very interesting things

00:00:24,740 --> 00:00:27,680
with small crafts but you know giraffe

00:00:26,330 --> 00:00:30,949
is particularly good when you actually

00:00:27,680 --> 00:00:32,660
have sort of big data in mind so before

00:00:30,949 --> 00:00:35,510
I actually start talking about giraffe

00:00:32,660 --> 00:00:38,479
itself let me just introduce myself so

00:00:35,510 --> 00:00:40,100
this is me I've been at the SF you know

00:00:38,479 --> 00:00:42,140
for quite some time so doing you know

00:00:40,100 --> 00:00:44,629
various things used to be at closed era

00:00:42,140 --> 00:00:47,929
doing Hadoop you know before that at

00:00:44,629 --> 00:00:49,639
Yahoo and I actually have two pitches

00:00:47,929 --> 00:00:52,339
you know so I'll just get them out of

00:00:49,639 --> 00:00:54,109
the way we will continue with the

00:00:52,339 --> 00:00:56,210
presentation so I am also one of the

00:00:54,109 --> 00:00:58,729
three authors in the you know upcoming

00:00:56,210 --> 00:01:01,280
book called giraffe in action from

00:00:58,729 --> 00:01:03,859
Manning so you know support hungry

00:01:01,280 --> 00:01:05,960
authors you know by our book let's pitch

00:01:03,859 --> 00:01:07,760
number one and pitch number 2 i'm

00:01:05,960 --> 00:01:11,030
working for the company called pivotal

00:01:07,760 --> 00:01:13,100
we're pretty interesting startup company

00:01:11,030 --> 00:01:15,710
you know doing a big data and

00:01:13,100 --> 00:01:17,420
platform-as-a-service things and i'm

00:01:15,710 --> 00:01:20,180
really hiring so i'm actually building a

00:01:17,420 --> 00:01:23,450
team of people who can help me figuring

00:01:20,180 --> 00:01:24,860
out next generation platform so yeah if

00:01:23,450 --> 00:01:26,270
you're interested you know catch me

00:01:24,860 --> 00:01:28,820
after the presentation and that was my

00:01:26,270 --> 00:01:31,870
second pitch so let's actually get right

00:01:28,820 --> 00:01:33,950
to the giraffe business so in this

00:01:31,870 --> 00:01:36,200
presentation I will actually try to talk

00:01:33,950 --> 00:01:39,320
a little bit about you know where it all

00:01:36,200 --> 00:01:41,030
came from and sort of backgrounds of you

00:01:39,320 --> 00:01:44,570
know Hadoop and big data processing how

00:01:41,030 --> 00:01:46,280
it relates to graphs how you know graph

00:01:44,570 --> 00:01:48,500
workloads actually tend to be different

00:01:46,280 --> 00:01:49,990
compared to MapReduce will introduce

00:01:48,500 --> 00:01:53,240
this thing called bulk synchronous

00:01:49,990 --> 00:01:54,380
parallel you know for those of you who

00:01:53,240 --> 00:01:56,990
don't know it you know it will be a

00:01:54,380 --> 00:01:58,550
primer on what it is and then we will

00:01:56,990 --> 00:02:01,670
talk about in a browser I've implemented

00:01:58,550 --> 00:02:05,450
and I will also try to show you code you

00:02:01,670 --> 00:02:06,500
know samples so my real goal that at the

00:02:05,450 --> 00:02:08,209
end of this presentation you can

00:02:06,500 --> 00:02:09,349
actually start writing your graph

00:02:08,209 --> 00:02:11,319
processing applications in top of

00:02:09,349 --> 00:02:16,340
Sheriff because it's really quite simple

00:02:11,319 --> 00:02:18,650
so let's get down to business as all you

00:02:16,340 --> 00:02:19,350
know of us now doc cutting was the

00:02:18,650 --> 00:02:22,530
original implementation

00:02:19,350 --> 00:02:25,860
of Hadoop Hadoop consisted of HDFS and

00:02:22,530 --> 00:02:27,990
MapReduce but he duped really came from

00:02:25,860 --> 00:02:30,030
google papers so what Doug did he

00:02:27,990 --> 00:02:32,250
basically read two papers one on gfs in

00:02:30,030 --> 00:02:35,490
a Google file system and another one on

00:02:32,250 --> 00:02:38,820
MapReduce and he implemented both in the

00:02:35,490 --> 00:02:40,770
free and open source java based Hadoop

00:02:38,820 --> 00:02:42,750
thing so if you think about you know

00:02:40,770 --> 00:02:45,810
where gfs and MapReduce sort of what

00:02:42,750 --> 00:02:49,770
kind of constraints or what kind of you

00:02:45,810 --> 00:02:53,550
know use cases they were aiming to solve

00:02:49,770 --> 00:02:55,410
at Google gfs was really meant to be you

00:02:53,550 --> 00:02:57,780
know fully highly distributed so it's

00:02:55,410 --> 00:02:59,340
kind of like scale-out storage it was

00:02:57,780 --> 00:03:01,470
meant to be replicated because you know

00:02:59,340 --> 00:03:03,180
it was again them to run on the

00:03:01,470 --> 00:03:06,330
commodity hardware which fails all the

00:03:03,180 --> 00:03:07,710
time and in order to achieve the first

00:03:06,330 --> 00:03:10,320
two they basically had to make a

00:03:07,710 --> 00:03:12,060
compromise by not making by not making a

00:03:10,320 --> 00:03:13,640
deposit file system so gfs actually

00:03:12,060 --> 00:03:17,100
happens to be an on politics file system

00:03:13,640 --> 00:03:19,110
one of the biggest limitations in gfs

00:03:17,100 --> 00:03:21,090
and you know hence HDFS you cannot

00:03:19,110 --> 00:03:23,430
really for example seek in the file and

00:03:21,090 --> 00:03:25,020
then right you know you can only keep

00:03:23,430 --> 00:03:26,460
writing to a file you know maybe you can

00:03:25,020 --> 00:03:30,300
keep a pending depending on the version

00:03:26,460 --> 00:03:33,150
of Hadoop but most of the time your data

00:03:30,300 --> 00:03:34,710
is you know sort of right and then read

00:03:33,150 --> 00:03:36,870
multiple times and read is actually

00:03:34,710 --> 00:03:38,670
optimized for streaming so if you're

00:03:36,870 --> 00:03:41,790
trying to seek in the file all the time

00:03:38,670 --> 00:03:44,430
again not a good idea a streaming and

00:03:41,790 --> 00:03:47,760
you know scale out is what gfs and HDFS

00:03:44,430 --> 00:03:50,490
are really good at on top of the file

00:03:47,760 --> 00:03:52,740
system google and you know hadoop by

00:03:50,490 --> 00:03:55,320
extension implemented the compute

00:03:52,740 --> 00:03:56,850
framework called MapReduce with a big

00:03:55,320 --> 00:03:58,920
inside that you know you don't really

00:03:56,850 --> 00:04:00,180
have to get your data to where the

00:03:58,920 --> 00:04:02,280
computation is you know if your

00:04:00,180 --> 00:04:03,810
computation is portable and that was

00:04:02,280 --> 00:04:05,820
part of the reason you know Hadoop aziah

00:04:03,810 --> 00:04:07,350
actually implemented in Java you can

00:04:05,820 --> 00:04:08,520
actually push your computation to where

00:04:07,350 --> 00:04:10,080
your data is because you know with

00:04:08,520 --> 00:04:11,640
Charlie doesn't really matter you know

00:04:10,080 --> 00:04:14,700
where you compute as long as there is a

00:04:11,640 --> 00:04:16,739
JVM on that node so MapReduce was meant

00:04:14,700 --> 00:04:18,930
to be distributed it was also meant to

00:04:16,739 --> 00:04:20,700
be batch oriented in a sense that you

00:04:18,930 --> 00:04:23,280
would actually expect your jobs to take

00:04:20,700 --> 00:04:27,450
a long time and produce sort of a final

00:04:23,280 --> 00:04:29,310
result and at the end of the day you

00:04:27,450 --> 00:04:32,220
know the algorithm that you would run on

00:04:29,310 --> 00:04:33,090
MapReduce by and large tend to be

00:04:32,220 --> 00:04:35,730
embarrassingly parallel

00:04:33,090 --> 00:04:38,310
well so you can share the computation as

00:04:35,730 --> 00:04:39,660
wide as possible and it is only during

00:04:38,310 --> 00:04:41,130
the reduced phase where you would

00:04:39,660 --> 00:04:42,840
actually combine you know some of the

00:04:41,130 --> 00:04:44,580
results from different computational

00:04:42,840 --> 00:04:46,050
units so as you can see I mean there are

00:04:44,580 --> 00:04:48,389
quite a few limitations but amazingly

00:04:46,050 --> 00:04:50,520
enough the map videos has been a really

00:04:48,389 --> 00:04:51,810
successful framework so there is all

00:04:50,520 --> 00:04:53,610
sorts of you know computational

00:04:51,810 --> 00:04:55,500
paradigms that map into it well

00:04:53,610 --> 00:04:59,070
unfortunately referencing is just not

00:04:55,500 --> 00:05:01,620
one of them so one size really doesn't

00:04:59,070 --> 00:05:04,560
fit all in that reduce you know you have

00:05:01,620 --> 00:05:06,330
this key value approach so map is how we

00:05:04,560 --> 00:05:08,760
get the key so you extract the keys from

00:05:06,330 --> 00:05:10,169
the data and that's the math phase then

00:05:08,760 --> 00:05:12,270
there is a shuffle phase essentially you

00:05:10,169 --> 00:05:14,940
know soaring the keys and the reduced

00:05:12,270 --> 00:05:16,889
phase is you know a single bit of code

00:05:14,940 --> 00:05:19,470
gets to see all of the values

00:05:16,889 --> 00:05:22,200
corresponding to a single key in one go

00:05:19,470 --> 00:05:24,180
so that's you know the reduce phase but

00:05:22,200 --> 00:05:27,650
in graphs it's not really even clear

00:05:24,180 --> 00:05:30,150
what would be you know a natural sort of

00:05:27,650 --> 00:05:32,100
key you know well I guess you can come

00:05:30,150 --> 00:05:34,760
up with some but it doesn't really lend

00:05:32,100 --> 00:05:38,060
itself easily to that type of processing

00:05:34,760 --> 00:05:41,070
so MapReduce also has this idea of

00:05:38,060 --> 00:05:42,960
pipelining in a sense that once a phase

00:05:41,070 --> 00:05:45,360
of map reducing you know map shuffle

00:05:42,960 --> 00:05:47,910
reduces done the only place where you

00:05:45,360 --> 00:05:50,789
would store result is an HDFS all of the

00:05:47,910 --> 00:05:52,620
you know very at times complex Maori

00:05:50,789 --> 00:05:54,599
states that you've created but you know

00:05:52,620 --> 00:05:56,430
for doing all these three phases is now

00:05:54,599 --> 00:05:58,979
gone so if you actually have you know

00:05:56,430 --> 00:06:01,830
multistage MapReduce job you basically

00:05:58,979 --> 00:06:03,750
will keep going in and out of HDFS all

00:06:01,830 --> 00:06:05,460
the time for intermediate results which

00:06:03,750 --> 00:06:07,110
is a little bit like you know imagine if

00:06:05,460 --> 00:06:08,460
UNIX didn't have pipes right you know

00:06:07,110 --> 00:06:09,810
you would basically have to create you

00:06:08,460 --> 00:06:11,910
know temporary files all the time you

00:06:09,810 --> 00:06:13,410
know not really efficient and especially

00:06:11,910 --> 00:06:16,310
if you create again non-trivial memory

00:06:13,410 --> 00:06:18,690
state you know not really good idea and

00:06:16,310 --> 00:06:22,260
again MapReduce is a very particular API

00:06:18,690 --> 00:06:24,630
for working with your data there been a

00:06:22,260 --> 00:06:27,060
couple of attempts to actually map graph

00:06:24,630 --> 00:06:30,090
processing into MapReduce the craziest

00:06:27,060 --> 00:06:32,250
attempt I've seen was at Facebook so

00:06:30,090 --> 00:06:34,080
before sheriff existed at Facebook they

00:06:32,250 --> 00:06:36,330
actually needed to you know analyze the

00:06:34,080 --> 00:06:38,370
social graph and the only thing they had

00:06:36,330 --> 00:06:40,620
was high so high with a sequel on top of

00:06:38,370 --> 00:06:43,860
Hadoop so there was this really smart

00:06:40,620 --> 00:06:45,539
dude who used to do c++ who came up with

00:06:43,860 --> 00:06:46,590
this idea that you know literally all

00:06:45,539 --> 00:06:48,480
graft

00:06:46,590 --> 00:06:50,940
could be mapped into a linear algebra

00:06:48,480 --> 00:06:53,010
you know you can basically do pretty

00:06:50,940 --> 00:06:54,420
much anything using you know a jason c

00:06:53,010 --> 00:06:56,430
matrix and you know operations from the

00:06:54,420 --> 00:06:58,920
adjacency matrix but when he came up

00:06:56,430 --> 00:07:01,290
with Dan was he could actually map

00:06:58,920 --> 00:07:04,260
linear algebra into sequel you know in a

00:07:01,290 --> 00:07:05,880
very very convoluted way but it worked

00:07:04,260 --> 00:07:07,710
and you know the kind of sequel that he

00:07:05,880 --> 00:07:09,090
showed me which is like I haven't seen

00:07:07,710 --> 00:07:11,880
that type of sequel you know being

00:07:09,090 --> 00:07:13,350
produced by like you know hibernate you

00:07:11,880 --> 00:07:14,490
know it's really mind-boggling but

00:07:13,350 --> 00:07:16,320
apparently it works so they actually

00:07:14,490 --> 00:07:18,740
started analyzing their social graph way

00:07:16,320 --> 00:07:21,090
back well before you're off using hive

00:07:18,740 --> 00:07:24,240
but of course you know giraffe is a much

00:07:21,090 --> 00:07:26,370
more natural fit so let's actually talk

00:07:24,240 --> 00:07:28,530
a little bit about you know why is it

00:07:26,370 --> 00:07:33,450
different with graphs compare it to the

00:07:28,530 --> 00:07:35,910
your typical big data use case so when

00:07:33,450 --> 00:07:37,320
you store unstructured data in HDFS and

00:07:35,910 --> 00:07:39,780
that's what you know it's really all

00:07:37,320 --> 00:07:41,850
about you talk about tuples right and

00:07:39,780 --> 00:07:43,890
these samples are essentially you know

00:07:41,850 --> 00:07:45,720
points in a some kind of a space you

00:07:43,890 --> 00:07:47,040
know solution typically space and it

00:07:45,720 --> 00:07:48,810
can't describe you know those tuples can

00:07:47,040 --> 00:07:51,150
describe your customer data your product

00:07:48,810 --> 00:07:53,520
data your interaction data but when you

00:07:51,150 --> 00:07:55,530
start talking graphs what you're really

00:07:53,520 --> 00:07:57,720
doing you're basically coming up with

00:07:55,530 --> 00:08:00,000
connection between those doubles and two

00:07:57,720 --> 00:08:02,310
things happen first of all unlike in

00:08:00,000 --> 00:08:04,590
most traditional graph databases you

00:08:02,310 --> 00:08:07,080
actually do just that you come up with

00:08:04,590 --> 00:08:09,270
those connections upfront you might not

00:08:07,080 --> 00:08:11,130
even know that you know two tuples might

00:08:09,270 --> 00:08:12,990
necessarily be connected by some kind of

00:08:11,130 --> 00:08:14,760
an edge it may actually be knowledge

00:08:12,990 --> 00:08:16,890
that you somehow you know derive later

00:08:14,760 --> 00:08:21,150
on by the time that all the data is

00:08:16,890 --> 00:08:22,950
stored in your HDFS and as we all know

00:08:21,150 --> 00:08:25,290
you know Big Data is all about sort of

00:08:22,950 --> 00:08:28,290
growth but if your data size growth

00:08:25,290 --> 00:08:29,820
linearly your connection sort of edge

00:08:28,290 --> 00:08:32,370
you know number actually grows

00:08:29,820 --> 00:08:33,900
exponentially so even storing all of the

00:08:32,370 --> 00:08:35,190
edges would not be a good idea you

00:08:33,900 --> 00:08:37,320
actually have to come up with what the

00:08:35,190 --> 00:08:41,010
edges are you know most of the time sort

00:08:37,320 --> 00:08:43,260
of on the fly and that's what really to

00:08:41,010 --> 00:08:45,240
me is different about applying graph

00:08:43,260 --> 00:08:47,280
processing techniques to Big Data right

00:08:45,240 --> 00:08:49,680
you know you don't actually up front

00:08:47,280 --> 00:08:51,810
know what the connections are you

00:08:49,680 --> 00:08:54,210
actually extract them sort of based on

00:08:51,810 --> 00:08:56,130
some kind of you know model and it may

00:08:54,210 --> 00:08:57,930
very well be that you know one run on

00:08:56,130 --> 00:08:59,640
the same data will actually come up with

00:08:57,930 --> 00:09:01,140
one set of edges and

00:08:59,640 --> 00:09:02,370
front run on the very same set of data

00:09:01,140 --> 00:09:03,750
would come up with a different set of

00:09:02,370 --> 00:09:05,400
edges and a good example of that would

00:09:03,750 --> 00:09:07,410
be you know suppose you are trying to

00:09:05,400 --> 00:09:08,970
match customers to products right so you

00:09:07,410 --> 00:09:10,380
might actually have a social graph of

00:09:08,970 --> 00:09:11,610
your customers and that's just fine you

00:09:10,380 --> 00:09:13,650
essentially connecting all the tuples

00:09:11,610 --> 00:09:15,120
you know between the customers but then

00:09:13,650 --> 00:09:16,560
you know the next iteration could be you

00:09:15,120 --> 00:09:18,300
could actually be matching customers to

00:09:16,560 --> 00:09:20,130
the products and then you know the very

00:09:18,300 --> 00:09:23,460
sort of same data set would produce you

00:09:20,130 --> 00:09:25,230
know different drafts so what kind of

00:09:23,460 --> 00:09:26,970
challenges do we actually have while

00:09:25,230 --> 00:09:28,860
doing it well again data is dynamic so

00:09:26,970 --> 00:09:34,980
there's no way of doing sort of scheme

00:09:28,860 --> 00:09:37,770
on right the algorithms that we tend to

00:09:34,980 --> 00:09:39,180
run again tend to be explorative and

00:09:37,770 --> 00:09:41,310
iterative like I said I mean you might

00:09:39,180 --> 00:09:43,320
actually load up the data you know come

00:09:41,310 --> 00:09:44,640
up with one set of you know edges but

00:09:43,320 --> 00:09:46,080
then it will sound like oh man I

00:09:44,640 --> 00:09:48,150
actually need a different set of edges

00:09:46,080 --> 00:09:49,830
and it would be really wastefully if you

00:09:48,150 --> 00:09:52,170
actually had to reload all of the data

00:09:49,830 --> 00:09:55,920
you know just like mapreduce makes you

00:09:52,170 --> 00:09:58,260
do so we have to do something else and

00:09:55,920 --> 00:10:01,290
you know you can actually use graph

00:09:58,260 --> 00:10:03,750
database which is one of the things that

00:10:01,290 --> 00:10:08,340
you know I believe there's a few our

00:10:03,750 --> 00:10:10,350
talks in this track and you know they're

00:10:08,340 --> 00:10:12,480
good in the sense that I cannot really

00:10:10,350 --> 00:10:13,740
use them that much but you know my

00:10:12,480 --> 00:10:15,870
friends tell me that you know if you

00:10:13,740 --> 00:10:17,430
really need neo4j you know where to get

00:10:15,870 --> 00:10:22,760
it where to moving on it's it's

00:10:17,430 --> 00:10:24,900
available their benefits you know to me

00:10:22,760 --> 00:10:26,280
the biggest benefit is that it's a

00:10:24,900 --> 00:10:27,900
self-contained system right you know if

00:10:26,280 --> 00:10:29,060
you install it if you maintain a time in

00:10:27,900 --> 00:10:31,050
it kinda gives you what you need right

00:10:29,060 --> 00:10:32,430
but the short coming is it is a

00:10:31,050 --> 00:10:34,050
self-contained system so like the

00:10:32,430 --> 00:10:36,060
internal state of that system can only

00:10:34,050 --> 00:10:38,310
be gotten you know from calling the

00:10:36,060 --> 00:10:40,620
api's that that system provides so

00:10:38,310 --> 00:10:41,760
unlike what you're off is doing where it

00:10:40,620 --> 00:10:43,560
basically just looks at your

00:10:41,760 --> 00:10:46,230
unstructured data wherever the data is

00:10:43,560 --> 00:10:48,510
you actually have to feed the data into

00:10:46,230 --> 00:10:50,310
the graph database first and only then

00:10:48,510 --> 00:10:51,870
you can work with it but again the flip

00:10:50,310 --> 00:10:53,640
side is you know it's highly optimized

00:10:51,870 --> 00:10:54,810
for that type of workload so there is

00:10:53,640 --> 00:10:57,660
all sorts of interesting things that

00:10:54,810 --> 00:11:00,450
they do it really depends on your use

00:10:57,660 --> 00:11:02,700
case if you can define a schema for your

00:11:00,450 --> 00:11:03,960
graph up front I mean there's no reason

00:11:02,700 --> 00:11:05,630
not to try you know something like this

00:11:03,960 --> 00:11:08,160
but if your scheme is not actually

00:11:05,630 --> 00:11:10,470
definable giraffe is pretty much the

00:11:08,160 --> 00:11:11,700
only game in town for now actually it's

00:11:10,470 --> 00:11:13,110
not I mean there is also a project

00:11:11,700 --> 00:11:15,630
called Apache Hama

00:11:13,110 --> 00:11:17,250
but I'm not sure how active it is it

00:11:15,630 --> 00:11:20,670
used to be pretty active but now that

00:11:17,250 --> 00:11:22,320
Facebook invested in giraffe basically a

00:11:20,670 --> 00:11:23,790
few engineers just full time you know

00:11:22,320 --> 00:11:25,890
optimizing and working in it I think

00:11:23,790 --> 00:11:27,390
Sheriff is way ahead of you know most of

00:11:25,890 --> 00:11:30,600
the graph processing tools on top of

00:11:27,390 --> 00:11:34,320
Hadoop Sookie insides that you have

00:11:30,600 --> 00:11:36,779
brought to this problem is well let's

00:11:34,320 --> 00:11:38,339
keep a state in memory for as long as we

00:11:36,779 --> 00:11:39,959
actually need it so there is absolutely

00:11:38,339 --> 00:11:42,089
no reason to throw away that state you

00:11:39,959 --> 00:11:43,850
know that you know what MapReduce does

00:11:42,089 --> 00:11:46,260
we can actually keep it in memory and

00:11:43,850 --> 00:11:47,760
that makes it interesting from the

00:11:46,260 --> 00:11:49,470
implementation side because you're off

00:11:47,760 --> 00:11:52,140
actually happens to be the first

00:11:49,470 --> 00:11:54,870
framework that can leverage this next

00:11:52,140 --> 00:11:56,899
generation who do scheduling and you

00:11:54,870 --> 00:11:59,339
know provisioning capability called yarn

00:11:56,899 --> 00:12:00,810
where it's not really about fitting

00:11:59,339 --> 00:12:03,149
everything into MapReduce you actually

00:12:00,810 --> 00:12:04,769
can have different frameworks running on

00:12:03,149 --> 00:12:06,660
top of the very same could do so

00:12:04,769 --> 00:12:10,410
MapReduce just being one of them giraffe

00:12:06,660 --> 00:12:12,390
would be a different framework we are

00:12:10,410 --> 00:12:14,010
leveraging HDFS as I said you know the

00:12:12,390 --> 00:12:17,040
repository from structured data but the

00:12:14,010 --> 00:12:18,449
biggest sort of insight is how do we

00:12:17,040 --> 00:12:21,540
actually do the computation and that's

00:12:18,449 --> 00:12:23,699
you know BSP so what's BSB BSB stands

00:12:21,540 --> 00:12:27,899
for bulk synchronous parallel and it's

00:12:23,699 --> 00:12:29,579
really a very simple idea of how you can

00:12:27,899 --> 00:12:32,190
achieve a middle ground between

00:12:29,579 --> 00:12:34,529
something as restrictive as MapReduce

00:12:32,190 --> 00:12:37,260
where there's absolutely you know few

00:12:34,529 --> 00:12:38,910
points of communicating any kind of data

00:12:37,260 --> 00:12:40,110
between you know workers again if you

00:12:38,910 --> 00:12:42,060
think about it I mean the only two

00:12:40,110 --> 00:12:44,699
points where communication happens is

00:12:42,060 --> 00:12:46,380
once mappers are done you know the data

00:12:44,699 --> 00:12:47,910
needs to be sorted that's the one you

00:12:46,380 --> 00:12:49,440
know one point of communication and once

00:12:47,910 --> 00:12:51,899
the data is sorted that data needs to be

00:12:49,440 --> 00:12:54,269
streamed to reducers and that's the only

00:12:51,899 --> 00:12:57,269
other point of communication so suppose

00:12:54,269 --> 00:12:59,510
we want you know to have workers be able

00:12:57,269 --> 00:13:01,829
to communicate with each other so a

00:12:59,510 --> 00:13:04,019
different extreme would be to let them

00:13:01,829 --> 00:13:05,670
communicate absolutely freely you know

00:13:04,019 --> 00:13:08,519
at any time anything can send a message

00:13:05,670 --> 00:13:10,829
to anything else and the only thing that

00:13:08,519 --> 00:13:13,500
you are waiting in such a system is you

00:13:10,829 --> 00:13:16,050
know a bunch of deadlocks you know those

00:13:13,500 --> 00:13:19,769
of you who've done MPI know what I'm

00:13:16,050 --> 00:13:22,170
talking about ah so what's the middle

00:13:19,769 --> 00:13:24,089
ground well bulk synchronous parallel

00:13:22,170 --> 00:13:25,440
you know BSP offers the middle ground

00:13:24,089 --> 00:13:27,560
where you actually partition your

00:13:25,440 --> 00:13:30,180
communication on computation phases

00:13:27,560 --> 00:13:33,450
basically separating them by by barriers

00:13:30,180 --> 00:13:35,220
so you have as many local processing you

00:13:33,450 --> 00:13:37,440
know within the unit's done as possible

00:13:35,220 --> 00:13:39,990
but once all of the processing is done

00:13:37,440 --> 00:13:41,940
that's when you hit barrier number two

00:13:39,990 --> 00:13:44,430
right so a barrier number one is when

00:13:41,940 --> 00:13:46,260
everything starts computing theory or

00:13:44,430 --> 00:13:48,570
number two is when everything is done

00:13:46,260 --> 00:13:50,610
doing the local computation and between

00:13:48,570 --> 00:13:52,260
barrier number two and beer number three

00:13:50,610 --> 00:13:53,580
there is an absolutely unrestricted

00:13:52,260 --> 00:13:55,560
communication happening you know

00:13:53,580 --> 00:13:58,020
essentially Massachusetts and but there

00:13:55,560 --> 00:13:59,850
is no computation happening at all so

00:13:58,020 --> 00:14:01,710
the computation will start happening

00:13:59,850 --> 00:14:03,750
once all of the messages get delivered

00:14:01,710 --> 00:14:05,310
and the computation will take into

00:14:03,750 --> 00:14:07,290
account the messages that were delivered

00:14:05,310 --> 00:14:09,240
to the local state but while the

00:14:07,290 --> 00:14:10,800
communication is in flight no

00:14:09,240 --> 00:14:12,060
computation has happened so you

00:14:10,800 --> 00:14:13,350
basically have this you know kind of

00:14:12,060 --> 00:14:15,150
like kitchen giving States right you

00:14:13,350 --> 00:14:16,560
know you do computation the messages

00:14:15,150 --> 00:14:18,630
then computation the message is the

00:14:16,560 --> 00:14:22,170
computation and messages and it actually

00:14:18,630 --> 00:14:23,820
happens to be a rather nice model for

00:14:22,170 --> 00:14:26,580
graph processing so let's actually try

00:14:23,820 --> 00:14:28,770
to apply two graphs so this is an

00:14:26,580 --> 00:14:31,860
example graph of my Twitter you know

00:14:28,770 --> 00:14:34,500
network so my twitter handle is air

00:14:31,860 --> 00:14:36,030
harder and I'm falling you know apache

00:14:34,500 --> 00:14:38,370
software foundation obviously so there

00:14:36,030 --> 00:14:41,360
is a friend of mine Constantine who I

00:14:38,370 --> 00:14:44,520
you know we both follow each other and

00:14:41,360 --> 00:14:47,730
he also is you know following SF but SF

00:14:44,520 --> 00:14:49,740
is not following anybody so with BS be

00:14:47,730 --> 00:14:51,960
applied to graphs you know at every

00:14:49,740 --> 00:14:54,450
given vertex is where you have your

00:14:51,960 --> 00:14:55,830
local computational sort of power right

00:14:54,450 --> 00:14:58,080
you know you actually have to think like

00:14:55,830 --> 00:15:00,510
a vertex so what does a vertex think

00:14:58,080 --> 00:15:02,700
well verdicts nose and things you know

00:15:00,510 --> 00:15:04,950
it's local state basically has control

00:15:02,700 --> 00:15:07,050
over any kind of you know local state

00:15:04,950 --> 00:15:10,350
and memory it can create just happens to

00:15:07,050 --> 00:15:12,180
be a java thread right I it knows its

00:15:10,350 --> 00:15:14,160
neighbors so the verdicts knows you know

00:15:12,180 --> 00:15:15,930
who who the vertex is connected to and

00:15:14,160 --> 00:15:17,220
it can traverse the network of the

00:15:15,930 --> 00:15:20,460
neighbors but it does not necessarily

00:15:17,220 --> 00:15:21,720
know who is connected to the verdicts so

00:15:20,460 --> 00:15:23,310
you can basically get all of the

00:15:21,720 --> 00:15:26,100
outgoing edges but you don't know

00:15:23,310 --> 00:15:28,290
incoming edges so verdicts can also send

00:15:26,100 --> 00:15:30,720
a message to just about anything you

00:15:28,290 --> 00:15:32,340
know within the network so there does

00:15:30,720 --> 00:15:33,870
not need to be a connection for a

00:15:32,340 --> 00:15:36,200
message to arrive from point A to point

00:15:33,870 --> 00:15:39,050
B you can just send arbitrary messages

00:15:36,200 --> 00:15:41,160
verdicts can declare that it's done and

00:15:39,050 --> 00:15:43,079
amazingly enough

00:15:41,160 --> 00:15:45,300
verdicts can actually mutate the graph

00:15:43,079 --> 00:15:47,040
topology so it would not be out of the

00:15:45,300 --> 00:15:49,290
question to start with absolutely no

00:15:47,040 --> 00:15:50,699
vertices and just build out the complete

00:15:49,290 --> 00:15:52,649
graph in memory you know based on

00:15:50,699 --> 00:15:54,089
certain criteria because you know a

00:15:52,649 --> 00:15:57,269
single products can't just build you

00:15:54,089 --> 00:16:00,029
know portions of the graph that topology

00:15:57,269 --> 00:16:02,250
is considered to be sort of most of the

00:16:00,029 --> 00:16:03,629
time at least is considered to be kind

00:16:02,250 --> 00:16:05,579
of like messaging so it will be

00:16:03,629 --> 00:16:07,649
available to the workers you know for

00:16:05,579 --> 00:16:10,800
the next step so they will not start

00:16:07,649 --> 00:16:14,790
computing immediately so these five

00:16:10,800 --> 00:16:21,569
points is the fool giraffe sort of API

00:16:14,790 --> 00:16:23,370
right there basically BSP happens like I

00:16:21,569 --> 00:16:24,750
said at the level of individual vertices

00:16:23,370 --> 00:16:27,180
then there is a bunch of you know

00:16:24,750 --> 00:16:31,199
message delivery and it keeps keeps

00:16:27,180 --> 00:16:32,939
going on so without further ado here is

00:16:31,199 --> 00:16:34,889
the first you know you're off hello

00:16:32,939 --> 00:16:36,509
world so what this little snippet of

00:16:34,889 --> 00:16:38,519
code does well first of all it shows

00:16:36,509 --> 00:16:40,139
that Java makes you right tons of

00:16:38,519 --> 00:16:43,439
boilerplate but you know we'll get to it

00:16:40,139 --> 00:16:46,529
in a minute but you know the real bit

00:16:43,439 --> 00:16:50,910
happens right here so we basically have

00:16:46,529 --> 00:16:53,160
extended the basic computation abstract

00:16:50,910 --> 00:16:55,649
class which is an entry entry point into

00:16:53,160 --> 00:16:57,509
writing anything in Sheriff you know and

00:16:55,649 --> 00:16:59,550
the only method that we actually have to

00:16:57,509 --> 00:17:01,920
refine to make it work is compute so

00:16:59,550 --> 00:17:03,930
compute is what happens at the level of

00:17:01,920 --> 00:17:05,669
fish individual verdicts so we didn't

00:17:03,930 --> 00:17:07,949
compute like I said I mean we can do I

00:17:05,669 --> 00:17:11,459
don't know println so we can train the

00:17:07,949 --> 00:17:13,439
our ID you know who we are and then we

00:17:11,459 --> 00:17:15,059
can iterate over all of the edges you

00:17:13,439 --> 00:17:17,579
know that are outgoing edges from this

00:17:15,059 --> 00:17:19,770
vertex you know printing the IDS of the

00:17:17,579 --> 00:17:21,600
neighbors we're connected to so this is

00:17:19,770 --> 00:17:23,220
basically the smallest I guess you know

00:17:21,600 --> 00:17:28,169
think that you can do with your off

00:17:23,220 --> 00:17:30,960
pretty simple actually did get cut out

00:17:28,169 --> 00:17:34,020
so at the end there is also a halting

00:17:30,960 --> 00:17:36,990
state so hopefully on the further slides

00:17:34,020 --> 00:17:38,940
it will not get cut out so all of these

00:17:36,990 --> 00:17:41,640
variables all of these type variables

00:17:38,940 --> 00:17:43,770
that you actually have to write their

00:17:41,640 --> 00:17:46,559
very important and they define you know

00:17:43,770 --> 00:17:47,880
this sort of my t4 of Sheriff api's so

00:17:46,559 --> 00:17:49,620
first of all you basically have to

00:17:47,880 --> 00:17:52,320
declare a type of what is the vertex ID

00:17:49,620 --> 00:17:54,690
so vertex ID is what you use to actually

00:17:52,320 --> 00:17:55,080
reference the verdicts if you're trying

00:17:54,690 --> 00:17:56,399
to

00:17:55,080 --> 00:17:58,380
the message for example if you're trying

00:17:56,399 --> 00:18:00,390
to connect you know yourself or some

00:17:58,380 --> 00:18:02,220
other verdicts you actually have to know

00:18:00,390 --> 00:18:04,950
vortex ID this is how you reference them

00:18:02,220 --> 00:18:06,510
then products actually happens to have

00:18:04,950 --> 00:18:08,159
the data you know piece of data

00:18:06,510 --> 00:18:11,789
associated with it and that's the second

00:18:08,159 --> 00:18:13,230
type variable then you have to define

00:18:11,789 --> 00:18:18,090
what is the piece of data associated

00:18:13,230 --> 00:18:19,320
with each edge third one and fourth one

00:18:18,090 --> 00:18:21,179
what is the type of message that

00:18:19,320 --> 00:18:23,100
vertices will be sending to each other

00:18:21,179 --> 00:18:24,600
so you know for all together now of

00:18:23,100 --> 00:18:26,700
course if you don't if you are not

00:18:24,600 --> 00:18:28,200
interested in you know some subset of

00:18:26,700 --> 00:18:30,750
these api's you can just you know

00:18:28,200 --> 00:18:33,120
declare it now writable and speaking of

00:18:30,750 --> 00:18:35,370
rider balls the only thing that in this

00:18:33,120 --> 00:18:38,039
that needs to be something other than

00:18:35,370 --> 00:18:41,549
just writable is the first one vertex ID

00:18:38,039 --> 00:18:43,500
so it also needs to be comparable uh why

00:18:41,549 --> 00:18:46,559
because you basically your grant gets

00:18:43,500 --> 00:18:49,409
partitioned based on vortex IDs and the

00:18:46,559 --> 00:18:53,149
vertices get assigned to workers within

00:18:49,409 --> 00:18:55,470
your giraffe application based on the

00:18:53,149 --> 00:19:01,289
partitioning that happens at the level

00:18:55,470 --> 00:19:04,380
of the first type variable so let's

00:19:01,289 --> 00:19:06,510
actually take a step back and again like

00:19:04,380 --> 00:19:09,179
I showed you a bit of code but like what

00:19:06,510 --> 00:19:13,139
what is the input for that bit of code

00:19:09,179 --> 00:19:15,480
so where do ages and you know vertices

00:19:13,139 --> 00:19:17,850
come from when you actually start your

00:19:15,480 --> 00:19:18,990
giraffe application well like I said I

00:19:17,850 --> 00:19:21,330
mean you don't actually even have to

00:19:18,990 --> 00:19:23,279
define graph a front because you can

00:19:21,330 --> 00:19:25,169
actually build it in memory so you can

00:19:23,279 --> 00:19:27,570
you know your first sort of super step

00:19:25,169 --> 00:19:29,820
you know your first BSP step could be

00:19:27,570 --> 00:19:31,769
you just invent the graph right you know

00:19:29,820 --> 00:19:33,240
based on some criteria or whatever you

00:19:31,769 --> 00:19:35,940
just build in the memory and the next

00:19:33,240 --> 00:19:38,429
one the next step will have that apology

00:19:35,940 --> 00:19:39,389
as the working topology of the graph so

00:19:38,429 --> 00:19:41,429
that's one but it's not really

00:19:39,389 --> 00:19:42,840
particularly interesting what's really

00:19:41,429 --> 00:19:45,269
interesting is you know how you can

00:19:42,840 --> 00:19:47,070
extract the verdicts and edge

00:19:45,269 --> 00:19:49,559
information from the raw data that you

00:19:47,070 --> 00:19:51,360
have a new HDFS and for that you know

00:19:49,559 --> 00:19:54,600
giraffe follows a very Hadoop like

00:19:51,360 --> 00:19:56,909
approach ah so it defines edge input

00:19:54,600 --> 00:19:59,429
format and verdicts input format you

00:19:56,909 --> 00:20:02,159
know there is a built-in support for

00:19:59,429 --> 00:20:04,260
most of the storage subsystems that

00:20:02,159 --> 00:20:06,210
happen to give ailable on Hadoop so

00:20:04,260 --> 00:20:08,370
there is what is the one for HDFS there

00:20:06,210 --> 00:20:10,220
is one for each base and accumulo there

00:20:08,370 --> 00:20:13,110
is a you know backend for Gore ax

00:20:10,220 --> 00:20:14,570
high-wage catalog you know all the usual

00:20:13,110 --> 00:20:16,650
bits and pieces of the Hadoop ecosystem

00:20:14,570 --> 00:20:18,270
but you can write your own I mean that's

00:20:16,650 --> 00:20:19,980
that's that's absolutely possible so I

00:20:18,270 --> 00:20:23,370
mean I know that somebody wrote one for

00:20:19,980 --> 00:20:26,220
Cassandra and that should fine so the

00:20:23,370 --> 00:20:28,980
way it actually happens is I suppose you

00:20:26,220 --> 00:20:31,620
have you know some data in HDFS and for

00:20:28,980 --> 00:20:35,010
our simple example the type of data that

00:20:31,620 --> 00:20:37,230
I decided to use and the type of input

00:20:35,010 --> 00:20:38,880
format that i decided to specify is

00:20:37,230 --> 00:20:42,030
essentially an adjacency matrix you know

00:20:38,880 --> 00:20:45,750
just encoding the first integer here is

00:20:42,030 --> 00:20:47,970
the vertex ID and all the integer is

00:20:45,750 --> 00:20:51,870
following it are the verdicts ideas of

00:20:47,970 --> 00:20:53,790
the neighbors for this guy so suppose

00:20:51,870 --> 00:20:55,440
you have two files in HDFS you know one

00:20:53,790 --> 00:20:57,450
looking like this and the other one

00:20:55,440 --> 00:21:00,720
looking like that you give it to giraffe

00:20:57,450 --> 00:21:03,210
when you start it up input format kicks

00:21:00,720 --> 00:21:05,730
in input format builds the graph

00:21:03,210 --> 00:21:07,200
topology based on the vertex idea that

00:21:05,730 --> 00:21:08,910
graph topology gets partitioned between

00:21:07,200 --> 00:21:12,690
different workers you know the workers

00:21:08,910 --> 00:21:14,550
get assigned to different nodes bunch of

00:21:12,690 --> 00:21:17,429
computation happens up until the point

00:21:14,550 --> 00:21:20,970
where we don't have any messages left to

00:21:17,429 --> 00:21:23,160
be processed if we don't and if all of

00:21:20,970 --> 00:21:25,200
the vertices voted to halt so you know

00:21:23,160 --> 00:21:29,160
they are not doing any computation the

00:21:25,200 --> 00:21:31,290
entire giraffe application exits and if

00:21:29,160 --> 00:21:33,270
you specify output format at that point

00:21:31,290 --> 00:21:35,760
you can actually have your state of the

00:21:33,270 --> 00:21:38,490
graph written you know being flashed

00:21:35,760 --> 00:21:40,620
back into whatever storage you decide to

00:21:38,490 --> 00:21:42,240
use what's interesting is you don't

00:21:40,620 --> 00:21:44,610
actually have to use you know the same

00:21:42,240 --> 00:21:46,380
storage so in my example I'm taking data

00:21:44,610 --> 00:21:48,900
from HDFS and I'm putting data back into

00:21:46,380 --> 00:21:51,570
HDFS but this could have been HBase and

00:21:48,900 --> 00:21:56,250
that is you know fine to be HDFS so you

00:21:51,570 --> 00:21:58,170
can have any combinations so what really

00:21:56,250 --> 00:22:00,059
is what really is happening you have

00:21:58,170 --> 00:22:02,030
some kind of you know storage you have

00:22:00,059 --> 00:22:04,740
some kind of storage for output and

00:22:02,030 --> 00:22:07,950
these little guys they actually happen

00:22:04,740 --> 00:22:09,960
to be either yarn containers uh you know

00:22:07,950 --> 00:22:13,500
this is next generation Hadoop Hadoop

00:22:09,960 --> 00:22:16,290
two or the hack that refuses to run on

00:22:13,500 --> 00:22:19,140
just pure Hadoop is those can be mapped

00:22:16,290 --> 00:22:21,240
on the jobs so you know here's a cool

00:22:19,140 --> 00:22:23,100
pro tip of how you can abuse Hadoop

00:22:21,240 --> 00:22:24,540
you don't actually have to generate any

00:22:23,100 --> 00:22:26,250
Keys you know in your map where you can

00:22:24,540 --> 00:22:28,320
just sit there right and that's

00:22:26,250 --> 00:22:29,970
essentially what you're of that so it's

00:22:28,320 --> 00:22:31,650
it's forever and for as long as it does

00:22:29,970 --> 00:22:35,940
it you know Hadoop is happy because like

00:22:31,650 --> 00:22:38,130
you know something is running before we

00:22:35,940 --> 00:22:39,870
move along let's hash again just recap

00:22:38,130 --> 00:22:42,660
you know what is the vertex view so the

00:22:39,870 --> 00:22:45,240
vertex U is rather simple messages come

00:22:42,660 --> 00:22:47,190
in messages come out and by the way the

00:22:45,240 --> 00:22:48,840
message data you know for an interest of

00:22:47,190 --> 00:22:51,260
full disclosure could have different

00:22:48,840 --> 00:22:54,630
types for incoming and outgoing messages

00:22:51,260 --> 00:22:56,670
there are very few really obscure use

00:22:54,630 --> 00:22:58,170
cases where it's useful so for all of

00:22:56,670 --> 00:22:59,850
the practical purposes you can just

00:22:58,170 --> 00:23:01,980
assume that you know message data one

00:22:59,850 --> 00:23:03,900
type and message data to type are the

00:23:01,980 --> 00:23:05,580
same but other than that again you have

00:23:03,900 --> 00:23:07,590
vertex data associated with your

00:23:05,580 --> 00:23:10,530
verdicts you have vertex ID and you can

00:23:07,590 --> 00:23:14,160
outgoing a sort of arrows you know edge

00:23:10,530 --> 00:23:16,410
each labeled with edge data so for our

00:23:14,160 --> 00:23:18,270
next example I think I have a little bit

00:23:16,410 --> 00:23:20,130
of time left so far the next example let

00:23:18,270 --> 00:23:21,690
us actually do something interesting so

00:23:20,130 --> 00:23:23,400
we all know that the only difference

00:23:21,690 --> 00:23:27,360
between Twitter and Facebook is that one

00:23:23,400 --> 00:23:29,700
uses you know directional drafts and the

00:23:27,360 --> 00:23:32,940
other one doesn't but it's pretty easy

00:23:29,700 --> 00:23:34,530
to turn one and two the other and it's

00:23:32,940 --> 00:23:37,440
actually necessary operation because

00:23:34,530 --> 00:23:38,880
giraffe by default supports directional

00:23:37,440 --> 00:23:41,970
graphs right so if you want to simulate

00:23:38,880 --> 00:23:44,850
in you know something else you can just

00:23:41,970 --> 00:23:45,960
build additional edges you know into

00:23:44,850 --> 00:23:48,990
your graph and that's exactly what we

00:23:45,960 --> 00:23:51,900
will be doing here so again here's you

00:23:48,990 --> 00:23:54,560
know compute that we are redefining you

00:23:51,900 --> 00:23:56,730
know the same way we did for hello world

00:23:54,560 --> 00:23:59,130
first of all this is a very common

00:23:56,730 --> 00:24:01,020
pattern in giraffe application first of

00:23:59,130 --> 00:24:02,880
all we try to see what is the number of

00:24:01,020 --> 00:24:05,730
the super step super step is that block

00:24:02,880 --> 00:24:07,410
you know of bsp processing and every

00:24:05,730 --> 00:24:09,390
single blog gets numbered starting from

00:24:07,410 --> 00:24:12,600
zero all the way to you know application

00:24:09,390 --> 00:24:14,790
exits so if we happen to be just

00:24:12,600 --> 00:24:17,430
starting so you know the super static

00:24:14,790 --> 00:24:21,210
will 0 what we do is we send messages to

00:24:17,430 --> 00:24:23,220
all the edges that we have thus

00:24:21,210 --> 00:24:26,760
notifying them that we are connected to

00:24:23,220 --> 00:24:28,470
to them right because as I said we do

00:24:26,760 --> 00:24:29,880
know all of our neighbors but neighbors

00:24:28,470 --> 00:24:31,740
do not know that we are connected to

00:24:29,880 --> 00:24:33,210
them so if we want to build back

00:24:31,740 --> 00:24:35,309
reference you know sort of back edge

00:24:33,210 --> 00:24:37,470
into the graph we actually have to know

00:24:35,309 --> 00:24:40,710
if I the neighbor that we are connected

00:24:37,470 --> 00:24:43,950
to them and once this is done we

00:24:40,710 --> 00:24:46,110
basically vote to help but because there

00:24:43,950 --> 00:24:48,179
are messages in the queue the

00:24:46,110 --> 00:24:50,309
application doesn't exit so we go to the

00:24:48,179 --> 00:24:52,289
super step one so super step one

00:24:50,309 --> 00:24:54,299
basically happens over here so we get

00:24:52,289 --> 00:24:57,149
all of the messages that were sent to us

00:24:54,299 --> 00:24:59,220
from previous super sub zero we iterate

00:24:57,149 --> 00:25:02,159
all of those messages and what those

00:24:59,220 --> 00:25:04,289
messages tell us is who is connected to

00:25:02,159 --> 00:25:06,269
us so the only thing that we need to do

00:25:04,289 --> 00:25:08,820
is we need to build an edge in a vertex

00:25:06,269 --> 00:25:12,389
add edge to be connected back to that

00:25:08,820 --> 00:25:15,509
guy so essentially you know if you take

00:25:12,389 --> 00:25:18,749
this node what it will do it will send

00:25:15,509 --> 00:25:20,820
the notifications to this one and this

00:25:18,749 --> 00:25:22,379
one and the in the next super step you

00:25:20,820 --> 00:25:24,029
know those guys will kick in they will

00:25:22,379 --> 00:25:27,179
process the messages and we'll build the

00:25:24,029 --> 00:25:32,879
back reference you know so that's pretty

00:25:27,179 --> 00:25:35,009
simple running giraffe is actually

00:25:32,879 --> 00:25:36,600
pretty simple but before but it's a

00:25:35,009 --> 00:25:42,929
little bit time-consuming so let me

00:25:36,600 --> 00:25:44,850
actually let me actually show you what I

00:25:42,929 --> 00:25:46,499
will be doing here so the only thing

00:25:44,850 --> 00:25:49,080
that you need to do is you would need to

00:25:46,499 --> 00:25:50,490
set up environment and here I basically

00:25:49,080 --> 00:25:54,779
have Hadoop you know i just downloaded

00:25:50,490 --> 00:25:58,009
it from you know a apache web site you

00:25:54,779 --> 00:26:01,049
know download a giraffe release you know

00:25:58,009 --> 00:26:02,970
this is interesting because not a lot of

00:26:01,049 --> 00:26:04,919
people realize that you can actually run

00:26:02,970 --> 00:26:06,659
Hadoop without any kind of cluster or

00:26:04,919 --> 00:26:09,090
anything running at all so there is a

00:26:06,659 --> 00:26:10,619
mode in Hadoop called local mode where

00:26:09,090 --> 00:26:12,389
everything happens within the same jvm

00:26:10,619 --> 00:26:14,190
it's kind of the same processing but you

00:26:12,389 --> 00:26:16,649
are limited to just one single mapper

00:26:14,190 --> 00:26:18,929
well one single reducer for that matter

00:26:16,649 --> 00:26:20,369
but your app doesn't care because you

00:26:18,929 --> 00:26:23,460
know we can just like map everything

00:26:20,369 --> 00:26:26,039
into the single processing unit an

00:26:23,460 --> 00:26:28,559
interesting way of considering it is to

00:26:26,039 --> 00:26:30,779
point Hadoop Commodore at the empty

00:26:28,559 --> 00:26:34,049
subdirectory which happens to be this

00:26:30,779 --> 00:26:35,580
guy because by default local mode is

00:26:34,049 --> 00:26:36,840
what to do this it's only when you

00:26:35,580 --> 00:26:38,369
actually configure it to do something

00:26:36,840 --> 00:26:41,159
else it will do something else but by

00:26:38,369 --> 00:26:46,139
default it does local mode so with that

00:26:41,159 --> 00:26:48,360
bit of configuration in place the way

00:26:46,139 --> 00:26:49,170
you would actually execute giraffe is

00:26:48,360 --> 00:26:51,600
something like the

00:26:49,170 --> 00:26:54,780
and I will go over you know what every

00:26:51,600 --> 00:26:58,080
single line in here means and does but

00:26:54,780 --> 00:26:59,460
for now let's just let just started so

00:26:58,080 --> 00:27:01,380
you can see you know who do kicked in

00:26:59,460 --> 00:27:03,270
and unfortunately this process will take

00:27:01,380 --> 00:27:04,740
a little bit longer than you know I

00:27:03,270 --> 00:27:09,450
might like and I will tell you how to

00:27:04,740 --> 00:27:15,450
avoid this in a minute so let's get back

00:27:09,450 --> 00:27:16,830
to slides so yeah Apache Hadoop 1.2 is

00:27:15,450 --> 00:27:19,470
what I'm using you know Apache giraffe

00:27:16,830 --> 00:27:21,060
1.1 I'm actually release manager for

00:27:19,470 --> 00:27:23,130
this release so like hopefully we will

00:27:21,060 --> 00:27:25,200
push it down soon so it will be released

00:27:23,130 --> 00:27:26,820
artifact but for now I really highly

00:27:25,200 --> 00:27:28,830
recommend using the snapshot you know

00:27:26,820 --> 00:27:31,440
the previous release of giraffe is good

00:27:28,830 --> 00:27:33,300
but this one had tender loving care from

00:27:31,440 --> 00:27:36,510
facebook so it really is better than you

00:27:33,300 --> 00:27:37,920
know what we had on 1.0 apache maven is

00:27:36,510 --> 00:27:39,330
the build system you know for sure off

00:27:37,920 --> 00:27:40,740
so if you want to tinker with it you

00:27:39,330 --> 00:27:43,410
know maven is highly recommended and

00:27:40,740 --> 00:27:48,120
this is a subtle points so sheriff

00:27:43,410 --> 00:27:49,800
actual requires jdk 7 well maybe we will

00:27:48,120 --> 00:27:52,650
change that for this release but at

00:27:49,800 --> 00:27:54,420
least it used to and what it really

00:27:52,650 --> 00:27:56,400
means is that if you actually have a

00:27:54,420 --> 00:27:58,530
fully distributed cluster you actually

00:27:56,400 --> 00:28:01,410
have to run jdk 7 on all of the nodes

00:27:58,530 --> 00:28:03,480
right so giraffe you know being a

00:28:01,410 --> 00:28:06,060
MapReduce application actually would be

00:28:03,480 --> 00:28:07,980
executed within the same jvm that you

00:28:06,060 --> 00:28:09,660
are running all over your cluster and if

00:28:07,980 --> 00:28:13,800
that JVM happens to be jadek you know

00:28:09,660 --> 00:28:16,530
JVM six it will not work so like I said

00:28:13,800 --> 00:28:17,820
you know just downloading things and you

00:28:16,530 --> 00:28:19,440
know setting environment variables is

00:28:17,820 --> 00:28:22,650
all you need to do to actually kick

00:28:19,440 --> 00:28:24,270
started if you are using maven and if

00:28:22,650 --> 00:28:26,070
you're just developing your project I

00:28:24,270 --> 00:28:27,810
mean the only dependency that you have

00:28:26,070 --> 00:28:30,300
to declare is unsure of core and on

00:28:27,810 --> 00:28:36,420
Hadoop core that'll they'll get you

00:28:30,300 --> 00:28:38,340
going right away and here's how I ran it

00:28:36,420 --> 00:28:40,680
so basically sheriff is a command line

00:28:38,340 --> 00:28:42,000
utility you give it the jar file you

00:28:40,680 --> 00:28:43,950
know that contains the code that you've

00:28:42,000 --> 00:28:45,570
developed you give it the name of the

00:28:43,950 --> 00:28:48,060
class that has the compute method that

00:28:45,570 --> 00:28:50,430
you want to execute on your graph you

00:28:48,060 --> 00:28:53,010
give it an input path in whatever file

00:28:50,430 --> 00:28:56,460
system is configured and again an

00:28:53,010 --> 00:28:57,930
interesting aspect of HDFS is that HDFS

00:28:56,460 --> 00:28:59,130
doesn't really have to be h DX s right

00:28:57,930 --> 00:29:01,410
you know kadu can actually work with

00:28:59,130 --> 00:29:02,550
your local filesystem just fine that's

00:29:01,410 --> 00:29:05,370
what I'm using here

00:29:02,550 --> 00:29:07,230
a local sort of mode of hadoop would be

00:29:05,370 --> 00:29:09,570
absolutely fine to just use your local

00:29:07,230 --> 00:29:13,440
files but this is essentially you know

00:29:09,570 --> 00:29:14,790
pass to all of my all of my files this

00:29:13,440 --> 00:29:17,130
is the input format that i'm specifying

00:29:14,790 --> 00:29:18,720
and unfortunately this is the bit where

00:29:17,130 --> 00:29:20,970
giraffe gets you know kind of clunky

00:29:18,720 --> 00:29:23,820
because this input format have actually

00:29:20,970 --> 00:29:26,400
it has to match the code you know this

00:29:23,820 --> 00:29:29,400
for type variable that you know you put

00:29:26,400 --> 00:29:31,020
in your code and if you mess it up here

00:29:29,400 --> 00:29:33,270
it will not give you a helpful message

00:29:31,020 --> 00:29:35,610
message it will just fail so you know be

00:29:33,270 --> 00:29:38,040
careful but this input format you know

00:29:35,610 --> 00:29:40,170
produces the edges and you know nodes

00:29:38,040 --> 00:29:42,660
and exactly the types that we've encoded

00:29:40,170 --> 00:29:44,640
so this is this is an interesting bit

00:29:42,660 --> 00:29:46,470
you know you have to you have to specify

00:29:44,640 --> 00:29:48,600
that the number of workers is one

00:29:46,470 --> 00:29:50,550
because that's all we get in local mode

00:29:48,600 --> 00:29:52,500
but you can specify as many workers as

00:29:50,550 --> 00:29:55,170
you want so you know typically if you

00:29:52,500 --> 00:29:57,720
have a big loop cluster I guess this is

00:29:55,170 --> 00:29:59,490
better now you can specify you know

00:29:57,720 --> 00:30:01,170
typically like you know slightly less

00:29:59,490 --> 00:30:03,480
than a total number of nodes if you want

00:30:01,170 --> 00:30:05,280
to fully utilize utilize your cluster so

00:30:03,480 --> 00:30:07,440
this is your responsibility sheriff will

00:30:05,280 --> 00:30:09,240
not figure out how many workers you want

00:30:07,440 --> 00:30:13,800
in this you know distributed network to

00:30:09,240 --> 00:30:15,870
be instantiated and these are settings

00:30:13,800 --> 00:30:18,690
for sheriff itself so this is the only

00:30:15,870 --> 00:30:20,930
really useful one if your half split

00:30:18,690 --> 00:30:23,100
master worker we setting it to false

00:30:20,930 --> 00:30:25,530
again because we are running in the

00:30:23,100 --> 00:30:28,830
Hadoop local mode we only ever get a

00:30:25,530 --> 00:30:30,990
single mapper so we cannot really split

00:30:28,830 --> 00:30:32,700
different functionality because your app

00:30:30,990 --> 00:30:34,140
has you know different services we

00:30:32,700 --> 00:30:38,910
actually have to run everything in a

00:30:34,140 --> 00:30:41,940
single mapper and off you go it's pretty

00:30:38,910 --> 00:30:44,730
much it again the troubling bit is that

00:30:41,940 --> 00:30:47,100
you know i started at some time ago but

00:30:44,730 --> 00:30:48,810
it's still going on so it like it takes

00:30:47,100 --> 00:30:50,790
you know up to a minute or two even on

00:30:48,810 --> 00:30:53,010
the local data which is really tiny data

00:30:50,790 --> 00:30:56,250
set and the reason for that is much more

00:30:53,010 --> 00:30:59,130
about than giraffe so hopefully we will

00:30:56,250 --> 00:31:01,380
you know change some of it but the best

00:30:59,130 --> 00:31:04,020
way to be as productive as possible

00:31:01,380 --> 00:31:06,150
developing on sheriff it actually goes

00:31:04,020 --> 00:31:07,290
back to how you would unit test your

00:31:06,150 --> 00:31:11,910
application you know your sheriff

00:31:07,290 --> 00:31:15,150
application and in giraffe if you write

00:31:11,910 --> 00:31:16,350
something like this and you will use

00:31:15,150 --> 00:31:18,419
internal verdicts run

00:31:16,350 --> 00:31:21,330
what it will do it will basically run

00:31:18,419 --> 00:31:24,570
the same code on essentially a mocked

00:31:21,330 --> 00:31:27,480
environment right so you will give the

00:31:24,570 --> 00:31:30,840
graph seed which is a array of strings

00:31:27,480 --> 00:31:33,419
you know simulating your input data you

00:31:30,840 --> 00:31:35,250
will expect you know iterable essential

00:31:33,419 --> 00:31:37,440
of strings that will be the simulation

00:31:35,250 --> 00:31:38,970
of your output data and you can

00:31:37,440 --> 00:31:40,289
configure your f you know however you

00:31:38,970 --> 00:31:42,120
want you know just like I did on the

00:31:40,289 --> 00:31:43,950
command line using the configuration

00:31:42,120 --> 00:31:45,990
object so if you have this bit of code

00:31:43,950 --> 00:31:47,280
this your you know let's say unit test

00:31:45,990 --> 00:31:48,990
right you know you can totally step

00:31:47,280 --> 00:31:50,549
through it you know in your ID you don't

00:31:48,990 --> 00:31:52,320
actually have to bootstrap anything you

00:31:50,549 --> 00:31:54,539
know from khoob you know it's really

00:31:52,320 --> 00:31:56,429
easy way of how you can like just really

00:31:54,539 --> 00:31:57,330
start you know developing on giraffe but

00:31:56,429 --> 00:31:58,860
of course you know when it's time to

00:31:57,330 --> 00:32:00,450
actually execute it on Hadoop you will

00:31:58,860 --> 00:32:05,370
still go through the same set of steps

00:32:00,450 --> 00:32:09,500
that hopefully finished by now no it's

00:32:05,370 --> 00:32:09,500
not this is really taking a long thing

00:32:10,340 --> 00:32:15,510
and I think I'm actually out of time so

00:32:13,470 --> 00:32:25,799
I will not cover some of the more you

00:32:15,510 --> 00:32:28,049
know advanced use cases yeah so yeah so

00:32:25,799 --> 00:32:29,850
what it's really doing and again like

00:32:28,049 --> 00:32:31,890
I'm saying anything as way more to do

00:32:29,850 --> 00:32:33,960
with Hadoop than with giraffe so like if

00:32:31,890 --> 00:32:37,650
you look into it it's basically copying

00:32:33,960 --> 00:32:39,600
the jar files into the distributed cache

00:32:37,650 --> 00:32:41,250
and of course you know the local mode it

00:32:39,600 --> 00:32:42,720
has absolutely no business doing it

00:32:41,250 --> 00:32:45,870
because you know everything is available

00:32:42,720 --> 00:32:47,299
on my local machine anyway and because

00:32:45,870 --> 00:32:49,590
again you know for real Hadoop

00:32:47,299 --> 00:32:51,960
deployments this step doesn't really

00:32:49,590 --> 00:32:54,840
have to be all that performant you know

00:32:51,960 --> 00:32:56,130
nobody really you know looked into you

00:32:54,840 --> 00:32:57,720
know optimizing it so i think they are

00:32:56,130 --> 00:33:00,360
reading like bite by bite you know each

00:32:57,720 --> 00:33:02,280
jar file or something you know so again

00:33:00,360 --> 00:33:04,020
it's just to do copying a bunch of data

00:33:02,280 --> 00:33:06,030
from point A to point B without

00:33:04,020 --> 00:33:09,480
absolutely any need you know whatsoever

00:33:06,030 --> 00:33:10,980
for doing it so but you know once it's

00:33:09,480 --> 00:33:12,900
done the giraffe application will

00:33:10,980 --> 00:33:14,850
basically kick in will print you know a

00:33:12,900 --> 00:33:16,409
bunch of messages so you know the whole

00:33:14,850 --> 00:33:18,030
point is just to demonstrate how you

00:33:16,409 --> 00:33:19,770
could actually run it on your laptop and

00:33:18,030 --> 00:33:22,110
if you're interested in you know making

00:33:19,770 --> 00:33:27,929
Hadoop nicer in this setup you know well

00:33:22,110 --> 00:33:29,250
help us you know some patches um so yeah

00:33:27,929 --> 00:33:34,660
questions

00:33:29,250 --> 00:33:42,640
okay hello okay any questions yeah I

00:33:34,660 --> 00:33:46,120
think over the right extra didn't quite

00:33:42,640 --> 00:33:48,490
get how do you collect data from the

00:33:46,120 --> 00:33:50,850
compute method because you you showed

00:33:48,490 --> 00:33:54,430
how you send messages but how do you

00:33:50,850 --> 00:33:57,190
finally collect them to HDFS or whatever

00:33:54,430 --> 00:34:03,900
yeah that's that's this one this slide

00:33:57,190 --> 00:34:03,900
let me show it to you again so basically

00:34:06,660 --> 00:34:16,240
just a good yes so this is this step and

00:34:12,100 --> 00:34:18,430
it's actually optional it may very well

00:34:16,240 --> 00:34:20,530
be the case that you are absolutely not

00:34:18,430 --> 00:34:22,960
interested in the state of the graph at

00:34:20,530 --> 00:34:26,520
the end of the it you know at the end of

00:34:22,960 --> 00:34:29,680
the entire run you may be looking for a

00:34:26,520 --> 00:34:31,090
maximum value of some kind right so

00:34:29,680 --> 00:34:32,800
maybe the whole point of your graph

00:34:31,090 --> 00:34:35,800
processing is to find you know some

00:34:32,800 --> 00:34:37,780
local maximum and once you've done it

00:34:35,800 --> 00:34:39,280
like that's the output of your job so

00:34:37,780 --> 00:34:40,990
like there's absolutely no need to

00:34:39,280 --> 00:34:44,350
actually output anything that has to do

00:34:40,990 --> 00:34:45,940
with your graph but you can do that and

00:34:44,350 --> 00:34:47,470
that's how you get the data out of the

00:34:45,940 --> 00:34:50,380
giraffe application and how would you

00:34:47,470 --> 00:34:52,690
collect the maximum then that's actually

00:34:50,380 --> 00:34:54,850
the slides that I had to skip so sheriff

00:34:52,690 --> 00:34:55,960
has the notion of aggregators so it's

00:34:54,850 --> 00:34:57,880
kind of like think of it as a global

00:34:55,960 --> 00:34:59,680
variable that you can keep a throughout

00:34:57,880 --> 00:35:02,410
the run and the state of the global

00:34:59,680 --> 00:35:04,470
variable can be output just like you

00:35:02,410 --> 00:35:06,610
know at any random point and in fact

00:35:04,470 --> 00:35:08,050
what you could also do if you're

00:35:06,610 --> 00:35:10,360
actually doing you know a bit of

00:35:08,050 --> 00:35:11,860
heuristic you can actually look at it

00:35:10,360 --> 00:35:13,810
and say like well maybe it's not you

00:35:11,860 --> 00:35:14,980
know the optimal one but it's good

00:35:13,810 --> 00:35:16,600
enough so I'm quitting the entire

00:35:14,980 --> 00:35:20,050
application because it basically fits

00:35:16,600 --> 00:35:22,090
into some kind of you know range so you

00:35:20,050 --> 00:35:23,800
can output it and on top of that you can

00:35:22,090 --> 00:35:26,230
even quit the entire application at that

00:35:23,800 --> 00:35:30,480
point it may be the last question what

00:35:26,230 --> 00:35:32,860
happens if the no didn't vote to out and

00:35:30,480 --> 00:35:35,380
but it doesn't receive any messages

00:35:32,860 --> 00:35:37,570
later on it keeps running essentially oh

00:35:35,380 --> 00:35:38,349
yes so the method will be called anyway

00:35:37,570 --> 00:35:46,509
yeah

00:35:38,349 --> 00:35:49,269
Thanks so with the bsp model right

00:35:46,509 --> 00:35:52,239
you're super step is only as fast as the

00:35:49,269 --> 00:35:55,720
slowest task within that super step how

00:35:52,239 --> 00:35:57,910
does giraffe determine how to split the

00:35:55,720 --> 00:36:01,749
individual tasks so that they complete

00:35:57,910 --> 00:36:03,190
at roughly the same time yeah so like I

00:36:01,749 --> 00:36:07,650
said there is a partitioning based on

00:36:03,190 --> 00:36:11,109
the initially based on the verdicts ID

00:36:07,650 --> 00:36:13,569
so the partitioner is pluggable so the

00:36:11,109 --> 00:36:16,210
default one is just you know cash-based

00:36:13,569 --> 00:36:18,069
you know pretty simple you can you can

00:36:16,210 --> 00:36:19,420
write your own implementation and you

00:36:18,069 --> 00:36:23,349
know you can partition based on any

00:36:19,420 --> 00:36:25,119
criteria giraffe actually does rebalance

00:36:23,349 --> 00:36:36,220
so that's what you have to keep in mind

00:36:25,119 --> 00:36:37,779
so if you look at 2222 so the fact that

00:36:36,220 --> 00:36:40,210
these guys get assigned you know like

00:36:37,779 --> 00:36:42,099
this for one super step doesn't mean

00:36:40,210 --> 00:36:43,509
that they have to keep you know being

00:36:42,099 --> 00:36:45,999
assigned you know like this for the next

00:36:43,509 --> 00:36:48,039
super step before you're off transitions

00:36:45,999 --> 00:36:50,079
to the next super step it actually does

00:36:48,039 --> 00:36:52,779
rebalancing you know based on whatever

00:36:50,079 --> 00:36:54,130
criteria you can built in and if you you

00:36:52,779 --> 00:36:56,289
know start detecting that you know you

00:36:54,130 --> 00:36:59,829
have some slowpokes in your execution

00:36:56,289 --> 00:37:02,140
you can notify the you know rebalance ur

00:36:59,829 --> 00:37:05,400
through your partitioning scheme that

00:37:02,140 --> 00:37:10,749
you know repartitioning is necessary

00:37:05,400 --> 00:37:13,200
okay any more questions wellsir yeah

00:37:10,749 --> 00:37:13,200
yeah

00:37:18,809 --> 00:37:26,200
high somewhere early in your slide you

00:37:21,730 --> 00:37:30,759
had a slide about graph databases yes

00:37:26,200 --> 00:37:32,619
isn't it a bit of strange comparison

00:37:30,759 --> 00:37:36,700
because graph databases artist or graphs

00:37:32,619 --> 00:37:40,779
and and this is to process graphs so for

00:37:36,700 --> 00:37:43,059
just to give an example to to find all

00:37:40,779 --> 00:37:45,759
incoming edges for a note is only now in

00:37:43,059 --> 00:37:47,109
l4j it's it's a millisecond operation

00:37:45,759 --> 00:37:49,660
whereas in in giraffe you'll have to

00:37:47,109 --> 00:37:52,740
actually go through all the nodes to run

00:37:49,660 --> 00:37:55,809
all this the things so isn't a bit like

00:37:52,740 --> 00:37:59,589
PHP in MySQL what is better there is

00:37:55,809 --> 00:38:01,480
it's not grub databases to store grafts

00:37:59,589 --> 00:38:03,789
and this framework is to process graph

00:38:01,480 --> 00:38:05,890
on large scale well I mean if the only

00:38:03,789 --> 00:38:07,420
thing that graph databases did was you

00:38:05,890 --> 00:38:09,160
know they would basically let you store

00:38:07,420 --> 00:38:10,630
and retrieve the node then I would agree

00:38:09,160 --> 00:38:12,640
with you right you know but it actually

00:38:10,630 --> 00:38:14,109
it actually I'll execute the queries

00:38:12,640 --> 00:38:15,700
right it actually lets you do the

00:38:14,109 --> 00:38:16,930
queries so once you start doing the

00:38:15,700 --> 00:38:18,819
queries I mean it's processing right

00:38:16,930 --> 00:38:22,450
it's kind of like you know saying sequel

00:38:18,819 --> 00:38:24,549
is not Turing complete well it probably

00:38:22,450 --> 00:38:26,170
isn't but it still doesn't prevent

00:38:24,549 --> 00:38:31,450
facebook from essentially using sequel

00:38:26,170 --> 00:38:33,490
to the graph processing so yes I mean it

00:38:31,450 --> 00:38:35,109
may be a strange comparison you know

00:38:33,490 --> 00:38:37,119
once you kind of like just visualize it

00:38:35,109 --> 00:38:38,440
and try to partition everything but to

00:38:37,119 --> 00:38:40,210
me it's not because it's actually the

00:38:38,440 --> 00:38:41,829
very same things that you know again

00:38:40,210 --> 00:38:43,539
like I'm saying facebook today is doing

00:38:41,829 --> 00:38:46,000
with giraffe it could have totally done

00:38:43,539 --> 00:38:48,940
with the graph database exactly the same

00:38:46,000 --> 00:38:51,009
type of processing part of the reason

00:38:48,940 --> 00:38:52,509
they are not doing it is you know what I

00:38:51,009 --> 00:38:54,609
mentioned in you know some of the slides

00:38:52,509 --> 00:38:56,829
when I did the comparison is because

00:38:54,609 --> 00:38:59,650
once you store your data in a graph

00:38:56,829 --> 00:39:03,309
database you basically lose the ability

00:38:59,650 --> 00:39:04,690
to efficiently work on that data using

00:39:03,309 --> 00:39:07,809
the tools that are available in Hadoop

00:39:04,690 --> 00:39:09,670
ecosystem so if you want that same data

00:39:07,809 --> 00:39:12,069
to be available to your pig and your

00:39:09,670 --> 00:39:14,619
hive and your presto and your impala and

00:39:12,069 --> 00:39:16,299
your sequel on top of Hadoop like it's

00:39:14,619 --> 00:39:17,619
tough because it's not really stored in

00:39:16,299 --> 00:39:20,319
a central location it stored on a graph

00:39:17,619 --> 00:39:22,000
database like you know what do you do so

00:39:20,319 --> 00:39:23,529
they opted out you know for using graph

00:39:22,000 --> 00:39:25,599
and they're pretty happy with it but

00:39:23,529 --> 00:39:27,309
like I'm saying if all you do is graph

00:39:25,599 --> 00:39:29,109
processing graph databases could

00:39:27,309 --> 00:39:30,430
actually be a good choice it's just that

00:39:29,109 --> 00:39:31,000
they do much more than just graph

00:39:30,430 --> 00:39:34,390
processing on

00:39:31,000 --> 00:39:36,670
same data set and maybe a very good

00:39:34,390 --> 00:39:39,760
question you've mentioned two times that

00:39:36,670 --> 00:39:41,770
as a graph grows linearly the number of

00:39:39,760 --> 00:39:43,810
relations grow exponentially put do you

00:39:41,770 --> 00:39:45,880
mean better so if you basically double

00:39:43,810 --> 00:39:48,880
the number of nodes in your graph the

00:39:45,880 --> 00:39:50,560
potential again not not the ones that

00:39:48,880 --> 00:39:52,540
you know of but the potential number of

00:39:50,560 --> 00:39:56,820
connections grows exponentially it grows

00:39:52,540 --> 00:39:56,820
quadratically as far as I remember this

00:39:57,840 --> 00:40:02,430
well different different types of I

00:40:08,460 --> 00:40:20,800
smell some fight outside of this room

00:40:11,430 --> 00:40:24,130
which is good so correct me if I'm wrong

00:40:20,800 --> 00:40:28,210
or if I took this incorrectly if I want

00:40:24,130 --> 00:40:31,150
to use a graph based data store to

00:40:28,210 --> 00:40:34,720
process and give me result sets for a

00:40:31,150 --> 00:40:36,760
real time system which has not really

00:40:34,720 --> 00:40:39,070
low latency but low enough to be in the

00:40:36,760 --> 00:40:40,990
one millisecond under sorry one second

00:40:39,070 --> 00:40:43,120
or under one second range is this a good

00:40:40,990 --> 00:40:45,010
fit or as some other graph database best

00:40:43,120 --> 00:40:47,140
suited for that or is this more like for

00:40:45,010 --> 00:40:49,420
batch processing and getting sort of bi

00:40:47,140 --> 00:40:50,980
sort of information out it's it really

00:40:49,420 --> 00:40:52,840
is more for batch processing I mean

00:40:50,980 --> 00:40:55,420
there is there is ways to get data out

00:40:52,840 --> 00:40:57,310
of the you know sheriff and you can

00:40:55,420 --> 00:40:59,890
actually like the intermediate you know

00:40:57,310 --> 00:41:01,420
points of execution you know certain

00:40:59,890 --> 00:41:03,250
super steps you can actually dumped you

00:41:01,420 --> 00:41:05,410
know the graph state but it really is

00:41:03,250 --> 00:41:08,410
more for batch processing there is

00:41:05,410 --> 00:41:10,750
actually an alternative now on Hadoop

00:41:08,410 --> 00:41:12,550
called spark that does you know

00:41:10,750 --> 00:41:14,650
full-fledged memory processing so on

00:41:12,550 --> 00:41:16,450
spark they have a reimplement ation of

00:41:14,650 --> 00:41:20,220
essentially I believe you know the same

00:41:16,450 --> 00:41:23,140
BSP model called graph X I think uh

00:41:20,220 --> 00:41:25,210
which then you can actually inspect that

00:41:23,140 --> 00:41:26,890
model so the whole idea of spark is that

00:41:25,210 --> 00:41:28,450
it does computation but it actually lets

00:41:26,890 --> 00:41:30,310
you sort of tinker with the model is you

00:41:28,450 --> 00:41:31,840
know is it compute the output format are

00:41:30,310 --> 00:41:34,330
then the output formats which take

00:41:31,840 --> 00:41:36,460
graphs like which output graphs into

00:41:34,330 --> 00:41:38,950
standard graph formats which other tools

00:41:36,460 --> 00:41:41,440
can be visualized oh yeah absolutely so

00:41:38,950 --> 00:41:43,869
there's actually it's fine funny funnily

00:41:41,440 --> 00:41:45,640
enough I mean there is like a dot

00:41:43,869 --> 00:41:47,410
you know pretty popular you know graphic

00:41:45,640 --> 00:41:49,150
Fe and everything all other tools can

00:41:47,410 --> 00:41:53,859
sort of lower these Arabs in pretty much

00:41:49,150 --> 00:41:56,259
here you probably get like you know I'm

00:41:53,859 --> 00:41:57,339
cautious you know when I like mentioned

00:41:56,259 --> 00:41:59,410
dog because you know you don't really

00:41:57,339 --> 00:42:01,329
want to overwhelm it with you know like

00:41:59,410 --> 00:42:02,829
really big graph but if you you know

00:42:01,329 --> 00:42:04,299
somehow like I don't know if you do any

00:42:02,829 --> 00:42:05,650
kind of clustering or whatnot and you

00:42:04,299 --> 00:42:07,089
end up with a much smaller graph yeah

00:42:05,650 --> 00:42:12,339
that's that's not be fine i mean people

00:42:07,089 --> 00:42:14,559
do it all the time okay maybe time for

00:42:12,339 --> 00:42:17,079
last question because actually we have a

00:42:14,559 --> 00:42:20,769
coffee break so if someone prefers

00:42:17,079 --> 00:42:23,849
discussion to coffee now then thank you

00:42:20,769 --> 00:42:23,849

YouTube URL: https://www.youtube.com/watch?v=alegx3sP7hc


