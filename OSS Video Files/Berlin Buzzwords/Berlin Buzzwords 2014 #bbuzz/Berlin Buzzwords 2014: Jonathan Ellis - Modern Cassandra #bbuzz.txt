Title: Berlin Buzzwords 2014: Jonathan Ellis - Modern Cassandra #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Cassandra continues to be the weapon of choice for developers dealing with performance at scale. Whether in social networking (Instagram), scientific computing (SPring-8), or retail (eBay), Cassandra continues to deliver. This talk will look at new features in Cassandra 2.x and the upcoming 3.0, such as lightweight transactions, virtual nodes, a new data model and query language, and more.

Read more:
https://2014.berlinbuzzwords.de/session/modern-cassandra

About Jonathan Ellis:
https://2014.berlinbuzzwords.de/user/329/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,490 --> 00:00:11,310
I'm project here of Apache

00:00:08,550 --> 00:00:14,370
I've been doing that for about five

00:00:11,310 --> 00:00:16,350
years four years ago i started a company

00:00:14,370 --> 00:00:19,740
called datastax to commercialize

00:00:16,350 --> 00:00:22,920
Cassandra we have a booth over in the

00:00:19,740 --> 00:00:25,200
the sponsors Hall welcome to stop by and

00:00:22,920 --> 00:00:31,710
find out more about what we're doing

00:00:25,200 --> 00:00:33,840
with Cassandra so cassandra has a little

00:00:31,710 --> 00:00:37,350
bit of a reputation still from the early

00:00:33,840 --> 00:00:41,480
days as a database primarily used in

00:00:37,350 --> 00:00:44,879
social media that impression is is

00:00:41,480 --> 00:00:48,149
outdated now there's thousands of

00:00:44,879 --> 00:00:51,030
companies using Cassandra in all kinds

00:00:48,149 --> 00:00:54,199
of different use cases and different

00:00:51,030 --> 00:00:58,739
workloads and different business areas

00:00:54,199 --> 00:01:03,300
I'd like to look at three of those that

00:00:58,739 --> 00:01:05,880
are representative in in some ways of

00:01:03,300 --> 00:01:10,680
how people use Cassandra and use that as

00:01:05,880 --> 00:01:15,300
a basis for our introduction here so the

00:01:10,680 --> 00:01:19,490
first of those is ebay so big ecommerce

00:01:15,300 --> 00:01:23,510
site uses cassandra in multiple places

00:01:19,490 --> 00:01:26,730
one big one is when you're viewing a

00:01:23,510 --> 00:01:29,900
ebay product page when you're looking at

00:01:26,730 --> 00:01:34,130
an item for sale and the like and want

00:01:29,900 --> 00:01:37,680
buttons those are driven by cassandra

00:01:34,130 --> 00:01:40,260
there's something called the hunch taste

00:01:37,680 --> 00:01:43,770
graph also driven by cassandra the red

00:01:40,260 --> 00:01:45,690
laser time series analytics also by

00:01:43,770 --> 00:01:49,500
cassandra so there's there's actually

00:01:45,690 --> 00:01:51,720
lots of different teams at ebay using

00:01:49,500 --> 00:01:52,890
cassandra for different things and

00:01:51,720 --> 00:01:56,240
there's there's several different

00:01:52,890 --> 00:01:58,950
reasons that ebay chose to build these

00:01:56,240 --> 00:02:02,070
applications on top of Cassandra and I

00:01:58,950 --> 00:02:06,480
want to look into those in a little more

00:02:02,070 --> 00:02:09,119
depth so first of all a several of

00:02:06,480 --> 00:02:13,290
ebay's applications using Cassandra are

00:02:09,119 --> 00:02:17,640
for time series data so when I say time

00:02:13,290 --> 00:02:20,910
series data I mean event data that has a

00:02:17,640 --> 00:02:22,290
chronological component to it so I've

00:02:20,910 --> 00:02:25,650
got three examples

00:02:22,290 --> 00:02:31,140
on this slide on the left you have

00:02:25,650 --> 00:02:32,819
sensor data or monitoring data for you

00:02:31,140 --> 00:02:38,010
know machines or for an Internet of

00:02:32,819 --> 00:02:41,569
Things that kind of data where no I my

00:02:38,010 --> 00:02:45,060
temperature was X at 11 45 this morning

00:02:41,569 --> 00:02:48,030
it was y at 1146 you know that

00:02:45,060 --> 00:02:52,680
progression of data is an example of

00:02:48,030 --> 00:02:56,519
time series in the upper right this is a

00:02:52,680 --> 00:02:59,970
web server activity log so the log of

00:02:56,519 --> 00:03:03,090
activities of users on your websites can

00:02:59,970 --> 00:03:06,299
be useful time series data what have my

00:03:03,090 --> 00:03:08,760
friends been doing on this site also an

00:03:06,299 --> 00:03:11,599
example of time series data and at the

00:03:08,760 --> 00:03:16,500
bottom we have financial market data

00:03:11,599 --> 00:03:21,989
also a good example of of time series so

00:03:16,500 --> 00:03:26,930
Cassandra's data model gives you the

00:03:21,989 --> 00:03:29,760
ability to have sorted data within

00:03:26,930 --> 00:03:31,769
within a partition and distribute that

00:03:29,760 --> 00:03:33,629
across your cluster which makes it a

00:03:31,769 --> 00:03:36,989
very good fit for time series so a lot

00:03:33,629 --> 00:03:41,909
of people are using it for that another

00:03:36,989 --> 00:03:44,970
factor in ebays review was Cassandra

00:03:41,909 --> 00:03:48,870
support for multiple data centers what

00:03:44,970 --> 00:03:53,129
I've tried to diagram here is a real

00:03:48,870 --> 00:03:55,590
world deployment scenario where I have

00:03:53,129 --> 00:03:59,940
two data centers in the cloud so in

00:03:55,590 --> 00:04:02,659
Amazon ec2 or Google compute engine and

00:03:59,940 --> 00:04:05,159
two on premise data centers and

00:04:02,659 --> 00:04:09,750
cassandra is totally flexible about

00:04:05,159 --> 00:04:17,549
mixing heterogeneous deployments

00:04:09,750 --> 00:04:19,229
together like this it's not just multi

00:04:17,549 --> 00:04:21,810
data center support in the sense of i

00:04:19,229 --> 00:04:26,729
can fail over if I have a problem but

00:04:21,810 --> 00:04:29,370
rather it's active active so each each

00:04:26,729 --> 00:04:32,789
user in any of these data centers can

00:04:29,370 --> 00:04:34,700
perform reads and writes at local Layton

00:04:32,789 --> 00:04:35,810
sees which then get replicated

00:04:34,700 --> 00:04:39,860
asynchronous

00:04:35,810 --> 00:04:42,080
Lee to the other data centers and

00:04:39,860 --> 00:04:44,900
Cassandra's smart about this when I'm

00:04:42,080 --> 00:04:47,389
replicating too when I know that I need

00:04:44,900 --> 00:04:51,770
to replicate two machines in another

00:04:47,389 --> 00:04:55,130
data center I won't send three copies of

00:04:51,770 --> 00:05:00,080
that row over the network I'll send one

00:04:55,130 --> 00:05:02,750
copy over the web and then the replica

00:05:00,080 --> 00:05:07,280
that receives it will then forward it to

00:05:02,750 --> 00:05:12,530
the other replicas in in that other data

00:05:07,280 --> 00:05:15,770
center ebay also wanted Cassandra

00:05:12,530 --> 00:05:17,840
support for distributed counters so this

00:05:15,770 --> 00:05:20,389
you might be a little difficult to see

00:05:17,840 --> 00:05:23,840
in the back but what's going on in this

00:05:20,389 --> 00:05:27,740
slide is I'm showing how Cassandra's

00:05:23,840 --> 00:05:30,830
counters are partitioned across each

00:05:27,740 --> 00:05:34,370
replica so the the tough part so you're

00:05:30,830 --> 00:05:37,639
probably thinking okay a counter I just

00:05:34,370 --> 00:05:41,030
you know update set x equals x plus 1

00:05:37,639 --> 00:05:44,840
you know how hard can that be so it's a

00:05:41,030 --> 00:05:48,620
it's really trivial if you have a single

00:05:44,840 --> 00:05:51,580
master handling all your updates it is

00:05:48,620 --> 00:05:53,750
not trivial if you want to handle this

00:05:51,580 --> 00:05:56,600
asynchronously across multiple data

00:05:53,750 --> 00:05:59,690
centers if I just if I just had every

00:05:56,600 --> 00:06:02,330
once in there increments to a single

00:05:59,690 --> 00:06:06,169
master then you know my latency from

00:06:02,330 --> 00:06:08,570
from New York to London you know is is

00:06:06,169 --> 00:06:11,990
going to be you know 20 30 milliseconds

00:06:08,570 --> 00:06:18,350
that's a lot of latency to add on to

00:06:11,990 --> 00:06:21,650
each request so instead what we do is we

00:06:18,350 --> 00:06:24,650
partition a counter across each replica

00:06:21,650 --> 00:06:27,979
and then each replica is authorized to

00:06:24,650 --> 00:06:31,490
handle increments for its partition and

00:06:27,979 --> 00:06:33,380
then the the information isn't

00:06:31,490 --> 00:06:36,410
propagated to the other replicas

00:06:33,380 --> 00:06:39,650
asynchronously so in this diagram in the

00:06:36,410 --> 00:06:43,190
upper left I have a counter whose total

00:06:39,650 --> 00:06:46,159
value is 3 because the a partition has a

00:06:43,190 --> 00:06:48,080
count of three the other partitions have

00:06:46,159 --> 00:06:49,340
counts of zero they haven't handled any

00:06:48,080 --> 00:06:52,400
increments yet

00:06:49,340 --> 00:06:54,290
in the upper right we're going to handle

00:06:52,400 --> 00:06:57,470
two simultaneous increments the a

00:06:54,290 --> 00:07:00,919
replica and the B replica get requests

00:06:57,470 --> 00:07:03,470
to increment the counter by two so a

00:07:00,919 --> 00:07:06,530
increments it's partitioned by two from

00:07:03,470 --> 00:07:12,860
three to five the increments it's from

00:07:06,530 --> 00:07:17,150
zero to two now notice that the counter

00:07:12,860 --> 00:07:20,150
value is inconsistent in this diagram in

00:07:17,150 --> 00:07:22,550
the upper right because a thinks the

00:07:20,150 --> 00:07:25,100
value of the counter is 5b thinks the

00:07:22,550 --> 00:07:28,340
value is five and see thinks the value

00:07:25,100 --> 00:07:31,160
is 3 so that's because they haven't been

00:07:28,340 --> 00:07:34,160
able to replicate those updates yet at

00:07:31,160 --> 00:07:36,770
the bottom this is the state after the

00:07:34,160 --> 00:07:39,110
updates have been replicated now every

00:07:36,770 --> 00:07:44,960
replica knows that the total value of

00:07:39,110 --> 00:07:47,300
the counter is seven I Bay also was

00:07:44,960 --> 00:07:49,910
interested in Cassandra's Hadoop support

00:07:47,300 --> 00:07:51,770
so let me clarify this a little bit

00:07:49,910 --> 00:07:53,419
because there's different levels of

00:07:51,770 --> 00:07:56,419
Hadoop support that a database can

00:07:53,419 --> 00:07:59,419
provide you know at a very basic level I

00:07:56,419 --> 00:08:02,570
can provide an input format or an output

00:07:59,419 --> 00:08:06,440
format that allows me to split up the

00:08:02,570 --> 00:08:09,260
data into you know sets that Hadoop can

00:08:06,440 --> 00:08:11,840
run MapReduce jobs over and output data

00:08:09,260 --> 00:08:15,340
back into the database but Cassandra

00:08:11,840 --> 00:08:19,220
does more than that because Cassandra's

00:08:15,340 --> 00:08:22,580
has very flexible support for

00:08:19,220 --> 00:08:26,539
asynchronous replication I can I can

00:08:22,580 --> 00:08:31,539
split up my cluster into your

00:08:26,539 --> 00:08:35,510
application replicas in dark blue and

00:08:31,539 --> 00:08:38,120
Hadoop replicas in light green and what

00:08:35,510 --> 00:08:40,550
this does for me is I can tell Cassandra

00:08:38,120 --> 00:08:43,219
now make sure the data is replicated to

00:08:40,550 --> 00:08:45,620
both of these halves of the cluster so

00:08:43,219 --> 00:08:47,800
each of them has their own copy what

00:08:45,620 --> 00:08:51,440
that means now is I can run my

00:08:47,800 --> 00:08:55,339
analytical jobs in Hadoop and they will

00:08:51,440 --> 00:08:58,640
just touch the green nodes so they won't

00:08:55,339 --> 00:09:01,610
interfere with anything going on serving

00:08:58,640 --> 00:09:02,780
up millions of requests per second to

00:09:01,610 --> 00:09:05,810
the users

00:09:02,780 --> 00:09:07,700
my live web site so this this gives me a

00:09:05,810 --> 00:09:10,370
workload separation where I don't need

00:09:07,700 --> 00:09:14,180
to worry about causing performance

00:09:10,370 --> 00:09:19,370
degradation to the application that

00:09:14,180 --> 00:09:23,410
that's important to my users another

00:09:19,370 --> 00:09:30,290
good example of using cassandra is

00:09:23,410 --> 00:09:32,750
Adobe's audience manager product so

00:09:30,290 --> 00:09:37,220
audience manager is kind of a content

00:09:32,750 --> 00:09:39,530
management system for analytics and

00:09:37,220 --> 00:09:41,480
online advertising and they chose

00:09:39,530 --> 00:09:44,030
Cassandra for some of the same reasons

00:09:41,480 --> 00:09:46,910
that eBay did multi data centers on

00:09:44,030 --> 00:09:50,000
there but also some different ones so

00:09:46,910 --> 00:09:53,210
looking at those a little bit that at

00:09:50,000 --> 00:09:56,330
the top of Adobe's list was low latency

00:09:53,210 --> 00:09:59,420
especially on reads so this is another

00:09:56,330 --> 00:10:02,900
place where perception of cassandra is

00:09:59,420 --> 00:10:07,580
has lagged behind a little bit the

00:10:02,900 --> 00:10:09,080
actual product so cassandra has you may

00:10:07,580 --> 00:10:13,010
have heard that Cassandra's fast it

00:10:09,080 --> 00:10:16,220
writes but not not fast it reads so this

00:10:13,010 --> 00:10:17,930
is this is a production Cassandra

00:10:16,220 --> 00:10:20,680
monitoring system this isn't Adobe's

00:10:17,930 --> 00:10:24,890
cluster but this is a is a production

00:10:20,680 --> 00:10:28,490
Cassandra cluster and these are these

00:10:24,890 --> 00:10:31,880
are the read Layton sees four of you

00:10:28,490 --> 00:10:35,120
know three days in december i guess it's

00:10:31,880 --> 00:10:38,000
actually five days but you can see that

00:10:35,120 --> 00:10:40,730
the latency this is in milliseconds so

00:10:38,000 --> 00:10:42,710
the latency average is about half a

00:10:40,730 --> 00:10:45,350
millisecond a little less than half a

00:10:42,710 --> 00:10:48,530
millisecond the 95th percentile latency

00:10:45,350 --> 00:10:51,110
we have spikes up to you know five

00:10:48,530 --> 00:10:55,400
hundred microseconds but very

00:10:51,110 --> 00:10:57,650
consistently under a millisecond so so

00:10:55,400 --> 00:11:01,880
Cassandra can deliver this in the real

00:10:57,650 --> 00:11:06,310
world today we're looking at making this

00:11:01,880 --> 00:11:08,660
even better in two dot one so that the

00:11:06,310 --> 00:11:11,960
Cassandra two dot one release is in beta

00:11:08,660 --> 00:11:14,810
2 now looking to release it next month

00:11:11,960 --> 00:11:15,840
but here we have the blue line this is

00:11:14,810 --> 00:11:20,670
this is a

00:11:15,840 --> 00:11:23,460
performance of operations per second in

00:11:20,670 --> 00:11:25,710
the blue line is 20 and the orange line

00:11:23,460 --> 00:11:28,890
is two dot one so you can see that that

00:11:25,710 --> 00:11:30,720
the absolute performance I mean it's

00:11:28,890 --> 00:11:33,060
it's a little better in two dot one it's

00:11:30,720 --> 00:11:35,670
maybe five percent better but it's about

00:11:33,060 --> 00:11:38,310
the same but the important difference is

00:11:35,670 --> 00:11:41,430
that is that it's much less variable so

00:11:38,310 --> 00:11:43,230
in 20 we have some spikes where where it

00:11:41,430 --> 00:11:45,330
got worse during compaction 'he's where

00:11:43,230 --> 00:11:47,460
it got worse during jvm garbage

00:11:45,330 --> 00:11:50,610
collections in two dot one it's much

00:11:47,460 --> 00:11:52,730
smoother so we're your that's that's an

00:11:50,610 --> 00:11:54,990
important value to us to provide

00:11:52,730 --> 00:11:59,130
consistently good performance not just

00:11:54,990 --> 00:12:00,870
good performance on average so I want to

00:11:59,130 --> 00:12:02,970
talk a little bit about how Cassandra

00:12:00,870 --> 00:12:07,290
does this across a cluster and how we

00:12:02,970 --> 00:12:10,740
how we spread data across a cluster so

00:12:07,290 --> 00:12:16,380
the fundamental way we do this is is

00:12:10,740 --> 00:12:20,310
called consistent hashing on the rose

00:12:16,380 --> 00:12:21,900
primary key it's it's a little bit more

00:12:20,310 --> 00:12:23,760
complicated than that it's actually we

00:12:21,900 --> 00:12:26,790
actually use the first element of the

00:12:23,760 --> 00:12:28,590
primary key is the partition key but in

00:12:26,790 --> 00:12:30,510
in the simple case where it's not a

00:12:28,590 --> 00:12:33,330
compound primary key they you know it

00:12:30,510 --> 00:12:34,620
equates to the same thing so I'm going

00:12:33,330 --> 00:12:38,130
to take this I'm going to take this

00:12:34,620 --> 00:12:39,900
primary key here my username is going to

00:12:38,130 --> 00:12:43,290
be my primary key in this example and

00:12:39,900 --> 00:12:46,440
I'm going to hash it and earlier

00:12:43,290 --> 00:12:49,440
versions of Cassandra used md5 we use

00:12:46,440 --> 00:12:51,750
murmur hash now because it's faster it's

00:12:49,440 --> 00:12:55,410
important to note that we don't need a

00:12:51,750 --> 00:12:57,630
cryptographic hash so we're totally fine

00:12:55,410 --> 00:13:00,930
with hash collisions because all we're

00:12:57,630 --> 00:13:05,070
doing is we're using this hash to assign

00:13:00,930 --> 00:13:07,890
rose to replicas it's it that that's all

00:13:05,070 --> 00:13:11,490
we're using it for so once once we hash

00:13:07,890 --> 00:13:16,500
the the primary keys we're also going to

00:13:11,490 --> 00:13:19,020
assign numbers from our hash range to

00:13:16,500 --> 00:13:20,940
each of the nodes in our cluster so here

00:13:19,020 --> 00:13:24,330
I've got four nodes in my cluster I'm

00:13:20,940 --> 00:13:28,140
going to give the first one token zero

00:13:24,330 --> 00:13:29,190
the next one you know token for next one

00:13:28,140 --> 00:13:32,100
token eight

00:13:29,190 --> 00:13:37,440
next one token see so you know counting

00:13:32,100 --> 00:13:39,020
up in hex along that 64-bit space so I'm

00:13:37,440 --> 00:13:42,000
going to take those hash values that I

00:13:39,020 --> 00:13:45,780
computed with murmur hash and I'm going

00:13:42,000 --> 00:13:50,580
to binary search across the tokens in my

00:13:45,780 --> 00:13:52,830
cluster to see which which machine that

00:13:50,580 --> 00:13:55,920
row goes on so Jim goes to note see

00:13:52,830 --> 00:13:59,190
Carol goes to know d Johnny goes to note

00:13:55,920 --> 00:14:02,010
a and Suzy goes to note C again so this

00:13:59,190 --> 00:14:06,420
this shows how we can pick a single

00:14:02,010 --> 00:14:09,390
replica for each row how do we

00:14:06,420 --> 00:14:13,110
generalize from that to multiple

00:14:09,390 --> 00:14:17,190
replicas there's there's actually

00:14:13,110 --> 00:14:18,870
there's actually a pluggable component

00:14:17,190 --> 00:14:21,960
in Cassandra called the replication

00:14:18,870 --> 00:14:23,970
strategy that handles this and the the

00:14:21,960 --> 00:14:27,390
simplest way as we can just say well if

00:14:23,970 --> 00:14:30,080
I picked your replica d for my first

00:14:27,390 --> 00:14:33,390
replica I can just go around clockwise

00:14:30,080 --> 00:14:35,310
around the token ring and say nodes a

00:14:33,390 --> 00:14:39,990
and B are going to be my other two

00:14:35,310 --> 00:14:41,400
replicas but in in practice we want to

00:14:39,990 --> 00:14:45,630
be a little more sophisticated about

00:14:41,400 --> 00:14:48,960
this because we know that failures in

00:14:45,630 --> 00:14:51,570
real clusters are not random they're

00:14:48,960 --> 00:14:55,050
correlated and they're often correlated

00:14:51,570 --> 00:14:59,220
to physical location in your data center

00:14:55,050 --> 00:15:02,750
so we allow you to tell Cassandra what

00:14:59,220 --> 00:15:05,760
data center and what rack each machine

00:15:02,750 --> 00:15:09,240
lives in and that way we can make sure

00:15:05,760 --> 00:15:11,250
that we only have one replica per rack

00:15:09,240 --> 00:15:13,890
and we'll make sure to scatter the

00:15:11,250 --> 00:15:17,280
replicas across multiple racks and that

00:15:13,890 --> 00:15:19,710
way if I have a switch failure that can

00:15:17,280 --> 00:15:22,500
that can often take out an entire rack a

00:15:19,710 --> 00:15:25,790
power failure again Rack is often the

00:15:22,500 --> 00:15:27,570
unit of failure even if I have a cooler

00:15:25,790 --> 00:15:29,820
malfunction in the data center and

00:15:27,570 --> 00:15:32,130
machines near the cooler start

00:15:29,820 --> 00:15:34,710
overheating now the rack is a useful

00:15:32,130 --> 00:15:36,960
abstraction that says these machines are

00:15:34,710 --> 00:15:39,830
close together and I want the the

00:15:36,960 --> 00:15:43,280
replicas far apart so I can avoid

00:15:39,830 --> 00:15:43,280
correlated failures

00:15:43,410 --> 00:15:48,279
I've oversimplified just a little bit

00:15:46,749 --> 00:15:51,399
here because I've been talking about a

00:15:48,279 --> 00:15:54,399
single token per node and in practice we

00:15:51,399 --> 00:15:57,279
split it up into hundreds of tokens per

00:15:54,399 --> 00:16:00,029
node the principle is the same but by

00:15:57,279 --> 00:16:04,709
het by splitting it up into lots of

00:16:00,029 --> 00:16:07,929
tokens per node it lets us parallel eyes

00:16:04,709 --> 00:16:09,939
operations across the cluster so if I'm

00:16:07,929 --> 00:16:12,910
going to add a new machine to the

00:16:09,939 --> 00:16:16,779
cluster we call this bootstrapping so

00:16:12,910 --> 00:16:20,769
here I've got the the virtual nodes as

00:16:16,779 --> 00:16:23,259
little squares on the slide and I'm

00:16:20,769 --> 00:16:25,929
going to going to send some of those

00:16:23,259 --> 00:16:29,439
from each node to the new one so I

00:16:25,929 --> 00:16:32,289
basically pick it's actually the new

00:16:29,439 --> 00:16:37,209
node that picks its tokens at random but

00:16:32,289 --> 00:16:39,850
it results in you know a proportional

00:16:37,209 --> 00:16:42,459
amount of data being taken from each of

00:16:39,850 --> 00:16:46,089
the existing nodes and sent to the new

00:16:42,459 --> 00:16:48,129
one so by doing this my the impact of

00:16:46,089 --> 00:16:50,949
doing that bootstrap is spread across

00:16:48,129 --> 00:16:54,459
the entire cluster rather than focused

00:16:50,949 --> 00:16:56,949
on just one or two this this also

00:16:54,459 --> 00:16:59,289
impacts rebuilding a note if I lose a

00:16:56,949 --> 00:17:00,579
machine and I need to rebuild it I want

00:16:59,289 --> 00:17:05,559
two parallel eyes that across the

00:17:00,579 --> 00:17:07,449
cluster as much as possible so the end

00:17:05,559 --> 00:17:10,299
result would be that the new node has

00:17:07,449 --> 00:17:13,750
you know the same amount of data as the

00:17:10,299 --> 00:17:15,850
original nodes which all have you lost a

00:17:13,750 --> 00:17:23,559
little bit of data by sending that to

00:17:15,850 --> 00:17:27,009
the new one the last example I wanted to

00:17:23,559 --> 00:17:32,440
talk about is Instagram how they're

00:17:27,009 --> 00:17:34,389
using Cassandra one of their key

00:17:32,440 --> 00:17:38,019
qualities they need it in the database

00:17:34,389 --> 00:17:40,539
was durable rights so in other words if

00:17:38,019 --> 00:17:43,179
I send a right to Cassandra if I do an

00:17:40,539 --> 00:17:46,269
insert and Cassandra says yes it is

00:17:43,179 --> 00:17:49,000
inserted then even if I lose power even

00:17:46,269 --> 00:17:52,450
if I lose an entire data center that

00:17:49,000 --> 00:17:55,389
data should still be there when when I

00:17:52,450 --> 00:17:56,679
recover so the way Cassandra does that

00:17:55,389 --> 00:17:59,110
is similar to most

00:17:56,679 --> 00:18:02,200
relational databases where it has a

00:17:59,110 --> 00:18:06,940
commit log that writes get appended to

00:18:02,200 --> 00:18:09,940
before they get acknowledged so in the

00:18:06,940 --> 00:18:12,879
upper left i'm updating a column in a

00:18:09,940 --> 00:18:15,340
row and so there's a dotted line

00:18:12,879 --> 00:18:18,970
dividing the slide below the line is on

00:18:15,340 --> 00:18:21,159
disk above the line is in memory so i'm

00:18:18,970 --> 00:18:23,590
going to append it to the commit log and

00:18:21,159 --> 00:18:25,570
then i'm going to put it in a structure

00:18:23,590 --> 00:18:28,929
in Cassandra's storage engine that's

00:18:25,570 --> 00:18:31,629
called a mem table and in the mem table

00:18:28,929 --> 00:18:34,240
I can group up updates to a single row

00:18:31,629 --> 00:18:36,700
efficiently so notice here's here's

00:18:34,240 --> 00:18:38,789
another column to the same row so in the

00:18:36,700 --> 00:18:41,559
mem table it's part of the same

00:18:38,789 --> 00:18:43,269
structure in the commit log there's two

00:18:41,559 --> 00:18:45,309
distinct entries because I never

00:18:43,269 --> 00:18:48,340
override an entry in the commit log I

00:18:45,309 --> 00:18:52,470
just append new information that means

00:18:48,340 --> 00:18:56,220
that even if I'm on a spinning disk

00:18:52,470 --> 00:19:00,460
rather than SSD even if I'm on a

00:18:56,220 --> 00:19:02,289
magnetic hard disk it's still very fast

00:19:00,460 --> 00:19:06,850
because I'm not having to do any seeks

00:19:02,289 --> 00:19:09,100
to move that disk head around so I'm

00:19:06,850 --> 00:19:12,789
going to do some more updates to

00:19:09,100 --> 00:19:14,499
different rows and ultimately my commit

00:19:12,789 --> 00:19:17,649
log gets full enough that I'm that I'm

00:19:14,499 --> 00:19:21,759
ready to turn it into a data file on

00:19:17,649 --> 00:19:24,309
disk that's called a flush in the

00:19:21,759 --> 00:19:27,820
Cassandra storage engine and so I turn

00:19:24,309 --> 00:19:30,369
it into a data file and I create an

00:19:27,820 --> 00:19:32,769
index and bloom filter for it and once

00:19:30,369 --> 00:19:36,129
that's done and once I've synced that to

00:19:32,769 --> 00:19:38,019
disk then I don't need to keep those

00:19:36,129 --> 00:19:40,419
commit log entries around because the

00:19:38,019 --> 00:19:43,690
commit logs just there in case I need to

00:19:40,419 --> 00:19:46,779
replay from disk after some kind of

00:19:43,690 --> 00:19:48,700
power failure or maybe someone killed

00:19:46,779 --> 00:19:51,330
ash 92 the Cassandra process you know

00:19:48,700 --> 00:19:53,230
anything that causes it to stop

00:19:51,330 --> 00:19:55,210
unceremoniously that's what the commit

00:19:53,230 --> 00:19:58,570
logs there for so now that I know it's

00:19:55,210 --> 00:20:01,330
it's it's durable on disk in the storage

00:19:58,570 --> 00:20:03,279
file I don't need that commit log data

00:20:01,330 --> 00:20:05,559
anymore and I can clean that up I can

00:20:03,279 --> 00:20:09,609
recycle the commit log segment and reuse

00:20:05,559 --> 00:20:10,400
it so Instagram reports that they have

00:20:09,609 --> 00:20:12,260
fire

00:20:10,400 --> 00:20:17,680
the six nines of availability on

00:20:12,260 --> 00:20:21,050
Cassandra so 99.999% availability

00:20:17,680 --> 00:20:23,990
there's a number of different ways that

00:20:21,050 --> 00:20:29,750
Cassandra helps achieve that I want to

00:20:23,990 --> 00:20:32,630
look at just one of those so when I'm

00:20:29,750 --> 00:20:35,020
doing a reading a Cassandra cluster the

00:20:32,630 --> 00:20:38,660
client sends its request to some

00:20:35,020 --> 00:20:42,170
Cassandra node that that becomes the

00:20:38,660 --> 00:20:44,630
coordinator for this request so any node

00:20:42,170 --> 00:20:46,700
in the Cassandra cluster can be a

00:20:44,630 --> 00:20:51,710
coordinator and in fact it's good

00:20:46,700 --> 00:20:54,050
practice to spread your requests across

00:20:51,710 --> 00:20:56,480
all the nodes in the cluster so that no

00:20:54,050 --> 00:20:59,480
single node becomes overloaded and

00:20:56,480 --> 00:21:03,950
becomes a bottleneck so any node in the

00:20:59,480 --> 00:21:06,950
cluster can be the coordinator and it

00:21:03,950 --> 00:21:11,090
doesn't necessarily have to be a replica

00:21:06,950 --> 00:21:13,750
for that row and in fact in this example

00:21:11,090 --> 00:21:17,870
it's not a replica so the coordinator

00:21:13,750 --> 00:21:21,680
each each node in the cluster tracks how

00:21:17,870 --> 00:21:23,990
busy and how fast to respond the other

00:21:21,680 --> 00:21:25,850
nodes in the cluster are so the

00:21:23,990 --> 00:21:30,500
coordinator knows there's three replicas

00:21:25,850 --> 00:21:34,520
and it knows that that this one here is

00:21:30,500 --> 00:21:37,370
the fastest to reply recently so it's

00:21:34,520 --> 00:21:39,710
going to route the request to that

00:21:37,370 --> 00:21:41,720
replica and then the replica gives the

00:21:39,710 --> 00:21:46,760
coordinator the row and the coordinator

00:21:41,720 --> 00:21:48,500
gives it to the client so that's that's

00:21:46,760 --> 00:21:51,290
the simple case when when everything

00:21:48,500 --> 00:21:54,290
goes according to plan now a more

00:21:51,290 --> 00:21:58,220
interesting case is when the coordinator

00:21:54,290 --> 00:22:01,550
sends a request to a replica and then

00:21:58,220 --> 00:22:03,560
the replica dies or it loses network

00:22:01,550 --> 00:22:06,350
connectivity or you know something

00:22:03,560 --> 00:22:10,850
happens so that that replica can't

00:22:06,350 --> 00:22:12,980
respond to the request now in older

00:22:10,850 --> 00:22:16,100
versions of Cassandra the coordinator

00:22:12,980 --> 00:22:18,920
would say I couldn't do it sorry about

00:22:16,100 --> 00:22:21,290
that and it would send a timeout

00:22:18,920 --> 00:22:24,270
exception to the client

00:22:21,290 --> 00:22:26,190
starting in 20 which is the the current

00:22:24,270 --> 00:22:29,040
stable release we added something called

00:22:26,190 --> 00:22:31,440
rapid read protection so now when when

00:22:29,040 --> 00:22:34,970
the coordinators first request doesn't

00:22:31,440 --> 00:22:39,620
come back to it for any reason it will

00:22:34,970 --> 00:22:43,230
perform additional requests to other

00:22:39,620 --> 00:22:47,010
replicas and and failover that will fail

00:22:43,230 --> 00:22:49,050
over within a single request so this is

00:22:47,010 --> 00:22:51,420
this is actually configurable about how

00:22:49,050 --> 00:22:55,430
aggressive you want it to be by default

00:22:51,420 --> 00:22:57,840
it will retry the slowest one percent of

00:22:55,430 --> 00:23:01,340
requests but you can make it more

00:22:57,840 --> 00:23:04,980
aggressive even up to saying always do

00:23:01,340 --> 00:23:06,870
one more request than I have to for the

00:23:04,980 --> 00:23:09,420
consistency level that was requested

00:23:06,870 --> 00:23:11,910
also so by doing doing that extra

00:23:09,420 --> 00:23:14,160
redundant requests will give me lower

00:23:11,910 --> 00:23:15,450
latency because now I just have to wait

00:23:14,160 --> 00:23:20,850
for whichever one gets back to me

00:23:15,450 --> 00:23:23,100
fastest as well as providing protection

00:23:20,850 --> 00:23:25,950
against failures so here's what that

00:23:23,100 --> 00:23:28,920
looks like in an experiment where we had

00:23:25,950 --> 00:23:31,680
a four node Cassandra cluster we're

00:23:28,920 --> 00:23:34,230
doing reads from it as hard as we can

00:23:31,680 --> 00:23:38,310
and then midway through it we killed one

00:23:34,230 --> 00:23:40,230
of the nodes so like I said there's dim

00:23:38,310 --> 00:23:42,480
you can have different configurable

00:23:40,230 --> 00:23:45,830
levels of read protection and those are

00:23:42,480 --> 00:23:48,180
the lines on the top just different

00:23:45,830 --> 00:23:49,860
configurations of that but the line that

00:23:48,180 --> 00:23:52,290
goes all the way to the bottom that's

00:23:49,860 --> 00:23:55,230
with no read protection so that's what

00:23:52,290 --> 00:23:58,410
happens when it has to timeout those

00:23:55,230 --> 00:24:03,450
requests and it and it's not not able to

00:23:58,410 --> 00:24:05,010
failover until a new request comes in so

00:24:03,450 --> 00:24:13,500
that's that's been a big success for us

00:24:05,010 --> 00:24:16,230
in 20 if i were to sum up the last you

00:24:13,500 --> 00:24:18,510
know the last 34 years of cassandra

00:24:16,230 --> 00:24:21,300
development I'd say our core values have

00:24:18,510 --> 00:24:26,850
been massive scalability high

00:24:21,300 --> 00:24:28,560
performance and reliability if you're

00:24:26,850 --> 00:24:32,370
curious by the way the graph on the

00:24:28,560 --> 00:24:34,350
right was from a paper published by

00:24:32,370 --> 00:24:40,200
researchers at the university of tehran

00:24:34,350 --> 00:24:43,080
to where they the x-axis is the number

00:24:40,200 --> 00:24:46,770
of nodes in the cluster and the y-axis

00:24:43,080 --> 00:24:48,360
is the operations per second and it's an

00:24:46,770 --> 00:24:50,789
interesting paper i would recommend

00:24:48,360 --> 00:24:53,160
checking it out they did half a dozen

00:24:50,789 --> 00:24:56,480
different workloads this one here is a

00:24:53,160 --> 00:24:59,039
mix of reads writes and sequential scans

00:24:56,480 --> 00:25:02,220
and and they broke that down into the

00:24:59,039 --> 00:25:03,330
different components in the paper so

00:25:02,220 --> 00:25:07,559
that's that's kind of what we've

00:25:03,330 --> 00:25:09,360
delivered in Apache Cassandra you know

00:25:07,559 --> 00:25:13,919
as kind of our mission statement for the

00:25:09,360 --> 00:25:16,289
first few years last year we added a new

00:25:13,919 --> 00:25:21,690
core value of productivity and ease of

00:25:16,289 --> 00:25:25,679
use so we created a cassandra query

00:25:21,690 --> 00:25:29,179
language based on SQL these statements

00:25:25,679 --> 00:25:33,570
on the right are valid in both SQL and

00:25:29,179 --> 00:25:39,240
cql so create table create index select

00:25:33,570 --> 00:25:43,130
from where they have those in common too

00:25:39,240 --> 00:25:46,700
if this the short version of cql is

00:25:43,130 --> 00:25:49,950
cassandra is a distributed system so

00:25:46,700 --> 00:25:52,200
we're not going to support joins in the

00:25:49,950 --> 00:25:54,750
language joins are going to kill your

00:25:52,200 --> 00:25:57,530
performance in the distributed system so

00:25:54,750 --> 00:26:02,070
we're going to emphasize denormalization

00:25:57,530 --> 00:26:05,370
instead a second principle is that we

00:26:02,070 --> 00:26:08,730
are going to emphasize ventually

00:26:05,370 --> 00:26:11,520
insistence e instead of transactions now

00:26:08,730 --> 00:26:13,559
sometimes you do need transaction like

00:26:11,520 --> 00:26:16,049
function functionality and I'll show you

00:26:13,559 --> 00:26:19,919
how Cassandra's answer to that in a

00:26:16,049 --> 00:26:22,200
little bit but fundamentally eventual

00:26:19,919 --> 00:26:24,240
consistency lets you be it lets you

00:26:22,200 --> 00:26:26,730
deliver availability and it lets you

00:26:24,240 --> 00:26:28,799
deliver performance much better than you

00:26:26,730 --> 00:26:31,470
can do if you're focused on acid

00:26:28,799 --> 00:26:34,590
transactions there's a great performance

00:26:31,470 --> 00:26:36,960
by an engineer at Netflix called

00:26:34,590 --> 00:26:40,110
eventual consistency is not hopeful

00:26:36,960 --> 00:26:43,740
consistency that's a great introduction

00:26:40,110 --> 00:26:45,890
to this concept of how eventual

00:26:43,740 --> 00:26:53,880
consistency is your friend

00:26:45,890 --> 00:26:56,040
so just a quick taste of of cql and how

00:26:53,880 --> 00:26:59,900
we how we think about data modeling in

00:26:56,040 --> 00:27:02,850
Cassandra if I have a use erste below

00:26:59,900 --> 00:27:04,880
relational database and I want to allow

00:27:02,850 --> 00:27:10,400
users to have multiple email addresses

00:27:04,880 --> 00:27:13,290
I'm going to create an addresses table

00:27:10,400 --> 00:27:16,440
with a many-to-one relationship to my

00:27:13,290 --> 00:27:20,340
users and then I'll pull those out at

00:27:16,440 --> 00:27:25,170
runtime with a join so we I already said

00:27:20,340 --> 00:27:29,760
we don't have joins in c ql by design so

00:27:25,170 --> 00:27:32,430
the what we do instead is we would use a

00:27:29,760 --> 00:27:37,440
collection to hold the email addresses

00:27:32,430 --> 00:27:41,550
so i just in line that into the user row

00:27:37,440 --> 00:27:43,410
as a set so so you can see that I my

00:27:41,550 --> 00:27:47,630
column definition here for email

00:27:43,410 --> 00:27:51,270
addresses is a set of texts so

00:27:47,630 --> 00:27:54,960
collections in Cassandra are typed so in

00:27:51,270 --> 00:27:57,930
this case it's uh it's a set of text

00:27:54,960 --> 00:28:01,470
entries and then I can add email

00:27:57,930 --> 00:28:06,000
addresses to that row by doing this so

00:28:01,470 --> 00:28:10,350
here I'm saying take the union of the

00:28:06,000 --> 00:28:13,250
existing email addresses and these new

00:28:10,350 --> 00:28:16,170
ones now I could I could also say

00:28:13,250 --> 00:28:18,870
replace if I didn't have that email

00:28:16,170 --> 00:28:23,060
addresses plus I could just say set

00:28:18,870 --> 00:28:27,510
email address as equals this new set

00:28:23,060 --> 00:28:31,380
generally speaking this if if you're

00:28:27,510 --> 00:28:32,940
just going to be obliterating what's

00:28:31,380 --> 00:28:36,000
already there and replacing it with a

00:28:32,940 --> 00:28:38,190
new collection you know that I guess

00:28:36,000 --> 00:28:39,780
that's that's fine and that's useful but

00:28:38,190 --> 00:28:43,370
it's more useful to be able to

00:28:39,780 --> 00:28:46,850
incrementally and performant Lee add new

00:28:43,370 --> 00:28:50,010
entries to the collection so unlike

00:28:46,850 --> 00:28:52,230
unlike document databases for instance

00:28:50,010 --> 00:28:55,770
when I add new entries to the collection

00:28:52,230 --> 00:28:59,120
I'm just writing that new entry I'm not

00:28:55,770 --> 00:29:01,800
rewriting the entire row

00:28:59,120 --> 00:29:04,980
so I mentioned that sometimes you do

00:29:01,800 --> 00:29:06,800
need transaction like functionality so

00:29:04,980 --> 00:29:12,030
what I'm concerned about their is

00:29:06,800 --> 00:29:17,310
imposing a linear view of operations on

00:29:12,030 --> 00:29:19,920
the database so an example that I like

00:29:17,310 --> 00:29:24,990
to use is if you're allowing users to

00:29:19,920 --> 00:29:28,500
register for your application then it

00:29:24,990 --> 00:29:30,900
it's it's you need to be you want to

00:29:28,500 --> 00:29:36,060
make very sure that only one user

00:29:30,900 --> 00:29:39,830
registers for a given name so in the in

00:29:36,060 --> 00:29:43,110
a without some kind of transactions I

00:29:39,830 --> 00:29:45,150
can't provide this so here's what that

00:29:43,110 --> 00:29:49,530
here's here's what an attempt to do this

00:29:45,150 --> 00:29:52,080
might look like without transactions one

00:29:49,530 --> 00:29:55,830
client asks Cassandra does this user

00:29:52,080 --> 00:29:58,560
already exist Cassandra says no at the

00:29:55,830 --> 00:30:01,110
same time another user as Cassandra does

00:29:58,560 --> 00:30:03,930
this user exists Cassandra says no at

00:30:01,110 --> 00:30:05,910
the same time the first user says okay

00:30:03,930 --> 00:30:09,330
well since it didn't exist I'm going to

00:30:05,910 --> 00:30:10,770
insert the row the second user says well

00:30:09,330 --> 00:30:14,160
since it didn't exist I'm going to

00:30:10,770 --> 00:30:16,410
insert the row as well so what what ends

00:30:14,160 --> 00:30:20,370
up happening in Cassandra is the second

00:30:16,410 --> 00:30:21,780
one ends up overriding the data from the

00:30:20,370 --> 00:30:24,450
first one because these could be

00:30:21,780 --> 00:30:27,210
happening on different replicas entirely

00:30:24,450 --> 00:30:30,930
or even in different data centers so so

00:30:27,210 --> 00:30:33,510
there's no an insert in Cassandra is not

00:30:30,930 --> 00:30:36,560
there's no concept of there's a

00:30:33,510 --> 00:30:39,690
uniqueness constraint that will reject

00:30:36,560 --> 00:30:41,580
duplicate rows so there's an that that

00:30:39,690 --> 00:30:43,740
concept doesn't exist in Cassandra so

00:30:41,580 --> 00:30:46,020
it's going to accept both of the inserts

00:30:43,740 --> 00:30:50,550
and then the you know one of them is

00:30:46,020 --> 00:30:52,620
going to overwrite the other so we added

00:30:50,550 --> 00:30:54,750
we added a feature called lightweight

00:30:52,620 --> 00:30:58,170
transactions and what that does is it

00:30:54,750 --> 00:31:02,820
lets you specify to Cassandra the otha

00:30:58,170 --> 00:31:06,540
the update and the condition to check

00:31:02,820 --> 00:31:09,030
before performing that update and wrap

00:31:06,540 --> 00:31:11,760
that into a single statement so in this

00:31:09,030 --> 00:31:12,540
case when we're inserting new rows that

00:31:11,760 --> 00:31:15,060
that

00:31:12,540 --> 00:31:19,350
check is just at the bottom here if not

00:31:15,060 --> 00:31:24,690
exists so if I say insert if not exists

00:31:19,350 --> 00:31:29,600
and I have multiple clients doing this

00:31:24,690 --> 00:31:33,420
at the same time one of them will get a

00:31:29,600 --> 00:31:35,280
success result which is you know it'll

00:31:33,420 --> 00:31:38,490
get back a result set that says applied

00:31:35,280 --> 00:31:42,270
is true the other will get back applied

00:31:38,490 --> 00:31:45,390
is false and then as as extra

00:31:42,270 --> 00:31:48,840
information here's the row that already

00:31:45,390 --> 00:31:51,900
exists that you thought didn't exist and

00:31:48,840 --> 00:31:54,720
so now it's up to you the application to

00:31:51,900 --> 00:31:57,600
decide do you want to update the

00:31:54,720 --> 00:31:59,640
existing rule or do you want to insert a

00:31:57,600 --> 00:32:03,660
different row so you've got that

00:31:59,640 --> 00:32:06,480
information now so to you know when

00:32:03,660 --> 00:32:09,590
you're updating instead of inserting

00:32:06,480 --> 00:32:13,400
then your your if statement can include

00:32:09,590 --> 00:32:17,280
existing column values so i can check

00:32:13,400 --> 00:32:19,830
column values that are it's restricted

00:32:17,280 --> 00:32:23,040
to a single partition so remember i said

00:32:19,830 --> 00:32:25,950
that that partitions are the unit of how

00:32:23,040 --> 00:32:27,960
we spread data across the cluster so by

00:32:25,950 --> 00:32:30,090
restricting lightweight transactions to

00:32:27,960 --> 00:32:32,610
a single partition that means i know

00:32:30,090 --> 00:32:34,860
that i only need to coordinate across

00:32:32,610 --> 00:32:37,290
one set of replicas I don't need to

00:32:34,860 --> 00:32:41,070
coordinate across know though entire

00:32:37,290 --> 00:32:44,760
cluster so that lets me provide

00:32:41,070 --> 00:32:47,460
boundaries on you know how how

00:32:44,760 --> 00:32:53,000
concurrent I can I can make this without

00:32:47,460 --> 00:32:53,000
getting into trouble so under the hood

00:32:53,780 --> 00:32:59,670
lightweight transactions are built on

00:32:55,920 --> 00:33:01,950
paxos which gives us some very desirable

00:32:59,670 --> 00:33:04,530
properties from Cassandra's perspective

00:33:01,950 --> 00:33:07,020
first of all it's quorum based meaning

00:33:04,530 --> 00:33:10,980
as long as I have a majority of the

00:33:07,020 --> 00:33:14,370
replicas for that partition I can make

00:33:10,980 --> 00:33:17,640
progress so it's it's totally so it's

00:33:14,370 --> 00:33:19,680
totally okay for some replicas to be

00:33:17,640 --> 00:33:22,140
down as long as I still have a majority

00:33:19,680 --> 00:33:24,870
so that's important for Cassandra's

00:33:22,140 --> 00:33:26,370
goals of delivering availability packs

00:33:24,870 --> 00:33:29,400
of state is also durable

00:33:26,370 --> 00:33:33,540
so even if I I start a lightweight

00:33:29,400 --> 00:33:37,010
transaction and partway through the

00:33:33,540 --> 00:33:39,450
coordinator dies that's that's still

00:33:37,010 --> 00:33:41,450
going that's not going to affect my

00:33:39,450 --> 00:33:47,970
correctness I'm going to be able to

00:33:41,450 --> 00:33:49,590
finish that with with a new leader if

00:33:47,970 --> 00:33:51,780
for those of you who said you're already

00:33:49,590 --> 00:33:54,240
using Cassandra we added a new

00:33:51,780 --> 00:33:57,240
consistency level for this consistency

00:33:54,240 --> 00:34:01,620
level dot serial means i'm doing a read

00:33:57,240 --> 00:34:03,360
and I want that read to know peer into

00:34:01,620 --> 00:34:06,330
the lightweight transaction machinery

00:34:03,360 --> 00:34:10,950
and and let me know what that most

00:34:06,330 --> 00:34:12,780
recent value is as eaten as even

00:34:10,950 --> 00:34:17,250
including lightweight transactions that

00:34:12,780 --> 00:34:19,740
are in process of being committed down

00:34:17,250 --> 00:34:23,399
at the bottom though is the big you know

00:34:19,740 --> 00:34:26,820
danger warning sign language we're doing

00:34:23,399 --> 00:34:29,550
for round trips between each replica and

00:34:26,820 --> 00:34:31,560
the coordinator for a lightweight

00:34:29,550 --> 00:34:35,490
transaction so it's lightweight in the

00:34:31,560 --> 00:34:38,520
sense that it doesn't perform locking

00:34:35,490 --> 00:34:42,179
and lightweight in the sense that

00:34:38,520 --> 00:34:44,340
there's no begin transaction commit or

00:34:42,179 --> 00:34:46,620
rollback it's it's rolled into a single

00:34:44,340 --> 00:34:51,179
statement it's not lightweight in the

00:34:46,620 --> 00:34:53,850
performance sense so no you did the

00:34:51,179 --> 00:34:55,740
wrong conclusion from this would be hey

00:34:53,850 --> 00:34:56,970
I've got lightweight transaction so I'm

00:34:55,740 --> 00:34:59,310
going to build my entire application

00:34:56,970 --> 00:35:01,530
using this that would be the wrong

00:34:59,310 --> 00:35:03,330
lesson the right lesson is it's

00:35:01,530 --> 00:35:05,910
available for when you really need it

00:35:03,330 --> 00:35:09,120
and when the alternative is you know

00:35:05,910 --> 00:35:13,590
corruption or inflicting zookeeper on

00:35:09,120 --> 00:35:16,040
yourself so those of you who view

00:35:13,590 --> 00:35:19,040
zookeeper know what I'm talking about

00:35:16,040 --> 00:35:19,040
yes

00:35:20,000 --> 00:35:25,800
or conflict we detected the reader

00:35:23,580 --> 00:35:28,020
actually needs to say I want to be aware

00:35:25,800 --> 00:35:33,930
of conflicting transactions with this

00:35:28,020 --> 00:35:35,820
year lie today no sorry yeah let me take

00:35:33,930 --> 00:35:40,650
questions offline because i only have

00:35:35,820 --> 00:35:42,840
five minutes or three minutes now so

00:35:40,650 --> 00:35:45,750
coming up in two dot one I want to just

00:35:42,840 --> 00:35:52,859
give you a quick taste of what is about

00:35:45,750 --> 00:35:56,010
to be released next month so I i showed

00:35:52,859 --> 00:35:59,520
you collections earlier collections do

00:35:56,010 --> 00:36:03,960
not nest I cannot have a map of sets or

00:35:59,520 --> 00:36:06,570
a set of lists but in two dot one I can

00:36:03,960 --> 00:36:08,580
create my own types which could contain

00:36:06,570 --> 00:36:12,300
collections and then I can have

00:36:08,580 --> 00:36:16,140
collections of my type so if you look

00:36:12,300 --> 00:36:19,800
here my address type contains a set of

00:36:16,140 --> 00:36:24,600
phone numbers my user type contains a

00:36:19,800 --> 00:36:27,869
map of addresses and so that gives me

00:36:24,600 --> 00:36:31,350
this nest ability but now it but it but

00:36:27,869 --> 00:36:35,730
it's strongly typed now so I don't I I

00:36:31,350 --> 00:36:39,780
get that benefit of a strong schema as

00:36:35,730 --> 00:36:42,570
well as the ability to build a structure

00:36:39,780 --> 00:36:45,530
in my database that matches my object

00:36:42,570 --> 00:36:49,619
hierarchy so a night and I can pull out

00:36:45,530 --> 00:36:51,270
different pieces of those types with cql

00:36:49,619 --> 00:36:56,910
so the query here I'm pulling out the

00:36:51,270 --> 00:37:00,540
citi field and phone field from the the

00:36:56,910 --> 00:37:04,760
addresses and that's what I get back for

00:37:00,540 --> 00:37:07,500
one of my users we've also added

00:37:04,760 --> 00:37:11,850
indexing to collections so here I've got

00:37:07,500 --> 00:37:14,130
a set of texts for my tags column in my

00:37:11,850 --> 00:37:18,359
songs table and then I can create an

00:37:14,130 --> 00:37:20,369
index on that tags column and use that

00:37:18,359 --> 00:37:22,530
in a query notice that it that we've

00:37:20,369 --> 00:37:25,859
added a new keyword here though so we've

00:37:22,530 --> 00:37:28,530
added the contains keyword we could have

00:37:25,859 --> 00:37:30,660
you could have reused the existing in

00:37:28,530 --> 00:37:34,350
keyword and then I could have said where

00:37:30,660 --> 00:37:37,080
blues in tags I could have I could

00:37:34,350 --> 00:37:41,340
reverse that to use the N key word we

00:37:37,080 --> 00:37:44,280
went with contains because maps are a

00:37:41,340 --> 00:37:49,200
special case because maps have keys and

00:37:44,280 --> 00:37:51,660
values so by default or rather when you

00:37:49,200 --> 00:37:54,000
use the contains keyword on a map it's

00:37:51,660 --> 00:37:57,930
going to check the values but we also

00:37:54,000 --> 00:38:02,550
added the contains keys keyword that

00:37:57,930 --> 00:38:06,360
will let you check the keys of the map

00:38:02,550 --> 00:38:08,060
collection as well so beta 2 is out now

00:38:06,360 --> 00:38:10,500
if you want to play with two dot one

00:38:08,060 --> 00:38:12,300
works we're hoping to do a release

00:38:10,500 --> 00:38:15,990
candidate next week and get the final

00:38:12,300 --> 00:38:17,940
out for the end of June finally I just

00:38:15,990 --> 00:38:19,410
wanted to give you a heads up for some

00:38:17,940 --> 00:38:23,190
other cassandra talks at berlin

00:38:19,410 --> 00:38:25,410
buzzwords later today gary deuce Babic

00:38:23,190 --> 00:38:27,750
from rackspace is talking about blue

00:38:25,410 --> 00:38:30,900
flood which is a metrics processing

00:38:27,750 --> 00:38:33,750
system built on Cassandra also later

00:38:30,900 --> 00:38:36,840
today is talk on the cassandra java

00:38:33,750 --> 00:38:39,810
driver tomorrow we have one on time

00:38:36,840 --> 00:38:41,820
series with cassandra and a longer data

00:38:39,810 --> 00:38:44,430
modeling talk so the data modeling talk

00:38:41,820 --> 00:38:46,080
tomorrow is an 80 minute session and

00:38:44,430 --> 00:38:49,200
we'll be able to get into some more

00:38:46,080 --> 00:38:51,180
details on doing cassandra modeling and

00:38:49,200 --> 00:38:53,910
then finally not part of Berlin

00:38:51,180 --> 00:38:57,090
buzzwords but also in Berlin tomorrow at

00:38:53,910 --> 00:39:00,630
7pm we're doing a cassandra users need

00:38:57,090 --> 00:39:02,100
up it's about 15 minutes away and and it

00:39:00,630 --> 00:39:06,120
you know if you google for Berlin

00:39:02,100 --> 00:39:08,790
cassandra meet up there it is so I'll be

00:39:06,120 --> 00:39:11,400
happy to take questions oh you can find

00:39:08,790 --> 00:39:15,030
me at the the data sex booths in the

00:39:11,400 --> 00:39:17,750
sponsors room and thanks for your time

00:39:15,030 --> 00:39:17,750
and enjoy the conference

00:39:19,020 --> 00:39:21,080

YouTube URL: https://www.youtube.com/watch?v=-0rdNsn2Rrg


