Title: Berlin Buzzwords 2019: Niels Basjes â€“ How to handle 100k events per second #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	In the last decades many systems have been used that were described as "queues" (AQ, ActiveMQ, RabbitMQ, etc.), yet from a computer science perspective these are not queues at all. Many of us have learned to work quite effectively with these messaging systems and we all understand that we cannot expect to receive the messages in any particular order and that we get all messages exactly once (which we can expect with a queue). With the arrival of Apache Kafka and Flink a new class of applications became possible. 

In this talk I will go into several real applications from the bol.com context that all revolve around low latency behavioral analytics. I will talk about the entire end-to-end pipeline from the webbrowser and application server to application and discuss many of the things to think about when creating your analysis application. I will also touch upon using state machines as a way of doing this type of behavioral analysis using very simple software and show example algorithms from our context.

Read more:
https://2019.berlinbuzzwords.de/19/session/measuring-20-how-handle-100k-events-second

About Niels Basjes:
https://2019.berlinbuzzwords.de/users/niels-basjes

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,819 --> 00:00:14,809
hi good afternoon so the reason I'm not

00:00:12,440 --> 00:00:16,820
on the schedule is because yesterday I

00:00:14,809 --> 00:00:19,430
found out somebody dropped out and I was

00:00:16,820 --> 00:00:22,580
asked to talk so I'm going to talk about

00:00:19,430 --> 00:00:25,880
measuring 2.0 it's a streaming project

00:00:22,580 --> 00:00:28,339
we have at Bolcom in this talk I'm going

00:00:25,880 --> 00:00:30,949
to talk a bit about the context why we

00:00:28,339 --> 00:00:34,910
have this project it's about measuring

00:00:30,949 --> 00:00:39,050
interactions and intere in essence the

00:00:34,910 --> 00:00:41,120
whole chain of using that data I have a

00:00:39,050 --> 00:00:43,850
background in both computer science and

00:00:41,120 --> 00:00:48,410
business and have been doing software

00:00:43,850 --> 00:00:50,510
development algorithm research and doing

00:00:48,410 --> 00:00:52,550
open source software for a long time and

00:00:50,510 --> 00:00:56,810
actually Isabel mentioned me this

00:00:52,550 --> 00:00:59,060
morning in her keynote I have talked

00:00:56,810 --> 00:01:00,800
about this project before at Borland

00:00:59,060 --> 00:01:03,710
buzzwords so you can look it up on

00:01:00,800 --> 00:01:06,259
YouTube and my colleague Yvonne also

00:01:03,710 --> 00:01:08,990
talked about it when we were just in

00:01:06,259 --> 00:01:12,140
production because we're in Germany I

00:01:08,990 --> 00:01:12,859
assume only fusion you know what Bolcom

00:01:12,140 --> 00:01:15,710
is well

00:01:12,859 --> 00:01:17,929
simply put comparable to Amazon we're an

00:01:15,710 --> 00:01:21,649
online marketplace where lots of

00:01:17,929 --> 00:01:24,819
companies sell stuff we have over 20

00:01:21,649 --> 00:01:28,939
million available products things like

00:01:24,819 --> 00:01:31,759
chrome costs and we have a lot of

00:01:28,939 --> 00:01:34,399
additional information on what we put on

00:01:31,759 --> 00:01:37,130
the page and that's exactly where the

00:01:34,399 --> 00:01:39,619
problem starts the key requirement we

00:01:37,130 --> 00:01:40,789
have is that we want to measure what is

00:01:39,619 --> 00:01:42,889
happening on the page

00:01:40,789 --> 00:01:46,819
what are we showing why are we showing

00:01:42,889 --> 00:01:49,340
it and how are people responding and the

00:01:46,819 --> 00:01:51,439
use cases are all over the place we want

00:01:49,340 --> 00:01:54,469
to do straight on dashboards but also

00:01:51,439 --> 00:01:56,090
personalization site improvement data

00:01:54,469 --> 00:01:59,929
science for all kinds of purposes and

00:01:56,090 --> 00:02:02,389
fraud prevention and the main question

00:01:59,929 --> 00:02:04,189
I've been asked is why did you start in

00:02:02,389 --> 00:02:07,090
new projects we already have Google

00:02:04,189 --> 00:02:10,310
Analytics on mature and stuff like that

00:02:07,090 --> 00:02:14,569
the main reason is data quality and

00:02:10,310 --> 00:02:16,400
details so if you look at a page most of

00:02:14,569 --> 00:02:19,010
these JavaScript based solutions like

00:02:16,400 --> 00:02:19,370
Omniture and Google they measure the

00:02:19,010 --> 00:02:22,849
page

00:02:19,370 --> 00:02:25,459
and that's it and what we want is we

00:02:22,849 --> 00:02:26,930
want to know exactly all of the parts we

00:02:25,459 --> 00:02:29,599
put on the page and why we put them

00:02:26,930 --> 00:02:31,790
there now it's not uncommon that our

00:02:29,599 --> 00:02:34,430
website does three four thousand pages

00:02:31,790 --> 00:02:37,760
per second and if we measure everything

00:02:34,430 --> 00:02:41,810
on all pages should yield something like

00:02:37,760 --> 00:02:46,340
30 events per second and or a per page

00:02:41,810 --> 00:02:48,019
which is about 100k per second now the

00:02:46,340 --> 00:02:49,940
project has been partially implemented

00:02:48,019 --> 00:02:53,720
not all aspects of the website have been

00:02:49,940 --> 00:02:56,030
tagged yet but around this time we're

00:02:53,720 --> 00:03:00,680
already part passing the 1 billion

00:02:56,030 --> 00:03:02,930
measurements per day now the alternative

00:03:00,680 --> 00:03:05,000
systems like Omniture and Google

00:03:02,930 --> 00:03:09,040
Analytics are all JavaScript based and

00:03:05,000 --> 00:03:12,470
in my opinion they are broken

00:03:09,040 --> 00:03:14,599
fundamentally broken primarily because

00:03:12,470 --> 00:03:16,879
you're measuring a side effect you're

00:03:14,599 --> 00:03:19,430
not measuring the page you're measuring

00:03:16,879 --> 00:03:22,849
the effect of a script that is on the

00:03:19,430 --> 00:03:26,329
page and that means that stuff goes

00:03:22,849 --> 00:03:28,940
missing stuff goes duplicate and in fact

00:03:26,329 --> 00:03:31,160
the measuring of orders is on the order

00:03:28,940 --> 00:03:34,639
confirmation page which is a side effect

00:03:31,160 --> 00:03:37,489
of a side effect these tags are

00:03:34,639 --> 00:03:39,769
blockable ITP that is built into the

00:03:37,489 --> 00:03:43,310
Safari browsers and into Firefox ad

00:03:39,769 --> 00:03:45,440
blockers robots hackers all those who

00:03:43,310 --> 00:03:50,569
things cannot be measured using

00:03:45,440 --> 00:03:52,970
javascript and many of those are boxed

00:03:50,569 --> 00:03:56,299
so in all mature you have something

00:03:52,970 --> 00:03:58,419
called an Ivar and that's a variable you

00:03:56,299 --> 00:04:01,400
can put in but they're fixed in number

00:03:58,419 --> 00:04:06,019
so you always have not enough of them

00:04:01,400 --> 00:04:08,840
and in Google Analytics we at some point

00:04:06,019 --> 00:04:11,480
saw this you know number of users and

00:04:08,840 --> 00:04:14,569
then we saw a bump and then a systematic

00:04:11,480 --> 00:04:16,459
uplift so I started digging what

00:04:14,569 --> 00:04:19,700
happened there turns out that if you

00:04:16,459 --> 00:04:22,130
zoom in by the hour we are loading

00:04:19,700 --> 00:04:26,900
additional labels that came out of our

00:04:22,130 --> 00:04:28,880
data science to tag users the tagging of

00:04:26,900 --> 00:04:30,590
the users actually changed the

00:04:28,880 --> 00:04:32,300
measurements in number of visitors or

00:04:30,590 --> 00:04:34,830
users on the site

00:04:32,300 --> 00:04:36,710
so in my opinion javascript-based

00:04:34,830 --> 00:04:39,990
measurements are something like this

00:04:36,710 --> 00:04:44,610
then people still say we're data-driven

00:04:39,990 --> 00:04:46,410
and I go like yes seriously an

00:04:44,610 --> 00:04:48,120
additional problem of the JavaScript

00:04:46,410 --> 00:04:50,100
measurements is that in general for

00:04:48,120 --> 00:04:54,210
doing kind of personalization they're

00:04:50,100 --> 00:04:56,910
too old so because they're loaded every

00:04:54,210 --> 00:04:58,170
24 hours personalization is hey I look

00:04:56,910 --> 00:05:01,050
at this product maybe these are

00:04:58,170 --> 00:05:03,780
interesting you know that's that's not

00:05:01,050 --> 00:05:05,790
what I want and the key thing here is

00:05:03,780 --> 00:05:10,530
that the data quality the value of the

00:05:05,790 --> 00:05:12,090
day that drops rapidly by time and for

00:05:10,530 --> 00:05:14,040
those situations where you want to

00:05:12,090 --> 00:05:17,790
personalize the website you really want

00:05:14,040 --> 00:05:20,220
to be here in the low seconds area to be

00:05:17,790 --> 00:05:25,100
up to speed with a visitor at on the

00:05:20,220 --> 00:05:30,120
side now do note that not all systems

00:05:25,100 --> 00:05:32,670
require such a low latency if you're

00:05:30,120 --> 00:05:34,800
optimizing if you're doing analysis for

00:05:32,670 --> 00:05:38,490
future visitors like building a

00:05:34,800 --> 00:05:40,860
recommender data set batch is fine but

00:05:38,490 --> 00:05:43,020
if you're doing something that analyzes

00:05:40,860 --> 00:05:45,830
the behavior of the visitor on the site

00:05:43,020 --> 00:05:50,010
you want to be as fast as possible so

00:05:45,830 --> 00:05:53,130
the project supports not only the very

00:05:50,010 --> 00:05:54,660
old-style Hadoop type of processing but

00:05:53,130 --> 00:05:57,240
we also ship the data into bigquery

00:05:54,660 --> 00:05:59,970
because we use both Hadoop and the

00:05:57,240 --> 00:06:03,060
Google cloud and we want to support

00:05:59,970 --> 00:06:08,720
stream processing things like flink or

00:06:03,060 --> 00:06:11,940
beam so the project m2 is it's all about

00:06:08,720 --> 00:06:14,400
making better measurements to allow

00:06:11,940 --> 00:06:16,620
better processing and make the site and

00:06:14,400 --> 00:06:20,070
all the personalization applications

00:06:16,620 --> 00:06:22,500
more relevant and of course like I said

00:06:20,070 --> 00:06:25,170
we want to measure everything all

00:06:22,500 --> 00:06:27,870
interactions all visitors even hackers

00:06:25,170 --> 00:06:31,200
in Google bot make the site data the

00:06:27,870 --> 00:06:32,580
measurements more reliable and reducing

00:06:31,200 --> 00:06:36,360
the load on the client because although

00:06:32,580 --> 00:06:39,830
javascript is really heavy and drop the

00:06:36,360 --> 00:06:42,210
latency basically we want everything and

00:06:39,830 --> 00:06:45,400
of course we want it easier for

00:06:42,210 --> 00:06:47,500
developer to build because that's nasty

00:06:45,400 --> 00:06:49,419
the business or data science people they

00:06:47,500 --> 00:06:52,270
want to ask different questions they

00:06:49,419 --> 00:06:54,970
didn't know beforehand and we took into

00:06:52,270 --> 00:06:58,389
account the whole privacy and security

00:06:54,970 --> 00:07:01,509
discussions which at one and says no

00:06:58,389 --> 00:07:02,979
long-term profiling no security problems

00:07:01,509 --> 00:07:05,410
and the business sells it needs

00:07:02,979 --> 00:07:10,990
long-term profile which is a bit of a

00:07:05,410 --> 00:07:13,500
catch but we figure that out simply put

00:07:10,990 --> 00:07:16,060
the goal is to make the best possible

00:07:13,500 --> 00:07:19,120
interaction data stream knowing what

00:07:16,060 --> 00:07:20,889
happened on the website and to do that

00:07:19,120 --> 00:07:23,770
we had to make a lot of choices over the

00:07:20,889 --> 00:07:28,720
years let's start with the start of the

00:07:23,770 --> 00:07:32,010
flow measuring the fundamental choice we

00:07:28,720 --> 00:07:35,710
made is that we measure where it happens

00:07:32,010 --> 00:07:37,960
so where we are absolutely sure the

00:07:35,710 --> 00:07:40,479
thing happened that the user did or that

00:07:37,960 --> 00:07:42,970
we did that's where we do it that means

00:07:40,479 --> 00:07:45,820
usually that most of the measurements

00:07:42,970 --> 00:07:48,910
are done in some kind of front-end close

00:07:45,820 --> 00:07:52,510
to the user surface either the web shop

00:07:48,910 --> 00:07:55,150
or the API that services our app or very

00:07:52,510 --> 00:07:56,979
close to a system that owns the event

00:07:55,150 --> 00:08:02,440
that actually happens like our basket

00:07:56,979 --> 00:08:05,770
server or ordering service it's

00:08:02,440 --> 00:08:07,720
important to realize that usually this

00:08:05,770 --> 00:08:10,300
means we are not measuring these things

00:08:07,720 --> 00:08:13,510
in the browser because in the browser

00:08:10,300 --> 00:08:15,880
it's always a side effect now for

00:08:13,510 --> 00:08:18,010
example measuring a page in the data

00:08:15,880 --> 00:08:21,280
center in our servers we measure what we

00:08:18,010 --> 00:08:23,110
actually put in the page we do have

00:08:21,280 --> 00:08:24,940
measurements at the browser end but

00:08:23,110 --> 00:08:27,789
those are things that actually happen in

00:08:24,940 --> 00:08:30,159
the browser which part was actually in

00:08:27,789 --> 00:08:35,560
view and what was the screen resolution

00:08:30,159 --> 00:08:38,680
for example orders are now measured in

00:08:35,560 --> 00:08:41,770
our ordering service and the order

00:08:38,680 --> 00:08:43,839
confirmation page that in the JavaScript

00:08:41,770 --> 00:08:46,600
based solutions are classified as the

00:08:43,839 --> 00:08:48,670
order are now just classified as the

00:08:46,600 --> 00:08:52,630
viewing of an order that was just placed

00:08:48,670 --> 00:08:56,829
so this is not anymore the order but

00:08:52,630 --> 00:08:58,830
viewing of an existing order what we

00:08:56,829 --> 00:09:01,320
also do and that is has to do

00:08:58,830 --> 00:09:04,350
with the volume and denormalizing

00:09:01,320 --> 00:09:07,200
everything is that when the event occurs

00:09:04,350 --> 00:09:10,770
we record as many attributes as we can

00:09:07,200 --> 00:09:13,260
so we record the product number things

00:09:10,770 --> 00:09:17,190
like the product type the title etc at

00:09:13,260 --> 00:09:20,040
the moment of creating the page also if

00:09:17,190 --> 00:09:22,850
we have an offer from a seller we record

00:09:20,040 --> 00:09:25,560
the price in the condition and seller ID

00:09:22,850 --> 00:09:27,750
now people have said to me that yeah but

00:09:25,560 --> 00:09:30,360
you only need the offer ID because all

00:09:27,750 --> 00:09:32,310
of these others can be joined later well

00:09:30,360 --> 00:09:35,490
there are two very important reasons why

00:09:32,310 --> 00:09:38,640
you can't our webshop our ordering

00:09:35,490 --> 00:09:41,370
system has some caching built in so even

00:09:38,640 --> 00:09:43,020
though the cache may be seconds even

00:09:41,370 --> 00:09:46,020
within that second you can have a

00:09:43,020 --> 00:09:48,240
situation where you are saying I showed

00:09:46,020 --> 00:09:50,250
you five euros when the reality was it

00:09:48,240 --> 00:09:55,140
was six euros because you were still

00:09:50,250 --> 00:09:57,000
looking at a slightly older price now

00:09:55,140 --> 00:09:59,550
looking at the general pattern of what

00:09:57,000 --> 00:10:04,110
we want to do with the data in our

00:09:59,550 --> 00:10:06,510
situation is cause and effect if you

00:10:04,110 --> 00:10:08,910
don't have cause and effect if all the

00:10:06,510 --> 00:10:10,550
measurements are single things that are

00:10:08,910 --> 00:10:14,700
completely independent and unrelated

00:10:10,550 --> 00:10:16,500
it's easy processing is easy and none of

00:10:14,700 --> 00:10:20,520
the things that I'm going to show you

00:10:16,500 --> 00:10:23,460
matter but our use cases are things like

00:10:20,520 --> 00:10:26,340
banner optimization or a bee testing or

00:10:23,460 --> 00:10:28,970
search suggestions or attribution

00:10:26,340 --> 00:10:32,010
modeling and in all of these cases it's

00:10:28,970 --> 00:10:35,330
we do something we show something and

00:10:32,010 --> 00:10:38,160
then the user either responds or doesn't

00:10:35,330 --> 00:10:40,230
so it's always a cause and effect an

00:10:38,160 --> 00:10:42,480
action and a reaction and it's that

00:10:40,230 --> 00:10:46,700
causality that we're really interested

00:10:42,480 --> 00:10:50,910
in and in those chains of causality

00:10:46,700 --> 00:10:53,450
ordering of events matters so if you

00:10:50,910 --> 00:10:57,270
click a banner and then buy a product

00:10:53,450 --> 00:11:00,000
then the banner is most likely part of

00:10:57,270 --> 00:11:02,520
the reason why you bought it if the

00:11:00,000 --> 00:11:05,820
order of these two events is reverse it

00:11:02,520 --> 00:11:10,230
was definitely not a reason for buying

00:11:05,820 --> 00:11:12,380
it so the was or was not is based on the

00:11:10,230 --> 00:11:15,750
order of the events

00:11:12,380 --> 00:11:19,050
now nice thing about having ordered

00:11:15,750 --> 00:11:21,450
events is that analyzing them figuring

00:11:19,050 --> 00:11:23,010
out what happened can be done with

00:11:21,450 --> 00:11:26,010
something like a finite state machine

00:11:23,010 --> 00:11:30,000
which in general is a very very suitable

00:11:26,010 --> 00:11:34,350
for low latency analysis and finding of

00:11:30,000 --> 00:11:36,960
specific patterns in streams in general

00:11:34,350 --> 00:11:39,090
I have found that you need to go one

00:11:36,960 --> 00:11:41,490
step more advanced and that is a

00:11:39,090 --> 00:11:43,860
pushdown automata Moton

00:11:41,490 --> 00:11:47,010
which is essentially a state machine

00:11:43,860 --> 00:11:52,830
with a little bit of memory and I'll

00:11:47,010 --> 00:11:54,720
show you an example later on so why is

00:11:52,830 --> 00:11:58,230
this event ordering so important well

00:11:54,720 --> 00:12:00,450
let's assume I have an IOT situation so

00:11:58,230 --> 00:12:02,760
not a webshop at an IOT situation where

00:12:00,450 --> 00:12:05,160
I'm measuring the temperature of

00:12:02,760 --> 00:12:07,140
something and a fast temperature change

00:12:05,160 --> 00:12:10,230
is dangerous it may explode or something

00:12:07,140 --> 00:12:13,410
and if that happens I want to be alerted

00:12:10,230 --> 00:12:15,420
immediately and I do that by

00:12:13,410 --> 00:12:18,000
implementing a very simple state machine

00:12:15,420 --> 00:12:21,000
that calculates the delta from the

00:12:18,000 --> 00:12:23,730
previous measurement a very simple push

00:12:21,000 --> 00:12:25,500
down automata automaton and the memory

00:12:23,730 --> 00:12:28,110
is just the current the previous

00:12:25,500 --> 00:12:30,240
measurement so I take the Delta from

00:12:28,110 --> 00:12:34,920
these two and if it stays within bounds

00:12:30,240 --> 00:12:37,740
all is fine but if I introduce ordering

00:12:34,920 --> 00:12:40,620
problems and all I did here is reshuffle

00:12:37,740 --> 00:12:43,320
the orders you get lots and lots of

00:12:40,620 --> 00:12:47,670
problems you get false positives and you

00:12:43,320 --> 00:12:52,980
get false negatives now do you realize

00:12:47,670 --> 00:12:57,030
that repairing event ordering is hard it

00:12:52,980 --> 00:12:59,700
is needless complexity but also it takes

00:12:57,030 --> 00:13:01,890
time because the general pattern you do

00:12:59,700 --> 00:13:03,750
when you want to repair ordering in an

00:13:01,890 --> 00:13:06,960
event stream is that you create a

00:13:03,750 --> 00:13:10,140
time-based buffer of the estimated

00:13:06,960 --> 00:13:12,240
maximum out of ordinates and that slides

00:13:10,140 --> 00:13:14,520
over time with your events and then

00:13:12,240 --> 00:13:16,770
within that buffer you really reshuffle

00:13:14,520 --> 00:13:19,830
them and when you're certain enough or

00:13:16,770 --> 00:13:22,080
an event is ordered correctly you output

00:13:19,830 --> 00:13:25,350
it to the next step but that can be

00:13:22,080 --> 00:13:25,769
several minutes later and that is too

00:13:25,350 --> 00:13:29,459
long

00:13:25,769 --> 00:13:31,920
for a use case in our use case we would

00:13:29,459 --> 00:13:35,879
also really like to have exactly once

00:13:31,920 --> 00:13:38,519
also their deduplication knowing what

00:13:35,879 --> 00:13:42,749
happened before is pretty hard to

00:13:38,519 --> 00:13:45,629
maintain and and hard to build and in

00:13:42,749 --> 00:13:50,399
our volumes would mean a pretty large

00:13:45,629 --> 00:13:53,279
memory buffer - to guarantee that so

00:13:50,399 --> 00:13:55,649
simply put we need ordering guarantees

00:13:53,279 --> 00:13:58,679
per session because the event stream

00:13:55,649 --> 00:14:01,439
we're looking at is a session a single

00:13:58,679 --> 00:14:02,790
visitor that visits the website and

00:14:01,439 --> 00:14:08,610
that's the cause and effect relationship

00:14:02,790 --> 00:14:12,619
we're looking at the only way you can do

00:14:08,610 --> 00:14:15,959
that if you can guarantee end-to-end

00:14:12,619 --> 00:14:18,449
ordering so from where the user does

00:14:15,959 --> 00:14:20,429
something you measure it you transport

00:14:18,449 --> 00:14:23,009
the data into your processing stack and

00:14:20,429 --> 00:14:25,290
the first phases of the processing must

00:14:23,009 --> 00:14:29,610
be able to support these ordering

00:14:25,290 --> 00:14:33,029
guarantees now the measuring point I

00:14:29,610 --> 00:14:36,240
strongly recommend using a single entity

00:14:33,029 --> 00:14:39,240
and then using a single measurement

00:14:36,240 --> 00:14:41,100
system for that single entity because if

00:14:39,240 --> 00:14:43,829
you have multiple instances for example

00:14:41,100 --> 00:14:46,319
you do a round-robin load balancing each

00:14:43,829 --> 00:14:49,319
instance will have a measuring output

00:14:46,319 --> 00:14:51,980
buffer that will retain measurements for

00:14:49,319 --> 00:14:55,410
a short while whatever that time is and

00:14:51,980 --> 00:14:57,749
because they will be flushed independent

00:14:55,410 --> 00:15:01,049
of the actual events you will get race

00:14:57,749 --> 00:15:03,089
conditions and does ordering problems so

00:15:01,049 --> 00:15:04,920
in IOT you see something like this you

00:15:03,089 --> 00:15:07,579
know you have a measuring device and a

00:15:04,920 --> 00:15:10,919
sensor that are tightly coupled and

00:15:07,579 --> 00:15:13,619
eyeball become we are saying one visitor

00:15:10,919 --> 00:15:15,569
should be on one single instance of our

00:15:13,619 --> 00:15:18,689
webshop we have a lot of them but you

00:15:15,569 --> 00:15:22,199
know so we have a session routing is for

00:15:18,689 --> 00:15:24,329
the data quality and must-have now do

00:15:22,199 --> 00:15:27,110
you note that there that it is not

00:15:24,329 --> 00:15:29,669
perfect but the impact of this

00:15:27,110 --> 00:15:31,829
imperfection is negligible because it

00:15:29,669 --> 00:15:34,860
only affects the view measurements that

00:15:31,829 --> 00:15:38,759
are which parts of the page are actually

00:15:34,860 --> 00:15:39,660
shown on the screen and the orders so

00:15:38,759 --> 00:15:43,140
now we have

00:15:39,660 --> 00:15:45,870
neatly ordered measurements and we need

00:15:43,140 --> 00:15:47,370
to transport them transport them in such

00:15:45,870 --> 00:15:52,110
a way that our processing stack can

00:15:47,370 --> 00:15:56,040
handle them so we need ordering first-in

00:15:52,110 --> 00:15:57,960
first-out people call that a queue or in

00:15:56,040 --> 00:16:00,510
some cases I would call it a partition

00:15:57,960 --> 00:16:02,760
queue and then you would pin a specific

00:16:00,510 --> 00:16:06,150
session to a specific partition to

00:16:02,760 --> 00:16:08,700
maintain the ordering so Wikipedia says

00:16:06,150 --> 00:16:10,860
quite nicely the entities are kept in

00:16:08,700 --> 00:16:13,590
order that's what the queue defines and

00:16:10,860 --> 00:16:16,380
and what I call partition queue is

00:16:13,590 --> 00:16:20,070
essentially a single thing that collects

00:16:16,380 --> 00:16:23,130
a couple of queues together the big

00:16:20,070 --> 00:16:25,170
problem in our IT land is that there are

00:16:23,130 --> 00:16:30,720
so many systems that call themselves

00:16:25,170 --> 00:16:32,430
queue that are not JMS SS subsea thing

00:16:30,720 --> 00:16:35,700
called queue which does not maintain

00:16:32,430 --> 00:16:38,540
order there is a thing called active MQ

00:16:35,700 --> 00:16:41,490
which does not maintain order

00:16:38,540 --> 00:16:43,740
google has pops-up they have an

00:16:41,490 --> 00:16:45,780
elaborate marketing page explaining that

00:16:43,740 --> 00:16:51,890
you do not need order because they can't

00:16:45,780 --> 00:16:55,500
can't deliver it within Bolcom while ago

00:16:51,890 --> 00:16:57,960
people's implemented a new queueing

00:16:55,500 --> 00:16:59,520
system they call a human system but

00:16:57,960 --> 00:17:04,319
people ran into that it doesn't maintain

00:16:59,520 --> 00:17:07,620
order now luckily there are a couple of

00:17:04,319 --> 00:17:09,870
systems that do maintain order and the

00:17:07,620 --> 00:17:12,300
most well-known that we also use in

00:17:09,870 --> 00:17:13,920
production is apache Kafka

00:17:12,300 --> 00:17:16,439
but do you note that there is also a

00:17:13,920 --> 00:17:19,800
patchy pulsar that sports that maintains

00:17:16,439 --> 00:17:23,870
ordering few months ago I was made aware

00:17:19,800 --> 00:17:26,370
of a pretty new project called Pro Vega

00:17:23,870 --> 00:17:28,850
it's not ready yet it's not a patchy

00:17:26,370 --> 00:17:31,890
light also but it is Apache License

00:17:28,850 --> 00:17:35,430
which also is set to maintain ordering

00:17:31,890 --> 00:17:38,940
but I haven't tried it yet now we as a

00:17:35,430 --> 00:17:43,380
company are doing data center and Google

00:17:38,940 --> 00:17:44,100
Cloud amazon has kinases which is the

00:17:43,380 --> 00:17:47,010
wrong clouds

00:17:44,100 --> 00:17:50,190
Microsoft has event ups which is the

00:17:47,010 --> 00:17:52,770
wrong cloud so I'm telling my colleagues

00:17:50,190 --> 00:17:53,290
ok go ahead and use Google pops up

00:17:52,770 --> 00:17:56,170
because

00:17:53,290 --> 00:17:58,420
it's a good system as long as you

00:17:56,170 --> 00:18:01,630
remember that the ordering is messed up

00:17:58,420 --> 00:18:04,660
and yet you get at least once so some

00:18:01,630 --> 00:18:07,920
events come in twice treat it as a high

00:18:04,660 --> 00:18:11,290
io distributed set and you're fine if

00:18:07,920 --> 00:18:14,080
you need ordering use Apache Kafka

00:18:11,290 --> 00:18:16,720
because then you have something where

00:18:14,080 --> 00:18:18,670
ordering is maintained and in

00:18:16,720 --> 00:18:20,620
combination with Apache flink you can

00:18:18,670 --> 00:18:23,980
guarantee exactly once for your

00:18:20,620 --> 00:18:27,880
processing Kafka in my words is an high

00:18:23,980 --> 00:18:32,290
i/o partition queue so now we have the

00:18:27,880 --> 00:18:35,650
data at our processing and how do we

00:18:32,290 --> 00:18:37,330
process well we have of course the same

00:18:35,650 --> 00:18:39,370
requirements for the processing stack

00:18:37,330 --> 00:18:42,520
low latency exactly once

00:18:39,370 --> 00:18:44,920
ordering guarantees and support for a

00:18:42,520 --> 00:18:47,650
push them down automaton per session you

00:18:44,920 --> 00:18:48,850
know Keit stateful processing where the

00:18:47,650 --> 00:18:54,370
key would then be something like a

00:18:48,850 --> 00:18:56,740
session ID we had a good look at Apache

00:18:54,370 --> 00:18:58,660
beam primarily because it runs natively

00:18:56,740 --> 00:19:02,110
in the Google pod it's essentially the

00:18:58,660 --> 00:19:04,930
open source version of the Google API it

00:19:02,110 --> 00:19:09,130
does support low latency except it does

00:19:04,930 --> 00:19:12,280
at least once exactly once is done by

00:19:09,130 --> 00:19:14,200
additional deduplication and there are

00:19:12,280 --> 00:19:17,680
no ordering guarantees and there's no

00:19:14,200 --> 00:19:20,470
natural key stateful processing and the

00:19:17,680 --> 00:19:22,630
primary reason for that is that in my

00:19:20,470 --> 00:19:26,290
opinion all of the Google tools have a

00:19:22,630 --> 00:19:29,500
very clear requirement and that is that

00:19:26,290 --> 00:19:31,240
they support dynamic scaling when you

00:19:29,500 --> 00:19:33,490
increase the load the system can

00:19:31,240 --> 00:19:34,930
automatically use more resources if you

00:19:33,490 --> 00:19:38,410
decrease the load it can scale down

00:19:34,930 --> 00:19:40,720
again this dynamic behavior is as far as

00:19:38,410 --> 00:19:44,350
I can tell the primary reason why these

00:19:40,720 --> 00:19:46,420
features are not there in addition to

00:19:44,350 --> 00:19:49,810
that I personally have a dislike for the

00:19:46,420 --> 00:19:52,000
Java API but that's a personal thing the

00:19:49,810 --> 00:19:54,100
nice thing about bean bill is that it

00:19:52,000 --> 00:19:56,380
runs both on dataflow and on other

00:19:54,100 --> 00:20:00,220
execution engines like flink so you can

00:19:56,380 --> 00:20:04,170
run a beam job on fling on a Hadoop

00:20:00,220 --> 00:20:04,170
stack I've tried that actually works

00:20:04,600 --> 00:20:10,749
now my preference is Apache fling for

00:20:07,729 --> 00:20:13,970
this type of processing because fling

00:20:10,749 --> 00:20:16,249
internally requires ordering guarantees

00:20:13,970 --> 00:20:18,889
and low latency and that has all to do

00:20:16,249 --> 00:20:21,080
with that it does the exactly ones

00:20:18,889 --> 00:20:23,840
processing by the chain D Lamport

00:20:21,080 --> 00:20:25,909
checkpointing system so internally it

00:20:23,840 --> 00:20:28,940
has the low lay to see exactly once with

00:20:25,909 --> 00:20:31,340
ordering guarantees and as a consequence

00:20:28,940 --> 00:20:35,960
something like keyed stateful processing

00:20:31,340 --> 00:20:39,649
is just there and the recovery of heat

00:20:35,960 --> 00:20:43,249
stateful processing with a recover in a

00:20:39,649 --> 00:20:45,379
disaster scenario also works but you get

00:20:43,249 --> 00:20:47,509
the quote unquote fixed scaling they're

00:20:45,379 --> 00:20:49,909
working on making that more dynamic but

00:20:47,509 --> 00:20:54,019
still it the underlying design model is

00:20:49,909 --> 00:20:56,269
there the java api is in my opinion much

00:20:54,019 --> 00:20:58,879
better if you say i have a data stream

00:20:56,269 --> 00:21:00,830
and then i do a key by you get a key to

00:20:58,879 --> 00:21:02,539
data stream if you say I'm doing a

00:21:00,830 --> 00:21:05,029
window on the data stream you get a

00:21:02,539 --> 00:21:06,830
window to data stream so that makes

00:21:05,029 --> 00:21:09,590
makes it for the developer a lot easier

00:21:06,830 --> 00:21:11,960
and it runs on both the Hadoop stack and

00:21:09,590 --> 00:21:13,849
on cubed Andy's colleagues of mine are

00:21:11,960 --> 00:21:18,200
running a fling in the Google cloud

00:21:13,849 --> 00:21:20,809
works fine so now we have the whole

00:21:18,200 --> 00:21:23,960
stack from measuring to processing and

00:21:20,809 --> 00:21:28,879
everything in between and then something

00:21:23,960 --> 00:21:31,580
changes people have improved insights

00:21:28,879 --> 00:21:34,460
new business models new wishes things go

00:21:31,580 --> 00:21:37,489
away so the records we get new fields

00:21:34,460 --> 00:21:41,599
fields get dropped etc and in general

00:21:37,489 --> 00:21:45,289
you see that a streaming scenario you

00:21:41,599 --> 00:21:47,889
have a system that produce data you have

00:21:45,289 --> 00:21:51,409
a streaming interface like Kafka but

00:21:47,889 --> 00:21:58,809
essentially all of them need a byte

00:21:51,409 --> 00:22:04,549
array as their payload and you remember

00:21:58,809 --> 00:22:07,429
that in addition to the adding of new

00:22:04,549 --> 00:22:09,349
fields and new features you have

00:22:07,429 --> 00:22:12,590
multiple applications you do rolling

00:22:09,349 --> 00:22:16,519
upgrades you don't do canary releases so

00:22:12,590 --> 00:22:17,669
the reality is that you have the same at

00:22:16,519 --> 00:22:21,659
the consumer

00:22:17,669 --> 00:22:24,959
and the reality is that at every moment

00:22:21,659 --> 00:22:30,450
in time your topics will contain a

00:22:24,959 --> 00:22:34,559
mixture of versions of the data and so

00:22:30,450 --> 00:22:36,899
we need something that supports that we

00:22:34,559 --> 00:22:39,539
need something that allows us to define

00:22:36,899 --> 00:22:41,519
a record to fill in the fields convert

00:22:39,539 --> 00:22:43,649
that into a set of bytes and back the

00:22:41,519 --> 00:22:46,559
things that support data types data

00:22:43,649 --> 00:22:52,289
structures and bi-directional schema

00:22:46,559 --> 00:22:55,079
evolution two ways now being a PMC for

00:22:52,289 --> 00:22:56,639
any committer for Apache Avro I had a

00:22:55,079 --> 00:22:59,219
look at what people around the world are

00:22:56,639 --> 00:23:02,539
already doing and we decided to just put

00:22:59,219 --> 00:23:06,119
it into Avro as a part of the stack so

00:23:02,539 --> 00:23:09,479
you can now do a schema definition in

00:23:06,119 --> 00:23:11,459
for example the IDL the form it looks

00:23:09,479 --> 00:23:14,339
like this it's actually quite easy to do

00:23:11,459 --> 00:23:16,739
and then the code generation creates

00:23:14,339 --> 00:23:19,229
codes with builders and getters and

00:23:16,739 --> 00:23:21,119
setters that allows filling in the data

00:23:19,229 --> 00:23:25,200
structures and finding out what is

00:23:21,119 --> 00:23:27,779
mandatory and what not very easy it also

00:23:25,200 --> 00:23:29,909
creates Javadoc that includes the

00:23:27,779 --> 00:23:31,859
comments that you put in there as part

00:23:29,909 --> 00:23:34,769
of the Javadoc for the downstream

00:23:31,859 --> 00:23:37,289
consumers to read so it also makes

00:23:34,769 --> 00:23:41,609
documenting the fields for the consumers

00:23:37,289 --> 00:23:44,909
easier in order to do this I added the

00:23:41,609 --> 00:23:47,579
Avro message format which is essentially

00:23:44,909 --> 00:23:49,859
a serialization of a single records into

00:23:47,579 --> 00:23:52,099
a bunch of bytes and it was designed for

00:23:49,859 --> 00:23:54,419
this use guys or this type of use case

00:23:52,099 --> 00:23:57,059
there is however one thing you really

00:23:54,419 --> 00:23:59,940
need and that is a schema database of

00:23:57,059 --> 00:24:02,659
some sort a very simple key value thing

00:23:59,940 --> 00:24:06,019
64 bit long being the key which is the

00:24:02,659 --> 00:24:08,009
fingerprint of the schema and the string

00:24:06,019 --> 00:24:14,940
representation of the schema which is in

00:24:08,009 --> 00:24:18,839
a JSON format if I then use this to fill

00:24:14,940 --> 00:24:21,959
in data to shove it into Kafka from

00:24:18,839 --> 00:24:24,389
Apache flink I first of all need to

00:24:21,959 --> 00:24:27,029
create something called a serializer for

00:24:24,389 --> 00:24:29,069
that data type which has a method and

00:24:27,029 --> 00:24:31,289
you could do something like person to

00:24:29,069 --> 00:24:33,600
byte buffer and then pull out the

00:24:31,289 --> 00:24:35,880
the actual bytearray I was recently told

00:24:33,600 --> 00:24:39,840
that this is the wrong call to do that

00:24:35,880 --> 00:24:42,389
so be aware don't copy it directly and

00:24:39,840 --> 00:24:45,059
then in your your application you have a

00:24:42,389 --> 00:24:47,909
data stream of the generated class and

00:24:45,059 --> 00:24:51,419
then you say hey add a sink for Kafka

00:24:47,909 --> 00:24:53,250
and use that serialize ER and from there

00:24:51,419 --> 00:24:56,159
the data your records will be serialized

00:24:53,250 --> 00:24:58,080
in the correct format shift into Avro a

00:24:56,159 --> 00:25:01,799
shift into Kafka and then you can

00:24:58,080 --> 00:25:04,470
consume them and the consume code also

00:25:01,799 --> 00:25:06,059
has something in the serialization area

00:25:04,470 --> 00:25:09,120
but now it's a DC rÃ©aliser

00:25:06,059 --> 00:25:12,360
which is one additional step because the

00:25:09,120 --> 00:25:17,760
the message decoder from Avro for your

00:25:12,360 --> 00:25:20,250
class needs to be able to retrieve the

00:25:17,760 --> 00:25:22,289
fingerprint from your data store if it

00:25:20,250 --> 00:25:25,919
receives a message for which it doesn't

00:25:22,289 --> 00:25:27,630
have the schema yet if once it retrieves

00:25:25,919 --> 00:25:28,950
it it caches it and keeps it in memory

00:25:27,630 --> 00:25:32,880
in a compiled form so it's really

00:25:28,950 --> 00:25:34,980
efficient and then you create the D

00:25:32,880 --> 00:25:38,970
serialization method that just does this

00:25:34,980 --> 00:25:41,070
and then your application you say hey I

00:25:38,970 --> 00:25:44,970
want to get a data stream from person

00:25:41,070 --> 00:25:49,559
from the source in Flynn Kafka with that

00:25:44,970 --> 00:25:51,360
D serializer and in the event that D

00:25:49,559 --> 00:25:53,789
serialization problem occurs you get

00:25:51,360 --> 00:25:56,220
garbage you get a nil value back so be

00:25:53,789 --> 00:25:59,580
sure to drop the nil values otherwise

00:25:56,220 --> 00:26:00,990
the rest of the chain may fail now to

00:25:59,580 --> 00:26:05,940
give you a bit of an overview of what

00:26:00,990 --> 00:26:08,190
the project looks like today we have our

00:26:05,940 --> 00:26:12,179
web shop where we built the HTML there

00:26:08,190 --> 00:26:13,679
we have included a library attached to

00:26:12,179 --> 00:26:15,389
that that creates the measurements and

00:26:13,679 --> 00:26:19,409
puts them serializes them and puts them

00:26:15,389 --> 00:26:22,860
in Kafka that is in our webshop then the

00:26:19,409 --> 00:26:25,769
HTML is tagged with IDs and those get

00:26:22,860 --> 00:26:28,049
put in the HTML and the browser then

00:26:25,769 --> 00:26:30,029
sends the in view measurements to

00:26:28,049 --> 00:26:34,320
separate endpoints that also puts them

00:26:30,029 --> 00:26:36,450
in Kafka things like orders are from our

00:26:34,320 --> 00:26:38,850
checkout system into our ordering system

00:26:36,450 --> 00:26:41,549
and those are also firing measurement

00:26:38,850 --> 00:26:43,559
endpoint into Kafka so there we have a

00:26:41,549 --> 00:26:45,470
very complete stream of what is actually

00:26:43,559 --> 00:26:48,240
being done by our visitor

00:26:45,470 --> 00:26:51,390
the next step is something we call the

00:26:48,240 --> 00:26:54,510
session Iser because it detects the

00:26:51,390 --> 00:26:57,180
visit pattern as in after 30 minutes

00:26:54,510 --> 00:27:00,720
being idle you're attached a new visit

00:26:57,180 --> 00:27:04,470
ID which makes sense to make things more

00:27:00,720 --> 00:27:06,990
palatable we add geoip information so we

00:27:04,470 --> 00:27:09,980
know the country and the ISP or came

00:27:06,990 --> 00:27:13,620
from and we use the user agent analyzer

00:27:09,980 --> 00:27:14,970
so we can pull out all the fields from

00:27:13,620 --> 00:27:17,190
the user agent that we're really

00:27:14,970 --> 00:27:19,290
interested in and drop the actual user

00:27:17,190 --> 00:27:23,370
agent string because the it's so unique

00:27:19,290 --> 00:27:25,950
it's a PII thing now that data is shoved

00:27:23,370 --> 00:27:29,850
into Kafka files and in bigquery for

00:27:25,950 --> 00:27:32,880
analysis by our people but because this

00:27:29,850 --> 00:27:35,610
is a full detail with everything in it

00:27:32,880 --> 00:27:39,750
it's a PII think so only our security

00:27:35,610 --> 00:27:43,380
and fraud teams have access to this to

00:27:39,750 --> 00:27:46,770
allow personalization we have added an

00:27:43,380 --> 00:27:51,240
anonymized step that drops a lot of

00:27:46,770 --> 00:27:53,340
fields keeps the customer number so we

00:27:51,240 --> 00:27:56,250
can still recognize the person on the

00:27:53,340 --> 00:27:58,830
website but only the number no not too

00:27:56,250 --> 00:28:00,420
many other things and that is available

00:27:58,830 --> 00:28:06,090
for all kinds of personalization

00:28:00,420 --> 00:28:08,100
campaigning etc so now that we have a

00:28:06,090 --> 00:28:11,010
bit of an overview of the entire stack

00:28:08,100 --> 00:28:14,490
how about showing a way of using the

00:28:11,010 --> 00:28:18,330
data the way it was intended now this is

00:28:14,490 --> 00:28:20,850
just a sketch of how it can be used we

00:28:18,330 --> 00:28:23,790
have a search suggestion application on

00:28:20,850 --> 00:28:26,000
our website and was originally written

00:28:23,790 --> 00:28:31,559
by me about 10 years ago in Map Reduce

00:28:26,000 --> 00:28:33,360
yes that long ago and essentially what

00:28:31,559 --> 00:28:36,360
we're saying is that if you're doing a

00:28:33,360 --> 00:28:39,470
search and right after that you're going

00:28:36,360 --> 00:28:42,240
to a product page then we say

00:28:39,470 --> 00:28:44,580
essentially an attribution model this

00:28:42,240 --> 00:28:46,559
page is caused by this search term and

00:28:44,580 --> 00:28:48,809
if you then do an add to cart and a

00:28:46,559 --> 00:28:51,480
purchase hey those are all caused by

00:28:48,809 --> 00:28:53,730
that search term because then we can say

00:28:51,480 --> 00:28:57,390
hey the further you go the more valuable

00:28:53,730 --> 00:28:58,850
it is the more relevant it is now at an

00:28:57,390 --> 00:29:02,340
high level

00:28:58,850 --> 00:29:04,740
full m2 stream comes in we would like to

00:29:02,340 --> 00:29:07,020
do some stateful analysis you know is

00:29:04,740 --> 00:29:09,179
this a valuable event how valuable is

00:29:07,020 --> 00:29:12,030
this event and then if it's relevant

00:29:09,179 --> 00:29:15,300
ship out something relevant and then do

00:29:12,030 --> 00:29:18,059
a very simple scoring and aggregating to

00:29:15,300 --> 00:29:20,990
convert the search term into a

00:29:18,059 --> 00:29:24,440
suggestion and then store it somewhere

00:29:20,990 --> 00:29:27,900
where the website can consume it again

00:29:24,440 --> 00:29:30,660
like I said before ordering is important

00:29:27,900 --> 00:29:33,600
up to a certain point and that's about

00:29:30,660 --> 00:29:35,640
here in this example after that the

00:29:33,600 --> 00:29:39,090
ordering of the events is not that

00:29:35,640 --> 00:29:41,880
relevant anymore you just put them in

00:29:39,090 --> 00:29:44,130
the database and if one overtakes the

00:29:41,880 --> 00:29:45,600
other it's it's not much of a thing

00:29:44,130 --> 00:29:49,110
because you're just adding scores

00:29:45,600 --> 00:29:52,200
together so how would such a state

00:29:49,110 --> 00:29:53,850
machine how could that look well

00:29:52,200 --> 00:29:55,590
actually it's a pushdown automaton

00:29:53,850 --> 00:29:58,320
because I need to remember the search

00:29:55,590 --> 00:30:01,170
term so I have an initial state where

00:29:58,320 --> 00:30:04,290
I'm essentially not doing search and

00:30:01,170 --> 00:30:06,900
then if I do a search for Forex I go

00:30:04,290 --> 00:30:09,210
into the state searched and I'm

00:30:06,900 --> 00:30:13,040
outputting an event that okay apparently

00:30:09,210 --> 00:30:16,170
a search something was searched for X

00:30:13,040 --> 00:30:19,620
then I go to the product page and was

00:30:16,170 --> 00:30:23,510
found I owe a product page caused by X

00:30:19,620 --> 00:30:26,220
now our product page is an overview of

00:30:23,510 --> 00:30:29,220
recommendations of alternative prices

00:30:26,220 --> 00:30:33,330
etc so you can argue that if you stay

00:30:29,220 --> 00:30:36,420
within that you stay within found and

00:30:33,330 --> 00:30:38,340
after that if you do an Add to Cart you

00:30:36,420 --> 00:30:43,290
know a we have an Add to Cart for the

00:30:38,340 --> 00:30:45,179
word X if in any of these stages you do

00:30:43,290 --> 00:30:51,990
something different you go back to the

00:30:45,179 --> 00:30:54,179
start and we forget X so using this it

00:30:51,990 --> 00:30:55,920
should be possible to build a very very

00:30:54,179 --> 00:31:00,059
low latency search suggesting

00:30:55,920 --> 00:31:02,220
application and make the site more

00:31:00,059 --> 00:31:04,230
relevant now this is just an example we

00:31:02,220 --> 00:31:07,380
see a lot of situations where this may

00:31:04,230 --> 00:31:09,809
be useful now if this is the kind of

00:31:07,380 --> 00:31:12,490
thing that you really like these are a

00:31:09,809 --> 00:31:14,860
few books I highly recommend

00:31:12,490 --> 00:31:15,720
I think I saw the author of this one in

00:31:14,860 --> 00:31:22,720
the audience

00:31:15,720 --> 00:31:24,220
yep he's there so and like I said if

00:31:22,720 --> 00:31:25,990
you're interested in these are highly

00:31:24,220 --> 00:31:29,170
recommended especially this one because

00:31:25,990 --> 00:31:31,840
it goes into the foundations we're

00:31:29,170 --> 00:31:33,550
always looking for good people willing

00:31:31,840 --> 00:31:35,980
to do things like this doing machine

00:31:33,550 --> 00:31:38,770
learning on this type of thing we have a

00:31:35,980 --> 00:31:49,720
website on that and I assume you have

00:31:38,770 --> 00:31:52,080
questions put your hand up if you have a

00:31:49,720 --> 00:31:52,080
question

00:32:00,660 --> 00:32:05,520
guess I'm curious how you deal with

00:32:02,340 --> 00:32:07,350
tabbed browsing in cases where the exact

00:32:05,520 --> 00:32:11,010
ordering events doesn't necessarily

00:32:07,350 --> 00:32:12,960
indicate they're like causal ordering so

00:32:11,010 --> 00:32:15,630
for example if a pager user has two

00:32:12,960 --> 00:32:16,860
pages open clicks on one of them it does

00:32:15,630 --> 00:32:19,470
something then goes back to the other

00:32:16,860 --> 00:32:20,970
one and does something yeah then it

00:32:19,470 --> 00:32:23,190
might be that the intermediate clicks

00:32:20,970 --> 00:32:26,310
don't really have any causal ordering

00:32:23,190 --> 00:32:28,110
but yes yeah you're right one of the

00:32:26,310 --> 00:32:30,720
things we've designed into the data

00:32:28,110 --> 00:32:34,950
structure is that every event get a

00:32:30,720 --> 00:32:39,720
globally and for every unique ID and the

00:32:34,950 --> 00:32:43,680
next event gets the idea of what caused

00:32:39,720 --> 00:32:45,420
it as the cause ID so although we

00:32:43,680 --> 00:32:49,320
haven't built that and it's clearly not

00:32:45,420 --> 00:32:51,900
shown in my simplified example you can

00:32:49,320 --> 00:32:55,680
see what was the previous what was the

00:32:51,900 --> 00:32:58,650
cause of the event you're seeing now so

00:32:55,680 --> 00:33:02,340
if you maintain a couple of events in

00:32:58,650 --> 00:33:04,800
your state you can actually detect this

00:33:02,340 --> 00:33:07,500
but it will take quite a bit of memory

00:33:04,800 --> 00:33:09,860
to do that reliably for all visitors and

00:33:07,500 --> 00:33:12,390
then it becomes also a trade-off

00:33:09,860 --> 00:33:16,140
okay how many people actually do that

00:33:12,390 --> 00:33:19,320
and how bad is it to be wrong for this

00:33:16,140 --> 00:33:26,340
use case but yeah we actually thought

00:33:19,320 --> 00:33:29,220
about that yes after ordering is done

00:33:26,340 --> 00:33:33,570
what sort of algorithms do you use to

00:33:29,220 --> 00:33:35,790
find relevant events sorry I couldn't

00:33:33,570 --> 00:33:37,560
understand question after the ordering

00:33:35,790 --> 00:33:40,170
is done like what what sort of

00:33:37,560 --> 00:33:43,890
algorithms do you use to find the

00:33:40,170 --> 00:33:45,630
relevant events like when once the

00:33:43,890 --> 00:33:48,930
ordering is done once the when people

00:33:45,630 --> 00:33:52,350
order something yeah what sort of like

00:33:48,930 --> 00:33:54,570
relevant relevant algorithms to use for

00:33:52,350 --> 00:33:57,990
oh that depends on what we want to do

00:33:54,570 --> 00:34:00,510
with it you know if we'd want to do

00:33:57,990 --> 00:34:02,790
attribution modeling for advertising we

00:34:00,510 --> 00:34:05,880
use different models than for example

00:34:02,790 --> 00:34:09,389
the attribution of search keywords so

00:34:05,880 --> 00:34:11,399
that really varies per use case and it's

00:34:09,389 --> 00:34:13,900
up to the team because we have a scrum

00:34:11,399 --> 00:34:16,270
organization so every team that

00:34:13,900 --> 00:34:18,190
needs to build something of course they

00:34:16,270 --> 00:34:19,990
talked to each other about how do you

00:34:18,190 --> 00:34:21,580
approach this and can I reduce reuse

00:34:19,990 --> 00:34:25,120
stuff

00:34:21,580 --> 00:34:26,830
it varies and a lot of lot of situations

00:34:25,120 --> 00:34:28,980
is a very simple rule based thing

00:34:26,830 --> 00:34:31,200
because those usually perform better

00:34:28,980 --> 00:34:36,490
right and the second question is that

00:34:31,200 --> 00:34:40,810
you said during these relevants such you

00:34:36,490 --> 00:34:48,390
the ordering does not matter if like I

00:34:40,810 --> 00:34:51,670
mean even even then like let me check I

00:34:48,390 --> 00:34:53,940
think you mean this one yes exactly

00:34:51,670 --> 00:34:57,310
because what rolls out here is

00:34:53,940 --> 00:35:01,960
essentially that this word was seen on

00:34:57,310 --> 00:35:04,540
the PDP on the product page and from

00:35:01,960 --> 00:35:07,390
there on it's just a time assigning a

00:35:04,540 --> 00:35:09,730
score and figuring out for which letters

00:35:07,390 --> 00:35:12,880
this is a recommendation and then all

00:35:09,730 --> 00:35:15,630
that happens here is that in the

00:35:12,880 --> 00:35:19,600
existing set these are incremented and

00:35:15,630 --> 00:35:23,800
if two measurements for the same thing

00:35:19,600 --> 00:35:26,770
swap places the effect is negligible in

00:35:23,800 --> 00:35:30,310
fact I'm even willing to say that while

00:35:26,770 --> 00:35:32,350
here you are doing in flink terms stream

00:35:30,310 --> 00:35:35,530
processing based on the time in the

00:35:32,350 --> 00:35:37,390
events here you can do it in the time of

00:35:35,530 --> 00:35:39,490
processing and just win do it together

00:35:37,390 --> 00:35:41,620
for a few seconds and then batch it in

00:35:39,490 --> 00:35:44,680
in the data in the data store there

00:35:41,620 --> 00:35:47,280
every few seconds or a minute or

00:35:44,680 --> 00:35:47,280
something like that

00:35:56,400 --> 00:36:01,990
we good yes yeah so so like the key

00:36:00,070 --> 00:36:04,510
themed I think you emphasize this

00:36:01,990 --> 00:36:06,190
maintaining order in events and I think

00:36:04,510 --> 00:36:07,930
one of the things that you brought up is

00:36:06,190 --> 00:36:11,200
that in order to do that you have to

00:36:07,930 --> 00:36:13,120
route sessions to the same node in order

00:36:11,200 --> 00:36:14,950
to have a single clock right it's right

00:36:13,120 --> 00:36:17,470
the events or our captains and also

00:36:14,950 --> 00:36:21,700
single output buffer that's actually the

00:36:17,470 --> 00:36:23,200
bigger impact right okay I'm not sure I

00:36:21,700 --> 00:36:24,430
completely understood that so maybe you

00:36:23,200 --> 00:36:26,560
can expand than that but but the other

00:36:24,430 --> 00:36:30,400
quiet but the question is really what's

00:36:26,560 --> 00:36:32,170
the cost of do see is it difficult to do

00:36:30,400 --> 00:36:33,940
the routing I mean you have issues right

00:36:32,170 --> 00:36:36,430
when you want to roll out updates to

00:36:33,940 --> 00:36:38,440
your servers okay you'd like to be able

00:36:36,430 --> 00:36:42,130
to send someone to a new node Onegin yes

00:36:38,440 --> 00:36:45,480
yeah in those scenarios you will

00:36:42,130 --> 00:36:48,070
undoubtedly have some damage to the data

00:36:45,480 --> 00:36:51,280
one of the things we're doing there is

00:36:48,070 --> 00:36:53,230
that we are using a small number of

00:36:51,280 --> 00:36:55,180
instances and do session replication

00:36:53,230 --> 00:36:58,420
within the small number of instances and

00:36:55,180 --> 00:36:58,870
then switch the user to the other two

00:36:58,420 --> 00:37:01,360
another one

00:36:58,870 --> 00:37:03,510
and because it's a small number you can

00:37:01,360 --> 00:37:07,900
still have the session replication and

00:37:03,510 --> 00:37:10,150
you know don't break stuff too much but

00:37:07,900 --> 00:37:11,800
in general yes you would there's a high

00:37:10,150 --> 00:37:14,860
probability in those scenarios that you

00:37:11,800 --> 00:37:16,630
break stuff but we try to keep it as

00:37:14,860 --> 00:37:19,030
limited as possible because that makes

00:37:16,630 --> 00:37:22,300
the assumptions you can make in the

00:37:19,030 --> 00:37:26,590
downstream processing a lot easier and a

00:37:22,300 --> 00:37:29,740
lot time layer and go much lower in your

00:37:26,590 --> 00:37:33,580
latency then it's possibly if you do if

00:37:29,740 --> 00:37:35,800
you don't do this my question is how do

00:37:33,580 --> 00:37:39,180
you handle scaling so for example on

00:37:35,800 --> 00:37:42,130
Black Friday do you maintain always

00:37:39,180 --> 00:37:43,830
fixed amount of for example clusters for

00:37:42,130 --> 00:37:48,280
Kafka or do you scale them like

00:37:43,830 --> 00:37:50,680
dynamically well Kafka is similar to

00:37:48,280 --> 00:37:56,770
what the approach if link has is stuck

00:37:50,680 --> 00:38:00,670
in a fixed number of parallelism so we

00:37:56,770 --> 00:38:03,010
have created such a number of partitions

00:38:00,670 --> 00:38:05,190
in Kafka that we can handle the Black

00:38:03,010 --> 00:38:05,190
Friday

00:38:08,319 --> 00:38:15,549
and it works yeah

00:38:13,239 --> 00:38:18,529
any more questions

00:38:15,549 --> 00:38:19,579
No so could everyone give a copy of the

00:38:18,529 --> 00:38:22,209
hands for Niels in his great

00:38:19,579 --> 00:38:22,209

YouTube URL: https://www.youtube.com/watch?v=xY-pdiIrSCA


