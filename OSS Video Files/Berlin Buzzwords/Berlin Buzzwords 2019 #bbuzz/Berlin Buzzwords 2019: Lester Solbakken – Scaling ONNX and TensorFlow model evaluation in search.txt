Title: Berlin Buzzwords 2019: Lester Solbakken â€“ Scaling ONNX and TensorFlow model evaluation in search
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	With the advances in deep learning and the corresponding increase in machine learning frameworks in recent years, a new class of software has emerged: model servers. These promise, among other things, performance and scalability. 

There is however a large class of applications where such model servers are inadequate. For instance, search and recommendation applications must efficiently evaluate models over potentially many thousands of data points as part of handling a query. In such cases the amount of data transferred to the model servers can quickly saturate the network and thus decrease total system throughput and degrade quality of service.

In this talk we present a solution to this problem which is to evaluate the models where data is stored rather than moving data to where the model is hosted. We base our solution on Vespa, an open-sourced platform developed at Yahoo for building scalable real-time data processing applications over large data sets. Vespa has  native features to import ONNX and TensorFlow models and represent the computational graphs in its internal tensor language. 

In this talk we will show how this achieves model evaluation performance at web-scale, and that even if one does not take advantage of specialized hardware such as GPUs and TPUs, the total system throughput can scale much better.

Read more:
https://2019.berlinbuzzwords.de/19/session/scaling-onnx-and-tensorflow-model-evaluation-search
BerlinBuzzwords, Buzzwords, bbuzz, #bbuzz, bigdata, big data, search, scale, store, stream, open source
About Lester Solbakken:
https://2019.berlinbuzzwords.de/users/lester-solbakken

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,680 --> 00:00:16,279
yeah hi my name's Lester welcome I'm

00:00:11,990 --> 00:00:18,140
from the vespa team at Verizon media if

00:00:16,279 --> 00:00:19,550
you're not familiar with Vespa don't

00:00:18,140 --> 00:00:22,099
worry I'll I'll try to get back to that

00:00:19,550 --> 00:00:25,579
a little bit later but all you need to

00:00:22,099 --> 00:00:27,890
know is that Vespa has its roots in

00:00:25,579 --> 00:00:30,380
being a vertical search platform hence

00:00:27,890 --> 00:00:31,970
it's a name but it's evolved into

00:00:30,380 --> 00:00:34,190
something more of a kind of a big data

00:00:31,970 --> 00:00:35,719
serving engine at least that's what we

00:00:34,190 --> 00:00:37,670
call it

00:00:35,719 --> 00:00:40,579
Vespas mainly developed by team in

00:00:37,670 --> 00:00:44,989
norway that's wrong I'm in Norway we

00:00:40,579 --> 00:00:47,059
open sourced it 2017 but it's been

00:00:44,989 --> 00:00:51,320
worked on many years before that at

00:00:47,059 --> 00:00:52,999
primarily Yahoo Yahoo was acquired by a

00:00:51,320 --> 00:00:58,969
Verizon a couple years back and we're

00:00:52,999 --> 00:01:00,679
now called Verizon Media Group so what

00:00:58,969 --> 00:01:04,129
I'm here to talk to you about today is

00:01:00,679 --> 00:01:06,340
about using machine learning production

00:01:04,129 --> 00:01:09,830
particularly towards search type

00:01:06,340 --> 00:01:11,450
applications and investigates and solve

00:01:09,830 --> 00:01:13,880
some of the kind of issues or problems

00:01:11,450 --> 00:01:16,340
or challenges that we quickly meet as we

00:01:13,880 --> 00:01:17,960
attempt to scale up these solutions both

00:01:16,340 --> 00:01:29,210
in terms of traffic and in terms of

00:01:17,960 --> 00:01:31,430
contents okay so with the rise of the

00:01:29,210 --> 00:01:33,560
various deep learning platforms other

00:01:31,430 --> 00:01:35,390
recently the last few years we've seen a

00:01:33,560 --> 00:01:40,250
corresponding kind of increase in a new

00:01:35,390 --> 00:01:41,660
class of software model servers model

00:01:40,250 --> 00:01:43,730
servers work from the premise that you

00:01:41,660 --> 00:01:45,980
can take your model that you've trained

00:01:43,730 --> 00:01:47,990
in some of these platforms tensorflow at

00:01:45,980 --> 00:01:51,530
PI torch and so on and you deploy it on

00:01:47,990 --> 00:01:53,480
this model server the model servers

00:01:51,530 --> 00:01:55,460
solves things such as performance

00:01:53,480 --> 00:01:57,980
scalability lifecycle management

00:01:55,460 --> 00:02:00,530
versioning and so on and it kind of puts

00:01:57,980 --> 00:02:03,700
your model behind some interface RPC

00:02:00,530 --> 00:02:06,110
HTTP or whatever and you can put your

00:02:03,700 --> 00:02:08,590
application in front of that inquiry to

00:02:06,110 --> 00:02:11,360
this model server typical use cases

00:02:08,590 --> 00:02:13,879
image classification sending you image

00:02:11,360 --> 00:02:16,310
get back a distribution of objects or

00:02:13,879 --> 00:02:18,410
probabilities of objects and image

00:02:16,310 --> 00:02:20,680
image captioning getting value

00:02:18,410 --> 00:02:24,550
description' what's happening the image

00:02:20,680 --> 00:02:27,230
texts the translation text generation

00:02:24,550 --> 00:02:29,390
impetus sentence game-playing

00:02:27,230 --> 00:02:31,400
impetus abhorred and so on many

00:02:29,390 --> 00:02:32,930
different types of applications and for

00:02:31,400 --> 00:02:37,550
these sorts of applications this

00:02:32,930 --> 00:02:40,040
solution works well for search however

00:02:37,550 --> 00:02:42,050
it's a little bit different in general

00:02:40,040 --> 00:02:44,090
research kind of type applications we

00:02:42,050 --> 00:02:46,520
have a set that works kind of like this

00:02:44,090 --> 00:02:49,130
we have some input some query coming in

00:02:46,520 --> 00:02:52,520
for text search to be like query terms

00:02:49,130 --> 00:02:54,260
and so on for ads and so might be a user

00:02:52,520 --> 00:02:56,600
ID coming in same thing with

00:02:54,260 --> 00:02:58,340
personalization usually have some sort

00:02:56,600 --> 00:03:00,800
of query enrichment going on first

00:02:58,340 --> 00:03:03,050
adding stuff to to the query what do we

00:03:00,800 --> 00:03:05,810
know about the user and user issuing the

00:03:03,050 --> 00:03:08,000
query and so on the query gets sent down

00:03:05,810 --> 00:03:10,670
to a set of search notes for search

00:03:08,000 --> 00:03:12,290
service of content servers and there's a

00:03:10,670 --> 00:03:14,300
lot there there's a lot of computation

00:03:12,290 --> 00:03:17,840
going on depending on the application

00:03:14,300 --> 00:03:19,550
and a ordered list of results are sent

00:03:17,840 --> 00:03:23,630
back again and eventually back to the

00:03:19,550 --> 00:03:25,370
user and lately we've seen a rise in

00:03:23,630 --> 00:03:27,769
interest in doing this kind of ranking

00:03:25,370 --> 00:03:31,220
with this computation with machine learn

00:03:27,769 --> 00:03:32,690
learning such as learning to rank and

00:03:31,220 --> 00:03:36,709
more recently things like neural

00:03:32,690 --> 00:03:38,209
information retrieval and so on and what

00:03:36,709 --> 00:03:39,860
we want to do is we want to evaluate the

00:03:38,209 --> 00:03:41,900
model and each of the results coming

00:03:39,860 --> 00:03:43,340
back from the content server right so

00:03:41,900 --> 00:03:45,170
where previously where we had this model

00:03:43,340 --> 00:03:47,540
service we have one data point in and

00:03:45,170 --> 00:03:49,220
kind of data coming out with this we

00:03:47,540 --> 00:03:50,690
have many data points that we need to

00:03:49,220 --> 00:03:53,720
run through this the mall server and

00:03:50,690 --> 00:03:57,250
that quickly leads to a problem of

00:03:53,720 --> 00:04:00,620
network capacity fairly quickly actually

00:03:57,250 --> 00:04:02,450
the bottleneck becomes the network

00:04:00,620 --> 00:04:03,890
capacity so for instance if you have a

00:04:02,450 --> 00:04:06,140
query you're returning a thousand

00:04:03,890 --> 00:04:08,480
results per query each of these the

00:04:06,140 --> 00:04:10,700
results have it kind of a data of around

00:04:08,480 --> 00:04:12,769
500 floats per results which you can

00:04:10,700 --> 00:04:15,019
reach very quickly with learning to rank

00:04:12,769 --> 00:04:17,359
features or like war and document the

00:04:15,019 --> 00:04:18,829
beddings word embeddings and so on if

00:04:17,359 --> 00:04:23,180
you're running on a 10 gigabit network

00:04:18,829 --> 00:04:25,460
you can sustain the max 300 queries per

00:04:23,180 --> 00:04:27,560
second max right then you're using all

00:04:25,460 --> 00:04:30,770
your network capacity to this bet is

00:04:27,560 --> 00:04:33,090
generally a bad state of affairs

00:04:30,770 --> 00:04:35,280
to put this in context a little bit at

00:04:33,090 --> 00:04:36,840
Yahoo we have multiple application

00:04:35,280 --> 00:04:40,020
running in the tens of thousands of

00:04:36,840 --> 00:04:41,939
query per second per Colo so for these

00:04:40,020 --> 00:04:43,889
kinds of applications this sort of

00:04:41,939 --> 00:04:48,930
architecture does not work it's not

00:04:43,889 --> 00:04:50,699
scale at all so the solution to this

00:04:48,930 --> 00:04:53,520
looking at the equation on the right

00:04:50,699 --> 00:04:56,310
hand side there or two left there either

00:04:53,520 --> 00:04:58,259
to send less data for results for

00:04:56,310 --> 00:04:59,310
instance you can't do that for learning

00:04:58,259 --> 00:05:02,279
to rank because the features are

00:04:59,310 --> 00:05:03,870
calculated on the content servers but if

00:05:02,279 --> 00:05:05,879
you're not doing that you can maybe you

00:05:03,870 --> 00:05:07,379
know send some of the features to the

00:05:05,879 --> 00:05:09,150
model servers create your own kind of

00:05:07,379 --> 00:05:14,189
custom software and top model server

00:05:09,150 --> 00:05:15,779
there increases memory on the snows

00:05:14,189 --> 00:05:17,279
maybe have some update issues between

00:05:15,779 --> 00:05:19,740
what's going on the content and model

00:05:17,279 --> 00:05:21,599
servers and so on but the problem really

00:05:19,740 --> 00:05:22,949
is that what's going on in content

00:05:21,599 --> 00:05:25,500
servers and what's going on model

00:05:22,949 --> 00:05:27,050
servers might be evaluating this ranking

00:05:25,500 --> 00:05:29,719
based on the different set of features

00:05:27,050 --> 00:05:31,650
and that actually lessens the kind of

00:05:29,719 --> 00:05:33,240
probability of you getting there

00:05:31,650 --> 00:05:35,550
globally best results that you can

00:05:33,240 --> 00:05:37,319
because the model service can only work

00:05:35,550 --> 00:05:39,150
on the results coming from the content

00:05:37,319 --> 00:05:40,529
servers so there needs to be some sort

00:05:39,150 --> 00:05:42,029
of correlation between what's going on

00:05:40,529 --> 00:05:45,300
and that's difficult to do if they're

00:05:42,029 --> 00:05:48,000
running on different set of features you

00:05:45,300 --> 00:05:49,860
can also try to return less results per

00:05:48,000 --> 00:05:51,419
query but again if you don't have good

00:05:49,860 --> 00:05:54,710
correlation between what's going on in

00:05:51,419 --> 00:05:57,599
these two two areas and then you

00:05:54,710 --> 00:06:01,319
decreasing potential quality of your

00:05:57,599 --> 00:06:03,210
system so the real solution is you know

00:06:01,319 --> 00:06:06,120
don't move data around but instead

00:06:03,210 --> 00:06:08,189
evaluate the models on the content

00:06:06,120 --> 00:06:11,969
servers and that's something we've been

00:06:08,189 --> 00:06:14,669
working a lot on on vespa by importing

00:06:11,969 --> 00:06:17,039
these kinds of models that is flow on X

00:06:14,669 --> 00:06:23,180
and X G boosts and evaluate him directly

00:06:17,039 --> 00:06:23,180
on the content servers as well

00:06:26,590 --> 00:06:31,450
okay just to describe a little bit more

00:06:28,510 --> 00:06:34,270
more about footrest base I mentioned

00:06:31,450 --> 00:06:37,680
that Verizon acquired Yahoo a few years

00:06:34,270 --> 00:06:40,750
back has its roots from from from Yahoo

00:06:37,680 --> 00:06:42,490
we were merged together with AOL and for

00:06:40,750 --> 00:06:44,710
a while we were called off recently

00:06:42,490 --> 00:06:48,000
rebranded to Verizon media group and

00:06:44,710 --> 00:06:51,400
basically working as a kind of a

00:06:48,000 --> 00:06:55,510
technology producer for a large set of

00:06:51,400 --> 00:06:57,610
websites on the line but at Yahoo or

00:06:55,510 --> 00:06:59,139
Verizon media the Spezza long history

00:06:57,610 --> 00:07:01,990
and it's always been a very popular

00:06:59,139 --> 00:07:04,720
piece of software inside of Yahoo we

00:07:01,990 --> 00:07:07,990
have hundreds of US publications running

00:07:04,720 --> 00:07:09,700
serving over billion users per month at

00:07:07,990 --> 00:07:11,410
any given time given time it's running

00:07:09,700 --> 00:07:13,660
over hundreds of thousands of queries

00:07:11,410 --> 00:07:15,340
per second all over the world over

00:07:13,660 --> 00:07:18,760
billions of content items so it's in

00:07:15,340 --> 00:07:20,440
fairly heavy use some small examples was

00:07:18,760 --> 00:07:22,870
used for and a Flickr image search for

00:07:20,440 --> 00:07:25,630
instance on the right-hand side there

00:07:22,870 --> 00:07:28,350
you have the front page of Yahoo it has

00:07:25,630 --> 00:07:32,740
personalized recommendations for

00:07:28,350 --> 00:07:35,289
articles also does real-time native ads

00:07:32,740 --> 00:07:38,560
in between there and those things like

00:07:35,289 --> 00:07:40,570
real-time bidding and so on and one of

00:07:38,560 --> 00:07:43,810
the more fun applications that we have

00:07:40,570 --> 00:07:45,669
is that on all the kind of news pages

00:07:43,810 --> 00:07:47,800
that you who like Yahoo Finance and so

00:07:45,669 --> 00:07:49,240
on there are comments sections and as we

00:07:47,800 --> 00:07:51,240
all know comment sections are generally

00:07:49,240 --> 00:07:54,550
garbage

00:07:51,240 --> 00:07:56,289
so there is a piece there that where all

00:07:54,550 --> 00:07:58,810
these comments are ranked by this by

00:07:56,289 --> 00:08:00,280
using a neural network and it's kind of

00:07:58,810 --> 00:08:01,960
difficult to know how you should you

00:08:00,280 --> 00:08:05,020
rank these comments what's the objective

00:08:01,960 --> 00:08:07,360
function so we train this using a

00:08:05,020 --> 00:08:09,340
reinforcement learning algorithm which

00:08:07,360 --> 00:08:11,380
trains the model PRT periodically

00:08:09,340 --> 00:08:13,360
and pushes the model to two vespa and

00:08:11,380 --> 00:08:19,270
based on data it just continues to cycle

00:08:13,360 --> 00:08:21,789
to improve the model to do all this

00:08:19,270 --> 00:08:22,960
Vespas a rich set of core features I'm

00:08:21,789 --> 00:08:24,880
not going to go through all of them

00:08:22,960 --> 00:08:28,630
there's a talk later today and the mate

00:08:24,880 --> 00:08:30,400
might go into a few more but three ones

00:08:28,630 --> 00:08:34,730
I want to kind of focus on a little bit

00:08:30,400 --> 00:08:39,610
is its elasticity scalability and

00:08:34,730 --> 00:08:39,610
capacity for advanced relevant scoring

00:08:40,540 --> 00:08:45,680
so picking a little bit under the hood

00:08:43,130 --> 00:08:48,760
of a spa we can see how it kind of

00:08:45,680 --> 00:08:51,140
achieves its performance at scale so

00:08:48,760 --> 00:08:53,000
whenever we have some query coming in

00:08:51,140 --> 00:08:54,680
there's always this query handler which

00:08:53,000 --> 00:08:57,920
is where we do the query enrichment and

00:08:54,680 --> 00:08:59,600
so on and if that's processed and and in

00:08:57,920 --> 00:09:03,910
massage a little bit it gets sent down

00:08:59,600 --> 00:09:07,880
to one or all the content partitions so

00:09:03,910 --> 00:09:10,160
in any single game content partition we

00:09:07,880 --> 00:09:12,800
goes through a set of stages so the

00:09:10,160 --> 00:09:15,650
first is a matching stage we're all kind

00:09:12,800 --> 00:09:18,710
of relevant or at least somewhat

00:09:15,650 --> 00:09:23,480
relevant documents related to the query

00:09:18,710 --> 00:09:24,860
are fetched binary decision then we

00:09:23,480 --> 00:09:26,540
typically have a first phase ranking

00:09:24,860 --> 00:09:29,510
function which is a ranking from the

00:09:26,540 --> 00:09:33,130
function it's efficiently evaluated very

00:09:29,510 --> 00:09:35,330
cheap one which calls down the amount of

00:09:33,130 --> 00:09:37,130
documents to be rear and in the second

00:09:35,330 --> 00:09:38,300
phase and that's typically where you

00:09:37,130 --> 00:09:41,150
have you kind of computationally

00:09:38,300 --> 00:09:43,760
expensive computation to be to be done

00:09:41,150 --> 00:09:46,150
and thus we can reduce the latency or

00:09:43,760 --> 00:09:48,050
the time spent inside these continents

00:09:46,150 --> 00:09:49,550
so for instance if you have a machine

00:09:48,050 --> 00:09:50,930
learn model that would typically be

00:09:49,550 --> 00:09:53,000
something that's very computation

00:09:50,930 --> 00:09:55,610
expensive to evaluate so that would be

00:09:53,000 --> 00:09:59,800
in your second phase ranking but

00:09:55,610 --> 00:10:02,770
typically we would add in a first phase

00:09:59,800 --> 00:10:09,380
function to narrow down the search space

00:10:02,770 --> 00:10:11,240
anyway that's by utilizes all the course

00:10:09,380 --> 00:10:14,180
on the nodes as efficiently as possible

00:10:11,240 --> 00:10:15,590
but if you need to reduce latency even

00:10:14,180 --> 00:10:17,390
more you can add additional content

00:10:15,590 --> 00:10:21,050
partitions to distribute the workload

00:10:17,390 --> 00:10:22,160
over a larger number of nodes that's

00:10:21,050 --> 00:10:24,410
what makes it easy to do that

00:10:22,160 --> 00:10:28,120
automatically distribute the data among

00:10:24,410 --> 00:10:28,120
these nodes and and so on

00:10:33,250 --> 00:10:38,389
so we've been hard at work this last

00:10:36,920 --> 00:10:40,490
couple years to add this kind of

00:10:38,389 --> 00:10:42,470
integration with with Vespa tensorflow

00:10:40,490 --> 00:10:43,730
all necks if you not familiar with all

00:10:42,470 --> 00:10:45,649
necks by the way it's the open neural

00:10:43,730 --> 00:10:49,029
network exchange format it's basically

00:10:45,649 --> 00:10:54,259
the kind of deep learning format all the

00:10:49,029 --> 00:10:55,790
vendors other than Google are using XG

00:10:54,259 --> 00:10:57,829
boost is what you want to use if you

00:10:55,790 --> 00:10:59,930
want to participate in the Kegel

00:10:57,829 --> 00:11:04,339
competition for instance it's actually

00:10:59,930 --> 00:11:06,319
useful whenever you create a application

00:11:04,339 --> 00:11:08,120
for Vespa you recreate you create what

00:11:06,319 --> 00:11:10,279
we call a application package and it's a

00:11:08,120 --> 00:11:12,889
kind of a declarative package of state

00:11:10,279 --> 00:11:13,910
what should the application do how

00:11:12,889 --> 00:11:16,100
should do it and so on

00:11:13,910 --> 00:11:18,290
we try to make this using machine and

00:11:16,100 --> 00:11:20,569
models and that's as easy to possible

00:11:18,290 --> 00:11:22,100
that easy to use as possible so you

00:11:20,569 --> 00:11:25,430
pretty much just drop your model into

00:11:22,100 --> 00:11:26,120
this this application package and when

00:11:25,430 --> 00:11:29,180
you do that

00:11:26,120 --> 00:11:31,100
respite takes care of importing the lis

00:11:29,180 --> 00:11:32,149
models and make them available so you

00:11:31,100 --> 00:11:34,490
can use them directly in your

00:11:32,149 --> 00:11:38,149
handwritten a ranking expressions such

00:11:34,490 --> 00:11:41,000
as this so for instance this allows you

00:11:38,149 --> 00:11:42,980
to very easily string together different

00:11:41,000 --> 00:11:46,189
models from different sources and so on

00:11:42,980 --> 00:11:48,439
so for instance you can have a versus a

00:11:46,189 --> 00:11:50,540
click probability model trained in in

00:11:48,439 --> 00:11:53,149
terms of flow may be a dwell time

00:11:50,540 --> 00:11:55,189
estimator trained in PI torch and so on

00:11:53,149 --> 00:11:57,980
and you can combine these nonlinearly

00:11:55,189 --> 00:12:00,550
using extra boost and so on and not

00:11:57,980 --> 00:12:02,600
saying you should just saying it could

00:12:00,550 --> 00:12:05,329
but this is a kind of very cool very

00:12:02,600 --> 00:12:09,220
unique feature that Vesper has I don't

00:12:05,329 --> 00:12:09,220
think yeah I've seen this other places

00:12:12,189 --> 00:12:21,949
so when we import these models into to

00:12:18,410 --> 00:12:27,559
Vespa we don't rely on any kind of

00:12:21,949 --> 00:12:29,660
external kind of executor for that we

00:12:27,559 --> 00:12:32,120
execute them in the ranking language in

00:12:29,660 --> 00:12:34,399
Vespa so a few years back I think two

00:12:32,120 --> 00:12:36,379
years back we introduced something

00:12:34,399 --> 00:12:38,990
called the tensor API which is an

00:12:36,379 --> 00:12:41,870
extension to our own kind of ranking a

00:12:38,990 --> 00:12:43,890
language which is an extension that

00:12:41,870 --> 00:12:46,630
handles

00:12:43,890 --> 00:12:50,920
multi-dimensional data or any

00:12:46,630 --> 00:12:53,019
dimensional data really and the API was

00:12:50,920 --> 00:12:55,750
designed to have like a very small set

00:12:53,019 --> 00:12:57,220
of core features which could represent a

00:12:55,750 --> 00:13:00,010
large class of different types of

00:12:57,220 --> 00:13:01,180
computation this is in contrast to for

00:13:00,010 --> 00:13:02,649
instance tensor flow if you've been

00:13:01,180 --> 00:13:04,630
working with that you know that the API

00:13:02,649 --> 00:13:06,550
can be very large very confusing many

00:13:04,630 --> 00:13:07,839
operations doing the same thing so we're

00:13:06,550 --> 00:13:11,200
trying to go kind of the opposite

00:13:07,839 --> 00:13:13,390
opposite direction so here in this this

00:13:11,200 --> 00:13:14,470
example here on the left hand side here

00:13:13,390 --> 00:13:17,350
we have a computational graph

00:13:14,470 --> 00:13:19,630
representing a single layer and a neural

00:13:17,350 --> 00:13:21,579
network however matrix multiplication

00:13:19,630 --> 00:13:24,040
between the placeholder which is tensor

00:13:21,579 --> 00:13:27,190
for speak for inputs weights is the

00:13:24,040 --> 00:13:29,230
weight strained by your your machine

00:13:27,190 --> 00:13:30,910
learning algorithm and matrix

00:13:29,230 --> 00:13:34,180
multiplication multiplication between

00:13:30,910 --> 00:13:35,589
those two you add in the bias and do a

00:13:34,180 --> 00:13:39,130
rectified linear unit at the end for

00:13:35,589 --> 00:13:40,839
instance so this is converted into the

00:13:39,130 --> 00:13:45,279
expression on the right hand side there

00:13:40,839 --> 00:13:47,339
and on the back end on the the kind of

00:13:45,279 --> 00:13:49,899
node where this all this is calculated

00:13:47,339 --> 00:13:53,110
when it sees this expression it

00:13:49,899 --> 00:13:54,790
optimizes this so for instance join and

00:13:53,110 --> 00:13:56,950
reduce its optimized to again a single

00:13:54,790 --> 00:13:59,170
step so we don't have to introduce

00:13:56,950 --> 00:14:01,810
temporary tensors and so on so have many

00:13:59,170 --> 00:14:03,670
of these kind of smaller optimizations

00:14:01,810 --> 00:14:05,380
and the benefit here is that when we

00:14:03,670 --> 00:14:07,470
have different models coming from

00:14:05,380 --> 00:14:09,910
different sources they're all kind of

00:14:07,470 --> 00:14:11,950
translated to this format so we have

00:14:09,910 --> 00:14:23,950
kind of one set up at one place that we

00:14:11,950 --> 00:14:25,480
need to to keep optimizing so we one is

00:14:23,950 --> 00:14:29,680
the test this a little bit we set up a

00:14:25,480 --> 00:14:34,209
benchmark for this we wanted to kind of

00:14:29,680 --> 00:14:37,300
test this hypothesis that sending data

00:14:34,209 --> 00:14:40,449
around is not the smart thing to do we

00:14:37,300 --> 00:14:42,190
set up a test we're kind of emulating a

00:14:40,449 --> 00:14:44,260
recommendation system a blog

00:14:42,190 --> 00:14:46,240
recommendation system where we have a

00:14:44,260 --> 00:14:47,860
user representation with a vector and we

00:14:46,240 --> 00:14:51,880
have a document representation with

00:14:47,860 --> 00:14:53,620
another vector and we set up a first

00:14:51,880 --> 00:14:56,260
phase which is basically a dot product

00:14:53,620 --> 00:14:58,779
between a typical recommendation system

00:14:56,260 --> 00:15:00,730
and then we have a second phase which is

00:14:58,779 --> 00:15:02,820
a neural network which we try to

00:15:00,730 --> 00:15:05,950
evaluate on the content node and

00:15:02,820 --> 00:15:07,120
alternatively on external model server

00:15:05,950 --> 00:15:09,339
here tensorflow

00:15:07,120 --> 00:15:11,490
and additionally we have the green one

00:15:09,339 --> 00:15:13,959
which is baseline with data which is the

00:15:11,490 --> 00:15:15,940
during the first phase but sending back

00:15:13,959 --> 00:15:18,370
the data as if you were going to send it

00:15:15,940 --> 00:15:19,990
to an external server but not doing

00:15:18,370 --> 00:15:22,930
anything with it just just adding a

00:15:19,990 --> 00:15:24,760
baseline and in the model itself is

00:15:22,930 --> 00:15:29,860
about two hundred thousand parameters so

00:15:24,760 --> 00:15:31,390
it's a fairly reasonably sized model so

00:15:29,860 --> 00:15:33,339
these are some of the results that we

00:15:31,390 --> 00:15:36,820
got on the left-hand side here we see

00:15:33,339 --> 00:15:39,130
the latency evolving as we increase the

00:15:36,820 --> 00:15:42,940
number of clients so clients is in a

00:15:39,130 --> 00:15:44,709
number of kind of clients pushing at the

00:15:42,940 --> 00:15:45,850
same time acquiring at the same time of

00:15:44,709 --> 00:15:48,520
two hundred and twentieth which is

00:15:45,850 --> 00:15:50,860
fairly heavy heavy traffic on the right

00:15:48,520 --> 00:15:52,920
hand side we see the throughputs or the

00:15:50,860 --> 00:15:55,360
QPS queries per second how that evolves

00:15:52,920 --> 00:15:58,120
an interesting kind of thing to notice

00:15:55,360 --> 00:16:00,640
here is the green line on the QPS there

00:15:58,120 --> 00:16:03,220
it flats out fairly early actually very

00:16:00,640 --> 00:16:05,649
early around less than twenty twenty

00:16:03,220 --> 00:16:10,329
clients and that is a point where we

00:16:05,649 --> 00:16:12,070
reach network saturation right so we

00:16:10,329 --> 00:16:14,079
cannot push more data through this

00:16:12,070 --> 00:16:18,190
network and of course latency increases

00:16:14,079 --> 00:16:19,990
because of this and you see it tends to

00:16:18,190 --> 00:16:23,350
flow tracks then below that a little bit

00:16:19,990 --> 00:16:25,329
and just to be kind of clear here no

00:16:23,350 --> 00:16:26,920
amount of hardware acceleration that you

00:16:25,329 --> 00:16:30,220
can put on your tensor flow no to your

00:16:26,920 --> 00:16:33,010
external motor server via GPUs or TP use

00:16:30,220 --> 00:16:35,279
or quantum computers or whatever can

00:16:33,010 --> 00:16:37,120
allow you to push through that green

00:16:35,279 --> 00:16:39,640
line there right it's the hard

00:16:37,120 --> 00:16:45,700
scalability ceiling of just sending data

00:16:39,640 --> 00:16:49,060
around so the blue line there is as the

00:16:45,700 --> 00:16:51,490
vespa you see that obviously scales much

00:16:49,060 --> 00:16:59,040
more better you're not sending data

00:16:51,490 --> 00:17:01,240
round so this is how it looks with

00:16:59,040 --> 00:17:02,770
running I think it was three content

00:17:01,240 --> 00:17:05,800
nodes we're sending my with a thousand

00:17:02,770 --> 00:17:07,090
results per per query we can improve

00:17:05,800 --> 00:17:10,570
upon this

00:17:07,090 --> 00:17:12,580
if you want to have a better latency and

00:17:10,570 --> 00:17:14,140
so on you can add additional content

00:17:12,580 --> 00:17:15,820
notes this will have the effect of

00:17:14,140 --> 00:17:17,410
decreasing Elaine see because we're

00:17:15,820 --> 00:17:20,340
distributing the work around a larger

00:17:17,410 --> 00:17:25,290
number of nodes this has a kind of a

00:17:20,340 --> 00:17:28,150
diminishing effect however you can't add

00:17:25,290 --> 00:17:31,870
infinite number of of content nodes so

00:17:28,150 --> 00:17:35,110
go down to zero this is on dolls law you

00:17:31,870 --> 00:17:36,430
might be familiar with and basically

00:17:35,110 --> 00:17:38,290
says that whenever you're doing some

00:17:36,430 --> 00:17:40,990
computation you have just a fraction of

00:17:38,290 --> 00:17:43,270
the work is parallelizable and only the

00:17:40,990 --> 00:17:47,890
paralyzer part will be affected by

00:17:43,270 --> 00:17:51,760
adding additional content notes so this

00:17:47,890 --> 00:17:54,610
has a diminishing effect however there

00:17:51,760 --> 00:17:56,800
is another way of using these additional

00:17:54,610 --> 00:18:02,080
computation notes or content notes if

00:17:56,800 --> 00:18:04,210
you want to and that is actually by

00:18:02,080 --> 00:18:05,500
having them doing more work so if you

00:18:04,210 --> 00:18:07,210
can satisfied with your

00:18:05,500 --> 00:18:10,840
SLA if you're satisfied with running at

00:18:07,210 --> 00:18:13,330
say 100 milliseconds at 95 percent

00:18:10,840 --> 00:18:15,040
percentile meaning that 95% of all your

00:18:13,330 --> 00:18:17,680
queries are going on less than hundred

00:18:15,040 --> 00:18:23,260
milliseconds you can kind keep adding a

00:18:17,680 --> 00:18:27,810
work to your content notes to actually

00:18:23,260 --> 00:18:30,280
rewrite a larger number of documents so

00:18:27,810 --> 00:18:31,570
the earlier I mentioned what you really

00:18:30,280 --> 00:18:33,640
want to do with your machine learning

00:18:31,570 --> 00:18:35,440
models is that you really want to run

00:18:33,640 --> 00:18:37,180
them on all of the documents that you

00:18:35,440 --> 00:18:38,920
have available but typically that's much

00:18:37,180 --> 00:18:41,200
too expensive to do that's why you have

00:18:38,920 --> 00:18:43,630
to introduce this face to the ranking

00:18:41,200 --> 00:18:47,560
but by adding additional content nodes

00:18:43,630 --> 00:18:49,540
you can use the additional compute to

00:18:47,560 --> 00:18:51,280
actually rewrite a larger number of

00:18:49,540 --> 00:18:53,880
documents and this is something else

00:18:51,280 --> 00:18:56,740
feels much more linearly this is a

00:18:53,880 --> 00:18:59,590
Gustav's Ohm's law is the workload

00:18:56,740 --> 00:19:04,330
increases as parallelity increases and

00:18:59,590 --> 00:19:05,800
this is something you cannot do when you

00:19:04,330 --> 00:19:07,630
think some external model server right

00:19:05,800 --> 00:19:08,920
because you're just trying to push more

00:19:07,630 --> 00:19:12,640
data across the network and you're

00:19:08,920 --> 00:19:16,410
already capped at writes so it's an

00:19:12,640 --> 00:19:16,410
interesting interesting effect

00:19:17,190 --> 00:19:27,850
okay so to conclude extra monsters don't

00:19:24,310 --> 00:19:33,070
really scale that well for when you're

00:19:27,850 --> 00:19:35,290
using machine learning in search and to

00:19:33,070 --> 00:19:37,120
to combat that you put the you put the

00:19:35,290 --> 00:19:39,640
evaluation on the content notes and that

00:19:37,120 --> 00:19:41,440
gives you many more options to scale in

00:19:39,640 --> 00:19:44,800
how you can control latency you can

00:19:41,440 --> 00:19:46,480
control throughput and you can increase

00:19:44,800 --> 00:19:48,160
the number of rearing results with your

00:19:46,480 --> 00:19:53,040
machine learning models just leading to

00:19:48,160 --> 00:19:53,040
higher potential quality of your results

00:19:53,670 --> 00:19:59,620
multi-phase ranking when you whenever

00:19:57,880 --> 00:20:00,880
you introduce like a first phase in the

00:19:59,620 --> 00:20:02,230
second phases so only mentioned

00:20:00,880 --> 00:20:03,900
previously it's very important with the

00:20:02,230 --> 00:20:05,980
correlation between these phases

00:20:03,900 --> 00:20:09,150
unfortunately we see many many times

00:20:05,980 --> 00:20:13,150
where users of a Vespa particularly have

00:20:09,150 --> 00:20:15,070
not been really that cognizant of what's

00:20:13,150 --> 00:20:16,240
what what's going on in the first phase

00:20:15,070 --> 00:20:19,450
and what's going on in the second phase

00:20:16,240 --> 00:20:24,040
that's leading to a worst kind of system

00:20:19,450 --> 00:20:26,440
level retrieval question for me to you

00:20:24,040 --> 00:20:28,540
if any you have been working with multi

00:20:26,440 --> 00:20:32,340
phase ranking come find me afterwards I

00:20:28,540 --> 00:20:32,340
really would like to hear your stories

00:20:32,640 --> 00:20:37,450
model support in our machine learning

00:20:35,620 --> 00:20:39,130
model support in Vespa is ongoing work

00:20:37,450 --> 00:20:42,280
there's some sort of models that we

00:20:39,130 --> 00:20:45,100
still don't support currently but it is

00:20:42,280 --> 00:20:48,130
an open source project if you'd like to

00:20:45,100 --> 00:20:53,910
contribute come find me afterwards as

00:20:48,130 --> 00:20:57,779
well that was always gonna say thank you

00:20:53,910 --> 00:21:00,159
[Applause]

00:20:57,779 --> 00:21:07,019
okay do you have any question for a

00:21:00,159 --> 00:21:07,019
lessor so one want to ask something yep

00:21:11,309 --> 00:21:16,570
hey thanks very much for the for the

00:21:13,809 --> 00:21:18,759
talk it was really interesting and just

00:21:16,570 --> 00:21:20,830
seeing the talk makes me sort of want to

00:21:18,759 --> 00:21:22,509
play around with desperate and I'm

00:21:20,830 --> 00:21:26,139
wondering if you have any pointers what

00:21:22,509 --> 00:21:27,999
the best place to start would be so

00:21:26,139 --> 00:21:29,950
you're asking about if you have any kind

00:21:27,999 --> 00:21:32,619
of simple places to start playing around

00:21:29,950 --> 00:21:35,190
with this so you see will find many

00:21:32,619 --> 00:21:39,460
resources at our homepage Vespa AI

00:21:35,190 --> 00:21:41,860
including many sample applications and

00:21:39,460 --> 00:21:43,119
the use cases there which we could

00:21:41,860 --> 00:21:46,200
really just get you started including

00:21:43,119 --> 00:21:49,240
sample data and so on so there is a

00:21:46,200 --> 00:21:50,950
tutorial there which builds upon this

00:21:49,240 --> 00:21:51,309
blog recommendation application that I

00:21:50,950 --> 00:21:53,889
showed

00:21:51,309 --> 00:21:57,779
so that has all the data available and

00:21:53,889 --> 00:21:57,779

YouTube URL: https://www.youtube.com/watch?v=wjdxOwQbs2k


