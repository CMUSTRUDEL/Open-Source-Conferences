Title: Berlin Buzzwords 2019: Owen O'Malley – Introducing Apache Iceberg: Tables Designed for Object Stores
Publication date: 2019-06-18
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Hive tables are an integral part of the big data ecosystem, but the simple directory-based design that made them ubiquitous is increasingly problematic. Netflix uses tables backed by S3 that, like other object stores, don’t fit this directory-based model: listings are much slower, renames are not atomic, and results are eventually consistent. Even tables in HDFS are problematic at scale, and reliable query behavior requires readers to acquire locks and wait.

I will present an overview of Apache Iceberg, a new open source project that defines a new table layout addresses the challenges of current Apache Hive tables, with properties specifically designed for cloud object stores, such as S3. Iceberg is joined Apache Incubator last year. It specifies the portable table format and standardizes many important features, including:

* All reads use snapshot isolation without locking.
* No directory listings are required for query planning.
* Files can be added, removed, or replaced atomically.
* Full schema evolution supports changes in the table over time.
* Partitioning evolution enables changes to the physical layout without breaking existing queries.
* Data files are stored as Avro, ORC, or Parquet.
* Support for Spark, Hive, and Presto.

Read more:
https://2019.berlinbuzzwords.de/19/session/introducing-apache-iceberg-tables-designed-object-stores

About Owen O'Malley:
https://2019.berlinbuzzwords.de/users/owen-omalley

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,760 --> 00:00:14,750
hello as he said I'm Owen O'Malley I'm a

00:00:12,559 --> 00:00:15,269
co-founder of cloud heiress strange is

00:00:14,750 --> 00:00:18,330
that

00:00:15,269 --> 00:00:20,400
me to say but I'm very happy to be here

00:00:18,330 --> 00:00:22,590
I apologize if I'm a little jet-lagged

00:00:20,400 --> 00:00:28,770
at least I got him on Saturday instead

00:00:22,590 --> 00:00:31,140
of lost yesterday so iceberg is a new

00:00:28,770 --> 00:00:36,989
pod length that came into the Apache

00:00:31,140 --> 00:00:38,550
Incubator this year and it came out of

00:00:36,989 --> 00:00:40,350
Netflix and it came out of their

00:00:38,550 --> 00:00:42,780
production use case so what are we going

00:00:40,350 --> 00:00:45,449
to talk about I'm going to talk about a

00:00:42,780 --> 00:00:47,820
use case and wide Netflix came up with

00:00:45,449 --> 00:00:50,400
iceberg to start with then I'm gonna

00:00:47,820 --> 00:00:53,930
talk about some of the restrictions on

00:00:50,400 --> 00:00:57,600
the hive cables that everyone uses today

00:00:53,930 --> 00:01:03,570
what iceberg does instead and then how

00:00:57,600 --> 00:01:06,899
to get started so iceberg really came

00:01:03,570 --> 00:01:08,579
out of the performance out of s3 now

00:01:06,899 --> 00:01:10,399
there's some other characteristics of s3

00:01:08,579 --> 00:01:12,479
that are also very important but

00:01:10,399 --> 00:01:16,530
fundamentally came out of performance

00:01:12,479 --> 00:01:19,170
and so they have huge amounts of data a

00:01:16,530 --> 00:01:22,170
Netflix of course they are continually

00:01:19,170 --> 00:01:27,359
keeping data on everything that you

00:01:22,170 --> 00:01:29,609
watch and so they need to analyze that

00:01:27,359 --> 00:01:31,350
because Netflix is a data-driven

00:01:29,609 --> 00:01:34,740
business they need to know what people

00:01:31,350 --> 00:01:39,600
are watching and who's watching what so

00:01:34,740 --> 00:01:41,880
they run these queries a lot and this is

00:01:39,600 --> 00:01:46,920
a time series they've got one month of

00:01:41,880 --> 00:01:49,380
data is 2.7 million files and they

00:01:46,920 --> 00:01:50,999
couldn't process more than a few days so

00:01:49,380 --> 00:01:56,219
then the query it looks like that where

00:01:50,999 --> 00:02:01,279
you are selecting distinct tags from a

00:01:56,219 --> 00:02:05,459
range of dates so when they ran it on

00:02:01,279 --> 00:02:08,640
hive they got 400,000 splits right those

00:02:05,459 --> 00:02:10,740
are individual units of work and they

00:02:08,640 --> 00:02:15,530
explained query just doing the planning

00:02:10,740 --> 00:02:18,800
took nine and a half minutes that's a

00:02:15,530 --> 00:02:22,140
long time but it's not unheard of and

00:02:18,800 --> 00:02:27,630
when they replaced it with iceberg they

00:02:22,140 --> 00:02:28,740
got down to 15,000 splits and the whole

00:02:27,630 --> 00:02:32,250
thing ran in

00:02:28,740 --> 00:02:34,860
13 minutes granted it was 61 hours of

00:02:32,250 --> 00:02:37,470
cumulative task time but the wall time

00:02:34,860 --> 00:02:40,470
was 13 minutes and it only took 10

00:02:37,470 --> 00:02:43,380
seconds of planning now with the file

00:02:40,470 --> 00:02:46,020
formats you can do more work upfront and

00:02:43,380 --> 00:02:48,600
figure out that you don't need to run as

00:02:46,020 --> 00:02:51,060
many tasks and so if they turn that on

00:02:48,600 --> 00:02:53,910
an iceberg they could spend more time

00:02:51,060 --> 00:02:56,190
planning and as matter of fact 25

00:02:53,910 --> 00:02:59,610
seconds planning but then the wall clock

00:02:56,190 --> 00:03:02,130
time got down to 42 seconds so that is

00:02:59,610 --> 00:03:06,360
why they wanted iceberg they wanted to

00:03:02,130 --> 00:03:09,540
get the performance so what is the table

00:03:06,360 --> 00:03:12,810
format the first question that everyone

00:03:09,540 --> 00:03:16,290
thinks is that is it a file format and

00:03:12,810 --> 00:03:18,780
the answer is absolutely no you actually

00:03:16,290 --> 00:03:22,170
even when you're using iceberg are still

00:03:18,780 --> 00:03:27,420
storing your data in your Avro park' or

00:03:22,170 --> 00:03:29,790
orc files as you are now and what the

00:03:27,420 --> 00:03:31,890
table format is is actually a layer of

00:03:29,790 --> 00:03:36,060
abstraction in-between it's how you are

00:03:31,890 --> 00:03:38,400
organizing those files so that you get

00:03:36,060 --> 00:03:43,800
them grouped up into a table so that you

00:03:38,400 --> 00:03:48,450
can do planning quickly and run your job

00:03:43,800 --> 00:03:50,370
quickly so what would a good table

00:03:48,450 --> 00:03:52,170
format be first of all it'd be specified

00:03:50,370 --> 00:03:55,800
because you really want to be able to

00:03:52,170 --> 00:03:57,750
layout your data once and then use it

00:03:55,800 --> 00:03:59,370
for a lot of different execution engines

00:03:57,750 --> 00:04:03,330
you want to use the same data with hive

00:03:59,370 --> 00:04:06,600
with SPARC with presto right with all of

00:04:03,330 --> 00:04:09,380
them together and you want it to be well

00:04:06,600 --> 00:04:12,990
defined what goes where it should

00:04:09,380 --> 00:04:15,870
support atomic changes now part of the

00:04:12,990 --> 00:04:18,570
no sequel world has always been we don't

00:04:15,870 --> 00:04:20,730
need transactions well we found out that

00:04:18,570 --> 00:04:22,830
wasn't actually true we actually do want

00:04:20,730 --> 00:04:25,020
to be able to change the data and be

00:04:22,830 --> 00:04:28,470
able to mutate it with getting

00:04:25,020 --> 00:04:31,980
consistent results with people seeing

00:04:28,470 --> 00:04:33,630
the same results based on and consistent

00:04:31,980 --> 00:04:37,370
results based on when they start their

00:04:33,630 --> 00:04:40,050
query we also need schema evolution if

00:04:37,370 --> 00:04:42,190
you've got your data that's been

00:04:40,050 --> 00:04:43,960
accumulating over years

00:04:42,190 --> 00:04:46,570
almost guaranteed that you're going to

00:04:43,960 --> 00:04:48,880
need to change the columns that you have

00:04:46,570 --> 00:04:51,190
in that data you're going to add columns

00:04:48,880 --> 00:04:53,050
you're going to delete columns sometimes

00:04:51,190 --> 00:04:56,350
you don't even know that types you're

00:04:53,050 --> 00:04:58,780
going to end up with but in any case you

00:04:56,350 --> 00:05:00,940
want to make sure that it supports the

00:04:58,780 --> 00:05:04,840
evolution of those types and moves

00:05:00,940 --> 00:05:08,890
forward and you finally need to enable

00:05:04,840 --> 00:05:11,770
efficient access there's all the storage

00:05:08,890 --> 00:05:15,670
formats at this point support predicate

00:05:11,770 --> 00:05:18,130
push down that means that the query

00:05:15,670 --> 00:05:21,760
execution engines actually tell the the

00:05:18,130 --> 00:05:24,820
loader when it's reading a file ok I

00:05:21,760 --> 00:05:26,560
only need rows that satisfy particular

00:05:24,820 --> 00:05:29,080
predicates so for example if you're

00:05:26,560 --> 00:05:31,450
searching for a particular time range

00:05:29,080 --> 00:05:33,700
you actually push those predicates down

00:05:31,450 --> 00:05:35,560
into the reader so that it can just read

00:05:33,700 --> 00:05:40,090
the parts of the file that are needed

00:05:35,560 --> 00:05:42,880
for that particular query additional

00:05:40,090 --> 00:05:46,510
features it's really really good if it's

00:05:42,880 --> 00:05:51,520
a solid abstraction and doesn't leak the

00:05:46,510 --> 00:05:53,380
details out to the upper levels and you

00:05:51,520 --> 00:05:58,690
want to be able to evolve that layout

00:05:53,380 --> 00:06:02,350
over time ok now let me talk about where

00:05:58,690 --> 00:06:05,130
the current state of affairs when hive

00:06:02,350 --> 00:06:08,590
was first getting written by Facebook

00:06:05,130 --> 00:06:11,919
they actually came up with some really

00:06:08,590 --> 00:06:14,470
nice structures that were similar to

00:06:11,919 --> 00:06:20,910
some ones that actually the guy who

00:06:14,470 --> 00:06:23,700
started it saw a yahoo and so basically

00:06:20,910 --> 00:06:26,710
they create directories based on

00:06:23,700 --> 00:06:30,310
particular values of particular columns

00:06:26,710 --> 00:06:32,890
so for example this is a table that's

00:06:30,310 --> 00:06:36,820
sorted first are broken down first by

00:06:32,890 --> 00:06:38,590
date and then by our within the date in

00:06:36,820 --> 00:06:40,570
general I wouldn't recommend breaking

00:06:38,590 --> 00:06:42,010
down by hour because if you do the math

00:06:40,570 --> 00:06:44,260
you quickly end up with a lot of

00:06:42,010 --> 00:06:46,900
partitions very quickly I've doesn't do

00:06:44,260 --> 00:06:48,940
well if you do that but certainly by day

00:06:46,900 --> 00:06:53,530
makes a lot of sense

00:06:48,940 --> 00:06:55,689
and then within down at the bottom you

00:06:53,530 --> 00:06:59,559
have the files at the individual

00:06:55,689 --> 00:07:01,209
Pass wrote so here you can see that the

00:06:59,559 --> 00:07:03,219
important characteristics are you've got

00:07:01,209 --> 00:07:06,339
a directory structure you kind of needed

00:07:03,219 --> 00:07:08,110
to traverse the directory structure to

00:07:06,339 --> 00:07:11,829
see what data you have and then the data

00:07:08,110 --> 00:07:14,829
is actually done with the leaves now

00:07:11,829 --> 00:07:16,360
when you want to do a filter hi

00:07:14,829 --> 00:07:19,779
factually does the first level of

00:07:16,360 --> 00:07:23,169
filtering by only reading the

00:07:19,779 --> 00:07:26,259
directories that can match the predicate

00:07:23,169 --> 00:07:28,689
so for example if you only needed the

00:07:26,259 --> 00:07:31,809
19th hour out of that date then you

00:07:28,689 --> 00:07:33,579
could just go to that directory enesta

00:07:31,809 --> 00:07:40,839
directory and pull out the data that you

00:07:33,579 --> 00:07:43,989
need no that works but now you need this

00:07:40,839 --> 00:07:46,779
know where are the partitions so I've

00:07:43,989 --> 00:07:49,239
keeps a meta store and it tracks

00:07:46,779 --> 00:07:51,669
information about the the partitions

00:07:49,239 --> 00:07:53,469
that you have it also tracks the schema

00:07:51,669 --> 00:07:56,139
information and how your types have

00:07:53,469 --> 00:08:00,129
changed over time and it tracks some

00:07:56,139 --> 00:08:04,929
table statistics now it allows you to

00:08:00,129 --> 00:08:08,319
filter by partition values but there's a

00:08:04,929 --> 00:08:11,649
really big caveat there it'll filter on

00:08:08,319 --> 00:08:15,209
the client side and it won't filter on

00:08:11,649 --> 00:08:20,939
the database side unless your predicate

00:08:15,209 --> 00:08:24,339
your partition column is the string type

00:08:20,939 --> 00:08:27,059
it uses that external sequel database

00:08:24,339 --> 00:08:29,739
which is kind of annoying the cloud and

00:08:27,059 --> 00:08:32,620
the file system is the only place that

00:08:29,739 --> 00:08:34,419
the individual files are tracked and

00:08:32,620 --> 00:08:37,990
there's no way to add the profile

00:08:34,419 --> 00:08:41,349
statistics into the system now a few

00:08:37,990 --> 00:08:42,459
years ago those of us that Hortonworks

00:08:41,349 --> 00:08:45,220
were like okay

00:08:42,459 --> 00:08:48,100
hives structure is okay that that works

00:08:45,220 --> 00:08:52,149
for us but we want to be able to support

00:08:48,100 --> 00:08:56,309
acid transactions on our data and so we

00:08:52,149 --> 00:09:00,339
started implementing a and implemented a

00:08:56,309 --> 00:09:02,620
hive acid layout as its called

00:09:00,339 --> 00:09:05,800
and so we left the partition structure

00:09:02,620 --> 00:09:08,150
the same and in we added bucket files

00:09:05,800 --> 00:09:11,140
and Delta files so that you could

00:09:08,150 --> 00:09:14,570
do insert updates and deletes all with

00:09:11,140 --> 00:09:17,029
snapshot isolation from each other that

00:09:14,570 --> 00:09:23,320
was really really handy for our users

00:09:17,029 --> 00:09:26,900
and it's been very well received however

00:09:23,320 --> 00:09:29,600
now everything is a happy thing in hi

00:09:26,900 --> 00:09:31,490
flange first the partitions are only in

00:09:29,600 --> 00:09:33,140
the meta store and the files are on the

00:09:31,490 --> 00:09:36,230
file system so now you have to look to

00:09:33,140 --> 00:09:41,270
places in order to plan your query the

00:09:36,230 --> 00:09:43,460
bucketing is defined by java's hive

00:09:41,270 --> 00:09:46,400
implementation or sorry harvest Java

00:09:43,460 --> 00:09:48,680
implementation of the hash and so if you

00:09:46,400 --> 00:09:51,140
don't lay things out using that hash

00:09:48,680 --> 00:09:55,010
function now all of a sudden when hive

00:09:51,140 --> 00:09:57,020
tries to read it it will make bad

00:09:55,010 --> 00:10:00,140
assumptions and won't find your data

00:09:57,020 --> 00:10:02,480
right you can get incorrect results and

00:10:00,140 --> 00:10:04,970
so that leads to people doing things

00:10:02,480 --> 00:10:08,480
like saying ok spark can read hive data

00:10:04,970 --> 00:10:12,140
but it can't write to it and that blows

00:10:08,480 --> 00:10:13,910
away one of Hadoop's ecosystems really

00:10:12,140 --> 00:10:15,740
big strengths which is you can read your

00:10:13,910 --> 00:10:19,430
data at the way that you want to read it

00:10:15,740 --> 00:10:21,950
for this particular query another

00:10:19,430 --> 00:10:26,050
addition or problem until we added the

00:10:21,950 --> 00:10:28,640
acid layout was that the only atomic

00:10:26,050 --> 00:10:30,440
operation you had was adding a new

00:10:28,640 --> 00:10:32,390
partition that's why you often see

00:10:30,440 --> 00:10:34,250
people doing hourly partitions is

00:10:32,390 --> 00:10:36,170
because that was the only operation that

00:10:34,250 --> 00:10:38,300
you could do atomically you could write

00:10:36,170 --> 00:10:40,250
all the files and then add into the meta

00:10:38,300 --> 00:10:42,290
store and that worked any other

00:10:40,250 --> 00:10:44,180
combination didn't work deleting a

00:10:42,290 --> 00:10:48,080
partition adding files to a partition

00:10:44,180 --> 00:10:51,709
all of those would lead to race

00:10:48,080 --> 00:10:55,040
conditions on reading the data and it

00:10:51,709 --> 00:10:59,120
required a directory listing per

00:10:55,040 --> 00:11:02,240
partition so that is pretty painful in

00:10:59,120 --> 00:11:05,870
HDFS although it's ok where that's

00:11:02,240 --> 00:11:09,440
really problematic is the cloud object

00:11:05,870 --> 00:11:11,600
stores that's exactly why the hive query

00:11:09,440 --> 00:11:12,980
planning was taking so long it's because

00:11:11,600 --> 00:11:17,780
it's trying to do those directory

00:11:12,980 --> 00:11:19,640
listings in s3 s3 has another problem

00:11:17,780 --> 00:11:21,360
actually it's got two other problems

00:11:19,640 --> 00:11:24,690
that didn't matter here

00:11:21,360 --> 00:11:27,230
another the second one is that it's

00:11:24,690 --> 00:11:30,600
directories are eventually consistent

00:11:27,230 --> 00:11:33,029
your tasks can write a file in destory

00:11:30,600 --> 00:11:35,820
get back the response saying it's done

00:11:33,029 --> 00:11:38,430
if you look you'll see it but some other

00:11:35,820 --> 00:11:40,589
tasks on a different computer can look

00:11:38,430 --> 00:11:42,510
and sometimes it'll see it sometimes it

00:11:40,589 --> 00:11:46,589
won't see it now eventually it'll always

00:11:42,510 --> 00:11:48,360
see it but it's that eventually part

00:11:46,589 --> 00:11:50,430
that becomes really problematic because

00:11:48,360 --> 00:11:53,760
if your table is missing some of its

00:11:50,430 --> 00:11:57,390
data that's not just unfortunate that's

00:11:53,760 --> 00:11:59,640
wrong results and you can get in big

00:11:57,390 --> 00:12:06,300
trouble for your business by missing

00:11:59,640 --> 00:12:09,420
data the third problem actually is that

00:12:06,300 --> 00:12:11,040
the directory layout in s3 creates

00:12:09,420 --> 00:12:13,980
hotspots that are really hard to work

00:12:11,040 --> 00:12:16,980
around there are also some less obvious

00:12:13,980 --> 00:12:19,820
problems the first is that because all

00:12:16,980 --> 00:12:22,860
the partition values are being stringify

00:12:19,820 --> 00:12:25,140
some of the cases like null get

00:12:22,860 --> 00:12:27,480
translated to this crazy hive default

00:12:25,140 --> 00:12:29,430
partition thing if you ever seen that

00:12:27,480 --> 00:12:33,390
that's because your partition value had

00:12:29,430 --> 00:12:37,709
a null value in it another one is that

00:12:33,390 --> 00:12:40,290
the file or the statistics becomes stale

00:12:37,709 --> 00:12:43,620
so it's possible to get incorrect

00:12:40,290 --> 00:12:46,769
results based on the stale information

00:12:43,620 --> 00:12:52,470
and the hive meta store and not even

00:12:46,769 --> 00:12:57,630
know it the other an additional concern

00:12:52,470 --> 00:12:59,699
is that the hive table layouts are

00:12:57,630 --> 00:13:02,100
continually evolving and they're not

00:12:59,699 --> 00:13:04,519
documented anywhere right if you need to

00:13:02,100 --> 00:13:08,550
find out the details of the hive acid

00:13:04,519 --> 00:13:10,740
format you end up looking at code or

00:13:08,550 --> 00:13:13,110
asking someone who knows but mostly you

00:13:10,740 --> 00:13:17,000
end up looking at code and finally the

00:13:13,110 --> 00:13:20,430
bucket definition is tied the hive code

00:13:17,000 --> 00:13:22,800
other annoyances you need to know how

00:13:20,430 --> 00:13:24,510
that data is laid out right you need to

00:13:22,800 --> 00:13:26,519
know that if you just say time stamp

00:13:24,510 --> 00:13:29,520
greater than X you're gonna be doing it

00:13:26,519 --> 00:13:32,250
table skin if you end up wanting to do

00:13:29,520 --> 00:13:34,470
work instead of that you probably meant

00:13:32,250 --> 00:13:35,230
to say the temp temp is bigger than X

00:13:34,470 --> 00:13:37,930
and

00:13:35,230 --> 00:13:40,720
the condition on the day once you do

00:13:37,930 --> 00:13:42,460
that then you can get partition pruning

00:13:40,720 --> 00:13:43,780
but until you do that you aren't going

00:13:42,460 --> 00:13:46,870
to get it you're just going to do a

00:13:43,780 --> 00:13:49,690
whole table skin another annoyance is

00:13:46,870 --> 00:13:53,230
that the schema evolution in hive is

00:13:49,690 --> 00:13:54,700
defined by the file format so CSV it

00:13:53,230 --> 00:13:57,970
doesn't do anything everything is

00:13:54,700 --> 00:14:01,180
partition is position based parque and

00:13:57,970 --> 00:14:03,790
work will do it by name and so you get

00:14:01,180 --> 00:14:06,400
more flexibility actually everyone work

00:14:03,790 --> 00:14:09,190
for case mostly does it by position

00:14:06,400 --> 00:14:11,710
still then you also have additional

00:14:09,190 --> 00:14:14,410
problems like which format support

00:14:11,710 --> 00:14:20,200
decimal which ones support maps to a

00:14:14,410 --> 00:14:23,980
struct keys and so on so now as I said

00:14:20,200 --> 00:14:26,530
iceberg came out of Netflix and so I've

00:14:23,980 --> 00:14:28,720
done some work on iceberg but mostly I

00:14:26,530 --> 00:14:33,520
was their champion getting into Apache

00:14:28,720 --> 00:14:36,970
Incubator now icebergs design was

00:14:33,520 --> 00:14:40,440
basically that ok instead of using the

00:14:36,970 --> 00:14:45,010
hive meta store we're going to keep a

00:14:40,440 --> 00:14:47,140
track of all the files in s3 and so it

00:14:45,010 --> 00:14:49,600
keeps track of the list of files over

00:14:47,140 --> 00:14:54,250
time and every write produces a new

00:14:49,600 --> 00:14:56,980
snapshot so that gives us the ability to

00:14:54,250 --> 00:14:59,050
do this where readers use the current

00:14:56,980 --> 00:15:03,460
snapshot writers are writing a new

00:14:59,050 --> 00:15:06,360
snapshot and as time goes forward you

00:15:03,460 --> 00:15:08,560
get a series of these histories of

00:15:06,360 --> 00:15:11,080
showing which files are being included

00:15:08,560 --> 00:15:12,750
in the table and which ones aren't so

00:15:11,080 --> 00:15:16,110
any change is an atomic operation

00:15:12,750 --> 00:15:21,400
whether you're appending new data or

00:15:16,110 --> 00:15:23,820
merging or rewriting files in reality it

00:15:21,400 --> 00:15:30,130
gets a little more complicated than that

00:15:23,820 --> 00:15:32,560
so in particular iceberg implements it

00:15:30,130 --> 00:15:38,320
tracks the schema the partition layout

00:15:32,560 --> 00:15:41,020
and the properties of each of your your

00:15:38,320 --> 00:15:43,900
files and it tracks the old snapshots

00:15:41,020 --> 00:15:45,400
for garbage collection because you need

00:15:43,900 --> 00:15:46,370
to know when you can delete those old

00:15:45,400 --> 00:15:49,490
ones

00:15:46,370 --> 00:15:52,279
each metadata file is itself immutable

00:15:49,490 --> 00:15:54,680
but it allows us to keep track of more

00:15:52,279 --> 00:15:57,920
metadata about each of those files and

00:15:54,680 --> 00:16:00,649
so it's a copy-on-write system so you

00:15:57,920 --> 00:16:02,930
never can rewrite the old data you just

00:16:00,649 --> 00:16:05,899
make a new copy with the updated

00:16:02,930 --> 00:16:08,420
information and move it forward and of

00:16:05,899 --> 00:16:11,809
course you can roll back if things go

00:16:08,420 --> 00:16:15,529
badly now you don't want to rewrite

00:16:11,809 --> 00:16:17,589
those snapshots every single time and so

00:16:15,529 --> 00:16:20,990
iceberg doesn't make you do that it

00:16:17,589 --> 00:16:24,470
divides snapshots into manifest files

00:16:20,990 --> 00:16:26,990
and those manifest files contain the

00:16:24,470 --> 00:16:31,430
directory listings themselves and so

00:16:26,990 --> 00:16:34,430
they can store data across many

00:16:31,430 --> 00:16:38,509
partitions and you can reuse them across

00:16:34,430 --> 00:16:42,980
snapshots so in this example m 0 1 & 2

00:16:38,509 --> 00:16:47,480
or manifest files version 1 or a

00:16:42,980 --> 00:16:53,660
snapshot one is using M 0 and M one R

00:16:47,480 --> 00:16:59,059
sorry M 0 s 2 added m0 and m1 and then s

00:16:53,660 --> 00:17:03,470
3 replaced them both with us with M 2 so

00:16:59,059 --> 00:17:05,120
you can use those two to reuse parts of

00:17:03,470 --> 00:17:07,250
their table if you're just adding new

00:17:05,120 --> 00:17:08,630
stuff it's easy to add a new manifest

00:17:07,250 --> 00:17:12,140
you don't have to rewrite the old

00:17:08,630 --> 00:17:14,779
manifests you just need to write a new

00:17:12,140 --> 00:17:16,069
manifest and add in to your snapshot so

00:17:14,779 --> 00:17:21,020
it cuts down on the write amplification

00:17:16,069 --> 00:17:23,919
that you're having to do so what goes

00:17:21,020 --> 00:17:27,890
into them all the list of all the files

00:17:23,919 --> 00:17:30,830
and iceburg tracking data it also lets

00:17:27,890 --> 00:17:33,740
you track the partition values and the

00:17:30,830 --> 00:17:37,330
per column upper and lower bounds so

00:17:33,740 --> 00:17:40,490
that you can do partition pruning and

00:17:37,330 --> 00:17:44,210
actually even bucket pruning based on

00:17:40,490 --> 00:17:47,960
just the contents of the manifest and it

00:17:44,210 --> 00:17:50,890
also gives you row count sizes null

00:17:47,960 --> 00:17:53,630
counts a lot of extra detail that the

00:17:50,890 --> 00:17:56,120
optimizer so want in order to be able to

00:17:53,630 --> 00:18:00,250
operate efficiently on the data itself

00:17:56,120 --> 00:18:03,350
so that you can get much better query

00:18:00,250 --> 00:18:04,820
optimization without actually looking at

00:18:03,350 --> 00:18:07,820
the files you just look need to look at

00:18:04,820 --> 00:18:12,470
the manifests okay so how do you update

00:18:07,820 --> 00:18:17,090
these things the you start by doing a

00:18:12,470 --> 00:18:19,040
commit and the commit will need to take

00:18:17,090 --> 00:18:21,290
the current version create a new

00:18:19,040 --> 00:18:24,260
metadata version and manifest files and

00:18:21,290 --> 00:18:27,620
atomically swap them so how do you do it

00:18:24,260 --> 00:18:30,320
do that the easiest way is to use the

00:18:27,620 --> 00:18:33,110
database to actually point to where the

00:18:30,320 --> 00:18:37,640
root is or you can actually do atomic

00:18:33,110 --> 00:18:41,830
rename if you're in HDFS the atomic swap

00:18:37,640 --> 00:18:48,320
actually guarantees linear history and

00:18:41,830 --> 00:18:52,190
yeah so iceberg goes on the assumption

00:18:48,320 --> 00:18:55,400
that everything will work by default and

00:18:52,190 --> 00:18:57,530
that no one else is writing to the table

00:18:55,400 --> 00:19:02,150
that's true in the vast majority of

00:18:57,530 --> 00:19:04,520
cases now if you get unlucky and someone

00:19:02,150 --> 00:19:06,830
else is operating on the table it

00:19:04,520 --> 00:19:10,760
actually will detect the conflict and

00:19:06,830 --> 00:19:14,480
force the operation to retry with the

00:19:10,760 --> 00:19:18,350
newer metadata so you basically need to

00:19:14,480 --> 00:19:22,940
keep your the code keeps the assumptions

00:19:18,350 --> 00:19:25,280
about what state the query was in what

00:19:22,940 --> 00:19:28,880
it updated and then looks for conflicts

00:19:25,280 --> 00:19:31,220
assuming everything is good so for

00:19:28,880 --> 00:19:32,990
example if your input if you were just

00:19:31,220 --> 00:19:34,880
trying to merge several small files you

00:19:32,990 --> 00:19:36,730
could be merging two small Avro files

00:19:34,880 --> 00:19:40,100
and replacing them with a parquet file

00:19:36,730 --> 00:19:41,720
that one would be fine as long as none

00:19:40,100 --> 00:19:48,800
of those three files were getting

00:19:41,720 --> 00:19:50,570
deleted now if you someone did delete it

00:19:48,800 --> 00:19:54,530
one of those files and you would need to

00:19:50,570 --> 00:19:58,280
retry again and redo the operation with

00:19:54,530 --> 00:20:02,000
the updated state so what does that mean

00:19:58,280 --> 00:20:06,140
that means that now we are in a state

00:20:02,000 --> 00:20:10,190
where we don't need to do the directory

00:20:06,140 --> 00:20:13,220
listings right if you're in s3 iceburg

00:20:10,190 --> 00:20:14,000
can operate without accessing any of the

00:20:13,220 --> 00:20:21,620
data at all

00:20:14,000 --> 00:20:23,870
all and a rather sorry this iceberg can

00:20:21,620 --> 00:20:27,860
operate without doing any s3 directory

00:20:23,870 --> 00:20:29,540
listings at all that really is a huge

00:20:27,860 --> 00:20:31,460
speed up and means that you're

00:20:29,540 --> 00:20:34,700
guaranteed to get consistent operations

00:20:31,460 --> 00:20:38,630
you also get to avoid the prefix up

00:20:34,700 --> 00:20:43,730
problem where the the path operators try

00:20:38,630 --> 00:20:48,730
to open the files directly in place and

00:20:43,730 --> 00:20:51,830
so they get overloaded this s3 servers

00:20:48,730 --> 00:20:54,410
so you never are renaming things

00:20:51,830 --> 00:20:58,730
everything's are in place that makes

00:20:54,410 --> 00:21:02,690
life much much better and you get faster

00:20:58,730 --> 00:21:05,180
planning right you do one man one set of

00:21:02,690 --> 00:21:09,650
manifest reads and you don't have to do

00:21:05,180 --> 00:21:12,680
any of the directory listings now one of

00:21:09,650 --> 00:21:17,060
the advantage of that is that because

00:21:12,680 --> 00:21:20,240
now your partitions can be smaller you

00:21:17,060 --> 00:21:22,550
can actually get more pruning right

00:21:20,240 --> 00:21:25,490
because the hive meta store was your

00:21:22,550 --> 00:21:28,700
limit on how much scale out you could

00:21:25,490 --> 00:21:31,220
have on this data now you can actually

00:21:28,700 --> 00:21:35,030
go through and have smaller partitions

00:21:31,220 --> 00:21:37,040
and get away with it another hidden

00:21:35,030 --> 00:21:42,200
feature is that because as part of

00:21:37,040 --> 00:21:45,290
icebergs design you can treat the

00:21:42,200 --> 00:21:49,190
bucketing as very similar to partitions

00:21:45,290 --> 00:21:51,740
so you can actually get bucket pruning

00:21:49,190 --> 00:21:56,990
and only look at the buckets that are

00:21:51,740 --> 00:21:59,020
relevant for your particular query so

00:21:56,990 --> 00:22:02,150
that gives you a whole nother set of

00:21:59,020 --> 00:22:06,050
operations that you can push down that

00:22:02,150 --> 00:22:09,880
let you read fewer and fewer files which

00:22:06,050 --> 00:22:09,880
is exactly how you get the speed up

00:22:10,000 --> 00:22:18,950
finally you when you're using high

00:22:13,880 --> 00:22:22,250
formats you have your choice if you do a

00:22:18,950 --> 00:22:27,309
blind split you're just randomly cutting

00:22:22,250 --> 00:22:31,539
files at HDFS boundaries or pick

00:22:27,309 --> 00:22:33,999
random spots in an s3 blob with iceberg

00:22:31,539 --> 00:22:36,669
you actually record where the logical

00:22:33,999 --> 00:22:39,340
divisions are in the file so you can

00:22:36,669 --> 00:22:41,320
actually make specific splits and say

00:22:39,340 --> 00:22:45,070
I'm gonna cut it this offset that offset

00:22:41,320 --> 00:22:48,460
because I know how much data is there

00:22:45,070 --> 00:22:52,210
and where those points are so you don't

00:22:48,460 --> 00:22:56,860
need to to do the random probing you can

00:22:52,210 --> 00:23:03,720
use exactly the correct split and find

00:22:56,860 --> 00:23:06,070
out where those cut points are another

00:23:03,720 --> 00:23:08,740
characteristic is that it actually

00:23:06,070 --> 00:23:12,419
defines what the valid scheme evolution

00:23:08,740 --> 00:23:18,970
is right so you can add drop rename or

00:23:12,419 --> 00:23:21,820
reorder columns it actually does that by

00:23:18,970 --> 00:23:23,259
assigning IDs for the columns inside so

00:23:21,820 --> 00:23:28,149
you don't need to worry about the IDs

00:23:23,259 --> 00:23:30,879
but that lets you rename things and keep

00:23:28,149 --> 00:23:35,080
this scheme evolution alone I recently

00:23:30,879 --> 00:23:36,129
had a customer that in hive actually

00:23:35,080 --> 00:23:38,049
they were in the middle switching from

00:23:36,129 --> 00:23:40,539
hive to spark which is a whole nother

00:23:38,049 --> 00:23:43,419
set of issues but but that's what they

00:23:40,539 --> 00:23:47,619
were doing and in hive had actually

00:23:43,419 --> 00:23:49,779
worked because they basically took some

00:23:47,619 --> 00:23:53,950
work files and just dumped them in to a

00:23:49,779 --> 00:23:54,519
table it wasn't great it wasn't good but

00:23:53,950 --> 00:23:56,919
it worked

00:23:54,519 --> 00:24:00,879
now the unfortunate thing is they used

00:23:56,919 --> 00:24:04,299
the wrong names for the columns again

00:24:00,879 --> 00:24:05,860
hive they got lucky and it did the right

00:24:04,299 --> 00:24:08,110
thing because of the version of hives

00:24:05,860 --> 00:24:11,499
that they were using when they switch to

00:24:08,110 --> 00:24:14,799
spark now because spark had it a

00:24:11,499 --> 00:24:16,240
different code path it said oh the

00:24:14,799 --> 00:24:17,860
columns that I'm looking for aren't

00:24:16,240 --> 00:24:20,830
there I'll give you nulls for everything

00:24:17,860 --> 00:24:24,940
so they basically were getting moles out

00:24:20,830 --> 00:24:27,610
of their data and that all came about

00:24:24,940 --> 00:24:29,470
because the different execution engines

00:24:27,610 --> 00:24:31,539
hive and spark we're doing different

00:24:29,470 --> 00:24:34,960
kinds of schema evolution and so it

00:24:31,539 --> 00:24:39,309
didn't play well together with iceberg

00:24:34,960 --> 00:24:41,020
they defined one set of rules and so now

00:24:39,309 --> 00:24:43,270
all the execution engines

00:24:41,020 --> 00:24:45,630
the same scheme evolution across the

00:24:43,270 --> 00:24:49,410
different platforms

00:24:45,630 --> 00:24:53,290
it also standardizes the date and time

00:24:49,410 --> 00:24:55,570
characteristics the timestamp okay how

00:24:53,290 --> 00:24:59,640
many of you guys know what a mess Hadoop

00:24:55,570 --> 00:25:02,080
is with timestamps only a few of you

00:24:59,640 --> 00:25:05,020
more of you should be aware of just how

00:25:02,080 --> 00:25:08,320
messed up they are my favorite is that I

00:25:05,020 --> 00:25:09,970
was talking to the park' team and the

00:25:08,320 --> 00:25:13,750
park' depending on whether you're

00:25:09,970 --> 00:25:15,490
running from spark Impala or hive has

00:25:13,750 --> 00:25:19,570
completely different semantics for

00:25:15,490 --> 00:25:22,330
timestamp and they can't tell which one

00:25:19,570 --> 00:25:25,690
wrote it so they can't tell how to undo

00:25:22,330 --> 00:25:28,030
the semantics it's awesome and then they

00:25:25,690 --> 00:25:30,309
were trying to get hive to change its

00:25:28,030 --> 00:25:33,059
semantics so that the semantics of time

00:25:30,309 --> 00:25:35,620
stamp in hive would change depending on

00:25:33,059 --> 00:25:39,610
which file format they were storing in

00:25:35,620 --> 00:25:43,679
and like no no bad idea don't do that

00:25:39,610 --> 00:25:46,120
so we talked them off that cliff but but

00:25:43,679 --> 00:25:49,770
iceberg standardizing the timestamp

00:25:46,120 --> 00:25:54,520
stuff is really good stuff we need that

00:25:49,770 --> 00:25:58,210
and actually it even pushed orc to do a

00:25:54,520 --> 00:25:59,920
little bit better getting consistent

00:25:58,210 --> 00:26:03,130
support for decimals also a really big

00:25:59,920 --> 00:26:05,410
deal and of course it's got to support

00:26:03,130 --> 00:26:06,970
them the complicated types because of

00:26:05,410 --> 00:26:10,270
course you guys are all denormalizing

00:26:06,970 --> 00:26:11,620
your data as you should for the Big Data

00:26:10,270 --> 00:26:13,929
stuff

00:26:11,620 --> 00:26:16,540
it also supports hidden partitioning

00:26:13,929 --> 00:26:18,309
another feature of icebergs partitioning

00:26:16,540 --> 00:26:21,190
that's really handy is it's not

00:26:18,309 --> 00:26:27,910
hierarchical so if you have things split

00:26:21,190 --> 00:26:30,370
up by product and split up by country in

00:26:27,910 --> 00:26:32,640
hive you'd have to pick one of those to

00:26:30,370 --> 00:26:36,360
be the first level of the directory and

00:26:32,640 --> 00:26:39,340
the other one to be subordinate to it in

00:26:36,360 --> 00:26:41,050
iceberg they actually can be independent

00:26:39,340 --> 00:26:45,700
so you can choose whichever order you

00:26:41,050 --> 00:26:48,300
want and they'll both be available for

00:26:45,700 --> 00:26:53,080
previck you push down regardless of

00:26:48,300 --> 00:26:55,230
which order they would be an ordered in

00:26:53,080 --> 00:26:55,230
hi

00:26:55,620 --> 00:27:02,500
and finally get the the mixed file

00:26:59,350 --> 00:27:04,300
support and reliable metrics because the

00:27:02,500 --> 00:27:06,190
metrics are reliable in iceburg

00:27:04,300 --> 00:27:09,160
precisely because they're getting

00:27:06,190 --> 00:27:11,980
updated along with writing the data in

00:27:09,160 --> 00:27:14,650
diceberg so it's not two steps like it

00:27:11,980 --> 00:27:17,650
is in hive you write the data and part

00:27:14,650 --> 00:27:23,530
of that update is exactly updating the

00:27:17,650 --> 00:27:26,190
statistics okay so what have we done

00:27:23,530 --> 00:27:29,200
into other projects

00:27:26,190 --> 00:27:31,860
Ryan blue from Netflix has been working

00:27:29,200 --> 00:27:34,720
a lot on the data source v2 for spark

00:27:31,860 --> 00:27:39,340
trying to get more logical plans and

00:27:34,720 --> 00:27:41,590
behaviors in work we needed to add

00:27:39,340 --> 00:27:45,780
additional statistics and we actually

00:27:41,590 --> 00:27:49,020
are adding timestamp with local timezone

00:27:45,780 --> 00:27:54,300
because we only had timestamp instant or

00:27:49,020 --> 00:27:57,430
timestamp the local timestamp and

00:27:54,300 --> 00:28:00,220
partying Avro improvements again calm

00:27:57,430 --> 00:28:04,540
resolution by ID and a new

00:28:00,220 --> 00:28:10,570
materialization API so how do you get

00:28:04,540 --> 00:28:13,620
started with iceberg about six months

00:28:10,570 --> 00:28:16,420
ago at the end of the year I talked the

00:28:13,620 --> 00:28:19,390
Facebook guy or sorry the Netflix guys

00:28:16,420 --> 00:28:23,080
into contributing it to Apache so that's

00:28:19,390 --> 00:28:26,220
now an Apache paddling so you can go to

00:28:23,080 --> 00:28:29,050
Apache Incubator and pick up the code

00:28:26,220 --> 00:28:32,080
you contribute it just like any other

00:28:29,050 --> 00:28:35,440
Apache project complete with github

00:28:32,080 --> 00:28:37,570
issues and pull requests by the way it's

00:28:35,440 --> 00:28:39,730
been great having Apache actually

00:28:37,570 --> 00:28:42,160
support github issues and pull requests

00:28:39,730 --> 00:28:43,990
I don't know about you guys but whenever

00:28:42,160 --> 00:28:47,860
I go back to hive and have to deal with

00:28:43,990 --> 00:28:51,790
JIRA and downloading the issues off of

00:28:47,860 --> 00:28:56,640
there I'm like really so painful but oh

00:28:51,790 --> 00:28:59,410
well so the supported engines spark is

00:28:56,640 --> 00:29:03,179
supported pressed is supported someone

00:28:59,410 --> 00:29:09,089
did read-only support for pig and

00:29:03,179 --> 00:29:12,779
you can go to the dev list and ask lots

00:29:09,089 --> 00:29:14,969
of questions so what's some of the

00:29:12,779 --> 00:29:19,529
future work some of the future work is

00:29:14,969 --> 00:29:22,200
integration with hive Netflix doesn't

00:29:19,529 --> 00:29:25,019
use hai very much so they didn't start

00:29:22,200 --> 00:29:29,700
with that there's a Python library

00:29:25,019 --> 00:29:31,649
there's arrow support so they actually

00:29:29,700 --> 00:29:35,249
that's part of the Python support and

00:29:31,649 --> 00:29:38,580
actually one of the big pieces that has

00:29:35,249 --> 00:29:41,940
been lighting up the the dev list

00:29:38,580 --> 00:29:47,639
recently is dealing with Delta files now

00:29:41,940 --> 00:29:50,549
when I first read the the the spec for

00:29:47,639 --> 00:29:52,229
iceberg I was like oh it's pretty clear

00:29:50,549 --> 00:29:54,989
you want Delta files here right because

00:29:52,229 --> 00:29:57,179
we'd already done hive acid and we're

00:29:54,989 --> 00:30:00,299
like of course you want Delta files to

00:29:57,179 --> 00:30:03,830
be in here and so now we're going

00:30:00,299 --> 00:30:06,659
through and formalizing that actually

00:30:03,830 --> 00:30:09,089
there are some big companies that very

00:30:06,659 --> 00:30:13,979
much want that and so it'll take the

00:30:09,089 --> 00:30:16,589
form of probably two pieces one with the

00:30:13,979 --> 00:30:18,659
most important one is to have natural

00:30:16,589 --> 00:30:22,109
keys so if you've got a table that

00:30:18,659 --> 00:30:24,239
sorted on your primary key you want to

00:30:22,109 --> 00:30:26,580
take updates based on that primary key

00:30:24,239 --> 00:30:29,249
and then guarantee uniqueness in that

00:30:26,580 --> 00:30:33,450
primary key so you basically will record

00:30:29,249 --> 00:30:36,119
new versions saying hey that key out of

00:30:33,450 --> 00:30:38,690
that file is no longer there and it gets

00:30:36,119 --> 00:30:44,639
replaced with this new version from the

00:30:38,690 --> 00:30:47,729
the updated file there also be support

00:30:44,639 --> 00:30:50,039
for artificial keys so that if you get

00:30:47,729 --> 00:30:56,070
updates you can delete them if your

00:30:50,039 --> 00:31:00,839
table doesn't have natural keys okay so

00:30:56,070 --> 00:31:03,809
that's my presentation any questions yes

00:31:00,839 --> 00:31:05,550
queuing let me head over the mic if it's

00:31:03,809 --> 00:31:15,200
just wait a second

00:31:05,550 --> 00:31:17,250
are we recorded - um quick question

00:31:15,200 --> 00:31:19,530
performance-wise for writing and

00:31:17,250 --> 00:31:21,330
mentioned lot about reading mm-hmm

00:31:19,530 --> 00:31:23,160
it sounds like it's gonna do a bit more

00:31:21,330 --> 00:31:24,420
computation than for example maybe spark

00:31:23,160 --> 00:31:26,100
on high for the less intelligent

00:31:24,420 --> 00:31:28,020
bucketing and things is it comparable

00:31:26,100 --> 00:31:30,780
was the sort of situation there it's

00:31:28,020 --> 00:31:33,210
actually pretty comparable it's it's not

00:31:30,780 --> 00:31:35,310
that much more on the right load it's a

00:31:33,210 --> 00:31:39,530
little bit more because you're updating

00:31:35,310 --> 00:31:43,020
that's three blob with the results but

00:31:39,530 --> 00:31:46,020
assuming you don't have contention for

00:31:43,020 --> 00:31:48,420
writing the same files it's pretty

00:31:46,020 --> 00:31:51,780
comparable and on that note does it suit

00:31:48,420 --> 00:31:53,730
larger writes or lots of small writes or

00:31:51,780 --> 00:31:57,570
is it disease a favor one or the other

00:31:53,730 --> 00:31:59,850
um wait I'm sorry if you if it was

00:31:57,570 --> 00:32:03,630
trickling in data row by row is it gonna

00:31:59,850 --> 00:32:06,450
suit okay so this is absolutely not

00:32:03,630 --> 00:32:08,700
going to be super great for the case

00:32:06,450 --> 00:32:11,520
where you're trickling data and a few

00:32:08,700 --> 00:32:14,030
rows at a time right this isn't going to

00:32:11,520 --> 00:32:17,790
be the way you implement your online

00:32:14,030 --> 00:32:20,730
database that's not the intent here the

00:32:17,790 --> 00:32:25,080
intent here is yes you can delete

00:32:20,730 --> 00:32:29,550
records you know for gdpr it's great if

00:32:25,080 --> 00:32:32,010
you want to insert new rack new

00:32:29,550 --> 00:32:34,260
partitions that's great but you're much

00:32:32,010 --> 00:32:36,510
better off inserting things a million

00:32:34,260 --> 00:32:39,420
rows at a time than a few rows at a time

00:32:36,510 --> 00:32:42,090
so you can do a few rows but not

00:32:39,420 --> 00:32:52,400
millions of times in a second Thanks

00:32:42,090 --> 00:32:55,410
sure thanks for a great talk

00:32:52,400 --> 00:32:57,480
do you have an opinion on the recently

00:32:55,410 --> 00:33:02,850
open source Delta Lake and how would you

00:32:57,480 --> 00:33:06,690
compare it to iceberg yeah the first

00:33:02,850 --> 00:33:11,100
thing about Delta is that it's Apache

00:33:06,690 --> 00:33:16,290
licensed but it's a day to bricks thing

00:33:11,100 --> 00:33:18,960
and so what that means is that just like

00:33:16,290 --> 00:33:21,299
Isabel was talking about

00:33:18,960 --> 00:33:26,850
it's not open governance if you want to

00:33:21,299 --> 00:33:28,470
contribute to Delta it is you're going

00:33:26,850 --> 00:33:30,809
to have to contribute your code the data

00:33:28,470 --> 00:33:34,200
breaks right it's not like you're giving

00:33:30,809 --> 00:33:37,500
it to a open source Apache project

00:33:34,200 --> 00:33:40,610
you're giving it to data bricks if they

00:33:37,500 --> 00:33:43,289
decide to relicense it that's up to them

00:33:40,610 --> 00:33:46,020
so you're basically giving work to data

00:33:43,289 --> 00:33:51,240
bricks that doesn't make give me warm

00:33:46,020 --> 00:33:53,760
fuzzies but it depends what you're

00:33:51,240 --> 00:34:00,270
looking for other interesting bits out

00:33:53,760 --> 00:34:02,789
of data bricks Delta is it looks like

00:34:00,270 --> 00:34:05,370
according to their own documentation

00:34:02,789 --> 00:34:07,860
that you only get atomicity if you're

00:34:05,370 --> 00:34:12,270
using HDFS you don't get it out of cloud

00:34:07,860 --> 00:34:15,030
stores so I've only looked a bit at it

00:34:12,270 --> 00:34:17,460
I've I'll give a disclaimer but those

00:34:15,030 --> 00:34:22,460
are the two things that bother me about

00:34:17,460 --> 00:34:25,290
and I can see that I suspect that Delta

00:34:22,460 --> 00:34:27,690
got released that way precisely because

00:34:25,290 --> 00:34:36,480
they don't want the cloud providers

00:34:27,690 --> 00:34:38,849
shipping it Thanks presentation actually

00:34:36,480 --> 00:34:44,490
that was my question but oh so our own

00:34:38,849 --> 00:34:48,089
hoody that's iPod she so I actually

00:34:44,490 --> 00:34:50,639
haven't looked as much at hoodie hoodie

00:34:48,089 --> 00:34:53,399
you kind of sorry hoodie is an Apache

00:34:50,639 --> 00:34:55,500
project that came out of over uber

00:34:53,399 --> 00:34:57,960
definitely seems to be developing their

00:34:55,500 --> 00:35:00,089
own ecosystem of a bunch of different

00:34:57,960 --> 00:35:03,540
pieces and I just haven't looked at it

00:35:00,089 --> 00:35:06,470
as much I know Ryan has Ryan Ryan blue

00:35:03,540 --> 00:35:09,000
is the one the guy who started iceberg

00:35:06,470 --> 00:35:12,119
so he he's talked to them a fair amount

00:35:09,000 --> 00:35:14,760
but I haven't I'm sorry okay I have a

00:35:12,119 --> 00:35:16,560
complementary question sorry it was not

00:35:14,760 --> 00:35:18,420
clear for me how can i integrate this

00:35:16,560 --> 00:35:20,099
into a different frame were like let's

00:35:18,420 --> 00:35:22,710
think I want to write a connector for

00:35:20,099 --> 00:35:24,619
Frank is this Dave babies are stable

00:35:22,710 --> 00:35:29,010
because this is like a wall protocol

00:35:24,619 --> 00:35:32,490
it's absolutely possible I mean the the

00:35:29,010 --> 00:35:36,010
new ones are getting added

00:35:32,490 --> 00:35:40,350
for example the big one was contributed

00:35:36,010 --> 00:35:42,640
as well as the presto one the API is a

00:35:40,350 --> 00:35:45,220
reasonably stable it's still a young

00:35:42,640 --> 00:35:48,430
puzzling so it's they're not super

00:35:45,220 --> 00:35:52,960
stable but absolutely adapters for flink

00:35:48,430 --> 00:35:57,250
would be awesome wait I have a question

00:35:52,960 --> 00:36:00,190
again a follow-on an OD meat on the

00:35:57,250 --> 00:36:01,450
bones there so hoody was kind of came

00:36:00,190 --> 00:36:02,920
from a different background he'll to

00:36:01,450 --> 00:36:06,070
support machine learning pipelines as

00:36:02,920 --> 00:36:07,840
well so this notion of being able to do

00:36:06,070 --> 00:36:09,640
what they call time travel to make a

00:36:07,840 --> 00:36:11,740
query on on what was the value of this

00:36:09,640 --> 00:36:14,620
roam with this primary key at this point

00:36:11,740 --> 00:36:16,570
in time and also data validation so as

00:36:14,620 --> 00:36:18,310
you're ingesting data into the system to

00:36:16,570 --> 00:36:20,230
be able to have a scheme or some rules

00:36:18,310 --> 00:36:24,340
in which to validate it is that on the

00:36:20,230 --> 00:36:27,460
roadmap or sir are you time travel is is

00:36:24,340 --> 00:36:30,130
on the roadmap because the snapshots

00:36:27,460 --> 00:36:37,290
give that to you very very easily right

00:36:30,130 --> 00:36:40,030
it's easy to travel through time I guess

00:36:37,290 --> 00:36:45,850
what are you looking for in terms of the

00:36:40,030 --> 00:36:48,190
validation so your data's a schema but

00:36:45,850 --> 00:36:50,530
you don't know so in machine learning

00:36:48,190 --> 00:36:53,050
all data is numeric pretty much so your

00:36:50,530 --> 00:36:55,360
schema is F P 32 and even your

00:36:53,050 --> 00:36:57,370
categorical variables will be F P 32 but

00:36:55,360 --> 00:36:59,650
you want to know range is valid ranges

00:36:57,370 --> 00:37:01,300
so typically that's a problem in

00:36:59,650 --> 00:37:02,770
pipelines because you have to know if

00:37:01,300 --> 00:37:04,720
the value is outside the range it's an

00:37:02,770 --> 00:37:06,400
anomaly and you might want to write it

00:37:04,720 --> 00:37:08,470
you know you could write them up filter

00:37:06,400 --> 00:37:11,950
you know but you know having support for

00:37:08,470 --> 00:37:15,220
doing that so all so the min and Max of

00:37:11,950 --> 00:37:18,250
each column is automatically computed so

00:37:15,220 --> 00:37:23,290
and stored in the manifest so it'd be

00:37:18,250 --> 00:37:25,630
easy to get ranges out this is in t FX

00:37:23,290 --> 00:37:28,300
so 10-spoke pipelines and and there's

00:37:25,630 --> 00:37:31,910
also an amazon project called doopy i

00:37:28,300 --> 00:37:36,859
think it was dalton berlin

00:37:31,910 --> 00:37:38,480
okay okay oh just a note for the time

00:37:36,859 --> 00:37:42,230
troll I guess that's just the absence of

00:37:38,480 --> 00:37:43,940
garbage collection if you don't alright

00:37:42,230 --> 00:37:48,109
sure well let's take one last question

00:37:43,940 --> 00:37:51,259
here all right thanks for the talk

00:37:48,109 --> 00:37:53,539
my question is about migration so let's

00:37:51,259 --> 00:37:56,359
say you have a system now where you

00:37:53,539 --> 00:37:58,789
Sparky writing hive layout so how would

00:37:56,359 --> 00:38:04,430
you migrate to using both producing and

00:37:58,789 --> 00:38:07,599
consuming using iceberg that's a good

00:38:04,430 --> 00:38:10,819
question I don't have a good answer I

00:38:07,599 --> 00:38:13,210
suspect that the way that I mean like I

00:38:10,819 --> 00:38:15,980
said the hive integration isn't very

00:38:13,210 --> 00:38:17,869
plugged together yet it's it's in

00:38:15,980 --> 00:38:20,359
progress but but it's not plugged

00:38:17,869 --> 00:38:22,579
together what I suspect will end up

00:38:20,359 --> 00:38:25,039
happening is that you'll end up saying

00:38:22,579 --> 00:38:27,740
this is an iceberg table that's a hive

00:38:25,039 --> 00:38:29,539
table and then migrating I don't think

00:38:27,740 --> 00:38:31,819
you're going to be able to have one

00:38:29,539 --> 00:38:33,829
table that's both iceberg and hive at

00:38:31,819 --> 00:38:37,660
the same time right okay thanks

00:38:33,829 --> 00:38:39,810
all right okay so thank you speak again

00:38:37,660 --> 00:38:40,690
[Applause]

00:38:39,810 --> 00:38:44,130
[Music]

00:38:40,690 --> 00:38:44,130

YouTube URL: https://www.youtube.com/watch?v=z7p_m17BXs8


