Title: Berlin Buzzwords 2019: Fredrik Vraalsen â€“ Building applications with Serverless Framework ...
Publication date: 2019-06-28
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Fredrik Vraalsen talking about "Building applications with Serverless Framework and AWS Lambda".

In this workshop, you will become familiar with how to build serverless applications using Serverless Framework (serverless.com). Serverless Framework enables you to quickly define and configure your application as well as the resources it needs using simple yaml files, and then deploy it to your cloud platform of choice. In this workshop we will be using AWS Lambda. The application itself can be written using a variety of programming languages, such as JavaScript, Python or Java. We will cover how to create a REST API to call your functions, as well as how to use other triggers such as file uploads to S3, modifications in DynamoDB tables, or Kinesis event streams.

Read more:
https://2019.berlinbuzzwords.de/19/session/building-applications-serverless-framework-and-aws-lambda

About Fredrik Vraalsen:
https://2019.berlinbuzzwords.de/users/fredrik-vraalsen

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,720 --> 00:00:10,019
all right thank you welcome to this

00:00:08,400 --> 00:00:13,349
workshop so nice to see so many of you

00:00:10,019 --> 00:00:15,389
here yeah

00:00:13,349 --> 00:00:18,060
thanks for the introduction my name is

00:00:15,389 --> 00:00:22,890
Eric Ralston I'm going to talk today

00:00:18,060 --> 00:00:24,779
about mainly a doublet lambda and a tool

00:00:22,890 --> 00:00:27,420
called service framework which makes it

00:00:24,779 --> 00:00:31,399
simpler to build service applications

00:00:27,420 --> 00:00:34,730
using lambda and other cloud services

00:00:31,399 --> 00:00:37,140
we'll go through some of these topics

00:00:34,730 --> 00:00:38,850
how service framework helps us with

00:00:37,140 --> 00:00:41,190
configuration and deployment of our

00:00:38,850 --> 00:00:43,620
application we'll see some examples of

00:00:41,190 --> 00:00:45,660
how we can build different kinds of

00:00:43,620 --> 00:00:47,370
solutions using a DeBellis lambda such

00:00:45,660 --> 00:00:49,980
as a back-end api's or like rest

00:00:47,370 --> 00:00:53,670
services for example how can do event

00:00:49,980 --> 00:00:55,559
processing and we'll go into some of

00:00:53,670 --> 00:00:57,559
these other topics as well in terms how

00:00:55,559 --> 00:01:01,109
you could orchestrate multiple functions

00:00:57,559 --> 00:01:03,449
into a larger application we'll touch on

00:01:01,109 --> 00:01:05,010
things like performance and some tips

00:01:03,449 --> 00:01:10,260
for packaging your application and

00:01:05,010 --> 00:01:12,420
testing so this is a workshop but it's

00:01:10,260 --> 00:01:16,130
you can sort of if you want you can

00:01:12,420 --> 00:01:18,960
clone this repository from github and

00:01:16,130 --> 00:01:20,700
follow along the code if you want you

00:01:18,960 --> 00:01:23,549
can set up your own free Amazon account

00:01:20,700 --> 00:01:28,229
to run things but there are no exercises

00:01:23,549 --> 00:01:30,540
and such along the way but we'll yeah

00:01:28,229 --> 00:01:36,990
also the slides are on a link to the

00:01:30,540 --> 00:01:37,500
slides a PDF is on that web page so Who

00:01:36,990 --> 00:01:39,329
am I

00:01:37,500 --> 00:01:43,409
my name is Radhika said I come from Oslo

00:01:39,329 --> 00:01:45,630
in Norway where I just was since I was

00:01:43,409 --> 00:01:47,670
here last I started working at the city

00:01:45,630 --> 00:01:51,509
government you know slow as a data

00:01:47,670 --> 00:01:53,220
platform architect but before then I've

00:01:51,509 --> 00:01:55,170
been I just realized when I was writing

00:01:53,220 --> 00:01:57,420
this talk that I've reached a milestone

00:01:55,170 --> 00:02:01,170
I've been a professional developer for

00:01:57,420 --> 00:02:04,439
more than half my life so that's a I've

00:02:01,170 --> 00:02:04,950
been working mostly with like back-end

00:02:04,439 --> 00:02:06,899
development

00:02:04,950 --> 00:02:08,250
Java and that kind of stuff but in the

00:02:06,899 --> 00:02:12,170
past couple years working on data

00:02:08,250 --> 00:02:12,170
platform and data analysis

00:02:12,439 --> 00:02:18,299
so what do we do in the city government

00:02:15,659 --> 00:02:20,280
why are we building a data platform and

00:02:18,299 --> 00:02:23,280
what does this have to do with lambda

00:02:20,280 --> 00:02:25,650
and serverless so the city provides a

00:02:23,280 --> 00:02:26,939
lot of services a lot of different

00:02:25,650 --> 00:02:29,250
domains for arranging from like

00:02:26,939 --> 00:02:31,829
infrastructure waterworks and recycling

00:02:29,250 --> 00:02:36,870
and stuff to healthcare and schools and

00:02:31,829 --> 00:02:39,319
kindergartens and whatnot and currently

00:02:36,870 --> 00:02:42,510
these are organized very much as silos

00:02:39,319 --> 00:02:45,269
so there's little sort of collaboration

00:02:42,510 --> 00:02:50,430
between them in terms of sharing data

00:02:45,269 --> 00:02:53,459
sharing services and so on so about two

00:02:50,430 --> 00:02:56,790
years ago there was this vision called

00:02:53,459 --> 00:02:58,319
the story of Tim it's a YouTube video

00:02:56,790 --> 00:02:59,930
it's in a region but it's subtitled if

00:02:58,319 --> 00:03:02,189
you want to have a look at it

00:02:59,930 --> 00:03:05,540
essentially they we want to reshape how

00:03:02,189 --> 00:03:05,540
we provide services to the public

00:03:06,109 --> 00:03:11,280
essentially the users the citizens

00:03:09,810 --> 00:03:15,840
should not have to care about how the

00:03:11,280 --> 00:03:18,479
government is organized so this entails

00:03:15,840 --> 00:03:21,239
having access to the data that you need

00:03:18,479 --> 00:03:23,599
when you need it for example sharing

00:03:21,239 --> 00:03:26,790
giving access to the wants to need it

00:03:23,599 --> 00:03:29,699
providing more personalized content and

00:03:26,790 --> 00:03:31,620
services and also proactive services so

00:03:29,699 --> 00:03:33,419
that we can act you don't necessarily

00:03:31,620 --> 00:03:34,889
have to come to us we can provide you

00:03:33,419 --> 00:03:37,019
services when we know that you need them

00:03:34,889 --> 00:03:39,120
based on life events or things that

00:03:37,019 --> 00:03:41,970
happen where you live or other kinds of

00:03:39,120 --> 00:03:45,239
needs and also you should only need to

00:03:41,970 --> 00:03:46,979
provide us with the data once as opposed

00:03:45,239 --> 00:03:48,540
to having to submit the same information

00:03:46,979 --> 00:03:51,180
multiple times and act as kind of a

00:03:48,540 --> 00:03:53,790
mailman between the different parts of

00:03:51,180 --> 00:03:58,909
the city government least that's our our

00:03:53,790 --> 00:04:01,889
vision so we're building a data platform

00:03:58,909 --> 00:04:05,509
we have a data lake in the middle here

00:04:01,889 --> 00:04:07,859
and we're very focused on things like

00:04:05,509 --> 00:04:09,959
having control over our metadata we have

00:04:07,859 --> 00:04:11,759
a lot of different data sources so my

00:04:09,959 --> 00:04:13,560
previous company we worked with ad

00:04:11,759 --> 00:04:15,259
targeting and stuff like that so it was

00:04:13,560 --> 00:04:17,880
a fairly small domain but here we have

00:04:15,259 --> 00:04:20,130
50 different business units which each

00:04:17,880 --> 00:04:21,959
each their own domain and it's not

00:04:20,130 --> 00:04:24,210
really big data as much as it's very

00:04:21,959 --> 00:04:25,790
diverse data but we do need to share

00:04:24,210 --> 00:04:28,230
across

00:04:25,790 --> 00:04:31,050
so what we're trying to do is build this

00:04:28,230 --> 00:04:33,180
kind of service platform which this data

00:04:31,050 --> 00:04:35,160
platform is part of to make it easier

00:04:33,180 --> 00:04:37,260
for developers both in the city

00:04:35,160 --> 00:04:41,280
government and outside to provide new

00:04:37,260 --> 00:04:44,070
services as I mentioned it's not big

00:04:41,280 --> 00:04:45,570
data per se so our assumption as we

00:04:44,070 --> 00:04:48,300
started building this is that service

00:04:45,570 --> 00:04:51,420
would be a good fit because we have to

00:04:48,300 --> 00:04:54,420
provide very many services but it's not

00:04:51,420 --> 00:04:58,140
very high volume so we're doing things

00:04:54,420 --> 00:05:00,500
like we're handling a lot of documents

00:04:58,140 --> 00:05:02,310
essentially like Excel Word and so on

00:05:00,500 --> 00:05:06,720
different systems that we need to

00:05:02,310 --> 00:05:08,250
integrate with sensor data come in and

00:05:06,720 --> 00:05:10,320
of course it varies like the regular

00:05:08,250 --> 00:05:13,950
data into integrations with databases

00:05:10,320 --> 00:05:16,380
and external data sources we want to

00:05:13,950 --> 00:05:18,330
provide as much as as possible out again

00:05:16,380 --> 00:05:21,419
as open data for other partners or other

00:05:18,330 --> 00:05:24,510
people to use share with the businesses

00:05:21,419 --> 00:05:28,950
and commercial partners but also across

00:05:24,510 --> 00:05:31,380
the public sector and of course enable

00:05:28,950 --> 00:05:38,580
the users in the city to to do things

00:05:31,380 --> 00:05:41,310
like analytics analytics and insights so

00:05:38,580 --> 00:05:43,740
mentioned there's a broad specter here

00:05:41,310 --> 00:05:45,479
so one of the things I read recently was

00:05:43,740 --> 00:05:47,910
this article from Todd works which I

00:05:45,479 --> 00:05:50,430
thought was interesting and to share

00:05:47,910 --> 00:05:53,160
which talks about this data mesh idea

00:05:50,430 --> 00:05:55,620
where you have distributed data domains

00:05:53,160 --> 00:05:59,400
like I said we have very many different

00:05:55,620 --> 00:06:01,560
domains and we need to have an

00:05:59,400 --> 00:06:03,810
infrastructure below that so that the

00:06:01,560 --> 00:06:09,570
different data owners can create their

00:06:03,810 --> 00:06:11,220
own data pipelines and such so that's

00:06:09,570 --> 00:06:14,790
kind of what we're trying to build a lot

00:06:11,220 --> 00:06:16,050
of tooling and tools to enable them to

00:06:14,790 --> 00:06:18,060
build their own data pipelines

00:06:16,050 --> 00:06:20,160
self-serve so we shouldn't be sort of

00:06:18,060 --> 00:06:28,979
the man-in-the-middle bottleneck for

00:06:20,160 --> 00:06:30,360
them as I said service seems to be a

00:06:28,979 --> 00:06:33,390
good fit here because we will have a lot

00:06:30,360 --> 00:06:37,650
of small pipelines love small data sets

00:06:33,390 --> 00:06:39,539
and so on so what are we using service

00:06:37,650 --> 00:06:42,300
for in our data platform turn

00:06:39,539 --> 00:06:44,819
we're using it to build a set of micro

00:06:42,300 --> 00:06:46,860
services essentially arrest api's to do

00:06:44,819 --> 00:06:47,909
things like the metadata API is and

00:06:46,860 --> 00:06:50,580
things like that but we're also building

00:06:47,909 --> 00:06:52,460
processing pipelines so components for

00:06:50,580 --> 00:06:54,330
doing data transformations data

00:06:52,460 --> 00:06:57,659
validations and connecting these

00:06:54,330 --> 00:07:00,000
together and the the goal right now we

00:06:57,659 --> 00:07:02,250
need to code this thing essentially

00:07:00,000 --> 00:07:04,800
ourselves but the goal is for the

00:07:02,250 --> 00:07:09,240
different developers and users to be

00:07:04,800 --> 00:07:11,789
able to define this on their own we're

00:07:09,240 --> 00:07:15,150
using a lot of different services in

00:07:11,789 --> 00:07:16,710
Amazon these are but a few like your

00:07:15,150 --> 00:07:20,009
necess for stream processing for example

00:07:16,710 --> 00:07:22,169
api gateway for rest api s-- step

00:07:20,009 --> 00:07:23,729
functions for orchestrations and so on

00:07:22,169 --> 00:07:25,379
and we'll touch on some of these today

00:07:23,729 --> 00:07:32,460
but not all of them and we use a whole

00:07:25,379 --> 00:07:33,930
lot more as well so by the way sorry I

00:07:32,460 --> 00:07:35,340
forgot to mention at the beginning but

00:07:33,930 --> 00:07:40,590
if you have any questions feel free to

00:07:35,340 --> 00:07:42,810
stop me and along the way in and ask so

00:07:40,590 --> 00:07:45,110
we'll touch on these a little bit but

00:07:42,810 --> 00:07:48,960
the goal here is of course to talk about

00:07:45,110 --> 00:07:51,990
service framework and AWS lambda so

00:07:48,960 --> 00:07:55,800
anyone here already using Amazon lambda

00:07:51,990 --> 00:07:57,779
a couple of people yeah so a lot of this

00:07:55,800 --> 00:08:00,180
should be fairly familiar for you for

00:07:57,779 --> 00:08:02,880
you right I won't be able to go into

00:08:00,180 --> 00:08:05,419
much depth on these topics essentially

00:08:02,880 --> 00:08:09,599
since I'm trying to cover a fairly broad

00:08:05,419 --> 00:08:11,460
topic but so the actual lambda functions

00:08:09,599 --> 00:08:13,139
themselves will be very very simple I'm

00:08:11,460 --> 00:08:15,479
just trying to show you how we can use

00:08:13,139 --> 00:08:16,770
service framework to help with some of

00:08:15,479 --> 00:08:21,889
the issues that you have when you

00:08:16,770 --> 00:08:25,020
usually develop these functions so

00:08:21,889 --> 00:08:26,960
lambda is a compute service in Amazon

00:08:25,020 --> 00:08:32,240
Cloud that lets you deploy and run

00:08:26,960 --> 00:08:36,360
simple functions so what does that mean

00:08:32,240 --> 00:08:39,870
you can you can set up sorry you can set

00:08:36,360 --> 00:08:42,180
up you can upload functions and have

00:08:39,870 --> 00:08:45,449
them run based on various events or

00:08:42,180 --> 00:08:47,820
triggers as opposed to uploading a

00:08:45,449 --> 00:08:51,360
container or a doctor image of sources

00:08:47,820 --> 00:08:52,500
or some sort so it's just sing single

00:08:51,360 --> 00:08:55,160
functions essentially

00:08:52,500 --> 00:08:57,840
you can have a lot of them of course

00:08:55,160 --> 00:08:59,550
it's service there are no servers to

00:08:57,840 --> 00:09:01,320
manage it Amazon takes care of all the

00:08:59,550 --> 00:09:03,390
scaling for you and a kind of stuff so

00:09:01,320 --> 00:09:04,860
you don't have to worry about that and

00:09:03,390 --> 00:09:07,080
and you essentially you pay as you go

00:09:04,860 --> 00:09:10,170
you pay for the amount of compute that

00:09:07,080 --> 00:09:11,490
you use plus the memory so this is these

00:09:10,170 --> 00:09:16,500
are kind of the two things you need to

00:09:11,490 --> 00:09:20,640
think about the runtime is charged per

00:09:16,500 --> 00:09:22,500
one tenth of a second I believe and also

00:09:20,640 --> 00:09:29,130
the amount of memory that you reserve

00:09:22,500 --> 00:09:30,570
for a function comes into play and yeah

00:09:29,130 --> 00:09:34,740
and it supports a ton of different

00:09:30,570 --> 00:09:37,080
languages so you I'm not sure if this is

00:09:34,740 --> 00:09:40,950
complete but this is the one I I saw or

00:09:37,080 --> 00:09:43,350
the ones I saw recently at least we're

00:09:40,950 --> 00:09:45,690
using Python for a bunch of stuff we

00:09:43,350 --> 00:09:47,610
were doing but also not Java but

00:09:45,690 --> 00:09:49,230
Cortland so anything that runs on the JV

00:09:47,610 --> 00:09:52,020
I'm essentially you can use as well if

00:09:49,230 --> 00:09:55,560
you use Java or Scala or coupling go at

00:09:52,020 --> 00:09:56,790
it and you can also if you want to do

00:09:55,560 --> 00:10:05,670
something else you can bring your own

00:09:56,790 --> 00:10:07,770
runtime essentially alright so all good

00:10:05,670 --> 00:10:11,670
all demos should start with hello world

00:10:07,770 --> 00:10:14,390
right so this is the simplest function I

00:10:11,670 --> 00:10:18,930
could create essentially it's Python

00:10:14,390 --> 00:10:22,350
just takes in so a lambda function will

00:10:18,930 --> 00:10:24,330
take in an event your input event and a

00:10:22,350 --> 00:10:26,130
context which is essentially the lambda

00:10:24,330 --> 00:10:29,460
runtime context where you can do things

00:10:26,130 --> 00:10:31,770
like query for how much memory you have

00:10:29,460 --> 00:10:34,110
available or how long run time you have

00:10:31,770 --> 00:10:36,210
left until your function is terminated

00:10:34,110 --> 00:10:39,960
because you can think it's 15 minutes is

00:10:36,210 --> 00:10:41,570
the limit you can run a function for and

00:10:39,960 --> 00:10:44,160
you can do things like logging and so on

00:10:41,570 --> 00:10:46,500
so what you do when you deploy this is

00:10:44,160 --> 00:10:48,300
essentially you just set up you deploy

00:10:46,500 --> 00:10:51,030
this code and you said you tell lambda

00:10:48,300 --> 00:10:53,460
what your function endpoint is for the

00:10:51,030 --> 00:10:58,950
name of the class or whatever and the

00:10:53,460 --> 00:11:01,699
function that it should call so we can

00:10:58,950 --> 00:11:07,829
do a quick demo

00:11:01,699 --> 00:11:14,309
if I can't ride some X note I need to go

00:11:07,829 --> 00:11:21,269
out here and then you just

00:11:14,309 --> 00:11:24,689
let's see mirror alright if we go here

00:11:21,269 --> 00:11:26,040
to lambda I already have deployed a

00:11:24,689 --> 00:11:34,079
couple of lambda functions let's see if

00:11:26,040 --> 00:11:36,360
I can zoom in a bit that are running it

00:11:34,079 --> 00:11:44,189
was create just a simple function from

00:11:36,360 --> 00:11:48,050
scratch you can just call it hello and I

00:11:44,189 --> 00:11:48,050
don't know note so we'll do it in Python

00:11:53,749 --> 00:11:59,879
we'll set up a new function for you and

00:11:57,540 --> 00:12:02,129
give you essentially an editor where you

00:11:59,879 --> 00:12:05,550
can see the function code and start

00:12:02,129 --> 00:12:08,220
running writing things and here you see

00:12:05,550 --> 00:12:11,790
the the lambda dashboard so we have our

00:12:08,220 --> 00:12:13,050
function here it also has constant you

00:12:11,790 --> 00:12:14,100
can have some connections for instance

00:12:13,050 --> 00:12:15,600
here it's automatically set up

00:12:14,100 --> 00:12:17,429
connection for the cloud watch logging

00:12:15,600 --> 00:12:19,679
infrastructure you can have various

00:12:17,429 --> 00:12:21,929
triggers on the on the other side here

00:12:19,679 --> 00:12:23,790
you can see this list here is kind of

00:12:21,929 --> 00:12:25,620
things that can trigger your function

00:12:23,790 --> 00:12:28,949
right now we just want to create a

00:12:25,620 --> 00:12:32,179
function itself so we requested Python

00:12:28,949 --> 00:12:35,670
so it's already set up every simple

00:12:32,179 --> 00:12:37,709
handler function for us which just

00:12:35,670 --> 00:12:40,290
returns a JSON object essentially with

00:12:37,709 --> 00:12:43,410
the status code and a body so this would

00:12:40,290 --> 00:12:46,799
work well for arrests kind of function

00:12:43,410 --> 00:12:49,290
and if you see here our handler is

00:12:46,799 --> 00:12:51,990
specified as essentially in Python it's

00:12:49,290 --> 00:12:54,769
the file name and the function name so

00:12:51,990 --> 00:12:58,199
we could do if we want to just call this

00:12:54,769 --> 00:13:02,999
hello for instance we also need to

00:12:58,199 --> 00:13:06,269
update what to call here if we want to

00:13:02,999 --> 00:13:09,089
test this we can do so when you create a

00:13:06,269 --> 00:13:11,240
test event right now we don't have any

00:13:09,089 --> 00:13:14,550
input data that we want so you can just

00:13:11,240 --> 00:13:24,779
have an empty empty JSON object

00:13:14,550 --> 00:13:27,510
I went alright let's test it if we

00:13:24,779 --> 00:13:31,200
scroll up we should see that execution

00:13:27,510 --> 00:13:35,220
succeeded and it returned this object

00:13:31,200 --> 00:13:36,720
that we saw right so you can get started

00:13:35,220 --> 00:13:38,550
easily with like playing around with the

00:13:36,720 --> 00:13:40,050
different runtimes and such with this

00:13:38,550 --> 00:13:42,810
and you can you can go and you can edit

00:13:40,050 --> 00:13:44,250
the code from this console and start

00:13:42,810 --> 00:13:49,260
testing and playing around with it and

00:13:44,250 --> 00:13:52,079
tweaking the different options here so

00:13:49,260 --> 00:13:54,660
but of course this that's not nice for

00:13:52,079 --> 00:13:56,010
prototyping and stuff but it's not the

00:13:54,660 --> 00:14:00,060
way you want to do it in in your

00:13:56,010 --> 00:14:05,480
production system because you want to

00:14:00,060 --> 00:14:12,149
have a bit more control so see if we can

00:14:05,480 --> 00:14:13,100
it's not running separate display so

00:14:12,149 --> 00:14:18,029
what's the problem

00:14:13,100 --> 00:14:19,970
well this guy is not too used of course

00:14:18,029 --> 00:14:22,649
you don't want to have manual

00:14:19,970 --> 00:14:24,829
configuration and deployment of heavy

00:14:22,649 --> 00:14:28,050
functions and stuff you want to have

00:14:24,829 --> 00:14:30,360
essentially your infrastructure as code

00:14:28,050 --> 00:14:33,209
or at least that's configuration which

00:14:30,360 --> 00:14:36,600
is typically case not not usually code

00:14:33,209 --> 00:14:39,270
but more like yeah Mel or Jason so if

00:14:36,600 --> 00:14:41,250
you if you're familiar with Amazon the

00:14:39,270 --> 00:14:46,230
the tool that you typically use is

00:14:41,250 --> 00:14:47,910
called cloud formation and it supports

00:14:46,230 --> 00:14:50,670
setting up your lambda functions but

00:14:47,910 --> 00:14:53,399
it's very much it's it's a lot of

00:14:50,670 --> 00:14:55,380
boilerplate you have to essentially set

00:14:53,399 --> 00:14:58,440
up all the different endpoints and

00:14:55,380 --> 00:15:00,540
connections and and so on and - so you

00:14:58,440 --> 00:15:04,589
have to repeat a lot of stuff for each

00:15:00,540 --> 00:15:06,480
function that you create a lot of people

00:15:04,589 --> 00:15:09,360
say use terraform for setting up their

00:15:06,480 --> 00:15:11,209
infrastructure and services in in in the

00:15:09,360 --> 00:15:14,459
cloud so terraform is similar to

00:15:11,209 --> 00:15:19,079
transformation but it supports several

00:15:14,459 --> 00:15:21,420
cloud several clouds essentially how

00:15:19,079 --> 00:15:23,970
Amazon realize that said it using cloud

00:15:21,420 --> 00:15:25,350
formation for this was very tedious so

00:15:23,970 --> 00:15:27,990
they created something else they called

00:15:25,350 --> 00:15:30,480
server this application model or Sam

00:15:27,990 --> 00:15:33,780
not sure why they have a squirrel but as

00:15:30,480 --> 00:15:36,150
a logo but there you have it but there

00:15:33,780 --> 00:15:37,350
are also other options so the one that

00:15:36,150 --> 00:15:39,060
we're going to look at today is the

00:15:37,350 --> 00:15:41,550
server list framework which is kind of

00:15:39,060 --> 00:15:44,190
similar in that it focuses on setting up

00:15:41,550 --> 00:15:48,720
lambdas and functions but it also

00:15:44,190 --> 00:15:50,400
supports multiple clouds and use another

00:15:48,720 --> 00:15:52,320
option that we looked at when we were

00:15:50,400 --> 00:15:53,550
serving different tools with something

00:15:52,320 --> 00:15:55,380
called two Lumi which is actually

00:15:53,550 --> 00:15:59,550
infrastructure as code where you instead

00:15:55,380 --> 00:16:01,430
of configuring as a mole or Jason you

00:15:59,550 --> 00:16:04,170
actually code your infrastructure using

00:16:01,430 --> 00:16:06,900
JavaScript or Python or go or I think it

00:16:04,170 --> 00:16:08,070
supports other languages as well so that

00:16:06,900 --> 00:16:09,600
has some nice features that you can

00:16:08,070 --> 00:16:11,490
actually do like unit testing for

00:16:09,600 --> 00:16:15,570
example of your infrastructure code and

00:16:11,490 --> 00:16:17,010
things like that but we saw at least

00:16:15,570 --> 00:16:18,450
when we were looking at it it didn't

00:16:17,010 --> 00:16:20,100
seem to have support for all the

00:16:18,450 --> 00:16:25,550
features in the cloud that we wanted to

00:16:20,100 --> 00:16:28,020
use but it is moving ahead quite rapidly

00:16:25,550 --> 00:16:31,010
but the talk of the topic of this talk

00:16:28,020 --> 00:16:35,520
is the service framework so what is that

00:16:31,010 --> 00:16:38,100
do it's a free tool that you can

00:16:35,520 --> 00:16:39,950
download they also have this Enterprise

00:16:38,100 --> 00:16:42,600
version where you get a bunch of other

00:16:39,950 --> 00:16:44,850
support for having things in the cloud

00:16:42,600 --> 00:16:47,940
as software-as-a-service but we're only

00:16:44,850 --> 00:16:49,740
using the free framework essentially so

00:16:47,940 --> 00:16:52,050
what does it do it helps you configure

00:16:49,740 --> 00:16:54,480
and deploy your service applications and

00:16:52,050 --> 00:16:57,570
thus is much simpler than if you have to

00:16:54,480 --> 00:17:00,000
use the built-in cloud formation for

00:16:57,570 --> 00:17:03,600
example it supports multiple cloud

00:17:00,000 --> 00:17:05,250
operators so Google Amazon Microsoft you

00:17:03,600 --> 00:17:08,430
can also run things on cuban artists or

00:17:05,250 --> 00:17:12,650
open wisk which is I think it's an

00:17:08,430 --> 00:17:12,650
Apache project to run service functions

00:17:14,450 --> 00:17:20,250
ok so let's look at a simple example so

00:17:17,880 --> 00:17:22,589
similar to this basic hello world

00:17:20,250 --> 00:17:25,500
function so if you want to follow along

00:17:22,589 --> 00:17:31,710
in this git repository it's and the

00:17:25,500 --> 00:17:34,200
directory called one underscore hello so

00:17:31,710 --> 00:17:36,750
one way to get started with service

00:17:34,200 --> 00:17:37,980
framework is you can use templates so

00:17:36,750 --> 00:17:40,730
they provide a bunch of templates for

00:17:37,980 --> 00:17:42,410
creating essentially the setup for you

00:17:40,730 --> 00:17:47,270
function so you can for example you can

00:17:42,410 --> 00:17:50,270
create a template for Amazon function

00:17:47,270 --> 00:17:54,290
written in Python 3 or Java or coupling

00:17:50,270 --> 00:17:56,179
and so on and this will generate the the

00:17:54,290 --> 00:17:59,270
basic configuration files and and

00:17:56,179 --> 00:18:03,320
bare-bones function implementation for

00:17:59,270 --> 00:18:05,240
you but let's start from scratch because

00:18:03,320 --> 00:18:07,299
it's kind of simple the kind of things

00:18:05,240 --> 00:18:09,590
that we want to do now so a very basic

00:18:07,299 --> 00:18:11,690
configuration file for service they use

00:18:09,590 --> 00:18:14,059
a llamo configuration files essentially

00:18:11,690 --> 00:18:17,090
to tell you tell service what it's

00:18:14,059 --> 00:18:20,809
supposed to do so here we have define

00:18:17,090 --> 00:18:22,910
that we want a service named hello we

00:18:20,809 --> 00:18:25,280
need to tell it which cloud provider we

00:18:22,910 --> 00:18:27,919
should run it so in this case Amazon in

00:18:25,280 --> 00:18:32,780
the EU West one region and our runtime

00:18:27,919 --> 00:18:35,240
is Python 3.7 and finally we need to

00:18:32,780 --> 00:18:37,429
tell it what functions to create so we

00:18:35,240 --> 00:18:40,910
have one function here called hello with

00:18:37,429 --> 00:18:42,500
a handler which is the Python code that

00:18:40,910 --> 00:18:47,900
we want to run when this function is

00:18:42,500 --> 00:18:49,460
triggered and that code is the same that

00:18:47,900 --> 00:18:53,120
we saw previously so it's just this

00:18:49,460 --> 00:18:57,950
handler Python file with a Hello method

00:18:53,120 --> 00:19:05,860
that will be called so let's see how

00:18:57,950 --> 00:19:08,419
this works if we deploy it so in that

00:19:05,860 --> 00:19:09,620
git repository if you if you check out

00:19:08,419 --> 00:19:13,160
the code there there are some

00:19:09,620 --> 00:19:15,530
instructions on how to set up how to set

00:19:13,160 --> 00:19:16,250
up everything if you want to play with

00:19:15,530 --> 00:19:18,500
this yourself

00:19:16,250 --> 00:19:22,630
in terms of installing server listening

00:19:18,500 --> 00:19:25,340
for service framework and so on

00:19:22,630 --> 00:19:33,710
essentially you get a command called

00:19:25,340 --> 00:19:35,330
service or SLS for short so we can see

00:19:33,710 --> 00:19:37,820
that we have this service the ml file

00:19:35,330 --> 00:19:41,890
here the same that we saw so let me try

00:19:37,820 --> 00:19:46,400
to deploy this what it will do here is

00:19:41,890 --> 00:19:48,110
first it will it's actually under the

00:19:46,400 --> 00:19:50,630
covers it's using cloud formation for

00:19:48,110 --> 00:19:53,419
setting things up for configuring the

00:19:50,630 --> 00:19:55,570
resources in in Amazon so what is doing

00:19:53,419 --> 00:19:58,130
it to begin with here is this

00:19:55,570 --> 00:20:00,800
an s3 bucket for you to deploy your code

00:19:58,130 --> 00:20:02,270
in this takes a little while the first

00:20:00,800 --> 00:20:03,980
time but once you once you have yours

00:20:02,270 --> 00:20:08,330
function set up deploying new versions

00:20:03,980 --> 00:20:10,880
is much faster of course I also suggest

00:20:08,330 --> 00:20:12,170
that if you want to do this at a larger

00:20:10,880 --> 00:20:14,330
scale you probably want to set up a

00:20:12,170 --> 00:20:15,860
dedicated deploy bucket and configure it

00:20:14,330 --> 00:20:21,170
to use that instead of creating a

00:20:15,860 --> 00:20:23,330
separate bucket for each function okay

00:20:21,170 --> 00:20:26,360
so it's created our s3 bucket and now

00:20:23,330 --> 00:20:31,130
it's packaged and uploaded all the code

00:20:26,360 --> 00:20:33,400
two lines of it up to s3 and it's

00:20:31,130 --> 00:20:40,580
creating another CloudFormation stack

00:20:33,400 --> 00:20:45,410
for the lambda infrastructure so once

00:20:40,580 --> 00:20:47,750
this is done we can check out our lambda

00:20:45,410 --> 00:20:51,830
console to see that we have a function

00:20:47,750 --> 00:20:55,040
here so I have a bunch of them deployed

00:20:51,830 --> 00:20:57,320
but this is the one that we if we look

00:20:55,040 --> 00:21:01,580
at the last modified time this is the

00:20:57,320 --> 00:21:02,870
one that we just deployed as you can see

00:21:01,580 --> 00:21:06,050
it's very similar to the one we looked

00:21:02,870 --> 00:21:10,670
at previously this is the code that I

00:21:06,050 --> 00:21:13,340
told it to deploy now we can we can test

00:21:10,670 --> 00:21:16,190
it like we did previously in the console

00:21:13,340 --> 00:21:18,400
on the web console or we can use service

00:21:16,190 --> 00:21:20,929
itself to to run the code and test it

00:21:18,400 --> 00:21:25,309
which is quite nice so you can invoke a

00:21:20,929 --> 00:21:28,059
function just give it a function name

00:21:25,309 --> 00:21:30,920
hello and it will call the function

00:21:28,059 --> 00:21:37,910
unable AWS for you and it will show you

00:21:30,920 --> 00:21:40,250
the result so that's a easier way to

00:21:37,910 --> 00:21:41,390
test your functions right now this

00:21:40,250 --> 00:21:42,950
function of course is very simple it

00:21:41,390 --> 00:21:47,929
doesn't take any input and so on but

00:21:42,950 --> 00:21:51,890
we'll get to that if I want to if I want

00:21:47,929 --> 00:21:55,100
to remove this again I can just say SLS

00:21:51,890 --> 00:21:57,679
remove and I will take remove the

00:21:55,100 --> 00:22:02,780
function and destroy the s3 bucket and

00:21:57,679 --> 00:22:05,680
so on I will do that right now alright

00:22:02,780 --> 00:22:14,050
so let's go back to the

00:22:05,680 --> 00:22:15,640
presentation all right so there are some

00:22:14,050 --> 00:22:18,520
screenshots basically showing what I

00:22:15,640 --> 00:22:20,950
just did but I didn't talk about was

00:22:18,520 --> 00:22:23,050
this this output that I got one to

00:22:20,950 --> 00:22:24,400
deploy finished which shows your buttons

00:22:23,050 --> 00:22:26,290
of information about your deployed

00:22:24,400 --> 00:22:29,020
function right now there's not a whole

00:22:26,290 --> 00:22:31,120
lot here it just shows me the service

00:22:29,020 --> 00:22:32,800
name it also has this concept of stage

00:22:31,120 --> 00:22:34,120
so you can have multiple stages in the

00:22:32,800 --> 00:22:35,740
same account if you want to have like a

00:22:34,120 --> 00:22:39,070
development version testing version

00:22:35,740 --> 00:22:42,340
production version and so on we've

00:22:39,070 --> 00:22:44,770
deployed our dev and production code in

00:22:42,340 --> 00:22:47,440
separate AWS accounts for us that

00:22:44,770 --> 00:22:49,570
doesn't really help us that much but if

00:22:47,440 --> 00:22:50,770
you want to have multiple development

00:22:49,570 --> 00:22:52,480
versions for example you can have that

00:22:50,770 --> 00:22:59,800
in the same account with different stage

00:22:52,480 --> 00:23:03,040
names yeah so essentially this function

00:22:59,800 --> 00:23:07,170
name is a composite of this service name

00:23:03,040 --> 00:23:07,170
the stage name and the function itself

00:23:09,870 --> 00:23:16,320
we saw the invoke and yeah if I did the

00:23:13,720 --> 00:23:19,180
remove it would just remove all the

00:23:16,320 --> 00:23:21,610
lambda infrastructure and also remove

00:23:19,180 --> 00:23:23,710
the s3 bucket in this case and so it set

00:23:21,610 --> 00:23:27,160
up a bucket specifically for this

00:23:23,710 --> 00:23:29,650
function so what are some use cases for

00:23:27,160 --> 00:23:32,650
for lambda I think I talked a little bit

00:23:29,650 --> 00:23:33,760
about it but you can do a lot of

00:23:32,650 --> 00:23:36,900
different things like you can build

00:23:33,760 --> 00:23:39,430
back-end API is REST API is for instance

00:23:36,900 --> 00:23:41,590
using this you can also host essentially

00:23:39,430 --> 00:23:44,830
simple by about applications using

00:23:41,590 --> 00:23:49,150
lambda you can use it for event

00:23:44,830 --> 00:23:51,640
processing or file processing things

00:23:49,150 --> 00:23:54,250
like ETL transferring data from between

00:23:51,640 --> 00:23:56,730
different sources the things that we are

00:23:54,250 --> 00:23:58,930
currently using it for are these

00:23:56,730 --> 00:24:00,820
back-end and the vendor file processing

00:23:58,930 --> 00:24:06,550
so essentially our data pipelines and

00:24:00,820 --> 00:24:11,890
and our various rest services I'm sure

00:24:06,550 --> 00:24:13,510
there are plenty more use cases but as

00:24:11,890 --> 00:24:15,690
you can see when we look at how you can

00:24:13,510 --> 00:24:18,130
trigger these so you can trigger lambda

00:24:15,690 --> 00:24:19,299
in many different ways

00:24:18,130 --> 00:24:21,760
you can use

00:24:19,299 --> 00:24:23,890
you can trigger it through HTTP calls

00:24:21,760 --> 00:24:26,020
through either this API gateway which

00:24:23,890 --> 00:24:31,120
we'll see later or also their

00:24:26,020 --> 00:24:32,529
application tool balancer you can

00:24:31,120 --> 00:24:34,210
trigger it from various kind of

00:24:32,529 --> 00:24:38,080
messaging or notification services so

00:24:34,210 --> 00:24:39,820
SNS is this notification service in AWS

00:24:38,080 --> 00:24:44,919
and you have asked us as a messaging

00:24:39,820 --> 00:24:50,289
queue Kinesis is a stream event stream

00:24:44,919 --> 00:24:52,720
as a service and s3 is their objects or

00:24:50,289 --> 00:24:55,330
so so for instance we trigger are some

00:24:52,720 --> 00:24:57,039
pipelines when new files are uploaded to

00:24:55,330 --> 00:25:01,029
s3 and that will trigger the processing

00:24:57,039 --> 00:25:02,740
pipelines you can also trigger lambdas

00:25:01,029 --> 00:25:05,710
when there are changes in your dynamo DB

00:25:02,740 --> 00:25:07,750
database for instance or when you can

00:25:05,710 --> 00:25:10,059
also listen to WebSockets to have more

00:25:07,750 --> 00:25:13,559
long-lived sort of conversations between

00:25:10,059 --> 00:25:16,440
your your web client and and an lambda

00:25:13,559 --> 00:25:18,549
if you have sensors you can use the IOT

00:25:16,440 --> 00:25:20,649
infrastructure in numbers on we

00:25:18,549 --> 00:25:23,140
essentially just use a REST API for that

00:25:20,649 --> 00:25:24,010
currently you can trigger it through

00:25:23,140 --> 00:25:26,039
other things as well

00:25:24,010 --> 00:25:28,630
I like suffer like speech interfaces

00:25:26,039 --> 00:25:29,679
cloud watch if you have a large on your

00:25:28,630 --> 00:25:31,330
infrastructure for example you can

00:25:29,679 --> 00:25:35,740
trigger various lambda functions through

00:25:31,330 --> 00:25:38,490
that or just simple scheduled calls like

00:25:35,740 --> 00:25:42,490
every hour or 10 minutes or so

00:25:38,490 --> 00:25:46,870
also we'll look at today the API gateway

00:25:42,490 --> 00:25:50,470
integration for rest services or HTTP

00:25:46,870 --> 00:25:54,250
services and message handling through

00:25:50,470 --> 00:25:55,480
SMS event time links resinous part of

00:25:54,250 --> 00:25:56,980
the reason for that I wanted to do

00:25:55,480 --> 00:26:02,580
kinetics for instance but that's not

00:25:56,980 --> 00:26:02,580
available in the free account so yeah

00:26:02,820 --> 00:26:09,059
all right any questions so far or

00:26:13,200 --> 00:26:17,740
all right so let's look at how we can

00:26:15,670 --> 00:26:23,650
extend our simple hello function with a

00:26:17,740 --> 00:26:30,370
REST API to do this we will put the API

00:26:23,650 --> 00:26:31,810
gateway in front of of lamda there's a

00:26:30,370 --> 00:26:33,940
bunch of documentation on how you can

00:26:31,810 --> 00:26:37,600
configure that here on the service

00:26:33,940 --> 00:26:40,450
framework web page the slides are

00:26:37,600 --> 00:26:44,440
available as I said on this on this link

00:26:40,450 --> 00:26:47,980
here so essentially what we need to do

00:26:44,440 --> 00:26:50,320
is extend our service configuration with

00:26:47,980 --> 00:26:52,540
this simple we need to define what

00:26:50,320 --> 00:26:53,980
events will trigger our our function

00:26:52,540 --> 00:26:56,410
instead of just triggering it manually

00:26:53,980 --> 00:27:01,140
so in this case we set up an HTTP event

00:26:56,410 --> 00:27:05,800
with the pass and a method fairly simple

00:27:01,140 --> 00:27:07,900
stuff we can have of course of course

00:27:05,800 --> 00:27:09,910
path parameters here like a name we can

00:27:07,900 --> 00:27:13,780
also have support for query parameters

00:27:09,910 --> 00:27:15,010
and various headers and such our code

00:27:13,780 --> 00:27:17,380
will have to look a bit different

00:27:15,010 --> 00:27:21,160
because the now instead of getting just

00:27:17,380 --> 00:27:23,380
a payload for instance as an input we

00:27:21,160 --> 00:27:25,300
will get an event which contains a lot

00:27:23,380 --> 00:27:27,550
of information about the HTTP requests

00:27:25,300 --> 00:27:32,500
so for instance our event will now

00:27:27,550 --> 00:27:34,000
contain a dictionary for a nested set of

00:27:32,500 --> 00:27:36,040
dictionaries so one of them is path

00:27:34,000 --> 00:27:41,320
parameters and here we can extract the

00:27:36,040 --> 00:27:43,990
name parameter from the URL and we can

00:27:41,320 --> 00:27:46,870
build up our response for example as a

00:27:43,990 --> 00:27:50,800
JSON object with our with our greeting

00:27:46,870 --> 00:27:52,870
and return this so in this case we need

00:27:50,800 --> 00:27:56,650
to return we need to build up the HTTP

00:27:52,870 --> 00:27:59,800
response essentially but our status

00:27:56,650 --> 00:28:04,750
which result code and and the body of

00:27:59,800 --> 00:28:08,440
the response so this this JSON dumps

00:28:04,750 --> 00:28:12,360
will take care of basically writing this

00:28:08,440 --> 00:28:12,360
into a string recording it to a string

00:28:14,490 --> 00:28:21,100
there's a ton of details on how to

00:28:17,290 --> 00:28:22,179
configure this stuff but yeah there's a

00:28:21,100 --> 00:28:24,279
link here for more

00:28:22,179 --> 00:28:25,509
more details you can probably read it

00:28:24,279 --> 00:28:29,009
right now but if you have a look at the

00:28:25,509 --> 00:28:31,119
PDFs it should be okay all right so

00:28:29,009 --> 00:28:33,759
let's have a look at the deploy here

00:28:31,119 --> 00:28:35,769
it's very similar to what we saw in the

00:28:33,759 --> 00:28:37,629
previous one but there is no more detail

00:28:35,769 --> 00:28:40,179
now so for instance if we look at the

00:28:37,629 --> 00:28:42,399
output for this the result one display

00:28:40,179 --> 00:28:45,220
is done we'll have an endpoint now so we

00:28:42,399 --> 00:28:50,249
have defined a get endpoint here which

00:28:45,220 --> 00:28:50,249
we can call so let's try that

00:28:53,759 --> 00:29:06,399
switch back to mirror all right I need

00:29:03,369 --> 00:29:08,289
to move to the correct and I redeploy it

00:29:06,399 --> 00:29:11,200
so I have a different URL and it's in my

00:29:08,289 --> 00:29:15,159
slides but okay so here's the endpoint

00:29:11,200 --> 00:29:21,580
that I want I don't want to have the

00:29:15,159 --> 00:29:28,350
curly brace name thing so let's say we

00:29:21,580 --> 00:29:32,200
want to greet Berlin all right

00:29:28,350 --> 00:29:35,320
so this will by the way if you are doing

00:29:32,200 --> 00:29:35,919
any sort of rest or HTTP stuff from the

00:29:35,320 --> 00:29:38,320
command line

00:29:35,919 --> 00:29:40,539
I suggest installing HTTP PI if you

00:29:38,320 --> 00:29:46,600
haven't already it's very much nicer to

00:29:40,539 --> 00:29:49,210
work with and curl all right so here we

00:29:46,600 --> 00:29:52,929
see a bunch of response details and also

00:29:49,210 --> 00:29:54,700
the message body that we open right we

00:29:52,929 --> 00:29:58,379
can of course also call this with the

00:29:54,700 --> 00:29:58,379
invoked function that we saw earlier

00:29:59,669 --> 00:30:05,559
with the function name hello if we try

00:30:02,320 --> 00:30:07,240
to do this now without anything else of

00:30:05,559 --> 00:30:09,100
course it will fail because we didn't

00:30:07,240 --> 00:30:11,919
provide the parameter so essentially we

00:30:09,100 --> 00:30:14,769
will get a Python error message back

00:30:11,919 --> 00:30:16,830
because we didn't have very good error

00:30:14,769 --> 00:30:19,090
handling in our code right so we didn't

00:30:16,830 --> 00:30:21,129
typically we would say this should be a

00:30:19,090 --> 00:30:24,399
400 error or something we don't provide

00:30:21,129 --> 00:30:26,019
the passed parameters but we can do that

00:30:24,399 --> 00:30:30,909
also here on the command line I just

00:30:26,019 --> 00:30:32,590
need to pass it a data pass the data and

00:30:30,909 --> 00:30:33,730
now in this case since we're not we're

00:30:32,590 --> 00:30:36,010
calling the function directly and not

00:30:33,730 --> 00:30:38,529
through the API gateway we have to

00:30:36,010 --> 00:30:40,929
emulate the event format that the API

00:30:38,529 --> 00:30:47,700
gateway sent us so we can do this

00:30:40,929 --> 00:30:50,700
through saying pass parameters named

00:30:47,700 --> 00:30:50,700
Berlin

00:30:56,000 --> 00:31:04,240
should work hopefully yeah there we go

00:31:00,230 --> 00:31:07,190
so now we also see that entire response

00:31:04,240 --> 00:31:08,779
that it sends back to API gateway so API

00:31:07,190 --> 00:31:10,789
get well we'll take this response and

00:31:08,779 --> 00:31:13,820
convert it to an HTTP response with the

00:31:10,789 --> 00:31:15,379
correct status code and and body and you

00:31:13,820 --> 00:31:19,279
can also add things like headers here if

00:31:15,379 --> 00:31:34,850
you want to for example do course or

00:31:19,279 --> 00:31:46,389
something like that all right

00:31:34,850 --> 00:31:48,440
um yeah we saw this stuff yeah all right

00:31:46,389 --> 00:31:50,570
one thing we haven't talked about is

00:31:48,440 --> 00:31:51,980
documentation so you go of course if you

00:31:50,570 --> 00:31:54,820
create a REST API you want good

00:31:51,980 --> 00:31:57,710
documentation of it out-of-the-box right

00:31:54,820 --> 00:32:00,289
that's one of the things you don't get

00:31:57,710 --> 00:32:04,190
here since with this default lambda

00:32:00,289 --> 00:32:07,879
integration you get pretty much all of

00:32:04,190 --> 00:32:11,720
the HTTP handling is inside your code

00:32:07,879 --> 00:32:13,279
and not dealt with by the API gateway it

00:32:11,720 --> 00:32:15,529
just essentially just wraps the entire

00:32:13,279 --> 00:32:18,860
HTTP request and sends it to your to

00:32:15,529 --> 00:32:22,519
your code so if I look at the if I look

00:32:18,860 --> 00:32:24,470
at the web console for API gateway I can

00:32:22,519 --> 00:32:27,409
have it generate things like swagger or

00:32:24,470 --> 00:32:29,269
open API specs from my from my endpoints

00:32:27,409 --> 00:32:31,370
but here there's very little information

00:32:29,269 --> 00:32:32,629
it just has the path essentially in the

00:32:31,370 --> 00:32:37,269
message and it doesn't tell me anything

00:32:32,629 --> 00:32:37,269
about response format for example

00:32:38,169 --> 00:32:43,519
fortunately there is a plug-in for

00:32:40,610 --> 00:32:46,190
service framework that allows you to to

00:32:43,519 --> 00:32:49,220
extend this so it's called service AWS

00:32:46,190 --> 00:32:51,649
documentation which allows you to

00:32:49,220 --> 00:32:54,909
document a bunch of the like endpoints

00:32:51,649 --> 00:32:58,340
input and output formats and so on

00:32:54,909 --> 00:33:00,230
and this will then allow you to generate

00:32:58,340 --> 00:33:03,980
this swagger or open API spec from the

00:33:00,230 --> 00:33:06,619
API gateway what you need to do is

00:33:03,980 --> 00:33:09,740
essentially okay if you fall in from the

00:33:06,619 --> 00:33:11,809
github repo you need to extend your

00:33:09,740 --> 00:33:16,040
several CML with this documentation

00:33:11,809 --> 00:33:18,110
essentially descriptions the path

00:33:16,040 --> 00:33:21,230
parameters in this case with names

00:33:18,110 --> 00:33:23,540
description and your method response

00:33:21,230 --> 00:33:26,120
types so and here we have an example of

00:33:23,540 --> 00:33:29,900
okay so if we respond with application

00:33:26,120 --> 00:33:33,160
Jason this is our response model and we

00:33:29,900 --> 00:33:38,059
can just define our response models as

00:33:33,160 --> 00:33:41,780
essentially Jason schemas and we can

00:33:38,059 --> 00:33:43,670
also if when you have multiple of these

00:33:41,780 --> 00:33:46,790
in your file you typically will also

00:33:43,670 --> 00:33:49,070
want to utilize this include mechanism

00:33:46,790 --> 00:33:51,679
so you don't have a huge service animal

00:33:49,070 --> 00:33:54,400
file with all the stuff but these models

00:33:51,679 --> 00:33:56,570
can also be of course reused across

00:33:54,400 --> 00:33:58,130
different endpoints for example or our

00:33:56,570 --> 00:34:01,760
functions if they have the similar input

00:33:58,130 --> 00:34:04,760
or output formats so in order to apply

00:34:01,760 --> 00:34:09,129
this this plug-in you just have at this

00:34:04,760 --> 00:34:12,970
section here to your service CML file

00:34:09,129 --> 00:34:15,700
and now when you deploy you will get a

00:34:12,970 --> 00:34:17,659
swagger spec that contains more

00:34:15,700 --> 00:34:21,159
information than you can generate

00:34:17,659 --> 00:34:21,159
clients based on that and so on

00:34:27,690 --> 00:34:38,950
all right I realize I'm running very

00:34:34,929 --> 00:34:40,780
fast in my slides but please stop me if

00:34:38,950 --> 00:34:42,460
you have any questions or I think we can

00:34:40,780 --> 00:34:44,860
all so like I said but all of these

00:34:42,460 --> 00:34:46,389
examples are fairly simple but if we

00:34:44,860 --> 00:34:48,850
have time at the end we can go back and

00:34:46,389 --> 00:34:51,040
maybe try to extend some of them with

00:34:48,850 --> 00:34:55,630
some improved error handling or look at

00:34:51,040 --> 00:35:01,150
other ways to connect things so let's

00:34:55,630 --> 00:35:03,370
look at event processing in our case we

00:35:01,150 --> 00:35:09,550
want to use the the notification service

00:35:03,370 --> 00:35:12,370
as an S that's a trigger so similar to

00:35:09,550 --> 00:35:16,270
our HTTP endpoint we can set up an event

00:35:12,370 --> 00:35:18,220
type now with the type SNS and by

00:35:16,270 --> 00:35:20,230
default if you just give it a topic name

00:35:18,220 --> 00:35:22,180
it will actually also create this topic

00:35:20,230 --> 00:35:25,120
for you and give you all the necessary

00:35:22,180 --> 00:35:28,030
permissions to read from it and so on so

00:35:25,120 --> 00:35:31,150
I'm not you can also of course reuse

00:35:28,030 --> 00:35:34,450
topics that you have already set up but

00:35:31,150 --> 00:35:35,890
then you need to provide policies and

00:35:34,450 --> 00:35:37,660
roles and stuff to be able to access

00:35:35,890 --> 00:35:43,450
that so I'm not going to go into detail

00:35:37,660 --> 00:35:45,790
on that stuff today but you can of

00:35:43,450 --> 00:35:51,240
course reuse infrastructure that you've

00:35:45,790 --> 00:35:51,240
already set up in your in your account

00:35:52,440 --> 00:35:57,430
now our handler will look a bit

00:35:55,750 --> 00:35:59,170
different because of course this event

00:35:57,430 --> 00:36:02,980
format now will look different than the

00:35:59,170 --> 00:36:07,030
one that we had for HTTP requests so it

00:36:02,980 --> 00:36:08,830
will look more like like this all these

00:36:07,030 --> 00:36:11,560
different event formats are documented

00:36:08,830 --> 00:36:14,800
of course on the Amazon documentation

00:36:11,560 --> 00:36:16,330
pages but in this case we get not a

00:36:14,800 --> 00:36:20,200
single event at a time but a batch of

00:36:16,330 --> 00:36:25,270
events from SNS so you might get one or

00:36:20,200 --> 00:36:28,210
you might get multiple so we can do a

00:36:25,270 --> 00:36:31,050
for loop here over the records in in our

00:36:28,210 --> 00:36:35,950
input event and extract for example this

00:36:31,050 --> 00:36:37,690
the actual body of the message now in

00:36:35,950 --> 00:36:39,160
this case I haven't connected it this

00:36:37,690 --> 00:36:41,230
function to anything it except

00:36:39,160 --> 00:36:43,420
it's it's only writing essentially

00:36:41,230 --> 00:36:45,640
logging what it gets we can you know

00:36:43,420 --> 00:36:49,360
what we do in our case is typically

00:36:45,640 --> 00:36:51,040
right so for instance we have a bunch of

00:36:49,360 --> 00:36:53,500
sensor data coming in and we write the

00:36:51,040 --> 00:36:55,540
latest observed value into dynamo DB so

00:36:53,500 --> 00:36:58,020
we can build in a simple API on top of

00:36:55,540 --> 00:37:06,190
that just query for the last observed

00:36:58,020 --> 00:37:08,740
value so in order to publish an event

00:37:06,190 --> 00:37:12,520
here you can either go on the console

00:37:08,740 --> 00:37:14,470
the SNS to the SNS console and publish

00:37:12,520 --> 00:37:18,340
the event from there or you can use the

00:37:14,470 --> 00:37:22,300
command line tool for Amazon Web

00:37:18,340 --> 00:37:25,300
Services and USNS publish to your region

00:37:22,300 --> 00:37:29,110
and topic with a message so in this case

00:37:25,300 --> 00:37:32,080
Hello passwords you get some feedback on

00:37:29,110 --> 00:37:35,020
the message ID and if I look at at the

00:37:32,080 --> 00:37:38,260
logs I will see for the lambda function

00:37:35,020 --> 00:37:41,800
I will see that oh we got this event so

00:37:38,260 --> 00:37:48,850
you triggered successfully we can try

00:37:41,800 --> 00:37:52,180
this as well if we let's see where is my

00:37:48,850 --> 00:37:54,570
pointer there and I need to give you the

00:37:52,180 --> 00:37:54,570
display

00:38:00,450 --> 00:38:07,870
there's another service command here as

00:38:05,890 --> 00:38:13,240
well as info it's a it gives you this

00:38:07,870 --> 00:38:14,860
information that we had that you saw at

00:38:13,240 --> 00:38:16,420
the end of the deploy so if you want to

00:38:14,860 --> 00:38:18,400
get information about the end points and

00:38:16,420 --> 00:38:19,720
someone you can just run SLS info and it

00:38:18,400 --> 00:38:24,360
will tell you details about your

00:38:19,720 --> 00:38:27,850
deployed function um all right but here

00:38:24,360 --> 00:38:31,450
see I need to go into the actual console

00:38:27,850 --> 00:38:35,620
to see what it's called simple

00:38:31,450 --> 00:38:41,590
notification service see if we have any

00:38:35,620 --> 00:38:46,360
topics here yeah my events so I need to

00:38:41,590 --> 00:38:51,210
use this AR n which is basically the URL

00:38:46,360 --> 00:38:51,210
or identifier for for my my topic

00:39:05,829 --> 00:39:16,160
hello Berlin buzzword nope what did I do

00:39:15,619 --> 00:39:28,359
wrong

00:39:16,160 --> 00:39:33,859
published of course oh did it well sorry

00:39:28,359 --> 00:39:43,400
copy there anything why didn't it work

00:39:33,859 --> 00:39:45,289
there we go that's a bit slow the

00:39:43,400 --> 00:39:47,150
network again so that published and

00:39:45,289 --> 00:39:52,190
message there and if I go back to my

00:39:47,150 --> 00:39:56,059
logging cloud or logging console find

00:39:52,190 --> 00:40:04,400
the right log group which one was it

00:39:56,059 --> 00:40:05,869
event processing this is the one no no

00:40:04,400 --> 00:40:07,819
this is the one I think yeah it's you to

00:40:05,869 --> 00:40:10,700
see if always get tripped up by lead see

00:40:07,819 --> 00:40:12,950
here so here you can see the event that

00:40:10,700 --> 00:40:14,329
was published or the the log that we did

00:40:12,950 --> 00:40:16,130
but typically of course you will have

00:40:14,329 --> 00:40:19,420
this function do something more

00:40:16,130 --> 00:40:19,420
interesting than just log it

00:40:26,000 --> 00:40:38,599
all right and one of the things that we

00:40:35,599 --> 00:40:41,000
are using it for is to collect multiple

00:40:38,599 --> 00:40:44,119
functions together to create a pipeline

00:40:41,000 --> 00:40:46,040
and so how do you orchestrate the stuff

00:40:44,119 --> 00:40:51,230
how do you get them to work play well

00:40:46,040 --> 00:40:53,210
together essentially so you two two

00:40:51,230 --> 00:40:56,960
basic options I think one is that you

00:40:53,210 --> 00:40:58,330
can use the lambda functions fairly

00:40:56,960 --> 00:41:02,060
standalone and then connect them through

00:40:58,330 --> 00:41:04,369
like message queues or SNS topics and so

00:41:02,060 --> 00:41:08,990
forth or the nested stream for example

00:41:04,369 --> 00:41:12,200
and just use your regular Amazon API is

00:41:08,990 --> 00:41:14,180
to publish events from your lambdas

00:41:12,200 --> 00:41:15,619
because you can do whatever you want

00:41:14,180 --> 00:41:19,460
from your lambdas essentially and then

00:41:15,619 --> 00:41:22,070
that can trigger other other functions

00:41:19,460 --> 00:41:23,180
that gives you a nice decoupling which

00:41:22,070 --> 00:41:24,710
is can be good in some cases but

00:41:23,180 --> 00:41:26,840
sometimes you have something that works

00:41:24,710 --> 00:41:29,630
better as a whole and then you might

00:41:26,840 --> 00:41:31,849
want to have tools for orchestration

00:41:29,630 --> 00:41:36,109
orchestrating that better so what we use

00:41:31,849 --> 00:41:37,820
for that is Amazon step functions so

00:41:36,109 --> 00:41:42,470
this will essentially allows it to build

00:41:37,820 --> 00:41:44,750
a state machine or a flow another state

00:41:42,470 --> 00:41:50,839
machine it's more like a flow graph for

00:41:44,750 --> 00:41:54,859
your data and an execution so you can

00:41:50,839 --> 00:41:56,420
have various tasks that trigger in a

00:41:54,859 --> 00:41:58,790
sequence and you can have decision

00:41:56,420 --> 00:42:01,280
points where you can based on the event

00:41:58,790 --> 00:42:04,580
data you can trigger one or other paths

00:42:01,280 --> 00:42:06,560
in your graph for instance a check if

00:42:04,580 --> 00:42:08,270
you if you validate some data you can do

00:42:06,560 --> 00:42:12,109
different actions based on whether the

00:42:08,270 --> 00:42:18,440
data is valid or not so in here in our

00:42:12,109 --> 00:42:20,960
case we're defining this requires also

00:42:18,440 --> 00:42:22,730
another plugin by the way several step

00:42:20,960 --> 00:42:24,770
functions so there's a whole ecosystem

00:42:22,730 --> 00:42:28,240
of these kind of plugins for different

00:42:24,770 --> 00:42:31,040
different tools and different services

00:42:28,240 --> 00:42:34,640
so what we do here we have here we have

00:42:31,040 --> 00:42:38,140
two functions now one this is to get

00:42:34,640 --> 00:42:38,140
sensor data with temperatures

00:42:38,560 --> 00:42:42,490
very simplified compared to the one that

00:42:40,540 --> 00:42:45,010
we have in our system but so what we

00:42:42,490 --> 00:42:46,870
essentially try to do here is we want to

00:42:45,010 --> 00:42:49,840
validate that this temperature data is

00:42:46,870 --> 00:42:52,000
is valid according to our our schema for

00:42:49,840 --> 00:42:53,800
instance or that the values that we get

00:42:52,000 --> 00:42:56,410
in makes sense because we have some

00:42:53,800 --> 00:42:58,030
sensors now that are fairly cheap so

00:42:56,410 --> 00:42:59,680
they fail sometimes or they run out of

00:42:58,030 --> 00:43:02,410
power and then they suddenly reply a

00:42:59,680 --> 00:43:05,350
report that the temperature in the Oslo

00:43:02,410 --> 00:43:11,320
fjord is minus 273 degrees which is

00:43:05,350 --> 00:43:12,700
obviously false but zero Kelvin so those

00:43:11,320 --> 00:43:16,450
kinds of things we want to try to filter

00:43:12,700 --> 00:43:20,380
out right we also want this these

00:43:16,450 --> 00:43:22,090
sensors don't know much about other

00:43:20,380 --> 00:43:23,710
things like where they are located or

00:43:22,090 --> 00:43:27,370
descriptions and so on so we want to

00:43:23,710 --> 00:43:29,470
enrich the data with more information so

00:43:27,370 --> 00:43:31,240
we can do that so we have one function

00:43:29,470 --> 00:43:32,320
to validate the data that we get in and

00:43:31,240 --> 00:43:35,770
one to enrich it with some more

00:43:32,320 --> 00:43:38,740
information so we set up a state machine

00:43:35,770 --> 00:43:41,380
here so which we call process

00:43:38,740 --> 00:43:44,380
temperatures so it's triggered by an

00:43:41,380 --> 00:43:47,410
HTTP endpoint essentially so we can set

00:43:44,380 --> 00:43:49,420
up very similar to the HTTP trigger we

00:43:47,410 --> 00:43:53,080
use with API gateway this will trigger

00:43:49,420 --> 00:43:57,760
the stop function if we post an event to

00:43:53,080 --> 00:44:00,660
this to this pass and then we have the

00:43:57,760 --> 00:44:04,180
definition of our actual function

00:44:00,660 --> 00:44:09,700
orchestration so here we have a state

00:44:04,180 --> 00:44:11,710
machine called sorry I missing something

00:44:09,700 --> 00:44:15,730
anyway oh that was on the previous one

00:44:11,710 --> 00:44:17,820
so we have a set of tasks here and we

00:44:15,730 --> 00:44:21,070
start at this task about the temperature

00:44:17,820 --> 00:44:23,650
it connects to this one function that we

00:44:21,070 --> 00:44:26,440
created and here you can see this this

00:44:23,650 --> 00:44:31,120
naming here is what this is the actual

00:44:26,440 --> 00:44:33,610
cloud formation name that this function

00:44:31,120 --> 00:44:36,790
gets so if didn't know kind of that it

00:44:33,610 --> 00:44:44,020
will take this name here give capitalize

00:44:36,790 --> 00:44:46,180
it and add lambda function and then we

00:44:44,020 --> 00:44:48,850
say okay if this thing succeeds the next

00:44:46,180 --> 00:44:50,020
task is enriched location and it sort of

00:44:48,850 --> 00:44:51,200
you can build up a graph here

00:44:50,020 --> 00:44:57,069
essentially

00:44:51,200 --> 00:45:00,920
or yeah now our handler in this case has

00:44:57,069 --> 00:45:02,720
two functions in it one validate

00:45:00,920 --> 00:45:04,190
temperature and one enriched location so

00:45:02,720 --> 00:45:06,589
this validate temperature we'll just

00:45:04,190 --> 00:45:09,770
look at the temperature values and see

00:45:06,589 --> 00:45:14,359
if they make sense and if not it will

00:45:09,770 --> 00:45:16,670
throw an exception and it returns the

00:45:14,359 --> 00:45:18,200
event and that result from this will

00:45:16,670 --> 00:45:22,730
essentially then be passed on as the

00:45:18,200 --> 00:45:26,770
input to the next function in my next

00:45:22,730 --> 00:45:30,079
task exam in our state in our flow graph

00:45:26,770 --> 00:45:32,299
and here we'll just okay we won't look

00:45:30,079 --> 00:45:35,630
at we could add for example in our case

00:45:32,299 --> 00:45:38,180
we have a lookup table which gives us or

00:45:35,630 --> 00:45:39,589
we can look up the sensor ID and enrich

00:45:38,180 --> 00:45:41,089
it with more information with for

00:45:39,589 --> 00:45:42,130
example geographical location and

00:45:41,089 --> 00:45:49,010
description and so on

00:45:42,130 --> 00:45:51,799
and then return that so if we do this we

00:45:49,010 --> 00:45:57,160
will get a new end point essentially

00:45:51,799 --> 00:45:57,160
let's see if we can have a look at this

00:45:57,609 --> 00:46:03,760
keep losing my pointer here we go

00:46:08,310 --> 00:46:18,330
so we go into the orchestration here

00:46:11,030 --> 00:46:25,170
have a look at the info did you not

00:46:18,330 --> 00:46:28,220
deploy my own points interesting I just

00:46:25,170 --> 00:46:28,220
needed to have a check here

00:46:39,250 --> 00:46:45,920
alright ok this is why I have photos or

00:46:42,770 --> 00:46:53,900
pictures in my slides the demo effect

00:46:45,920 --> 00:46:55,849
hits alright let's go back here so much

00:46:53,900 --> 00:46:58,339
our why that didn't work but anyway

00:46:55,849 --> 00:47:02,230
so typically you will get this end point

00:46:58,339 --> 00:47:05,390
where you can post now supposed to get

00:47:02,230 --> 00:47:09,130
and what this thing if you use this HTTP

00:47:05,390 --> 00:47:11,000
RFI tool you can create a JSON object

00:47:09,130 --> 00:47:12,170
using this so this will essentially

00:47:11,000 --> 00:47:15,140
create a JSON object with the

00:47:12,170 --> 00:47:19,099
temperature field with the value 28 and

00:47:15,140 --> 00:47:21,770
as your body so this the response here

00:47:19,099 --> 00:47:27,160
from our HTTP endpoint is that it

00:47:21,770 --> 00:47:31,280
started executing our state machine and

00:47:27,160 --> 00:47:33,079
you will get something like this so we

00:47:31,280 --> 00:47:35,089
get a nice little graph showing us how

00:47:33,079 --> 00:47:44,000
the functions are connected together

00:47:35,089 --> 00:47:46,339
yes question right so the question is

00:47:44,000 --> 00:47:50,530
can you do asynchronous stuff in these

00:47:46,339 --> 00:47:55,339
or do you have to return the value so

00:47:50,530 --> 00:47:58,280
you can do asynchronous lambdas in

00:47:55,339 --> 00:47:59,750
general although in this case I think it

00:47:58,280 --> 00:48:02,839
makes more sense since you want the

00:47:59,750 --> 00:48:05,079
output here to go here if you're coding

00:48:02,839 --> 00:48:07,819
like for instance in in a language like

00:48:05,079 --> 00:48:11,480
JavaScript or using node that is more of

00:48:07,819 --> 00:48:15,680
a asynchronous approach then you in in

00:48:11,480 --> 00:48:16,880
your event handler or your method will

00:48:15,680 --> 00:48:19,730
take a third argument which is a

00:48:16,880 --> 00:48:21,950
callback function so you can once your

00:48:19,730 --> 00:48:23,839
code is done you will call that callback

00:48:21,950 --> 00:48:26,540
with your with your result is that

00:48:23,839 --> 00:48:30,020
answer your question yeah

00:48:26,540 --> 00:48:32,299
so in this case we're looking at the

00:48:30,020 --> 00:48:34,250
output from the are in enriched location

00:48:32,299 --> 00:48:36,680
so we see that okay got an input so it

00:48:34,250 --> 00:48:38,630
passed through the validation step it

00:48:36,680 --> 00:48:41,510
got the input just passed along and then

00:48:38,630 --> 00:48:46,280
the output from the function got added a

00:48:41,510 --> 00:48:48,710
location data now if we had call this

00:48:46,280 --> 00:48:51,020
with a temperature that was outside our

00:48:48,710 --> 00:48:52,100
defined range we would see something

00:48:51,020 --> 00:48:55,730
like this so in this

00:48:52,100 --> 00:48:59,780
case our valid date temperature function

00:48:55,730 --> 00:49:01,730
failed with an exception saying it's too

00:48:59,780 --> 00:49:07,190
cold since I passed it in the

00:49:01,730 --> 00:49:09,950
temperature minus 29 now this is a very

00:49:07,190 --> 00:49:11,720
simple example of course but in our

00:49:09,950 --> 00:49:14,180
system right now we have pipelines that

00:49:11,720 --> 00:49:15,890
look more like this for example so this

00:49:14,180 --> 00:49:18,200
is a simple pipeline to take in like

00:49:15,890 --> 00:49:20,030
Excel files convert them to CSV you do

00:49:18,200 --> 00:49:24,770
some validation and transformations on

00:49:20,030 --> 00:49:26,060
this and and yeah which the users of our

00:49:24,770 --> 00:49:28,820
pipeline can define what these

00:49:26,060 --> 00:49:31,460
transformations are and so on so you can

00:49:28,820 --> 00:49:35,150
build fairly complex complex stuff and

00:49:31,460 --> 00:49:38,300
also right now I only showed an example

00:49:35,150 --> 00:49:39,860
where in the previous one we're sort of

00:49:38,300 --> 00:49:41,510
the output of this goes directly has

00:49:39,860 --> 00:49:43,880
input to this but you can configure a

00:49:41,510 --> 00:49:46,040
bunch of more stuff like how essentially

00:49:43,880 --> 00:49:47,660
there's essentially a JSON object that

00:49:46,040 --> 00:49:48,980
flows through here which can you can

00:49:47,660 --> 00:49:52,010
modify along the way

00:49:48,980 --> 00:49:54,110
and tell it which parts to pass to this

00:49:52,010 --> 00:49:59,510
two different functions so we have one

00:49:54,110 --> 00:50:00,650
question here yes that's rich lambda

00:49:59,510 --> 00:50:03,050
function I'm not sure if there's it's

00:50:00,650 --> 00:50:04,640
similar time limit first that functions

00:50:03,050 --> 00:50:08,510
I don't think so so like the aggregate

00:50:04,640 --> 00:50:17,590
can be longer I think yeah questions in

00:50:08,510 --> 00:50:17,590
back this one yeah

00:50:30,779 --> 00:50:35,709
all right okay so why not so why are

00:50:34,179 --> 00:50:37,719
these different functions do we don't

00:50:35,709 --> 00:50:41,109
have a lot of overhead instead of

00:50:37,719 --> 00:50:42,880
putting everything into one one function

00:50:41,109 --> 00:50:45,819
so first of all these are written in

00:50:42,880 --> 00:50:48,069
different languages so some of the code

00:50:45,819 --> 00:50:52,209
here is Python but this one for example

00:50:48,069 --> 00:50:53,949
is Coplin so we're using here for

00:50:52,209 --> 00:50:56,259
example convert our excel file to CSV

00:50:53,949 --> 00:50:59,049
we're using a Python library for that

00:50:56,259 --> 00:51:03,429
but here we're using a Java library for

00:50:59,049 --> 00:51:04,809
doing a cc transformation so that allows

00:51:03,429 --> 00:51:05,829
us to build kind of things like that

00:51:04,809 --> 00:51:09,150
because otherwise we would have to

00:51:05,829 --> 00:51:11,589
reinvent one or the other essentially

00:51:09,150 --> 00:51:13,359
but yeah of course I mean the

00:51:11,589 --> 00:51:17,469
granularity here is something that you

00:51:13,359 --> 00:51:18,729
need to decide also in terms of I think

00:51:17,469 --> 00:51:20,949
the idea is here if you if you create

00:51:18,729 --> 00:51:24,009
these kind of pipelines how do you do

00:51:20,949 --> 00:51:29,650
error handling where does it retry where

00:51:24,009 --> 00:51:31,809
do try to recover if if something fails

00:51:29,650 --> 00:51:34,140
and so on so right now each of these

00:51:31,809 --> 00:51:36,909
steps will output or at least the first

00:51:34,140 --> 00:51:39,400
two or three will output files to s3

00:51:36,909 --> 00:51:43,079
which the next step will read in so we

00:51:39,400 --> 00:51:45,819
can we have more sort of history and

00:51:43,079 --> 00:51:47,380
audit log essentially of what was going

00:51:45,819 --> 00:51:52,029
on does that make sense

00:51:47,380 --> 00:51:58,509
yeah good questions but yeah for sure if

00:51:52,029 --> 00:52:00,069
this has been all one all one language

00:51:58,509 --> 00:52:02,169
it might make sense to combine some of

00:52:00,069 --> 00:52:04,419
these steps but these functions of

00:52:02,169 --> 00:52:06,640
course are also useful in other

00:52:04,419 --> 00:52:12,089
pipelines so we can combine these in

00:52:06,640 --> 00:52:12,089
different ways right yes question

00:52:20,730 --> 00:52:32,640
so sorry I didn't catch the test oh yes

00:52:35,370 --> 00:52:42,020
yeah so the question is what if we saw

00:52:39,900 --> 00:52:46,910
in previously that we have batching for

00:52:42,020 --> 00:52:46,910
input events for from SNS for instance

00:52:47,660 --> 00:52:56,130
so this one that the trigger for this

00:52:51,570 --> 00:52:59,430
depth function here is well in our

00:52:56,130 --> 00:53:00,630
example it was HTTP in our case it's

00:52:59,430 --> 00:53:03,420
actually another lambda function

00:53:00,630 --> 00:53:05,400
invoking it but so we don't have a batch

00:53:03,420 --> 00:53:07,470
of event coming in here I'm not sure how

00:53:05,400 --> 00:53:09,990
that would work really because how would

00:53:07,470 --> 00:53:12,480
you do like if one of them fails I don't

00:53:09,990 --> 00:53:17,070
think you can have like one event here

00:53:12,480 --> 00:53:19,050
fan-out like 500 on the next step if

00:53:17,070 --> 00:53:22,650
that's what you were asking about I

00:53:19,050 --> 00:53:24,810
think in our case the the the whole

00:53:22,650 --> 00:53:28,440
state machine or the whole flow will

00:53:24,810 --> 00:53:31,140
either fail or succeed yeah does that

00:53:28,440 --> 00:53:34,080
answer yeah but I think you know in our

00:53:31,140 --> 00:53:37,880
case all we always have one input event

00:53:34,080 --> 00:53:41,160
in this case yeah

00:53:37,880 --> 00:53:43,350
and there are so you you can use several

00:53:41,160 --> 00:53:45,720
different types of triggers for step

00:53:43,350 --> 00:53:47,640
functions as well but I'm not sure it

00:53:45,720 --> 00:53:49,830
doesn't have as many like you have HTTP

00:53:47,640 --> 00:53:51,780
endpoints you have scheduled timers and

00:53:49,830 --> 00:53:53,430
maybe one or two more but I don't think

00:53:51,780 --> 00:53:56,940
you can use SNS for example as a trigger

00:53:53,430 --> 00:53:58,200
so you'd have to from your SNS topic you

00:53:56,940 --> 00:54:01,200
could have a lambda that would then

00:53:58,200 --> 00:54:03,170
trigger your step functions right which

00:54:01,200 --> 00:54:07,890
would then call it once for each event

00:54:03,170 --> 00:54:10,040
so make sense any more questions yes one

00:54:07,890 --> 00:54:10,040
year

00:54:18,050 --> 00:54:23,490
so can the output or result from this

00:54:20,970 --> 00:54:25,590
fan out to multiple consumers again for

00:54:23,490 --> 00:54:28,620
instance it depends on what you do here

00:54:25,590 --> 00:54:32,340
like for instance here we write output

00:54:28,620 --> 00:54:33,750
s3 this will trigger another pipeline

00:54:32,340 --> 00:54:37,620
essentially which does further

00:54:33,750 --> 00:54:39,240
processing of this CSV file it will

00:54:37,620 --> 00:54:42,480
trigger essentially a number of

00:54:39,240 --> 00:54:43,980
pipelines so you could but but it's not

00:54:42,480 --> 00:55:00,360
connected to it's not this one

00:54:43,980 --> 00:55:03,150
triggering its directly yeah or or we

00:55:00,360 --> 00:55:05,160
could of course right to SNS or Genesis

00:55:03,150 --> 00:55:07,080
or something here right but if in order

00:55:05,160 --> 00:55:08,580
to trigger another orchestration like

00:55:07,080 --> 00:55:10,440
this you would have to have a lambda

00:55:08,580 --> 00:55:12,050
function in between your knesset stream

00:55:10,440 --> 00:55:20,430
for instance and and stuff functions

00:55:12,050 --> 00:55:21,900
that answer it yeah so yeah I went to

00:55:20,430 --> 00:55:26,940
the time at least for the stuff that we

00:55:21,900 --> 00:55:29,160
do now as we produce actually in each

00:55:26,940 --> 00:55:31,700
step here we produce intermediate files

00:55:29,160 --> 00:55:34,050
and s3 and that will trigger

00:55:31,700 --> 00:55:38,570
notifications which will then trigger

00:55:34,050 --> 00:55:38,570
potentially other runs of this yes

00:55:43,940 --> 00:55:51,380
you can do that yeah we're not doing it

00:55:47,660 --> 00:55:52,970
in this case but you can call you can do

00:55:51,380 --> 00:55:54,650
from your lambda you can do whatever you

00:55:52,970 --> 00:55:58,069
want essentially right as long as you

00:55:54,650 --> 00:56:01,309
keep within that time limit so if you

00:55:58,069 --> 00:56:04,579
can call out to two DynamoDB for example

00:56:01,309 --> 00:56:06,710
to query to join with data there or in

00:56:04,579 --> 00:56:08,299
our case we have some internal look-up

00:56:06,710 --> 00:56:12,259
tables that are basically just

00:56:08,299 --> 00:56:13,609
configuration yeah but you can call

00:56:12,259 --> 00:56:14,390
external services from your lambda

00:56:13,609 --> 00:56:19,849
function it's no problem

00:56:14,390 --> 00:56:23,329
oh yeah yeah yeah that could be an issue

00:56:19,849 --> 00:56:27,559
and we'll get into that briefly actually

00:56:23,329 --> 00:56:30,440
on the next slide all right any more

00:56:27,559 --> 00:56:33,579
questions before we move on all right

00:56:30,440 --> 00:56:33,579
let's talk about the performance then

00:56:33,789 --> 00:56:41,019
how many have heard of this cold start

00:56:36,920 --> 00:56:44,779
problem are you familiar with that yeah

00:56:41,019 --> 00:56:45,440
so okay so essentially the way lambda

00:56:44,779 --> 00:56:48,559
works

00:56:45,440 --> 00:56:51,140
I stole the next picture from a

00:56:48,559 --> 00:56:53,690
different from an article here the way

00:56:51,140 --> 00:56:58,640
it does is when you trigger a lambda it

00:56:53,690 --> 00:57:01,190
will first fetch the code from s3 it

00:56:58,640 --> 00:57:05,420
will start up a container to run with

00:57:01,190 --> 00:57:07,309
your runtime and and then once that is

00:57:05,420 --> 00:57:09,890
bitched at it's rad ready to actually

00:57:07,309 --> 00:57:12,470
run your code so this part here is

00:57:09,890 --> 00:57:17,390
what's called a cold start and it adds

00:57:12,470 --> 00:57:21,680
latency clear code right but what it

00:57:17,390 --> 00:57:27,499
does is it will reuse this container if

00:57:21,680 --> 00:57:29,150
you can within some time span defined by

00:57:27,499 --> 00:57:32,390
Amazon which they don't tell you but you

00:57:29,150 --> 00:57:33,859
know so you want to try to avoid this or

00:57:32,390 --> 00:57:36,259
at least keep this as low as possible

00:57:33,859 --> 00:57:39,619
but what they say is that you should

00:57:36,259 --> 00:57:42,739
really just worry about this part mainly

00:57:39,619 --> 00:57:44,359
because it's Amazon's challenge to sort

00:57:42,739 --> 00:57:46,099
of reduce this as much as possible right

00:57:44,359 --> 00:57:51,799
that's their deal that's what they can

00:57:46,099 --> 00:57:53,660
do but there are things you need to take

00:57:51,799 --> 00:57:56,120
into consideration here so that the

00:57:53,660 --> 00:57:59,690
lifecycle is essentially that so

00:57:56,120 --> 00:58:02,960
one container is one execution there's

00:57:59,690 --> 00:58:05,750
no parallelism inside one container so

00:58:02,960 --> 00:58:09,830
you run your handler when it's done it

00:58:05,750 --> 00:58:12,050
can run another or handle another event

00:58:09,830 --> 00:58:15,110
but it won't do multiple inside a single

00:58:12,050 --> 00:58:17,390
container in parallel so what happens if

00:58:15,110 --> 00:58:19,040
you have multiple requests or triggers

00:58:17,390 --> 00:58:22,100
coming in it will spin up more

00:58:19,040 --> 00:58:23,900
containers on demand right and then once

00:58:22,100 --> 00:58:27,560
they are idle it will spin them back

00:58:23,900 --> 00:58:30,200
down again but there's this time in

00:58:27,560 --> 00:58:32,780
between when the handler is done what it

00:58:30,200 --> 00:58:34,970
does is essentially it will freeze the

00:58:32,780 --> 00:58:37,940
container and the current state in the

00:58:34,970 --> 00:58:40,250
container and if another request comes

00:58:37,940 --> 00:58:42,500
in within some given time frame it will

00:58:40,250 --> 00:58:45,260
thaw that container and will enable that

00:58:42,500 --> 00:58:47,930
to to run that request or handle at

00:58:45,260 --> 00:58:49,640
request and then freeze it again but

00:58:47,930 --> 00:58:52,940
after some time the container will be

00:58:49,640 --> 00:58:55,160
destroyed and removed so what you want

00:58:52,940 --> 00:58:56,810
is to have essentially this happened

00:58:55,160 --> 00:59:04,220
right instead of building up new

00:58:56,810 --> 00:59:06,350
containers all the time what we see in

00:59:04,220 --> 00:59:10,490
terms of cold start in our code in in

00:59:06,350 --> 00:59:13,190
Python and and kopplin or Java JVM it

00:59:10,490 --> 00:59:14,630
varies but it can be and the cold start

00:59:13,190 --> 00:59:16,370
to be anywhere from like a couple

00:59:14,630 --> 00:59:16,670
hundred milliseconds to maybe a second

00:59:16,370 --> 00:59:20,350
or two

00:59:16,670 --> 00:59:22,810
typically whereas if you have a

00:59:20,350 --> 00:59:24,980
container that's ready it's your

00:59:22,810 --> 00:59:31,100
response time is done in a few

00:59:24,980 --> 00:59:34,520
milliseconds for the simple services one

00:59:31,100 --> 00:59:38,120
thing that can can make this quite a bit

00:59:34,520 --> 00:59:41,330
worse if we is if you're using virtual

00:59:38,120 --> 00:59:43,940
private cloud services if you if you

00:59:41,330 --> 00:59:47,780
want to for example query a database

00:59:43,940 --> 00:59:49,730
that's in a separate private subnet in

00:59:47,780 --> 00:59:51,860
your in your EPC configuration so you've

00:59:49,730 --> 00:59:55,220
sort of split your your account into

00:59:51,860 --> 00:59:59,320
multiple virtual clouds - for particular

00:59:55,220 --> 01:00:02,450
purposes and so on that's fairly common

00:59:59,320 --> 01:00:04,440
every time a lambda container needs to

01:00:02,450 --> 01:00:07,290
connect to that

01:00:04,440 --> 01:00:09,839
to that vbc it will need to set up a

01:00:07,290 --> 01:00:11,970
network interfaces a network connection

01:00:09,839 --> 01:00:15,540
and that can take a long time up to like

01:00:11,970 --> 01:00:17,490
10 seconds or so so that's one of the

01:00:15,540 --> 01:00:19,980
reasons why we for example decided to

01:00:17,490 --> 01:00:22,859
put all of our metadata and stuff for

01:00:19,980 --> 01:00:27,510
our pipelines in in dynamodb instead of

01:00:22,859 --> 01:00:31,260
in RDS their sequel as a service because

01:00:27,510 --> 01:00:33,030
that's not there you don't put you don't

01:00:31,260 --> 01:00:34,800
put DynamoDB in a V PC because you

01:00:33,030 --> 01:00:36,540
handle all the security stuff through

01:00:34,800 --> 01:00:37,710
their regular Identity and Access

01:00:36,540 --> 01:00:40,410
Management controls

01:00:37,710 --> 01:00:43,890
whereas if you run like a sequel

01:00:40,410 --> 01:00:45,510
database it will need to deal with its

01:00:43,890 --> 01:00:51,720
own security right it's not connected to

01:00:45,510 --> 01:00:56,910
that I am infrastructure so just to be

01:00:51,720 --> 01:00:58,410
aware that you can have long cold cold

01:00:56,910 --> 01:01:03,119
starts especially if you do things like

01:00:58,410 --> 01:01:07,380
connect to other subnets in your or

01:01:03,119 --> 01:01:08,880
other V pcs in your network they are

01:01:07,380 --> 01:01:10,950
working on improving this so instead of

01:01:08,880 --> 01:01:13,050
every container having to set up their

01:01:10,950 --> 01:01:15,240
own network interface it's gonna be one

01:01:13,050 --> 01:01:18,569
between the V pcs or something like that

01:01:15,240 --> 01:01:20,839
so there they announce something this

01:01:18,569 --> 01:01:23,160
winter I think that they're working on

01:01:20,839 --> 01:01:32,849
sometime this year it should be fixed

01:01:23,160 --> 01:01:37,230
not sure one yes yes right good point so

01:01:32,849 --> 01:01:40,470
resources that are costly those who

01:01:37,230 --> 01:01:44,130
typically in your code would typically

01:01:40,470 --> 01:01:46,650
be set up here right so you want to

01:01:44,130 --> 01:01:49,710
avoid that you do this in your in your

01:01:46,650 --> 01:01:51,359
handler code if you set up a network

01:01:49,710 --> 01:01:53,190
connection best connection for example

01:01:51,359 --> 01:01:55,680
or some other kind of resource that

01:01:53,190 --> 01:01:59,520
takes time to set up you want to have

01:01:55,680 --> 01:02:01,890
that be kept through this freeze and

01:01:59,520 --> 01:02:03,750
thaw cycle and you can do that if you

01:02:01,890 --> 01:02:05,430
just put it in like a global variable or

01:02:03,750 --> 01:02:09,450
a site a singleton or something that

01:02:05,430 --> 01:02:11,520
survives that sort of outside your

01:02:09,450 --> 01:02:15,150
handler function that's just part of the

01:02:11,520 --> 01:02:16,640
regular class setup for example then

01:02:15,150 --> 01:02:18,740
that will survive

01:02:16,640 --> 01:02:21,260
and you can reuse that and I've heard

01:02:18,740 --> 01:02:23,150
people do interesting things like put

01:02:21,260 --> 01:02:28,460
their cuff crack clients in there and so

01:02:23,150 --> 01:02:30,080
on which sounded area yeah and then they

01:02:28,460 --> 01:02:32,780
had to do things like check whether the

01:02:30,080 --> 01:02:37,480
client was still alive and restarted and

01:02:32,780 --> 01:02:37,480
so on so yeah you can do crazy stuff

01:02:38,380 --> 01:02:51,070
right and there in the end

01:02:45,680 --> 01:02:54,380
a couple more things so packaging your

01:02:51,070 --> 01:02:56,420
code we haven't looked at that in detail

01:02:54,380 --> 01:02:58,730
right but for now we've just written

01:02:56,420 --> 01:03:00,890
very very simple functions that I don't

01:02:58,730 --> 01:03:02,630
have any dependencies or anything but

01:03:00,890 --> 01:03:05,810
essentially it will so service framework

01:03:02,630 --> 01:03:07,190
will deploy a zip file but of course you

01:03:05,810 --> 01:03:11,090
have to tell it what should be in that

01:03:07,190 --> 01:03:13,010
zip file so for and you want to try to

01:03:11,090 --> 01:03:15,350
bundle all your dependencies in there so

01:03:13,010 --> 01:03:19,910
if you're writing Java code for example

01:03:15,350 --> 01:03:21,980
you can have Gradle or maven great like

01:03:19,910 --> 01:03:23,840
it's kind of fat jars that you can point

01:03:21,980 --> 01:03:27,590
to and have that be your deployment

01:03:23,840 --> 01:03:29,660
artifact you can also deploy and I

01:03:27,590 --> 01:03:31,850
actually think that's recommended is to

01:03:29,660 --> 01:03:34,130
have your dependencies in a separate

01:03:31,850 --> 01:03:36,110
jars in a library directory and have it

01:03:34,130 --> 01:03:38,660
put that into the zip file for some

01:03:36,110 --> 01:03:41,510
reason that supposed to be faster in the

01:03:38,660 --> 01:03:45,200
cold start or something less code to

01:03:41,510 --> 01:03:48,560
scan I think when I do the hot the hot

01:03:45,200 --> 01:03:51,190
path you can also do something called

01:03:48,560 --> 01:03:53,600
layers so if you have a lot of a lot of

01:03:51,190 --> 01:03:56,090
kind of libraries or defenses that you

01:03:53,600 --> 01:03:57,590
reuse across many functions you can

01:03:56,090 --> 01:03:58,940
create a layer that you can then build

01:03:57,590 --> 01:04:02,630
your function on top of so if you're

01:03:58,940 --> 01:04:04,850
using things like numpy or something you

01:04:02,630 --> 01:04:06,890
look pandas to do numerical analysis for

01:04:04,850 --> 01:04:08,930
example in Python you can create a layer

01:04:06,890 --> 01:04:11,060
that contains those libraries which can

01:04:08,930 --> 01:04:12,620
be fairly large like you can have I'm

01:04:11,060 --> 01:04:15,800
not sure about the size limits of the

01:04:12,620 --> 01:04:17,390
layers but these are easily like 40 50

01:04:15,800 --> 01:04:20,830
megabytes that you don't want to upload

01:04:17,390 --> 01:04:20,830
every time you upload your function

01:04:20,890 --> 01:04:24,800
that's a good idea to create a layer and

01:04:23,300 --> 01:04:26,480
then reuse it it's kind of similar to

01:04:24,800 --> 01:04:27,950
how you do docker images like you build

01:04:26,480 --> 01:04:30,080
on layers on top of each other I think

01:04:27,950 --> 01:04:32,320
you can have up to 5 layers

01:04:30,080 --> 01:04:35,570
on top of each other in in lambda

01:04:32,320 --> 01:04:37,400
another thing to think about is the

01:04:35,570 --> 01:04:39,710
runtime environment is Linux so if

01:04:37,400 --> 01:04:42,740
you're building your Python code and

01:04:39,710 --> 01:04:45,200
libraries dependencies on on Mac for

01:04:42,740 --> 01:04:47,480
example it will not include the correct

01:04:45,200 --> 01:04:51,410
libraries that like the native libraries

01:04:47,480 --> 01:04:53,120
so the thing you do there is for python

01:04:51,410 --> 01:04:54,980
there's another plugin that is called

01:04:53,120 --> 01:04:56,570
service Python requirements that will if

01:04:54,980 --> 01:04:58,370
you're not running on Linux it will boot

01:04:56,570 --> 01:05:03,740
up a docker image to do the packaging

01:04:58,370 --> 01:05:11,030
essentially so you get the Linux

01:05:03,740 --> 01:05:17,930
dependencies in there all right

01:05:11,030 --> 01:05:21,050
testing so we've seen a little bit of it

01:05:17,930 --> 01:05:23,060
so for instance if you if you just like

01:05:21,050 --> 01:05:26,390
write everything into your handler

01:05:23,060 --> 01:05:28,040
function a lot of concerns that are not

01:05:26,390 --> 01:05:30,560
part of your business logic will creep

01:05:28,040 --> 01:05:34,430
into your methods things like handing

01:05:30,560 --> 01:05:38,390
HTTP requests and responses dealing with

01:05:34,430 --> 01:05:40,700
the SNS event formats and such so like

01:05:38,390 --> 01:05:42,290
any sort of it's just good practice

01:05:40,700 --> 01:05:44,180
separate your business logic into

01:05:42,290 --> 01:05:46,700
separate methods and so on so you can

01:05:44,180 --> 01:05:50,750
test them unit test them and mock out

01:05:46,700 --> 01:05:53,720
for example if you call like for

01:05:50,750 --> 01:05:57,050
instance if you call other s.a.w

01:05:53,720 --> 01:05:59,240
services like dynamodb rs3 it's a good

01:05:57,050 --> 01:06:00,890
idea to to have those passed in as

01:05:59,240 --> 01:06:02,510
dependencies that you can unlock out or

01:06:00,890 --> 01:06:06,950
use things like in Python you have this

01:06:02,510 --> 01:06:10,820
the Python AWS libraries are called

01:06:06,950 --> 01:06:12,740
butoh or Poteau 3 and they have a motor

01:06:10,820 --> 01:06:14,840
library which is a mock version of that

01:06:12,740 --> 01:06:17,630
so you can have you can walk out and do

01:06:14,840 --> 01:06:20,170
tests for the different AWS services

01:06:17,630 --> 01:06:22,460
which is very useful so if you want to

01:06:20,170 --> 01:06:25,040
mock out s3 for example you can easily

01:06:22,460 --> 01:06:27,170
do that and then to set up your mocks

01:06:25,040 --> 01:06:30,590
and tests that things were put in the

01:06:27,170 --> 01:06:32,420
right place and so on another thing you

01:06:30,590 --> 01:06:34,910
can do is also to invoke the functions

01:06:32,420 --> 01:06:38,450
locally instead of calling out into the

01:06:34,910 --> 01:06:40,370
cloud so invoke local will run the

01:06:38,450 --> 01:06:42,109
function on on your local machine

01:06:40,370 --> 01:06:45,589
instead of

01:06:42,109 --> 01:06:47,029
course this can run into issues if you

01:06:45,589 --> 01:06:49,539
want to actually use other cloud

01:06:47,029 --> 01:06:51,799
infrastructure which is the big kind of

01:06:49,539 --> 01:06:54,039
hurdle here right if you want to use

01:06:51,799 --> 01:06:56,059
other services you either have to

01:06:54,039 --> 01:06:57,859
actually call into the cloud or and walk

01:06:56,059 --> 01:07:00,019
them out somehow or you can also try to

01:06:57,859 --> 01:07:01,940
use things like there are some tools

01:07:00,019 --> 01:07:04,279
that allow you to run things locally

01:07:01,940 --> 01:07:06,799
like kanessa light or you can actually

01:07:04,279 --> 01:07:08,809
download a local version of DynamoDB and

01:07:06,799 --> 01:07:11,420
run it on your machine typically you do

01:07:08,809 --> 01:07:14,869
this through docker images so there's

01:07:11,420 --> 01:07:20,049
also a bunch of tips and tricks for

01:07:14,869 --> 01:07:20,049
testing on the service framework webpage

01:07:22,660 --> 01:07:27,019
okay so there's a bunch of stuff I

01:07:24,529 --> 01:07:28,749
haven't talked about we have some time

01:07:27,019 --> 01:07:32,539
so we can go into stuff which we want

01:07:28,749 --> 01:07:35,210
setting up other infrastructure like

01:07:32,539 --> 01:07:37,309
outside that the stuff like if you have

01:07:35,210 --> 01:07:41,059
stuff that's shared between these

01:07:37,309 --> 01:07:42,890
functions or pipelines typically you

01:07:41,059 --> 01:07:44,960
want to set them up outside of service

01:07:42,890 --> 01:07:47,930
framework so we use terraform for that

01:07:44,960 --> 01:07:51,829
in our case so we just have to reference

01:07:47,930 --> 01:07:56,989
the IDs essentially in our inner service

01:07:51,829 --> 01:07:58,910
framework configurations access control

01:07:56,989 --> 01:08:01,869
I mentioned briefly but typically you

01:07:58,910 --> 01:08:04,579
also then need not only to reference the

01:08:01,869 --> 01:08:06,829
these infrastructure elements but also

01:08:04,579 --> 01:08:09,529
give them access you need to set up the

01:08:06,829 --> 01:08:11,029
lambda roles and so on so in our case we

01:08:09,529 --> 01:08:12,759
haven't set up anything like that so all

01:08:11,029 --> 01:08:15,230
of the stuff since we package everything

01:08:12,759 --> 01:08:16,670
we let several as framer create our s

01:08:15,230 --> 01:08:18,710
and s topic for example it it

01:08:16,670 --> 01:08:20,989
automatically set up the roles and

01:08:18,710 --> 01:08:23,659
policies to allow our lambda function to

01:08:20,989 --> 01:08:25,850
actually connect to that topic but if we

01:08:23,659 --> 01:08:27,259
have been reusing an existing topic we

01:08:25,850 --> 01:08:31,969
would need to provide the policies and

01:08:27,259 --> 01:08:34,810
stuff to do that in terms of the API

01:08:31,969 --> 01:08:38,449
gateway integrations we looked at

01:08:34,810 --> 01:08:40,880
there's a bunch of discussions on

01:08:38,449 --> 01:08:42,920
whether to use this what's called the

01:08:40,880 --> 01:08:45,020
lambda proxy which is what we saw which

01:08:42,920 --> 01:08:47,389
essentially passes the entire HTTP

01:08:45,020 --> 01:08:48,889
request into your into your method and

01:08:47,389 --> 01:08:51,440
lets you deal with all the details there

01:08:48,889 --> 01:08:53,089
whether you should use something call

01:08:51,440 --> 01:08:55,640
the lambda integration instead of lambda

01:08:53,089 --> 01:08:58,460
proxy so that lets the API

01:08:55,640 --> 01:09:00,650
I do its work actually like taking care

01:08:58,460 --> 01:09:03,740
of the HTTP part of it and just passing

01:09:00,650 --> 01:09:05,720
you and a more domain-specific or

01:09:03,740 --> 01:09:07,460
business specific event doing the

01:09:05,720 --> 01:09:10,850
transformations doing the status

01:09:07,460 --> 01:09:14,150
handling and so on so there this is a

01:09:10,850 --> 01:09:16,340
link but not sure if it's clickable in

01:09:14,150 --> 01:09:19,820
the PDF I'll have a I'll have a check if

01:09:16,340 --> 01:09:22,340
not I'll put it on the github repo so

01:09:19,820 --> 01:09:23,960
there's a this one links to like five

01:09:22,340 --> 01:09:27,710
four or five different articles with

01:09:23,960 --> 01:09:29,930
discussions on pros the cons there and

01:09:27,710 --> 01:09:32,630
related to that also validation of your

01:09:29,930 --> 01:09:35,300
input in our code now we would do the

01:09:32,630 --> 01:09:36,590
validation in our handler but in in the

01:09:35,300 --> 01:09:40,490
API gateway for example you could do

01:09:36,590 --> 01:09:44,480
validation of your response or requests

01:09:40,490 --> 01:09:47,560
and responses or requests I guess by

01:09:44,480 --> 01:09:52,430
providing for example Jason schemas and

01:09:47,560 --> 01:09:55,250
also things like the lambda proxy as far

01:09:52,430 --> 01:09:56,570
as I know doesn't handle or check that

01:09:55,250 --> 01:09:59,900
you for example call with the correct

01:09:56,570 --> 01:10:01,490
message or sorry

01:09:59,900 --> 01:10:04,340
with the collect correct content type

01:10:01,490 --> 01:10:06,050
and so on so you need to deal with a

01:10:04,340 --> 01:10:09,770
bunch of that stuff in your code whereas

01:10:06,050 --> 01:10:11,240
if you use the MOR the lambda

01:10:09,770 --> 01:10:13,370
integration it's more setup on the API

01:10:11,240 --> 01:10:19,310
gateway side but you get more out of it

01:10:13,370 --> 01:10:21,590
as well and versioning of your function

01:10:19,310 --> 01:10:23,060
so typically you will have if you have a

01:10:21,590 --> 01:10:24,560
continuous deployment pipeline for

01:10:23,060 --> 01:10:29,570
example you will have different versions

01:10:24,560 --> 01:10:31,850
of your function so SLS sorry service

01:10:29,570 --> 01:10:33,770
framework when it deploys it actually

01:10:31,850 --> 01:10:35,450
creates new versions of your of your

01:10:33,770 --> 01:10:37,580
function so if we go in and look at it

01:10:35,450 --> 01:10:40,750
every time I deploy now I'll get a new

01:10:37,580 --> 01:10:43,040
version and it just automatically

01:10:40,750 --> 01:10:44,360
redirects this list to the last one but

01:10:43,040 --> 01:10:46,220
you can you can have more control over

01:10:44,360 --> 01:10:47,990
this you can have aliases for example

01:10:46,220 --> 01:10:49,490
for your function version so you can say

01:10:47,990 --> 01:10:51,740
that okay this version should be run in

01:10:49,490 --> 01:10:53,600
production for my production endpoint

01:10:51,740 --> 01:10:55,610
this other version is my development

01:10:53,600 --> 01:11:02,300
version they can run on the latest at

01:10:55,610 --> 01:11:07,250
whatever time and so on alright to wrap

01:11:02,300 --> 01:11:08,650
up we looked at AWS and its function as

01:11:07,250 --> 01:11:10,929
a service it's very

01:11:08,650 --> 01:11:12,489
simple to set up you pay-as-you-go we

01:11:10,929 --> 01:11:14,890
don't have any servers that cost you

01:11:12,489 --> 01:11:18,429
money if they don't do anything and it's

01:11:14,890 --> 01:11:20,560
very scalable and performance from at

01:11:18,429 --> 01:11:25,840
least for our use cases have been very

01:11:20,560 --> 01:11:28,150
good service framework helps you with

01:11:25,840 --> 01:11:29,560
the configuration and deployment and

01:11:28,150 --> 01:11:33,400
testing and it has a bunch of plugins to

01:11:29,560 --> 01:11:35,469
do different things some things we're

01:11:33,400 --> 01:11:37,510
still evaluating this it seems to us

01:11:35,469 --> 01:11:39,010
that it doesn't support everything of

01:11:37,510 --> 01:11:41,080
course typically for these kind of tools

01:11:39,010 --> 01:11:42,219
that are more cloud agnostic they don't

01:11:41,080 --> 01:11:45,670
support everything or the latest

01:11:42,219 --> 01:11:47,949
greatest thing so another thing we see

01:11:45,670 --> 01:11:50,230
is I did it does make the easy stuff

01:11:47,949 --> 01:11:53,830
simpler it doesn't always make the

01:11:50,230 --> 01:11:56,350
harder stuff easier or it possible like

01:11:53,830 --> 01:12:00,550
for example this this API integration

01:11:56,350 --> 01:12:02,409
API a gateway integration lamda

01:12:00,550 --> 01:12:04,750
integration it doesn't support all

01:12:02,409 --> 01:12:06,270
validation stuff there for example so in

01:12:04,750 --> 01:12:10,570
that case we would have to go to either

01:12:06,270 --> 01:12:14,710
terraform or this service application

01:12:10,570 --> 01:12:16,480
model for from AWS but it's perfectly

01:12:14,710 --> 01:12:17,920
possible to mix and match here to have

01:12:16,480 --> 01:12:19,750
most of your stuff in service framework

01:12:17,920 --> 01:12:21,940
but if you need to go outside that you

01:12:19,750 --> 01:12:24,190
can also include other cloud formation

01:12:21,940 --> 01:12:26,739
stuff directly into your service framer

01:12:24,190 --> 01:12:28,570
configuration as well so yeah so we're

01:12:26,739 --> 01:12:31,210
still evaluating some of these but we've

01:12:28,570 --> 01:12:36,610
so far been we've been very happy with

01:12:31,210 --> 01:12:40,929
the service framework so far so any user

01:12:36,610 --> 01:12:44,080
final questions or feedback feel free to

01:12:40,929 --> 01:12:46,170
contact me after the talk or as well of

01:12:44,080 --> 01:12:46,170
course

01:12:51,889 --> 01:12:54,889
yeah

01:13:00,119 --> 01:13:06,889
hi really nice presentation Thanks

01:13:03,210 --> 01:13:11,699
borrowing just a question of the

01:13:06,889 --> 01:13:16,290
commands you show the server lists are

01:13:11,699 --> 01:13:19,880
those open to I mean the free ones right

01:13:16,290 --> 01:13:22,739
because you said there would be also uh

01:13:19,880 --> 01:13:25,980
yeah so it's all the the service

01:13:22,739 --> 01:13:27,869
framework is free if you use okay not

01:13:25,980 --> 01:13:29,820
sure the status of its like open source

01:13:27,869 --> 01:13:33,869
or something but it's it's on github I

01:13:29,820 --> 01:13:36,719
believe the enterprise stuff is more

01:13:33,869 --> 01:13:40,050
about like administration's things like

01:13:36,719 --> 01:13:41,880
you can put your configuration in the

01:13:40,050 --> 01:13:44,310
cloud and stuff like that as far as I

01:13:41,880 --> 01:13:46,920
know and the second one I think the

01:13:44,310 --> 01:13:53,070
second one would be how do you control

01:13:46,920 --> 01:13:56,880
the costs of your lambdas I mean a good

01:13:53,070 --> 01:13:59,610
question cost control like right now

01:13:56,880 --> 01:14:02,820
we're just monitoring things you can use

01:13:59,610 --> 01:14:04,619
for example the API gateway to do to

01:14:02,820 --> 01:14:06,300
handle some of this for your HTTP

01:14:04,619 --> 01:14:09,179
endpoints so you can you can give

01:14:06,300 --> 01:14:12,090
different clients different rate limits

01:14:09,179 --> 01:14:13,619
and things like that but we haven't we

01:14:12,090 --> 01:14:15,060
haven't done this in practice yet we

01:14:13,619 --> 01:14:18,860
don't really have that many users yet

01:14:15,060 --> 01:14:20,040
but we hope to get there okay yeah all

01:14:18,860 --> 01:14:26,270
right

01:14:20,040 --> 01:14:26,270
any more questions yeah all right here I

01:14:27,530 --> 01:14:33,530
would you mind giving a few more details

01:14:30,810 --> 01:14:35,849
about the layers and sharing across

01:14:33,530 --> 01:14:38,250
functions ah yes

01:14:35,849 --> 01:14:41,989
okay maybe I can I can see if I have an

01:14:38,250 --> 01:14:41,989
example that we can show for the layers

01:14:42,170 --> 01:14:47,340
let me see if we can find something so

01:14:46,139 --> 01:14:49,099
essentially what you can do in the

01:14:47,340 --> 01:14:51,510
layers is you canyou can package

01:14:49,099 --> 01:14:53,219
dependencies like libraries in there so

01:14:51,510 --> 01:14:56,670
you don't have to do that every time in

01:14:53,219 --> 01:14:58,619
in your function in your in your lambda

01:14:56,670 --> 01:15:04,310
packages because they can grow pretty

01:14:58,619 --> 01:15:04,310
big so let's see if I have an example

01:15:05,960 --> 01:15:11,030
trying to remember where okay so in in

01:15:08,790 --> 01:15:11,030
our

01:15:11,660 --> 01:15:28,110
and our Excel thing maybe nope now but I

01:15:26,190 --> 01:15:32,190
know that one thing we have done is in

01:15:28,110 --> 01:15:35,239
some cases we've put things like we have

01:15:32,190 --> 01:15:39,600
a bunch of functions that do Python

01:15:35,239 --> 01:15:41,190
transformations of things using pandas

01:15:39,600 --> 01:15:43,320
I'm not sure if you familiar with the

01:15:41,190 --> 01:15:45,420
it's like a numerical library and it's

01:15:43,320 --> 01:15:47,430
fairly big so we put this in in a layer

01:15:45,420 --> 01:15:51,830
so we can just say we have a dependency

01:15:47,430 --> 01:15:54,750
on this layer in our build process and

01:15:51,830 --> 01:15:56,280
and that that means we can reuse that

01:15:54,750 --> 01:15:58,530
across a ton of different functions and

01:15:56,280 --> 01:16:01,080
not have to package that every time so

01:15:58,530 --> 01:16:01,560
it saves us instead of packaging like 40

01:16:01,080 --> 01:16:02,969
megabytes

01:16:01,560 --> 01:16:07,020
every time we deploy a function we

01:16:02,969 --> 01:16:08,610
package maybe 100 kilobytes instead yeah

01:16:07,020 --> 01:16:17,370
so it's faster to deploy faster to

01:16:08,610 --> 01:16:23,190
package yeah not sure if I answered your

01:16:17,370 --> 01:16:25,920
question but yeah and you can also put

01:16:23,190 --> 01:16:28,620
these layers on top of each other so if

01:16:25,920 --> 01:16:30,140
you have multiple dependencies that

01:16:28,620 --> 01:16:33,030
build on each other you can you can

01:16:30,140 --> 01:16:38,070
organize this like I just didn't

01:16:33,030 --> 01:16:40,230
understand see how the layers are placed

01:16:38,070 --> 01:16:42,660
in the hard rain gauge how you create

01:16:40,230 --> 01:16:45,570
like like like I know how to outer

01:16:42,660 --> 01:16:48,150
layers are in say kubernetes or in doc

01:16:45,570 --> 01:16:51,420
docker images but not sure how how is it

01:16:48,150 --> 01:16:53,750
in this case how do you create them you

01:16:51,420 --> 01:16:59,370
mean or deploy them how doors are

01:16:53,750 --> 01:17:01,890
physically or done in the ice if said

01:16:59,370 --> 01:17:05,790
it's a zip file so how do you manage

01:17:01,890 --> 01:17:08,250
this way it's essentially just Union

01:17:05,790 --> 01:17:12,630
file system essentially I think you have

01:17:08,250 --> 01:17:14,219
a set of files and directories and it

01:17:12,630 --> 01:17:15,510
will put them all together into one big

01:17:14,219 --> 01:17:17,280
thing I'm not sure what happens if like

01:17:15,510 --> 01:17:19,170
one layer tries to override fill up

01:17:17,280 --> 01:17:22,020
files from the one below I assume it

01:17:19,170 --> 01:17:23,829
will replace them but they haven't tried

01:17:22,020 --> 01:17:28,539
that we only have one layer in our

01:17:23,829 --> 01:17:32,050
in our examples okay so the platform

01:17:28,539 --> 01:17:35,469
will take care of getting or figuring

01:17:32,050 --> 01:17:37,780
out which layers are common for know we

01:17:35,469 --> 01:17:39,489
have to specifically create layers yeah

01:17:37,780 --> 01:17:41,349
and there are some prepackaged once and

01:17:39,489 --> 01:17:42,999
then there's I think they think about

01:17:41,349 --> 01:17:45,429
having like a marketplace for these kind

01:17:42,999 --> 01:17:47,079
of things and so on but yeah yeah so we

01:17:45,429 --> 01:17:49,989
have to specifically create the layers

01:17:47,079 --> 01:17:51,999
in our case like in one one deployment

01:17:49,989 --> 01:17:54,909
at some point has to create a layer and

01:17:51,999 --> 01:17:56,409
then others can reuse it yeah all right

01:17:54,909 --> 01:17:57,300
okay thank you very much yeah you're

01:17:56,409 --> 01:18:06,879
welcome

01:17:57,300 --> 01:18:07,610
all right since we're done all right

01:18:06,879 --> 01:18:12,870
thank you

01:18:07,610 --> 01:18:12,870

YouTube URL: https://www.youtube.com/watch?v=fdr2N74Yebg


