Title: Berlin Buzzwords 2019: Conor Landry – Reindexing in record time (...) #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Conor Landry talking about "Reindexing in Record Time: How Shopify Indexes Over 800,000 Merchants' Data in Under 24 Hours".

Chances are, if you have shopped online, you’ve searched for the item you want to buy, placed that item in your digital cart, paid for that item, and had it delivered in record time. Each of those steps you took to enjoy your shiny, new item wouldn’t be possible without the help of search engines. Search engines help us find products, help merchants confirm your order, and ship it on time. How do we initially get all this data from slower, traditional databases into fast search engines?

In this talk, Conor Landry focuses on how Shopify indexes product, customer, order, and merchant data from MySQL to ElasticSearch in near real-time and how to reindex over 50 terabytes of data in less than 24 hours and the roadblocks we’ve encountered. Conor describes the challenges faced when handling data which is critical to the livelihoods of small business owners and well-known brands as well as strategies used by Shopify when scaling a search indexation system for the long term.

Read more:
https://2019.berlinbuzzwords.de/19/session/reindexing-record-time-how-shopify-indexes-over-800000-merchants-data-under-24-hours

About Conor Landry
https://2019.berlinbuzzwords.de/users/conor-landry

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,380 --> 00:00:11,280
thank you so as he said I'm Connor I'm

00:00:09,750 --> 00:00:12,990
going to talk about how weary and ex

00:00:11,280 --> 00:00:14,670
elastic

00:00:12,990 --> 00:00:15,299
and how we handle like mapping changes

00:00:14,670 --> 00:00:19,020
for good

00:00:15,299 --> 00:00:21,300
such and yeah so I work in Montreal

00:00:19,020 --> 00:00:23,009
Canada I almost missed this conference

00:00:21,300 --> 00:00:27,079
because my flight was canceled but I

00:00:23,009 --> 00:00:28,949
Here I am so so a little bit about

00:00:27,079 --> 00:00:30,390
Shopify who for those who don't know

00:00:28,949 --> 00:00:32,279
about us Shopify is an e-commerce

00:00:30,390 --> 00:00:34,290
platform that provides everything you

00:00:32,279 --> 00:00:35,010
need to sell online in social media or

00:00:34,290 --> 00:00:37,649
in person

00:00:35,010 --> 00:00:39,210
we have merchants that I saw a lot of

00:00:37,649 --> 00:00:41,450
products with us and bigger brands like

00:00:39,210 --> 00:00:45,120
Kylie cosmetics jeffree star all birds

00:00:41,450 --> 00:00:46,530
fashion OVA and 800,000 other businesses

00:00:45,120 --> 00:00:47,670
collectively they have sold over a

00:00:46,530 --> 00:00:49,440
hundred billion dollars in gross

00:00:47,670 --> 00:00:52,260
merchantville um-- and we have about

00:00:49,440 --> 00:00:54,329
4,000 employees not yeah so this to give

00:00:52,260 --> 00:00:55,020
a better sense of the scale of Shopify

00:00:54,329 --> 00:00:56,430
the last few years

00:00:55,020 --> 00:00:58,860
last Black Friday about ten percent of

00:00:56,430 --> 00:01:00,680
online sales were through us and five

00:00:58,860 --> 00:01:02,610
years ago that was below one percent so

00:01:00,680 --> 00:01:06,210
it's pretty crazy growing that quickly

00:01:02,610 --> 00:01:08,430
I've only been there two years so yeah

00:01:06,210 --> 00:01:09,719
so today we're gonna talk about first of

00:01:08,430 --> 00:01:11,450
all is how we run elasticsearch at

00:01:09,719 --> 00:01:13,619
Shopify for all of our development teams

00:01:11,450 --> 00:01:15,420
then a little bit about our Shopify

00:01:13,619 --> 00:01:17,340
Court's apology which is our main rails

00:01:15,420 --> 00:01:19,709
monolith if you aren't aware Shopify is

00:01:17,340 --> 00:01:24,030
really big in the rails community and we

00:01:19,709 --> 00:01:25,829
love monoliths fight me what R and X

00:01:24,030 --> 00:01:27,509
thing isn't like what it means to us how

00:01:25,829 --> 00:01:29,929
we're and X elasticsearch and then our

00:01:27,509 --> 00:01:32,789
future plans and improvements for this

00:01:29,929 --> 00:01:34,409
so yeah so a lot searchers Shopify I'm

00:01:32,789 --> 00:01:36,719
on a team of six engineers I shall have

00:01:34,409 --> 00:01:38,009
I called the search platform which means

00:01:36,719 --> 00:01:39,929
to be a common name for this team at

00:01:38,009 --> 00:01:42,119
other companies as well we have over 60

00:01:39,929 --> 00:01:45,719
lots of search clusters at Shopify that

00:01:42,119 --> 00:01:47,789
I know of that use our tooling and so

00:01:45,719 --> 00:01:50,009
you run these allow search clusters we

00:01:47,789 --> 00:01:52,979
use a kubernetes controller we built for

00:01:50,009 --> 00:01:54,659
a search you can see all of our clusters

00:01:52,979 --> 00:01:56,189
there most are pretty small but then our

00:01:54,659 --> 00:01:58,770
largest cluster there which is for the

00:01:56,189 --> 00:02:02,520
Shopify core monolith and rails has 50

00:01:58,770 --> 00:02:04,439
billion documents more or less and let's

00:02:02,520 --> 00:02:05,969
look a little bit more at that so this

00:02:04,439 --> 00:02:08,009
allows search cluster is set up in an

00:02:05,969 --> 00:02:09,750
active-active set up and fully

00:02:08,009 --> 00:02:11,910
replicated across two regions for

00:02:09,750 --> 00:02:13,709
resiliency purposes so if we have a DCP

00:02:11,910 --> 00:02:15,810
outage which happened a couple weeks ago

00:02:13,709 --> 00:02:17,520
we can simply failover to that the

00:02:15,810 --> 00:02:20,430
region and hopefully nothing is affected

00:02:17,520 --> 00:02:22,470
each region has about 90 nodes and hits

00:02:20,430 --> 00:02:25,530
about 100,000 queries per minute at peak

00:02:22,470 --> 00:02:27,360
during the day we're out all documents

00:02:25,530 --> 00:02:29,460
or all queries by shop ID and we shard

00:02:27,360 --> 00:02:31,740
all shops by

00:02:29,460 --> 00:02:33,720
so a single shard and elasticsearch has

00:02:31,740 --> 00:02:37,200
all the documents for a single shop

00:02:33,720 --> 00:02:39,720
which is it's okay it could be done

00:02:37,200 --> 00:02:41,790
better and we run everything with one

00:02:39,720 --> 00:02:44,460
replicas per shard and zoned across

00:02:41,790 --> 00:02:47,370
three zones in each region so if we lose

00:02:44,460 --> 00:02:49,470
a region then we're not gonna lose

00:02:47,370 --> 00:02:52,530
access to the documents but they might

00:02:49,470 --> 00:02:53,910
not be replicated so looking at Java

00:02:52,530 --> 00:02:55,290
like Cornell there's hundreds of

00:02:53,910 --> 00:02:57,390
services at Shopify the largest at

00:02:55,290 --> 00:03:00,870
Shopify core it's a pretty conventional

00:02:57,390 --> 00:03:02,880
rails monolith and we shard the monolith

00:03:00,870 --> 00:03:05,700
by shop ID as well which is convenient

00:03:02,880 --> 00:03:06,840
and it's 99% random kubernetes we've

00:03:05,700 --> 00:03:10,080
made the shift from data centers to

00:03:06,840 --> 00:03:11,459
kubernetes in 2017 and the only thing

00:03:10,080 --> 00:03:13,920
left in the data center is is some load

00:03:11,459 --> 00:03:18,239
balancing tooling which as well as for

00:03:13,920 --> 00:03:20,250
resiliency against GCP outages so we run

00:03:18,239 --> 00:03:22,170
those applicators we run in three

00:03:20,250 --> 00:03:23,580
regions one is your central US east and

00:03:22,170 --> 00:03:25,560
then also north america north east one

00:03:23,580 --> 00:03:29,010
it's special because one it's in

00:03:25,560 --> 00:03:30,840
Montreal where I live and also it runs a

00:03:29,010 --> 00:03:34,709
lot a large portion of the Canadian

00:03:30,840 --> 00:03:36,000
legal cannabis sales so we're not gonna

00:03:34,709 --> 00:03:38,370
talk about the Kanto region anymore

00:03:36,000 --> 00:03:40,170
because of legal restrictions but we'll

00:03:38,370 --> 00:03:43,080
talk more about the structure of Shopify

00:03:40,170 --> 00:03:44,850
Court so what you're looking at here is

00:03:43,080 --> 00:03:46,200
pretty much the structure of a single

00:03:44,850 --> 00:03:49,110
region of that shop of a core

00:03:46,200 --> 00:03:51,900
application it powers all of our online

00:03:49,110 --> 00:03:54,299
store functionality at Shopify and a pod

00:03:51,900 --> 00:03:56,489
in this case is not the same as a

00:03:54,299 --> 00:03:57,810
kubernetes pod we come up at the

00:03:56,489 --> 00:03:59,940
terminology pause before we move to

00:03:57,810 --> 00:04:01,950
Cooper Nettie's this is a Shopify pod

00:03:59,940 --> 00:04:04,590
though a choppa fight for us Shopify pod

00:04:01,950 --> 00:04:07,260
for us is the logical grouping of all

00:04:04,590 --> 00:04:10,260
sharded components that's necessary to

00:04:07,260 --> 00:04:12,840
run a Shopify and so this includes stuff

00:04:10,260 --> 00:04:14,760
like my sequel Redis caches and some

00:04:12,840 --> 00:04:15,900
cron utilities and notice that it like

00:04:14,760 --> 00:04:19,410
the web servers and search

00:04:15,900 --> 00:04:22,080
infrastructure is a shared resource web

00:04:19,410 --> 00:04:23,220
because scaling web doesn't fit well at

00:04:22,080 --> 00:04:24,870
this boundary architecture and search

00:04:23,220 --> 00:04:28,200
because it's

00:04:24,870 --> 00:04:29,760
prohibitively expensive to run like we

00:04:28,200 --> 00:04:30,780
have hundreds of pods would be expensive

00:04:29,760 --> 00:04:33,030
to run hundreds of elastic search

00:04:30,780 --> 00:04:35,550
clusters compared to too large lots of

00:04:33,030 --> 00:04:36,960
search clusters so just kind of like

00:04:35,550 --> 00:04:39,150
seeing how a request flows through this

00:04:36,960 --> 00:04:40,919
big rails application it goes through

00:04:39,150 --> 00:04:42,689
these web workers which is then routed

00:04:40,919 --> 00:04:44,279
to the correct pod and then

00:04:42,689 --> 00:04:46,229
korie's we'll go to the my sequel query

00:04:44,279 --> 00:04:50,580
and so like if a shop is on pod - I

00:04:46,229 --> 00:04:51,989
can't query resources on pod 3 and just

00:04:50,580 --> 00:04:54,299
like the reason I'm going over this is

00:04:51,989 --> 00:04:56,249
that it plays a big role in how we re

00:04:54,299 --> 00:04:58,589
index quickly since we can horizontally

00:04:56,249 --> 00:05:01,589
scale these pods and thus horizontally

00:04:58,589 --> 00:05:02,909
scale or relaxation finally then when

00:05:01,589 --> 00:05:06,469
search queries happen they go to the

00:05:02,909 --> 00:05:10,049
shared search resource and back to the

00:05:06,469 --> 00:05:11,789
user that's buying things online so what

00:05:10,049 --> 00:05:14,279
is rien dexing now so we're indexing a

00:05:11,789 --> 00:05:16,289
travel fight for us well first of all we

00:05:14,279 --> 00:05:17,849
have a lot of developers at Shopify and

00:05:16,289 --> 00:05:19,349
we have give them a lot of freedom for

00:05:17,849 --> 00:05:20,909
when they're working with elastic search

00:05:19,349 --> 00:05:24,029
so they can add new fields modify

00:05:20,909 --> 00:05:26,159
analysers delete indices create indices

00:05:24,029 --> 00:05:27,749
or change existing fields and when that

00:05:26,159 --> 00:05:29,219
happens as you know the elastic search

00:05:27,749 --> 00:05:32,399
you have to migrate all that data to a

00:05:29,219 --> 00:05:33,629
new version of the index so especially

00:05:32,399 --> 00:05:35,849
like if you add a new field you get to

00:05:33,629 --> 00:05:37,649
backfill that data and so for indexing

00:05:35,849 --> 00:05:39,029
for us is the process of migrating that

00:05:37,649 --> 00:05:39,959
data from one index version the old

00:05:39,029 --> 00:05:41,519
version to the new version

00:05:39,959 --> 00:05:43,019
putting that new version into production

00:05:41,519 --> 00:05:45,569
and serving that to the merchants and

00:05:43,019 --> 00:05:47,519
buyers this is different from real-time

00:05:45,569 --> 00:05:48,779
indexing which is what happens when like

00:05:47,519 --> 00:05:50,309
you create a product in your online

00:05:48,779 --> 00:05:52,379
store admin and then it gets into expert

00:05:50,309 --> 00:05:54,329
search and then people can search for it

00:05:52,379 --> 00:05:56,610
and buy it this is whenever we actually

00:05:54,329 --> 00:05:57,839
changed the mapping and we want to move

00:05:56,610 --> 00:06:03,419
all that data to a new version on the

00:05:57,839 --> 00:06:05,099
index so I'm going to go through kind of

00:06:03,419 --> 00:06:07,829
like a scenario of me being a product

00:06:05,099 --> 00:06:09,899
developer and adding a new fancy

00:06:07,829 --> 00:06:12,809
analyzer for autocompletes who like a

00:06:09,899 --> 00:06:13,889
product title field and we want to get

00:06:12,809 --> 00:06:17,039
there's a next mapping change into

00:06:13,889 --> 00:06:18,989
production as quickly as possible so we

00:06:17,039 --> 00:06:21,119
kind of start with writing the spy

00:06:18,989 --> 00:06:24,059
commit this slack command we have this

00:06:21,119 --> 00:06:26,699
slack chat bot called spy we have a

00:06:24,059 --> 00:06:28,319
pretty like broad chat ops

00:06:26,699 --> 00:06:29,429
infrastructure there's blog post on our

00:06:28,319 --> 00:06:31,079
engineering blog if you want to read

00:06:29,429 --> 00:06:34,499
more about that but we can do everything

00:06:31,079 --> 00:06:37,519
from like failover is bought blocking

00:06:34,499 --> 00:06:40,769
bots loadshedding traffic statistics

00:06:37,519 --> 00:06:43,439
starting R and X's so here I am typing

00:06:40,769 --> 00:06:45,149
spy es reindex start products and that

00:06:43,439 --> 00:06:48,179
starts a R and X for the product

00:06:45,149 --> 00:06:51,269
strategy we use the term strata strategy

00:06:48,179 --> 00:06:53,249
instead of indices in terms of

00:06:51,269 --> 00:06:53,909
relaxation because we have

00:06:53,249 --> 00:06:55,529
multi-language

00:06:53,909 --> 00:06:56,580
language search so when I reindex the

00:06:55,529 --> 00:06:59,250
product strategy

00:06:56,580 --> 00:07:04,110
indexes products products Japanese index

00:06:59,250 --> 00:07:06,360
and so on so right after that reindex

00:07:04,110 --> 00:07:08,400
starts we have to create a new alias and

00:07:06,360 --> 00:07:11,400
a new and apply templates to that alias

00:07:08,400 --> 00:07:13,379
so the new templates are they template

00:07:11,400 --> 00:07:15,150
for this new mapping is already an

00:07:13,379 --> 00:07:16,379
elastic search which was merged by

00:07:15,150 --> 00:07:20,550
developer when they added that title

00:07:16,379 --> 00:07:23,129
analyzer and the current index product 0

00:07:20,550 --> 00:07:25,050
well first of all we we go through and

00:07:23,129 --> 00:07:26,699
we use like a monotonically increasing

00:07:25,050 --> 00:07:28,409
version number for indices in elastic

00:07:26,699 --> 00:07:30,930
search so those products does 0 which in

00:07:28,409 --> 00:07:32,190
this case is the only index available in

00:07:30,930 --> 00:07:33,840
elastic search right now and the

00:07:32,190 --> 00:07:36,240
products alias points to it and then

00:07:33,840 --> 00:07:37,650
whenever we wants to create this new

00:07:36,240 --> 00:07:39,810
mapping with the new analyzer its

00:07:37,650 --> 00:07:42,900
products dot one and has the products

00:07:39,810 --> 00:07:44,370
new alias so when we index data for the

00:07:42,900 --> 00:07:46,229
reindex we point to the products new

00:07:44,370 --> 00:07:47,849
alias and search results are still

00:07:46,229 --> 00:07:49,800
served from the products alias and real

00:07:47,849 --> 00:07:52,529
time indexing is still done with the

00:07:49,800 --> 00:07:54,449
products alias so now we're ready to

00:07:52,529 --> 00:07:57,090
start moving documents from my sequel to

00:07:54,449 --> 00:07:59,250
elastic search and start denormalizing

00:07:57,090 --> 00:08:02,099
data from my sequel so that they can be

00:07:59,250 --> 00:08:06,330
in this do normalize format for elastic

00:08:02,099 --> 00:08:08,460
search so a Shopify all ring indexing

00:08:06,330 --> 00:08:12,029
loads happen in the monolith and it's

00:08:08,460 --> 00:08:14,009
done through a structure of like a very

00:08:12,029 --> 00:08:15,419
concurrent structure of this coordinator

00:08:14,009 --> 00:08:18,150
job and all these worker jobs that they

00:08:15,419 --> 00:08:20,009
create all of this is done in after job

00:08:18,150 --> 00:08:21,810
in Rex queue so if you know anything

00:08:20,009 --> 00:08:23,250
about rails you're probably familiar

00:08:21,810 --> 00:08:26,580
with active job which is like a

00:08:23,250 --> 00:08:28,680
background job framework for Ruby on

00:08:26,580 --> 00:08:32,010
Rails and rescue is the adapter for that

00:08:28,680 --> 00:08:33,839
to actually work on those jobs so first

00:08:32,010 --> 00:08:36,449
we create a coordinator job per pod

00:08:33,839 --> 00:08:38,339
which remember one of these pods it runs

00:08:36,449 --> 00:08:41,130
off of Redis with the rescue adapter on

00:08:38,339 --> 00:08:42,810
top of Redis we create this coordinator

00:08:41,130 --> 00:08:46,199
job then the coordinator job will create

00:08:42,810 --> 00:08:49,620
a worker job for every single shop so

00:08:46,199 --> 00:08:51,449
overall we're creating 800,000 jobs of

00:08:49,620 --> 00:08:53,160
course those don't all run at the same

00:08:51,449 --> 00:08:55,740
exact time we do throttles so we don't

00:08:53,160 --> 00:08:58,980
take down Shopify every time we index so

00:08:55,740 --> 00:09:01,410
to avoid scheduling too many jobs on too

00:08:58,980 --> 00:09:03,420
many shops at one time first of all we

00:09:01,410 --> 00:09:06,270
limit it to 300 jobs running at one time

00:09:03,420 --> 00:09:08,339
per pod which ends up being about 30,000

00:09:06,270 --> 00:09:09,480
to 50,000 jobs in total and the platform

00:09:08,339 --> 00:09:11,579
at once when we index

00:09:09,480 --> 00:09:13,680
and we also read all data from the my

00:09:11,579 --> 00:09:17,370
sequel read replicas so we don't take

00:09:13,680 --> 00:09:20,730
down the writer likewise each

00:09:17,370 --> 00:09:22,470
coordinator job has a keep track of its

00:09:20,730 --> 00:09:24,570
progress so it will report back to the

00:09:22,470 --> 00:09:26,010
so the worker job will tick that

00:09:24,570 --> 00:09:27,870
progress bar for each coordinator job

00:09:26,010 --> 00:09:29,190
once the rate index is complete we

00:09:27,870 --> 00:09:30,540
should see that a hundred percent of

00:09:29,190 --> 00:09:32,459
these jobs completed we'll see which

00:09:30,540 --> 00:09:34,500
one's failed and we need to retry those

00:09:32,459 --> 00:09:36,750
jobs if we need to and by doing that we

00:09:34,500 --> 00:09:44,610
can set s ellos based on what percent of

00:09:36,750 --> 00:09:46,139
a pawn shops are reindex so likewise

00:09:44,610 --> 00:09:48,990
again if you're familiar with rescue an

00:09:46,139 --> 00:09:51,300
active job this strategy won't work

00:09:48,990 --> 00:09:53,250
because we'll create too many jobs even

00:09:51,300 --> 00:09:55,440
with limiting it to 300 jobs at a time

00:09:53,250 --> 00:09:58,019
and we'll take down rescue and other

00:09:55,440 --> 00:10:00,000
jobs won't get time to run because it's

00:09:58,019 --> 00:10:04,709
not rescues not a scheduling system it's

00:10:00,000 --> 00:10:06,750
just a job working system and so this is

00:10:04,709 --> 00:10:10,560
where the iteration API comes in hand so

00:10:06,750 --> 00:10:12,300
the iteration API is a extension for

00:10:10,560 --> 00:10:14,339
active job which makes jobs interrupts a

00:10:12,300 --> 00:10:16,769
bulletin resumable and saves progress

00:10:14,339 --> 00:10:18,329
that all the jobs have made so in this

00:10:16,769 --> 00:10:20,220
case for us like a job can run for a

00:10:18,329 --> 00:10:21,839
maximum of 120 seconds per iteration

00:10:20,220 --> 00:10:24,000
at which point they're paused the

00:10:21,839 --> 00:10:25,050
current cursor position Andrian queued

00:10:24,000 --> 00:10:27,120
and we'll continue with the next time

00:10:25,050 --> 00:10:29,089
quanta they get and the way this looks

00:10:27,120 --> 00:10:31,440
when you're writing a job in Shopify is

00:10:29,089 --> 00:10:32,970
useful the job in the two sections the

00:10:31,440 --> 00:10:34,889
collection you want to process and you

00:10:32,970 --> 00:10:36,839
build an enumerator so in this case we

00:10:34,889 --> 00:10:38,819
pull up our strategy get all documents

00:10:36,839 --> 00:10:41,430
and batches and then you want to apply

00:10:38,819 --> 00:10:45,630
an action which for us is producing the

00:10:41,430 --> 00:10:48,510
build documents to kefka all jobs at

00:10:45,630 --> 00:10:50,100
Shopify use this API and we also share

00:10:48,510 --> 00:10:54,420
the same rescue workers with all other

00:10:50,100 --> 00:10:56,610
jobs at Shopify so we have to make sure

00:10:54,420 --> 00:10:57,600
that we don't take up their their fair

00:10:56,610 --> 00:10:59,279
share of work as well because that

00:10:57,600 --> 00:11:03,089
includes stuff like billing and

00:10:59,279 --> 00:11:05,579
checkouts so looking here this is a time

00:11:03,089 --> 00:11:07,740
series diagram of us running a Rand X

00:11:05,579 --> 00:11:09,480
with all of these worker jobs you can

00:11:07,740 --> 00:11:11,399
see like the darker yellow at near the

00:11:09,480 --> 00:11:13,410
bottom is this reindex starting and

00:11:11,399 --> 00:11:16,589
getting to run in not taking up the rest

00:11:13,410 --> 00:11:18,510
like all of the available reindex

00:11:16,589 --> 00:11:21,870
not all the available rescue worker

00:11:18,510 --> 00:11:23,370
pulled workers at one time and but we

00:11:21,870 --> 00:11:28,410
still get our good fair share of

00:11:23,370 --> 00:11:29,820
we limited that 300 jobs at a time so we

00:11:28,410 --> 00:11:31,710
have these jobs and we've started these

00:11:29,820 --> 00:11:33,030
jobs and they're running and now I want

00:11:31,710 --> 00:11:34,980
to index the data into elasticsearch

00:11:33,030 --> 00:11:37,830
from within that job and he saw hints of

00:11:34,980 --> 00:11:39,000
that with the produce akafuku line but

00:11:37,830 --> 00:11:40,590
now we'll talk a little bit more about

00:11:39,000 --> 00:11:42,330
that because it goes deeper than Kafka

00:11:40,590 --> 00:11:45,450
so we have all the Shopify rescue

00:11:42,330 --> 00:11:48,180
workers in this diagram on each host in

00:11:45,450 --> 00:11:50,910
one GCP region and whenever we produce

00:11:48,180 --> 00:11:53,190
two Kafka initially it actually produces

00:11:50,910 --> 00:11:54,750
to a system five message queue which

00:11:53,190 --> 00:11:56,370
runs on every single host and is shared

00:11:54,750 --> 00:11:59,910
by all of the rescue workers running on

00:11:56,370 --> 00:12:01,620
that host then we then we have a tool

00:11:59,910 --> 00:12:03,600
called calf Cobra just runs and pulls

00:12:01,620 --> 00:12:05,640
from that system 5q produces to the

00:12:03,600 --> 00:12:07,050
Kafka regional which uses calf c'mere

00:12:05,640 --> 00:12:09,510
maker to share it to the calf get

00:12:07,050 --> 00:12:10,920
aggregate cluster which is not within a

00:12:09,510 --> 00:12:12,450
single region it's shared among all

00:12:10,920 --> 00:12:13,800
regions and then our last search

00:12:12,450 --> 00:12:17,210
clusters will read from the Kafka

00:12:13,800 --> 00:12:19,290
aggregate into elastic search so

00:12:17,210 --> 00:12:21,390
thinking a little digging into that a

00:12:19,290 --> 00:12:22,980
little bit more the reason we use this

00:12:21,390 --> 00:12:24,870
calculator gate is it pretty much

00:12:22,980 --> 00:12:26,940
guarantees that we have a full

00:12:24,870 --> 00:12:28,830
replication across both regions since

00:12:26,940 --> 00:12:30,720
Kafka has the concept of like consumer

00:12:28,830 --> 00:12:33,390
offsets and committing offsets if we

00:12:30,720 --> 00:12:34,410
fail to consume a certain batch of

00:12:33,390 --> 00:12:36,540
documents and we're not going to lose

00:12:34,410 --> 00:12:39,030
track of those documents and we'll know

00:12:36,540 --> 00:12:41,190
that like the East region is you know

00:12:39,030 --> 00:12:42,540
it's indexed all these offsets and so as

00:12:41,190 --> 00:12:44,840
a central region and thus we can believe

00:12:42,540 --> 00:12:47,190
that both regions are fully replicated

00:12:44,840 --> 00:12:49,800
and then also the reason we use the

00:12:47,190 --> 00:12:51,690
system five message queue is kind of

00:12:49,800 --> 00:12:54,960
like a resiliency tool that helps the on

00:12:51,690 --> 00:12:56,730
call a little bit so if for example if

00:12:54,960 --> 00:12:59,430
caf-co were to go down which it does

00:12:56,730 --> 00:13:00,720
occasionally then the job of I rescue

00:12:59,430 --> 00:13:03,570
workers can continue working and

00:13:00,720 --> 00:13:04,800
producing documents to kafka however

00:13:03,570 --> 00:13:06,540
they're going to the system five message

00:13:04,800 --> 00:13:08,160
queue so this means that jobs don't get

00:13:06,540 --> 00:13:09,990
backed up and jobs don't get delayed in

00:13:08,160 --> 00:13:11,220
core and they can continue working as if

00:13:09,990 --> 00:13:13,800
they don't know about the incident

00:13:11,220 --> 00:13:15,360
that's happening and then when Kafka

00:13:13,800 --> 00:13:16,680
comes back up it'll just drain the queue

00:13:15,360 --> 00:13:18,870
and we'll continue working

00:13:16,680 --> 00:13:22,230
of course this queue is of limited size

00:13:18,870 --> 00:13:24,570
of the system 5q so if we don't solve

00:13:22,230 --> 00:13:25,890
the incident within like several hours

00:13:24,570 --> 00:13:28,110
then there is the chance that we can

00:13:25,890 --> 00:13:29,520
lose messages in that case for

00:13:28,110 --> 00:13:30,540
re-indexing it's not a big deal we can

00:13:29,520 --> 00:13:33,180
restart the re-index

00:13:30,540 --> 00:13:36,100
and we just lose time but that is a

00:13:33,180 --> 00:13:40,000
drawback of this system

00:13:36,100 --> 00:13:41,890
so I already kind of talked about this

00:13:40,000 --> 00:13:43,360
but so wow this is some fine message

00:13:41,890 --> 00:13:44,170
Hughes won the jobs can be simpler

00:13:43,360 --> 00:13:45,850
because they don't have to know about

00:13:44,170 --> 00:13:50,500
Kafka they're persistent against

00:13:45,850 --> 00:13:53,500
container restarts as well and why we

00:13:50,500 --> 00:13:56,740
use Kafka like I said the resiliency of

00:13:53,500 --> 00:13:58,900
Kafka is nice because once we produced a

00:13:56,740 --> 00:14:00,220
Kafka it is persistent and we know if

00:13:58,900 --> 00:14:02,710
Kafka goes down at least we don't lose

00:14:00,220 --> 00:14:04,930
Kafka data the implicit replication by

00:14:02,710 --> 00:14:07,330
using the Kafka aggregate the offset

00:14:04,930 --> 00:14:09,910
committing x' and of course the order

00:14:07,330 --> 00:14:11,830
guarantees of Kafka because if you write

00:14:09,910 --> 00:14:13,660
to a single partition and Kafka topic

00:14:11,830 --> 00:14:15,940
then you're guaranteed to have the order

00:14:13,660 --> 00:14:17,520
there so we write all the documents for

00:14:15,940 --> 00:14:19,930
a single shop into one partition and

00:14:17,520 --> 00:14:22,810
like use a modulus to write to each

00:14:19,930 --> 00:14:24,040
partition in that cafetalk and then we

00:14:22,810 --> 00:14:28,840
can guarantee that the order is there

00:14:24,040 --> 00:14:31,990
and we're not overwriting documents so

00:14:28,840 --> 00:14:34,210
bringing it together then looking at

00:14:31,990 --> 00:14:35,590
like the whole view of shopify we can

00:14:34,210 --> 00:14:37,030
see like the u.s. east region you a

00:14:35,590 --> 00:14:39,280
central region in our last historic

00:14:37,030 --> 00:14:40,840
regions the Kafka regional in each

00:14:39,280 --> 00:14:43,300
region the Kafka aggregate in our

00:14:40,840 --> 00:14:45,430
elasticsearch clusters and the flow then

00:14:43,300 --> 00:14:49,960
from each of those charted pods to the

00:14:45,430 --> 00:14:52,390
regional Kafka and elasticsearch so

00:14:49,960 --> 00:14:54,160
after everything is R and X a way

00:14:52,390 --> 00:14:55,480
started that reindex the documents were

00:14:54,160 --> 00:14:58,870
produced at Kafka they've been read from

00:14:55,480 --> 00:15:01,390
our consumer index we verified that

00:14:58,870 --> 00:15:02,560
everything completed successfully we

00:15:01,390 --> 00:15:05,200
need to put this new index under

00:15:02,560 --> 00:15:07,420
production it's pretty simple we just

00:15:05,200 --> 00:15:10,180
changed the alias name and now the

00:15:07,420 --> 00:15:12,880
products index is products dot one we

00:15:10,180 --> 00:15:16,420
don't do any cash warming which is kind

00:15:12,880 --> 00:15:18,130
of bad it means that there is like a

00:15:16,420 --> 00:15:19,600
small latency spike for merchants and

00:15:18,130 --> 00:15:22,360
users whenever we switch the index into

00:15:19,600 --> 00:15:24,340
production but that latency spike since

00:15:22,360 --> 00:15:25,690
our query rate is high enough goes down

00:15:24,340 --> 00:15:29,920
pretty quickly so maybe a couple

00:15:25,690 --> 00:15:32,710
thousand queries hit a bad get a bad

00:15:29,920 --> 00:15:35,140
query at times out but that's something

00:15:32,710 --> 00:15:38,170
and our future plans to improve on so

00:15:35,140 --> 00:15:39,550
now speaking of our future plans so the

00:15:38,170 --> 00:15:41,380
biggest problem that we have is my

00:15:39,550 --> 00:15:43,000
sequel query optimization going from

00:15:41,380 --> 00:15:45,900
this normalized format to a denormalized

00:15:43,000 --> 00:15:47,970
format in elasticsearch is really hard

00:15:45,900 --> 00:15:51,809
because

00:15:47,970 --> 00:15:53,220
like like you have to have a bunch of

00:15:51,809 --> 00:15:54,599
different associations with your queries

00:15:53,220 --> 00:15:56,759
like for our orders table for example

00:15:54,599 --> 00:15:59,309
there's close to 10 associations that to

00:15:56,759 --> 00:16:02,069
be loaded up just to build one document

00:15:59,309 --> 00:16:04,949
and we have a maximum query time of 25

00:16:02,069 --> 00:16:06,720
seconds in our core application so if

00:16:04,949 --> 00:16:08,039
one of those queries times out then that

00:16:06,720 --> 00:16:11,069
job has to retry and hopefully it

00:16:08,039 --> 00:16:13,229
doesn't fail the next time we do have

00:16:11,069 --> 00:16:15,329
some resiliency against those timeouts

00:16:13,229 --> 00:16:16,919
by decreasing the batch size that we

00:16:15,329 --> 00:16:20,369
request from elasticsearch and then my

00:16:16,919 --> 00:16:22,049
sequel query so when a query these order

00:16:20,369 --> 00:16:24,029
table and 10 associations on the order

00:16:22,049 --> 00:16:25,709
table if it fails with the batch size of

00:16:24,029 --> 00:16:29,759
a thousand then we'll try again with 500

00:16:25,709 --> 00:16:31,229
and maybe it'll work then but it's tough

00:16:29,759 --> 00:16:32,459
and then the second aspect of that is

00:16:31,229 --> 00:16:33,899
like pushing our products teams and

00:16:32,459 --> 00:16:35,819
educating them to think about the impact

00:16:33,899 --> 00:16:36,959
of these complex queries because we're

00:16:35,819 --> 00:16:38,099
not the ones writing these queries as

00:16:36,959 --> 00:16:40,229
well we don't know when someone

00:16:38,099 --> 00:16:43,619
necessarily adds a new index or adds a

00:16:40,229 --> 00:16:46,109
new field and so it's part of our

00:16:43,619 --> 00:16:48,119
efforts who educate product developers

00:16:46,109 --> 00:16:49,859
on how to write efficient my sequel

00:16:48,119 --> 00:16:53,639
queries and that helps with relaxation

00:16:49,859 --> 00:16:55,799
as well the other thing is we want to

00:16:53,639 --> 00:16:57,359
add top level index concurrency so like

00:16:55,799 --> 00:17:01,319
I said we were index with one job per

00:16:57,359 --> 00:17:03,389
shop there's shops that have well so

00:17:01,319 --> 00:17:05,069
they're shops that are legitimate which

00:17:03,389 --> 00:17:07,409
have for example like this shop right

00:17:05,069 --> 00:17:09,000
here has 35 million documents in

00:17:07,409 --> 00:17:11,189
elasticsearch and that's 35 million

00:17:09,000 --> 00:17:12,839
orders for example but there's also

00:17:11,189 --> 00:17:15,449
shops like if you've ever been scrolling

00:17:12,839 --> 00:17:17,370
on facebook and you see like the like

00:17:15,449 --> 00:17:17,970
get your last name on a t-shirt ads on

00:17:17,370 --> 00:17:19,649
facebook

00:17:17,970 --> 00:17:21,449
those are Shopify stores sometimes and

00:17:19,649 --> 00:17:23,789
those shops make a permutation of every

00:17:21,449 --> 00:17:25,470
single last name possible and they store

00:17:23,789 --> 00:17:27,600
all of those documents on Shopify and

00:17:25,470 --> 00:17:30,720
they get into elasticsearch so one shop

00:17:27,600 --> 00:17:33,899
could have five billion products that

00:17:30,720 --> 00:17:35,789
are just t-shirts in rejecting those

00:17:33,899 --> 00:17:37,710
sucks so like here in this time series

00:17:35,789 --> 00:17:39,210
diagram you can see this is the worker

00:17:37,710 --> 00:17:40,649
jobs and most of them complete within

00:17:39,210 --> 00:17:42,210
the first like couple hours of where in

00:17:40,649 --> 00:17:44,370
the three index starts but then there's

00:17:42,210 --> 00:17:46,559
always three or four shops that take up

00:17:44,370 --> 00:17:48,899
to like 24 hours and so they can

00:17:46,559 --> 00:17:51,600
complete so our long-term vision with

00:17:48,899 --> 00:17:53,730
this is to essentially split each of

00:17:51,600 --> 00:17:55,679
these shops into multiple worker jobs

00:17:53,730 --> 00:17:58,019
and that way we can have like a hundred

00:17:55,679 --> 00:18:00,619
workers Rhian dexing with the shops that

00:17:58,019 --> 00:18:00,619
are a problem

00:18:00,850 --> 00:18:06,350
so yeah so pretty much what we've

00:18:04,549 --> 00:18:07,460
covered is first of all lots of searches

00:18:06,350 --> 00:18:08,960
Shopify and how are you running up for

00:18:07,460 --> 00:18:10,700
developers and our infrastructure

00:18:08,960 --> 00:18:12,740
there's trouble like poor infrastructure

00:18:10,700 --> 00:18:14,799
what reindex thing is for us and how

00:18:12,740 --> 00:18:18,590
developers can add new mappings top

00:18:14,799 --> 00:18:19,999
mappings and analyzers and such how a

00:18:18,590 --> 00:18:22,669
great index at Shopify then after a

00:18:19,999 --> 00:18:33,289
developer does that and our future plans

00:18:22,669 --> 00:18:35,059
and that's everything so they still have

00:18:33,289 --> 00:18:37,309
a time for a couple questions so if you

00:18:35,059 --> 00:18:45,889
have any please raise your hand I see

00:18:37,309 --> 00:18:51,649
you thank you for your awesome

00:18:45,889 --> 00:18:57,740
presentation those three index

00:18:51,649 --> 00:19:02,869
re-indexing jobs sometimes fail for

00:18:57,740 --> 00:19:05,899
reason not known to a developer how long

00:19:02,869 --> 00:19:10,519
do you keep the old indexes you know the

00:19:05,899 --> 00:19:14,149
index dot zero for recovery or failover

00:19:10,519 --> 00:19:17,029
yeah so when we're indexing the old

00:19:14,149 --> 00:19:18,799
index is still live in production so if

00:19:17,029 --> 00:19:19,519
that reindex fails then there's really

00:19:18,799 --> 00:19:22,279
no impact

00:19:19,519 --> 00:19:24,289
we just restart Julie the new index do

00:19:22,279 --> 00:19:26,330
whatever we need to do to fix it once we

00:19:24,289 --> 00:19:28,580
switch the old index into production and

00:19:26,330 --> 00:19:30,799
we stop indexing in real time into the

00:19:28,580 --> 00:19:33,019
old index so we can't go back to the old

00:19:30,799 --> 00:19:35,840
index so once you've switched it that's

00:19:33,019 --> 00:19:37,309
it but before we do switch we verify

00:19:35,840 --> 00:19:41,179
that everything's good we can send like

00:19:37,309 --> 00:19:42,619
shadow traffic to it but yeah we have

00:19:41,179 --> 00:19:45,049
had situations where we switched the

00:19:42,619 --> 00:19:48,409
index before we knew it was verified to

00:19:45,049 --> 00:19:52,690
be properly indexed and that was an

00:19:48,409 --> 00:19:58,730
incident thank you

00:19:52,690 --> 00:20:00,040
anybody else no perfect thank you very

00:19:58,730 --> 00:20:05,540
much for your talk is amazing

00:20:00,040 --> 00:20:05,540

YouTube URL: https://www.youtube.com/watch?v=nPU-MtIkNlQ


