Title: Berlin Buzzwords 2019: William Benton – Band-Aids don’t fix bullet holes #bbuzz
Publication date: 2019-06-19
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	William Benton talking about "Band-Aids don’t fix bullet holes: Repairing the broken promises of ubiquitous machine learning".

Buoyed by expensive industrial research efforts, amazing engineering breakthroughs, and an ever-increasing volume of training data, machine learning techniques have recently seen successes on problems that seemed largely intractable twenty years ago. However, beneath awe-inspiring demos and impressive real-world results, there are cracks in the foundation: ordinary organizations struggle to get real insight or value out of their data and wonder how they’ve missed out on the promised democratization of AI and machine learning.  

This talk will diagnose how we got to this point. You’ll see how the incentives and rhetoric of software and infrastructure vendors have led to inflated expectations. We’ll show how internal political pressures can encourage teams to aim for moonshots instead of realistic and meaningful goals. You’ll learn why contemporary frameworks that have enjoyed prominent successes on perception problems are almost certainly not the best fit for gleaning insights from structured business data. Finally, you’ll see why many of the solutions the industry has offered to real-world machine learning woes are essentially “bandages” that cover deep problems without addressing their causes.

This talk won’t merely offer a diagnosis without a prescription; we’ll conclude by showing that the way to avoid disappointing machine learning initiatives in the future isn’t a patchwork of superficial fixes to help us ignore that we’re solving the wrong problems. Instead, we need to radically simplify the way we approach learning from data by embracing broader definitions of “AI” and “machine learning.” 

Organizations should prioritize results over emulating research labs and practitioners should focus first on fundamental techniques including summaries, sketches, and straightforward models. These techniques are unlikely to attract acclaim on social media or in the technology press, but they are broadly applicable, allow practitioners to realize business value quickly, produce interpretable results, and truly democratize machine intelligence.

Read more:
https://2019.berlinbuzzwords.de/19/session/band-aids-dont-fix-bullet-holes-repairing-broken-promises-ubiquitous-machine-learning

About William Benton:
https://2019.berlinbuzzwords.de/users/william-benton

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,290 --> 00:00:11,320
thank you so much good morning ever

00:00:09,760 --> 00:00:12,820
Benten it's an honor to be here at

00:00:11,320 --> 00:00:14,770
buzzwords and I'm really grateful for

00:00:12,820 --> 00:00:16,270
your time this morning after Isabelle's

00:00:14,770 --> 00:00:18,040
excellent keynote I feel like I should

00:00:16,270 --> 00:00:20,050
point out that I work for Red Hat but I

00:00:18,040 --> 00:00:21,520
don't speak for Red Hat and in

00:00:20,050 --> 00:00:23,020
particular I'm not speaking for Red Hat

00:00:21,520 --> 00:00:25,630
in these talk in this talk these are my

00:00:23,020 --> 00:00:27,070
own opinions the subtitle of this talk

00:00:25,630 --> 00:00:28,720
we have a colon and the title of this

00:00:27,070 --> 00:00:30,280
talk the subtitle is repairing the

00:00:28,720 --> 00:00:32,619
broken promises of Icarus machine

00:00:30,280 --> 00:00:34,270
learning I suspect maybe a couple of you

00:00:32,619 --> 00:00:35,620
have been to a talk or two where someone

00:00:34,270 --> 00:00:38,680
says hey we're sort of not getting what

00:00:35,620 --> 00:00:42,699
we expected out of machine learning it's

00:00:38,680 --> 00:00:44,739
that fair heard that once or twice so

00:00:42,699 --> 00:00:46,540
this is this is also such a talk right

00:00:44,739 --> 00:00:47,949
but I'm been to enough talks that have

00:00:46,540 --> 00:00:50,739
started with that premise that I hope I

00:00:47,949 --> 00:00:51,970
can offer you something new as a bonus

00:00:50,739 --> 00:00:53,650
since this talk is scheduled at the

00:00:51,970 --> 00:00:55,000
beginning of the day I'll be

00:00:53,650 --> 00:00:56,710
recommending some other talks you should

00:00:55,000 --> 00:01:00,070
go to today that are related to themes

00:00:56,710 --> 00:01:01,809
of this talk I feel like I owe it to all

00:01:00,070 --> 00:01:03,339
of you though to explain the title of

00:01:01,809 --> 00:01:05,860
the talk that comes before the colon

00:01:03,339 --> 00:01:08,610
know so I'll do that first quick show of

00:01:05,860 --> 00:01:12,100
hands who here has heard of Taylor Swift

00:01:08,610 --> 00:01:13,840
okay so so a couple of you just a

00:01:12,100 --> 00:01:16,270
level-set Taylor Swift is a relatively

00:01:13,840 --> 00:01:18,700
popular American pop star it's always

00:01:16,270 --> 00:01:21,430
difficult to distill in artists over as

00:01:18,700 --> 00:01:24,220
succinctly but from 10,000 meters the

00:01:21,430 --> 00:01:27,250
Swift deals with three themes the thrill

00:01:24,220 --> 00:01:30,370
of new love the heartbreak of lost love

00:01:27,250 --> 00:01:33,130
and unending rage for those who've

00:01:30,370 --> 00:01:35,140
wronged her a song that's in the latter

00:01:33,130 --> 00:01:37,030
category is called bad blood which deals

00:01:35,140 --> 00:01:39,340
in fairly general terms with an unnamed

00:01:37,030 --> 00:01:41,440
antagonist who has wronged MS Swift at

00:01:39,340 --> 00:01:42,970
some point in the past you might wonder

00:01:41,440 --> 00:01:44,710
what a hit single from four years ago

00:01:42,970 --> 00:01:46,240
has to do with the title of this talk or

00:01:44,710 --> 00:01:46,860
with machine learning but we're getting

00:01:46,240 --> 00:01:49,360
there

00:01:46,860 --> 00:01:50,950
shortly after bad blood was released I

00:01:49,360 --> 00:01:52,690
was at a truly excellent machine

00:01:50,950 --> 00:01:54,670
learning conference the details don't

00:01:52,690 --> 00:01:56,050
matter but as I was waiting for the

00:01:54,670 --> 00:01:59,200
keynotes to start I was looking at a

00:01:56,050 --> 00:02:01,270
slideshow of sponsor logos and bad blood

00:01:59,200 --> 00:02:03,100
was playing in the background the chorus

00:02:01,270 --> 00:02:04,840
includes these lines baby now we've got

00:02:03,100 --> 00:02:06,970
bad blood you know it used to be mad

00:02:04,840 --> 00:02:10,450
love but now we've got problems and I

00:02:06,970 --> 00:02:11,950
don't think we can solve them I was

00:02:10,450 --> 00:02:13,959
paying a lot of attention to the news

00:02:11,950 --> 00:02:15,519
that summer and it occurred to me that

00:02:13,959 --> 00:02:17,709
two of the biggest sponsors were

00:02:15,519 --> 00:02:20,260
mutually engaged in a really acrimonious

00:02:17,709 --> 00:02:22,239
lawsuit and one of the sponsors had

00:02:20,260 --> 00:02:23,560
recently rebuffed an acquisition offer

00:02:22,239 --> 00:02:25,599
from another

00:02:23,560 --> 00:02:27,390
and while every individual I knew at any

00:02:25,599 --> 00:02:30,099
of these companies was a friendly person

00:02:27,390 --> 00:02:31,900
technically excellent the public faces

00:02:30,099 --> 00:02:33,330
of their employers were all seriously at

00:02:31,900 --> 00:02:35,380
odds

00:02:33,330 --> 00:02:37,360
identifying inappropriate background

00:02:35,380 --> 00:02:40,239
music as a minor hobby of mine which

00:02:37,360 --> 00:02:42,130
makes wedding receptions disastrous as

00:02:40,239 --> 00:02:44,050
Miss Swift got to the bridge in which

00:02:42,130 --> 00:02:46,000
she points out that the relief her

00:02:44,050 --> 00:02:48,340
antagonist has offered is grossly

00:02:46,000 --> 00:02:50,950
insufficient band-aids don't fix bullet

00:02:48,340 --> 00:02:52,660
holes I reflected on whether or not this

00:02:50,950 --> 00:02:54,250
was inappropriate background music or

00:02:52,660 --> 00:02:56,049
actually just a perfect ironic

00:02:54,250 --> 00:02:57,850
commentary on the collection of

00:02:56,049 --> 00:03:00,220
technology industry frenemies that were

00:02:57,850 --> 00:03:01,569
on the big screen behind the stage I was

00:03:00,220 --> 00:03:03,760
amused enough to write this down in my

00:03:01,569 --> 00:03:05,110
notes to put in my trip report but the

00:03:03,760 --> 00:03:05,920
conference was starting so I stopped

00:03:05,110 --> 00:03:08,410
thinking about it

00:03:05,920 --> 00:03:09,790
the first keynote presenter opened by

00:03:08,410 --> 00:03:11,110
asking who in the audience was

00:03:09,790 --> 00:03:12,670
disappointed with how they're machine

00:03:11,110 --> 00:03:14,650
learning initiatives were working out I

00:03:12,670 --> 00:03:16,000
was fairly close to the front of the

00:03:14,650 --> 00:03:18,220
room but almost everyone I could see

00:03:16,000 --> 00:03:20,019
raised their hands he acknowledged this

00:03:18,220 --> 00:03:22,239
response and immediately launched into

00:03:20,019 --> 00:03:24,340
an extremely impressive and polished

00:03:22,239 --> 00:03:25,989
demo I'm fictionalizing the details

00:03:24,340 --> 00:03:27,549
because the details aren't important but

00:03:25,989 --> 00:03:29,350
it was one of these things where you

00:03:27,549 --> 00:03:31,120
have a bunch of vector embeddings of a

00:03:29,350 --> 00:03:33,220
bunch of wildly different domains and

00:03:31,120 --> 00:03:34,900
you combine them to make a parent magic

00:03:33,220 --> 00:03:37,239
in a way that doesn't totally make a lot

00:03:34,900 --> 00:03:39,310
of sense like we're gonna have separate

00:03:37,239 --> 00:03:41,680
embeddings for clothing and books and

00:03:39,310 --> 00:03:43,239
pastries and we have a way to combine

00:03:41,680 --> 00:03:44,769
them so that if you tell me what your

00:03:43,239 --> 00:03:46,390
favorite t-shirt is in the name of the

00:03:44,769 --> 00:03:50,200
last book you read I can recommend a

00:03:46,390 --> 00:03:51,819
doughnut to you the actual application

00:03:50,200 --> 00:03:53,950
was sort of dubious but the results were

00:03:51,819 --> 00:03:55,900
impressive and the really amazing part

00:03:53,950 --> 00:03:57,459
was that they had library to support to

00:03:55,900 --> 00:03:59,769
serve these kinds of queries in a very

00:03:57,459 --> 00:04:01,930
small amount of code and so you see the

00:03:59,769 --> 00:04:03,130
presenter concluded we're actually going

00:04:01,930 --> 00:04:05,920
to be able to get value out of machine

00:04:03,130 --> 00:04:08,440
learning after all and it hit me at that

00:04:05,920 --> 00:04:10,660
point being able to easily deploy a

00:04:08,440 --> 00:04:13,000
bunch of predefined models is actually

00:04:10,660 --> 00:04:15,250
not what's keeping me from getting value

00:04:13,000 --> 00:04:16,510
from machine learning it was sort of a

00:04:15,250 --> 00:04:18,760
minor detail that seemed to be

00:04:16,510 --> 00:04:21,070
addressing the wrong problem I again

00:04:18,760 --> 00:04:22,930
thought of miss Swift what if the real

00:04:21,070 --> 00:04:25,539
commentary of bad blood for that morning

00:04:22,930 --> 00:04:27,010
was not about the technology companies

00:04:25,539 --> 00:04:28,090
that appeared to hate each other even

00:04:27,010 --> 00:04:29,889
though all of their employees were

00:04:28,090 --> 00:04:31,780
friendly and excellent people but about

00:04:29,889 --> 00:04:33,220
how we're not really addressing the deep

00:04:31,780 --> 00:04:34,599
problems of using machine learning to

00:04:33,220 --> 00:04:36,159
create business value and we're solving

00:04:34,599 --> 00:04:38,330
the wrong things

00:04:36,159 --> 00:04:39,830
so in the rest of the talk I want to

00:04:38,330 --> 00:04:41,749
look at why machine learning systems are

00:04:39,830 --> 00:04:43,669
hard and see how we're using the wrong

00:04:41,749 --> 00:04:45,319
tools how we've created and are

00:04:43,669 --> 00:04:47,059
responding to the wrong incentives and

00:04:45,319 --> 00:04:50,870
how we're not really solving the deep

00:04:47,059 --> 00:04:52,309
problems we should so practitioners know

00:04:50,870 --> 00:04:54,080
that there's more to machine learning

00:04:52,309 --> 00:04:54,830
than just training a model and launching

00:04:54,080 --> 00:04:56,449
it into production

00:04:54,830 --> 00:04:57,949
there's an entire workflow that

00:04:56,449 --> 00:05:00,110
practitioners follow in order to solve

00:04:57,949 --> 00:05:01,759
real problems and the end result isn't

00:05:00,110 --> 00:05:04,069
just a model or a way to train a model

00:05:01,759 --> 00:05:05,419
it's a whole pipeline we start by

00:05:04,069 --> 00:05:08,449
formalizing the problem we're trying to

00:05:05,419 --> 00:05:10,159
solve we collect label clean and

00:05:08,449 --> 00:05:11,979
structure our data before evaluating

00:05:10,159 --> 00:05:14,029
different approaches to make sense of it

00:05:11,979 --> 00:05:15,800
we ensure that our results are

00:05:14,029 --> 00:05:18,199
defensible that we haven't overfed our

00:05:15,800 --> 00:05:20,029
training set and we're going to deploy

00:05:18,199 --> 00:05:22,129
that model into production as part of an

00:05:20,029 --> 00:05:25,339
application and monitor its performance

00:05:22,129 --> 00:05:27,110
over time now a careful practitioner

00:05:25,339 --> 00:05:29,539
could find a problem at any one of these

00:05:27,110 --> 00:05:31,909
steps and realize that he or she had to

00:05:29,539 --> 00:05:34,039
go back and revisit a decision made in

00:05:31,909 --> 00:05:35,809
an earlier step that's why this workload

00:05:34,039 --> 00:05:37,580
diagram has these arrows going back

00:05:35,809 --> 00:05:39,649
because sometimes these are things we

00:05:37,580 --> 00:05:42,499
need to fix some parts of this workflow

00:05:39,649 --> 00:05:43,610
directly inform or even provide the code

00:05:42,499 --> 00:05:45,770
that we're going to deploy into a

00:05:43,610 --> 00:05:47,749
production system right like our feature

00:05:45,770 --> 00:05:50,240
engineering step is going to sort of

00:05:47,749 --> 00:05:52,909
directly inform a feature extraction

00:05:50,240 --> 00:05:54,560
stage in a production pipeline our model

00:05:52,909 --> 00:05:56,360
training and tuning approach is going to

00:05:54,560 --> 00:05:59,389
represent another stage in a production

00:05:56,360 --> 00:06:00,919
pipeline and so on some of these

00:05:59,389 --> 00:06:02,509
components can we can turn more or less

00:06:00,919 --> 00:06:05,120
directly into code and we can

00:06:02,509 --> 00:06:07,159
incorporate these stages into a pipeline

00:06:05,120 --> 00:06:09,199
that takes raw training data extracts

00:06:07,159 --> 00:06:10,370
features trains a model and then

00:06:09,199 --> 00:06:13,879
ultimately move with that model into

00:06:10,370 --> 00:06:15,709
production we can reuse some components

00:06:13,879 --> 00:06:17,240
this pipeline for scoring like the data

00:06:15,709 --> 00:06:19,279
transformation and feature extraction

00:06:17,240 --> 00:06:20,990
routines for example as well as the

00:06:19,279 --> 00:06:23,240
model we trained in the training part

00:06:20,990 --> 00:06:25,459
with the ultimate goal of putting them

00:06:23,240 --> 00:06:26,899
all into production extracting feature

00:06:25,459 --> 00:06:29,689
vectors from more or less raw data

00:06:26,899 --> 00:06:31,249
making predictions tracking metrics

00:06:29,689 --> 00:06:34,039
about our predictions so we have an idea

00:06:31,249 --> 00:06:35,360
when we're going wrong and archiving the

00:06:34,039 --> 00:06:36,830
data we saw in the predictions we made

00:06:35,360 --> 00:06:38,779
so we can explain ourselves when

00:06:36,830 --> 00:06:41,139
regulators or stakeholders want to know

00:06:38,779 --> 00:06:43,610
why we did what we did in the future

00:06:41,139 --> 00:06:45,229
however we put these pipelines into

00:06:43,610 --> 00:06:46,759
production as parts of larger software

00:06:45,229 --> 00:06:48,600
systems that use machine learning to

00:06:46,759 --> 00:06:50,730
solve real business problems

00:06:48,600 --> 00:06:52,800
these systems are even more complex than

00:06:50,730 --> 00:06:54,750
the pipelines right if you're trying to

00:06:52,800 --> 00:06:57,090
identify which products to recommend to

00:06:54,750 --> 00:06:58,980
when customers are about to checkout in

00:06:57,090 --> 00:07:00,270
an e-commerce site at massive scale if

00:06:58,980 --> 00:07:01,920
you're trying to decide whether or not

00:07:00,270 --> 00:07:03,810
to make a securities trade in a very

00:07:01,920 --> 00:07:05,310
tiny window with someone else's money if

00:07:03,810 --> 00:07:07,470
you're trying to decide whether or not

00:07:05,310 --> 00:07:09,570
to decline an electronic payments

00:07:07,470 --> 00:07:11,160
transaction because it's fraudulent you

00:07:09,570 --> 00:07:13,230
have a complex system and machine

00:07:11,160 --> 00:07:14,700
learning is a part of that system but

00:07:13,230 --> 00:07:17,490
it's a small part that's a part that can

00:07:14,700 --> 00:07:19,650
make every other part more difficult if

00:07:17,490 --> 00:07:20,780
this diagram looks familiar it's because

00:07:19,650 --> 00:07:23,130
I'm borrowing it from a great paper

00:07:20,780 --> 00:07:24,510
hidden technical debt in machine

00:07:23,130 --> 00:07:26,760
learning systems which like Taylor

00:07:24,510 --> 00:07:28,800
Swift's bad blood was released in 2015

00:07:26,760 --> 00:07:30,870
starts with the premise that machine

00:07:28,800 --> 00:07:32,940
learning techniques are easy to develop

00:07:30,870 --> 00:07:35,040
but machine learning systems are hard to

00:07:32,940 --> 00:07:36,780
maintain because they have many moving

00:07:35,040 --> 00:07:39,060
parts all of which are sort of listed in

00:07:36,780 --> 00:07:40,500
boxes on this size slide and those sort

00:07:39,060 --> 00:07:42,570
of relative sizes are supposed to

00:07:40,500 --> 00:07:45,510
indicate maybe how important they are

00:07:42,570 --> 00:07:46,920
that tiny box in the middle is actually

00:07:45,510 --> 00:07:48,510
the machine learning code all of the

00:07:46,920 --> 00:07:50,520
other things ultimately represent more

00:07:48,510 --> 00:07:51,960
engineering effort but the interesting

00:07:50,520 --> 00:07:53,250
thing about the machine learning code in

00:07:51,960 --> 00:07:55,260
the middle is that it can make

00:07:53,250 --> 00:07:57,060
accidental dependencies or other bad

00:07:55,260 --> 00:07:58,560
engineering properties of these systems

00:07:57,060 --> 00:08:00,900
more important in ways that are hard to

00:07:58,560 --> 00:08:02,280
discover for example your future

00:08:00,900 --> 00:08:04,020
extraction routines are probably

00:08:02,280 --> 00:08:06,150
uncomfortably tightly coupled to your

00:08:04,020 --> 00:08:08,100
data ingestion pipeline if the way you

00:08:06,150 --> 00:08:10,350
structure your raw data changes your

00:08:08,100 --> 00:08:13,280
code might break but the more important

00:08:10,350 --> 00:08:14,970
question is will you notice

00:08:13,280 --> 00:08:16,680
practitioners are familiar with the

00:08:14,970 --> 00:08:18,390
phenomenon of data drift where the

00:08:16,680 --> 00:08:20,340
distribution of data that we observe in

00:08:18,390 --> 00:08:21,960
production materially diverges from the

00:08:20,340 --> 00:08:23,760
distribution of data that we trained on

00:08:21,960 --> 00:08:25,740
so the model that's performing

00:08:23,760 --> 00:08:28,440
reasonably well at one point might start

00:08:25,740 --> 00:08:32,580
performing adequately and then poorly a

00:08:28,440 --> 00:08:34,289
little while later the hidden technical

00:08:32,580 --> 00:08:36,090
debt paper points out that data drift is

00:08:34,289 --> 00:08:38,070
actually a special case of a general

00:08:36,090 --> 00:08:40,620
phenomenon of entanglement in machine

00:08:38,070 --> 00:08:42,599
learning systems if we add a future or

00:08:40,620 --> 00:08:44,070
remove a feature we've potentially

00:08:42,599 --> 00:08:46,770
created a change that impacts the whole

00:08:44,070 --> 00:08:48,930
system similarly if we change anything

00:08:46,770 --> 00:08:51,030
about the way we've trained the model

00:08:48,930 --> 00:08:52,740
like hyper parameter settings or even a

00:08:51,030 --> 00:08:54,360
random seed we could potentially

00:08:52,740 --> 00:08:56,550
preserve some other component that was

00:08:54,360 --> 00:08:57,839
accidentally depending on a detail we

00:08:56,550 --> 00:08:59,600
didn't think was important about how we

00:08:57,839 --> 00:09:01,670
were behaving

00:08:59,600 --> 00:09:03,440
another problem the hidden technical

00:09:01,670 --> 00:09:05,840
debt paper identifies is the problem of

00:09:03,440 --> 00:09:07,490
glue code many times these pipeline

00:09:05,840 --> 00:09:08,960
components are developed by teams other

00:09:07,490 --> 00:09:11,690
than the teams who ultimately put them

00:09:08,960 --> 00:09:14,150
into production and thus they're treated

00:09:11,690 --> 00:09:15,920
as black boxes these boxes need to be

00:09:14,150 --> 00:09:17,300
integrated together with so-called glue

00:09:15,920 --> 00:09:19,280
code that orchestrates these stages

00:09:17,300 --> 00:09:20,660
manipulates the output of one stage that

00:09:19,280 --> 00:09:24,050
it's an appropriate input for the next

00:09:20,660 --> 00:09:25,580
stage and so on so we know that the sort

00:09:24,050 --> 00:09:27,110
of actual machine learning parts of this

00:09:25,580 --> 00:09:29,330
the feature extraction and the model

00:09:27,110 --> 00:09:32,060
training requires skill insight and care

00:09:29,330 --> 00:09:34,040
but it's really easy to overlook how

00:09:32,060 --> 00:09:36,080
difficult it can be to write and

00:09:34,040 --> 00:09:37,910
maintain robust glue code I even did it

00:09:36,080 --> 00:09:42,320
in this slide right there's a lot of

00:09:37,910 --> 00:09:43,520
complexity hidden in those arrows as we

00:09:42,320 --> 00:09:45,350
develop more and more complicated

00:09:43,520 --> 00:09:47,330
pipelines maybe we have a bunch of

00:09:45,350 --> 00:09:49,940
branching experimental paths and rarely

00:09:47,330 --> 00:09:51,410
exercised branches not only we have more

00:09:49,940 --> 00:09:52,790
of this difficult in Braille glue code

00:09:51,410 --> 00:09:55,340
but we also have a much more difficult

00:09:52,790 --> 00:09:57,440
system to test I mean if you think about

00:09:55,340 --> 00:09:58,790
just writing a regular program if you

00:09:57,440 --> 00:10:00,200
have a function that gets beyond a

00:09:58,790 --> 00:10:02,450
certain length and has a lot of control

00:10:00,200 --> 00:10:04,580
flow in it right it's difficult to read

00:10:02,450 --> 00:10:06,140
it's nearly impossible to reason about

00:10:04,580 --> 00:10:07,400
and almost everyone has had the

00:10:06,140 --> 00:10:09,830
experience of sort of looking at a

00:10:07,400 --> 00:10:11,600
program scratching your head making a

00:10:09,830 --> 00:10:12,980
cup of coffee going for a walk in coming

00:10:11,600 --> 00:10:15,760
back and still trying to figure out how

00:10:12,980 --> 00:10:18,080
you got to some impossible state right a

00:10:15,760 --> 00:10:19,820
pipeline in which data can flow down one

00:10:18,080 --> 00:10:21,650
of exponentially many paths some of

00:10:19,820 --> 00:10:23,210
which may only be exercised rarely in

00:10:21,650 --> 00:10:25,670
the service of building several

00:10:23,210 --> 00:10:27,230
complementary models is prone to many of

00:10:25,670 --> 00:10:29,270
the same types of challenges failures

00:10:27,230 --> 00:10:30,890
and maintenance headaches except that's

00:10:29,270 --> 00:10:32,480
not just a bunch of ifs and a single

00:10:30,890 --> 00:10:34,250
function it's a bunch of separate

00:10:32,480 --> 00:10:35,450
programs written in different languages

00:10:34,250 --> 00:10:39,950
and probably running on different

00:10:35,450 --> 00:10:41,630
computers in any case this paper has

00:10:39,950 --> 00:10:44,230
been widely read and even more widely

00:10:41,630 --> 00:10:46,880
cited if you haven't read it you should

00:10:44,230 --> 00:10:49,160
take a photo of this QR code with your

00:10:46,880 --> 00:10:50,720
phone and then email the PDF to a device

00:10:49,160 --> 00:10:53,240
that you can stand actually reading a

00:10:50,720 --> 00:10:54,290
paper on I wanted to mention this paper

00:10:53,240 --> 00:10:56,030
because it's a really concise

00:10:54,290 --> 00:10:58,010
presentation of some serious problems

00:10:56,030 --> 00:10:59,900
but I'm not gonna spend a lot more time

00:10:58,010 --> 00:11:01,940
in the stock discussing this paper but I

00:10:59,900 --> 00:11:02,960
wanted to introduce it and spend some

00:11:01,940 --> 00:11:04,190
time on what I think is a really

00:11:02,960 --> 00:11:06,290
interesting problem which is how people

00:11:04,190 --> 00:11:09,290
have taken the message of this paper and

00:11:06,290 --> 00:11:11,270
responded to it I've seen this diagram

00:11:09,290 --> 00:11:12,650
cited in many contexts but the most

00:11:11,270 --> 00:11:13,410
fascinating for me is when someone says

00:11:12,650 --> 00:11:14,579
hey

00:11:13,410 --> 00:11:16,470
remember that hidden technical debt

00:11:14,579 --> 00:11:18,180
paper it argued that these systems are

00:11:16,470 --> 00:11:19,589
complex and have a lot of subtle

00:11:18,180 --> 00:11:21,360
dependencies which is why you need to

00:11:19,589 --> 00:11:24,329
buy our product or adopt our open-source

00:11:21,360 --> 00:11:25,649
project sometimes it would be a point

00:11:24,329 --> 00:11:27,149
product that addressed one of these

00:11:25,649 --> 00:11:28,709
responsibilities other times it would be

00:11:27,149 --> 00:11:31,199
a system that addressed several of these

00:11:28,709 --> 00:11:33,089
responsibilities but in every case this

00:11:31,199 --> 00:11:35,250
frame ignores that one of the key points

00:11:33,089 --> 00:11:36,959
to the paper which is that it's not the

00:11:35,250 --> 00:11:38,610
components themselves necessarily that

00:11:36,959 --> 00:11:40,439
add the complexity but the interactions

00:11:38,610 --> 00:11:42,750
the couplings the entanglement between

00:11:40,439 --> 00:11:44,879
these components so point solutions are

00:11:42,750 --> 00:11:46,439
really important but they don't address

00:11:44,879 --> 00:11:48,569
many of the problems of machine learning

00:11:46,439 --> 00:11:49,920
systems I could have the best model

00:11:48,569 --> 00:11:52,230
serving infrastructure in the world for

00:11:49,920 --> 00:11:53,879
example I would love to have that and I

00:11:52,230 --> 00:11:57,060
still not have a solution for continuous

00:11:53,879 --> 00:11:58,529
data quality monitoring and point

00:11:57,060 --> 00:11:59,910
solutions don't address the problem of

00:11:58,529 --> 00:12:02,220
gluing these components together and

00:11:59,910 --> 00:12:04,290
making the resulting system more robust

00:12:02,220 --> 00:12:07,019
we'll come back to this concrete problem

00:12:04,290 --> 00:12:08,550
later in the talk so we've talked about

00:12:07,019 --> 00:12:09,750
why machine learning is hard but now I

00:12:08,550 --> 00:12:12,120
just want to talk about what we're doing

00:12:09,750 --> 00:12:13,259
wrong I want to start that with the fact

00:12:12,120 --> 00:12:15,389
that many times we choose the wrong

00:12:13,259 --> 00:12:17,040
tools first but let's get some

00:12:15,389 --> 00:12:20,579
background on a particular class of

00:12:17,040 --> 00:12:22,800
problems classical computer vision tasks

00:12:20,579 --> 00:12:25,019
depend on building a database of image

00:12:22,800 --> 00:12:27,720
features colors textures shapes and so

00:12:25,019 --> 00:12:29,639
on and encoding them in a way so that

00:12:27,720 --> 00:12:31,980
the thing we want to identify is

00:12:29,639 --> 00:12:34,199
identifiable so here we have a face with

00:12:31,980 --> 00:12:36,870
heart eyes emoji we want it to be

00:12:34,199 --> 00:12:40,079
identifiable even if we rotate it up to

00:12:36,870 --> 00:12:41,579
a certain point or scale it right so a

00:12:40,079 --> 00:12:44,519
deep learning turns out to be really

00:12:41,579 --> 00:12:46,470
great for problems like this sort of

00:12:44,519 --> 00:12:49,470
computer vision problems speech and

00:12:46,470 --> 00:12:50,850
language processing get really amazing

00:12:49,470 --> 00:12:52,470
accuracy I remember taking computer

00:12:50,850 --> 00:12:54,600
vision in grad school and being shocked

00:12:52,470 --> 00:12:56,040
at the sort of accuracy you could get a

00:12:54,600 --> 00:12:58,610
publishable result in computer vision

00:12:56,040 --> 00:13:00,809
before the deep learning revolution with

00:12:58,610 --> 00:13:02,819
but the promise of deep learning is that

00:13:00,809 --> 00:13:04,649
we can to some extent elide this feature

00:13:02,819 --> 00:13:06,720
engineering work that we do in our

00:13:04,649 --> 00:13:09,059
pipeline because the network is sort of

00:13:06,720 --> 00:13:11,600
forced to learn useful features in order

00:13:09,059 --> 00:13:14,459
to generalize from more or less raw data

00:13:11,600 --> 00:13:16,290
so if you get your objective right the

00:13:14,459 --> 00:13:17,790
claim goes the features will sort of

00:13:16,290 --> 00:13:18,329
fall out of the early layers in the

00:13:17,790 --> 00:13:21,149
network

00:13:18,329 --> 00:13:22,589
put another way at a high level what's

00:13:21,149 --> 00:13:24,000
going on is that we have a neural

00:13:22,589 --> 00:13:25,290
network with a bunch of layers each of

00:13:24,000 --> 00:13:27,329
which is sort of like a separate model

00:13:25,290 --> 00:13:29,819
they're all trained together

00:13:27,329 --> 00:13:31,529
and because each layer can convey less

00:13:29,819 --> 00:13:34,379
information than the previous one

00:13:31,529 --> 00:13:36,389
there's this filtering effect so as a

00:13:34,379 --> 00:13:37,889
consequence the earlier layers wind up

00:13:36,389 --> 00:13:40,079
looking a lot like feature extractors

00:13:37,889 --> 00:13:41,699
and the later layers wind up looking a

00:13:40,079 --> 00:13:43,769
lot like a traditional model that uses

00:13:41,699 --> 00:13:46,110
those features the last layer is where

00:13:43,769 --> 00:13:47,399
you get our prediction which is what

00:13:46,110 --> 00:13:50,549
kind of thing is this that we're looking

00:13:47,399 --> 00:13:52,470
at so deep learning has had enormous

00:13:50,549 --> 00:13:54,239
successes for vision speech recognition

00:13:52,470 --> 00:13:55,529
and language processing and people have

00:13:54,239 --> 00:13:57,329
assumed that these successes would be

00:13:55,529 --> 00:13:59,970
easy to replicate and other problem

00:13:57,329 --> 00:14:01,589
domains but the same properties that

00:13:59,970 --> 00:14:03,989
make it possible to perform perceptual

00:14:01,589 --> 00:14:06,059
tasks impressively well with less time

00:14:03,989 --> 00:14:07,949
spent on feature engineering have really

00:14:06,059 --> 00:14:10,499
bad software engineering consequences

00:14:07,949 --> 00:14:12,480
for machine learning systems done

00:14:10,499 --> 00:14:14,339
properly manual feature engineering

00:14:12,480 --> 00:14:15,929
means that we've thrown away irrelevant

00:14:14,339 --> 00:14:17,910
details and kept the things that

00:14:15,929 --> 00:14:19,769
generalize well and provide some signal

00:14:17,910 --> 00:14:21,540
ideally with some insight about the

00:14:19,769 --> 00:14:23,579
problem space a technique that

00:14:21,540 --> 00:14:26,249
encourages us to provide all available

00:14:23,579 --> 00:14:28,139
information and let it algorithm sort it

00:14:26,249 --> 00:14:29,999
out may make it easier to get results

00:14:28,139 --> 00:14:31,470
quickly but it makes it much more likely

00:14:29,999 --> 00:14:34,860
that our system has some accidental

00:14:31,470 --> 00:14:36,809
dependencies on irrelevant details I'm

00:14:34,860 --> 00:14:38,730
gonna mention another paper now called

00:14:36,809 --> 00:14:40,889
intriguing properties of neural networks

00:14:38,730 --> 00:14:42,209
which had a fascinating result the

00:14:40,889 --> 00:14:44,279
authors showed that neural networks

00:14:42,209 --> 00:14:47,220
don't necessarily maintain a smoothness

00:14:44,279 --> 00:14:49,079
assumption what this means is that small

00:14:47,220 --> 00:14:50,970
changes in the input model should result

00:14:49,079 --> 00:14:53,129
in small changes to the output of the

00:14:50,970 --> 00:14:54,809
model in particular that a small input

00:14:53,129 --> 00:14:57,179
change shouldn't change how we classify

00:14:54,809 --> 00:14:58,949
an image but this doesn't always hold

00:14:57,179 --> 00:15:02,459
true for neural networks and deep

00:14:58,949 --> 00:15:03,929
learning like most widely used classical

00:15:02,459 --> 00:15:05,249
computer vision techniques these deep

00:15:03,929 --> 00:15:07,139
learning models are robust in the face

00:15:05,249 --> 00:15:08,759
of rotation and scaling I can't fool you

00:15:07,139 --> 00:15:11,279
this is still a face with heart eyes

00:15:08,759 --> 00:15:13,079
emoji but one of the very interesting

00:15:11,279 --> 00:15:14,819
consequences of this paper is that

00:15:13,079 --> 00:15:17,189
perturbing an image with almost

00:15:14,819 --> 00:15:19,049
imperceptible noise can cause it to be

00:15:17,189 --> 00:15:21,089
misclassified and even more

00:15:19,049 --> 00:15:22,619
interestingly it's possible to construct

00:15:21,089 --> 00:15:24,290
this noise in such a way that gets you

00:15:22,619 --> 00:15:27,059
misclassified with a particular result

00:15:24,290 --> 00:15:28,110
so in this case we've optimized noise to

00:15:27,059 --> 00:15:30,119
turn this face with heart eyes emoji

00:15:28,110 --> 00:15:32,759
into something that gets classified as a

00:15:30,119 --> 00:15:34,230
stack of pancakes emoji and I hope no

00:15:32,759 --> 00:15:36,899
one in the audience sees that on the

00:15:34,230 --> 00:15:39,350
right as a stack of pancakes doesn't

00:15:36,899 --> 00:15:42,260
look like a stack of pancakes

00:15:39,350 --> 00:15:43,460
so I think I'm gonna make what I hope is

00:15:42,260 --> 00:15:45,260
a non-controversial claim which is that

00:15:43,460 --> 00:15:47,240
in no other neighborhood of software

00:15:45,260 --> 00:15:49,040
engineering would we accept a technique

00:15:47,240 --> 00:15:50,960
that worked with cooperative inputs but

00:15:49,040 --> 00:15:53,600
failed inexplicably in the case of any

00:15:50,960 --> 00:15:55,130
attempted subversion imagine a network

00:15:53,600 --> 00:15:57,020
service that worked really well for

00:15:55,130 --> 00:15:59,420
inputs it expected but failed

00:15:57,020 --> 00:16:00,620
catastrophically on others if you're

00:15:59,420 --> 00:16:02,480
thinking that this sounds like the setup

00:16:00,620 --> 00:16:04,250
for any post mortem analysis of a

00:16:02,480 --> 00:16:07,040
security bug ever I basically agree with

00:16:04,250 --> 00:16:09,020
you I don't want to present this as an

00:16:07,040 --> 00:16:10,370
insurmountable obstacle trying to work

00:16:09,020 --> 00:16:11,930
around the brittleness of deep learning

00:16:10,370 --> 00:16:14,120
networks in the face of adversarial

00:16:11,930 --> 00:16:15,770
examples in many cases by constructing

00:16:14,120 --> 00:16:18,170
and using these examples while training

00:16:15,770 --> 00:16:21,320
the network's is a focus for researchers

00:16:18,170 --> 00:16:22,220
and practitioners in this space but if

00:16:21,320 --> 00:16:23,660
you're thinking of rolling up your

00:16:22,220 --> 00:16:25,460
sleeves and becoming a deep learning

00:16:23,660 --> 00:16:26,990
researcher to address these limitations

00:16:25,460 --> 00:16:27,350
so you can get good results on a new

00:16:26,990 --> 00:16:29,330
problem

00:16:27,350 --> 00:16:31,040
you'll want to consider my next point

00:16:29,330 --> 00:16:32,630
which is that you probably can't afford

00:16:31,040 --> 00:16:35,930
to do a lot of innovation in this space

00:16:32,630 --> 00:16:37,100
for many large problems identifying the

00:16:35,930 --> 00:16:39,050
trade-offs between different hyper

00:16:37,100 --> 00:16:40,640
parameter settings and architectures is

00:16:39,050 --> 00:16:42,110
literally the sort of task that

00:16:40,640 --> 00:16:46,250
governments build supercomputers to

00:16:42,110 --> 00:16:48,350
solve not long ago the paper I'm linking

00:16:46,250 --> 00:16:50,090
here reports hundreds of experiments to

00:16:48,350 --> 00:16:52,370
evaluate network architectures and hyper

00:16:50,090 --> 00:16:53,480
parameters for machine translation you

00:16:52,370 --> 00:16:55,130
might assume someone had done this

00:16:53,480 --> 00:16:56,720
already but they hadn't and the reason

00:16:55,130 --> 00:17:01,850
is actually in the abstract because it

00:16:56,720 --> 00:17:02,900
took 250 thousand hours of GPU time if

00:17:01,850 --> 00:17:05,810
you're doing this kind of work in the

00:17:02,900 --> 00:17:10,400
public cloud an hour of GPU time is one

00:17:05,810 --> 00:17:13,880
to three dollars that quickly turns into

00:17:10,400 --> 00:17:17,780
real money if you're doing this work on

00:17:13,880 --> 00:17:19,189
your own GPU great take 28 28 and a half

00:17:17,780 --> 00:17:22,100
years and divide it by the number of

00:17:19,189 --> 00:17:25,790
GPUs you have and you get to this get to

00:17:22,100 --> 00:17:27,350
this number when I proposed this talk

00:17:25,790 --> 00:17:29,060
I'd originally plan to discuss another

00:17:27,350 --> 00:17:30,590
potential pitfall of deep learning which

00:17:29,060 --> 00:17:32,420
is that classical machine learning

00:17:30,590 --> 00:17:33,740
techniques actually outperform deep

00:17:32,420 --> 00:17:35,450
neural networks on a range of really

00:17:33,740 --> 00:17:37,070
interesting problems but when the

00:17:35,450 --> 00:17:38,600
program came out I saw that szilárd

00:17:37,070 --> 00:17:40,670
would be speaking on gradient boosting

00:17:38,600 --> 00:17:42,020
and I said well I can just assert that

00:17:40,670 --> 00:17:43,430
classical machine learning techniques

00:17:42,020 --> 00:17:45,380
outperform deep neural networks on a

00:17:43,430 --> 00:17:46,970
range of interesting problems and refer

00:17:45,380 --> 00:17:48,530
you to his talk which is this afternoon

00:17:46,970 --> 00:17:51,340
and the poly is I'll tell you a for a

00:17:48,530 --> 00:17:51,340
more complete argument

00:17:51,460 --> 00:17:54,670
but the other thing is really sort of

00:17:53,050 --> 00:17:57,300
philosophical by focusing on deep

00:17:54,670 --> 00:18:01,300
learning we're focusing on the sort of

00:17:57,300 --> 00:18:03,520
smallest and easiest part to get right

00:18:01,300 --> 00:18:07,600
of the whole system right we want to

00:18:03,520 --> 00:18:09,760
move our focus to more of the system so

00:18:07,600 --> 00:18:11,950
why do we use the wrong tools I think

00:18:09,760 --> 00:18:13,780
part of it is that the wrong incentives

00:18:11,950 --> 00:18:16,450
have led us to salt lead led us to solve

00:18:13,780 --> 00:18:18,010
their own problems and the first bad

00:18:16,450 --> 00:18:20,170
incentive is one that applies to us as

00:18:18,010 --> 00:18:22,120
practitioners it's a social one right as

00:18:20,170 --> 00:18:23,890
an industry we tend to over ordered

00:18:22,120 --> 00:18:25,810
complexity and the esoteric it's much

00:18:23,890 --> 00:18:27,160
cooler to say I trained this enormous

00:18:25,810 --> 00:18:28,690
and incomprehensible neural network

00:18:27,160 --> 00:18:35,140
while heating my house with exhaust from

00:18:28,690 --> 00:18:36,670
the compute farm than it is to say hey I

00:18:35,140 --> 00:18:38,860
was able to improve our overall business

00:18:36,670 --> 00:18:40,810
metrics to exceed our goals by employing

00:18:38,860 --> 00:18:42,400
a linear model and fitting the

00:18:40,810 --> 00:18:43,990
parameters takes 12 seconds on my laptop

00:18:42,400 --> 00:18:45,460
and we can explain why the model made

00:18:43,990 --> 00:18:47,380
the decisions that made of stakeholders

00:18:45,460 --> 00:18:49,330
or regulators asked us to justify

00:18:47,380 --> 00:18:51,370
ourselves part of this incentive

00:18:49,330 --> 00:18:53,140
structure is totally salutary right it's

00:18:51,370 --> 00:18:55,450
that you know we're engineers we're

00:18:53,140 --> 00:18:57,730
scientists we want to reward curiosity

00:18:55,450 --> 00:18:58,960
and innovation we're excited about new

00:18:57,730 --> 00:19:00,130
things we want to see how they fit into

00:18:58,960 --> 00:19:02,080
our toolboxes we want to really

00:19:00,130 --> 00:19:03,550
understand what the trade-offs are but

00:19:02,080 --> 00:19:05,830
we can't privilege novelty and

00:19:03,550 --> 00:19:07,600
innovation over elegance especially when

00:19:05,830 --> 00:19:09,400
a simple solution works just as well or

00:19:07,600 --> 00:19:12,880
has far more appealing engineering

00:19:09,400 --> 00:19:14,650
trade-offs the second set of incentives

00:19:12,880 --> 00:19:15,850
I want to discuss are the incentives of

00:19:14,650 --> 00:19:17,980
vendors and even of open source

00:19:15,850 --> 00:19:19,270
communities if you remember that diagram

00:19:17,980 --> 00:19:20,950
from the hidden technical debt paper

00:19:19,270 --> 00:19:23,110
showing how all these components fit

00:19:20,950 --> 00:19:25,180
together and we discussed how focusing

00:19:23,110 --> 00:19:26,560
on machine learning code that tiny box

00:19:25,180 --> 00:19:31,480
in the middle leads to bad engineering

00:19:26,560 --> 00:19:33,940
outcomes I also mentioned earlier in the

00:19:31,480 --> 00:19:35,230
talk that if we focus on just any single

00:19:33,940 --> 00:19:37,870
box we're really solving the wrong

00:19:35,230 --> 00:19:39,550
problem right and a lot of people have

00:19:37,870 --> 00:19:41,440
repurposed this diagram to say yes my

00:19:39,550 --> 00:19:42,940
single box is actually the place where

00:19:41,440 --> 00:19:45,160
we should be focusing our efforts and

00:19:42,940 --> 00:19:47,800
you notice that the focus depends on

00:19:45,160 --> 00:19:50,200
who's talking right it turns out to be

00:19:47,800 --> 00:19:51,820
the areas that benefit particular

00:19:50,200 --> 00:19:53,260
vendors or particular projects and this

00:19:51,820 --> 00:19:54,460
has only gotten worse as machine

00:19:53,260 --> 00:19:56,650
learning has become a hotter and hotter

00:19:54,460 --> 00:19:59,620
area with more organizations putting an

00:19:56,650 --> 00:20:00,970
AI spin on their technologies storage

00:19:59,620 --> 00:20:03,670
vendors would love it if your machine

00:20:00,970 --> 00:20:06,120
learning initiatives were successful but

00:20:03,670 --> 00:20:08,440
they get paid if you buy more capacity

00:20:06,120 --> 00:20:10,630
so their incentives are to increase data

00:20:08,440 --> 00:20:12,310
gravity get you tied into an ecosystem

00:20:10,630 --> 00:20:15,400
and machine learning is one way to do

00:20:12,310 --> 00:20:17,350
this cloud vendors are happy when your

00:20:15,400 --> 00:20:19,480
machine learning initiative succeed but

00:20:17,350 --> 00:20:21,310
they get paid when you consume more of

00:20:19,480 --> 00:20:23,650
their resources so their incentive is to

00:20:21,310 --> 00:20:26,230
make it as easy as possible to consume

00:20:23,650 --> 00:20:28,990
more of their resources specialized

00:20:26,230 --> 00:20:30,670
hardware vendors delighted when your

00:20:28,990 --> 00:20:33,220
machine learning incentives succeed but

00:20:30,670 --> 00:20:35,080
they get paid when a larger swath of the

00:20:33,220 --> 00:20:36,280
compute industry as a whole looks more

00:20:35,080 --> 00:20:37,990
like the high-performance computing

00:20:36,280 --> 00:20:39,280
market so their incentive is to

00:20:37,990 --> 00:20:42,640
encourage techniques that require

00:20:39,280 --> 00:20:44,440
massive computing power so in the case

00:20:42,640 --> 00:20:45,670
of storage vendors and other platform

00:20:44,440 --> 00:20:48,880
vendors in general they're incentivized

00:20:45,670 --> 00:20:52,000
to help you build tooling to make and

00:20:48,880 --> 00:20:53,500
manage increasingly complicated data

00:20:52,000 --> 00:20:55,660
processing pipelines to the extent that

00:20:53,500 --> 00:20:57,790
it makes it more attractive to use a

00:20:55,660 --> 00:21:00,490
platform or a storage solution for that

00:20:57,790 --> 00:21:02,710
data for technical and strategic reasons

00:21:00,490 --> 00:21:05,230
this tooling often tightly couples the

00:21:02,710 --> 00:21:06,970
pipelines you deploy to the details of a

00:21:05,230 --> 00:21:08,410
particular storage solution or platform

00:21:06,970 --> 00:21:10,030
and this kind of entanglement can lead

00:21:08,410 --> 00:21:11,710
to maintenance difficulties and make

00:21:10,030 --> 00:21:16,120
migrating your systems or applications

00:21:11,710 --> 00:21:17,350
close to impossible cloud vendors almost

00:21:16,120 --> 00:21:19,300
every cloud vendor is gonna offer you

00:21:17,350 --> 00:21:20,860
tooling for hyper parameter tuning right

00:21:19,300 --> 00:21:22,390
I need to run a bunch of experiments at

00:21:20,860 --> 00:21:23,950
once I need to figure out which model is

00:21:22,390 --> 00:21:25,780
the best I need to use two hundred and

00:21:23,950 --> 00:21:30,010
fifty thousand hours of GPU time as

00:21:25,780 --> 00:21:31,270
simply as possible right because if you

00:21:30,010 --> 00:21:33,610
need to run all these experiments you'll

00:21:31,270 --> 00:21:35,020
be leasing more capacity from them some

00:21:33,610 --> 00:21:36,400
cloud vendors are even offering this

00:21:35,020 --> 00:21:37,990
sort of automatic machine learning

00:21:36,400 --> 00:21:39,310
tooling right where you evaluate a bunch

00:21:37,990 --> 00:21:42,580
of different models and a bunch of

00:21:39,310 --> 00:21:44,470
different hyper parameters at once for a

00:21:42,580 --> 00:21:45,670
particular problem it's supposed to take

00:21:44,470 --> 00:21:46,930
the human out of the loop right you

00:21:45,670 --> 00:21:48,580
don't even have to think about what kind

00:21:46,930 --> 00:21:50,590
of model you need it's an interesting

00:21:48,580 --> 00:21:52,920
research problem but it also makes it

00:21:50,590 --> 00:21:56,220
really easy to consume a lot more

00:21:52,920 --> 00:21:58,360
compute resources in the public cloud

00:21:56,220 --> 00:21:59,680
interestingly cloud vendors have an

00:21:58,360 --> 00:22:01,270
incentive to make it easier to build

00:21:59,680 --> 00:22:02,500
complete systems right tooling that

00:22:01,270 --> 00:22:04,660
provides the solution in this space

00:22:02,500 --> 00:22:06,070
makes their offering stickier tightly

00:22:04,660 --> 00:22:07,450
coupled applications to a particular

00:22:06,070 --> 00:22:10,180
cloud keeps their customers happy and

00:22:07,450 --> 00:22:11,770
keeps them coming back it's too bad that

00:22:10,180 --> 00:22:13,570
cloud providers don't also have an

00:22:11,770 --> 00:22:15,700
incentive to make my bills easier to

00:22:13,570 --> 00:22:17,690
monitor and understand

00:22:15,700 --> 00:22:19,370
another innovation that's come from

00:22:17,690 --> 00:22:20,990
cloud vendors and and also from a lot of

00:22:19,370 --> 00:22:22,790
startups in this space is the idea of a

00:22:20,990 --> 00:22:24,440
model marketplace a way to sort of

00:22:22,790 --> 00:22:26,630
purchase pre-trained models or access to

00:22:24,440 --> 00:22:28,010
model services to address particular

00:22:26,630 --> 00:22:29,720
application concerns I don't want to

00:22:28,010 --> 00:22:31,130
train an object recognition model I want

00:22:29,720 --> 00:22:33,110
to pull one off the shelf and treat it

00:22:31,130 --> 00:22:34,670
as a black box so there's a lot of

00:22:33,110 --> 00:22:36,110
interesting work in this area too but we

00:22:34,670 --> 00:22:38,570
need to take care that if we use these

00:22:36,110 --> 00:22:40,400
kinds of solutions we aren't just making

00:22:38,570 --> 00:22:42,799
it easier to deploy something that will

00:22:40,400 --> 00:22:44,600
be hard to manage right if we buy a

00:22:42,799 --> 00:22:46,190
model as a black box that makes implicit

00:22:44,600 --> 00:22:47,630
assumptions about features we can't

00:22:46,190 --> 00:22:51,290
really understand or debug what's going

00:22:47,630 --> 00:22:52,970
on with it so specialized hardware

00:22:51,290 --> 00:22:54,530
vendors have done a lot of research and

00:22:52,970 --> 00:22:56,480
engineering to make training complex

00:22:54,530 --> 00:22:58,610
models faster in many cases orders of

00:22:56,480 --> 00:23:00,620
magnitude faster than without special

00:22:58,610 --> 00:23:01,850
hardware support and this has led to a

00:23:00,620 --> 00:23:03,530
tremendous amount of applied research

00:23:01,850 --> 00:23:06,049
that's dramatically expanded the

00:23:03,530 --> 00:23:07,880
applicability of for example GPUs but

00:23:06,049 --> 00:23:09,710
it's also made our systems more complex

00:23:07,880 --> 00:23:11,990
to configure more complex to program and

00:23:09,710 --> 00:23:14,200
more expensive both in terms of hardware

00:23:11,990 --> 00:23:16,250
cost and in terms of environmental cost

00:23:14,200 --> 00:23:17,900
just so you know I'm not throwing stones

00:23:16,250 --> 00:23:19,370
you might even see someone who works for

00:23:17,900 --> 00:23:20,690
a system software vendor arguing that

00:23:19,370 --> 00:23:22,370
Linux containers and container

00:23:20,690 --> 00:23:24,970
orchestration wind up solving a lot of

00:23:22,370 --> 00:23:26,960
problems for machine learning systems

00:23:24,970 --> 00:23:28,490
everyone has a perspective and

00:23:26,960 --> 00:23:30,020
everyone's perspective is informed to

00:23:28,490 --> 00:23:31,820
some extent by their incentives right

00:23:30,020 --> 00:23:34,549
even if they aren't speaking for their

00:23:31,820 --> 00:23:36,110
employer and official capacity the title

00:23:34,549 --> 00:23:37,730
of that talk was Gil scott-heron Taylor

00:23:36,110 --> 00:23:41,090
Swift as a step in that direction I

00:23:37,730 --> 00:23:42,620
don't know which one but it's a step so

00:23:41,090 --> 00:23:44,030
this isn't to say that advances that are

00:23:42,620 --> 00:23:45,980
aligned with vendor incentives aren't

00:23:44,030 --> 00:23:47,450
valuable but far from it like all the

00:23:45,980 --> 00:23:48,740
projects I've mentioned are valuable and

00:23:47,450 --> 00:23:50,630
they represent awesome engineering

00:23:48,740 --> 00:23:52,700
effort I'm just encouraging everyone to

00:23:50,630 --> 00:23:54,950
really consider for any project what

00:23:52,700 --> 00:23:56,150
needs it addresses how the incentives of

00:23:54,950 --> 00:23:57,830
the organization or community they

00:23:56,150 --> 00:23:59,960
created it aligned with the incentives

00:23:57,830 --> 00:24:02,000
of our practitioner community and what

00:23:59,960 --> 00:24:04,760
additional work we'll need to do to each

00:24:02,000 --> 00:24:07,250
fill out a whole picture so we have

00:24:04,760 --> 00:24:08,510
problems right but I don't think it's

00:24:07,250 --> 00:24:09,799
that we can't solve them I think it's

00:24:08,510 --> 00:24:12,200
that we're solving the wrong ones

00:24:09,799 --> 00:24:14,030
the problem isn't that it takes too much

00:24:12,200 --> 00:24:15,799
Python code too many lines of code to

00:24:14,030 --> 00:24:18,290
deploy a deep learning model that has

00:24:15,799 --> 00:24:19,610
bad engineering properties anyway the

00:24:18,290 --> 00:24:20,870
problem isn't that we don't have enough

00:24:19,610 --> 00:24:22,190
canned models to solve interesting

00:24:20,870 --> 00:24:23,690
problems but that won't produce

00:24:22,190 --> 00:24:26,150
defensible results or will lead to a

00:24:23,690 --> 00:24:27,559
pipeline we can't debug and the problem

00:24:26,150 --> 00:24:28,980
isn't necessarily that we don't have a

00:24:27,559 --> 00:24:32,669
way to consume public cloud

00:24:28,980 --> 00:24:33,929
capacity as quickly as possible so we

00:24:32,669 --> 00:24:35,250
spent some time talking about why

00:24:33,929 --> 00:24:37,440
machine learning systems are hard to

00:24:35,250 --> 00:24:39,000
build and we diagnosed some places in

00:24:37,440 --> 00:24:40,470
which the wrong tools are employed by

00:24:39,000 --> 00:24:43,590
people and organizations to solve their

00:24:40,470 --> 00:24:44,880
own problems in the last section I want

00:24:43,590 --> 00:24:46,260
to discuss the problems we should be

00:24:44,880 --> 00:24:47,700
solving as a community and look at how

00:24:46,260 --> 00:24:50,690
we can make these systems easier to

00:24:47,700 --> 00:24:53,730
maintain just as easy as area to build

00:24:50,690 --> 00:24:55,440
so I don't have a lot of text on my

00:24:53,730 --> 00:24:56,730
slides in general but I don't want to

00:24:55,440 --> 00:24:58,860
read this to you so I'm gonna give

00:24:56,730 --> 00:25:06,780
everyone a second to read this quotation

00:24:58,860 --> 00:25:08,400
from my Edgar Dykstra got it okay so

00:25:06,780 --> 00:25:10,950
this infamous quote we see it's almost

00:25:08,400 --> 00:25:14,309
exactly 44 years old today and it's both

00:25:10,950 --> 00:25:15,660
funny and true it's funny because I hope

00:25:14,309 --> 00:25:17,340
we assume that the median pure

00:25:15,660 --> 00:25:19,169
mathematician is a much better

00:25:17,340 --> 00:25:22,710
mathematician than the median programmer

00:25:19,169 --> 00:25:24,390
like like not even close right but I

00:25:22,710 --> 00:25:26,340
think that's largely because better

00:25:24,390 --> 00:25:28,049
abstractions better tools and better

00:25:26,340 --> 00:25:30,090
engineering practices have democratized

00:25:28,049 --> 00:25:31,799
access to programming in a way that we

00:25:30,090 --> 00:25:33,780
have not democratized access to pure

00:25:31,799 --> 00:25:35,340
mathematics there's a wide range of

00:25:33,780 --> 00:25:37,080
people programming and creating valuable

00:25:35,340 --> 00:25:38,820
things with computers because it's

00:25:37,080 --> 00:25:39,960
easier to get started and it's easier to

00:25:38,820 --> 00:25:42,960
figure out where things have gone wrong

00:25:39,960 --> 00:25:44,460
so we can fix them but it's true because

00:25:42,960 --> 00:25:47,160
programming is still actually really

00:25:44,460 --> 00:25:48,450
hard like I wrote my first programs when

00:25:47,160 --> 00:25:49,980
the clash we're still recording good

00:25:48,450 --> 00:25:53,880
albums and I still occasionally get

00:25:49,980 --> 00:25:56,040
confused or stumped Pat shame is Swift I

00:25:53,880 --> 00:25:57,419
think we actually can solve the problems

00:25:56,040 --> 00:25:59,460
of machine learning systems to make them

00:25:57,419 --> 00:26:01,140
more robust and more maintainable and I

00:25:59,460 --> 00:26:02,549
think we can take cues from how we've

00:26:01,140 --> 00:26:05,250
begun to address these problems for

00:26:02,549 --> 00:26:07,290
software development consider how the

00:26:05,250 --> 00:26:08,880
dominant user interface for programming

00:26:07,290 --> 00:26:11,370
has changed since Dijkstra wrote that

00:26:08,880 --> 00:26:12,809
memo from decks of punch cards that ran

00:26:11,370 --> 00:26:15,360
overnight and returned a print out of

00:26:12,809 --> 00:26:16,620
either results or errors to compiled

00:26:15,360 --> 00:26:19,260
languages in which we could build

00:26:16,620 --> 00:26:21,059
programs interactively to Terminal two

00:26:19,260 --> 00:26:23,220
languages that support immediate

00:26:21,059 --> 00:26:25,410
interaction which can be compiled and

00:26:23,220 --> 00:26:29,130
run phrase by phrase as we type with

00:26:25,410 --> 00:26:30,570
live feedback for errors one lesson of

00:26:29,130 --> 00:26:31,919
better and faster feedback in

00:26:30,570 --> 00:26:33,929
programming language user interfaces

00:26:31,919 --> 00:26:35,400
which is not going to be news to anyone

00:26:33,929 --> 00:26:37,470
who's taught a programming class before

00:26:35,400 --> 00:26:39,450
is that you don't need to be perfect in

00:26:37,470 --> 00:26:41,110
advance if you can find out right away

00:26:39,450 --> 00:26:43,600
that something is wrong

00:26:41,110 --> 00:26:45,040
by making the dynamic behavior of our

00:26:43,600 --> 00:26:46,960
machine learning systems easier to

00:26:45,040 --> 00:26:49,000
observe we can make it easier to

00:26:46,960 --> 00:26:51,250
understand which is an important first

00:26:49,000 --> 00:26:54,040
step to making it possible to debug when

00:26:51,250 --> 00:26:55,720
things go wrong this is a general

00:26:54,040 --> 00:26:57,429
distributed systems problem right and

00:26:55,720 --> 00:26:58,870
it's actually something that several

00:26:57,429 --> 00:27:00,520
open source communities are working on

00:26:58,870 --> 00:27:02,530
and it turns out that taking advantage

00:27:00,520 --> 00:27:04,450
of general-purpose observability and

00:27:02,530 --> 00:27:06,100
monitoring functionality turns out to be

00:27:04,450 --> 00:27:08,110
a really great potential advantage of

00:27:06,100 --> 00:27:11,710
putting your machine learning systems on

00:27:08,110 --> 00:27:12,790
kubernetes we should also build better

00:27:11,710 --> 00:27:15,520
abstractions for machine learning

00:27:12,790 --> 00:27:17,020
systems overall another great sort of

00:27:15,520 --> 00:27:18,520
computer science quote is Alan Perlis

00:27:17,020 --> 00:27:20,320
saying that a programming language is

00:27:18,520 --> 00:27:23,020
low-level when its programs require

00:27:20,320 --> 00:27:24,700
attention to the irrelevant if you think

00:27:23,020 --> 00:27:26,380
about programming think about the

00:27:24,700 --> 00:27:28,990
spectrum between machine language

00:27:26,380 --> 00:27:30,850
assembly language low-level languages

00:27:28,990 --> 00:27:32,770
and high-level languages just whatever

00:27:30,850 --> 00:27:34,750
definitions you have for those concepts

00:27:32,770 --> 00:27:37,450
right now if you think of our machine

00:27:34,750 --> 00:27:38,710
learning systems the last thing you

00:27:37,450 --> 00:27:40,299
built with machine learning how many

00:27:38,710 --> 00:27:41,740
your elleven are accidental details did

00:27:40,299 --> 00:27:47,350
you have to think about with the tools

00:27:41,740 --> 00:27:49,030
you used another way we can make things

00:27:47,350 --> 00:27:50,500
simpler is to really focus on feature

00:27:49,030 --> 00:27:54,040
engineering and get rid of useless

00:27:50,500 --> 00:27:55,600
features you probably have an idea that

00:27:54,040 --> 00:27:57,880
most of your features aren't containing

00:27:55,600 --> 00:27:59,980
any information right like if you run

00:27:57,880 --> 00:28:01,840
PCA on your features you could probably

00:27:59,980 --> 00:28:05,559
get rid of 90% of them and your model

00:28:01,840 --> 00:28:07,299
will perform just as well but you also

00:28:05,559 --> 00:28:08,500
ideally have some domain knowledge maybe

00:28:07,299 --> 00:28:10,840
you know that some features are

00:28:08,500 --> 00:28:13,290
correlated with the source maybe you

00:28:10,840 --> 00:28:16,120
know that some are even causally related

00:28:13,290 --> 00:28:17,830
by eliminating some of these before you

00:28:16,120 --> 00:28:19,059
use them to make any decisions elsewhere

00:28:17,830 --> 00:28:22,000
in your system you can make the overall

00:28:19,059 --> 00:28:23,440
system easier to maintain and debug by

00:28:22,000 --> 00:28:25,419
ensuring it's less likely to develop

00:28:23,440 --> 00:28:28,390
accidental dependencies on redundant or

00:28:25,419 --> 00:28:30,100
low information features let's put it

00:28:28,390 --> 00:28:31,270
another way just because a lot of

00:28:30,100 --> 00:28:32,530
algorithms can deal with high

00:28:31,270 --> 00:28:35,470
dimensional data doesn't mean they

00:28:32,530 --> 00:28:38,169
should by applying domain insight to

00:28:35,470 --> 00:28:39,220
winnow down the feature set this also

00:28:38,169 --> 00:28:41,350
gets to sort of what feature engineering

00:28:39,220 --> 00:28:43,120
should be right extracting meaningful

00:28:41,350 --> 00:28:46,480
general patterns from data to use the

00:28:43,120 --> 00:28:48,190
signals for our model we can also make

00:28:46,480 --> 00:28:50,620
things simpler by focusing on simpler

00:28:48,190 --> 00:28:51,880
models instead of using our engineering

00:28:50,620 --> 00:28:53,679
cleverness to figure out how to use

00:28:51,880 --> 00:28:54,760
complex intellectual frameworks to solve

00:28:53,679 --> 00:28:56,950
complex problems

00:28:54,760 --> 00:28:59,440
we should be clever in how we make a

00:28:56,950 --> 00:29:00,820
complex problem simple what's the most

00:28:59,440 --> 00:29:03,190
straightforward technique that will get

00:29:00,820 --> 00:29:04,570
us good results could a little extra

00:29:03,190 --> 00:29:07,240
feature engineering enable us to use a

00:29:04,570 --> 00:29:09,040
simpler model instead of optimizing

00:29:07,240 --> 00:29:10,720
model parameters what if we identified

00:29:09,040 --> 00:29:13,299
ways that a basic summary of our data

00:29:10,720 --> 00:29:14,440
could solve a problem just as well there

00:29:13,299 --> 00:29:17,080
are a lot of cases where a clever

00:29:14,440 --> 00:29:18,220
application of a sketch could solve the

00:29:17,080 --> 00:29:20,799
kind of problem that we might be

00:29:18,220 --> 00:29:22,299
inclined to use model form with the

00:29:20,799 --> 00:29:23,950
bonus that the trade-offs and

00:29:22,299 --> 00:29:25,600
engineering properties of sketches are

00:29:23,950 --> 00:29:27,010
typically easier to understand than

00:29:25,600 --> 00:29:30,490
those of conventional machine learning

00:29:27,010 --> 00:29:31,750
models for an example application of how

00:29:30,490 --> 00:29:32,980
to use sketches to solve a machine

00:29:31,750 --> 00:29:34,330
learning problem let me refer you to a

00:29:32,980 --> 00:29:35,980
wonderful talk this afternoon

00:29:34,330 --> 00:29:37,510
so if he Watson will be discussing some

00:29:35,980 --> 00:29:39,400
real-world concerns and implementing

00:29:37,510 --> 00:29:40,900
recommender systems and as part of her

00:29:39,400 --> 00:29:42,610
talk she'll show how the min hash sketch

00:29:40,900 --> 00:29:45,070
a probabilistic data structure for

00:29:42,610 --> 00:29:46,720
identifying set similarity at scale can

00:29:45,070 --> 00:29:47,890
support personalized recommendations

00:29:46,720 --> 00:29:49,419
with much more attractive

00:29:47,890 --> 00:29:51,250
maintainability properties than

00:29:49,419 --> 00:29:53,710
conventional techniques based on matrix

00:29:51,250 --> 00:29:56,010
factorization and if none of that makes

00:29:53,710 --> 00:29:58,059
sense that's all explained in the talk

00:29:56,010 --> 00:29:59,260
to talk about another problem we're

00:29:58,059 --> 00:30:03,390
solving let's look at this hypothetical

00:29:59,260 --> 00:30:03,390
line of code what kind of thing is X

00:30:05,730 --> 00:30:12,640
it's a float what does it mean is it a

00:30:10,840 --> 00:30:14,740
threshold I mean is it a probability is

00:30:12,640 --> 00:30:16,960
it a scaling factor we don't know right

00:30:14,740 --> 00:30:18,940
and not knowing what kind of thing X is

00:30:16,960 --> 00:30:22,630
makes it harder to maintain whatever

00:30:18,940 --> 00:30:24,190
code includes this line you could argue

00:30:22,630 --> 00:30:26,380
that maybe we solved this problem by

00:30:24,190 --> 00:30:28,360
choosing better variable names or

00:30:26,380 --> 00:30:30,220
commenting our code or any number of

00:30:28,360 --> 00:30:32,470
other ad-hoc ways to sort of have

00:30:30,220 --> 00:30:34,450
documentation but these might or may not

00:30:32,470 --> 00:30:36,730
buy us much in the long run write

00:30:34,450 --> 00:30:40,480
comments and even variable names get

00:30:36,730 --> 00:30:42,130
update when your code changes there is

00:30:40,480 --> 00:30:43,809
one kind of code documentation that's

00:30:42,130 --> 00:30:47,650
been proven useful over time though and

00:30:43,809 --> 00:30:49,030
that's the type signature type

00:30:47,650 --> 00:30:50,500
signatures are useful because you can

00:30:49,030 --> 00:30:54,340
automatically check when they get out of

00:30:50,500 --> 00:30:56,679
date now will I hear some of you

00:30:54,340 --> 00:30:59,140
objecting the type systems I know about

00:30:56,679 --> 00:31:00,340
only talk about the shape of the values

00:30:59,140 --> 00:31:03,190
I'm dealing with they don't talk about

00:31:00,340 --> 00:31:05,049
the kinds of things that we're putting

00:31:03,190 --> 00:31:06,190
into those shapes right we just talked

00:31:05,049 --> 00:31:07,360
about floats we don't talk about

00:31:06,190 --> 00:31:09,620
thresholds

00:31:07,360 --> 00:31:11,030
that's a reasonable objection I hope you

00:31:09,620 --> 00:31:12,740
won't still have it at lunchtime because

00:31:11,030 --> 00:31:14,360
I hope right after this talk ends you'll

00:31:12,740 --> 00:31:16,940
head over to the front salon and see

00:31:14,360 --> 00:31:18,380
Eric Aronson Justin scan encode and

00:31:16,940 --> 00:31:21,560
check units that you're using in your

00:31:18,380 --> 00:31:22,850
computations his talk will focus on how

00:31:21,560 --> 00:31:24,800
using unit types will make your data

00:31:22,850 --> 00:31:25,820
engineering work more reliable but I bet

00:31:24,800 --> 00:31:27,080
you'll come out of it with some ideas

00:31:25,820 --> 00:31:30,710
for how to use these types for machine

00:31:27,080 --> 00:31:32,240
learning as well another place where

00:31:30,710 --> 00:31:34,090
advanced type systems that have made

00:31:32,240 --> 00:31:36,500
software engineering better could also

00:31:34,090 --> 00:31:39,890
make machine learning systems better is

00:31:36,500 --> 00:31:41,570
in encoding contracts above behavior for

00:31:39,890 --> 00:31:43,490
a motivating example consider the noun

00:31:41,570 --> 00:31:46,640
list how many Python programmers do we

00:31:43,490 --> 00:31:48,140
have in here many people's something

00:31:46,640 --> 00:31:52,310
other than Python anything other than

00:31:48,140 --> 00:31:54,980
Python not Python okay so the Python

00:31:52,310 --> 00:31:57,620
library uses the word list to mean a

00:31:54,980 --> 00:32:00,380
data structure that has essentially

00:31:57,620 --> 00:32:02,530
constant time access to a bunch of you

00:32:00,380 --> 00:32:05,660
can assume they're contiguous elements

00:32:02,530 --> 00:32:07,730
every other part of the computing

00:32:05,660 --> 00:32:09,590
industry uses lists to mean a linked

00:32:07,730 --> 00:32:11,270
list where if you actually want to

00:32:09,590 --> 00:32:14,690
access something in that linked list you

00:32:11,270 --> 00:32:16,370
have to step through one at a time so if

00:32:14,690 --> 00:32:18,050
you want to access element five of a

00:32:16,370 --> 00:32:19,910
list you have to do five steps in

00:32:18,050 --> 00:32:21,110
general it takes the number of elements

00:32:19,910 --> 00:32:22,580
in the list that's proportional to the

00:32:21,110 --> 00:32:23,960
number of elements in the list taxes

00:32:22,580 --> 00:32:26,240
anything right its linear in the number

00:32:23,960 --> 00:32:29,120
of elements in list so I've seen this

00:32:26,240 --> 00:32:31,330
assumption from you know that lists

00:32:29,120 --> 00:32:34,010
should support constant time access

00:32:31,330 --> 00:32:35,840
break code in the scala language in

00:32:34,010 --> 00:32:38,120
particular or cause it to dramatically

00:32:35,840 --> 00:32:39,620
misbehave in production in many contexts

00:32:38,120 --> 00:32:41,840
and I've seen this everywhere from like

00:32:39,620 --> 00:32:44,120
code developed by super-smart interns

00:32:41,840 --> 00:32:47,090
who are Python programmers writing Scala

00:32:44,120 --> 00:32:49,370
against their will and I've seen code

00:32:47,090 --> 00:32:51,170
developed by Scala experts that depends

00:32:49,370 --> 00:32:53,110
on lists being constant time access and

00:32:51,170 --> 00:32:55,490
they really should know better

00:32:53,110 --> 00:32:57,920
stub structural type systems are a

00:32:55,490 --> 00:32:59,990
family of type systems that enable us to

00:32:57,920 --> 00:33:02,150
reason about the resource usage and

00:32:59,990 --> 00:33:04,400
complexity of code we write and applying

00:33:02,150 --> 00:33:05,270
research in this area to the programming

00:33:04,400 --> 00:33:06,680
languages that we use for machine

00:33:05,270 --> 00:33:08,210
learning systems could make them more

00:33:06,680 --> 00:33:11,420
reliable and robust by alerting us to

00:33:08,210 --> 00:33:12,950
these problems before we put something

00:33:11,420 --> 00:33:16,880
into production or publish a benchmark

00:33:12,950 --> 00:33:18,290
or whatever of course you might not want

00:33:16,880 --> 00:33:20,419
to experiment with type system research

00:33:18,290 --> 00:33:22,909
or features that your favorite program

00:33:20,419 --> 00:33:25,730
just directly support but you can still

00:33:22,909 --> 00:33:27,409
benefit from types even simply talking

00:33:25,730 --> 00:33:30,230
about the shapes of values you care

00:33:27,409 --> 00:33:31,970
about buys you something for those of

00:33:30,230 --> 00:33:34,399
you who are Python programmers I'm sure

00:33:31,970 --> 00:33:35,869
you have a story about how some value

00:33:34,399 --> 00:33:37,369
that you didn't expect to be someplace

00:33:35,869 --> 00:33:38,509
where you didn't expect it to because

00:33:37,369 --> 00:33:39,799
you're programmed to do something

00:33:38,509 --> 00:33:43,759
unexpected in a way that you can't

00:33:39,799 --> 00:33:46,460
explain right and sometimes this turns

00:33:43,759 --> 00:33:49,460
into wasting a week of GPU time or

00:33:46,460 --> 00:33:52,309
overfitting a model or you know any

00:33:49,460 --> 00:33:53,690
number of awful things by leaning on the

00:33:52,309 --> 00:33:55,580
type system that our languages provide

00:33:53,690 --> 00:33:57,320
where we can we can make our systems

00:33:55,580 --> 00:34:01,460
more reliable and easier to understand

00:33:57,320 --> 00:34:02,690
and maintain so we're at the conclusion

00:34:01,460 --> 00:34:04,220
we're in the homestretch thanks so much

00:34:02,690 --> 00:34:05,840
for being here for sticking through the

00:34:04,220 --> 00:34:07,369
talk I'd like to spend a few minutes

00:34:05,840 --> 00:34:09,710
reviewing what we've discussed so far

00:34:07,369 --> 00:34:11,059
today and those of you who are inclined

00:34:09,710 --> 00:34:12,530
to photograph slides there will be

00:34:11,059 --> 00:34:16,879
things you want to photograph in this

00:34:12,530 --> 00:34:19,069
section so have your camera handy first

00:34:16,879 --> 00:34:20,480
we reviewed why machine learning systems

00:34:19,069 --> 00:34:22,190
are easy developed but can be hard to

00:34:20,480 --> 00:34:24,049
maintain and we framed the beginning of

00:34:22,190 --> 00:34:25,639
our discussion around some of the

00:34:24,049 --> 00:34:27,159
concepts in this hidden technical debt

00:34:25,639 --> 00:34:29,839
and machine learning space systems paper

00:34:27,159 --> 00:34:31,940
we also saw how the way that people in

00:34:29,839 --> 00:34:33,559
the community have responded to and used

00:34:31,940 --> 00:34:35,589
this paper is actually sort of

00:34:33,559 --> 00:34:38,510
interesting in itself

00:34:35,589 --> 00:34:39,919
we saw how complicated models like the

00:34:38,510 --> 00:34:41,450
deep neural networks trained by deep

00:34:39,919 --> 00:34:43,730
learning techniques can be powerful for

00:34:41,450 --> 00:34:45,319
vision speech and language problems but

00:34:43,730 --> 00:34:47,419
can have some undesirable engineering

00:34:45,319 --> 00:34:49,460
properties they encourage unnecessary

00:34:47,419 --> 00:34:51,559
feature entanglement their predictions

00:34:49,460 --> 00:34:54,049
can often be subverted by uncooperative

00:34:51,559 --> 00:34:55,429
input and they're so expensive to train

00:34:54,049 --> 00:34:57,139
that only the most well-funded

00:34:55,429 --> 00:34:59,359
organizations can really conduct

00:34:57,139 --> 00:35:03,109
exhaustive experiments about how to use

00:34:59,359 --> 00:35:04,490
them in any domain we considered how the

00:35:03,109 --> 00:35:06,740
incentives of individuals and

00:35:04,490 --> 00:35:08,270
organizations aren't always aligned with

00:35:06,740 --> 00:35:09,589
our incentives as people who want to

00:35:08,270 --> 00:35:14,059
make applications of machine learning

00:35:09,589 --> 00:35:16,609
successful and finally we looked at just

00:35:14,059 --> 00:35:18,140
a few highlights of the last few decades

00:35:16,609 --> 00:35:20,180
of programming in general software

00:35:18,140 --> 00:35:21,500
engineering and to see if we can take

00:35:20,180 --> 00:35:24,290
any of those ideas and use them to make

00:35:21,500 --> 00:35:25,460
our machine learning systems better one

00:35:24,290 --> 00:35:27,410
of the things I love the most about

00:35:25,460 --> 00:35:29,630
buzzwords is that pretty much every slot

00:35:27,410 --> 00:35:32,119
I want to go to three or four of the

00:35:29,630 --> 00:35:34,070
four talks there are always fantastic

00:35:32,119 --> 00:35:35,120
talks that explain

00:35:34,070 --> 00:35:36,650
how to make something about data

00:35:35,120 --> 00:35:38,150
processing better in the real world and

00:35:36,650 --> 00:35:39,500
if I called out every talk you should

00:35:38,150 --> 00:35:41,390
also go to if you care about the things

00:35:39,500 --> 00:35:43,490
I talked about just now I'd still be

00:35:41,390 --> 00:35:44,660
talking for longer than anyone stay in

00:35:43,490 --> 00:35:46,610
the room

00:35:44,660 --> 00:35:48,170
I did mention three talks though that go

00:35:46,610 --> 00:35:49,670
into greater depth on some of the topics

00:35:48,170 --> 00:35:52,580
we discussed in this session I hope

00:35:49,670 --> 00:35:56,510
they're all on your calendar so it

00:35:52,580 --> 00:35:57,830
thanks again I'm I'm here if we have we

00:35:56,510 --> 00:35:59,030
have a little time for questions but

00:35:57,830 --> 00:36:00,560
here's how you can get in touch with me

00:35:59,030 --> 00:36:04,580
if you don't want to ask a question now

00:36:00,560 --> 00:36:05,840
I answer emails or tweets and I'll be

00:36:04,580 --> 00:36:07,190
happy to talk to anyone about any of

00:36:05,840 --> 00:36:11,260
these things at the conference thanks so

00:36:07,190 --> 00:36:11,260

YouTube URL: https://www.youtube.com/watch?v=A6M9TsznUjg


