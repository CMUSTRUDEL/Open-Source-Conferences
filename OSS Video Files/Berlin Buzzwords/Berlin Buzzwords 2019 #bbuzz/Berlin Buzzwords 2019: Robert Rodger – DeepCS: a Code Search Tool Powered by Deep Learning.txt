Title: Berlin Buzzwords 2019: Robert Rodger â€“ DeepCS: a Code Search Tool Powered by Deep Learning
Publication date: 2019-06-27
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	The principal difficulty in implementing a code search engine is the difference in syntax between the natural-language query and computer-language target. Because of this, engines based on traditional IR techniques often have difficulty returning relevant code snippets. 

In this talk we discuss DeepCS, presented by Gu et al at ICSE last year, which uses a deep learning based model to map method definitions and their corresponding textual descriptions to nearby locations in the same feature space. In so doing, this system is also able to map natural-language queries to points in this feature space close to relevant code. This deep learning-based technique performs significantly better than Lucene-based systems, and even out-performs the state-of-the-art system CodeHow.

Read more:
https://2019.berlinbuzzwords.de/19/session/deepcs-code-search-tool-powered-deep-learning

About Robert Rodger:
https://2019.berlinbuzzwords.de/users/robert-rodger

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:11,420 --> 00:00:17,670
said is Robert Roger I'm based in

00:00:14,760 --> 00:00:20,039
Amsterdam and I work as a data scientist

00:00:17,670 --> 00:00:22,050
and added in data science and

00:00:20,039 --> 00:00:25,680
engineering consultancy called ago

00:00:22,050 --> 00:00:27,600
data-driven by the way if any of you are

00:00:25,680 --> 00:00:29,940
looking at moving to the Netherlands and

00:00:27,600 --> 00:00:33,270
you'd like to do any work come talk to

00:00:29,940 --> 00:00:35,760
me during the break anyway last year I

00:00:33,270 --> 00:00:37,890
got up on stage here at Berlin buzzwords

00:00:35,760 --> 00:00:40,230
to talk about an idea for using machine

00:00:37,890 --> 00:00:42,600
learning to build database indexes a

00:00:40,230 --> 00:00:44,730
non-traditional technique for a field

00:00:42,600 --> 00:00:47,399
acquainted with the certainties of

00:00:44,730 --> 00:00:48,600
deterministic algorithms this year I'd

00:00:47,399 --> 00:00:50,519
like again to talk about applying

00:00:48,600 --> 00:00:53,940
machine learning in a novel way this

00:00:50,519 --> 00:00:55,530
time in this search base that's not to

00:00:53,940 --> 00:00:57,390
say that machine learning techniques are

00:00:55,530 --> 00:01:00,239
completely unfamiliar to the search

00:00:57,390 --> 00:01:02,309
community for instance machine learning

00:01:00,239 --> 00:01:05,550
models it's behind every learning to

00:01:02,309 --> 00:01:07,229
rank system which re-rank sets of

00:01:05,550 --> 00:01:09,270
documents based on signals drawn from

00:01:07,229 --> 00:01:12,360
the documents themselves and the query

00:01:09,270 --> 00:01:14,130
that prompted the retrieval machine

00:01:12,360 --> 00:01:16,920
learning classifiers are applied to the

00:01:14,130 --> 00:01:20,460
text of user queries to categorize query

00:01:16,920 --> 00:01:22,320
intent and machine learning systems like

00:01:20,460 --> 00:01:24,360
google's ranked brain can completely

00:01:22,320 --> 00:01:26,670
reformulate in frequently encountered

00:01:24,360 --> 00:01:29,070
user queries in such a way that they

00:01:26,670 --> 00:01:30,990
resemble more common queries for which

00:01:29,070 --> 00:01:34,110
your system is much higher confidence in

00:01:30,990 --> 00:01:36,359
its results now all these techniques

00:01:34,110 --> 00:01:38,490
however surround the system which

00:01:36,359 --> 00:01:40,890
creates the internal representation of

00:01:38,490 --> 00:01:42,899
our documents and typically the way we

00:01:40,890 --> 00:01:45,960
do it now with what essentially boils

00:01:42,899 --> 00:01:47,969
down to inverted indexes is a pretty

00:01:45,960 --> 00:01:50,520
reasonable approach even after applying

00:01:47,969 --> 00:01:53,460
query expansion methods or when we want

00:01:50,520 --> 00:01:55,649
to rewrite we eventually end up wanting

00:01:53,460 --> 00:01:57,450
to check for the presence or absence of

00:01:55,649 --> 00:01:59,939
specific words in our document and

00:01:57,450 --> 00:02:03,689
that's exactly what inverted ingesys do

00:01:59,939 --> 00:02:05,670
well but what happens when the query

00:02:03,689 --> 00:02:08,039
language and the language of the

00:02:05,670 --> 00:02:10,259
document are different for instance if

00:02:08,039 --> 00:02:14,430
our query language had been say English

00:02:10,259 --> 00:02:17,670
and our target language say German we

00:02:14,430 --> 00:02:19,160
could at least try the query using

00:02:17,670 --> 00:02:21,590
Google Translate and

00:02:19,160 --> 00:02:24,110
parsing the result using the usual ir

00:02:21,590 --> 00:02:26,030
tooling but what do you do when the

00:02:24,110 --> 00:02:30,050
query language is English while the

00:02:26,030 --> 00:02:32,240
document language is say Java now this

00:02:30,050 --> 00:02:34,010
is the problem of code search more

00:02:32,240 --> 00:02:36,170
concretely say you're in the situation

00:02:34,010 --> 00:02:37,730
where were working at a company and

00:02:36,170 --> 00:02:40,640
there's this big existing code base

00:02:37,730 --> 00:02:43,310
contributed to by over time dozens or

00:02:40,640 --> 00:02:45,290
more devs and you want to avoid

00:02:43,310 --> 00:02:47,570
rewriting some functionality if there's

00:02:45,290 --> 00:02:49,130
already some method that handles it now

00:02:47,570 --> 00:02:52,310
traditionally what you do is you'd ask

00:02:49,130 --> 00:02:54,320
around by the senior devs or by whomever

00:02:52,310 --> 00:02:57,020
had shoulder the sort of archivist role

00:02:54,320 --> 00:03:00,740
or you refer to some internal code

00:02:57,020 --> 00:03:03,200
documentation source but in stitute

00:03:00,740 --> 00:03:05,780
tional knowledge has a nasty habit of

00:03:03,200 --> 00:03:08,020
going on vacation or getting sick or

00:03:05,780 --> 00:03:11,090
moving on to another company and

00:03:08,020 --> 00:03:13,610
documentation needs to be well first of

00:03:11,090 --> 00:03:16,130
all written and thereafter consistently

00:03:13,610 --> 00:03:17,960
updated so even assuming you've got

00:03:16,130 --> 00:03:20,300
excellent documentation coverage and

00:03:17,960 --> 00:03:21,770
it's up to date that remains the problem

00:03:20,300 --> 00:03:25,730
of searching that documentation with

00:03:21,770 --> 00:03:27,980
traditional IR tools for one IR tools

00:03:25,730 --> 00:03:30,890
tend to focus on word presence and

00:03:27,980 --> 00:03:32,720
ignore word order and these tools will

00:03:30,890 --> 00:03:35,390
then fail us when we for instance want

00:03:32,720 --> 00:03:37,760
the method to queue an event on the run

00:03:35,390 --> 00:03:39,500
sorry to be run on the thread and

00:03:37,760 --> 00:03:42,500
instead gets it through the method that

00:03:39,500 --> 00:03:44,510
will run an event on a thread queue for

00:03:42,500 --> 00:03:46,910
another our tools tend not to be aware

00:03:44,510 --> 00:03:49,580
of semantics synonyms so if we query for

00:03:46,910 --> 00:03:51,530
read in and the documentation says parse

00:03:49,580 --> 00:03:54,770
our search system will miss relevant

00:03:51,530 --> 00:03:57,440
hits and lastly ir tools tend to be

00:03:54,770 --> 00:03:59,810
confused by noise so for instance the

00:03:57,440 --> 00:04:02,090
query get the content of an input stream

00:03:59,810 --> 00:04:04,730
as a string using a specified character

00:04:02,090 --> 00:04:06,200
encoding as relevant token specified and

00:04:04,730 --> 00:04:09,160
character which might lead our search

00:04:06,200 --> 00:04:11,930
system to produce false positives so

00:04:09,160 --> 00:04:13,520
let's say then that a best you can treat

00:04:11,930 --> 00:04:15,920
the doc strings of your codebase is a

00:04:13,520 --> 00:04:18,700
weak search signal how do you then go

00:04:15,920 --> 00:04:21,049
about finding relevant code snippets

00:04:18,700 --> 00:04:23,060
well in principle all that you need to

00:04:21,049 --> 00:04:26,810
know is embedded in the structure and

00:04:23,060 --> 00:04:29,360
syntax of the code itself the problem is

00:04:26,810 --> 00:04:31,460
then finding a way to pair the code with

00:04:29,360 --> 00:04:33,230
a relevant natural language description

00:04:31,460 --> 00:04:34,520
of what the code does

00:04:33,230 --> 00:04:36,890
in such a way that our natural language

00:04:34,520 --> 00:04:38,510
queries can find it and what I want to

00:04:36,890 --> 00:04:41,060
talk about today one very modern

00:04:38,510 --> 00:04:44,570
solution to this problem is called deep

00:04:41,060 --> 00:04:47,600
CS deep CS creates a mathematical

00:04:44,570 --> 00:04:49,670
representation of both the code and of

00:04:47,600 --> 00:04:51,620
natural language where both code

00:04:49,670 --> 00:04:53,330
snippets and English sentences are

00:04:51,620 --> 00:04:55,340
mapped to vectors in the same vector

00:04:53,330 --> 00:04:57,950
space in such a way that for instance

00:04:55,340 --> 00:05:00,110
the vector for the query extract an

00:04:57,950 --> 00:05:01,430
object from an XML file gets mapped

00:05:00,110 --> 00:05:05,420
close to the vector of the code doing

00:05:01,430 --> 00:05:05,930
exactly that now be clear this is not my

00:05:05,420 --> 00:05:07,790
idea

00:05:05,930 --> 00:05:10,400
deep CES was designed by a team

00:05:07,790 --> 00:05:11,870
consisting of gel dong goo and Sung Kim

00:05:10,400 --> 00:05:14,750
from the Hong Kong University of Science

00:05:11,870 --> 00:05:16,520
and Technology and Hongzhang from the

00:05:14,750 --> 00:05:18,950
University of Newcastle in Australia I

00:05:16,520 --> 00:05:20,690
just think this is a really neat idea

00:05:18,950 --> 00:05:22,850
and an interesting example of how

00:05:20,690 --> 00:05:25,550
contemporary machine learning can play a

00:05:22,850 --> 00:05:26,060
role in search what those of you who

00:05:25,550 --> 00:05:29,900
attended

00:05:26,060 --> 00:05:34,430
Jonathas talk yesterday Bal Vespa might

00:05:29,900 --> 00:05:35,750
have heard it too as search 2.0 now

00:05:34,430 --> 00:05:37,460
despite the academic origins of this

00:05:35,750 --> 00:05:39,410
talk I promised a beginner level

00:05:37,460 --> 00:05:42,500
introduction so this is how I propose we

00:05:39,410 --> 00:05:44,540
walk through the idea first we'll talk

00:05:42,500 --> 00:05:47,120
about vector representations of natural

00:05:44,540 --> 00:05:48,740
language and in particular how we can go

00:05:47,120 --> 00:05:51,740
from representations for words to

00:05:48,740 --> 00:05:53,690
representations of documents second

00:05:51,740 --> 00:05:55,220
we'll talk about how the same idea can

00:05:53,690 --> 00:05:58,040
be used to create vector representations

00:05:55,220 --> 00:06:00,130
of code third we'll talk about how we

00:05:58,040 --> 00:06:02,660
can learn these two representations

00:06:00,130 --> 00:06:04,880
simultaneously through what's called a

00:06:02,660 --> 00:06:06,440
joint embedding and what the authors of

00:06:04,880 --> 00:06:08,540
the paper accomplished in a model they

00:06:06,440 --> 00:06:11,620
called the code description and

00:06:08,540 --> 00:06:13,850
embedding neural network or code n N and

00:06:11,620 --> 00:06:16,070
lastly we'll talk about how these

00:06:13,850 --> 00:06:18,920
representations can be first trained and

00:06:16,070 --> 00:06:20,330
then used to perform code search in a

00:06:18,920 --> 00:06:25,400
system the authors of the paper called

00:06:20,330 --> 00:06:25,990
deep CS sound good ok now let's get

00:06:25,400 --> 00:06:27,950
started

00:06:25,990 --> 00:06:30,350
so let's start with the more familiar

00:06:27,950 --> 00:06:32,570
bit transforming a bit of text into a

00:06:30,350 --> 00:06:34,850
vector now as a first approach we could

00:06:32,570 --> 00:06:38,150
start with a large data set of text

00:06:34,850 --> 00:06:40,520
examples and make a dictionary using all

00:06:38,150 --> 00:06:43,190
the unique words we find in a data set

00:06:40,520 --> 00:06:45,140
and then to prepare our documents

00:06:43,190 --> 00:06:47,060
mathematically we make a vector for each

00:06:45,140 --> 00:06:47,380
of our documents where every vector has

00:06:47,060 --> 00:06:48,610
become

00:06:47,380 --> 00:06:51,310
opponent for every word in our

00:06:48,610 --> 00:06:53,080
dictionary say with component for apples

00:06:51,310 --> 00:06:55,390
somewhere at the beginning and likewise

00:06:53,080 --> 00:06:58,150
a component for zombie somewhere in near

00:06:55,390 --> 00:06:59,650
the end and the values that we put in

00:06:58,150 --> 00:07:01,720
these vector components are then equal

00:06:59,650 --> 00:07:03,910
to the number of times the corresponding

00:07:01,720 --> 00:07:06,040
word appears in the document and a

00:07:03,910 --> 00:07:08,380
particular for word doesn't appear its

00:07:06,040 --> 00:07:10,630
component is zero and it's not hard to

00:07:08,380 --> 00:07:12,190
see that by an away the majority of the

00:07:10,630 --> 00:07:15,670
components in our document vector will

00:07:12,190 --> 00:07:17,440
be precisely that zero this type of

00:07:15,670 --> 00:07:19,840
vector is with one with lots of zeros

00:07:17,440 --> 00:07:23,050
what's called a sparse representation

00:07:19,840 --> 00:07:24,580
and it turns out that machine learning

00:07:23,050 --> 00:07:26,410
models when particular deep learning

00:07:24,580 --> 00:07:27,610
models don't perform particularly well

00:07:26,410 --> 00:07:29,860
with high dimensional sparse

00:07:27,610 --> 00:07:31,780
representations and although there are

00:07:29,860 --> 00:07:34,480
smart tricks we can employ to improve

00:07:31,780 --> 00:07:37,030
this representation for instance by

00:07:34,480 --> 00:07:39,010
reducing dimensionality using by

00:07:37,030 --> 00:07:41,650
removing stop words or regularizing

00:07:39,010 --> 00:07:44,380
using tf-idf at the end of the day most

00:07:41,650 --> 00:07:45,520
of the components remain zero and thus

00:07:44,380 --> 00:07:48,250
the representation remains

00:07:45,520 --> 00:07:50,200
unsatisfactory so what we instead want

00:07:48,250 --> 00:07:52,960
is a low dimensional dense

00:07:50,200 --> 00:07:54,220
representation and five years ago we

00:07:52,960 --> 00:07:55,960
almost discovered that has been the

00:07:54,220 --> 00:07:57,550
catalyst for many of the performance

00:07:55,960 --> 00:07:59,950
improvements we've seen recently an NLP

00:07:57,550 --> 00:08:02,290
research and as resulting applications

00:07:59,950 --> 00:08:04,990
it's called word the Veck sure is

00:08:02,290 --> 00:08:06,460
familiar to many of you in the room but

00:08:04,990 --> 00:08:10,030
for those of you who are unfamiliar with

00:08:06,460 --> 00:08:11,680
the details here's how it works we start

00:08:10,030 --> 00:08:14,320
with the I the observation that although

00:08:11,680 --> 00:08:16,300
our data exists in some high dimensional

00:08:14,320 --> 00:08:18,190
space not all of those dimensions are

00:08:16,300 --> 00:08:20,590
actually required for representing the

00:08:18,190 --> 00:08:22,900
data so for instance although we live in

00:08:20,590 --> 00:08:25,210
a three-dimensional world we can

00:08:22,900 --> 00:08:27,430
describe or each of us is at any moment

00:08:25,210 --> 00:08:29,890
using only two numbers namely your

00:08:27,430 --> 00:08:32,380
latitude and longitude to an extremely

00:08:29,890 --> 00:08:34,870
high accuracy relative to the scale of

00:08:32,380 --> 00:08:36,969
the planet now work Tyvek implements

00:08:34,870 --> 00:08:40,479
this idea in the word space it starts

00:08:36,969 --> 00:08:42,219
with the encoder/decoder model which go

00:08:40,479 --> 00:08:44,680
to technique for compressing high

00:08:42,219 --> 00:08:46,630
dimensional data and what this model

00:08:44,680 --> 00:08:48,910
does is it tries to squeeze high

00:08:46,630 --> 00:08:51,490
dimensional data into some low

00:08:48,910 --> 00:08:52,960
dimensional space and then only using

00:08:51,490 --> 00:08:55,390
that low dimensional representation

00:08:52,960 --> 00:08:57,940
predict what the high dimension version

00:08:55,390 --> 00:08:59,290
originally was and then in the

00:08:57,940 --> 00:09:01,240
traditional supervised machine learning

00:08:59,290 --> 00:09:03,580
way we start with two ran

00:09:01,240 --> 00:09:06,339
transformations one for the encoder one

00:09:03,580 --> 00:09:07,930
for the decoder we've seen an example we

00:09:06,339 --> 00:09:10,149
calculate the error on the prediction

00:09:07,930 --> 00:09:12,100
and we use this error to improve each of

00:09:10,149 --> 00:09:13,540
the transformation slightly we then

00:09:12,100 --> 00:09:15,730
iterate on this process over and over

00:09:13,540 --> 00:09:17,560
using lots of data until the error being

00:09:15,730 --> 00:09:18,910
generated finds the minimum and the

00:09:17,560 --> 00:09:20,950
resulting low dimensional

00:09:18,910 --> 00:09:23,020
representations created by the trained

00:09:20,950 --> 00:09:26,020
encoder transformation can then be used

00:09:23,020 --> 00:09:28,060
for downstream tasks that's how an

00:09:26,020 --> 00:09:30,640
encoder decoder model works in general

00:09:28,060 --> 00:09:32,649
but with word Tyvek the high dimensional

00:09:30,640 --> 00:09:34,630
representation of our word is one of

00:09:32,649 --> 00:09:36,250
those vectors which was as big as our

00:09:34,630 --> 00:09:38,709
dictionary with zeros everywhere except

00:09:36,250 --> 00:09:40,540
for a 1 and the component corresponding

00:09:38,709 --> 00:09:42,820
to our word it's called the one hot

00:09:40,540 --> 00:09:45,130
encoding and then this is transformed

00:09:42,820 --> 00:09:47,410
down to a much smaller vector using an

00:09:45,130 --> 00:09:48,700
encoder decoder like process but then

00:09:47,410 --> 00:09:50,230
instead of trying to predict the one

00:09:48,700 --> 00:09:52,480
high encoded vector of the original word

00:09:50,230 --> 00:09:54,520
we predict the one hot encodings of the

00:09:52,480 --> 00:09:57,160
words surrounding the original word in

00:09:54,520 --> 00:09:59,380
the text so for instance if our sentence

00:09:57,160 --> 00:10:01,480
was I hope no one has fallen asleep in

00:09:59,380 --> 00:10:04,450
my talk yet if we want to learn a

00:10:01,480 --> 00:10:06,910
representation of the word asleep we try

00:10:04,450 --> 00:10:10,209
to predict the words has fallen in and

00:10:06,910 --> 00:10:12,550
my and moreover while learning these

00:10:10,209 --> 00:10:14,980
transformations not only do you try to

00:10:12,550 --> 00:10:16,839
minimize the error and predicting one of

00:10:14,980 --> 00:10:18,850
the contexts words but he also tried to

00:10:16,839 --> 00:10:20,709
maximize the error in predicting a

00:10:18,850 --> 00:10:22,170
handful of additional words chosen at

00:10:20,709 --> 00:10:26,140
random from the dictionary like

00:10:22,170 --> 00:10:28,029
trampoline or esophagus and this might

00:10:26,140 --> 00:10:29,350
sound like a crazy idea but the

00:10:28,029 --> 00:10:30,820
resulting small dimensional

00:10:29,350 --> 00:10:32,470
representations of her words were

00:10:30,820 --> 00:10:34,750
responsible for huge leaps in NLP

00:10:32,470 --> 00:10:36,490
machine learning performance not the

00:10:34,750 --> 00:10:40,180
least of which was Google's neural Trank

00:10:36,490 --> 00:10:42,220
machine translation model ok now that

00:10:40,180 --> 00:10:44,020
we've got a small dimensional

00:10:42,220 --> 00:10:46,029
representation of our words how can we

00:10:44,020 --> 00:10:47,470
make a small dimensional representation

00:10:46,029 --> 00:10:49,390
of our document

00:10:47,470 --> 00:10:51,310
now one approach would be simply to take

00:10:49,390 --> 00:10:53,470
the average of all the individual word

00:10:51,310 --> 00:10:55,720
vectors in this text but it turns out

00:10:53,470 --> 00:10:56,890
this doesn't work the best not in the

00:10:55,720 --> 00:10:59,529
least because the ordering of the

00:10:56,890 --> 00:11:01,630
underlying words is ignored so for

00:10:59,529 --> 00:11:03,310
instance the phrase cast into string

00:11:01,630 --> 00:11:06,399
means something very different from the

00:11:03,310 --> 00:11:08,020
phrase cache string to int a more

00:11:06,399 --> 00:11:09,579
sophisticated approach would be to use

00:11:08,020 --> 00:11:11,770
an ordering aware a machine learning

00:11:09,579 --> 00:11:13,420
model and this class of models in the

00:11:11,770 --> 00:11:15,790
deep learning field is called the

00:11:13,420 --> 00:11:18,670
current neural network these are

00:11:15,790 --> 00:11:20,710
stateful models so when an input is

00:11:18,670 --> 00:11:22,660
passed through not only as an output

00:11:20,710 --> 00:11:25,390
generator vector sorry alpha vector

00:11:22,660 --> 00:11:28,210
generated but the internal state is

00:11:25,390 --> 00:11:30,730
updated as well and this internal state

00:11:28,210 --> 00:11:32,890
is itself just a vector and the idea is

00:11:30,730 --> 00:11:35,050
that as you pass your inputs through one

00:11:32,890 --> 00:11:37,390
by one this internal state vector

00:11:35,050 --> 00:11:39,970
remembers in some sense what is already

00:11:37,390 --> 00:11:41,950
seen and actually in real deep learning

00:11:39,970 --> 00:11:44,140
applications the output vector is

00:11:41,950 --> 00:11:46,120
typically ignored the only thing we care

00:11:44,140 --> 00:11:49,750
about is the internal state and how that

00:11:46,120 --> 00:11:51,880
changes ok so how do we then use this

00:11:49,750 --> 00:11:54,130
hidden memory vector to represent our

00:11:51,880 --> 00:11:56,140
documents well one approach would be to

00:11:54,130 --> 00:11:58,030
simply pass the word vectors of our

00:11:56,140 --> 00:11:59,680
individual words through the recurrent

00:11:58,030 --> 00:12:01,540
neural network in the same order as

00:11:59,680 --> 00:12:03,850
their source words are found in the text

00:12:01,540 --> 00:12:06,370
letting the hidden vector update itself

00:12:03,850 --> 00:12:09,010
over and over and then use the final

00:12:06,370 --> 00:12:10,750
version of the hidden vector as is once

00:12:09,010 --> 00:12:14,290
the word final word vector has been

00:12:10,750 --> 00:12:16,330
passed through however even though this

00:12:14,290 --> 00:12:18,460
vector in some sense remembers all of

00:12:16,330 --> 00:12:20,080
the words in the text especially for

00:12:18,460 --> 00:12:22,330
long documents this vector only has a

00:12:20,080 --> 00:12:24,520
very faint memory of the first words

00:12:22,330 --> 00:12:26,350
that is only the last words are

00:12:24,520 --> 00:12:28,980
contributing significantly to the value

00:12:26,350 --> 00:12:31,780
of the final state of the hidden vector

00:12:28,980 --> 00:12:34,860
alternatively we could make a copy of

00:12:31,780 --> 00:12:38,590
the hidden vector and after every update

00:12:34,860 --> 00:12:40,570
sorry make a copy F and then of the

00:12:38,590 --> 00:12:43,240
hidden vector after every update and

00:12:40,570 --> 00:12:45,280
then combine them somehow and this is

00:12:43,240 --> 00:12:47,380
like our previous proposal of simply

00:12:45,280 --> 00:12:50,110
combining the individual word vectors

00:12:47,380 --> 00:12:52,180
but now by using the hidden vectors each

00:12:50,110 --> 00:12:54,610
vectors information not only about the

00:12:52,180 --> 00:12:56,700
individual words but also the echoes of

00:12:54,610 --> 00:12:59,820
the words coming just before those words

00:12:56,700 --> 00:13:02,800
and therefore importantly their order

00:12:59,820 --> 00:13:04,930
all that remains is to decide on a way

00:13:02,800 --> 00:13:06,520
of combining the hidden vectors and we

00:13:04,930 --> 00:13:09,040
might actually be able to get away with

00:13:06,520 --> 00:13:11,280
simply averaging them but just as we

00:13:09,040 --> 00:13:14,080
propose to do with the word vectors

00:13:11,280 --> 00:13:16,000
sorry averaging them just like we

00:13:14,080 --> 00:13:17,230
propose to do with the word vectors but

00:13:16,000 --> 00:13:19,570
the authors of the paper did something

00:13:17,230 --> 00:13:21,820
different called max pooling and what

00:13:19,570 --> 00:13:23,650
you do is you make a new vector the same

00:13:21,820 --> 00:13:25,780
size as all of our hidden vectors and

00:13:23,650 --> 00:13:26,520
set the value of each component equal to

00:13:25,780 --> 00:13:28,590
the maximum

00:13:26,520 --> 00:13:32,970
all the values of that component across

00:13:28,590 --> 00:13:35,160
all hidden vectors here's an example of

00:13:32,970 --> 00:13:36,960
the result of this process

00:13:35,160 --> 00:13:39,990
it's from the paper from the same group

00:13:36,960 --> 00:13:42,510
but preceding the deep cs1 and we have

00:13:39,990 --> 00:13:45,050
here code related actions like start a

00:13:42,510 --> 00:13:47,670
new write operation on the file and

00:13:45,050 --> 00:13:50,520
remove the old entries of log file that

00:13:47,670 --> 00:13:52,020
have been converted into vectors these

00:13:50,520 --> 00:13:53,820
vectors have then been projected down to

00:13:52,020 --> 00:13:55,620
two dimensions for us in such a way that

00:13:53,820 --> 00:13:57,900
the spatial relationships between

00:13:55,620 --> 00:14:00,000
vectors are preserved and we see that

00:13:57,900 --> 00:14:03,180
natural clusters arise loading and

00:14:00,000 --> 00:14:06,170
reading actions form on blob as you save

00:14:03,180 --> 00:14:09,390
write actions and delete remove actions

00:14:06,170 --> 00:14:11,250
okay that's it for text to recap we

00:14:09,390 --> 00:14:13,590
start with some large body of text and

00:14:11,250 --> 00:14:15,060
identify the unique words we use the

00:14:13,590 --> 00:14:16,530
ordering of those words in the text to

00:14:15,060 --> 00:14:19,380
learn a small dimensional embedding of

00:14:16,530 --> 00:14:21,390
the individual words and finally to

00:14:19,380 --> 00:14:23,850
represent a phrase sentence or entire

00:14:21,390 --> 00:14:25,800
document we feed its words in order

00:14:23,850 --> 00:14:27,630
through a recurrent neural network and

00:14:25,800 --> 00:14:33,510
combine the resulting hidden vectors via

00:14:27,630 --> 00:14:35,130
max pooling okay now having seen how to

00:14:33,510 --> 00:14:36,840
make a small dimensional representation

00:14:35,130 --> 00:14:38,940
of text we'd like to do something

00:14:36,840 --> 00:14:40,710
similar with our code snippet however

00:14:38,940 --> 00:14:44,160
there are a few things we need to keep

00:14:40,710 --> 00:14:46,680
in mind first of all on the surface the

00:14:44,160 --> 00:14:49,710
variable and class names we use are

00:14:46,680 --> 00:14:51,530
themselves text but unlike most in most

00:14:49,710 --> 00:14:55,200
natural languages there's no implied

00:14:51,530 --> 00:14:57,480
requirement that the word used directly

00:14:55,200 --> 00:15:00,540
relate to the action being performed or

00:14:57,480 --> 00:15:02,550
to the role of the object on the other

00:15:00,540 --> 00:15:04,950
hand despite how we choose to name

00:15:02,550 --> 00:15:06,980
things the underlying API calls and

00:15:04,950 --> 00:15:09,540
control flows are completely and

00:15:06,980 --> 00:15:12,150
unambiguous and noticing that these

00:15:09,540 --> 00:15:14,790
calls are ordered and that there's only

00:15:12,150 --> 00:15:17,220
a finite number of them we realize that

00:15:14,790 --> 00:15:19,410
we can again just build a dictionary of

00:15:17,220 --> 00:15:20,970
the API calls and train a small

00:15:19,410 --> 00:15:23,880
dimensional embedding of them much like

00:15:20,970 --> 00:15:25,650
what we did with work Tyvek so to do so

00:15:23,880 --> 00:15:28,020
we start with some large collection of

00:15:25,650 --> 00:15:30,440
code snippets and for each we generate

00:15:28,020 --> 00:15:32,670
and traverse its abstract syntax tree

00:15:30,440 --> 00:15:35,160
collecting an ordered sequence of API

00:15:32,670 --> 00:15:37,920
calls for instance for each constructor

00:15:35,160 --> 00:15:39,590
invocation new C we append to our

00:15:37,920 --> 00:15:43,370
sequence the API C new

00:15:39,590 --> 00:15:45,620
or for each method call om where o is an

00:15:43,370 --> 00:15:48,650
instance of Class C we append the API

00:15:45,620 --> 00:15:50,780
call cm similar for loops and other

00:15:48,650 --> 00:15:52,510
control logic we can append the

00:15:50,780 --> 00:15:55,880
Constituent API calls in some

00:15:52,510 --> 00:15:57,260
deterministic way we then look at all of

00:15:55,880 --> 00:15:59,600
the sequences of all of our code

00:15:57,260 --> 00:16:02,240
snippets we make a dictionary of all the

00:15:59,600 --> 00:16:04,400
API calls we find and we assign a

00:16:02,240 --> 00:16:06,380
Manhattan coding to each API call

00:16:04,400 --> 00:16:08,510
we then trained and encoder/decoder

00:16:06,380 --> 00:16:09,860
model to predict for a given one hunting

00:16:08,510 --> 00:16:12,260
coding of an API call a particular

00:16:09,860 --> 00:16:14,210
sequence the one hot encoding of its

00:16:12,260 --> 00:16:16,790
neighboring API calls in that sequence

00:16:14,210 --> 00:16:18,530
and just like in the text case this

00:16:16,790 --> 00:16:22,670
results in a small dimensional

00:16:18,530 --> 00:16:24,320
representation of each API call lastly

00:16:22,670 --> 00:16:26,120
to obtain a representation of the API

00:16:24,320 --> 00:16:28,400
sequence of each code snippet we then

00:16:26,120 --> 00:16:29,930
pass these vectors in order through our

00:16:28,400 --> 00:16:31,490
current neural network though a

00:16:29,930 --> 00:16:32,780
different one from that which we use for

00:16:31,490 --> 00:16:35,480
the text but having the same

00:16:32,780 --> 00:16:37,160
architecture we collect the generated

00:16:35,480 --> 00:16:40,550
hidden vectors and aggregate them via

00:16:37,160 --> 00:16:42,110
max pooling of course there's some

00:16:40,550 --> 00:16:44,300
flexibility in the order and the choice

00:16:42,110 --> 00:16:46,700
of the API calls and how we structure

00:16:44,300 --> 00:16:48,710
the control logic and so this is not

00:16:46,700 --> 00:16:50,300
completely sufficient to represent the

00:16:48,710 --> 00:16:53,270
intent behind the code snippet in a

00:16:50,300 --> 00:16:55,490
general way so to achieve that what the

00:16:53,270 --> 00:16:58,340
team behind the CS did was to combine

00:16:55,490 --> 00:17:00,230
the API sequence embedding with the weak

00:16:58,340 --> 00:17:02,480
signals originating from the text of the

00:17:00,230 --> 00:17:05,660
code snippet so first we create a

00:17:02,480 --> 00:17:07,610
representation of the method name

00:17:05,660 --> 00:17:09,860
what we do is we split the method name

00:17:07,610 --> 00:17:11,600
into its constituent tokens we have

00:17:09,860 --> 00:17:13,520
words in word order and therefore we can

00:17:11,600 --> 00:17:16,970
apply a word to vector techniques by

00:17:13,520 --> 00:17:18,350
which by now we are familiar along with

00:17:16,970 --> 00:17:21,050
the third recurrent neural network to

00:17:18,350 --> 00:17:23,060
generate an embedding vector secondly

00:17:21,050 --> 00:17:25,370
just to ensure we don't miss any

00:17:23,060 --> 00:17:27,110
potential signals we take the method

00:17:25,370 --> 00:17:29,390
body split all the variable and method

00:17:27,110 --> 00:17:31,550
names into their constituent words apply

00:17:29,390 --> 00:17:33,170
the familiar bag of Ord tricks we know

00:17:31,550 --> 00:17:35,150
from information retrieval like deep

00:17:33,170 --> 00:17:37,490
lubrication natural language stop word

00:17:35,150 --> 00:17:39,920
removal and code language stop word

00:17:37,490 --> 00:17:43,130
removal that is removing all the

00:17:39,920 --> 00:17:44,660
language keywords and then we embed the

00:17:43,130 --> 00:17:47,390
remaining words and some small

00:17:44,660 --> 00:17:50,180
dimensional vectors we don't pass these

00:17:47,390 --> 00:17:52,610
vectors to an RNN so since unlike the

00:17:50,180 --> 00:17:53,600
method named words and API calls they

00:17:52,610 --> 00:17:55,880
have no strict or

00:17:53,600 --> 00:17:57,590
so instead we put them through a normal

00:17:55,880 --> 00:18:00,620
feed-forward neural network and max pull

00:17:57,590 --> 00:18:04,970
the resulting vectors so as a concrete

00:18:00,620 --> 00:18:07,820
example here's some simple code that

00:18:04,970 --> 00:18:10,190
converts a date into a calendar we have

00:18:07,820 --> 00:18:13,730
the method name and we break it up into

00:18:10,190 --> 00:18:15,560
the words - and calendar the API

00:18:13,730 --> 00:18:19,220
sequence calendar get instance and

00:18:15,560 --> 00:18:21,980
calendar set time and lastly we have the

00:18:19,220 --> 00:18:24,800
unique tokens calendar get instance set

00:18:21,980 --> 00:18:27,530
time and date which have been extracted

00:18:24,800 --> 00:18:30,110
from the code method body after having

00:18:27,530 --> 00:18:33,890
chopped the Java keywords final and

00:18:30,110 --> 00:18:35,720
return now this process leaves us with

00:18:33,890 --> 00:18:37,310
three separate vectors representing all

00:18:35,720 --> 00:18:39,890
the signals we can squeeze out from a

00:18:37,310 --> 00:18:41,720
method definition together though they

00:18:39,890 --> 00:18:43,480
have more dimensions than the text

00:18:41,720 --> 00:18:47,060
vectors we'll use to represent queries

00:18:43,480 --> 00:18:49,670
as the method name alone has this many

00:18:47,060 --> 00:18:51,260
dimensions so since we want to be able

00:18:49,670 --> 00:18:53,360
to compare the two vectors they'll have

00:18:51,260 --> 00:18:55,520
to have the same length and so simply

00:18:53,360 --> 00:18:58,340
concatenate avec ters if the code method

00:18:55,520 --> 00:19:00,590
won't work instead as a final step we

00:18:58,340 --> 00:19:02,630
concatenate the three put them through a

00:19:00,590 --> 00:19:04,790
feat for all feed-forward neural network

00:19:02,630 --> 00:19:07,250
whose output layer is the same size as

00:19:04,790 --> 00:19:08,870
the text vector so in addition to

00:19:07,250 --> 00:19:11,510
resulting in a vector of the appropriate

00:19:08,870 --> 00:19:14,090
size as an added benefit this last

00:19:11,510 --> 00:19:15,590
transmission will also learn to mix the

00:19:14,090 --> 00:19:18,290
individual signals coming from the

00:19:15,590 --> 00:19:22,070
method name API calls and method tokens

00:19:18,290 --> 00:19:24,470
in the most implemented way so that's it

00:19:22,070 --> 00:19:26,150
for code to recap first we created an

00:19:24,470 --> 00:19:27,860
embedding vector for the sequence of API

00:19:26,150 --> 00:19:29,870
calls first by making a small

00:19:27,860 --> 00:19:32,360
dimensional representation for each of

00:19:29,870 --> 00:19:33,890
the API calls and then by combining them

00:19:32,360 --> 00:19:36,860
with the recurrent neural network in max

00:19:33,890 --> 00:19:39,140
pooling second we create an embedding of

00:19:36,860 --> 00:19:40,730
the method name by breaking it up into

00:19:39,140 --> 00:19:42,980
its constituent words and then using

00:19:40,730 --> 00:19:45,050
work Tyvek another recurrent neural net

00:19:42,980 --> 00:19:49,130
and max pooling to combine the resulting

00:19:45,050 --> 00:19:51,200
word vectors we find all the unique word

00:19:49,130 --> 00:19:52,160
tokens in the method body convert them

00:19:51,200 --> 00:19:53,990
to their small dimensional

00:19:52,160 --> 00:19:56,300
representations pass them through a feed

00:19:53,990 --> 00:19:59,060
for knurled what neural network and max

00:19:56,300 --> 00:20:00,800
pool the results and finally we mix and

00:19:59,060 --> 00:20:03,080
down simple the result of three

00:20:00,800 --> 00:20:05,060
embedding vectors first by concatenating

00:20:03,080 --> 00:20:06,830
them and lastly by passing this

00:20:05,060 --> 00:20:14,120
concatenated vector through a fee

00:20:06,830 --> 00:20:15,620
for know that so now that we have

00:20:14,120 --> 00:20:17,690
methods both for creating vectors from

00:20:15,620 --> 00:20:19,880
text and for creating vectors from code

00:20:17,690 --> 00:20:21,769
how do we learn these transformations in

00:20:19,880 --> 00:20:24,140
such a way that the vectors representing

00:20:21,769 --> 00:20:26,450
text describing some action are close to

00:20:24,140 --> 00:20:29,120
or equal to the vectors representing

00:20:26,450 --> 00:20:30,200
code that performs those actions now

00:20:29,120 --> 00:20:32,720
normally training a machine learning

00:20:30,200 --> 00:20:35,269
model works by providing the model with

00:20:32,720 --> 00:20:38,570
an input and a lot target output to

00:20:35,269 --> 00:20:39,769
predict based on that input and if the

00:20:38,570 --> 00:20:41,600
model doesn't correctly predict this

00:20:39,769 --> 00:20:43,549
target we use the error between the

00:20:41,600 --> 00:20:45,169
prediction and target to slightly adjust

00:20:43,549 --> 00:20:46,460
the internal parameters of the model so

00:20:45,169 --> 00:20:50,149
that the next time it makes a better

00:20:46,460 --> 00:20:52,100
prediction but in the embedding case we

00:20:50,149 --> 00:20:53,120
don't really have a target output for

00:20:52,100 --> 00:20:55,370
either the natural language

00:20:53,120 --> 00:20:58,549
transformation model nor the code

00:20:55,370 --> 00:21:01,039
transformation model instead our target

00:20:58,549 --> 00:21:02,929
is that the vector for a piece of text

00:21:01,039 --> 00:21:04,820
describing an action and the vector

00:21:02,929 --> 00:21:07,190
embedding the code actually performing

00:21:04,820 --> 00:21:10,519
that action be close to one another if

00:21:07,190 --> 00:21:11,960
not overlapping to achieve this we use a

00:21:10,519 --> 00:21:13,789
technique what's called a joint

00:21:11,960 --> 00:21:15,559
embedding where instead of looking at

00:21:13,789 --> 00:21:17,450
the output of the natural language and

00:21:15,559 --> 00:21:20,779
code transformation separately we

00:21:17,450 --> 00:21:22,690
compare their vector outputs and we seek

00:21:20,779 --> 00:21:24,830
to minimize the angle between them

00:21:22,690 --> 00:21:26,899
additionally just like in work Tyvek it

00:21:24,830 --> 00:21:28,909
turns out that this training procedure

00:21:26,899 --> 00:21:32,539
works even better if we simultaneously

00:21:28,909 --> 00:21:34,700
also seek to maximize the angle between

00:21:32,539 --> 00:21:37,190
the code snippets vector and the vector

00:21:34,700 --> 00:21:38,480
of some action description that has

00:21:37,190 --> 00:21:41,870
nothing to do with the action the code

00:21:38,480 --> 00:21:43,700
is actually performing so it's by

00:21:41,870 --> 00:21:45,730
optimizing for this combined gold that

00:21:43,700 --> 00:21:48,200
we simultaneously train or to embeddings

00:21:45,730 --> 00:21:50,090
and this joint model is what the authors

00:21:48,200 --> 00:21:52,909
of the paper called the code description

00:21:50,090 --> 00:21:56,450
embedding neural network or code and

00:21:52,909 --> 00:21:58,760
then okay so we've got a method for

00:21:56,450 --> 00:22:01,010
embedding textual description of actions

00:21:58,760 --> 00:22:03,590
into a small dimensional vector space

00:22:01,010 --> 00:22:05,299
we've got a method for embedding the

00:22:03,590 --> 00:22:07,399
associated code definitions into the

00:22:05,299 --> 00:22:08,809
same small dimensional vector space and

00:22:07,399 --> 00:22:11,419
we've got a trick for learning both

00:22:08,809 --> 00:22:12,980
embedding simultaneously and this is a

00:22:11,419 --> 00:22:14,870
supervised machine learning problem so

00:22:12,980 --> 00:22:18,919
where's all the training data going to

00:22:14,870 --> 00:22:20,710
come from now as a demonstration of the

00:22:18,919 --> 00:22:22,870
feasibility of this idea the

00:22:20,710 --> 00:22:26,409
team behind code and then developed a

00:22:22,870 --> 00:22:27,760
system for code search called DPS the

00:22:26,409 --> 00:22:31,210
system works in three phases

00:22:27,760 --> 00:22:32,740
the first uses 18 million Java methods

00:22:31,210 --> 00:22:35,140
and their associated docstrings

00:22:32,740 --> 00:22:36,960
all scrape from github to train both the

00:22:35,140 --> 00:22:39,850
code and the natural language embeddings

00:22:36,960 --> 00:22:42,070
this is done once the code and

00:22:39,850 --> 00:22:44,429
docstrings are then discarded and only

00:22:42,070 --> 00:22:47,830
the learned transformations are retained

00:22:44,429 --> 00:22:50,049
this training dataset is purposely quite

00:22:47,830 --> 00:22:51,940
broad in order to guarantee that the

00:22:50,049 --> 00:22:54,789
representations learned will also be

00:22:51,940 --> 00:22:59,140
useful for as yet unseen code bases that

00:22:54,789 --> 00:23:00,460
is to say yours next you throw away all

00:22:59,140 --> 00:23:01,659
the code snippets from your code base

00:23:00,460 --> 00:23:03,760
without docstrings

00:23:01,659 --> 00:23:05,260
into the system and the previously

00:23:03,760 --> 00:23:06,669
learned transformations are then used to

00:23:05,260 --> 00:23:09,580
convert all of these methods into

00:23:06,669 --> 00:23:11,320
vectors this is done once but you can

00:23:09,580 --> 00:23:13,149
imagine a situation where code that was

00:23:11,320 --> 00:23:15,419
touched during the day has its vector

00:23:13,149 --> 00:23:17,860
representation recalculated at night

00:23:15,419 --> 00:23:20,260
notice through that this is a relatively

00:23:17,860 --> 00:23:21,549
cheap operation relative to the training

00:23:20,260 --> 00:23:26,169
required to learn the original

00:23:21,549 --> 00:23:28,419
transformations and lastly now that dps

00:23:26,169 --> 00:23:30,220
is ready for search when a user of the

00:23:28,419 --> 00:23:33,100
system wants to find a relevant piece of

00:23:30,220 --> 00:23:35,860
code he or she enters their natural

00:23:33,100 --> 00:23:37,539
language query into the system deep CS

00:23:35,860 --> 00:23:39,490
transforms the textual query into a

00:23:37,539 --> 00:23:41,529
vector performs a search to find the

00:23:39,490 --> 00:23:43,149
k-nearest code vectors through that

00:23:41,529 --> 00:23:45,250
natural language query vector and

00:23:43,149 --> 00:23:47,230
returns the corresponding code snippets

00:23:45,250 --> 00:23:51,549
to the user in decreasing order of

00:23:47,230 --> 00:23:53,230
proximity okay so we've talked today

00:23:51,549 --> 00:23:55,240
about an interesting problem to

00:23:53,230 --> 00:23:57,070
embedding code snippets and how that can

00:23:55,240 --> 00:23:58,630
be jointly learned with a text embedding

00:23:57,070 --> 00:24:01,750
to improve performance on the code

00:23:58,630 --> 00:24:04,059
search problem now a natural question is

00:24:01,750 --> 00:24:06,580
of course to ask is this just a nice

00:24:04,059 --> 00:24:09,220
story or is there really some strength

00:24:06,580 --> 00:24:11,710
to the idea so as a test the author's

00:24:09,220 --> 00:24:14,260
scraped almost 10,000 Java projects that

00:24:11,710 --> 00:24:15,250
have at least 20 stars on github none of

00:24:14,260 --> 00:24:17,620
which were included in the training

00:24:15,250 --> 00:24:20,409
corpus and then they encoded the methods

00:24:17,620 --> 00:24:22,240
of those projects they then made a

00:24:20,409 --> 00:24:23,860
benchmark of queries from the top 50

00:24:22,240 --> 00:24:25,919
voted Java programming questions on

00:24:23,860 --> 00:24:28,059
Stack Overflow queries such as

00:24:25,919 --> 00:24:30,549
generating random integers in a specific

00:24:28,059 --> 00:24:32,230
range how do I get a platform dependent

00:24:30,549 --> 00:24:34,559
newline character

00:24:32,230 --> 00:24:36,730
and removing whitespace from strings

00:24:34,559 --> 00:24:38,049
they then looked at the ordered search

00:24:36,730 --> 00:24:41,080
results with those produced by lean

00:24:38,049 --> 00:24:43,210
leucine bass systems and by Koh Tao

00:24:41,080 --> 00:24:44,890
which is what the the authors of the

00:24:43,210 --> 00:24:47,950
paper considered state-of-the-art and

00:24:44,890 --> 00:24:49,809
found that and both in terms of position

00:24:47,950 --> 00:24:52,270
of the first relevant result where lower

00:24:49,809 --> 00:24:54,220
is better and percentage of relevant

00:24:52,270 --> 00:24:56,799
results in the top end results or higher

00:24:54,220 --> 00:24:59,830
is better deep CS vastly outperformed

00:24:56,799 --> 00:25:01,900
its competitors and what this tells me

00:24:59,830 --> 00:25:04,360
is that there are almost certainly other

00:25:01,900 --> 00:25:06,130
niche search tasks out there for which

00:25:04,360 --> 00:25:07,480
using machine learning and joint

00:25:06,130 --> 00:25:10,030
embeddings will provide a market

00:25:07,480 --> 00:25:13,720
performance boost over the non embedding

00:25:10,030 --> 00:25:16,690
techniques used currently rounding up

00:25:13,720 --> 00:25:18,669
I'd like to thank gyeonggu Hong Yu Jiang

00:25:16,690 --> 00:25:20,890
and some Hong Kim for their novel idea

00:25:18,669 --> 00:25:22,450
my employer NGO data-driven for flying

00:25:20,890 --> 00:25:25,120
me out here and let me speak on company

00:25:22,450 --> 00:25:26,980
time and you the audience for your

00:25:25,120 --> 00:25:31,169
attention and now if there's any time

00:25:26,980 --> 00:25:31,169
for questions I'd be happy to field them

00:25:34,500 --> 00:25:45,850
thank you very much Robert we'll go

00:25:38,799 --> 00:25:48,580
first with the questions wondering were

00:25:45,850 --> 00:25:51,010
there any explorations of how well this

00:25:48,580 --> 00:25:53,380
would work for languages with code

00:25:51,010 --> 00:25:55,390
generation capabilities whether they're

00:25:53,380 --> 00:25:58,570
like really primitive like C's macros or

00:25:55,390 --> 00:26:01,179
more advanced like Russ macros because

00:25:58,570 --> 00:26:02,410
Java seems like an easy scenario for

00:26:01,179 --> 00:26:06,850
this compared to a lot of other

00:26:02,410 --> 00:26:08,440
languages so the paper is so the

00:26:06,850 --> 00:26:10,900
proof-of-concept was done in Java in the

00:26:08,440 --> 00:26:13,090
paper the authors made a mention that

00:26:10,900 --> 00:26:16,780
they said that this should work in

00:26:13,090 --> 00:26:18,640
principle for any other language so long

00:26:16,780 --> 00:26:22,840
as people are using reasonable names for

00:26:18,640 --> 00:26:27,090
their variables ok we have another

00:26:22,840 --> 00:26:29,260
question or yeah exactly

00:26:27,090 --> 00:26:31,390
wonderful talk thank you so much did the

00:26:29,260 --> 00:26:33,520
authors look at like sort of security

00:26:31,390 --> 00:26:35,290
analysis like am i calling methods out

00:26:33,520 --> 00:26:36,790
of order am i violating a contract I

00:26:35,290 --> 00:26:39,960
mean did they sort of think of that as

00:26:36,790 --> 00:26:39,960
future work or no

00:26:43,060 --> 00:26:55,970
yes correlation to commit messages maybe

00:26:54,160 --> 00:26:56,810
sorry one more time please our

00:26:55,970 --> 00:26:58,460
correlation

00:26:56,810 --> 00:27:01,610
did you check any correlation with

00:26:58,460 --> 00:27:11,710
connect messages messages no that was

00:27:01,610 --> 00:27:11,710
not done any other interesting question

00:27:12,700 --> 00:27:21,140
come on I'm gonna count to three one two

00:27:18,920 --> 00:27:23,730
three okay no more questions thank you

00:27:21,140 --> 00:27:28,259
again Robert thank you

00:27:23,730 --> 00:27:28,259

YouTube URL: https://www.youtube.com/watch?v=d50Li6_Npn0


