Title: Berlin Buzzwords 2019: Sakshi Shukla â€“ Bias in NLP 101 #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Bias in NLP 101 talks about the biases present in Texts based for Men and Women. For this, a dataset based on movies reviews from India is used and worked on by IBM. The intention is on having various situations where we can check if the model predicts a biased result as based on the stereotypical notions developed in the society for both men and women. 

Since movies replicate the reality in the most significant way their reviews being less in length would get biased. To de-bias the system, a model called DeCogTeller is used which results in the correct form of Gender. This project is a Talk Session on Machine Learning and Natural Language Processing and also brings out the challenges companies like Google, Amazon and Facebook faced while recruiting women. 

Read more:
https://2019.berlinbuzzwords.de/19/session/bias-nlp-101

About Sakshi Shukla:
https://2019.berlinbuzzwords.de/users/sakshi-shukla

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,840 --> 00:00:11,420
thank you everyone okay so I'll be

00:00:08,900 --> 00:00:13,820
starting with bias in NLP and I'll be

00:00:11,420 --> 00:00:15,949
covering a very generic part of the

00:00:13,820 --> 00:00:19,880
biases so generally whenever we address

00:00:15,949 --> 00:00:22,189
a bias we we try to in and we try to

00:00:19,880 --> 00:00:24,849
click on the disparity which is present

00:00:22,189 --> 00:00:28,220
in the data or in the input that we give

00:00:24,849 --> 00:00:30,740
so what exactly is a disparity we can

00:00:28,220 --> 00:00:33,710
somewhat relate it to a problem

00:00:30,740 --> 00:00:36,860
statement so there's so many problems

00:00:33,710 --> 00:00:39,920
like we see this bias everywhere in

00:00:36,860 --> 00:00:42,230
various NLP models like you see it in

00:00:39,920 --> 00:00:44,360
movies you see it over books you can see

00:00:42,230 --> 00:00:47,540
it on language translations so all these

00:00:44,360 --> 00:00:49,610
all these examples are which are like

00:00:47,540 --> 00:00:52,460
really close to our old a you know

00:00:49,610 --> 00:00:54,140
lifestyle we witness biases in natural

00:00:52,460 --> 00:00:58,070
language processing models now we will

00:00:54,140 --> 00:00:59,630
see how so generally if anyone here is

00:00:58,070 --> 00:01:02,270
working at Google please don't get

00:00:59,630 --> 00:01:03,920
offended but generally we think like

00:01:02,270 --> 00:01:05,750
Google Facebook Amazon these are like

00:01:03,920 --> 00:01:07,759
really good companies and we think that

00:01:05,750 --> 00:01:09,950
you know these companies are like

00:01:07,759 --> 00:01:11,810
extremely good and we want to be in them

00:01:09,950 --> 00:01:14,590
and we think that they are working very

00:01:11,810 --> 00:01:17,240
well and they don't have as much as

00:01:14,590 --> 00:01:20,509
error they have like error free

00:01:17,240 --> 00:01:25,600
environment but it's not actually truth

00:01:20,509 --> 00:01:28,909
we'll find out how so what happens is

00:01:25,600 --> 00:01:31,969
the Google ads are shown where they

00:01:28,909 --> 00:01:34,340
actually claim with the job scales for

00:01:31,969 --> 00:01:36,710
women and it has turned out that they

00:01:34,340 --> 00:01:39,229
show less paid jobs for the same

00:01:36,710 --> 00:01:41,359
position as what they show to a guy now

00:01:39,229 --> 00:01:43,729
how exactly the model understands that

00:01:41,359 --> 00:01:46,999
it's a guy or a good so obviously on

00:01:43,729 --> 00:01:49,969
prior information which it stores it

00:01:46,999 --> 00:01:52,579
labels through various features it

00:01:49,969 --> 00:01:55,459
understands okay it's either it's a boy

00:01:52,579 --> 00:01:58,280
or a girl and then like that it actually

00:01:55,459 --> 00:02:02,029
proves that you know this is a female

00:01:58,280 --> 00:02:04,279
category based persons so they will show

00:02:02,029 --> 00:02:06,109
them less paid jobs for the same

00:02:04,279 --> 00:02:09,069
position of what a guy would be getting

00:02:06,109 --> 00:02:11,959
and this has been proven similarly

00:02:09,069 --> 00:02:14,870
Facebook is also like that and so is

00:02:11,959 --> 00:02:17,260
with Amazon so there was actually a

00:02:14,870 --> 00:02:19,430
campaign there was rate that was raised

00:02:17,260 --> 00:02:22,250
because you know there was so many

00:02:19,430 --> 00:02:24,049
adds that was shown on Facebook with the

00:02:22,250 --> 00:02:28,340
similar problem that Google was also

00:02:24,049 --> 00:02:30,590
doing and AI model at Amazon was not

00:02:28,340 --> 00:02:33,409
recruiting women it was actually

00:02:30,590 --> 00:02:35,239
scraping out so if maybe let's say I had

00:02:33,409 --> 00:02:37,400
a profile which was exactly similar of

00:02:35,239 --> 00:02:39,500
what a guy was having so maybe the

00:02:37,400 --> 00:02:41,120
profile that was needed a guy would get

00:02:39,500 --> 00:02:43,879
selected and my profile would get

00:02:41,120 --> 00:02:46,970
rejected based on the gender and it's

00:02:43,879 --> 00:02:48,319
not because there's there's an intention

00:02:46,970 --> 00:02:50,480
behind it it's it's happening

00:02:48,319 --> 00:02:51,769
unintentionally and that is the bias

00:02:50,480 --> 00:02:54,260
which is behind it

00:02:51,769 --> 00:02:56,750
so we'll find how exactly you know these

00:02:54,260 --> 00:03:00,349
companies are now working to scrub that

00:02:56,750 --> 00:03:02,540
out so like I said it's not only with

00:03:00,349 --> 00:03:07,250
jobs there with our everyday life

00:03:02,540 --> 00:03:10,939
so it says she is in Polish and he is

00:03:07,250 --> 00:03:15,439
honest if I pronounced it wrongly then

00:03:10,939 --> 00:03:19,069
please pardon me CRI in NASA policia CIA

00:03:15,439 --> 00:03:22,310
sang nurse when I'm translating it it is

00:03:19,069 --> 00:03:24,290
taking up common CR which is he and

00:03:22,310 --> 00:03:27,889
sheets common for both in Filipino from

00:03:24,290 --> 00:03:30,709
English but when I retranslated it's

00:03:27,889 --> 00:03:33,079
actually what it's doing it's the first

00:03:30,709 --> 00:03:36,319
CR which was a she in the first

00:03:33,079 --> 00:03:37,970
statement has been taken as a he so you

00:03:36,319 --> 00:03:40,280
know generally whenever we think of

00:03:37,970 --> 00:03:42,739
corpse whenever you think of Pulis well

00:03:40,280 --> 00:03:44,750
we say policeman we never we usually

00:03:42,739 --> 00:03:46,970
never say policewoman I mean there can

00:03:44,750 --> 00:03:50,150
be a girl also right it's not necessary

00:03:46,970 --> 00:03:52,940
that all all police people are boys or

00:03:50,150 --> 00:03:54,440
our guys so these are the basic biases

00:03:52,940 --> 00:03:56,870
which are actually present in these

00:03:54,440 --> 00:03:59,750
models which we actually don't realize

00:03:56,870 --> 00:04:02,150
so it's something which is there and

00:03:59,750 --> 00:04:02,919
it's occupations are again decided by

00:04:02,150 --> 00:04:05,359
genders

00:04:02,919 --> 00:04:07,699
whenever you know whenever we say call

00:04:05,359 --> 00:04:09,530
the cops we never think that a cab would

00:04:07,699 --> 00:04:11,690
become a police cab would be coming with

00:04:09,530 --> 00:04:13,699
four or five women in it we generally

00:04:11,690 --> 00:04:15,470
think you know male police would be

00:04:13,699 --> 00:04:19,070
arriving I think that's you're in

00:04:15,470 --> 00:04:22,039
Germany as well right right yeah so

00:04:19,070 --> 00:04:24,650
exactly so even Google translation has

00:04:22,039 --> 00:04:27,320
been has not actually worked well it has

00:04:24,650 --> 00:04:31,370
failed here by by not correcting the

00:04:27,320 --> 00:04:32,870
right gender so obviously in books also

00:04:31,370 --> 00:04:35,060
there's bias present

00:04:32,870 --> 00:04:36,410
like if you see generally you say the

00:04:35,060 --> 00:04:38,930
girl is gonna take care of the house and

00:04:36,410 --> 00:04:40,669
the boy will go out and work the girl

00:04:38,930 --> 00:04:43,130
would cook the boy would do something

00:04:40,669 --> 00:04:45,919
else maybe eat or whatever so even these

00:04:43,130 --> 00:04:47,630
basic examples and this is a book for a

00:04:45,919 --> 00:04:49,850
toddler I mean it's not something that

00:04:47,630 --> 00:04:52,430
is there for somebody who's like well

00:04:49,850 --> 00:04:53,930
educated it's even in the beginning from

00:04:52,430 --> 00:04:55,490
very beginning that's what we're

00:04:53,930 --> 00:04:57,949
actually consuming and we're not even

00:04:55,490 --> 00:05:00,830
realizing that it's actually not right

00:04:57,949 --> 00:05:02,540
there's a bias even here so that's what

00:05:00,830 --> 00:05:04,840
I said initially that you know even our

00:05:02,540 --> 00:05:07,520
lifestyle is filled with so many biases

00:05:04,840 --> 00:05:09,229
with Disney movies I think most of you

00:05:07,520 --> 00:05:12,440
must have seen even though I see more

00:05:09,229 --> 00:05:14,419
guys but I think that's ok we generally

00:05:12,440 --> 00:05:17,270
see that you know maybe it's the white

00:05:14,419 --> 00:05:19,460
or or or Cinderella in the end the

00:05:17,270 --> 00:05:21,410
prince comes at the climax and he fixes

00:05:19,460 --> 00:05:24,200
everything and the girl is suffering

00:05:21,410 --> 00:05:25,729
maybe she's sisters are not helping or

00:05:24,200 --> 00:05:28,760
or maybe you know the mother is not

00:05:25,729 --> 00:05:31,760
helping her so you know it's why do we

00:05:28,760 --> 00:05:35,030
need to introduce a male character to

00:05:31,760 --> 00:05:36,770
sort of solve the problem why not we can

00:05:35,030 --> 00:05:39,260
deal with the with the protagonist

00:05:36,770 --> 00:05:42,200
itself even though all these feet all

00:05:39,260 --> 00:05:44,539
these are female driven films all of

00:05:42,200 --> 00:05:46,250
them have a protagonist who are women I

00:05:44,539 --> 00:05:48,620
mean Snow White was a woman it was all

00:05:46,250 --> 00:05:51,260
based on her life with the Indian you

00:05:48,620 --> 00:05:53,240
know the the prince comes and you know

00:05:51,260 --> 00:05:55,610
the apple that she eats comes out

00:05:53,240 --> 00:05:58,039
because of the horse and everything so

00:05:55,610 --> 00:06:00,700
you know those things are there that it

00:05:58,039 --> 00:06:04,789
has to be introduced through a prince

00:06:00,700 --> 00:06:07,070
considering this example now with I'm

00:06:04,789 --> 00:06:08,930
not too sure if you guys know about it

00:06:07,070 --> 00:06:12,380
but generally what happens in Bollywood

00:06:08,930 --> 00:06:14,440
movies like I said in movies so this is

00:06:12,380 --> 00:06:17,150
a very basic example whatever

00:06:14,440 --> 00:06:19,550
characteristics we associate for a hero

00:06:17,150 --> 00:06:22,490
is not generally how we associate for

00:06:19,550 --> 00:06:24,979
the hero in horizon for the actress and

00:06:22,490 --> 00:06:27,860
here I'll read out the dialogue Rohit is

00:06:24,979 --> 00:06:31,360
an aspiring singer who works as a sales

00:06:27,860 --> 00:06:35,479
man in a car showroom run by mr. Malik

00:06:31,360 --> 00:06:38,660
one day he meets Sonya daughter of mr.

00:06:35,479 --> 00:06:42,260
Saxena when he goes to deliver a car to

00:06:38,660 --> 00:06:46,070
her home as her birthday present so what

00:06:42,260 --> 00:06:46,580
exactly what exactly is is something

00:06:46,070 --> 00:06:49,729
that you can

00:06:46,580 --> 00:06:53,210
find out from here can anyone actually

00:06:49,729 --> 00:06:57,169
try what is the disparity in this

00:06:53,210 --> 00:06:59,509
dialogue will obviously discuss it in

00:06:57,169 --> 00:07:02,599
the further slides but if anyone can

00:06:59,509 --> 00:07:06,199
still catch the description says Rohit

00:07:02,599 --> 00:07:06,979
is an aspiring singer and he's a

00:07:06,199 --> 00:07:09,770
Salesman

00:07:06,979 --> 00:07:14,800
and he's going to deliver a car and etc

00:07:09,770 --> 00:07:14,800
and Sonia is daughter of mr. Saxena

00:07:15,069 --> 00:07:28,400
sorry girls are pampered yeah it says a

00:07:24,710 --> 00:07:31,539
sales man okay

00:07:28,400 --> 00:07:34,190
so I'll explain you so here if you see

00:07:31,539 --> 00:07:37,879
what what is there's an undervalue

00:07:34,190 --> 00:07:39,919
statement that Sonia Saxena is just

00:07:37,879 --> 00:07:42,319
daughter of mr. Saxena there's no

00:07:39,919 --> 00:07:43,159
description given which is daughter of

00:07:42,319 --> 00:07:45,830
mr. Saxena

00:07:43,159 --> 00:07:48,110
whereas to both of them are leading

00:07:45,830 --> 00:07:50,750
people one is the boy one is the girl

00:07:48,110 --> 00:07:52,460
Rohit is the boy he has been given a

00:07:50,750 --> 00:07:54,560
really long description that you know

00:07:52,460 --> 00:07:57,560
he's a Salesman he wants to be a singer

00:07:54,560 --> 00:07:59,120
he's going to deliver a car etc etc but

00:07:57,560 --> 00:08:02,449
for the girl it's just that she's

00:07:59,120 --> 00:08:04,759
daughter of somebody is that description

00:08:02,449 --> 00:08:07,690
enough to actually make you realize that

00:08:04,759 --> 00:08:10,849
what is the personality of this person I

00:08:07,690 --> 00:08:12,620
mean it could have written it in the

00:08:10,849 --> 00:08:16,099
other way also the true hit is you know

00:08:12,620 --> 00:08:18,289
through it as a boy and she is maybe she

00:08:16,099 --> 00:08:19,969
graduated or she's staying there and she

00:08:18,289 --> 00:08:22,310
sing nothing has been described about

00:08:19,969 --> 00:08:24,500
her only a simple line has been given

00:08:22,310 --> 00:08:25,930
that she is or daughter often she's the

00:08:24,500 --> 00:08:29,479
daughter of mr. Saxena

00:08:25,930 --> 00:08:32,539
so these undervalued statements which we

00:08:29,479 --> 00:08:35,959
generally ignore actually help us in

00:08:32,539 --> 00:08:38,570
finding the bias and the entire

00:08:35,959 --> 00:08:40,909
description that says is basically that

00:08:38,570 --> 00:08:43,729
she is getting a present but the way it

00:08:40,909 --> 00:08:45,949
has been picturised that we not intend

00:08:43,729 --> 00:08:49,459
on looking on these critical points but

00:08:45,949 --> 00:08:51,560
we actually focus on the outcomes which

00:08:49,459 --> 00:08:55,310
is very basic and simple and it's too

00:08:51,560 --> 00:08:57,860
direct yet we chose to not we still

00:08:55,310 --> 00:09:00,050
chose to miss out these critical points

00:08:57,860 --> 00:09:03,739
in the undervalued statements

00:09:00,050 --> 00:09:05,809
so obviously if had I not been pointed

00:09:03,739 --> 00:09:07,970
it I I don't think most of you must have

00:09:05,809 --> 00:09:11,540
actually figured it so it's a difficult

00:09:07,970 --> 00:09:14,329
to actually detect bias like that so how

00:09:11,540 --> 00:09:16,759
should one actually try to do it what

00:09:14,329 --> 00:09:19,009
are the myse so it's very simple to

00:09:16,759 --> 00:09:21,410
detect bias you need to analyze the

00:09:19,009 --> 00:09:24,290
basic stereotypes which we actually miss

00:09:21,410 --> 00:09:27,049
out you need to you need to develop a

00:09:24,290 --> 00:09:29,720
dataset in such a way that you're able

00:09:27,049 --> 00:09:31,819
to you know you are able to pinpoint the

00:09:29,720 --> 00:09:34,179
D bias and you're able to put that as an

00:09:31,819 --> 00:09:36,199
input so that your training model works

00:09:34,179 --> 00:09:39,350
accordingly and gives you the best

00:09:36,199 --> 00:09:42,790
accuracy results so it's very important

00:09:39,350 --> 00:09:45,499
to make data set which is D biased and

00:09:42,790 --> 00:09:47,600
obviously then your model would be

00:09:45,499 --> 00:09:52,730
developed using machine learning

00:09:47,600 --> 00:09:55,579
algorithm okay so how do you analyze the

00:09:52,730 --> 00:09:59,360
stereotypes the jhen the broadly three

00:09:55,579 --> 00:10:01,249
characteristics through words right if I

00:09:59,360 --> 00:10:03,350
say she's a pretty girl it's very

00:10:01,249 --> 00:10:05,449
obvious if I just say oh there's

00:10:03,350 --> 00:10:07,069
somebody she's looking me pretty like

00:10:05,449 --> 00:10:09,549
that or maybe something it's it's very

00:10:07,069 --> 00:10:13,220
obvious that you know it's for a girl

00:10:09,549 --> 00:10:16,459
right whenever we use a guy has to be

00:10:13,220 --> 00:10:17,839
rich a guy has to be successful we

00:10:16,459 --> 00:10:21,220
generally don't associate these

00:10:17,839 --> 00:10:24,110
adjectives these words with with women

00:10:21,220 --> 00:10:26,660
so these are this is like a very fine

00:10:24,110 --> 00:10:29,299
line and a base line approach you need

00:10:26,660 --> 00:10:31,249
to work on the stereo to work on the

00:10:29,299 --> 00:10:33,860
biasing you need to you need to extract

00:10:31,249 --> 00:10:36,679
the stereotypes and change it you need

00:10:33,860 --> 00:10:40,009
to make it unanimously exclusive for

00:10:36,679 --> 00:10:42,439
both the categories that's one and words

00:10:40,009 --> 00:10:45,860
are the most primary thing then the

00:10:42,439 --> 00:10:47,899
second is the actions now generally what

00:10:45,860 --> 00:10:50,209
we say is like okay she's a girl she

00:10:47,899 --> 00:10:54,049
must be cooking is a boy must have gone

00:10:50,209 --> 00:10:56,299
you know out to work so we are giving

00:10:54,049 --> 00:10:58,220
these these actions these these

00:10:56,299 --> 00:11:00,649
performance to a specific category

00:10:58,220 --> 00:11:03,019
without realizing like if you remember

00:11:00,649 --> 00:11:05,149
the the book that I showed you which was

00:11:03,019 --> 00:11:07,299
basically for a toddler it had the same

00:11:05,149 --> 00:11:10,759
example the girls can cook in the boys

00:11:07,299 --> 00:11:13,640
eat or play was something like that so

00:11:10,759 --> 00:11:16,160
we are actually not realizing that we

00:11:13,640 --> 00:11:20,030
are giving these informations ourselves

00:11:16,160 --> 00:11:22,490
only so we need to take care to to to

00:11:20,030 --> 00:11:24,320
remove the bias we need to take care

00:11:22,490 --> 00:11:26,870
that we actually work on these

00:11:24,320 --> 00:11:28,730
stereotypes so first category will be

00:11:26,870 --> 00:11:30,710
the words the adjective the second

00:11:28,730 --> 00:11:33,620
category belongs to the actions that

00:11:30,710 --> 00:11:37,190
we're labeling like you see if you see

00:11:33,620 --> 00:11:39,230
here if it's visible to the people at

00:11:37,190 --> 00:11:45,920
the back also it says that the girl

00:11:39,230 --> 00:11:49,040
marries enjoyed it says the boy beats so

00:11:45,920 --> 00:11:51,140
it's it's it's very like regressive but

00:11:49,040 --> 00:11:54,190
there are some places where it actually

00:11:51,140 --> 00:11:56,510
happens that you know girls are actually

00:11:54,190 --> 00:11:58,700
the consider that you know we will just

00:11:56,510 --> 00:12:01,400
put them inside the house and they'll be

00:11:58,700 --> 00:12:03,770
working or the boys would go out and

00:12:01,400 --> 00:12:05,480
work and they they can do not rule the

00:12:03,770 --> 00:12:08,450
world and everything so we don't realize

00:12:05,480 --> 00:12:10,550
but these actions are also there so we

00:12:08,450 --> 00:12:13,370
need to work on that and finally we have

00:12:10,550 --> 00:12:16,190
the occupation like the example I gave

00:12:13,370 --> 00:12:17,990
in the translation one that she is a

00:12:16,190 --> 00:12:21,350
police but it came out to be he's a

00:12:17,990 --> 00:12:23,960
polis these action these occupations are

00:12:21,350 --> 00:12:25,700
also play an important role so generally

00:12:23,960 --> 00:12:29,000
we come across that you know girls

00:12:25,700 --> 00:12:32,660
generally teachers secretaries we never

00:12:29,000 --> 00:12:34,640
think of a girl as a sports person even

00:12:32,660 --> 00:12:36,200
in this example I don't think so they

00:12:34,640 --> 00:12:38,000
also have it yeah they don't have it

00:12:36,200 --> 00:12:42,230
it's not even there

00:12:38,000 --> 00:12:45,500
I mean it's so rare and it's so lesson

00:12:42,230 --> 00:12:48,230
number otherwise we at least think of it

00:12:45,500 --> 00:12:50,120
you know okay boys would become maybe

00:12:48,230 --> 00:12:52,520
less maybe more but at least we have

00:12:50,120 --> 00:12:56,570
that window open in our heads it's not

00:12:52,520 --> 00:12:58,750
even open for women doctors extremely

00:12:56,570 --> 00:13:01,010
less police officers extremely less

00:12:58,750 --> 00:13:03,320
students we generally think okay they're

00:13:01,010 --> 00:13:06,140
good you know some of them would be even

00:13:03,320 --> 00:13:08,000
less interested would not be interested

00:13:06,140 --> 00:13:10,340
or sometimes we force so that number

00:13:08,000 --> 00:13:12,620
also decreases these occupations that we

00:13:10,340 --> 00:13:15,290
you know label are also playing an

00:13:12,620 --> 00:13:19,700
important part which we don't realize

00:13:15,290 --> 00:13:22,610
that's one more thing now how do we

00:13:19,700 --> 00:13:26,030
suffer like I said about the movies for

00:13:22,610 --> 00:13:27,370
data set generally what what was seen

00:13:26,030 --> 00:13:32,070
was there were 400

00:13:27,370 --> 00:13:37,990
plus movies from 1970 to 2000 2017 and

00:13:32,070 --> 00:13:42,120
only 5000 8558 female cast members were

00:13:37,990 --> 00:13:47,440
there but for the guys there were 938

00:13:42,120 --> 00:13:49,450
sorry 380 9000 380 so these things we

00:13:47,440 --> 00:13:54,370
don't realize it's almost half the

00:13:49,450 --> 00:13:57,130
number approximately so in general for

00:13:54,370 --> 00:13:59,440
actors actresses for any occupation the

00:13:57,130 --> 00:14:01,030
number goes half are the under then if

00:13:59,440 --> 00:14:03,130
it's a teaching job or if it's a

00:14:01,030 --> 00:14:05,650
secretary job jobs which are actually

00:14:03,130 --> 00:14:07,690
labeled for women and these are the

00:14:05,650 --> 00:14:09,220
things whenever we train a model we miss

00:14:07,690 --> 00:14:12,490
out on these informations and that's

00:14:09,220 --> 00:14:14,800
where biases are actually seen in in our

00:14:12,490 --> 00:14:17,170
model so whenever we are making a model

00:14:14,800 --> 00:14:19,090
in such a way that it has to be uniform

00:14:17,170 --> 00:14:21,700
we need to take care of these points

00:14:19,090 --> 00:14:23,860
because these are some points which are

00:14:21,700 --> 00:14:26,290
not available you just pick out the

00:14:23,860 --> 00:14:28,570
information maybe online or you create

00:14:26,290 --> 00:14:30,670
or while you creating also we generally

00:14:28,570 --> 00:14:33,160
overlook these and we create the data

00:14:30,670 --> 00:14:35,620
set and we train a model and then you

00:14:33,160 --> 00:14:37,720
know afterwards we realize that there

00:14:35,620 --> 00:14:40,150
are some biases present so it's very

00:14:37,720 --> 00:14:42,880
important to work with them like

00:14:40,150 --> 00:14:47,230
considering this data set only it can be

00:14:42,880 --> 00:14:51,100
found on with this link so you guys can

00:14:47,230 --> 00:14:53,230
also see it for the same example that I

00:14:51,100 --> 00:14:57,430
gave I'll be showing you some results

00:14:53,230 --> 00:14:59,560
later on so if you see the pink one the

00:14:57,430 --> 00:15:01,800
pink the red one is for girls and the

00:14:59,560 --> 00:15:05,890
blue one is for boys so there are like

00:15:01,800 --> 00:15:09,940
around as few Bollywood movies if you

00:15:05,890 --> 00:15:12,730
see there has been like I think not more

00:15:09,940 --> 00:15:16,800
than five movies would be you know where

00:15:12,730 --> 00:15:19,030
70% of the information is about a girl

00:15:16,800 --> 00:15:21,550
maybe you don't you might not know the

00:15:19,030 --> 00:15:23,800
context also but even if through the

00:15:21,550 --> 00:15:25,750
imagery you can't find that okay this

00:15:23,800 --> 00:15:27,730
much this much has been the

00:15:25,750 --> 00:15:30,520
participation by a woman and this much

00:15:27,730 --> 00:15:32,680
has been the participation by a boy so

00:15:30,520 --> 00:15:34,570
even with these figures we understand

00:15:32,680 --> 00:15:38,200
that we are actually taking them out

00:15:34,570 --> 00:15:40,390
even though they are the protagonists so

00:15:38,200 --> 00:15:44,680
we tend on losing this

00:15:40,390 --> 00:15:47,650
and of information so if you see

00:15:44,680 --> 00:15:52,120
basically if you see the percentage

00:15:47,650 --> 00:15:56,470
it was extremely lesson in initial from

00:15:52,120 --> 00:15:59,230
1970s to 2000 and then it actually it

00:15:56,470 --> 00:16:01,420
was a strong inclination upwards and

00:15:59,230 --> 00:16:04,630
then it became again constant and there

00:16:01,420 --> 00:16:07,150
was an uphill again so why because

00:16:04,630 --> 00:16:09,910
generally this around this time there

00:16:07,150 --> 00:16:12,130
were very few female centric movies then

00:16:09,910 --> 00:16:15,220
few of them like biopics and extract

00:16:12,130 --> 00:16:17,710
travel made which again is the story of

00:16:15,220 --> 00:16:19,780
a women it's not basically a common film

00:16:17,710 --> 00:16:21,730
or a common movie where you're not the

00:16:19,780 --> 00:16:24,880
guy and the girl is there it's basically

00:16:21,730 --> 00:16:28,030
it's only dependent on the women that's

00:16:24,880 --> 00:16:32,260
whether the graph has increased so

00:16:28,030 --> 00:16:37,450
that's one thing so it's the growth is

00:16:32,260 --> 00:16:41,710
less than 5% in in I think 20 to 30

00:16:37,450 --> 00:16:44,920
years now how exactly the bias can be

00:16:41,710 --> 00:16:46,590
removed which we've seen so far which is

00:16:44,920 --> 00:16:50,800
very important

00:16:46,590 --> 00:16:56,200
so this decock teller has been invented

00:16:50,800 --> 00:16:59,650
by IBM at India I'm not too sure if the

00:16:56,200 --> 00:17:02,470
German IBM is aware about it but it has

00:16:59,650 --> 00:17:03,700
been I'm not sure about it Vidya so

00:17:02,470 --> 00:17:07,480
they've been working because it's

00:17:03,700 --> 00:17:08,920
basically based on movie data set so I

00:17:07,480 --> 00:17:11,890
don't think so they they would know

00:17:08,920 --> 00:17:13,900
about it but what exactly what exactly

00:17:11,890 --> 00:17:16,540
is your is that they're giving the facts

00:17:13,900 --> 00:17:18,579
which is very common like how the basic

00:17:16,540 --> 00:17:21,459
NLP model works you give the information

00:17:18,579 --> 00:17:23,290
then you perform the word embedding

00:17:21,459 --> 00:17:26,440
problem you generate the word embedding

00:17:23,290 --> 00:17:29,280
and then you specify gender based like

00:17:26,440 --> 00:17:31,660
how we were associating teachers and

00:17:29,280 --> 00:17:34,180
secretaries okay female look at this is

00:17:31,660 --> 00:17:35,830
like meal okay that is common can be

00:17:34,180 --> 00:17:39,820
done by both of them something like that

00:17:35,830 --> 00:17:42,940
and then what you do is after that when

00:17:39,820 --> 00:17:44,800
you've given a biased data you extract

00:17:42,940 --> 00:17:47,320
okay all the information the occupation

00:17:44,800 --> 00:17:49,240
everything has been extracted then you

00:17:47,320 --> 00:17:52,120
interchange it and you check the

00:17:49,240 --> 00:17:53,779
plausibility of it so you are

00:17:52,120 --> 00:17:56,210
interchange the infant

00:17:53,779 --> 00:17:58,849
has been interchanging based on the

00:17:56,210 --> 00:18:03,340
gender and you check how accurate is it

00:17:58,849 --> 00:18:06,259
and that's how your bias can be removed

00:18:03,340 --> 00:18:08,389
so generally what we do is we don't

00:18:06,259 --> 00:18:11,629
follow this step we don't follow

00:18:08,389 --> 00:18:15,529
anything from your we don't follow it

00:18:11,629 --> 00:18:17,570
before although the first flowchart in

00:18:15,529 --> 00:18:19,909
the second one but we don't go generally

00:18:17,570 --> 00:18:22,039
for the third one and then unless we

00:18:19,909 --> 00:18:23,029
work it out for both the genders how

00:18:22,039 --> 00:18:26,269
would we know that there is a

00:18:23,029 --> 00:18:29,389
possibility for you know females also to

00:18:26,269 --> 00:18:31,729
be in that part otherwise we'll again

00:18:29,389 --> 00:18:32,809
have that Google Translation problem

00:18:31,729 --> 00:18:35,659
statement that was there we'll witness

00:18:32,809 --> 00:18:38,509
more examples like that so that's one

00:18:35,659 --> 00:18:41,089
and this is extremely important because

00:18:38,509 --> 00:18:43,940
this is how that gender thing would be

00:18:41,089 --> 00:18:45,499
removed if we change the gender you'll

00:18:43,940 --> 00:18:47,629
understand okay so there is a

00:18:45,499 --> 00:18:52,580
possibility that even a female can be a

00:18:47,629 --> 00:18:54,649
police cop so that's very important now

00:18:52,580 --> 00:18:57,529
how does this algorithm actually work

00:18:54,649 --> 00:18:59,719
the decock teller you can search it

00:18:57,529 --> 00:19:02,659
online also there's a paper I'll show

00:18:59,719 --> 00:19:05,450
the reference also so you generally pre

00:19:02,659 --> 00:19:08,899
process your data you generate the word

00:19:05,450 --> 00:19:11,119
vectors after doing after performing

00:19:08,899 --> 00:19:13,849
this you extract the information and

00:19:11,119 --> 00:19:16,009
then when you classify what you try to

00:19:13,849 --> 00:19:18,440
do it you try to do it in both the ways

00:19:16,009 --> 00:19:21,019
you interchange the information and

00:19:18,440 --> 00:19:22,759
check how well is it working and after

00:19:21,019 --> 00:19:24,769
this is generally for the movie data

00:19:22,759 --> 00:19:27,169
because that's what I've been focusing

00:19:24,769 --> 00:19:29,839
more on and then you detect bias based

00:19:27,169 --> 00:19:31,700
on the actions okay so if it was working

00:19:29,839 --> 00:19:33,710
for both of them then it's fine if it

00:19:31,700 --> 00:19:37,219
was not working then there's a bias then

00:19:33,710 --> 00:19:40,729
you work on it and then after working on

00:19:37,219 --> 00:19:44,899
it your bias has been removed so if you

00:19:40,729 --> 00:19:47,359
see the gender predictions the accuracy

00:19:44,899 --> 00:19:50,659
for the movie data set which was there

00:19:47,359 --> 00:19:52,639
it was following the KN and machine

00:19:50,659 --> 00:19:57,229
learning algorithm if you guys are aware

00:19:52,639 --> 00:20:01,039
of it anybody has any doubt okay

00:19:57,229 --> 00:20:03,769
yeah so the knn algorithm was working so

00:20:01,039 --> 00:20:07,190
for k equal to one it was like there was

00:20:03,769 --> 00:20:09,049
a there's a huge inclination angle but

00:20:07,190 --> 00:20:13,849
for the green one it was initially

00:20:09,049 --> 00:20:18,229
steady but in the end it it went up the

00:20:13,849 --> 00:20:20,779
orange one where K was actually 50 the

00:20:18,229 --> 00:20:22,729
orange one was constant it was not

00:20:20,779 --> 00:20:24,799
constant but didn't it would it steadily

00:20:22,729 --> 00:20:30,889
increased in the in the end there was a

00:20:24,799 --> 00:20:33,950
a pill point so it the maximum accuracy

00:20:30,889 --> 00:20:37,789
that has been achieved is when K is

00:20:33,950 --> 00:20:40,099
equal to one so that's that's the

00:20:37,789 --> 00:20:43,489
maximum accuracy you can get for your

00:20:40,099 --> 00:20:47,809
bias removal for the movie data set and

00:20:43,489 --> 00:20:50,419
this is for the training data so as as

00:20:47,809 --> 00:20:52,489
as accurate you can work on your

00:20:50,419 --> 00:20:54,649
training data it's far more because the

00:20:52,489 --> 00:20:56,749
testing data the accuracy even decreases

00:20:54,649 --> 00:21:00,649
so you need to really work on the

00:20:56,749 --> 00:21:02,960
training data to remove the bias so this

00:21:00,649 --> 00:21:04,970
is the D biasing system I'm following

00:21:02,960 --> 00:21:09,139
the same example here as well

00:21:04,970 --> 00:21:11,200
so Rohith is an aspiring singer who

00:21:09,139 --> 00:21:17,570
works as a salesman in a car showroom

00:21:11,200 --> 00:21:19,669
run by mr. Malik one day he needs Sonia

00:21:17,570 --> 00:21:22,039
daughter of mr. Saxena

00:21:19,669 --> 00:21:25,369
when he goes to deliver the car at a

00:21:22,039 --> 00:21:28,909
place on her birthday now here if you

00:21:25,369 --> 00:21:33,649
see how basically decock Taylor has been

00:21:28,909 --> 00:21:37,519
working it's basically that he is that

00:21:33,649 --> 00:21:40,669
Rohit he because it's a Salesman it has

00:21:37,519 --> 00:21:43,369
already selected it as that he's a

00:21:40,669 --> 00:21:46,339
singer he wants to deliver he has

00:21:43,369 --> 00:21:49,729
aspires like he has desires so all of

00:21:46,339 --> 00:21:51,679
that has been labeled to the blue if you

00:21:49,729 --> 00:21:53,779
can see this tree they've been assigned

00:21:51,679 --> 00:21:56,839
the blue label which is figuring that

00:21:53,779 --> 00:21:58,849
you know all this is somewhat labeled

00:21:56,839 --> 00:22:03,499
and characterized towards what a guy

00:21:58,849 --> 00:22:06,039
would like or a guy would be the pink

00:22:03,499 --> 00:22:08,989
one where it's indicating Sonia is

00:22:06,039 --> 00:22:12,979
actually just giving that she is a girl

00:22:08,989 --> 00:22:15,200
because she's a daughter so only that

00:22:12,979 --> 00:22:19,159
much information has been received so

00:22:15,200 --> 00:22:20,010
when you D buys this information now

00:22:19,159 --> 00:22:24,090
what does happen

00:22:20,010 --> 00:22:28,860
is Sonia sorry Rohit she it has not

00:22:24,090 --> 00:22:31,620
taken it as she is the saleswoman now it

00:22:28,860 --> 00:22:36,030
has changed the Jane changed the gender

00:22:31,620 --> 00:22:40,350
like the work in terms of gender and it

00:22:36,030 --> 00:22:43,710
says that Sonia is an aspiring sorry

00:22:40,350 --> 00:22:45,780
Rohit is an aspiring singer who works as

00:22:43,710 --> 00:22:48,120
a saleswoman so now that name remains

00:22:45,780 --> 00:22:50,880
the same but the characteristic that was

00:22:48,120 --> 00:22:53,190
all about the guy has been changed for a

00:22:50,880 --> 00:22:55,110
girl so here the information has been

00:22:53,190 --> 00:22:59,340
changed from sales man to a saleswoman

00:22:55,110 --> 00:23:02,970
and you know that she has dreams and she

00:22:59,340 --> 00:23:07,560
wants to aspire still remains of that of

00:23:02,970 --> 00:23:09,330
a boy because that again is not changed

00:23:07,560 --> 00:23:12,150
the only information that has been taken

00:23:09,330 --> 00:23:13,470
out is that she is Roy as a saleswoman

00:23:12,150 --> 00:23:15,660
so because of sales woman

00:23:13,470 --> 00:23:17,820
it has taken it to be that Rohith is a

00:23:15,660 --> 00:23:19,110
woman but here again the other

00:23:17,820 --> 00:23:21,090
information that she's an aspiring

00:23:19,110 --> 00:23:21,750
singer or whatever has not been

00:23:21,090 --> 00:23:26,600
converted

00:23:21,750 --> 00:23:30,660
so there's another bias here also and

00:23:26,600 --> 00:23:34,140
similarly with Sonia that daughter off

00:23:30,660 --> 00:23:36,930
has been swapped to son off that

00:23:34,140 --> 00:23:39,510
information has changed Sonia anyway

00:23:36,930 --> 00:23:41,220
didn't have much labels she just has it

00:23:39,510 --> 00:23:43,980
you know she is a daughter which has not

00:23:41,220 --> 00:23:45,960
been changed to a boy to son off but

00:23:43,980 --> 00:23:49,350
you're like you see the deliverer and

00:23:45,960 --> 00:23:53,790
the aspiring feature has not been

00:23:49,350 --> 00:23:55,860
changed so that this is also again with

00:23:53,790 --> 00:23:58,590
D biased ex there's another there's

00:23:55,860 --> 00:24:02,880
still a bias present in that it's not

00:23:58,590 --> 00:24:05,910
yet biased completely so this is the

00:24:02,880 --> 00:24:08,700
reference you guys can I think the

00:24:05,910 --> 00:24:11,250
present the Peabody would be shared I

00:24:08,700 --> 00:24:15,330
believe yeah so you can go through these

00:24:11,250 --> 00:24:15,990
papers and you can you know refer it

00:24:15,330 --> 00:24:18,240
from here

00:24:15,990 --> 00:24:20,630
it's basically research paper I can show

00:24:18,240 --> 00:24:20,630
you also

00:24:31,300 --> 00:24:35,850
oh it's not selecting the entire thing

00:24:49,639 --> 00:24:53,179
there's a space

00:24:57,700 --> 00:25:00,390
oh yeah

00:25:08,710 --> 00:25:17,070
oh it's not taking that thing completely

00:25:13,419 --> 00:25:33,149
okay I'm not too sure it's not working

00:25:17,070 --> 00:25:33,149
sorry okay oh yeah

00:25:37,370 --> 00:25:44,790
finally so okay yeah you can follow

00:25:41,580 --> 00:25:47,310
these research papers also to know more

00:25:44,790 --> 00:25:51,090
how to remove bias because it has its

00:25:47,310 --> 00:25:53,550
this topic was basically on basically on

00:25:51,090 --> 00:25:56,850
gender bias because we were figuring on

00:25:53,550 --> 00:25:58,680
the basic level 101 so that's what it is

00:25:56,850 --> 00:26:05,280
seen it's a really good paper you can

00:25:58,680 --> 00:26:07,050
you know refer it and yeah there's one

00:26:05,280 --> 00:26:10,640
more publication you can see that as

00:26:07,050 --> 00:26:10,640
well wait I'll open that also

00:26:20,420 --> 00:26:26,770
I think this one should work properly Oh

00:26:24,710 --> 00:26:26,770
No

00:26:30,470 --> 00:26:34,010
not sweet

00:26:40,020 --> 00:26:43,440
where's this book

00:26:50,010 --> 00:26:55,960
yeah so this is also really it's

00:26:52,990 --> 00:26:59,020
basically from Ukraine and Russia you

00:26:55,960 --> 00:27:01,240
can follow this link as well it's also

00:26:59,020 --> 00:27:03,340
very handy on also for the data set it's

00:27:01,240 --> 00:27:05,020
available here you can see that I've

00:27:03,340 --> 00:27:09,610
mentioned that in the presentation as

00:27:05,020 --> 00:27:11,140
well so if you want to reach out to me

00:27:09,610 --> 00:27:14,080
you can that's my Twitter handle

00:27:11,140 --> 00:27:15,880
LinkedIn you can mail me I have a

00:27:14,080 --> 00:27:18,430
youtube channel also for various

00:27:15,880 --> 00:27:20,380
technocrats to come on board and discuss

00:27:18,430 --> 00:27:22,660
various problems that they've been

00:27:20,380 --> 00:27:25,660
facing and how exactly they gained

00:27:22,660 --> 00:27:29,050
recognition so I generally what I do is

00:27:25,660 --> 00:27:30,760
I do a Q&A series where I call a lot of

00:27:29,050 --> 00:27:32,530
great technocrats on my channel and I

00:27:30,760 --> 00:27:33,700
ask them questions how exactly we went

00:27:32,530 --> 00:27:35,620
there and what are the scholarship

00:27:33,700 --> 00:27:37,960
opportunities how did you apply maybe

00:27:35,620 --> 00:27:40,570
somebody's working at who or anywhere so

00:27:37,960 --> 00:27:42,310
what are the resources to follow so if

00:27:40,570 --> 00:27:45,550
you're interested you can also warden to

00:27:42,310 --> 00:27:57,070
yourself as a speaker and yeah that's it

00:27:45,550 --> 00:27:58,780
thank you we still have a bit of time

00:27:57,070 --> 00:28:00,640
for questions so if you have any

00:27:58,780 --> 00:28:02,790
questions about this amazing topic

00:28:00,640 --> 00:28:08,020
please go ahead

00:28:02,790 --> 00:28:12,700
I checked with Bangla and it is the same

00:28:08,020 --> 00:28:14,860
so like Bengali language in English -

00:28:12,700 --> 00:28:17,380
Bengali Bengali - - English and I

00:28:14,860 --> 00:28:20,320
realize it is the same even when Bengali

00:28:17,380 --> 00:28:25,800
is an almost general gender-neutral

00:28:20,320 --> 00:28:25,800
language so yeah this is an issue now

00:28:36,830 --> 00:28:40,770
so we generally don't change the

00:28:39,150 --> 00:28:42,570
interpol arity of these statements and

00:28:40,770 --> 00:28:44,280
it's like it's very common you try out

00:28:42,570 --> 00:28:46,200
with other languages also and I'm sure

00:28:44,280 --> 00:28:50,510
they're going to work like this one it's

00:28:46,200 --> 00:28:52,410
going to fail I which was familiar for

00:28:50,510 --> 00:28:54,930
people who are to understand because

00:28:52,410 --> 00:28:56,160
then Hindi languages the the letters and

00:28:54,930 --> 00:29:03,360
alphabets would be difficult to

00:28:56,160 --> 00:29:07,050
interpret by them but Hindi and other

00:29:03,360 --> 00:29:09,540
most other languages already have gender

00:29:07,050 --> 00:29:11,190
in their sentences so I would probably

00:29:09,540 --> 00:29:13,470
understand if machine learning

00:29:11,190 --> 00:29:16,530
algorithms would take it but Bangla is

00:29:13,470 --> 00:29:20,940
an almost gender-neutral language so I

00:29:16,530 --> 00:29:22,290
did not expect that like you we

00:29:20,940 --> 00:29:24,420
generally say that you know it's

00:29:22,290 --> 00:29:26,790
specific for a man you know if you say

00:29:24,420 --> 00:29:28,680
that I am going it's it's probable that

00:29:26,790 --> 00:29:30,930
I am going would be set by a boy because

00:29:28,680 --> 00:29:34,530
the verbs are like that but even in

00:29:30,930 --> 00:29:36,690
Hindi when you like go for like more

00:29:34,530 --> 00:29:38,190
complex sentences not just I am going

00:29:36,690 --> 00:29:40,260
because when you train your model you

00:29:38,190 --> 00:29:41,460
just don't you know input these

00:29:40,260 --> 00:29:43,500
information you give a lot of

00:29:41,460 --> 00:29:54,320
information so those informations are

00:29:43,500 --> 00:29:57,300
also not freely bias-free yeah thank you

00:29:54,320 --> 00:30:00,450
so you mentioned that the the model you

00:29:57,300 --> 00:30:02,820
are using that is based you are trying

00:30:00,450 --> 00:30:05,280
to remove the bias based on other text

00:30:02,820 --> 00:30:06,690
right from news text in this example but

00:30:05,280 --> 00:30:09,660
isn't that really dangerous because

00:30:06,690 --> 00:30:11,790
actually all texts have these biases so

00:30:09,660 --> 00:30:16,710
you're actually trying to remove biases

00:30:11,790 --> 00:30:18,750
with bias text it's not that's what I

00:30:16,710 --> 00:30:20,880
said it's not it has not completely

00:30:18,750 --> 00:30:23,580
removed the bias if you saw that example

00:30:20,880 --> 00:30:26,580
the deliver thing and this aspiring

00:30:23,580 --> 00:30:28,830
thing was still in blue okay so it's not

00:30:26,580 --> 00:30:30,720
completely removing it but it's giving

00:30:28,830 --> 00:30:33,390
an intuition that it's possible with the

00:30:30,720 --> 00:30:35,550
other person as well so it's actually

00:30:33,390 --> 00:30:37,470
working very well obviously there are

00:30:35,550 --> 00:30:39,930
few points which can still be worked on

00:30:37,470 --> 00:30:42,060
like you said that because this was

00:30:39,930 --> 00:30:45,150
basically on a particular data set

00:30:42,060 --> 00:30:48,360
it has not been universally accepted so

00:30:45,150 --> 00:30:50,180
I think I am still working on it but for

00:30:48,360 --> 00:30:52,980
now this is just based on one product

00:30:50,180 --> 00:30:54,480
but yeah it's not completely removing

00:30:52,980 --> 00:30:56,400
and it's still working on that complete

00:30:54,480 --> 00:30:57,600
pattern I think they will never work on

00:30:56,400 --> 00:31:01,440
the complete thing because then again

00:30:57,600 --> 00:31:05,100
it's not right for any an ml model you

00:31:01,440 --> 00:31:06,570
need to introduce bias also at times so

00:31:05,100 --> 00:31:12,500
that's where the blue thing was again

00:31:06,570 --> 00:31:12,500
still there yeah anything else yeah

00:31:16,490 --> 00:31:24,360
would you say that this this approach to

00:31:20,130 --> 00:31:27,000
reduce bias is more applicable to to

00:31:24,360 --> 00:31:29,430
certain problems because we've made the

00:31:27,000 --> 00:31:31,980
experience led for a lot of models if

00:31:29,430 --> 00:31:34,140
you reduce the bias in the in the data

00:31:31,980 --> 00:31:37,230
the model gets worse at predicting

00:31:34,140 --> 00:31:40,100
because the data you get in still will

00:31:37,230 --> 00:31:43,160
be biased so if you predict the job of

00:31:40,100 --> 00:31:47,100
film your mother is gonna get worse

00:31:43,160 --> 00:31:49,290
where I would see this to be very useful

00:31:47,100 --> 00:31:52,380
this thing's way it would be really

00:31:49,290 --> 00:31:55,680
horrible if if this bias creeps in like

00:31:52,380 --> 00:31:57,750
if you want to have a machine learning

00:31:55,680 --> 00:32:00,990
algorithm that predicts something for

00:31:57,750 --> 00:32:03,390
like hiring decisions or something so

00:32:00,990 --> 00:32:05,670
would you say that certain problems are

00:32:03,390 --> 00:32:11,880
more prone to this or where this would

00:32:05,670 --> 00:32:14,910
be more useful thank you for the

00:32:11,880 --> 00:32:17,100
question so if in the pre in the

00:32:14,910 --> 00:32:18,810
beginning I showed that you know Google

00:32:17,100 --> 00:32:21,420
and Facebook all of them had already

00:32:18,810 --> 00:32:24,420
they had developed a model which was not

00:32:21,420 --> 00:32:26,400
accepting or was not giving that much of

00:32:24,420 --> 00:32:29,190
pay scale no we're not accepting them as

00:32:26,400 --> 00:32:31,170
recruiters or anything if it were a girl

00:32:29,190 --> 00:32:34,920
or if it was you know as a female

00:32:31,170 --> 00:32:37,520
candidate so generally this bias is

00:32:34,920 --> 00:32:39,330
mainly where you know there has been

00:32:37,520 --> 00:32:42,440
applications where there's been a

00:32:39,330 --> 00:32:45,450
rejection based on gender it's not

00:32:42,440 --> 00:32:47,430
universally you know it's not

00:32:45,450 --> 00:32:50,640
universally accepted for all the

00:32:47,430 --> 00:32:54,330
applications it's it's restricted to

00:32:50,640 --> 00:32:55,650
gender based approaches only for now but

00:32:54,330 --> 00:32:58,140
like you said you

00:32:55,650 --> 00:33:01,260
it can really work well you know maybe

00:32:58,140 --> 00:33:03,390
with with cabs and all and we can

00:33:01,260 --> 00:33:06,510
actually say you know if that's the area

00:33:03,390 --> 00:33:09,180
which is not very safe for women then

00:33:06,510 --> 00:33:10,800
you know give them priority maybe there

00:33:09,180 --> 00:33:13,140
should be a service which should be you

00:33:10,800 --> 00:33:15,000
know readily available or something like

00:33:13,140 --> 00:33:17,910
that so these biases are introduced to

00:33:15,000 --> 00:33:20,970
focus more on how to prevent the imagery

00:33:17,910 --> 00:33:26,730
of of gender which is not giving that

00:33:20,970 --> 00:33:34,350
much of coverage anything else anybody

00:33:26,730 --> 00:33:37,530
has a question yes so this means all our

00:33:34,350 --> 00:33:39,840
data sets must be clean this way more or

00:33:37,530 --> 00:33:42,300
less have you an example which is the

00:33:39,840 --> 00:33:45,540
biggest relevant data set which has been

00:33:42,300 --> 00:33:49,830
already cleaned or is it just in the

00:33:45,540 --> 00:33:52,230
future like I said you need to make a

00:33:49,830 --> 00:33:54,450
data set because not data sets are very

00:33:52,230 --> 00:33:56,190
raw they're not cleaned even if you're

00:33:54,450 --> 00:33:58,980
creating your own you need to clean that

00:33:56,190 --> 00:34:01,080
up so you need to work on that part

00:33:58,980 --> 00:34:03,360
manually and then you need to obviously

00:34:01,080 --> 00:34:05,820
you can apply some ml models for that

00:34:03,360 --> 00:34:07,530
but it the cleaning part and all the

00:34:05,820 --> 00:34:10,429
considerations has to be done manually

00:34:07,530 --> 00:34:13,470
considering all the options available

00:34:10,429 --> 00:34:15,810
yeah because there hasn't been a system

00:34:13,470 --> 00:34:18,390
to develop the cleaning there has been a

00:34:15,810 --> 00:34:22,130
system to detect after cleaning but the

00:34:18,390 --> 00:34:22,130
development part is not yet considered

00:34:23,060 --> 00:34:31,530
anybody else has a question I think

00:34:28,409 --> 00:34:33,659
we're done so thank you so you can mail

00:34:31,530 --> 00:34:35,370
me if there is still some question left

00:34:33,659 --> 00:34:38,370
so thank you everyone

00:34:35,370 --> 00:34:40,230
I actually flow down from India to give

00:34:38,370 --> 00:34:41,940
this talk and it's been a great

00:34:40,230 --> 00:34:43,340
experience at boiling buzzwords so thank

00:34:41,940 --> 00:34:46,859
you so much

00:34:43,340 --> 00:34:46,859

YouTube URL: https://www.youtube.com/watch?v=jOXGvLsnYC8


