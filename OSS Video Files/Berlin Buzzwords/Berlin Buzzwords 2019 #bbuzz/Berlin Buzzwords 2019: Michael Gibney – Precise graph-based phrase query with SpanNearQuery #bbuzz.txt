Title: Berlin Buzzwords 2019: Michael Gibney â€“ Precise graph-based phrase query with SpanNearQuery #bbuzz
Publication date: 2019-06-18
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	The current implementation of SpanNearQuery in Apache Lucene sacrifices precision and completeness in favor of performance. This tradeoff significantly limits Lucene's usefulness for some potential high-value use cases. This talk introduces a stable patch that makes SpanQuery matching precise and complete, while maintaining performance comparable to that of the extant implementation.

The patch will be discussed in the context of the issue LUCENE-7398, describing the central problem and how it differs from the problem addressed by the new IntervalsSource API.

We will discuss details of the patch implementation, in an effort to familiarize the audience with techniques involved (and hopefully inspire further innovation). Details covered will include:

1. The main word-lattice data structure (linked, circular, resizable-array-backed, 2-dimensional queue with stable node references and support for efficient binary seek, serial traversal, and arbitrary node removal).
2. Performance/GC management for linked data structures (as opposed to the array-based data structures that are more common in the Lucene codebase)
3. Recording in the index: positionLength, nextStartPosition lookahead, and information about possible decrease of endPosition (de-"sausagization" of the index)

Read more:
https://2019.berlinbuzzwords.de/19/session/complete-precise-graph-based-phrase-query-spannearquery

About Michael Gibney:
https://2019.berlinbuzzwords.de/users/michael-gibney

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:07,480 --> 00:00:15,000
thank you all so much for

00:00:09,410 --> 00:00:17,070
for coming out so the tide

00:00:15,000 --> 00:00:19,560
included a specific call-out to Spain

00:00:17,070 --> 00:00:22,230
near query the implementation that I'm

00:00:19,560 --> 00:00:25,490
going to be talking about has was was

00:00:22,230 --> 00:00:29,130
done with span queries but as I'll be

00:00:25,490 --> 00:00:30,869
returning to as kind of a theme a lot of

00:00:29,130 --> 00:00:32,129
the issues that I'm going to be

00:00:30,869 --> 00:00:34,170
discussing about are actually more

00:00:32,129 --> 00:00:36,480
general than that and have to do with

00:00:34,170 --> 00:00:39,540
phrase query or proximity query in in

00:00:36,480 --> 00:00:41,940
Lucene I'm going to start with a brief

00:00:39,540 --> 00:00:44,790
overview of the leucine token stream

00:00:41,940 --> 00:00:46,800
structure the structure is what makes it

00:00:44,790 --> 00:00:51,960
possible to express the richness of

00:00:46,800 --> 00:00:55,320
indexed and query to input so Lucien's

00:00:51,960 --> 00:00:57,629
token stream API was initially assumed a

00:00:55,320 --> 00:01:01,530
more or less linear stream of tokens so

00:00:57,629 --> 00:01:03,180
each token was assumed to have a

00:01:01,530 --> 00:01:05,810
position length of 1 and they were

00:01:03,180 --> 00:01:07,860
assumed to progress one after another

00:01:05,810 --> 00:01:11,910
since the addition of the position

00:01:07,860 --> 00:01:13,850
lengths attribute in leucine 3.6 it's

00:01:11,910 --> 00:01:18,119
possible to represent tokens as

00:01:13,850 --> 00:01:21,630
branching graph that that can branch and

00:01:18,119 --> 00:01:25,320
merge back together uses of the token

00:01:21,630 --> 00:01:27,330
stream API at index time it's used to

00:01:25,320 --> 00:01:30,030
determine the tokens and token structure

00:01:27,330 --> 00:01:33,240
serialize to the index and a query time

00:01:30,030 --> 00:01:35,909
it's used by various query parsers to

00:01:33,240 --> 00:01:39,000
construct the the structure of queries

00:01:35,909 --> 00:01:40,290
that are run against the index so we

00:01:39,000 --> 00:01:44,250
have an example here of OLED

00:01:40,290 --> 00:01:48,540
manufacturing which would be indexed or

00:01:44,250 --> 00:01:50,549
parsed for query purposes as organic

00:01:48,540 --> 00:01:55,740
light-emitting diode manufacturing or

00:01:50,549 --> 00:02:02,909
organic LED manufacturing or OLED

00:01:55,740 --> 00:02:04,259
manufacturing so talk about the latent

00:02:02,909 --> 00:02:05,970
potential of the position length

00:02:04,259 --> 00:02:09,540
attribute it was introduced in leucine

00:02:05,970 --> 00:02:12,930
3.6 but position length attribute is

00:02:09,540 --> 00:02:15,480
that information is discarded at index

00:02:12,930 --> 00:02:17,850
time it's not written into the index and

00:02:15,480 --> 00:02:22,920
so that assumption of a linear token

00:02:17,850 --> 00:02:26,370
stream persists for indexed content I

00:02:22,920 --> 00:02:28,870
have a nice picture of braided rivers

00:02:26,370 --> 00:02:30,790
which look cool and are basically stream

00:02:28,870 --> 00:02:33,790
that separate apart and come back

00:02:30,790 --> 00:02:35,560
together so it's possible to do this

00:02:33,790 --> 00:02:39,849
without losing information about token

00:02:35,560 --> 00:02:41,230
adjacency and different branches so to

00:02:39,849 --> 00:02:43,209
fully leverage the structure of the

00:02:41,230 --> 00:02:46,030
token stream API at indexing time would

00:02:43,209 --> 00:02:48,700
require three things to happen more or

00:02:46,030 --> 00:02:50,290
less in concert with each other we would

00:02:48,700 --> 00:02:52,599
have to store the position lengths in

00:02:50,290 --> 00:02:54,640
the index augment the postings API to

00:02:52,599 --> 00:02:57,310
expose position length to index readers

00:02:54,640 --> 00:03:00,340
and update the query implementations to

00:02:57,310 --> 00:03:02,260
leverage index position links as exposed

00:03:00,340 --> 00:03:04,109
through the postings API it's been

00:03:02,260 --> 00:03:10,890
pointed out that the last of these is

00:03:04,109 --> 00:03:14,019
the most challenging and has been a

00:03:10,890 --> 00:03:16,120
problem and causes problems for word

00:03:14,019 --> 00:03:20,709
delimiter graph filter used at index

00:03:16,120 --> 00:03:24,849
time etc so leucine 739 8 is the issue

00:03:20,709 --> 00:03:27,280
that it forms the umbrella for a lot of

00:03:24,849 --> 00:03:29,739
what I'm talking about here it's the

00:03:27,280 --> 00:03:32,409
issue is nominally about nested span

00:03:29,739 --> 00:03:35,290
queries but the issue is actually more

00:03:32,409 --> 00:03:37,750
general than that in the absence of

00:03:35,290 --> 00:03:41,620
index position links nested span queries

00:03:37,750 --> 00:03:43,269
were are the the primary and perhaps the

00:03:41,620 --> 00:03:46,450
only way of generating or encountering

00:03:43,269 --> 00:03:47,799
variable in sub-clauses so the issue

00:03:46,450 --> 00:03:51,840
that we're really talking about here is

00:03:47,799 --> 00:03:54,370
variable length subclasses and where he

00:03:51,840 --> 00:03:57,430
phrase queries over those proximity

00:03:54,370 --> 00:03:59,319
queries so we're gonna talk about an

00:03:57,430 --> 00:04:00,639
implementation with span near query but

00:03:59,319 --> 00:04:03,609
the issues are more general than that

00:04:00,639 --> 00:04:06,040
and they apply also to any phrase or

00:04:03,609 --> 00:04:11,260
proximity query implementation as well

00:04:06,040 --> 00:04:14,430
as to the interval source API so for

00:04:11,260 --> 00:04:17,620
purposes of simplicity and generality

00:04:14,430 --> 00:04:20,620
we're gonna focus on the position order

00:04:17,620 --> 00:04:23,760
abstraction of the spans API so not the

00:04:20,620 --> 00:04:26,889
entire spans API necessarily but

00:04:23,760 --> 00:04:31,479
basically the the spans API specifies

00:04:26,889 --> 00:04:34,780
that positions will be exposed in order

00:04:31,479 --> 00:04:38,620
in increasing order by start position of

00:04:34,780 --> 00:04:41,470
a token and then within that secondary

00:04:38,620 --> 00:04:44,080
sort on end position within a token

00:04:41,470 --> 00:04:46,870
so the span positions are advanced

00:04:44,080 --> 00:04:47,410
forward only and within a given - oh I

00:04:46,870 --> 00:04:51,460
already said that

00:04:47,410 --> 00:04:53,850
ah so we make no other assumptions the

00:04:51,460 --> 00:05:00,670
end position might be arbitrarily larger

00:04:53,850 --> 00:05:04,080
and the start position and so we might

00:05:00,670 --> 00:05:06,940
have actually a decreasing end position

00:05:04,080 --> 00:05:10,900
which introduces the possibility for new

00:05:06,940 --> 00:05:15,010
matches so and this helps us avoid the

00:05:10,900 --> 00:05:18,430
special case avoid the the trap of sort

00:05:15,010 --> 00:05:20,440
of considering simple term spans to be a

00:05:18,430 --> 00:05:27,070
special case with a static implicit

00:05:20,440 --> 00:05:28,780
position length of 1 so this ordering

00:05:27,070 --> 00:05:31,720
happens to be a part of the spans API

00:05:28,780 --> 00:05:33,600
but it's more generally useful because

00:05:31,720 --> 00:05:35,830
it's kind of the most sensible way to

00:05:33,600 --> 00:05:37,360
provide an absolute ordering for

00:05:35,830 --> 00:05:43,150
something that has a start position and

00:05:37,360 --> 00:05:45,540
an end position so this is a little

00:05:43,150 --> 00:05:49,419
picture that references the previous

00:05:45,540 --> 00:05:51,450
diagram of OLED manufacturing and shows

00:05:49,419 --> 00:05:54,580
how it's currently recorded in the index

00:05:51,450 --> 00:05:57,400
so the position length is discarded so

00:05:54,580 --> 00:06:03,130
all of the terms appear but the

00:05:57,400 --> 00:06:06,000
adjacency relationships are lost so this

00:06:03,130 --> 00:06:09,130
can yield surprising results for

00:06:06,000 --> 00:06:11,950
proximity based queries when graph token

00:06:09,130 --> 00:06:14,620
filters are applied at index time in the

00:06:11,950 --> 00:06:18,280
example above partially redundant

00:06:14,620 --> 00:06:20,860
synthetic phrases like OLED LED emitting

00:06:18,280 --> 00:06:23,710
diode manufacturing would perfectly

00:06:20,860 --> 00:06:25,330
match phrase queries whereas a query for

00:06:23,710 --> 00:06:27,820
the original text in the document

00:06:25,330 --> 00:06:30,900
OLED manufacturing would only match with

00:06:27,820 --> 00:06:33,160
a slop greater than or equal to 3 and

00:06:30,900 --> 00:06:34,600
you can kind of see that because the

00:06:33,160 --> 00:06:36,550
OLED which appears in the original

00:06:34,600 --> 00:06:40,000
document is actually separated from

00:06:36,550 --> 00:06:42,700
manufacturing the current recommended

00:06:40,000 --> 00:06:44,560
practice of performing synonym and word

00:06:42,700 --> 00:06:46,650
delimiter graph filter expansion only at

00:06:44,560 --> 00:06:50,080
query time does work around this issue

00:06:46,650 --> 00:06:52,060
but this practice is problematic because

00:06:50,080 --> 00:06:53,590
of the importance of context in

00:06:52,060 --> 00:06:56,020
determining

00:06:53,590 --> 00:06:58,180
in determining appropriate token stream

00:06:56,020 --> 00:07:00,160
manipulations and there's a lot of

00:06:58,180 --> 00:07:01,930
context that's available at index time

00:07:00,160 --> 00:07:07,930
that is simply not available at query

00:07:01,930 --> 00:07:10,810
time so this is also an example of a

00:07:07,930 --> 00:07:14,860
case where decreasing end position can

00:07:10,810 --> 00:07:17,230
cause matches to not be found so match

00:07:14,860 --> 00:07:22,330
one would not be found until advancing

00:07:17,230 --> 00:07:24,160
to position D in in clause two and then

00:07:22,330 --> 00:07:26,470
once you're at position D in Clause two

00:07:24,160 --> 00:07:30,130
you have to be able to backtrack to

00:07:26,470 --> 00:07:32,250
position C in Clause oh sorry zero-based

00:07:30,130 --> 00:07:32,250
indexing

00:07:32,639 --> 00:07:37,960
you have to be able to backtrack to

00:07:34,960 --> 00:07:40,840
position C when your end position

00:07:37,960 --> 00:07:45,070
decreases so it can get it can get

00:07:40,840 --> 00:07:46,750
tricky so as a replacement for span near

00:07:45,070 --> 00:07:48,760
query the current implementation of the

00:07:46,750 --> 00:07:51,510
interval source API essentially has

00:07:48,760 --> 00:07:57,720
similar properties to multi phrase query

00:07:51,510 --> 00:08:00,100
which falls back on an assumption that

00:07:57,720 --> 00:08:03,240
sorry falls back on enumerate all

00:08:00,100 --> 00:08:06,340
possible paths through a given query so

00:08:03,240 --> 00:08:08,169
that works in some ways but it also

00:08:06,340 --> 00:08:11,050
introduces the potential for exponential

00:08:08,169 --> 00:08:12,580
query expansion which is which is

00:08:11,050 --> 00:08:14,320
problematic particularly if you're

00:08:12,580 --> 00:08:20,590
interested in heavily leveraging

00:08:14,320 --> 00:08:25,660
synonyms at index time so just checking

00:08:20,590 --> 00:08:27,570
on okay I'm doing okay time wise so for

00:08:25,660 --> 00:08:30,520
the foundation for this implementation

00:08:27,570 --> 00:08:34,570
we have a wrapper that implements the

00:08:30,520 --> 00:08:39,190
spans interface which can be wrapped

00:08:34,570 --> 00:08:41,380
around any spans implementation that

00:08:39,190 --> 00:08:43,599
will support backtracking efficiently

00:08:41,380 --> 00:08:50,170
without buffering any more positions

00:08:43,599 --> 00:08:52,990
than necessary so the two key method

00:08:50,170 --> 00:08:56,500
signatures that that I'll call out here

00:08:52,990 --> 00:08:59,380
are the next match which includes the

00:08:56,500 --> 00:09:02,260
concept of this is as a replacement for

00:08:59,380 --> 00:09:05,110
an extra next start position which is

00:09:02,260 --> 00:09:07,380
how you advance through spans so this

00:09:05,110 --> 00:09:09,750
next match is basically the same thing

00:09:07,380 --> 00:09:12,860
except it includes information about a

00:09:09,750 --> 00:09:16,140
hard minimum start a soft minimum start

00:09:12,860 --> 00:09:18,900
start ceiling and a minimum end so the

00:09:16,140 --> 00:09:20,550
idea is to basically abstract all the

00:09:18,900 --> 00:09:22,590
information that's necessary to

00:09:20,550 --> 00:09:25,620
determine whether you need to buffer

00:09:22,590 --> 00:09:28,920
positions when you advance or whether

00:09:25,620 --> 00:09:31,800
positions can be discarded and then the

00:09:28,920 --> 00:09:34,710
reset obviously or obviously provides

00:09:31,800 --> 00:09:36,510
the backtracking capability so you can

00:09:34,710 --> 00:09:39,150
say I want to go back to this start

00:09:36,510 --> 00:09:42,810
position and and proceed from there so

00:09:39,150 --> 00:09:47,730
you can replay positions I think it

00:09:42,810 --> 00:09:54,150
covered these yeah and so the start

00:09:47,730 --> 00:09:58,740
ceiling allows you to I don't get into

00:09:54,150 --> 00:10:02,190
those more deeply so the the properties

00:09:58,740 --> 00:10:05,340
of a backtrack of the spans wrapper it's

00:10:02,190 --> 00:10:06,990
implemented as a position queue sorted

00:10:05,340 --> 00:10:09,840
according to the order of the positions

00:10:06,990 --> 00:10:12,420
from the backing spans so it's a it's a

00:10:09,840 --> 00:10:14,820
sorted queue sorted by virtue of the

00:10:12,420 --> 00:10:19,410
fact that the input coming from spans is

00:10:14,820 --> 00:10:22,620
already sorted to conform to to the

00:10:19,410 --> 00:10:28,230
position order specification of the

00:10:22,620 --> 00:10:31,460
spans API so it's linked for a fishing

00:10:28,230 --> 00:10:35,160
iteration and node removal because

00:10:31,460 --> 00:10:37,650
depending on the position length some of

00:10:35,160 --> 00:10:40,800
these some of these positions are going

00:10:37,650 --> 00:10:43,830
to be able to be dropped so they they

00:10:40,800 --> 00:10:47,160
can be pruned out of the queue so it's

00:10:43,830 --> 00:10:52,200
kind of a weird queue that normally

00:10:47,160 --> 00:10:53,850
would remove from would insert one end

00:10:52,200 --> 00:10:56,790
and remove from the other but you can

00:10:53,850 --> 00:10:58,830
also take things out of the middle it's

00:10:56,790 --> 00:11:02,790
also array backed using a circular

00:10:58,830 --> 00:11:04,230
buffer to support sufficient binary seek

00:11:02,790 --> 00:11:06,480
because if you're gonna have to be

00:11:04,230 --> 00:11:10,700
backtracking and resetting to a specific

00:11:06,480 --> 00:11:17,150
position you want that to be fast and

00:11:10,700 --> 00:11:17,150
the array is dynamically resizable with

00:11:18,000 --> 00:11:28,319
the the positions basically the array

00:11:22,379 --> 00:11:30,329
index is done all the way up to the

00:11:28,319 --> 00:11:35,160
maximum integer value and then rolls

00:11:30,329 --> 00:11:37,560
over by design to allow Windows and to

00:11:35,160 --> 00:11:42,209
allow stable references to the nodes to

00:11:37,560 --> 00:11:45,449
be to be preserved so for purposes of

00:11:42,209 --> 00:11:47,339
building matches over the subclause each

00:11:45,449 --> 00:11:48,990
sub Clause has its own queue of

00:11:47,339 --> 00:11:52,709
positions that's what we talked about in

00:11:48,990 --> 00:11:55,379
the last slide nodes in those queues are

00:11:52,709 --> 00:11:58,230
also linked laterally across subclasses

00:11:55,379 --> 00:11:59,730
sub-clauses to build matches without

00:11:58,230 --> 00:12:01,829
duplicating information about the

00:11:59,730 --> 00:12:04,079
positions so you already have the

00:12:01,829 --> 00:12:06,240
information stored you just want the

00:12:04,079 --> 00:12:09,209
nodes and the queue to do sort of double

00:12:06,240 --> 00:12:12,720
duty both within its position but also

00:12:09,209 --> 00:12:14,310
across positions to build the matches so

00:12:12,720 --> 00:12:24,600
you have kind of like a two dimensional

00:12:14,310 --> 00:12:26,040
queue in a way yeah so maintains

00:12:24,600 --> 00:12:29,639
references to the previous and next

00:12:26,040 --> 00:12:31,259
nodes within slop constraints and we

00:12:29,639 --> 00:12:35,720
mentioned the stable node references

00:12:31,259 --> 00:12:39,300
that are wrapped around and and that

00:12:35,720 --> 00:12:43,860
don't just use the index position in the

00:12:39,300 --> 00:12:46,319
backing array so the resulting word

00:12:43,860 --> 00:12:49,319
lattice that you end up with is built

00:12:46,319 --> 00:12:52,019
using the next match method signature

00:12:49,319 --> 00:12:55,680
that that was referred to in one of the

00:12:52,019 --> 00:12:58,680
previous slides and so calling next

00:12:55,680 --> 00:13:00,300
match as part of the algorithm for for

00:12:58,680 --> 00:13:02,399
building the matches drives a

00:13:00,300 --> 00:13:05,360
depth-first search to discover and build

00:13:02,399 --> 00:13:08,129
and cache the edges in the dynamic graph

00:13:05,360 --> 00:13:12,899
represented by valid nodes at a point in

00:13:08,129 --> 00:13:16,680
time so it's a dynamic graph in the

00:13:12,899 --> 00:13:19,529
sense that progressing in different ways

00:13:16,680 --> 00:13:22,470
can cause certain nodes to become

00:13:19,529 --> 00:13:27,959
invalidated so you can kind of prune

00:13:22,470 --> 00:13:30,000
them out and close the gaps up so in the

00:13:27,959 --> 00:13:31,230
phrase paths that are already explored

00:13:30,000 --> 00:13:34,060
or cashed

00:13:31,230 --> 00:13:36,040
enabling match tree traversal to be

00:13:34,060 --> 00:13:38,770
short-circuited and you have a

00:13:36,040 --> 00:13:41,800
downstream postfix subtree grafted on to

00:13:38,770 --> 00:13:44,170
upstream match prefixes you can see that

00:13:41,800 --> 00:13:48,100
happening here a little bit so you can

00:13:44,170 --> 00:13:50,890
see the links from B prime to be double

00:13:48,100 --> 00:13:55,530
Prime and C double prime have been

00:13:50,890 --> 00:13:59,730
explored from starting from position a

00:13:55,530 --> 00:14:02,860
but then those links from B prime to the

00:13:59,730 --> 00:14:04,630
two the third sub Clause are still going

00:14:02,860 --> 00:14:06,820
to be valid and we know they're going to

00:14:04,630 --> 00:14:12,460
be valid at B so you don't have to

00:14:06,820 --> 00:14:14,770
re-explore those and it's a little bit

00:14:12,460 --> 00:14:17,770
tricky to to explain what's going on

00:14:14,770 --> 00:14:22,630
here I had a very hard time coming up

00:14:17,770 --> 00:14:26,710
with a good example but what's going on

00:14:22,630 --> 00:14:30,880
here is that C prime D prime V Prime and

00:14:26,710 --> 00:14:33,460
F prime are all much closer to position

00:14:30,880 --> 00:14:35,920
C so you have links going to all of

00:14:33,460 --> 00:14:38,140
those so you can note the the fan out

00:14:35,920 --> 00:14:43,360
that the potential for fan out and also

00:14:38,140 --> 00:14:46,090
the sort of strange interactions of slop

00:14:43,360 --> 00:14:48,340
because as you move to different

00:14:46,090 --> 00:14:50,770
positions you are also consuming slop

00:14:48,340 --> 00:14:52,630
and are therefore left with less slop

00:14:50,770 --> 00:14:58,150
available to continue building the tree

00:14:52,630 --> 00:14:59,680
out to the to the right right so this is

00:14:58,150 --> 00:15:01,660
a linked approach a lot of the data

00:14:59,680 --> 00:15:03,040
structures in leucine are array based

00:15:01,660 --> 00:15:05,830
because they're very fast and the

00:15:03,040 --> 00:15:07,300
generate little garbage collection if

00:15:05,830 --> 00:15:08,910
we're using a linked approach which is

00:15:07,300 --> 00:15:12,370
kind of necessary for this

00:15:08,910 --> 00:15:15,670
two-dimensional situation where nodes

00:15:12,370 --> 00:15:17,980
can get yanked out and we want positions

00:15:15,670 --> 00:15:22,360
to close up there's the there's lots of

00:15:17,980 --> 00:15:24,610
small transient objects including just

00:15:22,360 --> 00:15:26,620
not just the storage nodes but also the

00:15:24,610 --> 00:15:29,830
linking nodes that allow us to build all

00:15:26,620 --> 00:15:33,180
these links so there's lots of garbage

00:15:29,830 --> 00:15:35,710
collection potentially so implementing a

00:15:33,180 --> 00:15:37,470
queue for the nodes and the linking

00:15:35,710 --> 00:15:40,540
nodes

00:15:37,470 --> 00:15:44,380
resulted in 2 times 2 10 times better

00:15:40,540 --> 00:15:46,180
performance in comparison to

00:15:44,380 --> 00:15:49,780
an implementation with object pooling

00:15:46,180 --> 00:15:51,670
disabled and the the variation there is

00:15:49,780 --> 00:15:53,530
in keeping with the intermittent nature

00:15:51,670 --> 00:15:59,100
of long garbage collection related

00:15:53,530 --> 00:16:02,290
pauses so that was made a big difference

00:15:59,100 --> 00:16:05,470
so the query implementation finally

00:16:02,290 --> 00:16:07,600
gives us a really solid reason to

00:16:05,470 --> 00:16:10,120
implement as opposed to ignore indexed

00:16:07,600 --> 00:16:11,770
position lengths for purposes of

00:16:10,120 --> 00:16:14,800
development testing and initial

00:16:11,770 --> 00:16:16,240
deployment most of the extra information

00:16:14,800 --> 00:16:20,080
that was needed was implemented with

00:16:16,240 --> 00:16:27,790
payloads and the encoding of the

00:16:20,080 --> 00:16:30,550
payloads was accomplished by a token

00:16:27,790 --> 00:16:32,260
filter this is that sits at the end sort

00:16:30,550 --> 00:16:35,800
of like the flattened graph filter but

00:16:32,260 --> 00:16:38,200
it reorders tokens to conform to the

00:16:35,800 --> 00:16:39,580
ordering specified by the spans api so

00:16:38,200 --> 00:16:43,030
at query time they're just read out in

00:16:39,580 --> 00:16:45,190
in a normal order i'd be interested to

00:16:43,030 --> 00:16:48,460
see what the performance impact would be

00:16:45,190 --> 00:16:50,020
of using a position length

00:16:48,460 --> 00:16:52,600
implementation that isn't based on

00:16:50,020 --> 00:16:55,920
payloads have a sense that it would

00:16:52,600 --> 00:16:58,690
probably be faster but yeah to be done

00:16:55,920 --> 00:17:00,970
there's also some other information that

00:16:58,690 --> 00:17:02,950
is useful start position look ahead

00:17:00,970 --> 00:17:08,470
so we're buffering all these positions

00:17:02,950 --> 00:17:10,330
but there are certain efficiencies that

00:17:08,470 --> 00:17:12,550
can be gained if you're able to sort of

00:17:10,330 --> 00:17:19,000
look ahead to the neck what the NARC

00:17:12,550 --> 00:17:23,020
next start position would be so if you

00:17:19,000 --> 00:17:25,210
do that then it's possible to to avoid

00:17:23,020 --> 00:17:27,850
buffering entirely for the normal use

00:17:25,210 --> 00:17:30,580
case because most people's data doesn't

00:17:27,850 --> 00:17:32,980
have this kind of rich structure but you

00:17:30,580 --> 00:17:34,810
can use the same approach and it just

00:17:32,980 --> 00:17:39,970
won't buffer if it doesn't need to

00:17:34,810 --> 00:17:43,660
buffer so there's some edge cases here

00:17:39,970 --> 00:17:45,910
you can also make some efficiencies if

00:17:43,660 --> 00:17:48,400
you know that the end position will

00:17:45,910 --> 00:17:51,040
never decrease across positions almost

00:17:48,400 --> 00:17:54,670
always it doesn't but for some cases it

00:17:51,040 --> 00:17:56,950
it might do so a lot of the time and if

00:17:54,670 --> 00:17:57,860
the assumption about the position links

00:17:56,950 --> 00:18:00,290
being one

00:17:57,860 --> 00:18:02,240
always is valid then you can take

00:18:00,290 --> 00:18:04,370
advantage of that as well

00:18:02,240 --> 00:18:05,809
this is currently implemented with four

00:18:04,370 --> 00:18:10,790
bytes that are pre allocated in the

00:18:05,809 --> 00:18:13,059
payload and sort of modified after the

00:18:10,790 --> 00:18:16,150
fact by the default indexing chain

00:18:13,059 --> 00:18:18,500
there's probably a better way to do that

00:18:16,150 --> 00:18:23,510
but but for a proof of concept this

00:18:18,500 --> 00:18:25,760
yielded excellent results so one last

00:18:23,510 --> 00:18:32,840
thing is common words how many minutes

00:18:25,760 --> 00:18:38,270
do I have left - okay so common words

00:18:32,840 --> 00:18:40,429
are a problem for phrases you can use a

00:18:38,270 --> 00:18:44,570
stop word filter which leaves holes

00:18:40,429 --> 00:18:47,210
which behave weirdly for for the words

00:18:44,570 --> 00:18:50,780
that you're removing from the index or

00:18:47,210 --> 00:18:55,370
from the queries or you can use common

00:18:50,780 --> 00:18:57,650
grams filter but that makes it so that

00:18:55,370 --> 00:19:00,980
any word that is included in your common

00:18:57,650 --> 00:19:09,770
gram has to always be included exactly

00:19:00,980 --> 00:19:11,660
next to the - its paired term so we're

00:19:09,770 --> 00:19:18,100
able to work around this by basically

00:19:11,660 --> 00:19:21,290
pre-filtering the conjunction spans and

00:19:18,100 --> 00:19:23,210
that worked nicely I can talk more about

00:19:21,290 --> 00:19:25,419
the details of that if people are

00:19:23,210 --> 00:19:28,340
interested afterwards there are

00:19:25,419 --> 00:19:30,770
different match modes because now that

00:19:28,340 --> 00:19:34,880
the matching is much more precise we can

00:19:30,770 --> 00:19:37,160
do things like greedy matching and her

00:19:34,880 --> 00:19:40,820
position matching and paren position

00:19:37,160 --> 00:19:45,049
that basically ensure that nested

00:19:40,820 --> 00:19:47,179
sub-clauses expose every valid end

00:19:45,049 --> 00:19:50,750
position so that we're certain to match

00:19:47,179 --> 00:19:53,179
everything that needs to be matched so

00:19:50,750 --> 00:19:56,240
I'm particularly interested in CJK

00:19:53,179 --> 00:19:58,640
indexing anything that has multi token

00:19:56,240 --> 00:20:00,380
orthographic variants word delimiter

00:19:58,640 --> 00:20:02,960
graph filter this also opens up the

00:20:00,380 --> 00:20:04,760
possibility for creative uses of engrams

00:20:02,960 --> 00:20:07,270
and shingles that would behave more

00:20:04,760 --> 00:20:07,270
predictably

00:20:07,680 --> 00:20:14,150
and yeah more nuanced scoring also

00:20:10,860 --> 00:20:16,260
potential non non text use cases so

00:20:14,150 --> 00:20:18,060
anything that can be represented as an

00:20:16,260 --> 00:20:20,130
ordered stream of discrete possibly

00:20:18,060 --> 00:20:23,010
overlapping elements each having a start

00:20:20,130 --> 00:20:27,630
position and an end position I could see

00:20:23,010 --> 00:20:29,760
this being useful obvious case would be

00:20:27,630 --> 00:20:33,270
time series data like travel scheduling

00:20:29,760 --> 00:20:38,130
where you could represent individual

00:20:33,270 --> 00:20:40,350
trips as tokens it could be pretty fun

00:20:38,130 --> 00:20:46,680
to mess around with that I have not done

00:20:40,350 --> 00:20:48,740
that but somebody should thank you some

00:20:46,680 --> 00:20:55,069
links and thank ya thank you

00:20:48,740 --> 00:20:55,069

YouTube URL: https://www.youtube.com/watch?v=yoo1hC_InsY


