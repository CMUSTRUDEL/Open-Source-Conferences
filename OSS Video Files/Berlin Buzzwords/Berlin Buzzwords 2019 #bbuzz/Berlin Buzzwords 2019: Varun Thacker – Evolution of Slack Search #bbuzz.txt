Title: Berlin Buzzwords 2019: Varun Thacker – Evolution of Slack Search #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	This talk is about building Slack’s Search Infrastructure. Apache Solr powers search across messages, files and many use-cases at Slack. We’ll first talk about how we can build our indexes billions of documents regularly.

How do we support our language internationalization efforts and what that means to search quality and latency. Lastly the talk will discuss some of the biggest challenges we face today and how we plan on tackling them.

Read more:
https://2019.berlinbuzzwords.de/19/session/evolution-slack-search

About Varun Thacker:
https://2019.berlinbuzzwords.de/users/varun-thacker

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,530 --> 00:00:12,180
hey guys so yeah I'm gonna be talking

00:00:10,050 --> 00:00:14,680
about how we build search it's like what

00:00:12,180 --> 00:00:17,260
are the like intricacies

00:00:14,680 --> 00:00:20,260
and what are the future things that we

00:00:17,260 --> 00:00:22,330
see we want to invest in and I basically

00:00:20,260 --> 00:00:25,090
I'm in the search and discovery team

00:00:22,330 --> 00:00:27,850
here at SLAC and in my previous job I

00:00:25,090 --> 00:00:32,910
worked at lucid works and where I was

00:00:27,850 --> 00:00:35,200
doing search as well so the quick

00:00:32,910 --> 00:00:37,480
summary of what I'm gonna be talking

00:00:35,200 --> 00:00:39,040
about is I'm gonna start off by talking

00:00:37,480 --> 00:00:41,410
a little bit about overall slack

00:00:39,040 --> 00:00:44,710
architecture so that it kind of gets a

00:00:41,410 --> 00:00:47,710
sense as to how such gets plugged in to

00:00:44,710 --> 00:00:49,630
this flow of things then a little bit

00:00:47,710 --> 00:00:52,329
about the indexing side of things how do

00:00:49,630 --> 00:00:56,440
we index data the offline indexes the

00:00:52,329 --> 00:00:58,809
online indexes then something that is

00:00:56,440 --> 00:01:01,480
called enterprise key management and how

00:00:58,809 --> 00:01:03,280
does that play with indexing some

00:01:01,480 --> 00:01:07,630
internationalization and localization

00:01:03,280 --> 00:01:09,670
efforts and how that played how such was

00:01:07,630 --> 00:01:12,460
playing a part in that and then a little

00:01:09,670 --> 00:01:16,950
bit about the query side of things so

00:01:12,460 --> 00:01:19,690
that's gonna be my talk a quick overall

00:01:16,950 --> 00:01:22,659
summary of slack right like so more than

00:01:19,690 --> 00:01:26,229
10 million people log on to slack on a

00:01:22,659 --> 00:01:28,420
daily basis so people create lots of

00:01:26,229 --> 00:01:30,700
message content which means search is

00:01:28,420 --> 00:01:34,570
very interesting that slack and how do

00:01:30,700 --> 00:01:38,170
we have people find things that they

00:01:34,570 --> 00:01:40,570
type like last month right so that's

00:01:38,170 --> 00:01:44,640
basically what our mission and what we

00:01:40,570 --> 00:01:47,259
want to do and slack brings people

00:01:44,640 --> 00:01:49,720
application and data so what does that

00:01:47,259 --> 00:01:53,590
mean like so people type in a lot of

00:01:49,720 --> 00:01:56,140
messages they share files they share

00:01:53,590 --> 00:01:59,770
links to tickets like Zen desks or

00:01:56,140 --> 00:02:02,530
Salesforce so all of these data we can

00:01:59,770 --> 00:02:06,490
make it searchable so that you can

00:02:02,530 --> 00:02:10,750
reference it in the future and this is

00:02:06,490 --> 00:02:12,730
what the search UI looks like so here's

00:02:10,750 --> 00:02:16,090
a few things that are slightly different

00:02:12,730 --> 00:02:20,140
from other search engines is each user

00:02:16,090 --> 00:02:23,410
has its own unique set of documents that

00:02:20,140 --> 00:02:26,470
they have searchable like you have

00:02:23,410 --> 00:02:28,510
channels that you talk in you have

00:02:26,470 --> 00:02:32,349
private channels that some

00:02:28,510 --> 00:02:34,239
if y'all have access to and you have DMS

00:02:32,349 --> 00:02:37,420
that you taught chat with your coworkers

00:02:34,239 --> 00:02:40,120
so every search is unique in that sense

00:02:37,420 --> 00:02:41,830
because you and your fellow coworker

00:02:40,120 --> 00:02:46,060
will have different access to different

00:02:41,830 --> 00:02:48,670
channels which means results are not

00:02:46,060 --> 00:02:52,299
very durable not very casual because

00:02:48,670 --> 00:02:54,670
they are different per user and that

00:02:52,299 --> 00:02:58,239
also means we have no real head queries

00:02:54,670 --> 00:03:01,329
as such because it's like pretty unique

00:02:58,239 --> 00:03:05,440
in that sense and for the first part now

00:03:01,329 --> 00:03:08,170
we talk a little bit about how a message

00:03:05,440 --> 00:03:10,239
gets sent on slack and then we will talk

00:03:08,170 --> 00:03:12,010
about how that flows into the search

00:03:10,239 --> 00:03:14,349
side of things so this is like a

00:03:12,010 --> 00:03:16,450
simplified architecture of what happens

00:03:14,349 --> 00:03:19,180
when you type a message right so you

00:03:16,450 --> 00:03:21,280
type in a message the message gets

00:03:19,180 --> 00:03:24,760
relayed to the API service the API

00:03:21,280 --> 00:03:27,640
service fans it out to all other slack

00:03:24,760 --> 00:03:30,959
lines that are connected to you and once

00:03:27,640 --> 00:03:35,139
that's done it's persisted in a database

00:03:30,959 --> 00:03:40,209
so that is the flow of how a message

00:03:35,139 --> 00:03:42,099
gets sent out right now in this what we

00:03:40,209 --> 00:03:45,310
want to talk about now is how does

00:03:42,099 --> 00:03:49,900
search play apart so this is like a

00:03:45,310 --> 00:03:52,419
screenshot of how all like each green

00:03:49,900 --> 00:03:55,510
box represents Arshad but essentially

00:03:52,419 --> 00:03:58,810
this is all search indexes within slack

00:03:55,510 --> 00:04:01,180
in some sense where each column what we

00:03:58,810 --> 00:04:06,540
are looking at is called a stripe a

00:04:01,180 --> 00:04:08,889
stripe is basically a combination of a

00:04:06,540 --> 00:04:12,040
computed collection which is basically a

00:04:08,889 --> 00:04:13,750
collection we built offline and a set of

00:04:12,040 --> 00:04:16,359
live collections which are time

00:04:13,750 --> 00:04:18,519
partitioned on a daily basis so we

00:04:16,359 --> 00:04:21,280
talked about seven days of live data and

00:04:18,519 --> 00:04:24,280
a computed index of everything before

00:04:21,280 --> 00:04:27,550
that and how a stripe is basically a

00:04:24,280 --> 00:04:31,240
logical concept on top of that to make

00:04:27,550 --> 00:04:34,979
search across all of it now each slack

00:04:31,240 --> 00:04:38,909
user gets routed to one of these 64

00:04:34,979 --> 00:04:42,160
slices or stripes out there so it's

00:04:38,909 --> 00:04:45,370
hosting data for like multiple users

00:04:42,160 --> 00:04:48,550
multiple teams co-locating them and the

00:04:45,370 --> 00:04:53,700
stripe is spanning across multiple solo

00:04:48,550 --> 00:04:56,200
collections now going back to the

00:04:53,700 --> 00:04:59,080
architecture and extending the slack

00:04:56,200 --> 00:05:01,000
architecture essentially once we we

00:04:59,080 --> 00:05:04,180
spoke about in the earlier slide how the

00:05:01,000 --> 00:05:06,100
message gets persisted right like after

00:05:04,180 --> 00:05:09,010
the message gets sent out to all other

00:05:06,100 --> 00:05:12,550
clients the message also gets persisted

00:05:09,010 --> 00:05:15,190
now right after that what happens is the

00:05:12,550 --> 00:05:19,290
event like a message sent event gets

00:05:15,190 --> 00:05:23,140
sent on the queue and the indexing queue

00:05:19,290 --> 00:05:25,930
essentially picks up that event goes to

00:05:23,140 --> 00:05:29,470
the database and gets the wrong message

00:05:25,930 --> 00:05:32,380
content again and uses that to index it

00:05:29,470 --> 00:05:40,630
in do solar so our search indexes are

00:05:32,380 --> 00:05:41,850
backed by solo and whoops yeah sorry

00:05:40,630 --> 00:05:45,910
about that

00:05:41,850 --> 00:05:49,000
so once the message gets indexed into so

00:05:45,910 --> 00:05:52,480
this is the overall architecture of how

00:05:49,000 --> 00:05:55,210
a message flows within the search

00:05:52,480 --> 00:05:57,280
infrastructure now like I showed in the

00:05:55,210 --> 00:06:00,130
previous slide it's a combination of

00:05:57,280 --> 00:06:02,919
building an offline index and a set of

00:06:00,130 --> 00:06:05,290
live collections so we will talk a

00:06:02,919 --> 00:06:09,960
little bit about how we build each of

00:06:05,290 --> 00:06:13,750
them and why we do it in that sense so

00:06:09,960 --> 00:06:17,980
building and offline index involves

00:06:13,750 --> 00:06:20,590
combining multiple small multiple moving

00:06:17,980 --> 00:06:23,110
pieces and making the gluing them

00:06:20,590 --> 00:06:25,540
together because you're basically saying

00:06:23,110 --> 00:06:28,600
build every single message that was ever

00:06:25,540 --> 00:06:32,200
sent or slack and have that tooling to

00:06:28,600 --> 00:06:35,770
be able to do so so we it's think of it

00:06:32,200 --> 00:06:38,800
as a service that's similar to how you

00:06:35,770 --> 00:06:41,710
can index via MapReduce and tools out

00:06:38,800 --> 00:06:45,400
there like the MapReduce er indexer

00:06:41,710 --> 00:06:49,600
tools so tools similar to those and how

00:06:45,400 --> 00:06:51,520
you can build large indexes in a Hadoop

00:06:49,600 --> 00:06:55,870
cluster and then be able to send those

00:06:51,520 --> 00:07:00,880
claw senders or indexes over to

00:06:55,870 --> 00:07:05,070
search cluster now how does this work it

00:07:00,880 --> 00:07:08,080
basically consists of three parts so

00:07:05,070 --> 00:07:12,100
there's some keep pressing something

00:07:08,080 --> 00:07:13,360
sorry it consists of three parts we

00:07:12,100 --> 00:07:15,970
create something called composite

00:07:13,360 --> 00:07:19,780
dimensions then there's a dog join and

00:07:15,970 --> 00:07:21,550
then the last is a solar index stage so

00:07:19,780 --> 00:07:24,700
we're gonna walk through each of these

00:07:21,550 --> 00:07:27,940
steps to show how the offline index gets

00:07:24,700 --> 00:07:30,760
billed now the first step in the process

00:07:27,940 --> 00:07:33,520
is called a composite dimension job

00:07:30,760 --> 00:07:36,970
you're essentially think of it basically

00:07:33,520 --> 00:07:39,310
for search we want all the data to be

00:07:36,970 --> 00:07:41,800
denormalized right not only do we want

00:07:39,310 --> 00:07:43,870
the message that you send all the

00:07:41,800 --> 00:07:47,080
reactions that people made to your

00:07:43,870 --> 00:07:49,600
message which channel was it in what was

00:07:47,080 --> 00:07:52,240
like was the file uploaded with the

00:07:49,600 --> 00:07:55,030
message was it a thread and did other

00:07:52,240 --> 00:07:58,270
people react to that so we want to

00:07:55,030 --> 00:08:01,360
denormalize all this data so essentially

00:07:58,270 --> 00:08:07,750
in the composite dimensions job all

00:08:01,360 --> 00:08:09,910
we're saying is build in memory it takes

00:08:07,750 --> 00:08:12,850
data from all of these auxiliary tables

00:08:09,910 --> 00:08:14,770
right like channels to convert internal

00:08:12,850 --> 00:08:17,410
channel IDs to channel names

00:08:14,770 --> 00:08:22,930
similarly users was there a file

00:08:17,410 --> 00:08:25,900
uploaded and basically we partitioned

00:08:22,930 --> 00:08:28,540
them per team so we take all this data

00:08:25,900 --> 00:08:32,430
and we partition all the data per team

00:08:28,540 --> 00:08:36,190
and each team is then assigned to 120 to

00:08:32,430 --> 00:08:39,010
128 shards so essentially we're building

00:08:36,190 --> 00:08:41,410
all the oxygen tables which are like

00:08:39,010 --> 00:08:44,380
metadata to the actual message and we're

00:08:41,410 --> 00:08:47,500
building it into sharding it out by 128

00:08:44,380 --> 00:08:49,830
and being able to create this and store

00:08:47,500 --> 00:08:52,600
this in memory so that when a message

00:08:49,830 --> 00:08:55,420
when we're indexing messages we can do

00:08:52,600 --> 00:08:57,730
direct translations to all the metadata

00:08:55,420 --> 00:08:58,870
for the message and be able to populate

00:08:57,730 --> 00:09:01,150
it with that

00:08:58,870 --> 00:09:04,480
so essentially composite dimensions is

00:09:01,150 --> 00:09:07,480
just providing us a 360-degree view of

00:09:04,480 --> 00:09:08,499
all the events that took place on the

00:09:07,480 --> 00:09:11,019
message

00:09:08,499 --> 00:09:15,039
and we are able to keep this in memory

00:09:11,019 --> 00:09:18,219
per team like so we load up per team now

00:09:15,039 --> 00:09:21,789
the second part of the stage is called

00:09:18,219 --> 00:09:24,489
the doctrine which is basically a thrift

00:09:21,789 --> 00:09:27,729
structure which combines the actual

00:09:24,489 --> 00:09:30,159
message with all of this data that was

00:09:27,729 --> 00:09:32,709
populated before so which channels will

00:09:30,159 --> 00:09:34,599
the message typed in which how many

00:09:32,709 --> 00:09:38,829
reactions were there on the message and

00:09:34,599 --> 00:09:41,289
things like that so we basically take

00:09:38,829 --> 00:09:43,689
all these input messages we know which

00:09:41,289 --> 00:09:46,599
team it belongs in we combine it with

00:09:43,689 --> 00:09:49,209
the team's composite dimension that we

00:09:46,599 --> 00:09:53,019
had built up earlier and we basically

00:09:49,209 --> 00:09:55,599
able to create one solar input document

00:09:53,019 --> 00:10:00,879
at the very end of this step so at the

00:09:55,599 --> 00:10:04,419
end of this steps we have built solar

00:10:00,879 --> 00:10:07,179
input documents or in a thrift structure

00:10:04,419 --> 00:10:10,239
which is not indexed but the raw data

00:10:07,179 --> 00:10:12,459
has been generated so this runs as the

00:10:10,239 --> 00:10:15,489
first phase of the MapReduce process

00:10:12,459 --> 00:10:19,209
right so we are able to now create all

00:10:15,489 --> 00:10:23,639
the messages out there now what will

00:10:19,209 --> 00:10:28,299
basically take us we on each message we

00:10:23,639 --> 00:10:32,109
then shard it and then on we create

00:10:28,299 --> 00:10:34,509
these shards in another MapReduce so in

00:10:32,109 --> 00:10:36,699
the reducer phase of the operation so

00:10:34,509 --> 00:10:39,579
basically we spin up a solar embedded

00:10:36,699 --> 00:10:43,419
solar and we are able to create each

00:10:39,579 --> 00:10:45,729
shot at each reducer right so it

00:10:43,419 --> 00:10:49,720
basically takes this dog shine thrift

00:10:45,729 --> 00:10:52,209
structure and it's using EMR and it's

00:10:49,720 --> 00:10:54,939
basically outputting at the very end a

00:10:52,209 --> 00:10:57,839
solar core which is basically a solar

00:10:54,939 --> 00:11:01,209
shot at the very end of this process so

00:10:57,839 --> 00:11:05,259
this is the three steps that we kind of

00:11:01,209 --> 00:11:09,879
build to have the ability to index

00:11:05,259 --> 00:11:13,479
messages offline and once these indexes

00:11:09,879 --> 00:11:17,529
are built we basically push them to s3

00:11:13,479 --> 00:11:20,169
and then the solar query the solar

00:11:17,529 --> 00:11:23,249
active servers are able to download the

00:11:20,169 --> 00:11:23,249
indexes of s

00:11:23,370 --> 00:11:29,200
so this is the indexing process now

00:11:26,529 --> 00:11:31,240
going back to the same stripes that I'd

00:11:29,200 --> 00:11:33,700
showed you earlier right you can see

00:11:31,240 --> 00:11:37,149
that the number of green dots in this

00:11:33,700 --> 00:11:38,950
section of it is varies in size that's

00:11:37,149 --> 00:11:42,100
because when we are computing this index

00:11:38,950 --> 00:11:44,980
offline you know per team or per stripe

00:11:42,100 --> 00:11:47,519
how many documents are there gonna be so

00:11:44,980 --> 00:11:50,290
the number of sharks that we create is

00:11:47,519 --> 00:11:52,269
based on the number of documents that

00:11:50,290 --> 00:11:55,390
are there so we can dynamically

00:11:52,269 --> 00:11:58,000
calculate how many sharks should each

00:11:55,390 --> 00:12:02,290
collection have while building these

00:11:58,000 --> 00:12:05,970
step well building this offline so at

00:12:02,290 --> 00:12:10,390
this point we build an index offline and

00:12:05,970 --> 00:12:13,480
on top of this index we will now be able

00:12:10,390 --> 00:12:17,220
to interlay seven live collections or

00:12:13,480 --> 00:12:20,170
seven like time based partitions and

00:12:17,220 --> 00:12:23,320
index any new data that comes in and

00:12:20,170 --> 00:12:26,020
overlay them on top and then we can do

00:12:23,320 --> 00:12:29,020
the same process over and over again so

00:12:26,020 --> 00:12:31,140
a few advantages of building a very

00:12:29,020 --> 00:12:35,130
extensive offline system like this is

00:12:31,140 --> 00:12:38,440
you can make schema changes very easily

00:12:35,130 --> 00:12:41,920
you can reindex with different field

00:12:38,440 --> 00:12:44,790
types you can upgrade solar versions

00:12:41,920 --> 00:12:46,720
like all of these start becoming very

00:12:44,790 --> 00:12:49,240
straightforward if you have the vast

00:12:46,720 --> 00:12:52,329
like if you have a tool like this to be

00:12:49,240 --> 00:12:54,970
able to index every single message like

00:12:52,329 --> 00:12:58,839
I said it's it's not easy sometimes

00:12:54,970 --> 00:13:00,880
because a lot of it has downstream

00:12:58,839 --> 00:13:04,209
dependencies on is this data available

00:13:00,880 --> 00:13:06,640
to us how is data is will this data

00:13:04,209 --> 00:13:08,560
bring down by my sequel shards because

00:13:06,640 --> 00:13:12,370
like you're just reading too much from

00:13:08,560 --> 00:13:14,920
my sequel so in our case we build

00:13:12,370 --> 00:13:17,880
backups of my sequel and that gets

00:13:14,920 --> 00:13:21,540
uploaded to s3 so we can read off

00:13:17,880 --> 00:13:27,579
offline database something like that and

00:13:21,540 --> 00:13:30,339
so we are able to build this up now this

00:13:27,579 --> 00:13:33,490
was like the essential flow behind how

00:13:30,339 --> 00:13:35,290
indexing works and now we're going to

00:13:33,490 --> 00:13:38,590
talk a little bit about

00:13:35,290 --> 00:13:40,210
what are the few features that slack

00:13:38,590 --> 00:13:43,150
provides and how does that affect

00:13:40,210 --> 00:13:45,130
indexing so there's a feature called

00:13:43,150 --> 00:13:48,310
enterprise key management where

00:13:45,130 --> 00:13:51,460
basically customers control the key used

00:13:48,310 --> 00:13:54,100
to encrypt the data any customers search

00:13:51,460 --> 00:13:58,210
index lives on an encrypted filesystem

00:13:54,100 --> 00:14:01,330
right so each customer's search index

00:13:58,210 --> 00:14:03,520
also needs to be built separately from

00:14:01,330 --> 00:14:06,460
everyone else's because that data is

00:14:03,520 --> 00:14:08,230
encrypted they sit on a different file

00:14:06,460 --> 00:14:11,640
system so what does that mean that

00:14:08,230 --> 00:14:16,150
they're they get individual stripes now

00:14:11,640 --> 00:14:18,820
so each ECAM user has their own stripes

00:14:16,150 --> 00:14:22,660
which means if you have like thousands

00:14:18,820 --> 00:14:25,150
of such users you can now have thousands

00:14:22,660 --> 00:14:27,880
and tens of thousands of solar

00:14:25,150 --> 00:14:30,400
collections in the same cluster so that

00:14:27,880 --> 00:14:32,320
propose like that hosts its own set of

00:14:30,400 --> 00:14:34,000
challenges and how there's the cluster

00:14:32,320 --> 00:14:38,320
scale when you have thousands of

00:14:34,000 --> 00:14:41,590
collections out there and even though

00:14:38,320 --> 00:14:44,110
indexer the offline indexing needs to be

00:14:41,590 --> 00:14:46,810
running separately from everyone else's

00:14:44,110 --> 00:14:49,420
so it's like it involved some more

00:14:46,810 --> 00:14:54,460
tooling on our site to allow something

00:14:49,420 --> 00:14:56,310
like this to work with search next up

00:14:54,460 --> 00:14:58,390
I'll talk a little bit about the

00:14:56,310 --> 00:15:03,730
internationalization and localization

00:14:58,390 --> 00:15:06,130
and how do we deal with people typing

00:15:03,730 --> 00:15:10,060
messages in different languages and how

00:15:06,130 --> 00:15:12,940
do you search for message when a user

00:15:10,060 --> 00:15:15,400
might type in a certain language but is

00:15:12,940 --> 00:15:20,530
looking for messages across everybody

00:15:15,400 --> 00:15:23,860
typing within slack right so Ana Legere

00:15:20,530 --> 00:15:26,920
is localization is the equivalent of

00:15:23,860 --> 00:15:28,270
like say having a blog post in different

00:15:26,920 --> 00:15:30,400
languages right so you can have an

00:15:28,270 --> 00:15:32,590
English variant or Spanish variant of

00:15:30,400 --> 00:15:36,340
French variant but internationalization

00:15:32,590 --> 00:15:39,010
is having the correct block-posts off to

00:15:36,340 --> 00:15:44,250
you based on knowing where the users

00:15:39,010 --> 00:15:44,250
coming from what their languages

00:15:48,270 --> 00:15:54,400
so what are the languages that are

00:15:52,030 --> 00:15:58,330
currently supported right so you can

00:15:54,400 --> 00:16:00,790
have German UK English US English and a

00:15:58,330 --> 00:16:04,260
set of like 8 languages that we natively

00:16:00,790 --> 00:16:08,800
support and all of these are

00:16:04,260 --> 00:16:10,690
configurable within 2 options so every

00:16:08,800 --> 00:16:12,880
time you create a workspace or a team

00:16:10,690 --> 00:16:17,590
within slack the administrator can set

00:16:12,880 --> 00:16:20,350
the default locale right the second

00:16:17,590 --> 00:16:21,820
option is your user preference so you

00:16:20,350 --> 00:16:26,650
can go and use the Preferences and

00:16:21,820 --> 00:16:29,470
select your locale that you were going

00:16:26,650 --> 00:16:32,170
to use so we want to use both of these

00:16:29,470 --> 00:16:36,070
and both of this information to be able

00:16:32,170 --> 00:16:39,550
to index the message correctly so now

00:16:36,070 --> 00:16:44,160
let's see if the workspace preference is

00:16:39,550 --> 00:16:48,640
in English but that is like the user is

00:16:44,160 --> 00:16:51,550
in speaking in Spanish right so like how

00:16:48,640 --> 00:16:55,920
does this how would this work right so a

00:16:51,550 --> 00:16:55,920
user types in a message in Spanish and

00:17:03,780 --> 00:17:07,780
these two were supposed to be like how

00:17:06,670 --> 00:17:10,740
messages look like in different

00:17:07,780 --> 00:17:13,810
languages but essentially what we do is

00:17:10,740 --> 00:17:16,689
like whenever a user types in a message

00:17:13,810 --> 00:17:19,180
their locale which in this case was

00:17:16,689 --> 00:17:22,120
Spanish gets indexed into a Spanish

00:17:19,180 --> 00:17:25,540
search field and since the team

00:17:22,120 --> 00:17:29,320
workspace was English we also index it

00:17:25,540 --> 00:17:34,740
within the English work language field

00:17:29,320 --> 00:17:40,750
and what happens when we search right

00:17:34,740 --> 00:17:42,780
okay this give me a second sorry about

00:17:40,750 --> 00:17:42,780
that

00:18:04,450 --> 00:18:23,620
it's loading better nari

00:18:07,159 --> 00:18:23,620
it's feel good okay that was

00:18:39,320 --> 00:18:47,690
give me a second I don't know sorry

00:19:03,270 --> 00:19:06,990
into shrinker broken

00:19:33,260 --> 00:19:39,929
all right yeah this renders much better

00:19:36,150 --> 00:19:42,390
right okay so essentially like going

00:19:39,929 --> 00:19:44,610
back to the example like since the

00:19:42,390 --> 00:19:48,330
workspace best friend says English this

00:19:44,610 --> 00:19:51,360
is how the message got indexed and the

00:19:48,330 --> 00:19:53,610
Spanish variant gets indexed with the

00:19:51,360 --> 00:19:57,240
Spanish field so what happens when we do

00:19:53,610 --> 00:19:59,910
a search essentially at search time we

00:19:57,240 --> 00:20:02,490
have to search across all the search or

00:19:59,910 --> 00:20:04,890
all the language fields because we don't

00:20:02,490 --> 00:20:07,320
really understand where the users

00:20:04,890 --> 00:20:09,240
searching and what he couldn't like that

00:20:07,320 --> 00:20:11,820
person could be searching for a message

00:20:09,240 --> 00:20:15,690
that would be typed by somebody else in

00:20:11,820 --> 00:20:18,059
say French right so at search time you

00:20:15,690 --> 00:20:22,500
kind of have to search across all of the

00:20:18,059 --> 00:20:24,929
fields and like different fields be like

00:20:22,500 --> 00:20:27,990
token ax is differently stem differently

00:20:24,929 --> 00:20:31,410
based on how the language semantics

00:20:27,990 --> 00:20:33,840
works right so in this case since we're

00:20:31,410 --> 00:20:35,400
searching across all the fields and in

00:20:33,840 --> 00:20:38,280
the example I'm kind of just showing you

00:20:35,400 --> 00:20:42,120
English and Spanish essentially the

00:20:38,280 --> 00:20:45,740
query term gets matched with in the

00:20:42,120 --> 00:20:48,540
Spanish field because the word gets

00:20:45,740 --> 00:20:51,450
stemmed and then the stem form matches

00:20:48,540 --> 00:20:53,429
the Spanish field but since the English

00:20:51,450 --> 00:20:56,130
field does not stem the word it doesn't

00:20:53,429 --> 00:20:58,020
match in this case the English field so

00:20:56,130 --> 00:21:00,600
in this case we get a match on this

00:20:58,020 --> 00:21:02,700
Spanish field and when we search we

00:21:00,600 --> 00:21:06,679
essentially match and then we show you

00:21:02,700 --> 00:21:06,679
the highlighted term there right so

00:21:06,799 --> 00:21:11,610
because of this strategy what are the

00:21:09,840 --> 00:21:14,730
potential improvements that we could

00:21:11,610 --> 00:21:18,390
make here right like if we can detect

00:21:14,730 --> 00:21:21,570
the message up front and instead of

00:21:18,390 --> 00:21:24,450
relying on the language preference of

00:21:21,570 --> 00:21:26,730
the team or the users language what we

00:21:24,450 --> 00:21:29,340
could do is have smarter tokenization

00:21:26,730 --> 00:21:32,700
strategies within Lucene where we could

00:21:29,340 --> 00:21:35,520
have one-to-one field but have different

00:21:32,700 --> 00:21:37,590
messages get tokenized differently so

00:21:35,520 --> 00:21:42,840
that at search time we only search

00:21:37,590 --> 00:21:45,059
across one field so that would be like

00:21:42,840 --> 00:21:46,409
something that we want to explore in the

00:21:45,059 --> 00:21:49,980
coming months

00:21:46,409 --> 00:21:53,190
and at we could also try detecting the

00:21:49,980 --> 00:21:55,409
language at query time if we don't do

00:21:53,190 --> 00:21:58,139
the smarter tokenization strategy so we

00:21:55,409 --> 00:21:59,639
will not see an explorer can we do

00:21:58,139 --> 00:22:02,460
something better rather than searching

00:21:59,639 --> 00:22:05,309
all the fields and hoping they get

00:22:02,460 --> 00:22:08,580
matched in one of them versus like

00:22:05,309 --> 00:22:11,720
having a smarter way to do so now those

00:22:08,580 --> 00:22:16,139
who feels that were like

00:22:11,720 --> 00:22:18,450
non-indo-european languages now for cgk

00:22:16,139 --> 00:22:22,230
we do things a little different because

00:22:18,450 --> 00:22:26,309
CJK essentially we can we have libraries

00:22:22,230 --> 00:22:28,649
that can detect that this message has

00:22:26,309 --> 00:22:32,309
been typed in cgk so we don't merely

00:22:28,649 --> 00:22:35,730
rely on like the workspace and the user

00:22:32,309 --> 00:22:39,870
local preferences we can also since we

00:22:35,730 --> 00:22:41,389
can detect the CJK characters so when we

00:22:39,870 --> 00:22:44,190
type in something in Japanese

00:22:41,389 --> 00:22:47,250
essentially we index it into a couple of

00:22:44,190 --> 00:22:50,639
search fields because we know that this

00:22:47,250 --> 00:22:52,649
message has been typed in cgk so we

00:22:50,639 --> 00:22:54,779
index it into a Japanese field and like

00:22:52,649 --> 00:22:57,200
CG k3 and like both of them have

00:22:54,779 --> 00:23:00,840
different tokenization behaviors and

00:22:57,200 --> 00:23:04,049
they are able to match the search

00:23:00,840 --> 00:23:06,779
injection so like the two types of

00:23:04,049 --> 00:23:08,899
language analysis we do for cgk we

00:23:06,779 --> 00:23:11,100
detect the languages and for all the

00:23:08,899 --> 00:23:14,330
indo-european languages we kind of rely

00:23:11,100 --> 00:23:18,090
on preferences out there to index in

00:23:14,330 --> 00:23:19,649
last up is the query architecture right

00:23:18,090 --> 00:23:24,120
now we talked a little bit about the

00:23:19,649 --> 00:23:26,970
localization efforts and all of how the

00:23:24,120 --> 00:23:29,340
message gets indexed overall how does

00:23:26,970 --> 00:23:31,679
this stack up right so we have the

00:23:29,340 --> 00:23:33,450
backend search intra like the back end

00:23:31,679 --> 00:23:36,120
where all of slack is written in and

00:23:33,450 --> 00:23:38,340
then we have a search proxy in front of

00:23:36,120 --> 00:23:44,789
it and in front of that we have the

00:23:38,340 --> 00:23:47,370
solar services so when we talk about the

00:23:44,789 --> 00:23:49,500
backend the web application essentially

00:23:47,370 --> 00:23:53,190
that's written in hat which is a

00:23:49,500 --> 00:23:55,679
programming language like PHP variant

00:23:53,190 --> 00:23:57,990
programming languages and a lot of

00:23:55,679 --> 00:24:00,160
application code of search is also

00:23:57,990 --> 00:24:03,760
written there because we

00:24:00,160 --> 00:24:05,830
to translate for that user what are the

00:24:03,760 --> 00:24:07,540
channels that they have access to what

00:24:05,830 --> 00:24:10,870
are the conversations they have access

00:24:07,540 --> 00:24:13,090
to what are the channel when you type in

00:24:10,870 --> 00:24:16,360
a message like this we need to be able

00:24:13,090 --> 00:24:18,820
to look up that user's internal IDs

00:24:16,360 --> 00:24:21,130
where the channel belongs in and be able

00:24:18,820 --> 00:24:23,080
to translate all of that so all of that

00:24:21,130 --> 00:24:30,190
happens in the backend search

00:24:23,080 --> 00:24:32,200
infrastructure and then those get sent

00:24:30,190 --> 00:24:34,750
over to the search proxy layer the

00:24:32,200 --> 00:24:38,200
search proxy is like a little written in

00:24:34,750 --> 00:24:41,560
Java which interacts with solar using

00:24:38,200 --> 00:24:46,750
solar G proxy now the search proxy has

00:24:41,560 --> 00:24:49,770
like a few like it plays a few role like

00:24:46,750 --> 00:24:53,890
it does a few things there essentially

00:24:49,770 --> 00:24:56,410
now that we have all of like the filter

00:24:53,890 --> 00:24:59,650
queries and like the ACLs correctly sent

00:24:56,410 --> 00:25:01,900
from the web application it can now do a

00:24:59,650 --> 00:25:05,530
little bit on the query rewriting side

00:25:01,900 --> 00:25:08,050
of things so it can do some detection

00:25:05,530 --> 00:25:11,860
there add synonyms things like that and

00:25:08,050 --> 00:25:15,550
then send it to search and when we get

00:25:11,860 --> 00:25:17,740
back search results essentially we can

00:25:15,550 --> 00:25:19,480
do rear ANCA within the proxy layer so

00:25:17,740 --> 00:25:22,990
the proxy layer kind of doubles up as a

00:25:19,480 --> 00:25:25,360
rear unkingly layer for us as well so we

00:25:22,990 --> 00:25:28,000
get back like over-over fetch search

00:25:25,360 --> 00:25:32,590
results from solar and then do the

00:25:28,000 --> 00:25:34,960
ranking within the proxy layer now while

00:25:32,590 --> 00:25:39,580
building like a proxy layer like a few

00:25:34,960 --> 00:25:41,560
things that we found out and like people

00:25:39,580 --> 00:25:43,540
should know about is when you're writing

00:25:41,560 --> 00:25:46,510
a proxy layer like sane Java or any

00:25:43,540 --> 00:25:50,980
other language essentially what you want

00:25:46,510 --> 00:25:54,280
to make sure you can control is say if

00:25:50,980 --> 00:25:57,310
you have a few bad or a few slow solar

00:25:54,280 --> 00:25:59,980
machines out there the proxy layer

00:25:57,310 --> 00:26:01,770
should not be holding up all the can all

00:25:59,980 --> 00:26:05,320
of the connections that it can make

00:26:01,770 --> 00:26:08,950
against only those machines resulting in

00:26:05,320 --> 00:26:11,290
the proxy not being to serve traffic to

00:26:08,950 --> 00:26:13,159
the rest of the solar servers so imagine

00:26:11,290 --> 00:26:16,909
like a bad solar server

00:26:13,159 --> 00:26:18,919
which is like the see the AWS machine

00:26:16,909 --> 00:26:21,919
has is in a bad state or something like

00:26:18,919 --> 00:26:23,720
that but all the connections go there

00:26:21,919 --> 00:26:26,299
and it's like just a black hole where

00:26:23,720 --> 00:26:29,269
like connections are being taken up and

00:26:26,299 --> 00:26:32,359
the like the search requests aren't

00:26:29,269 --> 00:26:34,759
being processed essentially now say if

00:26:32,359 --> 00:26:36,919
you have like hundreds of thousands of

00:26:34,759 --> 00:26:39,409
connections to that machine being made

00:26:36,919 --> 00:26:41,299
because a lot of searches are there what

00:26:39,409 --> 00:26:43,940
happens in the proxy layer right all

00:26:41,299 --> 00:26:47,690
those requests gets held up and at that

00:26:43,940 --> 00:26:49,609
point like it cannot serve requests to

00:26:47,690 --> 00:26:52,759
other machines we want to make sure that

00:26:49,609 --> 00:26:56,599
the proxy layer is aware of say a pearl

00:26:52,759 --> 00:26:59,499
collection basis or it knows it has some

00:26:56,599 --> 00:27:03,470
concepts of limiting requests and

00:26:59,499 --> 00:27:07,700
mitigating requests only so that a few

00:27:03,470 --> 00:27:10,999
bad solar nodes cannot affect the

00:27:07,700 --> 00:27:13,759
remaining solar nodes and we gonna talk

00:27:10,999 --> 00:27:16,460
a little bit like so this is like the

00:27:13,759 --> 00:27:18,979
overall proxy service that we have we

00:27:16,460 --> 00:27:21,590
like at some point we'll talk more

00:27:18,979 --> 00:27:23,359
details as to how do we do ranking there

00:27:21,590 --> 00:27:25,999
what are the pros what are the cons of

00:27:23,359 --> 00:27:27,979
doing query rewriting at that stage

00:27:25,999 --> 00:27:28,519
versus doing it within solar and things

00:27:27,979 --> 00:27:31,220
like that

00:27:28,519 --> 00:27:34,220
and we're gonna dive later and some

00:27:31,220 --> 00:27:36,169
other talk about that so this was like

00:27:34,220 --> 00:27:40,210
kind of the overall architecture of what

00:27:36,169 --> 00:27:43,690
we do so if you guys have questions

00:27:40,210 --> 00:27:43,690
please free to ask

00:27:44,350 --> 00:27:53,010
[Applause]

00:27:54,679 --> 00:28:00,570
hi I was wondering when you're searching

00:27:58,049 --> 00:28:03,750
over the fields for multiple languages

00:28:00,570 --> 00:28:07,950
do any like stemming or any kind of

00:28:03,750 --> 00:28:17,820
analysis on the search term so so the

00:28:07,950 --> 00:28:20,820
way it works is because like so every

00:28:17,820 --> 00:28:23,460
field type has its own set of stammers

00:28:20,820 --> 00:28:25,350
so I have abstracted like I've just

00:28:23,460 --> 00:28:26,760
called it stem Oh filter but like a

00:28:25,350 --> 00:28:28,320
different field type will have a

00:28:26,760 --> 00:28:31,350
different stem Oh with a different

00:28:28,320 --> 00:28:33,960
language attached to it so essentially

00:28:31,350 --> 00:28:36,390
that message has been indexed with that

00:28:33,960 --> 00:28:39,929
sort of tokenization and stemming

00:28:36,390 --> 00:28:42,900
involved so at search time how the query

00:28:39,929 --> 00:28:46,160
parser so when a query parser takes in

00:28:42,900 --> 00:28:49,110
the message it for purl field

00:28:46,160 --> 00:28:53,190
essentially stems it according to the

00:28:49,110 --> 00:28:56,220
same language so the same query term is

00:28:53,190 --> 00:28:59,970
now being analyzed differently of

00:28:56,220 --> 00:29:05,820
different fields when matching in and

00:28:59,970 --> 00:29:10,740
that makes sense yeah I was wondering

00:29:05,820 --> 00:29:12,660
like why is the CJK indexing flow yeah

00:29:10,740 --> 00:29:14,610
quite different to the other european

00:29:12,660 --> 00:29:17,730
languages because they are just another

00:29:14,610 --> 00:29:19,470
language so right now we do cgk

00:29:17,730 --> 00:29:23,100
differently because we have the ability

00:29:19,470 --> 00:29:26,669
to detect CJK up front so like whenever

00:29:23,100 --> 00:29:29,190
even if say the users locale is English

00:29:26,669 --> 00:29:31,799
but he types in a message in Japanese

00:29:29,190 --> 00:29:35,790
you can we are able to detect that

00:29:31,799 --> 00:29:37,650
that's Japanese essentially so because

00:29:35,790 --> 00:29:39,990
of that we know that we can index it

00:29:37,650 --> 00:29:42,059
into the Japanese field essentially but

00:29:39,990 --> 00:29:44,880
we don't have the we don't really do

00:29:42,059 --> 00:29:46,410
that for like say in like Spanish or

00:29:44,880 --> 00:29:54,419
something like that or somebody types in

00:29:46,410 --> 00:29:57,390
in Sui like on any other language we

00:29:54,419 --> 00:29:59,640
detect for cgk specifically so we say if

00:29:57,390 --> 00:30:01,080
this message is cgk then index at like

00:29:59,640 --> 00:30:08,340
that otherwise rely on

00:30:01,080 --> 00:30:10,320
the preferences the other potential

00:30:08,340 --> 00:30:12,540
improvement which you are suggesting for

00:30:10,320 --> 00:30:15,390
non-surgical languages is to detect in

00:30:12,540 --> 00:30:19,500
the same way as cgk yes and then

00:30:15,390 --> 00:30:21,090
eventually like and we'll only find out

00:30:19,500 --> 00:30:24,210
when we start actually working on this

00:30:21,090 --> 00:30:26,670
but like my hope is all of this if we

00:30:24,210 --> 00:30:29,940
detected at in next time we can even

00:30:26,670 --> 00:30:32,010
index it into one search field so that

00:30:29,940 --> 00:30:34,140
different languages get recognized

00:30:32,010 --> 00:30:37,290
differently but they all land up in the

00:30:34,140 --> 00:30:39,180
search search field and so that you're

00:30:37,290 --> 00:30:47,130
not searching multiple fields and paying

00:30:39,180 --> 00:30:49,470
the cost for that as well do you have a

00:30:47,130 --> 00:30:51,390
problem when one were have different

00:30:49,470 --> 00:30:54,810
meanings in different languages and then

00:30:51,390 --> 00:30:58,140
if we do actually like because in a lot

00:30:54,810 --> 00:31:00,000
of cases like you can do the wrong you

00:30:58,140 --> 00:31:02,220
can still find a false positive match

00:31:00,000 --> 00:31:04,190
because it means something else but

00:31:02,220 --> 00:31:07,290
because the tokenization

00:31:04,190 --> 00:31:13,320
matches it doesn't mean that it's

00:31:07,290 --> 00:31:15,180
actually finding the right thing we like

00:31:13,320 --> 00:31:21,510
we're still looking at ways to improve

00:31:15,180 --> 00:31:24,240
the idea yeah I was just wondering how

00:31:21,510 --> 00:31:27,570
you handle actually the cost of doing a

00:31:24,240 --> 00:31:30,510
long tight range search when it come if

00:31:27,570 --> 00:31:33,090
I have kind of like a 24 month archive

00:31:30,510 --> 00:31:35,460
okay how do you keep it from being

00:31:33,090 --> 00:31:38,760
really expensive and having kind of the

00:31:35,460 --> 00:31:49,940
year ago states have been online so

00:31:38,760 --> 00:31:53,790
that's a good point so basically if so

00:31:49,940 --> 00:31:55,730
every time you search like you have a

00:31:53,790 --> 00:32:02,580
option between recent and relevant

00:31:55,730 --> 00:32:04,620
search so basically like we we are like

00:32:02,580 --> 00:32:07,020
by default it's a relevant search so

00:32:04,620 --> 00:32:09,600
that means we can like search in the

00:32:07,020 --> 00:32:13,320
lives and then like prioritize the older

00:32:09,600 --> 00:32:14,789
messages based on that and score them

00:32:13,320 --> 00:32:17,429
differently as well

00:32:14,789 --> 00:32:19,799
while recent still has to like still

00:32:17,429 --> 00:32:22,799
search across everything so we don't

00:32:19,799 --> 00:32:25,409
really do much around like

00:32:22,799 --> 00:32:29,789
short-circuiting queries like if we find

00:32:25,409 --> 00:32:34,019
enough hits beyond a certain range as of

00:32:29,789 --> 00:32:36,839
now we've not really hit that problem to

00:32:34,019 --> 00:32:38,639
be honest like like a lot of like search

00:32:36,839 --> 00:32:41,009
the search system seems to be holding up

00:32:38,639 --> 00:32:44,749
fine without that so we'll get to it

00:32:41,009 --> 00:32:44,749
when it kind of becomes more prominent

00:32:47,029 --> 00:32:56,369
any more questions my question might be

00:32:53,099 --> 00:32:57,839
a little beyond your presentation but do

00:32:56,369 --> 00:33:02,009
you have some or do you think about some

00:32:57,839 --> 00:33:04,399
personalization for your search I'm sure

00:33:02,009 --> 00:33:07,319
like so like there was a talk recently

00:33:04,399 --> 00:33:09,719
that another quirk of minded where he

00:33:07,319 --> 00:33:13,889
talked about personalization in context

00:33:09,719 --> 00:33:18,329
to how we can do that with keeping

00:33:13,889 --> 00:33:21,869
messages secure like so like the trick

00:33:18,329 --> 00:33:25,319
there I guess is like we can't like

00:33:21,869 --> 00:33:27,839
every team is unique because the data

00:33:25,319 --> 00:33:30,929
belongs to them and then even within

00:33:27,839 --> 00:33:32,819
each slack team like different users my

00:33:30,929 --> 00:33:35,190
type different messages that different

00:33:32,819 --> 00:33:39,059
users don't have access to so all there

00:33:35,190 --> 00:33:41,940
was a talk based on how do we index a

00:33:39,059 --> 00:33:45,690
only public content from that team to be

00:33:41,940 --> 00:33:47,639
able to do like at least some user

00:33:45,690 --> 00:33:49,829
behavior and stuff like that so maybe

00:33:47,639 --> 00:33:56,669
check like that that should be a good

00:33:49,829 --> 00:33:59,969
starting point I'd say any more

00:33:56,669 --> 00:34:02,069
questions no okay so I thank you very

00:33:59,969 --> 00:34:02,600
much for your talk for ruining my round

00:34:02,069 --> 00:34:07,980
of applause

00:34:02,600 --> 00:34:07,980

YouTube URL: https://www.youtube.com/watch?v=tMNcqTfd7Qc


