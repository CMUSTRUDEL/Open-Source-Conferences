Title: Berlin Buzzwords 2019: Jim Dowling â€“ Hops in the Cloud #bbuzz
Publication date: 2019-06-28
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Hops is a European open-source, next-generation distribution of Apache Hadoop that is being repurposed for the cloud. In this talk, we will walk through some of recent technical developments in Hops, including solving the small files problem by stuffing them in metadata using NVMe disks, free-text search of file system with extended metadata (this is great for automated annotation of millions of images and then finding them in milliseconds with consistent), and most interestingly data-center level HA for HopsFS with millions of filesystem operations per second on real industrial workloads. 

So yes, we will tell you why a POSIX-style hierarchical filesystem with indexed extensible metadata is superior to an object store. Finally, we can show you what else you can do with Hops, and how we built Hopsworks, a horizontally scalable secure platform for Data and AI, using Hops' extended metadata.

Read more:
https://2019.berlinbuzzwords.de/19/session/hops-cloud

About Jim Dowling:
https://2019.berlinbuzzwords.de/users/jim-dowling

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,240 --> 00:00:12,360
so my name is Jim I'm coming from stock

00:00:09,750 --> 00:00:14,769
and I represent a company

00:00:12,360 --> 00:00:18,849
and we build an open-source

00:00:14,769 --> 00:00:21,789
called hops or hops works and let's go

00:00:18,849 --> 00:00:22,570
get going so you may or may not have

00:00:21,789 --> 00:00:24,669
heard of hops works

00:00:22,570 --> 00:00:29,199
I'm just curious show of hands how many

00:00:24,669 --> 00:00:32,140
have heard of hops a few okay so we're

00:00:29,199 --> 00:00:35,710
European firstly and we started out life

00:00:32,140 --> 00:00:38,320
as a next generation of distribution of

00:00:35,710 --> 00:00:40,899
Hadoop so we actually made Hadoop and in

00:00:38,320 --> 00:00:44,079
particular HDFS 16 times faster we did

00:00:40,899 --> 00:00:46,480
not work with Spotify and we won some

00:00:44,079 --> 00:00:48,159
prizes for us and because we were so

00:00:46,480 --> 00:00:50,350
good at Hadoop and we'd built up a team

00:00:48,159 --> 00:00:52,359
we added GPUs to Hadoop before anybody

00:00:50,350 --> 00:00:54,489
else did and we've worked on the file

00:00:52,359 --> 00:00:56,649
system since then we've added support

00:00:54,489 --> 00:00:59,800
for small files in the metadata layer

00:00:56,649 --> 00:01:01,260
using nvme disks and at the end of last

00:00:59,800 --> 00:01:04,239
year we released the world's first

00:01:01,260 --> 00:01:05,379
feature store for machine learning it's

00:01:04,239 --> 00:01:07,240
a data warehouse for machine learning

00:01:05,379 --> 00:01:08,350
features so we've done lots of new cool

00:01:07,240 --> 00:01:10,780
things we come from a research

00:01:08,350 --> 00:01:13,540
background but we're a company in our

00:01:10,780 --> 00:01:15,430
vc-backed company and I guess the big

00:01:13,540 --> 00:01:17,049
thing here is that you know we're not in

00:01:15,430 --> 00:01:18,970
the cloud this is Hadoop this is

00:01:17,049 --> 00:01:21,610
primarily on-premise but I'm gonna talk

00:01:18,970 --> 00:01:24,460
today a bit about the next part of our

00:01:21,610 --> 00:01:27,100
journey it's actually a world first it's

00:01:24,460 --> 00:01:30,910
the first hierarchical file system that

00:01:27,100 --> 00:01:33,610
is multi data center h-a that means you

00:01:30,910 --> 00:01:36,070
can run a POSIX like file system in the

00:01:33,610 --> 00:01:38,470
cloud across availability zones and you

00:01:36,070 --> 00:01:40,750
don't necessarily need to make

00:01:38,470 --> 00:01:43,270
compromises in terms of using an object

00:01:40,750 --> 00:01:45,010
store but it's gonna be a bit of a

00:01:43,270 --> 00:01:46,750
journey to get there now firstly I just

00:01:45,010 --> 00:01:48,400
tell you a bit about the platform we

00:01:46,750 --> 00:01:50,950
don't actually sell hops as a platform

00:01:48,400 --> 00:01:52,690
hops was our Hadoop distribution our

00:01:50,950 --> 00:01:54,580
marketing people say do not mention

00:01:52,690 --> 00:01:56,920
Hadoop you are not allowed mention

00:01:54,580 --> 00:01:58,630
Hadoop it's not cool so what I'll talk

00:01:56,920 --> 00:02:00,910
about are properties of the front of our

00:01:58,630 --> 00:02:03,790
platform that are unique so nobody else

00:02:00,910 --> 00:02:05,800
has them in an open source platform so

00:02:03,790 --> 00:02:08,560
the first one is that if you have

00:02:05,800 --> 00:02:10,479
sensitive data and you want to put that

00:02:08,560 --> 00:02:12,940
data into a cluster let's say a Hadoop

00:02:10,479 --> 00:02:15,280
cluster you cannot prevent people from

00:02:12,940 --> 00:02:17,050
reading it and maybe writing it to some

00:02:15,280 --> 00:02:19,030
other place in the cluster so you can't

00:02:17,050 --> 00:02:21,129
sandbox your data allow people to

00:02:19,030 --> 00:02:22,960
process it in place but in our platform

00:02:21,129 --> 00:02:24,580
you can you can do that with something

00:02:22,960 --> 00:02:26,590
called projects it's a new abstraction

00:02:24,580 --> 00:02:28,110
we'll see later on that we need

00:02:26,590 --> 00:02:31,440
distributed metadata

00:02:28,110 --> 00:02:32,970
and TLS certificates to do that the

00:02:31,440 --> 00:02:35,820
second thing is if you want to have a

00:02:32,970 --> 00:02:37,890
platform today that's on premise that

00:02:35,820 --> 00:02:40,440
has more GPUs than run on a single host

00:02:37,890 --> 00:02:44,280
ours is the only platform globally to do

00:02:40,440 --> 00:02:46,020
that on if you want to have Python per

00:02:44,280 --> 00:02:47,760
project so each project that you have

00:02:46,020 --> 00:02:50,160
you'd like to have different versions of

00:02:47,760 --> 00:02:52,650
scikit-learn or tensorflow you can do

00:02:50,160 --> 00:02:53,400
that the feature store I'll talk a bit

00:02:52,650 --> 00:02:55,200
about that later

00:02:53,400 --> 00:02:57,060
we're the only enterprise features store

00:02:55,200 --> 00:02:58,680
out there you can write your code as

00:02:57,060 --> 00:03:00,660
Jupiter notebooks and run them as jobs

00:02:58,680 --> 00:03:03,330
orchestrated by airflow so you can write

00:03:00,660 --> 00:03:05,640
full pipelines in Python if you if

00:03:03,330 --> 00:03:07,650
that's your language of choice which

00:03:05,640 --> 00:03:08,970
isn't mostly here but I talk a lot to

00:03:07,650 --> 00:03:11,310
pipe some people and they like that

00:03:08,970 --> 00:03:13,590
and another thing that we do is free

00:03:11,310 --> 00:03:15,959
text search you can you can search for

00:03:13,590 --> 00:03:17,400
any file in the file system and you can

00:03:15,959 --> 00:03:18,690
have hundreds of millions of files and

00:03:17,400 --> 00:03:21,480
find them in less than a second or

00:03:18,690 --> 00:03:23,250
directories or tags on those files so

00:03:21,480 --> 00:03:24,600
this is something you're probably used

00:03:23,250 --> 00:03:27,120
to at Dropbox and think it's normal

00:03:24,600 --> 00:03:29,700
right but try doing that in s3 right

00:03:27,120 --> 00:03:31,380
it's not gonna work and then finally

00:03:29,700 --> 00:03:32,970
we're the only only distributed file

00:03:31,380 --> 00:03:35,100
system to store small files in the

00:03:32,970 --> 00:03:36,630
metadata layer with nvme discs that's

00:03:35,100 --> 00:03:38,519
some of our unique things that we do and

00:03:36,630 --> 00:03:39,930
you know we know we're not that

00:03:38,519 --> 00:03:43,380
well-known we're European so we're

00:03:39,930 --> 00:03:45,060
trying to get the message out there so I

00:03:43,380 --> 00:03:46,019
thought to get the message out there I'd

00:03:45,060 --> 00:03:48,600
say well what can you do on your

00:03:46,019 --> 00:03:51,239
platform that's kind of cool so we have

00:03:48,600 --> 00:03:54,060
a customer a large automotive customer

00:03:51,239 --> 00:03:56,640
through a partner and what the partner

00:03:54,060 --> 00:03:59,730
is doing with us is that we're trying to

00:03:56,640 --> 00:04:01,170
take lots and lots of images do image

00:03:59,730 --> 00:04:03,390
classification on them so you can think

00:04:01,170 --> 00:04:05,760
self-driving cars but what kind of

00:04:03,390 --> 00:04:07,290
platform would you need to do that well

00:04:05,760 --> 00:04:09,360
in our platform you can take like a

00:04:07,290 --> 00:04:12,590
million images insert them into the file

00:04:09,360 --> 00:04:15,810
system in just seconds you can train

00:04:12,590 --> 00:04:17,100
models on all of that data again in

00:04:15,810 --> 00:04:18,959
minutes instead of hours by using

00:04:17,100 --> 00:04:21,299
hundreds of GPUs if you have them you

00:04:18,959 --> 00:04:23,040
can then run a spark job on the model

00:04:21,299 --> 00:04:25,410
that you've trained and what you can do

00:04:23,040 --> 00:04:27,660
is you can take all of the images the

00:04:25,410 --> 00:04:29,910
files as they currently are and you can

00:04:27,660 --> 00:04:32,940
annotate them by attaching a JSON object

00:04:29,910 --> 00:04:34,380
to them in the file system and that JSON

00:04:32,940 --> 00:04:36,600
object will magically appear over in

00:04:34,380 --> 00:04:38,550
lastik and then you can search an

00:04:36,600 --> 00:04:40,470
elastic and say please show me all of

00:04:38,550 --> 00:04:41,760
the images here that have more than

00:04:40,470 --> 00:04:43,650
three bicycles or two

00:04:41,760 --> 00:04:45,720
hours or something like that and get

00:04:43,650 --> 00:04:47,100
that response in sub-second now if

00:04:45,720 --> 00:04:48,360
you're an ops person you say well I

00:04:47,100 --> 00:04:50,520
could kind of do this I could put

00:04:48,360 --> 00:04:52,920
together a key value store and a file

00:04:50,520 --> 00:04:55,170
system and sort of elastic and hack it

00:04:52,920 --> 00:04:56,700
all together and but we've done the hard

00:04:55,170 --> 00:04:58,770
work for that we have the eventual

00:04:56,700 --> 00:05:01,110
consistency protocols between the method

00:04:58,770 --> 00:05:03,270
extended metadata and elastic and ops FS

00:05:01,110 --> 00:05:04,740
and what that means is if you take your

00:05:03,270 --> 00:05:06,720
million images and just remove the

00:05:04,740 --> 00:05:09,330
directory all the metadata is cleaned up

00:05:06,720 --> 00:05:11,010
it's all consistent if you're a data

00:05:09,330 --> 00:05:12,330
scientist the thing that's cool for you

00:05:11,010 --> 00:05:15,240
is that you can do all this in Python

00:05:12,330 --> 00:05:18,630
you can write all this in Python and and

00:05:15,240 --> 00:05:21,570
they like that okay so this sounds kind

00:05:18,630 --> 00:05:23,790
of cool but the problem is the elephant

00:05:21,570 --> 00:05:26,550
in the room is kubernetes like we are

00:05:23,790 --> 00:05:28,440
not kubernetes today my answer to that

00:05:26,550 --> 00:05:29,730
is kubernetes is just an implementation

00:05:28,440 --> 00:05:32,310
details whether you're yarn or

00:05:29,730 --> 00:05:33,960
kubernetes doesn't really matter but the

00:05:32,310 --> 00:05:36,570
one thing about kubernetes that people

00:05:33,960 --> 00:05:38,820
don't talk about is that if you're going

00:05:36,570 --> 00:05:42,060
to the cloud where is the data everyone

00:05:38,820 --> 00:05:45,840
assumes that data has to be basically in

00:05:42,060 --> 00:05:48,210
s3 right but does it and that's the the

00:05:45,840 --> 00:05:51,570
contents of the top primarily canwe is

00:05:48,210 --> 00:05:53,910
there is there future beyond s3 and my

00:05:51,570 --> 00:05:57,780
some of my issues with s3 is that what

00:05:53,910 --> 00:05:59,550
is s3 because s3 and Amazon means if I

00:05:57,780 --> 00:06:01,590
insert a file in the directory and then

00:05:59,550 --> 00:06:03,840
list the directory the file may not be

00:06:01,590 --> 00:06:06,810
there but Google Cloud we've actually

00:06:03,840 --> 00:06:08,970
solved that problem but Amazon haven't

00:06:06,810 --> 00:06:11,130
what's the same API so what is the

00:06:08,970 --> 00:06:13,890
behavior of the s3 API is it whatever

00:06:11,130 --> 00:06:15,390
amazon says so that's one of the

00:06:13,890 --> 00:06:16,800
challenges that I think everyone is

00:06:15,390 --> 00:06:18,330
having going to the cloud are you going

00:06:16,800 --> 00:06:20,700
to rewrite all our applications against

00:06:18,330 --> 00:06:21,570
s3 because lots of applications just

00:06:20,700 --> 00:06:24,270
won't work

00:06:21,570 --> 00:06:26,580
lots of applications assume implicitly

00:06:24,270 --> 00:06:29,250
that you can do atomic renaming of files

00:06:26,580 --> 00:06:31,100
that you can list the directory contents

00:06:29,250 --> 00:06:34,380
the directory and a file will be there

00:06:31,100 --> 00:06:35,700
so let's have a look we're gonna start

00:06:34,380 --> 00:06:38,310
in a bit of a journey so I am a

00:06:35,700 --> 00:06:40,890
professor also a kth associate professor

00:06:38,310 --> 00:06:42,390
so I'll do a little bit of I'm gonna try

00:06:40,890 --> 00:06:45,090
and convince you that this is a journey

00:06:42,390 --> 00:06:48,510
that we will all make at some stage with

00:06:45,090 --> 00:06:50,850
an analogy from databases so let's go

00:06:48,510 --> 00:06:52,410
back in time all the way back where was

00:06:50,850 --> 00:06:54,720
our data stored at the beginning well as

00:06:52,410 --> 00:06:55,440
on punch cards and you know they had

00:06:54,720 --> 00:06:58,350
MapReduce

00:06:55,440 --> 00:07:00,840
it's actually true to process that data

00:06:58,350 --> 00:07:03,630
I wasn't particularly fault-tolerant to

00:07:00,840 --> 00:07:05,730
so it wasn't so cool and but magnetic

00:07:03,630 --> 00:07:07,560
hard disks appeared in 1956 and the

00:07:05,730 --> 00:07:09,720
problem with the magnetic hard disks at

00:07:07,560 --> 00:07:11,970
that time was that you needed to know

00:07:09,720 --> 00:07:13,890
when you wanted to read a file what part

00:07:11,970 --> 00:07:15,840
of the disk was it on what sector was it

00:07:13,890 --> 00:07:17,820
on and in fact the block size in your

00:07:15,840 --> 00:07:19,350
file system was tightly coupled to the

00:07:17,820 --> 00:07:20,670
sector so you would change the box size

00:07:19,350 --> 00:07:23,310
if you're on a different sector of the

00:07:20,670 --> 00:07:24,870
disk now this is you know it is alien to

00:07:23,310 --> 00:07:26,760
us at this point in time we have fixed

00:07:24,870 --> 00:07:29,550
block sizes now for our file systems and

00:07:26,760 --> 00:07:31,080
but when the first databases came out

00:07:29,550 --> 00:07:33,120
what you have to do is you have to say

00:07:31,080 --> 00:07:34,560
well I want to read this record and you

00:07:33,120 --> 00:07:36,090
would have to tell the data but if it's

00:07:34,560 --> 00:07:38,490
a hierarchical and network database you

00:07:36,090 --> 00:07:39,990
have to say go to this particular sector

00:07:38,490 --> 00:07:41,880
of the disk on this cylinder and Vario

00:07:39,990 --> 00:07:44,400
find the record and this was done for

00:07:41,880 --> 00:07:45,510
efficiency if you didn't do it this way

00:07:44,400 --> 00:07:49,410
you just wouldn't have an efficient

00:07:45,510 --> 00:07:51,600
database so the nice thing was that when

00:07:49,410 --> 00:07:53,970
sequel came out sequel was originally

00:07:51,600 --> 00:07:56,820
the relational model introduced by card

00:07:53,970 --> 00:07:59,130
but also the work done by Jim Gray on

00:07:56,820 --> 00:08:00,660
transactions and indexing and with those

00:07:59,130 --> 00:08:02,160
two were put together you now had a

00:08:00,660 --> 00:08:05,130
sequel database which we know about

00:08:02,160 --> 00:08:07,320
today so you would specify your queries

00:08:05,130 --> 00:08:09,570
in a relational algebra and then the

00:08:07,320 --> 00:08:13,680
system or platform by IBM showed how you

00:08:09,570 --> 00:08:15,840
could take that abstract language sequel

00:08:13,680 --> 00:08:18,630
and converted into efficient disk

00:08:15,840 --> 00:08:19,919
accesses using indexes so now you didn't

00:08:18,630 --> 00:08:23,040
have to go look through the disk and

00:08:19,919 --> 00:08:25,320
know about where in the distal to find

00:08:23,040 --> 00:08:27,540
your data the problem of course was that

00:08:25,320 --> 00:08:29,940
that as data volumes grew these

00:08:27,540 --> 00:08:32,219
databases just couldn't cope right so a

00:08:29,940 --> 00:08:33,539
single service SQL database just

00:08:32,219 --> 00:08:35,610
couldn't cope and we know what kind of

00:08:33,539 --> 00:08:37,650
happened there at that point you know we

00:08:35,610 --> 00:08:39,900
had this evolution where we start out

00:08:37,650 --> 00:08:42,150
with called sequel and system or and we

00:08:39,900 --> 00:08:44,219
still have those systems today but no

00:08:42,150 --> 00:08:45,690
sequel is born but they need to scale we

00:08:44,219 --> 00:08:48,030
need to scale these databases but we

00:08:45,690 --> 00:08:50,400
have to compromise so what we have to do

00:08:48,030 --> 00:08:53,010
is we have to basically give up notions

00:08:50,400 --> 00:08:55,770
of consistent data we would we would

00:08:53,010 --> 00:08:57,900
embrace eventual consistency I insert

00:08:55,770 --> 00:08:59,910
something I read something it may not be

00:08:57,900 --> 00:09:01,380
exactly what I think it was now the

00:08:59,910 --> 00:09:02,250
application has to be rewritten to

00:09:01,380 --> 00:09:04,230
handler

00:09:02,250 --> 00:09:06,660
so many of you will know that in recent

00:09:04,230 --> 00:09:08,580
years there's been a new class of

00:09:06,660 --> 00:09:11,190
database systems that have basically

00:09:08,580 --> 00:09:12,960
said hang on we can scale would still be

00:09:11,190 --> 00:09:15,450
consistent and they're called new sequel

00:09:12,960 --> 00:09:18,660
databases so spanner by Google is very

00:09:15,450 --> 00:09:20,820
well known cockroach mem sequel I'd even

00:09:18,660 --> 00:09:23,580
put in there and then NDB which is an

00:09:20,820 --> 00:09:25,740
open source again European database

00:09:23,580 --> 00:09:27,570
built by Ericsson originally now called

00:09:25,740 --> 00:09:29,640
my sequel cluster it's part of my sequel

00:09:27,570 --> 00:09:31,950
family but it's an in-memory distributed

00:09:29,640 --> 00:09:35,640
database I actually worked on that team

00:09:31,950 --> 00:09:37,620
so that's part of our of our product so

00:09:35,640 --> 00:09:39,690
yeah that's the analogy right that we

00:09:37,620 --> 00:09:41,730
went from single server we gave away

00:09:39,690 --> 00:09:43,740
notions of consistency to scale and then

00:09:41,730 --> 00:09:45,900
we went back to consistent databases

00:09:43,740 --> 00:09:47,610
when we could solve that problem but in

00:09:45,900 --> 00:09:50,270
file systems where are we today we had

00:09:47,610 --> 00:09:52,530
POSIX is the father of api's and and

00:09:50,270 --> 00:09:54,660
semantics for file systems you have this

00:09:52,530 --> 00:09:56,040
POSIX standard you know you insert a

00:09:54,660 --> 00:09:58,920
file into a directory your list it

00:09:56,040 --> 00:10:01,020
should be there and we've managed to

00:09:58,920 --> 00:10:03,690
scale distributed file systems to single

00:10:01,020 --> 00:10:05,760
data centers quite well so you have NFS

00:10:03,690 --> 00:10:07,320
HDFS is POSIX like they're you know

00:10:05,760 --> 00:10:11,370
missing a little bit of politics but

00:10:07,320 --> 00:10:13,380
mostly it's there but s3 was developed

00:10:11,370 --> 00:10:15,510
and s3 like object stores were developed

00:10:13,380 --> 00:10:18,060
basically to help us scale our file

00:10:15,510 --> 00:10:19,830
systems to multiple data centers but we

00:10:18,060 --> 00:10:22,140
gave up the notion of consistent

00:10:19,830 --> 00:10:24,390
metadata we said we're good enough with

00:10:22,140 --> 00:10:25,560
eventually consistent metadata they

00:10:24,390 --> 00:10:27,690
probably know what's going to happen now

00:10:25,560 --> 00:10:30,300
I'm gonna say well hang on we're gonna

00:10:27,690 --> 00:10:33,450
can we go back to POSIX like or POSIX

00:10:30,300 --> 00:10:36,840
file systems and still scale across data

00:10:33,450 --> 00:10:39,090
centers and the answer is yes our file

00:10:36,840 --> 00:10:40,650
system hops FS is a hierarchical file

00:10:39,090 --> 00:10:43,230
system so it's suppose X like file

00:10:40,650 --> 00:10:45,780
system with HTS API Google compute

00:10:43,230 --> 00:10:47,700
Google Cloud store is going that way

00:10:45,780 --> 00:10:49,620
they're adding bits because they're

00:10:47,700 --> 00:10:52,430
building on spanner so they're using it

00:10:49,620 --> 00:10:56,010
this consistent metadata layered spanner

00:10:52,430 --> 00:10:58,230
to build at their file system and we're

00:10:56,010 --> 00:11:02,610
using NDB is not consistent metadata

00:10:58,230 --> 00:11:04,980
layer okay so why is strongly consistent

00:11:02,610 --> 00:11:06,780
metadata for file systems important well

00:11:04,980 --> 00:11:08,460
I mentioned already even certified you'd

00:11:06,780 --> 00:11:11,070
like it to be there POSIX like semantics

00:11:08,460 --> 00:11:13,440
are important applications expect this

00:11:11,070 --> 00:11:14,769
behavior and if you try and port an

00:11:13,440 --> 00:11:16,480
application from an

00:11:14,769 --> 00:11:19,600
premise or a legacy application to the

00:11:16,480 --> 00:11:21,579
cloud to work with s3 immediate you you

00:11:19,600 --> 00:11:23,769
may encounter issues and then you end up

00:11:21,579 --> 00:11:26,529
either rewriting your application or

00:11:23,769 --> 00:11:28,899
looking to some file system that like

00:11:26,529 --> 00:11:32,170
NFS or something like that that Amazon

00:11:28,899 --> 00:11:33,899
has I think one code was called Amazon

00:11:32,170 --> 00:11:36,610
file system I can remember the neighbors

00:11:33,899 --> 00:11:38,139
the other issue is atomic rename which

00:11:36,610 --> 00:11:40,929
if it seems like a very small thing it's

00:11:38,139 --> 00:11:43,089
supported in HDFS but not in s3 but it

00:11:40,929 --> 00:11:45,009
means all of the sequel databases you

00:11:43,089 --> 00:11:47,470
know the the snowflake had to be

00:11:45,009 --> 00:11:50,439
rewritten entirely to work with s3 but

00:11:47,470 --> 00:11:53,949
the hive's and the impalas of this world

00:11:50,439 --> 00:11:56,319
and spark sequel for that matter didn't

00:11:53,949 --> 00:11:58,269
work on s3 easily so they had to do a

00:11:56,319 --> 00:11:59,920
lot of work to make them rewrite them to

00:11:58,269 --> 00:12:03,970
work with s3 and even then they don't

00:11:59,920 --> 00:12:05,379
work as as well and then finally when

00:12:03,970 --> 00:12:06,910
you have strongly consistent metadata

00:12:05,379 --> 00:12:09,100
there's something really cool you can do

00:12:06,910 --> 00:12:11,679
that you may not be aware of which is

00:12:09,100 --> 00:12:13,420
you can get a consistent changelog from

00:12:11,679 --> 00:12:15,519
the database of what's happening in your

00:12:13,420 --> 00:12:17,739
filesystem metadata so that means you

00:12:15,519 --> 00:12:20,350
can you can stream that with some work

00:12:17,739 --> 00:12:22,149
to a system like elastic and then you

00:12:20,350 --> 00:12:24,069
can actually search for through your

00:12:22,149 --> 00:12:26,079
entire file system and you can do that

00:12:24,069 --> 00:12:27,040
in a consistent manner and the other

00:12:26,079 --> 00:12:29,079
thing that we're doing for machine

00:12:27,040 --> 00:12:31,569
learning is that you can basically also

00:12:29,079 --> 00:12:35,589
capture all of the file system events so

00:12:31,569 --> 00:12:37,899
if I write a PB file to a models data

00:12:35,589 --> 00:12:38,649
set I kind of know it's it's a protocol

00:12:37,899 --> 00:12:41,199
buffers file

00:12:38,649 --> 00:12:43,329
it's a tensor flow model a train tensor

00:12:41,199 --> 00:12:46,119
model so I can actually tag that and say

00:12:43,329 --> 00:12:48,160
that's a tense flow model and I can say

00:12:46,119 --> 00:12:49,959
ok it was run by this application and

00:12:48,160 --> 00:12:54,040
this application read this particular

00:12:49,959 --> 00:12:56,350
file that's that's a TF records file ok

00:12:54,040 --> 00:12:58,749
well then then I can see now that this

00:12:56,350 --> 00:13:01,449
model was trained by these TF records

00:12:58,749 --> 00:13:03,519
and so on so the file system helps you

00:13:01,449 --> 00:13:05,829
do that implicitly without you having to

00:13:03,519 --> 00:13:07,959
explicitly rewrite higher-level

00:13:05,829 --> 00:13:09,279
frameworks to work with data provenance

00:13:07,959 --> 00:13:11,619
and that's what's happening in machine

00:13:09,279 --> 00:13:13,779
learning we have T effects by Google we

00:13:11,619 --> 00:13:16,059
have ml flow by data breaks the

00:13:13,779 --> 00:13:17,529
rewriting the higher levels to put in

00:13:16,059 --> 00:13:21,249
the data provenance but we're doing it

00:13:17,529 --> 00:13:23,679
implicitly ok a little bit about the

00:13:21,249 --> 00:13:25,569
file system it's gonna be a little I'm

00:13:23,679 --> 00:13:27,249
not going to get too technical but just

00:13:25,569 --> 00:13:27,610
that you know what top surface is if

00:13:27,249 --> 00:13:30,400
there

00:13:27,610 --> 00:13:32,620
file system we basically have data nodes

00:13:30,400 --> 00:13:36,220
that store the the file data the block

00:13:32,620 --> 00:13:37,720
data we have named notes and we have a

00:13:36,220 --> 00:13:39,040
leader elected amongst them there's a

00:13:37,720 --> 00:13:41,320
leader election algorithm vendor and

00:13:39,040 --> 00:13:43,540
then we have this in memory database

00:13:41,320 --> 00:13:46,090
back in for the metadata and we also can

00:13:43,540 --> 00:13:47,830
store small files and nvme discs there

00:13:46,090 --> 00:13:50,590
that's kind of the architecture nets

00:13:47,830 --> 00:13:52,600
horizontally scalable at all layers and

00:13:50,590 --> 00:13:54,490
that's fine but what we need to do is

00:13:52,600 --> 00:13:57,840
make the top two layers data center AJ

00:13:54,490 --> 00:14:00,850
that's what we've been working on and

00:13:57,840 --> 00:14:02,380
going from working on a single data

00:14:00,850 --> 00:14:05,230
center to working on multiple data

00:14:02,380 --> 00:14:07,120
centers introduce a few problems so the

00:14:05,230 --> 00:14:08,770
obvious things are that there's going to

00:14:07,120 --> 00:14:11,710
be higher latency between the data

00:14:08,770 --> 00:14:13,750
centers and then you have issues related

00:14:11,710 --> 00:14:15,910
to you know network throughput bandwidth

00:14:13,750 --> 00:14:17,590
and things like that so we had to redo

00:14:15,910 --> 00:14:19,090
everything with the whole stack from the

00:14:17,590 --> 00:14:23,320
database level all the way up to the

00:14:19,090 --> 00:14:24,700
name nodes and this is a basic way it

00:14:23,320 --> 00:14:27,370
looks when you're finished it can say

00:14:24,700 --> 00:14:29,020
well we we can have this arbitrator node

00:14:27,370 --> 00:14:30,460
in zone one it's kind of like zookeeper

00:14:29,020 --> 00:14:32,230
you know make sure that if there's a

00:14:30,460 --> 00:14:35,140
split brain which which side will win

00:14:32,230 --> 00:14:37,750
and then if one of these data centers

00:14:35,140 --> 00:14:40,120
goes down so on - or zone 3 the file

00:14:37,750 --> 00:14:44,170
system is still available and that's

00:14:40,120 --> 00:14:45,910
great we did a lot of work on mitigating

00:14:44,170 --> 00:14:49,690
for the fact that file system operations

00:14:45,910 --> 00:14:51,430
may originate in this zone and go to

00:14:49,690 --> 00:14:52,480
this zone so I might write read a file

00:14:51,430 --> 00:14:54,640
across here and we did some

00:14:52,480 --> 00:14:56,560
optimizations to get it 36%

00:14:54,640 --> 00:14:59,980
performance improvement so we introduced

00:14:56,560 --> 00:15:02,140
data locality into the database and also

00:14:59,980 --> 00:15:05,500
into the - the name node layer and

00:15:02,140 --> 00:15:06,940
that's nice but what a lot of people

00:15:05,500 --> 00:15:09,600
would run it is because they you know

00:15:06,940 --> 00:15:12,040
you won't hate real h.a you just run a

00:15:09,600 --> 00:15:15,010
almost erratic of the file system at

00:15:12,040 --> 00:15:19,530
each each zone and they have tripled

00:15:15,010 --> 00:15:21,790
replication and even if you know two

00:15:19,530 --> 00:15:23,290
zones go down if you have enough

00:15:21,790 --> 00:15:25,360
replicas of your database you need three

00:15:23,290 --> 00:15:29,710
rapid your database then the thing will

00:15:25,360 --> 00:15:31,600
still be highly available so I mentioned

00:15:29,710 --> 00:15:33,280
already that the taking a consistent

00:15:31,600 --> 00:15:37,900
change log of the file system is is is

00:15:33,280 --> 00:15:40,510
is a fundamentally new and enabling

00:15:37,900 --> 00:15:42,490
technology so if you're interested we

00:15:40,510 --> 00:15:44,680
had a paper release recently at CC grid

00:15:42,490 --> 00:15:46,990
on us and the overhead of introducing

00:15:44,680 --> 00:15:49,450
this was about 4% into the file system

00:15:46,990 --> 00:15:50,950
it's not not going to kill you and but

00:15:49,450 --> 00:15:53,290
what we can do is we can take any

00:15:50,950 --> 00:15:55,750
changes in the file system metadata and

00:15:53,290 --> 00:15:57,880
push them to downstream systems so we're

00:15:55,750 --> 00:16:00,010
already doing two of the most important

00:15:57,880 --> 00:16:02,830
ones are elastic search for free text

00:16:00,010 --> 00:16:04,420
search and the other one is hive so what

00:16:02,830 --> 00:16:07,330
we've done is we put the metadata for

00:16:04,420 --> 00:16:09,520
hive into the same distributed metadata

00:16:07,330 --> 00:16:11,740
layer so now if you go to hive and

00:16:09,520 --> 00:16:15,010
record say it from the file system and

00:16:11,740 --> 00:16:17,380
you say remove this directory which is a

00:16:15,010 --> 00:16:18,730
hive database hives cleaned up the

00:16:17,380 --> 00:16:20,710
metadata cleaned up everything's cleaned

00:16:18,730 --> 00:16:22,900
up we actually do most of that with

00:16:20,710 --> 00:16:25,300
foreign keys but the hive has a few

00:16:22,900 --> 00:16:27,550
tables that are kind of orphaned so

00:16:25,300 --> 00:16:29,140
that's why we we take these cleaning

00:16:27,550 --> 00:16:32,080
events and just clean a pipe if that

00:16:29,140 --> 00:16:34,150
happens but this is fully extensible and

00:16:32,080 --> 00:16:36,930
we will extend it over time to add

00:16:34,150 --> 00:16:39,760
support to different systems

00:16:36,930 --> 00:16:42,010
so to summarize kind of where we are at

00:16:39,760 --> 00:16:45,220
the file system my view is that that

00:16:42,010 --> 00:16:46,900
like POSIX as the Empire the ancient

00:16:45,220 --> 00:16:50,110
empire is striking back right we're kind

00:16:46,900 --> 00:16:52,570
of saying hang on s3 it's not going to

00:16:50,110 --> 00:16:53,830
cut it what if I have a file system that

00:16:52,570 --> 00:16:56,920
could be highly available across data

00:16:53,830 --> 00:16:59,980
centers it can hit 1.6 million ops per

00:16:56,920 --> 00:17:02,290
second on a Hadoop workload by Spotify

00:16:59,980 --> 00:17:05,199
and this was done these experiments were

00:17:02,290 --> 00:17:07,630
done on Google Cloud and you can

00:17:05,199 --> 00:17:09,100
introduce nvme discs to store small

00:17:07,630 --> 00:17:12,430
files and that will help with the HT of

00:17:09,100 --> 00:17:14,620
a small files problem and all of this is

00:17:12,430 --> 00:17:15,370
done with TLS security which is the real

00:17:14,620 --> 00:17:17,050
killer

00:17:15,370 --> 00:17:19,780
going from on prem if you go from

00:17:17,050 --> 00:17:23,470
Kerberos to TLS that's going to kill you

00:17:19,780 --> 00:17:25,360
so we do it with TLS and you have HTS

00:17:23,470 --> 00:17:29,260
API this is ready you can use it now

00:17:25,360 --> 00:17:30,700
today in the cloud but the big thought

00:17:29,260 --> 00:17:32,440
is it's gonna cost you isn't it

00:17:30,700 --> 00:17:34,870
I mean it's gonna cost a little bit more

00:17:32,440 --> 00:17:36,400
like the estimates I read is that you

00:17:34,870 --> 00:17:39,520
know it's four times more expensive to

00:17:36,400 --> 00:17:41,350
have your data in HDFS than s3 so we're

00:17:39,520 --> 00:17:43,090
doing a parallel project right now that

00:17:41,350 --> 00:17:45,250
we've started we don't have any good

00:17:43,090 --> 00:17:47,170
results yet to put our block data in

00:17:45,250 --> 00:17:50,110
test three so we'll have the combination

00:17:47,170 --> 00:17:52,870
of the scalar metadata layer from how

00:17:50,110 --> 00:17:53,100
suffice with the block data in in s3 and

00:17:52,870 --> 00:17:56,610
then

00:17:53,100 --> 00:17:57,720
the cost will be comparative with s3 so

00:17:56,610 --> 00:18:00,990
that's kind of where we're going we're

00:17:57,720 --> 00:18:03,180
not quite there yet but why do you think

00:18:00,990 --> 00:18:05,060
hops is interesting because that's just

00:18:03,180 --> 00:18:07,920
the kind of enabling layer at the bottom

00:18:05,060 --> 00:18:09,780
so our company logical clocks we sell a

00:18:07,920 --> 00:18:11,670
platform cold helps works it's open

00:18:09,780 --> 00:18:13,350
source you can go and grab it and use it

00:18:11,670 --> 00:18:15,810
but you know we're a vendor we provide

00:18:13,350 --> 00:18:19,290
support and licensing and and all those

00:18:15,810 --> 00:18:21,900
things and what we market it as we say

00:18:19,290 --> 00:18:23,640
it's it's a a data intensive AI platform

00:18:21,900 --> 00:18:26,640
right which is completely buzzword

00:18:23,640 --> 00:18:28,620
compliance which is totally fine right

00:18:26,640 --> 00:18:30,210
you have to have a buzzword and I hadn't

00:18:28,620 --> 00:18:32,010
seen anyone else write data intensive a

00:18:30,210 --> 00:18:34,590
and I so you cannot get a corner of the

00:18:32,010 --> 00:18:36,750
market on that but what data intensive

00:18:34,590 --> 00:18:39,330
AI means is that you know people think

00:18:36,750 --> 00:18:41,160
AI is about resource allocation how to

00:18:39,330 --> 00:18:43,530
get my GPUs and things like that but

00:18:41,160 --> 00:18:45,120
really it's about managing data and how

00:18:43,530 --> 00:18:47,390
do you manage the pipeline's of data

00:18:45,120 --> 00:18:52,080
from your data lake or wherever it is

00:18:47,390 --> 00:18:53,880
into training the models and that's the

00:18:52,080 --> 00:18:58,560
challenge that I guess the industry is

00:18:53,880 --> 00:18:59,760
facing as a whole so if you have many of

00:18:58,560 --> 00:19:01,470
you will not have a machine learning

00:18:59,760 --> 00:19:02,730
background that this this slide you may

00:19:01,470 --> 00:19:04,020
not have seen before but if you have

00:19:02,730 --> 00:19:06,840
done machine learning I apologize

00:19:04,020 --> 00:19:08,670
because everyone shows us what this site

00:19:06,840 --> 00:19:11,040
is from a Google paper but it got called

00:19:08,670 --> 00:19:13,710
skully and some other co-authors it

00:19:11,040 --> 00:19:15,570
basically says that what people think is

00:19:13,710 --> 00:19:17,520
machine learning or deep learning is

00:19:15,570 --> 00:19:20,130
taking some data training a model of

00:19:17,520 --> 00:19:22,020
making predictions and if you do a

00:19:20,130 --> 00:19:24,180
course in deep learning and there's a

00:19:22,020 --> 00:19:25,770
course call fast it's really good fast

00:19:24,180 --> 00:19:28,470
AI they give you

00:19:25,770 --> 00:19:30,120
ready-made datasets you train them you

00:19:28,470 --> 00:19:31,680
feel really good and go I'm an expert

00:19:30,120 --> 00:19:35,130
now but when you go to the real world

00:19:31,680 --> 00:19:37,230
your data is not in a nice clean CSV

00:19:35,130 --> 00:19:39,720
file with no missing values it's it's

00:19:37,230 --> 00:19:41,430
all over the place it's in you know it's

00:19:39,720 --> 00:19:44,850
in your hive you're Cassandra your

00:19:41,430 --> 00:19:45,990
objects store or wherever and you need

00:19:44,850 --> 00:19:47,580
to integrate and pull that all together

00:19:45,990 --> 00:19:49,260
you need to make sure that you can

00:19:47,580 --> 00:19:51,150
collect the data you need to engineer it

00:19:49,260 --> 00:19:53,250
you need to validate the data make sure

00:19:51,150 --> 00:19:57,270
there's no missing values impute values

00:19:53,250 --> 00:19:59,340
you need to do a lot of work in terms of

00:19:57,270 --> 00:20:00,660
managing your data and that's where the

00:19:59,340 --> 00:20:02,460
feature store comes in so the feature

00:20:00,660 --> 00:20:04,380
store is an abstraction that we provide

00:20:02,460 --> 00:20:06,540
to data scientists and you say they just

00:20:04,380 --> 00:20:08,610
go to the features store and say

00:20:06,540 --> 00:20:11,490
I want these features give me some

00:20:08,610 --> 00:20:13,320
training data and they can assume that

00:20:11,490 --> 00:20:15,840
the data that comes out is clean and

00:20:13,320 --> 00:20:17,460
it's usable in their models and on the

00:20:15,840 --> 00:20:19,080
other side the data engineers need to

00:20:17,460 --> 00:20:21,210
make sure that the features that are in

00:20:19,080 --> 00:20:23,520
the feature store do fulfill those

00:20:21,210 --> 00:20:25,980
requirements so we provide tooling for

00:20:23,520 --> 00:20:28,260
through the talk last night at the if

00:20:25,980 --> 00:20:30,360
any went to the after afters about

00:20:28,260 --> 00:20:33,390
Amazon of a framework for doing this

00:20:30,360 --> 00:20:34,650
called that we actually use and we also

00:20:33,390 --> 00:20:36,600
supporting tf-x

00:20:34,650 --> 00:20:37,830
which is support for some of that but

00:20:36,600 --> 00:20:40,290
the feature stores you're the only

00:20:37,830 --> 00:20:42,030
vendor who provide this on the other

00:20:40,290 --> 00:20:44,610
side once you have your training data

00:20:42,030 --> 00:20:46,470
data scientists want to use lots of GPUs

00:20:44,610 --> 00:20:48,270
to train the models distributed training

00:20:46,470 --> 00:20:51,120
they want to check lots of hyper

00:20:48,270 --> 00:20:52,890
parameters again using lots of GPUs they

00:20:51,120 --> 00:20:54,090
want to write pipelines to run this

00:20:52,890 --> 00:20:57,690
stuff and they need to serve their

00:20:54,090 --> 00:20:59,910
models so we provide api's not just in

00:20:57,690 --> 00:21:01,830
general rest but Python api's and even

00:20:59,910 --> 00:21:03,870
Scala if you guys to do some of these

00:21:01,830 --> 00:21:05,520
things so most of the Skylar stuff is on

00:21:03,870 --> 00:21:07,740
the left and we have Python on the left

00:21:05,520 --> 00:21:10,530
as well or Java and then on the right

00:21:07,740 --> 00:21:13,380
mostly it's Python is one way of looking

00:21:10,530 --> 00:21:16,260
at it so the platform hops works if

00:21:13,380 --> 00:21:19,410
you're to kind of just break it down

00:21:16,260 --> 00:21:21,750
it's a platform it's not a product so

00:21:19,410 --> 00:21:23,840
you know Hadoop vendors release products

00:21:21,750 --> 00:21:26,730
which are 22 services

00:21:23,840 --> 00:21:29,750
ours is a REST API so it's a platform

00:21:26,730 --> 00:21:32,310
with a single REST API and you can do

00:21:29,750 --> 00:21:35,070
scalable deep learning and pipelines on

00:21:32,310 --> 00:21:37,410
it and to do that if you want to do it

00:21:35,070 --> 00:21:40,350
on more than one machine you'll need to

00:21:37,410 --> 00:21:42,690
support some form of data parallel

00:21:40,350 --> 00:21:46,110
processing so we support both fashion

00:21:42,690 --> 00:21:47,610
streaming and we support distributed

00:21:46,110 --> 00:21:49,710
machine learning and also serving of

00:21:47,610 --> 00:21:52,650
models so in the one platform you can do

00:21:49,710 --> 00:21:55,080
all these things and that's really the

00:21:52,650 --> 00:21:57,090
whole kind of cycle of machine learning

00:21:55,080 --> 00:21:59,490
pipelines so most of you will be

00:21:57,090 --> 00:22:01,800
familiar with ETL pipelines which is

00:21:59,490 --> 00:22:03,180
kinda classic data engineering concept

00:22:01,800 --> 00:22:04,640
machine learning pipelines are pretty

00:22:03,180 --> 00:22:08,160
similar

00:22:04,640 --> 00:22:09,270
we'll see one later on so the other

00:22:08,160 --> 00:22:10,440
thing that we introduced it's novel is

00:22:09,270 --> 00:22:12,870
the feature store is a kind of an

00:22:10,440 --> 00:22:14,670
intermediate step between the data

00:22:12,870 --> 00:22:17,220
engineering on the left-hand side and

00:22:14,670 --> 00:22:18,690
then the training of models done by data

00:22:17,220 --> 00:22:20,350
scientists on the right-hand side and

00:22:18,690 --> 00:22:22,410
then we have our files

00:22:20,350 --> 00:22:24,760
I'm supporting all of this underneath it

00:22:22,410 --> 00:22:26,950
we actually do support criminals for

00:22:24,760 --> 00:22:29,140
model serving as well and because that's

00:22:26,950 --> 00:22:30,669
become the de facto standard and then if

00:22:29,140 --> 00:22:32,230
you're training models it's not just

00:22:30,669 --> 00:22:34,840
deep learning you know a lot of people

00:22:32,230 --> 00:22:37,780
still use scikit-learn and other

00:22:34,840 --> 00:22:39,100
frameworks h2o and but what data

00:22:37,780 --> 00:22:40,120
scientists really want are jupiter

00:22:39,100 --> 00:22:41,710
notebooks they want to do everything in

00:22:40,120 --> 00:22:42,850
a joopa notebook and never leave it and

00:22:41,710 --> 00:22:45,520
that's kind of what we're really

00:22:42,850 --> 00:22:47,289
focusing on and and they want tools for

00:22:45,520 --> 00:22:50,140
visualizing their models like tensor

00:22:47,289 --> 00:22:52,360
board so a couple of bigger properties

00:22:50,140 --> 00:22:55,809
of the system are that we do TLS

00:22:52,360 --> 00:22:57,309
everywhere we have this multi-tenancy

00:22:55,809 --> 00:22:59,380
property that i mentioned already we

00:22:57,309 --> 00:23:01,450
call it secure collaboration and then

00:22:59,380 --> 00:23:04,059
this thing this platform can be a daily

00:23:01,450 --> 00:23:05,620
so we have customers who don't have data

00:23:04,059 --> 00:23:07,450
lakes and they put this in and say daily

00:23:05,620 --> 00:23:09,370
and we've customers who do have data

00:23:07,450 --> 00:23:11,230
lakes and this sits beside us because

00:23:09,370 --> 00:23:15,600
the existing vendors of data X data

00:23:11,230 --> 00:23:19,240
lakes do not provide this functionality

00:23:15,600 --> 00:23:20,830
so just to sort of the overview of the

00:23:19,240 --> 00:23:22,659
platform opsworks what kind of things

00:23:20,830 --> 00:23:24,309
are in it well we have a feature store

00:23:22,659 --> 00:23:25,000
distributed deep learning up so fast

00:23:24,309 --> 00:23:26,559
bah-bah-bah

00:23:25,000 --> 00:23:28,210
you can see a lot of features up there

00:23:26,559 --> 00:23:29,950
and on the board and you know no one's

00:23:28,210 --> 00:23:33,159
gonna read these the one point I wanted

00:23:29,950 --> 00:23:35,289
to make here was that if you're building

00:23:33,159 --> 00:23:37,539
a platform like this and you'll probably

00:23:35,289 --> 00:23:39,610
see 50 other platforms claiming exactly

00:23:37,539 --> 00:23:41,740
the same features as this right so why

00:23:39,610 --> 00:23:42,570
are we different what is underlying this

00:23:41,740 --> 00:23:45,100
is different

00:23:42,570 --> 00:23:47,740
the thing we do that's different is

00:23:45,100 --> 00:23:50,320
distributed consistent metadata in our

00:23:47,740 --> 00:23:52,299
file system and in fact that propagates

00:23:50,320 --> 00:23:54,100
all the way up to stack right so we get

00:23:52,299 --> 00:23:57,100
things like secure multi-tenancy from it

00:23:54,100 --> 00:23:58,659
we get this data provenance from it we

00:23:57,100 --> 00:24:00,640
got our nice file system from it and the

00:23:58,659 --> 00:24:02,860
feature store which is also in the

00:24:00,640 --> 00:24:04,330
distribute metadata layer it comes from

00:24:02,860 --> 00:24:05,890
that many of the other things you'll

00:24:04,330 --> 00:24:09,039
find in other platforms

00:24:05,890 --> 00:24:13,870
I guess the AI asset governance is also

00:24:09,039 --> 00:24:15,970
enabled by our platform as well but you

00:24:13,870 --> 00:24:18,280
know from the from it as a tech product

00:24:15,970 --> 00:24:20,140
or a platform this is how we differ from

00:24:18,280 --> 00:24:23,230
the others so I'm just going to go

00:24:20,140 --> 00:24:24,960
through an end-to-end pipeline in in

00:24:23,230 --> 00:24:27,220
hops works because this is basically

00:24:24,960 --> 00:24:30,520
what people use the platform for

00:24:27,220 --> 00:24:33,100
primarily it looks like an ETL pipeline

00:24:30,520 --> 00:24:34,190
you have a number of steps and the the

00:24:33,100 --> 00:24:36,200
lifecycle of day

00:24:34,190 --> 00:24:38,419
in machine learning pipelines is

00:24:36,200 --> 00:24:40,789
basically that you ingest the data you

00:24:38,419 --> 00:24:42,830
do feature engineering you train your

00:24:40,789 --> 00:24:44,389
models you validate your models you

00:24:42,830 --> 00:24:46,190
validate the data before you train them

00:24:44,389 --> 00:24:47,720
as well and then you serve your models

00:24:46,190 --> 00:24:49,580
and when they're served you then want to

00:24:47,720 --> 00:24:51,679
monitor them and if everything is going

00:24:49,580 --> 00:24:53,750
okay you continue but new data will

00:24:51,679 --> 00:24:56,240
always come in and you always need to

00:24:53,750 --> 00:25:00,139
then retrain your models to make them of

00:24:56,240 --> 00:25:03,950
today's so this pipeline if it's big

00:25:00,139 --> 00:25:05,990
data you will need user to like spark or

00:25:03,950 --> 00:25:07,779
or beam and they're effectively the two

00:25:05,990 --> 00:25:11,210
ecosystems that are out there right now

00:25:07,779 --> 00:25:13,789
so we do have like full support for

00:25:11,210 --> 00:25:15,110
spark and I would say alpha support for

00:25:13,789 --> 00:25:16,759
beam right now we have a talk in the

00:25:15,110 --> 00:25:19,279
beam somewhat later later this week and

00:25:16,759 --> 00:25:22,759
but the thing I guess that to take home

00:25:19,279 --> 00:25:24,980
from this is that in the in the deep

00:25:22,759 --> 00:25:26,539
learning community we just finished the

00:25:24,980 --> 00:25:29,090
Battle of the frameworks you know we had

00:25:26,539 --> 00:25:32,450
lots of more frameworks we had MX nests

00:25:29,090 --> 00:25:34,039
and we had C NT k and Tiano and a bunch

00:25:32,450 --> 00:25:36,110
of other frameworks chainer

00:25:34,039 --> 00:25:38,419
they all just disappeared we're now down

00:25:36,110 --> 00:25:42,049
to two frameworks one is PI torch and

00:25:38,419 --> 00:25:44,600
one is tensorflow and the battle that we

00:25:42,049 --> 00:25:46,940
see appearing right now as a vendor is

00:25:44,600 --> 00:25:48,919
who will control the pipeline because

00:25:46,940 --> 00:25:50,600
whether it's tensorflow or pie charts

00:25:48,919 --> 00:25:52,129
doesn't really matter right from the

00:25:50,600 --> 00:25:55,429
pipeline perspective it's just gonna say

00:25:52,129 --> 00:25:57,080
train here's the data go train so data

00:25:55,429 --> 00:25:59,779
breaks of course are pushing spark and

00:25:57,080 --> 00:26:03,679
Google are pushing beam and we're trying

00:25:59,779 --> 00:26:05,240
to keep our options open on both so that

00:26:03,679 --> 00:26:07,309
will be interesting in terms of seeing

00:26:05,240 --> 00:26:09,049
how the community of elif's because the

00:26:07,309 --> 00:26:12,220
one thing that the community has agreed

00:26:09,049 --> 00:26:14,659
on is that the pipeline is the unit of

00:26:12,220 --> 00:26:17,330
abstraction for building a AI

00:26:14,659 --> 00:26:23,059
application pipelines are what we do to

00:26:17,330 --> 00:26:25,639
take data and spit out models okay so

00:26:23,059 --> 00:26:27,320
just to get technical on the challenges

00:26:25,639 --> 00:26:28,340
in the pipeline and these are some of

00:26:27,320 --> 00:26:30,769
the things that the different

00:26:28,340 --> 00:26:32,509
communities are working on that if

00:26:30,769 --> 00:26:35,179
you're taking in large amounts of data

00:26:32,509 --> 00:26:36,500
and you're doing feature engineering on

00:26:35,179 --> 00:26:38,389
that you need to do it with data

00:26:36,500 --> 00:26:40,250
parallel processing and typically that's

00:26:38,389 --> 00:26:44,029
done with sea views it's not done with

00:26:40,250 --> 00:26:46,279
GPUs alright and when you're training

00:26:44,029 --> 00:26:46,880
models you can use lots of GPUs not CPUs

00:26:46,279 --> 00:26:48,770
so

00:26:46,880 --> 00:26:50,480
the spark community in particular are

00:26:48,770 --> 00:26:53,120
working at saying well how can we have

00:26:50,480 --> 00:26:55,640
this as one pipeline an in-memory

00:26:53,120 --> 00:26:56,540
pipeline from start to finish and the

00:26:55,640 --> 00:26:58,100
kind of there's a project called

00:26:56,540 --> 00:27:00,350
hydrogen and they're looking at it but

00:26:58,100 --> 00:27:02,780
they're really it's difficult right it's

00:27:00,350 --> 00:27:04,160
extremely difficult to take because the

00:27:02,780 --> 00:27:05,810
the you can see it as kind of like a

00:27:04,160 --> 00:27:08,300
funnel you start with large volume of

00:27:05,810 --> 00:27:09,590
data and typically it will reduce but

00:27:08,300 --> 00:27:11,450
sometimes it may actually get back

00:27:09,590 --> 00:27:14,120
bigger and you have some we had we had a

00:27:11,450 --> 00:27:16,970
data set of 10 terabytes of transaction

00:27:14,120 --> 00:27:20,980
data and we were doing and deep learning

00:27:16,970 --> 00:27:23,870
to predict fraud or money laundering and

00:27:20,980 --> 00:27:26,180
it turned into 40 terabytes yeah because

00:27:23,870 --> 00:27:27,560
we added we took we took windows of lots

00:27:26,180 --> 00:27:28,940
of number of transactions how many

00:27:27,560 --> 00:27:31,220
transactions in the last day the last

00:27:28,940 --> 00:27:32,660
week the last month you know network

00:27:31,220 --> 00:27:34,370
embeddings lots of things you know a lot

00:27:32,660 --> 00:27:37,910
of information that you can extract from

00:27:34,370 --> 00:27:39,560
that raw data so you you have large

00:27:37,910 --> 00:27:41,810
firms of data on the left it which is

00:27:39,560 --> 00:27:46,010
typically spark and then we have lots of

00:27:41,810 --> 00:27:47,630
GPUs tensorflow or pi torch so how do

00:27:46,010 --> 00:27:49,760
you make a big pipeline out of this so

00:27:47,630 --> 00:27:50,870
the one thing that we're we're actually

00:27:49,760 --> 00:27:52,280
arguing for now is that you need

00:27:50,870 --> 00:27:54,350
distributed storage you need to have a

00:27:52,280 --> 00:27:56,000
layer here and break up this pipeline

00:27:54,350 --> 00:27:56,450
there's no point in trying to do the

00:27:56,000 --> 00:27:58,370
whole thing

00:27:56,450 --> 00:28:00,650
so spark haven't managed to do the whole

00:27:58,370 --> 00:28:02,390
thing yet but it's doubtful anything

00:28:00,650 --> 00:28:04,790
reasonable will come out in the near

00:28:02,390 --> 00:28:06,710
future and with this is the model where

00:28:04,790 --> 00:28:08,390
we're predicating upon and in fact that

00:28:06,710 --> 00:28:10,190
distributed storage layer we're calling

00:28:08,390 --> 00:28:11,900
it the feature store alright so this is

00:28:10,190 --> 00:28:14,270
the place where you will have pipelines

00:28:11,900 --> 00:28:16,490
that publish features so they'll run

00:28:14,270 --> 00:28:18,410
every day every hour the feature data

00:28:16,490 --> 00:28:20,540
will be updated sometimes you won't want

00:28:18,410 --> 00:28:22,130
to have the feature data cached in the

00:28:20,540 --> 00:28:24,890
feature story just want the how do i

00:28:22,130 --> 00:28:26,840
compute the feature in there but this

00:28:24,890 --> 00:28:29,270
needs to be horizontally scalable the

00:28:26,840 --> 00:28:30,920
feature store and then the users are

00:28:29,270 --> 00:28:32,450
data scientists who want to train models

00:28:30,920 --> 00:28:34,700
that just write training pipelines

00:28:32,450 --> 00:28:36,620
they'll say ok I haven't I want to take

00:28:34,700 --> 00:28:38,990
these features generate some training

00:28:36,620 --> 00:28:40,400
data train the model I'll do some

00:28:38,990 --> 00:28:42,620
experimentation find good hyper

00:28:40,400 --> 00:28:44,780
parameters now I'm happy with that now

00:28:42,620 --> 00:28:46,400
we can have a distributed training

00:28:44,780 --> 00:28:47,960
notebook and then I could have another

00:28:46,400 --> 00:28:50,270
notebook to validate the model and

00:28:47,960 --> 00:28:51,560
service so those three then stages

00:28:50,270 --> 00:28:54,170
become three different notebooks that

00:28:51,560 --> 00:28:58,640
you can turn into an airflow job and now

00:28:54,170 --> 00:29:00,080
it's productionize so I've mentioned the

00:28:58,640 --> 00:29:00,710
feature store a few few times so I'll

00:29:00,080 --> 00:29:01,970
just tell you some

00:29:00,710 --> 00:29:04,580
the properties of it because you may not

00:29:01,970 --> 00:29:07,039
have heard them before it's it's a data

00:29:04,580 --> 00:29:09,260
warehouse for features so features are

00:29:07,039 --> 00:29:11,480
not the same as columns in data

00:29:09,260 --> 00:29:13,640
warehouses so an example of a feature

00:29:11,480 --> 00:29:16,520
might be let's say I have a new app

00:29:13,640 --> 00:29:18,740
that's released and I want to check the

00:29:16,520 --> 00:29:21,470
adoption of the app and I want to build

00:29:18,740 --> 00:29:23,390
a model to predict the adoption of the

00:29:21,470 --> 00:29:25,070
app so I might look at when the user

00:29:23,390 --> 00:29:26,570
installed the app that's in my data

00:29:25,070 --> 00:29:29,210
warehouse I might look when the user

00:29:26,570 --> 00:29:31,760
first used the app that's in the data

00:29:29,210 --> 00:29:33,260
warehouse or registered but the time

00:29:31,760 --> 00:29:34,490
between them is not in there in the data

00:29:33,260 --> 00:29:36,320
warehouse so I have to compute that

00:29:34,490 --> 00:29:38,390
that's a feature right you could compute

00:29:36,320 --> 00:29:39,559
it live or you could store that value in

00:29:38,390 --> 00:29:41,899
the feature store it's up to you and

00:29:39,559 --> 00:29:44,240
that's a really simple feature a complex

00:29:41,899 --> 00:29:45,830
feature would be if I have lots of

00:29:44,240 --> 00:29:47,779
transactions and I want to take a

00:29:45,830 --> 00:29:49,640
snapshot of the network around a given

00:29:47,779 --> 00:29:50,929
customer who had made lots of

00:29:49,640 --> 00:29:52,309
transactions with their neighbors in the

00:29:50,929 --> 00:29:54,110
last 24 hours and I compute and network

00:29:52,309 --> 00:29:57,350
embedding for us that's a complicated

00:29:54,110 --> 00:29:58,760
feature but we work with customers of up

00:29:57,350 --> 00:30:00,470
to four hundred five hundred features in

00:29:58,760 --> 00:30:01,789
the future store uber has said they have

00:30:00,470 --> 00:30:04,250
twenty thousand features in their

00:30:01,789 --> 00:30:06,289
features store and why it's different to

00:30:04,250 --> 00:30:07,789
a data warehouse apart from the fact you

00:30:06,289 --> 00:30:11,570
can have reusable features which is

00:30:07,789 --> 00:30:13,070
great but we're using hodie to make sure

00:30:11,570 --> 00:30:14,990
you can do incremental updates to the

00:30:13,070 --> 00:30:16,370
feature story in hive is where

00:30:14,990 --> 00:30:18,440
underneath visits of actually Apache

00:30:16,370 --> 00:30:20,750
hive but instead of having to drop a

00:30:18,440 --> 00:30:22,640
table and recreate the table when you

00:30:20,750 --> 00:30:25,010
update data you can do incremental

00:30:22,640 --> 00:30:27,890
updates with with hoody iceberg O's who

00:30:25,010 --> 00:30:30,620
supports it and data breaks Delta also

00:30:27,890 --> 00:30:32,120
support that functionality you want to

00:30:30,620 --> 00:30:34,490
do data validation before the day is

00:30:32,120 --> 00:30:36,380
published into the feature store tf-x

00:30:34,490 --> 00:30:38,120
the google stands for extended model has

00:30:36,380 --> 00:30:39,260
support for this there was a talk

00:30:38,120 --> 00:30:41,330
yesterday by Amazon they have a

00:30:39,260 --> 00:30:44,090
framework for this we're using the

00:30:41,330 --> 00:30:46,159
Amazon right now and then you want to

00:30:44,090 --> 00:30:47,679
make sure that you can look at your

00:30:46,159 --> 00:30:49,580
features which ones are being used

00:30:47,679 --> 00:30:51,080
governance so you know which ones are

00:30:49,580 --> 00:30:53,240
widely used which ones aren't used when

00:30:51,080 --> 00:30:55,490
do we allow users to publish them or not

00:30:53,240 --> 00:30:56,929
and then the final one time-traveled

00:30:55,490 --> 00:31:00,110
you'll hear this mentioned more and more

00:30:56,929 --> 00:31:01,190
in the next period of time because it's

00:31:00,110 --> 00:31:02,929
a really tough one it's a really

00:31:01,190 --> 00:31:06,529
interesting new property we need from

00:31:02,929 --> 00:31:09,500
data stores if for example we identify

00:31:06,529 --> 00:31:11,029
in a case of money laundering at the end

00:31:09,500 --> 00:31:13,789
of the year but the money laundering

00:31:11,029 --> 00:31:14,380
took place today I want to go back in

00:31:13,789 --> 00:31:19,450
time

00:31:14,380 --> 00:31:21,250
to the 17th of June and say okay how

00:31:19,450 --> 00:31:23,320
many transactions did this user do in

00:31:21,250 --> 00:31:25,360
the previous 24 hours before the 17th of

00:31:23,320 --> 00:31:27,580
June before and in the last week before

00:31:25,360 --> 00:31:29,050
that in the last month before that in a

00:31:27,580 --> 00:31:30,760
typical data warehouse you'd overwrite

00:31:29,050 --> 00:31:32,710
that value those windows will be

00:31:30,760 --> 00:31:34,990
overwritten every every time you run

00:31:32,710 --> 00:31:37,690
your pipeline but now we need to

00:31:34,990 --> 00:31:40,510
actually store all of that those those

00:31:37,690 --> 00:31:42,730
those updates and that's what we're

00:31:40,510 --> 00:31:44,350
doing with with hoodie and with hoodie

00:31:42,730 --> 00:31:46,570
you can basically then say I want to go

00:31:44,350 --> 00:31:48,430
back in time tell me that the value of

00:31:46,570 --> 00:31:50,560
this feature at this particular point in

00:31:48,430 --> 00:31:52,660
time and the reason why we need to do

00:31:50,560 --> 00:31:55,060
this is because I want to generate new

00:31:52,660 --> 00:31:57,610
training data I got my fraud case at the

00:31:55,060 --> 00:31:59,230
end of the year I want to see what the

00:31:57,610 --> 00:32:00,550
features were that were used and what

00:31:59,230 --> 00:32:02,140
the prediction was made at that time

00:32:00,550 --> 00:32:04,420
because at that time I may have

00:32:02,140 --> 00:32:06,460
predicted no fraud so but I need to

00:32:04,420 --> 00:32:07,990
change the label to make it proper

00:32:06,460 --> 00:32:09,820
trained it and say actually it was fraud

00:32:07,990 --> 00:32:13,480
let's retrain the model with this new

00:32:09,820 --> 00:32:14,950
feature vector so the type of

00:32:13,480 --> 00:32:17,230
applications that use the feature store

00:32:14,950 --> 00:32:18,790
it's online apps and batch apps so a

00:32:17,230 --> 00:32:21,070
batch app might take a pre trained model

00:32:18,790 --> 00:32:23,410
train model and just read it up and use

00:32:21,070 --> 00:32:25,090
it but online apps will maybe use those

00:32:23,410 --> 00:32:28,240
models that are served over over the

00:32:25,090 --> 00:32:30,820
network from there you'll typically do

00:32:28,240 --> 00:32:33,250
your training from we actually use spark

00:32:30,820 --> 00:32:37,930
to distribute training on tensor phone

00:32:33,250 --> 00:32:39,400
by torch now the the users view of the

00:32:37,930 --> 00:32:43,360
platform is very different if you're a

00:32:39,400 --> 00:32:44,920
developer your data scientists you might

00:32:43,360 --> 00:32:46,690
say well this is another team they do

00:32:44,920 --> 00:32:47,980
the feature engineering they don't even

00:32:46,690 --> 00:32:51,040
like PI spark they want to do it in

00:32:47,980 --> 00:32:52,570
Scala fine and you start from here you

00:32:51,040 --> 00:32:54,130
basically say well I want to generate

00:32:52,570 --> 00:32:55,990
some training data and this is where the

00:32:54,130 --> 00:32:57,910
future store comes into its own because

00:32:55,990 --> 00:32:58,810
someone might like PI torch someone

00:32:57,910 --> 00:33:01,390
might like tensorflow

00:32:58,810 --> 00:33:03,460
and someone may say well once scaleable

00:33:01,390 --> 00:33:04,810
and there's different file formats for

00:33:03,460 --> 00:33:08,410
those so you have TF records for

00:33:04,810 --> 00:33:10,030
tensorflow numpy for pi torch and uber

00:33:08,410 --> 00:33:12,280
at least the framework called peda storm

00:33:10,030 --> 00:33:15,340
for really scale it's basically parkade

00:33:12,280 --> 00:33:16,750
with metadata and then finally I'll have

00:33:15,340 --> 00:33:19,720
worked a notebook to validate your model

00:33:16,750 --> 00:33:21,130
and you write some more Python code and

00:33:19,720 --> 00:33:23,230
airflow to orchestrate that into a

00:33:21,130 --> 00:33:24,790
pipeline so they actually don't see

00:33:23,230 --> 00:33:27,280
anything underneath this and at this

00:33:24,790 --> 00:33:28,180
point I'll just kind of make our my view

00:33:27,280 --> 00:33:30,270
on hops in the

00:33:28,180 --> 00:33:34,600
which is kind of the title of the talk

00:33:30,270 --> 00:33:36,310
if I'm a Python developer do I really

00:33:34,600 --> 00:33:37,480
care if this thing is running on yarn or

00:33:36,310 --> 00:33:40,210
don't care if it's running in Cleburne

00:33:37,480 --> 00:33:42,420
edits or do I just want this managed

00:33:40,210 --> 00:33:46,240
platform that gives me this UI and this

00:33:42,420 --> 00:33:47,950
this this workflow so that's the

00:33:46,240 --> 00:33:51,940
question we're kind of faced with at

00:33:47,950 --> 00:33:53,500
some level and at that point I'll kind

00:33:51,940 --> 00:33:58,600
of just say conclude the talk you know

00:33:53,500 --> 00:34:00,160
that's our our our platform you can go

00:33:58,600 --> 00:34:01,960
to hop start site register for an

00:34:00,160 --> 00:34:03,700
account and try it out I didn't demo the

00:34:01,960 --> 00:34:04,900
platform and but you can see it in

00:34:03,700 --> 00:34:06,130
action there are some videos available

00:34:04,900 --> 00:34:08,320
on YouTube to show you how to get

00:34:06,130 --> 00:34:10,600
started if you want to try it out you

00:34:08,320 --> 00:34:13,540
can go to edit those images for AWS in

00:34:10,600 --> 00:34:15,640
Google Cloud and VirtualBox and you know

00:34:13,540 --> 00:34:17,740
we ran new projects and we'd like we

00:34:15,640 --> 00:34:19,380
need support and you know support us in

00:34:17,740 --> 00:34:22,960
whatever way you can tweet about us

00:34:19,380 --> 00:34:25,489
stars and github or whatever so with

00:34:22,960 --> 00:34:33,909
that thank you

00:34:25,489 --> 00:34:33,909
[Applause]

00:34:41,870 --> 00:34:46,220
if you move back to slides for the

00:34:44,180 --> 00:34:48,880
features and the time travels the future

00:34:46,220 --> 00:34:52,370
store yes and I mean one of the key

00:34:48,880 --> 00:34:55,340
problems with training data is that they

00:34:52,370 --> 00:34:58,010
need to well basically the model gets

00:34:55,340 --> 00:35:02,330
evaluated at all these points in time

00:34:58,010 --> 00:35:07,490
and the data volume of the features just

00:35:02,330 --> 00:35:08,810
explode I mean okay example firing you

00:35:07,490 --> 00:35:11,390
know what you want to evaluate a model

00:35:08,810 --> 00:35:14,030
at every game and and you want to

00:35:11,390 --> 00:35:16,130
evaluate a model on every game and every

00:35:14,030 --> 00:35:18,050
time you finish a level okay I think and

00:35:16,130 --> 00:35:19,850
yeah and then where we say evaluate a

00:35:18,050 --> 00:35:21,410
model you mean you you you say when you

00:35:19,850 --> 00:35:24,050
send a feature vector the model

00:35:21,410 --> 00:35:26,060
prediction prediction then if I'm

00:35:24,050 --> 00:35:27,920
supposed to you know I don't give the

00:35:26,060 --> 00:35:29,690
user a lollipop hammer or five extra

00:35:27,920 --> 00:35:32,420
moves or yes you know whatever feature

00:35:29,690 --> 00:35:34,160
in the game yeah I'm evaluating okay so

00:35:32,420 --> 00:35:36,590
that's an that's an online model being

00:35:34,160 --> 00:35:38,270
served over here yeah and then your

00:35:36,590 --> 00:35:39,890
applications making a call then the

00:35:38,270 --> 00:35:42,260
training but I'm training is not like

00:35:39,890 --> 00:35:44,630
here yeah yeah and the problem is that

00:35:42,260 --> 00:35:46,820
then the features you know they might be

00:35:44,630 --> 00:35:49,400
different that have you know if a user

00:35:46,820 --> 00:35:51,890
has 15 game ends you know in a day

00:35:49,400 --> 00:35:54,650
because we're playing yeah that's a lot

00:35:51,890 --> 00:35:56,720
of feature vectors so they I mean

00:35:54,650 --> 00:35:59,120
there's there's yeah I mean okay I'll

00:35:56,720 --> 00:36:01,400
try and handle that in a couple of parts

00:35:59,120 --> 00:36:02,930
right so what you're saying is that you

00:36:01,400 --> 00:36:05,030
will generate a lot of predictions and

00:36:02,930 --> 00:36:06,770
should I store the predictions I guess

00:36:05,030 --> 00:36:08,180
that's one point can I store all of

00:36:06,770 --> 00:36:11,180
these predictions with a huge volume of

00:36:08,180 --> 00:36:12,650
data so a guy called Steve Welsh I met

00:36:11,180 --> 00:36:15,470
last week from Google he worked on Borg

00:36:12,650 --> 00:36:17,900
he said on Borg which is the resource

00:36:15,470 --> 00:36:19,490
manager for Google they store trillions

00:36:17,900 --> 00:36:21,050
of data points and they don't care they

00:36:19,490 --> 00:36:22,940
store all the predictions they just go

00:36:21,050 --> 00:36:25,730
storages in Fitness is their assumption

00:36:22,940 --> 00:36:27,740
now for the rest of us and the feature

00:36:25,730 --> 00:36:29,810
store key so what you can do what we do

00:36:27,740 --> 00:36:31,730
I didn't show here is that when we serve

00:36:29,810 --> 00:36:33,170
models on tensorflow we actually can

00:36:31,730 --> 00:36:35,300
store all of the logs of all the

00:36:33,170 --> 00:36:37,460
predictions in Kafka and then you run a

00:36:35,300 --> 00:36:41,000
spark streaming job to monitor your

00:36:37,460 --> 00:36:43,820
model that way okay and your spark

00:36:41,000 --> 00:36:45,590
streaming job could just append a table

00:36:43,820 --> 00:36:47,540
in hive or a bigquery or whatever you

00:36:45,590 --> 00:36:50,300
want and then that data can grow

00:36:47,540 --> 00:36:51,410
infinitely right so the problem so this

00:36:50,300 --> 00:36:54,200
is the problem this is the problem the

00:36:51,410 --> 00:36:55,020
feature store addresses because if we

00:36:54,200 --> 00:36:56,580
didn't you can do

00:36:55,020 --> 00:36:57,840
and that's fine but if you use the

00:36:56,580 --> 00:37:00,420
feature store the feature store will

00:36:57,840 --> 00:37:02,880
store all the parkade updates as park'

00:37:00,420 --> 00:37:04,410
files and it'll grow but there's an

00:37:02,880 --> 00:37:07,260
assumption that this the growth in this

00:37:04,410 --> 00:37:08,910
will not be infinite at some level you

00:37:07,260 --> 00:37:11,390
know it'll be big but it won't you know

00:37:08,910 --> 00:37:13,530
maybe double triple quadruple the size

00:37:11,390 --> 00:37:14,820
but you'll be able to go back in time

00:37:13,530 --> 00:37:17,730
and find out the values of the features

00:37:14,820 --> 00:37:19,320
at that point I won't need to have the

00:37:17,730 --> 00:37:20,370
store of all the predictions because the

00:37:19,320 --> 00:37:22,560
store of all the predictions will be

00:37:20,370 --> 00:37:24,180
orders of magnitude larger than having

00:37:22,560 --> 00:37:25,500
the updates to the feature right I mean

00:37:24,180 --> 00:37:27,480
the problem in the future value is

00:37:25,500 --> 00:37:29,130
basically I need for the example of the

00:37:27,480 --> 00:37:31,770
fraud prediction I would need to be able

00:37:29,130 --> 00:37:33,720
to jump back in time to in between every

00:37:31,770 --> 00:37:36,420
transaction basically to see what it was

00:37:33,720 --> 00:37:38,370
you know before and after the actual

00:37:36,420 --> 00:37:40,410
transaction no I mean so if you make

00:37:38,370 --> 00:37:43,470
assumption that your transaction windows

00:37:40,410 --> 00:37:44,760
are updated daily once a day okay not

00:37:43,470 --> 00:37:46,710
every minute because you're gonna have a

00:37:44,760 --> 00:37:48,630
job running here that runs every day

00:37:46,710 --> 00:37:50,460
it'll update the windows and in the

00:37:48,630 --> 00:37:53,010
feature store and that update will be a

00:37:50,460 --> 00:37:54,570
park a file it'll be stored there and

00:37:53,010 --> 00:37:56,040
integrated into the feature store it's

00:37:54,570 --> 00:37:57,750
the latest value in the feature store

00:37:56,040 --> 00:37:59,670
but the older valuably still stored as a

00:37:57,750 --> 00:38:01,770
park a file so then when you want to

00:37:59,670 --> 00:38:04,800
search back in time it will you'll use

00:38:01,770 --> 00:38:07,440
that that time index to find the value

00:38:04,800 --> 00:38:09,240
in that Park a file I don't know if that

00:38:07,440 --> 00:38:10,860
makes it clear yeah I know but I was

00:38:09,240 --> 00:38:13,320
also thinking like in terms of say a

00:38:10,860 --> 00:38:15,330
credit transaction company that yes the

00:38:13,320 --> 00:38:18,750
risk like you know clora which is close

00:38:15,330 --> 00:38:20,970
to and it's used here like they probably

00:38:18,750 --> 00:38:23,190
want the feature matrix you know for

00:38:20,970 --> 00:38:24,570
every before every transaction for the

00:38:23,190 --> 00:38:27,000
training of if they're gonna prove that

00:38:24,570 --> 00:38:28,860
like I'm buying and buying a bunch of

00:38:27,000 --> 00:38:30,960
things because I'm travelling to Germany

00:38:28,860 --> 00:38:32,700
then I'm losing my credit card and then

00:38:30,960 --> 00:38:34,770
someone else starts buying things and

00:38:32,700 --> 00:38:36,720
then yeah I mean take it to kick in that

00:38:34,770 --> 00:38:39,170
mean I mean I think what you're talking

00:38:36,720 --> 00:38:41,190
about if if I'm correct here is that

00:38:39,170 --> 00:38:43,350
models can use a lot more information

00:38:41,190 --> 00:38:44,850
that changes a lot more frequently so we

00:38:43,350 --> 00:38:45,870
could use all of this data to make

00:38:44,850 --> 00:38:47,670
predictions which would cause

00:38:45,870 --> 00:38:50,160
exponential growth of the feature store

00:38:47,670 --> 00:38:51,180
is that the kind of point yes yeah so at

00:38:50,160 --> 00:38:54,570
some point you have to make a trade-off

00:38:51,180 --> 00:38:56,550
right and you can't use every potential

00:38:54,570 --> 00:38:58,680
feature to make predictions because the

00:38:56,550 --> 00:39:00,930
general rule if you have an online app

00:38:58,680 --> 00:39:02,850
and you want to make predictions with a

00:39:00,930 --> 00:39:04,240
model is that you can only use features

00:39:02,850 --> 00:39:07,630
that you can actually access

00:39:04,240 --> 00:39:09,190
and online in real time so we have a

00:39:07,630 --> 00:39:10,960
real time part of the featured store so

00:39:09,190 --> 00:39:13,810
you can make certain your millisecond

00:39:10,960 --> 00:39:15,760
lookups on feature values there but you

00:39:13,810 --> 00:39:17,290
can't make it infinitely large right you

00:39:15,760 --> 00:39:19,320
can't you can only have things that will

00:39:17,290 --> 00:39:21,670
you can insert there in reasonable time

00:39:19,320 --> 00:39:25,150
so I mean you know there's trade-offs

00:39:21,670 --> 00:39:28,349
need to be made we can take it offline

00:39:25,150 --> 00:39:28,349

YouTube URL: https://www.youtube.com/watch?v=vvUfgWvtqHE


