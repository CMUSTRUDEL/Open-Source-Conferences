Title: Berlin Buzzwords 2019: Paresh Paradkar â€“ Multilingual Search System - How to Build and Improve!?
Publication date: 2019-06-28
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	You are a multi-national company having customers in different countries where the business language is not necessarily English. How would you build a centralized search system catering for the needs of all your users? Apache Lucene/Elasticsearch/Apache Solr all provide different language-based text analyzers to analyze the text but which one should you use and when? We found overselves in exactly this situation. 

We are an email-security company having around 300 billion emails archived with us resulting in indices in petabytes of indexed data. Earlier we used whitespace analyzers from Lucene to be able to serve the searches in different languages but this approach although simplistic presented many limitations once we started to serve in different languages (e.g. German). I will explain how we overcome these problems by first identifying the language of the content through our own language detection model which in turn served as the guide for the selection of an analyzer to analyze the email in various languages. 

This talk will walk you through how to build multilingual search systems and explore different possible approaches. It will also discuss different problems one may run into when these language-based analyzers are used and what are the ways to improve the search results in these cases. 

In particular the talk will focus on the query-log analysis as an effective way to improve the multilingual search by providing the feedback to fine tune the analyzers used for stemming and lemmatization thereby increasing not only the recall but also the precision (relevance) of the search results.

Read more:
https://2019.berlinbuzzwords.de/19/session/multilingual-search-system-how-build-and-improve

About Paresh Paradkar:
https://2019.berlinbuzzwords.de/users/paresh-paradkar

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,190 --> 00:00:12,340
good morning everyone welcome to my talk

00:00:09,050 --> 00:00:15,320
I'm going to speak about how to build

00:00:12,340 --> 00:00:19,520
multilingual search system and how to

00:00:15,320 --> 00:00:22,790
improve that of maybe to be precise how

00:00:19,520 --> 00:00:25,099
we build it and how we improved it just

00:00:22,790 --> 00:00:27,980
a brief intro about myself I'm a senior

00:00:25,099 --> 00:00:31,099
software engineer at main cast Services

00:00:27,980 --> 00:00:33,640
Limited based in London and Apache

00:00:31,099 --> 00:00:37,149
leucine and elasticsearch enthusiasts I

00:00:33,640 --> 00:00:40,399
did my Master's here in Germany from

00:00:37,149 --> 00:00:42,320
University of Freiburg that's my

00:00:40,399 --> 00:00:47,449
LinkedIn account if you want to connect

00:00:42,320 --> 00:00:49,969
to me and let's take a let's take a

00:00:47,449 --> 00:00:53,739
glance at mine cause let me introduce my

00:00:49,969 --> 00:00:57,410
company mine cause mine cost makes

00:00:53,739 --> 00:00:59,440
billions of users of the three thirty

00:00:57,410 --> 00:01:04,519
two thousand two hundred plus customers

00:00:59,440 --> 00:01:06,470
the emails and business data safer they

00:01:04,519 --> 00:01:09,860
restore around two hundred and eighty

00:01:06,470 --> 00:01:13,100
seven billion plus emails under our

00:01:09,860 --> 00:01:18,020
management in the size of more than

00:01:13,100 --> 00:01:20,360
forty petabytes we process around 456

00:01:18,020 --> 00:01:22,130
million plus emails every day and our

00:01:20,360 --> 00:01:26,420
search volume is around three million

00:01:22,130 --> 00:01:29,600
messages or emails every week we have

00:01:26,420 --> 00:01:33,650
twelve data centers strategically placed

00:01:29,600 --> 00:01:36,170
around around the globe and that that's

00:01:33,650 --> 00:01:38,240
what we call basically grades - - so our

00:01:36,170 --> 00:01:41,450
customers in different geographical

00:01:38,240 --> 00:01:45,170
regions better and the story I'm going

00:01:41,450 --> 00:01:48,200
to tell you about today begins when mine

00:01:45,170 --> 00:01:52,360
cast open their data center started

00:01:48,200 --> 00:01:54,680
their German grid in early 2018 so

00:01:52,360 --> 00:01:56,540
although it's multilingual mostly we are

00:01:54,680 --> 00:01:59,570
going to focus on German and English

00:01:56,540 --> 00:02:03,100
because that's how we started serving

00:01:59,570 --> 00:02:08,179
customers nor the English first time

00:02:03,100 --> 00:02:11,629
yeah so that's the that's the famous

00:02:08,179 --> 00:02:13,370
joke about German Scrabble the fact that

00:02:11,629 --> 00:02:15,410
German companies were the first

00:02:13,370 --> 00:02:18,610
companies or customers that we serve

00:02:15,410 --> 00:02:22,690
whose data was non-english

00:02:18,610 --> 00:02:24,460
and our simplistic approach at the

00:02:22,690 --> 00:02:26,470
beginning laws that we will use just

00:02:24,460 --> 00:02:28,420
white face analyzer so you know we can

00:02:26,470 --> 00:02:30,310
we can break it on white space and then

00:02:28,420 --> 00:02:33,640
so whichever language it could be

00:02:30,310 --> 00:02:35,800
because most of the languages are broken

00:02:33,640 --> 00:02:40,020
down into white space but German in

00:02:35,800 --> 00:02:43,660
their unique or can say unmatched

00:02:40,020 --> 00:02:46,570
ability to create longest words by

00:02:43,660 --> 00:02:50,650
conjugating words after another it's

00:02:46,570 --> 00:02:52,480
really hard you know to break that word

00:02:50,650 --> 00:02:55,090
into sub words if you if you really

00:02:52,480 --> 00:02:57,220
tokenize only on white space so imagine

00:02:55,090 --> 00:02:59,650
a pain of a person for example searching

00:02:57,220 --> 00:03:01,270
for that work if you are if you are

00:02:59,650 --> 00:03:02,980
storing only on white space and

00:03:01,270 --> 00:03:07,780
searching on white space tokenizer

00:03:02,980 --> 00:03:10,510
so for example people might want to

00:03:07,780 --> 00:03:12,430
search that word just by searching for

00:03:10,510 --> 00:03:14,770
Gazette's isn't it going why can't they

00:03:12,430 --> 00:03:17,260
search why just guess it's an 500 emails

00:03:14,770 --> 00:03:21,670
containing that so we need to change

00:03:17,260 --> 00:03:23,769
something there and then maybe it's not

00:03:21,670 --> 00:03:28,630
that every email contains that many long

00:03:23,769 --> 00:03:32,080
words but some of the real you know real

00:03:28,630 --> 00:03:35,440
words from our real users data where

00:03:32,080 --> 00:03:39,030
like like these compound words and for

00:03:35,440 --> 00:03:41,560
example if we say people are finding

00:03:39,030 --> 00:03:44,410
words beystadium which means

00:03:41,560 --> 00:03:47,620
confirmation they are not going to find

00:03:44,410 --> 00:03:49,000
emails which contain for example you

00:03:47,620 --> 00:03:52,570
know term in which telugu means

00:03:49,000 --> 00:03:54,580
appointment confirmation or first and

00:03:52,570 --> 00:03:59,500
vegetable means delivery confirmation

00:03:54,580 --> 00:04:01,570
and so on so with system that tokenizer

00:03:59,500 --> 00:04:04,000
is just one white space we said okay we

00:04:01,570 --> 00:04:06,700
have to change something otherwise it's

00:04:04,000 --> 00:04:11,590
really not that user-friendly in terms

00:04:06,700 --> 00:04:15,489
of no searching email with containing

00:04:11,590 --> 00:04:17,729
compound words or big long words so what

00:04:15,489 --> 00:04:20,709
we and then another problem was actually

00:04:17,729 --> 00:04:23,410
that the character equivalences in

00:04:20,709 --> 00:04:27,010
german or you know this languages which

00:04:23,410 --> 00:04:30,130
which have like umlauts which can be

00:04:27,010 --> 00:04:34,390
represented by a character and in the

00:04:30,130 --> 00:04:36,790
II and specialist ed character which can

00:04:34,390 --> 00:04:40,450
be correct which can be also represented

00:04:36,790 --> 00:04:43,930
by SS so for example if a user searches

00:04:40,450 --> 00:04:45,670
on such as with use of character

00:04:43,930 --> 00:04:48,670
equivalences you will not be able to

00:04:45,670 --> 00:04:50,550
find email which is containing you know

00:04:48,670 --> 00:04:54,070
which is actually written with the

00:04:50,550 --> 00:04:56,920
umlauts or s set character so these were

00:04:54,070 --> 00:04:58,570
the two problems that basically started

00:04:56,920 --> 00:05:00,760
us to think that we cannot just have

00:04:58,570 --> 00:05:03,970
whitespace analyzer and we need some

00:05:00,760 --> 00:05:06,700
different strategy so we decided and and

00:05:03,970 --> 00:05:10,210
whether we use Lucene directly so we

00:05:06,700 --> 00:05:14,890
have to customize everything on our own

00:05:10,210 --> 00:05:17,440
so we decided to write our own custom

00:05:14,890 --> 00:05:19,780
analyzer which will sit on top of the

00:05:17,440 --> 00:05:23,830
German analyzer that we have in the

00:05:19,780 --> 00:05:26,380
scene so the analyzer basically does is

00:05:23,830 --> 00:05:29,590
it starts with a compound word token

00:05:26,380 --> 00:05:32,260
filter in the scene there are two types

00:05:29,590 --> 00:05:34,240
of compound word token filters one is

00:05:32,260 --> 00:05:37,870
dictionary compound word token filter

00:05:34,240 --> 00:05:40,870
and another is a hyphenation compound

00:05:37,870 --> 00:05:44,140
word and filter basically both use

00:05:40,870 --> 00:05:48,850
dictionaries but the hyphenation one

00:05:44,140 --> 00:05:50,650
uses high finishing gamma to identify

00:05:48,850 --> 00:05:53,680
where it can break basically your

00:05:50,650 --> 00:05:56,290
compound word into its of words and then

00:05:53,680 --> 00:05:58,750
those sub words could be looked into the

00:05:56,290 --> 00:06:01,690
dictionary to find if that really

00:05:58,750 --> 00:06:04,570
matches so that one is the -

00:06:01,690 --> 00:06:08,710
action-filled filter is basically faster

00:06:04,570 --> 00:06:10,720
so we chose to use that one to break our

00:06:08,710 --> 00:06:13,960
compounding words into the sub words and

00:06:10,720 --> 00:06:16,570
then those sub words would then be

00:06:13,960 --> 00:06:20,320
passed to german analyzer from the scene

00:06:16,570 --> 00:06:23,560
so that we can process the umlauts or

00:06:20,320 --> 00:06:28,210
estate characters into the character

00:06:23,560 --> 00:06:30,130
equivalences and then at the end we

00:06:28,210 --> 00:06:32,050
store it into the index again so that

00:06:30,130 --> 00:06:34,600
means we have the we have the

00:06:32,050 --> 00:06:36,790
possibility of breaking down compounding

00:06:34,600 --> 00:06:40,210
words into sub words and then you know

00:06:36,790 --> 00:06:42,880
from processing umlauts and estate

00:06:40,210 --> 00:06:43,890
characters etc so let's take an example

00:06:42,880 --> 00:06:47,670
maybe

00:06:43,890 --> 00:06:50,490
from our previous real data let's say we

00:06:47,670 --> 00:06:52,620
have user who is who is having an email

00:06:50,490 --> 00:06:55,260
with term invested ego which means

00:06:52,620 --> 00:06:58,290
appointment confirmation and we want to

00:06:55,260 --> 00:07:01,110
store it with our analyzer so what it

00:06:58,290 --> 00:07:04,230
will do is first it will using

00:07:01,110 --> 00:07:07,680
hyphenation grammar it will break down

00:07:04,230 --> 00:07:09,540
this word into termine best and which

00:07:07,680 --> 00:07:12,090
they do best rest is basically coming

00:07:09,540 --> 00:07:15,780
out of the static again because it's

00:07:12,090 --> 00:07:19,140
recursive so that means we have now word

00:07:15,780 --> 00:07:21,470
for appointment best and which data gong

00:07:19,140 --> 00:07:24,540
which means confirmation so we have the

00:07:21,470 --> 00:07:28,200
we have the broken down sub words and

00:07:24,540 --> 00:07:30,270
then we pass them to our german analyzer

00:07:28,200 --> 00:07:32,670
which will then you know process this

00:07:30,270 --> 00:07:35,880
umlaut channel and make them as

00:07:32,670 --> 00:07:38,730
lowercase and term invest and starting

00:07:35,880 --> 00:07:40,410
on so that's how basically the analyzer

00:07:38,730 --> 00:07:42,000
will break down the compounding words

00:07:40,410 --> 00:07:44,370
into the sub word so that we can match

00:07:42,000 --> 00:07:46,410
for example the longest word we had seen

00:07:44,370 --> 00:07:52,110
before you can also search it by

00:07:46,410 --> 00:07:54,780
matching visits for example so that's

00:07:52,110 --> 00:07:57,810
fine so our user is saying okay thanks I

00:07:54,780 --> 00:07:59,970
can't find sub words now but can i

00:07:57,810 --> 00:08:04,020
really search my both emails in German

00:07:59,970 --> 00:08:07,670
and English together now just an

00:08:04,020 --> 00:08:10,200
analyzer won't help it we need some more

00:08:07,670 --> 00:08:11,670
you know techniques to identify how we

00:08:10,200 --> 00:08:14,220
are going to store our emails in

00:08:11,670 --> 00:08:17,130
different languages so that's a criteria

00:08:14,220 --> 00:08:19,890
to to decide and then we need something

00:08:17,130 --> 00:08:23,910
which will understand what language our

00:08:19,890 --> 00:08:26,430
email basically is in so first would be

00:08:23,910 --> 00:08:29,850
the language detection model which will

00:08:26,430 --> 00:08:32,160
you know this beside which language the

00:08:29,850 --> 00:08:35,100
document belongs to so we decide we

00:08:32,160 --> 00:08:38,220
develop one in the logistic regression

00:08:35,100 --> 00:08:41,220
model based on Wikipedia data set to

00:08:38,220 --> 00:08:43,740
identify the language it detects

00:08:41,220 --> 00:08:46,020
languages with precision but the

00:08:43,740 --> 00:08:49,380
criteria here was not just precision but

00:08:46,020 --> 00:08:50,820
also high recall for English because as

00:08:49,380 --> 00:08:54,480
though we are serving now multilingual

00:08:50,820 --> 00:08:57,080
the more or the most customers are still

00:08:54,480 --> 00:09:01,760
having English emails in our

00:08:57,080 --> 00:09:03,410
corpus for our data so we cannot give

00:09:01,760 --> 00:09:05,570
the possibility that you know English

00:09:03,410 --> 00:09:09,620
email being recognized as non English so

00:09:05,570 --> 00:09:11,630
the idea for the language detection

00:09:09,620 --> 00:09:14,180
model was that model which will give the

00:09:11,630 --> 00:09:18,890
highest recall for English so that it

00:09:14,180 --> 00:09:21,470
will identify the email from different

00:09:18,890 --> 00:09:24,050
languages as them as but it will mostly

00:09:21,470 --> 00:09:29,329
focus on not identifying English as non

00:09:24,050 --> 00:09:32,029
English so so that part is is done there

00:09:29,329 --> 00:09:33,980
but we still need to know how we are

00:09:32,029 --> 00:09:40,459
going to store awareness in the scene

00:09:33,980 --> 00:09:44,140
index so there are three so there are

00:09:40,459 --> 00:09:46,640
three generally as accepted ways to to

00:09:44,140 --> 00:09:49,010
you know put multilingual documents

00:09:46,640 --> 00:09:50,990
inside your Lucene index one is

00:09:49,010 --> 00:09:53,529
basically you create a separate index

00:09:50,990 --> 00:09:56,180
for every language and then you just

00:09:53,529 --> 00:09:59,899
store your documents in every language

00:09:56,180 --> 00:10:02,029
in that respective index another is that

00:09:59,899 --> 00:10:04,220
you create a separate field for every

00:10:02,029 --> 00:10:07,250
language so your index remains only one

00:10:04,220 --> 00:10:09,980
but it has multiple fields and then

00:10:07,250 --> 00:10:11,600
another would be you just insert all the

00:10:09,980 --> 00:10:14,209
documents in one index and just have a

00:10:11,600 --> 00:10:17,959
field which tells you basically what

00:10:14,209 --> 00:10:21,320
language it belongs to so let's look at

00:10:17,959 --> 00:10:23,450
them one by one so first one is a

00:10:21,320 --> 00:10:26,540
separate index this is basically nothing

00:10:23,450 --> 00:10:28,760
but you create create a separate index

00:10:26,540 --> 00:10:31,850
whenever you want to you know support a

00:10:28,760 --> 00:10:34,339
language so new language comes in you

00:10:31,850 --> 00:10:36,440
want to support that then you just

00:10:34,339 --> 00:10:38,870
create an index and store start storing

00:10:36,440 --> 00:10:42,170
your documents in that language into

00:10:38,870 --> 00:10:43,910
this separate index this provides you

00:10:42,170 --> 00:10:46,970
know clearer structure because every

00:10:43,910 --> 00:10:49,190
language it has got their own index it

00:10:46,970 --> 00:10:51,020
also doesn't match up in your own

00:10:49,190 --> 00:10:56,449
frequency is considering every document

00:10:51,020 --> 00:10:57,970
is in its own data set and basically it

00:10:56,449 --> 00:11:00,470
also gives you better handling for

00:10:57,970 --> 00:11:01,850
linked particular language where you

00:11:00,470 --> 00:11:06,470
know the languor query regarding a

00:11:01,850 --> 00:11:08,510
particular language so that's that and

00:11:06,470 --> 00:11:11,000
then second one is the separate field

00:11:08,510 --> 00:11:13,340
for language in this approach you will

00:11:11,000 --> 00:11:14,660
have to create for every language a

00:11:13,340 --> 00:11:17,530
separate fear so for example if your

00:11:14,660 --> 00:11:19,850
content is your field is content and

00:11:17,530 --> 00:11:21,440
every email has a different language

00:11:19,850 --> 00:11:23,330
then you have to have separate field

00:11:21,440 --> 00:11:26,120
like English content German content

00:11:23,330 --> 00:11:28,370
French content and so on and now with

00:11:26,120 --> 00:11:30,170
this approach the problem is that if you

00:11:28,370 --> 00:11:31,640
start now saying okay I am going to

00:11:30,170 --> 00:11:33,410
support it tallien as well from tomorrow

00:11:31,640 --> 00:11:35,420
then you have to change your index

00:11:33,410 --> 00:11:36,800
schema you have to change your fields

00:11:35,420 --> 00:11:40,190
because you have to add new field there

00:11:36,800 --> 00:11:44,030
so to index all your documents in that

00:11:40,190 --> 00:11:46,490
index and it's very hard for for you

00:11:44,030 --> 00:11:50,120
know systems where the data is really

00:11:46,490 --> 00:11:52,880
large but it could be so it could be for

00:11:50,120 --> 00:11:55,610
easier for I don't know with some

00:11:52,880 --> 00:11:58,450
systems where your data your indexes are

00:11:55,610 --> 00:12:02,570
small and volatile maybe you can just

00:11:58,450 --> 00:12:06,200
reindex those it really depends on on

00:12:02,570 --> 00:12:08,180
your use cases basically but it brings

00:12:06,200 --> 00:12:10,400
overall overhead of you know extra

00:12:08,180 --> 00:12:14,720
fields into index while indexing as well

00:12:10,400 --> 00:12:16,760
as query and the third approach is you

00:12:14,720 --> 00:12:18,320
create separate documents per language

00:12:16,760 --> 00:12:20,540
so that means you basically nothing but

00:12:18,320 --> 00:12:22,100
you create one index there will be a

00:12:20,540 --> 00:12:26,180
field which will tell you what language

00:12:22,100 --> 00:12:28,339
that a document belongs to and you just

00:12:26,180 --> 00:12:31,420
reuse your fields basically there is no

00:12:28,339 --> 00:12:33,860
separate field for any separate language

00:12:31,420 --> 00:12:36,560
this also this approach also scales well

00:12:33,860 --> 00:12:38,540
with you know increasing number of

00:12:36,560 --> 00:12:40,760
language supports example you you have a

00:12:38,540 --> 00:12:42,680
new language you want to support you

00:12:40,760 --> 00:12:45,170
just store your document inside your

00:12:42,680 --> 00:12:47,230
index and just create you specify which

00:12:45,170 --> 00:12:50,240
language it is in your language field

00:12:47,230 --> 00:12:53,240
but since all languages are mixed up it

00:12:50,240 --> 00:12:58,400
could be a wrong tone frequencies

00:12:53,240 --> 00:13:01,790
problem and yeah I mean it depends again

00:12:58,400 --> 00:13:03,770
your your project but what we did what

00:13:01,790 --> 00:13:07,130
we decided is to better keep it clean so

00:13:03,770 --> 00:13:12,640
we have every language its own its own

00:13:07,130 --> 00:13:15,110
index basically so so we have now

00:13:12,640 --> 00:13:17,570
language detection model we have now a

00:13:15,110 --> 00:13:19,130
way to this way to you know store all

00:13:17,570 --> 00:13:23,000
your emails in the separate language

00:13:19,130 --> 00:13:24,800
separate index but our friend is saying

00:13:23,000 --> 00:13:26,600
that okay I can find my German

00:13:24,800 --> 00:13:30,170
English humans now because I we are

00:13:26,600 --> 00:13:32,570
storing it separately but your system is

00:13:30,170 --> 00:13:34,790
returning email containing word best a

00:13:32,570 --> 00:13:38,779
legume a stop result when I search for

00:13:34,790 --> 00:13:40,940
best so he wants to have the ability to

00:13:38,779 --> 00:13:43,339
search the sub words but he says okay

00:13:40,940 --> 00:13:47,620
you cannot just search it you just have

00:13:43,339 --> 00:13:50,420
to improve it in terms of relevance and

00:13:47,620 --> 00:13:54,170
precision so the what what's the what

00:13:50,420 --> 00:13:56,630
problem here is that we have you know

00:13:54,170 --> 00:13:59,329
balance we have to balance our precision

00:13:56,630 --> 00:14:01,820
and recall basically we increase recall

00:13:59,329 --> 00:14:04,760
by adding new terms in our index

00:14:01,820 --> 00:14:06,620
well we D compounded it but we lost

00:14:04,760 --> 00:14:10,070
precision because now as as our friend

00:14:06,620 --> 00:14:12,980
was saying that we can we can match more

00:14:10,070 --> 00:14:17,990
emails or more documents to our queries

00:14:12,980 --> 00:14:20,959
because we have matching terms so there

00:14:17,990 --> 00:14:25,399
could be there could be two ways to to

00:14:20,959 --> 00:14:28,700
handle this one is to one is to do query

00:14:25,399 --> 00:14:30,380
log analysis so what what you can do is

00:14:28,700 --> 00:14:33,470
basically you analyze your quail logs

00:14:30,380 --> 00:14:35,839
and see what users are searching or how

00:14:33,470 --> 00:14:37,579
they are searching and can feed back

00:14:35,839 --> 00:14:40,790
that basically back to your analyzer

00:14:37,579 --> 00:14:43,100
saying you know to fine-tune your

00:14:40,790 --> 00:14:47,240
parameters where to cut your sub words

00:14:43,100 --> 00:14:49,160
how much longer you want to go for your

00:14:47,240 --> 00:14:52,070
sub words or how small they should be

00:14:49,160 --> 00:14:55,190
because if you if you cut for a small

00:14:52,070 --> 00:14:58,010
length of sub words it's going to create

00:14:55,190 --> 00:14:59,660
more terms and then it's going to be the

00:14:58,010 --> 00:15:02,390
situation that you are matching more

00:14:59,660 --> 00:15:04,220
emails so basically you can say okay I'm

00:15:02,390 --> 00:15:06,440
going to let's say I'm going to create

00:15:04,220 --> 00:15:08,839
sub words only if they are length of

00:15:06,440 --> 00:15:11,990
five or four and then you will not have

00:15:08,839 --> 00:15:14,300
that many that many terms created out of

00:15:11,990 --> 00:15:19,670
your analyzer and basically not matching

00:15:14,300 --> 00:15:21,709
everything so you minimize number of

00:15:19,670 --> 00:15:23,949
terms that are getting stored and number

00:15:21,709 --> 00:15:27,769
of emails that are matching there

00:15:23,949 --> 00:15:34,040
another way was to rewrite your queries

00:15:27,769 --> 00:15:36,620
so what what we can do is we store the

00:15:34,040 --> 00:15:38,150
output of our analyzer not basically

00:15:36,620 --> 00:15:40,760
into the same faith

00:15:38,150 --> 00:15:43,490
to a separate field so you can do is you

00:15:40,760 --> 00:15:46,220
create another field called I don't know

00:15:43,490 --> 00:15:49,940
sub words and store your context or your

00:15:46,220 --> 00:15:52,850
sub words into that field and whenever

00:15:49,940 --> 00:15:56,750
your user is searching you boost your

00:15:52,850 --> 00:16:01,340
actual field over your sub verse field

00:15:56,750 --> 00:16:03,470
so that docks containing you know the

00:16:01,340 --> 00:16:05,840
actual word that will be ranked higher

00:16:03,470 --> 00:16:08,630
than they than they were matched just

00:16:05,840 --> 00:16:10,610
due to being in the sub words so what

00:16:08,630 --> 00:16:13,430
I'm saying is for example a term in

00:16:10,610 --> 00:16:17,660
which lady goon created best best lady

00:16:13,430 --> 00:16:20,030
goon enter men so you save those Tulsa

00:16:17,660 --> 00:16:23,060
words into your another field so that

00:16:20,030 --> 00:16:25,280
you only boost when when somebody is

00:16:23,060 --> 00:16:28,760
searching for best you only only boost

00:16:25,280 --> 00:16:30,920
the words best in the content field and

00:16:28,760 --> 00:16:32,840
not from the sub words field so the guy

00:16:30,920 --> 00:16:36,080
was you know our user was complaining

00:16:32,840 --> 00:16:38,050
why I get emails with Bastida goon as a

00:16:36,080 --> 00:16:40,940
top result when I'm searching for best

00:16:38,050 --> 00:16:43,670
we can solve it by that way that we have

00:16:40,940 --> 00:16:49,040
only emails with best at the top of our

00:16:43,670 --> 00:16:51,320
result so in a nutshell I would say we

00:16:49,040 --> 00:16:53,240
build we build a language detection so

00:16:51,320 --> 00:16:55,280
basically scales for all languages

00:16:53,240 --> 00:16:57,830
German and English were the most

00:16:55,280 --> 00:17:00,050
interesting one because of our use case

00:16:57,830 --> 00:17:02,570
as well as the the ability to you know

00:17:00,050 --> 00:17:05,690
compound compound the words but in the

00:17:02,570 --> 00:17:06,830
nutshell we can say that we we need a

00:17:05,690 --> 00:17:08,900
language detection model which

00:17:06,830 --> 00:17:10,910
identifies which language document

00:17:08,900 --> 00:17:12,820
belongs to and we need to decide on a

00:17:10,910 --> 00:17:15,500
structure how we are going to store our

00:17:12,820 --> 00:17:18,110
emails or documents whether in one index

00:17:15,500 --> 00:17:20,660
or separate indexes then in case of

00:17:18,110 --> 00:17:22,280
languages where there is a compound in

00:17:20,660 --> 00:17:24,110
word there are compounding words or

00:17:22,280 --> 00:17:27,170
something like that we need an analyzer

00:17:24,110 --> 00:17:30,230
which analyzes correctly and you know

00:17:27,170 --> 00:17:33,080
brings the sub words out of it and then

00:17:30,230 --> 00:17:35,530
in terms of improvement we can always

00:17:33,080 --> 00:17:38,000
rewrite queries to boost the documents

00:17:35,530 --> 00:17:41,390
to boost the fields correct field so

00:17:38,000 --> 00:17:45,800
that the the relevancy matches better

00:17:41,390 --> 00:17:48,100
and there is no loss of precision so

00:17:45,800 --> 00:17:50,450
that's that's all from my side actually

00:17:48,100 --> 00:17:51,830
also and we are hiring smart engineers

00:17:50,450 --> 00:17:55,330
like all you guys

00:17:51,830 --> 00:17:59,980
is joining our music our careers page

00:17:55,330 --> 00:18:03,730
and yeah Wow

00:17:59,980 --> 00:18:06,019
question from like new candles that's a

00:18:03,730 --> 00:18:08,840
multilingual search is incredibly hard

00:18:06,019 --> 00:18:10,669
and incredibly important so you built a

00:18:08,840 --> 00:18:11,929
language detector which is a missing

00:18:10,669 --> 00:18:14,000
piece of infrastructure in the open

00:18:11,929 --> 00:18:15,529
source world some two questions do you

00:18:14,000 --> 00:18:17,539
know about Google's compact language

00:18:15,529 --> 00:18:19,700
detector which is an open source

00:18:17,539 --> 00:18:21,919
language detector factored out of the

00:18:19,700 --> 00:18:23,870
chromium open source web browser mm-hmm

00:18:21,919 --> 00:18:26,590
first question second question do you

00:18:23,870 --> 00:18:28,580
have plans to open-source your bottle

00:18:26,590 --> 00:18:29,750
sorry I did not get a second question

00:18:28,580 --> 00:18:31,490
what else do you have plans to open

00:18:29,750 --> 00:18:33,019
source your language detect oh okay

00:18:31,490 --> 00:18:34,490
there's a piece of infrastructure so

00:18:33,019 --> 00:18:36,019
many people need it really ought to be a

00:18:34,490 --> 00:18:39,529
solved problem by now in the open source

00:18:36,019 --> 00:18:42,710
world but it's not yeah yet so we build

00:18:39,529 --> 00:18:45,500
our own custom model because we are you

00:18:42,710 --> 00:18:47,510
know use it as a part of our email

00:18:45,500 --> 00:18:49,940
processing system basically to identify

00:18:47,510 --> 00:18:53,149
which which documents they are belonging

00:18:49,940 --> 00:18:56,029
to and we have a separate data analytics

00:18:53,149 --> 00:18:59,809
team which which created that that

00:18:56,029 --> 00:19:02,960
language detection models we are

00:18:59,809 --> 00:19:06,139
actually also quite involved in open

00:19:02,960 --> 00:19:09,519
source we are starting to involve as I

00:19:06,139 --> 00:19:12,110
can say but I don't really have any

00:19:09,519 --> 00:19:16,269
answer for that if we actually language

00:19:12,110 --> 00:19:16,269
detection models but I can check sure

00:19:16,809 --> 00:19:29,169
sure did you encounter a situation where

00:19:25,970 --> 00:19:32,600
a document could have multiple languages

00:19:29,169 --> 00:19:36,139
yes of course so how did you address

00:19:32,600 --> 00:19:39,500
that problem so in that case what we do

00:19:36,139 --> 00:19:41,870
is we rely on users query order you know

00:19:39,500 --> 00:19:44,659
corpus so for example if we contain an

00:19:41,870 --> 00:19:47,149
example of World Museum which is same in

00:19:44,659 --> 00:19:49,549
German and English basically in that

00:19:47,149 --> 00:19:51,080
case there will be English documents

00:19:49,549 --> 00:19:53,840
containing Museum as well as German

00:19:51,080 --> 00:19:55,399
documents containing Museum so in that

00:19:53,840 --> 00:19:57,769
case what we do is basically we search

00:19:55,399 --> 00:19:59,870
on all languages the user has because he

00:19:57,769 --> 00:20:01,700
has all languages he has documents in

00:19:59,870 --> 00:20:04,429
all languages so we have to show them

00:20:01,700 --> 00:20:04,980
but if his his corpus is more of German

00:20:04,429 --> 00:20:06,750
then we

00:20:04,980 --> 00:20:09,660
boost those that do those records

00:20:06,750 --> 00:20:13,700
basically so that it's a higher precise

00:20:09,660 --> 00:20:13,700

YouTube URL: https://www.youtube.com/watch?v=otRdUhHqJjI


