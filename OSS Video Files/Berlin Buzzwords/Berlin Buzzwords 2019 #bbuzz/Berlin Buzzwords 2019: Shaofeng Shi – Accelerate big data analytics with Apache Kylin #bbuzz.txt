Title: Berlin Buzzwords 2019: Shaofeng Shi â€“ Accelerate big data analytics with Apache Kylin #bbuzz
Publication date: 2019-06-27
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	To achieve high performance or interactive analytics on a big data set, most massively parallel processing (MPP) solutions resort to putting a large proportion of the dataset into memory and launch as many CPU cores as possible to deliver query results in time. By nature, MPP solutions could easily hit throughput bottlenecks at high concurrency or budget issues when datasets grow too large to fit in memory.

Apache Kylin proposed another solution to speed up analytical queries with pre-built OLAP Cube (which is essentially groups of aggregate tables). The Cubes are built with MapReduce/Spark on commodity hardware so that a large volume of datasets can be handled at a reasonable cost.

This talk will have the following detailed topics:
- Apache Kylin background
- Why OLAP Cube is needed for big data
- How Kylin build the Cube on Hadoop
- Performance benchmark
- Use cases

Read more:
https://2019.berlinbuzzwords.de/19/session/accelerate-big-data-analytics-apache-kylin

About Shaofeng Shi:
https://2019.berlinbuzzwords.de/users/shaofeng-shi

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,730 --> 00:00:15,279
welcome to these sessions my name is

00:00:11,379 --> 00:00:17,770
I'm from China Shanghai today the topic

00:00:15,279 --> 00:00:22,439
is how to accelerate the big data

00:00:17,770 --> 00:00:25,900
analytics on Hadoop with a particularly

00:00:22,439 --> 00:00:28,629
this is my this is about me

00:00:25,900 --> 00:00:34,300
I'm a particulate emitter and the PM is

00:00:28,629 --> 00:00:36,730
a member since 2014 when I was working

00:00:34,300 --> 00:00:39,430
for eBay in Shanghai

00:00:36,730 --> 00:00:44,430
now I'm the chief architecture in

00:00:39,430 --> 00:00:47,890
collisions this is today's agenda

00:00:44,430 --> 00:00:50,620
firstly I'd like to introduce the

00:00:47,890 --> 00:00:53,739
background of this project what's the

00:00:50,620 --> 00:00:57,040
problem we are facing and what's the

00:00:53,739 --> 00:00:59,260
solution we proposed next Li I will

00:00:57,040 --> 00:01:02,649
introduce a whole application build

00:00:59,260 --> 00:01:05,969
store and queries or OLAP cubes on

00:01:02,649 --> 00:01:09,850
Hadoop I will also give a simple

00:01:05,969 --> 00:01:14,549
provenance benchmark in the final part

00:01:09,850 --> 00:01:14,549
have you introduced several use cases

00:01:15,840 --> 00:01:22,630
the background is pretty simple because

00:01:19,689 --> 00:01:27,749
today more and more data are collected

00:01:22,630 --> 00:01:31,299
by the companies and when we produce

00:01:27,749 --> 00:01:35,380
processing the Big Data Hadoop is one of

00:01:31,299 --> 00:01:38,950
the best choice well if you're looking

00:01:35,380 --> 00:01:41,770
at the technical components in Hadoop

00:01:38,950 --> 00:01:45,520
you will find that most of them are

00:01:41,770 --> 00:01:48,340
developed for batch processing they were

00:01:45,520 --> 00:01:55,539
not designed for the low latency cycle

00:01:48,340 --> 00:01:57,969
queries that means when the data data

00:01:55,539 --> 00:02:01,539
size keep will increase you will spend

00:01:57,969 --> 00:02:05,950
more and more time in running the cycle

00:02:01,539 --> 00:02:09,300
queries that will cause worse and worse

00:02:05,950 --> 00:02:12,930
user experience for your data analysis

00:02:09,300 --> 00:02:18,280
so the Challenger where were facing is

00:02:12,930 --> 00:02:21,940
how to keep the high performance as the

00:02:18,280 --> 00:02:24,130
data grows to be precise the high

00:02:21,940 --> 00:02:24,940
performance away means the query

00:02:24,130 --> 00:02:29,530
Leighton

00:02:24,940 --> 00:02:33,130
should be at second to sub second level

00:02:29,530 --> 00:02:40,720
which is very comfortable for a human

00:02:33,130 --> 00:02:41,560
being we investigated the solutions in

00:02:40,720 --> 00:02:43,780
the domain

00:02:41,560 --> 00:02:47,260
there are mainly two technique

00:02:43,780 --> 00:02:49,390
technologies one is the massive parallel

00:02:47,260 --> 00:02:53,200
processing solution the other is a

00:02:49,390 --> 00:02:57,370
secure on hadoop solution the sample

00:02:53,200 --> 00:03:01,480
includes Amazon redshift pivotal current

00:02:57,370 --> 00:03:06,720
plan and also the presto Impala and the

00:03:01,480 --> 00:03:09,490
 oh well when we run in the

00:03:06,720 --> 00:03:14,310
benchmarks on big data we find that

00:03:09,490 --> 00:03:17,680
these MPP solutions couldn't work well

00:03:14,310 --> 00:03:20,530
because these solutions they are trying

00:03:17,680 --> 00:03:25,570
to accelerator the queries with

00:03:20,530 --> 00:03:26,970
following methodologies one is shared

00:03:25,570 --> 00:03:29,860
Anacin

00:03:26,970 --> 00:03:33,670
architecture they will distribute the

00:03:29,860 --> 00:03:38,250
data into multi nodes by certain keys

00:03:33,670 --> 00:03:38,250
and then when you query the data

00:03:56,709 --> 00:04:12,019
the office on Mecca is very stable okay

00:04:09,439 --> 00:04:15,039
they will distributed the data by

00:04:12,019 --> 00:04:18,320
certain rules to multi-node and then

00:04:15,039 --> 00:04:22,460
translated a single query to parallel

00:04:18,320 --> 00:04:26,470
processing so to reduce the total

00:04:22,460 --> 00:04:30,010
latency another optimization is to

00:04:26,470 --> 00:04:35,830
convert the data into columnar format

00:04:30,010 --> 00:04:39,650
with some type of the compression and

00:04:35,830 --> 00:04:43,010
also the vo in order to improve the

00:04:39,650 --> 00:04:46,400
performance they will catch the data in

00:04:43,010 --> 00:04:49,460
memory as much as possible the total

00:04:46,400 --> 00:04:55,400
architecture if you look at that there

00:04:49,460 --> 00:05:01,460
are many many bottlenecks such as memory

00:04:55,400 --> 00:05:04,330
the CPU and the network so the PPE

00:05:01,460 --> 00:05:07,280
solution they will have the following

00:05:04,330 --> 00:05:12,740
limitations the first user performance

00:05:07,280 --> 00:05:15,710
couldn't be very low latency early the

00:05:12,740 --> 00:05:21,260
latency is from tens of seconds to even

00:05:15,710 --> 00:05:24,949
tens of minutes secondly it couldn't so

00:05:21,260 --> 00:05:27,710
multiple concurrent users because one

00:05:24,949 --> 00:05:31,550
query may exhaust all the resources in

00:05:27,710 --> 00:05:34,880
the cluster the third problem is about

00:05:31,550 --> 00:05:37,460
others capability each time when you

00:05:34,880 --> 00:05:39,220
added note that we already read rest of

00:05:37,460 --> 00:05:43,190
the cluster you will find that it will

00:05:39,220 --> 00:05:46,930
needed to redistribute all the data it

00:05:43,190 --> 00:05:49,909
will take several hours to finish and

00:05:46,930 --> 00:05:52,729
when you add more and more order to this

00:05:49,909 --> 00:05:57,620
cluster the master node will become

00:05:52,729 --> 00:06:02,139
bottleneck this limited the cluster to

00:05:57,620 --> 00:06:02,139
expand it to a larger size

00:06:02,450 --> 00:06:11,000
so I think what a reliable solution for

00:06:08,030 --> 00:06:15,410
bigoted firstly it should be high

00:06:11,000 --> 00:06:18,440
performance and it can so multiple

00:06:15,410 --> 00:06:21,010
concurrent users with the Kudus

00:06:18,440 --> 00:06:24,920
capability so that you'd only needed to

00:06:21,010 --> 00:06:28,670
always move your data and it needed to

00:06:24,920 --> 00:06:30,710
be standard standard can polenta with so

00:06:28,670 --> 00:06:34,400
that you can integrate it with other

00:06:30,710 --> 00:06:39,140
system and the finally the OLAP solution

00:06:34,400 --> 00:06:44,600
need to be ease of use so we propose

00:06:39,140 --> 00:06:49,070
that our solution is to develop a new

00:06:44,600 --> 00:06:51,100
OLAP engine on top of Hadoop we call it

00:06:49,070 --> 00:06:58,550
as a bikini

00:06:51,100 --> 00:07:01,450
this project was originated in eBay the

00:06:58,550 --> 00:07:04,970
this diagram shows the position of

00:07:01,450 --> 00:07:08,240
killing it is running on top of Hadoop

00:07:04,970 --> 00:07:13,070
and the connector connecting with your

00:07:08,240 --> 00:07:16,310
bi to your dashboard your report is the

00:07:13,070 --> 00:07:19,820
core concept in about killing is using

00:07:16,310 --> 00:07:24,110
the OLAP cube technology it can support

00:07:19,820 --> 00:07:28,010
a very large data skill from PB to a TB

00:07:24,110 --> 00:07:31,520
to PB well the overall latency can be

00:07:28,010 --> 00:07:33,530
controlled as a subsection D level the

00:07:31,520 --> 00:07:36,440
query language is unsecure

00:07:33,530 --> 00:07:39,860
so that is a user doesn't need to learn

00:07:36,440 --> 00:07:42,830
another language we will provide the

00:07:39,860 --> 00:07:46,280
JDBC ODBC and the rest api for you to

00:07:42,830 --> 00:07:50,840
integrate it with other tools it will

00:07:46,280 --> 00:07:54,010
have a better bi integration for example

00:07:50,840 --> 00:07:57,590
connecting from tableau from Cognos from

00:07:54,010 --> 00:08:01,640
MicroStrategy and the hazard tools will

00:07:57,590 --> 00:08:04,880
provide a user-friendly web GUI for the

00:08:01,640 --> 00:08:08,060
kinetics to self serving and then you

00:08:04,880 --> 00:08:10,880
can integrate it with your user

00:08:08,060 --> 00:08:14,020
authentication system where out app and

00:08:10,880 --> 00:08:14,020
a single sound

00:08:14,200 --> 00:08:21,650
when we're talking about the OLAP cube

00:08:18,190 --> 00:08:25,100
Cuba is a data structure optimizes the

00:08:21,650 --> 00:08:28,730
father multi multiple - dimensional data

00:08:25,100 --> 00:08:32,590
access its performance can be very very

00:08:28,730 --> 00:08:35,780
fast this technology has been widely

00:08:32,590 --> 00:08:38,050
adopted by the data warehouse products

00:08:35,780 --> 00:08:41,780
in the past years

00:08:38,050 --> 00:08:45,580
well if you look at big data domains

00:08:41,780 --> 00:08:50,470
there is no to to to build the OLAP cube

00:08:45,580 --> 00:08:55,220
so we as we were thinking can we try to

00:08:50,470 --> 00:08:59,930
move the cube concept into the big data

00:08:55,220 --> 00:09:02,570
domain if we computed the cube and the

00:08:59,930 --> 00:09:07,850
creditor Cuba for Big Data what's the

00:09:02,570 --> 00:09:11,030
benefit firstly we will get the benefit

00:09:07,850 --> 00:09:13,850
under performance because the you don't

00:09:11,030 --> 00:09:17,960
need to scan your raw data the cube is

00:09:13,850 --> 00:09:20,510
rigid and the indexed so it can be 1,000

00:09:17,960 --> 00:09:25,100
the faster than reading from raw data

00:09:20,510 --> 00:09:30,950
and it will be cost effective for

00:09:25,100 --> 00:09:35,170
example once the cube will be billed the

00:09:30,950 --> 00:09:39,140
queries after that will be very fast so

00:09:35,170 --> 00:09:42,440
you just need to run one comprehensive

00:09:39,140 --> 00:09:47,630
bill and the Kin the provenance

00:09:42,440 --> 00:09:49,610
increment isn't next under the cubes

00:09:47,630 --> 00:09:52,730
they are very easy to learn to

00:09:49,610 --> 00:09:55,660
understand to create to update by your

00:09:52,730 --> 00:10:00,520
analytics that means that theta

00:09:55,660 --> 00:10:05,420
engineers doesn't need to involve this

00:10:00,520 --> 00:10:09,190
you you might concern is it possible to

00:10:05,420 --> 00:10:12,530
build the cube for such huge data set

00:10:09,190 --> 00:10:15,230
you don't need to worry about that a bit

00:10:12,530 --> 00:10:21,620
because Hadoop is enough maturity today

00:10:15,230 --> 00:10:26,150
we can use it to do this process this is

00:10:21,620 --> 00:10:29,660
the overall data flow in a very killing

00:10:26,150 --> 00:10:33,620
we separated the data flow into two part

00:10:29,660 --> 00:10:37,279
one party is offline data flow the other

00:10:33,620 --> 00:10:41,300
is online data flow marketing in the

00:10:37,279 --> 00:10:44,600
blue and Corinne the offline data flow

00:10:41,300 --> 00:10:47,420
it is a cube building process a public

00:10:44,600 --> 00:10:51,230
hearing will generated the job steps to

00:10:47,420 --> 00:10:55,220
extract the source data from high Kafka

00:10:51,230 --> 00:10:58,070
or other system extracted them to the

00:10:55,220 --> 00:11:01,570
Hadoop and then wrong the MapReduce or

00:10:58,070 --> 00:11:05,029
spark jobs against the same and the

00:11:01,570 --> 00:11:10,040
converted to HBase format and loaded

00:11:05,029 --> 00:11:13,279
into HBase once the Kilby build you will

00:11:10,040 --> 00:11:17,089
be able to query them with the unceasing

00:11:13,279 --> 00:11:19,700
Co you can connect the from your peer

00:11:17,089 --> 00:11:22,970
tools or dashboard to a killing killing

00:11:19,700 --> 00:11:26,480
will translate will pass the seeker and

00:11:22,970 --> 00:11:29,060
the translator it to cube or visiting it

00:11:26,480 --> 00:11:32,510
will no longer needed to touch your

00:11:29,060 --> 00:11:35,589
source data so the online data flow will

00:11:32,510 --> 00:11:35,589
be much faster

00:11:38,350 --> 00:11:43,940
someone may mention that is the cube the

00:11:41,930 --> 00:11:47,200
same concept as them what he realized

00:11:43,940 --> 00:11:51,080
the view they are similar but they are

00:11:47,200 --> 00:11:54,310
they are different in one cube by

00:11:51,080 --> 00:11:58,130
default it will have many qubits

00:11:54,310 --> 00:12:02,709
one dimension cube will have two and

00:11:58,130 --> 00:12:06,980
power Q boards each keyboard is actually

00:12:02,709 --> 00:12:12,550
materialized view and this is the sample

00:12:06,980 --> 00:12:15,140
of a four dimensional cube this is a

00:12:12,550 --> 00:12:19,100
cube order of the four dimension and

00:12:15,140 --> 00:12:24,500
there will be four three dimension cube

00:12:19,100 --> 00:12:28,279
cuboid and six two dimension keyboard

00:12:24,500 --> 00:12:31,550
and for one dimensional cube or all

00:12:28,279 --> 00:12:35,880
these keyboard will be built in one job

00:12:31,550 --> 00:12:40,889
so all the data in all of them

00:12:35,880 --> 00:12:44,250
consistent when you browse your data or

00:12:40,889 --> 00:12:46,050
up or down killing will pick up the pace

00:12:44,250 --> 00:12:48,810
to match the keyboard to answer your

00:12:46,050 --> 00:12:52,620
question so even if you draw up to a

00:12:48,810 --> 00:13:01,860
very high level data the performance can

00:12:52,620 --> 00:13:05,130
be satisfied this page shows how killing

00:13:01,860 --> 00:13:07,769
builded the cube we divided the cube

00:13:05,130 --> 00:13:11,730
building into several steps and the each

00:13:07,769 --> 00:13:15,660
step is maybe a high job MapReduce job

00:13:11,730 --> 00:13:19,110
or HBase job killing will exactly the

00:13:15,660 --> 00:13:28,949
same one by one and automatically so you

00:13:19,110 --> 00:13:31,680
don't need to write any code the first

00:13:28,949 --> 00:13:34,670
step really is to extract the source

00:13:31,680 --> 00:13:38,310
data to Hadoop and design killing will

00:13:34,670 --> 00:13:40,670
extract the dimension values for them to

00:13:38,310 --> 00:13:44,790
build the dimension dimension

00:13:40,670 --> 00:13:48,050
dictionaries then it will start several

00:13:44,790 --> 00:13:50,910
round jobs to build the cubes gradually

00:13:48,050 --> 00:13:54,300
by default we will use the belayer

00:13:50,910 --> 00:13:57,660
cubing process it will firstly build the

00:13:54,300 --> 00:14:00,720
base keyboard which which contains all

00:13:57,660 --> 00:14:04,079
the dimensions and then based on it we

00:14:00,720 --> 00:14:08,399
will create aggregated to cater and

00:14:04,079 --> 00:14:10,910
minus one cue balls and that repeated

00:14:08,399 --> 00:14:14,279
this until all the cue balls be

00:14:10,910 --> 00:14:16,470
calculated finally we will convert the

00:14:14,279 --> 00:14:21,110
cube to the edge piece for matter under

00:14:16,470 --> 00:14:27,329
loading into HBase how killing

00:14:21,110 --> 00:14:31,829
persistence the cube in HBase we know

00:14:27,329 --> 00:14:35,100
that Cuba is composed by many cue board

00:14:31,829 --> 00:14:38,990
and the HQ board has the dimensions and

00:14:35,100 --> 00:14:42,680
the measures and the query early is to

00:14:38,990 --> 00:14:46,350
scan a certain cue board with some

00:14:42,680 --> 00:14:48,959
dimensions so we will combine the cube

00:14:46,350 --> 00:14:49,800
order ID together with the dimension

00:14:48,959 --> 00:14:53,129
values as

00:14:49,800 --> 00:14:56,489
HBase rocky and the proteges measures

00:14:53,129 --> 00:15:08,040
also called matrix to HBase the column

00:14:56,489 --> 00:15:11,399
values how we query the OLAP cubes in

00:15:08,040 --> 00:15:14,480
killing to make the system easy to use

00:15:11,399 --> 00:15:17,369
we use cycle as the query language

00:15:14,480 --> 00:15:20,369
killing in degrees opportunity cassette

00:15:17,369 --> 00:15:25,259
as the psychic passer and the optimizer

00:15:20,369 --> 00:15:29,429
let's see an example this is a typical

00:15:25,259 --> 00:15:34,379
lab query that selects two dimensions

00:15:29,429 --> 00:15:39,389
add the two matrix from two tables there

00:15:34,379 --> 00:15:43,980
will be a filter condition and the

00:15:39,389 --> 00:15:47,329
finally attained either salty this query

00:15:43,980 --> 00:15:50,610
will be falsely deposited into such an

00:15:47,329 --> 00:15:55,259
execution plan and will be equals i

00:15:50,610 --> 00:15:58,339
cuted from bottom to up well looking at

00:15:55,259 --> 00:16:01,470
this plan the table joy and the

00:15:58,339 --> 00:16:06,199
aggregation surely is the most

00:16:01,470 --> 00:16:09,559
time-consuming part which means if you

00:16:06,199 --> 00:16:15,179
executor in this plan the time

00:16:09,559 --> 00:16:18,749
complexity is at least Owen Wow how

00:16:15,179 --> 00:16:23,639
killing to it killing will pre calculate

00:16:18,749 --> 00:16:26,009
the data pre a cricketer the data into

00:16:23,639 --> 00:16:29,549
cubes that means the table join and the

00:16:26,009 --> 00:16:32,790
allocation has already been finished in

00:16:29,549 --> 00:16:35,790
the queue building phase then for this

00:16:32,790 --> 00:16:38,459
plan we can rewrite it to start from

00:16:35,790 --> 00:16:42,629
this cube and the to some filter and

00:16:38,459 --> 00:16:47,220
then minus RT as we know the human is

00:16:42,629 --> 00:16:51,299
already created under indexed so the

00:16:47,220 --> 00:16:54,509
cube visiting will be in consistent time

00:16:51,299 --> 00:16:56,899
complexity with which is almost the

00:16:54,509 --> 00:16:56,899
whole one

00:16:59,000 --> 00:17:06,870
we did a performance benchmark to

00:17:03,060 --> 00:17:10,550
compare killing with hype with the star

00:17:06,870 --> 00:17:15,660
schema benchmark this diagram shows how

00:17:10,550 --> 00:17:19,140
how much be improved when changed to a

00:17:15,660 --> 00:17:24,270
play killing we can say for each query

00:17:19,140 --> 00:17:28,650
at least it will be improved 200 times

00:17:24,270 --> 00:17:30,750
and the most the speed biggest in

00:17:28,650 --> 00:17:35,760
speed-up I use a more than one salt in

00:17:30,750 --> 00:17:39,570
the time this this is unfair because

00:17:35,760 --> 00:17:42,870
most of computation has already been

00:17:39,570 --> 00:17:46,700
finished in killing but the user

00:17:42,870 --> 00:17:50,090
experience II is very different another

00:17:46,700 --> 00:17:56,460
diagram shoes as the data increases

00:17:50,090 --> 00:17:59,730
chilis latency is very stable well for a

00:17:56,460 --> 00:18:06,150
petty high it really increases as your

00:17:59,730 --> 00:18:10,020
data increases killing also have many

00:18:06,150 --> 00:18:11,310
advanced features for example when you

00:18:10,020 --> 00:18:14,190
have many dimensions

00:18:11,310 --> 00:18:17,940
you can't even a partial cube with some

00:18:14,190 --> 00:18:19,170
rules of is some algorithm we call a

00:18:17,940 --> 00:18:23,870
queue Brenner

00:18:19,170 --> 00:18:28,620
it also support very high cardinality

00:18:23,870 --> 00:18:33,360
dimension such as user ID cell phone

00:18:28,620 --> 00:18:39,120
number etc and it'll support precisely

00:18:33,360 --> 00:18:41,550
count distinct measure on UHC column it

00:18:39,120 --> 00:18:44,850
also support incremental data loader so

00:18:41,550 --> 00:18:48,750
that you don't need to refresh history

00:18:44,850 --> 00:18:52,800
data when you load muted it also

00:18:48,750 --> 00:18:56,550
supported use Kafka and a DBMS as a data

00:18:52,800 --> 00:19:00,560
source in the production environment you

00:18:56,550 --> 00:19:03,780
can scale it to multi node cluster and

00:19:00,560 --> 00:19:06,090
even you can separate the cube building

00:19:03,780 --> 00:19:08,760
and the cube query into different Arab

00:19:06,090 --> 00:19:11,510
cluster we called reader/writer

00:19:08,760 --> 00:19:11,510
separation

00:19:12,020 --> 00:19:16,890
currently the killing we Street auto is

00:19:15,870 --> 00:19:22,340
on the way

00:19:16,890 --> 00:19:22,340
the main feature is the real-time or lab

00:19:24,230 --> 00:19:31,170
this is a summary of the best scenario

00:19:27,090 --> 00:19:33,960
for upper killing the first scenario is

00:19:31,170 --> 00:19:37,560
a dashboard reporting and the binney's

00:19:33,960 --> 00:19:40,590
intelligent Mac radiator from Lex a

00:19:37,560 --> 00:19:42,870
database or MPP solutions

00:19:40,590 --> 00:19:45,570
the second scenario is the

00:19:42,870 --> 00:19:49,500
multi-dimensional data exploration you

00:19:45,570 --> 00:19:53,430
can let your data energy tix

00:19:49,500 --> 00:19:57,510
to self so to find the values in the

00:19:53,430 --> 00:19:59,850
data the sort of scenario is to offload

00:19:57,510 --> 00:20:04,020
the traditional data warehouse on to

00:19:59,850 --> 00:20:08,370
Hadoop killing can help you to as salary

00:20:04,020 --> 00:20:11,250
to these queries and also many users use

00:20:08,370 --> 00:20:17,180
killing for the user behavior analysis

00:20:11,250 --> 00:20:20,370
such as to computer the TV movie to the

00:20:17,180 --> 00:20:25,290
funeral and as his retention analysis

00:20:20,370 --> 00:20:28,710
etc and also it can be used the father

00:20:25,290 --> 00:20:31,680
transparent query acceleration the user

00:20:28,710 --> 00:20:34,140
doesn't and needed to via the underlying

00:20:31,680 --> 00:20:39,840
and genes the Justice Center the on

00:20:34,140 --> 00:20:45,840
sensical queries ok nextly

00:20:39,840 --> 00:20:48,390
let's look at several use cases this is

00:20:45,840 --> 00:20:50,880
one of the biggest a particulate

00:20:48,390 --> 00:20:54,450
improvement in the world it is in the

00:20:50,880 --> 00:20:57,540
matron and the damping me twenty mean is

00:20:54,450 --> 00:21:01,560
the biggest online to offline service

00:20:57,540 --> 00:21:06,960
provider in China there are many see is

00:21:01,560 --> 00:21:11,040
a degree reaching hotel travel and also

00:21:06,960 --> 00:21:14,280
mobile mobile the share the in

00:21:11,040 --> 00:21:17,630
orange is also a business line in mate

00:21:14,280 --> 00:21:20,760
one importing I see there are a lot of

00:21:17,630 --> 00:21:25,830
mobile on the street

00:21:20,760 --> 00:21:29,220
my fur has more than 300 300 million

00:21:25,830 --> 00:21:33,080
active users and the other three milling

00:21:29,220 --> 00:21:35,429
merchants so you can imagine how much

00:21:33,080 --> 00:21:38,880
transaction data and the user behavior

00:21:35,429 --> 00:21:42,690
data be collected and they in four years

00:21:38,880 --> 00:21:46,169
ago they decided to build a central or

00:21:42,690 --> 00:21:49,919
lab platform for this for the whole

00:21:46,169 --> 00:21:53,250
company they did some investigation and

00:21:49,919 --> 00:21:55,980
the selected after killing to do this to

00:21:53,250 --> 00:22:01,340
serving thousands of their business

00:21:55,980 --> 00:22:05,730
analysts and their external partners

00:22:01,340 --> 00:22:09,630
till last year there are more than 1,000

00:22:05,730 --> 00:22:13,080
the cubes in the killing and the total

00:22:09,630 --> 00:22:17,730
data feeding into killing is more than a

00:22:13,080 --> 00:22:22,980
trillion rules the cube storage is near

00:22:17,730 --> 00:22:25,890
close to one pp every day they will have

00:22:22,980 --> 00:22:28,830
more than three million cycle queries

00:22:25,890 --> 00:22:33,960
and most of these queries are triggered

00:22:28,830 --> 00:22:38,220
by the analysts while on such a huge

00:22:33,960 --> 00:22:41,100
dataset and with so many queries most of

00:22:38,220 --> 00:22:48,750
the queries can be answered in around a

00:22:41,100 --> 00:22:55,470
one second the second a youth kiss use

00:22:48,750 --> 00:23:00,150
from Yahoo Japan Yahoo Japan is the one

00:22:55,470 --> 00:23:03,000
of the biggest pato in in Japan it

00:23:00,150 --> 00:23:06,840
provided the email service news services

00:23:03,000 --> 00:23:10,410
and also inertial beans they used to use

00:23:06,840 --> 00:23:13,679
Apache Impala as an education engines

00:23:10,410 --> 00:23:16,650
for for data analysis well it's a

00:23:13,679 --> 00:23:20,340
performance couldn't fulfill has the

00:23:16,650 --> 00:23:24,419
data crews so they migrated ATF from an

00:23:20,340 --> 00:23:28,850
Impala to killing after this most of the

00:23:24,419 --> 00:23:34,519
queries were reduced from one minute

00:23:28,850 --> 00:23:37,700
second and also as I mentioned the

00:23:34,519 --> 00:23:42,440
killing can be deployed into two Hadoop

00:23:37,700 --> 00:23:47,090
cluster so the deploys the killings jobs

00:23:42,440 --> 00:23:50,240
are in DC one and the deploys a killing

00:23:47,090 --> 00:23:55,070
query server in DC to DC one is a shared

00:23:50,240 --> 00:23:57,799
Hadoop cluster in America and the DC 2

00:23:55,070 --> 00:24:02,059
is in Japan which is closer to two layer

00:23:57,799 --> 00:24:04,070
and analysis after killing builders the

00:24:02,059 --> 00:24:07,940
raw data into cube killing can

00:24:04,070 --> 00:24:11,120
automatically pushes the cube into the

00:24:07,940 --> 00:24:15,529
query cluster nearly the cube can be

00:24:11,120 --> 00:24:19,100
much smaller than the original raw data

00:24:15,529 --> 00:24:23,929
so use this they can see what a lot of

00:24:19,100 --> 00:24:29,809
bandwidth you can read them all on this

00:24:23,929 --> 00:24:32,840
tech blog the so the use case is from

00:24:29,809 --> 00:24:37,820
Telecom e telling Co me is a mobile

00:24:32,840 --> 00:24:42,259
payments company in spam they used to

00:24:37,820 --> 00:24:46,460
use my secure and the vodka to do the

00:24:42,259 --> 00:24:48,799
data analytics well as data karo and the

00:24:46,460 --> 00:24:51,559
growth they are syncing the new

00:24:48,799 --> 00:24:55,490
solutions and finally they deployed the

00:24:51,559 --> 00:24:58,580
Hadoop and killing to support their

00:24:55,490 --> 00:25:06,860
interactive queries and this solution

00:24:58,580 --> 00:25:14,169
has been a very good result the first

00:25:06,860 --> 00:25:17,480
use case is from a UX group yesterday

00:25:14,169 --> 00:25:21,559
evening I have dinner with him and know

00:25:17,480 --> 00:25:26,330
the whole storage in our X where X is a

00:25:21,559 --> 00:25:29,860
global online marketplace the data team

00:25:26,330 --> 00:25:35,600
is important but there are penises in

00:25:29,860 --> 00:25:39,110
more than 40 countries they used to use

00:25:35,600 --> 00:25:42,260
Apache Amazon redshift as a data

00:25:39,110 --> 00:25:44,690
warehouse well as the data grows

00:25:42,260 --> 00:25:49,640
redshift couldn't a skill so they

00:25:44,690 --> 00:25:54,799
decided to migrate the data to s3 and

00:25:49,640 --> 00:25:56,919
zambuta an ethics over above s3 and as

00:25:54,799 --> 00:26:00,410
they trotted a couple of solutions

00:25:56,919 --> 00:26:04,030
included the redshift spectrum and the

00:26:00,410 --> 00:26:07,040
opposite route and the other and finally

00:26:04,030 --> 00:26:11,419
they find that only killing can match

00:26:07,040 --> 00:26:15,410
the requirement one major reason is that

00:26:11,419 --> 00:26:20,290
their penis and acid are using tableau

00:26:15,410 --> 00:26:23,630
as the - well other other engines the

00:26:20,290 --> 00:26:25,760
couldn't match the requirement the

00:26:23,630 --> 00:26:28,309
performance requirement or they couldn't

00:26:25,760 --> 00:26:33,230
match the P I integral integration

00:26:28,309 --> 00:26:38,360
requirement they said the killing is a

00:26:33,230 --> 00:26:41,240
game changer with its stream faster

00:26:38,360 --> 00:26:46,970
performance and the simplest integration

00:26:41,240 --> 00:26:50,030
with tableau here are some useful links

00:26:46,970 --> 00:26:56,120
you can approach to learn more about

00:26:50,030 --> 00:27:00,530
educating and we have the meetups we'll

00:26:56,120 --> 00:27:03,919
be holding in China in US and sometimes

00:27:00,530 --> 00:27:07,299
in the Euro mainly in Spain or in

00:27:03,919 --> 00:27:11,260
England and the usual future we also

00:27:07,299 --> 00:27:14,990
wish we can have the metaphor in Berlin

00:27:11,260 --> 00:27:20,450
if you need the enterprise solution you

00:27:14,990 --> 00:27:24,770
can conduct the Cajuns and the last I

00:27:20,450 --> 00:27:28,100
have one time oh I believe the live demo

00:27:24,770 --> 00:27:34,040
will give more comprehensive

00:27:28,100 --> 00:27:37,900
understanding about this tool this demo

00:27:34,040 --> 00:27:37,900
is made by my colleague Oh

00:27:38,140 --> 00:27:44,990
the demo is trying to show the user

00:27:41,630 --> 00:27:47,240
experience the difference between before

00:27:44,990 --> 00:27:49,840
user and particularly and after using up

00:27:47,240 --> 00:27:49,840
the killing

00:28:29,020 --> 00:28:31,710
okay

00:28:47,049 --> 00:28:55,429
here we will firstly use highway as the

00:28:50,419 --> 00:28:58,429
engine on a milling dataset you can see

00:28:55,429 --> 00:29:00,460
that when you click a button the page is

00:28:58,429 --> 00:29:05,090
just a refreshing refreshing refreshing

00:29:00,460 --> 00:29:09,650
the experiencing is very bad nextly we

00:29:05,090 --> 00:29:13,220
builded the cube for that they said then

00:29:09,650 --> 00:29:16,730
when you make any selection on the other

00:29:13,220 --> 00:29:22,029
web GUI or on the power bi tools that

00:29:16,730 --> 00:29:22,029
will refresh the page in the real time

00:29:30,030 --> 00:29:33,160
[Music]

00:29:49,730 --> 00:29:53,410
[Music]

00:29:56,630 --> 00:30:07,820
it it also can be used as a back-end ng4

00:30:03,660 --> 00:30:13,850
tableau here we have a data set with

00:30:07,820 --> 00:30:17,730
several billion rows let's see how it's

00:30:13,850 --> 00:30:21,120
active in tableau when you drag some

00:30:17,730 --> 00:30:28,560
filter all dimensions and make some

00:30:21,120 --> 00:30:30,030
selections the reporter or the diagram

00:30:28,560 --> 00:30:32,900
will refreshed

00:30:30,030 --> 00:30:32,900
very quickly

00:30:40,290 --> 00:30:47,820
you can also define some segments level

00:30:45,240 --> 00:30:50,970
on your data for example you can define

00:30:47,820 --> 00:30:55,100
some community the column and design

00:30:50,970 --> 00:30:55,100
using ET in your report

00:31:22,170 --> 00:31:39,270
okay this is all the transition at the

00:31:26,330 --> 00:31:41,850
demo so anyone has any questions okay

00:31:39,270 --> 00:31:44,030
let's think shopping for you stop thank

00:31:41,850 --> 00:31:44,030
you

00:31:45,050 --> 00:31:55,320
now we have time for questions

00:31:47,930 --> 00:31:57,060
wanna fight or other question about the

00:31:55,320 --> 00:31:58,950
persistence layer I saw there's a plan

00:31:57,060 --> 00:32:01,110
to replace HBase with pocket or

00:31:58,950 --> 00:32:05,070
something else is is this going to be

00:32:01,110 --> 00:32:06,540
somehow like next release or sorry I saw

00:32:05,070 --> 00:32:08,280
this I have a question about the

00:32:06,540 --> 00:32:10,620
persistence and they're so successful

00:32:08,280 --> 00:32:12,630
now it persists to edge pace and that's

00:32:10,620 --> 00:32:14,820
all there's a plan to replace HBase with

00:32:12,630 --> 00:32:18,330
arcade or something else's yeah it's

00:32:14,820 --> 00:32:19,620
going to be next release or yeah this is

00:32:18,330 --> 00:32:22,590
a good question

00:32:19,620 --> 00:32:24,870
currently in a particular the Cuba is

00:32:22,590 --> 00:32:27,360
started in HBase and the in the

00:32:24,870 --> 00:32:30,660
meanwhile we are developing the packet

00:32:27,360 --> 00:32:34,440
storage yeah and it has been you know

00:32:30,660 --> 00:32:39,000
stage the status and the the release

00:32:34,440 --> 00:32:43,220
data hasn't been determined because the

00:32:39,000 --> 00:32:43,220
performance is still needed to optimize

00:32:43,250 --> 00:32:52,290
maybe it's an another major release

00:32:47,690 --> 00:32:54,750
country only actress well in in the

00:32:52,290 --> 00:33:00,750
commercial version we have replaced the

00:32:54,750 --> 00:33:04,710
HBase with the Coleman FL format thank

00:33:00,750 --> 00:33:06,420
you yeah is there anything do you have

00:33:04,710 --> 00:33:09,090
any technology for identifying

00:33:06,420 --> 00:33:11,940
correlated dimensions like each

00:33:09,090 --> 00:33:14,250
dimension is expensive is there Oh is

00:33:11,940 --> 00:33:18,270
there any way to analyze the data to say

00:33:14,250 --> 00:33:20,460
you don't need some dimension or just

00:33:18,270 --> 00:33:23,790
ways of reducing the dimensionality of

00:33:20,460 --> 00:33:27,900
the cube based on the data okay let me

00:33:23,790 --> 00:33:31,860
confirm are you asking that does killing

00:33:27,900 --> 00:33:33,800
provider any way to optimize dimensions

00:33:31,860 --> 00:33:38,670
right yeah

00:33:33,800 --> 00:33:42,150
actually killing have a set of rules to

00:33:38,670 --> 00:33:44,520
optimize this I didn't list them on

00:33:42,150 --> 00:33:48,090
today's presentation because that is a

00:33:44,520 --> 00:33:51,030
little technical let me give you some

00:33:48,090 --> 00:33:53,670
example firstly in killing you can

00:33:51,030 --> 00:33:58,980
divide the dimensions into several

00:33:53,670 --> 00:34:01,250
groups yes this is avoid avoid that the

00:33:58,980 --> 00:34:05,880
mutual combinations in too many

00:34:01,250 --> 00:34:08,130
dimensions and in each group you can't

00:34:05,880 --> 00:34:14,640
define some rules for example Hiraki

00:34:08,130 --> 00:34:15,480
who's the nation country the country

00:34:14,640 --> 00:34:20,040
nation

00:34:15,480 --> 00:34:23,340
Sadie region etc and also you can define

00:34:20,040 --> 00:34:27,570
some counter views for example an ID and

00:34:23,340 --> 00:34:32,310
a name they will always appear together

00:34:27,570 --> 00:34:37,290
or or some something else and also you

00:34:32,310 --> 00:34:39,540
can define this some mandatory rules for

00:34:37,290 --> 00:34:43,020
example in your dashboard or you know

00:34:39,540 --> 00:34:46,380
report some conditions are required so

00:34:43,020 --> 00:34:49,860
when you define the cubic you can set

00:34:46,380 --> 00:34:51,900
these these dimensions as mandatory so

00:34:49,860 --> 00:34:54,660
that the killing doesn't calculated

00:34:51,900 --> 00:34:59,640
those conventions that has no that

00:34:54,660 --> 00:35:05,970
dimension yes we provide many rules for

00:34:59,640 --> 00:35:16,200
you to optimize the cube the other

00:35:05,970 --> 00:35:18,590
question yes so I'm wondering so now

00:35:16,200 --> 00:35:21,330
it's coming well with tableau and

00:35:18,590 --> 00:35:24,000
basically we test with power bi the

00:35:21,330 --> 00:35:27,450
Microsoft solution and it only supports

00:35:24,000 --> 00:35:31,860
the direct import something so we have

00:35:27,450 --> 00:35:33,720
to dump all the data to low no and is

00:35:31,860 --> 00:35:36,150
there any other use case like any other

00:35:33,720 --> 00:35:38,640
situations support and many others

00:35:36,150 --> 00:35:43,530
visualization tools are supported except

00:35:38,640 --> 00:35:47,610
for tableau or currently tableau is the

00:35:43,530 --> 00:35:50,700
most popular used VI tools for

00:35:47,610 --> 00:35:53,220
application and for power bi you just

00:35:50,700 --> 00:35:56,310
mentioned the papi I needed to attract

00:35:53,220 --> 00:35:59,970
the data to its server and the tool

00:35:56,310 --> 00:36:02,340
running this but recently probably I'll

00:35:59,970 --> 00:36:05,100
release the a plug-in called the

00:36:02,340 --> 00:36:09,060
director query the req required means

00:36:05,100 --> 00:36:12,480
that it can push down any queries to the

00:36:09,060 --> 00:36:16,580
underlying engine how we a dozen tanida

00:36:12,480 --> 00:36:20,250
to cache the data so we developed the

00:36:16,580 --> 00:36:22,950
implement or driver for power bi direct

00:36:20,250 --> 00:36:27,870
query so the the video that I just

00:36:22,950 --> 00:36:32,060
showed is using that driver direct query

00:36:27,870 --> 00:36:38,490
driver yeah and only with this you can

00:36:32,060 --> 00:36:42,180
to the analytics on the big data set for

00:36:38,490 --> 00:36:44,940
the Pattullo users we also ask you is

00:36:42,180 --> 00:36:50,070
the user to use the director connector

00:36:44,940 --> 00:36:53,870
mode only then let tableau to generator

00:36:50,070 --> 00:36:57,510
the secrets and ascend to optically

00:36:53,870 --> 00:37:01,470
otherwise tableau will send the selected

00:36:57,510 --> 00:37:04,940
star hurry to try to catch all the data

00:37:01,470 --> 00:37:04,940
then it will crush

00:37:07,740 --> 00:37:14,010
do we have any other question okay then

00:37:11,940 --> 00:37:18,570
now I would ask a question because we

00:37:14,010 --> 00:37:21,630
serve time so while we add more columns

00:37:18,570 --> 00:37:24,030
to our dataset the storage space needed

00:37:21,630 --> 00:37:25,710
somehow arises exponentially right

00:37:24,030 --> 00:37:30,690
because we have all these combination of

00:37:25,710 --> 00:37:34,410
the dimensions so right now you just

00:37:30,690 --> 00:37:37,440
store all of them in HBase right do you

00:37:34,410 --> 00:37:38,970
plan to do some like following up on the

00:37:37,440 --> 00:37:42,450
question before you plan to do some

00:37:38,970 --> 00:37:46,280
compression or something in along that

00:37:42,450 --> 00:37:49,650
lines to minimize the storage space or

00:37:46,280 --> 00:37:56,640
do you - do you plan to save everything

00:37:49,650 --> 00:37:59,550
like always firstly we know that for q4

00:37:56,640 --> 00:38:03,660
the dimension if it has many dimensions

00:37:59,550 --> 00:38:06,540
the combination will be many many to

00:38:03,660 --> 00:38:09,089
many and the amount of them many of them

00:38:06,540 --> 00:38:11,700
are useless so it only needed to

00:38:09,089 --> 00:38:17,010
calculate so many dimensional

00:38:11,700 --> 00:38:20,310
combinations so killing two totals three

00:38:17,010 --> 00:38:24,270
we develop the components are called

00:38:20,310 --> 00:38:28,609
cubes and the current pass your query

00:38:24,270 --> 00:38:32,400
history and the two collected the most

00:38:28,609 --> 00:38:36,390
most frequently visited keyboards and

00:38:32,400 --> 00:38:38,970
then optimize the cube so the cue ball

00:38:36,390 --> 00:38:41,640
will be much smaller than before but

00:38:38,970 --> 00:38:45,119
it's a performance that can be kept at

00:38:41,640 --> 00:38:49,500
the same level yeah in the future we

00:38:45,119 --> 00:38:53,490
will keep optimize this in this way to

00:38:49,500 --> 00:39:00,180
make the cube more acceptable for the

00:38:53,490 --> 00:39:04,140
users okay then if there is no other

00:39:00,180 --> 00:39:08,240
question we can thank shelfing again for

00:39:04,140 --> 00:39:08,240

YouTube URL: https://www.youtube.com/watch?v=Yc5wzRB4Ot8


