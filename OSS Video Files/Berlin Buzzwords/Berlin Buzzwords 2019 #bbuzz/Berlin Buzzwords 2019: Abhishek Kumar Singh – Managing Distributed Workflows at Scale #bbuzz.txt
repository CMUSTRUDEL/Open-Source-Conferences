Title: Berlin Buzzwords 2019: Abhishek Kumar Singh – Managing Distributed Workflows at Scale #bbuzz
Publication date: 2019-06-28
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Abhishek Kumar Singh talking about "Managing Distributed Workflows at Scale - Kubernetes Jobs in Action".

Kubernetes has been widely accepted as the de-facto deployment orchestrator for managing and scaling containers in the cloud. While it's capabilities have been well acknowledged in the deployment space, its ability to manage business-specific workflows is yet to see a wide application.

This talk gives an insight into how we at Unbxd used 'Kubernetes Jobs' to build a Workflow Orchestration Engine that helps to configure and manage complex sequence of processes where the output of each step is used as an input for the next node. This architecture can also use existing microservices running on any platform as a node in the workflow and the data routing intelligence remains with the workflow orchestration layer (in the form of fault-tolerant DAGs).   

This dynamic and configurable workflow also helps in scaling the architecture well, as the inter-node data flow is controlled by the Orchestrator from within a Kubernetes pod (using a distributed message queue).

Read more:
https://2019.berlinbuzzwords.de/19/session/managing-distributed-workflows-scale-kubernetes-jobs-action

About Abhishek Kumar Singh:
https://2019.berlinbuzzwords.de/users/abhishek-kumar-singh

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,840 --> 00:00:09,719
good morning everyone good afternoon

00:00:07,980 --> 00:00:12,910
everyone thank you for joining me today

00:00:09,719 --> 00:00:16,390
for my talk so I'll be talking about

00:00:12,910 --> 00:00:18,850
his jobs and distributed you know

00:00:16,390 --> 00:00:28,090
workflow orchestration engine that we

00:00:18,850 --> 00:00:31,660
built ad unboxed so so starting off so I

00:00:28,090 --> 00:00:36,430
work at unboxed which is an e-commerce

00:00:31,660 --> 00:00:39,070
discovery platform so so basically you

00:00:36,430 --> 00:00:41,890
know towards our approach to solving

00:00:39,070 --> 00:00:44,500
these complex ecommerce problems we have

00:00:41,890 --> 00:00:47,739
come a long way with these products so

00:00:44,500 --> 00:00:51,640
Site Search browse recommendations and

00:00:47,739 --> 00:00:54,160
product information management I work in

00:00:51,640 --> 00:00:56,800
the enterprise products team where we

00:00:54,160 --> 00:00:58,780
bring Big Data information retrieval

00:00:56,800 --> 00:01:01,180
automation and intuitive interfaces

00:00:58,780 --> 00:01:05,009
together to help our clients experience

00:01:01,180 --> 00:01:07,180
the power of scale unbox PIM is a recent

00:01:05,009 --> 00:01:10,360
development is a recent product that we

00:01:07,180 --> 00:01:12,430
recently launched so you know Maj PIM is

00:01:10,360 --> 00:01:14,800
a one-stop solution to collect manage

00:01:12,430 --> 00:01:17,050
and enrich ecommerce data and

00:01:14,800 --> 00:01:19,539
distributed out to e-commerce channels

00:01:17,050 --> 00:01:21,580
and partners an important aspect of

00:01:19,539 --> 00:01:24,729
inbox PIM is its workflow automation

00:01:21,580 --> 00:01:28,569
engine and today's that presentation is

00:01:24,729 --> 00:01:30,250
going to be all about that so this is

00:01:28,569 --> 00:01:34,060
how my agenda looks like I'll give an

00:01:30,250 --> 00:01:35,170
overview of the workflows and then we'll

00:01:34,060 --> 00:01:36,849
talk about the objectives in the

00:01:35,170 --> 00:01:38,740
components of the workflow castration

00:01:36,849 --> 00:01:40,240
engine that we have built and the final

00:01:38,740 --> 00:01:42,399
architecture of course and then we'll

00:01:40,240 --> 00:01:43,810
move on to the kubernetes and its

00:01:42,399 --> 00:01:45,130
features and which features have been

00:01:43,810 --> 00:01:49,390
used in which components of the

00:01:45,130 --> 00:01:52,330
orchestration engine so beginning with

00:01:49,390 --> 00:01:54,190
in with the overview of the workflow so

00:01:52,330 --> 00:01:55,989
a typical workflow in the e-commerce

00:01:54,190 --> 00:01:58,780
ingestion pipeline looks something like

00:01:55,989 --> 00:02:01,780
this here I have taken the example from

00:01:58,780 --> 00:02:03,880
an e-commerce brand aggregator so say

00:02:01,780 --> 00:02:06,459
there's this guy who has his catalog

00:02:03,880 --> 00:02:08,470
coming from a lot of brands and then he

00:02:06,459 --> 00:02:09,849
and his end goal is to basically you

00:02:08,470 --> 00:02:13,420
know publish it out to one or more

00:02:09,849 --> 00:02:15,700
ecommerce platforms so so this is how

00:02:13,420 --> 00:02:19,090
his flow actually looks like he gets his

00:02:15,700 --> 00:02:21,190
data from s3 into an Box Pam and here he

00:02:19,090 --> 00:02:23,320
filters it out based on a particular

00:02:21,190 --> 00:02:24,370
brand based on the filters it branches

00:02:23,320 --> 00:02:26,099
out into two different kind of

00:02:24,370 --> 00:02:27,989
operations that he decides

00:02:26,099 --> 00:02:30,870
and you know as we can see on one kind

00:02:27,989 --> 00:02:32,189
of a product he applies this operation

00:02:30,870 --> 00:02:34,469
called modify pricing and then

00:02:32,189 --> 00:02:37,069
watermarks images and so on so you know

00:02:34,469 --> 00:02:40,859
the extent to which these workflows can

00:02:37,069 --> 00:02:43,530
expand can fan-out is unlimited and it

00:02:40,859 --> 00:02:46,139
has to be highly configurable so all

00:02:43,530 --> 00:02:48,659
that we need here is an automation

00:02:46,139 --> 00:02:52,799
engine and engine that lets you

00:02:48,659 --> 00:02:56,150
configure your workflow and and lets you

00:02:52,799 --> 00:02:59,939
automate it no matter what the scale is

00:02:56,150 --> 00:03:02,340
so so what we have to build is a

00:02:59,939 --> 00:03:03,659
workflow automation engine so these are

00:03:02,340 --> 00:03:05,790
the primary objectives we'll be looking

00:03:03,659 --> 00:03:08,129
at it should be scalable and fault

00:03:05,790 --> 00:03:10,260
tolerant it should enable time or even

00:03:08,129 --> 00:03:14,449
based triggers and should also expose

00:03:10,260 --> 00:03:14,449
rest api is for workflow configuration

00:03:14,750 --> 00:03:21,750
the components of the engine are going

00:03:17,669 --> 00:03:25,650
to be a node data streams the workflow

00:03:21,750 --> 00:03:27,719
itself event listeners scheduler and the

00:03:25,650 --> 00:03:29,549
orchestrator will have a look at each

00:03:27,719 --> 00:03:32,129
these each of these components in detail

00:03:29,549 --> 00:03:34,500
in the coming slides so starting off a

00:03:32,129 --> 00:03:36,810
node can be seen as the most basic

00:03:34,500 --> 00:03:39,449
entity in the workflow every node

00:03:36,810 --> 00:03:42,329
contains a special business logic that

00:03:39,449 --> 00:03:44,849
has been applied on a finite stream of

00:03:42,329 --> 00:03:47,040
data it receives a node constitutes of

00:03:44,849 --> 00:03:50,069
four logical steps internally so

00:03:47,040 --> 00:03:51,479
configure takes the takes care of

00:03:50,069 --> 00:03:54,449
converting the configurations it

00:03:51,479 --> 00:03:57,269
receives into a finite stream of data

00:03:54,449 --> 00:04:00,060
and passes it on to the execute node

00:03:57,269 --> 00:04:03,599
execute has a business logic which is to

00:04:00,060 --> 00:04:05,819
be applied on this data and then and

00:04:03,599 --> 00:04:07,919
yeah and and the business logic which is

00:04:05,819 --> 00:04:09,989
there inside execute can either be an

00:04:07,919 --> 00:04:11,819
in-memory processing of the data or it

00:04:09,989 --> 00:04:14,519
can simply be hitting external API is

00:04:11,819 --> 00:04:16,739
the end goal is to transform the set of

00:04:14,519 --> 00:04:18,810
data that it receives and again the kind

00:04:16,739 --> 00:04:22,800
of data that is being moved across these

00:04:18,810 --> 00:04:24,570
stages of a node is finite this is this

00:04:22,800 --> 00:04:25,740
is bounded data basically it is some

00:04:24,570 --> 00:04:28,500
kind of batch processing is going on

00:04:25,740 --> 00:04:30,990
over here the third step is init output

00:04:28,500 --> 00:04:32,940
metadata Varian you know the results

00:04:30,990 --> 00:04:35,310
which have been achieved in the execute

00:04:32,940 --> 00:04:36,810
state is being converted into some

00:04:35,310 --> 00:04:39,449
metadata I'll explain in the next slide

00:04:36,810 --> 00:04:40,040
why metadata and the finalized step is

00:04:39,449 --> 00:04:42,740
where you

00:04:40,040 --> 00:04:45,010
all kind of cleanup happens and this is

00:04:42,740 --> 00:04:47,120
how if a workflow has you know a

00:04:45,010 --> 00:04:49,520
particular sequence this is how the

00:04:47,120 --> 00:04:52,010
dataflow looks like the node from one

00:04:49,520 --> 00:04:55,430
gets transferred to the next one talking

00:04:52,010 --> 00:04:57,230
about the i/o data streams so the

00:04:55,430 --> 00:05:00,860
communication between two nodes happens

00:04:57,230 --> 00:05:02,270
in the form of metadata so as we know in

00:05:00,860 --> 00:05:04,640
a work flow the data which flows between

00:05:02,270 --> 00:05:06,290
two nodes has to be you know one node

00:05:04,640 --> 00:05:07,520
may filter or simply process on a

00:05:06,290 --> 00:05:11,060
particular set of data and that gets

00:05:07,520 --> 00:05:14,450
passed on to the next node so here what

00:05:11,060 --> 00:05:16,970
we do is we checkpoint this data which

00:05:14,450 --> 00:05:19,040
you know gets output from a particular

00:05:16,970 --> 00:05:20,750
node and this checkpoint thing if you

00:05:19,040 --> 00:05:23,090
start doing it for millions of data it

00:05:20,750 --> 00:05:25,460
gets you know it doesn't really scale so

00:05:23,090 --> 00:05:27,830
what what we have designed is that you

00:05:25,460 --> 00:05:30,140
know every node should output a set of

00:05:27,830 --> 00:05:33,980
metadata and every node will have a

00:05:30,140 --> 00:05:36,320
logic to basically convert the metadata

00:05:33,980 --> 00:05:39,770
into the actual set of data that it

00:05:36,320 --> 00:05:41,180
should actually be processing on there

00:05:39,770 --> 00:05:43,520
is another advantage with parsing on

00:05:41,180 --> 00:05:45,290
metadata is that it can be check pointed

00:05:43,520 --> 00:05:46,640
as well in case you know some kind of

00:05:45,290 --> 00:05:49,010
hardware failure or something happen and

00:05:46,640 --> 00:05:50,720
you know your workflow just wakes up it

00:05:49,010 --> 00:05:55,700
can simply start up from the last node

00:05:50,720 --> 00:05:57,920
that had executed so the output data

00:05:55,700 --> 00:05:59,840
streams is encoded in the third step

00:05:57,920 --> 00:06:02,330
which is the init output metadata steps

00:05:59,840 --> 00:06:06,860
of a node and it is decoded back in the

00:06:02,330 --> 00:06:09,710
configure step now so after we have the

00:06:06,860 --> 00:06:11,270
grasp of the basic concepts involved in

00:06:09,710 --> 00:06:14,690
this workflow the node and the streams

00:06:11,270 --> 00:06:16,040
we can configure overflow so when we

00:06:14,690 --> 00:06:18,590
configure a workflow these are the

00:06:16,040 --> 00:06:21,470
things which are needed a directed

00:06:18,590 --> 00:06:23,030
acyclic graph which is you know we

00:06:21,470 --> 00:06:25,280
configure several nodes and then we

00:06:23,030 --> 00:06:27,490
define a particular sequence it there

00:06:25,280 --> 00:06:30,080
can be fan outs and fan ins there so

00:06:27,490 --> 00:06:32,270
that's what a dag structure is all about

00:06:30,080 --> 00:06:34,880
a workflow should have some trigger

00:06:32,270 --> 00:06:36,410
details based on what kind of a trigger

00:06:34,880 --> 00:06:39,110
should a workflow act should actually

00:06:36,410 --> 00:06:41,630
start it should you know it can be an

00:06:39,110 --> 00:06:45,200
event listener or it can be a scheduler

00:06:41,630 --> 00:06:49,330
and these specific node configurations

00:06:45,200 --> 00:06:51,680
so you know it can be something like

00:06:49,330 --> 00:06:53,780
there is a node which works on the

00:06:51,680 --> 00:06:54,120
pricing scales the pricing out there you

00:06:53,780 --> 00:06:55,830
just

00:06:54,120 --> 00:06:59,010
on the thresholds these can be static

00:06:55,830 --> 00:07:01,230
configurations so each node operates on

00:06:59,010 --> 00:07:03,540
some data there can be two kind of data

00:07:01,230 --> 00:07:04,740
the static data is the configurations

00:07:03,540 --> 00:07:06,600
that you pass while configuring the

00:07:04,740 --> 00:07:07,770
workflow and the dynamic data is the

00:07:06,600 --> 00:07:12,449
data that has been passed on by the

00:07:07,770 --> 00:07:13,889
previous node onto it and this is the

00:07:12,449 --> 00:07:15,240
last in the most important component of

00:07:13,889 --> 00:07:18,180
the workflow orchestration engine is the

00:07:15,240 --> 00:07:20,880
orchestrator itself so the orchestrator

00:07:18,180 --> 00:07:25,229
has some responsibilities it has to

00:07:20,880 --> 00:07:26,940
react whenever a time trigger or a event

00:07:25,229 --> 00:07:29,220
listener lets it know that a particular

00:07:26,940 --> 00:07:30,780
workflow has to be started it goes to

00:07:29,220 --> 00:07:32,610
the directed acyclic graph which has

00:07:30,780 --> 00:07:35,130
been configured inside the workflow and

00:07:32,610 --> 00:07:37,139
starts the particular nodes it also

00:07:35,130 --> 00:07:39,510
exposes some API is out through which

00:07:37,139 --> 00:07:41,729
you know this triggers can be informed

00:07:39,510 --> 00:07:43,260
and when the nodes are executing they

00:07:41,729 --> 00:07:45,780
can checkpoint their information we'll

00:07:43,260 --> 00:07:48,360
be looking at that information that

00:07:45,780 --> 00:07:49,979
detail in the coming slides a workflow

00:07:48,360 --> 00:07:51,990
Orchestrator is complemented by a

00:07:49,979 --> 00:07:54,270
workflow meta store which can be an

00:07:51,990 --> 00:07:56,010
outside service which has all the

00:07:54,270 --> 00:07:58,910
configurations related to all the

00:07:56,010 --> 00:08:00,810
workflows we have ever configured and a

00:07:58,910 --> 00:08:02,220
persistence layer of course where all

00:08:00,810 --> 00:08:05,600
this you know all the check pointed

00:08:02,220 --> 00:08:07,979
States and all these information is kept

00:08:05,600 --> 00:08:09,570
well after having discussed all the

00:08:07,979 --> 00:08:11,820
components all the building blocks

00:08:09,570 --> 00:08:14,130
involved in this architecture let's move

00:08:11,820 --> 00:08:16,680
on to the actual architecture there so

00:08:14,130 --> 00:08:19,919
the orchestrator receives an event

00:08:16,680 --> 00:08:22,410
through one of the triggers and goes

00:08:19,919 --> 00:08:24,060
through the dag structure of the

00:08:22,410 --> 00:08:26,639
workflow and starts a particular node

00:08:24,060 --> 00:08:28,590
the node executes each of its internal

00:08:26,639 --> 00:08:30,120
steps and keeps on checkpointing the

00:08:28,590 --> 00:08:32,580
data back to the workflow Orchestrator

00:08:30,120 --> 00:08:36,599
which persists it's in 2 DB after each

00:08:32,580 --> 00:08:39,360
of its steps the orchestrator basically

00:08:36,599 --> 00:08:42,300
keep the orchestrator here only stands

00:08:39,360 --> 00:08:44,190
as a particular service which starts a

00:08:42,300 --> 00:08:46,350
particular node and keeps on receiving

00:08:44,190 --> 00:08:48,270
events back from the node after the

00:08:46,350 --> 00:08:51,240
finalized step of any node it knows what

00:08:48,270 --> 00:08:53,160
next node has to be started and one

00:08:51,240 --> 00:08:54,870
important aspect that I hadn't discussed

00:08:53,160 --> 00:08:57,570
till now here are the external micro

00:08:54,870 --> 00:08:59,279
services so then the execute step of the

00:08:57,570 --> 00:09:01,800
node which actually is supposed to have

00:08:59,279 --> 00:09:04,290
all the business logic in it may simply

00:09:01,800 --> 00:09:05,880
leverage all the external API is you

00:09:04,290 --> 00:09:07,800
already have out there all the business

00:09:05,880 --> 00:09:10,200
logic or the processing logic you have

00:09:07,800 --> 00:09:13,260
in your system the workflow will help

00:09:10,200 --> 00:09:15,360
you in automating this so you know so

00:09:13,260 --> 00:09:16,709
this micro-services find a very

00:09:15,360 --> 00:09:19,500
important place in this architecture as

00:09:16,709 --> 00:09:22,170
well now you know when when I have

00:09:19,500 --> 00:09:24,779
actually described the particular flow

00:09:22,170 --> 00:09:27,120
and the architecture of this system here

00:09:24,779 --> 00:09:29,850
I'll go on to the technology which has

00:09:27,120 --> 00:09:33,570
been used to implement this so yes

00:09:29,850 --> 00:09:36,269
kubernetes so kubernetes as we all know

00:09:33,570 --> 00:09:38,370
is a widely used container Orchestrator

00:09:36,269 --> 00:09:40,579
it's open source and it believes in

00:09:38,370 --> 00:09:43,110
managing applications and not machines

00:09:40,579 --> 00:09:45,300
deployment is a very common use case for

00:09:43,110 --> 00:09:47,040
kubernetes with kubernetes deployment

00:09:45,300 --> 00:09:49,050
rockets and controllers keep monitoring

00:09:47,040 --> 00:09:49,769
your application and in case it goes

00:09:49,050 --> 00:09:51,990
down or something

00:09:49,769 --> 00:09:54,329
kubernetes does everything it can to

00:09:51,990 --> 00:09:56,220
auto heal so it will spin another

00:09:54,329 --> 00:09:59,220
container up and recover for itself

00:09:56,220 --> 00:10:00,750
here deployment is not just about the

00:09:59,220 --> 00:10:02,430
initial launching of the container but

00:10:00,750 --> 00:10:04,260
it's something much bigger in kubernetes

00:10:02,430 --> 00:10:06,720
apart from deployment

00:10:04,260 --> 00:10:10,560
it also lets us do much more that we'll

00:10:06,720 --> 00:10:13,290
look up in the next slides kubernetes

00:10:10,560 --> 00:10:15,839
jobs so kubernetes jobs is a special

00:10:13,290 --> 00:10:17,760
feature which lets us run batch jobs in

00:10:15,839 --> 00:10:19,410
covenant is cluster it differs from

00:10:17,760 --> 00:10:21,720
other controller objects like that of

00:10:19,410 --> 00:10:24,320
deployment because in equipment

00:10:21,720 --> 00:10:26,790
kubernetes environment because it is

00:10:24,320 --> 00:10:29,100
managed as a task that has to run till

00:10:26,790 --> 00:10:30,390
completion not like a deployment where

00:10:29,100 --> 00:10:32,790
it has to be continuously running

00:10:30,390 --> 00:10:34,680
so here kubernetes make sure that the

00:10:32,790 --> 00:10:37,589
kubernetes job goes till the completion

00:10:34,680 --> 00:10:39,000
state in case some kind of restarts

00:10:37,589 --> 00:10:42,029
happen or some kind of hardware failure

00:10:39,000 --> 00:10:43,709
happens kubernetes make sure that all

00:10:42,029 --> 00:10:46,529
the jobs which were running and have not

00:10:43,709 --> 00:10:48,930
attained of completed or a failed state

00:10:46,529 --> 00:10:51,329
or he started again so it gets our

00:10:48,930 --> 00:10:53,339
responsibility to make sure that the

00:10:51,329 --> 00:10:56,550
jobs know that it can be restarted in it

00:10:53,339 --> 00:10:58,500
has to you know check start resume from

00:10:56,550 --> 00:11:00,270
a last check pointed place so it's

00:10:58,500 --> 00:11:03,899
useful for large computations and batch

00:11:00,270 --> 00:11:05,430
oriented tasks and also you know it

00:11:03,899 --> 00:11:07,709
falls it's fall torrance because

00:11:05,430 --> 00:11:09,600
kubernetes meant make sure that it goes

00:11:07,709 --> 00:11:10,949
till the completion state another

00:11:09,600 --> 00:11:13,079
feature that we have used in this

00:11:10,949 --> 00:11:15,740
architecture our kubernetes cron jobs

00:11:13,079 --> 00:11:18,750
these are essentially kubernetes jobs

00:11:15,740 --> 00:11:20,940
with the same fault tolerance but with

00:11:18,750 --> 00:11:22,950
scheduler configurations so basically

00:11:20,940 --> 00:11:25,290
you can specify the same job

00:11:22,950 --> 00:11:27,540
configurations with with the specific

00:11:25,290 --> 00:11:29,580
scheduler configuration which is of the

00:11:27,540 --> 00:11:31,740
format of a Linux crontab expression and

00:11:29,580 --> 00:11:36,560
kubernetes will make sure that that

00:11:31,740 --> 00:11:40,290
point of time this job is started so and

00:11:36,560 --> 00:11:44,790
everything else is exactly similar as

00:11:40,290 --> 00:11:47,220
covenant is jobs and now we will see

00:11:44,790 --> 00:11:48,660
which feature like the kubernetes jobs

00:11:47,220 --> 00:11:50,580
in the cron jobs have been used in what

00:11:48,660 --> 00:11:53,340
part of the architecture so the event

00:11:50,580 --> 00:11:56,790
listener event listener is a managed

00:11:53,340 --> 00:12:00,210
kubernetes pod it's a it's a deployment

00:11:56,790 --> 00:12:02,220
object basically so here you know a set

00:12:00,210 --> 00:12:04,260
of jobs are running long-running jobs

00:12:02,220 --> 00:12:05,940
are running which have a Kafka consumer

00:12:04,260 --> 00:12:07,740
running inside that and that is

00:12:05,940 --> 00:12:09,870
listening on a particular topic which

00:12:07,740 --> 00:12:12,030
has every event generated in your system

00:12:09,870 --> 00:12:13,380
whatsoever to make this kind of a system

00:12:12,030 --> 00:12:15,780
work we also need to have a particular

00:12:13,380 --> 00:12:20,610
topic where we are ingesting every

00:12:15,780 --> 00:12:22,530
special event in our system the Shuler

00:12:20,610 --> 00:12:25,230
as you all must have guessed already

00:12:22,530 --> 00:12:27,240
it's a covenant a scheduled job the

00:12:25,230 --> 00:12:29,820
responsibility of this Shatila is to is

00:12:27,240 --> 00:12:32,250
this kind of a job is to just wake up

00:12:29,820 --> 00:12:35,940
whenever kubernetes lets it know that it

00:12:32,250 --> 00:12:38,580
has to wake up and start a particular

00:12:35,940 --> 00:12:39,780
workflow it lets it let's the

00:12:38,580 --> 00:12:41,460
orchestrator know that this particular

00:12:39,780 --> 00:12:42,900
workflow has to start and the

00:12:41,460 --> 00:12:44,910
orchestrator knows everything else about

00:12:42,900 --> 00:12:47,610
the workflow so it it will take care of

00:12:44,910 --> 00:12:49,020
you know executing the nodes inside the

00:12:47,610 --> 00:12:52,950
workflow in the same sequence as we

00:12:49,020 --> 00:12:55,650
define anything that the node whenever

00:12:52,950 --> 00:12:59,090
an Orchestrator starts a particular step

00:12:55,650 --> 00:13:01,890
in the workflow that executes as a

00:12:59,090 --> 00:13:04,890
kubernetes run to completion job the

00:13:01,890 --> 00:13:07,260
kubernetes bad job so here it will be

00:13:04,890 --> 00:13:09,330
our responsibility that each node each

00:13:07,260 --> 00:13:11,490
new business logic that we add to this

00:13:09,330 --> 00:13:15,480
architecture we have to deploy that as a

00:13:11,490 --> 00:13:16,890
docker image and when we are configuring

00:13:15,480 --> 00:13:19,380
we have to give the name of the docker

00:13:16,890 --> 00:13:20,580
image that set and the orchestrator will

00:13:19,380 --> 00:13:22,530
make sure that the docker image has

00:13:20,580 --> 00:13:24,420
started as the kubernetes job and then

00:13:22,530 --> 00:13:26,130
it will be our cube and it is the

00:13:24,420 --> 00:13:30,140
responsibility of the kubernetes to make

00:13:26,130 --> 00:13:32,640
sure that it runs till completion and

00:13:30,140 --> 00:13:34,110
yes the orchestrator and every other

00:13:32,640 --> 00:13:36,180
micro services are running

00:13:34,110 --> 00:13:39,000
as covenant esports so they are highly

00:13:36,180 --> 00:13:40,709
available so as I explained if anything

00:13:39,000 --> 00:13:44,459
happens to them covenants will make sure

00:13:40,709 --> 00:13:47,640
that they get up the orchestrator is

00:13:44,459 --> 00:13:50,300
special in one aspect that all the

00:13:47,640 --> 00:13:51,510
triggers that it receives from the

00:13:50,300 --> 00:13:53,700
scheduler or

00:13:51,510 --> 00:13:55,860
event listener they all go to a queue

00:13:53,700 --> 00:13:57,959
rather than you know being served then

00:13:55,860 --> 00:14:00,029
in there for all obvious reasons that

00:13:57,959 --> 00:14:01,680
even if it's you know just in case down

00:14:00,029 --> 00:14:05,279
for even a second or something no

00:14:01,680 --> 00:14:08,550
trigger should be missed out so that's

00:14:05,279 --> 00:14:10,620
our is now as you all must have guessed

00:14:08,550 --> 00:14:13,649
by now the orchestrator is a code and

00:14:10,620 --> 00:14:15,959
that controls kubernetes so there is

00:14:13,649 --> 00:14:17,760
some library that we have used the

00:14:15,959 --> 00:14:20,510
fabricate kubernetes client this lets

00:14:17,760 --> 00:14:23,760
you control kubernetes through your code

00:14:20,510 --> 00:14:25,410
so you can do everything that you can do

00:14:23,760 --> 00:14:27,690
with kubernetes rest api is using this

00:14:25,410 --> 00:14:29,279
library so this is nothing but a wrapper

00:14:27,690 --> 00:14:33,360
over the kubernetes rest lance where

00:14:29,279 --> 00:14:34,980
it's works really well and that's the

00:14:33,360 --> 00:14:37,529
dependency out there and yes

00:14:34,980 --> 00:14:40,290
instead of yml files it uses a well

00:14:37,529 --> 00:14:42,779
structured strictly type java objects to

00:14:40,290 --> 00:14:45,000
configure your jobs it also exposes into

00:14:42,779 --> 00:14:47,220
F interfaces to filter filter your jobs

00:14:45,000 --> 00:14:50,970
and pods based on a particular label or

00:14:47,220 --> 00:14:52,410
a metadata basically everything that you

00:14:50,970 --> 00:14:55,529
could have done using kubernetes

00:14:52,410 --> 00:14:59,660
command line or at rest EPA's so that's

00:14:55,529 --> 00:15:02,519
how it is now what happens at scale so

00:14:59,660 --> 00:15:04,199
this architecture was defined designed

00:15:02,519 --> 00:15:05,399
for scale because we were using

00:15:04,199 --> 00:15:08,519
something else previously and that

00:15:05,399 --> 00:15:11,970
didn't use to scale that well and wasn't

00:15:08,519 --> 00:15:13,649
this much configurable either so if the

00:15:11,970 --> 00:15:14,310
number of workflows executing in

00:15:13,649 --> 00:15:16,850
parallel

00:15:14,310 --> 00:15:19,430
increases drastically it directly

00:15:16,850 --> 00:15:21,269
impacts the number of nodes for which

00:15:19,430 --> 00:15:23,699
resources should be allotted by

00:15:21,269 --> 00:15:25,949
kubernetes scaling kubernetes cluster is

00:15:23,699 --> 00:15:27,990
as easy as adding another instance to

00:15:25,949 --> 00:15:29,850
the cluster now resource allocation can

00:15:27,990 --> 00:15:33,000
be planned based on the expected number

00:15:29,850 --> 00:15:36,120
of parallel workflows running at any

00:15:33,000 --> 00:15:39,240
point of given time so it's pretty easy

00:15:36,120 --> 00:15:41,940
for us another aspect that I would want

00:15:39,240 --> 00:15:43,890
to highlight here is that you know

00:15:41,940 --> 00:15:45,990
scaling by rate limiting so something

00:15:43,890 --> 00:15:47,730
that we learned over the years that you

00:15:45,990 --> 00:15:50,100
know

00:15:47,730 --> 00:15:51,959
in case you experience a burst in

00:15:50,100 --> 00:15:53,850
traffic or a high traffic all of a

00:15:51,959 --> 00:15:55,500
sudden you might be tempted to scale

00:15:53,850 --> 00:15:58,529
your processing units your metal weighs

00:15:55,500 --> 00:16:01,320
out but you know what happens is this

00:15:58,529 --> 00:16:03,120
also exerts a higher amount of first on

00:16:01,320 --> 00:16:07,470
your database or your persistence layer

00:16:03,120 --> 00:16:09,180
or the downstream micro-services now you

00:16:07,470 --> 00:16:11,130
might say that you will also use kale

00:16:09,180 --> 00:16:13,019
the databases out but in real primary

00:16:11,130 --> 00:16:15,480
real-life practice what we have observed

00:16:13,019 --> 00:16:17,880
is that scaling out a database might be

00:16:15,480 --> 00:16:20,010
pretty easy scale it back scaling it

00:16:17,880 --> 00:16:22,380
back down is not so easy at all so you

00:16:20,010 --> 00:16:26,339
know in that case scaling my rate

00:16:22,380 --> 00:16:28,440
limiting works really well so here what

00:16:26,339 --> 00:16:30,839
you actually do is you make sure that

00:16:28,440 --> 00:16:33,449
the middle a middleware through which

00:16:30,839 --> 00:16:35,519
you know each request has to go to has

00:16:33,449 --> 00:16:37,470
to be throttled at a particular

00:16:35,519 --> 00:16:39,420
parallelism say that can be 5 and out of

00:16:37,470 --> 00:16:41,730
6 and here's something so you know that

00:16:39,420 --> 00:16:44,070
will ensure that even in the worst cases

00:16:41,730 --> 00:16:45,870
in the highest cases of bursts the

00:16:44,070 --> 00:16:48,300
actual number of parallel requests going

00:16:45,870 --> 00:16:50,399
on to your final persistence layers will

00:16:48,300 --> 00:16:53,160
be will be throttled at a given my

00:16:50,399 --> 00:16:55,470
maximum so kubernetes helps you in rate

00:16:53,160 --> 00:16:57,870
limiting the processing layer as we can

00:16:55,470 --> 00:17:00,060
specify the upper cap on the number of

00:16:57,870 --> 00:17:02,880
pods or the resources that can be

00:17:00,060 --> 00:17:04,620
running parallely while every new node

00:17:02,880 --> 00:17:06,360
will be queued for future when the

00:17:04,620 --> 00:17:07,860
resource will be available thereby

00:17:06,360 --> 00:17:09,240
limiting any kind of burst on the

00:17:07,860 --> 00:17:13,410
underlying DB of the micro services

00:17:09,240 --> 00:17:16,040
layers being used by the nodes and some

00:17:13,410 --> 00:17:19,079
good practices that we learned while

00:17:16,040 --> 00:17:20,939
implementing this the orchestration

00:17:19,079 --> 00:17:22,919
engine should be made platform agnostic

00:17:20,939 --> 00:17:26,100
I'm not really sure platform is the

00:17:22,919 --> 00:17:28,199
right word here or not but what I mean

00:17:26,100 --> 00:17:30,390
is that the orchestration engine should

00:17:28,199 --> 00:17:31,350
be written in a way that today if it is

00:17:30,390 --> 00:17:33,419
working with the kubernetes cluster

00:17:31,350 --> 00:17:36,260
tomorrow it can be working with other

00:17:33,419 --> 00:17:40,200
kind of other kind of container

00:17:36,260 --> 00:17:42,260
orchestrators as well so so basically we

00:17:40,200 --> 00:17:44,730
should keep in mind of this fact

00:17:42,260 --> 00:17:47,940
cleaning up the pods in kubernetes so

00:17:44,730 --> 00:17:51,179
you know so what happens is when a job

00:17:47,940 --> 00:17:52,530
completes no more pods are created but

00:17:51,179 --> 00:17:54,540
the pods are not deleted either

00:17:52,530 --> 00:17:56,309
kubernetes keeps them around so that you

00:17:54,540 --> 00:17:58,679
can still view the logs of completed

00:17:56,309 --> 00:18:00,720
jobs and check for errors warnings or

00:17:58,679 --> 00:18:01,530
other diagnostic outputs the job output

00:18:00,720 --> 00:18:03,450
also remain

00:18:01,530 --> 00:18:06,120
after it is completed so that you can

00:18:03,450 --> 00:18:09,000
view the status but in our case we will

00:18:06,120 --> 00:18:10,680
be anyways be managing the states of the

00:18:09,000 --> 00:18:13,860
nodes in much more excruciating detail

00:18:10,680 --> 00:18:15,810
and much more granular level so these go

00:18:13,860 --> 00:18:17,700
states only take up resources inside the

00:18:15,810 --> 00:18:20,580
kubernetes cluster it's better to delete

00:18:17,700 --> 00:18:22,890
them so you can have a look at the

00:18:20,580 --> 00:18:25,230
kubernetes TTL controllers you know

00:18:22,890 --> 00:18:26,970
that's how the configuration goes inside

00:18:25,230 --> 00:18:29,310
the job configuration that will you know

00:18:26,970 --> 00:18:31,590
make sure that the jobs are cleaned up

00:18:29,310 --> 00:18:33,360
and yes you'll have to back up your logs

00:18:31,590 --> 00:18:35,570
before the pods are deleted otherwise

00:18:33,360 --> 00:18:39,630
you'll be losing all the logs out there

00:18:35,570 --> 00:18:42,330
and yes as I have described in the

00:18:39,630 --> 00:18:44,280
architecture slide a node may simply

00:18:42,330 --> 00:18:45,930
rely on other micro services or an

00:18:44,280 --> 00:18:48,120
existing data pipeline for its data

00:18:45,930 --> 00:18:49,980
processing this helps in offloading the

00:18:48,120 --> 00:18:52,260
responsibility to an external system

00:18:49,980 --> 00:18:54,300
well the node on the other hand make

00:18:52,260 --> 00:18:56,460
sure that the synchronous process is

00:18:54,300 --> 00:18:58,230
completed and let's the orchestrator

00:18:56,460 --> 00:19:00,870
know about the same and the next nodes

00:18:58,230 --> 00:19:04,460
can be triggered so just in case you

00:19:00,870 --> 00:19:06,510
have some massively parallel jobs this

00:19:04,460 --> 00:19:08,490
node the code that you've written is

00:19:06,510 --> 00:19:10,500
inside the node just that is not going

00:19:08,490 --> 00:19:12,420
to help you out you should you know take

00:19:10,500 --> 00:19:14,190
help of some external engines and yes

00:19:12,420 --> 00:19:16,530
the node can be you know checking out

00:19:14,190 --> 00:19:19,200
and some on some metadata that you might

00:19:16,530 --> 00:19:22,370
have devised in your system and it will

00:19:19,200 --> 00:19:26,160
help you in sequential icing the process

00:19:22,370 --> 00:19:27,930
that's my team at unboxed and thank you

00:19:26,160 --> 00:19:30,110
very much you can get in touch with me

00:19:27,930 --> 00:19:33,650
on my email id or in my Twitter handle

00:19:30,110 --> 00:19:36,420
do we have time for questions

00:19:33,650 --> 00:19:42,960
there is actually time one minute for

00:19:36,420 --> 00:19:45,740
questions so questions in the room

00:19:42,960 --> 00:19:45,740
there's one

00:19:49,840 --> 00:19:55,760
hi thanks for your talk

00:19:52,610 --> 00:19:58,040
one question I had what was what is the

00:19:55,760 --> 00:20:02,450
benefit of your system against something

00:19:58,040 --> 00:20:05,810
like Apache airflow or Luigi so as far

00:20:02,450 --> 00:20:08,210
as I know airflow does not give you so

00:20:05,810 --> 00:20:10,100
we have used air flows in the past for

00:20:08,210 --> 00:20:13,430
managing you know a small scale or you

00:20:10,100 --> 00:20:14,780
know a very hard coded workflows so in

00:20:13,430 --> 00:20:16,520
that case you know air flow takes its

00:20:14,780 --> 00:20:19,850
configurations only in the form of a

00:20:16,520 --> 00:20:21,920
Python program here the software that we

00:20:19,850 --> 00:20:23,930
have it exposes a UI to the clients and

00:20:21,920 --> 00:20:25,940
the clients want to drag and drop stuff

00:20:23,930 --> 00:20:27,770
and you know configure each node and

00:20:25,940 --> 00:20:29,180
make their own workflows so there it

00:20:27,770 --> 00:20:31,670
won't really help unless you actually

00:20:29,180 --> 00:20:34,520
write another engine that converts this

00:20:31,670 --> 00:20:36,590
UI data into a Python code that's

00:20:34,520 --> 00:20:39,110
something second thing is I don't really

00:20:36,590 --> 00:20:41,390
remember the name that air flow uses for

00:20:39,110 --> 00:20:44,030
its parallelism we have seen that at

00:20:41,390 --> 00:20:46,700
scale it starts trembling it doesn't

00:20:44,030 --> 00:20:49,400
really work that well I don't remember

00:20:46,700 --> 00:20:51,440
the name of that component but yes there

00:20:49,400 --> 00:20:53,540
are some problems with that so that's

00:20:51,440 --> 00:20:55,910
how we were motivated to use kubernetes

00:20:53,540 --> 00:20:57,560
in itself the whole ecosystem would be

00:20:55,910 --> 00:21:04,250
around kubernetes and the workflow

00:20:57,560 --> 00:21:09,170
itself quickly do you guys plan to

00:21:04,250 --> 00:21:14,000
donate it to me yes so yes I would love

00:21:09,170 --> 00:21:15,740
to donate this but you know for as in

00:21:14,000 --> 00:21:17,870
you know when something goes out to the

00:21:15,740 --> 00:21:20,630
open source it has to have a minimal

00:21:17,870 --> 00:21:22,340
standard of quality code out there so

00:21:20,630 --> 00:21:25,070
you know there is a particular team in

00:21:22,340 --> 00:21:26,660
my company that actually reviews the

00:21:25,070 --> 00:21:29,240
code and all that it's actually under

00:21:26,660 --> 00:21:32,500
review and if they approve it I would

00:21:29,240 --> 00:21:35,390
definitely love to you know open sources

00:21:32,500 --> 00:21:36,140
okay thank you very much Abhishek thank

00:21:35,390 --> 00:21:39,939
you

00:21:36,140 --> 00:21:39,939

YouTube URL: https://www.youtube.com/watch?v=LjVfRz_q8YA


