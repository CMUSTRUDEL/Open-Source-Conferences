Title: Berlin Buzzwords 2019: Robin Moffatt – From zero to hero with Apache Kafka's connect API #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Integrating Apache Kafka with other systems in a reliable and scalable way is often a key part of a streaming platform. Fortunately, Apache Kafka includes the Connect API that enables streaming integration both in and out of Kafka. Like any technology, understanding its architecture and deployment patterns is key to successful use, as is knowing where to go looking when things aren’t working.

This talk will discuss the key design concepts within Kafka Connect and the pros and cons of standalone vs distributed deployment modes. We’ll do a live demo of building pipelines with Kafka Connect for streaming data in from databases, and out to targets including Elasticsearch. With some gremlins along the way, we’ll go hands-on in methodically diagnosing and resolving common issues encountered with Kafka Connect. The talk will finish off by discussing more advanced topics including Single Message Transforms, and deployment of Kafka Connect in containers.

Read more:
https://2019.berlinbuzzwords.de/19/session/zero-hero-apache-kafkas-connect-api

About Robin Moffatt:
https://2019.berlinbuzzwords.de/users/robin-moffatt

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:06,350 --> 00:00:11,650
oh good afternoon everyone Thanks

00:00:09,430 --> 00:00:13,750
to the dishonor it's a kind of mom and

00:00:11,650 --> 00:00:14,200
so if you fall asleep I shall be

00:00:13,750 --> 00:00:15,849
offended

00:00:14,200 --> 00:00:19,420
put it down to the heat and nothing else

00:00:15,849 --> 00:00:21,369
so I work at confluence that's

00:00:19,420 --> 00:00:23,919
confluence not confluence as always

00:00:21,369 --> 00:00:25,689
causes confusion confluent one of the

00:00:23,919 --> 00:00:27,430
companies who contribute to the open

00:00:25,689 --> 00:00:28,810
source Apache kafka projects and we also

00:00:27,430 --> 00:00:31,899
have confident platform when she's a

00:00:28,810 --> 00:00:35,590
distribution of Kafka so quick show of

00:00:31,899 --> 00:00:37,739
hands who's using Kafka today almost

00:00:35,590 --> 00:00:40,300
everyone who's not using Kafka today

00:00:37,739 --> 00:00:43,410
that's really kinda like the balance is

00:00:40,300 --> 00:00:45,940
now shifted and he's using Kafka Connect

00:00:43,410 --> 00:00:46,870
who kind of thinks they maybe should be

00:00:45,940 --> 00:00:49,480
and that's why they're here but they're

00:00:46,870 --> 00:00:50,920
not using it yet okay and who's here cuz

00:00:49,480 --> 00:00:54,310
it's like some I want hang out

00:00:50,920 --> 00:00:57,700
so Kafka Connect is part of Apache Kafka

00:00:54,310 --> 00:01:01,030
and it gives you a way to integrate

00:00:57,700 --> 00:01:04,150
other systems into Kafka and from Kafka

00:01:01,030 --> 00:01:06,520
to other systems so you can use Kafka

00:01:04,150 --> 00:01:08,680
connects to Poland data from a database

00:01:06,520 --> 00:01:11,259
from a message queue from a flat file

00:01:08,680 --> 00:01:13,899
from anywhere you want to and stream it

00:01:11,259 --> 00:01:16,840
into Kafka and you can use calc connect

00:01:13,899 --> 00:01:19,539
to stream data from Kafka down to

00:01:16,840 --> 00:01:22,149
somewhere else stream data from Kafka to

00:01:19,539 --> 00:01:24,189
elasticsearch to monger to influx to a

00:01:22,149 --> 00:01:26,710
database to wherever you want to put it

00:01:24,189 --> 00:01:27,999
and the thing about Kafka connectors it

00:01:26,710 --> 00:01:30,670
lets you build these end-to-end

00:01:27,999 --> 00:01:33,789
integrations without needing to write

00:01:30,670 --> 00:01:35,289
any code so we're all engineers or

00:01:33,789 --> 00:01:37,090
software developers or techies of some

00:01:35,289 --> 00:01:39,130
various form and we all love to write

00:01:37,090 --> 00:01:41,200
things and build things from scratch and

00:01:39,130 --> 00:01:42,639
as much fun as it is to reinvent the

00:01:41,200 --> 00:01:44,859
wheel each time and kind of build their

00:01:42,639 --> 00:01:46,539
own frameworks to do these things cough

00:01:44,859 --> 00:01:48,969
could connect solves this common problem

00:01:46,539 --> 00:01:50,829
that people have of I've got data here

00:01:48,969 --> 00:01:52,119
and I want to get it to there and then

00:01:50,829 --> 00:01:54,399
I've got data there and I want to also

00:01:52,119 --> 00:01:56,350
get it over there because CAF could

00:01:54,399 --> 00:01:59,619
connect just gives you a configuration

00:01:56,350 --> 00:02:01,389
file so as a as a data engineer as we

00:01:59,619 --> 00:02:03,340
all are nowadays you just set up a

00:02:01,389 --> 00:02:04,569
better JSON you say I want to connect to

00:02:03,340 --> 00:02:06,279
the system I want to pull in this

00:02:04,569 --> 00:02:08,860
information and stream it into this

00:02:06,279 --> 00:02:09,580
particular Kafka topic and it's kind of

00:02:08,860 --> 00:02:11,620
as easy as that

00:02:09,580 --> 00:02:13,270
or it's mostly as easy as that so that's

00:02:11,620 --> 00:02:16,330
all we've got another 37 minutes to talk

00:02:13,270 --> 00:02:18,340
about it so people use Kafka Connect for

00:02:16,330 --> 00:02:21,130
different things people use it for just

00:02:18,340 --> 00:02:22,750
very simple fairly dumb pipelines

00:02:21,130 --> 00:02:24,370
we're going to offload some data from

00:02:22,750 --> 00:02:26,560
here and we're gonna go and put it over

00:02:24,370 --> 00:02:27,940
there I've got data in our transactional

00:02:26,560 --> 00:02:30,340
system over here we want to go in

00:02:27,940 --> 00:02:32,980
stick it in a bucket over there or

00:02:30,340 --> 00:02:34,390
because Kafka persists data they say

00:02:32,980 --> 00:02:35,530
well we're going to take the data from

00:02:34,390 --> 00:02:37,320
here we're going to put it over there

00:02:35,530 --> 00:02:38,980
but we're also going to put it over here

00:02:37,320 --> 00:02:40,990
because that's the great thing about

00:02:38,980 --> 00:02:43,750
Kafka being this distributed persisted

00:02:40,990 --> 00:02:45,700
commit log you can reuse your data you

00:02:43,750 --> 00:02:47,230
bring the data in once and they say well

00:02:45,700 --> 00:02:48,700
we'd like to put it over here for this

00:02:47,230 --> 00:02:50,740
use case we'd like to put it over there

00:02:48,700 --> 00:02:52,630
for that use case and we'll put it over

00:02:50,740 --> 00:02:54,160
here as well for something else and you

00:02:52,630 --> 00:02:56,410
can use Kafka connect to say the same

00:02:54,160 --> 00:02:59,230
data and put it in different places to

00:02:56,410 --> 00:03:01,240
suit your purpose you can also use Kafka

00:02:59,230 --> 00:03:03,070
connect to simply say well my

00:03:01,240 --> 00:03:05,020
applications generating data I'm using

00:03:03,070 --> 00:03:06,580
Kafka as a broker between my services

00:03:05,020 --> 00:03:09,160
but some of our data

00:03:06,580 --> 00:03:11,320
I want to write out somewhere so option

00:03:09,160 --> 00:03:12,820
one is we build that into our service we

00:03:11,320 --> 00:03:14,560
write ourselves a service that does all

00:03:12,820 --> 00:03:16,180
this kind of stuff and we say well we'll

00:03:14,560 --> 00:03:18,340
connect to the data store and we'll

00:03:16,180 --> 00:03:20,080
worry about if the networks down or how

00:03:18,340 --> 00:03:21,310
often do we retry and where do we store

00:03:20,080 --> 00:03:23,200
the credentials and we'll worry about

00:03:21,310 --> 00:03:25,930
that because that's a sensible thing to

00:03:23,200 --> 00:03:27,880
try and build into a service or we say

00:03:25,930 --> 00:03:30,010
well we're generating data we're writing

00:03:27,880 --> 00:03:32,080
its cathc or any way we want to get that

00:03:30,010 --> 00:03:33,880
data down somewhere else we'll decoupled

00:03:32,080 --> 00:03:35,920
that responsibility so our application

00:03:33,880 --> 00:03:38,020
our services our service thing which is

00:03:35,920 --> 00:03:39,970
what it should be doing and then Kafka

00:03:38,020 --> 00:03:41,680
connectors responsible for saying data

00:03:39,970 --> 00:03:43,840
in this topic we'd like to go and also

00:03:41,680 --> 00:03:45,220
put it over there we'd like to monitor

00:03:43,840 --> 00:03:47,050
what's going on between our services

00:03:45,220 --> 00:03:49,390
we'd like to put an audit off the data

00:03:47,050 --> 00:03:51,790
and those topics copy that to

00:03:49,390 --> 00:03:54,820
elasticsearch or stream those metrics to

00:03:51,790 --> 00:03:57,190
in flux and so on but you can also use

00:03:54,820 --> 00:03:59,739
Kafka connect as a way I've started to

00:03:57,190 --> 00:04:01,660
migrate your architectures away from an

00:03:59,739 --> 00:04:03,400
older way of doing things may be built

00:04:01,660 --> 00:04:05,560
around calico a monolith or the database

00:04:03,400 --> 00:04:07,420
sat underneath it and move it more

00:04:05,560 --> 00:04:09,489
towards an event-driven way of doing

00:04:07,420 --> 00:04:10,750
things without having to actually just

00:04:09,489 --> 00:04:13,630
tear everything up and start all over

00:04:10,750 --> 00:04:15,550
again so you can say well we've got our

00:04:13,630 --> 00:04:17,169
existing application is writing to the

00:04:15,550 --> 00:04:18,280
database the database which is this

00:04:17,169 --> 00:04:20,530
lovely thing that we've loved and

00:04:18,280 --> 00:04:22,270
cherished for so long and we can't get

00:04:20,530 --> 00:04:24,280
rid of that we can't rewrite that

00:04:22,270 --> 00:04:25,270
application just for the sake of wanting

00:04:24,280 --> 00:04:27,730
to move to a different way of doing

00:04:25,270 --> 00:04:30,370
things what we can do is we can take the

00:04:27,730 --> 00:04:32,080
events out of the database stream those

00:04:30,370 --> 00:04:33,460
into Kafka and use them to drive new

00:04:32,080 --> 00:04:36,010
applications that we're building

00:04:33,460 --> 00:04:38,050
so without impacting the existing

00:04:36,010 --> 00:04:39,340
application we start to migrate to a

00:04:38,050 --> 00:04:41,830
more flexible way of doing things by

00:04:39,340 --> 00:04:43,960
taking those events out of the database

00:04:41,830 --> 00:04:46,030
using transaction logs and change data

00:04:43,960 --> 00:04:47,680
capture all that kind of good stuff so

00:04:46,030 --> 00:04:50,199
we're not impacting the databases for a

00:04:47,680 --> 00:04:52,210
low impact very low latency we can

00:04:50,199 --> 00:04:54,220
outdrive new services new applications

00:04:52,210 --> 00:04:56,740
with those events so there's different

00:04:54,220 --> 00:04:58,900
reasons why people use Kafka Connect so

00:04:56,740 --> 00:05:01,419
I want to show you carefully connected

00:04:58,900 --> 00:05:03,490
action and I honestly really do want to

00:05:01,419 --> 00:05:05,949
but whether I can or not is up to my

00:05:03,490 --> 00:05:08,020
laptop so I've done this demo before and

00:05:05,949 --> 00:05:09,970
it really did crash and burn I've set it

00:05:08,020 --> 00:05:11,889
running now and we'll see if it's

00:05:09,970 --> 00:05:16,330
actually going to behave so cross your

00:05:11,889 --> 00:05:17,469
fingers so first off we've got Kafka

00:05:16,330 --> 00:05:19,629
Connect and it looks like it's running

00:05:17,469 --> 00:05:20,650
all this Kurds on gets up by the way if

00:05:19,629 --> 00:05:23,259
you want to try it out for yourselves

00:05:20,650 --> 00:05:25,539
and what I'm going to do is simply take

00:05:23,259 --> 00:05:27,909
data from a database stream it into

00:05:25,539 --> 00:05:29,110
Kafka and then take that data from Kafka

00:05:27,909 --> 00:05:31,599
and stream it down to a couple of other

00:05:29,110 --> 00:05:34,180
places and the points here are it's very

00:05:31,599 --> 00:05:35,949
easy to do it's just a few JSON files to

00:05:34,180 --> 00:05:37,360
say get the data from here take the data

00:05:35,949 --> 00:05:39,940
from there and put it over there and

00:05:37,360 --> 00:05:41,560
it's also streaming missus event-driven

00:05:39,940 --> 00:05:42,909
this is not kind of I'm going to wait a

00:05:41,560 --> 00:05:44,680
for a while and then kind of collect a

00:05:42,909 --> 00:05:46,090
batch and then maybe tonight I'll send

00:05:44,680 --> 00:05:48,580
it over there and then tomorrow do some

00:05:46,090 --> 00:05:50,050
processing as stuff changes in the

00:05:48,580 --> 00:05:51,430
database we're going to stream it into

00:05:50,050 --> 00:05:52,990
Kafka and we're going to stream it over

00:05:51,430 --> 00:05:56,349
to other places where we might want to

00:05:52,990 --> 00:05:58,120
use it so it starts off with we're in my

00:05:56,349 --> 00:05:59,919
sequel it's just a relational database

00:05:58,120 --> 00:06:02,560
it could be any relational database and

00:05:59,919 --> 00:06:03,699
we've got some data net information

00:06:02,560 --> 00:06:05,770
about some orders that people have

00:06:03,699 --> 00:06:07,389
placed who place the order what did they

00:06:05,770 --> 00:06:09,550
order when do they place the order and

00:06:07,389 --> 00:06:13,120
so on and I'm going to set a DAT

00:06:09,550 --> 00:06:16,690
generator running so we set that running

00:06:13,120 --> 00:06:18,779
and over in my sequel if I reek weary

00:06:16,690 --> 00:06:19,990
yet you'll see 1518 was the first record

00:06:18,779 --> 00:06:23,319
15:26

00:06:19,990 --> 00:06:24,759
so there's new data arriving and what

00:06:23,319 --> 00:06:26,469
I'm going to do there is I'm gonna set a

00:06:24,759 --> 00:06:28,000
new script running you can see from our

00:06:26,469 --> 00:06:29,560
cheat sheet what's going on you can also

00:06:28,000 --> 00:06:31,509
use that cheat sheet to run this for

00:06:29,560 --> 00:06:32,770
yourselves and we're going to run this

00:06:31,509 --> 00:06:35,349
which is simply going to echo to the

00:06:32,770 --> 00:06:36,610
screen the data as it changes every

00:06:35,349 --> 00:06:38,289
couple of seconds we get a new row

00:06:36,610 --> 00:06:41,889
coming in which you can see from this

00:06:38,289 --> 00:06:43,870
create time stamp here now we're going

00:06:41,889 --> 00:06:46,930
to get that data from the database and

00:06:43,870 --> 00:06:49,629
stream it into Kafka unto that we're

00:06:46,930 --> 00:06:51,250
going to use Kafka connect so Kafka

00:06:49,629 --> 00:06:53,020
connects I'm going to talk about how it

00:06:51,250 --> 00:06:53,919
works and what connectors are and

00:06:53,020 --> 00:06:55,470
plugins and all this kind of stuff

00:06:53,919 --> 00:06:57,270
afterwards but

00:06:55,470 --> 00:06:59,430
I'm just going to show it you so to

00:06:57,270 --> 00:07:01,530
start with we just configure it we're

00:06:59,430 --> 00:07:03,600
going to post it a configuration on the

00:07:01,530 --> 00:07:05,100
REST API here's the connector that we're

00:07:03,600 --> 00:07:06,780
going to use it's a connector from a

00:07:05,100 --> 00:07:08,730
project called Derby's IAM and it

00:07:06,780 --> 00:07:10,740
connects to my sequel it uses the my

00:07:08,730 --> 00:07:13,110
sequel bin log the transaction log to

00:07:10,740 --> 00:07:15,660
get the events out and it also snapshots

00:07:13,110 --> 00:07:16,950
what already in the database so you say

00:07:15,660 --> 00:07:18,570
you connect to the data so over here

00:07:16,950 --> 00:07:22,290
we're interested in this particular

00:07:18,570 --> 00:07:29,070
table the orders table so I'm gonna copy

00:07:22,290 --> 00:07:30,420
and paste that over here so that'll run

00:07:29,070 --> 00:07:31,410
and if I go look at the CAF could

00:07:30,420 --> 00:07:33,000
connect log you'll see it'll start

00:07:31,410 --> 00:07:34,230
churning away and actually start firing

00:07:33,000 --> 00:07:37,380
up that connector and you'll see it

00:07:34,230 --> 00:07:41,010
doing snapshots and all kind of stuff so

00:07:37,380 --> 00:07:42,480
if we go back over here if I say to CAF

00:07:41,010 --> 00:07:44,700
could connect what is the status of that

00:07:42,480 --> 00:07:46,590
connector it'll say I've got a source

00:07:44,700 --> 00:07:48,300
it's using the bayesian connector it's

00:07:46,590 --> 00:07:49,860
running and so are all of its tusks

00:07:48,300 --> 00:07:51,630
which is really good thanks it means

00:07:49,860 --> 00:07:52,920
we're getting data into Kafka

00:07:51,630 --> 00:07:55,680
well we think we're getting data into

00:07:52,920 --> 00:07:57,390
Kafka if we go and check in my sequel

00:07:55,680 --> 00:07:59,970
okay here's my sequel here's the data

00:07:57,390 --> 00:08:02,340
being generated let's put that on that

00:07:59,970 --> 00:08:05,220
side of the screen on this side of the

00:08:02,340 --> 00:08:08,550
screen let's have a look at actually

00:08:05,220 --> 00:08:11,160
what's in Kafka so I'm going to use the

00:08:08,550 --> 00:08:13,050
Kafka Avro console consumer to say

00:08:11,160 --> 00:08:17,669
what's the data that's currently in that

00:08:13,050 --> 00:08:19,800
topic and hopefully any moment now it

00:08:17,669 --> 00:08:24,030
will say the data in that topic is

00:08:19,800 --> 00:08:26,310
whatever is in the database there we go

00:08:24,030 --> 00:08:27,510
so we've got data in my sequel I should

00:08:26,310 --> 00:08:29,730
probably put it the other way around so

00:08:27,510 --> 00:08:31,080
our data in my sequel being generated

00:08:29,730 --> 00:08:32,760
being written to buy whatever

00:08:31,080 --> 00:08:36,270
applications right into the database

00:08:32,760 --> 00:08:37,710
streaming in real time into Kafka so

00:08:36,270 --> 00:08:40,710
that's kind of useful because we've now

00:08:37,710 --> 00:08:42,300
got Kafka a topic in Kafka which any

00:08:40,710 --> 00:08:44,640
application can hook up to and say Oh

00:08:42,300 --> 00:08:46,560
anytime there's a new order I would like

00:08:44,640 --> 00:08:48,570
to know about it please and the

00:08:46,560 --> 00:08:51,030
connector itself it says was it created

00:08:48,570 --> 00:08:52,740
was it updated was it deleted captures

00:08:51,030 --> 00:08:56,280
deletes as well deletes are also events

00:08:52,740 --> 00:08:59,250
so you can integrate your database into

00:08:56,280 --> 00:09:01,800
Kafka using Kafka connect but you can

00:08:59,250 --> 00:09:03,720
also integrate Kafka into other places

00:09:01,800 --> 00:09:06,690
and push the data out stream the data

00:09:03,720 --> 00:09:07,890
out so one use here might be I've got a

00:09:06,690 --> 00:09:09,390
service that wants to know how a new

00:09:07,890 --> 00:09:11,130
order has been created our

00:09:09,390 --> 00:09:14,070
the kafka consumer or the Kafka streams

00:09:11,130 --> 00:09:16,110
API to read that topic directly but you

00:09:14,070 --> 00:09:17,850
might also say anytime there's a new

00:09:16,110 --> 00:09:19,500
order it and I want to analyze that I

00:09:17,850 --> 00:09:21,450
want to visualize that I want to put it

00:09:19,500 --> 00:09:23,160
somewhere else and use a tool

00:09:21,450 --> 00:09:25,020
appropriate for that I want to go and

00:09:23,160 --> 00:09:27,360
put it into elasticsearch I don't you

00:09:25,020 --> 00:09:30,480
can put it into a graph database so

00:09:27,360 --> 00:09:33,810
let's do that so we're going to create

00:09:30,480 --> 00:09:35,700
ourselves a new connector and again all

00:09:33,810 --> 00:09:37,260
we say is here's a bit of JSON the

00:09:35,700 --> 00:09:38,640
connector we're going to use is the

00:09:37,260 --> 00:09:40,020
elasticsearch link connector you have

00:09:38,640 --> 00:09:42,000
sources and sinks which are kind of

00:09:40,020 --> 00:09:43,800
obvious by the name send over to

00:09:42,000 --> 00:09:49,860
elasticsearch anything that's in this

00:09:43,800 --> 00:09:54,930
topic here's my elasticsearch host so we

00:09:49,860 --> 00:09:56,100
copy that in and paste it and then we're

00:09:54,930 --> 00:09:58,980
also going to stream it over to new or

00:09:56,100 --> 00:10:02,940
4j then again here's the connector neo4j

00:09:58,980 --> 00:10:04,590
sync connector and then bits of like

00:10:02,940 --> 00:10:06,300
connect a specific configuration so

00:10:04,590 --> 00:10:10,290
here's the cipher information on how to

00:10:06,300 --> 00:10:12,000
handle a particular data itself so we

00:10:10,290 --> 00:10:14,700
send those two connectors over to Kafka

00:10:12,000 --> 00:10:15,840
Connect we say two Kafka Connect tell me

00:10:14,700 --> 00:10:17,760
about your connectors tell me about

00:10:15,840 --> 00:10:19,860
their status it says about three

00:10:17,760 --> 00:10:22,020
connectors that are sync from the sorry

00:10:19,860 --> 00:10:23,400
source for your database with 3zm and

00:10:22,020 --> 00:10:26,820
I've got two sinks and they're all

00:10:23,400 --> 00:10:29,400
running which is splendid because what

00:10:26,820 --> 00:10:32,280
it means is we can then gone have a look

00:10:29,400 --> 00:10:35,160
at this data itself so let's head over

00:10:32,280 --> 00:10:37,710
we can use the REST API but only use

00:10:35,160 --> 00:10:42,270
Cabana cuz I like it and we can also use

00:10:37,710 --> 00:10:44,850
the neo4j browser so it could be a race

00:10:42,270 --> 00:10:46,650
which one manages to load up first with

00:10:44,850 --> 00:10:48,810
the fans on my laptop going up full pelt

00:10:46,650 --> 00:10:51,570
as well but what's happening is the date

00:10:48,810 --> 00:10:52,920
is coming into the database divisions

00:10:51,570 --> 00:10:54,930
reading the binary log of a transaction

00:10:52,920 --> 00:10:57,090
log streaming into a Kafka topic Kafka

00:10:54,930 --> 00:11:00,000
connects taking that data pushing it out

00:10:57,090 --> 00:11:01,980
both to elasticsearch and to neo4j and

00:11:00,000 --> 00:11:04,890
the key thing here is you've got the

00:11:01,980 --> 00:11:06,510
Kafka topic and the elastic search and

00:11:04,890 --> 00:11:08,670
neo4j completely independent

00:11:06,510 --> 00:11:10,980
if one connector dies the other one just

00:11:08,670 --> 00:11:12,930
carries on and that date has persisted

00:11:10,980 --> 00:11:14,250
for as long as we've told it to so when

00:11:12,930 --> 00:11:17,220
the connector recovers it will just

00:11:14,250 --> 00:11:19,320
carry on processing from where it got to

00:11:17,220 --> 00:11:23,010
we can have a look over here we've got

00:11:19,320 --> 00:11:25,020
data streaming in in real time it's 1731

00:11:23,010 --> 00:11:26,790
indeed so the data's coming through live

00:11:25,020 --> 00:11:28,980
from the day space into cargo being

00:11:26,790 --> 00:11:30,180
pushed out to elasticsearch okay you see

00:11:28,980 --> 00:11:33,710
that update there it's updating every

00:11:30,180 --> 00:11:36,120
five seconds the same thing with neo

00:11:33,710 --> 00:11:40,590
working on Avalon Kurtz who you bought

00:11:36,120 --> 00:11:41,730
some cars just show me the first 25

00:11:40,590 --> 00:11:44,340
people who bought some cars you got

00:11:41,730 --> 00:11:45,840
these people here we can start to drill

00:11:44,340 --> 00:11:49,950
in so here's that person where do they

00:11:45,840 --> 00:11:54,360
live this is all that person hopefully

00:11:49,950 --> 00:11:55,260
lives somewhere they live in London who

00:11:54,360 --> 00:11:58,860
are the other people who live in London

00:11:55,260 --> 00:12:00,480
and so on so it depends what is it you

00:11:58,860 --> 00:12:01,860
want to do with the data as to where you

00:12:00,480 --> 00:12:03,390
put it and this is the beautiful thing

00:12:01,860 --> 00:12:05,850
about using CAF green your architectures

00:12:03,390 --> 00:12:07,530
because you can actually say the data

00:12:05,850 --> 00:12:08,940
goes through Kafka because that's a very

00:12:07,530 --> 00:12:11,160
sensible place to store events because

00:12:08,940 --> 00:12:12,600
they what drive our business and then I

00:12:11,160 --> 00:12:14,190
want to do graph analysis great I'll

00:12:12,600 --> 00:12:16,020
stream it to neo I also want to do

00:12:14,190 --> 00:12:17,370
search I'll stream it to elastic I also

00:12:16,020 --> 00:12:19,770
want to do something else I'll stick it

00:12:17,370 --> 00:12:21,510
say in flux or snowflake or wherever I

00:12:19,770 --> 00:12:24,060
want that data you don't have to say

00:12:21,510 --> 00:12:25,650
well I'll use this kind of one box here

00:12:24,060 --> 00:12:28,910
that kind of vaguely satisfies all of

00:12:25,650 --> 00:12:33,390
them but none of them particularly well

00:12:28,910 --> 00:12:37,590
so without demo done and actually

00:12:33,390 --> 00:12:39,120
succeeding which is nice let's

00:12:37,590 --> 00:12:44,880
understand a bit more about what Kafka

00:12:39,120 --> 00:12:47,400
Connect actually is so Kafka Connect is

00:12:44,880 --> 00:12:49,110
built on this idea as a modular system

00:12:47,400 --> 00:12:51,990
as a modular framework it's part of

00:12:49,110 --> 00:12:53,190
Apache Kafka Apache Kafka is a

00:12:51,990 --> 00:12:55,320
distributor commit log s an event

00:12:53,190 --> 00:12:58,110
streaming platform as a producer and

00:12:55,320 --> 00:13:00,630
consumer API also has the connect API it

00:12:58,110 --> 00:13:01,890
also has the streams API these are parts

00:13:00,630 --> 00:13:04,500
of a patrick after if you're using

00:13:01,890 --> 00:13:07,470
apache Kafka you already have Kafka

00:13:04,500 --> 00:13:11,670
Connect since version 0.1 I think sorry

00:13:07,470 --> 00:13:13,770
zero dot ten so at its half it sits

00:13:11,670 --> 00:13:17,430
between a system where you've got data

00:13:13,770 --> 00:13:19,440
and Kafka or Kafka and a system where

00:13:17,430 --> 00:13:21,960
you want to put data if you're

00:13:19,440 --> 00:13:23,839
integrating with HDFS with s3 with any

00:13:21,960 --> 00:13:26,070
system and you're writing your own

00:13:23,839 --> 00:13:28,560
possibly you don't actually need to

00:13:26,070 --> 00:13:30,690
probably you really shouldn't be maybe

00:13:28,560 --> 00:13:31,890
you should but most of the time not most

00:13:30,690 --> 00:13:34,910
time Kafka connects what you should be

00:13:31,890 --> 00:13:37,170
using so it has an idea of connectors

00:13:34,910 --> 00:13:39,810
connectors are the jar files they're

00:13:37,170 --> 00:13:41,820
plugins that you can write yourself it's

00:13:39,810 --> 00:13:43,110
just part of a Java API but the

00:13:41,820 --> 00:13:44,790
beautiful thing about it is that

00:13:43,110 --> 00:13:47,130
probably someone already has

00:13:44,790 --> 00:13:48,750
someone's worked out how did I interact

00:13:47,130 --> 00:13:51,000
with this database how do I interact

00:13:48,750 --> 00:13:52,830
with that target place how do I interact

00:13:51,000 --> 00:13:55,140
with it and they've included that

00:13:52,830 --> 00:13:57,030
knowledge that source or target specific

00:13:55,140 --> 00:13:59,460
information of connecting to it into a

00:13:57,030 --> 00:14:01,350
jar so then you say okay Kafka kudex use

00:13:59,460 --> 00:14:04,640
this particular plugin and now Kafka

00:14:01,350 --> 00:14:07,290
Connect knows how to talk to X Y or Z so

00:14:04,640 --> 00:14:08,970
it can pull the information in and it

00:14:07,290 --> 00:14:11,220
will stream that through into the first

00:14:08,970 --> 00:14:13,080
bit of Kafka Connect and all we had to

00:14:11,220 --> 00:14:15,060
do to configure it to say well use this

00:14:13,080 --> 00:14:18,120
particular connector connector class is

00:14:15,060 --> 00:14:20,760
this and for a non programmer like me

00:14:18,120 --> 00:14:22,050
I'm not really bit of Python but not

00:14:20,760 --> 00:14:24,390
really I'm mostly like a data engineer

00:14:22,050 --> 00:14:25,800
what we call nowadays connector dot

00:14:24,390 --> 00:14:27,210
class sounds a bit scary but that's

00:14:25,800 --> 00:14:28,830
pretty much as scary as it gets

00:14:27,210 --> 00:14:31,650
it's just JSON and there's plenty of

00:14:28,830 --> 00:14:33,420
examples out there so the connector

00:14:31,650 --> 00:14:36,780
knows how to connect the source system

00:14:33,420 --> 00:14:38,430
or the target system and it then passes

00:14:36,780 --> 00:14:39,450
internally and this is all kind of like

00:14:38,430 --> 00:14:41,220
just under the covers you don't actually

00:14:39,450 --> 00:14:43,260
see this when you're running it it

00:14:41,220 --> 00:14:46,020
passes a connect record it obstructs

00:14:43,260 --> 00:14:48,270
away the idea of actually it's a JDBC

00:14:46,020 --> 00:14:50,280
record or it says something from the bin

00:14:48,270 --> 00:14:53,010
log or it's something from a CSV file it

00:14:50,280 --> 00:14:55,050
obstructs it into I've got some data and

00:14:53,010 --> 00:14:57,450
I've also got a schema and we'll see in

00:14:55,050 --> 00:14:58,740
a moment how important schemas are so it

00:14:57,450 --> 00:15:00,150
passes that internally as a connect

00:14:58,740 --> 00:15:03,510
record and it passes it to the

00:15:00,150 --> 00:15:07,350
converters so Kafka connect has

00:15:03,510 --> 00:15:08,700
connectors Kafka connect has converters

00:15:07,350 --> 00:15:11,280
and they all begin with C and it gets

00:15:08,700 --> 00:15:13,320
very confusing but a converter is

00:15:11,280 --> 00:15:15,210
responsible for saying here is this

00:15:13,320 --> 00:15:18,270
abstracted idea of some data plus a

00:15:15,210 --> 00:15:20,820
schema I don't go write that to Kafka in

00:15:18,270 --> 00:15:23,520
a certain way because your messages in

00:15:20,820 --> 00:15:26,580
Kafka they're just bytes Kafka doesn't

00:15:23,520 --> 00:15:28,050
care what it is it's just bytes which is

00:15:26,580 --> 00:15:29,940
really powerful but it also means that

00:15:28,050 --> 00:15:31,800
as data engineers using this kind of

00:15:29,940 --> 00:15:34,290
thing the onus is on us the

00:15:31,800 --> 00:15:36,740
responsibility is on us to decide how

00:15:34,290 --> 00:15:39,870
are we going to serialize that data and

00:15:36,740 --> 00:15:43,460
there's good ways and there's less good

00:15:39,870 --> 00:15:43,460
ways to serialize your data

00:15:44,710 --> 00:15:50,300
so if you care about your data if you

00:15:48,680 --> 00:15:51,860
care about your colleagues if you don't

00:15:50,300 --> 00:15:54,140
hate your colleagues you'll hopefully

00:15:51,860 --> 00:15:56,780
bear in mind that the schema that goes

00:15:54,140 --> 00:15:59,420
with data is pretty important it's

00:15:56,780 --> 00:16:01,400
really important my colleague Gwen

00:15:59,420 --> 00:16:03,680
Shapiro has this great expression the

00:16:01,400 --> 00:16:05,630
the schema is the API between your

00:16:03,680 --> 00:16:07,520
services it's the contract between your

00:16:05,630 --> 00:16:09,350
services and whether we're talking about

00:16:07,520 --> 00:16:10,670
offloading data from a database to put

00:16:09,350 --> 00:16:12,500
somewhere else for someone to use or

00:16:10,670 --> 00:16:14,690
taking data from a message queue for

00:16:12,500 --> 00:16:17,540
someone to write a service against the

00:16:14,690 --> 00:16:19,490
schema is massively important but if you

00:16:17,540 --> 00:16:21,230
write a chunk of CSV onto a file server

00:16:19,490 --> 00:16:22,940
somewhere you're basically sticking two

00:16:21,230 --> 00:16:24,890
fingers up whoever's using it saying

00:16:22,940 --> 00:16:26,780
well you figure it out or you're saying

00:16:24,890 --> 00:16:28,790
well anytime you want to use it you come

00:16:26,780 --> 00:16:31,670
and ask me and we all know how well that

00:16:28,790 --> 00:16:33,500
kind of coupling works out so by caring

00:16:31,670 --> 00:16:35,030
about our schemas by saying we're going

00:16:33,500 --> 00:16:37,550
to write our data in a fallout which

00:16:35,030 --> 00:16:39,380
supports schemas then we're actually

00:16:37,550 --> 00:16:40,730
making it easier to use the data we're

00:16:39,380 --> 00:16:43,970
making it easier to keep things more

00:16:40,730 --> 00:16:45,230
loosely coupled so you can use Alfre you

00:16:43,970 --> 00:16:47,810
can use prototype off there they kind of

00:16:45,230 --> 00:16:50,660
like the two main contenders here Avro

00:16:47,810 --> 00:16:52,040
is built into kind of confident platform

00:16:50,660 --> 00:16:54,350
and elements of Kafka Connect and it

00:16:52,040 --> 00:16:57,140
kind of makes it easier to use there's a

00:16:54,350 --> 00:16:58,220
community converter for protobuf and

00:16:57,140 --> 00:17:00,410
this is the beautiful thing out it will

00:16:58,220 --> 00:17:01,580
be in pluggable so it ships with an Avro

00:17:00,410 --> 00:17:03,290
converter if you download come from

00:17:01,580 --> 00:17:06,080
platform but you can also go and

00:17:03,290 --> 00:17:07,220
download approach above converter but

00:17:06,080 --> 00:17:10,070
when you implement one of these

00:17:07,220 --> 00:17:12,200
pipelines you say I'm going to use this

00:17:10,070 --> 00:17:14,870
converter I'm going to write my data in

00:17:12,200 --> 00:17:16,880
Avro I'm going to write it in JSON I'm

00:17:14,870 --> 00:17:19,970
feeling brave I'm going to write it in

00:17:16,880 --> 00:17:21,440
CSV as off to you how you write it or if

00:17:19,970 --> 00:17:22,790
you're consuming it you need to

00:17:21,440 --> 00:17:24,290
understand from the person who wrote it

00:17:22,790 --> 00:17:26,990
to that topic well how have you C

00:17:24,290 --> 00:17:29,750
realized it is it JSON is it Avro is a

00:17:26,990 --> 00:17:31,100
CSV I hate you it's kind of it's it's up

00:17:29,750 --> 00:17:32,840
to the people of building these things

00:17:31,100 --> 00:17:35,240
and obviously it makes an awful lot of

00:17:32,840 --> 00:17:36,950
sense to standardize so to standard I am

00:17:35,240 --> 00:17:39,920
biased but I would say standardize on

00:17:36,950 --> 00:17:41,210
Avro it's very richly supported it works

00:17:39,920 --> 00:17:43,820
very well at least you share your

00:17:41,210 --> 00:17:46,580
schemas so for example if you are using

00:17:43,820 --> 00:17:50,210
Avro then the schema itself gets stored

00:17:46,580 --> 00:17:52,070
in a schema registry so you get your

00:17:50,210 --> 00:17:53,300
data comes in which come from a database

00:17:52,070 --> 00:17:55,460
it's come from a flat file it's come

00:17:53,300 --> 00:17:57,140
from a message queue it's got payload

00:17:55,460 --> 00:17:58,910
and it's got a schema

00:17:57,140 --> 00:18:01,490
so you could say well we'll put the

00:17:58,910 --> 00:18:02,990
whole thing onto a message and our every

00:18:01,490 --> 00:18:04,370
single time I get a value I'll store the

00:18:02,990 --> 00:18:06,559
value I'm the schemer I'll put on the

00:18:04,370 --> 00:18:08,450
Kafka queue it could do but it's kind of

00:18:06,559 --> 00:18:10,340
quite a big message Avro takes a much

00:18:08,450 --> 00:18:13,040
more sensible approach it says here is

00:18:10,340 --> 00:18:15,080
your payload and then the schemer we

00:18:13,040 --> 00:18:16,880
will attach that the information about

00:18:15,080 --> 00:18:18,049
that schemer a reference to that schemer

00:18:16,880 --> 00:18:20,150
into their payload what they would

00:18:18,049 --> 00:18:22,820
rights Kafka and I a nice little binary

00:18:20,150 --> 00:18:26,360
form but the schema itself gets stored

00:18:22,820 --> 00:18:29,390
up in the scheme registry so then when

00:18:26,360 --> 00:18:31,490
we come to use that data whether it's

00:18:29,390 --> 00:18:32,929
Kafka Connect whether its case equal

00:18:31,490 --> 00:18:34,910
whether it's Kafka streams with us your

00:18:32,929 --> 00:18:38,179
own Kafka consuming application

00:18:34,910 --> 00:18:41,419
regardless it can deserialize that Avro

00:18:38,179 --> 00:18:43,669
data it'll go up to the scheme registry

00:18:41,419 --> 00:18:46,220
it'll say kind of a schema phrase for

00:18:43,669 --> 00:18:48,380
this particular ID and then it can reads

00:18:46,220 --> 00:18:51,230
it can deserialize that data and that

00:18:48,380 --> 00:18:55,220
data now has a forlorn schema has anyone

00:18:51,230 --> 00:18:56,450
heard of K sequel a few k sequel is part

00:18:55,220 --> 00:18:58,580
confluent platform it lets you use

00:18:56,450 --> 00:19:00,470
sequel streaming sequel over your data

00:18:58,580 --> 00:19:03,679
and Kafka if you're using that for

00:19:00,470 --> 00:19:05,360
example you can simply say do a sequel

00:19:03,679 --> 00:19:06,740
query against this topic and you've got

00:19:05,360 --> 00:19:09,169
all of your columns and your data types

00:19:06,740 --> 00:19:10,850
defined if you don't you have to type

00:19:09,169 --> 00:19:12,919
them all in manually it's that idea

00:19:10,850 --> 00:19:14,660
schemas are so important to anywhere

00:19:12,919 --> 00:19:17,570
where you're working with the data so

00:19:14,660 --> 00:19:20,600
anyway enough about schemas when you're

00:19:17,570 --> 00:19:22,790
building Kafka connect connector you

00:19:20,600 --> 00:19:24,620
specify the converter each converter has

00:19:22,790 --> 00:19:26,150
got its own parameters so if you're

00:19:24,620 --> 00:19:27,110
using the Afro converter to say well I

00:19:26,150 --> 00:19:28,940
need to tell it where to store the

00:19:27,110 --> 00:19:31,130
schema the schema goes in the schema

00:19:28,940 --> 00:19:32,720
registry URL if you're using JSON you

00:19:31,130 --> 00:19:35,929
say do I want to use schemas within the

00:19:32,720 --> 00:19:37,580
JSON and so on and so on so part of this

00:19:35,929 --> 00:19:39,380
is about understanding how to actually

00:19:37,580 --> 00:19:41,540
structure that configuration so you've

00:19:39,380 --> 00:19:43,309
got the key and the value converter

00:19:41,540 --> 00:19:45,770
because kafka messages are key value

00:19:43,309 --> 00:19:47,450
pairs and if you want you can use

00:19:45,770 --> 00:19:50,059
different see relation methods for the

00:19:47,450 --> 00:19:51,679
key and for the value so here we're just

00:19:50,059 --> 00:19:53,240
going to use Avro for both which is kind

00:19:51,679 --> 00:19:55,429
of quite a sensible place from which to

00:19:53,240 --> 00:19:57,290
start so your value converter the key

00:19:55,429 --> 00:20:01,190
converter and they've got the parameters

00:19:57,290 --> 00:20:02,929
for each of the two converters so we've

00:20:01,190 --> 00:20:04,220
got connectors which specify how to get

00:20:02,929 --> 00:20:06,650
the data in and out from the actual

00:20:04,220 --> 00:20:08,960
source and target systems we've got

00:20:06,650 --> 00:20:10,580
converters which are kind of generic and

00:20:08,960 --> 00:20:13,789
we can plug in different ones

00:20:10,580 --> 00:20:15,710
and then we got transforms so the

00:20:13,789 --> 00:20:18,049
transforms are an optional part of it

00:20:15,710 --> 00:20:20,600
but let you do transformations on the

00:20:18,049 --> 00:20:22,940
data as it passes through so you could

00:20:20,600 --> 00:20:25,399
say as this data comes in i would like

00:20:22,940 --> 00:20:26,720
to drop a certain field as this data

00:20:25,399 --> 00:20:28,779
comes in i would like to change the

00:20:26,720 --> 00:20:32,480
topic name to be something different

00:20:28,779 --> 00:20:33,890
excuse me as the data goes out I would

00:20:32,480 --> 00:20:35,990
like to route it to a different index

00:20:33,890 --> 00:20:37,850
name based on timestamp and so on and so

00:20:35,990 --> 00:20:40,370
on so you can do light forms of

00:20:37,850 --> 00:20:42,110
transformation on it it's not for

00:20:40,370 --> 00:20:43,700
building aggregates it's not for doing

00:20:42,110 --> 00:20:45,380
highly complex joins and all kind of

00:20:43,700 --> 00:20:47,360
stuff like that that's what you do

00:20:45,380 --> 00:20:49,760
something like Kafka streams or K sequel

00:20:47,360 --> 00:20:51,500
for but doing these kind of light

00:20:49,760 --> 00:20:53,899
transformation pieces are really useful

00:20:51,500 --> 00:20:56,570
the configuration is not entirely

00:20:53,899 --> 00:20:58,279
accessible but it does make sense so

00:20:56,570 --> 00:20:59,779
here's an example of Trance of two

00:20:58,279 --> 00:21:01,159
different transformations one of them

00:20:59,779 --> 00:21:03,710
were going to add the date to the topic

00:21:01,159 --> 00:21:04,880
one of them we just call label foo bar

00:21:03,710 --> 00:21:06,200
because we want to make it nice and

00:21:04,880 --> 00:21:08,120
difficult for people to understand

00:21:06,200 --> 00:21:09,679
actually what's going on but the point

00:21:08,120 --> 00:21:11,750
is when you create your transformations

00:21:09,679 --> 00:21:13,100
you prefix it with transforms and then

00:21:11,750 --> 00:21:14,539
you say here are the two different

00:21:13,100 --> 00:21:16,850
transformations one of them is our date

00:21:14,539 --> 00:21:19,490
topic one of them is label foo bar to

00:21:16,850 --> 00:21:20,950
make the point these are just labels so

00:21:19,490 --> 00:21:25,730
then when you actually configure them

00:21:20,950 --> 00:21:27,260
transforms label dot configuration

00:21:25,730 --> 00:21:29,510
information so this one here it's using

00:21:27,260 --> 00:21:32,269
the time stamp router which takes two

00:21:29,510 --> 00:21:33,919
different parameters this one here we're

00:21:32,269 --> 00:21:35,149
going to drop a particular going to

00:21:33,919 --> 00:21:36,409
rename a particular field so we're gonna

00:21:35,149 --> 00:21:38,269
rename delivery address to shipping

00:21:36,409 --> 00:21:39,740
address but it's got a light

00:21:38,269 --> 00:21:44,779
modification of data as it passes

00:21:39,740 --> 00:21:47,690
through it's kind of useful all of it is

00:21:44,779 --> 00:21:49,700
extensible all of it you can go and

00:21:47,690 --> 00:21:51,289
write your own all of its public driver

00:21:49,700 --> 00:21:52,549
API is you can write your own connectors

00:21:51,289 --> 00:21:54,289
you can write your own transformation as

00:21:52,549 --> 00:21:57,080
you create in converters and people do

00:21:54,289 --> 00:21:58,850
when it's brilliant to see you can also

00:21:57,080 --> 00:22:01,250
go and take advantage of what everyone

00:21:58,850 --> 00:22:03,080
else has written and download them so

00:22:01,250 --> 00:22:04,820
confluent hub is one place to go and get

00:22:03,080 --> 00:22:09,230
them a bunch of different converters

00:22:04,820 --> 00:22:11,750
connectors transformations and so on so

00:22:09,230 --> 00:22:14,139
a brief pause now another cuff and they

00:22:11,750 --> 00:22:14,139
were carry on

00:22:15,610 --> 00:22:21,769
excuse me so now deploying so we've

00:22:20,029 --> 00:22:23,299
learned a bit about kind of what happens

00:22:21,769 --> 00:22:25,669
underneath the covers just enough to

00:22:23,299 --> 00:22:27,769
understand water all these configuration

00:22:25,669 --> 00:22:28,820
items that we're actually setting rather

00:22:27,769 --> 00:22:30,470
than just like here's something I found

00:22:28,820 --> 00:22:32,510
on Stack Overflow and I can like tweak

00:22:30,470 --> 00:22:34,870
it until it works it's useful to

00:22:32,510 --> 00:22:36,889
understand what our converters

00:22:34,870 --> 00:22:39,320
particularly converters are what trip

00:22:36,889 --> 00:22:40,909
most people up with kafka connect and

00:22:39,320 --> 00:22:42,260
they're fantastically powerful when you

00:22:40,909 --> 00:22:44,779
understand what they're there for

00:22:42,260 --> 00:22:46,190
they're a real pain if you don't quite

00:22:44,779 --> 00:22:49,309
and you just kind of feeling until the

00:22:46,190 --> 00:22:51,139
damn thing works so you've got your

00:22:49,309 --> 00:22:52,220
configuration and it's working now you

00:22:51,139 --> 00:22:53,019
need to know how to actually go and

00:22:52,220 --> 00:22:56,389
deploy it

00:22:53,019 --> 00:23:00,769
so kafka Connect is built around this

00:22:56,389 --> 00:23:03,440
idea of tasks an each task sorry built

00:23:00,769 --> 00:23:06,440
around connectors and each connector is

00:23:03,440 --> 00:23:08,269
executed by a task so we've got a

00:23:06,440 --> 00:23:10,309
connector that's taking data from a

00:23:08,269 --> 00:23:13,210
topic it's streaming it down to s3 that

00:23:10,309 --> 00:23:15,860
works actually carried out by an s3 task

00:23:13,210 --> 00:23:17,929
we've got another connector it's reading

00:23:15,860 --> 00:23:20,919
and data from a database using JDBC sync

00:23:17,929 --> 00:23:23,570
it's also got a task or maybe two

00:23:20,919 --> 00:23:25,639
because kafka Connect can paralyze the

00:23:23,570 --> 00:23:27,980
work that it's performing depending on

00:23:25,639 --> 00:23:29,149
the source or target system if you're

00:23:27,980 --> 00:23:30,830
reading from a single flat file

00:23:29,149 --> 00:23:32,929
paralyzing that probably isn't gonna

00:23:30,830 --> 00:23:34,460
make much sense if you're ingesting data

00:23:32,929 --> 00:23:35,990
from a database you've got ten different

00:23:34,460 --> 00:23:38,179
tables paralyzing that makes a lot of

00:23:35,990 --> 00:23:40,760
sense until the DBA phones up and shouts

00:23:38,179 --> 00:23:42,019
at you but kalfa Connect can do

00:23:40,760 --> 00:23:44,149
parallelism and that's going to come

00:23:42,019 --> 00:23:45,289
down to the connector itself so the

00:23:44,149 --> 00:23:46,850
person who wrote the connector will

00:23:45,289 --> 00:23:49,570
understand does it make sense to

00:23:46,850 --> 00:23:52,460
paralyze this kind of ingest or egress

00:23:49,570 --> 00:23:54,919
so the tasks I want to carry that out

00:23:52,460 --> 00:23:57,200
and the tasks themselves run within a

00:23:54,919 --> 00:23:59,360
worker so the work is responsible for

00:23:57,200 --> 00:24:03,080
actually giving the tasks a place to

00:23:59,360 --> 00:24:06,139
live and run and the workers write the

00:24:03,080 --> 00:24:08,450
offsets so kafka connect stores the

00:24:06,139 --> 00:24:10,010
offsets and this is another of the many

00:24:08,450 --> 00:24:11,330
reasons why you should use Kafka

00:24:10,010 --> 00:24:13,460
collectors have been tempted to brew

00:24:11,330 --> 00:24:15,169
your own because you might say oh well

00:24:13,460 --> 00:24:17,029
I've got this data here I just need to

00:24:15,169 --> 00:24:18,889
go to HFS I'll write a spark job on that

00:24:17,029 --> 00:24:20,299
phone but then tomorrow someone says

00:24:18,889 --> 00:24:22,700
well can you also write it to s3 and

00:24:20,299 --> 00:24:24,080
also to neo anyway well that's three

00:24:22,700 --> 00:24:25,549
completely different technologies I've

00:24:24,080 --> 00:24:26,400
hard coded all of this to work with this

00:24:25,549 --> 00:24:28,950
one over here

00:24:26,400 --> 00:24:32,220
so Kafka connects abstract all of that

00:24:28,950 --> 00:24:34,380
it has technology specific connectors it

00:24:32,220 --> 00:24:36,450
has generic things for converting it

00:24:34,380 --> 00:24:38,790
also tracks where did it get to for each

00:24:36,450 --> 00:24:40,140
individual task this one managed to

00:24:38,790 --> 00:24:42,120
write all of the data to elasticsearch

00:24:40,140 --> 00:24:44,160
this what it fell over because neo4j did

00:24:42,120 --> 00:24:44,640
something wrong and nod it broke or

00:24:44,160 --> 00:24:46,380
something

00:24:44,640 --> 00:24:48,480
so this connector here has only got to

00:24:46,380 --> 00:24:50,250
this particular offset when we bring it

00:24:48,480 --> 00:24:52,620
back up this connector knows that I'll

00:24:50,250 --> 00:24:54,000
find I'll carry on from there so Kafka

00:24:52,620 --> 00:24:56,640
Connect does all of these good things so

00:24:54,000 --> 00:24:58,640
it tracks the offsets so you can run

00:24:56,640 --> 00:25:02,750
carefully connect in two different modes

00:24:58,640 --> 00:25:05,610
standalone and distributed and this

00:25:02,750 --> 00:25:07,140
after converters is probably the second

00:25:05,610 --> 00:25:08,520
thing which causes people the most

00:25:07,140 --> 00:25:11,010
confusion not always the problems are

00:25:08,520 --> 00:25:12,929
just about confusion so stand-alone road

00:25:11,010 --> 00:25:14,990
is just a standalone worker it's just a

00:25:12,929 --> 00:25:17,010
JVM process that sits there and it runs

00:25:14,990 --> 00:25:19,950
whether you're running standalone or

00:25:17,010 --> 00:25:22,290
distributed Kafka Connect does not run

00:25:19,950 --> 00:25:24,330
on your brokers nothing runs on your

00:25:22,290 --> 00:25:26,970
brokers except maybe zookeeper and even

00:25:24,330 --> 00:25:29,400
then some people would argue but connect

00:25:26,970 --> 00:25:31,260
run separately they can run on a laptop

00:25:29,400 --> 00:25:32,730
it can run on kubernetes it can run on

00:25:31,260 --> 00:25:35,160
wherever but it does not run on your

00:25:32,730 --> 00:25:37,290
brokers so it's a JVM process you can

00:25:35,160 --> 00:25:39,540
really stand alone but it's standalone

00:25:37,290 --> 00:25:42,870
it's a single instance it writes all of

00:25:39,540 --> 00:25:44,010
its offsets to a flat file when you shut

00:25:42,870 --> 00:25:45,390
it down and bring it back up it will

00:25:44,010 --> 00:25:47,820
read those offsets and they all carry on

00:25:45,390 --> 00:25:50,370
doing that once you reach the capacity

00:25:47,820 --> 00:25:52,050
of that JVM so you're running three

00:25:50,370 --> 00:25:53,190
different tasks and there's like tons of

00:25:52,050 --> 00:25:55,320
data coming through from the database

00:25:53,190 --> 00:25:57,390
and tons a day to going back out you

00:25:55,320 --> 00:26:00,660
kind of like hit saturation points you

00:25:57,390 --> 00:26:02,429
can't scale it well you kind of can you

00:26:00,660 --> 00:26:05,040
could say well we'll just partition it

00:26:02,429 --> 00:26:06,990
will now run to one of them is going to

00:26:05,040 --> 00:26:09,780
run the s3 work one of them is going to

00:26:06,990 --> 00:26:12,450
run the JDBC work and that's fine until

00:26:09,780 --> 00:26:14,300
you saturate a JDBC one and then you've

00:26:12,450 --> 00:26:17,550
got nowhere to go

00:26:14,300 --> 00:26:20,970
also it's not fault tolerance if that

00:26:17,550 --> 00:26:22,559
goes bang you're hosed you have to bring

00:26:20,970 --> 00:26:24,870
it back up nothing happens until you

00:26:22,559 --> 00:26:26,670
bring it back up so the other way of

00:26:24,870 --> 00:26:30,090
running kalfa correct is called

00:26:26,670 --> 00:26:32,340
distributed and it's not as scary as it

00:26:30,090 --> 00:26:33,660
sounds if you're new to distributed

00:26:32,340 --> 00:26:35,550
systems if you're new to Kafka

00:26:33,660 --> 00:26:37,200
distributed worker sounds like oh my god

00:26:35,550 --> 00:26:39,600
I'll go for the standalone that sounds

00:26:37,200 --> 00:26:40,200
much much easier but distributed is

00:26:39,600 --> 00:26:41,909
actually

00:26:40,200 --> 00:26:45,299
my opinion a much better place to start

00:26:41,909 --> 00:26:47,940
and this is why you can run distributed

00:26:45,299 --> 00:26:49,470
on a single node doesn't have to be

00:26:47,940 --> 00:26:51,840
distributed you can run it on a single

00:26:49,470 --> 00:26:53,940
node but when you run Africa headaches

00:26:51,840 --> 00:26:55,889
and distributed mode it stores all of

00:26:53,940 --> 00:26:59,279
its configuration all of the offsets all

00:26:55,889 --> 00:27:00,299
of that kind of stuff in Kafka because

00:26:59,279 --> 00:27:03,149
Kafka persists data

00:27:00,299 --> 00:27:05,399
Kafka is its permanent store of data so

00:27:03,149 --> 00:27:07,559
Kafka connectors using CAF good store of

00:27:05,399 --> 00:27:09,600
that good information which means that

00:27:07,559 --> 00:27:11,340
if you then want to scale it out you

00:27:09,600 --> 00:27:13,019
bring up a new worker and that worker

00:27:11,340 --> 00:27:15,870
says ah I'm part of that same group and

00:27:13,019 --> 00:27:17,309
it has all of its information its

00:27:15,870 --> 00:27:20,399
offsets this configuration and so on

00:27:17,309 --> 00:27:23,399
held centrally in Kafka and Kafka is

00:27:20,399 --> 00:27:25,889
disputed and fault tolerance etc etc so

00:27:23,399 --> 00:27:27,210
going from a single node are just like

00:27:25,889 --> 00:27:30,210
mucking around like this is all good

00:27:27,210 --> 00:27:31,860
whatever - oh we need to scale out is a

00:27:30,210 --> 00:27:35,399
case of like well we'll bring up a new

00:27:31,860 --> 00:27:37,740
worker with the same group ID so all of

00:27:35,399 --> 00:27:39,809
the learning of where do I find my log

00:27:37,740 --> 00:27:41,490
files where is it storing the stuff how

00:27:39,809 --> 00:27:43,490
do I configure it what's the rest API

00:27:41,490 --> 00:27:45,899
and so on you do all of that once

00:27:43,490 --> 00:27:47,370
whereas if you go from standalone and

00:27:45,899 --> 00:27:49,110
standalone you can figure with a flat

00:27:47,370 --> 00:27:50,700
file over here or something and you go

00:27:49,110 --> 00:27:52,230
from standalone to like oh crap we need

00:27:50,700 --> 00:27:54,450
more capacity or we need fault tolerance

00:27:52,230 --> 00:27:57,059
now you need to relearn and kind of redo

00:27:54,450 --> 00:27:59,130
stuff so there are reasons why people do

00:27:57,059 --> 00:28:00,809
use standalone maybe you need kinda like

00:27:59,130 --> 00:28:02,429
locality specific stuff like you reading

00:28:00,809 --> 00:28:04,350
from a particular local file which

00:28:02,429 --> 00:28:06,360
wouldn't make sense to run like anywhere

00:28:04,350 --> 00:28:08,340
there are reasons for standalone but

00:28:06,360 --> 00:28:11,639
generally I say use distribution less

00:28:08,340 --> 00:28:12,659
you've got a reason not to so dis you et

00:28:11,639 --> 00:28:15,299
gives you an easy way to scale

00:28:12,659 --> 00:28:17,370
distributed is also fault tolerant so

00:28:15,299 --> 00:28:19,260
Kafka Connect is like the runtime for

00:28:17,370 --> 00:28:22,590
these tasks Kafka Connect will say well

00:28:19,260 --> 00:28:24,389
oh no we lost a worker but we need to

00:28:22,590 --> 00:28:25,860
make sure that tasks keeps running so to

00:28:24,389 --> 00:28:27,510
say well I'll bring it back over here

00:28:25,860 --> 00:28:29,010
and obviously if you don't have the

00:28:27,510 --> 00:28:30,600
capacity it's going to keep on trying to

00:28:29,010 --> 00:28:31,860
run it but at least that we running a

00:28:30,600 --> 00:28:35,210
little bit maybe just kind of like a

00:28:31,860 --> 00:28:38,190
bits degraded but at least it's running

00:28:35,210 --> 00:28:40,440
you can also partition your distributed

00:28:38,190 --> 00:28:42,929
clusters so you can say well I want to

00:28:40,440 --> 00:28:44,580
isolate these things entirely or maybe

00:28:42,929 --> 00:28:47,070
I've just got different teams run Kafka

00:28:44,580 --> 00:28:49,559
connects we don't have to have one great

00:28:47,070 --> 00:28:51,389
big hairy cluster of Kafka connects you

00:28:49,559 --> 00:28:53,039
can have one on this team one on that

00:28:51,389 --> 00:28:54,150
team three on that team however you want

00:28:53,039 --> 00:28:56,560
to deploy it it's up to you

00:28:54,150 --> 00:28:58,270
but the key thing is kafka Connect and

00:28:56,560 --> 00:29:03,310
distribution knows gives you that

00:28:58,270 --> 00:29:05,590
scalability and fault-tolerance so I

00:29:03,310 --> 00:29:07,270
made kapha Connect sound a bit scary by

00:29:05,590 --> 00:29:08,590
saying people have problems with air

00:29:07,270 --> 00:29:11,530
some people have problems with that so

00:29:08,590 --> 00:29:12,610
I've gone through enough thoughts what

00:29:11,530 --> 00:29:14,020
are the things that people actually have

00:29:12,610 --> 00:29:16,210
problems with and what's the best way to

00:29:14,020 --> 00:29:18,760
learn to troubleshoot it because kafka

00:29:16,210 --> 00:29:20,770
Connect is brilliant Khafre Connect is

00:29:18,760 --> 00:29:22,540
super powerful but it's got a few of

00:29:20,770 --> 00:29:24,220
these little speed bumps on the way that

00:29:22,540 --> 00:29:25,600
maybe trip people up and they kind of

00:29:24,220 --> 00:29:28,000
give up on it before actually giving it

00:29:25,600 --> 00:29:29,650
a fair chance so if a troubleshooting

00:29:28,000 --> 00:29:32,560
cough could connect there's a few

00:29:29,650 --> 00:29:35,650
concepts that I wanna share with you so

00:29:32,560 --> 00:29:39,280
as we talked about connectors themselves

00:29:35,650 --> 00:29:41,080
run tasks one or many tasks and you can

00:29:39,280 --> 00:29:42,970
use the REST API of Kafka connect to say

00:29:41,080 --> 00:29:44,740
well is it running or not which is kind

00:29:42,970 --> 00:29:46,480
of a fair question to ask so you say is

00:29:44,740 --> 00:29:47,680
the connector running it says yes the

00:29:46,480 --> 00:29:49,870
connectors running you think brilliant

00:29:47,680 --> 00:29:51,880
but where the hell is my data and kapha

00:29:49,870 --> 00:29:54,640
Connect says well the connect is running

00:29:51,880 --> 00:29:55,840
but actually the task isn't you see and

00:29:54,640 --> 00:29:58,510
it's just one of these funny semantic

00:29:55,840 --> 00:30:00,520
things so under the covers each

00:29:58,510 --> 00:30:02,830
connector is executed by one or more

00:30:00,520 --> 00:30:04,960
tasks if all of those tasks are failed

00:30:02,830 --> 00:30:06,850
the connector can be running but you

00:30:04,960 --> 00:30:10,210
ain't get no data so you have to go and

00:30:06,850 --> 00:30:12,070
check both assuming it says the task has

00:30:10,210 --> 00:30:14,020
failed and you say well I don't know why

00:30:12,070 --> 00:30:16,000
it's failed you can use the REST API to

00:30:14,020 --> 00:30:17,800
actually get a stack trace why did it

00:30:16,000 --> 00:30:19,090
fail and you can kind of read it off

00:30:17,800 --> 00:30:20,560
there and it's got all the kind of line

00:30:19,090 --> 00:30:22,330
breaks encoded within it which is a bit

00:30:20,560 --> 00:30:23,710
hairy at some point you'll eventually

00:30:22,330 --> 00:30:27,040
want to go and look at the worker

00:30:23,710 --> 00:30:28,900
log so as any good troubleshooter knows

00:30:27,040 --> 00:30:30,220
the logs Jenny where you need to go and

00:30:28,900 --> 00:30:32,230
find out what's happening it's not

00:30:30,220 --> 00:30:34,510
enough just say it broke where's my data

00:30:32,230 --> 00:30:36,160
go and have a look at the worker log the

00:30:34,510 --> 00:30:39,040
Kafka connect worker log is where all of

00:30:36,160 --> 00:30:40,840
the stuff gets written to so depending

00:30:39,040 --> 00:30:42,220
on how you start Kafka connects depends

00:30:40,840 --> 00:30:43,660
on where you'll find this if you use in

00:30:42,220 --> 00:30:45,880
conference a li it's confident log

00:30:43,660 --> 00:30:47,200
compose Kath whatever it's written to

00:30:45,880 --> 00:30:50,170
stand it out and you can send it to

00:30:47,200 --> 00:30:52,210
different places so the first thing is

00:30:50,170 --> 00:30:54,640
you search through it and you search for

00:30:52,210 --> 00:30:56,920
this the first instance from the bottom

00:30:54,640 --> 00:30:58,300
of error and it'll say this task is

00:30:56,920 --> 00:31:00,760
being killed and will not recover and so

00:30:58,300 --> 00:31:02,020
it manually restarted but which P point

00:31:00,760 --> 00:31:04,630
people say have found the error

00:31:02,020 --> 00:31:06,520
what's this mean why is it broken and a

00:31:04,630 --> 00:31:08,330
bit disappointed when we say well don't

00:31:06,520 --> 00:31:10,480
know that's just the symptom but

00:31:08,330 --> 00:31:13,370
just says Kafka Connect saying it broke

00:31:10,480 --> 00:31:15,049
doesn't tell you why so you then go back

00:31:13,370 --> 00:31:16,429
up the log and it search for the

00:31:15,049 --> 00:31:17,840
previous error at which point you

00:31:16,429 --> 00:31:20,240
actually get the output from the task

00:31:17,840 --> 00:31:23,059
and the task says here's why I failed

00:31:20,240 --> 00:31:25,190
here's the stack of trees so you find

00:31:23,059 --> 00:31:27,950
the error you find out why the task says

00:31:25,190 --> 00:31:33,320
it broke some of the common errors that

00:31:27,950 --> 00:31:37,370
we get this one probably the most common

00:31:33,320 --> 00:31:38,840
one anyone seen this before yeah well

00:31:37,370 --> 00:31:41,240
not but no too embarrassed to show you

00:31:38,840 --> 00:31:43,429
and so unknown magic by it sounds fun

00:31:41,240 --> 00:31:46,039
but it's not unknown magic by it means

00:31:43,429 --> 00:31:48,529
that you've tried to deserialize some

00:31:46,039 --> 00:31:50,510
data using the Alpha roadie serializer

00:31:48,529 --> 00:31:52,100
but it's not Avro because you know I

00:31:50,510 --> 00:31:53,299
said that with alpha row it stores the

00:31:52,100 --> 00:31:54,769
schemer it stores information at that

00:31:53,299 --> 00:31:56,690
scheme like well indicator to the

00:31:54,769 --> 00:31:58,399
schemer in the scheme registry so this

00:31:56,690 --> 00:32:00,860
magic byte which is the fun little thing

00:31:58,399 --> 00:32:03,200
that magic byte has information about

00:32:00,860 --> 00:32:05,870
what's the schemer ID and if you try and

00:32:03,200 --> 00:32:07,399
DC rise message which isn't Avro there

00:32:05,870 --> 00:32:08,600
will be no little magic byte they'll be

00:32:07,399 --> 00:32:10,820
like a curly brace or something like

00:32:08,600 --> 00:32:13,639
that says JSON so if you're trying to

00:32:10,820 --> 00:32:16,700
read JSON data and you've said the

00:32:13,639 --> 00:32:18,860
converter is out fro Kafka Canet will

00:32:16,700 --> 00:32:21,039
try and convert it from Avro but it's

00:32:18,860 --> 00:32:24,620
JSON so just use the correct converter

00:32:21,039 --> 00:32:27,559
that's a quite an easy one but then you

00:32:24,620 --> 00:32:29,090
say well it is Avro I promise it's Avro

00:32:27,559 --> 00:32:30,830
look here's my settings for the thing

00:32:29,090 --> 00:32:33,860
that's writing to it and it could well

00:32:30,830 --> 00:32:35,299
be Avro but either from you playing

00:32:33,860 --> 00:32:36,139
around when you're setting your boss and

00:32:35,299 --> 00:32:38,510
when I was just playing silly buggers

00:32:36,139 --> 00:32:40,580
with you someone's writing JSON data to

00:32:38,510 --> 00:32:42,200
talk to the topic or maybe they started

00:32:40,580 --> 00:32:44,000
off writing JSON data and then they

00:32:42,200 --> 00:32:46,460
wrote Avro but either way you got kinda

00:32:44,000 --> 00:32:47,659
like mixed messages on the topic so

00:32:46,460 --> 00:32:49,940
Kafka Connect will start from the

00:32:47,659 --> 00:32:51,909
beginning the topic if it hits a non

00:32:49,940 --> 00:32:54,200
Avro one you're gonna get the same error

00:32:51,909 --> 00:32:56,059
but the cool thing with Kafka Connect is

00:32:54,200 --> 00:32:59,570
it can handle errors it's got dead

00:32:56,059 --> 00:33:01,669
letter cues so in some instances it can

00:32:59,570 --> 00:33:03,590
actually say if I fail to deserialize a

00:33:01,669 --> 00:33:05,809
message either I can kind I just fall

00:33:03,590 --> 00:33:07,909
over and cry or I can go write it

00:33:05,809 --> 00:33:09,230
somewhere else and just carry on and

00:33:07,909 --> 00:33:11,510
it's up to you when you're building that

00:33:09,230 --> 00:33:13,789
pipeline say well what makes sense if I

00:33:11,510 --> 00:33:15,320
hit a bad message is that possible is

00:33:13,789 --> 00:33:17,029
that plausible I'll just carry on or

00:33:15,320 --> 00:33:18,620
actually stop the world there should

00:33:17,029 --> 00:33:21,760
never be any bad messages we need to

00:33:18,620 --> 00:33:24,370
troubleshoot that and not continue

00:33:21,760 --> 00:33:27,490
default behavior is fail fast fall over

00:33:24,370 --> 00:33:30,040
throw toys out the pram complain you

00:33:27,490 --> 00:33:32,200
could also say well the opposite screw

00:33:30,040 --> 00:33:32,860
it we'll just ignore it if we can't

00:33:32,200 --> 00:33:36,610
handle it

00:33:32,860 --> 00:33:39,940
drop it which is kind of brave you can

00:33:36,610 --> 00:33:42,520
also say well be tolerant don't fall

00:33:39,940 --> 00:33:44,770
over but if you hit a message that you

00:33:42,520 --> 00:33:46,120
can't deserialize you can't handle right

00:33:44,770 --> 00:33:48,250
it to a dead letter q which is a

00:33:46,120 --> 00:33:50,680
separate topic and that means you can

00:33:48,250 --> 00:33:52,450
actually process it again so in that

00:33:50,680 --> 00:33:54,100
example we got mixed Avro and JSON you

00:33:52,450 --> 00:33:56,110
can say well if you can't deserialize it

00:33:54,100 --> 00:33:58,180
as Avro write it to another topic read

00:33:56,110 --> 00:34:00,810
that topic with a JSON converter and

00:33:58,180 --> 00:34:04,720
maybe you'll have better luck

00:34:00,810 --> 00:34:07,540
another common problem no fields found

00:34:04,720 --> 00:34:09,460
using key in value schemas of JSON with

00:34:07,540 --> 00:34:11,230
C games enable requires schema and

00:34:09,460 --> 00:34:14,560
payload fails what the hell do these

00:34:11,230 --> 00:34:15,760
mean it's all about the schema your

00:34:14,560 --> 00:34:17,679
mother the schema that I was getting

00:34:15,760 --> 00:34:20,350
quite passionate about before schemas

00:34:17,679 --> 00:34:22,960
matter and when we're writing data down

00:34:20,350 --> 00:34:25,300
to a target system that target system

00:34:22,960 --> 00:34:27,040
may or may not be kind of wanting a

00:34:25,300 --> 00:34:28,389
schema if we're talking days for

00:34:27,040 --> 00:34:30,639
elasticsearch we can say well can i

00:34:28,389 --> 00:34:32,620
we'll just let it guess and quite often

00:34:30,639 --> 00:34:33,879
that's fine if we're writing data to a

00:34:32,620 --> 00:34:35,679
relational database the relational

00:34:33,879 --> 00:34:37,659
database is probably to say well hey I'm

00:34:35,679 --> 00:34:39,340
a relational database I kind of schemers

00:34:37,659 --> 00:34:40,600
or what make the world go round so if

00:34:39,340 --> 00:34:44,080
you try write to a relational database

00:34:40,600 --> 00:34:47,129
or other systems without a schema you're

00:34:44,080 --> 00:34:50,260
going to get an error so if you're using

00:34:47,129 --> 00:34:51,820
JSON sorry if you using Avro data you're

00:34:50,260 --> 00:34:53,980
in luck because you've got a schema

00:34:51,820 --> 00:34:55,179
that's the whole point of it so you've

00:34:53,980 --> 00:34:57,220
got a schema which comes from the scheme

00:34:55,179 --> 00:34:58,840
registry you got each individual message

00:34:57,220 --> 00:35:03,460
each individual message picks up its

00:34:58,840 --> 00:35:06,070
schema and life is good but if you've

00:35:03,460 --> 00:35:07,600
got JSON data that looks like that so

00:35:06,070 --> 00:35:09,610
you've just got a better JSON a lump of

00:35:07,600 --> 00:35:11,920
JSON let's check out a database you

00:35:09,610 --> 00:35:13,390
might eyeball that and say well it's got

00:35:11,920 --> 00:35:14,860
a schema it's got like an author ID

00:35:13,390 --> 00:35:16,090
which looks like a number and a make

00:35:14,860 --> 00:35:16,650
that looks like a string that's got a

00:35:16,090 --> 00:35:19,090
schemer

00:35:16,650 --> 00:35:20,980
but you've not actually declared the

00:35:19,090 --> 00:35:22,960
schema it doesn't have a schema that's

00:35:20,980 --> 00:35:24,970
why it's saying I don't have a schema

00:35:22,960 --> 00:35:26,890
and you could argue that maybe the

00:35:24,970 --> 00:35:28,390
connector itself could infer it or

00:35:26,890 --> 00:35:30,490
whatever whatever but it doesn't that's

00:35:28,390 --> 00:35:32,770
just how it works so one option is you

00:35:30,490 --> 00:35:35,290
say we're going to use JSON but we're

00:35:32,770 --> 00:35:35,650
gonna embed the schema which case you

00:35:35,290 --> 00:35:39,160
must

00:35:35,650 --> 00:35:42,610
choose that format of it the other

00:35:39,160 --> 00:35:44,650
option is to use case equal to take your

00:35:42,610 --> 00:35:46,420
sauce JSON which kind of you can look at

00:35:44,650 --> 00:35:48,940
it and guess what the schemer is to take

00:35:46,420 --> 00:35:51,610
that and actually apply a schema Turris

00:35:48,940 --> 00:35:52,720
and re serialize it this is a really

00:35:51,610 --> 00:35:53,980
cool thing that you can do with case

00:35:52,720 --> 00:35:54,790
equal every single message that applies

00:35:53,980 --> 00:35:58,090
on a topic

00:35:54,790 --> 00:35:59,290
apply a schema re serialize it someone's

00:35:58,090 --> 00:36:01,720
mucking around with you they're saying

00:35:59,290 --> 00:36:03,310
here's a topic it got CSV on hahaha you

00:36:01,720 --> 00:36:06,010
say well I'll take that data that's in

00:36:03,310 --> 00:36:07,930
CSV I'll apply the schema and I'll

00:36:06,010 --> 00:36:10,720
resize it to Avro and now everyone else

00:36:07,930 --> 00:36:14,130
can use it in a friendly way so the same

00:36:10,720 --> 00:36:14,130
thing you can use with Kafka correct

00:36:14,430 --> 00:36:19,600
finally containers so you can run Kafka

00:36:19,210 --> 00:36:21,070
connect

00:36:19,600 --> 00:36:23,680
wherever you want to you can run kapha

00:36:21,070 --> 00:36:26,260
connecting containers I run it on docker

00:36:23,680 --> 00:36:28,030
it's a lot easier to use like with

00:36:26,260 --> 00:36:30,730
docker compose and so on there are two

00:36:28,030 --> 00:36:32,950
different connects images that confluent

00:36:30,730 --> 00:36:35,050
publish this calf could connect base

00:36:32,950 --> 00:36:37,570
which is just the base Kafka Connect and

00:36:35,050 --> 00:36:39,750
then this Kafka Connect which is clue to

00:36:37,570 --> 00:36:42,610
the elasticsearch connector the HDFS

00:36:39,750 --> 00:36:46,240
JDBC in a couple of others but when

00:36:42,610 --> 00:36:48,610
you're bringing other connectors in

00:36:46,240 --> 00:36:50,170
other plugins in you can get them from

00:36:48,610 --> 00:36:51,370
confluent hub you can kind of like

00:36:50,170 --> 00:36:52,240
download it yourself and build it

00:36:51,370 --> 00:36:54,160
yourself but you can get it from

00:36:52,240 --> 00:36:57,640
confluent hub pre-built and you need to

00:36:54,160 --> 00:36:59,620
get that jar into your container you

00:36:57,640 --> 00:37:01,900
could do it at runtime so you can simply

00:36:59,620 --> 00:37:03,610
say when I run I'm going to first run

00:37:01,900 --> 00:37:06,340
confluent hub install and pull down the

00:37:03,610 --> 00:37:08,400
particular connector and then spin up

00:37:06,340 --> 00:37:11,200
Kafka codex and run that afterwards or

00:37:08,400 --> 00:37:12,700
you can build a new image whichever way

00:37:11,200 --> 00:37:15,670
you want to do it it's up to you depends

00:37:12,700 --> 00:37:17,890
away on a store your data and so on and

00:37:15,670 --> 00:37:19,960
you can also automate creating the

00:37:17,890 --> 00:37:23,110
connectors so you can say well his

00:37:19,960 --> 00:37:26,290
docker compose I'm gonna install the

00:37:23,110 --> 00:37:27,970
particular connector I'm in a way if

00:37:26,290 --> 00:37:29,290
Kafka connected me up and then I'm going

00:37:27,970 --> 00:37:33,070
to send it the particular configuration

00:37:29,290 --> 00:37:34,810
the JDBC configuration which so means

00:37:33,070 --> 00:37:36,430
you can have just docker compose up and

00:37:34,810 --> 00:37:38,320
it kind of like downloads it runs it

00:37:36,430 --> 00:37:43,810
configures the connector and if things

00:37:38,320 --> 00:37:46,030
start running so that is all I have I

00:37:43,810 --> 00:37:46,630
have two minutes to spare for a few

00:37:46,030 --> 00:37:49,410
questions

00:37:46,630 --> 00:37:51,880
and that's my Twitter

00:37:49,410 --> 00:37:54,040
so where's my toe I'm off is my Twitter

00:37:51,880 --> 00:37:55,900
if you would like to tweet me tell me if

00:37:54,040 --> 00:37:58,060
you liked it so if you didn't the slides

00:37:55,900 --> 00:38:00,850
are on there I will tweet them

00:37:58,060 --> 00:38:02,440
afterwards as well but Oh before I do

00:38:00,850 --> 00:38:03,220
questions and there are drinks

00:38:02,440 --> 00:38:05,140
downstairs

00:38:03,220 --> 00:38:08,110
I believe confluence like currently

00:38:05,140 --> 00:38:09,160
sponsoring so I'll hang around I'll do a

00:38:08,110 --> 00:38:11,420
couple questions now about me downstairs

00:38:09,160 --> 00:38:15,510
as well if any wants to chat thank you

00:38:11,420 --> 00:38:18,010
[Applause]

00:38:15,510 --> 00:38:22,480
how'd I do a quick speaker selfie as

00:38:18,010 --> 00:38:25,180
well other questions

00:38:22,480 --> 00:38:32,980
there's one in the center let me come

00:38:25,180 --> 00:38:35,410
over Thanks yeah would you use tafakkur

00:38:32,980 --> 00:38:39,010
connector backup and restore compacted

00:38:35,410 --> 00:38:43,150
topics or could you shoot you back a

00:38:39,010 --> 00:38:45,940
freestyle kafka cut slovak conducted you

00:38:43,150 --> 00:38:49,020
can certainly write them oh sorry read

00:38:45,940 --> 00:38:51,700
them I don't see why not

00:38:49,020 --> 00:38:53,350
so we can't cuff the Kinect can't create

00:38:51,700 --> 00:38:54,610
at our target topic but you probably

00:38:53,350 --> 00:38:56,440
want to pre trace it with appropriate

00:38:54,610 --> 00:39:03,970
configuration but I don't see why that

00:38:56,440 --> 00:39:06,420
wouldn't work no okay thank you any more

00:39:03,970 --> 00:39:06,420
Christians

00:39:08,980 --> 00:39:16,190
it's a free-to-use is it I see that it's

00:39:13,490 --> 00:39:19,940
commute confident community license yes

00:39:16,190 --> 00:39:22,789
and not a party to bundle how is it

00:39:19,940 --> 00:39:26,000
so Kafka connect is part of a Patrick

00:39:22,789 --> 00:39:27,950
Africa which is Apache 2.0 licensed the

00:39:26,000 --> 00:39:35,809
different connectors have varying

00:39:27,950 --> 00:39:38,829
licenses so for example the the JDBC

00:39:35,809 --> 00:39:40,880
connector the elastic search connector

00:39:38,829 --> 00:39:42,920
HDFS connector part of confident

00:39:40,880 --> 00:39:44,270
platform I believe they're confident

00:39:42,920 --> 00:39:45,950
community license but I would need to

00:39:44,270 --> 00:39:47,900
double-check I can't promise other

00:39:45,950 --> 00:39:50,029
connectors what may be proprietary

00:39:47,900 --> 00:39:54,650
others may be Apache to do a kind of it

00:39:50,029 --> 00:39:57,589
depends on who wrote them thank you and

00:39:54,650 --> 00:40:00,140
everything so our developer gives us

00:39:57,589 --> 00:40:03,230
produce Jason without other also where I

00:40:00,140 --> 00:40:07,609
saw you present is creating a flow

00:40:03,230 --> 00:40:12,369
schema by K scale so it's any or right

00:40:07,609 --> 00:40:15,829
is that how you code like how you relate

00:40:12,369 --> 00:40:18,500
correlated state the cascade connect

00:40:15,829 --> 00:40:20,720
because you use SQL to create a grow but

00:40:18,500 --> 00:40:25,760
it's like isolated between calculator

00:40:20,720 --> 00:40:26,960
it's somehow related to let me

00:40:25,760 --> 00:40:29,390
paraphrase you have understood correctly

00:40:26,960 --> 00:40:31,730
so you your interesting this idea of

00:40:29,390 --> 00:40:33,529
resale realizing data using K sequel and

00:40:31,730 --> 00:40:37,760
how does it relate Africa correct yeah

00:40:33,529 --> 00:40:40,730
so k sequel is part confident platform

00:40:37,760 --> 00:40:42,589
its runs also stomach excuses Kafka

00:40:40,730 --> 00:40:45,289
streams under the covers but it's just a

00:40:42,589 --> 00:40:47,210
components that's reading data from a

00:40:45,289 --> 00:40:50,150
calcio topic and writing data to a Kafka

00:40:47,210 --> 00:40:51,799
topic just as any other application it's

00:40:50,150 --> 00:40:54,109
just that it's using the Avro

00:40:51,799 --> 00:40:56,450
serialize and deserialize Avro

00:40:54,109 --> 00:40:57,520
serializer to write data that it's read

00:40:56,450 --> 00:41:00,200
as JSON

00:40:57,520 --> 00:41:04,549
does that answer your question yeah okay

00:41:00,200 --> 00:41:06,920
great okay okay

00:41:04,549 --> 00:41:08,329
I make it six o'clock so going again if

00:41:06,920 --> 00:41:09,890
you want to hang around here I'll be

00:41:08,329 --> 00:41:11,359
downstairs as well alright let's do it

00:41:09,890 --> 00:41:12,170
this way let's take the rest of the

00:41:11,359 --> 00:41:14,150
questions offline

00:41:12,170 --> 00:41:16,930
there's reps of all our session for

00:41:14,150 --> 00:41:19,849
today we hope you enjoyed the buzzwords

00:41:16,930 --> 00:41:21,980
well let's think we Robin again for for

00:41:19,849 --> 00:41:24,170
the talk and QA

00:41:21,980 --> 00:41:27,020
and then let's meet downstairs for the

00:41:24,170 --> 00:41:29,170
get-together in the paddy is what the

00:41:27,020 --> 00:41:29,170

YouTube URL: https://www.youtube.com/watch?v=oNK3lB8Z-ZA


