Title: Berlin Buzzwords 2019: Philipp Schmidt – Large-scale Data Quality Verification (...)
Publication date: 2019-06-28
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Philipp Schmidt talking about "Large-scale Data Quality Verification - How to Unit-test Your Data with deequ".

Every day, companies rely on data to guide every single business process and decision. Missing or incorrect information seriously compromises the customer experience and any decision process downstream. Therefore, a crucial, but tedious task for every team involved in data processing is to verify the quality of their data. In this talk, we will show how to continuously verify data quality by defining metrics and constraints, resulting in better testing for data pipelines and machine learning applications.

Read more:
https://2019.berlinbuzzwords.de/19/session/large-scale-data-quality-verification-how-unit-test-your-data-deequ

About Philipp Schmidt:
https://2019.berlinbuzzwords.de/users/philipp-schmidt

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:07,750 --> 00:00:13,850
hi guys what's up so my name is philipp

00:00:11,410 --> 00:00:15,140
nice three two all of you and

00:00:13,850 --> 00:00:17,720
I would like to talk to you about

00:00:15,140 --> 00:00:20,120
large-scale data quality verification

00:00:17,720 --> 00:00:23,960
with our somewhat newly released data

00:00:20,120 --> 00:00:26,060
quality library called DQ all right so

00:00:23,960 --> 00:00:27,890
the the first question that you that you

00:00:26,060 --> 00:00:29,689
probably that you might be asking

00:00:27,890 --> 00:00:31,940
yourself is okay how is data quality

00:00:29,689 --> 00:00:34,730
relevant to me and and how might it be

00:00:31,940 --> 00:00:38,450
relevant to my team and I'm gonna try to

00:00:34,730 --> 00:00:39,530
motivate it with a few examples and and

00:00:38,450 --> 00:00:41,900
then after that we're gonna have a look

00:00:39,530 --> 00:00:46,339
at a few prominent use cases of our

00:00:41,900 --> 00:00:48,110
library how-to so the obvious case is

00:00:46,339 --> 00:00:50,540
just so you have a single batch of data

00:00:48,110 --> 00:00:52,190
and you want to verify data quality but

00:00:50,540 --> 00:00:53,659
then also we look at the generalization

00:00:52,190 --> 00:00:57,220
of this where you have like ever-growing

00:00:53,659 --> 00:00:59,690
data bases for repetition data sets and

00:00:57,220 --> 00:01:01,309
so yeah our library is open source it's

00:00:59,690 --> 00:01:04,400
hosted on github there will be a link at

00:01:01,309 --> 00:01:07,820
the end of the presentation and yeah

00:01:04,400 --> 00:01:09,979
what I so just to get this out in the

00:01:07,820 --> 00:01:11,630
beginning I think asserting data quality

00:01:09,979 --> 00:01:14,030
before you actually work with the data

00:01:11,630 --> 00:01:16,130
and consume it in downstream consumers I

00:01:14,030 --> 00:01:18,649
think that can we have seen that in the

00:01:16,130 --> 00:01:21,350
past that it can save you from some some

00:01:18,649 --> 00:01:24,170
debugging and error fixing work all

00:01:21,350 --> 00:01:26,689
right so why should we care about data

00:01:24,170 --> 00:01:28,840
quality so the first reason that I want

00:01:26,689 --> 00:01:32,899
to that I want to bring here is because

00:01:28,840 --> 00:01:35,810
data so most of human and algorithmic

00:01:32,899 --> 00:01:37,250
decision decision processes or

00:01:35,810 --> 00:01:40,189
decision-making processes are backed by

00:01:37,250 --> 00:01:42,920
data so that means if the data is just

00:01:40,189 --> 00:01:47,030
wrong or or missing we have the wrong

00:01:42,920 --> 00:01:48,380
conclusions so I think that's maybe the

00:01:47,030 --> 00:01:52,789
most important one because it's so

00:01:48,380 --> 00:01:55,189
general but also what was found is that

00:01:52,789 --> 00:01:57,590
data quality can have an impact on ml

00:01:55,189 --> 00:02:01,429
models and what they found is on a sigma

00:01:57,590 --> 00:02:03,229
2016 tutorial is that data quality can

00:02:01,429 --> 00:02:04,939
be considered sort of as the most

00:02:03,229 --> 00:02:07,579
important hyper parameter of an

00:02:04,939 --> 00:02:10,220
algorithm that you have what they did is

00:02:07,579 --> 00:02:13,580
they took an IMDB movie review movie

00:02:10,220 --> 00:02:15,020
review sentiment data set so they tried

00:02:13,580 --> 00:02:17,450
to predict the sentiment of a data set

00:02:15,020 --> 00:02:20,960
and they just took standard out of the

00:02:17,450 --> 00:02:22,610
box I could learn pipeline with stop

00:02:20,960 --> 00:02:25,910
words removal and all of that and HBO on

00:02:22,610 --> 00:02:27,470
top so hyper parameter optimization but

00:02:25,910 --> 00:02:29,950
what they found is so does

00:02:27,470 --> 00:02:32,090
purple bar here is that they actually

00:02:29,950 --> 00:02:33,560
received much better predictive

00:02:32,090 --> 00:02:36,380
performance after they cleaned up their

00:02:33,560 --> 00:02:39,710
data so they had a quality kind of an

00:02:36,380 --> 00:02:42,830
impact on a male models okay and if you

00:02:39,710 --> 00:02:45,050
if you look at so this little box here

00:02:42,830 --> 00:02:47,450
that's the ML code that's usually

00:02:45,050 --> 00:02:49,730
scikit-learn and and all of that but

00:02:47,450 --> 00:02:52,070
usually in a production scenario ml

00:02:49,730 --> 00:02:54,620
models coexist with other very very

00:02:52,070 --> 00:02:56,360
data-centric components and that might

00:02:54,620 --> 00:02:57,950
be just data collection data

00:02:56,360 --> 00:03:00,500
verification and then of course feature

00:02:57,950 --> 00:03:05,110
extraction and as all of these

00:03:00,500 --> 00:03:07,910
components are so they deal with data

00:03:05,110 --> 00:03:11,690
you might consider also looking at air

00:03:07,910 --> 00:03:14,209
quality for these components to have the

00:03:11,690 --> 00:03:16,880
most efficient okay so the last point

00:03:14,209 --> 00:03:19,850
that I want to make is on operational

00:03:16,880 --> 00:03:21,740
stability so everyone I'm sure knows of

00:03:19,850 --> 00:03:24,230
the infamous nullpointerexception and

00:03:21,740 --> 00:03:26,209
they and these except type of exceptions

00:03:24,230 --> 00:03:28,520
can bring any production system to an

00:03:26,209 --> 00:03:31,790
abrupt halt really and this usually

00:03:28,520 --> 00:03:33,140
happens if there is data missing and

00:03:31,790 --> 00:03:35,840
this is a very obvious case right so

00:03:33,140 --> 00:03:38,180
just systems crash but there's also a

00:03:35,840 --> 00:03:39,560
much more subtle one and so I try to

00:03:38,180 --> 00:03:41,209
depict the Machine loading model with

00:03:39,560 --> 00:03:43,670
this cloud I didn't find a better

00:03:41,209 --> 00:03:45,230
picture but imagine this is a machine

00:03:43,670 --> 00:03:47,060
learning model and it is configured to

00:03:45,230 --> 00:03:49,730
be consuming some upstream data source

00:03:47,060 --> 00:03:51,170
and of course this model is making basic

00:03:49,730 --> 00:03:54,080
assumptions about the data for example

00:03:51,170 --> 00:03:56,420
of the scale of an attribute and here so

00:03:54,080 --> 00:03:58,940
it things that it's square meters but

00:03:56,420 --> 00:04:01,760
maybe at an arbitrary point in time the

00:03:58,940 --> 00:04:03,709
data producer might decide okay well I

00:04:01,760 --> 00:04:06,350
want to change it from square meters to

00:04:03,709 --> 00:04:07,760
square feet and if that happens then

00:04:06,350 --> 00:04:10,580
it's very likely that the model will not

00:04:07,760 --> 00:04:12,320
crash but it will rather produce

00:04:10,580 --> 00:04:14,450
predictions that are slightly off and

00:04:12,320 --> 00:04:17,080
finding these kind of these kind of

00:04:14,450 --> 00:04:20,000
errors is very hard because they don't

00:04:17,080 --> 00:04:25,700
yeah they they don't make some services

00:04:20,000 --> 00:04:30,229
and systems fail really okay so looking

00:04:25,700 --> 00:04:31,850
at quality assurance so split by if we

00:04:30,229 --> 00:04:34,220
either look at software systems or data

00:04:31,850 --> 00:04:36,470
I think in in software systems at least

00:04:34,220 --> 00:04:37,340
we know how to do this very well so

00:04:36,470 --> 00:04:39,800
there's a long and well-established

00:04:37,340 --> 00:04:41,210
practice of how to assure the quality

00:04:39,800 --> 00:04:43,190
and software systems

00:04:41,210 --> 00:04:45,350
so depending on the complexity of the

00:04:43,190 --> 00:04:46,580
component you might just write unit

00:04:45,350 --> 00:04:48,770
tests and if it becomes bigger

00:04:46,580 --> 00:04:49,960
integration and acceptance tests and

00:04:48,770 --> 00:04:52,070
olive and all of that

00:04:49,960 --> 00:04:56,090
oftentimes what we have observed with

00:04:52,070 --> 00:04:58,070
data is that data so verifying the

00:04:56,090 --> 00:05:01,759
quality of your data often has resulted

00:04:58,070 --> 00:05:05,360
in very repetitive and ad-hoc efforts

00:05:01,759 --> 00:05:08,120
and having this sort of approach does

00:05:05,360 --> 00:05:10,370
not really scale in a way and that's why

00:05:08,120 --> 00:05:13,610
we are working and have recent quite

00:05:10,370 --> 00:05:15,110
recently you release DQ this is Eddy the

00:05:13,610 --> 00:05:18,259
melody framework that I'm gonna be

00:05:15,110 --> 00:05:19,940
talking today about and this allows you

00:05:18,259 --> 00:05:23,090
so if you take one thing out of this

00:05:19,940 --> 00:05:24,680
talk please remember you can unit test

00:05:23,090 --> 00:05:29,210
your data with that so that's the basic

00:05:24,680 --> 00:05:29,539
catchphrase of this library okay all

00:05:29,210 --> 00:05:33,110
right

00:05:29,539 --> 00:05:35,330
so the that's schematic overview of DQ

00:05:33,110 --> 00:05:37,520
so here in in yellow and there are

00:05:35,330 --> 00:05:39,889
basically two inputs to DQ so the first

00:05:37,520 --> 00:05:42,229
one is data of course right and the

00:05:39,889 --> 00:05:44,900
other one is you as the user interacting

00:05:42,229 --> 00:05:47,539
with this framework or library and what

00:05:44,900 --> 00:05:50,110
what the user usually does is the user

00:05:47,539 --> 00:05:54,889
defines these data quality constraints

00:05:50,110 --> 00:05:56,180
this is here on the top alright so how

00:05:54,889 --> 00:05:58,520
does this working let's let's look at

00:05:56,180 --> 00:06:00,650
the center of this schematic at the

00:05:58,520 --> 00:06:02,030
center of the queue our metrics

00:06:00,650 --> 00:06:04,430
computations and they are backed by

00:06:02,030 --> 00:06:06,199
SPARC and I will talk about it in a

00:06:04,430 --> 00:06:08,120
minute why that's the case

00:06:06,199 --> 00:06:10,449
and when when these metrics have been

00:06:08,120 --> 00:06:12,199
computed then they meet these

00:06:10,449 --> 00:06:13,310
constraints that have been user

00:06:12,199 --> 00:06:15,680
user-defined

00:06:13,310 --> 00:06:17,720
and then depending on the outcome of the

00:06:15,680 --> 00:06:20,360
of the of the evaluation of these

00:06:17,720 --> 00:06:23,720
constraint verification the tests might

00:06:20,360 --> 00:06:25,039
either fail or succeed right and then

00:06:23,720 --> 00:06:27,199
that will give you an indication of

00:06:25,039 --> 00:06:29,180
whether the data quality is at the

00:06:27,199 --> 00:06:36,080
desired bar that you want to want to be

00:06:29,180 --> 00:06:38,449
a tad mmm all right so a typical unit

00:06:36,080 --> 00:06:40,789
tests in DQ obviously has to scale to

00:06:38,449 --> 00:06:43,550
big datasets so that's the reason why we

00:06:40,789 --> 00:06:45,530
have used Apache spark and most of our

00:06:43,550 --> 00:06:47,830
metrics are formulated as ask your

00:06:45,530 --> 00:06:50,719
aggregation queries over the data

00:06:47,830 --> 00:06:52,789
nothing in our design and acute Isis

00:06:50,719 --> 00:06:54,440
particularly to spark because you could

00:06:52,789 --> 00:06:55,930
be just plugging in any SQL

00:06:54,440 --> 00:06:59,780
client back-end that supports

00:06:55,930 --> 00:07:01,430
user-defined aggregation functions so

00:06:59,780 --> 00:07:03,890
okay given that it's scale to big

00:07:01,430 --> 00:07:05,750
datasets then what it usually does is it

00:07:03,890 --> 00:07:06,950
computes one or several data quality

00:07:05,750 --> 00:07:09,170
metrics for example you might be

00:07:06,950 --> 00:07:12,110
interested in knowing okay what is the

00:07:09,170 --> 00:07:13,610
how many nodes do I have in this set of

00:07:12,110 --> 00:07:16,640
columns or this particular column in

00:07:13,610 --> 00:07:18,530
this data batch and so this would be the

00:07:16,640 --> 00:07:20,870
completeness metric here there are also

00:07:18,530 --> 00:07:22,550
and there's a bunch of others and then

00:07:20,870 --> 00:07:25,310
given given that we have computed these

00:07:22,550 --> 00:07:27,740
metrics with spark in the backend then

00:07:25,310 --> 00:07:29,960
we then we will apply your user-defined

00:07:27,740 --> 00:07:32,180
validation code so you might be

00:07:29,960 --> 00:07:33,710
asserting then okay I know the the

00:07:32,180 --> 00:07:36,440
completeness or there are 2% missing

00:07:33,710 --> 00:07:38,630
values is that acceptable to me as an

00:07:36,440 --> 00:07:40,370
engineer or is that already too much and

00:07:38,630 --> 00:07:47,150
depending on that you can reformulate

00:07:40,370 --> 00:07:48,680
your verification logic okay okay so

00:07:47,150 --> 00:07:51,710
let's have a look at some code because

00:07:48,680 --> 00:07:54,250
maybe this is most actionable alright so

00:07:51,710 --> 00:07:57,200
in the queue we have we expose several

00:07:54,250 --> 00:07:59,630
several api's the first one here is a

00:07:57,200 --> 00:08:02,810
verification suit and the other one is a

00:07:59,630 --> 00:08:04,490
check API and I've bold-faced all of the

00:08:02,810 --> 00:08:07,100
checks here that are data being declared

00:08:04,490 --> 00:08:09,560
here and what this is doing essentially

00:08:07,100 --> 00:08:11,900
is we we have some data spark data frame

00:08:09,560 --> 00:08:13,970
in that case that we want to that we

00:08:11,900 --> 00:08:15,230
want to verify the data on and then

00:08:13,970 --> 00:08:16,910
we're checking for the completeness of

00:08:15,230 --> 00:08:20,270
the customer ID and the title column

00:08:16,910 --> 00:08:22,460
that means we allow for 4 0 null values

00:08:20,270 --> 00:08:25,190
in these two columns on that particular

00:08:22,460 --> 00:08:27,440
batch of data and then we also certain

00:08:25,190 --> 00:08:29,419
the uniqueness of the customer ID that

00:08:27,440 --> 00:08:31,930
means that there should be no duplicates

00:08:29,419 --> 00:08:33,979
on this column over all of the rows and

00:08:31,930 --> 00:08:36,500
then here it becomes a little bit more

00:08:33,979 --> 00:08:40,160
interesting so we also have you can be

00:08:36,500 --> 00:08:42,200
passing in so Adam over here this is

00:08:40,160 --> 00:08:44,240
basically the user-defined validation

00:08:42,200 --> 00:08:46,190
code so what we do is in the backend we

00:08:44,240 --> 00:08:47,690
compute the count distinctive titles so

00:08:46,190 --> 00:08:49,490
the number of distinct titles and there

00:08:47,690 --> 00:08:51,950
in the database there's there's present

00:08:49,490 --> 00:08:53,780
that might be known or there might be a

00:08:51,950 --> 00:08:55,940
value that you would wish for and you

00:08:53,780 --> 00:08:57,980
could assert with any kind of logic that

00:08:55,940 --> 00:09:00,260
you want with this and the interesting

00:08:57,980 --> 00:09:01,970
thing here is that num titles might be

00:09:00,260 --> 00:09:04,580
stemming from another part of your

00:09:01,970 --> 00:09:06,470
system you might even be calling and

00:09:04,580 --> 00:09:07,820
while while the code is executing

00:09:06,470 --> 00:09:09,350
another service

00:09:07,820 --> 00:09:11,350
be getting a hold of the most relevant

00:09:09,350 --> 00:09:14,450
value that you want to assert against

00:09:11,350 --> 00:09:16,910
okay and then yeah some some more you

00:09:14,450 --> 00:09:18,500
can yeah you can also assert against

00:09:16,910 --> 00:09:21,170
histogram values so you can just compute

00:09:18,500 --> 00:09:22,730
the histogram of a particular of a

00:09:21,170 --> 00:09:25,490
particular column in this case device

00:09:22,730 --> 00:09:27,380
type and you want to assert in this case

00:09:25,490 --> 00:09:29,450
at least that it should be no more than

00:09:27,380 --> 00:09:32,120
84 percent of phones present in this

00:09:29,450 --> 00:09:34,210
column and priority should always be

00:09:32,120 --> 00:09:36,320
either high or low

00:09:34,210 --> 00:09:39,650
just one more sentence about this year

00:09:36,320 --> 00:09:42,980
so they in normally checked these checks

00:09:39,650 --> 00:09:45,920
anomaly checks are a bit special as they

00:09:42,980 --> 00:09:47,990
are as they are making use of a metrics

00:09:45,920 --> 00:09:50,500
repository I'm gonna dive into that how

00:09:47,990 --> 00:09:53,450
to use that and how its how its applied

00:09:50,500 --> 00:09:56,210
in a minute but essentially what this

00:09:53,450 --> 00:09:58,870
check is asserting is that the size so

00:09:56,210 --> 00:10:00,860
sorry so the size of this data set is

00:09:58,870 --> 00:10:02,630
essentially similar to previously

00:10:00,860 --> 00:10:08,480
observed ones if you compute that every

00:10:02,630 --> 00:10:10,700
day for example okay now let's have a

00:10:08,480 --> 00:10:12,070
look at UM at a concrete example of how

00:10:10,700 --> 00:10:14,330
you would do that so that would be the

00:10:12,070 --> 00:10:15,800
the code that I just showed you is to

00:10:14,330 --> 00:10:17,510
see the singular data better and you

00:10:15,800 --> 00:10:19,820
want to verify their equality against it

00:10:17,510 --> 00:10:21,290
but now you have ever-growing databases

00:10:19,820 --> 00:10:22,430
for example you ingest impression logs

00:10:21,290 --> 00:10:24,590
and now how do I do that

00:10:22,430 --> 00:10:26,030
if the data is petitioned in a sensible

00:10:24,590 --> 00:10:28,250
way and here I'm assuming daily

00:10:26,030 --> 00:10:29,810
petitions and what we want to do we have

00:10:28,250 --> 00:10:32,200
basically two objections we want to

00:10:29,810 --> 00:10:34,730
assert the data quality on every day

00:10:32,200 --> 00:10:37,670
individually and we want to assert the

00:10:34,730 --> 00:10:41,660
data quality verify the data quality of

00:10:37,670 --> 00:10:43,520
the data overall right so here let's

00:10:41,660 --> 00:10:46,250
have a look at the naive implementation

00:10:43,520 --> 00:10:47,990
first so today right it's Tuesday and

00:10:46,250 --> 00:10:50,450
what we have done here we have applied

00:10:47,990 --> 00:10:51,920
DQ on Sunday we have applied it on

00:10:50,450 --> 00:10:53,990
Monday and the output of these

00:10:51,920 --> 00:10:55,700
computations were always the metrics and

00:10:53,990 --> 00:10:59,990
the outcome of our unit test that they

00:10:55,700 --> 00:11:01,790
succeed or that they fail and then in

00:10:59,990 --> 00:11:03,590
the nave of computation for example on

00:11:01,790 --> 00:11:05,480
Tuesday you might be interested in okay

00:11:03,590 --> 00:11:08,420
wait hang on a second what is the data

00:11:05,480 --> 00:11:10,340
quality of Sunday Monday and Tuesday

00:11:08,420 --> 00:11:12,200
if you if I just Union all of that data

00:11:10,340 --> 00:11:14,270
so in the naive approach what you would

00:11:12,200 --> 00:11:16,220
need to do is you would you would need

00:11:14,270 --> 00:11:18,410
to rescan the data which can be

00:11:16,220 --> 00:11:19,900
arbitrarily costly right because the raw

00:11:18,410 --> 00:11:22,390
data can be can be

00:11:19,900 --> 00:11:24,400
huge and ideally we want to save

00:11:22,390 --> 00:11:27,760
ourselves some computations on the data

00:11:24,400 --> 00:11:30,070
if we can that is what the incremental

00:11:27,760 --> 00:11:31,870
approach is basically addressing so we

00:11:30,070 --> 00:11:34,660
what we're doing exactly the same so

00:11:31,870 --> 00:11:38,170
today is still Tuesday we have to apply

00:11:34,660 --> 00:11:39,640
DQ on Sunday and on Monday however we

00:11:38,170 --> 00:11:41,170
have not only computed the metrics but

00:11:39,640 --> 00:11:43,720
we have also computed intermediate

00:11:41,170 --> 00:11:46,450
states these are these states that are

00:11:43,720 --> 00:11:49,210
on the bottom here and these states are

00:11:46,450 --> 00:11:51,670
you can imagine them sort of as summary

00:11:49,210 --> 00:11:52,990
statistics of the data regarding the

00:11:51,670 --> 00:11:56,110
metrics that you want compute one to

00:11:52,990 --> 00:11:58,390
compute and these these states are much

00:11:56,110 --> 00:12:00,370
much smaller usually than the raw data

00:11:58,390 --> 00:12:03,910
and you can combine them all so

00:12:00,370 --> 00:12:08,110
efficiently and then what you can do is

00:12:03,910 --> 00:12:10,510
maybe I will just show some code so I've

00:12:08,110 --> 00:12:12,370
tried to boldface here a bit so the new

00:12:10,510 --> 00:12:15,700
petition that's that there would be the

00:12:12,370 --> 00:12:17,680
petition of today so Tuesday and from

00:12:15,700 --> 00:12:19,090
and here in this particular example we

00:12:17,680 --> 00:12:22,420
are interested in the completeness of

00:12:19,090 --> 00:12:24,580
the origin column and we compute from

00:12:22,420 --> 00:12:26,620
the we compute the state from there from

00:12:24,580 --> 00:12:29,080
the data this is this summary statistic

00:12:26,620 --> 00:12:30,580
for the completeness metric then in the

00:12:29,080 --> 00:12:32,200
next line what we do is we load the

00:12:30,580 --> 00:12:33,760
previously computed States that we might

00:12:32,200 --> 00:12:37,510
have computed two days ago and one day

00:12:33,760 --> 00:12:39,700
ago and then we we take the Union you

00:12:37,510 --> 00:12:41,230
can call it whatever you want you can

00:12:39,700 --> 00:12:43,380
sum over the states or you can Union

00:12:41,230 --> 00:12:46,180
them and then we arrive at the overall

00:12:43,380 --> 00:12:49,710
table state that includes the state from

00:12:46,180 --> 00:12:52,030
Sunday up up until Tuesday including and

00:12:49,710 --> 00:12:53,350
then given given the state you can

00:12:52,030 --> 00:12:55,870
compute metrics from the state's

00:12:53,350 --> 00:12:58,480
directly and that is probably more

00:12:55,870 --> 00:12:59,860
cheaper than having to scan all of the

00:12:58,480 --> 00:13:01,690
data again and having to load all of

00:12:59,860 --> 00:13:09,760
that so that's the basic motivation for

00:13:01,690 --> 00:13:11,650
this all right so the the second use

00:13:09,760 --> 00:13:14,440
case that I did I want to show that I

00:13:11,650 --> 00:13:17,170
want to show you here is a continuous

00:13:14,440 --> 00:13:18,460
sort of continuous hands more hands of

00:13:17,170 --> 00:13:22,810
the wheel solution to data quality

00:13:18,460 --> 00:13:24,700
verification so imagine the same

00:13:22,810 --> 00:13:26,770
assumption so you compute data quality

00:13:24,700 --> 00:13:29,380
metrics every day and let's imagine this

00:13:26,770 --> 00:13:31,120
value column here I didn't add any units

00:13:29,380 --> 00:13:32,950
but let's say that's the number of rows

00:13:31,120 --> 00:13:35,500
of some of some data frame that you

00:13:32,950 --> 00:13:37,570
inspecting everyday and then this dashed

00:13:35,500 --> 00:13:40,330
line is your your user-defined

00:13:37,570 --> 00:13:43,360
so so your defined threshold that should

00:13:40,330 --> 00:13:45,640
not never be exceeded for example data

00:13:43,360 --> 00:13:47,560
frames should never be larger than two

00:13:45,640 --> 00:13:49,540
million rows right but coming up with

00:13:47,560 --> 00:13:52,390
these absolute numbers can be sometimes

00:13:49,540 --> 00:13:54,970
very tricky so that's what anomaly is

00:13:52,390 --> 00:13:56,950
these anomaly based detection anomaly

00:13:54,970 --> 00:13:59,950
detection based checks are doing they

00:13:56,950 --> 00:14:02,350
are making the assumption that the data

00:13:59,950 --> 00:14:04,510
today should be very similar to the data

00:14:02,350 --> 00:14:06,160
in the metrics space should be very

00:14:04,510 --> 00:14:11,650
similar to the data that we have seen

00:14:06,160 --> 00:14:13,750
earlier okay another code sample and

00:14:11,650 --> 00:14:16,030
here maybe I can spend a minute talking

00:14:13,750 --> 00:14:18,100
about the metrics repository so this is

00:14:16,030 --> 00:14:20,530
exactly the same so we still have a look

00:14:18,100 --> 00:14:22,750
at the verification suit we have a and

00:14:20,530 --> 00:14:24,820
we declare these checks over the data

00:14:22,750 --> 00:14:27,010
and here in this particular instance we

00:14:24,820 --> 00:14:30,270
make use of a file system back to matrix

00:14:27,010 --> 00:14:32,560
repository and the metrics repository is

00:14:30,270 --> 00:14:34,570
just your container where you can write

00:14:32,560 --> 00:14:37,150
metrics to and you can read it from and

00:14:34,570 --> 00:14:39,070
what this anomaly check here the online

00:14:37,150 --> 00:14:40,930
normal in this case is doing it's

00:14:39,070 --> 00:14:43,630
reading the relevant metrics from the

00:14:40,930 --> 00:14:46,570
metrics repository that you that it

00:14:43,630 --> 00:14:48,490
needs to be estimating sort of what is

00:14:46,570 --> 00:14:49,990
the what is a normal data set size

00:14:48,490 --> 00:14:53,650
because here we're looking at the size

00:14:49,990 --> 00:14:57,040
metric on the right hand side and given

00:14:53,650 --> 00:14:59,770
that the the SEM given that this anomaly

00:14:57,040 --> 00:15:01,960
track has a basic idea of what a typical

00:14:59,770 --> 00:15:03,630
data set size might look like it can

00:15:01,960 --> 00:15:06,730
then make the test either fail or

00:15:03,630 --> 00:15:10,840
succeed depending on whether the data is

00:15:06,730 --> 00:15:13,390
similar in that space and we implement a

00:15:10,840 --> 00:15:15,520
number of different very rather simple

00:15:13,390 --> 00:15:18,100
and straightforward strategies so the

00:15:15,520 --> 00:15:21,340
one might just be a simple threshold in

00:15:18,100 --> 00:15:23,290
strategy like laid out here or I'm an

00:15:21,340 --> 00:15:26,530
online normal estimator so that does

00:15:23,290 --> 00:15:29,050
just takes text mean of all of the right

00:15:26,530 --> 00:15:31,030
so it computes the mean of the sizes and

00:15:29,050 --> 00:15:35,280
the standard deviation and make sort of

00:15:31,030 --> 00:15:39,670
makes this normal assumption all right

00:15:35,280 --> 00:15:41,470
so let me let me try to wrap up the talk

00:15:39,670 --> 00:15:43,480
so I hope that I could have convinced

00:15:41,470 --> 00:15:46,300
you that human and algorithmic

00:15:43,480 --> 00:15:46,760
decision-making is mostly data backed at

00:15:46,300 --> 00:15:48,910
least

00:15:46,760 --> 00:15:51,050
should be if you want to be objective

00:15:48,910 --> 00:15:52,970
and for that reason I think we should

00:15:51,050 --> 00:15:54,170
care a lot about the air quality because

00:15:52,970 --> 00:15:56,720
that directly impacts our

00:15:54,170 --> 00:15:58,490
decision-making what we often have found

00:15:56,720 --> 00:16:01,310
is that data quality verification

00:15:58,490 --> 00:16:03,140
results in ad-hoc efforts that are very

00:16:01,310 --> 00:16:08,300
tedious to the engineers implementing

00:16:03,140 --> 00:16:10,430
that and yeah it is not reproducible and

00:16:08,300 --> 00:16:14,200
not standardized in a way and for that

00:16:10,430 --> 00:16:16,190
reason we have released the hue that is

00:16:14,200 --> 00:16:17,540
basically enabling you to have a

00:16:16,190 --> 00:16:18,980
framework for all of this where you

00:16:17,540 --> 00:16:21,020
don't have to write custom scripts to

00:16:18,980 --> 00:16:23,540
check for the completeness the sizes and

00:16:21,020 --> 00:16:25,580
all of that and it gives you hope we

00:16:23,540 --> 00:16:27,530
hope at least a concise API to come up

00:16:25,580 --> 00:16:31,550
to declare these checks over the data

00:16:27,530 --> 00:16:34,850
and also as part of the cue as I as I've

00:16:31,550 --> 00:16:36,170
shown earlier we enable the use cases of

00:16:34,850 --> 00:16:38,150
ever-growing databases that are

00:16:36,170 --> 00:16:40,130
partitioned in some sensible manner and

00:16:38,150 --> 00:16:43,070
then also the more hands of the wheel

00:16:40,130 --> 00:16:47,030
solution with anomaly detection based

00:16:43,070 --> 00:16:49,160
checks if you are if you are interested

00:16:47,030 --> 00:16:51,560
in all of the nitty-gritty details of

00:16:49,160 --> 00:16:53,840
the experiments that we ran before I'm

00:16:51,560 --> 00:16:56,150
releasing the library we have a real DB

00:16:53,840 --> 00:16:58,070
paper please feel free to check it out

00:16:56,150 --> 00:17:00,680
please feel free to check it out and

00:16:58,070 --> 00:17:02,750
yeah definitely give the it's the red

00:17:00,680 --> 00:17:04,490
power look if you want and we also have

00:17:02,750 --> 00:17:07,850
quite recently actually released the

00:17:04,490 --> 00:17:09,380
m-80s big data blog post thanks for your

00:17:07,850 --> 00:17:11,690
attention I'm happy to take questions

00:17:09,380 --> 00:17:14,949
now

00:17:11,690 --> 00:17:14,949
[Applause]

00:17:15,669 --> 00:17:30,919
any questions Oh first of all thanks

00:17:26,720 --> 00:17:33,200
also Philip and as I said DQ'd currently

00:17:30,919 --> 00:17:37,840
just works on batches of data is there

00:17:33,200 --> 00:17:41,470
any plans to have it for streaming data

00:17:37,840 --> 00:17:44,150
yeah that that's a great question I mean

00:17:41,470 --> 00:17:48,020
so at the moment we don't support it the

00:17:44,150 --> 00:17:50,809
direct streaming case however I mean you

00:17:48,020 --> 00:17:53,150
can always batch stream data if that

00:17:50,809 --> 00:17:54,320
helps but at the moment unfortunately my

00:17:53,150 --> 00:18:04,190
answer is that we don't support that

00:17:54,320 --> 00:18:05,950
kind of use case hi we think it's really

00:18:04,190 --> 00:18:08,419
good work we're actually using DQ and

00:18:05,950 --> 00:18:10,820
but how does it compare with I mean T

00:18:08,419 --> 00:18:13,490
effects with the data validation and in

00:18:10,820 --> 00:18:15,410
particular they impute a schema based on

00:18:13,490 --> 00:18:17,630
a given training data and they ever does

00:18:15,410 --> 00:18:19,220
adjacent file and then when new data

00:18:17,630 --> 00:18:21,410
comes in they'll automatically validated

00:18:19,220 --> 00:18:24,410
and print out the you know the

00:18:21,410 --> 00:18:27,340
differences and and and retros that

00:18:24,410 --> 00:18:30,830
break that give any plans for that and

00:18:27,340 --> 00:18:33,470
what about Python maybe you take any mo

00:18:30,830 --> 00:18:35,830
file as a you know I mean any any

00:18:33,470 --> 00:18:38,450
thoughts on the you know the road map

00:18:35,830 --> 00:18:41,000
yeah the road map is pretty much open

00:18:38,450 --> 00:18:43,460
we've had several requests to support

00:18:41,000 --> 00:18:45,890
also Python based environments for

00:18:43,460 --> 00:18:47,390
example PI spark there are some

00:18:45,890 --> 00:18:50,990
components that I didn't talk about that

00:18:47,390 --> 00:18:52,580
are part of DQ that is the an automatic

00:18:50,990 --> 00:18:54,290
constraint suggestion that maybe comes a

00:18:52,580 --> 00:18:56,960
bit closer to what you have access in

00:18:54,290 --> 00:19:00,970
terms of schema validation because we

00:18:56,960 --> 00:19:04,460
try to automate the degeneration of the

00:19:00,970 --> 00:19:06,290
of the constraint verification logic so

00:19:04,460 --> 00:19:08,360
we look at some data and we do that for

00:19:06,290 --> 00:19:10,010
for some amount of time and then we

00:19:08,360 --> 00:19:13,610
probably have a good idea of what all of

00:19:10,010 --> 00:19:15,440
these constraints should look like but

00:19:13,610 --> 00:19:17,090
but as I bet as you ask them so the road

00:19:15,440 --> 00:19:21,309
map is pretty much open and we're happy

00:19:17,090 --> 00:19:21,309
to to take requests

00:19:24,080 --> 00:19:29,990
any more questions we have one minute I

00:19:33,080 --> 00:19:38,429
would like to ask two questions first

00:19:36,179 --> 00:19:41,760
one is how many people are working on

00:19:38,429 --> 00:19:45,510
this project that changes from time to

00:19:41,760 --> 00:19:49,110
time or a couple definitely okay okay

00:19:45,510 --> 00:19:52,620
I'm the second vision I see two use

00:19:49,110 --> 00:19:58,380
cases first one it's like because DQ is

00:19:52,620 --> 00:20:02,610
a unit test library so I could write at

00:19:58,380 --> 00:20:07,110
the spark job and then then create some

00:20:02,610 --> 00:20:14,100
artificial data and test and write tests

00:20:07,110 --> 00:20:20,669
in DQ and check it or I can I can have a

00:20:14,100 --> 00:20:24,990
like data trigger job which I would run

00:20:20,669 --> 00:20:28,200
every day for example so what's the the

00:20:24,990 --> 00:20:32,520
use case what's the primary use case for

00:20:28,200 --> 00:20:34,590
the it's like to run it's regularly or

00:20:32,520 --> 00:20:38,789
to run it from time to time and I want

00:20:34,590 --> 00:20:41,370
to check that my my version correctly

00:20:38,789 --> 00:20:44,159
yeah so my answer is definitely gonna be

00:20:41,370 --> 00:20:46,919
you you you yeah you should aim of doing

00:20:44,159 --> 00:20:50,370
that continuously every day okay because

00:20:46,919 --> 00:20:52,020
if you do that that way that only then

00:20:50,370 --> 00:20:55,860
you have the best idea of how your data

00:20:52,020 --> 00:20:57,690
behaves and yeah so we have seen abrupt

00:20:55,860 --> 00:20:58,950
changes and data distributions and if

00:20:57,690 --> 00:21:01,200
you don't do that every day you might

00:20:58,950 --> 00:21:03,000
just be missing the point and once the

00:21:01,200 --> 00:21:04,740
data has been basically propagated to

00:21:03,000 --> 00:21:05,669
all of your consumers then it's already

00:21:04,740 --> 00:21:07,980
too late right

00:21:05,669 --> 00:21:09,990
so because then they have may be

00:21:07,980 --> 00:21:12,330
consumed already this faulty data that

00:21:09,990 --> 00:21:14,220
might have been caught I'm saying might

00:21:12,330 --> 00:21:17,760
here but might have been caught too by

00:21:14,220 --> 00:21:17,909
data quality verifications okay thank

00:21:17,760 --> 00:21:21,000
you

00:21:17,909 --> 00:21:22,020
Thanks thank you very much Philippa

00:21:21,000 --> 00:21:25,190
Thanks

00:21:22,020 --> 00:21:25,190

YouTube URL: https://www.youtube.com/watch?v=Y2vzoAJl7ro


