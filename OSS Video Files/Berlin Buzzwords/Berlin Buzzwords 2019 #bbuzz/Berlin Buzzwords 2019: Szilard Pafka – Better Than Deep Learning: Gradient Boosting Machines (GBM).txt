Title: Berlin Buzzwords 2019: Szilard Pafka â€“ Better Than Deep Learning: Gradient Boosting Machines (GBM)
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	With all the hype about deep learning and "AI", it is not well publicized that for structured/tabular data widely encountered in business applications it is actually another machine learning algorithm, the gradient boosting machine (GBM) that most often achieves the highest accuracy in supervised learning tasks. 

In this talk we'll review some of the main GBM implementations available as R and Python packages such as xgboost, h2o, lightgbm etc, we'll discuss some of their main features and characteristics, and we'll see how tuning GBMs and creating ensembles of the best models can achieve the best prediction accuracy for many business problems.

Read more:
https://2019.berlinbuzzwords.de/19/session/better-deep-learning-gradient-boosting-machines-gbm

About Szilard Pafka:
https://2019.berlinbuzzwords.de/users/szilard-pafka

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:07,350 --> 00:00:10,380
thank you for having me how many of you

00:00:09,390 --> 00:00:14,630
have heard of

00:00:10,380 --> 00:00:17,510
learning so yeah 99

00:00:14,630 --> 00:00:19,820
much everyone very good forget deep

00:00:17,510 --> 00:00:21,619
learning for now so I'm going I'm here

00:00:19,820 --> 00:00:27,050
to talk about something way better than

00:00:21,619 --> 00:00:30,590
deep learning so before that a few words

00:00:27,050 --> 00:00:32,930
about myself so 2025 years ago I was

00:00:30,590 --> 00:00:35,930
doing physics and like most of my

00:00:32,930 --> 00:00:39,920
colleagues I switched to finance and

00:00:35,930 --> 00:00:43,910
then almost 15 years ago I moved to

00:00:39,920 --> 00:00:44,420
California to do what later and now it's

00:00:43,910 --> 00:00:50,600
called

00:00:44,420 --> 00:00:52,700
data science I also found it then

00:00:50,600 --> 00:00:55,520
organized the first the very first data

00:00:52,700 --> 00:01:00,020
meetup in Los Angeles and I also been

00:00:55,520 --> 00:01:06,680
teaching at two universities one in

00:01:00,020 --> 00:01:09,020
Europe and one in California and I think

00:01:06,680 --> 00:01:11,479
I have some unpopular opinions which I'm

00:01:09,020 --> 00:01:13,549
going to express some of them here so

00:01:11,479 --> 00:01:15,470
don't blame my employer they are really

00:01:13,549 --> 00:01:20,060
nice people and I'm really grateful they

00:01:15,470 --> 00:01:22,490
let me come here talk so how many of you

00:01:20,060 --> 00:01:25,399
have seen this slide from Andrea and

00:01:22,490 --> 00:01:27,619
this is from a couple of years ago so

00:01:25,399 --> 00:01:30,950
Andrea angee is one of the most famous

00:01:27,619 --> 00:01:35,420
person in machine learning deep learning

00:01:30,950 --> 00:01:37,069
ai world and he's been saying this for

00:01:35,420 --> 00:01:39,530
many many years that basically deep

00:01:37,069 --> 00:01:41,450
learning beats every other machine

00:01:39,530 --> 00:01:45,319
learning algorithm especially if you

00:01:41,450 --> 00:01:45,859
have enough data and of course deep

00:01:45,319 --> 00:01:48,049
learning

00:01:45,859 --> 00:01:50,270
combined with with reinforcement

00:01:48,049 --> 00:01:53,509
learning and other techniques had has

00:01:50,270 --> 00:01:55,759
had tremendous success in for example

00:01:53,509 --> 00:01:59,810
beating world champions in various games

00:01:55,759 --> 00:02:03,950
and also we helped that it will solve

00:01:59,810 --> 00:02:08,690
race soon driving so we don't have to

00:02:03,950 --> 00:02:13,240
drive and some people are hoping that AI

00:02:08,690 --> 00:02:17,810
is coming very soon whatever that means

00:02:13,240 --> 00:02:19,970
so I think this is very nice but but the

00:02:17,810 --> 00:02:21,950
truth is a little bit more nuanced so

00:02:19,970 --> 00:02:25,590
indeed deep learning has had a

00:02:21,950 --> 00:02:29,370
tremendous success in computer vision

00:02:25,590 --> 00:02:33,390
and also in somewhat in predicting

00:02:29,370 --> 00:02:35,849
sequences like text lsdm for example and

00:02:33,390 --> 00:02:40,260
also combined with reinforcement

00:02:35,849 --> 00:02:42,269
learning as I was talking about it has

00:02:40,260 --> 00:02:44,190
tremendous success in virtual

00:02:42,269 --> 00:02:48,180
environments like games where you can

00:02:44,190 --> 00:02:50,940
create as much data as you want but this

00:02:48,180 --> 00:02:54,930
is not really the case in businesses and

00:02:50,940 --> 00:03:01,260
also I'm kind of skeptical about its

00:02:54,930 --> 00:03:04,200
deep learning is really AI so I mean why

00:03:01,260 --> 00:03:07,310
some people have have been doing machine

00:03:04,200 --> 00:03:10,380
learning for twenty thirty years with

00:03:07,310 --> 00:03:12,959
practical business applications for

00:03:10,380 --> 00:03:16,769
example fraud detection crazy scoring

00:03:12,959 --> 00:03:18,420
marketing all these areas I've been

00:03:16,769 --> 00:03:19,980
doing machine learning in this for like

00:03:18,420 --> 00:03:22,019
15 years there have been some other

00:03:19,980 --> 00:03:26,040
people doing this for like twenty

00:03:22,019 --> 00:03:28,319
twenty-five and even more years also

00:03:26,040 --> 00:03:31,560
machine learning has been successful in

00:03:28,319 --> 00:03:36,480
many many other domains like telcos term

00:03:31,560 --> 00:03:40,010
prediction insurance car manufacturing

00:03:36,480 --> 00:03:44,609
or any kind of manufacturing of fog

00:03:40,010 --> 00:03:47,880
detection and many many other domains so

00:03:44,609 --> 00:03:52,140
a couple of years ago I I tried in some

00:03:47,880 --> 00:03:53,790
domains to beat traditional machine

00:03:52,140 --> 00:03:59,910
learning algorithms with deep learning

00:03:53,790 --> 00:04:05,100
and I couldn't so in some domains simply

00:03:59,910 --> 00:04:08,130
other algorithms were better and a bit

00:04:05,100 --> 00:04:10,769
later here is tienkie chen who is author

00:04:08,130 --> 00:04:14,329
of the most popular of one of the most

00:04:10,769 --> 00:04:17,519
popular boosting radium boosting

00:04:14,329 --> 00:04:19,950
implementation has talked at my meetup

00:04:17,519 --> 00:04:21,870
in Los Angeles and basically he

00:04:19,950 --> 00:04:24,539
mentioned that more than half of the

00:04:21,870 --> 00:04:27,380
cattles are not win with deep learning

00:04:24,539 --> 00:04:33,210
but actually with gradient boosting

00:04:27,380 --> 00:04:37,470
machines so this is kind of my answer to

00:04:33,210 --> 00:04:39,389
end ranks like so if you have a problem

00:04:37,470 --> 00:04:41,879
in business we

00:04:39,389 --> 00:04:45,000
you have tabular structural data that

00:04:41,879 --> 00:04:48,030
comes from a relational database then

00:04:45,000 --> 00:04:50,330
it's much more likely that gradient

00:04:48,030 --> 00:04:53,159
boosting will perform will outperform

00:04:50,330 --> 00:04:57,389
neural nets or deep learning or call it

00:04:53,159 --> 00:04:59,370
AI if you really want and even cago CEO

00:04:57,389 --> 00:05:02,699
Anthony has been saying that a little

00:04:59,370 --> 00:05:06,960
bit later that basically in most of this

00:05:02,699 --> 00:05:10,710
kind of tabular data problem problems XG

00:05:06,960 --> 00:05:15,870
boost has been the the winning algorithm

00:05:10,710 --> 00:05:18,150
in cattles but for some of us doing

00:05:15,870 --> 00:05:22,919
machine learning for many many years

00:05:18,150 --> 00:05:25,650
this is not surprising so here are two

00:05:22,919 --> 00:05:28,050
papers from 2006 which I consider

00:05:25,650 --> 00:05:30,210
excellent and still not outdated so

00:05:28,050 --> 00:05:32,430
these guys have looked at a lot of

00:05:30,210 --> 00:05:35,819
algorithms on a lot of data sets and

00:05:32,430 --> 00:05:38,069
have compared accuracy and the top

00:05:35,819 --> 00:05:41,490
algorithms were random forests neural

00:05:38,069 --> 00:05:46,500
nets boosting support vector machines so

00:05:41,490 --> 00:05:48,719
it's not all neural nets and I actually

00:05:46,500 --> 00:05:52,949
been using random forests and gradient

00:05:48,719 --> 00:05:56,310
boosting around 2007 those were my first

00:05:52,949 --> 00:05:59,159
machine learning models in production

00:05:56,310 --> 00:06:02,960
and I could obtain very very good

00:05:59,159 --> 00:06:06,419
accuracy on various business problems

00:06:02,960 --> 00:06:08,939
but algorithms not everything so you

00:06:06,419 --> 00:06:11,279
need to clean your data you need to do

00:06:08,939 --> 00:06:14,909
feature engineering and if you really

00:06:11,279 --> 00:06:17,219
want very top accuracy then you need to

00:06:14,909 --> 00:06:20,879
do many many models and you need to

00:06:17,219 --> 00:06:23,360
unsampled them so this is how you win ok

00:06:20,879 --> 00:06:27,210
go for example so you do all this work

00:06:23,360 --> 00:06:30,389
and ultimately you have to do all the

00:06:27,210 --> 00:06:32,729
work that's done that's been done many

00:06:30,389 --> 00:06:34,740
many years ago it was called data mining

00:06:32,729 --> 00:06:38,009
and now it's called data science so you

00:06:34,740 --> 00:06:40,110
have to explore the data clean transform

00:06:38,009 --> 00:06:43,819
the data do the modeling validate the

00:06:40,110 --> 00:06:46,319
models and all these things and

00:06:43,819 --> 00:06:48,509
ultimately I would say that if you step

00:06:46,319 --> 00:06:50,250
back even further then it's not really

00:06:48,509 --> 00:06:52,830
about the tools it's really about

00:06:50,250 --> 00:06:54,720
solving business problems so

00:06:52,830 --> 00:06:57,150
you have to understand the problem and

00:06:54,720 --> 00:07:03,930
map it in the right way to machine

00:06:57,150 --> 00:07:06,659
learning so um but if you really and

00:07:03,930 --> 00:07:08,699
this talk is about the algorithm or the

00:07:06,659 --> 00:07:10,860
algorithm so if you really want some

00:07:08,699 --> 00:07:13,220
kind of key quick advice on what kind of

00:07:10,860 --> 00:07:17,120
problems what kind of algorithms to use

00:07:13,220 --> 00:07:21,840
taking all this into consideration then

00:07:17,120 --> 00:07:22,470
here you go if you have tabular data try

00:07:21,840 --> 00:07:24,960
first

00:07:22,470 --> 00:07:27,270
gradient boosting or random forests if

00:07:24,960 --> 00:07:29,430
you have very small data like a thousand

00:07:27,270 --> 00:07:31,169
observations then almost everything

00:07:29,430 --> 00:07:35,129
other than linear models some very

00:07:31,169 --> 00:07:39,000
simple models with overfit if you have a

00:07:35,129 --> 00:07:43,169
lot of data then GBM some random forest

00:07:39,000 --> 00:07:46,169
will become computationally too much to

00:07:43,169 --> 00:07:48,750
do so you are back again to some simple

00:07:46,169 --> 00:07:51,330
linear models usually with stochastic

00:07:48,750 --> 00:07:53,940
gradient descent because this is the

00:07:51,330 --> 00:07:57,419
fastest way to to solve this and it

00:07:53,940 --> 00:08:00,659
works on very large datasets and indeed

00:07:57,419 --> 00:08:03,569
if you have image if you have text then

00:08:00,659 --> 00:08:06,060
yeah by all means do use deep learning

00:08:03,569 --> 00:08:10,199
so the planning is really the best for

00:08:06,060 --> 00:08:12,960
that but a better answer would be it

00:08:10,199 --> 00:08:16,349
depends and I think by now you have seen

00:08:12,960 --> 00:08:18,630
that the title of my talk was completely

00:08:16,349 --> 00:08:22,259
misguided it was just to make you come

00:08:18,630 --> 00:08:24,840
here so gradient boosting is not better

00:08:22,259 --> 00:08:27,090
than deep learning it's better at some

00:08:24,840 --> 00:08:29,490
problems and deep learning is better at

00:08:27,090 --> 00:08:32,159
some other problems for example with

00:08:29,490 --> 00:08:35,250
tabular data typically gradient boosting

00:08:32,159 --> 00:08:37,949
is better with images deep learning it's

00:08:35,250 --> 00:08:40,140
way better than anything else so if you

00:08:37,949 --> 00:08:42,599
have images or some other problems where

00:08:40,140 --> 00:08:46,649
the planning shines then by all means

00:08:42,599 --> 00:08:48,959
use deep learning so another thing would

00:08:46,649 --> 00:08:52,350
be just try them all so there are like

00:08:48,959 --> 00:08:54,920
four or five algorithms and like linear

00:08:52,350 --> 00:08:58,110
models random for is gradient boosting

00:08:54,920 --> 00:09:01,709
they are related the new Anette support

00:08:58,110 --> 00:09:04,140
vector machines and maybe logistic

00:09:01,709 --> 00:09:06,040
regressions so just try them all and see

00:09:04,140 --> 00:09:11,649
whatever works better

00:09:06,040 --> 00:09:16,120
your data spend also time tuning the

00:09:11,649 --> 00:09:18,850
models do ensembles if you each accuracy

00:09:16,120 --> 00:09:22,029
really matters then you might need to do

00:09:18,850 --> 00:09:25,990
like 100 models and and doing a sum of

00:09:22,029 --> 00:09:28,300
them and again feature engineering is

00:09:25,990 --> 00:09:30,910
very important how you map the domain

00:09:28,300 --> 00:09:33,610
knowledge of your business problem into

00:09:30,910 --> 00:09:37,240
features into inputs that can be learned

00:09:33,610 --> 00:09:40,660
by by whatever machine algorithm you are

00:09:37,240 --> 00:09:42,190
using but ultimately I've been talking

00:09:40,660 --> 00:09:44,589
mostly about how to get the most

00:09:42,190 --> 00:09:47,589
accurate model but sometimes accuracy is

00:09:44,589 --> 00:09:50,279
not like the most important thing so in

00:09:47,589 --> 00:09:53,259
some cases you might be constrained to

00:09:50,279 --> 00:09:55,720
do a model that you can explain either

00:09:53,259 --> 00:10:00,250
because of regulatory pressure or some

00:09:55,720 --> 00:10:03,190
something else so in that case maybe a

00:10:00,250 --> 00:10:08,079
linear model is is better so it

00:10:03,190 --> 00:10:11,050
basically it depends so let's talk about

00:10:08,079 --> 00:10:13,660
gradient boosting though so gradient

00:10:11,050 --> 00:10:16,269
boosting is basically the building block

00:10:13,660 --> 00:10:20,230
is trees and trees you can think of it

00:10:16,269 --> 00:10:24,010
as like a partitioning of the space

00:10:20,230 --> 00:10:26,949
which splits by various variables at

00:10:24,010 --> 00:10:30,910
very split so basically from the data we

00:10:26,949 --> 00:10:35,199
learn this structure the tree all those

00:10:30,910 --> 00:10:36,850
plates and also to split variables in

00:10:35,199 --> 00:10:40,930
the split point so we learned this from

00:10:36,850 --> 00:10:45,880
data and this is one tree and how do we

00:10:40,930 --> 00:10:49,630
get gradient boosting from trees here is

00:10:45,880 --> 00:10:53,410
an example of another tree from another

00:10:49,630 --> 00:10:55,689
problem and before gradient boosting I

00:10:53,410 --> 00:11:01,569
would say a few words about edibles so

00:10:55,689 --> 00:11:05,439
this is this came in 97 so edibles

00:11:01,569 --> 00:11:09,399
builds trees sequentially and we will

00:11:05,439 --> 00:11:11,680
average them and each tree is trying to

00:11:09,399 --> 00:11:14,319
minimize the errors that the other trees

00:11:11,680 --> 00:11:16,899
before made and the way we do this is

00:11:14,319 --> 00:11:19,630
that those of separation that were

00:11:16,899 --> 00:11:21,430
misclassified by the previous trees the

00:11:19,630 --> 00:11:23,949
I've waited so that the next three

00:11:21,430 --> 00:11:26,230
focuses more on those trees and this is

00:11:23,949 --> 00:11:29,589
how we build tree after tree and then we

00:11:26,230 --> 00:11:33,250
stop at some point and gradient boosting

00:11:29,589 --> 00:11:36,160
is something similar we build

00:11:33,250 --> 00:11:38,769
iteratively trees I don't expect you to

00:11:36,160 --> 00:11:41,350
understand this it doesn't really you

00:11:38,769 --> 00:11:44,740
don't really need to especially in the

00:11:41,350 --> 00:11:47,410
first round you know in order to to go

00:11:44,740 --> 00:11:52,120
and use gradient boosting but basically

00:11:47,410 --> 00:11:56,310
it's it's a gradient descent algorithm

00:11:52,120 --> 00:11:56,310
in the functional space of these trees

00:11:56,339 --> 00:12:02,970
more importantly what if you want to use

00:11:59,860 --> 00:12:05,860
gradient boosting what kind of packages

00:12:02,970 --> 00:12:09,720
that are open source and are available

00:12:05,860 --> 00:12:12,519
for you to use so I talked to people I

00:12:09,720 --> 00:12:14,500
looked up cattle forums I looked up on

00:12:12,519 --> 00:12:18,550
the internet which are the packages that

00:12:14,500 --> 00:12:21,160
people are using and this a few years

00:12:18,550 --> 00:12:23,170
ago when I did this this were the ones

00:12:21,160 --> 00:12:25,959
that most people were using that were

00:12:23,170 --> 00:12:28,120
open source and why open source because

00:12:25,959 --> 00:12:30,880
it's free but not also because of that

00:12:28,120 --> 00:12:35,920
because there are great communities

00:12:30,880 --> 00:12:38,110
meetups conferences like this also there

00:12:35,920 --> 00:12:40,870
is very good documentation nowadays so

00:12:38,110 --> 00:12:42,970
there are tons of books on our Python if

00:12:40,870 --> 00:12:45,279
you do data science you can ask

00:12:42,970 --> 00:12:47,649
questions of Stack Overflow sometimes

00:12:45,279 --> 00:12:50,939
you get better documentation then with

00:12:47,649 --> 00:12:55,449
with very expensive paid products and

00:12:50,939 --> 00:12:59,110
all of these packages are available from

00:12:55,449 --> 00:13:01,990
our Brighton which most they design this

00:12:59,110 --> 00:13:05,980
who are using open source are a kind of

00:13:01,990 --> 00:13:08,649
using so a couple of years ago I started

00:13:05,980 --> 00:13:11,230
this little benchmark because I couldn't

00:13:08,649 --> 00:13:15,990
find any internet any information on the

00:13:11,230 --> 00:13:18,939
Internet that was kind of comparing this

00:13:15,990 --> 00:13:22,589
packages so I kind of put it on myself

00:13:18,939 --> 00:13:26,350
but it's still kind of like a limited

00:13:22,589 --> 00:13:27,760
benchmarks but if you go to this repo

00:13:26,350 --> 00:13:30,880
you would see the route this kind of

00:13:27,760 --> 00:13:33,520
scalability in graphs that I'm not going

00:13:30,880 --> 00:13:36,130
to go in much detail now

00:13:33,520 --> 00:13:38,560
what's more important is which are the

00:13:36,130 --> 00:13:40,660
algorithms that are the best and I think

00:13:38,560 --> 00:13:43,990
that based on this all this work the

00:13:40,660 --> 00:13:46,930
best are X G boost h2o and then

00:13:43,990 --> 00:13:51,370
something that came later by GBM that's

00:13:46,930 --> 00:13:55,480
open source by Microsoft and all the

00:13:51,370 --> 00:13:58,060
three packages they have or libraries

00:13:55,480 --> 00:14:00,790
they they have an R package or AB item

00:13:58,060 --> 00:14:05,860
package that you can download and start

00:14:00,790 --> 00:14:09,910
using it in like one minute so why not

00:14:05,860 --> 00:14:12,190
spark kind of interestingly there is a

00:14:09,910 --> 00:14:14,260
big difference between various

00:14:12,190 --> 00:14:16,810
implementations even the most people

00:14:14,260 --> 00:14:19,690
implement the same algorithm from the

00:14:16,810 --> 00:14:23,470
same book still there is like a hundred

00:14:19,690 --> 00:14:26,050
X and even a 10 X + e 100x difference

00:14:23,470 --> 00:14:29,230
between various implementation when it

00:14:26,050 --> 00:14:32,560
comes to running time so you're not

00:14:29,230 --> 00:14:35,140
gonna wait there 100x time if you have a

00:14:32,560 --> 00:14:38,170
better library so I'm gonna give you a

00:14:35,140 --> 00:14:41,440
little bit later more numbers on the

00:14:38,170 --> 00:14:46,600
spark but spark is just not really good

00:14:41,440 --> 00:14:49,029
for gradient boosting but ok so you

00:14:46,600 --> 00:14:52,029
would say that I have big data so how

00:14:49,029 --> 00:14:54,579
can I do machine learning on all this my

00:14:52,029 --> 00:15:00,810
Big Data so maybe you don't have big

00:14:54,579 --> 00:15:04,060
data or actually there are surveys that

00:15:00,810 --> 00:15:05,890
what kind of sizes people are using for

00:15:04,060 --> 00:15:09,250
for analytics and for machine learning

00:15:05,890 --> 00:15:12,010
the sizes are smaller and especially

00:15:09,250 --> 00:15:15,820
because you might have bigger old data

00:15:12,010 --> 00:15:18,070
but when you do machine learning you

00:15:15,820 --> 00:15:20,200
don't do it on on clicks for example you

00:15:18,070 --> 00:15:22,089
do it on users so you do some

00:15:20,200 --> 00:15:25,390
aggregation of the raw data and some

00:15:22,089 --> 00:15:27,370
pre-processing some refinement and by

00:15:25,390 --> 00:15:30,970
the time you get to do machine learning

00:15:27,370 --> 00:15:33,700
you have this model matrix so data might

00:15:30,970 --> 00:15:36,940
matrix is usually called that it's much

00:15:33,700 --> 00:15:38,920
smaller is basically the number of items

00:15:36,940 --> 00:15:42,550
you are doing machine learning on let's

00:15:38,920 --> 00:15:44,140
say your users and then times the number

00:15:42,550 --> 00:15:46,270
of columns the number of features you

00:15:44,140 --> 00:15:48,730
have

00:15:46,270 --> 00:15:51,279
I don't know maybe if you 10,000,000

00:15:48,730 --> 00:15:56,490
user 100 million user a few companies

00:15:51,279 --> 00:15:59,740
have billions of human users right and

00:15:56,490 --> 00:16:02,470
RAM is plentiful and cheap so you can

00:15:59,740 --> 00:16:05,770
get hundreds of gigabytes of RAM for

00:16:02,470 --> 00:16:09,100
very cheap also you can just go to the

00:16:05,770 --> 00:16:11,050
cloud this is one provider and then when

00:16:09,100 --> 00:16:12,959
I started the benchmarks this was the

00:16:11,050 --> 00:16:18,640
biggest Ram you could get on one machine

00:16:12,959 --> 00:16:22,089
so this was 250 gigs of RAM now nowadays

00:16:18,640 --> 00:16:24,550
you have 4 terabyte of RAM on this kind

00:16:22,089 --> 00:16:27,940
of instance and if you have special

00:16:24,550 --> 00:16:30,990
customer then they give you also like 12

00:16:27,940 --> 00:16:37,029
terabyte of RAM in one single machine

00:16:30,990 --> 00:16:39,520
and I also looked at how data set size

00:16:37,029 --> 00:16:42,160
is increased from the surveys and it

00:16:39,520 --> 00:16:45,490
seems like around 20 percent per year

00:16:42,160 --> 00:16:49,089
while Ram on easy to do around 50

00:16:45,490 --> 00:16:52,060
percent per year so Ram goo just way

00:16:49,089 --> 00:16:56,550
faster so more and more data actually

00:16:52,060 --> 00:16:59,740
fits on the RAM on one single server and

00:16:56,550 --> 00:17:02,440
this was kind of controversial all these

00:16:59,740 --> 00:17:04,660
things I was saying couple of years ago

00:17:02,440 --> 00:17:07,390
but I think by now people have realized

00:17:04,660 --> 00:17:11,110
so this is a survey on Twitter so

00:17:07,390 --> 00:17:14,110
probably meaningless done about a year

00:17:11,110 --> 00:17:16,030
ago so I asked what do you care you want

00:17:14,110 --> 00:17:19,120
your machine learning to work on bigger

00:17:16,030 --> 00:17:22,449
data or to be faster or you don't care

00:17:19,120 --> 00:17:27,459
at all so most people want faster

00:17:22,449 --> 00:17:31,960
machine learning and fast matters we

00:17:27,459 --> 00:17:34,510
like fast because you need to when you

00:17:31,960 --> 00:17:36,520
do machine learning you need to do cross

00:17:34,510 --> 00:17:39,309
validation you might need to do hyper

00:17:36,520 --> 00:17:42,940
parameter tuning which means run a lot

00:17:39,309 --> 00:17:45,400
of models maybe hundred models you want

00:17:42,940 --> 00:17:48,670
an samples again you might need to run

00:17:45,400 --> 00:17:52,090
hundreds of models so if one model takes

00:17:48,670 --> 00:17:57,450
a day you that's not very good so the

00:17:52,090 --> 00:18:00,300
faster the better and

00:17:57,450 --> 00:18:01,710
forget my old benchmark so that was a

00:18:00,300 --> 00:18:05,850
little bit too broad so now I

00:18:01,710 --> 00:18:09,450
concentrated lately on just the gbm's so

00:18:05,850 --> 00:18:15,840
here's another repo comparing just the

00:18:09,450 --> 00:18:18,240
best tools not spark and then I made

00:18:15,840 --> 00:18:20,430
this very easily reproducible so

00:18:18,240 --> 00:18:22,760
basically there is a docker fire for

00:18:20,430 --> 00:18:29,400
this so you can just with this command

00:18:22,760 --> 00:18:31,440
reproduce all my results on and you can

00:18:29,400 --> 00:18:36,260
run it on your data if you want but this

00:18:31,440 --> 00:18:39,930
is on some public data set on a million

00:18:36,260 --> 00:18:43,080
records 10 million records so you can

00:18:39,930 --> 00:18:45,990
see that now late by GBM which is kind

00:18:43,080 --> 00:18:54,930
of the new a Chinese tool is kind of the

00:18:45,990 --> 00:18:56,940
fastest also GPUs is a big high because

00:18:54,930 --> 00:18:59,610
of deep learning but actually it's been

00:18:56,940 --> 00:19:03,680
successful so for other things like

00:18:59,610 --> 00:19:06,060
sequel and tabular data and also for

00:19:03,680 --> 00:19:08,970
gradient boosting so with gradient

00:19:06,060 --> 00:19:11,970
boosting you don't get the speed-up you

00:19:08,970 --> 00:19:16,590
get for newer networks so you don't get

00:19:11,970 --> 00:19:20,460
10x 100x you get more moderate speed ups

00:19:16,590 --> 00:19:23,910
and also these tools are kind of a newer

00:19:20,460 --> 00:19:26,850
so 1/2 years old so they are maybe less

00:19:23,910 --> 00:19:29,220
mature but I think XG boost by now is

00:19:26,850 --> 00:19:35,040
the GPU implementation is pretty mature

00:19:29,220 --> 00:19:37,230
so that's the fastest and unlike larger

00:19:35,040 --> 00:19:42,090
data sets but still if it fits on the

00:19:37,230 --> 00:19:44,220
GPU then it can even be the CPU version

00:19:42,090 --> 00:19:47,840
but all this kind of depends a little

00:19:44,220 --> 00:19:51,630
bit on your data and your CPU and GPU

00:19:47,840 --> 00:19:57,150
hardware so these are like decent CPUs

00:19:51,630 --> 00:19:59,430
and GPUs also it's interesting to look

00:19:57,150 --> 00:20:02,190
at the memory requirement especially on

00:19:59,430 --> 00:20:05,100
larger data sets so I opted here a

00:20:02,190 --> 00:20:10,060
little bit 200 million records so you

00:20:05,100 --> 00:20:13,180
see large EPM runs on 5 gigabytes of

00:20:10,060 --> 00:20:16,330
Ram so it's you don't need hundreds of

00:20:13,180 --> 00:20:20,140
of gigabytes of RAM even for pretty

00:20:16,330 --> 00:20:23,080
large datasets it's running though for

00:20:20,140 --> 00:20:25,030
like five minutes so if you need to run

00:20:23,080 --> 00:20:27,370
like hundreds or thousands of models

00:20:25,030 --> 00:20:33,610
then you might you can parallelize

00:20:27,370 --> 00:20:38,410
easily right also on GPUs the best XG

00:20:33,610 --> 00:20:44,410
boost runs free fast on 100 million

00:20:38,410 --> 00:20:47,950
records on this pretty good GPU and uses

00:20:44,410 --> 00:20:51,430
only six gigabyte of GPU Ram although he

00:20:47,950 --> 00:20:54,160
here the RAM of the server is like

00:20:51,430 --> 00:20:57,640
hundreds of gigabytes one here the RAM

00:20:54,160 --> 00:21:00,430
of the basically the GPU memories is

00:20:57,640 --> 00:21:03,160
just sixteen gigabytes so it needs to

00:21:00,430 --> 00:21:11,620
fit on on the GPU if you want to use

00:21:03,160 --> 00:21:14,230
extra boost so light GBM and extra boost

00:21:11,620 --> 00:21:16,570
are great they are the fastest they are

00:21:14,230 --> 00:21:22,510
faster than a stool yet

00:21:16,570 --> 00:21:25,120
I like h2o for production things because

00:21:22,510 --> 00:21:28,270
it's very easy to deploy it and to

00:21:25,120 --> 00:21:31,750
create like a real time that service

00:21:28,270 --> 00:21:35,410
restful api so basically this is all you

00:21:31,750 --> 00:21:39,090
need you explore the model you run those

00:21:35,410 --> 00:21:44,370
things and you build a wharf I and then

00:21:39,090 --> 00:21:49,000
you run your prediction web service

00:21:44,370 --> 00:21:52,420
which is listening on a HTTP or HTTPS

00:21:49,000 --> 00:21:56,770
port and basically you you can get

00:21:52,420 --> 00:22:01,060
scores with HTTP or else weak requests

00:21:56,770 --> 00:22:03,550
so that's all that's needed I have here

00:22:01,060 --> 00:22:07,200
the full example including the training

00:22:03,550 --> 00:22:10,090
of the model also if you need real-time

00:22:07,200 --> 00:22:13,510
prediction scoring restful api to

00:22:10,090 --> 00:22:18,910
integrate with other things then look at

00:22:13,510 --> 00:22:22,510
h2o and ultimately for machine learning

00:22:18,910 --> 00:22:23,980
it matters not only this kind of feature

00:22:22,510 --> 00:22:26,020
engineering training

00:22:23,980 --> 00:22:29,230
unique modeling part and in relating the

00:22:26,020 --> 00:22:31,450
model but also the deployment and the

00:22:29,230 --> 00:22:34,080
scoring and then what we do with the

00:22:31,450 --> 00:22:37,299
scores we we use them in business

00:22:34,080 --> 00:22:42,130
applications and we have monitoring and

00:22:37,299 --> 00:22:45,790
all this so I have some spark numbers

00:22:42,130 --> 00:22:47,919
though so people didn't like that spark

00:22:45,790 --> 00:22:50,860
I was saying that spark is a hundred

00:22:47,919 --> 00:22:54,669
times slower so I repeated this

00:22:50,860 --> 00:22:58,900
experiments with the latest version a

00:22:54,669 --> 00:23:02,290
few months ago and it's still like 200

00:22:58,900 --> 00:23:05,140
times slower than light GPM so you can

00:23:02,290 --> 00:23:08,740
see that even on larger data set this is

00:23:05,140 --> 00:23:12,669
large ebn number this is spark run time

00:23:08,740 --> 00:23:16,120
this is 200 X and if you have here

00:23:12,669 --> 00:23:18,760
bigger data then it's this is not gonna

00:23:16,120 --> 00:23:20,970
get much better anytime soon right

00:23:18,760 --> 00:23:24,419
so this is like two orders of magnitude

00:23:20,970 --> 00:23:30,340
and it didn't really improve so it's not

00:23:24,419 --> 00:23:33,880
gonna get within 10x same and also like

00:23:30,340 --> 00:23:39,520
it has horrible memory management so it

00:23:33,880 --> 00:23:41,410
uses about 100x RAM so it basically if

00:23:39,520 --> 00:23:43,990
you run it on the same server even with

00:23:41,410 --> 00:23:47,320
hundreds of gigabytes of RAM it will

00:23:43,990 --> 00:23:54,160
just crash it so this was on a server

00:23:47,320 --> 00:23:56,350
with a terabyte of RAM and for for the

00:23:54,160 --> 00:24:00,880
10 million records it basically it

00:23:56,350 --> 00:24:03,960
crashed and meanwhile I GBM uses like

00:24:00,880 --> 00:24:03,960
five gigs of RAM

00:24:08,110 --> 00:24:13,850
but spark people will tell me that I

00:24:11,870 --> 00:24:17,510
have to run it on a cluster okay so

00:24:13,850 --> 00:24:20,990
let's run it on a cluster since he's so

00:24:17,510 --> 00:24:24,680
slow I'm gonna do only ten trees I'm not

00:24:20,990 --> 00:24:27,350
gonna wait for hours or days on 100

00:24:24,680 --> 00:24:29,540
million records it's still running even

00:24:27,350 --> 00:24:32,900
with ten trees it's running for like

00:24:29,540 --> 00:24:34,910
half an hour now it's using a little bit

00:24:32,900 --> 00:24:37,460
less memory so it's not going to crash

00:24:34,910 --> 00:24:40,670
on a terabyte if you have a cluster that

00:24:37,460 --> 00:24:44,360
you have then you have even more RAM so

00:24:40,670 --> 00:24:47,390
that's gonna be fine however if you do

00:24:44,360 --> 00:24:52,820
'xg boost or light GBM on just one

00:24:47,390 --> 00:24:56,690
server 16 cores it's gonna run like 30

00:24:52,820 --> 00:25:00,800
times faster and it's gonna use just a

00:24:56,690 --> 00:25:05,420
little bit of memory so it's way easier

00:25:00,800 --> 00:25:09,770
to scale up this if you need then to

00:25:05,420 --> 00:25:15,910
scale out an ellipse gradient boosting

00:25:09,770 --> 00:25:18,440
machine implementation and if you have a

00:25:15,910 --> 00:25:23,720
spark cluster you can actually beat it

00:25:18,440 --> 00:25:29,270
with even like an old low-end laptop so

00:25:23,720 --> 00:25:32,660
on one core 70 seconds versus 300

00:25:29,270 --> 00:25:41,720
seconds on the smaller data so that it

00:25:32,660 --> 00:25:45,640
fits on the RAM of one small laptop yeah

00:25:41,720 --> 00:25:48,200
I told you it's not gonna be popular so

00:25:45,640 --> 00:25:51,770
what we want him doing machine learning

00:25:48,200 --> 00:25:55,280
in the distributed setting is really

00:25:51,770 --> 00:25:59,810
hard so don't blame the developers of

00:25:55,280 --> 00:26:02,360
spark for this and unfortunately kind of

00:25:59,810 --> 00:26:05,480
where we are now is that we have tools

00:26:02,360 --> 00:26:08,630
that have much less features slow and

00:26:05,480 --> 00:26:13,790
then they're actually also buggy so

00:26:08,630 --> 00:26:15,350
spark is good for ETL maybes I wouldn't

00:26:13,790 --> 00:26:19,460
say it's like the greatest thing but

00:26:15,350 --> 00:26:20,990
it's it's okay for ETL and then if you

00:26:19,460 --> 00:26:24,140
need Grady boosting or much

00:26:20,990 --> 00:26:27,530
learning then spark integrates with h2o

00:26:24,140 --> 00:26:31,280
and also with XG boosts we can use

00:26:27,530 --> 00:26:34,550
either of these two things from spark I

00:26:31,280 --> 00:26:36,980
didn't test this extensively but I

00:26:34,550 --> 00:26:39,740
tested a little bit and they seemed okay

00:26:36,980 --> 00:26:41,929
ish so I don't know if they implemented

00:26:39,740 --> 00:26:45,830
all the features and if it's how stable

00:26:41,929 --> 00:26:48,170
it is production environment but

00:26:45,830 --> 00:26:50,120
ultimately what machine learning is is

00:26:48,170 --> 00:26:54,610
not about big data is more about

00:26:50,120 --> 00:26:59,450
computation a lot of computation so

00:26:54,610 --> 00:27:01,970
people who know about internal structure

00:26:59,450 --> 00:27:05,510
of CPUs in memory and memory hierarchy

00:27:01,970 --> 00:27:08,780
those are gonna be able to write a much

00:27:05,510 --> 00:27:13,820
more powerful gradient boosting and

00:27:08,780 --> 00:27:16,520
neural network libraries then for

00:27:13,820 --> 00:27:19,400
example something that's been developed

00:27:16,520 --> 00:27:22,870
with big data in mind and ultimately now

00:27:19,400 --> 00:27:25,850
we are going to the GPUs and GPUs have

00:27:22,870 --> 00:27:30,830
proved also very useful for gradient

00:27:25,850 --> 00:27:33,320
boosting so one thing is the data

00:27:30,830 --> 00:27:38,960
scientist I want for my tools is also

00:27:33,320 --> 00:27:41,780
like very high level API so that I don't

00:27:38,960 --> 00:27:44,270
need to write a lot of code so if you

00:27:41,780 --> 00:27:49,160
use any of those h2o HD boobs like GBM

00:27:44,270 --> 00:27:52,370
then basically training that gradient

00:27:49,160 --> 00:27:54,740
boosting machine is one of few lines of

00:27:52,370 --> 00:27:57,640
code like you couldn't call this one

00:27:54,740 --> 00:28:00,860
line of code basically so this is just

00:27:57,640 --> 00:28:02,570
you transform it into some kind of data

00:28:00,860 --> 00:28:04,850
structure and that's very efficient for

00:28:02,570 --> 00:28:07,760
machine learning and then this is the

00:28:04,850 --> 00:28:11,059
training you specify some parameters I'm

00:28:07,760 --> 00:28:13,670
gonna say few words about that one slide

00:28:11,059 --> 00:28:17,630
later and then here is you train and

00:28:13,670 --> 00:28:19,880
then here is you predict on new data so

00:28:17,630 --> 00:28:22,070
gradient boosting they have a lot of

00:28:19,880 --> 00:28:25,210
parameters some of the most important

00:28:22,070 --> 00:28:27,440
are the number of trees I explained what

00:28:25,210 --> 00:28:30,080
basically we're training trees and we

00:28:27,440 --> 00:28:33,500
are averaging them the depth of the tree

00:28:30,080 --> 00:28:34,530
the max depth of the tree the learning

00:28:33,500 --> 00:28:37,260
rate

00:28:34,530 --> 00:28:40,520
this prevents somewhat overfitting this

00:28:37,260 --> 00:28:44,220
is the way how we combine the trees and

00:28:40,520 --> 00:28:46,980
then something called core they already

00:28:44,220 --> 00:28:51,870
stopped and I'm gonna mention very soon

00:28:46,980 --> 00:28:54,150
actually now so with gradient boosting

00:28:51,870 --> 00:28:56,490
up the problem similar with new Ornette

00:28:54,150 --> 00:29:00,240
if you train it too long then is gonna

00:28:56,490 --> 00:29:02,130
over fit so you're gonna think that if

00:29:00,240 --> 00:29:03,810
you look at your training set you would

00:29:02,130 --> 00:29:05,910
get better and better accuracy but

00:29:03,810 --> 00:29:08,130
actually on an independent holdout or

00:29:05,910 --> 00:29:10,620
test set you would see that basically

00:29:08,130 --> 00:29:13,560
your accuracy starts decreasing as

00:29:10,620 --> 00:29:17,010
you're overfitting so you have to stop

00:29:13,560 --> 00:29:19,350
here basically if you run it further

00:29:17,010 --> 00:29:21,780
you're gonna and you take this all the

00:29:19,350 --> 00:29:24,420
strees then you're gonna get lower

00:29:21,780 --> 00:29:27,960
accuracy and you just wasted all this

00:29:24,420 --> 00:29:32,690
computation time so stop early it's

00:29:27,960 --> 00:29:34,710
faster you don't wait so much time

00:29:32,690 --> 00:29:36,540
especially if you're doing it in the

00:29:34,710 --> 00:29:40,310
cloud it's gonna cost you money as well

00:29:36,540 --> 00:29:43,110
and it's gonna get better accuracy and

00:29:40,310 --> 00:29:45,450
then doing all this stopping in all the

00:29:43,110 --> 00:29:49,860
stop implementations is very easy so

00:29:45,450 --> 00:29:53,960
it's just like a few parameters to set

00:29:49,860 --> 00:29:53,960
up so look it up

00:29:55,260 --> 00:30:00,450
tuning gradient boosting is not very

00:29:58,350 --> 00:30:03,450
easy there are very good tutorials so

00:30:00,450 --> 00:30:05,370
I'm just gonna point two of them so the

00:30:03,450 --> 00:30:10,110
slides will be available so I'm on

00:30:05,370 --> 00:30:13,830
Twitter I will post the slides so just

00:30:10,110 --> 00:30:15,510
follow me on Twitter and you will see

00:30:13,830 --> 00:30:20,820
where are the slides and you get all

00:30:15,510 --> 00:30:23,400
these links a little bit more about

00:30:20,820 --> 00:30:27,800
tuning you can do manual tuning research

00:30:23,400 --> 00:30:30,980
or random search if you did grid search

00:30:27,800 --> 00:30:33,660
up to now then read this paper

00:30:30,980 --> 00:30:36,300
explaining why random search is almost

00:30:33,660 --> 00:30:40,770
always better than grid search that's

00:30:36,300 --> 00:30:43,230
all I'm gonna say now and I also have a

00:30:40,770 --> 00:30:45,600
github in which I've been doing some

00:30:43,230 --> 00:30:48,299
kind of extensive random parameter

00:30:45,600 --> 00:30:50,759
search here are the parameters

00:30:48,299 --> 00:30:54,089
and it gives you an idea of what kind of

00:30:50,759 --> 00:30:56,339
depths of trees and and what kind of

00:30:54,089 --> 00:31:01,339
learning rate is is best but you have to

00:30:56,339 --> 00:31:04,739
experiment with that on your own problem

00:31:01,339 --> 00:31:08,369
so you would think that if you have a

00:31:04,739 --> 00:31:12,839
lot of course or a lot of CPUs then

00:31:08,369 --> 00:31:16,049
these tools will be faster which it kind

00:31:12,839 --> 00:31:19,830
of depends so here is XG boost on 10

00:31:16,049 --> 00:31:23,279
million records so run on one core two

00:31:19,830 --> 00:31:26,099
cores four cores eight cores and 16

00:31:23,279 --> 00:31:33,209
cores on a CPU so you see it gets faster

00:31:26,099 --> 00:31:36,359
so this is runtime but if you have

00:31:33,209 --> 00:31:39,479
happen to have a server that has two CPU

00:31:36,359 --> 00:31:41,159
sockets so on on in the cloud most

00:31:39,479 --> 00:31:44,969
high-end servers they have multiple

00:31:41,159 --> 00:31:49,609
sockets it means that you will

00:31:44,969 --> 00:31:52,799
experience some kind of slowdown and the

00:31:49,609 --> 00:31:56,399
slowdown is because between the two CPUs

00:31:52,799 --> 00:31:59,159
the inter socket connection is slower so

00:31:56,399 --> 00:32:01,349
if the CPU stores on data on this memory

00:31:59,159 --> 00:32:04,619
bank then it would have to go through

00:32:01,349 --> 00:32:07,529
the other CPU and it causes a slowdown

00:32:04,619 --> 00:32:10,349
so XG boost is just not written in a way

00:32:07,529 --> 00:32:13,769
that it's called Numa aware that will

00:32:10,349 --> 00:32:17,309
deal with this thing and sometimes you

00:32:13,769 --> 00:32:20,190
get outrages slow down for instance here

00:32:17,309 --> 00:32:25,919
is like GBM on one core and here it is

00:32:20,190 --> 00:32:28,499
on like on machine with 64 cores so you

00:32:25,919 --> 00:32:32,489
would see that from here to here you get

00:32:28,499 --> 00:32:34,679
like you get 20 times slower if you run

00:32:32,489 --> 00:32:39,559
on all course then if you run it on one

00:32:34,679 --> 00:32:43,859
single core here the same very

00:32:39,559 --> 00:32:46,169
surprising so if you think that you just

00:32:43,859 --> 00:32:48,329
show it to the biggest machine you can

00:32:46,169 --> 00:32:52,529
find in the cloud and that's not good

00:32:48,329 --> 00:32:54,839
it's gonna be slower so also if you have

00:32:52,529 --> 00:32:58,049
hyper-threaded course then it's gonna

00:32:54,839 --> 00:33:01,630
slow down on these things so what I kind

00:32:58,049 --> 00:33:08,530
of recommend is basically

00:33:01,630 --> 00:33:11,350
learn your CPU structure and Van run a

00:33:08,530 --> 00:33:15,280
training session on just the physical

00:33:11,350 --> 00:33:19,420
cores of one CPU for instance this this

00:33:15,280 --> 00:33:20,920
is a server with two CPU socket and this

00:33:19,420 --> 00:33:23,260
out the hyper threaded course

00:33:20,920 --> 00:33:25,900
so forget about hyper threaded course

00:33:23,260 --> 00:33:28,480
and those are the two sockets so you can

00:33:25,900 --> 00:33:30,970
run like one training session here

00:33:28,480 --> 00:33:33,100
another one here or you can run

00:33:30,970 --> 00:33:36,400
different cross-validation folds or you

00:33:33,100 --> 00:33:38,470
can run different hyper parameters or so

00:33:36,400 --> 00:33:40,540
something that's independent and there

00:33:38,470 --> 00:33:43,030
is no communication between these

00:33:40,540 --> 00:33:45,580
processes and/or threads and those

00:33:43,030 --> 00:33:47,830
threads because if there is then it's

00:33:45,580 --> 00:33:50,140
gonna be slowed down just because these

00:33:47,830 --> 00:33:58,210
tools are not written in a luma aware

00:33:50,140 --> 00:34:03,670
way but even if you write you run it

00:33:58,210 --> 00:34:06,250
only on physical cores so you don't get

00:34:03,670 --> 00:34:08,830
linear scaling so you see here like on

00:34:06,250 --> 00:34:11,710
one core to core 16 cores you don't get

00:34:08,830 --> 00:34:15,610
done on 16 cores you don't get the 16

00:34:11,710 --> 00:34:18,730
times speed up I have to speed up a

00:34:15,610 --> 00:34:21,730
little bit but here basically for

00:34:18,730 --> 00:34:25,420
instance on XG boost on 10 million

00:34:21,730 --> 00:34:27,970
records on one core if that's the unit

00:34:25,420 --> 00:34:31,030
on 16 course is just gonna be three

00:34:27,970 --> 00:34:34,150
times faster and this is kind of the

00:34:31,030 --> 00:34:37,390
best you can get and like on smaller

00:34:34,150 --> 00:34:43,270
data is gonna be even less speed-up so

00:34:37,390 --> 00:34:45,430
mind is kind of scaling things and if

00:34:43,270 --> 00:34:48,940
you have to run like hundreds of models

00:34:45,430 --> 00:34:53,530
at the time this is joint work with a

00:34:48,940 --> 00:34:57,670
friend then we concluded that the best

00:34:53,530 --> 00:35:00,520
way to parallelize and also to obtain

00:34:57,670 --> 00:35:02,680
the most reboot if you need to run let's

00:35:00,520 --> 00:35:08,880
say hundreds of models like in some use

00:35:02,680 --> 00:35:08,880
cases is to just run one model per core

00:35:09,600 --> 00:35:15,160
so that's

00:35:12,280 --> 00:35:17,830
that's how you can train most Hmong more

00:35:15,160 --> 00:35:22,290
models in the same unit on time on the

00:35:17,830 --> 00:35:25,390
same hardware so basically here is a

00:35:22,290 --> 00:35:28,530
comparison table kind of reiterating

00:35:25,390 --> 00:35:32,400
what I was talking a little bit so if

00:35:28,530 --> 00:35:37,660
speed is your what you care the most

00:35:32,400 --> 00:35:40,780
then if you have a CPU then use Lai GBM

00:35:37,660 --> 00:35:42,970
if you have a GPU use XE boost if you

00:35:40,780 --> 00:35:48,460
care a lot about this kind of real-time

00:35:42,970 --> 00:35:53,260
scoring or production then use h2o so

00:35:48,460 --> 00:35:55,390
this is kind of the same also I've been

00:35:53,260 --> 00:35:58,060
asking on Twitter what things people are

00:35:55,390 --> 00:36:01,030
using a lot of my followers using random

00:35:58,060 --> 00:36:04,870
forests gbm's not surprisingly so does

00:36:01,030 --> 00:36:08,560
the history there is some bias here also

00:36:04,870 --> 00:36:10,900
interestingly XG boost is still the most

00:36:08,560 --> 00:36:13,900
used it became very popular with kegels

00:36:10,900 --> 00:36:18,540
and it seems like people just don't know

00:36:13,900 --> 00:36:25,990
about the other libraries maybe but

00:36:18,540 --> 00:36:30,610
SPARC not very much used and CPU versus

00:36:25,990 --> 00:36:34,510
GPU so this was a year ago this is like

00:36:30,610 --> 00:36:38,230
a month ago so basically more and more

00:36:34,510 --> 00:36:41,050
people are using GPUs for Grady boosting

00:36:38,230 --> 00:36:43,150
as well and this is I couldn't do this

00:36:41,050 --> 00:36:44,890
on Twitter because it has only four you

00:36:43,150 --> 00:36:48,250
can only ask a question with four

00:36:44,890 --> 00:36:52,150
options so if we included some other

00:36:48,250 --> 00:36:54,490
libraries then basically this is the

00:36:52,150 --> 00:36:57,580
poor results again around the hundred

00:36:54,490 --> 00:37:01,480
people so kind of people know that this

00:36:57,580 --> 00:37:04,690
we are the top tools and way less people

00:37:01,480 --> 00:37:06,610
are using the nd are the ones so by now

00:37:04,690 --> 00:37:09,280
gbm's have been around this

00:37:06,610 --> 00:37:13,210
implementation for like three four five

00:37:09,280 --> 00:37:17,560
years and people have figure out what's

00:37:13,210 --> 00:37:19,930
the best and also people using this to

00:37:17,560 --> 00:37:23,740
win competitions for example this was a

00:37:19,930 --> 00:37:26,800
couple of months ago and the story here

00:37:23,740 --> 00:37:32,940
is the same do feature engineer

00:37:26,800 --> 00:37:32,940
you slide GBM and XG boost and then

00:37:33,090 --> 00:37:41,080
neural nets are not as good but use them

00:37:36,130 --> 00:37:45,370
anyway in an ensemble and then blend all

00:37:41,080 --> 00:37:48,550
this into an ensemble and basically this

00:37:45,370 --> 00:37:52,420
is kind of the last slide so if you have

00:37:48,550 --> 00:37:55,630
tabular data then look first at deviance

00:37:52,420 --> 00:37:58,900
and not only deep learning and about a

00:37:55,630 --> 00:38:02,800
lot of other repos I have so this will

00:37:58,900 --> 00:38:05,470
be posted with the slides and I think

00:38:02,800 --> 00:38:07,930
I'm just out of time anyway so maybe we

00:38:05,470 --> 00:38:10,140
have time for a question or two thank

00:38:07,930 --> 00:38:10,140
you

00:38:13,710 --> 00:38:17,620
we still have a couple of minutes for

00:38:15,700 --> 00:38:25,120
questions so please raise your hands

00:38:17,620 --> 00:38:27,900
even ask something yes I have a question

00:38:25,120 --> 00:38:30,730
because you compared the speed of doing

00:38:27,900 --> 00:38:32,830
machine learning for example in AWS and

00:38:30,730 --> 00:38:35,950
sparked and did you try also there are

00:38:32,830 --> 00:38:38,590
algorithms provided by AWS they are also

00:38:35,950 --> 00:38:41,200
always bragging that they're faster than

00:38:38,590 --> 00:38:43,060
open source one and actually better

00:38:41,200 --> 00:38:47,050
accuracy so did you try those I think

00:38:43,060 --> 00:38:49,630
it's with sage maker or mmm-hmm no I I

00:38:47,050 --> 00:38:51,490
was playing a little bit a couple of

00:38:49,630 --> 00:38:54,690
weeks ago but I wouldn't say that I

00:38:51,490 --> 00:38:54,690
really tried it so

00:39:08,450 --> 00:39:12,780
if you look into tensorflow

00:39:10,530 --> 00:39:15,300
and installing it on on Pisan you see a

00:39:12,780 --> 00:39:17,820
lot of Fortran code from the seventies

00:39:15,300 --> 00:39:20,610
of the last century popping up in the

00:39:17,820 --> 00:39:22,590
background scipy numpy and synthesis do

00:39:20,610 --> 00:39:26,900
you believe that this could be also

00:39:22,590 --> 00:39:30,710
applied by the profession of numerical

00:39:26,900 --> 00:39:34,200
algorithms and then you get better and

00:39:30,710 --> 00:39:36,210
maybe complete different results so for

00:39:34,200 --> 00:39:38,250
gradient boosting what takes most of the

00:39:36,210 --> 00:39:41,040
time with these implementations and on

00:39:38,250 --> 00:39:43,950
the data sizes I've been talking about

00:39:41,040 --> 00:39:46,740
like millions or ten millions of Records

00:39:43,950 --> 00:39:50,220
is basically doing each split on a

00:39:46,740 --> 00:39:53,280
variable but instead of splitting on any

00:39:50,220 --> 00:39:55,380
possible data points all these three

00:39:53,280 --> 00:39:57,270
best algorithms they are first doing

00:39:55,380 --> 00:40:01,650
some kind of histogram and then they are

00:39:57,270 --> 00:40:04,500
trying all only this splits so like 90

00:40:01,650 --> 00:40:08,810
plus percent of the time takes in

00:40:04,500 --> 00:40:12,090
computing these histograms and basically

00:40:08,810 --> 00:40:15,750
this is different from things that have

00:40:12,090 --> 00:40:20,880
been done in Fortran in the 60s 70s I'm

00:40:15,750 --> 00:40:23,640
really big fan on on on not so shiny

00:40:20,880 --> 00:40:26,670
tech what works and a lot of that is

00:40:23,640 --> 00:40:29,340
written like Fortran code from the 70s

00:40:26,670 --> 00:40:30,870
but in this case is not the case because

00:40:29,340 --> 00:40:35,550
random forests and gradient boosting

00:40:30,870 --> 00:40:41,130
comes from the 90s and this histograms

00:40:35,550 --> 00:40:43,200
are are not as easy to parallelize as

00:40:41,130 --> 00:40:48,330
the neural net that's why you don't get

00:40:43,200 --> 00:40:52,440
the same speed up with GPUs but still

00:40:48,330 --> 00:40:58,050
it's it kind of helps if you so it's

00:40:52,440 --> 00:40:59,940
it's faster now and GPUs but also they

00:40:58,050 --> 00:41:02,790
are improving on the implementation is

00:40:59,940 --> 00:41:07,760
well but it's not going to get as much

00:41:02,790 --> 00:41:10,230
of a speed factor as for the new Anette

00:41:07,760 --> 00:41:11,650
awesome thank you very much all right

00:41:10,230 --> 00:41:16,059
thank you

00:41:11,650 --> 00:41:16,059

YouTube URL: https://www.youtube.com/watch?v=qjuizRba3ZQ


