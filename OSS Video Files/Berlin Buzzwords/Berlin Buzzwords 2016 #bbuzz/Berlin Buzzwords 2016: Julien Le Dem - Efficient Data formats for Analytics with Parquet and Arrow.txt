Title: Berlin Buzzwords 2016: Julien Le Dem - Efficient Data formats for Analytics with Parquet and Arrow
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Hadoop makes it relatively easy to store petabytes of data. However, storing data is not enough; columnar layouts for storage and in-memory execution allow the analysis of large amounts of data very quickly and efficiently. It provides the ability for multiple applications to share a common data representation and perform operations at full CPU throughput using SIMD and Vectorization. 

For interoperability, row based encodings - CSV, Thrift, Avro - combined with general purpose compression algorithms - GZip, LZO, Snappy - are common but inefficient. As discussed extensively in the database literature, a columnar layout with statistics and sorting provides vertical and horizontal partitioning, thus keeping IO to a minimum. Additionally a number of key big data technologies have or will soon have in-memory columnar capabilities. This includes Kudu, Ibis and Drill. Sharing a common in-memory columnar representation allows interoperability without the usual cost of serialization.

Understanding modern CPU architecture is critical to maximizing processing throughput. We’ll discuss the advantages of columnar layouts in Parquet and Arrow for in-memory processing and data encodings used for storage - dictionary, bit-packing, prefix coding. We’ll dissect and explain the design choices that enable us to achieve all three goals of interoperability, space and query efficiency. In addition, we’ll provide an overview of what’s coming in Parquet and Arrow in the next year.

Read more:
https://2016.berlinbuzzwords.de/session/efficient-data-formats-analytics-parquet-and-arrow

About Julien Le Dem:
https://2016.berlinbuzzwords.de/users/julien-le-dem

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:02,480 --> 00:00:08,370
hi so I'm going to talk about parking a

00:00:05,279 --> 00:00:10,860
robust columnar formats one in

00:00:08,370 --> 00:00:13,759
particular is on disk optimized for on

00:00:10,860 --> 00:00:16,740
disk and the other one for in memory so

00:00:13,759 --> 00:00:19,050
before we start just read a bit about

00:00:16,740 --> 00:00:22,080
myself if you're wondering where my

00:00:19,050 --> 00:00:24,779
accent comes from French and I live in

00:00:22,080 --> 00:00:26,490
California so I guess that would be some

00:00:24,779 --> 00:00:30,359
mix but I'm guess it's more French

00:00:26,490 --> 00:00:33,050
accent than anything else so i'm working

00:00:30,359 --> 00:00:36,600
at dreamy oh and we work on big data

00:00:33,050 --> 00:00:38,340
analytics solution before that I worked

00:00:36,600 --> 00:00:42,930
at Twitter on data platform where I

00:00:38,340 --> 00:00:45,510
started working making parque and I'm an

00:00:42,930 --> 00:00:48,030
Apache member and currently being

00:00:45,510 --> 00:00:54,230
involved with parquet and arrow the

00:00:48,030 --> 00:01:00,359
equator formerly walking on pig so today

00:00:54,230 --> 00:01:01,530
I'm going to talk about a few things so

00:01:00,359 --> 00:01:03,899
first I'm going to talk about the

00:01:01,530 --> 00:01:07,080
benefits of columnar formats in

00:01:03,899 --> 00:01:10,860
particular in data processing data

00:01:07,080 --> 00:01:13,080
analysis query execution and then I'm

00:01:10,860 --> 00:01:16,229
going to talk about another very

00:01:13,080 --> 00:01:18,299
important thing is driving standards in

00:01:16,229 --> 00:01:20,430
the community in the open source so that

00:01:18,299 --> 00:01:22,110
we can actually build on ecosystems with

00:01:20,430 --> 00:01:26,119
all those systems because you've been

00:01:22,110 --> 00:01:29,970
hearing about spark and Hadoop and

00:01:26,119 --> 00:01:31,380
Impala and all those things and really

00:01:29,970 --> 00:01:34,170
there are a lot of things you can choose

00:01:31,380 --> 00:01:35,579
from and they can work together but they

00:01:34,170 --> 00:01:40,530
can work together only if we have

00:01:35,579 --> 00:01:43,110
standards data formats in between so

00:01:40,530 --> 00:01:45,299
first I'm going to talk about benefits

00:01:43,110 --> 00:01:48,720
of columnar formats you know with a

00:01:45,299 --> 00:01:54,360
kitten slides so that looks good for the

00:01:48,720 --> 00:01:57,689
internet so columnar layout if you think

00:01:54,360 --> 00:02:00,689
of like any type of table so here have a

00:01:57,689 --> 00:02:03,420
simple example of a logical table we

00:02:00,689 --> 00:02:05,790
have three columns a B and C and we are

00:02:03,420 --> 00:02:07,979
when we were presented it in a computer

00:02:05,790 --> 00:02:10,500
whether it's on disk or in memory you

00:02:07,979 --> 00:02:13,319
need a linear representation when you

00:02:10,500 --> 00:02:13,680
have all the data one bit after the

00:02:13,319 --> 00:02:15,959
other

00:02:13,680 --> 00:02:18,959
so if you choose a row layout you're

00:02:15,959 --> 00:02:22,379
going to put each row after the order so

00:02:18,959 --> 00:02:25,019
you will have a 1 B 1 cm 1 a2 b2 c2 and

00:02:22,379 --> 00:02:27,120
values for each column interleaves and

00:02:25,019 --> 00:02:28,739
each column would be a different type so

00:02:27,120 --> 00:02:30,780
you have data from different types into

00:02:28,739 --> 00:02:32,579
your leave like that when you have a

00:02:30,780 --> 00:02:35,010
columnar layout you choose to put all

00:02:32,579 --> 00:02:37,379
the values for one column together first

00:02:35,010 --> 00:02:39,329
and then all the values for the second

00:02:37,379 --> 00:02:41,400
column and then all the values for the

00:02:39,329 --> 00:02:43,620
third column which means that you're

00:02:41,400 --> 00:02:46,799
going to put together values that are

00:02:43,620 --> 00:02:49,560
all of the same type and now all much

00:02:46,799 --> 00:02:53,909
more ammo genius and that will have

00:02:49,560 --> 00:02:56,159
several advantage advantages and first

00:02:53,909 --> 00:02:58,790
when you're doing analysis and you're

00:02:56,159 --> 00:03:01,409
doing a sequel query on some data set

00:02:58,790 --> 00:03:04,079
typically the data set has many columns

00:03:01,409 --> 00:03:07,019
and maybe dozens maybe hundreds

00:03:04,079 --> 00:03:08,370
depending but when you're doing a query

00:03:07,019 --> 00:03:10,230
in you analyzing something you're

00:03:08,370 --> 00:03:13,079
accessing only a few of those columns so

00:03:10,230 --> 00:03:14,549
the columnar layout will make it easy to

00:03:13,079 --> 00:03:16,650
access only those columns because

00:03:14,549 --> 00:03:19,349
instead of scanning the entire data and

00:03:16,650 --> 00:03:21,329
doing a lot of small cigs on the data

00:03:19,349 --> 00:03:24,329
set you're just going to be able to read

00:03:21,329 --> 00:03:26,400
entire big chunks of the columns you

00:03:24,329 --> 00:03:28,829
need and then do big jumps a few big

00:03:26,400 --> 00:03:32,129
jumps is much more efficient than a lot

00:03:28,829 --> 00:03:34,139
of tiny ones and another aspect is when

00:03:32,129 --> 00:03:36,659
compressing data because we put together

00:03:34,139 --> 00:03:39,180
all values from the same type instead of

00:03:36,659 --> 00:03:40,530
having a string and int string an int we

00:03:39,180 --> 00:03:43,379
have a bunch of string so a bunch of

00:03:40,530 --> 00:03:45,239
integers we can compress all them all

00:03:43,379 --> 00:03:46,470
together much more efficiently for

00:03:45,239 --> 00:03:48,930
example let's say you have an integer

00:03:46,470 --> 00:03:51,359
value and you know the maximum value for

00:03:48,930 --> 00:03:53,909
the entire column then you can use only

00:03:51,359 --> 00:03:57,180
the number of bits that are required for

00:03:53,909 --> 00:03:59,280
this maximum value and you can beat pack

00:03:57,180 --> 00:04:02,639
them together to use a lot less memory

00:03:59,280 --> 00:04:05,159
than four bytes per value and make it

00:04:02,639 --> 00:04:07,079
much more compact and similarly if

00:04:05,159 --> 00:04:11,040
you're compressing strings with regular

00:04:07,079 --> 00:04:15,509
brute force algorithm being it the you

00:04:11,040 --> 00:04:17,759
know either the zip LW family type of

00:04:15,509 --> 00:04:19,320
algorithm more snappy or regular ones

00:04:17,759 --> 00:04:20,849
the algorithm will be much more

00:04:19,320 --> 00:04:23,070
efficient because you're putting

00:04:20,849 --> 00:04:24,840
together things are more homogeneous

00:04:23,070 --> 00:04:27,229
instead of mixing a lot of different

00:04:24,840 --> 00:04:27,229
things together

00:04:28,300 --> 00:04:35,140
so I've been saying that parquet is one

00:04:31,640 --> 00:04:39,800
disk arrow is for in memory but really I

00:04:35,140 --> 00:04:43,010
started by building park a 4 on disk

00:04:39,800 --> 00:04:44,930
storage and really you could ask why

00:04:43,010 --> 00:04:48,440
don't you put just park in memory and

00:04:44,930 --> 00:04:50,300
the main reason is because there are

00:04:48,440 --> 00:04:53,210
different trade-offs that we want to

00:04:50,300 --> 00:04:56,180
make and optimize the format for

00:04:53,210 --> 00:04:59,300
different access patterns so when we

00:04:56,180 --> 00:05:00,980
talk about disk storage the same you're

00:04:59,300 --> 00:05:02,510
going to write the data once and it's

00:05:00,980 --> 00:05:04,430
going to be accessed by many different

00:05:02,510 --> 00:05:07,460
queries that may access different

00:05:04,430 --> 00:05:11,210
columns so the access pattern on which

00:05:07,460 --> 00:05:13,130
column it's accessing its its it has to

00:05:11,210 --> 00:05:15,560
be written once and it's going to be

00:05:13,130 --> 00:05:17,840
reused many times and when we read the

00:05:15,560 --> 00:05:19,400
data it's mostly streaming right we're

00:05:17,840 --> 00:05:21,470
going to read all the values one after

00:05:19,400 --> 00:05:23,540
the other possibly we're going to skip

00:05:21,470 --> 00:05:26,390
world chunks and I'm going to talk about

00:05:23,540 --> 00:05:30,740
that but we mostly access them in order

00:05:26,390 --> 00:05:33,020
and and so there's a priority to I your

00:05:30,740 --> 00:05:35,360
rejection even though we still want good

00:05:33,020 --> 00:05:37,700
cpu throughput but the priority is I

00:05:35,360 --> 00:05:39,920
your addiction in this case for any

00:05:37,700 --> 00:05:42,470
in-memory data structure it's more

00:05:39,920 --> 00:05:44,660
transient so it's being built on the fly

00:05:42,470 --> 00:05:46,910
for the purpose of the execution of a

00:05:44,660 --> 00:05:48,080
query of doing some processing on it

00:05:46,910 --> 00:05:50,600
like for example if you're doing a

00:05:48,080 --> 00:05:53,360
sequel query is going to build only

00:05:50,600 --> 00:05:55,580
those columns that you're crying and the

00:05:53,360 --> 00:05:57,920
access could be easier we are looking at

00:05:55,580 --> 00:05:59,510
all the values are in other cases you

00:05:57,920 --> 00:06:01,250
may want to have random access to

00:05:59,510 --> 00:06:04,160
individual values for example you are

00:06:01,250 --> 00:06:05,570
doing a joint and is doing a joining

00:06:04,160 --> 00:06:08,810
memory it may have want to access

00:06:05,570 --> 00:06:11,720
different values are referring to values

00:06:08,810 --> 00:06:12,830
for indexing in data structures so

00:06:11,720 --> 00:06:15,080
you're doing some indexing and your

00:06:12,830 --> 00:06:16,880
reference index of the value so in

00:06:15,080 --> 00:06:19,430
Parque we don't necessarily optimized

00:06:16,880 --> 00:06:22,070
for constant access to a value in the

00:06:19,430 --> 00:06:26,810
list in arrow it's optimized for

00:06:22,070 --> 00:06:28,640
constant access time of the value and so

00:06:26,810 --> 00:06:31,520
in memory we really want to prioritize

00:06:28,640 --> 00:06:33,470
for CPU stupid so we still need good I

00:06:31,520 --> 00:06:36,910
oh and representation but it's really

00:06:33,470 --> 00:06:39,139
the CPU throughput that's most important

00:06:36,910 --> 00:06:42,860
so now I'm going to talk a little bit

00:06:39,139 --> 00:06:44,509
both first about parque and in more

00:06:42,860 --> 00:06:50,090
details on how it works and then i'll

00:06:44,509 --> 00:06:51,830
talk about error so first they both

00:06:50,090 --> 00:06:54,979
support nested data structures so i

00:06:51,830 --> 00:06:57,199
listed it for both format because real

00:06:54,979 --> 00:06:59,750
life data structures are nested and we

00:06:57,199 --> 00:07:04,300
have lists of nested objects and

00:06:59,750 --> 00:07:09,350
structures and several layers deep and

00:07:04,300 --> 00:07:12,340
second it's a compact format so one

00:07:09,350 --> 00:07:16,190
thing it provides its better compression

00:07:12,340 --> 00:07:19,090
than regular are oriented with brute

00:07:16,190 --> 00:07:21,289
force compression algorithm on top and

00:07:19,090 --> 00:07:23,210
that's because of the type of where

00:07:21,289 --> 00:07:24,889
encodings if we have a bunch of integers

00:07:23,210 --> 00:07:27,740
we know the max value we can make them

00:07:24,889 --> 00:07:30,860
more compact or if we know it's a bunch

00:07:27,740 --> 00:07:33,080
of strings we can also do dictionary

00:07:30,860 --> 00:07:35,360
encoding which means build a dictionary

00:07:33,080 --> 00:07:38,210
and then code only integers and that's

00:07:35,360 --> 00:07:39,919
much more compact another thing like

00:07:38,210 --> 00:07:41,900
that and we get better compression just

00:07:39,919 --> 00:07:43,729
by the columnar layout and putting

00:07:41,900 --> 00:07:46,970
together things that looks the same as

00:07:43,729 --> 00:07:50,930
opposed to interleaving values that look

00:07:46,970 --> 00:07:53,570
different and so in can do up to messiah

00:07:50,930 --> 00:07:55,220
by the projection push down which is

00:07:53,570 --> 00:07:57,409
reading only the columns you need and

00:07:55,220 --> 00:07:59,599
filter push downs which is taking

00:07:57,409 --> 00:08:01,970
advantage of statistics that are part of

00:07:59,599 --> 00:08:09,110
the file to skip entire chunks of the

00:08:01,970 --> 00:08:12,080
file so one of the main advantages of

00:08:09,110 --> 00:08:14,120
columnar storage on disk for queries is

00:08:12,080 --> 00:08:15,919
first you access only the column you

00:08:14,120 --> 00:08:18,409
need because you can do a scan on only

00:08:15,919 --> 00:08:20,419
those columns you need to load and based

00:08:18,409 --> 00:08:23,389
on statistics in the files you can skip

00:08:20,419 --> 00:08:26,000
whole chunks of the data let's say you

00:08:23,389 --> 00:08:27,800
filter by a certain range if you know

00:08:26,000 --> 00:08:30,349
the minimum value in maximum value in

00:08:27,800 --> 00:08:31,760
each chunk you can end those minimum

00:08:30,349 --> 00:08:34,130
value don't match the range you know you

00:08:31,760 --> 00:08:35,719
can skip the entire block you can skip

00:08:34,130 --> 00:08:37,880
entire chunks of the data set so that

00:08:35,719 --> 00:08:40,490
with both vertical and horizontal

00:08:37,880 --> 00:08:45,020
partitioning you limit the i/o to the

00:08:40,490 --> 00:08:47,060
minimum and and so this slide is about

00:08:45,020 --> 00:08:50,570
nesting because all my example on slide

00:08:47,060 --> 00:08:52,780
are simple and flat schemas but really

00:08:50,570 --> 00:08:56,710
parka supports nisti data structure and

00:08:52,780 --> 00:08:59,510
this is just giving an idea of how

00:08:56,710 --> 00:09:01,430
nested structures are turning to a flat

00:08:59,510 --> 00:09:03,650
representation which we capture all the

00:09:01,430 --> 00:09:05,270
nice things with repetition and all this

00:09:03,650 --> 00:09:07,820
and if you want more details there's a

00:09:05,270 --> 00:09:12,260
link on the page with a blog post that

00:09:07,820 --> 00:09:13,970
goes in the details on how that work so

00:09:12,260 --> 00:09:18,800
now I'm going to talk a little bit about

00:09:13,970 --> 00:09:21,050
arrow so like I said it's also supports

00:09:18,800 --> 00:09:23,930
nested data structures and it's

00:09:21,050 --> 00:09:26,720
optimized for CPU store boot on modern

00:09:23,930 --> 00:09:29,990
CPUs and I'll go a little bit into how

00:09:26,720 --> 00:09:32,090
modern CPUs work and why it's important

00:09:29,990 --> 00:09:33,980
to understand how they work to present

00:09:32,090 --> 00:09:36,110
the data in a way that is going to be

00:09:33,980 --> 00:09:40,340
processed efficiently so things to think

00:09:36,110 --> 00:09:41,750
about our pipelining seemly instruction

00:09:40,340 --> 00:09:43,880
are single instruction multiple data

00:09:41,750 --> 00:09:45,830
those are extending instructions on

00:09:43,880 --> 00:09:48,380
processors that say do the same

00:09:45,830 --> 00:09:51,950
operation on those four values at a time

00:09:48,380 --> 00:09:53,780
and cash locality and because processors

00:09:51,950 --> 00:09:56,600
have cash on them that is much faster to

00:09:53,780 --> 00:09:59,150
access than main memory and the other

00:09:56,600 --> 00:10:03,020
aspect of arrow is because the in-memory

00:09:59,150 --> 00:10:05,990
representation is is the format you work

00:10:03,020 --> 00:10:07,730
on it can we can use scatter gather iOS

00:10:05,990 --> 00:10:09,890
there's no serialization when you send

00:10:07,730 --> 00:10:11,990
it over the network there's no extra CPU

00:10:09,890 --> 00:10:13,670
work to transform the in-memory

00:10:11,990 --> 00:10:15,590
representation to the wire

00:10:13,670 --> 00:10:18,050
representation we just send the

00:10:15,590 --> 00:10:20,120
in-memory representation on the wire and

00:10:18,050 --> 00:10:23,960
read it back on the other end and that's

00:10:20,120 --> 00:10:25,910
all you need so typically in systems

00:10:23,960 --> 00:10:27,860
that send it over the wire if you use

00:10:25,910 --> 00:10:29,480
thrift the proto buff or whatever you

00:10:27,860 --> 00:10:31,640
have your in memory representation and

00:10:29,480 --> 00:10:33,800
then you need to serialize it into your

00:10:31,640 --> 00:10:39,200
wire representation which takes a lot of

00:10:33,800 --> 00:10:42,590
CPU so in this mode the goal is to make

00:10:39,200 --> 00:10:46,550
everything one representation and to

00:10:42,590 --> 00:10:49,100
remove all that overhead so this slide

00:10:46,550 --> 00:10:51,440
is about CPU pipelining so if you think

00:10:49,100 --> 00:10:53,240
of modern CPUs you know they don't

00:10:51,440 --> 00:10:55,490
execute one instruction after the other

00:10:53,240 --> 00:10:58,940
anymore so that was very the first sip

00:10:55,490 --> 00:11:03,930
user were made now CPU instructions are

00:10:58,940 --> 00:11:05,940
decomposed into like 12 steps

00:11:03,930 --> 00:11:08,040
and what the CPU is trying to do because

00:11:05,940 --> 00:11:10,470
not all the area of the CPU is used at

00:11:08,040 --> 00:11:11,970
once is trying to prepare start

00:11:10,470 --> 00:11:14,370
processing the next instruction before

00:11:11,970 --> 00:11:15,899
the previous one is finished which means

00:11:14,370 --> 00:11:18,570
we have staggered execution of

00:11:15,899 --> 00:11:20,040
instruction which means that you need to

00:11:18,570 --> 00:11:22,170
be able to know what the next

00:11:20,040 --> 00:11:25,230
instruction is going to be before the

00:11:22,170 --> 00:11:27,420
previous instruction is done and so to

00:11:25,230 --> 00:11:29,730
do that the CPU is trying to guess

00:11:27,420 --> 00:11:32,010
there's a branch predictor what's called

00:11:29,730 --> 00:11:34,020
a branch predictor and the CPU is going

00:11:32,010 --> 00:11:36,300
because sometime if you have a if in

00:11:34,020 --> 00:11:39,020
your code if the value was this do that

00:11:36,300 --> 00:11:41,760
if the value was something I'll do this

00:11:39,020 --> 00:11:43,140
then that's a branch and that's where

00:11:41,760 --> 00:11:45,209
the CPU is going to do one thing or the

00:11:43,140 --> 00:11:47,670
other that means the different execution

00:11:45,209 --> 00:11:49,800
instruction is going to be calm and

00:11:47,670 --> 00:11:51,300
that's called a data dependency right so

00:11:49,800 --> 00:11:54,180
like the next instructions depend on the

00:11:51,300 --> 00:11:56,310
result of the current one which means we

00:11:54,180 --> 00:11:59,160
need to wait which is what you see here

00:11:56,310 --> 00:12:00,810
so here you have for Imaginary

00:11:59,160 --> 00:12:03,089
instruction ABCD and you can imagine

00:12:00,810 --> 00:12:05,450
that's like a tetris break falling to

00:12:03,089 --> 00:12:08,430
the bottom with time so we are moving

00:12:05,450 --> 00:12:10,830
downwards as time goes on and so in a

00:12:08,430 --> 00:12:13,050
perfect world we start and I have a

00:12:10,830 --> 00:12:15,240
pipeline of four steps like imaginary

00:12:13,050 --> 00:12:17,370
pipeline we start preparing instruction

00:12:15,240 --> 00:12:19,860
a and it goes through the CPU pipeline

00:12:17,370 --> 00:12:22,860
and after four cycles of the CPU it's

00:12:19,860 --> 00:12:23,940
done and we do all the temp thing for

00:12:22,860 --> 00:12:26,190
the instruction we assume they all

00:12:23,940 --> 00:12:28,890
independent we can all process them

00:12:26,190 --> 00:12:31,290
staggered and we do you know foreign

00:12:28,890 --> 00:12:34,050
strip on average we do for instruction

00:12:31,290 --> 00:12:38,160
we do one instruction per cycle on

00:12:34,050 --> 00:12:40,950
average but if we were wrong and when we

00:12:38,160 --> 00:12:42,540
else finish realized well wait a minute

00:12:40,950 --> 00:12:45,150
we're not having the right instruction

00:12:42,540 --> 00:12:46,709
we need to start over and start another

00:12:45,150 --> 00:12:49,050
instruction right we need to wait until

00:12:46,709 --> 00:12:52,050
this one is finished then we have what

00:12:49,050 --> 00:12:55,350
you call a bubble and you're losing the

00:12:52,050 --> 00:12:57,000
entire number of CPUs of the lens the

00:12:55,350 --> 00:12:59,640
depth of your pipeline which means you

00:12:57,000 --> 00:13:02,760
can lose up to 12 cycles so there is a

00:12:59,640 --> 00:13:05,399
huge difference between Ryan code that

00:13:02,760 --> 00:13:07,410
will we have an independent instruction

00:13:05,399 --> 00:13:09,060
that can go in power almost in parallel

00:13:07,410 --> 00:13:10,920
versus something that we introduce

00:13:09,060 --> 00:13:13,829
bubbles with the CPU always has to wait

00:13:10,920 --> 00:13:16,170
before starting strong in executing the

00:13:13,829 --> 00:13:17,790
next instruction and so that's where

00:13:16,170 --> 00:13:19,860
columnar is very important

00:13:17,790 --> 00:13:22,200
because you're going to do the exact

00:13:19,860 --> 00:13:24,480
same thing on all the values over and

00:13:22,200 --> 00:13:26,580
over and because you're writing your

00:13:24,480 --> 00:13:28,830
code in that way you can have a tight

00:13:26,580 --> 00:13:31,350
loop that says hey do this on all the

00:13:28,830 --> 00:13:32,520
values so increment index do the

00:13:31,350 --> 00:13:34,080
processing on the value right there

00:13:32,520 --> 00:13:36,060
result increment the index do the

00:13:34,080 --> 00:13:37,650
processing which means is always doing

00:13:36,060 --> 00:13:40,110
the same thing in the tight loop instead

00:13:37,650 --> 00:13:41,940
of you know looking at all the values of

00:13:40,110 --> 00:13:45,090
your top pole and doing different things

00:13:41,940 --> 00:13:47,610
and doing much more efficient and taking

00:13:45,090 --> 00:13:50,430
much more adventure advantage of the

00:13:47,610 --> 00:13:52,560
process of pipeline so that's the reason

00:13:50,430 --> 00:13:56,130
and you know you can refer to the monet

00:13:52,560 --> 00:13:58,620
DB paper which is like the inventor of

00:13:56,130 --> 00:14:01,200
vectorized execution in databases and

00:13:58,620 --> 00:14:03,660
that's basically the main advantage is

00:14:01,200 --> 00:14:06,300
we use the most of the CPU it doesn't

00:14:03,660 --> 00:14:10,290
have to wait to know what could you do

00:14:06,300 --> 00:14:13,650
next and the part about the cash

00:14:10,290 --> 00:14:15,210
locality like you know you have the main

00:14:13,650 --> 00:14:17,610
memory of your computer there's the bus

00:14:15,210 --> 00:14:20,760
in between and there's a CPU and the

00:14:17,610 --> 00:14:23,790
cash is right on the CPU so accessing

00:14:20,760 --> 00:14:25,710
the cash is much faster so the CPU will

00:14:23,790 --> 00:14:27,780
say whenever it needs to access memory

00:14:25,710 --> 00:14:30,860
if we go to the RAM size send me that

00:14:27,780 --> 00:14:33,780
data we go over and then process is it

00:14:30,860 --> 00:14:35,670
and what it will try to do is try to

00:14:33,780 --> 00:14:38,250
keep working on the same amount of

00:14:35,670 --> 00:14:40,110
memory and right on it locally because

00:14:38,250 --> 00:14:42,420
it can do go very fast in reading and

00:14:40,110 --> 00:14:47,280
writing in the cache and only when we

00:14:42,420 --> 00:14:49,950
need to fetch more data we sent over

00:14:47,280 --> 00:14:52,050
data that we are done working with then

00:14:49,950 --> 00:14:53,940
we will have this latency so and here

00:14:52,050 --> 00:14:56,030
every time we go through the bus the

00:14:53,940 --> 00:15:00,450
processors to wait to get the next thing

00:14:56,030 --> 00:15:02,220
so big and in culinary execution because

00:15:00,450 --> 00:15:03,720
we work on one column at a time and we

00:15:02,220 --> 00:15:06,690
do the same thing we get better cash

00:15:03,720 --> 00:15:09,620
locality then if we get all the rows you

00:15:06,690 --> 00:15:12,390
remember when we in a wrong until

00:15:09,620 --> 00:15:13,890
representation in memory we get all the

00:15:12,390 --> 00:15:15,720
different values so we have to execute

00:15:13,890 --> 00:15:18,360
all the instructions for all the values

00:15:15,720 --> 00:15:21,240
like evaluate expression for one or at a

00:15:18,360 --> 00:15:23,400
time so data is more at your genius and

00:15:21,240 --> 00:15:26,490
we do more thing and so there is more

00:15:23,400 --> 00:15:28,830
back and forth between ram and cpu and

00:15:26,490 --> 00:15:31,740
if we do columnar there is more locality

00:15:28,830 --> 00:15:36,270
because we we can keep it tighter to the

00:15:31,740 --> 00:15:40,740
we love working and so all those points

00:15:36,270 --> 00:15:43,080
about CPU efficiency so basically I've

00:15:40,740 --> 00:15:44,880
talked about those things and like it's

00:15:43,080 --> 00:15:46,830
just a representation that show you

00:15:44,880 --> 00:15:49,230
different types and now the interleave

00:15:46,830 --> 00:15:51,150
or they're the same and the one thing

00:15:49,230 --> 00:15:54,180
that you don't talk about was Cindy

00:15:51,150 --> 00:15:56,330
single instruction multiple data and in

00:15:54,180 --> 00:15:58,650
such cases the CPU is operation

00:15:56,330 --> 00:16:00,150
instruction that you say hey process

00:15:58,650 --> 00:16:02,250
that same thing on for value in parallel

00:16:00,150 --> 00:16:04,920
so when you use those instruction it's

00:16:02,250 --> 00:16:06,870
playing for X throughput right because

00:16:04,920 --> 00:16:09,900
it's just a processor belief or for

00:16:06,870 --> 00:16:13,050
instruction in parallel totally and so

00:16:09,900 --> 00:16:14,970
that's thanks to column narrow

00:16:13,050 --> 00:16:16,350
presentation that's very easy to say hey

00:16:14,970 --> 00:16:21,300
we're going to do four times the same

00:16:16,350 --> 00:16:23,400
instruction again and again so to give

00:16:21,300 --> 00:16:26,940
you a little idea of how the arrow

00:16:23,400 --> 00:16:30,060
nested or presentation works if we look

00:16:26,940 --> 00:16:33,510
at name is a variable length value right

00:16:30,060 --> 00:16:36,450
it's a string so in value the first

00:16:33,510 --> 00:16:39,450
value is 3 bytes long the second value

00:16:36,450 --> 00:16:41,910
is 4 bytes long so you have an upset

00:16:39,450 --> 00:16:45,870
array that delimits points to the

00:16:41,910 --> 00:16:49,260
beginning of each value age is a fixed

00:16:45,870 --> 00:16:50,550
lens value so you have a plane array of

00:16:49,260 --> 00:16:54,840
all the values one after the other

00:16:50,550 --> 00:16:58,350
because a fixed length and for phones

00:16:54,840 --> 00:17:00,630
with choose a list of strings you can

00:16:58,350 --> 00:17:04,650
see that you can compose those things

00:17:00,630 --> 00:17:06,420
right so we have a variable lens values

00:17:04,650 --> 00:17:08,700
which are strings and I'm you know

00:17:06,420 --> 00:17:10,380
truncating it here so that it fits in

00:17:08,700 --> 00:17:12,300
the slide where's the offset that

00:17:10,380 --> 00:17:15,270
delimits the strings and then we have

00:17:12,300 --> 00:17:17,630
another upset vector that points to the

00:17:15,270 --> 00:17:20,130
beginning of each string so the first

00:17:17,630 --> 00:17:23,580
list has two elements the second list at

00:17:20,130 --> 00:17:26,280
one element so you can see that the lens

00:17:23,580 --> 00:17:28,050
are presented by the first second offset

00:17:26,280 --> 00:17:29,730
we have two elements 1 elements and they

00:17:28,050 --> 00:17:33,110
both point to the beginning of the

00:17:29,730 --> 00:17:33,110
values in there

00:17:34,890 --> 00:17:42,060
so another thing for the in-memory

00:17:36,960 --> 00:17:44,640
processing is arrow comes with a memory

00:17:42,060 --> 00:17:47,250
allocator based on Nettie and a notion

00:17:44,640 --> 00:17:50,100
of tree tree of alligators so that he

00:17:47,250 --> 00:17:51,810
can deal with quotas and providing

00:17:50,100 --> 00:17:55,320
memory because when you have a query

00:17:51,810 --> 00:17:57,660
execution you want to understand which

00:17:55,320 --> 00:17:59,780
operator in the query execution is doing

00:17:57,660 --> 00:18:02,760
what and how much memory it's using and

00:17:59,780 --> 00:18:04,860
limit how much its operator is using for

00:18:02,760 --> 00:18:06,870
example you may have a hash join and

00:18:04,860 --> 00:18:09,300
also another apparent that applies a

00:18:06,870 --> 00:18:11,580
function to the values and they both

00:18:09,300 --> 00:18:13,830
will use memory and you want to be able

00:18:11,580 --> 00:18:16,260
to allocate different code as so that

00:18:13,830 --> 00:18:20,760
the join may want to spill data to disk

00:18:16,260 --> 00:18:24,870
and things like that so now the the

00:18:20,760 --> 00:18:26,400
other very important part of this is to

00:18:24,870 --> 00:18:29,340
build that as a community-driven

00:18:26,400 --> 00:18:35,250
standard because it's not just about the

00:18:29,340 --> 00:18:37,950
technology the thing is there's a common

00:18:35,250 --> 00:18:39,990
need like a lot of project and I'm sure

00:18:37,950 --> 00:18:42,450
some quotes are looking into columnar

00:18:39,990 --> 00:18:44,730
execution because since the monet DB

00:18:42,450 --> 00:18:46,140
paper that's a natural evolution that's

00:18:44,730 --> 00:18:49,380
how we are going to make all those

00:18:46,140 --> 00:18:52,790
sequel execution or query much faster

00:18:49,380 --> 00:18:57,330
like in Impala in sparks equal in drill

00:18:52,790 --> 00:18:59,190
in all those systems and so all those

00:18:57,330 --> 00:19:02,520
project are looking at those things and

00:18:59,190 --> 00:19:05,580
so the goal is to make that we may as

00:19:02,520 --> 00:19:07,410
well all do it the same way and at least

00:19:05,580 --> 00:19:09,900
for the in-memory representation and

00:19:07,410 --> 00:19:12,060
that will have a lot of benefits for the

00:19:09,900 --> 00:19:13,760
ecosystem and any bling inter

00:19:12,060 --> 00:19:17,160
communication between all those system

00:19:13,760 --> 00:19:18,720
the other benefits of course is to share

00:19:17,160 --> 00:19:20,790
the effort and building that thing

00:19:18,720 --> 00:19:25,260
together instead of reinventing the same

00:19:20,790 --> 00:19:27,060
thing ten times but truly sharing the

00:19:25,260 --> 00:19:28,560
effort is something that that's one

00:19:27,060 --> 00:19:30,780
driving force but that would not be

00:19:28,560 --> 00:19:32,250
enough right like it's not because it's

00:19:30,780 --> 00:19:33,630
the right thing to do to implement

00:19:32,250 --> 00:19:35,550
something once that people have

00:19:33,630 --> 00:19:37,830
different opinion but the other driving

00:19:35,550 --> 00:19:40,590
vector to agreeing on how we want we're

00:19:37,830 --> 00:19:43,020
going to do this is to be able to be

00:19:40,590 --> 00:19:45,300
interoperable laughter the way and so

00:19:43,020 --> 00:19:47,370
bail on success stuff park a park is

00:19:45,300 --> 00:19:48,720
started as like just Clara and the

00:19:47,370 --> 00:19:51,360
Impala team

00:19:48,720 --> 00:19:53,610
and me at twitter trying to build a

00:19:51,360 --> 00:19:56,370
standard and after being very open to

00:19:53,610 --> 00:19:58,679
accept the drill community is a spark

00:19:56,370 --> 00:20:01,770
community and then it started becoming a

00:19:58,679 --> 00:20:03,780
standard kind of grass root thing and it

00:20:01,770 --> 00:20:05,370
worked really well but now this

00:20:03,780 --> 00:20:07,350
community is built right those people

00:20:05,370 --> 00:20:08,809
already have interacted and negotiated

00:20:07,350 --> 00:20:11,340
how's this thing is supposed to work

00:20:08,809 --> 00:20:13,260
based on that now it's easier to build

00:20:11,340 --> 00:20:16,140
aro and use the same thing for the

00:20:13,260 --> 00:20:19,049
in-memory processing so it's really a

00:20:16,140 --> 00:20:21,330
start up from the start and so I had a

00:20:19,049 --> 00:20:23,460
few quotes by the impala team but the

00:20:21,330 --> 00:20:26,940
spark team talking about the drill team

00:20:23,460 --> 00:20:28,590
about you know how vectorized execution

00:20:26,940 --> 00:20:32,510
and column nine memory is important and

00:20:28,590 --> 00:20:32,510
I'm sure you've heard it in other tops

00:20:32,870 --> 00:20:37,650
so some of the arrow goals are to have a

00:20:35,730 --> 00:20:41,760
well-documented and cross-language

00:20:37,650 --> 00:20:42,840
compatible spec and you know I talked

00:20:41,760 --> 00:20:45,450
about how it's designed to take

00:20:42,840 --> 00:20:47,820
advantage of modern CPUs and it's

00:20:45,450 --> 00:20:51,090
unbeatable in those all those execution

00:20:47,820 --> 00:20:53,330
and jeans and the goal is not to replace

00:20:51,090 --> 00:20:56,220
a huge part of everything the goal is to

00:20:53,330 --> 00:20:59,250
agree on the format and be inter parable

00:20:56,220 --> 00:21:01,950
have a few libraries in common and let

00:20:59,250 --> 00:21:04,049
every engine innovate in what's is

00:21:01,950 --> 00:21:06,450
specific to them whether it's flink

00:21:04,049 --> 00:21:08,789
order it sparks equal whether it's

00:21:06,450 --> 00:21:11,990
Impala whether it's Apache drill all

00:21:08,789 --> 00:21:14,340
those things provide their own value and

00:21:11,990 --> 00:21:15,780
you know i mean this common library

00:21:14,340 --> 00:21:19,620
doesn't prevent them from doing their

00:21:15,780 --> 00:21:22,260
own thing but it's interoperable the

00:21:19,620 --> 00:21:24,390
arrow project started so it's officially

00:21:22,260 --> 00:21:28,049
was created at the apache foundation and

00:21:24,390 --> 00:21:32,460
in February and the member of the PMC

00:21:28,049 --> 00:21:34,650
come from all those all those projects

00:21:32,460 --> 00:21:39,360
so you are going to recognize a lot of

00:21:34,650 --> 00:21:41,010
Apache project are the things and all

00:21:39,360 --> 00:21:42,900
those people are getting together and

00:21:41,010 --> 00:21:45,179
the goal is to make sure it's really

00:21:42,900 --> 00:21:47,789
something that we all agree on so that

00:21:45,179 --> 00:21:51,179
it really becomes a success in it's the

00:21:47,789 --> 00:21:53,970
beginning of the ecosystem so things

00:21:51,179 --> 00:21:58,850
like you recognized in this both grey

00:21:53,970 --> 00:22:02,580
engines like drill Impala and pandas

00:21:58,850 --> 00:22:05,700
spark storm or

00:22:02,580 --> 00:22:09,570
and also storage layers like parquet

00:22:05,700 --> 00:22:12,059
could you Hadoop Cassandra and those

00:22:09,570 --> 00:22:15,149
things will have to interact very

00:22:12,059 --> 00:22:17,490
closely I will talk a little bit about

00:22:15,149 --> 00:22:21,870
the angle to build this ecosystem and

00:22:17,490 --> 00:22:23,460
why it's important so today when you

00:22:21,870 --> 00:22:25,590
want to integrate things you cannot mean

00:22:23,460 --> 00:22:28,620
one-to-one integration in all those

00:22:25,590 --> 00:22:30,779
systems like for example parque victor

00:22:28,620 --> 00:22:32,909
is code there some that lives in spark

00:22:30,779 --> 00:22:34,950
some of that leaves in drill some that

00:22:32,909 --> 00:22:36,960
lives in impala and they all like their

00:22:34,950 --> 00:22:40,200
own slightly different implementation of

00:22:36,960 --> 00:22:42,450
it and they all have their own in-memory

00:22:40,200 --> 00:22:44,700
representation that is easier columnar

00:22:42,450 --> 00:22:47,850
or not some are oriented in memory some

00:22:44,700 --> 00:22:49,320
are culinary ented and its really it's a

00:22:47,850 --> 00:22:50,820
mess like every project needs to

00:22:49,320 --> 00:22:52,919
integrate with the other one they need

00:22:50,820 --> 00:22:54,269
to find a format that will be standing

00:22:52,919 --> 00:22:56,039
in between the two there's probably

00:22:54,269 --> 00:22:57,809
going to be serialization disser ization

00:22:56,039 --> 00:23:00,029
to convert from one format to the other

00:22:57,809 --> 00:23:03,480
it's inefficient and there's a lot of

00:23:00,029 --> 00:23:05,639
work duplication and typically when you

00:23:03,480 --> 00:23:08,130
hear about talks about people who do

00:23:05,639 --> 00:23:10,529
profiling they realize that a lot of CPU

00:23:08,130 --> 00:23:12,299
is wasted on serialization dissertation

00:23:10,529 --> 00:23:14,909
but it's something that you're stuck

00:23:12,299 --> 00:23:17,460
with if all the system didn't agree on

00:23:14,909 --> 00:23:22,070
what format they were going to talk up

00:23:17,460 --> 00:23:26,460
from so with error noticed like complex

00:23:22,070 --> 00:23:29,010
simple your arrow becomes like a

00:23:26,460 --> 00:23:31,559
standard that helps make things easy to

00:23:29,010 --> 00:23:34,159
communicate because if everyone agree on

00:23:31,559 --> 00:23:36,630
the in memory format like we're doing

00:23:34,159 --> 00:23:38,130
then they can integrate once and then

00:23:36,630 --> 00:23:40,350
everything works together because there

00:23:38,130 --> 00:23:43,019
is a common columnar format that is

00:23:40,350 --> 00:23:44,639
efficient for processing and also is

00:23:43,019 --> 00:23:47,340
standard so we can just send it across

00:23:44,639 --> 00:23:49,559
the wire there is no average for

00:23:47,340 --> 00:23:52,019
cross-system communication so we get a

00:23:49,559 --> 00:23:54,480
lot of CPU saving not by optimizing

00:23:52,019 --> 00:23:57,929
stuff but just by agreeing on how we're

00:23:54,480 --> 00:23:59,130
going to transfer it and there's

00:23:57,929 --> 00:24:01,710
opportunities for sharing of

00:23:59,130 --> 00:24:03,510
functionality for example you can make

00:24:01,710 --> 00:24:05,610
something that converts parka to a row

00:24:03,510 --> 00:24:08,100
in memory sure make it really good

00:24:05,610 --> 00:24:10,529
shares it once and then everybody's

00:24:08,100 --> 00:24:13,409
using it so now you know instead of

00:24:10,529 --> 00:24:14,940
having variable performance querying

00:24:13,409 --> 00:24:16,020
parquet depending on the system you're

00:24:14,940 --> 00:24:18,360
using like

00:24:16,020 --> 00:24:21,450
currently for example hives some time is

00:24:18,360 --> 00:24:23,820
less good than spark or drill that are

00:24:21,450 --> 00:24:26,070
really committed to use park it very

00:24:23,820 --> 00:24:27,660
well instead like everybody will have

00:24:26,070 --> 00:24:29,970
like very good benefits and we don't

00:24:27,660 --> 00:24:34,730
like spend the effort in each project to

00:24:29,970 --> 00:24:38,010
make that very good so language bindings

00:24:34,730 --> 00:24:39,750
parka started with I did all the Java

00:24:38,010 --> 00:24:43,380
binding so they were like Java libraries

00:24:39,750 --> 00:24:47,280
that were used in sparks equal in drill

00:24:43,380 --> 00:24:49,020
in many other places and there was C++

00:24:47,280 --> 00:24:51,420
code but that was like tightly coupled

00:24:49,020 --> 00:24:53,700
with the Impala code now there is an

00:24:51,420 --> 00:24:56,280
effort by West McKinney works at

00:24:53,700 --> 00:24:59,460
Cloudera has been doing a very good CPP

00:24:56,280 --> 00:25:01,920
c++ library that can be reused for the

00:24:59,460 --> 00:25:04,830
c++ code in particular is interested in

00:25:01,920 --> 00:25:07,050
integrating with python and pandas which

00:25:04,830 --> 00:25:09,510
is like big data library like for people

00:25:07,050 --> 00:25:13,920
who use are they like to use pandas in

00:25:09,510 --> 00:25:16,050
python as well and on arrow there's a

00:25:13,920 --> 00:25:18,870
java implementation the c++

00:25:16,050 --> 00:25:23,160
implementation same usual suspects is

00:25:18,870 --> 00:25:24,840
being done by Wes and once you have a

00:25:23,160 --> 00:25:28,970
c++ implementation you can integrate

00:25:24,840 --> 00:25:32,130
with all those native code based code

00:25:28,970 --> 00:25:36,750
JVM languages like Java or Skala can use

00:25:32,130 --> 00:25:39,960
the Java implementation and the primary

00:25:36,750 --> 00:25:42,770
initial focus we say here it's a raid

00:25:39,960 --> 00:25:46,650
right manage memory because there's well

00:25:42,770 --> 00:25:49,410
RPC layers that could be shared and

00:25:46,650 --> 00:25:50,820
right now it's not a common library but

00:25:49,410 --> 00:25:52,530
the main thing is once you agree on the

00:25:50,820 --> 00:25:55,290
format you don't need to put all the

00:25:52,530 --> 00:25:58,670
tools in one library but it's valuable

00:25:55,290 --> 00:26:01,700
to share the effort on some of those pcs

00:25:58,670 --> 00:26:05,910
so I'm going to talk a little bit about

00:26:01,700 --> 00:26:08,420
the RPC remote procedure call and IPC

00:26:05,910 --> 00:26:12,360
inter-process communication which

00:26:08,420 --> 00:26:14,300
basically either sending the data to

00:26:12,360 --> 00:26:16,470
some other machine on the cluster are

00:26:14,300 --> 00:26:20,820
sharing the data in between two

00:26:16,470 --> 00:26:24,180
processes on the same node so a little

00:26:20,820 --> 00:26:26,760
bit on how you look at the data so we

00:26:24,180 --> 00:26:28,999
say the representation is columnar but

00:26:26,760 --> 00:26:30,799
we don't really load the interior

00:26:28,999 --> 00:26:33,319
cassette in memory write the data set

00:26:30,799 --> 00:26:37,219
will be logically split in record

00:26:33,319 --> 00:26:39,049
batches likes in here and really when

00:26:37,219 --> 00:26:40,579
you're doing a core execution our

00:26:39,049 --> 00:26:44,869
machine learning or whatever it is on

00:26:40,579 --> 00:26:46,399
this data in memory you're going to work

00:26:44,869 --> 00:26:48,379
on the subset of data at a time it's

00:26:46,399 --> 00:26:50,119
just world map reduce thing right you

00:26:48,379 --> 00:26:52,249
load a bunch of data you do some

00:26:50,119 --> 00:26:56,509
processing you send it over you know the

00:26:52,249 --> 00:26:58,729
next thing and so upfront you need to

00:26:56,509 --> 00:27:00,579
define the schema for what vectors

00:26:58,729 --> 00:27:02,299
you're going to find in your data

00:27:00,579 --> 00:27:06,049
optionally you can have a dictionary

00:27:02,299 --> 00:27:08,119
batch for you know instead of keeping

00:27:06,049 --> 00:27:10,639
valuable lens values you can build a

00:27:08,119 --> 00:27:12,799
dictionary and replace them by IDs which

00:27:10,639 --> 00:27:14,599
is fixed lens integers and there are a

00:27:12,799 --> 00:27:16,909
lot of values for query execution of

00:27:14,599 --> 00:27:18,499
having fixed lens values instead of

00:27:16,909 --> 00:27:21,139
variable lens right if you're doing an

00:27:18,499 --> 00:27:22,939
aggregation instead of keeping a hash

00:27:21,139 --> 00:27:26,029
table of the values to keep incrementing

00:27:22,939 --> 00:27:28,609
the counts you just keep a plane array

00:27:26,029 --> 00:27:30,769
and instead of looking up a hash you

00:27:28,609 --> 00:27:31,969
directly look up the index of the values

00:27:30,769 --> 00:27:35,299
right because when you build a

00:27:31,969 --> 00:27:37,549
dictionary like IDs for that dictionary

00:27:35,299 --> 00:27:39,589
will be between 0 and n there will be

00:27:37,549 --> 00:27:41,799
packed and you can just build an array

00:27:39,589 --> 00:27:46,549
and do a much faster aggregation

00:27:41,799 --> 00:27:48,949
evaluation like that so you have schema

00:27:46,549 --> 00:27:50,929
optionally dictionary and then a bunch

00:27:48,949 --> 00:27:56,569
of record batch is representing the data

00:27:50,929 --> 00:27:58,879
and if we drill down in that so in a

00:27:56,569 --> 00:28:01,519
single record batch you have a data

00:27:58,879 --> 00:28:03,979
header that describes some offsets and

00:28:01,519 --> 00:28:06,259
then you will have each vectors

00:28:03,979 --> 00:28:09,619
representing the data so if you remember

00:28:06,259 --> 00:28:12,049
that example I add on the other side you

00:28:09,619 --> 00:28:15,919
have the vectors for the name so the

00:28:12,049 --> 00:28:19,669
bitmap is for is it not now we just keep

00:28:15,919 --> 00:28:21,559
01 to see if it's now then the offset is

00:28:19,669 --> 00:28:23,959
remember name is a variable length value

00:28:21,559 --> 00:28:25,759
so we say where the value start and then

00:28:23,959 --> 00:28:28,369
the actual data which is all the values

00:28:25,759 --> 00:28:30,139
one after the other edge is fixed with

00:28:28,369 --> 00:28:34,189
so we have a bitmap to see if it's not

00:28:30,139 --> 00:28:37,449
all not and then we have all the values

00:28:34,189 --> 00:28:37,449
one after another and so on

00:28:39,300 --> 00:28:45,790
so each vector is contiguous in memory

00:28:42,270 --> 00:28:47,290
it doesn't have to be contiguous all the

00:28:45,790 --> 00:28:48,820
vectors don't have to be contiguous one

00:28:47,290 --> 00:28:51,040
after the other in memory because when

00:28:48,820 --> 00:28:52,870
you're building them each one is a

00:28:51,040 --> 00:28:55,360
different stream right we add values to

00:28:52,870 --> 00:28:57,640
them but once we send them other the

00:28:55,360 --> 00:28:59,650
wire because they're going to be written

00:28:57,640 --> 00:29:02,080
like we're going to tell the network

00:28:59,650 --> 00:29:03,940
layer to say a write this to network or

00:29:02,080 --> 00:29:05,830
existing network and so on and so forth

00:29:03,940 --> 00:29:08,160
on the other end is going to be a

00:29:05,830 --> 00:29:10,570
tightly packed data structures which all

00:29:08,160 --> 00:29:13,720
local and after one another so when you

00:29:10,570 --> 00:29:19,780
receive it it's an ice pack pack data

00:29:13,720 --> 00:29:23,560
structure so the main thing about moving

00:29:19,780 --> 00:29:25,540
data between systems so in RPC we want

00:29:23,560 --> 00:29:28,000
to avoid service agent Dessler ization

00:29:25,540 --> 00:29:30,730
so we just we can send the data but

00:29:28,000 --> 00:29:32,230
avoiding transformation because the wire

00:29:30,730 --> 00:29:36,370
format is different from the in memory

00:29:32,230 --> 00:29:40,630
format and we make I've written here

00:29:36,370 --> 00:29:43,060
layer DVD because we haven't shared yet

00:29:40,630 --> 00:29:45,430
as a standard library how to do their

00:29:43,060 --> 00:29:47,860
raid rights and like each system does

00:29:45,430 --> 00:29:50,680
its thing but it's going to be a shell

00:29:47,860 --> 00:29:56,170
library to how to send those packets but

00:29:50,680 --> 00:29:58,630
truly very it's very simple right we

00:29:56,170 --> 00:30:00,520
have this record bachelor presentation

00:29:58,630 --> 00:30:06,040
just send it over the wire with a proper

00:30:00,520 --> 00:30:09,400
header and for IPC we have a prototype

00:30:06,040 --> 00:30:10,900
using a memory mapped files and for

00:30:09,400 --> 00:30:13,600
integration with between drill and

00:30:10,900 --> 00:30:17,230
Python and you can imagine you have your

00:30:13,600 --> 00:30:19,990
soul Apache drill is then it's written

00:30:17,230 --> 00:30:21,720
in Java it is a sequel engine written in

00:30:19,990 --> 00:30:25,090
Java so currently write your

00:30:21,720 --> 00:30:28,090
user-defined function in Java but with

00:30:25,090 --> 00:30:31,320
arrow because drill is using arrow in

00:30:28,090 --> 00:30:34,360
its in memory or presentation and

00:30:31,320 --> 00:30:39,220
actually I'm going to show them that

00:30:34,360 --> 00:30:40,950
slide about this sorry it's truncated

00:30:39,220 --> 00:30:44,350
but if you have your sequel engine

00:30:40,950 --> 00:30:46,720
producing arrow you can pass a pointer

00:30:44,350 --> 00:30:49,290
to it two different process which is

00:30:46,720 --> 00:30:51,980
Python which is not training on the JVM

00:30:49,290 --> 00:30:54,530
that can read this data

00:30:51,980 --> 00:30:56,630
and I know the previews took talked

00:30:54,530 --> 00:30:58,490
about it's bad to have shared memory but

00:30:56,630 --> 00:31:00,679
in there you have to remember that this

00:30:58,490 --> 00:31:04,340
is an immutable piece of data right we

00:31:00,679 --> 00:31:06,290
write a record batch we finalize it now

00:31:04,340 --> 00:31:08,600
it's immutable it's not going to change

00:31:06,290 --> 00:31:10,790
anymore and we share a pointer to it in

00:31:08,600 --> 00:31:12,919
read only when it's going to be read

00:31:10,790 --> 00:31:15,890
only by this process and now this

00:31:12,919 --> 00:31:17,360
process can produce a new record batch

00:31:15,890 --> 00:31:19,970
which is the output of what this

00:31:17,360 --> 00:31:22,250
user-defined function is doing and this

00:31:19,970 --> 00:31:24,320
can be passed back to the sequel up to

00:31:22,250 --> 00:31:27,559
the next sequel operator to the quarry

00:31:24,320 --> 00:31:30,260
execution engine and what that means is

00:31:27,559 --> 00:31:32,809
that there's zero overhead to do cross

00:31:30,260 --> 00:31:35,360
process communication inter-process

00:31:32,809 --> 00:31:38,330
communication and because we can just

00:31:35,360 --> 00:31:40,970
have shared pointer to an immutable data

00:31:38,330 --> 00:31:42,919
structure producing new one and share

00:31:40,970 --> 00:31:46,309
pointer to the result with a new

00:31:42,919 --> 00:31:49,580
immutable data structure and that means

00:31:46,309 --> 00:31:51,559
that now you can use Python to do your

00:31:49,580 --> 00:31:57,140
favorite gdf instead of having to do

00:31:51,559 --> 00:31:59,360
whatever each system is implementing as

00:31:57,140 --> 00:32:01,490
a user-defined function framework or

00:31:59,360 --> 00:32:05,150
interface and that means once you wrote

00:32:01,490 --> 00:32:07,130
it for Python in Python for let's say

00:32:05,150 --> 00:32:09,140
drill is going to work for spark it's

00:32:07,130 --> 00:32:11,059
going to work for in power because they

00:32:09,140 --> 00:32:14,720
all did now the interface between the

00:32:11,059 --> 00:32:17,330
two is Error so first we removed all

00:32:14,720 --> 00:32:20,540
costs of you know it's not going to be

00:32:17,330 --> 00:32:23,150
jni with the overhead of calling an atty

00:32:20,540 --> 00:32:24,919
from Java and it's not going to have to

00:32:23,150 --> 00:32:26,929
convert the representation to another

00:32:24,919 --> 00:32:28,309
one it's all standard so there's no

00:32:26,929 --> 00:32:31,610
overhead of inter-process communication

00:32:28,309 --> 00:32:33,320
and its standard which means those

00:32:31,610 --> 00:32:39,559
user-defined function that we work for

00:32:33,320 --> 00:32:40,730
each query engine and if I go back so

00:32:39,559 --> 00:32:47,540
that was for inter process communication

00:32:40,730 --> 00:32:50,870
and for our pc i have an example of what

00:32:47,540 --> 00:32:55,669
a query execution physical plan looks

00:32:50,870 --> 00:33:01,159
like and so this example is select some

00:32:55,669 --> 00:33:03,650
of a from t group by B and so you start

00:33:01,159 --> 00:33:05,900
by scanning parka files and really like

00:33:03,650 --> 00:33:09,350
we did before we're going to read only

00:33:05,900 --> 00:33:12,710
two columns and turn them into our own

00:33:09,350 --> 00:33:14,480
columns in memory and then here we have

00:33:12,710 --> 00:33:17,510
three lines because there are three

00:33:14,480 --> 00:33:21,080
machines in my cluster so that's machine

00:33:17,510 --> 00:33:23,480
one machine to mantri machine 3 3 nodes

00:33:21,080 --> 00:33:26,870
and each machine we do partial

00:33:23,480 --> 00:33:28,640
aggregation and prepare blocks for doing

00:33:26,870 --> 00:33:31,820
a shuffle and send you their machine

00:33:28,640 --> 00:33:34,160
right so we take the key space and using

00:33:31,820 --> 00:33:36,740
consistent hashing or whatever mechanism

00:33:34,160 --> 00:33:38,720
we're going to decide which machine will

00:33:36,740 --> 00:33:41,810
be responsible for the total aggregation

00:33:38,720 --> 00:33:43,490
view of that particular key and so based

00:33:41,810 --> 00:33:45,680
on that you can prepare if we have three

00:33:43,490 --> 00:33:47,870
nodes in our cluster each node will

00:33:45,680 --> 00:33:52,040
prepare three outputs with partial

00:33:47,870 --> 00:33:54,620
aggregation result fudge set of keys and

00:33:52,040 --> 00:33:56,480
then this is going to be sent over the

00:33:54,620 --> 00:33:59,360
wire so here we stand the same machine

00:33:56,480 --> 00:34:00,920
so there's no copy but if we go on

00:33:59,360 --> 00:34:03,200
another machine we just send this about

00:34:00,920 --> 00:34:06,980
the wire there's no extra sterilization

00:34:03,200 --> 00:34:10,159
decision logic is just copied right over

00:34:06,980 --> 00:34:12,909
and then the final aggregation which is

00:34:10,159 --> 00:34:15,860
combining those three views together is

00:34:12,909 --> 00:34:18,770
done and then when sending the result

00:34:15,860 --> 00:34:21,710
set to the client is just same thing

00:34:18,770 --> 00:34:23,750
against the in-memory representation is

00:34:21,710 --> 00:34:30,800
just sent over the wire directly to the

00:34:23,750 --> 00:34:32,570
client and so really that's it right

00:34:30,800 --> 00:34:34,669
because the immemorial presentation is

00:34:32,570 --> 00:34:38,120
the same as the wire representation we

00:34:34,669 --> 00:34:42,800
remove all this overhead are converting

00:34:38,120 --> 00:34:45,050
things and also each step you know you

00:34:42,800 --> 00:34:47,320
can imagine loading from kudu kudu is

00:34:45,050 --> 00:34:50,120
columnar representation on disk and

00:34:47,320 --> 00:34:53,030
currently when you use a kudu client

00:34:50,120 --> 00:34:54,409
it's going to convert it into assemble

00:34:53,030 --> 00:34:57,740
those columns to present it to the

00:34:54,409 --> 00:35:00,440
client in a row oriented form and and

00:34:57,740 --> 00:35:02,030
then if it's if you're reading that with

00:35:00,440 --> 00:35:03,770
apache drill for example is going to

00:35:02,030 --> 00:35:05,540
turn in back into column now our

00:35:03,770 --> 00:35:09,140
presentation so kind of you go back and

00:35:05,540 --> 00:35:12,010
forth which is really inefficient when

00:35:09,140 --> 00:35:14,630
once they're both integrated with arrow

00:35:12,010 --> 00:35:17,450
is going to go directly from the

00:35:14,630 --> 00:35:19,130
columnar representation of kudu to the

00:35:17,450 --> 00:35:20,779
in-memory corner

00:35:19,130 --> 00:35:26,200
entation and you don't go back and forth

00:35:20,779 --> 00:35:31,549
between columnar and royalty and on that

00:35:26,200 --> 00:35:34,250
you know almost done so the next step is

00:35:31,549 --> 00:35:38,509
to do the shared park at euro conversion

00:35:34,250 --> 00:35:40,369
I've add those shared library so there's

00:35:38,509 --> 00:35:43,069
really a common format that all those

00:35:40,369 --> 00:35:46,519
project have agreed on now we need is

00:35:43,069 --> 00:35:48,289
some shared libraries and do the

00:35:46,519 --> 00:35:52,609
integration in some of them and there's

00:35:48,289 --> 00:35:55,119
also interest in the Intel persistent

00:35:52,609 --> 00:35:58,940
memory library called Apache mnemonic

00:35:55,119 --> 00:36:01,250
he's looking into that and so persistent

00:35:58,940 --> 00:36:04,130
memory is not really interesting because

00:36:01,250 --> 00:36:05,509
it persistent or at least not yet it's

00:36:04,130 --> 00:36:09,319
interesting because it's a layer in

00:36:05,509 --> 00:36:12,799
between SSD like disk and memory which

00:36:09,319 --> 00:36:14,359
is slightly higher latency that Ram but

00:36:12,799 --> 00:36:17,029
it's much cheaper you can have a lot

00:36:14,359 --> 00:36:20,630
more of it for the price so it's kind of

00:36:17,029 --> 00:36:21,819
having a good compromise in between so

00:36:20,630 --> 00:36:25,970
if you want to join the conversation

00:36:21,819 --> 00:36:28,700
like it more interested in this you can

00:36:25,970 --> 00:36:32,839
lo look at the dev mailing list i'll

00:36:28,700 --> 00:36:39,079
follow the twitter accounts and i'll be

00:36:32,839 --> 00:36:41,890
open to questions are we done thank you

00:36:39,079 --> 00:36:41,890
for the presentation

00:36:44,930 --> 00:36:55,050
so I see there is a question here I was

00:36:52,560 --> 00:36:56,970
just wondering um what if you need to

00:36:55,050 --> 00:36:59,910
transport the data from a little endian

00:36:56,970 --> 00:37:05,130
tuba canyon system and vice versa how

00:36:59,910 --> 00:37:10,200
would you handle that so so the Indian X

00:37:05,130 --> 00:37:12,420
is fixed so most system sorry I'm always

00:37:10,200 --> 00:37:14,670
forget which one is which but for

00:37:12,420 --> 00:37:16,710
example the Java default representation

00:37:14,670 --> 00:37:19,290
is the wrong one so can someone remind

00:37:16,710 --> 00:37:21,180
me which one is the most common that we

00:37:19,290 --> 00:37:25,140
use all the time it's little Indian

00:37:21,180 --> 00:37:28,680
right so every hardware system right now

00:37:25,140 --> 00:37:31,320
it's easier little-endian or supports

00:37:28,680 --> 00:37:33,630
both right like I think that's a point

00:37:31,320 --> 00:37:35,520
so it's going to be a little engine and

00:37:33,630 --> 00:37:38,550
Java is big Indian by default right

00:37:35,520 --> 00:37:41,760
that's incorrect right because I always

00:37:38,550 --> 00:37:45,030
got confused so parka is a little engine

00:37:41,760 --> 00:37:47,610
and because we were we were walking with

00:37:45,030 --> 00:37:50,130
the Impala team and their system is in

00:37:47,610 --> 00:37:52,410
C++ so they want to be able to do the

00:37:50,130 --> 00:37:54,600
trick when you know the cast an array of

00:37:52,410 --> 00:37:57,060
integers into an array of bytes and it's

00:37:54,600 --> 00:37:58,980
going to do the right thing so it's all

00:37:57,060 --> 00:38:00,900
little endian by default so the same

00:37:58,980 --> 00:38:03,450
here there's not going to be any

00:38:00,900 --> 00:38:06,240
conversion because the format is defined

00:38:03,450 --> 00:38:07,740
as it's all little Indian and that's it

00:38:06,240 --> 00:38:10,350
so you don't need to know what the

00:38:07,740 --> 00:38:12,030
system is underneath because I believe

00:38:10,350 --> 00:38:14,550
unless someone knows or something else

00:38:12,030 --> 00:38:17,700
little-endian is the best cheerful

00:38:14,550 --> 00:38:19,050
choice for current hardware at the time

00:38:17,700 --> 00:38:21,630
it's kind of big engine is kind of

00:38:19,050 --> 00:38:24,510
outdated so from practical standpoint

00:38:21,630 --> 00:38:27,150
this is not a problem no it's not a

00:38:24,510 --> 00:38:30,930
hardware isn't there yeah it's not the

00:38:27,150 --> 00:38:33,810
choice you get to make on Indiana's it

00:38:30,930 --> 00:38:35,340
is the format as one in DNS and that the

00:38:33,810 --> 00:38:38,820
way it is and there's no need to know

00:38:35,340 --> 00:38:42,500
what the system does because part of the

00:38:38,820 --> 00:38:42,500
strander is you have to do little engine

00:38:43,950 --> 00:38:47,670
I'm not going to take sides over the

00:38:46,140 --> 00:38:48,810
religious issue of ending this I'm just

00:38:47,670 --> 00:38:50,670
going to serve this is actually a

00:38:48,810 --> 00:38:52,740
complete repetition of the religious war

00:38:50,670 --> 00:38:55,820
between Apollo and son in about nineteen

00:38:52,740 --> 00:39:01,350
eighty nine about the wire format of

00:38:55,820 --> 00:39:04,170
Apollo RPC versus NFS and of course NFS

00:39:01,350 --> 00:39:06,270
is what their policy with recipient

00:39:04,170 --> 00:39:07,800
makes good basically saying well never

00:39:06,270 --> 00:39:10,950
stable wire format we're going to

00:39:07,800 --> 00:39:12,690
produce what we know I'm confused you

00:39:10,950 --> 00:39:14,370
basically hard-coded the exact opposite

00:39:12,690 --> 00:39:17,220
of what son got through with NFS on you

00:39:14,370 --> 00:39:20,310
so you basically said tell Intel is the

00:39:17,220 --> 00:39:22,260
part that wins there's nothing wrong

00:39:20,310 --> 00:39:30,180
with that I'm just you know server-side

00:39:22,260 --> 00:39:33,870
it actually makes sense I thank you for

00:39:30,180 --> 00:39:36,630
a top I'm curse about spilling so when

00:39:33,870 --> 00:39:40,770
the memory is too low to hold a narrow

00:39:36,630 --> 00:39:44,280
file in memory mm-hmm so like all these

00:39:40,770 --> 00:39:45,780
systems that do in-memory processing the

00:39:44,280 --> 00:39:48,540
one thing you want to look at is how

00:39:45,780 --> 00:39:52,470
does this system behave when the data

00:39:48,540 --> 00:39:54,630
set isn't fit in memory so arrow doesn't

00:39:52,470 --> 00:39:57,600
define that right arrow defined how you

00:39:54,630 --> 00:40:00,300
represent a given record batch which is

00:39:57,600 --> 00:40:01,560
a subset of your data set so it defined

00:40:00,300 --> 00:40:04,230
that you're going to be able to split

00:40:01,560 --> 00:40:06,980
your data set in record batches and have

00:40:04,230 --> 00:40:09,960
each chunk in memory at a time and then

00:40:06,980 --> 00:40:12,450
it's kind of up to each system how they

00:40:09,960 --> 00:40:14,880
do it so typically like for example if

00:40:12,450 --> 00:40:16,980
you do a join of your doing aggregations

00:40:14,880 --> 00:40:18,900
you're going to spill to disk right so

00:40:16,980 --> 00:40:20,880
let's say I'm doing aggregation and I

00:40:18,900 --> 00:40:22,860
keep the keys and the current values and

00:40:20,880 --> 00:40:25,680
then at some point the key set doesn't

00:40:22,860 --> 00:40:28,430
fit in memory anymore so you're going to

00:40:25,680 --> 00:40:30,720
spill something and if I go back to this

00:40:28,430 --> 00:40:32,520
for example in the aggregation that's a

00:40:30,720 --> 00:40:34,620
good example because here it's

00:40:32,520 --> 00:40:36,180
simplified you know and there's only one

00:40:34,620 --> 00:40:37,730
record batch and then there's only one

00:40:36,180 --> 00:40:41,040
record batch for each of those things

00:40:37,730 --> 00:40:43,500
but it doesn't need to be right so here

00:40:41,040 --> 00:40:45,060
what you can do is we're building those

00:40:43,500 --> 00:40:46,710
partial aggregation in memory so I'm

00:40:45,060 --> 00:40:49,080
doing the sum from the point of view of

00:40:46,710 --> 00:40:51,420
that node and then when I want the

00:40:49,080 --> 00:40:53,400
global son but I sum all those partial

00:40:51,420 --> 00:40:56,280
sums together and I get the global some

00:40:53,400 --> 00:40:57,119
which mean then here if I run out of

00:40:56,280 --> 00:41:00,569
memory

00:40:57,119 --> 00:41:02,999
point against pill to disk or you know

00:41:00,569 --> 00:41:06,240
send it to the next one directly and say

00:41:02,999 --> 00:41:08,519
hey I'm partially done there are too

00:41:06,240 --> 00:41:10,319
many distinct keys I cannot keep it in

00:41:08,519 --> 00:41:12,450
memory knee anymore I can spill it to

00:41:10,319 --> 00:41:16,079
disk and do you know a disk-based merge

00:41:12,450 --> 00:41:18,509
on this side or send it over and start a

00:41:16,079 --> 00:41:20,490
new one and start aggregated for the

00:41:18,509 --> 00:41:24,990
other keys right and that's still

00:41:20,490 --> 00:41:27,539
correct because some is an associative

00:41:24,990 --> 00:41:30,150
operation right so you can do this part

00:41:27,539 --> 00:41:31,499
of the sum and then a doesn't fit in

00:41:30,150 --> 00:41:34,410
memory because I have too many distinct

00:41:31,499 --> 00:41:36,210
keys flash it to disk start over and the

00:41:34,410 --> 00:41:38,519
contract is but just send them all

00:41:36,210 --> 00:41:41,099
someday mole on the other end and you

00:41:38,519 --> 00:41:42,960
can do a disk best some maybe I was

00:41:41,099 --> 00:41:44,579
sorting the keys on purpose so that this

00:41:42,960 --> 00:41:48,049
is more efficient when I write it to

00:41:44,579 --> 00:41:52,049
disk or something so that's one example

00:41:48,049 --> 00:41:54,269
so basically the system that uses arrow

00:41:52,049 --> 00:41:57,420
will have to implement some strategies

00:41:54,269 --> 00:41:59,249
right so you can start aggregating it

00:41:57,420 --> 00:42:01,170
then it doesn't fit anymore you save it

00:41:59,249 --> 00:42:02,849
you write it to disk you keep going in

00:42:01,170 --> 00:42:06,539
memory as much as you can and then you

00:42:02,849 --> 00:42:08,519
combine the results and of course if you

00:42:06,539 --> 00:42:10,410
spill to disk then you take a hit is

00:42:08,519 --> 00:42:12,569
going to be much faster if it's old fits

00:42:10,410 --> 00:42:15,749
in memory butts like it's a typical

00:42:12,569 --> 00:42:17,400
query execution problems and what are

00:42:15,749 --> 00:42:19,859
the trade-offs so that's where also that

00:42:17,400 --> 00:42:23,009
this format needs to be as compact as

00:42:19,859 --> 00:42:24,660
possible for in memory because you want

00:42:23,009 --> 00:42:26,849
to do as much as you can directly in

00:42:24,660 --> 00:42:29,190
memory right to avoid overhead of

00:42:26,849 --> 00:42:32,279
pointers and thing so columnar is good

00:42:29,190 --> 00:42:34,259
for that as well because you know if you

00:42:32,279 --> 00:42:36,359
have an array of objects that pones to

00:42:34,259 --> 00:42:38,460
other values you add the overhead of a

00:42:36,359 --> 00:42:40,739
lot like in Java typically a lot of

00:42:38,460 --> 00:42:43,079
references we cease trucks that would

00:42:40,739 --> 00:42:46,319
not be a problem but let's say if you do

00:42:43,079 --> 00:42:49,650
the typical like how a lot of the system

00:42:46,319 --> 00:42:52,499
started they are the list of Java like

00:42:49,650 --> 00:42:54,539
if you look at how big works back in the

00:42:52,499 --> 00:42:56,880
day you have a you would have a list of

00:42:54,539 --> 00:42:59,999
top pole object and topple object is a

00:42:56,880 --> 00:43:02,309
map of string name field name to the

00:42:59,999 --> 00:43:04,529
actual value which is very inefficient

00:43:02,309 --> 00:43:06,269
uses a lot of memory but the column now

00:43:04,529 --> 00:43:09,720
representation like that is more compact

00:43:06,269 --> 00:43:10,720
so the game is as much as possible do

00:43:09,720 --> 00:43:12,340
everything in memory

00:43:10,720 --> 00:43:14,980
it doesn't work anymore you need to fall

00:43:12,340 --> 00:43:16,390
back to disk do something send partial

00:43:14,980 --> 00:43:17,830
results and they're going to be more

00:43:16,390 --> 00:43:22,119
work involved but have some kind of

00:43:17,830 --> 00:43:23,740
trade off oh by the way yeah we have one

00:43:22,119 --> 00:43:26,500
last question but you have to keep it

00:43:23,740 --> 00:43:28,270
really short okay so you can take it

00:43:26,500 --> 00:43:30,369
offline so it's going to be a yes or no

00:43:28,270 --> 00:43:34,359
answer not just kidding thanks for the

00:43:30,369 --> 00:43:36,670
talk you said avril packages or blocks

00:43:34,359 --> 00:43:39,090
they are mutable other plans to

00:43:36,670 --> 00:43:42,010
introduce versioning updates or do you

00:43:39,090 --> 00:43:45,099
always think you update the on disk

00:43:42,010 --> 00:43:47,590
format and then you per query load in

00:43:45,099 --> 00:43:53,170
memory arrow packages and you ever want

00:43:47,590 --> 00:43:54,490
to reuse them in the future so I'm not

00:43:53,170 --> 00:43:56,380
sure if I'm going to answer your

00:43:54,490 --> 00:43:58,690
question but so I've said this is like

00:43:56,380 --> 00:44:00,910
transient in memory but you can reuse

00:43:58,690 --> 00:44:03,340
the same format for spilling to disk or

00:44:00,910 --> 00:44:06,849
like you are on this cash right you can

00:44:03,340 --> 00:44:08,740
temporarily serialize it to disk or do a

00:44:06,849 --> 00:44:14,230
lot of stuff or keep it keep an

00:44:08,740 --> 00:44:16,000
in-memory cache as well sorry can you

00:44:14,230 --> 00:44:18,310
repeat maybe we can take it offline I

00:44:16,000 --> 00:44:20,589
said which part was important to you so

00:44:18,310 --> 00:44:24,339
I think the initial assumption is it's

00:44:20,589 --> 00:44:26,320
you created once per query yeah so sorry

00:44:24,339 --> 00:44:28,810
I missed that part so yeah you're free

00:44:26,320 --> 00:44:31,480
to modify the content it's not it's not

00:44:28,810 --> 00:44:32,980
just up and only you're free to go

00:44:31,480 --> 00:44:35,109
editor value maybe you're doing

00:44:32,980 --> 00:44:37,210
aggregations right and you keep updating

00:44:35,109 --> 00:44:38,560
that some in the Earl you have in memory

00:44:37,210 --> 00:44:40,720
and you at some point you're spawning to

00:44:38,560 --> 00:44:42,820
it and that's fine it's just when you're

00:44:40,720 --> 00:44:45,220
done writing to this record batch and

00:44:42,820 --> 00:44:47,290
you finalize it basically you're saying

00:44:45,220 --> 00:44:49,930
I'm not modifying it anymore and at that

00:44:47,290 --> 00:44:51,400
point it becomes immutable it doesn't

00:44:49,930 --> 00:44:53,320
have to be mutable from the start or

00:44:51,400 --> 00:44:55,240
happen only you can mutate it as much as

00:44:53,320 --> 00:44:58,119
you want but you're a single rider to it

00:44:55,240 --> 00:45:00,339
and then you finalize it and you say

00:44:58,119 --> 00:45:01,720
it's immutable now other people can read

00:45:00,339 --> 00:45:03,130
it and then can read it in parallel and

00:45:01,720 --> 00:45:06,190
it's fine because nobody else is

00:45:03,130 --> 00:45:08,530
mutating it and that's that the theory

00:45:06,190 --> 00:45:11,050
behind that and also one thing I wanted

00:45:08,530 --> 00:45:13,089
to precise is this is typically not

00:45:11,050 --> 00:45:15,609
being going to be visible from the user

00:45:13,089 --> 00:45:17,230
code instead unless you're doing some

00:45:15,609 --> 00:45:18,730
special integration between two of those

00:45:17,230 --> 00:45:20,710
systems so that you're actually

00:45:18,730 --> 00:45:21,829
contributing to making arrow work

00:45:20,710 --> 00:45:25,789
everywhere

00:45:21,829 --> 00:45:28,430
but basically usually it's hidden in the

00:45:25,789 --> 00:45:30,380
system right so when Impala uses it or

00:45:28,430 --> 00:45:31,789
drill uses it its internal or

00:45:30,380 --> 00:45:35,029
presentation it's not something that you

00:45:31,789 --> 00:45:38,809
see or they're going to be a client that

00:45:35,029 --> 00:45:41,569
abstracts it at for you and so I'm like

00:45:38,809 --> 00:45:45,069
thank you and we can you know can have

00:45:41,569 --> 00:45:50,499
drinks and talk more about that stuff

00:45:45,069 --> 00:45:50,499

YouTube URL: https://www.youtube.com/watch?v=43O5BCabBcU


