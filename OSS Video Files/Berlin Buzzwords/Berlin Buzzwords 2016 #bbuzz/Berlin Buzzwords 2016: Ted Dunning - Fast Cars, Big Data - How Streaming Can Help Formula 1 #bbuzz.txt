Title: Berlin Buzzwords 2016: Ted Dunning - Fast Cars, Big Data - How Streaming Can Help Formula 1 #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Modern cars produce data. Lots of data. And Formula 1 cars produce more than their share.

I will present a working demonstration of how modern data streaming can be applied to the data acquisition and analysis problem posed by modern motorsports.

Instead of bringing multiple Formula 1 cars to the talk, I will show how we instrumented a high fidelity physics-based automotive simulator to produce realistic data from simulated cars running on the Spa-Francorchamps track. We move data from the cars, to the pits, to the engineers back at HQ.

The result is near real-time visualization and comparison of performance and a great exposition of how to move data using messaging systems like Kafka. The code from this talk will be made available as open source.

Read more:
https://2016.berlinbuzzwords.de/session/fast-cars-big-data-how-streaming-can-help-formula-1

About Ted Dunning:
https://2016.berlinbuzzwords.de/users/ted-dunning

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:02,270 --> 00:00:07,710
yeah hmm I don't think those would be

00:00:05,279 --> 00:00:09,059
fast cars not if we're in traffic that

00:00:07,710 --> 00:00:11,219
would be the slow cars I don't know how

00:00:09,059 --> 00:00:14,190
to fix those so I'm going to talk though

00:00:11,219 --> 00:00:17,220
a little bit about how we can apply

00:00:14,190 --> 00:00:22,170
streaming data fast data big data too

00:00:17,220 --> 00:00:24,529
fast cars in particular some examples

00:00:22,170 --> 00:00:28,740
from Formula One but also some more

00:00:24,529 --> 00:00:30,779
general examples you can catch me by a

00:00:28,740 --> 00:00:33,989
number of different ways I work for map

00:00:30,779 --> 00:00:36,270
our chief application architect I work

00:00:33,989 --> 00:00:39,059
with a lot of projects at apache I'm

00:00:36,270 --> 00:00:41,309
currently chief a blur or no I'm sorry

00:00:39,059 --> 00:00:44,910
VP of incubator at Apache that's a

00:00:41,309 --> 00:00:49,469
temporary role and you can find buzz

00:00:44,910 --> 00:00:52,110
words is that blinking hmm i also have

00:00:49,469 --> 00:00:54,329
email addresses that you can use so

00:00:52,110 --> 00:00:58,079
today what I want to talk about is

00:00:54,329 --> 00:01:00,510
what's the point of big data in

00:00:58,079 --> 00:01:03,690
motorsports what what can it help with

00:01:00,510 --> 00:01:06,960
how we can play with it too without

00:01:03,690 --> 00:01:10,080
buying a Formula One car some people I

00:01:06,960 --> 00:01:12,090
know who have small race cars and these

00:01:10,080 --> 00:01:13,890
sort of solutions will help them I'm

00:01:12,090 --> 00:01:17,280
also going to talk about a particular

00:01:13,890 --> 00:01:21,240
general technique of building random

00:01:17,280 --> 00:01:25,439
data which is as good as it needs to be

00:01:21,240 --> 00:01:27,270
in a particularly appropriate way every

00:01:25,439 --> 00:01:30,600
time I look away it blinks every time I

00:01:27,270 --> 00:01:33,299
look at it it doesn't I'm not sure how

00:01:30,600 --> 00:01:35,790
we simulate that hopefully it will get

00:01:33,299 --> 00:01:37,770
better and then I'll talk a little bit

00:01:35,790 --> 00:01:44,780
about how the particular simulation in

00:01:37,770 --> 00:01:48,990
this case works let's see if that helps

00:01:44,780 --> 00:01:52,950
so in Formula One sports right now the

00:01:48,990 --> 00:01:55,439
that's pretty famous that they use data

00:01:52,950 --> 00:02:04,140
at least fast data made no difference at

00:01:55,439 --> 00:02:06,390
all they use fast data to to improve

00:02:04,140 --> 00:02:08,970
their operations to improve their

00:02:06,390 --> 00:02:11,640
ability to perform in a number of ways

00:02:08,970 --> 00:02:13,830
often what you'll see are these kinds of

00:02:11,640 --> 00:02:16,050
diagrams these are

00:02:13,830 --> 00:02:19,410
many of the car parameters that are

00:02:16,050 --> 00:02:23,010
sampled in real time in fact normally

00:02:19,410 --> 00:02:30,690
there are about 300 data sources data

00:02:23,010 --> 00:02:33,270
streams that sample I can't tell if I'm

00:02:30,690 --> 00:02:35,850
improving it or not the top line when

00:02:33,270 --> 00:02:38,520
you can see it is the RPM of the Inchon

00:02:35,850 --> 00:02:41,010
you can see during fast straightaways

00:02:38,520 --> 00:02:43,850
that's fairly constant when it's

00:02:41,010 --> 00:02:47,310
decelerating it's downshifting very fast

00:02:43,850 --> 00:02:50,990
it's difficult to see here but some of

00:02:47,310 --> 00:02:53,640
the downshifts very difficult to see

00:02:50,990 --> 00:02:57,780
you'll have to trust me the second line

00:02:53,640 --> 00:03:00,140
the blue line which is completely now no

00:02:57,780 --> 00:03:03,570
longer intermediate the second line is

00:03:00,140 --> 00:03:14,660
the speed of the car certainly help here

00:03:03,570 --> 00:03:14,660
did somebody could get on this so okay

00:03:16,490 --> 00:03:22,850
does it alternative no

00:03:29,310 --> 00:03:37,980
so we'll try again the blue line is the

00:03:34,410 --> 00:03:42,349
speed of the car this will appear at a

00:03:37,980 --> 00:03:42,349
moment everybody hold their breath I

00:03:44,630 --> 00:03:53,580
think its upstream okay so we're going

00:03:51,120 --> 00:03:55,560
to continue the blue line then is the

00:03:53,580 --> 00:03:57,510
speed of the car you can see it

00:03:55,560 --> 00:04:01,170
accelerating in the straightaways you

00:03:57,510 --> 00:04:04,019
can see a decelerating in in going into

00:04:01,170 --> 00:04:06,120
curves the gearshift shifts down so this

00:04:04,019 --> 00:04:09,060
car was so fast at it shifts that within

00:04:06,120 --> 00:04:11,730
60 meters as a car decelerates from over

00:04:09,060 --> 00:04:13,709
200 kilometers an hour to about 100

00:04:11,730 --> 00:04:16,049
kilometers an hour and still only goes

00:04:13,709 --> 00:04:20,519
60 meters down the road it shifts down

00:04:16,049 --> 00:04:25,160
seven times which is amazingly fast more

00:04:20,519 --> 00:04:28,440
most important in one of these sorts of

00:04:25,160 --> 00:04:32,520
measurements these graphs are some of

00:04:28,440 --> 00:04:35,610
the bottom ones which show the throttle

00:04:32,520 --> 00:04:38,250
in the golden color come on come on I

00:04:35,610 --> 00:04:41,070
will start talking about that in a

00:04:38,250 --> 00:04:43,169
moment this is the throttle here you can

00:04:41,070 --> 00:04:45,690
see that they have it maximum most of

00:04:43,169 --> 00:04:47,490
the time minimum sometimes a little bit

00:04:45,690 --> 00:04:50,789
intermediate and the bottom is the brake

00:04:47,490 --> 00:04:55,080
and then they overlay these so very

00:04:50,789 --> 00:04:57,990
often you might this x-axis is distance

00:04:55,080 --> 00:05:00,030
not time distance around the track so

00:04:57,990 --> 00:05:02,910
when you overlay them you can see how

00:05:00,030 --> 00:05:05,370
two different cars negotiated the track

00:05:02,910 --> 00:05:07,979
differently the blue one was able to

00:05:05,370 --> 00:05:10,320
maintain considerably more speed going

00:05:07,979 --> 00:05:12,600
into the curve switch from decelerating

00:05:10,320 --> 00:05:15,900
to accelerating much more quickly and

00:05:12,600 --> 00:05:18,510
over the entire graph much wider than

00:05:15,900 --> 00:05:21,120
this they gained about 1.6 seconds in a

00:05:18,510 --> 00:05:23,580
single lap in the controls you can see

00:05:21,120 --> 00:05:26,510
the red card started breaking much more

00:05:23,580 --> 00:05:29,370
quickly the blues car braked much more

00:05:26,510 --> 00:05:32,010
vigorously and quickly and then was able

00:05:29,370 --> 00:05:34,220
to shift back into the higher gears more

00:05:32,010 --> 00:05:37,680
quickly by using kinetic energy recovery

00:05:34,220 --> 00:05:41,400
so there's a lot of visual sort of

00:05:37,680 --> 00:05:42,870
visualization sorts of intuitions that

00:05:41,400 --> 00:05:46,050
people have been getting for about

00:05:42,870 --> 00:05:48,500
10 or 15 years from these things but

00:05:46,050 --> 00:05:51,810
nowadays they're going much further

00:05:48,500 --> 00:05:53,550
they're doing predictive analysis based

00:05:51,810 --> 00:05:55,620
on the track temperature the outside

00:05:53,550 --> 00:05:58,800
temperature how the person is trach

00:05:55,620 --> 00:06:01,620
taking the track what will be the tire

00:05:58,800 --> 00:06:05,280
temperatures they normally run the tires

00:06:01,620 --> 00:06:07,500
at 75 to 90 degrees Celsius very very

00:06:05,280 --> 00:06:09,720
hot but if they go just a few degrees

00:06:07,500 --> 00:06:11,850
hotter that the tires start bleeding

00:06:09,720 --> 00:06:13,500
more quickly and depending on how hard

00:06:11,850 --> 00:06:15,420
they break into the corners they can

00:06:13,500 --> 00:06:17,760
predict how high the temperatures will

00:06:15,420 --> 00:06:20,190
be coming out of there and how quickly

00:06:17,760 --> 00:06:23,070
they'll go back down that then drives

00:06:20,190 --> 00:06:25,350
how quickly the tires wear out how

00:06:23,070 --> 00:06:27,780
quickly the tires were out how much drag

00:06:25,350 --> 00:06:30,030
they have in the corners due to slippage

00:06:27,780 --> 00:06:32,730
or the use of the aerodynamics will

00:06:30,030 --> 00:06:36,390
drive how much fuel they use they also

00:06:32,730 --> 00:06:38,670
then can show how those changing weights

00:06:36,390 --> 00:06:40,530
will affect the car and the driver

00:06:38,670 --> 00:06:43,530
different drivers have different skills

00:06:40,530 --> 00:06:46,340
at different times as the tires wear

00:06:43,530 --> 00:06:49,530
down the car gets slower as the gas

00:06:46,340 --> 00:06:51,480
becomes lighter than the car gets faster

00:06:49,530 --> 00:06:55,110
so there's a lot of that they also do

00:06:51,480 --> 00:06:57,510
game theoretic alternatives because when

00:06:55,110 --> 00:06:59,760
they pit stop against when the other

00:06:57,510 --> 00:07:01,860
people take pit stops that will put new

00:06:59,760 --> 00:07:04,740
tires on new fuel make the car slower

00:07:01,860 --> 00:07:06,840
faster against the other people in the

00:07:04,740 --> 00:07:09,600
race it'll also there's very few over

00:07:06,840 --> 00:07:12,240
takings except for pit stops and then

00:07:09,600 --> 00:07:14,310
they do monte carlo analysis against

00:07:12,240 --> 00:07:17,760
possible weather during the 90-minute

00:07:14,310 --> 00:07:19,380
race it's very complex programming they

00:07:17,760 --> 00:07:21,720
all want to be able to do this on

00:07:19,380 --> 00:07:23,610
large-scale computers back at the

00:07:21,720 --> 00:07:25,380
factory they can't do this in the

00:07:23,610 --> 00:07:27,480
computational systems they have in the

00:07:25,380 --> 00:07:29,730
pit so they need to be able to move the

00:07:27,480 --> 00:07:31,650
data back and then of course they want

00:07:29,730 --> 00:07:34,320
to look at this in a larger sense across

00:07:31,650 --> 00:07:36,360
multiple races how are they going to be

00:07:34,320 --> 00:07:39,090
improving the chances of different

00:07:36,360 --> 00:07:41,310
drivers on the team so the outputs then

00:07:39,090 --> 00:07:43,650
our tactical decisions what are the

00:07:41,310 --> 00:07:45,680
probable outcomes and so on there's also

00:07:43,650 --> 00:07:50,420
a lot of value in marketing because

00:07:45,680 --> 00:07:53,790
Grand Prix Formula one fans tend to be

00:07:50,420 --> 00:07:55,529
gearheads and data freaks so they just

00:07:53,790 --> 00:07:58,499
love these and

00:07:55,529 --> 00:08:00,119
mated representations of races and

00:07:58,499 --> 00:08:03,689
things like that which are of course all

00:08:00,119 --> 00:08:07,139
driven by data and the more data the

00:08:03,689 --> 00:08:09,719
better now you also get people leaking

00:08:07,139 --> 00:08:11,759
information this for instance is based I

00:08:09,719 --> 00:08:14,279
just plotted it this morning if you go

00:08:11,759 --> 00:08:16,469
to these bit Lee's over here you can

00:08:14,279 --> 00:08:19,529
find that this person did screenshots

00:08:16,469 --> 00:08:23,729
and character recognition in real time

00:08:19,529 --> 00:08:27,839
during the race of the Australian Grand

00:08:23,729 --> 00:08:30,349
Prix and told data out of those and

00:08:27,839 --> 00:08:33,479
managed to get the position velocity

00:08:30,349 --> 00:08:35,539
throttle settings and everything during

00:08:33,479 --> 00:08:39,870
the entire race for one car driver this

00:08:35,539 --> 00:08:42,959
crazy and you know you get occasional

00:08:39,870 --> 00:08:44,699
little inputs like this but it just it

00:08:42,959 --> 00:08:47,639
isn't really going to work it's not

00:08:44,699 --> 00:08:50,689
going to help us outside the very small

00:08:47,639 --> 00:08:53,610
community to build interesting software

00:08:50,689 --> 00:08:56,189
it's there's going to be no data that's

00:08:53,610 --> 00:08:58,079
consistent no data that's going to be

00:08:56,189 --> 00:09:01,110
high enough quality no data that's

00:08:58,079 --> 00:09:03,660
that's going to be usable except in very

00:09:01,110 --> 00:09:06,449
special cases you'll be able to make one

00:09:03,660 --> 00:09:10,800
plot from one race for one driver

00:09:06,449 --> 00:09:14,519
sometimes you can't really do fun

00:09:10,800 --> 00:09:18,649
interesting work when data is so

00:09:14,519 --> 00:09:22,139
intermittent and so highly variable so

00:09:18,649 --> 00:09:24,920
this highlights a problem that we have

00:09:22,139 --> 00:09:27,240
in trying to build something interesting

00:09:24,920 --> 00:09:29,550
distributable interesting more than

00:09:27,240 --> 00:09:34,290
trying to sell the one customer and that

00:09:29,550 --> 00:09:37,259
is real data has real problems real data

00:09:34,290 --> 00:09:39,509
is something we can't share typically if

00:09:37,259 --> 00:09:42,600
we do get it then we're going to be

00:09:39,509 --> 00:09:45,569
contractually bound to not share with

00:09:42,600 --> 00:09:47,550
anybody else we can't usually even get

00:09:45,569 --> 00:09:49,980
it even when we're partnering with the

00:09:47,550 --> 00:09:52,319
people who generate the data they have

00:09:49,980 --> 00:09:54,660
obligations to the people who are

00:09:52,319 --> 00:09:56,579
actually having them equipment being

00:09:54,660 --> 00:09:58,980
measured so they can't even release it

00:09:56,579 --> 00:10:01,679
to us and we certainly can't release it

00:09:58,980 --> 00:10:03,660
to somebody else we can't break it i

00:10:01,679 --> 00:10:07,980
mean the interesting thing about data

00:10:03,660 --> 00:10:09,209
like this is what happens if and that if

00:10:07,980 --> 00:10:11,339
of course

00:10:09,209 --> 00:10:13,949
is a non-starter we aren't allowed to

00:10:11,339 --> 00:10:15,629
have 10 cars crash and watch what

00:10:13,949 --> 00:10:18,300
happens with the data we're not allowed

00:10:15,629 --> 00:10:20,429
to have the real systems go down and see

00:10:18,300 --> 00:10:23,790
how the failure tolerance handles all of

00:10:20,429 --> 00:10:25,470
that none of this really can be done and

00:10:23,790 --> 00:10:27,179
that's would be the fun stuff you know

00:10:25,470 --> 00:10:29,790
break it what's one thing if it runs

00:10:27,179 --> 00:10:33,509
it's much more interesting if it doesn't

00:10:29,790 --> 00:10:35,279
and also there's this continual problem

00:10:33,509 --> 00:10:39,509
with real data is it comes from the real

00:10:35,279 --> 00:10:42,059
world and we're not in charge that means

00:10:39,509 --> 00:10:44,879
that whatever is happening there we

00:10:42,059 --> 00:10:47,999
don't necessarily know except via the

00:10:44,879 --> 00:10:50,850
data and so we can't really tell if

00:10:47,999 --> 00:10:53,699
we're seeing the phenomena in the data

00:10:50,850 --> 00:10:56,189
that are real whereas if we were to make

00:10:53,699 --> 00:10:58,379
the data if we control the world we

00:10:56,189 --> 00:11:00,660
would be able to inject true phenomena

00:10:58,379 --> 00:11:02,670
that we really knew and then watch how

00:11:00,660 --> 00:11:05,369
it burbles through the system we'd be

00:11:02,670 --> 00:11:09,569
able to see that so real data has real

00:11:05,369 --> 00:11:12,720
problems there and fake data would have

00:11:09,569 --> 00:11:15,329
some very nice properties that we might

00:11:12,720 --> 00:11:18,059
like to take advantage of it can be

00:11:15,329 --> 00:11:20,910
built at any scale we can build it to be

00:11:18,059 --> 00:11:23,819
as odd or as normal as possible we can

00:11:20,910 --> 00:11:25,410
build it with true knowledge of the

00:11:23,819 --> 00:11:28,829
state of the universe and we can build

00:11:25,410 --> 00:11:31,679
it with arbitrary fidelity now this this

00:11:28,829 --> 00:11:36,119
idea of arbitrary fidelity is not that

00:11:31,679 --> 00:11:39,959
it will match exactly the mechanisms the

00:11:36,119 --> 00:11:44,100
exact form of the data but instead this

00:11:39,959 --> 00:11:47,699
is a thing we've been doing recently

00:11:44,100 --> 00:11:53,369
with customers is what we do is build

00:11:47,699 --> 00:11:55,679
KPI preserving data generators the idea

00:11:53,369 --> 00:11:58,499
is here we have live data real data we

00:11:55,679 --> 00:12:00,240
have a security boundary which hides all

00:11:58,499 --> 00:12:03,949
of the data and all of the real

00:12:00,240 --> 00:12:06,749
mechanisms away from anybody's useful

00:12:03,949 --> 00:12:08,879
observation so we have live data going

00:12:06,749 --> 00:12:12,329
into some system under test and we have

00:12:08,879 --> 00:12:15,779
key performance indicators or failure

00:12:12,329 --> 00:12:18,329
modes that we can observe in the system

00:12:15,779 --> 00:12:20,669
if it's a machine learning system then

00:12:18,329 --> 00:12:21,279
those are like false positive ratios or

00:12:20,669 --> 00:12:24,759
there

00:12:21,279 --> 00:12:26,740
score distributions whatever we care

00:12:24,759 --> 00:12:30,370
about in the data if it's going to be

00:12:26,740 --> 00:12:32,129
cars and such it's going to be speeds or

00:12:30,370 --> 00:12:34,930
in the range that's reasonable

00:12:32,129 --> 00:12:37,180
accelerations look reasonable data is

00:12:34,930 --> 00:12:41,829
going to be coming in at a plausible

00:12:37,180 --> 00:12:44,499
rate and so on so whatever we care most

00:12:41,829 --> 00:12:48,029
about the system according to the task

00:12:44,499 --> 00:12:50,980
that we're setting to ourselves will be

00:12:48,029 --> 00:12:52,689
replicated in the data if we're

00:12:50,980 --> 00:12:55,269
designing a system and trying to prove

00:12:52,689 --> 00:12:57,100
that it will handle volumes then volumes

00:12:55,269 --> 00:12:58,839
are the key thing if we're trying to

00:12:57,100 --> 00:13:01,600
prove that we can resolve certain

00:12:58,839 --> 00:13:04,480
frequencies of input then speed of

00:13:01,600 --> 00:13:07,269
sampling and insertion of those kinds of

00:13:04,480 --> 00:13:09,399
things are the key but then if all we do

00:13:07,269 --> 00:13:11,800
is we take fake data put it into the

00:13:09,399 --> 00:13:13,959
same system and make it so that the

00:13:11,800 --> 00:13:18,040
failure signatures and the KPIs match

00:13:13,959 --> 00:13:23,199
the data itself here could be very very

00:13:18,040 --> 00:13:25,389
different in furthermore we can just

00:13:23,199 --> 00:13:28,480
transport the seed and the schema of

00:13:25,389 --> 00:13:30,689
that randomly generated data out of the

00:13:28,480 --> 00:13:33,100
security boundary that's easy to do

00:13:30,689 --> 00:13:36,610
because it's easy to convince somebody

00:13:33,100 --> 00:13:40,019
that within one kilobyte of data that's

00:13:36,610 --> 00:13:42,189
in spectabile we could not have

00:13:40,019 --> 00:13:45,430
compromised millions of data points

00:13:42,189 --> 00:13:48,009
millions of people's data or hours of

00:13:45,430 --> 00:13:50,439
machine data so this is a relatively

00:13:48,009 --> 00:13:53,139
easy thing to do getting this live data

00:13:50,439 --> 00:13:55,000
out is essentially impossible but once

00:13:53,139 --> 00:13:57,540
we get the generator for this fig date

00:13:55,000 --> 00:14:00,220
out we can generate new fake data and

00:13:57,540 --> 00:14:03,730
have some confidence that if we put it

00:14:00,220 --> 00:14:05,860
into similar systems as are running in

00:14:03,730 --> 00:14:08,709
here possibly with innovations and

00:14:05,860 --> 00:14:12,819
novelties that it will produce similar

00:14:08,709 --> 00:14:15,220
results when we find good candidates

00:14:12,819 --> 00:14:17,620
things that appear to work better than

00:14:15,220 --> 00:14:19,990
the live system we can bring that new

00:14:17,620 --> 00:14:22,749
system under test inside the security

00:14:19,990 --> 00:14:24,850
boundary and verify that live data and

00:14:22,749 --> 00:14:26,769
fake data work the same to the extent

00:14:24,850 --> 00:14:29,079
they don't we can modify the generator

00:14:26,769 --> 00:14:31,360
again to produce more and more fadila

00:14:29,079 --> 00:14:34,660
test data under any scenarios that we

00:14:31,360 --> 00:14:35,170
like this is a technique for generating

00:14:34,660 --> 00:14:39,430
data

00:14:35,170 --> 00:14:41,889
that has far fewer far more degrees of

00:14:39,430 --> 00:14:44,230
freedom excuse me far fewer parameters

00:14:41,889 --> 00:14:48,070
that we need to match and therefore it's

00:14:44,230 --> 00:14:50,170
much much easier technique than actually

00:14:48,070 --> 00:14:52,839
matching the exact distribution it's

00:14:50,170 --> 00:14:55,600
also more secure in many cases we may

00:14:52,839 --> 00:14:57,190
not in certain circumstances even need

00:14:55,600 --> 00:14:59,560
to match the dimensionality of the

00:14:57,190 --> 00:15:02,889
original input in order to get useful

00:14:59,560 --> 00:15:07,120
simulations the fundamental idea here is

00:15:02,889 --> 00:15:10,779
that if it breaks the same if those fail

00:15:07,120 --> 00:15:14,470
your indicators if the KPIs match it's

00:15:10,779 --> 00:15:16,420
as good as the original if it weren't

00:15:14,470 --> 00:15:20,170
true that there's some important

00:15:16,420 --> 00:15:22,600
difference then fine we'll just make a

00:15:20,170 --> 00:15:25,570
self-fulfilling prophecy we will add

00:15:22,600 --> 00:15:30,850
that important difference into the KPIs

00:15:25,570 --> 00:15:33,100
and we will turn again to match so by a

00:15:30,850 --> 00:15:36,490
circular argument data that matches this

00:15:33,100 --> 00:15:39,490
white match is in every important way so

00:15:36,490 --> 00:15:42,880
let's do that all we need to do is pick

00:15:39,490 --> 00:15:45,220
some reasonable and plausible KPIs in

00:15:42,880 --> 00:15:47,560
this sort of thing we need sample data

00:15:45,220 --> 00:15:49,990
we need rates and volumes to be about

00:15:47,560 --> 00:15:52,870
right we need the number of samples to

00:15:49,990 --> 00:15:54,850
be realistic we need the complexity of

00:15:52,870 --> 00:15:56,769
the samples to be realistic we need

00:15:54,850 --> 00:15:59,680
somewhat plausible physics you know the

00:15:56,769 --> 00:16:01,990
car can't go from 0 to 500 kilometers

00:15:59,680 --> 00:16:04,029
per hour in 3 seconds we have to have

00:16:01,990 --> 00:16:07,149
reasonable accelerations so that we get

00:16:04,029 --> 00:16:09,430
decent visualizations we need to have

00:16:07,149 --> 00:16:12,010
plausible data semantics it should

00:16:09,430 --> 00:16:14,170
roughly look like the real data so that

00:16:12,010 --> 00:16:16,060
it compresses the same way can be

00:16:14,170 --> 00:16:19,269
understood by humans looking at it the

00:16:16,060 --> 00:16:21,910
same way and of course as always with

00:16:19,269 --> 00:16:24,550
cars your mileage may vary we may have

00:16:21,910 --> 00:16:27,579
other KPIs that you'd like that would

00:16:24,550 --> 00:16:29,350
change your way of doing this we're

00:16:27,579 --> 00:16:32,160
going to build an emulation that roughly

00:16:29,350 --> 00:16:35,529
with very whole lot of hand waving

00:16:32,160 --> 00:16:37,779
matches the physics of the situation

00:16:35,529 --> 00:16:40,089
matches the physics of the cars and

00:16:37,779 --> 00:16:42,579
we're going to turn that data spec into

00:16:40,089 --> 00:16:46,060
KPIs and we're going to match by tuning

00:16:42,579 --> 00:16:47,500
the data spec until we get decent way of

00:16:46,060 --> 00:16:53,029
generating data

00:16:47,500 --> 00:16:56,120
so it turns out this is pretty easy to

00:16:53,029 --> 00:16:59,209
do pretty easy to build realistic things

00:16:56,120 --> 00:17:00,500
now the real system is complex this is

00:16:59,209 --> 00:17:03,230
the real system that we might be

00:17:00,500 --> 00:17:04,850
designing there's an RF link the cars

00:17:03,230 --> 00:17:07,850
are out there on the track spinning

00:17:04,850 --> 00:17:09,920
around they transmit data via an RF

00:17:07,850 --> 00:17:12,650
length they have about 12 megabits per

00:17:09,920 --> 00:17:16,100
second they come in here the data first

00:17:12,650 --> 00:17:19,130
goes to the FIA that the referees pit it

00:17:16,100 --> 00:17:21,319
needs to be caught their first in a

00:17:19,130 --> 00:17:24,199
secure data set then it'll be

00:17:21,319 --> 00:17:27,260
distributed to each team I've only drawn

00:17:24,199 --> 00:17:29,929
one team what happens there is local

00:17:27,260 --> 00:17:32,690
analytics we need to do replication to

00:17:29,929 --> 00:17:34,940
an engineering work station so that if

00:17:32,690 --> 00:17:37,010
somebody has a laptop or something data

00:17:34,940 --> 00:17:39,169
can be streamed into that and persisted

00:17:37,010 --> 00:17:41,480
there so when they disconnect and go to

00:17:39,169 --> 00:17:43,280
a hotel that I they have access to a

00:17:41,480 --> 00:17:45,350
full history we also need to be

00:17:43,280 --> 00:17:47,270
streaming back to the factory to be

00:17:45,350 --> 00:17:50,090
doing all of those Monte Carlo and

00:17:47,270 --> 00:17:53,030
strategic simulations back there and of

00:17:50,090 --> 00:17:56,260
course archiving it hopefully something

00:17:53,030 --> 00:17:59,870
like Apache drill to be analyzed offline

00:17:56,260 --> 00:18:03,440
the particular implementation that we're

00:17:59,870 --> 00:18:05,840
going to use here today uses vampire

00:18:03,440 --> 00:18:07,490
streams for that but at these low data

00:18:05,840 --> 00:18:10,790
rates that were demonstrating kafka

00:18:07,490 --> 00:18:12,919
would work just fine the particular demo

00:18:10,790 --> 00:18:14,570
we're going to use here is considerably

00:18:12,919 --> 00:18:18,549
simplified because it's supposed to run

00:18:14,570 --> 00:18:21,250
in just a few VMs it's going to have a

00:18:18,549 --> 00:18:23,390
physics-based pseudo physics-based

00:18:21,250 --> 00:18:25,880
simulator there it's going to be

00:18:23,390 --> 00:18:28,610
generating engine and performance

00:18:25,880 --> 00:18:31,190
criterion they're based on emulations of

00:18:28,610 --> 00:18:35,000
a robot driven race going to drive that

00:18:31,190 --> 00:18:37,580
into streams out to a simple interface

00:18:35,000 --> 00:18:40,070
built on bootstrap and d3 for

00:18:37,580 --> 00:18:42,410
visualization jetty for a reddit rest

00:18:40,070 --> 00:18:45,850
interface to data and of course archive

00:18:42,410 --> 00:18:47,780
into a database and access the data via

00:18:45,850 --> 00:18:50,929
drill I think the part we're going to

00:18:47,780 --> 00:18:56,230
demo today is just this upper channel to

00:18:50,929 --> 00:18:56,230
show it generating data and accessing it

00:18:56,410 --> 00:19:01,930
make sense

00:18:58,800 --> 00:19:04,300
the hard part is this idea of KPI

00:19:01,930 --> 00:19:06,160
matching you'll see how some of the KPIs

00:19:04,300 --> 00:19:08,920
are things that were not yet matching

00:19:06,160 --> 00:19:10,990
the key generator here that we're going

00:19:08,920 --> 00:19:12,880
to use and that we're going to tune to

00:19:10,990 --> 00:19:15,160
produce the data we need and data

00:19:12,880 --> 00:19:17,830
volumes that we need is something called

00:19:15,160 --> 00:19:21,970
torques which is extensively used in

00:19:17,830 --> 00:19:24,580
research largely ironically in AI you'd

00:19:21,970 --> 00:19:26,380
think it was used in video games when it

00:19:24,580 --> 00:19:30,340
produces things like that but the idea

00:19:26,380 --> 00:19:32,170
is that a quasi physical situation like

00:19:30,340 --> 00:19:34,540
that where you have opponents who are

00:19:32,170 --> 00:19:36,130
doing actual physical things and you're

00:19:34,540 --> 00:19:38,020
trying to overtake them and they're

00:19:36,130 --> 00:19:40,570
trying to keep you from overtaking them

00:19:38,020 --> 00:19:43,870
is a very interesting domain to work in

00:19:40,570 --> 00:19:47,400
and having a simulator to do that gives

00:19:43,870 --> 00:19:51,070
everybody access to a common ground for

00:19:47,400 --> 00:19:54,400
competing in building these artificially

00:19:51,070 --> 00:19:56,920
intelligent drivers you can also of

00:19:54,400 --> 00:19:59,290
course control these manually but the

00:19:56,920 --> 00:20:02,290
primary use lately over the last five or

00:19:59,290 --> 00:20:04,750
so years for torques is actually in

00:20:02,290 --> 00:20:07,510
building automated systems there's a an

00:20:04,750 --> 00:20:09,280
ongoing grand prix of of robots that

00:20:07,510 --> 00:20:12,310
compete in various kinds of races

00:20:09,280 --> 00:20:16,630
various kinds of absurd situations that

00:20:12,310 --> 00:20:20,470
they're subjected to so we use torts and

00:20:16,630 --> 00:20:24,010
tune its inputs to produce boys are

00:20:20,470 --> 00:20:25,960
realistic outputs and we would like to

00:20:24,010 --> 00:20:28,120
prove out that the particular task here

00:20:25,960 --> 00:20:32,920
has proved out software architectures

00:20:28,120 --> 00:20:35,920
test certain software architectures for

00:20:32,920 --> 00:20:39,850
building data pipelines and for pushing

00:20:35,920 --> 00:20:42,370
data through all of those replicas we

00:20:39,850 --> 00:20:47,050
also want to turn you is to match the

00:20:42,370 --> 00:20:49,510
customer expectations not actually to

00:20:47,050 --> 00:20:51,070
play video games no no instead what

00:20:49,510 --> 00:20:54,340
we're going to be doing is simulating a

00:20:51,070 --> 00:20:56,620
production system and especially failure

00:20:54,340 --> 00:20:59,770
scenarios in order to prove to the

00:20:56,620 --> 00:21:02,710
customers that the systems downstream of

00:20:59,770 --> 00:21:05,880
the the RF link can handle things in

00:21:02,710 --> 00:21:07,990
adverse circumstances and we want to see

00:21:05,880 --> 00:21:10,360
supposing that the cars produced 10

00:21:07,990 --> 00:21:11,270
times as much volume that they then they

00:21:10,360 --> 00:21:14,210
do too

00:21:11,270 --> 00:21:19,430
eh could we build a system that handles

00:21:14,210 --> 00:21:22,700
that as well so the current status it

00:21:19,430 --> 00:21:24,350
works in a limited fashion it's

00:21:22,700 --> 00:21:26,690
available on github or at least it will

00:21:24,350 --> 00:21:28,910
be available shortly on github it's on

00:21:26,690 --> 00:21:30,980
github just hasn't been turned to public

00:21:28,910 --> 00:21:33,350
yet all of the systems you'll see here

00:21:30,980 --> 00:21:36,080
today are available the idea is that

00:21:33,350 --> 00:21:39,260
it's built with one vm for the physics

00:21:36,080 --> 00:21:42,020
simulator pushing data out to a data

00:21:39,260 --> 00:21:44,180
collection vm those both run in fairly

00:21:42,020 --> 00:21:46,520
small instances so you're not going to

00:21:44,180 --> 00:21:49,370
see huge performance out of them you can

00:21:46,520 --> 00:21:52,640
replicate either one on to more capable

00:21:49,370 --> 00:21:56,780
hardware and get much more capable

00:21:52,640 --> 00:21:59,210
results now we don't currently simulate

00:21:56,780 --> 00:22:02,150
enough of the parameters to get the data

00:21:59,210 --> 00:22:04,310
volumes that would be normally required

00:22:02,150 --> 00:22:07,400
in a production setting notably things

00:22:04,310 --> 00:22:10,970
like tire temperature times for at least

00:22:07,400 --> 00:22:16,570
multiple breaks disks and things like

00:22:10,970 --> 00:22:20,000
that are not simulated the 300 or so

00:22:16,570 --> 00:22:22,040
data sensors in the car which are

00:22:20,000 --> 00:22:24,650
sampled in rates from once every 10

00:22:22,040 --> 00:22:27,140
seconds to 10,000 times per second are

00:22:24,650 --> 00:22:30,650
here represented because of the limits

00:22:27,140 --> 00:22:33,190
of this simulator x samples 50 times per

00:22:30,650 --> 00:22:36,650
second against for six or eight

00:22:33,190 --> 00:22:40,160
parameters but this will be enhanced

00:22:36,650 --> 00:22:42,530
over time as we go forward the data rate

00:22:40,160 --> 00:22:45,530
is currently also fixed the real data

00:22:42,530 --> 00:22:47,570
rate changes over time as the car goes

00:22:45,530 --> 00:22:50,210
into corners for instance in a real

00:22:47,570 --> 00:22:51,980
formula 1 car the data rate spikes right

00:22:50,210 --> 00:22:54,560
there because a lot of self is changing

00:22:51,980 --> 00:22:57,170
in a straightaway the data weight rate

00:22:54,560 --> 00:23:00,800
drops a lot because things change much

00:22:57,170 --> 00:23:03,830
less quickly the data is currently

00:23:00,800 --> 00:23:07,280
collected in pure JSON but in practice

00:23:03,830 --> 00:23:09,620
it would be collected in the in the RF

00:23:07,280 --> 00:23:11,690
link through the RF link even in a

00:23:09,620 --> 00:23:14,780
columnar compressed form if you think

00:23:11,690 --> 00:23:19,370
about it data is commonly thought of in

00:23:14,780 --> 00:23:21,740
rows but if you store it as columns here

00:23:19,370 --> 00:23:23,540
are kind of json versions of those if

00:23:21,740 --> 00:23:24,950
you store it in columns then each of

00:23:23,540 --> 00:23:27,440
these arrays which

00:23:24,950 --> 00:23:30,770
a single column have the same kind of

00:23:27,440 --> 00:23:33,470
data from a same kind of distribution if

00:23:30,770 --> 00:23:35,660
for instance column c1 as timestamps

00:23:33,470 --> 00:23:37,910
they're going to step forward by

00:23:35,660 --> 00:23:40,130
relatively constant amounts and so

00:23:37,910 --> 00:23:43,880
they're subject to a lot of compression

00:23:40,130 --> 00:23:47,030
you can stick one of these columnar sub

00:23:43,880 --> 00:23:49,700
tables in as a single value if you have

00:23:47,030 --> 00:23:52,310
sufficiently general data structures and

00:23:49,700 --> 00:23:54,710
so in a message stream you could have

00:23:52,310 --> 00:23:57,710
messages coming along in time that

00:23:54,710 --> 00:23:59,920
actually have small blobs in them which

00:23:57,710 --> 00:24:03,680
are columnar data structures themselves

00:23:59,920 --> 00:24:06,950
those are subject to lots of compression

00:24:03,680 --> 00:24:09,050
and a very efficient standard way of

00:24:06,950 --> 00:24:10,730
storing time series data here's an

00:24:09,050 --> 00:24:14,810
example of how much from compression you

00:24:10,730 --> 00:24:17,390
can get we took 64-bit time samples time

00:24:14,810 --> 00:24:22,280
stamps we have differing amounts of

00:24:17,390 --> 00:24:25,010
jitter on a hundred micro second inter

00:24:22,280 --> 00:24:28,310
sample period and if the jitter is small

00:24:25,010 --> 00:24:31,250
the compressed size becomes very very

00:24:28,310 --> 00:24:33,710
small even with five microseconds out of

00:24:31,250 --> 00:24:36,410
a hundred jitter you get 10 to 1

00:24:33,710 --> 00:24:39,140
compression of those timestamps using

00:24:36,410 --> 00:24:41,920
very simple I can't quite read it but

00:24:39,140 --> 00:24:44,810
basically the idea here is you XOR

00:24:41,920 --> 00:24:47,840
adjacent samples and then you do binary

00:24:44,810 --> 00:24:50,660
packing of the residuals the residuals

00:24:47,840 --> 00:24:53,540
are mostly zeros because very few bits

00:24:50,660 --> 00:24:55,220
change this is a very common technique

00:24:53,540 --> 00:24:58,790
it's even more general than Delta

00:24:55,220 --> 00:25:01,760
sampling and other data the actual

00:24:58,790 --> 00:25:04,220
samples will also compress comparably in

00:25:01,760 --> 00:25:06,740
many cases now another thing to keep in

00:25:04,220 --> 00:25:09,050
mind is we want to have something that

00:25:06,740 --> 00:25:11,510
has the power of JSON in terms of

00:25:09,050 --> 00:25:14,390
flexibility here's a sample piece of

00:25:11,510 --> 00:25:17,090
data that only has three sensors for

00:25:14,390 --> 00:25:20,660
instance we'd like to be able to extend

00:25:17,090 --> 00:25:24,350
it fairly transparently adding another

00:25:20,660 --> 00:25:26,740
or one or two data samples and have the

00:25:24,350 --> 00:25:29,990
queries on new and old data work

00:25:26,740 --> 00:25:32,150
transparently across all of that that's

00:25:29,990 --> 00:25:35,420
one of the key advantages for using

00:25:32,150 --> 00:25:38,700
something like drill so I'm going to ask

00:25:35,420 --> 00:25:40,590
tug to come on up he's got the same

00:25:38,700 --> 00:25:43,590
I should running on his machine we'll

00:25:40,590 --> 00:25:49,080
see if we can plug it in and show how it

00:25:43,590 --> 00:25:56,790
looks if Murphy is is happy with us that

00:25:49,080 --> 00:25:59,120
is this is live demo after all that's

00:25:56,790 --> 00:25:59,120
right here

00:26:05,760 --> 00:26:14,070
do you want to the noisemaker you want

00:26:08,790 --> 00:26:16,020
me to talk he wants me to talk I will

00:26:14,070 --> 00:26:25,080
translate he'll speak English and I'll

00:26:16,020 --> 00:26:33,180
translate there comes a raise lots of

00:26:25,080 --> 00:26:35,130
cars that's a car you can see multiple

00:26:33,180 --> 00:26:37,710
cars here they're jostling for position

00:26:35,130 --> 00:26:40,590
and you can see that their speeds are

00:26:37,710 --> 00:26:43,050
quite comparable as they go along that's

00:26:40,590 --> 00:26:45,690
a time axis on the horizontal right now

00:26:43,050 --> 00:26:52,250
on this visualization not a distance

00:26:45,690 --> 00:26:56,580
axis there we go here's the distance

00:26:52,250 --> 00:26:59,280
measurement this is for rpms here they

00:26:56,580 --> 00:27:01,410
these cars you can see Jags in the speed

00:26:59,280 --> 00:27:05,100
curve those are typically caused by

00:27:01,410 --> 00:27:07,200
shifting anomalies and you can see

00:27:05,100 --> 00:27:08,880
things shifting as they go along all of

00:27:07,200 --> 00:27:13,610
this is controlled automatically by a

00:27:08,880 --> 00:27:13,610
bot ridden to plug into the simulator

00:27:15,350 --> 00:27:22,820
here comes a drill query so the data is

00:27:20,070 --> 00:27:26,970
being archived in a map our DB table in

00:27:22,820 --> 00:27:30,030
JSON format i believe it's nested json

00:27:26,970 --> 00:27:33,150
format so that you get a header and then

00:27:30,030 --> 00:27:35,520
you get a raise of records this is the

00:27:33,150 --> 00:27:37,440
row embedded form instead of the column

00:27:35,520 --> 00:27:39,750
embedded form but the same idea of

00:27:37,440 --> 00:27:49,200
flattening would apply to the column at

00:27:39,750 --> 00:27:54,600
vetted form so what query was that so

00:27:49,200 --> 00:27:56,040
the average speed by car and race so

00:27:54,600 --> 00:27:59,430
let's take a look at the core again can

00:27:56,040 --> 00:28:01,590
you make that bigger and so this is a

00:27:59,430 --> 00:28:04,110
real-time query as things have been

00:28:01,590 --> 00:28:07,440
collected right so there's a sub-query

00:28:04,110 --> 00:28:12,060
in there which is flattening the data

00:28:07,440 --> 00:28:14,940
that makes it look completely relational

00:28:12,060 --> 00:28:17,100
it's being pulled out of a map our DB

00:28:14,940 --> 00:28:19,170
table but that's the same as the HBase

00:28:17,100 --> 00:28:22,050
API that could as well have been

00:28:19,170 --> 00:28:25,740
in HBase table there the flattened here

00:28:22,050 --> 00:28:29,300
takes that array and makes records for

00:28:25,740 --> 00:28:34,770
every element in the flattened array and

00:28:29,300 --> 00:28:37,650
then you get normal sequel syntax above

00:28:34,770 --> 00:28:41,220
that for computing averages except for

00:28:37,650 --> 00:28:44,010
the fact that we have nested values and

00:28:41,220 --> 00:28:46,350
we saw the nested values previously in

00:28:44,010 --> 00:28:47,880
those slides and again that's an

00:28:46,350 --> 00:28:51,960
extension to sequel that's very useful

00:28:47,880 --> 00:28:53,880
for these IOT sorts of applications the

00:28:51,960 --> 00:28:55,580
race is still going on red seems to be

00:28:53,880 --> 00:29:01,680
doing quite well oops there it is

00:28:55,580 --> 00:29:05,070
finished somebody won okay so there you

00:29:01,680 --> 00:29:06,570
go any questions anybody have an

00:29:05,070 --> 00:29:11,040
application like this with real-time

00:29:06,570 --> 00:29:15,570
data real-time measurements any physics

00:29:11,040 --> 00:29:19,460
sorts of things happening out there okay

00:29:15,570 --> 00:29:19,460
we're going to start assigning questions

00:29:20,030 --> 00:29:31,110
so yes okay so it's time for QA anyone

00:29:27,990 --> 00:29:33,170
wants to go first i'm going to victimize

00:29:31,110 --> 00:29:36,000
people i know first we aren't careful

00:29:33,170 --> 00:29:38,610
I'm not sure it's an entirely related

00:29:36,000 --> 00:29:43,310
question but you said there are boats

00:29:38,610 --> 00:29:46,140
that are controlling these simulators

00:29:43,310 --> 00:29:49,650
has anyone working on well there is a

00:29:46,140 --> 00:29:52,530
lot of hype about self-driving cars is

00:29:49,650 --> 00:29:56,160
anyone building self-driving racing cars

00:29:52,530 --> 00:29:58,140
these are self-driving racing cars if I

00:29:56,160 --> 00:29:59,940
actually had to build my own racing car

00:29:58,140 --> 00:30:02,610
and pay for that I don't think I'd want

00:29:59,940 --> 00:30:04,620
one of these spots to to be running it

00:30:02,610 --> 00:30:07,170
because a lot of times these things are

00:30:04,620 --> 00:30:09,570
tuned to take chances particularly

00:30:07,170 --> 00:30:12,470
that's how you win sometimes against

00:30:09,570 --> 00:30:15,900
other BOTS but these are very similar to

00:30:12,470 --> 00:30:18,840
those self-driving cars except that they

00:30:15,900 --> 00:30:22,920
get a few extra cues from the race

00:30:18,840 --> 00:30:26,420
simulator so for instance the in torques

00:30:22,920 --> 00:30:30,240
the centerline of the track is given as

00:30:26,420 --> 00:30:32,610
an input to the bots so that they can

00:30:30,240 --> 00:30:34,380
tell what the veer off angle is

00:30:32,610 --> 00:30:37,110
for the center line for where they're

00:30:34,380 --> 00:30:39,840
going at the given moment and steering

00:30:37,110 --> 00:30:42,210
toward that center line is the simplest

00:30:39,840 --> 00:30:46,770
and quickest but you don't do nearly as

00:30:42,210 --> 00:30:48,600
well as if you do cutting into the apex

00:30:46,770 --> 00:30:51,660
of corners and things like that the

00:30:48,600 --> 00:30:54,150
advanced spots in these simulations and

00:30:51,660 --> 00:30:56,940
in the follow-on Stu torques actually do

00:30:54,150 --> 00:30:59,549
neural net learning as they're doing

00:30:56,940 --> 00:31:02,610
laps so as the track conditions change

00:30:59,549 --> 00:31:05,610
or where as competitors change they will

00:31:02,610 --> 00:31:08,880
change how they take each curve in order

00:31:05,610 --> 00:31:11,370
to learn fast laps so these are

00:31:08,880 --> 00:31:14,700
comparable but much much simpler because

00:31:11,370 --> 00:31:16,559
the sensory input is simpler I don't

00:31:14,700 --> 00:31:19,559
know if anybody's building real race

00:31:16,559 --> 00:31:22,500
cars my guess is that would be a very

00:31:19,559 --> 00:31:24,330
expensive hobby at least initially if

00:31:22,500 --> 00:31:27,090
you've ever seen the Google cars

00:31:24,330 --> 00:31:31,080
especially in the rain they drive kind

00:31:27,090 --> 00:31:33,809
of like this they're very hesitant in

00:31:31,080 --> 00:31:35,640
any sort of difficult situation so if

00:31:33,809 --> 00:31:37,650
cars were passing them like that they

00:31:35,640 --> 00:31:40,320
would just kind of back out there

00:31:37,650 --> 00:31:41,610
wouldn't be a great race car and that's

00:31:40,320 --> 00:31:43,740
because of the cost of the cars of

00:31:41,610 --> 00:31:46,200
course and the cost of the liability in

00:31:43,740 --> 00:31:50,669
this sort of situation they would be

00:31:46,200 --> 00:31:53,549
very expensive I think thank you are

00:31:50,669 --> 00:31:56,100
there other questions so do they really

00:31:53,549 --> 00:31:58,200
drive back do they really good crumbs

00:31:56,100 --> 00:32:00,299
what you said that they are baking

00:31:58,200 --> 00:32:02,280
copper you know when someone is passing

00:32:00,299 --> 00:32:04,919
by do they stop I've never seen a Google

00:32:02,280 --> 00:32:07,440
car back up but I have seen it stop and

00:32:04,919 --> 00:32:09,990
then move just an inch at a time forward

00:32:07,440 --> 00:32:11,760
in situations where it can't really see

00:32:09,990 --> 00:32:14,940
around the corner rain is on the

00:32:11,760 --> 00:32:17,610
scanning sensors and so it's nearly

00:32:14,940 --> 00:32:20,940
blinded as a real human would be but

00:32:17,610 --> 00:32:24,720
they tend to have a do no harm sort of

00:32:20,940 --> 00:32:26,929
attitude one did get a ticket recently

00:32:24,720 --> 00:32:29,820
because it was driving too slowly and

00:32:26,929 --> 00:32:34,890
they have matched so far the accident

00:32:29,820 --> 00:32:37,850
rates of 75 year olds which is kind of

00:32:34,890 --> 00:32:39,900
you know cautious driving but humans

00:32:37,850 --> 00:32:43,100
assume that the other people are going

00:32:39,900 --> 00:32:45,480
to be reasonable and take chances with

00:32:43,100 --> 00:32:46,230
generally good results and generally

00:32:45,480 --> 00:32:48,540
without some

00:32:46,230 --> 00:32:50,640
prising the drivers behind them sure

00:32:48,540 --> 00:32:53,429
there was a question in the middle of

00:32:50,640 --> 00:33:02,880
the back oh yeah he's going to throw you

00:32:53,429 --> 00:33:04,710
I just translate for you too so so the

00:33:02,880 --> 00:33:07,230
question is why did we choose JSON

00:33:04,710 --> 00:33:09,570
instead of Avro or proto buffs or

00:33:07,230 --> 00:33:10,830
something like that well the simplest

00:33:09,570 --> 00:33:13,620
reason is because it has to go on a

00:33:10,830 --> 00:33:17,580
slide and that looks just a whole lot

00:33:13,620 --> 00:33:19,950
better on slides but a more important

00:33:17,580 --> 00:33:23,429
reason is that this is the first version

00:33:19,950 --> 00:33:26,429
of this and so visual debug ability is

00:33:23,429 --> 00:33:28,980
very important right now a there would

00:33:26,429 --> 00:33:32,970
be a variety of formats that would be

00:33:28,980 --> 00:33:35,190
usable keep going forward some are not

00:33:32,970 --> 00:33:37,320
because it has to still be a record by

00:33:35,190 --> 00:33:39,780
record format so something that proto

00:33:37,320 --> 00:33:41,910
before Avro would be acceptable as long

00:33:39,780 --> 00:33:44,429
as we have a schema registry in the case

00:33:41,910 --> 00:33:48,150
of both of those some other / formats

00:33:44,429 --> 00:33:51,120
like arrow might be very useful for the

00:33:48,150 --> 00:33:54,059
little glob of data so where we have

00:33:51,120 --> 00:33:56,429
Colin or compressed data arrow would be

00:33:54,059 --> 00:34:01,260
a very reasonable candidate for that

00:33:56,429 --> 00:34:04,860
compressed columns once you get to it

00:34:01,260 --> 00:34:08,609
any sort of binary form of JSON oh hi is

00:34:04,860 --> 00:34:11,820
a very reasonable category there because

00:34:08,609 --> 00:34:13,889
it's a binary JSON encoding SBE simple

00:34:11,820 --> 00:34:15,770
binary encoding which is designed by the

00:34:13,889 --> 00:34:19,020
financial world would it be another one

00:34:15,770 --> 00:34:21,359
given that we started and are using oh

00:34:19,020 --> 00:34:24,330
hi in other areas we would probably

00:34:21,359 --> 00:34:27,510
gravitate toward that it would be

00:34:24,330 --> 00:34:29,490
efficient for the binary and compressed

00:34:27,510 --> 00:34:32,159
and coatings and it would still have

00:34:29,490 --> 00:34:34,619
jasons maddox which many of these other

00:34:32,159 --> 00:34:37,710
things do not have so that's probably

00:34:34,619 --> 00:34:40,320
where we'll go there's no good reason to

00:34:37,710 --> 00:34:44,490
use just plain JSON unless data rates

00:34:40,320 --> 00:34:50,010
are low and you know Jason's pretty fast

00:34:44,490 --> 00:34:51,840
but not as fast as a dedicated binary

00:34:50,010 --> 00:34:56,520
encoding it's got good physical

00:34:51,840 --> 00:35:00,270
properties starting punched guns better

00:34:56,520 --> 00:35:01,980
than punch cards you say yeah yeah been

00:35:00,270 --> 00:35:06,960
there done that i hope i don't do it

00:35:01,980 --> 00:35:09,660
ever again hi it seemed to seems to me

00:35:06,960 --> 00:35:11,910
that if you have like 10 or 20 or like a

00:35:09,660 --> 00:35:15,440
thousand kpi's you will have to do

00:35:11,910 --> 00:35:20,190
millions of simulations before you feed

00:35:15,440 --> 00:35:23,070
on them correctly is this true so the

00:35:20,190 --> 00:35:26,400
data rate the computational load for

00:35:23,070 --> 00:35:29,520
different kpi's varies a lot the data

00:35:26,400 --> 00:35:33,200
rates already for doing the

00:35:29,520 --> 00:35:35,700
visualization dwarf the physics based

00:35:33,200 --> 00:35:37,830
computations in the in the current

00:35:35,700 --> 00:35:40,050
situation so if we want to make things

00:35:37,830 --> 00:35:42,570
more efficient and produce lots and lots

00:35:40,050 --> 00:35:46,710
of kpi's so for instance we can pretty

00:35:42,570 --> 00:35:49,619
easily emulate the tire temperature

00:35:46,710 --> 00:35:53,490
based on a simple cooling model and a

00:35:49,619 --> 00:35:55,350
simple energy conversion model due to

00:35:53,490 --> 00:35:58,020
tire slippage so we could get tire

00:35:55,350 --> 00:35:59,820
temperatures break temperatures and and

00:35:58,020 --> 00:36:02,760
many other things like that very quickly

00:35:59,820 --> 00:36:05,550
and those change very slowly outside of

00:36:02,760 --> 00:36:07,650
the corners and so a variable speed

00:36:05,550 --> 00:36:10,290
integrator on those will have no

00:36:07,650 --> 00:36:13,050
problems keeping up the overall engine

00:36:10,290 --> 00:36:16,680
speed and car speed and acceleration

00:36:13,050 --> 00:36:19,560
model runs at a 50 Hertz integration

00:36:16,680 --> 00:36:21,869
rate that sampling on that and it runs a

00:36:19,560 --> 00:36:23,280
variable step integrator within that but

00:36:21,869 --> 00:36:26,760
the total amount of floating-point

00:36:23,280 --> 00:36:28,830
computation is really quite long so I

00:36:26,760 --> 00:36:32,240
don't think that adding another hundred

00:36:28,830 --> 00:36:34,830
kpi's would be nearly as expensive as

00:36:32,240 --> 00:36:37,320
simulating the cars and doing the

00:36:34,830 --> 00:36:40,530
collision detection and I think it will

00:36:37,320 --> 00:36:44,330
be far far less expensive than rendering

00:36:40,530 --> 00:36:46,320
the the visualization of the race itself

00:36:44,330 --> 00:36:53,340
so I don't think that should be a

00:36:46,320 --> 00:36:55,980
problem but you compress the entire data

00:36:53,340 --> 00:36:57,780
set in a singular random seed in this

00:36:55,980 --> 00:37:01,530
case we compress the entire data set

00:36:57,780 --> 00:37:02,550
into a single race and car configuration

00:37:01,530 --> 00:37:05,460
file we

00:37:02,550 --> 00:37:08,370
don't constrain the seed even we could

00:37:05,460 --> 00:37:10,730
and that is good practice it's often

00:37:08,370 --> 00:37:13,350
true that you can match your KPIs

00:37:10,730 --> 00:37:15,930
suppose I have Gaussian distributions if

00:37:13,350 --> 00:37:19,050
i set the seed i might be able to say

00:37:15,930 --> 00:37:21,870
big one close little one far another big

00:37:19,050 --> 00:37:24,300
one far away if i don't constrain the

00:37:21,870 --> 00:37:26,580
seeds i might get distribution over many

00:37:24,300 --> 00:37:28,830
many different configurations and i

00:37:26,580 --> 00:37:32,340
might not match the KPIs the specific

00:37:28,830 --> 00:37:34,080
kpi's i want so constraining the seed

00:37:32,340 --> 00:37:38,490
and learning across different seed

00:37:34,080 --> 00:37:40,350
values might help me match the KPIs in

00:37:38,490 --> 00:37:45,210
certain situations and I've certainly

00:37:40,350 --> 00:37:47,580
seen that in this one we have very very

00:37:45,210 --> 00:37:49,710
loose KPIs and so I don't think that

00:37:47,580 --> 00:37:54,420
that's necessary I think we do need to

00:37:49,710 --> 00:37:56,850
add measurements to match the volume and

00:37:54,420 --> 00:38:01,830
data right kpi's but I don't think we

00:37:56,850 --> 00:38:04,380
need to do much else ok you is this

00:38:01,830 --> 00:38:08,310
github what is not available publicly

00:38:04,380 --> 00:38:13,620
right now it is public so yes it is it's

00:38:08,310 --> 00:38:15,570
under github.com / there it is it's

00:38:13,620 --> 00:38:21,590
almost invisible there it is more

00:38:15,570 --> 00:38:25,650
visible racing time series so yeah

00:38:21,590 --> 00:38:31,320
clever man I believe there is a time for

00:38:25,650 --> 00:38:33,180
one more question everyone is looking

00:38:31,320 --> 00:38:35,160
surely there's more video game players

00:38:33,180 --> 00:38:37,290
here and that they just don't want to

00:38:35,160 --> 00:38:38,760
admit it I think they already thinking

00:38:37,290 --> 00:38:44,070
about this chip that they need to

00:38:38,760 --> 00:38:46,920
collect in single-threaded q yeah during

00:38:44,070 --> 00:38:48,540
the months does not necessarily need to

00:38:46,920 --> 00:38:51,990
be single threaded that's an interesting

00:38:48,540 --> 00:38:53,880
point it's very common here that you'll

00:38:51,990 --> 00:38:55,740
have a lot of different delays between

00:38:53,880 --> 00:38:58,950
different channels so you've already

00:38:55,740 --> 00:39:01,320
lost ordering between cars of data and

00:38:58,950 --> 00:39:03,630
the ordering of data within a single car

00:39:01,320 --> 00:39:05,610
versus another car doesn't entirely make

00:39:03,630 --> 00:39:07,110
sense except at the level of

00:39:05,610 --> 00:39:09,360
milliseconds not at the level of

00:39:07,110 --> 00:39:13,950
microseconds and so would be very

00:39:09,360 --> 00:39:16,349
natural to partition the stream on car

00:39:13,950 --> 00:39:19,380
so we would already have quite a bit of

00:39:16,349 --> 00:39:22,410
parallelism there and then within

00:39:19,380 --> 00:39:25,500
certain parameters you need to maintain

00:39:22,410 --> 00:39:27,900
lockstep of the sampling but other slow

00:39:25,500 --> 00:39:29,400
ones could also be pulled out into

00:39:27,900 --> 00:39:32,070
different topics so we wouldn't have

00:39:29,400 --> 00:39:33,780
single threaded I was actually talking

00:39:32,070 --> 00:39:35,310
about this chips distribution that

00:39:33,780 --> 00:39:38,730
people will be collecting you know

00:39:35,310 --> 00:39:40,589
outside of this room yesterday United so

00:39:38,730 --> 00:39:43,950
maybe next time you will paralyze their

00:39:40,589 --> 00:39:48,810
task on those two sure we have two

00:39:43,950 --> 00:39:50,550
partitions for that each of us okay so

00:39:48,810 --> 00:39:53,510
we add the top of the hour Thank You Ted

00:39:50,550 --> 00:39:53,510

YouTube URL: https://www.youtube.com/watch?v=0jeBnTpyzyA


