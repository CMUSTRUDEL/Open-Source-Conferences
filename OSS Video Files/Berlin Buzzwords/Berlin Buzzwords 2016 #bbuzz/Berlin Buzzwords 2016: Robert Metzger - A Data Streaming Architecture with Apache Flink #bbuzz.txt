Title: Berlin Buzzwords 2016: Robert Metzger - A Data Streaming Architecture with Apache Flink #bbuzz
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Data Streaming is emerging as a new and increasingly popular architectural pattern for the data infrastructure. Data streaming architectures embrace the fact that data in practice never has the form of static data sets, but is continuously produced as streams of events over time. Moving away from centralized "state of the world" databases and warehouses, the applications work directly on the streams of events and on application-specific local state that is an aggregate of the history of events.

Among the many disruptive promises of streaming architectures are:
- decreased latency from signal to decision
- a unified way of handling real-time and historic data processing
- time travel queries
- simple versioning of applications and their state (think git update/rollback)
- simplification of data processing stack.

This talk introduces the data streaming architecture paradigm, and shows how to build an exemplary set of simple but representative applications using the open source systems Apache Flink and Apache Kafka. Delivered by the creators of the Apache Flink framework, the talk explains the building blocks of data streaming applications, including:
- event stream logs
- transformations and windows
- working with time
- application state and consistency

Read more:
https://2016.berlinbuzzwords.de/session/data-streaming-architecture-apache-flink

About Robert Metzger:
https://2016.berlinbuzzwords.de/users/robert-metzger

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:03,770 --> 00:00:08,910
thanks a lot for the nice introduction

00:00:05,759 --> 00:00:11,460
um hoga so I'm going to talk about

00:00:08,910 --> 00:00:13,710
apache fling today and my name is Robert

00:00:11,460 --> 00:00:16,410
Metzker I am a committee mpm CMM of the

00:00:13,710 --> 00:00:18,270
Pesci fling project and I'm also working

00:00:16,410 --> 00:00:21,359
at data artisans here in Berlin a

00:00:18,270 --> 00:00:25,670
company that has been founded by some of

00:00:21,359 --> 00:00:29,070
the creators of Apache fling and and

00:00:25,670 --> 00:00:33,840
today's talk is going to introduce you

00:00:29,070 --> 00:00:35,760
to a pet chief link but i would like to

00:00:33,840 --> 00:00:38,160
start first with my take on the stream

00:00:35,760 --> 00:00:39,960
processing space so why i think that

00:00:38,160 --> 00:00:41,430
streaming is a really big thing so if

00:00:39,960 --> 00:00:43,230
you look at the program of this

00:00:41,430 --> 00:00:46,079
conference you also saw that a lot of

00:00:43,230 --> 00:00:47,640
talks are about streaming and i'm giving

00:00:46,079 --> 00:00:50,820
my personal view where I think that

00:00:47,640 --> 00:00:52,230
streaming is such a big thing and then I

00:00:50,820 --> 00:00:54,960
will do a little introduction to fling

00:00:52,230 --> 00:00:58,140
and and then I will take one very common

00:00:54,960 --> 00:01:00,629
batch use case and review how we can

00:00:58,140 --> 00:01:03,570
transform this batch use case into a

00:01:00,629 --> 00:01:05,850
streaming use case to show you how you

00:01:03,570 --> 00:01:10,049
can run existing batch workloads with a

00:01:05,850 --> 00:01:14,670
streaming engine and also what new use

00:01:10,049 --> 00:01:18,210
cases or new business values you can get

00:01:14,670 --> 00:01:21,110
out of a streaming system and after that

00:01:18,210 --> 00:01:25,380
I will do a little demo where I showcase

00:01:21,110 --> 00:01:29,040
this streaming etl job and and how to

00:01:25,380 --> 00:01:31,079
get started and use flink and soap a

00:01:29,040 --> 00:01:34,110
chief link is an open-source stream

00:01:31,079 --> 00:01:36,540
processing framework while building the

00:01:34,110 --> 00:01:39,090
engine we are focusing on low latency

00:01:36,540 --> 00:01:41,220
and high throughput and we are also

00:01:39,090 --> 00:01:45,149
focusing on state food stream processing

00:01:41,220 --> 00:01:48,450
which means that unlike first generation

00:01:45,149 --> 00:01:51,049
stream processors the system is aware of

00:01:48,450 --> 00:01:53,460
what you're doing within your coat so

00:01:51,049 --> 00:01:55,229
when you for example have a variable

00:01:53,460 --> 00:01:57,750
that is counting a number of events or

00:01:55,229 --> 00:02:00,509
you have a hashmap and we're all your

00:01:57,750 --> 00:02:02,280
data is contained you can tell fling

00:02:00,509 --> 00:02:05,490
look this is my data please take care of

00:02:02,280 --> 00:02:06,899
it make backups and so on and this gives

00:02:05,490 --> 00:02:09,539
you a lot of advantages over other

00:02:06,899 --> 00:02:11,970
systems and of course it's a distributed

00:02:09,539 --> 00:02:13,950
system similar to all the other big data

00:02:11,970 --> 00:02:17,239
frame works it runs on come on

00:02:13,950 --> 00:02:19,830
the hardware and and s name suggests

00:02:17,239 --> 00:02:23,310
it's developed in the purchase of a

00:02:19,830 --> 00:02:26,940
foundation in March we released the 1.0

00:02:23,310 --> 00:02:29,160
version and and the community is

00:02:26,940 --> 00:02:33,150
currently discussing and working towards

00:02:29,160 --> 00:02:35,160
the 1.1 release of link stefan yesterday

00:02:33,150 --> 00:02:37,200
presented for example query with sade

00:02:35,160 --> 00:02:40,590
which might be a new feature that is in

00:02:37,200 --> 00:02:43,310
the 1.1 version and fling is used widely

00:02:40,590 --> 00:02:47,370
in production i think some of our users

00:02:43,310 --> 00:02:49,290
present here at passwords and you can

00:02:47,370 --> 00:02:54,239
find blog posts and so on describing

00:02:49,290 --> 00:02:58,140
what they are doing with link um so what

00:02:54,239 --> 00:03:01,410
about this whole streaming pass that

00:02:58,140 --> 00:03:03,780
everybody is talking about and in my

00:03:01,410 --> 00:03:06,680
opinion streaming is the biggest change

00:03:03,780 --> 00:03:10,970
in the data infrastructure since Hadoop

00:03:06,680 --> 00:03:13,680
for multiple reasons so number one and

00:03:10,970 --> 00:03:15,959
it radically simplifies your

00:03:13,680 --> 00:03:19,440
infrastructure so you need fewer systems

00:03:15,959 --> 00:03:22,019
you don't need to build a fancy lambda

00:03:19,440 --> 00:03:24,810
architecture to cover your patch and

00:03:22,019 --> 00:03:28,200
your real time streaming workloads you

00:03:24,810 --> 00:03:30,660
can do everything in one system you can

00:03:28,200 --> 00:03:35,370
do your own with you can do more with

00:03:30,660 --> 00:03:37,440
your data and you can do it faster so by

00:03:35,370 --> 00:03:38,850
having this low latency engine that can

00:03:37,440 --> 00:03:40,680
do batch processing and stream

00:03:38,850 --> 00:03:42,540
processing at the same time you can

00:03:40,680 --> 00:03:46,769
cover your existing use cases and you

00:03:42,540 --> 00:03:48,930
can add new ones as well and yeah you

00:03:46,769 --> 00:03:51,299
can confirm quickly subsume bet you can

00:03:48,930 --> 00:03:54,900
get rid of the whole of a lot of fetch

00:03:51,299 --> 00:03:57,269
pipelines and you might wonder now house

00:03:54,900 --> 00:03:59,819
is possible am I not a bit over

00:03:57,269 --> 00:04:01,889
promising here but the reason is why I

00:03:59,819 --> 00:04:04,769
am why I can confidently say that I

00:04:01,889 --> 00:04:08,099
really believe that streaming systems

00:04:04,769 --> 00:04:10,470
are a really big thing is that um if you

00:04:08,099 --> 00:04:12,510
look at the way data is produced in the

00:04:10,470 --> 00:04:14,220
real world you will see that data is

00:04:12,510 --> 00:04:16,950
actually produced in a continuous

00:04:14,220 --> 00:04:19,590
fashion so if you have for example a web

00:04:16,950 --> 00:04:21,450
server running somewhere and in the

00:04:19,590 --> 00:04:23,220
batch world you basically have two web

00:04:21,450 --> 00:04:25,349
server and then you have local lock

00:04:23,220 --> 00:04:27,419
fight so every every times every time

00:04:25,349 --> 00:04:28,080
somebody clicks somewhere an entry is

00:04:27,419 --> 00:04:31,110
written

00:04:28,080 --> 00:04:33,720
to the apache server web block and then

00:04:31,110 --> 00:04:37,830
you have like mapreduce shop also or

00:04:33,720 --> 00:04:40,050
flume which is pulling the data from the

00:04:37,830 --> 00:04:41,550
web servers into HDFS and then you have

00:04:40,050 --> 00:04:46,069
another job which is pulling the data

00:04:41,550 --> 00:04:48,629
from HDFS into Cassandra whatever and

00:04:46,069 --> 00:04:51,259
there's a traditional way of processing

00:04:48,629 --> 00:04:54,090
data with a batch system however the

00:04:51,259 --> 00:04:55,830
data is actually produced in a real-time

00:04:54,090 --> 00:04:57,389
fashion so every time somebody is

00:04:55,830 --> 00:05:00,659
clicking somewhere a new event is

00:04:57,389 --> 00:05:03,659
generated and new systems like blink and

00:05:00,659 --> 00:05:05,789
Kafka embrace this nature so instead of

00:05:03,659 --> 00:05:09,090
storing the fire first on the web server

00:05:05,789 --> 00:05:11,729
then in HDFS and then to the Cassandra

00:05:09,090 --> 00:05:13,860
or whatever database and you can just

00:05:11,729 --> 00:05:16,199
stream the events from the web server

00:05:13,860 --> 00:05:17,879
immediately to Kafka and then the stream

00:05:16,199 --> 00:05:22,949
processor can pick up the data from

00:05:17,879 --> 00:05:24,840
Kafka and process it from there so real

00:05:22,949 --> 00:05:26,849
world data is produced continuously and

00:05:24,840 --> 00:05:32,099
we just need to commute continuously

00:05:26,849 --> 00:05:34,110
processes it process it as well and to

00:05:32,099 --> 00:05:36,870
do that I am suggesting to use a pet

00:05:34,110 --> 00:05:39,000
chief link and this is the Apache fling

00:05:36,870 --> 00:05:41,370
stack so when you're downloading flink

00:05:39,000 --> 00:05:44,909
from our website you're getting this

00:05:41,370 --> 00:05:47,190
entire stack and and the core of the

00:05:44,909 --> 00:05:49,860
stack is this streaming data flow

00:05:47,190 --> 00:05:54,389
runtime it's an engine that allows you

00:05:49,860 --> 00:05:57,389
to em execute and data flow graphs in a

00:05:54,389 --> 00:05:58,560
distributed system and for starting this

00:05:57,389 --> 00:06:00,479
engine there are different options

00:05:58,560 --> 00:06:02,669
there's the yarn deployment option

00:06:00,479 --> 00:06:05,370
there's the standalone cluster mode and

00:06:02,669 --> 00:06:07,800
that some of our users are also using

00:06:05,370 --> 00:06:09,870
for deploying flink on docker and

00:06:07,800 --> 00:06:11,490
there's the local mode the local mode

00:06:09,870 --> 00:06:15,330
allows you to start fleeing from within

00:06:11,490 --> 00:06:17,400
your IDE and or on your laptop and which

00:06:15,330 --> 00:06:19,529
allows you to set breakpoints the park

00:06:17,400 --> 00:06:23,250
and try out the code that will run on

00:06:19,529 --> 00:06:25,560
the cluster later also locally and and

00:06:23,250 --> 00:06:28,800
the interesting observation about this

00:06:25,560 --> 00:06:31,830
engine is that it doesn't really know

00:06:28,800 --> 00:06:33,659
it's not really aware of the the source

00:06:31,830 --> 00:06:35,940
of the application that it's executing

00:06:33,659 --> 00:06:38,250
so whether the program has been

00:06:35,940 --> 00:06:40,529
implemented in the data stream API or

00:06:38,250 --> 00:06:41,940
the data set API for patch doesn't

00:06:40,529 --> 00:06:43,770
really matter for the engine

00:06:41,940 --> 00:06:47,190
so there is like an abstraction between

00:06:43,770 --> 00:06:48,660
these api's and the engine and and for

00:06:47,190 --> 00:06:50,640
the data set for the best programs

00:06:48,660 --> 00:06:53,130
there's an optimizer which is generating

00:06:50,640 --> 00:06:55,050
a graph for the engine but since it's a

00:06:53,130 --> 00:06:58,410
streaming talk I will only focus on the

00:06:55,050 --> 00:07:02,130
streaming side of our api's so the data

00:06:58,410 --> 00:07:04,620
stream api is very nice easy to use API

00:07:02,130 --> 00:07:09,030
that feels very natural for Java

00:07:04,620 --> 00:07:10,890
programmers and and it's the most

00:07:09,030 --> 00:07:13,080
commonly used API for for writing

00:07:10,890 --> 00:07:15,510
streaming programs it's very similar to

00:07:13,080 --> 00:07:17,520
high level API is known from the bash

00:07:15,510 --> 00:07:20,610
world so it's really easy to get started

00:07:17,520 --> 00:07:23,850
with fling and and on top we've built

00:07:20,610 --> 00:07:26,430
some use case specific API sand

00:07:23,850 --> 00:07:28,470
abstractions so for example we have a CP

00:07:26,430 --> 00:07:31,110
library for complex event processing and

00:07:28,470 --> 00:07:32,850
which allows you to detect event

00:07:31,110 --> 00:07:34,970
patterns in your stream so if you're

00:07:32,850 --> 00:07:38,220
doing some I don't know fraud analysis

00:07:34,970 --> 00:07:42,300
or behavior detection or so you can use

00:07:38,220 --> 00:07:46,290
this EP and engine and which can do

00:07:42,300 --> 00:07:50,250
pattern matching um an upcoming feature

00:07:46,290 --> 00:07:54,110
in 1.1 is stream sequel which allows you

00:07:50,250 --> 00:07:57,750
to define sequel statements on streams

00:07:54,110 --> 00:08:00,890
so this will allow users or even more

00:07:57,750 --> 00:08:03,630
users to enter the streaming space and

00:08:00,890 --> 00:08:05,550
then the Apache beam another Apache

00:08:03,630 --> 00:08:08,700
project currently undergoing incubation

00:08:05,550 --> 00:08:13,830
it has been contributed to apache from

00:08:08,700 --> 00:08:18,810
google and theme is basically um one API

00:08:13,830 --> 00:08:21,090
for different engines in the OP source

00:08:18,810 --> 00:08:24,810
world and also on Google's cloud engine

00:08:21,090 --> 00:08:26,970
and and flink as one of the most

00:08:24,810 --> 00:08:28,530
sophisticated runners for this beam API

00:08:26,970 --> 00:08:30,900
so currently spark and flame are

00:08:28,530 --> 00:08:32,460
supported and then the SS compatibility

00:08:30,900 --> 00:08:34,110
matrix on their website where you can

00:08:32,460 --> 00:08:36,360
see which features are supported by

00:08:34,110 --> 00:08:38,550
which engine but the cool thing about

00:08:36,360 --> 00:08:40,800
themes you can implement your stuff in

00:08:38,550 --> 00:08:44,640
one a p.i and run it with different

00:08:40,800 --> 00:08:46,410
engines and there are other layers like

00:08:44,640 --> 00:08:48,540
the storm ap added allows you to run

00:08:46,410 --> 00:08:50,550
existing storm drops on fling and

00:08:48,540 --> 00:08:54,130
there's patches are more which allows

00:08:50,550 --> 00:08:59,290
you to to do machine learning on streams

00:08:54,130 --> 00:09:05,140
um maybe a different way of looking at

00:08:59,290 --> 00:09:08,890
fling is looking at its features so I've

00:09:05,140 --> 00:09:11,950
created four categories and number one

00:09:08,890 --> 00:09:14,050
is true streaming so it's a real

00:09:11,950 --> 00:09:16,150
streaming engine that is focusing on

00:09:14,050 --> 00:09:17,980
high throughput and low latency so by

00:09:16,150 --> 00:09:19,870
having this engine that can deploy this

00:09:17,980 --> 00:09:22,030
data flow graphs which are continuously

00:09:19,870 --> 00:09:24,850
running and immediately pushing data

00:09:22,030 --> 00:09:28,420
forward as it enters the system and we

00:09:24,850 --> 00:09:31,900
can process the data really quick and

00:09:28,420 --> 00:09:33,970
fast and also this engine has very well

00:09:31,900 --> 00:09:36,130
behaved behavior when it comes to flow

00:09:33,970 --> 00:09:39,760
control so the system naturally behaves

00:09:36,130 --> 00:09:42,550
under under load situations and it can

00:09:39,760 --> 00:09:44,170
slow down em upstream operators if

00:09:42,550 --> 00:09:47,410
operators are not fast enough for

00:09:44,170 --> 00:09:49,570
processing the data and then the engine

00:09:47,410 --> 00:09:51,790
has support for event time which allows

00:09:49,570 --> 00:09:53,770
you which basically makes a system aware

00:09:51,790 --> 00:09:56,260
of the time when the event happened in

00:09:53,770 --> 00:09:59,730
reality and this way we can reproach

00:09:56,260 --> 00:10:02,740
reprocess historic data and we are not

00:09:59,730 --> 00:10:06,490
the system is not suffering from network

00:10:02,740 --> 00:10:08,770
failures or out of clock out of order

00:10:06,490 --> 00:10:11,230
clocks I will explain later how exactly

00:10:08,770 --> 00:10:14,170
the event time is working but it's

00:10:11,230 --> 00:10:19,270
really crucial feature for for streaming

00:10:14,170 --> 00:10:21,100
engine em the api's and libraries as I

00:10:19,270 --> 00:10:23,200
said a very rich for example you can do

00:10:21,100 --> 00:10:27,790
the complex event processing and we have

00:10:23,200 --> 00:10:30,340
very flexible window API that allows you

00:10:27,790 --> 00:10:32,710
to define windows on time on the number

00:10:30,340 --> 00:10:35,230
of elements you can also build session

00:10:32,710 --> 00:10:38,530
windows that allow you to em detect

00:10:35,230 --> 00:10:40,660
basically user sessions or for example a

00:10:38,530 --> 00:10:42,610
user session when somebody is clicking

00:10:40,660 --> 00:10:45,910
on web servers so if link allows you to

00:10:42,610 --> 00:10:48,040
detect that some events belong to the

00:10:45,910 --> 00:10:52,720
same user and then you can do analysis

00:10:48,040 --> 00:10:55,290
on that session m and flink allows you

00:10:52,720 --> 00:10:58,830
to do state food stream processing so

00:10:55,290 --> 00:11:02,440
the system can be made aware of the

00:10:58,830 --> 00:11:04,710
state and the variables that you're

00:11:02,440 --> 00:11:07,059
using within your application code and

00:11:04,710 --> 00:11:10,029
with this we can

00:11:07,059 --> 00:11:12,189
and provide exactly once guarantees for

00:11:10,029 --> 00:11:14,459
m4 tolerance so when something is

00:11:12,189 --> 00:11:17,019
failing we can restore your state and

00:11:14,459 --> 00:11:20,229
the internal windows of link are also

00:11:17,019 --> 00:11:22,059
using the the registered state of link

00:11:20,229 --> 00:11:23,859
so we are also taking care of the window

00:11:22,059 --> 00:11:27,159
content so they are not lost in case of

00:11:23,859 --> 00:11:30,429
a failure and we have these safe points

00:11:27,159 --> 00:11:33,369
that allow you to create globally

00:11:30,429 --> 00:11:35,979
consistent checkpoints and of your state

00:11:33,369 --> 00:11:38,469
this way you can take a snapshot of your

00:11:35,979 --> 00:11:40,269
state store it in HDFS and then for

00:11:38,469 --> 00:11:42,909
example deploy a new version of your job

00:11:40,269 --> 00:11:44,589
or upgrade your flink version or to some

00:11:42,909 --> 00:11:48,759
plaster maintenance and then you can

00:11:44,589 --> 00:11:54,399
restore your job from that specific safe

00:11:48,759 --> 00:11:56,379
point so let's move an existing and data

00:11:54,399 --> 00:12:02,309
analysis job into the streaming world

00:11:56,379 --> 00:12:06,639
and one very common use case is etl

00:12:02,309 --> 00:12:09,669
extract transform load and I've looked a

00:12:06,639 --> 00:12:12,459
bit for definition for etl and I found

00:12:09,669 --> 00:12:14,619
many that's why I came up with my own so

00:12:12,459 --> 00:12:17,229
Adam won't edit one more and my

00:12:14,619 --> 00:12:20,769
definition is just move data from A to B

00:12:17,229 --> 00:12:24,549
and transform it on the way and so the

00:12:20,769 --> 00:12:27,639
old approach for atl m works like this

00:12:24,549 --> 00:12:31,449
you have some data sources for example

00:12:27,639 --> 00:12:36,399
server logs or machine data mobile or

00:12:31,449 --> 00:12:38,889
IOT some sensors and so on and and then

00:12:36,399 --> 00:12:42,219
you are ingesting this data into one

00:12:38,889 --> 00:12:45,729
common raw data store some Hadoop

00:12:42,219 --> 00:12:47,949
vendors call this the data lake and we

00:12:45,729 --> 00:12:49,539
just put your raw data and you'll figure

00:12:47,949 --> 00:12:54,359
out later on what to do with the data

00:12:49,539 --> 00:12:56,999
and then you have some periodic jobs and

00:12:54,359 --> 00:12:59,499
which read the data from this data lake

00:12:56,999 --> 00:13:01,749
normalize it clean it and then put it

00:12:59,499 --> 00:13:05,409
into some system like elasticsearch or

00:13:01,749 --> 00:13:08,739
into a or RC columnar file format

00:13:05,409 --> 00:13:11,769
for later analysis for example with

00:13:08,739 --> 00:13:14,439
Impala or hive and so on and then the

00:13:11,769 --> 00:13:16,359
last step is to do aggregations on the

00:13:14,439 --> 00:13:18,579
data for example if you're doing a

00:13:16,359 --> 00:13:20,220
website or so and you want to see the

00:13:18,579 --> 00:13:23,280
the user behavior

00:13:20,220 --> 00:13:26,160
again you're using periodic batch jobs

00:13:23,280 --> 00:13:29,990
and ingest the data and into these data

00:13:26,160 --> 00:13:38,390
stores like Cassandra Redis or my sequel

00:13:29,990 --> 00:13:42,660
um so how can we move such a batch

00:13:38,390 --> 00:13:45,360
architecture into the streaming world so

00:13:42,660 --> 00:13:48,480
instead of using HDFS we're using a

00:13:45,360 --> 00:13:51,330
Patrick Kafka for the for storing the

00:13:48,480 --> 00:13:54,300
raw data so all these data sources are

00:13:51,330 --> 00:13:56,580
just sending their stuff into Kafka and

00:13:54,300 --> 00:14:00,780
we figure out later on what exactly to

00:13:56,580 --> 00:14:05,700
do with it then we are using a stream

00:14:00,780 --> 00:14:07,650
processor and apache flink has for

00:14:05,700 --> 00:14:09,870
example a connector for patrick africa

00:14:07,650 --> 00:14:12,240
which is also exactly once for the

00:14:09,870 --> 00:14:14,310
consumer so in case of a failure and

00:14:12,240 --> 00:14:16,440
flink we can just restore and nothing is

00:14:14,310 --> 00:14:19,860
lost on everything is in sync then

00:14:16,440 --> 00:14:22,380
within fling and I've put like a generic

00:14:19,860 --> 00:14:25,320
data cleansing task here and which is

00:14:22,380 --> 00:14:26,670
removing invalid records and so on and

00:14:25,320 --> 00:14:28,890
then some of the data goes to a

00:14:26,670 --> 00:14:30,780
transformation and alert generation and

00:14:28,890 --> 00:14:33,830
the output of this data could for

00:14:30,780 --> 00:14:36,270
example go to elasticsearch again and to

00:14:33,830 --> 00:14:38,640
rolling file sync so the rolling file

00:14:36,270 --> 00:14:40,470
sync is for example creating new

00:14:38,640 --> 00:14:47,250
directory every minute where the system

00:14:40,470 --> 00:14:49,080
is putting data into em yeah and the the

00:14:47,250 --> 00:14:52,440
other part of the clean data is going

00:14:49,080 --> 00:14:55,530
into time windows to do some real time

00:14:52,440 --> 00:14:59,070
applications so we have for example jdbc

00:14:55,530 --> 00:15:01,890
sync or a cassandra sink um that allows

00:14:59,070 --> 00:15:07,320
you to send data from fling and into

00:15:01,890 --> 00:15:08,970
these other systems so you see that the

00:15:07,320 --> 00:15:11,610
architecture is already a bit simplified

00:15:08,970 --> 00:15:13,350
because you have this one system here in

00:15:11,610 --> 00:15:16,140
the middle that is taking care of the

00:15:13,350 --> 00:15:18,630
clean data and the aggregated data and

00:15:16,140 --> 00:15:22,020
that is also taking care of consistency

00:15:18,630 --> 00:15:24,810
so as I said the Cassandra the kafka

00:15:22,020 --> 00:15:26,520
connector is taking part of links

00:15:24,810 --> 00:15:28,740
snapshotting so it's exactly once and

00:15:26,520 --> 00:15:31,890
then for example our Cassandra sink is

00:15:28,740 --> 00:15:33,720
also providing em exactly one semantics

00:15:31,890 --> 00:15:37,970
for idempotent updates

00:15:33,720 --> 00:15:41,009
so the data is always in sync and

00:15:37,970 --> 00:15:44,029
another advantage of this architecture

00:15:41,009 --> 00:15:48,240
is that you're reducing the latency and

00:15:44,029 --> 00:15:50,670
drastically so the events are processed

00:15:48,240 --> 00:15:52,500
immediately as they arrive as they enter

00:15:50,670 --> 00:15:55,199
into this architecture into the system

00:15:52,500 --> 00:15:57,029
in the previous architecture you saw

00:15:55,199 --> 00:15:59,040
that I had these two loading jobs

00:15:57,029 --> 00:16:02,040
between the stages that we are loading

00:15:59,040 --> 00:16:05,339
the data from the web servers IOT data

00:16:02,040 --> 00:16:08,009
into an HFS and then this other job

00:16:05,339 --> 00:16:10,889
which is creating the aggregated data in

00:16:08,009 --> 00:16:13,589
this case data is processed as it enters

00:16:10,889 --> 00:16:16,319
the system it can immediately walk into

00:16:13,589 --> 00:16:20,879
an elastic search or a file system so

00:16:16,319 --> 00:16:22,500
the latency goes down drastically and so

00:16:20,879 --> 00:16:25,949
if you if you're comparing the different

00:16:22,500 --> 00:16:28,680
approaches that exists in space and this

00:16:25,949 --> 00:16:32,040
periodic petshop approach is in the

00:16:28,680 --> 00:16:33,480
range of hours maybe some users if

00:16:32,040 --> 00:16:35,129
they're using fancy hardware they can

00:16:33,480 --> 00:16:37,910
get down to minutes with a batch system

00:16:35,129 --> 00:16:40,829
but it's pretty expensive and and

00:16:37,910 --> 00:16:43,019
tedious to to run such an infrastructure

00:16:40,829 --> 00:16:45,540
because you have to keep all your bed

00:16:43,019 --> 00:16:48,120
shops running all the time it's

00:16:45,540 --> 00:16:51,120
complicated then for example apache

00:16:48,120 --> 00:16:53,279
spark has this batch processor with

00:16:51,120 --> 00:16:56,430
micro purchase built in so basically

00:16:53,279 --> 00:16:58,379
it's just a logical and development from

00:16:56,430 --> 00:17:02,850
this periodic batch shop that is

00:16:58,379 --> 00:17:04,980
triggered by a workflow manager and to

00:17:02,850 --> 00:17:08,520
this triggering into the street into the

00:17:04,980 --> 00:17:10,110
best processor itself so spark itself or

00:17:08,520 --> 00:17:14,100
other systems that are using micro

00:17:10,110 --> 00:17:16,169
patches and they just trigger batches

00:17:14,100 --> 00:17:18,329
continuously and from the outside it

00:17:16,169 --> 00:17:20,220
looks like a stream processor with that

00:17:18,329 --> 00:17:22,500
approach you can get down to seconds and

00:17:20,220 --> 00:17:24,589
but only with the stream processor you

00:17:22,500 --> 00:17:28,530
can really get into this milliseconds m

00:17:24,589 --> 00:17:32,510
latency area and you see this little

00:17:28,530 --> 00:17:35,309
stars here so your mileage may vary and

00:17:32,510 --> 00:17:38,669
don't believe my numbers believe your

00:17:35,309 --> 00:17:40,320
own numbers and I can they say this was

00:17:38,669 --> 00:17:42,270
confidence because Flinx really easy to

00:17:40,320 --> 00:17:46,049
use so in half a day you should be able

00:17:42,270 --> 00:17:47,700
to do little proof of concept and try it

00:17:46,049 --> 00:17:49,860
out yourself on your own

00:17:47,700 --> 00:17:51,480
hardware with your own requirements and

00:17:49,860 --> 00:17:54,149
so on and then you can do your own

00:17:51,480 --> 00:17:57,960
measurements and see what the latency is

00:17:54,149 --> 00:17:59,789
that you get for you use case um so

00:17:57,960 --> 00:18:03,659
please don't trust numbers try it out

00:17:59,789 --> 00:18:08,850
yourself another advantage of advantage

00:18:03,659 --> 00:18:13,380
is this event time awareness so the

00:18:08,850 --> 00:18:15,419
events that we are processing and they

00:18:13,380 --> 00:18:18,090
might arrive out of order in the system

00:18:15,419 --> 00:18:19,769
so if you're looking in this at this

00:18:18,090 --> 00:18:21,870
architecture again you have these three

00:18:19,769 --> 00:18:23,370
different sources and these three

00:18:21,870 --> 00:18:28,470
different sources might have different

00:18:23,370 --> 00:18:31,529
clocks um so imagine for example we have

00:18:28,470 --> 00:18:33,779
this mobile phone and somebody with a

00:18:31,529 --> 00:18:37,139
mobile phone is walking into a factory

00:18:33,779 --> 00:18:40,019
so the factory Wi-Fi recognizes okay

00:18:37,139 --> 00:18:43,019
somebody entered the factory then this

00:18:40,019 --> 00:18:45,389
Wi-Fi recognizes okay he's in a factory

00:18:43,019 --> 00:18:48,330
a nap on the way on the mobile is

00:18:45,389 --> 00:18:50,309
sending a message to the web server so

00:18:48,330 --> 00:18:53,880
these blue events here are all related

00:18:50,309 --> 00:18:56,850
to the same real world event however the

00:18:53,880 --> 00:18:59,519
clock of the mobile is off so the Wi-Fi

00:18:56,850 --> 00:19:03,299
clock says at 11 28 a person entered the

00:18:59,519 --> 00:19:07,950
factory the app set says close to 11 29

00:19:03,299 --> 00:19:11,399
I got this event so the clocks are out

00:19:07,950 --> 00:19:13,470
of sync another issue is network still a

00:19:11,399 --> 00:19:17,880
network delay so maybe the network of

00:19:13,470 --> 00:19:19,679
the factory is delayed or not working

00:19:17,880 --> 00:19:22,350
all the time so it might happen that

00:19:19,679 --> 00:19:24,149
events arrived much later in kafka then

00:19:22,350 --> 00:19:26,429
events from the web server or from the

00:19:24,149 --> 00:19:28,230
mobile phone another issue is machine

00:19:26,429 --> 00:19:30,899
failures it could happen that for

00:19:28,230 --> 00:19:34,830
example machines at fling or in Kafka or

00:19:30,899 --> 00:19:37,559
somewhere else are failing and this way

00:19:34,830 --> 00:19:40,380
we are not processing data for for short

00:19:37,559 --> 00:19:44,909
period of time with stream processors

00:19:40,380 --> 00:19:47,100
event I'm aware we can basically tell

00:19:44,909 --> 00:19:49,440
the system to use the time when the

00:19:47,100 --> 00:19:51,269
event happened in reality and not the

00:19:49,440 --> 00:19:53,519
time when the event arrived in the

00:19:51,269 --> 00:19:56,250
system and this way you can do stuff

00:19:53,519 --> 00:19:59,789
like reprocessing and you always get

00:19:56,250 --> 00:20:04,919
correct results also in case of failures

00:19:59,789 --> 00:20:06,600
so let's turn this into reality and so

00:20:04,919 --> 00:20:10,429
I've prepared a little demo that is also

00:20:06,600 --> 00:20:14,129
doing this streaming et al so I've

00:20:10,429 --> 00:20:18,479
created two drops one very small job so

00:20:14,129 --> 00:20:20,850
this small job M is so this says if what

00:20:18,479 --> 00:20:22,649
does it say flink Twitter sauce so

00:20:20,850 --> 00:20:25,070
inflicted as a Twitter search sauce

00:20:22,649 --> 00:20:28,320
which is reading data from Twitter and

00:20:25,070 --> 00:20:30,659
intro Kafka and then there's a second

00:20:28,320 --> 00:20:32,999
chop the streaming et al chop and which

00:20:30,659 --> 00:20:35,940
is reading the data from Kafka into this

00:20:32,999 --> 00:20:39,779
job so let's have a look at what the job

00:20:35,940 --> 00:20:41,279
is doing so this is the topology here's

00:20:39,779 --> 00:20:44,070
the kafka sauce which is reading the

00:20:41,279 --> 00:20:45,570
data from Kafka here's a theater

00:20:44,070 --> 00:20:48,179
operation which is featuring all the

00:20:45,570 --> 00:20:51,509
records that are system data so in from

00:20:48,179 --> 00:20:53,220
this M Twitter data source you're

00:20:51,509 --> 00:20:55,830
getting tweets and you're also getting

00:20:53,220 --> 00:20:58,080
some system data from from Twitter and

00:20:55,830 --> 00:21:00,179
with this further I'm just removing all

00:20:58,080 --> 00:21:02,669
the system data to get the tweets only

00:21:00,179 --> 00:21:05,309
and then it's splitting up into three

00:21:02,669 --> 00:21:07,619
different flows the first flow is a

00:21:05,309 --> 00:21:09,960
rolling file sync here I'm filtering for

00:21:07,619 --> 00:21:11,729
all the tweets which are English so I'm

00:21:09,960 --> 00:21:13,859
figuring on the field length language

00:21:11,729 --> 00:21:16,049
equals English and then i'm writing this

00:21:13,859 --> 00:21:18,599
to a rolling file sync so in a class

00:21:16,049 --> 00:21:20,909
that you would use HDFS or MS and s3 um

00:21:18,599 --> 00:21:24,830
or you can also use a local file system

00:21:20,909 --> 00:21:27,389
like I do in my demo the second flow is

00:21:24,830 --> 00:21:30,899
doing a window aggregation so I'm

00:21:27,389 --> 00:21:33,539
counting the language of the tweets in

00:21:30,899 --> 00:21:35,369
10 seconds ambling windows so I'm

00:21:33,539 --> 00:21:38,399
collecting tweets for 10 seconds then

00:21:35,369 --> 00:21:40,080
I'm looking at their language and I'm

00:21:38,399 --> 00:21:42,029
counting each of it each of the

00:21:40,080 --> 00:21:43,499
languages and then I'm sending the

00:21:42,029 --> 00:21:45,179
result after 10 seconds into

00:21:43,499 --> 00:21:46,950
elasticsearch at the tumbling window

00:21:45,179 --> 00:21:48,539
which is always collecting for 10

00:21:46,950 --> 00:21:50,359
seconds then pushing the result and then

00:21:48,539 --> 00:21:53,999
it's collecting for another 10 seconds m

00:21:50,359 --> 00:21:56,669
and the last job here or the last part

00:21:53,999 --> 00:22:01,859
of this job is doing a streaming word

00:21:56,669 --> 00:22:05,190
count so I'm extracting only the text

00:22:01,859 --> 00:22:07,349
field from the tweet and to get just the

00:22:05,190 --> 00:22:09,330
string not just full Jason with all the

00:22:07,349 --> 00:22:11,519
user data location and so on from

00:22:09,330 --> 00:22:13,739
Twitter then I'm tokenizing the word so

00:22:11,519 --> 00:22:14,970
I'm taking this whole text string and

00:22:13,739 --> 00:22:17,759
splitting it through to into the

00:22:14,970 --> 00:22:21,509
individual words and into a tableau to

00:22:17,759 --> 00:22:23,759
so much comma 1 it's just the classical

00:22:21,509 --> 00:22:25,950
word count you know from every juice so

00:22:23,759 --> 00:22:28,409
this is basically the mappers and this

00:22:25,950 --> 00:22:29,970
is the reducer in this case since we're

00:22:28,409 --> 00:22:32,220
on a stream we cannot really reduce

00:22:29,970 --> 00:22:36,230
because we would basically collect data

00:22:32,220 --> 00:22:39,450
forever and that's why I'm using a

00:22:36,230 --> 00:22:42,690
one-minute time window for collecting

00:22:39,450 --> 00:22:45,259
the frequencies of the word so there's a

00:22:42,690 --> 00:22:48,509
window standing open for one minute and

00:22:45,259 --> 00:22:51,210
collecting em all the words and deck

00:22:48,509 --> 00:22:53,159
hounds at every 10 seconds I'm

00:22:51,210 --> 00:22:55,769
evaluating this window so it's basically

00:22:53,159 --> 00:22:58,049
a sliding window every 10 seconds it's

00:22:55,769 --> 00:23:01,169
sliding forward but it's always of size

00:22:58,049 --> 00:23:03,720
10 and this way this operator is

00:23:01,169 --> 00:23:07,409
emitting every 10 seconds and the

00:23:03,720 --> 00:23:09,720
frequencies of the words in the tweets

00:23:07,409 --> 00:23:12,739
of the last minute then there's a

00:23:09,720 --> 00:23:15,869
top-end window which is just sorting the

00:23:12,739 --> 00:23:18,869
frequency and so that I can get like the

00:23:15,869 --> 00:23:22,049
top 10 tweets of the last minute and

00:23:18,869 --> 00:23:23,609
then I'm writing this to Kafka so that's

00:23:22,049 --> 00:23:30,749
the job that I'm going to present you

00:23:23,609 --> 00:23:34,200
now um you can see my screen and so the

00:23:30,749 --> 00:23:40,889
first thing that I'm going to do is I'm

00:23:34,200 --> 00:23:43,739
going to connect to the consume tweet

00:23:40,889 --> 00:23:46,710
word count topic and Kafka so as I said

00:23:43,739 --> 00:23:50,070
I'm writing the data to Kafka and this

00:23:46,710 --> 00:23:54,929
topic is reading the events from Kafka

00:23:50,070 --> 00:24:01,710
am reading these top end events from

00:23:54,929 --> 00:24:04,200
Kafka alright the next step is to start

00:24:01,710 --> 00:24:06,059
fling so when you download flink you get

00:24:04,200 --> 00:24:13,580
basically can you see the screen it's

00:24:06,059 --> 00:24:13,580
probably a bit too small M yeah

00:24:14,400 --> 00:24:19,280
I try to increase the font size a bit

00:24:20,030 --> 00:24:33,000
okay so I'm doing bin so now I've

00:24:30,900 --> 00:24:36,240
started fling and once you started flink

00:24:33,000 --> 00:24:38,720
on localhost 8081 you can see the web

00:24:36,240 --> 00:24:41,190
interface the web dashboard of link and

00:24:38,720 --> 00:24:43,140
so right now you see that there's one

00:24:41,190 --> 00:24:45,870
task manager connected test one

00:24:43,140 --> 00:24:50,970
processing slots and they are all

00:24:45,870 --> 00:24:53,220
available and and yeah one more thing so

00:24:50,970 --> 00:24:54,870
as I said you can do patch and stream

00:24:53,220 --> 00:24:56,400
processing with the same engine and what

00:24:54,870 --> 00:24:58,620
I'm going to do now is I'm basically

00:24:56,400 --> 00:25:00,780
going to do batch processing only with a

00:24:58,620 --> 00:25:03,080
streaming engine because i have a kafka

00:25:00,780 --> 00:25:05,880
topic as I've showed you on the slides

00:25:03,080 --> 00:25:09,690
with the tweets and the scaf car topic

00:25:05,880 --> 00:25:12,150
contains I think 500 megabytes of all

00:25:09,690 --> 00:25:14,640
tweets from a few days ago so I'm now

00:25:12,150 --> 00:25:17,820
just reading all the oat we old tweets

00:25:14,640 --> 00:25:19,920
from Kafka em and the interesting thing

00:25:17,820 --> 00:25:22,650
is that due to this event time I will

00:25:19,920 --> 00:25:24,870
still get counts on the top end window

00:25:22,650 --> 00:25:27,210
because i'm using event time it would

00:25:24,870 --> 00:25:31,820
still for the historic data create

00:25:27,210 --> 00:25:42,660
correct windows em with accounts so um

00:25:31,820 --> 00:25:46,160
let me start the streaming etl job um so

00:25:42,660 --> 00:25:48,660
now you see here that the job is running

00:25:46,160 --> 00:25:51,480
so you see this topology that I showed

00:25:48,660 --> 00:25:56,250
you earlier you see that here the

00:25:51,480 --> 00:26:00,320
operators are M processing data so it's

00:25:56,250 --> 00:26:03,500
processed 40,000 records already 59 m

00:26:00,320 --> 00:26:06,570
and counting and here you're seeing ah

00:26:03,500 --> 00:26:09,930
it's not very well visible so here

00:26:06,570 --> 00:26:12,720
you're seeing all the top end counts for

00:26:09,930 --> 00:26:15,210
the tweets so you see here the the time

00:26:12,720 --> 00:26:18,600
so it's currently a tune first now sat

00:26:15,210 --> 00:26:23,040
jun 2nd m end the top end for the top 10

00:26:18,600 --> 00:26:26,160
m verts from the word count and written

00:26:23,040 --> 00:26:27,370
into Kafka into this M word count topic

00:26:26,160 --> 00:26:30,760
in Kafka

00:26:27,370 --> 00:26:38,200
um maybe I can increase the font size

00:26:30,760 --> 00:26:43,360
even further so that you can see it so

00:26:38,200 --> 00:26:47,470
can you now see it so so now we are at

00:26:43,360 --> 00:26:50,530
June 6 15 24 so that is probably the

00:26:47,470 --> 00:26:53,110
last time when i try it out this demo

00:26:50,530 --> 00:26:56,620
and you see that the most frequent word

00:26:53,110 --> 00:27:00,820
is retweet and then it's probably new

00:26:56,620 --> 00:27:03,190
line a the and so on so I didn't do any

00:27:00,820 --> 00:27:07,059
stop word filtering or so I just took

00:27:03,190 --> 00:27:09,820
the raw data ok so now it's stopped

00:27:07,059 --> 00:27:13,420
doing anything because there's no new

00:27:09,820 --> 00:27:16,140
data being written into Kafka so what I

00:27:13,420 --> 00:27:19,450
will do now is I start a little job

00:27:16,140 --> 00:27:22,720
called Twitter into Kafka so it's

00:27:19,450 --> 00:27:28,390
connecting to Twitter I'm out let me

00:27:22,720 --> 00:27:32,500
open em another terminal so this is

00:27:28,390 --> 00:27:35,260
called consume tweet it's just the

00:27:32,500 --> 00:27:37,390
listening on the kafka topic where i'm

00:27:35,260 --> 00:27:41,590
going to write the tweets so i'm now

00:27:37,390 --> 00:27:45,700
starting this job i'm just starting it

00:27:41,590 --> 00:27:47,740
from the IDE and now you should see ya

00:27:45,700 --> 00:27:50,080
so now all the tweets are showing up in

00:27:47,740 --> 00:27:52,270
this so this is just the tweets that I'm

00:27:50,080 --> 00:27:58,510
getting from Twitter that I'm riding

00:27:52,270 --> 00:28:02,970
into Kafka and um I think in one minute

00:27:58,510 --> 00:28:06,130
we will start seeing em the word count

00:28:02,970 --> 00:28:09,309
for the top end of these tweets that I'm

00:28:06,130 --> 00:28:11,950
currently collecting em from Twitter the

00:28:09,309 --> 00:28:14,320
reason why there is a delay is because

00:28:11,950 --> 00:28:17,440
i'm using water marks so if link has

00:28:14,320 --> 00:28:21,010
this built-in bottom arc system for am

00:28:17,440 --> 00:28:23,290
handling late arrivals so as i explained

00:28:21,010 --> 00:28:26,470
we have these events that arrived out of

00:28:23,290 --> 00:28:31,090
order and so the question for the

00:28:26,470 --> 00:28:35,320
windows in flink is always um ok it's

00:28:31,090 --> 00:28:38,890
starting and so now we saw jun 7 11 26

00:28:35,320 --> 00:28:41,560
and so it is now apparently you have 10

00:28:38,890 --> 00:28:44,980
minutes left m

00:28:41,560 --> 00:28:50,200
so now it's slowly starting to write the

00:28:44,980 --> 00:28:52,510
top n apparently I'm getting some I

00:28:50,200 --> 00:28:54,580
don't know arab tweets so I'm just it's

00:28:52,510 --> 00:28:57,310
not the full stream of Twitter it's just

00:28:54,580 --> 00:29:02,830
the sample of the of the tweets from

00:28:57,310 --> 00:29:05,350
them um so yeah so now you see it's it's

00:29:02,830 --> 00:29:08,770
starting to write events to the top n

00:29:05,350 --> 00:29:13,120
and you also see in the web interface

00:29:08,770 --> 00:29:17,790
and that it's processing data in real

00:29:13,120 --> 00:29:23,140
time and s events come from the from the

00:29:17,790 --> 00:29:26,010
Twitter sauce also I've created a little

00:29:23,140 --> 00:29:33,360
I'm really not an expert in cabana

00:29:26,010 --> 00:29:44,260
cabana dashboard and where you can see

00:29:33,360 --> 00:29:46,540
the where you can see the data so here

00:29:44,260 --> 00:29:48,400
you can see the distribution of the

00:29:46,540 --> 00:29:51,160
languages so English is of course the

00:29:48,400 --> 00:29:53,950
success english is of course the most

00:29:51,160 --> 00:29:57,940
frequent language and angie's also see

00:29:53,950 --> 00:30:02,590
the M the counts of the languages so

00:29:57,940 --> 00:30:04,870
this is produced by this part of the of

00:30:02,590 --> 00:30:06,700
the streaming flow so we've looked at

00:30:04,870 --> 00:30:08,830
this one so here's the cough casting

00:30:06,700 --> 00:30:11,800
which is writing the top end to Kafka

00:30:08,830 --> 00:30:13,360
this is the aggregation to elasticsearch

00:30:11,800 --> 00:30:14,950
so there we have this 10-second window

00:30:13,360 --> 00:30:16,540
which is counting the frequency of the

00:30:14,950 --> 00:30:18,430
languages and writing it to

00:30:16,540 --> 00:30:20,740
elasticsearch and this is the rolling

00:30:18,430 --> 00:30:26,890
file sync so we can also take a look

00:30:20,740 --> 00:30:30,240
into em The Rolling fight sing and so

00:30:26,890 --> 00:30:30,240
it's located

00:30:34,299 --> 00:30:40,850
so here the rolling sink and here you

00:30:37,880 --> 00:30:45,019
see that for every minute we are

00:30:40,850 --> 00:30:48,529
creating a new directory em I can go

00:30:45,019 --> 00:30:53,120
into the 28th for example and then

00:30:48,529 --> 00:30:56,269
there's a file it's three megabytes and

00:30:53,120 --> 00:31:00,260
there you see all the raw this is where

00:30:56,269 --> 00:31:02,990
the signs are coming from yeah so here

00:31:00,260 --> 00:31:06,649
you see the raw and tweets written to a

00:31:02,990 --> 00:31:12,019
file and so this way you can see that

00:31:06,649 --> 00:31:13,850
there are different data sings where

00:31:12,019 --> 00:31:18,980
this one source is writing different

00:31:13,850 --> 00:31:24,380
kinds of data to different ends all

00:31:18,980 --> 00:31:27,289
right so let me close with the talk so

00:31:24,380 --> 00:31:30,380
if you like this stuff if you would like

00:31:27,289 --> 00:31:32,000
to do more stuff like this I've had a

00:31:30,380 --> 00:31:33,950
virtual machine running at Google Cloud

00:31:32,000 --> 00:31:36,559
for the last seven days where I was

00:31:33,950 --> 00:31:38,710
collecting Twitter data so I've like 100

00:31:36,559 --> 00:31:41,360
gigabytes of Twitter stream data and

00:31:38,710 --> 00:31:45,620
tomorrow we will organize a flink

00:31:41,360 --> 00:31:49,159
hackathon and which is in a to telecom

00:31:45,620 --> 00:31:51,590
building at bitten bet yeah it's in

00:31:49,159 --> 00:31:53,960
closer quads back and not it fit in my

00:31:51,590 --> 00:31:56,059
Bloods and so you can still sign up I've

00:31:53,960 --> 00:31:57,889
asked there are 10 spots left so if

00:31:56,059 --> 00:32:00,679
you're interested in working with

00:31:57,889 --> 00:32:03,500
fleeing and you can do a self-paced and

00:32:00,679 --> 00:32:05,000
workshop you can work with the data that

00:32:03,500 --> 00:32:06,620
I collected we have also other data

00:32:05,000 --> 00:32:09,320
sources all you can do your own stuff

00:32:06,620 --> 00:32:11,990
and some of the fling computers will be

00:32:09,320 --> 00:32:14,240
there to help you so this is a great

00:32:11,990 --> 00:32:17,179
opportunity to to learn more about flink

00:32:14,240 --> 00:32:18,679
it's also free and if you like this

00:32:17,179 --> 00:32:22,340
location and if you would like to learn

00:32:18,679 --> 00:32:29,779
more about flink this fling forward in

00:32:22,340 --> 00:32:31,399
September in Berlin and the M sub you

00:32:29,779 --> 00:32:32,960
can still submit talks for the

00:32:31,399 --> 00:32:36,649
conference and you can still get tickets

00:32:32,960 --> 00:32:38,149
for the early bird rate so this is going

00:32:36,649 --> 00:32:40,610
to be a great conference last year was

00:32:38,149 --> 00:32:42,169
also very good and yeah my employer is

00:32:40,610 --> 00:32:44,990
currently hiring so if you're interested

00:32:42,169 --> 00:32:46,330
in working with open source projects

00:32:44,990 --> 00:32:49,120
then please

00:32:46,330 --> 00:32:51,880
talk to me and now's the time for you to

00:32:49,120 --> 00:32:53,559
ask questions and you can also send me

00:32:51,880 --> 00:32:55,419
an email follow me on twitter follow the

00:32:53,559 --> 00:32:58,029
patchy fling project on twitter we have

00:32:55,419 --> 00:32:59,799
the mailing lists known from other

00:32:58,029 --> 00:33:02,080
apache projects and we have two very

00:32:59,799 --> 00:33:05,100
good blocks where you can read about the

00:33:02,080 --> 00:33:26,250
latest features and developments of link

00:33:05,100 --> 00:33:26,250
so are there any questions um yes

00:33:32,800 --> 00:33:41,230
I can also repeat the question okay you

00:33:39,580 --> 00:33:43,810
can also say the question and repeat

00:33:41,230 --> 00:33:53,290
itself so I'm kind of new in the

00:33:43,810 --> 00:33:55,180
streaming world so so the top right

00:33:53,290 --> 00:33:58,360
corner would you like to the aggregates

00:33:55,180 --> 00:34:01,030
in one minute windows mm-hmm how do you

00:33:58,360 --> 00:34:04,480
tend that console it all that on the

00:34:01,030 --> 00:34:07,810
larger scale what's the way here mmhmm

00:34:04,480 --> 00:34:10,360
yeah yeah and the question is how do you

00:34:07,810 --> 00:34:20,920
do windows that are larger than 10

00:34:10,360 --> 00:34:23,290
seconds or one minute um and so so let

00:34:20,920 --> 00:34:25,630
me let me first answer like one part of

00:34:23,290 --> 00:34:28,480
the questions or fling supports so the

00:34:25,630 --> 00:34:30,610
windows are using this internal State

00:34:28,480 --> 00:34:34,179
back ends of fling so they allow you to

00:34:30,610 --> 00:34:37,480
store state and on the local file system

00:34:34,179 --> 00:34:41,169
in in rocks TP so we're using roxy be as

00:34:37,480 --> 00:34:43,660
a state back-end and which is like a key

00:34:41,169 --> 00:34:45,910
value store on the hard disk and this

00:34:43,660 --> 00:34:48,550
way we we are not limited to the the

00:34:45,910 --> 00:34:50,080
heap size we can use basically infinite

00:34:48,550 --> 00:34:52,240
amount or the amount of hard disk space

00:34:50,080 --> 00:34:55,169
available for keeping window contents

00:34:52,240 --> 00:34:57,340
and around so you can build windows

00:34:55,169 --> 00:34:58,920
multi-month or whatever depending on the

00:34:57,340 --> 00:35:01,870
amount of data that you're having and

00:34:58,920 --> 00:35:04,660
then and there are different ways of

00:35:01,870 --> 00:35:06,190
getting data from the window so this you

00:35:04,660 --> 00:35:08,650
can basically get an iterator over all

00:35:06,190 --> 00:35:11,350
the events in the window and then you

00:35:08,650 --> 00:35:13,150
can do whatever analysis you want and

00:35:11,350 --> 00:35:15,190
they are also aggregated windows where

00:35:13,150 --> 00:35:18,220
you for example specify that you want to

00:35:15,190 --> 00:35:19,810
do em just account and in this case

00:35:18,220 --> 00:35:21,910
you're not keeping all the data we are

00:35:19,810 --> 00:35:24,550
just keeping the counts for each key and

00:35:21,910 --> 00:35:26,170
this way the the space requirements are

00:35:24,550 --> 00:35:28,960
much lower because you only to store one

00:35:26,170 --> 00:35:31,780
there you perky and not all the data for

00:35:28,960 --> 00:35:33,280
multiple month yeah but what you're

00:35:31,780 --> 00:35:35,470
actually doing with the data and window

00:35:33,280 --> 00:35:36,790
depends on you so you can specify the

00:35:35,470 --> 00:35:39,310
size of your window you can also specify

00:35:36,790 --> 00:35:42,010
windows on number of events you can also

00:35:39,310 --> 00:35:43,780
say just the last 1000 events or two

00:35:42,010 --> 00:35:45,100
million events and then you get an

00:35:43,780 --> 00:35:46,540
iterator over the events and then you

00:35:45,100 --> 00:35:48,580
can do your analysis and

00:35:46,540 --> 00:35:52,230
omit the outcome of the window so this

00:35:48,580 --> 00:35:52,230
is just an outcome of one window

00:35:54,330 --> 00:36:00,040
consider the following use case where

00:35:56,980 --> 00:36:02,680
you who receive a file on HDFS every

00:36:00,040 --> 00:36:05,110
five seconds and then say one gigabyte

00:36:02,680 --> 00:36:07,480
file every five seconds and then you

00:36:05,110 --> 00:36:10,080
need to make some transformation some

00:36:07,480 --> 00:36:13,090
aggregations joins with data from hive

00:36:10,080 --> 00:36:15,880
and some transformation on that data the

00:36:13,090 --> 00:36:18,670
incoming data and the output is elastic

00:36:15,880 --> 00:36:21,940
search or hive mm-hmm what would be the

00:36:18,670 --> 00:36:25,900
advantage is to use fleeing over a spark

00:36:21,940 --> 00:36:29,560
streaming for example and so in flink we

00:36:25,900 --> 00:36:31,660
will soon have a new em like streaming

00:36:29,560 --> 00:36:33,160
five source and flink 1.1 so their

00:36:31,660 --> 00:36:34,840
current is also file sauce but the new

00:36:33,160 --> 00:36:37,180
one will also be integrated with the

00:36:34,840 --> 00:36:39,370
check pointing and so you can just tell

00:36:37,180 --> 00:36:41,170
Frank look this is directory watch it

00:36:39,370 --> 00:36:43,180
and every time something you appears in

00:36:41,170 --> 00:36:46,090
this directory it will be ingested

00:36:43,180 --> 00:36:47,850
immediately and introduced streaming

00:36:46,090 --> 00:36:50,800
engine and then you can submit it to

00:36:47,850 --> 00:36:53,200
elasticsearch to aggregations right and

00:36:50,800 --> 00:36:55,450
so on basically what I presented and so

00:36:53,200 --> 00:36:57,430
there is support for watching of

00:36:55,450 --> 00:37:01,330
directory and collecting events from the

00:36:57,430 --> 00:37:03,700
directory and and yet the difference

00:37:01,330 --> 00:37:05,020
between flink and spark streaming is

00:37:03,700 --> 00:37:07,990
basically what I had also my

00:37:05,020 --> 00:37:10,360
presentation that flink is like a real

00:37:07,990 --> 00:37:13,630
streaming engine so I am supports low

00:37:10,360 --> 00:37:17,020
latency and it has support for event

00:37:13,630 --> 00:37:19,030
time and it has these more sophisticated

00:37:17,020 --> 00:37:23,140
window constructs you can also build

00:37:19,030 --> 00:37:24,760
custom windows with it the thing about

00:37:23,140 --> 00:37:27,160
this microbe etching approach is that

00:37:24,760 --> 00:37:28,750
you always have latency your ass basic

00:37:27,160 --> 00:37:31,930
guarantee that you have latency because

00:37:28,750 --> 00:37:34,330
of this scheduling time so you redeploy

00:37:31,930 --> 00:37:36,220
a bad shop every five seconds or so so

00:37:34,330 --> 00:37:39,910
your latency is guaranteed to be at

00:37:36,220 --> 00:37:40,750
least five seconds and also for example

00:37:39,910 --> 00:37:43,360
if you have a connection to

00:37:40,750 --> 00:37:45,070
elasticsearch or some JDBC or so you

00:37:43,360 --> 00:37:46,840
need to reestablish this connection

00:37:45,070 --> 00:37:48,900
every five seconds because every five

00:37:46,840 --> 00:37:52,240
seconds you are redeploying another job

00:37:48,900 --> 00:37:53,710
and also it doesn't feel natural to as

00:37:52,240 --> 00:37:56,260
an application developer you always have

00:37:53,710 --> 00:37:58,090
to be aware that my job is really broad

00:37:56,260 --> 00:37:59,380
every five seconds I have to

00:37:58,090 --> 00:38:00,160
re-establish a connection to a lessee

00:37:59,380 --> 00:38:02,920
search every five

00:38:00,160 --> 00:38:04,869
seconds in flink it just feels much more

00:38:02,920 --> 00:38:08,140
natural to have this operator and you

00:38:04,869 --> 00:38:10,059
know it's running all the time so it's

00:38:08,140 --> 00:38:18,160
just a more natural way of processing

00:38:10,059 --> 00:38:20,049
data there was another question can you

00:38:18,160 --> 00:38:25,630
explain more how you deal with out of

00:38:20,049 --> 00:38:30,460
order items and yeah em so how do we

00:38:25,630 --> 00:38:32,740
deal with out of other events and so

00:38:30,460 --> 00:38:36,400
basically what we are doing is in flink

00:38:32,740 --> 00:38:44,530
that is um so in this in this topology

00:38:36,400 --> 00:38:46,839
here damn okay now scroll so here there

00:38:44,530 --> 00:38:49,299
is an operator called timestamp

00:38:46,839 --> 00:38:53,740
extractor so here we are just getting

00:38:49,299 --> 00:38:56,490
some Jason from Kafka and here there is

00:38:53,740 --> 00:38:59,319
a timestamp extractor which is and

00:38:56,490 --> 00:39:03,280
extracting along from the chase and data

00:38:59,319 --> 00:39:06,520
and this long is the time of the event

00:39:03,280 --> 00:39:08,559
in reality so it's not the time when the

00:39:06,520 --> 00:39:10,240
event arrived in the system because that

00:39:08,559 --> 00:39:13,930
can be out of order to to various

00:39:10,240 --> 00:39:16,029
reasons and and we are using this long

00:39:13,930 --> 00:39:19,089
field and putting it through the record

00:39:16,029 --> 00:39:21,730
so the system has em to each record

00:39:19,089 --> 00:39:23,970
there's a special internal field with a

00:39:21,730 --> 00:39:27,849
long representing the time of the event

00:39:23,970 --> 00:39:31,049
okay so this way operators are aware of

00:39:27,849 --> 00:39:34,720
that time so for example this tumbling m

00:39:31,049 --> 00:39:38,170
/ language count window which is doing a

00:39:34,720 --> 00:39:42,400
window / 10 seconds and it's using this

00:39:38,170 --> 00:39:44,589
time stamp field for determining the the

00:39:42,400 --> 00:39:46,420
10 seconds so it just looks at this

00:39:44,589 --> 00:39:48,520
field and then puts it to the right

00:39:46,420 --> 00:39:52,569
window so it can happen that we have

00:39:48,520 --> 00:39:56,440
multiple windows open because i don't

00:39:52,569 --> 00:39:57,940
know maybe four so because events are

00:39:56,440 --> 00:40:00,299
arriving out of order we have to

00:39:57,940 --> 00:40:03,220
basically start different windows it

00:40:00,299 --> 00:40:04,869
parallel and as soon as a watermark

00:40:03,220 --> 00:40:06,430
arrives so from time to time this

00:40:04,869 --> 00:40:08,410
timestamp x factor is sending a

00:40:06,430 --> 00:40:10,660
watermark and it's what I'm access I

00:40:08,410 --> 00:40:12,700
guarantee that there will be no later

00:40:10,660 --> 00:40:14,680
event after this watermark

00:40:12,700 --> 00:40:18,070
and once the watermark arrives the

00:40:14,680 --> 00:40:19,869
window it knows okay I've received all

00:40:18,070 --> 00:40:21,339
the events that belong to this time

00:40:19,869 --> 00:40:23,260
window and then I'm triggering the

00:40:21,339 --> 00:40:25,650
window and I'm doing the evaluation and

00:40:23,260 --> 00:40:28,089
sending the results are to elasticsearch

00:40:25,650 --> 00:40:35,320
so we're using water marks and this

00:40:28,089 --> 00:40:37,300
event time field for the time you're how

00:40:35,320 --> 00:40:39,400
do you configure the water marks yeah so

00:40:37,300 --> 00:40:43,510
that's up to the user so the thing is

00:40:39,400 --> 00:40:47,109
the water max and it really depends on

00:40:43,510 --> 00:40:49,510
your use case and so what fling provides

00:40:47,109 --> 00:40:51,670
by default is for example watermark

00:40:49,510 --> 00:40:55,359
extractor which is chest looking at the

00:40:51,670 --> 00:40:57,730
highest M times M seen so far and then

00:40:55,359 --> 00:41:00,160
subtracting a certain amount of time so

00:40:57,730 --> 00:41:03,599
basically this says I'm accepting

00:41:00,160 --> 00:41:06,310
lateness of the events up to one minute

00:41:03,599 --> 00:41:09,820
so then you have to basically wait one

00:41:06,310 --> 00:41:11,440
minute until you get the results but it

00:41:09,820 --> 00:41:13,839
can still happen that events arrive

00:41:11,440 --> 00:41:16,869
after this 10a after this one minute and

00:41:13,839 --> 00:41:19,089
then you have to do some custom handling

00:41:16,869 --> 00:41:21,400
m of these events but it's just reality

00:41:19,089 --> 00:41:23,710
I mean the three processor cannot change

00:41:21,400 --> 00:41:25,599
reality events arrived out of order in

00:41:23,710 --> 00:41:29,040
the system we're just providing the

00:41:25,599 --> 00:41:32,920
tools for the user to to cope with that

00:41:29,040 --> 00:41:34,630
yeah but there's very nice documentation

00:41:32,920 --> 00:41:36,369
fling with nice pictures and so on

00:41:34,630 --> 00:41:37,930
showing how this exactly works with the

00:41:36,369 --> 00:41:39,940
different operators in the low water max

00:41:37,930 --> 00:41:41,349
how to propagate and so on because you

00:41:39,940 --> 00:41:45,430
always need to use the lowest water mac

00:41:41,349 --> 00:41:48,339
form all incoming streams also connect

00:41:45,430 --> 00:41:51,670
to this question are when you talk about

00:41:48,339 --> 00:41:53,589
the event time are you you shown this

00:41:51,670 --> 00:41:55,930
example before when you're when the

00:41:53,589 --> 00:41:58,180
clocks on your client devices are not in

00:41:55,930 --> 00:42:01,210
sync mm-hmm and you assume this is the

00:41:58,180 --> 00:42:03,430
event time that also means you might not

00:42:01,210 --> 00:42:05,710
get the real order because the field you

00:42:03,430 --> 00:42:08,589
described is sent by the client who's

00:42:05,710 --> 00:42:11,109
yeah there might be not only a network

00:42:08,589 --> 00:42:13,300
delay but clocks out of sync yeah so you

00:42:11,109 --> 00:42:16,540
only providing infrastructure for a

00:42:13,300 --> 00:42:19,060
problem that exists and not a new

00:42:16,540 --> 00:42:20,530
solution to the problem inside yeah the

00:42:19,060 --> 00:42:23,170
thing is the advantage is that the

00:42:20,530 --> 00:42:25,670
system is aware of event time so in

00:42:23,170 --> 00:42:28,760
storm and also in this tradition

00:42:25,670 --> 00:42:31,700
betch approaches so if you're back then

00:42:28,760 --> 00:42:33,500
like my talk like how etl with a badge

00:42:31,700 --> 00:42:37,490
system there there's no infrastructure

00:42:33,500 --> 00:42:39,530
at all for treating this time issue you

00:42:37,490 --> 00:42:42,079
solution to the problem itself it's just

00:42:39,530 --> 00:42:44,000
you don't have to implement exactly we

00:42:42,079 --> 00:42:46,460
just provide infrastructure for the user

00:42:44,000 --> 00:42:48,170
to handle this problem and it's quite

00:42:46,460 --> 00:42:52,369
easy because we have standard tools for

00:42:48,170 --> 00:42:54,230
this our windowing our windows am aware

00:42:52,369 --> 00:42:56,869
of this yeah but you are right I mean

00:42:54,230 --> 00:43:03,579
it's not a magic solution for this

00:42:56,869 --> 00:43:03,579

YouTube URL: https://www.youtube.com/watch?v=4sV7-37uRFU


