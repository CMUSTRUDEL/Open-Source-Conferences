Title: Berlin Buzzwords 2016: Volker Janz - Real Time Marketing with Kafka, Storm, Cassandra & Spark #bbuzz
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	The combination of Apache Kafka as a event bus, Apache Storm for real- or neartime processing, Apache Cassandra as an operational storage layer as well as Apache Spark to perform analytical queries against this storage turned out to be a extremely well performing system.

With increasing marketing costs per registration, it is even more important to keep players within the game as well as provide them with attractive offers aiming to increase the customer lifetime value and also create a better game experience.

To that end, we introduced interstitials that offer premium features or discounts for the player at InnoGames. Even though this is already a useful instrument, we aimed to customize those interstitials according to the behavior of the player. Therefore, we created a system that works with generic messages that contain data about user interactions, in real- or neartime -- later referred to as events. The system builds up a player profile that contains all game-relevant information about the players in a central location.

Read more:
https://2016.berlinbuzzwords.de/session/real-time-marketing-kafka-storm-cassandra-and-pinch-spark

About Volker Janz:
https://2016.berlinbuzzwords.de/users/volker-janz

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:03,710 --> 00:00:09,929
okay good thank you everybody for coming

00:00:08,160 --> 00:00:11,880
actually like you can see in the title

00:00:09,929 --> 00:00:14,730
there a lot of passwords a lot of

00:00:11,880 --> 00:00:17,400
technologies so my presentation is quite

00:00:14,730 --> 00:00:20,369
packed so I have to be fast to finish in

00:00:17,400 --> 00:00:22,230
time so I jump directly into throt

00:00:20,369 --> 00:00:24,090
introduction so my name is Fargo and i'm

00:00:22,230 --> 00:00:26,220
a software developer team lead software

00:00:24,090 --> 00:00:28,890
developer actually foreigner games and

00:00:26,220 --> 00:00:30,960
you can find me on twitter linkedin sing

00:00:28,890 --> 00:00:32,850
whatever you like and actually I'm

00:00:30,960 --> 00:00:33,899
leading a team of data engineers and we

00:00:32,850 --> 00:00:35,420
are dealing with the whole data

00:00:33,899 --> 00:00:37,950
infrastructure there no games and

00:00:35,420 --> 00:00:40,050
shortly about inner games so we are

00:00:37,950 --> 00:00:42,690
online gaming company and we have more

00:00:40,050 --> 00:00:45,090
than 160 million players all around the

00:00:42,690 --> 00:00:47,370
world and more than 400 people working

00:00:45,090 --> 00:00:50,820
for no games from 30 different countries

00:00:47,370 --> 00:00:53,010
and the main office is in Hamburg but we

00:00:50,820 --> 00:00:54,660
also have the office in Dusseldorf well

00:00:53,010 --> 00:00:56,850
and the first thing I want to do is I

00:00:54,660 --> 00:00:58,250
want to tell you a little secret so the

00:00:56,850 --> 00:01:00,750
secret about how to create a successful

00:00:58,250 --> 00:01:02,579
company with free to play online games

00:01:00,750 --> 00:01:04,619
so inner games is a successful company

00:01:02,579 --> 00:01:07,200
in that direction so i want to tell you

00:01:04,619 --> 00:01:08,939
the whole secret about it and it's

00:01:07,200 --> 00:01:11,070
fairly simple so first of all you

00:01:08,939 --> 00:01:13,409
develop an awesome game then you get

00:01:11,070 --> 00:01:15,840
some users then you sell virtual goods

00:01:13,409 --> 00:01:17,610
and then you have profit and that's

00:01:15,840 --> 00:01:20,670
basically it so that's a whole secret

00:01:17,610 --> 00:01:22,409
about well of course it's not that easy

00:01:20,670 --> 00:01:24,990
to do actually develop an awesome game

00:01:22,409 --> 00:01:27,420
is quite hard to do and then you have to

00:01:24,990 --> 00:01:29,939
get users somehow and also to sell

00:01:27,420 --> 00:01:31,619
virtual goods is not that easy so what

00:01:29,939 --> 00:01:33,689
you have to do for that is you have to

00:01:31,619 --> 00:01:36,060
do kind of marketing and also kind of

00:01:33,689 --> 00:01:39,540
CRM so customer relationship management

00:01:36,060 --> 00:01:41,520
and my talk will focus on a use case in

00:01:39,540 --> 00:01:42,990
that direction so it's about CRM and

00:01:41,520 --> 00:01:45,450
especially it's about those

00:01:42,990 --> 00:01:47,460
interstitials and i will show you such

00:01:45,450 --> 00:01:49,290
an interstitial on this slide so in

00:01:47,460 --> 00:01:52,229
interstitial Ferno games is basically

00:01:49,290 --> 00:01:54,090
nothing but in game pop-up message so

00:01:52,229 --> 00:01:56,610
imagine you play game and then this

00:01:54,090 --> 00:01:59,189
pop-up appears and you can get ten

00:01:56,610 --> 00:02:01,860
percent bonus if you buy crowns in this

00:01:59,189 --> 00:02:03,630
example and this is basically something

00:02:01,860 --> 00:02:05,430
that we're doing batch why's that means

00:02:03,630 --> 00:02:08,009
every night we are selecting a bunch of

00:02:05,430 --> 00:02:09,840
users based on some yeah some

00:02:08,009 --> 00:02:13,470
requirements and then we are sending out

00:02:09,840 --> 00:02:15,420
those campaigns basically now the whole

00:02:13,470 --> 00:02:17,310
idea that we have now for this project i

00:02:15,420 --> 00:02:19,950
want to show you today i want to tell

00:02:17,310 --> 00:02:22,290
you a little story for it so this is Bob

00:02:19,950 --> 00:02:25,110
and Bob plays forge of Empires obviously

00:02:22,290 --> 00:02:26,610
that's the game that in games has

00:02:25,110 --> 00:02:29,280
implemented so if you like to give it a

00:02:26,610 --> 00:02:31,080
try just feel free well if the talk is

00:02:29,280 --> 00:02:34,080
boring or something please play forge of

00:02:31,080 --> 00:02:35,879
Empires at least okay so Bob plays for

00:02:34,080 --> 00:02:37,739
two vampires and now he gets such an

00:02:35,879 --> 00:02:39,750
interstitial message but he doesn't know

00:02:37,739 --> 00:02:41,370
what to do with it so it's not very

00:02:39,750 --> 00:02:44,519
attractive for him it's just getting in

00:02:41,370 --> 00:02:45,810
ten percent free diamonds now and yeah

00:02:44,519 --> 00:02:47,670
he doesn't know what to do was it so he

00:02:45,810 --> 00:02:49,230
clicks it away and then he continues

00:02:47,670 --> 00:02:51,930
playing the game and then all of a

00:02:49,230 --> 00:02:53,849
sudden he loses three battles in a row

00:02:51,930 --> 00:02:56,010
so he is stuck in the game he doesn't

00:02:53,849 --> 00:02:59,340
know what to do and he's very frustrated

00:02:56,010 --> 00:03:02,340
now after a few seconds later he now

00:02:59,340 --> 00:03:04,110
gets this interstitial and it's an

00:03:02,340 --> 00:03:06,360
interstitial that is tailored for him so

00:03:04,110 --> 00:03:08,280
that means it's exactly what he needs in

00:03:06,360 --> 00:03:10,170
this moment he needs for battle tanks to

00:03:08,280 --> 00:03:11,790
win this battle obviously so that's

00:03:10,170 --> 00:03:14,400
what's missing in his army composition

00:03:11,790 --> 00:03:17,129
basically and yeah he gets this

00:03:14,400 --> 00:03:20,340
interstitial he is very happy he accepts

00:03:17,129 --> 00:03:22,560
the offer and yeah he's happy we have a

00:03:20,340 --> 00:03:24,390
new paying user basically and we

00:03:22,560 --> 00:03:26,220
prevented a term so that means he

00:03:24,390 --> 00:03:28,680
continues to play our game and he's

00:03:26,220 --> 00:03:30,659
happy was it and this is the motivation

00:03:28,680 --> 00:03:32,549
behind the project real-time cm and

00:03:30,659 --> 00:03:34,980
that's what i want to show you today and

00:03:32,549 --> 00:03:37,049
basically what it is it's individualized

00:03:34,980 --> 00:03:39,780
marketing based on user behavior and

00:03:37,049 --> 00:03:42,150
real-time and what do you need for that

00:03:39,780 --> 00:03:45,450
so first of all you need data of course

00:03:42,150 --> 00:03:47,510
and the dinner games we are mainly

00:03:45,450 --> 00:03:50,010
tracking so-called events which

00:03:47,510 --> 00:03:52,290
basically show the user interaction that

00:03:50,010 --> 00:03:54,450
means when you play our game you

00:03:52,290 --> 00:03:57,000
interact with the game and we record

00:03:54,450 --> 00:03:58,680
those interactions as events so for

00:03:57,000 --> 00:04:00,900
example this is forge of Empires again

00:03:58,680 --> 00:04:02,819
and if you know for example build a

00:04:00,900 --> 00:04:05,909
building then you get a build event if

00:04:02,819 --> 00:04:08,220
you yeah do a fight then you get a fight

00:04:05,909 --> 00:04:11,189
event and so on and so forth so it's

00:04:08,220 --> 00:04:13,140
fairly simple but you can imagine it's

00:04:11,189 --> 00:04:15,870
quite a lot of data that you can

00:04:13,140 --> 00:04:17,790
generate that way and actually talking

00:04:15,870 --> 00:04:20,250
about passwords and numbers I brought

00:04:17,790 --> 00:04:22,530
you a number that's big number it's 500

00:04:20,250 --> 00:04:24,900
million and this is actually the number

00:04:22,530 --> 00:04:26,730
of events that we collect per day so

00:04:24,900 --> 00:04:28,680
that's the more the data that we

00:04:26,730 --> 00:04:30,390
have actually that's of course compared

00:04:28,680 --> 00:04:32,220
to some other bigger companies it's not

00:04:30,390 --> 00:04:33,810
that much but I think for our business

00:04:32,220 --> 00:04:37,140
and our company size it's already quite

00:04:33,810 --> 00:04:39,570
quite a bit so that's what we have to

00:04:37,140 --> 00:04:41,940
deal with basically and to collect this

00:04:39,570 --> 00:04:43,770
data and to process the data we build up

00:04:41,940 --> 00:04:45,900
kind of a data infrastructure at dinner

00:04:43,770 --> 00:04:50,280
games so that's what my team is doing

00:04:45,900 --> 00:04:52,230
basically and it looks like this so this

00:04:50,280 --> 00:04:54,530
is basically the data processing

00:04:52,230 --> 00:04:58,500
infrastructure that inner games has and

00:04:54,530 --> 00:05:00,240
well it's fairly simple but imagine this

00:04:58,500 --> 00:05:02,760
is all in a distributed world so it's

00:05:00,240 --> 00:05:04,560
very simplified that picture but let me

00:05:02,760 --> 00:05:06,840
guide you use route from the left to the

00:05:04,560 --> 00:05:08,100
right basically so on the left side as

00:05:06,840 --> 00:05:10,770
you can see we have games and other

00:05:08,100 --> 00:05:12,630
systems they are sending events to a

00:05:10,770 --> 00:05:15,060
gateway which is the entry point for the

00:05:12,630 --> 00:05:17,130
whole system and then it continues to

00:05:15,060 --> 00:05:18,720
push those messages into a message bus

00:05:17,130 --> 00:05:21,090
and that's where the real time

00:05:18,720 --> 00:05:23,060
processing happens for the return

00:05:21,090 --> 00:05:26,010
processing we offer also have kind of a

00:05:23,060 --> 00:05:27,840
persistence layer that is optimized for

00:05:26,010 --> 00:05:30,210
write and read performance so this is

00:05:27,840 --> 00:05:32,190
all production storage and then in the

00:05:30,210 --> 00:05:34,170
end we have a loop of course for

00:05:32,190 --> 00:05:36,030
distributed storage and distributed

00:05:34,170 --> 00:05:38,580
processing and we are mainly doing batch

00:05:36,030 --> 00:05:40,410
processing in that area in the end you

00:05:38,580 --> 00:05:42,120
can see that we have different systems

00:05:40,410 --> 00:05:43,620
that are connected to our Hadoop cluster

00:05:42,120 --> 00:05:45,600
so we have a business intelligence

00:05:43,620 --> 00:05:47,850
system based on SQL server that is

00:05:45,600 --> 00:05:50,520
fetching the data from Hadoop we also

00:05:47,850 --> 00:05:51,750
have a great analytics Department so

00:05:50,520 --> 00:05:54,060
with data scientists and they are

00:05:51,750 --> 00:05:55,710
working with the data day by day and of

00:05:54,060 --> 00:05:58,310
course we have other systems such as

00:05:55,710 --> 00:06:01,170
power bi for example water blue and

00:05:58,310 --> 00:06:04,710
talking about timings as you can see on

00:06:01,170 --> 00:06:06,660
the top as soon as an event gets

00:06:04,710 --> 00:06:08,970
generated in the game it takes about

00:06:06,660 --> 00:06:11,130
three to five seconds until it arrives

00:06:08,970 --> 00:06:13,440
at the gateway this is because we have a

00:06:11,130 --> 00:06:15,420
special process that is that decouples

00:06:13,440 --> 00:06:17,610
the sending of the event from the game

00:06:15,420 --> 00:06:19,710
itself so it makes it very safe if the

00:06:17,610 --> 00:06:22,440
whole system explodes the game will

00:06:19,710 --> 00:06:24,180
still work result of any problems so

00:06:22,440 --> 00:06:26,580
we're talking about seconds in that area

00:06:24,180 --> 00:06:28,410
after it arrives at the gateway we are

00:06:26,580 --> 00:06:30,510
talking about milliseconds so after that

00:06:28,410 --> 00:06:32,670
we can process all those events within

00:06:30,510 --> 00:06:34,140
milliseconds and then of course on the

00:06:32,670 --> 00:06:36,480
batch side we are talking about hours

00:06:34,140 --> 00:06:38,849
because we are processing the etsy less

00:06:36,480 --> 00:06:41,999
our the last month's whatever

00:06:38,849 --> 00:06:43,649
so that's basically the whole yeah the

00:06:41,999 --> 00:06:47,580
whole data infrastructure that we have

00:06:43,649 --> 00:06:49,080
and when we talk about the left side we

00:06:47,580 --> 00:06:50,669
talk about the data pipeline at little

00:06:49,080 --> 00:06:54,029
games and the right side for us is

00:06:50,669 --> 00:06:55,830
basically data platform and since we're

00:06:54,029 --> 00:06:59,580
on the billion passwords I have brought

00:06:55,830 --> 00:07:01,559
you some passwords basically those are

00:06:59,580 --> 00:07:04,199
the technologies that we use for it so

00:07:01,559 --> 00:07:05,939
on the client side we have a client

00:07:04,199 --> 00:07:09,149
implemented with the gold language oh

00:07:05,939 --> 00:07:11,189
it's quite cool and quite handy and for

00:07:09,149 --> 00:07:14,249
the Gateway we used Java basically a

00:07:11,189 --> 00:07:16,919
wizard with drop wizard framework which

00:07:14,249 --> 00:07:19,409
is a framework to implement rest api so

00:07:16,919 --> 00:07:22,529
it's very simple then of course we use

00:07:19,409 --> 00:07:24,330
Kafka like everyone and then for the

00:07:22,529 --> 00:07:26,729
near real-time processing we're using

00:07:24,330 --> 00:07:28,709
apache storm patchy Cassandra for the

00:07:26,729 --> 00:07:30,869
production storage and on the Hadoop

00:07:28,709 --> 00:07:32,309
side we are basically using most of the

00:07:30,869 --> 00:07:34,830
stuff that yeah the Hadoop

00:07:32,309 --> 00:07:36,689
infrastructure officers likes hive and

00:07:34,830 --> 00:07:38,789
spark and different other technologies

00:07:36,689 --> 00:07:41,099
so that's basically how it looks like

00:07:38,789 --> 00:07:42,869
and when we are now talking about this

00:07:41,099 --> 00:07:44,610
real-time cm project that I showed you

00:07:42,869 --> 00:07:46,979
in the beginning it's happening in the

00:07:44,610 --> 00:07:49,709
data pipeline part so I focus on that

00:07:46,979 --> 00:07:51,990
part right now and to give you an idea

00:07:49,709 --> 00:07:55,409
of what happens there it's fairly simple

00:07:51,990 --> 00:07:57,089
basically so if five simple steps that

00:07:55,409 --> 00:07:59,759
are happening within this real-time CRM

00:07:57,089 --> 00:08:02,459
process first of all we have events they

00:07:59,759 --> 00:08:04,099
get sent to the Gateway the Gateway is

00:08:02,459 --> 00:08:07,319
then pushing it to the message bus and

00:08:04,099 --> 00:08:10,800
then the events gets processed event by

00:08:07,319 --> 00:08:12,569
event so as a stream and then two things

00:08:10,800 --> 00:08:14,369
happen basically first of all we update

00:08:12,569 --> 00:08:16,889
a player profile with those events in

00:08:14,369 --> 00:08:19,740
Cassandra that means it's basically some

00:08:16,889 --> 00:08:22,499
kind of storage that can tell you

00:08:19,740 --> 00:08:24,689
everything about a player in real time

00:08:22,499 --> 00:08:27,059
so of course we do not record everything

00:08:24,689 --> 00:08:28,919
so every single event but those events

00:08:27,059 --> 00:08:31,079
that are interesting for us and then

00:08:28,919 --> 00:08:32,550
after that we trigger campaign based on

00:08:31,079 --> 00:08:34,439
the current player profile and the

00:08:32,550 --> 00:08:37,199
incoming event and that's basically it

00:08:34,439 --> 00:08:39,360
so that's the whole magic behind the

00:08:37,199 --> 00:08:40,740
real time see I'm project and the key

00:08:39,360 --> 00:08:42,930
technologies for that are basically

00:08:40,740 --> 00:08:45,899
kafka storm and cassandra in our case

00:08:42,930 --> 00:08:47,819
and i want to focus on how we use those

00:08:45,899 --> 00:08:51,899
technologies for that project right now

00:08:47,819 --> 00:08:52,560
so starting with Kafka I think most of

00:08:51,899 --> 00:08:54,360
you know

00:08:52,560 --> 00:08:56,240
I use it in production so who's using it

00:08:54,360 --> 00:08:58,770
in production just quick heads up so

00:08:56,240 --> 00:09:01,440
okay that looks good so I can make it

00:08:58,770 --> 00:09:04,140
very fast it's very good okay so

00:09:01,440 --> 00:09:06,840
basically you know that in Kafka you

00:09:04,140 --> 00:09:08,850
have brokers which is basically Africa

00:09:06,840 --> 00:09:11,010
class i have several producers and

00:09:08,850 --> 00:09:13,410
several continued consumers and the cool

00:09:11,010 --> 00:09:15,180
thing about it is that you can wyd runs

00:09:13,410 --> 00:09:18,270
but read it many times as often as you

00:09:15,180 --> 00:09:20,820
want and internally you know that as

00:09:18,270 --> 00:09:22,920
well i'm pretty sure you have brokers

00:09:20,820 --> 00:09:24,660
you have petitions so from this example

00:09:22,920 --> 00:09:26,970
we have a topic called locks it has two

00:09:24,660 --> 00:09:29,970
partitions and replication factor of two

00:09:26,970 --> 00:09:31,440
and it's yeah it's spreaded around the

00:09:29,970 --> 00:09:33,690
cluster on those two brokers so that's

00:09:31,440 --> 00:09:35,310
basically the idea behind it but I think

00:09:33,690 --> 00:09:38,250
more interesting is how does it look at

00:09:35,310 --> 00:09:40,770
innoGames so I dinner games we have five

00:09:38,250 --> 00:09:43,440
notes five Kafka notes with eight cores

00:09:40,770 --> 00:09:45,839
64 gig memory and 1.8 terabyte this

00:09:43,440 --> 00:09:47,220
space each and those are basically the

00:09:45,839 --> 00:09:50,730
settings you can see them on the right

00:09:47,220 --> 00:09:52,529
we are using Kafka 08 22 so we have to

00:09:50,730 --> 00:09:54,240
update soon because they're cool new

00:09:52,529 --> 00:09:56,520
features that we want to use actually

00:09:54,240 --> 00:09:58,230
and this is how it basically look

00:09:56,520 --> 00:10:00,930
basically look looks like it in all

00:09:58,230 --> 00:10:02,850
games and talking about the topics that

00:10:00,930 --> 00:10:05,700
are no games we have currently just

00:10:02,850 --> 00:10:07,110
three topics so it's kind of the first

00:10:05,700 --> 00:10:08,430
project that we have added business

00:10:07,110 --> 00:10:11,670
pipeline so that's why we only have

00:10:08,430 --> 00:10:13,830
sweet topics for now but let me talk

00:10:11,670 --> 00:10:15,420
about those topics so I start from the

00:10:13,830 --> 00:10:17,430
bottom and then I go to the top actually

00:10:15,420 --> 00:10:19,470
so I start with the raw topic that's a

00:10:17,430 --> 00:10:22,350
topic where we actually store the events

00:10:19,470 --> 00:10:24,450
so it's our main topic for everything

00:10:22,350 --> 00:10:26,370
then we have a maintenance topic it's

00:10:24,450 --> 00:10:27,570
clear what it is so we install

00:10:26,370 --> 00:10:29,100
maintenance information or the

00:10:27,570 --> 00:10:30,900
components in the pipeline that you've

00:10:29,100 --> 00:10:33,690
seen sent some information to this topic

00:10:30,900 --> 00:10:35,339
and you can just check everything on the

00:10:33,690 --> 00:10:36,959
maintenance topic and then we have the

00:10:35,339 --> 00:10:39,510
serum trigger topic which is quite

00:10:36,959 --> 00:10:42,720
interesting because we actually use

00:10:39,510 --> 00:10:44,490
Kafka as a message bus means if we want

00:10:42,720 --> 00:10:46,890
to communicate that this interstitial

00:10:44,490 --> 00:10:49,110
should be shown to a player we write it

00:10:46,890 --> 00:10:50,640
back to Kafka and then another system is

00:10:49,110 --> 00:10:52,890
reading it from Kafka and actually is

00:10:50,640 --> 00:10:56,400
performing this interstitial metric

00:10:52,890 --> 00:10:59,190
basically and you can see in the bottom

00:10:56,400 --> 00:11:01,350
or raw partition has or while topic has

00:10:59,190 --> 00:11:03,839
16 petitions for application factor of 2

00:11:01,350 --> 00:11:05,640
and it's thought I think seven days of

00:11:03,839 --> 00:11:07,560
dating or four days of data

00:11:05,640 --> 00:11:10,830
anything fails over the weekend we

00:11:07,560 --> 00:11:13,260
should be safe hopefully that's about

00:11:10,830 --> 00:11:15,930
Kafka now it's a bit more interesting

00:11:13,260 --> 00:11:17,310
because we chose storm for the stream

00:11:15,930 --> 00:11:19,530
processing part and actually that's

00:11:17,310 --> 00:11:22,590
where all the logic happens so I think

00:11:19,530 --> 00:11:24,270
many of you know storm already it's it's

00:11:22,590 --> 00:11:27,030
kind of the old school distributed

00:11:24,270 --> 00:11:29,220
real-time computation system and still I

00:11:27,030 --> 00:11:30,810
want to show you the basic features or

00:11:29,220 --> 00:11:34,650
components of storm of the idea behind

00:11:30,810 --> 00:11:36,210
of storm so storm has spots which are

00:11:34,650 --> 00:11:40,080
basically the sources of data streams

00:11:36,210 --> 00:11:41,760
storm has balls within bolts you do the

00:11:40,080 --> 00:11:43,620
actually processing you can also

00:11:41,760 --> 00:11:46,170
generate new data streams out of those

00:11:43,620 --> 00:11:48,270
bolts and in the end you have a topology

00:11:46,170 --> 00:11:50,790
which is just the composition of those

00:11:48,270 --> 00:11:53,880
components so very very simple basically

00:11:50,790 --> 00:11:55,670
and to show to you by an example this

00:11:53,880 --> 00:11:59,040
for example is a temperature alerting

00:11:55,670 --> 00:12:00,780
topology so very simple on the left side

00:11:59,040 --> 00:12:02,220
you see a temperature ball that we'd see

00:12:00,780 --> 00:12:04,680
current temperature former sensor and

00:12:02,220 --> 00:12:07,530
every few milliseconds it's emitting

00:12:04,680 --> 00:12:10,740
those values to the next bolt which is

00:12:07,530 --> 00:12:12,960
the filter board and the filter balls

00:12:10,740 --> 00:12:14,940
actually checking if the temperature

00:12:12,960 --> 00:12:17,250
value is more than a specific threshold

00:12:14,940 --> 00:12:19,590
and if that happens it will send another

00:12:17,250 --> 00:12:21,720
message to the next bolt which is then

00:12:19,590 --> 00:12:25,050
the Warren Bould and it will just I

00:12:21,720 --> 00:12:26,400
don't know write an email or raise an

00:12:25,050 --> 00:12:29,250
alarm or something like that so that's

00:12:26,400 --> 00:12:31,170
really simple example for storm topology

00:12:29,250 --> 00:12:33,390
and if you look at the implementation

00:12:31,170 --> 00:12:35,460
it's also not that communicated so this

00:12:33,390 --> 00:12:37,350
will be the filter bolt basically you

00:12:35,460 --> 00:12:38,790
can see it on the right side that you

00:12:37,350 --> 00:12:40,710
have a prepare message where you can

00:12:38,790 --> 00:12:43,350
prepare your bold you have an execute

00:12:40,710 --> 00:12:46,050
method that gets the actual tuple couple

00:12:43,350 --> 00:12:47,880
so that's where all your logic goes and

00:12:46,050 --> 00:12:50,160
then you have a declare output fields

00:12:47,880 --> 00:12:52,650
message where you can declare what kind

00:12:50,160 --> 00:12:56,370
of output your bald is generating so

00:12:52,650 --> 00:12:58,650
that's basically it so extremely simple

00:12:56,370 --> 00:13:00,690
but also very low level storm also has a

00:12:58,650 --> 00:13:03,150
high level API called trident but that's

00:13:00,690 --> 00:13:05,310
not part of this talk and you can look

00:13:03,150 --> 00:13:07,680
it up if you like but what we did with

00:13:05,310 --> 00:13:11,370
storm is this so this is the CRM

00:13:07,680 --> 00:13:14,700
topology which is the main processor for

00:13:11,370 --> 00:13:16,260
the whole reason MCM stuff so let me go

00:13:14,700 --> 00:13:18,360
through it step by step we have the

00:13:16,260 --> 00:13:19,590
kafka sport that's reading stuff from

00:13:18,360 --> 00:13:22,350
Kafka obviously it's

00:13:19,590 --> 00:13:24,990
meaning the raw topic the events then we

00:13:22,350 --> 00:13:27,630
have a validation bolt because like

00:13:24,990 --> 00:13:30,360
every Big Data project data quality is

00:13:27,630 --> 00:13:32,160
always an issue so we have schemas and

00:13:30,360 --> 00:13:33,900
we are validating our events with those

00:13:32,160 --> 00:13:36,720
schemas but that's a different story

00:13:33,900 --> 00:13:38,850
just what is important is we only want

00:13:36,720 --> 00:13:40,980
valid events of course and those valid

00:13:38,850 --> 00:13:43,950
events get then partitioned by player

00:13:40,980 --> 00:13:45,090
and we do this using player petitioner

00:13:43,950 --> 00:13:48,420
bold and that's a very important

00:13:45,090 --> 00:13:50,580
component because if you if you work in

00:13:48,420 --> 00:13:51,870
a distributed system and you don't think

00:13:50,580 --> 00:13:55,650
about purchase how to partition your

00:13:51,870 --> 00:13:57,170
data you can get really easy very much

00:13:55,650 --> 00:14:00,450
in trouble because in this example

00:13:57,170 --> 00:14:03,390
imagine you have much more instances

00:14:00,450 --> 00:14:05,370
than just one of those CRM bold of this

00:14:03,390 --> 00:14:07,470
young bold and now the question is if

00:14:05,370 --> 00:14:09,660
you have multiple events how get they

00:14:07,470 --> 00:14:11,850
distributed to those instances of the CM

00:14:09,660 --> 00:14:14,610
bold and by default storm is doing this

00:14:11,850 --> 00:14:16,800
randomly but if you know yeah once we

00:14:14,610 --> 00:14:19,680
have the events in a specific order for

00:14:16,800 --> 00:14:20,820
player you can get in trouble that's why

00:14:19,680 --> 00:14:22,320
we have a player petitioner board

00:14:20,820 --> 00:14:24,510
actually very simple but extremely

00:14:22,320 --> 00:14:27,360
important so think about partitioning

00:14:24,510 --> 00:14:28,800
can stream processing and this year in

00:14:27,360 --> 00:14:31,140
bold is something that it will show you

00:14:28,800 --> 00:14:33,300
in a bit so just imagine that's where

00:14:31,140 --> 00:14:35,070
the magic happens and then as I already

00:14:33,300 --> 00:14:37,020
told you before we are writing the

00:14:35,070 --> 00:14:38,460
information that it interstitial has to

00:14:37,020 --> 00:14:40,050
be triggered back to cough guards so

00:14:38,460 --> 00:14:44,670
that another system can read it and

00:14:40,050 --> 00:14:48,990
actually perform this action right okay

00:14:44,670 --> 00:14:56,850
so this is a CRM bolt um let me drink a

00:14:48,990 --> 00:14:58,680
bit of water just okay so this is the

00:14:56,850 --> 00:15:01,260
c-arm bolt this is where all the logic

00:14:58,680 --> 00:15:03,600
happens and this is actually quite cool

00:15:01,260 --> 00:15:05,790
because it's very simple but also very

00:15:03,600 --> 00:15:07,590
powerful it's doing just two things

00:15:05,790 --> 00:15:10,380
basically first of all it gets an event

00:15:07,590 --> 00:15:12,090
of course and then you have every event

00:15:10,380 --> 00:15:13,860
has a type like you've seen in the

00:15:12,090 --> 00:15:16,140
beginning there's a fight event for

00:15:13,860 --> 00:15:17,910
example and for each event type you can

00:15:16,140 --> 00:15:20,040
define handlers update handlers or

00:15:17,910 --> 00:15:21,960
campaign handlers or both if you like an

00:15:20,040 --> 00:15:24,240
update handler is basically just

00:15:21,960 --> 00:15:26,790
updating the player profile so it's

00:15:24,240 --> 00:15:28,890
doing some updates in Cassandra the

00:15:26,790 --> 00:15:31,590
campaign Henda is then actually trigger

00:15:28,890 --> 00:15:34,140
triggering those campaigns by

00:15:31,590 --> 00:15:36,000
using the event that's currently in

00:15:34,140 --> 00:15:38,880
coming and also using the current player

00:15:36,000 --> 00:15:42,020
profile so by those two informations it

00:15:38,880 --> 00:15:44,280
will trigger a campaign event eventually

00:15:42,020 --> 00:15:46,740
and now the interesting part is that

00:15:44,280 --> 00:15:49,560
those handlers are actually defined in

00:15:46,740 --> 00:15:51,570
JavaScript and we are using the NASA on

00:15:49,560 --> 00:15:54,330
JavaScript engine which is part of Java

00:15:51,570 --> 00:15:57,480
8 and it's quite cool and Henley so we

00:15:54,330 --> 00:15:59,700
build a fancy class that we just used to

00:15:57,480 --> 00:16:03,510
call this a JavaScript engine and then

00:15:59,700 --> 00:16:05,850
in the end you have the updater and

00:16:03,510 --> 00:16:08,610
campaign handlers defined in JavaScript

00:16:05,850 --> 00:16:10,860
and that's quite cool so actually what

00:16:08,610 --> 00:16:12,780
storm is doing for us it's applying the

00:16:10,860 --> 00:16:16,680
JavaScript snippets that we have to each

00:16:12,780 --> 00:16:18,150
and every event and those JavaScript

00:16:16,680 --> 00:16:20,820
snippets are basically containing the

00:16:18,150 --> 00:16:23,610
whole logic that we want to apply so if

00:16:20,820 --> 00:16:26,100
you look in the example on the top this

00:16:23,610 --> 00:16:28,290
one is counting the login events so you

00:16:26,100 --> 00:16:30,240
can apply photo I only want to do it for

00:16:28,290 --> 00:16:32,040
login events and then you can actually

00:16:30,240 --> 00:16:35,400
perform some actions with a framework

00:16:32,040 --> 00:16:37,710
object that we provided to this script

00:16:35,400 --> 00:16:39,420
basically so that's how we define the

00:16:37,710 --> 00:16:41,790
logic and the cool thing about it is you

00:16:39,420 --> 00:16:43,410
can actually change the logic at one

00:16:41,790 --> 00:16:46,160
time so you don't have to redeploy the

00:16:43,410 --> 00:16:48,690
topology you just let it run you

00:16:46,160 --> 00:16:50,040
integrate new JavaScript and so you can

00:16:48,690 --> 00:16:52,980
change the behavior of the whole system

00:16:50,040 --> 00:16:54,750
at runtime which is quite cool ok so

00:16:52,980 --> 00:16:57,930
that's basically how we do it in storm

00:16:54,750 --> 00:17:00,930
and talking about storm also wanted to

00:16:57,930 --> 00:17:03,840
talk a bit about alternatives we use

00:17:00,930 --> 00:17:06,150
song because of historical reasons so we

00:17:03,840 --> 00:17:08,670
was the first thing that we tried out it

00:17:06,150 --> 00:17:11,070
worked for us and so we got stuck with

00:17:08,670 --> 00:17:13,500
it but it's fine I mean it has some

00:17:11,070 --> 00:17:16,320
drawbacks but if you look at the latest

00:17:13,500 --> 00:17:20,070
versions they actually improved a lot so

00:17:16,320 --> 00:17:22,230
in November 2015 storm 0-10 was released

00:17:20,070 --> 00:17:24,750
and it had some cool features like for

00:17:22,230 --> 00:17:27,209
example Flux which is a declarative

00:17:24,750 --> 00:17:29,400
language to define topologies so before

00:17:27,209 --> 00:17:31,860
that it was really like you had to write

00:17:29,400 --> 00:17:34,620
a lot of glue code to define a topology

00:17:31,860 --> 00:17:37,170
a lot of Java code that is I don't know

00:17:34,620 --> 00:17:39,060
it's very hard to test and it's it's not

00:17:37,170 --> 00:17:40,590
very developer-friendly actually but

00:17:39,060 --> 00:17:43,200
with looks you have just the Yama

00:17:40,590 --> 00:17:44,970
description of your topology you have

00:17:43,200 --> 00:17:45,420
property files you put it together and

00:17:44,970 --> 00:17:47,100
the

00:17:45,420 --> 00:17:49,500
you can deploy a topology so it's quite

00:17:47,100 --> 00:17:51,480
cool beside that you have rolling

00:17:49,500 --> 00:17:53,070
upgrades you have some security features

00:17:51,480 --> 00:17:55,860
and you have improved logging so that's

00:17:53,070 --> 00:17:59,640
already really really good milestone and

00:17:55,860 --> 00:18:01,430
then this year in April actually when

00:17:59,640 --> 00:18:04,350
the dupe summit was happening in Dublin

00:18:01,430 --> 00:18:06,630
they release version wonder though which

00:18:04,350 --> 00:18:09,030
is quite cool because it has a lot of

00:18:06,630 --> 00:18:12,150
performance improvements so on the

00:18:09,030 --> 00:18:14,460
website it says like 16 times faster at

00:18:12,150 --> 00:18:16,680
least three times faster so of course it

00:18:14,460 --> 00:18:18,060
depends on your use case but they

00:18:16,680 --> 00:18:20,190
actually works on the performance which

00:18:18,060 --> 00:18:23,310
is quite good then you have pacemaker

00:18:20,190 --> 00:18:25,230
which is Zhu keep a replacement for work

00:18:23,310 --> 00:18:27,390
our heartbeats also very cool and useful

00:18:25,230 --> 00:18:29,550
and as you can see on the bottom they

00:18:27,390 --> 00:18:31,410
also thought about how to store state

00:18:29,550 --> 00:18:33,300
for the different components so now you

00:18:31,410 --> 00:18:36,390
have a distributed cache and you offer

00:18:33,300 --> 00:18:38,880
also a state management so it has quite

00:18:36,390 --> 00:18:41,010
cool features and even more features

00:18:38,880 --> 00:18:43,380
like back pressure resource away

00:18:41,010 --> 00:18:45,810
scheduler it now has native windowing

00:18:43,380 --> 00:18:47,520
API and you have improved debug and

00:18:45,810 --> 00:18:50,190
profiling especially in the storm you I

00:18:47,520 --> 00:18:51,870
of you haven't seen that so you can have

00:18:50,190 --> 00:18:55,530
double sampling and stuff like that so

00:18:51,870 --> 00:18:57,510
they improved a lot so storm is not it's

00:18:55,530 --> 00:18:59,100
not dead or something so you can still

00:18:57,510 --> 00:19:01,110
use it for those projects even though

00:18:59,100 --> 00:19:03,330
there are a lot of alternatives as we

00:19:01,110 --> 00:19:06,000
have seen on the conference and yeah you

00:19:03,330 --> 00:19:07,470
have to choose wisely and for example we

00:19:06,000 --> 00:19:09,210
have caprica streams you have spark

00:19:07,470 --> 00:19:11,190
streaming and you have linked so those

00:19:09,210 --> 00:19:13,710
are the things that are actually quite

00:19:11,190 --> 00:19:15,450
hot right now and yeah you have to think

00:19:13,710 --> 00:19:17,370
about those alternatives for our use

00:19:15,450 --> 00:19:20,460
case storm was completely fine because

00:19:17,370 --> 00:19:22,290
we decoupled the whole logic in this

00:19:20,460 --> 00:19:24,570
JavaScript snippet so we could do it

00:19:22,290 --> 00:19:26,310
with any of those systems of course we

00:19:24,570 --> 00:19:29,460
require low latency and that's why we

00:19:26,310 --> 00:19:31,230
also chose storm basically and to give

00:19:29,460 --> 00:19:33,210
you some more input you should check out

00:19:31,230 --> 00:19:35,400
those presentations actually the first

00:19:33,210 --> 00:19:37,260
two were happening yesterday about kafka

00:19:35,400 --> 00:19:40,170
streams and apache chief link and the

00:19:37,260 --> 00:19:41,850
last one was to talk about the new storm

00:19:40,170 --> 00:19:44,160
version that the hadoop summit so if you

00:19:41,850 --> 00:19:46,980
I'm the state where you have to choose a

00:19:44,160 --> 00:19:48,390
stream processing system just have a

00:19:46,980 --> 00:19:51,980
look at those presentation they're very

00:19:48,390 --> 00:19:54,210
good and very informative about that ok

00:19:51,980 --> 00:19:56,550
the next component in this whole

00:19:54,210 --> 00:19:58,720
infrastructure that we use for the way

00:19:56,550 --> 00:20:01,210
to MCM stuff was Cassandra

00:19:58,720 --> 00:20:05,950
and like I said we use it to store

00:20:01,210 --> 00:20:08,590
basically a player profile so talking

00:20:05,950 --> 00:20:10,419
about Cassandra I'm just quick question

00:20:08,590 --> 00:20:13,240
who views using Cassandra and production

00:20:10,419 --> 00:20:16,960
does anyone use it okay it's well that's

00:20:13,240 --> 00:20:18,370
quite a lot actually yeah I will make it

00:20:16,960 --> 00:20:21,070
short so basically cassandra has a

00:20:18,370 --> 00:20:23,890
project developed at Facebook its own

00:20:21,070 --> 00:20:28,210
source and it's based on Dena Modi be by

00:20:23,890 --> 00:20:29,530
amazon and a big table by google and for

00:20:28,210 --> 00:20:31,720
this presentation i thought how can I

00:20:29,530 --> 00:20:33,909
explain Cassandra with just one slide

00:20:31,720 --> 00:20:36,250
basically and I think that's quite

00:20:33,909 --> 00:20:38,950
accurate it's it's not perfect but I

00:20:36,250 --> 00:20:40,570
think it describes it very well so if

00:20:38,950 --> 00:20:42,820
you think about Cassandra and the data

00:20:40,570 --> 00:20:45,669
model behind Cassandra just think about

00:20:42,820 --> 00:20:47,890
it as a sort of map that has a rocky and

00:20:45,669 --> 00:20:51,850
then again a sorted map with key value

00:20:47,890 --> 00:20:55,900
pairs I think that's basically the idea

00:20:51,850 --> 00:20:57,640
behind Cassandra and you can see based

00:20:55,900 --> 00:21:00,400
on the rocky you have a hash value and

00:20:57,640 --> 00:21:02,620
then with consistent hashing at the data

00:21:00,400 --> 00:21:04,780
gets distributed around the cluster and

00:21:02,620 --> 00:21:07,030
you have also data replication so I

00:21:04,780 --> 00:21:09,010
think that's basically the idea behind

00:21:07,030 --> 00:21:12,570
cassandra is also explaining it very

00:21:09,010 --> 00:21:14,799
well so that's basically kasama and

00:21:12,570 --> 00:21:17,080
going more in detail about the data

00:21:14,799 --> 00:21:19,000
model in Cassandra on the left side you

00:21:17,080 --> 00:21:20,380
have an example for bad data model on

00:21:19,000 --> 00:21:22,990
the right side of an example for good

00:21:20,380 --> 00:21:24,970
data model so what you should not do is

00:21:22,990 --> 00:21:27,010
you should not have something like a

00:21:24,970 --> 00:21:29,679
rocky based on the hour or the day and

00:21:27,010 --> 00:21:33,820
then x them payloads timestamp a lotus

00:21:29,679 --> 00:21:35,740
the rose as the column sorry because the

00:21:33,820 --> 00:21:37,299
issue that you have is what you want is

00:21:35,740 --> 00:21:39,880
you want to spread the data evenly

00:21:37,299 --> 00:21:41,799
across the cluster and if you have the

00:21:39,880 --> 00:21:44,409
hour for example is a rocky you have the

00:21:41,799 --> 00:21:45,970
issue that the queries of the same all

00:21:44,409 --> 00:21:48,159
will always go to the same node so you

00:21:45,970 --> 00:21:51,520
don't have parallelization at all so

00:21:48,159 --> 00:21:53,200
it's it's very bad to do it like this on

00:21:51,520 --> 00:21:55,690
the other hand if you use the timestamp

00:21:53,200 --> 00:21:57,429
for example so for time series data you

00:21:55,690 --> 00:21:59,169
might have the issue that you have two

00:21:57,429 --> 00:22:01,659
different payloads with the same time

00:21:59,169 --> 00:22:03,370
stem and you overwrite your data so a

00:22:01,659 --> 00:22:05,890
good model would be if you choose

00:22:03,370 --> 00:22:07,990
something that spreads the data evenly

00:22:05,890 --> 00:22:10,419
across the cluster like the combination

00:22:07,990 --> 00:22:12,999
of the hour or the day together with an

00:22:10,419 --> 00:22:15,489
ID for us it's the player ID for example

00:22:12,999 --> 00:22:17,409
and you have to use the time give your

00:22:15,489 --> 00:22:19,809
ID instead of just the time stamps that

00:22:17,409 --> 00:22:21,399
will be a good model and basically as

00:22:19,809 --> 00:22:24,369
you can see on this slide your gold

00:22:21,399 --> 00:22:25,989
should really be you should spread the

00:22:24,369 --> 00:22:27,999
data even across the cluster of course

00:22:25,989 --> 00:22:29,919
you should minimize the number of

00:22:27,999 --> 00:22:35,079
petition sweet means if you want to know

00:22:29,919 --> 00:22:37,349
something it's good to only read as few

00:22:35,079 --> 00:22:39,999
partitions as possible of course and

00:22:37,349 --> 00:22:41,769
what is also very useful is if you start

00:22:39,999 --> 00:22:43,329
thinking about your queries and then

00:22:41,769 --> 00:22:46,709
start thinking about your data model

00:22:43,329 --> 00:22:50,169
when working with Cassandra well and

00:22:46,709 --> 00:22:52,569
that's what we came up with for our data

00:22:50,169 --> 00:22:56,289
model for the player profile so

00:22:52,569 --> 00:22:58,209
basically what we did there was we have

00:22:56,289 --> 00:23:00,069
the unique player ID which is a unique

00:22:58,209 --> 00:23:02,739
identifier for the player we have the

00:23:00,069 --> 00:23:04,899
day and that's basically our okey so by

00:23:02,739 --> 00:23:07,539
this roki we distribute the data across

00:23:04,899 --> 00:23:09,129
our cluster and then for the column keys

00:23:07,539 --> 00:23:11,499
we have time you your IDs like I

00:23:09,129 --> 00:23:13,089
mentioned before and then we just store

00:23:11,499 --> 00:23:15,129
the type of the event and the event

00:23:13,089 --> 00:23:17,349
itself and then we can use it in your

00:23:15,129 --> 00:23:19,749
application and can read the data from

00:23:17,349 --> 00:23:24,699
it so that's basically how that works

00:23:19,749 --> 00:23:26,259
and yeah the issue was it is you are

00:23:24,699 --> 00:23:28,809
very fast if you want to know something

00:23:26,259 --> 00:23:31,899
about a play on a specific day for

00:23:28,809 --> 00:23:33,909
example if you think back to Bob yeah

00:23:31,899 --> 00:23:35,409
Bob lost three fights in a row if you

00:23:33,909 --> 00:23:38,679
want to know that it's very simple you

00:23:35,409 --> 00:23:40,749
just I'm Crary Bob by its play ID and

00:23:38,679 --> 00:23:42,789
also for the specific day and then you

00:23:40,749 --> 00:23:45,939
know okay he was losing three fights

00:23:42,789 --> 00:23:48,639
fights already that day now the issues

00:23:45,939 --> 00:23:50,559
if you want to know all the players had

00:23:48,639 --> 00:23:52,569
already lost three fights to turn it

00:23:50,559 --> 00:23:54,909
around and you want to do an analytical

00:23:52,569 --> 00:23:57,249
query on that data then you might have

00:23:54,909 --> 00:23:59,439
an issue because you have to scan all

00:23:57,249 --> 00:24:01,989
the row keys and that might take some

00:23:59,439 --> 00:24:05,799
time and that's why we also threw an

00:24:01,989 --> 00:24:08,619
pinch of spark so we use sparkfun order

00:24:05,799 --> 00:24:10,719
to get queries in that case and I'm sure

00:24:08,619 --> 00:24:12,189
most of you know spark already so again

00:24:10,719 --> 00:24:14,169
the question who's using Sparkle

00:24:12,189 --> 00:24:17,349
production I think a lot of you yes

00:24:14,169 --> 00:24:19,509
that's what I saw it so again spark is

00:24:17,349 --> 00:24:21,489
basically a general-purpose classic

00:24:19,509 --> 00:24:24,429
computation system that means that you

00:24:21,489 --> 00:24:26,319
can use it for any purpose and it can

00:24:24,429 --> 00:24:26,990
run standalone it can run on missiles

00:24:26,319 --> 00:24:29,210
that come on

00:24:26,990 --> 00:24:31,429
so it's very flexible and environment

00:24:29,210 --> 00:24:33,260
you have different data source API so

00:24:31,429 --> 00:24:35,809
you can use data from Hadoop from

00:24:33,260 --> 00:24:37,970
Cassandra hive HBase whatever you like

00:24:35,809 --> 00:24:40,640
and then you have for example the data

00:24:37,970 --> 00:24:44,210
frame API right now there's also data

00:24:40,640 --> 00:24:46,429
set API which is even it's even on a

00:24:44,210 --> 00:24:48,260
higher level basically and with this

00:24:46,429 --> 00:24:49,670
data frame API you can then use

00:24:48,260 --> 00:24:51,200
different libraries and different

00:24:49,670 --> 00:24:54,380
languages to work with your data after

00:24:51,200 --> 00:24:56,570
that spark is generating a daj order for

00:24:54,380 --> 00:24:59,809
your application code and will process

00:24:56,570 --> 00:25:02,150
the data on the cluster basically that's

00:24:59,809 --> 00:25:06,410
what spark is doing now what we did with

00:25:02,150 --> 00:25:08,510
it is that we actually install this bar

00:25:06,410 --> 00:25:11,300
cluster alongside our Cassandra cluster

00:25:08,510 --> 00:25:14,929
solve this issue and this is quite cool

00:25:11,300 --> 00:25:17,240
because there is a actually a spark

00:25:14,929 --> 00:25:19,160
Cassandra connector by datastax that we

00:25:17,240 --> 00:25:21,350
use together with the fact that we have

00:25:19,160 --> 00:25:23,270
data locality and based on that we can

00:25:21,350 --> 00:25:29,720
do very fast analytical queries on

00:25:23,270 --> 00:25:31,760
Cassandra data and actually this solved

00:25:29,720 --> 00:25:33,800
the issue that we are not able to find

00:25:31,760 --> 00:25:38,660
out how many players lost three fights

00:25:33,800 --> 00:25:42,170
already beside that we tried another

00:25:38,660 --> 00:25:43,730
project called the spark job server I

00:25:42,170 --> 00:25:46,610
don't know if you know that but it's

00:25:43,730 --> 00:25:50,809
quite cool it offers you a REST API that

00:25:46,610 --> 00:25:52,940
is able to to run spark logic on a

00:25:50,809 --> 00:25:55,100
sparker cluster and yeah you can

00:25:52,940 --> 00:25:58,370
interact with it using a REST API and

00:25:55,100 --> 00:26:02,360
the cool fact about it is that it has so

00:25:58,370 --> 00:26:04,940
it's managing context objects so in this

00:26:02,360 --> 00:26:06,740
case in this example as you can see we

00:26:04,940 --> 00:26:09,020
are cashing a Cassandra table so we're

00:26:06,740 --> 00:26:11,000
using Cassandra SQL context and we are

00:26:09,020 --> 00:26:14,360
cashing a Cassandra table if it's not

00:26:11,000 --> 00:26:16,250
cached already so in the bottom part

00:26:14,360 --> 00:26:18,440
right here the first time you run the

00:26:16,250 --> 00:26:19,850
code this will take quite a while but

00:26:18,440 --> 00:26:21,950
the second time we run the code will be

00:26:19,850 --> 00:26:24,020
very fast and if you have different

00:26:21,950 --> 00:26:25,940
analytical crevasse than that on the

00:26:24,020 --> 00:26:28,550
data they will also be fast because the

00:26:25,940 --> 00:26:29,900
data is cached already of course you

00:26:28,550 --> 00:26:32,150
have to clear the cache from time to

00:26:29,900 --> 00:26:34,250
time like like how we like it for

00:26:32,150 --> 00:26:36,980
example every hour so we have to recast

00:26:34,250 --> 00:26:39,149
it but using that spark drop servers

00:26:36,980 --> 00:26:42,100
quite powerful to do those analytical

00:26:39,149 --> 00:26:43,840
this is actually something that we don't

00:26:42,100 --> 00:26:46,330
use in production right now it's just

00:26:43,840 --> 00:26:47,860
playing around with it and because we

00:26:46,330 --> 00:26:50,620
have to find a solution how to run those

00:26:47,860 --> 00:26:53,169
analytical crows and for now it works

00:26:50,620 --> 00:26:55,269
quite good so if you are interested in

00:26:53,169 --> 00:27:00,820
that give it a try and give us feedback

00:26:55,269 --> 00:27:02,409
if it works for you actually ok so now

00:27:00,820 --> 00:27:04,779
want to put all that stuff together

00:27:02,409 --> 00:27:06,549
again so I've shown you several

00:27:04,779 --> 00:27:08,350
components of this real-time see I'm

00:27:06,549 --> 00:27:11,289
project that we run a dental games and

00:27:08,350 --> 00:27:12,850
it's all happening within this data

00:27:11,289 --> 00:27:17,230
infrastructure basically on the left

00:27:12,850 --> 00:27:19,450
sides on the data pipeline and there are

00:27:17,230 --> 00:27:22,120
some takeaways that we learned from that

00:27:19,450 --> 00:27:24,100
so first of all Kafka is quite awesome I

00:27:22,120 --> 00:27:26,590
think we agree with that because it

00:27:24,100 --> 00:27:29,259
works really well first before we use

00:27:26,590 --> 00:27:31,179
Kafka we used Kestrel and we had several

00:27:29,259 --> 00:27:32,679
issues with it and then we switch to

00:27:31,179 --> 00:27:35,590
Kafka now everything is fine and

00:27:32,679 --> 00:27:37,960
actually the more we use it as its

00:27:35,590 --> 00:27:41,379
desire to be so as a message bus the

00:27:37,960 --> 00:27:44,710
better it gets so it's quite cool storm

00:27:41,379 --> 00:27:46,929
actually it works quite well for us so

00:27:44,710 --> 00:27:50,230
it makes it easy to actually write those

00:27:46,929 --> 00:27:52,149
real time computation applications but

00:27:50,230 --> 00:27:53,649
we had performance issues and it was

00:27:52,149 --> 00:27:55,720
also in the beginning very hard to

00:27:53,649 --> 00:27:58,330
define topologies because you had such

00:27:55,720 --> 00:28:00,039
so much glue cult there was really hard

00:27:58,330 --> 00:28:02,559
to test and it was not developer

00:28:00,039 --> 00:28:04,480
friendly but like I've shown you those

00:28:02,559 --> 00:28:07,899
disadvantages were addressed and also

00:28:04,480 --> 00:28:11,409
resolved partially in the newest version

00:28:07,899 --> 00:28:14,970
0 10 and on one dot oh and yeah so

00:28:11,409 --> 00:28:17,679
that's about stone last but not least

00:28:14,970 --> 00:28:20,139
reacting to user behavior is quite

00:28:17,679 --> 00:28:22,179
powerful so I cannot give you any

00:28:20,139 --> 00:28:24,070
numbers on uplifts how much uplift it

00:28:22,179 --> 00:28:26,019
generates because it's just running on

00:28:24,070 --> 00:28:28,149
some test markets right now but it looks

00:28:26,019 --> 00:28:30,279
already quite promising and if you think

00:28:28,149 --> 00:28:32,049
about it it's quite powerful because

00:28:30,279 --> 00:28:34,389
it's basically the next step to do

00:28:32,049 --> 00:28:37,240
marketing or CRM based on the user

00:28:34,389 --> 00:28:39,879
behavior real-time now the issue is with

00:28:37,240 --> 00:28:41,740
that it's not only a challenge for the

00:28:39,879 --> 00:28:43,779
developers it's also a challenge for the

00:28:41,740 --> 00:28:46,539
game designers in our case for example

00:28:43,779 --> 00:28:49,029
imagine you have some back in the

00:28:46,539 --> 00:28:50,620
JavaScript some wrong definition and all

00:28:49,029 --> 00:28:53,309
of a sudden all the players we get those

00:28:50,620 --> 00:28:56,249
for battle tanks for free and

00:28:53,309 --> 00:28:58,889
Sonia complete game is ruined and the

00:28:56,249 --> 00:29:01,499
whole game balance is broken so it's

00:28:58,889 --> 00:29:03,509
quite dangerous to to react in real time

00:29:01,499 --> 00:29:05,820
to the user behavior if you not think

00:29:03,509 --> 00:29:08,059
enough about it before so it's also

00:29:05,820 --> 00:29:10,889
challenged for the game designers and

00:29:08,059 --> 00:29:13,950
combining storm actually with Nassau was

00:29:10,889 --> 00:29:15,570
a great idea it was very fun to do we

00:29:13,950 --> 00:29:17,279
didn't had any performance issues so far

00:29:15,570 --> 00:29:19,230
and it's quite cool because you can

00:29:17,279 --> 00:29:22,200
define the whole logic in JavaScript

00:29:19,230 --> 00:29:24,269
which is quite a good feeling and it can

00:29:22,200 --> 00:29:26,879
be changed at one time of course that

00:29:24,269 --> 00:29:28,980
being said the whole project was very

00:29:26,879 --> 00:29:31,499
fun to do and we used a lot of those

00:29:28,980 --> 00:29:33,360
technologies actually the next step for

00:29:31,499 --> 00:29:36,389
us would be to think more about the

00:29:33,360 --> 00:29:38,490
analytical queries on Cassandra also

00:29:36,389 --> 00:29:40,860
maybe we think about switching from

00:29:38,490 --> 00:29:42,960
storm tough link or just give it a try

00:29:40,860 --> 00:29:45,059
maybe and yeah and then we have of

00:29:42,960 --> 00:29:47,039
course to bring it from the test markets

00:29:45,059 --> 00:29:49,879
to the production markets to see what

00:29:47,039 --> 00:29:53,669
kind of uplifted can generate for us so

00:29:49,879 --> 00:29:56,190
that was actually quite fast and there

00:29:53,669 --> 00:29:58,590
was a project already so I hope I could

00:29:56,190 --> 00:30:00,450
give you some yeah small insights about

00:29:58,590 --> 00:30:02,610
what to do what you can do with those

00:30:00,450 --> 00:30:04,860
technologies some practically use case

00:30:02,610 --> 00:30:07,230
and if you have any questions feel free

00:30:04,860 --> 00:30:09,649
to ask so thank you very much thank you

00:30:07,230 --> 00:30:09,649
very much

00:30:15,380 --> 00:30:25,680
questions we have 10 minutes to take the

00:30:18,120 --> 00:30:31,670
questions any questions I think oh yeah

00:30:25,680 --> 00:30:31,670
I see a hand here and one day right okay

00:30:37,460 --> 00:30:44,910
I'm curious if how it has if it has

00:30:41,880 --> 00:30:47,730
worked well for you to run this park on

00:30:44,910 --> 00:30:52,950
top of Cassandra and what the largest

00:30:47,730 --> 00:30:54,990
rdd sizes that you process or so you get

00:30:52,950 --> 00:30:57,060
the question and I'm curious if you had

00:30:54,990 --> 00:31:00,150
any problems running a spark on top of

00:30:57,060 --> 00:31:01,770
Sandra and especially since it's part of

00:31:00,150 --> 00:31:05,130
the critical pipeline for the real time

00:31:01,770 --> 00:31:06,720
processing actually know so far we

00:31:05,130 --> 00:31:09,870
didn't have any issues but i have to say

00:31:06,720 --> 00:31:11,940
that it's we are the use case with

00:31:09,870 --> 00:31:13,680
Spargo Cassandra that we that we tried

00:31:11,940 --> 00:31:15,750
all this very small so it's it's really

00:31:13,680 --> 00:31:17,820
currently we are not using all events

00:31:15,750 --> 00:31:21,210
and to be stored in the player profile

00:31:17,820 --> 00:31:23,880
so it's it fits a memory it's fine for

00:31:21,210 --> 00:31:27,240
for what I can say right now but yeah it

00:31:23,880 --> 00:31:28,710
might be well it can be hard in the when

00:31:27,240 --> 00:31:31,380
you have too much data that doesn't fit

00:31:28,710 --> 00:31:34,110
into memory but for now it works quite

00:31:31,380 --> 00:31:35,850
well and it's also just yet we just gave

00:31:34,110 --> 00:31:37,710
it a try it's just testing how to do

00:31:35,850 --> 00:31:40,320
those another ticket crow is actually

00:31:37,710 --> 00:31:43,260
it's based on a blog post from data sex

00:31:40,320 --> 00:31:44,910
so they have a presentation about it you

00:31:43,260 --> 00:31:46,560
can google for it and you get more

00:31:44,910 --> 00:31:51,600
details about it but for us it works so

00:31:46,560 --> 00:31:54,320
far so that's what I can tell thank you

00:31:51,600 --> 00:31:54,320
any more questions

00:32:01,269 --> 00:32:07,490
so really simple question if you already

00:32:03,919 --> 00:32:10,130
have storm cluster yes Oh like flink why

00:32:07,490 --> 00:32:15,500
not to do some analytics already on that

00:32:10,130 --> 00:32:18,529
step right like all this chain you can

00:32:15,500 --> 00:32:20,600
click precompute something so requesters

00:32:18,529 --> 00:32:24,230
why of doing the analytics not already

00:32:20,600 --> 00:32:26,240
installed you mean that's why basically

00:32:24,230 --> 00:32:29,480
what we build up is I can go few slides

00:32:26,240 --> 00:32:31,429
back so this one maybe so the production

00:32:29,480 --> 00:32:33,080
storage in that case or the whole data

00:32:31,429 --> 00:32:36,799
pipeline doesn't know the whole history

00:32:33,080 --> 00:32:39,200
of of the player so that's one issue for

00:32:36,799 --> 00:32:41,090
us and we keep the whole history in

00:32:39,200 --> 00:32:43,250
Hadoop and that's why we need some data

00:32:41,090 --> 00:32:46,039
from a tube as well so that's why we do

00:32:43,250 --> 00:32:48,380
it later on the analytics basically for

00:32:46,039 --> 00:32:50,149
now of course you can think of doing

00:32:48,380 --> 00:32:52,010
more analytics already on the left side

00:32:50,149 --> 00:32:54,679
actually we are thinking about it but

00:32:52,010 --> 00:32:56,450
for us the issue is that we have a lot

00:32:54,679 --> 00:33:01,130
of data and it's all within Hadoop

00:32:56,450 --> 00:33:08,620
currently that's the issue so yeah but

00:33:01,130 --> 00:33:11,590
good point yes any more questions super

00:33:08,620 --> 00:33:15,590
how big is your Cassandra cluster

00:33:11,590 --> 00:33:17,539
current is just eight notes so it's it's

00:33:15,590 --> 00:33:20,450
quite big notes but yeah I got screwy

00:33:17,539 --> 00:33:22,789
right and I saw you had what 16

00:33:20,450 --> 00:33:25,669
partitions in Kafka like / tapia how

00:33:22,789 --> 00:33:28,490
come you chose 16 in this case 16 good

00:33:25,669 --> 00:33:31,279
question will work for us so I could go

00:33:28,490 --> 00:33:33,320
back to the slide actually yeah we chose

00:33:31,279 --> 00:33:37,100
16 positions because that workforce

00:33:33,320 --> 00:33:39,500
quite well so it was trying out it's not

00:33:37,100 --> 00:33:41,600
by the book basically but that work for

00:33:39,500 --> 00:33:43,490
us quite well so it's just by

00:33:41,600 --> 00:33:45,110
benchmarking yeah because I was thinking

00:33:43,490 --> 00:33:47,690
you know you might I want to use more

00:33:45,110 --> 00:33:50,809
because what you had five note 6 notes

00:33:47,690 --> 00:33:53,389
would we have five to eight cores yes

00:33:50,809 --> 00:33:56,630
caicos that makes what 40 coins in total

00:33:53,389 --> 00:33:58,340
if you then have 16 petitions and you

00:33:56,630 --> 00:34:00,320
know it seems that you have a bottleneck

00:33:58,340 --> 00:34:02,059
on the petition side yeah at some point

00:34:00,320 --> 00:34:06,210
maybe yeah what's up well that might be

00:34:02,059 --> 00:34:16,500
true yes good point okay

00:34:06,210 --> 00:34:18,390
any other questions thank you so much

00:34:16,500 --> 00:34:21,410
for good that was a neat presentation

00:34:18,390 --> 00:34:21,410

YouTube URL: https://www.youtube.com/watch?v=8WlpnmQzqww


