Title: Berlin Buzzwords 2016: Dominic Heger - ExpAn: A Python library for advanced statistical analysis ...
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Find more information here: https://berlinbuzzwords.de/session/expan-python-library-advanced-statistical-analysis-ab-tests

A/B tests, or randomized controlled experiments, have been widely applied in different industries to optimize the business process and the user experience. Here we'll introduce a Python library, ExpAn, intended for the statistical analysis of A/B tests.

The input data to ExpAn has a standard format, which is defined to interface with different data sources. The main statistical functions in ExpAn are all standalone and work with either the library-specific input data structure or some Python built-in data types. Among others, the functions can be used to assess whether the randomization is appropriate, and measure the expectation and error margin of the uplift due to the treatment. We also implemented a robust discretization algorithm to handle typical heavy-tailed distributions in the real world. Finally, a generic result structure is designed to incorporate results from different types of analyses.

One can easily feed data from other domain-specific data fetching modules into ExpAn. Other advanced algorithms for the analysis of A/B test data can be implemented and plugged into ExpAn, eg. a Bayesian hypothesis testing scheme instead of the frequentist approach. The generality of the result structure also makes it handy to apply different kinds of visualization on top of the data.

This is a talk sponsored by Zalando.

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:03,440 --> 00:00:14,969
hi everyone can you hear me well so

00:00:07,859 --> 00:00:17,340
great so I'd like to talk about expand

00:00:14,969 --> 00:00:20,580
which is a new library that we are

00:00:17,340 --> 00:00:24,119
developing at zalando and it's all about

00:00:20,580 --> 00:00:27,689
the statistical analysis of AP tests so

00:00:24,119 --> 00:00:30,869
few questions who view has worked with a

00:00:27,689 --> 00:00:33,870
be testing before okay that's great

00:00:30,869 --> 00:00:38,040
because our library is really convenient

00:00:33,870 --> 00:00:41,610
for for users and Who am has worked on

00:00:38,040 --> 00:00:43,829
developing tools for a be testing okay

00:00:41,610 --> 00:00:45,899
just a few so this is really great we

00:00:43,829 --> 00:00:48,870
are open source and maybe you have

00:00:45,899 --> 00:00:52,620
something to contribute and we would

00:00:48,870 --> 00:00:56,250
love to see that so yeah but let me

00:00:52,620 --> 00:01:00,809
first tell you a little bit about my

00:00:56,250 --> 00:01:04,159
story so I'm with Solano only for three

00:01:00,809 --> 00:01:06,390
months now so I'm pretty in you and

00:01:04,159 --> 00:01:09,360
before that I've been a research

00:01:06,390 --> 00:01:11,460
assistant working on the analysis of

00:01:09,360 --> 00:01:16,590
brain activity and brain computer

00:01:11,460 --> 00:01:19,320
interfaces so what researchers are

00:01:16,590 --> 00:01:22,320
concerned about is the brain and how it

00:01:19,320 --> 00:01:25,049
works and it's really like there's a lot

00:01:22,320 --> 00:01:28,049
of complex behavior there and nobody

00:01:25,049 --> 00:01:31,530
really understands how it works and what

00:01:28,049 --> 00:01:36,450
researchers do is they do experiments

00:01:31,530 --> 00:01:39,240
and these experiments are like we invite

00:01:36,450 --> 00:01:41,240
people to our lab glue electrodes on

00:01:39,240 --> 00:01:45,450
their heads and measure brain activity

00:01:41,240 --> 00:01:48,439
when we expose them to an experiment

00:01:45,450 --> 00:01:51,509
that we've designed for given hypothesis

00:01:48,439 --> 00:01:54,659
so this is usually done with a very

00:01:51,509 --> 00:01:58,110
small number of subjects we have because

00:01:54,659 --> 00:02:01,500
it's super time-consuming and yeah

00:01:58,110 --> 00:02:05,520
costly therefore but after measuring

00:02:01,500 --> 00:02:07,500
these brain activity is we do

00:02:05,520 --> 00:02:11,700
statistical analysis of that and that's

00:02:07,500 --> 00:02:13,680
the exciting part and because there you

00:02:11,700 --> 00:02:15,989
can really get all the in

00:02:13,680 --> 00:02:19,129
sites and come up with new hypothesis to

00:02:15,989 --> 00:02:22,370
do new experiments so this is scientific

00:02:19,129 --> 00:02:25,500
experimentation and we can pretty much

00:02:22,370 --> 00:02:28,469
transfer that to the business world so

00:02:25,500 --> 00:02:29,939
we want to do experiments there but we

00:02:28,469 --> 00:02:32,430
are not really interested in how the

00:02:29,939 --> 00:02:36,450
brain works but a lot of what we are

00:02:32,430 --> 00:02:42,180
doing is about our customers and how

00:02:36,450 --> 00:02:45,209
about how we can do our services as best

00:02:42,180 --> 00:02:48,299
as possible to fit their needs so we

00:02:45,209 --> 00:02:50,579
want to gain insights from their complex

00:02:48,299 --> 00:02:52,680
behavior so they are really super

00:02:50,579 --> 00:02:55,680
diverse and a lot of things play into

00:02:52,680 --> 00:02:58,939
that like emotions and and things like

00:02:55,680 --> 00:03:01,379
that and so and we want to do

00:02:58,939 --> 00:03:04,680
experimentation and we are in a

00:03:01,379 --> 00:03:08,549
fantastic situation here because and we

00:03:04,680 --> 00:03:12,090
can easily expose things that we want to

00:03:08,549 --> 00:03:14,280
try in experiments to a lot of people so

00:03:12,090 --> 00:03:16,590
this is really great from a statistical

00:03:14,280 --> 00:03:20,400
point of view because we get great

00:03:16,590 --> 00:03:23,699
statistical validity but also these

00:03:20,400 --> 00:03:26,040
results and have some real world meaning

00:03:23,699 --> 00:03:28,079
and we can expect that the things that

00:03:26,040 --> 00:03:31,579
we are trying and experimenting are

00:03:28,079 --> 00:03:36,599
really working out in the real world so

00:03:31,579 --> 00:03:40,829
again this is the statistical analysis

00:03:36,599 --> 00:03:44,009
is really the exciting part and because

00:03:40,829 --> 00:03:48,209
here we can get new insights and drive

00:03:44,009 --> 00:03:53,280
our processes to build better and better

00:03:48,209 --> 00:03:55,500
project or product and so we come up

00:03:53,280 --> 00:03:58,470
with new hypotheses from what we've

00:03:55,500 --> 00:04:01,229
learned and this can be a lot of

00:03:58,470 --> 00:04:04,680
different stuff here so and of course

00:04:01,229 --> 00:04:06,750
typically a/b testing is changing the

00:04:04,680 --> 00:04:09,720
front end for example the color of a

00:04:06,750 --> 00:04:12,540
button in a nap or and but it can also

00:04:09,720 --> 00:04:17,159
be something like Becca and algorithm

00:04:12,540 --> 00:04:19,650
and the x-band that I want to talk about

00:04:17,159 --> 00:04:22,919
now is all about this statistical

00:04:19,650 --> 00:04:27,810
analysis of these tests or experiments

00:04:22,919 --> 00:04:30,630
that we are doing and yeah there

00:04:27,810 --> 00:04:35,400
you can can use that for all different

00:04:30,630 --> 00:04:41,310
kinds of experiments so the x-men

00:04:35,400 --> 00:04:44,610
library is a library for statistical

00:04:41,310 --> 00:04:48,960
analysis of these randomly control

00:04:44,610 --> 00:04:52,110
trials and you can use that for a be

00:04:48,960 --> 00:04:53,730
testing and multivariate testing and it

00:04:52,110 --> 00:04:57,480
supports all different kinds of

00:04:53,730 --> 00:04:59,780
mattresses and metrics that you want to

00:04:57,480 --> 00:05:03,030
include in these analysis like

00:04:59,780 --> 00:05:05,130
categorical variables and continuous

00:05:03,030 --> 00:05:07,979
well bones and things like that and it's

00:05:05,130 --> 00:05:10,260
open source now released under the MIT

00:05:07,979 --> 00:05:14,280
license and available on github and

00:05:10,260 --> 00:05:17,910
written in Python so what it's excellent

00:05:14,280 --> 00:05:20,479
not it's not a complete a be testing

00:05:17,910 --> 00:05:25,020
framework so it's not about the

00:05:20,479 --> 00:05:27,180
assignment of the different entities

00:05:25,020 --> 00:05:30,780
like the the customers to the different

00:05:27,180 --> 00:05:33,930
variants and it's also not about how you

00:05:30,780 --> 00:05:36,690
get your business data in for for the

00:05:33,930 --> 00:05:38,580
analysis I think for the first point for

00:05:36,690 --> 00:05:41,250
the random assignment there are nice

00:05:38,580 --> 00:05:44,910
projects on github I did a little bit of

00:05:41,250 --> 00:05:47,250
research about that and I think they

00:05:44,910 --> 00:05:50,580
would greatly work together with x-men

00:05:47,250 --> 00:05:55,340
and for the data retrieval process you

00:05:50,580 --> 00:05:58,289
might want to have your custom developed

00:05:55,340 --> 00:06:03,539
processes there yet just to retrieve

00:05:58,289 --> 00:06:08,160
your business relevant metrics so I'd

00:06:03,539 --> 00:06:13,020
like to talk about three parts of the

00:06:08,160 --> 00:06:16,440
library and that's the experiment object

00:06:13,020 --> 00:06:19,380
and that's the most important thing for

00:06:16,440 --> 00:06:22,020
the user to interact where you can do

00:06:19,380 --> 00:06:25,470
all the analysis from that and I will go

00:06:22,020 --> 00:06:28,680
into a bit of a detail on what goes into

00:06:25,470 --> 00:06:31,370
this experiment object and how you can

00:06:28,680 --> 00:06:35,039
use it then I will talk about the

00:06:31,370 --> 00:06:37,770
analyses and in the end show you our

00:06:35,039 --> 00:06:40,860
results structure and yeah what you get

00:06:37,770 --> 00:06:45,310
out of that so

00:06:40,860 --> 00:06:50,910
to create an experiment object you need

00:06:45,310 --> 00:06:54,160
three kinds of ingredients its metadata

00:06:50,910 --> 00:06:57,550
features and key performance indicators

00:06:54,160 --> 00:07:02,860
and I will go shortly into a bit of

00:06:57,550 --> 00:07:06,850
details about that so metadata is quite

00:07:02,860 --> 00:07:08,770
simple key value pairs and that give you

00:07:06,850 --> 00:07:11,950
additional information about the

00:07:08,770 --> 00:07:15,220
experiment so this is very simple but

00:07:11,950 --> 00:07:19,090
it's very important as well so to have

00:07:15,220 --> 00:07:20,950
information about the experiment ID the

00:07:19,090 --> 00:07:24,100
experiment names the different data

00:07:20,950 --> 00:07:27,640
sources involved different units timings

00:07:24,100 --> 00:07:30,820
and things like that and that you want

00:07:27,640 --> 00:07:35,040
to store together with with the data to

00:07:30,820 --> 00:07:39,250
ya be able to make good analyses the

00:07:35,040 --> 00:07:41,740
second thing is features and this is all

00:07:39,250 --> 00:07:44,700
about properties of the entities that

00:07:41,740 --> 00:07:46,900
you are investigating so for example

00:07:44,700 --> 00:07:50,350
demographic properties about your

00:07:46,900 --> 00:07:53,590
customers that you know and here we have

00:07:50,350 --> 00:07:58,050
for example this guy and we know he is

00:07:53,590 --> 00:08:01,900
male 35 years old and the frequent buyer

00:07:58,050 --> 00:08:04,560
so things like that and can go into the

00:08:01,900 --> 00:08:08,380
features and these features are and

00:08:04,560 --> 00:08:10,990
don't change during the experiment and

00:08:08,380 --> 00:08:14,650
they are there for you like measured

00:08:10,990 --> 00:08:16,660
before the treatment start and you can

00:08:14,650 --> 00:08:20,050
represent that in such a table where you

00:08:16,660 --> 00:08:25,120
have like an idea of the entity here and

00:08:20,050 --> 00:08:27,550
all the features over there and the

00:08:25,120 --> 00:08:33,729
third part is kpi's key performance

00:08:27,550 --> 00:08:37,360
indicators and it is yeah the measures

00:08:33,729 --> 00:08:40,270
of the effects that you you want to see

00:08:37,360 --> 00:08:42,400
so this is about your business metrics

00:08:40,270 --> 00:08:45,940
for example but can also be like

00:08:42,400 --> 00:08:50,110
conversion rates and we can have several

00:08:45,940 --> 00:08:53,760
of them and they are typically measured

00:08:50,110 --> 00:08:56,220
of course after the treatment star

00:08:53,760 --> 00:08:58,970
yeah when this experiment runs or

00:08:56,220 --> 00:09:02,700
afterwards if you can reduce them and

00:08:58,970 --> 00:09:06,750
they also can be time resolved so for

00:09:02,700 --> 00:09:09,210
example here we see we have the with two

00:09:06,750 --> 00:09:13,260
different entities here and we can

00:09:09,210 --> 00:09:15,150
represent that kpi's in such a table so

00:09:13,260 --> 00:09:19,110
we have the variants that were exposed

00:09:15,150 --> 00:09:21,870
and this is like a time resolution that

00:09:19,110 --> 00:09:26,120
can be in days or months or hours or

00:09:21,870 --> 00:09:30,590
whatever and then we have some kpi's i'm

00:09:26,120 --> 00:09:33,660
showing like revenue or whatever other

00:09:30,590 --> 00:09:40,110
metric you want to have for your

00:09:33,660 --> 00:09:43,160
analyses so if you want to load that

00:09:40,110 --> 00:09:46,050
into x-band we want to create an

00:09:43,160 --> 00:09:48,900
experiment object and this is really

00:09:46,050 --> 00:09:51,780
just a one liner in Python code so what

00:09:48,900 --> 00:09:54,750
we provide is the baseline variant this

00:09:51,780 --> 00:09:57,630
is where against we do all the analysis

00:09:54,750 --> 00:10:01,110
like the existing system for example

00:09:57,630 --> 00:10:03,510
when we when we do some changes then the

00:10:01,110 --> 00:10:05,580
metrics which are the KPI and the

00:10:03,510 --> 00:10:08,970
features and the third thing is the

00:10:05,580 --> 00:10:11,220
metadata and when you execute that you

00:10:08,970 --> 00:10:14,070
will see that some initial checks are

00:10:11,220 --> 00:10:19,650
performed and then you most likely will

00:10:14,070 --> 00:10:22,020
get an output let's s summary of what

00:10:19,650 --> 00:10:24,420
you put in like here we have my fancy

00:10:22,020 --> 00:10:27,630
experiment from the metadata which has

00:10:24,420 --> 00:10:31,530
two features for KPIs and the primary

00:10:27,630 --> 00:10:36,240
KPIs revenue we have like a million

00:10:31,530 --> 00:10:43,860
entities here and two variants where a

00:10:36,240 --> 00:10:47,100
is the yeah the baseline variant okay so

00:10:43,860 --> 00:10:49,440
let me talk about the analysis we are

00:10:47,100 --> 00:10:53,910
currently providing we have three

00:10:49,440 --> 00:10:57,470
different kinds of yeah basic analysis

00:10:53,910 --> 00:11:03,360
that we currently include in the library

00:10:57,470 --> 00:11:07,400
the first one is maybe the most common

00:11:03,360 --> 00:11:10,200
analysis when you think about a/b tests

00:11:07,400 --> 00:11:16,770
identify significant differences between

00:11:10,200 --> 00:11:18,720
the different variants so what you what

00:11:16,770 --> 00:11:23,040
you experience when you perform such an

00:11:18,720 --> 00:11:26,370
experiment is that for two variants like

00:11:23,040 --> 00:11:31,070
a and B the red one and blue one and you

00:11:26,370 --> 00:11:38,160
will encounter yet a distribution of

00:11:31,070 --> 00:11:40,200
values for for the four KPI so what we

00:11:38,160 --> 00:11:43,590
are interested in here is the difference

00:11:40,200 --> 00:11:46,230
between those two so the Delta KPI and

00:11:43,590 --> 00:11:49,860
we want to find out if this is

00:11:46,230 --> 00:11:52,650
significantly different from zero so we

00:11:49,860 --> 00:11:56,550
have some variance of course in this KPI

00:11:52,650 --> 00:11:58,560
for each of the two variances and we

00:11:56,550 --> 00:12:02,250
want to perform a statistical test that

00:11:58,560 --> 00:12:05,310
shows that and we can specify then

00:12:02,250 --> 00:12:07,260
confident thresholds there so that we

00:12:05,310 --> 00:12:09,210
can say for example we've won a

00:12:07,260 --> 00:12:13,850
ninety-five percent certain that the

00:12:09,210 --> 00:12:17,310
observed the Delta M is within an

00:12:13,850 --> 00:12:21,020
interval and what comes out of that is

00:12:17,310 --> 00:12:24,420
something like this so we have here the

00:12:21,020 --> 00:12:28,470
observed Delta and a confidence interval

00:12:24,420 --> 00:12:30,960
and if it looks like this so the you

00:12:28,470 --> 00:12:33,660
observe and the interval is way above

00:12:30,960 --> 00:12:38,870
the zero line then any we are good and

00:12:33,660 --> 00:12:42,960
then it's significant difference and you

00:12:38,870 --> 00:12:48,420
new version of variance is working

00:12:42,960 --> 00:12:52,200
better than the baseline variant and it

00:12:48,420 --> 00:12:54,540
looks like this then it crosses the

00:12:52,200 --> 00:12:59,100
boundary and then we cannot or such a

00:12:54,540 --> 00:13:01,920
conclusion so this is the basic Delta

00:12:59,100 --> 00:13:07,850
and you can do that with the experiment

00:13:01,920 --> 00:13:11,430
object executing the Delta method

00:13:07,850 --> 00:13:14,190
there's a second analysis that we

00:13:11,430 --> 00:13:16,800
provide is the trend analysis and then

00:13:14,190 --> 00:13:19,990
here we add in another dimension which

00:13:16,800 --> 00:13:25,750
is time so

00:13:19,990 --> 00:13:30,010
for the delta that we have we provide at

00:13:25,750 --> 00:13:33,209
the time resolution and also so they

00:13:30,010 --> 00:13:37,330
existed how the delta progresses and the

00:13:33,209 --> 00:13:40,540
corresponding confidence interval and we

00:13:37,330 --> 00:13:42,310
can use that for recognizing patterns in

00:13:40,540 --> 00:13:45,640
the data over time like seasonal

00:13:42,310 --> 00:13:48,990
patterns for example but also recency

00:13:45,640 --> 00:13:53,190
effects or long-term effects of that

00:13:48,990 --> 00:13:59,130
yeah the treatment that we are doing and

00:13:53,190 --> 00:14:01,089
the third analysis is to quantify the

00:13:59,130 --> 00:14:03,190
differential treatment effects of

00:14:01,089 --> 00:14:07,180
subgroups so this is about digging into

00:14:03,190 --> 00:14:09,700
the data and I'm identifying yeah

00:14:07,180 --> 00:14:12,940
customer groups that might perform

00:14:09,700 --> 00:14:15,250
better or worse and yet we have a

00:14:12,940 --> 00:14:20,880
breakdown according to the features that

00:14:15,250 --> 00:14:26,020
we also provide and and here for example

00:14:20,880 --> 00:14:28,450
and you can see we are doing this

00:14:26,020 --> 00:14:32,470
subgroup analysis on the future age and

00:14:28,450 --> 00:14:37,770
it was and we have a pinning algorithm

00:14:32,470 --> 00:14:41,980
in our library that can and split this

00:14:37,770 --> 00:14:44,620
continuous variable into chunks so we

00:14:41,980 --> 00:14:48,370
have a young customer group here from 18

00:14:44,620 --> 00:14:51,070
to 24 years and we have an yeah oldham

00:14:48,370 --> 00:14:54,310
group here above twenty four years and

00:14:51,070 --> 00:14:56,920
we see that for this KPI that we are

00:14:54,310 --> 00:15:00,040
investigating in this example it works

00:14:56,920 --> 00:15:03,700
really well for the young customers but

00:15:00,040 --> 00:15:05,529
not for the older ones and you can use

00:15:03,700 --> 00:15:10,000
such insights for example to do

00:15:05,529 --> 00:15:13,480
personalization and just expose your

00:15:10,000 --> 00:15:21,880
changes or your new variant to subgroup

00:15:13,480 --> 00:15:26,980
of your customers so the third thing i'd

00:15:21,880 --> 00:15:30,339
like to go into is the result and here

00:15:26,980 --> 00:15:33,910
we have a result object that is a really

00:15:30,339 --> 00:15:36,670
generic structure for all the analysis

00:15:33,910 --> 00:15:41,230
we provide currently with the experiment

00:15:36,670 --> 00:15:46,030
object and um it looks like this it's

00:15:41,230 --> 00:15:48,610
based on a panel data frame and there's

00:15:46,030 --> 00:15:53,050
also the metadata included but I show

00:15:48,610 --> 00:15:55,690
here only the data frame and so we have

00:15:53,050 --> 00:15:57,910
the metrics here in the first column and

00:15:55,690 --> 00:16:03,190
you can imagine that you can have

00:15:57,910 --> 00:16:08,680
different metrics as well and and what

00:16:03,190 --> 00:16:11,440
we can see is here we we show the sample

00:16:08,680 --> 00:16:14,700
sizes for weary and a we have thousand

00:16:11,440 --> 00:16:18,130
for variant p we have five thousand and

00:16:14,700 --> 00:16:26,140
also the variant meaning result for this

00:16:18,130 --> 00:16:30,340
kpi and which is 20 and 20 3.5 and this

00:16:26,140 --> 00:16:33,820
makes an uplift of 3.5 and we can also

00:16:30,340 --> 00:16:38,380
show here the confidence intervals here

00:16:33,820 --> 00:16:40,900
is the 2.5 and 97.5 centered but you can

00:16:38,380 --> 00:16:45,460
of course add additional percentage to

00:16:40,900 --> 00:16:50,890
really display a distribution and so

00:16:45,460 --> 00:16:54,760
this is for the basic Delta and for the

00:16:50,890 --> 00:16:58,720
subgroup analysis you can imagine just

00:16:54,760 --> 00:17:00,520
to copy that whole table over and show

00:16:58,720 --> 00:17:03,370
that for the different subgroups like

00:17:00,520 --> 00:17:06,580
for age in subgroup 20 to 30 you have a

00:17:03,370 --> 00:17:08,200
similar analysis than that and you can

00:17:06,580 --> 00:17:13,710
have that for different subgroups of

00:17:08,200 --> 00:17:15,970
course and you can also have that for

00:17:13,710 --> 00:17:20,530
different time points for the trend

00:17:15,970 --> 00:17:23,590
analysis so that's basically it for the

00:17:20,530 --> 00:17:27,900
result structure and to put it in a

00:17:23,590 --> 00:17:32,620
nutshell and we have just released

00:17:27,900 --> 00:17:36,430
expand on github as a new library for

00:17:32,620 --> 00:17:38,830
the analysis of a/b tests we provide

00:17:36,430 --> 00:17:42,270
some standardized input output formats

00:17:38,830 --> 00:17:45,320
so far and a basic battery of

00:17:42,270 --> 00:17:48,320
statistical analysis methods

00:17:45,320 --> 00:17:52,039
but this is really part of yeah Solano's

00:17:48,320 --> 00:17:55,850
open source strategy so yeah this is an

00:17:52,039 --> 00:17:58,850
early release and we want to want to put

00:17:55,850 --> 00:18:03,350
in a lot of more stuff and you are more

00:17:58,850 --> 00:18:06,889
than invited to contribute and the

00:18:03,350 --> 00:18:10,429
library currently uses parametric

00:18:06,889 --> 00:18:12,350
statistics but also if your data is not

00:18:10,429 --> 00:18:16,039
normally distributed you can use

00:18:12,350 --> 00:18:18,440
bootstrapping and you can use it as a

00:18:16,039 --> 00:18:21,379
human in an eye Python notebook for

00:18:18,440 --> 00:18:23,330
example but also in a micro service so

00:18:21,379 --> 00:18:27,679
if you want to run that in the back end

00:18:23,330 --> 00:18:32,090
and that's totally fine and as I said

00:18:27,679 --> 00:18:34,610
available on github and pi PI so next

00:18:32,090 --> 00:18:36,889
steps are for example we're working on

00:18:34,610 --> 00:18:41,200
the visualization and we are thinking

00:18:36,889 --> 00:18:41,200
about putting some patients tatiseigi

00:18:43,299 --> 00:18:53,809
come so that's probably my last slide

00:18:50,600 --> 00:18:57,110
but i also have a python notebook that i

00:18:53,809 --> 00:19:01,279
want to show you and just to get a bit

00:18:57,110 --> 00:19:04,490
more of the code but let me first finish

00:19:01,279 --> 00:19:06,799
that up so this is again the URL if you

00:19:04,490 --> 00:19:10,070
want to reach out i'm here at the

00:19:06,799 --> 00:19:13,610
conference and Zhi Bao is also here at

00:19:10,070 --> 00:19:17,960
the conference and so please come to us

00:19:13,610 --> 00:19:20,450
talk to us send us an email and again I

00:19:17,960 --> 00:19:23,690
with the company only for three months

00:19:20,450 --> 00:19:26,929
so I did only a tiny bit of the work

00:19:23,690 --> 00:19:29,899
that I've i'm showing you here and these

00:19:26,929 --> 00:19:34,789
people all contributed i guess a lot

00:19:29,899 --> 00:19:38,870
more than me but yeah so then i'd like

00:19:34,789 --> 00:19:45,470
to switch over to the to the notebook

00:19:38,870 --> 00:19:47,950
and this is by the way also i'm on

00:19:45,470 --> 00:19:47,950
github

00:19:50,750 --> 00:20:00,630
so you can just download that and yeah

00:19:57,830 --> 00:20:03,809
walkthrough by yourself if you wanna if

00:20:00,630 --> 00:20:07,880
you are interested so can also install

00:20:03,809 --> 00:20:11,760
it simply with pip install of course and

00:20:07,880 --> 00:20:15,630
and to provide some data so first of all

00:20:11,760 --> 00:20:20,780
we we generate some random data here to

00:20:15,630 --> 00:20:23,400
to just show a few things of the library

00:20:20,780 --> 00:20:26,730
this looks like this so we have quite a

00:20:23,400 --> 00:20:29,850
number of metrics here and they are

00:20:26,730 --> 00:20:33,270
normally distributed and shifted things

00:20:29,850 --> 00:20:36,750
like that and we provide the metadata as

00:20:33,270 --> 00:20:40,170
a Python dictionary by the way yeah this

00:20:36,750 --> 00:20:46,950
is just upon us data frame none here we

00:20:40,170 --> 00:20:51,330
have a Python dictionary and so what we

00:20:46,950 --> 00:20:54,650
can do now is as I have seen on the

00:20:51,330 --> 00:20:58,620
slide if you've seen on the slide and

00:20:54,650 --> 00:21:03,440
just execute or create an experiment

00:20:58,620 --> 00:21:06,990
object where is it yep like this and

00:21:03,440 --> 00:21:09,900
just the baseline very end the data and

00:21:06,990 --> 00:21:13,380
the metadata and then we can use that

00:21:09,900 --> 00:21:16,140
for all different kinds of analytics we

00:21:13,380 --> 00:21:20,100
want to do for example here the Delta

00:21:16,140 --> 00:21:23,670
for the KPI normally equal unequal

00:21:20,100 --> 00:21:26,090
variance and it will output something

00:21:23,670 --> 00:21:31,470
like I've shown on the slide this

00:21:26,090 --> 00:21:34,620
results table and and yeah we can also

00:21:31,470 --> 00:21:37,200
do that with bootstrapping if we just

00:21:34,620 --> 00:21:40,290
provide the parameter assume normal is

00:21:37,200 --> 00:21:43,650
false and then the results are in this

00:21:40,290 --> 00:21:51,540
case pretty much the same but it takes

00:21:43,650 --> 00:21:54,150
longer and also here is yeah the results

00:21:51,540 --> 00:21:56,610
off of the whole different kinds of

00:21:54,150 --> 00:22:01,149
metrics that we are currently in the

00:21:56,610 --> 00:22:05,950
half in this simulated data set and

00:22:01,149 --> 00:22:12,070
and we also have an example of the

00:22:05,950 --> 00:22:14,409
subgroup analysis here so I guess that's

00:22:12,070 --> 00:22:19,299
it thank you very much for your

00:22:14,409 --> 00:22:40,929
attention and I'm glad to answer

00:22:19,299 --> 00:22:43,359
questions maybe bio can wave his hand so

00:22:40,929 --> 00:22:49,299
that people can reach out to him after

00:22:43,359 --> 00:22:52,929
the session so thank you for the

00:22:49,299 --> 00:22:55,389
presentation my question is do you have

00:22:52,929 --> 00:22:57,849
any possibility of persisting this data

00:22:55,389 --> 00:23:03,129
or is it just on demand when you want a

00:22:57,849 --> 00:23:05,469
report to be generated I'm not sure if I

00:23:03,129 --> 00:23:07,899
got the question right so what do you

00:23:05,469 --> 00:23:10,719
mean by possibility to process this

00:23:07,899 --> 00:23:13,629
litter cysts like stories in a database

00:23:10,719 --> 00:23:16,029
or anything do you do this for like

00:23:13,629 --> 00:23:19,509
long-term analysis or something like

00:23:16,029 --> 00:23:24,969
that I'm sure we can just store that

00:23:19,509 --> 00:23:28,359
data in files or databases wherever you

00:23:24,969 --> 00:23:31,779
want it so it's it's a panel data frame

00:23:28,359 --> 00:23:35,710
in principle and a Python dictionary so

00:23:31,779 --> 00:23:38,379
yeah it's really easy to to yeah have

00:23:35,710 --> 00:23:42,399
that stored but it's not into integrated

00:23:38,379 --> 00:23:45,009
um yeah it's partly integrated so I

00:23:42,399 --> 00:23:48,089
think we can we can write into file

00:23:45,009 --> 00:23:54,059
easily from from pandas for example and

00:23:48,089 --> 00:23:57,580
yeah but I think if you have your custom

00:23:54,059 --> 00:24:00,399
database at your business then yeah you

00:23:57,580 --> 00:24:03,339
want to include something that's very

00:24:00,399 --> 00:24:05,440
specific to your your business and then

00:24:03,339 --> 00:24:07,929
you want to just implement that on your

00:24:05,440 --> 00:24:10,059
own but I guess it's pretty easy and

00:24:07,929 --> 00:24:12,039
maybe a second question about

00:24:10,059 --> 00:24:15,729
scalability so

00:24:12,039 --> 00:24:20,649
what what scenarios have you tried like

00:24:15,729 --> 00:24:24,700
how much data for how long okay very

00:24:20,649 --> 00:24:27,149
good question so I'm with the company

00:24:24,700 --> 00:24:30,429
for very short time so I'm not really

00:24:27,149 --> 00:24:34,019
super experience but it's like we are

00:24:30,429 --> 00:24:38,440
running big tests with that currently

00:24:34,019 --> 00:24:43,119
there are on our website and yeah this

00:24:38,440 --> 00:24:46,359
really scales in that sense but I think

00:24:43,119 --> 00:24:49,269
a key point is here that a lot of the

00:24:46,359 --> 00:24:52,419
data that you gather and can be

00:24:49,269 --> 00:24:56,289
aggregated for these tests because you

00:24:52,419 --> 00:25:00,119
yeah probably depending on your

00:24:56,289 --> 00:25:04,359
experiment and don't want to include all

00:25:00,119 --> 00:25:07,059
tracking data that you you have from a

00:25:04,359 --> 00:25:11,440
user on the website for example so maybe

00:25:07,059 --> 00:25:15,340
it's just that you want to have one

00:25:11,440 --> 00:25:19,029
event if a person converges that was

00:25:15,340 --> 00:25:22,899
exposed to experiment or not and so this

00:25:19,029 --> 00:25:25,619
is not really usually a huge amount of

00:25:22,899 --> 00:25:36,369
data so that scales well in this terms

00:25:25,619 --> 00:25:39,309
okay thank you hello and so a quick

00:25:36,369 --> 00:25:41,979
question does does your library contain

00:25:39,309 --> 00:25:43,869
any consistency check support for

00:25:41,979 --> 00:25:46,749
example if two experiments run at the

00:25:43,869 --> 00:25:48,549
same time and we think that the metrics

00:25:46,749 --> 00:25:51,609
are not didn't affect a very independent

00:25:48,549 --> 00:25:53,409
but in reality at least on websites but

00:25:51,609 --> 00:25:55,570
I don't know many experiments that are

00:25:53,409 --> 00:25:58,559
completely independent a click can

00:25:55,570 --> 00:26:02,769
affect something else so are there any

00:25:58,559 --> 00:26:05,799
checks in that respect or some

00:26:02,769 --> 00:26:09,549
statistical methodologies that yeah as

00:26:05,799 --> 00:26:13,059
we currently include checks about the

00:26:09,549 --> 00:26:16,720
features so that the different variants

00:26:13,059 --> 00:26:21,759
are independent for example and and we

00:26:16,720 --> 00:26:25,210
also and yeah have things planned to

00:26:21,759 --> 00:26:27,490
check if different experiments

00:26:25,210 --> 00:26:32,320
interfere with each other but this is

00:26:27,490 --> 00:26:43,899
currently not in the library yet thank

00:26:32,320 --> 00:26:47,980
you any more questions oh sure okay so

00:26:43,899 --> 00:26:51,390
one more than do you there are

00:26:47,980 --> 00:26:55,270
situations where Iran experiment and the

00:26:51,390 --> 00:26:57,640
amount of change is so small that the

00:26:55,270 --> 00:26:59,620
control and the treatment kind of they

00:26:57,640 --> 00:27:02,200
don't show any significant difference

00:26:59,620 --> 00:27:05,830
but there is when there is this very

00:27:02,200 --> 00:27:07,809
very small change is that capture by I'm

00:27:05,830 --> 00:27:10,539
talking about trigger logging if you

00:27:07,809 --> 00:27:13,600
know what I'm like triggering triggering

00:27:10,539 --> 00:27:15,909
like it's an event that happens 10 / no

00:27:13,600 --> 00:27:17,260
not in person 0.5 percent of the cases

00:27:15,909 --> 00:27:19,299
but we think that zero point five

00:27:17,260 --> 00:27:21,940
percent of the cases it affects the

00:27:19,299 --> 00:27:25,090
results so you don't see it often but

00:27:21,940 --> 00:27:27,070
when you see it it's significant and I

00:27:25,090 --> 00:27:30,490
don't know if that's captured by I mean

00:27:27,070 --> 00:27:36,490
libraries that are generally i generally

00:27:30,490 --> 00:27:39,070
built on okay so i'm not quite true if i

00:27:36,490 --> 00:27:41,890
can answer that correctly but i think

00:27:39,070 --> 00:27:44,440
and we have no special mechanism

00:27:41,890 --> 00:27:48,130
currently to deal with that in the

00:27:44,440 --> 00:27:51,370
library and you can try maybe to use the

00:27:48,130 --> 00:27:54,100
subgroup analysis for that if you can

00:27:51,370 --> 00:27:56,529
really specify that as a feature when

00:27:54,100 --> 00:27:59,460
this event happens and then you don't

00:27:56,529 --> 00:28:03,399
have to investigate your whole an

00:27:59,460 --> 00:28:07,179
experiment based just to see some small

00:28:03,399 --> 00:28:11,260
effect that is only yeah interesting for

00:28:07,179 --> 00:28:14,640
a specific subgroup yeah but if you have

00:28:11,260 --> 00:28:17,279
a good algorithm for that or any idea

00:28:14,640 --> 00:28:22,450
you're more than welcome to contribute

00:28:17,279 --> 00:28:27,120
thank you we have one more question

00:28:22,450 --> 00:28:27,120
probably the last one put the session

00:28:29,220 --> 00:28:36,090
I thanks for your presentation just

00:28:33,870 --> 00:28:39,450
python has a lot of very good modules

00:28:36,090 --> 00:28:41,789
for statistical analysis and machine

00:28:39,450 --> 00:28:44,039
learning and so on so I was curious how

00:28:41,789 --> 00:28:48,059
much did you have to call yourself and

00:28:44,039 --> 00:28:53,190
what libraries did you use and yeah

00:28:48,059 --> 00:28:55,350
great question I think yeah you can

00:28:53,190 --> 00:29:00,690
download the code of course and look at

00:28:55,350 --> 00:29:03,809
it so we are using syfy a numpy and that

00:29:00,690 --> 00:29:08,480
i really like convenient to do these

00:29:03,809 --> 00:29:11,010
things and they also include basic

00:29:08,480 --> 00:29:13,409
statistical tests I for example like

00:29:11,010 --> 00:29:17,039
choice square tests and that we are

00:29:13,409 --> 00:29:19,860
using and but this is the library is

00:29:17,039 --> 00:29:23,010
more like the framework around and so

00:29:19,860 --> 00:29:27,960
that we have these data structures that

00:29:23,010 --> 00:29:31,200
we yeah can really like provide analyses

00:29:27,960 --> 00:29:33,780
specific for these AP tests that we want

00:29:31,200 --> 00:29:38,460
to do and this becomes then quite some

00:29:33,780 --> 00:29:40,980
code and yeah there are a lot of things

00:29:38,460 --> 00:29:44,220
planned and also in the library that are

00:29:40,980 --> 00:29:47,730
not covered by these basic libraries

00:29:44,220 --> 00:29:51,240
that are available in Python but yeah we

00:29:47,730 --> 00:29:53,789
want to of course use saipin Empire

00:29:51,240 --> 00:30:01,200
stuff as much as possible and also panos

00:29:53,789 --> 00:30:04,890
is part of what we are using do you want

00:30:01,200 --> 00:30:08,090
to take one more question any any more

00:30:04,890 --> 00:30:08,090
questions all right

00:30:12,299 --> 00:30:19,899
hi you show that the software was about

00:30:16,960 --> 00:30:23,080
analysis of the AP test however usually

00:30:19,899 --> 00:30:26,379
the analysis is also input for defining

00:30:23,080 --> 00:30:28,869
the new ib tests for instance how long

00:30:26,379 --> 00:30:32,619
should a test be run especially on your

00:30:28,869 --> 00:30:34,450
size with so much diversity is that

00:30:32,619 --> 00:30:37,359
something you guys do in a different

00:30:34,450 --> 00:30:39,279
framework or could you tell me something

00:30:37,359 --> 00:30:42,700
about setting up the new ib tests for

00:30:39,279 --> 00:30:47,080
tomorrow mmm okay yeah so I think there

00:30:42,700 --> 00:30:50,639
are different questions one is of course

00:30:47,080 --> 00:30:55,690
how to come up with new good hypotheses

00:30:50,639 --> 00:30:57,399
and this is yeah something where for

00:30:55,690 --> 00:31:00,489
example the subgroup analysis provide

00:30:57,399 --> 00:31:02,919
some insights maybe but yeah there they

00:31:00,489 --> 00:31:06,009
go in a lot of other things and a lot of

00:31:02,919 --> 00:31:09,359
domain knowledge just to set up the next

00:31:06,009 --> 00:31:13,210
good test and the other question is

00:31:09,359 --> 00:31:16,710
about sample size I guess a sample size

00:31:13,210 --> 00:31:18,909
duration of the test yeah so that's

00:31:16,710 --> 00:31:21,009
another thing that's currently not

00:31:18,909 --> 00:31:23,470
completely covered with our library we

00:31:21,009 --> 00:31:27,759
are doing that currently and yeah

00:31:23,470 --> 00:31:30,580
manually or with other tools and yeah

00:31:27,759 --> 00:31:33,279
but may this is maybe a good thing to

00:31:30,580 --> 00:31:34,960
include next future yeah because the the

00:31:33,279 --> 00:31:37,859
analysis framework know so much about

00:31:34,960 --> 00:31:39,909
previous experiments and therefore these

00:31:37,859 --> 00:31:42,580
results can give you some input about

00:31:39,909 --> 00:31:44,229
the new experiments hmm or do you wait

00:31:42,580 --> 00:31:49,539
until it converges and then stop the

00:31:44,229 --> 00:31:51,249
test sometimes people just stop the test

00:31:49,539 --> 00:31:58,739
when it converges when they like the

00:31:51,249 --> 00:31:58,739
output and they stop yep okay okay

00:32:02,160 --> 00:32:07,790
thank you so much Dominic thank you very

00:32:05,160 --> 00:32:07,790

YouTube URL: https://www.youtube.com/watch?v=5q8RNX9Mbhs


