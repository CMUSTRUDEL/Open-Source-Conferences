Title: Berlin Buzzwords 2016: Frank Conrad & Ranbir Chawla - The world wide 60B transaction per day journey
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	A case study on how we grew the AudienceScience Helios programmatic advertising management system to more than 1 Million transactions per second through our entire data pipeline including Kafka, Storm and Hadoop.

The AudienceScience Helios system acts as:
A real time bidding client to all major exchanges (more than 70 integrations world wide) A service for direct integration with publisher’s website / ad server A engine data collection and management of billions of users The system runs world wide across 6 global data centers.

We’ll share what we learned along our journey and how we are moving to even greater scale in the Cloud for 2016.

Read more:
https://2016.berlinbuzzwords.de/session/world-wide-60-billion-transaction-day-journey

About Frank Conrad:
https://2016.berlinbuzzwords.de/users/frank-conrad

About Ranbir Chawla:
https://2016.berlinbuzzwords.de/users/ranbir-chawla

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:02,720 --> 00:00:07,020
okay good well we're not doing a demo

00:00:05,190 --> 00:00:09,179
today so we got all the technical

00:00:07,020 --> 00:00:10,440
difficulties out at the beginning my

00:00:09,179 --> 00:00:13,309
name is Rob your towel on the vp of

00:00:10,440 --> 00:00:15,719
engineering at audience science yeah I'm

00:00:13,309 --> 00:00:20,279
Francona dimes a sweet tea file she

00:00:15,719 --> 00:00:22,380
ticked off audience signs and a little

00:00:20,279 --> 00:00:23,670
bit of background on audience science or

00:00:22,380 --> 00:00:25,350
a fully integrated end-to-end

00:00:23,670 --> 00:00:27,539
advertising solution for some of the

00:00:25,350 --> 00:00:29,539
world's largest digital advertisers we

00:00:27,539 --> 00:00:34,620
process and respond to 80 billion

00:00:29,539 --> 00:00:36,149
advertising requests a day and we do

00:00:34,620 --> 00:00:39,540
this in over 42 countries worldwide

00:00:36,149 --> 00:00:41,190
including China our solution allows

00:00:39,540 --> 00:00:42,570
advertisers to effectively manage and

00:00:41,190 --> 00:00:45,780
leverage their consumer data to produce

00:00:42,570 --> 00:00:47,100
industry-leading roi we have five points

00:00:45,780 --> 00:00:53,219
and presents two in the United States

00:00:47,100 --> 00:00:54,570
one in Europe to in a pack yep when we

00:00:53,219 --> 00:00:57,300
wanted to talk to you a little bit about

00:00:54,570 --> 00:01:00,300
today was just the challenges of doing

00:00:57,300 --> 00:01:03,000
what we do at such a large scale I'll

00:01:00,300 --> 00:01:06,720
hand it over to Frank yeah and basically

00:01:03,000 --> 00:01:09,210
we started 2014 with 20 billion right

00:01:06,720 --> 00:01:11,430
now 80 billion so it's a lot growing

00:01:09,210 --> 00:01:15,330
forward and we had a lot of challenges

00:01:11,430 --> 00:01:19,200
with it and it's really about the

00:01:15,330 --> 00:01:23,850
details if you're if you look to a

00:01:19,200 --> 00:01:26,670
Formula One car it's every screw matters

00:01:23,850 --> 00:01:29,610
if you do the wrong stuff in the engine

00:01:26,670 --> 00:01:32,970
will die or something like that and it's

00:01:29,610 --> 00:01:37,380
really the things about that every step

00:01:32,970 --> 00:01:39,299
must must and must be working and you

00:01:37,380 --> 00:01:41,970
are at the starting time always looks to

00:01:39,299 --> 00:01:47,119
the complex problem to the heavy things

00:01:41,970 --> 00:01:51,119
work what you have to do but the things

00:01:47,119 --> 00:01:54,110
what will kill you probably are the more

00:01:51,119 --> 00:01:58,979
trivial stuff more basic stuff or things

00:01:54,110 --> 00:02:07,759
which you start thinking about later too

00:01:58,979 --> 00:02:07,759
late and so sorry

00:02:10,710 --> 00:02:18,190
the things are what I really want to say

00:02:15,880 --> 00:02:21,130
you have to start with the right data

00:02:18,190 --> 00:02:23,710
model what is the things what you do you

00:02:21,130 --> 00:02:28,750
have to think about as the scale you

00:02:23,710 --> 00:02:32,560
need does does your data model allows

00:02:28,750 --> 00:02:36,580
the sufficient parallelism to do this

00:02:32,560 --> 00:02:40,300
store store your data in a way that you

00:02:36,580 --> 00:02:43,270
can effectively use it many times you

00:02:40,300 --> 00:02:45,570
start our stores the data in a simple

00:02:43,270 --> 00:02:48,100
way but then the usage get very very

00:02:45,570 --> 00:02:54,670
expensive and you have a lot of problems

00:02:48,100 --> 00:02:56,620
with it and another thing of what you

00:02:54,670 --> 00:03:00,850
really need to care about is your out

00:02:56,620 --> 00:03:03,130
layers so the standard stuff typically

00:03:00,850 --> 00:03:06,100
you get relatively good done what you

00:03:03,130 --> 00:03:08,500
kill you is your out layers users which

00:03:06,100 --> 00:03:11,530
have ten thousand impressions for

00:03:08,500 --> 00:03:14,320
whatever reasons or your data records

00:03:11,530 --> 00:03:16,900
get huge for only a couple of users but

00:03:14,320 --> 00:03:20,080
this this big users basically killed

00:03:16,900 --> 00:03:23,050
kill your process because it needs too

00:03:20,080 --> 00:03:28,390
much memory and then then your jvm dies

00:03:23,050 --> 00:03:31,090
are you have a stream processing and

00:03:28,390 --> 00:03:33,010
certain users take too much cpu times

00:03:31,090 --> 00:03:35,620
and because of the synchronization what

00:03:33,010 --> 00:03:40,660
you internally have it delays everything

00:03:35,620 --> 00:03:43,800
so such a things is is really really the

00:03:40,660 --> 00:03:48,610
stuff at productions what you need to

00:03:43,800 --> 00:03:51,640
care care about and then also how to

00:03:48,610 --> 00:03:58,019
design things think about how you add

00:03:51,640 --> 00:04:02,140
things on on top of it because sing

00:03:58,019 --> 00:04:04,870
requirements from from production from a

00:04:02,140 --> 00:04:08,380
product will become put more and more

00:04:04,870 --> 00:04:11,980
stuff on it and if you not think a

00:04:08,380 --> 00:04:15,580
little bit ahead you get at that scale

00:04:11,980 --> 00:04:18,130
really really problems and then what

00:04:15,580 --> 00:04:20,169
many guys also forget it's easy to

00:04:18,130 --> 00:04:20,620
create data or it's a challenge to

00:04:20,169 --> 00:04:23,199
create

00:04:20,620 --> 00:04:26,169
but it's a much bigger challenge to

00:04:23,199 --> 00:04:28,810
delete and expire data so so a lot of

00:04:26,169 --> 00:04:32,800
databases what you have for example

00:04:28,810 --> 00:04:38,350
user-oriented stuff and so on if you do

00:04:32,800 --> 00:04:41,080
a data expiring not by design in a in a

00:04:38,350 --> 00:04:43,300
good way you will really suffer this

00:04:41,080 --> 00:04:45,940
cleanup jobs which runs over your

00:04:43,300 --> 00:04:50,199
database will kill your performance so

00:04:45,940 --> 00:04:53,620
so basically this is a very big learning

00:04:50,199 --> 00:04:57,660
that in in any data storage what you

00:04:53,620 --> 00:05:00,100
design dancing don't forget about

00:04:57,660 --> 00:05:04,330
deletion of data if you not have

00:05:00,100 --> 00:05:06,430
naturally time time-oriented data then

00:05:04,330 --> 00:05:09,610
it's easy but everything which is

00:05:06,430 --> 00:05:11,710
user-oriented or campaign oriented or

00:05:09,610 --> 00:05:14,530
whatever as a witch don't have a real

00:05:11,710 --> 00:05:22,180
time component in in it it gets

00:05:14,530 --> 00:05:25,930
problematic and also the important thing

00:05:22,180 --> 00:05:27,550
is one on scale as I said said before

00:05:25,930 --> 00:05:31,030
you have to have the right parallelism

00:05:27,550 --> 00:05:33,400
otherwise you will not get the amount of

00:05:31,030 --> 00:05:35,200
performance you need about dynamic

00:05:33,400 --> 00:05:37,539
parallelism think about dynamic

00:05:35,200 --> 00:05:40,289
parallelism what I mean the set is that

00:05:37,539 --> 00:05:44,110
you can scale up and scale down your

00:05:40,289 --> 00:05:46,300
your your runtime to the need what you

00:05:44,110 --> 00:05:48,400
have specially if you leverage the cloud

00:05:46,300 --> 00:05:50,949
or something like this it can start more

00:05:48,400 --> 00:05:53,349
service to your rush hour and low others

00:05:50,949 --> 00:05:56,110
down later again but if you don't have a

00:05:53,349 --> 00:05:58,120
designed in an interior system it will

00:05:56,110 --> 00:06:01,660
not work and you can't really leverage

00:05:58,120 --> 00:06:03,490
it think about really do as an Crone

00:06:01,660 --> 00:06:06,280
processing for everything what you

00:06:03,490 --> 00:06:09,699
couldn't where else is possible what you

00:06:06,280 --> 00:06:12,789
don't have to do in in in your real-time

00:06:09,699 --> 00:06:14,740
response for example to to a bit request

00:06:12,789 --> 00:06:19,210
or to request which comes from the user

00:06:14,740 --> 00:06:22,240
browser don't do it do it later then

00:06:19,210 --> 00:06:25,419
Zhen Zhen really look tools the needed

00:06:22,240 --> 00:06:28,030
latency and throughput because you can

00:06:25,419 --> 00:06:31,510
design systems with a low latency but

00:06:28,030 --> 00:06:33,969
and and don't have a high throughput and

00:06:31,510 --> 00:06:38,049
visa versa so you have to find you

00:06:33,969 --> 00:06:41,169
needed compromise for that then another

00:06:38,049 --> 00:06:44,199
thing really which do need to think

00:06:41,169 --> 00:06:47,289
about is eventual consistency this

00:06:44,199 --> 00:06:52,689
concept can you help you a lot in a huge

00:06:47,289 --> 00:06:58,029
distributed way we we have as as we said

00:06:52,689 --> 00:06:59,979
we have five data centers and in a world

00:06:58,029 --> 00:07:03,939
where we receive all the bit requests

00:06:59,979 --> 00:07:06,129
and user requests and if you want to

00:07:03,939 --> 00:07:08,889
have some consistency this is something

00:07:06,129 --> 00:07:12,039
which producing huge huge effort and has

00:07:08,889 --> 00:07:14,439
done so much value if you think about a

00:07:12,039 --> 00:07:18,189
little bit more eventual consistency you

00:07:14,439 --> 00:07:21,429
can solve a lot of problems but with

00:07:18,189 --> 00:07:25,499
much less resources then the other piece

00:07:21,429 --> 00:07:29,439
in in your design is really think about

00:07:25,499 --> 00:07:34,089
production-oriented this means have some

00:07:29,439 --> 00:07:37,360
dynamic limiting so if if you get more

00:07:34,089 --> 00:07:41,019
traffic or if your new code release has

00:07:37,360 --> 00:07:43,389
some problems or other things that you

00:07:41,019 --> 00:07:46,919
have a chance to limit the incoming

00:07:43,389 --> 00:07:50,769
requests do some straddling do some

00:07:46,919 --> 00:07:53,439
operations on ten percent of C requests

00:07:50,769 --> 00:07:56,199
is better than do nothing because your

00:07:53,439 --> 00:07:58,599
complete system get overloaded and do

00:07:56,199 --> 00:08:02,919
nothing's in it anymore so basically

00:07:58,599 --> 00:08:07,719
that is a big thing which helps you to

00:08:02,919 --> 00:08:14,979
survive and in certain situations think

00:08:07,719 --> 00:08:17,319
about how you can good do a good catch

00:08:14,979 --> 00:08:19,329
up so if you have a stream processing

00:08:17,319 --> 00:08:21,639
and it goes behind because of

00:08:19,329 --> 00:08:24,549
performance problems of hardware issues

00:08:21,639 --> 00:08:28,360
of whatever that you have ways of

00:08:24,549 --> 00:08:34,300
efficient ketchup processing which needs

00:08:28,360 --> 00:08:37,599
earth to you need to care about it's

00:08:34,300 --> 00:08:39,789
also how to handle unreliable networks

00:08:37,599 --> 00:08:43,809
so if you transfer data from one data

00:08:39,789 --> 00:08:46,120
center to the other you will have

00:08:43,809 --> 00:08:47,529
sometimes the problems that then your

00:08:46,120 --> 00:08:49,420
network doesn't

00:08:47,529 --> 00:08:52,120
doesn't work as your local network at

00:08:49,420 --> 00:08:54,970
home in a good ways that you have packet

00:08:52,120 --> 00:08:58,329
loss that you get unexpected Layton sees

00:08:54,970 --> 00:09:01,749
things and so on how you mitigate them

00:08:58,329 --> 00:09:03,819
in into production everything in your

00:09:01,749 --> 00:09:05,559
test environment will always work well

00:09:03,819 --> 00:09:07,930
because you don't have then typically

00:09:05,559 --> 00:09:09,939
network problems the network problems

00:09:07,930 --> 00:09:12,279
comes in in the real world and

00:09:09,939 --> 00:09:15,220
especially if you have to go to China

00:09:12,279 --> 00:09:19,509
and have the great firewall between that

00:09:15,220 --> 00:09:22,329
that is really something it a which can

00:09:19,509 --> 00:09:24,040
cause any possible network issue what

00:09:22,329 --> 00:09:28,269
you can think about and there will be

00:09:24,040 --> 00:09:31,540
happened because of Murphy and this is

00:09:28,269 --> 00:09:33,490
basically also the next piece in another

00:09:31,540 --> 00:09:36,399
direction of your heart where's your

00:09:33,490 --> 00:09:39,480
service think about that you have not i

00:09:36,399 --> 00:09:42,879
equal service you have some which have

00:09:39,480 --> 00:09:44,709
24 cause the others have certainty to

00:09:42,879 --> 00:09:47,319
cause they have a little bit more

00:09:44,709 --> 00:09:49,749
memories a slightly different cpu

00:09:47,319 --> 00:09:52,839
performance characteristics that your

00:09:49,749 --> 00:09:55,569
system still can can completely leverage

00:09:52,839 --> 00:09:58,480
them and not too slow service dear

00:09:55,569 --> 00:10:00,730
slower server in your cluster dictator

00:09:58,480 --> 00:10:04,120
the overall performance of the system

00:10:00,730 --> 00:10:09,339
it's easily to achieve such things if

00:10:04,120 --> 00:10:12,309
you do not not look too then as i said

00:10:09,339 --> 00:10:15,670
before it's really as the things about

00:10:12,309 --> 00:10:17,740
the out layers in terms of compute power

00:10:15,670 --> 00:10:22,480
what you need for the certain requests

00:10:17,740 --> 00:10:25,569
memory things and also look up time into

00:10:22,480 --> 00:10:28,509
databases or stores or what you have to

00:10:25,569 --> 00:10:30,850
do and see really most important stuff

00:10:28,509 --> 00:10:33,579
for production-oriented is monitor

00:10:30,850 --> 00:10:36,519
monitor monitor design in your

00:10:33,579 --> 00:10:40,740
application that you can monitor all all

00:10:36,519 --> 00:10:44,949
the critical values have some available

00:10:40,740 --> 00:10:47,889
to graph anna or something like this as

00:10:44,949 --> 00:10:51,240
well as you can leverage to a monitoring

00:10:47,889 --> 00:10:55,209
system like nagios where you can check

00:10:51,240 --> 00:10:58,570
certain thresholds and and to do

00:10:55,209 --> 00:11:02,470
alerting to it that is really something

00:10:58,570 --> 00:11:07,180
well like what I can must recommend and

00:11:02,470 --> 00:11:11,440
typically the development is forget the

00:11:07,180 --> 00:11:16,260
developers forget about monitoring but

00:11:11,440 --> 00:11:16,260
if you don't do it right you fly blind

00:11:16,770 --> 00:11:27,100
and the things what we do in end in

00:11:24,180 --> 00:11:30,520
monitoring is we use a simple tool

00:11:27,100 --> 00:11:33,580
called called nagios for that it's very

00:11:30,520 --> 00:11:36,970
known and and the reason why we use it

00:11:33,580 --> 00:11:40,240
is it's easy to script and it's easy to

00:11:36,970 --> 00:11:42,430
version control because if you go in a

00:11:40,240 --> 00:11:44,590
larger deployment where you have a lot

00:11:42,430 --> 00:11:47,320
of servers a lot of service instances

00:11:44,590 --> 00:11:51,790
which you need to check and you have to

00:11:47,320 --> 00:11:54,220
check every instance not a general one

00:11:51,790 --> 00:11:56,200
general to the load balancer you have to

00:11:54,220 --> 00:11:58,450
check every bitter behind the load

00:11:56,200 --> 00:12:02,200
balancers that they're behaving as you

00:11:58,450 --> 00:12:04,690
expected because because Murphy said

00:12:02,200 --> 00:12:08,410
that one JVM or one server get a problem

00:12:04,690 --> 00:12:11,920
and not all all all at the same same

00:12:08,410 --> 00:12:14,140
same time and that you can achieve this

00:12:11,920 --> 00:12:16,780
also on a larger scale is really neat

00:12:14,140 --> 00:12:19,960
that the configuration is done in a

00:12:16,780 --> 00:12:22,390
completely automated way in a also part

00:12:19,960 --> 00:12:25,930
of your deployment you have to see it as

00:12:22,390 --> 00:12:29,950
a part of your deployment of your total

00:12:25,930 --> 00:12:33,100
stuff and as I said that Hardware can

00:12:29,950 --> 00:12:35,620
can really result that your workflows in

00:12:33,100 --> 00:12:40,150
a cluster or your bitter response time

00:12:35,620 --> 00:12:42,450
get really bad and a partners like

00:12:40,150 --> 00:12:44,920
Google they're looking to the

00:12:42,450 --> 00:12:46,960
ninety-five percent percentile offer its

00:12:44,920 --> 00:12:53,050
once time and if you are not good in

00:12:46,960 --> 00:12:55,030
that 0l throttle you and another pic

00:12:53,050 --> 00:13:00,210
powder on and it is basically a

00:12:55,030 --> 00:13:03,820
functional monitoring so many many times

00:13:00,210 --> 00:13:07,090
guys using check proc and sees as a

00:13:03,820 --> 00:13:09,670
process which chooses drop so everything

00:13:07,090 --> 00:13:12,329
is fine this is something which is

00:13:09,670 --> 00:13:14,639
typically by far

00:13:12,329 --> 00:13:18,809
not sufficient you should really do

00:13:14,639 --> 00:13:21,360
implement functional checks implement a

00:13:18,809 --> 00:13:23,480
kind of l2 and monitoring that you

00:13:21,360 --> 00:13:26,339
simulate something at the front end and

00:13:23,480 --> 00:13:28,889
basically looking on every step on the

00:13:26,339 --> 00:13:32,309
whole chain until reporting everything

00:13:28,889 --> 00:13:34,499
that this data is showing up because was

00:13:32,309 --> 00:13:38,519
that you can make sure that your whole

00:13:34,499 --> 00:13:41,970
pipeline is working and you have to do

00:13:38,519 --> 00:13:45,470
this really on every instance because if

00:13:41,970 --> 00:13:48,929
one bidder or 18 server doesn't forward

00:13:45,470 --> 00:13:51,149
there's their data records to the

00:13:48,929 --> 00:13:53,730
reporting system you have some number

00:13:51,149 --> 00:13:58,410
discrepancies and you can only figure

00:13:53,730 --> 00:14:01,679
that out if you're if you have a check

00:13:58,410 --> 00:14:08,509
against each instance of your sort of

00:14:01,679 --> 00:14:08,509
your software what you are running and

00:14:09,559 --> 00:14:17,959
with end-to-end monitoring you do this

00:14:13,579 --> 00:14:21,329
you will see a lot of more problems

00:14:17,959 --> 00:14:24,179
earlier earlier as the customer see it

00:14:21,329 --> 00:14:28,949
if you make it properly you know the

00:14:24,179 --> 00:14:32,519
problems before the customer okay now

00:14:28,949 --> 00:14:39,660
now now we go a little bit to the dupe

00:14:32,519 --> 00:14:41,610
staff as a as busy we reprocess huge

00:14:39,660 --> 00:14:45,779
amount of data and how to do it right

00:14:41,610 --> 00:14:48,989
now or how we do it right now is really

00:14:45,779 --> 00:14:51,929
mainly MapReduce jobs which we are

00:14:48,989 --> 00:14:54,360
running two processes we have started a

00:14:51,929 --> 00:14:58,230
couple of years ago with 16 notes right

00:14:54,360 --> 00:15:02,730
now we are more than 500 notes we are

00:14:58,230 --> 00:15:08,899
running them right now based on a cloud

00:15:02,730 --> 00:15:12,629
era infrastructure and the thing is

00:15:08,899 --> 00:15:15,239
where are the pieces what we are learned

00:15:12,629 --> 00:15:18,089
learned learns there is really do as

00:15:15,239 --> 00:15:20,999
much as possible in the mapper face from

00:15:18,089 --> 00:15:22,619
from from the work minimize the shuffle

00:15:20,999 --> 00:15:25,880
data even if you have a very good

00:15:22,619 --> 00:15:29,600
Network 10-gigabit shuffling is

00:15:25,880 --> 00:15:33,140
so this this stuff we're mapreduce get

00:15:29,600 --> 00:15:35,390
can easily get inefficient and this is

00:15:33,140 --> 00:15:37,670
one of the things why spark and many

00:15:35,390 --> 00:15:43,010
times can be more more efficient because

00:15:37,670 --> 00:15:47,180
they do a clever way a better way in in

00:15:43,010 --> 00:15:49,940
traveling data around then then it has a

00:15:47,180 --> 00:15:53,120
big piece is the output of the drops

00:15:49,940 --> 00:15:57,380
must be in a format and in a in a way

00:15:53,120 --> 00:16:00,620
that also optimal for the next input so

00:15:57,380 --> 00:16:03,410
it doesn't fit that you're your actual

00:16:00,620 --> 00:16:05,480
job running well and produce a huge

00:16:03,410 --> 00:16:07,670
amount of small output files which end

00:16:05,480 --> 00:16:11,600
up sets the next job who's consuming

00:16:07,670 --> 00:16:17,480
this get a lot of mappers and you get

00:16:11,600 --> 00:16:20,540
there z.z inefficiencies and if you get

00:16:17,480 --> 00:16:23,180
more and more nodes many guys say oh

00:16:20,540 --> 00:16:25,610
have one big clusters and everything

00:16:23,180 --> 00:16:30,350
runs faster and every we run everything

00:16:25,610 --> 00:16:33,590
on one cluster it it's theoretically a

00:16:30,350 --> 00:16:37,430
good thing in a practice it's not so

00:16:33,590 --> 00:16:39,830
good because if this class i get for

00:16:37,430 --> 00:16:43,880
whatever reasons a problem you don't you

00:16:39,830 --> 00:16:46,850
don't have any cluster also if you want

00:16:43,880 --> 00:16:49,730
to upgrade things can you upgrade your

00:16:46,850 --> 00:16:51,760
whole software at one point typically

00:16:49,730 --> 00:16:54,280
not typically you have a group

00:16:51,760 --> 00:16:56,810
immigration process if you have mitral

00:16:54,280 --> 00:17:00,530
the stuff already split it across

00:16:56,810 --> 00:17:05,120
multiple clusters it helps you to to do

00:17:00,530 --> 00:17:08,240
this operational things in a in a much

00:17:05,120 --> 00:17:10,850
easier way what you should care about is

00:17:08,240 --> 00:17:14,540
that you can relatively easy move a note

00:17:10,850 --> 00:17:16,490
from one class I into the other things

00:17:14,540 --> 00:17:23,600
that the networking isn't this very

00:17:16,490 --> 00:17:26,870
transparent to to do such things as i

00:17:23,600 --> 00:17:30,650
said the optimization on on just drops

00:17:26,870 --> 00:17:33,590
itself is the runtime of each caste

00:17:30,650 --> 00:17:37,480
should be in a in a reason of a couple

00:17:33,590 --> 00:17:39,440
of minutes 55 minutes ago to obtain a

00:17:37,480 --> 00:17:42,460
good optimum

00:17:39,440 --> 00:17:47,000
the number of meta mapas entering

00:17:42,460 --> 00:17:51,879
reducers must be really another contest

00:17:47,000 --> 00:17:57,409
eration is how much shuffle you need

00:17:51,879 --> 00:18:02,539
does this get into a problem as then you

00:17:57,409 --> 00:18:06,169
have to do their redesign stuff as as

00:18:02,539 --> 00:18:11,299
well as the size and the amount of

00:18:06,169 --> 00:18:14,019
output files then another thing what you

00:18:11,299 --> 00:18:17,990
sometimes will see on the cluster that

00:18:14,019 --> 00:18:22,429
four get a huge load that so you load on

00:18:17,990 --> 00:18:26,389
the service goes two hundred or even

00:18:22,429 --> 00:18:28,970
more even higher this is see a good

00:18:26,389 --> 00:18:32,929
indicator that you have a GC problem

00:18:28,970 --> 00:18:34,669
that as your GC runs crazy and this is

00:18:32,929 --> 00:18:38,289
one thing what is relatively difficult

00:18:34,669 --> 00:18:46,100
to monitor and on the Hadoop classes

00:18:38,289 --> 00:18:50,419
that the GC is is a problem and it will

00:18:46,100 --> 00:18:54,049
be have a huge effect to Z huge effect

00:18:50,419 --> 00:18:56,299
to the total run time of of your tasks

00:18:54,049 --> 00:18:59,659
typically you should really enables

00:18:56,299 --> 00:19:03,220
speculative task execution this will

00:18:59,659 --> 00:19:08,210
help you especially on hardware problems

00:19:03,220 --> 00:19:11,299
a lot to make the things but not on s3

00:19:08,210 --> 00:19:16,279
rights there you get a huge problem to

00:19:11,299 --> 00:19:19,669
do this then then another important

00:19:16,279 --> 00:19:23,389
learning is a create file system

00:19:19,669 --> 00:19:26,960
instance for every input and output what

00:19:23,389 --> 00:19:29,269
you do separately otherwise you are

00:19:26,960 --> 00:19:31,789
limited to one kind of file system so if

00:19:29,269 --> 00:19:35,360
you run in Amazon or Google Cloud and

00:19:31,789 --> 00:19:37,100
have someone HDFS someone on the objects

00:19:35,360 --> 00:19:40,190
toward you need to have separated

00:19:37,100 --> 00:19:45,350
otherwise you get a huge amount of error

00:19:40,190 --> 00:19:46,870
messages and your drop don't run good

00:19:45,350 --> 00:19:48,850
now

00:19:46,870 --> 00:19:51,640
and talk a little bit also about how we

00:19:48,850 --> 00:19:53,530
use storm use storm to do a lot of our

00:19:51,640 --> 00:19:55,330
real-time processing for user

00:19:53,530 --> 00:19:57,430
segmentation so user information will

00:19:55,330 --> 00:20:00,190
come in and we still stream directly

00:19:57,430 --> 00:20:02,620
from Kafka to storm into Cassandra and

00:20:00,190 --> 00:20:04,270
into Hadoop and we found a couple of

00:20:02,620 --> 00:20:07,150
things you know that we've learned over

00:20:04,270 --> 00:20:08,650
time right really only group things if

00:20:07,150 --> 00:20:12,820
you need to within the storm topology

00:20:08,650 --> 00:20:16,120
right if you're processing time for the

00:20:12,820 --> 00:20:17,440
if you're a few streams and the

00:20:16,120 --> 00:20:20,230
processing time have a large

00:20:17,440 --> 00:20:29,980
distribution so some pieces that you're

00:20:20,230 --> 00:20:32,290
going to go for so if you if you in your

00:20:29,980 --> 00:20:33,760
topology or if in your streaming you go

00:20:32,290 --> 00:20:35,230
out to an external system to gather

00:20:33,760 --> 00:20:37,270
information about a step within that

00:20:35,230 --> 00:20:39,490
stream and you can have a large

00:20:37,270 --> 00:20:41,200
distribution of response time right that

00:20:39,490 --> 00:20:43,120
will slow your whole topology down so if

00:20:41,200 --> 00:20:44,559
you're going to get external data look

00:20:43,120 --> 00:20:46,210
for an in-memory data store like

00:20:44,559 --> 00:20:48,670
Baltimore it's something that you can

00:20:46,210 --> 00:20:50,050
quickly get with a with a normalized

00:20:48,670 --> 00:20:52,890
response time that you can kind of count

00:20:50,050 --> 00:20:57,190
on within a very short window of time

00:20:52,890 --> 00:20:59,080
also minimize again shuffling right this

00:20:57,190 --> 00:21:01,510
is the same problem whether you're in

00:20:59,080 --> 00:21:03,400
spark cassandra hadoop storm any of

00:21:01,510 --> 00:21:05,380
these things right keep the data where

00:21:03,400 --> 00:21:08,620
the data is to the absolute maximum that

00:21:05,380 --> 00:21:10,300
you can and same message all the time

00:21:08,620 --> 00:21:11,920
right it's a java-based process don't

00:21:10,300 --> 00:21:14,830
create any extra garbage that you don't

00:21:11,920 --> 00:21:16,300
need think it through a lot right when

00:21:14,830 --> 00:21:18,070
you get to this massive amount of scale

00:21:16,300 --> 00:21:19,540
the less you can create the absolute

00:21:18,070 --> 00:21:21,610
better right there's no amount of GC

00:21:19,540 --> 00:21:23,290
tuning that makes up for 80 billion of

00:21:21,610 --> 00:21:25,150
these a day if you're being sloppy and

00:21:23,290 --> 00:21:29,590
it's just it's the little things in here

00:21:25,150 --> 00:21:31,660
that make all the difference so as I

00:21:29,590 --> 00:21:34,000
mentioned we use Baltimore it as a large

00:21:31,660 --> 00:21:35,590
scale ski store right we use both in

00:21:34,000 --> 00:21:38,530
memory Baltimore and we also use the

00:21:35,590 --> 00:21:40,450
read-only stores right the in memory of

00:21:38,530 --> 00:21:42,640
all the more it's very stable it scales

00:21:40,450 --> 00:21:44,170
well it's really performant but it's

00:21:42,640 --> 00:21:45,700
really hard to monitor what's in memory

00:21:44,170 --> 00:21:47,230
right so you have to be careful make

00:21:45,700 --> 00:21:48,760
sure that the input is quality because

00:21:47,230 --> 00:21:50,530
we don't have quality input it's hard to

00:21:48,760 --> 00:21:55,090
go back and debug data that you're

00:21:50,530 --> 00:21:56,560
getting out of that data store later the

00:21:55,090 --> 00:21:58,300
read only stores the file based

00:21:56,560 --> 00:21:59,740
read-only stores are extremely efficient

00:21:58,300 --> 00:22:00,700
we can produce some of MapReduce very

00:21:59,740 --> 00:22:03,250
quickly

00:22:00,700 --> 00:22:06,190
swap those in it scales well it's very

00:22:03,250 --> 00:22:08,680
performant it's very very stable we use

00:22:06,190 --> 00:22:10,990
the newest Baltimore client which is

00:22:08,680 --> 00:22:12,550
doing handles failure and edge cases a

00:22:10,990 --> 00:22:15,610
lot better earlier versions we've had

00:22:12,550 --> 00:22:17,710
issues with in the past one of our

00:22:15,610 --> 00:22:19,630
challenges has been how to swap in the

00:22:17,710 --> 00:22:21,220
data from the big rhian Olynyk stores

00:22:19,630 --> 00:22:24,430
right how to move tremendous amount of

00:22:21,220 --> 00:22:26,200
data into a read-only store versus you

00:22:24,430 --> 00:22:27,670
know what was previously there we've

00:22:26,200 --> 00:22:28,840
done it a number of different ways just

00:22:27,670 --> 00:22:30,340
to let you know one of the things we're

00:22:28,840 --> 00:22:32,350
trying now is removing some of these to

00:22:30,340 --> 00:22:34,930
the cloud to an M amazon environment is

00:22:32,350 --> 00:22:37,360
literally using block storage in Amazon

00:22:34,930 --> 00:22:39,360
and loading an EBS volume with the new

00:22:37,360 --> 00:22:41,320
data store right and basically

00:22:39,360 --> 00:22:42,940
unmounting the old one mounting the new

00:22:41,320 --> 00:22:45,340
one restarting voldemort and using the

00:22:42,940 --> 00:22:46,900
cloud to its advantage right for the

00:22:45,340 --> 00:22:48,400
first time in that particular scenario

00:22:46,900 --> 00:22:50,650
you actually can change the whole file

00:22:48,400 --> 00:22:52,450
system out so the download to that other

00:22:50,650 --> 00:22:54,520
EBS volume might happen on a smaller

00:22:52,450 --> 00:22:56,020
instance off to the side you can get it

00:22:54,520 --> 00:23:04,290
all ready to go and swap it in

00:22:56,020 --> 00:23:10,390
milliseconds it works much faster yeah

00:23:04,290 --> 00:23:13,060
hmm ok just ya Kefka ok we use Kafka as

00:23:10,390 --> 00:23:15,160
our main pipeline delivery system right

00:23:13,060 --> 00:23:16,510
given how much traffic we use we're

00:23:15,160 --> 00:23:18,610
fairly conservative with this so we

00:23:16,510 --> 00:23:20,860
haven't moved up to Kafka nine yet but

00:23:18,610 --> 00:23:23,140
what this is a great lesson from a

00:23:20,860 --> 00:23:25,330
scaling perspective right my message to

00:23:23,140 --> 00:23:27,730
everyone is don't design your software

00:23:25,330 --> 00:23:29,140
around your hardware right or configure

00:23:27,730 --> 00:23:31,600
your software based around your hardware

00:23:29,140 --> 00:23:33,070
so this originally the pipeline was

00:23:31,600 --> 00:23:35,140
designed to run on a bunch of dell

00:23:33,070 --> 00:23:36,820
servers that had capacity for a massive

00:23:35,140 --> 00:23:39,190
number of disk drives in them right

00:23:36,820 --> 00:23:42,310
which resulted in brokers it had large

00:23:39,190 --> 00:23:45,040
amounts of data 17 18 20 terabytes worth

00:23:42,310 --> 00:23:46,480
of data on one kafka broker right that

00:23:45,040 --> 00:23:47,740
sounds great you can pack them in you

00:23:46,480 --> 00:23:49,720
can get a lot of stuff through the rack

00:23:47,740 --> 00:23:52,150
in and out the only problem is when one

00:23:49,720 --> 00:23:54,220
of them dies you're now replicating and

00:23:52,150 --> 00:23:56,680
trying to fix 20 terabytes worth of data

00:23:54,220 --> 00:23:57,880
right so it turns out that just because

00:23:56,680 --> 00:23:59,410
you have the hardware to do something

00:23:57,880 --> 00:24:00,970
its scale doesn't necessarily make it

00:23:59,410 --> 00:24:02,590
the best way to do it it's actually

00:24:00,970 --> 00:24:04,720
turned out better for us to start

00:24:02,590 --> 00:24:07,090
building them as smaller more dynamic

00:24:04,720 --> 00:24:08,560
brokers many more of them smaller amount

00:24:07,090 --> 00:24:10,720
of drive if you lose something you lose

00:24:08,560 --> 00:24:12,160
so much smaller percentage replication

00:24:10,720 --> 00:24:14,290
and rebalancing actually turns out to be

00:24:12,160 --> 00:24:18,040
much better so just a quick less

00:24:14,290 --> 00:24:21,550
learn from us on that one again you know

00:24:18,040 --> 00:24:23,050
we were going up 2.9 right it's much

00:24:21,550 --> 00:24:24,580
more efficient and are from we haven't

00:24:23,050 --> 00:24:27,690
tried Kafka streaming or anything else

00:24:24,580 --> 00:24:29,650
like that yet we use the mirror maker

00:24:27,690 --> 00:24:31,120
requires a lot of tuning and

00:24:29,650 --> 00:24:33,610
experimentation from our perspective

00:24:31,120 --> 00:24:36,100
right uses a lot of memory but it does

00:24:33,610 --> 00:24:38,860
work but it does we are advice to you is

00:24:36,100 --> 00:24:40,440
is definitely be in a tuning form with

00:24:38,860 --> 00:24:43,360
that and be ready to play with that

00:24:40,440 --> 00:24:45,250
we've moved now to using the mirrors

00:24:43,360 --> 00:24:46,450
within docker containers and we'll talk

00:24:45,250 --> 00:24:47,950
a little bit more about how we run

00:24:46,450 --> 00:24:50,200
docker and our organization but that

00:24:47,950 --> 00:24:52,240
gives us the ability to their stateless

00:24:50,200 --> 00:24:56,650
so it gives us the ability to fire them

00:24:52,240 --> 00:24:58,600
up and go quickly you want to talk about

00:24:56,650 --> 00:25:01,090
this on droid you already did okay I'll

00:24:58,600 --> 00:25:03,520
go ahead so just some lessons learned

00:25:01,090 --> 00:25:04,810
with the scaling Cassandra as well we

00:25:03,520 --> 00:25:06,400
run this for a number of different

00:25:04,810 --> 00:25:08,020
things we run it for some time series

00:25:06,400 --> 00:25:09,880
database which I'll talk about in near

00:25:08,020 --> 00:25:12,790
the end and we also use it as a key

00:25:09,880 --> 00:25:14,590
store in a user store cross data center

00:25:12,790 --> 00:25:16,840
replication and our experience needs a

00:25:14,590 --> 00:25:18,670
very very solid network and very focused

00:25:16,840 --> 00:25:21,430
deployment right repairing across

00:25:18,670 --> 00:25:23,170
worldwide data center replications can

00:25:21,430 --> 00:25:24,490
be quite challenging we've actually

00:25:23,170 --> 00:25:26,410
moved away from that we've actually

00:25:24,490 --> 00:25:28,510
moved to a much more eventual

00:25:26,410 --> 00:25:29,800
consistency type model just based on how

00:25:28,510 --> 00:25:31,060
we're growing as a company in some of

00:25:29,800 --> 00:25:33,910
the places we're having a network it

00:25:31,060 --> 00:25:35,500
just didn't work for us again as Frank

00:25:33,910 --> 00:25:37,240
said this is a real key thing here

00:25:35,500 --> 00:25:39,430
optimize your data model to avoid

00:25:37,240 --> 00:25:41,110
deletes if you can find a natural way

00:25:39,430 --> 00:25:42,990
for Cassandra to delete your data do

00:25:41,110 --> 00:25:45,010
that it's much better and it's in it

00:25:42,990 --> 00:25:46,690
compacts a lot better with the different

00:25:45,010 --> 00:25:49,390
compaction strategies than physically

00:25:46,690 --> 00:25:51,190
deleting data it works wonderfully for

00:25:49,390 --> 00:25:53,140
time series data because of that right

00:25:51,190 --> 00:25:54,850
if you can use the time series and the

00:25:53,140 --> 00:25:57,700
TTL is to naturally reduce your data

00:25:54,850 --> 00:25:59,950
you'll win a lot again removing an

00:25:57,700 --> 00:26:03,340
unnecessary cleanup just try not to do

00:25:59,950 --> 00:26:04,840
that in Cassandra key value mapping if

00:26:03,340 --> 00:26:06,670
the key and the value of very similar

00:26:04,840 --> 00:26:08,800
size right it's very inefficient needs

00:26:06,670 --> 00:26:11,860
specific tuning we found Baltimore works

00:26:08,800 --> 00:26:13,390
better in situations like that and the

00:26:11,860 --> 00:26:15,340
number one thing for that our learning

00:26:13,390 --> 00:26:17,560
is no matter what SSD you get to put in

00:26:15,340 --> 00:26:19,330
Cassandra make sure the trim works make

00:26:17,560 --> 00:26:21,310
sure your controller card works do some

00:26:19,330 --> 00:26:24,390
bench testing on your hardware before

00:26:21,310 --> 00:26:24,390
you try to do this scale

00:26:26,789 --> 00:26:32,410
so we use and we've moved to measles

00:26:29,950 --> 00:26:34,090
marathon for our micro services or

00:26:32,410 --> 00:26:35,770
architecture and our in our points of

00:26:34,090 --> 00:26:38,289
distribution and our outside data

00:26:35,770 --> 00:26:40,660
centers for our bidders ad servers so we

00:26:38,289 --> 00:26:42,970
run docker with measles marathon we find

00:26:40,660 --> 00:26:44,530
that works really well for us it means

00:26:42,970 --> 00:26:46,240
this is really good at finding just that

00:26:44,530 --> 00:26:48,429
little extra spot to put just that extra

00:26:46,240 --> 00:26:50,320
little docker container compared to

00:26:48,429 --> 00:26:52,600
running VMS and having to shrink or

00:26:50,320 --> 00:26:54,179
expand or try to find the the optimum vm

00:26:52,600 --> 00:26:55,990
layout within a data center right

00:26:54,179 --> 00:26:59,320
measles provides us a lot more

00:26:55,990 --> 00:27:01,450
flexibility scaling across a lot of

00:26:59,320 --> 00:27:03,580
heterogeneous equipment as Frank said

00:27:01,450 --> 00:27:05,650
right you very often find over time that

00:27:03,580 --> 00:27:08,140
you've built up one machine in a iraq

00:27:05,650 --> 00:27:09,880
that has 24 cores another rackettes 32

00:27:08,140 --> 00:27:12,429
right means this is great for finding

00:27:09,880 --> 00:27:14,230
ways to fit everything in there it's

00:27:12,429 --> 00:27:16,630
also allowed us to do some other

00:27:14,230 --> 00:27:20,289
experimentations for example instead of

00:27:16,630 --> 00:27:21,850
having a heterogeneous Bank of two three

00:27:20,289 --> 00:27:23,890
four hundred bidders talking to every ad

00:27:21,850 --> 00:27:25,480
provider out there we can actually make

00:27:23,890 --> 00:27:27,909
groupings now and have a set of docker

00:27:25,480 --> 00:27:29,409
containers scaled for Google and another

00:27:27,909 --> 00:27:30,970
set of docker container scaled for

00:27:29,409 --> 00:27:32,710
library land another scale for a

00:27:30,970 --> 00:27:34,179
different provider and with marathon

00:27:32,710 --> 00:27:35,650
it's very very easy for us to just say

00:27:34,179 --> 00:27:37,539
hey we're getting pounded by googled

00:27:35,650 --> 00:27:39,280
this week for this campaign just make

00:27:37,539 --> 00:27:41,140
this bigger right and we can actually

00:27:39,280 --> 00:27:42,970
shut off the ones that aren't we still

00:27:41,140 --> 00:27:45,309
don't do that automatically we still do

00:27:42,970 --> 00:27:48,970
that with human control but it provides

00:27:45,309 --> 00:27:50,919
tremendous flexibility there's some

00:27:48,970 --> 00:27:53,830
hints to about using docker here in this

00:27:50,919 --> 00:27:55,210
kind of environment right never try to

00:27:53,830 --> 00:27:56,890
do when you're in a big you know three

00:27:55,210 --> 00:27:58,659
four or five hundred containers even 100

00:27:56,890 --> 00:28:00,700
containers don't do them all at once

00:27:58,659 --> 00:28:01,840
when you have an orchestration mechanism

00:28:00,700 --> 00:28:04,360
whether it's warm weather it's measles

00:28:01,840 --> 00:28:06,340
do ten percent do twenty percent do

00:28:04,360 --> 00:28:08,590
thirty percent give the poor docker

00:28:06,340 --> 00:28:10,299
registry a chance to recover from

00:28:08,590 --> 00:28:11,799
everybody hitting it all at once right

00:28:10,299 --> 00:28:13,120
you'll find much more smoother

00:28:11,799 --> 00:28:14,799
deployments and it gives you a great

00:28:13,120 --> 00:28:16,659
opportunity to do canary deployments as

00:28:14,799 --> 00:28:18,549
well do ten percent double check it if

00:28:16,659 --> 00:28:21,940
it's good to another ten do another ten

00:28:18,549 --> 00:28:23,650
to another ten clean up make sure you

00:28:21,940 --> 00:28:25,000
get rid of your old instances right make

00:28:23,650 --> 00:28:27,280
sure you get rid of old instance data

00:28:25,000 --> 00:28:29,380
make sure you've got a logging strategy

00:28:27,280 --> 00:28:30,669
that works right where your logs are you

00:28:29,380 --> 00:28:32,559
going to use this log you're not going

00:28:30,669 --> 00:28:35,380
to use syslog just don't let your

00:28:32,559 --> 00:28:37,270
instance your docker container hosts

00:28:35,380 --> 00:28:39,720
fill up with log files to come a mistake

00:28:37,270 --> 00:28:41,890
and we've had that happen to us in the

00:28:39,720 --> 00:28:44,050
minimize your doctor images you cows

00:28:41,890 --> 00:28:46,300
have all heard this right make them as

00:28:44,050 --> 00:28:47,710
small as possible but don't leave off

00:28:46,300 --> 00:28:49,900
some of the core tools in our experience

00:28:47,710 --> 00:28:51,670
this is our opinion right if you need to

00:28:49,900 --> 00:28:53,290
be able to make you more apt get in your

00:28:51,670 --> 00:28:54,940
docker container be able to go and get

00:28:53,290 --> 00:28:56,230
that one tool you need right now on that

00:28:54,940 --> 00:28:58,600
one container and production for

00:28:56,230 --> 00:28:59,800
debugging allow yourself to do that lots

00:28:58,600 --> 00:29:01,030
of people say you're not going to debug

00:28:59,800 --> 00:29:03,370
in production and the reality is

00:29:01,030 --> 00:29:04,930
sometimes you are so allow yourself the

00:29:03,370 --> 00:29:08,680
ability to do that just makes your life

00:29:04,930 --> 00:29:10,990
a lot easier two in the morning and

00:29:08,680 --> 00:29:13,690
let's talk a little bit about some our

00:29:10,990 --> 00:29:15,490
move to the cloud so right now we

00:29:13,690 --> 00:29:18,190
started this off as a 500 node cluster

00:29:15,490 --> 00:29:19,660
in a fixed data center right and you

00:29:18,190 --> 00:29:22,300
have some beautiful advantages when you

00:29:19,660 --> 00:29:24,310
do that right you have a networking team

00:29:22,300 --> 00:29:26,230
that builds you 160 gigabit spline that

00:29:24,310 --> 00:29:27,490
absolutely positively screams and the

00:29:26,230 --> 00:29:30,460
data works beautifully and everything

00:29:27,490 --> 00:29:32,470
works but you have 500 nodes and the day

00:29:30,460 --> 00:29:34,180
that you need 600 nodes you aren't going

00:29:32,470 --> 00:29:35,920
to have them right and so everybody

00:29:34,180 --> 00:29:38,050
stacks up and things stack up so you

00:29:35,920 --> 00:29:40,180
don't have the elasticity so you move to

00:29:38,050 --> 00:29:41,320
the cloud okay and that sounds great and

00:29:40,180 --> 00:29:43,000
everything's wonderful and it's going to

00:29:41,320 --> 00:29:45,580
be elastic but there's gotchas there too

00:29:43,000 --> 00:29:47,590
you've lost that 160 gig spine right

00:29:45,580 --> 00:29:48,790
you've lost that network team that you

00:29:47,590 --> 00:29:50,080
can call in the middle of the night to

00:29:48,790 --> 00:29:53,200
find out why you're having trouble

00:29:50,080 --> 00:29:55,180
you've lost sometimes the capacity to

00:29:53,200 --> 00:29:58,480
scale the number of instances you think

00:29:55,180 --> 00:30:01,450
you have aren't always there also object

00:29:58,480 --> 00:30:03,070
store versus HDFS the two different

00:30:01,450 --> 00:30:04,780
animals be very careful as you start

00:30:03,070 --> 00:30:06,850
using object stores you're going to want

00:30:04,780 --> 00:30:08,410
to make sure that you don't go back and

00:30:06,850 --> 00:30:09,940
forth if you have multiple jobs and

00:30:08,410 --> 00:30:12,430
you're going to read from s3 in process

00:30:09,940 --> 00:30:14,890
do local HDFS for your temporary data

00:30:12,430 --> 00:30:16,510
right s3 Google object store they all

00:30:14,890 --> 00:30:18,910
take a lot longer than local file to

00:30:16,510 --> 00:30:20,800
write so you'll slow yourself down grab

00:30:18,910 --> 00:30:22,930
your data do all your processing at the

00:30:20,800 --> 00:30:26,110
end of an extended workflow write it

00:30:22,930 --> 00:30:28,570
back out and we right now are leveraging

00:30:26,110 --> 00:30:29,770
q-ball for scalable Hadoop and spark so

00:30:28,570 --> 00:30:32,860
that's an interesting product to check

00:30:29,770 --> 00:30:34,570
out cue ball actually orchestrates ec2

00:30:32,860 --> 00:30:36,790
instances or google cloud or as your

00:30:34,570 --> 00:30:38,710
instances to make cluster manages

00:30:36,790 --> 00:30:40,360
automatic scaling up and down and using

00:30:38,710 --> 00:30:42,060
spot instances etc it's an interesting

00:30:40,360 --> 00:30:46,450
product we're just starting to use that

00:30:42,060 --> 00:30:48,010
and then lastly talked real quick about

00:30:46,450 --> 00:30:50,500
how we're scaling our time series data

00:30:48,010 --> 00:30:51,820
so as you imagine is we start building

00:30:50,500 --> 00:30:52,330
more data centers and we start scaling

00:30:51,820 --> 00:30:54,070
out

00:30:52,330 --> 00:30:57,159
we've gotten to the point where as much

00:30:54,070 --> 00:30:59,409
as we like to monitor right writing 1.2

00:30:57,159 --> 00:31:01,870
to 1.5 million metrics per minute into

00:30:59,409 --> 00:31:04,360
graphite we've run out of ways to scale

00:31:01,870 --> 00:31:05,890
carbon and whisper and make that all

00:31:04,360 --> 00:31:07,120
work so we've come up with some

00:31:05,890 --> 00:31:08,830
interesting ideas we've kind of done a

00:31:07,120 --> 00:31:10,630
little homegrown experiment we spent a

00:31:08,830 --> 00:31:11,799
lot of time looking at what was in the

00:31:10,630 --> 00:31:13,690
environment in the open source

00:31:11,799 --> 00:31:16,090
environment especially with Cassandra

00:31:13,690 --> 00:31:18,549
and HBase in terms of time series to

00:31:16,090 --> 00:31:19,899
basically made a very simple time series

00:31:18,549 --> 00:31:22,179
data base out of Cassandra it works

00:31:19,899 --> 00:31:24,159
beautifully for it right wyd tables it's

00:31:22,179 --> 00:31:25,539
that everybody's doing it we did a

00:31:24,159 --> 00:31:27,610
couple of interesting things we thought

00:31:25,539 --> 00:31:30,639
we'd mentioned we use the strata o

00:31:27,610 --> 00:31:32,200
leucine plugin in Cassandra and we

00:31:30,639 --> 00:31:33,789
actually take our metric names and our

00:31:32,200 --> 00:31:36,340
metric tags we put them in a second

00:31:33,789 --> 00:31:37,929
table and we use leucine to scan that

00:31:36,340 --> 00:31:40,539
table and index that table so if you

00:31:37,929 --> 00:31:42,190
want to do a wild card of a bunch of

00:31:40,539 --> 00:31:44,230
metrics like you would in graphite right

00:31:42,190 --> 00:31:45,549
you can find those metric names and get

00:31:44,230 --> 00:31:48,010
that list and then Joe and use that to

00:31:45,549 --> 00:31:50,889
extract your time series rose separately

00:31:48,010 --> 00:31:52,330
we use spark is our custom query engine

00:31:50,889 --> 00:31:53,889
against this it's worked out really

00:31:52,330 --> 00:31:56,260
really well sparking it's time to work

00:31:53,889 --> 00:31:58,149
very well together so we actually have a

00:31:56,260 --> 00:32:00,399
custom Scala based query engine that

00:31:58,149 --> 00:32:02,380
goes in through spark extracts data out

00:32:00,399 --> 00:32:04,120
through Cassandra and is able to process

00:32:02,380 --> 00:32:05,830
it do the necessary math and all the

00:32:04,120 --> 00:32:09,429
necessary functions to that data and

00:32:05,830 --> 00:32:11,559
present one array back to a Java

00:32:09,429 --> 00:32:14,019
microservice that actually mocks the

00:32:11,559 --> 00:32:15,519
graphite API so basically what we've

00:32:14,019 --> 00:32:17,380
done is if you want to use graphite or

00:32:15,519 --> 00:32:19,570
graphene you go to that mock service it

00:32:17,380 --> 00:32:22,539
goes to spark it does the queries and it

00:32:19,570 --> 00:32:24,039
reads out through Cassandra a next

00:32:22,539 --> 00:32:26,049
experiment that we're starting to do now

00:32:24,039 --> 00:32:28,480
is how do we start doing anomaly

00:32:26,049 --> 00:32:30,700
detection and curve detection right so

00:32:28,480 --> 00:32:32,169
looking at you know if you've got

00:32:30,700 --> 00:32:33,610
everything's writing fine and your ad

00:32:32,169 --> 00:32:35,440
servers are fine and all of a sudden

00:32:33,610 --> 00:32:37,059
something drops off we're still at the

00:32:35,440 --> 00:32:38,679
point now we've got humans watching some

00:32:37,059 --> 00:32:40,149
of that our next experiment is how do we

00:32:38,679 --> 00:32:41,590
get spark to start watching that right

00:32:40,149 --> 00:32:43,029
and start telling us when the

00:32:41,590 --> 00:32:45,070
differentials are tanking or something's

00:32:43,029 --> 00:32:46,929
going wrong with curves etc but it's

00:32:45,070 --> 00:32:48,250
it's an exciting project that we're

00:32:46,929 --> 00:32:50,350
doing and we think that scales really

00:32:48,250 --> 00:32:52,120
well we've been able to do it from much

00:32:50,350 --> 00:32:54,309
lower cost in Amazon than trying to run

00:32:52,120 --> 00:32:55,360
graphite so we suggested you know to

00:32:54,309 --> 00:32:56,950
people if you're looking for a way to

00:32:55,360 --> 00:32:58,389
scale your time series definitely look

00:32:56,950 --> 00:33:01,210
at Cassandra there's lots of tools out

00:32:58,389 --> 00:33:06,299
there for that and that's pretty much

00:33:01,210 --> 00:33:06,299
what we have for you any questions

00:33:21,460 --> 00:33:29,470
ok

00:33:23,890 --> 00:33:34,000
so I have it I question okay well these

00:33:29,470 --> 00:33:38,530
these amount of data is is it a new

00:33:34,000 --> 00:33:43,150
process well I I saw it in IT systems

00:33:38,530 --> 00:33:46,180
but your what do you process with it so

00:33:43,150 --> 00:33:48,220
we have will take a will take an 80

00:33:46,180 --> 00:33:51,160
billion bid request today right for

00:33:48,220 --> 00:33:52,840
advertising bidding from different open

00:33:51,160 --> 00:33:54,820
our TV providers right and so we've got

00:33:52,840 --> 00:33:56,350
to go through those at the bid edge and

00:33:54,820 --> 00:33:57,910
decide what we're bidding on what we're

00:33:56,350 --> 00:34:00,040
not bidding on based on users that we

00:33:57,910 --> 00:34:01,900
find in our system based on the number

00:34:00,040 --> 00:34:03,790
of other criteria and then basically

00:34:01,900 --> 00:34:06,010
either accept or reject those been make

00:34:03,790 --> 00:34:08,350
a bit or not make a bit right then what

00:34:06,010 --> 00:34:09,730
our system has to do is then also take

00:34:08,350 --> 00:34:11,649
that data and bring it back so it's

00:34:09,730 --> 00:34:13,570
obviously important to us what ads we've

00:34:11,649 --> 00:34:15,820
sold what bids we've made what we

00:34:13,570 --> 00:34:17,050
haven't been on and why right because

00:34:15,820 --> 00:34:18,340
that also tells us interesting

00:34:17,050 --> 00:34:20,500
information about what's in the ad

00:34:18,340 --> 00:34:21,669
ecosphere right now what users are

00:34:20,500 --> 00:34:23,770
coming in and what they're looking at

00:34:21,669 --> 00:34:25,390
and what people are doing on the web so

00:34:23,770 --> 00:34:27,460
we also process all of that data that we

00:34:25,390 --> 00:34:29,500
don't bid on so we take some of that is

00:34:27,460 --> 00:34:31,659
sampled data right and some of that is

00:34:29,500 --> 00:34:33,130
actual data into the system and bring

00:34:31,659 --> 00:34:38,620
that all back and then process all of

00:34:33,130 --> 00:34:41,440
that prediction think is the next you do

00:34:38,620 --> 00:34:47,610
with it and why is it integrated in your

00:34:41,440 --> 00:34:49,990
system already so good it's working yeah

00:34:47,610 --> 00:34:52,929
things the predictor suffered she was

00:34:49,990 --> 00:34:56,230
talking about is basically the stuff for

00:34:52,929 --> 00:35:01,210
the monitoring and for the calculation

00:34:56,230 --> 00:35:04,480
numbers our system based we have an

00:35:01,210 --> 00:35:08,770
internal da DMP data management platform

00:35:04,480 --> 00:35:11,920
where we also collecting information

00:35:08,770 --> 00:35:17,770
about user behavior in a similar way as

00:35:11,920 --> 00:35:20,410
you had heard from bol before in in in

00:35:17,770 --> 00:35:24,280
the other talk and we combine this with

00:35:20,410 --> 00:35:27,330
the data what what we get out of our bit

00:35:24,280 --> 00:35:33,100
requests and compression requests and

00:35:27,330 --> 00:35:35,700
built out of them z models for future

00:35:33,100 --> 00:35:37,240
bidding for certain users or four

00:35:35,700 --> 00:35:39,940
generals

00:35:37,240 --> 00:35:45,040
strategies and there's also a lot of

00:35:39,940 --> 00:35:48,280
machine learning in it which feeds ends

00:35:45,040 --> 00:35:50,850
and back and to give you some some

00:35:48,280 --> 00:35:55,780
numbers we are right now writing

00:35:50,850 --> 00:35:58,810
approximately 70 17 terabytes a day to

00:35:55,780 --> 00:36:01,720
history from worldwide to one data

00:35:58,810 --> 00:36:04,119
center so and there's a couple of

00:36:01,720 --> 00:36:11,369
challenges in this if you want to scale

00:36:04,119 --> 00:36:16,470
is 32 that sings so any questions what

00:36:11,369 --> 00:36:16,470

YouTube URL: https://www.youtube.com/watch?v=bbrgIMD2vnA


