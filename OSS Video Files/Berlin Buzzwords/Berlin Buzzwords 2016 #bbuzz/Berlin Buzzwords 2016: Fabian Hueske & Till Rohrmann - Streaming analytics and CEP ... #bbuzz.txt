Title: Berlin Buzzwords 2016: Fabian Hueske & Till Rohrmann - Streaming analytics and CEP ... #bbuzz
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Complex event processing (CEP) and stream analytics are commonly treated as distinct classes of stream processing applications. While CEP workloads identify patterns from event streams in near real-time, stream analytics queries ingest and aggregate high-volume streams. Both types of use cases have very different requirements which resulted in diverging system designs. CEP systems excel at low-latency processing whereas engines for stream analytics achieve high throughput usually due to distributed scale-out architectures.

Recent advances in open source stream processing yielded systems that can process several millions of events per second at sub-second latency. Systems like Apache Flink enable applications that include typical CEP features as well as heavy aggregations. An example of these use cases is an application that ingests network monitoring events, identifies access patterns such as intrusion attempts using CEP technology, and analyzes and aggregates identified access patterns.

In this talk we will show how Apache Flink unifies CEP and stream analytics workloads. Guided by examples, we introduce Flinkâ€™s CEP-enriched StreamSQL interface and discuss how queries are compiled, optimized, and executed on Flink.

Read more:
https://2016.berlinbuzzwords.de/session/computing-recommendations-extreme-scale-apache-flink

About Fabian Hueske:
https://2016.berlinbuzzwords.de/users/fabian-hueske

About Till Rohrmann:
https://2016.berlinbuzzwords.de/users/till-rohrmann

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:02,990 --> 00:00:08,490
yeah welcome everybody um in this talk

00:00:06,089 --> 00:00:10,980
till and I are ya going to talk about

00:00:08,490 --> 00:00:14,009
two different kinds of Street processing

00:00:10,980 --> 00:00:17,490
applications streaming analytics and

00:00:14,009 --> 00:00:19,350
complex event processing yeah till I

00:00:17,490 --> 00:00:21,660
myself were both committees at Apache

00:00:19,350 --> 00:00:24,930
fling and currently working at data

00:00:21,660 --> 00:00:26,670
artisans so you might have noticed the

00:00:24,930 --> 00:00:28,859
stream person is really picking up it's

00:00:26,670 --> 00:00:31,529
one of the dominating topics here this

00:00:28,859 --> 00:00:35,130
conference and also at previous

00:00:31,529 --> 00:00:36,930
conferences and this is because most

00:00:35,130 --> 00:00:38,700
data is actually produced s stream so

00:00:36,930 --> 00:00:42,239
there's usually not one big block of

00:00:38,700 --> 00:00:45,960
data that you try to analyze but data is

00:00:42,239 --> 00:00:47,969
constantly produced so maybe coming from

00:00:45,960 --> 00:00:51,710
from server server logs or application

00:00:47,969 --> 00:00:54,449
locks click locks mobile devices

00:00:51,710 --> 00:00:57,859
Internet of Things is another big topic

00:00:54,449 --> 00:01:04,110
here so data is constantly produced and

00:00:57,859 --> 00:01:07,200
arrives in data centers and so no people

00:01:04,110 --> 00:01:09,600
are thinking about okay how can I really

00:01:07,200 --> 00:01:12,900
leverage this continuously produced data

00:01:09,600 --> 00:01:15,960
what can I do with it and how can I

00:01:12,900 --> 00:01:17,520
analyze this data as it arrives and

00:01:15,960 --> 00:01:20,729
there are different kinds of

00:01:17,520 --> 00:01:24,270
applications and use cases for this so

00:01:20,729 --> 00:01:30,320
one of these use cases is complex event

00:01:24,270 --> 00:01:33,750
processing so the goal of CP is to

00:01:30,320 --> 00:01:37,710
usually detect patterns in an event

00:01:33,750 --> 00:01:40,799
stream and then whenever you detect a

00:01:37,710 --> 00:01:45,600
certain pattern you want to yeah

00:01:40,799 --> 00:01:49,259
assemble or or merge or aggregate and a

00:01:45,600 --> 00:01:52,530
new kind of event that aggregates the

00:01:49,259 --> 00:01:55,500
the events that basically trigger this

00:01:52,530 --> 00:01:58,200
pattern to yeah or trigger this pattern

00:01:55,500 --> 00:02:01,439
and there are a number of applications

00:01:58,200 --> 00:02:04,799
for this kinds of workloads for instance

00:02:01,439 --> 00:02:07,710
there is like process monitoring so

00:02:04,799 --> 00:02:10,200
imagine your warehouse have detect all

00:02:07,710 --> 00:02:13,890
your items with RFID tags then you can

00:02:10,200 --> 00:02:15,450
basically automatically or get automatic

00:02:13,890 --> 00:02:16,980
whenever something enters your warehouse

00:02:15,450 --> 00:02:18,330
whenever something is moved within the

00:02:16,980 --> 00:02:21,300
warehouse or whenever something leaves

00:02:18,330 --> 00:02:23,240
the warehouse there is intrusion

00:02:21,300 --> 00:02:26,069
detection for networks so whenever a

00:02:23,240 --> 00:02:29,850
certain suspicious pattern occurs that

00:02:26,069 --> 00:02:31,830
some some pot ranges are are tested so

00:02:29,850 --> 00:02:37,040
these are kinds of patterns that you can

00:02:31,830 --> 00:02:40,200
try to detect from server logs and or

00:02:37,040 --> 00:02:43,770
for financial trading so whenever you

00:02:40,200 --> 00:02:45,840
see that certain buy or sell patterns

00:02:43,770 --> 00:02:48,450
occur you can you can you can trigger

00:02:45,840 --> 00:02:52,170
certain certain actions either to buy

00:02:48,450 --> 00:02:54,120
our stocks or to sell stocks and these

00:02:52,170 --> 00:02:56,430
kinds of workloads are very very

00:02:54,120 --> 00:02:58,380
demanding on the on if you want to

00:02:56,430 --> 00:02:59,790
execute that on a stream processor it's

00:02:58,380 --> 00:03:01,440
very very demanding on the stream

00:02:59,790 --> 00:03:04,110
process because you need very low

00:03:01,440 --> 00:03:06,930
latency to actually do that so you don't

00:03:04,110 --> 00:03:09,360
want to react like one minute after a

00:03:06,930 --> 00:03:14,070
certain pattern pattern occurred you

00:03:09,360 --> 00:03:16,860
usually want to want to detect the

00:03:14,070 --> 00:03:20,670
pattern instantly and also instantly a

00:03:16,860 --> 00:03:26,489
trigger an action so imagine a certain

00:03:20,670 --> 00:03:28,350
pattern occurs in a in a stock I you

00:03:26,489 --> 00:03:29,880
wanted instantly Iraq you don't want to

00:03:28,350 --> 00:03:33,120
wait for one minute and everybody else

00:03:29,880 --> 00:03:35,340
has already yeah leveraged there's

00:03:33,120 --> 00:03:38,070
certain the certain event and you're

00:03:35,340 --> 00:03:43,920
basically the last another thing that's

00:03:38,070 --> 00:03:45,959
very important is you that yeah you need

00:03:43,920 --> 00:03:48,090
to be able to take the time when the

00:03:45,959 --> 00:03:50,250
event was created into account so it's

00:03:48,090 --> 00:03:52,019
not not enough to win whenever you

00:03:50,250 --> 00:03:55,290
detect patterns it's not enough to

00:03:52,019 --> 00:03:56,489
detect the patterns on basically looking

00:03:55,290 --> 00:03:58,489
looking at the time when the event

00:03:56,489 --> 00:04:01,829
arrives at the operator or at the

00:03:58,489 --> 00:04:03,239
processing at the data processor you

00:04:01,829 --> 00:04:05,310
want to want to take the time when the

00:04:03,239 --> 00:04:07,650
event was created into account so this

00:04:05,310 --> 00:04:09,810
is what what we call events on and you

00:04:07,650 --> 00:04:11,640
also don't want to lose certain patterns

00:04:09,810 --> 00:04:12,870
right when something when when you when

00:04:11,640 --> 00:04:15,120
you lose an event or an event occurs

00:04:12,870 --> 00:04:16,709
twice this might met mess up your

00:04:15,120 --> 00:04:22,680
pattern you might not be able to detect

00:04:16,709 --> 00:04:24,690
that another yeah so another application

00:04:22,680 --> 00:04:26,820
for for stream processing is analytics

00:04:24,690 --> 00:04:27,480
so this is like the traditional approach

00:04:26,820 --> 00:04:29,490
to

00:04:27,480 --> 00:04:32,550
energox like the batch approach so you

00:04:29,490 --> 00:04:34,280
have yeah clients updating a

00:04:32,550 --> 00:04:36,780
transactional database systems and

00:04:34,280 --> 00:04:39,450
periodically you load the data from the

00:04:36,780 --> 00:04:42,450
transactional database into a data

00:04:39,450 --> 00:04:46,230
warehouse or HDFS and then periodically

00:04:42,450 --> 00:04:48,480
you analyze the data in this in this bed

00:04:46,230 --> 00:04:50,670
store using a patch engine and the

00:04:48,480 --> 00:04:53,880
problem with this is there is a delay

00:04:50,670 --> 00:04:56,970
right so whenever you do this periodic

00:04:53,880 --> 00:04:59,610
et al there are some some some some

00:04:56,970 --> 00:05:01,890
delay if you do that once a day your

00:04:59,610 --> 00:05:07,050
data might be one day old when you when

00:05:01,890 --> 00:05:09,630
you analyze it so stream processing or

00:05:07,050 --> 00:05:11,010
stream analytics it's an application

00:05:09,630 --> 00:05:15,150
where you basically try to analyze the

00:05:11,010 --> 00:05:17,850
data as it arrives so you get continuous

00:05:15,150 --> 00:05:21,900
results and the data is basically and

00:05:17,850 --> 00:05:24,810
lies as it arrives you can also think of

00:05:21,900 --> 00:05:27,930
this this stream analytics as a superset

00:05:24,810 --> 00:05:30,210
of the previous batch analytics ride so

00:05:27,930 --> 00:05:32,460
if you consider like a like a batch

00:05:30,210 --> 00:05:34,020
being a finite stream so whenever you

00:05:32,460 --> 00:05:36,270
read something from a from a from a file

00:05:34,020 --> 00:05:38,130
system this is basically a stream so

00:05:36,270 --> 00:05:40,170
even like the programming languages

00:05:38,130 --> 00:05:42,660
consider this as a stream streaming

00:05:40,170 --> 00:05:46,680
abstraction so this is just a finite

00:05:42,660 --> 00:05:49,740
stream so bad energy axis it's kinda

00:05:46,680 --> 00:05:52,170
like a yeah it's upset from from stream

00:05:49,740 --> 00:05:53,970
analytics and the requirements or the

00:05:52,170 --> 00:05:57,210
stream process so if you want to do

00:05:53,970 --> 00:05:58,380
something such a workload is yeah I need

00:05:57,210 --> 00:06:02,090
to be able to cope with very high

00:05:58,380 --> 00:06:05,670
throughput so there are use cases where

00:06:02,090 --> 00:06:06,990
millions of events arrive per second so

00:06:05,670 --> 00:06:10,290
your stream process I needs to be able

00:06:06,990 --> 00:06:13,560
to to handle that so massive massive

00:06:10,290 --> 00:06:15,660
parallelism is required here you also

00:06:13,560 --> 00:06:17,340
want to do that of of course exactly

00:06:15,660 --> 00:06:22,590
once you don't want to count some things

00:06:17,340 --> 00:06:25,380
twice or or lose your date loose data

00:06:22,590 --> 00:06:27,960
and again event time is important and

00:06:25,380 --> 00:06:29,730
also like advanced windowing

00:06:27,960 --> 00:06:36,630
capabilities what I'm going to talk

00:06:29,730 --> 00:06:38,490
about this briefly uh later um yeah so

00:06:36,630 --> 00:06:40,470
as I said before Dylan I we're both

00:06:38,490 --> 00:06:41,249
committees at Apogee fling and this talk

00:06:40,470 --> 00:06:43,289
will also be

00:06:41,249 --> 00:06:47,099
about how flink handed see these kinds

00:06:43,289 --> 00:06:50,099
of different applications and yeah fling

00:06:47,099 --> 00:06:52,619
is a scalable data processor that for

00:06:50,099 --> 00:06:56,209
for for streaming data and it actually

00:06:52,619 --> 00:06:59,879
meets the requirements of both of CP and

00:06:56,209 --> 00:07:02,339
streaming analytics so it's provides low

00:06:59,879 --> 00:07:04,289
latency which is required by our complex

00:07:02,339 --> 00:07:06,239
event processing and also high

00:07:04,289 --> 00:07:10,229
throughput so you can scale out fling

00:07:06,239 --> 00:07:13,229
and achieve very high throughput with a

00:07:10,229 --> 00:07:15,719
system flick also supports arm event

00:07:13,229 --> 00:07:22,759
time has very good support for window

00:07:15,719 --> 00:07:25,829
and features exactly one semantics and

00:07:22,759 --> 00:07:28,319
so the core programming abstraction or

00:07:25,829 --> 00:07:31,889
API for flink is the support data stream

00:07:28,319 --> 00:07:35,519
API which is available in Scala and java

00:07:31,889 --> 00:07:39,179
and yeah this is I wouldn't say it's not

00:07:35,519 --> 00:07:40,679
really a low-level API but it's if you

00:07:39,179 --> 00:07:43,879
want to want to implement like something

00:07:40,679 --> 00:07:47,579
like a certain detecting patterns or

00:07:43,879 --> 00:07:49,079
certain kinds of stream analytics there

00:07:47,579 --> 00:07:53,639
is some some ovett involved if you want

00:07:49,079 --> 00:07:59,759
to implement that so this talk will be

00:07:53,639 --> 00:08:02,789
about yeah introducing or showing some

00:07:59,759 --> 00:08:05,129
new api's that that that fling has

00:08:02,789 --> 00:08:08,789
recently added or will add with the next

00:08:05,129 --> 00:08:11,549
version and this API so for I am complex

00:08:08,789 --> 00:08:13,049
event processing so with version 10 so

00:08:11,549 --> 00:08:17,309
which was released I don't know maybe

00:08:13,049 --> 00:08:20,729
two month ago we added this cep library

00:08:17,309 --> 00:08:23,939
which is basically library to define

00:08:20,729 --> 00:08:26,429
define patterns and actions so what you

00:08:23,939 --> 00:08:30,019
do when a certain pattern occurred and

00:08:26,429 --> 00:08:33,569
the community is currently working on on

00:08:30,019 --> 00:08:35,610
stream sequel so basically a way to ease

00:08:33,569 --> 00:08:39,509
the definition of stream analytics

00:08:35,610 --> 00:08:41,099
queries we also a brief you talk about

00:08:39,509 --> 00:08:43,229
how these two different kinds of

00:08:41,099 --> 00:08:46,769
applications can actually arm can I can

00:08:43,229 --> 00:08:48,180
actually be merged or show you some some

00:08:46,769 --> 00:08:51,779
applications where it makes sense to

00:08:48,180 --> 00:08:55,020
combine CP and stream analytics in the

00:08:51,779 --> 00:08:57,870
same in the same yeah epic

00:08:55,020 --> 00:08:59,400
occasion and why it might be why it's a

00:08:57,870 --> 00:09:01,350
good idea to use a stream processor

00:08:59,400 --> 00:09:05,220
that's actually capable of doing both

00:09:01,350 --> 00:09:06,930
like providing low latency for a strip

00:09:05,220 --> 00:09:09,800
for for complex event processing and

00:09:06,930 --> 00:09:13,740
high throughput for the analytical part

00:09:09,800 --> 00:09:16,380
all of this is should say is still quite

00:09:13,740 --> 00:09:19,230
early work as I said so CP has been

00:09:16,380 --> 00:09:23,100
added with a light yeah latest version

00:09:19,230 --> 00:09:25,380
of fling stream sequel will be released

00:09:23,100 --> 00:09:31,650
for the next version and so this is all

00:09:25,380 --> 00:09:34,260
basically very very early so no I'm

00:09:31,650 --> 00:09:36,810
going to talk a little bit about a use

00:09:34,260 --> 00:09:38,970
case that we use like this the driving

00:09:36,810 --> 00:09:42,510
example in this talk and this is about

00:09:38,970 --> 00:09:45,000
yeah tracking tracking events in an

00:09:42,510 --> 00:09:48,540
order process so this is a very very

00:09:45,000 --> 00:09:50,310
simple example here we have on the left

00:09:48,540 --> 00:09:52,320
hand side we have the customer the

00:09:50,310 --> 00:09:55,680
customer places an order at a warehouse

00:09:52,320 --> 00:10:00,590
so firstly the end the warehouse

00:09:55,680 --> 00:10:04,260
generates and order received event

00:10:00,590 --> 00:10:06,390
whenever it receives such an order and

00:10:04,260 --> 00:10:10,950
then somebody in the warehouse will go

00:10:06,390 --> 00:10:12,960
assemble all the parts that the order

00:10:10,950 --> 00:10:17,670
requests it in a package whenever the

00:10:12,960 --> 00:10:19,890
package is ready the package is given to

00:10:17,670 --> 00:10:23,430
a delivery service so whenever this

00:10:19,890 --> 00:10:27,450
happens there is a order shipped event

00:10:23,430 --> 00:10:30,420
triggered and sent into the data

00:10:27,450 --> 00:10:32,730
infrastructure of the of the company and

00:10:30,420 --> 00:10:34,440
whenever then this order is actually

00:10:32,730 --> 00:10:37,110
delivered to the customer then the

00:10:34,440 --> 00:10:39,600
delivery service will again send an

00:10:37,110 --> 00:10:41,280
event and say okay this order was

00:10:39,600 --> 00:10:43,800
delivered so it's a pretty simple thing

00:10:41,280 --> 00:10:46,380
so we have the order received at the

00:10:43,800 --> 00:10:48,180
warehouse the order is shipped basically

00:10:46,380 --> 00:10:49,830
leaves the warehouse and then the order

00:10:48,180 --> 00:10:54,600
arrives at the customer the RS delivered

00:10:49,830 --> 00:10:58,380
so these kinds of events are in our use

00:10:54,600 --> 00:11:01,020
case the order received we mocked up

00:10:58,380 --> 00:11:03,180
with this smaller envelope icon the

00:11:01,020 --> 00:11:06,540
authorship is the package and then the

00:11:03,180 --> 00:11:08,670
t-shirt is when the package arrives at

00:11:06,540 --> 00:11:12,480
the customer pick he yeah

00:11:08,670 --> 00:11:16,410
pex the package end yeah receives it is

00:11:12,480 --> 00:11:18,630
a nice t-shirt yeah we're gonna model

00:11:16,410 --> 00:11:23,730
that as a stream of events so the events

00:11:18,630 --> 00:11:25,920
are very simple just 33 attributes the

00:11:23,730 --> 00:11:27,510
first one is an order ID so we want to

00:11:25,920 --> 00:11:29,700
be able to track the different orders

00:11:27,510 --> 00:11:33,000
are there's a time stamp whenever this

00:11:29,700 --> 00:11:37,380
event occurred and the last one is a

00:11:33,000 --> 00:11:39,230
status which is received shipped or

00:11:37,380 --> 00:11:48,450
delivered depending on what kind of

00:11:39,230 --> 00:11:50,310
event happened yeah so this is the use

00:11:48,450 --> 00:11:54,860
case that we're going to talk about in

00:11:50,310 --> 00:11:57,590
this talk no I'm yeah talking about the

00:11:54,860 --> 00:12:00,120
stream analytics side of this talk

00:11:57,590 --> 00:12:03,960
basically yeah aggregating this this

00:12:00,120 --> 00:12:07,560
data so I talked before a little bit

00:12:03,960 --> 00:12:10,280
about the traditional approach where the

00:12:07,560 --> 00:12:14,670
data was loaded from the transactional

00:12:10,280 --> 00:12:18,300
transactional data bases into an data

00:12:14,670 --> 00:12:21,150
warehouse or HDFS or whatever whatever

00:12:18,300 --> 00:12:22,800
other data storm and usually what you do

00:12:21,150 --> 00:12:26,120
when you have such an infrastructure

00:12:22,800 --> 00:12:28,350
usually if you repeat the queries over

00:12:26,120 --> 00:12:31,080
changing data sets right you say okay

00:12:28,350 --> 00:12:33,000
give me a report for the last week give

00:12:31,080 --> 00:12:36,720
me a report for the last week and so on

00:12:33,000 --> 00:12:38,280
so it's a usually the same query with

00:12:36,720 --> 00:12:40,440
different parameters but all over again

00:12:38,280 --> 00:12:42,780
so you repeat the query and these clear

00:12:40,440 --> 00:12:46,440
is tend to join and aggregate large

00:12:42,780 --> 00:12:48,360
tables when now so this is the

00:12:46,440 --> 00:12:51,540
traditional batch approach when you know

00:12:48,360 --> 00:12:52,950
look at how stream processors do that so

00:12:51,540 --> 00:12:57,030
stream processors if you do that with a

00:12:52,950 --> 00:12:59,670
stream processor um you don't issued

00:12:57,030 --> 00:13:02,100
multiple queries you just start one

00:12:59,670 --> 00:13:04,230
query and this is like a standing theory

00:13:02,100 --> 00:13:06,240
so the query does never terminate it

00:13:04,230 --> 00:13:09,510
consumes the stream of events and it

00:13:06,240 --> 00:13:11,970
produces a stream of results so there's

00:13:09,510 --> 00:13:14,490
one query which is active the whole time

00:13:11,970 --> 00:13:18,810
so the data is inject ingested into this

00:13:14,490 --> 00:13:20,700
stream processor and the processor does

00:13:18,810 --> 00:13:22,499
some computation aggregation on the data

00:13:20,700 --> 00:13:26,969
and produces the results

00:13:22,499 --> 00:13:29,249
and I said before so usually on there

00:13:26,969 --> 00:13:30,659
there are lots of use cases where this

00:13:29,249 --> 00:13:34,469
is actually done a very high volume

00:13:30,659 --> 00:13:37,889
stream so where millions of events enter

00:13:34,469 --> 00:13:40,139
the system for a second but now one

00:13:37,889 --> 00:13:42,149
question arises so how do how can you

00:13:40,139 --> 00:13:44,729
can you actually do aggregations on

00:13:42,149 --> 00:13:46,919
stream so this stream is infinite so it

00:13:44,729 --> 00:13:50,339
does never end whenever when when can

00:13:46,919 --> 00:13:54,029
you close your you're a contract account

00:13:50,339 --> 00:13:55,619
aggregation for instance yeah so this

00:13:54,029 --> 00:13:58,979
problem is solved with the concept of

00:13:55,619 --> 00:14:02,669
windows so windows split an infinite

00:13:58,979 --> 00:14:05,939
stream into finite batches so one of

00:14:02,669 --> 00:14:07,889
these batches is called a window and in

00:14:05,939 --> 00:14:09,029
this window you can then it's it's a

00:14:07,889 --> 00:14:11,669
finite thing you can compute an

00:14:09,029 --> 00:14:13,829
aggregate so it's a very simple and the

00:14:11,669 --> 00:14:16,199
different kinds of windows so the most

00:14:13,829 --> 00:14:19,289
common ones are like a tumbling window

00:14:16,199 --> 00:14:21,959
which has a fixed size and tumbling

00:14:19,289 --> 00:14:26,339
winners do not overlap so you say for

00:14:21,959 --> 00:14:28,769
instance I want to count how many orders

00:14:26,339 --> 00:14:32,129
arrived in each hour so then you start

00:14:28,769 --> 00:14:34,739
at eleven o'clock the window is then

00:14:32,129 --> 00:14:39,299
collect the data from eleven o'clock to

00:14:34,739 --> 00:14:44,009
twelve o'clock or two eleven 4959 and

00:14:39,299 --> 00:14:45,599
then once you're at eleven of 59 you

00:14:44,009 --> 00:14:47,339
then say okay this window is closed I

00:14:45,599 --> 00:14:49,049
can do my aggregation and emit the

00:14:47,339 --> 00:14:51,659
result so this is the tumbling window

00:14:49,049 --> 00:14:54,659
case they are two sliding windows which

00:14:51,659 --> 00:14:56,609
have a fixed size but you also say I

00:14:54,659 --> 00:15:00,239
want to count let's say every five

00:14:56,609 --> 00:15:02,099
minutes data worth of ten minutes then

00:15:00,239 --> 00:15:04,529
you basically start a window every five

00:15:02,099 --> 00:15:08,929
minutes and every window has the size of

00:15:04,529 --> 00:15:12,179
10 minutes and to make this in order to

00:15:08,929 --> 00:15:14,069
obtain consist of the meaningful results

00:15:12,179 --> 00:15:17,789
out of this he actually need this

00:15:14,069 --> 00:15:20,159
support for event time because if you if

00:15:17,789 --> 00:15:23,699
you have an operator which says okay

00:15:20,159 --> 00:15:25,799
collect my data worth of one hour and

00:15:23,699 --> 00:15:27,599
this one hour is basically determined by

00:15:25,799 --> 00:15:29,489
the by the operator by the machine

00:15:27,599 --> 00:15:33,029
looking at the wall clock time then this

00:15:29,489 --> 00:15:35,849
basically then the result determines or

00:15:33,029 --> 00:15:36,570
the the processing speed of this machine

00:15:35,849 --> 00:15:39,120
determines the

00:15:36,570 --> 00:15:40,890
result right so if this machine is very

00:15:39,120 --> 00:15:43,200
slow and can only consume very very

00:15:40,890 --> 00:15:44,460
little data in one hour the result will

00:15:43,200 --> 00:15:46,170
be different from a machine which is

00:15:44,460 --> 00:15:48,330
very fast they can consume a lot of data

00:15:46,170 --> 00:15:51,300
right so you want to have the result

00:15:48,330 --> 00:15:53,220
being independent of the processing

00:15:51,300 --> 00:15:56,010
speed of the system of the machine and

00:15:53,220 --> 00:15:58,560
so on and this is why this support for

00:15:56,010 --> 00:16:00,900
event time is very crucial when you use

00:15:58,560 --> 00:16:03,480
event time you look at the data in the

00:16:00,900 --> 00:16:05,420
records in the events and this one

00:16:03,480 --> 00:16:08,430
basically determines whenever you can

00:16:05,420 --> 00:16:12,900
what goes into which window and when it

00:16:08,430 --> 00:16:16,650
window can be closed so here's exactly

00:16:12,900 --> 00:16:18,900
this example here counting orders by by

00:16:16,650 --> 00:16:23,430
our so we have this event stream here

00:16:18,900 --> 00:16:25,770
and the events arriving at the system we

00:16:23,430 --> 00:16:27,720
have a two o'clock from two o'clock to

00:16:25,770 --> 00:16:30,270
three o'clock that's just one order

00:16:27,720 --> 00:16:34,710
arriving so here the purple envelope so

00:16:30,270 --> 00:16:35,940
when the logical time of the FD of the

00:16:34,710 --> 00:16:38,010
stream processor arrives at three

00:16:35,940 --> 00:16:39,900
o'clock it says okay let me count what I

00:16:38,010 --> 00:16:42,420
got in this last hour and then it says

00:16:39,900 --> 00:16:46,500
okay I just got 11 order so the result

00:16:42,420 --> 00:16:49,800
is 1 and so on for the others how can

00:16:46,500 --> 00:16:51,000
that be expressed in string sequel so

00:16:49,800 --> 00:16:53,400
basically the work that we're currently

00:16:51,000 --> 00:16:55,650
doing the fling community so here's a

00:16:53,400 --> 00:16:59,250
simple simple sequel statement that

00:16:55,650 --> 00:17:02,640
exactly performs this computation so we

00:16:59,250 --> 00:17:03,750
have a select stream which is a little

00:17:02,640 --> 00:17:05,370
bit different from the from regular

00:17:03,750 --> 00:17:07,230
secret so we have this stream key words

00:17:05,370 --> 00:17:08,810
here which basically says okay and the

00:17:07,230 --> 00:17:11,459
result is going to be a stream and the

00:17:08,810 --> 00:17:15,600
and the table from which I read is also

00:17:11,459 --> 00:17:17,640
stream it says from events events is the

00:17:15,600 --> 00:17:19,260
stream the input stream like a in

00:17:17,640 --> 00:17:21,630
regular secure this would be the like

00:17:19,260 --> 00:17:23,100
the table then we have the way across

00:17:21,630 --> 00:17:25,860
where you put a put a predicate and

00:17:23,100 --> 00:17:28,530
local predicate we say okay we only

00:17:25,860 --> 00:17:31,650
interested in the events with the status

00:17:28,530 --> 00:17:33,150
received so we only want to count how

00:17:31,650 --> 00:17:35,940
many orders arrived at the warehouse and

00:17:33,150 --> 00:17:38,750
then in the group by clause there is

00:17:35,940 --> 00:17:41,930
something special right so there's this

00:17:38,750 --> 00:17:46,530
tumbler function which basically

00:17:41,930 --> 00:17:50,190
computes window ID a we know d so the ID

00:17:46,530 --> 00:17:51,660
in which window a certain record where

00:17:50,190 --> 00:17:53,730
will be aggregated and hear you say

00:17:51,660 --> 00:17:55,830
tumble times Tim so we're looking at the

00:17:53,730 --> 00:18:00,240
timestamp column of the events so we

00:17:55,830 --> 00:18:04,040
want to make this I want one to use the

00:18:00,240 --> 00:18:06,930
time of time of the event to impugn the

00:18:04,040 --> 00:18:11,850
aggregates and then we say interval one

00:18:06,930 --> 00:18:15,240
hour which basically means okay so every

00:18:11,850 --> 00:18:19,280
hour so I basically round down the the

00:18:15,240 --> 00:18:22,470
timestamp for each hour and with each

00:18:19,280 --> 00:18:24,210
for for every record that arrives with a

00:18:22,470 --> 00:18:28,410
certain times am I can put this record

00:18:24,210 --> 00:18:30,630
in exactly 11 bin one group over which I

00:18:28,410 --> 00:18:33,060
can aggregate and the select class you

00:18:30,630 --> 00:18:36,390
if you see the current star as cancer

00:18:33,060 --> 00:18:38,970
this is like a regular aggregation

00:18:36,390 --> 00:18:41,460
function which is evaluated on the

00:18:38,970 --> 00:18:43,440
window and then it says also a tumble

00:18:41,460 --> 00:18:46,080
start which is just a function to

00:18:43,440 --> 00:18:51,960
determine the beginning timestamp of the

00:18:46,080 --> 00:18:55,610
window yeah this slide is a little bit

00:18:51,960 --> 00:18:58,620
about the implementation details behind

00:18:55,610 --> 00:19:01,590
how flink is going to going to implement

00:18:58,620 --> 00:19:04,200
this stream sequel feature so we are

00:19:01,590 --> 00:19:10,440
heavily leveraging a patrick outside

00:19:04,200 --> 00:19:12,780
care that is a is a sequel parza and

00:19:10,440 --> 00:19:16,140
planar framework so it comes with an as

00:19:12,780 --> 00:19:20,840
its own sequel optimizer and it's very

00:19:16,140 --> 00:19:24,180
very extendable so we are using that so

00:19:20,840 --> 00:19:26,640
it also has its own catalog so we you

00:19:24,180 --> 00:19:28,830
can register data streams or data set so

00:19:26,640 --> 00:19:32,790
the same feature can also be used for

00:19:28,830 --> 00:19:36,150
for sequel of a static data sets and

00:19:32,790 --> 00:19:39,170
when a secret curie comes we are the

00:19:36,150 --> 00:19:41,160
cats at pazzo takes the query so the

00:19:39,170 --> 00:19:43,890
syndics are shot before is exactly

00:19:41,160 --> 00:19:47,640
basically taken from from the cat side

00:19:43,890 --> 00:19:50,160
project it passes the secret theory it

00:19:47,640 --> 00:19:52,740
looks up the tables in the cat's a

00:19:50,160 --> 00:19:56,370
catalog it then generates a logical plan

00:19:52,740 --> 00:19:57,720
so whoever is familiar with curie up

00:19:56,370 --> 00:20:00,330
Tomas a shin this is basically like a

00:19:57,720 --> 00:20:02,550
blueprint for a query optimizer we then

00:20:00,330 --> 00:20:03,750
pass the logical plan to the cats at

00:20:02,550 --> 00:20:05,430
optimizer and pain

00:20:03,750 --> 00:20:07,350
on whether the query will be executed on

00:20:05,430 --> 00:20:08,940
a stream or on a data set so whether

00:20:07,350 --> 00:20:10,820
it's going to be analyzed aesthetic data

00:20:08,940 --> 00:20:13,020
or streaming data we use different

00:20:10,820 --> 00:20:16,940
optimizations for the optimization and

00:20:13,020 --> 00:20:19,650
then generate either regular data set

00:20:16,940 --> 00:20:21,960
API plan weather data stream API plan so

00:20:19,650 --> 00:20:26,130
all of this is basically compiled down

00:20:21,960 --> 00:20:27,660
to the regular fling ApS they did a set

00:20:26,130 --> 00:20:30,620
at API for batch processing a data

00:20:27,660 --> 00:20:33,870
stream it we have for stream processing

00:20:30,620 --> 00:20:37,110
all right so with this I'm handing over

00:20:33,870 --> 00:20:39,630
to term he's going to talk about the CP

00:20:37,110 --> 00:20:41,910
side and how CP and stream seeker can be

00:20:39,630 --> 00:20:44,880
integrated yeah thnkx oven for the great

00:20:41,910 --> 00:20:46,530
introduction to stream sequel I'm till

00:20:44,880 --> 00:20:49,860
and now I'm going to talk a little bit

00:20:46,530 --> 00:20:53,660
about Madame pattern detection on event

00:20:49,860 --> 00:20:56,940
streams using complex event processing

00:20:53,660 --> 00:20:58,560
so consider again the use case fargin

00:20:56,940 --> 00:21:01,830
just presented to you the ala

00:20:58,560 --> 00:21:04,320
fulfillment but this times we not only

00:21:01,830 --> 00:21:06,690
want to count the number of orders we've

00:21:04,320 --> 00:21:08,880
received within the last hour but we

00:21:06,690 --> 00:21:12,600
want to monitor all the processes a

00:21:08,880 --> 00:21:14,610
little bit more so for example we would

00:21:12,600 --> 00:21:17,790
be interested in detecting when a

00:21:14,610 --> 00:21:21,210
delivery is delayed or even lost so that

00:21:17,790 --> 00:21:23,300
we can inform the customer or center the

00:21:21,210 --> 00:21:27,900
customer notification to improve his

00:21:23,300 --> 00:21:30,180
user experience or other interesting

00:21:27,900 --> 00:21:34,170
information would be how long does it

00:21:30,180 --> 00:21:37,710
actually take from receiving an order to

00:21:34,170 --> 00:21:40,290
preparing the item to giving it to the

00:21:37,710 --> 00:21:45,000
delivery service and that's that can be

00:21:40,290 --> 00:21:46,710
done using complex event processing but

00:21:45,000 --> 00:21:49,620
first we have to define for the

00:21:46,710 --> 00:21:51,930
different processes on the service level

00:21:49,620 --> 00:21:54,480
agreements to distinguish when process

00:21:51,930 --> 00:21:58,290
is delayed or when it is successfully

00:21:54,480 --> 00:22:01,920
completed furthermore we introduce for

00:21:58,290 --> 00:22:07,620
new events which represents the assaults

00:22:01,920 --> 00:22:11,820
of the DA the CP computation so we have

00:22:07,620 --> 00:22:14,670
a person success event which tells that

00:22:11,820 --> 00:22:16,530
the the receiving an order and the

00:22:14,670 --> 00:22:17,340
preparation of the item giving to the

00:22:16,530 --> 00:22:20,270
delivery sir

00:22:17,340 --> 00:22:22,920
us was successfully completed in time

00:22:20,270 --> 00:22:25,800
and then we have the process warning

00:22:22,920 --> 00:22:27,960
event which does that it has not been

00:22:25,800 --> 00:22:31,680
completed in time and the same holds

00:22:27,960 --> 00:22:33,960
true for all the same things for the

00:22:31,680 --> 00:22:35,610
delivery process so when you see for

00:22:33,960 --> 00:22:38,430
example the delivery warning event you

00:22:35,610 --> 00:22:42,330
know that you can tell the customer that

00:22:38,430 --> 00:22:49,920
is shipment might not be or delivered in

00:22:42,330 --> 00:22:51,510
time since all theory is great let's

00:22:49,920 --> 00:22:54,990
take a look at a concrete example so we

00:22:51,510 --> 00:22:58,590
have again our stream of input events

00:22:54,990 --> 00:23:01,350
where the letters are the orders the

00:22:58,590 --> 00:23:05,760
parcels are the the shipment events and

00:23:01,350 --> 00:23:09,240
the t-shirts denote the successfully

00:23:05,760 --> 00:23:13,170
delivered items events so what we are

00:23:09,240 --> 00:23:15,690
now looking for is events which don't

00:23:13,170 --> 00:23:17,430
have a corresponding successor so for

00:23:15,690 --> 00:23:21,720
example if you take a look at the blue

00:23:17,430 --> 00:23:24,000
parcel it is not succeeded by a blue

00:23:21,720 --> 00:23:27,060
t-shirt which would denote that this

00:23:24,000 --> 00:23:30,600
parcel has been given to the customer so

00:23:27,060 --> 00:23:33,390
if we don't see such an event within a

00:23:30,600 --> 00:23:36,210
given time interval so for our use case

00:23:33,390 --> 00:23:38,730
I think I've we will see later on this

00:23:36,210 --> 00:23:41,340
light that's one day we want to generate

00:23:38,730 --> 00:23:44,850
this delivery warning and the same

00:23:41,340 --> 00:23:47,100
applies to the to the all events if we

00:23:44,850 --> 00:23:49,190
don't process the orders in time then

00:23:47,100 --> 00:23:52,410
there are some problems in our warehouse

00:23:49,190 --> 00:23:54,030
which we have to fix so these these

00:23:52,410 --> 00:23:56,360
warning events can trigger some kind of

00:23:54,030 --> 00:24:02,160
countermeasures to solve these problems

00:23:56,360 --> 00:24:04,110
but we are also interested in a

00:24:02,160 --> 00:24:06,510
successfully completed event so if you

00:24:04,110 --> 00:24:08,220
take a look at the green letter we see

00:24:06,510 --> 00:24:09,810
that it is succeeded by a green passer

00:24:08,220 --> 00:24:11,760
which denotes that the internal

00:24:09,810 --> 00:24:14,460
processing has been completed now we

00:24:11,760 --> 00:24:16,740
interested in the time it took and the

00:24:14,460 --> 00:24:22,290
same applies to the green passer which

00:24:16,740 --> 00:24:26,220
is succeeded by the green t-shirt okay

00:24:22,290 --> 00:24:29,150
how would we model that with flink so

00:24:26,220 --> 00:24:30,990
first of all we have to define what

00:24:29,150 --> 00:24:33,960
successful

00:24:30,990 --> 00:24:37,770
processing actions mean posting action

00:24:33,960 --> 00:24:40,770
means that is if we see if we receive an

00:24:37,770 --> 00:24:44,670
audit event we also want to we see or we

00:24:40,770 --> 00:24:47,580
want to see here a blue passer which is

00:24:44,670 --> 00:24:49,920
a shipment event within one hour for

00:24:47,580 --> 00:24:52,710
example if you see that we are safe and

00:24:49,920 --> 00:24:57,360
we can generate our processing success

00:24:52,710 --> 00:25:00,840
event if we see the auto event and it is

00:24:57,360 --> 00:25:04,550
not followed by this shipment event we

00:25:00,840 --> 00:25:07,860
want to generate the process warning so

00:25:04,550 --> 00:25:11,640
in order to define the the success at

00:25:07,860 --> 00:25:13,800
pattern we have to we have to do the

00:25:11,640 --> 00:25:17,190
following with link so first of all we

00:25:13,800 --> 00:25:19,230
define a pattern consisting of a

00:25:17,190 --> 00:25:25,130
starting event which is identified by

00:25:19,230 --> 00:25:30,480
the name received and additionally has a

00:25:25,130 --> 00:25:33,900
subtype constraint so event is the super

00:25:30,480 --> 00:25:37,260
type of all events so the order shipment

00:25:33,900 --> 00:25:39,870
a delivery event and so without the

00:25:37,260 --> 00:25:43,890
subtype event every event would match to

00:25:39,870 --> 00:25:46,050
the first received event but since we

00:25:43,890 --> 00:25:48,750
are interested in all events we tell the

00:25:46,050 --> 00:25:55,140
system that the event has to be a

00:25:48,750 --> 00:25:57,210
subtype order next we say that we the

00:25:55,140 --> 00:26:02,220
audit event has to be followed by an

00:25:57,210 --> 00:26:06,720
event called shipped here this this

00:26:02,220 --> 00:26:09,600
event must be a shipment event otherwise

00:26:06,720 --> 00:26:11,490
it wouldn't be a valid pattern so we

00:26:09,600 --> 00:26:13,770
could have specified again a subtype

00:26:11,490 --> 00:26:18,950
constraint but to show you that you can

00:26:13,770 --> 00:26:21,960
also specify more complex conditions I

00:26:18,950 --> 00:26:24,360
used the where clause where I checked

00:26:21,960 --> 00:26:27,230
the event or the status of the event

00:26:24,360 --> 00:26:30,300
that the Stella slee equals shipped

00:26:27,230 --> 00:26:33,720
which is the case for all shipment

00:26:30,300 --> 00:26:37,140
events and additionally we define a time

00:26:33,720 --> 00:26:40,559
interval for a valid pattern so we say

00:26:37,140 --> 00:26:43,520
that the pattern must occur within one

00:26:40,559 --> 00:26:46,820
hour otherwise it times out

00:26:43,520 --> 00:26:49,280
okay after we've defined this pattern we

00:26:46,820 --> 00:26:53,300
apply it on our input stream of events

00:26:49,280 --> 00:26:57,010
which is keyed by the odd ID because we

00:26:53,300 --> 00:26:59,750
want to do I want to do this this

00:26:57,010 --> 00:27:05,510
pattern matching for every order

00:26:59,750 --> 00:27:07,310
individually and the result is this

00:27:05,510 --> 00:27:09,410
processing pattern stream on which we

00:27:07,310 --> 00:27:12,740
can apply a Select clause which

00:27:09,410 --> 00:27:16,940
basically generates the result of the

00:27:12,740 --> 00:27:20,150
these complex event processing so here

00:27:16,940 --> 00:27:21,830
as user one defines two functions the

00:27:20,150 --> 00:27:24,760
first function is the timeout handler

00:27:21,830 --> 00:27:27,830
which is called for every partial match

00:27:24,760 --> 00:27:30,740
which has not been completed and the

00:27:27,830 --> 00:27:35,330
given time interval of one hour in this

00:27:30,740 --> 00:27:37,520
case and since we wanted to generate the

00:27:35,330 --> 00:27:41,930
process warning in this case we simply

00:27:37,520 --> 00:27:45,080
return a process one event the side of

00:27:41,930 --> 00:27:49,580
this time attempt on the second function

00:27:45,080 --> 00:27:51,170
till this select call is the user select

00:27:49,580 --> 00:27:55,090
function which is called for every

00:27:51,170 --> 00:27:58,460
successfully match pattern sequence and

00:27:55,090 --> 00:28:01,820
here we simply return the poster success

00:27:58,460 --> 00:28:03,770
event with the all ID the timestamp of

00:28:01,820 --> 00:28:07,370
the shipment event and the duration it

00:28:03,770 --> 00:28:10,010
took form from like receiving the order

00:28:07,370 --> 00:28:12,080
to sending the item to the delivery

00:28:10,010 --> 00:28:15,740
serves are giving the item to the

00:28:12,080 --> 00:28:20,300
delivery service and we basically do the

00:28:15,740 --> 00:28:22,070
same for for the delivery success and

00:28:20,300 --> 00:28:29,120
warning events as well just with a

00:28:22,070 --> 00:28:32,870
different like time devil okay so far so

00:28:29,120 --> 00:28:35,570
good we've seen how we can analyze event

00:28:32,870 --> 00:28:38,150
streams and generate we searched from it

00:28:35,570 --> 00:28:42,320
using a complex event processing but

00:28:38,150 --> 00:28:45,320
what if we are also interested in like

00:28:42,320 --> 00:28:49,520
calculating aggregations on these newly

00:28:45,320 --> 00:28:54,080
generated events that could be

00:28:49,520 --> 00:28:57,170
interesting for example to to calculate

00:28:54,080 --> 00:28:59,990
the number of of delayed shipments

00:28:57,170 --> 00:29:03,260
day so that you can assess the

00:28:59,990 --> 00:29:07,600
reliability of your delivery service for

00:29:03,260 --> 00:29:12,260
example here in our example we see that

00:29:07,600 --> 00:29:14,360
that we have in the first day we've to

00:29:12,260 --> 00:29:17,990
deliver your warnings which means that

00:29:14,360 --> 00:29:20,000
two deliveries we're delayed and in the

00:29:17,990 --> 00:29:23,050
on this second day we see there were

00:29:20,000 --> 00:29:26,600
three so it's basically counting these

00:29:23,050 --> 00:29:30,080
the results of the CP the delivery

00:29:26,600 --> 00:29:34,310
warnings another interesting metric for

00:29:30,080 --> 00:29:36,610
the business could be the average and

00:29:34,310 --> 00:29:41,200
processing time so how long does it take

00:29:36,610 --> 00:29:42,950
um from while receiving the item to

00:29:41,200 --> 00:29:47,960
processing it and giving it to the

00:29:42,950 --> 00:29:50,090
delivery service on average mmm so it's

00:29:47,960 --> 00:29:56,030
this kind of aggregation is a natural

00:29:50,090 --> 00:29:57,290
fit for for for stream secret so what we

00:29:56,030 --> 00:30:00,980
are doing now is combining both

00:29:57,290 --> 00:30:04,370
approaches we first have as a result a

00:30:00,980 --> 00:30:07,040
data stream of either delivery warnings

00:30:04,370 --> 00:30:11,900
or delivery successes as we side of our

00:30:07,040 --> 00:30:15,920
previous CP processing and we know mmm

00:30:11,900 --> 00:30:17,600
sit out sorry the delivery success

00:30:15,920 --> 00:30:22,160
events because we only want to count the

00:30:17,600 --> 00:30:24,890
delivery warnings next we have to

00:30:22,160 --> 00:30:26,960
register this data stream giving it a

00:30:24,890 --> 00:30:30,920
unique name to to a reference it from a

00:30:26,960 --> 00:30:34,090
stream sequel statement and last but not

00:30:30,920 --> 00:30:37,460
least we can apply our stream sequel or

00:30:34,090 --> 00:30:40,940
swim secure query the one you've already

00:30:37,460 --> 00:30:43,970
seen for counting like orders per hour

00:30:40,940 --> 00:30:49,940
just this time delayed shipments per day

00:30:43,970 --> 00:30:55,030
I think yes on our data stream of

00:30:49,940 --> 00:30:55,030
delivery warnings that's it basically

00:30:55,360 --> 00:31:02,420
that's quite nice hmm but it still feels

00:30:58,790 --> 00:31:03,950
a bit clumsy right it's I mean on the

00:31:02,420 --> 00:31:05,420
one hand you have this the stream seeker

00:31:03,950 --> 00:31:07,310
which is really a declarative and easy

00:31:05,420 --> 00:31:09,740
to use and on the other hand you still

00:31:07,310 --> 00:31:10,840
have to program java so it would be

00:31:09,740 --> 00:31:12,940
better to

00:31:10,840 --> 00:31:15,040
have like all the clarity of language

00:31:12,940 --> 00:31:17,770
for both things so what we are currently

00:31:15,040 --> 00:31:22,000
working on and what's our vision is to

00:31:17,770 --> 00:31:26,080
unify the CP and swim sequence so that

00:31:22,000 --> 00:31:29,920
we have a CP and which stream sequel

00:31:26,080 --> 00:31:32,410
language and with this language the

00:31:29,920 --> 00:31:34,600
second use case like calculating the

00:31:32,410 --> 00:31:37,960
average processing time I could be

00:31:34,600 --> 00:31:40,060
realized well and the way you see on the

00:31:37,960 --> 00:31:42,880
slide it's not carved in stones of my

00:31:40,060 --> 00:31:48,720
change but here we see that and we have

00:31:42,880 --> 00:31:52,720
a have nested query the inner create

00:31:48,720 --> 00:31:57,520
calculates the the pattern there we see

00:31:52,720 --> 00:32:00,910
that we want to detect one and event a

00:31:57,520 --> 00:32:03,370
which is followed by B and partitioned

00:32:00,910 --> 00:32:06,490
by order ID so that we do it per order

00:32:03,370 --> 00:32:09,550
and order by the time so that we have

00:32:06,490 --> 00:32:12,490
event I'm guarantees we say that these

00:32:09,550 --> 00:32:15,990
two events have to appear within one

00:32:12,490 --> 00:32:18,670
hour and that a the status I should be

00:32:15,990 --> 00:32:20,440
received and B should be shipped which

00:32:18,670 --> 00:32:22,720
is equivalent to a being an all day

00:32:20,440 --> 00:32:24,340
event and B being a shipment event and

00:32:22,720 --> 00:32:27,040
for these two events we calculate the

00:32:24,340 --> 00:32:30,900
duration which is then the input for the

00:32:27,040 --> 00:32:33,850
outer our query which averages them and

00:32:30,900 --> 00:32:38,110
per day that it basically that's it

00:32:33,850 --> 00:32:41,980
basically I think that's much nicer and

00:32:38,110 --> 00:32:48,640
then like mixing like Java programming

00:32:41,980 --> 00:32:51,190
with sequel okay that brings me to my

00:32:48,640 --> 00:32:52,930
confusion or to our conclusion hmm so

00:32:51,190 --> 00:32:56,130
what have you seen today you've seen

00:32:52,930 --> 00:33:00,070
that Apache fling is well suited for

00:32:56,130 --> 00:33:04,240
analytics as well as CP work lots it

00:33:00,070 --> 00:33:08,620
offers like intuitive easy-to-use api's

00:33:04,240 --> 00:33:11,170
for force secret as well as CP arm which

00:33:08,620 --> 00:33:15,660
gets you easily started and helps to get

00:33:11,170 --> 00:33:20,380
a yo-yo a problem soft on a high level

00:33:15,660 --> 00:33:23,110
and the combination of both all things

00:33:20,380 --> 00:33:25,330
allows you to two opens up

00:33:23,110 --> 00:33:28,330
the new field of applications where not

00:33:25,330 --> 00:33:30,220
only can do the usual sequel things but

00:33:28,330 --> 00:33:32,260
also can easily detect and extract

00:33:30,220 --> 00:33:36,220
temporal patterns in your event streams

00:33:32,260 --> 00:33:41,350
which well allows you to extract more

00:33:36,220 --> 00:33:43,990
information out of the data and that

00:33:41,350 --> 00:33:46,210
brings me only to two things which I

00:33:43,990 --> 00:33:48,910
left one thing is to draw attention to

00:33:46,210 --> 00:33:51,040
the upcoming think forward conference

00:33:48,910 --> 00:33:53,049
which is taking place at the same venue

00:33:51,040 --> 00:33:56,710
from the 12 to the fourteenth of

00:33:53,049 --> 00:33:58,980
September so if you have experience with

00:33:56,710 --> 00:34:02,260
flink and want to talk about it then

00:33:58,980 --> 00:34:04,870
well please submit a talk and talk about

00:34:02,260 --> 00:34:06,190
it if you're not that fired but still

00:34:04,870 --> 00:34:10,149
want to hear more and learn more about

00:34:06,190 --> 00:34:11,679
fling then get a ticket and well listen

00:34:10,149 --> 00:34:14,830
to others talking about the experience

00:34:11,679 --> 00:34:18,280
with think it's a cool cool conference I

00:34:14,830 --> 00:34:21,850
can tell you and if you can't wait that

00:34:18,280 --> 00:34:23,889
long and once you get started and

00:34:21,850 --> 00:34:28,929
working on fling and even get paid ferd

00:34:23,889 --> 00:34:30,460
then check out the curious webpage on of

00:34:28,929 --> 00:34:32,950
data absence because we are currently

00:34:30,460 --> 00:34:34,690
looking for new hires for our team yeah

00:34:32,950 --> 00:34:35,919
and thank you very much for your

00:34:34,690 --> 00:34:38,369
attention and I hope you have enjoyed

00:34:35,919 --> 00:34:38,369
the talk

00:34:41,960 --> 00:34:46,260
thank you till thank you for being for

00:34:44,310 --> 00:34:57,390
this talk and we have some time for

00:34:46,260 --> 00:34:58,950
questions so if you do window operations

00:34:57,390 --> 00:35:00,870
where do you keep state and what do you

00:34:58,950 --> 00:35:05,160
do if the state becomes excessively

00:35:00,870 --> 00:35:06,390
large so the state packet and fling is

00:35:05,160 --> 00:35:08,670
actually pluggable so there are

00:35:06,390 --> 00:35:11,700
different kinds of ways to store the

00:35:08,670 --> 00:35:13,020
state so right now we support or fling

00:35:11,700 --> 00:35:18,300
comes with two different state backends

00:35:13,020 --> 00:35:23,010
one is basically the JVM heap which is

00:35:18,300 --> 00:35:24,930
very fast and of course yeah you can run

00:35:23,010 --> 00:35:27,800
out of memory there so it doesn't grow

00:35:24,930 --> 00:35:30,900
infinite and another state back end is

00:35:27,800 --> 00:35:34,110
it's using rocks to be so which is

00:35:30,900 --> 00:35:36,630
persisting to disk so and the state is

00:35:34,110 --> 00:35:40,140
regularly back up to a stable storage

00:35:36,630 --> 00:35:43,590
like HDFS or and then when a failure

00:35:40,140 --> 00:35:51,840
occurs it's basically yeah restart from

00:35:43,590 --> 00:35:54,270
there more questions all right what's

00:35:51,840 --> 00:35:57,480
the roadmap for a stream sequel and when

00:35:54,270 --> 00:35:59,970
can we expect aggregates yeah the rock

00:35:57,480 --> 00:36:01,410
tricky question so actually what I what

00:35:59,970 --> 00:36:03,590
I've done here is not working yet so

00:36:01,410 --> 00:36:06,650
this is more like I look into the future

00:36:03,590 --> 00:36:10,560
so with the next version so fling 11

00:36:06,650 --> 00:36:12,600
we're gonna we will have a support for a

00:36:10,560 --> 00:36:14,670
very basic stream sequel support which

00:36:12,600 --> 00:36:17,160
includes filters and projections and

00:36:14,670 --> 00:36:19,910
unions so this is rather made for four

00:36:17,160 --> 00:36:25,920
transformations of transforming data

00:36:19,910 --> 00:36:28,710
stream and then we are basically address

00:36:25,920 --> 00:36:31,050
the the next features windows aggregates

00:36:28,710 --> 00:36:35,160
and possibly even join us for the next

00:36:31,050 --> 00:36:36,930
version for flink 12 so fling 11 yeah I

00:36:35,160 --> 00:36:41,820
mean in the end it's a community

00:36:36,930 --> 00:36:44,040
decision but i would say six weeks are

00:36:41,820 --> 00:36:46,950
realistic for for pushing out of 11

00:36:44,040 --> 00:36:49,260
release thanks

00:36:46,950 --> 00:37:01,440
I think we have time for one more

00:36:49,260 --> 00:37:03,960
question hi in the examples you're using

00:37:01,440 --> 00:37:05,550
the timestamp of the event so is there

00:37:03,960 --> 00:37:08,520
an easy way to deal with the delayed

00:37:05,550 --> 00:37:13,980
messages the messages that derive after

00:37:08,520 --> 00:37:16,880
the window yeah so um that's a very good

00:37:13,980 --> 00:37:20,220
point so i was a bit simplifying here so

00:37:16,880 --> 00:37:22,140
when you look at the event at the time

00:37:20,220 --> 00:37:24,060
of the event you cannot really guarantee

00:37:22,140 --> 00:37:26,430
that the events arrive in order right so

00:37:24,060 --> 00:37:28,800
the order the events might arrive out of

00:37:26,430 --> 00:37:31,200
order flick is using the concept of

00:37:28,800 --> 00:37:33,240
water marks which is basic like an upper

00:37:31,200 --> 00:37:36,119
bound for the for the current time so

00:37:33,240 --> 00:37:40,079
whenever when the operator receives a

00:37:36,119 --> 00:37:42,270
watermark or which says hey it's three

00:37:40,079 --> 00:37:45,839
o'clock no more events should appear

00:37:42,270 --> 00:37:47,849
that are before three o'clock then we

00:37:45,839 --> 00:37:50,250
close the window of course it might

00:37:47,849 --> 00:37:54,150
happen that there's still one more event

00:37:50,250 --> 00:37:56,520
or more events before that time but

00:37:54,150 --> 00:37:58,290
flick is also able to to handle these

00:37:56,520 --> 00:37:59,970
kinds of events are you you have to

00:37:58,290 --> 00:38:04,339
write custom code for it but in

00:37:59,970 --> 00:38:07,290
principle as possible thanks again and

00:38:04,339 --> 00:38:09,839
if you want to learn more about fling

00:38:07,290 --> 00:38:12,140
then just come back to this room after

00:38:09,839 --> 00:38:16,890
lunch where Dylan Fabien will have a

00:38:12,140 --> 00:38:19,680
workshop so perhaps we should rename

00:38:16,890 --> 00:38:23,490
this room from instead of saying Frank's

00:38:19,680 --> 00:38:25,760
along this will be Flinx along see you

00:38:23,490 --> 00:38:25,760

YouTube URL: https://www.youtube.com/watch?v=gvzOXyluqbo


