Title: Berlin Buzzwords 2016: Ira Cohen - Learning the learner: Using machine learning to track performance
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Ira Cohen talking about "Learning the learner: Using machine learning to track performance of machine learning algorithms".

So you’ve created some machine learning algorithms and tested them out in the lab, and they seem to be working fine. But how can you monitor them in production, especially if they are constantly learning and updating, and you have many of them? This was the challenge we faced at Anodot and I’ll talk about the interesting way we solved it. 

At Anodot, we have approximately 30 different types of machine learning algorithms, each one with its own parameters and tuning capabilities, designed to provide real time anomaly detection. Many of these are online learning algorithms, which get updated with every new piece of data that arrives. Adding to the complexity, the outputs of some of the algorithms act as the inputs others. These algorithms run constantly on the vast number of signals that are sent to our SaaS cloud (currently more than 35 million signals are reported to Anodot every 1 to 5 minutes). We knew from day one that it was crucial to track the performance of these algorithms, so we would know if something happened that improved or degraded their performance, but we were faced with a challenge – how to accomplish this?  

Read more:
https://2016.berlinbuzzwords.de/session/learning-learner-using-machine-learning-track-performance-machine-learning-algorithms

About Ira Cohen:
https://2016.berlinbuzzwords.de/users/ira-cohen

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:02,090 --> 00:00:06,899
what's she learning so I'm the chief

00:00:04,770 --> 00:00:08,420
data scientist for a small start-up

00:00:06,899 --> 00:00:10,920
called Ana dot we're based in Israel

00:00:08,420 --> 00:00:13,610
it's a startup that does anomaly

00:00:10,920 --> 00:00:18,119
detection I'll describe it a little bit

00:00:13,610 --> 00:00:20,910
in a few minutes and as a startup doing

00:00:18,119 --> 00:00:22,590
machine learning we reached quickly

00:00:20,910 --> 00:00:25,170
reached a point where we need to track

00:00:22,590 --> 00:00:28,650
what our algorithms are doing and I'll

00:00:25,170 --> 00:00:30,929
describe how we do it so a little bit

00:00:28,650 --> 00:00:32,730
about the machine learning how many

00:00:30,929 --> 00:00:34,469
people here have background in machine

00:00:32,730 --> 00:00:38,790
learning okay

00:00:34,469 --> 00:00:43,079
so quite a quite a bit so when I when I

00:00:38,790 --> 00:00:44,640
did my PhD image in uh in this area you

00:00:43,079 --> 00:00:47,070
know it used to be about publishing

00:00:44,640 --> 00:00:49,890
papers you take data set you get

00:00:47,070 --> 00:00:51,750
slightly better results and the previous

00:00:49,890 --> 00:00:54,059
guys on the data set in computer vision

00:00:51,750 --> 00:00:55,710
or text or whatever feel it was you

00:00:54,059 --> 00:00:58,440
publish your papers and I see him now

00:00:55,710 --> 00:01:00,090
and nibs you do some theoretical work it

00:00:58,440 --> 00:01:04,199
was very nice

00:01:00,090 --> 00:01:07,350
but we were this was the only people who

00:01:04,199 --> 00:01:09,750
knew what machine learning is but in the

00:01:07,350 --> 00:01:11,220
last few years as we all know there is

00:01:09,750 --> 00:01:14,970
the coming of age of machine learning

00:01:11,220 --> 00:01:18,299
and with many many applications that are

00:01:14,970 --> 00:01:20,100
actually working for large companies it

00:01:18,299 --> 00:01:22,200
started with with a lot of things around

00:01:20,100 --> 00:01:24,479
ads and recommendation systems in

00:01:22,200 --> 00:01:26,549
e-commerce and we're moving into

00:01:24,479 --> 00:01:28,650
autonomic cars that I have deep learning

00:01:26,549 --> 00:01:30,960
engines in them that are going to

00:01:28,650 --> 00:01:33,689
actually change the way our life

00:01:30,960 --> 00:01:36,780
completely so machine learning is coming

00:01:33,689 --> 00:01:39,570
of age and it's also you know sexy now

00:01:36,780 --> 00:01:41,720
to be a data scientist which is I would

00:01:39,570 --> 00:01:44,220
say a synonym for machine learning the

00:01:41,720 --> 00:01:47,329
sexiest job of the 21st century

00:01:44,220 --> 00:01:50,790
according to the Harvard Business Review

00:01:47,329 --> 00:01:53,520
that's that's nice so it's nice to be in

00:01:50,790 --> 00:01:56,969
the field that that is being appreciated

00:01:53,520 --> 00:02:01,310
today but we still have a long way to go

00:01:56,969 --> 00:02:03,750
until until it actually reaches maturity

00:02:01,310 --> 00:02:05,549
so let me talk a little bit about the

00:02:03,750 --> 00:02:09,000
practical machine learning process you

00:02:05,549 --> 00:02:10,560
when you start you start with a problem

00:02:09,000 --> 00:02:13,310
you define it you define a business

00:02:10,560 --> 00:02:15,500
problem is either it's given to you or

00:02:13,310 --> 00:02:17,120
you have to go and figure out what is

00:02:15,500 --> 00:02:18,680
the problem that is the first step and

00:02:17,120 --> 00:02:22,010
decide that machine learning can

00:02:18,680 --> 00:02:23,660
actually help solve that problem the

00:02:22,010 --> 00:02:26,690
next step is to start collecting and

00:02:23,660 --> 00:02:29,599
preparing the data and that is a very

00:02:26,690 --> 00:02:31,700
difficult step but necessarily one if

00:02:29,599 --> 00:02:34,569
you want to try out any algorithms and

00:02:31,700 --> 00:02:37,459
then after you have your data you

00:02:34,569 --> 00:02:39,590
created your features you start training

00:02:37,459 --> 00:02:41,989
and testing your models and you iterate

00:02:39,590 --> 00:02:43,400
a lot of times over that you fail for a

00:02:41,989 --> 00:02:45,080
hundred times and then one hundred and

00:02:43,400 --> 00:02:47,180
first time you actually get some decent

00:02:45,080 --> 00:02:48,650
results you're happy you have good

00:02:47,180 --> 00:02:50,569
models you believe you've found the

00:02:48,650 --> 00:02:52,730
right algorithm that will solve that

00:02:50,569 --> 00:02:55,430
business problem now you deploy it to

00:02:52,730 --> 00:02:57,560
production another step which which is

00:02:55,430 --> 00:02:59,120
not that easy now that you deploy to

00:02:57,560 --> 00:03:01,370
production you have to start tracking

00:02:59,120 --> 00:03:03,500
and monitoring it because it's actually

00:03:01,370 --> 00:03:05,480
going to be in front of people it's

00:03:03,500 --> 00:03:07,190
going to do something continuously and

00:03:05,480 --> 00:03:10,220
you have to make sure it continues to do

00:03:07,190 --> 00:03:12,560
that so that's that's the the five steps

00:03:10,220 --> 00:03:16,359
of the machine learning process and if

00:03:12,560 --> 00:03:19,519
you look at the the world out there of

00:03:16,359 --> 00:03:21,890
of tools and companies that are building

00:03:19,519 --> 00:03:23,959
machine learning capabilities both

00:03:21,890 --> 00:03:29,060
internally and providing them either as

00:03:23,959 --> 00:03:32,239
an open-source or as as actually

00:03:29,060 --> 00:03:34,370
commercial offerings then you see they

00:03:32,239 --> 00:03:35,959
focus a lot first they focused a lot on

00:03:34,370 --> 00:03:37,489
collecting and preparing the data

00:03:35,959 --> 00:03:40,760
I mean Hadoop is basically a new

00:03:37,489 --> 00:03:42,440
platform for for collecting the data and

00:03:40,760 --> 00:03:44,480
then on top of it so many things that

00:03:42,440 --> 00:03:45,650
prepare the data and you have spark and

00:03:44,480 --> 00:03:48,980
you have to affect that which is a

00:03:45,650 --> 00:03:51,769
commercial company doing data data

00:03:48,980 --> 00:03:53,989
preparation and data refinement now you

00:03:51,769 --> 00:03:56,389
have a lot of companies coming out in

00:03:53,989 --> 00:03:59,590
the last year both in the open source

00:03:56,389 --> 00:04:01,880
world and in the non open source doing

00:03:59,590 --> 00:04:04,160
automation around training and testing

00:04:01,880 --> 00:04:06,530
the models so they would they're trying

00:04:04,160 --> 00:04:08,660
to democratize it so everybody can use

00:04:06,530 --> 00:04:10,069
these models without having deep

00:04:08,660 --> 00:04:13,280
knowledge of the theory behind them

00:04:10,069 --> 00:04:14,090
tensorflow AWS machine learning Azure

00:04:13,280 --> 00:04:16,549
machine learning

00:04:14,090 --> 00:04:18,709
companies like data and spark ml lid

00:04:16,549 --> 00:04:20,180
which is an open source and of course

00:04:18,709 --> 00:04:21,859
once you train and test the model you

00:04:20,180 --> 00:04:23,479
need the mechanism that will actually

00:04:21,859 --> 00:04:26,090
deploy it to production so a lot of

00:04:23,479 --> 00:04:26,970
companies work around that and if you

00:04:26,090 --> 00:04:32,160
look at the

00:04:26,970 --> 00:04:34,050
scape and this is from March 2016 it's

00:04:32,160 --> 00:04:36,420
an analysis of the landscape of machine

00:04:34,050 --> 00:04:39,830
intelligence it's getting very very

00:04:36,420 --> 00:04:41,430
crowded and busy I mean it's small but

00:04:39,830 --> 00:04:44,130
for a reason

00:04:41,430 --> 00:04:46,110
and actually since March there were at

00:04:44,130 --> 00:04:48,540
least a few others that came into this

00:04:46,110 --> 00:04:51,930
into this world that are not in this

00:04:48,540 --> 00:04:54,030
slide so lots of tools platforms and

00:04:51,930 --> 00:04:56,820
solutions but there's actually one area

00:04:54,030 --> 00:04:59,010
the fifth step that you don't see a lot

00:04:56,820 --> 00:05:02,010
of solutions you don't see it being

00:04:59,010 --> 00:05:04,470
addressed by any of the either open

00:05:02,010 --> 00:05:06,540
source or commercial solution I deployed

00:05:04,470 --> 00:05:08,190
it I have my models in production what

00:05:06,540 --> 00:05:09,830
happens now how do I track and monitor

00:05:08,190 --> 00:05:12,930
them what do I do

00:05:09,830 --> 00:05:14,730
so this is a slide I got from data which

00:05:12,930 --> 00:05:17,760
is the company doing machine learning

00:05:14,730 --> 00:05:20,340
platform and they put in their slides

00:05:17,760 --> 00:05:22,230
well you deploy your models you evaluate

00:05:20,340 --> 00:05:24,480
them now monitor and manage them but

00:05:22,230 --> 00:05:26,340
what are the steps for doing that oh

00:05:24,480 --> 00:05:28,710
they're pretty simple actually if you

00:05:26,340 --> 00:05:30,510
think about it the first step you define

00:05:28,710 --> 00:05:32,580
what are the performance metrics and

00:05:30,510 --> 00:05:34,560
expected behaviors of your model if I

00:05:32,580 --> 00:05:36,450
deployed a recommendation engine and I

00:05:34,560 --> 00:05:39,630
tested it on a lot of training and test

00:05:36,450 --> 00:05:41,400
data I can expect some performance out

00:05:39,630 --> 00:05:43,290
of it when it goes into production and

00:05:41,400 --> 00:05:45,479
then I can track it over time and see

00:05:43,290 --> 00:05:47,250
whether that changed if I'm doing

00:05:45,479 --> 00:05:49,200
classification of faces or

00:05:47,250 --> 00:05:51,810
classifications of cats and dogs and

00:05:49,200 --> 00:05:54,270
other objects I know more or less what

00:05:51,810 --> 00:05:56,520
is my distribution of object recognition

00:05:54,270 --> 00:05:58,770
that I should expect and if that changes

00:05:56,520 --> 00:06:01,590
maybe my models go wrong

00:05:58,770 --> 00:06:03,120
or if I'm building an atonal autonomic

00:06:01,590 --> 00:06:05,280
car and I trained it on a lot of

00:06:03,120 --> 00:06:07,290
different objects and to react to a lot

00:06:05,280 --> 00:06:10,110
of different situations if there is a

00:06:07,290 --> 00:06:12,990
new situation that comes up I would want

00:06:10,110 --> 00:06:14,970
to track it with metrics detect it and

00:06:12,990 --> 00:06:18,840
then refine the models if that is

00:06:14,970 --> 00:06:21,930
necessary so but how do I do that how do

00:06:18,840 --> 00:06:23,760
I actually track and monitor all these

00:06:21,930 --> 00:06:26,340
different performance metrics for all

00:06:23,760 --> 00:06:28,380
these models that I've trained and doing

00:06:26,340 --> 00:06:32,070
it manually it's probably not an option

00:06:28,380 --> 00:06:34,680
and why because it's hard there are many

00:06:32,070 --> 00:06:36,930
models out there if I'm deploying a

00:06:34,680 --> 00:06:39,419
Phenom II karzai I cannot do it manually

00:06:36,930 --> 00:06:40,710
well I cannot wait to the data scientist

00:06:39,419 --> 00:06:42,990
detects that there is an issue

00:06:40,710 --> 00:06:44,910
that I didn't recognize the kangaroo in

00:06:42,990 --> 00:06:47,340
Australia because I never trained my

00:06:44,910 --> 00:06:49,440
models on detecting kangaroos and wait

00:06:47,340 --> 00:06:52,680
for the cars to hit it before I actually

00:06:49,440 --> 00:06:55,680
go and fix these models so it won't be

00:06:52,680 --> 00:06:57,360
an option to do it manually so what is

00:06:55,680 --> 00:06:59,310
this solution well there are a lot of

00:06:57,360 --> 00:07:01,949
solutions I'm going to propose one today

00:06:59,310 --> 00:07:03,870
and it's one that is kind of biased

00:07:01,949 --> 00:07:09,150
towards what our company is doing which

00:07:03,870 --> 00:07:10,949
is anomaly detection so so forgive me if

00:07:09,150 --> 00:07:12,840
it's a little bit biased towards that

00:07:10,949 --> 00:07:15,990
but it is a solution that works and it's

00:07:12,840 --> 00:07:19,919
not only me that that is proposing this

00:07:15,990 --> 00:07:22,770
solution out there so a little bit about

00:07:19,919 --> 00:07:24,330
an adult an adult an adult platform is

00:07:22,770 --> 00:07:26,280
not meant for tracking machine learning

00:07:24,330 --> 00:07:29,220
algorithms it's really it's a business

00:07:26,280 --> 00:07:30,570
incident detection platform we take time

00:07:29,220 --> 00:07:32,870
series signals from many different

00:07:30,570 --> 00:07:37,770
sources for many different domains

00:07:32,870 --> 00:07:40,979
either telcos IOT web services ecommerce

00:07:37,770 --> 00:07:43,020
sites industrial IOT and the platform

00:07:40,979 --> 00:07:44,550
provides a lot of analytics capability

00:07:43,020 --> 00:07:46,650
and machine learning capabilities one of

00:07:44,550 --> 00:07:49,080
them or the major one in there is

00:07:46,650 --> 00:07:50,909
anomaly detection and the reason we

00:07:49,080 --> 00:07:52,710
provide that is to detect business

00:07:50,909 --> 00:07:55,139
incidents automatically without the need

00:07:52,710 --> 00:07:57,690
of dashboards or manual configurations

00:07:55,139 --> 00:07:59,370
of static threshold alerts and all the

00:07:57,690 --> 00:08:02,099
normal things that people do in their

00:07:59,370 --> 00:08:04,979
monitoring system so some examples of

00:08:02,099 --> 00:08:06,900
business incidents for example this is a

00:08:04,979 --> 00:08:09,060
drop in number of visitors for one of

00:08:06,900 --> 00:08:12,360
our customers and you can see that

00:08:09,060 --> 00:08:15,210
they're the drop is somewhere in the

00:08:12,360 --> 00:08:18,539
middle of the towards the end of the day

00:08:15,210 --> 00:08:20,669
but not on a weekend and the reason it's

00:08:18,539 --> 00:08:22,349
an anomaly because there is a change in

00:08:20,669 --> 00:08:24,840
the temporal pattern that is very easy

00:08:22,349 --> 00:08:26,930
for our eyes to see not necessarily very

00:08:24,840 --> 00:08:29,550
easy for algorithms to track and detect

00:08:26,930 --> 00:08:31,969
this is decrease in ad conversion on

00:08:29,550 --> 00:08:34,800
Android for an for an antic company or

00:08:31,969 --> 00:08:38,430
my favorite one a price glitch in an

00:08:34,800 --> 00:08:41,870
e-commerce site where they posted a gift

00:08:38,430 --> 00:08:45,420
card worth $200 for just 15 Twitter

00:08:41,870 --> 00:08:47,820
Facebook people got the word started

00:08:45,420 --> 00:08:49,800
buying really quickly at that low price

00:08:47,820 --> 00:08:51,329
who wouldn't buy and this happens to

00:08:49,800 --> 00:08:53,940
Airlines a lot and two other ecommerce

00:08:51,329 --> 00:08:54,529
sites and then they see an increase in

00:08:53,940 --> 00:08:56,089
the number of

00:08:54,529 --> 00:08:58,399
purchases of grief gift cards which is

00:08:56,089 --> 00:09:00,920
great but then the revenue does not

00:08:58,399 --> 00:09:02,509
increase and the revenue is the graph

00:09:00,920 --> 00:09:05,540
below does not really increase as

00:09:02,509 --> 00:09:07,759
expected so that anomaly is actually a

00:09:05,540 --> 00:09:09,800
price represents a price glitch if you

00:09:07,759 --> 00:09:13,279
don't detect it really quickly you're

00:09:09,800 --> 00:09:15,019
going to lose a lot of money so this is

00:09:13,279 --> 00:09:16,819
why we built an add-on not for tracking

00:09:15,019 --> 00:09:19,639
machine learning but then on the road on

00:09:16,819 --> 00:09:21,050
the way at the data scientists it's very

00:09:19,639 --> 00:09:22,519
easy to understand why the same

00:09:21,050 --> 00:09:27,019
technology can be applied for another

00:09:22,519 --> 00:09:30,259
domain so anomaly detection really helps

00:09:27,019 --> 00:09:32,059
to detect the unknowns and this saves

00:09:30,259 --> 00:09:35,990
time and money to a lot of industries

00:09:32,059 --> 00:09:38,959
from web services IOT and security and

00:09:35,990 --> 00:09:40,639
as I mentioned it helps to also track

00:09:38,959 --> 00:09:44,360
machine learning algorithms and close

00:09:40,639 --> 00:09:45,949
the loop by detecting unknowns that you

00:09:44,360 --> 00:09:48,769
didn't think about when you created your

00:09:45,949 --> 00:09:50,540
models so you created a classifiers for

00:09:48,769 --> 00:09:52,069
detecting 20,000 objects when they're

00:09:50,540 --> 00:09:54,620
twenty thousand and one object came

00:09:52,069 --> 00:09:55,490
along you wanted to be able to detect

00:09:54,620 --> 00:09:56,930
that that happened

00:09:55,490 --> 00:09:59,149
so you can refine your models and

00:09:56,930 --> 00:10:00,649
improve them either automatically or

00:09:59,149 --> 00:10:03,050
manually better at least know about it

00:10:00,649 --> 00:10:05,720
otherwise your models and production are

00:10:03,050 --> 00:10:08,300
going to perform very poorly or if you

00:10:05,720 --> 00:10:10,370
introduced a bug like this this is a

00:10:08,300 --> 00:10:12,559
graph of classification accuracy of an

00:10:10,370 --> 00:10:14,589
algorithm and you introduced a bug at

00:10:12,559 --> 00:10:17,660
some point in time because you deployed

00:10:14,589 --> 00:10:19,670
tuning algorithm and all of a sudden

00:10:17,660 --> 00:10:21,350
your clock your accuracy drops you want

00:10:19,670 --> 00:10:23,300
to be able to detect that as quickly as

00:10:21,350 --> 00:10:26,480
possible so you can fix it and this is

00:10:23,300 --> 00:10:28,759
just like as I said coming of age of

00:10:26,480 --> 00:10:31,129
machine learning it has to go out of the

00:10:28,759 --> 00:10:33,279
lab into the real world and therefore it

00:10:31,129 --> 00:10:35,779
has to be monitored and tracked and

00:10:33,279 --> 00:10:40,670
fixed really quickly when it goes wrong

00:10:35,779 --> 00:10:42,970
just like any other software so a little

00:10:40,670 --> 00:10:45,889
bit about what is anomaly detection so

00:10:42,970 --> 00:10:47,990
anomaly detection is it's an interesting

00:10:45,889 --> 00:10:50,029
field in machine learning let me start

00:10:47,990 --> 00:10:51,800
with an exercise and see if there is

00:10:50,029 --> 00:10:57,050
somebody quick here that can detect the

00:10:51,800 --> 00:10:59,059
anomaly in this picture right there are

00:10:57,050 --> 00:11:00,920
always two people or three people that

00:10:59,059 --> 00:11:04,759
detect it really quickly it took me five

00:11:00,920 --> 00:11:06,679
minutes so don't feel bad so this is

00:11:04,759 --> 00:11:08,209
this is the anomaly here this penguin

00:11:06,679 --> 00:11:10,009
and the reason is if you

00:11:08,209 --> 00:11:13,100
haven't figured out and the eyes are

00:11:10,009 --> 00:11:16,759
pointed down so we were very good at

00:11:13,100 --> 00:11:20,179
detecting visual anomalies but this was

00:11:16,759 --> 00:11:22,759
this was a simple a simple case that

00:11:20,179 --> 00:11:24,410
were the anomaly is clear usually it's

00:11:22,759 --> 00:11:27,829
in really an ill-posed problem what is

00:11:24,410 --> 00:11:32,209
an anomaly anomaly is almost subjective

00:11:27,829 --> 00:11:34,399
so here are six pictures and and you can

00:11:32,209 --> 00:11:36,319
probably based on some definitions say

00:11:34,399 --> 00:11:38,720
that each one of them is an anomaly in

00:11:36,319 --> 00:11:41,809
some sense right there is only one male

00:11:38,720 --> 00:11:48,350
one black-and-white picture only one

00:11:41,809 --> 00:11:52,040
black woman only one an unknown sex

00:11:48,350 --> 00:11:54,800
woman so every one of these is an

00:11:52,040 --> 00:11:56,839
anomaly in some sense and it's okay so

00:11:54,800 --> 00:11:59,240
it's a defined problem but still we use

00:11:56,839 --> 00:12:02,179
it we use it because we define some sort

00:11:59,240 --> 00:12:03,889
of definition of what is normal and

00:12:02,179 --> 00:12:07,730
based on that we can say what is

00:12:03,889 --> 00:12:09,470
abnormal so I'm going to I mean we're

00:12:07,730 --> 00:12:12,350
mainly talking about anomaly detection

00:12:09,470 --> 00:12:13,670
not in images but rather in time series

00:12:12,350 --> 00:12:16,040
signal because we're tracking

00:12:13,670 --> 00:12:18,350
performance of things with metrics so

00:12:16,040 --> 00:12:20,779
metrics you can you can plot over time

00:12:18,350 --> 00:12:22,850
and really an anomaly detection in time

00:12:20,779 --> 00:12:25,730
series signal is an an unexpected change

00:12:22,850 --> 00:12:28,009
in something parallel pattern for one or

00:12:25,730 --> 00:12:30,439
more combinations of Time series signals

00:12:28,009 --> 00:12:32,420
and these are real anomalies for various

00:12:30,439 --> 00:12:35,269
type of businesses and you can see

00:12:32,420 --> 00:12:38,059
sometimes you have multiple series that

00:12:35,269 --> 00:12:40,429
are abnormal you have temporal patterns

00:12:38,059 --> 00:12:42,920
that are seasonal and you have trends

00:12:40,429 --> 00:12:46,699
that change over time and this happens

00:12:42,920 --> 00:12:51,619
and these are now these are anomalies by

00:12:46,699 --> 00:12:53,839
their models definition so a little bit

00:12:51,619 --> 00:12:56,990
about anomaly detection methods so we're

00:12:53,839 --> 00:12:58,730
on the same page so the general scheme

00:12:56,990 --> 00:13:01,220
of anomaly detection at least for time

00:12:58,730 --> 00:13:04,339
series and again I keep mentioning it's

00:13:01,220 --> 00:13:07,100
for time series is you you have your

00:13:04,339 --> 00:13:09,980
graph and the bold blue line is the

00:13:07,100 --> 00:13:11,540
actual graph and what you try to create

00:13:09,980 --> 00:13:14,629
is a model of what is the normal

00:13:11,540 --> 00:13:16,910
behavior of that graph or combination of

00:13:14,629 --> 00:13:18,799
graphs using a statistical model that is

00:13:16,910 --> 00:13:21,199
basically that choice of statistical

00:13:18,799 --> 00:13:21,800
model defines what is normal once you

00:13:21,199 --> 00:13:23,630
have that

00:13:21,800 --> 00:13:26,600
devised a statistical test to determine

00:13:23,630 --> 00:13:29,420
if any of the samples are within that

00:13:26,600 --> 00:13:32,630
are explained by the model and visually

00:13:29,420 --> 00:13:35,390
it's that that is the statistical test

00:13:32,630 --> 00:13:37,550
is shown here using that bandit light

00:13:35,390 --> 00:13:40,279
blue land band around the graph itself

00:13:37,550 --> 00:13:42,380
so any sample that is that is not

00:13:40,279 --> 00:13:45,860
explained by the model in this case goes

00:13:42,380 --> 00:13:48,260
outside the normal range is an anomaly

00:13:45,860 --> 00:13:49,880
and that's how you flag it as an anomaly

00:13:48,260 --> 00:13:52,730
and it normally can be bad and it

00:13:49,880 --> 00:13:54,620
normally can be good we make no I mean

00:13:52,730 --> 00:13:56,329
the context of an anomaly is really in

00:13:54,620 --> 00:13:58,940
the domain that you're applying it to

00:13:56,329 --> 00:14:00,920
but an anomaly is a change of the

00:13:58,940 --> 00:14:04,670
deviate a strong deviation from the

00:14:00,920 --> 00:14:08,779
normal pattern so there are two main

00:14:04,670 --> 00:14:11,540
types of anomaly detection methods one

00:14:08,779 --> 00:14:13,040
is online anomaly detection algorithms

00:14:11,540 --> 00:14:14,630
these are the most commonly used at

00:14:13,040 --> 00:14:16,640
least for time series signals when

00:14:14,630 --> 00:14:19,579
you're applying them for in real-time

00:14:16,640 --> 00:14:22,370
and in them you actually initialize the

00:14:19,579 --> 00:14:24,350
model with some samples small sample set

00:14:22,370 --> 00:14:26,240
and for each new sample you test whether

00:14:24,350 --> 00:14:27,770
it's an anomaly or not and then you

00:14:26,240 --> 00:14:31,520
update your model so these models

00:14:27,770 --> 00:14:33,170
continues to get updated and and these

00:14:31,520 --> 00:14:35,630
are the types of models we use it and a

00:14:33,170 --> 00:14:38,930
dot because if you want to scale out and

00:14:35,630 --> 00:14:42,110
B be correct for millions of these time

00:14:38,930 --> 00:14:43,970
series signals then you have to be you

00:14:42,110 --> 00:14:47,329
know you have to be adaptive and the

00:14:43,970 --> 00:14:50,959
world changes all the time some example

00:14:47,329 --> 00:14:53,120
algorithms for for people who are

00:14:50,959 --> 00:14:55,820
interested you've got simple moving

00:14:53,120 --> 00:14:57,890
averages exponential forgetting colt

00:14:55,820 --> 00:15:00,410
winter's algorithm which is double

00:14:57,890 --> 00:15:02,209
triple exponential Kalman filters a lot

00:15:00,410 --> 00:15:04,399
of these are taken from the statistics

00:15:02,209 --> 00:15:07,339
and signal processing world and not

00:15:04,399 --> 00:15:09,620
surprisingly so because a single

00:15:07,339 --> 00:15:13,430
processing deals with time-series

00:15:09,620 --> 00:15:15,410
signals by the lot the other set of

00:15:13,430 --> 00:15:17,959
algorithms are batch anomaly detection

00:15:15,410 --> 00:15:19,910
algorithm they're more there they're

00:15:17,959 --> 00:15:21,769
better for things that are more static

00:15:19,910 --> 00:15:23,449
that don't change or you don't need to

00:15:21,769 --> 00:15:26,300
run them in real-time and you don't need

00:15:23,449 --> 00:15:29,510
to get anomalies in real-time and also

00:15:26,300 --> 00:15:32,750
you can afford very large batch jobs of

00:15:29,510 --> 00:15:35,120
learning in those cases the scheme is

00:15:32,750 --> 00:15:37,490
you collect historical samples

00:15:35,120 --> 00:15:40,550
you segment the samples to kind of

00:15:37,490 --> 00:15:42,740
similar to behaving segments clustering

00:15:40,550 --> 00:15:45,560
algorithms typically are what are being

00:15:42,740 --> 00:15:47,450
used and then you cluster them to these

00:15:45,560 --> 00:15:49,790
segments using some similarity measure

00:15:47,450 --> 00:15:52,610
and that similar similarity measure is

00:15:49,790 --> 00:15:54,680
what defines normalcy in a sense and

00:15:52,610 --> 00:15:56,870
then you mark anomalies any segments or

00:15:54,680 --> 00:15:59,720
any samples that are out that are not

00:15:56,870 --> 00:16:03,020
very close to any large cluster so

00:15:59,720 --> 00:16:06,830
though that those are the anomalies some

00:16:03,020 --> 00:16:08,960
sample algorithms for that PCA one-sided

00:16:06,830 --> 00:16:11,720
support vector machines multimodal

00:16:08,960 --> 00:16:13,610
distributions hidden Markov models that

00:16:11,720 --> 00:16:16,490
are used a lot in speech but also as

00:16:13,610 --> 00:16:19,190
anomaly detectors some of the most

00:16:16,490 --> 00:16:21,830
commercial common ones are variations of

00:16:19,190 --> 00:16:24,589
k-means DB scan meaning shift algorithms

00:16:21,830 --> 00:16:26,390
and again you use the clustering you

00:16:24,589 --> 00:16:28,550
find the clusters and then you find the

00:16:26,390 --> 00:16:32,390
outliers that are outside of these

00:16:28,550 --> 00:16:37,870
clusters so I've talked about some of

00:16:32,390 --> 00:16:42,620
these algorithms now let's switch to how

00:16:37,870 --> 00:16:44,240
the topic of today how do we use anomaly

00:16:42,620 --> 00:16:45,529
detection to track machine learning

00:16:44,240 --> 00:16:49,520
algorithms so I'm going to talk about

00:16:45,529 --> 00:16:51,320
Anna duck as a use case but you can

00:16:49,520 --> 00:16:53,750
think about it it's something you can

00:16:51,320 --> 00:16:55,670
use for any other system that has

00:16:53,750 --> 00:16:57,560
machine learning so our system in

00:16:55,670 --> 00:17:00,620
general has five steps to detect

00:16:57,560 --> 00:17:02,420
anomalies in the first step we collect

00:17:00,620 --> 00:17:05,929
all these time series signals in real

00:17:02,420 --> 00:17:07,790
time and then we learn continuously the

00:17:05,929 --> 00:17:09,890
normal behavior models and here actually

00:17:07,790 --> 00:17:11,750
are hidden about twelve different

00:17:09,890 --> 00:17:15,439
algorithms plus a classification

00:17:11,750 --> 00:17:18,770
algorithm because we we have to like I

00:17:15,439 --> 00:17:20,990
said not the choice of model that you

00:17:18,770 --> 00:17:23,179
have will define what is normal and

00:17:20,990 --> 00:17:26,660
there are so many types of different

00:17:23,179 --> 00:17:28,280
signals not all of them are very not all

00:17:26,660 --> 00:17:31,160
them are the same so you have to

00:17:28,280 --> 00:17:33,140
classify it first to what is what what

00:17:31,160 --> 00:17:36,170
normal distribution fits it best and

00:17:33,140 --> 00:17:38,480
then apply that so we have a bunch of

00:17:36,170 --> 00:17:40,340
algorithms there we have a second level

00:17:38,480 --> 00:17:43,130
learning that learns to differentiate

00:17:40,340 --> 00:17:45,650
between anomaly patterns so that's

00:17:43,130 --> 00:17:47,300
another set of algorithmic models the

00:17:45,650 --> 00:17:48,710
fourth step we actually learn the

00:17:47,300 --> 00:17:50,899
relationships between it

00:17:48,710 --> 00:17:54,110
all these different signals and that

00:17:50,899 --> 00:17:56,240
that is a set of different clustering

00:17:54,110 --> 00:17:57,440
algorithms that we use internally and at

00:17:56,240 --> 00:17:59,990
the end we do semi-supervised learning

00:17:57,440 --> 00:18:02,779
by taking feedback from our users both

00:17:59,990 --> 00:18:05,059
implicit and explicit and improving the

00:18:02,779 --> 00:18:07,490
the models based on that so we have a

00:18:05,059 --> 00:18:09,620
chain of a bunch of different algorithms

00:18:07,490 --> 00:18:12,350
just to give you some statistics and the

00:18:09,620 --> 00:18:14,270
architecture of what we have so you can

00:18:12,350 --> 00:18:16,070
gaze at the architecture but I'll give

00:18:14,270 --> 00:18:19,190
you the statistics so we process today

00:18:16,070 --> 00:18:21,559
over three billion samples a day about

00:18:19,190 --> 00:18:23,779
75 million different time series signals

00:18:21,559 --> 00:18:26,720
this is from all our customers each one

00:18:23,779 --> 00:18:28,250
of these metrics has it at least two

00:18:26,720 --> 00:18:30,679
models one for the normal distribution

00:18:28,250 --> 00:18:32,750
one for the abnormal sometimes it has

00:18:30,679 --> 00:18:35,690
three if it's in transition between the

00:18:32,750 --> 00:18:38,720
models these models get updated with

00:18:35,690 --> 00:18:40,909
every sample that comes in we have about

00:18:38,720 --> 00:18:43,370
300 million correlation leagues that are

00:18:40,909 --> 00:18:46,179
updated daily so we run the correlation

00:18:43,370 --> 00:18:49,399
algorithms every day seven million

00:18:46,179 --> 00:18:51,830
models that take into account different

00:18:49,399 --> 00:18:55,070
seasonal patterns that get updated every

00:18:51,830 --> 00:18:57,350
day 30 types of learning algorithms from

00:18:55,070 --> 00:18:59,120
classification to seasonality detection

00:18:57,350 --> 00:19:02,149
trend based signing and they all have to

00:18:59,120 --> 00:19:04,549
interact to produce the result to our

00:19:02,149 --> 00:19:07,549
customers which is find anomalies on

00:19:04,549 --> 00:19:09,590
their side so there is a chain of

00:19:07,549 --> 00:19:12,110
algorithms and we have to monitor all of

00:19:09,590 --> 00:19:13,909
them it's not just one algorithm that we

00:19:12,110 --> 00:19:15,860
keep track of its performance we have to

00:19:13,909 --> 00:19:17,510
track all of them and make sure they

00:19:15,860 --> 00:19:21,230
work in concert to produce the right

00:19:17,510 --> 00:19:23,960
result that's that's not not very simple

00:19:21,230 --> 00:19:26,539
and the architecture is complex I mean

00:19:23,960 --> 00:19:28,399
we use a lot of open sources if

00:19:26,539 --> 00:19:32,750
anybody's interested afterwards I can I

00:19:28,399 --> 00:19:34,130
can describe it in more detail so how do

00:19:32,750 --> 00:19:36,200
we track the performance of our

00:19:34,130 --> 00:19:39,260
algorithms we track them with metrics as

00:19:36,200 --> 00:19:42,649
I mentioned and we have and this is an

00:19:39,260 --> 00:19:47,630
example of an anomaly we had recently

00:19:42,649 --> 00:19:49,610
where we deployed something wrong in

00:19:47,630 --> 00:19:52,789
some of the algorithms and it affected

00:19:49,610 --> 00:19:54,620
all of them so we had an increase in the

00:19:52,789 --> 00:19:57,830
nut in the distribution a change in the

00:19:54,620 --> 00:20:00,860
distribution of seasons detected for all

00:19:57,830 --> 00:20:02,210
the metrics we had a lot of model

00:20:00,860 --> 00:20:04,460
switching that happened

00:20:02,210 --> 00:20:06,290
identity between different models that

00:20:04,460 --> 00:20:09,170
we have so our classifier wasn't working

00:20:06,290 --> 00:20:10,880
well our anomaly quality scores also

00:20:09,170 --> 00:20:13,550
changed and you can see it in the drop

00:20:10,880 --> 00:20:15,980
there so basically this anomaly was a

00:20:13,550 --> 00:20:17,960
result of a deployment and of course I

00:20:15,980 --> 00:20:20,570
could graph them in dashboards and track

00:20:17,960 --> 00:20:22,190
them manually but I don't want to do

00:20:20,570 --> 00:20:24,860
that because I have an anomaly detection

00:20:22,190 --> 00:20:29,390
system so why not let it do the job so

00:20:24,860 --> 00:20:31,310
of course we set up our means to get a

00:20:29,390 --> 00:20:34,490
lot of anomaly alert these are some

00:20:31,310 --> 00:20:36,320
additional examples so this is this is

00:20:34,490 --> 00:20:38,960
an alert we got and this is an email

00:20:36,320 --> 00:20:40,520
alert for my for my inbox showing that

00:20:38,960 --> 00:20:44,750
there is an abnormal increase in the

00:20:40,520 --> 00:20:47,000
number of high score anomalies and high

00:20:44,750 --> 00:20:49,370
score are normally metrics and when we

00:20:47,000 --> 00:20:51,860
get and and when I see this you know I

00:20:49,370 --> 00:20:53,540
don't know these numbers I couldn't even

00:20:51,860 --> 00:20:55,520
think about how to set a static

00:20:53,540 --> 00:20:57,440
threshold for that but I when I see this

00:20:55,520 --> 00:21:00,410
jump of an increase I know there must be

00:20:57,440 --> 00:21:02,300
something wrong either customers really

00:21:00,410 --> 00:21:04,280
sent us different data that caused this

00:21:02,300 --> 00:21:06,440
or something went wrong with the

00:21:04,280 --> 00:21:08,300
algorithm and in this case we actually

00:21:06,440 --> 00:21:09,920
realize what that we need a new type of

00:21:08,300 --> 00:21:12,860
model and we developed a new type of

00:21:09,920 --> 00:21:15,050
model so it will take into account this

00:21:12,860 --> 00:21:19,430
new behavior that one of the customers

00:21:15,050 --> 00:21:22,390
actually sent to us another example this

00:21:19,430 --> 00:21:24,860
is a normally with multiple metrics

00:21:22,390 --> 00:21:27,380
these metrics represent the distribution

00:21:24,860 --> 00:21:29,450
of seasonality detection algorithms in

00:21:27,380 --> 00:21:32,600
our system actually this was a good

00:21:29,450 --> 00:21:36,200
anomaly you can see it more or less May

00:21:32,600 --> 00:21:37,880
24th 25th we deployed a new type of an

00:21:36,200 --> 00:21:40,520
improvement or sin anomaly detection

00:21:37,880 --> 00:21:43,490
algorithm and it actually we saw the

00:21:40,520 --> 00:21:46,670
distribution changed as we expected to

00:21:43,490 --> 00:21:48,980
provide better results in seasonality

00:21:46,670 --> 00:21:50,780
detection so we expected this anomaly

00:21:48,980 --> 00:21:52,820
and when we got it we were very happy

00:21:50,780 --> 00:21:55,520
but the point is we didn't have to

00:21:52,820 --> 00:21:58,880
actually track it with any dashboard we

00:21:55,520 --> 00:22:01,190
could just look at our wait for wait for

00:21:58,880 --> 00:22:08,540
the email alert and know that it

00:22:01,190 --> 00:22:12,170
happened so to summarize anomaly

00:22:08,540 --> 00:22:14,000
detection this fifth step is really a

00:22:12,170 --> 00:22:15,030
mean to start closing the loop of the

00:22:14,000 --> 00:22:17,220
machine learning

00:22:15,030 --> 00:22:19,620
so to track and monitor anomaly

00:22:17,220 --> 00:22:22,170
detection can help you it's not the

00:22:19,620 --> 00:22:25,170
final step it's the first step the first

00:22:22,170 --> 00:22:27,840
step is to really take these anomalies

00:22:25,170 --> 00:22:30,000
and actually help the machines improve

00:22:27,840 --> 00:22:31,620
themselves automatically which is

00:22:30,000 --> 00:22:32,940
probably something you would need when

00:22:31,620 --> 00:22:34,830
you have mission-critical applications

00:22:32,940 --> 00:22:36,540
where you don't have the luxury of

00:22:34,830 --> 00:22:39,780
getting alerts about anomalies and

00:22:36,540 --> 00:22:41,730
fixing them kind of in the lab so for

00:22:39,780 --> 00:22:45,330
example autonomic cars would probably

00:22:41,730 --> 00:22:47,430
require an additional step after

00:22:45,330 --> 00:22:50,670
anomalies to actually start fixing and

00:22:47,430 --> 00:22:52,680
identifying new objects so with that I

00:22:50,670 --> 00:23:00,960
will conclude thank you I'll take any

00:22:52,680 --> 00:23:03,660
questions thank you very much we have

00:23:00,960 --> 00:23:09,170
plenty of time for questions we have 15

00:23:03,660 --> 00:23:09,170
minutes I think so please fire far away

00:23:12,560 --> 00:23:16,580
so you all get coffee

00:23:20,130 --> 00:23:24,220
and there was a step in one of your

00:23:22,420 --> 00:23:27,490
slides called something like behavioral

00:23:24,220 --> 00:23:28,870
topology learning which you lost over

00:23:27,490 --> 00:23:30,220
quite quickly oh yeah can you talk a

00:23:28,870 --> 00:23:34,840
little bit more about what that involves

00:23:30,220 --> 00:23:37,330
sure so yeah I didn't want to use this

00:23:34,840 --> 00:23:42,450
talk as a self-promotion complete

00:23:37,330 --> 00:23:45,100
self-promotion to to our algorithms so

00:23:42,450 --> 00:23:47,440
so what do we do here our approach has

00:23:45,100 --> 00:23:50,080
been Bottoms Up approach to doing

00:23:47,440 --> 00:23:53,830
anomaly detection so we will first take

00:23:50,080 --> 00:23:55,480
each signal and learn that by itself do

00:23:53,830 --> 00:23:59,410
the abnormal learning and then try to

00:23:55,480 --> 00:24:01,270
combine the anomalous metrics by knowing

00:23:59,410 --> 00:24:04,290
who is related to whom so here we

00:24:01,270 --> 00:24:09,670
actually use three types of algorithms

00:24:04,290 --> 00:24:12,040
one is looking at their the correlations

00:24:09,670 --> 00:24:14,740
in normal time here we're using a

00:24:12,040 --> 00:24:17,470
variation of neural network deep

00:24:14,740 --> 00:24:21,100
learning algorithms so it's not linear

00:24:17,470 --> 00:24:23,170
correlations but basically using stacked

00:24:21,100 --> 00:24:26,110
autoencoders you can take in the signals

00:24:23,170 --> 00:24:28,360
the output is a bunch of alphabet and on

00:24:26,110 --> 00:24:31,750
that you do actually similarity between

00:24:28,360 --> 00:24:34,720
pairs of metrics so that's that's one

00:24:31,750 --> 00:24:36,940
algorithm another algorithm is looking

00:24:34,720 --> 00:24:39,940
at their correlations when they are

00:24:36,940 --> 00:24:44,350
abnormal so whenever there are anomalous

00:24:39,940 --> 00:24:49,030
metrics so let's say I'm measuring an

00:24:44,350 --> 00:24:51,220
entire system when something goes wrong

00:24:49,030 --> 00:24:53,080
there are a lot of different signals

00:24:51,220 --> 00:24:54,070
that go wrong at the same time and they

00:24:53,080 --> 00:24:56,980
become anomalous

00:24:54,070 --> 00:24:58,690
so by looking at their looking whether

00:24:56,980 --> 00:25:01,270
they're correlated they cluster together

00:24:58,690 --> 00:25:03,100
during these abnormal times rather than

00:25:01,270 --> 00:25:05,260
their behavior during normal time

00:25:03,100 --> 00:25:07,030
actually gives us a very strong hint of

00:25:05,260 --> 00:25:08,679
whether they're related or not so that

00:25:07,030 --> 00:25:10,300
that actually is one of the strongest

00:25:08,679 --> 00:25:12,429
hints that we have today it's even

00:25:10,300 --> 00:25:16,090
stronger than than the normal patterns

00:25:12,429 --> 00:25:18,100
so a lot of times they look completely

00:25:16,090 --> 00:25:20,380
different during normal times if you

00:25:18,100 --> 00:25:21,790
look at memory and CPU for example even

00:25:20,380 --> 00:25:23,110
for the same machine it might look

00:25:21,790 --> 00:25:24,910
completely different but if something

00:25:23,110 --> 00:25:29,130
goes wrong with the machine they will be

00:25:24,910 --> 00:25:32,170
anomalous regardless that's an analogy

00:25:29,130 --> 00:25:33,490
and the third one is using metadata that

00:25:32,170 --> 00:25:35,320
we get from this

00:25:33,490 --> 00:25:37,150
themselves usually the signals are not

00:25:35,320 --> 00:25:38,650
just sent as numbers but with some

00:25:37,150 --> 00:25:40,660
metadata around them and then we do

00:25:38,650 --> 00:25:43,030
clustering around that to know whether

00:25:40,660 --> 00:25:44,920
they're related or not so at the end we

00:25:43,030 --> 00:25:47,140
get this large graph with a lot of links

00:25:44,920 --> 00:25:51,450
between the metrics and that provides an

00:25:47,140 --> 00:25:51,450
input to our anomaly detection system

00:25:52,710 --> 00:26:02,110
yes um you showed would look like an

00:26:00,010 --> 00:26:03,910
email that you received once an anomaly

00:26:02,110 --> 00:26:05,080
was detected and it seems like this

00:26:03,910 --> 00:26:08,200
provides at the bottom there a way to

00:26:05,080 --> 00:26:09,700
give feedback on false positives how do

00:26:08,200 --> 00:26:11,440
you handle the detection of false

00:26:09,700 --> 00:26:14,050
negatives for quality control purposes

00:26:11,440 --> 00:26:14,470
alright so we haven't figured that out

00:26:14,050 --> 00:26:17,429
yet

00:26:14,470 --> 00:26:20,679
I mean we get them but we get them

00:26:17,429 --> 00:26:22,000
directly from the customer that says oh

00:26:20,679 --> 00:26:23,890
there was something and you didn't catch

00:26:22,000 --> 00:26:26,440
it that doesn't happen a lot because

00:26:23,890 --> 00:26:29,020
often they don't I mean even if we

00:26:26,440 --> 00:26:30,820
didn't catch it they don't know because

00:26:29,020 --> 00:26:33,340
they didn't know before

00:26:30,820 --> 00:26:36,220
and and they're more sensitive to false

00:26:33,340 --> 00:26:37,870
positives then to false negatives unless

00:26:36,220 --> 00:26:40,270
it's a really critical issue that we

00:26:37,870 --> 00:26:42,010
missed so that happens less so in those

00:26:40,270 --> 00:26:43,870
cases we actually go and do deep

00:26:42,010 --> 00:26:47,230
investigations ourselves and try to

00:26:43,870 --> 00:26:49,480
figure out you know why why our models

00:26:47,230 --> 00:26:51,610
didn't pick it up and sometimes we have

00:26:49,480 --> 00:26:54,870
to change the models so we haven't found

00:26:51,610 --> 00:26:57,520
a way to automate the false negatives

00:26:54,870 --> 00:27:02,200
this is really for the false positives

00:26:57,520 --> 00:27:04,690
and various types of false positives and

00:27:02,200 --> 00:27:08,520
also reinforces the good the true

00:27:04,690 --> 00:27:08,520
positives which is also important

00:27:16,580 --> 00:27:22,530
my question is very raised previous one

00:27:19,620 --> 00:27:26,940
how do you integrate the user feedback

00:27:22,530 --> 00:27:33,720
back to your models right so the way we

00:27:26,940 --> 00:27:35,280
take the user feedback today so we have

00:27:33,720 --> 00:27:37,140
this notion of abnormal behavior

00:27:35,280 --> 00:27:39,810
learning which I glossed over really

00:27:37,140 --> 00:27:43,890
quickly as well so that that part is

00:27:39,810 --> 00:27:45,600
actually builds a model or that ranks

00:27:43,890 --> 00:27:48,960
different anomalies and gives them a

00:27:45,600 --> 00:27:51,000
probability score based on how they look

00:27:48,960 --> 00:27:53,700
compared to all other patterns of

00:27:51,000 --> 00:27:55,200
anomalies that we've saw before and when

00:27:53,700 --> 00:27:57,030
when we get the user feedback we

00:27:55,200 --> 00:28:00,450
actually actually start changing those

00:27:57,030 --> 00:28:03,030
scores so if and this is the way we do

00:28:00,450 --> 00:28:05,730
it we do it today we have ideas of how

00:28:03,030 --> 00:28:08,280
we can integrate into other parts of the

00:28:05,730 --> 00:28:09,420
system this is the first point we tackle

00:28:08,280 --> 00:28:11,640
because it seemed to be the most

00:28:09,420 --> 00:28:14,040
relevant so if a user says this is not

00:28:11,640 --> 00:28:16,740
interesting it means this anomaly

00:28:14,040 --> 00:28:18,210
pattern is not interesting for a set of

00:28:16,740 --> 00:28:20,610
metrics that are related to each other

00:28:18,210 --> 00:28:22,260
and we start reducing this score instead

00:28:20,610 --> 00:28:26,340
of say instead of mathematics so not

00:28:22,260 --> 00:28:27,690
interesting could be well because let me

00:28:26,340 --> 00:28:31,470
give you an example if the number of

00:28:27,690 --> 00:28:33,840
visitors jumped from 20 to 100 and it's

00:28:31,470 --> 00:28:36,330
usually 20 let's say it's 20 for a month

00:28:33,840 --> 00:28:38,520
and jump to 100 and they market is not

00:28:36,330 --> 00:28:40,500
interesting mathematically speaking this

00:28:38,520 --> 00:28:43,200
looks like a very very big anomaly

00:28:40,500 --> 00:28:46,230
because it went from 20 to 100 5 times

00:28:43,200 --> 00:28:49,050
500% increase but then you ask them and

00:28:46,230 --> 00:28:52,260
they say well but this is in babbling

00:28:49,050 --> 00:28:55,080
and you know 20 to 100 visitors is

00:28:52,260 --> 00:28:58,910
nothing compared to another country like

00:28:55,080 --> 00:29:02,130
the UK or Germany were usually I have a

00:28:58,910 --> 00:29:04,740
20,000 a day so I don't care to get this

00:29:02,130 --> 00:29:06,420
anomaly so it's not that mathematically

00:29:04,740 --> 00:29:09,030
it's wrong it's really it's a false

00:29:06,420 --> 00:29:11,250
positive contextually rather than then

00:29:09,030 --> 00:29:14,760
mathematically and then that score lets

00:29:11,250 --> 00:29:16,620
us actually give it a lower weight so

00:29:14,760 --> 00:29:21,750
next time he might not get it compared

00:29:16,620 --> 00:29:23,780
to other anomalies and and the same for

00:29:21,750 --> 00:29:26,910
whether something is interesting or not

00:29:23,780 --> 00:29:27,720
something is interesting so maybe if the

00:29:26,910 --> 00:29:32,520
number of visitors

00:29:27,720 --> 00:29:34,260
jumped from 20,000 to 20,000 2020 mm it

00:29:32,520 --> 00:29:37,200
might be very interesting anomaly for

00:29:34,260 --> 00:29:38,220
that person not because mathematically

00:29:37,200 --> 00:29:40,409
it's more interesting than others

00:29:38,220 --> 00:29:45,510
because contextually and from business

00:29:40,409 --> 00:29:51,690
side it is more interesting we have time

00:29:45,510 --> 00:29:53,940
for more questions so if anybody's

00:29:51,690 --> 00:29:56,669
interested in more details about the

00:29:53,940 --> 00:29:58,320
anomaly detection part I'll be happy to

00:29:56,669 --> 00:30:00,799
provide it provide it later

00:29:58,320 --> 00:30:06,900
anything about our architecture or

00:30:00,799 --> 00:30:11,970
algorithms I'll be wrong uh we do have

00:30:06,900 --> 00:30:14,220
one more question so you use neural

00:30:11,970 --> 00:30:17,130
networks and what type of learning

00:30:14,220 --> 00:30:20,850
algorithms you're using your neural

00:30:17,130 --> 00:30:24,360
networks we're using stacked autoencoder

00:30:20,850 --> 00:30:28,559
so these are not supervised algorithms

00:30:24,360 --> 00:30:31,020
um what else that's it that's it okay so

00:30:28,559 --> 00:30:33,419
no type of genetic algorithms or

00:30:31,020 --> 00:30:35,190
something like no and not for this

00:30:33,419 --> 00:30:38,190
component we don't use genetic

00:30:35,190 --> 00:30:40,590
algorithms anywhere right now in our and

00:30:38,190 --> 00:30:44,669
how do you handle the the question of

00:30:40,590 --> 00:30:50,580
topology of your your own networks brute

00:30:44,669 --> 00:30:58,890
force we have enough data so we just the

00:30:50,580 --> 00:31:00,360
heck out of it well if we don't have any

00:30:58,890 --> 00:31:03,440
more questions let's thank our speaker

00:31:00,360 --> 00:31:03,440
zero once again

00:31:05,680 --> 00:31:13,380
and you have just got yourself five

00:31:09,280 --> 00:31:13,380

YouTube URL: https://www.youtube.com/watch?v=z2ITun71E-k


