Title: Berlin Buzzwords 2016: Tobias Kässmann - Gain speed and space with NLP in Solr #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	With 100M+ products in a single Solr index it is hard work to keep response times as low as possible. A key factor is decreasing the index size and the number of terms indexed. We try to store the significant terms only. For long product descriptions this is challenging. Applying NLP at index time is a costly operation. In this talk I’ll present a smart algorithm that run fast enough to be applied at index time.

Read more:
https://2016.berlinbuzzwords.de/session/gain-speed-and-space-nlp-solr

About Tobias Kässmann:
https://2016.berlinbuzzwords.de/users/tobias-kassmann

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:02,090 --> 00:00:07,080
hey hello my name is device and today i

00:00:05,220 --> 00:00:09,990
want to show you how you can gain gain

00:00:07,080 --> 00:00:12,269
speed in space with an LPN solar as you

00:00:09,990 --> 00:00:14,969
know my mansion I just renamed my talk

00:00:12,269 --> 00:00:16,740
to gain speed and precision with NLP and

00:00:14,969 --> 00:00:19,920
solar because I think it's a more

00:00:16,740 --> 00:00:21,330
interesting topic in this talk I first

00:00:19,920 --> 00:00:23,789
want to start with some basic

00:00:21,330 --> 00:00:25,710
information about me I'm husband and

00:00:23,789 --> 00:00:27,900
father of two beautiful children i'm

00:00:25,710 --> 00:00:31,740
from hamburg and i'm working in hamburg

00:00:27,900 --> 00:00:34,020
for shopping 24 and also the initiation

00:00:31,740 --> 00:00:36,540
initiator and organizer of the search

00:00:34,020 --> 00:00:39,030
technology meet up in hamburg so if you

00:00:36,540 --> 00:00:40,920
visit hamburg feel free to join the

00:00:39,030 --> 00:00:44,250
meetup feel free to learn something

00:00:40,920 --> 00:00:48,239
about search technology the next meetup

00:00:44,250 --> 00:00:50,760
will be on wednesday and we're having a

00:00:48,239 --> 00:00:53,219
great time with two talks from the

00:00:50,760 --> 00:00:56,460
berlin password and a third talk from

00:00:53,219 --> 00:00:59,219
ebay clannad saying we have free time if

00:00:56,460 --> 00:01:03,239
you are in hamburg feel free to join yes

00:00:59,219 --> 00:01:07,380
i want to start with informations about

00:01:03,239 --> 00:01:09,689
shopping 24 what we do basically

00:01:07,380 --> 00:01:13,439
shopping 24 is a product search website

00:01:09,689 --> 00:01:17,460
so we do not sell any product we just

00:01:13,439 --> 00:01:20,430
deliver terrific to shops and we've got

00:01:17,460 --> 00:01:23,189
a lot of different website where we show

00:01:20,430 --> 00:01:25,049
products to customers in our back end

00:01:23,189 --> 00:01:27,450
you will see on the left side we are

00:01:25,049 --> 00:01:30,960
basically using apache solr as a search

00:01:27,450 --> 00:01:33,390
entering we using spring for our api we

00:01:30,960 --> 00:01:36,270
using postgres for all of our bag and

00:01:33,390 --> 00:01:40,229
processes and varnish and github for for

00:01:36,270 --> 00:01:42,299
our projects in the last year our

00:01:40,229 --> 00:01:45,360
management team came around and said

00:01:42,299 --> 00:01:47,759
okay we've got a new requirement for a

00:01:45,360 --> 00:01:49,500
began team and we're currently

00:01:47,759 --> 00:01:52,200
absolutely fine with three million

00:01:49,500 --> 00:01:54,659
products on our website but in the next

00:01:52,200 --> 00:01:57,299
year we want to scale up to 100 million

00:01:54,659 --> 00:01:59,850
products and we are in our bag and team

00:01:57,299 --> 00:02:01,890
sit together and we're like okay now

00:01:59,850 --> 00:02:05,579
that's going to be a hard challenge but

00:02:01,890 --> 00:02:08,399
we saw and we saw three problems we saw

00:02:05,579 --> 00:02:11,640
the disk space of our solar in ecstasy

00:02:08,399 --> 00:02:13,890
indexes explode we saw that is the

00:02:11,640 --> 00:02:16,920
precision you know search we saw

00:02:13,890 --> 00:02:19,830
because when you throw another 97

00:02:16,920 --> 00:02:21,840
million products into the search index

00:02:19,830 --> 00:02:25,590
what happens with their with a precision

00:02:21,840 --> 00:02:27,390
in our search result does it preserve

00:02:25,590 --> 00:02:29,880
does it increase decrease what happens

00:02:27,390 --> 00:02:32,120
and we currently have no shouting we're

00:02:29,880 --> 00:02:36,000
now working on it but it's really hard

00:02:32,120 --> 00:02:37,950
to manage all of these products we sit

00:02:36,000 --> 00:02:41,100
together and we finally came out of this

00:02:37,950 --> 00:02:44,250
meeting this meeting and we say okay we

00:02:41,100 --> 00:02:47,670
have to reduce as a first step our data

00:02:44,250 --> 00:02:51,930
we have to reduce text to do not store

00:02:47,670 --> 00:02:55,050
the whole text that we that we've got in

00:02:51,930 --> 00:02:57,090
our products we've we store products and

00:02:55,050 --> 00:03:00,410
they consist of title description

00:02:57,090 --> 00:03:03,209
product media data attributes price

00:03:00,410 --> 00:03:05,610
something else and we saw two simple

00:03:03,209 --> 00:03:09,360
winnings ahead when we do the reduction

00:03:05,610 --> 00:03:11,840
of the text or the index data we saw

00:03:09,360 --> 00:03:16,200
that the speed will increases and the

00:03:11,840 --> 00:03:19,440
space because due to fewer index terms

00:03:16,200 --> 00:03:21,060
in our index so i want to show you in

00:03:19,440 --> 00:03:25,260
this presentation in this short talks

00:03:21,060 --> 00:03:27,630
just two solutions how we do this how do

00:03:25,260 --> 00:03:30,090
we reduce the text and the first

00:03:27,630 --> 00:03:33,060
solution is for just for product

00:03:30,090 --> 00:03:38,190
descriptions and the second solution is

00:03:33,060 --> 00:03:42,150
for usual text so we take a look into

00:03:38,190 --> 00:03:46,400
our products in our data that we store

00:03:42,150 --> 00:03:49,680
in solar and we saw that a usual

00:03:46,400 --> 00:03:51,739
description consists of about ninety

00:03:49,680 --> 00:03:56,010
five percent of irrelevant text or

00:03:51,739 --> 00:03:58,980
rename that SEO text so we've just a

00:03:56,010 --> 00:04:03,600
very very small amount of relevant text

00:03:58,980 --> 00:04:05,700
and this is pretty bad for a for a

00:04:03,600 --> 00:04:08,970
search engine because every chokin that

00:04:05,700 --> 00:04:12,390
is in the SEO or irrelevant text is a

00:04:08,970 --> 00:04:14,670
signal for our search engine and so we

00:04:12,390 --> 00:04:18,600
finally figured out SEO text equals evil

00:04:14,670 --> 00:04:21,000
equals not relevant or irrelevant what

00:04:18,600 --> 00:04:25,020
do we have in our descriptions we have a

00:04:21,000 --> 00:04:27,210
lot of sentences like okay we we have

00:04:25,020 --> 00:04:27,630
here really good-looking blue jeans and

00:04:27,210 --> 00:04:31,170
you can

00:04:27,630 --> 00:04:33,000
on these blue jeans with a great red t

00:04:31,170 --> 00:04:35,400
shirt with blue stripes and something

00:04:33,000 --> 00:04:39,450
else you can sit in it on your couch and

00:04:35,400 --> 00:04:41,850
enjoy this television with a friend so

00:04:39,450 --> 00:04:44,340
and so on we've got a lot of text that

00:04:41,850 --> 00:04:47,700
is not not relevant for a search engine

00:04:44,340 --> 00:04:49,860
and we're in a team sit together and

00:04:47,700 --> 00:04:54,810
thought about okay how can we filter out

00:04:49,860 --> 00:04:57,030
these bad sentences and we are like okay

00:04:54,810 --> 00:04:59,460
how they're really good stuff out there

00:04:57,030 --> 00:05:01,890
okay we can use the OpenAPI framework

00:04:59,460 --> 00:05:04,470
with a lot of great algorithm

00:05:01,890 --> 00:05:06,210
implementation we can use SVM's we can

00:05:04,470 --> 00:05:09,570
use deep learning technologies we can

00:05:06,210 --> 00:05:12,420
use neural network neural networks to

00:05:09,570 --> 00:05:14,430
train a model and that differ between

00:05:12,420 --> 00:05:16,590
good or bad sentences we can use

00:05:14,430 --> 00:05:19,340
keyboard instruction but we're ended up

00:05:16,590 --> 00:05:24,360
with a much much more simpler solution

00:05:19,340 --> 00:05:26,850
we blaze this solution into the last

00:05:24,360 --> 00:05:29,790
step here in the solar indexing process

00:05:26,850 --> 00:05:33,240
and this is our ahora old product

00:05:29,790 --> 00:05:35,640
pipeline at shoving 24 so we just gather

00:05:33,240 --> 00:05:37,800
the data from our customers we do some

00:05:35,640 --> 00:05:41,010
basic prepossession some normalization

00:05:37,800 --> 00:05:44,330
and after that we do some named entity

00:05:41,010 --> 00:05:50,100
recognition where we choose to recognize

00:05:44,330 --> 00:05:52,380
entities that we know but you can we

00:05:50,100 --> 00:05:54,420
also want the the unknown entities in

00:05:52,380 --> 00:05:59,760
the description so we have to use them

00:05:54,420 --> 00:06:02,820
as a signal for our search engine we

00:05:59,760 --> 00:06:04,470
placed we named our component the

00:06:02,820 --> 00:06:07,140
analyzing sentence tokenizer and we

00:06:04,470 --> 00:06:09,210
place it in a third step at our in our

00:06:07,140 --> 00:06:11,820
solar indexing process so we've got a

00:06:09,210 --> 00:06:17,010
few field type that is called text we

00:06:11,820 --> 00:06:20,040
first do some basic HTML strip charm to

00:06:17,010 --> 00:06:25,100
get rid of all of the HTML text then we

00:06:20,040 --> 00:06:28,020
do dublicate whitespace remove mminton

00:06:25,100 --> 00:06:30,900
and then comes the analyzing sentence

00:06:28,020 --> 00:06:33,360
organizer which has a one and only

00:06:30,900 --> 00:06:39,360
required I commend a list of stop words

00:06:33,360 --> 00:06:41,310
and here's how it works it's so pretty

00:06:39,360 --> 00:06:43,430
simple because it

00:06:41,310 --> 00:06:45,930
relies on the key assumption that

00:06:43,430 --> 00:06:48,090
sentences in product description with

00:06:45,930 --> 00:06:53,130
useful information do not contain a lot

00:06:48,090 --> 00:06:55,980
of stop words so how it works here we've

00:06:53,130 --> 00:06:59,370
got a description from from a product we

00:06:55,980 --> 00:07:01,170
just do another basic clean up in this

00:06:59,370 --> 00:07:03,560
cleanup we do some things to to get a

00:07:01,170 --> 00:07:06,480
better analysis to get a better

00:07:03,560 --> 00:07:08,370
sentenced organizing we split the

00:07:06,480 --> 00:07:10,740
description into sentences and then

00:07:08,370 --> 00:07:12,840
you've ended up with a simple list of

00:07:10,740 --> 00:07:16,610
sentences just iterate over these

00:07:12,840 --> 00:07:19,800
sentences and here comes a little helper

00:07:16,610 --> 00:07:25,080
if the sentence contains a lot of signs

00:07:19,800 --> 00:07:27,120
a lot of science means if the the ratio

00:07:25,080 --> 00:07:29,010
of the signs is above a given threshold

00:07:27,120 --> 00:07:31,830
then you have to split the sentence

00:07:29,010 --> 00:07:33,660
again this is a little helper to get rid

00:07:31,830 --> 00:07:36,000
of all of the the bad data we get from

00:07:33,660 --> 00:07:38,490
our customers where are no punctuation

00:07:36,000 --> 00:07:42,150
any description or something like that

00:07:38,490 --> 00:07:45,900
and after that you've got the completely

00:07:42,150 --> 00:07:48,870
splitted sentences and just build a

00:07:45,900 --> 00:07:52,050
ratio between the stopper account and a

00:07:48,870 --> 00:07:55,470
word count and check is under or above a

00:07:52,050 --> 00:08:02,580
given threshold and this works pretty

00:07:55,470 --> 00:08:05,910
well so we've got 323 learnings and the

00:08:02,580 --> 00:08:08,520
first learning was that for the sentence

00:08:05,910 --> 00:08:11,820
tokenizing process we we've got a first

00:08:08,520 --> 00:08:14,790
version of the sentence organizer where

00:08:11,820 --> 00:08:17,910
we use the sentence organized from the

00:08:14,790 --> 00:08:20,370
open NLP framework and it delivers a

00:08:17,910 --> 00:08:22,350
really really good quality but it's

00:08:20,370 --> 00:08:24,810
absolutely not as fast as a regular

00:08:22,350 --> 00:08:28,580
expression so this is our sentence

00:08:24,810 --> 00:08:32,940
organizer we do not use any fancy stuff

00:08:28,580 --> 00:08:35,850
this is our yeah this is it and the

00:08:32,940 --> 00:08:38,070
other learnings are that you do not have

00:08:35,850 --> 00:08:40,530
to analyze text that is just one

00:08:38,070 --> 00:08:42,900
sentence if you have any set of just one

00:08:40,530 --> 00:08:46,950
sentence pestle through except you have

00:08:42,900 --> 00:08:49,650
to split it again so this is very simple

00:08:46,950 --> 00:08:51,870
approach and we finally ended up with a

00:08:49,650 --> 00:08:54,590
word reduction in our descriptions of

00:08:51,870 --> 00:08:57,020
about sixty to eighty percent

00:08:54,590 --> 00:09:00,230
and the precision of our search result

00:08:57,020 --> 00:09:07,400
preserves or if not better increases to

00:09:00,230 --> 00:09:10,520
do this simple stuff yeah but what

00:09:07,400 --> 00:09:12,620
happens if you if you do not have any

00:09:10,520 --> 00:09:16,790
product description because on a product

00:09:12,620 --> 00:09:20,420
description you can you can define bad

00:09:16,790 --> 00:09:22,490
sentences by by the the ratio of a stop

00:09:20,420 --> 00:09:25,580
word in a sentence what happens if you

00:09:22,490 --> 00:09:30,530
have usual text here's an example from

00:09:25,580 --> 00:09:33,860
Wikipedia the Apache Solr entry and what

00:09:30,530 --> 00:09:35,570
happens if you have you used the

00:09:33,860 --> 00:09:38,060
analyzing sentence organizer to analyze

00:09:35,570 --> 00:09:40,550
these sentences he will throw away all

00:09:38,060 --> 00:09:42,440
of these sentences this is pretty bad

00:09:40,550 --> 00:09:45,080
but what happens if you also want to

00:09:42,440 --> 00:09:49,250
reduce the data if you also if you just

00:09:45,080 --> 00:09:52,490
want to store the relevant text or the

00:09:49,250 --> 00:09:55,520
relevant keywords of a text we're using

00:09:52,490 --> 00:09:58,070
an other component the rake algorithm

00:09:55,520 --> 00:10:04,240
and this is just another Swiss Army

00:09:58,070 --> 00:10:07,640
knife for you two to get home with and I

00:10:04,240 --> 00:10:12,080
want to show you a really short example

00:10:07,640 --> 00:10:15,410
how the rake algorithm works here's a

00:10:12,080 --> 00:10:20,810
here's the first sentence of of the

00:10:15,410 --> 00:10:23,290
Apache Solr Wikipedia article and the

00:10:20,810 --> 00:10:26,300
reg algorithm works as follow following

00:10:23,290 --> 00:10:28,760
he uses to stop words and punctuation is

00:10:26,300 --> 00:10:32,600
boundaries for keyword candidates so you

00:10:28,760 --> 00:10:34,910
can see you've got the keyword candidate

00:10:32,600 --> 00:10:36,890
solar open source enterprise search

00:10:34,910 --> 00:10:41,810
platform written java apache Lucene

00:10:36,890 --> 00:10:43,910
project and his an in from the are stop

00:10:41,810 --> 00:10:46,580
words or the boundaries and the

00:10:43,910 --> 00:10:49,010
punctuation after that he calculates a

00:10:46,580 --> 00:10:51,200
score for for each candidate so imagine

00:10:49,010 --> 00:10:54,290
you have a really large text this makes

00:10:51,200 --> 00:10:56,240
a lot of more sense but this is just an

00:10:54,290 --> 00:10:59,000
example he calculates a score for each

00:10:56,240 --> 00:11:02,060
candidate and returns one-third of the

00:10:59,000 --> 00:11:05,540
top candidates as keyword results so

00:11:02,060 --> 00:11:07,059
this is pretty simple not a lot higher

00:11:05,540 --> 00:11:11,949
istics needed

00:11:07,059 --> 00:11:15,029
and it works absolutely good we use the

00:11:11,949 --> 00:11:18,549
rake keyword extraction algorithm to

00:11:15,029 --> 00:11:22,299
with two and a half enhancement we

00:11:18,549 --> 00:11:24,519
define or when you use it you have to

00:11:22,299 --> 00:11:30,129
define your own stubble domain-specific

00:11:24,519 --> 00:11:32,319
stop words you can use just the usual

00:11:30,129 --> 00:11:36,669
stop words that are flowing around like

00:11:32,319 --> 00:11:39,699
this that and or something like that you

00:11:36,669 --> 00:11:42,189
have to define additional work types

00:11:39,699 --> 00:11:46,119
like verbs or something like that you

00:11:42,189 --> 00:11:50,399
have to define more signs or URLs if

00:11:46,119 --> 00:11:55,149
it's fits into your domain of text and

00:11:50,399 --> 00:11:57,399
we've implemented another feature into

00:11:55,149 --> 00:12:00,699
the rake algorithm we propagate score

00:11:57,399 --> 00:12:04,119
from overlapping or related keywords so

00:12:00,699 --> 00:12:06,099
we what happens if you have the text and

00:12:04,119 --> 00:12:09,189
you extracted the keyword you go boss

00:12:06,099 --> 00:12:11,319
and the keyword you go and you have a

00:12:09,189 --> 00:12:13,539
score for each keyword you can propagate

00:12:11,319 --> 00:12:16,649
the score or multiply a factor into each

00:12:13,539 --> 00:12:21,729
other keyword because you know this is a

00:12:16,649 --> 00:12:23,589
overlapping keyword this will bring it

00:12:21,729 --> 00:12:29,649
to a much better score calculation of

00:12:23,589 --> 00:12:33,369
the rake of the Ray algorithm and after

00:12:29,649 --> 00:12:36,069
all of our keyword extraction algorithms

00:12:33,369 --> 00:12:38,439
we use an additional filtering of the

00:12:36,069 --> 00:12:42,489
extracted keyword so this is one half

00:12:38,439 --> 00:12:49,689
enhancement because i think it's is very

00:12:42,489 --> 00:12:51,549
common yes the analyzing sentence

00:12:49,689 --> 00:12:55,389
tokenizer is available for free at

00:12:51,549 --> 00:12:57,279
shopping 24 / solar analyzers if you

00:12:55,389 --> 00:13:00,939
have any questions you can mail me at

00:12:57,279 --> 00:13:03,699
work at tobias kathmandu te or visit the

00:13:00,939 --> 00:13:08,439
developer block of shopping 24 develop a

00:13:03,699 --> 00:13:10,059
des 24.com and that's what that was my

00:13:08,439 --> 00:13:12,909
talk about how to gain speed in space

00:13:10,059 --> 00:13:15,789
with an MP and solar I think I was a

00:13:12,909 --> 00:13:21,089
little bit faster as previously

00:13:15,789 --> 00:13:21,089
previously thought yeah thank you

00:13:25,290 --> 00:13:35,470
are there any questions well thank you

00:13:32,890 --> 00:13:39,880
for your for your insightful and fast

00:13:35,470 --> 00:13:41,980
talk yeah you mentioned that it's

00:13:39,880 --> 00:13:44,770
helpful to have kind of domain-specific

00:13:41,980 --> 00:13:52,510
stop words could you give an example of

00:13:44,770 --> 00:13:55,000
what you mean by this yes so we've got a

00:13:52,510 --> 00:13:57,490
lot of texts that are coming from our

00:13:55,000 --> 00:13:59,620
partners so okay I can give you an

00:13:57,490 --> 00:14:04,450
example where we where use this rake

00:13:59,620 --> 00:14:06,730
algorithm we've when we've got out when

00:14:04,450 --> 00:14:08,590
we've got a website with a long-running

00:14:06,730 --> 00:14:16,270
text and you want to show products to to

00:14:08,590 --> 00:14:18,190
the text we just get the text let the

00:14:16,270 --> 00:14:20,740
reiki word extraction algorithm extract

00:14:18,190 --> 00:14:22,570
the key words and then throw the

00:14:20,740 --> 00:14:27,460
extracted keywords just into our search

00:14:22,570 --> 00:14:32,260
engine this is pretty good this works

00:14:27,460 --> 00:14:36,339
pretty well and in that case we we

00:14:32,260 --> 00:14:39,070
define a lot of verbs as stop words so

00:14:36,339 --> 00:14:42,610
we just do not have these usual stop

00:14:39,070 --> 00:14:44,890
words like yeah this end or something

00:14:42,610 --> 00:14:48,850
like that so you can use other word

00:14:44,890 --> 00:14:53,100
types like nouns verbs but they are

00:14:48,850 --> 00:14:53,100
absolutely domain-specific so

00:14:57,930 --> 00:15:03,330
do you have access to query logs query

00:15:01,050 --> 00:15:05,970
locked yeah because I think from that

00:15:03,330 --> 00:15:07,680
you can gain a lot of information and

00:15:05,970 --> 00:15:09,870
actually like all these stop words

00:15:07,680 --> 00:15:11,550
usually don't appear in query logs so if

00:15:09,870 --> 00:15:15,930
you if you get your hands on query logs

00:15:11,550 --> 00:15:18,870
so yeah yeah we have access but you

00:15:15,930 --> 00:15:21,810
think to to generate a stoppered file or

00:15:18,870 --> 00:15:24,000
what for what yeah I mean you get the

00:15:21,810 --> 00:15:27,149
data what people are really searching

00:15:24,000 --> 00:15:28,890
for so you could get your stop words

00:15:27,149 --> 00:15:32,310
from but you can also include it into

00:15:28,890 --> 00:15:36,570
your ranking yeah yeah we do this we do

00:15:32,310 --> 00:15:40,709
this but so you mean the first first

00:15:36,570 --> 00:15:42,720
approach for the first approach yeah I

00:15:40,709 --> 00:15:45,089
mean in general okay like yeah yeah we

00:15:42,720 --> 00:15:47,690
do that you can you can like also create

00:15:45,089 --> 00:15:51,029
some some relevance score for words

00:15:47,690 --> 00:15:55,020
based on queer locks seeing which terms

00:15:51,029 --> 00:16:04,290
are really important yeah yeah we do

00:15:55,020 --> 00:16:06,180
this thank you hello thank you for your

00:16:04,290 --> 00:16:07,709
presentation how did you make sure your

00:16:06,180 --> 00:16:08,850
recall didn't suffer it because

00:16:07,709 --> 00:16:10,529
potentially you're filtering out

00:16:08,850 --> 00:16:12,839
sentences that still have important

00:16:10,529 --> 00:16:14,550
information sorry flurry again so how

00:16:12,839 --> 00:16:16,709
did you make sure your recall that in

00:16:14,550 --> 00:16:19,020
software you're optimizing for improving

00:16:16,709 --> 00:16:21,089
precision or not losing precision but

00:16:19,020 --> 00:16:23,520
since you're using this simple heuristic

00:16:21,089 --> 00:16:25,200
right that if the sentence doesn't have

00:16:23,520 --> 00:16:27,360
many stop words then it's potentially

00:16:25,200 --> 00:16:28,920
not very rich in information how do you

00:16:27,360 --> 00:16:31,650
make sure that you're not losing

00:16:28,920 --> 00:16:35,160
information by using that use yeah we

00:16:31,650 --> 00:16:38,990
tested it manually with a last lot of

00:16:35,160 --> 00:16:41,760
data so we are not 100% sure that we've

00:16:38,990 --> 00:16:46,040
throw away some mystery interesting

00:16:41,760 --> 00:16:49,850
information but it's a lot of data and

00:16:46,040 --> 00:16:52,740
we're more happy about to throw away

00:16:49,850 --> 00:16:56,510
near nearly about sixty to eighty

00:16:52,740 --> 00:17:00,329
percent of the Unruh lovin data and we

00:16:56,510 --> 00:17:02,910
we implemented the testing and it looks

00:17:00,329 --> 00:17:07,740
pretty good so I have no exact numbers

00:17:02,910 --> 00:17:09,000
but humans looking at humans looking at

00:17:07,740 --> 00:17:10,550
the results in basically making

00:17:09,000 --> 00:17:18,470
judgments yeah

00:17:10,550 --> 00:17:20,990
yeah I also thanks my one question about

00:17:18,470 --> 00:17:22,700
this slide we mentioned that you return

00:17:20,990 --> 00:17:26,360
a third of the top candidates how does

00:17:22,700 --> 00:17:28,850
rake do this scoring and is it done on

00:17:26,360 --> 00:17:31,880
the sentence level on a whole document

00:17:28,850 --> 00:17:33,710
level so do you have a whole heuristics

00:17:31,880 --> 00:17:36,530
that the algorithm uses for this scoring

00:17:33,710 --> 00:17:39,800
or now the the reg algorithm chest

00:17:36,530 --> 00:17:43,250
calculates a score with a term frequency

00:17:39,800 --> 00:17:48,680
and document frequency and another

00:17:43,250 --> 00:17:50,630
factor and then or is the list and then

00:17:48,680 --> 00:17:54,190
returns one-third of the top keyword

00:17:50,630 --> 00:17:57,890
candidates ok so for phrases justice um

00:17:54,190 --> 00:18:06,050
yeah combination of those yet numbers

00:17:57,890 --> 00:18:07,640
okay hi hi I'm how does this your

00:18:06,050 --> 00:18:10,850
solution scale because in the beginning

00:18:07,640 --> 00:18:14,360
you're describing a problem of gun from

00:18:10,850 --> 00:18:17,450
3 million products to a hundred million

00:18:14,360 --> 00:18:20,480
hmm and with this approach it seems like

00:18:17,450 --> 00:18:27,940
yeah you can shorten the text but you

00:18:20,480 --> 00:18:33,320
cannot keep shortening it or can you we

00:18:27,940 --> 00:18:39,080
we shorten the text sorry again so I

00:18:33,320 --> 00:18:41,750
like my colleague has an answer it's

00:18:39,080 --> 00:18:44,180
just a hi I'm Tristan it's just one

00:18:41,750 --> 00:18:46,220
building block to to reducing or to

00:18:44,180 --> 00:18:47,810
increasing the number of documents was

00:18:46,220 --> 00:18:49,550
and the first building block was

00:18:47,810 --> 00:18:51,410
reducing the index eyes with the

00:18:49,550 --> 00:18:53,420
existing three million documents and

00:18:51,410 --> 00:18:56,690
there was a key success so we could

00:18:53,420 --> 00:19:02,440
reduce the overall solar index to gain

00:18:56,690 --> 00:19:02,440
space to blow up to 100 million products

00:19:06,220 --> 00:19:14,140
aside from the speed concerns how how do

00:19:11,330 --> 00:19:19,000
you check your improving your precision

00:19:14,140 --> 00:19:24,380
and you are not hurting your recall

00:19:19,000 --> 00:19:28,400
we've got a lot of bad queries so a list

00:19:24,380 --> 00:19:30,410
of bad queries and we just check these

00:19:28,400 --> 00:19:36,220
queries the list of curries and saw that

00:19:30,410 --> 00:19:36,220

YouTube URL: https://www.youtube.com/watch?v=LD0TCESH_eI


