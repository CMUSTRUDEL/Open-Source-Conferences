Title: Berlin Buzzwords 2016: Neha Narkhede - Application development and data in the emerging world ...
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	For a long time, a substantial portion of data processing that companies did ran as big batch jobs -- CSV files dumped out of databases, log files collected at the end of the day etc. But businesses operate in real-time and the software they run is catching up. 

Rather than processing data only at the end of the day, why not react to it continuously as the data arrives? This is the emerging world of stream processing. But stream processing only becomes possible when the fundamental data capture is done in a streaming fashion; after all, you can’t process a daily batch of CSV dumps as a stream. 

This shift towards stream processing has driven the popularity of Apache Kafka. Making all the organization's data is available centrally as free-flowing streams enables a company's business logic to be represented as stream processing operations. Essentially, applications are stream processors in this new world of stream processing.

In her keynote, she will explain how the fundamental nature of application development will change as stream processing goes mainstream.

Read more:
https://2016.berlinbuzzwords.de/session/application-development-and-data-emerging-world-stream-processing

About Neha Narkhede:
https://2016.berlinbuzzwords.de/users/neha-narkhede

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:02,580 --> 00:00:07,769
thank you for the kind introduction

00:00:04,590 --> 00:00:10,200
hello Berlin it's my first time here and

00:00:07,769 --> 00:00:11,850
I think this is a great conference thank

00:00:10,200 --> 00:00:14,150
you for spending your morning at this

00:00:11,850 --> 00:00:15,410
talk today I'm going to talk about

00:00:14,150 --> 00:00:18,449
applications

00:00:15,410 --> 00:00:20,880
micro-services in this emerging world of

00:00:18,449 --> 00:00:23,099
stream processing as you can tell just

00:00:20,880 --> 00:00:25,289
from the schedule of Berlin buzzwords

00:00:23,099 --> 00:00:27,329
stream processing stream data is

00:00:25,289 --> 00:00:30,539
everywhere more than half the talks are

00:00:27,329 --> 00:00:32,160
about this subject and there is a lot

00:00:30,539 --> 00:00:32,790
happening in this world related to this

00:00:32,160 --> 00:00:37,649
topic

00:00:32,790 --> 00:00:40,649
in fact unbounded unordered large-scale

00:00:37,649 --> 00:00:43,289
data sets are increasingly common in

00:00:40,649 --> 00:00:46,890
day-to-day business whether it's web

00:00:43,289 --> 00:00:48,780
blogs mobile usage statistics data from

00:00:46,890 --> 00:00:51,500
sensor networks or the Internet of

00:00:48,780 --> 00:00:54,390
Things stream data is everywhere and

00:00:51,500 --> 00:00:57,269
there is a huge push towards you know

00:00:54,390 --> 00:00:58,829
getting faster results so the question

00:00:57,269 --> 00:01:02,640
is you know how do you move to this

00:00:58,829 --> 00:01:05,040
stream oriented architecture know for

00:01:02,640 --> 00:01:07,830
some time stream processing was thought

00:01:05,040 --> 00:01:09,750
of as or maybe he still thought of as a

00:01:07,830 --> 00:01:12,210
faster MapReduce layer may be

00:01:09,750 --> 00:01:15,290
appropriate for niche problems like

00:01:12,210 --> 00:01:18,420
faster machine learning or ir analytics

00:01:15,290 --> 00:01:21,510
but in my five years of working on Kafka

00:01:18,420 --> 00:01:23,600
and on stream processing I've learned

00:01:21,510 --> 00:01:27,090
that stream processing is in fact

00:01:23,600 --> 00:01:29,160
software that computes core functions in

00:01:27,090 --> 00:01:31,740
the business implements the business

00:01:29,160 --> 00:01:35,160
logic and is much more than computing

00:01:31,740 --> 00:01:38,250
analytics about your business and that

00:01:35,160 --> 00:01:40,710
insight has a lot of impact on how we

00:01:38,250 --> 00:01:43,650
develop applications and will completely

00:01:40,710 --> 00:01:45,480
change how we will develop stateful you

00:01:43,650 --> 00:01:47,550
know loosely coupled micro services in

00:01:45,480 --> 00:01:50,310
the days to come that is what I want to

00:01:47,550 --> 00:01:52,620
focus on in today's talk and tell you

00:01:50,310 --> 00:01:55,920
where Kafka and Kafka streams fits in

00:01:52,620 --> 00:01:58,650
this big picture so let's start off with

00:01:55,920 --> 00:02:00,690
you know what is stream processing we've

00:01:58,650 --> 00:02:03,330
we've used a couple of definitions for

00:02:00,690 --> 00:02:05,310
this term so let me you know use my own

00:02:03,330 --> 00:02:08,519
definition which is that it is a

00:02:05,310 --> 00:02:12,360
paradigm for processing unbounded

00:02:08,519 --> 00:02:14,130
datasets so if you focus on paradigms

00:02:12,360 --> 00:02:15,930
for programming you know there are many

00:02:14,130 --> 00:02:17,670
ways of splitting this pie

00:02:15,930 --> 00:02:19,590
but today I'm going to focus on a

00:02:17,670 --> 00:02:22,109
specific way which is the way an

00:02:19,590 --> 00:02:25,590
application gets its import and the way

00:02:22,109 --> 00:02:28,170
it produces its output and if you focus

00:02:25,590 --> 00:02:29,819
on this input-output dimension then

00:02:28,170 --> 00:02:33,000
there are essentially three paradigms

00:02:29,819 --> 00:02:35,489
for programming the first one is request

00:02:33,000 --> 00:02:37,680
response systems these are services that

00:02:35,489 --> 00:02:39,750
we are most familiar with you know they

00:02:37,680 --> 00:02:42,750
are synchronous they are tightly coupled

00:02:39,750 --> 00:02:45,900
the latency sensitive you send one input

00:02:42,750 --> 00:02:47,879
in and you wait for an output and the

00:02:45,900 --> 00:02:51,200
only way to scale these services is by

00:02:47,879 --> 00:02:53,340
deploying more instances of the service

00:02:51,200 --> 00:02:56,459
then on the other end of the spectrum

00:02:53,340 --> 00:02:59,400
you have you know bad systems here you

00:02:56,459 --> 00:03:02,010
send all your input in and you wait for

00:02:59,400 --> 00:03:03,959
systems to crunch all that data before

00:03:02,010 --> 00:03:06,840
you know a couple of hours later they

00:03:03,959 --> 00:03:09,450
send you all the output back essentially

00:03:06,840 --> 00:03:12,780
the big difference is that bad systems

00:03:09,450 --> 00:03:14,730
view data is being bounded in nature the

00:03:12,780 --> 00:03:17,060
difference from request response is that

00:03:14,730 --> 00:03:20,609
they're loosely coupled the latency

00:03:17,060 --> 00:03:22,680
expectation is very different now we

00:03:20,609 --> 00:03:25,919
arrive at you know stream processing

00:03:22,680 --> 00:03:28,560
here you send some inputs in and you get

00:03:25,919 --> 00:03:31,590
some outputs back and the definition of

00:03:28,560 --> 00:03:35,010
some is left to the program you can have

00:03:31,590 --> 00:03:38,280
you know it be either one input or all

00:03:35,010 --> 00:03:39,479
your inputs the output is available at

00:03:38,280 --> 00:03:42,750
variable times two

00:03:39,479 --> 00:03:45,780
either you have one output item for

00:03:42,750 --> 00:03:48,870
every input item or one output item for

00:03:45,780 --> 00:03:51,900
every n input item in a stream

00:03:48,870 --> 00:03:54,689
processing is conflated as a notion that

00:03:51,900 --> 00:03:56,989
is related to you know the concept of

00:03:54,689 --> 00:03:59,280
real time but really it is a

00:03:56,989 --> 00:04:03,090
generalization of the two extremes

00:03:59,280 --> 00:04:04,829
request response and badge the other

00:04:03,090 --> 00:04:06,810
thing about stream processing is that it

00:04:04,829 --> 00:04:09,479
is misunderstood as being something that

00:04:06,810 --> 00:04:13,709
is probably like you know transient or

00:04:09,479 --> 00:04:15,870
approximate or lossy now some systems

00:04:13,709 --> 00:04:18,509
might have been that way but that is a

00:04:15,870 --> 00:04:21,239
drawback of the way some systems are

00:04:18,509 --> 00:04:23,490
designed it isn't really an inherent

00:04:21,239 --> 00:04:27,360
property of the stream processing

00:04:23,490 --> 00:04:29,490
paradigm in fact stream processing

00:04:27,360 --> 00:04:31,949
systems can be made to compute

00:04:29,490 --> 00:04:35,690
accurate results much like bad systems

00:04:31,949 --> 00:04:38,610
do as well as do that efficiently and

00:04:35,690 --> 00:04:40,440
there is this you know misunderstanding

00:04:38,610 --> 00:04:41,190
because while processing unbounded

00:04:40,440 --> 00:04:43,860
datasets

00:04:41,190 --> 00:04:45,479
there are various tricky trade-offs

00:04:43,860 --> 00:04:51,509
involved and those trade-offs are

00:04:45,479 --> 00:04:54,630
correctness latency and cost and you

00:04:51,509 --> 00:04:56,789
know some systems specifically choose to

00:04:54,630 --> 00:04:58,590
optimize or design along some of these

00:04:56,789 --> 00:05:01,289
dimensions while giving other dimensions

00:04:58,590 --> 00:05:03,630
up when what we really should be doing

00:05:01,289 --> 00:05:05,699
is giving the user the flexibility to

00:05:03,630 --> 00:05:08,520
pick the trade-offs that are right for

00:05:05,699 --> 00:05:11,190
their application and this is really

00:05:08,520 --> 00:05:13,889
important because one size does not fit

00:05:11,190 --> 00:05:15,840
all you know some applications are

00:05:13,889 --> 00:05:19,259
really care about correctness like

00:05:15,840 --> 00:05:22,440
billing while some may not like clock

00:05:19,259 --> 00:05:25,289
processing some might care about latency

00:05:22,440 --> 00:05:29,400
like alerting while some may not like

00:05:25,289 --> 00:05:32,069
ETL while some applications actually may

00:05:29,400 --> 00:05:33,900
not be okay with fully optimizing you

00:05:32,069 --> 00:05:35,250
know the cost associated with fully

00:05:33,900 --> 00:05:38,460
optimizing along the other two

00:05:35,250 --> 00:05:40,530
dimensions so in this talk you know I

00:05:38,460 --> 00:05:42,780
want to focus on stream processing and

00:05:40,530 --> 00:05:45,180
its impact on mainstream application

00:05:42,780 --> 00:05:47,039
development how you know the entire

00:05:45,180 --> 00:05:50,130
company's business logic can be

00:05:47,039 --> 00:05:54,180
architected as loosely coupled stateful

00:05:50,130 --> 00:05:56,370
stream processors and in this talk I'm

00:05:54,180 --> 00:05:59,270
going to you know walk you through this

00:05:56,370 --> 00:06:01,560
example from a brick and mortar retailer

00:05:59,270 --> 00:06:02,849
you know for retailers there are many

00:06:01,560 --> 00:06:04,320
inputs that are interesting but there

00:06:02,849 --> 00:06:07,469
are really two inputs that are really

00:06:04,320 --> 00:06:11,729
important and that is sales of things

00:06:07,469 --> 00:06:13,590
and shipments of things and for healthy

00:06:11,729 --> 00:06:15,630
retailer hopefully these two inputs are

00:06:13,590 --> 00:06:18,979
never-ending so you can really represent

00:06:15,630 --> 00:06:21,150
these as streams of sales and shipments

00:06:18,979 --> 00:06:24,060
now it's worth noting that these two

00:06:21,150 --> 00:06:26,490
inputs are useful for a large variety of

00:06:24,060 --> 00:06:28,050
applications down streams you can say

00:06:26,490 --> 00:06:29,969
you need to send it to Hadoop or the

00:06:28,050 --> 00:06:32,130
warehouse for analytics you send it to

00:06:29,969 --> 00:06:34,320
some kind of monitoring system for fraud

00:06:32,130 --> 00:06:37,139
alerting or you index it on a search

00:06:34,320 --> 00:06:38,849
system in addition to that you might

00:06:37,139 --> 00:06:40,740
want to do some sort of incremental

00:06:38,849 --> 00:06:42,029
processing now this may not be working

00:06:40,740 --> 00:06:43,350
this way in in brick-and-mortar

00:06:42,029 --> 00:06:46,290
retailers today

00:06:43,350 --> 00:06:48,390
which is that the ability to make price

00:06:46,290 --> 00:06:51,300
adjustments and inventory adjustments as

00:06:48,390 --> 00:06:55,830
demand and sales for items keep changing

00:06:51,300 --> 00:06:57,990
over time so taking a look at this

00:06:55,830 --> 00:07:00,450
picture a little differently at the top

00:06:57,990 --> 00:07:03,360
are things that fall in the what

00:07:00,450 --> 00:07:05,310
happened category these are essentially

00:07:03,360 --> 00:07:10,230
events that are interesting to this

00:07:05,310 --> 00:07:12,540
retail business and everything else is a

00:07:10,230 --> 00:07:14,370
response to that event as a response you

00:07:12,540 --> 00:07:16,560
might send it to Hadoop or you might

00:07:14,370 --> 00:07:18,810
index it on a search system or is a

00:07:16,560 --> 00:07:22,160
response you might choose to process it

00:07:18,810 --> 00:07:25,830
and trigger inventory adjustments and

00:07:22,160 --> 00:07:28,380
this insight actually influences you

00:07:25,830 --> 00:07:30,450
know some some choices for the

00:07:28,380 --> 00:07:34,050
application architecture for this

00:07:30,450 --> 00:07:36,480
example you know if you focus on the way

00:07:34,050 --> 00:07:37,830
applications get their input and output

00:07:36,480 --> 00:07:40,410
like we talked about there are a couple

00:07:37,830 --> 00:07:41,940
of choices the first one is you don't

00:07:40,410 --> 00:07:43,980
have to worry about that at all

00:07:41,940 --> 00:07:46,350
if you architect this as one large

00:07:43,980 --> 00:07:48,660
monolith then the set of problems we

00:07:46,350 --> 00:07:50,430
will talk about are very different but

00:07:48,660 --> 00:07:52,380
practically you know companies don't

00:07:50,430 --> 00:07:53,640
write applications this way parts of

00:07:52,380 --> 00:07:55,110
your business logic are written by

00:07:53,640 --> 00:07:57,270
different teams they're managed

00:07:55,110 --> 00:07:59,010
differently and because they are written

00:07:57,270 --> 00:08:01,830
as separate services they in fact need

00:07:59,010 --> 00:08:03,990
to communicate and exchange data and

00:08:01,830 --> 00:08:06,270
oftentimes you know the most common

00:08:03,990 --> 00:08:08,610
mechanism is of using synchronous

00:08:06,270 --> 00:08:10,500
communication request response this

00:08:08,610 --> 00:08:14,190
actually works but there are downsides

00:08:10,500 --> 00:08:15,990
worth talking about first one is the

00:08:14,190 --> 00:08:18,420
total amount of parallelism if you think

00:08:15,990 --> 00:08:20,640
about it of processing an event through

00:08:18,420 --> 00:08:22,710
all these downstream services is fairly

00:08:20,640 --> 00:08:25,590
limited with synchronous communication

00:08:22,710 --> 00:08:27,450
the same sales event has to get

00:08:25,590 --> 00:08:30,000
processed one by one by every service

00:08:27,450 --> 00:08:33,300
which means the total you know latency

00:08:30,000 --> 00:08:35,160
of processing an event is the addition

00:08:33,300 --> 00:08:37,380
of all the you know time taken by

00:08:35,160 --> 00:08:40,590
downstream services instead of the time

00:08:37,380 --> 00:08:43,650
taken by just the slowest service the

00:08:40,590 --> 00:08:46,650
second downside is lack of resiliency

00:08:43,650 --> 00:08:48,690
things are tightly coupled a slight

00:08:46,650 --> 00:08:50,850
degradation or failure of a service

00:08:48,690 --> 00:08:54,690
actually affects a whole bunch of

00:08:50,850 --> 00:08:56,790
downstream services I will argue that we

00:08:54,690 --> 00:08:57,120
need to make a fundamental shift towards

00:08:56,790 --> 00:08:58,860
thing

00:08:57,120 --> 00:09:02,580
thinking about you know asynchronous

00:08:58,860 --> 00:09:05,370
communication and this is at the heart

00:09:02,580 --> 00:09:06,930
of you know the reactive manifesto for

00:09:05,370 --> 00:09:09,839
those of you who are following it is

00:09:06,930 --> 00:09:13,260
this concept of an asynchronous boundary

00:09:09,839 --> 00:09:15,300
between applications the goal is you

00:09:13,260 --> 00:09:17,670
know with this asynchronous boundary you

00:09:15,300 --> 00:09:20,610
in fact decouple the various components

00:09:17,670 --> 00:09:22,830
of your architecture and if you do that

00:09:20,610 --> 00:09:25,230
then you get a whole bunch of advantages

00:09:22,830 --> 00:09:27,810
that you you know did not have prior to

00:09:25,230 --> 00:09:29,880
this which is with decoupling first of

00:09:27,810 --> 00:09:32,700
all all the downstream service services

00:09:29,880 --> 00:09:35,339
can process events in parallel things

00:09:32,700 --> 00:09:38,490
are a resilient you know failure of one

00:09:35,339 --> 00:09:42,600
service is decoupled from failure of you

00:09:38,490 --> 00:09:44,220
know affecting other services now in

00:09:42,600 --> 00:09:46,380
addition you know in order to achieve

00:09:44,220 --> 00:09:48,839
this asynchronous communication you

00:09:46,380 --> 00:09:50,820
would likely buffer this data and for

00:09:48,839 --> 00:09:53,760
buffering in order and processing in

00:09:50,820 --> 00:09:56,010
order you might use a queue if you did

00:09:53,760 --> 00:09:58,529
that then now you have some choices you

00:09:56,010 --> 00:10:02,220
know if you pick a traditional queue

00:09:58,529 --> 00:10:04,500
then you'd have to you know send an

00:10:02,220 --> 00:10:07,260
event to every downstream queue one by

00:10:04,500 --> 00:10:09,029
one the same sales event is processed by

00:10:07,260 --> 00:10:11,190
several applications you might want to

00:10:09,029 --> 00:10:13,890
send that event by one by one to all

00:10:11,190 --> 00:10:16,410
those applications that is wasteful it

00:10:13,890 --> 00:10:18,870
might work it's wasteful you know as as

00:10:16,410 --> 00:10:21,450
one service fails it might fall out of

00:10:18,870 --> 00:10:24,089
sync with other services so a natural

00:10:21,450 --> 00:10:27,630
choice is to use a pub subsystem the

00:10:24,089 --> 00:10:29,970
need is you write it once and you read

00:10:27,630 --> 00:10:31,980
it multiple times this greatly

00:10:29,970 --> 00:10:34,440
simplifies your architecture because now

00:10:31,980 --> 00:10:37,980
you just have one pub sub Q versus

00:10:34,440 --> 00:10:39,660
several ones this is essentially you

00:10:37,980 --> 00:10:41,490
know what Kafka does you know Kafka is a

00:10:39,660 --> 00:10:43,980
pub subsystem it is a very

00:10:41,490 --> 00:10:46,170
high-performance pub sub system that

00:10:43,980 --> 00:10:49,770
allows you to write at high throughput

00:10:46,170 --> 00:10:52,440
in low latency okay so the second

00:10:49,770 --> 00:10:56,130
component you know of this reactive

00:10:52,440 --> 00:10:58,050
manifesto is being able to handle back

00:10:56,130 --> 00:11:00,180
pressure if you buffer events then you

00:10:58,050 --> 00:11:02,850
have to worry about what happens when a

00:11:00,180 --> 00:11:04,890
service fails how how long do you have

00:11:02,850 --> 00:11:08,310
to buffer data in order to not run out

00:11:04,890 --> 00:11:09,990
of memory here I'd like to say that if

00:11:08,310 --> 00:11:10,920
your queue was in fact persistent then

00:11:09,990 --> 00:11:12,779
you don't have to worry

00:11:10,920 --> 00:11:14,730
backpressure it is solved by definition

00:11:12,779 --> 00:11:17,370
this is what Kafka does it is a

00:11:14,730 --> 00:11:19,440
high-performance persistent queue it can

00:11:17,370 --> 00:11:24,329
buffer way more data that can ever live

00:11:19,440 --> 00:11:26,040
in an applications memory so in this

00:11:24,329 --> 00:11:29,339
talk I want to you know walk you through

00:11:26,040 --> 00:11:31,110
how to build a reactive retail business

00:11:29,339 --> 00:11:33,810
you know an architecture that relies on

00:11:31,110 --> 00:11:35,459
asynchronous communication it ends up

00:11:33,810 --> 00:11:38,490
with you know applications that are

00:11:35,459 --> 00:11:39,180
loosely coupled they're stateful they're

00:11:38,490 --> 00:11:42,209
resilient

00:11:39,180 --> 00:11:46,320
and easier to develop and also degrade

00:11:42,209 --> 00:11:49,260
gracefully now part of the reactive

00:11:46,320 --> 00:11:50,970
manifesto is reactor streams in case any

00:11:49,260 --> 00:11:53,360
of you following that it is it is a

00:11:50,970 --> 00:11:55,860
standard for streaming libraries that

00:11:53,360 --> 00:11:58,139
you know that dictates that a streaming

00:11:55,860 --> 00:12:00,329
library is one that processes unbounded

00:11:58,139 --> 00:12:02,699
streams of events it asynchronously

00:12:00,329 --> 00:12:04,920
passes passes messages around in

00:12:02,699 --> 00:12:07,440
sequence and it definitely has the

00:12:04,920 --> 00:12:08,730
ability to tackle back pressure so in

00:12:07,440 --> 00:12:10,740
the remainder of the talk you will see

00:12:08,730 --> 00:12:13,320
how Kafka and Kafka streams actually

00:12:10,740 --> 00:12:15,600
does conform to this reactor streams

00:12:13,320 --> 00:12:18,779
manifesto however has a possibly

00:12:15,600 --> 00:12:21,240
different set of api's okay so back to

00:12:18,779 --> 00:12:23,579
stream processing you know it is a set

00:12:21,240 --> 00:12:25,860
of functions written on top of what

00:12:23,579 --> 00:12:30,149
happened data which we've seen that I'll

00:12:25,860 --> 00:12:32,430
just events over time what it noticed is

00:12:30,149 --> 00:12:34,980
Kafka is sort of rising as the de facto

00:12:32,430 --> 00:12:36,990
standard for storing stream data and so

00:12:34,980 --> 00:12:39,600
stream processing boils down to writing

00:12:36,990 --> 00:12:43,800
functions on top of Kafka events to

00:12:39,600 --> 00:12:46,470
produce more Kafka events now over time

00:12:43,800 --> 00:12:48,720
as in the last five years as Kafka has

00:12:46,470 --> 00:12:51,540
been adopted across thousands of

00:12:48,720 --> 00:12:53,579
companies worldwide I've noticed that

00:12:51,540 --> 00:12:55,769
there are you know two broad approaches

00:12:53,579 --> 00:12:59,610
that have emerged as stream processing

00:12:55,769 --> 00:13:01,709
approaches on Kafka the first one is you

00:12:59,610 --> 00:13:04,410
know what I'd call do-it-yourself stream

00:13:01,709 --> 00:13:07,170
processing this is you take the Kafka

00:13:04,410 --> 00:13:10,199
libraries and you decide to do the rest

00:13:07,170 --> 00:13:11,730
of the stream processing yourself and if

00:13:10,199 --> 00:13:13,829
you did that then there are you know a

00:13:11,730 --> 00:13:15,470
couple of hard problems that you should

00:13:13,829 --> 00:13:17,790
know that you'd have to deal with

00:13:15,470 --> 00:13:18,899
basically even if you don't use Kafka

00:13:17,790 --> 00:13:20,970
you have to deal with a lot of these

00:13:18,899 --> 00:13:23,290
problems to be able to do stream

00:13:20,970 --> 00:13:25,060
processing correctly and efficiently

00:13:23,290 --> 00:13:27,340
and those are problems like you know

00:13:25,060 --> 00:13:30,130
ensuring that data arrives in order as

00:13:27,340 --> 00:13:32,050
well as gets processed in order ensuring

00:13:30,130 --> 00:13:35,410
that you have the ability to scale both

00:13:32,050 --> 00:13:39,780
data and processing out ensuring that

00:13:35,410 --> 00:13:42,220
you have semantics even as machines fail

00:13:39,780 --> 00:13:44,140
ensuring that you have the ability to

00:13:42,220 --> 00:13:45,760
manage state to be able to support

00:13:44,140 --> 00:13:50,200
stream processing operations like

00:13:45,760 --> 00:13:52,330
windowed aggregates and joints having

00:13:50,200 --> 00:13:54,280
the ability to reprocess data so when

00:13:52,330 --> 00:13:56,290
you you know upgrade your applications

00:13:54,280 --> 00:13:58,420
you can make sure that all your data are

00:13:56,290 --> 00:14:01,630
conforms to the new logic of your

00:13:58,420 --> 00:14:05,650
application and last but not the least

00:14:01,630 --> 00:14:09,430
the notion of time is very important to

00:14:05,650 --> 00:14:11,530
correctness in stream processing now

00:14:09,430 --> 00:14:14,070
before we talk about you know how Kafka

00:14:11,530 --> 00:14:17,080
and Kafka 'seems solves these problems

00:14:14,070 --> 00:14:18,550
very differently let's first take a look

00:14:17,080 --> 00:14:20,620
at the second approach which is quite

00:14:18,550 --> 00:14:22,360
popular which is using one of the many

00:14:20,620 --> 00:14:25,090
stream processing frameworks that have

00:14:22,360 --> 00:14:27,370
emerged since Kafka has been adopted

00:14:25,090 --> 00:14:30,190
widely you know spark has of streaming

00:14:27,370 --> 00:14:32,500
module there is storm there Sam's are

00:14:30,190 --> 00:14:34,600
which some of us built at LinkedIn there

00:14:32,500 --> 00:14:37,440
is flink which is a really cool

00:14:34,600 --> 00:14:40,420
technology upcoming built locally here

00:14:37,440 --> 00:14:41,410
now these you know systems are pretty

00:14:40,420 --> 00:14:44,770
cool there's a ton of innovation

00:14:41,410 --> 00:14:47,380
happening in this space if you'd notice

00:14:44,770 --> 00:14:49,540
that you know that one of the core

00:14:47,380 --> 00:14:52,300
design traits of these systems is that

00:14:49,540 --> 00:14:55,960
they're coming from the world of making

00:14:52,300 --> 00:14:57,850
a faster MapReduce layer and as a result

00:14:55,960 --> 00:15:02,560
of that there are some design traits

00:14:57,850 --> 00:15:04,870
that these systems share at the core the

00:15:02,560 --> 00:15:07,240
core concept is taking part of your

00:15:04,870 --> 00:15:10,900
application which does stream processing

00:15:07,240 --> 00:15:13,720
and it modeling that as a job that runs

00:15:10,900 --> 00:15:15,660
on these a cluster of machines that runs

00:15:13,720 --> 00:15:17,770
the stream processing framework and

00:15:15,660 --> 00:15:20,500
because of that there are several

00:15:17,770 --> 00:15:23,440
implications there is a specific way to

00:15:20,500 --> 00:15:26,170
configure your job a specific way to

00:15:23,440 --> 00:15:28,600
package and deploy your job on a central

00:15:26,170 --> 00:15:30,550
cluster and these systems need to think

00:15:28,600 --> 00:15:32,560
about resource management how do you

00:15:30,550 --> 00:15:35,890
multiplex several jobs on a shared

00:15:32,560 --> 00:15:36,880
cluster but these systems are actually

00:15:35,890 --> 00:15:38,440
useful

00:15:36,880 --> 00:15:40,750
for a large class of stream processing

00:15:38,440 --> 00:15:43,480
problems basically any kind of iterative

00:15:40,750 --> 00:15:46,780
processing where you know like machine

00:15:43,480 --> 00:15:47,620
learning or graph processing or any kind

00:15:46,780 --> 00:15:50,680
of long-running

00:15:47,620 --> 00:15:52,930
you know analytics queries like C that

00:15:50,680 --> 00:15:56,380
require a sequel interface actually run

00:15:52,930 --> 00:15:58,810
really well on these systems the

00:15:56,380 --> 00:16:01,300
problems you know that these systems aim

00:15:58,810 --> 00:16:03,430
to solve they actually influence the

00:16:01,300 --> 00:16:06,790
design the system design that these

00:16:03,430 --> 00:16:08,170
systems have adopted and I think that

00:16:06,790 --> 00:16:11,260
you know the only way to know if a

00:16:08,170 --> 00:16:14,050
system design is applicable to the real

00:16:11,260 --> 00:16:17,440
world is to build a system that conforms

00:16:14,050 --> 00:16:19,000
to that design to deploy it you know for

00:16:17,440 --> 00:16:23,050
real applications and then observe the

00:16:19,000 --> 00:16:24,700
adoption when my time at LinkedIn I was

00:16:23,050 --> 00:16:27,160
lucky enough to be part of a team that

00:16:24,700 --> 00:16:30,430
built Apache Sam's which essentially one

00:16:27,160 --> 00:16:33,460
of these systems and we built it and we

00:16:30,430 --> 00:16:36,010
deployed it for in-house applications

00:16:33,460 --> 00:16:38,410
and we also open sourced it and through

00:16:36,010 --> 00:16:40,510
that we learned a lot of things the core

00:16:38,410 --> 00:16:42,550
sort of lesson learned was that one of

00:16:40,510 --> 00:16:45,400
the key misconceptions in stream

00:16:42,550 --> 00:16:50,290
processing is that it will be used only

00:16:45,400 --> 00:16:52,990
as a sort of a fast MapReduce layer when

00:16:50,290 --> 00:16:54,970
in fact it turned out that stream

00:16:52,990 --> 00:16:57,460
processing and the most compelling

00:16:54,970 --> 00:16:59,290
applications that want to do any kind of

00:16:57,460 --> 00:17:01,420
stream processing actually look

00:16:59,290 --> 00:17:04,360
different from what a hive query or a

00:17:01,420 --> 00:17:06,730
spark job looks like and in fact look a

00:17:04,360 --> 00:17:08,380
lot like what an asynchronous micro

00:17:06,730 --> 00:17:10,810
service might look like which is

00:17:08,380 --> 00:17:14,800
different from just a fast version of

00:17:10,810 --> 00:17:16,959
your batch job so then we arrived at

00:17:14,800 --> 00:17:18,699
well you know what needs to be done to

00:17:16,959 --> 00:17:20,319
make stream processing more applicable

00:17:18,699 --> 00:17:22,150
or approachable to application

00:17:20,319 --> 00:17:24,040
developers since application developers

00:17:22,150 --> 00:17:27,490
develop these asynchronous micro

00:17:24,040 --> 00:17:29,980
services and there are some of these

00:17:27,490 --> 00:17:32,080
design traits have you know interesting

00:17:29,980 --> 00:17:34,420
implications if you observe you know

00:17:32,080 --> 00:17:36,370
they're just for config management and

00:17:34,420 --> 00:17:38,530
packaging alone there are tons and tons

00:17:36,370 --> 00:17:41,350
of tools there's docker and chef and

00:17:38,530 --> 00:17:43,810
puppet and salt and whole list of custom

00:17:41,350 --> 00:17:46,090
tools for resource management you have

00:17:43,810 --> 00:17:48,430
mezzos and kubernetes and that whole

00:17:46,090 --> 00:17:50,220
ecosystem is changing essentially

00:17:48,430 --> 00:17:51,780
application developers are now

00:17:50,220 --> 00:17:53,790
standardizing on any one of these

00:17:51,780 --> 00:17:56,220
problems anytime soon so we can't really

00:17:53,790 --> 00:17:57,510
you know dictate how you know

00:17:56,220 --> 00:18:01,400
applications want to solve these

00:17:57,510 --> 00:18:04,650
problems the second is you know for for

00:18:01,400 --> 00:18:06,690
stream processing to be approachable to

00:18:04,650 --> 00:18:08,280
applications it has to be lightweight it

00:18:06,690 --> 00:18:11,340
has to be something that can be embedded

00:18:08,280 --> 00:18:13,980
by just about any application and so we

00:18:11,340 --> 00:18:17,120
arrived at this architecture decision of

00:18:13,980 --> 00:18:19,860
making this as a library in fact it is

00:18:17,120 --> 00:18:21,330
architected as a Kafka library and that

00:18:19,860 --> 00:18:24,570
is done very much on purpose

00:18:21,330 --> 00:18:27,570
Kafka oddly provides several primitives

00:18:24,570 --> 00:18:29,910
that are core and fundamental to stream

00:18:27,570 --> 00:18:31,980
processing the idea is in order to make

00:18:29,910 --> 00:18:34,080
this a lightweight layer we have to

00:18:31,980 --> 00:18:36,180
rebuild you know you have to build on

00:18:34,080 --> 00:18:37,320
top of Kafka primitives not we are

00:18:36,180 --> 00:18:41,070
collect and rebuild

00:18:37,320 --> 00:18:42,720
you know what cough quality solves so

00:18:41,070 --> 00:18:44,850
just to give you an idea you know some

00:18:42,720 --> 00:18:47,220
of us some of you might have attended

00:18:44,850 --> 00:18:48,900
the Kafka stream's library talks so this

00:18:47,220 --> 00:18:51,660
might seem familiar but for those of you

00:18:48,900 --> 00:18:53,850
who didn't you know streams is a java

00:18:51,660 --> 00:18:55,950
library it has two interfaces it's

00:18:53,850 --> 00:18:58,620
pretty simple one is a callback api and

00:18:55,950 --> 00:19:01,530
the other reserve dsm this callback api

00:18:58,620 --> 00:19:03,890
looks some you know pretty simple you

00:19:01,530 --> 00:19:07,080
have a key in a value which is

00:19:03,890 --> 00:19:09,120
essentially a message in Kafka you

00:19:07,080 --> 00:19:11,640
implement the process API which is your

00:19:09,120 --> 00:19:13,530
logic as part of the process API you

00:19:11,640 --> 00:19:15,660
might send they went out and that's

00:19:13,530 --> 00:19:17,700
basically it so in some sense this

00:19:15,660 --> 00:19:19,950
encapsulates the entire scope of stream

00:19:17,700 --> 00:19:21,780
processing you have input events you

00:19:19,950 --> 00:19:25,830
write your processing logic you have

00:19:21,780 --> 00:19:28,140
output events here's the code that you

00:19:25,830 --> 00:19:29,760
actually write to you know create some

00:19:28,140 --> 00:19:32,400
of these processors the config tells it

00:19:29,760 --> 00:19:34,620
where to connect to Kafka you have your

00:19:32,400 --> 00:19:36,990
source which tells it which topics to

00:19:34,620 --> 00:19:38,370
ingest you have your processor class and

00:19:36,990 --> 00:19:39,720
then you have a sync node to tell it

00:19:38,370 --> 00:19:42,420
where to send that data and that's

00:19:39,720 --> 00:19:45,000
basically it this is you know in fact

00:19:42,420 --> 00:19:48,720
all that you write for your application

00:19:45,000 --> 00:19:51,360
you can deploy it as one process or you

00:19:48,720 --> 00:19:53,550
can deploy it as multiple processes or

00:19:51,360 --> 00:19:55,650
you can put it in a docker image or run

00:19:53,550 --> 00:19:58,080
it on mezzos Kafka streams is just a

00:19:55,650 --> 00:19:59,910
library it doesn't dictate anywhere

00:19:58,080 --> 00:20:02,790
anything about how you should package

00:19:59,910 --> 00:20:03,929
and deploy your application okay so the

00:20:02,790 --> 00:20:06,749
second API is actually

00:20:03,929 --> 00:20:09,779
a high-level DSL you know it provides a

00:20:06,749 --> 00:20:13,639
sort of a functional API is like filter

00:20:09,779 --> 00:20:17,369
and map and join and produce and so on

00:20:13,639 --> 00:20:19,950
whether you use the low-level API or the

00:20:17,369 --> 00:20:21,809
DSL you know or even any other stream

00:20:19,950 --> 00:20:24,330
processing system oftentimes the

00:20:21,809 --> 00:20:25,830
operators form a topology where the

00:20:24,330 --> 00:20:27,929
first and the last knowns are possibly

00:20:25,830 --> 00:20:29,279
the source and the sink nodes and then

00:20:27,929 --> 00:20:33,179
the remaining our operators are

00:20:29,279 --> 00:20:34,710
dependent on each other so many times

00:20:33,179 --> 00:20:37,529
the question is like how do you map this

00:20:34,710 --> 00:20:39,570
logical topology to physical processes

00:20:37,529 --> 00:20:41,759
in Kafka streams it's very simple you

00:20:39,570 --> 00:20:43,529
just write your code in you deploy it it

00:20:41,759 --> 00:20:47,100
does the scale out and partitioning

00:20:43,529 --> 00:20:49,019
transparently as you will see okay

00:20:47,100 --> 00:20:51,509
so now I'm going to go through you know

00:20:49,019 --> 00:20:54,210
some of these problems in how Kafka as

00:20:51,509 --> 00:20:56,429
well as Kafka seems offers a solution to

00:20:54,210 --> 00:21:00,119
some of these you know problems the

00:20:56,429 --> 00:21:01,559
first one is ordering if if you familiar

00:21:00,119 --> 00:21:03,629
with Kafka this might seem like a

00:21:01,559 --> 00:21:05,580
familiar abstraction this is a

00:21:03,629 --> 00:21:08,220
fundamental in abstraction at the heart

00:21:05,580 --> 00:21:10,409
of Kafka which is a log no a log is

00:21:08,220 --> 00:21:12,929
nothing but an abstract data structure

00:21:10,409 --> 00:21:15,119
that has some properties it is like a

00:21:12,929 --> 00:21:17,789
structured array of messages so data is

00:21:15,119 --> 00:21:20,639
ordered it is a pent only and hence

00:21:17,789 --> 00:21:24,269
immutable so data written once does not

00:21:20,639 --> 00:21:27,029
change and records are identified by a

00:21:24,269 --> 00:21:30,570
unique log sequence number which in

00:21:27,029 --> 00:21:32,970
Kafka we call it an offset now not only

00:21:30,570 --> 00:21:34,559
is data written or appended in order

00:21:32,970 --> 00:21:36,960
it's also read in order you just give it

00:21:34,559 --> 00:21:37,639
an offset and you scan ahead from there

00:21:36,960 --> 00:21:40,860
on

00:21:37,639 --> 00:21:43,200
so this log is what provides the

00:21:40,860 --> 00:21:46,350
ordering semantics that are required for

00:21:43,200 --> 00:21:47,700
stream processing now that is the

00:21:46,350 --> 00:21:49,529
logical version of the log but

00:21:47,700 --> 00:21:50,190
physically if you wanted to scale this

00:21:49,529 --> 00:21:51,840
log out

00:21:50,190 --> 00:21:54,149
then you'd shard it into multiple

00:21:51,840 --> 00:21:56,220
partitions and if you did that then that

00:21:54,149 --> 00:21:58,610
is exactly the back end of apache Kafka

00:21:56,220 --> 00:22:01,080
where logically a log is a topic

00:21:58,610 --> 00:22:03,059
physically a topic lives in partitions

00:22:01,080 --> 00:22:05,399
that are replicated across a whole bunch

00:22:03,059 --> 00:22:07,320
of brokers as events come in they're

00:22:05,399 --> 00:22:09,779
appended to one of the partitions logs

00:22:07,320 --> 00:22:14,840
and we have a policy for maintaining a

00:22:09,779 --> 00:22:17,270
fixed window of the log no part of

00:22:14,840 --> 00:22:20,150
processing you know part of sort of

00:22:17,270 --> 00:22:21,920
ability is data parallelism which is how

00:22:20,150 --> 00:22:24,200
do you distribute data across machines

00:22:21,920 --> 00:22:26,210
which is what we just looked at but the

00:22:24,200 --> 00:22:28,280
other part is how do you you know scale

00:22:26,210 --> 00:22:30,290
processing how do you scale processing

00:22:28,280 --> 00:22:33,650
across machines and to that effect you

00:22:30,290 --> 00:22:35,600
know Kafka has this primitive called

00:22:33,650 --> 00:22:37,880
group management which essentially

00:22:35,600 --> 00:22:41,050
allows a group of processes which is

00:22:37,880 --> 00:22:43,760
your application to form a group and

00:22:41,050 --> 00:22:46,490
coordinate some action and that action

00:22:43,760 --> 00:22:48,560
changes whether you're a Kafka consumer

00:22:46,490 --> 00:22:50,890
or streams libraries or a Kafka consumer

00:22:48,560 --> 00:22:54,290
the coordinated action is you know which

00:22:50,890 --> 00:22:57,080
consumers own which topics so that in

00:22:54,290 --> 00:23:00,530
total they consume the entire topic in

00:22:57,080 --> 00:23:02,390
collection as the user you don't have to

00:23:00,530 --> 00:23:04,520
worry about any of this all you do is

00:23:02,390 --> 00:23:06,890
you embed the Kafka consumer library you

00:23:04,520 --> 00:23:08,930
tell it which topics to ingest data from

00:23:06,890 --> 00:23:11,960
whether you start one instance of the

00:23:08,930 --> 00:23:13,430
application or in instances what this

00:23:11,960 --> 00:23:15,320
had the way it works underneath the

00:23:13,430 --> 00:23:17,390
covers is this consumer library does

00:23:15,320 --> 00:23:20,690
this group management underneath the

00:23:17,390 --> 00:23:22,670
covers so it is actually operationally

00:23:20,690 --> 00:23:25,040
simple to consume large amounts of data

00:23:22,670 --> 00:23:27,380
as you can see the important thing bit

00:23:25,040 --> 00:23:29,600
is that this is actually the same thing

00:23:27,380 --> 00:23:30,710
that's required for stream processing

00:23:29,600 --> 00:23:33,680
this

00:23:30,710 --> 00:23:37,520
now in this Kafka streams your consumer

00:23:33,680 --> 00:23:38,870
is replaced by this topology and the

00:23:37,520 --> 00:23:41,000
group management does the same thing

00:23:38,870 --> 00:23:43,670
which is distribute the input partitions

00:23:41,000 --> 00:23:46,940
across the instances of your application

00:23:43,670 --> 00:23:49,280
that embed the streams library the the

00:23:46,940 --> 00:23:51,770
really important part is that each of

00:23:49,280 --> 00:23:53,900
the processes involved in your

00:23:51,770 --> 00:23:56,120
application are actually peers there is

00:23:53,900 --> 00:23:58,520
no special master or slave there is no

00:23:56,120 --> 00:24:01,610
special job manager or task manager it

00:23:58,520 --> 00:24:03,950
is hence operationally simple to deploy

00:24:01,610 --> 00:24:07,540
as well as it scales well from

00:24:03,950 --> 00:24:10,520
development all the way to production in

00:24:07,540 --> 00:24:12,290
terms of fault tolerance the same group

00:24:10,520 --> 00:24:15,950
management capability is extremely

00:24:12,290 --> 00:24:17,720
important it not only does it not only

00:24:15,950 --> 00:24:20,030
allows you to coordinate actions it also

00:24:17,720 --> 00:24:21,770
detects failures so if one of the

00:24:20,030 --> 00:24:24,230
instances of your consumer applications

00:24:21,770 --> 00:24:26,300
fail the whole group rebalance is so

00:24:24,230 --> 00:24:28,100
that it takes you know the remaining

00:24:26,300 --> 00:24:30,620
instances take up the load of the failed

00:24:28,100 --> 00:24:31,160
instance and the same thing is actually

00:24:30,620 --> 00:24:33,110
useful

00:24:31,160 --> 00:24:36,140
Kafka streams if one of the machines

00:24:33,110 --> 00:24:38,360
that runs your topology fails streams

00:24:36,140 --> 00:24:40,940
Kafka streams automatically detects that

00:24:38,360 --> 00:24:43,220
failure and it restarts your topology on

00:24:40,940 --> 00:24:44,930
another live instance of your

00:24:43,220 --> 00:24:46,430
application so we actually don't have to

00:24:44,930 --> 00:24:51,260
rebuild any of this it's just available

00:24:46,430 --> 00:24:53,030
through the Kafka client protocol then

00:24:51,260 --> 00:24:55,160
comes state management you know first of

00:24:53,030 --> 00:24:58,840
all you know why does state management

00:24:55,160 --> 00:25:01,070
appear in stream processing at all and

00:24:58,840 --> 00:25:02,810
the reason is you know if you look at

00:25:01,070 --> 00:25:05,030
some of these common stream processing

00:25:02,810 --> 00:25:06,560
operations you'll notice that there are

00:25:05,030 --> 00:25:08,870
some operations like filter and map

00:25:06,560 --> 00:25:10,700
which are essentially a record at a time

00:25:08,870 --> 00:25:12,590
you know either take a record and you

00:25:10,700 --> 00:25:14,450
filter it out or you send it across or

00:25:12,590 --> 00:25:16,580
you take a record and you map it out you

00:25:14,450 --> 00:25:17,840
don't have to remember anything about a

00:25:16,580 --> 00:25:22,190
record in some of these operations

00:25:17,840 --> 00:25:24,260
essentially there's stateless then there

00:25:22,190 --> 00:25:25,880
are some operations like joins and

00:25:24,260 --> 00:25:28,040
window drag records where you in fact

00:25:25,880 --> 00:25:30,950
have to remember something about a group

00:25:28,040 --> 00:25:35,510
of records in order to compute that

00:25:30,950 --> 00:25:36,440
operation so for state management now

00:25:35,510 --> 00:25:38,780
you have two choices

00:25:36,440 --> 00:25:40,730
you know first one is remote state it's

00:25:38,780 --> 00:25:42,410
really easy you take your state and you

00:25:40,730 --> 00:25:46,550
just stick it at some key value store

00:25:42,410 --> 00:25:48,410
that you know and trust this works but

00:25:46,550 --> 00:25:51,980
there is an inherent performance

00:25:48,410 --> 00:25:54,440
impedance mismatch it matters Kafka can

00:25:51,980 --> 00:25:56,690
process data at a very fast rate at

00:25:54,440 --> 00:25:59,000
almost hundreds of thousands of messages

00:25:56,690 --> 00:26:00,800
per second but a key value store when

00:25:59,000 --> 00:26:03,700
used in this manner can only process

00:26:00,800 --> 00:26:06,260
maybe thousands of messages per second

00:26:03,700 --> 00:26:08,780
so then the second option is to push

00:26:06,260 --> 00:26:12,050
your state within your processor so that

00:26:08,780 --> 00:26:14,810
it's available locally since all the

00:26:12,050 --> 00:26:17,630
data that is required for a stream

00:26:14,810 --> 00:26:19,610
processor is available locally it is

00:26:17,630 --> 00:26:23,270
incredibly fast but then there are

00:26:19,610 --> 00:26:26,150
several upsides as well a it provides

00:26:23,270 --> 00:26:28,430
better isolation one fast processor

00:26:26,150 --> 00:26:30,410
cannot take down the state used by many

00:26:28,430 --> 00:26:33,230
other life services which could happen

00:26:30,410 --> 00:26:35,600
here and being it is flexible

00:26:33,230 --> 00:26:39,350
this local status pluggable it could be

00:26:35,600 --> 00:26:41,720
as simple as a in-memory hash map or it

00:26:39,350 --> 00:26:43,760
could be an embedded rocks TB store or

00:26:41,720 --> 00:26:44,960
it could be your customized read or

00:26:43,760 --> 00:26:48,520
write optimized data

00:26:44,960 --> 00:26:51,200
structure so Kafka streams provides

00:26:48,520 --> 00:26:53,779
local state and it provides local state

00:26:51,200 --> 00:26:55,580
that is fault-tolerant that is important

00:26:53,779 --> 00:26:56,950
since if your processor fails part of

00:26:55,580 --> 00:27:00,380
your state disappears

00:26:56,950 --> 00:27:03,169
that is not grained so what Kafka James

00:27:00,380 --> 00:27:06,200
does is it every right made to a state

00:27:03,169 --> 00:27:08,240
store is transparently written to a

00:27:06,200 --> 00:27:11,450
highly durable and highly available

00:27:08,240 --> 00:27:13,610
Kafka topping so even if your processor

00:27:11,450 --> 00:27:15,260
dies in it it gets restarted somewhere

00:27:13,610 --> 00:27:18,350
else which is also something that it

00:27:15,260 --> 00:27:20,679
does it merely reads the state from this

00:27:18,350 --> 00:27:24,429
highly available topic and it recreates

00:27:20,679 --> 00:27:27,440
that state before you process more data

00:27:24,429 --> 00:27:29,690
so you might think you know if it if

00:27:27,440 --> 00:27:31,909
every single write in a state database

00:27:29,690 --> 00:27:36,130
is written to Kafka wouldn't it run out

00:27:31,909 --> 00:27:38,630
of space and it wouldn't because of

00:27:36,130 --> 00:27:40,640
capability in Kafka called

00:27:38,630 --> 00:27:43,100
log compaction essentially it is a

00:27:40,640 --> 00:27:45,559
special garbage collection policy that

00:27:43,100 --> 00:27:48,230
applies to key data

00:27:45,559 --> 00:27:50,480
you know topics that have keys so the

00:27:48,230 --> 00:27:52,909
way this works is you know imagine or

00:27:50,480 --> 00:27:55,700
consider that every write made to your

00:27:52,909 --> 00:27:58,850
external database is available as a

00:27:55,700 --> 00:28:00,770
message in Kafka where a message has a

00:27:58,850 --> 00:28:03,080
key and a value the key is the primary

00:28:00,770 --> 00:28:07,039
key of the row and the values the entire

00:28:03,080 --> 00:28:09,320
content of the ROM so for a row that

00:28:07,039 --> 00:28:11,659
gets frequently updated you have several

00:28:09,320 --> 00:28:14,960
messages that belong to that key in this

00:28:11,659 --> 00:28:16,490
Kafka log like the key a here and for

00:28:14,960 --> 00:28:18,320
rows that never get updated

00:28:16,490 --> 00:28:21,679
there's just the single message that

00:28:18,320 --> 00:28:23,929
lives in this Kafka log forever what law

00:28:21,679 --> 00:28:25,880
compaction does is in fact it's it scans

00:28:23,929 --> 00:28:28,520
the log periodically and it deletes

00:28:25,880 --> 00:28:30,559
older values to maintain only the latest

00:28:28,520 --> 00:28:31,940
value for a key which is what you want

00:28:30,559 --> 00:28:35,149
in your database which is the latest

00:28:31,940 --> 00:28:38,029
value of a key what that means is that

00:28:35,149 --> 00:28:39,950
every row that exists in your external

00:28:38,029 --> 00:28:42,380
database also exists in this log

00:28:39,950 --> 00:28:44,659
compacted topic so if you want to

00:28:42,380 --> 00:28:46,490
recreate your state of database you set

00:28:44,659 --> 00:28:50,779
your offset to zero and scan this log

00:28:46,490 --> 00:28:54,049
compacted table in this log abstraction

00:28:50,779 --> 00:28:56,630
when combined with the law compaction

00:28:54,049 --> 00:28:58,759
capability actually solves a really hard

00:28:56,630 --> 00:29:01,940
problem for stream processing which is

00:28:58,759 --> 00:29:04,940
reprocessing data now the reason you did

00:29:01,940 --> 00:29:06,799
you do want this is you know let's

00:29:04,940 --> 00:29:09,829
consider the example of an application

00:29:06,799 --> 00:29:12,589
that reads a stream of user visits to

00:29:09,829 --> 00:29:15,109
your website and it computes aggregates

00:29:12,589 --> 00:29:17,509
per geo region and it updates some kind

00:29:15,109 --> 00:29:20,239
of Google Analytics tile dashboard that

00:29:17,509 --> 00:29:22,879
your user then feeds but then assume

00:29:20,239 --> 00:29:26,059
that you know you have to upgrade our

00:29:22,879 --> 00:29:28,009
application to completely remove some

00:29:26,059 --> 00:29:29,449
kind of Geo codes now when you deploy

00:29:28,009 --> 00:29:31,849
their application you know that your

00:29:29,449 --> 00:29:34,339
dashboard shows incorrect counts for

00:29:31,849 --> 00:29:36,469
some past windows which is not a great

00:29:34,339 --> 00:29:39,319
experience so what you want to do is in

00:29:36,469 --> 00:29:41,209
fact reprocess past windows to reflect

00:29:39,319 --> 00:29:46,279
the counts to reflect your latest

00:29:41,209 --> 00:29:49,969
business logic so note that with law

00:29:46,279 --> 00:29:51,919
compaction every row that exists in your

00:29:49,969 --> 00:29:53,449
external state also exists in this law

00:29:51,919 --> 00:29:55,579
compactor topic so if you wanted to

00:29:53,449 --> 00:29:58,639
reprocess something you set the offset

00:29:55,579 --> 00:30:00,109
to zero and you scan ahead and you

00:29:58,639 --> 00:30:02,419
recompute your state so here's how this

00:30:00,109 --> 00:30:04,339
works you have some you have your

00:30:02,419 --> 00:30:06,199
application that is reading from the

00:30:04,339 --> 00:30:08,659
tail and it's updating the state where

00:30:06,199 --> 00:30:11,649
your user reads are happening if you

00:30:08,659 --> 00:30:15,049
wanted to reprocess the data you start a

00:30:11,649 --> 00:30:17,719
separate independent instance of this

00:30:15,049 --> 00:30:19,759
state and you set your offset to zero

00:30:17,719 --> 00:30:21,169
this is actually possible in a pub sub

00:30:19,759 --> 00:30:23,440
system like Kafka because you can

00:30:21,169 --> 00:30:25,849
independently consume the same topic

00:30:23,440 --> 00:30:28,429
initially the state starts off empty but

00:30:25,849 --> 00:30:31,849
then as it reads it catches up it fills

00:30:28,429 --> 00:30:34,369
up slowly until when it reaches the end

00:30:31,849 --> 00:30:36,369
when it reaches the end now you have two

00:30:34,369 --> 00:30:38,419
versions of your state store your

00:30:36,369 --> 00:30:40,579
application is in fact reading from the

00:30:38,419 --> 00:30:42,109
old one but you can flip the switch on

00:30:40,579 --> 00:30:44,779
your load balancer and have your reads

00:30:42,109 --> 00:30:47,629
happen from this new version of your

00:30:44,779 --> 00:30:50,629
application which now you know updates

00:30:47,629 --> 00:30:55,219
the concert now it shows the counts that

00:30:50,629 --> 00:30:57,739
reflect your latest business logic and

00:30:55,219 --> 00:30:59,929
last but not the least you know time the

00:30:57,739 --> 00:31:02,209
concept of time as I mentioned is very

00:30:59,929 --> 00:31:03,769
important to correctness in stream

00:31:02,209 --> 00:31:05,929
processing in fact we've paid a ton of

00:31:03,769 --> 00:31:08,690
attention to modeling time correctly in

00:31:05,929 --> 00:31:11,839
Kafka streams and her work is influenced

00:31:08,690 --> 00:31:12,970
by this insight shared by the dataflow

00:31:11,839 --> 00:31:15,580
team at

00:31:12,970 --> 00:31:19,419
which essentially says that no stream

00:31:15,580 --> 00:31:23,200
data is never complete and Canon will

00:31:19,419 --> 00:31:25,570
always you know arrive out of order what

00:31:23,200 --> 00:31:27,039
that means is you know your stream

00:31:25,570 --> 00:31:29,440
processing library needs to have a

00:31:27,039 --> 00:31:30,760
first-class ability to deal with out of

00:31:29,440 --> 00:31:33,370
order data it shouldn't be an

00:31:30,760 --> 00:31:36,309
afterthought so with respect to time

00:31:33,370 --> 00:31:39,279
there are two concepts worth paying

00:31:36,309 --> 00:31:41,769
attention to one is event time which is

00:31:39,279 --> 00:31:44,740
when your event occurs or gets created

00:31:41,769 --> 00:31:48,070
and the second is processing time which

00:31:44,740 --> 00:31:50,320
is when it gets processed now due to

00:31:48,070 --> 00:31:52,059
various delays and bottlenecks these two

00:31:50,320 --> 00:31:56,440
things can actually converge and diverge

00:31:52,059 --> 00:31:58,149
and the loss of correctness in a lot of

00:31:56,440 --> 00:32:00,190
stream processing systems actually

00:31:58,149 --> 00:32:01,720
occurs because they conflate these two

00:32:00,190 --> 00:32:04,120
concepts which leads to incorrect

00:32:01,720 --> 00:32:06,820
results so as an example let's take the

00:32:04,120 --> 00:32:08,889
example of windowing back to our example

00:32:06,820 --> 00:32:11,649
of the application that reads user

00:32:08,889 --> 00:32:13,029
visits updates a dashboard it's likely

00:32:11,649 --> 00:32:16,570
it is doing some kind of window tag

00:32:13,029 --> 00:32:19,149
Ricketts let's assume that a mobile user

00:32:16,570 --> 00:32:21,279
visits the website and right before the

00:32:19,149 --> 00:32:24,669
event reaches the application servers it

00:32:21,279 --> 00:32:27,909
loses Network coverage only to regain it

00:32:24,669 --> 00:32:29,769
you know maybe 12 hours later so 12

00:32:27,909 --> 00:32:31,960
hours later when the event reaches the

00:32:29,769 --> 00:32:34,240
application servers now you have two

00:32:31,960 --> 00:32:37,659
choices you can either count it in the

00:32:34,240 --> 00:32:39,130
current 10-minute window or you can

00:32:37,659 --> 00:32:41,529
count it in the 10 minute window

00:32:39,130 --> 00:32:44,260
12 hours ago which is when that event

00:32:41,529 --> 00:32:46,269
actually happened if you did the latter

00:32:44,260 --> 00:32:48,100
then it reflects the true state of the

00:32:46,269 --> 00:32:50,019
world which is that you're counting by

00:32:48,100 --> 00:32:52,149
event time but if you did the former

00:32:50,019 --> 00:32:54,730
then you're counting by processing time

00:32:52,149 --> 00:32:56,380
which in fact reflects incorrect counts

00:32:54,730 --> 00:32:59,380
the event did not happen right now it

00:32:56,380 --> 00:33:01,750
happened 12 hours later the implication

00:32:59,380 --> 00:33:03,490
that this insight has is that your

00:33:01,750 --> 00:33:07,059
stream processing thing has to have the

00:33:03,490 --> 00:33:10,240
ability to update past data for a stream

00:33:07,059 --> 00:33:12,519
or update past Windows and this is

00:33:10,240 --> 00:33:14,710
closely tied to a fundamental

00:33:12,519 --> 00:33:16,210
abstraction that kafka streams provides

00:33:14,710 --> 00:33:18,549
which is what I'm going to talk about

00:33:16,210 --> 00:33:22,450
next but before that you know we've

00:33:18,549 --> 00:33:24,510
rounded up talking about the fundamental

00:33:22,450 --> 00:33:26,470
problems in stream processing as well as

00:33:24,510 --> 00:33:28,960
reactor streams and how

00:33:26,470 --> 00:33:32,220
Kafka and Kafka stream's manages these

00:33:28,960 --> 00:33:34,630
next up I'm gonna talk about important

00:33:32,220 --> 00:33:37,870
abstraction that I just mentioned is

00:33:34,630 --> 00:33:41,230
fundamental to solving you know updating

00:33:37,870 --> 00:33:43,240
past problems or past data for your

00:33:41,230 --> 00:33:46,780
stream which is essentially the stream

00:33:43,240 --> 00:33:49,690
table duality now traditional databases

00:33:46,780 --> 00:33:52,809
are all about maintaining tables full of

00:33:49,690 --> 00:33:55,299
state but what they don't do such a good

00:33:52,809 --> 00:33:58,299
job of is reacting to streams of events

00:33:55,299 --> 00:34:03,970
where event could be anything from ping

00:33:58,299 --> 00:34:06,070
from a mobile device or weblog stream

00:34:03,970 --> 00:34:07,990
processing systems like storm are coming

00:34:06,070 --> 00:34:09,940
from the other end of the equation they

00:34:07,990 --> 00:34:13,750
are great at processing streams of

00:34:09,940 --> 00:34:16,830
events but computing state off of that

00:34:13,750 --> 00:34:20,349
stream is somewhat of an afterthought I

00:34:16,830 --> 00:34:22,179
will argue that you know fundamentally

00:34:20,349 --> 00:34:25,510
what needs to happen in an asynchronous

00:34:22,179 --> 00:34:27,909
application is combining tables which

00:34:25,510 --> 00:34:31,240
represent the current state of the world

00:34:27,909 --> 00:34:33,849
with streams that represents how that

00:34:31,240 --> 00:34:36,040
state is changing let's take an example

00:34:33,849 --> 00:34:38,440
to understand this duality you know this

00:34:36,040 --> 00:34:41,589
is a very simple example of a Kafka

00:34:38,440 --> 00:34:43,570
stream it has three messages in every

00:34:41,589 --> 00:34:47,169
message in Kafka is a key in a value the

00:34:43,570 --> 00:34:49,149
keys in fact optional but exists here if

00:34:47,169 --> 00:34:53,230
you notice closely the stream is special

00:34:49,149 --> 00:34:56,260
it updates the value you know later

00:34:53,230 --> 00:34:59,080
messages update value for keys that are

00:34:56,260 --> 00:35:02,890
present in the previous messages so lets

00:34:59,080 --> 00:35:03,640
you know call this an update string as a

00:35:02,890 --> 00:35:06,369
thought exercise

00:35:03,640 --> 00:35:08,920
let's try converting this update stream

00:35:06,369 --> 00:35:12,099
to a table so with the first message you

00:35:08,920 --> 00:35:14,890
create the first row where the key is

00:35:12,099 --> 00:35:16,780
the primary key of the row if you read

00:35:14,890 --> 00:35:18,760
the second message you add a second row

00:35:16,780 --> 00:35:21,940
and the third message in fact just

00:35:18,760 --> 00:35:25,930
updates the first row okay that's great

00:35:21,940 --> 00:35:27,430
but let's take a step further and take a

00:35:25,930 --> 00:35:30,280
look at the changelog stream for this

00:35:27,430 --> 00:35:33,130
table for those fear not aware databases

00:35:30,280 --> 00:35:36,190
to this but change logs are streams

00:35:33,130 --> 00:35:39,820
where message corresponds to any row

00:35:36,190 --> 00:35:40,570
that got updated so notice the changelog

00:35:39,820 --> 00:35:42,490
stream

00:35:40,570 --> 00:35:44,830
actually is exactly the update stream

00:35:42,490 --> 00:35:46,960
that we initially started with so update

00:35:44,830 --> 00:35:49,300
streams can be converted to tables can

00:35:46,960 --> 00:35:52,270
be converted to streams streams and

00:35:49,300 --> 00:35:56,200
tables are you on so you might think you

00:35:52,270 --> 00:35:57,940
know so what so let's go back to you

00:35:56,200 --> 00:35:59,800
know our a retail example and let's

00:35:57,940 --> 00:36:02,860
explore this duality in both directions

00:35:59,800 --> 00:36:06,490
starting from converting streams to

00:36:02,860 --> 00:36:08,350
tables you know recollect that streams

00:36:06,490 --> 00:36:11,470
in our example or streams of sales and

00:36:08,350 --> 00:36:14,050
shipments let's say if we join these two

00:36:11,470 --> 00:36:17,290
streams of sales and shipments then you

00:36:14,050 --> 00:36:19,000
get a really important view which is a

00:36:17,290 --> 00:36:20,440
real-time view of the inventory on hand

00:36:19,000 --> 00:36:22,570
table so here's how this works

00:36:20,440 --> 00:36:25,720
you have sales and shipments streams for

00:36:22,570 --> 00:36:28,090
simplicity they have the same format our

00:36:25,720 --> 00:36:30,040
key is the item ID and the store code

00:36:28,090 --> 00:36:33,400
which is a composite key and the count

00:36:30,040 --> 00:36:34,990
is the value now note that both these

00:36:33,400 --> 00:36:38,950
streams are actually update streams

00:36:34,990 --> 00:36:40,990
later messages give updates for keys

00:36:38,950 --> 00:36:43,120
that were appeared earlier in the

00:36:40,990 --> 00:36:45,250
message so we could convert and these

00:36:43,120 --> 00:36:50,340
streams are represented internally as

00:36:45,250 --> 00:36:53,260
tables now as part of the join operation

00:36:50,340 --> 00:36:55,900
what happens here is pretty simple with

00:36:53,260 --> 00:36:59,710
a message on the shipment stream you

00:36:55,900 --> 00:37:03,190
increment the value in the row in the

00:36:59,710 --> 00:37:05,230
table and for a message on the sales

00:37:03,190 --> 00:37:08,110
stream you decrement the value for that

00:37:05,230 --> 00:37:10,270
item in the store so the end of this

00:37:08,110 --> 00:37:13,060
we've converted these two important

00:37:10,270 --> 00:37:16,170
streams into a table which is the

00:37:13,060 --> 00:37:18,880
real-time view of the inventory on hang

00:37:16,170 --> 00:37:20,890
now let's explore this duality from the

00:37:18,880 --> 00:37:23,020
other direction which is you know tables

00:37:20,890 --> 00:37:24,970
to streams so we have this table which

00:37:23,020 --> 00:37:27,670
is the inventory on hand as a result of

00:37:24,970 --> 00:37:31,510
the join operation as I mentioned every

00:37:27,670 --> 00:37:33,700
table is backed by a Kafka topic the

00:37:31,510 --> 00:37:35,170
concept of this is a changelog stream so

00:37:33,700 --> 00:37:37,630
this is what a changelog stream looks

00:37:35,170 --> 00:37:39,790
like for this specific inventory on hand

00:37:37,630 --> 00:37:42,940
table the things in orange on the left

00:37:39,790 --> 00:37:46,210
are the rows that got updated and every

00:37:42,940 --> 00:37:49,570
message here is you know a message where

00:37:46,210 --> 00:37:51,970
a row guard updated okay so this change

00:37:49,570 --> 00:37:54,400
lock stream actually implements or it

00:37:51,970 --> 00:37:56,890
enables two very important

00:37:54,400 --> 00:37:59,859
functions of retail companies that are

00:37:56,890 --> 00:38:02,410
possibly not implemented this way which

00:37:59,859 --> 00:38:04,329
is price adjustments which is what

00:38:02,410 --> 00:38:09,039
Amazon does a great job of in real-time

00:38:04,329 --> 00:38:12,160
and inventory adjustments so what the

00:38:09,039 --> 00:38:14,410
you know the reorder sort of inventory

00:38:12,160 --> 00:38:16,960
application does is it consumes all it

00:38:14,410 --> 00:38:19,630
subscribes to this changelog stream and

00:38:16,960 --> 00:38:21,730
for every message it checks the latest

00:38:19,630 --> 00:38:24,400
inventory count if it drops below a

00:38:21,730 --> 00:38:27,880
threshold it triggers the reorder

00:38:24,400 --> 00:38:29,890
inventory our action the price

00:38:27,880 --> 00:38:31,869
adjustment application does something

00:38:29,890 --> 00:38:34,000
completely different it subscribes to

00:38:31,869 --> 00:38:36,880
the same changelog stream but it

00:38:34,000 --> 00:38:38,890
consumes messages it computes some kind

00:38:36,880 --> 00:38:41,319
of a demand model which is possibly

00:38:38,890 --> 00:38:44,200
internal state stored within the

00:38:41,319 --> 00:38:48,339
processor and then it and it when it

00:38:44,200 --> 00:38:50,680
triggers the change price events now

00:38:48,339 --> 00:38:52,630
going one step further the demand model

00:38:50,680 --> 00:38:55,119
that is computed by this application in

00:38:52,630 --> 00:38:57,430
fact will have its own change log stream

00:38:55,119 --> 00:38:59,289
that might enable some real-time

00:38:57,430 --> 00:39:02,380
marketing campaign applications so this

00:38:59,289 --> 00:39:06,279
is in fact useful for several downstream

00:39:02,380 --> 00:39:08,200
applications you can see an emergent

00:39:06,279 --> 00:39:10,750
property of some of the concepts I just

00:39:08,200 --> 00:39:13,690
introduced that may not be as obvious is

00:39:10,750 --> 00:39:16,450
then this stream table duality when

00:39:13,690 --> 00:39:19,109
combined with local state that is made

00:39:16,450 --> 00:39:23,200
queryable allows us to develop these

00:39:19,109 --> 00:39:25,150
stateful applications very easily here's

00:39:23,200 --> 00:39:27,940
how this works for this example back to

00:39:25,150 --> 00:39:29,829
our example of this joint operation

00:39:27,940 --> 00:39:32,740
where we get an inventory on hand table

00:39:29,829 --> 00:39:35,109
if you look at the application view of

00:39:32,740 --> 00:39:36,609
things we have several instances of your

00:39:35,109 --> 00:39:39,039
application that does this join

00:39:36,609 --> 00:39:41,650
operation every instance of your

00:39:39,039 --> 00:39:43,960
application has a subset of the sales in

00:39:41,650 --> 00:39:46,500
shipment stream as a result of that it

00:39:43,960 --> 00:39:48,640
has a subset or a shards of the

00:39:46,500 --> 00:39:53,130
inventory on hand table which is

00:39:48,640 --> 00:39:56,170
embedded internally now let's consider

00:39:53,130 --> 00:39:58,390
that kafka streams in fact maize makes

00:39:56,170 --> 00:40:00,940
this you know internal table queryable

00:39:58,390 --> 00:40:03,490
and this is in fact a plan feature and

00:40:00,940 --> 00:40:07,640
kafka streams coming up soon which is

00:40:03,490 --> 00:40:09,740
queryable state if you had this in

00:40:07,640 --> 00:40:12,380
Journal state table which is variable

00:40:09,740 --> 00:40:15,080
than you your application can now expose

00:40:12,380 --> 00:40:18,050
the rest api which allows you to get the

00:40:15,080 --> 00:40:21,650
latest gown for every item in every

00:40:18,050 --> 00:40:23,510
store so what we've done there you know

00:40:21,650 --> 00:40:25,310
here with respect to this application is

00:40:23,510 --> 00:40:27,620
recollect this is an asynchronous

00:40:25,310 --> 00:40:30,620
stateful application what we've done is

00:40:27,620 --> 00:40:32,270
we've combined the table which

00:40:30,620 --> 00:40:34,400
represents the current state of the

00:40:32,270 --> 00:40:35,870
world which represents the current state

00:40:34,400 --> 00:40:37,360
of the inventory for this retail

00:40:35,870 --> 00:40:40,040
business and we've combined it with

00:40:37,360 --> 00:40:43,370
streams that represent how that state is

00:40:40,040 --> 00:40:45,850
changing now this may not be you know

00:40:43,370 --> 00:40:48,110
practical in all sorts of applications

00:40:45,850 --> 00:40:50,750
oftentimes you just want to stick your

00:40:48,110 --> 00:40:53,060
state in an external database and get

00:40:50,750 --> 00:40:55,100
done with it however if you have some

00:40:53,060 --> 00:40:56,660
kind of lightweight operation you

00:40:55,100 --> 00:40:59,270
probably don't want to maintain a

00:40:56,660 --> 00:41:01,610
distributed database or if you have ton

00:40:59,270 --> 00:41:04,070
of data that you need to process your

00:41:01,610 --> 00:41:07,100
data then you possibly will benefit from

00:41:04,070 --> 00:41:09,260
using local state but is either a hash

00:41:07,100 --> 00:41:13,400
table or a rocks TB store and can be

00:41:09,260 --> 00:41:15,560
very powerful now some of the things you

00:41:13,400 --> 00:41:19,190
know to conclude sort of Kafka seems

00:41:15,560 --> 00:41:23,450
helps you build these loosely coupled

00:41:19,190 --> 00:41:25,250
you know stateful stateful applications

00:41:23,450 --> 00:41:27,290
with ease in other words it allows you

00:41:25,250 --> 00:41:32,570
to build reactive and stateful

00:41:27,290 --> 00:41:34,790
applications within a common theme that

00:41:32,570 --> 00:41:36,500
you will observe and that is worth

00:41:34,790 --> 00:41:39,500
mentioning here in Kafka as well as

00:41:36,500 --> 00:41:42,650
Kafka shims is that simplicity is valued

00:41:39,500 --> 00:41:44,950
what our experience has been is that for

00:41:42,650 --> 00:41:50,230
successful adoption of infrastructure

00:41:44,950 --> 00:41:53,270
operational simplicity is you know key

00:41:50,230 --> 00:41:56,030
for example you know this is the

00:41:53,270 --> 00:41:57,530
architecture that you end up with if for

00:41:56,030 --> 00:41:59,750
example your stream processing system

00:41:57,530 --> 00:42:01,910
does not have any support for aggregates

00:41:59,750 --> 00:42:03,710
so it has to store the aggregates in

00:42:01,910 --> 00:42:05,840
some external key value store and hence

00:42:03,710 --> 00:42:07,820
you know deploy and maintain and

00:42:05,840 --> 00:42:10,670
understand that key value store keep it

00:42:07,820 --> 00:42:13,730
in sync or with your stream processing

00:42:10,670 --> 00:42:16,190
job if your stream processing system has

00:42:13,730 --> 00:42:18,470
no ability to allow reprocessing of data

00:42:16,190 --> 00:42:21,170
then you have to reprocess that data

00:42:18,470 --> 00:42:22,930
anyway through a batch pipeline that

00:42:21,170 --> 00:42:25,580
might

00:42:22,930 --> 00:42:28,190
this might work in fact this is um a

00:42:25,580 --> 00:42:30,110
popular lambda architecture but in the

00:42:28,190 --> 00:42:32,300
end what your application now looks like

00:42:30,110 --> 00:42:34,610
is it's pretty complex it has to query

00:42:32,300 --> 00:42:36,890
these two independent views one is

00:42:34,610 --> 00:42:41,450
offline one is online merge the results

00:42:36,890 --> 00:42:43,760
before showing that to the user this

00:42:41,450 --> 00:42:46,040
works but the number of distributed

00:42:43,760 --> 00:42:47,870
thing is that you have now committed to

00:42:46,040 --> 00:42:50,720
write a simple stream processing

00:42:47,870 --> 00:42:53,780
application is has increased a lot this

00:42:50,720 --> 00:42:55,670
is lot of moving parts and that is what

00:42:53,780 --> 00:42:57,560
might you know stop application

00:42:55,670 --> 00:43:00,740
developers from writing stream

00:42:57,560 --> 00:43:02,390
processing systems or with ease in fact

00:43:00,740 --> 00:43:04,520
with Kafka streams we've paid a ton of

00:43:02,390 --> 00:43:07,280
attention to this we've taken aggregates

00:43:04,520 --> 00:43:09,250
we've provided that as optional local

00:43:07,280 --> 00:43:12,650
state that is available out of the box

00:43:09,250 --> 00:43:14,570
we've taken reprocessing and provided

00:43:12,650 --> 00:43:18,500
that as a primitive that is available

00:43:14,570 --> 00:43:20,720
from Kafka and Kafka shrimps so now what

00:43:18,500 --> 00:43:23,270
your application looks like is fairly

00:43:20,720 --> 00:43:26,170
simple and that is important in order to

00:43:23,270 --> 00:43:30,890
make stream processing approachable to

00:43:26,170 --> 00:43:32,330
mainstream application development so

00:43:30,890 --> 00:43:34,670
you might have concluded you know you

00:43:32,330 --> 00:43:37,780
love Kafka streams but as you've

00:43:34,670 --> 00:43:40,940
observed it only works on top of Kafka

00:43:37,780 --> 00:43:42,830
so how do you get your data in and out

00:43:40,940 --> 00:43:45,620
of Kafka in order to use Kafka streams

00:43:42,830 --> 00:43:47,840
and the answer is a Kafka Connect in the

00:43:45,620 --> 00:43:49,910
O 9 release of Apache Kafka the

00:43:47,840 --> 00:43:51,530
community released a feature called

00:43:49,910 --> 00:43:54,710
Kafka Connect which is essentially a

00:43:51,530 --> 00:43:58,010
framework that makes it easy to develop

00:43:54,710 --> 00:44:00,200
and use connections to Kafka connectors

00:43:58,010 --> 00:44:03,260
to Kafka to all sorts of other systems

00:44:00,200 --> 00:44:04,730
and sources what it does is it it solves

00:44:03,260 --> 00:44:07,010
a lot of common problems that every

00:44:04,730 --> 00:44:09,230
connector needs to worry about which is

00:44:07,010 --> 00:44:11,810
scale out partitioning offset management

00:44:09,230 --> 00:44:14,180
you know exactly ones or at least one

00:44:11,810 --> 00:44:16,940
semantics and so on and in the five or

00:44:14,180 --> 00:44:18,380
six months that we've released this the

00:44:16,940 --> 00:44:20,390
community has written more than 30

00:44:18,380 --> 00:44:23,960
connectors to all sorts of systems from

00:44:20,390 --> 00:44:26,660
Cassandra elasticsearch to Oracle in my

00:44:23,960 --> 00:44:30,740
sequel databases and the list will keep

00:44:26,660 --> 00:44:33,260
on growing so this is how you know this

00:44:30,740 --> 00:44:35,540
all plays out in the big picture this is

00:44:33,260 --> 00:44:35,870
what I am interested in exploring which

00:44:35,540 --> 00:44:39,380
is

00:44:35,870 --> 00:44:42,680
the ability to deploy this kafka base

00:44:39,380 --> 00:44:45,740
stream data platform as essentially a

00:44:42,680 --> 00:44:48,920
central nervous system of the company it

00:44:45,740 --> 00:44:52,220
is the basis for development and

00:44:48,920 --> 00:44:55,220
communication between asynchronous micro

00:44:52,220 --> 00:44:58,040
services and applications it is a

00:44:55,220 --> 00:44:59,930
building block for stateful stream

00:44:58,040 --> 00:45:02,170
processing applications as well as an

00:44:59,930 --> 00:45:05,360
enabler for other stream processing

00:45:02,170 --> 00:45:07,550
frameworks and clusters and last but not

00:45:05,360 --> 00:45:09,650
least it is the central source of truth

00:45:07,550 --> 00:45:13,250
pipeline that feeds your hadoop in your

00:45:09,650 --> 00:45:15,410
warehouse this is what some of us made

00:45:13,250 --> 00:45:16,760
possible at LinkedIn as well as at

00:45:15,410 --> 00:45:21,740
several other companies that have

00:45:16,760 --> 00:45:23,390
adopted Kafka at scale if you were

00:45:21,740 --> 00:45:25,400
interested in trying out Kafka streams

00:45:23,390 --> 00:45:27,260
you can download the confluent platform

00:45:25,400 --> 00:45:30,830
or check out the OU 10 release of Apache

00:45:27,260 --> 00:45:33,380
Kafka admittedly some of the things I've

00:45:30,830 --> 00:45:36,320
talked about our ahead of what Kafka

00:45:33,380 --> 00:45:38,150
streams has in today's version of the

00:45:36,320 --> 00:45:40,190
software but we are looking for feedback

00:45:38,150 --> 00:45:41,930
these are early days you know give it a

00:45:40,190 --> 00:45:43,760
try if you're already a Kafka user this

00:45:41,930 --> 00:45:48,020
is a great way to try out new

00:45:43,760 --> 00:45:49,880
functionality a lot of the ideas you

00:45:48,020 --> 00:45:51,740
know that we've talked about here and in

00:45:49,880 --> 00:45:54,290
fact some of the talks at Berlin

00:45:51,740 --> 00:45:56,900
buzzwords have adopted are actually

00:45:54,290 --> 00:45:59,120
written on the confluent blog so if

00:45:56,900 --> 00:46:00,710
you're interested check it out and last

00:45:59,120 --> 00:46:02,480
but not least you can also check out the

00:46:00,710 --> 00:46:04,370
book that is coming up on Kafka which is

00:46:02,480 --> 00:46:06,700
Kafka the definitive guide thank you

00:46:04,370 --> 00:46:06,700
very much

00:46:13,589 --> 00:46:18,759
Thank You Neha for this interesting talk

00:46:16,419 --> 00:46:21,279
I'm sure there are questions we have two

00:46:18,759 --> 00:46:25,179
microphones up here so if you would just

00:46:21,279 --> 00:46:32,439
want to step forward and ask questions

00:46:25,179 --> 00:46:34,799
to Neha we have one hi

00:46:32,439 --> 00:46:40,599
I have a question about partitioning

00:46:34,799 --> 00:46:42,579
Kafka yeah so normally if you want to to

00:46:40,599 --> 00:46:43,989
scale out processing of a topic you

00:46:42,579 --> 00:46:46,959
partition the topic and you define

00:46:43,989 --> 00:46:48,160
consumer group and as I know Sam Kafka

00:46:46,959 --> 00:46:49,839
will automatically detect failures

00:46:48,160 --> 00:46:51,999
inside this consumer group and

00:46:49,839 --> 00:46:55,779
reallocate consumers to cover all the

00:46:51,999 --> 00:46:58,739
partitions Wood Kafka how would Kafka

00:46:55,779 --> 00:47:01,779
react to situations where consumers are

00:46:58,739 --> 00:47:03,369
unbalanced so some of them are there is

00:47:01,779 --> 00:47:05,739
to consumers one is two times faster

00:47:03,369 --> 00:47:08,380
than the other so each of them will

00:47:05,739 --> 00:47:10,179
consume half of the data but you can

00:47:08,380 --> 00:47:11,890
easily imagine a situation where the

00:47:10,179 --> 00:47:14,979
first one have nothing to do and the

00:47:11,890 --> 00:47:17,829
slow one will get stuck forever and the

00:47:14,979 --> 00:47:20,079
queue will grow infinitely how does this

00:47:17,829 --> 00:47:22,089
situation so it's solved yes there are

00:47:20,079 --> 00:47:24,849
two you know parts to your question one

00:47:22,089 --> 00:47:28,239
is how does you know how does in general

00:47:24,849 --> 00:47:30,640
Kafka consumers deal with you know

00:47:28,239 --> 00:47:34,539
falling behind do you run out of queue

00:47:30,640 --> 00:47:36,459
space and that answer is you know tied

00:47:34,539 --> 00:47:38,140
to how Kafka handles back pressure so it

00:47:36,459 --> 00:47:40,779
is a persistent queue don't you don't

00:47:38,140 --> 00:47:43,390
run out of you know you don't know not

00:47:40,779 --> 00:47:45,579
of memory it just sits in this Kafka log

00:47:43,390 --> 00:47:47,679
what you will see is your car your

00:47:45,579 --> 00:47:50,289
consumers offset starts falling behind

00:47:47,679 --> 00:47:53,169
and that is because Kafka's consumer

00:47:50,289 --> 00:47:55,359
model is pull based not push based so it

00:47:53,169 --> 00:47:56,919
actually pulls data as it requires if

00:47:55,359 --> 00:47:59,049
it's slow it's the only thing that gets

00:47:56,919 --> 00:48:00,849
affected the second question you had is

00:47:59,049 --> 00:48:03,069
you know how does the load balancing

00:48:00,849 --> 00:48:06,519
actually work on the consumer so the way

00:48:03,069 --> 00:48:08,529
it works is the policy is pluggable so

00:48:06,519 --> 00:48:10,449
the default policy that might chip with

00:48:08,529 --> 00:48:13,359
the Kafka consumer might be something as

00:48:10,449 --> 00:48:15,159
simple as wrong Robin or it could be you

00:48:13,359 --> 00:48:17,679
know the one that Kafka streams uses is

00:48:15,159 --> 00:48:19,959
Co partitioning if you want to use

00:48:17,679 --> 00:48:21,530
joints where the two partitions from the

00:48:19,959 --> 00:48:24,500
same topic need to

00:48:21,530 --> 00:48:27,350
when the same processor so in fact it's

00:48:24,500 --> 00:48:30,710
pluggable you know you could use various

00:48:27,350 --> 00:48:32,300
different mechanisms I from my

00:48:30,710 --> 00:48:34,610
experience would have noted is that the

00:48:32,300 --> 00:48:37,640
simplest one is you over partition your

00:48:34,610 --> 00:48:39,140
topic and you have the ability to just

00:48:37,640 --> 00:48:42,680
round-robin the partitions amongst

00:48:39,140 --> 00:48:44,450
consumers if if there happened to be you

00:48:42,680 --> 00:48:47,690
know this if you happen to be one

00:48:44,450 --> 00:48:51,290
particular partition that is hot then

00:48:47,690 --> 00:48:52,520
you do end up with a case where you know

00:48:51,290 --> 00:48:54,890
you either have two repartition your

00:48:52,520 --> 00:48:56,210
data so that it skills well or you just

00:48:54,890 --> 00:49:00,410
deploy more instances of your

00:48:56,210 --> 00:49:01,970
application does that answer you if the

00:49:00,410 --> 00:49:04,040
problem is not in the partitioning but

00:49:01,970 --> 00:49:07,160
in a consumer is one of the consumers is

00:49:04,040 --> 00:49:09,620
slow so if there a way to partition the

00:49:07,160 --> 00:49:12,170
data according to consumer performance

00:49:09,620 --> 00:49:14,810
yes that's that's what I meant by you

00:49:12,170 --> 00:49:17,900
know sort of custom policies that might

00:49:14,810 --> 00:49:20,770
be applicable here yeah so some custom

00:49:17,900 --> 00:49:23,240
policies are you might want to you know

00:49:20,770 --> 00:49:25,340
partition or load balanced data in your

00:49:23,240 --> 00:49:27,830
consumer based on resource consumption

00:49:25,340 --> 00:49:30,200
where you know some of your consumers

00:49:27,830 --> 00:49:32,650
might be on you know sort of more

00:49:30,200 --> 00:49:35,150
powerful nodes while some may not and

00:49:32,650 --> 00:49:38,210
from from my expense I haven't really

00:49:35,150 --> 00:49:39,800
seen you know sort of load balancing

00:49:38,210 --> 00:49:41,150
policies that work that way but it's

00:49:39,800 --> 00:49:44,630
certainly possible if you write your own

00:49:41,150 --> 00:49:49,070
you can plug it in your consumer and you

00:49:44,630 --> 00:49:57,110
know see the effect of that any more

00:49:49,070 --> 00:50:00,860
questions well it seems that you have

00:49:57,110 --> 00:50:02,690
explained everything well or I will hang

00:50:00,860 --> 00:50:04,640
out right after this talk in case any of

00:50:02,690 --> 00:50:08,320
you have more questions thank you very

00:50:04,640 --> 00:50:08,320
much so there was no way

00:50:08,349 --> 00:50:10,410

YouTube URL: https://www.youtube.com/watch?v=JQnNHO5506w


