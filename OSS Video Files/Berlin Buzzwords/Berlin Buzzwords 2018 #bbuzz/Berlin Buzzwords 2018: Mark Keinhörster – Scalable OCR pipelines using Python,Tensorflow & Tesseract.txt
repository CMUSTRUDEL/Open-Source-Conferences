Title: Berlin Buzzwords 2018: Mark Keinhörster – Scalable OCR pipelines using Python,Tensorflow & Tesseract
Publication date: 2018-06-18
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	In this talk we make a trip through the world of text recognition with free software and go step by step through the individual sections of a flexible and scalable OCR application. In a live demo you will be shown how Tesseract is used for text recognition and how the quality can be significantly improved doing a little pre-processing with openCV. Subsequently the documents are stored and indexed in Elasticsearch to allow full-text search. All this with just a few lines of code and all in the sense of interactive programming with Jupyter.

Agenda
- Quirks and pitfalls in text recognition of scanned documents
- Potential of pre-processing with openCV
- Use Tesseract at scale
- Quantify, compare and revaluate results
- Use of Tensorflow in a production-ready application

Read more:
https://2018.berlinbuzzwords.de/18/session/scalable-ocr-pipelines-using-python-tensorflow-and-tesseract

About Mark Keinhörster:
https://2018.berlinbuzzwords.de/users/mark-keinhorster

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:04,630 --> 00:00:11,150
hello thanks a lot um yeah short for my

00:00:08,149 --> 00:00:14,840
person I'm I'm mark I am working as a

00:00:11,150 --> 00:00:17,180
data guy at code centric mainly working

00:00:14,840 --> 00:00:20,530
with Python working on deep learning

00:00:17,180 --> 00:00:22,730
tasks and a bit of data engineering and

00:00:20,530 --> 00:00:25,550
yeah I'm happy that so many people are

00:00:22,730 --> 00:00:28,070
here and I would like to to talk a

00:00:25,550 --> 00:00:31,880
little bit about the journey of OCR our

00:00:28,070 --> 00:00:34,670
journey of yeah grabbing text from from

00:00:31,880 --> 00:00:36,320
scanned pages at one of our customers

00:00:34,670 --> 00:00:38,300
and this is actually a system that we

00:00:36,320 --> 00:00:40,460
built and that's working in production

00:00:38,300 --> 00:00:42,700
and I would like to to show you a little

00:00:40,460 --> 00:00:46,840
bit so what we did how we did it and

00:00:42,700 --> 00:00:49,430
especially how easy it is to build a

00:00:46,840 --> 00:00:52,309
good working system and then where all

00:00:49,430 --> 00:00:57,829
the tweaks that you can apply to build

00:00:52,309 --> 00:00:59,300
an even better system so yeah shortly so

00:00:57,829 --> 00:01:01,010
what we are talking about for people

00:00:59,300 --> 00:01:05,509
that don't know OCR we are talking about

00:01:01,010 --> 00:01:08,899
optical character recognition and mainly

00:01:05,509 --> 00:01:11,209
we the the the least things that we are

00:01:08,899 --> 00:01:12,979
working on in reality is the OCR itself

00:01:11,209 --> 00:01:19,100
we use tesseract for it which is a

00:01:12,979 --> 00:01:21,319
library or a binary by Google awesome so

00:01:19,100 --> 00:01:23,209
let's go through it and I will show you

00:01:21,319 --> 00:01:25,670
a little bit the tips and tricks that

00:01:23,209 --> 00:01:28,130
you can do to utilize an open source

00:01:25,670 --> 00:01:30,889
library or an open source system using

00:01:28,130 --> 00:01:32,659
stuff like open CV and then we will go

00:01:30,889 --> 00:01:34,459
on and see okay where can we store data

00:01:32,659 --> 00:01:39,520
what can we do to make our results

00:01:34,459 --> 00:01:41,899
better and we will see how you can even

00:01:39,520 --> 00:01:45,799
yeah trick a little bit to get better

00:01:41,899 --> 00:01:48,859
results using elasticsearch let's start

00:01:45,799 --> 00:01:50,749
so the first thing is we want to start

00:01:48,859 --> 00:01:54,349
this what kind of little things we can

00:01:50,749 --> 00:01:55,819
do with a few lines of code best at best

00:01:54,349 --> 00:02:00,649
that we don't have to implement them

00:01:55,819 --> 00:02:03,560
ourselves to make documents better for

00:02:00,649 --> 00:02:05,149
text recognition so and we go through

00:02:03,560 --> 00:02:07,310
this and in the end we see how can we

00:02:05,149 --> 00:02:11,000
scale so it's a bit of code don't get

00:02:07,310 --> 00:02:13,490
scared this is all quite easy code the

00:02:11,000 --> 00:02:15,350
first thing is sometimes when you

00:02:13,490 --> 00:02:17,210
receive scans or a big scan pipeline

00:02:15,350 --> 00:02:17,930
people they put the cheese on the

00:02:17,210 --> 00:02:19,250
scanner

00:02:17,930 --> 00:02:22,390
sometimes they are quite corrupt a a

00:02:19,250 --> 00:02:24,500
little bit skewed rotated whatever and

00:02:22,390 --> 00:02:26,780
most of the tools that you have

00:02:24,500 --> 00:02:29,480
they have big problems and what we

00:02:26,780 --> 00:02:31,879
noticed it it's quite easy to write to

00:02:29,480 --> 00:02:33,170
write some code to rotate cake pages so

00:02:31,879 --> 00:02:37,430
let's look into this kind of algorithm

00:02:33,170 --> 00:02:40,639
it's a few lines of code so this is a

00:02:37,430 --> 00:02:43,579
sample page it's the page is bigger this

00:02:40,639 --> 00:02:47,150
one sample out of a magazine and you see

00:02:43,579 --> 00:02:49,700
this is how a lot of scans appear and if

00:02:47,150 --> 00:02:51,230
you try to recognize text in any with

00:02:49,700 --> 00:02:53,419
any tool you will get really really bad

00:02:51,230 --> 00:02:55,489
results even though these tools have a

00:02:53,419 --> 00:02:58,549
rotation engine or rotation algorithm

00:02:55,489 --> 00:03:02,239
inside it's very generic and often

00:02:58,549 --> 00:03:05,090
doesn't work so well but first thing

00:03:02,239 --> 00:03:07,340
that we can do is let's binarize this

00:03:05,090 --> 00:03:10,750
image and the next algorithm that we use

00:03:07,340 --> 00:03:13,519
it works best on on black white on black

00:03:10,750 --> 00:03:14,959
images so the first thing is so what is

00:03:13,519 --> 00:03:17,030
binarization you see here it's a

00:03:14,959 --> 00:03:20,569
threshold in and we simply say

00:03:17,030 --> 00:03:24,199
everything with a pixel value that's

00:03:20,569 --> 00:03:27,560
less than 200 it gets black and

00:03:24,199 --> 00:03:30,560
everything else is white or it's on

00:03:27,560 --> 00:03:32,930
maximum now we have a colored image so

00:03:30,560 --> 00:03:35,540
and you see here even threshold in it's

00:03:32,930 --> 00:03:37,069
super fast so I don't show you we don't

00:03:35,540 --> 00:03:39,650
have so much time I won't show you the

00:03:37,069 --> 00:03:41,569
code in life but this one is a really

00:03:39,650 --> 00:03:44,329
really fast operation we use open CV

00:03:41,569 --> 00:03:46,310
open computer vision it's an awesome

00:03:44,329 --> 00:03:49,849
open source library which is open

00:03:46,310 --> 00:03:52,400
sourced by Intel and it's it's fairly

00:03:49,849 --> 00:03:55,699
easy to convert such images with just a

00:03:52,400 --> 00:03:58,639
few lines of code one line of code okay

00:03:55,699 --> 00:04:02,090
so what's the next thing we want to

00:03:58,639 --> 00:04:03,370
detect lines to see do we have to rotate

00:04:02,090 --> 00:04:06,949
the image

00:04:03,370 --> 00:04:09,680
another great algorithm also in open CV

00:04:06,949 --> 00:04:14,060
is the half lines or probabilistic half

00:04:09,680 --> 00:04:16,579
lines with a few parameters like we give

00:04:14,060 --> 00:04:19,070
him the line length we living the max

00:04:16,579 --> 00:04:20,900
line gaps or the gaps that this

00:04:19,070 --> 00:04:24,440
algorithm searches and allows to have

00:04:20,900 --> 00:04:26,960
basically gaps in a full line so he

00:04:24,440 --> 00:04:30,260
tries to find lines when there is no gap

00:04:26,960 --> 00:04:31,760
that's longer than 20 pixels and what we

00:04:30,260 --> 00:04:35,030
do is if we can find line

00:04:31,760 --> 00:04:38,000
with just one line of code then we look

00:04:35,030 --> 00:04:43,250
over all the lines take a simple average

00:04:38,000 --> 00:04:46,910
of the angle of deadline to the 180

00:04:43,250 --> 00:04:50,960
degrees and after that we simply rotate

00:04:46,910 --> 00:04:53,360
the image which is a fairly simple ya

00:04:50,960 --> 00:04:56,390
call of OpenCV I don't even have it in

00:04:53,360 --> 00:05:01,700
here and we basically correct our image

00:04:56,390 --> 00:05:03,230
by by the average angle how does it look

00:05:01,700 --> 00:05:06,530
so what we see here is we get the lines

00:05:03,230 --> 00:05:10,040
fine we we take the coordinates out of

00:05:06,530 --> 00:05:11,900
it we calculate the angle and sum all

00:05:10,040 --> 00:05:13,940
these angles that we find up this is the

00:05:11,900 --> 00:05:15,710
nave approach of course if you have a

00:05:13,940 --> 00:05:17,960
little bit harder examples you might

00:05:15,710 --> 00:05:19,670
want to on quantities ation there are

00:05:17,960 --> 00:05:22,820
sometimes some some problems in it so

00:05:19,670 --> 00:05:26,270
what you take is the most you take the

00:05:22,820 --> 00:05:28,850
lines that are all quite similar all

00:05:26,270 --> 00:05:31,900
right we calculate the average angle and

00:05:28,850 --> 00:05:37,450
that's what we need to correct our page

00:05:31,900 --> 00:05:41,750
ok -7° let's see how it how it works

00:05:37,450 --> 00:05:44,390
quite well this is basically production

00:05:41,750 --> 00:05:47,360
code and it works really really good and

00:05:44,390 --> 00:05:49,250
you see here we find most of the lines

00:05:47,360 --> 00:05:52,430
we find we get a sum of all these

00:05:49,250 --> 00:05:55,820
average them and even these outliers you

00:05:52,430 --> 00:05:58,580
they don't hurt us so much you but you

00:05:55,820 --> 00:06:00,590
can quantify them away even that this

00:05:58,580 --> 00:06:03,140
technique it works quite awesome it even

00:06:00,590 --> 00:06:05,990
works for passports so we do a lot of

00:06:03,140 --> 00:06:08,750
text detection in passports IDs end and

00:06:05,990 --> 00:06:10,720
end and it's perfect to basically put

00:06:08,750 --> 00:06:14,390
the IDS in the right in the right way

00:06:10,720 --> 00:06:17,060
all right the next thing so we can

00:06:14,390 --> 00:06:19,130
rotate an image what we've noticed is an

00:06:17,060 --> 00:06:21,920
image has a lot of text different text

00:06:19,130 --> 00:06:24,320
sizes and and end and what we want to

00:06:21,920 --> 00:06:26,450
find out is where our text segment that

00:06:24,320 --> 00:06:27,920
looks similar and most of the time text

00:06:26,450 --> 00:06:29,390
segments that looked similar that are

00:06:27,920 --> 00:06:33,650
the segment's that are close together

00:06:29,390 --> 00:06:36,170
and that leads us to the problem let's

00:06:33,650 --> 00:06:38,000
break it a little bit down we want to

00:06:36,170 --> 00:06:40,030
find text segments first that we can

00:06:38,000 --> 00:06:43,350
feed to our algorithm or to tesseract

00:06:40,030 --> 00:06:45,590
and for that we use delation and erode

00:06:43,350 --> 00:06:49,170
this is also a few lines of code and

00:06:45,590 --> 00:06:51,720
simple probabilistic computing which

00:06:49,170 --> 00:06:54,570
means it's fast to just detect text

00:06:51,720 --> 00:06:56,940
segments and for that adulation is

00:06:54,570 --> 00:06:59,480
nothing more than a convolution it's

00:06:56,940 --> 00:07:03,510
basically a kernel everything on and

00:06:59,480 --> 00:07:06,150
it's a matrix multiplication in a

00:07:03,510 --> 00:07:08,550
sliding window and you assign the value

00:07:06,150 --> 00:07:10,890
in the in the middle of your sliding

00:07:08,550 --> 00:07:15,120
window basically here does everybody

00:07:10,890 --> 00:07:16,230
know what a convolution is no that's a

00:07:15,120 --> 00:07:18,090
convolution so if you hear it

00:07:16,230 --> 00:07:19,530
convolutional neural network it's

00:07:18,090 --> 00:07:22,650
basically pretty close to what we are

00:07:19,530 --> 00:07:24,540
doing here just you don't use the the or

00:07:22,650 --> 00:07:26,490
or end operator you simply do a

00:07:24,540 --> 00:07:28,860
multiplication by a random initialized

00:07:26,490 --> 00:07:31,230
sliding window and you add always the

00:07:28,860 --> 00:07:33,120
value of this multiplication basically

00:07:31,230 --> 00:07:35,220
in this end in the center and this is

00:07:33,120 --> 00:07:36,870
how the tens of flow or whatever

00:07:35,220 --> 00:07:39,420
convolutional networks are made up and

00:07:36,870 --> 00:07:41,130
what kind of these kind of matrices here

00:07:39,420 --> 00:07:42,840
the kernels get trained to get better

00:07:41,130 --> 00:07:46,680
features it's the whole idea of

00:07:42,840 --> 00:07:49,860
convolutional networks so let's look how

00:07:46,680 --> 00:07:53,360
this works what we see is we start

00:07:49,860 --> 00:07:56,010
delation dilation means we want to have

00:07:53,360 --> 00:07:58,290
broader whites so if we notice something

00:07:56,010 --> 00:08:02,220
white we want to make make a little bit

00:07:58,290 --> 00:08:05,460
more fat so that's nice that what we see

00:08:02,220 --> 00:08:07,530
here so we bring these kind of segments

00:08:05,460 --> 00:08:09,570
closer together and you already notice

00:08:07,530 --> 00:08:12,750
here you have got some good lines could

00:08:09,570 --> 00:08:14,520
we do the same part here again with a

00:08:12,750 --> 00:08:15,900
little bit different kernel a kernel

00:08:14,520 --> 00:08:18,930
that's not a square but the kernel

00:08:15,900 --> 00:08:20,790
that's pretty large and afterwards we do

00:08:18,930 --> 00:08:23,040
an erode and erode is basically the

00:08:20,790 --> 00:08:24,750
opposite of a delayed it just shrinks

00:08:23,040 --> 00:08:27,390
the whites a little bit down but what we

00:08:24,750 --> 00:08:30,360
do with this closing operation we narrow

00:08:27,390 --> 00:08:32,310
down the gaps so if there was a we

00:08:30,360 --> 00:08:33,930
delayed we shrink down and what we have

00:08:32,310 --> 00:08:36,150
is where was a dark point before there's

00:08:33,930 --> 00:08:38,700
no white and what we have here is we

00:08:36,150 --> 00:08:39,450
start closing together things that

00:08:38,700 --> 00:08:44,090
belong together

00:08:39,450 --> 00:08:48,930
we simply merge text with pixels and now

00:08:44,090 --> 00:08:51,420
if we do a fine contour that's also

00:08:48,930 --> 00:08:56,550
OpenCV fairly easy to find contours in

00:08:51,420 --> 00:08:57,260
an indented picture we find boxes and we

00:08:56,550 --> 00:09:00,350
find the box

00:08:57,260 --> 00:09:02,390
from the stuff that we connected and a

00:09:00,350 --> 00:09:05,000
little bit of filtering that's knave we

00:09:02,390 --> 00:09:07,220
have better approaches but basically

00:09:05,000 --> 00:09:08,810
this is already enough to filter all the

00:09:07,220 --> 00:09:11,180
boxes that do not belong so all the

00:09:08,810 --> 00:09:14,890
contours that are too small and if we

00:09:11,180 --> 00:09:17,420
see after filtering what happens an

00:09:14,890 --> 00:09:20,660
algorithm that selects stuff that

00:09:17,420 --> 00:09:22,580
belongs together in nearly every

00:09:20,660 --> 00:09:23,390
application in nearly every image with a

00:09:22,580 --> 00:09:24,980
few lines of code

00:09:23,390 --> 00:09:28,100
it's nice magic if you show this to

00:09:24,980 --> 00:09:29,630
management everybody is happy and adjust

00:09:28,100 --> 00:09:32,510
a few lines of code pretty fast

00:09:29,630 --> 00:09:33,560
operations with convolutions so these

00:09:32,510 --> 00:09:36,820
are the text parts

00:09:33,560 --> 00:09:39,490
cool let's look again we have more

00:09:36,820 --> 00:09:43,970
there's some way to talk there is the

00:09:39,490 --> 00:09:46,250
distance AI and tensor flow just to to

00:09:43,970 --> 00:09:48,920
bring bring everything down to the real

00:09:46,250 --> 00:09:51,320
thing we do not use tensor flow for

00:09:48,920 --> 00:09:54,380
object character recognition there are

00:09:51,320 --> 00:09:57,250
way better models built by way into more

00:09:54,380 --> 00:09:59,780
intelligent people then for example me

00:09:57,250 --> 00:10:02,360
with a lot of more training data to

00:09:59,780 --> 00:10:03,890
build object character recognition it's

00:10:02,360 --> 00:10:06,470
a solve problem and I would like to

00:10:03,890 --> 00:10:08,080
utilize what's get what we got but what

00:10:06,470 --> 00:10:10,340
we noticed is we had a lot of pictures

00:10:08,080 --> 00:10:13,730
pictures take processing time and

00:10:10,340 --> 00:10:15,470
pictures lead to problems and what we

00:10:13,730 --> 00:10:18,350
did is because we didn't find a solution

00:10:15,470 --> 00:10:21,830
before we trained a classifier that can

00:10:18,350 --> 00:10:24,830
decide between pictures and decide

00:10:21,830 --> 00:10:26,420
between written text so we can filter

00:10:24,830 --> 00:10:28,400
out and we can under pictures for

00:10:26,420 --> 00:10:32,840
example filter what's on the picture and

00:10:28,400 --> 00:10:34,640
put this also in an index and this

00:10:32,840 --> 00:10:36,260
classifier is a simple convolutional net

00:10:34,640 --> 00:10:38,930
looks pretty close to the amnesty

00:10:36,260 --> 00:10:43,130
example but it's fine it's fast it works

00:10:38,930 --> 00:10:47,030
pretty well the results after evaluation

00:10:43,130 --> 00:10:49,190
was a 100 percent recognition rate and I

00:10:47,030 --> 00:10:50,690
couldn't believe that so what we started

00:10:49,190 --> 00:10:52,640
here and this is the next important

00:10:50,690 --> 00:10:55,010
thing that I if you use tensorflow if

00:10:52,640 --> 00:10:57,470
you lose Kiera's if you lose use

00:10:55,010 --> 00:10:59,840
convolutional neural networks look what

00:10:57,470 --> 00:11:03,340
the network is looking on so and there

00:10:59,840 --> 00:11:08,060
is same thing it's called great cam

00:11:03,340 --> 00:11:11,090
computer-aided segmentation and

00:11:08,060 --> 00:11:13,220
what you can do is basically you can use

00:11:11,090 --> 00:11:15,710
the the gradients from your machine

00:11:13,220 --> 00:11:18,740
learning model and apply them to the

00:11:15,710 --> 00:11:21,910
last convolutional layer and basically

00:11:18,740 --> 00:11:25,100
calculate back of a sample image

00:11:21,910 --> 00:11:27,830
why did your convolutional network

00:11:25,100 --> 00:11:29,960
decided for a class and because of what

00:11:27,830 --> 00:11:32,360
segments in your image and that's what

00:11:29,960 --> 00:11:34,780
you see here so we looked at our own

00:11:32,360 --> 00:11:39,230
results and said hey what made you

00:11:34,780 --> 00:11:40,550
decide for this class for text and it's

00:11:39,230 --> 00:11:42,650
quite nice what you see here the red

00:11:40,550 --> 00:11:44,800
things it's like a heat map the red part

00:11:42,650 --> 00:11:47,560
makes the convolutional Network decide

00:11:44,800 --> 00:11:50,300
why it's text and it's looking at text

00:11:47,560 --> 00:11:52,250
and you see here the blue one this one

00:11:50,300 --> 00:11:55,550
was which is little bit deciding against

00:11:52,250 --> 00:11:58,640
it but it's fine and if we compared with

00:11:55,550 --> 00:12:01,490
an image we can see all nice there are

00:11:58,640 --> 00:12:04,490
some whatever it looks at but there are

00:12:01,490 --> 00:12:06,920
some edges that are not normal for text

00:12:04,490 --> 00:12:08,540
and because of these edges because of

00:12:06,920 --> 00:12:11,180
everything that's read it decided that

00:12:08,540 --> 00:12:13,040
this class here is a picture select the

00:12:11,180 --> 00:12:14,690
next step if you have a model and you

00:12:13,040 --> 00:12:18,110
want a little bit to understand what

00:12:14,690 --> 00:12:20,960
your model is doing try to visualize

00:12:18,110 --> 00:12:23,540
where the CNN where your network is

00:12:20,960 --> 00:12:27,410
looking at it's one learning that we got

00:12:23,540 --> 00:12:29,660
from us cool okay that's the a iPod so

00:12:27,410 --> 00:12:33,520
for the easy things but it works quite

00:12:29,660 --> 00:12:36,650
well tesseract what does it do

00:12:33,520 --> 00:12:39,640
it's an instant it's a binary that you

00:12:36,650 --> 00:12:43,430
can basically use and you have a lot of

00:12:39,640 --> 00:12:45,980
apps libraries around it like we are

00:12:43,430 --> 00:12:47,360
using pi OCR it's open source it's

00:12:45,980 --> 00:12:50,420
maintained by a small group of people

00:12:47,360 --> 00:12:52,310
but I can really really recommend it

00:12:50,420 --> 00:12:53,990
because they are really responsive so I

00:12:52,310 --> 00:12:56,300
had a question I found something and

00:12:53,990 --> 00:12:59,390
they were responsive in releasing a new

00:12:56,300 --> 00:13:02,770
version in in just a few days so really

00:12:59,390 --> 00:13:06,470
nice guys tesseract internally uses a

00:13:02,770 --> 00:13:08,510
neural network and LS TM and what you

00:13:06,470 --> 00:13:10,100
get out is a format which gives you

00:13:08,510 --> 00:13:13,940
lines words and locations that it

00:13:10,100 --> 00:13:15,800
detected how does it look PSM is the

00:13:13,940 --> 00:13:17,650
page segmentation mode there's a lot in

00:13:15,800 --> 00:13:20,630
the documentation where you can look in

00:13:17,650 --> 00:13:21,590
the interesting part is here create hoc

00:13:20,630 --> 00:13:23,630
are

00:13:21,590 --> 00:13:25,220
which gives you basically lines in lines

00:13:23,630 --> 00:13:28,520
are words that you can concatenate them

00:13:25,220 --> 00:13:30,200
and you see here also if you build the

00:13:28,520 --> 00:13:34,700
right docker container with tesseract

00:13:30,200 --> 00:13:37,130
inside it's very easy to use it and it

00:13:34,700 --> 00:13:39,800
after the pre-processing it gives you so

00:13:37,130 --> 00:13:41,960
we have a rate of 80 percent of a

00:13:39,800 --> 00:13:45,100
hundred percent recognition rate so 80

00:13:41,960 --> 00:13:49,040
percent of all images have a recognition

00:13:45,100 --> 00:13:51,830
recognition rate of 100 percent it's the

00:13:49,040 --> 00:13:56,540
next interesting question how do we get

00:13:51,830 --> 00:13:59,210
the recognition rate and that's where we

00:13:56,540 --> 00:14:02,420
use elastic search for and this is where

00:13:59,210 --> 00:14:03,740
you can really make a difference so what

00:14:02,420 --> 00:14:07,700
you see here is the the levenshtein

00:14:03,740 --> 00:14:11,810
distance that we are using if we have a

00:14:07,700 --> 00:14:13,870
vert recognized with the typo we have an

00:14:11,810 --> 00:14:16,970
index which contains a dictionary and

00:14:13,870 --> 00:14:18,650
this is the dictionary of the German

00:14:16,970 --> 00:14:21,010
language that's why you're working in

00:14:18,650 --> 00:14:23,270
and the dictionary of domain domain

00:14:21,010 --> 00:14:24,920
language for example we are working for

00:14:23,270 --> 00:14:27,010
an insurance company an insurance

00:14:24,920 --> 00:14:31,190
company is some special words in Germany

00:14:27,010 --> 00:14:33,830
for the whole domain and having these

00:14:31,190 --> 00:14:36,830
two in our dictionary gives us the

00:14:33,830 --> 00:14:40,370
possibility to get word to get suggested

00:14:36,830 --> 00:14:43,370
set Jess chance for every word that we

00:14:40,370 --> 00:14:47,060
recognized and our native approach is

00:14:43,370 --> 00:14:49,820
let's see if we find words that are in

00:14:47,060 --> 00:14:53,420
the dictionary that's a hit and if not

00:14:49,820 --> 00:14:55,220
at least give us some suggestions and if

00:14:53,420 --> 00:14:56,780
we find the document that has all words

00:14:55,220 --> 00:14:57,920
already in the dictionary and we don't

00:14:56,780 --> 00:15:00,200
need any suggestion we have a hundred

00:14:57,920 --> 00:15:01,730
percent recognition rate all right it's

00:15:00,200 --> 00:15:03,320
a quite naive approach but it works good

00:15:01,730 --> 00:15:05,959
and it gives you a nice ground truth if

00:15:03,320 --> 00:15:08,270
you work on that kind of topic plus we

00:15:05,959 --> 00:15:11,780
use the elastic search also as a search

00:15:08,270 --> 00:15:14,270
engine to have to be able to search

00:15:11,780 --> 00:15:16,970
inside pictures with this kind of

00:15:14,270 --> 00:15:22,670
metadata and you even get pictures where

00:15:16,970 --> 00:15:25,430
tesseract was detecting a typo which

00:15:22,670 --> 00:15:28,339
means we have two ways to make our model

00:15:25,430 --> 00:15:29,990
better we can make our dictionary better

00:15:28,339 --> 00:15:33,260
maybe add something to the

00:15:29,990 --> 00:15:35,180
domain-specific language we can build up

00:15:33,260 --> 00:15:38,030
on picture pre-processing and make our

00:15:35,180 --> 00:15:40,520
just better or even use new versions of

00:15:38,030 --> 00:15:42,530
tesseract and compile them maybe in a

00:15:40,520 --> 00:15:43,940
better manner to make a CEREC faster so

00:15:42,530 --> 00:15:47,000
you have a lot of little things where

00:15:43,940 --> 00:15:48,890
you can work on basically just for for

00:15:47,000 --> 00:15:50,960
detecting text and you can utilize

00:15:48,890 --> 00:15:54,200
elasticsearch that gives you some real

00:15:50,960 --> 00:15:57,710
nice suggestions and yeah makes

00:15:54,200 --> 00:16:00,320
searching way easier and this is a yeah

00:15:57,710 --> 00:16:04,400
this is the real real world and it works

00:16:00,320 --> 00:16:08,420
really fast five minutes left

00:16:04,400 --> 00:16:10,310
perfect last slide scalability now we

00:16:08,420 --> 00:16:13,640
want to talk about scalability so okay

00:16:10,310 --> 00:16:16,970
elasticsearch scalability by itself you

00:16:13,640 --> 00:16:19,670
can scale it around clusters fine

00:16:16,970 --> 00:16:24,070
but how can you scale the OCR process

00:16:19,670 --> 00:16:26,560
itself but I've shown you rotation

00:16:24,070 --> 00:16:28,370
dilation and then text segmentation

00:16:26,560 --> 00:16:31,700
sharpening of text

00:16:28,370 --> 00:16:35,660
maybe deleting some watermarks all the

00:16:31,700 --> 00:16:39,650
stuff that we do we put it into into

00:16:35,660 --> 00:16:42,290
their own containers and per container

00:16:39,650 --> 00:16:44,240
we can really scale to wherever you like

00:16:42,290 --> 00:16:46,550
so you can't make the process itself

00:16:44,240 --> 00:16:50,810
faster but where you can really well

00:16:46,550 --> 00:16:53,810
scale is you can basically make more

00:16:50,810 --> 00:16:57,080
images or or be able to detect more text

00:16:53,810 --> 00:16:59,180
in more images at the same time for

00:16:57,080 --> 00:17:03,470
example so what we notice is first of

00:16:59,180 --> 00:17:05,540
all it's a quite quite old setup which

00:17:03,470 --> 00:17:10,430
gets better is we don't have any

00:17:05,540 --> 00:17:13,010
kubernetes whatsoever so no no nice load

00:17:10,430 --> 00:17:15,140
balancing or let's say no nice load

00:17:13,010 --> 00:17:17,300
utilization but what we could notice is

00:17:15,140 --> 00:17:20,270
we can build containers with simple

00:17:17,300 --> 00:17:24,050
docker on board

00:17:20,270 --> 00:17:28,220
utilities to apply to every container

00:17:24,050 --> 00:17:31,010
and 90% load all the time which is

00:17:28,220 --> 00:17:32,270
awesome for for ops the guys from from

00:17:31,010 --> 00:17:35,060
operations they loved it because they

00:17:32,270 --> 00:17:37,520
say hey please go to 90% it's not

00:17:35,060 --> 00:17:41,420
allowing something but we utilize the

00:17:37,520 --> 00:17:43,160
full CPU all the VMS quite well and we

00:17:41,420 --> 00:17:45,350
can even structure our our

00:17:43,160 --> 00:17:47,860
infrastructure landscape to have a

00:17:45,350 --> 00:17:51,000
pre-processing parts to have

00:17:47,860 --> 00:17:52,269
rotation in parts or to have here

00:17:51,000 --> 00:17:54,370
tesseract

00:17:52,269 --> 00:17:56,289
recognition parts and this is really

00:17:54,370 --> 00:17:57,850
important because what you see is what

00:17:56,289 --> 00:17:59,230
I've shown you the small pre-processing

00:17:57,850 --> 00:18:02,980
steps they are super fast they are in

00:17:59,230 --> 00:18:06,820
0.1 seconds per image that's fine but

00:18:02,980 --> 00:18:10,299
tesseract itself 300 dps DPI's takes

00:18:06,820 --> 00:18:12,970
around 12 seconds and the goal is to

00:18:10,299 --> 00:18:14,470
have 140 images per day hundred and

00:18:12,970 --> 00:18:17,169
forty thousand images per day to

00:18:14,470 --> 00:18:19,750
recognize and for that what you need is

00:18:17,169 --> 00:18:22,149
a really really good scalability

00:18:19,750 --> 00:18:25,840
mechanism and the next part is if you

00:18:22,149 --> 00:18:30,370
look here your elastic needs to be quite

00:18:25,840 --> 00:18:32,860
well optimized so the speaker before and

00:18:30,370 --> 00:18:34,090
elastic new features some of the new

00:18:32,860 --> 00:18:36,669
features you mentioned like

00:18:34,090 --> 00:18:40,320
bootstrapping and bootstrapping checks

00:18:36,669 --> 00:18:43,120
that was where all things that hit us

00:18:40,320 --> 00:18:45,510
this part here you have to see elastic

00:18:43,120 --> 00:18:48,429
is asking for every word in the text

00:18:45,510 --> 00:18:51,700
elastic gets asked on its image onyx

00:18:48,429 --> 00:18:55,899
index did you know this word which

00:18:51,700 --> 00:18:58,059
results in 1000 calls okay with batching

00:18:55,899 --> 00:19:02,889
but 1,000 queries basically per image

00:18:58,059 --> 00:19:04,360
per use if you do this four hundred and

00:19:02,889 --> 00:19:06,130
forty thousand images per day you know

00:19:04,360 --> 00:19:08,110
what kind of load elastic is working on

00:19:06,130 --> 00:19:11,769
our current dictionaries around three

00:19:08,110 --> 00:19:14,860
million where it's not and yeah in the

00:19:11,769 --> 00:19:17,139
end we store all the results all the

00:19:14,860 --> 00:19:20,019
metadata basically in elastic and

00:19:17,139 --> 00:19:24,070
provide it as a big big search engine

00:19:20,019 --> 00:19:26,080
layer in front of texts basically that

00:19:24,070 --> 00:19:28,029
this is the small journey a little bit

00:19:26,080 --> 00:19:30,490
of what I wanted to show you and I think

00:19:28,029 --> 00:19:32,120
the five minutes are done quite fast

00:19:30,490 --> 00:19:34,600
thanks

00:19:32,120 --> 00:19:36,660
[Applause]

00:19:34,600 --> 00:19:36,660

YouTube URL: https://www.youtube.com/watch?v=9mJ0C7KttYg


