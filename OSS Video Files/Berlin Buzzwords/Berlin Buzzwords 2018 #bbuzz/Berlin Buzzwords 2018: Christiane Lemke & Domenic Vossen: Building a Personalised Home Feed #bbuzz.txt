Title: Berlin Buzzwords 2018: Christiane Lemke & Domenic Vossen: Building a Personalised Home Feed #bbuzz
Publication date: 2018-06-18
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Christiane Lemke and Domenic Vossen talking about "Building a Personalised Home Feed Using Kafka Streams and Elasticsearch"

At eBay Kleinanzeigen, we aim to inspire our users with a feed of the best items tailored to them. This becomes an interesting problem with more than 20 million monthly users and over 28 million live ads, with thousands of interactions taking place on our platform every second.

Some of the challenges that pop up are how to deal with new visitors, or ones that only visit occasionally. The posted items are often also very short-lived, as many get sold quickly. This requires us to be responsive (near real-time) with respect to our inventory and the usersâ€™ behaviour to help them find a match and be successful.

Technologies such as Kafka Streams and Elasticsearch allow us to approach the problem in a modern, elegant and scalable way, without the need for specialised clusters and long-running overnight batch jobs.

This talk is presented by eBay Tech.

Read more:
https://2018.berlinbuzzwords.de/18/session/building-personalised-home-feed-using-kafka-streams-and-elasticsearch

About Christiane Lemke:
https://2018.berlinbuzzwords.de/users/christiane-lemke

About Domenic Vossen:
https://2018.berlinbuzzwords.de/users/domenic-vossen

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:04,600 --> 00:00:09,670
yep thank you everybody like you said

00:00:07,390 --> 00:00:11,740
I'm Dominic this is Cassie Anna we both

00:00:09,670 --> 00:00:13,809
worked for eBay Clannad saggin and we

00:00:11,740 --> 00:00:16,180
wanted to share quickly before lunch a

00:00:13,809 --> 00:00:19,630
little bit of a story of how we build a

00:00:16,180 --> 00:00:21,670
personalized home feed for eBay kay you

00:00:19,630 --> 00:00:24,550
bake Lana tagging using Kafka streams

00:00:21,670 --> 00:00:26,890
and elasticsearch now first of all for

00:00:24,550 --> 00:00:30,189
those who don't know it I wanted to say

00:00:26,890 --> 00:00:32,770
a few words about eBay climate Zygon it

00:00:30,189 --> 00:00:35,950
looks like this it's the largest c2c

00:00:32,770 --> 00:00:40,329
offering here in Germany we have about

00:00:35,950 --> 00:00:43,210
20 million unique users per month and we

00:00:40,329 --> 00:00:45,789
have about 28 million live ads at any

00:00:43,210 --> 00:00:48,640
given time in our inventory and we're

00:00:45,789 --> 00:00:51,449
still rapidly growing and traditionally

00:00:48,640 --> 00:00:54,910
we've been mainly a search focused

00:00:51,449 --> 00:00:56,680
platform where we cater mostly to the

00:00:54,910 --> 00:00:57,730
people that kind of have an idea of what

00:00:56,680 --> 00:01:00,100
they're looking for they come to our

00:00:57,730 --> 00:01:01,960
site put in a query make use of our

00:01:00,100 --> 00:01:04,090
categories in attribute structure and

00:01:01,960 --> 00:01:07,540
then hopefully find what they're looking

00:01:04,090 --> 00:01:10,170
for we wanted to add a bit of a more an

00:01:07,540 --> 00:01:13,780
inspiring and discovery kind of

00:01:10,170 --> 00:01:15,040
experience on top of that so that even

00:01:13,780 --> 00:01:17,259
if you don't know exactly what you're

00:01:15,040 --> 00:01:19,270
looking for you come to our platform and

00:01:17,259 --> 00:01:21,729
we kind of inspire you with our hidden

00:01:19,270 --> 00:01:23,439
gems and hopefully something that's

00:01:21,729 --> 00:01:26,890
tailored to you so that you can just

00:01:23,439 --> 00:01:29,320
sort of browse around and yeah just have

00:01:26,890 --> 00:01:31,930
fun also on our platform and that's how

00:01:29,320 --> 00:01:34,600
we came up with the feed you know it

00:01:31,930 --> 00:01:37,780
also from other platforms when you open

00:01:34,600 --> 00:01:40,659
our home screen now you see a feed where

00:01:37,780 --> 00:01:42,310
you can scroll down and hopefully those

00:01:40,659 --> 00:01:45,549
items that you see there are tailored to

00:01:42,310 --> 00:01:50,729
you so that's what we wanted to

00:01:45,549 --> 00:01:54,820
introduce and we have actually and now

00:01:50,729 --> 00:01:57,399
why would this be tricky to give a bit

00:01:54,820 --> 00:01:59,320
of background of our our domain so we

00:01:57,399 --> 00:02:01,810
have of course lots of data so I already

00:01:59,320 --> 00:02:04,360
told you we have like 20 million unique

00:02:01,810 --> 00:02:05,259
users per month and that means that if

00:02:04,360 --> 00:02:07,509
we want to show you something

00:02:05,259 --> 00:02:09,610
interesting we need to keep track of

00:02:07,509 --> 00:02:10,990
what you're actually interested in so

00:02:09,610 --> 00:02:12,850
your interactions on the platform we

00:02:10,990 --> 00:02:15,760
need to store them process that somehow

00:02:12,850 --> 00:02:18,760
so we need a way to accommodate for that

00:02:15,760 --> 00:02:20,530
data that's continuously growing

00:02:18,760 --> 00:02:23,440
next that we have what we call

00:02:20,530 --> 00:02:25,930
occasional visitors so every six months

00:02:23,440 --> 00:02:28,540
or you have some users that come to our

00:02:25,930 --> 00:02:29,860
platform say every six months and maybe

00:02:28,540 --> 00:02:31,270
they're looking for something totally

00:02:29,860 --> 00:02:33,730
different like now they're suddenly

00:02:31,270 --> 00:02:34,990
looking for a racing bike in order to

00:02:33,730 --> 00:02:37,270
accommodate that we kind of want to

00:02:34,990 --> 00:02:39,130
capture that information in that same

00:02:37,270 --> 00:02:40,540
session so that when you come to the

00:02:39,130 --> 00:02:42,970
home screen from the search page again

00:02:40,540 --> 00:02:45,490
we immediately can show you racing bikes

00:02:42,970 --> 00:02:47,560
and items like that and not the couch

00:02:45,490 --> 00:02:51,310
that you were searching for six months

00:02:47,560 --> 00:02:53,680
ago then we also have a problem which we

00:02:51,310 --> 00:02:55,330
call yeah short-lived inventory so what

00:02:53,680 --> 00:02:58,960
we mean by short-lived inventory is kind

00:02:55,330 --> 00:03:01,270
of that we have a lot of items that are

00:02:58,960 --> 00:03:03,220
either very specialized or they're very

00:03:01,270 --> 00:03:05,490
popular and they can be gone within a

00:03:03,220 --> 00:03:07,930
few minutes and if we show you these

00:03:05,490 --> 00:03:09,340
awesome items and every time you click

00:03:07,930 --> 00:03:10,540
on them they're gone you're not going to

00:03:09,340 --> 00:03:12,970
have a good experience so we want to

00:03:10,540 --> 00:03:16,360
limit that that risk of showing you

00:03:12,970 --> 00:03:18,040
these stale items and finally we have if

00:03:16,360 --> 00:03:20,140
you have finding interesting items so we

00:03:18,040 --> 00:03:23,440
have 28 million live ads but we need to

00:03:20,140 --> 00:03:25,600
sift through that B in order to yeah in

00:03:23,440 --> 00:03:27,430
order to present the 30 or for the most

00:03:25,600 --> 00:03:33,160
interesting ones to show you immediately

00:03:27,430 --> 00:03:34,750
on the first screen cool so we just

00:03:33,160 --> 00:03:36,850
heard why it's not super straightforward

00:03:34,750 --> 00:03:39,400
for a platform like ours to build a

00:03:36,850 --> 00:03:41,200
personalized film feed but looking at

00:03:39,400 --> 00:03:42,550
other platforms in the past years we

00:03:41,200 --> 00:03:44,709
could see that more and more of them

00:03:42,550 --> 00:03:46,360
make sort of personalized feed their

00:03:44,709 --> 00:03:48,850
default landing experience of whatever

00:03:46,360 --> 00:03:50,680
they have on that platform so we can see

00:03:48,850 --> 00:03:53,410
some examples of LinkedIn Pinterest

00:03:50,680 --> 00:03:55,390
Facebook obviously but up until a year

00:03:53,410 --> 00:03:57,370
ago our Start screen from the app still

00:03:55,390 --> 00:04:00,489
looked like this so it could obviously

00:03:57,370 --> 00:04:02,410
be a little more inspiring and so before

00:04:00,489 --> 00:04:03,850
before embarking on that journey of

00:04:02,410 --> 00:04:05,860
building a first nice feed for ourselves

00:04:03,850 --> 00:04:07,360
it made all the sense in the world to

00:04:05,860 --> 00:04:10,570
have a look at what other people are

00:04:07,360 --> 00:04:12,520
doing in that area and we found a lot of

00:04:10,570 --> 00:04:14,320
stuff on the internet publications tech

00:04:12,520 --> 00:04:16,480
blog post especially helpful where

00:04:14,320 --> 00:04:18,010
Pinterest and LinkedIn so if you're

00:04:16,480 --> 00:04:21,669
working there thanks very much for that

00:04:18,010 --> 00:04:23,470
and in general we found two main

00:04:21,669 --> 00:04:25,120
directions you can take when building a

00:04:23,470 --> 00:04:27,310
personalized feed on your on your home

00:04:25,120 --> 00:04:29,830
page I'm just going to give a very

00:04:27,310 --> 00:04:31,900
high-level overview on both of them the

00:04:29,830 --> 00:04:33,280
first one is to pre calculate feeds

00:04:31,900 --> 00:04:36,550
every user that you have on your

00:04:33,280 --> 00:04:38,949
platform this is a simple overview let's

00:04:36,550 --> 00:04:40,870
go through it step by step so you start

00:04:38,949 --> 00:04:42,580
by looking at the items you have it

00:04:40,870 --> 00:04:44,500
could be pins for Pinterest for Eva

00:04:42,580 --> 00:04:46,300
Klein and siding it's its items it's our

00:04:44,500 --> 00:04:48,850
ads with the stuff that our users

00:04:46,300 --> 00:04:51,370
actually want to sell the next thing you

00:04:48,850 --> 00:04:53,919
look at is at the interactions on your

00:04:51,370 --> 00:04:56,620
on your platform so for Facebook this

00:04:53,919 --> 00:04:59,289
could be shares likes posts things like

00:04:56,620 --> 00:05:01,539
that for us it could be searches or item

00:04:59,289 --> 00:05:05,560
details view someone something piqued

00:05:01,539 --> 00:05:06,789
your interest then big magic box you

00:05:05,560 --> 00:05:08,260
start sticking it into a

00:05:06,789 --> 00:05:10,330
machine-learning pipeline with the

00:05:08,260 --> 00:05:12,220
complexity of your choice

00:05:10,330 --> 00:05:14,979
you build some sort of target function

00:05:12,220 --> 00:05:17,650
like something to do with engagement or

00:05:14,979 --> 00:05:19,600
maybe even revenue and then you build a

00:05:17,650 --> 00:05:21,789
model and you can do that in a more

00:05:19,600 --> 00:05:23,470
batch processing fashion or you can also

00:05:21,789 --> 00:05:25,210
do it event-driven as data comes in

00:05:23,470 --> 00:05:27,280
either way you'll end up with a model

00:05:25,210 --> 00:05:30,370
that would score incoming items for each

00:05:27,280 --> 00:05:34,120
user and what would come out of that

00:05:30,370 --> 00:05:36,400
would be created and created you would

00:05:34,120 --> 00:05:37,690
create feeds with them and update them

00:05:36,400 --> 00:05:39,940
as you go and you would store them

00:05:37,690 --> 00:05:42,190
somewhere so if the user comes to your

00:05:39,940 --> 00:05:44,130
platform all the works already done you

00:05:42,190 --> 00:05:46,300
just pick up the feed and that's it

00:05:44,130 --> 00:05:48,190
so what do we like about this approach

00:05:46,300 --> 00:05:49,870
obviously a retrieval is super super

00:05:48,190 --> 00:05:53,050
fast because works already done as I

00:05:49,870 --> 00:05:54,610
said you can have some very good

00:05:53,050 --> 00:05:55,900
complexity and your machine learning

00:05:54,610 --> 00:05:57,729
pipeline you can do some very cool

00:05:55,900 --> 00:05:59,470
ranking in there what we don't like

00:05:57,729 --> 00:06:00,970
about it is that a lot of the work that

00:05:59,470 --> 00:06:02,949
you do actually goes waste it because

00:06:00,970 --> 00:06:05,860
imagine you just build this awesome item

00:06:02,949 --> 00:06:07,030
is this awesome function um found these

00:06:05,860 --> 00:06:08,830
awesome items you sneak into someone's

00:06:07,030 --> 00:06:10,389
feed and then they only come back to

00:06:08,830 --> 00:06:12,310
your platform like a month later so

00:06:10,389 --> 00:06:14,590
that's very rude and by that time you

00:06:12,310 --> 00:06:18,580
update their feeds a million times for

00:06:14,590 --> 00:06:21,729
nothing it's also very hard to maintain

00:06:18,580 --> 00:06:24,159
these materialized feeds so if one items

00:06:21,729 --> 00:06:26,020
interested interesting for a million of

00:06:24,159 --> 00:06:28,120
users and it disappears on your platform

00:06:26,020 --> 00:06:30,280
then you have to delete it in a million

00:06:28,120 --> 00:06:32,740
feeds and that's maybe not a big problem

00:06:30,280 --> 00:06:36,430
for Pinterest but it is for us because

00:06:32,740 --> 00:06:38,919
our items are quite short-lived then the

00:06:36,430 --> 00:06:41,050
last point is how how fast can you be

00:06:38,919 --> 00:06:42,760
and getting new items and to users feeds

00:06:41,050 --> 00:06:45,250
depending on whether or not you do batch

00:06:42,760 --> 00:06:45,760
processing or how complex your pipeline

00:06:45,250 --> 00:06:47,140
is

00:06:45,760 --> 00:06:48,880
does it take you one minute for new

00:06:47,140 --> 00:06:51,250
items to appear does it take you half an

00:06:48,880 --> 00:06:52,810
hour we think that's quite a crucial

00:06:51,250 --> 00:06:54,790
factor for the success of your feet

00:06:52,810 --> 00:06:58,270
that's quite hard to get right in this

00:06:54,790 --> 00:07:00,310
scenario what else can you do you can

00:06:58,270 --> 00:07:04,270
also try calculating your feet on demand

00:07:00,310 --> 00:07:06,220
same thing you again look into items on

00:07:04,270 --> 00:07:08,500
your site on user interactions with your

00:07:06,220 --> 00:07:10,140
site but this time you just stick them

00:07:08,500 --> 00:07:12,970
into databases may be some sort of

00:07:10,140 --> 00:07:14,500
aggregations maybe some sort of machine

00:07:12,970 --> 00:07:18,250
learning as well to come up with clever

00:07:14,500 --> 00:07:19,720
user features in the end they end up in

00:07:18,250 --> 00:07:21,760
separate databases you could have one

00:07:19,720 --> 00:07:24,010
for your items several for your your

00:07:21,760 --> 00:07:25,960
user features and when the user comes to

00:07:24,010 --> 00:07:28,150
the site you take the information you

00:07:25,960 --> 00:07:30,730
have of the user and you search your

00:07:28,150 --> 00:07:32,710
item repository with that and you build

00:07:30,730 --> 00:07:35,140
the feeds as the user comes to the

00:07:32,710 --> 00:07:37,060
platform what do we think about that

00:07:35,140 --> 00:07:38,830
obviously we like that you don't need

00:07:37,060 --> 00:07:41,620
any extra storage for the materialized

00:07:38,830 --> 00:07:43,360
feeds then you don't have the problem of

00:07:41,620 --> 00:07:45,550
outdated items so yeah I no longer

00:07:43,360 --> 00:07:47,560
deleting items from 1 million feeds if

00:07:45,550 --> 00:07:49,960
they disappear the architecture is

00:07:47,560 --> 00:07:51,820
arguably a bit simpler and since all the

00:07:49,960 --> 00:07:53,890
work happens when the user comes to the

00:07:51,820 --> 00:07:55,930
platform it's actually quite a lot

00:07:53,890 --> 00:07:58,810
easier to implement any IB test that you

00:07:55,930 --> 00:08:00,790
can think of what's a bit more difficult

00:07:58,810 --> 00:08:03,490
with this approach is response times and

00:08:00,790 --> 00:08:05,920
there's only so much you can do in terms

00:08:03,490 --> 00:08:07,420
of ranking when the users already

00:08:05,920 --> 00:08:10,720
waiting for their feed so you've got a

00:08:07,420 --> 00:08:13,180
few limits here what did we go for for

00:08:10,720 --> 00:08:16,050
our first implementation we went for the

00:08:13,180 --> 00:08:18,460
on-demand version mainly because of the

00:08:16,050 --> 00:08:21,040
simpler architecture and we really liked

00:08:18,460 --> 00:08:22,600
the simpler test setup and we really

00:08:21,040 --> 00:08:24,880
didn't even want to start maintaining

00:08:22,600 --> 00:08:28,030
when maintaining a million users feeds

00:08:24,880 --> 00:08:29,740
and we always kept it in the back of our

00:08:28,030 --> 00:08:31,360
heads that we could do pre calculation

00:08:29,740 --> 00:08:34,030
of feeds if we needed to but we're not

00:08:31,360 --> 00:08:36,790
quite there yet so that's the general

00:08:34,030 --> 00:08:38,470
approach and now we'd like to tell you

00:08:36,790 --> 00:08:42,040
about we actually built the current

00:08:38,470 --> 00:08:43,810
version on our platform right so the

00:08:42,040 --> 00:08:46,540
first part of this problem is

00:08:43,810 --> 00:08:48,790
calculating the user features so I

00:08:46,540 --> 00:08:49,990
talked about earlier is we have these

00:08:48,790 --> 00:08:52,000
interactions of the users on the

00:08:49,990 --> 00:08:54,070
platform which are these item views and

00:08:52,000 --> 00:08:56,040
the user searches those are the main

00:08:54,070 --> 00:08:59,290
bits of information that we use to

00:08:56,040 --> 00:09:01,839
personalize this feed

00:08:59,290 --> 00:09:03,579
yeah we need a way to scale ibly store

00:09:01,839 --> 00:09:07,209
that information and later process that

00:09:03,579 --> 00:09:09,430
information and we decided to choose

00:09:07,209 --> 00:09:11,199
Kafka for storing these items views and

00:09:09,430 --> 00:09:13,480
these user searches because it's uh

00:09:11,199 --> 00:09:14,949
first of all we're already using Kafka

00:09:13,480 --> 00:09:17,860
inside our application to decouple

00:09:14,949 --> 00:09:21,370
certain subsystems using a event-driven

00:09:17,860 --> 00:09:23,170
architecture and that made it also the

00:09:21,370 --> 00:09:26,589
sort of the tool of choice it made sense

00:09:23,170 --> 00:09:29,440
to store this data also in Kafka it also

00:09:26,589 --> 00:09:32,829
scales really well and it decouples the

00:09:29,440 --> 00:09:36,029
ingestion from the processing so that's

00:09:32,829 --> 00:09:39,069
good so we have our initial data and

00:09:36,029 --> 00:09:40,690
secondly we thought it quite resembled a

00:09:39,069 --> 00:09:43,029
sort of or we thought it a streaming

00:09:40,690 --> 00:09:45,040
solution would makes sense because we

00:09:43,029 --> 00:09:47,769
have this continuous data flowing in and

00:09:45,040 --> 00:09:49,930
and the volume is quite high and the

00:09:47,769 --> 00:09:52,120
benefit of a streaming solution is that

00:09:49,930 --> 00:09:55,509
you can increment your model with every

00:09:52,120 --> 00:09:57,519
incoming event so rather than having to

00:09:55,509 --> 00:10:00,370
do these batch jobs and having the delay

00:09:57,519 --> 00:10:02,529
we can stay up-to-date and catch the

00:10:00,370 --> 00:10:05,079
user in the same session which is one of

00:10:02,529 --> 00:10:09,550
the earlier problem statements that we

00:10:05,079 --> 00:10:11,079
needed to solve so we have Kafka and we

00:10:09,550 --> 00:10:13,269
want a streaming solution so it kind of

00:10:11,079 --> 00:10:15,690
made sense to look at Kafka streams to

00:10:13,269 --> 00:10:19,930
see if that would suit our needs and

00:10:15,690 --> 00:10:23,139
what we like about Kafka streams is that

00:10:19,930 --> 00:10:24,699
you don't need a specialized cluster for

00:10:23,139 --> 00:10:27,610
that like you would with flink for the

00:10:24,699 --> 00:10:30,180
streaming part if you already have your

00:10:27,610 --> 00:10:32,230
data in these Kafka topics so it runs as

00:10:30,180 --> 00:10:34,149
included library inside your java

00:10:32,230 --> 00:10:37,209
application and you just consume these

00:10:34,149 --> 00:10:39,550
these data topics and then you process

00:10:37,209 --> 00:10:41,470
the data and you write the models back

00:10:39,550 --> 00:10:44,709
to separate Kafka topics which you can

00:10:41,470 --> 00:10:48,130
then later retrieve for yeah for anyone

00:10:44,709 --> 00:10:49,870
that requests them really and another

00:10:48,130 --> 00:10:52,050
nice thing about that is that it becomes

00:10:49,870 --> 00:10:54,819
really flexible you can really quickly

00:10:52,050 --> 00:10:56,620
create new Kafka streams applications

00:10:54,819 --> 00:10:58,600
that you can put side-by-side so what we

00:10:56,620 --> 00:11:00,639
have is for instance from the same input

00:10:58,600 --> 00:11:03,040
data we have favorite categories

00:11:00,639 --> 00:11:05,139
trending searches top location and we

00:11:03,040 --> 00:11:08,829
have a few things more that we sort of

00:11:05,139 --> 00:11:10,540
can just set next to our main our user

00:11:08,829 --> 00:11:12,220
features and then we have in parallel

00:11:10,540 --> 00:11:13,200
multiple Kafka streams applications

00:11:12,220 --> 00:11:15,540
which allow is also

00:11:13,200 --> 00:11:17,640
to quickly iterate on what we have and

00:11:15,540 --> 00:11:24,030
to a/b test new features that we come up

00:11:17,640 --> 00:11:26,880
with yeah that's pretty much why we like

00:11:24,030 --> 00:11:29,070
after screens yeah so now that we solve

00:11:26,880 --> 00:11:32,010
the the part with the user information

00:11:29,070 --> 00:11:34,080
we still need to solve our short-lived

00:11:32,010 --> 00:11:36,720
item problem and the searching and

00:11:34,080 --> 00:11:38,790
finding um relevant items for you so we

00:11:36,720 --> 00:11:41,010
need an item repository and this needs

00:11:38,790 --> 00:11:44,160
to do think two things it's it needs to

00:11:41,010 --> 00:11:45,540
be a very up-to-date version of the

00:11:44,160 --> 00:11:47,880
things you currently have on your

00:11:45,540 --> 00:11:50,100
platform and second it needs to support

00:11:47,880 --> 00:11:51,690
a range of retrieval functionality some

00:11:50,100 --> 00:11:53,310
ranking things like that so you can

00:11:51,690 --> 00:11:55,680
actually build a cool product on top of

00:11:53,310 --> 00:11:58,620
that and we didn't have to look very far

00:11:55,680 --> 00:12:00,750
we went for elastic search as our item

00:11:58,620 --> 00:12:02,850
repository first it was already a tool

00:12:00,750 --> 00:12:04,620
in our toolbox we use it for our core

00:12:02,850 --> 00:12:10,110
search and some other use cases at you a

00:12:04,620 --> 00:12:11,760
client saying and yeah it's as you I'm

00:12:10,110 --> 00:12:13,590
pretty sure although it's a scalable

00:12:11,760 --> 00:12:15,780
distributed search engine it has a range

00:12:13,590 --> 00:12:17,610
of ranking functionalities you can do a

00:12:15,780 --> 00:12:19,830
sorting by geographic distance out of

00:12:17,610 --> 00:12:21,690
the box you can do sorting by recency

00:12:19,830 --> 00:12:23,220
out of the box and if all that built and

00:12:21,690 --> 00:12:26,310
sorting isn't enough you can even build

00:12:23,220 --> 00:12:28,860
your own ranking functions so with that

00:12:26,310 --> 00:12:30,990
we were able to run a good range of very

00:12:28,860 --> 00:12:33,810
cool experiments like rerunning a user's

00:12:30,990 --> 00:12:36,210
past searches or finding trending items

00:12:33,810 --> 00:12:37,620
in your area or drawing items from your

00:12:36,210 --> 00:12:39,750
favorite categories or categories

00:12:37,620 --> 00:12:41,250
related to that or even using elastics

00:12:39,750 --> 00:12:45,090
more like this feature to come up with

00:12:41,250 --> 00:12:47,070
items that user last viewed so in the

00:12:45,090 --> 00:12:49,290
end we built queries based on the user

00:12:47,070 --> 00:12:51,270
information and we pull results from

00:12:49,290 --> 00:12:52,650
elastic search then we mix all them

00:12:51,270 --> 00:12:55,020
together and wave them somehow and

00:12:52,650 --> 00:13:00,360
that's how the final personalized feed

00:12:55,020 --> 00:13:02,420
gets built and presented to the user so

00:13:00,360 --> 00:13:05,040
when you stick those two parts together

00:13:02,420 --> 00:13:09,960
what do you get you get actually a

00:13:05,040 --> 00:13:12,540
system that is it showing ya quite

00:13:09,960 --> 00:13:15,420
simple architecture so what you see on

00:13:12,540 --> 00:13:17,220
the right side is the ingestion flow

00:13:15,420 --> 00:13:19,350
that I talked about so the searches and

00:13:17,220 --> 00:13:21,420
the item views they go into kafka topics

00:13:19,350 --> 00:13:24,180
and then we have a user feature service

00:13:21,420 --> 00:13:26,700
on top of that which embeds the kafka

00:13:24,180 --> 00:13:29,130
streams library and it consumes from

00:13:26,700 --> 00:13:31,320
these input topics does the processing

00:13:29,130 --> 00:13:34,710
and then writes the output back to Kafka

00:13:31,320 --> 00:13:36,630
topics and on the left hand side we then

00:13:34,710 --> 00:13:38,400
have the feed request flow so you open

00:13:36,630 --> 00:13:40,470
the app and the request goes to our feed

00:13:38,400 --> 00:13:42,780
service the feed service that retrieves

00:13:40,470 --> 00:13:45,300
the finalized models and uses those

00:13:42,780 --> 00:13:47,540
models to create multiples elasticsearch

00:13:45,300 --> 00:13:49,770
requests and they get fired off to the

00:13:47,540 --> 00:13:53,180
elasticsearch repository which has a

00:13:49,770 --> 00:13:56,070
view on our 28 million live items and

00:13:53,180 --> 00:13:58,440
the response the weighted response gets

00:13:56,070 --> 00:14:01,560
mixed together and presented back to the

00:13:58,440 --> 00:14:03,750
user and yeah we're really happy with

00:14:01,560 --> 00:14:05,880
this set up because each individual part

00:14:03,750 --> 00:14:07,710
can be scaled so depending on whether

00:14:05,880 --> 00:14:10,220
data ingestion needs to be skilled or

00:14:07,710 --> 00:14:13,560
the processing or on the other side

00:14:10,220 --> 00:14:15,840
actually serving the request we have a

00:14:13,560 --> 00:14:21,120
quite a flexible architecture that can

00:14:15,840 --> 00:14:23,190
accommodate for our growth so final

00:14:21,120 --> 00:14:26,430
thoughts we built this thing it's

00:14:23,190 --> 00:14:29,160
serving several thousands feeds every

00:14:26,430 --> 00:14:32,670
second it we are confident it will

00:14:29,160 --> 00:14:34,800
continue to scale in the future if we

00:14:32,670 --> 00:14:36,810
were to put a finger on one point of

00:14:34,800 --> 00:14:38,280
concern and the whole system it would be

00:14:36,810 --> 00:14:40,080
our elastic cluster because we're

00:14:38,280 --> 00:14:42,330
growing in two dimensions here it's like

00:14:40,080 --> 00:14:45,030
very scalable it's inherently scalable

00:14:42,330 --> 00:14:46,680
but we grow both in the number of feed

00:14:45,030 --> 00:14:48,630
requests that we have on the platform

00:14:46,680 --> 00:14:50,700
and we grow in the number of queries

00:14:48,630 --> 00:14:52,680
that we fire for each feed request that

00:14:50,700 --> 00:14:54,450
we get so that's something to keep an

00:14:52,680 --> 00:14:56,280
eye on in the future and requires quite

00:14:54,450 --> 00:14:58,530
a big cluster already

00:14:56,280 --> 00:15:00,270
however as Dominic said we were really

00:14:58,530 --> 00:15:03,090
happy with the set up it solves our

00:15:00,270 --> 00:15:04,620
product needs for now we have the

00:15:03,090 --> 00:15:08,030
flexibility we need to run fast

00:15:04,620 --> 00:15:11,930
experiments and do very cool things

00:15:08,030 --> 00:15:14,580
there is a tech blog article in the eBay

00:15:11,930 --> 00:15:16,830
Berlin tech blogs also on the buzz words

00:15:14,580 --> 00:15:18,390
website and with some links about how

00:15:16,830 --> 00:15:20,730
other people have approached it and the

00:15:18,390 --> 00:15:24,420
contents of the talk and if there's

00:15:20,730 --> 00:15:26,610
anything that we can't answer before the

00:15:24,420 --> 00:15:28,830
well-deserved lunch break please come

00:15:26,610 --> 00:15:30,990
approach us at the eBay take booth and

00:15:28,830 --> 00:15:32,610
also if you want to talk about the jobs

00:15:30,990 --> 00:15:35,839
that we have in the eBay tech universe

00:15:32,610 --> 00:15:35,839
so thanks for listening

00:15:36,190 --> 00:15:39,269
[Music]

00:15:40,300 --> 00:15:50,620
thank you we still have five minutes for

00:15:43,490 --> 00:15:55,070
questions any okay so navigates um how

00:15:50,620 --> 00:16:00,320
would you show for new users so it's a

00:15:55,070 --> 00:16:03,590
cold start problem yeah so we are lucky

00:16:00,320 --> 00:16:05,900
that we have many users on our apps and

00:16:03,590 --> 00:16:09,410
we have very high login rates

00:16:05,900 --> 00:16:10,610
so usually we were quite confident and

00:16:09,410 --> 00:16:12,650
the user that we see is actually our

00:16:10,610 --> 00:16:15,650
user and the Cosco streams thing we

00:16:12,650 --> 00:16:17,030
monitored the percentage of feeds where

00:16:15,650 --> 00:16:19,550
we don't have any personalized results

00:16:17,030 --> 00:16:21,290
for and it was actually not point

00:16:19,550 --> 00:16:22,940
something percent so something we didn't

00:16:21,290 --> 00:16:24,290
tackle at that moment so usually as soon

00:16:22,940 --> 00:16:25,880
as you do your first interaction you

00:16:24,290 --> 00:16:27,110
will have some sort of personalization

00:16:25,880 --> 00:16:29,510
in your feed and that was good enough

00:16:27,110 --> 00:16:31,550
for us yeah and before that it's just

00:16:29,510 --> 00:16:33,740
randomized so in the moment you click

00:16:31,550 --> 00:16:36,620
any of those items we are able to in

00:16:33,740 --> 00:16:40,360
near-real-time already incorporate that

00:16:36,620 --> 00:16:40,360
into the feet the next time you visit it

00:16:41,770 --> 00:16:49,520
when you introduce personalized feed

00:16:46,340 --> 00:16:52,970
what do you solve for and how do you

00:16:49,520 --> 00:16:57,230
measure it that it increased some metric

00:16:52,970 --> 00:17:00,560
or and what would this matrix B so what

00:16:57,230 --> 00:17:03,380
we essentially solve for is is well the

00:17:00,560 --> 00:17:05,000
people that would not otherwise be

00:17:03,380 --> 00:17:06,680
spending more time on our platform so we

00:17:05,000 --> 00:17:08,750
can easily measure that because we then

00:17:06,680 --> 00:17:11,180
expect that if we do a good job

00:17:08,750 --> 00:17:13,340
those people spend more time both on the

00:17:11,180 --> 00:17:15,620
feed but also on individual items and

00:17:13,340 --> 00:17:18,200
and we just track basically the time

00:17:15,620 --> 00:17:22,040
spent and and the items viewed on our on

00:17:18,200 --> 00:17:24,920
our website and with longer sessions

00:17:22,040 --> 00:17:27,380
more of views also it's a combination of

00:17:24,920 --> 00:17:28,640
those things but indeed yeah so that's

00:17:27,380 --> 00:17:32,990
what we track and that's what we sort of

00:17:28,640 --> 00:17:36,110
optimize for and in your architecture

00:17:32,990 --> 00:17:39,920
diagram you saw we saw that you carrying

00:17:36,110 --> 00:17:46,340
some kind of state of dead streams I

00:17:39,920 --> 00:17:48,320
assume this Kafka tables yeah how big is

00:17:46,340 --> 00:17:50,420
that state you are having there how big

00:17:48,320 --> 00:17:54,920
is the state that's a good

00:17:50,420 --> 00:17:58,340
question so we have first of all we have

00:17:54,920 --> 00:18:00,710
a replication factor of three three so

00:17:58,340 --> 00:18:03,770
for redundancy purposes of got effector

00:18:00,710 --> 00:18:05,990
that in and we have 50 partitions forty

00:18:03,770 --> 00:18:08,420
megabytes partition with 40 megabytes

00:18:05,990 --> 00:18:15,530
for partition so it's 40 times 50 times

00:18:08,420 --> 00:18:18,140
3 something like that yeah so it's it's

00:18:15,530 --> 00:18:20,540
it's a 2 part so the source of truth is

00:18:18,140 --> 00:18:23,330
the Kafka topic underneath so that's a

00:18:20,540 --> 00:18:25,670
compacted Kafka topic which is yeah like

00:18:23,330 --> 00:18:28,760
key value like a key value table but in

00:18:25,670 --> 00:18:32,510
the append-only law kind of that stores

00:18:28,760 --> 00:18:35,750
the user ID with the the model but in

00:18:32,510 --> 00:18:38,390
order to serve it we use Kafka streams

00:18:35,750 --> 00:18:39,950
this sort of query API which also is

00:18:38,390 --> 00:18:42,230
necessary so Kafka streams under the

00:18:39,950 --> 00:18:44,360
hood it uses rocks DB to make sure that

00:18:42,230 --> 00:18:46,040
I can do random access lookups because

00:18:44,360 --> 00:18:48,230
it needs that also for the stream

00:18:46,040 --> 00:18:51,320
processing and then there's a thin layer

00:18:48,230 --> 00:18:52,700
on top of that that we reuse to serve

00:18:51,320 --> 00:18:54,290
these models it's been working well for

00:18:52,700 --> 00:18:55,970
us some people have said like maybe

00:18:54,290 --> 00:18:57,590
that's not to be used for querying but

00:18:55,970 --> 00:18:59,570
so far we've been using an in production

00:18:57,590 --> 00:19:00,890
and yeah it's working quite well but if

00:18:59,570 --> 00:19:03,890
you would want to serve it in another

00:19:00,890 --> 00:19:06,110
way you can simply read through this

00:19:03,890 --> 00:19:08,480
compacted topic which has the models and

00:19:06,110 --> 00:19:10,100
you can stick it in any kind of database

00:19:08,480 --> 00:19:13,370
that you would like if you would require

00:19:10,100 --> 00:19:15,140
some other yeah some other properties

00:19:13,370 --> 00:19:16,850
which is also very nice it's a flexible

00:19:15,140 --> 00:19:21,170
set up we could always move to some

00:19:16,850 --> 00:19:26,870
other way of querying these models we

00:19:21,170 --> 00:19:29,900
have one more question those user feeds

00:19:26,870 --> 00:19:32,270
are they completely personalized by your

00:19:29,900 --> 00:19:35,630
algorithm or also can the user somehow

00:19:32,270 --> 00:19:38,240
guide it by marking some categories or

00:19:35,630 --> 00:19:40,100
something like this not yet product wise

00:19:38,240 --> 00:19:42,950
we wanted to keep it as a black box so

00:19:40,100 --> 00:19:45,320
inspiring and very surprising do you

00:19:42,950 --> 00:19:46,700
have do you have any measures on how

00:19:45,320 --> 00:19:50,360
effective it is in terms of like

00:19:46,700 --> 00:19:53,030
click-through like improvements in

00:19:50,360 --> 00:19:56,240
engagement let's say given the

00:19:53,030 --> 00:19:58,100
improvements to the feeder yeah we do we

00:19:56,240 --> 00:19:59,750
always see significant about not always

00:19:58,100 --> 00:20:01,910
but for most of the experiments that we

00:19:59,750 --> 00:20:03,030
do we see some significant increases and

00:20:01,910 --> 00:20:05,940
view items

00:20:03,030 --> 00:20:09,570
and events and engagement and session

00:20:05,940 --> 00:20:11,610
length yeah okay thank you all right

00:20:09,570 --> 00:20:19,880
thank you

00:20:11,610 --> 00:20:19,880

YouTube URL: https://www.youtube.com/watch?v=5Vm1d-oxZeU


