Title: Berlin Buzzwords 2018: Holden Karau – Working with Tensorflow from the JVM #bbuzz
Publication date: 2018-06-18
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Holden Karau talking about "Working with Tensorflow from the JVM: How Big Data and Deep Learning can be BFFs".

Tensorflow is all kind of fancy, from helping startups raising their Series A in Silicon Valley to detecting if something is a cat. However, when things start to get "real" you may find yourself no longer dealing with mnist.csv, and instead needing do large scale data prep as well as training. 

This talk will explore how Tensorflow can be used in conjunction with Apache Spark, Flink, and BEAM to create a full machine learning pipeline including that annoying "feature engineering" and "data prep" components that we like to pretend don’t exist. We’ll also talk about how these feature prep stages need to be integrated into the serving layer.

This talk will also explore how Apache Arrow impacts cross-language development for big-data including things like deep learning. Even if you’re not trying to raise a round of funding in Silicon Valley, this talk will give you tools to do interesting machine learning problems at scale (or find more cats).

Read more:
https://2018.berlinbuzzwords.de/18/session/working-tensorflow-jvm-how-big-data-and-deep-learning-can-be-bffs

About Holden Karau:
https://2018.berlinbuzzwords.de/users/holden-karau

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:05,290 --> 00:00:08,350
thank you

00:00:09,420 --> 00:00:17,520
I am also joined by my co-presenter boo

00:00:13,880 --> 00:00:19,230
she travels with me everywhere so yeah

00:00:17,520 --> 00:00:21,000
my name is Holton my preferred pronouns

00:00:19,230 --> 00:00:23,580
are she or her it's tattooed on my wrist

00:00:21,000 --> 00:00:26,250
in case you or I forget mornings can be

00:00:23,580 --> 00:00:28,890
a little rough sometimes I'm a developer

00:00:26,250 --> 00:00:30,420
advocate at Google it's nice they pay me

00:00:28,890 --> 00:00:31,050
money and they work on open-source

00:00:30,420 --> 00:00:34,260
software

00:00:31,050 --> 00:00:36,390
it keeps me happy and I'm on the spork

00:00:34,260 --> 00:00:38,879
PMC and I contribute to beam and a lot

00:00:36,390 --> 00:00:42,960
of other Apache projects as well

00:00:38,879 --> 00:00:45,540
previously I was at IBM Alpine dataworks

00:00:42,960 --> 00:00:48,449
Google Foursquare on Amazon I'm a

00:00:45,540 --> 00:00:51,680
co-author of two books the second one I

00:00:48,449 --> 00:00:54,150
realize that you can negotiate royalties

00:00:51,680 --> 00:00:55,890
with publishers so if you're looking to

00:00:54,150 --> 00:00:57,989
buy a book about SPARC and you don't

00:00:55,890 --> 00:01:01,409
really care what's inside definitely buy

00:00:57,989 --> 00:01:03,629
the second one you can also follow me on

00:01:01,409 --> 00:01:06,540
Twitter and if you happen to be curious

00:01:03,629 --> 00:01:07,860
about like how ASF code reviews go in

00:01:06,540 --> 00:01:10,890
really large projects

00:01:07,860 --> 00:01:14,969
I'm live-streaming some code reviews as

00:01:10,890 --> 00:01:16,920
well I don't know I think it's fun and

00:01:14,969 --> 00:01:18,960
then I actually set aside time to do

00:01:16,920 --> 00:01:22,320
code reviews which is always a challenge

00:01:18,960 --> 00:01:24,509
when you're busy in addition to I am

00:01:22,320 --> 00:01:26,429
professionally I'm trans queer Canadian

00:01:24,509 --> 00:01:28,979
and part of the leather community this

00:01:26,429 --> 00:01:31,140
isn't super important or directly

00:01:28,979 --> 00:01:32,399
related to deep learning but I think

00:01:31,140 --> 00:01:33,420
especially for those of us who are going

00:01:32,399 --> 00:01:35,640
to be working on machine learning

00:01:33,420 --> 00:01:37,950
related problems it's important to

00:01:35,640 --> 00:01:39,689
remember that we all come from a variety

00:01:37,950 --> 00:01:40,890
of backgrounds and we should work

00:01:39,689 --> 00:01:42,899
together to make sure that we build

00:01:40,890 --> 00:01:44,609
systems that work for everyone and we

00:01:42,899 --> 00:01:47,130
don't just build crap that reinforces

00:01:44,609 --> 00:01:49,560
our existing crappy systems and we can

00:01:47,130 --> 00:01:52,200
make more awesome things that being said

00:01:49,560 --> 00:01:56,399
I give you know techniques to do that so

00:01:52,200 --> 00:01:59,069
good luck ok yep this is boo she also

00:01:56,399 --> 00:02:00,420
uses she/her pronouns and she is the

00:01:59,069 --> 00:02:02,189
author of learning to bark and

00:02:00,420 --> 00:02:04,679
high-performance barking you can also

00:02:02,189 --> 00:02:06,899
follow her on Twitter she does not

00:02:04,679 --> 00:02:11,069
currently have a twitch but maybe maybe

00:02:06,899 --> 00:02:12,960
one day soon and why does my employer

00:02:11,069 --> 00:02:15,240
care and this is in response to

00:02:12,960 --> 00:02:16,740
something that I was asked this morning

00:02:15,240 --> 00:02:18,810
by someone who was like so I get that

00:02:16,740 --> 00:02:20,340
you work on open source but why does

00:02:18,810 --> 00:02:22,049
Google care about these systems don't

00:02:20,340 --> 00:02:22,989
they have like awesome other internal

00:02:22,049 --> 00:02:24,670
systems

00:02:22,989 --> 00:02:27,099
and yet we have a lot of really awesome

00:02:24,670 --> 00:02:28,870
internal code but we also think it's

00:02:27,099 --> 00:02:30,010
really important to support all of the

00:02:28,870 --> 00:02:31,390
wonderful tools in the Big Data

00:02:30,010 --> 00:02:33,280
ecosystem

00:02:31,390 --> 00:02:35,349
things like Apache beam and Apache spark

00:02:33,280 --> 00:02:37,840
we have on Google cloud with hosted

00:02:35,349 --> 00:02:40,329
solutions and even if it's not a thing

00:02:37,840 --> 00:02:42,519
where we have this hosted solution it's

00:02:40,329 --> 00:02:44,739
fun because you can just run it on

00:02:42,519 --> 00:02:46,810
Google cloud and then we make money or

00:02:44,739 --> 00:02:48,340
at least that's something along the

00:02:46,810 --> 00:02:50,319
lines of what I vaguely understood from

00:02:48,340 --> 00:02:52,299
our business plan for like five minutes

00:02:50,319 --> 00:02:54,430
of it that I like was paying attention

00:02:52,299 --> 00:02:57,190
and then I walked out because it was

00:02:54,430 --> 00:02:59,440
getting kind of confusing and so that's

00:02:57,190 --> 00:03:01,180
not official but just TLDR somehow this

00:02:59,440 --> 00:03:01,930
results in money for my employer don't

00:03:01,180 --> 00:03:04,720
worry about me

00:03:01,930 --> 00:03:07,000
I will continue to get paid okay so

00:03:04,720 --> 00:03:09,849
hopefully you're nice people I am really

00:03:07,000 --> 00:03:13,780
curious how many people here are spark

00:03:09,849 --> 00:03:16,150
users okay how many people are flink

00:03:13,780 --> 00:03:18,010
users because I'm in Berlin that is less

00:03:16,150 --> 00:03:19,510
than I was expecting maybe there is

00:03:18,010 --> 00:03:23,220
another flink talk happening at the same

00:03:19,510 --> 00:03:27,010
time okay how many people are beam users

00:03:23,220 --> 00:03:30,040
that is about what I expected that that

00:03:27,010 --> 00:03:30,459
was three but it's cool bhima's beam is

00:03:30,040 --> 00:03:33,010
awesome

00:03:30,459 --> 00:03:34,810
and it's gonna get more awesome so I'm

00:03:33,010 --> 00:03:37,419
gonna talk about big data outside of the

00:03:34,810 --> 00:03:38,709
JVM in general because tensorflow is

00:03:37,419 --> 00:03:41,919
sort of we can just think of it as a

00:03:38,709 --> 00:03:43,870
special use case of trying to do big

00:03:41,919 --> 00:03:45,760
data outside of the JVM and then we'll

00:03:43,870 --> 00:03:47,590
look at tensorflow on spark and then

00:03:45,760 --> 00:03:50,349
we'll also look at tensorflow on Beam

00:03:47,590 --> 00:03:53,079
and we'll talk about how Apache Aero

00:03:50,349 --> 00:03:56,590
can totally change these things to no

00:03:53,079 --> 00:04:00,129
longer resemble a did you have the Ford

00:03:56,590 --> 00:04:01,870
Pinto here is that no okay it's an

00:04:00,129 --> 00:04:05,290
American car that was notable for

00:04:01,870 --> 00:04:07,720
catching on fire and so will make this

00:04:05,290 --> 00:04:09,790
better than our v1 product which

00:04:07,720 --> 00:04:12,699
currently catches on fire okay

00:04:09,790 --> 00:04:16,239
so PI spark actually how many people are

00:04:12,699 --> 00:04:18,699
PI spark users in the house not that

00:04:16,239 --> 00:04:21,760
many so I apologize to all of you PI

00:04:18,699 --> 00:04:23,440
spark users it's getting better but PI

00:04:21,760 --> 00:04:25,630
so spark is the Python interface to

00:04:23,440 --> 00:04:27,880
spark it's the general technique used

00:04:25,630 --> 00:04:30,699
for a lot of other language on top of

00:04:27,880 --> 00:04:33,150
spark and it's also the same thing that

00:04:30,699 --> 00:04:35,590
is used to power tensor flow on spark we

00:04:33,150 --> 00:04:36,520
pretty much looked at the possibility of

00:04:35,590 --> 00:04:39,099
writing

00:04:36,520 --> 00:04:40,479
our own bindings and one that's hard oh

00:04:39,099 --> 00:04:45,490
but we already know how to talk to

00:04:40,479 --> 00:04:47,860
Python yay and the performance is bad

00:04:45,490 --> 00:04:49,300
but that's that's okay so why is the

00:04:47,860 --> 00:04:52,389
performance bad for all of these things

00:04:49,300 --> 00:04:54,280
we use this thing called pickling which

00:04:52,389 --> 00:04:57,460
is about as performant as that pickle

00:04:54,280 --> 00:04:58,840
which is not very fast sockets to

00:04:57,460 --> 00:05:00,460
communicate data and then because that

00:04:58,840 --> 00:05:03,099
wasn't enough we decided we'd also use

00:05:00,460 --> 00:05:04,180
UNIX pipes and then over top of that we

00:05:03,099 --> 00:05:06,340
were like you know what this really

00:05:04,180 --> 00:05:11,169
needs is another format for interchange

00:05:06,340 --> 00:05:14,199
so we threw JSON on top of that yeah

00:05:11,169 --> 00:05:17,500
that didn't go so well and it looks kind

00:05:14,199 --> 00:05:19,389
of like this we essentially end up

00:05:17,500 --> 00:05:21,039
copying or lambda expressions from

00:05:19,389 --> 00:05:22,840
Python into the JVM and then on the

00:05:21,039 --> 00:05:25,360
workers we end up receiving these lambda

00:05:22,840 --> 00:05:27,580
expressions and our data and we take the

00:05:25,360 --> 00:05:30,430
both the data and the lambda expressions

00:05:27,580 --> 00:05:32,740
and send them through to Python and even

00:05:30,430 --> 00:05:34,090
if you're gonna work in Scala and you're

00:05:32,740 --> 00:05:35,860
gonna use tensorflow

00:05:34,090 --> 00:05:38,500
oh they told me not to move yeah

00:05:35,860 --> 00:05:40,960
whatever you're gonna miss like this

00:05:38,500 --> 00:05:42,310
part's gonna go away so PI 4j will not

00:05:40,960 --> 00:05:44,229
be in your life and you'll be happy

00:05:42,310 --> 00:05:45,610
about that but this part is still gonna

00:05:44,229 --> 00:05:48,280
be in your life and I'm really sorry

00:05:45,610 --> 00:05:51,340
camera person I just don't have a

00:05:48,280 --> 00:05:54,340
pointer thing uh yeah and in flink it's

00:05:51,340 --> 00:05:58,029
pretty much the same thing except with

00:05:54,340 --> 00:06:01,449
slightly different formats so how does

00:05:58,029 --> 00:06:04,000
this impact big data systems besides

00:06:01,449 --> 00:06:05,620
SPARC it makes double serialization

00:06:04,000 --> 00:06:08,529
costs make all of this very expensive

00:06:05,620 --> 00:06:10,240
and slow and that's fine I'm a cloud

00:06:08,529 --> 00:06:14,139
provider I sell you resources by the

00:06:10,240 --> 00:06:15,430
hour if it takes twice as long to run I

00:06:14,139 --> 00:06:18,130
make twice as much money

00:06:15,430 --> 00:06:20,259
I think I'm not super sure on how our

00:06:18,130 --> 00:06:23,860
business model works but that seems

00:06:20,259 --> 00:06:25,210
probable the only downside is it turns

00:06:23,860 --> 00:06:26,830
out that you might go like wow this is

00:06:25,210 --> 00:06:28,389
really slow I'm just not gonna do this

00:06:26,830 --> 00:06:31,090
and then that's sad because that's money

00:06:28,389 --> 00:06:32,319
that I'm not making and it might also be

00:06:31,090 --> 00:06:34,479
said for you because it's a problem that

00:06:32,319 --> 00:06:35,710
you're not solving there's a bunch of

00:06:34,479 --> 00:06:37,270
other things that are bad the error

00:06:35,710 --> 00:06:38,620
messages make no sense but if you're

00:06:37,270 --> 00:06:42,009
working with tensorflow you're already

00:06:38,620 --> 00:06:43,930
experiencing that and the dependency

00:06:42,009 --> 00:06:46,120
management makes limited amounts of

00:06:43,930 --> 00:06:48,159
sense and this is amazing because we

00:06:46,120 --> 00:06:49,690
take a system that is designed to

00:06:48,159 --> 00:06:50,139
distribute Java packages and then we

00:06:49,690 --> 00:06:53,199
make it

00:06:50,139 --> 00:06:54,849
tribute native code and then that goes

00:06:53,199 --> 00:06:58,599
about as well as that sounds which is

00:06:54,849 --> 00:07:00,340
poorly and so I want to be clear a lot

00:06:58,599 --> 00:07:02,650
of times when I talk about how PI spark

00:07:00,340 --> 00:07:03,879
and flink work people are like wow those

00:07:02,650 --> 00:07:05,710
both sound terrible

00:07:03,879 --> 00:07:09,569
what should I use instead and I'm here

00:07:05,710 --> 00:07:09,569
to tell you everything else sucks too

00:07:09,749 --> 00:07:13,360
some of the systems even looked at that

00:07:12,009 --> 00:07:20,340
and when you know what this needs

00:07:13,360 --> 00:07:23,409
XML it did not need XML but that's fine

00:07:20,340 --> 00:07:25,360
so don't worry everything sucks and this

00:07:23,409 --> 00:07:27,639
same general approach will apply to the

00:07:25,360 --> 00:07:30,400
other systems even the ones that use XML

00:07:27,639 --> 00:07:33,129
so ok tensorflow one spark let's let's

00:07:30,400 --> 00:07:35,680
do the hello world part and we're gonna

00:07:33,129 --> 00:07:37,300
train em NIST yay and so there's there's

00:07:35,680 --> 00:07:38,650
a tensor flow on spark package which we

00:07:37,300 --> 00:07:41,169
can just use out of the box we have to

00:07:38,650 --> 00:07:43,569
use it in Python though but under the

00:07:41,169 --> 00:07:47,830
hood it's Python calling Java calling

00:07:43,569 --> 00:07:50,680
wife calling Java again calling Python

00:07:47,830 --> 00:07:52,689
calling C++ code and so we get this like

00:07:50,680 --> 00:07:54,819
sort of turtles in the middle situation

00:07:52,689 --> 00:07:56,979
it's not quite all the way down at the

00:07:54,819 --> 00:07:59,289
bottom you always find C++ or Fortran

00:07:56,979 --> 00:08:02,620
there to give you a helping hand and a

00:07:59,289 --> 00:08:06,580
segfault and so this this works

00:08:02,620 --> 00:08:07,750
surprisingly but you might not be very

00:08:06,580 --> 00:08:10,539
excited about this because you're

00:08:07,750 --> 00:08:12,279
probably Java users so one of the things

00:08:10,539 --> 00:08:14,770
that we should do in addition to

00:08:12,279 --> 00:08:17,979
exposing this from Java is make it not

00:08:14,770 --> 00:08:19,360
bad the performance of this is really

00:08:17,979 --> 00:08:21,250
terrible

00:08:19,360 --> 00:08:22,810
and there's some cool things which have

00:08:21,250 --> 00:08:25,229
happened which now make it possible for

00:08:22,810 --> 00:08:28,000
us to make this performance really cool

00:08:25,229 --> 00:08:31,149
and this cat is very happy about this

00:08:28,000 --> 00:08:33,310
new performance paradigm and Apache

00:08:31,149 --> 00:08:35,909
arrow will allow you to transfer data as

00:08:33,310 --> 00:08:40,120
fast as this cat is switching universes

00:08:35,909 --> 00:08:44,110
not a guarantee and the nice thing is it

00:08:40,120 --> 00:08:46,329
supports SPARC and GPUs and R and Python

00:08:44,110 --> 00:08:48,579
it supports arbitrary Java libraries so

00:08:46,329 --> 00:08:50,110
if you're not a spark user you could

00:08:48,579 --> 00:08:51,579
totally add support for this to

00:08:50,110 --> 00:08:54,190
whichever a Java library that you're

00:08:51,579 --> 00:08:57,370
working in and actually deal 4j has

00:08:54,190 --> 00:08:58,510
support for arrow for example but you

00:08:57,370 --> 00:09:00,390
know it's going to take a little bit of

00:08:58,510 --> 00:09:02,709
work if you want to say add it to flank

00:09:00,390 --> 00:09:04,060
and so there's this really nice

00:09:02,709 --> 00:09:06,160
performance graph from someone

00:09:04,060 --> 00:09:09,940
which implies this will be 242 times

00:09:06,160 --> 00:09:14,500
faster that's a lie but it will be

00:09:09,940 --> 00:09:16,150
faster probably and and at least for for

00:09:14,500 --> 00:09:17,380
our purposes it will be we're probably

00:09:16,150 --> 00:09:19,090
not even gonna get this three times

00:09:17,380 --> 00:09:22,630
faster but it does it does make the

00:09:19,090 --> 00:09:23,650
stuff go a little better and so why why

00:09:22,630 --> 00:09:25,330
we're talking about this so we're

00:09:23,650 --> 00:09:26,650
talking about this because we're in Java

00:09:25,330 --> 00:09:28,870
and we want to get our data into

00:09:26,650 --> 00:09:31,450
tensorflow and we want to do that in a

00:09:28,870 --> 00:09:33,010
way which isn't incredibly slow and

00:09:31,450 --> 00:09:34,720
Apache arrow is pretty much the only

00:09:33,010 --> 00:09:37,000
option right now for that we could also

00:09:34,720 --> 00:09:40,590
write files out to disks and TF record

00:09:37,000 --> 00:09:43,720
and that that's slow disks are not fast

00:09:40,590 --> 00:09:45,910
even SSDs and so instead we can use

00:09:43,720 --> 00:09:48,340
arrow as this fast interchange library

00:09:45,910 --> 00:09:49,930
and in the future maybe we could go

00:09:48,340 --> 00:09:53,440
directly into tensorflow rather than

00:09:49,930 --> 00:09:56,500
going Java arrow Python tensorflow but

00:09:53,440 --> 00:09:58,420
that would be work and other projects

00:09:56,500 --> 00:09:59,770
are using a to write all of our friends

00:09:58,420 --> 00:10:03,400
are jumping off of the cliff so you

00:09:59,770 --> 00:10:06,760
should join us in this party that logic

00:10:03,400 --> 00:10:07,930
works and so not just spark and so if

00:10:06,760 --> 00:10:10,630
you actually want to integrate other

00:10:07,930 --> 00:10:11,920
things besides tensorflow you should

00:10:10,630 --> 00:10:15,970
definitely check that arrow and see if

00:10:11,920 --> 00:10:18,580
it fits your project and so to rewrite

00:10:15,970 --> 00:10:20,110
our code in spark to use this we

00:10:18,580 --> 00:10:22,690
essentially take our register function

00:10:20,110 --> 00:10:24,160
and we just call pandas UDF instead and

00:10:22,690 --> 00:10:25,720
we can see here we're doing very

00:10:24,160 --> 00:10:27,820
advanced numeric computation we're

00:10:25,720 --> 00:10:29,710
adding two integers together which

00:10:27,820 --> 00:10:31,950
honestly Java a little touch-and-go

00:10:29,710 --> 00:10:33,970
write a numeric computation in Java I

00:10:31,950 --> 00:10:35,290
don't know and so with the fact that we

00:10:33,970 --> 00:10:36,880
can add two integers together is a

00:10:35,290 --> 00:10:39,250
really good sign of how we're gonna be

00:10:36,880 --> 00:10:41,500
able to power tensor flow and strangely

00:10:39,250 --> 00:10:43,690
enough that's what we're gonna do and so

00:10:41,500 --> 00:10:46,270
we can we can do this to TF on spark the

00:10:43,690 --> 00:10:48,010
the core of TF on spark is this thing

00:10:46,270 --> 00:10:50,740
which takes for each partition and

00:10:48,010 --> 00:10:52,870
starts tensorflow on each of the nodes

00:10:50,740 --> 00:10:55,720
and feeds it the data that it needs from

00:10:52,870 --> 00:10:59,020
your big data system and that's that's

00:10:55,720 --> 00:11:01,900
cool and that's kind of fast it's a

00:10:59,020 --> 00:11:05,080
little unreliable and we'll talk about

00:11:01,900 --> 00:11:07,090
this and so we can make this train

00:11:05,080 --> 00:11:09,550
function and we can rewrite it into this

00:11:07,090 --> 00:11:11,020
UDF which returns zeros because for we

00:11:09,550 --> 00:11:14,800
forgot to add support for returning

00:11:11,020 --> 00:11:16,630
nulls but that's okay this this sad hack

00:11:14,800 --> 00:11:17,980
for now is probably the one part of the

00:11:16,630 --> 00:11:21,880
code which will survive

00:11:17,980 --> 00:11:25,810
stun all of my experience and this just

00:11:21,880 --> 00:11:28,600
ends up taking our input data as a

00:11:25,810 --> 00:11:30,460
narrow thing which is converted into

00:11:28,600 --> 00:11:32,260
pandas behind the scene and then we

00:11:30,460 --> 00:11:33,340
convert it into tuples and then we feed

00:11:32,260 --> 00:11:34,660
it to tensorflow

00:11:33,340 --> 00:11:37,600
and if that sounds like a lot of

00:11:34,660 --> 00:11:42,100
conversion it is but it it's a bit

00:11:37,600 --> 00:11:44,290
faster and so this this design looks

00:11:42,100 --> 00:11:45,820
like this and so you can just go ahead

00:11:44,290 --> 00:11:48,640
and make this change in your own like

00:11:45,820 --> 00:11:52,720
private Fork and then your magic em

00:11:48,640 --> 00:11:54,910
mystical all the way back here will

00:11:52,720 --> 00:11:58,360
suddenly get faster it will also get

00:11:54,910 --> 00:11:59,740
slightly less reliable but that's a

00:11:58,360 --> 00:12:04,210
trade-off everyone's willing to make

00:11:59,740 --> 00:12:07,180
right right okay so the TLDR is the

00:12:04,210 --> 00:12:09,010
SPARC scheduler has some issues when it

00:12:07,180 --> 00:12:11,050
comes to scheduling deep learning jobs

00:12:09,010 --> 00:12:14,290
in that we assume that we can just

00:12:11,050 --> 00:12:17,500
restart any individual partition oh and

00:12:14,290 --> 00:12:21,790
now the mood lighting takes effect as we

00:12:17,500 --> 00:12:24,160
move into okay right and so we swap

00:12:21,790 --> 00:12:25,720
pickles for arrow batch records and now

00:12:24,160 --> 00:12:28,360
we have a little panda which is much

00:12:25,720 --> 00:12:29,910
cuter than that circle and that panda

00:12:28,360 --> 00:12:34,030
can go into tensorflow and that's fun

00:12:29,910 --> 00:12:36,790
yay happy pandas okay so what could we

00:12:34,030 --> 00:12:39,070
do how can we make this like actually

00:12:36,790 --> 00:12:40,510
awesome so the first thing that we could

00:12:39,070 --> 00:12:42,220
do is we could go back look at this

00:12:40,510 --> 00:12:44,110
lambda where we're turning it into

00:12:42,220 --> 00:12:46,750
tuples and go that sounds kind of

00:12:44,110 --> 00:12:50,410
unnecessary and that's totally a thing

00:12:46,750 --> 00:12:52,960
that we could get rid of but there are

00:12:50,410 --> 00:12:54,550
reasons why doesn't work we could start

00:12:52,960 --> 00:12:57,160
using memory-mapped arrow so right now

00:12:54,550 --> 00:12:59,230
we still put arrow records over top of

00:12:57,160 --> 00:13:02,950
unix sockets and so that was like a

00:12:59,230 --> 00:13:04,960
great plan memory mapped could be better

00:13:02,950 --> 00:13:07,480
since we're not actually sending a lot

00:13:04,960 --> 00:13:09,400
of data back though who knows if this is

00:13:07,480 --> 00:13:10,660
going to make a big difference and the

00:13:09,400 --> 00:13:12,580
other one that we could do which is

00:13:10,660 --> 00:13:15,310
really exciting and we're looking at in

00:13:12,580 --> 00:13:17,590
spark as well is if we sorry that I keep

00:13:15,310 --> 00:13:19,690
jumping back we can see that arrow can

00:13:17,590 --> 00:13:21,520
read directly from park' and one of the

00:13:19,690 --> 00:13:23,350
things we could do is if we know that

00:13:21,520 --> 00:13:26,050
we're running essentially just a tensor

00:13:23,350 --> 00:13:28,330
flow job one raw park' data we could

00:13:26,050 --> 00:13:30,400
just cut the jvm out although that might

00:13:28,330 --> 00:13:31,699
not be so exciting for the people in

00:13:30,400 --> 00:13:35,839
here that like Java

00:13:31,699 --> 00:13:39,470
but for those of you who don't we could

00:13:35,839 --> 00:13:42,470
get rid of Java um and that's cool okay

00:13:39,470 --> 00:13:44,899
so that's all fun and good but you're

00:13:42,470 --> 00:13:46,730
here to access tensorflow from the JVM

00:13:44,899 --> 00:13:54,709
so now we have to create multi-language

00:13:46,730 --> 00:13:56,509
pipelines limited excitement okay

00:13:54,709 --> 00:13:58,939
No so multi-language pipelines are

00:13:56,509 --> 00:14:01,040
amazing because the alternative is that

00:13:58,939 --> 00:14:03,679
I learn how to rewrite tensorflow in

00:14:01,040 --> 00:14:06,230
Java and that does not sound like fun I

00:14:03,679 --> 00:14:08,540
just want to use tensor flow from Java I

00:14:06,230 --> 00:14:11,089
don't want to have to make it that is

00:14:08,540 --> 00:14:14,629
way too much work and I am not paid by

00:14:11,089 --> 00:14:17,089
the hour anymore and so we actually have

00:14:14,629 --> 00:14:20,299
these things and we can kind of make

00:14:17,089 --> 00:14:21,799
them work in spark and in the future

00:14:20,299 --> 00:14:25,069
will maybe be able to make them work in

00:14:21,799 --> 00:14:25,819
beam and right now in practice it's

00:14:25,069 --> 00:14:28,009
really painful

00:14:25,819 --> 00:14:30,949
but let's go look at the pain and see

00:14:28,009 --> 00:14:32,989
how we can do it yay okay so sparkling

00:14:30,949 --> 00:14:34,790
ml is the project where I've done this

00:14:32,989 --> 00:14:39,019
you can go check it out it's on github

00:14:34,790 --> 00:14:40,519
sparkling pandas and it supports other

00:14:39,019 --> 00:14:42,379
things besides tensorflow

00:14:40,519 --> 00:14:44,749
turns out no one gives a about the

00:14:42,379 --> 00:14:46,910
other things besides tensorflow if

00:14:44,749 --> 00:14:50,269
anyone's really into NLP it does some

00:14:46,910 --> 00:14:53,269
cool NLP stuff but come find me later

00:14:50,269 --> 00:14:57,709
NLP friends so okay how do we how do we

00:14:53,269 --> 00:15:01,839
make this work we use our good friend

00:14:57,709 --> 00:15:03,679
Piper J and we make a Java class for

00:15:01,839 --> 00:15:06,259
representing the interface for what our

00:15:03,679 --> 00:15:08,119
Python code is going to be and we allow

00:15:06,259 --> 00:15:09,980
it to call into that with arbitrary

00:15:08,119 --> 00:15:11,989
parameters and if you really want to

00:15:09,980 --> 00:15:14,809
look at start-up about pie you can

00:15:11,989 --> 00:15:17,059
definitely go look on the github but

00:15:14,809 --> 00:15:18,529
this is the short version of it is we

00:15:17,059 --> 00:15:20,179
put a bunch of functions in here and

00:15:18,529 --> 00:15:23,929
then we say this is the class that we're

00:15:20,179 --> 00:15:26,029
implementing and then we only allow Java

00:15:23,929 --> 00:15:28,850
to call us Java will give us the spark

00:15:26,029 --> 00:15:30,259
session information the name of the

00:15:28,850 --> 00:15:32,089
function that it wants to call and a

00:15:30,259 --> 00:15:34,039
bunch of parameters and we'll just

00:15:32,089 --> 00:15:37,609
evaluate those parameters as an AST

00:15:34,039 --> 00:15:40,009
literal what could go wrong many things

00:15:37,609 --> 00:15:42,409
but provided that you have no malicious

00:15:40,009 --> 00:15:44,419
users ever this is fine if you have

00:15:42,409 --> 00:15:45,500
malicious users this is an excellent way

00:15:44,419 --> 00:15:51,020
to execute arbitrary

00:15:45,500 --> 00:15:53,150
code okay so the the Java boilerplate

00:15:51,020 --> 00:15:56,060
looks like pretty much the the inverse

00:15:53,150 --> 00:15:57,880
of this it's also pretty boring and it's

00:15:56,060 --> 00:16:01,790
the the full details are in a few files

00:15:57,880 --> 00:16:03,350
but now we can use it for NLP I promise

00:16:01,790 --> 00:16:08,180
we'll use it for tensor flow shortly but

00:16:03,350 --> 00:16:09,560
the NLP example is funner and simpler so

00:16:08,180 --> 00:16:13,940
the first thing that we do how many

00:16:09,560 --> 00:16:16,730
people are familiar with Spacey yay for

00:16:13,940 --> 00:16:20,350
people's um this example is going to be

00:16:16,730 --> 00:16:23,240
great for them for the rest of you oh

00:16:20,350 --> 00:16:25,370
wait I'm in Europe you you experience

00:16:23,240 --> 00:16:26,810
languages oh and I'm in Germany you

00:16:25,370 --> 00:16:29,690
experience languages where space

00:16:26,810 --> 00:16:32,420
tokenization is perhaps not ideal yes

00:16:29,690 --> 00:16:33,890
sometimes and so perhaps using the

00:16:32,420 --> 00:16:36,650
standard word count example that we all

00:16:33,890 --> 00:16:38,870
see you get a bad count of the words and

00:16:36,650 --> 00:16:41,570
so this is exciting we can rewrite our

00:16:38,870 --> 00:16:44,180
word count example to use Spacey take a

00:16:41,570 --> 00:16:46,220
really effective tokenization and we all

00:16:44,180 --> 00:16:50,240
know this is big data so word count is

00:16:46,220 --> 00:16:52,400
our use case so we we take in an input

00:16:50,240 --> 00:16:54,140
series and this is essentially we can

00:16:52,400 --> 00:16:55,610
take in pandas dataframes or we can take

00:16:54,140 --> 00:16:57,500
in series when we don't have structured

00:16:55,610 --> 00:16:58,760
data because I'm just getting a list of

00:16:57,500 --> 00:17:02,950
strings we're just gonna take in a

00:16:58,760 --> 00:17:05,240
series Spacey magic get is essentially a

00:17:02,950 --> 00:17:07,010
whole bunch of which you really

00:17:05,240 --> 00:17:09,020
don't want to look at that just

00:17:07,010 --> 00:17:11,449
initializes Spacey on the workers and

00:17:09,020 --> 00:17:12,770
make sure as everything is happy and it

00:17:11,449 --> 00:17:14,720
allows for reuse

00:17:12,770 --> 00:17:17,300
if you end up tokenizing a lot of data

00:17:14,720 --> 00:17:20,480
and then inside we call our happy little

00:17:17,300 --> 00:17:24,410
function and because I did this in

00:17:20,480 --> 00:17:26,689
Python to seven because reasons we are

00:17:24,410 --> 00:17:28,520
today I do it in Python 3 whatever I did

00:17:26,689 --> 00:17:31,480
it in something and it was painful so I

00:17:28,520 --> 00:17:33,440
have to explicitly called Unicode I

00:17:31,480 --> 00:17:35,330
remembering which virtual line of I have

00:17:33,440 --> 00:17:37,850
active when I'm like writing code for a

00:17:35,330 --> 00:17:39,470
slide as hard but this is this is pretty

00:17:37,850 --> 00:17:42,590
cool and we can tokenize our text and

00:17:39,470 --> 00:17:45,050
it'll go fast and on the JVM side we we

00:17:42,590 --> 00:17:46,880
just call it like a regular spark ml

00:17:45,050 --> 00:17:49,490
pipeline stage we specify our input

00:17:46,880 --> 00:17:51,230
columns and our output columns and our

00:17:49,490 --> 00:17:52,910
language which is English because it

00:17:51,230 --> 00:17:55,100
turns out that I don't speak any other

00:17:52,910 --> 00:17:56,810
languages besides English so I assure

00:17:55,100 --> 00:17:58,820
you this probably works better for

00:17:56,810 --> 00:18:01,639
German but

00:17:58,820 --> 00:18:04,940
to be fair I don't know but it probably

00:18:01,639 --> 00:18:07,490
does and that's good enough okay and so

00:18:04,940 --> 00:18:12,769
here's here's this very fancy diagram we

00:18:07,490 --> 00:18:14,120
can see oh okay seems like people are

00:18:12,769 --> 00:18:16,190
really excited about the tensor flow

00:18:14,120 --> 00:18:19,429
stuff and not so much NLP that's fine so

00:18:16,190 --> 00:18:21,200
spark deep learning pipelines are yet

00:18:19,429 --> 00:18:24,409
another way to do deep learning on top

00:18:21,200 --> 00:18:25,460
of spark and they have some limitations

00:18:24,409 --> 00:18:27,320
that you can read about if you're

00:18:25,460 --> 00:18:28,820
particularly interested in them but we

00:18:27,320 --> 00:18:32,870
can expose the spark deep learning

00:18:28,820 --> 00:18:35,330
package from Python into Scala in pretty

00:18:32,870 --> 00:18:39,429
much the same way how we do with

00:18:35,330 --> 00:18:45,289
everything else in sparkling ml and so

00:18:39,429 --> 00:18:48,259
we have to write this kind of not so

00:18:45,289 --> 00:18:50,419
pretty bit because the the first

00:18:48,259 --> 00:18:53,299
function that we were looking at back

00:18:50,419 --> 00:18:55,580
here is a really simple function it it

00:18:53,299 --> 00:18:57,200
takes in one column you know it's

00:18:55,580 --> 00:19:00,200
totally fine I can write that as UDF

00:18:57,200 --> 00:19:02,509
really simply but not everything can be

00:19:00,200 --> 00:19:04,639
directly defined as UDF some things

00:19:02,509 --> 00:19:06,230
actually need to take in a data frame

00:19:04,639 --> 00:19:08,690
and this is especially true for deep

00:19:06,230 --> 00:19:10,279
learning things where we want to look at

00:19:08,690 --> 00:19:13,129
a bunch of things on the data frame at

00:19:10,279 --> 00:19:17,419
the same time so we taken a data frame

00:19:13,129 --> 00:19:20,029
we make all kinds of happiness the Scala

00:19:17,419 --> 00:19:24,470
side looks pretty similar the model name

00:19:20,029 --> 00:19:26,600
yeah woo okay

00:19:24,470 --> 00:19:28,940
everyone's very excited by the Skylar

00:19:26,600 --> 00:19:30,580
boilerplate code ferrites no ok the

00:19:28,940 --> 00:19:33,710
front row is just like no I don't care

00:19:30,580 --> 00:19:35,629
ok and so this is this is this this is

00:19:33,710 --> 00:19:37,759
the part with the actual sadness where

00:19:35,629 --> 00:19:39,259
we take our our Scala parameters and we

00:19:37,759 --> 00:19:48,549
serialize them as strings to give to

00:19:39,259 --> 00:19:48,549
Python this is bad but it's not that bad

00:19:49,029 --> 00:19:53,779
ok yes and so whatever you can just if

00:19:52,190 --> 00:19:55,669
there's more parameters that you want to

00:19:53,779 --> 00:19:59,240
access in the model you can just add it

00:19:55,669 --> 00:20:01,100
here and do that because I got lazy and

00:19:59,240 --> 00:20:03,169
I added the minimum number of parameters

00:20:01,100 --> 00:20:05,000
required for this to work but you can

00:20:03,169 --> 00:20:07,279
come and add the parameters that you

00:20:05,000 --> 00:20:08,750
care about so you can access them and

00:20:07,279 --> 00:20:11,029
set them on your model and have happy

00:20:08,750 --> 00:20:11,860
fun times so you can sort of set model

00:20:11,029 --> 00:20:15,730
name you can have other

00:20:11,860 --> 00:20:18,580
there too and in a magical possible

00:20:15,730 --> 00:20:21,010
future our data will not have to flow

00:20:18,580 --> 00:20:24,970
through Python first that magical

00:20:21,010 --> 00:20:27,220
possible future requires a lot more code

00:20:24,970 --> 00:20:29,400
than exists today though so this magical

00:20:27,220 --> 00:20:33,610
possible future is like definitely a

00:20:29,400 --> 00:20:36,100
patches welcome scenario so let's not

00:20:33,610 --> 00:20:38,260
focus on that okay cool so that is how

00:20:36,100 --> 00:20:40,299
to make sure work with spark and the

00:20:38,260 --> 00:20:42,580
various Python based systems and

00:20:40,299 --> 00:20:44,679
exposing the Python systems into the JVM

00:20:42,580 --> 00:20:48,309
but there are other ways to do this too

00:20:44,679 --> 00:20:51,190
we could use DL for J it also uses arrow

00:20:48,309 --> 00:20:54,010
probably I'm like 50 percent sure from

00:20:51,190 --> 00:20:57,520
reading their code it looks like they do

00:20:54,010 --> 00:20:59,590
but the DL for J code is kind of gnarly

00:20:57,520 --> 00:21:04,929
so I'm not a hundred percent sure where

00:20:59,590 --> 00:21:06,760
they use it this leads us really well

00:21:04,929 --> 00:21:07,990
into our next point you you might find

00:21:06,760 --> 00:21:09,790
yourself trying to train a deep learning

00:21:07,990 --> 00:21:12,669
model and then finding yourself needing

00:21:09,790 --> 00:21:13,990
to do this thing called feature prep how

00:21:12,669 --> 00:21:18,070
many people will spend their time doing

00:21:13,990 --> 00:21:20,350
feature prep there are less hands than I

00:21:18,070 --> 00:21:22,419
expected how many times how many people

00:21:20,350 --> 00:21:27,040
spend their times doing really cool ml

00:21:22,419 --> 00:21:30,190
 that is not feature prep there is

00:21:27,040 --> 00:21:32,679
two hands interesting interesting I want

00:21:30,190 --> 00:21:34,419
to talk to you about your jobs later but

00:21:32,679 --> 00:21:35,470
so there's a really good chance that the

00:21:34,419 --> 00:21:37,150
people who didn't raise their hand just

00:21:35,470 --> 00:21:40,840
aren't using this yet and it turns out

00:21:37,150 --> 00:21:43,600
that as as cool as this deal for J and

00:21:40,840 --> 00:21:45,580
all of these fun systems are our data

00:21:43,600 --> 00:21:48,070
has to be in a format that we can do our

00:21:45,580 --> 00:21:50,049
cool deep learning on it and so we've

00:21:48,070 --> 00:21:51,040
got two different options and probably

00:21:50,049 --> 00:21:52,120
there are more that I just don't

00:21:51,040 --> 00:21:53,980
remember

00:21:52,120 --> 00:21:57,160
there are pre-built packages that are

00:21:53,980 --> 00:21:59,410
designed to allow us to do our feature

00:21:57,160 --> 00:22:02,620
prep in a way that it can be reused at

00:21:59,410 --> 00:22:05,110
run to at serving time and another

00:22:02,620 --> 00:22:07,120
option is we could just write piles of

00:22:05,110 --> 00:22:10,900
custom code and then we could just keep

00:22:07,120 --> 00:22:12,669
it in sync by hand from training time to

00:22:10,900 --> 00:22:14,140
serving time and it definitely wouldn't

00:22:12,669 --> 00:22:17,020
get out of sync and start returning the

00:22:14,140 --> 00:22:19,000
wrong results and there's one person who

00:22:17,020 --> 00:22:20,770
finds that very amusing I want to know

00:22:19,000 --> 00:22:22,929
what results you were predicting anyways

00:22:20,770 --> 00:22:25,090
so another possible future is we could

00:22:22,929 --> 00:22:25,720
use Apache beam and those three people

00:22:25,090 --> 00:22:28,990
in the audience

00:22:25,720 --> 00:22:31,510
would be very excited and we could use

00:22:28,990 --> 00:22:34,210
tf-x on top of beam on top of spark or

00:22:31,510 --> 00:22:38,410
tf-x on top of beam on top of link and

00:22:34,210 --> 00:22:39,790
then yeah happiness and so if we did

00:22:38,410 --> 00:22:42,640
that we could get access to things like

00:22:39,790 --> 00:22:45,160
TF transform and this allows us to

00:22:42,640 --> 00:22:46,870
represent all of our like kinda gnarly

00:22:45,160 --> 00:22:48,850
feature prep stuff and have it

00:22:46,870 --> 00:22:51,490
automatically compiled in for serving

00:22:48,850 --> 00:22:54,190
into our graph it's really fun it runs

00:22:51,490 --> 00:22:58,000
on top of a patchy beam and it currently

00:22:54,190 --> 00:23:00,070
doesn't work outside of Google cloud

00:22:58,000 --> 00:23:03,640
platform um if you're a Google customer

00:23:00,070 --> 00:23:05,800
it's great and that's lovely but I I

00:23:03,640 --> 00:23:07,750
also want other people to be able to use

00:23:05,800 --> 00:23:10,090
it and we're working on it but if you go

00:23:07,750 --> 00:23:13,300
home and try and use this today you will

00:23:10,090 --> 00:23:18,880
be sad but that being said let's look at

00:23:13,300 --> 00:23:20,770
the code and so yeah we can scale to

00:23:18,880 --> 00:23:23,610
zero one strings to ants all of the

00:23:20,770 --> 00:23:26,260
Commons sort of feature prep stuff and

00:23:23,610 --> 00:23:28,120
for example like scaling to 0 to 1 or

00:23:26,260 --> 00:23:29,560
computing the mean I mean that requires

00:23:28,120 --> 00:23:33,700
that we actually see all of the data

00:23:29,560 --> 00:23:35,560
first right I can't just do this with

00:23:33,700 --> 00:23:36,640
like a hash function and hope for the

00:23:35,560 --> 00:23:39,220
best

00:23:36,640 --> 00:23:41,950
and so TF transform runs this sort of

00:23:39,220 --> 00:23:45,520
analyze pass on our data and it outputs

00:23:41,950 --> 00:23:47,320
pure tensor flow constant tensors that

00:23:45,520 --> 00:23:49,930
can then be used in your same serving

00:23:47,320 --> 00:23:53,260
graph and you can use the fun happy you

00:23:49,930 --> 00:23:55,780
know tensorflow serving ecosystem and

00:23:53,260 --> 00:23:57,250
there's a whole bunch of things in there

00:23:55,780 --> 00:24:01,570
for pretty much any use case that you

00:23:57,250 --> 00:24:05,020
want and so let's let's focus on the

00:24:01,570 --> 00:24:06,970
limitations of this so non JVM beam does

00:24:05,020 --> 00:24:08,530
not work so well outside of Google in

00:24:06,970 --> 00:24:10,270
its environment so if you want to make

00:24:08,530 --> 00:24:11,650
something in production today you're

00:24:10,270 --> 00:24:13,480
kind of stuck with one of the other

00:24:11,650 --> 00:24:15,220
things that we've been talking about or

00:24:13,480 --> 00:24:21,070
becoming a Google cloud customer which

00:24:15,220 --> 00:24:23,860
is great you could we use gr PC and

00:24:21,070 --> 00:24:28,060
protobuf instead of Aero performance is

00:24:23,860 --> 00:24:30,040
more or less equivalent it's just not

00:24:28,060 --> 00:24:33,310
invented here syndrome I guess would be

00:24:30,040 --> 00:24:36,020
the not so polite description but it

00:24:33,310 --> 00:24:38,990
predates Aero as well

00:24:36,020 --> 00:24:41,270
and there's exciting new work if there

00:24:38,990 --> 00:24:43,640
are Python 3 users in the audience we

00:24:41,270 --> 00:24:46,610
don't support Python 3 I'm really sorry

00:24:43,640 --> 00:24:48,980
you can come join me there is a sort of

00:24:46,610 --> 00:24:55,100
tracking JIRA where we have lots of fun

00:24:48,980 --> 00:24:58,040
things it kind of there's a hacked up

00:24:55,100 --> 00:25:00,950
prototype that you can look at and you

00:24:58,040 --> 00:25:02,480
can even run go on top of it the go part

00:25:00,950 --> 00:25:06,500
is completely unrelated to tensorflow

00:25:02,480 --> 00:25:08,600
but kind of fun except in that it allows

00:25:06,500 --> 00:25:11,000
us to run native code and therefore we

00:25:08,600 --> 00:25:13,880
could theoretically run tensorflow

00:25:11,000 --> 00:25:18,309
stuff on top of it that way - are there

00:25:13,880 --> 00:25:21,350
any go users whoa come talk to me about

00:25:18,309 --> 00:25:23,690
running go on big data with beam if

00:25:21,350 --> 00:25:26,270
you're so inclined and you can run it on

00:25:23,690 --> 00:25:30,170
top of Google Cloud today or sort of

00:25:26,270 --> 00:25:32,780
Apache flink kind of my very much kind

00:25:30,170 --> 00:25:34,760
of and in the future maybe also spark

00:25:32,780 --> 00:25:36,050
which would be really convenient because

00:25:34,760 --> 00:25:38,809
I don't want to have to learn a lot of

00:25:36,050 --> 00:25:41,390
aflink I'm pretty lazy and as a San

00:25:38,809 --> 00:25:45,679
Francisco kid you know I learned spark

00:25:41,390 --> 00:25:48,200
first so yeah ok cool so how does how

00:25:45,679 --> 00:25:50,870
does this stuff relate to tensor flow so

00:25:48,200 --> 00:25:52,490
tensor flow is in Python kind of and if

00:25:50,870 --> 00:25:54,410
we support multi-language pipelines and

00:25:52,490 --> 00:25:57,290
beam we can do the same tricks that we

00:25:54,410 --> 00:25:58,550
did inside of spark to make this stuff

00:25:57,290 --> 00:26:00,980
work and then we can actually be

00:25:58,550 --> 00:26:03,740
portable and we can use cool libraries

00:26:00,980 --> 00:26:05,690
like TF transform so we don't have to

00:26:03,740 --> 00:26:08,570
write like giant piles of custom code

00:26:05,690 --> 00:26:11,240
and write custom model exporting code it

00:26:08,570 --> 00:26:15,140
doesn't work today or tomorrow but

00:26:11,240 --> 00:26:17,420
eventually here's a whole bunch of

00:26:15,140 --> 00:26:19,400
resources if anyone's interested in like

00:26:17,420 --> 00:26:23,030
playing with these things

00:26:19,400 --> 00:26:24,980
sparkling ml is not production ready

00:26:23,030 --> 00:26:26,570
either it's like a project that I work

00:26:24,980 --> 00:26:32,059
on with some friends who occasionally

00:26:26,570 --> 00:26:36,380
show up but it probably works and if it

00:26:32,059 --> 00:26:37,910
doesn't you could fix it and I mean it

00:26:36,380 --> 00:26:40,760
passes its test suite which is pretty

00:26:37,910 --> 00:26:41,840
promising - we're gonna get to my most

00:26:40,760 --> 00:26:44,840
important slide which is

00:26:41,840 --> 00:26:48,650
high-performance spark it is completely

00:26:44,840 --> 00:26:49,850
unrelated to this talk but I do get the

00:26:48,650 --> 00:26:52,679
highest royalties on

00:26:49,850 --> 00:26:54,509
so I strongly encourage those of you

00:26:52,679 --> 00:26:58,009
with a corporate expense account to

00:26:54,509 --> 00:27:01,080
purchase this book and not return it

00:26:58,009 --> 00:27:03,119
cats love it and if you buy the printed

00:27:01,080 --> 00:27:04,769
copy your cat will love the book

00:27:03,119 --> 00:27:06,480
although definitely buy the e-book copy

00:27:04,769 --> 00:27:11,190
as well because I get double royalties

00:27:06,480 --> 00:27:13,110
on the e-book hmm okay I will be talking

00:27:11,190 --> 00:27:17,159
about dealing with contributor overload

00:27:13,110 --> 00:27:18,929
later on this week if you want an excuse

00:27:17,159 --> 00:27:20,789
to go to New York I'll be there later on

00:27:18,929 --> 00:27:23,129
this month and you can come join me at

00:27:20,789 --> 00:27:25,590
any of these other fun events and

00:27:23,129 --> 00:27:35,879
otherwise I think happy question time if

00:27:25,590 --> 00:27:39,960
people have questions or okay I know he

00:27:35,879 --> 00:27:47,519
has a question are you not gonna ask me

00:27:39,960 --> 00:27:50,399
that question I it's up to you tonight

00:27:47,519 --> 00:27:52,440
perfect hi so you were talking a lot of

00:27:50,399 --> 00:27:54,419
all these confirmations and all this

00:27:52,440 --> 00:27:56,779
software and so on but what about the

00:27:54,419 --> 00:27:59,070
last resort in terms of performance

00:27:56,779 --> 00:28:01,259
since here we need to go from this

00:27:59,070 --> 00:28:03,659
platform to this framework and so on

00:28:01,259 --> 00:28:04,710
what about my last result of it I mean

00:28:03,659 --> 00:28:06,929
is it going to take more time to

00:28:04,710 --> 00:28:08,700
actually get what it wants or yeah I

00:28:06,929 --> 00:28:10,609
mean every time we copy data is

00:28:08,700 --> 00:28:16,950
expensive

00:28:10,609 --> 00:28:18,119
that being said is so this theory let's

00:28:16,950 --> 00:28:23,009
go all the way back

00:28:18,119 --> 00:28:25,259
magic slide magic slide come on there's

00:28:23,009 --> 00:28:27,869
a lot of slides okay yeah the theory

00:28:25,259 --> 00:28:29,489
with with Apache arrow is that

00:28:27,869 --> 00:28:31,470
well we'll be jumping between all of

00:28:29,489 --> 00:28:33,869
these systems if our data remains in the

00:28:31,470 --> 00:28:35,639
same format it's not so expensive for

00:28:33,869 --> 00:28:37,470
today right now we still end up copying

00:28:35,639 --> 00:28:40,259
that from the memory of these different

00:28:37,470 --> 00:28:42,090
systems but we could use shared memory

00:28:40,259 --> 00:28:46,019
buffers it's just that every attempt of

00:28:42,090 --> 00:28:49,799
that so far is mostly resulted in a lot

00:28:46,019 --> 00:28:52,830
of exceptions and a few sec faults but

00:28:49,799 --> 00:28:54,869
it's not inherently not going to work as

00:28:52,830 --> 00:28:56,340
a shared memory buffer and once it

00:28:54,869 --> 00:28:58,109
starts being passed around as a shared

00:28:56,340 --> 00:28:59,940
memory buffer the cost of switching

00:28:58,109 --> 00:29:01,530
between these systems will guide a lot

00:28:59,940 --> 00:29:06,179
less expensive

00:29:01,530 --> 00:29:07,799
does that answer your question yeah the

00:29:06,179 --> 00:29:12,600
present is bad the future is better

00:29:07,799 --> 00:29:19,500
maybe not a guarantee oh yeah okay I had

00:29:12,600 --> 00:29:22,169
a question about your runs for bean so

00:29:19,500 --> 00:29:23,789
you have Python go Java and God knows

00:29:22,169 --> 00:29:26,220
what come what's coming next and you

00:29:23,789 --> 00:29:28,980
have fling spark epics blah blah blah

00:29:26,220 --> 00:29:31,380
yeah and so the different permutations

00:29:28,980 --> 00:29:33,840
on the combinations so spa Python SDK

00:29:31,380 --> 00:29:36,000
with flink runner and go SDK with spark

00:29:33,840 --> 00:29:38,070
Runner how do you regression test all of

00:29:36,000 --> 00:29:46,440
these in a release it's kind of seems

00:29:38,070 --> 00:29:48,539
like pain yeah okay I mean it's it's

00:29:46,440 --> 00:29:51,770
really slow and annoying to test a whole

00:29:48,539 --> 00:29:55,740
bunch of different things but it's okay

00:29:51,770 --> 00:29:58,020
cloud computers magic we can just run a

00:29:55,740 --> 00:30:00,809
whole bunch of computers and test the

00:29:58,020 --> 00:30:05,340
matrix so the next the next question is

00:30:00,809 --> 00:30:07,140
about the beam in the flink run a lot of

00:30:05,340 --> 00:30:08,549
time to just start up the workflow and

00:30:07,140 --> 00:30:11,130
you know for the pipeline to start

00:30:08,549 --> 00:30:14,130
executing yeah I know it's experimental

00:30:11,130 --> 00:30:16,049
it's so what is happening in between the

00:30:14,130 --> 00:30:17,520
distinct like five to six minutes just

00:30:16,049 --> 00:30:18,570
to load up on my laptop or the pipeline

00:30:17,520 --> 00:30:21,030
to start up on my laptop

00:30:18,570 --> 00:30:23,760
that's a great question um so it does a

00:30:21,030 --> 00:30:26,970
whole bunch of things it starts a bunch

00:30:23,760 --> 00:30:29,880
of containers which takes just a little

00:30:26,970 --> 00:30:32,429
bit of time and then once it started the

00:30:29,880 --> 00:30:34,200
containers it also starts flink waits

00:30:32,429 --> 00:30:36,600
for flink to finish starting and then

00:30:34,200 --> 00:30:38,130
start some more containers and waits for

00:30:36,600 --> 00:30:40,549
those containers to finish starting as

00:30:38,130 --> 00:30:43,140
well and so pretty much it's just

00:30:40,549 --> 00:30:49,860
booting a whole bunch of like mini VMs

00:30:43,140 --> 00:30:52,679
and oh and the other part is there there

00:30:49,860 --> 00:30:57,600
was like a race condition and so we

00:30:52,679 --> 00:30:59,460
fixed that in the traditional way which

00:30:57,600 --> 00:31:02,429
is just waiting a long time and hoping

00:30:59,460 --> 00:31:04,110
it doesn't happen so yeah my question

00:31:02,429 --> 00:31:05,880
knows more of it the Python SDK and the

00:31:04,110 --> 00:31:09,120
ping fling Python is to hear or know oh

00:31:05,880 --> 00:31:10,530
yeah so we for the for the Python SDK

00:31:09,120 --> 00:31:13,020
though like we start a different

00:31:10,530 --> 00:31:14,899
container with extra happy things inside

00:31:13,020 --> 00:31:16,459
of it

00:31:14,899 --> 00:31:19,219
and the extra happy things take a while

00:31:16,459 --> 00:31:21,589
to start because we so with the with the

00:31:19,219 --> 00:31:23,149
Java direct runner right now it's not

00:31:21,589 --> 00:31:24,829
actually starting a whole bunch of

00:31:23,149 --> 00:31:27,139
different containers and doing the our

00:31:24,829 --> 00:31:32,149
pcs over them it's just like I'm in Java

00:31:27,139 --> 00:31:35,589
I'm awesome and so the overheads a lot

00:31:32,149 --> 00:31:39,769
lower to start a new pipeline for now

00:31:35,589 --> 00:31:43,699
the last question sorry let's see if I

00:31:39,769 --> 00:31:45,469
have a beam with a fling runner and in

00:31:43,699 --> 00:31:47,509
fling if I was just writing my job in

00:31:45,469 --> 00:31:49,099
Flint I can paralyze each operator and

00:31:47,509 --> 00:31:50,799
you know the number of parallel I can

00:31:49,099 --> 00:31:54,049
set the parallelism for each operator I

00:31:50,799 --> 00:31:55,549
kind of find that missing in beam and

00:31:54,049 --> 00:31:57,949
you know trying to translate that from

00:31:55,549 --> 00:32:03,589
beam to fling how does it all work at is

00:31:57,949 --> 00:32:08,049
it still working yeah okay so that's

00:32:03,589 --> 00:32:08,049
that's not super well exposed in the API

00:32:08,379 --> 00:32:17,779
yeah I think that is an area of active

00:32:14,239 --> 00:32:20,709
work would be V but it's not like people

00:32:17,779 --> 00:32:23,599
aren't aware of this sort of like

00:32:20,709 --> 00:32:25,189
challenge with the the decreased

00:32:23,599 --> 00:32:28,389
knowledge of what's happening inside the

00:32:25,189 --> 00:32:32,209
box and it's trying to find ways to

00:32:28,389 --> 00:32:34,609
expose enough information about the the

00:32:32,209 --> 00:32:35,959
runners without making it so that this

00:32:34,609 --> 00:32:38,059
pipeline that you've written which is in

00:32:35,959 --> 00:32:39,919
theory like portable across all of these

00:32:38,059 --> 00:32:41,899
different runners becomes locked into

00:32:39,919 --> 00:32:44,389
that one runner right we don't want that

00:32:41,899 --> 00:32:50,989
to happen and so it's it's complicated

00:32:44,389 --> 00:32:55,819
to do well so it takes time cool are

00:32:50,989 --> 00:32:59,659
there any non beam questions yes I have

00:32:55,819 --> 00:33:02,779
a question do any of these problems that

00:32:59,659 --> 00:33:04,819
you present it go away if we don't need

00:33:02,779 --> 00:33:07,549
to train any models with tons of flow

00:33:04,819 --> 00:33:10,609
but just want to use model and just just

00:33:07,549 --> 00:33:13,639
use it for making predictions I mean if

00:33:10,609 --> 00:33:15,649
you just want to serve your tensor flow

00:33:13,639 --> 00:33:17,899
models do you want to serve your tensor

00:33:15,649 --> 00:33:19,429
flow models with like Big Data mm-hmm

00:33:17,899 --> 00:33:23,809
or do you want to just serve it

00:33:19,429 --> 00:33:27,380
streaming streaming oh yeah I mean the

00:33:23,809 --> 00:33:29,380
first is don't touch the JVM just

00:33:27,380 --> 00:33:32,120
forget all of this and just use

00:33:29,380 --> 00:33:39,640
like the happy TF serving library so we

00:33:32,120 --> 00:33:42,710
have some legacy kudos oh well then that

00:33:39,640 --> 00:33:44,630
depends on your legacy code I guess is

00:33:42,710 --> 00:33:48,460
the short answer I would have to take a

00:33:44,630 --> 00:33:48,460
look at it and I really don't want to

00:33:51,970 --> 00:34:08,890
any other question yes thank you again

00:34:01,930 --> 00:34:08,890

YouTube URL: https://www.youtube.com/watch?v=zFwQm58EIpo


