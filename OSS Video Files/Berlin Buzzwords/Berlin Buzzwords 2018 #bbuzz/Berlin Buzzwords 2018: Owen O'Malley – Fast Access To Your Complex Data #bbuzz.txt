Title: Berlin Buzzwords 2018: Owen O'Malley – Fast Access To Your Complex Data #bbuzz
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Owen O'Malley talking about "Fast Access To Your Complex Data - Avro, JSON, ORC, and Parquet".

The landscape for storing your big data is quite complex, with several competing formats and different implementations of each format. Understanding your use of the data is critical for picking the format. Depending on your use case, the different formats perform very differently. Although you can use a hammer to drive a screw, it isn’t fast or easy to do so.

The use cases that we’ve examined are:
- Reading all of the columns
- Reading a few of the columns
- Filtering using a filter predicate
- Writing the data

While previous work has compared the size and speed from Apache Hive, this presentation will present benchmarks from Apache Spark including the new work that radically improves the performance of Spark on ORC. This presentation will also include tips and suggestions to optimize the performance of your application while reading and writing the data.

Read more:
https://2018.berlinbuzzwords.de/18/session/fast-access-your-complex-data-avro-json-orc-and-parquet

About Owen O'Malley:
https://2018.berlinbuzzwords.de/users/owen-omalley

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:04,930 --> 00:00:13,310
so am i I've worked on Hadoop for longer

00:00:08,960 --> 00:00:15,590
than I should admit but I was the tech

00:00:13,310 --> 00:00:18,920
lead for MapReduce I worked on adding

00:00:15,590 --> 00:00:22,730
security and more recently I've been

00:00:18,920 --> 00:00:24,680
working on hive and orc I've worked on a

00:00:22,730 --> 00:00:27,189
lot of the different file formats I've

00:00:24,680 --> 00:00:30,470
done work on sequence file our C file

00:00:27,189 --> 00:00:34,340
orc file of course T file does anyone

00:00:30,470 --> 00:00:36,260
actually know what T file is aha C it's

00:00:34,340 --> 00:00:39,710
buried in the Hadoop source code and

00:00:36,260 --> 00:00:43,519
it's not very often used although they

00:00:39,710 --> 00:00:47,510
just put in a thing for log aggregation

00:00:43,519 --> 00:00:49,219
that uses it and so I had one of our

00:00:47,510 --> 00:00:51,620
engineers come on and look me up because

00:00:49,219 --> 00:00:54,079
he I was the only name on the design doc

00:00:51,620 --> 00:00:55,429
that he recognized and he was trying to

00:00:54,079 --> 00:00:58,699
ask questions from something that I

00:00:55,429 --> 00:01:01,370
touched like 10 years previous that was

00:00:58,699 --> 00:01:05,089
exciting and if I worked on the Avro

00:01:01,370 --> 00:01:07,220
requirements back when Doug was getting

00:01:05,089 --> 00:01:09,140
started with that so what was the goal

00:01:07,220 --> 00:01:11,150
for this I had previously done a

00:01:09,140 --> 00:01:14,180
benchmark comparing the different file

00:01:11,150 --> 00:01:17,230
formats in terms of how I've used them

00:01:14,180 --> 00:01:21,380
but we've been doing a lot of work

00:01:17,230 --> 00:01:24,230
actually one of my co-workers named Dong

00:01:21,380 --> 00:01:28,520
Joon has done a lot of work speeding up

00:01:24,230 --> 00:01:31,460
the spark access for orc and so I wanted

00:01:28,520 --> 00:01:33,530
to take the previous benchmarks and

00:01:31,460 --> 00:01:36,080
extend them so that they would work with

00:01:33,530 --> 00:01:38,270
spark and test it the way that spark

00:01:36,080 --> 00:01:42,050
accesses your data because that's always

00:01:38,270 --> 00:01:44,900
been part of Hadoop's central goal and

00:01:42,050 --> 00:01:46,940
promise is that once you put your data

00:01:44,900 --> 00:01:48,350
into the system you can access it the

00:01:46,940 --> 00:01:51,680
way that makes the most sense for you

00:01:48,350 --> 00:01:56,660
and so testing how different people are

00:01:51,680 --> 00:01:59,960
accessing the data can help all the file

00:01:56,660 --> 00:02:01,940
formats get better this really was a

00:01:59,960 --> 00:02:04,400
science experiment right we didn't know

00:02:01,940 --> 00:02:09,170
what we didn't know and so we wanted to

00:02:04,400 --> 00:02:12,200
find out not only what performs the best

00:02:09,170 --> 00:02:15,950
but also where are they failing what can

00:02:12,200 --> 00:02:18,200
do better and so on I wanted to use real

00:02:15,950 --> 00:02:20,900
and diverse datasets

00:02:18,200 --> 00:02:24,140
I've seen some benchmarks before that

00:02:20,900 --> 00:02:27,620
either use t PCBs which is a very common

00:02:24,140 --> 00:02:32,120
data set and very well known but it's

00:02:27,620 --> 00:02:36,470
all synthetic data which leads to some

00:02:32,120 --> 00:02:40,910
insane properties of the data that can

00:02:36,470 --> 00:02:43,310
mess up your benchmarks and so having

00:02:40,910 --> 00:02:46,220
real data was important to me and I

00:02:43,310 --> 00:02:48,799
wanted the benchmarks to be open sourced

00:02:46,220 --> 00:02:50,900
and in a public place so that anyone

00:02:48,799 --> 00:02:53,239
could come in with feedback or

00:02:50,900 --> 00:02:56,900
suggestions and see exactly what what it

00:02:53,239 --> 00:02:58,430
does okay so I wanted to talk about the

00:02:56,900 --> 00:03:03,530
file formats a little bit

00:02:58,430 --> 00:03:05,360
Avro was designed by Doug cutting it's a

00:03:03,530 --> 00:03:08,239
cross language file format for Hadoop

00:03:05,360 --> 00:03:10,370
one of its early it was the first one to

00:03:08,239 --> 00:03:13,220
really do schema evolution in a big way

00:03:10,370 --> 00:03:16,190
and so that really was one of its

00:03:13,220 --> 00:03:18,530
defining characteristics the schema was

00:03:16,190 --> 00:03:21,349
a great read from the data

00:03:18,530 --> 00:03:23,090
unlike protobuf or thrift which was

00:03:21,349 --> 00:03:25,459
because it was originally designed as a

00:03:23,090 --> 00:03:29,450
file format it really wasn't designed as

00:03:25,459 --> 00:03:30,980
a messaging or RPC level interface but

00:03:29,450 --> 00:03:33,590
even though that's how it's typically

00:03:30,980 --> 00:03:35,299
used now and it's a row major format

00:03:33,590 --> 00:03:36,970
which means each row is written out

00:03:35,299 --> 00:03:39,200
together

00:03:36,970 --> 00:03:43,970
Avro by the way does anyone know what it

00:03:39,200 --> 00:03:46,730
what the name came from it was the name

00:03:43,970 --> 00:03:49,370
of an airplane company that Doug son

00:03:46,730 --> 00:03:53,420
liked actually of course the most

00:03:49,370 --> 00:03:55,670
popular one of his projects Hadoop got

00:03:53,420 --> 00:03:58,000
its name because of that was the name

00:03:55,670 --> 00:04:01,340
his son's name for a stuffed elephant

00:03:58,000 --> 00:04:04,040
Jason of course is incredibly common

00:04:01,340 --> 00:04:06,799
it's a serialization format for HTTP in

00:04:04,040 --> 00:04:10,130
JavaScript it's a text format with many

00:04:06,799 --> 00:04:11,480
many parsers the schema is completely

00:04:10,130 --> 00:04:13,660
integrated with the data so you

00:04:11,480 --> 00:04:16,519
basically if someone gives you a JSON

00:04:13,660 --> 00:04:18,079
document you basically have to read the

00:04:16,519 --> 00:04:20,870
whole thing in order to figure out what

00:04:18,079 --> 00:04:22,940
types are in there it's a row major

00:04:20,870 --> 00:04:27,050
format each rows are in together and

00:04:22,940 --> 00:04:30,919
usually compression is put in on top or

00:04:27,050 --> 00:04:31,820
core or C was originally started as part

00:04:30,919 --> 00:04:34,490
of five

00:04:31,820 --> 00:04:37,700
to replace our C file and now it's a

00:04:34,490 --> 00:04:39,590
top-level project this schema is

00:04:37,700 --> 00:04:42,650
segregated into the footer so you just

00:04:39,590 --> 00:04:44,630
have one copy of the schema it's a

00:04:42,650 --> 00:04:46,760
column major format so that you can read

00:04:44,630 --> 00:04:48,590
individual columns without reading the

00:04:46,760 --> 00:04:51,320
other columns we'll see why that matters

00:04:48,590 --> 00:04:53,990
a lot in a little bit and it's got a

00:04:51,320 --> 00:04:56,600
rich type type model and it's stored

00:04:53,990 --> 00:04:59,570
top-down it's got two integrated

00:04:56,600 --> 00:05:02,180
compression index and stats so part of

00:04:59,570 --> 00:05:03,800
what our C file did in particular was it

00:05:02,180 --> 00:05:07,820
treated each of the columns as a blob

00:05:03,800 --> 00:05:09,200
that made sense originally but it meant

00:05:07,820 --> 00:05:10,550
that you couldn't do anything

00:05:09,200 --> 00:05:12,800
higher-level you couldn't have any

00:05:10,550 --> 00:05:15,470
understanding of what the data was in

00:05:12,800 --> 00:05:17,630
the file without a lot of outside

00:05:15,470 --> 00:05:19,730
information that turned out to be a

00:05:17,630 --> 00:05:22,940
serious mistake and that's a lot of what

00:05:19,730 --> 00:05:26,750
we're trying to fix while we were

00:05:22,940 --> 00:05:29,270
designing or C the guys at Twitter were

00:05:26,750 --> 00:05:31,220
designing park' and they based it on

00:05:29,270 --> 00:05:36,080
Google's Dremel paper which we looked at

00:05:31,220 --> 00:05:36,650
- they also segregated the schema into

00:05:36,080 --> 00:05:39,350
the footer

00:05:36,650 --> 00:05:41,390
it's a col major format they have a much

00:05:39,350 --> 00:05:46,040
simpler type model where you don't

00:05:41,390 --> 00:05:51,410
represent the types as precisely oh you

00:05:46,040 --> 00:05:53,540
wanted to change the mics and they push

00:05:51,410 --> 00:05:56,330
all the data down to the leaves excuse

00:05:53,540 --> 00:05:58,510
me well we have some technical changes

00:05:56,330 --> 00:05:58,510
here

00:06:08,729 --> 00:06:11,539
whoops

00:06:12,960 --> 00:06:17,610
okay so all the data was pushed to the

00:06:15,870 --> 00:06:19,800
leaves of the tree so what that meant

00:06:17,610 --> 00:06:22,110
was that intermediate columns if you

00:06:19,800 --> 00:06:24,330
have structures or a list of struts in

00:06:22,110 --> 00:06:25,800
the middle of your data all that

00:06:24,330 --> 00:06:27,990
information is pushed to the leaves

00:06:25,800 --> 00:06:29,490
which means that you gain some

00:06:27,990 --> 00:06:33,509
advantages but it means there's some

00:06:29,490 --> 00:06:36,059
duplication there okay so what data sets

00:06:33,509 --> 00:06:37,379
did we look at the first one was the New

00:06:36,059 --> 00:06:39,419
York taxi data set

00:06:37,379 --> 00:06:43,169
every time someone takes a taxi in new

00:06:39,419 --> 00:06:45,960
york new york publishes a data set with

00:06:43,169 --> 00:06:47,159
a row for each ride they tell you where

00:06:45,960 --> 00:06:49,770
they got picked up where they get

00:06:47,159 --> 00:06:55,099
dropped off what time it was

00:06:49,770 --> 00:06:58,559
and how much they paid in fares and tip

00:06:55,099 --> 00:07:01,080
so that really awesome data set actually

00:06:58,559 --> 00:07:05,309
there's some great data analytics that

00:07:01,080 --> 00:07:07,199
was done at that URL and it had some

00:07:05,309 --> 00:07:09,599
really interesting characteristics like

00:07:07,199 --> 00:07:11,129
even though the data is totally

00:07:09,599 --> 00:07:14,339
anonymous there's nothing in there about

00:07:11,129 --> 00:07:16,560
people they you can tell who some people

00:07:14,339 --> 00:07:18,419
are because when they get dropped off at

00:07:16,560 --> 00:07:22,080
a house out and not in the city because

00:07:18,419 --> 00:07:25,469
the city is pretty has a lot of people

00:07:22,080 --> 00:07:28,129
overlapping but when they go out to the

00:07:25,469 --> 00:07:31,319
suburbs then you can tell who someone is

00:07:28,129 --> 00:07:33,659
and so you can pick out individual

00:07:31,319 --> 00:07:37,050
writers and you can see a lot of

00:07:33,659 --> 00:07:40,349
patterns so we've pulled 22 million rows

00:07:37,050 --> 00:07:43,139
out of that now the next one is

00:07:40,349 --> 00:07:45,870
unfortunately generated data but it's

00:07:43,139 --> 00:07:48,599
based on a customer that I've worked

00:07:45,870 --> 00:07:50,580
with a lot and I used their real schema

00:07:48,599 --> 00:07:52,110
and it's because there really wasn't

00:07:50,580 --> 00:07:54,899
anything that I could find that was

00:07:52,110 --> 00:07:57,870
public that matched the same kind of

00:07:54,899 --> 00:08:01,379
sales data we use the properties from

00:07:57,870 --> 00:08:03,749
their real data to generate the random

00:08:01,379 --> 00:08:05,189
data that we used for this there's a

00:08:03,749 --> 00:08:08,399
little bit of structures mostly

00:08:05,189 --> 00:08:10,889
timestamp strings Long's billions and we

00:08:08,399 --> 00:08:12,839
picked arbitrarily 25 million rows to

00:08:10,889 --> 00:08:14,999
kind of match you to other data sets the

00:08:12,839 --> 00:08:18,329
third one that's kind of fun is the

00:08:14,999 --> 00:08:19,919
github blog archives github makes

00:08:18,329 --> 00:08:20,310
available to Google and then at Google

00:08:19,919 --> 00:08:23,810
Howe

00:08:20,310 --> 00:08:27,210
the data where you get one row for every

00:08:23,810 --> 00:08:31,680
public action on a public repo so you

00:08:27,210 --> 00:08:35,039
can see oh oh and did a commit to or

00:08:31,680 --> 00:08:38,490
core only did a commits on hive that day

00:08:35,039 --> 00:08:41,219
and there'll be a roof to show it now

00:08:38,490 --> 00:08:43,680
this data is insanely complex it's got

00:08:41,219 --> 00:08:47,150
seven hundred and four columns with a

00:08:43,680 --> 00:08:49,920
lot of structures and a lot of nulls and

00:08:47,150 --> 00:08:52,770
just a half month of data still gives

00:08:49,920 --> 00:08:55,110
you ten million rows now one of the

00:08:52,770 --> 00:08:57,180
problems with that is the standard one

00:08:55,110 --> 00:08:59,970
actually that comes up a lot in

00:08:57,180 --> 00:09:02,460
companies of github doesn't really

00:08:59,970 --> 00:09:05,450
provide the schema for that data so I

00:09:02,460 --> 00:09:09,480
pulled a few million rows and I was like

00:09:05,450 --> 00:09:12,240
what types are in here and so we ended

00:09:09,480 --> 00:09:13,800
up writing a tool that couldn't go

00:09:12,240 --> 00:09:16,280
through the Jason and discover the

00:09:13,800 --> 00:09:18,630
schema and that's what we used for this

00:09:16,280 --> 00:09:20,550
at first I was gonna put the schema on

00:09:18,630 --> 00:09:26,160
the slide but then the schema is huge

00:09:20,550 --> 00:09:28,440
and so that was not a good plan okay so

00:09:26,160 --> 00:09:31,770
which versions of the software did I

00:09:28,440 --> 00:09:35,100
want to test you spark two three one I

00:09:31,770 --> 00:09:39,120
used Avro one eight two I tried to use

00:09:35,100 --> 00:09:42,870
110 but spark two three actually I had a

00:09:39,120 --> 00:09:47,910
problem with using the newer version of

00:09:42,870 --> 00:09:49,500
Avro I used orc 151 part k18 oh sorry

00:09:47,910 --> 00:09:54,780
all the way around it was part K not

00:09:49,500 --> 00:09:57,960
Avro yeah it was okay and then Sparky

00:09:54,780 --> 00:10:00,810
Avro four zero zero now it's really easy

00:09:57,960 --> 00:10:02,490
to say that in the slide getting it to

00:10:00,810 --> 00:10:05,760
actually work and build a single jar

00:10:02,490 --> 00:10:07,320
with all those things was not fun I was

00:10:05,760 --> 00:10:09,210
tempted to call it maven help but in

00:10:07,320 --> 00:10:11,310
reality maven just made it possible to

00:10:09,210 --> 00:10:12,450
do so it wasn't maven thought it was

00:10:11,310 --> 00:10:13,980
just the fact that I was trying to

00:10:12,450 --> 00:10:16,290
combine a bunch of different software

00:10:13,980 --> 00:10:20,250
that have these huge dependency trees

00:10:16,290 --> 00:10:21,870
and make them all work together okay

00:10:20,250 --> 00:10:24,030
there were a couple configurations that

00:10:21,870 --> 00:10:27,330
were really important the first is that

00:10:24,030 --> 00:10:29,430
orc probably can push down is turned off

00:10:27,330 --> 00:10:32,459
by default in spark and so you need to

00:10:29,430 --> 00:10:34,369
turn that to true and dungeons work

00:10:32,459 --> 00:10:37,319
actually

00:10:34,369 --> 00:10:39,360
hasn't been set as the default yet and

00:10:37,319 --> 00:10:40,889
so you need to set native equal to true

00:10:39,360 --> 00:10:42,769
otherwise you'll get the older

00:10:40,889 --> 00:10:47,309
implementation that goes through hive

00:10:42,769 --> 00:10:50,189
input format finally to get Avro to work

00:10:47,309 --> 00:10:52,679
you need to set a config but not in the

00:10:50,189 --> 00:10:56,279
spark config but in the associated

00:10:52,679 --> 00:11:00,769
Hadoop config to tell the Avro Reader to

00:10:56,279 --> 00:11:00,769
not ignore files that don't end in Avro

00:11:00,889 --> 00:11:10,649
unfortunately when I tried the oh sorry

00:11:07,709 --> 00:11:13,350
let me back up sorry the benchmark uses

00:11:10,649 --> 00:11:14,819
sparks sequels file format interface

00:11:13,350 --> 00:11:17,339
because that provided all the

00:11:14,819 --> 00:11:19,589
functionality we needed Jason or canned

00:11:17,339 --> 00:11:21,629
park' are all in spark that's like great

00:11:19,589 --> 00:11:23,790
this is exactly what I need

00:11:21,629 --> 00:11:26,459
afro didn't have one but then data

00:11:23,790 --> 00:11:29,759
breaks the people who work on spark made

00:11:26,459 --> 00:11:32,160
one available awesome unfortunately when

00:11:29,759 --> 00:11:34,679
you tried to run it you get the plane

00:11:32,160 --> 00:11:37,769
crash and it doesn't support all the

00:11:34,679 --> 00:11:39,509
spark types basically Avro doesn't have

00:11:37,769 --> 00:11:44,129
a time stamp field or a decimal field

00:11:39,509 --> 00:11:46,529
and so hive uses in 96 and bytes and

00:11:44,129 --> 00:11:49,079
sets a flag saying oh this is actually

00:11:46,529 --> 00:11:53,189
timestamp or this is actually a decimal

00:11:49,079 --> 00:11:56,819
and the Avro spark reader didn't handle

00:11:53,189 --> 00:11:58,230
it so it crashed so ignoring Avro for

00:11:56,819 --> 00:12:00,389
the rest of it oh and I actually talked

00:11:58,230 --> 00:12:02,449
about it a little bit so first I wanted

00:12:00,389 --> 00:12:06,449
to go through and just generate the data

00:12:02,449 --> 00:12:08,100
and why does the data size matter well

00:12:06,449 --> 00:12:10,319
because you're still storing this right

00:12:08,100 --> 00:12:15,869
you've got three copies of each of your

00:12:10,319 --> 00:12:19,769
data files in Hadoop and when Facebook

00:12:15,869 --> 00:12:23,009
moved from HDFS or moved from our C file

00:12:19,769 --> 00:12:25,410
to ork they saved I think it was 100

00:12:23,009 --> 00:12:27,420
terabytes and so they were mentioning a

00:12:25,410 --> 00:12:33,329
lot of servers because they suddenly

00:12:27,420 --> 00:12:36,179
didn't need them anymore the it's also a

00:12:33,329 --> 00:12:38,490
big factor in in the read speed HDFS

00:12:36,179 --> 00:12:41,220
read speeds are typically about 15

00:12:38,490 --> 00:12:44,160
megabytes a second in the real cluster

00:12:41,220 --> 00:12:46,040
the HDFS guys like the clay claim it's

00:12:44,160 --> 00:12:49,040
100 megabytes a second

00:12:46,040 --> 00:12:50,750
and it's not that only works if you've

00:12:49,040 --> 00:12:52,970
got a completely empty cluster that's

00:12:50,750 --> 00:12:54,529
only running your benchmark which I

00:12:52,970 --> 00:12:56,449
don't know how many of you have empty

00:12:54,529 --> 00:13:00,399
clusters if you do come talk to me I can

00:12:56,449 --> 00:13:03,980
get you some workload to run on them but

00:13:00,399 --> 00:13:07,490
that so 15 megabytes is much more

00:13:03,980 --> 00:13:09,490
realistic number or can park a both used

00:13:07,490 --> 00:13:12,970
run length encoding in dictionaries and

00:13:09,490 --> 00:13:15,529
all the formats have general compression

00:13:12,970 --> 00:13:16,040
with general compression you have a

00:13:15,529 --> 00:13:18,319
trade-off

00:13:16,040 --> 00:13:20,269
there's Z lib which gives you tighter

00:13:18,319 --> 00:13:24,500
compression but it's slower and snappy

00:13:20,269 --> 00:13:29,470
that is some compression is faster okay

00:13:24,500 --> 00:13:32,449
so here we've got the different file

00:13:29,470 --> 00:13:34,970
choices by the way if you're ever doing

00:13:32,449 --> 00:13:37,370
benchmarking and someone talks you into

00:13:34,970 --> 00:13:40,730
doing a matrix it's a really bad idea

00:13:37,370 --> 00:13:45,589
because all it takes is for file formats

00:13:40,730 --> 00:13:47,959
across three data sets and then across

00:13:45,589 --> 00:13:50,540
three different compression formats to

00:13:47,959 --> 00:13:53,839
make a really complicated chart and a

00:13:50,540 --> 00:13:55,880
lot of benchmarking so you can see the

00:13:53,839 --> 00:13:58,819
jason with no compression was the

00:13:55,880 --> 00:14:01,730
absolute worst parquet was e lib did the

00:13:58,819 --> 00:14:08,060
best and the other ones are in the

00:14:01,730 --> 00:14:09,529
middle so don't use Jason I had one

00:14:08,060 --> 00:14:11,660
someone once tell me I don't know who

00:14:09,529 --> 00:14:16,850
this Jason guy is but clearly he's a bad

00:14:11,660 --> 00:14:19,430
guy you should be using the snappier z

00:14:16,850 --> 00:14:21,319
lib compression Avro has a small

00:14:19,430 --> 00:14:25,730
compression window which hurts and

00:14:21,319 --> 00:14:28,010
parchesi lib is is the smallest okay on

00:14:25,730 --> 00:14:30,920
sales on the other hand we got a very

00:14:28,010 --> 00:14:34,250
different picture Jason is still bad see

00:14:30,920 --> 00:14:36,620
bad Jason don't store your data in Jason

00:14:34,250 --> 00:14:40,699
and we'll really see that when we get to

00:14:36,620 --> 00:14:44,480
the read speed instead of just the sizes

00:14:40,699 --> 00:14:48,500
orc did the best park' did the next best

00:14:44,480 --> 00:14:52,490
and then Avril is at the back just

00:14:48,500 --> 00:14:55,939
before Jason this is actually musing

00:14:52,490 --> 00:14:59,029
because the customer that the scheme was

00:14:55,939 --> 00:14:59,660
based on uses orc extensively and so

00:14:59,029 --> 00:15:00,650
it's good

00:14:59,660 --> 00:15:02,420
that it works on their test case

00:15:00,650 --> 00:15:05,480
although it's probably a feedback loop

00:15:02,420 --> 00:15:09,230
or we fix the things that don't work

00:15:05,480 --> 00:15:10,880
well for them so what happened here we

00:15:09,230 --> 00:15:13,220
had a lot of columns with small

00:15:10,880 --> 00:15:14,900
cardinality so we got dictionaries lots

00:15:13,220 --> 00:15:18,200
of timestamp columns or it worked as

00:15:14,900 --> 00:15:21,670
well and doubles doubles actually turned

00:15:18,200 --> 00:15:24,710
out to not encode is tightly with ork

00:15:21,670 --> 00:15:25,250
partially because we didn't run Lincoln

00:15:24,710 --> 00:15:26,750
code

00:15:25,250 --> 00:15:29,240
although we're looking at that for the

00:15:26,750 --> 00:15:32,540
next version of the format and also

00:15:29,240 --> 00:15:34,910
because our performance engineer detuned

00:15:32,540 --> 00:15:37,790
them so that they don't gzip is hard

00:15:34,910 --> 00:15:40,010
either because he realized he could make

00:15:37,790 --> 00:15:43,460
the whole thing faster if he detuned the

00:15:40,010 --> 00:15:46,700
Zeeland compression for doubles and

00:15:43,460 --> 00:15:49,220
finally for github this one we get some

00:15:46,700 --> 00:15:51,910
interesting results Jason none is still

00:15:49,220 --> 00:15:55,220
huge but look at the best one that's

00:15:51,910 --> 00:15:57,740
average Aysen

00:15:55,220 --> 00:16:00,530
or Avril with Z live inland Jason with Z

00:15:57,740 --> 00:16:04,100
live is just after it followed by orc

00:16:00,530 --> 00:16:10,190
wizzy lib so the reason that that

00:16:04,100 --> 00:16:13,160
happened is basically what happens in

00:16:10,190 --> 00:16:15,050
this file or this dataset is you have so

00:16:13,160 --> 00:16:17,060
many columns the columnar compression

00:16:15,050 --> 00:16:21,200
actually doesn't work very well because

00:16:17,060 --> 00:16:24,470
you've got 900 columns each one starts a

00:16:21,200 --> 00:16:26,750
new zeeland stream at the top and so

00:16:24,470 --> 00:16:29,180
instead of remembering you've got HTTP

00:16:26,750 --> 00:16:33,410
once and saying oh just refer to that

00:16:29,180 --> 00:16:35,660
which happens in in Jason and Avro it

00:16:33,410 --> 00:16:39,230
instead needs to relearn that over and

00:16:35,660 --> 00:16:43,040
over again and so we need to investigate

00:16:39,230 --> 00:16:44,930
using C standard with a pre-loaded

00:16:43,040 --> 00:16:50,920
dictionary so that we can get better

00:16:44,930 --> 00:16:53,930
compression okay so what use case is the

00:16:50,920 --> 00:16:57,470
first one we just want to read all of

00:16:53,930 --> 00:16:59,420
the columns and all the rows one of the

00:16:57,470 --> 00:17:01,400
things that people often worry about is

00:16:59,420 --> 00:17:03,350
whether you can assign different workers

00:17:01,400 --> 00:17:04,640
to work on different pieces fortunately

00:17:03,350 --> 00:17:07,750
all the format's are good with that

00:17:04,640 --> 00:17:12,030
except for Jason when it's compressed

00:17:07,750 --> 00:17:16,050
and with spark

00:17:12,030 --> 00:17:18,000
one of the characteristics that wasn't

00:17:16,050 --> 00:17:21,810
obvious until we started hitting it is

00:17:18,000 --> 00:17:26,370
that when you're reading data through

00:17:21,810 --> 00:17:28,290
the file format interface spark will use

00:17:26,370 --> 00:17:30,570
columnar batch which is a faster

00:17:28,290 --> 00:17:33,000
internal representation if all of your

00:17:30,570 --> 00:17:37,220
types are primitive types so of ours

00:17:33,000 --> 00:17:37,220
only taxi fits into that category

00:17:37,370 --> 00:17:44,550
okay so first notice that I've put it on

00:17:42,660 --> 00:17:47,700
a logarithmic scale so each of those

00:17:44,550 --> 00:17:51,450
lines means it's twice as fast as the

00:17:47,700 --> 00:17:54,960
one above it and these are seconds as

00:17:51,450 --> 00:17:58,110
you did the read so you can see that the

00:17:54,960 --> 00:18:00,990
lowest one is Park a park a is a little

00:17:58,110 --> 00:18:04,730
faster orc is next and then Jason is

00:18:00,990 --> 00:18:08,100
really slow like really painfully slow

00:18:04,730 --> 00:18:12,420
he's it's 256 seconds instead of 8

00:18:08,100 --> 00:18:17,730
seconds so really painfully slow that's

00:18:12,420 --> 00:18:20,820
why you don't use Jason bad Jason Jason

00:18:17,730 --> 00:18:23,010
slow of course because it needs to do a

00:18:20,820 --> 00:18:25,470
lot of string parsing and park' is the

00:18:23,010 --> 00:18:27,840
fastest I suspect although I haven't

00:18:25,470 --> 00:18:29,940
verified it yet that it's because the

00:18:27,840 --> 00:18:31,650
even the faster orc reader which is in

00:18:29,940 --> 00:18:33,540
fact much faster than the old one is

00:18:31,650 --> 00:18:35,010
still going through an extra level of

00:18:33,540 --> 00:18:37,410
indirection it's going through the

00:18:35,010 --> 00:18:39,510
vectorized row batch to the or extract

00:18:37,410 --> 00:18:41,190
and then to call an or batch we've

00:18:39,510 --> 00:18:44,460
written some code that gets rid of that

00:18:41,190 --> 00:18:46,320
so it'll go straight from the vectorized

00:18:44,460 --> 00:18:49,350
row back to the columnar batch which

00:18:46,320 --> 00:18:51,060
will make it much much faster we just

00:18:49,350 --> 00:18:56,910
need to get that code committed and then

00:18:51,060 --> 00:19:00,300
released now sales we got a little bit

00:18:56,910 --> 00:19:02,160
different picture so here orc did really

00:19:00,300 --> 00:19:04,470
well partially because it's smaller and

00:19:02,160 --> 00:19:08,850
partially because the data is better

00:19:04,470 --> 00:19:11,070
suited for what work is fast at park' is

00:19:08,850 --> 00:19:13,910
a little bit slower and jason is still

00:19:11,070 --> 00:19:13,910
the slowpoke

00:19:14,760 --> 00:19:20,280
so here the read performance is

00:19:17,910 --> 00:19:22,110
dominated by the format it makes the

00:19:20,280 --> 00:19:24,750
most difference which format you've

00:19:22,110 --> 00:19:27,960
encoded and less about

00:19:24,750 --> 00:19:32,730
the particular compression picked okay

00:19:27,960 --> 00:19:36,150
now github times here actually Jason had

00:19:32,730 --> 00:19:38,789
its first and only when you can see that

00:19:36,150 --> 00:19:42,929
the fastest was Jason github now granted

00:19:38,789 --> 00:19:46,260
it's huge but it's still fast work is

00:19:42,929 --> 00:19:50,240
next in parque is is worse now when I

00:19:46,260 --> 00:19:53,850
talked to the Twitter guys the guys who

00:19:50,240 --> 00:19:55,770
started Parque they said oh yeah don't

00:19:53,850 --> 00:19:58,620
use Parque for that case I was like

00:19:55,770 --> 00:20:02,010
really that's the use case that it was

00:19:58,620 --> 00:20:05,070
that was dremel was made for but yeah

00:20:02,010 --> 00:20:07,640
they say don't use it like that now one

00:20:05,070 --> 00:20:10,590
of the other characteristics is that

00:20:07,640 --> 00:20:12,270
because there are so many columns the

00:20:10,590 --> 00:20:17,309
stripes are actually ending up pretty

00:20:12,270 --> 00:20:19,110
small and so if you're writing your

00:20:17,309 --> 00:20:22,880
application you really should configure

00:20:19,110 --> 00:20:29,640
a bigger stripe size for these very wide

00:20:22,880 --> 00:20:33,780
files you'll do much better now we're

00:20:29,640 --> 00:20:35,970
gonna add something in work to say to

00:20:33,780 --> 00:20:38,880
define the minimum number of rows per

00:20:35,970 --> 00:20:41,850
stripe because if you have too few rows

00:20:38,880 --> 00:20:49,140
then the the optimizations don't work

00:20:41,850 --> 00:20:50,640
well alright so the next use case is for

00:20:49,140 --> 00:20:53,190
column projection often when you're

00:20:50,640 --> 00:20:54,690
running your query or writing your

00:20:53,190 --> 00:20:57,120
program you only need a few of the

00:20:54,690 --> 00:20:59,909
columns so it's not that uncommon to

00:20:57,120 --> 00:21:03,179
have 100 columns and you just need two

00:20:59,909 --> 00:21:04,830
or three or five of them and so this is

00:21:03,179 --> 00:21:08,309
where the columnar formats actually

00:21:04,830 --> 00:21:10,620
change you can actually just read and

00:21:08,309 --> 00:21:12,900
decompress the bytes you need for those

00:21:10,620 --> 00:21:16,679
columns and you don't need to read the

00:21:12,900 --> 00:21:18,330
rest and spark-gap file format makes it

00:21:16,679 --> 00:21:21,330
really easy you get to pass in your

00:21:18,330 --> 00:21:25,679
desired schema and the file format is

00:21:21,330 --> 00:21:29,039
required to process that and figure out

00:21:25,679 --> 00:21:32,070
which columns it needs and not read the

00:21:29,039 --> 00:21:34,110
rest Jason and Avro obviously do that

00:21:32,070 --> 00:21:37,049
read the read first and then just drop

00:21:34,110 --> 00:21:38,840
the columns or can parque don't bother

00:21:37,049 --> 00:21:43,260
reading the data

00:21:38,840 --> 00:21:47,309
because it's faster okay so this is the

00:21:43,260 --> 00:21:50,159
percentage of the data that or can park

00:21:47,309 --> 00:21:53,399
a red in the different cases for the

00:21:50,159 --> 00:21:57,539
different you sets so I sorted by which

00:21:53,399 --> 00:21:59,610
use case and then the format and then

00:21:57,539 --> 00:22:02,039
the compression so you can see for

00:21:59,610 --> 00:22:04,679
github ork was reading about four

00:22:02,039 --> 00:22:06,240
percent typically and parquet was a

00:22:04,679 --> 00:22:08,220
little bit higher but about the same

00:22:06,240 --> 00:22:10,169
this is still a logarithmic scale so

00:22:08,220 --> 00:22:12,840
it's doubling oh no this is the one

00:22:10,169 --> 00:22:16,230
that's not logarithmic this is just the

00:22:12,840 --> 00:22:20,429
percentage for sales is somewhere

00:22:16,230 --> 00:22:24,960
between 3% and 4% parquet was a little

00:22:20,429 --> 00:22:27,809
bit bigger up at 8% and then for the

00:22:24,960 --> 00:22:29,549
taxi data set Parque went crazy and

00:22:27,809 --> 00:22:32,490
somehow read twenty percent of the data

00:22:29,549 --> 00:22:34,440
and for the two columns that I asked for

00:22:32,490 --> 00:22:35,580
and the benchmark and so I'm not quite

00:22:34,440 --> 00:22:40,740
sure what's up with that

00:22:35,580 --> 00:22:42,419
but it's repeatable it so you can see

00:22:40,740 --> 00:22:44,100
that obviously if something was a

00:22:42,419 --> 00:22:47,149
hundred percent then you don't have

00:22:44,100 --> 00:22:49,950
column projections so if you put Avro or

00:22:47,149 --> 00:22:52,710
Jason on here it clearly be at a hundred

00:22:49,950 --> 00:22:54,299
for all of these so all of them are much

00:22:52,710 --> 00:22:57,960
much better on the x come down

00:22:54,299 --> 00:23:01,399
correspondingly now predicate pushdown

00:22:57,960 --> 00:23:03,389
predicate push down is a nice

00:23:01,399 --> 00:23:07,380
characteristic of the more advanced

00:23:03,389 --> 00:23:11,850
readers when you're querying for example

00:23:07,380 --> 00:23:13,500
when you just want the names for

00:23:11,850 --> 00:23:17,039
employees that were hired between a

00:23:13,500 --> 00:23:20,159
given set of dates you express your

00:23:17,039 --> 00:23:23,220
sequel query like that the reader gets

00:23:20,159 --> 00:23:26,580
passed down oh I wanna hire date between

00:23:23,220 --> 00:23:31,220
those two dates and given its given to

00:23:26,580 --> 00:23:35,970
the file format during via filters and

00:23:31,220 --> 00:23:37,529
this is primarily useful on sorted

00:23:35,970 --> 00:23:40,380
columns we'll talk a little bit more

00:23:37,529 --> 00:23:44,940
about that on the next slide so orc and

00:23:40,380 --> 00:23:48,450
Parque index their rows with min and Max

00:23:44,940 --> 00:23:50,940
values for a whole set of ranges so

00:23:48,450 --> 00:23:52,200
sorted data means that it can use those

00:23:50,940 --> 00:23:53,940
men and maxes to

00:23:52,200 --> 00:24:00,809
say okay I don't need to read those rows

00:23:53,940 --> 00:24:02,820
I won't bother parsing it at all the now

00:24:00,809 --> 00:24:06,019
if you don't have sorted data for

00:24:02,820 --> 00:24:08,960
example I've got one customer that has

00:24:06,019 --> 00:24:11,970
wants to do queries on customer names

00:24:08,960 --> 00:24:13,710
customer names can't really be the sort

00:24:11,970 --> 00:24:17,970
key because they need to sort by time

00:24:13,710 --> 00:24:21,389
and so they need to query on customer

00:24:17,970 --> 00:24:24,360
names all the time but it it can't be

00:24:21,389 --> 00:24:28,139
the sort key so for them we've made

00:24:24,360 --> 00:24:31,700
bloom filters and so when you set up a

00:24:28,139 --> 00:24:35,730
bloom filter it records a probabilistic

00:24:31,700 --> 00:24:39,630
answer to whether specific values are in

00:24:35,730 --> 00:24:43,080
this set of rows and so they do take

00:24:39,630 --> 00:24:44,820
more space but they let you answer the

00:24:43,080 --> 00:24:49,350
question of okay do I need to look at

00:24:44,820 --> 00:24:53,850
this set of rows at all and only or

00:24:49,350 --> 00:24:55,679
caste that the readers for a predicate

00:24:53,850 --> 00:24:57,690
pushdown can either filter out the

00:24:55,679 --> 00:25:01,289
entire file the stripe the section of

00:24:57,690 --> 00:25:04,470
the file that you work on or in orc you

00:25:01,289 --> 00:25:07,649
can also get row groups down with 10,000

00:25:04,470 --> 00:25:11,159
rows now after this after you get the

00:25:07,649 --> 00:25:14,789
results back you still need to have the

00:25:11,159 --> 00:25:17,010
engine check the filter at the row by

00:25:14,789 --> 00:25:19,350
row level this will just say yes you

00:25:17,010 --> 00:25:20,610
need to read these 10,000 rows but there

00:25:19,350 --> 00:25:24,899
may be stuff in there that you don't

00:25:20,610 --> 00:25:28,260
want so you still need to check it all

00:25:24,899 --> 00:25:31,070
right now when we did this obviously we

00:25:28,260 --> 00:25:35,309
only did with or can't park a the dark

00:25:31,070 --> 00:25:38,519
wine is the total number of records the

00:25:35,309 --> 00:25:40,110
blue line is how many records Parque red

00:25:38,519 --> 00:25:43,950
and the green one is how many orc red

00:25:40,110 --> 00:25:48,149
and notice those lines are logarithmic

00:25:43,950 --> 00:25:52,409
again so in this case you've got 10,000

00:25:48,149 --> 00:25:57,360
for the taxi data that orc red versus

00:25:52,409 --> 00:26:00,690
the 22 million that that Park a red now

00:25:57,360 --> 00:26:02,490
what's going on here is that the Parque

00:26:00,690 --> 00:26:05,550
wieder decided it didn't know how to

00:26:02,490 --> 00:26:08,640
deal with timestamps so in

00:26:05,550 --> 00:26:10,770
of the predicate push down and so it

00:26:08,640 --> 00:26:15,630
silently said okay I'm just going to

00:26:10,770 --> 00:26:17,190
read everything and and did actually

00:26:15,630 --> 00:26:20,940
that's also what happened on the github

00:26:17,190 --> 00:26:23,700
side is it read all the records now

00:26:20,940 --> 00:26:24,960
sales it had an integer so the predicate

00:26:23,700 --> 00:26:28,320
push down actually worked

00:26:24,960 --> 00:26:30,690
but because orc has the indexes at the

00:26:28,320 --> 00:26:32,960
ten thousand row level it was able to

00:26:30,690 --> 00:26:36,270
read just ten thousand rows

00:26:32,960 --> 00:26:39,870
Parkay ended up reading sixty times that

00:26:36,270 --> 00:26:43,110
which is the size of its stripe and of

00:26:39,870 --> 00:26:46,140
course that that's still a hundred times

00:26:43,110 --> 00:26:48,840
better than the the total number of rows

00:26:46,140 --> 00:26:49,950
so you can see why this predicate push

00:26:48,840 --> 00:26:54,840
down is a big deal

00:26:49,950 --> 00:26:55,580
a few years ago when Yahoo was still in

00:26:54,840 --> 00:26:59,400
existence

00:26:55,580 --> 00:27:02,810
instead of being oh so now they were

00:26:59,400 --> 00:27:06,390
benchmarking hive against SPARC and they

00:27:02,810 --> 00:27:08,010
were running their queries and there was

00:27:06,390 --> 00:27:09,600
one query that was coming back really

00:27:08,010 --> 00:27:11,790
fast on hive they thought it was broken

00:27:09,600 --> 00:27:13,830
but it turned out that part of what was

00:27:11,790 --> 00:27:17,310
going on was exactly doing this

00:27:13,830 --> 00:27:22,110
predicate push down and so the hive

00:27:17,310 --> 00:27:24,000
running out of HDFS just needed one file

00:27:22,110 --> 00:27:25,800
in one section of one file so I was

00:27:24,000 --> 00:27:28,490
reading basically ten thousand rows and

00:27:25,800 --> 00:27:31,230
then it had its answer

00:27:28,490 --> 00:27:32,940
SPARC even once it was a memory had a

00:27:31,230 --> 00:27:35,220
hundred terabytes in memory so it took a

00:27:32,940 --> 00:27:37,500
lot of X executors looking through all

00:27:35,220 --> 00:27:42,570
of its memory to figure out oh okay

00:27:37,500 --> 00:27:44,610
here's the one row I need so so hive was

00:27:42,570 --> 00:27:47,250
much much faster than spark even once

00:27:44,610 --> 00:27:50,670
SPARC had everything cached in memory of

00:27:47,250 --> 00:27:56,700
course now with el ap el ap is the new

00:27:50,670 --> 00:27:59,520
execution engine for hive that caches

00:27:56,700 --> 00:28:01,440
data aggressively and so it would be

00:27:59,520 --> 00:28:05,550
much much faster actually we've had

00:28:01,440 --> 00:28:07,770
queries run against el ap where you're

00:28:05,550 --> 00:28:11,750
testing against a table with six billion

00:28:07,770 --> 00:28:14,529
rows to come back in less than a second

00:28:11,750 --> 00:28:18,889
really amazing actually

00:28:14,529 --> 00:28:24,409
okay so as I said parquet doesn't push

00:28:18,889 --> 00:28:26,149
down timestamp filters spark defaults or

00:28:24,409 --> 00:28:30,859
the predicate push down off you need to

00:28:26,149 --> 00:28:33,649
turn that on the small orc stripes or

00:28:30,859 --> 00:28:38,769
github meant that we ended up with a

00:28:33,649 --> 00:28:41,779
small read of less than 10,000 rows and

00:28:38,769 --> 00:28:43,849
because it's an optimization the file

00:28:41,779 --> 00:28:48,469
formats aren't very good about telling

00:28:43,849 --> 00:28:50,179
you when it's been turned off that's

00:28:48,469 --> 00:28:52,729
something we should actually get better

00:28:50,179 --> 00:28:55,009
at it took actually looking up the data

00:28:52,729 --> 00:28:57,349
of how much data had been read before I

00:28:55,009 --> 00:29:00,200
was sure okay the predicate push out

00:28:57,349 --> 00:29:02,479
actually happened and then you start

00:29:00,200 --> 00:29:06,289
looking for cases about why it didn't

00:29:02,479 --> 00:29:08,539
happen the benchmark actually uses a

00:29:06,289 --> 00:29:12,379
tracking file system so that it can see

00:29:08,539 --> 00:29:15,229
how many bytes got read or written okay

00:29:12,379 --> 00:29:18,440
another advantage of Orkin parquet is

00:29:15,229 --> 00:29:21,259
that they store some additional metadata

00:29:18,440 --> 00:29:23,899
in the file footer so they have the file

00:29:21,259 --> 00:29:26,269
schema but they also have the number of

00:29:23,899 --> 00:29:28,309
records in the min/max and count of each

00:29:26,269 --> 00:29:30,859
column and so if you need any of that

00:29:28,309 --> 00:29:37,759
it's actually pretty easy to get it in

00:29:30,859 --> 00:29:39,259
one access okay one of the most

00:29:37,759 --> 00:29:42,349
important things here is everything

00:29:39,259 --> 00:29:44,539
changes right open source systems are

00:29:42,349 --> 00:29:47,149
continually evolving right the

00:29:44,539 --> 00:29:49,219
environments change and so things are

00:29:47,149 --> 00:29:52,999
constantly changing right if you did a

00:29:49,219 --> 00:29:55,339
similar benchmark a couple of releases

00:29:52,999 --> 00:29:58,129
ago for spark it would have been a very

00:29:55,339 --> 00:30:01,359
different experience and so you actually

00:29:58,129 --> 00:30:01,359
need to take that into account

00:30:02,409 --> 00:30:07,339
the benchmarks will change but only when

00:30:05,359 --> 00:30:10,190
people come up with suggestions for what

00:30:07,339 --> 00:30:11,659
to do better the other thing is to

00:30:10,190 --> 00:30:13,700
really evaluate your needs

00:30:11,659 --> 00:30:16,369
most people really need column

00:30:13,700 --> 00:30:19,009
projection and predicate push down is a

00:30:16,369 --> 00:30:21,169
nice to have but you definitely want to

00:30:19,009 --> 00:30:23,809
be in either work or parquet most of the

00:30:21,169 --> 00:30:27,140
time you want to determine how to sort

00:30:23,809 --> 00:30:29,420
your data we had some customers that

00:30:27,140 --> 00:30:32,420
trying to partition their data by both

00:30:29,420 --> 00:30:34,760
country and their product and that made

00:30:32,420 --> 00:30:37,429
it easy to query you could filter out

00:30:34,760 --> 00:30:38,960
things but it really meant that you

00:30:37,429 --> 00:30:41,360
ended up with some little teeny

00:30:38,960 --> 00:30:45,170
partitions right if you look at how many

00:30:41,360 --> 00:30:47,720
people in Guatemala want to use Product

00:30:45,170 --> 00:30:49,309
X that's a really small number compared

00:30:47,720 --> 00:30:53,150
to like the number in Germany for

00:30:49,309 --> 00:30:54,950
example that would be much larger for

00:30:53,150 --> 00:30:57,170
them it's much better to sort the data

00:30:54,950 --> 00:31:00,679
and then let predicates push down to

00:30:57,170 --> 00:31:02,299
work the data and then considering

00:31:00,679 --> 00:31:04,549
balloon filters is another really good

00:31:02,299 --> 00:31:05,929
case if you have equality bloom filters

00:31:04,549 --> 00:31:06,559
don't work for a less-than or

00:31:05,929 --> 00:31:08,690
greater-than

00:31:06,559 --> 00:31:10,340
they only work for strict equality so

00:31:08,690 --> 00:31:12,740
but if you have a lot of equality test

00:31:10,340 --> 00:31:17,830
where you're looking for point lookups

00:31:12,740 --> 00:31:29,179
they're really useful okay questions

00:31:17,830 --> 00:31:36,049
just think the speak up first oh come on

00:31:29,179 --> 00:31:38,840
someone have a question there's one when

00:31:36,049 --> 00:31:41,200
you read the data in SPARC you actually

00:31:38,840 --> 00:31:44,270
need to do an action in order to

00:31:41,200 --> 00:31:47,210
something happen so how do you separate

00:31:44,270 --> 00:31:49,580
the action time from the actual read

00:31:47,210 --> 00:31:51,590
time so that's part of I was using the

00:31:49,580 --> 00:31:54,710
the file format or rather the benchmarks

00:31:51,590 --> 00:31:56,840
are using the file format interface so

00:31:54,710 --> 00:31:59,960
that it doesn't launch tasks to read it

00:31:56,840 --> 00:32:01,970
so it wasn't just the declaring the task

00:31:59,960 --> 00:32:04,910
it was actually accessing the file

00:32:01,970 --> 00:32:06,559
format directly basically the point was

00:32:04,910 --> 00:32:10,070
to make micro benchmarks that would

00:32:06,559 --> 00:32:13,960
exactly isolate out the reading from the

00:32:10,070 --> 00:32:13,960
processing that would happen downstream

00:32:17,290 --> 00:32:25,940
give time you said that you don't really

00:32:23,090 --> 00:32:30,740
know what that okay is going crazy on

00:32:25,940 --> 00:32:33,919
this any explanation what is different

00:32:30,740 --> 00:32:38,870
than the data recommendation when to not

00:32:33,919 --> 00:32:40,669
use pocket okay so okay technically what

00:32:38,870 --> 00:32:41,030
I said Parque was going crazy it was

00:32:40,669 --> 00:32:42,470
just

00:32:41,030 --> 00:32:44,720
the predicate push them and that wasn't

00:32:42,470 --> 00:32:49,340
on the github data that was on the the

00:32:44,720 --> 00:32:51,050
sales data I think wasn't it actually

00:32:49,340 --> 00:33:01,040
that was the Cullen projection one right

00:32:51,050 --> 00:33:04,900
when we back up now the okay there

00:33:01,040 --> 00:33:04,900
should be a faster way to do this sorry

00:33:04,960 --> 00:33:10,310
yeah that was actually the taxi data

00:33:07,340 --> 00:33:13,690
where the column projection on the taxi

00:33:10,310 --> 00:33:16,490
data now the the Twitter guys and

00:33:13,690 --> 00:33:18,950
actually the results here also pointed

00:33:16,490 --> 00:33:21,170
out that for highly complex data for the

00:33:18,950 --> 00:33:26,210
github case you don't want to use Parque

00:33:21,170 --> 00:33:29,930
and when I looked at it it mostly seems

00:33:26,210 --> 00:33:31,370
to be about out memory allocation it's

00:33:29,930 --> 00:33:34,010
basically creating a lot of temporary

00:33:31,370 --> 00:33:40,880
objects and then throwing them away so

00:33:34,010 --> 00:33:43,300
it's great GC thrash there was a

00:33:40,880 --> 00:33:43,300
question here

00:33:46,499 --> 00:33:50,769
so I guess best in your presentation

00:33:49,029 --> 00:33:52,840
actually when I have a specific use case

00:33:50,769 --> 00:33:55,179
it will be really hard to look in the

00:33:52,840 --> 00:33:56,799
Internet advantages disadvantages of

00:33:55,179 --> 00:33:58,869
each of the formats the better way is

00:33:56,799 --> 00:34:01,409
just to take a sample of data set and

00:33:58,869 --> 00:34:04,749
use a benchmark do you have any

00:34:01,409 --> 00:34:08,079
inundation steeps so how I decided is

00:34:04,749 --> 00:34:10,000
not how to do proper benchmark okay well

00:34:08,079 --> 00:34:14,529
first of all I haven't pushed this code

00:34:10,000 --> 00:34:16,179
up yet but I will shortly the earlier

00:34:14,529 --> 00:34:18,579
benchmarking code is already in the orc

00:34:16,179 --> 00:34:23,679
project so you can actually download the

00:34:18,579 --> 00:34:25,299
github and it lets you down it has a

00:34:23,679 --> 00:34:27,639
script to download the data that you

00:34:25,299 --> 00:34:30,309
need or you can put your own data in and

00:34:27,639 --> 00:34:33,099
then process it you just need to fill in

00:34:30,309 --> 00:34:36,149
the benchmarking code the benchmarking

00:34:33,099 --> 00:34:39,190
code uses JM h which is java micro

00:34:36,149 --> 00:34:42,700
benchmark harness which is integrated

00:34:39,190 --> 00:34:47,669
into open JDK and does a really nice job

00:34:42,700 --> 00:34:50,409
of both letting you specify variants and

00:34:47,669 --> 00:34:51,789
measure the execution time and other

00:34:50,409 --> 00:34:53,829
attributes you're interested in

00:34:51,789 --> 00:34:57,309
so absolutely download the benchmarks

00:34:53,829 --> 00:35:01,180
put your real data and obviously for

00:34:57,309 --> 00:35:03,880
open talks and for the public benchmarks

00:35:01,180 --> 00:35:06,670
I can only use data that's publicly

00:35:03,880 --> 00:35:08,470
available right obviously the the

00:35:06,670 --> 00:35:13,240
customers that gave me this get the

00:35:08,470 --> 00:35:14,980
schema for the the sales table wouldn't

00:35:13,240 --> 00:35:17,470
even let me use the real column names I

00:35:14,980 --> 00:35:18,549
basically had to completely change the

00:35:17,470 --> 00:35:20,410
call names because they were that

00:35:18,549 --> 00:35:24,130
worried about anything leaking out about

00:35:20,410 --> 00:35:26,079
what they're doing but ya know it'd be

00:35:24,130 --> 00:35:31,690
relatively straightforward to plug it in

00:35:26,079 --> 00:35:35,410
and use your real data so absolutely so

00:35:31,690 --> 00:35:37,390
hi thank you I will be interested you

00:35:35,410 --> 00:35:39,460
talked about customers a lot I'm about

00:35:37,390 --> 00:35:42,579
the amount of adoption of column uniform

00:35:39,460 --> 00:35:44,079
odds like or cos mm-hm

00:35:42,579 --> 00:35:48,609
because I got the feeling that with the

00:35:44,079 --> 00:35:50,650
direction of the kafka Evaro marriage I

00:35:48,609 --> 00:35:53,200
would say that a lot of people just dump

00:35:50,650 --> 00:35:55,270
their every records into a tree because

00:35:53,200 --> 00:35:57,460
again and if you have a view from the

00:35:55,270 --> 00:35:58,910
market what's how often you see people

00:35:57,460 --> 00:36:01,609
really using something like

00:35:58,910 --> 00:36:04,460
Co actually mostly we do because

00:36:01,609 --> 00:36:07,339
they stream through it often they use

00:36:04,460 --> 00:36:10,280
Avro like you said to put it through

00:36:07,339 --> 00:36:12,109
Kafka but then once they land in hdfs

00:36:10,280 --> 00:36:13,329
then they want to put it into a columnar

00:36:12,109 --> 00:36:15,950
format because the performance

00:36:13,329 --> 00:36:19,609
advantages are really really high and

00:36:15,950 --> 00:36:22,520
you write it into HDFS once and then you

00:36:19,609 --> 00:36:26,440
read it a lot and so usually most

00:36:22,520 --> 00:36:30,260
customers find that yeah streams through

00:36:26,440 --> 00:36:33,170
their Kafka as as Avro but then once it

00:36:30,260 --> 00:36:37,059
lands in HDFS they they translate it

00:36:33,170 --> 00:36:37,059
over thank you

00:36:51,680 --> 00:36:59,610
thank you I would like to ask about our

00:36:55,890 --> 00:37:01,920
C version you is there a timeline for

00:36:59,610 --> 00:37:10,320
finalizing everyone what can we expect

00:37:01,920 --> 00:37:13,770
from it um so trying to be realistic

00:37:10,320 --> 00:37:16,410
it'll probably be out later this year we

00:37:13,770 --> 00:37:21,350
have it's still relatively early in the

00:37:16,410 --> 00:37:25,230
design at this point we're figuring out

00:37:21,350 --> 00:37:30,030
what needs to be there and what we want

00:37:25,230 --> 00:37:34,230
to change in the format so effectively

00:37:30,030 --> 00:37:36,630
ork had to format two versions of the

00:37:34,230 --> 00:37:38,880
format the first one that was in the

00:37:36,630 --> 00:37:40,830
very first release of hive that ork was

00:37:38,880 --> 00:37:44,010
in which was hive 0:11

00:37:40,830 --> 00:37:46,740
then in the very next release 0:12 we

00:37:44,010 --> 00:37:52,740
came out with what we now call version 1

00:37:46,740 --> 00:37:54,450
and version 2 is has been in the works

00:37:52,740 --> 00:37:55,980
we know a couple pieces that we

00:37:54,450 --> 00:37:59,160
absolutely need to fix we need to make

00:37:55,980 --> 00:38:01,710
time stamps better we screwed up the

00:37:59,160 --> 00:38:04,830
time zones oh my god the person who

00:38:01,710 --> 00:38:06,990
decided to ever put any time zone

00:38:04,830 --> 00:38:11,420
information and was was a really bad

00:38:06,990 --> 00:38:14,910
plan but I wish we could just use like

00:38:11,420 --> 00:38:16,740
standard UTC all the time but at least

00:38:14,910 --> 00:38:20,820
for programming that would make life

00:38:16,740 --> 00:38:24,300
much much better but yeah I'd say by the

00:38:20,820 --> 00:38:29,330
end of the year we should have at least

00:38:24,300 --> 00:38:29,330
out in alpha thank you

00:38:37,130 --> 00:38:44,970
let's take one more last question so for

00:38:42,240 --> 00:38:47,820
data with a complex schema

00:38:44,970 --> 00:38:50,670
like for instance github in a machine

00:38:47,820 --> 00:38:52,650
learning application where most of the

00:38:50,670 --> 00:38:55,830
columns would be used anyway for feature

00:38:52,650 --> 00:38:59,930
extraction do you still see an advantage

00:38:55,830 --> 00:39:02,460
to use columnar formats like marquee or

00:38:59,930 --> 00:39:07,610
if you're really just going to use it

00:39:02,460 --> 00:39:13,860
all the time and it's highly structured

00:39:07,610 --> 00:39:16,560
Avro does work pretty well so with the

00:39:13,860 --> 00:39:19,980
with the restriction that for spark the

00:39:16,560 --> 00:39:22,020
Avro adapter I'm not very happy I'm

00:39:19,980 --> 00:39:24,690
about ready to file a bug report on the

00:39:22,020 --> 00:39:29,220
Avro descriptor saying what the hell but

00:39:24,690 --> 00:39:32,630
that but that aside Avro does work very

00:39:29,220 --> 00:39:36,240
well for the highly structured case

00:39:32,630 --> 00:39:39,180
where you don't need to do column

00:39:36,240 --> 00:39:40,980
projection but the columnar ones are

00:39:39,180 --> 00:39:44,850
still better than Jason even in this

00:39:40,980 --> 00:39:47,220
case yes basically Jason is always

00:39:44,850 --> 00:39:49,320
really slow is really what it boils down

00:39:47,220 --> 00:39:51,630
to and if you've ever tried to actually

00:39:49,320 --> 00:39:56,370
look through the details of a JSON

00:39:51,630 --> 00:39:57,570
parser you'd understand why thank you

00:39:56,370 --> 00:40:04,519
very much

00:39:57,570 --> 00:40:04,519

YouTube URL: https://www.youtube.com/watch?v=aIcxFIyL6xo


