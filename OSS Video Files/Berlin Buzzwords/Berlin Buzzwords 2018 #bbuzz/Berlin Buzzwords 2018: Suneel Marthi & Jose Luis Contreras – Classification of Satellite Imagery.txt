Title: Berlin Buzzwords 2018: Suneel Marthi & Jose Luis Contreras – Classification of Satellite Imagery
Publication date: 2018-06-20
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Suneel Marthi and Jose Luis Contreras talking about "Large Scale Landuse Classification of Satellite Imagery".

With the abundance of Remote Sensing satellite imagery, the possibilities are endless as to the kind of insights that can be derived from them. One such use is to determine land use for agriculture and non-agricultural purposes.

In this talk, we’ll be looking at leveraging Sentinel-2 satellite imagery data along with OpenStreetMap labels to be able to classify land use as agricultural or non-agricultural. Sentinel-2 data has a 10-meter resolution in RGB bands and is well-suited for land use classification. Using these two datasets, many different machine learning tasks can be performed like - image segmentation into two classes (farm land and non-farm land) or more challenging task of identification of crop type being cultivated on fields.  

For this talk, we’ll be looking at leveraging Convolutional Neural Networks (CNNs) built with Apache MXNet to train Deep Learning models for land use classification. We’ll be covering the different Deep Learning Architectures considered for this particular use case along with the performance metrics for each of the different architectures.

We’ll be leveraging streaming pipelines built on Apache Flink for model training and inference. Developers will come away with a better understanding of how to analyze satellite imagery and the different Deep Learning architectures along with their pros/cons when analyzing satellite imagery for land use.

Read more:
https://2018.berlinbuzzwords.de/18/session/large-scale-landuse-classification-satellite-imagery

About Suneel Marthi:
https://2018.berlinbuzzwords.de/users/suneel-marthi

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:04,779 --> 00:00:10,070
hi everyone so I am vanilla Marty and

00:00:08,360 --> 00:00:12,469
this is Jose Luis Contreras he'll be my

00:00:10,070 --> 00:00:13,940
co-speaker today attainder today we'll

00:00:12,469 --> 00:00:16,789
be talking about satellite image

00:00:13,940 --> 00:00:18,350
classification for land use so how many

00:00:16,789 --> 00:00:20,600
of you here have to deal with satellite

00:00:18,350 --> 00:00:24,550
images or you have a use case something

00:00:20,600 --> 00:00:26,990
like that okay this one and two okay

00:00:24,550 --> 00:00:28,550
so the agenda today is we'll have a

00:00:26,990 --> 00:00:32,270
brief introduction of the data set and

00:00:28,550 --> 00:00:34,039
the image description and satellite

00:00:32,270 --> 00:00:36,440
images yeah you have cloud covers so how

00:00:34,039 --> 00:00:38,000
do you classify cloud cover images and

00:00:36,440 --> 00:00:40,220
segmentation we will be talking about

00:00:38,000 --> 00:00:41,810
semantic segmentation here and the whole

00:00:40,220 --> 00:00:42,890
thing once you train your models the

00:00:41,810 --> 00:00:44,540
whole thing would be put in a streaming

00:00:42,890 --> 00:00:46,880
pipeline and we had used Apache beam for

00:00:44,540 --> 00:00:52,100
that a small demo and future work coming

00:00:46,880 --> 00:00:53,870
up so the goal here is to identify tulip

00:00:52,100 --> 00:00:55,760
fields from Sentinel to satellite images

00:00:53,870 --> 00:00:58,040
so this is an example if you look at

00:00:55,760 --> 00:00:59,420
this image here yeah this may be a tulip

00:00:58,040 --> 00:01:01,399
field and this may be it really feel to

00:00:59,420 --> 00:01:04,390
this band here so that's what you're

00:01:01,399 --> 00:01:07,030
trying to identify from satellite images

00:01:04,390 --> 00:01:09,289
so this is the typical workflow

00:01:07,030 --> 00:01:10,909
basically you download your images you

00:01:09,289 --> 00:01:12,979
filter it for cloud you figure out the

00:01:10,909 --> 00:01:16,810
images that have cloud cover and then

00:01:12,979 --> 00:01:16,810
you segment your images for Philippines

00:01:29,270 --> 00:01:32,900
[Music]

00:01:32,940 --> 00:01:38,890
okay okay so before we go into the

00:01:37,090 --> 00:01:45,510
detail slide deck let me just run

00:01:38,890 --> 00:01:50,890
through a quick notebook here yeah so

00:01:45,510 --> 00:01:53,520
let me run through this notebook so now

00:01:50,890 --> 00:01:55,510
we'll be doing the data acquisition and

00:01:53,520 --> 00:01:56,800
we're in there actually downloading the

00:01:55,510 --> 00:01:59,710
satellite images Sentinel 2 from

00:01:56,800 --> 00:02:00,729
Sentinel to satellite and we have

00:01:59,710 --> 00:02:02,800
something called a web mapping service

00:02:00,729 --> 00:02:04,210
that's what WMS is for the Sentinel 2

00:02:02,800 --> 00:02:06,960
images will be downloading the data

00:02:04,210 --> 00:02:10,119
using the web mapping service instance

00:02:06,960 --> 00:02:12,610
and so let's do that

00:02:10,119 --> 00:02:14,590
once we do that we overlay the images

00:02:12,610 --> 00:02:19,600
with the tulip fields what the tulip

00:02:14,590 --> 00:02:21,430
fields are so so we are using a package

00:02:19,600 --> 00:02:27,070
called I buy leaflet for overlaying the

00:02:21,430 --> 00:02:28,900
image with the different layers so let's

00:02:27,070 --> 00:02:30,670
look at an interactive display so for

00:02:28,900 --> 00:02:32,260
this particular demo we're actually

00:02:30,670 --> 00:02:34,030
taking a map of Den Helder in

00:02:32,260 --> 00:02:37,290
Netherlands a political map and

00:02:34,030 --> 00:02:40,120
overlaying that with a satellite image

00:02:37,290 --> 00:02:43,540
so this is Den Helder in Netherlands

00:02:40,120 --> 00:02:45,190
okay so this is the political map now we

00:02:43,540 --> 00:02:47,200
don't know what is here in this area and

00:02:45,190 --> 00:02:49,890
this area so that's what we get from the

00:02:47,200 --> 00:02:49,890
satellite images

00:02:53,760 --> 00:02:58,590
so this satellite image is coming from

00:02:56,400 --> 00:03:00,959
the web mapping service for central do

00:02:58,590 --> 00:03:02,459
so let's add the layer so once you

00:03:00,959 --> 00:03:04,230
overlay your a political map with a

00:03:02,459 --> 00:03:06,030
satellite image this is how it looks and

00:03:04,230 --> 00:03:07,920
what we are trying to do here is to

00:03:06,030 --> 00:03:11,129
identify the tulip beds which are across

00:03:07,920 --> 00:03:12,810
this image so let's do that in order to

00:03:11,129 --> 00:03:14,069
do that we need some ground truth as to

00:03:12,810 --> 00:03:17,360
what the images is and whether to live

00:03:14,069 --> 00:03:23,849
better and that data is coming from

00:03:17,360 --> 00:03:25,799
signage aside from geo pedia so yeah

00:03:23,849 --> 00:03:27,599
this is what you would see as output so

00:03:25,799 --> 00:03:28,709
that's where the tulip beds are so

00:03:27,599 --> 00:03:30,510
that's the goal of this presentation

00:03:28,709 --> 00:03:33,120
today how do you identify the culet beds

00:03:30,510 --> 00:03:39,000
from satellite images let me go back to

00:03:33,120 --> 00:03:41,760
slide deck ok so from here Jose will

00:03:39,000 --> 00:03:52,109
talk to talk talk us through the process

00:03:41,760 --> 00:03:53,640
for doing this so first of all the kind

00:03:52,109 --> 00:03:56,400
of data we are getting the satellite

00:03:53,640 --> 00:03:58,530
images are coming from Sentinel 2 which

00:03:56,400 --> 00:04:01,349
is a satellite mission from the European

00:03:58,530 --> 00:04:06,060
Space Agency and it has two satellites

00:04:01,349 --> 00:04:10,230
which revisit each each region it's five

00:04:06,060 --> 00:04:13,829
days the data is coming from 13 spectral

00:04:10,230 --> 00:04:17,489
bands ranging from RGB to up to the

00:04:13,829 --> 00:04:20,880
shortwave infrared but we will only be

00:04:17,489 --> 00:04:23,099
using RGB here for different reasons we

00:04:20,880 --> 00:04:26,240
can cover later the spatial resolution

00:04:23,099 --> 00:04:28,860
for this images it's 10 meters per pixel

00:04:26,240 --> 00:04:30,750
for the RGB bands for the other bands

00:04:28,860 --> 00:04:33,349
some of them have 20 meter per pixel

00:04:30,750 --> 00:04:36,060
some of them 60 meters per pixel and

00:04:33,349 --> 00:04:40,610
they follow a free and open data policy

00:04:36,060 --> 00:04:40,610
which makes it pretty convenient for us

00:04:46,039 --> 00:04:51,389
ok so to download the images we use a

00:04:49,650 --> 00:04:53,220
tool which is very similar to the one

00:04:51,389 --> 00:04:55,650
shown it just showed you it was

00:04:53,220 --> 00:04:57,539
developed very material and basically

00:04:55,650 --> 00:05:00,030
what you have to do you select your

00:04:57,539 --> 00:05:02,099
polygon and it downloads for you the

00:05:00,030 --> 00:05:05,789
different tiles with all the satellite

00:05:02,099 --> 00:05:07,139
images these data is in RGB as I

00:05:05,789 --> 00:05:08,849
mentioned before

00:05:07,139 --> 00:05:13,439
and the images we are downloading our

00:05:08,849 --> 00:05:16,699
chips of 20 of 256 by 256 as you can see

00:05:13,439 --> 00:05:19,830
here we many times have traveled with

00:05:16,699 --> 00:05:22,229
lovely images this image is nice but the

00:05:19,830 --> 00:05:25,919
other one is super cloudy so that's a

00:05:22,229 --> 00:05:27,270
problem we have to solve go ahead that's

00:05:25,919 --> 00:05:29,669
a problem we have to solve because

00:05:27,270 --> 00:05:32,550
otherwise there's not much segmentation

00:05:29,669 --> 00:05:33,990
we can do if all we can see is clouds so

00:05:32,550 --> 00:05:37,289
how do we filter these clouds

00:05:33,990 --> 00:05:40,199
we are gonna we are gonna train a neural

00:05:37,289 --> 00:05:43,189
network to filter images as clear or

00:05:40,199 --> 00:05:48,810
cloudy and for that we are going to use

00:05:43,189 --> 00:05:51,930
resonate 50 and resonate 101 my these

00:05:48,810 --> 00:05:55,020
networks because they are one of the

00:05:51,930 --> 00:05:58,589
best networks for computer vision

00:05:55,020 --> 00:06:00,539
problems and also we found out well we

00:05:58,589 --> 00:06:04,770
didn't find out there is an art there is

00:06:00,539 --> 00:06:07,050
some articles that they tend to transfer

00:06:04,770 --> 00:06:09,360
learning better which will be useful in

00:06:07,050 --> 00:06:11,009
this case as we will see later so this

00:06:09,360 --> 00:06:14,219
is the basic building block of our resin

00:06:11,009 --> 00:06:17,069
it here the most interesting part maybe

00:06:14,219 --> 00:06:20,939
is this connection here which is a an

00:06:17,069 --> 00:06:23,520
identity mapping which basically will it

00:06:20,939 --> 00:06:26,279
allows resonance to be deeper than other

00:06:23,520 --> 00:06:29,069
networks so you can have resonate 50

00:06:26,279 --> 00:06:35,310
which is 50 layers resonate 101 with 100

00:06:29,069 --> 00:06:37,469
layers and even deeper networks so one

00:06:35,310 --> 00:06:39,990
problem as always with neural networks

00:06:37,469 --> 00:06:44,069
is how to find training data for this

00:06:39,990 --> 00:06:46,680
network because we could label data

00:06:44,069 --> 00:06:49,349
manually from our data source but it

00:06:46,680 --> 00:06:51,330
would be pretty time-consuming so we

00:06:49,349 --> 00:06:52,830
found this dataset in color which is

00:06:51,330 --> 00:06:54,719
coming from this planet

00:06:52,830 --> 00:06:57,569
understanding the MSM from space

00:06:54,719 --> 00:07:00,270
competition and it contains 40,000

00:06:57,569 --> 00:07:02,819
images which are labeled as clear like

00:07:00,270 --> 00:07:07,830
this one hazy partly cloudy or

00:07:02,819 --> 00:07:10,740
completely cloudy so apart from that

00:07:07,830 --> 00:07:13,909
data we also took 5,000 images from

00:07:10,740 --> 00:07:17,219
Sentinel to download it with our tool

00:07:13,909 --> 00:07:20,069
and we had to hand label them there was

00:07:17,219 --> 00:07:20,940
no other approach so from these images

00:07:20,069 --> 00:07:22,410
we have like T

00:07:20,940 --> 00:07:24,900
the percent of them which are cloudy and

00:07:22,410 --> 00:07:28,380
from the Cal competitions we have like

00:07:24,900 --> 00:07:29,700
thirty percent of them are cloudy I told

00:07:28,380 --> 00:07:32,100
you before that there are four classes

00:07:29,700 --> 00:07:33,990
in the Cal competition but here we will

00:07:32,100 --> 00:07:37,050
only consider to clear and cloudy and

00:07:33,990 --> 00:07:39,180
the cloudy class in globe's all the

00:07:37,050 --> 00:07:41,310
three other classes like he is partly

00:07:39,180 --> 00:07:43,890
cloudy and cloudy we will consider them

00:07:41,310 --> 00:07:46,050
all as cloudy because basically if we

00:07:43,890 --> 00:07:47,910
have any kind of clouds in the image it

00:07:46,050 --> 00:07:50,690
could be disturbing for our segmentation

00:07:47,910 --> 00:07:53,520
network so we prefer to filter them out

00:07:50,690 --> 00:07:58,470
so this is how we are gonna distribute

00:07:53,520 --> 00:08:01,020
our data we first of all have the out of

00:07:58,470 --> 00:08:03,750
which we use 3/4 of it to train the

00:08:01,020 --> 00:08:05,910
resonance and we save the final quarter

00:08:03,750 --> 00:08:09,890
for validation and choosing a model and

00:08:05,910 --> 00:08:13,890
then from the 5000 extra images we have

00:08:09,890 --> 00:08:17,850
we use again 75% of them to fine-tune

00:08:13,890 --> 00:08:19,560
the network we chose and the last 25% to

00:08:17,850 --> 00:08:23,640
test the system and evaluate the

00:08:19,560 --> 00:08:25,620
accuracy so these are the results we got

00:08:23,640 --> 00:08:28,050
as you can see both resonate 50 and

00:08:25,620 --> 00:08:31,800
resonate 101 are pretty similar in terms

00:08:28,050 --> 00:08:36,660
of performance what you can see here on

00:08:31,800 --> 00:08:39,479
the right the a box is the the number of

00:08:36,660 --> 00:08:43,020
training epochs we we had to convey for

00:08:39,479 --> 00:08:44,960
each of them as resonate 50 is a bit

00:08:43,020 --> 00:08:47,730
simpler it takes less time to converge

00:08:44,960 --> 00:08:50,310
so instead of 40 epipheo only had to

00:08:47,730 --> 00:08:52,050
train it for 20 and the second number

00:08:50,310 --> 00:08:53,850
you see there is the number of epochs on

00:08:52,050 --> 00:08:56,820
the fine-tuning data set we'll run them

00:08:53,850 --> 00:08:58,710
on so given that we have almost the same

00:08:56,820 --> 00:09:00,930
accuracy but it's a bit better for

00:08:58,710 --> 00:09:03,480
ResNet 50-plus it's simpler and it takes

00:09:00,930 --> 00:09:07,440
less less time to train it makes sense

00:09:03,480 --> 00:09:10,650
to choose less than 50 this is just an

00:09:07,440 --> 00:09:12,089
example of the result these are as you

00:09:10,650 --> 00:09:14,880
can guess these are the cloudy image

00:09:12,089 --> 00:09:19,920
clear images sorry and the other ones

00:09:14,880 --> 00:09:22,260
are the cloudy so the thing is after

00:09:19,920 --> 00:09:25,080
this whole process of filtering out the

00:09:22,260 --> 00:09:28,170
clouds we don't have that much data we

00:09:25,080 --> 00:09:29,580
have like 5,000 images maybe because

00:09:28,170 --> 00:09:31,950
there are many clouds in the Netherlands

00:09:29,580 --> 00:09:33,290
so we need to you do some data

00:09:31,950 --> 00:09:35,209
documentation to

00:09:33,290 --> 00:09:37,910
have more data for our segmentation

00:09:35,209 --> 00:09:39,829
Network which we will see later so for

00:09:37,910 --> 00:09:43,040
this we use augmenter which is this

00:09:39,829 --> 00:09:45,170
Python library which I highly recommend

00:09:43,040 --> 00:09:47,329
as you can see it super easy to define a

00:09:45,170 --> 00:09:49,699
pipeline you define a couple of

00:09:47,329 --> 00:09:52,100
operations and then you can get your own

00:09:49,699 --> 00:09:55,009
mental images the transformations we

00:09:52,100 --> 00:09:58,430
each charge q share in flips and

00:09:55,009 --> 00:10:01,490
rotations so we can see them in the next

00:09:58,430 --> 00:10:03,769
slide here on the here you have the

00:10:01,490 --> 00:10:05,630
original image and that's the result of

00:10:03,769 --> 00:10:07,670
applying some random transformations to

00:10:05,630 --> 00:10:10,250
it to troupe some changes in

00:10:07,670 --> 00:10:13,750
perspectives and rotations but we can

00:10:10,250 --> 00:10:17,300
still identify the two defeats from it

00:10:13,750 --> 00:10:21,649
so now on to the segmentation part which

00:10:17,300 --> 00:10:24,889
is where we will identify the tulip

00:10:21,649 --> 00:10:27,440
fields so this is an example of the

00:10:24,889 --> 00:10:29,540
expected outcome of the segmentation on

00:10:27,440 --> 00:10:30,470
the left you have the image with the

00:10:29,540 --> 00:10:33,829
tulip fields which are pretty

00:10:30,470 --> 00:10:35,779
identifiable on in red and what we

00:10:33,829 --> 00:10:40,060
expect to get are the polygons defining

00:10:35,779 --> 00:10:43,519
the bounding boxes of these tulip fields

00:10:40,060 --> 00:10:44,660
so how do we do this we use a unit which

00:10:43,519 --> 00:10:48,139
is a state-of-the-art

00:10:44,660 --> 00:10:50,540
CNN for segmentation it's gotten like

00:10:48,139 --> 00:10:53,000
the best results in most competitions

00:10:50,540 --> 00:10:56,000
and in most applications where it has

00:10:53,000 --> 00:10:58,069
been used it was originally designed for

00:10:56,000 --> 00:11:01,490
biomedical images but we found that it

00:10:58,069 --> 00:11:04,370
works pretty well for this too you can

00:11:01,490 --> 00:11:07,370
see the paper is pretty interesting this

00:11:04,370 --> 00:11:10,579
is the architecture of the network which

00:11:07,370 --> 00:11:13,880
has a contracting part here in which we

00:11:10,579 --> 00:11:16,160
get a smaller smaller layers each time

00:11:13,880 --> 00:11:18,199
but with a higher number of features and

00:11:16,160 --> 00:11:20,779
then we have an expanding part on the

00:11:18,199 --> 00:11:24,079
other side which is here in which we

00:11:20,779 --> 00:11:25,639
observe all and also the most

00:11:24,079 --> 00:11:28,010
interesting part is this skip

00:11:25,639 --> 00:11:29,899
connections here which map directly

00:11:28,010 --> 00:11:33,050
information from the first layers to the

00:11:29,899 --> 00:11:36,019
last ones so that we will kind of keep

00:11:33,050 --> 00:11:40,100
some spatial information in the end of

00:11:36,019 --> 00:11:41,269
the network then now we have there we

00:11:40,100 --> 00:11:43,370
have two slides about the implementation

00:11:41,269 --> 00:11:44,779
of the building blocks the most

00:11:43,370 --> 00:11:46,430
interesting part to see is the

00:11:44,779 --> 00:11:47,030
convolutional block which is the basic

00:11:46,430 --> 00:11:48,980
block

00:11:47,030 --> 00:11:50,690
which contains a convolution about

00:11:48,980 --> 00:11:52,970
normalization and they are real wacked

00:11:50,690 --> 00:11:54,950
evasion and then we have the down block

00:11:52,970 --> 00:11:58,160
and in the next slide the a block in

00:11:54,950 --> 00:12:02,090
case you're interested next one because

00:11:58,160 --> 00:12:04,460
we don't have that much time okay so for

00:12:02,090 --> 00:12:06,800
the training data of this unit we are

00:12:04,460 --> 00:12:09,410
using the satellite images we have but

00:12:06,800 --> 00:12:11,510
we also need ground truth right so the

00:12:09,410 --> 00:12:13,310
ground truth is coming from geopdf from

00:12:11,510 --> 00:12:17,780
synergize like Sony showed you before

00:12:13,310 --> 00:12:19,940
and it comes in the form of these images

00:12:17,780 --> 00:12:22,190
right here where we have the white

00:12:19,940 --> 00:12:28,880
polygons representing the tulip fields

00:12:22,190 --> 00:12:36,110
and the rest of it is blood so I will

00:12:28,880 --> 00:12:37,640
before about one more perfect the

00:12:36,110 --> 00:12:39,440
instead of starting by the loss function

00:12:37,640 --> 00:12:42,310
let's start with the evaluation metric

00:12:39,440 --> 00:12:46,160
which we used intersection over Union

00:12:42,310 --> 00:12:48,890
it's probably the most used metric for

00:12:46,160 --> 00:12:50,540
segmentation tasks and as you can see

00:12:48,890 --> 00:12:53,000
it's basically the rate here between the

00:12:50,540 --> 00:12:55,760
intersection of what you predicted to be

00:12:53,000 --> 00:12:57,860
tulip fields in this case and the union

00:12:55,760 --> 00:13:01,730
of what you predicted to be tulip fields

00:12:57,860 --> 00:13:04,490
and the ground truth it's also called

00:13:01,730 --> 00:13:06,440
the jacquard index and it's pretty

00:13:04,490 --> 00:13:08,060
similar to the dice coefficient which is

00:13:06,440 --> 00:13:12,080
the function we will be using a slot

00:13:08,060 --> 00:13:14,990
which is this one here so this one is

00:13:12,080 --> 00:13:16,910
the short dice coefficient loss it's

00:13:14,990 --> 00:13:19,400
similar to the dice coefficient is

00:13:16,910 --> 00:13:22,550
similar to intersection of a union but

00:13:19,400 --> 00:13:24,890
it measures kind of if you use it on a

00:13:22,550 --> 00:13:26,590
whole dataset it measures something more

00:13:24,890 --> 00:13:28,610
similar to the average performance

00:13:26,590 --> 00:13:30,740
instead of the worst case performance

00:13:28,610 --> 00:13:33,350
which would be the intersection of a

00:13:30,740 --> 00:13:36,320
union somehow so that's why we chose it

00:13:33,350 --> 00:13:40,100
and some interesting things we can have

00:13:36,320 --> 00:13:42,860
here prediction which we have there has

00:13:40,100 --> 00:13:44,990
the form of a probability that's why we

00:13:42,860 --> 00:13:47,120
call it soft dice coefficient instead of

00:13:44,990 --> 00:13:50,990
the regular one and it's the softmax

00:13:47,120 --> 00:13:53,210
output of the unit we have before and

00:13:50,990 --> 00:13:56,590
yeah well we have a minus because we

00:13:53,210 --> 00:13:56,590
need to make it our loss function

00:13:57,740 --> 00:14:04,640
so the results we got on this we got an

00:14:01,590 --> 00:14:09,150
intersection of a union score of 0.73

00:14:04,640 --> 00:14:12,420
after training for 28 bucks so to put

00:14:09,150 --> 00:14:15,690
this in context normally intersection of

00:14:12,420 --> 00:14:18,060
a union you consider 0.5 to be what it

00:14:15,690 --> 00:14:22,320
was considered to be a good result right

00:14:18,060 --> 00:14:26,100
now 0.6 is considered to be like the

00:14:22,320 --> 00:14:28,440
standard good result and to compare our

00:14:26,100 --> 00:14:31,440
results with some other state-of-the-art

00:14:28,440 --> 00:14:33,810
results we found the most similar thing

00:14:31,440 --> 00:14:36,240
we found is this the stl Cargill

00:14:33,810 --> 00:14:38,490
competition which was also satellite

00:14:36,240 --> 00:14:41,460
image segmentation and the best

00:14:38,490 --> 00:14:46,530
researcher was getting 0.84 intersection

00:14:41,460 --> 00:14:49,230
over union but the difference here is

00:14:46,530 --> 00:14:52,170
that they were segmenting crops versus

00:14:49,230 --> 00:14:54,570
buildings or water or different things

00:14:52,170 --> 00:14:56,280
in our case we are segmenting tulip

00:14:54,570 --> 00:14:59,370
fields from any other kind of thing

00:14:56,280 --> 00:15:02,030
which can also be corrupt so it's

00:14:59,370 --> 00:15:06,650
difficult to compare the results but

00:15:02,030 --> 00:15:11,040
yeah that's what we have now Sammy will

00:15:06,650 --> 00:15:13,380
until yeah so once you have trained your

00:15:11,040 --> 00:15:15,540
deep learning models the resonate and

00:15:13,380 --> 00:15:18,540
unit so obviously you want to deploy

00:15:15,540 --> 00:15:21,540
them in production and you want to go in

00:15:18,540 --> 00:15:22,710
for streaming inference and most people

00:15:21,540 --> 00:15:24,870
any frameworks that we have today are

00:15:22,710 --> 00:15:28,380
python-based for whatever reason I wish

00:15:24,870 --> 00:15:31,380
it was JVM based so how do you put this

00:15:28,380 --> 00:15:34,260
in a streaming pipeline and obviously

00:15:31,380 --> 00:15:36,660
one approach was to maybe make an RPC

00:15:34,260 --> 00:15:39,150
call from a fling fling pipeline to a

00:15:36,660 --> 00:15:40,740
Python model yeah sure you can do that

00:15:39,150 --> 00:15:43,230
it's doable it's doable

00:15:40,740 --> 00:15:46,680
the other approach was to go with the

00:15:43,230 --> 00:15:50,190
Google bean to go with Apache beam which

00:15:46,680 --> 00:15:51,060
has a Python SDK and a Java SDK so what

00:15:50,190 --> 00:15:53,460
is Apache Beam

00:15:51,060 --> 00:15:55,110
it's a agnostic unified batch in

00:15:53,460 --> 00:15:57,120
streaming programming model as many of

00:15:55,110 --> 00:16:00,030
you know it's called support for Java

00:15:57,120 --> 00:16:01,740
Python and go SDKs and yeah it's quite

00:16:00,030 --> 00:16:04,290
several backhand runners for Flinx Park

00:16:01,740 --> 00:16:06,140
and Google dataflow and a local data

00:16:04,290 --> 00:16:11,600
runner

00:16:06,140 --> 00:16:13,190
so via Apache beam why Apache beam so

00:16:11,600 --> 00:16:15,079
number one it's both portable portable

00:16:13,190 --> 00:16:17,120
in the sense that you're writing your

00:16:15,079 --> 00:16:18,890
code against the beam API and you can

00:16:17,120 --> 00:16:21,019
run it on a fling core a spark executor

00:16:18,890 --> 00:16:23,630
or any other executor that's supported

00:16:21,019 --> 00:16:25,190
by beam and the second reason is it's

00:16:23,630 --> 00:16:27,920
got a unified batch and streaming API

00:16:25,190 --> 00:16:30,829
and the third reason is a bit burn model

00:16:27,920 --> 00:16:33,380
SDK it's quite extensible model and SDKs

00:16:30,829 --> 00:16:37,940
so you can use the custom SDK to write

00:16:33,380 --> 00:16:39,860
your custom syncs and sources so the

00:16:37,940 --> 00:16:41,209
Apache beam mission has been that again

00:16:39,860 --> 00:16:42,980
Apache beam is a project from Google

00:16:41,209 --> 00:16:45,050
that's been open sourced into Apache and

00:16:42,980 --> 00:16:46,399
the vision is you have a different

00:16:45,050 --> 00:16:48,260
categories of users if you look at this

00:16:46,399 --> 00:16:51,230
the architecture of a beam you have a

00:16:48,260 --> 00:16:53,570
beam Java Python and other languages now

00:16:51,230 --> 00:16:56,360
they have go as of last week I believe

00:16:53,570 --> 00:16:57,769
or maybe last month so the end users

00:16:56,360 --> 00:17:00,380
create pipelines in a familiar language

00:16:57,769 --> 00:17:01,670
it could be Java or Python and then you

00:17:00,380 --> 00:17:04,699
have that different category of users

00:17:01,670 --> 00:17:06,980
like SDK writers the folks who write who

00:17:04,699 --> 00:17:08,720
want to who want to create new language

00:17:06,980 --> 00:17:11,209
packs a new language support for being

00:17:08,720 --> 00:17:12,770
so the counts you can make that

00:17:11,209 --> 00:17:15,199
available for different languages here

00:17:12,770 --> 00:17:17,900
and the most important part is the

00:17:15,199 --> 00:17:21,020
runners the backend runners so today we

00:17:17,900 --> 00:17:22,280
have fling spark dataflow and epics so

00:17:21,020 --> 00:17:26,480
you could have a different streaming

00:17:22,280 --> 00:17:28,130
engine as for this so if you look at the

00:17:26,480 --> 00:17:30,440
beam programming API if I write my code

00:17:28,130 --> 00:17:33,080
in Java beam API using the Java Beam API

00:17:30,440 --> 00:17:35,390
you create your pipeline using Java Beam

00:17:33,080 --> 00:17:38,990
API and then you specify your runner you

00:17:35,390 --> 00:17:41,570
want to run it on fling a spark or do

00:17:38,990 --> 00:17:42,950
you want to run it locally and if you

00:17:41,570 --> 00:17:46,100
look at the beam API they have something

00:17:42,950 --> 00:17:47,660
called FN or function runners and these

00:17:46,100 --> 00:17:50,059
function runners are defined in the

00:17:47,660 --> 00:17:51,740
high-level language and there that's the

00:17:50,059 --> 00:17:53,150
theological language and they're

00:17:51,740 --> 00:17:55,790
translated for each of the specific

00:17:53,150 --> 00:18:02,179
runners linker spark and executed on

00:17:55,790 --> 00:18:03,919
those frameworks so overall for this

00:18:02,179 --> 00:18:06,440
project this is your inference pipeline

00:18:03,919 --> 00:18:08,750
so jose trains the models jose trains

00:18:06,440 --> 00:18:09,799
resonate model in the unit model and how

00:18:08,750 --> 00:18:11,929
do you plug that into a streaming

00:18:09,799 --> 00:18:13,760
pipeline so this you ingest your

00:18:11,929 --> 00:18:16,760
satellite image data you call the

00:18:13,760 --> 00:18:18,980
resonate 54 classification and you call

00:18:16,760 --> 00:18:19,550
unit for segmentation all of that is one

00:18:18,980 --> 00:18:21,440
beam

00:18:19,550 --> 00:18:31,040
and we are running it on the flink

00:18:21,440 --> 00:18:33,290
executor any questions so far so this is

00:18:31,040 --> 00:18:35,690
for this particular code this is how the

00:18:33,290 --> 00:18:37,160
beam inference pipeline looks like you

00:18:35,690 --> 00:18:39,560
specify your pipeline options which

00:18:37,160 --> 00:18:42,410
would be your input output the model

00:18:39,560 --> 00:18:44,540
folder and this is the typical beam API

00:18:42,410 --> 00:18:47,690
with beam pipeline SP you read the

00:18:44,540 --> 00:18:49,340
images first and then I'm creating a

00:18:47,690 --> 00:18:51,380
window of all the images that kind of

00:18:49,340 --> 00:18:53,540
create a mini batch for inference and

00:18:51,380 --> 00:18:56,630
then you filter out the cloudy images

00:18:53,540 --> 00:18:58,100
and once you have the filter images you

00:18:56,630 --> 00:19:02,270
take the filtered images and segment

00:18:58,100 --> 00:19:08,450
them for unit classification so this is

00:19:02,270 --> 00:19:12,020
the typical beam pipeline so obviously

00:19:08,450 --> 00:19:14,300
if you look at this flow again yeah each

00:19:12,020 --> 00:19:17,240
of this is a beam function each of this

00:19:14,300 --> 00:19:21,770
steps here is a beam function and coats

00:19:17,240 --> 00:19:24,770
and beds for these so this is for a

00:19:21,770 --> 00:19:26,150
cloud classifier and do Flynn is it's a

00:19:24,770 --> 00:19:28,490
beam convention every function is a

00:19:26,150 --> 00:19:29,720
doofen and do you specify that as a part

00:19:28,490 --> 00:19:33,680
of a power tube which is again the beam

00:19:29,720 --> 00:19:35,270
API so for you are filtering out the

00:19:33,680 --> 00:19:38,930
cloudy from this is filtering with the

00:19:35,270 --> 00:19:40,730
cloudy images so I'm loading I'm loading

00:19:38,930 --> 00:19:43,160
my resinate here as part of the

00:19:40,730 --> 00:19:45,170
initialization process and then I have

00:19:43,160 --> 00:19:49,190
once I have that resonant image I am

00:19:45,170 --> 00:19:51,350
calling yeah I'm calling the making the

00:19:49,190 --> 00:19:53,090
predictions here for the resume for the

00:19:51,350 --> 00:19:56,300
resonant images I guess a resonant model

00:19:53,090 --> 00:19:57,710
and and that's you go get you up and I

00:19:56,300 --> 00:20:01,130
get a collection of all the clear images

00:19:57,710 --> 00:20:02,630
the ones that are not loading so the

00:20:01,130 --> 00:20:04,640
next step of once you have your clear

00:20:02,630 --> 00:20:06,620
images the next step is Jose explained

00:20:04,640 --> 00:20:09,470
was to segment those images for tulip

00:20:06,620 --> 00:20:12,350
fields so this is the beam API for that

00:20:09,470 --> 00:20:15,070
so you have a clear clear images which

00:20:12,350 --> 00:20:17,930
is a collection of file names and then

00:20:15,070 --> 00:20:23,660
then you create a mask mask image and

00:20:17,930 --> 00:20:27,340
then you save that as your output so

00:20:23,660 --> 00:20:27,340
let's do a big demo

00:20:32,230 --> 00:20:38,500
so here okay so the one challenge that

00:20:36,159 --> 00:20:40,210
we had been building this verse be may

00:20:38,500 --> 00:20:42,519
appear suppose only Python 2.7

00:20:40,210 --> 00:20:43,870
as of today okay it's not on Python

00:20:42,519 --> 00:20:47,649
three eight they're working on it

00:20:43,870 --> 00:20:49,450
nevertheless and yeah so the some of the

00:20:47,649 --> 00:20:51,580
code was written in Python 3 and Python

00:20:49,450 --> 00:20:52,990
2.7 in Python 3 and me not being a

00:20:51,580 --> 00:20:55,269
Python programmer it was kind of like a

00:20:52,990 --> 00:20:58,450
living hell for some time trying to get

00:20:55,269 --> 00:21:03,159
this working and the other challenge was

00:20:58,450 --> 00:21:05,769
the flink Runner for beans Python SDK is

00:21:03,159 --> 00:21:07,120
still experimental and thanks to the

00:21:05,769 --> 00:21:10,299
Google folks they really got it to work

00:21:07,120 --> 00:21:10,740
in some shape for this talk hats off to

00:21:10,299 --> 00:21:13,720
them

00:21:10,740 --> 00:21:16,480
so let's this is on Python 2 now so let

00:21:13,720 --> 00:21:18,460
me run this so it's going to take some

00:21:16,480 --> 00:21:23,049
time it's take some time to start up and

00:21:18,460 --> 00:21:34,659
load reload the deplaning models so well

00:21:23,049 --> 00:21:37,269
that's running well that's running let's

00:21:34,659 --> 00:21:39,010
talk about some important links so most

00:21:37,269 --> 00:21:42,100
of the satellite image data is available

00:21:39,010 --> 00:21:43,990
on it appears at AWS Earth and you can

00:21:42,100 --> 00:21:45,519
get this at you have data sets from the

00:21:43,990 --> 00:21:47,169
sentinel 2 from landsat and the

00:21:45,519 --> 00:21:49,269
different satellites here both an

00:21:47,169 --> 00:21:52,630
infrared band in the forex near-infrared

00:21:49,269 --> 00:21:54,309
as well as in RGB format and to

00:21:52,630 --> 00:21:56,019
understand better what unit is at how

00:21:54,309 --> 00:21:58,179
semantic segmentation works there's a

00:21:56,019 --> 00:22:00,340
very interesting medium.com post by this

00:21:58,179 --> 00:22:02,980
gentleman it's really interesting and

00:22:00,340 --> 00:22:04,539
explains very well how unit works and of

00:22:02,980 --> 00:22:09,519
course the papers on resonate and unit

00:22:04,539 --> 00:22:12,519
you could look at yeah and of course the

00:22:09,519 --> 00:22:15,600
beam api's and this light tag here so

00:22:12,519 --> 00:22:15,600
let's go back and see how our

00:22:17,480 --> 00:22:32,600
so this is going to take some time so

00:22:20,419 --> 00:22:35,389
let's how are we on time okay so this is

00:22:32,600 --> 00:22:37,970
going to take some time and yeah this is

00:22:35,389 --> 00:22:39,409
something I was I was kind of like maybe

00:22:37,970 --> 00:22:40,730
I should just put some German messages

00:22:39,409 --> 00:22:42,409
and you know translate them in English

00:22:40,730 --> 00:22:50,110
using a noodle machine translation model

00:22:42,409 --> 00:22:53,000
where this was running sure okay so

00:22:50,110 --> 00:22:55,250
let's say leather keep running but this

00:22:53,000 --> 00:22:57,049
is a sample output that you would see so

00:22:55,250 --> 00:23:00,260
here you have this satellite image do

00:22:57,049 --> 00:23:02,450
you see any tulip fields here or nothing

00:23:00,260 --> 00:23:06,230
so the output of that would be a blank

00:23:02,450 --> 00:23:08,750
black screen okay so let's look at the

00:23:06,230 --> 00:23:11,899
next image yeah do you see any tulip

00:23:08,750 --> 00:23:15,649
fields here yeah there is something here

00:23:11,899 --> 00:23:17,750
and something here and here so that's

00:23:15,649 --> 00:23:19,779
that's the output that you get so using

00:23:17,750 --> 00:23:22,610
a unit segmentation so that's where it

00:23:19,779 --> 00:23:24,529
identifies the way that you lis fields

00:23:22,610 --> 00:23:26,899
are and again let me remind you we are

00:23:24,529 --> 00:23:28,580
only using three channels GP channels we

00:23:26,899 --> 00:23:31,279
are not using the near-infrared and you

00:23:28,580 --> 00:23:35,080
know further bands which would provide

00:23:31,279 --> 00:23:39,440
much more information now the last

00:23:35,080 --> 00:23:41,840
sample here so this is seems to be a

00:23:39,440 --> 00:23:44,750
small chilly field and so that's the

00:23:41,840 --> 00:23:46,399
output of that but do you really think

00:23:44,750 --> 00:23:50,659
it's actually field it doesn't look like

00:23:46,399 --> 00:23:52,909
one to me it's a house yeah it's a

00:23:50,659 --> 00:23:55,039
building and the reason again for that

00:23:52,909 --> 00:23:57,350
is because we are only using the small

00:23:55,039 --> 00:23:59,299
RGB three channels three bands if you

00:23:57,350 --> 00:24:01,010
had been using more infrared band then

00:23:59,299 --> 00:24:05,240
maybe we had more information that we

00:24:01,010 --> 00:24:08,899
could go against so yeah it's still

00:24:05,240 --> 00:24:10,730
running okay give it some time so given

00:24:08,899 --> 00:24:12,980
this image which kind of you know brings

00:24:10,730 --> 00:24:18,289
brings which kind of serves as a segue

00:24:12,980 --> 00:24:20,899
to what the future work would be so

00:24:18,289 --> 00:24:22,639
classify rock formations so we are only

00:24:20,899 --> 00:24:25,039
using the RGB bands but if you are using

00:24:22,639 --> 00:24:27,740
the short shortwave infrared images

00:24:25,039 --> 00:24:30,950
which are kind of in the 2.1 to 2.2

00:24:27,740 --> 00:24:31,310
nanometer range all l appearing in

00:24:30,950 --> 00:24:32,660
remind

00:24:31,310 --> 00:24:35,090
the images that we have are in the 10

00:24:32,660 --> 00:24:36,650
meter range resolution if we can go with

00:24:35,090 --> 00:24:39,110
further resolution like 2 point 1 to 2

00:24:36,650 --> 00:24:41,090
point 2 nanometers we can actually

00:24:39,110 --> 00:24:43,010
classify rock formations well we all

00:24:41,090 --> 00:24:44,360
know that plants don't grow on rocks so

00:24:43,010 --> 00:24:47,690
that's a data point that we could yield

00:24:44,360 --> 00:24:49,430
leverage and also all the objects rocks

00:24:47,690 --> 00:24:51,050
they emit radiant flux it's called the

00:24:49,430 --> 00:24:53,360
amount of energy that they radiate out

00:24:51,050 --> 00:24:55,040
per unit time which is given which is

00:24:53,360 --> 00:24:57,290
known as radiant flux and that's the

00:24:55,040 --> 00:24:58,700
equation for that you could use that as

00:24:57,290 --> 00:25:00,650
information to classify whether this is

00:24:58,700 --> 00:25:05,210
a rock or a building or it's a crop

00:25:00,650 --> 00:25:08,210
field the other the other future use

00:25:05,210 --> 00:25:10,970
case would be how do you measure crop

00:25:08,210 --> 00:25:12,800
health so for this you can actually go

00:25:10,970 --> 00:25:15,620
with the near-infrared radiation

00:25:12,800 --> 00:25:17,060
images so plants obviously have

00:25:15,620 --> 00:25:19,990
chlorophyll and missile fill and other

00:25:17,060 --> 00:25:22,100
pigments and each of those they give out

00:25:19,990 --> 00:25:24,950
you know they give us some radiations

00:25:22,100 --> 00:25:27,140
infrared radiations and the amount of

00:25:24,950 --> 00:25:28,820
chlorophyll content differs between the

00:25:27,140 --> 00:25:31,640
plants as well as in the various growth

00:25:28,820 --> 00:25:33,050
stages of the same plant so depending on

00:25:31,640 --> 00:25:34,610
the infrared radiation that you get from

00:25:33,050 --> 00:25:36,290
the plants we can determine the health

00:25:34,610 --> 00:25:40,720
of the plant as well as classify this

00:25:36,290 --> 00:25:40,720
plant as dooley fields versus a Rose bed

00:25:42,130 --> 00:25:50,390
and of course the very last use case is

00:25:46,040 --> 00:25:52,880
just go with red band so have you ever

00:25:50,390 --> 00:25:54,770
wondered like if you look at an image if

00:25:52,880 --> 00:25:56,300
you look at any satellite image without

00:25:54,770 --> 00:25:57,830
there being a clear clear demarcation of

00:25:56,300 --> 00:26:00,770
the boundaries between the countries or

00:25:57,830 --> 00:26:03,080
the different regions yet just by a

00:26:00,770 --> 00:26:04,520
visual looking at it visually you are

00:26:03,080 --> 00:26:06,860
still able to figure out you know this

00:26:04,520 --> 00:26:07,880
is this could be this is Netherlands and

00:26:06,860 --> 00:26:09,440
this is Germany and this is where

00:26:07,880 --> 00:26:13,040
Germany starts and Netherlands stops

00:26:09,440 --> 00:26:14,150
this is Amsterdam versus Berlin have you

00:26:13,040 --> 00:26:15,290
ever figured out why you and I will to

00:26:14,150 --> 00:26:18,040
do that even though you're just looking

00:26:15,290 --> 00:26:20,330
at the infrared in just a plain image

00:26:18,040 --> 00:26:22,100
it's because most of the images are in

00:26:20,330 --> 00:26:24,350
red band which is more visually

00:26:22,100 --> 00:26:25,970
appealing to humans and it's kind of

00:26:24,350 --> 00:26:27,290
like a clustering what you're doing is a

00:26:25,970 --> 00:26:29,420
clustering oh this this particular

00:26:27,290 --> 00:26:30,370
region is Amsterdam versus this is

00:26:29,420 --> 00:26:33,020
Berlin

00:26:30,370 --> 00:26:36,140
so yeah that's definitely a possibility

00:26:33,020 --> 00:26:38,320
going forward okay great I have this

00:26:36,140 --> 00:26:38,320
running

00:26:43,560 --> 00:26:47,850
[Music]

00:26:45,500 --> 00:26:49,890
so this is the beam that's running okay

00:26:47,850 --> 00:26:51,030
beam API so if you look at this these

00:26:49,890 --> 00:26:54,360
are the different transformations that

00:26:51,030 --> 00:26:55,650
are happening and yeah that it's trading

00:26:54,360 --> 00:26:58,080
about that I'm reading about ten images

00:26:55,650 --> 00:27:06,360
and you know calculating the mask for

00:26:58,080 --> 00:27:10,670
those ten images okay so we are done

00:27:06,360 --> 00:27:10,670
with that let me switch to

00:27:31,780 --> 00:27:43,450
so let's look at this guy it's hard man

00:27:39,900 --> 00:27:46,300
so this one do we see any may Julie

00:27:43,450 --> 00:27:48,790
feels here yeah I see some Reds but is

00:27:46,300 --> 00:27:51,910
that really fields maybe not let's look

00:27:48,790 --> 00:27:54,120
at the mass for this divulge it to my

00:27:51,910 --> 00:27:54,120
mother

00:28:12,230 --> 00:28:17,750
okay so this is the mask for that this

00:28:16,309 --> 00:28:25,940
is the mask for the images or before

00:28:17,750 --> 00:28:27,110
it's a nice black screen so it's a nice

00:28:25,940 --> 00:28:29,090
black screen which means there's no

00:28:27,110 --> 00:28:31,389
tulips there so let's try a different

00:28:29,090 --> 00:28:31,389
one

00:28:36,850 --> 00:28:43,740
yeah yeah there's simply habit to leap

00:28:41,769 --> 00:28:52,210
bed there let's try a different one

00:28:43,740 --> 00:28:55,000
maybe not no okay let's go with this so

00:28:52,210 --> 00:28:57,190
this is another sample image so it's got

00:28:55,000 --> 00:29:02,220
something here could that be truly bits

00:28:57,190 --> 00:29:02,220
yeah maybe so let's look at the mask

00:29:35,830 --> 00:29:41,570
this is your original image and this is

00:29:38,630 --> 00:29:45,080
the mask so the original image had some

00:29:41,570 --> 00:29:47,000
tulips here and up here and there's

00:29:45,080 --> 00:29:48,980
something here yellow band so this is

00:29:47,000 --> 00:29:51,250
what the mask came up with which means

00:29:48,980 --> 00:29:54,320
yeah those are possible to look fields

00:29:51,250 --> 00:29:56,210
okay so this is kind of an example of

00:29:54,320 --> 00:29:57,830
you know this is all running in a bean

00:29:56,210 --> 00:29:59,180
streaming pipeline so you can have we

00:29:57,830 --> 00:30:00,710
can actually get the images live from

00:29:59,180 --> 00:30:02,480
satellite and you have your train models

00:30:00,710 --> 00:30:04,340
you put your train models in a streaming

00:30:02,480 --> 00:30:06,550
pipeline and you can make an inference

00:30:04,340 --> 00:30:08,810
this way

00:30:06,550 --> 00:30:10,790
so yeah this code is running on a flink

00:30:08,810 --> 00:30:12,260
runner by the way when the beams beam

00:30:10,790 --> 00:30:14,690
python is decaf link runner let me

00:30:12,260 --> 00:30:16,340
emphasize that it's still experimental

00:30:14,690 --> 00:30:22,990
but nevertheless it's working in some

00:30:16,340 --> 00:30:27,680
shape okay switching back to PowerPoint

00:30:22,990 --> 00:30:30,770
okay so we talked about this so that

00:30:27,680 --> 00:30:32,990
kind of concludes the stock and some of

00:30:30,770 --> 00:30:35,090
the credits for this Jose of course and

00:30:32,990 --> 00:30:38,060
kelan and Matthew from Amazon Berlin

00:30:35,090 --> 00:30:40,280
here who have done this earlier bus from

00:30:38,060 --> 00:30:41,540
Frankfurt he was the guy who came DUP

00:30:40,280 --> 00:30:43,310
came up with some of the ideas that we

00:30:41,540 --> 00:30:44,420
had used here and he's an expert on

00:30:43,310 --> 00:30:47,000
computer vision and land use

00:30:44,420 --> 00:30:48,590
classification and of course the Apache

00:30:47,000 --> 00:30:51,140
beam folks from Google they have been

00:30:48,590 --> 00:30:54,650
very helpful to get this Python SDK run

00:30:51,140 --> 00:30:55,910
off of link in some shape working and of

00:30:54,650 --> 00:30:58,100
course a few other folks from Amazon

00:30:55,910 --> 00:30:59,510
here Pascal and Jade Thunderball Jet

00:30:58,100 --> 00:31:02,840
Center Bell is the guy from Amazon who

00:30:59,510 --> 00:31:06,160
maintains the who maintains the data set

00:31:02,840 --> 00:31:08,930
and AWS the one I had pointed to earlier

00:31:06,160 --> 00:31:13,220
so Earth on AWS is maintained by jets

00:31:08,930 --> 00:31:15,410
under wall and of course a few of the

00:31:13,220 --> 00:31:17,240
open NLP folks and others who have been

00:31:15,410 --> 00:31:21,590
helping out with this slide tech at

00:31:17,240 --> 00:31:25,510
least one being the slide deck so with

00:31:21,590 --> 00:31:25,510
that concludes our talk any questions

00:31:28,250 --> 00:31:32,269
[Applause]

00:31:33,940 --> 00:31:40,310
hi thanks for the talk so I'm just

00:31:36,830 --> 00:31:47,420
curious do roofs cause a lot of problems

00:31:40,310 --> 00:31:50,570
for detecting two tulip fields whether

00:31:47,420 --> 00:31:52,670
roofs just out of curiosity roofs of

00:31:50,570 --> 00:31:58,250
houses is a kind of a square which is

00:31:52,670 --> 00:32:00,890
more or less red so should they

00:31:58,250 --> 00:32:03,530
definitely do and the reason for that is

00:32:00,890 --> 00:32:05,930
we are only using so for example this

00:32:03,530 --> 00:32:06,980
one the exact reason for that is we are

00:32:05,930 --> 00:32:08,720
only using three bands

00:32:06,980 --> 00:32:10,160
red green blue whereas if you have been

00:32:08,720 --> 00:32:11,480
taking more infrared bands and

00:32:10,160 --> 00:32:13,460
additional bands you would be getting

00:32:11,480 --> 00:32:15,440
better resolutions for example this one

00:32:13,460 --> 00:32:17,270
this not it you Liefeld it's a house

00:32:15,440 --> 00:32:19,010
it's a building and it's a roof of the

00:32:17,270 --> 00:32:20,300
building so it's kind of you know

00:32:19,010 --> 00:32:25,340
classifying that as actually filled

00:32:20,300 --> 00:32:26,690
because it's red in color sexual talk my

00:32:25,340 --> 00:32:29,540
question is how long this check to try

00:32:26,690 --> 00:32:34,100
train the units and which harp I did use

00:32:29,540 --> 00:32:37,610
two trains we were using a media Teta

00:32:34,100 --> 00:32:40,580
and V it didn't take too long it took

00:32:37,610 --> 00:32:42,590
like nine hours to train it 450 a box

00:32:40,580 --> 00:32:46,520
and then we chose the best performing

00:32:42,590 --> 00:32:49,850
epic which was after 23 so you could

00:32:46,520 --> 00:32:51,740
train it in 8 hours more or less so also

00:32:49,850 --> 00:32:53,720
another thing for training the deep

00:32:51,740 --> 00:32:55,070
learning models what we had seen is you

00:32:53,720 --> 00:32:56,330
train for different deep learning models

00:32:55,070 --> 00:32:58,880
with the different type of parameters

00:32:56,330 --> 00:33:00,410
and you get those models average them

00:32:58,880 --> 00:33:02,780
out train the fifths model with average

00:33:00,410 --> 00:33:07,640
values that works the best okay it works

00:33:02,780 --> 00:33:14,510
the best that's it so it's kind of like

00:33:07,640 --> 00:33:16,820
an in-sample training hi thanks

00:33:14,510 --> 00:33:18,590
so just a follow-up on that point Sunil

00:33:16,820 --> 00:33:21,650
if you're saying you run for models and

00:33:18,590 --> 00:33:26,390
get an ensemble of them here right SJM

00:33:21,650 --> 00:33:27,830
air so you know my intuition for hyper

00:33:26,390 --> 00:33:30,050
parameter searches that you would run

00:33:27,830 --> 00:33:32,540
many experiments maybe grid search or

00:33:30,050 --> 00:33:35,960
and you know nowadays people are doing

00:33:32,540 --> 00:33:39,590
model architecture search would you have

00:33:35,960 --> 00:33:41,720
benefited from a firm of GPUs where you

00:33:39,590 --> 00:33:43,580
could do large-scale so

00:33:41,720 --> 00:33:46,100
before you then start your 8 hours of

00:33:43,580 --> 00:33:48,680
training yeah definitely yeah

00:33:46,100 --> 00:33:49,670
and I totally agree with you Jim and

00:33:48,680 --> 00:33:51,680
that you're right

00:33:49,670 --> 00:33:53,930
and one other point that we do not

00:33:51,680 --> 00:33:56,020
mention here was the number of epochs we

00:33:53,930 --> 00:33:58,250
are chosen for the training the models

00:33:56,020 --> 00:33:59,840
so we kind of tried with different

00:33:58,250 --> 00:34:03,650
epochs number of epochs and we kind of

00:33:59,840 --> 00:34:05,210
plotted the graph and the accuracy of

00:34:03,650 --> 00:34:14,060
the different one and we just went with

00:34:05,210 --> 00:34:15,560
23 for example for resonate yeah so we

00:34:14,060 --> 00:34:17,000
kind of found that 23 was where the

00:34:15,560 --> 00:34:18,710
elbow happens in the graph when you plot

00:34:17,000 --> 00:34:20,870
the graph versus accuracy number of

00:34:18,710 --> 00:34:23,300
epochs was the accuracy and similarly

00:34:20,870 --> 00:34:24,800
for 43 with the resonate 101 since the

00:34:23,300 --> 00:34:26,000
accuracy was about the same we just went

00:34:24,800 --> 00:34:28,790
with the resonate 50 it's a smaller

00:34:26,000 --> 00:34:37,040
model compared to 101 and similarly for

00:34:28,790 --> 00:34:44,150
unite we didn't have it yeah unit I

00:34:37,040 --> 00:34:46,310
think we went with 24 23 23 pox again to

00:34:44,150 --> 00:34:49,220
your question Jim could have benefited

00:34:46,310 --> 00:34:50,480
with a GPU cluster so this most of this

00:34:49,220 --> 00:35:04,210
stuff was trained trained on a single

00:34:50,480 --> 00:35:04,210
GPU incidence Titan no more questions

00:35:07,950 --> 00:35:13,790
I'm just curious for them for the

00:35:11,640 --> 00:35:18,030
recognition of tulips it it looks like

00:35:13,790 --> 00:35:21,540
it's a lot based on color did you ever

00:35:18,030 --> 00:35:24,030
try to to build a model that just looks

00:35:21,540 --> 00:35:27,990
using open CV and look looking basically

00:35:24,030 --> 00:35:31,320
using statistical approaches for for

00:35:27,990 --> 00:35:32,460
energy or object detection yeah no we

00:35:31,320 --> 00:35:36,270
did not but yeah that's when I

00:35:32,460 --> 00:35:37,890
approached to definitely consider well

00:35:36,270 --> 00:35:39,329
the idea for this the idea behind this

00:35:37,890 --> 00:35:41,070
whole project was to actually use deep

00:35:39,329 --> 00:35:44,490
learning from the word go

00:35:41,070 --> 00:35:46,260
and so we've okay well thank you

00:35:44,490 --> 00:35:48,450
everyone for coming and thanks to our

00:35:46,260 --> 00:35:51,109
presenters here and we're gonna make a

00:35:48,450 --> 00:35:51,109
short quick

00:35:51,740 --> 00:35:55,560

YouTube URL: https://www.youtube.com/watch?v=Sosu9zKMPzo


