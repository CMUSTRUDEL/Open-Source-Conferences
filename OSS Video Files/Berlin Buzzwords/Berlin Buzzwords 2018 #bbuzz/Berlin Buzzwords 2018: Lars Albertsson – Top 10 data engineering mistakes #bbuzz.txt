Title: Berlin Buzzwords 2018: Lars Albertsson â€“ Top 10 data engineering mistakes #bbuzz
Publication date: 2018-06-13
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	A large fraction of big data projects fail to deliver return of investment, or take years before they do so. The reasons are typically a combination of project management, leadership, organisation, available competence, and technical failures. 

In this presentation, I will focus on the technical aspects, and present the most common or costly data engineering mistakes that I have experienced when building scalable data processing technology over the last five years, as well as advice for how to avoid them. 

The presentation includes war stories from large scale production environments, some that lead to reprocessing of petabytes of data, or DDoSing critical services with a Hadoop cluster, and what we learnt from the incidents.

Read more:
https://2018.berlinbuzzwords.de/18/session/top-10-data-engineering-mistakes

About Lars Albertsson:
https://2018.berlinbuzzwords.de/users/lars-albertsson

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:04,570 --> 00:00:09,309
right thank you everyone for coming I

00:00:06,910 --> 00:00:11,469
really did not expect to see a full room

00:00:09,309 --> 00:00:13,420
given that the elevator has broken down

00:00:11,469 --> 00:00:15,340
so thank you all for climbing the stairs

00:00:13,420 --> 00:00:16,599
and try not to breathe so much so that

00:00:15,340 --> 00:00:18,759
we can have some air in here

00:00:16,599 --> 00:00:20,860
now you might think I'm going to tell

00:00:18,759 --> 00:00:22,599
you what you're doing wrong I'm not I'm

00:00:20,860 --> 00:00:23,020
going to tell you where we have gone

00:00:22,599 --> 00:00:24,640
wrong

00:00:23,020 --> 00:00:26,980
like mistakes that we've made in the

00:00:24,640 --> 00:00:28,180
past so that we can all learn from them

00:00:26,980 --> 00:00:32,619
so that you can make different mistakes

00:00:28,180 --> 00:00:35,079
and come here and tell us about them I

00:00:32,619 --> 00:00:37,480
will say you know do this and don't do

00:00:35,079 --> 00:00:40,870
that I'm not telling you what to do I'm

00:00:37,480 --> 00:00:43,090
this is a short formulation for these

00:00:40,870 --> 00:00:45,910
are the conclusions that I have come to

00:00:43,090 --> 00:00:48,040
so that you can perhaps come to

00:00:45,910 --> 00:00:51,040
different conclusions or speed up the

00:00:48,040 --> 00:00:53,020
your path of learning and in everything

00:00:51,040 --> 00:00:55,090
whether it's data engineering whether

00:00:53,020 --> 00:00:56,890
it's playing the piano you know if you

00:00:55,090 --> 00:00:58,930
don't know any rules or guidelines it

00:00:56,890 --> 00:01:02,200
will sound terrible once you get some

00:00:58,930 --> 00:01:05,979
things in place you can provide value

00:01:02,200 --> 00:01:07,420
play as time goes by but in order to

00:01:05,979 --> 00:01:09,400
like do something really good you have

00:01:07,420 --> 00:01:12,070
to bend the rules so I'm trying to

00:01:09,400 --> 00:01:15,040
convey the sort of rules and practices

00:01:12,070 --> 00:01:19,900
that I have come to conclude you might

00:01:15,040 --> 00:01:21,610
come to different conclusions why am i

00:01:19,900 --> 00:01:23,830
standing here why am i suitable to do

00:01:21,610 --> 00:01:25,240
this well I've had the I've been very

00:01:23,830 --> 00:01:26,830
lucky in my career in a couple of

00:01:25,240 --> 00:01:28,120
aspects I worked for some very good

00:01:26,830 --> 00:01:29,799
companies that are really good at these

00:01:28,120 --> 00:01:32,500
things and picked a few things up from

00:01:29,799 --> 00:01:33,670
from very talented people in the last

00:01:32,500 --> 00:01:36,190
two and a half years I've been an

00:01:33,670 --> 00:01:38,320
independent consultant and I've been

00:01:36,190 --> 00:01:39,940
helping a whole bunch of companies so

00:01:38,320 --> 00:01:42,159
I've seen more like big data

00:01:39,940 --> 00:01:43,930
environments that the most people and in

00:01:42,159 --> 00:01:45,670
that many environments there are some

00:01:43,930 --> 00:01:47,110
patterns emerging things that people do

00:01:45,670 --> 00:01:51,880
over and over again and that's what I'm

00:01:47,110 --> 00:01:53,799
trying to convey to you so how do I find

00:01:51,880 --> 00:01:55,780
a mistake well a mistake is something

00:01:53,799 --> 00:01:58,119
that prevents you from reaching your end

00:01:55,780 --> 00:02:00,189
goals and there are actually different

00:01:58,119 --> 00:02:02,229
angles out there my definition of an end

00:02:00,189 --> 00:02:04,960
goal is like some kind of profit like

00:02:02,229 --> 00:02:09,869
either money or happy users or something

00:02:04,960 --> 00:02:13,419
of business value that there are other

00:02:09,869 --> 00:02:15,879
goals out there whether consciously or

00:02:13,419 --> 00:02:18,130
unconsciously and that's fine but that's

00:02:15,879 --> 00:02:21,310
that's my definition for the sake of the

00:02:18,130 --> 00:02:22,030
stalk and we do notice that people make

00:02:21,310 --> 00:02:25,480
a lot of mistake

00:02:22,030 --> 00:02:27,040
this is a gardener quote that maybe 60%

00:02:25,480 --> 00:02:29,140
failed mistakes was a little an

00:02:27,040 --> 00:02:29,770
underestimation I think it's like oh

00:02:29,140 --> 00:02:32,290
sorry

00:02:29,770 --> 00:02:34,420
60% fail Big Data projects was not an

00:02:32,290 --> 00:02:36,670
estimation maybe it's like more like 85%

00:02:34,420 --> 00:02:38,080
and I've seen some companies fail I've

00:02:36,670 --> 00:02:40,960
seen some companies succeed so I'm

00:02:38,080 --> 00:02:45,210
trying to convey the what I see as a

00:02:40,960 --> 00:02:47,560
difference this is a general blueprint

00:02:45,210 --> 00:02:49,480
this is the data tech conference I

00:02:47,560 --> 00:02:51,970
assume most of you are familiar with it

00:02:49,480 --> 00:02:53,890
you have services doing things on one

00:02:51,970 --> 00:02:55,510
hand you they emit events that you

00:02:53,890 --> 00:02:57,910
collect or they have stored data in

00:02:55,510 --> 00:03:00,040
databases and you collect that data put

00:02:57,910 --> 00:03:01,390
it into some kind of cold store role

00:03:00,040 --> 00:03:04,780
zone where you sort of keep it forever

00:03:01,390 --> 00:03:08,010
if it wasn't for GTR and then from there

00:03:04,780 --> 00:03:10,330
you build like processing pipelines

00:03:08,010 --> 00:03:15,250
Sparky loop whatever your favorite thing

00:03:10,330 --> 00:03:17,020
is and at the end there's a Negro stage

00:03:15,250 --> 00:03:20,680
where you take things out of your data

00:03:17,020 --> 00:03:23,890
lake and push it to make this make it

00:03:20,680 --> 00:03:25,600
useful yeah this is pictures focused on

00:03:23,890 --> 00:03:29,740
batch there is a stream equivalent I

00:03:25,600 --> 00:03:31,840
will come to that later in the talk so

00:03:29,740 --> 00:03:34,690
as you start out and this is the way

00:03:31,840 --> 00:03:36,730
that people started out years ago in

00:03:34,690 --> 00:03:38,560
some Sun and but it still happens is

00:03:36,730 --> 00:03:40,420
that you you write a little batch script

00:03:38,560 --> 00:03:42,220
or something and you take the data that

00:03:40,420 --> 00:03:44,830
you collected for a particular hour and

00:03:42,220 --> 00:03:48,820
you run spark on it because spark is a

00:03:44,830 --> 00:03:50,320
decent tool and then you want to do

00:03:48,820 --> 00:03:52,930
something on a daily basis in this

00:03:50,320 --> 00:03:54,430
example you want to count some before

00:03:52,930 --> 00:03:56,020
key performance index is you want to

00:03:54,430 --> 00:03:59,530
form sessions because if you want Pro

00:03:56,020 --> 00:04:04,240
product insights into your into your web

00:03:59,530 --> 00:04:07,240
shop or something so works fine for a

00:04:04,240 --> 00:04:09,130
while then you add your campaign

00:04:07,240 --> 00:04:11,740
management decides that you want some

00:04:09,130 --> 00:04:13,480
more information so you add some job to

00:04:11,740 --> 00:04:16,030
the write that is dependent on the stuff

00:04:13,480 --> 00:04:19,480
to the left in order to can to calculate

00:04:16,030 --> 00:04:22,150
the impact of your campaigns also works

00:04:19,480 --> 00:04:24,250
fine for a little while but then

00:04:22,150 --> 00:04:26,770
something goes wrong on the left and the

00:04:24,250 --> 00:04:29,140
dependent job on the right breaks not

00:04:26,770 --> 00:04:31,760
too bad you can you can go in you can

00:04:29,140 --> 00:04:34,160
rerun it manually but you start real

00:04:31,760 --> 00:04:37,760
that this doesn't scale beyond a few

00:04:34,160 --> 00:04:39,770
yobs that's even worse with other types

00:04:37,760 --> 00:04:42,200
of failures that might very well lead to

00:04:39,770 --> 00:04:43,400
silent data corruption so much later you

00:04:42,200 --> 00:04:44,900
will discover that you didn't have all

00:04:43,400 --> 00:04:48,920
the data and you were doing the wrong

00:04:44,900 --> 00:04:52,580
calculations this goes out of hand very

00:04:48,920 --> 00:04:54,380
quickly and the solution to this is

00:04:52,580 --> 00:04:55,910
something that diminishes your

00:04:54,380 --> 00:04:59,480
dependencies let's call the workflow

00:04:55,910 --> 00:05:02,690
Orchestrator now there are a bunch of

00:04:59,480 --> 00:05:05,090
them out there and if you only remember

00:05:02,690 --> 00:05:07,820
one thing from this talk that is pick up

00:05:05,090 --> 00:05:10,670
a good workflow Orchestrator that is the

00:05:07,820 --> 00:05:13,640
one of the keys to success unfortunately

00:05:10,670 --> 00:05:14,870
there are some some that our week out

00:05:13,640 --> 00:05:16,310
they're weak in the sense that you

00:05:14,870 --> 00:05:17,990
cannot express all the things that you

00:05:16,310 --> 00:05:20,630
need and you recognize them because they

00:05:17,990 --> 00:05:23,210
are using a non expressive languages as

00:05:20,630 --> 00:05:26,720
the DSL such as XML or graphical

00:05:23,210 --> 00:05:29,570
interfaces unfortunately many of the

00:05:26,720 --> 00:05:33,140
vendors ship the weak ones rather than

00:05:29,570 --> 00:05:35,630
the good ones Google is the shiny

00:05:33,140 --> 00:05:38,000
exception they recently four announced

00:05:35,630 --> 00:05:40,610
the managed airflow service airflow is

00:05:38,000 --> 00:05:42,500
one of the two good ones and the other

00:05:40,610 --> 00:05:45,890
ones called luigi both of these are

00:05:42,500 --> 00:05:49,610
perfectly fine tools if you're not using

00:05:45,890 --> 00:05:51,290
them have a look at them I tend to

00:05:49,610 --> 00:05:52,580
recommend Luigi to my clients I'm a

00:05:51,290 --> 00:05:55,610
little bit biased I used to work at

00:05:52,580 --> 00:05:57,320
Spotify where we made Luigi but it's

00:05:55,610 --> 00:05:59,120
much simpler it's it's easier to get

00:05:57,320 --> 00:06:00,590
started with it has a smaller scope I

00:05:59,120 --> 00:06:02,720
like simple too so they do one thing

00:06:00,590 --> 00:06:04,580
well and that leads to simpler

00:06:02,720 --> 00:06:06,320
operations and so forth but our flow has

00:06:04,580 --> 00:06:13,180
some some advantages as well so it's a

00:06:06,320 --> 00:06:16,310
trade-off all right off to number two

00:06:13,180 --> 00:06:19,070
Hadoop can do a whole lot of things it

00:06:16,310 --> 00:06:21,020
can look as a sequel database and it can

00:06:19,070 --> 00:06:23,180
even do transactions these days so

00:06:21,020 --> 00:06:24,710
that's great we don't need a sequel

00:06:23,180 --> 00:06:27,350
database anymore we can ditch our own

00:06:24,710 --> 00:06:32,690
ole Oracle database or when we want to

00:06:27,350 --> 00:06:35,690
just use Hadoop and we have really

00:06:32,690 --> 00:06:37,640
powerful tools that we can use queries

00:06:35,690 --> 00:06:40,550
all over the day lakes to bring out the

00:06:37,640 --> 00:06:43,910
data that we want in real time when we

00:06:40,550 --> 00:06:46,539
need it by querying a lot of data sets

00:06:43,910 --> 00:06:49,280
that sounds great

00:06:46,539 --> 00:06:52,490
now it's not so great because that's not

00:06:49,280 --> 00:06:55,160
what these Big Data technologies were

00:06:52,490 --> 00:06:57,289
meant for unlike the relational

00:06:55,160 --> 00:06:59,750
databases they're distributed and with

00:06:57,289 --> 00:07:01,669
disability technologies you sacrifice a

00:06:59,750 --> 00:07:03,050
whole lot of things in order to gain

00:07:01,669 --> 00:07:07,280
scalability and performance and

00:07:03,050 --> 00:07:11,180
redundancy now what you sacrifice is the

00:07:07,280 --> 00:07:13,520
multi purpose disability technology is a

00:07:11,180 --> 00:07:15,770
good at one thing typically one thing

00:07:13,520 --> 00:07:17,509
only or maybe two or three but with

00:07:15,770 --> 00:07:20,449
everything that you add you are

00:07:17,509 --> 00:07:22,430
sacrificing you know you are sacrificing

00:07:20,449 --> 00:07:25,819
quality

00:07:22,430 --> 00:07:27,650
so the Hadoop technologies and old

00:07:25,819 --> 00:07:30,099
surrounding technologies were made for

00:07:27,650 --> 00:07:32,780
offline processing not online processing

00:07:30,099 --> 00:07:35,150
so if you're using them for online just

00:07:32,780 --> 00:07:36,590
do as you used to do and bring in newest

00:07:35,150 --> 00:07:43,460
technologies you're actually making

00:07:36,590 --> 00:07:45,860
things worse and this sort of big data

00:07:43,460 --> 00:07:48,349
revolution that we're somewhere in the

00:07:45,860 --> 00:07:51,110
middle of is actually not about new cool

00:07:48,349 --> 00:07:52,729
technologies or lots of data yes there

00:07:51,110 --> 00:07:55,190
is some of that as well but that's not

00:07:52,729 --> 00:07:57,500
where I see companies getting most value

00:07:55,190 --> 00:08:01,190
will they get the most value from new

00:07:57,500 --> 00:08:03,370
ways of working and collaborating you

00:08:01,190 --> 00:08:05,630
instead of these peer-to-peer

00:08:03,370 --> 00:08:07,729
organizations on the Left where you have

00:08:05,630 --> 00:08:09,650
to talk to five different teams in order

00:08:07,729 --> 00:08:12,409
to get your data to do something

00:08:09,650 --> 00:08:13,820
innovative with data you pour down you

00:08:12,409 --> 00:08:15,620
have an organization that pours down all

00:08:13,820 --> 00:08:17,060
of the data and democratizes it

00:08:15,620 --> 00:08:18,560
internally in the organization which

00:08:17,060 --> 00:08:21,979
means that you can do it data innovation

00:08:18,560 --> 00:08:24,139
or faster either in for botching the de

00:08:21,979 --> 00:08:27,380
lake or stream storage for real-time

00:08:24,139 --> 00:08:29,569
processing the other big benefit is

00:08:27,380 --> 00:08:32,089
working in in pipelines where you save

00:08:29,569 --> 00:08:34,039
the raw data rather than say processing

00:08:32,089 --> 00:08:36,320
it first and then saying it so that you

00:08:34,039 --> 00:08:38,180
can go back if things go wrong and you

00:08:36,320 --> 00:08:41,779
can invent new things from the raw data

00:08:38,180 --> 00:08:43,669
and this gives a tremendous iteration

00:08:41,779 --> 00:08:46,040
speed and you really powers data

00:08:43,669 --> 00:08:51,740
innovation if you get it right if you

00:08:46,040 --> 00:08:55,250
are able to move quickly so say the data

00:08:51,740 --> 00:08:57,020
in one company we decided that we wanted

00:08:55,250 --> 00:08:59,029
to do important calculation it was

00:08:57,020 --> 00:08:59,209
important that we got it right so all of

00:08:59,029 --> 00:09:00,529
them

00:08:59,209 --> 00:09:05,059
nodes that were like saving data

00:09:00,529 --> 00:09:08,240
collecting data we asked them to to sort

00:09:05,059 --> 00:09:09,889
of push all the data into Hadoop and we

00:09:08,240 --> 00:09:12,110
only started the calculations once all

00:09:09,889 --> 00:09:15,949
the data was in there the problem is

00:09:12,110 --> 00:09:17,540
that all the data takes a while to

00:09:15,949 --> 00:09:19,309
figure out whether all the data is in

00:09:17,540 --> 00:09:20,899
there and when those started failing

00:09:19,309 --> 00:09:22,610
this took like longer and longer and

00:09:20,899 --> 00:09:24,619
longer and got more and more fragile so

00:09:22,610 --> 00:09:28,279
the more we scale out the worse the

00:09:24,619 --> 00:09:30,379
system rough and also this was good for

00:09:28,279 --> 00:09:31,970
reporting well really needed the good

00:09:30,379 --> 00:09:34,279
quality but if you just want to do

00:09:31,970 --> 00:09:35,809
dashboards of products inside quality is

00:09:34,279 --> 00:09:37,339
less important than getting results

00:09:35,809 --> 00:09:41,269
quickly so there is no good answer on

00:09:37,339 --> 00:09:43,610
how long you should wait so another

00:09:41,269 --> 00:09:45,829
company said ok we're not gonna wait

00:09:43,610 --> 00:09:47,720
we're gonna start processing and then

00:09:45,829 --> 00:09:50,179
we're going to monitor the late-teen

00:09:47,720 --> 00:09:52,009
coming data and when it once it reaches

00:09:50,179 --> 00:09:58,399
one two percent something we recalculate

00:09:52,009 --> 00:10:01,569
downstream now this has different

00:09:58,399 --> 00:10:04,189
disadvantages because the downstream

00:10:01,569 --> 00:10:06,079
calculations are no longer predictable

00:10:04,189 --> 00:10:08,600
and reproducible which is OK for some

00:10:06,079 --> 00:10:10,459
cases but not great if you want to do

00:10:08,600 --> 00:10:11,660
like machine learning or data science

00:10:10,459 --> 00:10:13,639
experiments where you tweak your

00:10:11,660 --> 00:10:17,439
algorithm and say did it get better no

00:10:13,639 --> 00:10:17,439
the data change so you don't really know

00:10:18,399 --> 00:10:23,540
on the tangential note on a different

00:10:20,899 --> 00:10:24,829
company the people doing the event

00:10:23,540 --> 00:10:27,139
collection said well we have this

00:10:24,829 --> 00:10:29,629
service that you IPG you look ups

00:10:27,139 --> 00:10:31,579
wouldn't all the events be better if we

00:10:29,629 --> 00:10:33,829
like decorated them with some geo

00:10:31,579 --> 00:10:36,199
information so let's do that before we

00:10:33,829 --> 00:10:38,779
save the data that was fine we got even

00:10:36,199 --> 00:10:41,540
better events until the PTO service

00:10:38,779 --> 00:10:43,040
broke down or did the wrong thing and

00:10:41,540 --> 00:10:45,230
we're now had some events with missing

00:10:43,040 --> 00:10:51,799
data or wrong data and that is painful

00:10:45,230 --> 00:10:53,629
to recover from anecdote number 4 we

00:10:51,799 --> 00:10:57,920
have some processing where we needed

00:10:53,629 --> 00:11:00,619
information in a database so Hadoop

00:10:57,920 --> 00:11:03,350
spoke they can go and look up things in

00:11:00,619 --> 00:11:05,089
databases to decorate the data so we

00:11:03,350 --> 00:11:08,540
just go to the production database and

00:11:05,089 --> 00:11:10,009
do that now if that if you're doing a

00:11:08,540 --> 00:11:11,660
large computation that might take down

00:11:10,009 --> 00:11:12,320
the production database but if you're

00:11:11,660 --> 00:11:15,440
doing a small

00:11:12,320 --> 00:11:17,450
a computation that's fine until you need

00:11:15,440 --> 00:11:20,120
to rerun it because then the production

00:11:17,450 --> 00:11:23,720
data has changed and your pipeline is no

00:11:20,120 --> 00:11:27,970
longer producible what do these four

00:11:23,720 --> 00:11:31,540
stories have in common they are

00:11:27,970 --> 00:11:33,980
deviating from functional principles

00:11:31,540 --> 00:11:36,500
now if you've learned functional

00:11:33,980 --> 00:11:38,450
programming you might have picked up

00:11:36,500 --> 00:11:40,660
principles of immutability once you've

00:11:38,450 --> 00:11:43,640
written something never change it

00:11:40,660 --> 00:11:46,370
idempotency right you should be able to

00:11:43,640 --> 00:11:48,290
repeat operations and reproduce ability

00:11:46,370 --> 00:11:52,670
right everything should be deterministic

00:11:48,290 --> 00:11:54,470
and what we do in functional programming

00:11:52,670 --> 00:11:56,330
not doesn't matter but this is this

00:11:54,470 --> 00:11:58,430
these are the same principles applied on

00:11:56,330 --> 00:12:00,680
an architecture level and following the

00:11:58,430 --> 00:12:01,520
following these principles makes your

00:12:00,680 --> 00:12:03,110
life a lot easier

00:12:01,520 --> 00:12:06,010
so once you've written data sets let me

00:12:03,110 --> 00:12:09,290
change them and make sure that your your

00:12:06,010 --> 00:12:10,880
execution is reproducible actually there

00:12:09,290 --> 00:12:12,950
are a whole bunch of exceptions to this

00:12:10,880 --> 00:12:14,960
reproduce ability in some cases it's

00:12:12,950 --> 00:12:17,300
worth sacrificing it for good reason but

00:12:14,960 --> 00:12:26,060
no when you are doing it and why you're

00:12:17,300 --> 00:12:28,190
doing it so the the solution to not go

00:12:26,060 --> 00:12:30,830
into in order to avoid going to the

00:12:28,190 --> 00:12:32,690
databases from jobs in production or

00:12:30,830 --> 00:12:35,330
production databases from from Hadoop

00:12:32,690 --> 00:12:37,640
jobs you can dump the databases on daily

00:12:35,330 --> 00:12:42,010
basis so for each day like you have a

00:12:37,640 --> 00:12:42,010
copy of the database that's great

00:12:42,110 --> 00:12:46,610
there are two sex parking scoop that can

00:12:44,000 --> 00:12:49,400
go out to a database and dump it so this

00:12:46,610 --> 00:12:51,740
is one of my favorite war stories we had

00:12:49,400 --> 00:12:53,270
a Cassandra cluster serving users was a

00:12:51,740 --> 00:12:56,780
big kiss under cluster like 40 or 50

00:12:53,270 --> 00:12:58,940
nodes something lots of users and we

00:12:56,780 --> 00:13:02,750
wanted that information on a daily basis

00:12:58,940 --> 00:13:04,880
and so we had a Hadoop cluster and this

00:13:02,750 --> 00:13:07,460
lot of data so we had like you know 20

00:13:04,880 --> 00:13:08,870
then 50 then 100 Hadoop machines going

00:13:07,460 --> 00:13:11,000
out to get all of that data so that

00:13:08,870 --> 00:13:12,770
created quite a load spike on the owner

00:13:11,000 --> 00:13:15,010
standard cluster Cassandra is very

00:13:12,770 --> 00:13:17,840
scalable so it could sustain the spike

00:13:15,010 --> 00:13:20,570
the spike became half an hour long in an

00:13:17,840 --> 00:13:22,580
hour and two hours and one day the spike

00:13:20,570 --> 00:13:24,339
got 25 hours long so we had a double

00:13:22,580 --> 00:13:26,959
spike you know

00:13:24,339 --> 00:13:28,060
that took down the login service the

00:13:26,959 --> 00:13:30,800
user people

00:13:28,060 --> 00:13:32,449
login service people were not happy so

00:13:30,800 --> 00:13:34,240
they put a network firewall in between

00:13:32,449 --> 00:13:36,019
the Hydra cluster and kazan cluster

00:13:34,240 --> 00:13:38,949
problem solved

00:13:36,019 --> 00:13:41,629
and we had to figure out a better way so

00:13:38,949 --> 00:13:43,879
I wasn't paying attention a couple years

00:13:41,629 --> 00:13:45,529
later somebody wanted to do the same

00:13:43,879 --> 00:13:47,420
thing this also with the login service

00:13:45,529 --> 00:13:49,519
and they decided not to go to the

00:13:47,420 --> 00:13:51,740
database but go to the nice little rest

00:13:49,519 --> 00:13:54,889
interface where you got very clean data

00:13:51,740 --> 00:13:56,660
and of course the same thing happened

00:13:54,889 --> 00:14:02,060
like this the the this cluster brought

00:13:56,660 --> 00:14:04,790
down the login service so this you can

00:14:02,060 --> 00:14:06,589
you can have a do pass as like a denial

00:14:04,790 --> 00:14:08,120
of service attack you this can happen on

00:14:06,589 --> 00:14:11,089
the egress side as well this is a

00:14:08,120 --> 00:14:13,129
recommendation scenario the people doing

00:14:11,089 --> 00:14:15,589
the recommendations they had this great

00:14:13,129 --> 00:14:17,060
new algorithm so they are computing big

00:14:15,589 --> 00:14:18,529
indexes for recommendations and they

00:14:17,060 --> 00:14:20,720
wanted to push this out to test it to

00:14:18,529 --> 00:14:22,730
the users so they dumped it all in the

00:14:20,720 --> 00:14:24,860
Cassandra cluster and Cassandra was

00:14:22,730 --> 00:14:26,269
replicated so Cassandra happily

00:14:24,860 --> 00:14:28,490
replicated this to the other data

00:14:26,269 --> 00:14:30,649
centers which took all of the cross

00:14:28,490 --> 00:14:32,329
Atlantic bandwidth which was needed to

00:14:30,649 --> 00:14:37,730
more much more important things such as

00:14:32,329 --> 00:14:40,309
serving users so forth so what we learn

00:14:37,730 --> 00:14:42,649
offline is the offline environment is

00:14:40,309 --> 00:14:44,300
very powerful it's a internal denial of

00:14:42,649 --> 00:14:46,209
service attack basically ready to kill

00:14:44,300 --> 00:14:48,259
your online environments at any moment

00:14:46,209 --> 00:14:50,839
so you want to separate these two

00:14:48,259 --> 00:14:53,899
environments right in the online things

00:14:50,839 --> 00:14:56,360
are really important if things break you

00:14:53,899 --> 00:14:57,829
have unhappy users out there and they

00:14:56,360 --> 00:14:59,750
give crappy feedback they shout on

00:14:57,829 --> 00:15:02,899
Twitter and things like this so you

00:14:59,750 --> 00:15:05,920
really need to like be proactive with

00:15:02,899 --> 00:15:08,120
making sure that these systems are up

00:15:05,920 --> 00:15:11,000
the impact is high so the proper

00:15:08,120 --> 00:15:13,399
built-in it's below the offline world

00:15:11,000 --> 00:15:16,069
you have internal people like like doing

00:15:13,399 --> 00:15:17,509
business insights and so forth there are

00:15:16,069 --> 00:15:19,519
a few of them and they are quite good at

00:15:17,509 --> 00:15:23,870
giving feedback because they come to

00:15:19,519 --> 00:15:25,399
your desk so things might go wrong right

00:15:23,870 --> 00:15:28,189
you you don't need to protect these

00:15:25,399 --> 00:15:30,079
things as well in the offline world so

00:15:28,189 --> 00:15:34,130
you can take a much higher risk and do

00:15:30,079 --> 00:15:37,220
your sort of quality assurance on a lazy

00:15:34,130 --> 00:15:38,689
basis when things when things go wrong

00:15:37,220 --> 00:15:40,399
you go in there and you do more Quality

00:15:38,689 --> 00:15:42,019
Assurance a much higher return on

00:15:40,399 --> 00:15:46,459
investment on on those type of coltish

00:15:42,019 --> 00:15:48,800
ones so what I learned is to separate

00:15:46,459 --> 00:15:50,660
the online world from the offline world

00:15:48,800 --> 00:15:53,120
the middle and also from the online

00:15:50,660 --> 00:15:54,740
world at the end and have very careful

00:15:53,120 --> 00:15:56,240
handovers in green so for dumping

00:15:54,740 --> 00:15:58,579
databases for example you can take a

00:15:56,240 --> 00:16:02,089
nightly backup of your database bring up

00:15:58,579 --> 00:16:03,980
an offline database and dump from that

00:16:02,089 --> 00:16:07,899
one that's not serving any users anymore

00:16:03,980 --> 00:16:09,769
and likewise if you're pushing like

00:16:07,899 --> 00:16:14,529
recommendation machine learning models

00:16:09,769 --> 00:16:17,120
out there throttle it and then keep

00:16:14,529 --> 00:16:19,279
several versions of these recommendation

00:16:17,120 --> 00:16:21,170
models or for detection models so you

00:16:19,279 --> 00:16:23,000
can switch back to an old one if the

00:16:21,170 --> 00:16:25,639
offline world fails you and you don't

00:16:23,000 --> 00:16:36,110
need get new models on a daily basis you

00:16:25,639 --> 00:16:39,230
still have a few old about so Ali is an

00:16:36,110 --> 00:16:41,240
old colleague of mine and he decided to

00:16:39,230 --> 00:16:43,939
do an academic career you know in the

00:16:41,240 --> 00:16:46,370
middle of the big data hype in like 2007

00:16:43,939 --> 00:16:49,279
eight or something he decided to go to

00:16:46,370 --> 00:16:51,110
academia and I said oh I work at Google

00:16:49,279 --> 00:16:53,000
time like Alec come to work with us

00:16:51,110 --> 00:16:55,399
we're doing super fun things no no no

00:16:53,000 --> 00:16:57,410
I'm gonna go to academia and then they

00:16:55,399 --> 00:16:59,329
built this thing called spark and he

00:16:57,410 --> 00:17:02,089
under by accident he ended up being CEO

00:16:59,329 --> 00:17:03,250
of the coolest startup one of the cooler

00:17:02,089 --> 00:17:06,380
startup lebra

00:17:03,250 --> 00:17:09,020
and he says like we have clients with

00:17:06,380 --> 00:17:11,150
petabytes and petabytes of data and they

00:17:09,020 --> 00:17:13,909
all want to do machine learning and I'm

00:17:11,150 --> 00:17:14,839
sure they have those clients in fact

00:17:13,909 --> 00:17:16,850
there are companies that generate

00:17:14,839 --> 00:17:19,429
petabytes of data even because they have

00:17:16,850 --> 00:17:21,039
like a billion users or they have things

00:17:19,429 --> 00:17:24,020
like jet engines that produce

00:17:21,039 --> 00:17:27,589
approximately as much data as a billion

00:17:24,020 --> 00:17:29,690
users now most companies don't have

00:17:27,589 --> 00:17:31,220
petabytes they have like gigabytes

00:17:29,690 --> 00:17:32,750
because they have uses the producer if

00:17:31,220 --> 00:17:34,460
you kilobytes per day and they are the

00:17:32,750 --> 00:17:35,960
number of users is limited by the you

00:17:34,460 --> 00:17:39,500
know the number of Swedish speakers or

00:17:35,960 --> 00:17:43,820
something so you actually don't need

00:17:39,500 --> 00:17:45,590
scale and your data might very well fit

00:17:43,820 --> 00:17:48,830
in memory well if you go for clusters

00:17:45,590 --> 00:17:51,020
anyway you you are taking a very large

00:17:48,830 --> 00:17:53,000
cost because in distributed systems

00:17:51,020 --> 00:17:54,580
lots and lots of things happen that

00:17:53,000 --> 00:17:58,400
never happened on single machine

00:17:54,580 --> 00:18:00,650
environment and you want to make sure

00:17:58,400 --> 00:18:04,429
that if you're going to cluster so

00:18:00,650 --> 00:18:06,590
scaling out that you get value for

00:18:04,429 --> 00:18:11,120
paying all for all of these problems and

00:18:06,590 --> 00:18:14,750
costs so mistake number farm is like

00:18:11,120 --> 00:18:18,080
scaling for imagine or or engineering

00:18:14,750 --> 00:18:19,820
for imaginary scaling problems your data

00:18:18,080 --> 00:18:22,040
is likely to fit a memory on the large

00:18:19,820 --> 00:18:26,450
cloud instances or at least a portion of

00:18:22,040 --> 00:18:30,260
your data remember the the value of big

00:18:26,450 --> 00:18:32,330
data is not about about sizes and

00:18:30,260 --> 00:18:33,470
necessarily machine learning either but

00:18:32,330 --> 00:18:37,370
about new ways of working in

00:18:33,470 --> 00:18:39,620
collaboration and enemy in the most

00:18:37,370 --> 00:18:41,750
successful project I've been in we're

00:18:39,620 --> 00:18:44,270
using spark but never in clusters we're

00:18:41,750 --> 00:18:47,179
only using it in local mode this is say

00:18:44,270 --> 00:18:49,100
a this is a company who has very little

00:18:47,179 --> 00:18:50,780
business outside of Sweden so we know we

00:18:49,100 --> 00:18:52,280
can't get more than 10 million users or

00:18:50,780 --> 00:18:59,750
something everything fits in memory only

00:18:52,280 --> 00:19:01,340
local mode works great so a client went

00:18:59,750 --> 00:19:04,220
came to me and said we have this issue

00:19:01,340 --> 00:19:06,410
that things are going slow by the day

00:19:04,220 --> 00:19:10,010
are you sure storing things in s3 is a

00:19:06,410 --> 00:19:13,070
good go strategy here and they pointed

00:19:10,010 --> 00:19:14,540
me to two blog post and that made me

00:19:13,070 --> 00:19:16,160
figure out what they were actually doing

00:19:14,540 --> 00:19:22,510
this blog post they have a job that

00:19:16,160 --> 00:19:24,800
reads like 1 year of data every day and

00:19:22,510 --> 00:19:27,140
there was another situation where we

00:19:24,800 --> 00:19:29,059
were having this large cluster and there

00:19:27,140 --> 00:19:30,650
was this system that was using like a

00:19:29,059 --> 00:19:32,809
large portion of the cluster all the

00:19:30,650 --> 00:19:34,700
time and we've figured out they must be

00:19:32,809 --> 00:19:36,770
doing something really important it

00:19:34,700 --> 00:19:39,410
turns out that they would be reading

00:19:36,770 --> 00:19:40,730
years and years of full raw data on a

00:19:39,410 --> 00:19:43,370
daily basis hundreds of terabytes

00:19:40,730 --> 00:19:44,929
computing something and then dropping it

00:19:43,370 --> 00:19:49,880
all so the next day they would read

00:19:44,929 --> 00:19:52,220
hundreds of terabytes again so usually

00:19:49,880 --> 00:19:55,820
you don't need to do that and it's

00:19:52,220 --> 00:19:57,830
better to change your algorithms so that

00:19:55,820 --> 00:20:00,380
they are incremental so you can process

00:19:57,830 --> 00:20:02,679
the new data and do as much as you want

00:20:00,380 --> 00:20:04,850
or as much as you can

00:20:02,679 --> 00:20:07,670
typically aggregate on the daily

00:20:04,850 --> 00:20:11,320
basis and then even if you want results

00:20:07,670 --> 00:20:14,780
on a regular base results for more than

00:20:11,320 --> 00:20:16,610
for a long logic time period you do only

00:20:14,780 --> 00:20:19,220
the final part of the computation so in

00:20:16,610 --> 00:20:22,130
this example let's say you want to make

00:20:19,220 --> 00:20:24,260
a top list of the of the most popular

00:20:22,130 --> 00:20:26,990
countries where you're a country where

00:20:24,260 --> 00:20:31,340
your customers are or the countries that

00:20:26,990 --> 00:20:33,560
plays the most orders and you want to do

00:20:31,340 --> 00:20:35,750
that on a daily basis or the rolling one

00:20:33,560 --> 00:20:38,300
year window if you do it the nay way

00:20:35,750 --> 00:20:41,120
you'd read all of the raw data I join

00:20:38,300 --> 00:20:42,950
with you use a database and produce the

00:20:41,120 --> 00:20:45,440
top list every day if you do it in an

00:20:42,950 --> 00:20:47,510
incremental way you would each day count

00:20:45,440 --> 00:20:49,280
the number of users in each country so

00:20:47,510 --> 00:20:51,140
you join with use a database and you do

00:20:49,280 --> 00:20:53,540
the heavy computation day by day and

00:20:51,140 --> 00:20:56,300
then only in the end to do form the top

00:20:53,540 --> 00:20:59,060
list you need to do you take the

00:20:56,300 --> 00:21:00,340
aggregates this is lots of data this is

00:20:59,060 --> 00:21:08,290
a small amount of data

00:21:00,340 --> 00:21:12,320
this is compressible okay splendid so

00:21:08,290 --> 00:21:14,060
the data lake has has like a real time

00:21:12,320 --> 00:21:15,890
variant as well it's called unified log

00:21:14,060 --> 00:21:17,750
instead of putting it all those things

00:21:15,890 --> 00:21:19,280
in and batches of files in your lake you

00:21:17,750 --> 00:21:21,920
put you put it in a stream so that

00:21:19,280 --> 00:21:23,030
everybody can consume this is the data

00:21:21,920 --> 00:21:25,970
take conference you're very familiar

00:21:23,030 --> 00:21:28,040
with it you can do similar pipelines

00:21:25,970 --> 00:21:31,610
like with stream processing that puts

00:21:28,040 --> 00:21:33,650
spits out data to new streams and do the

00:21:31,610 --> 00:21:37,520
same thing but but in real-time is that

00:21:33,650 --> 00:21:39,920
a batch and surely this must be much

00:21:37,520 --> 00:21:42,760
better I mean you get you don't have to

00:21:39,920 --> 00:21:46,040
wait to the next day to get results you

00:21:42,760 --> 00:21:49,790
some things you think need to be quick

00:21:46,040 --> 00:21:52,940
and in particularly the demos look much

00:21:49,790 --> 00:21:54,800
better right and you've heard some some

00:21:52,940 --> 00:21:59,270
cool companies say that these this is to

00:21:54,800 --> 00:22:01,430
what the cool guys are doing now this

00:21:59,270 --> 00:22:04,820
comes with the cost and this cost is

00:22:01,430 --> 00:22:08,000
called operations if you change things

00:22:04,820 --> 00:22:10,490
in your streaming environment you need

00:22:08,000 --> 00:22:12,830
to be much more careful so if you change

00:22:10,490 --> 00:22:15,860
your schema for example you need to do

00:22:12,830 --> 00:22:18,480
it in a careful manner because if the

00:22:15,860 --> 00:22:21,660
you're spinning on a data

00:22:18,480 --> 00:22:24,780
downstream pipelines or downstream jobs

00:22:21,660 --> 00:22:25,890
get get confused by the new data they

00:22:24,780 --> 00:22:27,450
will crash and then you will have an

00:22:25,890 --> 00:22:29,280
operational problem if you do the same

00:22:27,450 --> 00:22:31,110
thing in batch they will crash as well

00:22:29,280 --> 00:22:32,400
the batch is very forgiving because

00:22:31,110 --> 00:22:34,169
there are things like the workflow

00:22:32,400 --> 00:22:36,299
Orchestrator that will retry your failed

00:22:34,169 --> 00:22:41,450
jobs so once you fix that you recover

00:22:36,299 --> 00:22:44,580
again there are ways to deal with this

00:22:41,450 --> 00:22:46,740
this is scenario is more difficult if

00:22:44,580 --> 00:22:49,590
you have a bug and you spit out invalid

00:22:46,740 --> 00:22:52,710
data then your events will spread out

00:22:49,590 --> 00:22:56,309
through all of your pipelines and some

00:22:52,710 --> 00:22:58,500
of them will be bad and is instead of

00:22:56,309 --> 00:23:01,470
having you know 10 batches of bad things

00:22:58,500 --> 00:23:03,750
you have like 10 million events that are

00:23:01,470 --> 00:23:05,130
bad so you have to reach out to figure

00:23:03,750 --> 00:23:07,190
out which ones are the bad and try to

00:23:05,130 --> 00:23:09,450
discover myself this isn't much more

00:23:07,190 --> 00:23:11,820
difficult scenario to recover from and

00:23:09,450 --> 00:23:16,590
there are no tools to really help you

00:23:11,820 --> 00:23:18,120
here if you compare this with batch and

00:23:16,590 --> 00:23:20,100
take the operational scenario where you

00:23:18,120 --> 00:23:22,350
have a faulty job up there and it

00:23:20,100 --> 00:23:25,220
produces some like faulty recommendation

00:23:22,350 --> 00:23:28,530
in mixes or something what you do is you

00:23:25,220 --> 00:23:31,080
realize oh no the indexes are bad let's

00:23:28,530 --> 00:23:32,610
revert to the old ones because you say

00:23:31,080 --> 00:23:36,240
that you're saving a few copies of the

00:23:32,610 --> 00:23:38,520
old ones right now you fix your bug and

00:23:36,240 --> 00:23:40,559
then you just remove the faulty data

00:23:38,520 --> 00:23:42,000
sets which are I could proximately ten

00:23:40,559 --> 00:23:44,010
of them you can look in your workflow

00:23:42,000 --> 00:23:46,110
Orchestrator because it keeps a

00:23:44,010 --> 00:23:48,000
dependency tree so you know which they

00:23:46,110 --> 00:23:50,280
are you can build tooling for this but

00:23:48,000 --> 00:23:52,520
it's it's people haven't because it's

00:23:50,280 --> 00:23:55,820
fairly easy to do and then you're done

00:23:52,520 --> 00:23:58,020
nothing more operationally to do right

00:23:55,820 --> 00:23:59,730
because the workflow castrator we'll see

00:23:58,020 --> 00:24:01,890
though some datasets are missing and

00:23:59,730 --> 00:24:03,299
will refill for me at least if you use

00:24:01,890 --> 00:24:06,200
Luigi with our flow it's a bit more

00:24:03,299 --> 00:24:08,870
complicated but it's still not difficult

00:24:06,200 --> 00:24:12,450
this means that you can recover from

00:24:08,870 --> 00:24:15,059
programming errors in like about 30

00:24:12,450 --> 00:24:19,380
minutes even though they were in

00:24:15,059 --> 00:24:21,570
production and this speed gives immense

00:24:19,380 --> 00:24:25,980
power to your data innovation but you

00:24:21,570 --> 00:24:29,340
only get this much this is a confluence

00:24:25,980 --> 00:24:30,890
blog post on how to recover from a

00:24:29,340 --> 00:24:34,400
similar scenario in

00:24:30,890 --> 00:24:36,230
with Kafka streams it's a lot I don't

00:24:34,400 --> 00:24:38,780
know it's it's actually a really good

00:24:36,230 --> 00:24:41,660
blog post they described very well they

00:24:38,780 --> 00:24:43,400
do fantastic work but it all it doesn't

00:24:41,660 --> 00:24:45,080
cover the pipe lesson I read only it

00:24:43,400 --> 00:24:49,820
only works for if you have a single job

00:24:45,080 --> 00:24:52,160
and not a full pipeline of jobs so by

00:24:49,820 --> 00:24:53,780
all means go for stream processing but

00:24:52,160 --> 00:24:55,760
make sure that you pay a price in terms

00:24:53,780 --> 00:24:58,790
of operations so make sure that you have

00:24:55,760 --> 00:25:01,220
a use case for it right or that you

00:24:58,790 --> 00:25:02,960
perhaps you have a case where the data

00:25:01,220 --> 00:25:05,150
call it is not super important to you're

00:25:02,960 --> 00:25:07,640
only feeding dashboards or something

00:25:05,150 --> 00:25:10,040
that's fine now the good news is that

00:25:07,640 --> 00:25:11,540
this gap is decreasing because the

00:25:10,040 --> 00:25:12,590
tooling is getting better confluent is

00:25:11,540 --> 00:25:15,290
doing a fantastic job

00:25:12,590 --> 00:25:17,360
on pushing this to the world and so and

00:25:15,290 --> 00:25:20,809
I see some things coming out of Google

00:25:17,360 --> 00:25:22,400
as well so hopefully you know like five

00:25:20,809 --> 00:25:30,260
years or something I can't be standing

00:25:22,400 --> 00:25:33,230
here and saying this all right some

00:25:30,260 --> 00:25:35,270
sometimes the data looks wrong and you

00:25:33,230 --> 00:25:38,360
discover that after a while I was in a

00:25:35,270 --> 00:25:40,100
team where we really cared about the

00:25:38,360 --> 00:25:42,140
data quality really cared about getting

00:25:40,100 --> 00:25:43,940
things right and we were taking lots and

00:25:42,140 --> 00:25:46,610
lots of data from from the rest of the

00:25:43,940 --> 00:25:48,590
company and so the manager called us the

00:25:46,610 --> 00:25:50,179
sewer of the company because everything

00:25:48,590 --> 00:25:54,250
that people who were doing wrong or

00:25:50,179 --> 00:25:58,160
hazily upstream like we had to deal with

00:25:54,250 --> 00:25:59,840
at one point we saw that well wait a

00:25:58,160 --> 00:26:01,700
minute this content type is supposed to

00:25:59,840 --> 00:26:03,410
be like text or audio or video or

00:26:01,700 --> 00:26:05,000
something in one day was a content type

00:26:03,410 --> 00:26:06,799
 in some of the messages it

00:26:05,000 --> 00:26:08,870
turned out somebody pushed like an

00:26:06,799 --> 00:26:10,429
experiment to production upstream and

00:26:08,870 --> 00:26:18,169
they never noticed that we of course

00:26:10,429 --> 00:26:20,030
noticed so unless you are proactive with

00:26:18,169 --> 00:26:23,960
monitoring quality like this you will

00:26:20,030 --> 00:26:25,700
find problems weeks months years later

00:26:23,960 --> 00:26:29,929
and then they are much more expensive to

00:26:25,700 --> 00:26:32,150
deal with and there are four quality

00:26:29,929 --> 00:26:34,010
dimensions to care about timeliness is

00:26:32,150 --> 00:26:35,419
producing results on time that that

00:26:34,010 --> 00:26:36,919
one's not so difficult because if you

00:26:35,419 --> 00:26:39,950
don't produce things on time people

00:26:36,919 --> 00:26:40,970
explain correctness is about doing

00:26:39,950 --> 00:26:43,550
getting in the computations right

00:26:40,970 --> 00:26:44,600
testing can help you here so that one's

00:26:43,550 --> 00:26:46,370
fairly straight code

00:26:44,600 --> 00:26:49,039
completeness is more difficult that's

00:26:46,370 --> 00:26:50,840
but making sure that all of the data you

00:26:49,039 --> 00:26:51,889
wanted to compute on all of the things

00:26:50,840 --> 00:26:53,690
that happen during the month are

00:26:51,889 --> 00:26:56,149
actually in this month's data

00:26:53,690 --> 00:26:57,559
likewise consistency if you if you have

00:26:56,149 --> 00:26:59,360
two data sets that are supposed to

00:26:57,559 --> 00:27:01,820
contain the same things like to reports

00:26:59,360 --> 00:27:04,519
pointing to the same thing do they

00:27:01,820 --> 00:27:06,799
really have the same input data and the

00:27:04,519 --> 00:27:08,750
latter two you need to monitor for there

00:27:06,799 --> 00:27:12,950
are no tools or things out there that

00:27:08,750 --> 00:27:15,080
can help you so the mistake is writing

00:27:12,950 --> 00:27:18,350
your pipelines and then be happy you

00:27:15,080 --> 00:27:20,929
need to monitor the contents and you

00:27:18,350 --> 00:27:23,509
have a good one good tool is called

00:27:20,929 --> 00:27:27,320
counters or aggregators or accumulators

00:27:23,509 --> 00:27:29,690
so whenever you code and you see well

00:27:27,320 --> 00:27:31,460
this item here it's supposed to match a

00:27:29,690 --> 00:27:33,919
user in the database what if it doesn't

00:27:31,460 --> 00:27:35,149
ask yourself right and if it doesn't

00:27:33,919 --> 00:27:36,889
bump the counter

00:27:35,149 --> 00:27:39,440
don't do anything smart just bump the

00:27:36,889 --> 00:27:41,690
counter so that you can monitor whether

00:27:39,440 --> 00:27:43,039
things match your expectations or not

00:27:41,690 --> 00:27:47,210
that's how we found out about the

00:27:43,039 --> 00:27:48,500
contents like incident you need

00:27:47,210 --> 00:27:51,110
to go a bit further than that and

00:27:48,500 --> 00:27:52,580
actually measure consistency between the

00:27:51,110 --> 00:27:53,889
data sets and between the records and

00:27:52,580 --> 00:27:56,659
the data sets and so forth

00:27:53,889 --> 00:27:57,350
likewise here there are no tools to help

00:27:56,659 --> 00:27:59,779
you as well

00:27:57,350 --> 00:28:02,899
and the counters is one reason that

00:27:59,779 --> 00:28:05,149
we're I reckon it's one reason that I'd

00:28:02,899 --> 00:28:06,769
recommend people to stay off of sequel

00:28:05,149 --> 00:28:07,580
processing for for really important

00:28:06,769 --> 00:28:10,159
pipelines because

00:28:07,580 --> 00:28:11,929
sequel makes you forget about these

00:28:10,159 --> 00:28:14,120
things there are no natural places to

00:28:11,929 --> 00:28:18,759
put in these counters or compensate for

00:28:14,120 --> 00:28:20,690
that all right things break and

00:28:18,759 --> 00:28:22,009
sometimes you do something wrong

00:28:20,690 --> 00:28:24,169
sometimes somebody else has done

00:28:22,009 --> 00:28:29,659
something and then in formulas oh so

00:28:24,169 --> 00:28:32,779
things break the fact here is that you

00:28:29,659 --> 00:28:34,730
become reluctant and you become careful

00:28:32,779 --> 00:28:35,750
oh I'm not sure I won't change this

00:28:34,730 --> 00:28:40,850
because it might break something

00:28:35,750 --> 00:28:43,730
downstream and we had a situation where

00:28:40,850 --> 00:28:45,679
we were the old pipeline was no good so

00:28:43,730 --> 00:28:47,690
we built a new pipeline and that was

00:28:45,679 --> 00:28:50,950
really quick it's you can move things

00:28:47,690 --> 00:28:53,690
you can innovate quickly unfortunately

00:28:50,950 --> 00:28:55,370
changing and removing old things takes a

00:28:53,690 --> 00:28:58,080
lot of time so we have the old on

00:28:55,370 --> 00:29:00,750
running for like 18 months or so

00:28:58,080 --> 00:29:05,910
and that that old one was a lot of

00:29:00,750 --> 00:29:07,980
burden to us so the problem is that

00:29:05,910 --> 00:29:09,660
these changes that you want to make are

00:29:07,980 --> 00:29:11,700
often in different teams so your your

00:29:09,660 --> 00:29:15,300
it's difficult to go across the team

00:29:11,700 --> 00:29:18,870
boundaries so what do you have what you

00:29:15,300 --> 00:29:21,000
end up with is an inability to move

00:29:18,870 --> 00:29:23,850
quickly we measured this in one company

00:29:21,000 --> 00:29:26,520
we concluded that if we if we collect a

00:29:23,850 --> 00:29:28,830
new field at the client the average time

00:29:26,520 --> 00:29:30,930
for this to be used at the end was like

00:29:28,830 --> 00:29:35,010
one month and this is this is a good

00:29:30,930 --> 00:29:38,220
company like very agile independent

00:29:35,010 --> 00:29:39,930
teams and so forth right I've also seen

00:29:38,220 --> 00:29:43,250
teams where you don't have the

00:29:39,930 --> 00:29:45,960
communication channels and there's no

00:29:43,250 --> 00:29:49,260
there's no upper limit to how long time

00:29:45,960 --> 00:29:51,450
it takes to propagate new data from

00:29:49,260 --> 00:29:52,890
upstream I've also seen environments

00:29:51,450 --> 00:29:54,690
where this is technically tightly

00:29:52,890 --> 00:29:58,590
coordinated and then you can move on the

00:29:54,690 --> 00:30:00,030
order of days this significantly impacts

00:29:58,590 --> 00:30:03,780
you're able to to innovate as a

00:30:00,030 --> 00:30:06,600
different at the high speed the remedy

00:30:03,780 --> 00:30:09,540
the only remedy I am aware of is to do

00:30:06,600 --> 00:30:11,130
testing unturned so there you have a

00:30:09,540 --> 00:30:12,420
safe testing environment so you know

00:30:11,130 --> 00:30:15,060
when you break things then you can move

00:30:12,420 --> 00:30:16,710
fast but this is often against the

00:30:15,060 --> 00:30:20,300
company culture if you have autonomous

00:30:16,710 --> 00:30:20,300
teams the this can be difficult to do

00:30:20,420 --> 00:30:25,080
all right you've heard about the

00:30:23,490 --> 00:30:28,110
functional principles like you do

00:30:25,080 --> 00:30:29,790
imitability and you in order to be

00:30:28,110 --> 00:30:31,350
reproducible you dump all of the user

00:30:29,790 --> 00:30:33,150
data basis everywhere so you have

00:30:31,350 --> 00:30:36,390
thousands of copies of the user database

00:30:33,150 --> 00:30:37,860
and so forth and this is all great until

00:30:36,390 --> 00:30:39,930
somebody knocks on your shoulder and

00:30:37,860 --> 00:30:44,280
says what about privacy what about

00:30:39,930 --> 00:30:46,350
erasing users okay wait a minute and the

00:30:44,280 --> 00:30:49,380
most expensive engineering mistake that

00:30:46,350 --> 00:30:51,210
I've seen was most made in this area it

00:30:49,380 --> 00:30:53,460
turns out that takes a lot of effort to

00:30:51,210 --> 00:30:55,080
wash petabytes of data so you don't want

00:30:53,460 --> 00:31:00,690
to be in a situation where you haven't

00:30:55,080 --> 00:31:04,080
planned for this likewise if you allow

00:31:00,690 --> 00:31:07,160
your team's to do like to choose

00:31:04,080 --> 00:31:10,830
arbitrarily anything autonomous teams

00:31:07,160 --> 00:31:11,910
and they are supposed to be able to

00:31:10,830 --> 00:31:14,310
deploy really quickly

00:31:11,910 --> 00:31:16,860
without coordinating with with lots of

00:31:14,310 --> 00:31:20,310
people you end up in a situation where

00:31:16,860 --> 00:31:22,380
you have variance in things that don't

00:31:20,310 --> 00:31:25,080
really matter such as time format I

00:31:22,380 --> 00:31:26,670
think I counted to somewhere around 25

00:31:25,080 --> 00:31:29,820
different formats and one that Dale

00:31:26,670 --> 00:31:31,470
awake and some of them were really crazy

00:31:29,820 --> 00:31:34,700
but we couldn't change that because we

00:31:31,470 --> 00:31:37,830
were lacking the internal testing and

00:31:34,700 --> 00:31:40,530
also I had this situation where the I

00:31:37,830 --> 00:31:42,960
was writing pipeline and I was trying to

00:31:40,530 --> 00:31:44,250
use beam and I was happily writing along

00:31:42,960 --> 00:31:45,930
and when the job was done turned out

00:31:44,250 --> 00:31:47,430
that the input data was actually per K

00:31:45,930 --> 00:31:49,650
and I never looked because it was like

00:31:47,430 --> 00:31:51,650
ever everywhere so yeah guess I could

00:31:49,650 --> 00:31:54,320
have looked but this is something this

00:31:51,650 --> 00:31:57,030
needless variance is something that adds

00:31:54,320 --> 00:32:04,110
friction to your to your innovation

00:31:57,030 --> 00:32:07,920
speed so what you need to do there are

00:32:04,110 --> 00:32:09,420
some things that you need that you are

00:32:07,920 --> 00:32:10,860
better off planning early and I'm sorry

00:32:09,420 --> 00:32:12,810
through throwing like an enterprise

00:32:10,860 --> 00:32:15,300
buzzword at you here

00:32:12,810 --> 00:32:16,650
my definition of governance is the

00:32:15,300 --> 00:32:19,290
things that you should have done

00:32:16,650 --> 00:32:21,630
proactively so that would have prevented

00:32:19,290 --> 00:32:23,940
you from from a painful situation later

00:32:21,630 --> 00:32:27,000
and you can put anything in the housing

00:32:23,940 --> 00:32:28,950
one and there are a couple of types of

00:32:27,000 --> 00:32:32,010
painful situations that you can be in

00:32:28,950 --> 00:32:34,110
the ones to the right or like risk

00:32:32,010 --> 00:32:35,850
situations you found that you are not

00:32:34,110 --> 00:32:38,280
compliant or you have insufficient

00:32:35,850 --> 00:32:40,860
security or whatever so that's that's a

00:32:38,280 --> 00:32:44,550
way of risk mitigation once to the left

00:32:40,860 --> 00:32:47,280
are all about your innovation speed your

00:32:44,550 --> 00:32:48,930
ability to innovate with data quickly

00:32:47,280 --> 00:32:52,200
we've been through the development

00:32:48,930 --> 00:32:55,350
speeded quite a bit I want to point out

00:32:52,200 --> 00:32:58,530
this one I've seen a whole bunch of

00:32:55,350 --> 00:33:00,090
teams that say oh data is valuable

00:32:58,530 --> 00:33:02,340
that's great I have a lot of data

00:33:00,090 --> 00:33:03,510
it's my data and they didn't want to

00:33:02,340 --> 00:33:06,390
share that they don't want to put in

00:33:03,510 --> 00:33:09,300
lake or in the stream now it turns out

00:33:06,390 --> 00:33:12,270
that a bit of governance can loosen that

00:33:09,300 --> 00:33:13,770
up so if you say well you're not not

00:33:12,270 --> 00:33:15,480
actually putting it in the lake and then

00:33:13,770 --> 00:33:17,760
everybody can do anything on it there

00:33:15,480 --> 00:33:19,620
are rules here in the lake and one of

00:33:17,760 --> 00:33:21,060
these rules can be they need to go and

00:33:19,620 --> 00:33:23,730
talk to you before they use your data

00:33:21,060 --> 00:33:25,149
right so that you limit the degrees of

00:33:23,730 --> 00:33:27,219
freedom in

00:33:25,149 --> 00:33:29,169
order to make people want to share and

00:33:27,219 --> 00:33:31,779
feel comfortable that sharing right so

00:33:29,169 --> 00:33:38,619
rules and degrees of freedom can in it

00:33:31,779 --> 00:33:42,279
can increase your speed so if you look

00:33:38,619 --> 00:33:45,009
back there a couple of themes here one

00:33:42,279 --> 00:33:48,909
is up in the right corner is to sort of

00:33:45,009 --> 00:33:51,039
gravitate towards complexity more than

00:33:48,909 --> 00:33:53,679
protecting business value and working

00:33:51,039 --> 00:33:57,190
towards business value the other one is

00:33:53,679 --> 00:34:00,159
to sort of gravitate towards interesting

00:33:57,190 --> 00:34:03,309
technology or new technology more than

00:34:00,159 --> 00:34:05,409
the sort of the principles of Big Data

00:34:03,309 --> 00:34:08,200
and principles of pipeline and data

00:34:05,409 --> 00:34:13,059
sharing data democratize democracy and

00:34:08,200 --> 00:34:14,859
so forth and I find myself saying to

00:34:13,059 --> 00:34:17,710
clients over and over again a whole

00:34:14,859 --> 00:34:20,260
number of things if I keep things simple

00:34:17,710 --> 00:34:21,789
scope down as much as you can focus on

00:34:20,260 --> 00:34:23,289
the value what's the value of this

00:34:21,789 --> 00:34:25,779
pipeline that we're building or this

00:34:23,289 --> 00:34:28,359
technology we're building and you can

00:34:25,779 --> 00:34:31,389
have hyper profit for each actually you

00:34:28,359 --> 00:34:32,950
think that you cannot both the these the

00:34:31,389 --> 00:34:34,480
efforts that you have if you go for the

00:34:32,950 --> 00:34:39,700
hype things I have to go for the profit

00:34:34,480 --> 00:34:41,409
things are completely different you

00:34:39,700 --> 00:34:42,669
might notice that none of the things

00:34:41,409 --> 00:34:50,710
that I say have anything to do with

00:34:42,669 --> 00:34:54,730
technology so here are some links the

00:34:50,710 --> 00:34:57,940
upper three ones are similar talks but

00:34:54,730 --> 00:35:00,339
less technical more about building teams

00:34:57,940 --> 00:35:02,710
and organizations and so forth

00:35:00,339 --> 00:35:04,539
if you found some of the things that I

00:35:02,710 --> 00:35:06,730
said here confusing or you want to more

00:35:04,539 --> 00:35:10,210
know about the context I suggest you go

00:35:06,730 --> 00:35:12,190
for the reading list and the last one is

00:35:10,210 --> 00:35:19,660
some of my old presentations from from

00:35:12,190 --> 00:35:20,559
other conferences questions yeah thank

00:35:19,660 --> 00:35:26,980
you very much

00:35:20,559 --> 00:35:28,809
anybody has a question so I have one

00:35:26,980 --> 00:35:32,500
question what do you think from your

00:35:28,809 --> 00:35:35,960
experience what are the 10 mistakes pose

00:35:32,500 --> 00:35:44,510
the most biggest problem

00:35:35,960 --> 00:35:47,450
you have experience well I think that

00:35:44,510 --> 00:35:50,450
the picking up the new patterns the the

00:35:47,450 --> 00:35:53,540
pipelines and the collaboration is where

00:35:50,450 --> 00:35:57,290
you have the most profit if you come

00:35:53,540 --> 00:35:58,730
from like a legacy environment so and I

00:35:57,290 --> 00:36:00,830
think that's one of the reasons where I

00:35:58,730 --> 00:36:02,570
see where we see so many Enterprise

00:36:00,830 --> 00:36:04,580
projects fail right they're just

00:36:02,570 --> 00:36:07,330
bringing in new technology and then

00:36:04,580 --> 00:36:12,770
they're just making things worse

00:36:07,330 --> 00:36:14,750
likewise this one is the workflow

00:36:12,770 --> 00:36:16,910
Orchestrator is yes it's new technology

00:36:14,750 --> 00:36:19,880
but it's super simple and it really

00:36:16,910 --> 00:36:22,310
saves your sanity right and it makes you

00:36:19,880 --> 00:36:28,339
think in terms of pipelines and in terms

00:36:22,310 --> 00:36:29,060
of the delivering value the internal

00:36:28,339 --> 00:36:31,280
agility

00:36:29,060 --> 00:36:33,950
that's mistake everybody makes right i

00:36:31,280 --> 00:36:36,410
I've seen I only see few companies

00:36:33,950 --> 00:36:38,900
actually be good at this and they are

00:36:36,410 --> 00:36:43,640
very coordinated every they have a very

00:36:38,900 --> 00:36:47,290
specific company culture so so you can

00:36:43,640 --> 00:36:47,290
you can thrive without the tree a

00:36:51,820 --> 00:36:57,260
mistake is more costly that depends on

00:36:54,619 --> 00:36:59,510
your environment right if you're in an

00:36:57,260 --> 00:37:01,369
enterprise environment this one is bound

00:36:59,510 --> 00:37:03,560
to be costly because you will set out to

00:37:01,369 --> 00:37:05,630
do it big project we I've seen a bunch

00:37:03,560 --> 00:37:08,750
of like data like projects and then go

00:37:05,630 --> 00:37:10,430
on for years without and then then they

00:37:08,750 --> 00:37:13,130
start producing value once they've sort

00:37:10,430 --> 00:37:14,930
of picked important things up so you can

00:37:13,130 --> 00:37:17,240
spend a lot of money there but if you're

00:37:14,930 --> 00:37:18,830
a start-up that's not where you start

00:37:17,240 --> 00:37:22,220
right so it depends on your context

00:37:18,830 --> 00:37:25,280
Steve great talk laws purely

00:37:22,220 --> 00:37:26,930
hypothetically imagine you've got

00:37:25,280 --> 00:37:29,420
multiple data sources one of them is

00:37:26,930 --> 00:37:31,070
using one of those random time sources

00:37:29,420 --> 00:37:31,940
and they're an hour out so your data's

00:37:31,070 --> 00:37:34,060
coming out wrong

00:37:31,940 --> 00:37:36,440
how would you go about debugging that

00:37:34,060 --> 00:37:38,240
sorry I don't understand how are you to

00:37:36,440 --> 00:37:40,339
go about debugging say one of your data

00:37:38,240 --> 00:37:42,260
sources is slightly dirty so your

00:37:40,339 --> 00:37:44,800
answers are coming in they're out how do

00:37:42,260 --> 00:37:48,470
you you know begin to go backwards from

00:37:44,800 --> 00:37:49,910
bad results to from bad results - too

00:37:48,470 --> 00:37:53,120
bad incoming day

00:37:49,910 --> 00:37:54,740
yeah yeah I've done that on a number of

00:37:53,120 --> 00:38:00,400
occasions in sovereignty more we care

00:37:54,740 --> 00:38:04,930
about the results there are no shortcuts

00:38:00,400 --> 00:38:08,000
so once you end up with the bad results

00:38:04,930 --> 00:38:11,270
you just have to debug step by step but

00:38:08,000 --> 00:38:14,030
from there you can improve right so so

00:38:11,270 --> 00:38:16,040
this was in it in a company where we had

00:38:14,030 --> 00:38:18,500
a strong tradition of learning from

00:38:16,040 --> 00:38:21,260
mistakes and doing post-mortems and so

00:38:18,500 --> 00:38:23,060
forth and in those I mean these kinds of

00:38:21,260 --> 00:38:24,860
issues were always post-mortems because

00:38:23,060 --> 00:38:28,190
they we cared about that that particular

00:38:24,860 --> 00:38:31,010
those particular data sets so we would

00:38:28,190 --> 00:38:32,750
do you know five whys and see what went

00:38:31,010 --> 00:38:33,980
wrong why did it go wrong how can we

00:38:32,750 --> 00:38:35,990
prevent this from going wrong in the

00:38:33,980 --> 00:38:37,910
future and we would include all of the

00:38:35,990 --> 00:38:40,160
teams affected all the way out to the to

00:38:37,910 --> 00:38:42,500
the data collection and the client right

00:38:40,160 --> 00:38:44,540
and see so you must improve here in

00:38:42,500 --> 00:38:47,630
order for us to get the get the reports

00:38:44,540 --> 00:38:50,290
right and then this also was in the

00:38:47,630 --> 00:38:52,640
company where we had a very transparent

00:38:50,290 --> 00:38:53,330
culture so the results of the

00:38:52,640 --> 00:38:56,660
post-mortems

00:38:53,330 --> 00:38:58,040
were shown to everybody in order to have

00:38:56,660 --> 00:39:01,670
the kind of culture you must have a

00:38:58,040 --> 00:39:03,950
culture of no fear so you must be allow

00:39:01,670 --> 00:39:06,080
yourself to be vulnerable and nobody

00:39:03,950 --> 00:39:08,060
must ever never ever get punished for

00:39:06,080 --> 00:39:08,770
doing something wrong and being open

00:39:08,060 --> 00:39:11,120
about it

00:39:08,770 --> 00:39:18,350
unfortunately changing culture takes a

00:39:11,120 --> 00:39:20,600
long time any more questions yeah thank

00:39:18,350 --> 00:39:22,990
you very much for coming here thank a

00:39:20,600 --> 00:39:22,990
reputation

00:39:23,850 --> 00:39:25,910

YouTube URL: https://www.youtube.com/watch?v=mv7PLnwzLpM


