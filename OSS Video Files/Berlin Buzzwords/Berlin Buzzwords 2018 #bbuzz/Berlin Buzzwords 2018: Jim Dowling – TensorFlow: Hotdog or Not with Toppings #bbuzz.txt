Title: Berlin Buzzwords 2018: Jim Dowling – TensorFlow: Hotdog or Not with Toppings #bbuzz
Publication date: 2018-06-18
Playlist: Berlin Buzzwords 2018 #bbuzz
Description: 
	Jim Dowling talking about "TensorFlow: Hotdog or Not with Toppings (Training/Inferencing/Security/Distribution)"

Have you seen the Silicon Valley episode where they trained a neural network on a mobile phone to recognize whether an object is a Hotdog or not a Hotdog? We're going to build that app, and more, in just a few hours - we will do everything from training a neural network to having an app that can take photos, send it to a deployed server to test it against your trained model (model inferencing). All this will be done with end-to-end TLS security. 

This workshop is aimed at data scientists and data engineers who are eager to learn about deep learning with Big Data. We will write, train, and deploy a convolutional neural network in Tensorflow/Keras on the Hops platform as a Jupyter Notebook. We will show you how you can scale out training to reduce training time. 

We will also show you how to embed Deep Neural Networks (DNNs) in production pipelines for both training and inference using Apache Spark. We will base our tutorial on exercises given to students in Sweden’s first graduate course on Deep Learning – ID2223 at KTH.

Read more:
https://2018.berlinbuzzwords.de/18/session/tensorflow-hotdog-or-not-toppings-traininginferencingsecuritydistribution

About Jim Dowling:
https://2018.berlinbuzzwords.de/users/jim-dowling

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	00:00:04,560 --> 00:00:09,420
okay thanks this is a hands-on workshop

00:00:07,380 --> 00:00:10,740
so I've been giving out some stickers if

00:00:09,420 --> 00:00:12,629
anyone doesn't have any there's some

00:00:10,740 --> 00:00:14,520
more on the table over there there's a

00:00:12,629 --> 00:00:17,550
URL on the back of them so we're gonna

00:00:14,520 --> 00:00:19,890
go through this particular app we're

00:00:17,550 --> 00:00:22,950
actually going to do an end to end

00:00:19,890 --> 00:00:25,830
workflow from training a model to

00:00:22,950 --> 00:00:30,000
serving it to having a client make calls

00:00:25,830 --> 00:00:32,309
on the model okay so I'm with logical

00:00:30,000 --> 00:00:34,050
clocks and I'm gonna talk about our

00:00:32,309 --> 00:00:36,690
platform that supports tensorflow

00:00:34,050 --> 00:00:39,210
I'm gonna do an example which is hot dog

00:00:36,690 --> 00:00:41,969
or not and this is the URL that's on the

00:00:39,210 --> 00:00:43,890
back of your stickers so if you want to

00:00:41,969 --> 00:00:45,390
know what you're talking you can just

00:00:43,890 --> 00:00:47,190
register an email address it doesn't

00:00:45,390 --> 00:00:50,370
even have to be valid email address if

00:00:47,190 --> 00:00:51,989
you don't want to there's some people in

00:00:50,370 --> 00:00:54,510
Stockholm we should validate these while

00:00:51,989 --> 00:00:56,339
I'm talking here and then you should be

00:00:54,510 --> 00:00:59,129
able to so you will need to have in your

00:00:56,339 --> 00:01:02,069
password eight an uppercase letter so

00:00:59,129 --> 00:01:03,269
capitals and and a number so just

00:01:02,069 --> 00:01:05,309
remember that and you do have to

00:01:03,269 --> 00:01:09,659
unfortunately put in a what's your

00:01:05,309 --> 00:01:13,920
favorite pet or your mother's maiden

00:01:09,659 --> 00:01:16,380
name all right just a couple of words

00:01:13,920 --> 00:01:19,049
about our platform our platform is

00:01:16,380 --> 00:01:21,659
called hops it's the world's fastest

00:01:19,049 --> 00:01:23,939
Hadoop platform it's the only Hadoop

00:01:21,659 --> 00:01:27,060
distribution that supports GPUs in the

00:01:23,939 --> 00:01:28,649
cloud and we did some work with Spotify

00:01:27,060 --> 00:01:30,750
in Oracle where we got sixteen times a

00:01:28,649 --> 00:01:32,460
throughput of a dupe but I'm not going

00:01:30,750 --> 00:01:34,259
to talk about that so I'm going to talk

00:01:32,460 --> 00:01:36,840
we're gonna work with this platform this

00:01:34,259 --> 00:01:41,159
is the hops data platform we call it

00:01:36,840 --> 00:01:44,430
hops works and what it is is it's a user

00:01:41,159 --> 00:01:46,439
interface and you have access to all of

00:01:44,430 --> 00:01:47,909
these different open source services

00:01:46,439 --> 00:01:49,710
behind that user interface and the user

00:01:47,909 --> 00:01:50,429
interface also is a REST API so if you

00:01:49,710 --> 00:01:52,979
want to use the platform

00:01:50,429 --> 00:01:55,289
programmatically you can do it this

00:01:52,979 --> 00:01:58,020
little shield thing up there means we're

00:01:55,289 --> 00:02:00,689
basically built on TLS certificates not

00:01:58,020 --> 00:02:04,429
on Kerberos so you can integrate with

00:02:00,689 --> 00:02:07,829
this externally with your IOT apps or

00:02:04,429 --> 00:02:09,330
anything else outside of this and the

00:02:07,829 --> 00:02:10,890
it'll integrate still with Active

00:02:09,330 --> 00:02:13,560
Directory if you're if you're running in

00:02:10,890 --> 00:02:16,920
a Kerberos kind of environment so for

00:02:13,560 --> 00:02:18,300
the who here is a data scientist who

00:02:16,920 --> 00:02:20,620
actually writes Python

00:02:18,300 --> 00:02:23,710
all right I normally talk to data

00:02:20,620 --> 00:02:25,720
scientists ok so so this is M slightly

00:02:23,710 --> 00:02:27,640
off track so all of you will probably

00:02:25,720 --> 00:02:29,560
know spark and you're familiar with

00:02:27,640 --> 00:02:31,840
flink and things like that which we have

00:02:29,560 --> 00:02:35,530
in the platform but when you get into

00:02:31,840 --> 00:02:37,900
data science you typically will use a

00:02:35,530 --> 00:02:40,780
lot of Python right so python has become

00:02:37,900 --> 00:02:43,210
pretty much the de-facto language for

00:02:40,780 --> 00:02:44,410
data science we have or but if you're

00:02:43,210 --> 00:02:45,370
going to do deep learning and today

00:02:44,410 --> 00:02:47,980
we're going to look at deep learning

00:02:45,370 --> 00:02:50,320
Python is is the the name of the game

00:02:47,980 --> 00:02:52,120
and if you're going to do deep learning

00:02:50,320 --> 00:02:54,040
on Python there's a number of frameworks

00:02:52,120 --> 00:02:56,080
and you can see there's a big TF there

00:02:54,040 --> 00:02:57,160
for tensorflow that's pretty much the

00:02:56,080 --> 00:02:59,440
dominant platform

00:02:57,160 --> 00:03:01,060
you also have Kerris which is an API on

00:02:59,440 --> 00:03:03,310
top of tensorflow it's extremely popular

00:03:01,060 --> 00:03:04,780
and then this little droplet there

00:03:03,310 --> 00:03:06,640
represents something called PI torch

00:03:04,780 --> 00:03:11,620
which is getting quite a lot of

00:03:06,640 --> 00:03:14,170
attraction in the research community but

00:03:11,620 --> 00:03:16,090
what our platform basically does is it

00:03:14,170 --> 00:03:17,860
it's a it's you can think of it as being

00:03:16,090 --> 00:03:20,830
like a Hadoop distribution it'll run on

00:03:17,860 --> 00:03:22,060
the cloud or on pram but you can rise in

00:03:20,830 --> 00:03:22,570
Jupiter that's what we're going to do

00:03:22,060 --> 00:03:24,100
today

00:03:22,570 --> 00:03:26,530
we're gonna write a notebook in Jupiter

00:03:24,100 --> 00:03:31,270
and we're gonna run us and it'll run on

00:03:26,530 --> 00:03:33,220
top on yarn actually helps yarn and you

00:03:31,270 --> 00:03:35,470
can also write spark applications use

00:03:33,220 --> 00:03:38,620
Kafka and then we use a lot of other

00:03:35,470 --> 00:03:40,690
services like influx graph Anna elastic

00:03:38,620 --> 00:03:42,640
and Cabana for from monitoring

00:03:40,690 --> 00:03:44,230
applications and giving feedback but it

00:03:42,640 --> 00:03:46,450
is a kind of Hadoop distribution so you

00:03:44,230 --> 00:03:48,040
still have hive one feature that's

00:03:46,450 --> 00:03:50,380
currently under development and it's not

00:03:48,040 --> 00:03:53,350
in the version we look at today is we're

00:03:50,380 --> 00:03:55,060
using kubernetes for scalar model

00:03:53,350 --> 00:03:57,450
serving so we will do model serving but

00:03:55,060 --> 00:04:01,989
it won't be elastic today

00:03:57,450 --> 00:04:04,209
okay so who's heard of coop flow no one

00:04:01,989 --> 00:04:05,230
- - ok so at this point with data

00:04:04,209 --> 00:04:06,700
scientists they'll say well you should

00:04:05,230 --> 00:04:09,910
be doing coop though what's the story

00:04:06,700 --> 00:04:11,410
so cube flow is a recent framework that

00:04:09,910 --> 00:04:14,140
Google released for doing machine

00:04:11,410 --> 00:04:16,000
learning on kubernetes and if your data

00:04:14,140 --> 00:04:17,980
scientists you have to write your mo

00:04:16,000 --> 00:04:19,299
file defining your structure your

00:04:17,980 --> 00:04:22,000
infrastructure so you'll say I need

00:04:19,299 --> 00:04:25,390
these containers I'd like a container

00:04:22,000 --> 00:04:28,479
with this label that has GPU and and so

00:04:25,390 --> 00:04:30,850
on and with your infrastructure you need

00:04:28,479 --> 00:04:31,930
to then define a docker file because you

00:04:30,850 --> 00:04:35,110
want to install libraries

00:04:31,930 --> 00:04:37,240
when a Python libraries in there and you

00:04:35,110 --> 00:04:39,460
you basically write a docker file and

00:04:37,240 --> 00:04:40,750
then finally you have a tubular notebook

00:04:39,460 --> 00:04:43,509
enough you can go into your machine

00:04:40,750 --> 00:04:47,020
learning so hops is an alternative model

00:04:43,509 --> 00:04:51,610
to this what we're trying to do is make

00:04:47,020 --> 00:04:54,789
it easier to use some couvreux and I'll

00:04:51,610 --> 00:04:56,560
show you how in a minute so what we

00:04:54,789 --> 00:04:58,810
introduced in hops is we introduce some

00:04:56,560 --> 00:05:00,370
abstractions that we we think it helps

00:04:58,810 --> 00:05:02,110
make it easier to do machine learning so

00:05:00,370 --> 00:05:03,639
the main one is something called a

00:05:02,110 --> 00:05:06,039
project so everyone is familiar with

00:05:03,639 --> 00:05:10,360
github so you know good hope you have a

00:05:06,039 --> 00:05:11,889
project or a repository and in your repo

00:05:10,360 --> 00:05:14,770
you can add members and you can give

00:05:11,889 --> 00:05:17,199
them roles inside that repo so we have

00:05:14,770 --> 00:05:18,699
the same abstraction and we think this

00:05:17,199 --> 00:05:20,770
is the right way for machine learning

00:05:18,699 --> 00:05:23,289
people to work you have data engineers

00:05:20,770 --> 00:05:25,000
and data scientists in the same projects

00:05:23,289 --> 00:05:27,759
and they take hand of machine learning

00:05:25,000 --> 00:05:29,110
pipelines from end to end so data

00:05:27,759 --> 00:05:31,630
scientists aren't expected to do

00:05:29,110 --> 00:05:33,340
everything by themselves and data

00:05:31,630 --> 00:05:34,750
engineers aren't expected to do all the

00:05:33,340 --> 00:05:37,720
data science they work together as a

00:05:34,750 --> 00:05:39,780
team now the project abstraction is is

00:05:37,720 --> 00:05:41,979
security by design so we have

00:05:39,780 --> 00:05:45,490
certificates behind us

00:05:41,979 --> 00:05:47,320
it's very self-service it's designed for

00:05:45,490 --> 00:05:48,909
ease of use so you'll see that

00:05:47,320 --> 00:05:53,169
everything is basically point-and-click

00:05:48,909 --> 00:05:56,139
for data scientists and at the backend

00:05:53,169 --> 00:05:58,570
it's scale out deep learning so because

00:05:56,139 --> 00:06:00,130
we have GPUs and our cluster here you

00:05:58,570 --> 00:06:02,680
can run the jobs that we're going to

00:06:00,130 --> 00:06:06,190
look at today on one GPU on ten GPUs on

00:06:02,680 --> 00:06:07,570
50 GPUs hundred GPUs and then the

00:06:06,190 --> 00:06:10,840
question is well why would I care about

00:06:07,570 --> 00:06:12,669
many GPUs well we want to do parallel

00:06:10,840 --> 00:06:14,919
experiments if you're I'll talk about

00:06:12,669 --> 00:06:16,509
this in a bit but parallel experiments

00:06:14,919 --> 00:06:18,550
is when you start doing some data

00:06:16,509 --> 00:06:20,020
science and you want to maybe build a

00:06:18,550 --> 00:06:22,509
model architecture for your deep

00:06:20,020 --> 00:06:24,099
learning system you don't just write the

00:06:22,509 --> 00:06:25,840
code once and run it I mean nobody has

00:06:24,099 --> 00:06:28,240
ever written code once and compiled it

00:06:25,840 --> 00:06:30,009
and it worked and was fine uite really

00:06:28,240 --> 00:06:32,440
improve it so you may change the hyper

00:06:30,009 --> 00:06:34,150
parameters you may run some small

00:06:32,440 --> 00:06:36,370
experiments to see if this converges

00:06:34,150 --> 00:06:37,750
better and these parallel experiments

00:06:36,370 --> 00:06:39,460
you can actually run many of them in

00:06:37,750 --> 00:06:41,139
parallel there's no reason why if you

00:06:39,460 --> 00:06:43,150
have one GPU you have to sit there and

00:06:41,139 --> 00:06:45,070
wait for each experiment to finish

00:06:43,150 --> 00:06:45,840
before you start a new one if you have

00:06:45,070 --> 00:06:48,960
like 50g

00:06:45,840 --> 00:06:50,220
you just run 50 in parallel and then the

00:06:48,960 --> 00:06:52,560
second reason why scaler deep learning

00:06:50,220 --> 00:06:54,960
is important is because if you have a

00:06:52,560 --> 00:06:57,180
large data set and your training it can

00:06:54,960 --> 00:06:59,490
be very compute intensive and can take a

00:06:57,180 --> 00:07:01,050
long time so there's well-known data

00:06:59,490 --> 00:07:03,600
sets like image net with a million

00:07:01,050 --> 00:07:04,889
images and we work with some vehicle

00:07:03,600 --> 00:07:07,020
manufacturers and they have millions of

00:07:04,889 --> 00:07:09,180
images and if you train them in a single

00:07:07,020 --> 00:07:12,180
GPU you're looking at weeks and that's

00:07:09,180 --> 00:07:13,530
not a good use of your data scientist

00:07:12,180 --> 00:07:16,590
time if you're paying them you know

00:07:13,530 --> 00:07:18,090
relatively high salaries so distributed

00:07:16,590 --> 00:07:20,820
training will enable you to train your

00:07:18,090 --> 00:07:25,770
models much faster on larger volumes of

00:07:20,820 --> 00:07:27,510
data ok the next reason I I think it's

00:07:25,770 --> 00:07:31,340
interesting is that we have this is why

00:07:27,510 --> 00:07:33,990
it's easier to use hops is that we have

00:07:31,340 --> 00:07:36,180
persistent Python in the cluster so

00:07:33,990 --> 00:07:38,460
remember we have a github project this

00:07:36,180 --> 00:07:39,780
project abstraction and inside your

00:07:38,460 --> 00:07:41,430
project you have your own counter

00:07:39,780 --> 00:07:42,840
environment and that counter environment

00:07:41,430 --> 00:07:45,210
will live on every machine in the

00:07:42,840 --> 00:07:46,949
cluster so when you want to install

00:07:45,210 --> 00:07:48,870
something into your project a Python

00:07:46,949 --> 00:07:50,729
library you're installing it just for

00:07:48,870 --> 00:07:52,919
your project not for anyone elses so

00:07:50,729 --> 00:07:55,050
each project can have its own version of

00:07:52,919 --> 00:07:57,960
Python it can have its own version of

00:07:55,050 --> 00:08:02,690
every libraries and we think it's pretty

00:07:57,960 --> 00:08:05,789
easy for data sciences to use this and

00:08:02,690 --> 00:08:08,070
why all this can come together and work

00:08:05,789 --> 00:08:10,770
well in an enterprise environment well

00:08:08,070 --> 00:08:13,289
we don't use Kerberos we use certificate

00:08:10,770 --> 00:08:15,720
certificates are the the main mechanism

00:08:13,289 --> 00:08:17,789
we have to enable multi-tenancy in this

00:08:15,720 --> 00:08:21,030
platform to have many different projects

00:08:17,789 --> 00:08:24,389
coexist on the same platform and still

00:08:21,030 --> 00:08:26,669
be protected from one another so we have

00:08:24,389 --> 00:08:28,500
for when you're a member of a project

00:08:26,669 --> 00:08:30,030
you'll have a certificate created for

00:08:28,500 --> 00:08:34,140
every member of a project we call those

00:08:30,030 --> 00:08:35,789
per project user certificates so if you

00:08:34,140 --> 00:08:38,729
think about how you have two projects

00:08:35,789 --> 00:08:40,830
like project a and project B I can't

00:08:38,729 --> 00:08:43,349
copy data from project a to project B

00:08:40,830 --> 00:08:45,330
because I have a different user identity

00:08:43,349 --> 00:08:47,160
in project a than they have in project B

00:08:45,330 --> 00:08:51,330
so there's a different certificate for

00:08:47,160 --> 00:08:53,490
each user and a user a may have you know

00:08:51,330 --> 00:08:56,040
access rights for some data set and user

00:08:53,490 --> 00:08:57,320
B doesn't so user B can't read from that

00:08:56,040 --> 00:08:58,940
data set

00:08:57,320 --> 00:09:01,940
so it's not enough just to have

00:08:58,940 --> 00:09:04,370
certificates identifying an authorizing

00:09:01,940 --> 00:09:06,350
users you also need to have certificates

00:09:04,370 --> 00:09:08,449
for the services things like the name

00:09:06,350 --> 00:09:12,649
node resource manager caf-co brokers

00:09:08,449 --> 00:09:14,630
hive server etc another thing we have

00:09:12,649 --> 00:09:16,670
that's this is if you're familiar with

00:09:14,630 --> 00:09:18,019
Google this is this is basically the

00:09:16,670 --> 00:09:20,779
same as Google's internal security

00:09:18,019 --> 00:09:22,610
architecture for Google acquired every

00:09:20,779 --> 00:09:25,220
application you run will generate a new

00:09:22,610 --> 00:09:27,410
certificate for that and the advantage

00:09:25,220 --> 00:09:29,990
of that is then we can track what

00:09:27,410 --> 00:09:32,269
particular files users have read and

00:09:29,990 --> 00:09:35,750
written and we can save that and make

00:09:32,269 --> 00:09:37,550
that available for auditing ok so then

00:09:35,750 --> 00:09:40,009
the the platform is completely

00:09:37,550 --> 00:09:43,490
open-source but we are a company and we

00:09:40,009 --> 00:09:44,660
do have a you know a enterprise version

00:09:43,490 --> 00:09:47,440
that supports certificate revocation

00:09:44,660 --> 00:09:54,740
renewal and reloading

00:09:47,440 --> 00:09:56,120
okay so GPU in yarn so we have support

00:09:54,740 --> 00:09:58,250
for GPS and yarn is in the explicit

00:09:56,120 --> 00:10:00,410
resource you can basically say give me

00:09:58,250 --> 00:10:02,959
ten GPUs on this host four GPUs on this

00:10:00,410 --> 00:10:06,139
host you can use node labeling and yarn

00:10:02,959 --> 00:10:11,779
to say give me GPUs only on hosts that

00:10:06,139 --> 00:10:14,569
have InfiniBand support and the actual

00:10:11,779 --> 00:10:16,220
GPUs we only support in video for now so

00:10:14,569 --> 00:10:18,589
those of you who are familiar with GPUs

00:10:16,220 --> 00:10:21,470
know that Nvidia are the dominant player

00:10:18,589 --> 00:10:23,269
and deep learning right now so it's CUDA

00:10:21,470 --> 00:10:26,269
but you can have a mix of different

00:10:23,269 --> 00:10:28,490
types of NVIDIA GPUs so you may buy the

00:10:26,269 --> 00:10:31,190
commodity 1080 T eyes and you may buy

00:10:28,490 --> 00:10:33,199
the P 100's or V 100's so you can run

00:10:31,190 --> 00:10:35,899
all of them in the in the same cluster

00:10:33,199 --> 00:10:38,180
and it works okay obviously if you're

00:10:35,899 --> 00:10:39,410
gonna train you would and you you don't

00:10:38,180 --> 00:10:42,170
want to have a mix if you're if you have

00:10:39,410 --> 00:10:44,600
half of your GPUs are very high-end ones

00:10:42,170 --> 00:10:46,399
and half are low-end that may affect

00:10:44,600 --> 00:10:48,740
your training time because the slow ones

00:10:46,399 --> 00:10:49,850
may slow down the the faster ones so

00:10:48,740 --> 00:10:51,260
when you're training you should use

00:10:49,850 --> 00:10:53,449
things like no labeling to make sure

00:10:51,260 --> 00:10:56,600
that you're training them on the GPUs

00:10:53,449 --> 00:10:58,279
you want to you to train them up ok I

00:10:56,600 --> 00:11:00,439
can actually at this point I think I'm

00:10:58,279 --> 00:11:02,180
talking quite a lot so have people

00:11:00,439 --> 00:11:04,819
manage to log in create accounts and

00:11:02,180 --> 00:11:08,630
logged in one or two shaking their head

00:11:04,819 --> 00:11:09,710
I will I will it's on these stickers

00:11:08,630 --> 00:11:13,180
that we have are

00:11:09,710 --> 00:11:20,240
around here but it's the URL is hops io

00:11:13,180 --> 00:11:22,220
/tf hops don't I Oh /tf so if you type

00:11:20,240 --> 00:11:24,020
that in so you can register any email

00:11:22,220 --> 00:11:25,690
address just remember you just try a new

00:11:24,020 --> 00:11:28,490
email address with a new password and

00:11:25,690 --> 00:11:31,190
just remember it and then try and use

00:11:28,490 --> 00:11:33,350
that to log in and abettor minus-- okay

00:11:31,190 --> 00:11:35,480
so i'm gonna go over this is where you

00:11:33,350 --> 00:11:37,220
get to when you do that this is kind of

00:11:35,480 --> 00:11:39,170
the basic instructions for the the

00:11:37,220 --> 00:11:43,150
workshop so if you click on this hops

00:11:39,170 --> 00:11:45,920
here link you come to actual hops

00:11:43,150 --> 00:11:49,460
if you're curious tis basically running

00:11:45,920 --> 00:11:56,780
on it on a large VM and Google Google

00:11:49,460 --> 00:11:59,230
Cloud and so if I go back I just opened

00:11:56,780 --> 00:12:01,970
up the other one actually house it IOT F

00:11:59,230 --> 00:12:03,440
okay so what what what I'm gonna go

00:12:01,970 --> 00:12:05,570
through today I'm going to go through a

00:12:03,440 --> 00:12:09,080
bit more background just before we get

00:12:05,570 --> 00:12:10,490
started but when I'm gonna go through

00:12:09,080 --> 00:12:12,650
today is we're going to do a tensorflow

00:12:10,490 --> 00:12:15,110
tour and end-to-end machine learning

00:12:12,650 --> 00:12:17,540
workload and you're basically going to

00:12:15,110 --> 00:12:21,020
create a project called tensorflow demo

00:12:17,540 --> 00:12:23,000
and in that project you can train an

00:12:21,020 --> 00:12:24,620
export a model it's quite hard to read

00:12:23,000 --> 00:12:26,390
that text from down here so we're gonna

00:12:24,620 --> 00:12:28,130
train an export a model this will

00:12:26,390 --> 00:12:30,740
basically use that the hello world of

00:12:28,130 --> 00:12:32,540
deep learning columnist many of you will

00:12:30,740 --> 00:12:34,520
be bored with this but many of you may

00:12:32,540 --> 00:12:36,320
be new for you as well so when that

00:12:34,520 --> 00:12:38,690
models trained it will be a file a

00:12:36,320 --> 00:12:40,970
protocol buffer file and that file will

00:12:38,690 --> 00:12:44,660
live in our distributed file system HDFS

00:12:40,970 --> 00:12:48,050
and then we'll start a tensor flow model

00:12:44,660 --> 00:12:50,840
server number B and from that we can

00:12:48,050 --> 00:12:53,030
then load the the protocol buffers file

00:12:50,840 --> 00:12:54,590
and serve it and if you're familiar with

00:12:53,030 --> 00:12:57,770
thanks for serving you'll know that this

00:12:54,590 --> 00:12:59,030
will be using G RPC as an API and then

00:12:57,770 --> 00:13:02,330
we can what we need to do is basically

00:12:59,030 --> 00:13:05,390
copy the IP and port from our serve

00:13:02,330 --> 00:13:07,900
model and paste that into another

00:13:05,390 --> 00:13:10,670
notebook called a serving client and

00:13:07,900 --> 00:13:14,630
with that it we can then run the client

00:13:10,670 --> 00:13:17,750
and it will it will then send G or PC

00:13:14,630 --> 00:13:20,180
requests to the model server and it will

00:13:17,750 --> 00:13:22,340
classify the images that it sends and it

00:13:20,180 --> 00:13:23,480
will get back responses so if you're not

00:13:22,340 --> 00:13:26,420
familiar with

00:13:23,480 --> 00:13:28,490
I'll tell you a 10-second description of

00:13:26,420 --> 00:13:32,329
it em nest is hello world for deep

00:13:28,490 --> 00:13:34,579
learning it's it's a data set with tens

00:13:32,329 --> 00:13:36,769
of thousands of hand-drawn numbers you

00:13:34,579 --> 00:13:38,269
have the numbers from zero one two three

00:13:36,769 --> 00:13:41,690
all the way up to nine there's ten of

00:13:38,269 --> 00:13:43,130
them so what you do is you train the the

00:13:41,690 --> 00:13:45,199
images that come in are very small

00:13:43,130 --> 00:13:45,920
they're 28 by 28 pixels and they're

00:13:45,199 --> 00:13:47,839
grayscale

00:13:45,920 --> 00:13:51,019
so you're basically turning them into a

00:13:47,839 --> 00:13:53,839
big vector it's 768 Elements 28 by 28

00:13:51,019 --> 00:13:56,000
long and you basically feed that into

00:13:53,839 --> 00:13:57,649
the neural network so neural network can

00:13:56,000 --> 00:13:59,720
be a convolutional or a network of

00:13:57,649 --> 00:14:02,540
feed-forward it's quite easy to train a

00:13:59,720 --> 00:14:04,310
good prediction model on em nest and

00:14:02,540 --> 00:14:06,320
once you've trained us you'll get out

00:14:04,310 --> 00:14:09,260
the network itself you can save it as a

00:14:06,320 --> 00:14:10,880
protocol buffers file so once you've got

00:14:09,260 --> 00:14:14,690
this trained model I can take any

00:14:10,880 --> 00:14:17,089
different 28 by 28 pixel image with a

00:14:14,690 --> 00:14:18,920
number 100 draw a number on us send it

00:14:17,089 --> 00:14:21,170
to the train model and say tell me what

00:14:18,920 --> 00:14:26,110
number this is is this a 2 is it a 4 is

00:14:21,170 --> 00:14:29,120
it nays okay I'll tell you what I'll do

00:14:26,110 --> 00:14:31,010
I'll give you a quick look at this

00:14:29,120 --> 00:14:32,540
before we actually because you're gonna

00:14:31,010 --> 00:14:34,519
hot dog or not let's have a quick look

00:14:32,540 --> 00:14:36,920
at it before we we go in and start doing

00:14:34,519 --> 00:14:42,470
the other practical work so this is the

00:14:36,920 --> 00:14:44,510
hot dog or not notebook and what it

00:14:42,470 --> 00:14:47,990
basically does is I took a data set that

00:14:44,510 --> 00:14:51,019
I found on a kegel competition and I

00:14:47,990 --> 00:14:53,300
took a notebook by this guy Magnus

00:14:51,019 --> 00:14:55,430
Peterson he does really good tutorials

00:14:53,300 --> 00:14:57,769
much better than the official tensor

00:14:55,430 --> 00:15:00,079
flow tutorials and his and this is

00:14:57,769 --> 00:15:01,910
actually this is a I would say it's a

00:15:00,079 --> 00:15:04,459
sophisticated notebook at some level

00:15:01,910 --> 00:15:07,100
what it does is it takes the raw images

00:15:04,459 --> 00:15:09,769
now we've already cropped the images to

00:15:07,100 --> 00:15:12,529
200 by 200 and size and there's another

00:15:09,769 --> 00:15:14,029
notebook for that but with those raw

00:15:12,529 --> 00:15:16,430
images we're going to turn them into

00:15:14,029 --> 00:15:18,019
what we call tensor flow records has

00:15:16,430 --> 00:15:19,940
anyone heard of tens for records yeah

00:15:18,019 --> 00:15:22,730
couple of people so tempo records are

00:15:19,940 --> 00:15:24,949
the native file format for training

00:15:22,730 --> 00:15:26,959
tensor flow models and the reason why

00:15:24,949 --> 00:15:29,839
you want to use tensor flow records is

00:15:26,959 --> 00:15:32,269
because if you have GPUs and you're and

00:15:29,839 --> 00:15:35,569
we decide well I'm going to read the

00:15:32,269 --> 00:15:36,980
files I'm going to decode the images I'm

00:15:35,569 --> 00:15:39,050
now going to convert

00:15:36,980 --> 00:15:41,480
them into my you know twenty by twenty

00:15:39,050 --> 00:15:43,550
pixel or twenty by twenty array and

00:15:41,480 --> 00:15:45,740
there's three color child's here so it's

00:15:43,550 --> 00:15:46,970
25 20 by 3 that's actually quite

00:15:45,740 --> 00:15:49,100
compute-intensive

00:15:46,970 --> 00:15:51,649
and it will slow down training so your

00:15:49,100 --> 00:15:53,930
GPUs will be waiting a lot of the time

00:15:51,649 --> 00:15:56,389
for data to come in so you know if you

00:15:53,930 --> 00:15:59,000
have ten GPUs on a server and you have

00:15:56,389 --> 00:16:00,709
20 CPUs those CPUs probably can't decode

00:15:59,000 --> 00:16:04,459
those images fast enough and fetch them

00:16:00,709 --> 00:16:06,190
to feed those GPUs and keep them busy so

00:16:04,459 --> 00:16:09,829
in a production pipeline you should be

00:16:06,190 --> 00:16:12,620
doing your feature engineering and data

00:16:09,829 --> 00:16:14,449
wrangling in something like spark if its

00:16:12,620 --> 00:16:16,699
large data it can be just answer flow

00:16:14,449 --> 00:16:19,370
but then if you save your output in

00:16:16,699 --> 00:16:21,649
product in tensor flow records then you

00:16:19,370 --> 00:16:24,470
can make sure that you use those to

00:16:21,649 --> 00:16:26,600
train the models directly so this

00:16:24,470 --> 00:16:29,269
notebook just a brief overview it it

00:16:26,600 --> 00:16:31,790
basically it it we you can see that

00:16:29,269 --> 00:16:34,100
we're gonna have to import the images

00:16:31,790 --> 00:16:36,170
from a shared data set and that's doing

00:16:34,100 --> 00:16:38,810
it there and there's some images and

00:16:36,170 --> 00:16:40,399
then we're in this case defining it as a

00:16:38,810 --> 00:16:44,660
data set so reason it days a tape you

00:16:40,399 --> 00:16:46,670
know in in tensorflow and then we have

00:16:44,660 --> 00:16:49,579
some images here it's not a huge data

00:16:46,670 --> 00:16:51,139
set it's only a thousand images and we

00:16:49,579 --> 00:16:54,019
can plot a few of those images you can

00:16:51,139 --> 00:16:56,060
see here this is a hot dog true and this

00:16:54,019 --> 00:16:58,639
is labeled these are all labeled true as

00:16:56,060 --> 00:17:01,370
hot dogs actually and then we convert

00:16:58,639 --> 00:17:03,050
them here into tens for records and once

00:17:01,370 --> 00:17:07,370
we have our tents for records we can

00:17:03,050 --> 00:17:09,380
then store them in HDFS and then finally

00:17:07,370 --> 00:17:12,610
we'll define a parse function that our

00:17:09,380 --> 00:17:15,049
data set API will use to feed tensorflow

00:17:12,610 --> 00:17:16,400
and then we have another input function

00:17:15,049 --> 00:17:19,010
which will pull the records out of that

00:17:16,400 --> 00:17:20,900
and then finally we're training it down

00:17:19,010 --> 00:17:22,790
here so okay first we have to define our

00:17:20,900 --> 00:17:24,199
features but then we we do our training

00:17:22,790 --> 00:17:26,030
down here we're doing it on a very

00:17:24,199 --> 00:17:28,390
simple pre count the estimator this is

00:17:26,030 --> 00:17:31,190
the estimator framework in tensorflow

00:17:28,390 --> 00:17:33,380
if you're familiar with SPARC you you

00:17:31,190 --> 00:17:35,540
may have seen the data flow or the SPARC

00:17:33,380 --> 00:17:39,440
summit last week ten

00:17:35,540 --> 00:17:41,419
data data brakes just released a

00:17:39,440 --> 00:17:42,950
framework that looks exactly like this

00:17:41,419 --> 00:17:45,530
an estimator framework for doing deep

00:17:42,950 --> 00:17:48,890
learning from spark but this is the one

00:17:45,530 --> 00:17:50,460
that's native to tensorflow okay so once

00:17:48,890 --> 00:17:51,810
you've trained your model you can see

00:17:50,460 --> 00:17:54,030
you can get your prediction accuracy I'm

00:17:51,810 --> 00:17:55,200
on this conned estimator not getting

00:17:54,030 --> 00:17:58,170
great values and then we have some

00:17:55,200 --> 00:17:59,460
predictions and so on so you know if you

00:17:58,170 --> 00:18:00,840
want to take a picture of a hot dog

00:17:59,460 --> 00:18:03,210
while you're here and add it to the data

00:18:00,840 --> 00:18:06,120
set feel free to do that you know it

00:18:03,210 --> 00:18:09,060
should work but let's go back and see

00:18:06,120 --> 00:18:12,150
that's where we're gonna get to by the

00:18:09,060 --> 00:18:14,820
end of this at this point do we have

00:18:12,150 --> 00:18:17,430
questions feel free to play around if

00:18:14,820 --> 00:18:19,890
you're in there and start on the tour

00:18:17,430 --> 00:18:26,670
and do a tenth of a little tour and go

00:18:19,890 --> 00:18:28,790
ahead okay write a little bit of

00:18:26,670 --> 00:18:32,970
background then a little bit more Theory

00:18:28,790 --> 00:18:36,030
so why is why do why do we think

00:18:32,970 --> 00:18:38,160
distributed tensorflow is interesting

00:18:36,030 --> 00:18:40,950
why why do we want to actually use lots

00:18:38,160 --> 00:18:43,500
of GPUs to you know make our models go

00:18:40,950 --> 00:18:45,590
train faster or or do more

00:18:43,500 --> 00:18:48,420
experimentation and parallel on them

00:18:45,590 --> 00:18:49,920
well you can do it quite easily in in in

00:18:48,420 --> 00:18:53,670
SPARC actually we use SPARC as the

00:18:49,920 --> 00:18:56,850
framework to do data parallel training

00:18:53,670 --> 00:18:58,440
and data parallel experiments so if you

00:18:56,850 --> 00:19:00,000
imagine a set up for you like this one

00:18:58,440 --> 00:19:02,340
where you have a hundred GPUs we have a

00:19:00,000 --> 00:19:05,670
hundred GPUs all connected to our

00:19:02,340 --> 00:19:07,380
distributed file system HDFS what we do

00:19:05,670 --> 00:19:10,620
when were training deep learning models

00:19:07,380 --> 00:19:13,020
is we do what's called an iteration an I

00:19:10,620 --> 00:19:15,150
duration is when you take a batch in

00:19:13,020 --> 00:19:17,520
this case will be files corresponding to

00:19:15,150 --> 00:19:19,350
each image or tensorflow records which

00:19:17,520 --> 00:19:21,510
we're actually going to use so let's say

00:19:19,350 --> 00:19:23,520
we have a batch of to three thousand two

00:19:21,510 --> 00:19:25,500
hundred tens of flow records well we

00:19:23,520 --> 00:19:27,990
want 32 of them to go into one GPU

00:19:25,500 --> 00:19:30,480
thirty-two into the next and each GPU

00:19:27,990 --> 00:19:33,330
will get 32 of these records they would

00:19:30,480 --> 00:19:35,970
then do what we call a forward pass in

00:19:33,330 --> 00:19:37,860
in deep learning and then it would

00:19:35,970 --> 00:19:39,750
calculate the changes in the weights in

00:19:37,860 --> 00:19:42,720
our network we call those the gradients

00:19:39,750 --> 00:19:45,450
and each GPU will do it for their 32

00:19:42,720 --> 00:19:47,370
samples in this batch and they'll send

00:19:45,450 --> 00:19:49,020
their gradients up somewhere you can

00:19:47,370 --> 00:19:50,820
think of this abstractly because what

00:19:49,020 --> 00:19:52,410
we'll see later on is the parameter

00:19:50,820 --> 00:19:54,570
server that you think you should send

00:19:52,410 --> 00:19:57,300
these to is actually not the scale the

00:19:54,570 --> 00:20:00,420
model will be using in a few years we'll

00:19:57,300 --> 00:20:02,340
be using a ring so in the traditional

00:20:00,420 --> 00:20:03,720
parameter server model you would send

00:20:02,340 --> 00:20:06,270
them all up to a server who would

00:20:03,720 --> 00:20:08,190
aggregate all of these gradients compute

00:20:06,270 --> 00:20:10,470
a new model so the changes and weights

00:20:08,190 --> 00:20:12,060
from all of these 100 GPUs add them all

00:20:10,470 --> 00:20:14,310
together change the weights and send

00:20:12,060 --> 00:20:18,480
this new updated model down to all the

00:20:14,310 --> 00:20:21,990
GPUs that is one iteration now if you

00:20:18,480 --> 00:20:25,370
imagine that you have 30 let's say you

00:20:21,990 --> 00:20:28,260
have three million two hundred thousand

00:20:25,370 --> 00:20:30,150
samples well then we're going to do an a

00:20:28,260 --> 00:20:33,720
thousand of those iterations to cover

00:20:30,150 --> 00:20:35,400
all of the 3.2 million samples so once

00:20:33,720 --> 00:20:37,440
you've done all taosenai durations we

00:20:35,400 --> 00:20:39,630
call that an epoch and typically

00:20:37,440 --> 00:20:44,340
training models requires tens of epochs

00:20:39,630 --> 00:20:46,290
you know 10 20 30 50 so you can see that

00:20:44,340 --> 00:20:48,510
this is this is a cycle that's gonna go

00:20:46,290 --> 00:20:50,340
round and round and round now everyone

00:20:48,510 --> 00:20:52,200
here I guess is familiar with SPARC at

00:20:50,340 --> 00:20:53,760
some level so you can imagine pretty

00:20:52,200 --> 00:20:56,490
much how this is going to work in spark

00:20:53,760 --> 00:20:58,710
our GPUs will basically have an executor

00:20:56,490 --> 00:21:01,110
each in them and we're gonna have to

00:20:58,710 --> 00:21:04,500
have some way of kicking off the

00:21:01,110 --> 00:21:06,030
training from the driver so this is kind

00:21:04,500 --> 00:21:08,100
of how we do the parallel experiments in

00:21:06,030 --> 00:21:10,440
hops in that style of architecture our

00:21:08,100 --> 00:21:12,600
spark driver will start executors the

00:21:10,440 --> 00:21:14,460
executors will have one GPU each they'll

00:21:12,600 --> 00:21:17,040
have a full copy of the model on them

00:21:14,460 --> 00:21:18,990
and then they'll communicate with each

00:21:17,040 --> 00:21:22,110
other by reading and writing to the file

00:21:18,990 --> 00:21:24,000
system so that's so you know if you're

00:21:22,110 --> 00:21:26,070
doing distributed deep learning at any

00:21:24,000 --> 00:21:28,740
sorts somehow the nodes need to

00:21:26,070 --> 00:21:31,320
communicate with each other and Google

00:21:28,740 --> 00:21:33,150
also aspires the use of a distributed

00:21:31,320 --> 00:21:35,790
file system is the easiest way to do

00:21:33,150 --> 00:21:37,230
that because you need to do things like

00:21:35,790 --> 00:21:40,050
you need to collect the logs when you're

00:21:37,230 --> 00:21:42,000
training if we're doing parallel

00:21:40,050 --> 00:21:44,520
experiments each of these executors will

00:21:42,000 --> 00:21:46,290
run a different full experiment it's not

00:21:44,520 --> 00:21:47,910
the distributed training now they're

00:21:46,290 --> 00:21:49,770
gonna say I'm gonna do I'm gonna run all

00:21:47,910 --> 00:21:52,140
of the training for this particular

00:21:49,770 --> 00:21:53,790
model with these hyper parameters on

00:21:52,140 --> 00:21:55,460
this input data and they're going to

00:21:53,790 --> 00:21:57,660
write the output to the file system and

00:21:55,460 --> 00:21:58,800
all the executors will do that they'll

00:21:57,660 --> 00:22:00,480
write all their I puts to the file

00:21:58,800 --> 00:22:02,280
system what we'd like to be able to

00:22:00,480 --> 00:22:04,350
visualize that at the end was tensor

00:22:02,280 --> 00:22:05,970
board you know we'd like to be easily

00:22:04,350 --> 00:22:08,610
able to get our model service to pull

00:22:05,970 --> 00:22:10,680
out the Train models we'd like to be

00:22:08,610 --> 00:22:11,929
able to checkpoint our models and have

00:22:10,680 --> 00:22:14,609
easy access to them

00:22:11,929 --> 00:22:16,889
so there's a lot of reasons for using a

00:22:14,609 --> 00:22:19,649
distributed file system when you're

00:22:16,889 --> 00:22:21,929
building these systems okay let's have a

00:22:19,649 --> 00:22:23,839
look at the pipeline and we'll have a

00:22:21,929 --> 00:22:26,519
look at it at an abstract pipeline and

00:22:23,839 --> 00:22:28,139
then we'll look at some examples that

00:22:26,519 --> 00:22:28,739
you can you can play with while we're

00:22:28,139 --> 00:22:30,719
doing this

00:22:28,739 --> 00:22:32,879
so the typical pipeline you'll have is

00:22:30,719 --> 00:22:36,179
that data will have to come in from

00:22:32,879 --> 00:22:37,649
somewhere now if you're Ober uber has

00:22:36,179 --> 00:22:40,679
something called a feature store and the

00:22:37,649 --> 00:22:43,519
feature store abstracts out whether it's

00:22:40,679 --> 00:22:49,489
going from a Kafka topic or a hive table

00:22:43,519 --> 00:22:51,690
or just CSV files in HDFS or even s3 so

00:22:49,489 --> 00:22:53,519
many others will have to actually read

00:22:51,690 --> 00:22:55,499
from those sources directly and you know

00:22:53,519 --> 00:22:57,989
you might decide well you spark for this

00:22:55,499 --> 00:23:00,089
and have data frames representing the

00:22:57,989 --> 00:23:02,879
different features that I want to

00:23:00,089 --> 00:23:03,959
combine together to train my model but

00:23:02,879 --> 00:23:07,289
you still have to do all the work

00:23:03,959 --> 00:23:09,359
related to data wrangling so transform

00:23:07,289 --> 00:23:11,969
your data in our case we want to decode

00:23:09,359 --> 00:23:14,789
the images I want to take these hot dog

00:23:11,969 --> 00:23:17,299
images and and their JPEGs we need to

00:23:14,789 --> 00:23:21,059
decode them and turn them into vectors

00:23:17,299 --> 00:23:22,169
we want to maybe extract features from

00:23:21,059 --> 00:23:24,809
them so we need to know obviously the

00:23:22,169 --> 00:23:25,979
labels of the images and the next phase

00:23:24,809 --> 00:23:27,719
is quite a big box there it's

00:23:25,979 --> 00:23:29,099
experimentation so we then will need to

00:23:27,719 --> 00:23:31,229
play around with different networks and

00:23:29,099 --> 00:23:32,729
say ok what would be a good Network for

00:23:31,229 --> 00:23:34,979
this what will be good hyper parameters

00:23:32,729 --> 00:23:36,299
and once you've decided what good hyper

00:23:34,979 --> 00:23:38,339
parameters and good network is then you

00:23:36,299 --> 00:23:40,799
can train the model and then finally

00:23:38,339 --> 00:23:43,679
you'd like to test it and service now

00:23:40,799 --> 00:23:45,929
each of these boxes has way more detail

00:23:43,679 --> 00:23:47,999
than I can go into here things like

00:23:45,929 --> 00:23:49,799
serving we've had a number of talks at

00:23:47,999 --> 00:23:52,589
this conference very good ones as well

00:23:49,799 --> 00:23:56,159
about some of the challenges of how to

00:23:52,589 --> 00:23:57,869
you know handle serving but I can't go

00:23:56,159 --> 00:23:59,609
into all the detail in all of them but

00:23:57,869 --> 00:24:03,269
if you have specific questions we can we

00:23:59,609 --> 00:24:05,699
can take them so as a programmer if you

00:24:03,269 --> 00:24:08,009
want to write this pipeline and you're a

00:24:05,699 --> 00:24:09,539
Python programmer or your Python team

00:24:08,009 --> 00:24:11,339
because we're gonna have a team remember

00:24:09,539 --> 00:24:15,299
who are going to be responsible for this

00:24:11,339 --> 00:24:17,099
pipeline you can you can decide and use

00:24:15,299 --> 00:24:19,609
tensorflow and python the whole way if

00:24:17,099 --> 00:24:22,910
you want to if your data is not that big

00:24:19,609 --> 00:24:25,880
it's completely feasible

00:24:22,910 --> 00:24:27,440
underneath it what we provided services

00:24:25,880 --> 00:24:31,610
to help you build these pipelines or

00:24:27,440 --> 00:24:33,830
things like Kafka hive HDFS or we call

00:24:31,610 --> 00:24:37,670
hops of s and therefore serving we have

00:24:33,830 --> 00:24:39,740
early support for kubernetes what we

00:24:37,670 --> 00:24:41,920
typically would espouse though would is

00:24:39,740 --> 00:24:44,960
that people should use PI spark instead

00:24:41,920 --> 00:24:46,670
of tensorflow because obviously as you

00:24:44,960 --> 00:24:49,430
know pi spark can handle larger data

00:24:46,670 --> 00:24:52,100
volumes and can see a light but

00:24:49,430 --> 00:24:54,650
tensorflow then becomes just a library

00:24:52,100 --> 00:24:56,770
that's run inside a larger PI spark job

00:24:54,650 --> 00:24:58,940
that we'll see later on

00:24:56,770 --> 00:25:01,430
so again you can run on the same

00:24:58,940 --> 00:25:03,080
infrastructure so I'll do the first demo

00:25:01,430 --> 00:25:04,880
and you can try and follow with me is

00:25:03,080 --> 00:25:07,340
anybody not logged in and then you've

00:25:04,880 --> 00:25:09,320
got problems do you want to tell me why

00:25:07,340 --> 00:25:14,030
or create a new that's what you should

00:25:09,320 --> 00:25:15,740
do is create a new email address no you

00:25:14,030 --> 00:25:22,270
don't receive an email you just try and

00:25:15,740 --> 00:25:22,270
log in sorry

00:25:25,110 --> 00:25:34,440
you sorry you yeah hmm

00:25:35,190 --> 00:25:41,350
okay tell you what to do you should if

00:25:38,440 --> 00:25:43,510
you go back here there's this update

00:25:41,350 --> 00:25:45,490
your Google spreadsheet just write your

00:25:43,510 --> 00:25:47,370
email address that you created I have

00:25:45,490 --> 00:25:51,670
some guys on slack and I'll ask them to

00:25:47,370 --> 00:25:53,290
to just register the account so if you

00:25:51,670 --> 00:25:55,030
just write in your email in this

00:25:53,290 --> 00:25:59,790
spreadsheet down here anyone has any

00:25:55,030 --> 00:25:59,790
problems go to the house today or /tf

00:26:03,060 --> 00:26:06,060
okay

00:26:15,610 --> 00:26:21,430
okay so they should take care of this so

00:26:18,370 --> 00:26:23,650
just update this right in your and

00:26:21,430 --> 00:26:26,020
they'll and they'll fix that hopefully

00:26:23,650 --> 00:26:31,890
within a second okay I'm gonna log in

00:26:26,020 --> 00:26:34,120
and what I'm gonna do is I'm going to I

00:26:31,890 --> 00:26:37,360
think a log in here actually have two

00:26:34,120 --> 00:26:39,280
accounts open so for those who you

00:26:37,360 --> 00:26:40,660
haven't logged in so you just put in

00:26:39,280 --> 00:26:42,460
your email and password there is support

00:26:40,660 --> 00:26:46,290
for two factor authentication there is

00:26:42,460 --> 00:26:51,370
of course HTTPS but in this case I just

00:26:46,290 --> 00:26:53,290
disabled so if you haven't run this tour

00:26:51,370 --> 00:26:55,660
you can just click on the Tor tips and

00:26:53,290 --> 00:26:57,310
enable that it's quite annoying you'll

00:26:55,660 --> 00:26:59,620
need to disable it later on because it

00:26:57,310 --> 00:27:00,220
just keeps popping up and then click on

00:26:59,620 --> 00:27:03,160
tensorflow

00:27:00,220 --> 00:27:04,900
I've already done this so I'm just going

00:27:03,160 --> 00:27:07,390
to go in and look at tensorflow

00:27:04,900 --> 00:27:08,740
so this is what these are the projects

00:27:07,390 --> 00:27:10,300
here on the right-hand side that I

00:27:08,740 --> 00:27:13,330
mentioned before and if I want to create

00:27:10,300 --> 00:27:19,270
a new project let's call this one

00:27:13,330 --> 00:27:21,340
Stockholm account spelled Sacco okay so

00:27:19,270 --> 00:27:22,840
creating a project is relatively quick

00:27:21,340 --> 00:27:27,760
thing you can see it takes a couple of

00:27:22,840 --> 00:27:29,710
seconds and that project so I if I take

00:27:27,760 --> 00:27:32,560
my project here the demo tensorflow

00:27:29,710 --> 00:27:34,420
if you feel friendly you can talk to

00:27:32,560 --> 00:27:35,920
your neighbor and say well you know

00:27:34,420 --> 00:27:39,160
maybe I'll add your my neighbor to my

00:27:35,920 --> 00:27:41,740
project so you can go in here and I can

00:27:39,160 --> 00:27:43,690
add in this case I can add myself to the

00:27:41,740 --> 00:27:47,130
project and I can have myself as one of

00:27:43,690 --> 00:27:49,150
two roles data scientist or data owner

00:27:47,130 --> 00:27:53,980
so if you talk to your neighbor you can

00:27:49,150 --> 00:27:55,330
say well I'll add you whatever and now

00:27:53,980 --> 00:27:59,370
what I'm going to do is I'm going to do

00:27:55,330 --> 00:28:02,140
this first part of the pipeline which is

00:27:59,370 --> 00:28:03,400
data collection data transformation and

00:28:02,140 --> 00:28:05,740
feature extraction I'll just show you

00:28:03,400 --> 00:28:09,550
one one feature that I think is quite

00:28:05,740 --> 00:28:11,560
nice which is something called Google

00:28:09,550 --> 00:28:16,060
facets so what Google facets is is a

00:28:11,560 --> 00:28:18,340
plugin to to Jupiter and it allows you

00:28:16,060 --> 00:28:20,620
to take any data set that you can read

00:28:18,340 --> 00:28:22,240
up in a panda's data frame and there's

00:28:20,620 --> 00:28:25,830
kind of a practical limit of a better

00:28:22,240 --> 00:28:29,080
gigabyte or two gigabytes for this and

00:28:25,830 --> 00:28:30,610
it will then allow you to visualize

00:28:29,080 --> 00:28:32,230
both in numerical and categorical

00:28:30,610 --> 00:28:34,210
features you'll be able to see things

00:28:32,230 --> 00:28:36,070
like what columns have missing volumes

00:28:34,210 --> 00:28:38,740
used what are the average values for

00:28:36,070 --> 00:28:40,450
columns and there's another one called

00:28:38,740 --> 00:28:44,350
Google dive which allows you to actually

00:28:40,450 --> 00:28:47,740
look at individual records or rows in

00:28:44,350 --> 00:28:49,570
your in your data frame and you can then

00:28:47,740 --> 00:28:51,070
look at different distributions so I'll

00:28:49,570 --> 00:28:55,060
do a quick look at that just to show you

00:28:51,070 --> 00:28:56,620
how it works let's go back here so I'm

00:28:55,060 --> 00:28:58,660
going to start a Jupiter notebook for

00:28:56,620 --> 00:29:00,700
this you can just press Start Jupiter

00:28:58,660 --> 00:29:02,710
and it should just work if you're

00:29:00,700 --> 00:29:04,660
curious you can play around with this a

00:29:02,710 --> 00:29:07,330
bit I've set the limits of the amount of

00:29:04,660 --> 00:29:11,410
memory to about five or something

00:29:07,330 --> 00:29:12,760
gigabytes I don't remember number of

00:29:11,410 --> 00:29:16,840
parallel executions I think it were

00:29:12,760 --> 00:29:19,630
limited to two right now and this is

00:29:16,840 --> 00:29:22,000
like you can add jars and things and so

00:29:19,630 --> 00:29:24,430
on to your program if you want to but

00:29:22,000 --> 00:29:25,960
I'm just going to run it so the

00:29:24,430 --> 00:29:28,990
libraries that will be available to this

00:29:25,960 --> 00:29:32,440
notebook that I've just started will be

00:29:28,990 --> 00:29:35,590
will be visual you can find them here in

00:29:32,440 --> 00:29:37,720
the Python this these are micro-services

00:29:35,590 --> 00:29:39,640
on the left hand side so if I go into

00:29:37,720 --> 00:29:41,200
the Python one I can see which libraries

00:29:39,640 --> 00:29:44,130
are installed so these ones are

00:29:41,200 --> 00:29:47,080
available so pandas is already there

00:29:44,130 --> 00:29:49,450
facets is a is a plugin for Jupiter it's

00:29:47,080 --> 00:29:53,640
already enabled and tensorflow is pre

00:29:49,450 --> 00:29:56,770
installed you can see as is hops and

00:29:53,640 --> 00:29:58,150
HDFS connector called PI dupe so I'm

00:29:56,770 --> 00:30:00,610
going to go back to Jupiter so if you

00:29:58,150 --> 00:30:02,110
get this far to Jupiter and if you want

00:30:00,610 --> 00:30:04,660
to try out facets you just click on the

00:30:02,110 --> 00:30:06,760
facets folder and you can try facets

00:30:04,660 --> 00:30:09,370
overview or facets dive I will go with

00:30:06,760 --> 00:30:11,920
overview and you can see it's going to

00:30:09,370 --> 00:30:13,840
start a normal Python kernel so it's not

00:30:11,920 --> 00:30:18,250
running PI SPARC yes this is just normal

00:30:13,840 --> 00:30:20,800
Python and this will only work if you

00:30:18,250 --> 00:30:23,560
have Google Chrome if you're running

00:30:20,800 --> 00:30:26,290
Firefox it won't work I'm afraid it's a

00:30:23,560 --> 00:30:28,960
Google product what can I say but it's

00:30:26,290 --> 00:30:30,400
quite nice if you have Google Chrome so

00:30:28,960 --> 00:30:32,500
I'll just explain what this code is

00:30:30,400 --> 00:30:35,980
doing just so you can see it it's

00:30:32,500 --> 00:30:38,800
basically taking a number of features

00:30:35,980 --> 00:30:40,390
defined in a list here so we have these

00:30:38,800 --> 00:30:41,810
are the features that defined in a data

00:30:40,390 --> 00:30:44,470
set called the

00:30:41,810 --> 00:30:47,420
census data from you from the US and

00:30:44,470 --> 00:30:49,550
it's reading it from HDFS so it's going

00:30:47,420 --> 00:30:52,190
to take the path to your project there's

00:30:49,550 --> 00:30:53,990
a folder in there called test job data

00:30:52,190 --> 00:30:57,680
census idle top data

00:30:53,990 --> 00:31:00,470
it's the CSV file and it's going to read

00:30:57,680 --> 00:31:01,970
that up into this pandas dataframe train

00:31:00,470 --> 00:31:04,610
data and we'll do the same thing with

00:31:01,970 --> 00:31:06,350
the pandas dataframe test data so if

00:31:04,610 --> 00:31:09,020
you're curious about that data see this

00:31:06,350 --> 00:31:11,810
adult data and adult test we can go back

00:31:09,020 --> 00:31:14,810
to hops works and I'm going to go back

00:31:11,810 --> 00:31:17,210
here to data sets and we had this thing

00:31:14,810 --> 00:31:17,660
called test job here so I'll click on

00:31:17,210 --> 00:31:20,090
that

00:31:17,660 --> 00:31:23,780
and inside there I'm gonna follow this

00:31:20,090 --> 00:31:25,940
path data census and then we have adult

00:31:23,780 --> 00:31:27,680
test and adult data if you're curious

00:31:25,940 --> 00:31:30,320
you can download it or you can preview

00:31:27,680 --> 00:31:34,700
it and you can see some of the values

00:31:30,320 --> 00:31:38,090
there let me show you small amount of

00:31:34,700 --> 00:31:41,030
the values but I'm just gonna go back

00:31:38,090 --> 00:31:45,500
and show you what we have in here and

00:31:41,030 --> 00:31:48,080
facets so what we have here is we can

00:31:45,500 --> 00:31:50,240
see what we call numeric features so if

00:31:48,080 --> 00:31:53,120
you have a panda's data frame and it has

00:31:50,240 --> 00:31:56,360
in this case I think it has 11 if I'm

00:31:53,120 --> 00:32:00,020
not correct if not mistaken 15 sorry has

00:31:56,360 --> 00:32:02,720
15 features you can see that 6 of them

00:32:00,020 --> 00:32:04,970
are integers and 9 of them's are strings

00:32:02,720 --> 00:32:06,530
but data scientists don't like the terms

00:32:04,970 --> 00:32:08,150
integers and strings so they called in

00:32:06,530 --> 00:32:11,030
numeric features they don't like the

00:32:08,150 --> 00:32:12,620
term columns or the column features so

00:32:11,030 --> 00:32:14,690
what we would call a column events they

00:32:12,620 --> 00:32:16,400
were called a numeric feature and what

00:32:14,690 --> 00:32:20,930
we would call a column of strings

00:32:16,400 --> 00:32:22,460
they'll call a categorical feature now

00:32:20,930 --> 00:32:24,800
if you're if you know Python who

00:32:22,460 --> 00:32:27,380
programs Python here a bit just has a

00:32:24,800 --> 00:32:30,440
few okay so if you're curious about do I

00:32:27,380 --> 00:32:32,360
trust the values here you can see I was

00:32:30,440 --> 00:32:34,430
just checking earlier on I said well

00:32:32,360 --> 00:32:36,370
there's no missing columns here in the

00:32:34,430 --> 00:32:39,590
numeric features but if we look in the

00:32:36,370 --> 00:32:42,530
categorical features 5 percent of the

00:32:39,590 --> 00:32:46,130
work class features of missing values

00:32:42,530 --> 00:32:49,750
and countries of some missing values so

00:32:46,130 --> 00:32:51,980
what I did earlier on was I basically I

00:32:49,750 --> 00:32:54,080
did the following I just printed out

00:32:51,980 --> 00:32:55,140
which columns I want to check which

00:32:54,080 --> 00:32:56,310
columns have no

00:32:55,140 --> 00:33:00,480
you can see that it says work class

00:32:56,310 --> 00:33:04,020
occupation country have no values in

00:33:00,480 --> 00:33:06,810
there so that seems to to coincide with

00:33:04,020 --> 00:33:09,210
what we can see here in the in the

00:33:06,810 --> 00:33:11,370
visualization now the point of this

00:33:09,210 --> 00:33:13,320
visualization is if you've done machine

00:33:11,370 --> 00:33:15,090
learning before and deep learning you'll

00:33:13,320 --> 00:33:17,940
know that we have what's called training

00:33:15,090 --> 00:33:20,220
data and test data or validation data

00:33:17,940 --> 00:33:21,660
and you train on the training set and

00:33:20,220 --> 00:33:24,180
when you're happy that you've trained

00:33:21,660 --> 00:33:25,890
you then validate your training see how

00:33:24,180 --> 00:33:28,290
accurate the model is you've trained on

00:33:25,890 --> 00:33:30,600
this holdout data that you haven't used

00:33:28,290 --> 00:33:32,040
before but one of the things you want to

00:33:30,600 --> 00:33:33,990
do when you when you split your data

00:33:32,040 --> 00:33:35,760
into training and test data is you want

00:33:33,990 --> 00:33:38,430
to make sure that the data distribution

00:33:35,760 --> 00:33:40,410
that you're learning is the same in both

00:33:38,430 --> 00:33:42,240
the training data and the test data

00:33:40,410 --> 00:33:44,850
because if they're different you'll

00:33:42,240 --> 00:33:46,320
train on one data distribution and then

00:33:44,850 --> 00:33:48,390
you're going to try and validate it on a

00:33:46,320 --> 00:33:50,880
different distribution then you won't

00:33:48,390 --> 00:33:53,610
get good results so this is a tool which

00:33:50,880 --> 00:33:55,980
allows you to you can see here that we

00:33:53,610 --> 00:33:59,520
can see some of the the mean values and

00:33:55,980 --> 00:34:01,980
the max then the median and then you can

00:33:59,520 --> 00:34:04,350
see some other visualizations of this

00:34:01,980 --> 00:34:06,540
with histograms so you get a pretty

00:34:04,350 --> 00:34:08,760
quick feeling of whether that's your

00:34:06,540 --> 00:34:11,010
that you're happy with your data now if

00:34:08,760 --> 00:34:12,990
we got a python dive it's it's quite

00:34:11,010 --> 00:34:16,290
similar you can run it as well if you

00:34:12,990 --> 00:34:19,110
have a Google Karl and it's gonna read

00:34:16,290 --> 00:34:24,060
the same pandas dataframe but it's going

00:34:19,110 --> 00:34:25,290
to read it into a into into fastest dive

00:34:24,060 --> 00:34:28,260
so you can see it looks a bit different

00:34:25,290 --> 00:34:31,050
if you click on any point here on the

00:34:28,260 --> 00:34:33,120
right hand side you can see the it's

00:34:31,050 --> 00:34:36,330
gonna show the row the contents of that

00:34:33,120 --> 00:34:37,620
row so what this particular what's

00:34:36,330 --> 00:34:41,790
happening right now is that it's

00:34:37,620 --> 00:34:46,530
actually faceting the data by we can do

00:34:41,790 --> 00:34:48,390
by this is education status I think so

00:34:46,530 --> 00:34:50,640
you can you can you can basically change

00:34:48,390 --> 00:34:52,590
how you would like to view the data you

00:34:50,640 --> 00:34:55,260
can say well I'd like to view the data

00:34:52,590 --> 00:34:56,370
by my education status or by age and it

00:34:55,260 --> 00:34:58,590
will show you then the different

00:34:56,370 --> 00:35:01,020
distributions of people so we can see

00:34:58,590 --> 00:35:04,170
most people in this dataset are in these

00:35:01,020 --> 00:35:05,940
particular age brackets and then you if

00:35:04,170 --> 00:35:08,250
you think one of the data points is a

00:35:05,940 --> 00:35:10,109
little bit off like this one at the top

00:35:08,250 --> 00:35:12,420
we can get one up here we can see this

00:35:10,109 --> 00:35:16,260
one has an age of 90 and then some more

00:35:12,420 --> 00:35:19,410
details about them okay so this is what

00:35:16,260 --> 00:35:25,020
we call feature engineering I'm going to

00:35:19,410 --> 00:35:26,220
shut down this notebook I am the way you

00:35:25,020 --> 00:35:27,720
should have down when you come out here

00:35:26,220 --> 00:35:29,880
is you can just press this button here

00:35:27,720 --> 00:35:32,580
which will shut down your jupiter

00:35:29,880 --> 00:35:39,090
notebook for you I didn't actually close

00:35:32,580 --> 00:35:40,830
it here sorry for that does anyone have

00:35:39,090 --> 00:35:43,950
issues that aren't being handled right

00:35:40,830 --> 00:35:46,910
now you can write them into the to the

00:35:43,950 --> 00:35:50,150
into the spreadsheet

00:35:46,910 --> 00:35:56,670
any questions on facets no these enough

00:35:50,150 --> 00:35:58,260
right so let's move ahead yeah so if

00:35:56,670 --> 00:36:00,300
you're a Python person I just wanted to

00:35:58,260 --> 00:36:02,849
impress on you that the fact that your

00:36:00,300 --> 00:36:05,690
data is in HDFS just means that you get

00:36:02,849 --> 00:36:08,340
this one different line of code change

00:36:05,690 --> 00:36:11,450
sorry we go back here this right so it's

00:36:08,340 --> 00:36:14,400
not a big deal so the fact there many

00:36:11,450 --> 00:36:17,040
Python frameworks like tensorflow pandas

00:36:14,400 --> 00:36:21,300
PI spark they support HDFS natively

00:36:17,040 --> 00:36:22,920
which we do too okay so the next part of

00:36:21,300 --> 00:36:24,420
our pipeline and the pipeline's up on

00:36:22,920 --> 00:36:27,240
the top right hand side if you want to

00:36:24,420 --> 00:36:29,490
know where we are is we're still in the

00:36:27,240 --> 00:36:33,480
data perhaps the the you know the data

00:36:29,490 --> 00:36:35,580
wrangling phase and tensorflow since 1.4

00:36:33,480 --> 00:36:38,760
i believe in core they introduced the TF

00:36:35,580 --> 00:36:40,260
data api it's a new data set api and in

00:36:38,760 --> 00:36:43,320
it you can do you can put in map

00:36:40,260 --> 00:36:46,589
functions to you can pre-process your

00:36:43,320 --> 00:36:48,119
data in pipelines in tensorflow the only

00:36:46,589 --> 00:36:51,570
problem with it is that it's a bit of a

00:36:48,119 --> 00:36:54,240
mess right so if you like nice clean

00:36:51,570 --> 00:36:55,980
pythonic code this is not the code view

00:36:54,240 --> 00:36:57,869
right if you want to write a for loop

00:36:55,980 --> 00:37:00,270
you don't write a for loop and this is

00:36:57,869 --> 00:37:01,980
one of the reasons why for example pi

00:37:00,270 --> 00:37:05,130
torch is gaining adoption because it's

00:37:01,980 --> 00:37:07,619
very pythonic but there's no need to

00:37:05,130 --> 00:37:09,390
have to worry much about this this will

00:37:07,619 --> 00:37:11,460
only run on a single server you need to

00:37:09,390 --> 00:37:13,260
care about things like how many threads

00:37:11,460 --> 00:37:15,000
are used for pre batching how many

00:37:13,260 --> 00:37:17,460
parallel batches are used if you want

00:37:15,000 --> 00:37:19,770
this thing to work efficiently and but

00:37:17,460 --> 00:37:21,910
just our advice would be forget about it

00:37:19,770 --> 00:37:25,810
use PI spark

00:37:21,910 --> 00:37:27,190
and I put your data as TF records right

00:37:25,810 --> 00:37:29,700
because this will be efficient that will

00:37:27,190 --> 00:37:32,080
scale with the number of executors and

00:37:29,700 --> 00:37:35,740
then when your data is how you you will

00:37:32,080 --> 00:37:38,110
have a another job to orchestrate these

00:37:35,740 --> 00:37:39,910
different parts of your pipeline in our

00:37:38,110 --> 00:37:42,040
case we use spark to orchestrate two

00:37:39,910 --> 00:37:43,990
different jobs but maybe different

00:37:42,040 --> 00:37:48,550
people in different frameworks will use

00:37:43,990 --> 00:37:50,290
other other tools so this is what the

00:37:48,550 --> 00:37:53,740
code looks like in in pi spark for

00:37:50,290 --> 00:37:55,420
taking for example images and resizing

00:37:53,740 --> 00:37:57,400
them and cropping them that's what we

00:37:55,420 --> 00:38:00,310
want to do with our hot dog images I've

00:37:57,400 --> 00:38:01,720
done this already by the way you can see

00:38:00,310 --> 00:38:03,550
the code is very clean right you're

00:38:01,720 --> 00:38:05,830
basically reading images the path can be

00:38:03,550 --> 00:38:09,130
in HDFS here's a really nice feature

00:38:05,830 --> 00:38:11,800
this sample ratio so if you're in the

00:38:09,130 --> 00:38:13,360
experimentation phrase and you have 300

00:38:11,800 --> 00:38:15,490
gigabytes of data you don't necessarily

00:38:13,360 --> 00:38:17,740
want to read up 300 gigabytes of data

00:38:15,490 --> 00:38:20,590
into a data frame you might want to read

00:38:17,740 --> 00:38:27,070
up just you know 300 megabytes and then

00:38:20,590 --> 00:38:28,300
your sample ratio is 0.0 0.8 up in a

00:38:27,070 --> 00:38:30,700
data frame you can now apply a

00:38:28,300 --> 00:38:33,250
transformer this is using something

00:38:30,700 --> 00:38:34,990
called FML spark from Microsoft it's an

00:38:33,250 --> 00:38:37,150
open source library they have for

00:38:34,990 --> 00:38:40,510
machine learning on spark and they have

00:38:37,150 --> 00:38:43,210
a transformer for manipulating images

00:38:40,510 --> 00:38:46,930
and images support is now in spark 2.3

00:38:43,210 --> 00:38:48,490
and then Yahoo have a framework called

00:38:46,930 --> 00:38:50,620
tensorflow on spark and they have some

00:38:48,490 --> 00:38:54,160
libraries for taking data frames and

00:38:50,620 --> 00:38:56,590
exporting them as TF records so that's

00:38:54,160 --> 00:38:58,480
that's how you basically get to the

00:38:56,590 --> 00:39:02,380
point where you're now ready to play

00:38:58,480 --> 00:39:03,910
with your models and tensorflow this is

00:39:02,380 --> 00:39:07,270
where I get back to the distribution

00:39:03,910 --> 00:39:08,890
issue that when you're training a deep

00:39:07,270 --> 00:39:10,930
learning model you think well it's a

00:39:08,890 --> 00:39:12,790
loop a training loop but there that's

00:39:10,930 --> 00:39:14,140
the inner loop right the outer loop is

00:39:12,790 --> 00:39:16,710
when you actually need to look for

00:39:14,140 --> 00:39:19,600
things like a good model architecture a

00:39:16,710 --> 00:39:22,000
good set of hyper parameters and no

00:39:19,600 --> 00:39:23,770
matter what way cuts that takes a lot of

00:39:22,000 --> 00:39:25,450
time and a lot of experimentation and

00:39:23,770 --> 00:39:27,880
you can we think you can cut that down

00:39:25,450 --> 00:39:30,970
massively with distributor distribution

00:39:27,880 --> 00:39:32,890
running them in parallel so we look at

00:39:30,970 --> 00:39:35,320
that little way of doing that in in hops

00:39:32,890 --> 00:39:37,450
if you're curious and want to follow

00:39:35,320 --> 00:39:38,890
and do this you can are going to jump to

00:39:37,450 --> 00:39:41,470
the code is this hyper parameter

00:39:38,890 --> 00:39:44,470
optimization in hopes there's a notebook

00:39:41,470 --> 00:39:47,950
for this so if we go back here and start

00:39:44,470 --> 00:39:48,960
Jupiter this one is going to take a bit

00:39:47,950 --> 00:39:50,770
more time

00:39:48,960 --> 00:39:52,510
because you're going to do quite a lot

00:39:50,770 --> 00:39:57,820
of training so if you go to tensorflow

00:39:52,510 --> 00:39:59,440
and CNN and we have two of them here one

00:39:57,820 --> 00:40:02,440
is called hyper parameter search on C

00:39:59,440 --> 00:40:05,170
410 that will take half the evening so I

00:40:02,440 --> 00:40:08,470
would will kind of skip that I look at

00:40:05,170 --> 00:40:10,480
grid search on fashion M NIST so fashion

00:40:08,470 --> 00:40:12,910
Ebony's came from from Berlin actually

00:40:10,480 --> 00:40:14,650
it's a it's a data set to replace the

00:40:12,910 --> 00:40:17,080
classic MS amnesty

00:40:14,650 --> 00:40:19,420
10 different images of clothing 10

00:40:17,080 --> 00:40:25,210
different classes of clothing and it was

00:40:19,420 --> 00:40:26,670
released by by these guys I think what's

00:40:25,210 --> 00:40:30,970
the name of your company again

00:40:26,670 --> 00:40:32,820
her brain's gone dead I slander yes

00:40:30,970 --> 00:40:36,730
Lando released this data said thank you

00:40:32,820 --> 00:40:40,030
you saved me okay so I'm gonna run this

00:40:36,730 --> 00:40:41,230
but like so what happened when I run

00:40:40,030 --> 00:40:42,670
this I didn't I didn't go into any

00:40:41,230 --> 00:40:44,140
details you can pick a number of

00:40:42,670 --> 00:40:47,410
executors that you want to run in

00:40:44,140 --> 00:40:49,840
parallel if you have I'll just show you

00:40:47,410 --> 00:40:51,850
what happens when you run it in this

00:40:49,840 --> 00:40:54,700
case you just define a dict with the

00:40:51,850 --> 00:40:56,890
combinations of learning rate and

00:40:54,700 --> 00:40:59,770
dropout rate these are hyper parameters

00:40:56,890 --> 00:41:02,830
in the deep learning model in this case

00:40:59,770 --> 00:41:04,570
it'll run six different spark executors

00:41:02,830 --> 00:41:06,550
each of them will run just one

00:41:04,570 --> 00:41:08,740
experiment the first one will have a

00:41:06,550 --> 00:41:10,600
learning rate of zero zero one the

00:41:08,740 --> 00:41:12,550
second will have a dropout rate of 0.5

00:41:10,600 --> 00:41:14,050
and then and so on

00:41:12,550 --> 00:41:16,330
the second one will have a learn a

00:41:14,050 --> 00:41:19,390
learning rate of zero zero five and the

00:41:16,330 --> 00:41:21,280
dropout rate is zero five so when all

00:41:19,390 --> 00:41:24,520
that's run we'll get the results in in a

00:41:21,280 --> 00:41:30,130
in a nice tensor board that we can

00:41:24,520 --> 00:41:32,470
visualize and it kind of looks like this

00:41:30,130 --> 00:41:34,030
you'll see your learning rates here and

00:41:32,470 --> 00:41:37,200
then you can look up here to find out

00:41:34,030 --> 00:41:39,280
the best combination of learning rates

00:41:37,200 --> 00:41:40,660
and but there's another thing that I

00:41:39,280 --> 00:41:43,600
said don't press the button on because

00:41:40,660 --> 00:41:46,300
this is very compute-intensive it's a

00:41:43,600 --> 00:41:48,330
new area of my love not just research

00:41:46,300 --> 00:41:51,480
but development which is

00:41:48,330 --> 00:41:53,280
can the best computers find better

00:41:51,480 --> 00:41:55,950
models than the best machine learning

00:41:53,280 --> 00:42:00,030
experts and the answer is yes it's a

00:41:55,950 --> 00:42:02,820
resounding yes so Google this year and

00:42:00,030 --> 00:42:04,530
last year they released a two different

00:42:02,820 --> 00:42:06,480
frameworks one on reinforcement learning

00:42:04,530 --> 00:42:09,150
and another one on genetic algorithms

00:42:06,480 --> 00:42:11,730
and they improved the state of the art

00:42:09,150 --> 00:42:14,130
for training image classification on a

00:42:11,730 --> 00:42:16,920
well known data set called image net so

00:42:14,130 --> 00:42:19,560
they beat the best results gotten by

00:42:16,920 --> 00:42:21,240
humans by having computer programs

00:42:19,560 --> 00:42:24,000
reinforcement learning agents and

00:42:21,240 --> 00:42:26,940
genetic algorithms search for a better

00:42:24,000 --> 00:42:29,040
model architecture so you know a good

00:42:26,940 --> 00:42:31,260
you know the people in the area talk

00:42:29,040 --> 00:42:32,520
talk in the valley that you know a good

00:42:31,260 --> 00:42:35,070
data scientists and get a million

00:42:32,520 --> 00:42:36,990
dollars well if the good data scientist

00:42:35,070 --> 00:42:38,630
is just picking good combinations of

00:42:36,990 --> 00:42:41,010
hyper parameters and designing good

00:42:38,630 --> 00:42:42,240
models in terms of which layers it

00:42:41,010 --> 00:42:44,730
should have and how many it should have

00:42:42,240 --> 00:42:47,190
if the computer can do that by basically

00:42:44,730 --> 00:42:49,500
searching at using many GPUs in parallel

00:42:47,190 --> 00:42:52,260
well I don't know which will be cheaper

00:42:49,500 --> 00:42:52,800
in a few years make hay while the Sun

00:42:52,260 --> 00:42:55,470
shines

00:42:52,800 --> 00:42:57,330
is what we say okay so this this

00:42:55,470 --> 00:42:58,650
particular example we we have genetic

00:42:57,330 --> 00:43:04,530
algorithms and hops we have something

00:42:58,650 --> 00:43:06,060
called evolutionary search this is it

00:43:04,530 --> 00:43:09,180
will basically run for 10 generations

00:43:06,060 --> 00:43:12,210
it's doing genetic algorithms and you

00:43:09,180 --> 00:43:14,100
can specify mutation rates and crossover

00:43:12,210 --> 00:43:15,630
rates and how many executors your

00:43:14,100 --> 00:43:18,690
population size and number of executors

00:43:15,630 --> 00:43:19,860
inspark you're going to have and then it

00:43:18,690 --> 00:43:22,980
will search using different combinations

00:43:19,860 --> 00:43:24,750
of those hyper parameters okay so this

00:43:22,980 --> 00:43:26,610
is showing you just how it runs because

00:43:24,750 --> 00:43:29,340
it takes a little bit the time so the

00:43:26,610 --> 00:43:32,670
first run see far ten has ten different

00:43:29,340 --> 00:43:34,710
image classes so if you do random if you

00:43:32,670 --> 00:43:36,600
train around the model you'll get a 10%

00:43:34,710 --> 00:43:38,520
chance of being right so this is 10%

00:43:36,600 --> 00:43:40,350
chance of being right but after one

00:43:38,520 --> 00:43:42,690
generation you can see that we get some

00:43:40,350 --> 00:43:44,310
improvement and genetic algorithms will

00:43:42,690 --> 00:43:46,680
pick out the best ones and it will

00:43:44,310 --> 00:43:48,120
generate mutations from the best one so

00:43:46,680 --> 00:43:51,090
within one more generation we get a

00:43:48,120 --> 00:43:54,060
quite a large improvement in the model

00:43:51,090 --> 00:43:56,190
accuracy and then you know we got even

00:43:54,060 --> 00:43:57,330
more improvement so you know you're

00:43:56,190 --> 00:44:00,270
gonna have at some point you're gonna

00:43:57,330 --> 00:44:01,710
have diminishing returns on this but

00:44:00,270 --> 00:44:03,720
it's a good way of it's an

00:44:01,710 --> 00:44:06,780
promising new ways autumn Auto MLS

00:44:03,720 --> 00:44:08,070
called of having computers design the

00:44:06,780 --> 00:44:12,750
best models for your for your

00:44:08,070 --> 00:44:16,380
architecture okay so that's the

00:44:12,750 --> 00:44:18,780
experiment phase let's go back in and

00:44:16,380 --> 00:44:20,880
see how this thing was going right so I

00:44:18,780 --> 00:44:24,390
started this running earlier on didn't I

00:44:20,880 --> 00:44:26,520
the grid search so you can see it's it's

00:44:24,390 --> 00:44:32,330
that you still get a star going on here

00:44:26,520 --> 00:44:35,010
is anyone running this no one okay

00:44:32,330 --> 00:44:36,990
alright so I'll just show you when it's

00:44:35,010 --> 00:44:38,849
running if you want to look at how its

00:44:36,990 --> 00:44:41,160
proceeding here and you can just start

00:44:38,849 --> 00:44:43,140
it running now and and have a look at it

00:44:41,160 --> 00:44:46,530
what will happen is this button will

00:44:43,140 --> 00:44:49,260
appear here it says a spark UI for your

00:44:46,530 --> 00:44:51,990
session so if you press that button you

00:44:49,260 --> 00:44:53,369
can actually get to spark UI we can look

00:44:51,990 --> 00:44:54,990
at how many executors are running I

00:44:53,369 --> 00:44:57,599
think I only had one executor but maybe

00:44:54,990 --> 00:44:59,580
I had two you have only one executor and

00:44:57,599 --> 00:45:01,470
the driver so it's only gonna run one

00:44:59,580 --> 00:45:03,210
experiment at a time but you can have

00:45:01,470 --> 00:45:05,390
six you know if you had six executors

00:45:03,210 --> 00:45:07,619
then they'll run six in in parallel and

00:45:05,390 --> 00:45:09,900
you can look at the tensor board while

00:45:07,619 --> 00:45:11,190
it's training when this image when this

00:45:09,900 --> 00:45:13,170
run finishes the tensor board will

00:45:11,190 --> 00:45:15,390
disappear so it only appears for a

00:45:13,170 --> 00:45:17,220
little bit and you can see that you know

00:45:15,390 --> 00:45:22,080
it's it's getting quite good accuracy

00:45:17,220 --> 00:45:25,530
it's up to 83% and if you want to look

00:45:22,080 --> 00:45:27,599
at logs this is an issue that you have

00:45:25,530 --> 00:45:29,700
in yarn so if you're working in a yarn

00:45:27,599 --> 00:45:31,320
environment you'll know that logs are

00:45:29,700 --> 00:45:33,810
only aggregated when when the job

00:45:31,320 --> 00:45:35,280
completes which is no good so if you

00:45:33,810 --> 00:45:37,680
click on the Cabana boat and you get

00:45:35,280 --> 00:45:39,720
your logs in real time and you know if

00:45:37,680 --> 00:45:43,849
you want you can even do graphing a

00:45:39,720 --> 00:45:48,270
based on your logs and then we have some

00:45:43,849 --> 00:45:53,040
metrics for the job in coming in and

00:45:48,270 --> 00:45:54,420
graph Ana here and then there's also the

00:45:53,040 --> 00:45:56,970
yarn you are if you're interested in

00:45:54,420 --> 00:46:00,900
that for that so you can you can

00:45:56,970 --> 00:46:02,220
basically we go back here you can

00:46:00,900 --> 00:46:04,589
basically track the progress of your

00:46:02,220 --> 00:46:07,560
jobs not just in here and the notebook

00:46:04,589 --> 00:46:10,349
but also using this these set of you

00:46:07,560 --> 00:46:12,690
eyes here and you can break them out

00:46:10,349 --> 00:46:18,640
into new window if you want to

00:46:12,690 --> 00:46:20,950
okay anyone any questions or points

00:46:18,640 --> 00:46:24,880
we're not very interactive at this point

00:46:20,950 --> 00:46:28,269
okay so what I suggest is who has run

00:46:24,880 --> 00:46:31,450
something right who's run any tensile

00:46:28,269 --> 00:46:35,079
notebook well and a few over here okay

00:46:31,450 --> 00:46:37,329
so let's everyone try and who's in here

00:46:35,079 --> 00:46:39,849
let's try and just do the first part

00:46:37,329 --> 00:46:43,349
which was serving a model and then

00:46:39,849 --> 00:46:51,210
having a client run on that train model

00:46:43,349 --> 00:46:51,210
so yeah yeah

00:46:57,320 --> 00:47:01,900
moon is my friend but I I would say

00:46:59,360 --> 00:47:04,490
cause Apple is not so good

00:47:01,900 --> 00:47:06,200
Jupiter the development effort that's

00:47:04,490 --> 00:47:07,910
gone into making Jupiter better in the

00:47:06,200 --> 00:47:10,160
big data space in the last two years has

00:47:07,910 --> 00:47:12,170
been huge so we're using something

00:47:10,160 --> 00:47:13,400
called spark Colonel here for Jupiter

00:47:12,170 --> 00:47:19,550
which is quite nice and you obviously

00:47:13,400 --> 00:47:21,590
with the Python kernels Zeppelin we

00:47:19,550 --> 00:47:23,360
haven't and we and and we wrote a

00:47:21,590 --> 00:47:26,000
connector for Jupiter to put the

00:47:23,360 --> 00:47:27,650
notebooks in in HDFS so anybody logged

00:47:26,000 --> 00:47:29,320
in from any server anywhere they'll see

00:47:27,650 --> 00:47:31,880
the same view of all the notebooks

00:47:29,320 --> 00:47:36,280
the problem with Zeppelin is that it

00:47:31,880 --> 00:47:36,280
we've had stability issues with it and

00:47:36,730 --> 00:47:42,200
isolation so it's it's designed to have

00:47:39,080 --> 00:47:43,790
people be able to collaborate on the

00:47:42,200 --> 00:47:47,210
same notebook but in practice we don't

00:47:43,790 --> 00:47:49,070
find that it works that well so we have

00:47:47,210 --> 00:47:50,480
where that we're limp we've limbs that

00:47:49,070 --> 00:47:52,640
we have support or Zeppelin but it's

00:47:50,480 --> 00:47:55,190
really we're encouraging people to use

00:47:52,640 --> 00:47:59,290
things like hive and looking at sequel

00:47:55,190 --> 00:47:59,290
in there rather than writing spark jobs

00:47:59,380 --> 00:48:04,070
so we're moving more off to Jupiter okay

00:48:02,180 --> 00:48:05,840
let's do the model serving thing because

00:48:04,070 --> 00:48:07,900
I have one running all right I'll just

00:48:05,840 --> 00:48:09,770
stop that one and we'll do a new one

00:48:07,900 --> 00:48:13,130
okay so I'm going to start you there

00:48:09,770 --> 00:48:14,690
notebook and we go back here so anybody

00:48:13,130 --> 00:48:17,300
wants to follow along just go into your

00:48:14,690 --> 00:48:21,590
demo tensor flow tours start your

00:48:17,300 --> 00:48:24,560
Jupiter notebook and I was like killed

00:48:21,590 --> 00:48:25,850
my grid search one there oops we go back

00:48:24,560 --> 00:48:27,440
and kill this I'm gonna open it again

00:48:25,850 --> 00:48:30,050
you can click this button to open it

00:48:27,440 --> 00:48:32,240
again so if you click on the serving

00:48:30,050 --> 00:48:34,130
this is in the instructions actually so

00:48:32,240 --> 00:48:39,770
in the instructions that we had at the

00:48:34,130 --> 00:48:41,540
beginning which were here we're gonna

00:48:39,770 --> 00:48:43,220
follow these first instructions so you

00:48:41,540 --> 00:48:45,320
can follow along here you're basically

00:48:43,220 --> 00:48:47,810
gonna start a Jupiter notebook and we're

00:48:45,320 --> 00:48:52,070
gonna run this serving train and export

00:48:47,810 --> 00:48:54,830
model notebook so I'll go back here and

00:48:52,070 --> 00:48:57,950
we go to serving and then going to open

00:48:54,830 --> 00:49:01,820
this train and export model notebook so

00:48:57,950 --> 00:49:03,290
if I run this this is not a spark

00:49:01,820 --> 00:49:04,430
notebook this is a Python notebook you

00:49:03,290 --> 00:49:07,220
can see that we're kind of moving

00:49:04,430 --> 00:49:08,990
between spark and Python notebooks quite

00:49:07,220 --> 00:49:10,520
quickly

00:49:08,990 --> 00:49:12,020
I want getting an error because I think

00:49:10,520 --> 00:49:16,130
the models already there so have to put

00:49:12,020 --> 00:49:18,410
in a different directory so so I've

00:49:16,130 --> 00:49:20,420
already trained this so let's go back

00:49:18,410 --> 00:49:21,830
out so this will work fine for you I

00:49:20,420 --> 00:49:24,640
need to I would need to delete that but

00:49:21,830 --> 00:49:29,210
I'm just gonna skip the deleting part

00:49:24,640 --> 00:49:32,390
leave and then once you've run that that

00:49:29,210 --> 00:49:34,910
takes that goes very quickly I'll delete

00:49:32,390 --> 00:49:36,260
this one here and then you key then

00:49:34,910 --> 00:49:37,850
you'll come into this particular page

00:49:36,260 --> 00:49:39,530
here the model serving page so you can

00:49:37,850 --> 00:49:42,680
just click on this button here create

00:49:39,530 --> 00:49:45,920
model serving and from here you can then

00:49:42,680 --> 00:49:47,570
select the model that you trained so the

00:49:45,920 --> 00:49:51,170
models will be in this data set called

00:49:47,570 --> 00:49:54,740
models and the one we trained is go

00:49:51,170 --> 00:49:57,440
amnesty we can click on that there we go

00:49:54,740 --> 00:49:58,760
so I no need to still press the create

00:49:57,440 --> 00:50:01,190
saving button and you can enable

00:49:58,760 --> 00:50:03,970
batching for it to make it for improve

00:50:01,190 --> 00:50:06,440
throughput at the cost of latency and

00:50:03,970 --> 00:50:10,010
then just press run that's it and then

00:50:06,440 --> 00:50:11,930
your notebook pottier tensorflow serving

00:50:10,010 --> 00:50:16,220
server will start serving that model and

00:50:11,930 --> 00:50:21,200
you can see the port and host IP address

00:50:16,220 --> 00:50:22,880
that it starts running out has anyone

00:50:21,200 --> 00:50:26,780
gotten anywhere here without we're

00:50:22,880 --> 00:50:28,850
trying okay let's let's go back in there

00:50:26,780 --> 00:50:31,480
we'll do the client and we'll see how we

00:50:28,850 --> 00:50:34,910
do the client so I've copied the port

00:50:31,480 --> 00:50:36,800
from from this particular page here I

00:50:34,910 --> 00:50:40,040
copied that port that the server's

00:50:36,800 --> 00:50:42,560
listening on and now I'm going to paste

00:50:40,040 --> 00:50:46,190
it in here on line 21 whereas the server

00:50:42,560 --> 00:50:49,670
IP and port then here so if I just run

00:50:46,190 --> 00:50:51,890
this what it's going to do is this is a

00:50:49,670 --> 00:50:54,740
Python program and you can see it's

00:50:51,890 --> 00:50:58,510
importing numpy which we have already

00:50:54,740 --> 00:51:00,620
it's using tensorflow we've got HDFS

00:50:58,510 --> 00:51:03,470
because we're going to read some images

00:51:00,620 --> 00:51:06,110
from HDFS it's basically a gr PC

00:51:03,470 --> 00:51:07,640
application it's a gr PC client and it's

00:51:06,110 --> 00:51:09,830
a bit messy to be honest

00:51:07,640 --> 00:51:11,750
alright so there's you know you've quite

00:51:09,830 --> 00:51:14,240
a bit of work to setup your client so

00:51:11,750 --> 00:51:17,000
the good news is that the other bad news

00:51:14,240 --> 00:51:19,190
is that it only works at Python 2 7 so

00:51:17,000 --> 00:51:20,230
if you enabled a project with python 3 6

00:51:19,190 --> 00:51:22,920
this won't work

00:51:20,230 --> 00:51:26,079
but the good news from yesterday is that

00:51:22,920 --> 00:51:30,849
tensorflow serving 1.8 was released and

00:51:26,079 --> 00:51:32,859
that supports a REST API so now it'll be

00:51:30,849 --> 00:51:34,930
much easier to use that so here we go

00:51:32,859 --> 00:51:36,520
the client ran it it ran a bunch of

00:51:34,930 --> 00:51:38,230
images it sent them to the central

00:51:36,520 --> 00:51:41,290
server who's going to classify them and

00:51:38,230 --> 00:51:45,730
we can see it got an accuracy of 93

00:51:41,290 --> 00:51:49,750
percent okay so that was the amnesty

00:51:45,730 --> 00:51:52,510
that we sent some images to and it said

00:51:49,750 --> 00:51:55,750
you know I think this is a 2 and 93

00:51:52,510 --> 00:51:57,190
percent of the time it was correct and

00:51:55,750 --> 00:51:59,560
this will handle you know it's pretty

00:51:57,190 --> 00:52:01,240
efficient model server tens we're

00:51:59,560 --> 00:52:03,310
serving it can handle hundreds of

00:52:01,240 --> 00:52:05,440
requests per second and you can leave

00:52:03,310 --> 00:52:07,150
these running and you can monitor your

00:52:05,440 --> 00:52:08,890
logs but obviously you want have more

00:52:07,150 --> 00:52:11,680
hooks in here and we've arrested the I

00:52:08,890 --> 00:52:14,079
to this so you can get up this data if

00:52:11,680 --> 00:52:16,119
you want to for example do some

00:52:14,079 --> 00:52:19,990
retraining of your model so I'm going to

00:52:16,119 --> 00:52:21,480
go back to the slides because I didn't

00:52:19,990 --> 00:52:23,650
talk about distributed training but

00:52:21,480 --> 00:52:26,589
we'll get will we'll have a quick look

00:52:23,650 --> 00:52:28,510
at that now so I mentioned earlier that

00:52:26,589 --> 00:52:30,569
you know we're working with some people

00:52:28,510 --> 00:52:37,180
they have a million images of of

00:52:30,569 --> 00:52:39,010
self-driving vehicle situations and if

00:52:37,180 --> 00:52:40,839
you want to train a model on those

00:52:39,010 --> 00:52:43,599
million images you know many people will

00:52:40,839 --> 00:52:46,240
say use transfer learning take a trained

00:52:43,599 --> 00:52:48,760
model and then do this on top of us but

00:52:46,240 --> 00:52:49,869
you know if you want to be the best in

00:52:48,760 --> 00:52:51,819
your industry and have the best

00:52:49,869 --> 00:52:54,010
prediction models you will not do that

00:52:51,819 --> 00:52:56,349
you'll train from scratch because you'll

00:52:54,010 --> 00:52:59,800
get better results so the problem is on

00:52:56,349 --> 00:53:02,680
a single GPU this inner loop of training

00:52:59,800 --> 00:53:04,869
it can take weeks for that kind of data

00:53:02,680 --> 00:53:06,660
set so you want to get weeks down to

00:53:04,869 --> 00:53:08,920
minutes and that's kind of where the

00:53:06,660 --> 00:53:12,339
Facebook's and Google of this world are

00:53:08,920 --> 00:53:15,099
so the the imagenet data set which takes

00:53:12,339 --> 00:53:17,260
two weeks on a large GPU to train google

00:53:15,099 --> 00:53:21,220
recently got a training time down to 18

00:53:17,260 --> 00:53:23,710
minutes on 256 TPU v2s

00:53:21,220 --> 00:53:26,109
Facebook had one hour on the p1 hundreds

00:53:23,710 --> 00:53:27,280
last year they're actually kind of

00:53:26,109 --> 00:53:29,589
competing with each other and see who

00:53:27,280 --> 00:53:31,960
can do the best have the you know

00:53:29,589 --> 00:53:33,730
quickest training time so the problem

00:53:31,960 --> 00:53:36,340
with distribute training is that

00:53:33,730 --> 00:53:39,550
if you do it for example in a

00:53:36,340 --> 00:53:42,670
multi-tenant cloud environment you get

00:53:39,550 --> 00:53:44,560
all of the problems of you know network

00:53:42,670 --> 00:53:46,750
i/o from other jobs causing your

00:53:44,560 --> 00:53:48,430
training job to slow down so the

00:53:46,750 --> 00:53:52,960
Facebook's and Google's do these on

00:53:48,430 --> 00:53:54,790
dedicated networks network IO becomes

00:53:52,960 --> 00:53:56,020
one of the main bottlenecks now I

00:53:54,790 --> 00:53:58,720
mentioned earlier that the parameter

00:53:56,020 --> 00:54:00,210
server model is not the future and it's

00:53:58,720 --> 00:54:02,080
not what people are using to get

00:54:00,210 --> 00:54:04,840
high-performance distributed training

00:54:02,080 --> 00:54:07,300
the model that people are using is what

00:54:04,840 --> 00:54:09,100
we call ring all reduce it's a it's an

00:54:07,300 --> 00:54:11,770
algorithm from high performance

00:54:09,100 --> 00:54:15,010
computing and because we're Network IO

00:54:11,770 --> 00:54:16,600
band here what happens with the width we

00:54:15,010 --> 00:54:18,190
can see what's happening in the ring

00:54:16,600 --> 00:54:20,320
reduce out and it's just sending all the

00:54:18,190 --> 00:54:22,900
gradients between all the nodes so

00:54:20,320 --> 00:54:25,600
they're using both the upload and

00:54:22,900 --> 00:54:28,420
download bandwidth of all of the nodes

00:54:25,600 --> 00:54:30,880
efficiently in the parameter server

00:54:28,420 --> 00:54:32,170
model the GPU servers are basically just

00:54:30,880 --> 00:54:33,910
sending their data up and getting some

00:54:32,170 --> 00:54:35,530
down they're not necessarily using all

00:54:33,910 --> 00:54:37,750
their available bandwidth and they're

00:54:35,530 --> 00:54:39,660
doing it quite synchronously so what

00:54:37,750 --> 00:54:41,470
we'll see is some there are is some

00:54:39,660 --> 00:54:43,650
optimizations in the ring reduce

00:54:41,470 --> 00:54:46,120
algorithm that when you're computing

00:54:43,650 --> 00:54:48,580
gradients on the backwards pass through

00:54:46,120 --> 00:54:50,980
the layers you can send those gradients

00:54:48,580 --> 00:54:52,780
on a per layer basis while you're

00:54:50,980 --> 00:54:54,400
computing the gradients for the next

00:54:52,780 --> 00:54:55,900
layer so you don't need to just wait

00:54:54,400 --> 00:54:57,160
till you get to the bottom and send all

00:54:55,900 --> 00:54:59,500
the gradients you can do it as you

00:54:57,160 --> 00:55:01,240
progress down through your network and

00:54:59,500 --> 00:55:04,150
if you have a res net 101 or resin at

00:55:01,240 --> 00:55:06,160
150 150 layers there's a lot of

00:55:04,150 --> 00:55:09,280
opportunities for overlapping i/o and

00:55:06,160 --> 00:55:10,540
compute there so network bandwidth is

00:55:09,280 --> 00:55:12,790
the bottleneck for a distributed

00:55:10,540 --> 00:55:15,340
training this is me playing up two

00:55:12,790 --> 00:55:17,740
cliches factor the parameter server is a

00:55:15,340 --> 00:55:21,430
bottleneck and the the gradients flow

00:55:17,740 --> 00:55:24,040
freely like the guiness on the ring all

00:55:21,430 --> 00:55:25,900
reduced model now we've done some

00:55:24,040 --> 00:55:29,170
experiments on some of our GPU servers

00:55:25,900 --> 00:55:31,870
this is to GPU servers with ten GPUs

00:55:29,170 --> 00:55:33,820
each their ten ADT is we this is the

00:55:31,870 --> 00:55:35,470
cheapest infrastructure that if you want

00:55:33,820 --> 00:55:38,020
to build it on Prem this is what you do

00:55:35,470 --> 00:55:40,030
and we bought InfiniBand 40 gigabit

00:55:38,020 --> 00:55:43,210
cards they were I think seven of them

00:55:40,030 --> 00:55:45,730
for 700 euros with the switch and the

00:55:43,210 --> 00:55:48,020
the cards himself cost

00:55:45,730 --> 00:55:51,410
700 euros I think now is the current

00:55:48,020 --> 00:55:53,990
price but we got extremely good

00:55:51,410 --> 00:55:57,680
performance I mean we're getting about I

00:55:53,990 --> 00:56:00,320
think we're getting 1100 images for 10

00:55:57,680 --> 00:56:02,630
GPUs and about 1800 images per second

00:56:00,320 --> 00:56:04,100
this is what ResNet training this one

00:56:02,630 --> 00:56:06,350
here's inception but we get similar

00:56:04,100 --> 00:56:08,930
results for res not on and this is on

00:56:06,350 --> 00:56:10,310
the image net data set so it was

00:56:08,930 --> 00:56:12,620
interesting what that was I was at a

00:56:10,310 --> 00:56:15,230
talk recently by a large corporation who

00:56:12,620 --> 00:56:18,680
on their cloud on InfiniBand said well

00:56:15,230 --> 00:56:23,000
we got 1800 images per second on 64 a

00:56:18,680 --> 00:56:25,760
high-end Nvidia cards v1 hundreds and we

00:56:23,000 --> 00:56:29,030
were getting the same on 2010 ATT eyes

00:56:25,760 --> 00:56:30,170
which are the commodity cards so the

00:56:29,030 --> 00:56:31,520
reason why we're getting good results

00:56:30,170 --> 00:56:34,010
here is that you can see the network is

00:56:31,520 --> 00:56:36,200
this is over 10 gigabit per second for

00:56:34,010 --> 00:56:39,680
training between all the nodes but the

00:56:36,200 --> 00:56:41,270
GPU utilization is extremely high now if

00:56:39,680 --> 00:56:43,100
you look at the parameter server model

00:56:41,270 --> 00:56:45,350
this is what it looks like it's a bit of

00:56:43,100 --> 00:56:47,750
a mess GP utilization goes up and down

00:56:45,350 --> 00:56:50,020
as nodes are waiting for the model to

00:56:47,750 --> 00:56:52,010
come back from the parameter servers and

00:56:50,020 --> 00:56:57,110
network bandwidth is only used

00:56:52,010 --> 00:56:58,490
sporadically so then if you're convinced

00:56:57,110 --> 00:57:00,140
that distributed training is the way to

00:56:58,490 --> 00:57:05,510
go and you'd like to do it what does the

00:57:00,140 --> 00:57:08,060
code look like well horev odd is a is a

00:57:05,510 --> 00:57:10,610
layer that you write tensorflow code

00:57:08,060 --> 00:57:12,380
inside of and you basically just have a

00:57:10,610 --> 00:57:15,740
couple of primitives to work with you

00:57:12,380 --> 00:57:17,900
have this HVD object it allows you to

00:57:15,740 --> 00:57:19,670
wrap your optimizer so that the

00:57:17,900 --> 00:57:22,120
gradients get sent between all the GPUs

00:57:19,670 --> 00:57:24,770
you can use some MPI primitives like

00:57:22,120 --> 00:57:27,560
local ranked number of nodes in the

00:57:24,770 --> 00:57:29,930
system the H or size H V data size and

00:57:27,560 --> 00:57:31,520
you know in ranked 0 you might say well

00:57:29,930 --> 00:57:33,980
I'm going to check my models in the in

00:57:31,520 --> 00:57:36,530
the GPU at rank 0 and then the other

00:57:33,980 --> 00:57:39,110
ones I'll do whatever so this will just

00:57:36,530 --> 00:57:41,510
be boilerplate tensorflow code that

00:57:39,110 --> 00:57:43,220
you're putting in here and if you go

00:57:41,510 --> 00:57:45,160
with the distributed tense for model

00:57:43,220 --> 00:57:49,220
you'll find the code is quite complex

00:57:45,160 --> 00:57:51,730
it's a bit messy so the API that we have

00:57:49,220 --> 00:57:55,250
has it has a good bit more than just the

00:57:51,730 --> 00:57:59,170
things we've seen so far there's support

00:57:55,250 --> 00:57:59,170
for Kafka and SPARC its link

00:57:59,369 --> 00:58:03,579
we've seen some of the support for

00:58:01,450 --> 00:58:05,380
things like tensor board and HDFS so I

00:58:03,579 --> 00:58:07,869
showed already this example the model

00:58:05,380 --> 00:58:09,670
serving example and but what I didn't

00:58:07,869 --> 00:58:11,380
talk about was well you know once you're

00:58:09,670 --> 00:58:14,020
serving your model what do I do now

00:58:11,380 --> 00:58:18,250
right how does the loop get closed we

00:58:14,020 --> 00:58:20,290
start by ingesting data you know we go

00:58:18,250 --> 00:58:23,349
all the way to experimentation training

00:58:20,290 --> 00:58:25,119
and serving models but then you need to

00:58:23,349 --> 00:58:27,760
monitor your models in production so you

00:58:25,119 --> 00:58:30,099
need to look at the difference for

00:58:27,760 --> 00:58:31,300
example between your performance your

00:58:30,099 --> 00:58:33,310
prediction performance when you were

00:58:31,300 --> 00:58:34,720
training versus when you're actually

00:58:33,310 --> 00:58:37,300
serving so you need to maintain

00:58:34,720 --> 00:58:38,950
statistics so if a prediction comes in

00:58:37,300 --> 00:58:40,450
you need to then say well this

00:58:38,950 --> 00:58:43,660
prediction was made for this input data

00:58:40,450 --> 00:58:45,609
and this was the actual results and we

00:58:43,660 --> 00:58:47,800
save that data and we can then use that

00:58:45,609 --> 00:58:50,170
to compare it with what we trained on

00:58:47,800 --> 00:58:51,730
and say well there's some SKUs happening

00:58:50,170 --> 00:58:53,920
here some training serving SKUs

00:58:51,730 --> 00:58:56,349
happening it's now time to start a new

00:58:53,920 --> 00:58:58,000
round of training we need to train a new

00:58:56,349 --> 00:59:02,020
model on because we've new day the

00:58:58,000 --> 00:59:04,300
training data available and you know we

00:59:02,020 --> 00:59:06,369
call this covariant shift as the method

00:59:04,300 --> 00:59:08,440
that you would use to to signify when

00:59:06,369 --> 00:59:09,460
you when you want to do training now

00:59:08,440 --> 00:59:11,920
there may be other reasons why you

00:59:09,460 --> 00:59:13,270
wanted to retrain your models maybe you

00:59:11,920 --> 00:59:15,910
know some event has happened that you

00:59:13,270 --> 00:59:18,670
know about that affects the models maybe

00:59:15,910 --> 00:59:20,319
you just want to do periodically but you

00:59:18,670 --> 00:59:22,119
need to have the tools to do that so in

00:59:20,319 --> 00:59:24,849
our case you can use the REST API and

00:59:22,119 --> 00:59:29,589
the data stored in HDFS to decide on

00:59:24,849 --> 00:59:33,819
when to train now let's go back and

00:59:29,589 --> 00:59:35,859
let's let's look at this this actual

00:59:33,819 --> 00:59:38,170
practical part so we kind of did the

00:59:35,859 --> 00:59:40,930
first part which was the tour and we did

00:59:38,170 --> 00:59:44,349
our end-to-end pipeline for 4m nest

00:59:40,930 --> 00:59:50,849
let's have a look now ask the hot dog

00:59:44,349 --> 00:59:50,849
example unless anyone has any questions

00:59:51,359 --> 00:59:57,099
no ok I'll aim to finish a couple of

00:59:55,750 --> 00:59:59,980
minutes early as well so that we can

00:59:57,099 --> 01:00:06,160
enjoy the fine weather and fine cuisine

00:59:59,980 --> 01:00:10,450
of Berlin okay so let's go to this

01:00:06,160 --> 01:00:11,620
Python notebook for hot dogs so what you

01:00:10,450 --> 01:00:14,500
actually have to do here

01:00:11,620 --> 01:00:18,420
is you have to right-click on it to

01:00:14,500 --> 01:00:23,110
download it so I'm just gonna save us

01:00:18,420 --> 01:00:24,880
I'll save it to this Documents folder so

01:00:23,110 --> 01:00:27,760
the you know I could have included this

01:00:24,880 --> 01:00:29,950
in your in your tour but I just wanted

01:00:27,760 --> 01:00:33,220
to give you experience of how easy it is

01:00:29,950 --> 01:00:34,510
to get data into the system so if I want

01:00:33,220 --> 01:00:35,710
to create a new notebook let's say I'm

01:00:34,510 --> 01:00:43,150
gonna create a new notebook we'll call

01:00:35,710 --> 01:00:45,850
it the TF er what I want to do is I want

01:00:43,150 --> 01:00:48,640
to upload that Python notebook that I

01:00:45,850 --> 01:00:50,830
downloaded into this project but I also

01:00:48,640 --> 01:00:53,350
want to get the hot dog data set into

01:00:50,830 --> 01:00:56,080
the project so I can start with one or

01:00:53,350 --> 01:00:58,960
the other and this is this thing here is

01:00:56,080 --> 01:01:01,600
telling me I should start with searching

01:00:58,960 --> 01:01:04,360
for the data set hot dog and imported

01:01:01,600 --> 01:01:09,460
into my project and I also need to

01:01:04,360 --> 01:01:09,940
enable Python 3.6 so let's do those two

01:01:09,460 --> 01:01:12,670
things

01:01:09,940 --> 01:01:14,680
I'll start by enabling Python 3.6 so now

01:01:12,670 --> 01:01:17,560
I'm inside the new project I created TF

01:01:14,680 --> 01:01:19,180
ER and we go to Python and now you can

01:01:17,560 --> 01:01:20,440
see this didn't appear when you run the

01:01:19,180 --> 01:01:23,350
tour cuz when you run the tour it

01:01:20,440 --> 01:01:25,810
automatically enabled python 2 7 but you

01:01:23,350 --> 01:01:29,260
can see that you can choose here to

01:01:25,810 --> 01:01:29,860
activate python 2 7 or 3 6 if you have a

01:01:29,260 --> 01:01:31,780
ready-made

01:01:29,860 --> 01:01:34,180
environment and unaccounted environment

01:01:31,780 --> 01:01:36,160
as a Yano file you can just use that and

01:01:34,180 --> 01:01:38,080
import it and then that will set up your

01:01:36,160 --> 01:01:40,330
own account environment with the

01:01:38,080 --> 01:01:44,800
libraries that you need but I'm going to

01:01:40,330 --> 01:01:46,090
enable 3 6 and when you click on it you

01:01:44,800 --> 01:01:48,550
can see that it brings you to a new

01:01:46,090 --> 01:01:51,580
screen and you have to wait for this bar

01:01:48,550 --> 01:01:53,830
to disappear now we have a relatively a

01:01:51,580 --> 01:01:57,070
much larger clustered mess with like you

01:01:53,830 --> 01:02:00,330
know 36 nodes so it can take you know a

01:01:57,070 --> 01:02:03,280
minute or two for environment to be

01:02:00,330 --> 01:02:04,720
created on all the all the hosts the way

01:02:03,280 --> 01:02:07,080
it actually works is that we have a base

01:02:04,720 --> 01:02:09,340
environment that we clone on the hosts

01:02:07,080 --> 01:02:11,290
so an account allows you to clone them

01:02:09,340 --> 01:02:13,720
and you can see here that we have a

01:02:11,290 --> 01:02:14,950
condor channel so defaults is quite a

01:02:13,720 --> 01:02:18,340
well-known channel but you also have

01:02:14,950 --> 01:02:20,260
something called condor forge if you're

01:02:18,340 --> 01:02:23,380
familiar with Python

01:02:20,260 --> 01:02:25,000
you you will probably know that counter

01:02:23,380 --> 01:02:26,140
Forge but you can search in here for

01:02:25,000 --> 01:02:29,040
libraries I'm going to search for a

01:02:26,140 --> 01:02:31,720
couple will go back to the filter

01:02:29,040 --> 01:02:33,190
because I need a couple in here so if we

01:02:31,720 --> 01:02:37,000
go back to our instructions it will tell

01:02:33,190 --> 01:02:39,700
me that I need matplotlib pillow and

01:02:37,000 --> 01:02:41,260
numpy so you can see it's still not

01:02:39,700 --> 01:02:44,080
completed so it takes about a minute we

01:02:41,260 --> 01:02:45,730
let that continue going so what I'm

01:02:44,080 --> 01:02:47,170
going to do well that environment is

01:02:45,730 --> 01:02:49,900
being created is I'm going to take the

01:02:47,170 --> 01:02:51,700
next step which says find your hot dog

01:02:49,900 --> 01:02:54,520
data set and import it into the project

01:02:51,700 --> 01:02:56,650
so if you go up here and type in hot dog

01:02:54,520 --> 01:02:59,590
we can make a mistake and just go like

01:02:56,650 --> 01:03:00,850
that hot dog and we can see that there's

01:02:59,590 --> 01:03:03,070
lots of things so a lot of people have

01:03:00,850 --> 01:03:05,050
created projects called hot dog here and

01:03:03,070 --> 01:03:07,480
then we have something called public

01:03:05,050 --> 01:03:11,620
data set and cluster hot dog so if you

01:03:07,480 --> 01:03:13,210
click on that you can see that this is

01:03:11,620 --> 01:03:15,940
the hot dog data set and there's a bit

01:03:13,210 --> 01:03:19,030
of information about it and here's some

01:03:15,940 --> 01:03:20,260
good links there's a full standalone app

01:03:19,030 --> 01:03:22,450
if you want to look up that later

01:03:20,260 --> 01:03:23,980
there's a Kaggle computational hot dog

01:03:22,450 --> 01:03:27,100
or not in this app I touch boy torch

01:03:23,980 --> 01:03:28,540
notebook as well for us so what I'm

01:03:27,100 --> 01:03:30,610
going to do is if I press this button up

01:03:28,540 --> 01:03:32,230
here it's gonna say add this to one of

01:03:30,610 --> 01:03:35,980
your project so I'm gonna add it to this

01:03:32,230 --> 01:03:38,440
new ti4 project that I created and like

01:03:35,980 --> 01:03:42,670
select my project and I press add data

01:03:38,440 --> 01:03:43,960
set so it doesn't copy the data set so

01:03:42,670 --> 01:03:46,060
if you're working in a kind of a you

01:03:43,960 --> 01:03:49,360
know a big enterprise environment and

01:03:46,060 --> 01:03:51,010
you want to move data between projects

01:03:49,360 --> 01:03:52,960
as such you often end up copying the

01:03:51,010 --> 01:03:56,290
data in this case it's not it's linking

01:03:52,960 --> 01:03:59,230
the data in so if I go into my project

01:03:56,290 --> 01:04:02,290
and click on data sets you can see that

01:03:59,230 --> 01:04:04,990
it appears in here as data sets : : hot

01:04:02,290 --> 01:04:06,520
dog and the owner may not be you and

01:04:04,990 --> 01:04:08,710
maybe someone else in this case it is me

01:04:06,520 --> 01:04:11,620
and if you want you can go in and look

01:04:08,710 --> 01:04:13,630
at the data and we can click on this one

01:04:11,620 --> 01:04:17,110
hot dog here and I can just pick some

01:04:13,630 --> 01:04:18,730
random image here and preview us and it

01:04:17,110 --> 01:04:21,520
kind of looks like a hot dog it's hard

01:04:18,730 --> 01:04:23,740
to tell I think it might be a bit warped

01:04:21,520 --> 01:04:24,760
and there in the aspect ratio but you

01:04:23,740 --> 01:04:26,860
can have a look through your images

01:04:24,760 --> 01:04:30,580
there and I'm gonna go back and check if

01:04:26,860 --> 01:04:32,320
python is finished it has and I'm gonna

01:04:30,580 --> 01:04:34,600
install the libraries that I'm

01:04:32,320 --> 01:04:36,760
to install so I'm going to take my plot

01:04:34,600 --> 01:04:40,600
lid first I just take the latest version

01:04:36,760 --> 01:04:42,280
you can pick any version you want but it

01:04:40,600 --> 01:04:46,480
should be compatible one so I need to

01:04:42,280 --> 01:04:47,170
install matplotlib pillow and numpy so

01:04:46,480 --> 01:04:50,860
let's do it

01:04:47,170 --> 01:04:53,560
I think numpy is already installed but I

01:04:50,860 --> 01:04:55,390
can do it again just to be sure to be

01:04:53,560 --> 01:04:57,160
sure to be sure so you can pick your

01:04:55,390 --> 01:05:00,610
version here and then you just go

01:04:57,160 --> 01:05:03,730
install and if you want to watch you can

01:05:00,610 --> 01:05:06,760
see that they're ongoing here there is a

01:05:03,730 --> 01:05:09,700
button retry failed up so if you've used

01:05:06,760 --> 01:05:11,740
hip in particular we when we first

01:05:09,700 --> 01:05:13,930
started with this scheme we I taught

01:05:11,740 --> 01:05:15,310
this in a course at the university kdh

01:05:13,930 --> 01:05:18,190
in Stockholm with a hundred and thirty

01:05:15,310 --> 01:05:19,810
students and I gave them Conda and I

01:05:18,190 --> 01:05:22,240
said they got hers counter you got can't

01:05:19,810 --> 01:05:24,610
afford you have counter defaults and

01:05:22,240 --> 01:05:26,710
immediately it was like heckles of well

01:05:24,610 --> 01:05:29,200
I want this library opening eyes not

01:05:26,710 --> 01:05:30,940
encounter this is not there so you have

01:05:29,200 --> 01:05:34,600
a lot more libraries available in hip

01:05:30,940 --> 01:05:36,130
than you do encounter but the problem

01:05:34,600 --> 01:05:38,200
with pip is that it's a bit of a jungle

01:05:36,130 --> 01:05:39,850
so you can install one library and hip

01:05:38,200 --> 01:05:41,530
it will pull them one dependency and

01:05:39,850 --> 01:05:43,090
then install another library and the

01:05:41,530 --> 01:05:46,150
dependency may conflict with another

01:05:43,090 --> 01:05:52,270
library and problems can arise Conda is

01:05:46,150 --> 01:05:53,830
much better curated as a so let's have a

01:05:52,270 --> 01:05:58,300
look okay so I was gonna install the

01:05:53,830 --> 01:06:01,750
last one which was pillow so pillow is a

01:05:58,300 --> 01:06:03,760
library for manipulating images in this

01:06:01,750 --> 01:06:05,200
case we need to decode the images we're

01:06:03,760 --> 01:06:07,060
going to read JPEGs in and we want to

01:06:05,200 --> 01:06:11,620
decode them and store them and TF

01:06:07,060 --> 01:06:13,330
records okay so that's my environment

01:06:11,620 --> 01:06:14,770
setup it's I just need to wait for that

01:06:13,330 --> 01:06:17,350
one to finish but while we're waiting

01:06:14,770 --> 01:06:20,620
for it to finish we can go back and see

01:06:17,350 --> 01:06:24,010
what I'm supposed to do next I have to

01:06:20,620 --> 01:06:26,590
upload the the the notebook yes so let's

01:06:24,010 --> 01:06:29,230
do that so we can go to data set here

01:06:26,590 --> 01:06:30,940
and I'm gonna upload it in directly into

01:06:29,230 --> 01:06:33,310
Jupiter because when you open up Jupiter

01:06:30,940 --> 01:06:35,800
this is it shows this as your base

01:06:33,310 --> 01:06:40,930
folder now you can navigate around but

01:06:35,800 --> 01:06:43,530
I'm just gonna load it directly here it

01:06:40,930 --> 01:06:43,530
was this one

01:06:43,950 --> 01:06:49,230
and you can see it's pretty quick so now

01:06:46,680 --> 01:06:51,030
I can start Jupiter and we can have a

01:06:49,230 --> 01:06:55,829
look to what do I want to do I'm going

01:06:51,030 --> 01:06:58,309
to click on tensor flow and that looks

01:06:55,829 --> 01:07:05,010
okay to me

01:06:58,309 --> 01:07:10,079
let me start up Jupiter again okay so

01:07:05,010 --> 01:07:13,230
now I can open my hotdogs notebook is

01:07:10,079 --> 01:07:14,880
quite big and again this is Magnus

01:07:13,230 --> 01:07:17,280
Peterson who did the original version

01:07:14,880 --> 01:07:19,799
and he is very entertaining videos if

01:07:17,280 --> 01:07:21,030
you want to look them up on YouTube so

01:07:19,799 --> 01:07:24,869
what I'm gonna do is this is what you

01:07:21,030 --> 01:07:26,549
end up doing in in Python notebooks if

01:07:24,869 --> 01:07:29,160
you're not familiar with them you end up

01:07:26,549 --> 01:07:31,109
just executing each paragraph one at a

01:07:29,160 --> 01:07:32,640
time I just want to make sure that

01:07:31,109 --> 01:07:34,829
everything is installed and you can see

01:07:32,640 --> 01:07:37,140
that pillow is still ongoing so when if

01:07:34,829 --> 01:07:40,290
I get to pillow before its installed

01:07:37,140 --> 01:07:40,920
maybe we'll have an issue but it's still

01:07:40,290 --> 01:07:43,920
ongoing

01:07:40,920 --> 01:07:46,349
let's go so we're gonna this one's gonna

01:07:43,920 --> 01:07:49,410
use matplotlib which we have imported

01:07:46,349 --> 01:07:51,630
already should be okay again this is a

01:07:49,410 --> 01:07:54,869
it is not a PI spark notebook this is

01:07:51,630 --> 01:07:56,819
just a Python notebook some of them are

01:07:54,869 --> 01:07:58,049
PI spark and some are Python typically

01:07:56,819 --> 01:07:59,640
PI spark is when we want to do

01:07:58,049 --> 01:08:01,020
distributed things this is not

01:07:59,640 --> 01:08:04,799
distributed but you could make it

01:08:01,020 --> 01:08:06,930
distributed if you wanted to we're using

01:08:04,799 --> 01:08:09,990
tens for 1.8 which is the latest version

01:08:06,930 --> 01:08:12,990
for this month until another few weeks

01:08:09,990 --> 01:08:14,309
pass and then we'll have 1.9 and I'll

01:08:12,990 --> 01:08:15,780
just explain this code here at the

01:08:14,309 --> 01:08:19,529
beginning so all we did at the beginning

01:08:15,780 --> 01:08:20,699
was we imported some libraries so the

01:08:19,529 --> 01:08:23,640
first thing we're gonna do is we're

01:08:20,699 --> 01:08:26,069
gonna read the data set from HDFS and

01:08:23,640 --> 01:08:27,779
it's into this data tier here and then

01:08:26,069 --> 01:08:29,489
we have a training directory and a test

01:08:27,779 --> 01:08:31,679
directory there are sub sub directories

01:08:29,489 --> 01:08:35,100
of the data directory so you can see

01:08:31,679 --> 01:08:38,219
that the path given here is HDFS : :

01:08:35,100 --> 01:08:41,509
projects data sets + hot dog so I could

01:08:38,219 --> 01:08:45,330
just change this to look like this and

01:08:41,509 --> 01:08:47,190
that would be correct maybe slightly so

01:08:45,330 --> 01:08:49,350
where did I get this path from well this

01:08:47,190 --> 01:08:52,319
is the shared data set I imported and if

01:08:49,350 --> 01:08:55,589
I go back here we can go to our data

01:08:52,319 --> 01:08:57,509
sets and you can see this particular if

01:08:55,589 --> 01:08:59,130
I take the test for example you

01:08:57,509 --> 01:09:00,809
see the path is written up here to that

01:08:59,130 --> 01:09:03,599
particular folder so I can just copy it

01:09:00,809 --> 01:09:05,339
to my clipboard paste it in here and

01:09:03,599 --> 01:09:09,029
then I get that path because this is a

01:09:05,339 --> 01:09:10,980
shared directory if you have datasets

01:09:09,029 --> 01:09:13,380
inside your project you can use a

01:09:10,980 --> 01:09:14,940
relative path which is HDFS get

01:09:13,380 --> 01:09:16,409
underscore project path which will give

01:09:14,940 --> 01:09:19,409
the relative directory to your project

01:09:16,409 --> 01:09:21,569
but this data set is not in my project

01:09:19,409 --> 01:09:24,299
and unlinking it in from another project

01:09:21,569 --> 01:09:27,719
called data sets so I need to just give

01:09:24,299 --> 01:09:29,969
the full path to it okay so what this is

01:09:27,719 --> 01:09:31,859
going to do basically is define some

01:09:29,969 --> 01:09:34,049
variables it does an execute much code

01:09:31,859 --> 01:09:38,819
there's gonna be two classes of image

01:09:34,049 --> 01:09:42,119
hot dog and not hot dog and then what

01:09:38,819 --> 01:09:43,469
we're going to do is just look at what's

01:09:42,119 --> 01:09:46,589
in the directory just to make sure it's

01:09:43,469 --> 01:09:48,239
okay it's using pi tape to do it and it

01:09:46,589 --> 01:09:49,980
looks a bit messy but you can see that

01:09:48,239 --> 01:09:52,859
yeah those folders are in there that's

01:09:49,980 --> 01:09:55,289
correct and then the next thing we need

01:09:52,859 --> 01:09:57,179
to do is we're getting into data science

01:09:55,289 --> 01:09:58,739
here now so I'm gonna I'm not gonna go

01:09:57,179 --> 01:10:00,480
into huge detail on this but a few

01:09:58,739 --> 01:10:04,170
questions just shared at me and what

01:10:00,480 --> 01:10:05,880
kind of say okay what's going on here so

01:10:04,170 --> 01:10:08,219
this one this particular paragraph is

01:10:05,880 --> 01:10:10,559
quite long it's defining a class called

01:10:08,219 --> 01:10:13,440
data sets and data sets has some paths

01:10:10,559 --> 01:10:17,639
will give you a the path to your files

01:10:13,440 --> 01:10:20,730
your training data your test data and in

01:10:17,639 --> 01:10:22,289
fact what you might expect at this point

01:10:20,730 --> 01:10:24,059
is you might say well my training data

01:10:22,289 --> 01:10:25,920
is all the images you know I want to

01:10:24,059 --> 01:10:28,199
make a data set with all the images but

01:10:25,920 --> 01:10:30,269
this is actually going to make the

01:10:28,199 --> 01:10:31,619
training data will be the file names for

01:10:30,269 --> 01:10:33,239
the image it's not the actual images

01:10:31,619 --> 01:10:35,610
we're going to just set up a data set

01:10:33,239 --> 01:10:38,039
containing all of the file names and the

01:10:35,610 --> 01:10:39,619
first part will basically get our we'll

01:10:38,039 --> 01:10:42,119
have something called one hunt encoding

01:10:39,619 --> 01:10:44,250
our labels will be one huntin coded will

01:10:42,119 --> 01:10:45,659
basically say whether an images is a hot

01:10:44,250 --> 01:10:49,139
dog or not so you'll have a one when

01:10:45,659 --> 01:10:52,800
it's a hot dog zero if it's not and vice

01:10:49,139 --> 01:10:54,389
versa if it's if if it's the case so now

01:10:52,800 --> 01:10:58,980
we've defined that data set object we

01:10:54,389 --> 01:11:00,869
point it to our images and you can see

01:10:58,980 --> 01:11:02,849
it prints out basically the some of the

01:11:00,869 --> 01:11:04,949
directories there and then we can get

01:11:02,849 --> 01:11:06,900
the class names inside that data set so

01:11:04,949 --> 01:11:08,579
we've got hot dog now hot dog in our

01:11:06,900 --> 01:11:10,289
days that has all these file names and

01:11:08,579 --> 01:11:11,370
we can just have a look at that to

01:11:10,289 --> 01:11:12,930
basically print out

01:11:11,370 --> 01:11:15,780
the first filename so this is going to

01:11:12,930 --> 01:11:17,520
print that the training path for the

01:11:15,780 --> 01:11:19,170
first image and that's this one here you

01:11:17,520 --> 01:11:22,710
can look it up in the in the UI if you

01:11:19,170 --> 01:11:24,210
want to it's this one let's say we can

01:11:22,710 --> 01:11:26,760
we can do it briefly if we have a quick

01:11:24,210 --> 01:11:35,100
look it's that one there and if I go

01:11:26,760 --> 01:11:36,540
back to here we can just see which

01:11:35,100 --> 01:11:44,510
folder was in it's in the training data

01:11:36,540 --> 01:11:47,640
osa it's in the hot dog folder okay

01:11:44,510 --> 01:11:50,070
it's in here so this is the image that

01:11:47,640 --> 01:11:56,160
it's just gonna show and it kind of

01:11:50,070 --> 01:11:58,410
looks like that so if we keep going down

01:11:56,160 --> 01:11:59,970
we can see that we can get this is if

01:11:58,410 --> 01:12:02,670
you're not familiar with Python this

01:11:59,970 --> 01:12:05,880
next syntax here a method this method

01:12:02,670 --> 01:12:10,890
get test set for the data set object it

01:12:05,880 --> 01:12:13,080
can return basically a list with three

01:12:10,890 --> 01:12:15,870
three elements so we can then assign

01:12:13,080 --> 01:12:17,580
them in a single statement so we can get

01:12:15,870 --> 01:12:20,490
it'll return back the image pass test

01:12:17,580 --> 01:12:22,020
the the class labels for them the

01:12:20,490 --> 01:12:24,780
classes for them and then labels for

01:12:22,020 --> 01:12:26,730
those those files and we can look at the

01:12:24,780 --> 01:12:32,970
first file it's that one and the data

01:12:26,730 --> 01:12:34,470
set itself has 998 images I found some

01:12:32,970 --> 01:12:36,180
other ones you can go look in the net

01:12:34,470 --> 01:12:38,190
you can probably augment this data set

01:12:36,180 --> 01:12:40,170
it's not it's not a big one and we'll

01:12:38,190 --> 01:12:43,530
see later on the result of that it's not

01:12:40,170 --> 01:12:45,960
big the next paragraph is basically

01:12:43,530 --> 01:12:49,470
going to just be a paragraph to plot

01:12:45,960 --> 01:12:54,810
images with so we can view them later on

01:12:49,470 --> 01:12:57,150
and it uses matplotlib I believe it does

01:12:54,810 --> 01:12:58,890
and then we have a method a helper

01:12:57,150 --> 01:13:01,940
method to help us load scenario two

01:12:58,890 --> 01:13:05,700
paths into we're going to use this I am

01:13:01,940 --> 01:13:09,390
path I am read path to read and the

01:13:05,700 --> 01:13:12,630
images so so let's plot a few images to

01:13:09,390 --> 01:13:15,780
see if they're correct and it's running

01:13:12,630 --> 01:13:17,310
here you can change these numbers here

01:13:15,780 --> 01:13:19,130
if you want to plot a few different ones

01:13:17,310 --> 01:13:25,250
did I run them

01:13:19,130 --> 01:13:30,679
okay let's make it hopped over a couple

01:13:25,250 --> 01:13:36,050
of these let's run them again so I get a

01:13:30,679 --> 01:13:38,320
start from this beginning I'll clear the

01:13:36,050 --> 01:13:38,320
APIs

01:13:43,430 --> 01:13:53,090
I'm just gonna jump back to where we

01:13:45,260 --> 01:13:55,190
were before okay so we're plotting

01:13:53,090 --> 01:13:56,990
images load and finding loading images

01:13:55,190 --> 01:13:59,000
and they were plot a few images here

01:13:56,990 --> 01:14:00,200
okay so if you want to just try to check

01:13:59,000 --> 01:14:02,540
it a few different images just to

01:14:00,200 --> 01:14:06,140
convince yourself that this is actually

01:14:02,540 --> 01:14:08,480
doing it we can do it like that and then

01:14:06,140 --> 01:14:10,820
we get a bunch of different images okay

01:14:08,480 --> 01:14:12,980
so now we have some basic code to read

01:14:10,820 --> 01:14:15,020
up the the filenames for all the images

01:14:12,980 --> 01:14:18,980
we want to turn them into tensor flow

01:14:15,020 --> 01:14:20,630
records and we have we're defining a

01:14:18,980 --> 01:14:25,060
path that we're gonna store them in it's

01:14:20,630 --> 01:14:28,250
called resources train TF records and

01:14:25,060 --> 01:14:30,890
we're gonna store the the test TF

01:14:28,250 --> 01:14:34,790
records in the same folder and this data

01:14:30,890 --> 01:14:36,380
tear folder so that's our data tier that

01:14:34,790 --> 01:14:38,320
we can look at the data tier here it's

01:14:36,380 --> 01:14:40,190
in it's in here it's called resources

01:14:38,320 --> 01:14:42,320
it's this one here

01:14:40,190 --> 01:14:46,880
so we can see there's no TF records

01:14:42,320 --> 01:14:50,210
there and what we'll do is we'll define

01:14:46,880 --> 01:14:51,950
some functions to print progress as

01:14:50,210 --> 01:14:53,480
we're generating them and this is where

01:14:51,950 --> 01:14:57,790
we get into tensorflow code it's very

01:14:53,480 --> 01:15:01,400
messy so if you're using the TF record

01:14:57,790 --> 01:15:04,490
file format and data frames you have to

01:15:01,400 --> 01:15:06,680
define each feature you can see this is

01:15:04,490 --> 01:15:09,920
our feature and we have to take the

01:15:06,680 --> 01:15:12,920
value and wrap it inside an int 64 list

01:15:09,920 --> 01:15:14,750
and that's itself in wrapped inside a

01:15:12,920 --> 01:15:18,220
feature so you have multiple layers of

01:15:14,750 --> 01:15:22,490
nesting there which is not easy for

01:15:18,220 --> 01:15:23,750
understandability so what this code is

01:15:22,490 --> 01:15:25,310
going to do is going to decode the

01:15:23,750 --> 01:15:29,770
images you can see it says from pill

01:15:25,310 --> 01:15:32,330
import image and at some point here

01:15:29,770 --> 01:15:34,190
let's have a look in IM read we're going

01:15:32,330 --> 01:15:35,870
to decode the image so that's going to

01:15:34,190 --> 01:15:42,290
take a compressed JPEG and turn it into

01:15:35,870 --> 01:15:44,180
a bigger expanded array so let's run

01:15:42,290 --> 01:15:46,520
that code because it takes a few seconds

01:15:44,180 --> 01:15:48,140
to run now we're getting an error path

01:15:46,520 --> 01:15:53,420
train records not defined

01:15:48,140 --> 01:16:02,989
okay I must have skipped over that okay

01:15:53,420 --> 01:16:06,260
I think this one here okay so I skipped

01:16:02,989 --> 01:16:07,760
over one of the paragraphs so we can see

01:16:06,260 --> 01:16:09,920
what it's doing is it's now reading all

01:16:07,760 --> 01:16:12,770
the files it's converting them into

01:16:09,920 --> 01:16:15,350
tensorflow records it's it's doing this

01:16:12,770 --> 01:16:17,090
on the local file system because Pihl

01:16:15,350 --> 01:16:19,310
doesn't support HDFS which is a bit

01:16:17,090 --> 01:16:20,870
messy and then the copies a results back

01:16:19,310 --> 01:16:30,710
into HDFS so we'll have them available

01:16:20,870 --> 01:16:32,540
in our in our UI so we can go into our

01:16:30,710 --> 01:16:34,820
UI and actually just make sure that the

01:16:32,540 --> 01:16:36,590
file is appearing there someone's

01:16:34,820 --> 01:16:40,010
finished we can start converting the

01:16:36,590 --> 01:16:43,310
next one the test data you can see it's

01:16:40,010 --> 01:16:45,620
kicking off so if we refresh here we can

01:16:43,310 --> 01:16:47,480
see that our training records appeared

01:16:45,620 --> 01:16:49,880
in here and if you look at the size of

01:16:47,480 --> 01:16:52,280
it it's 57 megabytes so that's a little

01:16:49,880 --> 01:16:53,750
bit bigger than the training data set

01:16:52,280 --> 01:16:56,810
which was I think sixteen or twenty

01:16:53,750 --> 01:16:58,969
megabytes so when we decoded the images

01:16:56,810 --> 01:17:02,090
and expanded them they became a bit

01:16:58,969 --> 01:17:06,910
bigger so now we're converting our

01:17:02,090 --> 01:17:06,910
training data and that's nearly done

01:17:08,800 --> 01:17:13,850
don't and now what we want to do is is

01:17:12,290 --> 01:17:16,489
we're going to use the estimator

01:17:13,850 --> 01:17:18,590
framework in tensorflow this is what

01:17:16,489 --> 01:17:20,090
Google use internally and this is being

01:17:18,590 --> 01:17:23,810
used in production in a lot of places

01:17:20,090 --> 01:17:25,670
the estimated framework is a way of

01:17:23,810 --> 01:17:27,830
trying to abstract out training so that

01:17:25,670 --> 01:17:30,920
you just basically say here's the

01:17:27,830 --> 01:17:33,530
training data inside a data set in the

01:17:30,920 --> 01:17:37,429
data API and tensorflow here's my test

01:17:33,530 --> 01:17:39,890
data here's my evaluator function here's

01:17:37,429 --> 01:17:42,380
a function to give you the records one

01:17:39,890 --> 01:17:44,570
at a time here's a function to parse the

01:17:42,380 --> 01:17:48,980
records before you if you you feed them

01:17:44,570 --> 01:17:50,449
into tensorflow and and you put them all

01:17:48,980 --> 01:17:53,090
together and then you can run it as a

01:17:50,449 --> 01:17:54,650
single abstraction so i'm gonna define

01:17:53,090 --> 01:17:57,980
some of the functions for our estimator

01:17:54,650 --> 01:18:01,580
this one is basically going to decode

01:17:57,980 --> 01:18:04,010
our raw image data from from the from

01:18:01,580 --> 01:18:07,430
the TF records examples it's going to

01:18:04,010 --> 01:18:11,030
decode those images it needs to turn the

01:18:07,430 --> 01:18:13,040
the the int the 8-bit int into into a

01:18:11,030 --> 01:18:16,550
float32 because we're going to train and

01:18:13,040 --> 01:18:20,660
you know have 32 floating bits floating

01:18:16,550 --> 01:18:23,660
point numbers and then we have to

01:18:20,660 --> 01:18:25,370
extract the the labels and the and the

01:18:23,660 --> 01:18:28,190
bytes for the images so we return them

01:18:25,370 --> 01:18:30,230
back to the topple and then the next

01:18:28,190 --> 01:18:35,090
thing we do is we define this input

01:18:30,230 --> 01:18:36,620
function which allows you to when you're

01:18:35,090 --> 01:18:37,940
training you might be want to repeat

01:18:36,620 --> 01:18:39,680
going through the data set when you're

01:18:37,940 --> 01:18:42,380
testing you don't want to repeat and

01:18:39,680 --> 01:18:44,449
then we we need to pass back an iterator

01:18:42,380 --> 01:18:46,550
or we need to get nitrate err to our

01:18:44,449 --> 01:18:49,190
data set this is part of the the data

01:18:46,550 --> 01:18:52,820
set framework and with that ID Reiter we

01:18:49,190 --> 01:18:54,290
can get out an image and label and in

01:18:52,820 --> 01:18:56,360
this case we want a batch of them not an

01:18:54,290 --> 01:18:57,890
individual one and then we have to

01:18:56,360 --> 01:19:02,360
return them as addicts which is a little

01:18:57,890 --> 01:19:04,160
bit messy so this is our train it is the

01:19:02,360 --> 01:19:07,280
input function which will take our input

01:19:04,160 --> 01:19:09,680
file names and return back some records

01:19:07,280 --> 01:19:14,390
to the to the estimator and this is for

01:19:09,680 --> 01:19:16,010
the test function so let's do some

01:19:14,390 --> 01:19:19,820
training and we're going to do training

01:19:16,010 --> 01:19:21,410
on a pre count estimator so there's

01:19:19,820 --> 01:19:23,150
there's some pre-built ones that you

01:19:21,410 --> 01:19:26,960
don't write yourself this one is called

01:19:23,150 --> 01:19:27,680
the DNN classifier a deep neural network

01:19:26,960 --> 01:19:29,390
classifier

01:19:27,680 --> 01:19:32,420
I'll just started running here and then

01:19:29,390 --> 01:19:35,480
we can we can see what it looks like so

01:19:32,420 --> 01:19:36,590
the kind of unfortunate part of this

01:19:35,480 --> 01:19:39,050
framework is there's a lot of

01:19:36,590 --> 01:19:41,449
boilerplate code for each column or each

01:19:39,050 --> 01:19:43,520
feature you have to wrap your values you

01:19:41,449 --> 01:19:45,469
can see there in the feature image of

01:19:43,520 --> 01:19:47,060
feature columns this is part of the

01:19:45,469 --> 01:19:49,969
neural network how it's going to look

01:19:47,060 --> 01:19:51,680
it's going to have 512 hidden units in

01:19:49,969 --> 01:19:55,670
one layer tune or 56 in the next there

01:19:51,680 --> 01:19:58,130
128 in the next layer and this is the

01:19:55,670 --> 01:19:59,600
the at the estimator that we define we

01:19:58,130 --> 01:20:02,210
say that here the future columns are

01:19:59,600 --> 01:20:04,219
training with the network architecture

01:20:02,210 --> 01:20:05,719
structure we're going to use a Rallo

01:20:04,219 --> 01:20:07,820
activation function and there's going to

01:20:05,719 --> 01:20:11,840
be two classes it's a binary classifier

01:20:07,820 --> 01:20:14,180
so once it's train is finished training

01:20:11,840 --> 01:20:17,030
and this one is still training it should

01:20:14,180 --> 01:20:18,370
finish it's 200 steps and whereas we've

01:20:17,030 --> 01:20:20,820
already passed and step on

01:20:18,370 --> 01:20:24,220
one but it takes a little bit of time

01:20:20,820 --> 01:20:26,740
then you can then you can evaluate so

01:20:24,220 --> 01:20:29,020
the the estimator framework allows you

01:20:26,740 --> 01:20:31,210
to supply an evaluation function and

01:20:29,020 --> 01:20:34,120
we've just it's just been executed now

01:20:31,210 --> 01:20:35,820
and we can see what kind of results

01:20:34,120 --> 01:20:39,520
we'll get out of it

01:20:35,820 --> 01:20:44,610
so it takes a few seconds and we can see

01:20:39,520 --> 01:20:48,370
we're getting 50% so this is basically

01:20:44,610 --> 01:20:50,320
random this is not what we want now you

01:20:48,370 --> 01:20:52,630
can say well why is that well we have a

01:20:50,320 --> 01:20:54,580
data set of only a thousand images we

01:20:52,630 --> 01:20:57,880
can augment it you can do this is an

01:20:54,580 --> 01:21:00,130
exercise for home you can use transfer

01:20:57,880 --> 01:21:03,310
learning to take an existing train model

01:21:00,130 --> 01:21:04,780
we could rotate the images and deform

01:21:03,310 --> 01:21:06,910
them to make the data set larger

01:21:04,780 --> 01:21:08,740
probably you'll need on the order of a

01:21:06,910 --> 01:21:12,250
few tens of thousands of images to get

01:21:08,740 --> 01:21:13,690
this to work really well and the best

01:21:12,250 --> 01:21:15,910
option would be probably to use a pre

01:21:13,690 --> 01:21:17,860
train model and then train on top of it

01:21:15,910 --> 01:21:21,520
with these so there is another one down

01:21:17,860 --> 01:21:23,410
here we can see with another estimator

01:21:21,520 --> 01:21:25,240
that's defined I'm this takes a bit

01:21:23,410 --> 01:21:26,560
longer to run it's a convolutional

01:21:25,240 --> 01:21:28,540
neural net that was just a a

01:21:26,560 --> 01:21:31,540
feed-forward neural network and comp

01:21:28,540 --> 01:21:33,640
nets are better at doing image

01:21:31,540 --> 01:21:35,350
classification and this one I ran

01:21:33,640 --> 01:21:36,820
earlier and we can see the results we

01:21:35,350 --> 01:21:38,470
got was 58%

01:21:36,820 --> 01:21:40,870
I'm not going to train here cuz it takes

01:21:38,470 --> 01:21:42,880
a bit more time which is a bit better so

01:21:40,870 --> 01:21:44,500
there's a signal there that if we run

01:21:42,880 --> 01:21:47,230
and train that for a long time and we

01:21:44,500 --> 01:21:48,580
augmented and rotated our images that we

01:21:47,230 --> 01:21:52,810
should get at least some better

01:21:48,580 --> 01:21:55,750
predictions from okay I said I promised

01:21:52,810 --> 01:21:57,430
I'd finish up a little bit early so with

01:21:55,750 --> 01:21:59,380
that I'm just gonna say that look this

01:21:57,430 --> 01:22:01,870
is the framework it's open source it's a

01:21:59,380 --> 01:22:03,310
European project we come from the

01:22:01,870 --> 01:22:06,070
University but we have a start up we're

01:22:03,310 --> 01:22:07,870
trying to push this out there if you're

01:22:06,070 --> 01:22:09,340
interested in doing deep learning and

01:22:07,870 --> 01:22:13,600
particularly on large volumes of data

01:22:09,340 --> 01:22:14,830
with lots of GPUs come talk to me and a

01:22:13,600 --> 01:22:16,030
lot of people have worked on this and a

01:22:14,830 --> 01:22:20,410
lot of people are still working on us

01:22:16,030 --> 01:22:25,600
and it's a European platform for deep

01:22:20,410 --> 01:22:27,010
learning with big data called hops with

01:22:25,600 --> 01:22:30,600
that I'll take questions and I have a

01:22:27,010 --> 01:22:30,600
hand up over here on the side

01:22:31,470 --> 01:22:36,790
is it possible

01:22:33,870 --> 01:22:39,010
so can we upload our datasets two hops

01:22:36,790 --> 01:22:41,170
Hadoop and you know run our satellite

01:22:39,010 --> 01:22:42,640
images for example of course I mean we

01:22:41,170 --> 01:22:46,180
have a cluster in Lille I can give you

01:22:42,640 --> 01:22:47,980
access to with pet abideth and is there

01:22:46,180 --> 01:22:51,430
a storage limit like how much that I can

01:22:47,980 --> 01:22:54,190
upload we can we're flexible ok thank

01:22:51,430 --> 01:23:04,210
you with us thank you

01:22:54,190 --> 01:23:06,370
alright more questions anyone thanks for

01:23:04,210 --> 01:23:13,330
sticking here cool thank you one more

01:23:06,370 --> 01:23:15,130
quick we'll take it anyway so are we

01:23:13,330 --> 01:23:17,830
going to see pie-dar john harwood in

01:23:15,130 --> 01:23:22,390
hops sort of soon yes is the answer

01:23:17,830 --> 01:23:25,780
ok looking forward yeah I'm looking

01:23:22,390 --> 01:23:28,780
forward to a good model server or for

01:23:25,780 --> 01:23:30,910
trained PI torch models that's the big

01:23:28,780 --> 01:23:34,470
big kind of missing picture piece for

01:23:30,910 --> 01:23:36,280
fir for pi tours right now you know but

01:23:34,470 --> 01:23:38,680
all right thank you

01:23:36,280 --> 01:23:43,169
ok thanks

01:23:38,680 --> 01:23:43,169

YouTube URL: https://www.youtube.com/watch?v=9haYBKMU4mM


